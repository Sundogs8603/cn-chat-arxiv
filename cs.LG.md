# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space.](http://arxiv.org/abs/2310.11256) | 本文介绍了两种在高斯混合模型空间中的Gromov-Wasserstein类型距离，分别用于评估分布之间的距离和推导最优的点分配方案。 |
| [^2] | [CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion.](http://arxiv.org/abs/2310.11248) | CrossCodeEval是一个多元化和多语言的基准测试，用于跨文件代码补全，在真实的软件开发场景中，需要跨文件上下文理解才能准确完成代码。 |
| [^3] | [Entity Matching using Large Language Models.](http://arxiv.org/abs/2310.11244) | 这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。 |
| [^4] | [Learning to Sample Better.](http://arxiv.org/abs/2310.11232) | 本课程介绍了基于动态输运测度的生成建模方法的最新进展，重点讲述了如何通过数据学习这些映射，并通过正反馈循环改进蒙特卡洛采样技术。 |
| [^5] | [Zipformer: A faster and better encoder for automatic speech recognition.](http://arxiv.org/abs/2310.11230) | Zipformer是一种更快速、更节省内存、性能更好的自动语音识别编码器，通过U-Net-like编码器结构、重新组织的块结构、改进的LayerNorm、新的激活函数和新的优化器等方式实现了优化。实验证明它在LibriSpeech、Aishell-1和Wenet等数据集上表现出更快的收敛和更好的性能。 |
| [^6] | [Understanding Fairness Surrogate Functions in Algorithmic Fairness.](http://arxiv.org/abs/2310.11211) | 本文研究了算法公平性中的公平性代理函数，并发现了代理和公平性定义之间存在一个差距。这个差距决定了一个代理函数能否适当替代一个公平性定义。 |
| [^7] | [Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations.](http://arxiv.org/abs/2310.11207) | 本文研究了大型语言模型生成的自解释在情感分析和特征归因解释任务中的效果，并探讨了不同的引导方法。 |
| [^8] | [Federated Learning with Nonvacuous Generalisation Bounds.](http://arxiv.org/abs/2310.11203) | 这项研究提出了一种新的策略来在联邦学习中训练随机预测器，通过保护每个节点的隐私并且具有数值上非空的泛化界限，可以在保持预测性能的同时实现数据共享和保护隐私。 |
| [^9] | [EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms.](http://arxiv.org/abs/2310.11198) | 本研究探索了在运动意向解码领域应用不同通道关注机制的可行性，通过构建一个轻量级架构框架，并在同一环境中比较它们的影响，结果表明这些机制的易集成性和低计算复杂度使其成为BCI中运动意向解码的有效方法。 |
| [^10] | [A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback.](http://arxiv.org/abs/2310.11188) | 本论文提出了一种改进的EXP3算法MUD-EXP3，用于解决具有多用户延迟反馈的对抗性多臂赌博机问题。该算法通过考虑来自不同用户的重要性加权估计器，在每个回合做出决策。在已知终止回合索引$T$、用户数量$M$、臂数量$N$和延迟上界$d_{max}$的情况下，算法的遗憾值为$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$。对于未知$T$的情况，提出了一种自适应算法。 |
| [^11] | [Efficiently Visualizing Large Graphs.](http://arxiv.org/abs/2310.11186) | 本文介绍了一种名为t-SGNE的新颖降维方法，用于高效可视化大规模图表。该方法利用图表的邻域结构避免了耗时的成对相似度计算，并使时间复杂度从二次降低为线性。同时，还提出了一种适应t-SGNE的图表嵌入算法SPLEE。通过SPLEE获得高维嵌入，并使用t-SGNE降维进行可视化。 |
| [^12] | [MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection.](http://arxiv.org/abs/2310.11169) | MST-GAT是一种用于多模态时间序列异常检测的时空图注意力网络，通过使用多头注意力模块和关系注意力模块来捕捉模态之间的时空相关性，提高了异常检测性能。 |
| [^13] | [Serenade: A Model for Human-in-the-loop Automatic Chord Estimation.](http://arxiv.org/abs/2310.11165) | 该论文提出了一种人为参与的自动和弦估计模型，通过人与自回归模型共同创建和声注释，实现在音乐信息检索任务中的性能提升。 |
| [^14] | [Probing the Creativity of Large Language Models: Can models produce divergent semantic association?.](http://arxiv.org/abs/2310.11158) | 本研究使用分散联想任务探索大型语言模型的创造性思维能力。研究发现，先进的模型可以生成不同的语义关联，超过了大多数人类的水平。 |
| [^15] | [A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model.](http://arxiv.org/abs/2310.11143) | 本研究提出了一种基于机器学习的概率暴露模型，可以更准确地估计德国室内氡气分布，并具有更高的空间分辨率。 |
| [^16] | [BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference.](http://arxiv.org/abs/2310.11142) | BayesDiff提出了一种像素级不确定性估计方法，用于扩散模型生成结果。该方法通过贝叶斯推断和不确定性迭代原则来实现高效的估计，并可在图像生成任务中过滤低质量图像，增强成功生成结果，并纠正失败生成结果中的伪影。 |
| [^17] | [Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control.](http://arxiv.org/abs/2310.11138) | 本研究提出了一种新的集合强化学习算法TEEN，旨在通过促进更多样化的轨迹来提高预期回报的同时增强集合策略的样本多样性。 |
| [^18] | [Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation.](http://arxiv.org/abs/2310.11132) | 这项工作研究了一种针对混合类型数据集的非参数条件独立性检验方法，该方法使用基于k最近邻的CMI估计器和本地置换方案，针对现实世界中包含数值和分类变量的情况。 |
| [^19] | [FROST: Towards Energy-efficient AI-on-5G Platforms -- A GPU Power Capping Evaluation.](http://arxiv.org/abs/2310.11131) | FROST是一个能够通过对ML管道能量消耗进行分析并优化硬件的解决方案，能够在不降低模型准确性或引入延迟的情况下实现高达26.4%的能量节约。 |
| [^20] | [Topological Expressivity of ReLU Neural Networks.](http://arxiv.org/abs/2310.11130) | 本研究从拓扑的角度研究了ReLU神经网络在二元分类问题中的表达能力，通过衡量网络对数据拓扑结构的改变程度，发现深层ReLU网络比浅层网络具有指数级的拓扑简化能力。 |
| [^21] | [Sensitivity-Aware Amortized Bayesian Inference.](http://arxiv.org/abs/2310.11122) | 本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。 |
| [^22] | [Super resolution of histopathological frozen sections via deep learning preserving tissue structure.](http://arxiv.org/abs/2310.11112) | 本文提出了一种使用深度学习的超分辨率方法，可用于组织病理冰冻切片，通过保留关键图像细节，减少诊断错误的风险。 |
| [^23] | [Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data.](http://arxiv.org/abs/2310.11110) | 本文展示了在只有未标记数据的情况下，通过一些最小的先验信息，可以计算出精确的LDA投影向量。数值实验验证了这种最小信息的线性判别分析（MILDA）模型与有监督的LDA模型的性能接近。 |
| [^24] | [Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification.](http://arxiv.org/abs/2310.11104) | 本文研究了使用ReLUs作为激活函数的前馈神经网络的本地Lipschitz常数计算，并介绍了一种通过精确性验证计算上界的方法。 |
| [^25] | [HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning.](http://arxiv.org/abs/2310.11102) | HGCVAE是一种将生成式学习和对比学习整合为一体的异构图学习方法，通过利用生成式的自监督学习能力来解决异构图学习的挑战。 |
| [^26] | [Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads.](http://arxiv.org/abs/2310.11096) | 本文提出了Dysta，一种稀疏感知的动态和静态调度器，用于稀疏多DNN工作负载。通过分析多个稀疏DNN的使用情况，发现了优化的机会，并提出了Dysta调度器，利用静态稀疏模式和动态稀疏信息进行调度。 |
| [^27] | [Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs.](http://arxiv.org/abs/2310.11094) | 本论文介绍一种新的评估过拟合的得分，该得分通过监测深度模型在验证数据上的遗忘速率来衡量。实证结果发现，尽管整体上泛化性能得到改善，但在数据空间的某些区域中，泛化性能可能会下降。这一观察结果有助于澄清关于深度神经网络中过拟合的困惑情况。 |
| [^28] | [SODA: Robust Training of Test-Time Data Adaptors.](http://arxiv.org/abs/2310.11093) | SODA提出了一种通过训练数据适配器来提高测试数据适应性能的方法，解决了传统方法中由于数据适配器可能导致数据特征损坏带来的限制性改进问题。 |
| [^29] | [MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation.](http://arxiv.org/abs/2310.11088) | 本论文提出了一种名为MeKB-Rec的跨领域推荐方法，在推荐系统中解决了冷启动问题。该方法利用个人知识图谱作为领域不变的用户兴趣表示，通过学习语义表示和注入世界知识，实现了对新用户的零-shot推荐。 |
| [^30] | [Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection.](http://arxiv.org/abs/2310.11087) | 本研究提出了一种新颖的端到端方法，名为特征金字塔双向长短时记忆网络（FPbiLSTM），用于使用智能手机传感器进行交通方式检测。该方法通过减少所需的传感器数目和处理需求，实现更高效的建模过程，同时兼顾结果质量。通过扩展特征金字塔网络，它能够捕捉各种交通方式中的时间移动模式。 |
| [^31] | [In-Context Few-Shot Relation Extraction via Pre-Trained Language Models.](http://arxiv.org/abs/2310.11085) | 本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。 |
| [^32] | [CSG: Curriculum Representation Learning for Signed Graph.](http://arxiv.org/abs/2310.11083) | 本文提出了一种用于有符号图的课程表示学习框架（CSG），通过引入课程化训练方法和轻量级机制，实现了按难易程度优化样本展示顺序，从而提高有符号图神经网络（SGNN）模型的准确性和稳定性。 |
| [^33] | [Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction.](http://arxiv.org/abs/2310.11082) | 这篇论文提出了一种基于多组学采样的图转换器用于合成致死预测，通过引入浅层多视图GNN和标准的自注意力机制，解决了图神经网络在SL预测中的限制问题，并利用多组学数据中的非SL基因关系信息提高了预测性能。 |
| [^34] | [United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit.](http://arxiv.org/abs/2310.11077) | 本论文提出了一种新的深度网络集成分类器，通过分析和实证发现过拟合时分类器之间的方差增加，基于此构建了一种通过整个训练过程中最具一致性的预测结果来对抗过拟合的方法。在实验中表明，这种方法在多个图像和文本分类任务上的表现优于传统的集成方法。 |
| [^35] | [Resampling Stochastic Gradient Descent Cheaply for Efficient Uncertainty Quantification.](http://arxiv.org/abs/2310.11065) | 本研究提出了两种低成本重采样的方法，用于构建随机梯度下降解的置信区间，这一方法可以有效减少计算工作量，并绕过现有方法中的混合条件。 |
| [^36] | [Locally Differentially Private Graph Embedding.](http://arxiv.org/abs/2310.11060) | 该论文提出了一种局部差分隐私图嵌入框架（LDP-GE），该框架采用LDP机制来保护节点数据的隐私，并使用个性化PageRank作为近似度度量来学习节点表示。大量实验证明，LDP-GE在隐私和效用方面取得了有利的折衷效果，并且明显优于现有的方法。 |
| [^37] | [Causal Feature Selection via Transfer Entropy.](http://arxiv.org/abs/2310.11059) | 本论文提出了一种新的方法，通过将特征选择和因果发现相结合，在时间序列中实现了因果特征选择，并利用传输熵来估计信息的因果流动。 |
| [^38] | [Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation.](http://arxiv.org/abs/2310.11049) | 这篇论文介绍了我们在SemEval-2023法律评估任务6上的提交，主要集中在法律命名实体识别、法律判决预测和带解释的法院判决预测等子任务上。我们进行了多个实验，并取得了在各个子任务中具有竞争力的排名。 |
| [^39] | [Understanding Contrastive Learning via Distributionally Robust Optimization.](http://arxiv.org/abs/2310.11048) | 通过分布鲁棒优化的视角，该研究揭示了对比学习对采样偏差的内在容忍度，并提供了几个关键见解。 |
| [^40] | [Fast Graph Condensation with Structure-based Neural Tangent Kernel.](http://arxiv.org/abs/2310.11046) | 本文提出了一种以数据为中心的解决方案，将大型图数据集压缩为较小的集合而不会损失GNN的预测性能。通过将图结构压缩问题转化为核岭回归任务，利用基于结构的神经切线内核来捕捉图的拓扑结构。 |
| [^41] | [Matrix Compression via Randomized Low Rank and Low Precision Factorization.](http://arxiv.org/abs/2310.11028) | 通过随机化的低秩和低精度因式分解，我们提出了一种矩阵压缩算法，可以有效地减小存储和处理大型矩阵所需的计算资源和内存使用。 |
| [^42] | [SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning.](http://arxiv.org/abs/2310.11025) | 这项工作提出了一种带符号注意力的图变换器（SignGT），它能够自适应地捕捉各种频率信息，对于学习复杂节点关系的异质图等不同的图形非常高效。 |
| [^43] | [Compatible Transformer for Irregularly Sampled Multivariate Time Series.](http://arxiv.org/abs/2310.11022) | 本文提出了一种适用于不规则采样的多变量时间序列的兼容Transformer方法（CoFormer），通过利用内变量/间变量的注意力机制，实现对每个个体样本的综合时序交互特征学习。 |
| [^44] | [Pure Exploration in Asynchronous Federated Bandits.](http://arxiv.org/abs/2310.11015) | 该论文研究了异步联邦赌博机中的纯探索问题，并提出了首个在完全异步环境下实现近乎最优样本复杂性和高效通信成本的联邦异步多臂赌博机和线性赌博机纯探索算法。 |
| [^45] | [Hyperspectral In-Memory Computing with Optical Frequency Combs and Programmable Optical Memories.](http://arxiv.org/abs/2310.11014) | 提出了一种基于光学频率梳和可编程光记忆的高光谱内存计算架构，通过集成空间复用和频率复用的方法，解决了光学计算系统的并行性、可编程性和可扩展性问题。 |
| [^46] | [From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling.](http://arxiv.org/abs/2310.11011) | 本文综述了因果生成建模的技术，其中分为因果表示学习和可控反事实生成两个部分，这些模型融合了因果理论，解决了深度生成模型的一些根本性缺点，并提供了分布偏移鲁棒性、公平性和互操作性等有益属性。 |
| [^47] | [Adaptive Pairwise Encodings for Link Prediction.](http://arxiv.org/abs/2310.11009) | 提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。 |
| [^48] | [Correction Focused Language Model Training for Speech Recognition.](http://arxiv.org/abs/2310.11003) | 本研究介绍了一种为语音识别进行纠错的语言模型训练方法，通过定义易出错单词的分数并将其用作先验分布来指导训练，并利用大型语言模型进行纠错型训练。实验证明该方法在领域适应任务中有效，相对传统方法可以显著降低单词错误率。 |
| [^49] | [Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques.](http://arxiv.org/abs/2310.11001) | 本研究提出了一种利用物联网传感器网络和机器学习技术进行空间局部天气预测和异常检测的新方法。通过利用来自多个位置和物联网传感器的数据，创建高分辨率的天气模型，并监测天气参数的变化，系统能够提高预测的空间分辨率，并实时检测异常。这一系统有潜力改善决策。 |
| [^50] | [Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation.](http://arxiv.org/abs/2310.10998) | 本论文提出了一种在线传播框架和两种新的节点自适应传播方法，用于加速可扩展的图神经网络推论。这些方法能够根据节点的拓扑信息自定义每个节点的最佳传播深度，从而避免冗余特征传播，并通过简单的超参数灵活地管理准确性和延迟之间的权衡，以适应不同的延迟限制。 |
| [^51] | [Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques.](http://arxiv.org/abs/2310.10987) | 本研究使用学术、人口统计、社会经济和宏观经济等不同类型的数据进行大学辍学预测，并发现学术数据是对模型性能最具影响力的数据类型。 |
| [^52] | [Exact nonlinear state estimation.](http://arxiv.org/abs/2310.10976) | 本文引入了一种新的非线性估计理论，该理论试图弥合现有数据同化方法中的差距。具体而言，推导出了一个能够推广至任意非高斯分布的共轭变换滤波器 (CTF)，并提出了其集合近似版本 (ECTF)。 |
| [^53] | [Context-Aware Meta-Learning.](http://arxiv.org/abs/2310.10971) | 本文提出了一种上下文感知的元学习算法，可以在推理过程中学习新的视觉概念而无需微调。该方法在多个元学习基准中表现优异，超过或与目前的最先进算法相匹配。 |
| [^54] | [SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery.](http://arxiv.org/abs/2310.10970) | SD-PINN是一种基于深度学习的方法，能够通过一个神经网络恢复空间相关的偏微分方程（PDE）系数，无需领域特定的物理专业知识，并且对噪声具有稳健性。同时，它能够通过空间变化低秩假设恢复没有可用测量数据的位置的系数。 |
| [^55] | [The neural network models with delays for solving absolute value equations.](http://arxiv.org/abs/2310.10965) | 提出了一种具有延迟的无逆神经网络模型，用于求解绝对值方程，具有指数收敛性和解决一类特殊AVE的能力。 |
| [^56] | [Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction.](http://arxiv.org/abs/2310.10958) | 本文提出了一种通过线性预测来提高深度神经网络训练效率和性能的方法。实验结果表明，在相同的训练条件和时期下，通过采用该方法可以提高模型的性能。 |
| [^57] | [A State-Vector Framework for Dataset Effects.](http://arxiv.org/abs/2310.10955) | 本研究提出了一个状态向量框架，用于系统地研究数据集的效果。我们发现一些常用的语言理解数据集对模型有显著的效果，这些效果集中在几个语言维度上。此外，我们观察到数据集可能对模型的非相关维度产生"溢出"效应。这个框架为负责任和鲁棒模型开发中的数据集效果提供了一个系统的理解。 |
| [^58] | [A Local Graph Limits Perspective on Sampling-Based GNNs.](http://arxiv.org/abs/2310.10953) | 该论文提出了一种基于局部图界限的训练大型输入图的采样型图神经网络的理论框架，通过对小样本的训练，我们可以获得与整个图训练类似的结果。这为使用采样训练GNN提供了新的理论理解，并提供了在选择最佳模型、超参数和采样算法方面更高效的方法。 |
| [^59] | [Restricted Tweedie Stochastic Block Models.](http://arxiv.org/abs/2310.10952) | 这项研究提出了一种新的随机块模型，可以处理由非负零膨胀连续边权组成的邻接矩阵，特别适用于模拟国际贸易网络。该模型结合了节点信息和动态效应，并且可以独立于社区标签进行参数估计。一个高效的两步算法被开发用于估计协变效应和社区标签。 |
| [^60] | [Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control.](http://arxiv.org/abs/2310.10948) | 本文提出了一种基于异构图多智能体强化学习和交通理论的创新解决方案，通过将车辆编队和交通信号控制作为不同的强化学习智能体，并结合图神经网络实现协调，以优化交通流量和缓解城市拥堵。 |
| [^61] | [Multi-point Feedback of Bandit Convex Optimization with Hard Constraints.](http://arxiv.org/abs/2310.10946) | 本文研究了带有硬约束的强盗凸优化问题，并提出了一种基于惩罚的近端梯度下降方法，实现了次线性的遗憾和累积硬约束违反界。 |
| [^62] | [Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning.](http://arxiv.org/abs/2310.10943) | 这项研究比较了最优控制 (OC)和强化学习 (RL)方法在自主无人机赛车中的效果，发现强化学习方法优于最优控制方法。研究表明，强化学习能够直接优化任务层面的目标，并利用领域的随机因素，而最优控制的分解限制了控制器的行为范围。 |
| [^63] | [MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts.](http://arxiv.org/abs/2310.10941) | MASON-NLP提出了一种基于深度学习的方法，通过分析社交媒体文本检测抑郁症状。任务1的目标是评估不同条件的相关性。 |
| [^64] | [Fast and Simple Spectral Clustering in Theory and Practice.](http://arxiv.org/abs/2310.10939) | 本文提出了一种快速简单的谱聚类算法，通过使用幂方法计算的少量向量进行顶点嵌入，几乎线性时间计算，能够可靠地恢复出真实聚类，并且在实验中比其他聚类算法更快，聚类准确性相当。 |
| [^65] | [Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti.](http://arxiv.org/abs/2310.10935) | 这项研究引入了第一个用于孟加拉语和锡尔赫蒂语的意图检测和槽位填充的全面数据集，并发现大型语言模型在处理不充足数据的下游任务上表现出强大能力。 |
| [^66] | [Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care.](http://arxiv.org/abs/2310.10928) | 本研究旨在使用音频数据预测初级卫生保健中的抑郁风险，以减少误诊并改善整体诊断和治疗结果。 |
| [^67] | [Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines.](http://arxiv.org/abs/2310.10910) | 本研究比较了经典和量子计算范式中机器学习算法的效果，发现量子支持向量机（QSVM）在某些场景下可以达到与经典支持向量机（SVM）相媲美的准确率，但执行时间较长。同时指出，增加量子计算能力和并行度可以显著改善量子机器学习算法的性能。 |
| [^68] | [Heterogenous Memory Augmented Neural Networks.](http://arxiv.org/abs/2310.10909) | 本文介绍了一种新颖的异构内存增强方法，通过引入具有注意机制的可学习内存令牌，可以在不增加巨大计算开销的情况下有效提升性能。 |
| [^69] | [Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?.](http://arxiv.org/abs/2310.10908) | 该论文研究了密集预训练Transformer是否以及如何从自发的模块化结构中获益。 |
| [^70] | [Surrogate Active Subspaces for Jump-Discontinuous Functions.](http://arxiv.org/abs/2310.10907) | 该论文提出了一种针对不连续函数的替代主动子空间方法，扩展了活跃子空间的应用范围，并通过数值实验验证了该方法的有效性。 |
| [^71] | [Instilling Inductive Biases with Subnetworks.](http://arxiv.org/abs/2310.10899) | 通过子网络注入归纳偏置，这项研究探索了理解和控制神经网络行为的方法。通过发现功能子网络并利用它们，可以显著减少训练模型所需的数据量。 |
| [^72] | [Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection.](http://arxiv.org/abs/2310.10898) | 这项研究分析了不同的模块化最大化算法对于社区检测中实现最优划分的性能，并发现最常用的模块化方法与最优划分之间存在显著差异。 |
| [^73] | [Active Learning Framework for Cost-Effective TCR-Epitope Binding Affinity Prediction.](http://arxiv.org/abs/2310.10893) | 本研究提出了一个高效的TCR-表位结合亲和力预测的主动学习框架ActiveTCR，通过最小化注释成本并最大化性能提升，有效降低了预测过程的成本。 |
| [^74] | [The Calysto Scheme Project.](http://arxiv.org/abs/2310.10886) | Calysto Scheme是一个将Scheme转换为Python的项目，支持标准Scheme功能和Python库的互操作。它被广泛应用于教育和教学，且已被成功整合入Jupyter Notebook。 |
| [^75] | [BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling.](http://arxiv.org/abs/2310.10879) | 本论文提出了一种名为BLoad的训练方案，通过最小化填充量并实现高效的分布式数据并行训练，来提高训练效率和召回率。 |
| [^76] | [Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout.](http://arxiv.org/abs/2310.10878) | 通过神经网络实现的滚动展开方法可以控制连接和自动化车辆的生态驾驶，解决了现有方法中高计算和内存需求的问题，并实现了与强化学习相当的性能。 |
| [^77] | [Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey.](http://arxiv.org/abs/2310.10874) | 这篇研究基于世界价值观调查数据，应用机器学习方法研究了宗教信仰、价值观和行为的变化趋势。研究结果表明，在大多数国家中，年龄和收入是影响宗教性的最重要因素。 |
| [^78] | [Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2310.10856) | 本文提出了一种在信号化道路网络中，使用多智能体深度强化学习联合优化交通信号控制和车辆路径规划的方法，通过建立相关性和共享观测和奖励来促进代理之间的交互和合作，从而提高网络性能。 |
| [^79] | [CoTFormer: More Tokens With Attention Make Up For Less Depth.](http://arxiv.org/abs/2310.10845) | CoTFormer是一种transformer变体，通过使用隐含的链思考机制，实现了与更深模型相当的容量，并且在实证中显著优于更大的标准transformers。 |
| [^80] | [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks.](http://arxiv.org/abs/2310.10844) | 本论文调查了对大型语言模型进行恶意攻击的研究，发现即使经过安全调整的模型也容易受到攻击。这些攻击利用弱点并误导AI系统，对于复杂系统的攻击尤为明显。 |
| [^81] | [Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow.](http://arxiv.org/abs/2310.10843) | 本研究提出了一种使用密度估计进行概率分类的方法，通过使用高斯混合模型和蒙特卡洛自回归流对数据的似然进行建模，并展示了这种方法优于传统的分类器。这项工作为基于联合密度估计的其他概率分类器的提出开辟了新的研究方向。 |
| [^82] | [A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data.](http://arxiv.org/abs/2310.10841) | 这项工作提出了一种新的基于机器学习的算法，用于在传感器数据的时间序列中自动检测基于频率的事件。这种方法通过将时间序列数据映射到时间-频率域中的表示，并使用目标检测模型来提高事件的识别准确性。 |
| [^83] | [Approximating Two-Layer Feedforward Networks for Efficient Transformers.](http://arxiv.org/abs/2310.10837) | 本论文介绍了一种用于高效Transformer的近似两层前馈网络方法，通过稀疏的专家混合模型和产品-键存储实现资源高效的大型语言模型，与其他方法相比具有竞争力，并在参数相等的条件下展示了其在不同规模数据集上的优势。 |
| [^84] | [Gaussian processes based data augmentation and expected signature for time series classification.](http://arxiv.org/abs/2310.10836) | 该论文提出了一种基于高斯过程的数据增强和期望签名的时间序列特征提取模型，并通过有监督的任务学习到了最佳的特征提取方法。 |
| [^85] | [Provable Probabilistic Imaging using Score-Based Generative Priors.](http://arxiv.org/abs/2310.10835) | 本文提出了一种基于得分的生成先验的插入式蒙特卡洛算法，能够实现高质量图像重建和不确定性量化。 |
| [^86] | [Proper Laplacian Representation Learning.](http://arxiv.org/abs/2310.10833) | 本论文介绍了一种理论上可靠的方法和优化算法，用于近似Laplacian表示学习，以解决大规模强化学习中的探索、泛化和传递问题。 |
| [^87] | [Accurate Data-Driven Surrogates of Dynamical Systems for Forward Propagation of Uncertainty.](http://arxiv.org/abs/2310.10831) | 本文介绍了一种准确的基于数据驱动的动力系统代理模型方法，通过将随机选取近似应用于模型的动力学来构建代理模型。这种方法能够减小近似系统轨迹和状态变量误差。 |
| [^88] | [Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning.](http://arxiv.org/abs/2310.10818) | 该论文提出了一种利用混合模型基于后继特征强化学习方法，能够在具有不同转移动力学和奖励函数的任务之间实现样本高效的不确定性感知知识传递。 |
| [^89] | [Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms.](http://arxiv.org/abs/2310.10810) | 本文提出了一种新的鲁棒多智能体强化学习框架ERNIE，通过对抗性规范化促进策略的Lipschitz连续性，以提高鲁棒性和对抗噪声观察、转换动态的变化和智能体的恶意行为。 |
| [^90] | [Regularization properties of adversarially-trained linear regression.](http://arxiv.org/abs/2310.10807) | 本研究对对抗训练线性回归的正则化性质进行了研究，发现在过参数化情况下，对抗训练可以得到最小范数插值解，这一发现对理解对抗训练的效果和应用具有重要意义。 |
| [^91] | [Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification.](http://arxiv.org/abs/2310.10806) | 通过卷积神经网络模型识别糖尿病视网膜病变，提供准确的诊断结果，模型更易解释和抗过拟合，准确率达到71%。推动了糖尿病视网膜病变检测领域的发展。 |
| [^92] | [Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs.](http://arxiv.org/abs/2310.10791) | 本文研究了神经切向核函数（NTKs）在图神经网络（GNNs）中的应用。我们发现优化对齐等价于优化GNN中的图表示或图移位运算符，并建立了对于两层GNN对齐的最优性的理论保证。 |
| [^93] | [Demystifying Poisoning Backdoor Attacks from a Statistical Perspective.](http://arxiv.org/abs/2310.10780) | 从统计学角度揭开中毒后门攻击的神秘面纱，通过评估任何包含恒定触发器的后门攻击的有效性，确定了后门攻击成功的决定因素、最有效的攻击方向以及几乎不可察觉的人类触发器何时会成功。 |
| [^94] | [Is there a Trojan! : Literature survey and critical evaluation of the latest ML based modern intrusion detection systems in IoT environments.](http://arxiv.org/abs/2310.10778) | 本文调查和评估了基于机器学习的现代物联网环境下的入侵检测系统，并发现尽管已存在高准确性的机器学习算法，但缺乏生产级模型。 |
| [^95] | [Correcting model misspecification in physics-informed neural networks (PINNs).](http://arxiv.org/abs/2310.10776) | 本论文提出了一种纠正物理信息神经网络（PINNs）中模型错误规范的通用方法，用于发现控制方程，并通过稀疏和/或噪声数据实现了校正。 |
| [^96] | [Gotta be SAFE: A New Framework for Molecular Design.](http://arxiv.org/abs/2310.10773) | SAFE is a novel line notation for chemical structures that reimagines SMILES strings as an unordered sequence of interconnected fragment blocks, streamlining complex generative tasks and facilitating fragment-constrained design without the need for intricate decoding or graph-based models. It has been demonstrated to be effective through extensive experimentation. |
| [^97] | [Unsupervised Lead Sheet Generation via Semantic Compression.](http://arxiv.org/abs/2310.10772) | 该论文提出了无监督的主旋律生成方法，通过语义压缩将完整分谱转换为主旋律谱单。引入了一种新的模型Lead-AE来实现这一目标。 |
| [^98] | [Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models.](http://arxiv.org/abs/2310.10767) | 本文研究了神经网络中广义神经网络和高斯过程的对应关系，发现具有无限深度层并且宽度趋近于无穷大的神经网络收敛于高斯过程，揭示了广义神经网络的良性过拟合现象。 |
| [^99] | [Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches.](http://arxiv.org/abs/2310.10762) | 本研究通过比较人工神经网络和多元回归方法，探索了适用于人脑组织的最佳本构材料模型。结果表明，人工神经网络能够自动识别准确的本构模型。 |
| [^100] | [Statistical Barriers to Affine-equivariant Estimation.](http://arxiv.org/abs/2310.10758) | 本研究调查了对于鲁棒均值估计的仿射等变性估计器的数量化性能，并发现仿射等变性会导致恢复误差严重恶化，速率降低一个因子$\sqrt{d}$。传统估计器不是最优的或缺乏量化保证，而具有量化保证的最新估计器不具有仿射等变性或需要额外的分布条件。 |
| [^101] | [Deep Conditional Shape Models for 3D cardiac image segmentation.](http://arxiv.org/abs/2310.10756) | 本论文提出了一种使用深层条件形状模型的算法，可以用于3D心脏图像分割。通过学习与模态无关的形状模型和模态相关的细化网络，该算法可以高效地获得心脏左心室的分割结果。 |
| [^102] | [Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder.](http://arxiv.org/abs/2310.10745) | 本研究提出了一种名为Mori-Zwanzig自编码器（MZ-AE）的新方法，用于在低维空间中稳健地逼近Koopman算子，通过非线性自编码器和Mori-Zwanzig形式主义的集成实现对有限不变Koopman子空间的逼近，从而增强了精确性和准确预测复杂系统行为的能力。 |
| [^103] | [Fast Adversarial Label-Flipping Attack on Tabular Data.](http://arxiv.org/abs/2310.10744) | 本文提出了基于表格数据的快速对抗性标签翻转攻击，并强调了这种攻击对机器学习模型的潜在风险和误导。为了证明这种风险，提出了一种新颖高效的攻击方法FALFA。 |
| [^104] | [MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design.](http://arxiv.org/abs/2310.10732) | MOFDiff是一种粗粒度扩散模型，通过使用等变图神经网络生成粗粒度的MOF结构，并能有效生成有效和新颖的MOF结构。 |
| [^105] | [A representation learning approach to probe for dynamical dark energy in matter power spectra.](http://arxiv.org/abs/2310.10717) | 这项研究提出了一种使用变分自编码器（VAE）架构进行表示学习的方法，用于在观测研究中探测宇宙大尺度结构中的动态暗能量模型。通过只使用一个潜在参数，可以预测到95%（99%）的DE功率谱，在考虑宇宙方差的高斯误差范围内具有很好的准确性。 |
| [^106] | [Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations.](http://arxiv.org/abs/2310.10705) | 本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。 |
| [^107] | [Transparent Anomaly Detection via Concept-based Explanations.](http://arxiv.org/abs/2310.10702) | 本论文提出了一种透明的基于概念解释的异常检测方法（ACE），能够提供人类可解释的解释和异常预测。该方法在推进异常检测的透明度的同时，实现了有效的人机交互，并且在性能上要么更高，要么与黑盒不可解释模型相当。 |
| [^108] | [Reusing Pretrained Models by Multi-linear Operators for Efficient Training.](http://arxiv.org/abs/2310.10699) | 本文提出了一种通过多线性操作器重用预训练模型以实现有效训练的方法，解决了从头开始训练大型模型所需资源大的问题，并通过线性相关来增强加速能力。 |
| [^109] | [Robust Collaborative Filtering to Popularity Distribution Shift.](http://arxiv.org/abs/2310.10696) | 该论文提出了一种鲁棒的协同过滤方法，解决了训练数据中流行度偏差导致的泛化性能问题。通过评估和减少快捷方式程度，以及不需事先了解测试分布，提高了去偏见表示的质量和OOD泛化性能。 |
| [^110] | [Data-Driven Score-Based Models for Generating Stable Structures with Adaptive Crystal Cells.](http://arxiv.org/abs/2310.10695) | 本研究提出了一种基于数据驱动的评分模型来生成具有自适应晶胞的稳定结构。该方法通过学习可用数据中的晶格，使用两个去噪过程并行生成晶格，以实现对具有所需属性的新晶体结构的生成。 |
| [^111] | [Network Analysis of the iNaturalist Citizen Science Community.](http://arxiv.org/abs/2310.10693) | 本研究以iNaturalist公民科学平台为案例，通过网络分析的方法，探讨了公民科学项目的结构与用户之间的交互，提出了一个新颖的网络科学研究基准，并通过链接预测任务获得了新的认识。 |
| [^112] | [ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors.](http://arxiv.org/abs/2310.10692) | ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。 |
| [^113] | [Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation.](http://arxiv.org/abs/2310.10691) | 本研究通过使用扩散模型生成合成数据来提高机器学习模型在数字VLSI电路中的准确性。通过验证生成数据的质量和进行数据增强，我们证明了这种方法在预测分析中的有效性。 |
| [^114] | [A decoder-only foundation model for time-series forecasting.](http://arxiv.org/abs/2310.10688) | 本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。 |
| [^115] | [PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization.](http://arxiv.org/abs/2310.10685) | 该论文介绍了一种名为PS-AAS的方法，用于自动算法选择，具体应用于黑盒优化中。通过创建算法行为元表示，构建算法之间的图形，并利用图形算法选择多样化、代表性和非冗余的投资组合，从而提高了算法选择的性能和灵活性。 |
| [^116] | [Large Language Model Unlearning.](http://arxiv.org/abs/2310.10683) | 大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。 |
| [^117] | [Hybrid Quantum-Classical Machine Learning for Sentiment Analysis.](http://arxiv.org/abs/2310.10672) | 本文提出了一种使用混合量子-经典机器学习算法进行情感分析的方法，通过研究量子核方法和基于变分量子电路的分类器，并结合经典的降维技术，实现了在处理大规模数据集中表达的人类情感和观点的情感分析，并取得了相对于传统方法更好的性能。 |
| [^118] | [Smart OMVI: Obfuscated Malware Variant Identification using a novel dataset.](http://arxiv.org/abs/2310.10670) | 智能OMVI使用新的数据集OMD，通过识别混淆恶意软件变种来应对恶意软件的威胁。 |
| [^119] | [Enhancing Network Resilience through Machine Learning-powered Graph Combinatorial Optimization: Applications in Cyber Defense and Information Diffusion.](http://arxiv.org/abs/2310.10667) | 本论文致力于通过机器学习驱动的图组合优化方法来增强网络韧性，在网络安全和信息传播等领域具有较高的适用性。 |
| [^120] | [Extracting Physical Causality from Measurements to Detect and Localize False Data Injection Attacks.](http://arxiv.org/abs/2310.10666) | 本文提出了一种基于因果推断的联合FDIA检测和定位框架，以提取物理因果关系来检测和定位虚假数据插入攻击。 |
| [^121] | [Privacy Preservation in Artificial Intelligence and Extended Reality (AI-XR) Metaverses: A Survey.](http://arxiv.org/abs/2310.10665) | 这项调研讨论了在人工智能与扩展现实元宇宙中的隐私保护问题。作为一个新兴概念，元宇宙的隐私问题令人担忧，特别是在沉浸式虚拟体验变得越来越普及的情况下。元宇宙将利用多种技术进行发展，并收集用户数据来提供个性化和沉浸式的服务，但这也引发了隐私问题的关注。 |
| [^122] | [Nebula: Self-Attention for Dynamic Malware Analysis.](http://arxiv.org/abs/2310.10664) | Nebula是一个自注意力网络，用于动态分析恶意软件。它能够概括不同的行为表示和格式，并结合动态日志报告中的异构信息。实验证明Nebula在三个重要任务上表现出色。 |
| [^123] | [TII-SSRC-23 Dataset: Typological Exploration of Diverse Traffic Patterns for Intrusion Detection.](http://arxiv.org/abs/2310.10661) | TII-SSRC-23数据集是一个新颖而综合的数据集，旨在克服现有数据集在入侵检测中的不足，并提供了关于入侵检测任务关键特征的重要见解。 |
| [^124] | [Analysis and Detection against Network Attacks in the Overlapping Phenomenon of Behavior Attribute.](http://arxiv.org/abs/2310.10660) | 本研究发现在网络攻击中存在行为属性重叠的现象，并提出了基于深度学习的多标签检测模型 MLG-Model 来解决这个问题。 |
| [^125] | [Backdoor Attack through Machine Unlearning.](http://arxiv.org/abs/2310.10659) | 本文提出了一种基于机器遗忘的新型黑盒后门攻击方法，通过激活隐藏后门来输出恶意预测，从而针对深度学习模型的脆弱性进行攻击。 |
| [^126] | [VeriDIP: Verifying Ownership of Deep Neural Networks through Privacy Leakage Fingerprints.](http://arxiv.org/abs/2310.10656) | VeriDIP是一种新颖的所有权测试方法，通过利用隐私泄露指纹和使用较少私有样本的方法来验证深度神经网络模型的知识产权。 |
| [^127] | [Enhancing Trustworthiness in ML-Based Network Intrusion Detection with Uncertainty Quantification.](http://arxiv.org/abs/2310.10655) | 这项研究提出了一种通过不确定性量化来增强基于机器学习的网络入侵检测的可信度的方法。 |
| [^128] | [A Computational Framework for Solving Wasserstein Lagrangian Flows.](http://arxiv.org/abs/2310.10649) | 本研究提出了一个基于深度学习的计算框架，通过拉格朗日对偶形式处理不同的最优输运问题，不需要模拟轨迹或访问最优耦合，具有较高的性能。 |
| [^129] | [TacticAI: an AI assistant for football tactics.](http://arxiv.org/abs/2310.10553) | 提出了TacticAI，一种与利物浦足球俱乐部的领域专家密切合作开发和评价的AI足球战术助手。TacticAI能够通过预测和生成的方式帮助教练们分析角球情况，并为每个角球惯例选择成功可能性最高的球员配置。 |
| [^130] | [ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models.](http://arxiv.org/abs/2310.10505) | ReMax是一种用于对齐大型语言模型的简单、有效和高效的强化学习方法，相比于PPO，ReMax简化了实现，减少了内存使用，并解决了fine-tuning时的内存溢出问题。 |
| [^131] | [Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance.](http://arxiv.org/abs/2310.10021) | BOSS是一种通过最小监督和大型语言模型指导来自动学习解决新任务的方法，在初始技能集之外的任务中，代理不接收奖励反馈。通过该方法，BOSS能够从基本的原始技能中构建出各种复杂有用的行为。 |
| [^132] | [On Statistical Learning of Branch and Bound for Vehicle Routing Optimization.](http://arxiv.org/abs/2310.09986) | 本文研究了车辆路径优化中的分支界定算法的统计学习，并比较了三个神经网络模型在容量限制车辆路径问题中的表现。实验证明，这种方法在性能上能够与分支界定算法相匹配或有所改进，并且需要更少的计算资源。 |
| [^133] | [Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction.](http://arxiv.org/abs/2310.09903) | 本研究评估了特征选择方法在股市价格预测中的性能，通过选择最佳的技术指标组合来实现最少误差的预测。研究结果表明，不同的包装器特征选择方法在不同的机器学习方法中具有不同的表现。 |
| [^134] | [Mirage: Model-Agnostic Graph Distillation for Graph Classification.](http://arxiv.org/abs/2310.09486) | Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。 |
| [^135] | [Tokenizer Choice For LLM Training: Negligible or Crucial?.](http://arxiv.org/abs/2310.08754) | 在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。 |
| [^136] | [Splicing Up Your Predictions with RNA Contrastive Learning.](http://arxiv.org/abs/2310.08738) | 本研究将对比学习技术扩展到基因组数据，利用选择性剪接和基因复制产生的序列之间的功能相似性，学习到广义RNA同位素表示。我们的预训练策略在RNA半衰期和平均核糖体负载预测等任务上取得了竞争性的结果，在低数据条件下皮尔逊相关性增加了多达两倍。 |
| [^137] | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models.](http://arxiv.org/abs/2310.08659) | 本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。 |
| [^138] | [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains.](http://arxiv.org/abs/2310.07838) | 本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。 |
| [^139] | [BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations.](http://arxiv.org/abs/2310.07276) | BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。 |
| [^140] | [NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations.](http://arxiv.org/abs/2310.07184) | NeuroInspect是一个基于神经元的可解释的调试框架，通过确定网络中导致错误的神经元并可视化嵌入其中的特征，提供了人类可解释的解释。引入了CLIP-Illusion来生成特征图像，并以类为条件来考察神经元与决策层之间的联系。 |
| [^141] | [FABind: Fast and Accurate Protein-Ligand Binding.](http://arxiv.org/abs/2310.06763) | FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。 |
| [^142] | [Offline Imitation Learning with Variational Counterfactual Reasoning.](http://arxiv.org/abs/2310.04706) | 该论文提出了一个名为OILCA的框架，利用可识别的变分自动编码器生成"对抗性"样本，以解决离线模仿学习中数据稀缺、环境变化等问题。 |
| [^143] | [Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization.](http://arxiv.org/abs/2310.03708) | 本文提出了一种无强化学习的算法，称为多目标直接偏好优化（MODPO），它可以根据不同的偏好训练不同的语言模型，通过组合所有目标和特定权重来优化模型。 |
| [^144] | [zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning.](http://arxiv.org/abs/2310.02554) | zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。 |
| [^145] | [Decoding Imagery: Unleashing Large Language Models.](http://arxiv.org/abs/2309.16705) | 该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。 |
| [^146] | [Small-scale proxies for large-scale Transformer training instabilities.](http://arxiv.org/abs/2309.14322) | 本文研究了大规模Transformer训练中的不稳定性问题，并找到了对应的小规模代理模型来复现和研究这些问题。研究人员发现训练不稳定性的两个源头，并表明先前使用的缓解方法在小规模训练中同样有效。这个发现有助于将缓解方法推广到大规模训练中。 |
| [^147] | [Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech.](http://arxiv.org/abs/2309.14107) | 本研究使用基于wav2vec的模型，实现了言语失语症的自动检测和严重程度级别分类任务，并在准确率上取得了显著的提升。 |
| [^148] | [Analysis and Detection of Pathological Voice using Glottal Source Features.](http://arxiv.org/abs/2309.14080) | 本研究提供了对声门源特征的系统分析，并研究了它们在声音病理检测中的有效性。实验结果表明声门源包含的信息对于病理性声音的判别具有重要作用。 |
| [^149] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^150] | [Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems.](http://arxiv.org/abs/2309.12128) | 本研究通过探索连接理论和实践，提供了无监督神经网络在解决逆问题中的收敛和恢复性能保证。同时，我们还得出了对于两层具有平滑激活函数的深度逆先验网络的超参数化界限，该网络将从我们的保证中受益。 |
| [^151] | [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions.](http://arxiv.org/abs/2309.10150) | Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。 |
| [^152] | [Replacing softmax with ReLU in Vision Transformers.](http://arxiv.org/abs/2309.08586) | 在视觉变换器中，用ReLU替换softmax的注意力机制可以在计算性能上接近或匹配softmax注意力，并且通过序列长度进行除法可以缓解精度下降的问题。 |
| [^153] | [Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning.](http://arxiv.org/abs/2309.01289) | 本研究提出了一种名为联邦正交训练（FOT）的方法，用于解决连续联邦学习中的全局灾难性遗忘问题，该方法克服了现有方法对过去数据的不切实际假设和隐私原则的违反。 |
| [^154] | [Domain-adaptive Message Passing Graph Neural Network.](http://arxiv.org/abs/2308.16470) | 本论文提出了一个基于领域自适应的消息传递图神经网络(DM-GNN)，用于解决跨网络节点分类问题。该方法通过结合图神经网络和条件对抗领域自适应，能够学习可传递的节点分类信息。具体而言，通过双特征提取器构建GNN编码器，同时利用标签传播节点分类器和标签感知的传播方案来提高节点分类的准确性和泛化性能。 |
| [^155] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^156] | [Applications of machine Learning to improve the efficiency and range of microbial biosynthesis: a review of state-of-art techniques.](http://arxiv.org/abs/2308.13877) | 本文综述了机器学习在微生物生物合成中的应用，并提供了对两个关键领域的全面解释和应用的现状及问题。 |
| [^157] | [Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models.](http://arxiv.org/abs/2308.11890) | 本文提出了一个基于形状的分子生成问题，通过等变形状引导的生成模型ShapeMol成功生成了新颖、多样且类似给定形状条件的药物样分子。 |
| [^158] | [Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries.](http://arxiv.org/abs/2308.10875) | 这篇论文介绍了受自然启发的元启发式算法在人工智能中的重要性和应用，并提出了一种新算法CSO-MA，通过多个优化问题的应用展示了其灵活性和优越性能。 |
| [^159] | [An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training.](http://arxiv.org/abs/2307.16189) | 这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。 |
| [^160] | [When No-Rejection Learning is Optimal for Regression with Rejection.](http://arxiv.org/abs/2307.02932) | 本文研究了具有拒绝的回归问题，并调查了将其视为标准回归任务来学习预测器的无拒绝学习策略。 |
| [^161] | [Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities.](http://arxiv.org/abs/2307.01998) | 零代价神经架构搜索是一种不需要训练的方法，其核心思想是设计能够预测网络精确度的代理。本文综述了最新的零代价神经架构搜索方法，并在硬件感知和硬件无感知的NAS场景中展示了其有效性。 |
| [^162] | [Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis.](http://arxiv.org/abs/2306.17181) | 本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。 |
| [^163] | [Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits.](http://arxiv.org/abs/2306.14872) | 本文提出了一种新的数据驱动技术，跟踪不确定度椭球体的几何形状，为线性赌博机算法建立实例相关的频率后悔界，并实现了平衡算法性能与理论保证的效果。 |
| [^164] | [Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching.](http://arxiv.org/abs/2306.14079) | 本文提出了平滑的距离数据度量标准，并将其与离线强化学习相结合，以对抗不确定性和分布偏移的挑战。该方法不仅在最小化梯度不确定性时稳定收敛到数据，而且不易低估真实不确定性，是一种有前途的策略搜索方法。 |
| [^165] | [ALP: Action-Aware Embodied Learning for Perception.](http://arxiv.org/abs/2306.10190) | ALP是一个动作感知的具身学习框架，通过将动作信息融入表示学习，可以学习可普遍应用的任务无关的视觉表示，并在复杂的三维环境中积极探索和收集训练数据。 |
| [^166] | [Optimizer's Information Criterion: Dissecting and Correcting Bias in Data-Driven Optimization.](http://arxiv.org/abs/2306.10081) | 这项研究提出了一个称为优化器的信息准则(OIC)的通用偏差校正方法，帮助解决数据驱动优化中的乐观偏差问题。该方法直接近似一阶偏差，并且不需要解决额外的优化问题，是在决策选择方面的一个创新。 |
| [^167] | [Autonomous Drifting with 3 Minutes of Data via Learned Tire Models.](http://arxiv.org/abs/2306.06330) | 提出了一种基于神经常微分方程和神经-ExpTanh参数化的新型轮胎力模型，并将其作为现有非线性模型预测控制框架中解析刷式轮胎模型的替代品,成功实现了仅利用少于三分钟的驾驶数据，实现高性能的自主漂移 |
| [^168] | [CARSO: Counter-Adversarial Recall of Synthetic Observations.](http://arxiv.org/abs/2306.06081) | 本文提出了一种新的图像分类的对抗性防御机制CARSO，该方法可以比最先进的对抗性训练更好地保护分类器，通过利用生成模型进行对抗净化来进行最终分类，并成功地保护自己免受未预见的威胁和最终攻击。 |
| [^169] | [In-Sample Policy Iteration for Offline Reinforcement Learning.](http://arxiv.org/abs/2306.05726) | 本文提出了一种采用样本内策略迭代的算法来增强离线强化学习中的行为规则方法，在实验中取得了显著的改进。 |
| [^170] | [Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry.](http://arxiv.org/abs/2306.04810) | 本文提出了一种无需权重对称的有监督深度神经网络的生物合理方法，该方法利用相关信息最大化在层激活之间描述生物神经网络中的信号传播。通过坐标下降优化相应的目标和均方误差损失函数，可以产生一个更生物真实的神经网络结构。 |
| [^171] | [Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning.](http://arxiv.org/abs/2306.04265) | 本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。 |
| [^172] | [Rotating Features for Object Discovery.](http://arxiv.org/abs/2306.00600) | 本文提出了旋转特征作为将复杂值特征推广到高纬度的方法，并提出了一种新的评估过程来提取分布式表示中的物体。这些进展使得我们能够在真实世界的数据中扩展分布式以物体为中心的表示。 |
| [^173] | [Large Language Models Are Not Abstract Reasoners.](http://arxiv.org/abs/2305.19555) | 本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。 |
| [^174] | [Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks.](http://arxiv.org/abs/2305.19130) | 采用空间转换网络模块，可使舌部超声图像为基础的无声语音接口模型快速适应到不同的用户和会话，且能显著降低均方误差。 |
| [^175] | [Imitating Task and Motion Planning with Visuomotor Transformers.](http://arxiv.org/abs/2305.16309) | 本研究提出了一种名为OPTIMUS的新型模仿学习系统，通过模仿TAMP代理来训练大规模的视觉动作转换器策略。OPTIMUS引入了一个专门为模仿学习而设计的TAMP数据生成管道，可以用来训练性能优越的基于转换器的策略。 |
| [^176] | [How many samples are needed to leverage smoothness?.](http://arxiv.org/abs/2305.16014) | 本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。 |
| [^177] | [Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training.](http://arxiv.org/abs/2305.14342) | Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。 |
| [^178] | [Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design.](http://arxiv.org/abs/2305.13035) | 本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。 |
| [^179] | [Enhanced Meta Label Correction for Coping with Label Corruption.](http://arxiv.org/abs/2305.12961) | 提出了一种用于带有噪声标签的学习问题的增强元标签校正方法（EMLC），通过重新审视元学习过程和引入更准确的元梯度推导，以及使用新颖的教师架构和训练目标，实现了在标准基准测试中取得的最先进结果。 |
| [^180] | [LLM Itself Can Read and Generate CXR Images.](http://arxiv.org/abs/2305.11490) | 该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。 |
| [^181] | [Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance.](http://arxiv.org/abs/2305.10664) | 本文提出了一种新的方法进行关于具有无界方差权重的贝叶斯神经网络的后验推断，并表明后验分布集中在具有非标准超参数依赖性的稀疏促进和均值收缩先验周围。 |
| [^182] | [Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models.](http://arxiv.org/abs/2305.10120) | 针对大规模文本到图像模型可能被误用生成有害内容的问题，该论文提出了一种选择性遗忘方法，即持续学习方法，可在深度生成模型中实现可控的遗忘，用户可指定消除哪些概念。 |
| [^183] | [Understanding Model Averaging in Federated Learning on Heterogeneous Data.](http://arxiv.org/abs/2305.07845) | 本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。 |
| [^184] | [Optimizing Memory Mapping Using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.07440) | 本文提出了一种使用强化学习解决机器学习程序中内存映射问题的方法。 |
| [^185] | [HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting.](http://arxiv.org/abs/2305.07089) | HINT是一种用于概率预测的新型模型族，能够有效、准确地进行一致性预测，通过引入Bootstrap方法并为网络加入规范化特征提取和输出规范化来保证其性能，在多个数据集上的预测精度比现有技术更高。 |
| [^186] | [Neural Lyapunov Control for Discrete-Time Systems.](http://arxiv.org/abs/2305.06547) | 该论文介绍了一种用于离散时间系统的神经李雅普诺夫控制方法，该方法利用混合整数线性规划来验证稳定性条件，计算子水平集刻画吸引域，有效地学习控制策略。 |
| [^187] | [The emergence of clusters in self-attention dynamics.](http://arxiv.org/abs/2305.05465) | 本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。 |
| [^188] | [The Internal State of an LLM Knows When its Lying.](http://arxiv.org/abs/2304.13734) | 该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。 |
| [^189] | [Stable and low-precision training for large-scale vision-language models.](http://arxiv.org/abs/2304.13013) | 该研究介绍了用于大规模视觉语言模型稳定和低精度训练的新方法，包括SwitchBack和AdamW-Adafacto方法。这些方法提高了训练速度和稳定性。 |
| [^190] | [Editable User Profiles for Controllable Text Recommendation.](http://arxiv.org/abs/2304.04250) | 本文提出了一种新的概念值瓶颈模型LACE，用于可控文本推荐。该模型基于用户文档学习个性化的概念表示，并通过多种交互方式为用户提供了控制推荐的机制，验证了在离线和在线实验中该模型的推荐质量和有效性。 |
| [^191] | [Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling.](http://arxiv.org/abs/2304.02806) | 本文提出了一种新的图混合专家（GMoE）模型，旨在解决现实世界中的图具有多样的图结构和包含异构节点和边的问题。该模型可以增强GNN的泛化能力，适应多样的训练图结构的能力，并且不会增加计算开销。 |
| [^192] | [Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact Deformable Convolutional Transformers.](http://arxiv.org/abs/2303.13827) | 本文提出了一种紧凑型可形变卷积变换器（DC Transformer），能够高效地识别单一和混合型晶圆缺陷，聚焦于全局特征，准确预测缺陷的数目和类型。 |
| [^193] | [An Embarrassingly Simple Approach for Wafer Feature Extraction and Defect Pattern Recognition.](http://arxiv.org/abs/2303.11632) | 本文提出了一种非常简单但有效的从晶圆图像中提取特征的技术，其速度快，直观且可解释，在缺陷模式识别方面的表现优于传统的深度学习模型。 |
| [^194] | [Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration.](http://arxiv.org/abs/2303.11435) | InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。 |
| [^195] | [Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices.](http://arxiv.org/abs/2303.10019) | 本文提出一种新的多元概率CRPS学习方法，应用于日前电价预测中，相比于统一组合在CRPS方面取得了显著改进。 |
| [^196] | [It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness.](http://arxiv.org/abs/2303.09767) | 本文综述了有关数据对抗鲁棒性的研究，系统地总结了最新研究成果，并进一步讨论了未来研究方向和知识差距。 |
| [^197] | [Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning.](http://arxiv.org/abs/2303.09447) | 本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。 |
| [^198] | [A path in regression Random Forest looking for spatial dependence: a taxonomy and a systematic review.](http://arxiv.org/abs/2303.04693) | 在这项工作中，我们提出了一种分类法，根据前处理、中处理和/或后处理的时间点尝试将空间信息纳入回归随机森林中。此外，我们进行了系统回顾并分类最新采用的调整回归随机森林以适应空间相关数据的策略。 |
| [^199] | [Densely Connected $G$-invariant Deep Neural Networks with Signed Permutation Representations.](http://arxiv.org/abs/2303.04614) | 本文提出了一种具有带符号排列表示的密集连接$G$-不变深度神经网络($G$-DNN)架构，通过耦合权重，使得网络的前激活能够通过$G$的带符号排列表示进行变换，从而得到一族更丰富的$G$-不变架构。 |
| [^200] | [Deep Learning Enhanced Realized GARCH.](http://arxiv.org/abs/2302.08002) | 使用深度学习（LSTM）和实现波动率测量的新框架能够共同建模收益和实现波动率测量，在统计推断和预测方面具有出色的性能，并且能够适应波动率的定式事实。 |
| [^201] | [Towards Minimax Optimality of Model-based Robust Reinforcement Learning.](http://arxiv.org/abs/2302.05372) | 本文研究了在鲁棒强化学习中，对于仅具有对正常核心的生成模型访问权限时，获得ε-最优策略的样本复杂度。对于sa（s-）矩形不确定集合，已知最佳样本复杂度为ε^2/（H^4 * |S|^2 * |A|）（响应为ε^2/（H^4 * |S|^2 * |A|^2）），对于特定算法和基于总变差（TV）、KL或卡方散度的不确定集合。 |
| [^202] | [Removing Structured Noise with Diffusion Models.](http://arxiv.org/abs/2302.05290) | 本文提出了一种基于扩散模型的后验采样方法来去除包含结构性噪声的数据，相比于常规方法有较好的表现，对于医学成像等领域具有实际应用价值。 |
| [^203] | [UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models.](http://arxiv.org/abs/2302.04867) | 本文提出了一种称为UniPC的统一预测-修正框架，用于快速采样扩散模型(DPMs)，该框架通过引入统一修正器(UniC)和统一预测器(UniP)，可以显著提高采样质量，尤其是在较少步骤的情况下。 |
| [^204] | [Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions.](http://arxiv.org/abs/2302.03764) | 本论文提出了一种内存高效的自适应正则化方法，通过使用频繁方向草稿来降低矩阵预处理器的内存和计算需求。在深度学习任务中，该方法可以在保持性能的同时降低资源的使用。 |
| [^205] | [CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data.](http://arxiv.org/abs/2302.03246) | 本文提出了一种针对自相关和非平稳时间序列数据的约束性因果发现方法，可以识别出滞后和即时/同时的因果关系以及随时间变化的模块。 |
| [^206] | [MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields.](http://arxiv.org/abs/2302.02978) | 提出了一个多模态分类基准MuG，该基准包括八个来自不同类型游戏的数据集，涵盖了表格、文本和视觉模态。通过实验结果表明，该基准具有挑战性和多模态依赖性的特点。 |
| [^207] | [Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy.](http://arxiv.org/abs/2302.01068) | 本文通过利用合成样本实现全局优化，加入记录级差分隐私以保护隐私，验证了该方法的数据集有效性。 |
| [^208] | [Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning.](http://arxiv.org/abs/2301.11916) | 本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。 |
| [^209] | [A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics.](http://arxiv.org/abs/2301.04900) | 本文介绍了一种行为良好的图神经网络近似复杂动力学的方法，包括必要的偏置和适当的神经网络结构，并提出了评估泛化能力和推断时预测置信度的方法。 |
| [^210] | [SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2212.07489) | SMACv2是一个改进的合作多智能体强化学习基准，通过引入随机性和部分可观察性的限制，挑战算法在复杂场景中的泛化能力。 |
| [^211] | [Policy learning for many outcomes of interest: Combining optimal policy trees with multi-objective Bayesian optimisation.](http://arxiv.org/abs/2212.06312) | 该论文提出了一种多目标策略学习（MOPoL）方法，结合了最优决策树和多目标贝叶斯优化方法，可以平衡多个感兴趣的结果。 |
| [^212] | [On the Overlooked Structure of Stochastic Gradients.](http://arxiv.org/abs/2212.02083) | 本文对深度学习中随机梯度的结构进行了正式的统计检验，发现逐维梯度通常呈现幂律重尾，而逐次迭代的梯度和随机梯度噪声通常不呈现幂律重尾。 |
| [^213] | [Photo Rater: Photographs Auto-Selector with Deep Learning.](http://arxiv.org/abs/2211.14420) | Photo Rater是一个利用深度学习帮助摄影师选择最佳照片的计算机视觉项目。它通过三个神经网络对图像进行质量评估、模糊分类和审美评估，并根据得分对图像进行排序和呈现。 |
| [^214] | [Over-The-Air Clustered Wireless Federated Learning.](http://arxiv.org/abs/2211.03363) | 本论文提出了一种OTA半分散群集无线FL（CWFL）和CWFL-Prox算法，它在通信效率上优于分散FL策略，同时参数更新收敛到全局极小值。 |
| [^215] | [Predicting User-specific Future Activities using LSTM-based Multi-label Classification.](http://arxiv.org/abs/2211.03100) | 使用LSTM-based的多标签分类器，本研究提出了一个两阶段训练方法，通过预处理和微调提高了用户特定未来活动预测的性能。 |
| [^216] | [ImageCAS: A Large-Scale Dataset and Benchmark for Coronary Artery Segmentation based on Computed Tomography Angiography Images.](http://arxiv.org/abs/2211.01607) | 论文提出了一个基于计算机断层扫描血管造影图像的冠状动脉分割的大规模数据集和基准，解决了现有工作中数据集不足和方法比较困难的问题。 |
| [^217] | [CGAN-ECT: Tomography Image Reconstruction from Electrical Capacitance Measurements Using CGANs.](http://arxiv.org/abs/2209.03737) | 这项研究提出了一种使用条件生成对抗网络（CGAN）从电容测量中重建电容层析成像的方法，并创建了一个包含320K个合成图像测量对的新数据集进行训练和评估。 |
| [^218] | [The BUTTER Zone: An Empirical Study of Training Dynamics in Fully Connected Neural Networks.](http://arxiv.org/abs/2207.12547) | 本研究提供了一个实证数据集，调查了全连接神经网络的训练动态，并观察到稳定模式跨任务和拓扑结构持续存在。该研究旨在激发机器学习技术的科学研究，为推进该领域的理论发现提供必要的动力。 |
| [^219] | [Derivative-Informed Neural Operator: An Efficient Framework for High-Dimensional Parametric Derivative Learning.](http://arxiv.org/abs/2206.10745) | 导数信息驱动的神经算子（DINOs）是一个高效的框架，用于近似高维参数导数学习。通过压缩导数信息和高效应用于神经算子训练，DINOs可以提高算子和导数的准确性，应用于众多领域。 |
| [^220] | [SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction.](http://arxiv.org/abs/2206.09818) | SSM-DTA框架通过多任务训练和半监督训练方法，打破了药物靶点亲和性预测中数据稀缺的障碍。 |
| [^221] | [PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series.](http://arxiv.org/abs/2206.07940) | PROFHIT是一个概率鲁棒的分层时间序列预测模型，能够提供整个层次结构的预测分布，并引入一种新颖的分布一致性正则化方法。 |
| [^222] | [Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs.](http://arxiv.org/abs/2206.02346) | 本文研究了约束马尔可夫决策过程中优化问题的自然策略梯度原始-对偶方法。通过自然策略梯度上升和投影次梯度下降更新变量，我们的方法在全局收敛中实现了次线性速率，而且不受状态-动作空间大小限制。 |
| [^223] | [Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems.](http://arxiv.org/abs/2204.01815) | 本文介绍了一种新的一致性方法来解决矩阵和张量补全问题，在推荐系统应用中，我们证明了通过保留单位比例和一致性两个约束条件可以实现解的存在性与唯一性。 |
| [^224] | [A Theoretical Analysis on Independence-driven Importance Weighting for Covariate-shift Generalization.](http://arxiv.org/abs/2111.02355) | 本文通过理论分析，将独立性驱动的重要性加权算法解释为特征选择过程，并证明了在协变量偏移泛化中的有效性。 |
| [^225] | [Private Multi-Task Learning: Formulation and Applications to Federated Learning.](http://arxiv.org/abs/2108.12978) | 这项研究提出了一种用于在隐私敏感应用中进行多任务学习的算法，通过联合差分隐私保证了个性化联邦学习的隐私和效用，并在实验中表现出更好的性能。 |
| [^226] | [Prioritized training on points that are learnable, worth learning, and not yet learned (workshop version).](http://arxiv.org/abs/2107.02565) | 本文介绍了一种名为“黄金选择”的技术，它通过选择恰当的训练点来加快模型训练。与优化文献中通常选择的困难点和课程学习中通常优先选择的简单点不同，黄金选择选择的点既有较高的信息量又能表现良好，且可迁移到其他架构中。 |
| [^227] | [Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles.](http://arxiv.org/abs/2106.06483) | 本论文研究了在随机上下文推断设置中，针对累计遗憾最小化的最优模型选择问题。通过引入渐增类别复杂性和递减边际收益条件，我们提出了一种基于新颖误配测试的算法，并展示了模型选择在奖励估计中的优势。 |
| [^228] | [From Undecidability of Non-Triviality and Finiteness to Undecidability of Learnability.](http://arxiv.org/abs/2106.01382) | 本文证明了在PAC二分类、统一和通用的在线学习以及准确学习中，学习性都是不可判定的，即无法通过一般过程来确定新模型是否能够成功学习。我们通过将形式系统的一致性问题和图灵机的停机问题编码为函数类的平凡性/有限性与是否可学习相关联来证明这一结论。 |
| [^229] | [BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter.](http://arxiv.org/abs/2105.01331) | 本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。 |
| [^230] | [Gaussian Mixture Reduction with Composite Transportation Divergence.](http://arxiv.org/abs/2002.08410) | 本文提出了一种基于复合传输散度的高斯混合简化方法，用于解决高斯混合在递归更新中阶数指数增加的推断问题。 |
| [^231] | [Improving Native Ads CTR Prediction by Large Scale Event Embedding and Recurrent Networks.](http://arxiv.org/abs/1804.09133) | 本文通过大规模事件嵌入和循环网络，提出了一种改进的CTR预测方法，在原生广告中取得了显著优势。 |

# 详细

[^1]: 在高斯混合模型空间中引入了类似于Gromov-Wassertein的距离

    Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space. (arXiv:2310.11256v1 [stat.ML])

    [http://arxiv.org/abs/2310.11256](http://arxiv.org/abs/2310.11256)

    本文介绍了两种在高斯混合模型空间中的Gromov-Wasserstein类型距离，分别用于评估分布之间的距离和推导最优的点分配方案。

    

    本文介绍了两种在高斯混合模型集合上的Gromov-Wasserstein类型距离。第一种距离是在高斯测度空间上两个离散分布的Gromov-Wasserstein距离。该距离可以作为Gromov-Wasserstein的替代，用于评估分布之间的距离，但不能直接推导出最优的运输方案。为了设计出这样的运输方案，我们引入了另一种在不可比较的空间中的测度之间的距离，该距离与Gromov-Wasserstein密切相关。当将允许的运输耦合限制为高斯混合模型时，这定义了另一种高斯混合模型之间的距离，可以作为Gromov-Wasserstein的另一种替代，并允许推导出最优的点分配方案。

    In this paper, we introduce two Gromov-Wasserstein-type distances on the set of Gaussian mixture models. The first one takes the form of a Gromov-Wasserstein distance between two discrete distributionson the space of Gaussian measures. This distance can be used as an alternative to Gromov-Wasserstein for applications which only require to evaluate how far the distributions are from each other but does not allow to derive directly an optimal transportation plan between clouds of points. To design a way to define such a transportation plan, we introduce another distance between measures living in incomparable spaces that turns out to be closely related to Gromov-Wasserstein. When restricting the set of admissible transportation couplings to be themselves Gaussian mixture models in this latter, this defines another distance between Gaussian mixture models that can be used as another alternative to Gromov-Wasserstein and which allows to derive an optimal assignment between points. Finally,
    
[^2]: CrossCodeEval: 一个多元化和多语言的用于跨文件代码补全的基准测试

    CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion. (arXiv:2310.11248v1 [cs.LG])

    [http://arxiv.org/abs/2310.11248](http://arxiv.org/abs/2310.11248)

    CrossCodeEval是一个多元化和多语言的基准测试，用于跨文件代码补全，在真实的软件开发场景中，需要跨文件上下文理解才能准确完成代码。

    

    代码补全模型在近年来取得了显著进展，然而当前流行的评估数据集，如HumanEval和MBPP，主要集中在单个文件内的代码补全任务上。这种过于简化的设置无法准确地代表现实世界中的软件开发场景，其中存储库跨越多个文件，存在大量的跨文件依赖关系，需要访问和理解跨文件上下文才能正确完成代码。为了填补这一空白，我们提出了CrossCodeEval，一个多元化和多语言的代码补全基准测试，需要深入的跨文件上下文理解才能准确完成代码。CrossCodeEval基于四种流行的编程语言（Python，Java，TypeScript和C#）中的多样化的真实世界、开源、权限许可的存储库集合构建。为了创建严格要求跨文件上下文进行准确完成的示例，我们提出了一个简单而高效的静态方法。

    Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.  To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-
    
[^3]: 使用大型语言模型进行实体匹配

    Entity Matching using Large Language Models. (arXiv:2310.11244v1 [cs.CL])

    [http://arxiv.org/abs/2310.11244](http://arxiv.org/abs/2310.11244)

    这项研究探讨了使用大型语言模型（LLMs）作为实体匹配的替代方法，相较于预训练的语言模型（PLMs），LLMs对训练数据需求较少且更具鲁棒性。

    

    实体匹配是判断两个实体描述是否指的是同一个真实世界实体的任务。实体匹配是大多数数据集成流程中的核心步骤，也是许多电子商务应用的重要组成部分，这些应用需要将来自不同供应商的产品匹配起来。目前最先进的实体匹配方法通常依赖于预训练的语言模型（PLMs），如BERT或RoBERTa。然而，这些模型在实体匹配中存在两个主要缺点：（i）模型需要大量特定任务的训练数据；（ii）微调后的模型对于超出分布范围的实体不够健壮。本文研究了使用大型语言模型（LLMs）作为基于PLMs的匹配器的备选方案，相比之下，LLMs对领域特定训练数据需求较少且更具鲁棒性。我们的研究涵盖了托管的LLMs，如GPT3.5和GPT4，以及基于Llama2的开源LLMs，可以在本地运行。我们在零样本场景和…

    Entity Matching is the task of deciding whether two entity descriptions refer to the same real-world entity. Entity Matching is a central step in most data integration pipelines and an enabler for many e-commerce applications which require to match products offers from different vendors. State-of-the-art entity matching methods often rely on pre-trained language models (PLMs) such as BERT or RoBERTa. Two major drawbacks of these models for entity matching are that (i) the models require significant amounts of task-specific training data and (ii) the fine-tuned models are not robust concerning out-of-distribution entities. In this paper, we investigate using large language models (LLMs) for entity matching as a less domain-specific training data reliant and more robust alternative to PLM-based matchers. Our study covers hosted LLMs, such as GPT3.5 and GPT4, as well as open source LLMs based on Llama2 which can be run locally. We evaluate these models in a zero-shot scenario as well as a
    
[^4]: 学习更好的采样方法

    Learning to Sample Better. (arXiv:2310.11232v1 [cs.LG])

    [http://arxiv.org/abs/2310.11232](http://arxiv.org/abs/2310.11232)

    本课程介绍了基于动态输运测度的生成建模方法的最新进展，重点讲述了如何通过数据学习这些映射，并通过正反馈循环改进蒙特卡洛采样技术。

    

    这些讲义介绍了基于动态输运测度的生成建模方法的最新进展，通过这种方法，简单基础测度的样本被映射到感兴趣目标测度的样本。特别强调这些方法在蒙特卡洛采样技术中的应用，例如重要性采样和马尔可夫链蒙特卡洛。在这种情况下，讲义展示了如何通过MC采样生成的数据变分学习这些映射，并如何利用它们通过正反馈循环改进采样。

    These lecture notes provide an introduction to recent advances in generative modeling methods based on the dynamical transportation of measures, by means of which samples from a simple base measure are mapped to samples from a target measure of interest. Special emphasis is put on the applications of these methods to Monte-Carlo (MC) sampling techniques, such as importance sampling and Markov Chain Monte-Carlo (MCMC) schemes. In this context, it is shown how the maps can be learned variationally using data generated by MC sampling, and how they can in turn be used to improve such sampling in a positive feedback loop.
    
[^5]: Zipformer：一种更快速、更好的自动语音识别编码器

    Zipformer: A faster and better encoder for automatic speech recognition. (arXiv:2310.11230v1 [eess.AS])

    [http://arxiv.org/abs/2310.11230](http://arxiv.org/abs/2310.11230)

    Zipformer是一种更快速、更节省内存、性能更好的自动语音识别编码器，通过U-Net-like编码器结构、重新组织的块结构、改进的LayerNorm、新的激活函数和新的优化器等方式实现了优化。实验证明它在LibriSpeech、Aishell-1和Wenet等数据集上表现出更快的收敛和更好的性能。

    

    Conformer已成为自动语音识别（ASR）中最流行的编码器模型。它在变换器中加入了卷积模块以学习局部和全局依赖关系。本文介绍了一种更快速、更节省内存、性能更好的变换器——Zipformer。建模改变包括：1）类似U-Net的编码器结构，中间堆栈在较低的帧率下运行；2）重新组织的块结构，增加了更多的模块，其中我们重复使用注意力权重以提高效率；3）一种改进的LayerNorm形式，称为BiasNorm，允许我们保留一些长度信息；4）新的激活函数SwooshR和SwooshL的性能优于Swish。我们还提出了一种新的优化器，称为ScaledAdam，它通过当前张量的规模来缩放更新，以保持相对变化大致相同，并明确学习参数规模。与Adam相比，它实现了更快的收敛和更好的性能。在LibriSpeech、Aishell-1和Wenet上进行了大量实验。

    The Conformer has become the most popular encoder model for automatic speech recognition (ASR). It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer. Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4) new activation functions SwooshR and SwooshL work better than Swish. We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster convergence and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and Wenet
    
[^6]: 理解算法公平性中的公平性代理函数

    Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])

    [http://arxiv.org/abs/2310.11211](http://arxiv.org/abs/2310.11211)

    本文研究了算法公平性中的公平性代理函数，并发现了代理和公平性定义之间存在一个差距。这个差距决定了一个代理函数能否适当替代一个公平性定义。

    

    已观察到机器学习算法对某些人群产生偏见的预测。为了减轻这种偏见并实现可比的准确性，一种有希望的方法是引入涉及公平性定义的代理函数，并解决一个受限制的优化问题。然而，在以往的研究中，一个有趣的问题是这种公平性代理函数可能导致不公平的结果。在本研究中，为了深入理解这个问题，我们以广泛使用的公平性定义——人口统计平等——为例，从理论和实证上证明了公平性定义和公平性代理函数之间存在一个代理-公平性差距。这个"差距"直接决定了一个代理函数是否适合替代一个公平性定义。此外，关于这个"差距"的理论分析和实验结果激发了我们的兴趣，表明无限制的代理函数将受到决策边界远离的点的影响。

    It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
    
[^7]: 大型语言模型能否自我解释？LLM生成的自解释研究。

    Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations. (arXiv:2310.11207v1 [cs.CL])

    [http://arxiv.org/abs/2310.11207](http://arxiv.org/abs/2310.11207)

    本文研究了大型语言模型生成的自解释在情感分析和特征归因解释任务中的效果，并探讨了不同的引导方法。

    

    大型语言模型（LLMs）如ChatGPT在各种自然语言处理（NLP）任务中展现出优越的性能，包括情感分析、数学推理和摘要。此外，由于这些模型在人类对话中进行指导，以产生“有帮助”的回答，它们通常会在回答中提供解释，我们称之为自解释。例如，在分析电影评论的情感时，模型可以输出情感的积极性，并列出评论中带有情感的词语（如“fantastic”和“memorable”）作为解释。这些自动生成的自解释有多好？本文在情感分析和特征归因解释的任务中对此问题进行了研究，后者是可解释性文献中最常研究的设置（针对ChatGPT之前的模型）。具体来说，我们研究了不同的方法来引导模型生成自解释。

    Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce "helpful" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as "fantastic" and "memorable" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit 
    
[^8]: 具有非空泛化界限的联邦学习

    Federated Learning with Nonvacuous Generalisation Bounds. (arXiv:2310.11203v1 [cs.LG])

    [http://arxiv.org/abs/2310.11203](http://arxiv.org/abs/2310.11203)

    这项研究提出了一种新的策略来在联邦学习中训练随机预测器，通过保护每个节点的隐私并且具有数值上非空的泛化界限，可以在保持预测性能的同时实现数据共享和保护隐私。

    

    我们引入了一种新的策略来训练联邦学习中的随机预测器，在这种策略中，网络的每个节点通过发布本地预测器但对其他节点保密其训练数据集的方式来保护其隐私。然后，我们构建一个全局的随机预测器，它在PAC-Bayesian泛化界限的意义上继承了本地私有预测器的属性。我们考虑了同步情况，即所有节点共享相同的训练目标（从泛化界限导出），以及异步情况，即每个节点可以有自己的个性化训练目标。通过一系列的数值实验，我们证明了我们的方法实现了与将所有数据集共享给所有节点的批处理方法相当的预测性能。此外，这些预测器支持着在保护每个节点隐私的同时具有数值上非空的泛化界限。我们明确地计算了预测性能的增量。

    We introduce a novel strategy to train randomised predictors in federated learning, where each node of the network aims at preserving its privacy by releasing a local predictor but keeping secret its training dataset with respect to the other nodes. We then build a global randomised predictor which inherits the properties of the local private predictors in the sense of a PAC-Bayesian generalisation bound. We consider the synchronous case where all nodes share the same training objective (derived from a generalisation bound), and the asynchronous case where each node may have its own personalised training objective. We show through a series of numerical experiments that our approach achieves a comparable predictive performance to that of the batch approach where all datasets are shared across nodes. Moreover the predictors are supported by numerically nonvacuous generalisation bounds while preserving privacy for each node. We explicitly compute the increment on predictive performance an
    
[^9]: EEG运动意向解码：一种与通道关注机制相比较的分析框架

    EEG motor imagery decoding: A framework for comparative analysis with channel attention mechanisms. (arXiv:2310.11198v1 [cs.HC])

    [http://arxiv.org/abs/2310.11198](http://arxiv.org/abs/2310.11198)

    本研究探索了在运动意向解码领域应用不同通道关注机制的可行性，通过构建一个轻量级架构框架，并在同一环境中比较它们的影响，结果表明这些机制的易集成性和低计算复杂度使其成为BCI中运动意向解码的有效方法。

    

    本研究的目标是探讨在大脑-计算机接口（BCI）领域中应用各种通道关注机制于运动意向解码。通道关注机制可以视为传统用于运动意向解码的空间滤波器的强大演进。本研究通过将这些机制整合到一个轻量级架构框架中，系统地比较它们的影响。我们精心构建了一个简单而轻量级的基准架构，旨在无缝集成不同的通道关注机制。这种方法与之前的研究相反，之前的研究只研究一个关注机制，并且通常构建一个非常复杂、有时嵌套的架构。我们的框架使我们能够在相同情况下评估和比较不同的关注机制的影响。易于集成不同的通道关注机制以及低计算复杂度使我们能够高效地研究和推进BCI中的运动意向解码。

    The objective of this study is to investigate the application of various channel attention mechanisms within the domain of brain-computer interface (BCI) for motor imagery decoding. Channel attention mechanisms can be seen as a powerful evolution of spatial filters traditionally used for motor imagery decoding. This study systematically compares such mechanisms by integrating them into a lightweight architecture framework to evaluate their impact. We carefully construct a straightforward and lightweight baseline architecture designed to seamlessly integrate different channel attention mechanisms. This approach is contrary to previous works which only investigate one attention mechanism and usually build a very complex, sometimes nested architecture. Our framework allows us to evaluate and compare the impact of different attention mechanisms under the same circumstances. The easy integration of different channel attention mechanisms as well as the low computational complexity enables us
    
[^10]: 一种改进的EXP3算法及其在具有多用户延迟反馈的对抗性赌博机问题中的应用

    A Modified EXP3 and Its Adaptive Variant in Adversarial Bandits with Multi-User Delayed Feedback. (arXiv:2310.11188v1 [cs.LG])

    [http://arxiv.org/abs/2310.11188](http://arxiv.org/abs/2310.11188)

    本论文提出了一种改进的EXP3算法MUD-EXP3，用于解决具有多用户延迟反馈的对抗性多臂赌博机问题。该算法通过考虑来自不同用户的重要性加权估计器，在每个回合做出决策。在已知终止回合索引$T$、用户数量$M$、臂数量$N$和延迟上界$d_{max}$的情况下，算法的遗憾值为$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$。对于未知$T$的情况，提出了一种自适应算法。

    

    对于延迟反馈的对抗性多臂赌博机问题，我们考虑延迟反馈结果来自多个用户，并且对内部分布没有限制。当玩家选择一个臂时，来自多个用户的反馈可能不会立即接收到，而是在一个未知的时间延迟之后才能接收到。在一个回合中，不同用户的反馈延迟没有潜在的相关性。因此，我们形式化了一个具有多用户延迟反馈的对抗性多臂赌博机问题，并设计了一种名为MUD-EXP3的改进EXP3算法，该算法通过考虑来自不同用户的重要性加权估计器在每个回合做出决策。在已知终止回合索引$T$、用户数量$M$、臂数量$N$和延迟上界$d_{max}$的前提下，我们证明了一个$\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$的遗憾值。此外，对于未知$T$的更常见情况，设计了一种自适应算法。

    For the adversarial multi-armed bandit problem with delayed feedback, we consider that the delayed feedback results are from multiple users and are unrestricted on internal distribution. As the player picks an arm, feedback from multiple users may not be received instantly yet after an arbitrary delay of time which is unknown to the player in advance. For different users in a round, the delays in feedback have no latent correlation. Thus, we formulate an adversarial multi-armed bandit problem with multi-user delayed feedback and design a modified EXP3 algorithm named MUD-EXP3, which makes a decision at each round by considering the importance-weighted estimator of the received feedback from different users. On the premise of known terminal round index $T$, the number of users $M$, the number of arms $N$, and upper bound of delay $d_{max}$, we prove a regret of $\mathcal{O}(\sqrt{TM^2\ln{N}(N\mathrm{e}+4d_{max})})$. Furthermore, for the more common case of unknown $T$, an adaptive algor
    
[^11]: 高效可视化大规模图表

    Efficiently Visualizing Large Graphs. (arXiv:2310.11186v1 [cs.LG])

    [http://arxiv.org/abs/2310.11186](http://arxiv.org/abs/2310.11186)

    本文介绍了一种名为t-SGNE的新颖降维方法，用于高效可视化大规模图表。该方法利用图表的邻域结构避免了耗时的成对相似度计算，并使时间复杂度从二次降低为线性。同时，还提出了一种适应t-SGNE的图表嵌入算法SPLEE。通过SPLEE获得高维嵌入，并使用t-SGNE降维进行可视化。

    

    大多数现有的基于降维的图表可视化方法由于性能问题仅适用于相对较小的图表。在本文中，我们提出了一种新颖的用于图表可视化的降维方法，称为t分布随机图表邻域嵌入（t-SGNE）。t-SGNE专门设计用于图表中的聚类结构可视化。作为标准t-SNE方法的一种变体，t-SGNE避免了耗时的成对相似度计算。相反，它利用图表的邻域结构将时间复杂度从二次降低为线性，从而支持更大的图表。此外，为了适应t-SGNE，我们将拉普拉斯特征映射与图表中的最短路径算法相结合，形成图表嵌入算法最短路径拉普拉斯特征映射嵌入（SPLEE）。通过SPLEE获得大规模图表的高维嵌入，然后使用t-SGNE将其降维以进行可视化，我们能够可视化具有高效的大规模图表。

    Most existing graph visualization methods based on dimension reduction are limited to relatively small graphs due to performance issues. In this work, we propose a novel dimension reduction method for graph visualization, called t-Distributed Stochastic Graph Neighbor Embedding (t-SGNE). t-SGNE is specifically designed to visualize cluster structures in the graph. As a variant of the standard t-SNE method, t-SGNE avoids the time-consuming computations of pairwise similarity. Instead, it uses the neighbor structures of the graph to reduce the time complexity from quadratic to linear, thus supporting larger graphs. In addition, to suit t-SGNE, we combined Laplacian Eigenmaps with the shortest path algorithm in graphs to form the graph embedding algorithm ShortestPath Laplacian Eigenmaps Embedding (SPLEE). Performing SPLEE to obtain a high-dimensional embedding of the large-scale graph and then using t-SGNE to reduce its dimension for visualization, we are able to visualize graphs with up
    
[^12]: MST-GAT: 基于多模态时空图注意力网络的时间序列异常检测

    MST-GAT: A Multimodal Spatial-Temporal Graph Attention Network for Time Series Anomaly Detection. (arXiv:2310.11169v1 [cs.LG])

    [http://arxiv.org/abs/2310.11169](http://arxiv.org/abs/2310.11169)

    MST-GAT是一种用于多模态时间序列异常检测的时空图注意力网络，通过使用多头注意力模块和关系注意力模块来捕捉模态之间的时空相关性，提高了异常检测性能。

    

    多模态时间序列（MTS）异常检测对于维护工作设备（例如水处理系统和航天器）的安全和稳定至关重要，这些设备的数据特征是多变量时间序列，并具有多样化的模态。尽管最近的深度学习方法在异常检测方面显示出巨大潜力，但它们并没有明确地捕捉到不同模态的单变量时间序列之间的时空关系，导致更多的假阴性和假阳性。在本文中，我们提出了一种多模态时空图注意力网络（MST-GAT）来解决这个问题。MST-GAT首先使用多模态图注意力网络（M-GAT）和时域卷积网络来捕捉多模态时间序列的时空相关性。具体来说，M-GAT使用多头注意力模块和两个关系注意力模块（即模态内和模态间注意力）来明确地建模模态相关性。此外，MST-GAT优化了重建误差和间隔损失函数来提高异常检测性能。

    Multimodal time series (MTS) anomaly detection is crucial for maintaining the safety and stability of working devices (e.g., water treatment system and spacecraft), whose data are characterized by multivariate time series with diverse modalities. Although recent deep learning methods show great potential in anomaly detection, they do not explicitly capture spatial-temporal relationships between univariate time series of different modalities, resulting in more false negatives and false positives. In this paper, we propose a multimodal spatial-temporal graph attention network (MST-GAT) to tackle this problem. MST-GAT first employs a multimodal graph attention network (M-GAT) and a temporal convolution network to capture the spatial-temporal correlation in multimodal time series. Specifically, M-GAT uses a multi-head attention module and two relational attention modules (i.e., intra- and inter-modal attention) to model modal correlations explicitly. Furthermore, MST-GAT optimizes the reco
    
[^13]: Serenade: 人为参与的自动和弦估计模型

    Serenade: A Model for Human-in-the-loop Automatic Chord Estimation. (arXiv:2310.11165v1 [cs.SD])

    [http://arxiv.org/abs/2310.11165](http://arxiv.org/abs/2310.11165)

    该论文提出了一种人为参与的自动和弦估计模型，通过人与自回归模型共同创建和声注释，实现在音乐信息检索任务中的性能提升。

    

    计算机和声分析对于自动的分割、语料库分析和自动和弦标签估计等音乐信息检索任务非常重要。然而，最近关于音乐和声的模糊性导致了有限的一致性，这显露出了目前通用指标如准确性存在的瓶颈。通常情况下，这些问题要么在训练数据中通过创建多数规则注释进行解决，要么在训练阶段通过学习软目标进行解决。我们提出了一个新的替代方法，其中人类和自回归模型共同为音频轨迹创建和声注释。在自动生成和声预测后，人类稀疏地标注模型置信度较低的部分，然后模型根据人类的指导调整其预测结果。我们在一组流行音乐的数据集上评估了我们的模型，并展示了这种人为参与的方法能够提高和声分析的性能。

    Computational harmony analysis is important for MIR tasks such as automatic segmentation, corpus analysis and automatic chord label estimation. However, recent research into the ambiguous nature of musical harmony, causing limited inter-rater agreement, has made apparent that there is a glass ceiling for common metrics such as accuracy. Commonly, these issues are addressed either in the training data itself by creating majority-rule annotations or during the training phase by learning soft targets. We propose a novel alternative approach in which a human and an autoregressive model together co-create a harmonic annotation for an audio track. After automatically generating harmony predictions, a human sparsely annotates parts with low model confidence and the model then adjusts its predictions following human guidance. We evaluate our model on a dataset of popular music and we show that, with this human-in-the-loop approach, harmonic analysis performance improves over a model-only appro
    
[^14]: 探索大型语言模型的创造力：模型能否产生不同的语义关联？

    Probing the Creativity of Large Language Models: Can models produce divergent semantic association?. (arXiv:2310.11158v1 [cs.CL])

    [http://arxiv.org/abs/2310.11158](http://arxiv.org/abs/2310.11158)

    本研究使用分散联想任务探索大型语言模型的创造性思维能力。研究发现，先进的模型可以生成不同的语义关联，超过了大多数人类的水平。

    

    大型语言模型在处理语言方面具有出色的能力，但这些模型是否能进一步生成创造性的内容仍不清楚。本研究旨在从认知角度研究大型语言模型的创造思维能力。我们利用分散联想任务（DAT），这是一种客观衡量创造力的方法，要求模型生成不相关的单词，并计算它们之间的语义距离。我们比较了不同模型和解码策略的结果。我们的研究发现：（1）在使用贪婪搜索策略时，GPT-4超过了96％的人类，而GPT-3.5-turbo超过了平均水平；（2）对于模型而言，随机抽样和温度调节是获得更高DAT分数的有效方法，但在创造力和稳定性之间存在权衡。这些结果表明，先进的大型语言模型具有不同的语义关联，这是创造力的基本过程。

    Large language models possess remarkable capacity for processing language, but it remains unclear whether these models can further generate creative content. The present study aims to investigate the creative thinking of large language models through a cognitive perspective. We utilize the divergent association task (DAT), an objective measurement of creativity that asks models to generate unrelated words and calculates the semantic distance between them. We compare the results across different models and decoding strategies. Our findings indicate that: (1) When using the greedy search strategy, GPT-4 outperforms 96% of humans, while GPT-3.5-turbo exceeds the average human level. (2) Stochastic sampling and temperature scaling are effective to obtain higher DAT scores for models except GPT-4, but face a trade-off between creativity and stability. These results imply that advanced large language models have divergent semantic associations, which is a fundamental process underlying creat
    
[^15]: 一种基于机器学习的概率暴露模型的德国高分辨率室内氡气地图

    A new high-resolution indoor radon map for Germany using a machine learning based probabilistic exposure model. (arXiv:2310.11143v1 [stat.ML])

    [http://arxiv.org/abs/2310.11143](http://arxiv.org/abs/2310.11143)

    本研究提出了一种基于机器学习的概率暴露模型，可以更准确地估计德国室内氡气分布，并具有更高的空间分辨率。

    

    室内氡气是一种致癌的放射性气体，可以在室内积累。通常情况下，全国范围内的室内氡暴露是基于广泛的测量活动估计得来的。然而，样本的特征往往与人口特征不同，这是由于许多相关因素，如地质源氡气的可用性或楼层水平。此外，样本大小通常不允许以高空间分辨率进行暴露估计。我们提出了一种基于模型的方法，可以比纯数据方法更加现实地估计室内氡分布，并具有更高的空间分辨率。我们采用了两阶段建模方法：1）应用分位数回归森林，使用环境和建筑数据作为预测因子，估计了德国每个住宅楼的每个楼层的室内氡概率分布函数；2）使用概率蒙特卡罗抽样技术使它们组合和。

    Radon is a carcinogenic, radioactive gas that can accumulate indoors. Indoor radon exposure at the national scale is usually estimated on the basis of extensive measurement campaigns. However, characteristics of the sample often differ from the characteristics of the population due to the large number of relevant factors such as the availability of geogenic radon or floor level. Furthermore, the sample size usually does not allow exposure estimation with high spatial resolution. We propose a model-based approach that allows a more realistic estimation of indoor radon distribution with a higher spatial resolution than a purely data-based approach. We applied a two-stage modelling approach: 1) a quantile regression forest using environmental and building data as predictors was applied to estimate the probability distribution function of indoor radon for each floor level of each residential building in Germany; (2) a probabilistic Monte Carlo sampling technique enabled the combination and
    
[^16]: BayesDiff: 通过贝叶斯推断估计扩散中的像素级不确定性

    BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference. (arXiv:2310.11142v1 [cs.CV])

    [http://arxiv.org/abs/2310.11142](http://arxiv.org/abs/2310.11142)

    BayesDiff提出了一种像素级不确定性估计方法，用于扩散模型生成结果。该方法通过贝叶斯推断和不确定性迭代原则来实现高效的估计，并可在图像生成任务中过滤低质量图像，增强成功生成结果，并纠正失败生成结果中的伪影。

    

    扩散模型在图像生成方面表现出色，但仍存在质量较低的生成结果，并且由于缺乏适当的样本度量，对其进行鉴别仍然具有挑战性。为了解决这个问题，我们提出了BayesDiff，一种基于贝叶斯推断的用于扩散模型生成结果的像素级不确定性估计器。具体而言，我们提出了一种新的不确定性迭代原则来描述扩散中的不确定性动态，并利用最后一层的拉普拉斯近似来实现高效的贝叶斯推断。估计的像素级不确定性不仅可以聚合成样本级度量，以过滤出质量较低的图像，而且还可以在文本转图像任务中增强成功的生成结果并纠正失败的生成结果中的伪影。大量实验证明了BayesDiff的有效性及其在实际应用中的潜力。

    Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.
    
[^17]: 保持各种轨迹：促进连续控制中探索集合策略的方法

    Keep Various Trajectories: Promoting Exploration of Ensemble Policies in Continuous Control. (arXiv:2310.11138v1 [cs.LG])

    [http://arxiv.org/abs/2310.11138](http://arxiv.org/abs/2310.11138)

    本研究提出了一种新的集合强化学习算法TEEN，旨在通过促进更多样化的轨迹来提高预期回报的同时增强集合策略的样本多样性。

    

    深度强化学习（DRL）与集合方法的结合已被证明在解决复杂的顺序决策问题上非常有效。这一成功主要归功于多模型的利用，它增强了策略的鲁棒性和值函数估计的准确性。然而，目前关于现有集合强化学习方法的实证成功的分析还很有限。我们的新分析揭示了之前的集合DRL算法的样本效率可能受到不够多样化的子策略的限制。在这些发现的启发下，我们的研究引入了一种新的集合强化学习算法，称为Trajectories-awarE Ensemble exploratioN (TEEN)。TEEN的主要目标是在提高预期回报的同时促进更多样化的轨迹。通过大量实验，我们证明TEEN不仅增强了集合策略的样本多样性

    The combination of deep reinforcement learning (DRL) with ensemble methods has been proved to be highly effective in addressing complex sequential decision-making problems. This success can be primarily attributed to the utilization of multiple models, which enhances both the robustness of the policy and the accuracy of value function estimation. However, there has been limited analysis of the empirical success of current ensemble RL methods thus far. Our new analysis reveals that the sample efficiency of previous ensemble DRL algorithms may be limited by sub-policies that are not as diverse as they could be. Motivated by these findings, our study introduces a new ensemble RL algorithm, termed \textbf{T}rajectories-awar\textbf{E} \textbf{E}nsemble exploratio\textbf{N} (TEEN). The primary goal of TEEN is to maximize the expected return while promoting more diverse trajectories. Through extensive experiments, we demonstrate that TEEN not only enhances the sample diversity of the ensemble
    
[^18]: 非参数的混合连续-分类变量条件独立性检验：一种新方法和数值评估

    Non-parametric Conditional Independence Testing for Mixed Continuous-Categorical Variables: A Novel Method and Numerical Evaluation. (arXiv:2310.11132v1 [cs.LG])

    [http://arxiv.org/abs/2310.11132](http://arxiv.org/abs/2310.11132)

    这项工作研究了一种针对混合类型数据集的非参数条件独立性检验方法，该方法使用基于k最近邻的CMI估计器和本地置换方案，针对现实世界中包含数值和分类变量的情况。

    

    条件独立性检验（CIT）是机器学习中常见的任务，例如变量选择，也是基于约束的因果发现的主要组成部分。大多数当下的CIT方法假设所有变量都是数值或所有变量都是分类的，然而，许多现实世界的应用涉及包含数值和分类变量的混合类型数据集。非参数的CIT可以使用基于条件互信息（CMI）估计器结合本地置换方案进行。最近，已提出了两种基于k最近邻（k-NN）的混合类型数据集的新型CMI估计器。与任何k-NN方法一样，这些估计器依赖于距离度量的定义。一种方法通过对分类变量进行one-hot编码来计算距离，从而将分类变量视为离散数值变量，而另一种方法则通过使用分类变量作为条件来通过熵来表示CMI。在这项工作中，我们研究这些方法

    Conditional independence testing (CIT) is a common task in machine learning, e.g., for variable selection, and a main component of constraint-based causal discovery. While most current CIT approaches assume that all variables are numerical or all variables are categorical, many real-world applications involve mixed-type datasets that include numerical and categorical variables. Non-parametric CIT can be conducted using conditional mutual information (CMI) estimators combined with a local permutation scheme. Recently, two novel CMI estimators for mixed-type datasets based on k-nearest-neighbors (k-NN) have been proposed. As with any k-NN method, these estimators rely on the definition of a distance metric. One approach computes distances by a one-hot encoding of the categorical variables, essentially treating categorical variables as discrete-numerical, while the other expresses CMI by entropy terms where the categorical variables appear as conditions only. In this work, we study these 
    
[^19]: FROST:面向能效AI-on-5G平台的GPU功耗调整评估

    FROST: Towards Energy-efficient AI-on-5G Platforms -- A GPU Power Capping Evaluation. (arXiv:2310.11131v1 [cs.LG])

    [http://arxiv.org/abs/2310.11131](http://arxiv.org/abs/2310.11131)

    FROST是一个能够通过对ML管道能量消耗进行分析并优化硬件的解决方案，能够在不降低模型准确性或引入延迟的情况下实现高达26.4%的能量节约。

    

    开放无线接入网络（O-RAN）是一个迅速增长的市场，在未来几年有着预计的增长。无线接入网络（RAN）对网络的设备支出具有最大的影响，更重要的是，它消耗了总能耗的73%。这使得它成为通过集成机器学习（ML）来优化的理想目标。然而，在这样的生态系统中，ML的能耗常常被忽视。我们的工作通过提出FROST灵活重构与在线系统调优方法来解决这一关键问题，这是一个符合O-RAN规范和原则的能量感知ML管道解决方案。FROST能够对ML管道的能量消耗进行分析，并相应地优化硬件，从而限制功耗。我们的研究结果表明，FROST能够实现高达26.4%的能量节约，同时不影响模型的准确性或引入显著的时间延迟。

    The Open Radio Access Network (O-RAN) is a burgeoning market with projected growth in the upcoming years. RAN has the highest CAPEX impact on the network and, most importantly, consumes 73% of its total energy. That makes it an ideal target for optimisation through the integration of Machine Learning (ML). However, the energy consumption of ML is frequently overlooked in such ecosystems. Our work addresses this critical aspect by presenting FROST Flexible Reconfiguration method with Online System Tuning - a solution for energy-aware ML pipelines that adhere to O-RAN's specifications and principles. FROST is capable of profiling the energy consumption of an ML pipeline and optimising the hardware accordingly, thereby limiting the power draw. Our findings indicate that FROST can achieve energy savings of up to 26.4% without compromising the model's accuracy or introducing significant time delays.
    
[^20]: ReLU神经网络的拓扑表达能力

    Topological Expressivity of ReLU Neural Networks. (arXiv:2310.11130v1 [cs.LG])

    [http://arxiv.org/abs/2310.11130](http://arxiv.org/abs/2310.11130)

    本研究从拓扑的角度研究了ReLU神经网络在二元分类问题中的表达能力，通过衡量网络对数据拓扑结构的改变程度，发现深层ReLU网络比浅层网络具有指数级的拓扑简化能力。

    

    我们从拓扑的角度研究了ReLU神经网络在二元分类问题中的表达能力。最近的实证研究表明，神经网络通过改变拓扑结构，将一个拓扑复杂的数据集转化为一个拓扑简单的数据集。这种拓扑简化可以用Betti数来衡量，Betti数是拓扑空间的代数不变量。我们使用相同的衡量指标来确定给定架构下ReLU神经网络可以实现的拓扑简化的上下界。因此，我们通过揭示ReLU神经网络捕捉数据的底层拓扑结构的能力，为深入理解ReLU神经网络在二元分类问题中的表达能力做出了贡献。特别是，结果表明，深层ReLU神经网络在拓扑简化方面比浅层网络具有指数级的能力。

    We study the expressivity of ReLU neural networks in the setting of a binary classification problem from a topological perspective. Recently, empirical studies showed that neural networks operate by changing topology, transforming a topologically complicated data set into a topologically simpler one as it passes through the layers. This topological simplification has been measured by Betti numbers, which are algebraic invariants of a topological space. We use the same measure to establish lower and upper bounds on the topological simplification a ReLU neural network can achieve with a given architecture. We therefore contribute to a better understanding of the expressivity of ReLU neural networks in the context of binary classification problems by shedding light on their ability to capture the underlying topological structure of the data. In particular the results show that deep ReLU neural networks are exponentially more powerful than shallow ones in terms of topological simplificatio
    
[^21]: 敏感性感知的摊销贝叶斯推断

    Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])

    [http://arxiv.org/abs/2310.11122](http://arxiv.org/abs/2310.11122)

    本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。

    

    贝叶斯推断是在不确定性下进行概率推理和决策的强大框架。现代贝叶斯工作流程中的基本选择涉及似然函数和先验分布的规范、后验逼近器和数据。每个选择都可以显着影响基于模型的推断和后续决策，因此需要进行敏感性分析。在这项工作中，我们提出了一种多方面的方法，将敏感性分析整合到摊销贝叶斯推断（ABI，即基于神经网络的模拟推断）中。首先，我们利用权重共享在训练过程中编码替代似然和先验规范之间的结构相似性，以最小的计算开销。其次，我们利用神经网络的快速推断来评估对各种数据扰动或预处理程序的敏感性。与大多数其他贝叶斯方法相比，这两个步骤都避免了昂贵的计算。

    Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
    
[^22]: 使用深度学习保持组织结构的组织冰冻切片的超分辨率

    Super resolution of histopathological frozen sections via deep learning preserving tissue structure. (arXiv:2310.11112v1 [eess.IV])

    [http://arxiv.org/abs/2310.11112](http://arxiv.org/abs/2310.11112)

    本文提出了一种使用深度学习的超分辨率方法，可用于组织病理冰冻切片，通过保留关键图像细节，减少诊断错误的风险。

    

    组织病理学在医学诊断中起着关键作用。与准备永久性切片相比，制备冰冻切片的过程更快，可以在手术期间进行，样本扫描时间应得到优化。超分辨率技术可以在低放大倍数下对样本进行成像，从而节省扫描时间。本文提出了一种新的组织病理冰冻切片的超分辨率方法，重点是获得更好的畸变度量，而不是追求可能会损害关键诊断信息的逼真图像。我们的深度学习架构通过学习插值图像与真实图像之间的误差，生成高分辨率图像，同时保留关键图像细节，减少诊断错误的风险。通过在频域中使用损失函数，将更高的权重分配给重建的图像。

    Histopathology plays a pivotal role in medical diagnostics. In contrast to preparing permanent sections for histopathology, a time-consuming process, preparing frozen sections is significantly faster and can be performed during surgery, where the sample scanning time should be optimized. Super-resolution techniques allow imaging the sample in lower magnification and sparing scanning time. In this paper, we present a new approach to super resolution for histopathological frozen sections, with focus on achieving better distortion measures, rather than pursuing photorealistic images that may compromise critical diagnostic information. Our deep-learning architecture focuses on learning the error between interpolated images and real images, thereby it generates high-resolution images while preserving critical image details, reducing the risk of diagnostic misinterpretation. This is done by leveraging the loss functions in the frequency domain, assigning higher weights to the reconstruction 
    
[^23]: 最小信息线性判别分析：使用未标记数据训练LDA模型

    Minimally Informed Linear Discriminant Analysis: training an LDA model with unlabelled data. (arXiv:2310.11110v1 [cs.LG])

    [http://arxiv.org/abs/2310.11110](http://arxiv.org/abs/2310.11110)

    本文展示了在只有未标记数据的情况下，通过一些最小的先验信息，可以计算出精确的LDA投影向量。数值实验验证了这种最小信息的线性判别分析（MILDA）模型与有监督的LDA模型的性能接近。

    

    线性判别分析（LDA）是最古老且最流行的线性方法之一，用于有监督分类问题。本文证明，如果有一些最小的先验信息，那么可以基于未标记数据计算出LDA模型的精确投影向量。更具体地说，我们展示了只需要以下三个信息中的任意一个即可计算LDA投影向量，如果只有未标记数据可用：（1）两个类别中任意一个的类别平均值，（2）两个类别平均值之间的差异（经过缩放），或者（3）类别协方差矩阵（经过缩放）。这些理论结果在数值实验中得到了验证，证明这种最小信息的线性判别分析（MILDA）模型与有监督的LDA模型的性能非常接近。此外，我们还展示了MILDA投影向量可以通过一个封闭形式计算出来，并且计算成本与LDA相当。

    Linear Discriminant Analysis (LDA) is one of the oldest and most popular linear methods for supervised classification problems. In this paper, we demonstrate that it is possible to compute the exact projection vector from LDA models based on unlabelled data, if some minimal prior information is available. More precisely, we show that only one of the following three pieces of information is actually sufficient to compute the LDA projection vector if only unlabelled data are available: (1) the class average of one of the two classes, (2) the difference between both class averages (up to a scaling), or (3) the class covariance matrices (up to a scaling). These theoretical results are validated in numerical experiments, demonstrating that this minimally informed Linear Discriminant Analysis (MILDA) model closely matches the performance of a supervised LDA model. Furthermore, we show that the MILDA projection vector can be computed in a closed form with a computational cost comparable to LD
    
[^24]: ReLU-FNNs的本地Lipschitz常数计算：使用精确性验证计算上界

    Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification. (arXiv:2310.11104v1 [math.OC])

    [http://arxiv.org/abs/2310.11104](http://arxiv.org/abs/2310.11104)

    本文研究了使用ReLUs作为激活函数的前馈神经网络的本地Lipschitz常数计算，并介绍了一种通过精确性验证计算上界的方法。

    

    本文关注使用修正线性单元（ReLUs）作为激活函数的前馈神经网络（FNNs）的本地Lipschitz常数计算。对于目标输入，FNN的本地Lipschitz常数是衡量其可靠性的合理指标。通过使用捕捉ReLUs行为的乘法器的标准过程，我们首先将本地Lipschitz常数的上界计算问题简化为一个半定规划问题（SDP）。在这里，我们引入了新的共正乘法器来准确捕捉ReLU的行为。然后，通过考虑用于上界计算的SDP的对偶问题，我们进一步得出了一个可行的测试来确定计算上界的精确性。然而，对于具有数百个ReLU的实际FNNs，这些SDP是无法解决的。为了解决这个问题，我们进一步提出了一种构造减序模型的方法，其输入输出属性与原始模型相同。

    This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original
    
[^25]: HGCVAE: 将生成式学习和对比学习整合为一体的异构图学习方法

    HGCVAE: Integrating Generative and Contrastive Learning for Heterogeneous Graph Learning. (arXiv:2310.11102v1 [cs.LG])

    [http://arxiv.org/abs/2310.11102](http://arxiv.org/abs/2310.11102)

    HGCVAE是一种将生成式学习和对比学习整合为一体的异构图学习方法，通过利用生成式的自监督学习能力来解决异构图学习的挑战。

    

    生成式自监督学习（SSL）在图学习中展示了巨大的潜力和越来越多的关注。本研究旨在探索生成式SSL在异构图学习（HGL）中的问题。以往关于异构图的SSL方法主要依赖对比学习，需要设计复杂的视图来捕捉异质性。然而，现有的生成式SSL方法并未充分利用生成模型的能力来解决HGL的挑战。在本文中，我们提出了HGCVAE，一种新颖的对比变分图自编码器，使HGL摆脱了复杂异质性的负担。HGCVAE不再专注于复杂的异质性，而是充分利用了生成式SSL的潜力。HGCVAE创新地将对比学习与生成式SSL相结合，引入了几个关键创新。首先，我们采用渐进机制生成高质量的hard样本，

    Generative self-supervised learning (SSL) has exhibited significant potential and garnered increasing interest in graph learning. In this study, we aim to explore the problem of generative SSL in the context of heterogeneous graph learning (HGL). The previous SSL approaches for heterogeneous graphs have primarily relied on contrastive learning, necessitating the design of complex views to capture heterogeneity. However, existing generative SSL methods have not fully leveraged the capabilities of generative models to address the challenges of HGL. In this paper, we present HGCVAE, a novel contrastive variational graph auto-encoder that liberates HGL from the burden of intricate heterogeneity capturing. Instead of focusing on complicated heterogeneity, HGCVAE harnesses the full potential of generative SSL. HGCVAE innovatively consolidates contrastive learning with generative SSL, introducing several key innovations. Firstly, we employ a progressive mechanism to generate high-quality hard
    
[^26]: 稀疏-DySta：用于稀疏多DNN工作负载的稀疏感知动态和静态调度

    Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads. (arXiv:2310.11096v1 [cs.DC])

    [http://arxiv.org/abs/2310.11096](http://arxiv.org/abs/2310.11096)

    本文提出了Dysta，一种稀疏感知的动态和静态调度器，用于稀疏多DNN工作负载。通过分析多个稀疏DNN的使用情况，发现了优化的机会，并提出了Dysta调度器，利用静态稀疏模式和动态稀疏信息进行调度。

    

    在边缘设备（如手机）和数据中心中，同时运行多个深度神经网络（DNN）已成为新兴工作负载，其中多个任务为单个用户的日常活动提供服务，并且从数百万用户中提出各种请求，如大型语言模型。为了减少这些工作负载的高昂计算和内存需求，引入了各种高效的稀疏化方法，导致不同类型的DNN模型普遍稀疏。在这个背景下，以往文献中很少探讨稀疏多DNN工作负载的调度问题。本文系统地分析了多个稀疏DNN的使用情况，并探讨了优化的机会。基于这些发现，我们提出了Dysta，一种新颖的双层动态和静态调度器，利用静态稀疏模式和动态稀疏信息进行稀疏多DNN调度。

    Running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyses the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling.
    
[^27]: 重学已遗忘知识：关于遗忘，过拟合和无需训练的深度神经网络集成

    Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs. (arXiv:2310.11094v1 [cs.LG])

    [http://arxiv.org/abs/2310.11094](http://arxiv.org/abs/2310.11094)

    本论文介绍一种新的评估过拟合的得分，该得分通过监测深度模型在验证数据上的遗忘速率来衡量。实证结果发现，尽管整体上泛化性能得到改善，但在数据空间的某些区域中，泛化性能可能会下降。这一观察结果有助于澄清关于深度神经网络中过拟合的困惑情况。

    

    深度神经网络中过拟合的不经常发生令人困惑。理论预测，随着模型变得更大，它们最终应该变得过度适应某个特定的训练集，从而导致泛化下降。然而，在图像分类的实证结果中，增加深度模型的训练时间或使用更大的模型几乎从不损害泛化。这是因为我们衡量过拟合的方式太过有限吗？在这里，我们引入了一种新的评估过拟合程度的得分，该得分监测深度模型在验证数据上的遗忘速率。这个分数表明，尽管整体上泛化性能得到改善，但在数据空间的某些区域中，泛化性能可能会下降。当用这种方式测量时，我们展示了过拟合可以在验证精度降低和不降低的情况下发生，并且可能比以前认为的更常见。这一观察结果可能有助于澄清前述的困惑局面。

    The infrequent occurrence of overfit in deep neural networks is perplexing. On the one hand, theory predicts that as models get larger they should eventually become too specialized for a specific training set, with ensuing decrease in generalization. In contrast, empirical results in image classification indicate that increasing the training time of deep models or using bigger models almost never hurts generalization. Is it because the way we measure overfit is too limited? Here, we introduce a novel score for quantifying overfit, which monitors the forgetting rate of deep models on validation data. Presumably, this score indicates that even while generalization improves overall, there are certain regions of the data space where it deteriorates. When thus measured, we show that overfit can occur with and without a decrease in validation accuracy, and may be more common than previously appreciated. This observation may help to clarify the aforementioned confusing picture. We use our obs
    
[^28]: SODA: 训练强鲁棒性的测试数据适配器

    SODA: Robust Training of Test-Time Data Adaptors. (arXiv:2310.11093v1 [cs.LG])

    [http://arxiv.org/abs/2310.11093](http://arxiv.org/abs/2310.11093)

    SODA提出了一种通过训练数据适配器来提高测试数据适应性能的方法，解决了传统方法中由于数据适配器可能导致数据特征损坏带来的限制性改进问题。

    

    通过适应性模型可以减轻由于分布偏移导致的性能下降。然而，隐私问题可能导致模型参数无法访问。一种有前途的方法是利用零阶优化（ZOO）来训练数据适配器，以将测试数据调整到适合已部署的模型。然而，使用ZOO训练的数据适配器通常带来了有限的改进，原因是数据适配器可能导致数据特征的损坏。为了解决这个问题，我们在测试时间数据适应的背景下重新审视了ZOO。我们发现，问题直接来源于用于优化数据适配器的梯度的不可靠估计，这主要是由于分配给测试数据的伪标签的不可靠性。基于这个观察，我们提出了伪标签稳健的数据适应（SODA）来改进数据适应的性能。具体来说，SODA利用高置信度的预测标签作为可靠的参考。

    Adapting models deployed to test distributions can mitigate the performance degradation caused by distribution shifts. However, privacy concerns may render model parameters inaccessible. One promising approach involves utilizing zeroth-order optimization (ZOO) to train a data adaptor to adapt the test data to fit the deployed models. Nevertheless, the data adaptor trained with ZOO typically brings restricted improvements due to the potential corruption of data features caused by the data adaptor. To address this issue, we revisit ZOO in the context of test-time data adaptation. We find that the issue directly stems from the unreliable estimation of the gradients used to optimize the data adaptor, which is inherently due to the unreliable nature of the pseudo-labels assigned to the test data. Based on this observation, we propose pseudo-label-robust data adaptation (SODA) to improve the performance of data adaptation. Specifically, SODA leverages high-confidence predicted labels as reli
    
[^29]: MeKB-Rec：个人知识图谱学习用于跨领域推荐

    MeKB-Rec: Personal Knowledge Graph Learning for Cross-Domain Recommendation. (arXiv:2310.11088v1 [cs.IR])

    [http://arxiv.org/abs/2310.11088](http://arxiv.org/abs/2310.11088)

    本论文提出了一种名为MeKB-Rec的跨领域推荐方法，在推荐系统中解决了冷启动问题。该方法利用个人知识图谱作为领域不变的用户兴趣表示，通过学习语义表示和注入世界知识，实现了对新用户的零-shot推荐。

    

    在现代推荐系统中，如何针对新用户有效地进行推荐，即冷启动问题，一直是一个长期存在的挑战。我们提出了个人知识图谱（PKG）作为一个领域不变的兴趣表示，并提出了一种名为MeKB-Rec的新型跨领域推荐范式。我们首先将知识图谱中的用户和实体进行关联，构建了用户兴趣的PKG，即MeKB。然后我们学习了MeKB的语义表示，用于跨领域推荐。为了高效利用CDR中有限的训练数据，MeKB-Rec采用了预训练语言模型将世界知识注入到对用户兴趣的理解中。与大多数现有系统不同，我们的方法在领域之间建立了语义映射，消除了对领域内用户行为的要求，实现了对新用户的零-shot推荐。

    It is a long-standing challenge in modern recommender systems to effectively make recommendations for new users, namely the cold-start problem. Cross-Domain Recommendation (CDR) has been proposed to address this challenge, but current ways to represent users' interests across systems are still severely limited. We introduce Personal Knowledge Graph (PKG) as a domain-invariant interest representation, and propose a novel CDR paradigm named MeKB-Rec. We first link users and entities in a knowledge base to construct a PKG of users' interests, named MeKB. Then we learn a semantic representation of MeKB for the cross-domain recommendation. To efficiently utilize limited training data in CDR, MeKB-Rec employs Pretrained Language Models to inject world knowledge into understanding users' interests. Beyond most existing systems, our approach builds a semantic mapping across domains which breaks the requirement for in-domain user behaviors, enabling zero-shot recommendations for new users in a 
    
[^30]: 使用智能手机传感器进行交通方式检测的特征金字塔双向长短时记忆网络

    Feature Pyramid biLSTM: Using Smartphone Sensors for Transportation Mode Detection. (arXiv:2310.11087v1 [cs.LG])

    [http://arxiv.org/abs/2310.11087](http://arxiv.org/abs/2310.11087)

    本研究提出了一种新颖的端到端方法，名为特征金字塔双向长短时记忆网络（FPbiLSTM），用于使用智能手机传感器进行交通方式检测。该方法通过减少所需的传感器数目和处理需求，实现更高效的建模过程，同时兼顾结果质量。通过扩展特征金字塔网络，它能够捕捉各种交通方式中的时间移动模式。

    

    智能手机的广泛利用为惯性测量单元提供了广泛的可用性，提供了一系列的传感数据，可以有利于检测交通方式。本研究的目标是提出一种新颖的端到端方法，以有效地探索从智能手机收集的少量传感数据，实现对常见日常出行活动的准确模式检测。我们的方法称为特征金字塔双向长短时记忆网络（FPbiLSTM），其特点是能够减少所需的传感器数目和处理需求，从而在不牺牲结果质量的情况下，实现更高效的建模过程。FPbiLSTM在现有的卷积神经网络双向长短时记忆模型的基础上扩展了特征金字塔网络，充分利用浅层丰富性和深层特征的韧性，捕捉各种交通方式中的时间移动模式。它表现出了优秀的性能。

    The widespread utilization of smartphones has provided extensive availability to Inertial Measurement Units, providing a wide range of sensory data that can be advantageous for the detection of transportation modes. The objective of this study is to propose a novel end-to-end approach to effectively explore a reduced amount of sensory data collected from a smartphone to achieve accurate mode detection in common daily traveling activities. Our approach, called Feature Pyramid biLSTM (FPbiLSTM), is characterized by its ability to reduce the number of sensors required and processing demands, resulting in a more efficient modeling process without sacrificing the quality of the outcomes than the other current models. FPbiLSTM extends an existing CNN biLSTM model with the Feature Pyramid Network, leveraging the advantages of both shallow layer richness and deeper layer feature resilience for capturing temporal moving patterns in various transportation modes. It exhibits an excellent performa
    
[^31]: 基于预训练语言模型的上下文少样本关系抽取

    In-Context Few-Shot Relation Extraction via Pre-Trained Language Models. (arXiv:2310.11085v1 [cs.CL])

    [http://arxiv.org/abs/2310.11085](http://arxiv.org/abs/2310.11085)

    本研究提出了基于预训练语言模型的上下文少样本关系抽取框架，首次将关系抽取任务重新定义为定制的上下文少样本学习范式。与现有方法相比，该框架不需要命名实体识别和文档人工注释，并且可以轻松更新到新的关系集合。通过评估使用DocRED数据集，验证了该框架的有效性。

    

    关系提取旨在从文本文档中推断结构化的人类知识。基于语言模型的最先进方法通常有两个限制：(1)它们要求命名实体作为输入或推断它们，从而引入了额外的噪声，(2)它们需要人工对文档进行注释。为解决这些问题，我们提出了一种新颖的基于预训练语言模型的上下文少样本关系抽取框架。据我们所知，我们是第一个将关系抽取任务重新定义为定制的上下文少样本学习范式的研究者。通过这种方式，我们在消除了命名实体识别和文档人工注释的需求的同时，实现了关键性的优势。与现有的基于微调的方法不同，我们的框架具有灵活性，可以在无需重新训练的情况下轻松更新到新的关系集合。我们使用DocRED评估了我们的框架，这是目前最大的公开可用的文档级关系提取数据集。

    Relation extraction aims at inferring structured human knowledge from textual documents. State-of-the-art methods based on language models commonly have two limitations: (1) they require named entities to be either given as input or infer them, which introduces additional noise, and (2) they require human annotations of documents. As a remedy, we present a novel framework for in-context few-shot relation extraction via pre-trained language models. To the best of our knowledge, we are the first to reformulate the relation extraction task as a tailored in-context few-shot learning paradigm. Thereby, we achieve crucial benefits in that we eliminate the need for both named entity recognition and human annotation of documents. Unlike existing methods based on fine-tuning, our framework is flexible in that it can be easily updated for a new set of relations without re-training. We evaluate our framework using DocRED, the largest publicly available dataset for document-level relation extracti
    
[^32]: CSG: 用于有符号图的课程表示学习

    CSG: Curriculum Representation Learning for Signed Graph. (arXiv:2310.11083v1 [cs.LG])

    [http://arxiv.org/abs/2310.11083](http://arxiv.org/abs/2310.11083)

    本文提出了一种用于有符号图的课程表示学习框架（CSG），通过引入课程化训练方法和轻量级机制，实现了按难易程度优化样本展示顺序，从而提高有符号图神经网络（SGNN）模型的准确性和稳定性。

    

    有符号图对于建模具有正负连接的复杂关系非常有价值，有符号图神经网络已成为其分析的重要工具。然而，在我们的工作之前，没有针对有符号图神经网络的特定训练方案，并且传统的随机抽样方法没有解决图结构中不同学习困难的问题。我们提出了一种基于课程的训练方法，其中样本从易到难，灵感来自于人类学习。为了衡量学习困难，我们引入了一个轻量级机制，并创建了用于有符号图的课程表示学习框架（CSG）。通过对六个真实数据集的实证验证，我们取得了令人印象深刻的结果，在链接符号预测（AUC）方面将SGNN模型的准确性提高了高达23.7％，并且在AUC的标准差方面显著提高了稳定性，最多减少了8.4。

    Signed graphs are valuable for modeling complex relationships with positive and negative connections, and Signed Graph Neural Networks (SGNNs) have become crucial tools for their analysis. However, prior to our work, no specific training plan existed for SGNNs, and the conventional random sampling approach did not address varying learning difficulties within the graph's structure. We proposed a curriculum-based training approach, where samples progress from easy to complex, inspired by human learning. To measure learning difficulty, we introduced a lightweight mechanism and created the Curriculum representation learning framework for Signed Graphs (CSG). This framework optimizes the order in which samples are presented to the SGNN model. Empirical validation across six real-world datasets showed impressive results, enhancing SGNN model accuracy by up to 23.7% in link sign prediction (AUC) and significantly improving stability with an up to 8.4 reduction in the standard deviation of AUC
    
[^33]: 基于多组学采样的图转换器用于合成致死预测

    Multi-omics Sampling-based Graph Transformer for Synthetic Lethality Prediction. (arXiv:2310.11082v1 [cs.LG])

    [http://arxiv.org/abs/2310.11082](http://arxiv.org/abs/2310.11082)

    这篇论文提出了一种基于多组学采样的图转换器用于合成致死预测，通过引入浅层多视图GNN和标准的自注意力机制，解决了图神经网络在SL预测中的限制问题，并利用多组学数据中的非SL基因关系信息提高了预测性能。

    

    合成致死（SL）预测是用于识别两个基因的共突变是否导致细胞死亡的方法。目前的策略是将SL预测抽象为在SL数据中的基因节点上的边分类任务，并通过图神经网络（GNNs）实现。然而，GNNs存在消息传递机制的限制，包括过度平滑和过度压缩问题。此外，利用大规模多组学数据中的非SL基因关系信息来促进SL预测面临着一个非常具有挑战性的问题。为了解决这些问题，我们提出了一种新的基于多组学采样的图转换器用于SL预测（MSGT-SL）。具体而言，我们引入了一个浅层多视图GNN来从SL和多组学数据中获取局部结构模式。此外，我们将编码多视图信息的基因特征输入到标准的自注意力机制中，以捕捉长程依赖关系。值得注意的是，我们从SL数据中的批量基因开始，采用并行化的方法。

    Synthetic lethality (SL) prediction is used to identify if the co-mutation of two genes results in cell death. The prevalent strategy is to abstract SL prediction as an edge classification task on gene nodes within SL data and achieve it through graph neural networks (GNNs). However, GNNs suffer from limitations in their message passing mechanisms, including over-smoothing and over-squashing issues. Moreover, harnessing the information of non-SL gene relationships within large-scale multi-omics data to facilitate SL prediction poses a non-trivial challenge. To tackle these issues, we propose a new multi-omics sampling-based graph transformer for SL prediction (MSGT-SL). Concretely, we introduce a shallow multi-view GNN to acquire local structural patterns from both SL and multi-omics data. Further, we input gene features that encode multi-view information into the standard self-attention to capture long-range dependencies. Notably, starting with batch genes from SL data, we adopt paral
    
[^34]: 团结一致：利用分时一致性集合来对抗过拟合

    United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit. (arXiv:2310.11077v1 [cs.LG])

    [http://arxiv.org/abs/2310.11077](http://arxiv.org/abs/2310.11077)

    本论文提出了一种新的深度网络集成分类器，通过分析和实证发现过拟合时分类器之间的方差增加，基于此构建了一种通过整个训练过程中最具一致性的预测结果来对抗过拟合的方法。在实验中表明，这种方法在多个图像和文本分类任务上的表现优于传统的集成方法。

    

    深度神经网络已经成为解决许多图像分类任务的首选方法，主要是因为它们可以拟合原始图像上定义的非常复杂的函数。这种强大学习器的缺点是过拟合训练集的危险，导致泛化能力差，通常通过正则化和训练的“提前停止”来避免。在本文中，我们提出了一种新的深度网络集成分类器，它对抗过拟合非常有效。我们首先对回归模型进行理论分析，证明了当发生过拟合时，分类器之间的方差增加，这一点已在常用的深度网络中得到了实证。在这些结果的指导下，我们构建了一种新的基于集成的预测方法，旨在对抗过拟合，其中预测结果是通过整个训练过程中最具一致性的预测结果确定的。在多个图像和文本分类数据集上，我们展示了当常规集成遭受过拟合时，我们的方法能够更好地应对。

    Deep neural networks have become the method of choice for solving many image classification tasks, largely because they can fit very complex functions defined over raw images. The downside of such powerful learners is the danger of overfitting the training set, leading to poor generalization, which is usually avoided by regularization and "early stopping" of the training. In this paper, we propose a new deep network ensemble classifier that is very effective against overfit. We begin with the theoretical analysis of a regression model, whose predictions - that the variance among classifiers increases when overfit occurs - is demonstrated empirically in deep networks in common use. Guided by these results, we construct a new ensemble-based prediction method designed to combat overfit, where the prediction is determined by the most consensual prediction throughout the training. On multiple image and text classification datasets, we show that when regular ensembles suffer from overfit, ou
    
[^35]: 低成本重采样随机梯度下降用于高效不确定性量化

    Resampling Stochastic Gradient Descent Cheaply for Efficient Uncertainty Quantification. (arXiv:2310.11065v1 [stat.ML])

    [http://arxiv.org/abs/2310.11065](http://arxiv.org/abs/2310.11065)

    本研究提出了两种低成本重采样的方法，用于构建随机梯度下降解的置信区间，这一方法可以有效减少计算工作量，并绕过现有方法中的混合条件。

    

    随机梯度下降（SGD）或随机逼近在模型训练和随机优化中被广泛使用。虽然有大量关于其收敛性分析的文献，但对从SGD获得的解进行推断的研究只是最近才开始，但由于对不确定性量化的日益需求而变得重要。我们研究了两种计算上廉价的基于重采样的方法来构建SGD解的置信区间。一个方法通过从数据中进行替换重采样来使用多个但少量的SGD并行进行操作，另一个方法以在线方式进行操作。我们的方法可以被视为对已建立的Bootstrap方案进行增强，以显着减少重采样需求方面的计算工作量，同时绕过现有批处理方法中复杂的混合条件。我们通过最近的所谓低成本bootstrap思想和SGD的Berry-Esseen型边界来实现这些目标。

    Stochastic gradient descent (SGD) or stochastic approximation has been widely used in model training and stochastic optimization. While there is a huge literature on analyzing its convergence, inference on the obtained solutions from SGD has only been recently studied, yet is important due to the growing need for uncertainty quantification. We investigate two computationally cheap resampling-based methods to construct confidence intervals for SGD solutions. One uses multiple, but few, SGDs in parallel via resampling with replacement from the data, and another operates this in an online fashion. Our methods can be regarded as enhancements of established bootstrap schemes to substantially reduce the computation effort in terms of resampling requirements, while at the same time bypassing the intricate mixing conditions in existing batching methods. We achieve these via a recent so-called cheap bootstrap idea and Berry-Esseen-type bound for SGD.
    
[^36]: 局部差分隐私图嵌入

    Locally Differentially Private Graph Embedding. (arXiv:2310.11060v1 [cs.CR])

    [http://arxiv.org/abs/2310.11060](http://arxiv.org/abs/2310.11060)

    该论文提出了一种局部差分隐私图嵌入框架（LDP-GE），该框架采用LDP机制来保护节点数据的隐私，并使用个性化PageRank作为近似度度量来学习节点表示。大量实验证明，LDP-GE在隐私和效用方面取得了有利的折衷效果，并且明显优于现有的方法。

    

    图嵌入被证明是学习图中节点潜在表示的强大工具。然而，尽管在各种基于图的机器学习任务中表现出卓越性能，但在涉及敏感信息的图数据上进行学习可能引发重大的隐私问题。为了解决这个问题，本文研究了开发能满足局部差分隐私（LDP）的图嵌入算法的问题。我们提出了一种新颖的隐私保护图嵌入框架LDP-GE，用于保护节点数据的隐私。具体而言，我们提出了一种LDP机制来混淆节点数据，并采用个性化PageRank作为近似度度量来学习节点表示。然后，我们从理论上分析了LDP-GE框架的隐私保证和效用。在几个真实世界的图数据集上进行的大量实验表明，LDP-GE在隐私-效用权衡方面取得了有利的效果，并且明显优于现有的方法。

    Graph embedding has been demonstrated to be a powerful tool for learning latent representations for nodes in a graph. However, despite its superior performance in various graph-based machine learning tasks, learning over graphs can raise significant privacy concerns when graph data involves sensitive information. To address this, in this paper, we investigate the problem of developing graph embedding algorithms that satisfy local differential privacy (LDP). We propose LDP-GE, a novel privacy-preserving graph embedding framework, to protect the privacy of node data. Specifically, we propose an LDP mechanism to obfuscate node data and adopt personalized PageRank as the proximity measure to learn node representations. Then, we theoretically analyze the privacy guarantees and utility of the LDP-GE framework. Extensive experiments conducted over several real-world graph datasets demonstrate that LDP-GE achieves favorable privacy-utility trade-offs and significantly outperforms existing appr
    
[^37]: 通过传输熵进行因果特征选择

    Causal Feature Selection via Transfer Entropy. (arXiv:2310.11059v1 [cs.LG])

    [http://arxiv.org/abs/2310.11059](http://arxiv.org/abs/2310.11059)

    本论文提出了一种新的方法，通过将特征选择和因果发现相结合，在时间序列中实现了因果特征选择，并利用传输熵来估计信息的因果流动。

    

    机器学习算法旨在捕捉特征之间的复杂关系。在这种情况下，数据的高维度经常导致模型性能不佳，存在过拟合的风险。特征选择是选择相关且非冗余特征子集的过程，因此是减轻这些问题的关键步骤。然而，传统的特征选择方法不检查所选特征与目标之间的因果关系，这可能在实际应用中导致误导性结果。相反，因果发现旨在使用观察数据识别特征之间的因果关系。在本文中，我们提出了一种新的方法，将特征选择和因果发现交叉应用于时间序列。我们介绍了一种新的因果特征选择方法，它依赖于前向和后向特征选择过程，并利用传输熵来估计信息的因果流动。

    Machine learning algorithms are designed to capture complex relationships between features. In this context, the high dimensionality of data often results in poor model performance, with the risk of overfitting. Feature selection, the process of selecting a subset of relevant and non-redundant features, is, therefore, an essential step to mitigate these issues. However, classical feature selection approaches do not inspect the causal relationship between selected features and target, which can lead to misleading results in real-world applications. Causal discovery, instead, aims to identify causal relationships between features with observational data. In this paper, we propose a novel methodology at the intersection between feature selection and causal discovery, focusing on time series. We introduce a new causal feature selection approach that relies on the forward and backward feature selection procedures and leverages transfer entropy to estimate the causal flow of information from
    
[^38]: SemEval-2023任务6中的非纳任务:法律评估方法论。(arXiv:2310.11049v1 [cs.CL])

    Nonet at SemEval-2023 Task 6: Methodologies for Legal Evaluation. (arXiv:2310.11049v1 [cs.CL])

    [http://arxiv.org/abs/2310.11049](http://arxiv.org/abs/2310.11049)

    这篇论文介绍了我们在SemEval-2023法律评估任务6上的提交，主要集中在法律命名实体识别、法律判决预测和带解释的法院判决预测等子任务上。我们进行了多个实验，并取得了在各个子任务中具有竞争力的排名。

    

    本文描述了我们在SemEval-2023法律评估任务6上的提交。我们的提交主要集中在三个子任务上：任务B的法律命名实体识别(L-NER)，任务C1的法律判决预测(LJP)和任务C2的带解释的法院判决预测(CJPE)。我们对这些子任务进行了各种实验，并详细呈现了结果，包括数据统计和方法论。值得注意的是，像本研究中所涉及的法律任务正在因自动化法律分析和支持的需求增加而变得越来越重要。我们的团队在排行榜上报告的任务B、任务C1和任务C2中分别获得了15th、11th和1st的竞争排名。

    This paper describes our submission to the SemEval-2023 for Task 6 on LegalEval: Understanding Legal Texts. Our submission concentrated on three subtasks: Legal Named Entity Recognition (L-NER) for Task-B, Legal Judgment Prediction (LJP) for Task-C1, and Court Judgment Prediction with Explanation (CJPE) for Task-C2. We conducted various experiments on these subtasks and presented the results in detail, including data statistics and methodology. It is worth noting that legal tasks, such as those tackled in this research, have been gaining importance due to the increasing need to automate legal analysis and support. Our team obtained competitive rankings of 15$^{th}$, 11$^{th}$, and 1$^{st}$ in Task-B, Task-C1, and Task-C2, respectively, as reported on the leaderboard.
    
[^39]: 通过分布鲁棒优化理解对比学习

    Understanding Contrastive Learning via Distributionally Robust Optimization. (arXiv:2310.11048v1 [cs.LG])

    [http://arxiv.org/abs/2310.11048](http://arxiv.org/abs/2310.11048)

    通过分布鲁棒优化的视角，该研究揭示了对比学习对采样偏差的内在容忍度，并提供了几个关键见解。

    

    该研究揭示了对比学习（CL）对采样偏差的内在容忍度，其中负样本可能包含类似的语义（例如标签）。然而，现有理论在解释这一现象方面存在不足。我们通过分布鲁棒优化（DRO）的视角分析CL，得出了几个关键见解：（1）CL本质上是在负采样分布上进行DRO，从而实现对各种潜在分布的强大性能和对采样偏差的鲁棒性；（2）温度$\tau$的设计不仅仅是一种启发式方法，而是作为一个拉格朗日系数，调节潜在分布集合的大小；（3）在DRO和互信息之间建立了一个理论连接，从而为“InfoNCE作为MI估计”的提供了新证据，以及基于$\phi$-散度的广义互信息的新估计方法。我们还确定了CL的创新点。

    This study reveals the inherent tolerance of contrastive learning (CL) towards sampling bias, wherein negative samples may encompass similar semantics (\eg labels). However, existing theories fall short in providing explanations for this phenomenon. We bridge this research gap by analyzing CL through the lens of distributionally robust optimization (DRO), yielding several key insights: (1) CL essentially conducts DRO over the negative sampling distribution, thus enabling robust performance across a variety of potential distributions and demonstrating robustness to sampling bias; (2) The design of the temperature $\tau$ is not merely heuristic but acts as a Lagrange Coefficient, regulating the size of the potential distribution set; (3) A theoretical connection is established between DRO and mutual information, thus presenting fresh evidence for ``InfoNCE as an estimate of MI'' and a new estimation approach for $\phi$-divergence-based generalized mutual information. We also identify CL'
    
[^40]: 基于结构的神经切线内核的快速图结构压缩

    Fast Graph Condensation with Structure-based Neural Tangent Kernel. (arXiv:2310.11046v1 [cs.LG])

    [http://arxiv.org/abs/2310.11046](http://arxiv.org/abs/2310.11046)

    本文提出了一种以数据为中心的解决方案，将大型图数据集压缩为较小的集合而不会损失GNN的预测性能。通过将图结构压缩问题转化为核岭回归任务，利用基于结构的神经切线内核来捕捉图的拓扑结构。

    

    互联网技术的快速发展造成了大量的图结构数据。图神经网络（GNN）作为一种有效的图挖掘方法，在处理大规模图数据时会导致大量的计算资源开销。本文提出了一种以数据为中心的解决方案，将大型图数据集压缩为较小的集合，而不会损失GNN的预测性能。然而，现有的方法通过计算密集型的双层优化架构来压缩图结构数据，同样也会带来巨大的计算开销。本文将图结构压缩问题改为核岭回归任务，而不是在双层优化的内循环中迭代训练GNN。具体来说，本文提出了一种新的图数据集压缩框架（GC-SNTK），其中开发了一种基于结构的神经切线内核（SNTK）来捕捉图的拓扑结构。

    The rapid development of Internet technology has given rise to a vast amount of graph-structured data. Graph Neural Networks (GNNs), as an effective method for various graph mining tasks, incurs substantial computational resource costs when dealing with large-scale graph data. A data-centric manner solution is proposed to condense the large graph dataset into a smaller one without sacrificing the predictive performance of GNNs. However, existing efforts condense graph-structured data through a computational intensive bi-level optimization architecture also suffer from massive computation costs. In this paper, we propose reforming the graph condensation problem as a Kernel Ridge Regression (KRR) task instead of iteratively training GNNs in the inner loop of bi-level optimization. More specifically, We propose a novel dataset condensation framework (GC-SNTK) for graph-structured data, where a Structure-based Neural Tangent Kernel (SNTK) is developed to capture the topology of graph and s
    
[^41]: 矩阵压缩通过随机低秩低精度因式分解

    Matrix Compression via Randomized Low Rank and Low Precision Factorization. (arXiv:2310.11028v1 [cs.LG])

    [http://arxiv.org/abs/2310.11028](http://arxiv.org/abs/2310.11028)

    通过随机化的低秩和低精度因式分解，我们提出了一种矩阵压缩算法，可以有效地减小存储和处理大型矩阵所需的计算资源和内存使用。

    

    矩阵在各个研究领域中都非常有用，因为它们提供了一种方便的框架，可以以结构化的方式组织和操作数据。然而，现代矩阵可能包含数十亿个元素，使得它们的存储和处理对计算资源和内存使用要求很高。虽然这些矩阵非常大，但它们通常是近似低秩的。我们提出了一种算法，利用这种结构来获得任何矩阵 $\mathbf{A}$ 的低秩分解，即 $\mathbf{A} \approx \mathbf{L}\mathbf{R}$，其中 $\mathbf{L}$ 和 $\mathbf{R}$ 是低秩因子。$\mathbf{L}$ 和 $\mathbf{R}$ 中的元素总数可以显著少于 $\mathbf{A}$ 中的元素总数。此外，$\mathbf{L}$ 和 $\mathbf{R}$ 的条目被量化为低精度格式 $--$ 通过给出低秩和低精度因式分解来压缩 $\mathbf{A}$。我们的算法首先计算 $\mathbf$

    Matrices are exceptionally useful in various fields of study as they provide a convenient framework to organize and manipulate data in a structured manner. However, modern matrices can involve billions of elements, making their storage and processing quite demanding in terms of computational resources and memory usage. Although prohibitively large, such matrices are often approximately low rank. We propose an algorithm that exploits this structure to obtain a low rank decomposition of any matrix $\mathbf{A}$ as $\mathbf{A} \approx \mathbf{L}\mathbf{R}$, where $\mathbf{L}$ and $\mathbf{R}$ are the low rank factors. The total number of elements in $\mathbf{L}$ and $\mathbf{R}$ can be significantly less than that in $\mathbf{A}$. Furthermore, the entries of $\mathbf{L}$ and $\mathbf{R}$ are quantized to low precision formats $--$ compressing $\mathbf{A}$ by giving us a low rank and low precision factorization. Our algorithm first computes an approximate basis of the range space of $\mathb
    
[^42]: SignGT：基于带符号注意力的图变换器用于图表征学习

    SignGT: Signed Attention-based Graph Transformer for Graph Representation Learning. (arXiv:2310.11025v1 [cs.LG])

    [http://arxiv.org/abs/2310.11025](http://arxiv.org/abs/2310.11025)

    这项工作提出了一种带符号注意力的图变换器（SignGT），它能够自适应地捕捉各种频率信息，对于学习复杂节点关系的异质图等不同的图形非常高效。

    

    新兴的图变换器在图神经网络（GNNs）上的图表征学习方面取得了令人印象深刻的性能。在这项工作中，我们将自注意机制（图变换器的核心模块）视为在完全连接的图上的两步聚合操作。由于生成正注意值的特性，自注意机制等同于对所有节点进行平滑操作，保留了低频信息。然而，仅仅捕捉低频信息在学习复杂的节点关系上是低效的，尤其是对于包含高频信息至关重要的异质图等不同的图形。为此，我们提出了一种带符号注意力的图变换器（SignGT），以自适应地捕捉图中的各种频率信息。具体而言，SignGT开发了一种新的带符号自注意机制（SignSA），根据节点对的语义相关性生成带符号注意值。

    The emerging graph Transformers have achieved impressive performance for graph representation learning over graph neural networks (GNNs). In this work, we regard the self-attention mechanism, the core module of graph Transformers, as a two-step aggregation operation on a fully connected graph. Due to the property of generating positive attention values, the self-attention mechanism is equal to conducting a smooth operation on all nodes, preserving the low-frequency information. However, only capturing the low-frequency information is inefficient in learning complex relations of nodes on diverse graphs, such as heterophily graphs where the high-frequency information is crucial. To this end, we propose a Signed Attention-based Graph Transformer (SignGT) to adaptively capture various frequency information from the graphs. Specifically, SignGT develops a new signed self-attention mechanism (SignSA) that produces signed attention values according to the semantic relevance of node pairs. Hen
    
[^43]: 不规则采样的多变量时间序列的兼容Transformer

    Compatible Transformer for Irregularly Sampled Multivariate Time Series. (arXiv:2310.11022v1 [cs.LG])

    [http://arxiv.org/abs/2310.11022](http://arxiv.org/abs/2310.11022)

    本文提出了一种适用于不规则采样的多变量时间序列的兼容Transformer方法（CoFormer），通过利用内变量/间变量的注意力机制，实现对每个个体样本的综合时序交互特征学习。

    

    为了分析多变量时间序列，先前的大多数方法都假设时间序列的正则子采样，其中相邻测量之间的间隔和样本数量保持不变。实际上，由于传感器故障和干预，数据采集系统可能会产生不规则采样的时间序列。然而，现有的针对正则采样的多变量时间序列的方法无法直接处理不规则性，因为在时间和变量维度上存在错位。为了填补这个空白，我们提出了兼容Transformer（CoFormer），这是一种基于Transformer的编码器，用于在不规则的多变量时间序列中为每个个体样本实现全面的时序交互特征学习。在CoFormer中，我们将每个样本视为一个独特的变量-时间点，并利用内变量/间变量注意力来学习基于内变量/间变量相邻点的逐样本时序/交互特征。借助CoFormer作为核心，我们可以分析不规则的采样。

    To analyze multivariate time series, most previous methods assume regular subsampling of time series, where the interval between adjacent measurements and the number of samples remain unchanged. Practically, data collection systems could produce irregularly sampled time series due to sensor failures and interventions. However, existing methods designed for regularly sampled multivariate time series cannot directly handle irregularity owing to misalignment along both temporal and variate dimensions. To fill this gap, we propose Compatible Transformer (CoFormer), a transformer-based encoder to achieve comprehensive temporal-interaction feature learning for each individual sample in irregular multivariate time series. In CoFormer, we view each sample as a unique variate-time point and leverage intra-variate/inter-variate attentions to learn sample-wise temporal/interaction features based on intra-variate/inter-variate neighbors. With CoFormer as the core, we can analyze irregularly sample
    
[^44]: 异步联邦赌博机中的纯探索研究

    Pure Exploration in Asynchronous Federated Bandits. (arXiv:2310.11015v1 [cs.LG])

    [http://arxiv.org/abs/2310.11015](http://arxiv.org/abs/2310.11015)

    该论文研究了异步联邦赌博机中的纯探索问题，并提出了首个在完全异步环境下实现近乎最优样本复杂性和高效通信成本的联邦异步多臂赌博机和线性赌博机纯探索算法。

    

    我们研究了联邦纯探索问题，包括多臂赌博机和线性赌博机，其中M个代理通过与中央服务器通信，合作地确定最佳抽臂。为了增强对延迟和代理不可用性的鲁棒性，我们提出了第一个联邦异步多臂赌博机和线性赌博机纯探索算法，用于固定置信度。我们的理论分析表明，所提出的算法在完全异步环境中实现了近乎最优的样本复杂性和高效的通信成本。此外，基于合成和真实数据的实验结果从经验上阐明了所提出算法的效力和通信成本效率。

    We study the federated pure exploration problem of multi-armed bandits and linear bandits, where $M$ agents cooperatively identify the best arm via communicating with the central server. To enhance the robustness against latency and unavailability of agents that are common in practice, we propose the first federated asynchronous multi-armed bandit and linear bandit algorithms for pure exploration with fixed confidence. Our theoretical analysis shows the proposed algorithms achieve near-optimal sample complexities and efficient communication costs in a fully asynchronous environment. Moreover, experimental results based on synthetic and real-world data empirically elucidate the effectiveness and communication cost-efficiency of the proposed algorithms.
    
[^45]: 基于光学频率梳和可编程光记忆的高光谱内存计算

    Hyperspectral In-Memory Computing with Optical Frequency Combs and Programmable Optical Memories. (arXiv:2310.11014v1 [physics.optics])

    [http://arxiv.org/abs/2310.11014](http://arxiv.org/abs/2310.11014)

    提出了一种基于光学频率梳和可编程光记忆的高光谱内存计算架构，通过集成空间复用和频率复用的方法，解决了光学计算系统的并行性、可编程性和可扩展性问题。

    

    随着机器学习在各个行业的快速发展，对广泛的矩阵-向量乘法运算的需求增加，进而对传统的冯·诺伊曼计算架构的能力提出了挑战。为了解决这个问题，研究人员正在探索替代方案，如内存计算系统，以开发更快速、更能耗有效的硬件。尤其是基于光学的计算系统再次引起了人们的兴趣，它有望以更能耗有效的方式处理矩阵-向量乘法。尽管初步结果有所改善，但仍然没有开发出高度并行、可编程且可扩展的光学计算系统能够与电子计算硬件相媲美。在这个背景下，我们提出了一种集成了光学频率梳的空间复用和频率复用的高光谱内存计算架构，并使用空间光调制器作为可编程光记忆。

    The rapid advancements in machine learning across numerous industries have amplified the demand for extensive matrix-vector multiplication operations, thereby challenging the capacities of traditional von Neumann computing architectures. To address this, researchers are currently exploring alternatives such as in-memory computing systems to develop faster and more energy-efficient hardware. In particular, there is renewed interest in computing systems based on optics, which could potentially handle matrix-vector multiplication in a more energy-efficient way. Despite promising initial results, developing a highly parallel, programmable, and scalable optical computing system capable of rivaling electronic computing hardware still remains elusive. In this context, we propose a hyperspectral in-memory computing architecture that integrates space multiplexing with frequency multiplexing of optical frequency combs and uses spatial light modulators as a programmable optical memory, thereby bo
    
[^46]: 从可识别的因果表示到可控的反事实生成：因果生成建模综述

    From Identifiable Causal Representations to Controllable Counterfactual Generation: A Survey on Causal Generative Modeling. (arXiv:2310.11011v1 [cs.LG])

    [http://arxiv.org/abs/2310.11011](http://arxiv.org/abs/2310.11011)

    本文综述了因果生成建模的技术，其中分为因果表示学习和可控反事实生成两个部分，这些模型融合了因果理论，解决了深度生成模型的一些根本性缺点，并提供了分布偏移鲁棒性、公平性和互操作性等有益属性。

    

    深度生成模型在数据密度估计和从有限样本中生成数据方面取得了巨大的成功。然而，这些模型存在一些根本性的缺点，如缺乏可解释性、引入虚假相关性和差劲的超出分布的外推能力。为了解决这些挑战，可以将因果理论融入深度生成建模中。结构因果模型描述了数据生成过程，并对系统中变量之间的复杂因果关系和机制进行建模。因此，结构因果模型可以与深度生成模型自然地结合。因果模型为深度生成模型提供了几个有益的属性，如分布偏移鲁棒性、公平性和互操作性。本文提供了对因果生成建模的技术综述，分为因果表示学习和可控反事实生成两个部分。

    Deep generative models have shown tremendous success in data density estimation and data generation from finite samples. While these models have shown impressive performance by learning correlations among features in the data, some fundamental shortcomings are their lack of explainability, the tendency to induce spurious correlations, and poor out-of-distribution extrapolation. In an effort to remedy such challenges, one can incorporate the theory of causality in deep generative modeling. Structural causal models (SCMs) describe data-generating processes and model complex causal relationships and mechanisms among variables in a system. Thus, SCMs can naturally be combined with deep generative models. Causal models offer several beneficial properties to deep generative models, such as distribution shift robustness, fairness, and interoperability. We provide a technical survey on causal generative modeling categorized into causal representation learning and controllable counterfactual ge
    
[^47]: 自适应的对向编码用于链路预测

    Adaptive Pairwise Encodings for Link Prediction. (arXiv:2310.11009v1 [cs.LG])

    [http://arxiv.org/abs/2310.11009](http://arxiv.org/abs/2310.11009)

    提出了一种自适应的对向编码方法，用于解决链路预测中现有方法的归纳偏差问题。该方法将消息传递神经网络和启发式方法结合起来，能够更好地分类各种不同因素形成的链路。

    

    链路预测是一种常见的基于图结构数据的任务，在各个领域都有应用。经典方法通常使用手工设计的启发式策略来进行预测。启发式度量被选择为在与链路形成相关的基本因素上与之相关良好。近年来，出现了一类新的方法，将消息传递神经网络（MPNN）的优势与启发式方法结合起来。这些方法通过使用MPNN的输出以及捕捉候选链路中节点之间关系的“对向编码”来进行预测。它们已经在许多数据集上表现出强大的性能。然而，目前的对向编码往往具有强烈的归纳偏差，使用相同的基本因素来分类所有链路。这限制了现有方法学习如何正确分类可能由不同因素形成的各种不同链路的能力。为了解决这个问题，我们提出了一个自适应的对向编码方法。

    Link prediction is a common task on graph-structured data that has seen applications in a variety of domains. Classically, hand-crafted heuristics were used for this task. Heuristic measures are chosen such that they correlate well with the underlying factors related to link formation. In recent years, a new class of methods has emerged that combines the advantages of message-passing neural networks (MPNN) and heuristics methods. These methods perform predictions by using the output of an MPNN in conjunction with a "pairwise encoding" that captures the relationship between nodes in the candidate link. They have been shown to achieve strong performance on numerous datasets. However, current pairwise encodings often contain a strong inductive bias, using the same underlying factors to classify all links. This limits the ability of existing methods to learn how to properly classify a variety of different links that may form from different factors. To address this limitation, we propose a 
    
[^48]: 为语音识别进行纠错的语言模型训练

    Correction Focused Language Model Training for Speech Recognition. (arXiv:2310.11003v1 [cs.CL])

    [http://arxiv.org/abs/2310.11003](http://arxiv.org/abs/2310.11003)

    本研究介绍了一种为语音识别进行纠错的语言模型训练方法，通过定义易出错单词的分数并将其用作先验分布来指导训练，并利用大型语言模型进行纠错型训练。实验证明该方法在领域适应任务中有效，相对传统方法可以显著降低单词错误率。

    

    语言模型（LM）通常被用于提高自动语音识别（ASR）的性能，尤其是在领域适应任务中。传统的LM训练方式将语料库中的所有单词平等对待，导致ASR性能的改进效果不佳。本文引入了一种新颖的纠错型LM训练方法，旨在优先处理ASR易出错的单词。我们定义了单词级ASR易出错分数，表示ASR错误识别的可能性，并将其形成一个先验单词分布以指导LM训练。为了在仅有文本语料库的情况下实现纠错型训练，我们采用大型语言模型（LLM）作为易出错分数预测器和文本生成器，并进行多任务微调。领域适应任务的实验结果表明了我们提出的方法的有效性。与传统的LM相比，纠错型训练在足够数据集上可以达到相对5.5%的单词错误率（WER）降低。

    Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likelihood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Compared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient te
    
[^49]: 基于物联网传感器网络和机器学习技术的空间局部天气预测和异常检测

    Spatially-resolved hyperlocal weather prediction and anomaly detection using IoT sensor networks and machine learning techniques. (arXiv:2310.11001v1 [cs.LG])

    [http://arxiv.org/abs/2310.11001](http://arxiv.org/abs/2310.11001)

    本研究提出了一种利用物联网传感器网络和机器学习技术进行空间局部天气预测和异常检测的新方法。通过利用来自多个位置和物联网传感器的数据，创建高分辨率的天气模型，并监测天气参数的变化，系统能够提高预测的空间分辨率，并实时检测异常。这一系统有潜力改善决策。

    

    准确和及时的空间局部天气预测对于农业到灾害管理等各种应用至关重要。本文提出了一种新颖的方法，将物联网传感器网络和先进的机器学习技术结合起来，结合空间局部天气预测和异常检测。我们的方法利用来自多个空间分布但相对接近的位置和物联网传感器的数据，创建高分辨率的天气模型，能够预测短期、局部的天气条件，如温度、压力和湿度。通过监测这些位置的天气参数的变化，我们的系统能够提高预测的空间分辨率，并实时有效地检测异常。此外，我们的系统利用无监督学习算法来识别异常的天气模式，提供及时的警报。我们的研究结果表明，这个系统有潜力改善决策。

    Accurate and timely hyperlocal weather predictions are essential for various applications, ranging from agriculture to disaster management. In this paper, we propose a novel approach that combines hyperlocal weather prediction and anomaly detection using IoT sensor networks and advanced machine learning techniques. Our approach leverages data from multiple spatially-distributed yet relatively close locations and IoT sensors to create high-resolution weather models capable of predicting short-term, localized weather conditions such as temperature, pressure, and humidity. By monitoring changes in weather parameters across these locations, our system is able to enhance the spatial resolution of predictions and effectively detect anomalies in real-time. Additionally, our system employs unsupervised learning algorithms to identify unusual weather patterns, providing timely alerts. Our findings indicate that this system has the potential to enhance decision-making.
    
[^50]: 使用节点自适应传播加速可扩展的图神经网络推论

    Accelerating Scalable Graph Neural Network Inference with Node-Adaptive Propagation. (arXiv:2310.10998v1 [cs.LG])

    [http://arxiv.org/abs/2310.10998](http://arxiv.org/abs/2310.10998)

    本论文提出了一种在线传播框架和两种新的节点自适应传播方法，用于加速可扩展的图神经网络推论。这些方法能够根据节点的拓扑信息自定义每个节点的最佳传播深度，从而避免冗余特征传播，并通过简单的超参数灵活地管理准确性和延迟之间的权衡，以适应不同的延迟限制。

    

    图神经网络（GNNs）在各种应用中展现出了非凡的功效。然而，大规模图的规模使得GNNs的实时推论面临巨大挑战。尽管现有的可扩展GNNs利用线性传播对特征进行预处理并加速训练和推论过程，但在对未知节点进行推论时仍然存在可扩展性问题，因为特征预处理需要已知且固定的图。为了进一步加速这种归纳设置下的可扩展GNNs推论，我们提出了一个在线传播框架和两种新的节点自适应传播方法，可以根据节点的拓扑信息自定义每个节点的最佳传播深度，从而避免冗余特征传播。通过简单的超参数，可以灵活地管理准确性和延迟之间的权衡，以适应不同的延迟限制。此外，为了补偿损失的精度，我们还引入了一个补偿机制，该机制允许传播的层数超过所选择的深度，以提高精度。

    Graph neural networks (GNNs) have exhibited exceptional efficacy in a diverse array of applications. However, the sheer size of large-scale graphs presents a significant challenge to real-time inference with GNNs. Although existing Scalable GNNs leverage linear propagation to preprocess the features and accelerate the training and inference procedure, these methods still suffer from scalability issues when making inferences on unseen nodes, as the feature preprocessing requires the graph to be known and fixed. To further accelerate Scalable GNNs inference in this inductive setting, we propose an online propagation framework and two novel node-adaptive propagation methods that can customize the optimal propagation depth for each node based on its topological information and thereby avoid redundant feature propagation. The trade-off between accuracy and latency can be flexibly managed through simple hyper-parameters to accommodate various latency constraints. Moreover, to compensate for 
    
[^51]: 为什么学生会辍学？使用机器学习技术进行大学退学预测和相关因素分析

    Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques. (arXiv:2310.10987v1 [cs.LG])

    [http://arxiv.org/abs/2310.10987](http://arxiv.org/abs/2310.10987)

    本研究使用学术、人口统计、社会经济和宏观经济等不同类型的数据进行大学辍学预测，并发现学术数据是对模型性能最具影响力的数据类型。

    

    毕业率和辍学率一直是教育机构和学生严肃考虑的问题。高辍学率对个人学生和教育机构都有负面影响。为了解决这个问题，本研究使用学术、人口统计、社会经济和宏观经济等多种数据类型进行了大学辍学预测。此外，我们进行了相关因素分析，以分析哪种类型的数据对机器学习模型在预测毕业和辍学状态方面的性能最具影响力。这些特征被用于训练四个二元分类器，以确定学生是否会毕业或辍学。分类器在预测辍学状态方面的整体性能显示出平均ROC-AUC得分为0.935。在排除了所有与学术相关的特征后，平均ROC-AUC得分从0.935降至0.811，表明学术数据是对模型性能最具影响力的数据类型。

    Graduation and dropout rates have always been a serious consideration for educational institutions and students. High dropout rates negatively impact both the lives of individual students and institutions. To address this problem, this study examined university dropout prediction using academic, demographic, socioeconomic, and macroeconomic data types. Additionally, we performed associated factor analysis to analyze which type of data would be most influential on the performance of machine learning models in predicting graduation and dropout status. These features were used to train four binary classifiers to determine if students would graduate or drop out. The overall performance of the classifiers in predicting dropout status had an average ROC-AUC score of 0.935. The data type most influential to the model performance was found to be academic data, with the average ROC-AUC score dropping from 0.935 to 0.811 when excluding all academic-related features from the data set. Preliminary
    
[^52]: 精确非线性状态估计

    Exact nonlinear state estimation. (arXiv:2310.10976v1 [stat.ME])

    [http://arxiv.org/abs/2310.10976](http://arxiv.org/abs/2310.10976)

    本文引入了一种新的非线性估计理论，该理论试图弥合现有数据同化方法中的差距。具体而言，推导出了一个能够推广至任意非高斯分布的共轭变换滤波器 (CTF)，并提出了其集合近似版本 (ECTF)。

    

    地球科学中的大多数数据同化方法基于高斯假设。尽管这些假设方便了高效的算法，但它们会导致分析偏差和后续预测恶化。非参数、基于粒子的数据同化算法具有更高的准确性，但其在高维模型中的应用仍面临操作上的挑战。本文借鉴了生成人工智能领域的最新进展，提出了一种试图弥合数据同化方法中现有差距的新的非线性估计理论。具体而言，推导出了一个共轭变换滤波器 (CTF)，并显示其能够推广至任意非高斯分布。新的滤波器具有几个优点，例如能够保留先前状态中的统计关系并收敛至高精度的观测值。同时还提出了新理论的一个集合近似 (ECTF)。

    The majority of data assimilation (DA) methods in the geosciences are based on Gaussian assumptions. While these assumptions facilitate efficient algorithms, they cause analysis biases and subsequent forecast degradations. Non-parametric, particle-based DA algorithms have superior accuracy, but their application to high-dimensional models still poses operational challenges. Drawing inspiration from recent advances in the field of generative artificial intelligence (AI), this article introduces a new nonlinear estimation theory which attempts to bridge the existing gap in DA methodology. Specifically, a Conjugate Transform Filter (CTF) is derived and shown to generalize the celebrated Kalman filter to arbitrarily non-Gaussian distributions. The new filter has several desirable properties, such as its ability to preserve statistical relationships in the prior state and convergence to highly accurate observations. An ensemble approximation of the new theory (ECTF) is also presented and va
    
[^53]: 上下文感知元学习

    Context-Aware Meta-Learning. (arXiv:2310.10971v1 [cs.LG])

    [http://arxiv.org/abs/2310.10971](http://arxiv.org/abs/2310.10971)

    本文提出了一种上下文感知的元学习算法，可以在推理过程中学习新的视觉概念而无需微调。该方法在多个元学习基准中表现优异，超过或与目前的最先进算法相匹配。

    

    ChatGPT等大型语言模型展示了在推理过程中无需微调就能学习新概念的卓越能力。然而，用于推理过程中检测新对象的视觉模型尚未能够复制这种能力，而是表现糟糕或需要对类似对象进行元训练和/或微调。在这项工作中，我们提出了一种元学习算法，通过在推理过程中学习新的视觉概念而无需微调来模仿大型语言模型。我们的方法利用一个冻结的预训练特征提取器，并类似于上下文学习，将元学习重新定义为在已知标签的数据点和未知标签的测试数据点上的序列建模。在11个元学习基准中的8个中，我们的方法 - 无需元训练或微调 - 超过或与在这些基准上经过元训练的最先进算法P>M>F相匹配。

    Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.
    
[^54]: SD-PINN: 基于深度学习的空间相关偏微分方程恢复

    SD-PINN: Deep Learning based Spatially Dependent PDEs Recovery. (arXiv:2310.10970v1 [cs.LG])

    [http://arxiv.org/abs/2310.10970](http://arxiv.org/abs/2310.10970)

    SD-PINN是一种基于深度学习的方法，能够通过一个神经网络恢复空间相关的偏微分方程（PDE）系数，无需领域特定的物理专业知识，并且对噪声具有稳健性。同时，它能够通过空间变化低秩假设恢复没有可用测量数据的位置的系数。

    

    物理知识驱动的神经网络（PINN）能够直接从物理测量中恢复在整个空间域中保持不变的偏微分方程（PDE）系数。在本文中，我们提出了一种空间相关的物理知识驱动神经网络（SD-PINN），它通过一个单一的神经网络来恢复空间相关的PDE系数，消除了对领域特定的物理专业知识的要求。所提出的方法由于加入了物理约束而对噪声具有稳健性。它还能够将PDE系数的空间变化低秩假设纳入考虑，从而恢复没有可用测量数据的位置的系数。

    The physics-informed neural network (PINN) is capable of recovering partial differential equation (PDE) coefficients that remain constant throughout the spatial domain directly from physical measurements. In this work, we propose a spatially dependent physics-informed neural network (SD-PINN), which enables the recovery of coefficients in spatially-dependent PDEs using a single neural network, eliminating the requirement for domain-specific physical expertise. The proposed method exhibits robustness to noise owing to the incorporation of physical constraints. It can also incorporate the low-rank assumption of the spatial variation for the PDE coefficients to recover the coefficients at locations without available measurements.
    
[^55]: 具有延迟的神经网络模型用于求解绝对值方程

    The neural network models with delays for solving absolute value equations. (arXiv:2310.10965v1 [math.DS])

    [http://arxiv.org/abs/2310.10965](http://arxiv.org/abs/2310.10965)

    提出了一种具有延迟的无逆神经网络模型，用于求解绝对值方程，具有指数收敛性和解决一类特殊AVE的能力。

    

    提出了一种具有混合延迟的无逆神经网络模型，用于求解绝对值方程(AVE) Ax -|x| - b = 0，该模型包括一种具有离散延迟的无逆神经网络模型作为特例。通过使用Lyapunov-Krasovskii理论和线性矩阵不等式（LMI）方法，证明了所开发的神经网络模型对AVE的解具有指数收敛性。与现有用于求解AVE的神经网络模型相比，所提出的模型具有解决一类具有∥A^ -1 ∥>1- 的AVE的能力。给出了数值模拟结果，证明了两个延迟神经网络模型的有效性。

    An inverse-free neural network model with mixed delays is proposed for solving the absolute value equation (AVE) $Ax -|x| - b =0$, which includes an inverse-free neural network model with discrete delay as a special case. By using the Lyapunov-Krasovskii theory and the linear matrix inequality (LMI) method, the developed neural network models are proved to be exponentially convergent to the solution of the AVE. Compared with the existing neural network models for solving the AVE, the proposed models feature the ability of solving a class of AVE with $\|A^{-1}\|>1$. Numerical simulations are given to show the effectiveness of the two delayed neural network models.
    
[^56]: 通过线性预测提高深度神经网络的训练效率和性能

    Enhancing Deep Neural Network Training Efficiency and Performance through Linear Prediction. (arXiv:2310.10958v1 [cs.LG])

    [http://arxiv.org/abs/2310.10958](http://arxiv.org/abs/2310.10958)

    本文提出了一种通过线性预测来提高深度神经网络训练效率和性能的方法。实验结果表明，在相同的训练条件和时期下，通过采用该方法可以提高模型的性能。

    

    深度神经网络（DNN）在计算机视觉和自然语言处理等领域取得了显著的成功。然而，训练一个有效的DNN模型仍然面临着挑战。本文旨在提出一种优化DNN训练效果的方法，旨在提高模型性能。首先，根据观察到的DNN参数在训练过程中遵循某种规律的观察，发现了参数预测可以提高模型训练效率和性能的潜力。其次，考虑到DNN模型参数的数量级、硬件限制和随机梯度下降（SGD）对噪声容忍度的特性，采用参数线性预测（PLP）方法来进行DNN参数预测。最后，在一些代表性的骨架上进行验证。实验结果表明，在相同的训练条件和时期下，与正常的训练方式相比，通过采用所提出的方法，能够提高模型的性能。

    Deep neural networks (DNN) have achieved remarkable success in various fields, including computer vision and natural language processing. However, training an effective DNN model still poses challenges. This paper aims to propose a method to optimize the training effectiveness of DNN, with the goal of improving model performance. Firstly, based on the observation that the DNN parameters change in certain laws during training process, the potential of parameter prediction for improving model training efficiency and performance is discovered. Secondly, considering the magnitude of DNN model parameters, hardware limitations and characteristics of Stochastic Gradient Descent (SGD) for noise tolerance, a Parameter Linear Prediction (PLP) method is exploit to perform DNN parameter prediction. Finally, validations are carried out on some representative backbones. Experiment results show that compare to the normal training ways, under the same training conditions and epochs, by employing propo
    
[^57]: 一个用于数据集效果的状态向量框架

    A State-Vector Framework for Dataset Effects. (arXiv:2310.10955v1 [cs.CL])

    [http://arxiv.org/abs/2310.10955](http://arxiv.org/abs/2310.10955)

    本研究提出了一个状态向量框架，用于系统地研究数据集的效果。我们发现一些常用的语言理解数据集对模型有显著的效果，这些效果集中在几个语言维度上。此外，我们观察到数据集可能对模型的非相关维度产生"溢出"效应。这个框架为负责任和鲁棒模型开发中的数据集效果提供了一个系统的理解。

    

    近期基于深度神经网络（DNN）的系统的成功很大程度上受到了用于训练的高质量数据集的影响。然而，数据集的效果，特别是它们之间的相互作用，仍然不够深入研究。本文提出了一个状态向量框架，以便在这个方向上进行严格的研究。该框架将理想化探测测试结果作为向量空间的基础。该框架使我们能够量化独立和互动数据集的效果。我们发现一些常用的语言理解数据集的显著效果是特征性的，并且集中在几个语言维度上。此外，我们还观察到一些"溢出"效应：数据集可能会影响模型在看似与预期任务无关的维度上的表现。我们的状态向量框架为系统地理解数据集效果，这是负责任和鲁棒模型开发中的关键组成部分，铺平了道路。

    The impressive success of recent deep neural network (DNN)-based systems is significantly influenced by the high-quality datasets used in training. However, the effects of the datasets, especially how they interact with each other, remain underexplored. We propose a state-vector framework to enable rigorous studies in this direction. This framework uses idealized probing test results as the bases of a vector space. This framework allows us to quantify the effects of both standalone and interacting datasets. We show that the significant effects of some commonly-used language understanding datasets are characteristic and are concentrated on a few linguistic dimensions. Additionally, we observe some ``spill-over'' effects: the datasets could impact the models along dimensions that may seem unrelated to the intended tasks. Our state-vector framework paves the way for a systematic understanding of the dataset effects, a crucial component in responsible and robust model development.
    
[^58]: 基于局部图界限的采样型图神经网络的视角

    A Local Graph Limits Perspective on Sampling-Based GNNs. (arXiv:2310.10953v1 [cs.LG])

    [http://arxiv.org/abs/2310.10953](http://arxiv.org/abs/2310.10953)

    该论文提出了一种基于局部图界限的训练大型输入图的采样型图神经网络的理论框架，通过对小样本的训练，我们可以获得与整个图训练类似的结果。这为使用采样训练GNN提供了新的理论理解，并提供了在选择最佳模型、超参数和采样算法方面更高效的方法。

    

    我们提出了一个理论框架，通过对大型输入图中的小型固定大小的采样子图进行训练，来训练图神经网络（GNN）。该框架适用于各种模型，包括常用的基于采样的GNN，如GraphSAGE和FastGCN。借助图的局部界限理论，我们证明，在温和的假设下，通过对大型输入图的小样本进行采样训练的参数与在整个图上训练相同结构的参数在ε-邻域内。我们以ε的函数推导出样本数量、图的大小和训练步骤的界限。我们的结果为训练GNN时使用采样提供了一种新颖的理论理解。它们还暗示，通过对输入图的小样本进行训练，从业者可以更高效地识别和选择最佳模型、超参数和采样算法。我们通过实验证明了我们的结果。

    We propose a theoretical framework for training Graph Neural Networks (GNNs) on large input graphs via training on small, fixed-size sampled subgraphs. This framework is applicable to a wide range of models, including popular sampling-based GNNs, such as GraphSAGE and FastGCN. Leveraging the theory of graph local limits, we prove that, under mild assumptions, parameters learned from training sampling-based GNNs on small samples of a large input graph are within an $\epsilon$-neighborhood of the outcome of training the same architecture on the whole graph. We derive bounds on the number of samples, the size of the graph, and the training steps required as a function of $\epsilon$. Our results give a novel theoretical understanding for using sampling in training GNNs. They also suggest that by training GNNs on small samples of the input graph, practitioners can identify and select the best models, hyperparameters, and sampling algorithms more efficiently. We empirically illustrate our re
    
[^59]: 限制的Tweedie随机块模型

    Restricted Tweedie Stochastic Block Models. (arXiv:2310.10952v1 [stat.ML])

    [http://arxiv.org/abs/2310.10952](http://arxiv.org/abs/2310.10952)

    这项研究提出了一种新的随机块模型，可以处理由非负零膨胀连续边权组成的邻接矩阵，特别适用于模拟国际贸易网络。该模型结合了节点信息和动态效应，并且可以独立于社区标签进行参数估计。一个高效的两步算法被开发用于估计协变效应和社区标签。

    

    随机块模型 (SBM) 是在网络中进行社区检测的广泛应用框架，其中网络结构通常由邻接矩阵表示。然而，传统的SBM不能直接应用于由非负的零膨胀连续边权组成的邻接矩阵。为了模拟国际贸易网络，其中边权表示国家之间的贸易价值，我们提出了一种基于限制Tweedie分布的创新SBM。此外，我们还结合了节点信息，如国家之间的地理距离，并考虑其对边权的动态影响。值得注意的是，我们证明在节点数足够大的情况下，估计这个协变效应时，可以独立于每个节点的社区标签，在计算我们模型参数的最大似然估计器时。这个结果使得我们能够开发一种高效的两步算法，将协变效应的估计与社区标签的估计分离开来。

    The stochastic block model (SBM) is a widely used framework for community detection in networks, where the network structure is typically represented by an adjacency matrix. However, conventional SBMs are not directly applicable to an adjacency matrix that consists of non-negative zero-inflated continuous edge weights. To model the international trading network, where edge weights represent trading values between countries, we propose an innovative SBM based on a restricted Tweedie distribution. Additionally, we incorporate nodal information, such as the geographical distance between countries, and account for its dynamic effect on edge weights. Notably, we show that given a sufficiently large number of nodes, estimating this covariate effect becomes independent of community labels of each node when computing the maximum likelihood estimator of parameters in our model. This result enables the development of an efficient two-step algorithm that separates the estimation of covariate effe
    
[^60]: 通过协作解决城市拥堵：基于异构GNN的协调编队和交通信号控制的多智能体强化学习方法

    Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control. (arXiv:2310.10948v1 [cs.LG])

    [http://arxiv.org/abs/2310.10948](http://arxiv.org/abs/2310.10948)

    本文提出了一种基于异构图多智能体强化学习和交通理论的创新解决方案，通过将车辆编队和交通信号控制作为不同的强化学习智能体，并结合图神经网络实现协调，以优化交通流量和缓解城市拥堵。

    

    多年来，强化学习已经成为一种流行的方法，用于独立或分层方式开发信号控制和车辆编队策略。然而，在实时中联合控制这两者以减轻交通拥堵带来了新的挑战，如信号控制和编队之间固有的物理和行为异质性，以及它们之间的协调。本文提出了一种创新的解决方案来应对这些挑战，基于异构图多智能体强化学习和交通理论。我们的方法包括：1）将编队和信号控制设计为不同的强化学习智能体，具有自己的观测、动作和奖励函数，以优化交通流量；2）通过在多智能体强化学习中引入图神经网络来设计协调，以促进区域范围内智能体之间的无缝信息交换。我们通过SUMO模拟环境评估了我们的方法。

    Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale. We evaluate our approach through SUMO simu
    
[^61]: 多点反馈的带有硬约束的强盗凸优化

    Multi-point Feedback of Bandit Convex Optimization with Hard Constraints. (arXiv:2310.10946v1 [cs.LG])

    [http://arxiv.org/abs/2310.10946](http://arxiv.org/abs/2310.10946)

    本文研究了带有硬约束的强盗凸优化问题，并提出了一种基于惩罚的近端梯度下降方法，实现了次线性的遗憾和累积硬约束违反界。

    

    本文研究了带有约束的强盗凸优化问题，其中学习者旨在在局部损失函数信息下生成一系列决策，以同时降低累积损失和累积约束违反。我们采用累积“硬”约束违反作为约束违反的度量，其定义为$\sum_{t=1}^{T}\max\{g_t(\boldsymbol{x}_t), 0\}$。由于最大操作符的存在，相对于传统的“长期”约束违反度量，严格可行解无法消除违反约束的影响。我们提出了一种基于惩罚的近端梯度下降方法，该方法在遗憾和累积硬约束违反方面都实现了次线性增长，其中梯度估计使用了两个点的函数评估。准确地说，我们的算法实现了$O(d^2T^{\max\{c,1-c\}})$的遗憾界和$O(d^2T^{1-\frac{c}{2}})$的累积硬约束违反界。

    This paper studies bandit convex optimization with constraints, where the learner aims to generate a sequence of decisions under partial information of loss functions such that the cumulative loss is reduced as well as the cumulative constraint violation is simultaneously reduced. We adopt the cumulative \textit{hard} constraint violation as the metric of constraint violation, which is defined by $\sum_{t=1}^{T} \max\{g_t(\boldsymbol{x}_t), 0\}$. Owing to the maximum operator, a strictly feasible solution cannot cancel out the effects of violated constraints compared to the conventional metric known as \textit{long-term} constraints violation. We present a penalty-based proximal gradient descent method that attains a sub-linear growth of both regret and cumulative hard constraint violation, in which the gradient is estimated with a two-point function evaluation. Precisely, our algorithm attains $O(d^2T^{\max\{c,1-c\}})$ regret bounds and $O(d^2T^{1-\frac{c}{2}})$ cumulative hard constr
    
[^62]: 在自主赛车中达到极限: 最优控制与强化学习比较

    Reaching the Limit in Autonomous Racing: Optimal Control versus Reinforcement Learning. (arXiv:2310.10943v1 [cs.RO])

    [http://arxiv.org/abs/2310.10943](http://arxiv.org/abs/2310.10943)

    这项研究比较了最优控制 (OC)和强化学习 (RL)方法在自主无人机赛车中的效果，发现强化学习方法优于最优控制方法。研究表明，强化学习能够直接优化任务层面的目标，并利用领域的随机因素，而最优控制的分解限制了控制器的行为范围。

    

    机器人学中一个核心问题是如何为敏捷移动机器人设计控制系统。本文系统地研究了这个问题，重点是自主无人机赛车。我们发现，在这个设置下，使用强化学习(RL)训练的神经网络控制器胜过最优控制(OC)方法。然后我们调查了哪些基本因素对RL的成功或OC的限制有所贡献。我们的研究表明，RL相对于OC的根本优势不在于优化目标的效果更好，而是在于它优化了一个更好的目标。OC将问题分解为规划和控制，使用一个明确的中间表示，如轨迹，作为接口。这种分解限制了控制器可以表达的行为范围，当面临未建模的影响时，导致控制性能较差。相反，RL可以直接优化任务层面的目标，并且可以利用领域随机因素。

    A central question in robotics is how to design a control system for an agile mobile robot. This paper studies this question systematically, focusing on a challenging setting: autonomous drone racing. We show that a neural network controller trained with reinforcement learning (RL) outperformed optimal control (OC) methods in this setting. We then investigated which fundamental factors have contributed to the success of RL or have limited OC. Our study indicates that the fundamental advantage of RL over OC is not that it optimizes its objective better but that it optimizes a better objective. OC decomposes the problem into planning and control with an explicit intermediate representation, such as a trajectory, that serves as an interface. This decomposition limits the range of behaviors that can be expressed by the controller, leading to inferior control performance when facing unmodeled effects. In contrast, RL can directly optimize a task-level objective and can leverage domain rando
    
[^63]: MASON-NLP在eRisk 2023上的贡献：基于深度学习的社交媒体文本检测抑郁症状

    MASON-NLP at eRisk 2023: Deep Learning-Based Detection of Depression Symptoms from Social Media Texts. (arXiv:2310.10941v1 [cs.CL])

    [http://arxiv.org/abs/2310.10941](http://arxiv.org/abs/2310.10941)

    MASON-NLP提出了一种基于深度学习的方法，通过分析社交媒体文本检测抑郁症状。任务1的目标是评估不同条件的相关性。

    

    抑郁症是一种对人们生活产生深远影响的心理健康障碍。最近的研究表明，可以通过个体的交流方式（包括口头和书面文字）检测到抑郁症的迹象。特别是，社交媒体帖子是一个可供我们检查抑郁症状的丰富而方便的文本来源。贝克抑郁量表（BDI）问卷通常用于衡量抑郁症的严重程度，它是本研究中可以辅助使用的工具之一。我们可以将研究范围缩小到这些症状，因为每个BDI问题都与特定的抑郁症状相关联。需要记住，并非每个抑郁症患者同时表现出所有症状，而是其中的一些综合出现。因此，能确定一个句子或一段用户生成的内容与特定条件相关是非常有价值的。基于这一点，eRisk 2023任务1的设计目的正是为了评估不同条件的相关性。

    Depression is a mental health disorder that has a profound impact on people's lives. Recent research suggests that signs of depression can be detected in the way individuals communicate, both through spoken words and written texts. In particular, social media posts are a rich and convenient text source that we may examine for depressive symptoms. The Beck Depression Inventory (BDI) Questionnaire, which is frequently used to gauge the severity of depression, is one instrument that can aid in this study. We can narrow our study to only those symptoms since each BDI question is linked to a particular depressive symptom. It's important to remember that not everyone with depression exhibits all symptoms at once, but rather a combination of them. Therefore, it is extremely useful to be able to determine if a sentence or a piece of user-generated content is pertinent to a certain condition. With this in mind, the eRisk 2023 Task 1 was designed to do exactly that: assess the relevance of diffe
    
[^64]: 理论与实践中的快速简单谱聚类

    Fast and Simple Spectral Clustering in Theory and Practice. (arXiv:2310.10939v1 [cs.DS])

    [http://arxiv.org/abs/2310.10939](http://arxiv.org/abs/2310.10939)

    本文提出了一种快速简单的谱聚类算法，通过使用幂方法计算的少量向量进行顶点嵌入，几乎线性时间计算，能够可靠地恢复出真实聚类，并且在实验中比其他聚类算法更快，聚类准确性相当。

    

    谱聚类是一种流行且有效的算法，用于在图G中找到k个聚类。在传统的谱聚类算法中，通过计算图拉普拉斯矩阵的k个特征向量，将图G的顶点嵌入到R^k中。然而，计算这个嵌入是计算上昂贵的，并且支配着算法的运行时间。在本文中，我们提出了一种简单的谱聚类算法，该算法基于由幂方法计算的O(log(k))个向量的顶点嵌入。顶点嵌入的计算时间与图的大小几乎呈线性关系，并且在输入图的自然假设下，该算法可以可靠地恢复出真实的聚类。我们在几个合成数据集和真实世界数据集上评估了新算法，发现它比其他聚类算法快得多，同时产生的聚类准确性几乎相同。

    Spectral clustering is a popular and effective algorithm designed to find $k$ clusters in a graph $G$. In the classical spectral clustering algorithm, the vertices of $G$ are embedded into $\mathbb{R}^k$ using $k$ eigenvectors of the graph Laplacian matrix. However, computing this embedding is computationally expensive and dominates the running time of the algorithm. In this paper, we present a simple spectral clustering algorithm based on a vertex embedding with $O(\log(k))$ vectors computed by the power method. The vertex embedding is computed in nearly-linear time with respect to the size of the graph, and the algorithm provably recovers the ground truth clusters under natural assumptions on the input graph. We evaluate the new algorithm on several synthetic and real-world datasets, finding that it is significantly faster than alternative clustering algorithms, while producing results with approximately the same clustering accuracy.
    
[^65]: 基于家庭助手的意图检测和槽位填充：孟加拉语和锡尔赫蒂语的数据集和分析

    Intent Detection and Slot Filling for Home Assistants: Dataset and Analysis for Bangla and Sylheti. (arXiv:2310.10935v1 [cs.CL])

    [http://arxiv.org/abs/2310.10935](http://arxiv.org/abs/2310.10935)

    这项研究引入了第一个用于孟加拉语和锡尔赫蒂语的意图检测和槽位填充的全面数据集，并发现大型语言模型在处理不充足数据的下游任务上表现出强大能力。

    

    随着语音助手在我们高度技术化的社会中的地位得到巩固，有必要适应多样化的语言环境，包括低资源语言的口语形式。我们的研究引入了首个用于正式孟加拉语、口语孟加拉语和锡尔赫蒂语的意图检测和槽位填充的全面数据集，总共包含10个唯一意图的984个样本。我们的分析揭示了大型语言模型在处理不充足数据的下游任务上的强大能力。在口语孟加拉语的意图检测中，GPT-3.5模型达到了令人印象深刻的0.94的F1得分，在槽位填充中达到了0.51的F1得分。

    As voice assistants cement their place in our technologically advanced society, there remains a need to cater to the diverse linguistic landscape, including colloquial forms of low-resource languages. Our study introduces the first-ever comprehensive dataset for intent detection and slot filling in formal Bangla, colloquial Bangla, and Sylheti languages, totaling 984 samples across 10 unique intents. Our analysis reveals the robustness of large language models for tackling downstream tasks with inadequate data. The GPT-3.5 model achieves an impressive F1 score of 0.94 in intent detection and 0.51 in slot filling for colloquial Bangla.
    
[^66]: 在初级卫生保健中使用音频数据促进抑郁风险评估

    Using Audio Data to Facilitate Depression Risk Assessment in Primary Health Care. (arXiv:2310.10928v1 [cs.HC])

    [http://arxiv.org/abs/2310.10928](http://arxiv.org/abs/2310.10928)

    本研究旨在使用音频数据预测初级卫生保健中的抑郁风险，以减少误诊并改善整体诊断和治疗结果。

    

    电子健康是初级卫生保健中的有价值工具，抑郁症是常见的病症。初级卫生保健是大多数抑郁症患者的首要联系点，但约有25%的初级卫生保健医生的诊断不准确。许多其他障碍也妨碍了初级卫生保健中的抑郁症检测和治疗。人工智能（AI）可能有助于减少抑郁症在初级卫生保健中的误诊，并改善整体诊断和治疗结果。电子健康咨询常常出现视频问题，如连接不良或通话中断。对于可能缺乏稳定的互联网连接的低收入患者而言，仅音频的电子健康更实用。因此，我们的研究专注于使用音频数据来预测抑郁风险。目标是：1）收集24人的音频数据（12人患有抑郁症，12人没有心理健康或重大健康问题的诊断）；2）构建一个机器学习模型来预测抑郁风险。使用了自动机器学习工具TPOT来选择最佳的机器学习算法。

    Telehealth is a valuable tool for primary health care (PHC), where depression is a common condition. PHC is the first point of contact for most people with depression, but about 25% of diagnoses made by PHC physicians are inaccurate. Many other barriers also hinder depression detection and treatment in PHC. Artificial intelligence (AI) may help reduce depression misdiagnosis in PHC and improve overall diagnosis and treatment outcomes. Telehealth consultations often have video issues, such as poor connectivity or dropped calls. Audio-only telehealth is often more practical for lower-income patients who may lack stable internet connections. Thus, our study focused on using audio data to predict depression risk. The objectives were to: 1) Collect audio data from 24 people (12 with depression and 12 without mental health or major health condition diagnoses); 2) Build a machine learning model to predict depression risk. TPOT, an autoML tool, was used to select the best machine learning algo
    
[^67]: 量子时代的机器学习：量子与经典支持向量机比较

    Machine Learning in the Quantum Age: Quantum vs. Classical Support Vector Machines. (arXiv:2310.10910v1 [cs.LG])

    [http://arxiv.org/abs/2310.10910](http://arxiv.org/abs/2310.10910)

    本研究比较了经典和量子计算范式中机器学习算法的效果，发现量子支持向量机（QSVM）在某些场景下可以达到与经典支持向量机（SVM）相媲美的准确率，但执行时间较长。同时指出，增加量子计算能力和并行度可以显著改善量子机器学习算法的性能。

    

    本研究旨在对比经典和量子计算范式中机器学习算法的效果。特别是，通过强调支持向量机（SVM），我们审视经典SVM和在量子硬件上运行的量子支持向量机（QSVM）在鸢尾花数据集上的分类能力。采用基于Qiskit库的广泛实验范围和超参数优化的方法。研究结果发现，在特定场景中，QSVM可以达到与经典SVM相媲美的准确率，尽管执行时间目前较长。此外，我们强调，增加量子计算能力和并行度可以显著改善量子机器学习算法的性能。这项研究提供了关于量子时代下机器学习应用的现状和未来潜力的宝贵见解。

    This work endeavors to juxtapose the efficacy of machine learning algorithms within classical and quantum computational paradigms. Particularly, by emphasizing on Support Vector Machines (SVM), we scrutinize the classification prowess of classical SVM and Quantum Support Vector Machines (QSVM) operational on quantum hardware over the Iris dataset. The methodology embraced encapsulates an extensive array of experiments orchestrated through the Qiskit library, alongside hyperparameter optimization. The findings unveil that in particular scenarios, QSVMs extend a level of accuracy that can vie with classical SVMs, albeit the execution times are presently protracted. Moreover, we underscore that augmenting quantum computational capacity and the magnitude of parallelism can markedly ameliorate the performance of quantum machine learning algorithms. This inquiry furnishes invaluable insights regarding the extant scenario and future potentiality of machine learning applications in the quantum
    
[^68]: 异构内存增强神经网络

    Heterogenous Memory Augmented Neural Networks. (arXiv:2310.10909v1 [cs.LG])

    [http://arxiv.org/abs/2310.10909](http://arxiv.org/abs/2310.10909)

    本文介绍了一种新颖的异构内存增强方法，通过引入具有注意机制的可学习内存令牌，可以在不增加巨大计算开销的情况下有效提升性能。

    

    已经证明，半参数方法，将标准神经网络与非参数组件（如外部存储模块和数据检索）相结合，在数据稀缺和超出分布范围的情况下尤其有帮助。然而，现有的半参数方法主要依赖于独立的原始数据点——这种策略在大规模计算成本和当前注意机制处理大量标记的能力方面都很困难。在本文中，我们引入了一种新颖的异构内存增强方法，该方法通过引入具有注意机制的可学习内存令牌，可以在不增加巨大计算开销的情况下有效提升性能。我们的通用方法可以与各种骨干网络（MLP、CNN、GNN和Transformer）无缝结合，以插入和播放的方式进行。我们广泛评估了我们的方法在各种基于图像和图的任务下在分布内（ID）和分布外条件下的表现。

    It has been shown that semi-parametric methods, which combine standard neural networks with non-parametric components such as external memory modules and data retrieval, are particularly helpful in data scarcity and out-of-distribution (OOD) scenarios. However, existing semi-parametric methods mostly depend on independent raw data points - this strategy is difficult to scale up due to both high computational costs and the incapacity of current attention mechanisms with a large number of tokens. In this paper, we introduce a novel heterogeneous memory augmentation approach for neural networks which, by introducing learnable memory tokens with attention mechanism, can effectively boost performance without huge computational overhead. Our general-purpose method can be seamlessly combined with various backbones (MLP, CNN, GNN, and Transformer) in a plug-and-play manner. We extensively evaluate our approach on various image and graph-based tasks under both in-distribution (ID) and OOD condi
    
[^69]: 自发性模块化结构：密集预训练Transformer能否从自发模块化结构中获益？

    Emergent Mixture-of-Experts: Can Dense Pre-trained Transformers Benefit from Emergent Modular Structures?. (arXiv:2310.10908v1 [cs.LG])

    [http://arxiv.org/abs/2310.10908](http://arxiv.org/abs/2310.10908)

    该论文研究了密集预训练Transformer是否以及如何从自发的模块化结构中获益。

    

    将模块化设计引入神经网络能够展示出较好的泛化能力和学习效率等优点。现有的模块化神经网络通常是“显式”的，因为它们的模块化架构是预先定义的，每个模块都被期望实现不同的功能。相反，最近的研究表明在标准的预训练Transformer中存在“隐式”的模块化结构，即“自发模块化”。他们表明这样的模块化结构在早期预训练阶段就会出现，并且完全是自发的。然而，大多数Transformer模型仍然被视为单体模型，没有充分利用其模块化的特性。因此，鉴于显式模块化架构的优良特性，我们探索了密集预训练Transformer是否以及如何从自发模块化结构中获益的问题。

    Incorporating modular designs into neural networks demonstrates superior out-of-generalization, learning efficiency, etc. Existing modular neural networks are generally $\textit{explicit}$ because their modular architectures are pre-defined, and individual modules are expected to implement distinct functions. Conversely, recent works reveal that there exist $\textit{implicit}$ modular structures in standard pre-trained transformers, namely $\textit{Emergent Modularity}$. They indicate that such modular structures exhibit during the early pre-training phase and are totally spontaneous. However, most transformers are still treated as monolithic models with their modular natures underutilized. Therefore, given the excellent properties of explicit modular architecture, we explore $\textit{whether and how dense pre-trained transformers can benefit from emergent modular structures.}$ To study this question, we construct \textbf{E}mergent $\textbf{M}$ixture-$\textbf{o}$f-$\textbf{E}$xperts (E
    
[^70]: 针对跳跃不连续函数的替代主动子空间

    Surrogate Active Subspaces for Jump-Discontinuous Functions. (arXiv:2310.10907v1 [stat.ML])

    [http://arxiv.org/abs/2310.10907](http://arxiv.org/abs/2310.10907)

    该论文提出了一种针对不连续函数的替代主动子空间方法，扩展了活跃子空间的应用范围，并通过数值实验验证了该方法的有效性。

    

    替代建模和活跃子空间已经成为计算科学和工程领域的强大范例。将这些技术应用于社会科学中的计算模型，突显了它们在处理离散输出的Agent-Based模型等不连续模拟器时的局限性。然而，之前的应用研究已经表明，对于这类估计器，替代计算的活跃子空间可以产生有趣的结果。但是，由于活跃子空间是通过梯度定义的，当将该方法应用于不连续模拟器时，估计的是什么量还不清楚。本文首先展示了进行此类分析时可能出现的一些病态情况。这促使我们将活跃子空间扩展到不连续函数上，澄清了在此类分析中实际估计的内容。我们还对合成测试函数进行了数值实验，比较了活跃子空间的高斯过程估计。

    Surrogate modeling and active subspaces have emerged as powerful paradigms in computational science and engineering. Porting such techniques to computational models in the social sciences brings into sharp relief their limitations in dealing with discontinuous simulators, such as Agent-Based Models, which have discrete outputs. Nevertheless, prior applied work has shown that surrogate estimates of active subspaces for such estimators can yield interesting results. But given that active subspaces are defined by way of gradients, it is not clear what quantity is being estimated when this methodology is applied to a discontinuous simulator. We begin this article by showing some pathologies that can arise when conducting such an analysis. This motivates an extension of active subspaces to discontinuous functions, clarifying what is actually being estimated in such analyses. We also conduct numerical experiments on synthetic test functions to compare Gaussian process estimates of active sub
    
[^71]: 通过子网络注入归纳偏置

    Instilling Inductive Biases with Subnetworks. (arXiv:2310.10899v1 [cs.LG])

    [http://arxiv.org/abs/2310.10899](http://arxiv.org/abs/2310.10899)

    通过子网络注入归纳偏置，这项研究探索了理解和控制神经网络行为的方法。通过发现功能子网络并利用它们，可以显著减少训练模型所需的数据量。

    

    尽管人工神经网络在各种任务上取得了最近的成功，但对于这些模型的精确解决方案，我们几乎没有知识或控制能力。注入归纳偏置--对一些解决方案偏好--是理解和控制这些模型行为的一个有前景的途径。已经进行了大量工作来研究模型固有的归纳偏置，并通过手动设计的结构或精心策划的训练方式注入不同的归纳偏置。在这项工作中，我们探索了一种更机械的方法：子任务归纳。我们的方法发现了一个在训练模型中实现特定子任务的功能子网络，并使用它来注入对利用该子任务的解决方案的归纳偏置。子任务归纳灵活高效，在两个实验中我们证明了它的有效性。

    Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to a
    
[^72]: 对于社区检测中模块化最大化在近似、启发式和图神经网络算法中的分析

    Analyzing Modularity Maximization in Approximation, Heuristic, and Graph Neural Network Algorithms for Community Detection. (arXiv:2310.10898v1 [cs.SI])

    [http://arxiv.org/abs/2310.10898](http://arxiv.org/abs/2310.10898)

    这项研究分析了不同的模块化最大化算法对于社区检测中实现最优划分的性能，并发现最常用的模块化方法与最优划分之间存在显著差异。

    

    社区检测是计算科学中的一个基本问题，在各个领域都有应用。通常通过最大化一个目标函数（模块性）在网络节点的划分上来检测社区。我们的研究探讨了不同模块化最大化算法在实现最优划分方面的性能。我们使用了104个网络，包括来自不同背景的真实世界实例和具有模块化结构的合成图。我们分析了十种近似模块化算法，对比了一种精确的基准线方法，该方法是一种精确的整数规划方法，可以全局优化模块性。分析的十种算法包括八个启发式算法，两种图神经网络算法的变种，以及几种Bayan近似算法的变种。我们的分析揭示了最常用的模块化方法得到的划分与网络的最优划分之间的显著差异，如下所示。

    Community detection, a fundamental problem in computational sciences, finds applications in various domains. Heuristics are often employed to detect communities through maximizing an objective function, modularity, over partitions of network nodes. Our research delves into the performance of different modularity maximization algorithms in achieving optimal partitions. We use 104 networks, comprising real-world instances from diverse contexts and synthetic graphs with modular structures. We analyze ten inexact modularity-based algorithms against an exact baseline which is an exact integer programming method that globally optimizes modularity. The ten algorithms analyzed include eight heuristics, two variations of a graph neural network algorithm, and several variations of the Bayan approximation algorithm. Our analysis uncovers substantial dissimilarities between the partitions obtained by most commonly used modularity-based methods and any optimal partition of the networks, as indicate
    
[^73]: 高效TCR-表位结合亲和力预测的主动学习框架

    Active Learning Framework for Cost-Effective TCR-Epitope Binding Affinity Prediction. (arXiv:2310.10893v1 [q-bio.QM])

    [http://arxiv.org/abs/2310.10893](http://arxiv.org/abs/2310.10893)

    本研究提出了一个高效的TCR-表位结合亲和力预测的主动学习框架ActiveTCR，通过最小化注释成本并最大化性能提升，有效降低了预测过程的成本。

    

    T细胞受体（TCR）是适应性免疫系统的关键组成部分，负责通过识别寄主细胞表面呈现的表位序列来应对威胁。最近，使用机器学习/深度学习来预测TCR和表位序列之间的结合亲和力引起了广泛关注。然而，其成功受到缺乏大量已注释的TCR-表位对的限制。注释它们的结合亲和力需要昂贵且耗时的湿实验评估。为了降低注释成本，我们提出了ActiveTCR，一个结合主动学习和TCR-表位结合亲和力预测模型的框架。ActiveTCR从一小组标记的训练对开始，迭代地搜索值得注释的未标记TCR-表位对。它旨在最大化性能提升，同时最小化注释成本。我们将四种查询策略与随机抽样基准进行了比较，并证明ActiveTCR能够降低注释成本。

    T cell receptors (TCRs) are critical components of adaptive immune systems, responsible for responding to threats by recognizing epitope sequences presented on host cell surface. Computational prediction of binding affinity between TCRs and epitope sequences using machine/deep learning has attracted intense attention recently. However, its success is hindered by the lack of large collections of annotated TCR-epitope pairs. Annotating their binding affinity requires expensive and time-consuming wet-lab evaluation. To reduce annotation cost, we present ActiveTCR, a framework that incorporates active learning and TCR-epitope binding affinity prediction models. Starting with a small set of labeled training pairs, ActiveTCR iteratively searches for unlabeled TCR-epitope pairs that are ''worth'' for annotation. It aims to maximize performance gains while minimizing the cost of annotation. We compared four query strategies with a random sampling baseline and demonstrated that ActiveTCR reduce
    
[^74]: Calysto Scheme项目

    The Calysto Scheme Project. (arXiv:2310.10886v1 [cs.PL])

    [http://arxiv.org/abs/2310.10886](http://arxiv.org/abs/2310.10886)

    Calysto Scheme是一个将Scheme转换为Python的项目，支持标准Scheme功能和Python库的互操作。它被广泛应用于教育和教学，且已被成功整合入Jupyter Notebook。

    

    Calysto Scheme是用Continuation-Passing Style编写的Scheme，经过一系列保持正确性的程序转换后转化为Python。它支持标准Scheme功能，包括call/cc，以及语法扩展、用于自动回溯的非确定性操作符和许多扩展，以便与Python进行互操作。由于其基于Python，它可以利用现代Python库，包括用于机器学习和其他教学环境的库。尽管Calysto Scheme的开发目的是为了教育，但由于其简单性和易安装性，它被证明在一般情况下也很有用。它已经集成到Jupyter Notebook生态系统中，并在教室中用于教授编程语言入门课程，添加了一些有趣且独特的元素。

    Calysto Scheme is written in Scheme in Continuation-Passing Style, and converted through a series of correctness-preserving program transformations into Python. It has support for standard Scheme functionality, including call/cc, as well as syntactic extensions, a nondeterministic operator for automatic backtracking, and many extensions to allow Python interoperation. Because of its Python foundation, it can take advantage of modern Python libraries, including those for machine learning and other pedagogical contexts. Although Calysto Scheme was developed with educational purposes in mind, it has proven to be generally useful due to its simplicity and ease of installation. It has been integrated into the Jupyter Notebook ecosystem and used in the classroom to teach introductory Programming Languages with some interesting and unique twists.
    
[^75]: BLoad：增强神经网络训练的高效顺序数据处理方法

    BLoad: Enhancing Neural Network Training with Efficient Sequential Data Handling. (arXiv:2310.10879v1 [cs.LG])

    [http://arxiv.org/abs/2310.10879](http://arxiv.org/abs/2310.10879)

    本论文提出了一种名为BLoad的训练方案，通过最小化填充量并实现高效的分布式数据并行训练，来提高训练效率和召回率。

    

    随着现代深度神经网络模型的复杂性不断增加和数据集的扩大，需要开发优化且可扩展的训练方法。本白皮书中，我们解决了使用不同大小的序列进行神经网络模型训练的高效性挑战。为了解决这个问题，我们提出了一种新的训练方案，能够在序列的分布式数据并行训练中实现高效处理，同时还能最小化额外开销。通过使用这个方案，我们能够将填充量减少超过100倍，同时不删除任何帧，从而在实验证明了总体上增加了训练时间和召回率。

    The increasing complexity of modern deep neural network models and the expanding sizes of datasets necessitate the development of optimized and scalable training methods. In this white paper, we addressed the challenge of efficiently training neural network models using sequences of varying sizes. To address this challenge, we propose a novel training scheme that enables efficient distributed data-parallel training on sequences of different sizes with minimal overhead. By using this scheme we were able to reduce the padding amount by more than 100$x$ while not deleting a single frame, resulting in an overall increased performance on both training time and Recall in our experiments.
    
[^76]: 使用基于神经网络的滚动展开方法控制连接和自动化车辆的生态驾驶

    Eco-Driving Control of Connected and Automated Vehicles using Neural Network based Rollout. (arXiv:2310.10878v1 [cs.LG])

    [http://arxiv.org/abs/2310.10878](http://arxiv.org/abs/2310.10878)

    通过神经网络实现的滚动展开方法可以控制连接和自动化车辆的生态驾驶，解决了现有方法中高计算和内存需求的问题，并实现了与强化学习相当的性能。

    

    连接和自动驾驶车辆通过使用车到一切信息来优化车辆速度和动力传动系统动力学，具有最小化能源消耗的潜力。现有的确定性和随机方法旨在解决生态驾驶问题，通常存在计算和内存需求较高的问题，这使得在线实施具有挑战性。本文提出了一种分层多视角优化框架，通过神经网络实现。神经网络学习全程价值函数以解决路径信息的变异性，然后用于近似滚动视界优化中的终端成本。在现实世界的路线模拟中，证明了该方法与通过强化学习获得的随机优化解具有可比性，同时不需要复杂的训练范式和可忽略的车载内存。

    Connected and autonomous vehicles have the potential to minimize energy consumption by optimizing the vehicle velocity and powertrain dynamics with Vehicle-to-Everything info en route. Existing deterministic and stochastic methods created to solve the eco-driving problem generally suffer from high computational and memory requirements, which makes online implementation challenging.  This work proposes a hierarchical multi-horizon optimization framework implemented via a neural network. The neural network learns a full-route value function to account for the variability in route information and is then used to approximate the terminal cost in a receding horizon optimization. Simulations over real-world routes demonstrate that the proposed approach achieves comparable performance to a stochastic optimization solution obtained via reinforcement learning, while requiring no sophisticated training paradigm and negligible on-board memory.
    
[^77]: 二十一世纪的宗教归属：基于世界价值观调查的机器学习视角

    Religious Affiliation in the Twenty-First Century: A Machine Learning Perspective on the World Value Survey. (arXiv:2310.10874v1 [cs.LG])

    [http://arxiv.org/abs/2310.10874](http://arxiv.org/abs/2310.10874)

    这篇研究基于世界价值观调查数据，应用机器学习方法研究了宗教信仰、价值观和行为的变化趋势。研究结果表明，在大多数国家中，年龄和收入是影响宗教性的最重要因素。

    

    本文是对世界价值观调查全球数据的定量分析研究。利用随机森林，我们旨在识别宗教性的关键因素，并使用国家级数据将调查中的受访者分为宗教和非宗教。我们使用重采样技术平衡数据，并提高不平衡学习性能指标。变量重要性分析的结果表明，在大多数国家中，年龄和收入是最重要的变量。通过与宗教和人类行为的基本社会学理论相结合，讨论了结果。这项研究是机器学习在参与世界价值观调查的30个国家的数据中识别潜在模式的应用。

    This paper is a quantitative analysis of the data collected globally by the World Value Survey. The data is used to study the trajectories of change in individuals' religious beliefs, values, and behaviors in societies. Utilizing random forest, we aim to identify the key factors of religiosity and classify respondents of the survey as religious and non religious using country level data. We use resampling techniques to balance the data and improve imbalanced learning performance metrics. The results of the variable importance analysis suggest that Age and Income are the most important variables in the majority of countries. The results are discussed with fundamental sociological theories regarding religion and human behavior. This study is an application of machine learning in identifying the underlying patterns in the data of 30 countries participating in the World Value Survey. The results from variable importance analysis and classification of imbalanced data provide valuable insigh
    
[^78]: 使用多智能体深度强化学习在信号化道路网络中联合优化交通信号控制和车辆路径规划

    Joint Optimization of Traffic Signal Control and Vehicle Routing in Signalized Road Networks using Multi-Agent Deep Reinforcement Learning. (arXiv:2310.10856v1 [eess.SY])

    [http://arxiv.org/abs/2310.10856](http://arxiv.org/abs/2310.10856)

    本文提出了一种在信号化道路网络中，使用多智能体深度强化学习联合优化交通信号控制和车辆路径规划的方法，通过建立相关性和共享观测和奖励来促进代理之间的交互和合作，从而提高网络性能。

    

    城市交通拥堵是困扰现代道路网络的严重问题。为了减轻这个问题并提高交通效率，交通信号控制和车辆路径规划已被证明是有效的措施。本文提出了一种在信号化道路网络中联合优化交通信号控制和车辆路径规划的方法。其目标是通过使用多智能体深度强化学习（MADRL）同时控制信号时序和路径选择来提高网络性能。通过建立代理之间的相关性并使其共享观测和奖励，促进代理之间的交互和合作，从而增强个体的训练。采用多智能体优势演员-评论员算法来处理多智能体环境，使用深度神经网络（DNN）

    Urban traffic congestion is a critical predicament that plagues modern road networks. To alleviate this issue and enhance traffic efficiency, traffic signal control and vehicle routing have proven to be effective measures. In this paper, we propose a joint optimization approach for traffic signal control and vehicle routing in signalized road networks. The objective is to enhance network performance by simultaneously controlling signal timings and route choices using Multi-Agent Deep Reinforcement Learning (MADRL). Signal control agents (SAs) are employed to establish signal timings at intersections, whereas vehicle routing agents (RAs) are responsible for selecting vehicle routes. By establishing relevance between agents and enabling them to share observations and rewards, interaction and cooperation among agents are fostered, which enhances individual training. The Multi-Agent Advantage Actor-Critic algorithm is used to handle multi-agent environments, and Deep Neural Network (DNN) s
    
[^79]: CoTFormer：更多的关注令牌弥补了更少的深度

    CoTFormer: More Tokens With Attention Make Up For Less Depth. (arXiv:2310.10845v1 [cs.CL])

    [http://arxiv.org/abs/2310.10845](http://arxiv.org/abs/2310.10845)

    CoTFormer是一种transformer变体，通过使用隐含的链思考机制，实现了与更深模型相当的容量，并且在实证中显著优于更大的标准transformers。

    

    持续发展越来越大和更深的基础模型的竞赛正在进行中。然而，像链思考（CoT）方法这样的技术在实现最佳下游性能方面仍起着重要作用。在这项工作中，我们建立了使用链思考和使用更深的transformer之间的近似平行关系。基于这一洞见，我们引入了CoTFormer，一种使用隐含链思考机制来实现与更深模型相当容量的transformer变体。我们的实证发现证明了CoTFormer的有效性，因为它们明显优于更大的标准transformers。

    The race to continually develop ever larger and deeper foundational models is underway. However, techniques like the Chain-of-Thought (CoT) method continue to play a pivotal role in achieving optimal downstream performance. In this work, we establish an approximate parallel between using chain-of-thought and employing a deeper transformer. Building on this insight, we introduce CoTFormer, a transformer variant that employs an implicit CoT-like mechanism to achieve capacity comparable to a deeper model. Our empirical findings demonstrate the effectiveness of CoTFormers, as they significantly outperform larger standard transformers.
    
[^80]: 对大型语言模型的恶意攻击所揭示的漏洞调查

    Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. (arXiv:2310.10844v1 [cs.CL])

    [http://arxiv.org/abs/2310.10844](http://arxiv.org/abs/2310.10844)

    本论文调查了对大型语言模型进行恶意攻击的研究，发现即使经过安全调整的模型也容易受到攻击。这些攻击利用弱点并误导AI系统，对于复杂系统的攻击尤为明显。

    

    大型语言模型（LLMs）在体系结构和能力方面迅速发展，随着它们在复杂系统中的深入整合，审查其安全性变得更加紧迫。本文调查了对LLMs进行恶意攻击的新兴跨学科领域的研究，该领域是可信任的机器学习的一个子领域，结合自然语言处理和安全性的观点。先前的研究表明，即使是经过安全调整的LLMs（通过指导调整和通过人类反馈进行加强学习）也可能受到恶意攻击的影响，这些攻击利用弱点并误导人工智能系统，如ChatGPT和Bard等模型的“越狱”攻击的普遍存在证明了这一点。在本调查中，我们首先概述了大型语言模型，描述了它们的安全对齐，并根据各种学习结构对现有研究进行分类：仅文本攻击、多模态攻击以及专门针对复杂系统的其他攻击方法。

    Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as
    
[^81]: 通过使用高斯混合模型和蒙特卡洛自回归流进行概率分类的密度估计

    Probabilistic Classification by Density Estimation Using Gaussian Mixture Model and Masked Autoregressive Flow. (arXiv:2310.10843v1 [stat.ML])

    [http://arxiv.org/abs/2310.10843](http://arxiv.org/abs/2310.10843)

    本研究提出了一种使用密度估计进行概率分类的方法，通过使用高斯混合模型和蒙特卡洛自回归流对数据的似然进行建模，并展示了这种方法优于传统的分类器。这项工作为基于联合密度估计的其他概率分类器的提出开辟了新的研究方向。

    

    密度估计是一类重要的概率机器学习问题，它用于估计数据的分布。其中一类密度估计器是混合模型，如通过期望最大化得到的高斯混合模型（GMM）。另一类密度估计器是生成模型，它们从输入的潜变量生成数据。其中一种生成模型是蒙特卡洛自回归流（MAF），它利用归一化流和自回归网络。本文中，我们将密度估计器用于分类，尽管它们通常用于估计数据的分布。我们使用密度估计器（具体来说是GMM和MAF）对数据的类别的似然进行建模。所提出的分类器优于仅使用单个高斯分布对似然进行建模的较简单的分类器，如线性判别分析。这项工作为提出基于联合密度估计的其他概率分类器开辟了研究空间。

    Density estimation, which estimates the distribution of data, is an important category of probabilistic machine learning. A family of density estimators is mixture models, such as Gaussian Mixture Model (GMM) by expectation maximization. Another family of density estimators is the generative models which generate data from input latent variables. One of the generative models is the Masked Autoregressive Flow (MAF) which makes use of normalizing flows and autoregressive networks. In this paper, we use the density estimators for classification, although they are often used for estimating the distribution of data. We model the likelihood of classes of data by density estimation, specifically using GMM and MAF. The proposed classifiers outperform simpler classifiers such as linear discriminant analysis which model the likelihood using only a single Gaussian distribution. This work opens the research door for proposing other probabilistic classifiers based on joint density estimation.
    
[^82]: 一种基于机器学习的算法用于检测传感器数据时间序列中基于频率的事件

    A Machine Learning-based Algorithm for Automated Detection of Frequency-based Events in Recorded Time Series of Sensor Data. (arXiv:2310.10841v1 [cs.LG])

    [http://arxiv.org/abs/2310.10841](http://arxiv.org/abs/2310.10841)

    这项工作提出了一种新的基于机器学习的算法，用于在传感器数据的时间序列中自动检测基于频率的事件。这种方法通过将时间序列数据映射到时间-频率域中的表示，并使用目标检测模型来提高事件的识别准确性。

    

    自动化事件检测已经成为通过传感器数据监控技术系统行为的基本实践之一。在汽车行业中，这些方法在追踪时间序列数据中的事件方面需求量很大。为了评估主动车辆安全系统，进行了各种各样的驾驶场景。这些场景涉及使用外部传感器记录车辆行为，以评估实际性能。在这种情况下，自动化检测方法不仅加快了评估速度，还通过避免数据检查中主观的、基于人的评价来标准化和客观化评估。本研究提出了一种新的事件检测方法，可以在时间序列数据中识别基于频率的事件。为此，时间序列数据被映射到时间-频率域中的表示，即scalogram。在过滤scalogram以增强信号的相关部分之后，使用目标检测模型检测事件。

    Automated event detection has emerged as one of the fundamental practices to monitor the behavior of technical systems by means of sensor data. In the automotive industry, these methods are in high demand for tracing events in time series data. For assessing the active vehicle safety systems, a diverse range of driving scenarios is conducted. These scenarios involve the recording of the vehicle's behavior using external sensors, enabling the evaluation of operational performance. In such setting, automated detection methods not only accelerate but also standardize and objectify the evaluation by avoiding subjective, human-based appraisals in the data inspection. This work proposes a novel event detection method that allows to identify frequency-based events in time series data. To this aim, the time series data is mapped to representations in the time-frequency domain, known as scalograms. After filtering scalograms to enhance relevant parts of the signal, an object detection model is 
    
[^83]: 用于高效Transformer的近似两层前馈网络

    Approximating Two-Layer Feedforward Networks for Efficient Transformers. (arXiv:2310.10837v1 [cs.LG])

    [http://arxiv.org/abs/2310.10837](http://arxiv.org/abs/2310.10837)

    本论文介绍了一种用于高效Transformer的近似两层前馈网络方法，通过稀疏的专家混合模型和产品-键存储实现资源高效的大型语言模型，与其他方法相比具有竞争力，并在参数相等的条件下展示了其在不同规模数据集上的优势。

    

    如何在不牺牲性能的情况下减少神经网络(NNs)的计算和存储需求？许多最近的研究使用稀疏的专家混合模型(MoEs)构建资源高效的大型语言模型(LMs)。在这里，我们介绍了关于MoEs的几个新颖观点，提出了一个将各种方法统一起来以近似两层NNs(例如Transformer的前馈块)的通用框架，包括产品-键存储(PKMs)。借助这个框架的见解，我们提出了改进MoEs和PKMs的方法。与之前在计算相等条件下比较MoEs与密集基准的工作不同，我们的评估条件是参数相等，这对于正确评估LMs至关重要。我们展示了我们的MoEs在WikiText-103和enwiki8数据集的两个不同规模上与密集的Transformer-XL相竞争，同时资源效率更高。这证明MoEs不仅适用于超大型LMs，也适用于任何规模的资源-

    How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-
    
[^84]: 基于高斯过程的数据增强和期望签名应用于时间序列分类

    Gaussian processes based data augmentation and expected signature for time series classification. (arXiv:2310.10836v1 [cs.LG])

    [http://arxiv.org/abs/2310.10836](http://arxiv.org/abs/2310.10836)

    该论文提出了一种基于高斯过程的数据增强和期望签名的时间序列特征提取模型，并通过有监督的任务学习到了最佳的特征提取方法。

    

    签名是描述路径（即从区间到欧几里得空间的连续函数）的基本对象。同样，期望签名提供了随机过程的统计描述。我们提出了一个基于期望签名的时间序列特征提取模型。该模型通过基于高斯过程的数据增强计算得到。一个主要特点是通过使用该模型的有监督任务学习到了最佳的特征提取方法。

    The signature is a fundamental object that describes paths (that is, continuous functions from an interval to a Euclidean space). Likewise, the expected signature provides a statistical description of the law of stochastic processes. We propose a feature extraction model for time series built upon the expected signature. This is computed through a Gaussian processes based data augmentation. One of the main features is that an optimal feature extraction is learnt through the supervised task that uses the model.
    
[^85]: 用基于得分的生成先验的可证明的概率成像

    Provable Probabilistic Imaging using Score-Based Generative Priors. (arXiv:2310.10835v1 [eess.IV])

    [http://arxiv.org/abs/2310.10835](http://arxiv.org/abs/2310.10835)

    本文提出了一种基于得分的生成先验的插入式蒙特卡洛算法，能够实现高质量图像重建和不确定性量化。

    

    在解决反问题时，估计高质量图像并量化其不确定性是图像重建算法中的两个理想特点。本文提出了插入式蒙特卡洛（PMC）作为一种对一般反问题可能解空间进行建模的原则性框架。PMC能够通过后验采样来结合丰富的基于得分的生成先验进行高质量图像重建，并进行不确定性量化。具体而言，我们引入了两种PMC算法，可以视为传统插入式先验（PnP）和去噪正则化（RED）算法的采样模拟。我们还建立了对PMC算法收敛性的理论分析。我们的分析为两种算法提供了非渐近稳定性保证，即使在非对数凹似然和不完美得分网络的情况下也是如此。

    Estimating high-quality images while also quantifying their uncertainty are two desired features in an image reconstruction algorithm for solving ill-posed inverse problems. In this paper, we propose plug-and-play Monte Carlo (PMC) as a principled framework for characterizing the space of possible solutions to a general inverse problem. PMC is able to incorporate expressive score-based generative priors for high-quality image reconstruction while also performing uncertainty quantification via posterior sampling. In particular, we introduce two PMC algorithms which can be viewed as the sampling analogues of the traditional plug-and-play priors (PnP) and regularization by denoising (RED) algorithms. We also establish a theoretical analysis for characterizing the convergence of the PMC algorithms. Our analysis provides non-asymptotic stationarity guarantees for both algorithms, even in the presence of non-log-concave likelihoods and imperfect score networks. We demonstrate the performance
    
[^86]: 适当的Laplacian表示学习

    Proper Laplacian Representation Learning. (arXiv:2310.10833v1 [cs.LG])

    [http://arxiv.org/abs/2310.10833](http://arxiv.org/abs/2310.10833)

    本论文介绍了一种理论上可靠的方法和优化算法，用于近似Laplacian表示学习，以解决大规模强化学习中的探索、泛化和传递问题。

    

    在解决大规模强化学习问题时，学习状态的良好表示对于探索、泛化和传递是至关重要的。Laplacian表示是一种有希望的方法，通过引入内在奖励来解决这些问题，以实现时间延长的动作发现和奖励塑造，以及信息丰富的状态编码。为了获得Laplacian表示，需要计算图Laplacian的特征系统，这通常通过与深度学习方法兼容的优化目标进行近似。然而，这些近似方法依赖于无法高效调整的超参数，收敛到所需特征向量的任意旋转，并且无法精确地恢复相应的特征值。本文提出了一种理论上可靠的目标和相应的优化算法，用于近似Laplacian表示。

    The ability to learn good representations of states is essential for solving large reinforcement learning problems, where exploration, generalization, and transfer are particularly challenging. The Laplacian representation is a promising approach to address these problems by inducing intrinsic rewards for temporally-extended action discovery and reward shaping, and informative state encoding. To obtain the Laplacian representation one needs to compute the eigensystem of the graph Laplacian, which is often approximated through optimization objectives compatible with deep learning approaches. These approximations, however, depend on hyperparameters that are impossible to tune efficiently, converge to arbitrary rotations of the desired eigenvectors, and are unable to accurately recover the corresponding eigenvalues. In this paper we introduce a theoretically sound objective and corresponding optimization algorithm for approximating the Laplacian representation. Our approach naturally reco
    
[^87]: 准确的基于数据驱动的动力系统代理模型用于不确定性正向传播

    Accurate Data-Driven Surrogates of Dynamical Systems for Forward Propagation of Uncertainty. (arXiv:2310.10831v1 [math.NA])

    [http://arxiv.org/abs/2310.10831](http://arxiv.org/abs/2310.10831)

    本文介绍了一种准确的基于数据驱动的动力系统代理模型方法，通过将随机选取近似应用于模型的动力学来构建代理模型。这种方法能够减小近似系统轨迹和状态变量误差。

    

    随机选取（SC）是一种构建代理模型用于不确定性量化的非侵入式方法。在动力系统中，SC特别适用于完全场不确定性传播，其描绘了具有随机输入参数的模型主要解场的分布。然而，由于即使在最简单的动力系统中，参数到解的映射具有高度非线性的特性，构建的SC代理模型通常不准确。本文提出了一种替代方法，即将SC近似应用于模型的动力学而非解。通过将数据驱动的稀疏非线性动力学（SINDy）框架与SC结合，我们构建动力学代理模型，并通过时间进行积分以构建代理解。我们证明了SC-over-dynamics框架在近似系统轨迹以及状态变量误差方面都具有更小的误差。

    Stochastic collocation (SC) is a well-known non-intrusive method of constructing surrogate models for uncertainty quantification. In dynamical systems, SC is especially suited for full-field uncertainty propagation that characterizes the distributions of the high-dimensional primary solution fields of a model with stochastic input parameters. However, due to the highly nonlinear nature of the parameter-to-solution map in even the simplest dynamical systems, the constructed SC surrogates are often inaccurate. This work presents an alternative approach, where we apply the SC approximation over the dynamics of the model, rather than the solution. By combining the data-driven sparse identification of nonlinear dynamics (SINDy) framework with SC, we construct dynamics surrogates and integrate them through time to construct the surrogate solutions. We demonstrate that the SC-over-dynamics framework leads to smaller errors, both in terms of the approximated system trajectories as well as the 
    
[^88]: 利用混合模型基于后继特征强化学习跨任务传递的不确定性感知方法

    Uncertainty-aware transfer across tasks using hybrid model-based successor feature reinforcement learning. (arXiv:2310.10818v1 [cs.LG])

    [http://arxiv.org/abs/2310.10818](http://arxiv.org/abs/2310.10818)

    该论文提出了一种利用混合模型基于后继特征强化学习方法，能够在具有不同转移动力学和奖励函数的任务之间实现样本高效的不确定性感知知识传递。

    

    对于复杂和大规模的决策问题，样本效率对于开发实用的强化学习（RL）至关重要。将来自先前经验的知识转移和泛化到下游任务能够显著提高样本效率。最近的研究表明，后继特征（SF）RL算法能够在具有不同奖励但相同转移动力学的任务之间实现知识泛化。最近提出结合模型基于（MB）方法和SF算法可以缓解固定转移动力学的限制。此外，不确定性感知的探索方法被广泛认为是提高样本效率的另一种吸引人的方法。将混合模型基于后继特征（MB-SF）和不确定性的两个思想结合起来，提出了一种解决跨任务样本高效不确定性感知知识传递问题的方法。

    Sample efficiency is central to developing practical reinforcement learning (RL) for complex and large-scale decision-making problems. The ability to transfer and generalize knowledge gained from previous experiences to downstream tasks can significantly improve sample efficiency. Recent research indicates that successor feature (SF) RL algorithms enable knowledge generalization between tasks with different rewards but identical transition dynamics. It has recently been hypothesized that combining model-based (MB) methods with SF algorithms can alleviate the limitation of fixed transition dynamics. Furthermore, uncertainty-aware exploration is widely recognized as another appealing approach for improving sample efficiency. Putting together two ideas of hybrid model-based successor feature (MB-SF) and uncertainty leads to an approach to the problem of sample efficient uncertainty-aware knowledge transfer across tasks with different transition dynamics or/and reward functions. In this pa
    
[^89]: 鲁棒多智能体强化学习: 对抗性规范化的理论基础和稳定算法

    Robust Multi-Agent Reinforcement Learning via Adversarial Regularization: Theoretical Foundation and Stable Algorithms. (arXiv:2310.10810v1 [cs.LG])

    [http://arxiv.org/abs/2310.10810](http://arxiv.org/abs/2310.10810)

    本文提出了一种新的鲁棒多智能体强化学习框架ERNIE，通过对抗性规范化促进策略的Lipschitz连续性，以提高鲁棒性和对抗噪声观察、转换动态的变化和智能体的恶意行为。

    

    多智能体强化学习在多个领域都显示出良好的结果。然而，由于对环境的微小变化敏感，多智能体强化学习策略往往缺乏鲁棒性。这对于在现实世界中部署多智能体强化学习算法构成了严重问题，因为测试环境可能与训练环境略有不同。本文通过控制策略的Lipschitz常数来提高鲁棒性，并在温和条件下证明了Lipschitz且接近最优策略的存在。基于这些见解，我们提出了一个新的鲁棒多智能体强化学习框架ERNIE，通过对抗性规范化促进策略对于状态观察和动作的Lipschitz连续性。ERNIE框架对于噪声观察、转换动态的变化和智能体的恶意行为具有鲁棒性。然而，ERNIE的对抗性规范化可能引入一些训练不稳定性。

    Multi-Agent Reinforcement Learning (MARL) has shown promising results across several domains. Despite this promise, MARL policies often lack robustness and are therefore sensitive to small changes in their environment. This presents a serious concern for the real world deployment of MARL algorithms, where the testing environment may slightly differ from the training environment. In this work we show that we can gain robustness by controlling a policy's Lipschitz constant, and under mild conditions, establish the existence of a Lipschitz and close-to-optimal policy. Based on these insights, we propose a new robust MARL framework, ERNIE, that promotes the Lipschitz continuity of the policies with respect to the state observations and actions by adversarial regularization. The ERNIE framework provides robustness against noisy observations, changing transition dynamics, and malicious actions of agents. However, ERNIE's adversarial regularization may introduce some training instability. To 
    
[^90]: 对对抗训练线性回归的正则化性质的研究

    Regularization properties of adversarially-trained linear regression. (arXiv:2310.10807v1 [stat.ML])

    [http://arxiv.org/abs/2310.10807](http://arxiv.org/abs/2310.10807)

    本研究对对抗训练线性回归的正则化性质进行了研究，发现在过参数化情况下，对抗训练可以得到最小范数插值解，这一发现对理解对抗训练的效果和应用具有重要意义。

    

    最先进的机器学习模型对于由对手构造的非常小的输入扰动可能存在漏洞。对抗训练是一种有效的防御方法。它将问题建模为一个极小极大问题，在训练数据受到最坏情况攻击时寻找最佳解决方案。线性模型是可以观察到漏洞的简单模型，也是我们研究的重点。在这种情况下，对抗训练导致一个凸优化问题，可以形式化为有限和的最小化。我们对线性回归中对抗训练的解与其他正则化方法进行了比较分析。我们的主要发现是：（A）只要最大扰动半径小于阈值，对抗训练可以得到在过参数化情况下（参数数目大于数据数目）的最小范数插值解；相反，最小范数插值器就是通过对抗训练得到的解。

    State-of-the-art machine learning models can be vulnerable to very small input perturbations that are adversarially constructed. Adversarial training is an effective approach to defend against it. Formulated as a min-max problem, it searches for the best solution when the training data were corrupted by the worst-case attacks. Linear models are among the simple models where vulnerabilities can be observed and are the focus of our study. In this case, adversarial training leads to a convex optimization problem which can be formulated as the minimization of a finite sum. We provide a comparative analysis between the solution of adversarial training in linear regression and other regularization methods. Our main findings are that: (A) Adversarial training yields the minimum-norm interpolating solution in the overparameterized regime (more parameters than data), as long as the maximum disturbance radius is smaller than a threshold. And, conversely, the minimum-norm interpolator is the solu
    
[^91]: 用于糖尿病视网膜病变特征提取和分类的卷积神经网络模型

    Convolutional Neural Network Model for Diabetic Retinopathy Feature Extraction and Classification. (arXiv:2310.10806v1 [eess.IV])

    [http://arxiv.org/abs/2310.10806](http://arxiv.org/abs/2310.10806)

    通过卷积神经网络模型识别糖尿病视网膜病变，提供准确的诊断结果，模型更易解释和抗过拟合，准确率达到71%。推动了糖尿病视网膜病变检测领域的发展。

    

    在医疗市场上应用人工智能引起了越来越多的关注，但它在像糖尿病视网膜病变这样的悄无声息的疾病的及时诊断方面提供了帮助。为了诊断糖尿病视网膜病变（DR），眼科医生使用视网膜后面的彩色底片照片来通过一个困难而耗时的过程识别小的鲜明特征。我们的工作通过卷积神经网络模型创建了一个新颖的模型，并通过底片图像输入来识别DR的严重程度。我们通过卷积层对4种已知的DR特征进行分类，包括微小动脉瘤、棉丝斑、渗出物和出血，在不需要额外用户输入的情况下能够提供准确的诊断。所提出的模型更易解释和抗过拟合。我们展示了初步结果，敏感性为97%，准确率为71%。我们的贡献是提出了一个可解释性较强且准确率与更复杂模型相当的模型。因此，我们的模型推动了DR检测领域的发展。

    The application of Artificial Intelligence in the medical market brings up increasing concerns but aids in more timely diagnosis of silent progressing diseases like Diabetic Retinopathy. In order to diagnose Diabetic Retinopathy (DR), ophthalmologists use color fundus images, or pictures of the back of the retina, to identify small distinct features through a difficult and time-consuming process. Our work creates a novel CNN model and identifies the severity of DR through fundus image input. We classified 4 known DR features, including micro-aneurysms, cotton wools, exudates, and hemorrhages, through convolutional layers and were able to provide an accurate diagnostic without additional user input. The proposed model is more interpretable and robust to overfitting. We present initial results with a sensitivity of 97% and an accuracy of 71%. Our contribution is an interpretable model with similar accuracy to more complex models. With that, our model advances the field of DR detection an
    
[^92]: 神经切向核函数为具有交叉协方差图的图神经网络提供了动力学

    Neural Tangent Kernels Motivate Graph Neural Networks with Cross-Covariance Graphs. (arXiv:2310.10791v1 [cs.LG])

    [http://arxiv.org/abs/2310.10791](http://arxiv.org/abs/2310.10791)

    本文研究了神经切向核函数（NTKs）在图神经网络（GNNs）中的应用。我们发现优化对齐等价于优化GNN中的图表示或图移位运算符，并建立了对于两层GNN对齐的最优性的理论保证。

    

    神经切向核函数（NTKs）提供了分析过参数化神经网络的学习和泛化行为的理论基础。对于有监督学习任务，NTK核函数的特征向量与给定数据之间的关联（在本文中称为对齐）可以控制梯度下降的收敛速度以及对未见数据的泛化能力。在这个概念的基础上，我们研究了NTKs和对齐在图神经网络（GNNs）的背景下的应用，我们的分析揭示了优化对齐等价于优化GNN中的图表示或图移位运算符。我们的结果进一步建立了对于两层GNN对齐的最优性的理论保证，这些保证由图移位运算符作为输入和输出数据之间的交叉协方差函数的函数所决定。通过对NTKs的分析得出的理论洞察力，通过我们的实验证实了这些洞察力。

    Neural tangent kernels (NTKs) provide a theoretical regime to analyze the learning and generalization behavior of over-parametrized neural networks. For a supervised learning task, the association between the eigenvectors of the NTK kernel and given data (a concept referred to as alignment in this paper) can govern the rate of convergence of gradient descent, as well as generalization to unseen data. Building upon this concept, we investigate NTKs and alignment in the context of graph neural networks (GNNs), where our analysis reveals that optimizing alignment translates to optimizing the graph representation or the graph shift operator in a GNN. Our results further establish the theoretical guarantees on the optimality of the alignment for a two-layer GNN and these guarantees are characterized by the graph shift operator being a function of the cross-covariance between the input and the output data. The theoretical insights drawn from the analysis of NTKs are validated by our experime
    
[^93]: 从统计学角度揭开中毒后门攻击的神秘面纱

    Demystifying Poisoning Backdoor Attacks from a Statistical Perspective. (arXiv:2310.10780v1 [cs.CR])

    [http://arxiv.org/abs/2310.10780](http://arxiv.org/abs/2310.10780)

    从统计学角度揭开中毒后门攻击的神秘面纱，通过评估任何包含恒定触发器的后门攻击的有效性，确定了后门攻击成功的决定因素、最有效的攻击方向以及几乎不可察觉的人类触发器何时会成功。

    

    在现实世界中，对机器学习的依赖日益增长，强调了理解和确保其安全性的重要性。中毒后门攻击由于其隐蔽性和潜在的严重后果而构成了重大的安全风险。这类攻击涉及将触发器嵌入学习模型中，以在存在活动触发器时引起恶意行为，同时在没有触发器的情况下维持正常功能。本文通过为受损模型在清洁和后门测试数据上的性能建立严格的下限和上限，评估了任何包含恒定触发器的后门攻击的有效性。所开发的理论回答了一系列基本但以前未被充分探索的问题，包括（1）后门攻击成功的决定因素是什么，（2）最有效的后门攻击方向是什么，以及（3）几乎不可察觉的人类触发器何时会成功。我们得到的理解...

    The growing dependence on machine learning in real-world applications emphasizes the importance of understanding and ensuring its safety. Backdoor attacks pose a significant security risk due to their stealthy nature and potentially serious consequences. Such attacks involve embedding triggers within a learning model with the intention of causing malicious behavior when an active trigger is present while maintaining regular functionality without it. This paper evaluates the effectiveness of any backdoor attack incorporating a constant trigger, by establishing tight lower and upper boundaries for the performance of the compromised model on both clean and backdoor test data. The developed theory answers a series of fundamental but previously underexplored problems, including (1) what are the determining factors for a backdoor attack's success, (2) what is the direction of the most effective backdoor attack, and (3) when will a human-imperceptible trigger succeed. Our derived understandin
    
[^94]: 是否有特洛伊木马！: 对基于机器学习的现代物联网环境下最新入侵检测系统的文献调查和批判性评估

    Is there a Trojan! : Literature survey and critical evaluation of the latest ML based modern intrusion detection systems in IoT environments. (arXiv:2310.10778v1 [cs.CR])

    [http://arxiv.org/abs/2310.10778](http://arxiv.org/abs/2310.10778)

    本文调查和评估了基于机器学习的现代物联网环境下的入侵检测系统，并发现尽管已存在高准确性的机器学习算法，但缺乏生产级模型。

    

    近年来，物联网作为一个领域已经在数据量和网络安全威胁方面与移动网络环境相媲美。物联网环境中的数据机密性和隐私已经成为安全研究的重要领域。越来越多的安全专家对设计健壮的入侵检测系统以保护物联网环境表示兴趣，作为传统安全方法的补充。由于物联网设备资源受限，并且具有异构的协议栈，大多数传统的入侵检测方法在这些架构限制下效果不佳。这促使安全研究人员创新地将机器学习和入侵检测系统结合起来，以解决非学习型入侵检测系统在物联网生态系统中的缺点。尽管各种机器学习算法已经在物联网数据集上取得了较高的准确性，但我们还缺乏足够的生产级模型。本文对现有的基于机器学习的物联网入侵检测系统进行了综述和评估。

    IoT as a domain has grown so much in the last few years that it rivals that of the mobile network environments in terms of data volumes as well as cybersecurity threats. The confidentiality and privacy of data within IoT environments have become very important areas of security research within the last few years. More and more security experts are interested in designing robust IDS systems to protect IoT environments as a supplement to the more traditional security methods. Given that IoT devices are resource-constrained and have a heterogeneous protocol stack, most traditional intrusion detection approaches don't work well within these schematic boundaries. This has led security researchers to innovate at the intersection of Machine Learning and IDS to solve the shortcomings of non-learning based IDS systems in the IoT ecosystem.  Despite various ML algorithms already having high accuracy with IoT datasets, we can see a lack of sufficient production grade models. This survey paper det
    
[^95]: 纠正物理信息神经网络（PINNs）中模型错误规范的方法

    Correcting model misspecification in physics-informed neural networks (PINNs). (arXiv:2310.10776v1 [cs.LG])

    [http://arxiv.org/abs/2310.10776](http://arxiv.org/abs/2310.10776)

    本论文提出了一种纠正物理信息神经网络（PINNs）中模型错误规范的通用方法，用于发现控制方程，并通过稀疏和/或噪声数据实现了校正。

    

    在计算科学中，通过数据驱动发现控制方程已经成为获得准确物理模型的新范式，并可能成为理论推导的可能替代方法。最近开发的物理信息神经网络（PINNs）已经被用于学习各种科学领域中的控制方程。尽管PINNs在发现控制方程方面非常有效，但是PINNs中编码的物理模型在复杂系统中可能存在严重的错误规范，因为某些物理过程可能并没有完全被理解，导致PINN预测的准确性较差。在这项工作中，我们提出了一种纠正PINNs中错误规范的物理模型的通用方法，为了通过一些稀疏和/或噪声数据来发现控制方程。具体来说，我们首先对假设的物理模型进行编码，然后使用其他深度神经网络（DNNs）来建模不完美模型与观测之间的差异。

    Data-driven discovery of governing equations in computational science has emerged as a new paradigm for obtaining accurate physical models and as a possible alternative to theoretical derivations. The recently developed physics-informed neural networks (PINNs) have also been employed to learn governing equations given data across diverse scientific disciplines. Despite the effectiveness of PINNs for discovering governing equations, the physical models encoded in PINNs may be misspecified in complex systems as some of the physical processes may not be fully understood, leading to the poor accuracy of PINN predictions. In this work, we present a general approach to correct the misspecified physical models in PINNs for discovering governing equations, given some sparse and/or noisy data. Specifically, we first encode the assumed physical models, which may be misspecified, then employ other deep neural networks (DNNs) to model the discrepancy between the imperfect models and the observatio
    
[^96]: 《必须安全：一种新的分子设计框架》

    Gotta be SAFE: A New Framework for Molecular Design. (arXiv:2310.10773v1 [cs.LG])

    [http://arxiv.org/abs/2310.10773](http://arxiv.org/abs/2310.10773)

    SAFE is a novel line notation for chemical structures that reimagines SMILES strings as an unordered sequence of interconnected fragment blocks, streamlining complex generative tasks and facilitating fragment-constrained design without the need for intricate decoding or graph-based models. It has been demonstrated to be effective through extensive experimentation.

    

    传统的分子字符串表示，如SMILES，在基于人工智能的分子设计中经常面临问题，因为它们以非连续的方式描述了分子的亚结构。为了解决这个问题，我们引入了基于顺序连接的片段嵌入（SAFE）——一种新颖的化学结构线性符号表示法。SAFE将SMILES字符串重新构想为一个无序的互连片段块序列，同时与现有的SMILES解析器完全兼容。它简化了复杂的生成任务，包括骨架装饰、片段连接、聚合物生成和骨架跳跃，同时促进了受片段约束的设计的自回归生成，从而消除了繁琐的解码或基于图的模型的需求。我们通过在包含11亿个SAFE表示的数据集上训练了一个8700万参数的类GPT2模型，展示了SAFE的有效性。通过广泛的实验，我们展示了我们的SAFE-GPT模型展示了多功能的...

    Traditional molecular string representations, such as SMILES, often pose challenges for AI-driven molecular design due to their non-sequential depiction of molecular substructures. To address this issue, we introduce Sequential Attachment-based Fragment Embedding (SAFE), a novel line notation for chemical structures. SAFE reimagines SMILES strings as an unordered sequence of interconnected fragment blocks while maintaining full compatibility with existing SMILES parsers. It streamlines complex generative tasks, including scaffold decoration, fragment linking, polymer generation, and scaffold hopping, while facilitating autoregressive generation for fragment-constrained design, thereby eliminating the need for intricate decoding or graph-based models. We demonstrate the effectiveness of SAFE by training an 87-million-parameter GPT2-like model on a dataset containing 1.1 billion SAFE representations. Through extensive experimentation, we show that our SAFE-GPT model exhibits versatile an
    
[^97]: 通过语义压缩实现无监督的主旋律生成

    Unsupervised Lead Sheet Generation via Semantic Compression. (arXiv:2310.10772v1 [cs.SD])

    [http://arxiv.org/abs/2310.10772](http://arxiv.org/abs/2310.10772)

    该论文提出了无监督的主旋律生成方法，通过语义压缩将完整分谱转换为主旋律谱单。引入了一种新的模型Lead-AE来实现这一目标。

    

    主旋律谱单已经在生成音乐研究中变得常见，被用作下游任务（如多轨音乐生成和自动编曲）的初步压缩表示。尽管如此，研究人员在寻找配对的主旋律谱单和完整分谱时通常会回到确定性的降维方法（如天际线算法）来生成主旋律谱单，很少关注主旋律谱单本身的质量以及它们如何准确反映编曲后的对应版本。为了解决这些问题，我们提出了条件主旋律谱单生成的问题（即在给定其完整分谱版本的情况下生成主旋律谱单），并展示这一任务可以被构建为无监督的音乐压缩任务，其中主旋律谱单代表了分谱的压缩潜在版本。我们引入了一种新的模型，称为Lead-AE，它将主旋律谱单建模为原始序列的离散子选择，使用可微分的top-k操作。

    Lead sheets have become commonplace in generative music research, being used as an initial compressed representation for downstream tasks like multitrack music generation and automatic arrangement. Despite this, researchers have often fallen back on deterministic reduction methods (such as the skyline algorithm) to generate lead sheets when seeking paired lead sheets and full scores, with little attention being paid toward the quality of the lead sheets themselves and how they accurately reflect their orchestrated counterparts. To address these issues, we propose the problem of conditional lead sheet generation (i.e. generating a lead sheet given its full score version), and show that this task can be formulated as an unsupervised music compression task, where the lead sheet represents a compressed latent version of the score. We introduce a novel model, called Lead-AE, that models the lead sheets as a discrete subselection of the original sequence, using a differentiable top-k operato
    
[^98]: 广义神经网络作为高斯过程：来自深度平衡模型的启示

    Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models. (arXiv:2310.10767v1 [cs.LG])

    [http://arxiv.org/abs/2310.10767](http://arxiv.org/abs/2310.10767)

    本文研究了神经网络中广义神经网络和高斯过程的对应关系，发现具有无限深度层并且宽度趋近于无穷大的神经网络收敛于高斯过程，揭示了广义神经网络的良性过拟合现象。

    

    具有宽度层的神经网络由于与高斯过程的等价性而受到极大关注，在保持泛化性能的同时完美拟合训练数据，这被称为良性过拟合。然而，现有的结果主要集中在浅层或有限深度的网络上，需要对具有无限深度层的广义神经网络进行全面分析，例如神经常微分方程(ODE)和深度平衡模型(DEQ)。在本文中，我们特别研究了深度平衡模型(DEQ)，它是一个具有共享权重矩阵的无限深度神经网络。我们的分析揭示了当DEQ层的宽度趋近于无穷大时，它收敛到一个高斯过程，从而建立了所谓的神经网络与高斯过程(NNGP)的对应关系。值得注意的是，即使深度和宽度的极限互换，在典型的无限深度多层网络中也不会观察到这种收敛。

    Neural networks with wide layers have attracted significant attention due to their equivalence to Gaussian processes, enabling perfect fitting of training data while maintaining generalization performance, known as benign overfitting. However, existing results mainly focus on shallow or finite-depth networks, necessitating a comprehensive analysis of wide neural networks with infinite-depth layers, such as neural ordinary differential equations (ODEs) and deep equilibrium models (DEQs). In this paper, we specifically investigate the deep equilibrium model (DEQ), an infinite-depth neural network with shared weight matrices across layers. Our analysis reveals that as the width of DEQ layers approaches infinity, it converges to a Gaussian process, establishing what is known as the Neural Network and Gaussian Process (NNGP) correspondence. Remarkably, this convergence holds even when the limits of depth and width are interchanged, which is not observed in typical infinite-depth Multilayer 
    
[^99]: 探索基于超弹性材料模型的人脑皮层的发现：多元分析与人工神经网络方法的比较

    Exploring hyperelastic material model discovery for human brain cortex: multivariate analysis vs. artificial neural network approaches. (arXiv:2310.10762v1 [cs.LG])

    [http://arxiv.org/abs/2310.10762](http://arxiv.org/abs/2310.10762)

    本研究通过比较人工神经网络和多元回归方法，探索了适用于人脑组织的最佳本构材料模型。结果表明，人工神经网络能够自动识别准确的本构模型。

    

    传统的计算方法，如有限元分析，为揭示脑部物理行为的潜在机制提供了有价值的见解。然而，准确预测脑部物理性质需要有效的本构模型来表示复杂的脑组织力学特性。在本研究中，我们旨在确定最适合人脑组织的本构材料模型。为了实现这一目标，我们将人工神经网络和多元回归方法应用于一般化的广泛接受的经典模型，并比较了这两种方法得到的结果。为了评估模型的适用性和效果，除了防止潜在的过拟合的方法外，两种方法在所有设置上保持一致。我们的结果表明，人工神经网络能够从给定的可接受的估计器中自动识别准确的本构模型。尽管如此，五项和两项……

    Traditional computational methods, such as the finite element analysis, have provided valuable insights into uncovering the underlying mechanisms of brain physical behaviors. However, precise predictions of brain physics require effective constitutive models to represent the intricate mechanical properties of brain tissue. In this study, we aimed to identify the most favorable constitutive material model for human brain tissue. To achieve this, we applied artificial neural network and multiple regression methods to a generalization of widely accepted classic models, and compared the results obtained from these two approaches. To evaluate the applicability and efficacy of the model, all setups were kept consistent across both methods, except for the approach to prevent potential overfitting. Our results demonstrate that artificial neural networks are capable of automatically identifying accurate constitutive models from given admissible estimators. Nonetheless, the five-term and two-ter
    
[^100]: 统计二元估计的仿射等变性的统计障碍

    Statistical Barriers to Affine-equivariant Estimation. (arXiv:2310.10758v1 [math.ST])

    [http://arxiv.org/abs/2310.10758](http://arxiv.org/abs/2310.10758)

    本研究调查了对于鲁棒均值估计的仿射等变性估计器的数量化性能，并发现仿射等变性会导致恢复误差严重恶化，速率降低一个因子$\sqrt{d}$。传统估计器不是最优的或缺乏量化保证，而具有量化保证的最新估计器不具有仿射等变性或需要额外的分布条件。

    

    我们研究了对于鲁棒均值估计而言仿射等变性估计器的数量化性能。作为一个自然的稳定性要求，这类仿射等变性估计器的构建已经在统计文献中广泛研究。我们在两种最近广泛研究的异常值模型下定量评估了这些估计器：重尾和对抗性破坏设置。我们建立了下界，显示仿射等变性会导致恢复误差严重恶化，定量速率降低一个因子$\sqrt{d}$。我们发现，在仿射等变性估计器的类别中，经典估计器如Tukey中位数（Tukey '75）和Stahel-Donoho估计器（Stahel '81和Donoho '82）在数量化方面要么不是最优的，要么缺乏任何数量化的保证。另一方面，具有强大量化保证的最新估计器要么不具有仿射等变性，要么需要额外的分布条件。

    We investigate the quantitative performance of affine-equivariant estimators for robust mean estimation. As a natural stability requirement, the construction of such affine-equivariant estimators has been extensively studied in the statistics literature. We quantitatively evaluate these estimators under two outlier models which have been the subject of much recent work: the heavy-tailed and adversarial corruption settings. We establish lower bounds which show that affine-equivariance induces a strict degradation in recovery error with quantitative rates degrading by a factor of $\sqrt{d}$ in both settings. We find that classical estimators such as the Tukey median (Tukey '75) and Stahel-Donoho estimator (Stahel '81 and Donoho '82) are either quantitatively sub-optimal even within the class of affine-equivariant estimators or lack any quantitative guarantees. On the other hand, recent estimators with strong quantitative guarantees are not affine-equivariant or require additional distrib
    
[^101]: 3D心脏图像分割的深层条件形状模型

    Deep Conditional Shape Models for 3D cardiac image segmentation. (arXiv:2310.10756v1 [eess.IV])

    [http://arxiv.org/abs/2310.10756](http://arxiv.org/abs/2310.10756)

    本论文提出了一种使用深层条件形状模型的算法，可以用于3D心脏图像分割。通过学习与模态无关的形状模型和模态相关的细化网络，该算法可以高效地获得心脏左心室的分割结果。

    

    解剖结构的确定通常是许多医学图像分析工作流的第一步。虽然卷积神经网络可以实现高性能，但这些网络没有结合解剖形状信息。我们介绍了一种新的分割算法，使用深层条件形状模型（DCSMs）作为核心组件。通过使用深层隐式形状表示，该算法学习到了一个与模态无关的形状模型，可以为任何感兴趣的解剖特征生成有符号距离函数。为了使生成的形状拟合图像，形状模型是根据可以自动检测到或由用户提供的解剖标记条件化的。最后，我们添加了一个模态相关的、轻量级的细化网络，用于捕捉隐式函数未表示的任何细节。所提出的DCSM框架在多个3D模态（增强CT，非增强CT，3D超声心动图）的心脏左心室（LV）分割问题上进行了评估。

    Delineation of anatomical structures is often the first step of many medical image analysis workflows. While convolutional neural networks achieve high performance, these do not incorporate anatomical shape information. We introduce a novel segmentation algorithm that uses Deep Conditional Shape models (DCSMs) as a core component. Using deep implicit shape representations, the algorithm learns a modality-agnostic shape model that can generate the signed distance functions for any anatomy of interest. To fit the generated shape to the image, the shape model is conditioned on anatomic landmarks that can be automatically detected or provided by the user. Finally, we add a modality-dependent, lightweight refinement network to capture any fine details not represented by the implicit function. The proposed DCSM framework is evaluated on the problem of cardiac left ventricle (LV) segmentation from multiple 3D modalities (contrast-enhanced CT, non-contrasted CT, 3D echocardiography-3DE). We de
    
[^102]: Mori-Zwanzig潜变空间Koopman闭包用于非线性自编码器

    Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder. (arXiv:2310.10745v1 [cs.LG])

    [http://arxiv.org/abs/2310.10745](http://arxiv.org/abs/2310.10745)

    本研究提出了一种名为Mori-Zwanzig自编码器（MZ-AE）的新方法，用于在低维空间中稳健地逼近Koopman算子，通过非线性自编码器和Mori-Zwanzig形式主义的集成实现对有限不变Koopman子空间的逼近，从而增强了精确性和准确预测复杂系统行为的能力。

    

    Koopman算子提供了一种吸引人的方法来实现非线性系统的全局线性化，使其成为简化复杂动力学理解的宝贵方法。虽然数据驱动的方法在逼近有限Koopman算子方面表现出了潜力，但它们面临着各种挑战，例如选择合适的可观察量、降维和准确预测复杂系统行为的能力。本研究提出了一种名为Mori-Zwanzig自编码器（MZ-AE）的新方法，用于在低维空间中稳健地逼近Koopman算子。所提出的方法利用非线性自编码器提取关键可观察量来逼近有限不变Koopman子空间，并利用Mori-Zwanzig形式主义集成非马尔可夫校正机制。因此，该方法在非线性自编码器的潜变流形中产生了动力学的封闭表示，从而提高了精确性和...

    The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and s
    
[^103]: 对表格数据的快速对抗性标签翻转攻击

    Fast Adversarial Label-Flipping Attack on Tabular Data. (arXiv:2310.10744v1 [cs.LG])

    [http://arxiv.org/abs/2310.10744](http://arxiv.org/abs/2310.10744)

    本文提出了基于表格数据的快速对抗性标签翻转攻击，并强调了这种攻击对机器学习模型的潜在风险和误导。为了证明这种风险，提出了一种新颖高效的攻击方法FALFA。

    

    机器学习模型越来越多地应用于需要高可靠性的领域，如网络安全。然而，这些模型仍然容易受到各种攻击，其中对抗性标签翻转攻击构成了重大威胁。在标签翻转攻击中，攻击者恶意地翻转一部分训练标签以破坏机器学习模型。本文提出了对这些攻击的重大担忧，因为这些攻击可以将高度偏斜的数据集伪装成易于解决的分类问题，往往会误导机器学习从业者降低防御措施并错误估计潜在风险。在表格数据设置中，这种担忧更加严重，因为识别真实标签需要专业知识，使得恶意标签翻转攻击很容易逃脱检测。为了证明这种风险体现在对手的目标中，我们提出了FALFA（快速对抗性标签翻转攻击），一种新颖高效的对抗性标签生成攻击方法。FALFA基于...

    Machine learning models are increasingly used in fields that require high reliability such as cybersecurity. However, these models remain vulnerable to various attacks, among which the adversarial label-flipping attack poses significant threats. In label-flipping attacks, the adversary maliciously flips a portion of training labels to compromise the machine learning model. This paper raises significant concerns as these attacks can camouflage a highly skewed dataset as an easily solvable classification problem, often misleading machine learning practitioners into lower defenses and miscalculations of potential risks. This concern amplifies in tabular data settings, where identifying true labels requires expertise, allowing malicious label-flipping attacks to easily slip under the radar. To demonstrate this risk is inherited in the adversary's objective, we propose FALFA (Fast Adversarial Label-Flipping Attack), a novel efficient attack for crafting adversarial labels. FALFA is based on
    
[^104]: MOFDiff: 针对金属有机骨架设计的粗粒度扩散方法

    MOFDiff: Coarse-grained Diffusion for Metal-Organic Framework Design. (arXiv:2310.10732v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.10732](http://arxiv.org/abs/2310.10732)

    MOFDiff是一种粗粒度扩散模型，通过使用等变图神经网络生成粗粒度的MOF结构，并能有效生成有效和新颖的MOF结构。

    

    金属有机骨架 (MOFs) 因其异常多孔性和可调控的化学性质而在气体储存和捕获碳等应用中引起了极大的兴趣。它们的模块化特性使得可以使用基于模板的方法根据已知的网络拓扑结构组合分子构建块来生成假设性的MOFs。然而，这些方法鉴别高性能MOFs的能力常常受到由结果化学空间的有限多样性限制。在这项工作中，我们提出了MOFDiff: 通过对构建块的坐标和标识进行去噪扩散过程生成粗粒度的MOF结构，并通过一种新颖的组装算法确定完整的原子级MOF结构。我们使用等变图神经网络来作为扩散模型，以尊重置换对称性和旋转平移对称性。我们全面评估了我们模型生成有效和新颖MOF结构的能力。

    Metal-organic frameworks (MOFs) are of immense interest in applications such as gas storage and carbon capture due to their exceptional porosity and tunable chemistry. Their modular nature has enabled the use of template-based methods to generate hypothetical MOFs by combining molecular building blocks in accordance with known network topologies. However, the ability of these methods to identify top-performing MOFs is often hindered by the limited diversity of the resulting chemical space. In this work, we propose MOFDiff: a coarse-grained (CG) diffusion model that generates CG MOF structures through a denoising diffusion process over the coordinates and identities of the building blocks. The all-atom MOF structure is then determined through a novel assembly algorithm. Equivariant graph neural networks are used for the diffusion model to respect the permutational and roto-translational symmetries. We comprehensively evaluate our model's capability to generate valid and novel MOF struct
    
[^105]: 一种表示学习方法用于探测宇宙大尺度结构中的动态暗能量

    A representation learning approach to probe for dynamical dark energy in matter power spectra. (arXiv:2310.10717v1 [astro-ph.CO])

    [http://arxiv.org/abs/2310.10717](http://arxiv.org/abs/2310.10717)

    这项研究提出了一种使用变分自编码器（VAE）架构进行表示学习的方法，用于在观测研究中探测宇宙大尺度结构中的动态暗能量模型。通过只使用一个潜在参数，可以预测到95%（99%）的DE功率谱，在考虑宇宙方差的高斯误差范围内具有很好的准确性。

    

    我们提出了DE-VAE，一种变分自编码器（VAE）架构，用于在宇宙大尺度结构的观测研究中寻找动态暗能量（DE）模型的压缩表示。DE-VAE在$.01-2.5 \ h/\rm{Mpc}$的波数$k$和四个红移值$z \in (0.1,0.48,0.78,1.5)$上训练了一个最典型的动态DE参数化的压缩表示，并且使用两个额外参数来描述演化的DE态方程。这些压缩表示与标准的冷暗物质（CDM）参数连接，并映射回重构的提升；压缩和重构组件都被参数化为神经网络。显著的是，我们发现一个单一的潜在参数足以在广泛的宇宙学参数范围内，预测到95%（99%）的DE功率谱，并且在包括宇宙方差的高斯误差的1$ \sigma$（2$ \sigma$）内。

    We present DE-VAE, a variational autoencoder (VAE) architecture to search for a compressed representation of dynamical dark energy (DE) models in observational studies of the cosmic large-scale structure. DE-VAE is trained on matter power spectra boosts generated at wavenumbers $k\in(0.01-2.5) \ h/\rm{Mpc}$ and at four redshift values $z\in(0.1,0.48,0.78,1.5)$ for the most typical dynamical DE parametrization with two extra parameters describing an evolving DE equation of state. The boosts are compressed to a lower-dimensional representation, which is concatenated with standard cold dark matter (CDM) parameters and then mapped back to reconstructed boosts; both the compression and the reconstruction components are parametrized as neural networks. Remarkably, we find that a single latent parameter is sufficient to predict 95% (99%) of DE power spectra generated over a broad range of cosmological parameters within $1\sigma$ ($2\sigma$) of a Gaussian error which includes cosmic variance, 
    
[^106]: 用于识别半导体晶圆地图中缺陷模式的机器学习技术：一项调查、实证和实验评估

    Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])

    [http://arxiv.org/abs/2310.10705](http://arxiv.org/abs/2310.10705)

    本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。

    

    本文综述了利用机器学习（ML）技术识别半导体制造中晶圆缺陷的方法学。尽管越来越多的研究证明了ML在晶圆缺陷识别中的有效性，但在这个主题上缺乏全面的综述。本文试图弥补这个空白，通过整合现有文献，深入分析各种ML算法在晶圆缺陷检测领域的优势、局限性和潜在应用。我们提出了一种创新的方法学分类体系，详细分类了算法，并提供了更细致的子技术划分。这个分类体系从广泛的方法学类别开始，到具体的子技术结束。它帮助研究人员理解不同算法以及它们的技术之间的复杂关系。我们采用严谨的实证和实验评估来验证算法性能。

    This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
    
[^107]: 透明的基于概念解释的异常检测

    Transparent Anomaly Detection via Concept-based Explanations. (arXiv:2310.10702v1 [cs.LG])

    [http://arxiv.org/abs/2310.10702](http://arxiv.org/abs/2310.10702)

    本论文提出了一种透明的基于概念解释的异常检测方法（ACE），能够提供人类可解释的解释和异常预测。该方法在推进异常检测的透明度的同时，实现了有效的人机交互，并且在性能上要么更高，要么与黑盒不可解释模型相当。

    

    深度学习技术的进步提升了异常检测的性能。然而，现实世界和安全关键应用需要超出准确性的透明度和推理能力。异常检测的任务集中在找出给定样本是否遵循学习到的分布。现有方法缺乏对其结果进行清晰解释的能力。因此，为了克服这一挑战，我们提出了透明的异常检测概念解释（ACE）方法。ACE能够以概念的形式提供人类可解释的解释和异常预测。据我所知，这是第一篇提出设计可解释异常检测的论文。除了促进异常检测的透明度，它还可以实现有效的人机交互。我们提出的模型结果要么更高，要么与黑盒不可解释模型相当。我们验证了ACE在三个现实数据集上的性能。

    Advancements in deep learning techniques have given a boost to the performance of anomaly detection. However, real-world and safety-critical applications demand a level of transparency and reasoning beyond accuracy. The task of anomaly detection (AD) focuses on finding whether a given sample follows the learned distribution. Existing methods lack the ability to reason with clear explanations for their outcomes. Hence to overcome this challenge, we propose Transparent {A}nomaly Detection {C}oncept {E}xplanations (ACE). ACE is able to provide human interpretable explanations in the form of concepts along with anomaly prediction. To the best of our knowledge, this is the first paper that proposes interpretable by-design anomaly detection. In addition to promoting transparency in AD, it allows for effective human-model interaction. Our proposed model shows either higher or comparable results to black-box uninterpretable models. We validate the performance of ACE across three realistic data
    
[^108]: 通过多线性操作器重用预训练模型以实现有效训练

    Reusing Pretrained Models by Multi-linear Operators for Efficient Training. (arXiv:2310.10699v1 [cs.LG])

    [http://arxiv.org/abs/2310.10699](http://arxiv.org/abs/2310.10699)

    本文提出了一种通过多线性操作器重用预训练模型以实现有效训练的方法，解决了从头开始训练大型模型所需资源大的问题，并通过线性相关来增强加速能力。

    

    从头开始训练大型模型通常需要大量资源。为了解决这个问题，最近的研究如bert2BERT和LiGO使用预训练的小型模型初始化大型模型（称为“目标模型”），从而加速了训练过程。尽管这些先前研究取得了一些成功，但它们只映射部分权重成长预训练模型，忽略了整个模型可能存在的相关性。正如我们在本文中所展示的，预训练模型和目标模型的权重之间存在着相互作用。因此，部分映射可能无法捕捉到完整的信息，并导致成长不足。本文提出了一种方法，将目标模型的每个权重与预训练模型的所有权重进行线性相关，以进一步增强加速能力。我们利用多线性操作器来降低计算和空间复杂度，实现可接受的资源需求。

    Training large models from scratch usually costs a substantial amount of resources. Towards this problem, recent studies such as bert2BERT and LiGO have reused small pretrained models to initialize a large model (termed the ``target model''), leading to a considerable acceleration in training. Despite the successes of these previous studies, they grew pretrained models by mapping partial weights only, ignoring potential correlations across the entire model. As we show in this paper, there are inter- and intra-interactions among the weights of both the pretrained and the target models. As a result, the partial mapping may not capture the complete information and lead to inadequate growth. In this paper, we propose a method that linearly correlates each weight of the target model to all the weights of the pretrained model to further enhance acceleration ability. We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements. Ex
    
[^109]: 鲁棒的协同过滤与流行度分布变化

    Robust Collaborative Filtering to Popularity Distribution Shift. (arXiv:2310.10696v1 [cs.LG])

    [http://arxiv.org/abs/2310.10696](http://arxiv.org/abs/2310.10696)

    该论文提出了一种鲁棒的协同过滤方法，解决了训练数据中流行度偏差导致的泛化性能问题。通过评估和减少快捷方式程度，以及不需事先了解测试分布，提高了去偏见表示的质量和OOD泛化性能。

    

    在领先的协同过滤（CF）模型中，用户和项目的表示往往会学习训练数据中的流行度偏差作为快捷方式。流行度快捷方式对于在分布（ID）性能上是好的，但对于超出分布（OOD）的数据（即在测试数据的流行度分布与训练数据不同时），泛化能力较差。为了填补这个差距，去偏见策略尝试评估表示中的快捷方式程度，并将其减少。然而，存在两个不足之处：（1）在测量快捷方式程度时，大多数策略只使用单一方面的统计指标（即项目频率对项目和用户频率对用户方面），不能适应用户-项目对的组合程度；（2）在减少快捷方式时，许多策略假设测试分布事先已知。这导致质量较低的去偏见表示。更糟糕的是，这些策略以牺牲OOD泛化性能为代价。

    In leading collaborative filtering (CF) models, representations of users and items are prone to learn popularity bias in the training data as shortcuts. The popularity shortcut tricks are good for in-distribution (ID) performance but poorly generalized to out-of-distribution (OOD) data, i.e., when popularity distribution of test data shifts w.r.t. the training one. To close the gap, debiasing strategies try to assess the shortcut degrees and mitigate them from the representations. However, there exist two deficiencies: (1) when measuring the shortcut degrees, most strategies only use statistical metrics on a single aspect (i.e., item frequency on item and user frequency on user aspect), failing to accommodate the compositional degree of a user-item pair; (2) when mitigating shortcuts, many strategies assume that the test distribution is known in advance. This results in low-quality debiased representations. Worse still, these strategies achieve OOD generalizability with a sacrifice on 
    
[^110]: 基于数据驱动的评分模型用于生成具有自适应晶胞的稳定结构

    Data-Driven Score-Based Models for Generating Stable Structures with Adaptive Crystal Cells. (arXiv:2310.10695v1 [physics.comp-ph])

    [http://arxiv.org/abs/2310.10695](http://arxiv.org/abs/2310.10695)

    本研究提出了一种基于数据驱动的评分模型来生成具有自适应晶胞的稳定结构。该方法通过学习可用数据中的晶格，使用两个去噪过程并行生成晶格，以实现对具有所需属性的新晶体结构的生成。

    

    由于其复杂性，发现新的功能性和稳定的材料是一个巨大的挑战。本研究旨在通过使用机器学习生成模型生成具有所需属性（例如化学稳定性和指定的化学成分）的新晶体结构。与生成分子相比，晶体结构由于晶体的周期性以及与空间群相关的特定对称性约束而带来新的困难。本研究采用基于退火Langevin动力学的评分概率模型来适应晶体生成任务。所提出的方法的创新之处在于晶胞的晶格不是固定的。在模型的训练过程中，晶格从可用数据中进行学习，而在生成新的化学结构的采样过程中，使用两个去噪过程并行生成晶格。

    The discovery of new functional and stable materials is a big challenge due to its complexity. This work aims at the generation of new crystal structures with desired properties, such as chemical stability and specified chemical composition, by using machine learning generative models. Compared to the generation of molecules, crystal structures pose new difficulties arising from the periodic nature of the crystal and from the specific symmetry constraints related to the space group. In this work, score-based probabilistic models based on annealed Langevin dynamics, which have shown excellent performance in various applications, are adapted to the task of crystal generation. The novelty of the presented approach resides in the fact that the lattice of the crystal cell is not fixed. During the training of the model, the lattice is learned from the available data, whereas during the sampling of a new chemical structure, two denoising processes are used in parallel to generate the lattice 
    
[^111]: iNaturalist公民科学社区的网络分析

    Network Analysis of the iNaturalist Citizen Science Community. (arXiv:2310.10693v1 [cs.SI])

    [http://arxiv.org/abs/2310.10693](http://arxiv.org/abs/2310.10693)

    本研究以iNaturalist公民科学平台为案例，通过网络分析的方法，探讨了公民科学项目的结构与用户之间的交互，提出了一个新颖的网络科学研究基准，并通过链接预测任务获得了新的认识。

    

    近年来，公民科学已成为科学界的重要组成部分。其能够从数千名公民科学家那里获取数据和专业知识，使其具有无可替代的价值。尽管该领域越来越受欢迎，但公民科学项目的交互和结构仍然被了解很少并且被少分析。我们以iNaturalist公民科学平台为案例研究，分析公民科学项目的结构。我们将iNaturalist中的数据构建为一个二分网络，并使用可视化和已建立的网络科学技术来了解公民科学项目中用户之间的结构和交互。最后，我们通过使用iNaturalist数据创建一个网络，该网络相对于其他常见的基准网络具有独特的结构，提出了一个新颖的网络科学研究基准。我们通过进行链接预测任务证明了该网络可以用于获得新的认识。

    In recent years, citizen science has become a larger and larger part of the scientific community. Its ability to crowd source data and expertise from thousands of citizen scientists makes it invaluable. Despite the field's growing popularity, the interactions and structure of citizen science projects are still poorly understood and under analyzed. We use the iNaturalist citizen science platform as a case study to analyze the structure of citizen science projects. We frame the data from iNaturalist as a bipartite network and use visualizations as well as established network science techniques to gain insights into the structure and interactions between users in citizen science projects. Finally, we propose a novel unique benchmark for network science research by using the iNaturalist data to create a network which has an unusual structure relative to other common benchmark networks. We demonstrate using a link prediction task that this network can be used to gain novel insights into a v
    
[^112]: ACES: 使用自我目标语言模型和语义描述符生成多样的编程难题

    ACES: generating diverse programming puzzles with autotelic language models and semantic descriptors. (arXiv:2310.10692v1 [cs.LG])

    [http://arxiv.org/abs/2310.10692](http://arxiv.org/abs/2310.10692)

    ACES是一种使用自我目标语言模型和语义描述符生成多样化的编程难题的方法，能够优化有趣的多样性和少样本生成。

    

    寻找和选择新颖有趣的问题是好奇心、科学和创新的核心。在Python编程难题的无限空间中，我们研究了自动问题生成。现有的生成模型通常旨在建模参考分布，没有明确的多样性优化。其他方法在有限的手工编码表示空间或不可解释的学习嵌入空间中明确优化多样性，这些嵌入空间可能与人类对有趣变化的感知不符。通过ACES（自我目标代码探索与语义描述符），我们引入了一种新的自我目标生成方法，利用大型语言模型（LLM）生成语义描述符，直接优化有趣的多样性，以及少样本生成。每个难题都标记有10个维度，每个维度捕捉了解决它所需的编程技能。ACES生成并追求新颖可行的目标。

    Finding and selecting new and interesting problems to solve is at the heart of curiosity, science and innovation. We here study automated problem generation in the context of the open-ended space of python programming puzzles. Existing generative models often aim at modeling a reference distribution without any explicit diversity optimization. Other methods explicitly optimizing for diversity do so either in limited hand-coded representation spaces or in uninterpretable learned embedding spaces that may not align with human perceptions of interesting variations. With ACES (Autotelic Code Exploration via Semantic descriptors), we introduce a new autotelic generation method that leverages semantic descriptors produced by a large language model (LLM) to directly optimize for interesting diversity, as well as few-shot-based generation. Each puzzle is labeled along 10 dimensions, each capturing a programming skill required to solve it. ACES generates and pursues novel and feasible goals to 
    
[^113]: 使用扩散模型提高数字VLSI电路的机器学习模型准确性：关于合成数据生成的研究

    Enhancing ML model accuracy for Digital VLSI circuits using diffusion models: A study on synthetic data generation. (arXiv:2310.10691v1 [cs.LG])

    [http://arxiv.org/abs/2310.10691](http://arxiv.org/abs/2310.10691)

    本研究通过使用扩散模型生成合成数据来提高机器学习模型在数字VLSI电路中的准确性。通过验证生成数据的质量和进行数据增强，我们证明了这种方法在预测分析中的有效性。

    

    过去几年中，生成式人工智能取得了显著的增长，扩散模型已成为图像生成的最新技术。本研究调查了在电子电路的人工数据生成中使用扩散模型来提高后续机器学习模型的准确性，用于性能评估、设计和测试等任务，当训练数据通常非常有限。我们利用HSPICE设计环境中的22纳米CMOS技术节点模拟得到了代表性真实训练数据用于我们提出的扩散模型。我们的研究结果表明，使用扩散模型生成的合成数据与真实数据非常相似。我们验证了生成数据的质量，并证明数据增强在数字电路的VLSI设计的预测分析中具有明显效果。

    Generative AI has seen remarkable growth over the past few years, with diffusion models being state-of-the-art for image generation. This study investigates the use of diffusion models in generating artificial data generation for electronic circuits for enhancing the accuracy of subsequent machine learning models in tasks such as performance assessment, design, and testing when training data is usually known to be very limited. We utilize simulations in the HSPICE design environment with 22nm CMOS technology nodes to obtain representative real training data for our proposed diffusion model. Our results demonstrate the close resemblance of synthetic data using diffusion model to real data. We validate the quality of generated data, and demonstrate that data augmentation certainly effective in predictive analysis of VLSI design for digital circuits.
    
[^114]: 一种仅解码器的时间序列预测基础模型

    A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])

    [http://arxiv.org/abs/2310.10688](http://arxiv.org/abs/2310.10688)

    本论文介绍了一种基于补丁解码器式注意力模型的时间序列预测基础模型，该模型在零样本情况下在各种公共数据集上的性能接近最先进的监督预测模型。

    

    受到自然语言处理中大型语言模型的最新进展的启发，我们设计了一种用于预测的时间序列基础模型，其在各种公共数据集上的开箱即用的零样本性能接近每个个别数据集上最先进的监督预测模型的准确性。我们的模型基于在大型时间序列语料库上预训练的补丁解码器式注意力模型，并可以适用于不同的预测历史长度、预测长度和时间粒度。

    Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
    
[^115]: PS-AAS: 自动算法选择的投资组合选择在黑盒优化中的应用

    PS-AAS: Portfolio Selection for Automated Algorithm Selection in Black-Box Optimization. (arXiv:2310.10685v1 [cs.LG])

    [http://arxiv.org/abs/2310.10685](http://arxiv.org/abs/2310.10685)

    该论文介绍了一种名为PS-AAS的方法，用于自动算法选择，具体应用于黑盒优化中。通过创建算法行为元表示，构建算法之间的图形，并利用图形算法选择多样化、代表性和非冗余的投资组合，从而提高了算法选择的性能和灵活性。

    

    自动算法选择（AAS）的性能很大程度上取决于要选择的算法投资组合。选择投资组合是一个非常复杂的任务，需要在大型投资组合的高灵活性和AAS任务的增加复杂性之间进行权衡。在实际中，选择投资组合的最常见方法可能是在一些感兴趣的参考任务中选择表现良好的算法。在本研究中，我们旨在探索替代的数据驱动的投资组合选择技术。我们提出的方法创建算法行为元表示，根据它们的元表示相似性从一组算法中构建图形，并应用图形算法来选择多样化、代表性和非冗余的最终投资组合。我们评估了两种不同的元表示技术（SHAP和performance2vec）来选择附加的投资组合，共计324种不同的算法。

    The performance of automated algorithm selection (AAS) strongly depends on the portfolio of algorithms to choose from. Selecting the portfolio is a non-trivial task that requires balancing the trade-off between the higher flexibility of large portfolios with the increased complexity of the AAS task. In practice, probably the most common way to choose the algorithms for the portfolio is a greedy selection of the algorithms that perform well in some reference tasks of interest.  We set out in this work to investigate alternative, data-driven portfolio selection techniques. Our proposed method creates algorithm behavior meta-representations, constructs a graph from a set of algorithms based on their meta-representation similarity, and applies a graph algorithm to select a final portfolio of diverse, representative, and non-redundant algorithms. We evaluate two distinct meta-representation techniques (SHAP and performance2vec) for selecting complementary portfolios from a total of 324 diff
    
[^116]: 大型语言模型的去学习研究

    Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])

    [http://arxiv.org/abs/2310.10683](http://arxiv.org/abs/2310.10683)

    大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。

    

    我们研究了如何对大型语言模型（LLMs）进行去学习，即忘记不受欢迎的（非）行为。我们展示了至少三种情境可以从去学习中使LLMs与人类偏好保持一致：（1）删除有害回复，（2）按要求删除受版权保护的内容，以及（3）消除幻觉。作为对齐技术的一种，去学习具有三个优点：（1）只需要负面（例如有害）示例，这比在RLHF（基于人类反馈的强化学习）中所需的正面（例如有帮助且通常由人类编写）示例更容易和更便宜地收集（例如通过红队测试或用户报告）；（2）计算效率高；（3）当我们知道哪些训练样本导致了不良行为时，它特别有效。据我们所知，我们的工作是首次探索LLM去学习的工作之一。我们也是首次在LLM去学习中制定了设置、目标和评估。我们表明，如果从业者只有有限的

    We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
    
[^117]: 融合量子和经典机器学习的情感分析方法

    Hybrid Quantum-Classical Machine Learning for Sentiment Analysis. (arXiv:2310.10672v1 [cs.CL])

    [http://arxiv.org/abs/2310.10672](http://arxiv.org/abs/2310.10672)

    本文提出了一种使用混合量子-经典机器学习算法进行情感分析的方法，通过研究量子核方法和基于变分量子电路的分类器，并结合经典的降维技术，实现了在处理大规模数据集中表达的人类情感和观点的情感分析，并取得了相对于传统方法更好的性能。

    

    量子计算与经典机器学习的合作在自然语言处理中具有潜在优势，特别是在处理大规模数据集中表达的人类情感和观点的情感分析方面。本文提出了一种使用混合量子-经典机器学习算法进行情感分析的方法。我们研究了量子核方法和基于变分量子电路的分类器，并将它们与经典的降维技术（如PCA和Haar小波变换）结合起来。该方法在基于英语和孟加拉语的两个不同数据集上进行了评估。实验结果表明，在对数据进行降维处理后，基于量子的混合算法的性能一致且优于经典方法。

    The collaboration between quantum computing and classical machine learning offers potential advantages in natural language processing, particularly in the sentiment analysis of human emotions and opinions expressed in large-scale datasets. In this work, we propose a methodology for sentiment analysis using hybrid quantum-classical machine learning algorithms. We investigate quantum kernel approaches and variational quantum circuit-based classifiers and integrate them with classical dimension reduction techniques such as PCA and Haar wavelet transform. The proposed methodology is evaluated using two distinct datasets, based on English and Bengali languages. Experimental results show that after dimensionality reduction of the data, performance of the quantum-based hybrid algorithms were consistent and better than classical methods.
    
[^118]: 智能OMVI：使用新的数据集识别混淆恶意软件变种

    Smart OMVI: Obfuscated Malware Variant Identification using a novel dataset. (arXiv:2310.10670v1 [cs.CR])

    [http://arxiv.org/abs/2310.10670](http://arxiv.org/abs/2310.10670)

    智能OMVI使用新的数据集OMD，通过识别混淆恶意软件变种来应对恶意软件的威胁。

    

    在数字时代，网络安全已成为一个重要问题，随着每天计算机使用的增长。网络犯罪分子现在进行的不仅仅是病毒传播和计算机黑客行为。由于威胁到一个国家的生存，网络战争已经发展起来。恶意软件分析作为对抗攻击的第一道防线，并且是网络犯罪的重要组成部分。每天，恶意软件攻击目标众多的计算机用户、企业和政府机构，造成数十亿美元的损失。尽管安全专家拥有各种工具来识别恶意软件，但恶意软件可以通过其设计师进行微小巧妙的调整来规避多个杀毒软件的检测。为了应对这一挑战，开发了一个名为混淆恶意软件数据集(OMD)的新数据集。该数据集包含40个不同的恶意软件家族，有21924个样本，并且采用了模拟攻击者策略的混淆技术。

    Cybersecurity has become a significant issue in the digital era as a result of the growth in everyday computer use. Cybercriminals now engage in more than virus distribution and computer hacking. Cyberwarfare has developed as a result because it has become a threat to a nation's survival. Malware analysis serves as the first line of defence against an attack and is a significant component of cybercrime. Every day, malware attacks target a large number of computer users, businesses, and governmental agencies, causing billions of dollars in losses. Malware may evade multiple AV software with a very minor, cunning tweak made by its designers, despite the fact that security experts have a variety of tools at their disposal to identify it. To address this challenge, a new dataset called the Obfuscated Malware Dataset (OMD) has been developed. This dataset comprises 40 distinct malware families having 21924 samples, and it incorporates obfuscation techniques that mimic the strategies employe
    
[^119]: 通过机器学习驱动的图组合优化增强网络韧性：在网络安全和信息传播中的应用

    Enhancing Network Resilience through Machine Learning-powered Graph Combinatorial Optimization: Applications in Cyber Defense and Information Diffusion. (arXiv:2310.10667v1 [cs.CR])

    [http://arxiv.org/abs/2310.10667](http://arxiv.org/abs/2310.10667)

    本论文致力于通过机器学习驱动的图组合优化方法来增强网络韧性，在网络安全和信息传播等领域具有较高的适用性。

    

    随着计算和网络通信技术的不断发展，网络基础设施及其应用环境变得越来越复杂。由于复杂性的增加，网络更容易发生硬件故障，并且更容易受到网络攻击的影响。因此，对于快速发展的网络中心化应用而言，网络韧性对于减少攻击的影响并确保网络在攻击、故障或中断期间提供可接受的服务水平至关重要。在这方面，本论文旨在开发有效的方法来增强网络韧性。现有的增强网络韧性的方法侧重于确定网络中的瓶颈节点和边缘，并设计积极的响应措施来保护网络免受攻击。然而，现有解决方案通常考虑更宽泛的应用领域，并在应用于特定应用领域，如网络安全时具有有限的适用性。

    With the burgeoning advancements of computing and network communication technologies, network infrastructures and their application environments have become increasingly complex. Due to the increased complexity, networks are more prone to hardware faults and highly susceptible to cyber-attacks. Therefore, for rapidly growing network-centric applications, network resilience is essential to minimize the impact of attacks and to ensure that the network provides an acceptable level of services during attacks, faults or disruptions. In this regard, this thesis focuses on developing effective approaches for enhancing network resilience. Existing approaches for enhancing network resilience emphasize on determining bottleneck nodes and edges in the network and designing proactive responses to safeguard the network against attacks. However, existing solutions generally consider broader application domains and possess limited applicability when applied to specific application areas such as cyber
    
[^120]: 从测量中提取物理因果关系以检测和定位虚假数据插入攻击

    Extracting Physical Causality from Measurements to Detect and Localize False Data Injection Attacks. (arXiv:2310.10666v1 [cs.CR])

    [http://arxiv.org/abs/2310.10666](http://arxiv.org/abs/2310.10666)

    本文提出了一种基于因果推断的联合FDIA检测和定位框架，以提取物理因果关系来检测和定位虚假数据插入攻击。

    

    虚假数据插入攻击（FDIA）已成为现代网络物理电力系统中日益关注的问题。大多数现有的FDIA检测技术将原始测量数据投影到高维潜在空间中，以区分正常样本和攻击样本。这些方法更多地关注数据值的统计相关性，因此容易受到系统操作点变化或FDIA类型和强度变化引起的数据分布漂移的影响，尤其是对于FDIA定位任务来说。而因果推断则提取了不同测量之间协同波动背后的因果关系。因果性模式由基本物理定律（如欧姆定律和基尔霍夫定律）确定。它们对FDIA引起的物理定律违反敏感，但在系统操作点漂移时趋于稳定。借助这个优势，本文提出了基于因果推断的联合FDIA检测和定位框架。

    False Data Injection Attack (FDIA) has become a growing concern in modern cyber-physical power systems. Most existing FDIA detection techniques project the raw measurement data into a high-dimensional latent space to separate normal and attacked samples. These approaches focus more on the statistical correlations of data values and are therefore susceptible to data distribution drifts induced by changes in system operating points or changes in FDIA types and strengths, especially for FDIA localization tasks. Causal inference, on the other hand, extracts the causality behind the coordinated fluctuations of different measurements. The causality patterns are determined by fundamental physical laws such as Ohm's Law and Kirchhoff's Law. They are sensitive to the violation of physical laws caused by FDIA, but tend to remain stable with the drift of system operating points. Leveraging this advantage, this paper proposes a joint FDIA detection and localization framework based on causal infere
    
[^121]: 人工智能与扩展现实（AI-XR）元宇宙中的隐私保护：一项调研

    Privacy Preservation in Artificial Intelligence and Extended Reality (AI-XR) Metaverses: A Survey. (arXiv:2310.10665v1 [cs.CR])

    [http://arxiv.org/abs/2310.10665](http://arxiv.org/abs/2310.10665)

    这项调研讨论了在人工智能与扩展现实元宇宙中的隐私保护问题。作为一个新兴概念，元宇宙的隐私问题令人担忧，特别是在沉浸式虚拟体验变得越来越普及的情况下。元宇宙将利用多种技术进行发展，并收集用户数据来提供个性化和沉浸式的服务，但这也引发了隐私问题的关注。

    

    元宇宙是一个新兴概念，它设想了一个虚拟宇宙，一个协作空间，个体可以在其中互动、创造和参与各种活动。随着这个概念的发展和沉浸式虚拟体验的普及，元宇宙中的隐私成为一个重要问题。元宇宙隐私问题指的是在虚拟现实（VR）环境中的个人信息和数据隐私面临的挑战和关切，因为共享的VR空间概念变得更加可接近。元宇宙将借助人工智能（AI）、扩展现实（XR）、混合现实（MR）和基于5G/6G通信的技术进步，为用户提供个性化和沉浸式服务。此外，为了实现更个性化的体验，元宇宙依赖于收集精细化的用户数据，这导致了各种隐私问题。因此，在完全实现元宇宙的潜力之前，隐私保护是非常重要的。

    The metaverse is a nascent concept that envisions a virtual universe, a collaborative space where individuals can interact, create, and participate in a wide range of activities. Privacy in the metaverse is a critical concern as the concept evolves and immersive virtual experiences become more prevalent. The metaverse privacy problem refers to the challenges and concerns surrounding the privacy of personal information and data within Virtual Reality (VR) environments as the concept of a shared VR space becomes more accessible. Metaverse will harness advancements from various technologies such as Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and 5G/6G-based communication to provide personalized and immersive services to its users. Moreover, to enable more personalized experiences, the metaverse relies on the collection of fine-grained user data that leads to various privacy issues. Therefore, before the potential of the metaverse can be fully realized, privacy
    
[^122]: Nebula:用于动态恶意软件分析的自注意力网络

    Nebula: Self-Attention for Dynamic Malware Analysis. (arXiv:2310.10664v1 [cs.CR])

    [http://arxiv.org/abs/2310.10664](http://arxiv.org/abs/2310.10664)

    Nebula是一个自注意力网络，用于动态分析恶意软件。它能够概括不同的行为表示和格式，并结合动态日志报告中的异构信息。实验证明Nebula在三个重要任务上表现出色。

    

    动态分析通过在受控环境中执行程序并将其行为存储在日志报告中，可以检测Windows恶意软件。先前的工作已经开始在这些报告上训练机器学习模型以进行恶意软件检测或分类。然而，大多数方法仅考虑了卷积和长短期记忆网络，只关注运行时调用的API，并未考虑其他相关的异构信息来源，如网络和文件操作。此外，代码和预训练模型很难获取，这限制了该研究领域中结果的可重现性。在本文中，我们通过提出Nebula来克服这些限制，这是一个多功能的、基于自注意力的转换器神经架构，可以概括不同的行为表示和格式，结合动态日志报告中的异构信息。我们展示了Nebula的有效性，它在三个重要任务上的实验中表现出色。

    Dynamic analysis enables detecting Windows malware by executing programs in a controlled environment, and storing their actions in log reports. Previous work has started training machine learning models on such reports to perform either malware detection or malware classification. However, most of the approaches (i) have only considered convolutional and long-short term memory networks, (ii) they have been built focusing only on APIs called at runtime, without considering other relevant though heterogeneous sources of information like network and file operations, and (iii) the code and pretrained models are hardly available, hindering reproducibility of results in this research area. In this work, we overcome these limitations by presenting Nebula, a versatile, self-attention transformer-based neural architecture that can generalize across different behavior representations and formats, combining heterogeneous information from dynamic log reports. We show the efficacy of Nebula on thre
    
[^123]: TII-SSRC-23数据集：对入侵检测中不同流量模式的类型学探索

    TII-SSRC-23 Dataset: Typological Exploration of Diverse Traffic Patterns for Intrusion Detection. (arXiv:2310.10661v1 [cs.CR])

    [http://arxiv.org/abs/2310.10661](http://arxiv.org/abs/2310.10661)

    TII-SSRC-23数据集是一个新颖而综合的数据集，旨在克服现有数据集在入侵检测中的不足，并提供了关于入侵检测任务关键特征的重要见解。

    

    网络入侵检测系统主要基于机器学习，其效果受到训练数据集的影响。确保这些数据集准确反映良性和恶意流量的多方面特性对于创建能够识别和应对各种入侵模式的模型至关重要。然而，现有数据集往往不足，缺乏必要的多样性和与当代网络环境的匹配性，从而限制了入侵检测的有效性。本文介绍了TII-SSRC-23，这是一个新颖而综合的数据集，旨在克服这些挑战。我们的数据集包含各种类型和子类型的流量，是研究社区的强大而多功能的工具。此外，我们还进行了特征重要性分析，为入侵检测任务提供了重要见解。通过广泛的实验，我们还建立了坚实的基础。

    The effectiveness of network intrusion detection systems, predominantly based on machine learning, are highly influenced by the dataset they are trained on. Ensuring an accurate reflection of the multifaceted nature of benign and malicious traffic in these datasets is essential for creating models capable of recognizing and responding to a wide array of intrusion patterns. However, existing datasets often fall short, lacking the necessary diversity and alignment with the contemporary network environment, thereby limiting the effectiveness of intrusion detection. This paper introduces TII-SSRC-23, a novel and comprehensive dataset designed to overcome these challenges. Comprising a diverse range of traffic types and subtypes, our dataset is a robust and versatile tool for the research community. Additionally, we conduct a feature importance analysis, providing vital insights into critical features for intrusion detection tasks. Through extensive experimentation, we also establish firm b
    
[^124]: 行为属性重叠现象中的网络攻击分析与检测

    Analysis and Detection against Network Attacks in the Overlapping Phenomenon of Behavior Attribute. (arXiv:2310.10660v1 [cs.CR])

    [http://arxiv.org/abs/2310.10660](http://arxiv.org/abs/2310.10660)

    本研究发现在网络攻击中存在行为属性重叠的现象，并提出了基于深度学习的多标签检测模型 MLG-Model 来解决这个问题。

    

    网络攻击的增加给安全造成了重大威胁。研究人员提出了用于支持相关领域研究的网络攻击数据集。基于这些数据集，提出了许多攻击检测方法。然而，我们发现在攻击之间存在着行为属性重叠的显著现象，即数据集中存在多个具有相同特征但不同标签的样本。在本文中，我们验证了这一现象并重新标注了一些知名数据集（如UNSW-NB15、CCCS-CIC-AndMal-2020）。此外，以多标签方式检测网络攻击可以获得更多信息，为追踪攻击源和构建入侵检测系统提供支持。因此，我们提出了基于深度学习的多标签检测模型MLD-Model，其中采用Wassers值函数进行训练。

    The proliferation of network attacks poses a significant threat. Researchers propose datasets for network attacks to support research in related fields. Then, many attack detection methods based on these datasets are proposed. These detection methods, whether two-classification or multi-classification, belong to single-label learning, i.e., only one label is given to each sample. However, we discover that there is a noteworthy phenomenon of behavior attribute overlap between attacks, The presentation of this phenomenon in a dataset is that there are multiple samples with the same features but different labels. In this paper, we verify the phenomenon in well-known datasets(UNSW-NB15, CCCS-CIC-AndMal-2020) and re-label these data. In addition, detecting network attacks in a multi-label manner can obtain more information, providing support for tracing the attack source and building IDS. Therefore, we propose a multi-label detection model based on deep learning, MLD-Model, in which Wassers
    
[^125]: 通过机器遗忘实施后门攻击

    Backdoor Attack through Machine Unlearning. (arXiv:2310.10659v1 [cs.CR])

    [http://arxiv.org/abs/2310.10659](http://arxiv.org/abs/2310.10659)

    本文提出了一种基于机器遗忘的新型黑盒后门攻击方法，通过激活隐藏后门来输出恶意预测，从而针对深度学习模型的脆弱性进行攻击。

    

    最近几年，由于深度学习研究和应用的快速发展，人工智能的安全问题变得越来越突出。后门攻击是一种针对深度学习模型的攻击，攻击者通过嵌入的触发器激活隐藏的后门，从而输出可能与给定输入的预期输出不符的恶意预测。在本文中，我们提出了一种基于机器遗忘的新型黑盒后门攻击。攻击者首先通过精心设计的样本（包括毒数据和缓解数据）扩充训练集，训练一个“善意”模型。然后，攻击者提交遗忘请求，以移除缓解样本对模型的影响，逐步激活隐藏的后门。由于后门是在迭代的遗忘过程中植入的，这显著增加了现有的后门检测方法的计算开销。

    In recent years, the security issues of artificial intelligence have become increasingly prominent due to the rapid development of deep learning research and applications. Backdoor attack is an attack targeting the vulnerability of deep learning models, where hidden backdoors are activated by triggers embedded by the attacker, thereby outputting malicious predictions that may not align with the intended output for a given input. In this work, we propose a novel black-box backdoor attack based on machine unlearning. The attacker first augments the training set with carefully designed samples, including poison and mitigation data, to train a 'benign' model. Then, the attacker posts unlearning requests for the mitigation samples to remove the impact of relevant data on the model, gradually activating the hidden backdoor. Since backdoors are implanted during the iterative unlearning process, it significantly increases the computational overhead of existing defense methods for backdoor dete
    
[^126]: VeriDIP: 通过隐私泄露指纹验证深度神经网络的所有权

    VeriDIP: Verifying Ownership of Deep Neural Networks through Privacy Leakage Fingerprints. (arXiv:2310.10656v1 [cs.CR])

    [http://arxiv.org/abs/2310.10656](http://arxiv.org/abs/2310.10656)

    VeriDIP是一种新颖的所有权测试方法，通过利用隐私泄露指纹和使用较少私有样本的方法来验证深度神经网络模型的知识产权。

    

    部署机器学习作为一项服务会导致模型抄袭，从而导致版权侵权。所有权测试技术旨在识别模型指纹以验证抄袭行为。然而，先前的工作往往依赖于过度拟合或鲁棒性特征作为指纹，缺乏理论保证，并在广义模型上表现出不足的性能。在本文中，我们提出了一种名为VeriDIP的新型所有权测试方法，用于验证DNN模型的知识产权。VeriDIP做出了两个主要贡献。(1)它利用隐私推断攻击来估计隐私泄露的下界，从而反映了给定模型的指纹。隐私泄露指纹突出了模型记忆敏感训练数据集的独特模式。(2)我们引入了一种使用较少私有样本增强所有权测试性能的新方法。大量实验证实了VeriDIP的有效性和效率。

    Deploying Machine Learning as a Service gives rise to model plagiarism, leading to copyright infringement. Ownership testing techniques are designed to identify model fingerprints for verifying plagiarism. However, previous works often rely on overfitting or robustness features as fingerprints, lacking theoretical guarantees and exhibiting under-performance on generalized models. In this paper, we propose a novel ownership testing method called VeriDIP, which verifies a DNN model's intellectual property. VeriDIP makes two major contributions. (1) It utilizes membership inference attacks to estimate the lower bound of privacy leakage, which reflects the fingerprint of a given model. The privacy leakage fingerprints highlight the unique patterns through which the models memorize sensitive training datasets. (2) We introduce a novel approach using less private samples to enhance the performance of ownership testing.  Extensive experimental results confirm that VeriDIP is effective and eff
    
[^127]: 用不确定性量化增强基于机器学习的网络入侵检测的可信度

    Enhancing Trustworthiness in ML-Based Network Intrusion Detection with Uncertainty Quantification. (arXiv:2310.10655v1 [cs.CR])

    [http://arxiv.org/abs/2310.10655](http://arxiv.org/abs/2310.10655)

    这项研究提出了一种通过不确定性量化来增强基于机器学习的网络入侵检测的可信度的方法。

    

    互联网和相关通信技术的发展不断增加了网络攻击的风险。在这种情况下，入侵检测系统（IDS）发挥着至关重要的作用，这是一种旨在识别和缓解对现代网络的攻击的安全设备。在过去的十年中，基于机器学习（ML）的数据驱动方法越来越受到欢迎，用于执行IDS所需的分类任务。然而，为了适应这个目的而采用的典型的ML模型没有适当地考虑到与其自身预测相关的不确定性。这带来了显著的挑战，因为它们往往会为误分类的输入和属于未知类别（例如新型攻击）的输入产生误导性较高的分类得分，限制了现有基于ML的解决方案的可信度。在本文中，我们认为基于ML的IDS应该始终提供准确的不确定性量化，以避免过度自信的预测。事实上，一种准确的不确定性量化可以为进一步的决策提供重要的参考，从而增强ML-based IDS的可信度。

    The evolution of Internet and its related communication technologies have consistently increased the risk of cyber-attacks. In this context, a crucial role is played by Intrusion Detection Systems (IDSs), which are security devices designed to identify and mitigate attacks to modern networks. In the last decade, data-driven approaches based on Machine Learning (ML) have gained more and more popularity for executing the classification tasks required by IDSs. However, typical ML models adopted for this purpose do not properly take into account the uncertainty associated with their own prediction. This poses significant challenges, as they tend to produce misleadingly high classification scores for both misclassified inputs and inputs belonging to unknown classes (e.g. novel attacks), limiting the trustworthiness of existing ML-based solutions. In this paper we argue that ML-based IDSs should always provide accurate uncertainty quantification to avoid overconfident predictions. In fact, a
    
[^128]: 用于求解Wasserstein Lagrangian流的计算框架

    A Computational Framework for Solving Wasserstein Lagrangian Flows. (arXiv:2310.10649v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10649](http://arxiv.org/abs/2310.10649)

    本研究提出了一个基于深度学习的计算框架，通过拉格朗日对偶形式处理不同的最优输运问题，不需要模拟轨迹或访问最优耦合，具有较高的性能。

    

    通过选择不同的基础几何（动能）和密度路径的正则化（势能），可以对最优输运的动力学形式进行推广。这些组合产生不同的变分问题（Lagrangians），涵盖了许多最优输运问题的变体，如Schrödinger桥、不平衡最优输运和带有物理约束的最优输运等。一般而言，最优密度路径是未知的，解决这些变分问题在计算上具有挑战性。借助拉格朗日对偶形式，我们提出了一个新颖的基于深度学习的框架，从统一的角度处理所有这些问题。我们的方法不需要模拟或反向传播学习动力学的轨迹，也不需要访问最优耦合。我们展示了所提出框架的多功能性，通过超越了其他方法的表现。

    The dynamical formulation of the optimal transport can be extended through various choices of the underlying geometry ($\textit{kinetic energy}$), and the regularization of density paths ($\textit{potential energy}$). These combinations yield different variational problems ($\textit{Lagrangians}$), encompassing many variations of the optimal transport problem such as the Schr\"odinger bridge, unbalanced optimal transport, and optimal transport with physical constraints, among others. In general, the optimal density path is unknown, and solving these variational problems can be computationally challenging. Leveraging the dual formulation of the Lagrangians, we propose a novel deep learning based framework approaching all of these problems from a unified perspective. Our method does not require simulating or backpropagating through the trajectories of the learned dynamics, and does not need access to optimal couplings. We showcase the versatility of the proposed framework by outperformin
    
[^129]: TacticAI:一种足球战术的人工智能助手

    TacticAI: an AI assistant for football tactics. (arXiv:2310.10553v1 [cs.LG])

    [http://arxiv.org/abs/2310.10553](http://arxiv.org/abs/2310.10553)

    提出了TacticAI，一种与利物浦足球俱乐部的领域专家密切合作开发和评价的AI足球战术助手。TacticAI能够通过预测和生成的方式帮助教练们分析角球情况，并为每个角球惯例选择成功可能性最高的球员配置。

    

    辨别对手团队实施的战术关键模式并开发有效的应对方法是现代足球的核心问题。然而，以算法的方式来解决这个问题仍是一个未解决的研究挑战。为了解决这个需求，我们提出了TacticAI，一种与利物浦足球俱乐部的领域专家密切合作开发和评价的AI足球战术助手。我们专注于分析角球，因为它们给教练们提供了直接的干预和改进机会。TacticAI包含了一个预测和生成的组件，使教练能够有效地采样和探索每个角球惯例的替代球员配置，并选择那些预测成功可能性最高的。我们通过一些相关的基准任务对TacticAI进行了验证：预测接收球员和射门尝试以及推荐球员位置调整。TacticAI的实用性通过与利物浦足球领域专家进行的定性研究得到了验证。

    Identifying key patterns of tactics implemented by rival teams, and developing effective responses, lies at the heart of modern football. However, doing so algorithmically remains an open research challenge. To address this unmet need, we propose TacticAI, an AI football tactics assistant developed and evaluated in close collaboration with domain experts from Liverpool FC. We focus on analysing corner kicks, as they offer coaches the most direct opportunities for interventions and improvements. TacticAI incorporates both a predictive and a generative component, allowing the coaches to effectively sample and explore alternative player setups for each corner kick routine and to select those with the highest predicted likelihood of success. We validate TacticAI on a number of relevant benchmark tasks: predicting receivers and shot attempts and recommending player position adjustments. The utility of TacticAI is validated by a qualitative study conducted with football domain experts at Liv
    
[^130]: ReMax:一种用于对齐大型语言模型的简单、有效和高效的强化学习方法

    ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. (arXiv:2310.10505v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10505](http://arxiv.org/abs/2310.10505)

    ReMax是一种用于对齐大型语言模型的简单、有效和高效的强化学习方法，相比于PPO，ReMax简化了实现，减少了内存使用，并解决了fine-tuning时的内存溢出问题。

    

    对齐对于训练大型语言模型（LLMs）非常重要。目前解决这个问题的主要策略是通过从人类反馈中进行强化学习（RLHF），其中PPO是事实上的算法。然而，众所周知，PPO在计算效率上存在问题，这是本论文试图解决的挑战。我们在RLHF任务中确定了三个重要特性：快速模拟、确定性转换和轨迹级奖励，这些特性在PPO中没有得到充分利用。基于这些观察，我们开发了一种针对RLHF的新算法，称为ReMax。ReMax的算法设计是基于一种广为使用的算法REINFORCE，但配备了一种新的方差减少技术。我们的方法相对于PPO具有三重优势：首先，ReMax实现简单，消除了PPO中的许多与规模相关且繁琐的超参数。其次，ReMax原则上可以节约约50%的内存使用。结果导致PPO在进行fine-tuning时出现内存溢出的问题。

    Alignment is of critical importance for training large language models (LLMs). The predominant strategy to address this is through Reinforcement Learning from Human Feedback (RLHF), where PPO serves as the de-facto algorithm. Yet, PPO is known to suffer from computational inefficiency, which is a challenge that this paper aims to address. We identify three important properties in RLHF tasks: fast simulation, deterministic transitions, and trajectory-level rewards, which are not leveraged in PPO. Based on such observations, we develop a new algorithm tailored for RLHF, called ReMax. The algorithm design of ReMax is built on a celebrated algorithm REINFORCE but is equipped with a new variance-reduction technique.  Our method has three-fold advantages over PPO: first, ReMax is simple to implement and removes many hyper-parameters in PPO, which are scale-sensitive and laborious to tune. Second, ReMax saves about 50% memory usage in principle. As a result, PPO runs out-of-memory when fine-t
    
[^131]: 自主学习技能：利用大型语言模型指导学习解决新任务

    Bootstrap Your Own Skills: Learning to Solve New Tasks with Large Language Model Guidance. (arXiv:2310.10021v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.10021](http://arxiv.org/abs/2310.10021)

    BOSS是一种通过最小监督和大型语言模型指导来自动学习解决新任务的方法，在初始技能集之外的任务中，代理不接收奖励反馈。通过该方法，BOSS能够从基本的原始技能中构建出各种复杂有用的行为。

    

    我们提出了BOSS，一种通过最小监督来自动学习解决长时程、复杂且有意义的新任务的方法。传统的强化学习方法需要专家的监督，以示范或富含奖励函数的形式来学习长时程任务。相反，我们的BOSS方法通过执行“技能引导”来学习完成新任务，其中一个具备一组原始技能的代理与环境互动，在初始技能集之外的任务中不接收奖励反馈。这种引导阶段由大型语言模型(LLMs)指导，向代理提供有意义的技能组合。通过这个过程，BOSS能够从基本的原始技能中构建出各种复杂有用的行为。我们在逼真的家庭环境中的实验中证明，通过使用我们的LLM引导引导的BOSS训练的代理可以成功学习并解决新任务。

    We propose BOSS, an approach that automatically learns to solve new long-horizon, complex, and meaningful tasks by growing a learned skill library with minimal supervision. Prior work in reinforcement learning require expert supervision, in the form of demonstrations or rich reward functions, to learn long-horizon tasks. Instead, our approach BOSS (BOotStrapping your own Skills) learns to accomplish new tasks by performing "skill bootstrapping," where an agent with a set of primitive skills interacts with the environment to practice new skills without receiving reward feedback for tasks outside of the initial skill set. This bootstrapping phase is guided by large language models (LLMs) that inform the agent of meaningful skills to chain together. Through this process, BOSS builds a wide range of complex and useful behaviors from a basic set of primitive skills. We demonstrate through experiments in realistic household environments that agents trained with our LLM-guided bootstrapping p
    
[^132]: 关于车辆路径优化的分支界定算法的统计学习（arXiv：2310.09986v2 [cs.LG] 更新）

    On Statistical Learning of Branch and Bound for Vehicle Routing Optimization. (arXiv:2310.09986v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.09986](http://arxiv.org/abs/2310.09986)

    本文研究了车辆路径优化中的分支界定算法的统计学习，并比较了三个神经网络模型在容量限制车辆路径问题中的表现。实验证明，这种方法在性能上能够与分支界定算法相匹配或有所改进，并且需要更少的计算资源。

    

    最近，机器学习的分支界定算法已经显示出在近似求解NP困难问题方面的潜力。在本文中，我们利用并全面比较了三个神经网络的结果 - 图卷积神经网络（GCNN），GraphSAGE和图注意网络（GAT） - 来解决容量限制车辆路径问题。我们训练这些神经网络以模拟计算密集型的强分支策略的决策过程。神经网络在CVRLIB的六个具有不同拓扑结构的实例上进行训练，并在另外八个实例上进行评估。此外，我们将求解CVRP实例所需的最小车辆数减少到了一个装箱问题中，并以类似的方式进行了处理。通过严格的实验证明，我们发现这种方法可以与使用强分支策略的分支界定算法的性能相匹配或改进，同时需要的计算资源 significantly less comp.

    Recently, machine learning of the branch and bound algorithm has shown promise in approximating competent solutions to NP-hard problems. In this paper, we utilize and comprehensively compare the outcomes of three neural networks--graph convolutional neural network (GCNN), GraphSAGE, and graph attention network (GAT)--to solve the capacitated vehicle routing problem. We train these neural networks to emulate the decision-making process of the computationally expensive Strong Branching strategy. The neural networks are trained on six instances with distinct topologies from the CVRPLIB and evaluated on eight additional instances. Moreover, we reduced the minimum number of vehicles required to solve a CVRP instance to a bin-packing problem, which was addressed in a similar manner. Through rigorous experimentation, we found that this approach can match or improve upon the performance of the branch and bound algorithm with the Strong Branching strategy while requiring significantly less comp
    
[^133]: 评估特征选择在股市价格预测中的性能，以确定最有效的技术指标

    Evaluation of feature selection performance for identification of best effective technical indicators on stock market price prediction. (arXiv:2310.09903v1 [q-fin.ST])

    [http://arxiv.org/abs/2310.09903](http://arxiv.org/abs/2310.09903)

    本研究评估了特征选择方法在股市价格预测中的性能，通过选择最佳的技术指标组合来实现最少误差的预测。研究结果表明，不同的包装器特征选择方法在不同的机器学习方法中具有不同的表现。

    

    鉴于技术指标对股市预测的影响，特征选择对选择最佳指标至关重要。一种考虑在特征选择过程中模型性能的特征选择方法是包装器特征选择方法。本研究旨在通过特征选择鉴定出最少误差的预测股市价格的最佳股市指标组合。为评估包装器特征选择技术对股市预测的影响，本文在过去10年苹果公司的数据上使用了10个评估器和123个技术指标进行了SFS和SBS的考察。此外，通过提出的方法，将由3天时间窗口创建的数据转化为适用于回归方法的输入。从观察结果可以得出：（1）每种包装器特征选择方法在不同的机器学习方法中具有不同的结果，每种方法在不同的预测准确性上也有所不同。

    Due to the influence of many factors, including technical indicators on stock market prediction, feature selection is important to choose the best indicators. One of the feature selection methods that consider the performance of models during feature selection is the wrapper feature selection method. The aim of this research is to identify a combination of the best stock market indicators through feature selection to predict the stock market price with the least error. In order to evaluate the impact of wrapper feature selection techniques on stock market prediction, in this paper SFS and SBS with 10 estimators and 123 technical indicators have been examined on the last 10 years of Apple Company. Also, by the proposed method, the data created by the 3-day time window were converted to the appropriate input for regression methods. Based on the results observed: (1) Each wrapper feature selection method has different results with different machine learning methods, and each method is mor
    
[^134]: Mirage: 图分类的模型无关图蒸馏

    Mirage: Model-Agnostic Graph Distillation for Graph Classification. (arXiv:2310.09486v1 [cs.LG])

    [http://arxiv.org/abs/2310.09486](http://arxiv.org/abs/2310.09486)

    Mirage是一种模型无关的图蒸馏算法，旨在构建一个更小的合成训练集，以在资源有限的环境中实现图分类，并克服了现有算法对完整数据集的依赖性和对建模流程变化的敏感性。

    

    GNNs和其他深度学习模型一样，对数据和计算需求量很大。急需在大型数据集上扩展GNN的训练，以便在资源有限的环境中使用它们。图蒸馏是为此目的而努力，旨在从原始训练数据构建一个更小的合成训练集，而不会显著影响模型性能。虽然初步工作取得了一些进展，但这项工作基于两个关键观察：(1)现有的图蒸馏算法本身依赖于使用完整数据集进行训练，这就破坏了图蒸馏的前提。(2)蒸馏过程对目标GNN架构和超参数具有特异性，因此对建模流程的变化不具备鲁棒性。我们通过设计一种名为Mirage的图分类蒸馏算法来避免这些限制。Mirage建立在一个洞察的基础上，即一个消息传递的GNN将输入图分解为计算的多重集合。

    GNNs, like other deep learning models, are data and computation hungry. There is a pressing need to scale training of GNNs on large datasets to enable their usage on low-resource environments. Graph distillation is an effort in that direction with the aim to construct a smaller synthetic training set from the original training data without significantly compromising model performance. While initial efforts are promising, this work is motivated by two key observations: (1) Existing graph distillation algorithms themselves rely on training with the full dataset, which undermines the very premise of graph distillation. (2) The distillation process is specific to the target GNN architecture and hyper-parameters and thus not robust to changes in the modeling pipeline. We circumvent these limitations by designing a distillation algorithm called Mirage for graph classification. Mirage is built on the insight that a message-passing GNN decomposes the input graph into a multiset of computation 
    
[^135]: LLM训练中的分词选择：微不足道还是至关重要？

    Tokenizer Choice For LLM Training: Negligible or Crucial?. (arXiv:2310.08754v1 [cs.LG])

    [http://arxiv.org/abs/2310.08754](http://arxiv.org/abs/2310.08754)

    在LLM训练中，分词器的选择对模型的后续性能、成本有着显著影响，常见的分词器评估指标不一定预测模型的性能。

    

    近期LLM的成功主要是由于策划训练数据集、扩展模型架构和数据集规模，以及预训练目标的进步，而分词器的影响则是一个盲点。通过对24个单语和多语言LLM进行训练，并对不同的分词器算法和参数进行大范围实验，我们对分词器选择对LLM的后续性能、训练和推理成本的影响进行了全面研究。我们的研究表明，分词器选择对模型的后续性能、训练和推理成本有着显著影响。特别是，我们发现常见的分词器评估指标（如丰富度和平等性）并不总是对模型的后续性能具有预测能力，这使得这些指标成为对分词器评估的可疑选择。此外，我们还展示了针对五种最常见的欧洲语言训练的多语言分词器需要词汇表的大小。

    The recent success of LLMs has been predominantly driven by curating the training dataset composition, scaling of model architectures and dataset sizes and advancements in pretraining objectives, leaving tokenizer influence as a blind spot. Shedding light on this underexplored area, we conduct a comprehensive study on the influence of tokenizer choice on LLM downstream performance by training 24 mono- and multilingual LLMs at a 2.6B parameter scale, ablating different tokenizer algorithms and parameterizations. Our studies highlight that the tokenizer choice can significantly impact the model's downstream performance, training and inference costs. In particular, we find that the common tokenizer evaluation metrics fertility and parity are not always predictive of model downstream performance, rendering these metrics a questionable choice for tokenizer evaluation. Furthermore, we show that multilingual tokenizers trained on the five most frequent European languages require vocabulary si
    
[^136]: 用RNA对比学习改进预测准确性

    Splicing Up Your Predictions with RNA Contrastive Learning. (arXiv:2310.08738v1 [cs.LG])

    [http://arxiv.org/abs/2310.08738](http://arxiv.org/abs/2310.08738)

    本研究将对比学习技术扩展到基因组数据，利用选择性剪接和基因复制产生的序列之间的功能相似性，学习到广义RNA同位素表示。我们的预训练策略在RNA半衰期和平均核糖体负载预测等任务上取得了竞争性的结果，在低数据条件下皮尔逊相关性增加了多达两倍。

    

    鉴于基因组数据迅速积累，我们对RNA调控代码的理解尚不完整。近期在其他领域的自我监督方法已经证明了学习数据生成过程中的规则（例如语言中的句子结构）的能力。受此启发，我们通过利用通过选择性剪接和基因复制产生的序列之间的功能相似性，将对比学习技术扩展到基因组数据上。我们的新颖数据集和对比目标使得学习到广义RNA同位素表示。我们在RNA半衰期和平均核糖体负载预测等下游任务上证明了它们的实用性。我们的预训练策略在两项任务的线性探索中取得了竞争性的结果，并在低数据条件下皮尔逊相关性增加了多达两倍。重要的是，我们对学习的潜在空间的探索揭示了我们对比目标的语义有意义的表示。

    In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Recent self-supervised methods in other domains have demonstrated the ability to learn rules underlying the data-generating process such as sentence structure in language. Inspired by this, we extend contrastive learning techniques to genomic data by utilizing functional similarities between sequences generated through alternative splicing and gene duplication. Our novel dataset and contrastive objective enable the learning of generalized RNA isoform representations. We validate their utility on downstream tasks such as RNA half-life and mean ribosome load prediction. Our pre-training strategy yields competitive results using linear probing on both tasks, along with up to a two-fold increase in Pearson correlation in low-data conditions. Importantly, our exploration of the learned latent space reveals that our contrastive objective yields semantically meaningful representa
    
[^137]: LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models

    LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])

    [http://arxiv.org/abs/2310.08659](http://arxiv.org/abs/2310.08659)

    本论文提出了LoftQ：一种针对大型语言模型的LoRA精调感知量化框架。该框架同时对LLM进行量化，并为LoRA精调找到适当的低秩初始化，以缓解量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。

    

    量化是为大型语言模型提供服务的不可或缺的技术，并最近被应用于LoRA精调中。本文关注在预训练模型上同时应用量化和LoRA精调的场景。在这种情况下，常常观察到完整精调和量化加LoRA精调方法之间在下游任务表现上存在一致的差距。为了解决这个问题，我们提出了LoftQ（LoRA-Fine-Tuning-aware Quantization）——一种新的量化框架，用于同时对LLM进行量化，并找到适当的低秩初始化来进行LoRA精调。这种初始化减轻了量化模型和全精度模型之间的差异，并显著提高了下游任务的泛化能力。我们在自然语言理解、问答、摘要和自然语言生成任务上评估了我们的方法。实验证明，我们的方法非常有效，在性能上优于现有的方法。

    Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
    
[^138]: 探索有限领域知识传递的基本限制

    Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])

    [http://arxiv.org/abs/2310.07838](http://arxiv.org/abs/2310.07838)

    本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。

    

    我们对通过从教师到概率化学生分类器的n个样本进行知识传递的统计效率进行了表征，其中输入空间S和标签A为有限域。我们发现，在三个渐进级别上的特权信息可以加快传递的速度。在第一级别上，只有具有困难标签的样本是已知的，最大似然估计器能够达到最小化速率sqrt(|S||A|/n)。第二级别上，除了已知的困难标签样本外，还有采样标签的教师概率可用，这将收敛速度的下界提高到|S||A|/n。然而，在第二个数据采集协议下，最小化交叉熵损失的朴素适应会导致渐近偏差的学生。我们克服了这个限制，并通过使用一种新颖的经验变体的平方误差逻辑损失来实现了基本限制。第三级别进一步赋予学生软标签。

    We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
    
[^139]: BioT5：在生物学中利用化学知识和自然语言关联丰富跨模态整合

    BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations. (arXiv:2310.07276v1 [cs.CL])

    [http://arxiv.org/abs/2310.07276](http://arxiv.org/abs/2310.07276)

    BioT5是一个全面的预训练框架，在生物学中利用化学知识和自然语言关联丰富了跨模态整合，通过鲁棒的分子表示和上下文知识提取，实现了更有效的信息利用，展现出卓越的性能。

    

    最近在生物研究领域的进展利用分子、蛋白质和自然语言的整合来增强药物发现。然而，当前的模型存在一些限制，如生成无效的分子SMILES、对上下文信息的利用不足以及对结构化和非结构化知识的等量处理。为了解决这些问题，我们提出了一个全面的预训练框架BioT5，它通过化学知识和自然语言关联丰富了生物学中的跨模态整合。BioT5利用SELFIES进行100%鲁棒的分子表示，并从非结构化的生物文献中提取生物实体周围上下文的知识。此外，BioT5区分结构化和非结构化知识，从而更有效地利用信息。在微调后，BioT5在各种任务中展现出卓越的性能，表明其强大的能力。

    Recent advancements in biological research leverage the integration of molecules, proteins, and natural language to enhance drug discovery. However, current models exhibit several limitations, such as the generation of invalid molecular SMILES, underutilization of contextual information, and equal treatment of structured and unstructured knowledge. To address these issues, we propose $\mathbf{BioT5}$, a comprehensive pre-training framework that enriches cross-modal integration in biology with chemical knowledge and natural language associations. $\mathbf{BioT5}$ utilizes SELFIES for $100%$ robust molecular representations and extracts knowledge from the surrounding context of bio-entities in unstructured biological literature. Furthermore, $\mathbf{BioT5}$ distinguishes between structured and unstructured knowledge, leading to more effective utilization of information. After fine-tuning, BioT5 shows superior performance across a wide range of tasks, demonstrating its strong capability 
    
[^140]: NeuroInspect：通过类条件可视化实现的可解释的基于神经元的调试框架

    NeuroInspect: Interpretable Neuron-based Debugging Framework through Class-conditional Visualizations. (arXiv:2310.07184v1 [cs.CV])

    [http://arxiv.org/abs/2310.07184](http://arxiv.org/abs/2310.07184)

    NeuroInspect是一个基于神经元的可解释的调试框架，通过确定网络中导致错误的神经元并可视化嵌入其中的特征，提供了人类可解释的解释。引入了CLIP-Illusion来生成特征图像，并以类为条件来考察神经元与决策层之间的联系。

    

    尽管深度学习在各个领域取得了显著进展，但深度学习模型仍然容易出错。这个问题需要深度学习从业者使用有效的调试工具来解释网络中的决策过程。然而，现有的调试方法常常需要额外的数据或调整决策过程，限制了它们的适用性。为了解决这个问题，我们提出了NeuroInspect，这是一个基于神经元的可解释的调试框架，包括三个关键阶段：反事实解释、特征可视化和虚假相关性削减。我们的调试框架首先确定网络中导致错误的神经元，然后可视化嵌入在这些神经元中的特征，以便人类解释。为了提供这些解释，我们引入了CLIP-Illusion，一种新颖的特征可视化方法，它生成代表特征的图像，并以类为条件来考察神经元与决策层之间的联系。

    Despite deep learning (DL) has achieved remarkable progress in various domains, the DL models are still prone to making mistakes. This issue necessitates effective debugging tools for DL practitioners to interpret the decision-making process within the networks. However, existing debugging methods often demand extra data or adjustments to the decision process, limiting their applicability. To tackle this problem, we present NeuroInspect, an interpretable neuron-based debugging framework with three key stages: counterfactual explanations, feature visualizations, and false correlation mitigation. Our debugging framework first pinpoints neurons responsible for mistakes in the network and then visualizes features embedded in the neurons to be human-interpretable. To provide these explanations, we introduce CLIP-Illusion, a novel feature visualization method that generates images representing features conditioned on classes to examine the connection between neurons and the decision layer. W
    
[^141]: FABind: 快速准确的蛋白-配体结合

    FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06763](http://arxiv.org/abs/2310.06763)

    FABind是一个结合了口袋预测和对接的端到端模型，旨在实现快速准确的蛋白-配体结合预测。

    

    在药物发现中，对蛋白质和配体之间的相互作用进行建模并准确预测其结合结构是一项关键但具有挑战性的任务。深度学习的最新进展在应对这一挑战方面显示出了希望，采样法和回归法成为两种突出的方法。然而，这些方法都存在明显的局限性。采样法通常由于需要生成多个候选结构来进行选择而效率较低。而回归法提供了快速的预测，但可能会导致准确性降低。另外，蛋白质大小的变化通常需要外部模块来选择合适的结合口袋，进一步影响效率。在这项工作中，我们提出了FABind，一个将口袋预测和对接相结合的端到端模型，以实现准确和快速的蛋白-配体结合。

    Modeling the interaction between proteins and ligands and accurately predicting their binding structures is a critical yet challenging task in drug discovery. Recent advancements in deep learning have shown promise in addressing this challenge, with sampling-based and regression-based methods emerging as two prominent approaches. However, these methods have notable limitations. Sampling-based methods often suffer from low efficiency due to the need for generating multiple candidate structures for selection. On the other hand, regression-based methods offer fast predictions but may experience decreased accuracy. Additionally, the variation in protein sizes often requires external modules for selecting suitable binding pockets, further impacting efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model that combines pocket prediction and docking to achieve accurate and fast protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed pocket prediction
    
[^142]: 离线模仿学习与变分逆向推理

    Offline Imitation Learning with Variational Counterfactual Reasoning. (arXiv:2310.04706v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.04706](http://arxiv.org/abs/2310.04706)

    该论文提出了一个名为OILCA的框架，利用可识别的变分自动编码器生成"对抗性"样本，以解决离线模仿学习中数据稀缺、环境变化等问题。

    

    在离线模仿学习中，智能体旨在学习一种最优的专家行为策略，而不需要额外的在线环境交互。然而，在许多真实场景中，例如机器人操作中，离线数据集是从没有奖励的次优行为中收集来的。由于专家数据稀缺，智能体通常只能简单地记住贫乏的轨迹，并且容易受到环境变化的影响，缺乏对新环境的泛化能力。为了有效地消除会对智能体造成偏差并阻碍泛化的伪特征，我们提出了一个名为OILCA的框架，即离线模仿学习与对抗数据增强。具体来说，我们利用可识别的变分自动编码器生成"对抗性"样本。我们从理论上分析了对抗性识别和泛化的改善。

    In offline Imitation Learning (IL), an agent aims to learn an optimal expert behavior policy without additional online environment interactions. However, in many real-world scenarios, such as robotics manipulation, the offline dataset is collected from suboptimal behaviors without rewards. Due to the scarce expert data, the agents usually suffer from simply memorizing poor trajectories and are vulnerable to the variations in the environments, lacking the capability of generalizing to new environments. To effectively remove spurious features that would otherwise bias the agent and hinder generalization, we propose a framework named \underline{O}ffline \underline{I}mitation \underline{L}earning with \underline{C}ounterfactual data \underline{A}ugmentation (OILCA). In particular, we leverage the identifiable variational autoencoder to generate \textit{counterfactual} samples. We theoretically analyze the counterfactual identification and the improvement of generalization. Moreover, we con
    
[^143]: 超越一视同仁：多目标直接偏好优化

    Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])

    [http://arxiv.org/abs/2310.03708](http://arxiv.org/abs/2310.03708)

    本文提出了一种无强化学习的算法，称为多目标直接偏好优化（MODPO），它可以根据不同的偏好训练不同的语言模型，通过组合所有目标和特定权重来优化模型。

    

    语言模型（LM）通过强化学习与人类反馈的协同作用，能够很好地与普通标记者保持一致，但可能不适应各种各样的人类偏好。因此，最近的研究方法选择通过收集多维度反馈并为每个维度创建不同的奖励（例如，有益性，无害性，诚实性）进行个性化。通过使用不同的奖励权重，可以通过多目标强化学习（MORL）将LM调整到不同的偏好。然而，强化学习的微调在MORLHF中不稳定且耗费资源，特别是因为各种常常矛盾的目标。在本文中，我们提出了多目标直接偏好优化（MODPO），这是一种无强化学习的算法，它将直接偏好优化（DPO）扩展到多个对齐目标。基本上，MODPO通过训练不同的LM来代表不同的集体奖励模型，这些模型将所有目标和特定权重进行组合。通过简单的交叉熵损失，LM根据MOD进行优化。

    Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
    
[^144]: zkFL: 基于零知识证明的联邦学习梯度聚合

    zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])

    [http://arxiv.org/abs/2310.02554](http://arxiv.org/abs/2310.02554)

    zkFL是一种基于零知识证明的联邦学习梯度聚合方法，通过提供每轮的证明来解决协调者恶意行为的问题。

    

    联邦学习是一种机器学习范式，使多个分散的客户端在中央协调者的组织下共同训练一个模型。传统的联邦学习解决方案依赖于对中央协调者的信任，它以公平诚实的方式形成客户端的群体。然而，在现实中，恶意的协调者可能会放弃并替换客户端的训练模型，或者发动虚假客户端的肆意攻击。这种恶意行为让协调者在联邦学习环境中拥有更多控制客户端和决定最终训练结果的权力。本文介绍了zkFL，它利用零知识证明(ZKPs)来解决训练模型聚合过程中的恶意协调者问题。为了保证正确的聚合结果，协调者需要每轮提供一个证明。这个证明可以向客户端证明协调者忠实执行预期行为。为了进一步保护客户端隐私和数据安全，我们还引入了差分隐私机制，并对zkFL进行了实验评估。

    Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
    
[^145]: 解码图像：释放大型语言模型。

    Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])

    [http://arxiv.org/abs/2309.16705](http://arxiv.org/abs/2309.16705)

    该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。

    

    在一个挑战-响应研究中，我们对Google Bard进行了64个视觉挑战，旨在探究多模态大型语言模型（LLMs）的能力。这些挑战涵盖了各种类别，包括“视觉情境推理”，“视觉文本推理”和“下一场景预测”等，以确定Bard在融合视觉和语言分析方面的能力。我们的研究结果显示，Bard倾向于根据图片做出有根据的猜测，特别是在确定图片中的线索时。与GPT4等其他模型不同，Bard似乎不依赖于像Tesseract这样的光学字符识别库，而是像Google Lens和Visual API这样的深度学习模型一样，识别复杂图片中的文本。显着的是，Bard可以通过视觉方式解决ChatGPT无法理解的验证码，推荐使用Tesseract解决方案。此外，虽然Bard模型基于视觉输入提出了解决方案，但它无法重建或修改原始的视觉对象来支持其结论。

    In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
    
[^146]: 大规模Transformer训练不稳定性的小规模代理

    Small-scale proxies for large-scale Transformer training instabilities. (arXiv:2309.14322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14322](http://arxiv.org/abs/2309.14322)

    本文研究了大规模Transformer训练中的不稳定性问题，并找到了对应的小规模代理模型来复现和研究这些问题。研究人员发现训练不稳定性的两个源头，并表明先前使用的缓解方法在小规模训练中同样有效。这个发现有助于将缓解方法推广到大规模训练中。

    

    已有研究团队在训练大规模Transformer模型时报告了在小规模训练中未出现的训练不稳定性问题。尽管这些不稳定性问题的原因具有科学意义，但需要大量资源进行复现，使得研究变得困难。本文旨在寻找在小规模上复现和研究训练的稳定性和不稳定性的方法。首先，我们关注先前工作中描述的两个训练不稳定性的源头：注意力层中logits的增长（Dehghani等人，2023）和输出logits与对数概率之间的发散（Chowdhery等人，2022）。通过在各个尺度上测量学习率和损失之间的关系，我们发现当以高学习率训练小模型时，这些不稳定性也会出现，并且先前在大规模上使用的缓解方法在这个情景下同样有效。这促使我们进一步研究程度上是否可以将这些缓解方法推广到大规模训练中。

    Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training stability and instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to
    
[^147]: 基于Wav2vec的言语失语症检测和严重程度级别分类

    Wav2vec-based Detection and Severity Level Classification of Dysarthria from Speech. (arXiv:2309.14107v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.14107](http://arxiv.org/abs/2309.14107)

    本研究使用基于wav2vec的模型，实现了言语失语症的自动检测和严重程度级别分类任务，并在准确率上取得了显著的提升。

    

    通过声学语音信号的自动检测和严重程度级别分类，可以作为医学诊断中的工具用于言语失语症。本研究使用预训练的wav2vec 2.0模型作为特征提取器，构建了言语失语症的检测和严重程度级别分类系统。实验使用了广泛使用的UA-speech数据库。在检测实验中，结果表明使用wav2vec模型的第一层嵌入效果最好，相比于最佳基准特征（声谱图），准确率提高了1.23%。在研究的严重程度级别分类任务中，结果表明最终层的嵌入相比于最佳基准特征（梅尔频率倒谱系数）准确率提高了10.62%。

    Automatic detection and severity level classification of dysarthria directly from acoustic speech signals can be used as a tool in medical diagnosis. In this work, the pre-trained wav2vec 2.0 model is studied as a feature extractor to build detection and severity level classification systems for dysarthric speech. The experiments were carried out with the popularly used UA-speech database. In the detection experiments, the results revealed that the best performance was obtained using the embeddings from the first layer of the wav2vec model that yielded an absolute improvement of 1.23% in accuracy compared to the best performing baseline feature (spectrogram). In the studied severity level classification task, the results revealed that the embeddings from the final layer gave an absolute improvement of 10.62% in accuracy compared to the best baseline features (mel-frequency cepstral coefficients).
    
[^148]: 使用声门源特征进行病理性声音的分析和检测

    Analysis and Detection of Pathological Voice using Glottal Source Features. (arXiv:2309.14080v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.14080](http://arxiv.org/abs/2309.14080)

    本研究提供了对声门源特征的系统分析，并研究了它们在声音病理检测中的有效性。实验结果表明声门源包含的信息对于病理性声音的判别具有重要作用。

    

    自动检测声音病理能够实现客观评估和早期干预诊断。本研究对声门源特征进行了系统分析，并研究了它们在声音病理检测中的有效性。通过使用准封闭相位（QCP）的声门逆滤波方法估计得到的声门流动、使用零频率滤波（ZFF）方法计算得到的近似声门源信号，以及直接使用声学声音信号来提取声门源特征。此外，我们提出从由QCP和ZFF计算出的声门源波形中推导出梅尔频率倒谱系数（MFCCs），以有效捕捉病理性声音的声门源频谱变化。实验使用了两个数据库，即医院Universitario Principe de Asturias（HUPA）数据库和Saarbrucken Voice Disorders（SVD）数据库。特征分析揭示了声门源中包含的信息的判别作用。

    Automatic detection of voice pathology enables objective assessment and earlier intervention for the diagnosis. This study provides a systematic analysis of glottal source features and investigates their effectiveness in voice pathology detection. Glottal source features are extracted using glottal flows estimated with the quasi-closed phase (QCP) glottal inverse filtering method, using approximate glottal source signals computed with the zero frequency filtering (ZFF) method, and using acoustic voice signals directly. In addition, we propose to derive mel-frequency cepstral coefficients (MFCCs) from the glottal source waveforms computed by QCP and ZFF to effectively capture the variations in glottal source spectra of pathological voice. Experiments were carried out using two databases, the Hospital Universitario Principe de Asturias (HUPA) database and the Saarbrucken Voice Disorders (SVD) database. Analysis of features revealed that the glottal source contains information that discri
    
[^149]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^150]: 无监督神经网络在逆问题中的收敛和恢复性能保证

    Convergence and Recovery Guarantees of Unsupervised Neural Networks for Inverse Problems. (arXiv:2309.12128v1 [cs.LG])

    [http://arxiv.org/abs/2309.12128](http://arxiv.org/abs/2309.12128)

    本研究通过探索连接理论和实践，提供了无监督神经网络在解决逆问题中的收敛和恢复性能保证。同时，我们还得出了对于两层具有平滑激活函数的深度逆先验网络的超参数化界限，该网络将从我们的保证中受益。

    

    近年来，神经网络已成为解决逆问题的主要方法。虽然已经有很多这样的方法被提出来经验性地解决逆问题，但我们仍然缺乏对这些方法的明确理论保证。另一方面，许多研究已经证明，通过过参数化来控制神经切向核，神经网络可以在更通用的设置下收敛到最优解。在这项工作中，我们探索如何连接这两个领域，并为无监督前馈多层神经网络解决逆问题的训练过程提供确定性的收敛和恢复性能保证。我们还推导出超参数化界限，在这些界限下，具有平滑激活函数的两层深度逆先验网络将受益于我们的保证。

    Neural networks have become a prominent approach to solve inverse problems in recent years. While a plethora of such methods was developed to solve inverse problems empirically, we are still lacking clear theoretical guarantees for these methods. On the other hand, many works proved convergence to optimal solutions of neural networks in a more general setting using overparametrization as a way to control the Neural Tangent Kernel. In this work we investigate how to bridge these two worlds and we provide deterministic convergence and recovery guarantees for the class of unsupervised feedforward multilayer neural networks trained to solve inverse problems. We also derive overparametrization bounds under which a two-layers Deep Inverse Prior network with smooth activation function will benefit from our guarantees.
    
[^151]: Q-Transformer：通过自回归Q-函数提供可扩展的离线强化学习

    Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions. (arXiv:2309.10150v1 [cs.RO])

    [http://arxiv.org/abs/2309.10150](http://arxiv.org/abs/2309.10150)

    Q-Transformer是一种可扩展的离线强化学习方法，通过使用Transformer来表示Q函数并利用离线数据集进行训练。它在大规模真实世界机器人操作任务中表现优越。

    

    在这项工作中，我们提出了一种可扩展的强化学习方法，用于训练可以利用人类演示和自主采集数据的大型离线数据集的多任务策略。我们的方法使用Transformer提供可扩展的Q函数表示，通过离线时差备份进行训练。因此，我们将该方法称为Q-Transformer。通过将每个动作维度进行离散化，并将每个动作维度的Q值表示为单独的标记，我们可以应用高容量序列建模技术进行Q学习。我们提出了几个设计决策，使其在离线RL训练中表现出良好性能，并展示了Q-Transformer在大规模多样化的真实世界机器人操作任务套件上优于以往的离线RL算法和模仿学习技术。该项目的网站和视频可以在https://q-transformer.github.io找到。

    In this work, we present a scalable reinforcement learning method for training multi-task policies from large offline datasets that can leverage both human demonstrations and autonomously collected data. Our method uses a Transformer to provide a scalable representation for Q-functions trained via offline temporal difference backups. We therefore refer to the method as Q-Transformer. By discretizing each action dimension and representing the Q-value of each action dimension as separate tokens, we can apply effective high-capacity sequence modeling techniques for Q-learning. We present several design decisions that enable good performance with offline RL training, and show that Q-Transformer outperforms prior offline RL algorithms and imitation learning techniques on a large diverse real-world robotic manipulation task suite. The project's website and videos can be found at https://q-transformer.github.io
    
[^152]: 在视觉变换器中用ReLU替换softmax

    Replacing softmax with ReLU in Vision Transformers. (arXiv:2309.08586v1 [cs.CV])

    [http://arxiv.org/abs/2309.08586](http://arxiv.org/abs/2309.08586)

    在视觉变换器中，用ReLU替换softmax的注意力机制可以在计算性能上接近或匹配softmax注意力，并且通过序列长度进行除法可以缓解精度下降的问题。

    

    先前的研究观察到当将注意力softmax替换为ReLU这样的逐点激活时，精度会下降。在视觉变换器的背景下，我们发现当通过序列长度除以注意力下降被缓解。我们在ImageNet-21k数据集上训练小型到大型视觉变换器的实验表明，在计算方面，ReLU注意力可以达到或匹配softmax注意力的性能。

    Previous research observed accuracy degradation when replacing the attention softmax with a point-wise activation such as ReLU. In the context of vision transformers, we find that this degradation is mitigated when dividing by sequence length. Our experiments training small to large vision transformers on ImageNet-21k indicate that ReLU-attention can approach or match the performance of softmax-attention in terms of scaling behavior as a function of compute.
    
[^153]: 联邦正交训练：减轻连续联邦学习中的全局灾难性遗忘

    Federated Orthogonal Training: Mitigating Global Catastrophic Forgetting in Continual Federated Learning. (arXiv:2309.01289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01289](http://arxiv.org/abs/2309.01289)

    本研究提出了一种名为联邦正交训练（FOT）的方法，用于解决连续联邦学习中的全局灾难性遗忘问题，该方法克服了现有方法对过去数据的不切实际假设和隐私原则的违反。

    

    联邦学习（FL）因其能够实现分散数据上的隐私保护训练而受到了极大的关注。当前联邦学习领域的文献主要集中在单任务学习上。然而，随着时间的推移，客户端可能会出现新的任务，全局模型应该在不遗忘之前任务的情况下学习这些任务。这种真实场景被称为连续联邦学习（CFL）。CFL面临的主要挑战是全局灾难性遗忘，即当全局模型在新任务上训练时，其在旧任务上的性能下降。近期有一些关于CFL的研究提出了解决全局灾难性遗忘问题的方法。然而，这些方法要么对过去数据样本的可用性做出了不切实际的假设，要么违反了FL的隐私原则。我们提出了一种新方法，联邦正交训练（FOT），以克服这些缺点并解决全局灾难性遗忘问题。

    Federated Learning (FL) has gained significant attraction due to its ability to enable privacy-preserving training over decentralized data. Current literature in FL mostly focuses on single-task learning. However, over time, new tasks may appear in the clients and the global model should learn these tasks without forgetting previous tasks. This real-world scenario is known as Continual Federated Learning (CFL). The main challenge of CFL is Global Catastrophic Forgetting, which corresponds to the fact that when the global model is trained on new tasks, its performance on old tasks decreases. There have been a few recent works on CFL to propose methods that aim to address the global catastrophic forgetting problem. However, these works either have unrealistic assumptions on the availability of past data samples or violate the privacy principles of FL. We propose a novel method, Federated Orthogonal Training (FOT), to overcome these drawbacks and address the global catastrophic forgetting
    
[^154]: 基于领域自适应的消息传递图神经网络

    Domain-adaptive Message Passing Graph Neural Network. (arXiv:2308.16470v1 [cs.LG])

    [http://arxiv.org/abs/2308.16470](http://arxiv.org/abs/2308.16470)

    本论文提出了一个基于领域自适应的消息传递图神经网络(DM-GNN)，用于解决跨网络节点分类问题。该方法通过结合图神经网络和条件对抗领域自适应，能够学习可传递的节点分类信息。具体而言，通过双特征提取器构建GNN编码器，同时利用标签传播节点分类器和标签感知的传播方案来提高节点分类的准确性和泛化性能。

    

    近来，跨网络节点分类(CNNC)吸引了越来越多的关注，旨在通过从具有丰富标签的源网络中转移知识，对标签不充分的目标网络中的节点进行分类。为了解决CNNC问题，我们提出了一种结合了图神经网络(GNN)和条件对抗领域自适应的领域自适应消息传递图神经网络(DM-GNN)，能够学习可传递的节点分类信息。首先，通过双特征提取器构建了GNN编码器，将自我嵌入学习与邻居嵌入学习分离，以共同捕捉连接节点之间的共性和差异。其次，提出了一种标签传播节点分类器，通过将节点自身预测和邻居预测相结合来细化每个节点的标签预测。此外，针对有标签源网络设计了标签感知的传播方案，以促进标签的传播。

    Cross-network node classification (CNNC), which aims to classify nodes in a label-deficient target network by transferring the knowledge from a source network with abundant labels, draws increasing attention recently. To address CNNC, we propose a domain-adaptive message passing graph neural network (DM-GNN), which integrates graph neural network (GNN) with conditional adversarial domain adaptation. DM-GNN is capable of learning informative representations for node classification that are also transferrable across networks. Firstly, a GNN encoder is constructed by dual feature extractors to separate ego-embedding learning from neighbor-embedding learning so as to jointly capture commonality and discrimination between connected nodes. Secondly, a label propagation node classifier is proposed to refine each node's label prediction by combining its own prediction and its neighbors' prediction. In addition, a label-aware propagation scheme is devised for the labeled source network to promo
    
[^155]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^156]: 机器学习在微生物生物合成中的应用：提高效率和范围的先进技术综述

    Applications of machine Learning to improve the efficiency and range of microbial biosynthesis: a review of state-of-art techniques. (arXiv:2308.13877v1 [q-bio.SC])

    [http://arxiv.org/abs/2308.13877](http://arxiv.org/abs/2308.13877)

    本文综述了机器学习在微生物生物合成中的应用，并提供了对两个关键领域的全面解释和应用的现状及问题。

    

    在现代世界中，技术达到了顶峰。已经探索了不同的编程和技术途径用于数据分析、自动化和机器人技术。机器学习是优化数据分析、进行准确预测和加速/改进现有功能的关键。因此，目前正在发展人工智能领域的机器学习，并且正在探索其在不同领域的应用。其中一个突出的领域就是微生物生物合成。本文提供了微生物生物合成中不同机器学习程序的综合概述，并对机器学习和微生物生物合成领域分别进行了简要描述。该信息包括过去的趋势、现代的发展、未来的改进、流程的解释以及当前面临的问题。因此，本文的主要贡献是梳理了两个关键领域的发展，并提供了对其应用的全面解释。

    In the modern world, technology is at its peak. Different avenues in programming and technology have been explored for data analysis, automation, and robotics. Machine learning is key to optimize data analysis, make accurate predictions, and hasten/improve existing functions. Thus, presently, the field of machine learning in artificial intelligence is being developed and its uses in varying fields are being explored. One field in which its uses stand out is that of microbial biosynthesis. In this paper, a comprehensive overview of the differing machine learning programs used in biosynthesis is provided, alongside brief descriptions of the fields of machine learning and microbial biosynthesis separately. This information includes past trends, modern developments, future improvements, explanations of processes, and current problems they face. Thus, this paper's main contribution is to distill developments in, and provide a holistic explanation of, 2 key fields and their applicability to 
    
[^157]: 通过等变扩散模型进行基于形状的3D分子生成

    Shape-conditioned 3D Molecule Generation via Equivariant Diffusion Models. (arXiv:2308.11890v1 [cs.LG])

    [http://arxiv.org/abs/2308.11890](http://arxiv.org/abs/2308.11890)

    本文提出了一个基于形状的分子生成问题，通过等变形状引导的生成模型ShapeMol成功生成了新颖、多样且类似给定形状条件的药物样分子。

    

    配体基药物设计旨在识别与已知活性分子形状相似的新型药物候选物。本文提出了一个基于形状的分子生成问题，即在给定分子的形状条件下生成3D分子结构。为了解决这个问题，我们开发了一个等变形状引导的生成模型ShapeMol。ShapeMol由一个等变形状编码器和一个基于这些编码生成3D分子的等变扩散模型组成。实验结果表明，ShapeMol能够生成新颖、多样且类似给定形状条件的药物样分子。这些结果展示了ShapeMol在设计具有所需3D形状并与蛋白靶点结合的药物候选物方面的潜力。

    Ligand-based drug design aims to identify novel drug candidates of similar shapes with known active molecules. In this paper, we formulated an in silico shape-conditioned molecule generation problem to generate 3D molecule structures conditioned on the shape of a given molecule. To address this problem, we developed a translation- and rotation-equivariant shape-guided generative model ShapeMol. ShapeMol consists of an equivariant shape encoder that maps molecular surface shapes into latent embeddings, and an equivariant diffusion model that generates 3D molecules based on these embeddings. Experimental results show that ShapeMol can generate novel, diverse, drug-like molecules that retain 3D molecular shapes similar to the given shape condition. These results demonstrate the potential of ShapeMol in designing drug candidates of desired 3D shapes binding to protein target pockets.
    
[^158]: 人工智能中的元启发式算法及其在生物信息学、生物统计学、生态学和制造业中的应用

    Metaheuristic Algorithms in Artificial Intelligence with Applications to Bioinformatics, Biostatistics, Ecology and, the Manufacturing Industries. (arXiv:2308.10875v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2308.10875](http://arxiv.org/abs/2308.10875)

    这篇论文介绍了受自然启发的元启发式算法在人工智能中的重要性和应用，并提出了一种新算法CSO-MA，通过多个优化问题的应用展示了其灵活性和优越性能。

    

    受自然启发的元启发式算法是人工智能的重要组成部分，并在不同学科领域中应用于解决各种类型的挑战性优化问题。我们应用了一种新提出的受自然启发的元启发式算法，称为具有突变代理的竞争性群体优化器(CSO-MA)，并证明了它相对于竞争对手在统计科学中各种优化问题上的灵活性和超越性能。特别是，我们展示了该算法高效且可以整合各种成本结构或多个用户指定的非线性约束。我们的应用包括(i)在生物信息学中通过单细胞广义趋势模型找到参数的最大似然估计以研究伪时态，(ii) 估计教育研究中常用的Rasch模型的参数，(iii) 在马尔可夫更新模型中为Cox回归找到M-估计，(iv) 矩阵补全以填补两个连连不通图中的缺失值。

    Nature-inspired metaheuristic algorithms are important components of artificial intelligence, and are increasingly used across disciplines to tackle various types of challenging optimization problems. We apply a newly proposed nature-inspired metaheuristic algorithm called competitive swarm optimizer with mutated agents (CSO-MA) and demonstrate its flexibility and out-performance relative to its competitors in a variety of optimization problems in the statistical sciences. In particular, we show the algorithm is efficient and can incorporate various cost structures or multiple user-specified nonlinear constraints. Our applications include (i) finding maximum likelihood estimates of parameters in a single cell generalized trend model to study pseudotime in bioinformatics, (ii) estimating parameters in a commonly used Rasch model in education research, (iii) finding M-estimates for a Cox regression in a Markov renewal model and (iv) matrix completion to impute missing values in a two com
    
[^159]: 用于16位神经网络训练中数值不稳定性的高效方法

    An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])

    [http://arxiv.org/abs/2307.16189](http://arxiv.org/abs/2307.16189)

    这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。

    

    在这项研究中，我们深入探讨了在16位计算中使用流行的优化算法（如RMSProp和Adam）时观察到的数值不稳定性的复杂性。这种不稳定性通常在深度神经网络的训练阶段中出现，导致学习过程受到干扰，从而妨碍了这些模型的有效部署。我们确定了单一超参数epsilon是这种数值不稳定性的主要原因。对16位计算中这些优化器中epsilon的作用进行了深入探索，发现微调其值可以恢复RMSProp和Adam的功能，从而实现有效利用16位神经网络。我们提出了一种新的方法来减轻被发现的数值不稳定性问题。该方法利用Adam优化器的更新，并显著改善了16位神经网络的学习过程的鲁棒性。

    In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
    
[^160]: 当拒绝学习对具有拒绝的回归问题最优时

    When No-Rejection Learning is Optimal for Regression with Rejection. (arXiv:2307.02932v1 [cs.LG])

    [http://arxiv.org/abs/2307.02932](http://arxiv.org/abs/2307.02932)

    本文研究了具有拒绝的回归问题，并调查了将其视为标准回归任务来学习预测器的无拒绝学习策略。

    

    拒绝学习是研究人类和人工智能在预测任务上相互作用的典型模型。该模型包括一个预测器和一个拒绝器。在样本到达时，拒绝器首先决定是否接受它；如果接受，预测器完成预测任务；如果被拒绝，则将预测推迟给人类。学习问题需要同时学习预测器和拒绝器。这改变了传统损失函数的结构，通常导致非凸性和一致性问题。对于带有拒绝的分类问题，一些研究开发了代理损失函数，同时具有可验证的一致性保证；与此同时，关于回归问题的研究较少。我们研究了带有拒绝的回归问题并研究了将其视为标准回归任务来学习预测器的无拒绝学习策略。

    Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish tha
    
[^161]: 零代价神经架构搜索：挑战、解决方案和机遇

    Zero-Shot Neural Architecture Search: Challenges, Solutions, and Opportunities. (arXiv:2307.01998v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.01998](http://arxiv.org/abs/2307.01998)

    零代价神经架构搜索是一种不需要训练的方法，其核心思想是设计能够预测网络精确度的代理。本文综述了最新的零代价神经架构搜索方法，并在硬件感知和硬件无感知的NAS场景中展示了其有效性。

    

    最近，零代价（或无需训练）神经架构搜索（NAS）方法被提出来将NAS从昂贵的训练过程中解放出来。零代价NAS方法的关键思想是设计能够预测某些给定网络精确度的代理，而无需训练网络参数。到目前为止，已经提出的代理通常受到深度学习的理论理解的最新进展的启发，并在几个数据集和NAS基准测试上显示出了巨大潜力。本文旨在全面审查和比较最先进的零代价NAS方法，重点关注它们对硬件的意识。为此，我们首先回顾主流的零代价代理并讨论它们的理论基础。然后，我们通过大规模实验比较这些零代价代理，并展示它们在硬件感知和硬件无感知的NAS场景中的有效性。最后，我们指出了设计更好的代理的几个有前途的思路。

    Recently, zero-shot (or training-free) Neural Architecture Search (NAS) approaches have been proposed to liberate NAS from the expensive training process. The key idea behind zero-shot NAS approaches is to design proxies that can predict the accuracy of some given networks without training the network parameters. The proxies proposed so far are usually inspired by recent progress in theoretical understanding of deep learning and have shown great potential on several datasets and NAS benchmarks. This paper aims to comprehensively review and compare the state-of-the-art (SOTA) zero-shot NAS approaches, with an emphasis on their hardware awareness. To this end, we first review the mainstream zero-shot proxies and discuss their theoretical underpinnings. We then compare these zero-shot proxies through large-scale experiments and demonstrate their effectiveness in both hardware-aware and hardware-oblivious NAS scenarios. Finally, we point out several promising ideas to design better proxies
    
[^162]: 使用生成对抗网络生成无监督文本嵌入空间用于文本合成

    Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])

    [http://arxiv.org/abs/2306.17181](http://arxiv.org/abs/2306.17181)

    本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。

    

    生成对抗网络（GAN）是一种用于数据合成的模型，通过生成器和判别器的竞争来创建逼真的数据。尽管GAN在图像合成方面得到了广泛研究，但在自然语言生成方面存在固有的限制。因为自然语言由离散的标记组成，生成器在通过反向传播更新梯度时遇到困难；因此，大多数文本-GAN研究使用奖励系统以随机标记为基础生成句子。因此，先前研究中的生成器在对抗训练之前以自回归方式进行预训练，导致合成的句子重复训练数据。在本文中，我们使用类似原始GAN的框架来合成句子。更具体地说，我们提出了文本嵌入空间生成对抗网络（TESGAN），它生成连续的文本嵌入空间来解决梯度反向传播的问题。

    Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
    
[^163]: 线性赌博机中平衡性能与理论保证的几何感知方法

    Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits. (arXiv:2306.14872v1 [cs.LG])

    [http://arxiv.org/abs/2306.14872](http://arxiv.org/abs/2306.14872)

    本文提出了一种新的数据驱动技术，跟踪不确定度椭球体的几何形状，为线性赌博机算法建立实例相关的频率后悔界，并实现了平衡算法性能与理论保证的效果。

    

    本文受线性赌博机算法表现良好的实证性能与悲观理论后悔界之间的不一致性启发，提出一种新的数据驱动技术，跟踪不确定度椭球体的几何形状，为包括贪心、OFUL和汤普森抽样算法在内的广泛算法类建立实例相关的频率后悔界，在保留基本算法大部分优良特性的同时“校正”基本算法在某些实例中表现差的问题，实现了渐近最优后悔界。我们通过仿真实验验证了该方法的有效性。

    This paper is motivated by recent developments in the linear bandit literature, which have revealed a discrepancy between the promising empirical performance of algorithms such as Thompson sampling and Greedy, when compared to their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometry of the uncertainty ellipsoid, enabling us to establish an instance-dependent frequentist regret bound for a broad class of algorithms, including Greedy, OFUL, and Thompson sampling. This result empowers us to identify and ``course-correct" instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\tilde{\mathcal{O}}(d\sqrt{T})$, while retaining most of the desirable properties of the base algorithms. We present sim
    
[^164]: 利用梯度对抗不确定性：通过扩散分数匹配实现离线强化学习

    Fighting Uncertainty with Gradients: Offline Reinforcement Learning via Diffusion Score Matching. (arXiv:2306.14079v1 [cs.LG])

    [http://arxiv.org/abs/2306.14079](http://arxiv.org/abs/2306.14079)

    本文提出了平滑的距离数据度量标准，并将其与离线强化学习相结合，以对抗不确定性和分布偏移的挑战。该方法不仅在最小化梯度不确定性时稳定收敛到数据，而且不易低估真实不确定性，是一种有前途的策略搜索方法。

    

    离线优化范式，例如离线强化学习（RL）或模仿学习（IL），允许策略搜索算法利用离线数据，但需要仔细处理不确定性以避免分布偏移的挑战。由于其在高维度中的有效性，基于梯度的策略搜索方法是一种有前途的方向；然而，我们需要更仔细地考虑这些方法如何与不确定性估计相互影响。我们声称，为了让不确定性度量适用于基于梯度的优化，它必须在最小化梯度不确定性时稳定地收敛到数据，并且不易低估真实不确定性。我们研究了平滑的数据距离作为度量标准，并展示了它不仅稳定地收敛到数据，而且还允许我们通过Lipschitz常数来分析模型偏差。此外，我们建立了平滑的数据距离和数据似然之间的等价性。

    Offline optimization paradigms such as offline Reinforcement Learning (RL) or Imitation Learning (IL) allow policy search algorithms to make use of offline data, but require careful incorporation of uncertainty in order to circumvent the challenges of distribution shift. Gradient-based policy search methods are a promising direction due to their effectiveness in high dimensions; however, we require a more careful consideration of how these methods interplay with uncertainty estimation. We claim that in order for an uncertainty metric to be amenable for gradient-based optimization, it must be (i) stably convergent to data when uncertainty is minimized with gradients, and (ii) not prone to underestimation of true uncertainty. We investigate smoothed distance to data as a metric, and show that it not only stably converges to data, but also allows us to analyze model bias with Lipschitz constants. Moreover, we establish an equivalence between smoothed distance to data and data likelihood, 
    
[^165]: ALP: 动作感知的具身学习用于感知

    ALP: Action-Aware Embodied Learning for Perception. (arXiv:2306.10190v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.10190](http://arxiv.org/abs/2306.10190)

    ALP是一个动作感知的具身学习框架，通过将动作信息融入表示学习，可以学习可普遍应用的任务无关的视觉表示，并在复杂的三维环境中积极探索和收集训练数据。

    

    当前在训练和基准测试视觉模型中的方法过于依赖被动的、策划好的数据集。尽管这些数据集上训练的模型在分类、检测和分割等各种任务上显示出了强大的性能，但它们基本上无法适应一个不断变化的世界，因为输入数据的分布不断变化。因此，我们是否可以以更人类中心和自适应的方式来进行学习呢？在本文中，我们引入了动作感知的具身学习框架（ALP），通过优化强化学习策略和逆动力学预测目标的结合，将动作信息融入到表示学习中。我们的方法在复杂的三维环境中积极探索，既学习可普遍应用的任务无关的视觉表示，又收集下游训练数据。我们证明了ALP的表现优于现有方法。

    Current methods in training and benchmarking vision models exhibit an over-reliance on passive, curated datasets. Although models trained on these datasets have shown strong performance in a wide variety of tasks such as classification, detection, and segmentation, they fundamentally are unable to generalize to an ever-evolving world due to constant out-of-distribution shifts of input data. Therefore, instead of training on fixed datasets, can we approach learning in a more human-centric and adaptive manner? In this paper, we introduce Action-Aware Embodied Learning for Perception (ALP), an embodied learning framework that incorporates action information into representation learning through a combination of optimizing a reinforcement learning policy and an inverse dynamics prediction objective. Our method actively explores in complex 3D environments to both learn generalizable task-agnostic visual representations as well as collect downstream training data. We show that ALP outperforms
    
[^166]: 优化器的信息准则：剖析和纠正数据驱动优化中的偏差

    Optimizer's Information Criterion: Dissecting and Correcting Bias in Data-Driven Optimization. (arXiv:2306.10081v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10081](http://arxiv.org/abs/2306.10081)

    这项研究提出了一个称为优化器的信息准则(OIC)的通用偏差校正方法，帮助解决数据驱动优化中的乐观偏差问题。该方法直接近似一阶偏差，并且不需要解决额外的优化问题，是在决策选择方面的一个创新。

    

    在数据驱动的优化中，所得决策的样本表现通常存在着对真实表现的乐观偏差，这种现象通常被称为优化器的诅咒，与机器学习中的过拟合密切相关。传统的纠正这种偏差的技术，如交叉验证，需要反复解决额外的优化问题，因此计算代价很高。我们开发了一种通用的偏差校正方法，建立在我们称之为优化器的信息准则（OIC）的基础上，直接近似一阶偏差，不需要解决任何额外的优化问题。我们的OIC将著名的赤池信息准则推广到数据驱动优化中，关键是评估客观表现，不仅涉及模型拟合，还涉及其与下游优化的相互作用。因此，它可以用于决策选择而不仅仅是模型选择。我们将我们的方法应用于一系列问题的数据驱动优化。

    In data-driven optimization, the sample performance of the obtained decision typically incurs an optimistic bias against the true performance, a phenomenon commonly known as the Optimizer's Curse and intimately related to overfitting in machine learning. Common techniques to correct this bias, such as cross-validation, require repeatedly solving additional optimization problems and are therefore computationally expensive. We develop a general bias correction approach, building on what we call Optimizer's Information Criterion (OIC), that directly approximates the first-order bias and does not require solving any additional optimization problems. Our OIC generalizes the celebrated Akaike Information Criterion to evaluate the objective performance in data-driven optimization, which crucially involves not only model fitting but also its interplay with the downstream optimization. As such it can be used for decision selection instead of only model selection. We apply our approach to a rang
    
[^167]: 基于学习轮胎模型的三分钟数据自主漂移

    Autonomous Drifting with 3 Minutes of Data via Learned Tire Models. (arXiv:2306.06330v1 [eess.SY])

    [http://arxiv.org/abs/2306.06330](http://arxiv.org/abs/2306.06330)

    提出了一种基于神经常微分方程和神经-ExpTanh参数化的新型轮胎力模型，并将其作为现有非线性模型预测控制框架中解析刷式轮胎模型的替代品,成功实现了仅利用少于三分钟的驾驶数据，实现高性能的自主漂移

    

    在附着极限附近，轮胎所产生的力是非线性和错综复杂的。在这个区域内高效、准确的建模可以提高安全性，特别是在需要高力的紧急情况下。为此，我们提出了一种基于神经常微分方程和神经-ExpTanh参数化的新型轮胎力模型。这些模型旨在满足物理洞察力假设，同时具有足够的保真度，以直接从车辆状态测量中捕捉高阶效应。它们被用作现有非线性模型预测控制框架中解析刷式轮胎模型的替代品。通过使用Toyota Supra上的实验表明，少于三分钟的驾驶数据足以在各种轨迹上实现高性能的自主漂移，最高速度可达45mph。与基准模型的比较显示轨迹提高了4倍。

    Near the limits of adhesion, the forces generated by a tire are nonlinear and intricately coupled. Efficient and accurate modelling in this region could improve safety, especially in emergency situations where high forces are required. To this end, we propose a novel family of tire force models based on neural ordinary differential equations and a neural-ExpTanh parameterization. These models are designed to satisfy physically insightful assumptions while also having sufficient fidelity to capture higher-order effects directly from vehicle state measurements. They are used as drop-in replacements for an analytical brush tire model in an existing nonlinear model predictive control framework. Experiments with a customized Toyota Supra show that scarce amounts of driving data -- less than three minutes -- is sufficient to achieve high-performance autonomous drifting on various trajectories with speeds up to 45mph. Comparisons with the benchmark model show a $4 \times$ improvement in track
    
[^168]: CARSO: 对抗性合成观测的反对抗性召回

    CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])

    [http://arxiv.org/abs/2306.06081](http://arxiv.org/abs/2306.06081)

    本文提出了一种新的图像分类的对抗性防御机制CARSO，该方法可以比最先进的对抗性训练更好地保护分类器，通过利用生成模型进行对抗净化来进行最终分类，并成功地保护自己免受未预见的威胁和最终攻击。

    

    本文提出了一种新的对抗性防御机制CARSO，用于图像分类，灵感来自认知神经科学的线索。该方法与对抗训练具有协同互补性，并依赖于被攻击分类器的内部表示的知识。通过利用生成模型进行对抗净化，该方法采样输入的重构来进行最终分类。在各种图像数据集和分类器体系结构上进行的实验评估表明，CARSO能够比最先进的对抗性训练更好地保护分类器——同时具有可接受的清洁准确度损失。此外，防御体系结构成功地保护自己免受未预见的威胁和最终攻击。代码和预训练模型可在https://github.com/获得。

    In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
    
[^169]: 离线强化学习的样本内政策迭代方法

    In-Sample Policy Iteration for Offline Reinforcement Learning. (arXiv:2306.05726v1 [cs.LG])

    [http://arxiv.org/abs/2306.05726](http://arxiv.org/abs/2306.05726)

    本文提出了一种采用样本内策略迭代的算法来增强离线强化学习中的行为规则方法，在实验中取得了显著的改进。

    

    离线强化学习通过利用以前收集到的数据来推导出有效的控制策略。为了解决由于数据覆盖不足而导致的错误，行为规则方法优化控制策略的同时，同时最小化偏离数据收集策略的误差。然而，当离线数据集由次优策略收集时，这些方法经常表现出不佳的实际性能。在本文中，我们提出了一种采用样本内策略迭代的新算法，它在离线强化学习中显著增强了行为规则方法。核心见解是通过不断改进用于行为规则的策略，样本内政策迭代逐渐改进自身，同时隐式避免查询样本外的行动，以避免灾难性的学习失败。我们的理论分析验证了其学习仅利用数据集中良好覆盖的行动学习样本内最优策略的能力。此外，我们在四个任务上进行了广泛的实验，证明我们的算法在现实世界的离线强化学习应用中能够显著改进现有方法。

    Offline reinforcement learning (RL) seeks to derive an effective control policy from previously collected data. To circumvent errors due to inadequate data coverage, behavior-regularized methods optimize the control policy while concurrently minimizing deviation from the data collection policy. Nevertheless, these methods often exhibit subpar practical performance, particularly when the offline dataset is collected by sub-optimal policies. In this paper, we propose a novel algorithm employing in-sample policy iteration that substantially enhances behavior-regularized methods in offline RL. The core insight is that by continuously refining the policy used for behavior regularization, in-sample policy iteration gradually improves itself while implicitly avoids querying out-of-sample actions to avert catastrophic learning failures. Our theoretical analysis verifies its ability to learn the in-sample optimal policy, exclusively utilizing actions well-covered by the dataset. Moreover, we pr
    
[^170]: 相关信息最大化：一种无需权重对称的有监督深度神经网络的生物合理方法

    Correlative Information Maximization: A Biologically Plausible Approach to Supervised Deep Neural Networks without Weight Symmetry. (arXiv:2306.04810v1 [cs.NE])

    [http://arxiv.org/abs/2306.04810](http://arxiv.org/abs/2306.04810)

    本文提出了一种无需权重对称的有监督深度神经网络的生物合理方法，该方法利用相关信息最大化在层激活之间描述生物神经网络中的信号传播。通过坐标下降优化相应的目标和均方误差损失函数，可以产生一个更生物真实的神经网络结构。

    

    反向传播算法在训练大规模人工神经网络方面取得了显著的成功，但其生物合理性受到争议，现在仍然存在一个开放的问题，即大脑是否采用类似于它的监督学习机制。在本文中，我们提出了在层激活之间进行相关信息最大化的替代规范方法，以描述生物神经网络中信号在前向和后向方向上传播的机制。这种新框架解决了有关传统人工神经网络和反向传播算法生物合理性的许多问题。相应目标的坐标下降优化，与均方误差损失函数相结合，可以产生一个神经网络结构，模拟一种具有树突处理和侧抑制神经元的更生物真实的多室金字塔形神经元网络。

    The backpropagation algorithm has experienced remarkable success in training large-scale artificial neural networks, however, its biological-plausibility is disputed, and it remains an open question whether the brain employs supervised learning mechanisms akin to it. Here, we propose correlative information maximization between layer activations as an alternative normative approach to describe the signal propagation in biological neural networks in both forward and backward directions. This new framework addresses many concerns about the biological-plausibility of conventional artificial neural networks and the backpropagation algorithm. The coordinate descent-based optimization of the corresponding objective, combined with the mean square error loss function for fitting labeled supervision data, gives rise to a neural network structure that emulates a more biologically realistic network of multi-compartment pyramidal neurons with dendritic processing and lateral inhibitory neurons. Fu
    
[^171]: 置换等变图框架在异质半监督学习中的应用

    Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised Learning. (arXiv:2306.04265v1 [cs.LG])

    [http://arxiv.org/abs/2306.04265](http://arxiv.org/abs/2306.04265)

    本文介绍了一个用于异质半监督学习的新型图神经网络模型PEGFAN，它使用置换等变图框架实现了多尺度特征提取，表现优于其他最先进模型，特别是在相对较大和密集连接的数据集中。

    

    异质图的本质与同质图显著不同，这表明1-hop以外的聚合方式并引起早期图神经网络模型的困难。本文展示了一种新的多尺度提取方法，通过构建具有置换等变性，高效性和稀疏性的Haar-type图框架，在图上深度学习任务中实现。我们进一步使用我们构建的图框架设计了图框架神经网络模型PEGFAN。实验在合成数据集和9个基准数据集上进行，与其他最先进的模型进行性能比较。结果表明，我们的模型在某些异质图数据集（包括相对较大和更密集的连接的大部分异质数据集）上可以达到最佳性能，并在其余数据集上具有竞争性能。

    The nature of heterophilous graphs is significantly different with that of homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and causes difficulties in early graph neural network models. In this paper, we develop a new way to implement multi-scale extraction via constructing Haar-type graph framelets with desired properties of permutation equivariance, efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn a graph framelet neural network model PEGFAN using our constructed graph framelets. The experiments are conducted on a synthetic dataset and 9 benchmark datasets to compare performance with other state-of-the-art models. The result shows that our model can achieve best performance on certain datasets of heterophilous graphs (including the majority of heterophilous datasets with relatively larger sizes and denser connections) and competitive performance on the remaining.
    
[^172]: 旋转特征用于物体发现

    Rotating Features for Object Discovery. (arXiv:2306.00600v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00600](http://arxiv.org/abs/2306.00600)

    本文提出了旋转特征作为将复杂值特征推广到高纬度的方法，并提出了一种新的评估过程来提取分布式表示中的物体。这些进展使得我们能够在真实世界的数据中扩展分布式以物体为中心的表示。

    

    人类认知中的绑定问题涉及大脑如何在固定的神经连接网络中表示和连接物体，仍然存在激烈的争论。大多数无监督学习的机器学习方法都集中在基于插槽的方法上，由于其离散性和难以表达不确定性的特点，可能有一定的局限性。最近，复杂自动编码器被提出作为一个学习连续和分布式以物体为中心的代替方法。然而，它只适用于简单的玩具数据。在本文中，我们提出了旋转特征，将复值特征推广到更高维度，并提出了一种提取分布式表示中物体的新评估方法。此外，我们展示了我们的方法对预训练特征的适用性。这些进展使我们能够从简单的玩具数据扩展到真实世界数据中的分布式以物体为中心的表示。

    The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work 
    
[^173]: 大型语言模型不能作为抽象推理器

    Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])

    [http://arxiv.org/abs/2305.19555](http://arxiv.org/abs/2305.19555)

    本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。

    

    大型语言模型在自然语言处理任务上表现出极好的性能，包括文本理解和常识推理等。然而，这些成功的机制尚不清楚，LLMs是否能够达到人类的认知能力或这些模型是否还存在根本性的局限性也不确定。抽象推理是认知的基本任务，包括从少量数据中找到和应用一般模式。评估深度神经结构在这个任务上的表现可以揭示它们在推理方面的潜在局限性和广泛的泛化能力，这是一个目前未被探索的领域。本文对最先进的LLMs进行了大量评估，发现它们在抽象推理任务中的表现非常有限，并探究了造成这种差异的原因。

    Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
    
[^174]: 基于空间转换网络的舌部超声无声语音接口的适应性研究

    Adaptation of Tongue Ultrasound-Based Silent Speech Interfaces Using Spatial Transformer Networks. (arXiv:2305.19130v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2305.19130](http://arxiv.org/abs/2305.19130)

    采用空间转换网络模块，可使舌部超声图像为基础的无声语音接口模型快速适应到不同的用户和会话，且能显著降低均方误差。

    

    最新的深度学习算法能够在一定条件下，从发音运动数据中合成可懂的语音，这些成果来自于无声语音接口（SSI）。然而，所得到的模型往往特定于某一说话人，使得在不同用户之间进行快速切换变得麻烦。即使是同一个讲者，在不同时间进行模型应用效果也较差。为了帮助舌部超声图像为基础的 SSI 模型快速适应到不同的用户和会话，我们扩展了深度网络，并利用空间转换网络 (STN) 模块，能够对输入图像进行仿射变换。虽然 STN 只占网络的约 10%，但实验表明，仅适应 STN 模块就可以将均方误差平均减少 88%，而不重新训练整个网络。当将网络适应到同一讲者的不同记录会话时，改进效果更大（大约 92%）。

    Thanks to the latest deep learning algorithms, silent speech interfaces (SSI) are now able to synthesize intelligible speech from articulatory movement data under certain conditions. However, the resulting models are rather speaker-specific, making a quick switch between users troublesome. Even for the same speaker, these models perform poorly cross-session, i.e. after dismounting and re-mounting the recording equipment. To aid quick speaker and session adaptation of ultrasound tongue imaging-based SSI models, we extend our deep networks with a spatial transformer network (STN) module, capable of performing an affine transformation on the input images. Although the STN part takes up only about 10% of the network, our experiments show that adapting just the STN module might allow to reduce MSE by 88% on the average, compared to retraining the whole network. The improvement is even larger (around 92%) when adapting the network to different recording sessions from the same speaker.
    
[^175]: 用视觉动作转换器模拟任务和动作规划

    Imitating Task and Motion Planning with Visuomotor Transformers. (arXiv:2305.16309v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.16309](http://arxiv.org/abs/2305.16309)

    本研究提出了一种名为OPTIMUS的新型模仿学习系统，通过模仿TAMP代理来训练大规模的视觉动作转换器策略。OPTIMUS引入了一个专门为模仿学习而设计的TAMP数据生成管道，可以用来训练性能优越的基于转换器的策略。

    

    模仿学习是训练机器人操作策略的强大工具，使其能够从专家演示中学习而无需手动编程或试错。然而，常见的数据收集方法，如人工监督，因为耗时和劳动密集而难以扩展。相反，任务和动作规划（TAMP）可以自主地生成大规模的多样化演示数据集。在这项工作中，我们展示了由TAMP监督员生成的大规模数据集与灵活的Transformer模型相结合是机器人操作的强大范例。为此，我们提出了一种名为OPTIMUS的新型模仿学习系统，通过模仿TAMP代理来训练大规模的视觉动作转换器策略。OPTIMUS引入了一个专门为模仿学习而设计的TAMP数据生成管道，可以用来训练性能优越的基于转换器的策略。在本文中，我们对设计进行了全面的研究

    Imitation learning is a powerful tool for training robot manipulation policies, allowing them to learn from expert demonstrations without manual programming or trial-and-error. However, common methods of data collection, such as human supervision, scale poorly, as they are time-consuming and labor-intensive. In contrast, Task and Motion Planning (TAMP) can autonomously generate large-scale datasets of diverse demonstrations. In this work, we show that the combination of large-scale datasets generated by TAMP supervisors and flexible Transformer models to fit them is a powerful paradigm for robot manipulation. To that end, we present a novel imitation learning system called OPTIMUS that trains large-scale visuomotor Transformer policies by imitating a TAMP agent. OPTIMUS introduces a pipeline for generating TAMP data that is specifically curated for imitation learning and can be used to train performant transformer-based policies. In this paper, we present a thorough study of the design
    
[^176]: 当前机器学习需要多少样本才能利用平滑性？

    How many samples are needed to leverage smoothness?. (arXiv:2305.16014v1 [stat.ML])

    [http://arxiv.org/abs/2305.16014](http://arxiv.org/abs/2305.16014)

    本文通过研究泛化误差的新下界，探讨了学习平滑函数时需要的样本数量及其机器学习问题中的挑战。

    

    统计学习的核心原则之一是，目标函数的平滑性可以打破维度灾难。然而，通过泰勒展开学习平滑函数需要足够接近一起的样本来获得高阶导数的有意义估计，这在数据量相对较小的机器学习问题中似乎很困难。本文通过推导广义泛化误差的新的下界，研究了常数和瞬态区域在实践中通常被忽略却发挥了主导作用的问题。

    A core principle in statistical learning is that smoothness of target functions allows to break the curse of dimensionality. However, learning a smooth function through Taylor expansions requires enough samples close to one another to get meaningful estimate of high-order derivatives, which seems hard in machine learning problems where the ratio between number of data and input dimension is relatively small. Should we really hope to break the curse of dimensionality based on Taylor expansion estimation? What happens if Taylor expansions are replaced by Fourier or wavelet expansions? By deriving a new lower bound on the generalization error, this paper investigates the role of constants and transitory regimes which are usually not depicted beyond classical learning theory statements while that play a dominant role in practice.
    
[^177]: Sophia：一种用于语言模型预训练的可扩展的随机二阶优化器

    Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14342](http://arxiv.org/abs/2305.14342)

    Sophia是一种用于语言模型预训练的可扩展的二阶优化算法，使用对角Hessian作为预调节器，并进行元素级别的裁剪控制更新大小。

    

    鉴于语言模型预训练的巨大成本，优化算法的微小改进将会大大降低训练的时间和成本。Adam及其变种一直是最先进的，而更复杂的二阶（基于Hessian的）优化器往往会带来太多的每步开销。在这篇论文中，我们提出了Sophia，一种简单可扩展的二阶优化器，它使用轻量级估计的对角Hessian作为预调节器。更新步骤是梯度的移动平均值除以估计Hessian的移动平均值，然后进行元素级别的裁剪。裁剪控制了最坏情况下的更新大小，并控制了Hessian在轨迹上的非凸性和快速变化的负面影响。Sophia只在每几次迭代中估计对角Hessian，这几乎没有平均每步的时间和内存开销。在使用GPT m进行语言建模时，

    Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
    
[^178]: 论文标题：使ViT成形：计算-优化模型设计的缩放定律。

    Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.13035](http://arxiv.org/abs/2305.13035)

    本研究通过改进缩放定律方法推测出计算-优化模型形状，成功实现了形状优化视觉变换器SoViT，该模型在相同计算量下，取得了与超过其两倍大小的模型相竞争的结果。

    

    近期，缩放定律被用来推导在给定计算时间范围内的计算-优化模型大小（参数数量）。我们发展并改进了这些方法，以推测如宽度和深度等计算-优化模型形状，并在视觉变换器中成功实现了这一点。我们经过形状优化的视觉变换器SoViT，在仅使用相同数量的计算量进行预训练的情况下，取得了与超过其两倍大小的模型相竞争的结果。例如，SoViT-400m/14在ILSRCV2012上取得了90.3%的微调准确度，超过了更大的ViT-g/14，在相同设置下接近ViT-G/14，同时推断成本也不到一半。我们进行了多个任务的彻底评估，例如图像分类、字幕、VQA和零-shot转移，在广泛领域中展示了我们模型的有效性并确定了其限制。总体而言，我们的研究发现挑战了盲目扩大视觉模型的现有方法。

    Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models 
    
[^179]: 用于应对标签错误的增强元标签校正方法

    Enhanced Meta Label Correction for Coping with Label Corruption. (arXiv:2305.12961v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.12961](http://arxiv.org/abs/2305.12961)

    提出了一种用于带有噪声标签的学习问题的增强元标签校正方法（EMLC），通过重新审视元学习过程和引入更准确的元梯度推导，以及使用新颖的教师架构和训练目标，实现了在标准基准测试中取得的最先进结果。

    

    传统的处理带有噪声标签的学习方法已经成功处理了人为注入噪声的数据集，但仍然不能充分处理现实世界的噪声。随着元学习在机器学习的各个领域中的广泛应用，研究人员利用辅助的干净小数据集来元校正训练标签。然而，现有的元标签校正方法并没有充分发挥其潜力。在本研究中，我们提出了一种用于带有噪声标签的学习问题的增强元标签校正方法（简称EMLC），重新审视了元学习过程，并引入了更快速和更准确的元梯度推导。我们提出了一种新颖的针对带有噪声标签学习问题的教师架构，配备了新颖的训练目标。EMLC优于先前的方法，在所有标准基准测试中取得了最先进的结果。值得注意的是，EMLC提升了对噪声现实世界数据的先前研究成果。

    Traditional methods for learning with the presence of noisy labels have successfully handled datasets with artificially injected noise but still fall short of adequately handling real-world noise. With the increasing use of meta-learning in the diverse fields of machine learning, researchers leveraged auxiliary small clean datasets to meta-correct the training labels. Nonetheless, existing meta-label correction approaches are not fully exploiting their potential. In this study, we propose an Enhanced Meta Label Correction approach abbreviated as EMLC for the learning with noisy labels (LNL) problem. We re-examine the meta-learning process and introduce faster and more accurate meta-gradient derivations. We propose a novel teacher architecture tailored explicitly to the LNL problem, equipped with novel training objectives. EMLC outperforms prior approaches and achieves state-of-the-art results in all standard benchmarks. Notably, EMLC enhances the previous art on the noisy real-world da
    
[^180]: LLM自身可读取和生成CXR图像

    LLM Itself Can Read and Generate CXR Images. (arXiv:2305.11490v1 [cs.CV])

    [http://arxiv.org/abs/2305.11490](http://arxiv.org/abs/2305.11490)

    该论文提出了一种新方法，可以在不需要进行结构更改、额外训练、或训练专门网络的情况下，通过微调预先训练的LLM来读取和生成像文本一样的图像，并应用于胸部X线（CXR）图像的生成任务中。

    

    借助于近期大语言模型（LLMs）的显著发展，人们正积极尝试将LLMs的实用性扩展到多模态任务。已经有人尝试连接语言和视觉信息，并且也在不断尝试为LLMs添加视觉能力。然而，现有的尝试只使用LLMs作为图像解码器，没有尝试通过自然语言来生成图像。通过采用VQ-GAN框架，将图像的潜在表示视为一种文本标记，我们提出了一种新方法，可以微调预先训练的LLM，以像文本一样读取和生成图像，而无需进行结构更改、额外的训练目标或训练专门的网络，同时仍保留LLM的指令跟随能力。我们将此框架应用于胸部X线（CXR）图像的生成任务中，因为这是一个复杂信息在视觉和语言之间翻译的领域。

    Building on the recent remarkable development of large language models (LLMs), active attempts are being made to extend the utility of LLMs to multimodal tasks. There have been previous efforts to link language and visual information, and attempts to add visual capabilities to LLMs are ongoing as well. However, existing attempts use LLMs only as image decoders and no attempt has been made to generate images in the same line as the natural language. By adopting a VQ-GAN framework in which latent representations of images are treated as a kind of text tokens, we present a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM. We apply this framework to chest X-ray (CXR) image and report generation tasks as it is a domain in which translation of complex information between visual and 
    
[^181]: 权重具有无界方差的无限宽贝叶斯神经网络后验推断

    Posterior Inference on Infinitely Wide Bayesian Neural Networks under Weights with Unbounded Variance. (arXiv:2305.10664v1 [stat.ML])

    [http://arxiv.org/abs/2305.10664](http://arxiv.org/abs/2305.10664)

    本文提出了一种新的方法进行关于具有无界方差权重的贝叶斯神经网络的后验推断，并表明后验分布集中在具有非标准超参数依赖性的稀疏促进和均值收缩先验周围。

    

    由Neal（1996）的经典而有影响力的作品已知，具有一层隐藏层的贝叶斯神经网络的无限宽度标度极限是一个高斯过程，当网络权重具有有界先验方差时。Neal的结果已扩展到具有多个隐藏层和卷积神经网络的网络，也具有高斯过程标度极限。高斯过程的易处理属性允许直接的后验推断和不确定性量化，相比有限宽度的网络，极大地简化了极限过程的研究。然而，具有无界方差的神经网络权重面临着独特的挑战。在这种情况下，经典的中心极限定理失效，据适当条件下的稳定$\alpha$过程的标度极限的文献较多的是前向模拟，而在这些权重下的后验推断问题仍然是一个未解决的问题。在本文中，我们提出了关于具有无界方差权重的贝叶斯神经网络后验推断的新理论洞察力。具体而言，我们建立了一种新的后验收缩速率结果，并表明后验分布集中在具有非标准超参数依赖性的稀疏促进和均值收缩先验周围。

    From the classical and influential works of Neal (1996), it is known that the infinite width scaling limit of a Bayesian neural network with one hidden layer is a Gaussian process, \emph{when the network weights have bounded prior variance}. Neal's result has been extended to networks with multiple hidden layers and to convolutional neural networks, also with Gaussian process scaling limits. The tractable properties of Gaussian processes then allow straightforward posterior inference and uncertainty quantification, considerably simplifying the study of the limit process compared to a network of finite width. Neural network weights with unbounded variance, however, pose unique challenges. In this case, the classical central limit theorem breaks down and it is well known that the scaling limit is an $\alpha$-stable process under suitable conditions. However, current literature is primarily limited to forward simulations under these processes and the problem of posterior inference under s
    
[^182]: 选择性遗忘：深度生成模型中的持续学习方法

    Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])

    [http://arxiv.org/abs/2305.10120](http://arxiv.org/abs/2305.10120)

    针对大规模文本到图像模型可能被误用生成有害内容的问题，该论文提出了一种选择性遗忘方法，即持续学习方法，可在深度生成模型中实现可控的遗忘，用户可指定消除哪些概念。

    

    近年来，大规模文本到图像模型的广泛使用引发了人们对这些模型可能被误用生成有害、误导或不当内容的担忧。受此问题的启发，我们提出了一种受持续学习启发的技术，用于有选择性地遗忘预训练的深度生成模型中的概念。我们的方法称为选择性遗忘，可以实现可控的遗忘，用户可以指定该如何遗忘一个概念。选择性遗忘可应用于变分似然模型，涵盖了各种流行的深度生成框架，包括变分自编码器和大规模文本到图像扩散模型。不同模型上的实验证明，我们的方法可以诱导遗忘各种概念，从标准数据集中的整个类别到文本到图像模型中的名人和裸体提示。我们的代码可公开访问，网址为https://github.com/clear-nus/selective-amnesia。

    The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.
    
[^183]: 理解异构数据联邦学习中的模型平均

    Understanding Model Averaging in Federated Learning on Heterogeneous Data. (arXiv:2305.07845v1 [cs.LG])

    [http://arxiv.org/abs/2305.07845](http://arxiv.org/abs/2305.07845)

    本文研究了异构数据联邦学习中的模型平均技术，通过可视化损失/错误景观揭示了客户端模型环绕全局模型在一个共同的盆地内，并且发现全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    

    模型平均是联邦学习中广泛采用的一种技术，它会聚集训练于异构数据上的多个客户端模型以获得表现良好的全局模型。然而，其成功背后的原理尚不是很清楚。本文通过可视化损失/错误景观来研究模型平均的几何特性，揭示了客户端模型环绕全局模型在一个共同的盆地内，并且即使全局模型表现优异，也可能偏离盆地底部。进一步的分析表明，全局模型在早期训练后的误差主要来自客户端数据集和全局数据集之间非重叠的数据及全局模型与客户端模型之间的最大距离两个因素。

    Model averaging, a widely adopted technique in federated learning (FL), aggregates multiple client models trained on heterogeneous data to obtain a well-performed global model. However, the rationale behind its success is not well understood. To shed light on this issue, we investigate the geometric properties of model averaging by visualizing the loss/error landscape. The geometrical visualization shows that the client models surround the global model within a common basin, and the global model may deviate from the bottom of the basin even though it performs better than the client models. To further understand this phenomenon, we decompose the expected prediction error of the global model into five factors related to client models. Specifically, we find that the global-model error after early training mainly comes from i) the client-model error on non-overlapping data between client datasets and the global dataset and ii) the maximal distance between the global and client models. Insp
    
[^184]: 使用深度强化学习优化内存映射

    Optimizing Memory Mapping Using Deep Reinforcement Learning. (arXiv:2305.07440v1 [cs.PF])

    [http://arxiv.org/abs/2305.07440](http://arxiv.org/abs/2305.07440)

    本文提出了一种使用强化学习解决机器学习程序中内存映射问题的方法。

    

    资源调度和分配是许多高影响系统的关键组成部分，涵盖拥塞控制到云计算。在这篇论文中，我们专注于调度问题的一个特定实例，即编译机器学习程序期间出现的内存映射问题：即将张量映射到不同的内存层以优化执行时间。我们介绍了一种使用强化学习解决内存映射问题的方法。使用强化学习是解决顺序决策问题和高维数据输入组合搜索空间的解决方案。

    Resource scheduling and allocation is a critical component of many high impact systems ranging from congestion control to cloud computing. Finding more optimal solutions to these problems often has significant impact on resource and time savings, reducing device wear-and-tear, and even potentially improving carbon emissions. In this paper, we focus on a specific instance of a scheduling problem, namely the memory mapping problem that occurs during compilation of machine learning programs: That is, mapping tensors to different memory layers to optimize execution time.  We introduce an approach for solving the memory mapping problem using Reinforcement Learning. RL is a solution paradigm well-suited for sequential decision making problems that are amenable to planning, and combinatorial search spaces with high-dimensional data inputs. We formulate the problem as a single-player game, which we call the mallocGame, such that high-reward trajectories of the game correspond to efficient memo
    
[^185]: HINT:层次混合网络用于一致概率预测

    HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting. (arXiv:2305.07089v1 [stat.ML])

    [http://arxiv.org/abs/2305.07089](http://arxiv.org/abs/2305.07089)

    HINT是一种用于概率预测的新型模型族，能够有效、准确地进行一致性预测，通过引入Bootstrap方法并为网络加入规范化特征提取和输出规范化来保证其性能，在多个数据集上的预测精度比现有技术更高。

    

    我们提出了一种名为"Hierarchical Mixture Networks"（HINT）的模型族，用于有效而准确的一致性预测。我们通过多元混合并使用复合似然函数进行优化来专门针对该任务进行网络特化，并通过引入Bootstrap方法加以协调。此外，我们在网络中引入了规范化特征提取和输出规范化，以应对时间序列尺度变化。与现有最先进技术相比，我们展示了在五个数据集上的8％ sCRPS增强精度。我们对模型部件进行了消融研究并广泛研究了多元混合的理论性质。 HINT的代码可以在https://github.com/Nixtla/neuralforecast上获得。

    We present the Hierarchical Mixture Networks (HINT), a model family for efficient and accurate coherent forecasting. We specialize the networks on the task via a multivariate mixture optimized with composite likelihood and made coherent via bootstrap reconciliation. Additionally, we robustify the networks to stark time series scale variations, incorporating normalized feature extraction and recomposition of output scales within their architecture. We demonstrate 8% sCRPS improved accuracy across five datasets compared to the existing state-of-the-art. We conduct ablation studies on our model's components and extensively investigate the theoretical properties of the multivariate mixture. HINT's code is available at this https://github.com/Nixtla/neuralforecast.
    
[^186]: 离散系统的神经李雅普诺夫控制

    Neural Lyapunov Control for Discrete-Time Systems. (arXiv:2305.06547v1 [cs.LG])

    [http://arxiv.org/abs/2305.06547](http://arxiv.org/abs/2305.06547)

    该论文介绍了一种用于离散时间系统的神经李雅普诺夫控制方法，该方法利用混合整数线性规划来验证稳定性条件，计算子水平集刻画吸引域，有效地学习控制策略。

    

    尽管线性系统的稳定性已经被充分了解，但对于具有非线性动力学的系统仍然是一个主要的挑战。在这种情况下的一般方法是利用李雅普诺夫稳定性理论来计算李雅普诺夫控制函数和相关控制策略的组合。然而，对于一般的非线性系统寻找李雅普诺夫函数是一个具有挑战性的任务。为了解决这个挑战，最近提出了几种使用神经网络表示李雅普诺夫函数的方法。然而，这些方法仅针对连续时间系统设计。我们提出了第一种适用于离散时间系统学习神经李雅普诺夫控制的方法。三个关键要素使我们能够有效地学习可证明稳定的控制策略。第一个是通过混合整数线性规划来验证离散时间系统中的稳定性条件，这是一种新的方法。第二个是计算子水平集的一种新方法，该方法刻画了吸引域的区域

    While ensuring stability for linear systems is well understood, it remains a major challenge for systems with nonlinear dynamics. A general approach in such cases is to leverage Lyapunov stability theory to compute a combination of a Lyapunov control function and an associated control policy. However, finding Lyapunov functions for general nonlinear systems is a challenging task. To address this challenge, several methods have been recently proposed that represent Lyapunov functions using neural networks. However, such approaches have been designed exclusively for continuous-time systems. We propose the first approach for learning neural Lyapunov control in discrete-time systems. Three key ingredients enable us to effectively learn provably stable control policies. The first is a novel mixed-integer linear programming approach for verifying the stability conditions in discrete-time systems. The second is a novel approach for computing sub-level sets which characterize the region of att
    
[^187]: 自注意力动态中的聚类现象

    The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v1 [cs.LG])

    [http://arxiv.org/abs/2305.05465](http://arxiv.org/abs/2305.05465)

    本文证实了当Transformer处理一系列token时，出现“领导者”的经验观察，即随着时间趋于无穷大，代表token的粒子会聚集在特定的极限对象附近，这取决于价值矩阵的谱。

    

    将Transformer视为相互作用的粒子系统，当权重不随时间变化时，本文描述了学习表示的几何形状。我们展示了代表token的粒子随着时间趋于无穷大而趋向于特定的极限对象。出现的极限对象类型取决于价值矩阵的谱。此外，在一维情况下，我们证明了自我注意力矩阵收敛于低秩布尔矩阵。这些结果的组合在数学上证实了Vaswani等人的经验观察，即Transformer处理一系列token时会出现“领导者”。

    Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. The type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. \cite{vaswani2017attention} that \emph{leaders} appear in a sequence of tokens when processed by Transformers.
    
[^188]: 一个LLM知道自己在撒谎的内部状态

    The Internal State of an LLM Knows When its Lying. (arXiv:2304.13734v1 [cs.CL])

    [http://arxiv.org/abs/2304.13734](http://arxiv.org/abs/2304.13734)

    该论文研究了LLM生成不准确或虚假信息的问题，提出了一种简单而有效的方法，利用LLM的隐藏层激活来确定语句的真实性。在实验中，该方法表现出较好的检测效果，并有利于提高LLM的可信度。

    

    虽然大型语言模型（LLM）在各种任务中表现出了卓越的性能，但它们（可能）最为突出的缺点是以自信的语气生成不准确或虚假的信息。本文假设LLM的内部状态可以用于揭示一个语句的真实性。因此，我们介绍了一种简单但有效的方法来检测LLM所生成语句的真实性，该方法利用LLM的隐藏层激活来确定语句的真实性。为了训练和评估我们的方法，我们构建了一个包含六个不同主题的数据集，其中包含真实和虚假的语句。一个分类器被训练出来，根据LLM的激活值来检测哪个语句是真实的或虚假的。具体而言，分类器接收LLM为数据集中每个语句生成的激活值作为输入。我们的实验表明，我们检测语句真实性的方法甚至比少量提示方法表现更好，凸显了利用LLM的内部状态来提高其可信度的潜力。

    While Large Language Models (LLMs) have shown exceptional performance in various tasks, their (arguably) most prominent drawback is generating inaccurate or false information with a confident tone. In this paper, we hypothesize that the LLM's internal state can be used to reveal the truthfulness of a statement. Therefore, we introduce a simple yet effective method to detect the truthfulness of LLM-generated statements, which utilizes the LLM's hidden layer activations to determine the veracity of statements. To train and evaluate our method, we compose a dataset of true and false statements in six different topics. A classifier is trained to detect which statement is true or false based on an LLM's activation values. Specifically, the classifier receives as input the activation values from the LLM for each of the statements in the dataset. Our experiments demonstrate that our method for detecting statement veracity significantly outperforms even few-shot prompting methods, highlighting
    
[^189]: 大规模视觉语言模型的稳定和低精度训练

    Stable and low-precision training for large-scale vision-language models. (arXiv:2304.13013v1 [cs.LG])

    [http://arxiv.org/abs/2304.13013](http://arxiv.org/abs/2304.13013)

    该研究介绍了用于大规模视觉语言模型稳定和低精度训练的新方法，包括SwitchBack和AdamW-Adafacto方法。这些方法提高了训练速度和稳定性。

    

    我们介绍了新的方法，用于加速和稳定大语言-视觉模型的训练。为加速训练，我们引入了SwitchBack，这是一种线性层用于int8量化训练，其提供了13-25％的速度提升，而与1B参数CLIP ViT-Huge的bfloat16训练的性能相匹配，在目前为止是最大的int8训练。我们的重点是int8，因为GPU支持float8很少，虽然我们也通过模拟分析了float8训练。为了稳定训练，我们分析了损失峰值，并发现它们在二次梯度估计器的AdamW second moment之后1-8次迭代中一致发生低估。因此，我们推荐使用AdamW-Adafacto方法。

    We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) Towards accelerating training, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) Towards stable training, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafacto
    
[^190]: 可编辑用户档案的可控文本推荐方法

    Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])

    [http://arxiv.org/abs/2304.04250](http://arxiv.org/abs/2304.04250)

    本文提出了一种新的概念值瓶颈模型LACE，用于可控文本推荐。该模型基于用户文档学习个性化的概念表示，并通过多种交互方式为用户提供了控制推荐的机制，验证了在离线和在线实验中该模型的推荐质量和有效性。

    

    实现高质量推荐的方法通常依赖于从交互数据中学习潜在表示。然而这些方法没有提供给用户控制所接收的推荐的机制。本文提出了LACE，一种新颖的概念值瓶颈模型，用于可控文本推荐。LACE基于用户交互的文档检索，将每个用户表示为简洁的可读的概念集，并基于用户文档学习概念的个性化表示。该基于概念的用户档案被利用来做出推荐。我们的模型设计通过透明的用户档案，提供了控制推荐的多种直观交互方式。我们首先在三个推荐任务（温启动、冷启动和零样本）的六个数据集上进行了离线评估，验证了从LACE获得的推荐质量。接下来，我们在在线实验中验证了LACE的有效性和用户控制能力。

    Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
    
[^191]: 图混合专家：显式多样性建模下的大规模图学习

    Graph Mixture of Experts: Learning on Large-Scale Graphs with Explicit Diversity Modeling. (arXiv:2304.02806v1 [cs.LG])

    [http://arxiv.org/abs/2304.02806](http://arxiv.org/abs/2304.02806)

    本文提出了一种新的图混合专家（GMoE）模型，旨在解决现实世界中的图具有多样的图结构和包含异构节点和边的问题。该模型可以增强GNN的泛化能力，适应多样的训练图结构的能力，并且不会增加计算开销。

    

    图神经网络已被广泛应用于图数据的学习。然而，现实世界中的图通常具有多样的图结构，并且包含异构节点和边。为了增强GNN的泛化能力，进一步提高训练图结构的多样性已成为常见做法。但是，简单地增加GNN模型容量将会导致更高的推理成本和GNN难以训练的问题。本文将专家混合（MoE）的思想引入到GNN中，旨在增强其适应多样的训练图结构的能力，而不会增加计算开销。我们的新图混合专家（GMoE）模型使得图中的每个节点可以动态地选择其自己的最佳信息。

    Graph neural networks (GNNs) have been widely applied to learning over graph data. Yet, real-world graphs commonly exhibit diverse graph structures and contain heterogeneous nodes and edges. Moreover, to enhance the generalization ability of GNNs, it has become common practice to further increase the diversity of training graph structures by incorporating graph augmentations and/or performing large-scale pre-training on more graphs. Therefore, it becomes essential for a GNN to simultaneously model diverse graph structures. Yet, naively increasing the GNN model capacity will suffer from both higher inference costs and the notorious trainability issue of GNNs. This paper introduces the Mixture-of-Expert (MoE) idea to GNNs, aiming to enhance their ability to accommodate the diversity of training graph structures, without incurring computational overheads. Our new Graph Mixture of Expert (GMoE) model enables each node in the graph to dynamically select its own optimal \textit{information a
    
[^192]: 高效混合型晶圆缺陷图案识别使用紧凑型可形变卷积变换器

    Efficient Mixed-Type Wafer Defect Pattern Recognition Using Compact Deformable Convolutional Transformers. (arXiv:2303.13827v1 [cs.CV])

    [http://arxiv.org/abs/2303.13827](http://arxiv.org/abs/2303.13827)

    本文提出了一种紧凑型可形变卷积变换器（DC Transformer），能够高效地识别单一和混合型晶圆缺陷，聚焦于全局特征，准确预测缺陷的数目和类型。

    

    晶圆制造是一项复杂的任务，涉及数千个步骤。晶圆缺陷图案识别（DPR）对于找到问题的根本原因并进一步提高晶圆铸造的产量至关重要。与单一类型DPR相比，混合类型DPR由于空间特征的多样性，缺陷的不确定性和存在数目等原因更加复杂。为了准确预测缺陷的数目和类型，我们提出了一种新颖的紧凑型可形变卷积变换器（DC Transformer）。具体来说，DC Transformer通过可学习的可形变内核和多头注意力，聚焦于晶圆图中存在的全局特征。所提出的方法简洁地模拟了晶圆图和缺陷之间的内部关系。在一个包含38种缺陷模式的真实数据集上评估了DC Transformer。实验结果表明，DC Transformer在识别单一以及混合型晶圆缺陷方面表现得异常优秀。

    Manufacturing wafers is an intricate task involving thousands of steps. Defect Pattern Recognition (DPR) of wafer maps is crucial to find the root cause of the issue and further improving the yield in the wafer foundry. Mixed-type DPR is much more complicated compared to single-type DPR due to varied spatial features, the uncertainty of defects, and the number of defects present. To accurately predict the number of defects as well as the types of defects, we propose a novel compact deformable convolutional transformer (DC Transformer). Specifically, DC Transformer focuses on the global features present in the wafer map by virtue of learnable deformable kernels and multi-head attention to the global features. The proposed method succinctly models the internal relationship between the wafer maps and the defects. DC Transformer is evaluated on a real dataset containing 38 defect patterns. Experimental results show that DC Transformer performs exceptionally well in recognizing both single 
    
[^193]: 一种极其简单的芯片特征提取和缺陷模式识别方法

    An Embarrassingly Simple Approach for Wafer Feature Extraction and Defect Pattern Recognition. (arXiv:2303.11632v1 [cs.CV])

    [http://arxiv.org/abs/2303.11632](http://arxiv.org/abs/2303.11632)

    本文提出了一种非常简单但有效的从晶圆图像中提取特征的技术，其速度快，直观且可解释，在缺陷模式识别方面的表现优于传统的深度学习模型。

    

    在制造过程中，识别芯片图中的缺陷模式对于找到潜在问题的根本原因并提高晶圆的产量至关重要。目前使用深度神经网络来识别这些缺陷模式。这些方法通常非常庞大，并具有显着的推理时间。它们还需要GPU支持才能有效运作。所有这些问题使这些模型不适合于制造晶圆时的在线预测。在本文中，我们提出了一种极其简单但有效的方法来从晶圆图像中提取特征。提出的方法极其快速，直观，非参数化且可解释。实验结果表明，我们提出的流程优于传统的深度学习模型。我们的特征提取不需要训练或微调，同时保留了数据点的相对形状和位置，如我们的可解释性分析所揭示的那样。

    Identifying defect patterns in a wafer map during manufacturing is crucial to find the root cause of the underlying issue and provides valuable insights on improving yield in the foundry. Currently used methods use deep neural networks to identify the defects. These methods are generally very huge and have significant inference time. They also require GPU support to efficiently operate. All these issues make these models not fit for on-line prediction in the manufacturing foundry. In this paper, we propose an extremely simple yet effective technique to extract features from wafer images. The proposed method is extremely fast, intuitive, and non-parametric while being explainable. The experiment results show that the proposed pipeline outperforms conventional deep learning models. Our feature extraction requires no training or fine-tuning while preserving the relative shape and location of data points as revealed by our interpretability analysis.
    
[^194]: 直接迭代反演：图像修复的替代方法

    Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration. (arXiv:2303.11435v1 [eess.IV])

    [http://arxiv.org/abs/2303.11435](http://arxiv.org/abs/2303.11435)

    InDI是一种新的监督式图像修复公式，通过逐步改进图像质量来生成比现有回归方法更真实和详细的图像，修复效果更具有感知质量。

    

    直接迭代反演（InDI）是一种新的监督式图像修复公式，它避免了所谓的“均值回归”效应，并生成比现有回归方法更真实和详细的图像。它通过逐步改进图像质量来实现，类似于生成式去噪扩散模型。图像修复是一个欠定问题，多个高质量图像都可能是给定低质量输入的可行重构。因此，单步回归模型的结果通常是所有可能解释的聚合结果，因此缺乏细节和真实感。

    Inversion by Direct Iteration (InDI) is a new formulation for supervised image restoration that avoids the so-called ``regression to the mean'' effect and produces more realistic and detailed images than existing regression-based methods. It does this by gradually improving image quality in small steps, similar to generative denoising diffusion models.  Image restoration is an ill-posed problem where multiple high-quality images are plausible reconstructions of a given low-quality input. Therefore, the outcome of a single step regression model is typically an aggregate of all possible explanations, therefore lacking details and realism. % The main advantage of InDI is that it does not try to predict the clean target image in a single step but instead gradually improves the image in small steps, resulting in better perceptual quality.  While generative denoising diffusion models also work in small steps, our formulation is distinct in that it does not require knowledge of any analytic f
    
[^195]: 多元概率CRPS学习及其在日前电价预测中的应用

    Multivariate Probabilistic CRPS Learning with an Application to Day-Ahead Electricity Prices. (arXiv:2303.10019v1 [stat.ML])

    [http://arxiv.org/abs/2303.10019](http://arxiv.org/abs/2303.10019)

    本文提出一种新的多元概率CRPS学习方法，应用于日前电价预测中，相比于统一组合在CRPS方面取得了显著改进。

    

    本文提出了一种考虑分位数和协变量依赖关系的多元概率预测的结合方法，并通过平滑过程允许在线学习。通过维数降低和罚函数平滑等两种平滑方法来将标准CRPS学习框架推广到多元维度中。将该方法应用于预测日前电价，相比于统一组合，在CRPS方面取得了显著改进。

    This paper presents a new method for combining (or aggregating or ensembling) multivariate probabilistic forecasts, taking into account dependencies between quantiles and covariates through a smoothing procedure that allows for online learning. Two smoothing methods are discussed: dimensionality reduction using Basis matrices and penalized smoothing. The new online learning algorithm generalizes the standard CRPS learning framework into multivariate dimensions. It is based on Bernstein Online Aggregation (BOA) and yields optimal asymptotic learning properties. We provide an in-depth discussion on possible extensions of the algorithm and several nested cases related to the existing literature on online forecast combination. The methodology is applied to forecasting day-ahead electricity prices, which are 24-dimensional distributional forecasts. The proposed method yields significant improvements over uniform combination in terms of continuous ranked probability score (CRPS). We discuss 
    
[^196]: 关于数据的一切：对数据对抗鲁棒性影响的研究综述

    It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness. (arXiv:2303.09767v1 [cs.LG])

    [http://arxiv.org/abs/2303.09767](http://arxiv.org/abs/2303.09767)

    本文综述了有关数据对抗鲁棒性的研究，系统地总结了最新研究成果，并进一步讨论了未来研究方向和知识差距。

    

    对抗性样本是攻击者有意设计用于混淆机器学习模型以便其犯错的输入。这些样本对基于机器学习的系统的适用性，特别是在涉及生命和安全的领域，构成了严重威胁。为了解决这个问题，对抗鲁棒性领域研究对抗攻击机制和防御策略。本综述回顾了有关模型使用的数据对其抗攻击鲁棒性影响的文献。它系统地识别和总结了这个领域内的最新研究，并进一步讨论了知识的差距和有前途的未来研究方向。

    Adversarial examples are inputs to machine learning models that an attacker has intentionally designed to confuse the model into making a mistake. Such examples pose a serious threat to the applicability of machine-learning-based systems, especially in life- and safety-critical domains. To address this problem, the area of adversarial robustness investigates mechanisms behind adversarial attacks and defenses against these attacks. This survey reviews literature that focuses on the effects of data used by a model on the model's adversarial robustness. It systematically identifies and summarizes the state-of-the-art research in this area and further discusses gaps of knowledge and promising future research directions.
    
[^197]: 使用Prompt-Tuning的原型转向针对无需重复训练的连续学习

    Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])

    [http://arxiv.org/abs/2303.09447](http://arxiv.org/abs/2303.09447)

    本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。

    

    原型作为类别嵌入的一种表示，已被探索用于减少连续学习情境下的内存占用或减轻遗忘。然而，基于原型的方法仍然存在语义漂移和原型干扰导致的性能急剧恶化的问题。在本研究中，我们提出了对比原型提示（CPP）方法，并展示了任务特定提示调整，当在对比学习目标上进行优化时，可以有效地解决这两个障碍并显着提高原型的性能。我们的实验表明，CPP在四个具有挑战性的类增量学习基准测试中表现出色，相对于现有最先进方法有4%至6%的绝对提升。此外，CPP不需要重复训练，它极大地缩小了连续学习和离线联合学习之间的性能差距，展示了一种有前途的Transformer体系结构下连续学习系统的设计方案。

    Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
    
[^198]: 在回归随机森林中寻找空间依赖的路径：分类和系统回顾

    A path in regression Random Forest looking for spatial dependence: a taxonomy and a systematic review. (arXiv:2303.04693v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.04693](http://arxiv.org/abs/2303.04693)

    在这项工作中，我们提出了一种分类法，根据前处理、中处理和/或后处理的时间点尝试将空间信息纳入回归随机森林中。此外，我们进行了系统回顾并分类最新采用的调整回归随机森林以适应空间相关数据的策略。

    

    随机森林（RF）是一种著名的数据驱动算法，在多个领域中应用广泛，因为它在建模响应变量和预测变量之间的关系时具有很大的灵活性，即使在存在强非线性关系的情况下也适用。在环境应用中，常常出现感兴趣的现象可能存在空间和/或时间依赖性，这在RF的标准版本中没有明确考虑到。在这项工作中，我们提出了一种分类法，根据它们在何时（前处理、中处理和/或后处理）尝试将空间信息纳入回归RF中来对策略进行分类。此外，我们根据《系统回顾和Meta分析首选报告项目》（PRISMA）提供的标准，对最近采用的调整回归RF以适应空间相关数据的策略进行系统回顾和分类。后者是一种可重复的方法，用于收集和处理关于特定主题的不同来源的现有文献。

    Random Forest (RF) is a well-known data-driven algorithm applied in several fields thanks to its flexibility in modeling the relationship between the response variable and the predictors, also in case of strong non-linearities. In environmental applications, it often occurs that the phenomenon of interest may present spatial and/or temporal dependence that is not taken explicitly into account by RF in its standard version. In this work, we propose a taxonomy to classify strategies according to when (Pre-, In- and/or Post-processing) they try to include the spatial information into regression RF. Moreover, we provide a systematic review and classify the most recent strategies adopted to "adjust" regression RF to spatially dependent data, based on the criteria provided by the Preferred Reporting Items for Systematic reviews and Meta-Analysis (PRISMA). The latter consists of a reproducible methodology for collecting and processing existing literature on a specified topic from different so
    
[^199]: 具有带符号排列表示的密集连接的$G$-不变深度神经网络

    Densely Connected $G$-invariant Deep Neural Networks with Signed Permutation Representations. (arXiv:2303.04614v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04614](http://arxiv.org/abs/2303.04614)

    本文提出了一种具有带符号排列表示的密集连接$G$-不变深度神经网络($G$-DNN)架构，通过耦合权重，使得网络的前激活能够通过$G$的带符号排列表示进行变换，从而得到一族更丰富的$G$-不变架构。

    

    我们介绍并研究了对于有限群$G$，具有ReLU激活函数的密集连接$G$-不变深度神经网络($G$-DNN)架构。与文献中其他$G$-不变架构不同，我们所提出的$G$-DNN的前激活能够通过$G$的带符号排列表示(signed perm-reps)进行变换。此外，$G$-DNN的各个层不要求是$G$-等变的；而是通过将输入网络的前激活函数限制为$G$-等变函数的方式，在所有层之间耦合权重。结果是一族更丰富的$G$-不变架构，这在以前从未见过。我们通过权重的重新参数化推导了$G$-DNN的高效实现，并得出了一个架构“可接受”的充分必要条件——即非退化且与更小的架构不相同。我们提供了相关代码。

    We introduce and investigate, for finite groups $G$, $G$-invariant deep neural network ($G$-DNN) architectures with ReLU activation that are densely connected-- i.e., include all possible skip connections. In contrast to other $G$-invariant architectures in the literature, the preactivations of the$G$-DNNs presented here are able to transform by \emph{signed} permutation representations (signed perm-reps) of $G$. Moreover, the individual layers of the $G$-DNNs are not required to be $G$-equivariant; instead, the preactivations are constrained to be $G$-equivariant functions of the network input in a way that couples weights across all layers. The result is a richer family of $G$-invariant architectures never seen previously. We derive an efficient implementation of $G$-DNNs after a reparameterization of weights, as well as necessary and sufficient conditions for an architecture to be ``admissible''-- i.e., nondegenerate and inequivalent to smaller architectures. We include code that al
    
[^200]: 深度学习增强的实现GARCH模型

    Deep Learning Enhanced Realized GARCH. (arXiv:2302.08002v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2302.08002](http://arxiv.org/abs/2302.08002)

    使用深度学习（LSTM）和实现波动率测量的新框架能够共同建模收益和实现波动率测量，在统计推断和预测方面具有出色的性能，并且能够适应波动率的定式事实。

    

    我们提出了一种新的波动率建模方法，将深度学习（LSTM）和实现波动率测量结合起来。这种LSTM增强的实现GARCH框架融合了金融计量学、高频交易数据和深度学习的建模进展。通过顺序蒙特卡洛方法采用贝叶斯推断进行统计推断和预测。新的框架能够共同建模收益和实现波动率测量，具有非常好的内样本拟合和超过几个基准模型的预测性能，同时能够很好地适应波动率的定式事实。使用广泛交易的31个股票指数进行了全面实证研究，涵盖了包括COVID-19大流行在内的时间段。

    We propose a new approach to volatility modeling by combining deep learning (LSTM) and realized volatility measures. This LSTM-enhanced realized GARCH framework incorporates and distills modeling advances from financial econometrics, high frequency trading data and deep learning. Bayesian inference via the Sequential Monte Carlo method is employed for statistical inference and forecasting. The new framework can jointly model the returns and realized volatility measures, has an excellent in-sample fit and superior predictive performance compared to several benchmark models, while being able to adapt well to the stylized facts in volatility. The performance of the new framework is tested using a wide range of metrics, from marginal likelihood, volatility forecasting, to tail risk forecasting and option pricing. We report on a comprehensive empirical study using 31 widely traded stock indices over a time period that includes COVID-19 pandemic.
    
[^201]: 迈向模型基础鲁棒强化学习的最小最大优化

    Towards Minimax Optimality of Model-based Robust Reinforcement Learning. (arXiv:2302.05372v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05372](http://arxiv.org/abs/2302.05372)

    本文研究了在鲁棒强化学习中，对于仅具有对正常核心的生成模型访问权限时，获得ε-最优策略的样本复杂度。对于sa（s-）矩形不确定集合，已知最佳样本复杂度为ε^2/（H^4 * |S|^2 * |A|）（响应为ε^2/（H^4 * |S|^2 * |A|^2）），对于特定算法和基于总变差（TV）、KL或卡方散度的不确定集合。

    

    我们研究了在只有对正常核心的生成模型访问权限时，获得ε-最优策略的采样复杂度。这个问题在非鲁棒情况下已经得到了广泛研究，并且已知任何应用于经验MDP的规划方法，只需要用ε^2/（H^3 * |S| * |A|）个样本来估计，均可提供ε-最优策略，从而最小最大优化。鲁棒情况下的结果更加少见。对于sa（s-）矩形不确定集合，已知最佳样本复杂度为ε^2/（H^4 * |S|^2 * |A|）（响应为ε^2/（H^4 * |S|^2 * |A|^2）），对于特定算法和基于总变差（TV）、KL或卡方散度的不确定集合。在本文中，我们考虑用Lp球定义的不确定集合（回复到TV情况），并且...

    We study the sample complexity of obtaining an $\epsilon$-optimal policy in \emph{Robust} discounted Markov Decision Processes (RMDPs), given only access to a generative model of the nominal kernel. This problem is widely studied in the non-robust case, and it is known that any planning approach applied to an empirical MDP estimated with $\tilde{\mathcal{O}}(\frac{H^3 \mid S \mid\mid A \mid}{\epsilon^2})$ samples provides an $\epsilon$-optimal policy, which is minimax optimal. Results in the robust case are much more scarce. For $sa$(resp $s$-)rectangular uncertainty sets, the best known sample complexity is $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid}{\epsilon^2})$ (resp. $\tilde{\mathcal{O}}(\frac{H^4 \mid S \mid^2\mid A \mid^2}{\epsilon^2})$), for specific algorithms and when the uncertainty set is based on the total variation (TV), the KL or the Chi-square divergences. In this paper, we consider uncertainty sets defined with an $L_p$-ball (recovering the TV case), and
    
[^202]: 采用扩散模型去除结构性噪声

    Removing Structured Noise with Diffusion Models. (arXiv:2302.05290v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05290](http://arxiv.org/abs/2302.05290)

    本文提出了一种基于扩散模型的后验采样方法来去除包含结构性噪声的数据，相比于常规方法有较好的表现，对于医学成像等领域具有实际应用价值。

    

    解决不适定反问题需要仔细制定有关感兴趣信号的先验信念，并对它们在有噪声测量中的表现进行准确的描述。基于稀疏性的手工制定信号先验越来越多地被数据驱动的深度生成模型所取代，并且几个团队最近展示了最新的基于分数的扩散模型具有强大的性能和灵活性。在本文中，我们展示了基于扩散模型的后验采样范式可以扩展到包括丰富、结构化的噪声模型。为此，我们提出了联合条件反向扩散过程，其中包含噪声和信号生成分布的学习分数。我们在各种存在结构性噪声的反问题中展示了强大的性能增益，优于使用归一化流和对抗网络的竞争基线。这在医学成像等领域开辟了扩散模型在实践中进行更准确建模的新机会和相关应用。

    Solving ill-posed inverse problems requires careful formulation of prior beliefs over the signals of interest and an accurate description of their manifestation into noisy measurements. Handcrafted signal priors based on e.g. sparsity are increasingly replaced by data-driven deep generative models, and several groups have recently shown that state-of-the-art score-based diffusion models yield particularly strong performance and flexibility. In this paper, we show that the powerful paradigm of posterior sampling with diffusion models can be extended to include rich, structured, noise models. To that end, we propose a joint conditional reverse diffusion process with learned scores for the noise and signal-generating distribution. We demonstrate strong performance gains across various inverse problems with structured noise, outperforming competitive baselines that use normalizing flows and adversarial networks. This opens up new opportunities and relevant practical applications of diffusi
    
[^203]: UniPC：一种用于快速采样扩散模型的统一预测-修正框架

    UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models. (arXiv:2302.04867v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04867](http://arxiv.org/abs/2302.04867)

    本文提出了一种称为UniPC的统一预测-修正框架，用于快速采样扩散模型(DPMs)，该框架通过引入统一修正器(UniC)和统一预测器(UniP)，可以显著提高采样质量，尤其是在较少步骤的情况下。

    

    扩散概率模型(DPMs)在高分辨率图像合成方面表现出非常有希望的能力。然而，由于对去噪网络的多次评估，从预训练的DPM中进行采样非常耗时，因此加速DPM采样变得越来越重要。虽然设计出了快速采样器的最新进展，但现有方法仍无法在许多应用中生成令人满意的图像，这些应用更青睐于较少的步骤(如<10)。在本文中，我们开发了一种统一修正器(UniC)，它可以在任何现有的DPM采样器之后应用，提高精确度的阶数而无需额外的模型评估，并推导出一个支持任意阶数的统一预测器(UniP)作为副产品。通过结合UniP和UniC，我们提出了一种称为UniPC的统一预测-修正框架，用于快速采样DPMs，它具有任意阶数的统一解析形式，并且可以显著提高采样的质量，特别是在极端情况下。

    Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM is time-consuming due to the multiple evaluations of the denoising network, making it more and more important to accelerate the sampling of DPMs. Despite recent progress in designing fast samplers, existing methods still cannot generate satisfying images in many applications where fewer steps (e.g., $<$10) are favored. In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods, especially in extremel
    
[^204]: Sketchy: 内存高效的自适应正则化方法与频繁方向的应用

    Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions. (arXiv:2302.03764v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03764](http://arxiv.org/abs/2302.03764)

    本论文提出了一种内存高效的自适应正则化方法，通过使用频繁方向草稿来降低矩阵预处理器的内存和计算需求。在深度学习任务中，该方法可以在保持性能的同时降低资源的使用。

    

    自适应正则化方法在许多任务中展现了卓越的性能，但在内存和运行时间方面可能受到限制。我们发现在深度学习任务中，Kronecker因子梯度协方差矩阵的谱聚焦在一个变化的小的主特征空间上，这促使我们采用低秩的草稿方法。我们描述了一种通用方法，使用频繁方向（FD）草稿来减少维护矩阵预处理器的内存和计算需求。尽管之前的方法已经探索了在二阶优化中应用FD的方法，但我们提出了一种新颖的分析方法，允许在资源需求和遗憾保证的退化之间进行高效插值: 在在线凸优化（OCO）设置中，我们使用仅$dk$的内存与完整矩阵$d^2$的内存遗憾匹配，直到在底部$d-k$的特征值上添加误差为止。

    Adaptive regularization methods that exploit more than the diagonal entries exhibit state of the art performance for many tasks, but can be prohibitive in terms of memory and running time. We find the spectra of the Kronecker-factored gradient covariance matrix in deep learning (DL) training tasks are concentrated on a small leading eigenspace that changes throughout training, motivating a low-rank sketching approach. We describe a generic method for reducing memory and compute requirements of maintaining a matrix preconditioner using the Frequent Directions (FD) sketch. While previous approaches have explored applying FD for second-order optimization, we present a novel analysis which allows efficient interpolation between resource requirements and the degradation in regret guarantees with rank $k$: in the online convex optimization (OCO) setting over dimension $d$, we match full-matrix $d^2$ memory regret using only $dk$ memory up to additive error in the bottom $d-k$ eigenvalues of 
    
[^205]: CDANs: 来自自相关和非平稳时间序列数据的时间因果发现

    CDANs: Temporal Causal Discovery from Autocorrelated and Non-Stationary Time Series Data. (arXiv:2302.03246v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03246](http://arxiv.org/abs/2302.03246)

    本文提出了一种针对自相关和非平稳时间序列数据的约束性因果发现方法，可以识别出滞后和即时/同时的因果关系以及随时间变化的模块。

    

    时间序列数据在医疗领域的许多方面中被发现，如医学时间序列、电子健康记录（EHR）、生命体征测量和可穿戴设备。因果发现涉及从观测数据中估计因果关系，对于提取有关人类健康的行动洞察力具有重要潜力。在本研究中，我们提出了一种针对自相关和非平稳时间序列数据的新型基于约束的因果发现方法（CDANs）。我们提出的方法解决了现有因果发现方法在自相关和非平稳时间序列数据方面的几个限制，如高维度、无法识别滞后因果关系和忽视变化模式。我们的方法识别出随时间变化的滞后和即时/同时的因果关系以及变化的模块。该方法通过考虑滞后父节点来优化约束搜索中的条件集合。

    Time series data are found in many areas of healthcare such as medical time series, electronic health records (EHR), measurements of vitals, and wearable devices. Causal discovery, which involves estimating causal relationships from observational data, holds the potential to play a significant role in extracting actionable insights about human health. In this study, we present a novel constraint-based causal discovery approach for autocorrelated and non-stationary time series data (CDANs). Our proposed method addresses several limitations of existing causal discovery methods for autocorrelated and non-stationary time series data, such as high dimensionality, the inability to identify lagged causal relationships, and overlooking changing modules. Our approach identifies lagged and instantaneous/contemporaneous causal relationships along with changing modules that vary over time. The method optimizes the conditioning sets in a constraint-based search by considering lagged parents instead
    
[^206]: MuG: 一个多模态分类基准，用于带有表格、文本和视觉字段的游戏数据

    MuG: A Multimodal Classification Benchmark on Game Data with Tabular, Textual, and Visual Fields. (arXiv:2302.02978v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02978](http://arxiv.org/abs/2302.02978)

    提出了一个多模态分类基准MuG，该基准包括八个来自不同类型游戏的数据集，涵盖了表格、文本和视觉模态。通过实验结果表明，该基准具有挑战性和多模态依赖性的特点。

    

    先前的研究已经证明了整合多个数据源的优势，相对于传统的单一模态数据，引发了许多新的多模态应用的出现。我们提出了一个多模态分类基准MuG，其中包含了八个数据集，可以让研究人员评估和改进自己的模型。这些数据集来自四种不同类型的游戏，涵盖了表格、文本和视觉模态。我们进行了多方面的数据分析，提供了基准的洞见，包括标签平衡比、缺失特征的百分比、每个模态中数据的分布，以及标签和输入模态之间的相关性。我们还展示了几个最先进的单一模态分类器和多模态分类器的实验结果，这些结果显示了基准的具有挑战性和多模态依赖性的特点。MuG已经在https://github.com/lujiaying/MUG-Bench上发布，其中包括数据、教程和实现。

    Previous research has demonstrated the advantages of integrating data from multiple sources over traditional unimodal data, leading to the emergence of numerous novel multimodal applications. We propose a multimodal classification benchmark MuG with eight datasets that allows researchers to evaluate and improve their models. These datasets are collected from four various genres of games that cover tabular, textual, and visual modalities. We conduct multi-aspect data analysis to provide insights into the benchmark, including label balance ratios, percentages of missing features, distributions of data within each modality, and the correlations between labels and input modalities. We further present experimental results obtained by several state-of-the-art unimodal classifiers and multimodal classifiers, which demonstrate the challenging and multimodal-dependent properties of the benchmark. MuG is released at https://github.com/lujiaying/MUG-Bench with the data, tutorials, and implemented
    
[^207]: Fed-GLOSS-DP: 利用具有记录级差分隐私的合成集进行联邦全局学习

    Fed-GLOSS-DP: Federated, Global Learning using Synthetic Sets with Record Level Differential Privacy. (arXiv:2302.01068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01068](http://arxiv.org/abs/2302.01068)

    本文通过利用合成样本实现全局优化，加入记录级差分隐私以保护隐私，验证了该方法的数据集有效性。

    

    本文提出了Fed-GLOSS-DP，一种新颖的保护隐私的联邦学习方法。与以前的线性逐点梯度分享方案（如FedAvg）不同，我们的公式利用从客户端接收到的合成样本实现了一种全局优化。这些合成样本作为损失替代物，通过模拟本地区域内真实图像的实用性来近似本地损失地形。我们还引入了一种衡量有效逼近区域的方法，反映了近似的质量。因此，服务器可以恢复全局损失地形并全面优化模型。此外，受日益严重的隐私问题的启发，我们演示了我们的方法与记录级差分隐私（DP）无缝配合，为客户端上的每个数据记录提供理论上的隐私保证。广泛的结果验证了我们的公式在具有高度倾斜分布的各种数据集上的有效性。

    This work proposes Fed-GLOSS-DP, a novel privacy-preserving approach for federated learning. Unlike previous linear point-wise gradient-sharing schemes, such as FedAvg, our formulation enables a type of global optimization by leveraging synthetic samples received from clients. These synthetic samples, serving as loss surrogates, approximate local loss landscapes by simulating the utility of real images within a local region. We additionally introduce an approach to measure effective approximation regions reflecting the quality of the approximation. Therefore, the server can recover the global loss landscape and comprehensively optimize the model. Moreover, motivated by the emerging privacy concerns, we demonstrate that our approach seamlessly works with record-level differential privacy (DP), granting theoretical privacy guarantees for every data record on the clients. Extensive results validate the efficacy of our formulation on various datasets with highly skewed distributions. Our m
    
[^208]: 大型语言模型可被视为隐含的主题模型：解释和寻找好的示范以实现上下文学习

    Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning. (arXiv:2301.11916v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.11916](http://arxiv.org/abs/2301.11916)

    本研究发现，大型语言模型可以被视为隐式的主题模型，并提出了一种算法，从注释数据中选择最佳示范，大大提高了上下文学习的能力。

    

    近年来，预训练的大型语言模型表现出了在推理时实现少量样本学习能力的显著效率，被称为上下文学习。 然而，现有文献强调这种能力对少量样本示范的选择很敏感。本研究旨在通过贝叶斯视角研究上下文学习现象，将大型语言模型视为从示范中隐含地推断出相关信息的主题模型。在此前提下，我们提出了一种算法，用于从一组注释数据中选择最佳示范，并证明相对于随机选择基线的平均值，在八个不同的真实文本分类数据集上平均每个 GPT2 和 GPT3 模型有显着的 12.5% 的提升。我们的实证发现支持我们的假设，即大型语言模型可被视为隐含的主题模型。

    In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that la
    
[^209]: 一种行为良好的图神经近似复杂动力学的方法

    A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics. (arXiv:2301.04900v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2301.04900](http://arxiv.org/abs/2301.04900)

    本文介绍了一种行为良好的图神经网络近似复杂动力学的方法，包括必要的偏置和适当的神经网络结构，并提出了评估泛化能力和推断时预测置信度的方法。

    

    数据驱动的常微分方程近似提供了一种有前景的方法来发现动力系统模型，特别是对于缺乏明确原理的复杂系统。本文着重研究了一类由网络邻接矩阵耦合的常微分方程系统描述的复杂系统。许多现实世界中的系统，包括金融、社交和神经系统，属于这类动力学模型。我们提出了使用神经网络近似这种动力系统的关键要素，包括必要的偏置和适当的神经网络结构。强调与静态监督学习的区别，我们提倡在统计学习理论的经典假设之外评估泛化能力。为了在推断时估计预测的置信度，我们引入了一个专用的空模型。通过研究各种复杂网络动力学，我们展示了神经网络的能力。

    Data-driven approximations of ordinary differential equations offer a promising alternative to classical methods in discovering a dynamical system model, particularly in complex systems lacking explicit first principles. This paper focuses on a complex system whose dynamics is described with a system of ordinary differential equations, coupled via a network adjacency matrix. Numerous real-world systems, including financial, social, and neural systems, belong to this class of dynamical models. We propose essential elements for approximating such dynamical systems using neural networks, including necessary biases and an appropriate neural architecture. Emphasizing the differences from static supervised learning, we advocate for evaluating generalization beyond classical assumptions of statistical learning theory. To estimate confidence in prediction during inference time, we introduce a dedicated null model. By studying various complex network dynamics, we demonstrate the neural network'
    
[^210]: SMACv2：一种改进的合作多智能体强化学习基准

    SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2212.07489v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07489](http://arxiv.org/abs/2212.07489)

    SMACv2是一个改进的合作多智能体强化学习基准，通过引入随机性和部分可观察性的限制，挑战算法在复杂场景中的泛化能力。

    

    具有挑战性的基准在机器学习的最新进展中起到了关键作用。在合作多智能体强化学习中，星际争霸多智能体挑战（SMAC）已成为集中训练和分散执行的受欢迎的测试平台。然而，在多年的持续改进后，算法现在已经实现了接近完美的性能。在这项工作中，我们进行了新的分析，证明SMAC缺乏需要复杂的“闭环”策略的随机性和部分可观察性。特别是，我们展示了一个只依赖于时间步骤的“开环”策略可以在许多SMAC场景中实现非平凡的胜率。为了解决这个限制，我们引入了SMACv2，这是基准的一个新版本，其中的场景是程序生成的，并且在评估过程中要求智能体对以前未见过的设置（来自同一分布）进行泛化。我们还引入了扩展的部分可观察性挑战（EPO），它可以测试智能体在部分可观察态下的学习能力。

    The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which au
    
[^211]: 对多个感兴趣结果的策略学习：将最优策略树与多目标贝叶斯优化相结合

    Policy learning for many outcomes of interest: Combining optimal policy trees with multi-objective Bayesian optimisation. (arXiv:2212.06312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06312](http://arxiv.org/abs/2212.06312)

    该论文提出了一种多目标策略学习（MOPoL）方法，结合了最优决策树和多目标贝叶斯优化方法，可以平衡多个感兴趣的结果。

    

    学习最优策略的方法使用因果机器学习模型来创建人类可解释的规则，以在不同政策干预分配中进行选择。然而，在现实中的政策制定环境中，决策者通常关心不同结果之间的平衡，而不仅仅是单纯地最大化一个结果的效用。本文提出了一种被称为多目标策略学习（MOPoL）的方法，它将策略学习的最优决策树与多目标贝叶斯优化方法相结合，以探索多个结果之间的权衡。它通过构建非支配模型的帕累托前沿来实现，这些模型在不同的超参数设置下控制着结果的权重。关键在于，一个低成本的贪心树可以作为非常计算昂贵的最优树的准确代理，用于决策目的，这意味着可以反复拟合模型来学习帕累托前沿。该方法应用于实际案例

    Methods for learning optimal policies use causal machine learning models to create human-interpretable rules for making choices around the allocation of different policy interventions. However, in realistic policy-making contexts, decision-makers often care about trade-offs between outcomes, not just single-mindedly maximising utility for one outcome. This paper proposes an approach termed Multi-Objective Policy Learning (MOPoL) which combines optimal decision trees for policy learning with a multi-objective Bayesian optimisation approach to explore the trade-off between multiple outcomes. It does this by building a Pareto frontier of non-dominated models for different hyperparameter settings which govern outcome weighting. The key here is that a low-cost greedy tree can be an accurate proxy for the very computationally costly optimal tree for the purposes of making decisions which means models can be repeatedly fit to learn a Pareto frontier. The method is applied to a real-world case
    
[^212]: 关于随机梯度的被忽视的结构

    On the Overlooked Structure of Stochastic Gradients. (arXiv:2212.02083v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02083](http://arxiv.org/abs/2212.02083)

    本文对深度学习中随机梯度的结构进行了正式的统计检验，发现逐维梯度通常呈现幂律重尾，而逐次迭代的梯度和随机梯度噪声通常不呈现幂律重尾。

    

    随机梯度与深度神经网络（DNN）的优化和泛化密切相关。一些研究试图通过梯度噪声的重尾性质来解释随机优化在深度学习中的成功，而其他研究则提出了对梯度噪声的重尾假设的理论和实证证据。不幸的是，在深度学习中，用于分析随机梯度结构和重尾的正式统计检验还没有得到充分开发。在本文中，我们主要做出两个贡献。首先，我们对随机梯度和梯度噪声在参数和迭代中的分布进行了正式的统计检验。我们的统计检验发现，逐维梯度通常表现出幂律重尾，而逐次迭代的梯度和由小批量训练引起的随机梯度噪声通常不表现出幂律重尾。其次，我们进一步发现协方差特性。

    Stochastic gradients closely relate to both optimization and generalization of deep neural networks (DNNs). Some works attempted to explain the success of stochastic optimization for deep learning by the arguably heavy-tail properties of gradient noise, while other works presented theoretical and empirical evidence against the heavy-tail hypothesis on gradient noise. Unfortunately, formal statistical tests for analyzing the structure and heavy tails of stochastic gradients in deep learning are still under-explored. In this paper, we mainly make two contributions. First, we conduct formal statistical tests on the distribution of stochastic gradients and gradient noise across both parameters and iterations. Our statistical tests reveal that dimension-wise gradients usually exhibit power-law heavy tails, while iteration-wise gradients and stochastic gradient noise caused by minibatch training usually do not exhibit power-law heavy tails. Second, we further discover that the covariance spe
    
[^213]: Photo Rater: 使用深度学习的自动选择摄影照片系统

    Photo Rater: Photographs Auto-Selector with Deep Learning. (arXiv:2211.14420v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14420](http://arxiv.org/abs/2211.14420)

    Photo Rater是一个利用深度学习帮助摄影师选择最佳照片的计算机视觉项目。它通过三个神经网络对图像进行质量评估、模糊分类和审美评估，并根据得分对图像进行排序和呈现。

    

    Photo Rater是一个计算机视觉项目，利用神经网络帮助摄影师在拍摄同一场景的照片中选择最佳照片。这个过程通常被称为“筛选”在摄影中，如果手动完成会很繁琐和耗时。Photo Rater利用三个独立的神经网络完成这样的任务：一个用于一般图像质量评估，一个用于分类照片是否模糊（因为手抖或者聚焦不准），一个用于评估整体审美（包括照片构图等）。在通过每个神经网络处理图像后，Photo Rater为每个图像输出一个最终得分，根据得分排名并呈现给用户。

    Photo Rater is a computer vision project that uses neural networks to help photographers select the best photo among those that are taken based on the same scene. This process is usually referred to as "culling" in photography, and it can be tedious and time-consuming if done manually. Photo Rater utilizes three separate neural networks to complete such a task: one for general image quality assessment, one for classifying whether the photo is blurry (either due to unsteady hands or out-of-focusness), and one for assessing general aesthetics (including the composition of the photo, among others). After feeding the image through each neural network, Photo Rater outputs a final score for each image, ranking them based on this score and presenting it to the user.
    
[^214]: 无线群集化联邦学习中的空中聚合

    Over-The-Air Clustered Wireless Federated Learning. (arXiv:2211.03363v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03363](http://arxiv.org/abs/2211.03363)

    本论文提出了一种OTA半分散群集无线FL（CWFL）和CWFL-Prox算法，它在通信效率上优于分散FL策略，同时参数更新收敛到全局极小值。

    

    隐私和带宽限制导致在无线系统中使用联邦学习（FL），即在不共享原始数据的情况下协作进行机器学习（ML）模型训练。使用带宽受限的上行无线信道时，空中（OTA）FL更受青睐，因为客户端可以同时向服务器传输参数更新。由于延迟增加和服务器故障，可能无法使用强大的服务器进行参数聚合。在没有强大服务器的情况下，采用分散策略，即客户端与其邻居通信，以获得共识ML模型，但通信成本巨大。在这项工作中，我们提出了OTA半分散群集无线FL（CWFL）和CWFL-Prox算法，与分散FL策略相比，该算法具有较高的通信效率，同时参数更新收敛于全局极小值，每个聚类的收敛速度为O（1/T）。使用MNIST和CIFAR10数据集，我们证明了...

    Privacy and bandwidth constraints have led to the use of federated learning (FL) in wireless systems, where training a machine learning (ML) model is accomplished collaboratively without sharing raw data. While using bandwidth-constrained uplink wireless channels, over-the-air (OTA) FL is preferred since the clients can transmit parameter updates simultaneously to a server. A powerful server may not be available for parameter aggregation due to increased latency and server failures. In the absence of a powerful server, decentralised strategy is employed where clients communicate with their neighbors to obtain a consensus ML model while incurring huge communication cost. In this work, we propose the OTA semi-decentralised clustered wireless FL (CWFL) and CWFL-Prox algorithms, which is communication efficient as compared to the decentralised FL strategy, while the parameter updates converge to global minima as O(1/T) for each cluster. Using the MNIST and CIFAR10 datasets, we demonstrate 
    
[^215]: 使用基于LSTM的多标签分类器预测用户特定的未来活动

    Predicting User-specific Future Activities using LSTM-based Multi-label Classification. (arXiv:2211.03100v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03100](http://arxiv.org/abs/2211.03100)

    使用LSTM-based的多标签分类器，本研究提出了一个两阶段训练方法，通过预处理和微调提高了用户特定未来活动预测的性能。

    

    基于先前活动的用户特定未来活动预测可以极大地提高护士提供的服务质量，但在医疗领域中进行预测是具有挑战性的，因为与其他领域不同，医疗领域的活动涉及护士和患者，并且随时间变化。本文采用各种数据处理技术来组织和修改数据结构，并使用基于LSTM的多标签分类器进行新颖的两阶段训练方法（用户不可知的预训练和用户特定的微调）。我们的实验在验证集上达到了31.58%的准确率，57.94%的精确度，68.31%的召回率和60.38%的F1分数。我们得出结论，适当的数据预处理和两阶段训练过程可以提高性能。这个实验是我们团队“不是局部最小值的粉丝”参加的“第四个护士护理活动识别挑战”的一部分。

    User-specific future activity prediction in the healthcare domain based on previous activities can drastically improve the services provided by the nurses. It is challenging because, unlike other domains, activities in healthcare involve both nurses and patients, and they also vary from hour to hour. In this paper, we employ various data processing techniques to organize and modify the data structure and an LSTM-based multi-label classifier for a novel 2-stage training approach (user-agnostic pre-training and user-specific fine-tuning). Our experiment achieves a validation accuracy of 31.58\%, precision 57.94%, recall 68.31%, and F1 score 60.38%. We concluded that proper data pre-processing and a 2-stage training process resulted in better performance. This experiment is a part of the "Fourth Nurse Care Activity Recognition Challenge" by our team "Not A Fan of Local Minima".
    
[^216]: ImageCAS:一种基于计算机断层扫描血管造影图像的冠状动脉分割的大规模数据集和基准

    ImageCAS: A Large-Scale Dataset and Benchmark for Coronary Artery Segmentation based on Computed Tomography Angiography Images. (arXiv:2211.01607v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2211.01607](http://arxiv.org/abs/2211.01607)

    论文提出了一个基于计算机断层扫描血管造影图像的冠状动脉分割的大规模数据集和基准，解决了现有工作中数据集不足和方法比较困难的问题。

    

    心血管疾病占非传染性疾病的近一半。冠状动脉的血管狭窄被认为是心血管疾病的主要风险。计算机断层扫描血管造影（CTA）是一种广泛应用于冠状动脉诊断的无创成像模式，由于其出色的图像分辨率。在临床上，冠状动脉的分割对于冠状动脉疾病的诊断和量化是必要的。最近，提出了各种方法来解决这个问题。然而，一方面，大多数工作依赖于自有数据集，只有少数工作将数据集发表到公众领域，其中只包含数十个图像。另一方面，它们的源代码尚未发布，大多数后续工作也没有与现有工作进行比较，这使得很难判断方法的有效性，并阻碍了社区对这个具有挑战性但关键性问题进一步的探索。在本文中，我们提出了一个基于计算机断层扫描血管造影图像的冠状动脉分割的大规模数据集和基准。

    Cardiovascular disease (CVD) accounts for about half of non-communicable diseases. Vessel stenosis in the coronary artery is considered to be the major risk of CVD. Computed tomography angiography (CTA) is one of the widely used noninvasive imaging modalities in coronary artery diagnosis due to its superior image resolution. Clinically, segmentation of coronary arteries is essential for the diagnosis and quantification of coronary artery disease. Recently, a variety of works have been proposed to address this problem. However, on one hand, most works rely on in-house datasets, and only a few works published their datasets to the public which only contain tens of images. On the other hand, their source code have not been published, and most follow-up works have not made comparison with existing works, which makes it difficult to judge the effectiveness of the methods and hinders the further exploration of this challenging yet critical problem in the community. In this paper, we propose 
    
[^217]: CGAN-ECT：使用CGAN从电容测量中重建层析成像。（arXiv:2209.03737v3 [eess.IV] UPDATED）

    CGAN-ECT: Tomography Image Reconstruction from Electrical Capacitance Measurements Using CGANs. (arXiv:2209.03737v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.03737](http://arxiv.org/abs/2209.03737)

    这项研究提出了一种使用条件生成对抗网络（CGAN）从电容测量中重建电容层析成像的方法，并创建了一个包含320K个合成图像测量对的新数据集进行训练和评估。

    

    由于电容层析成像（ECT）在多个工业领域的快速增长应用，迫切需要开发高质量且快速的图像重建方法来从原始电容测量中得到图像。深度学习作为一种有效的非线性映射工具，已经在包括电气层析成像在内的许多领域中广泛应用。在本文中，我们提出了一种用于从电容测量中重建ECT图像的条件生成对抗网络（CGAN）模型。该CGAN模型的初始图像是由电容测量构造的。据我们所知，这是首次将电容测量表示为图像形式。我们创建了一个包含320K个合成图像测量对的新ECT数据集进行训练，并使用测试数据集、受污染数据和流动模式来评估所提出的CGAN-ECT模型的可行性和泛化能力。

    Due to the rapid growth of Electrical Capacitance Tomography (ECT) applications in several industrial fields, there is a crucial need for developing high quality, yet fast, methodologies of image reconstruction from raw capacitance measurements. Deep learning, as an effective non-linear mapping tool for complicated functions, has been going viral in many fields including electrical tomography. In this paper, we propose a Conditional Generative Adversarial Network (CGAN) model for reconstructing ECT images from capacitance measurements. The initial image of the CGAN model is constructed from the capacitance measurement. To our knowledge, this is the first time to represent the capacitance measurements in an image form. We have created a new massive ECT dataset of 320K synthetic image measurements pairs for training, and testing the proposed model. The feasibility and generalization ability of the proposed CGAN-ECT model are evaluated using testing dataset, contaminated data and flow pat
    
[^218]: BUTTER区域：全连接神经网络训练动态的实证研究

    The BUTTER Zone: An Empirical Study of Training Dynamics in Fully Connected Neural Networks. (arXiv:2207.12547v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12547](http://arxiv.org/abs/2207.12547)

    本研究提供了一个实证数据集，调查了全连接神经网络的训练动态，并观察到稳定模式跨任务和拓扑结构持续存在。该研究旨在激发机器学习技术的科学研究，为推进该领域的理论发现提供必要的动力。

    

    我们提供了一个实证数据集，调查了全连接前馈多层感知机神经网络的深度学习现象。该数据集现在可以在网上免费获取，记录了在不同超参数选择下的架构、任务、深度、网络大小（参数数量）、学习率、批次大小和正则化惩罚的每个时代的训练和泛化性能。通过平均重复每个实验24次，共记录了1100万个训练运行和400亿个时代。总计使用了1.7 TB的数据集，耗费了11000个CPU核年、72.3个GPU年和163个节点年。在调查数据集时，我们观察到了跨任务和拓扑结构持续存在的稳定模式。我们的目标是引发对机器学习技术的科学研究，为推进该领域超越高能耗和启发式实践所需的理论发现提供动力。

    We present an empirical dataset surveying the deep learning phenomenon on fully-connected feed-forward multilayer perceptron neural networks. The dataset, which is now freely available online, records the per-epoch training and generalization performance of 483 thousand distinct hyperparameter choices of architectures, tasks, depths, network sizes (number of parameters), learning rates, batch sizes, and regularization penalties. Repeating each experiment an average of 24 times resulted in 11 million total training runs and 40 billion epochs recorded. Accumulating this 1.7 TB dataset utilized 11 thousand CPU core-years, 72.3 GPU-years, and 163 node-years. In surveying the dataset, we observe durable patterns persisting across tasks and topologies. We aim to spark scientific study of machine learning techniques as a catalyst for the theoretical discoveries needed to progress the field beyond energy-intensive and heuristic practices.
    
[^219]: 导数信息驱动的神经算子：一个高维参数导数学习的高效框架

    Derivative-Informed Neural Operator: An Efficient Framework for High-Dimensional Parametric Derivative Learning. (arXiv:2206.10745v4 [math.NA] UPDATED)

    [http://arxiv.org/abs/2206.10745](http://arxiv.org/abs/2206.10745)

    导数信息驱动的神经算子（DINOs）是一个高效的框架，用于近似高维参数导数学习。通过压缩导数信息和高效应用于神经算子训练，DINOs可以提高算子和导数的准确性，应用于众多领域。

    

    我们提出了导数信息驱动的神经算子（DINOs），这是一个通用的神经网络家族，用于近似作为从输入函数空间到输出函数空间或感兴趣的值的无限维映射的算子。在离散化之后，输入和输出都是高维的。我们旨在不仅以更高的准确性近似算子，而且近似它们对输入函数值参数的导数（雅可比矩阵），以增强基于导数的算法在许多应用中的能力，例如贝叶斯逆问题，参数不确定性下的优化和最优实验设计。主要的困难包括生成导数训练数据的计算成本和问题的高维度导致大的训练成本。为了解决这些挑战，我们利用导数的固有低维性质，开发了用于压缩导数信息并将其高效地应用于神经算子训练的算法。

    We propose derivative-informed neural operators (DINOs), a general family of neural networks to approximate operators as infinite-dimensional mappings from input function spaces to output function spaces or quantities of interest. After discretizations both inputs and outputs are high-dimensional. We aim to approximate not only the operators with improved accuracy but also their derivatives (Jacobians) with respect to the input function-valued parameter to empower derivative-based algorithms in many applications, e.g., Bayesian inverse problems, optimization under parameter uncertainty, and optimal experimental design. The major difficulties include the computational cost of generating derivative training data and the high dimensionality of the problem leading to large training cost. To address these challenges, we exploit the intrinsic low-dimensionality of the derivatives and develop algorithms for compressing derivative information and efficiently imposing it in neural operator trai
    
[^220]: SSM-DTA: 打破药物靶点亲和性预测中的数据稀缺障碍

    SSM-DTA: Breaking the Barriers of Data Scarcity in Drug-Target Affinity Prediction. (arXiv:2206.09818v3 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2206.09818](http://arxiv.org/abs/2206.09818)

    SSM-DTA框架通过多任务训练和半监督训练方法，打破了药物靶点亲和性预测中数据稀缺的障碍。

    

    准确预测药物靶点亲和性（DTA）在早期药物研发中至关重要，有助于识别可以与特定靶点有效相互作用并调节其活性的药物。尽管湿实验仍然是最可靠的方法，但它们耗时耗力，导致有限的数据可用性，给深度学习方法带来挑战。现有方法主要集中在基于现有DTA数据开发技术上，没有充分解决数据稀缺问题。为了克服这个难题，我们提出了SSM-DTA框架，它包含三种简单而高效的策略：（1）多任务训练方法，将DTA预测与蒙版语言建模（MLM）结合起来，使用成对的药物-靶点数据。（2）半监督训练方法，利用大规模的无配对分子和蛋白质来增强药物和靶点的表示。这种方法与现有方法不同。

    Accurate prediction of Drug-Target Affinity (DTA) is of vital importance in early-stage drug discovery, facilitating the identification of drugs that can effectively interact with specific targets and regulate their activities. While wet experiments remain the most reliable method, they are time-consuming and resource-intensive, resulting in limited data availability that poses challenges for deep learning approaches. Existing methods have primarily focused on developing techniques based on the available DTA data, without adequately addressing the data scarcity issue. To overcome this challenge, we present the SSM-DTA framework, which incorporates three simple yet highly effective strategies: (1) A multi-task training approach that combines DTA prediction with masked language modeling (MLM) using paired drug-target data. (2) A semi-supervised training method that leverages large-scale unpaired molecules and proteins to enhance drug and target representations. This approach differs from
    
[^221]: PROFHIT: 面向分层时间序列的概率鲁棒预测

    PROFHIT: Probabilistic Robust Forecasting for Hierarchical Time-series. (arXiv:2206.07940v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.07940](http://arxiv.org/abs/2206.07940)

    PROFHIT是一个概率鲁棒的分层时间序列预测模型，能够提供整个层次结构的预测分布，并引入一种新颖的分布一致性正则化方法。

    

    概率性分层时间序列预测是时间序列预测的重要变种，其目标是建模和预测具有分层关系的多变量时间序列。大多数方法关注点预测，并没有提供良好校准的概率预测分布。最近的概率预测方法在点预测和分布样本上也引入了分层关系，但没有考虑预测分布的一致性。之前的工作也默默地假设数据集总是与给定的分层关系一致，并且不适应显示与此假设偏离的真实世界数据集。我们填补了这两个差距，并提出了PROFHIT，这是一个完全概率性的分层预测模型，能够同时建模整个层次结构的预测分布。PROFHIT采用灵活的概率贝叶斯方法，并引入一种新颖的分布一致性正则化方法。

    Probabilistic hierarchical time-series forecasting is an important variant of time-series forecasting, where the goal is to model and forecast multivariate time-series that have underlying hierarchical relations. Most methods focus on point predictions and do not provide well-calibrated probabilistic forecasts distributions. Recent state-of-art probabilistic forecasting methods also impose hierarchical relations on point predictions and samples of distribution which does not account for coherency of forecast distributions. Previous works also silently assume that datasets are always consistent with given hierarchical relations and do not adapt to real-world datasets that show deviation from this assumption. We close both these gaps and propose PROFHIT, which is a fully probabilistic hierarchical forecasting model that jointly models forecast distribution of entire hierarchy. PROFHIT uses a flexible probabilistic Bayesian approach and introduces a novel Distributional Coherency regulari
    
[^222]: 自然策略梯度原始-对偶方法在约束MDP中的收敛性和样本复杂度研究

    Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs. (arXiv:2206.02346v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.02346](http://arxiv.org/abs/2206.02346)

    本文研究了约束马尔可夫决策过程中优化问题的自然策略梯度原始-对偶方法。通过自然策略梯度上升和投影次梯度下降更新变量，我们的方法在全局收敛中实现了次线性速率，而且不受状态-动作空间大小限制。

    

    我们研究了顺序决策问题，旨在最大化预期总奖励，同时满足对预期总效用的约束。我们使用自然策略梯度方法来解决约束马尔可夫决策过程（约束MDP）的折扣无限时序优化控制问题。具体地，我们提出了一种新的自然策略梯度原始-对偶（NPG-PD）方法，该方法通过自然策略梯度上升更新原始变量，通过投影次梯度下降更新对偶变量。尽管底层最大化涉及非凸目标函数和非凸约束集，但在softmax策略参数化下，我们证明了我们的方法在优化间隙和约束违规方面实现全局收敛，并具有次线性速率。此类收敛与状态-动作空间的大小无关，即无维度限制。此外，对于对数线性和一般平滑策略参数化，我们确立了收敛性和样本复杂度界限。

    We study sequential decision making problems aimed at maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we esta
    
[^223]: 具有可证明的一致性和公平保证的推荐系统张量补全

    Tensor Completion with Provable Consistency and Fairness Guarantees for Recommender Systems. (arXiv:2204.01815v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2204.01815](http://arxiv.org/abs/2204.01815)

    本文介绍了一种新的一致性方法来解决矩阵和张量补全问题，在推荐系统应用中，我们证明了通过保留单位比例和一致性两个约束条件可以实现解的存在性与唯一性。

    

    我们引入了一种新的基于一致性的方法来定义和解决非负/正矩阵和张量补全问题。该框架的新颖之处在于，我们不是人为地将问题形式化为任意优化问题，例如，最小化一个结构量，如秩或范数，而是展示了一个单一的属性/约束：保留单位比例一致性，保证了解的存在，并在相对较弱的支持假设下保证了解的唯一性。该框架和解算法也直接推广到任意维度的张量中，同时保持了固定维度 d 的问题规模的线性计算复杂性。在推荐系统应用中，我们证明了两个合理的性质，这些性质应该适用于任何 RS 问题的解，足以允许在我们的框架内建立唯一性保证。关键理论贡献是展示了这些约束下解的存在性与唯一性。

    We introduce a new consistency-based approach for defining and solving nonnegative/positive matrix and tensor completion problems. The novelty of the framework is that instead of artificially making the problem well-posed in the form of an application-arbitrary optimization problem, e.g., minimizing a bulk structural measure such as rank or norm, we show that a single property/constraint: preserving unit-scale consistency, guarantees the existence of both a solution and, under relatively weak support assumptions, uniqueness. The framework and solution algorithms also generalize directly to tensors of arbitrary dimensions while maintaining computational complexity that is linear in problem size for fixed dimension d. In the context of recommender system (RS) applications, we prove that two reasonable properties that should be expected to hold for any solution to the RS problem are sufficient to permit uniqueness guarantees to be established within our framework. Key theoretical contribu
    
[^224]: 对于协变量偏移泛化的基于独立性驱动的重要性加权算法的理论分析

    A Theoretical Analysis on Independence-driven Importance Weighting for Covariate-shift Generalization. (arXiv:2111.02355v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.02355](http://arxiv.org/abs/2111.02355)

    本文通过理论分析，将独立性驱动的重要性加权算法解释为特征选择过程，并证明了在协变量偏移泛化中的有效性。

    

    协变量偏移泛化是分布之外（OOD）泛化中的典型情况，要求在未知的测试分布上表现良好，该分布与可访问的训练分布以协变量转移的形式有所不同。最近，稳定学习文献中的独立性驱动的重要性加权算法在处理包括回归算法和深度神经网络在内的多个学习模型上显示出了经验有效性，但它们的理论分析尚缺失。本文通过将它们解释为特征选择过程，从理论上证明了这些算法的有效性。我们首先指定了一组变量，称为最小稳定变量集，该集合是处理协变量偏移泛化的常见损失函数（如均方损失和二元交叉熵损失）的最小最优变量集。随后，我们证明在理想条件下，在这些算法下，独立性驱动的重要性加权算法可以实现这个最小稳定变量集的有效选择。

    Covariate-shift generalization, a typical case in out-of-distribution (OOD) generalization, requires a good performance on the unknown test distribution, which varies from the accessible training distribution in the form of covariate shift. Recently, independence-driven importance weighting algorithms in stable learning literature have shown empirical effectiveness to deal with covariate-shift generalization on several learning models, including regression algorithms and deep neural networks, while their theoretical analyses are missing. In this paper, we theoretically prove the effectiveness of such algorithms by explaining them as feature selection processes. We first specify a set of variables, named minimal stable variable set, that is the minimal and optimal set of variables to deal with covariate-shift generalization for common loss functions, such as the mean squared loss and binary cross-entropy loss. Afterward, we prove that under ideal conditions, independence-driven importan
    
[^225]: 私有多任务学习：表述与应用于联邦学习

    Private Multi-Task Learning: Formulation and Applications to Federated Learning. (arXiv:2108.12978v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.12978](http://arxiv.org/abs/2108.12978)

    这项研究提出了一种用于在隐私敏感应用中进行多任务学习的算法，通过联合差分隐私保证了个性化联邦学习的隐私和效用，并在实验中表现出更好的性能。

    

    许多机器学习问题依赖于多任务学习（MTL），其中目标是同时解决多个相关的机器学习任务。MTL对于隐私敏感的应用特别重要，例如医疗保健、金融和物联网计算，这些应用需要共享来自多个不同来源的敏感数据进行学习。在这项工作中，我们通过联合差分隐私（JDP）为MTL形式化客户级隐私的概念，JDP是差分隐私在机制设计和分布式优化中的一种放松。然后，我们提出了一种用于均值正则化MTL的算法，这是个性化联邦学习应用中常用的目标，同时满足JDP。我们对我们的目标和求解器进行了分析，提供了隐私和效用的可证明保证。从实证上看，相对于常见的联邦学习基准测试，我们发现我们的方法在隐私/效用权衡方面提供了更好的性能。

    Many problems in machine learning rely on multi-task learning (MTL), in which the goal is to solve multiple related machine learning tasks simultaneously. MTL is particularly relevant for privacy-sensitive applications in areas such as healthcare, finance, and IoT computing, where sensitive data from multiple, varied sources are shared for the purpose of learning. In this work, we formalize notions of client-level privacy for MTL via joint differential privacy (JDP), a relaxation of differential privacy for mechanism design and distributed optimization. We then propose an algorithm for mean-regularized MTL, an objective commonly used for applications in personalized federated learning, subject to JDP. We analyze our objective and solver, providing certifiable guarantees on both privacy and utility. Empirically, we find that our method provides improved privacy/utility trade-offs relative to global baselines across common federated learning benchmarks.
    
[^226]: 优先训练可学习、值得学习且尚未学习的点（研讨会版本）

    Prioritized training on points that are learnable, worth learning, and not yet learned (workshop version). (arXiv:2107.02565v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.02565](http://arxiv.org/abs/2107.02565)

    本文介绍了一种名为“黄金选择”的技术，它通过选择恰当的训练点来加快模型训练。与优化文献中通常选择的困难点和课程学习中通常优先选择的简单点不同，黄金选择选择的点既有较高的信息量又能表现良好，且可迁移到其他架构中。

    

    我们引入了“黄金选择”技术，一种用于更快地训练模型的技术，它选择了一系列“恰到好处”的训练点。我们提出了一种信息论的获取函数——可约的验证损失，并使用一个小型代理模型——GoldiProx来高效选择能够最大化验证集信息的训练点。我们发现，通常在优化文献中选择的“困难”（例如高损失）点往往是有噪声的，而通常优先选择用于课程学习的“简单”（例如低噪声）样本提供的信息较少。此外，通常被主动学习所针对的带有不确定标签的点往往与任务的相关性较小。相反，黄金选择选择的点既适中又具有较好表现。此外，所选择的序列可以迁移到其他架构中，从而实践者可以在无需重新创建的情况下共享和复用。

    We introduce Goldilocks Selection, a technique for faster model training which selects a sequence of training points that are "just right". We propose an information-theoretic acquisition function -- the reducible validation loss -- and compute it with a small proxy model -- GoldiProx -- to efficiently choose training points that maximize information about a validation set. We show that the "hard" (e.g. high loss) points usually selected in the optimization literature are typically noisy, while the "easy" (e.g. low noise) samples often prioritized for curriculum learning confer less information. Further, points with uncertain labels, typically targeted by active learning, tend to be less relevant to the task. In contrast, Goldilocks Selection chooses points that are "just right" and empirically outperforms the above approaches. Moreover, the selected sequence can transfer to other architectures; practitioners can share and reuse it without the need to recreate it.
    
[^227]: 在具有许多类别的上下文推断中通过离线神谕进行最优模型选择

    Optimal Model Selection in Contextual Bandits with Many Classes via Offline Oracles. (arXiv:2106.06483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.06483](http://arxiv.org/abs/2106.06483)

    本论文研究了在随机上下文推断设置中，针对累计遗憾最小化的最优模型选择问题。通过引入渐增类别复杂性和递减边际收益条件，我们提出了一种基于新颖误配测试的算法，并展示了模型选择在奖励估计中的优势。

    

    在监督学习中，模型选择提供了一种无成本的保证，就好像最优平衡偏差和方差的模型是先验已知的一样。我们研究了在随机上下文推断设置中实现类似保证的可行性。最近的研究 [Marinov and Zimmert, 2021] 鉴别出没有算法能够保证无成本的遗憾界限的情况。然而，我们发现在渐增类别复杂性和随着类别复杂性增加最佳策略价值边际收益递减的温和条件下，无成本模型选择是可行的。我们的算法基于一种新颖的误配测试，我们的分析展示了模型选择在奖励估计中的优势。与先前关于上下文推断中模型选择的工作不同，我们的算法在收集更多数据时会仔细地适应逐渐演变的偏差-方差权衡。特别地，我们的算法和分析超越了适应时间复杂性的范畴。

    Model selection in supervised learning provides costless guarantees as if the model that best balances bias and variance was known a priori. We study the feasibility of similar guarantees for cumulative regret minimization in the stochastic contextual bandit setting. Recent work [Marinov and Zimmert, 2021] identifies instances where no algorithm can guarantee costless regret bounds. Nevertheless, we identify benign conditions where costless model selection is feasible: gradually increasing class complexity, and diminishing marginal returns for best-in-class policy value with increasing class complexity. Our algorithm is based on a novel misspecification test, and our analysis demonstrates the benefits of using model selection for reward estimation. Unlike prior work on model selection in contextual bandits, our algorithm carefully adapts to the evolving bias-variance trade-off as more data is collected. In particular, our algorithm and analysis go beyond adapting to the complexity of t
    
[^228]: 从非平凡性和有限性的不可判定到可学习性的不可判定

    From Undecidability of Non-Triviality and Finiteness to Undecidability of Learnability. (arXiv:2106.01382v3 [cs.CC] UPDATED)

    [http://arxiv.org/abs/2106.01382](http://arxiv.org/abs/2106.01382)

    本文证明了在PAC二分类、统一和通用的在线学习以及准确学习中，学习性都是不可判定的，即无法通过一般过程来确定新模型是否能够成功学习。我们通过将形式系统的一致性问题和图灵机的停机问题编码为函数类的平凡性/有限性与是否可学习相关联来证明这一结论。

    

    机器学习的研究者和实践者不断扩大成功的学习模型的多样性。他们通过深入的理论分析和经验性的启发来实现这一点。然而，目前没有已知的通用过程可以严格评估新提出的模型是否确实能够从数据中成功学习。我们展示了这样的过程不能存在。对于PAC二分类、统一和通用的在线学习，以及通过教师-学习者交互进行准确学习，学习性在一般情况下是不可判定的，即在形式系统中公理的独立性和无法计算性的意义上都是如此。我们的证明通过可计算的构造将形式系统的一致性问题和图灵机的停机问题编码为某些函数类是平凡/有限还是高度复杂，然后将这些类与通过已建立的学习性特征化是否可学习相关联。

    Machine learning researchers and practitioners steadily enlarge the multitude of successful learning models. They achieve this through in-depth theoretical analyses and experiential heuristics. However, there is no known general-purpose procedure for rigorously evaluating whether newly proposed models indeed successfully learn from data. We show that such a procedure cannot exist. For PAC binary classification, uniform and universal online learning, and exact learning through teacher-learner interactions, learnability is in general undecidable, both in the sense of independence of the axioms in a formal system and in the sense of uncomputability. Our proofs proceed via computable constructions that encode the consistency problem for formal systems and the halting problem for Turing machines into whether certain function classes are trivial/finite or highly complex, which we then relate to whether these classes are learnable via established characterizations of learnability through comp
    
[^229]: BLM-17m: 一个用于推特上黑人生命至关重要话题检测的大规模数据集

    BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2105.01331](http://arxiv.org/abs/2105.01331)

    本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。

    

    人权保护是世界上最重要的问题之一。本文旨在提供一个涵盖最近几个月全球影响深远的人权矛盾之一——乔治·弗洛伊德事件的数据集。我们提出了一个带有17百万推文的主题检测标记数据集。这些推文是从2020年5月25日至2020年8月21日收集的，涵盖了这一事件开始后的89天。我们通过监测全球和本地报纸的最热门新闻主题对数据集进行了标记。除此之外，我们还提供了两个基线模型，TF-IDF和LDA。我们使用三个不同的k值对这两种方法的精确度、召回率和F1分数进行了评估。收集到的数据集可以在https://github.com/MeysamAsgariC/BLMT 上找到。

    Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
    
[^230]: 用复合传输散度进行高斯混合简化

    Gaussian Mixture Reduction with Composite Transportation Divergence. (arXiv:2002.08410v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2002.08410](http://arxiv.org/abs/2002.08410)

    本文提出了一种基于复合传输散度的高斯混合简化方法，用于解决高斯混合在递归更新中阶数指数增加的推断问题。

    

    高斯混合在密度估计、信念传播和贝叶斯滤波等各种应用中被广泛用于逼近密度函数。这些应用通常利用高斯混合作为递归更新的初始近似。这些递归过程中的一个关键挑战源于混合阶数的指数增加，导致难以求解的推断问题。为了克服这个困难，可以使用高斯混合简化（GMR）将高阶高斯混合近似为低阶混合。尽管现有的基于聚类的方法在性能和计算效率上表现良好，但它们的收敛性质和最优目标仍然未知。在本文中，我们提出了一种基于复合传输散度的新型优化GMR方法。我们开发了一个主元最小化算法来计算简化的混合，并在g中建立了其理论收敛性。

    Gaussian mixtures are widely used for approximating density functions in various applications such as density estimation, belief propagation, and Bayesian filtering. These applications often utilize Gaussian mixtures as initial approximations that are updated recursively. A key challenge in these recursive processes stems from the exponential increase in the mixture's order, resulting in intractable inference. To overcome the difficulty, the Gaussian mixture reduction (GMR), which approximates a high order Gaussian mixture by one with a lower order, can be used. Although existing clustering-based methods are known for their satisfactory performance and computational efficiency, their convergence properties and optimal targets remain unknown. In this paper, we propose a novel optimization-based GMR method based on composite transportation divergence (CTD). We develop a majorization-minimization algorithm for computing the reduced mixture and establish its theoretical convergence under g
    
[^231]: 通过大规模事件嵌入和循环网络提高原生广告的CTR预测

    Improving Native Ads CTR Prediction by Large Scale Event Embedding and Recurrent Networks. (arXiv:1804.09133v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1804.09133](http://arxiv.org/abs/1804.09133)

    本文通过大规模事件嵌入和循环网络，提出了一种改进的CTR预测方法，在原生广告中取得了显著优势。

    

    点击率（CTR）预测对于原生广告非常重要，但由于没有直接的查询意图，因此很难。本文提出了一种大规模事件嵌入方案，通过对用户连续事件进行弱监督训练的孪生网络来编码每个用户浏览事件。CTR预测问题被建模为一个监督循环神经网络，自然地将用户历史建模为事件序列。我们提出的循环模型利用预训练的事件嵌入向量和注意层对用户历史进行建模。实验结果表明，我们的模型明显优于基线模型和一些变体。

    Click through rate (CTR) prediction is very important for Native advertisement but also hard as there is no direct query intent. In this paper we propose a large-scale event embedding scheme to encode the each user browsing event by training a Siamese network with weak supervision on the users' consecutive events. The CTR prediction problem is modeled as a supervised recurrent neural network, which naturally model the user history as a sequence of events. Our proposed recurrent models utilizing pretrained event embedding vectors and an attention layer to model the user history. Our experiments demonstrate that our model significantly outperforms the baseline and some variants.
    

