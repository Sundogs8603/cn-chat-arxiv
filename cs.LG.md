# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports.](http://arxiv.org/abs/2401.01883) | 本文的目标是通过分析网络威胁情报报告，从过去的网络攻击行为中挖掘出时间攻击模式，以帮助安全从业人员优先处理和主动防御网络攻击。 |
| [^2] | [Theoretical guarantees on the best-of-n alignment policy.](http://arxiv.org/abs/2401.01879) | 该论文研究了对齐生成模型的最佳n对齐策略，并证明了之前文献中的某个分析表达式是错误的。研究者们提出了一个新的KL散度估计方法，并通过实验证明其有效性。 |
| [^3] | [Graph Neural Networks for Surfactant Multi-Property Prediction.](http://arxiv.org/abs/2401.01874) | 本文研究了表面活性剂多性质预测的图神经网络模型。通过创建最大的CMC数据库和首个大型表面迁移浓度数据集，该模型成功预测了表面活性剂的关键性质。 |
| [^4] | [On the hardness of learning under symmetries.](http://arxiv.org/abs/2401.01869) | 本研究探讨通过梯度下降学习等变神经网络的困难性。我们发现已知的问题对称性无法缓解使用梯度下降学习神经网络的困难，并给出了多种网络形式的下界时间复杂度。 |
| [^5] | [Dataset Difficulty and the Role of Inductive Bias.](http://arxiv.org/abs/2401.01867) | 该论文研究了数据集中个别示例的难度评分，并发现不同训练运行、评分方法和模型架构之间的排名一致性通常是未知的。他们还提出了一种简单的方法来使用一些敏感性指纹模型架构。 |
| [^6] | [A Vision Check-up for Language Models.](http://arxiv.org/abs/2401.01862) | 本研究系统地评估了大型语言模型（LLMs）在生成和识别各种视觉概念方面的能力，并通过使用文本模型进行训练的视觉表示学习系统展示了LLMs对视觉世界的认知。实验结果表明，精确建模字符串可以教会LLMs关于视觉世界的多个方面，并且仅使用LLMs可以训练能够对自然图像进行语义评估的视觉模型。 |
| [^7] | [Optimal cross-learning for contextual bandits with unknown context distributions.](http://arxiv.org/abs/2401.01857) | 本文提出了一种针对“交叉学习”设置的上下文赌博算法，在未知上下文分布的情况下，通过协调多个时期内的学习算法的执行，实现了几乎紧致的遗憾界。此算法解决了学习在一价拍卖和睡觉赌徒问题中的应用难题。 |
| [^8] | [Transformer Neural Autoregressive Flows.](http://arxiv.org/abs/2401.01855) | 本文提出了一种称为变压器神经自回归流（T-NAFs）的新技术，通过利用变压器来解决神经自回归流的可扩展性和训练稳定性问题，并取得了实验验证。 |
| [^9] | [Multilingual Instruction Tuning With Just a Pinch of Multilinguality.](http://arxiv.org/abs/2401.01854) | 本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。 |
| [^10] | [The Power of Training: How Different Neural Network Setups Influence the Energy Demand.](http://arxiv.org/abs/2401.01851) | 本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。 |
| [^11] | [DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction.](http://arxiv.org/abs/2401.01846) | DGDNN是一种解耦的图传播神经网络用于股票走势预测，能够自动构建动态股票图并学习股票之间的任务最优依赖关系。 |
| [^12] | [Wasserstein Nonnegative Tensor Factorization with Manifold Regularization.](http://arxiv.org/abs/2401.01842) | 这个论文介绍了一种带有流形正则化的Wasserstein非负张量分解方法，通过最小化输入张量数据和重构数据之间的Wasserstein距离，利用特征的相关信息和样本的流形信息来进行特征提取和部分表示。 |
| [^13] | [Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes.](http://arxiv.org/abs/2401.01841) | 本文提出了一种自适应蒙特卡洛树搜索算法来应对非稳态环境下的决策问题，解决了传统方法中对环境动态假设的限制和规划过程的悲观性问题。 |
| [^14] | [Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling.](http://arxiv.org/abs/2401.01830) | 本文提出了一种新颖的文本增强方法，利用基于Transformer的BERT模型的填充-掩码功能，通过迭代掩码和语言模型预测进行替换，显著提高了自然语言处理任务的性能。 |
| [^15] | [Signal Processing in the Retina: Interpretable Graph Classifier to Predict Ganglion Cell Responses.](http://arxiv.org/abs/2401.01813) | 本文提出了一种可解释的基于图的分类器，用于预测神经节细胞对视觉刺激的放电。通过学习一个正半定度量矩阵来定义图节点之间的马氏距离，从而实现了对细胞潜在操作的理解。 |
| [^16] | [A quatum inspired neural network for geometric modeling.](http://arxiv.org/abs/2401.01801) | 这个论文介绍了一种创新的矩阵乘积态(MPS)的消息传递策略，通过这种策略可以更好地捕捉几何图中的复杂关系。 |
| [^17] | [CoMoSVC: Consistency Model-based Singing Voice Conversion.](http://arxiv.org/abs/2401.01792) | 本文提出了一种基于一致性模型的唱歌声音转换方法。通过设计扩散教师模型和提取自一致性学生模型，该方法实现了高质量的声音生成和高速的采样，相比于最先进的扩散方法，具有更快的推理速度，并在主观和客观指标上实现了可比或更优的声音转换性能。 |
| [^18] | [Deep learning the Hurst parameter of linear fractional processes and assessing its reliability.](http://arxiv.org/abs/2401.01789) | 本研究利用深度学习（特别是LSTM网络）评估了分数随机过程中的Hurst参数，并探讨了其可靠性。实证结果表明，在分数布朗运动和分数奥恩斯坦-乌伦贝克过程中，LSTM网络优于传统统计方法，但在线性分数稳定运动过程中准确性受限。 |
| [^19] | [Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review.](http://arxiv.org/abs/2401.01788) | 本文系统综述了机器学习和物联网在室外空气污染预测中的应用，以及使用的监测传感器和输入特征的组合。研究结果突出了高成本监测、低成本物联网和混合预测这三种方法。 |
| [^20] | [Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws.](http://arxiv.org/abs/2401.01783) | 该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。 |
| [^21] | [Understanding the Detrimental Class-level Effects of Data Augmentation.](http://arxiv.org/abs/2401.01764) | 理解了数据增强对类别级学习的影响，发现在图像分类任务中，数据增强在提高平均准确性的同时，会显著降低个别类别的准确性。这些影响主要出现在模糊、共现或细粒度区别的类别上，数据增强受控于模型对其中一个相关类别的偏好。 |
| [^22] | [Investigating the Suitability of Concept Drift Detection for Detecting Leakages in Water Distribution Networks.](http://arxiv.org/abs/2401.01733) | 本研究探索了概念漂移检测方法在水配水网络泄漏检测中的应用潜力，并提出了一种基于漂移检测的技术来定位泄漏。 |
| [^23] | [Task and Explanation Network.](http://arxiv.org/abs/2401.01732) | 该论文介绍了一个基本框架——任务和解释网络（TENet），它不仅能完成任务，还能解释为什么这样完成任务。它强调整个AI领域都应该重视可解释性。 |
| [^24] | [Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices.](http://arxiv.org/abs/2401.01728) | 本文提出了一种用于大型现代深度学习模型的异步分散式训练范式，通过连接互联网上的资源受限的异构个人计算机，利用计算能力来实现有利的性能指标。Ravnest通过有效地将计算节点组织成具有类似数据传输速率和计算能力的集群，实现了分散式训练，而不需要每个节点承载整个模型。 |
| [^25] | [EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector.](http://arxiv.org/abs/2401.01710) | 本论文提出了一种名为EPA的出域检测器，其受到神经崩溃的启发。通过观察特征与内域特征子空间之间的主角度，EPA能够更有效地衡量出域样本的可能性，并在实验证明其在性能和稳健性方面超过了其他方法。 |
| [^26] | [Zero-shot Active Learning Using Self Supervised Learning.](http://arxiv.org/abs/2401.01690) | 这项工作提出了一种新的无监督学习下的零样本主动学习方法，利用自监督学习得到的特征来选择最佳数据子集进行深度学习模型训练，从而提高模型在有限数据注释预算下的泛化性能。 |
| [^27] | [LESEN: Label-Efficient deep learning for Multi-parametric MRI-based Visual Pathway Segmentation.](http://arxiv.org/abs/2401.01654) | LESEN是一种标签高效深度学习方法，通过自我集成和可靠的未标注样本选择机制，有效地解决了基于多参数MRI的视觉途径分割中有限标注样本的问题，并取得了优越的性能。 |
| [^28] | [Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences.](http://arxiv.org/abs/2401.01641) | 本文提出了一种基于生成预训练方法的金融交易上下文嵌入模型，该模型在公共数据集上的测试中表现优于最先进的自监督方法，并在卡片欺诈检测问题上取得了显著的性能提升。 |
| [^29] | [Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data.](http://arxiv.org/abs/2401.01640) | 本研究通过比较自监督学习模型和监督学习模型在顺序数据上的表现发现，自监督学习模型能够实现与监督模型相当的性能，并显著提高公平性，这表明了其在人为中心的计算中的潜力。 |
| [^30] | [Synthetic Data in AI: Challenges, Applications, and Ethical Implications.](http://arxiv.org/abs/2401.01629) | 这篇论文讨论了人工智能中合成数据的挑战、应用和伦理影响。它介绍了合成数据的生成方法和应用领域，并强调了确保公平性、减少偏见和维护伦理标准在人工智能发展中的重要性。 |
| [^31] | [On the Expressive Power of Graph Neural Networks.](http://arxiv.org/abs/2401.01626) | 研究人员对图神经网络的表达能力和设计架构进行了大量工作，以提高其在各领域任务中的性能。主要方法包括研究GNN的通用逼近性质和其在区分不同图之间的能力程度。 |
| [^32] | [SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks.](http://arxiv.org/abs/2401.01625) | SCALA是一种使用稀疏化的对比学习框架，用于在属性网络中进行异常检测。该框架旨在改善网络的嵌入质量，并通过引入稀疏化来为每个节点提供一种新的衡量异常得分的方法。 |
| [^33] | [PLLaMa: An Open-source Large Language Model for Plant Science.](http://arxiv.org/abs/2401.01600) | PLLaMa是一种用于植物科学的开源大型语言模型，通过综合数据库增强，显著丰富了其在植物和农业科学方面的知识和专长，并通过与专业人员小组的合作验证了其准确性。 |
| [^34] | [Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay.](http://arxiv.org/abs/2401.01599) | 本文研究了核回归方法的泛化误差曲线，对核梯度下降方法和其他分析谱算法在核回归中的泛化误差进行了全面特征化，从而提高了对训练宽神经网络泛化行为的理解，并提出了一种新的技术贡献-分析功能论证。 |
| [^35] | [An Invariant Information Geometric Method for High-Dimensional Online Optimization.](http://arxiv.org/abs/2401.01579) | 本文介绍了一种基于不变性的演化策略算法，在贝叶斯能力上限的任务中与领先的贝叶斯优化方法竞争。我们构建了一个完全包含历史信息的框架，并在多维高斯分布上示例化了一种不变且可扩展的优化器。该算法相较于其他基于高斯的演化策略具有理论上的优势。 |
| [^36] | [Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal Prediction.](http://arxiv.org/abs/2401.01549) | 这篇论文介绍了一种通过符合性预测来模拟自解释神经网络的不确定性建模的方法。现有的自解释网络存在不足，无法提供关于同时生成的预测结果和相应解释的分布无关的不确定性量化，并且无法建立其之间的连接。 |
| [^37] | [The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers.](http://arxiv.org/abs/2401.01537) | 这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。 |
| [^38] | [Will 6G be Semantic Communications? Opportunities and Challenges from Task Oriented and Secure Communications to Integrated Sensing.](http://arxiv.org/abs/2401.01531) | 本文研究了面向任务和语义通信的机遇和挑战，通过多任务学习的集成，采用深度神经网络，在发送端使用编码器和多个解码器来处理多种任务，并在多接收器环境下利用分散式学习解决通信负载和隐私关注。需要注意深度学习模型的稳健性。 |
| [^39] | [Improved Bandits in Many-to-one Matching Markets with Incentive Compatibility.](http://arxiv.org/abs/2401.01528) | 本文研究了在具有激励兼容性的多对一匹配市场中改进赌博算法的问题，并提出了适应性探索-延迟接受（AETDA）算法来提高遗憾上限。 |
| [^40] | [S$^{2}$-DMs:Skip-Step Diffusion Models.](http://arxiv.org/abs/2401.01520) | 本论文提出了一种新的训练方法S$^{2}$-DMs，通过创新的$L_{skip}$重新整合选择性采样阶段中省略的信息，显著提高了样本质量，并且实现简单，对代码修改要求少，与各种采样算法兼容。 |
| [^41] | [Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review.](http://arxiv.org/abs/2401.01519) | 本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。 |
| [^42] | [AIRI: Predicting Retention Indices and their Uncertainties using Artificial Intelligence.](http://arxiv.org/abs/2401.01506) | 使用深度神经网络预测化合物保留指数，并量化其不确定性，以改善化学鉴定方法和库的质量。 (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.) |
| [^43] | [Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games.](http://arxiv.org/abs/2401.01502) | 本文提出了一种Pontryagin模式的神经算子，通过在前向和反向共轭状态回滚之间的差异上定义的损失，解决了在具有参数化状态约束的博弈中值的不连续性的收敛问题，并在安全性能上优于现有的最先进算法。 |
| [^44] | [Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction.](http://arxiv.org/abs/2401.01498) | 利用神经转导器实现了一个两阶段的文本到语音转换框架，通过语义标记预测实现了稳健高效的对齐建模，并通过非自回归语音生成器合成语音波形。该框架在语音质量和说话者相似度方面超过了基线模型。 |
| [^45] | [Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework.](http://arxiv.org/abs/2401.01493) | 这项研究提出了一个名为PRFL的隐私保护TFGC框架，基于联邦学习，旨在解决遥感目标细粒度分类中的数据隐私和通信效率问题。 |
| [^46] | [Natural Language Processing and Multimodal Stock Price Prediction.](http://arxiv.org/abs/2401.01487) | 本文使用自然语言处理和多模式股票价格预测，利用股票百分比变化作为训练数据，分析公开发布的新闻文章。结果展示了使用小型自然语言处理模型准确预测总体股票趋势的能力，以及特定数据特征和选择的有效性。 |
| [^47] | [Uncertainty Regularized Evidential Regression.](http://arxiv.org/abs/2401.01484) | 本文研究了证据回归网络（ERN）中的模型性能限制问题，并提出了一种基于正则化的改进方法，使ERN能够从整个训练集中学习。 |
| [^48] | [Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition.](http://arxiv.org/abs/2401.01482) | 本文研究了在目标识别中将地理多样知识融入提示以提高地理鲁棒性的方法，探索了通过大型语言模型获取地理特定对象知识并结合CLIP视觉语言模型的零样本和可学习软提示。通过提出一种地理知识正则化方法，实现了从源地理位置推广到未见目标地理位置的鲁棒性提升。 |
| [^49] | [Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting.](http://arxiv.org/abs/2401.01479) | Kernel-U-Net是一种层次和对称框架，用于多元时间序列预测。与现有模型相比，它具有较少的参数数量、灵活性和计算效率。 |
| [^50] | [A First Look at Information Highlighting in Stack Overflow Answers.](http://arxiv.org/abs/2401.01472) | 本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。 |
| [^51] | [Token Propagation Controller for Efficient Vision Transformer.](http://arxiv.org/abs/2401.01470) | 本文提出一种新颖的令牌传播控制器（TPC），通过结合暂停概率和重新开始概率，实现了对令牌的减少和重复利用的控制，从而提高了视觉Transformer的效率和令牌利用率。 |
| [^52] | [Point Cloud Classification via Deep Set Linearized Optimal Transport.](http://arxiv.org/abs/2401.01460) | 通过深度学习方法，我们提出了一种名为深度集合线性化最优传输的算法，用于将点云嵌入到L2空间，并构建了一个可以区分不同类别点云的分类器。我们的算法利用输入凸神经网络学习点云之间的Wasserstein-2距离的近似，并通过训练鉴别器网络来提高分类准确率。 |
| [^53] | [Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint.](http://arxiv.org/abs/2401.01458) | 本文提出了使用不确定性指纹进行神经网络的并发自测的方法，可以在在线操作中检测并纠正单个和多个永久和软错误，适用于始终开启的安全关键应用。 |
| [^54] | [ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification.](http://arxiv.org/abs/2401.01448) | ProbMCL是一个简单而有效的概率对比学习框架，用于解决多标签图像分类任务中的挑战。该方法通过采用监督对比学习和混合密度网络，在捕捉标签之间的依赖关系的同时，降低了复杂模块的计算需求和可解释性的不足。 |
| [^55] | [Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity.](http://arxiv.org/abs/2401.01442) | 本文介绍了一种具有干扰和数据异质性意识的分层无线联邦学习方法，通过优化接收机的归一化因子来最小化干扰影响，并利用梯度聚合对抗数据异质性，实现了较高的学习准确性，并且可以优于传统的分层算法。 |
| [^56] | [Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference.](http://arxiv.org/abs/2401.01426) | 本文提出了一种用于高维因果推断的模块化深度生成模型学习算法，该算法利用预训练的模型来回答由高维数据引起的因果查询。 |
| [^57] | [SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset.](http://arxiv.org/abs/2401.01425) | 本文旨在通过在OSHA数据集上的模仿学习，设计高速公路上的自动超车和换道策略规划模型SwapTransformer。通过引入辅助任务和比较基准模型，证明了该模型在仿真环境中的性能优势。 |
| [^58] | [VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics.](http://arxiv.org/abs/2401.01414) | VALD-MD是一种基于潜在扩散的视觉归因技术，通过结合潜在扩散模型和大型语言模型，在医学图像中生成正常对应的异常图像，从而揭示出与诊断相关的图像部分。 |
| [^59] | [Scalable network reconstruction in subquadratic time.](http://arxiv.org/abs/2401.01404) | 这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。 |
| [^60] | [Accelerating Black-Box Molecular Property Optimization by Adaptively Learning Sparse Subspaces.](http://arxiv.org/abs/2401.01398) | 通过自适应学习稀疏子空间加速黑箱分子性质优化的挑战已经通过学习更低维的编码得到解决 |
| [^61] | [Deep autoregressive modeling for land use land cover.](http://arxiv.org/abs/2401.01395) | 本研究使用深度自回归模型对土地利用和土地覆盖进行建模，在捕捉丰富的空间相关模式方面超过了基准模型，但需要进一步调整以产生更准确的预测分布。 |
| [^62] | [Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and Stochastic root finding.](http://arxiv.org/abs/2401.01393) | 本文介绍了回溯新的Q-Newton方法（BNQN），该方法是Newton方法的一种变体，具有强大的理论保证，易于实现，且在实验中有良好的表现。通过实验，发现BNQN在多项式和亚纯函数的根搜索中具有更平滑的吸引域，并通过与Newton流和Voronoi图的联系提出了一些挑战性的问题。此外，在面对随机扰动时，BNQN比Newton方法和随机松弛Newton方法更具鲁棒性。 |
| [^63] | [On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding.](http://arxiv.org/abs/2401.01391) | 本文针对采样率与学习神经隐式场的准确性之间的关系进行了研究，在傅里叶分析的基础上提出了一种简单有效的方法来确定适当的采样率，以解决MLP中噪声伪影的问题。 |
| [^64] | [Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition.](http://arxiv.org/abs/2401.01388) | 本论文研究了适用于长距离穿墙人体活动识别的定向天线系统，提出了ESP32-S3与定向双极天线和ESP32-S3与印刷倒F天线结合的两个有前景的系统。这些系统在WiFi-based HAR中展现出出色的性能。 |
| [^65] | [Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images.](http://arxiv.org/abs/2401.01386) | 这篇论文提出了一种使用全幻灯切片图像进行组织伪影分割与严重性分析的自动诊断方法。通过计算机视觉和人工智能，可以在没有人类监督的情况下对整个全幻灯切片图像进行自主分析，但受到组织伪影影响的区域需要被准确识别和排除。 |
| [^66] | [Strong Transitivity Relations and Graph Neural Networks.](http://arxiv.org/abs/2401.01384) | 这项研究提出了一种基于传递关系的相似性扩展，通过引入传递图神经网络（TransGNN）从全局和局部的角度捕捉整个图的相似性，显著提高了几个GNN模型的性能。 |
| [^67] | [Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data.](http://arxiv.org/abs/2401.01383) | 我们提出了一种使用联邦多轨迹GNN的方法，通过稀缺数据预测婴儿脑连接性。通过联邦学习，我们通过聚合多个医院的本地学习结果来提高模型性能，同时保护数据隐私。 |
| [^68] | [Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning.](http://arxiv.org/abs/2401.01375) | 本研究提出了一种利用高分辨率多光谱无人机影像和机器学习进行核桃水分胁迫测绘的方法，通过结合无人机影像和天气数据，使用随机森林模型有效估计了核桃树的茎水势，为核桃精准灌溉管理提供了重要的参考依据。 |
| [^69] | [Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks.](http://arxiv.org/abs/2401.01373) | 我们引入了一种张量卷积神经网络（T-CNN）来提高制造业中的缺陷检测任务，通过减少模型参数空间，我们实现了比等效CNN模型更快的训练速度和性能。与传统的人类视觉检查相比，在质量指标方面，T-CNN在参数数量上只有15倍少，训练时间快4%至19%。这项研究在实际制造应用中取得了显著的成果。 |
| [^70] | [RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems.](http://arxiv.org/abs/2401.01369) | RL-MPCA是一种基于强化学习的多阶段计算分配方法，用于解决推荐系统中在计算资源有限情况下的计算成本和业务收益之间的权衡问题。 |
| [^71] | [Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations.](http://arxiv.org/abs/2401.01364) | 该研究通过使用神经网络和后继表示训练，建立了一个能够模拟位置细胞动态和认知地图表示的多模态认知地图模型。该模型能够以超过90%的准确率从一种形式推断到另一种形式，对于改善当前人工智能系统对环境的理解具有潜在意义。 |
| [^72] | [Optimizing Convolutional Neural Network Architecture.](http://arxiv.org/abs/2401.01361) | 本文提出了一种优化卷积神经网络架构的方法，通过剪枝和知识蒸馏来建立卷积层的重要性。通过实证研究，我们证明了该方法在各种数据集和CNN架构上的性能优于其他先进方法。 |
| [^73] | [IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection.](http://arxiv.org/abs/2401.01343) | 本研究提出了一种通用模型，用于基于行为的物联网攻击检测。该模型通过改进滚动窗口特征提取、引入多步骤特征选择、使用隔离的训练和测试数据集以及使用可解释的人工智能技术来提高检测和性能，并且经过严格评估。 |
| [^74] | [Securing the Digital World: Protecting smart infrastructures and digital industries with Artificial Intelligence (AI)-enabled malware and intrusion detection.](http://arxiv.org/abs/2401.01342) | 本论文探究了基于人工智能的网络威胁检测，重点是评估基于机器学习的分类器和集成，用于异常恶意软件检测和网络入侵检测，并讨论了如何将这些模型整合到网络安全和移动设备环境中。 |
| [^75] | [Fairness Certification for Natural Language Processing and Large Language Models.](http://arxiv.org/abs/2401.01262) | 这项研究旨在为自然语言处理领域开发公平性认证方法。通过综述大量文献和专家访谈，我们提出了六个公平性标准，为操作化和测试过程提供了基础。 |
| [^76] | [Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework.](http://arxiv.org/abs/2401.00744) | 在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。 |
| [^77] | [Kernel Density Estimation for Multiclass Quantification.](http://arxiv.org/abs/2401.00490) | 本文提出了一种用于多类别量化的核密度估计方法，相比于目前的方法，它能更好地模拟数据中的类间信息。 |
| [^78] | [Diffusion Model with Perceptual Loss.](http://arxiv.org/abs/2401.00110) | 本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。 |
| [^79] | [Are All Unseen Data Out-of-Distribution?.](http://arxiv.org/abs/2312.16243) | 该论文研究了未见数据的分布在泛化中的挑战，并提出了重新定义OoD数据以及新的泛化上界，保证了对未见数据的模型有效性。 |
| [^80] | [M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy.](http://arxiv.org/abs/2312.15927) | 该论文提出了一种名为M3D的新型基于分布匹配的数据集压缩方法，通过最小化最大均值差异来提高压缩效率，克服了优化过程在实际和较大数据集上的应用难题。 |
| [^81] | [Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report).](http://arxiv.org/abs/2312.15097) | 本研究提出了一种名为“追索感知的集成”的方法，通过使用计算论证方法来解决模型多样性下提供反事实解释的挑战性。该方法保证了在模型多样性情况下CEs的鲁棒性，并可以适应用户的个性化偏好。 |
| [^82] | [SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image Segmentation.](http://arxiv.org/abs/2312.14705) | 这项研究提出了一种名为SCUNet++的自动肺栓塞分割方法，利用Swin Transformer作为编码器，并通过多重融合稠密跳跃连接在编码器和解码器之间实现特征融合，以解决传统方法无法充分考虑特征分层结构和局部、全局空间特征的问题。 |
| [^83] | [Manipulating Trajectory Prediction with Backdoors.](http://arxiv.org/abs/2312.13863) | 本文研究了在轨迹预测中经常被忽视的安全威胁——后门攻击。我们发现通过一些触发器的操纵，可以使现有的轨迹预测模型在受到特定条件刺激时预测错误，揭示了模型的局限性。 |
| [^84] | [Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models.](http://arxiv.org/abs/2312.13763) | 本研究针对文本到4D对象合成问题，提出了一种创新的组合生成方法。通过利用动态三维高斯簇和组合扩散模型，我们同时实现了时间一致性、高质量的视觉效果和逼真的几何特征。此外，我们还引入了一种新的规范方法来稳定优化过程并增强运动效果。 |
| [^85] | [Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models.](http://arxiv.org/abs/2312.13307) | 提出了一种步骤自适应训练的两阶段策略，解决了传统扩散模型中训练过程中的冲突问题，将模型大小调整与噪声预测难度相匹配，提高了生成效果。 |
| [^86] | [Topological Data Analysis for Neural Network Analysis: A Comprehensive Survey.](http://arxiv.org/abs/2312.05840) | 这篇综述全面探索了拓扑数据分析在神经网络分析中的应用。它讨论了通过拓扑信息来分析神经网络的特性，并探索了在深度学习中的实际应用领域。研究将工作分为四个领域，涵盖了神经网络结构、决策区域、内部表示和训练动力学等方面。 |
| [^87] | [Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and LAnguage in Conversational Environments.](http://arxiv.org/abs/2311.12564) | DISPLACE Challenge 2023是一个关于在多语言对话环境中进行发言人和语言辨识的挑战，通过评估和基准测试不同技术的表现来提高这种辨识的效率和准确性。 |
| [^88] | [Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents.](http://arxiv.org/abs/2310.19923) | Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。 |
| [^89] | [Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks.](http://arxiv.org/abs/2310.19274) | 本研究提出了一种基于图神经网络的方法，使用数字CT扫描图像预测岩石的有效弹性模量。通过将岩石图像转换为图数据集并经过训练，这种方法在预测弹性模量方面表现出良好的效果，并且相比于CNN具有更高的预测性能和更低的内存需求。 |
| [^90] | [Understanding the Effects of RLHF on LLM Generalisation and Diversity.](http://arxiv.org/abs/2310.06452) | 本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。 |
| [^91] | [What's the Magic Word? A Control Theory of LLM Prompting.](http://arxiv.org/abs/2310.04444) | 本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。 |
| [^92] | [Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection.](http://arxiv.org/abs/2310.04171) | 本研究针对欺诈检测问题，通过动态关系注意聚合机制，提出了一种基于图神经网络（GNN）的方法。该方法学习每个关系的节点表示，并利用可学习的注意函数进行节点表示的聚合。通过结合不同层次的节点表示，考虑目标节点的局部和全局结构，以提高欺诈检测性能。通过使用动态图注意力，可以自适应地计算关系之间的重要程度。 |
| [^93] | [On Memorization and Privacy Risks of Sharpness Aware Minimization.](http://arxiv.org/abs/2310.00488) | 本研究通过对过度参数化模型中的数据记忆的剖析，揭示了尖锐意识最小化算法在非典型数据点上实现的泛化收益。同时，也发现了与此算法相关的更高隐私风险，并提出了缓解策略，以达到更理想的准确度与隐私权衡。 |
| [^94] | [Low-budget Black-box Optimization Algorithms Evaluated on BBOB and OpenAI Gym.](http://arxiv.org/abs/2310.00077) | 这项研究旨在探讨机器学习和黑盒优化之间的交叉应用潜力，并通过比较实验评估了低成本黑盒优化算法在不同领域的效果。 |
| [^95] | [Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes.](http://arxiv.org/abs/2309.17207) | 本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。 |
| [^96] | [Auto-grading C programming assignments with CodeBERT and Random Forest Regressor.](http://arxiv.org/abs/2309.15216) | 本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。 |
| [^97] | [An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders.](http://arxiv.org/abs/2309.02084) | 本文提出了一种利用变分自动编码器进行无监督的离群检测的高效方法，通过引入误差减少(ER)离群得分来改进普通VAE，在各种数据集上得到了优于基准方法的实验结果。 |
| [^98] | [LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs.](http://arxiv.org/abs/2308.08469) | 这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。 |
| [^99] | [Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel.](http://arxiv.org/abs/2308.00583) | 使用量子核的支持向量回归方法被应用于半监督异常检测中，针对NISQ设备的限制，该方法具有理论保证、灵活性和兼容性。 |
| [^100] | [Fading memory as inductive bias in residual recurrent networks.](http://arxiv.org/abs/2307.14823) | 通过使用残差连接作为归纳偏差，我们提出了一种弱耦合残差循环网络，并研究了其对网络性能、动力学和记忆属性的影响。我们发现，几种不同形式的残差连接可以增加网络的表达能力，并产生有效的归纳偏差。 |
| [^101] | [Efficient selective attention LSTM for well log curve synthesis.](http://arxiv.org/abs/2307.10253) | 本文提出了一种高效的选择性注意力LSTM方法，用于预测缺失的井曲线。通过实验证实了该方法的有效性和可行性。 |
| [^102] | [On the hierarchical Bayesian modelling of frequency response functions.](http://arxiv.org/abs/2307.06263) | 结构健康监测(PBSHM)旨在在人群成员之间共享信息以改善健康状态的推断。由于差异和数据丢失所带来的挑战需要解决。 |
| [^103] | [Do DL models and training environments have an impact on energy consumption?.](http://arxiv.org/abs/2307.05520) | 本研究分析了模型架构和训练环境对训练更环保的计算机视觉模型的影响，并找出了能源效率和模型正确性之间的权衡关系。 |
| [^104] | [TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation.](http://arxiv.org/abs/2307.05134) | 本文提出了一种评估文本到图像生成中对齐性的新度量方法TIAM，该方法基于提示模板，可以更好地描述生成图像与提示中内容的对齐程度，包括对象类型、数量和颜色。研究结果表明，图像质量可以有很大的变化。 |
| [^105] | [Parallel Algorithms Align with Neural Execution.](http://arxiv.org/abs/2307.04049) | 这项研究发现，与顺序算法相比，并行算法能充分利用神经算法推理器的计算能力，从而减少训练时间并获得更好的预测性能。 |
| [^106] | [CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery.](http://arxiv.org/abs/2307.00859) | CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。 |
| [^107] | [The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions.](http://arxiv.org/abs/2306.07774) | 该论文提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播，通过将Lyapunov方程投影到低秩矩阵的流形上，使用数值稳定的动态低秩积分器求解，能够有效地处理高维数据。 |
| [^108] | [Hyperbolic Graph Diffusion Model for Molecule Generation.](http://arxiv.org/abs/2306.07618) | 本文提出了基于双曲图扩散模型的分子生成方法，可以更全面地捕捉分子的内部非欧几里德结构，实现数据生成，并提取复杂几何特征的能力。 |
| [^109] | [Sharper Bounds for $\ell_p$ Sensitivity Sampling.](http://arxiv.org/abs/2306.00732) | 该论文研究了$\ell_p$子空间嵌入的灵敏度采样界限，取得了比通用界限更好的结果，对于$1\leq p<2$的情况下，界限达到了$\mathfrak{S}^{2/p}$，对于$2<p<\infty$的情况下，界限达到了$\mathfrak{S}^{2-2/p}$。 |
| [^110] | [Large Language Models Are Not Abstract Reasoners.](http://arxiv.org/abs/2305.19555) | 本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。 |
| [^111] | [The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs).](http://arxiv.org/abs/2305.17033) | 这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。 |
| [^112] | [In the Name of Fairness: Assessing the Bias in Clinical Record De-identification.](http://arxiv.org/abs/2305.11348) | 本文研究了临床记录去识别系统在不同人口群体中的表现差异，揭示了其在名称去识别方面存在显著的偏见。 |
| [^113] | [Adversarial Representation Learning for Robust Privacy Preservation in Audio.](http://arxiv.org/abs/2305.00011) | 本研究提出了一种对抗性训练方法，用于学习音频的表征，从而有效地防止从音频记录的潜在特征中检测到语音活动，提出的方法能够使得包含语音的音频记录的潜在表征与不包含语音的音频记录的潜在表征无法被语音分类器区分出来。 |
| [^114] | [On the Optimality of Misspecified Spectral Algorithms.](http://arxiv.org/abs/2303.14942) | 在本文中，我们研究了非准确谱算法的最优性问题。我们证明了在一些特定的RKHSs上，谱算法对于所有的$s\in (0,1)$都是极小极大最优的。 |
| [^115] | [A unified recipe for deriving (time-uniform) PAC-Bayes bounds.](http://arxiv.org/abs/2302.03421) | 该论文提出了一种用于推导PAC-Bayesian泛化界限的统一框架，不同于传统的固定样本大小方式，该框架适用于所有停止时间。同时，该论文还提出了新的边界方法，也可以应用于非平稳损失函数和非独立同分布的数据。 |
| [^116] | [Bayesian posterior approximation with stochastic ensembles.](http://arxiv.org/abs/2212.08123) | 本文提出一种新方法，即使用随机神经网络集合来近似贝叶斯后验，并通过变分推断进行训练，实验证明该方法比其他流行的贝叶斯推断基线提供了更准确的后验估计。 |
| [^117] | [Selective classification using a robust meta-learning approach.](http://arxiv.org/abs/2212.05987) | 这项研究提出了使用鲁棒性元学习方法的选择性分类。通过一个辅助网络捕捉预测不确定性，并在训练和测试中应用，我们成功地最小化了辍学方差，从而提高了在真实世界中的分类性能。 |
| [^118] | [Low Variance Off-policy Evaluation with State-based Importance Sampling.](http://arxiv.org/abs/2212.03932) | 本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。 |
| [^119] | [Disentangled (Un)Controllable Features.](http://arxiv.org/abs/2211.00086) | 该论文提出了一种新方法，可以将潜在特征分解为可控和不可控的部分，并证明了这种分解表示在不同环境中易于解释和使用可解释的规划算法。 |
| [^120] | [DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality.](http://arxiv.org/abs/2210.13702) | 本文介绍了一种从模拟到现实的敏捷手部操作的转移方法，通过训练适应各种条件的策略和鲁棒位姿估计器，实现了在类人机器人手中进行稳健的灵巧操作，并证实了模拟到现实的转移对敏捷操作的可行性。 |
| [^121] | [Bridging the Gap Between Target Networks and Functional Regularization.](http://arxiv.org/abs/2210.12282) | 目标网络在深度强化学习中具有关键作用，但其对优化的影响还不为人所理解。本文提出了一种显式的功能正则化方法，通过替换目标网络来改善样本效率和性能。实验证明，这种方法能够更好地稳定训练，并取得了较好的结果。 |
| [^122] | [Validation of Composite Systems by Discrepancy Propagation.](http://arxiv.org/abs/2210.12061) | 本研究提出了一种通过差异传播验证复合系统的方法，可以从潜在不准确的模拟中推导出真实系统的故障概率上界。对于度量如最大平均差异 (MMD) ，我们开发了紧凑的凸松弛，并证明了该方法在高维输入空间中具有可比较的计算效率和良好的验证结果。 |
| [^123] | [Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training.](http://arxiv.org/abs/2208.12511) | 本文研究了通过降低优化难度来提高对抗训练中的鲁棒性。基于新颖的Bregman散度视角，分析了两种典型的对抗训练方法的学习目标，并找到了优化过程更容易的方法。提出的两种方法能够降低优化难度，并提供更好的鲁棒性。 |
| [^124] | [Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach.](http://arxiv.org/abs/2208.10962) | 本研究提出了一种新的深度学习方法，使用正则化稀疏自编码器预测良好的反应坐标以及MD轨迹的演化情况，并展示了正则化约束对于选择重要反应坐标的帮助。 |
| [^125] | [SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data.](http://arxiv.org/abs/2207.14650) | SYNTA是一种使用逼真的合成数据进行深度学习图像分析的新方法，可以在肌肉组织病理学中进行强大且专业水平的分割任务，无需手动注释。 |
| [^126] | [A New Frontier of AI: On-Device AI Training and Personalization.](http://arxiv.org/abs/2206.04688) | 基于深度学习的智能服务现已开始在设备上执行，以保护个人数据并降低云开销，而研究者提出了一种轻量级的设备上训练框架NNTrainer，可以在不牺牲准确性的同时减少内存消耗并实现智能服务的个性化。 |
| [^127] | [A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting.](http://arxiv.org/abs/2205.15580) | 这种方法在分布式非凸问题中通过方差缩减、部分参与和压缩通信的组合实现了最佳预言复杂度，同时无需所有节点参与和有界梯度假设。 |
| [^128] | [DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation.](http://arxiv.org/abs/2205.00147) | DIRA是一个用于DNN分类器的动态领域自适应的框架，使用正则化技术来解决灾难性遗忘问题，并通过少量样本实现重新训练和适应性。 |
| [^129] | [Improving Human Sequential Decision-Making with Reinforcement Learning.](http://arxiv.org/abs/2108.08454) | 该论文提出了一种利用机器学习算法从追踪数据中提取"最佳实践"并向人类传达的方法，以改进人类的顺序决策。通过一系列实验证实，在虚拟厨房管理任务中，这种方法能够显著提高性能。 |
| [^130] | [How to avoid machine learning pitfalls: a guide for academic researchers.](http://arxiv.org/abs/2108.02497) | 该论文总结了在机器学习中常见的错误，并提供了避免这些错误的方法和指南。其中包括在模型构建之前的准备工作、可靠地构建模型、稳健地评估模型、公平比较模型以及报告结果等五个关键阶段。 |

# 详细

[^1]: 从网络威胁情报报告中挖掘时间攻击模式

    Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports. (arXiv:2401.01883v1 [cs.CR])

    [http://arxiv.org/abs/2401.01883](http://arxiv.org/abs/2401.01883)

    本文的目标是通过分析网络威胁情报报告，从过去的网络攻击行为中挖掘出时间攻击模式，以帮助安全从业人员优先处理和主动防御网络攻击。

    

    防御网络攻击要求从高级对手行为入手。过去的网络攻击事件的网络威胁情报(CTI)报告描述了恶意行动的时间链。为了避免重复发生网络攻击事件，从过去的网络攻击行为中主动识别和防御重复链式行动 - 我们将其称为时间攻击模式。自动挖掘行动之间的模式提供了关于过去网络攻击的对手行为的结构化和可操作的信息。本文旨在通过从网络威胁情报报告中挖掘时间攻击模式，帮助安全从业人员优先处理和主动防御网络攻击。为此，我们提出了ChronoCTI，一个自动化的管道，用于从过去的网络威胁情报(CTI)报告中挖掘时间攻击模式。为构建ChronoCTI，我们构建了时间攻击模式的基准数据集，并应用了最先进的技术。

    Defending from cyberattacks requires practitioners to operate on high-level adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack incidents describe the chain of malicious actions with respect to time. To avoid repeating cyberattack incidents, practitioners must proactively identify and defend against recurring chain of actions - which we refer to as temporal attack patterns. Automatically mining the patterns among actions provides structured and actionable information on the adversary behavior of past cyberattacks. The goal of this paper is to aid security practitioners in prioritizing and proactive defense against cyberattacks by mining temporal attack patterns from cyberthreat intelligence reports. To this end, we propose ChronoCTI, an automated pipeline for mining temporal attack patterns from cyberthreat intelligence (CTI) reports of past cyberattacks. To construct ChronoCTI, we build the ground truth dataset of temporal attack patterns and apply state-of-
    
[^2]: 关于最佳n对齐策略的理论保证

    Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])

    [http://arxiv.org/abs/2401.01879](http://arxiv.org/abs/2401.01879)

    该论文研究了对齐生成模型的最佳n对齐策略，并证明了之前文献中的某个分析表达式是错误的。研究者们提出了一个新的KL散度估计方法，并通过实验证明其有效性。

    

    一个简单有效的生成模型对齐方法是最佳n对齐策略，该策略从一个基本策略中抽取n个样本，并根据奖励函数对它们进行排序，选择排名最高的样本。文献中常用的分析表达式声称最佳n对齐策略与基本策略之间的KL散度等于$\log (n) (n-1)/n$。我们证明了该论断的不正确性，并展示了它只是实际KL散度的一个上界。我们还研究了在不同情况下该上界的紧致性。最后，我们提出了一种新的KL散度估计方法，并通过几个例子的实验证明它能提供一个紧致的近似。

    A simple and effective method for the alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the base policy is equal to $\log (n) (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes. Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples.
    
[^3]: 表面活性剂多性质预测的图神经网络研究

    Graph Neural Networks for Surfactant Multi-Property Prediction. (arXiv:2401.01874v1 [physics.chem-ph])

    [http://arxiv.org/abs/2401.01874](http://arxiv.org/abs/2401.01874)

    本文研究了表面活性剂多性质预测的图神经网络模型。通过创建最大的CMC数据库和首个大型表面迁移浓度数据集，该模型成功预测了表面活性剂的关键性质。

    

    表面活性剂在化妆品、洗涤剂、油田回收和药物传递系统等不同工业领域中具有高度重要性。因此，已经开发了许多用于表面活性剂的定量结构-性能关系（QSPR）模型。每个预测模型通常侧重于一类表面活性剂，主要是非离子表面活性剂。图神经网络（GNNs）在离子液体、聚合物和药物等领域的性能预测中表现出了很好的预测性能。对于表面活性剂，GNNs可以成功预测关键胶束浓度（CMC），这是与胶束化相关的关键表面活性剂性质。对于QSPR和GNN模型的预测能力的一个关键因素是用于训练的数据的可用性。基于广泛的文献调研，我们创建了最大的可用CMC数据库，其中包含429个分子，并且首次进行了与泡沫相关的另一表面活性剂性质——表面迁移浓度（$\Gamma$$_{m}$）的大型数据收集，其中包含164个分子。

    Surfactants are of high importance in different industrial sectors such as cosmetics, detergents, oil recovery and drug delivery systems. Therefore, many quantitative structure-property relationship (QSPR) models have been developed for surfactants. Each predictive model typically focuses on one surfactant class, mostly nonionics. Graph Neural Networks (GNNs) have exhibited a great predictive performance for property prediction of ionic liquids, polymers and drugs in general. Specifically for surfactants, GNNs can successfully predict critical micelle concentration (CMC), a key surfactant property associated with micellization. A key factor in the predictive ability of QSPR and GNN models is the data available for training. Based on extensive literature search, we create the largest available CMC database with 429 molecules and the first large data collection for surface excess concentration ($\Gamma$$_{m}$), another surfactant property associated with foaming, with 164 molecules. Then
    
[^4]: 关于学习对称性困难性的研究

    On the hardness of learning under symmetries. (arXiv:2401.01869v1 [cs.LG])

    [http://arxiv.org/abs/2401.01869](http://arxiv.org/abs/2401.01869)

    本研究探讨通过梯度下降学习等变神经网络的困难性。我们发现已知的问题对称性无法缓解使用梯度下降学习神经网络的困难，并给出了多种网络形式的下界时间复杂度。

    

    我们研究了通过梯度下降学习等变神经网络的问题。将已知的对称性（"等变性"）纳入神经网络中，已经在从生物学到计算机视觉等领域的学习流程中经验上改善了性能。然而，丰富而独立的学习理论研究已经证明，在相关统计查询（CSQ）模型中，实际学习浅层全连接（即非对称）网络在复杂性上具有指数级的复杂度，该模型包括梯度下降。在这项工作中，我们提出了一个问题：已知的问题对称性是否足以缓解使用梯度下降学习神经网络的基本困难？我们的答案是否定的。具体地，我们给出了浅层图神经网络、卷积网络、不变多项式和置换子群的帧平均网络的下界，它们在相关输入维度上的时间复杂度要么超多项式，要么具有指数级的增长。

    We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries ("equivariance") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimens
    
[^5]: 数据集难度和归纳偏差的作用

    Dataset Difficulty and the Role of Inductive Bias. (arXiv:2401.01867v1 [cs.LG])

    [http://arxiv.org/abs/2401.01867](http://arxiv.org/abs/2401.01867)

    该论文研究了数据集中个别示例的难度评分，并发现不同训练运行、评分方法和模型架构之间的排名一致性通常是未知的。他们还提出了一种简单的方法来使用一些敏感性指纹模型架构。

    

    受数据集修剪和缺陷识别目标的启发，已经开发出一系列方法来评分数据集中的个别示例。这些方法，我们称之为“示例难度评分”，通常用于对示例进行排名或分类，但是不同训练运行、评分方法和模型架构之间的排名一致性通常是未知的。为了确定由于这些随机和受控效应而导致的示例排名的变化情况，我们系统地比较了不同得分公式在多个运行和模型架构的范围内。我们发现得分主要具有以下特点：它们在模型的单次运行中存在噪音，并且与一种难度概念强相关，并揭示了一些示例在某些模型架构的归纳偏差中从高度敏感到不敏感的变化。借鉴统计遗传学的方法，我们开发了一种简单的方法来使用一些敏感性指纹模型架构。

    Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset. These methods, which we call "example difficulty scores", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown. To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures. We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures. Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive 
    
[^6]: 语言模型的视觉检查

    A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])

    [http://arxiv.org/abs/2401.01862](http://arxiv.org/abs/2401.01862)

    本研究系统地评估了大型语言模型（LLMs）在生成和识别各种视觉概念方面的能力，并通过使用文本模型进行训练的视觉表示学习系统展示了LLMs对视觉世界的认知。实验结果表明，精确建模字符串可以教会LLMs关于视觉世界的多个方面，并且仅使用LLMs可以训练能够对自然图像进行语义评估的视觉模型。

    

    学习建模字符串之间关系是否能教会大型语言模型（LLMs）关于视觉世界的知识？我们系统地评估了LLMs生成和识别各种逐渐复杂的视觉概念的能力，然后演示了如何使用文本模型来训练初步的视觉表示学习系统。由于语言模型缺乏以像素形式获取或输出视觉信息的能力，我们在研究中使用代码来表示图像。虽然LLMs生成的图像看起来不像自然图像，但在图像生成和模型纠正这些生成图像的能力方面的结果表明，精确建模字符串可以教会语言模型关于视觉世界的许多方面。此外，利用与文本模型生成的图像进行自我监督的视觉表示学习实验证明了仅使用LLMs可以训练能够对自然图像进行语义评估的视觉模型的潜力。

    What does learning to model relationships between strings teach large language models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.
    
[^7]: 未知上下文分布下上下文赌博最优交叉学习

    Optimal cross-learning for contextual bandits with unknown context distributions. (arXiv:2401.01857v1 [cs.LG])

    [http://arxiv.org/abs/2401.01857](http://arxiv.org/abs/2401.01857)

    本文提出了一种针对“交叉学习”设置的上下文赌博算法，在未知上下文分布的情况下，通过协调多个时期内的学习算法的执行，实现了几乎紧致的遗憾界。此算法解决了学习在一价拍卖和睡觉赌徒问题中的应用难题。

    

    我们考虑在Balseiro等人的“交叉学习”设置中设计上下文赌博算法的问题，其中学习器观察到在所有可能的上下文中他们所选择的动作的损失，而不仅仅是当前回合的上下文。我们特别考虑在损失被对抗性地选择和上下文从未知分布独立同分布采样的情况下。在这个设置中，我们通过提供一个效率算法解决了Balseiro等人的一个开放问题，其遗憾界几乎达到紧致水平（对数因子），为$\widetilde{O}(\sqrt{TK})$，不依赖于上下文的数量。因此，我们首次获得了在未知值分布的一价拍卖中学习出几乎紧致的遗憾界和具有随机动作集的睡觉赌徒问题。

    We consider the problem of designing contextual bandit algorithms in the ``cross-learning'' setting of Balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round. We specifically consider the setting where losses are chosen adversarially and contexts are sampled i.i.d. from an unknown distribution. In this setting, we resolve an open problem of Balseiro et al. by providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of $\widetilde{O}(\sqrt{TK})$, independent of the number of contexts. As a consequence, we obtain the first nearly tight regret bounds for the problems of learning to bid in first-price auctions (under unknown value distributions) and sleeping bandits with a stochastic action set.  At the core of our algorithm is a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between
    
[^8]: 变压器神经自回归流

    Transformer Neural Autoregressive Flows. (arXiv:2401.01855v1 [cs.LG])

    [http://arxiv.org/abs/2401.01855](http://arxiv.org/abs/2401.01855)

    本文提出了一种称为变压器神经自回归流（T-NAFs）的新技术，通过利用变压器来解决神经自回归流的可扩展性和训练稳定性问题，并取得了实验验证。

    

    密度估计是机器学习中的一个核心问题，可以使用归一化流（NFs）来执行。NFs由一系列可逆变换组成，利用变量变换定理将复杂的目标分布转化为简单的分布。神经自回归流（NAFs）和块神经自回归流（B-NAFs）可以说是NF家族中性能最好的成员。然而，由于对网络结构施加的约束，它们存在可扩展性问题和训练不稳定性。在本文中，我们提出了一种新的解决方案，通过利用变压器来定义一种新型的神经流，称为变压器神经自回归流（T-NAFs）。T-NAFs将随机变量的每个维度视为单独的输入令牌，利用注意力掩码来强制执行自回归约束。我们采用一种类似于分期偿还的方法，变压器输出可逆变换的参数。实验证明.

    Density estimation, a central problem in machine learning, can be performed using Normalizing Flows (NFs). NFs comprise a sequence of invertible transformations, that turn a complex target distribution into a simple one, by exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs) and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant members of the NF family. However, they suffer scalability issues and training instability due to the constraints imposed on the network structure. In this paper, we propose a novel solution to these challenges by exploiting transformers to define a new class of neural flows called Transformer Neural Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable as a separate input token, using attention masking to enforce an autoregressive constraint. We take an amortization-inspired approach where the transformer outputs the parameters of an invertible transformation. The experimental results
    
[^9]: 多语言指令调优中的多语言性

    Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])

    [http://arxiv.org/abs/2401.01854](http://arxiv.org/abs/2401.01854)

    本研究研究了多语言指令调优中的多语言性对跨语言指令遵循的影响。研究发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，只有40个多语言示例能够显著提高多语言指令遵循。总体来说，多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    

    随着大型语言模型（LLMs）的全球采纳，它们在多语言指令遵循能力变得越来越重要。一种有前途的方法是跨语言转移，通过在另一种语言上微调，模型可以在某种语言上获得特定的功能。本文研究了多语言LLM在指令调优过程中的多语言性对跨语言指令遵循的影响。首先我们发现，即使在单语调优过程中，许多语言也可以将一些指令遵循能力转移到其他语言上。此外，我们发现在英语调优集合中，只有40个多语言示例能够显著提高多语言指令遵循，在调优过程中不论是已见语言还是未见语言。总的来说，我们观察到在多语言混合调优的模型在多种语言上的表现相比单语调优的模型要好或者不相上下，尽管使用的这些语言的训练示例数量只有10倍少。

    As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
    
[^10]: 训练的力量：不同的神经网络设置对能源需求的影响

    The Power of Training: How Different Neural Network Setups Influence the Energy Demand. (arXiv:2401.01851v1 [cs.LG])

    [http://arxiv.org/abs/2401.01851](http://arxiv.org/abs/2401.01851)

    本文研究了机器学习训练方案和学习范式对能源消耗的影响，并探讨了预训练和多任务训练在可持续机器学习方面的潜力。

    

    本研究探讨机器学习训练方案和学习范式的变化对相应能源消耗的影响。虽然数据的可用性提高和高性能硬件的创新推动了复杂模型的训练，但也支持了能源消耗和碳排放的消隐。因此，本研究的目标是增加人们对一般训练参数和过程（从学习率到批量大小再到知识传输）的能源影响的认识。使用不同的超参数初始化在两种不同的硬件配置上评估多种设置，以获得有意义的结果。在基准结果上进行了预训练和多任务训练实验，以确定它们对可持续机器学习的潜力。

    This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.
    
[^11]: DGDNN：解耦图传播神经网络用于股票走势预测

    DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction. (arXiv:2401.01846v1 [cs.LG])

    [http://arxiv.org/abs/2401.01846](http://arxiv.org/abs/2401.01846)

    DGDNN是一种解耦的图传播神经网络用于股票走势预测，能够自动构建动态股票图并学习股票之间的任务最优依赖关系。

    

    预测未来股票趋势对于学术界和工业界来说仍然具有挑战性，因为随机的股票间动态和层次性股票内部动态影响股票价格。近年来，图神经网络通过将多个股票构建为图结构的数据，在此问题上取得了显著的性能。然而，大多数这些方法依赖于人工定义的因素来构建静态股票图，无法捕捉迅速演变的股票间内在相互依赖关系。此外，这些方法通常忽视了股票的层次特征，并丢失了其中的特殊信息。在这项工作中，我们提出了一种新颖的图学习方法，无需专家知识来解决这些问题。首先，我们的方法从信号处理的角度自动构建动态股票图，通过基于熵的边生成。然后，我们通过广义图传播进一步学习股票之间的任务最优依赖关系。

    Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices. In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data. However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve. In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within. In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues. First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective. Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion
    
[^12]: 带有流形正则化的Wasserstein非负张量分解

    Wasserstein Nonnegative Tensor Factorization with Manifold Regularization. (arXiv:2401.01842v1 [cs.LG])

    [http://arxiv.org/abs/2401.01842](http://arxiv.org/abs/2401.01842)

    这个论文介绍了一种带有流形正则化的Wasserstein非负张量分解方法，通过最小化输入张量数据和重构数据之间的Wasserstein距离，利用特征的相关信息和样本的流形信息来进行特征提取和部分表示。

    

    非负张量分解(NTF)已成为从非负高阶数据中提取特征和基于部分表示的重要工具，同时保留了内在结构信息。然而，原始的NTF方法采用欧几里得或Kullback-Leibler散度作为损失函数，将每个特征都视为相等，导致忽视了特征的旁边信息。为了利用特征的相关信息和样本的流形信息，我们引入了Wasserstein流形非负张量分解(WMNTF)，其将输入张量数据的分布和重构的分布之间的Wasserstein距离最小化。尽管在非负矩阵分解(NMF)中已经有一些关于Wasserstein距离的研究，但它们忽视了高阶数据的空间结构信息。我们使用Wasserstein距离(也称为地球移动距离或最优传输距离)作为度量，并添加了一个图正则化项。

    Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data. However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features. To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction. Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data. We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer
    
[^13]: 按照你的学习行动：非稳态马尔可夫决策过程中的自适应决策

    Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes. (arXiv:2401.01841v1 [cs.AI])

    [http://arxiv.org/abs/2401.01841](http://arxiv.org/abs/2401.01841)

    本文提出了一种自适应蒙特卡洛树搜索算法来应对非稳态环境下的决策问题，解决了传统方法中对环境动态假设的限制和规划过程的悲观性问题。

    

    在顺序决策中，处理非稳态环境是一个基本（且在很大程度上是未解决的）挑战，其中外部环境条件随时间变化。这类问题通常被建模为非稳态马尔可夫决策过程（NSMDP）。然而，现有的NSMDP决策方法存在两个主要缺点：首先，它们假设当前时刻更新的环境动态是已知的（尽管未来动态可能会改变）；其次，规划过程主要是悲观的，即代理人会“安全行动”以考虑环境的非稳态演变。我们认为这两个假设在实践中是无效的-更新的环境条件很少是已知的，并且当代理人与环境交互时，它可以学习更新的动态并避免悲观，至少在其对动态有信心的状态下。我们提出了一种启发式搜索算法，称为自适应蒙特卡洛树搜索。

    A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \textit{Adaptive Monte Carlo Tree Se
    
[^14]: 迭代掩码填充：一种使用掩码语言建模的有效文本增强方法

    Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling. (arXiv:2401.01830v1 [cs.CL])

    [http://arxiv.org/abs/2401.01830](http://arxiv.org/abs/2401.01830)

    本文提出了一种新颖的文本增强方法，利用基于Transformer的BERT模型的填充-掩码功能，通过迭代掩码和语言模型预测进行替换，显著提高了自然语言处理任务的性能。

    

    数据增强是提高机器学习模型性能的有效技术，但在自然语言处理领域，它的应用远不及计算机视觉领域广泛。本文提出了一种新颖的文本增强方法，利用基于Transformer的BERT模型的填充-掩码功能。我们的方法涉及对句子中的单词进行迭代掩码，并用语言模型预测进行替换。我们在各种自然语言处理任务上测试了我们的方法，并发现它在许多情况下都很有效。我们的实验结果与现有的增强方法进行了比较。实验结果表明，我们的方法在主题分类数据集上显著提高了性能。

    Data augmentation is an effective technique for improving the performance of machine learning models. However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision. In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model. Our method involves iteratively masking words in a sentence and replacing them with language model predictions. We have tested our proposed method on various NLP tasks and found it to be effective in many cases. Our results are presented along with a comparison to existing augmentation methods. Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets.
    
[^15]: 视网膜中的信号处理：可解释的图分类器预测神经节细胞的响应

    Signal Processing in the Retina: Interpretable Graph Classifier to Predict Ganglion Cell Responses. (arXiv:2401.01813v1 [cs.LG])

    [http://arxiv.org/abs/2401.01813](http://arxiv.org/abs/2401.01813)

    本文提出了一种可解释的基于图的分类器，用于预测神经节细胞对视觉刺激的放电。通过学习一个正半定度量矩阵来定义图节点之间的马氏距离，从而实现了对细胞潜在操作的理解。

    

    在神经科学中，有一个普遍的假设认为视网膜中的神经节细胞通过选择性地检测观察场景中的视觉特征来被激活。虽然可以通过数据训练的深度神经网络预测神经节细胞的放电，但网络仍然是无法解释的，因此对细胞潜在操作的理解十分有限。为了从细胞放电中提取知识，在本文中我们从数据中学习了一个可解释的基于图的分类器，用于预测神经节细胞对视觉刺激的放电。具体而言，我们学习了一个正半定（PSD）度量矩阵$\mathbf{M} \succeq 0$，该矩阵定义了具有预计算特征向量的图节点（视觉事件）之间的马氏距离；计算的节点间距离导致边权重和可进行二进制分类的组合图。数学上，我们使用大边界最近邻的图适应形式来定义度量矩阵$\mathbf{M}$优化的目标。

    It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene. While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations. To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli. Specifically, we learn a positive semi-definite (PSD) metric matrix $\mathbf{M} \succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification. Mathematically, we define the objective of metric matrix $\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbo
    
[^16]: 一个量子启发的用于几何建模的神经网络

    A quatum inspired neural network for geometric modeling. (arXiv:2401.01801v1 [cs.LG])

    [http://arxiv.org/abs/2401.01801](http://arxiv.org/abs/2401.01801)

    这个论文介绍了一种创新的矩阵乘积态(MPS)的消息传递策略，通过这种策略可以更好地捕捉几何图中的复杂关系。

    

    通过将物理系统构想为3D多体点云，几何图神经网络(GNN)，如SE(3)/E(3)等效GNN，展示了良好的性能。特别是，它们高效的消息传递机制使它们能够熟练地对分子和晶体材料进行建模。然而，当前的几何GNN只提供了多体系统的平均场近似，封装在两体消息传递中，因此在捕捉这些几何图中的复杂关系方面有所欠缺。为了解决这个局限性，计算物理学中广泛使用的高阶张量来处理多体系统的张量网络被引入。然而，将这些张量化网络整合到GNN的消息传递框架中面临着可扩展性和对称性保持（如置换和旋转）的挑战。作为回应，我们引入了一种创新的等变矩阵乘积态(MPS)的消息传递策略，通过实现一个

    By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving a
    
[^17]: CoMoSVC: 基于一致性模型的唱歌声音转换

    CoMoSVC: Consistency Model-based Singing Voice Conversion. (arXiv:2401.01792v1 [eess.AS])

    [http://arxiv.org/abs/2401.01792](http://arxiv.org/abs/2401.01792)

    本文提出了一种基于一致性模型的唱歌声音转换方法。通过设计扩散教师模型和提取自一致性学生模型，该方法实现了高质量的声音生成和高速的采样，相比于最先进的扩散方法，具有更快的推理速度，并在主观和客观指标上实现了可比或更优的声音转换性能。

    

    基于扩散的唱歌声音转换（SVC）方法已经取得了显著的性能，产生了与目标音色相似度高的自然音频。然而，迭代采样过程导致推理速度慢，因此加速变得至关重要。在本文中，我们提出了基于一致性模型的CoMoSVC方法，旨在实现高质量生成和高速采样。首先，设计了针对SVC的基于扩散的教师模型，然后通过自一致性特性进一步提取学生模型，实现一步采样。在单个NVIDIA GTX4090 GPU上进行的实验证明，虽然CoMoSVC的推理速度比最先进的（SOTA）基于扩散的SVC系统要快得多，但在主观和客观指标上仍实现了可比或更优的转换性能。音频样本和代码可在网址https://comosvc.github.io/获取。

    The diffusion-based Singing Voice Conversion (SVC) methods have achieved remarkable performances, producing natural audios with high similarity to the target timbre. However, the iterative sampling process results in slow inference speed, and acceleration thus becomes crucial. In this paper, we propose CoMoSVC, a consistency model-based SVC method, which aims to achieve both high-quality generation and high-speed sampling. A diffusion-based teacher model is first specially designed for SVC, and a student model is further distilled under self-consistency properties to achieve one-step sampling. Experiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a significantly faster inference speed than the state-of-the-art (SOTA) diffusion-based SVC system, it still achieves comparable or superior conversion performance based on both subjective and objective metrics. Audio samples and codes are available at https://comosvc.github.io/.
    
[^18]: 深度学习线性分数过程的Hurst参数及其可靠性评估

    Deep learning the Hurst parameter of linear fractional processes and assessing its reliability. (arXiv:2401.01789v1 [stat.ML])

    [http://arxiv.org/abs/2401.01789](http://arxiv.org/abs/2401.01789)

    本研究利用深度学习（特别是LSTM网络）评估了分数随机过程中的Hurst参数，并探讨了其可靠性。实证结果表明，在分数布朗运动和分数奥恩斯坦-乌伦贝克过程中，LSTM网络优于传统统计方法，但在线性分数稳定运动过程中准确性受限。

    

    本研究探讨了深度学习（特别是长短时记忆网络）在估计分数随机过程中的Hurst参数的可靠性。研究集中在三种类型的过程上：分数布朗运动（fBm），分数奥恩斯坦-乌伦贝克（fOU）过程和线性分数稳定运动（lfsm）。研究包括对fBm和fOU的大规模数据集的快速生成，以便在合理的时间内训练LSTM网络。研究分析了LSTM网络在Hurst参数估计方面的准确性，包括均方根误差（RMSE），平均绝对误差（MAE），相对误差的分位数等各种性能指标。研究发现，在fBm和fOU过程的情况下，LSTM优于传统的统计方法；然而，在lfsm过程上的准确性有限。研究还深入探讨了训练长度和评估序列长度对LSTM性能的影响。该方法被应用于...

    This research explores the reliability of deep learning, specifically Long Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in fractional stochastic processes. The study focuses on three types of processes: fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process, and linear fractional stable motions (lfsm). The work involves a fast generation of extensive datasets for fBm and fOU to train the LSTM network on a large volume of data in a feasible time. The study analyses the accuracy of the LSTM network's Hurst parameter estimation regarding various performance measures like RMSE, MAE, MRE, and quantiles of the absolute and relative errors. It finds that LSTM outperforms the traditional statistical methods in the case of fBm and fOU processes; however, it has limited accuracy on lfsm processes. The research also delves into the implications of training length and valuation sequence length on the LSTM's performance. The methodology is applied by 
    
[^19]: 机器学习和物联网在室外空气污染监测和预测中的应用：一个系统文献综述

    Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review. (arXiv:2401.01788v1 [cs.LG])

    [http://arxiv.org/abs/2401.01788](http://arxiv.org/abs/2401.01788)

    本文系统综述了机器学习和物联网在室外空气污染预测中的应用，以及使用的监测传感器和输入特征的组合。研究结果突出了高成本监测、低成本物联网和混合预测这三种方法。

    

    根据世界卫生组织（WHO）的数据，空气污染每年致使七百万人死亡。室外空气污染是影响低收入、中收入和高收入国家的主要环境健康问题。在过去几年中，研究界已经探索了物联网和机器学习应用于室外空气污染预测的可能性。本文的总体目标是系统地综述机器学习和物联网在室外空气污染预测中的应用以及使用的监测传感器和输入特征的组合。本文为此综述制定了两个研究问题。在初始PRISMA阶段共收集了1086篇文献。通过筛选和确定资格的阶段，最终选择了37篇文章进行分析。对结果进行了基于成本的分析，突出了高成本监测、低成本物联网和混合预测。研究确定了三种预测方法：时间序列、基于特征和空间预测。

    According to the World Health Organization (WHO), air pollution kills seven million people every year. Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries. In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction. The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used. Two research questions were formulated for this review. 1086 publications were collected in the initial PRISMA stage. After the screening and eligibility phases, 37 papers were selected for inclusion. A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction. Three methods of prediction were identified: time series, feature-based and spatio-t
    
[^20]: 使用傅里叶神经算子逼近数值通量的超波古典守恒定律

    Approximating Numerical Flux by Fourier Neural Operators for the Hyperbolic Conservation Laws. (arXiv:2401.01783v1 [math.NA])

    [http://arxiv.org/abs/2401.01783](http://arxiv.org/abs/2401.01783)

    该研究通过在传统的数值方案中运用神经网络来替换数值通量，解决了神经网络方法缺乏鲁棒性和泛化能力的问题，并展示了该方法在超声古典守恒定律方面具有优势。

    

    传统的数值方案用于数值解PDE，最近发展了基于神经网络的方法。然而，使用神经网络的方法，如PINN和神经算子，缺乏鲁棒性和泛化能力。为了弥补这些缺点，有很多种类型的研究将传统的数值方案和机器学习方法结合起来，通过用神经网络替代数值方案中的一小部分来实现。在本文中，我们专注于超声古典守恒定律，将数值方案中的数值通量替换为神经算子。为此，我们构造了受数值方案启发的损失函数，并通过FNO逼近数值通量。通过实验证明，我们的方法通过与原始方法的比较具有数值方案和FNO的优势。例如，我们演示了我们的方法具有鲁棒性，分辨率不变性和数据驱动方法的可行性。

    Classical numerical schemes exist for solving PDEs numerically, and recently, neural network-based methods have been developed. However, methodologies using neural networks, such as PINN and neural operators, lack robustness and generalization power. To compensate for such drawbacks, there are many types of research combining classical numerical schemes and machine learning methods by replacing a small portion of the numerical schemes with neural networks. In this work, we focus on hyperbolic conservation laws and replace numerical fluxes in the numerical schemes by neural operator. For this, we construct losses that are motivated by numerical schemes for conservation laws and approximate numerical flux by FNO. Through experiments, we show that our methodology has advantages of both numerical schemes and FNO by comparing with original methods. For instance, we demonstrate our method gains robustness, resolution invariance property, and feasibility of a data-driven method. Our method es
    
[^21]: 理解数据增强的不利类级影响

    Understanding the Detrimental Class-level Effects of Data Augmentation. (arXiv:2401.01764v1 [cs.CV])

    [http://arxiv.org/abs/2401.01764](http://arxiv.org/abs/2401.01764)

    理解了数据增强对类别级学习的影响，发现在图像分类任务中，数据增强在提高平均准确性的同时，会显著降低个别类别的准确性。这些影响主要出现在模糊、共现或细粒度区别的类别上，数据增强受控于模型对其中一个相关类别的偏好。

    

    数据增强（DA）对于图像分类任务中模型性能的提升至关重要，它编码了不变性并提供了隐式正则化。然而，尽管DA提高了平均准确性，最近的研究表明其影响可能高度依赖于类别：达到最佳平均准确度的同时，会导致ImageNet上单个类别准确度显著下降高达20％。由于对这些影响的理解有限，对解决类别级准确度下降的进展很少。在这项工作中，我们提出了一个框架，用于理解DA与类别级学习动态之间的相互作用。利用更高质量的ImageNet多标签注释，我们系统地对受影响的类别进行分类，并发现大多数类别本质上是模糊的、共现的或涉及细粒度的区别，而DA控制了模型对其中一个相关的类别的偏好。许多先前报告的性能下降通过多标签注释可以解释。

    Data augmentation (DA) encodes invariance and provides implicit regularization critical to a model's performance in image classification tasks. However, while DA improves average accuracy, recent studies have shown that its impact can be highly class dependent: achieving optimal average accuracy comes at the cost of significantly hurting individual class accuracy by as much as 20% on ImageNet. There has been little progress in resolving class-level accuracy drops due to a limited understanding of these effects. In this work, we present a framework for understanding how DA interacts with class-level learning dynamics. Using higher-quality multi-label annotations on ImageNet, we systematically categorize the affected classes and find that the majority are inherently ambiguous, co-occur, or involve fine-grained distinctions, while DA controls the model's bias towards one of the closely related classes. While many of the previously reported performance drops are explained by multi-label an
    
[^22]: 研究概念漂移检测在水配水网络泄漏检测中的适用性

    Investigating the Suitability of Concept Drift Detection for Detecting Leakages in Water Distribution Networks. (arXiv:2401.01733v1 [cs.LG])

    [http://arxiv.org/abs/2401.01733](http://arxiv.org/abs/2401.01733)

    本研究探索了概念漂移检测方法在水配水网络泄漏检测中的应用潜力，并提出了一种基于漂移检测的技术来定位泄漏。

    

    泄漏是水配水网络中的重大风险，因为它们导致水资源的损失并增加了污染风险。泄漏检测是一项困难的任务，由于水配水网络的复杂动力学。特别是，很难检测到小泄漏。从机器学习的角度来看，泄漏可以被建模为概念漂移。因此，一种广泛的漂移检测方案似乎是检测泄漏的合适选择。在这项工作中，我们探讨了基于模型损失和基于分布的漂移检测方法在处理泄漏检测方面的潜力。我们还讨论了数据中的时序依赖问题，并提出了一种处理这个问题的方法，当应用基于分布的检测时。我们系统地评估了不同方法对不同大小和检测时间的泄漏。此外，我们提出了一种基于漂移检测的技术来定位泄漏。

    Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks. Leakage detection is a difficult task due to the complex dynamics of water distribution networks. In particular, small leakages are hard to detect. From a machine-learning perspective, leakages can be modeled as concept drift. Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages. In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection. We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection. We evaluate different methods systematically for leakages of different sizes and detection times. Additionally, we propose a first drift-detection-based technique for localizing leakages.
    
[^23]: 任务和解释网络

    Task and Explanation Network. (arXiv:2401.01732v1 [cs.LG])

    [http://arxiv.org/abs/2401.01732](http://arxiv.org/abs/2401.01732)

    该论文介绍了一个基本框架——任务和解释网络（TENet），它不仅能完成任务，还能解释为什么这样完成任务。它强调整个AI领域都应该重视可解释性。

    

    在最近几年中，深度网络的可解释性变得越来越重要。我们在此论文中提出，AI不仅要完成任务，还要解释为什么这样完成任务。我们提出了一个基本框架——任务和解释网络（TENet），它完全整合了任务完成和解释。我们认为整个AI领域都应该强调可解释性。

    Explainability in deep networks has gained increased importance in recent years. We argue herein that an AI must be tasked not just with a task but also with an explanation of why said task was accomplished as such. We present a basic framework -- Task and Explanation Network (TENet) -- which fully integrates task completion and its explanation. We believe that the field of AI as a whole should insist -- quite emphatically -- on explainability.
    
[^24]: Ravnest: 异构设备上的分散式异步训练

    Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices. (arXiv:2401.01728v1 [cs.LG])

    [http://arxiv.org/abs/2401.01728](http://arxiv.org/abs/2401.01728)

    本文提出了一种用于大型现代深度学习模型的异步分散式训练范式，通过连接互联网上的资源受限的异构个人计算机，利用计算能力来实现有利的性能指标。Ravnest通过有效地将计算节点组织成具有类似数据传输速率和计算能力的集群，实现了分散式训练，而不需要每个节点承载整个模型。

    

    现代的深度学习模型越来越大、越来越复杂，通过在大型数据集上进行训练，已经展示出了异常的泛化和准确性。这一趋势预计将继续。然而，这些模型的增大使得传统的集中式方法在这种规模上受到内存限制的挑战。本文提出了一种用于大型现代深度学习模型的异步分散式训练范式，利用了连接在互联网上的资源受限的异构个人计算机的计算能力，以实现有利的性能指标。Ravnest通过有效地将计算节点组织成具有类似数据传输速率和计算能力的集群来实现分散式训练，而不需要每个节点承载整个模型。这些集群参与$\textit{零气泡异步模型并行}$训练，利用$\textit{并行多环全局汇聚}$方法可以有效地进行通信和模型聚合。

    Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively e
    
[^25]: EPA：受神经崩溃启发的稳健的出域检测器

    EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector. (arXiv:2401.01710v1 [cs.LG])

    [http://arxiv.org/abs/2401.01710](http://arxiv.org/abs/2401.01710)

    本论文提出了一种名为EPA的出域检测器，其受到神经崩溃的启发。通过观察特征与内域特征子空间之间的主角度，EPA能够更有效地衡量出域样本的可能性，并在实验证明其在性能和稳健性方面超过了其他方法。

    

    出域（OOD）检测在确保神经网络安全性方面起着关键作用。现有的研究利用了内域（ID）样本在特征空间中形成一个子空间的事实，并取得了最先进的性能。然而，内域子空间的综合特征仍然未被充分探索。最近，神经崩溃（NC）的发现揭示了内域子空间的新特性。利用NC的洞察力，我们观察到特征与内域特征子空间之间的主角度在衡量出域可能性方面形成了一种优越的表示。基于这一观察，我们提出了一种新颖的NC受启发的出域评分函数，名为增强熵主角度（EPA），它集成了内域子空间的全局特征和其内部属性。我们对EPA和各种最先进的方法进行了实验比较，验证了其卓越的性能和稳健性。

    Out-of-distribution (OOD) detection plays a crucial role in ensuring the security of neural networks. Existing works have leveraged the fact that In-distribution (ID) samples form a subspace in the feature space, achieving state-of-the-art (SOTA) performance. However, the comprehensive characteristics of the ID subspace still leave under-explored. Recently, the discovery of Neural Collapse ($\mathcal{NC}$) sheds light on novel properties of the ID subspace. Leveraging insight from $\mathcal{NC}$, we observe that the Principal Angle between the features and the ID feature subspace forms a superior representation for measuring the likelihood of OOD. Building upon this observation, we propose a novel $\mathcal{NC}$-inspired OOD scoring function, named Entropy-enhanced Principal Angle (EPA), which integrates both the global characteristic of the ID subspace and its inner property. We experimentally compare EPA with various SOTA approaches, validating its superior performance and robustness
    
[^26]: 无监督学习下的零样本主动学习

    Zero-shot Active Learning Using Self Supervised Learning. (arXiv:2401.01690v1 [cs.LG])

    [http://arxiv.org/abs/2401.01690](http://arxiv.org/abs/2401.01690)

    这项工作提出了一种新的无监督学习下的零样本主动学习方法，利用自监督学习得到的特征来选择最佳数据子集进行深度学习模型训练，从而提高模型在有限数据注释预算下的泛化性能。

    

    人们常说深度学习算法对数据的需求很大。这些算法的性能通常随着注释数据的增加而改善。虽然收集未标记数据更容易（因为它们可以很容易地从互联网上获取），但注释它们是一项费时费力的任务。在给定的数据注释预算下，主动学习有助于选择适合注释的数据子集，以使在此预算下对该子集进行训练的深度学习模型具有最大的泛化性能。在这项工作中，我们旨在提出一种新的主动学习方法，它既是模型无关的，也不需要迭代过程。我们的目标是利用自监督学习得到的特征来进行主动学习任务。自监督学习的好处是，可以在没有任何注释的情况下获取输入数据的有用特征表示。

    Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.
    
[^27]: LESEN:基于多参数MRI的视觉途径分割的标签高效深度学习

    LESEN: Label-Efficient deep learning for Multi-parametric MRI-based Visual Pathway Segmentation. (arXiv:2401.01654v1 [eess.IV])

    [http://arxiv.org/abs/2401.01654](http://arxiv.org/abs/2401.01654)

    LESEN是一种标签高效深度学习方法，通过自我集成和可靠的未标注样本选择机制，有效地解决了基于多参数MRI的视觉途径分割中有限标注样本的问题，并取得了优越的性能。

    

    最近的研究表明，深度学习在基于多参数MRI的视觉途径分割中具有潜力。然而，获取用于训练的标注数据是费时费力的。因此，在有限标注样本的情况下，开发有效的算法至关重要。在这项工作中，我们提出了一种自我集成的标签高效深度学习方法（LESEN）。LESEN结合了有监督和无监督的损失，使得学生模型和教师模型可以互相学习，形成自我集成的均值教师框架。另外，我们引入了可靠的未标注样本选择（RUSS）机制，进一步提高了LESEN的效果。我们在人类连接组计划（HCP）数据集上的实验证明了我们的方法相对于现有技术的卓越性能，推动了临床和研究环境中的多模态VP分割的综合分析。实现代码将在以下链接处提供。

    Recent research has shown the potential of deep learning in multi-parametric MRI-based visual pathway (VP) segmentation. However, obtaining labeled data for training is laborious and time-consuming. Therefore, it is crucial to develop effective algorithms in situations with limited labeled samples. In this work, we propose a label-efficient deep learning method with self-ensembling (LESEN). LESEN incorporates supervised and unsupervised losses, enabling the student and teacher models to mutually learn from each other, forming a self-ensembling mean teacher framework. Additionally, we introduce a reliable unlabeled sample selection (RUSS) mechanism to further enhance LESEN's effectiveness. Our experiments on the human connectome project (HCP) dataset demonstrate the superior performance of our method when compared to state-of-the-art techniques, advancing multimodal VP segmentation for comprehensive analysis in clinical and research settings. The implementation code will be available at
    
[^28]: 朝着基于交易序列的基础采购模型: 预训练的生成自回归

    Towards a Foundation Purchasing Model: Pretrained Generative Autoregression on Transaction Sequences. (arXiv:2401.01641v1 [cs.LG])

    [http://arxiv.org/abs/2401.01641](http://arxiv.org/abs/2401.01641)

    本文提出了一种基于生成预训练方法的金融交易上下文嵌入模型，该模型在公共数据集上的测试中表现优于最先进的自监督方法，并在卡片欺诈检测问题上取得了显著的性能提升。

    

    机器学习模型广泛应用于现代金融系统中，用于欺诈检测和留存预测等用例。大多数模型基于有标签数据的监督学习和手动设计的特征。目前，大规模自监督生成模型在自然语言处理和计算机视觉方面取得了巨大成功，但尚未将其应用于金融交易的多变量时间序列。本文提出了一种生成预训练方法，可用于获取金融交易的上下文嵌入。公共数据集上的基准测试表明，它在一系列下游任务中优于最先进的自监督方法。我们还使用包含51亿笔交易的180个发卡银行的语料库进行大规模预训练嵌入模型，并将其应用于保留数据集上的卡片欺诈检测问题。嵌入模型显著提高了性能。

    Machine learning models underpin many modern financial systems for use cases such as fraud detection and churn prediction. Most are based on supervised learning with hand-engineered features, which relies heavily on the availability of labelled data. Large self-supervised generative models have shown tremendous success in natural language processing and computer vision, yet so far they haven't been adapted to multivariate time series of financial transactions. In this paper, we present a generative pretraining method that can be used to obtain contextualised embeddings of financial transactions. Benchmarks on public datasets demonstrate that it outperforms state-of-the-art self-supervised methods on a range of downstream tasks. We additionally perform large-scale pretraining of an embedding model using a corpus of data from 180 issuing banks containing 5.1 billion transactions and apply it to the card fraud detection problem on hold-out datasets. The embedding model significantly impro
    
[^29]: 对于顺序数据中的自监督和监督模型进行公平性评估

    Evaluating Fairness in Self-supervised and Supervised Models for Sequential Data. (arXiv:2401.01640v1 [cs.LG])

    [http://arxiv.org/abs/2401.01640](http://arxiv.org/abs/2401.01640)

    本研究通过比较自监督学习模型和监督学习模型在顺序数据上的表现发现，自监督学习模型能够实现与监督模型相当的性能，并显著提高公平性，这表明了其在人为中心的计算中的潜力。

    

    自监督学习（SSL）已成为大型模型的实际训练范式，其中预训练后跟随领域特定数据和标签的监督微调。本研究假设SSL模型会学习到更通用、因此更少有偏见的表示，探索预训练和微调策略对公平性（即在不同人口统计学分析中表现平等）的影响。在真实世界的时间序列数据上，受到以人为中心的应用的启发，我们通过系统比较SSL模型和相应的监督模型，从模型、层次和度量水平上解释归纳偏差。我们的研究结果表明，SSL能够实现与监督方法相当的性能，同时显著提高公平性，通过自监督学习可以在性能损失1%的情况下增加公平性高达27%。最终，这项工作强调了SSL在人为中心的计算中的潜力，特别是在高风险的数据情境中。

    Self-supervised learning (SSL) has become the de facto training paradigm of large models where pre-training is followed by supervised fine-tuning using domain-specific data and labels. Hypothesizing that SSL models would learn more generic, hence less biased, representations, this study explores the impact of pre-training and fine-tuning strategies on fairness (i.e., performing equally on different demographic breakdowns). Motivated by human-centric applications on real-world timeseries data, we interpret inductive biases on the model, layer, and metric levels by systematically comparing SSL models to their supervised counterparts. Our findings demonstrate that SSL has the capacity to achieve performance on par with supervised methods while significantly enhancing fairness--exhibiting up to a 27% increase in fairness with a mere 1% loss in performance through self-supervision. Ultimately, this work underscores SSL's potential in human-centric computing, particularly high-stakes, data-s
    
[^30]: AI中的合成数据:挑战，应用和伦理影响

    Synthetic Data in AI: Challenges, Applications, and Ethical Implications. (arXiv:2401.01629v1 [cs.LG])

    [http://arxiv.org/abs/2401.01629](http://arxiv.org/abs/2401.01629)

    这篇论文讨论了人工智能中合成数据的挑战、应用和伦理影响。它介绍了合成数据的生成方法和应用领域，并强调了确保公平性、减少偏见和维护伦理标准在人工智能发展中的重要性。

    

    在快速发展的人工智能领域中，合成数据的创造和利用变得越来越重要。本报告深入探讨了合成数据的多方面问题，特别强调这些数据集可能存在的挑战和潜在偏见。它探讨了合成数据生成的方法，包括传统的统计模型和先进的深度学习技术，并研究了它们在各个领域的应用。本报告还批判性地讨论了与合成数据集相关的伦理考虑和法律影响，强调了确保公平性，减少偏见和维护人工智能发展的伦理标准的机制的紧迫性。

    In the rapidly evolving field of artificial intelligence, the creation and utilization of synthetic datasets have become increasingly significant. This report delves into the multifaceted aspects of synthetic data, particularly emphasizing the challenges and potential biases these datasets may harbor. It explores the methodologies behind synthetic data generation, spanning traditional statistical models to advanced deep learning techniques, and examines their applications across diverse domains. The report also critically addresses the ethical considerations and legal implications associated with synthetic datasets, highlighting the urgent need for mechanisms to ensure fairness, mitigate biases, and uphold ethical standards in AI development.
    
[^31]: 关于图神经网络的表达能力研究

    On the Expressive Power of Graph Neural Networks. (arXiv:2401.01626v1 [cs.LG])

    [http://arxiv.org/abs/2401.01626](http://arxiv.org/abs/2401.01626)

    研究人员对图神经网络的表达能力和设计架构进行了大量工作，以提高其在各领域任务中的性能。主要方法包括研究GNN的通用逼近性质和其在区分不同图之间的能力程度。

    

    过去几年来，图神经网络的研究引起了相当大的兴趣。通过将深度学习扩展到图结构化数据，GNN可以解决社会科学、化学和医学等领域的各种任务。GNN架构的发展主要集中在改进节点或图分类等任务的实证性性能。然而，最近的一系列工作则寻求找到具有理论特性的GNN架构，通过研究其表达能力并设计最大化这种表达能力的架构。虽然关于如何定义GNN的表达能力还没有共识，但可以从几个有很好动机的角度来看待。也许最自然的方法是研究GNN的通用逼近性质，就像MLP的这种性质一样得到了广泛的研究。另一个方向关注的是GNN在区分不同图之间的能力程度。

    The study of Graph Neural Networks has received considerable interest in the past few years. By extending deep learning to graph-structured data, GNNs can solve a diverse set of tasks in fields including social science, chemistry, and medicine. The development of GNN architectures has largely been focused on improving empirical performance on tasks like node or graph classification. However, a line of recent work has instead sought to find GNN architectures that have desirable theoretical properties - by studying their expressive power and designing architectures that maximize this expressiveness.  While there is no consensus on the best way to define the expressiveness of a GNN, it can be viewed from several well-motivated perspectives. Perhaps the most natural approach is to study the universal approximation properties of GNNs, much in the way that this has been studied extensively for MLPs. Another direction focuses on the extent to which GNNs can distinguish between different graph
    
[^32]: SCALA: 基于稀疏化对比学习的属性网络异常检测

    SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks. (arXiv:2401.01625v1 [cs.SI])

    [http://arxiv.org/abs/2401.01625](http://arxiv.org/abs/2401.01625)

    SCALA是一种使用稀疏化的对比学习框架，用于在属性网络中进行异常检测。该框架旨在改善网络的嵌入质量，并通过引入稀疏化来为每个节点提供一种新的衡量异常得分的方法。

    

    属性网络上的异常检测旨在找到与其他大多数节点行为明显不同的节点。通常，网络数据包含实体间的关系信息，异常通常体现在这些关系中。因此，如何全面地建模网络中复杂的交互模式仍然是一个主要关注点。可以观察到，在网络中，异常违反了相似性假设。然而，大多数现有研究只是间接地考虑了这种现象，而不是显式地考虑。此外，正常实体的节点表示很容易受到异常节点引入的噪声关系的干扰。为了解决上述问题，我们提出了一种新颖的基于对比学习的属性网络异常检测框架SCALA，旨在改善网络的嵌入质量，并通过引入稀疏化来为每个节点提供一种新的衡量异常得分的方法。

    Anomaly detection on attributed networks aims to find the nodes whose behaviors are significantly different from other majority nodes. Generally, network data contains information about relationships between entities, and the anomaly is usually embodied in these relationships. Therefore, how to comprehensively model complex interaction patterns in networks is still a major focus. It can be observed that anomalies in networks violate the homophily assumption. However, most existing studies only considered this phenomenon obliquely rather than explicitly. Besides, the node representation of normal entities can be perturbed easily by the noise relationships introduced by anomalous nodes. To address the above issues, we present a novel contrastive learning framework for anomaly detection on attributed networks, \textbf{SCALA}, aiming to improve the embedding quality of the network and provide a new measurement of qualifying the anomaly score for each node by introducing sparsification into
    
[^33]: PLLaMa：一种用于植物科学的开源大型语言模型

    PLLaMa: An Open-source Large Language Model for Plant Science. (arXiv:2401.01600v1 [cs.CL])

    [http://arxiv.org/abs/2401.01600](http://arxiv.org/abs/2401.01600)

    PLLaMa是一种用于植物科学的开源大型语言模型，通过综合数据库增强，显著丰富了其在植物和农业科学方面的知识和专长，并通过与专业人员小组的合作验证了其准确性。

    

    大型语言模型（LLMs）在理解和与自然语言进行交互方面展示了出色的能力。然而，在植物科学等需要高准确性的专业领域中，由于缺乏相关领域的专业知识，它们的效果受到了限制。本文介绍了PLLaMa，一种从LLaMa-2进化而来的开源语言模型。它通过包括超过150万篇植物科学学术文章的综合数据库进行增强。这一发展显著丰富了PLLaMa在植物和农业科学方面的知识和专长。我们的初步测试中，涉及与植物和农业相关的特定数据集，显示PLLaMa显著提高了对植物科学相关主题的理解。此外，我们还组建了一个国际专业人员小组，包括植物科学家、农业工程师和植物育种员。该团队在验证PLLaMa的准确性方面起着关键作用。

    Large Language Models (LLMs) have exhibited remarkable capabilities in understanding and interacting with natural language across various sectors. However, their effectiveness is limited in specialized areas requiring high accuracy, such as plant science, due to a lack of specific expertise in these fields. This paper introduces PLLaMa, an open-source language model that evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising more than 1.5 million scholarly articles in plant science. This development significantly enriches PLLaMa with extensive knowledge and proficiency in plant and agricultural sciences. Our initial tests, involving specific datasets related to plants and agriculture, show that PLLaMa substantially improves its understanding of plant science-related topics. Moreover, we have formed an international panel of professionals, including plant scientists, agricultural engineers, and plant breeders. This team plays a crucial role in verifying the accura
    
[^34]: 分析谱算法在幂律衰减下的泛化误差曲线

    Generalization Error Curves for Analytic Spectral Algorithms under Power-law Decay. (arXiv:2401.01599v1 [cs.LG])

    [http://arxiv.org/abs/2401.01599](http://arxiv.org/abs/2401.01599)

    本文研究了核回归方法的泛化误差曲线，对核梯度下降方法和其他分析谱算法在核回归中的泛化误差进行了全面特征化，从而提高了对训练宽神经网络泛化行为的理解，并提出了一种新的技术贡献-分析功能论证。

    

    某些核回归方法的泛化误差曲线旨在确定在不同源条件、噪声水平和正则化参数选择下的泛化误差的确切顺序，而不是最小化率。在本文中，在温和的假设下，我们严格给出了核梯度下降方法（以及大类分析谱算法）在核回归中的泛化误差曲线的完整特征化。因此，我们可以提高核插值的近不一致性，并澄清具有更高资格的核回归算法的饱和效应，等等。由于神经切线核理论的帮助，这些结果极大地提高了我们对训练宽神经网络的泛化行为的理解。一种新颖的技术贡献，即分析功能论证，可能具有独立的兴趣。

    The generalization error curve of certain kernel regression method aims at determining the exact order of generalization error with various source condition, noise level and choice of the regularization parameter rather than the minimax rate. In this work, under mild assumptions, we rigorously provide a full characterization of the generalization error curves of the kernel gradient descent method (and a large class of analytic spectral algorithms) in kernel regression. Consequently, we could sharpen the near inconsistency of kernel interpolation and clarify the saturation effects of kernel regression algorithms with higher qualification, etc. Thanks to the neural tangent kernel theory, these results greatly improve our understanding of the generalization behavior of training the wide neural networks. A novel technical contribution, the analytic functional argument, might be of independent interest.
    
[^35]: 高维在线优化的不变信息几何方法

    An Invariant Information Geometric Method for High-Dimensional Online Optimization. (arXiv:2401.01579v1 [cs.LG])

    [http://arxiv.org/abs/2401.01579](http://arxiv.org/abs/2401.01579)

    本文介绍了一种基于不变性的演化策略算法，在贝叶斯能力上限的任务中与领先的贝叶斯优化方法竞争。我们构建了一个完全包含历史信息的框架，并在多维高斯分布上示例化了一种不变且可扩展的优化器。该算法相较于其他基于高斯的演化策略具有理论上的优势。

    

    在优化中，样本效率对于昂贵评估和零阶反馈的黑盒场景尤为重要。当计算资源充足时，贝叶斯优化通常优于演化策略。在本文中，我们介绍了一种完全基于不变性的演化策略算法，从其相应的框架中推导出来，在贝叶斯能力上限的任务中有效地与领先的贝叶斯优化方法竞争。具体而言，我们首先构建了一个完全包含历史信息的框架InvIGO，同时保持了完全不变和计算复杂性。然后，我们在多维高斯分布上示例化了InvIGO，得到了一种不变且可扩展的优化器SynCMA。进一步分析了我们的算法与其他基于高斯的演化策略的理论行为和优势。最后，我们将SynCMA与贝叶斯优化的主要算法进行了基准测试。

    Sample efficiency is crucial in optimization, particularly in black-box scenarios characterized by expensive evaluations and zeroth-order feedback. When computing resources are plentiful, Bayesian optimization is often favored over evolution strategies. In this paper, we introduce a full invariance oriented evolution strategies algorithm, derived from its corresponding framework, that effectively rivals the leading Bayesian optimization method in tasks with dimensions at the upper limit of Bayesian capability. Specifically, we first build the framework InvIGO that fully incorporates historical information while retaining the full invariant and computational complexity. We then exemplify InvIGO on multi-dimensional Gaussian, which gives an invariant and scalable optimizer SynCMA . The theoretical behavior and advantages of our algorithm over other Gaussian-based evolution strategies are further analyzed. Finally, We benchmark SynCMA against leading algorithms in Bayesian optimization an
    
[^36]: 通过符合性预测来模拟自解释神经网络的不确定性建模

    Towards Modeling Uncertainties of Self-explaining Neural Networks via Conformal Prediction. (arXiv:2401.01549v1 [cs.LG])

    [http://arxiv.org/abs/2401.01549](http://arxiv.org/abs/2401.01549)

    这篇论文介绍了一种通过符合性预测来模拟自解释神经网络的不确定性建模的方法。现有的自解释网络存在不足，无法提供关于同时生成的预测结果和相应解释的分布无关的不确定性量化，并且无法建立其之间的连接。

    

    尽管深度神经网络(DNN)取得了近年来的进展，但解释DNN的预测仍然具有挑战性。现有的DNN解释方法主要集中在事后解释，即使用另一个解释模型来提供解释。事后方法可能无法揭示DNN的实际原始推理过程，这引发了构建具有内置可解释性的DNN的需求。基于这一动机，提出了许多自解释神经网络，它们不仅能够生成准确的预测，还能够清晰直观地解释为什么做出特定决策。然而，现有的自解释网络在提供分布无关的不确定性量化方面存在局限性，同时生成的两个预测结果（即样本的最终预测和解释该预测的相应解释）之间也无法建立连接。

    Despite the recent progress in deep neural networks (DNNs), it remains challenging to explain the predictions made by DNNs. Existing explanation methods for DNNs mainly focus on post-hoc explanations where another explanatory model is employed to provide explanations. The fact that post-hoc methods can fail to reveal the actual original reasoning process of DNNs raises the need to build DNNs with built-in interpretability. Motivated by this, many self-explaining neural networks have been proposed to generate not only accurate predictions but also clear and intuitive insights into why a particular decision was made. However, existing self-explaining networks are limited in providing distribution-free uncertainty quantification for the two simultaneously generated prediction outcomes (i.e., a sample's final prediction and its corresponding explanations for interpreting that prediction). Importantly, they also fail to establish a connection between the confidence values assigned to the ge
    
[^37]: 欺骗的艺术：使用动态触发器的强健后门攻击

    The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of Triggers. (arXiv:2401.01537v1 [cs.CR])

    [http://arxiv.org/abs/2401.01537](http://arxiv.org/abs/2401.01537)

    这项研究介绍了一种使用动态触发器进行强健后门攻击的方法，通过巧妙设计的调整，使损坏的样本与干净的样本无法区分，实验证明这种方法可以成功地欺骗语音识别系统。

    

    由于人工智能行业的最新进展，机器学习作为服务（MLaaS）领域正在经历增长的实施。然而，这种增长引发了对AI防御机制的担忧，特别是对于来自不完全可信的第三方提供商的潜在隐蔽攻击。最近的研究发现，听觉后门可能使用某些修改作为其启动机制。DynamicTrigger作为一种方法被引入，用于进行使用巧妙设计的调整来确保损坏的样本与干净的样本无法区分的动态后门攻击。通过利用波动的信号采样率，并通过动态声音触发器（比如拍手声）对说话者身份进行掩盖，可以欺骗语音识别系统（ASR）。我们的实证测试表明，DynamicTrigger在隐蔽攻击中既有效又隐蔽，并在攻击过程中取得了令人印象深刻的成功率。

    The area of Machine Learning as a Service (MLaaS) is experiencing increased implementation due to recent advancements in the AI (Artificial Intelligence) industry. However, this spike has prompted concerns regarding AI defense mechanisms, specifically regarding potential covert attacks from third-party providers that cannot be entirely trusted. Recent research has uncovered that auditory backdoors may use certain modifications as their initiating mechanism. DynamicTrigger is introduced as a methodology for carrying out dynamic backdoor attacks that use cleverly designed tweaks to ensure that corrupted samples are indistinguishable from clean. By utilizing fluctuating signal sampling rates and masking speaker identities through dynamic sound triggers (such as the clapping of hands), it is possible to deceive speech recognition systems (ASR). Our empirical testing demonstrates that DynamicTrigger is both potent and stealthy, achieving impressive success rates during covert attacks while 
    
[^38]: 6G会成为语义通信吗？从面向任务和安全通信到集成感知的机遇与挑战

    Will 6G be Semantic Communications? Opportunities and Challenges from Task Oriented and Secure Communications to Integrated Sensing. (arXiv:2401.01531v1 [cs.NI])

    [http://arxiv.org/abs/2401.01531](http://arxiv.org/abs/2401.01531)

    本文研究了面向任务和语义通信的机遇和挑战，通过多任务学习的集成，采用深度神经网络，在发送端使用编码器和多个解码器来处理多种任务，并在多接收器环境下利用分散式学习解决通信负载和隐私关注。需要注意深度学习模型的稳健性。

    

    本文通过多任务学习的集成，探讨了下一代通信网络中面向任务和语义通信的机遇和挑战。该方法采用深度神经网络，在发送端使用专用编码器和接收端的多个任务特定解码器，共同训练以处理包括语义信息保留、源输入重建以及集成感知和通信等多种任务。为了将其应用从点对点链接扩展到多接收器环境，我们设想在各个接收器处部署解码器，通过分散式学习解决通信负载和隐私关注的挑战，利用联邦学习技术在分散式节点上分发模型更新。然而，该方法的有效性要取决于所采用的深度学习模型的稳健性。我们对潜在的漏洞进行了详细分析。

    This paper explores opportunities and challenges of task (goal)-oriented and semantic communications for next-generation (NextG) communication networks through the integration of multi-task learning. This approach employs deep neural networks representing a dedicated encoder at the transmitter and multiple task-specific decoders at the receiver, collectively trained to handle diverse tasks including semantic information preservation, source input reconstruction, and integrated sensing and communications. To extend the applicability from point-to-point links to multi-receiver settings, we envision the deployment of decoders at various receivers, where decentralized learning addresses the challenges of communication load and privacy concerns, leveraging federated learning techniques that distribute model updates across decentralized nodes. However, the efficacy of this approach is contingent on the robustness of the employed deep learning models. We scrutinize potential vulnerabilities s
    
[^39]: 在具有激励兼容性的多对一匹配市场中改进赌博算法

    Improved Bandits in Many-to-one Matching Markets with Incentive Compatibility. (arXiv:2401.01528v1 [cs.LG])

    [http://arxiv.org/abs/2401.01528](http://arxiv.org/abs/2401.01528)

    本文研究了在具有激励兼容性的多对一匹配市场中改进赌博算法的问题，并提出了适应性探索-延迟接受（AETDA）算法来提高遗憾上限。

    

    由于丰富的应用，双边匹配市场在文献中得到了广泛研究。由于参与者通常对自己的偏好不确定，最近采用在线算法通过迭代交互来学习偏好。然而，现有研究在多对一设置中的结果远非最优，并缺乏激励兼容性的保证。本文针对多对一市场在提高遗憾上限的同时确保激励兼容性的问题进行研究。我们首先提出了适应性探索-延迟接受（AETDA）算法，用于响应性设置。

    Two-sided matching markets have been widely studied in the literature due to their rich applications. Since participants are usually uncertain about their preferences, online algorithms have recently been adopted to learn them through iterative interactions. \citet{wang2022bandit} initiate the study of this problem in a many-to-one setting with \textit{responsiveness}. However, their results are far from optimal and lack guarantees of incentive compatibility. An extension of \citet{kong2023player} to this more general setting achieves a near-optimal bound for player-optimal regret. Nevertheless, due to the substantial requirement for collaboration, a single player's deviation could lead to a huge increase in its own cumulative rewards and an $O(T)$ regret for others. In this paper, we aim to enhance the regret bound in many-to-one markets while ensuring incentive compatibility. We first propose the adaptively explore-then-deferred-acceptance (AETDA) algorithm for responsiveness setting
    
[^40]: S$^{2}$-DMs：跳过步骤的扩散模型

    S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v1 [cs.CV])

    [http://arxiv.org/abs/2401.01520](http://arxiv.org/abs/2401.01520)

    本论文提出了一种新的训练方法S$^{2}$-DMs，通过创新的$L_{skip}$重新整合选择性采样阶段中省略的信息，显著提高了样本质量，并且实现简单，对代码修改要求少，与各种采样算法兼容。

    

    扩散模型已经成为强大的生成工具，样本质量与生成对抗网络（GANs）相当，并且反映了自回归模型的似然分数。其中一部分模型，如DDIMs，展示了固有的不对称性：它们在训练过程中使用$T$个步骤，但在生成过程中只从其中的子集进行采样。这种选择性采样方法虽然优化了速度，但无意中错过了未采样步骤中的重要信息，导致样本质量可能出现问题。为了解决这个问题，我们提出了S$^{2}$-DMs，一种新的训练方法，使用创新的$L_{skip}$，精心设计以重新整合在选择性采样阶段中省略的信息。这种方法的好处很多：它显著提高了样本质量，实现起来非常简单，需要最少的代码修改，并且足够灵活，可以与各种采样算法兼容。在CIFAR10数据集上，使用我们的模型训练的结果...

    Diffusion models have emerged as powerful generative tools, rivaling GANs in sample quality and mirroring the likelihood scores of autoregressive models. A subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry: they are trained over $T$ steps but only sample from a subset of $T$ during generation. This selective sampling approach, though optimized for speed, inadvertently misses out on vital information from the unsampled steps, leading to potential compromises in sample quality. To address this issue, we present the S$^{2}$-DMs, which is a new training method by using an innovative $L_{skip}$, meticulously designed to reintegrate the information omitted during the selective sampling phase. The benefits of this approach are manifold: it notably enhances sample quality, is exceptionally simple to implement, requires minimal code modifications, and is flexible enough to be compatible with various sampling algorithms. On the CIFAR10 dataset, models trained using our 
    
[^41]: 探索LLMs在心理学应用中的前沿：一份综述

    Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v1 [cs.LG])

    [http://arxiv.org/abs/2401.01519](http://arxiv.org/abs/2401.01519)

    本文综述了大型语言模型（LLMs）在心理学应用中的前沿，包括如何模拟人类认知和行为、提供创新工具进行文献回顾、假设生成、实验设计等。

    

    本文探索了大型语言模型（LLMs）在心理学应用中的前沿。心理学经历了几次理论变革，当前人工智能（AI）和机器学习，特别是LLMs的使用有望开启新的研究方向。我们详细探讨了LLMs如ChatGPT在心理学研究中的转变。文章讨论了LLMs在认知与行为心理学、临床与咨询心理学、教育与发展心理学以及社会与文化心理学等心理学分支中的影响，强调了它们模拟人类认知和行为方面的潜力。本文深入探讨了这些模型模拟人类文本生成的能力，为心理学中的文献回顾、假设生成、实验设计、实验对象、数据分析、学术写作和同行评审等提供创新工具。虽然LLMs在推动研究方法学方面起着重要作用，

    This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologie
    
[^42]: AIRI: 使用人工智能预测保留指数及其不确定性

    AIRI: Predicting Retention Indices and their Uncertainties using Artificial Intelligence. (arXiv:2401.01506v1 [cs.LG])

    [http://arxiv.org/abs/2401.01506](http://arxiv.org/abs/2401.01506)

    使用深度神经网络预测化合物保留指数，并量化其不确定性，以改善化学鉴定方法和库的质量。 (Predict the retention index of compounds using a deep neural network and quantify the uncertainty to enhance chemical identification methods and library quality.)

    

    Kovát's保留指数（RI）是使用气相色谱测量的化学结构鉴定中常用的指标。由于创建观察到的RI值的库是一项费时费力的任务，因此我们探索了使用深度神经网络从结构预测标准半极性柱的RI值。该网络生成的预测的平均绝对误差为15.1，在误差分布尾部的量化中，95%的绝对误差为46.5。由于人工智能保留指数（AIRI）网络的准确性，它被用于预测NIST EI-MS光谱库的RI值。这些RI值用于改进化学鉴定方法和提高库的质量。在使用预测模型时，估算不确定性是一项重要的实际需求。为了量化我们网络对每个单独预测的不确定性，我们使用了8个网络的输出来计算预测的标准

    The Kov\'ats Retention index (RI) is a quantity measured using gas chromatography and commonly used in the identification of chemical structures. Creating libraries of observed RI values is a laborious task, so we explore the use of a deep neural network for predicting RI values from structure for standard semipolar columns. This network generated predictions with a mean absolute error of 15.1 and, in a quantification of the tail of the error distribution, a 95th percentile absolute error of 46.5. Because of the Artificial Intelligence Retention Indices (AIRI) network's accuracy, it was used to predict RI values for the NIST EI-MS spectral libraries. These RI values are used to improve chemical identification methods and the quality of the library. Estimating uncertainty is an important practical need when using prediction models. To quantify the uncertainty of our network for each individual prediction, we used the outputs of an ensemble of 8 networks to calculate a predicted standard
    
[^43]: Pontryagin神经算子用于解决参数化普通和差分博弈

    Pontryagin Neural Operator for Solving Parametric General-Sum Differential Games. (arXiv:2401.01502v1 [cs.LG])

    [http://arxiv.org/abs/2401.01502](http://arxiv.org/abs/2401.01502)

    本文提出了一种Pontryagin模式的神经算子，通过在前向和反向共轭状态回滚之间的差异上定义的损失，解决了在具有参数化状态约束的博弈中值的不连续性的收敛问题，并在安全性能上优于现有的最先进算法。

    

    两个玩家的普通和差分博弈的值是Hamilton-Jacobi-Isaacs（HJI）方程的粘性解。这种博弈的值和策略逼近受到维度诅咒（CoD）的影响。通过物理信息神经网络（PINN）减轻CoD时，由于状态约束引起的值的不连续性，会遇到收敛问题。除了这些挑战之外，在对博弈参数空间进行学习时（例如，在信息不完整时进行博弈参数推断），通常需要学习可推广的值和策略。为了应对这些挑战，我们在本文中提出了一种Pontryagin模式的神经算子，在具有参数化状态约束的博弈中胜过现有的最先进算法（SOTA）。我们的关键贡献在于引入了一种在前向和反向共轭状态回滚之间的差异上定义的共轭状态损失，这种损失计算成本低廉。我们证明了共轭状态动力学的不连续性在该算子的性能改进中的重要性。

    The values of two-player general-sum differential games are viscosity solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy approximations for such games suffer from the curse of dimensionality (CoD). Alleviating CoD through physics-informed neural networks (PINN) encounters convergence issues when value discontinuity is present due to state constraints. On top of these challenges, it is often necessary to learn generalizable values and policies across a parametric space of games, e.g., for game parameter inference when information is incomplete. To address these challenges, we propose in this paper a Pontryagin-mode neural operator that outperforms existing state-of-the-art (SOTA) on safety performance across games with parametric state constraints. Our key contribution is the introduction of a costate loss defined on the discrepancy between forward and backward costate rollouts, which are computationally cheap. We show that the discontinuity of costate dynamics (in th
    
[^44]: 利用神经转导器进行两阶段文本到语音转换的语义标记预测

    Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction. (arXiv:2401.01498v1 [eess.AS])

    [http://arxiv.org/abs/2401.01498](http://arxiv.org/abs/2401.01498)

    利用神经转导器实现了一个两阶段的文本到语音转换框架，通过语义标记预测实现了稳健高效的对齐建模，并通过非自回归语音生成器合成语音波形。该框架在语音质量和说话者相似度方面超过了基线模型。

    

    我们提出了一个新颖的文本到语音（TTS）框架，它以神经转导器为核心。我们的方法将整个TTS流程划分为语义级别的序列到序列(seq2seq)建模和细粒度的声学建模阶段，利用从wav2vec2.0嵌入中获取的离散语义标记。为了实现稳健高效的对齐建模，我们使用了一个名为语义标记转导器的神经转导器来进行语义标记预测，从中获得了硬单调对齐约束的益处。随后，一个非自回归(NAR)语音生成器从这些语义标记有效地合成波形。另外，参考语音在每个阶段控制着时间动态和声学条件。这种解耦的框架减少了TTS的训练复杂性，同时允许每个阶段专注于语义和声学建模。我们在零-shot自适应TTS上的实验结果表明，我们的模型在语音质量和说话者相似度方面超越了基线模型。

    We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similari
    
[^45]: 自由午餐为联邦遥感目标细粒度分类提供了一个参数高效的框架

    Free Lunch for Federated Remote Sensing Target Fine-Grained Classification: A Parameter-Efficient Framework. (arXiv:2401.01493v1 [cs.LG])

    [http://arxiv.org/abs/2401.01493](http://arxiv.org/abs/2401.01493)

    这项研究提出了一个名为PRFL的隐私保护TFGC框架，基于联邦学习，旨在解决遥感目标细粒度分类中的数据隐私和通信效率问题。

    

    遥感目标的细粒度分类对军事和民用领域都具有重要意义。由于位置差异、数据规模增长和集中式服务器存储限制，这些数据通常存储在不同地区/国家的不同数据库中。然而，隐私法律和国家安全问题限制研究人员进一步分析这些敏感的遥感图像。此外，低资源的遥感设备在处理不断增长的数据和模型规模时，也面临通信开销和效率方面的挑战。为了解决上述挑战，本文提出了一种基于联邦学习的新型隐私保护TFGC框架，称为PRFL。所提出的框架允许每个客户端在统计异构（非独立同分布，IID）的环境中学习全局和局部知识，以增强私有数据的本地表示。

    Remote Sensing Target Fine-grained Classification (TFGC) is of great significance in both military and civilian fields. Due to location differences, growth in data size, and centralized server storage constraints, these data are usually stored under different databases across regions/countries. However, privacy laws and national security concerns constrain researchers from accessing these sensitive remote sensing images for further analysis. Additionally, low-resource remote sensing devices encounter challenges in terms of communication overhead and efficiency when dealing with the ever-increasing data and model scales. To solve the above challenges, this paper proposes a novel Privacy-Reserving TFGC Framework based on Federated Learning, dubbed PRFL. The proposed framework allows each client to learn global and local knowledge to enhance the local representation of private data in environments with extreme statistical heterogeneity (non. Independent and Identically Distributed, IID). 
    
[^46]: 自然语言处理与多模式股票价格预测

    Natural Language Processing and Multimodal Stock Price Prediction. (arXiv:2401.01487v1 [cs.LG])

    [http://arxiv.org/abs/2401.01487](http://arxiv.org/abs/2401.01487)

    本文使用自然语言处理和多模式股票价格预测，利用股票百分比变化作为训练数据，分析公开发布的新闻文章。结果展示了使用小型自然语言处理模型准确预测总体股票趋势的能力，以及特定数据特征和选择的有效性。

    

    在金融决策领域，预测股票价格是至关重要的。人工智能技术，如长短期记忆网络（LSTM）、支持向量机（SVM）和自然语言处理（NLP）模型常被用于预测股票价格。本文利用股票百分比变化作为训练数据，与传统使用原始货币值的方式相比，重点分析公开发布的新闻文章。百分比变化的选择旨在为模型提供关于价格波动的重要性及其对股票整体价格变化的影响的背景信息。该研究使用了专门的BERT自然语言处理模型来预测股票价格趋势，特别强调各种数据模态的重要性。结果展示了使用小型自然语言处理模型准确预测总体股票趋势的能力，并凸显了特定数据特征和选择的有效性。

    In the realm of financial decision-making, predicting stock prices is pivotal. Artificial intelligence techniques such as long short-term memory networks (LSTMs), support-vector machines (SVMs), and natural language processing (NLP) models are commonly employed to predict said prices. This paper utilizes stock percentage change as training data, in contrast to the traditional use of raw currency values, with a focus on analyzing publicly released news articles. The choice of percentage change aims to provide models with context regarding the significance of price fluctuations and overall price change impact on a given stock. The study employs specialized BERT natural language processing models to predict stock price trends, with a particular emphasis on various data modalities. The results showcase the capabilities of such strategies with a small natural language processing model to accurately predict overall stock trends, and highlight the effectiveness of certain data features and se
    
[^47]: 不确定性规范化的证据回归

    Uncertainty Regularized Evidential Regression. (arXiv:2401.01484v1 [cs.LG])

    [http://arxiv.org/abs/2401.01484](http://arxiv.org/abs/2401.01484)

    本文研究了证据回归网络（ERN）中的模型性能限制问题，并提出了一种基于正则化的改进方法，使ERN能够从整个训练集中学习。

    

    证据回归网络（ERN）是一种将深度学习与Dempster-Shafer理论相结合的新方法，用于预测目标并量化相关的不确定性。在底层理论的指导下，必须使用特定的激活函数来强制非负值，这种约束限制了模型从所有样本中学习的能力，从而危害了模型的性能。本文对这种限制进行了理论分析，并引入了一种改进方法来克服它。首先，我们定义了模型无法有效学习样本的区域。然后，我们对ERN进行了深入分析，研究了这个约束。基于我们分析的见解，通过引入一种新的正则化项，我们解决了这个限制，使ERN能够从整个训练集中学习。我们的广泛实验证实了我们的理论发现，并证明了所提出的解决方案的有效性。

    The Evidential Regression Network (ERN) represents a novel approach that integrates deep learning with Dempster-Shafer's theory to predict a target and quantify the associated uncertainty. Guided by the underlying theory, specific activation functions must be employed to enforce non-negative values, which is a constraint that compromises model performance by limiting its ability to learn from all samples. This paper provides a theoretical analysis of this limitation and introduces an improvement to overcome it. Initially, we define the region where the models can't effectively learn from the samples. Following this, we thoroughly analyze the ERN and investigate this constraint. Leveraging the insights from our analysis, we address the limitation by introducing a novel regularization term that empowers the ERN to learn from the whole training set. Our extensive experiments substantiate our theoretical findings and demonstrate the effectiveness of the proposed solution.
    
[^48]: 在目标识别中将地理多样知识融入提示以提高地理鲁棒性

    Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition. (arXiv:2401.01482v1 [cs.CV])

    [http://arxiv.org/abs/2401.01482](http://arxiv.org/abs/2401.01482)

    本文研究了在目标识别中将地理多样知识融入提示以提高地理鲁棒性的方法，探索了通过大型语言模型获取地理特定对象知识并结合CLIP视觉语言模型的零样本和可学习软提示。通过提出一种地理知识正则化方法，实现了从源地理位置推广到未见目标地理位置的鲁棒性提升。

    

    现有的目标识别模型在不同地理情景下缺乏鲁棒性，这是由于设计和环境中存在重要的领域转移。为了更准确地反映这些转移下的对象概念，需要调整类别表示。在缺乏目标地理位置训练数据的情况下，我们假设可以利用地理特定的对象类别描述性知识来增强鲁棒性。为此，我们探索了通过探测大型语言模型的地理特定对象知识的可行性，并研究了在CLIP视觉语言模型中集成知识的零样本和可学习软提示。特别地，我们提出了一种地理知识正则化方法，以确保在一组源地理位置上训练的软提示能够推广到未见过的目标地理位置集合。当仅依赖来自欧洲的数据进行训练时，我们在DollarStreet上的增益达到了+2.8个国家。

    Existing object recognition models have been shown to lack robustness in diverse geographical scenarios due to significant domain shifts in design and context. Class representations need to be adapted to more accurately reflect an object concept under these shifts. In the absence of training data from target geographies, we hypothesize that geography-specific descriptive knowledge of object categories can be leveraged to enhance robustness. For this purpose, we explore the feasibility of probing a large-language model for geography-specific object knowledge, and we investigate integrating knowledge in zero-shot and learnable soft prompting with the CLIP vision-language model. In particular, we propose a geography knowledge regularization method to ensure that soft prompts trained on a source set of geographies generalize to an unseen target set of geographies. Our gains on DollarStreet when generalizing from a model trained only on data from Europe are as large as +2.8 on countries fro
    
[^49]: Kernel-U-Net: 多元时间序列预测的层次和对称框架

    Kernel-U-Net: Hierarchical and Symmetrical Framework for Multivariate Time Series Forecasting. (arXiv:2401.01479v1 [cs.LG])

    [http://arxiv.org/abs/2401.01479](http://arxiv.org/abs/2401.01479)

    Kernel-U-Net是一种层次和对称框架，用于多元时间序列预测。与现有模型相比，它具有较少的参数数量、灵活性和计算效率。

    

    时间序列预测任务是基于历史信息预测未来趋势。最近基于U-Net的方法在预测真实数据集方面表现出优越性能。然而，这些模型的性能比基于补丁模型或线性模型的模型低。在这项工作中，我们提出了一种对称和层次化的框架，Kernel-U-Net，它在网络的每一层将输入序列切割成片段，然后使用卷积核进行计算。此外，它扩展了经典U-Net中的卷积核的概念，可以接受符合相同设计模式的自定义卷积核。与现有的线性或基于transformer的解决方案相比，我们的模型具有三个优势：1）参数数量较少：参数大小为$O(log(L)^2)$，其中$L$为回溯窗口大小；2）灵活性：其卷积核可以定制和适应数据集；3）计算效率：如果使用此模型，transformer模块的计算复杂度减小为$O(log(L)^2)$。

    Time series forecasting task predicts future trends based on historical information. Recent U-Net-based methods have demonstrated superior performance in predicting real-world datasets. However, the performance of these models is lower than patch-based models or linear models. In this work, we propose a symmetric and hierarchical framework, Kernel-U-Net, which cuts the input sequence into slices at each layer of the network and then computes them using kernels. Furthermore, it generalizes the concept of convolutional kernels in classic U-Net to accept custom kernels that follow the same design pattern. Compared to the existing linear or transformer-based solution, our model contains 3 advantages: 1) A small number of parameters: the parameters size is $O(log(L)^2)$ where $L$ is the look-back window size, 2) Flexibility: its kernels can be customized and fitted to the datasets, 3) Computation efficiency: the computation complexity of transformer modules is reduced to $O(log(L)^2)$ if th
    
[^50]: Stack Overflow回答中信息高亮的初探

    A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])

    [http://arxiv.org/abs/2401.01472](http://arxiv.org/abs/2401.01472)

    本论文进行了首次大规模的探索性研究，研究了Stack Overflow回答中的信息高亮。通过使用神经网络架构，开发了自动推荐突出内容的方法。

    

    背景：浏览Stack Overflow（SO）的知识仍然具有挑战性。为了使帖子对用户更生动，SO允许用户使用Markdown或HTML编写和编辑帖子，以便用户可以利用各种格式化样式（例如粗体、斜体和代码）来突出重要信息。然而，关于突出信息的研究仍然有限。目标：我们在最近的研究中进行了首次大规模的探索性研究，研究了SO回答中的信息高亮。为了扩展我们之前的研究，我们利用最初设计用于命名实体识别任务的神经网络架构，开发了自动推荐带有格式化样式的突出内容的方法。方法：本文研究了Stack Overflow的31,169,429个回答。为了训练推荐模型，我们选择了CNN和BERT模型，针对每种格式化类型（即粗体、斜体、代码和标题）使用我们从SO回答收集的突出信息数据集。

    Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
    
[^51]: 高效视觉Transformer的令牌传播控制器

    Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v1 [cs.CV])

    [http://arxiv.org/abs/2401.01470](http://arxiv.org/abs/2401.01470)

    本文提出一种新颖的令牌传播控制器（TPC），通过结合暂停概率和重新开始概率，实现了对令牌的减少和重复利用的控制，从而提高了视觉Transformer的效率和令牌利用率。

    

    视觉Transformer（ViTs）在各种计算机视觉任务上取得了有希望的结果，然而它们在输入令牌数量上的二次复杂度限制了它们在资源受限环境下的应用。以前的方法采用逐渐减少令牌来解决这个挑战，假设一个层中的令牌冗余意味着所有后续层中也有冗余。我们经验证明这个假设通常是不正确的，即一个层中多余的令牌在后面的层中可以是有用的。基于这个关键洞察力，我们提出了一种新颖的令牌传播控制器（TPC），它结合了两种不同的令牌分布，即暂停概率和重新开始概率，用来控制令牌的减少和重复利用，从而实现更高效的令牌利用。为了改善令牌分布的估计，我们提出了一种平滑机制，作为正则化器，有助于去除噪声异常值。此外

    Vision transformers (ViTs) have achieved promising results on a variety of Computer Vision tasks, however their quadratic complexity in the number of input tokens has limited their application specially in resource-constrained settings. Previous approaches that employ gradual token reduction to address this challenge assume that token redundancy in one layer implies redundancy in all the following layers. We empirically demonstrate that this assumption is often not correct, i.e., tokens that are redundant in one layer can be useful in later layers. We employ this key insight to propose a novel token propagation controller (TPC) that incorporates two different token-distributions, i.e., pause probability and restart probability to control the reduction and reuse of tokens respectively, which results in more efficient token utilization. To improve the estimates of token distributions, we propose a smoothing mechanism that acts as a regularizer and helps remove noisy outliers. Furthermore
    
[^52]: 通过深度集合线性化最优传输进行点云分类

    Point Cloud Classification via Deep Set Linearized Optimal Transport. (arXiv:2401.01460v1 [cs.LG])

    [http://arxiv.org/abs/2401.01460](http://arxiv.org/abs/2401.01460)

    通过深度学习方法，我们提出了一种名为深度集合线性化最优传输的算法，用于将点云嵌入到L2空间，并构建了一个可以区分不同类别点云的分类器。我们的算法利用输入凸神经网络学习点云之间的Wasserstein-2距离的近似，并通过训练鉴别器网络来提高分类准确率。

    

    我们引入了一个名为深度集合线性化最优传输的算法，旨在将点云高效地嵌入到L2空间中。这种嵌入在保留Wasserstein空间中特定低维结构的同时，构建了一个能够区分不同类别点云的分类器。我们的方法基于以下观察，即不同点云的最优传输映射之间的L2距离（源自共享的固定参考分布）在某些假设下近似表示这些点云之间的Wasserstein-2距离。为了学习这些传输映射的近似，我们采用输入凸神经网络（ICNNs），并且证明在特定条件下，这些ICNNs的样本之间的欧氏距离与真实分布之间的Wasserstein-2距离非常相似。此外，我们训练了一个鉴别器网络，对这些样本进行加权，并创建了一个置换不变的模型。

    We introduce Deep Set Linearized Optimal Transport, an algorithm designed for the efficient simultaneous embedding of point clouds into an $L^2-$space. This embedding preserves specific low-dimensional structures within the Wasserstein space while constructing a classifier to distinguish between various classes of point clouds. Our approach is motivated by the observation that $L^2-$distances between optimal transport maps for distinct point clouds, originating from a shared fixed reference distribution, provide an approximation of the Wasserstein-2 distance between these point clouds, under certain assumptions. To learn approximations of these transport maps, we employ input convex neural networks (ICNNs) and establish that, under specific conditions, Euclidean distances between samples from these ICNNs closely mirror Wasserstein-2 distances between the true distributions. Additionally, we train a discriminator network that attaches weights these samples and creates a permutation inva
    
[^53]: 使用不确定性指纹实现神经网络的并发自测

    Concurrent Self-testing of Neural Networks Using Uncertainty Fingerprint. (arXiv:2401.01458v1 [cs.LG])

    [http://arxiv.org/abs/2401.01458](http://arxiv.org/abs/2401.01458)

    本文提出了使用不确定性指纹进行神经网络的并发自测的方法，可以在在线操作中检测并纠正单个和多个永久和软错误，适用于始终开启的安全关键应用。

    

    神经网络（NN）越来越广泛地应用于硬件加速器（NN-HA）上的始终开启的安全关键应用中，这些应用采用各种存储技术。对于安全关键应用来说，可靠的持续操作是至关重要的。在在线操作中，NN因辐射、老化和热效应等因素而容易出现单个和多个永久和软错误。显式的NN-HA测试方法不能检测到推断过程中的瞬态故障，对于始终开启的应用来说也不适用，并且需要大量的测试向量生成和存储。因此，在本文中，我们提出了一种表示NN在线故障状态的“不确定性指纹”方法。此外，我们还提出了一种特定的双头NN拓扑结构，专门设计用于生成不确定性指纹和NN的主要预测。在在线操作过程中，通过匹配不确定性指纹，我们可以同时自测NN，可达到100

    Neural networks (NNs) are increasingly used in always-on safety-critical applications deployed on hardware accelerators (NN-HAs) employing various memory technologies. Reliable continuous operation of NN is essential for safety-critical applications. During online operation, NNs are susceptible to single and multiple permanent and soft errors due to factors such as radiation, aging, and thermal effects. Explicit NN-HA testing methods cannot detect transient faults during inference, are unsuitable for always-on applications, and require extensive test vector generation and storage. Therefore, in this paper, we propose the \emph{uncertainty fingerprint} approach representing the online fault status of NN. Furthermore, we propose a dual head NN topology specifically designed to produce uncertainty fingerprints and the primary prediction of the NN in \emph{a single shot}. During the online operation, by matching the uncertainty fingerprint, we can concurrently self-test NNs with up to $100
    
[^54]: ProbMCL: 简单的概率对比学习用于多标签视觉分类

    ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label Visual Classification. (arXiv:2401.01448v1 [cs.CV])

    [http://arxiv.org/abs/2401.01448](http://arxiv.org/abs/2401.01448)

    ProbMCL是一个简单而有效的概率对比学习框架，用于解决多标签图像分类任务中的挑战。该方法通过采用监督对比学习和混合密度网络，在捕捉标签之间的依赖关系的同时，降低了复杂模块的计算需求和可解释性的不足。

    

    在计算机视觉和医学影像等许多领域中，多标签图像分类是一项具有挑战性的任务。最近的进展引入了基于图和变压器的方法来提高性能并捕捉标签之间的依赖关系。然而，这些方法通常包含复杂的模块，需要大量计算，并且缺乏可解释性。在本文中，我们提出了概率多标签对比学习（ProbMCL），这是一个新颖的框架，用于解决多标签图像分类任务中的这些挑战。我们提出了一个简单而有效的方法，采用了监督对比学习，根据决策阈值将与锚图像具有足够标签的样本引入正样本集。这种结构通过将正样本对的嵌入拉近，并推离低于阈值的负样本来捕捉标签之间的依赖关系。我们通过将混合密度网络融入对比学习中来增强表示学习。

    Multi-label image classification presents a challenging task in many domains, including computer vision and medical imaging. Recent advancements have introduced graph-based and transformer-based methods to improve performance and capture label dependencies. However, these methods often include complex modules that entail heavy computation and lack interpretability. In this paper, we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel framework to address these challenges in multi-label image classification tasks. Our simple yet effective approach employs supervised contrastive learning, in which samples that share enough labels with an anchor image based on a decision threshold are introduced as a positive set. This structure captures label dependencies by pulling positive pair embeddings together and pushing away negative samples that fall below the threshold. We enhance representation learning by incorporating a mixture density network into contrastive learning 
    
[^55]: 具有干扰和数据异质性意识的分层无线联邦学习

    Hierarchical Over-the-Air Federated Learning with Awareness of Interference and Data Heterogeneity. (arXiv:2401.01442v1 [cs.IT])

    [http://arxiv.org/abs/2401.01442](http://arxiv.org/abs/2401.01442)

    本文介绍了一种具有干扰和数据异质性意识的分层无线联邦学习方法，通过优化接收机的归一化因子来最小化干扰影响，并利用梯度聚合对抗数据异质性，实现了较高的学习准确性，并且可以优于传统的分层算法。

    

    在无线网络上实施分层联邦学习时，可扩展性保证和处理干扰和设备数据异质性的能力至关重要。本文介绍了一种旨在应对这些挑战的学习方法，以及通过空中计算高效利用单一无线资源的可扩展传输方案。为了提供对抗数据异质性的能力，我们采用了梯度聚合。同时，通过优化接收机归一化因子，最小化了干扰的影响。为此，我们使用随机几何模型对多簇无线网络进行建模，并将聚合估计的均方误差表征为网络参数的函数。我们表明，尽管存在干扰和数据异质性，所提出的方案实现了较高的学习准确性，并且可以明显优于传统的分层算法。

    When implementing hierarchical federated learning over wireless networks, scalability assurance and the ability to handle both interference and device data heterogeneity are crucial. This work introduces a learning method designed to address these challenges, along with a scalable transmission scheme that efficiently uses a single wireless resource through over-the-air computation. To provide resistance against data heterogeneity, we employ gradient aggregations. Meanwhile, the impact of interference is minimized through optimized receiver normalizing factors. For this, we model a multi-cluster wireless network using stochastic geometry, and characterize the mean squared error of the aggregation estimations as a function of the network parameters. We show that despite the interference and the data heterogeneity, the proposed scheme achieves high learning accuracy and can significantly outperform the conventional hierarchical algorithm.
    
[^56]: 高维因果推断的模块化深度生成模型学习

    Modular Learning of Deep Causal Generative Models for High-dimensional Causal Inference. (arXiv:2401.01426v1 [cs.LG])

    [http://arxiv.org/abs/2401.01426](http://arxiv.org/abs/2401.01426)

    本文提出了一种用于高维因果推断的模块化深度生成模型学习算法，该算法利用预训练的模型来回答由高维数据引起的因果查询。

    

    Pearl的因果层次结构在观测、干预和反事实问题之间建立了明确的分离。研究人员提出了计算可辨识因果查询的声音和完整算法，在给定层次的因果结构和数据的情况下使用较低层次的层次的数据。然而，大多数这些算法假设我们可以准确估计数据的概率分布，这对于如图像这样的高维变量是一个不切实际的假设。另一方面，现代生成式深度学习架构可以被训练来学习如何准确地从这样的高维分布中采样。特别是随着图像基模型的最近兴起，利用预训练模型来回答带有这样高维数据的因果查询是非常有吸引力的。为了解决这个问题，我们提出了一个顺序训练算法，给定因果结构和预训练的条件生成模型，可以训练一个模型来估计由高维数据引起的因果关系。

    Pearl's causal hierarchy establishes a clear separation between observational, interventional, and counterfactual questions. Researchers proposed sound and complete algorithms to compute identifiable causal queries at a given level of the hierarchy using the causal structure and data from the lower levels of the hierarchy. However, most of these algorithms assume that we can accurately estimate the probability distribution of the data, which is an impractical assumption for high-dimensional variables such as images. On the other hand, modern generative deep learning architectures can be trained to learn how to accurately sample from such high-dimensional distributions. Especially with the recent rise of foundation models for images, it is desirable to leverage pre-trained models to answer causal queries with such high-dimensional data. To address this, we propose a sequential training algorithm that, given the causal structure and a pre-trained conditional generative model, can train a
    
[^57]: SwapTransformer：通过在OSHA数据集上的模仿学习，实现高速公路超车策略规划模型

    SwapTransformer: highway overtaking tactical planner model via imitation learning on OSHA dataset. (arXiv:2401.01425v1 [cs.AI])

    [http://arxiv.org/abs/2401.01425](http://arxiv.org/abs/2401.01425)

    本文旨在通过在OSHA数据集上的模仿学习，设计高速公路上的自动超车和换道策略规划模型SwapTransformer。通过引入辅助任务和比较基准模型，证明了该模型在仿真环境中的性能优势。

    

    本文研究了高速公路场景中的高级决策问题，例如换道和超车。具体而言，本文旨在改进用于高速公路上的自动超车和换道的出行辅助功能。在仿真环境中收集了约900万个样本，包括车道图像和其他动态物体。这个数据集被称为OSHA（模拟高速公路上的超车）数据集，用于解决这个挑战。为了解决这个问题，设计并实现了一个名为SwapTransformer的架构，作为在OSHA数据集上的模仿学习方法。此外，还提出了辅助任务，如未来点和车辆距离网络预测，以帮助模型更好地理解周围环境。在仿真环境中，将所提出的解决方案与多层感知机（MLP）和多头自注意力网络作为基准进行了性能比较。同时也展示了模型的性能。

    This paper investigates the high-level decision-making problem in highway scenarios regarding lane changing and over-taking other slower vehicles. In particular, this paper aims to improve the Travel Assist feature for automatic overtaking and lane changes on highways. About 9 million samples including lane images and other dynamic objects are collected in simulation. This data; Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this challenge. To solve this problem, an architecture called SwapTransformer is designed and implemented as an imitation learning approach on the OSHA dataset. Moreover, auxiliary tasks such as future points and car distance network predictions are proposed to aid the model in better understanding the surrounding environment. The performance of the proposed solution is compared with a multi-layer perceptron (MLP) and multi-head self-attention networks as baselines in a simulation environment. We also demonstrate the performance of the model 
    
[^58]: VALD-MD: 面向医疗诊断的基于潜在扩散的视觉归因

    VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics. (arXiv:2401.01414v1 [eess.IV])

    [http://arxiv.org/abs/2401.01414](http://arxiv.org/abs/2401.01414)

    VALD-MD是一种基于潜在扩散的视觉归因技术，通过结合潜在扩散模型和大型语言模型，在医学图像中生成正常对应的异常图像，从而揭示出与诊断相关的图像部分。

    

    医学影像中的视觉归因旨在清晰展示医学图像中与诊断相关的部分，与标准机器视觉流程中检测病变组织的方法相比（对临床医生来说不太直观可解释）。我们提出了一种新颖的生成式视觉归因技术，利用潜在扩散模型与领域特定的大型语言模型相结合，生成异常图像的正常对应图像。因此，两者之间的差异产生了指示诊断相关图像部分的映射。为了实现这一点，我们在图像生成过程中使用图像先验知识，并结合适当的条件机制，包括从医学科学和应用放射学中获取的自然语言文本提示。我们在包含COVID-19放射学数据库的实验中进行了定量评估。

    Visual attribution in medical imaging seeks to make evident the diagnostically-relevant components of a medical image, in contrast to the more common detection of diseased tissue deployed in standard machine vision pipelines (which are less straightforwardly interpretable/explainable to clinicians). We here present a novel generative visual attribution technique, one that leverages latent diffusion models in combination with domain-specific large language models, in order to generate normal counterparts of abnormal images. The discrepancy between the two hence gives rise to a mapping indicating the diagnostically-relevant image components. To achieve this, we deploy image priors in conjunction with appropriate conditioning mechanisms in order to control the image generative process, including natural language text prompts acquired from medical science and applied radiology. We perform experiments and quantitatively evaluate our results on the COVID-19 Radiography Database containing la
    
[^59]: 可扩展的子二次时间网络重建

    Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v1 [cs.DS])

    [http://arxiv.org/abs/2401.01404](http://arxiv.org/abs/2401.01404)

    这篇论文提出了一个可扩展的网络重建算法，能够在次二次时间内实现结果，通过随机的二阶邻居搜索产生最佳的边候选。

    

    网络重建是指在只有关于条件偶联的观测数据，例如时间序列或图模型的独立样本的情况下，确定N个节点之间未观测到的成对耦合。针对这个问题提出的算法的可扩展性的主要障碍是似乎无法避免的二次复杂度O(N^2)，即要考虑每种可能的成对耦合至少一次，尽管大多数感兴趣的网络都是稀疏的，非零耦合的数量只有O(N)。在这里，我们提出了一个适用于广泛重建问题的通用算法，其在子二次时间内实现结果，其数据相关复杂度宽松上界为O(N^(3/2)logN)，但具有更典型的对数线性复杂度O(Nlog^2 N)。我们的算法依赖于一个随机的二阶邻居搜索，产生了最佳的边候选。

    Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings -- typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $O(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that achieves its result in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on a stochastic second neighbor search that produces the best edge candidat
    
[^60]: 通过自适应学习稀疏子空间加速黑箱分子性质优化

    Accelerating Black-Box Molecular Property Optimization by Adaptively Learning Sparse Subspaces. (arXiv:2401.01398v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.01398](http://arxiv.org/abs/2401.01398)

    通过自适应学习稀疏子空间加速黑箱分子性质优化的挑战已经通过学习更低维的编码得到解决

    

    分子性质优化问题本质上具有挑战性，因为它们是在离散、非结构化空间上定义的，并且标记过程涉及昂贵的模拟或实验，这从根本上限制了可用数据的数量。贝叶斯优化是一种有效优化嘈杂的、黑箱目标函数（例如测量的性质值）的强大且流行的框架，因此对于分子性质优化而言是一个潜在的有吸引力的框架。要将贝叶斯优化应用于分子性质优化问题，必须选择一种结构化的分子表示方法，以便构建一个概率模型。许多分子表示方法已经被开发出来，然而它们都是高维的，这在贝叶斯优化过程中引入了重要的挑战，主要是因为维度的诅咒使得难以定义和执行适合的代理模型。最近通过学习更低维的编码来解决了这个挑战

    Molecular property optimization (MPO) problems are inherently challenging since they are formulated over discrete, unstructured spaces and the labeling process involves expensive simulations or experiments, which fundamentally limits the amount of available data. Bayesian optimization (BO) is a powerful and popular framework for efficient optimization of noisy, black-box objective functions (e.g., measured property values), thus is a potentially attractive framework for MPO. To apply BO to MPO problems, one must select a structured molecular representation that enables construction of a probabilistic surrogate model. Many molecular representations have been developed, however, they are all high-dimensional, which introduces important challenges in the BO process -- mainly because the curse of dimensionality makes it difficult to define and perform inference over a suitable class of surrogate models. This challenge has been recently addressed by learning a lower-dimensional encoding of 
    
[^61]: 深度自回归建模用于土地利用和土地覆盖

    Deep autoregressive modeling for land use land cover. (arXiv:2401.01395v1 [cs.CV])

    [http://arxiv.org/abs/2401.01395](http://arxiv.org/abs/2401.01395)

    本研究使用深度自回归模型对土地利用和土地覆盖进行建模，在捕捉丰富的空间相关模式方面超过了基准模型，但需要进一步调整以产生更准确的预测分布。

    

    由于地理要素之间的长程依赖以及与地形、生态和人类发展相关的明显空间模式，土地利用和土地覆盖（LULC）建模是一项具有挑战性的任务。我们发现土地利用空间模式建模与计算机视觉中的图像修复任务存在密切联系，并对具有约1900万参数的修改PixelCNN架构进行了LULC建模的研究。与基准空间统计模型相比，我们发现前者能够捕捉到更丰富的空间相关模式，如道路和水体，但不能产生校准的预测分布，这表明需要进行额外的调整。我们发现在与重要的生态相关土地利用统计数据（如斑块数量和相邻性）相比中存在预测性低分散的证据，这可以通过操作抽样可变性在一定程度上得到改善。

    Land use / land cover (LULC) modeling is a challenging task due to long-range dependencies between geographic features and distinct spatial patterns related to topography, ecology, and human development. We identify a close connection between modeling of spatial patterns of land use and the task of image inpainting from computer vision and conduct a study of a modified PixelCNN architecture with approximately 19 million parameters for modeling LULC. In comparison with a benchmark spatial statistical model, we find that the former is capable of capturing much richer spatial correlation patterns such as roads and water bodies but does not produce a calibrated predictive distribution, suggesting the need for additional tuning. We find evidence of predictive underdispersion with regard to important ecologically-relevant land use statistics such as patch count and adjacency which can be ameliorated to some extent by manipulating sampling variability.
    
[^62]: 回溯新Q-Newton方法，Newton流，Voronoi图和随机求根方法

    Backtracking New Q-Newton's method, Newton's flow, Voronoi's diagram and Stochastic root finding. (arXiv:2401.01393v1 [math.OC])

    [http://arxiv.org/abs/2401.01393](http://arxiv.org/abs/2401.01393)

    本文介绍了回溯新的Q-Newton方法（BNQN），该方法是Newton方法的一种变体，具有强大的理论保证，易于实现，且在实验中有良好的表现。通过实验，发现BNQN在多项式和亚纯函数的根搜索中具有更平滑的吸引域，并通过与Newton流和Voronoi图的联系提出了一些挑战性的问题。此外，在面对随机扰动时，BNQN比Newton方法和随机松弛Newton方法更具鲁棒性。

    

    最近第三作者引入了一种名为回溯新Q-Newton方法(BNQN)的Newton方法的变种，该方法具有很强的理论保证，易于实现，并具有良好的实验性能。先前进行的实验显示，使用BNQN可以找到多项式和亚纯函数的根时，吸引域具有一些显著的特性。总体上看，它们比Newton方法的吸引域更加平滑。在本文中，我们继续深入实验研究这一显著现象，并将BNQN与Newton流和Voronoi图联系起来。这种联系给出了一些具有挑战性的难题需要解释。实验还表明，与Newton方法和随机松弛Newton方法相比，BNQN对随机扰动更加稳健。

    A new variant of Newton's method - named Backtracking New Q-Newton's method (BNQN) - which has strong theoretical guarantee, is easy to implement, and has good experimental performance, was recently introduced by the third author.  Experiments performed previously showed some remarkable properties of the basins of attractions for finding roots of polynomials and meromorphic functions, with BNQN. In general, they look more smooth than that of Newton's method.  In this paper, we continue to experimentally explore in depth this remarkable phenomenon, and connect BNQN to Newton's flow and Voronoi's diagram. This link poses a couple of challenging puzzles to be explained. Experiments also indicate that BNQN is more robust against random perturbations than Newton's method and Random Relaxed Newton's method.
    
[^63]: 关于使用位置编码的MLP学习SDF的最优采样方法

    On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding. (arXiv:2401.01391v1 [cs.CV])

    [http://arxiv.org/abs/2401.01391](http://arxiv.org/abs/2401.01391)

    本文针对采样率与学习神经隐式场的准确性之间的关系进行了研究，在傅里叶分析的基础上提出了一种简单有效的方法来确定适当的采样率，以解决MLP中噪声伪影的问题。

    

    神经隐式场，如形状的神经有符号距离场（SDF），已成为许多应用中的强大表示方法，例如编码3D形状和执行碰撞检测。通常，隐式场由带有位置编码（PE）的多层感知器（MLP）进行编码以捕捉高频几何细节。然而，这种带有PE的MLP的一个显著副作用是学习到的隐式场中存在噪声伪影。尽管增加采样率通常可以缓解这些伪影，但在本文中，我们通过傅立叶分析的视角来解释这种不良现象。我们设计了一个工具来确定学习精确神经隐式场的适当采样率，而不会产生不良的副作用。具体而言，我们提出了一种简单而有效的方法，基于网络响应的傅里叶分析，用于估计带有随机权重的给定网络的内在频率。

    Neural implicit fields, such as the neural signed distance field (SDF) of a shape, have emerged as a powerful representation for many applications, e.g., encoding a 3D shape and performing collision detection. Typically, implicit fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding (PE) to capture high-frequency geometric details. However, a notable side effect of such PE-equipped MLPs is the noisy artifacts present in the learned implicit fields. While increasing the sampling rate could in general mitigate these artifacts, in this paper we aim to explain this adverse phenomenon through the lens of Fourier analysis. We devise a tool to determine the appropriate sampling rate for learning an accurate neural implicit field without undesirable side effects. Specifically, we propose a simple yet effective method to estimate the intrinsic frequency of a given network with randomized weights based on the Fourier analysis of the network's responses. It is observed that
    
[^64]: 适用于长距离穿墙人体活动识别的定向天线系统

    Directional Antenna Systems for Long-Range Through-Wall Human Activity Recognition. (arXiv:2401.01388v1 [cs.CV])

    [http://arxiv.org/abs/2401.01388](http://arxiv.org/abs/2401.01388)

    本论文研究了适用于长距离穿墙人体活动识别的定向天线系统，提出了ESP32-S3与定向双极天线和ESP32-S3与印刷倒F天线结合的两个有前景的系统。这些系统在WiFi-based HAR中展现出出色的性能。

    

    基于WiFi信道状态信息（CSI）的人体活动识别（HAR）使得在空间受限的环境中保持视觉隐私的情况下实现了非接触式、长距离感知。然而，尽管周围存在许多支持WiFi的设备，但很少有设备向用户公开CSI，导致感知硬件选择有限。Espressif ESP32的变体已经成为潜在的低成本、易于部署的WiFi CSI-based HAR解决方案。在这项工作中，对基于四个ESP32-S3的2.4GHz定向天线系统进行了评估，以评估其支持长距离穿墙HAR的能力。提出了两个有前景的系统，其中一个将ESP32-S3与定向双极天线相结合。据我们所知，这种组合是WiFi基础HAR中的首次演示。第二个系统依靠ESP32-S3的内置印刷倒F天线（PIFA）并通过平面反射器实现方向性。在全面评估中，这两个系统显示出很好的性能。

    WiFi Channel State Information (CSI)-based human activity recognition (HAR) enables contactless, long-range sensing in spatially constrained environments while preserving visual privacy. However, despite the presence of numerous WiFi-enabled devices around us, few expose CSI to users, resulting in a lack of sensing hardware options. Variants of the Espressif ESP32 have emerged as potential low-cost and easy-to-deploy solutions for WiFi CSI-based HAR. In this work, four ESP32-S3-based 2.4GHz directional antenna systems are evaluated for their ability to facilitate long-range through-wall HAR. Two promising systems are proposed, one of which combines the ESP32-S3 with a directional biquad antenna. This combination represents, to the best of our knowledge, the first demonstration of such a system in WiFi-based HAR. The second system relies on the built-in printed inverted-F antenna (PIFA) of the ESP32-S3 and achieves directionality through a plane reflector. In a comprehensive evaluation 
    
[^65]: 使用全幻灯切片图像的组织伪影分割与严重性分析的自动诊断

    Tissue Artifact Segmentation and Severity Analysis for Automated Diagnosis Using Whole Slide Images. (arXiv:2401.01386v1 [eess.IV])

    [http://arxiv.org/abs/2401.01386](http://arxiv.org/abs/2401.01386)

    这篇论文提出了一种使用全幻灯切片图像进行组织伪影分割与严重性分析的自动诊断方法。通过计算机视觉和人工智能，可以在没有人类监督的情况下对整个全幻灯切片图像进行自主分析，但受到组织伪影影响的区域需要被准确识别和排除。

    

    传统上，病理学分析和诊断是由专家在显微镜下通过观察玻璃切片标本进行手动眼球判断来完成的。全幻灯切片图像是从玻璃切片制作的数字标本。全幻灯切片图像使得标本能够在计算机屏幕上观察，并引发了计算病理学，其中利用计算机视觉和人工智能进行自动分析和诊断。借助当前的计算进展，整个全幻灯切片图像可以在没有人类监督的情况下进行自主分析。然而，如果全幻灯切片图像受到组织伪影（如组织褶皱或气泡）的影响，则分析可能会失败或导致错误的诊断，这取决于伪影的严重程度。现有的伪影检测方法依赖于专家对严重程度的评估，以消除受到伪影影响的区域进行分析。这个过程耗时、繁琐，并且有损于自动化分析或伪影去除的目标。

    Traditionally, pathological analysis and diagnosis are performed by manually eyeballing glass slide specimens under a microscope by an expert. The whole slide image is the digital specimen produced from the glass slide. Whole slide image enabled specimens to be observed on a computer screen and led to computational pathology where computer vision and artificial intelligence are utilized for automated analysis and diagnosis. With the current computational advancement, the entire whole slide image can be analyzed autonomously without human supervision. However, the analysis could fail or lead to wrong diagnosis if the whole slide image is affected by tissue artifacts such as tissue fold or air bubbles depending on the severity. Existing artifact detection methods rely on experts for severity assessment to eliminate artifact affected regions from the analysis. This process is time consuming, exhausting and undermines the goal of automated analysis or removal of artifacts without evaluatin
    
[^66]: 强传递关系与图神经网络

    Strong Transitivity Relations and Graph Neural Networks. (arXiv:2401.01384v1 [cs.SI])

    [http://arxiv.org/abs/2401.01384](http://arxiv.org/abs/2401.01384)

    这项研究提出了一种基于传递关系的相似性扩展，通过引入传递图神经网络（TransGNN）从全局和局部的角度捕捉整个图的相似性，显著提高了几个GNN模型的性能。

    

    在基于图的学习中，局部邻域在嵌入生成中起着关键作用。人们普遍认为节点应该具有类似于其邻居的嵌入。在这项研究中，我们试图将相似性的概念从附近邻域扩展到整个图。我们提供了一种基于传递关系的相似性扩展，使得图神经网络（GNNs）能够捕捉整个图的全局相似性和局部相似性。我们引入了传递图神经网络（TransGNN），它不仅考虑了局部节点相似性，还通过区分强传递关系和弱传递关系并利用它们来考虑全局相似性。我们在几个真实世界数据集上评估了我们的模型，并显示出它显著改善了几个知名GNN模型的性能，例如节点分类。

    Local neighborhoods play a crucial role in embedding generation in graph-based learning. It is commonly believed that nodes ought to have embeddings that resemble those of their neighbors. In this research, we try to carefully expand the concept of similarity from nearby neighborhoods to the entire graph. We provide an extension of similarity that is based on transitivity relations, which enables Graph Neural Networks (GNNs) to capture both global similarities and local similarities over the whole graph. We introduce Transitivity Graph Neural Network (TransGNN), which more than local node similarities, takes into account global similarities by distinguishing strong transitivity relations from weak ones and exploiting them. We evaluate our model over several real-world datasets and showed that it considerably improves the performance of several well-known GNN models, for tasks such as node classification.
    
[^67]: 使用稀缺数据和联邦多轨迹GNN预测婴儿脑连接性

    Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.01383](http://arxiv.org/abs/2401.01383)

    我们提出了一种使用联邦多轨迹GNN的方法，通过稀缺数据预测婴儿脑连接性。通过联邦学习，我们通过聚合多个医院的本地学习结果来提高模型性能，同时保护数据隐私。

    

    对于识别早期脑连接性发展的动态过程，了解婴儿脑网络在出生后的第一年中的复杂演化至关重要。现有的深度学习解决方案存在三个主要局限性。首先，它们不能泛化到多轨迹预测任务，其中每个图轨迹对应于特定的成像模态或连接类型（例如T1-w MRI）。其次，现有模型需要大量的训练数据集才能达到令人满意的性能，而这往往很难获取。第三，它们不能有效利用不完整的时间序列数据。为了解决这些限制，我们引入了FedGmTE-Net++，一种联邦图形多轨迹演化网络。通过联邦学习的力量，我们在有限的医院数据集中聚合了不同医院的本地学习结果。结果即可提高每个医院本地生成模型的性能，同时保护数据隐私。

    The understanding of the convoluted evolution of infant brain networks during the first postnatal year is pivotal for identifying the dynamics of early brain connectivity development. Existing deep learning solutions suffer from three major limitations. First, they cannot generalize to multi-trajectory prediction tasks, where each graph trajectory corresponds to a particular imaging modality or connectivity type (e.g., T1-w MRI). Second, existing models require extensive training datasets to achieve satisfactory performance which are often challenging to obtain. Third, they do not efficiently utilize incomplete time series data. To address these limitations, we introduce FedGmTE-Net++, a federated graph-based multi-trajectory evolution network. Using the power of federation, we aggregate local learnings among diverse hospitals with limited datasets. As a result, we enhance the performance of each hospital's local generative model, while preserving data privacy. The three key innovation
    
[^68]: 使用高分辨率多光谱无人机影像和机器学习来进行核桃水分胁迫的测绘

    Mapping Walnut water Stress with High Resolution Multispectral UAV Imagery and Machine Learning. (arXiv:2401.01375v1 [cs.CV])

    [http://arxiv.org/abs/2401.01375](http://arxiv.org/abs/2401.01375)

    本研究提出了一种利用高分辨率多光谱无人机影像和机器学习进行核桃水分胁迫测绘的方法，通过结合无人机影像和天气数据，使用随机森林模型有效估计了核桃树的茎水势，为核桃精准灌溉管理提供了重要的参考依据。

    

    有效监测核桃的水分状态和胁迫水平对于加利福尼亚州重要农作物核桃的精准灌溉管理至关重要。本研究提出了一个利用随机森林（RF）模型结合无人机航拍的高分辨率多光谱遥感影像和天气数据来测绘茎水势（SWP）的机器学习方法。从2017年到2018年，使用一架装备有七波段多光谱相机的无人机，在一个商业核桃园进行了五次飞行，同时伴随对抽样核桃植株的地面测量。RF回归模型利用来自正射无人机影像和天气数据的植被指数，有效地估计了地面测量的SWPs，达到了0.63的R^2值和0.80巴的平均绝对误差（MAE）。天气数据的整合尤为重要，以整合不同飞行日期的数据。显著的变量

    Effective monitoring of walnut water status and stress level across the whole orchard is an essential step towards precision irrigation management of walnuts, a significant crop in California. This study presents a machine learning approach using Random Forest (RF) models to map stem water potential (SWP) by integrating high-resolution multispectral remote sensing imagery from Unmanned Aerial Vehicle (UAV) flights with weather data. From 2017 to 2018, five flights of an UAV equipped with a seven-band multispectral camera were conducted over a commercial walnut orchard, paired with concurrent ground measurements of sampled walnut plants. The RF regression model, utilizing vegetation indices derived from orthomosaiced UAV imagery and weather data, effectively estimated ground-measured SWPs, achieving an $R^2$ of 0.63 and a mean absolute error (MAE) of 0.80 bars. The integration of weather data was particularly crucial for consolidating data across various flight dates. Significant variab
    
[^69]: 使用张量卷积神经网络提高制造业中的缺陷检测

    Boosting Defect Detection in Manufacturing using Tensor Convolutional Neural Networks. (arXiv:2401.01373v1 [cs.CV])

    [http://arxiv.org/abs/2401.01373](http://arxiv.org/abs/2401.01373)

    我们引入了一种张量卷积神经网络（T-CNN）来提高制造业中的缺陷检测任务，通过减少模型参数空间，我们实现了比等效CNN模型更快的训练速度和性能。与传统的人类视觉检查相比，在质量指标方面，T-CNN在参数数量上只有15倍少，训练时间快4%至19%。这项研究在实际制造应用中取得了显著的成果。

    

    缺陷检测是制造业质量控制阶段中最重要但也最具挑战性的任务之一。在本研究中，我们引入了一种张量卷积神经网络（T-CNN），并在罗伯特·博世制造厂生产的超声波传感器组件的真实缺陷检测应用中考察其性能。我们的量子启发式T-CNN在减少的模型参数空间上运行，极大地提高了等效CNN模型的训练速度和性能，而不会牺牲准确性。具体来说，我们演示了T-CNN可以通过质量指标来衡量与传统CNN相同的性能，但参数数量只有其15倍少，训练时间快4%至19%。我们的结果表明，T-CNN大大超越了传统人类视觉检查的结果，在当前制造应用中具有价值。

    Defect detection is one of the most important yet challenging tasks in the quality control stage in the manufacturing sector. In this work, we introduce a Tensor Convolutional Neural Network (T-CNN) and examine its performance on a real defect detection application in one of the components of the ultrasonic sensors produced at Robert Bosch's manufacturing plants. Our quantum-inspired T-CNN operates on a reduced model parameter space to substantially improve the training speed and performance of an equivalent CNN model without sacrificing accuracy. More specifically, we demonstrate how T-CNNs are able to reach the same performance as classical CNNs as measured by quality metrics, with up to fifteen times fewer parameters and 4% to 19% faster training times. Our results demonstrate that the T-CNN greatly outperforms the results of traditional human visual inspection, providing value in a current real application in manufacturing.
    
[^70]: RL-MPCA: 基于强化学习的多阶段计算分配方法用于推荐系统

    RL-MPCA: A Reinforcement Learning Based Multi-Phase Computation Allocation Approach for Recommender Systems. (arXiv:2401.01369v1 [cs.IR])

    [http://arxiv.org/abs/2401.01369](http://arxiv.org/abs/2401.01369)

    RL-MPCA是一种基于强化学习的多阶段计算分配方法，用于解决推荐系统中在计算资源有限情况下的计算成本和业务收益之间的权衡问题。

    

    推荐系统旨在从大量候选项中向用户推荐最合适的物品。随着用户请求的增加和服务（或模型）的复杂性增加，其计算成本也在增加。在计算资源有限的情况下，如何在计算成本和业务收益之间做出权衡成为一个重要问题。现有的研究集中于在队列截断场景中动态分配计算资源（即分配候选项的大小），并将计算资源分配问题建模为带约束条件的优化问题。其中一些研究集中于单阶段的计算资源分配，而其他研究则集中于多阶段的计算资源分配，但引入了一些关于队列截断场景的假设。然而，这些假设在其他情景下（如检索通道选择和预测模型选择）是不成立的。此外，现有的研究忽略了请求在不同阶段之间的状态转移过程，限制了其有效性。

    Recommender systems aim to recommend the most suitable items to users from a large number of candidates. Their computation cost grows as the number of user requests and the complexity of services (or models) increases. Under the limitation of computation resources (CRs), how to make a trade-off between computation cost and business revenue becomes an essential question. The existing studies focus on dynamically allocating CRs in queue truncation scenarios (i.e., allocating the size of candidates), and formulate the CR allocation problem as an optimization problem with constraints. Some of them focus on single-phase CR allocation, and others focus on multi-phase CR allocation but introduce some assumptions about queue truncation scenarios. However, these assumptions do not hold in other scenarios, such as retrieval channel selection and prediction model selection. Moreover, existing studies ignore the state transition process of requests between different phases, limiting the effectiven
    
[^71]: 基于神经网络和后继表示训练的多模态认知地图

    Multi-Modal Cognitive Maps based on Neural Networks trained on Successor Representations. (arXiv:2401.01364v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.01364](http://arxiv.org/abs/2401.01364)

    该研究通过使用神经网络和后继表示训练，建立了一个能够模拟位置细胞动态和认知地图表示的多模态认知地图模型。该模型能够以超过90%的准确率从一种形式推断到另一种形式，对于改善当前人工智能系统对环境的理解具有潜在意义。

    

    认知地图是关于大脑如何有效地组织记忆和提取上下文的一个提议概念。内隐-海马复合体在情节和关系记忆处理以及空间导航中起着重要作用，并被认为通过位置细胞和网格细胞建立认知地图。为了利用认知地图的有希望的特性，我们使用后继表示建立了一个多模态神经网络，能够模拟位置细胞动态和认知地图表示。在这里，我们使用由图像和词嵌入组成的多模态输入。网络学习了新输入与训练数据库之间的相似性，从而成功建立了认知地图的表示。随后，网络的预测可以以超过90%的准确率从一种形式推断到另一种形式。所提出的方法可能成为改进当前人工智能系统以更好理解环境的基石。

    Cognitive maps are a proposed concept on how the brain efficiently organizes memories and retrieves context out of them. The entorhinal-hippocampal complex is heavily involved in episodic and relational memory processing, as well as spatial navigation and is thought to built cognitive maps via place and grid cells. To make use of the promising properties of cognitive maps, we set up a multi-modal neural network using successor representations which is able to model place cell dynamics and cognitive map representations. Here, we use multi-modal inputs consisting of images and word embeddings. The network learns the similarities between novel inputs and the training database and therefore the representation of the cognitive map successfully. Subsequently, the prediction of the network can be used to infer from one modality to another with over $90\%$ accuracy. The proposed method could therefore be a building block to improve current AI systems for better understanding of the environment
    
[^72]: 优化卷积神经网络架构

    Optimizing Convolutional Neural Network Architecture. (arXiv:2401.01361v1 [cs.CV])

    [http://arxiv.org/abs/2401.01361](http://arxiv.org/abs/2401.01361)

    本文提出了一种优化卷积神经网络架构的方法，通过剪枝和知识蒸馏来建立卷积层的重要性。通过实证研究，我们证明了该方法在各种数据集和CNN架构上的性能优于其他先进方法。

    

    卷积神经网络（CNN）被广泛应用于应对诸如语音识别、自然语言处理或计算机视觉等具有挑战性的任务。随着CNN架构变得越来越大和复杂，其计算需求增加，增加了能源成本，并在资源受限设备上部署变得困难。在本文中，我们提出了一种基于剪枝和知识蒸馏的优化卷积神经网络架构（OCNNA），旨在建立卷积层的重要性。我们通过全面的实证研究评估了该提议，包括最好的已知数据集（CIFAR-10，CIFAR-100和Imagenet）和CNN架构（VGG-16，ResNet-50，DenseNet-40和MobileNet），通过精度下降和剩余参数比率作为客观指标来比较OCNNA与其他先进方法的性能。我们的方法与20多个卷积深度网络架构相比较。

    Convolutional Neural Networks (CNN) are widely used to face challenging tasks like speech recognition, natural language processing or computer vision. As CNN architectures get larger and more complex, their computational requirements increase, incurring significant energetic costs and challenging their deployment on resource-restricted devices. In this paper, we propose Optimizing Convolutional Neural Network Architecture (OCNNA), a novel CNN optimization and construction method based on pruning and knowledge distillation designed to establish the importance of convolutional layers. The proposal has been evaluated though a thorough empirical study including the best known datasets (CIFAR-10, CIFAR-100 and Imagenet) and CNN architectures (VGG-16, ResNet-50, DenseNet-40 and MobileNet), setting Accuracy Drop and Remaining Parameters Ratio as objective metrics to compare the performance of OCNNA against the other state-of-art approaches. Our method has been compared with more than 20 convo
    
[^73]: IoTGeM: 用于基于行为的物联网攻击检测的通用模型

    IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])

    [http://arxiv.org/abs/2401.01343](http://arxiv.org/abs/2401.01343)

    本研究提出了一种通用模型，用于基于行为的物联网攻击检测。该模型通过改进滚动窗口特征提取、引入多步骤特征选择、使用隔离的训练和测试数据集以及使用可解释的人工智能技术来提高检测和性能，并且经过严格评估。

    

    过去关于物联网设备网络的基于行为的攻击检测研究，所得到的机器学习模型的适应能力有限，并且往往没有得到证明。本文提出了一种针对建模物联网网络攻击的方法，着重于通用性，同时也能提高检测和性能。首先，我们提出了一种改进的滚动窗口特征提取方法，并引入了一个多步骤特征选择过程来减少过拟合。其次，我们使用隔离的训练和测试数据集来构建和测试模型，从而避免了先前模型在通用性方面的常见数据泄漏问题。第三，我们使用多样化的机器学习模型、评估指标和数据集对我们的方法进行了严格评估。最后，我们使用可解释的人工智能技术来增加模型的可信度，从而能够识别出支撑攻击准确检测的特征。

    Previous research on behaviour-based attack detection on networks of IoT devices has resulted in machine learning models whose ability to adapt to unseen data is limited, and often not demonstrated. In this paper we present an approach for modelling IoT network attacks that focuses on generalizability, yet also leads to better detection and performance. First, we present an improved rolling window approach for feature extraction, and introduce a multi-step feature selection process that reduces overfitting. Second, we build and test models using isolated train and test datasets, thereby avoiding common data leaks that have limited the generalizability of previous models. Third, we rigorously evaluate our methodology using a diverse portfolio of machine learning models, evaluation metrics and datasets. Finally, we build confidence in the models by using explainable AI techniques, allowing us to identify the features that underlie accurate detection of attacks.
    
[^74]: 保护智能基础设施和数字产业：利用人工智能（AI）加强恶意软件和入侵检测

    Securing the Digital World: Protecting smart infrastructures and digital industries with Artificial Intelligence (AI)-enabled malware and intrusion detection. (arXiv:2401.01342v1 [cs.CR])

    [http://arxiv.org/abs/2401.01342](http://arxiv.org/abs/2401.01342)

    本论文探究了基于人工智能的网络威胁检测，重点是评估基于机器学习的分类器和集成，用于异常恶意软件检测和网络入侵检测，并讨论了如何将这些模型整合到网络安全和移动设备环境中。

    

    最近几十年的技术进步以人工智能（AI）和机器学习（ML）等现代技术为动力。世界变得比以往任何时候都更数字化连接，但我们面临着严峻的挑战。其中最重要的挑战之一是网络犯罪，它已成为对政府、企业和社会的全球威胁。数字技术的普及性结合不断变化的技术基础为网络犯罪分子创造了一个复杂而强大的游乐场，这引发了对基于机器学习和深度学习的智能威胁检测系统的需求增加。本文研究了基于AI的网络威胁检测，以保护我们现代数字生态系统。主要关注评估基于ML的分类器和集成，用于异常恶意软件检测和网络入侵检测，并探讨在网络安全和移动设备环境下如何集成这些模型。

    The last decades have been characterized by unprecedented technological advances, many of them powered by modern technologies such as Artificial Intelligence (AI) and Machine Learning (ML). The world has become more digitally connected than ever, but we face major challenges. One of the most significant is cybercrime, which has emerged as a global threat to governments, businesses, and civil societies. The pervasiveness of digital technologies combined with a constantly shifting technological foundation has created a complex and powerful playground for cybercriminals, which triggered a surge in demand for intelligent threat detection systems based on machine and deep learning. This paper investigates AI-based cyber threat detection to protect our modern digital ecosystems. The primary focus is on evaluating ML-based classifiers and ensembles for anomaly-based malware detection and network intrusion detection and how to integrate those models in the context of network security, mobile s
    
[^75]: 自然语言处理和大型语言模型公平性认证

    Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])

    [http://arxiv.org/abs/2401.01262](http://arxiv.org/abs/2401.01262)

    这项研究旨在为自然语言处理领域开发公平性认证方法。通过综述大量文献和专家访谈，我们提出了六个公平性标准，为操作化和测试过程提供了基础。

    

    自然语言处理（NLP）在我们的日常生活中扮演着重要角色，特别是由于大型语言模型（LLM）的巨大进展。然而，NLP在招聘等公平关键应用场景中存在许多问题，例如作为专家系统或基于LLM的教育导师。由于NLP基于人类语言，可能会导致潜在的有害偏见渗入NLP系统，产生不公平的结果，歧视少数群体或引发法律问题。因此，开展NLP方法的公平性认证非常重要。我们采用定性研究方法，对算法公平性的大量文献进行了综述，并与该领域的多位专家进行了半结构化的专家访谈。我们系统地提出了NLP的六个公平性标准，并进一步细化为18个子类别。我们的标准为实施和测试过程提供了基础。

    Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
    
[^76]: 在晶体材料研究中，将协方差和表达能力融合为深度哈密顿回归：一种混合级联回归框架

    Harmonizing Covariance and Expressiveness for Deep Hamiltonian Regression in Crystalline Material Research: a Hybrid Cascaded Regression Framework. (arXiv:2401.00744v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2401.00744](http://arxiv.org/abs/2401.00744)

    在深度哈密顿回归中，实现协方差和网络表达能力之间的平衡一直是一个挑战。本文提出了一个混合级联回归框架，在第一阶段通过协变神经网络建模并产生协变特征和基线预测，辅助第二阶段学习协方差。同时，第二阶段使用非线性图形Transformer网络进行结构建模，提高了哈密顿预测的表达能力。

    

    在材料研究中，深度学习用于哈密顿回归量子系统需要满足协方差定律，其中实现SO(3)等变性而不损失网络的表达能力是一个难以解决的挑战，因为非线性映射的理论等变性保证受限。为了解决协方差-表达能力困境，我们提出了一种混合框架，分为两个级联回归阶段。第一阶段使用一个理论上保证的协变神经网络来建模三维原子系统的对称性，产生理论上的协变特征和基线哈密顿预测，帮助第二阶段学习协变性。同时，第二阶段使用我们提出的非线性三维图形Transformer网络来进行三维原子系统的结构建模，将第一阶段的输出精细化为具有更好表达能力的哈密顿预测。通过理论上的协变性和更好的表达能力，实现了对哈密顿回归的改进。

    Deep learning for Hamiltonian regression of quantum systems in material research necessitates satisfying the covariance laws, among which achieving SO(3)-equivariance without sacrificing the expressiveness of networks remains an elusive challenge due to the restriction to non-linear mappings on guaranteeing theoretical equivariance. To alleviate the covariance-expressiveness dilemma, we propose a hybrid framework with two cascaded regression stages. The first stage, with a theoretically-guaranteed covariant neural network modeling symmetry properties of 3D atom systems, yields theoretically covariant features and baseline Hamiltonian predictions, assisting the second stage in learning covariance. Meanwhile, the second stage, powered by a non-linear 3D graph Transformer network we propose for structural modeling of 3D atomic systems, refines the first stage's output as a fine-grained prediction of Hamiltonians with better expressiveness capability. The combination of a theoretically cov
    
[^77]: 多类别量化的核密度估计

    Kernel Density Estimation for Multiclass Quantification. (arXiv:2401.00490v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.00490](http://arxiv.org/abs/2401.00490)

    本文提出了一种用于多类别量化的核密度估计方法，相比于目前的方法，它能更好地模拟数据中的类间信息。

    

    一些学科，如社会科学、流行病学、情感分析或市场研究，关注的是了解人群中各类别的分布情况，而不是个体的标签。量化是一种监督式机器学习任务，旨在获得准确的类别频率预测器，并在标签偏移存在的情况下进行。分布匹配（DM）方法是迄今文献中提出的量化方法中最重要的家族之一。目前的DM方法通过后验概率的直方图对涉及的人群进行建模。在本文中，我们认为将这些方法应用于多类别设置是次优的，因为直方图变成了类别特定的，无法模拟数据中可能存在的类间信息。我们提出了一种基于多元密度的新的表示机制，通过核密度估计来建模。

    Several disciplines, like the social sciences, epidemiology, sentiment analysis, or market research, are interested in knowing the distribution of the classes in a population rather than the individual labels of the members thereof. Quantification is the supervised machine learning task concerned with obtaining accurate predictors of class prevalence, and to do so particularly in the presence of label shift. The distribution-matching (DM) approaches represent one of the most important families among the quantification methods that have been proposed in the literature so far. Current DM approaches model the involved populations by means of histograms of posterior probabilities. In this paper, we argue that their application to the multiclass setting is suboptimal since the histograms become class-specific, thus missing the opportunity to model inter-class information that may exist in the data. We propose a new representation mechanism based on multivariate densities that we model via k
    
[^78]: 使用感知损失的扩散模型

    Diffusion Model with Perceptual Loss. (arXiv:2401.00110v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2401.00110](http://arxiv.org/abs/2401.00110)

    本研究介绍了一种使用感知损失的扩散模型，通过无分类器指导实现了生成更真实样本的目的。

    

    使用均方误差损失训练的扩散模型倾向于生成不真实的样本。目前的最先进模型依靠无分类器指导来改善样本质量，然而其惊人的效果尚未完全理解。本文中，我们展示了无分类器指导的有效性在一定程度上源自其作为一种隐式感知指导的形式。因此，我们可以直接在扩散训练中加入感知损失来提高样本质量。由于扩散训练中使用的分数匹配目标与无监督训练感知网络时使用的去噪自动编码器目标非常相似，因此扩散模型本身就是一个感知网络，并可以用于生成有意义的感知损失。我们提出了一种新颖的自感知目标，其结果是扩散模型能够生成更真实的样本。对于条件生成，我们的方法仅改善样本质量，而不与条件绑定。

    Diffusion models trained with mean squared error loss tend to generate unrealistic samples. Current state-of-the-art models rely on classifier-free guidance to improve sample quality, yet its surprising effectiveness is not fully understood. In this paper, We show that the effectiveness of classifier-free guidance partly originates from it being a form of implicit perceptual guidance. As a result, we can directly incorporate perceptual loss in diffusion training to improve sample quality. Since the score matching objective used in diffusion training strongly resembles the denoising autoencoder objective used in unsupervised training of perceptual networks, the diffusion model itself is a perceptual network and can be used to generate meaningful perceptual loss. We propose a novel self-perceptual objective that results in diffusion models capable of generating more realistic samples. For conditional generation, our method only improves sample quality without entanglement with the condit
    
[^79]: 是否所有未见数据都是OoD数据？

    Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.16243](http://arxiv.org/abs/2312.16243)

    该论文研究了未见数据的分布在泛化中的挑战，并提出了重新定义OoD数据以及新的泛化上界，保证了对未见数据的模型有效性。

    

    处理未见数据的分布一直被视为是OoD（out-of-distribution），使其泛化成为一个重要挑战。很多证据表明，训练数据的规模增加可以单调地降低测试数据的泛化误差。然而，从其他观察和分析来看，这并不总是成立。特别是当训练数据包含多个来源领域，并且测试数据包含分布漂移时，不是所有的测试数据的泛化误差都会随着训练数据规模的增加而单调减小。在线性设置下，我们正式研究了这种非单调现象，并通过在不同视觉基准上进行经验证实。鉴于这些结果，我们重新定义了OoD数据，将其视为训练域的凸包之外的数据，并基于这个新定义证明了一个新的泛化上界。它意味着对于在训练阶段没有见过的数据，经过充分训练的模型的有效性可以得到保证。

    Distributions of unseen data have been all treated as out-of-distribution (OOD), making their generalization a significant challenge. Much evidence suggests that the size increase of training data can monotonically decrease generalization errors in test data. However, this is not true from other observations and analysis. In particular, when the training data have multiple source domains and the test data contain distribution drifts, then not all generalization errors on the test data decrease monotonically with the increasing size of training data. Such a non-decreasing phenomenon is formally investigated under a linear setting with empirical verification across varying visual benchmarks. Motivated by these results, we redefine the OOD data as a type of data outside the convex hull of the training domains and prove a new generalization bound based on this new definition. It implies that the effectiveness of a well-trained model can be guaranteed for the unseen data that is within the 
    
[^80]: M3D：通过最小化最大均值差异来进行数据集压缩

    M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy. (arXiv:2312.15927v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.15927](http://arxiv.org/abs/2312.15927)

    该论文提出了一种名为M3D的新型基于分布匹配的数据集压缩方法，通过最小化最大均值差异来提高压缩效率，克服了优化过程在实际和较大数据集上的应用难题。

    

    训练最先进的深度模型通常需要大量的数据，导致训练和存储成本高昂。为了解决这些挑战，提出了数据集压缩方法，通过学习一个小的合成集合来保留原始大规模数据集的关键信息。目前，以优化为导向的方法是数据集压缩领域中实现最先进结果的主要方法。然而，双层优化过程阻碍了这些方法在实际和较大的数据集上的实际应用。为了提高压缩效率，先前的工作提出了分布匹配（DM）作为替代方法，显著减少了压缩成本。然而，由于专注于对齐分布的一阶矩，目前的基于DM的方法与以优化为导向的方法相比，结果不太可比。在本文中，我们介绍了一种名为M3D的新型基于DM的数据集压缩方法。

    Training state-of-the-art (SOTA) deep models often requires extensive data, resulting in substantial training and storage costs. To address these challenges, dataset condensation has been developed to learn a small synthetic set that preserves essential information from the original large-scale dataset. Nowadays, optimization-oriented methods have been the primary method in the field of dataset condensation for achieving SOTA results. However, the bi-level optimization process hinders the practical application of such methods to realistic and larger datasets. To enhance condensation efficiency, previous works proposed Distribution-Matching (DM) as an alternative, which significantly reduces the condensation cost. Nonetheless, current DM-based methods have yielded less comparable results to optimization-oriented methods due to their focus on aligning only the first moment of the distributions. In this paper, we present a novel DM-based method named M3D for dataset condensation by Minimi
    
[^81]: 通过论据集成实现模型多样性下的追索（技术报告）

    Recourse under Model Multiplicity via Argumentative Ensembling (Technical Report). (arXiv:2312.15097v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.15097](http://arxiv.org/abs/2312.15097)

    本研究提出了一种名为“追索感知的集成”的方法，通过使用计算论证方法来解决模型多样性下提供反事实解释的挑战性。该方法保证了在模型多样性情况下CEs的鲁棒性，并可以适应用户的个性化偏好。

    

    模型多样性（MM）是指在解决同一预测任务时可以训练出多个性能相同的机器学习模型。最近的研究表明，在MM下获得的模型对于相同的输入可能产生不一致的预测。当出现这种情况时，为受到模型预测影响的个体提供反事实解释（CEs）变得具有挑战性。在本文中，我们正式定义了这个问题，称之为追索感知的集成，并确定了解决这个问题的方法应该满足的几个理想属性。我们发现现有的集成方法在不同方式下自然扩展以提供CEs时未能满足这些属性。然后，我们引入了论证集成，采用计算论证方法来确保CEs对MM的鲁棒性，并适应可定制的用户偏好。我们从理论和实验上证明了论证集成满足了这些属性。

    Model Multiplicity (MM) arises when multiple, equally performing machine learning models can be trained to solve the same prediction task. Recent studies show that models obtained under MM may produce inconsistent predictions for the same input. When this occurs, it becomes challenging to provide counterfactual explanations (CEs), a common means for offering recourse recommendations to individuals negatively affected by models' predictions. In this paper, we formalise this problem, which we name recourse-aware ensembling, and identify several desirable properties which methods for solving it should satisfy. We show that existing ensembling methods, naturally extended in different ways to provide CEs, fail to satisfy these properties. We then introduce argumentative ensembling, deploying computational argumentation to guarantee robustness of CEs to MM, while also accommodating customisable user preferences. We show theoretically and experimentally that argumentative ensembling satisfies
    
[^82]: SCUNet++: Swin-UNet和CNN瓶颈混合架构，具有多重融合稠密跳跃连接用于肺栓塞CT图像分割

    SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image Segmentation. (arXiv:2312.14705v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2312.14705](http://arxiv.org/abs/2312.14705)

    这项研究提出了一种名为SCUNet++的自动肺栓塞分割方法，利用Swin Transformer作为编码器，并通过多重融合稠密跳跃连接在编码器和解码器之间实现特征融合，以解决传统方法无法充分考虑特征分层结构和局部、全局空间特征的问题。

    

    肺栓塞（PE）是一种常见的肺部疾病，严重时可导致右心室肥厚和衰竭，在严重程度上仅次于心肌梗死和猝死。肺动脉CT血管造影（CTPA）是一种广泛使用的PE诊断方法。然而，由于影像技术的局限性，PE的检测在临床实践中存在挑战。CTPA可能产生与PE类似的噪声，使得确认其存在变得耗时且容易出现过度诊断。然而，传统的PE分割方法无法充分考虑特征的分层结构、PE CT图像的局部和全局空间特征。在本文中，我们提出了一种自动的PE分割方法，称为SCUNet++（Swin Conv UNet++）。该方法在编码器和解码器之间引入了多重融合稠密跳跃连接，利用Swin Transformer作为编码器。并在解码器子网络中融合不同尺度的特征，以弥补特征的缺失。

    Pulmonary embolism (PE) is a prevalent lung disease that can lead to right ventricular hypertrophy and failure in severe cases, ranking second in severity only to myocardial infarction and sudden death. Pulmonary artery CT angiography (CTPA) is a widely used diagnostic method for PE. However, PE detection presents challenges in clinical practice due to limitations in imaging technology. CTPA can produce noises similar to PE, making confirmation of its presence time-consuming and prone to overdiagnosis. Nevertheless, the traditional segmentation method of PE can not fully consider the hierarchical structure of features, local and global spatial features of PE CT images. In this paper, we propose an automatic PE segmentation method called SCUNet++ (Swin Conv UNet++). This method incorporates multiple fusion dense skip connections between the encoder and decoder, utilizing the Swin Transformer as the encoder. And fuses features of different scales in the decoder subnetwork to compensate f
    
[^83]: 通过后门操纵轨迹预测

    Manipulating Trajectory Prediction with Backdoors. (arXiv:2312.13863v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13863](http://arxiv.org/abs/2312.13863)

    本文研究了在轨迹预测中经常被忽视的安全威胁——后门攻击。我们发现通过一些触发器的操纵，可以使现有的轨迹预测模型在受到特定条件刺激时预测错误，揭示了模型的局限性。

    

    自动驾驶车辆需要预测周围车辆的轨迹，以确保在不确定和复杂的交通情况下进行安全操作。随着公司在现实世界中越来越多地应用轨迹预测，安全性变得越来越重要。本文着重研究后门——一种在其他领域被认可的安全威胁，但在轨迹预测中迄今被忽视。为此，我们描述并研究了四个可能影响轨迹预测的触发器。然后，我们展示了这些触发器（例如，刹车车辆），当与训练过程中的期望输出（例如，转弯）相关时，会导致最先进的轨迹预测模型的期望输出。换句话说，该模型在正常情况下表现良好，但容易受到后门攻击的影响。即使触发操纵是由目标车辆后面的非正常代理执行的，这种情况也适用。作为副作用，我们的分析揭示了轨迹预测模型中的有趣局限性。

    Autonomous vehicles ought to predict the surrounding agents' trajectories to allow safe maneuvers in uncertain and complex traffic situations. As companies increasingly apply trajectory prediction in the real world, security becomes a relevant concern. In this paper, we focus on backdoors - a security threat acknowledged in other fields but so far overlooked for trajectory prediction. To this end, we describe and investigate four triggers that could affect trajectory prediction. We then show that these triggers (for example, a braking vehicle), when correlated with a desired output (for example, a curve) during training, cause the desired output of a state-of-the-art trajectory prediction model. In other words, the model has good benign performance but is vulnerable to backdoors. This is the case even if the trigger maneuver is performed by a non-casual agent behind the target vehicle. As a side-effect, our analysis reveals interesting limitations within trajectory prediction models. F
    
[^84]: 通过动态三维高斯和组合扩散模型实现文本到4D对象的对齐

    Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models. (arXiv:2312.13763v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.13763](http://arxiv.org/abs/2312.13763)

    本研究针对文本到4D对象合成问题，提出了一种创新的组合生成方法。通过利用动态三维高斯簇和组合扩散模型，我们同时实现了时间一致性、高质量的视觉效果和逼真的几何特征。此外，我们还引入了一种新的规范方法来稳定优化过程并增强运动效果。

    

    文本引导的扩散模型已经在图像和视频生成领域取得了革命性的进展，并且在基于优化的3D对象合成方面也取得了成功。在这里，我们将重点放在了尚未充分探索的文本到4D对象的设置上，并使用分数蒸馏方法合成具有额外时间维度的动态的、动画的3D对象。与以往的工作相比，我们采用了一种新颖的组合生成方法，并将文本到图像、文本到视频和3D感知多视图扩散模型结合起来，在4D对象优化过程中提供反馈，从而同时实现时间一致性、高质量的视觉效果和逼真的几何特征。我们的方法称为Align Your Gaussians（AYG），利用了带有形变场的动态三维高斯簇进行4D表示。AYG的关键在于一种用来规范移动的三维高斯分布的新方法，从而稳定优化过程并引导运动。我们还提出了一种运动放大机制，使得模型能够更好地捕捉到运动。

    Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as w
    
[^85]: 并非所有步骤都相等：进展扩散模型的高效生成

    Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.13307](http://arxiv.org/abs/2312.13307)

    提出了一种步骤自适应训练的两阶段策略，解决了传统扩散模型中训练过程中的冲突问题，将模型大小调整与噪声预测难度相匹配，提高了生成效果。

    

    扩散模型在多种生成任务中展示了出色的效能，具有去噪模型的预测能力。目前，这些模型在所有时间步上都采用统一的去噪方法。然而，每个时间步的噪声潜在变化导致了训练中的冲突，限制了扩散模型的潜力。为了解决这个挑战，我们提出了一种新的两阶段训练策略，称为步骤自适应训练。在初始阶段，训练一个基础的去噪模型来包括所有的时间步。随后，我们将时间步分为不同的组，对每个组内的模型进行微调，以达到专门的去噪能力。我们认识到，不同时间步的噪声预测困难程度是不同的，所以我们引入了多样的模型大小要求。我们通过估计每个时间步的信噪比来动态调整模型大小，以进行微调之前。此调整简化了模型的训练流程并提高了生成效果。

    Diffusion models have demonstrated remarkable efficacy in various generative tasks with the predictive prowess of denoising model. Currently, these models employ a uniform denoising approach across all timesteps. However, the inherent variations in noisy latents at each timestep lead to conflicts during training, constraining the potential of diffusion models. To address this challenge, we propose a novel two-stage training strategy termed Step-Adaptive Training. In the initial stage, a base denoising model is trained to encompass all timesteps. Subsequently, we partition the timesteps into distinct groups, fine-tuning the model within each group to achieve specialized denoising capabilities. Recognizing that the difficulties of predicting noise at different timesteps vary, we introduce a diverse model size requirement. We dynamically adjust the model size for each timestep by estimating task difficulty based on its signal-to-noise ratio before fine-tuning. This adjustment is facilitat
    
[^86]: 神经网络分析中的拓扑数据分析: 一份综述

    Topological Data Analysis for Neural Network Analysis: A Comprehensive Survey. (arXiv:2312.05840v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05840](http://arxiv.org/abs/2312.05840)

    这篇综述全面探索了拓扑数据分析在神经网络分析中的应用。它讨论了通过拓扑信息来分析神经网络的特性，并探索了在深度学习中的实际应用领域。研究将工作分为四个领域，涵盖了神经网络结构、决策区域、内部表示和训练动力学等方面。

    

    本综述全面探索了拓扑数据分析（TDA）在神经网络分析中的应用。通过使用持久同调和Mapper等TDA工具，我们深入研究神经网络及其数据集的复杂结构和行为。我们讨论了通过TDA从数据和神经网络中获取拓扑信息的不同策略。此外，我们还回顾了如何利用拓扑信息来分析神经网络的特性，例如它们的泛化能力或表达能力。我们探索了深度学习的实际应用，特别关注对抗性检测和模型选择等领域。我们将研究工作分为四个广泛的领域: 1. 神经网络架构的描述; 2. 决策区域和边界的分析; 3. 内部表示、激活和参数的研究; 4. 训练动力学和损失函数的探索。

    This survey provides a comprehensive exploration of applications of Topological Data Analysis (TDA) within neural network analysis. Using TDA tools such as persistent homology and Mapper, we delve into the intricate structures and behaviors of neural networks and their datasets. We discuss different strategies to obtain topological information from data and neural networks by means of TDA. Additionally, we review how topological information can be leveraged to analyze properties of neural networks, such as their generalization capacity or expressivity. We explore practical implications of deep learning, specifically focusing on areas like adversarial detection and model selection. Our survey organizes the examined works into four broad domains: 1. Characterization of neural network architectures; 2. Analysis of decision regions and boundaries; 3. Study of internal representations, activations, and parameters; 4. Exploration of training dynamics and loss functions. Within each category,
    
[^87]: DISPLACE Challenge 2023 -- 会话环境中的发言人和语言辨识问题的总结

    Summary of the DISPLACE Challenge 2023 -- DIarization of SPeaker and LAnguage in Conversational Environments. (arXiv:2311.12564v3 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2311.12564](http://arxiv.org/abs/2311.12564)

    DISPLACE Challenge 2023是一个关于在多语言对话环境中进行发言人和语言辨识的挑战，通过评估和基准测试不同技术的表现来提高这种辨识的效率和准确性。

    

    在多语言社会中，当一个小地理区域中同时使用多种语言进行非正式对话时，现有的语音技术可能无法高效地从这些对话中提取信息，因为语音数据包含多种语言和发言人的丰富多样性。DISPLACE（会话环境中的发言人和语言辨识问题）挑战通过一个开放的评估和基准测试来评估多发言人和多语言的辨识技术在这种具有挑战性的情况下的表现。该挑战包括两个任务：任务1关注多语言环境中的发言人辨识，任务2则关注多发言人情境中的语言辨识。这两个任务使用相同的音频数据进行评估。为了促进评估，还创建了一个真实世界的数据集，包含多语言、多发言人的远场对话语音。此外，还创建了基准系统用于对挑战任务进行衡量。

    In multi-lingual societies, where multiple languages are spoken in a small geographic vicinity, informal conversations often involve mix of languages. Existing speech technologies may be inefficient in extracting information from such conversations, where the speech data is rich in diversity with multiple languages and speakers. The DISPLACE (DIarization of SPeaker and LAnguage in Conversational Environments) challenge constitutes an open-call for evaluating and bench-marking the speaker and language diarization technologies on this challenging condition. The challenge entailed two tracks: Track-1 focused on speaker diarization (SD) in multilingual situations while, Track-2 addressed the language diarization (LD) in a multi-speaker scenario. Both the tracks were evaluated using the same underlying audio data. To facilitate this evaluation, a real-world dataset featuring multilingual, multi-speaker conversational far-field speech was recorded and distributed. Furthermore, a baseline sys
    
[^88]: Jina Embeddings 2: 面向长篇文档的8192-Token通用文本嵌入模型

    Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])

    [http://arxiv.org/abs/2310.19923](http://arxiv.org/abs/2310.19923)

    Jina Embeddings 2是一个能够处理长篇文档的文本嵌入模型，突破了传统512个标记限制，提供了高达8192个标记的容量。

    

    文本嵌入模型已经成为将句子转化为固定大小特征向量的强大工具，这些向量包含了语义信息。尽管这些模型对于信息检索、语义聚类和文本重排序等任务至关重要，但大多数现有的开源模型，尤其是基于BERT等架构构建的模型，难以表示长篇文档，并且常常会进行截断。为了缓解这个挑战，一种常见的方法是将文档分割成更小的段落进行嵌入。然而，这种策略会导致更大的向量集合，进而增加内存消耗，并且在向量搜索时会出现计算密集和延迟升高的问题。为了解决这些挑战，我们介绍了Jina Embeddings 2，这是一个开源的文本嵌入模型，可以容纳高达8192个标记。该模型旨在突破传统的512个标记限制，能够灵活处理长篇文档。

    Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
    
[^89]: 使用图神经网络预测岩石的有效弹性模量

    Prediction of Effective Elastic Moduli of Rocks using Graph Neural Networks. (arXiv:2310.19274v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19274](http://arxiv.org/abs/2310.19274)

    本研究提出了一种基于图神经网络的方法，使用数字CT扫描图像预测岩石的有效弹性模量。通过将岩石图像转换为图数据集并经过训练，这种方法在预测弹性模量方面表现出良好的效果，并且相比于CNN具有更高的预测性能和更低的内存需求。

    

    本研究提出了一种基于图神经网络（GNN）的方法，通过数字CT扫描图像来预测岩石的有效弹性模量。我们使用Mapper算法将三维数字岩石图像转换为图数据集，包含了必要的几何信息。经过训练后，这些图在预测弹性模量方面表现出良好的效果。我们的GNN模型在不同的子立方体尺寸所得到的不同图尺寸上表现出稳健的预测能力。它不仅在测试数据集上表现良好，还在未见过的岩石和未探索的子立方体尺寸上保持高精度的预测。与卷积神经网络（CNN）的比较分析表明，GNN在预测未见过的岩石属性方面具有更优越的性能。此外，微结构的图表示显著降低了GPU内存需求（与CNN的网格表示相比），使得批次大小选择更加灵活。本研究表明...

    This study presents a Graph Neural Networks (GNNs)-based approach for predicting the effective elastic moduli of rocks from their digital CT-scan images. We use the Mapper algorithm to transform 3D digital rock images into graph datasets, encapsulating essential geometrical information. These graphs, after training, prove effective in predicting elastic moduli. Our GNN model shows robust predictive capabilities across various graph sizes derived from various subcube dimensions. Not only does it perform well on the test dataset, but it also maintains high prediction accuracy for unseen rocks and unexplored subcube sizes. Comparative analysis with Convolutional Neural Networks (CNNs) reveals the superior performance of GNNs in predicting unseen rock properties. Moreover, the graph representation of microstructures significantly reduces GPU memory requirements (compared to the grid representation for CNNs), enabling greater flexibility in the batch size selection. This work demonstrates t
    
[^90]: 理解RLHF对LLM泛化和多样性的影响

    Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06452](http://arxiv.org/abs/2310.06452)

    本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。

    

    在最广泛使用的AI模型中，如OpenAI的ChatGPT或Anthropic的Claude，使用强化学习从人类反馈中调整的大型语言模型（LLM）。尽管在这些方法的开发方面有大量的研究，但我们对RLHF过程中每个阶段的利与弊的理解仍然有限。为了填补这一空白，我们对每个阶段（即监督微调（SFT），奖励建模和RLHF）如何影响两个关键属性进行了全面分析：超出分布的泛化和输出多样性。在这些模型被广泛应用于真实世界中的各种情景的背景下，超出分布的泛化非常重要，而输出多样性指的是模型生成各种不同输出的能力，对于各种用例来说都非常重要。我们在摘要和指令遵循任务中对两个基本模型进行了分析，后者非常相关。

    Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
    
[^91]: 魔法词是什么？LLM提示的控制理论研究

    What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])

    [http://arxiv.org/abs/2310.04444](http://arxiv.org/abs/2310.04444)

    本论文将提示工程形式化为LLM上的最优控制问题，研究了给定token序列时是否存在一种最优提示能够准确预测最终的token，并提出了控制理论中的指标来描述LLM的可操纵性。

    

    提示工程在LLM的部署中是有效和重要的，但在数学上理解不足。在这里，我们将提示工程形式化为LLM上的最优控制问题，其中提示被认为是调节LLM输出分布的控制变量。在这个框架内，我们提出一个简单的问题：给定一个token序列，是否总存在一个我们可以添加的提示，使得LLM能够准确预测最终的token？我们将这样的最优提示称为魔法词，因为添加提示会导致LLM输出正确的答案。如果存在魔法词，我们能否找到它们？如果可以，它们的特性是什么？我们提供了将控制理论应用于自注意力头的分析分析，证明了其权重矩阵的奇异值函数为可控制性的上界。我们借鉴控制理论来提出了一种叫做$k-\epsilon$可控制性的指标，用于描述LLM的可操纵性。

    Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
    
[^92]: 动态关系注意力图神经网络用于欺诈检测

    Dynamic Relation-Attentive Graph Neural Networks for Fraud Detection. (arXiv:2310.04171v1 [cs.LG])

    [http://arxiv.org/abs/2310.04171](http://arxiv.org/abs/2310.04171)

    本研究针对欺诈检测问题，通过动态关系注意聚合机制，提出了一种基于图神经网络（GNN）的方法。该方法学习每个关系的节点表示，并利用可学习的注意函数进行节点表示的聚合。通过结合不同层次的节点表示，考虑目标节点的局部和全局结构，以提高欺诈检测性能。通过使用动态图注意力，可以自适应地计算关系之间的重要程度。

    

    欺诈检测旨在发现欺诈者通过留下假评论或进行异常交易欺骗其他用户。基于图的欺诈检测方法将这个任务视为一个包含两个类别（欺诈或正常）的分类问题。我们通过提出一种动态关系注意聚合机制，利用图神经网络（GNN）来解决这个问题。基于实际世界图表中包含不同类型的关系的观察，我们建议学习每个关系的节点表示，并使用可学习的注意函数聚合节点表示，该函数为每个关系分配不同的注意系数。此外，我们结合不同层次的节点表示，以考虑目标节点的局部和全局结构，这有助于提高在具有异质性的图上进行欺诈检测的性能。通过在所有聚合过程中采用动态图注意力，我们的方法可以自适应地计算关系之间的重要程度。

    Fraud detection aims to discover fraudsters deceiving other users by, for example, leaving fake reviews or making abnormal transactions. Graph-based fraud detection methods consider this task as a classification problem with two classes: frauds or normal. We address this problem using Graph Neural Networks (GNNs) by proposing a dynamic relation-attentive aggregation mechanism. Based on the observation that many real-world graphs include different types of relations, we propose to learn a node representation per relation and aggregate the node representations using a learnable attention function that assigns a different attention coefficient to each relation. Furthermore, we combine the node representations from different layers to consider both the local and global structures of a target node, which is beneficial to improving the performance of fraud detection on graphs with heterophily. By employing dynamic graph attention in all the aggregation processes, our method adaptively comput
    
[^93]: 关于尖锐意识最小化的记忆和隐私风险研究

    On Memorization and Privacy Risks of Sharpness Aware Minimization. (arXiv:2310.00488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00488](http://arxiv.org/abs/2310.00488)

    本研究通过对过度参数化模型中的数据记忆的剖析，揭示了尖锐意识最小化算法在非典型数据点上实现的泛化收益。同时，也发现了与此算法相关的更高隐私风险，并提出了缓解策略，以达到更理想的准确度与隐私权衡。

    

    在许多最近的研究中，设计寻求神经网络损失优化中更平坦的极值的算法成为焦点，因为有经验证据表明这会在许多数据集上导致更好的泛化性能。在这项工作中，我们通过过度参数化模型中的数据记忆视角来剖析这些性能收益。我们定义了一个新的度量指标，帮助我们确定相对于普通SGD，寻求更平坦极值的算法在哪些数据点上表现更好。我们发现，尖锐意识最小化（SAM）所实现的泛化收益在非典型数据点上特别显著，这需要记忆。这一认识帮助我们揭示与SAM相关的更高的隐私风险，并通过详尽的实证评估进行验证。最后，我们提出缓解策略，以实现更理想的准确度与隐私权衡。

    In many recent works, there is an increased focus on designing algorithms that seek flatter optima for neural network loss optimization as there is empirical evidence that it leads to better generalization performance in many datasets. In this work, we dissect these performance gains through the lens of data memorization in overparameterized models. We define a new metric that helps us identify which data points specifically do algorithms seeking flatter optima do better when compared to vanilla SGD. We find that the generalization gains achieved by Sharpness Aware Minimization (SAM) are particularly pronounced for atypical data points, which necessitate memorization. This insight helps us unearth higher privacy risks associated with SAM, which we verify through exhaustive empirical evaluations. Finally, we propose mitigation strategies to achieve a more desirable accuracy vs privacy tradeoff.
    
[^94]: 低成本黑盒优化算法在BBOB和OpenAI Gym上的评估

    Low-budget Black-box Optimization Algorithms Evaluated on BBOB and OpenAI Gym. (arXiv:2310.00077v1 [cs.LG])

    [http://arxiv.org/abs/2310.00077](http://arxiv.org/abs/2310.00077)

    这项研究旨在探讨机器学习和黑盒优化之间的交叉应用潜力，并通过比较实验评估了低成本黑盒优化算法在不同领域的效果。

    

    机器学习在计算机科学的各个领域中的广泛应用，包括黑盒优化（BBO）。近期的研究特别关注贝叶斯优化（BO）。基于BO的算法在机器学习社区中非常受欢迎，因为它们用于超参数优化和算法配置等。然而，随着问题维度和评估预算的增加，它们的效率会降低。与此同时，无导数优化方法在优化社区中独立发展。因此，我们迫切需要了解是否可以在机器学习和BBO之间进行交叉受精，即机器学习中广泛使用的算法在BBO中是否同样有效，反之亦然。比较实验通常涉及相对较小的基准和实验设置中的可见问题，如基线初始化不良、过度拟合等。

    The growing ubiquity of machine learning (ML) has led it to enter various areas of computer science, including black-box optimization (BBO). Recent research is particularly concerned with Bayesian optimization (BO). BO-based algorithms are popular in the ML community, as they are used for hyperparameter optimization and more generally for algorithm configuration. However, their efficiency decreases as the dimensionality of the problem and the budget of evaluations increase. Meanwhile, derivative-free optimization methods have evolved independently in the optimization community. Therefore, we urge to understand whether cross-fertilization is possible between the two communities, ML and BBO, i.e., whether algorithms that are heavily used in ML also work well in BBO and vice versa. Comparative experiments often involve rather small benchmarks and show visible problems in the experimental setup, such as poor initialization of baselines, overfitting due to problem-specific setting of hyperp
    
[^95]: 记忆健身房：对内存为基础的智能体在无尽任务中的部分可观察挑战

    Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])

    [http://arxiv.org/abs/2309.17207](http://arxiv.org/abs/2309.17207)

    本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。

    

    记忆健身房介绍了一个独特的基准测试，旨在测试深度强化学习智能体，特别是将门循环单元(GRU)与Transformer-XL(TrXL)相比，它们对于记忆长序列的能力、抗噪声和泛化能力。它采用了部分可观察的二维环境和离散控制，即Mortar Mayhem、Mystery Path和Searing Spotlights。这些最初是有限的环境被推广为新颖的无尽任务，作为一种自动课程，从车游戏"I packed my bag"中汲取灵感。这些无尽任务不仅有助于评估效率，而且有趣地评估了记忆为基础的方法的有效性。鉴于现有公开可用的记忆基准的稀缺性，我们提供了一个由TrXL和Proximal Policy Optimization驱动的实现。本实现利用TrXL作为以滑动窗口方法使用的情节性记忆。在有限环境的实验中，我们发现...

    Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
    
[^96]: 使用CodeBERT和Random Forest Regressor进行自动评分的C编程作业

    Auto-grading C programming assignments with CodeBERT and Random Forest Regressor. (arXiv:2309.15216v1 [cs.LG])

    [http://arxiv.org/abs/2309.15216](http://arxiv.org/abs/2309.15216)

    本研究提供了使用机器学习和深度学习方法对C编程作业进行自动评分的分析，并引入了基于代码的转换器词嵌入模型CodeBERT。测试结果表明该策略的有效性，并讨论了统计方法和深度学习技术之间的对比。

    

    手动评分编程作业因复杂性和主观性而具有挑战性。然而，使用深度学习进行自动评分简化了任务。它客观地评估代码质量，检测错误，并准确地分配分数，减轻了教师的负担，同时确保了高效和公平的评估。本研究使用回归、卷积神经网络（CNN）和长短期记忆（LSTM）等机器学习和深度学习方法对C编程作业进行自动评分的分析。使用一种基于代码的转换器词嵌入模型CodeBERT，将文本代码输入转换为向量，然后将向量输入到几个模型中。测试结果证明了建议策略的有效性，均方根误差（RMSE）为1.89。本研究还讨论了统计方法和深度学习技术之间的对比。

    Grading coding assignments manually is challenging due to complexity and subjectivity. However, auto-grading with deep learning simplifies the task. It objectively assesses code quality, detects errors, and assigns marks accurately, reducing the burden on instructors while ensuring efficient and fair assessment. This study provides an analysis of auto-grading of the C programming assignments using machine learning and deep learning approaches like regression, convolutional neural networks (CNN) and long short-term memory (LSTM). Using a code-based transformer word embedding model called CodeBERT, the textual code inputs were transformed into vectors, and the vectors were then fed into several models. The testing findings demonstrated the efficacy of the suggested strategy with a root mean squared error (RMSE) of 1.89. The contrast between statistical methods and deep learning techniques is discussed in the study.
    
[^97]: 一种利用变分自动编码器进行无监督的离群检测的高效方法

    An Efficient Approach to Unsupervised Out-of-Distribution Detection with Variational Autoencoders. (arXiv:2309.02084v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02084](http://arxiv.org/abs/2309.02084)

    本文提出了一种利用变分自动编码器进行无监督的离群检测的高效方法，通过引入误差减少(ER)离群得分来改进普通VAE，在各种数据集上得到了优于基准方法的实验结果。

    

    本文关注深度生成模型(DGMs)在无监督的离群检测中的应用。具体来说，我们专注于使用标准正态分布作为潜在变量的普通变分自动编码器(VAE)。这些模型具有较小的模型大小，可以更快地进行训练和推断，与更复杂的DGMs相比，使它们非常适用于资源有限的应用。我们提出了一种新的离群得分称为误差减少(ER)，专门为普通VAE设计。ER融合了从有损图像输入中重建图像的思想，并考虑了图像的科尔莫戈洛夫复杂性。对各种数据集的实验结果表明，我们的方法优于基准方法。我们的代码可在此处获取：https://github.com/ZJLAB-AMMI/VAE4OOD。

    This paper is concerned with deep generative models (DGMs) for unsupervised out-of-distribution (OOD) detection. In particular, we focus on vanilla Variational Autoencoders (VAE) that use a standard normal prior distribution for the latent variables. These models have a smaller model size, enabling faster training and inference, making them well-suited for resource-limited applications compared to more complex DGMs. We propose a novel OOD score called Error Reduction (ER) specifically designed for vanilla VAE. ER incorporate the idea of reconstructing image inputs from their lossy counterparts and takes into account the Kolmogorov complexity of the images. Experimental results on diverse datasets demonstrate the superiority of our approach over baseline methods. Our code is available at: https://github.com/ZJLAB-AMMI/VAE4OOD.
    
[^98]: LLM4TS:使用预训练的LLM进行两阶段微调用于时间序列预测

    LLM4TS: Two-Stage Fine-Tuning for Time-Series Forecasting with Pre-Trained LLMs. (arXiv:2308.08469v1 [cs.LG])

    [http://arxiv.org/abs/2308.08469](http://arxiv.org/abs/2308.08469)

    这项工作提出了LLM4TS方法，利用预训练的LLMs增强时间序列预测能力。通过两阶段微调和参数高效微调，提高了LLMs处理时间序列数据的能力。

    

    在这项工作中，我们利用预训练的大型语言模型（LLMs）来增强时间序列预测。借鉴了自然语言处理和计算机视觉统一模型的日益增长的兴趣，我们设想创建一个类似的模型用于长期时间序列预测。由于缺乏大规模的时间序列数据来构建稳健的基础模型，我们的方法LLM4TS专注于利用预训练的LLMs的优势。通过将时间序列修补与时间编码相结合，我们提高了LLMs处理时间序列数据的能力。受到聊天机器人领域的有监督微调的启发，我们优先进行两阶段的微调过程：首先进行有监督微调以使LLMs适应时间序列数据，然后进行任务特定的下游微调。此外，为了在不进行大量参数调整的情况下发挥预训练LLMs的灵活性，我们采用了几种参数高效微调（PEFT）技术。

    In this work, we leverage pre-trained Large Language Models (LLMs) to enhance time-series forecasting. Mirroring the growing interest in unifying models for Natural Language Processing and Computer Vision, we envision creating an analogous model for long-term time-series forecasting. Due to limited large-scale time-series data for building robust foundation models, our approach LLM4TS focuses on leveraging the strengths of pre-trained LLMs. By combining time-series patching with temporal encoding, we have enhanced the capability of LLMs to handle time-series data effectively. Inspired by the supervised fine-tuning in chatbot domains, we prioritize a two-stage fine-tuning process: first conducting supervised fine-tuning to orient the LLM towards time-series data, followed by task-specific downstream fine-tuning. Furthermore, to unlock the flexibility of pre-trained LLMs without extensive parameter adjustments, we adopt several Parameter-Efficient Fine-Tuning (PEFT) techniques. Drawing o
    
[^99]: 使用量子核的支持向量回归进行半监督异常检测

    Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel. (arXiv:2308.00583v1 [quant-ph])

    [http://arxiv.org/abs/2308.00583](http://arxiv.org/abs/2308.00583)

    使用量子核的支持向量回归方法被应用于半监督异常检测中，针对NISQ设备的限制，该方法具有理论保证、灵活性和兼容性。

    

    异常检测（AD）涉及识别与其他数据有所不同的观测或事件。机器学习技术在自动化这个过程中显示出成功，通过检测大规模数据中隐藏的模式和偏差。量子计算在机器学习中的潜力已得到广泛认可，导致了大量研究工作以开发适用于量子机器学习（QML）算法。特别是对近期NISQ设备的QML算法的搜索正在全力进行。然而，NISQ设备由于其有限的量子比特相干时间、较少的量子比特数量和高误差率，存在额外的挑战。基于量子核估计的核方法已成为NISQ设备上进行QML的一种有前途的方法，具有理论保证、灵活性和与NISQ约束的兼容性。特别是利用量子核估计的支持向量机（SVM）在各种监督学习任务中显示出成功。

    Anomaly detection (AD) involves identifying observations or events that deviate in some way from the rest of the data. Machine learning techniques have shown success in automating this process by detecting hidden patterns and deviations in large-scale data. The potential of quantum computing for machine learning has been widely recognized, leading to extensive research efforts to develop suitable quantum machine learning (QML) algorithms. In particular, the search for QML algorithms for near-term NISQ devices is in full swing. However, NISQ devices pose additional challenges due to their limited qubit coherence times, low number of qubits, and high error rates. Kernel methods based on quantum kernel estimation have emerged as a promising approach to QML on NISQ devices, offering theoretical guarantees, versatility, and compatibility with NISQ constraints. Especially support vector machines (SVM) utilizing quantum kernel estimation have shown success in various supervised learning tasks
    
[^100]: 残差循环网络中的褪退记忆作为归纳偏差

    Fading memory as inductive bias in residual recurrent networks. (arXiv:2307.14823v1 [cs.LG])

    [http://arxiv.org/abs/2307.14823](http://arxiv.org/abs/2307.14823)

    通过使用残差连接作为归纳偏差，我们提出了一种弱耦合残差循环网络，并研究了其对网络性能、动力学和记忆属性的影响。我们发现，几种不同形式的残差连接可以增加网络的表达能力，并产生有效的归纳偏差。

    

    残差连接被提出作为一种基于架构的归纳偏差，以解决反向传播算法训练的前馈和循环神经网络中梯度爆炸和消失的问题，并提高任务性能。然而，我们对残差连接如何影响循环神经网络的动态和褪退记忆属性知之甚少。在这里，我们引入了弱耦合残差循环网络(WCRNNs)，其中残差连接导致了明确定义的李雅普诺夫指数，并允许研究褪退记忆的属性。我们研究了WCRNNs的残差连接如何影响它们的性能、网络动力学和记忆属性在一组基准任务上。我们发现，几种不同形式的残差连接产生了有效的归纳偏差，从而增加了网络的表达能力。特别是，具有以下特点的残差连接：(i) 导致网络动态接近混沌的边缘，(ii) 允许网络在长时间内保持记忆。

    Residual connections have been proposed as architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increase task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks
    
[^101]: 高效的选择性注意力LSTM用于井曲线合成

    Efficient selective attention LSTM for well log curve synthesis. (arXiv:2307.10253v1 [cs.LG])

    [http://arxiv.org/abs/2307.10253](http://arxiv.org/abs/2307.10253)

    本文提出了一种高效的选择性注意力LSTM方法，用于预测缺失的井曲线。通过实验证实了该方法的有效性和可行性。

    

    非核心钻井逐渐成为地质工程中的主要勘探方法，井曲线作为地质信息的主要载体日益重要。然而，地质环境、测井设备、钻孔质量和突发事件等因素都会影响井曲线的质量。以往的重新测井或手工修正方法成本高效率低。本文提出了一种利用现有数据预测缺失井曲线的机器学习方法，并通过实验证实了其有效性和可行性。所提方法在传统的长短期记忆（LSTM）神经网络基础上加入了自注意机制来分析数据的空间依赖性。它有选择地将LSTM中的主导计算结果包括在内，将计算复杂度从O(n^2)降至O(nlogn)，从而提高了计算效率

    Non-core drilling has gradually become the primary exploration method in geological engineering, and well logging curves have increasingly gained importance as the main carriers of geological information. However, factors such as geological environment, logging equipment, borehole quality, and unexpected events can all impact the quality of well logging curves. Previous methods of re-logging or manual corrections have been associated with high costs and low efficiency. This paper proposes a machine learning method that utilizes existing data to predict missing well logging curves, and its effectiveness and feasibility have been validated through experiments. The proposed method builds upon the traditional Long Short-Term Memory (LSTM) neural network by incorporating a self-attention mechanism to analyze the spatial dependencies of the data. It selectively includes the dominant computational results in the LSTM, reducing the computational complexity from O(n^2) to O(nlogn) and improving
    
[^102]: 关于频率响应函数的分层贝叶斯建模

    On the hierarchical Bayesian modelling of frequency response functions. (arXiv:2307.06263v1 [cs.LG])

    [http://arxiv.org/abs/2307.06263](http://arxiv.org/abs/2307.06263)

    结构健康监测(PBSHM)旨在在人群成员之间共享信息以改善健康状态的推断。由于差异和数据丢失所带来的挑战需要解决。

    

    基于人群的结构健康监测(PBSHM)旨在在人群成员之间共享有价值的信息，例如正常和损伤状态的数据，以改进对成员健康状态的推断。即使人群由名义上相同的结构组成，由于材料性质、几何形状、边界条件或环境影响(例如温度变化)的细微差异产生了 benign variations。这些差异可以影响模态性质，并表现为频率响应函数(FRF)的共振峰特征的变化。许多SHM策略依赖于对结构的动态特性进行监测，因此 benign variations 对这些系统的实际实施是具有挑战性的。振动式SHM的另一个常见挑战是数据丢失，可能是由于传输问题、传感器故障、传感器之间的采样率不匹配等原因引起的。

    Population-based structural health monitoring (PBSHM) aims to share valuable information among members of a population, such as normal- and damage-condition data, to improve inferences regarding the health states of the members. Even when the population is comprised of nominally-identical structures, benign variations among the members will exist as a result of slight differences in material properties, geometry, boundary conditions, or environmental effects (e.g., temperature changes). These discrepancies can affect modal properties and present as changes in the characteristics of the resonance peaks of the frequency response function (FRF). Many SHM strategies depend on monitoring the dynamic properties of structures, so benign variations can be challenging for the practical implementation of these systems. Another common challenge with vibration-based SHM is data loss, which may result from transmission issues, sensor failure, a sample-rate mismatch between sensors, and other causes
    
[^103]: DL模型和训练环境对能源消耗有影响吗？

    Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])

    [http://arxiv.org/abs/2307.05520](http://arxiv.org/abs/2307.05520)

    本研究分析了模型架构和训练环境对训练更环保的计算机视觉模型的影响，并找出了能源效率和模型正确性之间的权衡关系。

    

    当前计算机视觉领域的研究主要集中在提高深度学习（DL）的正确性和推理时间性能上。然而，目前很少有关于训练DL模型带来巨大碳足迹的研究。本研究旨在分析模型架构和训练环境对训练更环保的计算机视觉模型的影响。我们将这个目标分为两个研究问题。首先，我们分析模型架构对实现更环保模型同时保持正确性在最佳水平的影响。其次，我们研究训练环境对生成更环保模型的影响。为了调查这些关系，我们在模型训练过程中收集了与能源效率和模型正确性相关的多个指标。然后，我们描述了模型架构在测量能源效率和模型正确性方面的权衡，以及它们与训练环境的关系。我们在一个实验平台上进行了这项研究。

    Current research in the computer vision field mainly focuses on improving Deep Learning (DL) correctness and inference time performance. However, there is still little work on the huge carbon footprint that has training DL models. This study aims to analyze the impact of the model architecture and training environment when training greener computer vision models. We divide this goal into two research questions. First, we analyze the effects of model architecture on achieving greener models while keeping correctness at optimal levels. Second, we study the influence of the training environment on producing greener models. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy efficiency and the models' correctness regarding model architecture, and their relationship with the training environment. We conduct this research in the context of a 
    
[^104]: TIAM -- 一种评估文本到图像生成中对齐性的度量方法

    TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])

    [http://arxiv.org/abs/2307.05134](http://arxiv.org/abs/2307.05134)

    本文提出了一种评估文本到图像生成中对齐性的新度量方法TIAM，该方法基于提示模板，可以更好地描述生成图像与提示中内容的对齐程度，包括对象类型、数量和颜色。研究结果表明，图像质量可以有很大的变化。

    

    合成图像生成的进展使得评估其质量变得至关重要。尽管已经提出了几种用于评估图像渲染的度量方法，但对于基于提示生成图像的文本到图像（T2I）模型而言，考虑到生成图像与提示中重要内容之间的相似程度等额外因素至关重要。此外，虽然生成的图像通常是从随机起始点开始的，但通常不考虑这一影响。本文提出了一种基于提示模板的新度量方法，用于研究提示中指定的内容与生成的图像之间的对齐性。它允许我们更好地描述对齐性，包括指定对象的类型、数量和颜色。我们对几个最近的T2I模型进行了研究，并获得了一个有趣的额外结果，即图像质量可以大幅度变化。

    The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
    
[^105]: 并行算法与神经执行同步

    Parallel Algorithms Align with Neural Execution. (arXiv:2307.04049v1 [cs.LG])

    [http://arxiv.org/abs/2307.04049](http://arxiv.org/abs/2307.04049)

    这项研究发现，与顺序算法相比，并行算法能充分利用神经算法推理器的计算能力，从而减少训练时间并获得更好的预测性能。

    

    神经算法推理器是并行处理器。教授它们顺序算法与其性质相矛盾，使得它们的计算中包含很多冗余。然而并行算法可以充分利用它们的计算能力，因此只需执行较少的层次。这大大减少了训练时间，我们观察到与CLRS框架上顺序实现的搜索、排序和寻找强连接组件相比，并行实现的训练时间大大缩短。此外，大多数情况下，并行版本的预测性能更好。

    Neural algorithmic reasoners are parallel processors. Teaching them sequential algorithms contradicts this nature, rendering a significant share of their computations redundant. Parallel algorithms however may exploit their full computational power, therefore requiring fewer layers to be executed. This drastically reduces training times, as we observe when comparing parallel implementations of searching, sorting and finding strongly connected components to their sequential counterparts on the CLRS framework. Additionally, parallel versions achieve strongly superior predictive performance in most cases.
    
[^106]: CardiGraphormer: 揭示自监督学习在颠覆药物发现中的力量

    CardiGraphormer: Unveiling the Power of Self-Supervised Learning in Revolutionizing Drug Discovery. (arXiv:2307.00859v1 [cs.LG])

    [http://arxiv.org/abs/2307.00859](http://arxiv.org/abs/2307.00859)

    CardiGraphormer是一种革命性的方法，结合了自监督学习、图神经网络和保持基数注意力，颠覆了药物发现的方式。它利用自监督学习学习分子表示并利用图神经网络提取分子指纹，提高了预测性能和可解释性，同时减少了计算时间，并在处理复杂数据和执行各种与图结构相关的任务方面表现出色。

    

    在广阔的药物发现领域中，已知药物约有15,000种，但只有大约4,200种得到了批准，化学空间的组合性质提供了一项艰巨的挑战。尽管人工智能成为了有力的伙伴，传统的人工智能框架仍面临重大障碍。本文介绍了CardiGraphormer，这是一种划时代的方法，通过结合自监督学习（SSL）、图神经网络（GNN）和保持基数注意力，从而颠覆药物发现。CardiGraphormer是Graphormer和保持基数注意力的新颖组合，利用SSL学习有效的分子表示，并利用GNN提取分子指纹，提高了预测性能和可解释性，并减少了计算时间。它在处理分子结构等复杂数据方面表现出色，并能执行与节点、节点对、子图或整个图结构相关的任务。

    In the expansive realm of drug discovery, with approximately 15,000 known drugs and only around 4,200 approved, the combinatorial nature of the chemical space presents a formidable challenge. While Artificial Intelligence (AI) has emerged as a powerful ally, traditional AI frameworks face significant hurdles. This manuscript introduces CardiGraphormer, a groundbreaking approach that synergizes self-supervised learning (SSL), Graph Neural Networks (GNNs), and Cardinality Preserving Attention to revolutionize drug discovery. CardiGraphormer, a novel combination of Graphormer and Cardinality Preserving Attention, leverages SSL to learn potent molecular representations and employs GNNs to extract molecular fingerprints, enhancing predictive performance and interpretability while reducing computation time. It excels in handling complex data like molecular structures and performs tasks associated with nodes, pairs of nodes, subgraphs, or entire graph structures. CardiGraphormer's potential a
    
[^107]: 降秩卡尔曼滤波器：在高维中进行近似低秩动态滤波

    The Rank-Reduced Kalman Filter: Approximate Dynamical-Low-Rank Filtering In High Dimensions. (arXiv:2306.07774v1 [stat.ML])

    [http://arxiv.org/abs/2306.07774](http://arxiv.org/abs/2306.07774)

    该论文提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播，通过将Lyapunov方程投影到低秩矩阵的流形上，使用数值稳定的动态低秩积分器求解，能够有效地处理高维数据。

    

    在高维动态系统的推断和模拟中，需要进行某种形式的降维才能使问题具有可处理性。在本文中，我们提出了一种新的近似高斯滤波和平滑方法，它将协方差矩阵的低秩近似传播。这是通过将预测步骤相关的Lyapunov方程投影到低秩矩阵的流形上来实现的，然后通过最近开发的数值稳定、动态低秩积分器求解这些方程。与此同时，通过注意协方差更新仅转换协方差矩阵的列空间，而该空间由构造得到，从而使更新步骤具有可处理性。算法与现有的基于集合的方法不同之处在于，协方差矩阵的低秩近似是确定性的，而不是随机的。关键在于，这使得该方法能够有效地处理高维数据。

    Inference and simulation in the context of high-dimensional dynamical systems remain computationally challenging problems. Some form of dimensionality reduction is required to make the problem tractable in general. In this paper, we propose a novel approximate Gaussian filtering and smoothing method which propagates low-rank approximations of the covariance matrices. This is accomplished by projecting the Lyapunov equations associated with the prediction step to a manifold of low-rank matrices, which are then solved by a recently developed, numerically stable, dynamical low-rank integrator. Meanwhile, the update steps are made tractable by noting that the covariance update only transforms the column space of the covariance matrix, which is low-rank by construction. The algorithm differentiates itself from existing ensemble-based approaches in that the low-rank approximations of the covariance matrices are deterministic, rather than stochastic. Crucially, this enables the method to repr
    
[^108]: 基于双曲图扩散模型的分子生成

    Hyperbolic Graph Diffusion Model for Molecule Generation. (arXiv:2306.07618v1 [cs.LG])

    [http://arxiv.org/abs/2306.07618](http://arxiv.org/abs/2306.07618)

    本文提出了基于双曲图扩散模型的分子生成方法，可以更全面地捕捉分子的内部非欧几里德结构，实现数据生成，并提取复杂几何特征的能力。

    

    最近，扩散模型在数据生成方面取得了显著的成果，例如生成高质量的图像。然而，化学分子通常具有复杂的非欧几里德空间结构，其行为动态变化且难以预测。大多数现有的扩散模型高度依赖于计算欧几里德空间中的概率分布，即高斯分布，不能捕捉分子的内部非欧几里德结构，特别是分子所表示的隐式流形表面的分层结构。观察到，双曲嵌入空间中的复杂分层结构变得更加明显且更容易被捕捉。为了充分利用扩散模型的数据生成能力和提取复杂几何特征的双曲嵌入的强大能力，我们提出将扩散模型扩展到双曲流形上进行分子生成，即基于双曲图扩散模型的分子生成。

    Recently, diffusion models have achieved remarkable performance in data generation, e.g., generating high-quality images. Nevertheless, chemistry molecules often have complex non-Euclidean spatial structures, with the behavior changing dynamically and unpredictably. Most existing diffusion models highly rely on computing the probability distribution, i.e., Gaussian distribution, in Euclidean space, which cannot capture internal non-Euclidean structures of molecules, especially the hierarchical structures of the implicit manifold surface represented by molecules. It has been observed that the complex hierarchical structures in hyperbolic embedding space become more prominent and easier to be captured. In order to leverage both the data generation power of diffusion models and the strong capability to extract complex geometric features of hyperbolic embedding, we propose to extend the diffusion model to hyperbolic manifolds for molecule generation, namely, Hyperbolic Graph Diffusion Mode
    
[^109]: $\ell_p$灵敏度采样的更严格界限

    Sharper Bounds for $\ell_p$ Sensitivity Sampling. (arXiv:2306.00732v1 [cs.DS])

    [http://arxiv.org/abs/2306.00732](http://arxiv.org/abs/2306.00732)

    该论文研究了$\ell_p$子空间嵌入的灵敏度采样界限，取得了比通用界限更好的结果，对于$1\leq p<2$的情况下，界限达到了$\mathfrak{S}^{2/p}$，对于$2<p<\infty$的情况下，界限达到了$\mathfrak{S}^{2-2/p}$。

    

    在大规模机器学习中，随机采样是一种近似数据集的流行方式，这种方式可以通过一小部分具有代表性的示例来进行。特别地，灵敏度采样是一种强烈研究的技术，它在极其普遍的情况下提供可证明的近似质量保证，同时将示例的数量减少到VC维$d$和总灵敏度$\mathfrak{S}$的乘积。然而，除了$\ell_2$子空间嵌入以外，很少有保证超过这个$\mathfrak{S}d$通用界限的知识，尽管以前的工作非常强调灵敏度采样。在这项工作中，我们首次展示了对于$ p\neq2$的$\ell_p$子空间嵌入的灵敏度采样界限，这些界限超过了一般的$\mathfrak{S}d$界限，对于$1\leq p<2$，我们取得了大约$\mathfrak{S}^{2/p}$的界限，并且对于$2<p<\infty$，取得了$\mathfrak{S}^{2-2/p}$的界限。在$1\leq p<2$的情况下，我们表明这个边界是密切相关的。

    In large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. In particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the VC dimension $d$ and the total sensitivity $\mathfrak S$ in remarkably general settings. However, guarantees going beyond this general bound of $\mathfrak S d$ are known in perhaps only one setting, for $\ell_2$ subspace embeddings, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for $\ell_p$ subspace embeddings for $p\neq 2$ that improve over the general $\mathfrak S d$ bound, achieving a bound of roughly $\mathfrak S^{2/p}$ for $1\leq p<2$ and $\mathfrak S^{2-2/p}$ for $2<p<\infty$. For $1\leq p<2$, we show that this bound is tight, in the sense that there exist matrices for which
    
[^110]: 大型语言模型不能作为抽象推理器

    Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])

    [http://arxiv.org/abs/2305.19555](http://arxiv.org/abs/2305.19555)

    本文通过对最先进的大型语言模型进行抽象推理任务评估，发现它们在这方面的表现十分有限，揭示了其在推理方面的局限性。

    

    大型语言模型在自然语言处理任务上表现出极好的性能，包括文本理解和常识推理等。然而，这些成功的机制尚不清楚，LLMs是否能够达到人类的认知能力或这些模型是否还存在根本性的局限性也不确定。抽象推理是认知的基本任务，包括从少量数据中找到和应用一般模式。评估深度神经结构在这个任务上的表现可以揭示它们在推理方面的潜在局限性和广泛的泛化能力，这是一个目前未被探索的领域。本文对最先进的LLMs进行了大量评估，发现它们在抽象推理任务中的表现非常有限，并探究了造成这种差异的原因。

    Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
    
[^111]: 《大脑肿瘤分割（BraTS）挑战2023：关注儿科（CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs）》

    The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.17033](http://arxiv.org/abs/2305.17033)

    这个论文介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，该挑战是首个专注于儿童脑肿瘤的BraTS挑战，旨在评估儿童脑胶质瘤的体积分割算法的发展。儿童中枢神经系统肿瘤是儿童癌症相关死亡的主要原因，并且对这些实体的诊断和治疗存在一些挑战。

    

    儿童中枢神经系统肿瘤是儿童癌症相关死亡的最常见原因。儿童高级别胶质瘤的五年生存率不到20％。由于罕见，对这些实体的诊断通常会延迟，其治疗主要基于历史治疗理念，并且临床试验需要多机构合作。MICCAI大脑肿瘤分割（BraTS）挑战是一个里程碑式的社区基准事件，已经成功创建资源12年，用于成人胶质瘤的分割和分析。在这里，我们介绍了CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023挑战，这是首个专注于儿童脑肿瘤的BraTS挑战，其中包括多个国际合作组织专注于儿科神经肿瘤和临床试验的数据。BraTS-PEDs 2023挑战侧重于评估用于儿童脑胶质瘤的体积分割算法的发展。

    Pediatric tumors of the central nervous system are the most common cause of cancer-related death in children. The five-year survival rate for high-grade gliomas in children is less than 20\%. Due to their rarity, the diagnosis of these entities is often delayed, their treatment is mainly based on historic treatment concepts, and clinical trials require multi-institutional collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a landmark community benchmark event with a successful history of 12 years of resource creation for the segmentation and analysis of adult glioma. Here we present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which represents the first BraTS challenge focused on pediatric brain tumors with data acquired across multiple international consortia dedicated to pediatric neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on benchmarking the development of volumentric segmentation algorithms for pediatric brain gli
    
[^112]: 以公平名义：评估临床记录去识别中的偏见

    In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])

    [http://arxiv.org/abs/2305.11348](http://arxiv.org/abs/2305.11348)

    本文研究了临床记录去识别系统在不同人口群体中的表现差异，揭示了其在名称去识别方面存在显著的偏见。

    

    数据共享对于开放科学和可重复研究至关重要，但合法共享临床数据需要从电子健康记录中删除受保护的健康信息。这个过程，称为去识别，通常通过许多商业和开源系统使用机器学习算法来实现。虽然这些系统在平均水平上已经显示出令人信服的结果，但它们在不同的人口群体中的表现差异还没有得到彻底的检查。在这项工作中，我们通过大规模实证分析，研究了临床笔记中的名称去识别系统的偏见。为了实现这一目的，我们创建了16个名称集，涵盖了四个人口统计学维度：性别、种族、名称流行度和流行的十年。我们将这些名称插入到100个手动筛选的临床模板中，并评估了九种公共和私人去识别方法的性能。我们的发现表明，在临床记录去识别系统的名称方面存在统计显著的偏见。

    Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant p
    
[^113]: 对抗性表征学习在音频隐私保护中的应用

    Adversarial Representation Learning for Robust Privacy Preservation in Audio. (arXiv:2305.00011v1 [cs.SD])

    [http://arxiv.org/abs/2305.00011](http://arxiv.org/abs/2305.00011)

    本研究提出了一种对抗性训练方法，用于学习音频的表征，从而有效地防止从音频记录的潜在特征中检测到语音活动，提出的方法能够使得包含语音的音频记录的潜在表征与不包含语音的音频记录的潜在表征无法被语音分类器区分出来。

    

    声音事件检测系统广泛应用于各种应用程序，如监视和环境监测，其中数据被自动收集、处理并发送到云中进行声音识别。然而，这个过程可能无意中泄露用户或其周围环境的敏感信息，因此引起了隐私关注。在本研究中，我们提出了一种新颖的对抗训练方法，用于学习音频记录的表征，从而有效地防止从记录的潜在特征中检测到语音活动。所提出的方法训练模型生成包含语音的音频记录的不变潜在表征，这些表征不能由语音分类器从非语音记录中区分出来。我们的工作创新之处在于，优化算法中语音分类器的权重定期被训练在有监督方式下的分类器的权重所替换。这增加了语音分类器的判别能力。

    Sound event detection systems are widely used in various applications such as surveillance and environmental monitoring where data is automatically collected, processed, and sent to a cloud for sound recognition. However, this process may inadvertently reveal sensitive information about users or their surroundings, hence raising privacy concerns. In this study, we propose a novel adversarial training method for learning representations of audio recordings that effectively prevents the detection of speech activity from the latent features of the recordings. The proposed method trains a model to generate invariant latent representations of speech-containing audio recordings that cannot be distinguished from non-speech recordings by a speech classifier. The novelty of our work is in the optimization algorithm, where the speech classifier's weights are regularly replaced with the weights of classifiers trained in a supervised manner. This increases the discrimination power of the speech cl
    
[^114]: 非准确谱算法的最优性

    On the Optimality of Misspecified Spectral Algorithms. (arXiv:2303.14942v2 [math.ST] CROSS LISTED)

    [http://arxiv.org/abs/2303.14942](http://arxiv.org/abs/2303.14942)

    在本文中，我们研究了非准确谱算法的最优性问题。我们证明了在一些特定的RKHSs上，谱算法对于所有的$s\in (0,1)$都是极小极大最优的。

    

    在非准确谱算法问题中，研究人员通常假设地下真实函数$f_{\rho}^{*} \in [\mathcal{H}]^{s}$，其中$\mathcal{H}$是一个再生核希尔伯特空间(RKHS)的较平滑插值空间，$s\in (0,1)$。现有的极小极大最优结果要求$\|f_{\rho}^{*}\|_{L^{\infty}}<\infty$，这隐含地要求$s > \alpha_{0}$，其中$\alpha_{0}\in (0,1)$是嵌入指数，一个依赖于$\mathcal{H}$的常数。关于谱算法是否对所有的$s\in (0,1)$都是最优的问题已经存在多年。在本文中，我们证明了谱算法是对于任意的$\alpha_{0}-\frac{1}{\beta} < s < 1$都是极小极大最优的，其中$\beta$是$\mathcal{H}$的特征值衰减率。我们还给出了几类满足$ \alpha_0 = \frac{1}{\beta} $的RKHSs，因此，谱算法在这些RKHSs上对于所有的$s\in (0,1)$都是极小极大最优的。

    In the misspecified spectral algorithms problem, researchers usually assume the underground true function $f_{\rho}^{*} \in [\mathcal{H}]^{s}$, a less-smooth interpolation space of a reproducing kernel Hilbert space (RKHS) $\mathcal{H}$ for some $s\in (0,1)$. The existing minimax optimal results require $\|f_{\rho}^{*}\|_{L^{\infty}}<\infty$ which implicitly requires $s > \alpha_{0}$ where $\alpha_{0}\in (0,1)$ is the embedding index, a constant depending on $\mathcal{H}$. Whether the spectral algorithms are optimal for all $s\in (0,1)$ is an outstanding problem lasting for years. In this paper, we show that spectral algorithms are minimax optimal for any $\alpha_{0}-\frac{1}{\beta} < s < 1$, where $\beta$ is the eigenvalue decay rate of $\mathcal{H}$. We also give several classes of RKHSs whose embedding index satisfies $ \alpha_0 = \frac{1}{\beta} $. Thus, the spectral algorithms are minimax optimal for all $s\in (0,1)$ on these RKHSs.
    
[^115]: 一种统一的方法推导（时间均匀的）PAC-Bayes界限

    A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03421](http://arxiv.org/abs/2302.03421)

    该论文提出了一种用于推导PAC-Bayesian泛化界限的统一框架，不同于传统的固定样本大小方式，该框架适用于所有停止时间。同时，该论文还提出了新的边界方法，也可以应用于非平稳损失函数和非独立同分布的数据。

    

    我们提出了一个框架，用于推导PAC-Bayesian泛化界限。与大多数关于此主题的文献不同，我们的界限是任何时间都有效的（即时间均匀的），这意味着它们适用于所有停止时间，而不仅仅是固定的样本大小。我们的方法按照以下顺序结合了四种工具：（a）非负超马丁格尔或反向亚马逊，（b）混合法，（c）Donsker-Varadhan公式（或其它凸性对偶原理）和（d）Ville不等式。我们的主要成果是一个PAC-Bayes定理，适用于广泛的离散随机过程类。我们展示了这个结果如何推出知名的经典PAC-Bayes界限，例如Seeger、McAllester、Maurer和Catoni的界限，以及许多最新的界限。我们还提出了几个新的界限。我们的框架还使我们能够放松传统的假设；特别地，我们考虑非平稳损失函数和非独立同分布的数据。

    We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
    
[^116]: 带有随机集合的贝叶斯后验近似

    Bayesian posterior approximation with stochastic ensembles. (arXiv:2212.08123v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08123](http://arxiv.org/abs/2212.08123)

    本文提出一种新方法，即使用随机神经网络集合来近似贝叶斯后验，并通过变分推断进行训练，实验证明该方法比其他流行的贝叶斯推断基线提供了更准确的后验估计。

    

    我们引入一种基于随机神经网络集的方法来近似贝叶斯后验。它将随机方法（如dropout）与深度集成相结合，并将随机集合公式化为分布族，并使用变分推断训练以近似贝叶斯后验。我们在一个玩具问题和CIFAR图像分类上实现了基于Monte Carlo Dropout，DropConnect和新颖的非参数版本的随机集合，并直接与哈密顿马尔可夫蒙特卡罗模拟比较质量来测试后验。结果表明，随机集合提供了比其他流行的贝叶斯推断基线更准确的后验估计。

    We introduce ensembles of stochastic neural networks to approximate the Bayesian posterior, combining stochastic methods such as dropout with deep ensembles. The stochastic ensembles are formulated as families of distributions and trained to approximate the Bayesian posterior with variational inference. We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and a novel non-parametric version of dropout and evaluate them on a toy problem and CIFAR image classification. For both tasks, we test the quality of the posteriors directly against Hamiltonian Monte Carlo simulations. Our results show that stochastic ensembles provide more accurate posterior estimates than other popular baselines for Bayesian inference.
    
[^117]: 使用鲁棒性元学习方法的选择性分类

    Selective classification using a robust meta-learning approach. (arXiv:2212.05987v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05987](http://arxiv.org/abs/2212.05987)

    这项研究提出了使用鲁棒性元学习方法的选择性分类。通过一个辅助网络捕捉预测不确定性，并在训练和测试中应用，我们成功地最小化了辍学方差，从而提高了在真实世界中的分类性能。

    

    预测不确定性-模型对其在输入上的准确性的自我意识-对于通过训练干预构建鲁棒模型和选择性分类等测试应用至关重要。我们提出了一种新颖的实例条件重新加权方法，利用辅助网络捕捉预测不确定性，并统一了这些训练时间和测试时间应用。辅助网络是在双层优化框架中使用元目标进行训练的。我们提议的一个重要贡献是最小化辍学方差的元目标，这是贝叶斯预测不确定性的近似值。我们通过控制实验表明，通过这个元目标，我们能够有效捕捉到不确定性的多样化特定概念，而之前的方法只能捕捉到某些方面。这些结果在真实世界的设置-选择性分类、标签噪声、领域适应、校准和数据集-Imagenet、Cifar100中转化为显著的收益。

    Predictive uncertainty-a model's self awareness regarding its accuracy on an input-is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditioned reweighting approach that captures predictive uncertainty using an auxiliary network and unifies these train- and test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing the dropout variance, an approximation of Bayesian Predictive uncertainty. We show in controlled experiments that we effectively capture the diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings-selective classification, label noise, domain adaptation, calibration-and across datasets-Imagenet, Cifar100, 
    
[^118]: 基于状态的重要性抽样方法实现低方差的行为策略离线评估

    Low Variance Off-policy Evaluation with State-based Importance Sampling. (arXiv:2212.03932v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03932](http://arxiv.org/abs/2212.03932)

    本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    

    在强化学习的离线评估中，需要评估目标策略的性能，而这需要使用由行为策略采集的样本数据。传统的重要性抽样方法由于计算动作概率比值的乘积而导致方差增加，从而在涉及长期规划的任务中出现估计不准确的问题。本文提出了一种基于状态的重要性抽样（SIS）方法，通过检测“忽略状态”的子轨迹来实现低方差的离线评估。

    In off-policy reinforcement learning, a behaviour policy performs exploratory interactions with the environment to obtain state-action-reward samples which are then used to learn a target policy that optimises the expected return. This leads to a problem of off-policy evaluation, where one needs to evaluate the target policy from samples collected by the often unrelated behaviour policy. Importance sampling is a traditional statistical technique that is often applied to off-policy evaluation. While importance sampling estimators are unbiased, their variance increases exponentially with the horizon of the decision process due to computing the importance weight as a product of action probability ratios, yielding estimates with low accuracy for domains involving long-term planning. This paper proposes state-based importance sampling (SIS), which drops the action probability ratios of sub-trajectories with "negligible states" -- roughly speaking, those for which the chosen actions have no 
    
[^119]: 解开 (不)可控特征

    Disentangled (Un)Controllable Features. (arXiv:2211.00086v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00086](http://arxiv.org/abs/2211.00086)

    该论文提出了一种新方法，可以将潜在特征分解为可控和不可控的部分，并证明了这种分解表示在不同环境中易于解释和使用可解释的规划算法。

    

    在具有高维状态的MDP背景下，下游任务通常在原始输入空间的压缩、低维表示上运行。因此，为了获得有用的表示，我们使用了各种学习目标。然而，这些表示通常缺乏对不同特征的可解释性。我们提出了一种新方法，能够将潜在特征分解为可控和不可控的部分。我们证明了得到的分区表示在三种类型的环境中是易于解释的，并且在分离的可控潜在分区中，能够在一组程序生成的迷宫环境中使用可解释的规划算法。

    In the context of MDPs with high-dimensional states, downstream tasks are predominantly applied on a compressed, low-dimensional representation of the original input space. A variety of learning objectives have therefore been used to attain useful representations. However, these representations usually lack interpretability of the different features. We present a novel approach that is able to disentangle latent features into a controllable and an uncontrollable partition. We illustrate that the resulting partitioned representations are easily interpretable on three types of environments and show that, in a distribution of procedurally generated maze environments, it is feasible to interpretably employ a planning algorithm in the isolated controllable latent partition.
    
[^120]: DeXtreme: 从模拟到现实的敏捷手部操作的转移

    DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to Reality. (arXiv:2210.13702v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.13702](http://arxiv.org/abs/2210.13702)

    本文介绍了一种从模拟到现实的敏捷手部操作的转移方法，通过训练适应各种条件的策略和鲁棒位姿估计器，实现了在类人机器人手中进行稳健的灵巧操作，并证实了模拟到现实的转移对敏捷操作的可行性。

    

    最近的研究表明，深度强化学习算法能够在模拟中学习复杂的机器人行为，包括多指操作领域。然而，由于模拟与现实之间存在差距，这样的模型在现实世界中的转移可能会很具挑战性。在本文中，我们提出了训练技术，可以让a）一种策略在类人机器人手中进行稳健的灵巧操作和b）一种适合提供可靠实时信息的鲁棒位姿估计器，用于被操作物体的状态。我们的策略经过在模拟中适应各种条件的训练。结果表明，我们基于视觉的策略在重新定向任务上明显优于文献中最好的基于视觉的策略，并且与通过运动捕捉系统提供特权状态信息的策略相竞争。我们的工作证实了敏捷操作在模拟到现实的转移中的可能性。

    Recent work has demonstrated the ability of deep reinforcement learning (RL) algorithms to learn complex robotic behaviours in simulation, including in the domain of multi-fingered manipulation. However, such models can be challenging to transfer to the real world due to the gap between simulation and reality. In this paper, we present our techniques to train a) a policy that can perform robust dexterous manipulation on an anthropomorphic robot hand and b) a robust pose estimator suitable for providing reliable real-time information on the state of the object being manipulated. Our policies are trained to adapt to a wide range of conditions in simulation. Consequently, our vision-based policies significantly outperform the best vision policies in the literature on the same reorientation task and are competitive with policies that are given privileged state information via motion capture systems. Our work reaffirms the possibilities of sim-to-real transfer for dexterous manipulation in 
    
[^121]: 缩小目标网络与功能正则化之间的差距

    Bridging the Gap Between Target Networks and Functional Regularization. (arXiv:2210.12282v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12282](http://arxiv.org/abs/2210.12282)

    目标网络在深度强化学习中具有关键作用，但其对优化的影响还不为人所理解。本文提出了一种显式的功能正则化方法，通过替换目标网络来改善样本效率和性能。实验证明，这种方法能够更好地稳定训练，并取得了较好的结果。

    

    引导法是深度强化学习成功的关键。然而，通过引导法学习价值函数常常导致训练不稳定，因为目标值变化快速。目标网络被用于通过使用滞后参数集来估计目标值来稳定训练。尽管目标网络很受欢迎，但它们对优化的影响仍不为人所理解。在这项工作中，我们展示了它们作为一种隐式正则化器的作用。这种正则化器有不灵活和非凸优点。为了克服这些问题，我们提出了一种显式的功能正则化方法，它是函数空间中的凸正则化器，可以轻松调节。我们理论上分析了我们方法的收敛性，并通过实验证明，用更具理论基础的功能正则化方法取代目标网络可以提高样本效率和性能。

    Bootstrapping is behind much of the successes of Deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values. Despite the popularity of Target Networks, their effect on the optimization is still misunderstood. In this work, we show that they act as an implicit regularizer. This regularizer has disadvantages such as being inflexible and non convex. To overcome these issues, we propose an explicit Functional Regularization that is a convex regularizer in function space and can easily be tuned. We analyze the convergence of our method theoretically and empirically demonstrate that replacing Target Networks with the more theoretically grounded Functional Regularization approach leads to better sample efficiency and performance improvements.
    
[^122]: 通过差异传播验证复合系统

    Validation of Composite Systems by Discrepancy Propagation. (arXiv:2210.12061v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12061](http://arxiv.org/abs/2210.12061)

    本研究提出了一种通过差异传播验证复合系统的方法，可以从潜在不准确的模拟中推导出真实系统的故障概率上界。对于度量如最大平均差异 (MMD) ，我们开发了紧凑的凸松弛，并证明了该方法在高维输入空间中具有可比较的计算效率和良好的验证结果。

    

    在工业应用中，基于给定质量标准评估真实世界系统的有效性是一项常见但昂贵的任务，因为需要大量的真实世界测试。通过模拟来验证这类系统提供了一种有前途且更便宜的替代方案，但需要评估模拟的准确性和端到端测量。此外，模拟和实际使用之间的协变量转移可能导致估计这类系统可靠性的困难。在这项工作中，我们提出了一种验证方法，通过复合系统传播分布差异度量的界限，从而允许我们从潜在不准确的模拟中推导出真实系统的故障概率上界。每个传播步骤都涉及一个优化问题，对于像最大平均差异 (MMD) 这样的度量，我们基于半定程序开发了紧凑的凸松弛。我们证明了我们的方法即使在高维输入空间中也具有可比较的计算效率，并且在多个复合系统验证任务上取得了良好的结果。

    Assessing the validity of a real-world system with respect to given quality criteria is a common yet costly task in industrial applications due to the vast number of required real-world tests. Validating such systems by means of simulation offers a promising and less expensive alternative, but requires an assessment of the simulation accuracy and therefore end-to-end measurements. Additionally, covariate shifts between simulations and actual usage can cause difficulties for estimating the reliability of such systems. In this work, we present a validation method that propagates bounds on distributional discrepancy measures through a composite system, thereby allowing us to derive an upper bound on the failure probability of the real system from potentially inaccurate simulations. Each propagation step entails an optimization problem, where -- for measures such as maximum mean discrepancy (MMD) -- we develop tight convex relaxations based on semidefinite programs. We demonstrate that our
    
[^123]: 降低难度和提高鲁棒性：基于 Bregman 散度视角的对抗训练

    Lower Difficulty and Better Robustness: A Bregman Divergence Perspective for Adversarial Training. (arXiv:2208.12511v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.12511](http://arxiv.org/abs/2208.12511)

    本文研究了通过降低优化难度来提高对抗训练中的鲁棒性。基于新颖的Bregman散度视角，分析了两种典型的对抗训练方法的学习目标，并找到了优化过程更容易的方法。提出的两种方法能够降低优化难度，并提供更好的鲁棒性。

    

    本文研究了通过降低优化难度来改善对抗训练（AT）中获得的对抗鲁棒性。为了更好地研究这个问题，我们建立了一个新颖的基于 Bregman 散度的对抗训练视角，在这个视角下，AT可以看作是训练数据点在负熵曲线上的滑动过程。基于这个视角，我们分析了两种典型的AT方法，即PGD-AT和TRADES的学习目标，并发现TRADES的优化过程比PGD-AT更容易，因为TRADES可以分离PGD-AT。此外，我们讨论了TRADES中熵的作用，并发现具有高熵的模型能够更好地学习鲁棒性。受到以上发现的启发，我们提出了两种方法，即FAIT和MER，它们不仅可以在10步PGD对手下降低优化难度，还能提供更好的鲁棒性。我们的工作表明，在10步PGD对手下降低优化难度是可行的，并能提供更好的鲁棒性。

    In this paper, we investigate on improving the adversarial robustness obtained in adversarial training (AT) via reducing the difficulty of optimization. To better study this problem, we build a novel Bregman divergence perspective for AT, in which AT can be viewed as the sliding process of the training data points on the negative entropy curve. Based on this perspective, we analyze the learning objectives of two typical AT methods, i.e., PGD-AT and TRADES, and we find that the optimization process of TRADES is easier than PGD-AT for that TRADES separates PGD-AT. In addition, we discuss the function of entropy in TRADES, and we find that models with high entropy can be better robustness learners. Inspired by the above findings, we propose two methods, i.e., FAIT and MER, which can both not only reduce the difficulty of optimization under the 10-step PGD adversaries, but also provide better robustness. Our work suggests that reducing the difficulty of optimization under the 10-step PGD a
    
[^124]: 使用正则化稀疏自编码器预测良好反应坐标和MD轨迹的未来演化：一种新的深度学习方法

    Prediction of good reaction coordinates and future evolution of MD trajectories using Regularized Sparse Autoencoders: A novel deep learning approach. (arXiv:2208.10962v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2208.10962](http://arxiv.org/abs/2208.10962)

    本研究提出了一种新的深度学习方法，使用正则化稀疏自编码器预测良好的反应坐标以及MD轨迹的演化情况，并展示了正则化约束对于选择重要反应坐标的帮助。

    

    鉴定反应坐标(RCs)是一个活跃的研究领域，因为RCs在确定化学反应的进展中起着关键作用。选择反应坐标通常基于启发式知识。然而，选择的标准之一是该坐标应清晰地捕获反应物和生成物状态。此外，坐标应该是最慢的，使得所有其他自由度可以沿着反应坐标轻松达到平衡。我们使用了一种正则化稀疏自编码器，即基于能量的模型，来发现一组关键的反应坐标。除了发现反应坐标，我们的模型还可以预测分子动力学(MD)轨迹的演化。我们展示了包括稀疏约束正则化有助于选择一个小但重要的一组反应坐标。

    Identifying reaction coordinates(RCs) is an active area of research, given the crucial role RCs play in determining the progress of a chemical reaction. The choice of the reaction coordinate is often based on heuristic knowledge. However, an essential criterion for the choice is that the coordinate should capture both the reactant and product states unequivocally. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. Also, the coordinate should be the slowest one so that all the other degrees of freedom can easily equilibrate along the reaction coordinate. We used a regularised sparse autoencoder, an energy-based model, to discover a crucial set of reaction coordinates. Along with discovering reaction coordinates, our model also predicts the evolution of a molecular dynamics(MD) trajectory. We showcased that including sparsity enforcing regularisation helps in choosing a small but important set of r
    
[^125]: 使用逼真的合成数据进行深度学习图像分析的SYNTA新方法 在肌肉组织病理学中的应用

    SYNTA: A novel approach for deep learning-based image analysis in muscle histopathology using photo-realistic synthetic data. (arXiv:2207.14650v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2207.14650](http://arxiv.org/abs/2207.14650)

    SYNTA是一种使用逼真的合成数据进行深度学习图像分析的新方法，可以在肌肉组织病理学中进行强大且专业水平的分割任务，无需手动注释。

    

    人工智能(AI)、机器学习和深度学习(DL)方法在生物医学图像分析领域变得越来越重要。然而，要发挥这些方法的全部潜力，需要大量代表性的实验获取图像，包含大量手动注释的对象作为训练数据。在这里，我们介绍了SYNTA（合成数据）作为一种用于生成合成的、逼真的、高度复杂的生物医学图像作为DL系统训练数据的新方法。我们展示了我们的方法在组织学截面中肌纤维和结缔组织分析的背景下的多功能性。我们证明，仅使用合成训练数据，在以前未见的真实世界数据上可以进行稳健和专家级的分割任务，而无需手动注释。作为一种完全参数化技术，我们的方法提供了一种可解释和可控的替代方案，而不需要生成对抗网络。

    Artificial intelligence (AI), machine learning, and deep learning (DL) methods are becoming increasingly important in the field of biomedical image analysis. However, to exploit the full potential of such methods, a representative number of experimentally acquired images containing a significant number of manually annotated objects is needed as training data. Here we introduce SYNTA (synthetic data) as a novel approach for the generation of synthetic, photo-realistic, and highly complex biomedical images as training data for DL systems. We show the versatility of our approach in the context of muscle fiber and connective tissue analysis in histological sections. We demonstrate that it is possible to perform robust and expert-level segmentation tasks on previously unseen real-world data, without the need for manual annotations using synthetic training data alone. Being a fully parametric technique, our approach poses an interpretable and controllable alternative to Generative Adversaria
    
[^126]: 人工智能的新领域：设备上的AI训练和个性化

    A New Frontier of AI: On-Device AI Training and Personalization. (arXiv:2206.04688v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04688](http://arxiv.org/abs/2206.04688)

    基于深度学习的智能服务现已开始在设备上执行，以保护个人数据并降低云开销，而研究者提出了一种轻量级的设备上训练框架NNTrainer，可以在不牺牲准确性的同时减少内存消耗并实现智能服务的个性化。

    

    现代消费电子设备开始在设备上执行基于深度学习的智能服务，而不是在云服务器上，以保持个人数据在设备上并降低网络和云开销。我们将这样的趋势视为通过使用用户数据更新神经网络而无需将数据暴露在设备之外来个性化智能服务的机会：设备上的训练。然而，设备资源有限带来了显著的困难。我们提出了一种轻量级的设备上训练框架NNTrainer，它提供高度内存效率的神经网络训练技术，并基于细粒度执行顺序分析进行主动交换。此外，其优化不会牺牲准确性，并且对训练算法是透明的；因此，先前的算法研究可以在NNTrainer之上实现。评估结果表明，NNTrainer可以将内存消耗减少到1/20（节省95%！），并有效地个性化智能服务。

    Modern consumer electronic devices have started executing deep learning-based intelligence services on devices, not cloud servers, to keep personal data on devices and to reduce network and cloud costs. We find such a trend as the opportunity to personalize intelligence services by updating neural networks with user data without exposing the data out of devices: on-device training. However, the limited resources of devices incurs significant difficulties. We propose a light-weight on-device training framework, NNTrainer, which provides highly memory-efficient neural network training techniques and proactive swapping based on fine-grained execution order analysis for neural networks. Moreover, its optimizations do not sacrifice accuracy and are transparent to training algorithms; thus, prior algorithmic studies may be implemented on top of NNTrainer. The evaluations show that NNTrainer can reduce memory consumption down to 1/20 (saving 95%!) and effectively personalizes intelligence ser
    
[^127]: 在部分参与环境中解决分布式非凸问题的计算和通信高效方法

    A Computation and Communication Efficient Method for Distributed Nonconvex Problems in the Partial Participation Setting. (arXiv:2205.15580v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15580](http://arxiv.org/abs/2205.15580)

    这种方法在分布式非凸问题中通过方差缩减、部分参与和压缩通信的组合实现了最佳预言复杂度，同时无需所有节点参与和有界梯度假设。

    

    我们提出了一种新方法，包括分布式优化和联邦学习的三个关键组成部分：随机梯度的方差缩减、部分参与和压缩通信。我们证明了这种新方法在部分参与环境中具有最佳的预言复杂度和最先进的通信复杂度。无论通信压缩特性如何，我们的方法成功地结合了方差缩减和部分参与：我们获得了最佳的预言复杂度，无需所有节点的参与，并且不需要有界梯度（差异性）的假设。

    We present a new method that includes three key components of distributed optimization and federated learning: variance reduction of stochastic gradients, partial participation, and compressed communication. We prove that the new method has optimal oracle complexity and state-of-the-art communication complexity in the partial participation setting. Regardless of the communication compression feature, our method successfully combines variance reduction and partial participation: we get the optimal oracle complexity, never need the participation of all nodes, and do not require the bounded gradients (dissimilarity) assumption.
    
[^128]: DIRA: 一种用于动态领域增量正则化自适应的框架

    DIRA: A Framework for Dynamic Domain Incremental Regularised Adaptation. (arXiv:2205.00147v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00147](http://arxiv.org/abs/2205.00147)

    DIRA是一个用于DNN分类器的动态领域自适应的框架，使用正则化技术来解决灾难性遗忘问题，并通过少量样本实现重新训练和适应性。

    

    自主系统（AS）经常使用深度神经网络（DNN）分类器，使它们能够在复杂、高维、非线性和动态变化的环境中运行。由于这些环境的复杂性，当DNN分类器面对开发过程中未识别的领域时，可能会在操作过程中输出错误分类。随着AS的数量增加，将系统从运行中移除进行重新训练变得不切实际。为增加AS的可靠性并克服这一限制，DNN分类器需要在操作过程中适应不同的操作领域，并能够使用少量样本（例如100个样本）进行重新训练。然而，已知在少量样本上重新训练DNN会导致灾难性遗忘。在本文中，我们介绍了一种名为动态增量正则化自适应（DIRA）的框架，用于使用正则化技术来实现DNN分类器的操作领域适应，从而克服灾难性遗忘并实现适应性。

    Autonomous systems (AS) often use Deep Neural Network (DNN) classifiers to allow them to operate in complex, high dimensional, non-linear, and dynamically changing environments. Due to the complexity of these environments, DNN classifiers may output misclassifications during operation when they face domains not identified during development. Removing a system from operation for retraining becomes impractical as the number of such AS increase. To increase AS reliability and overcome this limitation, DNN classifiers need to have the ability to adapt during operation when faced with different operational domains using a few samples (e.g. 100 samples). However, retraining DNNs on a few samples is known to cause catastrophic forgetting. In this paper, we introduce Dynamic Incremental Regularised Adaptation (DIRA), a framework for operational domain adaption of DNN classifiers using regularisation techniques to overcome catastrophic forgetting and achieve adaptation when retraining using few
    
[^129]: 用强化学习改进人类的顺序决策

    Improving Human Sequential Decision-Making with Reinforcement Learning. (arXiv:2108.08454v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.08454](http://arxiv.org/abs/2108.08454)

    该论文提出了一种利用机器学习算法从追踪数据中提取"最佳实践"并向人类传达的方法，以改进人类的顺序决策。通过一系列实验证实，在虚拟厨房管理任务中，这种方法能够显著提高性能。

    

    工作者花费大量时间学习如何做出正确决策。然而，评估一个给定决策的有效性可能很复杂，例如，决策结果通常是长期的，并且与原始决策之间存在复杂的关系。令人惊讶的是，尽管学习良好的决策策略很困难，但它们通常可以简洁明了地表达。针对顺序决策，我们设计了一种新颖的机器学习算法，能够从追踪数据中提取“最佳实践”并以可解释的“提示”的形式传达其见解给人类。我们的算法选择最佳的提示，以填补人类工作者采取的行动与最优策略之间的差距，同时考虑哪些行动对于提高性能具有重要意义。我们通过一系列随机对照实验评估了我们的方法，参与者管理一个虚拟厨房。我们的实验证明，提示可以显著提高性能。

    Workers spend a significant amount of time learning how to make good decisions. Evaluating the efficacy of a given decision, however, can be complicated -- e.g., decision outcomes are often long-term and relate to the original decision in complex ways. Surprisingly, even though learning good decision-making strategies is difficult, they can often be expressed in simple and concise forms. Focusing on sequential decision-making, we design a novel machine learning algorithm that is capable of extracting "best practices" from trace data and conveying its insights to humans in the form of interpretable "tips". Our algorithm selects the tip that best bridges the gap between the actions taken by human workers and those taken by the optimal policy in a way that accounts for which actions are consequential for achieving higher performance. We evaluate our approach through a series of randomized controlled experiments where participants manage a virtual kitchen. Our experiments show that the tip
    
[^130]: 如何避免机器学习陷阱：学术研究人员指南

    How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.02497](http://arxiv.org/abs/2108.02497)

    该论文总结了在机器学习中常见的错误，并提供了避免这些错误的方法和指南。其中包括在模型构建之前的准备工作、可靠地构建模型、稳健地评估模型、公平比较模型以及报告结果等五个关键阶段。

    

    本文概述了使用机器学习时常见的一些错误，以及如何避免它们。虽然对于具有基本机器学习技术理解的任何人都应该易于理解，但它最初是为研究生撰写的，并关注学术研究中特别关注的问题，例如进行严格比较和得出有效结论的需求。它涵盖了机器学习过程的五个阶段：如何在模型构建之前进行准备，如何可靠地构建模型，如何稳健地评估模型，如何公平比较模型，以及如何报告结果。

    This document outlines some of the common mistakes that occur when using machine learning, and what can be done to avoid them. Whilst it should be accessible to anyone with a basic understanding of machine learning techniques, it was originally written for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.
    

