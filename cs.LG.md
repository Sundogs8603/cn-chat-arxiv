# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [What User Behaviors Make the Differences During the Process of Visual Analytics?.](http://arxiv.org/abs/2311.00690) | 本文通过研究用户行为的数据采集和时间序列分类方法分析了视觉分析过程中的用户行为差异，揭示了用户行为的不同特征。 |
| [^2] | [Transformers are Efficient In-Context Estimators for Wireless Communication.](http://arxiv.org/abs/2311.00226) | 这项研究提出了一种新的方法，利用上下文估计来解决无线通信中的问题。传统方法忽略了信道的层次结构，而本研究利用了Transformers在上下文学习方面的优势，通过少量提示来实现了准确的传输符号估计。 |
| [^3] | [Locally Optimal Best Arm Identification with a Fixed Budget.](http://arxiv.org/abs/2310.19788) | 该研究解决了识别具有最高预期效果的治疗方案的问题，并提出了具有固定预算的局部最优算法来降低错误识别的概率。 |
| [^4] | [Improving Intrinsic Exploration by Creating Stationary Objectives.](http://arxiv.org/abs/2310.18144) | 该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。 |
| [^5] | [Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle.](http://arxiv.org/abs/2310.17378) | 本文通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限，通过限制网络梯度对于输入数据在优化轨迹上的扰动的灵敏度，不显式地依赖网络的深度。 |
| [^6] | [Towards Graph Foundation Models: A Survey and Beyond.](http://arxiv.org/abs/2310.11829) | 本文提出了图基础模型（GFMs）的概念，并对其关键特征和技术进行了全面阐述。同时，将现有GFMs工作分为三个类别，为进一步研究和开发图学习范式奠定了基础。 |
| [^7] | [Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation.](http://arxiv.org/abs/2310.11730) | 本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。 |
| [^8] | [Understanding Fairness Surrogate Functions in Algorithmic Fairness.](http://arxiv.org/abs/2310.11211) | 本文研究了算法公平性中的公平性代理函数，并发现了代理和公平性定义之间存在一个差距。这个差距决定了一个代理函数能否适当替代一个公平性定义。 |
| [^9] | [Classification of Spam URLs Using Machine Learning Approaches.](http://arxiv.org/abs/2310.05953) | 该研究使用机器学习模型对URL进行垃圾和非垃圾的分类，发现bagging模型具有最高的准确率，为96.5%。 |
| [^10] | [Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization.](http://arxiv.org/abs/2310.03234) | 本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。 |
| [^11] | [Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors.](http://arxiv.org/abs/2310.02980) | 本文研究表明使用随机初始化会导致对架构差异的严重高估，而使用标准消噪目标进行预训练可以在多种架构上实现显著的性能提升，并将Transformers与状态空间模型之间的差距缩小到很小。与之前的研究不同的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上的性能与S4相匹配，并且在PathX-256任务上改进了SSMs的最佳结果20个百分点。 |
| [^12] | [De Novo Drug Design with Joint Transformers.](http://arxiv.org/abs/2310.02066) | 提出了联合Transformer来解决全新药物设计的困难，同时生成新颖分子并预测目标属性。使用惩罚对数似然目标训练模型，在分子生成和预测精度方面取得了最先进的性能，相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。通过联合Transformer生成的新颖分子在全新药物设计中优于其他基于SMILES的优化方法。 |
| [^13] | [Space-Time Attention with Shifted Non-Local Search.](http://arxiv.org/abs/2309.16849) | 本文提出了一种名为偏移非局部搜索的方法，通过结合非局部搜索的质量和预测的偏移范围进行搜索，以纠正小的空间误差。与先前的工作相比，我们的方法在内存消耗上减少了10倍以上，并且速度提高了3倍以上。 |
| [^14] | [Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery.](http://arxiv.org/abs/2309.15400) | 通过AI驱动的知识发现技术，首次揭示了地形特征与降水模式之间的复杂关系，并在1995年左右揭示了一个显著的地形与降水关系转变现象。 |
| [^15] | [Neural Operators for Accelerating Scientific Simulations and Design.](http://arxiv.org/abs/2309.15325) | 本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。 |
| [^16] | [Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data.](http://arxiv.org/abs/2309.13409) | 本研究提出了一种利用分数差分来捕捉时间序列数据中短期和长期依赖关系的预测策略。通过将FD应用于金融数据并结合情感分析，实证结果证明FD在二元分类中的性能优于整数差分方法。 |
| [^17] | [Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation.](http://arxiv.org/abs/2309.12742) | 本文提出了一种名为"不变一致性学习"（ICON）的方法，通过给予源域和目标域相等的地位，学习一个不变的分类器，从而消除了在目标域中的虚假相关性不一致。 |
| [^18] | [FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning.](http://arxiv.org/abs/2309.08420) | 提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。 |
| [^19] | [Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy.](http://arxiv.org/abs/2309.06838) | 本研究提出了监督机器学习和基于物理的机器学习相结合的方法，用于预测搅拌摩擦增材制造中的峰值温度分布。实验结果表明，集成的机器学习方法在预测中表现出了较好的性能，最佳的SML方法为梯度提升法，最低的均方误差为165.78。 |
| [^20] | [ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation.](http://arxiv.org/abs/2309.05853) | 这项工作提出了一种新颖而高效的半监督主动学习方法，通过策略性地操作样本空间表示，可以使生成模型相对于客观函数进行微调。在目标分子生成的背景下，通过在化学空间代理内部策略性地操作，实现了最大化生成分子与蛋白质靶标之间吸引力相互作用的能力。 |
| [^21] | [The Effect of Intrinsic Dimension on Metric Learning under Compression.](http://arxiv.org/abs/2309.05751) | 本论文研究了内在维度对压缩下的度量学习的影响，提出了在对数据进行随机压缩后在低维空间内训练全秩度量的方法。理论保证了在不依赖环境维度的情况下，度量学习的误差可以被控制，并且在存在良性几何结构时效果更好。 |
| [^22] | [A Function Interpretation Benchmark for Evaluating Interpretability Methods.](http://arxiv.org/abs/2309.03886) | 本文介绍了一个用于评估自动解释性方法的基准套件，该套件包括了类似于传统系统组件的函数。 |
| [^23] | [A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation.](http://arxiv.org/abs/2309.02539) | 本论文提出了一种通用带通神经网络用于电影音频源分离，通过使用心理声学的频率尺度来定义频带并且利用共同编码器结构的信息共享特性提高了分离性能。 |
| [^24] | [BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge.](http://arxiv.org/abs/2308.16458) | BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。 |
| [^25] | [Learning Structure-from-Motion with Graph Attention Networks.](http://arxiv.org/abs/2308.15984) | 本文通过使用图注意力网络，将传统的学习结构运动问题中的子问题替换为直接学习模型，实现了对新序列的快速推断，提高了结构运动的学习性能。 |
| [^26] | [When Do Program-of-Thoughts Work for Reasoning?.](http://arxiv.org/abs/2308.15452) | 提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。 |
| [^27] | [Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates.](http://arxiv.org/abs/2308.14831) | 本文通过实证研究探究了持续学习中不同动态稀疏训练（DST）组件的影响，旨在填补关键研究空白并为持续学习下的DST提供最佳配置指导。 |
| [^28] | [Planning in the imagination: High-level planning on learned abstract search spaces.](http://arxiv.org/abs/2308.08693) | 本论文提出了一个名为PiZero的新方法，该方法使代理能够在自己创建的抽象搜索空间中进行高级规划，不受真实环境限制，可在任意时间尺度进行规划，并处理连续动作空间和部分可观察性的设置。在多个领域的实验中，该方法胜过可比的先前方法而无需假设访问环境模拟器。 |
| [^29] | [Fluid Property Prediction Leveraging AI and Robotics.](http://arxiv.org/abs/2308.02715) | 这项工作提出了一种基于视觉的方法来估计液体的粘度，通过学习视频中不同液体振荡模式的潜在表示，可以从视觉上推断液体的类别或动态粘度。 |
| [^30] | [Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data.](http://arxiv.org/abs/2308.01729) | 本论文提出了一种基于联合精算神经网络框架的横断面和纵向索赔计数模型，通过结合传统精算模型和神经网络，充分利用了两个模型的优势。 |
| [^31] | [Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach.](http://arxiv.org/abs/2307.13771) | 本文通过添加预训练模块，在差分隐私逻辑回归中提高了模型的准确性。 |
| [^32] | [Adversarial Training Over Long-Tailed Distribution.](http://arxiv.org/abs/2307.10205) | 本文研究了在长尾分布数据集上的敌对训练，提出了一种新的敌对训练框架--重新平衡敌对训练（REAT）。REAT能够解决敌对训练过程中产生的不平衡问题，有效增强模型的鲁棒性并保持准确性。 |
| [^33] | [Minimal Random Code Learning with Mean-KL Parameterization.](http://arxiv.org/abs/2307.07816) | 本文研究了最小随机编码学习（MIRACLE）的两个变体，提出了一种新的参数化方法Mean-KL，在压缩变分贝叶斯神经网络中实现了更快的收敛和良好的预测性能。 |
| [^34] | [Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network.](http://arxiv.org/abs/2307.07566) | 本研究开发了一个深度学习模型，可从右到左和头到脚的心搏图成分中预测背腹方向的信号。使用长短期记忆网络进行训练和验证，模型表现良好，均方误差为0.09。 |
| [^35] | [Optimal Scalarizations for Sublinear Hypervolume Regret.](http://arxiv.org/abs/2307.03288) | 研究了用于亚线性超体积遗憾度量的最优标量化方法，证明了具有均匀随机权重的超体积标量化方法在最小化超体积遗憾方面是最优的，并在多目标随机线性赌博机问题上进行了案例研究。 |
| [^36] | [Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation.](http://arxiv.org/abs/2307.02842) | 这篇论文研究了一种新颖的风险敏感强化学习方法，通过迭代条件风险价值目标以及线性和一般函数逼近方法，实现了安全性保证，并提出了高效的算法。通过对于不同逼近方法的实验结果，验证了算法的有效性和优越性。 |
| [^37] | [Large Language Models Empowered Autonomous Edge AI for Connected Intelligence.](http://arxiv.org/abs/2307.02779) | 大型语言模型赋能连接智能的自主边缘AI系统，通过云-边缘-客户端的分层架构和强大的GPT模型能力，实现高质量、低延迟、隐私保护的AI服务，满足用户个人需求并实现自动化。 |
| [^38] | [How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model.](http://arxiv.org/abs/2307.02129) | 本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。 |
| [^39] | [Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning.](http://arxiv.org/abs/2307.01708) | 本文研究了风险敏感强化学习中的分布模型等效性问题。我们提出了两种新的模型等价性概念，并展示了如何将这些概念应用于增强任何基于模型的风险敏感算法。 |
| [^40] | [RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark.](http://arxiv.org/abs/2306.17100) | RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。 |
| [^41] | [Adversarial Training for Graph Neural Networks.](http://arxiv.org/abs/2306.15427) | 该论文通过克服图学习设置的限制，引入可学习图扩散的灵活GNNs以及针对结构扰动的攻击方法，证明了对抗训练是针对对抗结构扰动的最先进防御方法。 |
| [^42] | [Trading-off price for data quality to achieve fair online allocation.](http://arxiv.org/abs/2306.13440) | 本文提出了一种多臂老虎机算法，该算法权衡了价格和数据质量，以解决在线分配问题中的公平性，使得对于源的选择，算法可以适应各种公平概念，具有一定的代价可以减少公平惩罚，并且证明了算法具有较小的遗憾界。 |
| [^43] | [Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference.](http://arxiv.org/abs/2306.12509) | 本文提出了一种称为深度语言网络（DLN）的架构，通过联合训练叠加的语言模型层（LLMs），使用变分推断算法进行提示训练，使得DLN-2的性能甚至可以与少量训练数据的GPT-4相媲美。 |
| [^44] | [EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations.](http://arxiv.org/abs/2306.12059) | 本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。 |
| [^45] | [Private Federated Frequency Estimation: Adapting to the Hardness of the Instance.](http://arxiv.org/abs/2306.09396) | 本论文提出了一种新的混合sketching算法并解决了一个基本问题：如何根据底层问题的难度设置sketch size。 |
| [^46] | [Augmentation-aware Self-supervised Learning with Guided Projector.](http://arxiv.org/abs/2306.06082) | 本文提出了一种名为CASSLE的方法，它通过修改自监督学习中的有向投影网络，利用增强信息来提高模型处理图像特征的鲁棒性。 |
| [^47] | [Interpreting and Improving Diffusion Models Using the Euclidean Distance Function.](http://arxiv.org/abs/2306.04848) | 本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。 |
| [^48] | [Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion.](http://arxiv.org/abs/2306.04633) | 本文提出了一种利用二维预训练模型，通过慢-快对比融合实现三维物体实例分割的方法，在大量物体的场景中具有可扩展性，同时不需要物体数量的上界限制，通过限制多视角一致性，提出了一个新的半真实数据集(Messy Rooms)。 |
| [^49] | [ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis.](http://arxiv.org/abs/2306.04527) | ContriMix是一种无需标识和手工调优的领域泛化技术，在显微镜图像中通过分离和学习生成合成图像的方式，解决了领域泛化的问题。 |
| [^50] | [Coneheads: Hierarchy Aware Attention.](http://arxiv.org/abs/2306.00392) | “锥注意力”是点积注意力的替代方案，它通过基于双曲锥的层次结构联系数据点，明确建模了真实世界数据集的复杂结构属性，提高了任务级性能并实现了优于点积注意力的效果，而且参数量更少。 |
| [^51] | [Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels.](http://arxiv.org/abs/2305.19518) | 本论文提出了一种基于标签检索的扩散模型，用于有效构造并利用伪干净标签，以减小嘈杂标签对分类影响。 |
| [^52] | [Hierarchical Graph Generation with $K^2$-trees.](http://arxiv.org/abs/2305.19125) | 本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。 |
| [^53] | [SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search.](http://arxiv.org/abs/2305.14912) | SVDinsTN是一种高效的张量网络表示方法，通过在完全连接的张量网络中插入对角因子，同时计算张量核和对角因子，从而实现最紧凑的TN结构。与现有的TN-SS方法相比，SVDinsTN在速度方面加快了10到10^3倍，并且保持了相当的水平。 |
| [^54] | [Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection.](http://arxiv.org/abs/2305.14735) | 本文提出了一种基于异常值的方法，用于识别在毒性检测中受到伤害的人群，发现对于这些异常值，模型性能较差，他们面临的毒性更高。 |
| [^55] | [SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes.](http://arxiv.org/abs/2305.13998) | SMT 2.0是一个开源的代理模型工具包，引入了处理混合变量和层次变量的能力，并通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。 |
| [^56] | [Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach.](http://arxiv.org/abs/2305.13289) | 本文提出了一种分布鲁棒优化 (DRO)的方法，用于解决离线强化学习中的数据有限性和分布转移问题。通过直接建模转移核的不确定性，并寻找在不确定性集合中最优化最坏情况下的性能的策略，实现了极小极大最优性。 |
| [^57] | [Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL.](http://arxiv.org/abs/2305.11032) | 本文提出了一种乐观自然策略梯度的在线强化学习策略优化框架，采用乐观策略评估子程序以鼓励探索，适用于线性MDP，样本复杂度具有最优维度依赖关系。 |
| [^58] | [Tree of Thoughts: Deliberate Problem Solving with Large Language Models.](http://arxiv.org/abs/2305.10601) | 本研究提出了一种新的推理框架——思维之树（ToT），可以增强语言模型的问题解决能力，帮助语言模型进行深思熟虑的决策，以及自我评估和全局选择。 |
| [^59] | [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision.](http://arxiv.org/abs/2305.03047) | 这篇论文提出了SELF-ALIGN方法，使用基于原则的推理和LLMs的生成能力以最少的人类监督实现AI代理的自我对齐。 |
| [^60] | [Stream Efficient Learning.](http://arxiv.org/abs/2305.02217) | 本文介绍了“流高效学习”的概念，该概念旨在解决从数据流中机器学习的效率问题，其泛化性能不仅取决于接收到了多少数据，而且还取决于有多少数据能够及时有效地被利用，加上资源和速度的考虑。 |
| [^61] | [Debiasing Conditional Stochastic Optimization.](http://arxiv.org/abs/2304.10613) | 本文提出了一种通用的随机外推技术，用于降低条件随机优化问题中的偏差，并证明在非凸光滑目标函数中，将外推与方差缩减技术相结合可以显著改善样本复杂度。 |
| [^62] | [Deep Reinforcement Learning Using Hybrid Quantum Neural Network.](http://arxiv.org/abs/2304.10159) | 该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。 |
| [^63] | [Long-term Forecasting with TiDE: Time-series Dense Encoder.](http://arxiv.org/abs/2304.08424) | TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。 |
| [^64] | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.](http://arxiv.org/abs/2303.17580) | 用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。 |
| [^65] | [Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks.](http://arxiv.org/abs/2303.16563) | 本论文提出了一种基于技能强化学习和规划的 Minecraft 开放式任务解决方案，通过学习基本技能和技能规划的方法有效提高了任务解决效率，实现了24个不同的任务并在大多数任务中优于基线算法。 |
| [^66] | [Robust Evaluation of Diffusion-Based Adversarial Purification.](http://arxiv.org/abs/2303.09051) | 本文分析了对基于扩散式净化方法的评估方式，并提出了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。同时，我们提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。 |
| [^67] | [Prefix-tree Decoding for Predicting Mass Spectra from Molecules.](http://arxiv.org/abs/2303.06470) | 本文提出了一种基于前缀树的中间策略，通过将质谱视为化学公式的集合来预测分子的质谱，克服了化学子公式的组合可能性。 |
| [^68] | [Enabling Non-Linear Quantum Operations through Variational Quantum Splines.](http://arxiv.org/abs/2303.04788) | 本文提出了一种新方法——广义QSplines，使用混合量子-经典计算来近似非线性量子激活函数，克服了原始QSplines在量子硬件方面的高要求，并适合嵌入现有的量子神经网络架构中。 |
| [^69] | [On the Risks of Stealing the Decoding Algorithms of Language Models.](http://arxiv.org/abs/2303.04729) | 这项工作首次展示，一个拥有典型API访问权限的对手可以以极低的金钱成本窃取GPT-2和GPT-3等LM的解码算法的类型和超参数。 |
| [^70] | [Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples.](http://arxiv.org/abs/2302.04440) | 本文提出了一种新的特征似然分数（FLS）评估深度生成模型泛化能力的度量指标，用于评估生成样本的新颖性、保真度和多样性，通过实验证明其优于现有的指标，并能够检测出过拟合问题。 |
| [^71] | [Convolutional Neural Operators for robust and accurate learning of PDEs.](http://arxiv.org/abs/2302.01178) | 该论文提出了卷积神经算子（CNO），它能够在学习偏微分方程的上下文中处理函数输入和输出，并且证明了 CNOs 可以近似偏微分方程中出现的算子到所需的精度。在测试中，CNOs 显着优于基线，这为鲁棒准确操作学习的另一种框架铺平了道路。 |
| [^72] | [Curvilinear object segmentation in medical images based on ODoS filter and deep learning network.](http://arxiv.org/abs/2301.07475) | 该论文提出了一种基于 ODoS 滤波器和深度学习网络的曲线结构分割框架，用于解决医学图像中曲线对象的自动分割问题。 |
| [^73] | [Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers.](http://arxiv.org/abs/2212.12474) | 本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。 |
| [^74] | [Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning.](http://arxiv.org/abs/2212.03181) | 本文提出了一种基于漏斗函数的强化学习算法，用于在连续状态空间中学习鲁棒满足信号时态逻辑规范的时间依赖策略。 |
| [^75] | [Highly over-parameterized classifiers generalize since bad solutions are rare.](http://arxiv.org/abs/2211.03570) | 本文研究发现，在过度参数化的情况下，零训练误差的全局最小值中“坏”方案的占比随训练数据的增加而指数级递减，并能解释高度参数化的神经网络具有出乎意料的良好泛化能力。 |
| [^76] | [Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation.](http://arxiv.org/abs/2211.02658) | 机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。 |
| [^77] | [Learning image representations for anomaly detection: application to discovery of histological alterations in drug development.](http://arxiv.org/abs/2210.07675) | 该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。 |
| [^78] | [What does a platypus look like? Generating customized prompts for zero-shot image classification.](http://arxiv.org/abs/2209.03320) | 本文介绍了一种通过结合大型语言模型和开放词汇模型生成自定义提示的方法，以提高零样本图像分类的准确性和效率。通过利用语言模型生成描述性句子的方式，这种方法能够捕捉图像类别的重要区分特征。 |
| [^79] | [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective.](http://arxiv.org/abs/2202.08063) | 低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。 |

# 详细

[^1]: 用户行为在视觉分析过程中的差异是什么？

    What User Behaviors Make the Differences During the Process of Visual Analytics?. (arXiv:2311.00690v1 [cs.HC])

    [http://arxiv.org/abs/2311.00690](http://arxiv.org/abs/2311.00690)

    本文通过研究用户行为的数据采集和时间序列分类方法分析了视觉分析过程中的用户行为差异，揭示了用户行为的不同特征。

    

    对视觉分析过程的理解可以从多个方面受益于可视化研究人员，包括改进可视化设计和开发先进的交互功能。然而，由于感知的复杂性和我们对相关用户行为的缺乏了解，用户行为的日志文件仍然难以分析。本文提出了一个关于用户行为的全面数据采集的研究，并结合时间序列分类方法进行分析。我们选择了一个经典的可视化应用，Covid-19数据分析，涵盖地理空间、时间序列和多属性的常见分析任务。我们的用户研究收集了关于多个可视化任务的用户行为，使用了两个可比较的系统，桌面和沉浸式可视化。我们总结了两个尺度上使用三种时间序列机器学习算法的分类结果，并探索了行为特征的影响。我们的结果揭示了用户行为的差异。

    The understanding of visual analytics process can benefit visualization researchers from multiple aspects, including improving visual designs and developing advanced interaction functions. However, the log files of user behaviors are still hard to analyze due to the complexity of sensemaking and our lack of knowledge on the related user behaviors. This work presents a study on a comprehensive data collection of user behaviors, and our analysis approach with time-series classification methods. We have chosen a classical visualization application, Covid-19 data analysis, with common analysis tasks covering geo-spatial, time-series and multi-attributes. Our user study collects user behaviors on a diverse set of visualization tasks with two comparable systems, desktop and immersive visualizations. We summarize the classification results with three time-series machine learning algorithms at two scales, and explore the influences of behavior features. Our results reveal that user behaviors c
    
[^2]: Transformers是无线通信中高效的上下文估计器

    Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v1 [eess.SP])

    [http://arxiv.org/abs/2311.00226](http://arxiv.org/abs/2311.00226)

    这项研究提出了一种新的方法，利用上下文估计来解决无线通信中的问题。传统方法忽略了信道的层次结构，而本研究利用了Transformers在上下文学习方面的优势，通过少量提示来实现了准确的传输符号估计。

    

    预训练的Transformers可以进行上下文学习，在只有少量提示的情况下，适应新的任务，而不需要任何显式的模型优化。受到这个属性的启发，我们提出了一种新的方法，称为上下文估计，用于估计从接收到的符号中的传输符号的经典通信问题。通信信道本质上是一个将传输符号映射到接收符号的噪声函数，这个函数可以由一个未知参数表示，其统计数据依赖于一个（也是未知的）潜在上下文。传统方法忽略了这种层次结构，只是试图使用已知的传输信号进行最小二乘估计，然后用于估计连续的未知传输符号。我们建立了基本联系，即Transformers在少量提示下展示出出色的上下文序列完成能力，因此它们应该能够隐式确定...

    Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine t
    
[^3]: 具有固定预算的局部最优最佳臂识别算法

    Locally Optimal Best Arm Identification with a Fixed Budget. (arXiv:2310.19788v1 [math.ST])

    [http://arxiv.org/abs/2310.19788](http://arxiv.org/abs/2310.19788)

    该研究解决了识别具有最高预期效果的治疗方案的问题，并提出了具有固定预算的局部最优算法来降低错误识别的概率。

    

    本研究探讨了识别最佳治疗方案的问题，即具有最高预期效果的治疗方案。我们旨在通过降低错误识别的概率来确定最佳治疗方案，这一问题在许多研究领域中已被探索，包括最佳臂识别（Best Arm Identification，BAI）和序列优化。在我们的实验中，治疗分配的轮数是固定的。在每一轮中，决策者将一种治疗方案分配给一个实验单元，并观察相应的结果，该结果遵循不同治疗方案之间方差不同的高斯分布。在实验结束时，我们根据观察结果推荐一种治疗方案作为最佳治疗方案的估计值。决策者的目标是设计一个实验，使错误识别最佳治疗方案的概率最小化。基于这一目标，我们开发了误识别概率的下界。

    This study investigates the problem of identifying the best treatment arm, a treatment arm with the highest expected outcome. We aim to identify the best treatment arm with a lower probability of misidentification, which has been explored under various names across numerous research fields, including \emph{best arm identification} (BAI) and ordinal optimization. In our experiments, the number of treatment-allocation rounds is fixed. In each round, a decision-maker allocates a treatment arm to an experimental unit and observes a corresponding outcome, which follows a Gaussian distribution with a variance different among treatment arms. At the end of the experiment, we recommend one of the treatment arms as an estimate of the best treatment arm based on the observations. The objective of the decision-maker is to design an experiment that minimizes the probability of misidentifying the best treatment arm. With this objective in mind, we develop lower bounds for the probability of misident
    
[^4]: 通过创建固定目标来改进内在探索

    Improving Intrinsic Exploration by Creating Stationary Objectives. (arXiv:2310.18144v1 [cs.LG])

    [http://arxiv.org/abs/2310.18144](http://arxiv.org/abs/2310.18144)

    该论文提出了一个新的方法：通过创建固定目标，将原始的非固定奖励转化为固定奖励，从而改善了强化学习中的内在探索。

    

    强化学习中的探索奖励通过定义自定义的内在目标来引导长期探索。基于计数的方法使用状态访问频率来获得探索奖励。本文发现，任何从基于计数的方法导出的内在奖励函数都是非固定的，因此为代理人构建了一个难以优化的目标。我们工作的关键贡献在于通过增强状态表示将原始的非固定奖励转化为固定奖励。为此，我们引入了用于探索的固定目标（SOFE）框架。SOFE需要识别不同探索奖励的足够统计量，并找到一种将这些统计量高效编码作为深度网络输入的方法。SOFE基于提出扩展状态空间的状态增强，但有希望简化代理目标的优化。我们的实验结果表明，SOFE改善了探索效果。

    Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives. Count-based methods use the frequency of state visits to derive an exploration bonus. In this paper, we identify that any intrinsic reward function derived from count-based methods is non-stationary and hence induces a difficult objective to optimize for the agent. The key contribution of our work lies in transforming the original non-stationary rewards into stationary rewards through an augmented state representation. For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework. SOFE requires identifying sufficient statistics for different exploration bonuses and finding an efficient encoding of these statistics to use as input to a deep network. SOFE is based on proposing state augmentations that expand the state space but hold the promise of simplifying the optimization of the agent's objective. Our experiments show that SOFE improves the
    
[^5]: 基于切向空间中的灵敏度的ReLU网络的优化相关泛化界限

    Optimization dependent generalization bound for ReLU networks based on sensitivity in the tangent bundle. (arXiv:2310.17378v1 [cs.LG])

    [http://arxiv.org/abs/2310.17378](http://arxiv.org/abs/2310.17378)

    本文通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限，通过限制网络梯度对于输入数据在优化轨迹上的扰动的灵敏度，不显式地依赖网络的深度。

    

    近年来，深度学习取得了一些非常有希望的结果，对于深度神经网络的泛化能力，然而文献仍然缺乏一个全面的理论解释为什么过度参数化的模型能够很好地泛化，同时拟合训练数据。在本文中，我们通过估计梯度下降得到的初始参数向量中可用的网络集合的Rademacher复杂度，提出了一个关于前向ReLU网络泛化误差的PAC类型界限。关键思想是将网络梯度对于输入数据在优化轨迹上的扰动的灵敏度限制在一个界限内。所得到的界限不显式地依赖网络的深度。我们的结果在MNIST和CIFAR-10数据集上得到实验证实。

    Recent advances in deep learning have given us some very promising results on the generalization ability of deep neural networks, however literature still lacks a comprehensive theory explaining why heavily over-parametrized models are able to generalize well while fitting the training data. In this paper we propose a PAC type bound on the generalization error of feedforward ReLU networks via estimating the Rademacher complexity of the set of networks available from an initial parameter vector via gradient descent. The key idea is to bound the sensitivity of the network's gradient to perturbation of the input data along the optimization trajectory. The obtained bound does not explicitly depend on the depth of the network. Our results are experimentally verified on the MNIST and CIFAR-10 datasets.
    
[^6]: 走向图基础模型：一项调查与进展

    Towards Graph Foundation Models: A Survey and Beyond. (arXiv:2310.11829v1 [cs.LG])

    [http://arxiv.org/abs/2310.11829](http://arxiv.org/abs/2310.11829)

    本文提出了图基础模型（GFMs）的概念，并对其关键特征和技术进行了全面阐述。同时，将现有GFMs工作分为三个类别，为进一步研究和开发图学习范式奠定了基础。

    

    基于其在自然语言处理和其他领域中的显著成功，基础模型已经成为各种人工智能应用的基本构建模块。与此同时，图机器学习经历了由浅层方法向深度学习方法的转变。基础模型的出现和同化能力引起了图机器学习研究者的兴趣，引发了关于开发下一个预训练于广泛图数据并可适应各种下游图任务的图学习范式的讨论。然而，目前对这类工作尚无明确的定义和系统的分析。在本文中，我们提出了图基础模型(GFMs)的概念，并首次对其关键特征和技术进行全面阐述。在此基础上，我们根据其可靠性将现有GFMs工作分为三个类别。

    Emerging as fundamental building blocks for diverse artificial intelligence applications, foundation models have achieved notable success across natural language processing and many other domains. Parallelly, graph machine learning has witnessed a transformative shift, with shallow methods giving way to deep learning approaches. The emergence and homogenization capabilities of foundation models have piqued the interest of graph machine learning researchers, sparking discussions about developing the next graph learning paradigm that is pre-trained on broad graph data and can be adapted to a wide range of downstream graph tasks. However, there is currently no clear definition and systematic analysis for this type of work. In this article, we propose the concept of graph foundation models (GFMs), and provide the first comprehensive elucidation on their key characteristics and technologies. Following that, we categorize existing works towards GFMs into three categories based on their relia
    
[^7]: 面向隐私保护推荐的联邦异构图神经网络

    Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])

    [http://arxiv.org/abs/2310.11730](http://arxiv.org/abs/2310.11730)

    本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。

    

    异构信息网络（HIN）通过元路径描述丰富的语义，已成为缓解推荐系统数据稀疏性的强大工具。现有的基于HIN的推荐系统持有数据的集中存储假设，并进行集中式模型训练。然而，由于隐私问题，现实世界的数据往往以分布式方式存储，导致集中式HIN推荐无法实现。本文提出将HIN分为客户端存储的私有HIN和服务器端的共享HIN。在此设置下，我们提出了一种基于联邦异构图神经网络（FedHGNN）的框架，可以在分布式HIN上协作训练推荐模型，同时不泄露用户隐私。具体而言，我们首先针对基于HIN的联合推荐，基于差分隐私的光下确定了隐私定义，旨在保护私有HIN的用户-商品交互，以及用户的隐私信息。

    Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
    
[^8]: 理解算法公平性中的公平性代理函数

    Understanding Fairness Surrogate Functions in Algorithmic Fairness. (arXiv:2310.11211v1 [cs.LG])

    [http://arxiv.org/abs/2310.11211](http://arxiv.org/abs/2310.11211)

    本文研究了算法公平性中的公平性代理函数，并发现了代理和公平性定义之间存在一个差距。这个差距决定了一个代理函数能否适当替代一个公平性定义。

    

    已观察到机器学习算法对某些人群产生偏见的预测。为了减轻这种偏见并实现可比的准确性，一种有希望的方法是引入涉及公平性定义的代理函数，并解决一个受限制的优化问题。然而，在以往的研究中，一个有趣的问题是这种公平性代理函数可能导致不公平的结果。在本研究中，为了深入理解这个问题，我们以广泛使用的公平性定义——人口统计平等——为例，从理论和实证上证明了公平性定义和公平性代理函数之间存在一个代理-公平性差距。这个"差距"直接决定了一个代理函数是否适合替代一个公平性定义。此外，关于这个"差距"的理论分析和实验结果激发了我们的兴趣，表明无限制的代理函数将受到决策边界远离的点的影响。

    It has been observed that machine learning algorithms exhibit biased predictions against certain population groups. To mitigate such bias while achieving comparable accuracy, a promising approach is to introduce surrogate functions of the concerned fairness definition and solve a constrained optimization problem. However, an intriguing issue in previous work is that such fairness surrogate functions may yield unfair results. In this work, in order to deeply understand this issue, taking a widely used fairness definition, demographic parity as an example, we both theoretically and empirically show that there is a surrogate-fairness gap between the fairness definition and the fairness surrogate function. The "gap" directly determines whether a surrogate function is an appropriate substitute for a fairness definition. Also, the theoretical analysis and experimental results about the "gap" motivate us that the unbounded surrogate functions will be affected by the points far from the decisi
    
[^9]: 使用机器学习方法进行垃圾URL的分类

    Classification of Spam URLs Using Machine Learning Approaches. (arXiv:2310.05953v1 [cs.CR])

    [http://arxiv.org/abs/2310.05953](http://arxiv.org/abs/2310.05953)

    该研究使用机器学习模型对URL进行垃圾和非垃圾的分类，发现bagging模型具有最高的准确率，为96.5%。

    

    由于提供了快速和免费的通信工具和平台，互联网每天被数十亿用户使用。然而，随着使用量的显著增加，每秒钟产生了大量的垃圾邮件，这不仅浪费了互联网资源，更重要的是浪费了用户的时间。本研究通过使用机器学习模型将URL分类为垃圾或非垃圾。我们首先从URL中提取特征，然后比较了几个模型的性能，包括k最近邻、bagging、随机森林、逻辑回归等。我们发现，bagging模型的准确率最高，达到了96.5%。这表明，bagging是一种有前景的用于将URL分类为垃圾或非垃圾的方法。

    The Internet is used by billions of users daily because it offers fast and free communication tools and platforms. Nevertheless, with this significant increase in usage, huge amounts of spam are generated every second, which wastes internet resources and, more importantly, users time. This study investigates using machine learning models to classify URLs as spam or non-spam. We first extract the features from the URL as it has only one feature, and then we compare the performance of several models, including k-nearest neighbors, bagging, random forest, logistic regression, and others. We find that bagging achieves the best accuracy, with an accuracy of 96.5%. This suggests that bagging is a promising approach for classifying URLs as spam or nonspam.
    
[^10]: 非光滑弱凸有限和耦合组合优化

    Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])

    [http://arxiv.org/abs/2310.03234](http://arxiv.org/abs/2310.03234)

    本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。

    

    本文研究了一类新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)。由于其在机器学习和人工智能领域的广泛应用以及其解决基于经验风险最小化的随机算法的局限性，FCCO引起了越来越多的关注。然而，目前对于FCCO的研究假设内外函数都是光滑的，限制了其能够解决更多种类的问题的潜力。我们的研究从非光滑弱凸FCCO的角度进行了扩展，其中外函数是弱凸且非递减的，内函数是弱凸的。我们分析了一种单循环算法，并确定其在找到Moreau环的ε-稳定点的复杂度。

    This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
    
[^11]: 永远不要从头开始训练：公正比较长序列模型需要数据驱动的先验知识

    Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. (arXiv:2310.02980v1 [cs.LG])

    [http://arxiv.org/abs/2310.02980](http://arxiv.org/abs/2310.02980)

    本文研究表明使用随机初始化会导致对架构差异的严重高估，而使用标准消噪目标进行预训练可以在多种架构上实现显著的性能提升，并将Transformers与状态空间模型之间的差距缩小到很小。与之前的研究不同的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上的性能与S4相匹配，并且在PathX-256任务上改进了SSMs的最佳结果20个百分点。

    

    建模序列之间的长程依赖一直是机器学习中的目标，并导致了一些架构，如状态空间模型，在处理长序列时比Transformers有显著的优势。然而，这些令人印象深刻的经验性进展主要是在随机初始化并通过预测输入序列的目标标签进行训练的基准测试（例如Long Range Arena）上展示出来的。在这项工作中，我们展示了随机初始化导致对架构之间差异的严重高估，并且使用标准消噪目标进行预训练（仅使用下游任务数据）可以在多种架构上实现显著的收益，并且可以在Transformers和状态空间模型（SSMs）之间得到很小的差距。与之前的研究形成鲜明对比的是，我们发现当正确预训练时，普通的Transformers在Long Range Arena上与S4的性能相匹配，并且我们在PathX-256任务上将SSMs的最佳报告结果提高了20个百分点。

    Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 
    
[^12]: 通过联合Transformer进行全新药物设计

    De Novo Drug Design with Joint Transformers. (arXiv:2310.02066v1 [cs.LG])

    [http://arxiv.org/abs/2310.02066](http://arxiv.org/abs/2310.02066)

    提出了联合Transformer来解决全新药物设计的困难，同时生成新颖分子并预测目标属性。使用惩罚对数似然目标训练模型，在分子生成和预测精度方面取得了最先进的性能，相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。通过联合Transformer生成的新颖分子在全新药物设计中优于其他基于SMILES的优化方法。

    

    全新药物设计需要同时生成超出训练数据的新颖分子并预测其目标属性，这对生成模型来说是一项艰巨任务。为了解决这个问题，我们提出了联合Transformer，它将Transformer的解码器、编码器和预测器结合为一个具有共享权重的联合生成模型。我们证明了使用惩罚对数似然目标来训练模型可以在分子生成方面实现最先进的性能，同时相比于仅微调解码器的Transformer，降低了新采样分子的预测误差42%。最后，我们提出了一种概率黑盒优化算法，通过使用联合Transformer生成具有改进目标属性的新颖分子，相比于训练数据，在全新药物设计中表现优于其他基于SMILES的优化方法。

    De novo drug design requires simultaneously generating novel molecules outside of training data and predicting their target properties, making it a hard task for generative models. To address this, we propose Joint Transformer that combines a Transformer decoder, a Transformer encoder, and a predictor in a joint generative model with shared weights. We show that training the model with a penalized log-likelihood objective results in state-of-the-art performance in molecule generation, while decreasing the prediction error on newly sampled molecules, as compared to a fine-tuned decoder-only Transformer, by 42%. Finally, we propose a probabilistic black-box optimization algorithm that employs Joint Transformer to generate novel molecules with improved target properties, as compared to the training data, outperforming other SMILES-based optimization methods in de novo drug design.
    
[^13]: 具有偏移非局部搜索的时空注意力

    Space-Time Attention with Shifted Non-Local Search. (arXiv:2309.16849v1 [cs.CV])

    [http://arxiv.org/abs/2309.16849](http://arxiv.org/abs/2309.16849)

    本文提出了一种名为偏移非局部搜索的方法，通过结合非局部搜索的质量和预测的偏移范围进行搜索，以纠正小的空间误差。与先前的工作相比，我们的方法在内存消耗上减少了10倍以上，并且速度提高了3倍以上。

    

    由于视频中物体的运动，有效计算注意力图对于视频来说是具有挑战性的。虽然标准的非局部搜索对于每个查询点周围的窗口具有高质量，但窗口的大小无法容纳运动。长程运动的方法使用辅助网络预测与每个查询位置的偏移量最相似的关键坐标。然而，准确预测此偏移的光流场仍然具有挑战性，即使对于大规模网络也是如此。小的空间不准确性会严重影响注意力模块的质量。本文提出了一种将非局部搜索的质量与预测的偏移量范围相结合的搜索策略。这种方法被命名为偏移非局部搜索，它在预测的偏移周围执行一个小的网格搜索，以纠正小的空间误差。我们的方法的原地计算消耗的内存比先前的工作少了10倍，速度快了3倍以上。在实验中，纠正了小的空间误差

    Efficiently computing attention maps for videos is challenging due to the motion of objects between frames. While a standard non-local search is high-quality for a window surrounding each query point, the window's small size cannot accommodate motion. Methods for long-range motion use an auxiliary network to predict the most similar key coordinates as offsets from each query location. However, accurately predicting this flow field of offsets remains challenging, even for large-scale networks. Small spatial inaccuracies significantly impact the attention module's quality. This paper proposes a search strategy that combines the quality of a non-local search with the range of predicted offsets. The method, named Shifted Non-Local Search, executes a small grid search surrounding the predicted offsets to correct small spatial errors. Our method's in-place computation consumes 10 times less memory and is over 3 times faster than previous work. Experimentally, correcting the small spatial err
    
[^14]: 通过AI驱动的知识发现，革新对地形降水的理解

    Revolutionizing Terrain-Precipitation Understanding through AI-driven Knowledge Discovery. (arXiv:2309.15400v1 [physics.ao-ph])

    [http://arxiv.org/abs/2309.15400](http://arxiv.org/abs/2309.15400)

    通过AI驱动的知识发现技术，首次揭示了地形特征与降水模式之间的复杂关系，并在1995年左右揭示了一个显著的地形与降水关系转变现象。

    

    在当今的气候科学中，推进我们对复杂地形地区气候过程的理解是一个重要的挑战，特别是在全球气候变化的背景下。尤其值得注意的是，在这些地区观测数据的缺乏对于理解其中微妙的气候动力学产生了显著限制。首次利用尖端的AI驱动的知识发现技术，我们发现了一些明确的方程来阐明地形特征和降水模式之间复杂关系，揭示了之前隐藏的控制这些关系的复杂性。这些迄今未披露的方程在应用于降水数据时与传统经验模型相比具有显著的准确性。在此基础上，我们揭示了一个被称为“1995年转折点”的现象，表明了大约在1995年左右地形与降水关系的显著转变。

    Advancing our understanding of climate processes in regions characterized by intricate terrain complexity is a paramount challenge in contemporary climate science, particularly in the context of global climate change. Notably, the scarcity of observational data in these regions has imposed substantial limitations on understanding the nuanced climate dynamics therein. For the first time, utilizing cutting-edge AI-driven knowledge discovery techniques, we have uncovered explicit equations that elucidate the intricate relationship between terrain features and precipitation patterns, illuminating the previously concealed complexities governing these relationships. These equations, thus far undisclosed, exhibit remarkable accuracy compared to conventional empirical models when applied to precipitation data. Building on this foundation, we reveal a phenomenon known as the '1995 turning point,' indicating a significant shift in the terrain-precipitation relationship in approximately 1995, rel
    
[^15]: 神经运算符加速科学模拟和设计

    Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])

    [http://arxiv.org/abs/2309.15325](http://arxiv.org/abs/2309.15325)

    本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。

    

    目前，科学发现和工程设计受限于物理实验的时间和成本，这些实验通常是通过试验和直觉选择的，并需要深入的领域专业知识。数值模拟是物理实验的替代方法，但对于复杂的现实领域来说，由于现有数值方法的计算需求，通常是不可行的。人工智能（AI）通过开发快速的数据驱动代理模型，提供了一个潜在的范式转变。特别是，一个称为神经运算符的AI框架提供了一个基于连续域函数之间映射学习的原则性框架，例如时空过程和偏微分方程（PDE）。它们可以在训练过程中未见过的新位置进行外推和预测解决方案，即进行零射超分辨率。神经运算符可以增强甚至替代许多应用中的现有模拟器，例如计算力学流体学。

    Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
    
[^16]: 时间序列预测：利用分数差分数据释放长期依赖关系

    Time-Series Forecasting: Unleashing Long-Term Dependencies with Fractionally Differenced Data. (arXiv:2309.13409v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13409](http://arxiv.org/abs/2309.13409)

    本研究提出了一种利用分数差分来捕捉时间序列数据中短期和长期依赖关系的预测策略。通过将FD应用于金融数据并结合情感分析，实证结果证明FD在二元分类中的性能优于整数差分方法。

    

    本研究介绍了一种新颖的预测策略，利用分数差分（FD）的能力来捕捉时间序列数据中的短期和长期依赖关系。与传统的整数差分方法不同，FD在保持系列记忆的同时稳定了它以供建模目的。通过将FD应用于来自SPY指数的金融数据，并结合新闻报道的情感分析，这个实证分析探讨了FD与目标变量的二元分类的效果。采用了监督分类算法来验证FD系列的性能。结果显示，FD相比整数差分具有优越性，这一点通过接收者操作特征/曲线下面积（ROCAUC）和马修斯相关系数（MCC）的评估得到确认。

    This study introduces a novel forecasting strategy that leverages the power of fractional differencing (FD) to capture both short- and long-term dependencies in time series data. Unlike traditional integer differencing methods, FD preserves memory in series while stabilizing it for modeling purposes. By applying FD to financial data from the SPY index and incorporating sentiment analysis from news reports, this empirical analysis explores the effectiveness of FD in conjunction with binary classification of target variables. Supervised classification algorithms were employed to validate the performance of FD series. The results demonstrate the superiority of FD over integer differencing, as confirmed by Receiver Operating Characteristic/Area Under the Curve (ROCAUC) and Mathews Correlation Coefficient (MCC) evaluations.
    
[^17]: 让UDA中的U变得重要：无监督领域自适应的不变一致性学习

    Make the U in UDA Matter: Invariant Consistency Learning for Unsupervised Domain Adaptation. (arXiv:2309.12742v1 [cs.LG])

    [http://arxiv.org/abs/2309.12742](http://arxiv.org/abs/2309.12742)

    本文提出了一种名为"不变一致性学习"（ICON）的方法，通过给予源域和目标域相等的地位，学习一个不变的分类器，从而消除了在目标域中的虚假相关性不一致。

    

    领域自适应(DA)在处理领域内部的相关特征（如类别身份）和领域特定特征（如环境）之间的虚假相关性时，总是面临挑战，这种相关性在目标领域中不具有普遍性。不幸的是，即使使用了额外的无监督目标领域，现有的无监督DA(UDA)方法仍然受到影响。这是因为源域监督只将目标域样本视为辅助数据（如通过伪标签），而目标域中有价值的去相关线索的固有分布被忽视了。我们建议通过给予两个域相等的地位，来使UDA中的U变得重要。具体而言，我们学习一个不变的分类器，其预测同时与源域标签和目标域聚类一致，从而消除了目标域中的虚假相关性不一致。我们将我们的方法称为"不变一致性学习"（ICON）。

    Domain Adaptation (DA) is always challenged by the spurious correlation between domain-invariant features (e.g., class identity) and domain-specific features (e.g., environment) that does not generalize to the target domain. Unfortunately, even enriched with additional unsupervised target domains, existing Unsupervised DA (UDA) methods still suffer from it. This is because the source domain supervision only considers the target domain samples as auxiliary data (e.g., by pseudo-labeling), yet the inherent distribution in the target domain -- where the valuable de-correlation clues hide -- is disregarded. We propose to make the U in UDA matter by giving equal status to the two domains. Specifically, we learn an invariant classifier whose prediction is simultaneously consistent with the labels in the source domain and clusters in the target domain, hence the spurious correlation inconsistent in the target domain is removed. We dub our approach "Invariant CONsistency learning" (ICON). Exte
    
[^18]: FedDCSR: 通过解缠表示学习实现联邦跨领域顺序推荐

    FedDCSR: Federated Cross-domain Sequential Recommendation via Disentangled Representation Learning. (arXiv:2309.08420v1 [cs.LG])

    [http://arxiv.org/abs/2309.08420](http://arxiv.org/abs/2309.08420)

    提出了一种名为FedDCSR的联邦跨领域顺序推荐框架，通过解缠表示学习来处理不同领域之间的序列特征异质性，并保护数据隐私。

    

    近年来，利用来自多个领域的用户序列数据的跨领域顺序推荐(CSR)受到了广泛关注。然而，现有的CSR方法需要在领域之间共享原始用户数据，这违反了《通用数据保护条例》(GDPR)。因此，有必要将联邦学习(FL)和CSR相结合，充分利用不同领域的知识，同时保护数据隐私。然而，不同领域之间的序列特征异质性对FL的整体性能有显著影响。在本文中，我们提出了FedDCSR，这是一种通过解缠表示学习的新型联邦跨领域顺序推荐框架。具体而言，为了解决不同领域之间的序列特征异质性，我们引入了一种称为领域内-领域间序列表示解缠(SRD)的方法，将用户序列特征解缠成领域共享和领域专属特征。

    Cross-domain Sequential Recommendation (CSR) which leverages user sequence data from multiple domains has received extensive attention in recent years. However, the existing CSR methods require sharing origin user data across domains, which violates the General Data Protection Regulation (GDPR). Thus, it is necessary to combine federated learning (FL) and CSR to fully utilize knowledge from different domains while preserving data privacy. Nonetheless, the sequence feature heterogeneity across different domains significantly impacts the overall performance of FL. In this paper, we propose FedDCSR, a novel federated cross-domain sequential recommendation framework via disentangled representation learning. Specifically, to address the sequence feature heterogeneity across domains, we introduce an approach called inter-intra domain sequence representation disentanglement (SRD) to disentangle the user sequence features into domain-shared and domain-exclusive features. In addition, we design
    
[^19]: 监督机器学习和基于物理的机器学习方法用于预测铝合金搅拌摩擦增材制造中的峰值温度分布

    Supervised Machine Learning and Physics based Machine Learning approach for prediction of peak temperature distribution in Additive Friction Stir Deposition of Aluminium Alloy. (arXiv:2309.06838v1 [cs.LG])

    [http://arxiv.org/abs/2309.06838](http://arxiv.org/abs/2309.06838)

    本研究提出了监督机器学习和基于物理的机器学习相结合的方法，用于预测搅拌摩擦增材制造中的峰值温度分布。实验结果表明，集成的机器学习方法在预测中表现出了较好的性能，最佳的SML方法为梯度提升法，最低的均方误差为165.78。

    

    增材搅拌摩擦沉积（AFSD）是一种新型的固态增材制造技术，它解决了传统粉末床熔炼和定向能量沉积方法中存在的孔隙率、开裂和性能各向异性等问题。然而，AFSD中的工艺参数、热量分布和得到的显微结构之间的相关性仍然不够清楚，这妨碍了性能的工艺优化。本研究运用了一种先进的框架，将监督机器学习（SML）和基于物理的神经网络（PINNs）相结合，以从工艺参数预测AFSD中的峰值温度分布。对于SML建模，使用了八种回归算法，而对于PINNs，使用了运输、波传播、热传导和量子力学的控制方程。在多个统计指标上，集成的机器学习方法表现出了较好的性能，梯度提升法是最佳的SML方法，最低的均方误差为165.78。

    Additive friction stir deposition (AFSD) is a novel solid-state additive manufacturing technique that circumvents issues of porosity, cracking, and properties anisotropy that plague traditional powder bed fusion and directed energy deposition approaches. However, correlations between process parameters, thermal profiles, and resulting microstructure in AFSD remain poorly understood. This hinders process optimization for properties. This work employs a cutting-edge framework combining supervised machine learning (SML) and physics-informed neural networks (PINNs) to predict peak temperature distribution in AFSD from process parameters. Eight regression algorithms were implemented for SML modeling, while four PINNs leveraged governing equations for transport, wave propagation, heat transfer, and quantum mechanics. Across multiple statistical measures, ensemble techniques like gradient boosting proved superior for SML, with lowest MSE of 165.78. The integrated ML approach was also applied 
    
[^20]: ChemSpaceAL:一个应用于特定蛋白质分子生成的高效主动学习方法

    ChemSpaceAL: An Efficient Active Learning Methodology Applied to Protein-Specific Molecular Generation. (arXiv:2309.05853v1 [cs.LG])

    [http://arxiv.org/abs/2309.05853](http://arxiv.org/abs/2309.05853)

    这项工作提出了一种新颖而高效的半监督主动学习方法，通过策略性地操作样本空间表示，可以使生成模型相对于客观函数进行微调。在目标分子生成的背景下，通过在化学空间代理内部策略性地操作，实现了最大化生成分子与蛋白质靶标之间吸引力相互作用的能力。

    

    生成人工智能模型的不可思议能力不可避免地导致了它们在药物发现领域的应用。因此，开发能够增强这些强大工具的能力和适用性的方法论具有巨大的兴趣。在这项工作中，我们提出了一种新颖而高效的半监督主动学习方法，该方法通过在构建的样本空间表示中策略性地操作，可以使生成模型相对于客观函数进行微调。在目标分子生成的背景下，我们展示了一种能够相对于基于吸引力相互作用评分函数进行微调的GPT基础分子生成模型的能力，通过在化学空间代理内部策略性地操作，从而最大化生成的分子与蛋白质靶标之间的吸引力相互作用。重要的是，我们的方法不需要对用于微调的所有数据点进行单独的评估。

    The incredible capabilities of generative artificial intelligence models have inevitably led to their application in the domain of drug discovery. It is therefore of tremendous interest to develop methodologies that enhance the abilities and applicability of these powerful tools. In this work, we present a novel and efficient semi-supervised active learning methodology that allows for the fine-tuning of a generative model with respect to an objective function by strategically operating within a constructed representation of the sample space. In the context of targeted molecular generation, we demonstrate the ability to fine-tune a GPT-based molecular generator with respect to an attractive interaction-based scoring function by strategically operating within a chemical space proxy, thereby maximizing attractive interactions between the generated molecules and a protein target. Importantly, our approach does not require the individual evaluation of all data points that are used for fine-
    
[^21]: 内在维度对压缩下的度量学习的影响

    The Effect of Intrinsic Dimension on Metric Learning under Compression. (arXiv:2309.05751v1 [cs.LG])

    [http://arxiv.org/abs/2309.05751](http://arxiv.org/abs/2309.05751)

    本论文研究了内在维度对压缩下的度量学习的影响，提出了在对数据进行随机压缩后在低维空间内训练全秩度量的方法。理论保证了在不依赖环境维度的情况下，度量学习的误差可以被控制，并且在存在良性几何结构时效果更好。

    

    度量学习旨在在输入空间中找到适当的距离度量，以改善基于距离的学习算法的性能。在高维环境中，度量学习还可以作为降维的手段，通过对学习的度量施加一个低秩约束。本文中，我们考虑的是对数据的一个随机压缩版本，然后在其中训练一个全秩的度量。我们给出了关于距离度量学习的误差的理论保证，这些保证不依赖于环境维度。我们的边界除了对来自有界支持的独立同分布数据没有显式的假设之外，并且在存在良性几何结构时自动收敛。在合成和真实数据集上的实验结果支持我们在高维环境中的理论发现。

    Metric learning aims at finding a suitable distance metric over the input space, to improve the performance of distance-based learning algorithms. In high-dimensional settings, metric learning can also play the role of dimensionality reduction, by imposing a low-rank restriction to the learnt metric. In this paper, instead of training a low-rank metric on high-dimensional data, we consider a randomly compressed version of the data, and train a full-rank metric there. We give theoretical guarantees on the error of distance-based metric learning, with respect to the random compression, which do not depend on the ambient dimension. Our bounds do not make any explicit assumptions, aside from i.i.d. data from a bounded support, and automatically tighten when benign geometrical structures are present. Experimental results on both synthetic and real data sets support our theoretical findings in high-dimensional settings.
    
[^22]: 一个用于评估解释性方法的功能解释基准

    A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])

    [http://arxiv.org/abs/2309.03886](http://arxiv.org/abs/2309.03886)

    本文介绍了一个用于评估自动解释性方法的基准套件，该套件包括了类似于传统系统组件的函数。

    

    使用人类可读的描述标记神经网络子模块对于许多下游任务非常有用：这些描述可以暴露失败、引导干预，甚至可以解释重要的模型行为。到目前为止，大多数基于机械原理的已训练网络描述都涉及到小模型、狭义现象，并且需要大量人力。在不断增加的模型大小和复杂性中标记出所有人可解释的子计算几乎肯定需要能够自动生成和验证描述的工具。最近，利用学习模型进行标记的技术开始受到关注，但评估其有效性的方法有限且临时。我们应该如何验证和比较开放式标记工具？本文介绍了FIND（函数解释和描述），一个用于评估自动解释方法构建模块的基准套件。FIND包含了类似于传统系统的组件的函数。

    Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of tr
    
[^23]: 一种用于电影音频源分离的通用带通神经网络

    A Generalized Bandsplit Neural Network for Cinematic Audio Source Separation. (arXiv:2309.02539v1 [eess.AS])

    [http://arxiv.org/abs/2309.02539](http://arxiv.org/abs/2309.02539)

    本论文提出了一种通用带通神经网络用于电影音频源分离，通过使用心理声学的频率尺度来定义频带并且利用共同编码器结构的信息共享特性提高了分离性能。

    

    电影音频源分离是音频源分离的一个相对较新的子任务，其目标是从混音中提取对话音轨、音乐音轨和特效音轨。在这项工作中，我们开发了一个模型，可以对频率轴的任何完全或过完备的分区进行泛化。基于心理声学的频率尺度用于确定带通的定义，现在具备冗余性以进行更可靠的特征提取。我们提出了一个损失函数，该损失函数基于信噪比和1-范数的稀疏促进属性。我们还利用共同编码器结构的信息共享特性，在训练和推断过程中减少计算复杂性，改善难以泛化的声音类别的分离性能，并在推断时提供灵活性，可轻松分离解码器。我们的最佳模型在Divide and Remaster数据集上取得了最先进的性能。

    Cinematic audio source separation is a relatively new subtask of audio source separation, with the aim of extracting the dialogue stem, the music stem, and the effects stem from their mixture. In this work, we developed a model generalizing the Bandsplit RNN for any complete or overcomplete partitions of the frequency axis. Psycho-acoustically motivated frequency scales were used to inform the band definitions which are now defined with redundancy for more reliable feature extraction. A loss function motivated by the signal-to-noise ratio and the sparsity-promoting property of the 1-norm was proposed. We additionally exploit the information-sharing property of a common-encoder setup to reduce computational complexity during both training and inference, improve separation performance for hard-to-generalize classes of sounds, and allow flexibility during inference time with easily detachable decoders. Our best model sets the state of the art on the Divide and Remaster dataset with perfor
    
[^24]: BioCoder: 一种带有上下文语用知识的生物信息学代码生成基准

    BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge. (arXiv:2308.16458v1 [cs.LG])

    [http://arxiv.org/abs/2308.16458](http://arxiv.org/abs/2308.16458)

    BioCoder是一个用于评估预训练模型在生成生物信息学代码方面的基准，涵盖了函数代码生成中的包依赖关系、类声明和全局变量，并通过模糊测试框架进行评估。

    

    预训练的语言模型（如ChatGPT）显著改进了代码生成。随着这些模型的扩大，需要输出来处理更复杂的任务的需求也越来越多。此外，在生物信息学中，生成功能程序由于领域知识量大、需要复杂的数据操作和复杂的功能依赖关系而面临额外的挑战。在这里，我们介绍了BioCoder，这是一个用于评估现有预训练模型在生成生物信息学代码方面的基准。与函数代码生成有关，BioCoder涵盖了可能的包依赖关系、类声明和全局变量。它包括来自GitHub的1026个Python和Java函数和1243个方法，以及来自Rosalind项目的253个示例。BioCoder还结合了一个用于评估的模糊测试框架，我们已经应用它来评估许多模型，包括InCoder、CodeGen、CodeGen2、SantaCoder、StarCoder、StarCoder+、InstructCodeT。

    Pre-trained language models like ChatGPT have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks. Moreover, in bioinformatics, generating functional programs poses additional notable challenges due to the amount of domain knowledge, the need for complicated data operations, and intricate functional dependencies between the operations. Here, we present BioCoder, a benchmark developed to evaluate existing pre-trained models in generating bioinformatics code. In relation to function-code generation, BioCoder covers potential package dependencies, class declarations, and global variables. It incorporates 1026 functions and 1243 methods in Python and Java from GitHub and 253 examples from the Rosalind Project. BioCoder incorporates a fuzz-testing framework for evaluation, and we have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT
    
[^25]: 使用图注意力网络学习结构运动

    Learning Structure-from-Motion with Graph Attention Networks. (arXiv:2308.15984v1 [cs.CV])

    [http://arxiv.org/abs/2308.15984](http://arxiv.org/abs/2308.15984)

    本文通过使用图注意力网络，将传统的学习结构运动问题中的子问题替换为直接学习模型，实现了对新序列的快速推断，提高了结构运动的学习性能。

    

    本文通过使用图注意力网络解决学习结构运动（SfM）的问题。SfM是一个经典的计算机视觉问题，通过迭代最小化重投影误差（称为束调整）解决，从良好的初始化开始。为了获得足够好的初始化结果，传统方法依赖于一系列子问题（如成对姿态估计、姿态平均化或三角测量），这些子问题提供了一个初始解，然后使用束调整对其进行改进。在本文中，我们通过学习一个模型，将这些子问题替换为以多个视图上检测到的2D关键点作为输入，并输出相应的相机姿态和3D关键点坐标。我们的模型利用图神经网络学习SfM特定的原语，并且我们展示它可以用于快速推断新的和未见过的序列的重建。实验结果表明，所提出的模型可以显著提高结构运动的学习性能。

    In this paper we tackle the problem of learning Structure-from-Motion (SfM) through the use of graph attention networks. SfM is a classic computer vision problem that is solved though iterative minimization of reprojection errors, referred to as Bundle Adjustment (BA), starting from a good initialization. In order to obtain a good enough initialization to BA, conventional methods rely on a sequence of sub-problems (such as pairwise pose estimation, pose averaging or triangulation) which provides an initial solution that can then be refined using BA. In this work we replace these sub-problems by learning a model that takes as input the 2D keypoints detected across multiple views, and outputs the corresponding camera poses and 3D keypoint coordinates. Our model takes advantage of graph neural networks to learn SfM-specific primitives, and we show that it can be used for fast inference of the reconstruction for new and unseen sequences. The experimental results show that the proposed mode
    
[^26]: 什么时候编程思维对推理起作用?

    When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v1 [cs.CL])

    [http://arxiv.org/abs/2308.15452](http://arxiv.org/abs/2308.15452)

    提出了复杂性影响推理分数（CIRS）来衡量编程语言对推理能力的影响，发现并非所有复杂性的代码数据都可以被学习或理解，适当的复杂性水平对于改善推理能力至关重要。

    

    大型语言模型（LLM）的推理能力在体现出人工智能领域中起着关键作用。尽管像编程思维提示这样的方法对于使用编程语言来解决复杂推理任务的LLM非常有效，但代码数据对推理能力的具体影响仍未充分探索。为了填补这一空白，我们提出了复杂性影响推理分数（CIRS），它结合了结构和逻辑属性，以衡量代码和推理能力之间的相关性。具体而言，我们使用抽象语法树来编码结构信息，并通过考虑难度和圈复杂度来计算逻辑复杂性。通过实证分析，我们发现并非所有复杂性的代码数据都可以被LLM学习或理解。最佳复杂性水平对于通过编程辅助提示改善推理能力至关重要。然后我们设计了一个自动合成的方法...

    The reasoning capabilities of Large Language Models (LLMs) play a pivotal role in the realm of embodied artificial intelligence. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score (CIRS), which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing
    
[^27]: 使用动态稀疏训练的持续学习：探索有效模型更新的算法

    Continual Learning with Dynamic Sparse Training: Exploring Algorithms for Effective Model Updates. (arXiv:2308.14831v1 [cs.LG])

    [http://arxiv.org/abs/2308.14831](http://arxiv.org/abs/2308.14831)

    本文通过实证研究探究了持续学习中不同动态稀疏训练（DST）组件的影响，旨在填补关键研究空白并为持续学习下的DST提供最佳配置指导。

    

    持续学习是指智能系统从连续的数据流中以尽可能少的计算开销顺序获取和保留知识的能力。为了实现这一目标，已在文献中引入了正则化、重放、架构和参数隔离等方法。使用稀疏网络进行参数隔离，可以将神经网络的不同部分分配给不同的任务，并允许在任务相似时共享参数。动态稀疏训练(DST)是发现这些稀疏网络并为每个任务进行隔离的一种重要方法。本文是首个对CL范式下不同DST组件效果进行实证研究的研究，旨在填补关键研究空白并为CL下的DST的最佳配置提供指导。因此，我们在著名的CIFAR100和miniImage数据集上进行了综合研究，探究了各种DST组件以找到每个任务的最佳拓扑结构。

    Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImag
    
[^28]: 计划在想象中：基于学习抽象搜索空间的高级规划

    Planning in the imagination: High-level planning on learned abstract search spaces. (arXiv:2308.08693v1 [cs.AI])

    [http://arxiv.org/abs/2308.08693](http://arxiv.org/abs/2308.08693)

    本论文提出了一个名为PiZero的新方法，该方法使代理能够在自己创建的抽象搜索空间中进行高级规划，不受真实环境限制，可在任意时间尺度进行规划，并处理连续动作空间和部分可观察性的设置。在多个领域的实验中，该方法胜过可比的先前方法而无需假设访问环境模拟器。

    

    我们提出了一种新的方法，称为PiZero，它使代理能够在自己创建的抽象搜索空间中进行计划，该搜索空间与真实环境完全解耦。与先前的方法不同，这使得代理能够以任意时间尺度进行高级规划，并以复合或时间扩展动作的形式进行推理，这在需要执行大量基本微操作以执行相关宏操作的环境中非常有用。此外，我们的方法比可比的先前方法更通用，因为它处理具有连续动作空间和部分可观察性的设置。我们在多个领域进行了评估，包括导航任务和Sokoban。实验证明，它在没有假设访问环境模拟器的情况下胜过可比的先前方法。

    We propose a new method, called PiZero, that gives an agent the ability to plan in an abstract search space of its own creation that is completely decoupled from the real environment. Unlike prior approaches, this enables the agent to perform high-level planning at arbitrary timescales and reason in terms of compound or temporally-extended actions, which can be useful in environments where large numbers of base-level micro-actions are needed to perform relevant macro-actions. In addition, our method is more general than comparable prior methods because it handles settings with continuous action spaces and partial observability. We evaluate our method on multiple domains, including navigation tasks and Sokoban. Experimentally, it outperforms comparable prior methods without assuming access to an environment simulator.
    
[^29]: AI和机器人技术在流体属性预测中的应用

    Fluid Property Prediction Leveraging AI and Robotics. (arXiv:2308.02715v1 [cs.LG])

    [http://arxiv.org/abs/2308.02715](http://arxiv.org/abs/2308.02715)

    这项工作提出了一种基于视觉的方法来估计液体的粘度，通过学习视频中不同液体振荡模式的潜在表示，可以从视觉上推断液体的类别或动态粘度。

    

    从视觉上推测液体属性是一项具有挑战性的任务，因为液体的行为和检测都十分复杂。然而，能够直接从视觉信息中推断液体属性对于自主流体处理系统非常有价值，因为摄像头很容易获取。此外，仅通过视觉预测液体属性可以加快流体表征的过程，在各种实验环境中节省大量时间和精力。本文提出了一种纯视觉方法来估计粘度，利用液体振荡行为与粘度直接相关的事实。具体地，我们利用3D卷积自编码器学习视频中不同液体振荡模式的潜在表示。借助这些潜在表示，我们可以从视频中直观地推断液体的类别或液体的动态粘度。

    Inferring liquid properties from vision is a challenging task due to the complex nature of fluids, both in behavior and detection. Nevertheless, the ability to infer their properties directly from visual information is highly valuable for autonomous fluid handling systems, as cameras are readily available. Moreover, predicting fluid properties purely from vision can accelerate the process of fluid characterization saving considerable time and effort in various experimental environments. In this work, we present a purely vision-based approach to estimate viscosity, leveraging the fact that the behavior of the fluid oscillations is directly related to the viscosity. Specifically, we utilize a 3D convolutional autoencoder to learn latent representations of different fluid-oscillating patterns present in videos. We leverage this latent representation to visually infer the category of fluid or the dynamics viscosity of fluid from video.
    
[^30]: 基于联合精算神经网络的横断面和纵向索赔计数数据的车载通信技术

    Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])

    [http://arxiv.org/abs/2308.01729](http://arxiv.org/abs/2308.01729)

    本论文提出了一种基于联合精算神经网络框架的横断面和纵向索赔计数模型，通过结合传统精算模型和神经网络，充分利用了两个模型的优势。

    

    我们提出了一种基于Mario W\"uthrich和Michael Merz提出的联合精算神经网络（CANN）框架的横断面和纵向索赔计数模型。CANN方法将传统的精算模型（如广义线性模型）与神经网络相结合，形成了一个包含经典回归模型和神经网络部分的双组件模型。CANN模型充分利用了两个模型的优势，既可以提供经典模型的可靠性和可解释性，又可以利用神经网络的灵活性和对复杂关系和交互作用的捕捉能力。在我们提出的模型中，我们使用了广为人知的对数线性索赔计数回归模型作为经典回归部分，使用了多层感知器（MLP）作为神经网络部分。MLP部分用于处理以向量形式表示的车辆驾驶行为的车载通信数据。

    We present novel cross-sectional and longitudinal claim count models for vehicle insurance built upon the Combined Actuarial Neural Network (CANN) framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach combines a classical actuarial model, such as a generalized linear model, with a neural network. This blending of models results in a two-component model comprising a classical regression model and a neural network part. The CANN model leverages the strengths of both components, providing a solid foundation and interpretability from the classical model while harnessing the flexibility and capacity to capture intricate relationships and interactions offered by the neural network. In our proposed models, we use well-known log-linear claim count regression models for the classical regression part and a multilayer perceptron (MLP) for the neural network part. The MLP part is used to process telematics car driving data given as a vector characterizing the driving behavior 
    
[^31]: 在差分隐私逻辑回归中的准确性增强：一种预训练方法

    Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach. (arXiv:2307.13771v1 [cs.LG])

    [http://arxiv.org/abs/2307.13771](http://arxiv.org/abs/2307.13771)

    本文通过添加预训练模块，在差分隐私逻辑回归中提高了模型的准确性。

    

    机器学习模型可以记忆训练数据集，因此在私有数据集上训练机器学习模型可能会侵犯个人隐私。差分隐私是一种严格的隐私保护方法，可在机器学习模型中保留底层训练数据集的隐私。然而，在差分隐私框架下训练机器学习模型通常会降低模型的准确性。本文旨在通过预训练模块提高差分隐私机器学习模型（特别是逻辑回归模型）的准确性。具体而言，我们首先在公开训练数据集上对模型进行预训练，该数据集不涉及隐私问题。然后，我们通过使用私有数据集和差分隐私逻辑回归对模型进行微调。数值结果表明，添加预训练模块显著提高了差分隐私逻辑回归的准确性。

    Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
    
[^32]: 长尾分布上的敌对训练

    Adversarial Training Over Long-Tailed Distribution. (arXiv:2307.10205v1 [cs.LG])

    [http://arxiv.org/abs/2307.10205](http://arxiv.org/abs/2307.10205)

    本文研究了在长尾分布数据集上的敌对训练，提出了一种新的敌对训练框架--重新平衡敌对训练（REAT）。REAT能够解决敌对训练过程中产生的不平衡问题，有效增强模型的鲁棒性并保持准确性。

    

    本文研究了在服从长尾分布的数据集上的敌对训练，这在之前的工作中很少被探索。与平衡数据集上的传统敌对训练相比，该过程面临着产生不平衡的敌对样本和不平衡的特征嵌入空间的困境，导致生成的模型在尾部数据上表现出低鲁棒性和准确性。为了解决这个问题，我们提出了一种新的敌对训练框架--重新平衡敌对训练（REAT）。该框架包括两个组成部分：（1）一种受有效样本数启发的新训练策略，用于引导模型生成更平衡和信息丰富的敌对样本；（2）一种精心构建的惩罚函数，用于强制满足一个令人满意的特征空间。不同数据集和模型结构上的评估结果证明，REAT能够有效增强模型的鲁棒性，并保持模型的准确性。代码可以在https://中找到

    In this paper, we study adversarial training on datasets that obey the long-tailed distribution, which is practical but rarely explored in previous works. Compared with conventional adversarial training on balanced datasets, this process falls into the dilemma of generating uneven adversarial examples (AEs) and an unbalanced feature embedding space, causing the resulting model to exhibit low robustness and accuracy on tail data. To combat that, we propose a new adversarial training framework -- Re-balancing Adversarial Training (REAT). This framework consists of two components: (1) a new training strategy inspired by the term effective number to guide the model to generate more balanced and informative AEs; (2) a carefully constructed penalty function to force a satisfactory feature space. Evaluation results on different datasets and model structures prove that REAT can effectively enhance the model's robustness and preserve the model's clean accuracy. The code can be found in https://
    
[^33]: 带有Mean-KL参数化的最小随机编码学习

    Minimal Random Code Learning with Mean-KL Parameterization. (arXiv:2307.07816v1 [cs.LG])

    [http://arxiv.org/abs/2307.07816](http://arxiv.org/abs/2307.07816)

    本文研究了最小随机编码学习（MIRACLE）的两个变体，提出了一种新的参数化方法Mean-KL，在压缩变分贝叶斯神经网络中实现了更快的收敛和良好的预测性能。

    

    本文研究了最小随机编码学习（MIRACLE）的两个变体在压缩变分贝叶斯神经网络中的定性行为和鲁棒性。MIRACLE实现了强大的条件高斯变分近似权重后验$Q_{\mathbf{w}}$，并使用相对熵编码来压缩从后验中抽样的权重，使用高斯编码分布$P_{\mathbf{w}}$。为了达到所需的压缩率，必须对$Q_{\mathbf{w}} \Vert P_{\mathbf{w}}$进行约束，这需要在传统的均值-方差（Mean-Var）参数化下进行计算上昂贵的退火过程。相反，我们通过其平均值和KL散度来参数化$Q_{\mathbf{w}}$，以通过构造将压缩成本约束为所需值。我们证明了使用Mean-KL参数化的变分训练收敛速度是传统方法的两倍，并且在训练后保持了预测性能。

    This paper studies the qualitative behavior and robustness of two variants of Minimal Random Code Learning (MIRACLE) used to compress variational Bayesian neural networks. MIRACLE implements a powerful, conditionally Gaussian variational approximation for the weight posterior $Q_{\mathbf{w}}$ and uses relative entropy coding to compress a weight sample from the posterior using a Gaussian coding distribution $P_{\mathbf{w}}$. To achieve the desired compression rate, $D_{\mathrm{KL}}[Q_{\mathbf{w}} \Vert P_{\mathbf{w}}]$ must be constrained, which requires a computationally expensive annealing procedure under the conventional mean-variance (Mean-Var) parameterization for $Q_{\mathbf{w}}$. Instead, we parameterize $Q_{\mathbf{w}}$ by its mean and KL divergence from $P_{\mathbf{w}}$ to constrain the compression cost to the desired value by construction. We demonstrate that variational training with Mean-KL parameterization converges twice as fast and maintains predictive performance after 
    
[^34]: 使用长短期记忆网络从右到左和头到脚的成分重建三轴心搏图

    Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network. (arXiv:2307.07566v1 [physics.med-ph])

    [http://arxiv.org/abs/2307.07566](http://arxiv.org/abs/2307.07566)

    本研究开发了一个深度学习模型，可从右到左和头到脚的心搏图成分中预测背腹方向的信号。使用长短期记忆网络进行训练和验证，模型表现良好，均方误差为0.09。

    

    本研究旨在开发一个深度学习模型，以预测从右到左和头到脚方向的心搏图（SCG）信号在背腹方向的信号（SCG_x和SCG_y）。用于训练和验证模型的数据集来自15名健康成年人。使用三轴加速度计将SCG信号记录在每个被试的胸部上。然后使用心电图R波进行分段，将片段进行降采样、归一化和零居中处理。得到的数据集被用来训练和验证一个具有两层和一个避免过拟合的dropout层的长短期记忆（LSTM）网络。网络以100个时间步长的SCG_x和SCG_y作为输入，表示一个心脏周期，输出映射到目标变量的向量。结果显示，LSTM模型的均方误差为0.09。

    This pilot study aims to develop a deep learning model for predicting seismocardiogram (SCG) signals in the dorsoventral direction from the SCG signals in the right-to-left and head-to-foot directions ($\textrm{SCG}_x$ and $\textrm{SCG}_y$). The dataset used for the training and validation of the model was obtained from 15 healthy adult subjects. The SCG signals were recorded using tri-axial accelerometers placed on the chest of each subject. The signals were then segmented using electrocardiogram R waves, and the segments were downsampled, normalized, and centered around zero. The resulting dataset was used to train and validate a long short-term memory (LSTM) network with two layers and a dropout layer to prevent overfitting. The network took as input 100-time steps of $\textrm{SCG}_x$ and $\textrm{SCG}_y$, representing one cardiac cycle, and outputted a vector that mapped to the target variable being predicted. The results showed that the LSTM model had a mean square error of 0.09 b
    
[^35]: 用于亚线性超体积遗憾度量的最优标量化方法

    Optimal Scalarizations for Sublinear Hypervolume Regret. (arXiv:2307.03288v1 [cs.LG])

    [http://arxiv.org/abs/2307.03288](http://arxiv.org/abs/2307.03288)

    研究了用于亚线性超体积遗憾度量的最优标量化方法，证明了具有均匀随机权重的超体积标量化方法在最小化超体积遗憾方面是最优的，并在多目标随机线性赌博机问题上进行了案例研究。

    

    标量化是一种通用的技术，可以应用于任何多目标设置中，将多个目标减少为一个，例如最近在RLHF中用于训练校准人类偏好的奖励模型。然而，一些人对这种经典方法持否定态度，因为已知线性标量化会忽略帕累托前沿的凹区域。为此，我们旨在找到简单的非线性标量化方法，以通过被支配的超体积来探索帕累托前沿上的多样化目标集。我们证明，具有均匀随机权重的超体积标量化令人惊讶地是为了证明最小化超体积遗憾而最优的，实现了 $O(T^{-1/k})$ 的最优亚线性遗憾界，同时匹配的下界表明在渐近情况下没有任何算法能做得更好。作为一个理论案例研究，我们考虑了多目标随机线性赌博机问题，并展示了通过利用超线性遗憾界的超体积标量化方法，

    Scalarization is a general technique that can be deployed in any multiobjective setting to reduce multiple objectives into one, such as recently in RLHF for training reward models that align human preferences. Yet some have dismissed this classical approach because linear scalarizations are known to miss concave regions of the Pareto frontier. To that end, we aim to find simple non-linear scalarizations that can explore a diverse set of $k$ objectives on the Pareto frontier, as measured by the dominated hypervolume. We show that hypervolume scalarizations with uniformly random weights are surprisingly optimal for provably minimizing the hypervolume regret, achieving an optimal sublinear regret bound of $O(T^{-1/k})$, with matching lower bounds that preclude any algorithm from doing better asymptotically. As a theoretical case study, we consider the multiobjective stochastic linear bandits problem and demonstrate that by exploiting the sublinear regret bounds of the hypervolume scalariz
    
[^36]: 可证明高效的迭代CVaR强化学习与函数逼近

    Provably Efficient Iterated CVaR Reinforcement Learning with Function Approximation. (arXiv:2307.02842v1 [cs.LG])

    [http://arxiv.org/abs/2307.02842](http://arxiv.org/abs/2307.02842)

    这篇论文研究了一种新颖的风险敏感强化学习方法，通过迭代条件风险价值目标以及线性和一般函数逼近方法，实现了安全性保证，并提出了高效的算法。通过对于不同逼近方法的实验结果，验证了算法的有效性和优越性。

    

    风险敏感的强化学习旨在优化平衡期望奖励和风险的策略。本文研究了一种新颖的风险敏感强化学习形式，采用迭代条件风险价值（CVaR）目标以线性和一般的函数逼近方法。这种名为带有函数逼近的ICVaR-RL的新形式，为每个决策步骤提供了一种可靠的安全保证方式。对于采用线性函数逼近的ICVaR-RL，我们提出了一个计算效率高的算法ICVaR-L，该算法的后悔度为$\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$，其中$\alpha$是风险水平，$d$是状态行动特征的维度，$H$是每个episode的长度，$K$是episode的数量。我们还建立了一个相匹配的下界$\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$，以验证ICVaR-L在$d$和$K$方面的最优性。对于采用一般函数逼近的ICVaR-RL，我们提出了算法ICVaR-G，它实现了...

    Risk-sensitive reinforcement learning (RL) aims to optimize policies that balance the expected reward and risk. In this paper, we investigate a novel risk-sensitive RL formulation with an Iterated Conditional Value-at-Risk (CVaR) objective under linear and general function approximations. This new formulation, named ICVaR-RL with function approximation, provides a principled way to guarantee safety at each decision step. For ICVaR-RL with linear function approximation, we propose a computationally efficient algorithm ICVaR-L, which achieves an $\widetilde{O}(\sqrt{\alpha^{-(H+1)}(d^2H^4+dH^6)K})$ regret, where $\alpha$ is the risk level, $d$ is the dimension of state-action features, $H$ is the length of each episode, and $K$ is the number of episodes. We also establish a matching lower bound $\Omega(\sqrt{\alpha^{-(H-1)}d^2K})$ to validate the optimality of ICVaR-L with respect to $d$ and $K$. For ICVaR-RL with general function approximation, we propose algorithm ICVaR-G, which achiev
    
[^37]: 大型语言模型赋能连接智能的自主边缘AI

    Large Language Models Empowered Autonomous Edge AI for Connected Intelligence. (arXiv:2307.02779v1 [cs.IT])

    [http://arxiv.org/abs/2307.02779](http://arxiv.org/abs/2307.02779)

    大型语言模型赋能连接智能的自主边缘AI系统，通过云-边缘-客户端的分层架构和强大的GPT模型能力，实现高质量、低延迟、隐私保护的AI服务，满足用户个人需求并实现自动化。

    

    无线网络的发展朝着连接智能的方向发展，这一概念设想了在超连接的网络物理世界中，人类、物体和智能之间实现无缝互联。边缘AI作为实现连接智能的有希望的解决方案，通过在网络边缘提供高质量、低延迟和隐私保护的AI服务。在本文中，我们介绍了一种自主边缘AI系统，该系统自动组织、适应和优化自己以满足用户的各种需求。该系统采用云-边缘-客户端的分层架构，其中大型语言模型——生成式预训练变换器（GPT）存放在云端，其他AI模型被共同部署在设备和边缘服务器上。通过利用GPT在语言理解、规划和代码生成方面的强大能力，我们提出了一个多功能的框架，有效协调边缘AI模型以满足用户的个人需求，同时实现自动化。

    The evolution of wireless networks gravitates towards connected intelligence, a concept that envisions seamless interconnectivity among humans, objects, and intelligence in a hyper-connected cyber-physical world. Edge AI emerges as a promising solution to achieve connected intelligence by delivering high-quality, low-latency, and privacy-preserving AI services at the network edge. In this article, we introduce an autonomous edge AI system that automatically organizes, adapts, and optimizes itself to meet users' diverse requirements. The system employs a cloud-edge-client hierarchical architecture, where the large language model, i.e., Generative Pretrained Transformer (GPT), resides in the cloud, and other AI models are co-deployed on devices and edge servers. By leveraging the powerful abilities of GPT in language understanding, planning, and code generation, we present a versatile framework that efficiently coordinates edge AI models to cater to users' personal demands while automati
    
[^38]: 深度神经网络如何学习组合性数据：随机层次模型

    How Deep Neural Networks Learn Compositional Data: The Random Hierarchy Model. (arXiv:2307.02129v1 [cs.LG])

    [http://arxiv.org/abs/2307.02129](http://arxiv.org/abs/2307.02129)

    本文研究了深度神经网络学习组合性数据的问题，通过对随机层次模型进行分类任务，发现深度CNN学习这个任务所需的训练数据数量随着类别数、组合数和迭代次数的增加而渐进增加。

    

    学习一般高维任务是非常困难的，因为它需要与维度成指数增长的训练数据数量。然而，深度卷积神经网络（CNN）在克服这一挑战方面显示出了卓越的成功。一种普遍的假设是可学习任务具有高度结构化，CNN利用这种结构建立了数据的低维表示。然而，我们对它们需要多少训练数据以及这个数字如何取决于数据结构知之甚少。本文回答了针对一个简单的分类任务的这个问题，该任务旨在捕捉真实数据的相关方面：随机层次模型。在这个模型中，$n_c$个类别中的每一个对应于$m$个同义组合的高层次特征，并且这些特征又通过一个重复$L$次的迭代过程由子特征组成。我们发现，需要深度CNN学习这个任务的训练数据数量$P^*$（i）随着$n_c m^L$的增长而渐进地增长，这只有...

    Learning generic high-dimensional tasks is notably hard, as it requires a number of training data exponential in the dimension. Yet, deep convolutional neural networks (CNNs) have shown remarkable success in overcoming this challenge. A popular hypothesis is that learnable tasks are highly structured and that CNNs leverage this structure to build a low-dimensional representation of the data. However, little is known about how much training data they require, and how this number depends on the data structure. This paper answers this question for a simple classification task that seeks to capture relevant aspects of real data: the Random Hierarchy Model. In this model, each of the $n_c$ classes corresponds to $m$ synonymic compositions of high-level features, which are in turn composed of sub-features through an iterative process repeated $L$ times. We find that the number of training data $P^*$ required by deep CNNs to learn this task (i) grows asymptotically as $n_c m^L$, which is only
    
[^39]: 风险敏感强化学习的分布模型等效性

    Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])

    [http://arxiv.org/abs/2307.01708](http://arxiv.org/abs/2307.01708)

    本文研究了风险敏感强化学习中的分布模型等效性问题。我们提出了两种新的模型等价性概念，并展示了如何将这些概念应用于增强任何基于模型的风险敏感算法。

    

    我们考虑学习用于风险敏感强化学习的模型的问题。我们在理论上证明了适当的价值等价性，这是一种学习模型的方法，可以用于在风险中性的情况下进行最优规划，但在风险敏感的情况下无法进行最优规划。我们利用分布式强化学习引入了两种新的模型等价性概念，其中一个是通用的，可以用于针对任何风险度量进行规划，但是计算复杂；另一个是实际的变体，允许选择可以进行最优规划的风险度量。我们展示了如何使用我们的框架来增强任何基于模型的风险敏感算法，并提供了表格和大规模实验来展示其能力。

    We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
    
[^40]: RL4CO: 用于组合优化的广泛强化学习基准测试

    RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark. (arXiv:2306.17100v1 [cs.LG])

    [http://arxiv.org/abs/2306.17100](http://arxiv.org/abs/2306.17100)

    RL4CO是一个用于组合优化的广泛强化学习基准测试，着重于可扩展性和泛化能力的评估，并展示了一些最新方法在样本效率和适应不同数据分布方面的表现相对较差，强调了对神经CO求解器性能的平衡评估的重要性。

    

    我们引入了RL4CO，这是一个广泛的强化学习（RL）用于组合优化（CO）的基准测试。RL4CO采用最先进的软件库和最佳实践，如模块化和配置管理，以便研究人员可以轻松修改神经网络架构、环境和算法。与现有的专注于特定任务（如旅行推销员问题）进行性能评估的方法不同，我们强调可扩展性和泛化能力对于各种优化任务的重要性。我们还系统地评估了各种模型在样本效率、零-shot泛化和适应不同数据分布方面的表现。我们的实验结果表明，一些最新的最先进方法在使用这些新指标进行评估时落后于之前的方法，这表明有必要更加平衡地评估神经CO求解器的性能。我们希望RL4CO能够为研究人员提供一个综合性的基准测试工具，以进一步推动强化学习在组合优化领域的研究。

    We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse optimization tasks. We also systematically benchmark sample efficiency, zero-shot generalization, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these new metrics, suggesting the necessity for a more balanced view of the performance of neural CO solvers. We hope RL4CO will 
    
[^41]: 针对图神经网络的对抗训练

    Adversarial Training for Graph Neural Networks. (arXiv:2306.15427v1 [cs.LG])

    [http://arxiv.org/abs/2306.15427](http://arxiv.org/abs/2306.15427)

    该论文通过克服图学习设置的限制，引入可学习图扩散的灵活GNNs以及针对结构扰动的攻击方法，证明了对抗训练是针对对抗结构扰动的最先进防御方法。

    

    尽管在图像领域取得了成功，但对抗训练在图神经网络（GNNs）对抗图结构扰动方面并没有明显效果。在修复对抗训练的过程中，我们发现并克服了之前工作中采用的图学习设置的基本理论和实际限制；我们揭示了基于可学习图扩散的更灵活 GNNs 能够适应对抗扰动，同时学习到的消息传递方案具有自然的可解释性；我们还引入了第一种针对结构扰动的攻击方法，能够同时对多个节点进行攻击，并能处理全局（图级别）和局部（节点级别）的约束。通过这些贡献，我们证明对抗训练是对抗结构扰动的最先进防御方法。

    Despite its success in the image domain, adversarial training does not (yet) stand out as an effective defense for Graph Neural Networks (GNNs) against graph structure perturbations. In the pursuit of fixing adversarial training (1) we show and overcome fundamental theoretical as well as practical limitations of the adopted graph learning setting in prior work; (2) we reveal that more flexible GNNs based on learnable graph diffusion are able to adjust to adversarial perturbations, while the learned message passing scheme is naturally interpretable; (3) we introduce the first attack for structure perturbations that, while targeting multiple nodes at once, is capable of handling global (graph-level) as well as local (node-level) constraints. Including these contributions, we demonstrate that adversarial training is a state-of-the-art defense against adversarial structure perturbations.
    
[^42]: 在数据质量和公平之间权衡价格，实现在线分配公平性

    Trading-off price for data quality to achieve fair online allocation. (arXiv:2306.13440v1 [cs.LG])

    [http://arxiv.org/abs/2306.13440](http://arxiv.org/abs/2306.13440)

    本文提出了一种多臂老虎机算法，该算法权衡了价格和数据质量，以解决在线分配问题中的公平性，使得对于源的选择，算法可以适应各种公平概念，具有一定的代价可以减少公平惩罚，并且证明了算法具有较小的遗憾界。

    

    本论文探讨了在线分配问题，其中包含了长期的公平惩罚。但与现有的研究不同的是，我们不假设决策者可以观察到受保护的属性——这在实践中经常是不现实的。相反，他们可以购买来自不同质量来源的数据以帮助估计它们，从而以一定的代价减少公平惩罚。我们将这个问题建模为多臂老虎机问题，其中每个臂对应于数据源的选择，同时包含在线分配问题。我们提出了一种算法，同时解决了两个问题，并证明它具有 $\mathcal{O}(\sqrt{T})$ 的遗憾界。一个关键困难是选择源所获得的回报由公平惩罚相关，这导致尽管是随机设置，但需要进行随机化。我们的算法考虑到源选择之前可用的上下文信息，并可以适应许多不同的公平概念。我们还展示了我们的算法在实验中的有效性。

    We consider the problem of online allocation subject to a long-term fairness penalty. Contrary to existing works, however, we do not assume that the decision-maker observes the protected attributes -- which is often unrealistic in practice. Instead they can purchase data that help estimate them from sources of different quality; and hence reduce the fairness penalty at some cost. We model this problem as a multi-armed bandit problem where each arm corresponds to the choice of a data source, coupled with the online allocation problem. We propose an algorithm that jointly solves both problems and show that it has a regret bounded by $\mathcal{O}(\sqrt{T})$. A key difficulty is that the rewards received by selecting a source are correlated by the fairness penalty, which leads to a need for randomization (despite a stochastic setting). Our algorithm takes into account contextual information available before the source selection, and can adapt to many different fairness notions. We also sho
    
[^43]: 深度语言网络：使用变分推断联合训练叠加LLM的提示层

    Deep Language Networks: Joint Prompt Training of Stacked LLMs using Variational Inference. (arXiv:2306.12509v1 [cs.CL])

    [http://arxiv.org/abs/2306.12509](http://arxiv.org/abs/2306.12509)

    本文提出了一种称为深度语言网络（DLN）的架构，通过联合训练叠加的语言模型层（LLMs），使用变分推断算法进行提示训练，使得DLN-2的性能甚至可以与少量训练数据的GPT-4相媲美。

    

    我们将大型语言模型（LLMs）视为网络中的随机“语言层”，其中可学习的参数是每个层的自然语言“提示”。我们将两个这样的层叠加在一起，将一个层的输出馈送到下一个层。我们将这种堆叠的结构称为“深度语言网络”（DLN）。首先，我们展示如何有效地针对单层语言网络（DLN-1）执行提示优化。然后，我们展示如何训练2层DLNs（DLN-2），其中必须学习两个提示。我们认为第一层的输出是一个潜在变量，需要进行边缘化，并设计了一种联合提示训练的变分推断算法。DLN-2比单层达到更高的性能，有时即使网络中的每个LLM更小且更弱，也可以与少量训练数据的GPT-4相媲美。DLN代码是开源的：https://github.com/microsoft/deep-language-networks。

    We view large language models (LLMs) as stochastic \emph{language layers} in a network, where the learnable parameters are the natural language \emph{prompts} at each layer. We stack two such layers, feeding the output of one layer to the next. We call the stacked architecture a \emph{Deep Language Network} (DLN). We first show how to effectively perform prompt optimization for a 1-Layer language network (DLN-1). We then show how to train 2-layer DLNs (DLN-2), where two prompts must be learnt. We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training. A DLN-2 reaches higher performance than a single layer, sometimes comparable to few-shot GPT-4 even when each LLM in the network is smaller and less powerful. The DLN code is open source: https://github.com/microsoft/deep-language-networks .
    
[^44]: EquiformerV2: 改进的等变Transformer，用于扩展到更高次表示

    EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. (arXiv:2306.12059v1 [cs.LG])

    [http://arxiv.org/abs/2306.12059](http://arxiv.org/abs/2306.12059)

    本文提出了EquiformerV2，通过使用新的卷积类型和架构改进，扩展了等变Transformer到更高的等变表示，在处理大型数据集时表现更好，能量和力的表现也得到了提高，计算效率也得到了提升。

    

    等变Transformer（例如Equiformer）已经证明了将Transformer应用于3D原子系统领域的功效。但是，由于计算复杂性，它们仍然局限于小数次等变表示。在本文中，我们调查了这些架构是否能够很好地扩展到更高的次数。从Equiformer开始，我们首先用eSCN卷积替换了$SO(3)$卷积，以有效地合并更高次的张量。然后，为了更好地利用更高次的能力，我们提出了三个架构改进——注意力重标准化、可分离的$S^2$激活和可分离层归一化。将这一切放在一起，我们提出了EquiformerV2，在大型OC20数据集上的表现优于以前的最先进方法，在力上提高了最多$12\%$，能量上提高了$4\%$，提供更好的速度-准确性权衡，并且在计算吸附能所需的DFT计算量方面缩减了2倍。

    Equivariant Transformers such as Equiformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equiformer, we first replace $SO(3)$ convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements -- attention re-normalization, separable $S^2$ activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to $12\%$ on forces, $4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$ reduction in DFT calculations needed for computing adsorption energies.
    
[^45]: 私有联邦频率估计：适应实例难度的算法

    Private Federated Frequency Estimation: Adapting to the Hardness of the Instance. (arXiv:2306.09396v1 [cs.DS])

    [http://arxiv.org/abs/2306.09396](http://arxiv.org/abs/2306.09396)

    本论文提出了一种新的混合sketching算法并解决了一个基本问题：如何根据底层问题的难度设置sketch size。

    

    在联邦频率估计（FFE）中，多个客户端使用遵守Secure Summation(SecSum)隐私约束的服务器协议通信，共同估计其数据的频率。对于单轮FFE，研究者已知使用count sketching算法几乎可以达到基本的精度-通信复杂度平衡。但是在更实际的多轮FEE设置中，我们发现简单的count sketching算法是严格次优的。我们提出了一种新的混合sketching算法，并证明其更精确。此外，我们回答了一个基本问题：为了适应底层问题的难度，从业者应该如何设置sketch size？ 我们提出了一个两阶段的方法来解决这个问题。

    In federated frequency estimation (FFE), multiple clients work together to estimate the frequencies of their collective data by communicating with a server that respects the privacy constraints of Secure Summation (SecSum), a cryptographic multi-party computation protocol that ensures that the server can only access the sum of client-held vectors. For single-round FFE, it is known that count sketching is nearly information-theoretically optimal for achieving the fundamental accuracy-communication trade-offs [Chen et al., 2022]. However, we show that under the more practical multi-round FEE setting, simple adaptations of count sketching are strictly sub-optimal, and we propose a novel hybrid sketching algorithm that is provably more accurate. We also address the following fundamental question: how should a practitioner set the sketch size in a way that adapts to the hardness of the underlying problem? We propose a two-phase approach that allows for the use of a smaller sketch size for s
    
[^46]: 增强感知的有向投影自监督学习

    Augmentation-aware Self-supervised Learning with Guided Projector. (arXiv:2306.06082v1 [cs.CV])

    [http://arxiv.org/abs/2306.06082](http://arxiv.org/abs/2306.06082)

    本文提出了一种名为CASSLE的方法，它通过修改自监督学习中的有向投影网络，利用增强信息来提高模型处理图像特征的鲁棒性。

    

    自监督学习是从无标签数据中学习健壮表示的强大技术。SimCLR和MoCo等方法通过学习对应用的数据增强保持不变，能够达到与监督方法相当的质量。然而，这种不变性可能对解决某些下游任务有害，这些任务依赖于受到预训练期间使用的增强影响的特征，例如颜色。在本文中，我们提出通过修改自监督架构的常见组件之一的有向投影网络，来促进表示空间对这些特征的敏感性。具体而言，我们为投影器补充有关应用于图像的增强的信息。为了让投影器在解决自监督学习任务时利用这种辅助指导，特征提取器学习在其表示中保留增强信息。我们的方法被称为有向投影自监督学习（CASSLE），通过这种方法提高了模型处理图像特征的鲁棒性。

    Self-supervised learning (SSL) is a powerful technique for learning robust representations from unlabeled data. By learning to remain invariant to applied data augmentations, methods such as SimCLR and MoCo are able to reach quality on par with supervised approaches. However, this invariance may be harmful to solving some downstream tasks which depend on traits affected by augmentations used during pretraining, such as color. In this paper, we propose to foster sensitivity to such characteristics in the representation space by modifying the projector network, a common component of self-supervised architectures. Specifically, we supplement the projector with information about augmentations applied to images. In order for the projector to take advantage of this auxiliary guidance when solving the SSL task, the feature extractor learns to preserve the augmentation information in its representations. Our approach, coined Conditional Augmentation-aware Selfsupervised Learning (CASSLE), is d
    
[^47]: 利用欧几里得距离函数解释和改进扩散模型

    Interpreting and Improving Diffusion Models Using the Euclidean Distance Function. (arXiv:2306.04848v1 [cs.LG])

    [http://arxiv.org/abs/2306.04848](http://arxiv.org/abs/2306.04848)

    本文利用欧几里得距离函数解释去噪扩散模型，并提出了一种新的采样器。采样器表现出了最先进的FID得分，并能够生成高质量的样本。

    

    去噪直觉上与投影有关。事实上，在流形假设下，添加随机噪声近似等价于正交扰动。因此，学习去噪近似于学习投影。本文利用这一观察结果，将去噪扩散模型解释为应用于欧几里得距离函数的近似梯度下降。随后，我们基于对去噪器投影误差的简单假设，提供DDIM（Denoising Diffusion Implicit Models）采样器的简单收敛分析。最后，我们基于理论结果的洞见提出一种基于对DDIM的两个简单修改的新采样器。仅需要5-10个函数评估，我们的采样器就能在预训练的CIFAR-10和CelebA模型上达到最先进的FID得分，并且可以在潜在扩散模型上生成高质量的样本。

    Denoising is intuitively related to projection. Indeed, under the manifold hypothesis, adding random noise is approximately equivalent to orthogonal perturbation. Hence, learning to denoise is approximately learning to project. In this paper, we use this observation to reinterpret denoising diffusion models as approximate gradient descent applied to the Euclidean distance function. We then provide straight-forward convergence analysis of the DDIM sampler under simple assumptions on the projection-error of the denoiser. Finally, we propose a new sampler based on two simple modifications to DDIM using insights from our theoretical results. In as few as 5-10 function evaluations, our sampler achieves state-of-the-art FID scores on pretrained CIFAR-10 and CelebA models and can generate high quality samples on latent diffusion models.
    
[^48]: 对比提升：通过慢-快对比融合实现三维物体实例分割

    Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion. (arXiv:2306.04633v1 [cs.CV])

    [http://arxiv.org/abs/2306.04633](http://arxiv.org/abs/2306.04633)

    本文提出了一种利用二维预训练模型，通过慢-快对比融合实现三维物体实例分割的方法，在大量物体的场景中具有可扩展性，同时不需要物体数量的上界限制，通过限制多视角一致性，提出了一个新的半真实数据集(Messy Rooms)。

    

    由于缺乏大规模注释的三维数据集，三维物体实例分割是一项具有挑战性的任务。本文展示了可以通过利用二维预训练模型来有效解决该任务。我们提出了一种新颖的方法，通过神经场表示将2D分段向上提升到3D，并将它们通过慢-快对比融合。这种方法鼓励跨帧的多视角一致性。我们方法的核心是一个可扩展的、适用于具有大量物体的场景的慢-快聚类目标函数。与先前的方法不同的是，我们的方法不需要对物体数量或跨帧物体跟踪进行设置上界。为了展示慢-快聚类的可扩展性，我们创建了一个名为Messy Rooms的新的半真实数据集，其中场景中最多有500个物体。我们的方法在ScanNet、Hypersim和Replica数据集的具有挑战性的场景中表现优于现有技术。

    Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as o
    
[^49]: ContriMix：显微镜图像分析中基于无监督内容属性分离的领域泛化方法

    ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v1 [eess.IV])

    [http://arxiv.org/abs/2306.04527](http://arxiv.org/abs/2306.04527)

    ContriMix是一种无需标识和手工调优的领域泛化技术，在显微镜图像中通过分离和学习生成合成图像的方式，解决了领域泛化的问题。

    

    针对组织学和荧光成像等显微镜图像中的领域泛化问题，我们提出了 ContriMix，它采用无监督学习方式分离出显微镜图像中的生物学内容和技术变异，并学习生成合成图像，避免了需要手工 fine-tuning 的问题。我们在组织学和荧光成像实验中验证了 ContriMix 的有效性，取得了基于领域泛化的最新成果。

    Domain generalization is critical for real-world applications of machine learning models to microscopy images, including histopathology and fluorescence imaging. Artifacts in histopathology arise through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. In fluorescence imaging, these artifacts stem from variations across experimental batches. The complexity and subtlety of these artifacts make the enumeration of data domains intractable. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content ("content") and technical variations ("attributes") in microscopy images. ContriMix does not rely on domain identifiers or handcrafted aug
    
[^50]: 锥机制: 层次感知注意力

    Coneheads: Hierarchy Aware Attention. (arXiv:2306.00392v1 [cs.LG])

    [http://arxiv.org/abs/2306.00392](http://arxiv.org/abs/2306.00392)

    “锥注意力”是点积注意力的替代方案，它通过基于双曲锥的层次结构联系数据点，明确建模了真实世界数据集的复杂结构属性，提高了任务级性能并实现了优于点积注意力的效果，而且参数量更少。

    

    注意力网络，如transformers，在许多领域已经实现了最先进的性能。这些网络严重依赖于点积注意力运算符，它通过取两个点的内积来计算它们之间的相似性。然而，内积不能明确地对真实世界数据集的复杂结构属性（如数据点之间的层次结构）进行建模。为了解决这个问题，我们引入了锥注意力，一种基于双曲锥的点积注意力的替代方案。锥注意力通过双曲锥定义的层次结构将两个点联系起来，直观地衡量了两个点的分歧，并给出了一个层次感知的相似度分数。我们在各种模型和任务上测试了锥注意力，并表明它在任务级性能上优于点积注意力和其他基线算法，并且能够以显著较少的参数匹配点积注意力。

    Attention networks such as transformers have achieved state-of-the-art performance in many domains. These networks rely heavily on the dot product attention operator, which computes the similarity between two points by taking their inner product. However, the inner product does not explicitly model the complex structural properties of real world datasets, such as hierarchies between data points. To remedy this, we introduce cone attention, a drop-in replacement for dot product attention based on hyperbolic entailment cones. Cone attention associates two points by the depth of their lowest common ancestor in a hierarchy defined by hyperbolic cones, which intuitively measures the divergence of two points and gives a hierarchy aware similarity score. We test cone attention on a wide variety of models and tasks and show that it improves task-level performance over dot product attention and other baselines, and is able to match dot-product attention with significantly fewer parameters. Our 
    
[^51]: 基于标签检索的扩散模型用于学习嘈杂标签

    Label-Retrieval-Augmented Diffusion Models for Learning from Noisy Labels. (arXiv:2305.19518v1 [cs.LG])

    [http://arxiv.org/abs/2305.19518](http://arxiv.org/abs/2305.19518)

    本论文提出了一种基于标签检索的扩散模型，用于有效构造并利用伪干净标签，以减小嘈杂标签对分类影响。

    

    学习如何从带有嘈杂标签的数据集中获取信息，一直是机器学习领域的一个重要课题。传统方法通常依赖于比较严格的假设，而且受限于特定类型的标签噪声。本文提出了一种标签噪声问题的生成模型，用于学习和判断随机的标签。作者利用这一模型，提出了基于标签检索的扩散模型，来有效的构造和利用伪干净标签，从而减小嘈杂标签的影响。

    Learning from noisy labels is an important and long-standing problem in machine learning for real applications. One of the main research lines focuses on learning a label corrector to purify potential noisy labels. However, these methods typically rely on strict assumptions and are limited to certain types of label noise. In this paper, we reformulate the label-noise problem from a generative-model perspective, $\textit{i.e.}$, labels are generated by gradually refining an initial random guess. This new perspective immediately enables existing powerful diffusion models to seamlessly learn the stochastic generative process. Once the generative uncertainty is modeled, we can perform classification inference using maximum likelihood estimation of labels. To mitigate the impact of noisy labels, we propose the $\textbf{L}$abel-$\textbf{R}$etrieval-$\textbf{A}$ugmented (LRA) diffusion model, which leverages neighbor consistency to effectively construct pseudo-clean labels for diffusion train
    
[^52]: 基于$K^2$-树的分级图生成

    Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19125](http://arxiv.org/abs/2305.19125)

    本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。

    

    从目标分布生成图是许多领域的一个重大挑战，包括药物发现和社交网络分析。在本文中，我们介绍了一种利用原本设计用于无损图压缩的$K^2$-树表示的新颖图生成方法。我们的动机源于$K^2$-树能够在进行紧凑生成的同时，捕获图的内在分层结构的能力。此外，我们还通过(1)提出了一种包含剪枝、扁平化和记号化过程的顺序K2树表示和(2)引入了一种基于Transformer的架构，旨在通过结合专业树形位置编码方案来生成序列。最后，我们对四个常规和两个分子图数据集进行了广泛的评估，以证实我们的算法在图生成方面的优越性。

    Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
    
[^53]: SVDinsTN: 一种集成的张量网络表示方法及有效的结构搜索方法

    SVDinsTN: An Integrated Method for Tensor Network Representation with Efficient Structure Search. (arXiv:2305.14912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.14912](http://arxiv.org/abs/2305.14912)

    SVDinsTN是一种高效的张量网络表示方法，通过在完全连接的张量网络中插入对角因子，同时计算张量核和对角因子，从而实现最紧凑的TN结构。与现有的TN-SS方法相比，SVDinsTN在速度方面加快了10到10^3倍，并且保持了相当的水平。

    

    张量网络（TN）表示是一种强大的数据分析和机器学习技术。其中一个挑战是张量网络结构搜索（TN-SS）问题，即寻找最优结构以实现紧凑的表示。现有的TN-SS方法主要采用双层优化方法，由于重复的结构评估导致计算成本过高。为解决这个问题，我们提出了一种高效的集成（单层）方法，命名为SVDinsTN，消除了重复繁琐的结构评估。通过为完全连接的TN的每个边插入一个对角因子，我们同时计算TN核和对角因子，因子稀疏性揭示了最紧凑的TN结构。实验结果表明，与现有的TN-SS方法相比，SVDinsTN在运行时间上实现了约10到10^3倍的加速，同时保持了可比较的水平。

    Tensor network (TN) representation is a powerful technique for data analysis and machine learning. It practically involves a challenging TN structure search (TN-SS) problem, which aims to search for the optimal structure to achieve a compact representation. Existing TN-SS methods mainly adopt a bi-level optimization method that leads to excessive computational costs due to repeated structure evaluations. To address this issue, we propose an efficient integrated (single-level) method named SVD-inspired TN decomposition (SVDinsTN), eliminating the need for repeated tedious structure evaluation. By inserting a diagonal factor for each edge of the fully-connected TN, we calculate TN cores and diagonal factors simultaneously, with factor sparsity revealing the most compact TN structure. Experimental results on real-world data demonstrate that SVDinsTN achieves approximately $10\sim{}10^3$ times acceleration in runtime compared to the existing TN-SS methods while maintaining a comparable lev
    
[^54]: 边缘聚焦：基于异常值的毒性检测中受损人群的识别

    Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])

    [http://arxiv.org/abs/2305.14735](http://arxiv.org/abs/2305.14735)

    本文提出了一种基于异常值的方法，用于识别在毒性检测中受到伤害的人群，发现对于这些异常值，模型性能较差，他们面临的毒性更高。

    

    衡量人工智能对边缘社区影响的标准方法是确定特定人口群体之间的性能差异。这些方法旨在解决针对弱势群体的伤害问题，但它们会掩盖由交叉子群或跨人口群体共享的伤害模式。相反，我们将“边缘”定义为具有远离“常态” 的人口属性的数据点，并度量针对这些异常值的伤害。我们提出了一种基于群体的性能差异指数（GPDI），以衡量数据集细分为子组对面临增加的伤害的识别程度。我们将我们的方法应用于检测毒性检测中的差异，并发现针对异常值的文本在所有类型的毒性检验中毒性更高，高达28％至86％。我们还发现，对于人口学异常值，模型性能始终较差，异常值和非异常值之间的错误差距高达10％。

    A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
    
[^55]: SMT 2.0：一个关注层次和混合变量高斯过程的代理模型工具包

    SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13998](http://arxiv.org/abs/2305.13998)

    SMT 2.0是一个开源的代理模型工具包，引入了处理混合变量和层次变量的能力，并通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。

    

    Surrogate Modeling Toolbox (SMT)是一个开源的Python包，提供了一系列代理建模方法、采样技术和一套示例问题。本文介绍了SMT 2.0，这是SMT的一个重要新版本，引入了显著的升级和新功能。这个版本增加了处理混合变量代理模型和层次变量的能力。这些类型的变量在多个代理建模应用中变得越来越重要。SMT 2.0还通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。这个版本还包括了处理带噪声和使用多保真度数据的新函数。据我们所知，SMT 2.0是第一个提出层次和混合输入的开源代理库。这个开源软件采用New BSD许可证进行分发。

    The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.
    
[^56]: 实现离线强化学习的极小极大样本复杂性：一种基于DRO的方法。

    Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13289](http://arxiv.org/abs/2305.13289)

    本文提出了一种分布鲁棒优化 (DRO)的方法，用于解决离线强化学习中的数据有限性和分布转移问题。通过直接建模转移核的不确定性，并寻找在不确定性集合中最优化最坏情况下的性能的策略，实现了极小极大最优性。

    

    离线强化学习旨在从先前收集的数据集中学习，而无需主动探索。该问题面临着诸多挑战，包括有限的数据可用性和分布转移。现有方法采用一种悲观的态度对待不确定性，通过惩罚未充分探索的状态-行为对的奖励来保守估计值函数。在本文中，我们展示了分布鲁棒优化（DRO）基于方法也可以应对这些挑战，并且是极小极大最优的。具体而言，我们直接建模转移核的不确定性，并构建一个统计合理的转移核不确定性集合。然后我们寻找在该不确定性集合上最优化最坏情况下的性能的策略。我们首先设计了一种基于度量的霍夫丁风格不确定性集合，这样真实的转移核以高概率位于该集合中。我们证明了为了实现$\epsilon$的次最优性差距，样本复杂度为$\mat。

    Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\epsilon$, the sample complexity is $\mat
    
[^57]: 乐观自然策略梯度：一种简单高效的在线强化学习策略优化框架

    Optimistic Natural Policy Gradient: a Simple Efficient Policy Optimization Framework for Online RL. (arXiv:2305.11032v1 [cs.LG])

    [http://arxiv.org/abs/2305.11032](http://arxiv.org/abs/2305.11032)

    本文提出了一种乐观自然策略梯度的在线强化学习策略优化框架，采用乐观策略评估子程序以鼓励探索，适用于线性MDP，样本复杂度具有最优维度依赖关系。

    

    尽管策略优化算法对于近期强化学习的实证成功发挥了重要作用，但策略优化的现有理论理解仍然相当有限 - 它们要么局限于表格MDP，要么在在线强化学习中存在高度亚最优的样本复杂度问题。本文提出了一种简单高效的在线强化学习策略优化框架 - 乐观自然策略梯度。乐观自然策略梯度可以看作是将经典自然策略梯度算法[Kakade，2001]与乐观策略评估子程序简单组合以鼓励探索。对于$d$-维线性MDP，乐观自然策略梯度具有计算效率，并且在$\tilde{O}(d^2/\varepsilon^3)$ 次采样内学习 $\varepsilon$ -最优策略，这是第一个具有最优维度依赖关系$\tilde {\Theta}(d^2)$样本复杂度的计算高效算法。它也超越了目前领先的一些状态of-the-art算法。

    While policy optimization algorithms have played an important role in recent empirical success of Reinforcement Learning (RL), the existing theoretical understanding of policy optimization remains rather limited -- they are either restricted to tabular MDPs or suffer from highly suboptimal sample complexity, especial in online RL where exploration is necessary. This paper proposes a simple efficient policy optimization framework -- Optimistic NPG for online RL. Optimistic NPG can be viewed as simply combining of the classic natural policy gradient (NPG) algorithm [Kakade, 2001] with optimistic policy evaluation subroutines to encourage exploration. For $d$-dimensional linear MDPs, Optimistic NPG is computationally efficient, and learns an $\varepsilon$-optimal policy within $\tilde{O}(d^2/\varepsilon^3)$ samples, which is the first computationally efficient algorithm whose sample complexity has the optimal dimension dependence $\tilde{\Theta}(d^2)$. It also improves over state-of-the-a
    
[^58]: Tree of Thoughts: 利用大语言模型进行深思熟虑的问题解决

    Tree of Thoughts: Deliberate Problem Solving with Large Language Models. (arXiv:2305.10601v1 [cs.CL])

    [http://arxiv.org/abs/2305.10601](http://arxiv.org/abs/2305.10601)

    本研究提出了一种新的推理框架——思维之树（ToT），可以增强语言模型的问题解决能力，帮助语言模型进行深思熟虑的决策，以及自我评估和全局选择。

    

    语言模型越来越广泛地用于解决各种任务的通用问题，但在推理过程中仍然受限于基于标记、从左到右的决策过程。这意味着在需要探索、战略前瞻或初始决策发挥关键作用的任务中，他们可能会遇到困难。为了克服这些挑战，我们引入了一种新的语言模型推理框架——思维之树（ToT），它将通常用于提示语言模型的思维链方法泛化，并使用一致的文本单位（思维）进行探究，这些思维作为解决问题的中间步骤。思维之树允许语言模型通过考虑多个不同的推理路径和自我评估来进行深思熟虑的决策，并决定下一步的行动，同时在必要时向前或向后跟踪以进行全局选择。我们的实验表明，ToT显著增强了语言模型的解决问题能力。

    Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abil
    
[^59]: 原则驱动自我对齐的最小人力监督的语言模型从零开始构建

    Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. (arXiv:2305.03047v1 [cs.LG])

    [http://arxiv.org/abs/2305.03047](http://arxiv.org/abs/2305.03047)

    这篇论文提出了SELF-ALIGN方法，使用基于原则的推理和LLMs的生成能力以最少的人类监督实现AI代理的自我对齐。

    

    最近的AI助手代理，如ChatGPT，主要依赖于监督微调和人类反馈的强化学习来对齐大型语言模型的输出与人类意图，确保它们是有用的、道德的、可靠的。然而，这种依赖性可能会极大地限制AI助手代理的真正潜力，因为获得人类监督的成本很高，相关问题有质量、可靠性、多样性、自一致性和不良偏见。为了解决这些挑战，我们提出了一种新的方法 SELF-ALIGN，它结合了基于原则的推理和LLMs的生成能力，以最少的人类监督实现AI代理的自我对齐。方法包括四个阶段：第一，我们使用LLM生成合成提示，使用主题引导方法增加提示多样性；第二，我们使用一小组人工编写的AI模型原则，并指导AI模型遵循；

    Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and gu
    
[^60]: 流数据高效学习

    Stream Efficient Learning. (arXiv:2305.02217v1 [cs.LG])

    [http://arxiv.org/abs/2305.02217](http://arxiv.org/abs/2305.02217)

    本文介绍了“流高效学习”的概念，该概念旨在解决从数据流中机器学习的效率问题，其泛化性能不仅取决于接收到了多少数据，而且还取决于有多少数据能够及时有效地被利用，加上资源和速度的考虑。

    

    许多真实世界应用的数据往往随着时间的积累以流的形式进行。与传统的机器学习研究关注于从给定的训练数据集中学习不同，从数据流中学习不能忽视流入的数据流可能是无休止的、规模巨大、变化未知，并且假设有足够的计算/存储资源可以及时处理所有接收到的数据是不现实的。因此，从数据流中学习的泛化性能不仅取决于接收到了多少数据，而且取决于有多少数据能够被及时地有效利用，加上资源和速度的考虑，再加上学习算法的能力和问题的复杂度。为此，在本文中我们介绍了机器学习吞吐量的概念，定义了流高效学习，并提出了一个初步的理论框架。

    Data in many real-world applications are often accumulated over time, like a stream. In contrast to conventional machine learning studies that focus on learning from a given training data set, learning from data streams cannot ignore the fact that the incoming data stream can be potentially endless with overwhelming size and unknown changes, and it is impractical to assume to have sufficient computational/storage resource such that all received data can be handled in time. Thus, the generalization performance of learning from data streams depends not only on how many data have been received, but also on how many data can be well exploited timely, with resource and rapidity concerns, in addition to the ability of learning algorithm and complexity of the problem. For this purpose, in this article we introduce the notion of machine learning throughput, define Stream Efficient Learning and present a preliminary theoretical framework.
    
[^61]: 消除条件随机优化偏差

    Debiasing Conditional Stochastic Optimization. (arXiv:2304.10613v1 [cs.LG])

    [http://arxiv.org/abs/2304.10613](http://arxiv.org/abs/2304.10613)

    本文提出了一种通用的随机外推技术，用于降低条件随机优化问题中的偏差，并证明在非凸光滑目标函数中，将外推与方差缩减技术相结合可以显著改善样本复杂度。

    

    本文研究了覆盖了多个应用领域，包括投资组合选择、强化学习、鲁棒学习、因果推断等的条件随机优化（CSO）问题。由于其嵌套结构，CSO目标的样本平均梯度存在偏差，因此需要较高的样本复杂度才能达到收敛。我们引入了一种有效降低偏差的通用随机外推技术。我们证明，在非凸光滑目标函数中，将这种外推与方差缩减技术相结合，可以达到比现有界限更好的样本复杂度。我们还开发了用于有限和变量的CSO的新算法，也显著改进了现有结果。最后，我们认为我们的去偏技术也可能是适用于其他随机优化问题的有趣工具。

    In this paper, we study the conditional stochastic optimization (CSO) problem which covers a variety of applications including portfolio selection, reinforcement learning, robust learning, causal inference, etc. The sample-averaged gradient of the CSO objective is biased due to its nested structure and therefore requires a high sample complexity to reach convergence. We introduce a general stochastic extrapolation technique that effectively reduces the bias. We show that for nonconvex smooth objectives, combining this extrapolation with variance reduction techniques can achieve a significantly better sample complexity than existing bounds. We also develop new algorithms for the finite-sum variant of CSO that also significantly improve upon existing results. Finally, we believe that our debiasing technique could be an interesting tool applicable to other stochastic optimization problems too.
    
[^62]: 基于混合量子神经网络的深度强化学习

    Deep Reinforcement Learning Using Hybrid Quantum Neural Network. (arXiv:2304.10159v1 [quant-ph])

    [http://arxiv.org/abs/2304.10159](http://arxiv.org/abs/2304.10159)

    该研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并评估了其潜力。最终总结了开发深度量子学习的前景和结论。

    

    量子计算对于促进当前机器学习算法处理更高数据维度或减少深度神经网络模型的总体训练参数的限制具有强烈的影响。本研究基于门控量子计算机，设计了一个参数化的量子电路来解决深度强化学习问题，并采用深度 Q-Learning 方法。该研究评估了其潜力。因此，设计并培训了一个基于最新的 Qiskit 和 PyTorch 框架的新型 PQC，以与完全经典的深度神经网络进行比较，带或不带集成 PQC。研究最后总结了其关于开发深度量子学习解决迷宫问题或其他强化学习问题的前景和结论。

    Quantum computation has a strong implication for advancing the current limitation of machine learning algorithms to deal with higher data dimensions or reducing the overall training parameters for a deep neural network model. Based on a gate-based quantum computer, a parameterized quantum circuit was designed to solve a model-free reinforcement learning problem with the deep-Q learning method. This research has investigated and evaluated its potential. Therefore, a novel PQC based on the latest Qiskit and PyTorch framework was designed and trained to compare with a full-classical deep neural network with and without integrated PQC. At the end of the research, the research draws its conclusion and prospects on developing deep quantum learning in solving a maze problem or other reinforcement learning problems.
    
[^63]: 用TiDE进行长期预测：时间序列稠密编码器

    Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])

    [http://arxiv.org/abs/2304.08424](http://arxiv.org/abs/2304.08424)

    TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。

    

    最近的研究表明，相比于基于Transformer的方法，简单的线性模型在长期时间序列预测中表现更好。鉴于此，我们提出了一种基于多层感知机(MLP)的编码器-解码器模型，即时间序列稠密编码器(TiDE)，用于长期时间序列预测。它既享有线性模型的简单性和速度，又能处理协变量和非线性依赖。从理论上讲，我们证明了我们模型的最简线性类比在一些假设下可以达到线性动态系统(LDS)的近乎最优误差率。实证上，我们表明，我们的方法可以在流行的长期时间序列预测基准测试中匹配或胜过以前的方法，同时比最佳的基于Transformer的模型快5-10倍。

    Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
    
[^64]: HuggingGPT: 在HugingFace中使用ChatGPT及其伙伴解决AI任务

    HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])

    [http://arxiv.org/abs/2303.17580](http://arxiv.org/abs/2303.17580)

    用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。

    

    解决不同领域和模态的复杂AI任务是通向人工智能的关键步骤。本文提出了一个系统，利用大型语言模型（LLMs）作为控制器来管理现有的AI模型以解决AI任务，语言成为通用接口来赋能它。具体来说，我们使用ChatGPT作为任务规划工具，根据HuggingFace中可用的模型功能描述来选择模型，在选定AI模型的情况下执行每个子任务，并总结响应。

    Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
    
[^65]: Plan4MC: 基于技能强化学习和规划的 Minecraft 开放式任务解决方案

    Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks. (arXiv:2303.16563v1 [cs.LG])

    [http://arxiv.org/abs/2303.16563](http://arxiv.org/abs/2303.16563)

    本论文提出了一种基于技能强化学习和规划的 Minecraft 开放式任务解决方案，通过学习基本技能和技能规划的方法有效提高了任务解决效率，实现了24个不同的任务并在大多数任务中优于基线算法。

    

    本论文研究在 Minecraft 中构建一个多任务智能体。在没有人工演示的情况下，用强化学习（RL）解决这个开放式环境中的长程任务是极其样本低效的。为了解决这个问题，我们将 Minecraft 任务的解决分解成学习基本技能和基于技能进行规划两个阶段。我们在 Minecraft 中提出了三种类型的细粒度基本技能，并使用具有内在奖励的 RL 方法来实现成功率高的基本技能学习。在技能规划方面，我们使用大型语言模型来发现技能之间的关系，预先构建技能图。当智能体解决任务时，我们的技能搜索算法在技能图上行走并生成适当的技能计划。在实验中，我们的方法解决了 24 个不同的 Minecraft 任务，其中许多任务需要连续执行超过 10 个技能。我们的方法在大多数任务中优于基线算法。项目的网址和代码可以在 https://www.rocwang.me/plan4mc.html 找到。

    We study building a multi-task agent in Minecraft. Without human demonstrations, solving long-horizon tasks in this open-ended environment with reinforcement learning (RL) is extremely sample inefficient. To tackle the challenge, we decompose solving Minecraft tasks into learning basic skills and planning over the skills. We propose three types of fine-grained basic skills in Minecraft, and use RL with intrinsic rewards to accomplish basic skills with high success rates. For skill planning, we use Large Language Models to find the relationships between skills and build a skill graph in advance. When the agent is solving a task, our skill search algorithm walks on the skill graph and generates the proper skill plans for the agent. In experiments, our method accomplishes 24 diverse Minecraft tasks, where many tasks require sequentially executing for more than 10 skills. Our method outperforms baselines in most tasks by a large margin. The project's website and code can be found at https:
    
[^66]: 扩散式对抗净化的鲁棒评估

    Robust Evaluation of Diffusion-Based Adversarial Purification. (arXiv:2303.09051v1 [cs.CV])

    [http://arxiv.org/abs/2303.09051](http://arxiv.org/abs/2303.09051)

    本文分析了对基于扩散式净化方法的评估方式，并提出了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。同时，我们提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。

    

    我们质疑当前对基于扩散式净化方法的评估方式。扩散式净化方法旨在消除测试数据点中的对抗性影响。由于基于训练和测试的解耦，该方法越来越受到关注，作为对抗性训练的替代方法。为了测量净化方法的鲁棒性，通常采用众所周知的白盒攻击。然而，由于这些攻击通常是为对抗性训练而量身定制的，因此不知道这些攻击是否对扩散式净化最有效。我们分析了当前的实践，并提供了一个新的指导方针，以衡量净化方法对抗性攻击的鲁棒性。基于我们的分析，我们进一步提出了一种新的净化策略，展示了与最先进的对抗性训练方法相竞争的结果。

    We question the current evaluation practice on diffusion-based purification methods. Diffusion-based purification methods aim to remove adversarial effects from an input data point at test time. The approach gains increasing attention as an alternative to adversarial training due to the disentangling between training and testing. Well-known white-box attacks are often employed to measure the robustness of the purification. However, it is unknown whether these attacks are the most effective for the diffusion-based purification since the attacks are often tailored for adversarial training. We analyze the current practices and provide a new guideline for measuring the robustness of purification methods against adversarial attacks. Based on our analysis, we further propose a new purification strategy showing competitive results against the state-of-the-art adversarial training approaches.
    
[^67]: 基于前缀树的分子质谱预测

    Prefix-tree Decoding for Predicting Mass Spectra from Molecules. (arXiv:2303.06470v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.06470](http://arxiv.org/abs/2303.06470)

    本文提出了一种基于前缀树的中间策略，通过将质谱视为化学公式的集合来预测分子的质谱，克服了化学子公式的组合可能性。

    This paper proposes an intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms, and decoding the formula set using a prefix tree structure, atom-type by atom-type, overcoming the combinatorial possibilities for chemical subformulae.

    计算预测分子的质谱已经实现了临床相关代谢物的发现。然而，这样的预测工具仍然存在局限性，因为它们占据了两个极端，要么通过过度刚性的约束和较差的时间复杂度组合分子来进行操作，要么通过解码有损和非物理离散化的光谱向量来进行操作。在这项工作中，我们介绍了一种新的中间策略，通过将质谱视为化学公式的集合来预测分子的质谱，这些化学公式本身是原子的多重集合。在首先对输入分子图进行编码后，我们解码一组化学子公式，每个化学子公式指定质谱中的一个预测峰，其强度由第二个模型预测。我们的关键洞察力是通过使用前缀树结构，逐个原子类型地解码公式集，克服了化学子公式的组合可能性。

    Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we introduce a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of chemical formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of chemical subformulae, each of which specify a predicted peak in the mass spectra, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for chemical subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, represent
    
[^68]: 通过变分量子样条使非线性量子操作成为可能

    Enabling Non-Linear Quantum Operations through Variational Quantum Splines. (arXiv:2303.04788v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2303.04788](http://arxiv.org/abs/2303.04788)

    本文提出了一种新方法——广义QSplines，使用混合量子-经典计算来近似非线性量子激活函数，克服了原始QSplines在量子硬件方面的高要求，并适合嵌入现有的量子神经网络架构中。

    This paper proposes a novel method, Generalised QSplines (GQSplines), for approximating non-linear quantum activation functions using hybrid quantum-classical computation, which overcomes the highly demanding requirements of the original QSplines in terms of quantum hardware and is suitable to be embedded in existing quantum neural network architectures.

    量子力学的假设仅对量子状态施加幺正变换，这对于量子机器学习算法来说是一个严重的限制。最近提出了量子样条（QSplines）来近似量子激活函数，以在量子算法中引入非线性。然而，QSplines使用HHL作为子程序，并需要一个容错的量子计算机才能正确实现。本文提出了广义QSplines（GQSplines），一种使用混合量子-经典计算来近似非线性量子激活函数的新方法。GQSplines克服了原始QSplines在量子硬件方面的高要求，并可以使用近期的量子计算机来实现。此外，所提出的方法依赖于灵活的问题表示，适合嵌入现有的量子神经网络架构中。

    The postulates of quantum mechanics impose only unitary transformations on quantum states, which is a severe limitation for quantum machine learning algorithms. Quantum Splines (QSplines) have recently been proposed to approximate quantum activation functions to introduce non-linearity in quantum algorithms. However, QSplines make use of the HHL as a subroutine and require a fault-tolerant quantum computer to be correctly implemented. This work proposes the Generalised QSplines (GQSplines), a novel method for approximating non-linear quantum activation functions using hybrid quantum-classical computation. The GQSplines overcome the highly demanding requirements of the original QSplines in terms of quantum hardware and can be implemented using near-term quantum computers. Furthermore, the proposed method relies on a flexible problem representation for non-linear approximation and it is suitable to be embedded in existing quantum neural network architectures. In addition, we provide a pr
    
[^69]: 论盗用语言模型解码算法的风险

    On the Risks of Stealing the Decoding Algorithms of Language Models. (arXiv:2303.04729v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04729](http://arxiv.org/abs/2303.04729)

    这项工作首次展示，一个拥有典型API访问权限的对手可以以极低的金钱成本窃取GPT-2和GPT-3等LM的解码算法的类型和超参数。

    

    现代语言模型（LM）生成文本的关键组成部分是选择和调整解码算法。这些算法确定如何从LM生成的内部概率分布中生成文本。选择解码算法并调整其超参数的过程需要显著的时间、手动工作和计算，还需要进行广泛的人类评估。因此，解码算法的身份和超参数被认为是极其有价值的。在这项工作中，我们首次展示了一个拥有典型API访问权限的对手可以以极低的金钱成本窃取其解码算法的类型和超参数。我们的攻击对用于文本生成API的流行LM有效，包括GPT-2和GPT-3。我们证明了只需花费几美元，例如0.8美元、1美元、4美元和40美元，就可以盗取此类信息。

    A key component of generating text from modern language models (LM) is the selection and tuning of decoding algorithms. These algorithms determine how to generate text from the internal probability distribution generated by the LM. The process of choosing a decoding algorithm and tuning its hyperparameters takes significant time, manual effort, and computation, and it also requires extensive human evaluation. Therefore, the identity and hyperparameters of such decoding algorithms are considered to be extremely valuable to their owners. In this work, we show, for the first time, that an adversary with typical API access to an LM can steal the type and hyperparameters of its decoding algorithms at very low monetary costs. Our attack is effective against popular LMs used in text generation APIs, including GPT-2 and GPT-3. We demonstrate the feasibility of stealing such information with only a few dollars, e.g., $\$0.8$, $\$1$, $\$4$, and $\$40$ for the four versions of GPT-3.
    
[^70]: 使用样本评估生成模型的泛化能力

    Feature Likelihood Score: Evaluating Generalization of Generative Models Using Samples. (arXiv:2302.04440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04440](http://arxiv.org/abs/2302.04440)

    本文提出了一种新的特征似然分数（FLS）评估深度生成模型泛化能力的度量指标，用于评估生成样本的新颖性、保真度和多样性，通过实验证明其优于现有的指标，并能够检测出过拟合问题。

    

    过去几年，深度生成模型的发展取得了令人印象深刻的进展，能够生成高维、复杂和照片般逼真的数据。然而，目前评估这些模型的方法仍然不完全：标准的基于似然的指标并不总是适用于这些模型，也很少与感知保真度相关，而基于样本的指标（如FID）对过拟合不敏感。为了解决这些局限性，我们提出了一种新的度量指标，称为特征似然分数（FLS），它是一个参数化的基于样本的分数，使用密度估计来提供全面的三相评估，考虑生成样本的新颖性（即与训练样本不同）、保真度和多样性。我们通过实验证明了FLS在检测过拟合问题上的能力，先前提出的度量指标无法解决这些问题。我们还对各种图像数据集和模型类别进行了广泛的FLS评估。

    The past few years have seen impressive progress in the development of deep generative models capable of producing high-dimensional, complex, and photo-realistic data. However, current methods for evaluating such models remain incomplete: standard likelihood-based metrics do not always apply and rarely correlate with perceptual fidelity, while sample-based metrics, such as FID, are insensitive to overfitting, i.e., inability to generalize beyond the training set. To address these limitations, we propose a new metric called the Feature Likelihood Score (FLS), a parametric sample-based score that uses density estimation to provide a comprehensive trichotomic evaluation accounting for novelty (i.e., different from the training samples), fidelity, and diversity of generated samples. We empirically demonstrate the ability of FLS to identify specific overfitting problem cases, where previously proposed metrics fail. We also extensively evaluate FLS on various image datasets and model classes
    
[^71]: 卷积神经算子用于偏微分方程的鲁棒准确学习

    Convolutional Neural Operators for robust and accurate learning of PDEs. (arXiv:2302.01178v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01178](http://arxiv.org/abs/2302.01178)

    该论文提出了卷积神经算子（CNO），它能够在学习偏微分方程的上下文中处理函数输入和输出，并且证明了 CNOs 可以近似偏微分方程中出现的算子到所需的精度。在测试中，CNOs 显着优于基线，这为鲁棒准确操作学习的另一种框架铺平了道路。

    

    尽管在传统机器学习中非常成功，基于卷积的神经网络架构在函数空间中存在不一致性，因此在学习偏微分方程的解算子的上下文中被广泛忽视。在这里，我们提出了卷积神经网络的新适应方法，以证明它们确实能够处理输入和输出的函数。结果产生的架构称为卷积神经算子（CNO），专门设计为在计算机上以离散形式实现时保持其基本的连续性质。我们证明了一个普适定理，以展示CNOs可以近似偏微分方程中出现的算子到所需的精度。 CNOs在一个包括各种具有可能具有多尺度解的偏微分方程的新套件上进行测试，并被观察到显着优于基线，为鲁棒准确操作学习的另一种框架铺平了道路。

    Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with possibly multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.
    
[^72]: 基于 ODoS 滤波器和深度学习网络的医学图像曲线对象分割

    Curvilinear object segmentation in medical images based on ODoS filter and deep learning network. (arXiv:2301.07475v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.07475](http://arxiv.org/abs/2301.07475)

    该论文提出了一种基于 ODoS 滤波器和深度学习网络的曲线结构分割框架，用于解决医学图像中曲线对象的自动分割问题。

    

    在医学图像中，曲线对象的自动分割对于人类疾病的诊断和评估起着重要作用。然而，由于不同的图像外观，曲线对象与其周围背景之间的低对比度，薄而不均匀的曲线结构以及不适当的背景光照条件等问题，这是一项具有挑战性的任务。为了克服这些挑战，我们提出了一个基于ODoS滤波器和深度学习网络的独特曲线结构分割框架，用于在医学图像中进行曲线对象分割。目前，许多深度学习模型强调开发深度体系结构，并忽略捕获曲线对象的结构特征，这可能导致不理想的结果。因此，我们提出了一种将ODoS滤波器作为深度学习网络一部分的新方法，以改进空间信息的提取。

    Automatic segmentation of curvilinear objects in medical images plays an important role in the diagnosis and evaluation of human diseases, yet it is a challenging uncertainty in the complex segmentation tasks due to different issues such as various image appearances, low contrast between curvilinear objects and their surrounding backgrounds, thin and uneven curvilinear structures, and improper background illumination conditions. To overcome these challenges, we present a unique curvilinear structure segmentation framework based on an oriented derivative of stick (ODoS) filter and a deep learning network for curvilinear object segmentation in medical images. Currently, a large number of deep learning models emphasize developing deep architectures and ignore capturing the structural features of curvilinear objects, which may lead to unsatisfactory results. Consequently, a new approach that incorporates an ODoS filter as part of a deep learning network is presented to improve the spatial 
    
[^73]: 物理学知识指导的高斯过程回归应用于解决线性偏微分方程

    Physics-Informed Gaussian Process Regression Generalizes Linear PDE Solvers. (arXiv:2212.12474v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12474](http://arxiv.org/abs/2212.12474)

    本文使用物理学知识指导的高斯过程回归方法，解决线性偏微分方程求解器无法量化近似误差的问题。

    

    线性偏微分方程是一类重要且广泛应用的机械模型，描述了物理过程，例如热传导、电磁学和波传播等。实践中，通常使用基于离散化的专门数值方法来解决偏微分方程。这些求解器通常使用未知模型参数的估计值以及如果可用的话，物理测量值用于初始化。这些求解器经常嵌入到具有下游应用的更大的科学模型中，因此误差量化起着关键作用。然而，经典的偏微分方程求解器忽略参数和测量不确定性，可能无法产生一致性的估计值，以用于计算其固有的逼近误差。本文通过将求解线性偏微分方程解释为物理学知识指导的高斯过程回归来解决这个问题。我们的框架基于高斯过程推理定理的一个关键推广，该定理适用于通过任意界面进行观察的情况。

    Linear partial differential equations (PDEs) are an important, widely applied class of mechanistic models, describing physical processes such as heat transfer, electromagnetism, and wave propagation. In practice, specialized numerical methods based on discretization are used to solve PDEs. They generally use an estimate of the unknown model parameters and, if available, physical measurements for initialization. Such solvers are often embedded into larger scientific models with a downstream application and thus error quantification plays a key role. However, by ignoring parameter and measurement uncertainty, classical PDE solvers may fail to produce consistent estimates of their inherent approximation error. In this work, we approach this problem in a principled fashion by interpreting solving linear PDEs as physics-informed Gaussian process (GP) regression. Our framework is based on a key generalization of the Gaussian process inference theorem to observations made via an arbitrary bou
    
[^74]: 基于漏斗的强化学习中信号时态逻辑任务的奖励塑形

    Funnel-based Reward Shaping for Signal Temporal Logic Tasks in Reinforcement Learning. (arXiv:2212.03181v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.03181](http://arxiv.org/abs/2212.03181)

    本文提出了一种基于漏斗函数的强化学习算法，用于在连续状态空间中学习鲁棒满足信号时态逻辑规范的时间依赖策略。

    

    信号时态逻辑（STL）是描述动态系统复杂时态和逻辑行为的强大框架。许多研究尝试利用强化学习来学习强制执行STL规范的控制器，然而，他们无法有效解决在连续状态空间中确保鲁棒满足和保持可控性的挑战。本文借助漏斗函数的概念，提出了一种可控的强化学习算法，用于学习连续状态空间中STL规范的鲁棒满足的时间依赖策略。我们在不同环境下演示了我们方法的实用性。

    Signal Temporal Logic (STL) is a powerful framework for describing the complex temporal and logical behaviour of the dynamical system. Numerous studies have attempted to employ reinforcement learning to learn a controller that enforces STL specifications; however, they have been unable to effectively tackle the challenges of ensuring robust satisfaction in continuous state space and maintaining tractability. In this paper, leveraging the concept of funnel functions, we propose a tractable reinforcement learning algorithm to learn a time-dependent policy for robust satisfaction of STL specification in continuous state space. We demonstrate the utility of our approach on several STL tasks using different environments.
    
[^75]: 过度参数化的分类器泛化能力强，因为糟糕的解决方案很少

    Highly over-parameterized classifiers generalize since bad solutions are rare. (arXiv:2211.03570v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03570](http://arxiv.org/abs/2211.03570)

    本文研究发现，在过度参数化的情况下，零训练误差的全局最小值中“坏”方案的占比随训练数据的增加而指数级递减，并能解释高度参数化的神经网络具有出乎意料的良好泛化能力。

    

    我们研究了过度参数化的分类器的泛化能力，其中通过经验风险最小化（ERM）学习导致零训练误差。在这些过度参数化的设置中，有许多具有零训练误差的全局最小值，其中一些比其他的更具有普适性。我们表明，在一定的条件下，“坏”全局最小值的分数，其真实误差大于ε，与训练数据n的数量指数级地递减到零。该范围取决于用于给定分类问题的分类器函数集合上真实误差的分布，不一定取决于分类器函数集合的大小或复杂度（例如参数数量）。这可能解释了即使高度参数化的神经网络也具有出乎意料的良好泛化能力。我们在合成数据集和MNIST的子集上通过实验来支持我们的数学框架。

    We study the generalization of over-parameterized classifiers where Empirical Risk Minimization (ERM) for learning leads to zero training error. In these over-parameterized settings there are many global minima with zero training error, some of which generalize better than others. We show that under certain conditions the fraction of "bad" global minima with a true error larger than {\epsilon} decays to zero exponentially fast with the number of training data n. The bound depends on the distribution of the true error over the set of classifier functions used for the given classification problem, and does not necessarily depend on the size or complexity (e.g. the number of parameters) of the classifier function set. This might explain the unexpectedly good generalization even of highly over-parameterized Neural Networks. We support our mathematical framework with experiments on a synthetic data set and a subset of MNIST.
    
[^76]: 利用生命周期自适应处理学习自适应系统中适应空间的漂移

    Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive Systems using Lifelong Self-Adaptation. (arXiv:2211.02658v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.02658](http://arxiv.org/abs/2211.02658)

    机器学习在自适应系统中成为热门方法，但利用机器学习会面临适应空间中的漂移问题。本文提出了一种名为"生命周期自适应"的新方法，能够更好地处理适应空间的漂移。

    

    最近，机器学习 (ML) 已成为支持自适应的热门方法。ML 已被用来处理自适应中的几个问题，例如在不确定性下维护最新的运行时模型和可扩展的决策制定。然而，利用 ML 存在固有的挑战。在本文中，我们着重讨论面向基于学习的自适应系统的一个特别重要的挑战：适应空间中的漂移。通过适应空间，我们指的是自适应系统在某一特定时间可以选择的适应选项的集合，以根据适应选项的质量属性进行适应。适应空间的漂移源于影响适应选项质量属性的不确定性。这种漂移可能意味着最终没有适应选项能够满足最初的适应目标，从而降低系统的质量，或者可能出现允许增强适应目标的适应选项。在 ML 中，这种漂移通常被称为概念漂移或实例漂移。为了解决这个挑战，我们提出了一种名为“生命周期自适应”的新方法。生命周期自适应对 ML powered self-adaptation 进行了扩展，使其能够更好地处理适应空间的漂移。

    Recently, machine learning (ML) has become a popular approach to support self-adaptation. ML has been used to deal with several problems in self-adaptation, such as maintaining an up-to-date runtime model under uncertainty and scalable decision-making. Yet, exploiting ML comes with inherent challenges. In this paper, we focus on a particularly important challenge for learning-based self-adaptive systems: drift in adaptation spaces. With adaptation space we refer to the set of adaptation options a self-adaptive system can select from at a given time to adapt based on the estimated quality properties of the adaptation options. Drift of adaptation spaces originates from uncertainties, affecting the quality properties of the adaptation options. Such drift may imply that eventually no adaptation option can satisfy the initial set of the adaptation goals, deteriorating the quality of the system, or adaptation options may emerge that allow enhancing the adaptation goals. In ML, such shift cor
    
[^77]: 学习图像表示以进行异常检测：在药物开发中发现组织学改变的应用

    Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.07675](http://arxiv.org/abs/2210.07675)

    该论文提出了一种基于CNN的异常检测系统，通过对健康组织进行辅助任务训练，使表示适应组织中的相关细节，实现对组织学图像中的异常情况检测。

    

    我们提出了一种用于组织病理学图像异常检测的系统。在组织学中，正常样本通常是大量存在的，而异常（病理）情况通常很少或不可用。在这种情况下，使用在健康数据上训练的单类分类器可以检测到分布外的异常样本。这样的方法与预训练的卷积神经网络（CNN）图像表示相结合，以前已经用于异常检测（AD）。但是，预训练的现成CNN表示可能对组织中的异常情况不敏感，而健康组织的自然变异可能导致远离的表示。为了使表示适应健康组织中的相关细节，我们建议在辅助任务上训练CNN，该任务区分不同物种、器官和染色试剂的健康组织。几乎不需要额外的标注工作量，因为健康样本可以自动获得上述标签。在训练中，我们强制执行

    We present a system for anomaly detection in histopathological images. In histology, normal samples are usually abundant, whereas anomalous (pathological) cases are scarce or not available. Under such settings, one-class classifiers trained on healthy data can detect out-of-distribution anomalous samples. Such approaches combined with pre-trained Convolutional Neural Network (CNN) representations of images were previously employed for anomaly detection (AD). However, pre-trained off-the-shelf CNN representations may not be sensitive to abnormal conditions in tissues, while natural variations of healthy tissue may result in distant representations. To adapt representations to relevant details in healthy tissue we propose training a CNN on an auxiliary task that discriminates healthy tissue of different species, organs, and staining reagents. Almost no additional labeling workload is required, since healthy samples come automatically with aforementioned labels. During training we enforce
    
[^78]: 长什么样的鸭嘴兽？生成自定义提示以进行零样本图像分类

    What does a platypus look like? Generating customized prompts for zero-shot image classification. (arXiv:2209.03320v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.03320](http://arxiv.org/abs/2209.03320)

    本文介绍了一种通过结合大型语言模型和开放词汇模型生成自定义提示的方法，以提高零样本图像分类的准确性和效率。通过利用语言模型生成描述性句子的方式，这种方法能够捕捉图像类别的重要区分特征。

    

    开放词汇模型是图像分类的一种有前景的新范式。与传统的分类模型不同，开放词汇模型在推断时对任意指定的类别集合进行分类，这些类别由自然语言表示。这种自然语言通常包含一组手写模板（例如，“一张{}的照片”），每个类别名称都会填充进去。本文介绍了一种简单的方法来生成更高准确度的提示，而不依赖于任务领域的显式知识，并且需要更少的手工构建句子。为了实现这一目标，我们将开放词汇模型与大型语言模型（LLMs）结合起来，创建出通过语言模型生成的自定义提示（CuPL，读作“couple”）。具体而言，我们利用LLMs中包含的知识来生成包含图像类别的重要区分特征的描述性句子。这使得模型更加重视这些特征。

    Open-vocabulary models are a promising new paradigm for image classification. Unlike traditional classification models, open-vocabulary models classify among any arbitrary set of categories specified with natural language during inference. This natural language, called "prompts", typically consists of a set of hand-written templates (e.g., "a photo of a {}") which are completed with each of the category names. This work introduces a simple method to generate higher accuracy prompts, without relying on any explicit knowledge of the task domain and with far fewer hand-constructed sentences. To achieve this, we combine open-vocabulary models with large language models (LLMs) to create Customized Prompts via Language models (CuPL, pronounced "couple"). In particular, we leverage the knowledge contained in LLMs in order to generate many descriptive sentences that contain important discriminating characteristics of the image categories. This allows the model to place a greater importance on 
    
[^79]: 低资源情境下的知识抽取：调研与展望

    Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08063](http://arxiv.org/abs/2202.08063)

    低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。

    

    知识抽取（KE）旨在从非结构化文本中提取结构信息，通常遭受数据匮乏和出现未见类型（低资源情境）的困扰。许多神经网络方法已广泛研究并取得了令人瞩目的表现。本文对低资源情境下KE进行文献综述，并将现有的工作系统性地分为三种范式：（1）利用高资源数据，（2）利用更强的模型，（3）同时利用数据和模型。此外，本文提出有前途的应用，并概述了未来研究的一些潜在方向。我们希望我们的调研可以帮助学术和工业界更好地理解这一领域，激发更多的创意，提升更广泛的应用。

    Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
    

