# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution](https://rss.arxiv.org/abs/2402.01586) | 本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。 |
| [^2] | [Fundamental Properties of Causal Entropy and Information Gain](https://rss.arxiv.org/abs/2402.01341) | 本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。 |
| [^3] | [Vaccine: Perturbation-aware Alignment for Large Language Model](https://rss.arxiv.org/abs/2402.01109) | 疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。 |
| [^4] | [Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks](https://arxiv.org/abs/2402.12369) | 提出了一种通过卷积神经网络识别TESS全画幅图像光变曲线中的短周期变量的方法，实现了高效率的大规模档案搜索。 |
| [^5] | [A Critical Evaluation of AI Feedback for Aligning Large Language Models](https://arxiv.org/abs/2402.12366) | 研究质疑复杂的强化学习在AI反馈中的必要性，表明使用更强的教师模型进行监督微调可以超越现有的RLAIF管道。 |
| [^6] | [Universal Physics Transformers](https://arxiv.org/abs/2402.12365) | 提出了通用物理变压器（UPTs）这一新颖学习范式，能够模拟广泛的时空问题，同时适用于拉格朗日和欧拉离散化方案，有效地传播动态并允许查询潜在空间 |
| [^7] | [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354) | LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。 |
| [^8] | [GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations](https://arxiv.org/abs/2402.12348) | 该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。 |
| [^9] | [An Adversarial Approach to Evaluating the Robustness of Event Identification Models](https://arxiv.org/abs/2402.12338) | 该论文提出了一种基于物理的模态分解方法用于提取事件分类特征，并评估了可解释分类器在面对对抗算法时的鲁棒性。 |
| [^10] | [Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models](https://arxiv.org/abs/2402.12336) | 通过无监督对抗微调，提出了一种强大的CLIP视觉编码器，用于增强各种视觉-语言模型的鲁棒性。恶意第三方提供操纵图像的用户隐形攻击得以杜绝。 |
| [^11] | [Generating Survival Interpretable Trajectories and Data](https://arxiv.org/abs/2402.12331) | 提出了一种新的模型，能够生成生存轨迹和数据，并通过特定结构的自动编码器解决了预测、数据补充和生成原型时间相关轨迹等任务 |
| [^12] | [Query-Based Adversarial Prompt Generation](https://arxiv.org/abs/2402.12329) | 该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。 |
| [^13] | [LLM Agents for Psychology: A Study on Gamified Assessments](https://arxiv.org/abs/2402.12326) | 本研究提出了PsychoGAT（心理游戏代理）以实现心理评估的通用游戏化，通过将强大的LLM代理纳入角色，将标准量表转化为个性化且具有吸引力的互动小说游戏。 |
| [^14] | [Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment](https://arxiv.org/abs/2402.12320) | 提出了一种在非GPS战场环境中通过使用地标锚定节点实现移动部队或防御部队的虚拟坐标获取的策略，并利用Yolov5模型进行地标识别和有效的立体匹配算法进行地标距离估计。 |
| [^15] | [Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness](https://arxiv.org/abs/2402.12319) | 引入了FairSAR，一种独特的遗憾度量，以解决动态环境下的公平意识在线学习挑战。 |
| [^16] | [Multi-View Conformal Learning for Heterogeneous Sensor Fusion](https://arxiv.org/abs/2402.12307) | 我们提出了用于异构传感器融合的多视角一致模型，并引入了基于集合交集的多视角半一致模型。 |
| [^17] | [Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering](https://arxiv.org/abs/2402.12302) | 提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。 |
| [^18] | [Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling](https://arxiv.org/abs/2402.12292) | 该论文引入了一种贝叶斯框架，通过将数据驱动的正则化策略融入概率框架，提出了一种正则化去噪的方法，并应用于图像反演任务中，在成像中推动了贝叶斯推断。 |
| [^19] | [Refining Minimax Regret for Unsupervised Environment Design](https://arxiv.org/abs/2402.12284) | 介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化，能够克服极小化遗憾策略在遗憾上界时学习停滞的限制。 |
| [^20] | [Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources -- A Case Study on Federated Fine-tuning of LLaMA 2](https://arxiv.org/abs/2402.12271) | 本文介绍了一种跨异构云和高性能计算资源的安全联邦学习框架，利用Globus Compute和亚马逊云服务，实现了端到端的隐私保护，文中以LLaMA 27B模型的联邦微调为例。 |
| [^21] | [End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching](https://arxiv.org/abs/2402.12269) | 提出了利用部分掩码融合的Gromov-Wasserstein匹配进行任意大小图的端对端监督预测方法，并展示了其在不同任务上相比竞争者更高的效率和多功能性。 |
| [^22] | [On the Byzantine-Resilience of Distillation-Based Federated Learning](https://arxiv.org/abs/2402.12265) | 基于蒸馏的联邦学习在拜占庭环境下表现出极强的弹性，介绍了两种新的拜占庭攻击，并提出了一种增强拜占庭弹性的新方法。 |
| [^23] | [Uncertainty quantification in fine-tuned LLMs using LoRA ensembles](https://arxiv.org/abs/2402.12264) | 使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。 |
| [^24] | [Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms](https://arxiv.org/abs/2402.12263) | 提出了面向门控循环单元的定制混合精度低于8位量化方案，使用遗传算法优化模型尺寸和准确性，在四个不同顺序任务上展示混合精度解决方案优于同质精度解决方案，实现了模型尺寸缩减25%至55%同时保持准确性。 |
| [^25] | [Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach](https://arxiv.org/abs/2402.12260) | 该研究提出了一种元多目标强化学习方法，用于在车联网中进行非正交时效最优信息传播，以减少信息时效和传输功耗。 |
| [^26] | [Synthetic location trajectory generation using categorical diffusion models](https://arxiv.org/abs/2402.12242) | 使用连续扩散过程和映射方法，提出了一种使用分类扩散模型生成合成位置轨迹的方法。 |
| [^27] | [Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis](https://arxiv.org/abs/2402.12241) | 该论文分析了在动态系统中利用梯度下降进行监督学习的递归神经网络的性能，并证明在不需要海量过参数化的情况下，梯度下降可以达到最优性。 |
| [^28] | [BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts](https://arxiv.org/abs/2402.12240) | BEARS是一种集成技术，可以让神经符号模型意识到它们学习的概念的语义模糊性，帮助用户识别和怀疑低质量概念。 |
| [^29] | [Learning to Defer in Content Moderation: The Human-AI Interplay](https://arxiv.org/abs/2402.12237) | 本文提出了一个模型，捕捉内容审核中人工智能的相互作用。 |
| [^30] | [The Fundamental Limits of Least-Privilege Learning](https://arxiv.org/abs/2402.12235) | 最小权限学习存在一个基本的权衡，即表示对于给定任务的实用性和其泄漏到任务外属性之间存在无法避免的权衡。 |
| [^31] | [Kernel KMeans clustering splits for end-to-end unsupervised decision trees](https://arxiv.org/abs/2402.12232) | 提出了一种新颖的端到端训练的无监督二叉树用于聚类，称为Kauri，通过贪婪最大化 kernel KMeans 目标来执行，无需定义质心，并在多个数据集上展示其性能优于其他方法。 |
| [^32] | [Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations](https://arxiv.org/abs/2402.12231) | 扩散回火是一种新颖的正则化技术，可改善概率数值方法在普通微分方程中的参数优化收敛性，实现对复杂动态系统中参数的可靠估计 |
| [^33] | [AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling](https://arxiv.org/abs/2402.12226) | AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。 |
| [^34] | [CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation](https://arxiv.org/abs/2402.12222) | CovRL使用强化学习结合大型语言模型和覆盖反馈，通过构建加权覆盖映射并应用于基于LLM的变异器，以提升对JavaScript引擎的模糊测试效果 |
| [^35] | [Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting](https://arxiv.org/abs/2402.12220) | 这项研究展示了如何利用贝叶斯学习技术应用于参数高效微调，以防止灾难性遗忘，实现了预训练知识的保留，并在语言建模和语音合成任务中取得成功。 |
| [^36] | [Reformatted Alignment](https://arxiv.org/abs/2402.12219) | 本文提出了一种名为ReAlign的简单有效方法，通过重新格式化指导数据的响应，显著提升了大型语言模型（LLMs）与人类价值观的对齐能力。 |
| [^37] | [Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT](https://arxiv.org/abs/2402.12201) | 字典学习技术在机械解释中攻克叠加，并提取更易理解的特征。该论文提出了一种电路发现框架，用于连接大量字典特征，相比于激活补丁，该框架受越界分布影响较小，并在渐近复杂度方面更有效。 |
| [^38] | [Zero shot VLMs for hate meme detection: Are we there yet?](https://arxiv.org/abs/2402.12198) | 本研究探讨了零-shot分类在处理复杂任务如恶意模因检测中的有效性 |
| [^39] | [Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data](https://arxiv.org/abs/2402.12190) | 提出了一种基于机器学习的框架，用于个性化反事实癌症治疗建议，集成了多种多组学技术的专家，可提供优越性能和决策解释。 |
| [^40] | [Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships](https://arxiv.org/abs/2402.12189) | 攻击者通过对预训练LM进行对抗微调，以放大原始训练数据的曝光，采用伪标签和机器生成概率来加强LM对预训练数据的保留。 |
| [^41] | [Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training](https://arxiv.org/abs/2402.12187) | 通过对抗训练，在深度学习中提出了一种对抗特征对齐的方法，以平衡鲁棒性和准确性，研究发现特征空间内部的不对齐经常导致 misclassification，这一方法旨在解决这些问题。 |
| [^42] | [Revisiting Data Augmentation in Deep Reinforcement Learning](https://arxiv.org/abs/2402.12181) | 重新审视深度强化学习中的数据增强，分析不同方法的影响，提出了如何更加原则地利用数据增强的建议。 |
| [^43] | [Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning](https://arxiv.org/abs/2402.12177) | Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。 |
| [^44] | [Learning Discretized Bayesian Networks with GOMEA](https://arxiv.org/abs/2402.12175) | 本文通过扩展基于Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA)的最先进结构学习方法，实现了联合学习变量离散化。 |
| [^45] | [Endowing Pre-trained Graph Models with Provable Fairness](https://arxiv.org/abs/2402.12161) | 提出了一种新的适配器调优框架，赋予预训练图模型具有可证明的公平性 |
| [^46] | [MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports](https://arxiv.org/abs/2402.12149) | 本文旨在定义和量化动量，为网球比赛的实时分析提供基础，通过建立基于数据驱动和基于经验公式的模型，采用多种机器学习算法进行融合，以探索竞技体育中的动量。 |
| [^47] | [Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement](https://arxiv.org/abs/2402.12146) | Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。 |
| [^48] | [Federated Bayesian Network Ensembles](https://arxiv.org/abs/2402.12142) | 联邦贝叶斯网络集成(FBNE)在联邦设置下表现优异，相较于本地模型和VertiBayes训练的模型，在保持类似性能的情况下提供了显著的训练速度提升。 |
| [^49] | [Molecule Generation and Optimization for Efficient Fragrance Creation](https://arxiv.org/abs/2402.12134) | 通过机器学习中心的方法，建立了一个将香水分子结构与人类嗅觉感知相连的混合模型，利用AI驱动的分子生成器和热力学模型，优化溶剂和分子组合，最终通过数学优化问题最小化新旧嗅觉体验之间的差异。 |
| [^50] | [DualView: Data Attribution from the Dual Perspective](https://arxiv.org/abs/2402.12118) | 提出了DualView，一种基于替代建模的后期数据归因方法，具有高效计算和优质评估结果。 |
| [^51] | [Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview](https://arxiv.org/abs/2402.12072) | 本论文概述了使用变分方法和机器学习解决成像中逆问题的方法，重点在于点估计器对抗性扰动下的鲁棒性以及探索数据一致解子空间以满足特定语义或纹理特性。 |
| [^52] | [Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks](https://arxiv.org/abs/2402.12067) | 经由类脑启发的慢特征分析方法生成的可解释视觉数据表示，能够在强化学习中提高导航任务表现。 |
| [^53] | [WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More](https://arxiv.org/abs/2402.12065) | 该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。 |
| [^54] | [Causal Equal Protection as Algorithmic Fairness](https://arxiv.org/abs/2402.12062) | 本文提出了一种新的算法公平性原则——平等保护，其关键在于将错误分类的风险均等化，避免了许多对传统分类平等原则的反例。 |
| [^55] | [All Language Models Large and Small](https://arxiv.org/abs/2402.12061) | LONDI框架可以在需要复杂决策和推理的地方选择性地使用大的语言模型，极大地降低了资源消耗。 |
| [^56] | [Linear bandits with polylogarithmic minimax regret](https://arxiv.org/abs/2402.12042) | 该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。 |
| [^57] | [Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations](https://arxiv.org/abs/2402.12038) | 本研究提出了Self-AMPLIFY方法，通过将事后解释方法应用于小型语言模型（SLMs），自动生成基于原因的解释，以提高它们自身的性能。 |
| [^58] | [Class-incremental Learning for Time Series: Benchmark and Evaluation](https://arxiv.org/abs/2402.12035) | 时间序列增量学习问题在图像和语言领域取得了进展，但在时间序列数据方面仍然相对较少研究，本文提出了一个全面的评估和基准测试方法。 |
| [^59] | [When Do Off-Policy and On-Policy Policy Gradient Methods Align?](https://arxiv.org/abs/2402.12034) | 该论文研究了离策略和在策略策略梯度方法之间的差异，并首次提出了减小该差距的条件，同时发现在条件不满足时会产生短板。 |
| [^60] | [Distilling Large Language Models for Text-Attributed Graph Learning](https://arxiv.org/abs/2402.12022) | 本研究旨在将大型语言模型和图模型的优势相结合，通过将LLMs的能力压缩到 TAG 学习的本地图模型中，解决它们之间的固有差距。 |
| [^61] | [An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking](https://arxiv.org/abs/2402.12015) | 基于Sarsa和Q-learning的智能目标跟踪索引策略，解决了雷达调度中短期性能和未来机动性的平衡挑战。 |
| [^62] | [Training Green AI Models Using Elite Samples](https://arxiv.org/abs/2402.12010) | 该论文提出了一个基于进化的采样框架，旨在识别精英训练样本，比较与传统方法的模型性能和能效优势，探讨其对可持续模型训练的可能性。 |
| [^63] | [Cluster Metric Sensitivity to Irrelevant Features](https://arxiv.org/abs/2402.12008) | 本文研究群集性能对添加到基线数据集中的嘈杂不相关变量的敏感度。 |
| [^64] | [Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models](https://arxiv.org/abs/2402.11997) | 大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。 |
| [^65] | [ISCUTE: Instance Segmentation of Cables Using Text Embedding](https://arxiv.org/abs/2402.11996) | 提出了一种基于文本提示的DLO实例分割技术，结合了CLIPSeg模型的文本条件语义分割能力和Segment Anything Model的零样本泛化能力，有效解决了传统方法在感知可变形直线对象如电线、电缆和柔性管道方面的挑战，性能超越了目前的技术水平，同时引入了一个新的DLO特定数据集。 |
| [^66] | [Network Inversion of Binarised Neural Nets](https://arxiv.org/abs/2402.11995) | 本文提出了一种新颖的方法，通过将训练后的二值化神经网络编码，实现了对其进行反漂的目的 |
| [^67] | [Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models](https://arxiv.org/abs/2402.11989) | 提出了隐私保护的低秩适应解决方案PrivateLoRA，通过最小化适应损失和代理攻击模型的MI增益来抵御成员推断攻击。 |
| [^68] | [Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling](https://arxiv.org/abs/2402.11985) | 提出了一种新的弱监督目标检测方法，利用可微的ROI提议网络和软ROI池化，在胸部X射线图像中胜过现有方法 |
| [^69] | [Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks](https://arxiv.org/abs/2402.11984) | 该研究提出了一种基于横向连接和Hebbian学习的神经操作新方法，能够通过投影保护知识 |
| [^70] | [Bayesian Active Learning for Censored Regression](https://arxiv.org/abs/2402.11973) | 该论文提出了一种在被审查回归中的贝叶斯主动学习方法($\mathcal{C}$-BALD)，通过推导被审查分布的熵和互信息，优化目标函数，在广泛数据集和模型下表现优异。 |
| [^71] | [Imbalance in Regression Datasets](https://arxiv.org/abs/2402.11963) | 回归数据集中的不平衡问题一直被忽视，本文通过理论分析和定义，展示了这一问题的重要性，并为未来研究提供了共同基础。 |
| [^72] | [DB-LLM: Accurate Dual-Binarization for Efficient LLMs](https://arxiv.org/abs/2402.11960) | 本文提出了一种名为DB-LLM的新颖双二值化方法，通过引入灵活双二值化(FDB)来平衡2位宽度的精度优势和二值化的效率优势，从而在提高LLMs的计算效率的同时保持了准确性。 |
| [^73] | [Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels](https://arxiv.org/abs/2402.11953) | 对抗样本的分类模式和时间侧信道的结合可以导致窃取预训练的CNN模型。 |
| [^74] | [A novel molecule generative model of VAE combined with Transformer](https://arxiv.org/abs/2402.11950) | 该研究提出了一种结合VAE与Transformer的模型，通过结构和参数优化，成功处理多样化分子的生成，生成的分子性能与现有模型相媲美，对生成具有未知结构的分子具有更优越的性能。 |
| [^75] | [Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model](https://arxiv.org/abs/2402.11948) | Mini-Hes提出了一种新的mini-block对角黑塞无约束优化方法，用于构建二阶潜在因子分析模型，解决了大数据量下二阶算法可行性的挑战。 |
| [^76] | [The effect of Leaky ReLUs on the training and generalization of overparameterized networks](https://arxiv.org/abs/2402.11942) | Leaky ReLU参数$\alpha=-1$在训练误差和泛化误差界方面是最优的选择。 |
| [^77] | [AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization](https://arxiv.org/abs/2402.11940) | 提出了一种新的对抗攻击策略AICAttack，旨在通过微小的图像扰动来攻击图像字幕模型，在黑盒攻击情景下具有良好的效果。 |
| [^78] | [SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning](https://arxiv.org/abs/2402.11933) | SLADE通过自监督学习在边缘流中迅速检测动态异常，无需依赖标签，主要通过观察节点交互模式的偏差来检测节点状态转变。 |
| [^79] | [Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching](https://arxiv.org/abs/2402.11925) | 我们提出了一种名为JD2P的新型离线架构，通过联合数据深化和预取技术，按顺序离线每个数据样本的特征，以减少物联网设备向边缘服务器传输数据时所需的能量消耗。 |
| [^80] | [A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning](https://arxiv.org/abs/2402.11922) | 提出了一种生成式预训练框架 GPDiff，通过在源城市数据优化的模型参数上进行预训练，将STG迁移学习转化为预训练生成式超网络，实现了对不同数据分布和特定城市特征的适应性。 |
| [^81] | [A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task](https://arxiv.org/abs/2402.11917) | 对在合成推理任务上训练的Transformer进行的机理分析揭示其实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置 |
| [^82] | [Scalable Virtual Valuations Combinatorial Auction Design by Combining Zeroth-Order and First-Order Optimization Method](https://arxiv.org/abs/2402.11904) | 本文提出了一种结合零阶和一阶优化方法，设计了可扩展的虚拟估值组合拍卖，以解决组合候选分配的可缩放性问题。 |
| [^83] | [Generative Semi-supervised Graph Anomaly Detection](https://arxiv.org/abs/2402.11887) | 提出了一种用于半监督图异常检测的生成式方法，通过生成模拟异常节点来训练判别性单类分类器，以更好地利用图中的已知正常节点。 |
| [^84] | [Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model](https://arxiv.org/abs/2402.11877) | 本文通过有限时间分析以及实证评估，探讨了集成模型方法的Q学习在样本复杂度方面的优势。 |
| [^85] | [LoRA Training in the NTK Regime has No Spurious Local Minima](https://arxiv.org/abs/2402.11867) | LoRA训练在NTK模式下消除了虚假局部最小值，有助于梯度下降找到低秩解并实现良好的泛化。 |
| [^86] | [Stochastic Hessian Fitting on Lie Group](https://arxiv.org/abs/2402.11858) | 本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。 |
| [^87] | [Communication-Efficient Distributed Learning with Local Immediate Error Compensation](https://arxiv.org/abs/2402.11857) | 提出了Local Immediate Error Compensated SGD（LIEC-SGD）优化算法，通过双向压缩和精心设计的补偿策略来减少通信成本，实时补偿局部压缩误差，优于现有工作。 |
| [^88] | [An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering](https://arxiv.org/abs/2402.11839) | 本文提出了一个增强的基于教学学习优化和灰狼优化器的混合方法，用于文本特征选择和聚类，旨在解决特征选择中的局部最优陷阱问题。 |
| [^89] | [UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction](https://arxiv.org/abs/2402.11838) | UniST是一种为城市时空预测设计的通用模型，通过灵活性、有效的生成式预训练以及丰富的掩码策略成功捕捉复杂的时空关系。 |
| [^90] | [Self-Guided Robust Graph Structure Refinement](https://arxiv.org/abs/2402.11837) | 本文提出了一个自主引导的GSR框架（SG-GSR），通过利用被攻击图中发现的干净子图，并提出了图增强和分组训练策略，以应对现有GSR方法在真实场景中受限的问题。 |
| [^91] | [Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization](https://arxiv.org/abs/2402.11835) | ABCs算法结合了Boltzmann Q-learning和反事实遗憾最小化，通过测量环境稳定性自适应选择探索比例，在单一智能体和多智能体领域表现出色。 |
| [^92] | [Microstructures and Accuracy of Graph Recall by Large Language Models](https://arxiv.org/abs/2402.11821) | 本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。 |
| [^93] | [Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before](https://arxiv.org/abs/2402.11816) | 开发了一种多阶对比学习（MCL）框架，以解决对比学习中的特征抑制问题，并确保模型学习全面的表示。 |
| [^94] | [HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?](https://arxiv.org/abs/2402.11815) | 提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能 |
| [^95] | [Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding](https://arxiv.org/abs/2402.11809) | 提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。 |
| [^96] | [Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling](https://arxiv.org/abs/2402.11800) | 延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。 |
| [^97] | [Generative Kaleidoscopic Networks](https://arxiv.org/abs/2402.11793) | 发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。 |
| [^98] | [Statistical Test for Generated Hypotheses by Diffusion Models](https://arxiv.org/abs/2402.11789) | 本研究提出了一种统计检验方法，通过选择性推断框架，在考虑生成图像是由训练的扩散模型产生的条件下，量化医学图像诊断结果的可靠性。 |
| [^99] | [What Evidence Do Language Models Find Convincing?](https://arxiv.org/abs/2402.11782) | 通过构建 ConflictingQA 数据集，并进行敏感性和反事实分析，研究发现当前语言模型在预测时很大程度上依赖于网站与查询的相关性，而忽视了人类认为重要的文本风格特征。 |
| [^100] | [Towards Theoretical Understandings of Self-Consuming Generative Models](https://arxiv.org/abs/2402.11778) | 通过构建理论框架，我们探讨了在自消耗循环中训练生成模型对数据分布学习的影响，证明了在足够大的训练数据集大小或真实数据比例条件下，合成数据分布与原始真实数据分布之间的总变差距离能够被有效控制。 |
| [^101] | [Uncovering Latent Human Wellbeing in Language Model Embeddings](https://arxiv.org/abs/2402.11777) | 本研究通过ETHICS Utilitarianism任务发现，预训练语言模型的表示隐含了对人类福祉的理解，且模型规模增加时，准确率呈非下降趋势。 |
| [^102] | [FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model](https://arxiv.org/abs/2402.11775) | 本研究通过使用自动角度超分辨率技术，将来自更快速采集的数据的角度估计提升到与需要较长时间获取的高分辨率数据相当，从而在纤维定向分布的估计中取得了显著进展。 |
| [^103] | [Dynamic Multi-Network Mining of Tensor Time Series](https://arxiv.org/abs/2402.11773) | 提出了一种新方法，Dynamic Multi-network Mining (DMM)，能够将张量时间序列转换为不同长度的段组，通过稀疏依赖网络提供聚类的可解释性和精确性。 |
| [^104] | [Evaluating the Effectiveness of Index-Based Treatment Allocation](https://arxiv.org/abs/2402.11771) | 本文介绍了一种评估基于指数的资源分配策略有效性的方法，通过翻译和扩展统计文献中的最新思想，提供了有效的估计器和计算渐近正确置信区间的方法。 |
| [^105] | [Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation](https://arxiv.org/abs/2402.11760) | 引入强化学习作为预测级联的简洁替代方案，减少计算成本。 |
| [^106] | [MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs](https://arxiv.org/abs/2402.11756) | MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。 |
| [^107] | [SPML: A DSL for Defending Language Models Against Prompt Attacks](https://arxiv.org/abs/2402.11755) | SPML是一种用于优化提示并监控基于大型语言模型聊天机器人输入的领域特定语言，用于防御恶意攻击并优化成本。 |
| [^108] | [Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing](https://arxiv.org/abs/2402.11752) | 引入了Diagonalisation Stochastic Gradient Descent（对角化SGD），通过重新参数化和平滑实现非可微模型的快速收敛SGD，在实证评估中表现出简单、快速、稳定，并且取得了数量级的工作规范化方差降低。 |
| [^109] | [Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation](https://arxiv.org/abs/2402.11747) | 本文研究了用于语音情绪识别的参数高效微调方法，提出了各种PEFT适配器并展示了其在分类离散情绪类别和预测情绪属性方面的有效性，同时通过减少可训练参数数量超越了完整微调。还提出了两阶段适应策略以提高模型对自然情感表达的捕捉能力。 |
| [^110] | [Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance](https://arxiv.org/abs/2402.11742) | 该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。 |
| [^111] | [Extraction of nonlinearity in neural networks and model compression with Koopman operator](https://arxiv.org/abs/2402.11740) | 本论文使用Koopman算子提取神经网络中的非线性，证明了受限非线性已足够进行手写数字分类，并提出了一种模型压缩方法，能在资源受限环境中处理大型网络。 |
| [^112] | [A Transition System Abstraction Framework for Neural Network Dynamical System Models](https://arxiv.org/abs/2402.11739) | 提出了一个过渡系统抽象框架，用于增强神经网络动力系统模型的可解释性，并通过人类手写动力学学习和验证应用进行验证。 |
| [^113] | [Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation](https://arxiv.org/abs/2402.11737) | 提出了一种基于模型等效评估的前馈神经网络压缩修复方法，通过计算两个神经网络之间的输出差异，初始化新的训练集并进行重新训练来改进压缩网络性能 |
| [^114] | [Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding](https://arxiv.org/abs/2402.11736) | 该论文研究了一种联合概率分布，其支持趋于最小化最坏情况误差，证明了它在最坏情况积分误差集中不等式上优于i.i.d.蒙特卡罗。 |
| [^115] | [The Effectiveness of Random Forgetting for Robust Generalization](https://arxiv.org/abs/2402.11733) | FOMO引入了一种新的学习范式，通过随机遗忘部分权重来调节信息并强调学习可泛化的特征，从而显著减少神经网络在对抗攻击下出现的稳健过拟合问题。 |
| [^116] | [Prospector Heads: Generalized Feature Attribution for Large Models & Data](https://arxiv.org/abs/2402.11729) | Prospector heads是一种高效且可解释的基于特征归因的替代方法，它可以应用于任何编码器和任何数据形态，并且通过对不同数据形态的实验，表现优越于传统方法。 |
| [^117] | [Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis](https://arxiv.org/abs/2402.11728) | 本研究探讨了分析师报告和盈利电话中的索赔对金融市场回报的影响，并构建了一个新的金融数据集用于索赔检测任务。提出了一种融入主题专家知识的新型弱监督模型，通过构建一种新的度量“乐观主义”展示了模型的实际效用。 |
| [^118] | [Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems](https://arxiv.org/abs/2402.11722) | 本文提出了可逆傅立叶神经算子(iFNO)，通过设计可逆傅立叶块和集成变分自动编码器，实现同时处理前向与反向问题的能力，为双向任务的学习提供有效的参数共享和信息交换，克服了不适定性、数据短缺和噪声等挑战。 |
| [^119] | [Learning Memory Kernels in Generalized Langevin Equations](https://arxiv.org/abs/2402.11705) | 提出一种学习广义朗之万方程中记忆核的新方法，通过正则化Prony方法估计相关函数并在Sobolev范数Loss函数和RKHS正则化下实现回归，在指数加权的$L^2$空间内获得改进性能，对比其他回归估计器展示了其优越性。 |
| [^120] | [Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation](https://arxiv.org/abs/2402.11702) | 大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。 |
| [^121] | [Explaining the Machine Learning Solution of the Ising Model](https://arxiv.org/abs/2402.11701) | 展示了如何通过神经网络和模型哈密顿量的对称性，解释铁磁伊辛模型的机器学习解决方案策略。 |
| [^122] | [Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks](https://arxiv.org/abs/2402.11687) | 本研究评估了量子神经网络模型窃取攻击的有效性，并提出了硬件变化诱发扰动和硬件与架构变化诱发扰动作为防御方法。 |
| [^123] | [Learning the Topology and Behavior of Discrete Dynamical Systems](https://arxiv.org/abs/2402.11686) | 本文研究了学习黑盒离散动力系统的行为和底层拓扑结构的问题，证明了这是一个计算上难以解决的问题，并提出了在某些条件下的高效学习方法。 |
| [^124] | [Learning Conditional Invariances through Non-Commutativity](https://arxiv.org/abs/2402.11682) | 学习非交换性条件不变性是一种更严格的优化方法，可以通过将优化导向目标特定编码器来实现对目标风险的更严格限制。 |
| [^125] | [A Fast Algorithm to Simulate Nonlinear Resistive Networks](https://arxiv.org/abs/2402.11674) | 提出一种新颖的方法用于模拟非线性电阻网络，通过将问题转化为具有线性不等式约束的二次规划问题，并利用快速、精确的坐标下降算法进行求解。 |
| [^126] | [Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry](https://arxiv.org/abs/2402.11670) | 对于在农业和林业中CNN应用的归因图进行全面评估，发现这些图表往往未能准确突出关键特征，与领域专家认为重要的特征不一致，这引发了对于其在理解神经网络决策过程中实用性的重大问题。 |
| [^127] | [Interpretable Short-Term Load Forecasting via Multi-Scale Temporal Decomposition](https://arxiv.org/abs/2402.11664) | 本文提出了一种可解释的深度学习方法，通过学习每个神经网络关注一个输入时间特征的线性组合，以及提出了一种多尺度时间序列分解方法来处理复杂的时间模式，在电力负荷预测中取得了更好的准确性。 |
| [^128] | [Dynamic planning in hierarchical active inference](https://arxiv.org/abs/2402.11658) | 通过研究在动态规划领域中模拟工具使用的目标，我们深入探讨了主动推断中的动态规划，该领域考虑到生物目标导向行为的两个关键方面 |
| [^129] | [Integrating Pre-Trained Language Model with Physical Layer Communications](https://arxiv.org/abs/2402.11656) | 提出了一个集成了物理层通信功能的实用设备间人工智能通信框架，通过端到端训练结合信道噪声以增强韧性，采用VQ-VAE实现高效稳健的通信，利用预训练Transformer提升通用性能 |
| [^130] | [Model-Free $\mu$-Synthesis: A Nonsmooth Optimization Perspective](https://arxiv.org/abs/2402.11654) | 本文基于一个策略优化视角，将基于子梯度的搜索方法扩展到无模型设置，并研究了两种无模型策略优化策略的有效性。 |
| [^131] | [Doubly Robust Inference in Causal Latent Factor Models](https://arxiv.org/abs/2402.11652) | 提出了一种双重稳健的估计量框架，可以在现代数据丰富的环境中估计存在未观察混杂因素下平均处理效应，具有良好的有限样本和渐近性质，并在参数速率下将其误差收敛为零均值高斯分布。 |
| [^132] | [Theoretical foundations for programmatic reinforcement learning](https://arxiv.org/abs/2402.11650) | 本文旨在探讨程序化强化学习的理论基础，给出了对程序化强化学习中关键问题的初步回答 |
| [^133] | [Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models](https://arxiv.org/abs/2402.11641) | 本文提出了一种利用大规模语言模型设计多功能图学习方法的新概念原型，重点关注“在哪里”和“如何”的角度。 |
| [^134] | [In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness](https://arxiv.org/abs/2402.11639) | 本文研究了在上下文学习框架中，Softmax注意力在适应预训练任务背景时的作用，发现注意力单元学会与Lipschitzness降低和标签噪声增加相关的窗口调整，以及在低维、线性问题上学会在推理前进行适当空间的投影。 |
| [^135] | [Poisoning Federated Recommender Systems with Fake Users](https://arxiv.org/abs/2402.11637) | 本研究介绍了一种无需额外信息的新型基于假用户的毒化攻击方法，用于在联邦推荐系统中执行推广攻击。 |
| [^136] | [Discrete Neural Algorithmic Reasoning](https://arxiv.org/abs/2402.11628) | 这项工作提出了一种强制神经推理器维护执行轨迹作为有限预定义状态组合的方法，通过对算法状态转换的监督训练，使模型能够与原始算法完美对齐，并在基准测试中取得了完美的测试成绩。 |
| [^137] | [Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2402.11622) | 提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。 |
| [^138] | [Self-evolving Autoencoder Embedded Q-Network](https://arxiv.org/abs/2402.11604) | 提出了一种将自进化自动编码器嵌入 Q 网络以增强强化学习代理探索能力的新方法 |
| [^139] | [Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI](https://arxiv.org/abs/2402.11594) | `spotRiverGUI`是一个为在线机器学习模型进行超参数调优的图形用户界面，简化了用户手动搜索最佳超参数设置的过程。 |
| [^140] | [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](https://arxiv.org/abs/2402.11592) | 本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。 |
| [^141] | [PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM](https://arxiv.org/abs/2402.11585) | PolypNextLSTM是一个轻量且快速的息肉视频分割网络，使用ConvNext和ConvLSTM，最大的创新在于参数最少且速度最快，性能超越了五种先进的基于图像和视频的深度学习模型。 |
| [^142] | [Continual Learning on Graphs: Challenges, Solutions, and Opportunities](https://arxiv.org/abs/2402.11565) | 对图上持续学习进行了全面评估和分类，弥补了欧几里得数据上持续学习研究的不足。 |
| [^143] | [Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation](https://arxiv.org/abs/2402.11558) | 基于深度学习的时间解耦对比扩散模型应用于时空插补，旨在通过生成模型来提高预测效能。 |
| [^144] | [Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling](https://arxiv.org/abs/2402.11552) | 本文提出了使用样条拟插值进行单变量密度估计，并将其应用于聚类建模，为构建适用的多元分布提供了新方法。 |
| [^145] | [PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction](https://arxiv.org/abs/2402.11538) | 提出了PASCL算法，在高能物理学中使用带扰动增强的监督对比学习，利用图结构重建粒子衰变层次树。 |
| [^146] | [Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution](https://arxiv.org/abs/2402.11525) | 提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。 |
| [^147] | [Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network](https://arxiv.org/abs/2402.11518) | 提出了一种利用大型语言模型驱动的元结构搜索框架，解决了手工设计元结构不易扩展以及忽视可解释性的问题 |
| [^148] | [Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics](https://arxiv.org/abs/2402.11515) | 该研究专注于优化深度强化学习在流体力学中主动流控制中的并行设置，通过拆解DRL框架、进行扩展性基准测试、提出混合并行化配置并优化多环境DRL训练中的I/O操作，提出了有效的并行化策略。 |
| [^149] | [URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification](https://arxiv.org/abs/2402.11495) | URLBERT是第一个专门针对URL分类或检测任务的预训练模型，引入了自监督对比学习和虚拟对抗训练两种新颖的预训练任务，以加强模型对URL结构的理解和提高从URL中提取语义特征的鲁棒性。 |
| [^150] | [Graph Out-of-Distribution Generalization via Causal Intervention](https://arxiv.org/abs/2402.11494) | GNN在离群分布泛化中的失败关键在于来自环境的潜在混杂偏差，因此引入了一个简单而原则性的方法来训练稳健GNN。 |
| [^151] | [LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation](https://arxiv.org/abs/2402.11485) | LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。 |
| [^152] | [DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning](https://arxiv.org/abs/2402.11472) | 基于图提示学习的DDIPrompt框架旨在解决药物相互作用事件预测中的高度不平衡事件分布和罕见事件标记数据稀缺性问题。 |
| [^153] | [A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models](https://arxiv.org/abs/2402.11469) | 本文研究了训练数据与模型鲁棒性之间的相关性，并提出通过提取不同特征来预测Transformer文本模型的对抗性稳健性的方法。 |
| [^154] | [Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective](https://arxiv.org/abs/2402.11463) | Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。 |
| [^155] | [Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge](https://arxiv.org/abs/2402.11459) | 提出了一种新颖的扩散桥生成模型 Re-Dock，用于灵活和现实的分子对接，通过能量到几何映射来共同建模结合能和构象，填补了对接中的实用性和构象预测方面的差距 |
| [^156] | [InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration](https://arxiv.org/abs/2402.11441) | 提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。 |
| [^157] | [Improved Indoor Localization with Machine Learning Techniques for IoT applications](https://arxiv.org/abs/2402.11433) | 该研究利用机器学习算法改善基于RSSI的室内定位，引入加权最小二乘技术和伪线性解决方案方法以解决非线性RSSI测量方程的问题。 |
| [^158] | [OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations](https://arxiv.org/abs/2402.11427) | OptEx是第一个通过利用并行计算来减轻一阶优化的迭代瓶颈并增强效率的框架，使用核化梯度估计实现迭代的并行化，提供理论保证。 |
| [^159] | [Online Local False Discovery Rate Control: A Resource Allocation Approach](https://arxiv.org/abs/2402.11425) | 该研究提出了一种在线局部虚发现率控制的资源分配方法，实现了$O(\sqrt{T})$的后悔率，并指出这种后悔率在一般情况下是不可改进的。 |
| [^160] | [LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models](https://arxiv.org/abs/2402.11417) | LoRETTA是一个通过张量训练分解显著减少可训练参数的超低参数高效框架，在大型语言模型的微调中表现出与大多数PEFT方法相媲美甚至更好的性能。 |
| [^161] | [A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)](https://arxiv.org/abs/2402.11413) | 介绍了一种名为多光谱自动转移技术（MATT）的方法，通过从RGB图像转置SAM分割掩模，实现了对多光谱图像的高精度和高效自动分割标记。 |
| [^162] | [Aligning Modalities in Vision Large Language Models via Preference Fine-tuning](https://arxiv.org/abs/2402.11411) | 本研究将幻觉问题视为对齐问题，并通过偏好调整解决，提出了POVID方法来生成反馈数据。 |
| [^163] | [An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration](https://arxiv.org/abs/2402.11410) | 给出了一种简单、高效、确定性的算法，该算法的校准距离误差最多为$2\sqrt{T}$ |
| [^164] | [Evaluating the Stability of Deep Learning Latent Feature Spaces](https://arxiv.org/abs/2402.11404) | 评估深度学习潜在特征空间稳定性的新方法，引入了可以确保一致性和可靠性的稳定性评估工作流程，包括了三种稳定性类型和一套全面评估的度量标准。 |
| [^165] | [GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation](https://arxiv.org/abs/2402.11401) | 提出了一种基于图的知识蒸馏框架，用于在文档图像中识别和定位文档对象，以减少大型模型在资源受限设备上的部署成本 |
| [^166] | [k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text](https://arxiv.org/abs/2402.11399) | k-SemStamp是一种简单而有效的语义水印方案，通过利用k均值聚类代替LSH来提高鲁棒性和抽样效率，同时保持生成质量，为机器生成文本检测提供了更有效的工具。 |
| [^167] | [Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications](https://arxiv.org/abs/2402.11397) | RPNNs具有固定内部权重和偏置，通过选择外部权重，它们展现出指数收敛速率来逼近任意无穷可微函数，在函数逼近问题上表现出高效准确的潜力 |
| [^168] | [Reinforcement learning to maximise wind turbine energy generation](https://arxiv.org/abs/2402.11384) | 提出了一种利用强化学习控制风力发电以最大化能量产生的方法，表现出在各种环境下比传统PID控制更优异，并在真实风条件下进行了对比验证。 |
| [^169] | [Multi Task Inverse Reinforcement Learning for Common Sense Reward](https://arxiv.org/abs/2402.11367) | 将奖励分解为任务特定奖励和常识奖励，探索如何从专家演示中学习常识奖励；研究发现，逆强化学习成功训练代理后，并不会学到有用的奖励函数。 |
| [^170] | [Data-Driven Stochastic AC-OPF using Gaussian Processes](https://arxiv.org/abs/2402.11365) | 该论文提出了一种基于高斯过程的数据驱动算法，用于解决随机交流（AC）概率约束（CC）最优潮流（OPF）问题，并通过多个IEEE测试案例展示了其实证效率。 |
| [^171] | [Exploiting T-norms for Deep Learning in Autonomous Driving](https://arxiv.org/abs/2402.11362) | 本文提出了如何定义内存高效的T-范数损失，允许在自动驾驶中利用T-范数进行事件检测任务，并在实验中展示了在GPU上运行的可行性。 |
| [^172] | [What Changed? Converting Representational Interventions to Natural Language](https://arxiv.org/abs/2402.11355) | 将表征空间的反事实转化为自然语言，以分析和解释模型干预所引起的语言变化，并减轻分类中的偏见。 |
| [^173] | [Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search](https://arxiv.org/abs/2402.11354) | 该论文提出了一种基于概率路由的方法，通过引入PEOs有效识别图中需要考虑进行精确距离计算的邻居，从而显著提高了基于图的近似最近邻搜索的效率。 |
| [^174] | [Variational Entropy Search for Adjusting Expected Improvement](https://arxiv.org/abs/2402.11345) | 本论文通过变分推断的方法，将期望改进（EI）视为最大值熵搜索（MES）的特殊情况，提出了变分熵搜索（VES）方法和 VES-Gamma 算法，成功调整 EI 并展示其在贝叶斯优化方面的实用性。 |
| [^175] | [Ransomware detection using stacked autoencoder for feature selection](https://arxiv.org/abs/2402.11342) | 提出并评估了一种使用堆叠自编码器进行特征选择的先进勒索软件检测和分类方法，通过结合堆叠自编码器和长短期记忆分类器，实现了提高勒索软件分层准确性的目标。 |
| [^176] | [Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking](https://arxiv.org/abs/2402.11339) | 通过引入预处理算法识别展现对称性的正则子超图，从而提高超图在高阶链接预测中的表达能力和区分能力。 |
| [^177] | [Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach](https://arxiv.org/abs/2402.11338) | 该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。 |
| [^178] | [SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems](https://arxiv.org/abs/2402.11322) | SpikeNAS提出了一种快速内存感知神经架构搜索框架，旨在帮助脉冲神经网络系统快速找到在给定内存预算下高准确性的适当架构。 |
| [^179] | [BiasBuster: a Neural Approach for Accurate Estimation of Population Statistics using Biased Location Data](https://arxiv.org/abs/2402.11318) | 该论文提出了一种应对使用偏倚位置数据计算人口统计数据造成的不准确性的方法，以解决这些数据集在敏感决策中可能产生的严重后果。 |
| [^180] | [Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics](https://arxiv.org/abs/2402.11317) | 提出了一种名为DORA的新方法，通过信息瓶颈原理在离线设置中学习适应性策略，解决了动态编码与环境数据之间的互信息与与行为策略的互信息之间的难题 |
| [^181] | [Fair Resource Allocation in Virtualized O-RAN Platforms](https://arxiv.org/abs/2402.11285) | 该论文通过实验评估了O-Cloud的能耗及其与服务器硬件、容量和数据流量特性的关系，提出了一种计算策略和无线策略，平衡能源节约和性能，确保它们在服务器和用户之间公平分配。 |
| [^182] | [TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method](https://arxiv.org/abs/2402.11274) | 提出了一种基于扩散模型的MRI重建方法TC-DiffRecon，旨在解决扩散模型导致的图像碎裂和不一致性以及生成图像过度平滑等问题。 |
| [^183] | [Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima](https://arxiv.org/abs/2402.11262) | 本文从平缓局部最小值的角度分析多模态推荐系统，并提出了一种名为镜像梯度（MG）的梯度策略，可以增强模型的稳健性，缓解来自多模态信息输入的不稳定风险。 |
| [^184] | [Aligning Large Language Models by On-Policy Self-Judgment](https://arxiv.org/abs/2402.11253) | 本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。 |
| [^185] | [Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection](https://arxiv.org/abs/2402.11242) | 提出了一种用于处理不平衡数据中嘈杂标签的方法，通过Class-Balance-based Sample Selection (CBS)防止忽视尾部类别样本，并通过Confidence-based Sample Augmentation (CSA)增强干净样本的可靠性。 |
| [^186] | [Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning](https://arxiv.org/abs/2402.11237) | 本文旨在通过利用拓扑数据分析提出一个统一的解决方案，检测深度学习中的快捷学习问题。 |
| [^187] | [ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs](https://arxiv.org/abs/2402.11235) | ZeroG是一个新框架，旨在实现图中跨数据集零射击迁移，解决了特征不对齐、标签空间不匹配和负迁移等挑战 |
| [^188] | [Adaptive Split Balancing for Optimal Random Forest](https://arxiv.org/abs/2402.11228) | 介绍了自适应分割平衡森林（ASBF），可在学习树表示的同时，在复杂情况下实现极小极优性，并提出了一个本地化版本，在H\"older类下达到最小极优性。 |
| [^189] | [On the Role of Similarity in Detecting Masquerading Files](https://arxiv.org/abs/2402.11227) | 坏演员可以通过使用伪装样本绕过机器学习解决方案，研究探讨了数字签名和机器学习之间的相互作用，并提出了通过相似性和聚类来改进安全解决方案的方法。 |
| [^190] | [Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement](https://arxiv.org/abs/2402.11224) | 本文揭示了神经网络的多项式逼近作为一种独立对象的研究，发现PANN对某些类型的逼近误差... |
| [^191] | [HEAL: Brain-inspired Hyperdimensional Efficient Active Learning](https://arxiv.org/abs/2402.11223) | HEAL是一种专为HDC分类量身定制的新型主动学习框架，通过不确定性和多样性引导的获取主动为未标记的数据点进行注释，实现更高效的数据集注释和降低劳动成本。 |
| [^192] | [AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods](https://arxiv.org/abs/2402.11215) | AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。 |
| [^193] | [Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges](https://arxiv.org/abs/2402.11203) | ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。 |
| [^194] | [Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients](https://arxiv.org/abs/2402.11198) | 提出了一种异步联邦学习(AFL)框架，旨在解决具有异构客户端的联邦学习中同步问题，实现线性加速。 |
| [^195] | [Maintaining Adversarial Robustness in Continuous Learning](https://arxiv.org/abs/2402.11196) | 提出了一种名为双梯度投影的方法，通过将梯度投影到两个关键子空间来实现持续鲁棒学习，有效地维持了神经网络对抗性鲁棒性。 |
| [^196] | [Minimally Supervised Topological Projections of Self-Organizing Maps for Phase of Flight Identification](https://arxiv.org/abs/2402.11185) | 本研究提出了一种最小监督自组织映射方法，利用最近邻多数投票来进行类别估计，在飞行阶段识别方面表现出很好的效果，并且对类别不平衡更加稳健 |
| [^197] | [Uncertainty Quantification of Graph Convolution Neural Network Models of Evolving Processes](https://arxiv.org/abs/2402.11179) | 本研究比较了使用哈密顿蒙特卡洛和斯坦变分梯度下降等方法，对建模复杂时空过程的神经网络进行参数不确定性量化，特别是应用于图卷积神经网络模型的演化过程，展示了... |
| [^198] | [How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization](https://arxiv.org/abs/2402.11173) | 提出了一种设计具有差分隐私算法的简单灵活框架，用于寻找非凸损失函数的近似稳定点，并获得了改进和有时是最优的速率。 |
| [^199] | [Trust Regions for Explanations via Black-Box Probabilistic Certification](https://arxiv.org/abs/2402.11168) | 通过黑盒概率认证解释的信任区域能够有效地洞察模型行为、保证解释的稳定性，并实现解释的重用 |
| [^200] | [Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits](https://arxiv.org/abs/2402.11156) | 提出一种新型低秩矩阵估计方法LowPopArt，通过最小化量B(Q)提供更紧密的恢复保证，同时提出了一种新颖的实验设计标准，以及两种适用于一般Arm集的低秩线性赌博算法。 |
| [^201] | [Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on Graphs](https://arxiv.org/abs/2402.11153) | 该调查综述了图像上的分布适应问题，提供最新的图OOD适应方法的回顾，覆盖了训练时和测试时的两种主要问题场景，并提出了分类法。 |
| [^202] | [Knowledge Distillation Based on Transformed Teacher Matching](https://arxiv.org/abs/2402.11148) | 通过放弃学生端的温度缩放，本文提出了一种名为转换教师匹配（TTM）的知识蒸馏变体，通过对温度缩放的重新解释，TTM在目标函数中引入了固有的Rényi熵项，从而实现了更好的学生泛化效果。 |
| [^203] | [Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos](https://arxiv.org/abs/2402.11145) | Providence是一个基于视觉编程的多模式机器学习工具，旨在帮助专家捕获人类行为线索，无需编写代码，具有可取的可用性和令人满意的输出。 |
| [^204] | [LiGNN: Graph Neural Networks at LinkedIn](https://arxiv.org/abs/2402.11139) | 本文介绍了在领英上开发和部署的LiGNN框架，包括对GNN表示学习的算法改进和大规模训练优化，为工作申请回复率、广告点击率和Feed每日活跃用户提高带来了约1%-2%的相对改善。 |
| [^205] | [Contrastive Instruction Tuning](https://arxiv.org/abs/2402.11138) | 提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性 |
| [^206] | [TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks](https://arxiv.org/abs/2402.11137) | 提出了TuneTables上下文优化技术，通过开发一种新的提示调整策略，将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。 |
| [^207] | [Speculative Streaming: Fast LLM Inference without Auxiliary Models](https://arxiv.org/abs/2402.11131) | 提出了一种Speculative Streaming方法，将草稿模型融入目标模型，并通过将微调目标从下一个令牌预测更改为未来的n-gram预测，加速解码1.8-3.1倍，同时保持生成质量。 |
| [^208] | [Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning (PIML) Methods: Towards Robust Metrics](https://arxiv.org/abs/2402.11126) | 本研究使用Kolmogorov n-宽度作为评估多任务PIML架构的有效性的方法，分析模型在各种PDE问题上学到的基函数。 |
| [^209] | [Disentanglement in Implicit Causal Models via Switch Variable](https://arxiv.org/abs/2402.11124) | 该论文通过软干预处理隐式潜在因果表征学习，在 Variational Autoencoder (VAE) 框架中引入了因果机制开关变量。 |
| [^210] | [Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy Learning and Evaluation Method](https://arxiv.org/abs/2402.11123) | 本研究提出了一种使用情境臂和离线策略学习的方法，通过观察数据建立个性化的华法林剂量策略，即使在缺乏基因型信息的情况下也可以超越基线方法。 |
| [^211] | [DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation](https://arxiv.org/abs/2402.11120) | 本文探讨了无监督领域自适应中对抗鲁棒性的问题，通过建立对抗目标损失的泛化界限来解决目标域标签缺失带来的挑战。 |
| [^212] | [Private PAC Learning May be Harder than Online Learning](https://arxiv.org/abs/2402.11119) | 私有PAC学习和在线学习之间的转换并不总是可以保持计算效率，在一些情况下是困难的。 |
| [^213] | [Dynamic nowcast of the New Zealand greenhouse gas inventory](https://arxiv.org/abs/2402.11107) | 通过机器学习方法，提出了一种动态预测新西兰国家温室气体排放的新方法，使得预测能提前至国家排放清单发布之前，并展现了较低误差的次年估计能力。 |
| [^214] | [Toward Learning Latent-Variable Representations of Microstructures by Optimizing in Spatial Statistics Space](https://arxiv.org/abs/2402.11103) | 通过在空间统计空间中最小化原始和重建图像之间的距离，实现学习微观结构的低维表示 |
| [^215] | [Physics-based material parameters extraction from perovskite experiments via Bayesian optimization](https://arxiv.org/abs/2402.11101) | 使用贝叶斯优化开发了一个分析平台，可以从钙钛矿实验中提取多个基本材料参数，加速材料发现和半导体优化 |
| [^216] | [Modular Graph Extraction for Handwritten Circuit Diagram Images](https://arxiv.org/abs/2402.11093) | 本文描述了一种模块化的端到端手写电路图像的图形提取方法，以解决从栅格图像中提取电气图的问题。 |
| [^217] | [Model Editing by Pure Fine-Tuning](https://arxiv.org/abs/2402.11078) | 纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。 |
| [^218] | [Towards Financially Inclusive Credit Products Through Financial Time Series Clustering](https://arxiv.org/abs/2402.11066) | 通过金融时间序列聚类，解决金融包容性信贷产品中客户分割的挑战 |
| [^219] | [Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains](https://arxiv.org/abs/2402.11039) | 提出了一种名为RAD的方法，通过领域标注的正则化来训练鲁棒的最后一层分类器，无需显式的领域标注，在具有领域标签噪声的情况下表现优越。 |
| [^220] | [Occlusion Resilient 3D Human Pose Estimation](https://arxiv.org/abs/2402.11036) | 通过将身体建模为时空图并引入一个精炼网络，来实现对遮挡具有鲁棒性的3D人体姿势估计。 |
| [^221] | [Training Bayesian Neural Networks with Sparse Subspace Variational Inference](https://arxiv.org/abs/2402.11025) | 提出了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架 |
| [^222] | [Automated Detection and Analysis of Data Practices Using A Real-World Corpus](https://arxiv.org/abs/2402.11006) | 本文提出了一种使用真实世界语料库自动检测和分析隐私政策中数据实践的方法，并通过实验和案例研究证明了其有效性。 |
| [^223] | [The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains](https://arxiv.org/abs/2402.11004) | 研究了大型语言模型如何通过训练在上下文学习中准确预测下一个标记概率，并发现了一个多阶段的过程。 |
| [^224] | [Analysis and Mortality Prediction using Multiclass Classification for Older Adults with Type 2 Diabetes](https://arxiv.org/abs/2402.10999) | 设计治疗方案需注意患者剩余生命和合并症，研究利用大规模数据集构建多类分类模型来预测老年2型糖尿病患者的死亡率。 |
| [^225] | [Provably Safe Neural Network Controllers via Differential Dynamic Logic](https://arxiv.org/abs/2402.10998) | 通过差分动态逻辑与神经网络验证相结合的VerSAILLE方法，实现了对神经网络控制系统在无限时间范围上的安全性证明。 |
| [^226] | [Accelerating Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2402.10991) | 提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。 |
| [^227] | [Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of Conjugate Variables in System Attacks](https://arxiv.org/abs/2402.10983) | 神经网络漏洞的量子启发式分析揭示了输入共轭在系统攻击中的作用，揭示了网络结构中的系统脆弱性，并展示了与量子物理不确定性原理之间的数学一致性，突显了神经网络系统的固有脆弱性和潜在的跨学科领域进展。 |
| [^228] | [mshw, a forecasting library to predict short-term electricity demand based on multiple seasonal Holt-Winters](https://arxiv.org/abs/2402.10982) | 这是一个用于预测短期电力需求的MATLAB工具箱，可以提供更准确的预测。 |
| [^229] | [Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning](https://arxiv.org/abs/2402.10981) | 本文研究了ReRAM神经形态电路中卡住故障对推理准确性的影响，并发现卡住和卡住缺陷对推理准确性有类似的影响，但如果在列之间存在空间缺陷变化，则推理准确性会受到影响。 |
| [^230] | [CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback](https://arxiv.org/abs/2402.10980) | 通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索 |
| [^231] | [Language Models with Conformal Factuality Guarantees](https://arxiv.org/abs/2402.10978) | 提出了一种能够通过连接语言建模和符合预测为语言模型提供高概率正确性保证的框架。 |
| [^232] | [Generative AI and Process Systems Engineering: The Next Frontier](https://arxiv.org/abs/2402.10977) | 新兴生成人工智能模型（如基础模型）在过程系统工程中的应用，提供了多功能的适应性，对合成与设计、优化与集成以及过程监控与控制等关键领域具有重要影响。 |
| [^233] | [On the Cross-Dataset Generalization of Machine Learning for Network Intrusion Detection](https://arxiv.org/abs/2402.10974) | 该研究通过在跨数据集框架中进行实验，对基于机器学习的网络入侵检测系统的泛化能力进行了综合分析，并提出了一个创新的数据集对其进行验证。 |
| [^234] | [Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases](https://arxiv.org/abs/2402.10972) | 本研究提出了针对偏头痛等慢性疾病的预测模型建设方法，探讨了最大预测时间跨度及其对选定特征的依赖性。 |
| [^235] | [Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model](https://arxiv.org/abs/2402.10965) | 大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。 |
| [^236] | [Optimal feature rescaling in machine learning based on neural networks](https://arxiv.org/abs/2402.10964) | 提出了一种通过遗传算法进行输入特征的最佳重缩放来改善神经网络训练效率和泛化性能的方法。 |
| [^237] | [GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements](https://arxiv.org/abs/2402.10963) | 提出了Stepwise ORMs (SORMs)，它们在合成数据上训练，以近似预测最优策略的未来预期奖励 |
| [^238] | [Measuring and Controlling Persona Drift in Language Model Dialogs](https://arxiv.org/abs/2402.10962) | 提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移 |
| [^239] | [Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts](https://arxiv.org/abs/2402.10958) | 提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。 |
| [^240] | [Sleep-Like Unsupervised Replay Improves Performance when Data are Limited or Unbalanced](https://arxiv.org/abs/2402.10956) | 研究发现，通过类似于睡眠的无监督重播阶段，可以显著提高在有限数据情况下训练的人工神经网络的准确性，这为解决数据有限或不平衡时的性能问题提供了新方法。 |
| [^241] | [DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting](https://arxiv.org/abs/2402.10951) | DAEDRA是一种旨在在被动报告中检测监管相关结果的大型语言模型，弥补了通用模型无法捕捉临床维度与专业模型在非专业报告上表现不佳的缺陷 |
| [^242] | [The Unreasonable Effectiveness of Eccentric Automatic Prompts](https://arxiv.org/abs/2402.10949) | 异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。 |
| [^243] | [CultureLLM: Incorporating Cultural Differences into Large Language Models](https://arxiv.org/abs/2402.10946) | 提出了一种名为CultureLLM的成本效益高的解决方案，通过使用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强来将文化差异纳入大型语言模型中，成功微调得到了涵盖富裕和低资源语言的9种文化特定LLMs以及一个统一模型（CultureLLM-One）。 |
| [^244] | [Text2Data: Low-Resource Data Generation with Textual Control](https://arxiv.org/abs/2402.10941) | Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。 |
| [^245] | [Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification](https://arxiv.org/abs/2402.10940) | 研究引入了医学熵的概念，通过神经机器翻译基于ICD-9代码的患者预测结果，量化了不确定性。 |
| [^246] | [A Lightweight Inception Boosted U-Net Neural Network for Routability Prediction](https://arxiv.org/abs/2402.10937) | 提出了一种轻量级Inception增强的U-Net神经网络模型，用于预测布线拥塞和设计规则检查热点，在实验中取得了显著的性能改进。 |
| [^247] | [ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters](https://arxiv.org/abs/2402.10930) | ConSmax是一种硬件友好型Softmax替代方案，通过引入可学习参数，在不影响性能的情况下实现了对原Softmax关键任务的高效处理。 |
| [^248] | [Numerical analysis of physics-informed neural networks and related models in physics-informed machine learning](https://arxiv.org/abs/2402.10926) | 这项研究全面回顾了物理信息神经网络及相关模型在物理信息机器学习中的数值分析结果，提供了对PINNs近似PDE过程中误差成分的统一框架分析。 |
| [^249] | [AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning](https://arxiv.org/abs/2402.10921) | 通过自适应缺失模态情绪识别, 该模型包括查询自适应融合和多模态联合嵌入学习两大特点，旨在提高情绪识别的准确性和鲁棒性。 |
| [^250] | [LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration](https://arxiv.org/abs/2402.10908) | 通过LLAMA2语言模型，建立了一个能够从社交媒体和紧急消息中识别和分类紧急情况的方法，可协助在全国范围内的紧急情况下公共安全话务员和大众，提供相关指导并通知政府机构。 |
| [^251] | [Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation](https://arxiv.org/abs/2402.10649) | 使用混合神经网络结合Hermite函数根作为配点，提高了解决二维薛定谔方程的效率和精度，与其他神经网络和方法相比取得了优秀的结果。 |
| [^252] | [Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment](https://arxiv.org/abs/2402.10207) | 本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。 |
| [^253] | [Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention](https://arxiv.org/abs/2402.10198) | 本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。 |
| [^254] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^255] | [Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data](https://arxiv.org/abs/2402.10100) | 本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。 |
| [^256] | [GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via Reinforcement Learning](https://arxiv.org/abs/2402.10074) | 本文提出了一种通过强化学习对图神经网络进行类平衡主动学习的框架GraphCBAL，该框架能够学习一种最佳策略，选择类平衡和信息丰富的节点进行注释，以最大化GNNs性能。 |
| [^257] | [LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild](https://arxiv.org/abs/2402.09997) | LoraRetriever提出了一种适应输入的LoRA检索与合成方法，用于弥合实际情况下大型语言模型接收到不同任务提示的差距。 |
| [^258] | [Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation](https://arxiv.org/abs/2402.09954) | 本研究通过对大型语言模型在基于角色生成对话方面进行实验，发现调整提示指令可以最直接有效且经济地提高生成质量，并且随机检索示范会取得最佳结果，而查询相同上下文的示范检索效果最差。即使破坏了示范中的多回合关联和单回合语义，对话生成仍然有效。 |
| [^259] | [Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection](https://arxiv.org/abs/2402.09820) | 这项研究提出使用深度学习来增强金融行业的网络安全韧性，并实现高级威胁检测。目前的网络威胁检测方法往往基于规则和传统的机器学习方法，无法适用大规模数据应用，并且无法有效检测未知威胁。 |
| [^260] | [On the Potential of Network-Based Features for Fraud Detection](https://arxiv.org/abs/2402.09495) | 本文研究了基于网络特征在欺诈检测中的潜力，通过使用个性化的PageRank算法来捕捉欺诈的社会动态。实验结果表明，集成PPR可以提高模型的预测能力并提供独特有价值的信息。 |
| [^261] | [Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?](https://arxiv.org/abs/2402.09303) | 研究对比了人类和深度神经网络在图像分类中的行为差异，发现人类具有即时概括能力，而DNNs存在滞后概括现象，这表明了表示分歧的存在。 |
| [^262] | [UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers](https://arxiv.org/abs/2402.09264) | UR2M是一个新颖的不确定性和资源感知的事件检测框架，针对微控制器上的应用，通过评估模型输出的可靠性来解决传统机器学习技术在数据分布变化时产生不准确预测的问题。 |
| [^263] | [IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture](https://arxiv.org/abs/2402.08923) | 本文提出了一种基于数据驱动的方法，用于使用Transformer架构进行人体姿态估计的最佳IMU放置。研究结果表明，该方法在姿态重建准确性方面优于传统的双向RNN模型，并且在使用只有6个IMU时，Transformer架构将24个IMU位置获取的数据与双向RNN具有相当的性能。这一优化选择的IMU放置策略结合了Transformer的并行性和性能优势，对基于IMU的姿态估计领域带来了显著的改进。 |
| [^264] | [Auto-Encoding Bayesian Inverse Games](https://arxiv.org/abs/2402.08902) | 本文研究了多个智能体在共同环境中的交互问题，并提出了一种贝叶斯方法来推断游戏参数。采用变分自动编码器（VAE）来构建游戏参数的后验分布，从而解决了现有方法中无法对不确定性进行定量化的问题。 |
| [^265] | [Graph Mamba: Towards Learning on Graphs with State Space Models](https://arxiv.org/abs/2402.08678) | 本文提出了一种基于选择性SSMs的新类GNNs框架——图马巴网络（GMNs），通过不依赖于Transformer、复杂的消息传递和位置/结构编码（SE/PE），解决了传统GNNs的过度压缩和无法捕捉长程依赖的问题。 |
| [^266] | [Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback](https://arxiv.org/abs/2402.08662) | 该论文通过将相位观察、简单的相位奖励和局部反馈动力学相结合，提出了一种使用分散相位振荡器学习动物步态的模型，有效地实现了新生步态偏好的策略。 |
| [^267] | [SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds](https://arxiv.org/abs/2402.08653) | SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。 |
| [^268] | [Knowledge Editing on Black-box Large Language Models](https://arxiv.org/abs/2402.08631) | 这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。 |
| [^269] | [ChatCell: Facilitating Single-Cell Analysis with Natural Language](https://arxiv.org/abs/2402.08303) | ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。 |
| [^270] | [LLaGA: Large Language and Graph Assistant](https://arxiv.org/abs/2402.08170) | LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。 |
| [^271] | [Contextual Multinomial Logit Bandits with General Value Functions](https://arxiv.org/abs/2402.08126) | 本文研究了具有一般价值函数的情境多项式逻辑回归赌博机，并提出了一套算法来处理线性情况，这些算法具有计算效率高、无维度的遗憾界限以及处理完全对抗性环境和奖励的能力。 |
| [^272] | [Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction](https://arxiv.org/abs/2402.07570) | 通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。 |
| [^273] | [Sampling from the Mean-Field Stationary Distribution](https://arxiv.org/abs/2402.07355) | 本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。 |
| [^274] | [The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey](https://arxiv.org/abs/2402.07249) | 本文通过系统调查，发现整合领域知识可以提高分子性质预测的准确性，同时利用多模态数据融合可以产生更精确的结果。 |
| [^275] | [Dynamic Graph Information Bottleneck](https://arxiv.org/abs/2402.06716) | 动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。 |
| [^276] | [Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty](https://arxiv.org/abs/2402.06529) | 本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。 |
| [^277] | [Multimodal Clinical Trial Outcome Prediction with Large Language Models](https://arxiv.org/abs/2402.06512) | 本研究提出了一种名为LIFTED的多模态临床试验结果预测方法，通过将不同模态数据转化为自然语言描述来统一数据，并构建统一的抗噪声编码器进行信息提取。 |
| [^278] | [Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams](https://arxiv.org/abs/2402.06122) | 本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。 |
| [^279] | [TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning](https://arxiv.org/abs/2402.05396) | 该论文提出了TASER方法，它是针对动态图表示学习的时间自适应采样技术，在准确性、效率和可扩展性方面进行了优化，解决了现实世界动态图中存在的噪声问题。 |
| [^280] | [E(3)-Equivariant Mesh Neural Networks](https://arxiv.org/abs/2402.04821) | E(3)-等变Mesh神经网络通过扩展E(n)-等变图神经网络的更新方程以包括网格面信息，并通过层次化进一步改进以考虑长程相互作用，实现了在网格任务上的优越性能，具有快速运行时间和无需昂贵预处理的特点。 |
| [^281] | [LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference](https://arxiv.org/abs/2402.04296) | 本论文介绍了一种光HGNN方法，将超图神经网络(HGNNs)转化为Multi-Layer Perceptron (MLPs)以提高推断速度。LightHGNN通过软标签将知识从teacher HGNN蒸馏到student MLPs，而LightHGNN$^+$则注入了可靠的高阶相关性。 |
| [^282] | [Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching](https://arxiv.org/abs/2402.04051) | 通过基于排列的权重匹配分析线性模式连接性，我们实验证明了通过权重匹配找到的排列可以改变权重矩阵奇异向量的方向，但不能改变奇异值。这一发现对于理解随机梯度下降的有效性及其在模型合并等领域的应用具有重要意义。 |
| [^283] | [A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation](https://arxiv.org/abs/2402.03358) | 这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。 |
| [^284] | [A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators](https://arxiv.org/abs/2402.03149) | 本文详细分析了基于微环的相干光学GEMM加速器的组织，其通过分裂、聚合、调制、加权和求和等方式操作光信号以加速深度神经网络中的矩阵-矩阵乘法，提高吞吐量和能量效率。 |
| [^285] | [Text-Guided Image Clustering](https://arxiv.org/abs/2402.02996) | 这篇论文提出了一种文本引导的图像聚类方法，使用图像字幕和视觉问答模型生成文本，然后对生成的文本进行聚类，并通过注入任务或领域知识来改进聚类结果。实验证明，获得的文本表示通常优于图像特征，而基于关键词的解释可以更好地描述聚类。 |
| [^286] | [TopoX: A Suite of Python Packages for Machine Learning on Topological Domains](https://arxiv.org/abs/2402.02441) | TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。 |
| [^287] | [Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction](https://arxiv.org/abs/2402.02171) | 我们提出了一种名为潜在IPS（LIPS）的新的Slate Bandit OPE估计器，通过在低维度的Slate抽象空间中定义重要性权重，并通过数据驱动的方式优化Slate抽象来减小偏差和方差。 |
| [^288] | [Quality and Trust in LLM-generated Code](https://arxiv.org/abs/2402.02047) | 本论文研究了机器学习生成代码的质量和信任问题，提出了校准的重要性，并探讨了如何确定模型生成代码的正确性。 |
| [^289] | [$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation](https://arxiv.org/abs/2402.02041) | 本文提出了一种应用于神经密度比估计的$\alpha$-散度损失函数($\alpha$-Div)，通过简洁实现和稳定优化解决了现有方法中存在的优化问题。实验证明了这种损失函数的稳定性，并提出了对DRE任务的估计准确性的研究，同时给出了样本要求的解决方案。 |
| [^290] | [Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey](https://arxiv.org/abs/2401.11963) | 通过整合进化算法与强化学习，进化强化学习（ERL）展现出卓越的性能提升，本综述呈现了ERL领域的各个研究分支，突出了EA辅助RL的优化、RL辅助EA的优化以及EA和RL的协同优化这三个主要研究方向。 |
| [^291] | [LightDiC: A Simple yet Effective Approach for Large-scale Digraph Representation Learning](https://arxiv.org/abs/2401.11772) | LightDiC是基于磁性拉普拉斯的可扩展有向图卷积方法，通过在离线预处理中进行拓扑相关计算，实现了可扩展性，适用于大规模数据库。 |
| [^292] | [PRewrite: Prompt Rewriting with Reinforcement Learning](https://arxiv.org/abs/2401.08189) | 本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。 |
| [^293] | [Combining Machine Learning and Ontology: A Systematic Literature Review](https://arxiv.org/abs/2401.07744) | 通过系统文献综述研究了将机器学习和本体整合的过程，发现了增强型本体学习、语义数据挖掘和学习与推理系统这三种主要的混合类别，比较了不同研究中使用的机器学习算法。 |
| [^294] | [Graph Language Models](https://arxiv.org/abs/2401.07105) | 引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。 |
| [^295] | [Hidden Minima in Two-Layer ReLU Networks](https://arxiv.org/abs/2312.16819) | 本文研究了两层ReLU网络中的隐藏极小值现象，并提出方法来研究这些隐藏极小值的独特解析性质。 |
| [^296] | [Best-of-Both-Worlds Algorithms for Linear Contextual Bandits](https://arxiv.org/abs/2312.15433) | 该论文研究了线性情境赌博问题的两全其美算法，实现了在对抗性和随机情况下接近最优的遗憾界，其中包括了针对最小次优差距的多对数级别速率和在对抗性情况下的第一阶或第二阶界以及基于Shannon熵正则项的FTRL算法。 |
| [^297] | [Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation](https://arxiv.org/abs/2312.15112) | 该研究提出了一种新颖的自适应方法，利用教师和学生的正确性以及学生对教师模仿程度来学习每个样本的知识融合比率，从而引入了样本内三边几何关系。 |
| [^298] | [Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks](https://arxiv.org/abs/2312.14922) | 神经网络在高维数据中发现统计模式，研究了如何高效地从高阶累积量中提取特征，并探讨了在尖峰累积量模型中的统计和计算限制。 |
| [^299] | [SIG: Speaker Identification in Literature via Prompt-Based Generation](https://arxiv.org/abs/2312.14590) | 通过基于生成的方法SIG，在文学作品中实现了说话者识别任务，支持跨领域评估和开放世界分类范式。 |
| [^300] | [Doubly Perturbed Task Free Continual Learning](https://arxiv.org/abs/2312.13027) | 提出了一种新颖的任务自由持续学习框架，在输入数据和决策制定上注入敌对扰动，提高了未来样本考虑的效果。 |
| [^301] | [PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping](https://arxiv.org/abs/2312.12065) | 该论文在PPO-Clip算法方面做出贡献，建立了其在表格和神经函数逼近设置中的全局收敛结果，特别突出了在神经函数逼近情境下的$O(1/\sqrt{T})$最小迭代收敛速率。 |
| [^302] | [An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention](https://arxiv.org/abs/2312.10325) | 提出了一种名为BSARec的新方法，超越了自注意力，在序列推荐中注入了归纳偏差，并集成了低频和高频信息以减轻过度平滑问题 |
| [^303] | [Robust Errant Beam Prognostics with Conditional Modeling for Particle Accelerators](https://arxiv.org/abs/2312.10040) | 应用条件建模的方法，结合Siamese神经网络模型进行监督式异常检测，提高了粒子加速器错误束预测的健壮性和总体可用性。 |
| [^304] | [A framework for conditional diffusion modelling with applications in motif scaffolding for protein design](https://arxiv.org/abs/2312.09236) | 该论文提出一种统一的条件扩散建模框架，基于Doob's h-transform，用于解决蛋白设计中的基序支架问题 |
| [^305] | [Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations](https://arxiv.org/abs/2312.08935) | Math-Shepherd提出了一种新的数学奖励模型，通过自动生成的过程监督数据实现LLMs的逐步验证和加强，显著提高了数学问题解决的准确性。 |
| [^306] | [Better Neural PDE Solvers Through Data-Free Mesh Movers](https://arxiv.org/abs/2312.05583) | 提出了一种神经PDE求解器，通过无数据神经网格适配器（DMM）解决了神经PDE求解器需要昂贵网格数据和解决空间的自由度和拓扑结构变化的挑战。 |
| [^307] | [Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm Ciar\'an](https://arxiv.org/abs/2312.02658) | 本研究对比了机器学习和数值天气预报模型对风暴Ciarán的预测，发现机器学习模型可以准确模拟高影响天气事件的大尺度结构和动力驱动器。 |
| [^308] | [How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking](https://arxiv.org/abs/2312.01619) | LEMR框架通过在未标记的验证集中策略性地标注实例，显著降低了模型选择中的标注成本，并在各种NLP任务中展现了其高效性。 |
| [^309] | [Directions of Curvature as an Explanation for Loss of Plasticity](https://arxiv.org/abs/2312.00246) | 曲率方向的丧失被认为是导致神经网络可塑性丧失的一个重要原因，并且我们通过系统调查和在多个任务中的研究结果支持了这一观点。 |
| [^310] | [Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization](https://arxiv.org/abs/2311.18703) | 该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。 |
| [^311] | [Eigenmatrix for unstructured sparse recovery](https://arxiv.org/abs/2311.16609) | 本文提出了一种名为特征矩阵的数据驱动构造，用于解决非结构稀疏恢复问题，对于样本值中的噪声和样本位置的非结构性质具有很好的适应性。 |
| [^312] | [A General Framework for User-Guided Bayesian Optimization](https://arxiv.org/abs/2311.14645) | ColaBO是第一个贝叶斯原理框架，允许领域专家定制优化程序，整合先验信念以加速优化。 |
| [^313] | [On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates](https://arxiv.org/abs/2311.13584) | 我们提出了对于基于扩散的生成模型在强对数凹数据分布假设下的完整收敛理论保证，获得了对于参数估计和采样算法的最优上限估计。 |
| [^314] | [LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions](https://arxiv.org/abs/2311.11904) | 提出了一种集成LLMs和VLMs的框架，以找到最佳的类别描述符，解决了图像分类中在精确构建文本表示和区分相似类别方面的挑战 |
| [^315] | [Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information](https://arxiv.org/abs/2311.11509) | 该论文引入了一种新的方法，在令牌级别检测对抗性提示，利用语言模型的能力预测下一个标记的概率。 |
| [^316] | [Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools](https://arxiv.org/abs/2311.10801) | 使用EarnMore方法，我们提出了一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。 |
| [^317] | [Leveraging Function Space Aggregation for Federated Learning at Scale](https://arxiv.org/abs/2311.10291) | 提出一种新算法FedFish，通过利用客户端学习函数的局部近似，并使用基于费舍尔信息的估计，实现了对联邦学习中的函数空间聚合，并在大规模交叉设备基准测试中表现出更好的鲁棒性和性能。 |
| [^318] | [Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge](https://arxiv.org/abs/2311.09195) | 提出一种新颖的自主强化学习算法，无需任务特定知识即可生成适应智能体学习进展的课程 |
| [^319] | [Adversarial Preference Optimization](https://arxiv.org/abs/2311.08045) | 提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。 |
| [^320] | [ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering](https://arxiv.org/abs/2311.07632) | 提出了一种新颖的Residual Message Graph Convolution Network（ResMGCN），用于快速而准确地预测生物医学相互作用。 |
| [^321] | [Feature emergence via margin maximization: case studies in algebraic tasks](https://arxiv.org/abs/2311.07568) | 本文研究了神经网络在代数任务中学到的特征，发现边界最大化原则可以完全指定网络学到的特征。 |
| [^322] | [Open-Set Graph Anomaly Detection via Normal Structure Regularisation](https://arxiv.org/abs/2311.06835) | 通过正常结构规范化方法，实现开放图异常检测模型对未知异常的广义检测能力 |
| [^323] | [Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion](https://arxiv.org/abs/2311.06318) | 提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。 |
| [^324] | [Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers](https://arxiv.org/abs/2311.04256) | 本文提出了基于犹豫模糊集的多种包含关系定义、犹豫模糊信息系统的基础命题和基于多强度智能分类器的健康状态诊断方法。 |
| [^325] | [DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing](https://arxiv.org/abs/2311.01450) | DreamSmooth通过学习预测时间平滑奖励而非精确奖励，优化了基于模型的强化学习，在稀疏奖励任务上表现出最先进的性能。 |
| [^326] | [Re-evaluating Retrosynthesis Algorithms with Syntheseus](https://arxiv.org/abs/2310.19796) | 使用Syntheseus建立的基准库重新评估了回溯合成算法，揭示了现有技术模型的系统性缺陷并提供了对未来工作的指导建议。 |
| [^327] | [Corruption-Robust Offline Reinforcement Learning with General Function Approximation](https://arxiv.org/abs/2310.14550) | 设计了一种新的不确定性加权迭代过程以提高耐腐败的离线强化学习算法的效率，并在单策略覆盖和已知腐败水平的情况下实现了次优性界。 |
| [^328] | [Transformers for Green Semantic Communication: Less Energy, More Semantics](https://arxiv.org/abs/2310.07592) | 本研究提出了一种名为“Energy-Optimized Semantic Loss”（EOSL）的新型多目标损失函数，通过对变压器模型进行全面实验，实现了在推断过程中节省高达90%能量并提高44%语义相似性表现的目标。 |
| [^329] | [Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks](https://arxiv.org/abs/2310.06549) | 标签平滑方法在深度学习中发挥重要作用，既能提升模型泛化能力和校准性，又可能成为模型隐私泄露的因素。研究揭示了结合负因子进行平滑可有效阻止模型反推攻击，提升隐私保护效果，超越了当前最先进的防御技术。 |
| [^330] | [Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity](https://arxiv.org/abs/2310.05175) | 本研究发现LLMs中的激活异常值与网络层稀疏度的非均匀性相关，并提出了Outlier Weighed Layerwise Sparsity（OWL）作为剪枝LLMs到高稀疏度的秘密调味料。 |
| [^331] | [CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation](https://arxiv.org/abs/2310.01407) | CoDi引入了一种新的方法，通过适应预先训练的潜在扩散模型接受额外的图像条件输入，可以显著降低生成高质量结果所需的采样步骤。 |
| [^332] | [Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models](https://arxiv.org/abs/2310.00566) | 大型语言模型在信用评分任务中表现出强大的通用能力，通过首个开源框架和CALM模型，可以帮助解决传统信用评分方法所面临的挑战，并同时解决潜在的偏见问题。 |
| [^333] | [From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning](https://arxiv.org/abs/2310.00492) | 指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整 |
| [^334] | [SINCERE: Supervised Information Noise-Contrastive Estimation REvisited](https://arxiv.org/abs/2309.14277) | SINCERE提出了一个理论上合理的监督扩展，避免了同一类别的图像相互排斥，通过更好地分离不同类别的嵌入，在保持竞争性分类准确性的同时实现了更好的效果。 |
| [^335] | [Early-Exit with Class Exclusion for Efficient Inference of Neural Networks](https://arxiv.org/abs/2309.13443) | 提出了一种基于类别排除的动态推理早期退出方法，通过在中间层利用学到的特征排除大量不相关类别，从而显著降低神经网络推理的计算成本。 |
| [^336] | [AFN: Adaptive Fusion Normalization via an Encoder-Decoder Framework](https://arxiv.org/abs/2308.03321) | 该论文介绍了一种新的自适应融合规范化方法，通过编码器-解码器框架，在领域泛化和图像分类任务中表现优越。 |
| [^337] | [CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking](https://arxiv.org/abs/2307.16847) | CroSSL是一种跨模态的自监督学习方法，通过隐藏掩码和跨模态聚合器实现对时间序列的学习，无需负样本对和数据预处理。 |
| [^338] | [Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems](https://arxiv.org/abs/2303.05754) | 提出了一种将扩散采样和Krylov子空间方法协同结合的新型高效采样策略。 |
| [^339] | [Distances for Markov Chains, and Their Differentiation](https://arxiv.org/abs/2302.08621) | 本文提出了一个统一框架，用于生成马尔可夫链的距离，包括带节点属性的有向图，这个框架称为最优输运马尔可夫（OTM）距禄。 |
| [^340] | [Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging](https://arxiv.org/abs/2302.06727) | 深度学习技术在英国生物库眼底成像中被用于诊断帕金森病，可在疾病症状发作之前实现准确筛查。 |
| [^341] | [Distributional GFlowNets with Quantile Flows](https://arxiv.org/abs/2302.05793) | 本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。 |
| [^342] | [STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow Prediction](https://arxiv.org/abs/2212.04548) | STLGRU是一种新型的交通预测模型，能够有效捕捉交通网络的动态局部和全局时空关系，适用于低功耗设备 |
| [^343] | [Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression](https://arxiv.org/abs/2211.10968) | 研究了在目标函数不一定包含在核空间中的情况下，分治核函数的功能线性回归算法的统计优化性。 |
| [^344] | [Semi-supervised Batch Learning From Logged Data](https://arxiv.org/abs/2209.07148) | 本研究基于反事实风险最小化框架提出了一种半监督批量学习方法，解决了在已记录数据中反馈缺失的问题，提出了一个新的上界来处理这种学习问题。 |
| [^345] | [On the Evaluation of User Privacy in Deep Neural Networks using Timing Side Channel](https://arxiv.org/abs/2208.01113) | 在本研究中，我们发现并报告了DL实现中的一种新型数据依赖性时序侧信道泄漏，并展示了一个实用的推理时攻击。 |
| [^346] | [Variance estimation in graphs with the fused lasso](https://arxiv.org/abs/2207.12638) | 研究了在一般图结构问题中的方差估计，开发了线性时间估计器并提供了上界，允许推广到更广泛的分布类。 |
| [^347] | [Learning Progress Driven Multi-Agent Curriculum](https://arxiv.org/abs/2205.10016) | 提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。 |
| [^348] | [Algebraic Machine Learning with an Application to Chemistry](https://arxiv.org/abs/2205.05795) | 本文开发了一种机器学习流程，能够捕获细粒度的几何信息，而无需依赖光滑性假设。 |
| [^349] | [Discovering stochastic dynamical equations from biological time series data](https://arxiv.org/abs/2205.02645) | 提出一种方程发现方法，结合传统随机微积分方法和最先进的方程发现技术，从时间序列数据中分析波动并输出可解释的SDE，在合成和真实数据集上验证了方法的普适性和适用性。 |
| [^350] | [Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network](https://arxiv.org/abs/2201.11653) | 本文调查了导致神经网络稀疏性和选择性增加的各种条件。 |
| [^351] | [Variance Reduction Based Experience Replay for Policy Optimization](https://arxiv.org/abs/2110.08902) | 引入了基于方差减少的经验回放框架，实现了选择性重复利用相关样本来改善策略梯度估计，并构建了高效的离策略算法PG-VRER。 |
| [^352] | [HCR-Net: A deep learning based script independent handwritten character recognition network](https://arxiv.org/abs/2108.06663) | HCR-Net提出了一种基于迁移学习的脱机手写字符识别网络，通过部分利用预训练网络的特征提取层，实现了更快、更高效的训练，更好的性能和泛化能力。 |
| [^353] | [Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings](https://arxiv.org/abs/2104.08928) | 提出了一种基于群稀疏矩阵分解的方法，用于在新领域进行词嵌入的传递学习，以解决不同领域单词含义差异的挑战。 |
| [^354] | [RadarScenes: A Real-World Radar Point Cloud Data Set for Automotive Applications](https://arxiv.org/abs/2104.02493) | 提出了一个新的汽车雷达数据集，用于开发基于机器学习的雷达感知算法，重点关注移动道路用户。 |
| [^355] | [Deep-Lock: Secure Authorization for Deep Neural Networks](https://arxiv.org/abs/2008.05966) | 本文提出了一种名为Deep-Lock的通用和轻量级基于密钥的模型锁定方案，通过使用S-盒对训练完毕的DNN模型的每个参数进行加密，并确保只有在应用正确的秘密密钥时模型才能正确运行，从而防止了DNN模型的未经授权使用。 |
| [^356] | [Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic](https://arxiv.org/abs/1910.08597) | 该方法提出了SplitSGD，通过简单而有效的稳态检测，在检测到稳态阶段时降低学习速率，使其适用于凸问题和训练神经网络，表现优于其他随机优化方法。 |
| [^357] | [AI Oversight and Human Mistakes: Evidence from Centre Court.](http://arxiv.org/abs/2401.16754) | 人工智能系统在纠正人类错误方面起到了积极作用，但此举也潜在导致心理成本，并影响人的决策。通过研究网球比赛中的Hawk-Eye审查系统，我们发现引入AI监督后，裁判员的错误率下降，心理成本导致他们更倾向于将球判为进界，从而产生了类型错判的转变。 |
| [^358] | [Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters.](http://arxiv.org/abs/2401.16457) | 本文引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，可在推理时逐渐过渡从模型的偏向状态到完全去偏的版本，并通过实验证明了其在分类和检索任务中的性能。 |
| [^359] | [Prompt Weight Experiments for LLM Instruction Fine-Tuning.](http://arxiv.org/abs/2401.13586) | LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。 |
| [^360] | [Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo.](http://arxiv.org/abs/2401.11665) | 本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。 |
| [^361] | [SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI.](http://arxiv.org/abs/2401.09627) | SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。 |
| [^362] | [Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility.](http://arxiv.org/abs/2401.09498) | 本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。 |
| [^363] | [Space and Time Continuous Physics Simulation From Partial Observations.](http://arxiv.org/abs/2401.09198) | 本研究提出了一种新颖的方法，可以从部分观测中进行连续的空间和时间物理模拟，并解决了基于固定支持网格的传统方法的缺点。这种方法通过在稀疏观测上进行训练，利用两个相互关联的动力系统在稀疏位置和连续域上进行预测和插值求解。 |
| [^364] | [Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling.](http://arxiv.org/abs/2401.08876) | 本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。 |
| [^365] | [An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation.](http://arxiv.org/abs/2401.06356) | 本文对知识蒸馏中参数选择的影响进行了大规模实证研究，揭示了不同选项对学生性能的整体影响，并找到了在各方面表现良好的单一配置。 |
| [^366] | [Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery.](http://arxiv.org/abs/2401.05394) | 该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。 |
| [^367] | [Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving.](http://arxiv.org/abs/2401.03160) | 本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。 |
| [^368] | [Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors.](http://arxiv.org/abs/2401.02739) | 本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。 |
| [^369] | [Reinforcement Unlearning.](http://arxiv.org/abs/2312.15910) | 强化学习中的消除学习是一种新兴的研究领域，旨在解决环境所有者有权撤销智能体训练数据的隐私问题。该领域面临三个主要挑战。 |
| [^370] | [Can LLMs Patch Security Issues?.](http://arxiv.org/abs/2312.00024) | 本文提出了一种新的方法, Feedback-Driven Solution Synthesis (FDSS), 旨在通过将LLMs与静态代码分析工具Bandit结合，解决代码中的安全漏洞问题。该方法在现有方法的基础上有显著改进，并引入了一个新的数据集PythonSecurityEval。 |
| [^371] | [SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation.](http://arxiv.org/abs/2310.12508) | 这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。 |
| [^372] | [Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning.](http://arxiv.org/abs/2310.11897) | 本文介绍了一种名为加速策略梯度的方法，通过应用Nesterov动量在强化学习中实现更快的全局收敛速度。通过在softmax策略参数化中收敛到最优策略，它以 $\tilde{O}(1/t^2)$ 的速率收敛。这是第一个基于Nesterov加速梯度在强化学习中全局收敛速率的研究。 |
| [^373] | [On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning.](http://arxiv.org/abs/2310.11840) | 这项工作通过对强化学习中17种目标规范形式的表达能力进行全面比较，填补了现有文献中的空白。通过将这些形式化方法进行预排序，并呈现为哈斯图，我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。 |
| [^374] | [Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation.](http://arxiv.org/abs/2310.11730) | 本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。 |
| [^375] | [Sensitivity-Aware Amortized Bayesian Inference.](http://arxiv.org/abs/2310.11122) | 本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。 |
| [^376] | [Large Language Model Unlearning.](http://arxiv.org/abs/2310.10683) | 大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。 |
| [^377] | [Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task.](http://arxiv.org/abs/2310.09336) | 组合能力以乘法方式出现：研究了条件扩散模型在合成任务中的组合泛化能力，结果显示这种能力受到底层数据生成过程的结构影响，且模型在学习到更高级的组合时存在困难。 |
| [^378] | [Understanding the Effects of RLHF on LLM Generalisation and Diversity.](http://arxiv.org/abs/2310.06452) | 本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。 |
| [^379] | [On Double-Descent in Reinforcement Learning with LSTD and Random Features.](http://arxiv.org/abs/2310.05518) | 本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。 |
| [^380] | [Entropy-MCMC: Sampling from Flat Basins with Ease.](http://arxiv.org/abs/2310.05401) | 本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。 |
| [^381] | [Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity.](http://arxiv.org/abs/2310.02277) | 本文研究通过稀疏性分析LLM预训练权重的任务中心角度，挑战了传统对于权重中冗余性的观点，并提出了"垃圾DNA假设"。 |
| [^382] | [Why do autoencoders work?.](http://arxiv.org/abs/2310.02250) | 自编码器是一种深度神经网络模型，通过调整参数实现输入数据和重构输出之间的最小差异，用于识别数据在高维空间中的内在维度，并且对于某些拓扑结构，存在难以找到完美重构网络的限制。 |
| [^383] | [Going Beyond Familiar Features for Deep Anomaly Detection.](http://arxiv.org/abs/2310.00797) | 该论文提出了一种超越熟悉特征的深度异常检测方法，通过利用可解释性在输入空间中捕捉新颖特征来避免假阴性。该方法在广泛的异常基准测试中取得了强大的性能，并且消除了昂贵的背景模型和密集匹配的需求。 |
| [^384] | [Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning.](http://arxiv.org/abs/2309.16808) | 本研究利用高分辨率影像和混合学习方法，通过图像数据中的特征提取和模式检测，估计了个别社区的人口密度、家庭收入中位数和教育程度。 |
| [^385] | [On the Posterior Distribution in Denoising: Application to Uncertainty Quantification.](http://arxiv.org/abs/2309.13598) | 该论文研究了去噪中的后验分布及其与后验均值之间的关系，并应用于预训练去噪器的不确定性量化。提出了一种高效计算后验分布主成分和近似边际分布的方法。不需要显式计算高阶矩张量或进行训练或微调。 |
| [^386] | [3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images.](http://arxiv.org/abs/2309.11015) | 本文提出了一种新颖的3D-U-SAM网络，用于少样本CBCT图像的牙齿分割。通过使用预训练的SAM和卷积逼近方法，以及跳跃连接融合特征，本方法在解决小样本问题上表现出很好的效果。 |
| [^387] | [Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning.](http://arxiv.org/abs/2309.10302) | 这篇论文提出了一种称为解耦训练（D-Train）的令人沮丧的、无超参数的多领域学习方法。该方法采用了一种三阶段的训练策略，首先进行预训练，然后在每个领域上进行后训练，最后进行头部微调，实现解耦训练以获得更好的性能。 |
| [^388] | [RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud.](http://arxiv.org/abs/2309.09737) | RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。 |
| [^389] | [VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference.](http://arxiv.org/abs/2309.08227) | 这项研究提出了一种具有实时推理能力的流式终身学习方法，采用虚拟梯度进行连续表示学习，借助语义记忆来抑制灾难性遗忘，并在多样化的数据上进行了广泛实验。 |
| [^390] | [DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning.](http://arxiv.org/abs/2309.05173) | DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。 |
| [^391] | [Deep Video Codec Control.](http://arxiv.org/abs/2308.16215) | 本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。 |
| [^392] | [Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings.](http://arxiv.org/abs/2308.11804) | 该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。 |
| [^393] | [A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models.](http://arxiv.org/abs/2308.08925) | 本文提出了一种针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的新方法，通过将攻击视为书写风格之间的风格转换，引入新的损失函数来生成欺骗性图像，实现了最先进的攻击成功率。 |
| [^394] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^395] | [OCDaf: Ordered Causal Discovery with Autoregressive Flows.](http://arxiv.org/abs/2308.07480) | 我们提出了一种新的有序因果推断方法，可以从观测数据中学习因果图，并在多种基准测试中展示出了最先进的性能。 |
| [^396] | [Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions.](http://arxiv.org/abs/2308.05724) | 该论文研究了使用动态激活函数优化前馈和卷积神经网络的性能。研究结果表明，在卷积神经网络和多层感知器中，复杂的分段线性激活函数比ReLU激活函数表现更好。 |
| [^397] | [CoRe Optimizer: An All-in-One Solution for Machine Learning.](http://arxiv.org/abs/2307.15663) | CoRe优化器是一种高性能的机器学习优化器，具有快速、平滑收敛、低计算需求和通用适用性的特点，在训练终身机器学习潜力方面表现出优势。 |
| [^398] | [Bandits with Deterministically Evolving States.](http://arxiv.org/abs/2307.11655) | 该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。 |
| [^399] | [A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency.](http://arxiv.org/abs/2307.10655) | 本文介绍了一篇系统综述，从新的视角探讨了在联邦学习中应该分享什么，注重模型效用、隐私泄露和通信效率。 |
| [^400] | [From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs.](http://arxiv.org/abs/2307.08433) | 这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。 |
| [^401] | [Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning.](http://arxiv.org/abs/2307.05209) | 我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。 |
| [^402] | [Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds.](http://arxiv.org/abs/2307.04866) | 该论文研究了使用腰部佩戴的加速计自动检测步态事件和行走距离的方法，通过分析市售智能手机加速计数据，实现了从广泛的步态速度范围中提取步态特征，可用于对Duchenne肌肉萎缩患儿和典型发育正常患者的评估。 |
| [^403] | [Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks.](http://arxiv.org/abs/2307.01649) | 本文研究了卷积残差网络在非参数分类任务中的性能。研究表明，通过使用权重衰减的ConvResNeXts，可以隐含地实现对模块的稀疏性，从而使网络能够适应低维流形的平滑性和结构，并高效地学习函数。 |
| [^404] | [Understanding Unfairness via Training Concept Influence.](http://arxiv.org/abs/2306.17828) | 通过观察训练数据的作用，研究模型不公平性的来源和影响，并通过改变样本的属性来计算训练样本对模型的不公平性的影响。 |
| [^405] | [Optimal Sensor Placement with Adaptive Constraints for Nuclear Digital Twins.](http://arxiv.org/abs/2306.13637) | 本文提出了一种数据驱动的技术，通过整合自适应的约束条件，用较少的传感器实现重建反应堆流场并创建核数字孪生体。 |
| [^406] | [Human Limits in Machine Learning: Prediction of Plant Phenotypes Using Soil Microbiome Data.](http://arxiv.org/abs/2306.11157) | 本论文深入研究了机器学习模型在预测土壤与植物表型之间联系方面的潜力，证明加入土壤物理化学性质和微生物种群密度等环境特征可以提高预测准确性。 |
| [^407] | [Datasheets for Machine Learning Sensors.](http://arxiv.org/abs/2306.08848) | 本研究提出了一种用于机器学习传感器的标准数据表模板，并讨论了其主要组成部分。这些数据表可以促进对传感器数据在机器学习应用中的理解和利用，并提供了客观的性能评估指标。 |
| [^408] | [Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic.](http://arxiv.org/abs/2306.02865) | 该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。 |
| [^409] | [Adaptive ship-radiated noise recognition with learnable fine-grained wavelet transform.](http://arxiv.org/abs/2306.01002) | 本文提出了一种自适应通用识别系统AGNet，通过转换可学习的微观参数，学习了不同频率下水下声音的特性，以解决在可变水下环境中识别船舶辐射噪音的问题。 |
| [^410] | [Balanced Training of Energy-Based Models with Adaptive Flow Sampling.](http://arxiv.org/abs/2306.00684) | 本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。 |
| [^411] | [Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments.](http://arxiv.org/abs/2306.00575) | 本文研究了时间预测在雾环境下预测复制中的应用，提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，可以在减少多余数据的同时，只有微小的数据可用性降低。 |
| [^412] | [Underwater-Art: Expanding Information Perspectives With Text Templates For Underwater Acoustic Target Recognition.](http://arxiv.org/abs/2305.19612) | 本文提出了一种基于文本模板的水下声学目标识别方法（UART），从不同视角整合相关信息，通过音频-频谱图-文本三模态对比学习框架，赋予UART用自然语言指导声学表示学习的能力，显著提高识别模型的可解释性和鲁棒性。 |
| [^413] | [A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks.](http://arxiv.org/abs/2305.19306) | 本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。 |
| [^414] | [Autoencoding Conditional Neural Processes for Representation Learning.](http://arxiv.org/abs/2305.18485) | 本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。 |
| [^415] | [Approximation theory of transformer networks for sequence modeling.](http://arxiv.org/abs/2305.18475) | 本文证明了变压器假设空间的普遍逼近定理，并提出了一种新的规律概念用于精确逼近速率估计，揭示了变压器适用于逼近哪些类型的序列关系，并讨论了其与传统序列建模方法之间的结构偏差。 |
| [^416] | [Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize.](http://arxiv.org/abs/2305.16830) | 本文提出了一种避免限制性假设的解决方案，利用机器学习模型的特性来提高学习损失函数的样本效率，在预测优化问题中实现了最先进的结果。 |
| [^417] | [vFedSec: Efficient Secure Aggregation for Vertical Federated Learning via Secure Layer.](http://arxiv.org/abs/2305.16794) | vFedSec提出了一个用于垂直联邦学习的新型Secure Layer，旨在使用最先进的安全模块，实现安全高效的联合训练。实验结果表明，该方法在保护数据隐私效果显著，不会影响训练性能。 |
| [^418] | [Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery.](http://arxiv.org/abs/2305.14259) | 本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。 |
| [^419] | [Multimodal Web Navigation with Instruction-Finetuned Foundation Models.](http://arxiv.org/abs/2305.11854) | 本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。 |
| [^420] | [DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects.](http://arxiv.org/abs/2305.04449) | 本论文介绍了一种名为DeformerNet的神经网络架构，通过学习三维可塑物体的低维表示来实现机器人对物体形状的操纵。这种方法不需要手工特征和物体特定的控制模型，可在仿真和真实机器人上进行演示和应用。 |
| [^421] | [Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees.](http://arxiv.org/abs/2305.03938) | 本文提出了一种新的双时间尺度框架，证明了其在温和条件下收敛性，该框架包括了各种流行的Adam家族算法，用于训练无平滑神经网络和应对重尾噪声的需求，并通过实验表明了其效率和鲁棒性。 |
| [^422] | [MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks.](http://arxiv.org/abs/2304.14979) | 本文介绍了一种新型框架MLCopilot，通过利用最先进的大语言模型，扩展其能力以理解结构化输入并进行深入推理以解决新型ML任务的能力，展示了MLCopilot在解决图像分类，文本分类和表格分类三项任务方面的巨大潜力。 |
| [^423] | [Spintronic Physical Reservoir for Autonomous Prediction and Long-Term Household Energy Load Forecasting.](http://arxiv.org/abs/2304.03343) | 本研究展示了利用自旋电子物理水库进行自治型长期预测任务的方法，可以用于建模混沌时间序列和动态时间序列数据，是适合在边缘设备上进行实时学习的。这里提出的基于微旋磁隧穿结的涡旋子可以作为实现此种RC的原型。 |
| [^424] | [Applications of No-Collision Transportation Maps in Manifold Learning.](http://arxiv.org/abs/2304.00199) | 本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。 |
| [^425] | [Towards black-box parameter estimation.](http://arxiv.org/abs/2303.15041) | 本文提出了一种基于弱参数结构假设的黑盒程序，用于估计统计模型参数。该程序可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。 |
| [^426] | [Can AI-Generated Text be Reliably Detected?.](http://arxiv.org/abs/2303.11156) | 本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。 |
| [^427] | [Recent Developments in Machine Learning Methods for Stochastic Control and Games.](http://arxiv.org/abs/2303.10257) | 本文回顾了基于机器学习的随机控制问题和博弈的计算方法，特别是着重介绍了使用深度学习算法解决高维度和非常复杂结构情况下问题的新方法。 |
| [^428] | [Meta contrastive label correction for financial time series.](http://arxiv.org/abs/2303.08103) | 本文针对股票价格预测中标记不准确的问题，提出了一种元对比标签校正方法。方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签，并提高分类性能。 |
| [^429] | [Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices.](http://arxiv.org/abs/2303.00492) | Lumos是第一个支持节点级联邦图学习的框架，具有特征和度数保护功能。它采用了树构造器和去中心化节点聚合策略来提高图的表示能力。 |
| [^430] | [A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization.](http://arxiv.org/abs/2302.08766) | 该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。 |
| [^431] | [On Sampling with Approximate Transport Maps.](http://arxiv.org/abs/2302.04763) | 本研究探讨了两种基于传输映射的抽样方法，研究结果表明，基于流的提议可以处理多峰目标，在高维度和训练不良的情况下使用依赖于重新参数化的方法更加稳健。 |
| [^432] | [Graph-based Time-Series Anomaly Detection: A Survey.](http://arxiv.org/abs/2302.00058) | 本文综述了基于图的时间序列异常检测，主要探讨了图表示学习的潜力和最先进的图异常检测技术在时间序列中的应用。 |
| [^433] | [Inverse Solvability and Security with Applications to Federated Learning.](http://arxiv.org/abs/2211.14115) | 介绍了逆可解性和安全性的概念，以及其在联邦学习中的应用。论文提供了模型示例，展示了如何通过增加用户数量来增加可解性和安全性。 |
| [^434] | [From Denoising Diffusions to Denoising Markov Models.](http://arxiv.org/abs/2211.03595) | 本论文提出了一个统一的框架，将去噪扩散模型推广到广泛的空间中，并导致分数匹配的原始扩展，适用于各种应用程序。 |
| [^435] | [A Theoretical Analysis of the Learning Dynamics under Class Imbalance.](http://arxiv.org/abs/2207.00391) | 本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。 |
| [^436] | [Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios.](http://arxiv.org/abs/2206.01900) | 本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。 |

# 详细

[^1]: TrustAgent: 通过代理构成实现安全可信赖的LLM代理

    TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution

    [https://rss.arxiv.org/abs/2402.01586](https://rss.arxiv.org/abs/2402.01586)

    本文介绍了一种基于代理构成的代理框架TrustAgent，该框架通过预先规划、规划过程中和计划后检查三种策略来提高LLM代理的安全性。实验结果表明，这些方法可以有效识别和预防潜在危险。此外，还研究了安全性与使用者满意度以及模型推理能力与效率之间的关系。

    

    近年来，基于LLM的代理引起了广泛关注，但其可信度仍未得到深入探索。由于代理可以直接与物理环境交互，其可靠性和安全性至关重要。本文提出了一种基于代理构成的代理框架TrustAgent，对LLM代理的安全性维度进行了初步研究。该框架包括三种策略：预先规划策略，在生成计划之前向模型注入安全知识；规划过程中策略，在生成计划时增强安全性；计划后检查策略，通过计划后检查确保安全性。通过实验分析，我们展示了这些方法如何通过识别和预防潜在危险有效提高LLM代理的安全性。此外，我们还探讨了安全性与使用者满意度之间的复杂关系，以及模型的推理能力与其效率之间的关联。

    The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
    
[^2]: 因果熵和信息增益的基本性质

    Fundamental Properties of Causal Entropy and Information Gain

    [https://rss.arxiv.org/abs/2402.01341](https://rss.arxiv.org/abs/2402.01341)

    本研究通过建立和分析因果熵和因果信息增益的基本性质，包括界限和链规则，阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义，为提升因果机器学习任务铺平了道路。

    

    最近的发展使得能够量化在结构因果模型(SCM)下的因果控制。这是通过引入一些量来编码在干预另一个变量时某个变量熵的变化来实现的。这些量被命名为因果熵和因果信息增益，旨在解决现有信息论方法在因果性在机器学习任务中起关键作用时的局限性。我们的研究通过建立和分析这些概念的基本性质，包括界限和链规则，对因果熵和因果信息增益的概念进行了形式上的理解。此外，我们阐明了因果熵与随机干预的关系，并提出了因果条件熵和因果条件信息增益的定义。总体而言，这个探索为提升因果机器学习任务铺平了道路。

    Recent developments enable the quantification of causal control given a structural causal model (SCM). This has been accomplished by introducing quantities which encode changes in the entropy of one variable when intervening on another. These measures, named causal entropy and causal information gain, aim to address limitations in existing information theoretical approaches for machine learning tasks where causality plays a crucial role. They have not yet been properly mathematically studied. Our research contributes to the formal understanding of the notions of causal entropy and causal information gain by establishing and analyzing fundamental properties of these concepts, including bounds and chain rules. Furthermore, we elucidate the relationship between causal entropy and stochastic interventions. We also propose definitions for causal conditional entropy and causal conditional information gain. Overall, this exploration paves the way for enhancing causal machine learning tasks th
    
[^3]: 疫苗：针对大规模语言模型的干扰感知对齐技术

    Vaccine: Perturbation-aware Alignment for Large Language Model

    [https://rss.arxiv.org/abs/2402.01109](https://rss.arxiv.org/abs/2402.01109)

    疫苗是一种针对大规模语言模型的干扰感知对齐技术，通过逐渐添加扰动产生不变的隐藏嵌入，提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    

    作为一种新的微调即服务范 paradigm，大型语言模型 (LLM) 为用户上传的一小部分有害数据提供了新的攻击面，这些数据很容易欺骗微调过程从而产生对齐失效的模型。我们进行了实证分析，揭示了一种可能导致对齐失效的有害嵌入漂移现象。受到我们的发现启发，我们提出了疫苗 (Vaccine) ，一种针对干扰感知的对齐技术，以减轻用户微调的安全风险。疫苗的核心思想是通过在对齐阶段逐渐添加精心设计的扰动，产生不变的隐藏嵌入，从而使嵌入能够抵御来自未经消毒的用户数据的有害扰动。我们在开源主流LLM（如Llama2，Opt，Vicuna）上的实验结果表明，疫苗能够提高对抗有害提示引起的嵌入漂移的对齐鲁棒性，同时保留对良性提示的推理能力。

    The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
    
[^4]: 通过卷积神经网络识别TESS全画幅图像光变曲线中的短周期变量

    Short-Period Variables in TESS Full-Frame Image Light Curves Identified via Convolutional Neural Networks

    [https://arxiv.org/abs/2402.12369](https://arxiv.org/abs/2402.12369)

    提出了一种通过卷积神经网络识别TESS全画幅图像光变曲线中的短周期变量的方法，实现了高效率的大规模档案搜索。

    

    arXiv:2402.12369v1 公告类型：交叉 抽象：过境系外行星勘测卫星（TESS）任务在其为期两年的主要任务期间测量了约85%的天空中的恒星光，导致数百万TESS 30分钟间隔光变曲线用于搜索过境系外行星。为了搜索这一巨大数据集，我们旨在提供一种既具有计算效率、生产性预测性，又最大程度减少所需人类搜索工作的方法。我们提出了一种卷积神经网络，用于识别短周期变量。为了对给定的光变曲线进行预测，我们的网络不需要使用其他方法确定的先前目标参数。我们的网络在单个GPU上对TESS 30分钟间隔光变曲线进行推断约5ms，从而实现了大规模的档案搜索。我们提出了14156个由我们的网络识别出的短周期变量的集合。我们识别的大多数变量属于两个突出的人群之一。

    arXiv:2402.12369v1 Announce Type: cross  Abstract: The Transiting Exoplanet Survey Satellite (TESS) mission measured light from stars in ~85% of the sky throughout its two-year primary mission, resulting in millions of TESS 30-minute cadence light curves to analyze in the search for transiting exoplanets. To search this vast dataset, we aim to provide an approach that is both computationally efficient, produces highly performant predictions, and minimizes the required human search effort. We present a convolutional neural network that we train to identify short period variables. To make a prediction for a given light curve, our network requires no prior target parameters identified using other methods. Our network performs inference on a TESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large scale archival searches. We present a collection of 14156 short-period variables identified by our network. The majority of our identified variables fall into two prominent popu
    
[^5]: 对大型语言模型进行AI反馈的关键评估

    A Critical Evaluation of AI Feedback for Aligning Large Language Models

    [https://arxiv.org/abs/2402.12366](https://arxiv.org/abs/2402.12366)

    研究质疑复杂的强化学习在AI反馈中的必要性，表明使用更强的教师模型进行监督微调可以超越现有的RLAIF管道。

    

    强化学习与AI反馈（RLAIF）是一种用于提高强大预训练语言模型的指令遵循能力的流行范式。 RLAIF首先使用来自教师模型的示范进行监督微调（SFT），然后再使用来自评论模型的反馈进行强化学习（RL）进一步微调模型。尽管最近流行的开源模型已经证明了从RL步骤中获得的性能显着提高，但在本文中，我们质疑是否复杂的RL步骤真正有必要为AI反馈。我们展示了RL步骤的改进几乎完全是因为使用较弱的教师模型（例如GPT-3.5）用于SFT数据收集而不是用于AI反馈生成的评论者（例如GPT-4）的广泛实践。具体而言，我们展示了简单的以GPT-4作为教师的监督微调优于现有的RLAIF管道。

    arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w
    
[^6]: 通用物理变压器

    Universal Physics Transformers

    [https://arxiv.org/abs/2402.12365](https://arxiv.org/abs/2402.12365)

    提出了通用物理变压器（UPTs）这一新颖学习范式，能够模拟广泛的时空问题，同时适用于拉格朗日和欧拉离散化方案，有效地传播动态并允许查询潜在空间

    

    基于深度神经网络的偏微分方程替代者近来引起了越来越多的关注。然而，类似于它们的数值对应物，在不同应用中使用不同的技术，即使系统的基础动态相似。一个著名的例子是在计算流体动力学中的拉格朗日和欧拉表述，这为神经网络有效地建模基于粒子而不是网格的动态构成了挑战。我们引入了通用物理变压器（UPTs），这是一种新颖的学习范式，它模拟了一系列时空问题 - 对拉格朗日和欧拉离散化方案。UPTs在没有基于网格或基于粒子的潜在结构的情况下运行，从而在网格和粒子之间实现了灵活性。UPTs在潜在空间中高效传播动态，强调了逆编码和解码技术。最后，UPTs允许查询潜在空间表现

    arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
    
[^7]: LoRA+: 大规模模型的高效低秩适应性

    LoRA+: Efficient Low Rank Adaptation of Large Models

    [https://arxiv.org/abs/2402.12354](https://arxiv.org/abs/2402.12354)

    LoRA+通过设置不同的学习率来改进原始LoRA的低效率问题，在保持计算成本不变的情况下提高了模型性能和微调速度。

    

    在这篇论文中，我们展示了低秩适应（LoRA）最初由胡等人（2021年）引入，导致对具有大宽度（嵌入维度）的模型进行微调时表现亚优。这是因为LoRA中的适配器矩阵A和B使用相同的学习率进行更新。通过对大宽度网络进行缩放参数的论证，我们展示了对适配器矩阵A和B使用相同的学习率不利于有效的特征学习。然后，我们表明LoRA的这种次优性可以简单地通过为LoRA适配器矩阵A和B设置不同的学习率以及一个精心选择的比率来进行校正。我们将这个提出的算法称为LoRA$+$。在我们广泛的实验证明中，LoRA$+$在相同计算成本下提高了性能（1-2％的改进）和微调速度（最多提速约2倍）。

    arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
    
[^8]: 通过博弈论评估揭示LLM的战略推理局限性的GTBench

    GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations

    [https://arxiv.org/abs/2402.12348](https://arxiv.org/abs/2402.12348)

    该论文通过博弈论任务评估了LLMs在竞争环境中的推理能力，观察到LLMs在不同游戏场景下表现出不同行为，具有重要的战略推理局限性。

    

    随着大型语言模型（LLMs）被整合到关键的现实世界应用中，它们的战略和逻辑推理能力变得越来越关键。本文通过博弈论任务评估LLMs在竞争环境中的推理能力，例如，需要纯逻辑和战略推理来与对手竞争的棋盘游戏和纸牌游戏。我们首先提出了GTBench，这是一个以语言驱动的环境，包括10个广泛认可的任务，涵盖了全面的游戏分类法：完整信息与不完整信息，动态与静态，以及概率与确定性场景。然后，我们研究了两个关键问题：（1）表征LLMs的博弈论推理；（2）LLM对抗LLM的比赛作为推理评估。我们观察到（1）LLMs在各种游戏场景下有不同的行为；例如，LLMs在完整和确定性游戏中失败，但它们在概率游戏中具有竞争力。

    arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
    
[^9]: 一种对事件识别模型鲁棒性进行评估的对抗方法

    An Adversarial Approach to Evaluating the Robustness of Event Identification Models

    [https://arxiv.org/abs/2402.12338](https://arxiv.org/abs/2402.12338)

    该论文提出了一种基于物理的模态分解方法用于提取事件分类特征，并评估了可解释分类器在面对对抗算法时的鲁棒性。

    

    智能机器学习方法正在积极用于事件检测和识别，可实时获得态势感知。然而，这些机器学习算法已被证明容易受到对传入遥测数据的对抗性攻击。本文考虑了一种基于物理的模态分解方法来提取事件分类的特征，并专注于可解释的分类器，包括逻辑回归和梯度提升，以区分两种类型的事件：负载损失和发电损失。然后，对生成的分类器进行对抗算法测试以评估其鲁棒性。对抗攻击在两种情境下进行测试：白盒设置，攻击者完全了解分类模型；灰盒设置，攻击者可以访问与用于训练分类器的相同网络的历史数据，但不知道分类

    arXiv:2402.12338v1 Announce Type: cross  Abstract: Intelligent machine learning approaches are finding active use for event detection and identification that allow real-time situational awareness. Yet, such machine learning algorithms have been shown to be susceptible to adversarial attacks on the incoming telemetry data. This paper considers a physics-based modal decomposition method to extract features for event classification and focuses on interpretable classifiers including logistic regression and gradient boosting to distinguish two types of events: load loss and generation loss. The resulting classifiers are then tested against an adversarial algorithm to evaluate their robustness. The adversarial attack is tested in two settings: the white box setting, wherein the attacker knows exactly the classification model; and the gray box setting, wherein the attacker has access to historical data from the same network as was used to train the classifier, but does not know the classifica
    
[^10]: Robust CLIP: 对视觉嵌入进行无监督对抗微调以获得强大的大规模视觉-语言模型

    Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models

    [https://arxiv.org/abs/2402.12336](https://arxiv.org/abs/2402.12336)

    通过无监督对抗微调，提出了一种强大的CLIP视觉编码器，用于增强各种视觉-语言模型的鲁棒性。恶意第三方提供操纵图像的用户隐形攻击得以杜绝。

    

    诸如OpenFlamingo、LLaVA和GPT-4之类的多模型基础模型越来越广泛地用于各种真实世界任务。先前的工作表明，这些模型在视觉模态上极易受到对抗性攻击的影响。这些攻击可以用来传播虚假信息或欺骗用户，因此构成了一个重大风险，这使得大型多模型基础模型的鲁棒性成为一项紧迫的问题。我们提出了一种无监督对抗微调方案，以获得强大的CLIP视觉编码器，在所有依赖于CLIP的视觉下游任务（VLMs、零样本分类）上具有鲁棒性。特别地，我们展示了一旦更换原始的CLIP模型，用户在使用VLMs时会受到恶意第三方提供的操纵图像的潜在攻击。

    arXiv:2402.12336v1 Announce Type: cross  Abstract: Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many vision-language models (VLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (VLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of VLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP mo
    
[^11]: 生成生存可解释轨迹和数据

    Generating Survival Interpretable Trajectories and Data

    [https://arxiv.org/abs/2402.12331](https://arxiv.org/abs/2402.12331)

    提出了一种新的模型，能够生成生存轨迹和数据，并通过特定结构的自动编码器解决了预测、数据补充和生成原型时间相关轨迹等任务

    

    提出了一种基于应用特定结构的自动编码器来生成生存轨迹和数据的新模型。 它解决了三个任务。 首先，它基于Beran估计器为新生成的特征向量提供事件时间的预测和生存函数的形式。 第二，该模型基于给定的训练集生成额外数据，可以补充原始数据集。 第三，最重要的是，它为对象生成了一个原型时间相关轨迹，描述了如何改变对象的特征以实现不同时间事件的时间。 轨迹可以看作是一种反事实解释。 由于将特定加权方案纳入变分自动编码器中，所提出的模型在训练和推理过程中表现出鲁棒性。 该模型还通过解决分类问题确定了新生成数据的被审查指标。

    arXiv:2402.12331v1 Announce Type: cross  Abstract: A new model for generating survival trajectories and data based on applying an autoencoder of a specific structure is proposed. It solves three tasks. First, it provides predictions in the form of the expected event time and the survival function for a new generated feature vector on the basis of the Beran estimator. Second, the model generates additional data based on a given training set that would supplement the original dataset. Third, the most important, it generates a prototype time-dependent trajectory for an object, which characterizes how features of the object could be changed to achieve a different time to an event. The trajectory can be viewed as a type of the counterfactual explanation. The proposed model is robust during training and inference due to a specific weighting scheme incorporating into the variational autoencoder. The model also determines the censored indicators of new generated data by solving a classificatio
    
[^12]: 基于查询的对抗性提示生成

    Query-Based Adversarial Prompt Generation

    [https://arxiv.org/abs/2402.12329](https://arxiv.org/abs/2402.12329)

    该研究提出了一种基于查询的对抗性攻击方法，通过利用远程语言模型的 API 访问构造对抗性示例，使模型以更高概率发出有害字符串，而非仅仅基于模型之间的转移性攻击。

    

    最近的研究表明，可以构造对抗性示例，导致一个对其进行了调整的语言模型产生有害字符串或执行有害行为。现有的攻击要么在白盒设置中（完全访问模型权重），要么通过可转移性：一种现象，即在一个模型上精心设计的对抗性示例通常在其他模型上仍然有效。我们通过基于查询的攻击改进以前的工作，利用 API 访问远程语言模型来构造对抗性示例，使模型以（明显）更高的概率发出有害字符串，而不能仅仅使用转移攻击。我们在 GPT-3.5 和 OpenAI 的安全分类器上验证了我们的攻击；我们能够让 GPT-3.5 发出有害字符串，而目前的转移攻击失败了，并且我们几乎以 100% 的概率规避了安全分类器。

    arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
    
[^13]: 基于LLM的心理学智能代理：一项关于游戏化评估的研究

    LLM Agents for Psychology: A Study on Gamified Assessments

    [https://arxiv.org/abs/2402.12326](https://arxiv.org/abs/2402.12326)

    本研究提出了PsychoGAT（心理游戏代理）以实现心理评估的通用游戏化，通过将强大的LLM代理纳入角色，将标准量表转化为个性化且具有吸引力的互动小说游戏。

    

    心理测量对于精神健康、自我理解和个人发展至关重要。传统方法，如自我报告量表和心理学家访谈，常常面临参与度和可获得性方面的挑战。虽然已经探讨了基于游戏和LLM的工具来提高用户兴趣并自动化评估，但它们难以平衡参与度和普适性。在这项工作中，我们提出了PsychoGAT（心理游戏代理），以实现心理评估的通用游戏化。主要洞察是强大的LLM既可以充当熟练的心理学家，也可以是创新的游戏设计师。通过将LLM代理纳入指定角色并精心管理它们的互动，PsychoGAT可以将任何标准量表转化为个性化且具有吸引力的互动小说游戏。为验证所提出的方法，我们进行心理度量评估以评估其有效性，并使用人类

    arXiv:2402.12326v1 Announce Type: new  Abstract: Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ huma
    
[^14]: 用于在非GPS战场环境中进行地标识别和移动节点定位的地标立体数据集

    Landmark Stereo Dataset for Landmark Recognition and Moving Node Localization in a Non-GPS Battlefield Environment

    [https://arxiv.org/abs/2402.12320](https://arxiv.org/abs/2402.12320)

    提出了一种在非GPS战场环境中通过使用地标锚定节点实现移动部队或防御部队的虚拟坐标获取的策略，并利用Yolov5模型进行地标识别和有效的立体匹配算法进行地标距离估计。

    

    本文提出了一种新的策略，利用地标锚定节点而不是基于无线电的锚定节点来获取移动部队或防御部队的虚拟坐标（地标ID，距离），从而帮助在GPS受限的战场环境中跟踪和操纵部队沿着安全路径前进。所提出的策略采用Yolov5模型进行地标识别，并利用高效的立体匹配算法进行地标距离估计。我们考虑到，移动节点携带一台配备校准立体视觉摄像头的低功耗移动设备，拍摄包含战场区域内地标的场景的立体图像，这些地标的位置存储在设备内的离线服务器中。我们创建了一个自定义的地标图像数据集MSTLandmarkv1，包含34个地标类别，以及另一个包含这34个地标示例的地标立体数据集MSTLandmarkSt。

    arXiv:2402.12320v1 Announce Type: cross  Abstract: In this paper, we have proposed a new strategy of using the landmark anchor node instead of a radio-based anchor node to obtain the virtual coordinates (landmarkID, DISTANCE) of moving troops or defense forces that will help in tracking and maneuvering the troops along a safe path within a GPS-denied battlefield environment. The proposed strategy implements landmark recognition using the Yolov5 model and landmark distance estimation using an efficient Stereo Matching Algorithm. We consider that a moving node carrying a low-power mobile device facilitated with a calibrated stereo vision camera that captures stereo images of a scene containing landmarks within the battlefield region whose locations are stored in an offline server residing within the device itself. We created a custom landmark image dataset called MSTLandmarkv1 with 34 landmark classes and another landmark stereo dataset of those 34 landmark instances called MSTLandmarkSt
    
[^15]: 具有公平意识的动态环境响应型在线元学习

    Dynamic Environment Responsive Online Meta-Learning with Fairness Awareness

    [https://arxiv.org/abs/2402.12319](https://arxiv.org/abs/2402.12319)

    引入了FairSAR，一种独特的遗憾度量，以解决动态环境下的公平意识在线学习挑战。

    

    具有公平意识的在线学习框架已经成为连续终身学习背景下的强大工具。在这种情况下，学习者的目标是随着时间推移逐渐获取新任务，同时在引入新任务时确保各个受保护子人口（如种族和性别）之间的统计平等。本文引入了一种独特的遗憾度量FairSAR，以应对不断发展的环境中的公平意识在线学习挑战。

    arXiv:2402.12319v1 Announce Type: cross  Abstract: The fairness-aware online learning framework has emerged as a potent tool within the context of continuous lifelong learning. In this scenario, the learner's objective is to progressively acquire new tasks as they arrive over time, while also guaranteeing statistical parity among various protected sub-populations, such as race and gender, when it comes to the newly introduced tasks. A significant limitation of current approaches lies in their heavy reliance on the i.i.d (independent and identically distributed) assumption concerning data, leading to a static regret analysis of the framework. Nevertheless, it's crucial to note that achieving low static regret does not necessarily translate to strong performance in dynamic environments characterized by tasks sampled from diverse distributions. In this paper, to tackle the fairness-aware online learning challenge in evolving settings, we introduce a unique regret measure, FairSAR, by inco
    
[^16]: 多视角一致学习用于异构传感器融合

    Multi-View Conformal Learning for Heterogeneous Sensor Fusion

    [https://arxiv.org/abs/2402.12307](https://arxiv.org/abs/2402.12307)

    我们提出了用于异构传感器融合的多视角一致模型，并引入了基于集合交集的多视角半一致模型。

    

    能够评估机器学习模型中个别预测的置信度对于决策至关重要，尤其是在诸如医疗诊断、安全和无人车等关键应用中。在过去几年，复杂的预测模型在解决困难任务方面取得了巨大成功，每天都有新方法被提出。而大多数机器学习模型的新发展主要集中在提高整体性能上，对个别预测的可信度考量较少，甚至在传感器融合的背景下更少。为此，我们构建并测试了用于异构传感器融合的多视角和单视角一致模型。由于基于一致性预测框架，我们的模型提供了理论上的边际置信保证。我们还提出了基于集合交集的多视角半一致模型。

    arXiv:2402.12307v1 Announce Type: cross  Abstract: Being able to assess the confidence of individual predictions in machine learning models is crucial for decision making scenarios. Specially, in critical applications such as medical diagnosis, security, and unmanned vehicles, to name a few. In the last years, complex predictive models have had great success in solving hard tasks and new methods are being proposed every day. While the majority of new developments in machine learning models focus on improving the overall performance, less effort is put on assessing the trustworthiness of individual predictions, and even to a lesser extent, in the context of sensor fusion. To this end, we build and test multi-view and single-view conformal models for heterogeneous sensor fusion. Our models provide theoretical marginal confidence guarantees since they are based on the conformal prediction framework. We also propose a multi-view semi-conformal model based on sets intersection. Through comp
    
[^17]: 谱聚类中特征向量的渐近高斯波动

    Asymptotic Gaussian Fluctuations of Eigenvectors in Spectral Clustering

    [https://arxiv.org/abs/2402.12302](https://arxiv.org/abs/2402.12302)

    提出的研究揭示了谱聚类中特征向量的渐近高斯波动现象，为精确预测谱聚类的分类性能提供了重要依据。

    

    谱聚类的性能依赖于相似矩阵的特征向量的条目波动，该波动直到现在仍未得到描述。本文表明，一般尖峰随机矩阵模型的信号+噪声结构被转移到相应的格拉姆核矩阵的特征向量上，并且它们的条目波动在大维度区域呈高斯分布。这种类似于中心极限定理的结果是准确预测谱聚类的分类性能的最后一块缺失的拼图。提出的证明非常通用，仅依赖于噪声的旋转不变性。对合成和真实数据的数值实验表明了这个现象的普适性。

    arXiv:2402.12302v1 Announce Type: cross  Abstract: The performance of spectral clustering relies on the fluctuations of the entries of the eigenvectors of a similarity matrix, which has been left uncharacterized until now. In this letter, it is shown that the signal $+$ noise structure of a general spike random matrix model is transferred to the eigenvectors of the corresponding Gram kernel matrix and the fluctuations of their entries are Gaussian in the large-dimensional regime. This CLT-like result was the last missing piece to precisely predict the classification performance of spectral clustering. The proposed proof is very general and relies solely on the rotational invariance of the noise. Numerical experiments on synthetic and real data illustrate the universality of this phenomenon.
    
[^18]: 正则化去噪：贝叶斯模型和随后的Langevin-within-split Gibbs采样

    Regularization by denoising: Bayesian model and Langevin-within-split Gibbs sampling

    [https://arxiv.org/abs/2402.12292](https://arxiv.org/abs/2402.12292)

    该论文引入了一种贝叶斯框架，通过将数据驱动的正则化策略融入概率框架，提出了一种正则化去噪的方法，并应用于图像反演任务中，在成像中推动了贝叶斯推断。

    

    本文介绍了一种贝叶斯框架，通过推导出一个概率化的相对应于正则化去噪（RED）范式的方法实现对图像反演。此外，它实现了一个蒙特卡洛算法，专门设计用于从所得的后验分布中采样，基于渐近精确数据增广（AXDA）。所提出的算法是一个嵌入了一个Langevin蒙特卡洛步骤的拆分Gibbs采样（SGS）的近似实例。所提出的方法应用于常见的成像任务，如去模糊、修补和超分辨率，通过大量的数值实验展示了其有效性。这些贡献通过在概率框架内利用数据驱动的正则化策略，促进了成像中的贝叶斯推断。

    arXiv:2402.12292v1 Announce Type: cross  Abstract: This paper introduces a Bayesian framework for image inversion by deriving a probabilistic counterpart to the regularization-by-denoising (RED) paradigm. It additionally implements a Monte Carlo algorithm specifically tailored for sampling from the resulting posterior distribution, based on an asymptotically exact data augmentation (AXDA). The proposed algorithm is an approximate instance of split Gibbs sampling (SGS) which embeds one Langevin Monte Carlo step. The proposed method is applied to common imaging tasks such as deblurring, inpainting and super-resolution, demonstrating its efficacy through extensive numerical experiments. These contributions advance Bayesian inference in imaging by leveraging data-driven regularization strategies within a probabilistic framework.
    
[^19]: 为无监督环境设计优化极小化遗憾

    Refining Minimax Regret for Unsupervised Environment Design

    [https://arxiv.org/abs/2402.12284](https://arxiv.org/abs/2402.12284)

    介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化，能够克服极小化遗憾策略在遗憾上界时学习停滞的限制。

    

    在无监督环境设计中，强化学习代理通过对对手最大化某个目标生成的环境配置（关卡）进行训练。遗憾是一种常用的目标，理论上导致具有良好鲁棒性保证的极小化遗憾（MMR）策略；特别是，代理的最大遗憾是有界的。然而，一旦代理在所有关卡上达到了这个遗憾上界，对手将只会对无法进一步减少遗憾的关卡进行采样。尽管在这些最大化遗憾的关卡之外可能存在性能改进空间，但学习停滞。在这项工作中，我们介绍了贝叶斯级别完美的MMR（BLP），它是极小化遗憾目标的精确化。我们正式证明，解决这个目标将导致MMR策略的子集，并且BLP策略在所有关卡上都与完美贝叶斯策略一致行事。

    arXiv:2402.12284v1 Announce Type: cross  Abstract: In unsupervised environment design, reinforcement learning agents are trained on environment configurations (levels) generated by an adversary that maximises some objective. Regret is a commonly used objective that theoretically results in a minimax regret (MMR) policy with desirable robustness guarantees; in particular, the agent's maximum regret is bounded. However, once the agent reaches this regret bound on all levels, the adversary will only sample levels where regret cannot be further reduced. Although there are possible performance improvements to be made outside of these regret-maximising levels, learning stagnates. In this work, we introduce Bayesian level-perfect MMR (BLP), a refinement of the minimax regret objective that overcomes this limitation. We formally show that solving for this objective results in a subset of MMR policies, and that BLP policies act consistently with a Perfect Bayesian policy over all levels. We fur
    
[^20]: 跨异构云和高性能计算资源的安全联邦学习——以LLaMA 2的联邦微调为例

    Secure Federated Learning Across Heterogeneous Cloud and High-Performance Computing Resources -- A Case Study on Federated Fine-tuning of LLaMA 2

    [https://arxiv.org/abs/2402.12271](https://arxiv.org/abs/2402.12271)

    本文介绍了一种跨异构云和高性能计算资源的安全联邦学习框架，利用Globus Compute和亚马逊云服务，实现了端到端的隐私保护，文中以LLaMA 27B模型的联邦微调为例。

    

    联邦学习使多个数据所有者能够协作训练健壮的机器学习模型，而无需转移大型或敏感的本地数据集，只需共享本地训练模型的参数。本文详细阐述了我们的高级隐私保护联邦学习（APPFL）框架的设计，通过利用Globus Compute（一种分布式函数即服务平台）和亚马逊云服务，在云计算设施和高性能计算资源之间简化端到端安全可靠的联邦学习实验。我们进一步演示了在多个云资源和超级计算机上使用APPFL来微调LLaMA 27B模型的用例。

    arXiv:2402.12271v1 Announce Type: cross  Abstract: Federated learning enables multiple data owners to collaboratively train robust machine learning models without transferring large or sensitive local datasets by only sharing the parameters of the locally trained models. In this paper, we elaborate on the design of our Advanced Privacy-Preserving Federated Learning (APPFL) framework, which streamlines end-to-end secure and reliable federated learning experiments across cloud computing facilities and high-performance computing resources by leveraging Globus Compute, a distributed function as a service platform, and Amazon Web Services. We further demonstrate the use case of APPFL in fine-tuning a LLaMA 2 7B model using several cloud resources and supercomputers.
    
[^21]: 利用部分掩码融合的Gromov-Wasserstein匹配进行任意大小图的端对端监督预测

    End-to-end Supervised Prediction of Arbitrary-size Graphs with Partially-Masked Fused Gromov-Wasserstein Matching

    [https://arxiv.org/abs/2402.12269](https://arxiv.org/abs/2402.12269)

    提出了利用部分掩码融合的Gromov-Wasserstein匹配进行任意大小图的端对端监督预测方法，并展示了其在不同任务上相比竞争者更高的效率和多功能性。

    

    我们提出了一种新颖的基于端到端深度学习的监督图预测（SGP）方法。我们引入了一种原始的基于最优输运（OT）的损失，部分掩码融合的Gromov-Wasserstein损失（PM-FGW），可以直接利用图表示，比如邻接和特征矩阵。PM-FGW具有SGP的所有理想属性：节点排列不变性，可微分性，通过比较它们的填充表示以及它们的掩码向量处理不同大小的图。此外，我们提出了一个灵活的基于transformer的架构，可以轻松适应不同类型的输入数据。在实验部分，三个不同的任务，一个新颖且具有挑战性的合成数据集（image2graph）和两个真实任务，图像到地图和指纹到分子 - 展示了该方法相比竞争者的效率和多功能性。

    arXiv:2402.12269v1 Announce Type: new  Abstract: We present a novel end-to-end deep learning-based approach for Supervised Graph Prediction (SGP). We introduce an original Optimal Transport (OT)-based loss, the Partially-Masked Fused Gromov-Wasserstein loss (PM-FGW), that allows to directly leverage graph representations such as adjacency and feature matrices. PM-FGW exhibits all the desirable properties for SGP: it is node permutation invariant, sub-differentiable and handles graphs of different sizes by comparing their padded representations as well as their masking vectors. Moreover, we present a flexible transformer-based architecture that easily adapts to different types of input data. In the experimental section, three different tasks, a novel and challenging synthetic dataset (image2graph) and two real-world tasks, image2map and fingerprint2molecule - showcase the efficiency and versatility of the approach compared to competitors.
    
[^22]: 论基于蒸馏的联邦学习在拜占庭环境下的弹性

    On the Byzantine-Resilience of Distillation-Based Federated Learning

    [https://arxiv.org/abs/2402.12265](https://arxiv.org/abs/2402.12265)

    基于蒸馏的联邦学习在拜占庭环境下表现出极强的弹性，介绍了两种新的拜占庭攻击，并提出了一种增强拜占庭弹性的新方法。

    

    由于在隐私、非独立同分布数据和通信成本方面的优势，使用知识蒸馏（KD）的联邦学习（FL）算法受到越来越多的关注。本文研究了这些方法在拜占庭环境中的性能，展示了基于KD的FL算法相当具有弹性，并分析了拜占庭客户端如何影响学习过程相对于联邦平均算法。根据这些见解，我们介绍了两种新的拜占庭攻击，并证明它们对先前的拜占庭弹性方法是有效的。此外，我们提出了FilterExp，一种旨在增强拜占庭弹性的新方法。

    arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
    
[^23]: 使用LoRA集成在精调LLMs中的不确定性量化

    Uncertainty quantification in fine-tuned LLMs using LoRA ensembles

    [https://arxiv.org/abs/2402.12264](https://arxiv.org/abs/2402.12264)

    使用LoRA集成在精调LLMs中提出了一种原则性不确定性量化方法，通过对不同数据域的低秩适应集成分析，推测了模型对特定架构难以学习的数据领域的信号。

    

    精调大型语言模型可以提高特定任务的性能，尽管对于精调模型学到了什么、遗忘了什么以及如何信任其预测仍然缺乏一个一般的理解。我们提出了使用计算效率高的低秩适应集成对精调LLMs进行基于后验逼近的原则性不确定性量化。我们使用基于Mistral-7b的低秩适应集成分析了三个常见的多项选择数据集，并对其在精调过程中和之后对不同目标领域的感知复杂性和模型效能进行了定量和定性的结论。具体而言，基于数值实验支持，我们对那些对于给定架构难以学习的数据领域的熵不确定性度量提出了假设。

    arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
    
[^24]: 面向门控循环单元的定制混合精度低于8位量化方案的研究，使用遗传算法

    Towards a tailored mixed-precision sub-8bit quantization scheme for Gated Recurrent Units using Genetic Algorithms

    [https://arxiv.org/abs/2402.12263](https://arxiv.org/abs/2402.12263)

    提出了面向门控循环单元的定制混合精度低于8位量化方案，使用遗传算法优化模型尺寸和准确性，在四个不同顺序任务上展示混合精度解决方案优于同质精度解决方案，实现了模型尺寸缩减25%至55%同时保持准确性。

    

    虽然深度神经网络模型压缩技术近年来取得了进展，但在超低功耗嵌入式设备上部署这些模型仍然具有挑战性。特别是针对门控循环单元（GRU）的量化方案很难调整，因为它们依赖于内部状态，无法充分从低于8位的量化中获益。在本研究中，我们提出了一种模块化整数量化方案，其中每个运算符的位宽可以独立选择。然后，我们采用遗传算法（GA）来探索可能位宽的庞大搜索空间，同时优化模型尺寸和准确性。我们在四个不同的顺序任务上评估了我们的方法，并表明混合精度解决方案在Pareto效率方面超过同质精度解决方案。在我们的结果中，我们实现了模型尺寸在25%至55%之间的缩减，同时保持了与t相当的准确性。

    arXiv:2402.12263v1 Announce Type: new  Abstract: Despite the recent advances in model compression techniques for deep neural networks, deploying such models on ultra-low-power embedded devices still proves challenging. In particular, quantization schemes for Gated Recurrent Units (GRU) are difficult to tune due to their dependence on an internal state, preventing them from fully benefiting from sub-8bit quantization. In this work, we propose a modular integer quantization scheme for GRUs where the bit width of each operator can be selected independently. We then employ Genetic Algorithms (GA) to explore the vast search space of possible bit widths, simultaneously optimising for model size and accuracy. We evaluate our methods on four different sequential tasks and demonstrate that mixed-precision solutions exceed homogeneous-precision ones in terms of Pareto efficiency. In our results, we achieve a model size reduction between 25% and 55% while maintaining an accuracy comparable with t
    
[^25]: 非正交时效最优信息传播在车联网中的应用：一种元多目标强化学习方法

    Non-orthogonal Age-Optimal Information Dissemination in Vehicular Networks: A Meta Multi-Objective Reinforcement Learning Approach

    [https://arxiv.org/abs/2402.12260](https://arxiv.org/abs/2402.12260)

    该研究提出了一种元多目标强化学习方法，用于在车联网中进行非正交时效最优信息传播，以减少信息时效和传输功耗。

    

    本文考虑在车联网中减少信息时效（AoI）和传输功耗，一个道路边缘单元（RSU）及时向车辆提供一组物理过程的更新。我们考虑了基于RSU的叠加消息传输和车辆上的连续干扰消除的非正交多模式信息传播。所述问题是一个多目标混合整数非线性规划问题，因此，获得帕累托最优前沿非常具有挑战性。首先，我们利用加权和方法将多目标问题分解为一组对应于每个预定义目标偏好权重的多个单目标子问题。然后，我们开发了一个混合深度Q网络（DQN）-深度确定性策略梯度（DDPG）模型来解决每个与预定义目标偏好权重相对应的优化子问题。

    arXiv:2402.12260v1 Announce Type: new  Abstract: This paper considers minimizing the age-of-information (AoI) and transmit power consumption in a vehicular network, where a roadside unit (RSU) provides timely updates about a set of physical processes to vehicles. We consider non-orthogonal multi-modal information dissemination, which is based on superposed message transmission from RSU and successive interference cancellation (SIC) at vehicles. The formulated problem is a multi-objective mixed-integer nonlinear programming problem; thus, a Pareto-optimal front is very challenging to obtain. First, we leverage the weighted-sum approach to decompose the multi-objective problem into a set of multiple single-objective sub-problems corresponding to each predefined objective preference weight. Then, we develop a hybrid deep Q-network (DQN)-deep deterministic policy gradient (DDPG) model to solve each optimization sub-problem respective to predefined objective-preference weight. The DQN optim
    
[^26]: 使用分类扩散模型生成合成位置轨迹

    Synthetic location trajectory generation using categorical diffusion models

    [https://arxiv.org/abs/2402.12242](https://arxiv.org/abs/2402.12242)

    使用连续扩散过程和映射方法，提出了一种使用分类扩散模型生成合成位置轨迹的方法。

    

    概率扩散模型（DPMs）已迅速发展成为模拟合成数据的主要生成模型之一，例如用于计算机视觉、音频、自然语言处理或生物分子生成。本文提出使用DPMs来生成合成个体位置轨迹（ILTs），ILTs是代表个体访问的物理位置的变量序列，对于移动研究至关重要，有助于理解人群的移动行为并最终为政治决策提供信息。我们将ILTs表示为多维分类随机变量，并建议使用连续DPM来建模它们的联合分布，首先在连续无约束空间中应用扩散过程，然后将连续变量映射到离散空间。我们证明了我们的模型可以通过条件比较合成出逼真的ILPs。

    arXiv:2402.12242v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have rapidly evolved to be one of the predominant generative models for the simulation of synthetic data, for instance, for computer vision, audio, natural language processing, or biomolecule generation. Here, we propose using DPMs for the generation of synthetic individual location trajectories (ILTs) which are sequences of variables representing physical locations visited by individuals. ILTs are of major importance in mobility research to understand the mobility behavior of populations and to ultimately inform political decision-making. We represent ILTs as multi-dimensional categorical random variables and propose to model their joint distribution using a continuous DPM by first applying the diffusion process in a continuous unconstrained space and then mapping the continuous variables into a discrete space. We demonstrate that our model can synthesize realistic ILPs by comparing conditionally an
    
[^27]: 递归神经网络的梯度下降收敛性：非渐近性分析

    Convergence of Gradient Descent for Recurrent Neural Networks: A Nonasymptotic Analysis

    [https://arxiv.org/abs/2402.12241](https://arxiv.org/abs/2402.12241)

    该论文分析了在动态系统中利用梯度下降进行监督学习的递归神经网络的性能，并证明在不需要海量过参数化的情况下，梯度下降可以达到最优性。

    

    我们分析在监督学习设置下利用梯度下降训练的递归神经网络在动态系统中的表现，并证明梯度下降可以在\emph{不}需要海量过参数化的情况下达到最优性。我们进行了深入的非渐近性分析，(i)利用序列长度$T$、样本大小$n$和环境维度$d$给出了网络大小$m$和迭代复杂度$\tau$的尖锐界限，(ii)确定了动态系统中长期依赖对收敛和网络宽度界限的显着影响，这些界限由激活函数的Lipschitz连续性决定的截止点来表征。值得注意的是，这一分析揭示了一个妥善初始化的递归神经网络在$n$个样本的情况下，可以通过网络大小$m$仅对数地随$n$扩展就达到最优性。这与以前的工作形成鲜明对比，前者需要高阶多项式分布。

    arXiv:2402.12241v1 Announce Type: new  Abstract: We analyze recurrent neural networks trained with gradient descent in the supervised learning setting for dynamical systems, and prove that gradient descent can achieve optimality \emph{without} massive overparameterization. Our in-depth nonasymptotic analysis (i) provides sharp bounds on the network size $m$ and iteration complexity $\tau$ in terms of the sequence length $T$, sample size $n$ and ambient dimension $d$, and (ii) identifies the significant impact of long-term dependencies in the dynamical system on the convergence and network width bounds characterized by a cutoff point that depends on the Lipschitz continuity of the activation function. Remarkably, this analysis reveals that an appropriately-initialized recurrent neural network trained with $n$ samples can achieve optimality with a network size $m$ that scales only logarithmically with $n$. This sharply contrasts with the prior works that require high-order polynomial dep
    
[^28]: BEARS 让神经符号模型意识到它们的推理捷径

    BEARS Make Neuro-Symbolic Models Aware of their Reasoning Shortcuts

    [https://arxiv.org/abs/2402.12240](https://arxiv.org/abs/2402.12240)

    BEARS是一种集成技术，可以让神经符号模型意识到它们学习的概念的语义模糊性，帮助用户识别和怀疑低质量概念。

    

    Neuro-Symbolic (NeSy)预测器符合符号知识-编码，例如安全约束，可能受到推理捷径（RSs）的影响：它们通过利用非预期的语义来学习与符号知识一致的概念。 RSs损害了可靠性和泛化，并且正如我们在本文中所展示的，它们与NeSy模型对预测概念过于自信有关。不幸的是，唯一可靠的缓解策略需要对概念进行昂贵的密集监督。我们提出的方法不是试图完全避免RSs，而是要确保NeSy模型意识到它们学习的概念的语义模糊性，从而使用户能够识别和怀疑低质量的概念。从三个简单的设计要求开始，我们得出bears（BE Aware of Reasoning Shortcuts），这是一种集成技术，可以校准模型的概念级信心，而不会损害预测准确性。

    arXiv:2402.12240v1 Announce Type: cross  Abstract: Neuro-Symbolic (NeSy) predictors that conform to symbolic knowledge - encoding, e.g., safety constraints - can be affected by Reasoning Shortcuts (RSs): They learn concepts consistent with the symbolic knowledge by exploiting unintended semantics. RSs compromise reliability and generalization and, as we show in this paper, they are linked to NeSy models being overconfident about the predicted concepts. Unfortunately, the only trustworthy mitigation strategy requires collecting costly dense supervision over the concepts. Rather than attempting to avoid RSs altogether, we propose to ensure NeSy models are aware of the semantic ambiguity of the concepts they learn, thus enabling their users to identify and distrust low-quality concepts. Starting from three simple desiderata, we derive bears (BE Aware of Reasoning Shortcuts), an ensembling technique that calibrates the model's concept-level confidence without compromising prediction accura
    
[^29]: 学习在内容审核中推迟：人工智能与人类协同作用

    Learning to Defer in Content Moderation: The Human-AI Interplay

    [https://arxiv.org/abs/2402.12237](https://arxiv.org/abs/2402.12237)

    本文提出了一个模型，捕捉内容审核中人工智能的相互作用。

    

    成功的在线平台内容审核依赖于人工智能协同方法。本文介绍了一个模型，捕捉内容审核中人工智能的相互作用。算法观察到即将发布的帖子的背景信息，做出分类和准入决策，并安排帖子进行人工审核。

    arXiv:2402.12237v1 Announce Type: cross  Abstract: Successful content moderation in online platforms relies on a human-AI collaboration approach. A typical heuristic estimates the expected harmfulness of a post and uses fixed thresholds to decide whether to remove it and whether to send it for human review. This disregards the prediction uncertainty, the time-varying element of human review capacity and post arrivals, and the selective sampling in the dataset (humans only review posts filtered by the admission algorithm).   In this paper, we introduce a model to capture the human-AI interplay in content moderation. The algorithm observes contextual information for incoming posts, makes classification and admission decisions, and schedules posts for human review. Only admitted posts receive human reviews on their harmfulness. These reviews help educate the machine-learning algorithms but are delayed due to congestion in the human review system. The classical learning-theoretic way to ca
    
[^30]: 最小权限学习的基本限制

    The Fundamental Limits of Least-Privilege Learning

    [https://arxiv.org/abs/2402.12235](https://arxiv.org/abs/2402.12235)

    最小权限学习存在一个基本的权衡，即表示对于给定任务的实用性和其泄漏到任务外属性之间存在无法避免的权衡。

    

    最少权限学习的承诺是找到对于学习任务有用的特征表示，但同时防止推断与该任务无关的任何敏感信息，这一点非常吸引人。然而，到目前为止，这个概念只是以非正式的方式陈述。因此，我们仍然不清楚我们是否以及如何实现这个目标。在这项工作中，我们首次为机器学习的最小权限原则提供了形式化，并描述了其可行性。我们证明了在表示对于给定任务的实用性和其泄漏到预期任务之外的属性之间存在基本权衡：不可能学习到对于预期任务具有高实用性的表示，同时又防止推断除任务标签本身之外的任何属性。这种权衡是无论使用何种技术来学习产生这些表示的特征映射都是成立的。我们经验性地验证了这一点。

    arXiv:2402.12235v1 Announce Type: new  Abstract: The promise of least-privilege learning -- to find feature representations that are useful for a learning task but prevent inference of any sensitive information unrelated to this task -- is highly appealing. However, so far this concept has only been stated informally. It thus remains an open question whether and how we can achieve this goal. In this work, we provide the first formalisation of the least-privilege principle for machine learning and characterise its feasibility. We prove that there is a fundamental trade-off between a representation's utility for a given task and its leakage beyond the intended task: it is not possible to learn representations that have high utility for the intended task but, at the same time prevent inference of any attribute other than the task label itself. This trade-off holds regardless of the technique used to learn the feature mappings that produce these representations. We empirically validate thi
    
[^31]: 将 Kernel KMeans 聚类拆分用于端到端无监督决策树

    Kernel KMeans clustering splits for end-to-end unsupervised decision trees

    [https://arxiv.org/abs/2402.12232](https://arxiv.org/abs/2402.12232)

    提出了一种新颖的端到端训练的无监督二叉树用于聚类，称为Kauri，通过贪婪最大化 kernel KMeans 目标来执行，无需定义质心，并在多个数据集上展示其性能优于其他方法。

    

    树是获取对相对较小数据集进行可解释预测的便利模型。虽然有很多关于监督学习中端到端构建这种树的提议，但在没有标签的情况下学习用于聚类的树仍然是一个未解决的挑战。大多数作品主要集中于使用树来解释另一个聚类算法的结果，我们在这里提出了一种新颖的端到端训练的无监督二叉树用于聚类：Kauri。该方法通过贪婪最大化 kernel KMeans 目标来执行，而无需定义质心。我们在多个数据集上将此模型与最近的无监督树进行比较，并展示当使用线性核时，Kauri 的性能相同。对于其他内核，Kauri 在许多情况下表现优于内核 KMeans 和 CART 决策树的串联。

    arXiv:2402.12232v1 Announce Type: cross  Abstract: Trees are convenient models for obtaining explainable predictions on relatively small datasets. Although there are many proposals for the end-to-end construction of such trees in supervised learning, learning a tree end-to-end for clustering without labels remains an open challenge. As most works focus on interpreting with trees the result of another clustering algorithm, we present here a novel end-to-end trained unsupervised binary tree for clustering: Kauri. This method performs a greedy maximisation of the kernel KMeans objective without requiring the definition of centroids. We compare this model on multiple datasets with recent unsupervised trees and show that Kauri performs identically when using a linear kernel. For other kernels, Kauri often outperforms the concatenation of kernel KMeans and a CART decision tree.
    
[^32]: 扩散回火改善概率积分器对普通微分方程参数估计的效果

    Diffusion Tempering Improves Parameter Estimation with Probabilistic Integrators for Ordinary Differential Equations

    [https://arxiv.org/abs/2402.12231](https://arxiv.org/abs/2402.12231)

    扩散回火是一种新颖的正则化技术，可改善概率数值方法在普通微分方程中的参数优化收敛性，实现对复杂动态系统中参数的可靠估计

    

    普通微分方程（ODEs）被广泛应用于描述科学中的动态系统，但确定解释实验测量结果的参数是具有挑战性的。我们提出了扩散回火这一新的正则化技术，它针对ODEs中的概率数值方法，改善了梯度优化参数估计的收敛性。通过迭代减少概率积分器的一个噪声参数，所提出的方法更可靠地收敛到真实参数。我们证明了我们的方法对于不同复杂性的动态系统是有效的，并展示它对于具有实际相关参数数量的Hodgkin-Huxley模型获得可靠的参数估计。

    arXiv:2402.12231v1 Announce Type: new  Abstract: Ordinary differential equations (ODEs) are widely used to describe dynamical systems in science, but identifying parameters that explain experimental measurements is challenging. In particular, although ODEs are differentiable and would allow for gradient-based parameter optimization, the nonlinear dynamics of ODEs often lead to many local minima and extreme sensitivity to initial conditions. We therefore propose diffusion tempering, a novel regularization technique for probabilistic numerical methods which improves convergence of gradient-based parameter optimization in ODEs. By iteratively reducing a noise parameter of the probabilistic integrator, the proposed method converges more reliably to the true parameters. We demonstrate that our method is effective for dynamical systems of different complexity and show that it obtains reliable parameter estimates for a Hodgkin-Huxley model with a practically relevant number of parameters.
    
[^33]: AnyGPT：统一的多模式离散序列建模语言模型

    AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling

    [https://arxiv.org/abs/2402.12226](https://arxiv.org/abs/2402.12226)

    AnyGPT是一个统一的多模态语言模型，通过离散表示实现各种模态的统一处理，能够在不改变大型语言模型架构或训练方式的情况下稳定训练，为新模态的无缝整合提供了可能。

    

    我们介绍了 AnyGPT，这是一个任意多模式语言模型，利用离散表示统一处理各种模态，包括语音、文本、图像和音乐。AnyGPT 可以稳定训练，无需对当前大型语言模型（LLM）架构或训练范式进行任何改动。相反，它仅依赖于数据级预处理，促进了新模态的无缝集成到LLM中，类似于新语言的整合。我们构建了一个多模式文本中心的数据集，用于多模式对齐预训练。利用生成模型，我们合成了第一个大规模任意多模式指令数据集。它包括108k个多轮对话示例，精细地交织各种模态，从而使模型能够处理多模态输入和输出的任意组合。实验结果表明，AnyGPT能够促进...

    arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
    
[^34]: 使用基于覆盖引导的强化学习对LLM基础变异进行JavaScript引擎模糊测试

    CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation

    [https://arxiv.org/abs/2402.12222](https://arxiv.org/abs/2402.12222)

    CovRL使用强化学习结合大型语言模型和覆盖反馈，通过构建加权覆盖映射并应用于基于LLM的变异器，以提升对JavaScript引擎的模糊测试效果

    

    模糊测试是一种有效的发现错误的技术，但在像需要精确语法输入的JavaScript引擎这样的复杂系统中很难应用。最近，研究人员采用语言模型对模糊测试中的上下文感知变异进行了改进以解决这一问题。然而，现有技术在利用覆盖引导进行模糊测试方面存在局限性，通常以黑盒方式执行。本文提出了一种名为CovRL（基于覆盖引导的强化学习）的新技术，将大型语言模型（LLMs）与从覆盖反馈中得到的强化学习相结合。我们的模糊器CovRL-Fuzz通过利用词频-逆文档频率（TF-IDF）方法将覆盖反馈直接集成到LLM中，构建加权覆盖映射。该映射在计算模糊测试奖励中起着关键作用，然后通过强化学习应用于基于LLM的变异器。通过这种方法，CovRL-Fuzz实现了...

    arXiv:2402.12222v1 Announce Type: cross  Abstract: Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input. Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem. However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a black-box manner. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with reinforcement learning from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the g
    
[^35]: 贝叶斯参数高效微调以克服灾难性遗忘

    Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting

    [https://arxiv.org/abs/2402.12220](https://arxiv.org/abs/2402.12220)

    这项研究展示了如何利用贝叶斯学习技术应用于参数高效微调，以防止灾难性遗忘，实现了预训练知识的保留，并在语言建模和语音合成任务中取得成功。

    

    虽然最初是被文本转语音合成模型的自适应所激发，但我们认为更通用的参数高效微调（PEFT）是进行这种自适应的适当框架。然而，灾难性遗忘仍然是PEFT面临的问题，它损害了预训练模型固有的能力。我们证明现有的贝叶斯学习技术可以应用于PEFT，以防止灾难性遗忘，只要能够可微地计算微调层的参数转换。在一系列关于语言建模和语音合成任务的基础性实验中，我们利用建立的拉普拉斯近似，包括对角线和Kronecker分解方法，来正则化PEFT与低秩适应（LoRA）并比较它们在保留预训练知识方面的性能。我们的结果表明，我们的方法可以克服灾难性遗忘，而不会降低微调性能。

    arXiv:2402.12220v1 Announce Type: cross  Abstract: Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning perfo
    
[^36]: 重新格式化对齐

    Reformatted Alignment

    [https://arxiv.org/abs/2402.12219](https://arxiv.org/abs/2402.12219)

    本文提出了一种名为ReAlign的简单有效方法，通过重新格式化指导数据的响应，显著提升了大型语言模型（LLMs）与人类价值观的对齐能力。

    

    优化微调数据对于将大型语言模型（LLMs）与人类价值观对齐至关重要。当前改善数据质量的方法要么耗时费力，要么容易受到LLM幻觉引起的事实错误影响。本文探讨提升现有指导数据质量以更好地与人类价值观对齐的方法，引入了一种名为ReAlign的简单有效方法，它将指导数据的响应重新格式化为更符合预先建立标准和编译证据的格式。该方法最小化了人类注释、幻觉和扩展困难，与现有对齐技术正交。实验结果表明，ReAlign显著提升了LLMs的整体对齐能力、数学推理、事实性和可读性。令人鼓舞的是，在不引入任何额外数据或先进训练技术的情况下，仅通过重新格式化响应，LLaMA-2-13

    arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
    
[^37]: 字典学习改进了机械解释中的无补丁电路发现：以奥赛罗-GPT为案例研究

    Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT

    [https://arxiv.org/abs/2402.12201](https://arxiv.org/abs/2402.12201)

    字典学习技术在机械解释中攻克叠加，并提取更易理解的特征。该论文提出了一种电路发现框架，用于连接大量字典特征，相比于激活补丁，该框架受越界分布影响较小，并在渐近复杂度方面更有效。

    

    稀疏字典学习是一种在机械解释中快速发展的技术，用于攻击叠加并从模型激活中提取更易理解的特征。本文基于提取的更单义特征进一步提出一个问题：我们如何识别连接大量字典特征的电路？我们提出了一个电路发现框架，替代了激活补丁。我们的框架在越界分布方面遭受较小，并在渐近复杂度方面证明更有效。我们框架中的基本单元是从所有模块中写入残余流的字典特征，包括嵌入、注意力输出和MLP输出。从任何对数、字典特征或注意力分数开始，我们成功追踪到所有令牌的较低级别字典特征，并计算它们对这些更具可解释性和局部模型行为的贡献。

    arXiv:2402.12201v1 Announce Type: new  Abstract: Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a
    
[^38]: 零-shot 可见语言模型用于仇恨模因检测：我们已经到达目标了吗？

    Zero shot VLMs for hate meme detection: Are we there yet?

    [https://arxiv.org/abs/2402.12198](https://arxiv.org/abs/2402.12198)

    本研究探讨了零-shot分类在处理复杂任务如恶意模因检测中的有效性

    

    社交媒体上的多媒体内容正在迅速发展，其中模因作为一种独特形式变得日益重要。不幸的是，一些恶意用户利用模因针对个人或易受攻击的社区，因此有必要识别和解决此类恶意模因。已经进行了大量研究来解决这个问题，通过开发仇恨模因检测模型。然而，传统的机器学习/深度学习模型的一个显著局限性是需要带标签的数据集才能进行准确分类。最近，研究界见证了几种可见语言模型的出现，在各种任务中展现出卓越的性能。在这项研究中，我们旨在调查这些可见语言模型在处理诸如仇恨模因检测等复杂任务中的有效性。我们使用各种提示设置来专注于对恶意/有害模因的零-shot 分类。通过我们的分析，我们o

    arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
    
[^39]: 基于AI的精准肿瘤学：基于多组学数据的个性化反事实治疗建议的机器学习框架

    Towards AI-Based Precision Oncology: A Machine Learning Framework for Personalized Counterfactual Treatment Suggestions based on Multi-Omics Data

    [https://arxiv.org/abs/2402.12190](https://arxiv.org/abs/2402.12190)

    提出了一种基于机器学习的框架，用于个性化反事实癌症治疗建议，集成了多种多组学技术的专家，可提供优越性能和决策解释。

    

    AI驱动的精准肿瘤学具有通过利用AI模型分析复杂患者特征与对应治疗结果之间互动的潜力，有望重塑癌症治疗。新技术平台促进了及时获取多模态肿瘤生物学数据，如单细胞多组学数据，使得这种数据的质量和数量可用于数据驱动的改进临床决策。本文提出了一个模块化的机器学习框架，旨在基于训练有关多种多组学技术的机器学习专家组成的集成来进行个性化反事实癌症治疗建议。这些专门的反事实专家根据技术不断聚合为性能更优越的专家，可提供决策的置信度和解释。

    arXiv:2402.12190v1 Announce Type: cross  Abstract: AI-driven precision oncology has the transformative potential to reshape cancer treatment by leveraging the power of AI models to analyze the interaction between complex patient characteristics and their corresponding treatment outcomes. New technological platforms have facilitated the timely acquisition of multimodal data on tumor biology at an unprecedented resolution, such as single-cell multi-omics data, making this quality and quantity of data available for data-driven improved clinical decision-making. In this work, we propose a modular machine learning framework designed for personalized counterfactual cancer treatment suggestions based on an ensemble of machine learning experts trained on diverse multi-omics technologies. These specialized counterfactual experts per technology are consistently aggregated into a more powerful expert with superior performance and can provide both confidence and an explanation of its decision. The
    
[^40]: 通过使用伪标签成员资格进行微调来增强训练数据曝光

    Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships

    [https://arxiv.org/abs/2402.12189](https://arxiv.org/abs/2402.12189)

    攻击者通过对预训练LM进行对抗微调，以放大原始训练数据的曝光，采用伪标签和机器生成概率来加强LM对预训练数据的保留。

    

    神经语言模型(LMs)由于数据记忆而容易受到训练数据提取攻击的影响。本文介绍了一种新的攻击场景，在这种场景中，攻击者对预训练LM进行对抗微调，以放大原始训练数据的曝光。该策略不同于先前的研究，其目的是加强LM对其预训练数据集的保留。为了实现这一目标，攻击者需要收集与预训练数据密切相关的生成文本。然而，如果没有实际数据集的知识，衡量生成文本中预训练数据的量是具有挑战性的。为了解决这个问题，我们提出利用目标LM的机器生成概率所表示的成员近似值为这些生成文本使用伪标签。随后，我们微调LM以支持那些更有可能源自预训练数据的生成文本，根据其成员资格。

    arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb
    
[^41]: 对抗特征对齐：通过对抗训练在深度学习中平衡鲁棒性和准确性

    Adversarial Feature Alignment: Balancing Robustness and Accuracy in Deep Learning via Adversarial Training

    [https://arxiv.org/abs/2402.12187](https://arxiv.org/abs/2402.12187)

    通过对抗训练，在深度学习中提出了一种对抗特征对齐的方法，以平衡鲁棒性和准确性，研究发现特征空间内部的不对齐经常导致 misclassification，这一方法旨在解决这些问题。

    

    深度学习模型在准确性方面不断取得进展，但仍然容易受到对抗性攻击的影响，这经常导致对抗性样本被错误分类。对抗训练被用来通过增强对这些攻击的鲁棒性来减轻这个问题。然而，这种方法通常会降低模型在干净的非对抗性样本上的标准准确性。深度学习模型需要在安全性方面平衡鲁棒性和准确性的必要性是显而易见的，但实现这种平衡仍然具有挑战性，其潜在原因尚未明确阐述。本文提出了一种新颖的对抗训练方法，称为对抗特征对齐（AFA），以解决这些问题。我们的研究揭示了一个有趣的见解：特征空间中的不对齐经常导致错误分类，无论样本是良性还是对抗性。AFA通过采用一种新颖的优化算法来减轻这种风险。

    arXiv:2402.12187v1 Announce Type: cross  Abstract: Deep learning models continue to advance in accuracy, yet they remain vulnerable to adversarial attacks, which often lead to the misclassification of adversarial examples. Adversarial training is used to mitigate this problem by increasing robustness against these attacks. However, this approach typically reduces a model's standard accuracy on clean, non-adversarial samples. The necessity for deep learning models to balance both robustness and accuracy for security is obvious, but achieving this balance remains challenging, and the underlying reasons are yet to be clarified. This paper proposes a novel adversarial training method called Adversarial Feature Alignment (AFA), to address these problems. Our research unveils an intriguing insight: misalignment within the feature space often leads to misclassification, regardless of whether the samples are benign or adversarial. AFA mitigates this risk by employing a novel optimization algor
    
[^42]: 重新审视深度强化学习中的数据增强

    Revisiting Data Augmentation in Deep Reinforcement Learning

    [https://arxiv.org/abs/2402.12181](https://arxiv.org/abs/2402.12181)

    重新审视深度强化学习中的数据增强，分析不同方法的影响，提出了如何更加原则地利用数据增强的建议。

    

    最近在基于图像的深度强化学习(DRL)中提出了各种数据增强技术。尽管它们在实证中证明了数据增强对于提高样本效率或泛化的有效性，但并不总是清楚哪种技术应该被优先选择。为了解决这个问题，我们分析了现有方法以更好地理解它们并揭示它们之间的联系。值得注意的是，通过表达这些方法的Q-targets和经验演员/评论家损失的方差，我们可以分析它们不同组成部分的影响并进行比较。我们进一步提出了一个关于如何选择不同的数据增强转换来计算目标Q值可能会影响这些方法的解释。这项分析提出了如何更加原则地利用数据增强的建议。此外，我们还包括了一个称为切线prop的正则化项。

    arXiv:2402.12181v1 Announce Type: cross  Abstract: Various data augmentation techniques have been recently proposed in image-based deep reinforcement learning (DRL). Although they empirically demonstrate the effectiveness of data augmentation for improving sample efficiency or generalization, which technique should be preferred is not always clear. To tackle this question, we analyze existing methods to better understand them and to uncover how they are connected. Notably, by expressing the variance of the Q-targets and that of the empirical actor/critic losses of these methods, we can analyze the effects of their different components and compare them. We furthermore formulate an explanation about how these methods may be affected by choosing different data augmentation transformations in calculating the target Q-values. This analysis suggests recommendations on how to exploit data augmentation in a more principled way. In addition, we include a regularization term called tangent prop,
    
[^43]: Mafin: 用模型增强微调来增强黑盒嵌入

    Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning

    [https://arxiv.org/abs/2402.12177](https://arxiv.org/abs/2402.12177)

    Mafin通过引入模型增强微调的方法，能够在只有黑盒嵌入可用的情况下显著提高性能。

    

    检索增强生成（RAG）已经成为缓解大型语言模型（LLMs）中幻觉的有效解决方案。RAG中的检索阶段通常涉及预训练的嵌入模型，将查询和段落转换为向量以捕获它们的语义。然而，当应用于特定领域知识时，标准的预训练嵌入模型可能表现出次优性能，需要进行微调。本文解决了仅能从黑盒模型获取嵌入的情况。我们引入了模型增强微调（Mafin）--一种通过用可训练的嵌入模型增强黑盒嵌入模型来进行微调的新方法。我们的结果表明，Mafin仅需要训练一个小的增强模型就可以显著提高黑盒嵌入的性能。我们验证了我们的方法在有标签和无标签数据集上的有效性。

    arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
    
[^44]: 使用GOMEA学习离散贝叶斯网络

    Learning Discretized Bayesian Networks with GOMEA

    [https://arxiv.org/abs/2402.12175](https://arxiv.org/abs/2402.12175)

    本文通过扩展基于Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA)的最先进结构学习方法，实现了联合学习变量离散化。

    

    贝叶斯网络模型描述了不确定性下随机变量之间的关系，可用于预测事件和结果的可能性，并结合观察到的证据。从可解释人工智能（XAI）的角度来看，这种模型具有紧凑性，而且捕获的关系可以直接由领域专家检查。然而，在实践中，数据往往是实值的。除非可以假设正态性，否则通常需要进行离散化。然而，最佳离散化取决于模型化的变量之间的关系。这增加了从数据中学习贝叶斯网络的复杂性。基于Gene-pool Optimal Mixing Evolutionary Algorithm（GOMEA）的现有最先进结构学习方法，我们在本工作中扩展了这一方法，以共同学习变量离散化。

    arXiv:2402.12175v1 Announce Type: new  Abstract: Bayesian networks model relationships between random variables under uncertainty and can be used to predict the likelihood of events and outcomes while incorporating observed evidence. From an eXplainable AI (XAI) perspective, such models are interesting as they tend to be compact. Moreover, captured relations can be directly inspected by domain experts. In practice, data is often real-valued. Unless assumptions of normality can be made, discretization is often required. The optimal discretization, however, depends on the relations modelled between the variables. This complicates learning Bayesian networks from data. For this reason, most literature focuses on learning conditional dependencies between sets of variables, called structure learning. In this work, we extend an existing state-of-the-art structure learning approach based on the Gene-pool Optimal Mixing Evolutionary Algorithm (GOMEA) to jointly learn variable discretizations. T
    
[^45]: 赋予预训练图模型具有可证明的公平性

    Endowing Pre-trained Graph Models with Provable Fairness

    [https://arxiv.org/abs/2402.12161](https://arxiv.org/abs/2402.12161)

    提出了一种新的适配器调优框架，赋予预训练图模型具有可证明的公平性

    

    预训练图模型（PGMs）旨在捕捉可转移的固有结构属性，并将其应用于不同的下游任务。类似于预训练语言模型，PGMs也会继承人类社会中的偏见，导致在下游应用中出现歧视行为。现有公平方法的去偏见过程通常与GNNs的参数优化相结合。然而，不同的下游任务在现实中可能与不同的敏感属性相关联，直接采用现有方法改善PGMs的公平性是不灵活且低效的。此外，大多数方法缺乏理论保证，即对模型预测公平性的可证明下限，这直接提供了实际场景下的保证。为了克服这些限制，我们提出了一种新的适配器调优框架，赋予预训练\textbf{图}模型具有\textbf{可证明}的\textbf{公}平\textbf{性}（称为

    arXiv:2402.12161v1 Announce Type: cross  Abstract: Pre-trained graph models (PGMs) aim to capture transferable inherent structural properties and apply them to different downstream tasks. Similar to pre-trained language models, PGMs also inherit biases from human society, resulting in discriminatory behavior in downstream applications. The debiasing process of existing fair methods is generally coupled with parameter optimization of GNNs. However, different downstream tasks may be associated with different sensitive attributes in reality, directly employing existing methods to improve the fairness of PGMs is inflexible and inefficient. Moreover, most of them lack a theoretical guarantee, i.e., provable lower bounds on the fairness of model predictions, which directly provides assurance in a practical scenario. To overcome these limitations, we propose a novel adapter-tuning framework that endows pre-trained \textbf{Graph} models with \textbf{P}rovable f\textbf{A}i\textbf{R}ness (called
    
[^46]: MLFEF: 采用经验公式的机器学习融合模型探索竞技体育中的动量

    MLFEF: Machine Learning Fusion Model with Empirical Formula to Explore the Momentum in Competitive Sports

    [https://arxiv.org/abs/2402.12149](https://arxiv.org/abs/2402.12149)

    本文旨在定义和量化动量，为网球比赛的实时分析提供基础，通过建立基于数据驱动和基于经验公式的模型，采用多种机器学习算法进行融合，以探索竞技体育中的动量。

    

    网球非常受欢迎，教练和运动员对除了技能之外的因素，如动量，也感到好奇。本文将尝试定义和量化动量，为网球比赛的实时分析提供基础。基于近年来网球大满贯男子单打比赛的数据，我们构建了两个模型，一个是基于数据驱动的模型，另一个是基于经验公式的模型。对于数据驱动模型，我们首先找到了大量的公开数据，包括过去五年网球比赛的公开数据和球员的个人信息数据。然后对数据进行预处理和特征工程处理，并建立了一个SVM、Random Forrest算法和XGBoost的融合模型。对于机制分析模型，基于许多网球运动员和爱好者的建议，选择了重要特征，使用滑动窗口算法计算权重，和不同的met

    arXiv:2402.12149v1 Announce Type: new  Abstract: Tennis is so popular that coaches and players are curious about factors other than skill, such as momentum. This article will try to define and quantify momentum, providing a basis for real-time analysis of tennis matches. Based on the tennis Grand Slam men's singles match data in recent years, we built two models, one is to build a model based on data-driven, and the other is to build a model based on empirical formulas. For the data-driven model, we first found a large amount of public data including public data on tennis matches in the past five years and personal information data of players. Then the data is preprocessed, and feature engineered, and a fusion model of SVM, Random Forrest algorithm and XGBoost was established. For the mechanism analysis model, important features were selected based on the suggestions of many tennis players and enthusiasts, the sliding window algorithm was used to calculate the weight, and different met
    
[^47]: Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement

    [https://arxiv.org/abs/2402.12146](https://arxiv.org/abs/2402.12146)

    Meta Ranking方法通过比较目标查询-响应对与参考查询-响应对来使较不具备能力的语言模型有效地评估单个响应的可靠性。

    

    尽管大型语言模型（LLMs）在广泛任务中展现强大性能，但仍面临幻觉等可靠性挑战。先前的研究表明，像GPT-4这样高能力的LLMs在评估单个响应的可靠性方面是有效的，而较不具备能力的LLMs通常被调整来评估对相同查询的响应的相对可靠性。为了使较不具备能力的LLMs有效地评估单个响应的可靠性，我们提出了一种名为$\textit{Meta}$ $\textit{Ranking}$（MR）的新方法。与先前直接评估响应的方法不同，我们通过将目标查询-响应对与参考查询-响应对进行比较来实现判断。我们发现在推理任务的LLM响应的错误检测中，MR表现出显著的有效性，即使在没有微调的情况下，较不具备能力的LLMs也能胜过强基线。我们进一步证明MR可以被用

    arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
    
[^48]: 联邦贝叶斯网络集成

    Federated Bayesian Network Ensembles

    [https://arxiv.org/abs/2402.12142](https://arxiv.org/abs/2402.12142)

    联邦贝叶斯网络集成(FBNE)在联邦设置下表现优异，相较于本地模型和VertiBayes训练的模型，在保持类似性能的情况下提供了显著的训练速度提升。

    

    arXiv:2402.12142v1 公布类型: 新的 摘要: 联邦学习允许我们在去中心化数据上运行机器学习算法，当由于隐私问题而不允许数据共享时。基于集成的学习通过训练多个(弱)分类器，然后对其输出进行聚合。联邦集成是应用于联邦设置的集成，其中集成中的每个分类器都在一个数据位置上进行训练。 在本文中，我们探讨了在一系列实验中使用联邦贝叶斯网络集成(FBNE)的情况，并将其性能与本地训练模型和使用VertiBayes进行训练的模型进行比较，后者是一种用于从去中心化数据训练贝叶斯网络的联邦学习算法。我们的研究结果表明，FBNE的表现优于本地模型，在大多数情况下在维持类似性能的情况下比VertiBayes提供了显著的训练速度增加，另外还具有其他优势。我们展示了FBNE是联邦环境中潜在有用的工具。

    arXiv:2402.12142v1 Announce Type: new  Abstract: Federated learning allows us to run machine learning algorithms on decentralized data when data sharing is not permitted due to privacy concerns. Ensemble-based learning works by training multiple (weak) classifiers whose output is aggregated. Federated ensembles are ensembles applied to a federated setting, where each classifier in the ensemble is trained on one data location.   In this article, we explore the use of federated ensembles of Bayesian networks (FBNE) in a range of experiments and compare their performance with locally trained models and models trained with VertiBayes, a federated learning algorithm to train Bayesian networks from decentralized data. Our results show that FBNE outperforms local models and provides a significant increase in training speed compared with VertiBayes while maintaining a similar performance in most settings, among other advantages. We show that FBNE is a potentially useful tool within the federat
    
[^49]: 分子生成和优化，用于高效香氛创造

    Molecule Generation and Optimization for Efficient Fragrance Creation

    [https://arxiv.org/abs/2402.12134](https://arxiv.org/abs/2402.12134)

    通过机器学习中心的方法，建立了一个将香水分子结构与人类嗅觉感知相连的混合模型，利用AI驱动的分子生成器和热力学模型，优化溶剂和分子组合，最终通过数学优化问题最小化新旧嗅觉体验之间的差异。

    

    这项研究介绍了一种以机器学习为中心的方法，通过实验量化香水感知来复制嗅觉体验。关键贡献包括一个将香水分子结构与人类嗅觉感知相连的混合模型。该模型包括一个利用图形和生成式神经网络的人工智能驱动的分子生成器，对气味强度进行量化和预测，以及为期望香氛优化的最佳溶剂和分子组合。此外，基于热力学的模型建立了嗅觉感知与液相浓度之间的联系。该方法利用迁移学习，基于蒸汽压和香味音符选择最合适的分子。最终，建立了一个数学优化问题，以最小化新旧嗅觉体验之间的差异。该方法通过重现两种独特香水的嗅觉体验来进行验证。

    arXiv:2402.12134v1 Announce Type: cross  Abstract: This research introduces a Machine Learning-centric approach to replicate olfactory experiences, validated through experimental quantification of perfume perception. Key contributions encompass a hybrid model connecting perfume molecular structure to human olfactory perception. This model includes an AI-driven molecule generator (utilizing Graph and Generative Neural Networks), quantification and prediction of odor intensity, and refinery of optimal solvent and molecule combinations for desired fragrances. Additionally, a thermodynamic-based model establishes a link between olfactory perception and liquid-phase concentrations. The methodology employs Transfer Learning and selects the most suitable molecules based on vapor pressure and fragrance notes. Ultimately, a mathematical optimization problem is formulated to minimize discrepancies between new and target olfactory experiences. The methodology is validated by reproducing two disti
    
[^50]: DualView：双重视角下的数据归因

    DualView: Data Attribution from the Dual Perspective

    [https://arxiv.org/abs/2402.12118](https://arxiv.org/abs/2402.12118)

    提出了DualView，一种基于替代建模的后期数据归因方法，具有高效计算和优质评估结果。

    

    本文提出了DualView，这是一种基于替代建模的后期数据归因方法，展示了高计算效率和良好的评估结果。我们专注于神经网络，在与文献相关的适当定量评估策略下评估了我们提出的技术，比较了与相关主要本地数据归因方法的性能。

    arXiv:2402.12118v1 Announce Type: cross  Abstract: Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to comp
    
[^51]: 变分方法与机器学习方法在逆问题中的鲁棒性和探索：概述

    Robustness and Exploration of Variational and Machine Learning Approaches to Inverse Problems: An Overview

    [https://arxiv.org/abs/2402.12072](https://arxiv.org/abs/2402.12072)

    本论文概述了使用变分方法和机器学习解决成像中逆问题的方法，重点在于点估计器对抗性扰动下的鲁棒性以及探索数据一致解子空间以满足特定语义或纹理特性。

    

    本文试图概述使用变分方法和机器学习来解决成像中逆问题的当前方法。重点关注点估计器及其对抗性扰动下的鲁棒性。此外，通过一维示例问题的数值实验结果，展示了不同方法的鲁棒性并在经验上验证了理论保证。该综述的另一个重点是通过明确指导来探索数据一致解的子空间，以满足特定语义或纹理特性。

    arXiv:2402.12072v1 Announce Type: cross  Abstract: This paper attempts to provide an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning. A special focus lies on point estimators and their robustness against adversarial perturbations. In this context results of numerical experiments for a one-dimensional toy problem are provided, showing the robustness of different approaches and empirically verifying theoretical guarantees. Another focus of this review is the exploration of the subspace of data consistent solutions through explicit guidance to satisfy specific semantic or textural properties.
    
[^52]: 可解释的类脑表示提升视觉导航任务中强化学习表现

    Interpretable Brain-Inspired Representations Improve RL Performance on Visual Navigation Tasks

    [https://arxiv.org/abs/2402.12067](https://arxiv.org/abs/2402.12067)

    经由类脑启发的慢特征分析方法生成的可解释视觉数据表示，能够在强化学习中提高导航任务表现。

    

    视觉导航需要一系列能力。其中一个关键能力是代理能够确定其在环境中的位置和朝向。之前的研究通常会假设这些信息已知，或者使用缺乏适当归纳偏差并随时间累积误差的方法。在这项工作中，我们展示了受神经科学研究启发的慢特征分析（SFA）方法如何克服这两个限制，通过生成可解释的视觉数据表示来编码代理的位置和朝向。我们在现代强化学习环境中应用SFA，分析和比较表示，并展示了层级SFA在导航任务上能够胜过其他特征提取方法。

    arXiv:2402.12067v1 Announce Type: new  Abstract: Visual navigation requires a whole range of capabilities. A crucial one of these is the ability of an agent to determine its own location and heading in an environment. Prior works commonly assume this information as given, or use methods which lack a suitable inductive bias and accumulate error over time. In this work, we show how the method of slow feature analysis (SFA), inspired by neuroscience research, overcomes both limitations by generating interpretable representations of visual data that encode location and heading of an agent. We employ SFA in a modern reinforcement learning context, analyse and compare representations and illustrate where hierarchical SFA can outperform other feature extractors on navigation tasks.
    
[^53]: WKVQuant：量化大型语言模型的参数权重和键值缓存以提高性能

    WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More

    [https://arxiv.org/abs/2402.12065](https://arxiv.org/abs/2402.12065)

    该论文提出了WKVQuant，一种专为大型语言模型设计的量化框架，通过量化权重和键值缓存来改善性能。

    

    大型语言模型（LLMs）面临着部署挑战，主要是由于其巨大的内存需求和自回归文本生成过程的计算需求。本文通过关注LLMs的量化来解决这些挑战，量化是一种通过将模型参数和激活转换为低比特整数来减少内存消耗的技术。我们批判性地分析了现有的量化方法，识别出它们在平衡量化LLMs的准确性和效率方面的局限性。为了超越这些局限性，我们提出了WKVQuant，这是一个专为量化LLMs的参数权重和键值（KV）缓存而设计的PTQ框架。具体而言，我们引入了仅考虑过去的量化以改善注意力计算。此外，我们还介绍了二维量化策略来处理KV缓存的分布，以及一种跨块重建正则化方法以帮助模型压缩。

    arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
    
[^54]: 因果平等保护与算法公平性

    Causal Equal Protection as Algorithmic Fairness

    [https://arxiv.org/abs/2402.12062](https://arxiv.org/abs/2402.12062)

    本文提出了一种新的算法公平性原则——平等保护，其关键在于将错误分类的风险均等化，避免了许多对传统分类平等原则的反例。

    

    过去十年，计算机科学和哲学的文献形成了不同的算法公平性标准。其中最受争议的分类平等要求，预测算法的错误分类在被保护特征所指示的群体中以相等频率发生。尽管分类平等具有直观吸引力，但已受到攻击。我们转向一个相关原则，即平等保护，该原则最初是在刑事司法领域发展起来的。平等保护的关键在于将错误分类的风险（将在规定的意义上具体说明）进行均等化，而不是将错误分类的比率均等化。我们展示了平等保护避免了许多对分类平等的反例。

    arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
    
[^55]: 所有语言模型的大小都一样吗？

    All Language Models Large and Small

    [https://arxiv.org/abs/2402.12061](https://arxiv.org/abs/2402.12061)

    LONDI框架可以在需要复杂决策和推理的地方选择性地使用大的语言模型，极大地降低了资源消耗。

    

    许多领先的语言模型（LMs）在训练和执行过程中使用高强度计算资源，这对于降低部署资源成本和更快执行决策任务等方面提出了挑战。我们引入了一种名为语言优化网络分布（LONDI）框架的新型即插即用LM框架。 LONDI学会了在需要进行复杂决策和推理的地方选择性地使用大的LM，而在其他地方使用低资源的LM。 LONDI由两个（离线）策略网络系统、一个LM、一个大的LM（LLM)和一个使用开关控制快速学习何时调用LLM的强化学习模块组成。 然后，我们介绍了一种在LLM调用和资源使用方面保持预算约束的LONDI变体。 从理论上讲，我们证明了LONDI学习激活所需解决任务的LLM的系统状态子集。

    arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
    
[^56]: 具有多对数极小极小遗憾的线性赌博机

    Linear bandits with polylogarithmic minimax regret

    [https://arxiv.org/abs/2402.12042](https://arxiv.org/abs/2402.12042)

    该研究提出了一种新的线性赌博机算法，解决了线性随机赌博机中最小极小遗憾的多对数缩放问题，通过加权最小二乘估计实现对设计矩阵特征值关系的控制，实现了累积遗憾的对数缩放。

    

    我们研究了一种线性随机赌博机的噪声模型，对于该模型，当我们选择越来越接近未知向量的单位球上的动作时，亚高斯噪声参数以线性方式消失。我们针对这个问题引入了一种算法，其在时间长度$T$的情况下呈对数$^3（T）$的最小遗憾缩放，与典型赌博机算法的平方根遗憾缩放形成鲜明对比。我们的策略基于加权最小二乘估计，通过几何论证实现了设计矩阵$V_t$在每个时间步骤$t$处的特征值关系$\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$，这些几何论证与噪声模型无关，并可能具有独立的兴趣。这使我们能够严格控制每个时间步骤的期望遗憾为$O(\frac1{t})$的数量级，从而导致累积遗憾的对数缩放。

    arXiv:2402.12042v1 Announce Type: cross  Abstract: We study a noise model for linear stochastic bandits for which the subgaussian noise parameter vanishes linearly as we select actions on the unit sphere closer and closer to the unknown vector. We introduce an algorithm for this problem that exhibits a minimax regret scaling as $\log^3(T)$ in the time horizon $T$, in stark contrast the square root scaling of this regret for typical bandit algorithms. Our strategy, based on weighted least-squares estimation, achieves the eigenvalue relation $\lambda_{\min} ( V_t ) = \Omega (\sqrt{\lambda_{\max}(V_t ) })$ for the design matrix $V_t$ at each time step $t$ through geometrical arguments that are independent of the noise model and might be of independent interest. This allows us to tightly control the expected regret in each time step to be of the order $O(\frac1{t})$, leading to the logarithmic scaling of the cumulative regret.
    
[^57]: Self-AMPLIFY：通过自我事后解释改进小型语言模型

    Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations

    [https://arxiv.org/abs/2402.12038](https://arxiv.org/abs/2402.12038)

    本研究提出了Self-AMPLIFY方法，通过将事后解释方法应用于小型语言模型（SLMs），自动生成基于原因的解释，以提高它们自身的性能。

    

    在本论文中，我们提出了Self-AMPLIFY方法，该方法通过应用于小型语言模型（SLMs）的事后解释方法自动生成基于原因的解释，从而提高它们自身的性能。Self-AMPLIFY是一个3步骤的方法，用于选择样本、生成理由和构建最终提示以利用上下文学习（ICL）。我们在两个需要推理能力的SLMs和两个数据集上评估了Self-AMPLIFY的性能：这些实验表明Self-AMPLIFY在与竞争对手相比表现出色。Self-AMPLIFY是第一个将事后解释方法应用于SLMs的方法，以生成解释并提高它们自身性能的方法。

    arXiv:2402.12038v1 Announce Type: new  Abstract: Incorporating natural language rationales in the prompt and In-Context Learning (ICL) has led to a significant improvement of Large Language Models (LLMs) performance. However, rationales currently require human-annotation or the use of auxiliary proxy models to target promising samples or generate high-quality rationales. In this work, we propose Self-AMPLIFY to generate automatically rationales from post hoc explanation methods applied to Small Language Models (SLMs) to improve their own performance. Self-AMPLIFY is a 3-step method that targets samples, generates rationales and builds a final prompt to leverage ICL. Self-AMPLIFY performance is evaluated on two SLMs and two datasets requiring reasoning abilities: these experiments show that Self-AMPLIFY achieves good results against competitors. Self-AMPLIFY is the first method to apply post hoc explanation methods to SLM to generate rationales to improve their own performance in a full
    
[^58]: 时间序列的增量式学习: 基准和评估

    Class-incremental Learning for Time Series: Benchmark and Evaluation

    [https://arxiv.org/abs/2402.12035](https://arxiv.org/abs/2402.12035)

    时间序列增量学习问题在图像和语言领域取得了进展，但在时间序列数据方面仍然相对较少研究，本文提出了一个全面的评估和基准测试方法。

    

    现实环境本质上是非平稳的，经常会随时间引入新的类别。这在时间序列分类中尤为常见，比如在医疗保健领域出现新的疾病分类，或者在人类活动识别中添加新的活动。在这种情况下，需要一个学习系统能够有效地吸收新的类别，同时避免对旧类别的灾难性遗忘，这就引发了增量式学习问题。然而，尽管在图像和语言领域取得了令人鼓舞的进展，但针对时间序列数据的增量式学习仍然相对较少研究。现有研究存在实验设计不一致的问题，需要对方法在各种数据集上进行全面评估和基准测试。为此，我们首先概述了时间序列增量学习（TSCIL）问题，突出了其独特挑战，并覆盖了...

    arXiv:2402.12035v1 Announce Type: cross  Abstract: Real-world environments are inherently non-stationary, frequently introducing new classes over time. This is especially common in time series classification, such as the emergence of new disease classification in healthcare or the addition of new activities in human activity recognition. In such cases, a learning system is required to assimilate novel classes effectively while avoiding catastrophic forgetting of the old ones, which gives rise to the Class-incremental Learning (CIL) problem. However, despite the encouraging progress in the image and language domains, CIL for time series data remains relatively understudied. Existing studies suffer from inconsistent experimental designs, necessitating a comprehensive evaluation and benchmarking of methods across a wide range of datasets. To this end, we first present an overview of the Time Series Class-incremental Learning (TSCIL) problem, highlight its unique challenges, and cover the 
    
[^59]: 离策略和在策略策略梯度方法何时能够一致？

    When Do Off-Policy and On-Policy Policy Gradient Methods Align?

    [https://arxiv.org/abs/2402.12034](https://arxiv.org/abs/2402.12034)

    该论文研究了离策略和在策略策略梯度方法之间的差异，并首次提出了减小该差距的条件，同时发现在条件不满足时会产生短板。

    

    策略梯度方法是广泛采用的在连续动作空间中的强化学习算法。这些方法在许多应用领域取得成功，然而由于其臭名昭著的样本效率低，它们的使用仍然局限于可以快速准确模拟的问题。改进样本效率的常见方法是修改它们的目标函数，使之能够从离策略样本中计算而无需重要性采样。一个成熟的离策略目标就是游荡目标。本文研究了游荡目标与传统在策略目标之间的差异，我们称之为在离之间的差距。我们提供了第一个理论分析，展示了减少在离差距的条件，同时建立了当这些条件未被满足时出现的缺陷的经验证据。

    arXiv:2402.12034v1 Announce Type: cross  Abstract: Policy gradient methods are widely adopted reinforcement learning algorithms for tasks with continuous action spaces. These methods succeeded in many application domains, however, because of their notorious sample inefficiency their use remains limited to problems where fast and accurate simulations are available. A common way to improve sample efficiency is to modify their objective function to be computable from off-policy samples without importance sampling. A well-established off-policy objective is the excursion objective. This work studies the difference between the excursion objective and the traditional on-policy objective, which we refer to as the on-off gap. We provide the first theoretical analysis showing conditions to reduce the on-off gap while establishing empirical evidence of shortfalls arising when these conditions are not met.
    
[^60]: 将大型语言模型压缩用于文本属性图学习

    Distilling Large Language Models for Text-Attributed Graph Learning

    [https://arxiv.org/abs/2402.12022](https://arxiv.org/abs/2402.12022)

    本研究旨在将大型语言模型和图模型的优势相结合，通过将LLMs的能力压缩到 TAG 学习的本地图模型中，解决它们之间的固有差距。

    

    文本属性图（TAGs）是连接的文本文档图。图模型可以有效学习TAGs，但它们的训练严重依赖于人工标注的标签，在许多应用中这些标签很少或甚至不可用。大型语言模型（LLMs）最近在少样本和零样本TAG学习中展示了显著能力，但它们存在可伸缩性、成本和隐私问题。因此，在这项工作中，我们专注于通过将LLMs的能力传授给TAG学习中的本地图模型，从而协同LLMs和图模型的互补优势。

    arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed 
    
[^61]: 基于Sarsa和Q-learning的异质智能目标跟踪索引策略

    An Index Policy Based on Sarsa and Q-learning for Heterogeneous Smart Target Tracking

    [https://arxiv.org/abs/2402.12015](https://arxiv.org/abs/2402.12015)

    基于Sarsa和Q-learning的智能目标跟踪索引策略，解决了雷达调度中短期性能和未来机动性的平衡挑战。

    

    在解决多个智能目标在主动和被动雷达网络中的非远见性雷达调度问题时，需要考虑短期增强跟踪性能和未来主动跟踪中目标机动性的概率。在调度主动和被动雷达的波束资源时获得长期跟踪性能带来挑战。为了解决这一挑战，我们将这个问题建模为由平行不平静老虎机过程组成的马尔可夫决策过程。每个老虎机过程与一个智能目标相关联，其估计状态根据不同的动作（目标是否被跟踪）而遵循不同的离散动态模型而演变。离散状态由动态模式定义。该问题呈现出维度诅咒，其中最优解通常难以处理。我们通过著名的不平静多臂老虎机启发式方法来求解。

    arXiv:2402.12015v1 Announce Type: cross  Abstract: In solving the non-myopic radar scheduling for multiple smart target tracking within an active and passive radar network, we need to consider both short-term enhanced tracking performance and a higher probability of target maneuvering in the future with active tracking. Acquiring the long-term tracking performance while scheduling the beam resources of active and passive radars poses a challenge. To address this challenge, we model this problem as a Markov decision process consisting of parallel restless bandit processes. Each bandit process is associated with a smart target, of which the estimation state evolves according to different discrete dynamic models for different actions - whether or not the target is being tracked. The discrete state is defined by the dynamic mode. The problem exhibits the curse of dimensionality, where optimal solutions are in general intractable. We resort to heuristics through the famous restless multi-ar
    
[^62]: 使用精英样本训练绿色人工智能模型

    Training Green AI Models Using Elite Samples

    [https://arxiv.org/abs/2402.12010](https://arxiv.org/abs/2402.12010)

    该论文提出了一个基于进化的采样框架，旨在识别精英训练样本，比较与传统方法的模型性能和能效优势，探讨其对可持续模型训练的可能性。

    

    AI模型训练量的大幅增加具有重要的环境影响，这需要更节能高效和可持续的人工智能实践。数据中心方法展现出训练节能人工智能模型的巨大潜力，而实例选择方法展示了使用最小化训练集训练人工智能模型的能力且性能下降可以忽略不计。本文提出了一个基于进化的采样框架，旨在识别针对数据集和模型的精英训练样本，比较模型性能和节能效益与典型模型训练实践的差异，并研究这一框架对促进可持续模型训练实践的可行性。

    arXiv:2402.12010v1 Announce Type: cross  Abstract: The substantial increase in AI model training has considerable environmental implications, mandating more energy-efficient and sustainable AI practices. On the one hand, data-centric approaches show great potential towards training energy-efficient AI models. On the other hand, instance selection methods demonstrate the capability of training AI models with minimised training sets and negligible performance degradation. Despite the growing interest in both topics, the impact of data-centric training set selection on energy efficiency remains to date unexplored. This paper presents an evolutionary-based sampling framework aimed at (i) identifying elite training samples tailored for datasets and model pairs, (ii) comparing model performance and energy efficiency gains against typical model training practice, and (iii) investigating the feasibility of this framework for fostering sustainable model training practices. To evaluate the propo
    
[^63]: 群集度量对无关特征的敏感度

    Cluster Metric Sensitivity to Irrelevant Features

    [https://arxiv.org/abs/2402.12008](https://arxiv.org/abs/2402.12008)

    本文研究群集性能对添加到基线数据集中的嘈杂不相关变量的敏感度。

    

    聚类算法在数据分析中被广泛使用，用于数据探索和发现。技术进步导致数据在容量、维度和复杂性方面不断增长。这为数据分析提供了巨大机会，因为数据可以用于许多不同目的的询问。然而，这也带来了挑战，比如在给定任务中识别相关特征。在监督任务中，可以利用各种方法优化任务目标（例如分类准确性）的输入特征。在无监督问题中，这些工具并不readily available，部分原因是无法定量地衡量无标签任务中特征的相关性。本文研究了群集性能对嘈杂的不相关变量进行迭代添加到具有明确定义群集的基线数据集的敏感度。我们展示了不同类型的无关变量如何影响结果。

    arXiv:2402.12008v1 Announce Type: cross  Abstract: Clustering algorithms are used extensively in data analysis for data exploration and discovery. Technological advancements lead to continually growth of data in terms of volume, dimensionality and complexity. This provides great opportunities in data analytics as the data can be interrogated for many different purposes. This however leads challenges, such as identification of relevant features for a given task. In supervised tasks, one can utilise a number of methods to optimise the input features for the task objective (e.g. classification accuracy). In unsupervised problems, such tools are not readily available, in part due to an inability to quantify feature relevance in unlabeled tasks. In this paper, we investigate the sensitivity of clustering performance noisy uncorrelated variables iteratively added to baseline datasets with well defined clusters. We show how different types of irrelevant variables can impact the outcome of a c
    
[^64]: 回忆那一年发生的事件？评估大型语言模型中的时间信息和推理能力

    Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models

    [https://arxiv.org/abs/2402.11997](https://arxiv.org/abs/2402.11997)

    大型语言模型在处理时间信息和推理方面存在显著限制，闭源模型可能暗示了不确定性认识与错误回应之间的权衡。

    

    大型语言模型（LLMs）越来越普遍，但它们对于推理和保留时间信息的能力仍然有限。这限制了它们在理解事件的顺序性对关键的现实场景中的应用。本文在一个新颖的大规模时间数据集\textbf{TempUN}上对最先进的模型进行实验，揭示了时间保留和推理能力方面的显著限制。有趣的是，闭源模型更频繁地显示出知识差距，可能暗示了不确定性认识和错误回应之间的权衡。此外，探索各种微调方法并没有带来主要性能改进。相关数据集和代码可在以下网址获得（https://github.com/lingoiitgn/TempUN）。

    arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
    
[^65]: ISCUTE：使用文本嵌入进行电缆实例分割

    ISCUTE: Instance Segmentation of Cables Using Text Embedding

    [https://arxiv.org/abs/2402.11996](https://arxiv.org/abs/2402.11996)

    提出了一种基于文本提示的DLO实例分割技术，结合了CLIPSeg模型的文本条件语义分割能力和Segment Anything Model的零样本泛化能力，有效解决了传统方法在感知可变形直线对象如电线、电缆和柔性管道方面的挑战，性能超越了目前的技术水平，同时引入了一个新的DLO特定数据集。

    

    在机器人技术和自动化领域，传统的对象识别和实例分割方法在感知类似电线、电缆和柔性管道等可变形直线对象（DLOs）方面面临着巨大挑战。这一挑战主要源于缺乏形状、颜色和纹理等明显属性，这需要量身定制的解决方案来实现精确识别。在这项工作中，我们提出了一种基于基于文本提示的、用户友好的DLO实例分割技术。具体来说，我们的方法结合了CLIPSeg模型的文本条件语义分割能力和Segment Anything Model (SAM) 的零样本泛化能力。我们展示了我们的方法在DLO实例分割方面超越了最先进技术，实现了$91.21\%$的平均交并比（mIoU）。我们还介绍了一个丰富多样的用于实例分割的DLO特定数据集。

    arXiv:2402.11996v1 Announce Type: cross  Abstract: In the field of robotics and automation, conventional object recognition and instance segmentation methods face a formidable challenge when it comes to perceiving Deformable Linear Objects (DLOs) like wires, cables, and flexible tubes. This challenge arises primarily from the lack of distinct attributes such as shape, color, and texture, which calls for tailored solutions to achieve precise identification. In this work, we propose a foundation model-based DLO instance segmentation technique that is text-promptable and user-friendly. Specifically, our approach combines the text-conditioned semantic segmentation capabilities of CLIPSeg model with the zero-shot generalization capabilities of Segment Anything Model (SAM). We show that our method exceeds SOTA performance on DLO instance segmentation, achieving a mIoU of $91.21\%$. We also introduce a rich and diverse DLO-specific dataset for instance segmentation.
    
[^66]: 二值化神经网络的网络反漂

    Network Inversion of Binarised Neural Nets

    [https://arxiv.org/abs/2402.11995](https://arxiv.org/abs/2402.11995)

    本文提出了一种新颖的方法，通过将训练后的二值化神经网络编码，实现了对其进行反漂的目的

    

    虽然神经网络的部署在各种应用中产生了令人印象深刻的结果，但它们的可解释性和理解依然是一个关键挑战。网络反漂是一种旨在从模型学习的内部表示中重建输入空间的技术，在揭示神经网络中输入到输出映射的黑匣子特性方面发挥着关键作用。在安全关键场景中，模型输出可能影响重要决策，相应输入空间的完整性至关重要，必须消除任何多余的“垃圾”以确保网络的可信度。二值神经网络（BNNs）以二值权重和激活为特征，提供了计算效率和减少内存需求，使它们适用于资源受限环境。本文介绍了一种通过将训练后的BNN编码来反漂的新方法

    arXiv:2402.11995v1 Announce Type: new  Abstract: While the deployment of neural networks, yielding impressive results, becomes more prevalent in various applications, their interpretability and understanding remain a critical challenge. Network inversion, a technique that aims to reconstruct the input space from the model's learned internal representations, plays a pivotal role in unraveling the black-box nature of input to output mappings in neural networks. In safety-critical scenarios, where model outputs may influence pivotal decisions, the integrity of the corresponding input space is paramount, necessitating the elimination of any extraneous "garbage" to ensure the trustworthiness of the network. Binarised Neural Networks (BNNs), characterized by binary weights and activations, offer computational efficiency and reduced memory requirements, making them suitable for resource-constrained environments. This paper introduces a novel approach to invert a trained BNN by encoding it int
    
[^67]: 隐私保护的低秩适应Latent扩散模型

    Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models

    [https://arxiv.org/abs/2402.11989](https://arxiv.org/abs/2402.11989)

    提出了隐私保护的低秩适应解决方案PrivateLoRA，通过最小化适应损失和代理攻击模型的MI增益来抵御成员推断攻击。

    

    低秩适应（LoRA）是一种有效的策略，用于通过最小化适应损失，自训练数据集中适应Latent扩散模型（LDM）以生成特定对象。然而，通过LoRA适应的LDM容易受到成员推断（MI）攻击的影响，这种攻击可以判断特定数据点是否属于私人训练数据集，因此面临严重的隐私泄露风险。为了抵御MI攻击，我们首次提出了一个直接的解决方案：隐私保护的LoRA（PrivateLoRA）。PrivateLoRA被构建为一个最小最大优化问题，其中通过最大化MI增益来训练代理攻击模型，而LDM则通过最小化适应损失和代理攻击模型的MI增益之和来进行调整。然而，我们在实践中发现PrivateLoRA存在稳定性优化问题，即由于梯度规模的大幅波动而妨碍适应。

    arXiv:2402.11989v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, w
    
[^68]: 具有可微可感知区域兴趣提议网络和软区域感知的胸部X射线弱监督目标检测

    Weakly Supervised Object Detection in Chest X-Rays with Differentiable ROI Proposal Networks and Soft ROI Pooling

    [https://arxiv.org/abs/2402.11985](https://arxiv.org/abs/2402.11985)

    提出了一种新的弱监督目标检测方法，利用可微的ROI提议网络和软ROI池化，在胸部X射线图像中胜过现有方法

    

    弱监督目标检测（WSup-OD）提高了图像分类算法的有用性和可解释性，无需额外监督。然而，在这一任务上多实例学习在自然图像中取得的成功并不能很好地转化为医学图像，因为它们的对象（即病理）具有非常不同的特征。本文提出了弱监督区域兴趣提议网络（WSRPN），一种使用专门的区域感知（ROI-attention）模块动态生成边界框提议的新方法。WSRPN与经典的骨干-头部分类算法很好地集成在一起，只需图像标签监督即可进行端到端训练。我们实验证明，我们的新方法在胸部X光图像中的疾病定位这一具有挑战性的任务中胜过了现有方法。

    arXiv:2402.11985v1 Announce Type: cross  Abstract: Weakly supervised object detection (WSup-OD) increases the usefulness and interpretability of image classification algorithms without requiring additional supervision. The successes of multiple instance learning in this task for natural images, however, do not translate well to medical images due to the very different characteristics of their objects (i.e. pathologies). In this work, we propose Weakly Supervised ROI Proposal Networks (WSRPN), a new method for generating bounding box proposals on the fly using a specialized region of interest-attention (ROI-attention) module. WSRPN integrates well with classic backbone-head classification algorithms and is end-to-end trainable with only image-label supervision. We experimentally demonstrate that our new method outperforms existing methods in the challenging task of disease localization in chest X-ray images. Code: https://github.com/philip-mueller/wsrpn
    
[^69]: 基于Hebbian学习的正交投影用于持续学习脉冲神经网络

    Hebbian Learning based Orthogonal Projection for Continual Learning of Spiking Neural Networks

    [https://arxiv.org/abs/2402.11984](https://arxiv.org/abs/2402.11984)

    该研究提出了一种基于横向连接和Hebbian学习的神经操作新方法，能够通过投影保护知识

    

    使用脉冲神经网络进行神经形态计算，对于能效高的人工智能应用具有潜力。然而，与终身不断学习不同，神经网络模型容易出现灾难性遗忘。神经操作如何解决这一问题是人工智能和神经科学中的重要问题。本研究开发了一种基于横向连接和Hebbian学习的神经操作新方法，可以通过投影保护知识。

    arXiv:2402.11984v1 Announce Type: cross  Abstract: Neuromorphic computing with spiking neural networks is promising for energy-efficient artificial intelligence (AI) applications. However, different from humans who continually learn different tasks in a lifetime, neural network models suffer from catastrophic forgetting. How could neuronal operations solve this problem is an important question for AI and neuroscience. Many previous studies draw inspiration from observed neuroscience phenomena and propose episodic replay or synaptic metaplasticity, but they are not guaranteed to explicitly preserve knowledge for neuron populations. Other works focus on machine learning methods with more mathematical grounding, e.g., orthogonal projection on high dimensional spaces, but there is no neural correspondence for neuromorphic computing. In this work, we develop a new method with neuronal operations based on lateral connections and Hebbian learning, which can protect knowledge by projecting act
    
[^70]: 被审查回归的贝叶斯主动学习

    Bayesian Active Learning for Censored Regression

    [https://arxiv.org/abs/2402.11973](https://arxiv.org/abs/2402.11973)

    该论文提出了一种在被审查回归中的贝叶斯主动学习方法($\mathcal{C}$-BALD)，通过推导被审查分布的熵和互信息，优化目标函数，在广泛数据集和模型下表现优异。

    

    贝叶斯主动学习基于信息论方法，专注于最大化新观测提供给模型参数的信息。通常通过最大化贝叶斯主动学习通过分歧（BALD）获得函数来实现。然而，我们强调，在新数据点遭受审查时估计BALD是具有挑战性的，其中只观察到目标的剪辑值。为了解决这个问题，我们推导了被审查分布的熵和互信息，并推导了被审查回归中的主动学习的BALD目标（$\mathcal{C}$-BALD）。我们提出了一种新颖的建模方法来估计$\mathcal{C}$-BALD目标，并将其用于被审查设置中的主动学习。通过一系列广泛的数据集和模型，我们证明$\mathcal{C}$-BALD在被审查回归中优于其他贝叶斯主动学习方法。

    arXiv:2402.11973v1 Announce Type: new  Abstract: Bayesian active learning is based on information theoretical approaches that focus on maximising the information that new observations provide to the model parameters. This is commonly done by maximising the Bayesian Active Learning by Disagreement (BALD) acquisitions function. However, we highlight that it is challenging to estimate BALD when the new data points are subject to censorship, where only clipped values of the targets are observed. To address this, we derive the entropy and the mutual information for censored distributions and derive the BALD objective for active learning in censored regression ($\mathcal{C}$-BALD). We propose a novel modelling approach to estimate the $\mathcal{C}$-BALD objective and use it for active learning in the censored setting. Across a wide range of datasets and models, we demonstrate that $\mathcal{C}$-BALD outperforms other Bayesian active learning methods in censored regression.
    
[^71]: 回归数据集中的不平衡问题

    Imbalance in Regression Datasets

    [https://arxiv.org/abs/2402.11963](https://arxiv.org/abs/2402.11963)

    回归数据集中的不平衡问题一直被忽视，本文通过理论分析和定义，展示了这一问题的重要性，并为未来研究提供了共同基础。

    

    就分类而言，类别不平衡的问题是众所周知的，并且已经得到了广泛研究。本文认为，在回归中存在的不平衡问题同样重要，但迄今为止被忽视：由于数据集目标分布中的欠表示和过多表示，回归器容易退化为朴素模型，系统地忽略不常见的训练数据并在训练期间经常见到的目标上进行过度表示。我们从理论上分析了这个问题，并利用得出的见解制定了对回归中不平衡的首个定义，我们展示这是分类中常用的不平衡度量的泛化。通过这样做，我们希望将关注点转向回归中被忽视的不平衡问题，并为未来研究奠定共同基础。

    arXiv:2402.11963v1 Announce Type: cross  Abstract: For classification, the problem of class imbalance is well known and has been extensively studied. In this paper, we argue that imbalance in regression is an equally important problem which has so far been overlooked: Due to under- and over-representations in a data set's target distribution, regressors are prone to degenerate to naive models, systematically neglecting uncommon training data and over-representing targets seen often during training. We analyse this problem theoretically and use resulting insights to develop a first definition of imbalance in regression, which we show to be a generalisation of the commonly employed imbalance measure in classification. With this, we hope to turn the spotlight on the overlooked problem of imbalance in regression and to provide common ground for future research.
    
[^72]: DB-LLM: 高效LLM的准确双二值化

    DB-LLM: Accurate Dual-Binarization for Efficient LLMs

    [https://arxiv.org/abs/2402.11960](https://arxiv.org/abs/2402.11960)

    本文提出了一种名为DB-LLM的新颖双二值化方法，通过引入灵活双二值化(FDB)来平衡2位宽度的精度优势和二值化的效率优势，从而在提高LLMs的计算效率的同时保持了准确性。

    

    大型语言模型(LLMs)显著推进了自然语言处理领域，然而高昂的内存和计算开销阻碍了它们的实际部署。量化成为改善LLMs计算效率的最有效方法之一。然而，现有的超低比特量化总是导致严重的精度下降。本文通过实证研究缓解了超低比特量化的微观和宏观特性，提出了一种新颖的LLMs双二值化方法，即DB-LLM。对于微观层面，我们考虑了2位宽度的准确性优势和二值化的效率优势，引入了灵活双二值化(FDB)。通过将2位量化权重分为两组独立的二进制数集，FDB确保了表示的准确性并引入了灵活性，利用二值化的高效位操作同时保留了

    arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta
    
[^73]: 通过对抗样本和时间侧信道揭示预训练的CNN模型

    Stealing the Invisible: Unveiling Pre-Trained CNN Models through Adversarial Examples and Timing Side-Channels

    [https://arxiv.org/abs/2402.11953](https://arxiv.org/abs/2402.11953)

    对抗样本的分类模式和时间侧信道的结合可以导致窃取预训练的CNN模型。

    

    机器学习在其众多应用中已成为许多技术系统的重要组成部分。在这一领域的常见做法是使用迁移学习，即调整预先训练的模型架构以适应特定任务。本文介绍了一种基于观察结果的方法，该方法认为对抗图像的分类模式可用作窃取模型的手段。此外，对抗图像的分类结合时间侧信道可以导致模型窃取方法。

    arXiv:2402.11953v1 Announce Type: cross  Abstract: Machine learning, with its myriad applications, has become an integral component of numerous technological systems. A common practice in this domain is the use of transfer learning, where a pre-trained model's architecture, readily available to the public, is fine-tuned to suit specific tasks. As Machine Learning as a Service (MLaaS) platforms increasingly use pre-trained models in their backends, it's crucial to safeguard these architectures and understand their vulnerabilities. In this work, we present an approach based on the observation that the classification patterns of adversarial images can be used as a means to steal the models. Furthermore, the adversarial image classifications in conjunction with timing side channels can lead to a model stealing method. Our approach, designed for typical user-level access in remote MLaaS environments exploits varying misclassifications of adversarial images across different models to fingerp
    
[^74]: 一种将VAE与Transformer结合的新型分子生成模型

    A novel molecule generative model of VAE combined with Transformer

    [https://arxiv.org/abs/2402.11950](https://arxiv.org/abs/2402.11950)

    该研究提出了一种结合VAE与Transformer的模型，通过结构和参数优化，成功处理多样化分子的生成，生成的分子性能与现有模型相媲美，对生成具有未知结构的分子具有更优越的性能。

    

    近年来，深度学习在药物发现中被广泛应用于分子生成。在该领域中，Transformer和VAE被广泛使用作为强大的模型，但由于它们之间的结构和性能不匹配，很少组合使用。本研究通过处理多样化分子的结构和参数优化，提出了将这两种模型结合的模型。所提出的模型在生成分子方面表现出与现有模型相媲美的性能，并在生成具有未知结构的分子方面表现出迄今为止卓越的性能。此外，所提出的模型成功利用了VAE的潜在表示来预测分子性质。消融研究表明，在生成新颖分子方面，VAE比语言模型等其他生成模型具有优势，并且分子可以用约32个维度变量来描述，比现有描述符和模型要小得多。

    arXiv:2402.11950v1 Announce Type: cross  Abstract: Recently, molecule generation using deep learning has been actively investigated in drug discovery. In this field, Transformer and VAE are widely used as powerful models, but they are rarely used in combination due to structural and performance mismatch of them. This study proposes a model that combines these two models through structural and parameter optimization in handling diverse molecules. The proposed model shows comparable performance to existing models in generating molecules, and showed by far superior performance in generating molecules with unseen structures. In addition, the proposed model successfully predicted molecular properties using the latent representation of VAE. Ablation studies suggested the advantage of VAE over other generative models like language model in generating novel molecules, and that the molecules can be described by ~32 dimensional variables, much smaller than existing descriptors and models. This s
    
[^75]: Mini-Hes：一种可并行化的二阶潜在因子分析模型

    Mini-Hes: A Parallelizable Second-order Latent Factor Analysis Model

    [https://arxiv.org/abs/2402.11948](https://arxiv.org/abs/2402.11948)

    Mini-Hes提出了一种新的mini-block对角黑塞无约束优化方法，用于构建二阶潜在因子分析模型，解决了大数据量下二阶算法可行性的挑战。

    

    与大量实体之间的交互在许多与大数据相关的任务中自然是高维且不完整的（HDI）。用户的行为特征隐藏在这些交互中，因此，有效地表示HDI数据是理解用户行为的基本任务。潜在因子分析（LFA）模型已被证明在表示HDI数据方面是有效的。 LFA模型的性能严重依赖于其训练过程，这是一个非凸优化问题。已经证明，在其训练过程中同时包含局部曲率和预处理梯度可以比使用一阶方法构建的LFA模型表现出更优异的性能。然而，随着数据量的增加，二阶算法的可行性面临挑战。为解决这一关键问题，本文提出了一种用于构建LFA模型的mini-block对角黑塞无约束（Mini-Hes）优化方法。

    arXiv:2402.11948v1 Announce Type: cross  Abstract: Interactions among large number of entities is naturally high-dimensional and incomplete (HDI) in many big data related tasks. Behavioral characteristics of users are hidden in these interactions, hence, effective representation of the HDI data is a fundamental task for understanding user behaviors. Latent factor analysis (LFA) model has proven to be effective in representing HDI data. The performance of an LFA model relies heavily on its training process, which is a non-convex optimization. It has been proven that incorporating local curvature and preprocessing gradients during its training process can lead to superior performance compared to LFA models built with first-order family methods. However, with the escalation of data volume, the feasibility of second-order algorithms encounters challenges. To address this pivotal issue, this paper proposes a mini-block diagonal hessian-free (Mini-Hes) optimization for building an LFA model.
    
[^76]: Leaky ReLU对超参数网络的训练和泛化的影响

    The effect of Leaky ReLUs on the training and generalization of overparameterized networks

    [https://arxiv.org/abs/2402.11942](https://arxiv.org/abs/2402.11942)

    Leaky ReLU参数$\alpha=-1$在训练误差和泛化误差界方面是最优的选择。

    

    我们研究了具有各种泄漏修正线性单元（ReLU）函数的超参数神经网络（NNs）的训练和泛化误差。更具体地，我们仔细地对这些NNs的训练误差的收敛速率和泛化误差进行了上界估计，并研究了这些界限对Leaky ReLU参数$\alpha$的依赖性。我们表明$\alpha=-1$，对应于绝对值激活函数，对于训练误差界是最优的。此外，在特定设置中，这也是泛化误差界的最优选择。数值实验在实践中支持了理论引导的实际选择。

    arXiv:2402.11942v1 Announce Type: new  Abstract: We investigate the training and generalization errors of overparameterized neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions. More specifically, we carefully upper bound both the convergence rate of the training error and the generalization error of such NNs and investigate the dependence of these bounds on the Leaky ReLU parameter, $\alpha$. We show that $\alpha =-1$, which corresponds to the absolute value activation function, is optimal for the training error bound. Furthermore, in special settings, it is also optimal for the generalization error bound. Numerical experiments empirically support the practical choices guided by the theory.
    
[^77]: AICAttack：基于注意力优化的对抗性图像字幕攻击

    AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization

    [https://arxiv.org/abs/2402.11940](https://arxiv.org/abs/2402.11940)

    提出了一种新的对抗攻击策略AICAttack，旨在通过微小的图像扰动来攻击图像字幕模型，在黑盒攻击情景下具有良好的效果。

    

    最近深度学习研究取得了在计算机视觉（CV）和自然语言处理（NLP）等许多任务上显著的成就。CV和NLP交叉点上的图像字幕问题中，相关模型对抗攻击的稳健性尚未得到充分研究。本文提出了一种新颖的对抗攻击策略，称为AICAttack（基于注意力的图像字幕攻击），旨在通过对图像进行微小扰动来攻击图像字幕模型。在黑盒攻击环境中运行，我们的算法不需要访问目标模型的架构、参数或梯度信息。我们引入了基于注意力的候选选择机制，可识别最佳像素进行攻击，然后采用差分进化（DE）来扰乱像素的RGB值。通过对基准上的广泛实验，我们证明了AICAttack的有效性。

    arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
    
[^78]: SLADE：通过自监督学习在边缘流中检测动态异常

    SLADE: Detecting Dynamic Anomalies in Edge Streams without Labels via Self-Supervised Learning

    [https://arxiv.org/abs/2402.11933](https://arxiv.org/abs/2402.11933)

    SLADE通过自监督学习在边缘流中迅速检测动态异常，无需依赖标签，主要通过观察节点交互模式的偏差来检测节点状态转变。

    

    为了检测真实世界图中的异常，如社交、电子邮件和金融网络，已经开发了各种方法。在大多数真实世界图随时间增长，自然地表示为边缘流的情况下，我们的目标是：(a)在异常发生时即时检测异常，(b)适应动态变化的状态，(c)处理动态异常标签的稀缺性。在本文中，我们提出了SLADE（边缘流异常检测的自监督学习），用于在边缘流中快速检测动态异常，而不依赖于标签。SLADE通过观察节点在时间上相互作用模式的偏差来检测节点进入异常状态的转变。为此，它训练一个深度神经网络执行两个自监督任务：(a)最小化节点表示中的漂移，(b)从短期生成长期交互模式。

    arXiv:2402.11933v1 Announce Type: new  Abstract: To detect anomalies in real-world graphs, such as social, email, and financial networks, various approaches have been developed. While they typically assume static input graphs, most real-world graphs grow over time, naturally represented as edge streams. In this context, we aim to achieve three goals: (a) instantly detecting anomalies as they occur, (b) adapting to dynamically changing states, and (c) handling the scarcity of dynamic anomaly labels. In this paper, we propose SLADE (Self-supervised Learning for Anomaly Detection in Edge Streams) for rapid detection of dynamic anomalies in edge streams, without relying on labels. SLADE detects the shifts of nodes into abnormal states by observing deviations in their interaction patterns over time. To this end, it trains a deep neural network to perform two self-supervised tasks: (a) minimizing drift in node representations and (b) generating long-term interaction patterns from short-term 
    
[^79]: 通过联合数据深化和预取实现能效边缘学习

    Energy-Efficient Edge Learning via Joint Data Deepening-and-Prefetching

    [https://arxiv.org/abs/2402.11925](https://arxiv.org/abs/2402.11925)

    我们提出了一种名为JD2P的新型离线架构，通过联合数据深化和预取技术，按顺序离线每个数据样本的特征，以减少物联网设备向边缘服务器传输数据时所需的能量消耗。

    

    通过利用物联网设备收集的实时数据对人工智能模型进行实时训练，实现无处不在的人工智能服务的愿景。为了解决能源受限的物联网设备传输高维且庞大数据的挑战，我们提出了一种新颖的离线架构，称为联合数据深化和预取（JD2P），其包含两个关键技术：数据深化和预取。

    arXiv:2402.11925v1 Announce Type: cross  Abstract: The vision of pervasive artificial intelligence (AI) services can be realized by training an AI model on time using real-time data collected by internet of things (IoT) devices. To this end, IoT devices require offloading their data to an edge server in proximity. However, transmitting high-dimensional and voluminous data from energy-constrained IoT devices poses a significant challenge. To address this limitation, we propose a novel offloading architecture, called joint data deepening-and-prefetching (JD2P), which is feature-by-feature offloading comprising two key techniques. The first one is data deepening, where each data sample's features are sequentially offloaded in the order of importance determined by the data embedding technique such as principle component analysis (PCA). Offloading is terminated once the already transmitted features are sufficient for accurate data classification, resulting in a reduction in the amount of tr
    
[^80]: 一种用于时空图迁移学习的生成式预训练框架

    A Generative Pre-Training Framework for Spatio-Temporal Graph Transfer Learning

    [https://arxiv.org/abs/2402.11922](https://arxiv.org/abs/2402.11922)

    提出了一种生成式预训练框架 GPDiff，通过在源城市数据优化的模型参数上进行预训练，将STG迁移学习转化为预训练生成式超网络，实现了对不同数据分布和特定城市特征的适应性。

    

    时空图（STG）学习对于智慧城市应用至关重要，然而在许多城市和地区往往存在数据稀缺问题。为了弥补这一差距，我们提出了一种新颖的生成式预训练框架 GPDiff，用于STG迁移学习。与传统方法不同，我们的解决方案采用了一种新颖的方法，通过在经过源城市数据优化的一系列模型参数上进行生成式预训练来执行STG迁移学习。

    arXiv:2402.11922v1 Announce Type: new  Abstract: Spatio-temporal graph (STG) learning is foundational for smart city applications, yet it is often hindered by data scarcity in many cities and regions. To bridge this gap, we propose a novel generative pre-training framework, GPDiff, for STG transfer learning. Unlike conventional approaches that heavily rely on common feature extraction or intricate transfer learning designs, our solution takes a novel approach by performing generative pre-training on a collection of model parameters optimized with data from source cities. We recast STG transfer learning as pre-training a generative hypernetwork, which generates tailored model parameters guided by prompts, allowing for adaptability to diverse data distributions and city-specific characteristics. GPDiff employs a diffusion model with a transformer-based denoising network, which is model-agnostic to integrate with powerful STG models. By addressing challenges arising from data gaps and the
    
[^81]: 在符号化多步推理任务上训练的Transformer的机理分析

    A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task

    [https://arxiv.org/abs/2402.11917](https://arxiv.org/abs/2402.11917)

    对在合成推理任务上训练的Transformer进行的机理分析揭示其实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置

    

    Transformer在一系列推理基准测试中展现出令人印象深刻的性能。为了评估这些能力在多大程度上是实际推理的结果，现有工作集中于开发复杂的行为研究基准。然而，这些研究并未提供关于驱动观察到的能力的内部机制的见解。为了改善我们对Transformer内部机制的理解，我们提出了对一个在合成推理任务上训练的Transformer进行全面的机理分析。我们确定了模型用来解决任务的一组可解释机制，并利用相关和因果证据验证了我们的发现。我们的结果表明，它实现了一个在并行运行的有界深度循环机制，并将中间结果存储在选定的令牌位置。我们预期我们在我们的合成环境中识别的主题可以提供有价值的见解

    arXiv:2402.11917v1 Announce Type: new  Abstract: Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights
    
[^82]: 通过结合零阶和一阶优化方法设计可扩展的虚拟估值组合拍卖

    Scalable Virtual Valuations Combinatorial Auction Design by Combining Zeroth-Order and First-Order Optimization Method

    [https://arxiv.org/abs/2402.11904](https://arxiv.org/abs/2402.11904)

    本文提出了一种结合零阶和一阶优化方法，设计了可扩展的虚拟估值组合拍卖，以解决组合候选分配的可缩放性问题。

    

    arXiv:2402.11904v1 公告类型: 交叉论坛 摘要: 自动化拍卖设计旨在利用机器学习发现高收入和激励兼容的机制。确保主导战略激励兼容性（DSIC）至关重要，而最有效的方法是将机制限制在仿射最大化拍卖（AMAs）范围内。然而，现有的基于AMA的方法面临挑战，如可扩展性问题（由组合候选分配导致）和收入的不可微性。在本文中，为了实现可扩展的AMA方法，我们进一步将拍卖机制限制在虚拟估值组合拍卖（VVCAs）范围内，这是具有更少参数的AMAs子集。最初，我们使用可并行化的动态规划算法计算VVCA的获胜分配。随后，我们提出了一种结合了零阶和一阶技术的新型优化方法来优化VVCA参数。

    arXiv:2402.11904v1 Announce Type: cross  Abstract: Automated auction design seeks to discover empirically high-revenue and incentive-compatible mechanisms using machine learning. Ensuring dominant strategy incentive compatibility (DSIC) is crucial, and the most effective approach is to confine the mechanism to Affine Maximizer Auctions (AMAs). Nevertheless, existing AMA-based approaches encounter challenges such as scalability issues (arising from combinatorial candidate allocations) and the non-differentiability of revenue. In this paper, to achieve a scalable AMA-based method, we further restrict the auction mechanism to Virtual Valuations Combinatorial Auctions (VVCAs), a subset of AMAs with significantly fewer parameters. Initially, we employ a parallelizable dynamic programming algorithm to compute the winning allocation of a VVCA. Subsequently, we propose a novel optimization method that combines both zeroth-order and first-order techniques to optimize the VVCA parameters. Extens
    
[^83]: 生成式半监督图异常检测

    Generative Semi-supervised Graph Anomaly Detection

    [https://arxiv.org/abs/2402.11887](https://arxiv.org/abs/2402.11887)

    提出了一种用于半监督图异常检测的生成式方法，通过生成模拟异常节点来训练判别性单类分类器，以更好地利用图中的已知正常节点。

    

    这项工作考虑了一个实际情境下的半监督图异常检测（GAD），在这个情境中，图中的部分节点被知晓是正常的，与大多数GAD研究中使用完全未标记图的无监督情况形成对比。我们发现，可以利用这些正常节点有助于提升现有无监督GAD方法在半监督情境下的检测性能。然而，它们对这些正常节点的利用是有限的。在本文中，我们提出了一种新颖的用于半监督情境的生成式GAD方法（GGAD），以更好地利用这些正常节点。其关键思想是生成模拟异常节点的异常节点，它们融合了本地结构和节点表示，为训练判别型单类分类器提供有效的负面节点样本。

    arXiv:2402.11887v1 Announce Type: new  Abstract: This work considers a practical semi-supervised graph anomaly detection (GAD) scenario, where part of the nodes in a graph are known to be normal, contrasting to the unsupervised setting in most GAD studies with a fully unlabeled graph. As expected, we find that having access to these normal nodes helps enhance the detection performance of existing unsupervised GAD methods when they are adapted to the semi-supervised setting. However, their utilization of these normal nodes is limited. In this paper, we propose a novel Generative GAD approach (GGAD) for the semi-supervised scenario to better exploit the normal nodes. The key idea is to generate outlier nodes that assimilate anomaly nodes in both local structure and node representations for providing effective negative node samples in training a discriminative one-class classifier. There have been many generative anomaly detection approaches, but they are designed for non-graph data, and 
    
[^84]: 在具有放松采样模型的在线模型的有限时间误差分析下的Q学习

    Finite-Time Error Analysis of Online Model-Based Q-Learning with a Relaxed Sampling Model

    [https://arxiv.org/abs/2402.11877](https://arxiv.org/abs/2402.11877)

    本文通过有限时间分析以及实证评估，探讨了集成模型方法的Q学习在样本复杂度方面的优势。

    

    强化学习在模型为基础的方法的出现下取得了显著进展。在这些方法中，Q学习在无模型设置中被证明是一种强大的算法。然而，将Q学习扩展到基于模型的框架仍然相对未被探索。在本文中，我们深入研究了Q学习与基于模型方法相结合时的样本复杂度。通过理论分析和实证评估，我们试图阐明在哪些条件下，基于模型的Q学习在样本效率方面优于其无模型对应物。

    arXiv:2402.11877v1 Announce Type: cross  Abstract: Reinforcement learning has witnessed significant advancements, particularly with the emergence of model-based approaches. Among these, $Q$-learning has proven to be a powerful algorithm in model-free settings. However, the extension of $Q$-learning to a model-based framework remains relatively unexplored. In this paper, we delve into the sample complexity of $Q$-learning when integrated with a model-based approach. Through theoretical analyses and empirical evaluations, we seek to elucidate the conditions under which model-based $Q$-learning excels in terms of sample efficiency compared to its model-free counterpart.
    
[^85]: LoRA训练在NTK模式下没有虚假局部最小值

    LoRA Training in the NTK Regime has No Spurious Local Minima

    [https://arxiv.org/abs/2402.11867](https://arxiv.org/abs/2402.11867)

    LoRA训练在NTK模式下消除了虚假局部最小值，有助于梯度下降找到低秩解并实现良好的泛化。

    

    低秩适应（LoRA）已成为参数高效微调大型语言模型（LLM）的标准方法，但我们对LoRA的理论理解有限。在这项工作中，我们从理论上分析了在神经切向核（NTK）模式下使用LoRA微调，其中包含$N$个数据点，结果显示：(i) 全面微调（不使用LoRA）允许秩为$r\lesssim \sqrt{N}$的低秩解; (ii) 使用秩为$r\gtrsim \sqrt{N}$的LoRA消除了虚假的局部最小值，使梯度下降可以找到低秩解; (iii) 使用LoRA找到的低秩解具有良好的泛化性能。

    arXiv:2402.11867v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuning of large language models (LLM), but our theoretical understanding of LoRA has been limited. In this work, we theoretically analyze LoRA fine-tuning in the neural tangent kernel (NTK) regime with $N$ data points, showing: (i) full fine-tuning (without LoRA) admits a low-rank solution of rank $r\lesssim \sqrt{N}$; (ii) using LoRA with rank $r\gtrsim \sqrt{N}$ eliminates spurious local minima, allowing gradient descent to find the low-rank solutions; (iii) the low-rank solution found using LoRA generalizes well.
    
[^86]: 在李群上的随机Hessian拟合

    Stochastic Hessian Fitting on Lie Group

    [https://arxiv.org/abs/2402.11858](https://arxiv.org/abs/2402.11858)

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆，揭示了不同Hessian拟合方法的收敛速率，并证明了在特定李群上的Hessian拟合问题在轻微条件下是强凸的。

    

    本文研究了在随机Hessian-向量乘积上拟合Hessian或其逆。使用了一个Hessian拟合准则，可用于推导大部分常用方法，如BFGS、高斯牛顿、AdaGrad等。我们的研究揭示了不同Hessian拟合方法的不同收敛速率，例如，在欧几里德空间中的梯度下降的次线性速率和对称正定（SPL）矩阵和某些李群上的梯度下降的线性速率。在特定且足够一般的李群上的Hessian拟合问题在轻微条件下被证明是强凸的。为了确认我们的分析，这些方法在不同设置下进行了测试，如有噪声的Hessian-向量乘积、时变的Hessians和低精度算术。这些发现对依赖于随机二阶优化的方法是有用的。

    arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
    
[^87]: 具有本地即时误差补偿的通信高效分布式学习

    Communication-Efficient Distributed Learning with Local Immediate Error Compensation

    [https://arxiv.org/abs/2402.11857](https://arxiv.org/abs/2402.11857)

    提出了Local Immediate Error Compensated SGD（LIEC-SGD）优化算法，通过双向压缩和精心设计的补偿策略来减少通信成本，实时补偿局部压缩误差，优于现有工作。

    

    梯度压缩与误差补偿已经引起了人们的重视，目的是减少分布式学习中沉重的通信开销。然而，现有的压缩方法要么在一个迭代中只执行单向压缩，造成较高的通信成本，要么在收敛速度较慢的情况下执行双向压缩。在这项工作中，我们提出了基于双向压缩和精心设计的补偿策略来突破上述瓶颈的本地即时误差补偿SGD（LIEC-SGD）优化算法。

    arXiv:2402.11857v1 Announce Type: new  Abstract: Gradient compression with error compensation has attracted significant attention with the target of reducing the heavy communication overhead in distributed learning. However, existing compression methods either perform only unidirectional compression in one iteration with higher communication cost, or bidirectional compression with slower convergence rate. In this work, we propose the Local Immediate Error Compensated SGD (LIEC-SGD) optimization algorithm to break the above bottlenecks based on bidirectional compression and carefully designed compensation approaches. Specifically, the bidirectional compression technique is to reduce the communication cost, and the compensation technique compensates the local compression error to the model update immediately while only maintaining the global error variable on the server throughout the iterations to boost its efficacy. Theoretically, we prove that LIEC-SGD is superior to previous works in
    
[^88]: 一个增强的基于教学学习优化（TLBO）与灰狼优化器（GWO）的文本特征选择和聚类方法

    An enhanced Teaching-Learning-Based Optimization (TLBO) with Grey Wolf Optimizer (GWO) for text feature selection and clustering

    [https://arxiv.org/abs/2402.11839](https://arxiv.org/abs/2402.11839)

    本文提出了一个增强的基于教学学习优化和灰狼优化器的混合方法，用于文本特征选择和聚类，旨在解决特征选择中的局部最优陷阱问题。

    

    文本文档聚类在组织和处理日益增多的文本文档方面起着至关重要的作用。大型文本文档中包含的无信息和冗余特征会降低聚类算法的有效性。特征选择（FS）是一种用于去除这些特征的众所周知的技术。由于FS可以被形式化为一个优化问题，因此各种元启发式算法被用来解决它。教学学习优化（TLBO）是一种受益于较少参数和快速收敛的新型元启发式算法。通过提出TLBO，灰狼优化器（GWO）和遗传算法（GA）算子的混合方法，本文提出了一个基于过滤的FS算法（TLBO-GWO）。选择了六个基准数据集，并将TLBO-GWO与三个进行比较。

    arXiv:2402.11839v1 Announce Type: cross  Abstract: Text document clustering can play a vital role in organizing and handling the everincreasing number of text documents. Uninformative and redundant features included in large text documents reduce the effectiveness of the clustering algorithm. Feature selection (FS) is a well-known technique for removing these features. Since FS can be formulated as an optimization problem, various meta-heuristic algorithms have been employed to solve it. Teaching-Learning-Based Optimization (TLBO) is a novel meta-heuristic algorithm that benefits from the low number of parameters and fast convergence. A hybrid method can simultaneously benefit from the advantages of TLBO and tackle the possible entrapment in the local optimum. By proposing a hybrid of TLBO, Grey Wolf Optimizer (GWO), and Genetic Algorithm (GA) operators, this paper suggests a filter-based FS algorithm (TLBO-GWO). Six benchmark datasets are selected, and TLBO-GWO is compared with three 
    
[^89]: UniST：一种为城市时空预测设计的提示增强型通用模型

    UniST: A Prompt-Empowered Universal Model for Urban Spatio-Temporal Prediction

    [https://arxiv.org/abs/2402.11838](https://arxiv.org/abs/2402.11838)

    UniST是一种为城市时空预测设计的通用模型，通过灵活性、有效的生成式预训练以及丰富的掩码策略成功捕捉复杂的时空关系。

    

    arXiv:2402.11838v1 公告类型：新的 摘要：城市时空预测对于决策至关重要，例如交通管理、资源优化和城市规划。尽管自然语言的预训练基础模型取得了显著突破，其中一个通用模型可以处理跨多个领域的多个任务，但城市时空建模落后。现有的城市预测方法通常针对特定的时空场景进行定制，需要特定任务的模型设计和大量域内训练数据。在这项工作中，我们提出了一种用于城市时空预测的通用模型UniST。借鉴自大型语言模型，UniST通过以下方式取得成功：(i) 对不同空间时间数据特征的灵活性，(ii) 有效的生成式预训练，采用精心设计的掩码策略来捕捉复杂的空间时间关系，(iii) 时空知识。

    arXiv:2402.11838v1 Announce Type: new  Abstract: Urban spatio-temporal prediction is crucial for informed decision-making, such as transportation management, resource optimization, and urban planning. Although pretrained foundation models for natural languages have experienced remarkable breakthroughs, wherein one general-purpose model can tackle multiple tasks across various domains, urban spatio-temporal modeling lags behind. Existing approaches for urban prediction are usually tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive in-domain training data. In this work, we propose a universal model, UniST, for urban spatio-temporal prediction. Drawing inspiration from large language models, UniST achieves success through: (i) flexibility towards diverse spatio-temporal data characteristics, (ii) effective generative pre-training with elaborated masking strategies to capture complex spatio-temporal relationships, (iii) spatio-temporal know
    
[^90]: 自主引导的稳健图结构细化

    Self-Guided Robust Graph Structure Refinement

    [https://arxiv.org/abs/2402.11837](https://arxiv.org/abs/2402.11837)

    本文提出了一个自主引导的GSR框架（SG-GSR），通过利用被攻击图中发现的干净子图，并提出了图增强和分组训练策略，以应对现有GSR方法在真实场景中受限的问题。

    

    近期研究发现，图神经网络容易受到对抗性攻击。为了抵御此类攻击，稳健图结构修正（GSR）方法旨在通过节点特征、图结构或外部信息来最小化对抗性边的影响。然而，我们发现现有的GSR方法受到狭窄假设的限制，比如假设干净的节点特征、适度的结构攻击以及可用的外部干净图，导致在真实场景中应用受限。本文提出了一个自主引导的GSR框架（SG-GSR），其利用给定被攻击图中发现的干净子图。此外，我们提出了一种新的图增强和分组训练策略来处理在干净子图提取中的两个技术挑战：1）结构信息的丢失，2）节点度分布的不平衡。大量实验证明

    arXiv:2402.11837v1 Announce Type: new  Abstract: Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstra
    
[^91]: 简单如ABC：统一Boltzmann Q-Learning和反事实遗憾最小化

    Easy as ABCs: Unifying Boltzmann Q-Learning and Counterfactual Regret Minimization

    [https://arxiv.org/abs/2402.11835](https://arxiv.org/abs/2402.11835)

    ABCs算法结合了Boltzmann Q-learning和反事实遗憾最小化，通过测量环境稳定性自适应选择探索比例，在单一智能体和多智能体领域表现出色。

    

    我们提出了ABCs（通过子站点稳定性进行自适应分支），这是一种结合了Boltzmann Q-learning（一种经典的单一智能体领域强化学习算法）和反事实遗憾最小化（CFR）的双赢算法。ABCs通过测量环境奖励和转换动态的稳定性，自适应地选择每次迭代要探索环境的比例。在马尔可夫决策过程中，ABCs与BQL相比最多只慢一个O（A）因子就能收敛到最优策略，其中A是环境中的动作数量。在两人零和博弈中，ABCs有保证收敛到纳什均衡（假设可以访问一个完美的神谕来检测稳定性），而BQL没有这样的保证。从经验上看，ABCs在OpenSpiel游戏库环境中表现出强劲的性能。

    arXiv:2402.11835v1 Announce Type: new  Abstract: We propose ABCs (Adaptive Branching through Child stationarity), a best-of-both-worlds algorithm combining Boltzmann Q-learning (BQL), a classic reinforcement learning algorithm for single-agent domains, and counterfactual regret minimization (CFR), a central algorithm for learning in multi-agent domains. ABCs adaptively chooses what fraction of the environment to explore each iteration by measuring the stationarity of the environment's reward and transition dynamics. In Markov decision processes, ABCs converges to the optimal policy with at most an O(A) factor slowdown compared to BQL, where A is the number of actions in the environment. In two-player zero-sum games, ABCs is guaranteed to converge to a Nash equilibrium (assuming access to a perfect oracle for detecting stationarity), while BQL has no such guarantees. Empirically, ABCs demonstrates strong performance when benchmarked across environments drawn from the OpenSpiel game libr
    
[^92]: 大型语言模型对图形召回的微结构和准确性

    Microstructures and Accuracy of Graph Recall by Large Language Models

    [https://arxiv.org/abs/2402.11821](https://arxiv.org/abs/2402.11821)

    本研究首次系统研究了大型语言模型对图形召回的准确性和偏见微结构，探讨了它们与人类的异同以及对其他图形推理任务的影响。

    

    图形数据对许多应用至关重要，其中很多数据以文本格式描述关系。因此，准确地召回和编码先前文本中描述的图形是大型语言模型(LLMs)需要展示的基本但关键能力，以执行涉及图形结构信息的推理任务。人类在图形召回方面的表现已被认知科学家研究了几十年，发现其经常呈现与人类处理社会关系一致的某些结构性偏见模式。然而，迄今为止，我们很少了解LLMs在类似图形召回任务中的行为：它们召回的图形是否也呈现某些偏见模式，如果是，它们与人类的表现有何不同并如何影响其他图形推理任务？在这项研究中，我们进行了第一次对LLMs进行图形召回的系统研究，研究其准确性和偏见微结构（局部结构）。

    arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
    
[^93]: 避免对比学习中的特征抑制：学习以前未曾学到的内容

    Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before

    [https://arxiv.org/abs/2402.11816](https://arxiv.org/abs/2402.11816)

    开发了一种多阶对比学习（MCL）框架，以解决对比学习中的特征抑制问题，并确保模型学习全面的表示。

    

    自监督对比学习已经成为从未标记数据中获取高质量表示的强大方法。然而，最近在标准对比学习（如SimCLR、CLIP中）中发现了特征抑制：在单个端到端训练阶段，对比模型仅捕获对比观点之间的一部分共享信息，而忽略了其他潜在有用的信息。具有特征抑制，对比模型通常无法学习足够适用于各种下游任务的表示。为了减轻特征抑制问题并确保对比模型学习全面的表示，我们开发了一种新颖的多阶对比学习（MCL）框架。与通常会导致特征抑制的标准对比学习不同，MCL逐渐学习以前未探索过的新特征，同时保持已经学到的内容。

    arXiv:2402.11816v1 Announce Type: cross  Abstract: Self-Supervised contrastive learning has emerged as a powerful method for obtaining high-quality representations from unlabeled data. However, feature suppression has recently been identified in standard contrastive learning ($e.g.$, SimCLR, CLIP): in a single end-to-end training stage, the contrastive model captures only parts of the shared information across contrasting views, while ignore the other potentially useful information. With feature suppression, contrastive models often fail to learn sufficient representations capable for various downstream tasks. To mitigate the feature suppression problem and ensure the contrastive model to learn comprehensive representations, we develop a novel Multistage Contrastive Learning (MCL) framework. Unlike standard contrastive learning that often result in feature suppression, MCL progressively learn new features that have not been explored in the previous stage, while maintaining the well-lea
    
[^94]: HU在SemEval-2024任务8A中的表现：对比学习能否学习嵌入以检测机器生成的文本？

    HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?

    [https://arxiv.org/abs/2402.11815](https://arxiv.org/abs/2402.11815)

    提出了一种基于对比学习的单一模型，用较少的参数实现与基线相当的机器生成文本检测性能

    

    这篇论文描述了我们为SemEval-2024任务8“多生成器、多领域和多语言黑匣子机器生成文本检测”开发的系统。由于大型语言模型（LLM）在虚假文本生成、网络钓鱼、考试作弊甚至抄袭版权材料中的使用，机器生成文本一直是主要关注的问题之一。许多系统已经被开发用于检测机器生成的文本。然而，这些系统中的大部分依赖于文本生成模型，这是一个在实际场景中不切实际的限制，因为通常不可能知道用户用于文本生成的具体模型。在这项工作中，我们提出了基于对比学习的单一模型，其使用基线参数的大约40%（149M比355M），但在测试数据集上表现出了可比的性能（在137个参与者中排名第21）。我们的关键发现是，即使没有多个模型的集成，

    arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
    
[^95]: 智能并行自动纠错解码：加速大型语言模型推理

    Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding

    [https://arxiv.org/abs/2402.11809](https://arxiv.org/abs/2402.11809)

    提出了Smart Parallel Auto-Correct Decoding (SPACE)方法，通过集成半自回归推理和猜测解码，实现了大型语言模型推理加速和并行生成验证令牌的功能。

    

    这项研究旨在加速拥有数十亿参数的大型语言模型（LLMs）的推理速度。我们提出了“智能并行自动纠错解码”（SPACE），这是一种创新方法，旨在实现LLMs的无损加速。通过集成半自回归推理和猜测解码能力，SPACE独特地使自回归LLMs能够并行生成和验证令牌。这是通过专门的半自回归监督微调过程实现的，该过程使现有LLMs具有同时预测多个令牌的能力。此外，一种自动纠错解码算法促进了单个模型调用内令牌序列的同时生成和验证。通过在一系列LLMs上进行广泛实验证明，SPACE在HumanEval-X上表现出2.7倍至4.0倍的推理加速。

    arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
    
[^96]: 具有延迟更新的随机逼近：马尔科夫采样下的有限时间速率

    Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling

    [https://arxiv.org/abs/2402.11800](https://arxiv.org/abs/2402.11800)

    延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。

    

    受大规模和多智能体强化学习应用的启发，我们研究了在马尔科夫采样下具有延迟更新的随机逼近（SA）方案的非渐近性能。虽然延迟的影响在优化中得到了广泛研究，但它们与底层马尔科夫过程相互作用以塑造SA的有限时间性能的方式仍然不太清楚。在这个背景下，我们的第一个主要贡献是证明在时间变化有界延迟下，延迟的SA更新规则确保最后迭代收敛到SA运算符固定点周围的球体具有指数快速的速度。值得注意的是，我们的界限在依赖于最大延迟$\tau_{max}$和混合时间$\tau_{mix}$方面是\emph{紧致的}。为了实现这一紧密界限，我们开发了一种新颖的归纳证明技术，与各种现有延迟优化分析不同，它依赖于建立未...

    arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
    
[^97]: 生成万花筒网络

    Generative Kaleidoscopic Networks

    [https://arxiv.org/abs/2402.11793](https://arxiv.org/abs/2402.11793)

    发现深层ReLU网络表现出过度泛化现象，利用这一特性设计了“生成万花筒网络”，通过递归映射随机输入噪声生成样本。

    

    发现深层ReLU网络（或多层感知器架构）表现出“过度泛化”现象。也就是说，那些在训练过程中没有看到的输入的输出值被映射到了在学习过程中观察到的输出范围附近。换句话说，多层感知器学习了一对多的映射，这种效应在增加层数或多层感知器的深度时更为明显。我们利用了深层ReLU网络的这一特性来设计一个数据集万花筒，称为“生成万花筒网络”。简而言之，如果我们学习一个多层感知器将输入 $x\in\mathbb{R}^D$ 映射到自身 $f_\mathcal{N}(x)\rightarrow x$，那么“万花筒采样”过程将从随机输入噪声 $z\in\mathbb{R}^D$ 开始，并递归地应用 $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$。经过燃烧期后，我们开始观察来自输入分布的样本，我们发现更深的

    arXiv:2402.11793v1 Announce Type: cross  Abstract: We discovered that the Deep ReLU networks (or Multilayer Perceptron architecture) demonstrate an 'over-generalization' phenomenon. That is, the output values for the inputs that were not seen during training are mapped close to the output range that were observed during the learning process. In other words, the MLP learns a many-to-one mapping and this effect is more prominent as we increase the number of layers or depth of the MLP. We utilize this property of Deep ReLU networks to design a dataset kaleidoscope, termed as 'Generative Kaleidoscopic Networks'. Briefly, if we learn a MLP to map from input $x\in\mathbb{R}^D$ to itself $f_\mathcal{N}(x)\rightarrow x$, the 'Kaleidoscopic sampling' procedure starts with a random input noise $z\in\mathbb{R}^D$ and recursively applies $f_\mathcal{N}(\cdots f_\mathcal{N}(z)\cdots )$. After a burn-in period duration, we start observing samples from the input distribution and we found that deeper 
    
[^98]: 通过扩散模型生成的假设的统计检验

    Statistical Test for Generated Hypotheses by Diffusion Models

    [https://arxiv.org/abs/2402.11789](https://arxiv.org/abs/2402.11789)

    本研究提出了一种统计检验方法，通过选择性推断框架，在考虑生成图像是由训练的扩散模型产生的条件下，量化医学图像诊断结果的可靠性。

    

    AI的增强性能加速了其融入科学研究。特别是，利用生成式AI创建科学假设是很有前途的，并且正在越来越多地应用于各个领域。然而，当使用AI生成的假设进行关键决策（如医学诊断）时，验证它们的可靠性至关重要。在本研究中，我们考虑使用扩散模型生成的图像进行医学诊断任务，并提出了一种统计检验来量化其可靠性。所提出的统计检验的基本思想是使用选择性推断框架，我们考虑在生成的图像是由经过训练的扩散模型产生的这一事实条件下的统计检验。利用所提出的方法，医学图像诊断结果的统计可靠性可以以p值的形式量化，从而实现在控制错误率的情况下进行决策。

    arXiv:2402.11789v1 Announce Type: cross  Abstract: The enhanced performance of AI has accelerated its integration into scientific research. In particular, the use of generative AI to create scientific hypotheses is promising and is increasingly being applied across various fields. However, when employing AI-generated hypotheses for critical decisions, such as medical diagnoses, verifying their reliability is crucial. In this study, we consider a medical diagnostic task using generated images by diffusion models, and propose a statistical test to quantify its reliability. The basic idea behind the proposed statistical test is to employ a selective inference framework, where we consider a statistical test conditional on the fact that the generated images are produced by a trained diffusion model. Using the proposed method, the statistical reliability of medical image diagnostic results can be quantified in the form of a p-value, allowing for decision-making with a controlled error rate. 
    
[^99]: 语言模型认为哪些证据令人信服？

    What Evidence Do Language Models Find Convincing?

    [https://arxiv.org/abs/2402.11782](https://arxiv.org/abs/2402.11782)

    通过构建 ConflictingQA 数据集，并进行敏感性和反事实分析，研究发现当前语言模型在预测时很大程度上依赖于网站与查询的相关性，而忽视了人类认为重要的文本风格特征。

    

    检索增强型语言模型越来越多地被赋予主观、有争议和矛盾的查询任务，如“阿斯巴甜是否与癌症有关”。为了解决这些模糊的查询，我们必须搜索大量网站，并考虑“我认为哪些证据是令人信服的？”。在这项工作中，我们研究了语言模型是如何回答这个问题的。特别是，我们构建了一个名为 ConflictingQA 的数据集，将有争议的查询与一系列包含不同事实（如定量结果）、论证风格（如权威呼声）和答案（是或否）的真实世界证据文档配对。我们使用这个数据集进行敏感性和反事实分析，探讨哪些文本特征最影响语言模型的预测。总体而言，我们发现当前模型在很大程度上依赖网站与查询的相关性，而在很大程度上忽视了人类认为重要的风格特征，比如文本是否是

    arXiv:2402.11782v1 Announce Type: new  Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as "is aspartame linked to cancer". To resolve these ambiguous queries, one must search through a large range of websites and consider "which, if any, of this evidence do I find convincing?". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text 
    
[^100]: 朝向自消耗生成模型的理论理解

    Towards Theoretical Understandings of Self-Consuming Generative Models

    [https://arxiv.org/abs/2402.11778](https://arxiv.org/abs/2402.11778)

    通过构建理论框架，我们探讨了在自消耗循环中训练生成模型对数据分布学习的影响，证明了在足够大的训练数据集大小或真实数据比例条件下，合成数据分布与原始真实数据分布之间的总变差距离能够被有效控制。

    

    这篇论文探讨了训练生成模型的新挑战，即在一个自消耗循环中训练模型，其中连续的模型世代通过混合之前世代的真实数据和合成数据来进行递归训练。我们构建了一个理论框架，以严格评估这种训练方案对未来模型学习的数据分布产生的影响。具体来说，我们推导了在不同混合训练场景下，未来模型产生的合成数据分布与原始真实数据分布之间的总变差（TV）距离的界限。我们的分析表明，在混合训练数据集的大小或真实数据比例足够大的条件下，这种距离可以被有效控制。有趣的是，我们进一步揭示了由扩大合成数据量引起的相变，理论上证明了虽然TV距离表现出初始上升，但却逐渐下降。

    arXiv:2402.11778v1 Announce Type: cross  Abstract: This paper tackles the emerging challenge of training generative models within a self-consuming loop, wherein successive generations of models are recursively trained on mixtures of real and synthetic data from previous generations. We construct a theoretical framework to rigorously evaluate how this training regimen impacts the data distributions learned by future models. Specifically, we derive bounds on the total variation (TV) distance between the synthetic data distributions produced by future models and the original real data distribution under various mixed training scenarios. Our analysis demonstrates that this distance can be effectively controlled under the condition that mixed training dataset sizes or proportions of real data are large enough. Interestingly, we further unveil a phase transition induced by expanding synthetic data amounts, proving theoretically that while the TV distance exhibits an initial ascent, it declin
    
[^101]: 在语言模型嵌入中揭示潜在的人类福祉

    Uncovering Latent Human Wellbeing in Language Model Embeddings

    [https://arxiv.org/abs/2402.11777](https://arxiv.org/abs/2402.11777)

    本研究通过ETHICS Utilitarianism任务发现，预训练语言模型的表示隐含了对人类福祉的理解，且模型规模增加时，准确率呈非下降趋势。

    

    语言模型是否隐含地学习了人类福祉的概念？我们通过ETHICS功利主义任务进行探讨，评估缩放是否增强了预训练模型的表示。我们的初步发现显示，无需任何提示工程或微调，OpenAI的text-embedding-ada-002的主成分达到73.9%的准确率。这与在整个ETHICS数据集上微调的BERT-large模型的74.6%准确率非常接近，表明预训练传达了对人类福祉的某种理解。接下来，我们考虑了四种语言模型系列，观察功利主义准确率随参数增加而变化。我们发现，使用足够数量的主成分时，性能随模型规模的增加而非减少。

    arXiv:2402.11777v1 Announce Type: cross  Abstract: Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.
    
[^102]: FOD-Swin-Net：基于变压器的深度模型实现纤维定向分布的角度超分辨率

    FOD-Swin-Net: angular super resolution of fiber orientation distribution using a transformer-based deep model

    [https://arxiv.org/abs/2402.11775](https://arxiv.org/abs/2402.11775)

    本研究通过使用自动角度超分辨率技术，将来自更快速采集的数据的角度估计提升到与需要较长时间获取的高分辨率数据相当，从而在纤维定向分布的估计中取得了显著进展。

    

    识别和表征脑部纤维束可以帮助理解许多疾病和病况。在这一过程中的一个重要步骤是利用扩散加权磁共振成像（DW-MRI）来估计纤维方向。然而，获得稳健的方向估计需要高分辨率数据，导致获取时间长且在临床上并不总是可用。在这项工作中，我们探讨了利用更快速获取的自动角度超分辨率来克服这一挑战。利用公开可用的Human Connectome Project（HCP）DW-MRI数据，我们训练了一个基于变压器的深度学习架构，实现了纤维定向分布（FOD）的角度超分辨率。我们的基于图块的方法，FOD-Swin-Net，能够使从32个方向推动的单壳重建与多壳288个方向FOD重建相媲美，大大减少了所需的数目

    arXiv:2402.11775v1 Announce Type: cross  Abstract: Identifying and characterizing brain fiber bundles can help to understand many diseases and conditions. An important step in this process is the estimation of fiber orientations using Diffusion-Weighted Magnetic Resonance Imaging (DW-MRI). However, obtaining robust orientation estimates demands high-resolution data, leading to lengthy acquisitions that are not always clinically available. In this work, we explore the use of automated angular super resolution from faster acquisitions to overcome this challenge. Using the publicly available Human Connectome Project (HCP) DW-MRI data, we trained a transformer-based deep learning architecture to achieve angular super resolution in fiber orientation distribution (FOD). Our patch-based methodology, FOD-Swin-Net, is able to bring a single-shell reconstruction driven from 32 directions to be comparable to a multi-shell 288 direction FOD reconstruction, greatly reducing the number of required d
    
[^103]: 张量时间序列的动态多网络挖掘

    Dynamic Multi-Network Mining of Tensor Time Series

    [https://arxiv.org/abs/2402.11773](https://arxiv.org/abs/2402.11773)

    提出了一种新方法，Dynamic Multi-network Mining (DMM)，能够将张量时间序列转换为不同长度的段组，通过稀疏依赖网络提供聚类的可解释性和精确性。

    

    时间序列的子序列聚类是数据挖掘中的一个重要任务，解释结果聚类也至关重要，因为通常我们没有关于数据的先验知识。因此，面对由包含时间戳在内的多种模式组成的大量张量时间序列，我们如何为张量时间序列实现子序列聚类并提供可解释的见解？在本文中，我们提出了一种新方法，即动态多网络挖掘（DMM），它将张量时间序列转换为由l1范数约束的一组各种长度的段组（即聚类）特征化的依赖网络。我们的方法具有以下特性。(a) 可解释性：它使用多个网络对聚类进行特征描述，每个网络是相应非时间模式的稀疏依赖网络，从而提供可见且可解释的关键关系见解。 (b) 精确性：它发现了聚类。。。

    arXiv:2402.11773v1 Announce Type: cross  Abstract: Subsequence clustering of time series is an essential task in data mining, and interpreting the resulting clusters is also crucial since we generally do not have prior knowledge of the data. Thus, given a large collection of tensor time series consisting of multiple modes, including timestamps, how can we achieve subsequence clustering for tensor time series and provide interpretable insights? In this paper, we propose a new method, Dynamic Multi-network Mining (DMM), that converts a tensor time series into a set of segment groups of various lengths (i.e., clusters) characterized by a dependency network constrained with l1-norm. Our method has the following properties. (a) Interpretable: it characterizes the cluster with multiple networks, each of which is a sparse dependency network of a corresponding non-temporal mode, and thus provides visible and interpretable insights into the key relationships. (b) Accurate: it discovers the clus
    
[^104]: 评估基于指数的治疗分配有效性

    Evaluating the Effectiveness of Index-Based Treatment Allocation

    [https://arxiv.org/abs/2402.11771](https://arxiv.org/abs/2402.11771)

    本文介绍了一种评估基于指数的资源分配策略有效性的方法，通过翻译和扩展统计文献中的最新思想，提供了有效的估计器和计算渐近正确置信区间的方法。

    

    当资源稀缺时，需要一种分配策略来决定谁能获得资源。本文介绍了一种评估基于指数的分配策略的方法，该策略通过使用随机对照试验的数据，将有限数量的资源分配给最需要的人。我们从统计文献中翻译和扩展了最近的想法，提出了一种高效的估计器和计算渐近正确置信区间的方法，从而有效地得出有效的统计结论。

    arXiv:2402.11771v1 Announce Type: cross  Abstract: When resources are scarce, an allocation policy is needed to decide who receives a resource. This problem occurs, for instance, when allocating scarce medical resources and is often solved using modern ML methods. This paper introduces methods to evaluate index-based allocation policies -- that allocate a fixed number of resources to those who need them the most -- by using data from a randomized control trial. Such policies create dependencies between agents, which render the assumptions behind standard statistical tests invalid and limit the effectiveness of estimators. Addressing these challenges, we translate and extend recent ideas from the statistics literature to present an efficient estimator and methods for computing asymptotically correct confidence intervals. This enables us to effectively draw valid statistical conclusions, a critical gap in previous work. Our extensive experiments validate our methodology in practical sett
    
[^105]: 强化学习作为预测级联的简洁替代方案：基于图像分割的案例研究

    Reinforcement Learning as a Parsimonious Alternative to Prediction Cascades: A Case Study on Image Segmentation

    [https://arxiv.org/abs/2402.11760](https://arxiv.org/abs/2402.11760)

    引入强化学习作为预测级联的简洁替代方案，减少计算成本。

    

    深度学习架构在计算机视觉任务中取得了最先进的性能，例如目标检测和图像分割。尽管这些架构导致了精度的提高，但在推断过程中通常会伴随着计算和内存需求的大幅增加。我们认为，预测级联导致了由于冗余中间步骤而增加的计算成本。

    arXiv:2402.11760v1 Announce Type: new  Abstract: Deep learning architectures have achieved state-of-the-art (SOTA) performance on computer vision tasks such as object detection and image segmentation. This may be attributed to the use of over-parameterized, monolithic deep learning architectures executed on large datasets. Although such architectures lead to increased accuracy, this is usually accompanied by a large increase in computation and memory requirements during inference. While this is a non-issue in traditional machine learning pipelines, the recent confluence of machine learning and fields like the Internet of Things has rendered such large architectures infeasible for execution in low-resource settings. In such settings, previous efforts have proposed decision cascades where inputs are passed through models of increasing complexity until desired performance is achieved. However, we argue that cascaded prediction leads to increased computational cost due to wasteful intermed
    
[^106]: MARS：用于生成式LLMs中不确定性估计的意义感知响应评分

    MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs

    [https://arxiv.org/abs/2402.11756](https://arxiv.org/abs/2402.11756)

    MARS提出了一种新的评分函数MARS，考虑了生成序列中每个标记的语义贡献，该方法改进了生成式LLMs中的不确定性估计性能。

    

    生成式大型语言模型（LLMs）因在各种任务中的卓越表现而被广泛利用。然而，它们产生不准确或误导性输出的倾向可能带来潜在风险，尤其是在高风险环境中。因此，估计生成式LLM输出的正确性是增强可靠性的重要任务。生成式LLMs中的不确定性估计（UE）是一个不断发展的领域，其中SOTA基于概率的方法通常采用长度标准化评分。在这项工作中，我们提出了一种名为意义感知响应评分（MARS）的替代长度标准化评分的UE方法。MARS是一种考虑在问题的上下文中生成序列中每个标记的语义贡献的新型评分函数。我们证明将MARS整合到UE方法中会在UE性能上带来普遍和显著的改进。我们使用三种不同的闭卷式问答来进行实验

    arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
    
[^107]: SPML: 一种用于防御语言模型受到提示攻击的领域特定语言

    SPML: A DSL for Defending Language Models Against Prompt Attacks

    [https://arxiv.org/abs/2402.11755](https://arxiv.org/abs/2402.11755)

    SPML是一种用于优化提示并监控基于大型语言模型聊天机器人输入的领域特定语言，用于防御恶意攻击并优化成本。

    

    大型语言模型（LLMs）已深刻改变了自然语言应用，越来越多地依赖于基于指令的定义来设计聊天机器人。然而，部署后，聊天机器人的定义是固定的，并且容易受到恶意用户的攻击，突出了防止不道德应用和财务损失的需要。现有研究探讨了用户提示对基于LLM的聊天机器人的影响，但尚未探索包含应用特定聊天机器人的攻击的实用方法。本文提出了系统提示元语言（SPML），这是一种用于优化提示并监控基于LLM的聊天机器人输入的领域特定语言。SPML主动检查攻击提示，确保用户输入与聊天机器人定义相符，防止在LLM主干上对其进行恶意执行，优化成本。它也通过编程语言能力简化了聊天机器人定义的制作，克服了自然语言的限制。

    arXiv:2402.11755v1 Announce Type: cross  Abstract: Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language 
    
[^108]: 对角化SGD：通过重新参数化和平滑实现非可微模型的快速收敛SGD

    Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via Reparameterisation and Smoothing

    [https://arxiv.org/abs/2402.11752](https://arxiv.org/abs/2402.11752)

    引入了Diagonalisation Stochastic Gradient Descent（对角化SGD），通过重新参数化和平滑实现非可微模型的快速收敛SGD，在实证评估中表现出简单、快速、稳定，并且取得了数量级的工作规范化方差降低。

    

    众所周知，对于非可微模型，展现出较低方差的重新参数化梯度估计器在实践中存在偏差。这可能危及基于梯度的优化方法（如随机梯度下降SGD）的正确性。我们引入了一个简单的语法框架来分块地定义非可微函数，并提出了一种系统方法，以获得使重新参数化梯度估计器无偏的平滑。我们的主要贡献是一种新颖的SGD变体，对角化随机梯度下降，它在优化过程中逐步提高平滑近似的准确性，并证明收敛到未平滑（原始）目标的稳定点。我们的实证评估显示，与现有技术相比，我们的方法简单、快速、稳定，并且在工作规范化方差上实现了数量级的降低。

    arXiv:2402.11752v1 Announce Type: cross  Abstract: It is well-known that the reparameterisation gradient estimator, which exhibits low variance in practice, is biased for non-differentiable models. This may compromise correctness of gradient-based optimisation methods such as stochastic gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable functions piecewisely and present a systematic approach to obtain smoothings for which the reparameterisation gradient estimator is unbiased. Our main contribution is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively enhances the accuracy of the smoothed approximation during optimisation, and we prove convergence to stationary points of the unsmoothed (original) objective. Our empirical evaluation reveals benefits over the state of the art: our approach is simple, fast, stable and attains orders of magnitude reduction in work-normalised variance.
    
[^109]: 语音情绪识别和领域适应的参数高效微调

    Parameter Efficient Finetuning for Speech Emotion Recognition and Domain Adaptation

    [https://arxiv.org/abs/2402.11747](https://arxiv.org/abs/2402.11747)

    本文研究了用于语音情绪识别的参数高效微调方法，提出了各种PEFT适配器并展示了其在分类离散情绪类别和预测情绪属性方面的有效性，同时通过减少可训练参数数量超越了完整微调。还提出了两阶段适应策略以提高模型对自然情感表达的捕捉能力。

    

    基础模型在语音情绪识别（SER）方面表现出卓越的性能。然而，鉴于情绪语料库中的数据有限，对大型预训练模型的所有参数进行微调既耗费资源又容易过拟合。本文研究了用于SER的参数高效微调（PEFT）。系统研究了各种PEFT适配器，旨在对离散情绪类别的分类和情绪属性的维度预测进行研究。结果表明，PEFT方法的组合超越了具有可训练参数数量显著减少的完整微调。此外，提出了一个两阶段适应策略，以适应在场景情绪数据上训练的模型，这种数据更易获得，使模型更擅长捕捉自然情感表达。内部和跨语料库实验验证了所提方法的有效性。

    arXiv:2402.11747v1 Announce Type: cross  Abstract: Foundation models have shown superior performance for speech emotion recognition (SER). However, given the limited data in emotion corpora, finetuning all parameters of large pre-trained models for SER can be both resource-intensive and susceptible to overfitting. This paper investigates parameter-efficient finetuning (PEFT) for SER. Various PEFT adaptors are systematically studied for both classification of discrete emotion categories and prediction of dimensional emotional attributes. The results demonstrate that the combination of PEFT methods surpasses full finetuning with a significant reduction in the number of trainable parameters. Furthermore, a two-stage adaptation strategy is proposed to adapt models trained on acted emotion data, which is more readily available, to make the model more adept at capturing natural emotional expressions. Both intra- and cross-corpus experiments validate the efficacy of the proposed approach in e
    
[^110]: 平衡数据，不平衡光谱：揭示具有光谱不平衡的类别差异

    Balanced Data, Imbalanced Spectra: Unveiling Class Disparities with Spectral Imbalance

    [https://arxiv.org/abs/2402.11742](https://arxiv.org/abs/2402.11742)

    该论文介绍了光谱不平衡的概念作为导致类别差异的潜在来源，并研究了光谱不平衡与类别偏见之间的联系，为理论和实践中的类别差异提供了一个理论框架，并在多个预训练编码器中验证了这种联系。

    

    分类模型被期望在不同类别上表现同样良好，但在实践中，它们的表现往往存在较大差距。这个类别偏见问题在样本不平衡的数据集中得到了广泛研究，但在平衡数据集中相对被忽视。在这项工作中，我们提出了特征中的光谱不平衡概念作为类别差异的潜在来源，并研究光谱不平衡与类别偏见在理论和实践中的联系。为了建立光谱不平衡与类别差距之间的关系，我们开发了一个用于研究类别差异的理论框架，并推导了高维混合模型设定中每类错误的精确表达式。然后我们在11个不同的最先进的预训练编码器中研究了这一现象，并展示了我们提出的框架如何用于比较编码器的质量，以及评估和组合数据增强策略。

    arXiv:2402.11742v1 Announce Type: new  Abstract: Classification models are expected to perform equally well for different classes, yet in practice, there are often large gaps in their performance. This issue of class bias is widely studied in cases of datasets with sample imbalance, but is relatively overlooked in balanced datasets. In this work, we introduce the concept of spectral imbalance in features as a potential source for class disparities and study the connections between spectral imbalance and class bias in both theory and practice. To build the connection between spectral imbalance and class gap, we develop a theoretical framework for studying class disparities and derive exact expressions for the per-class error in a high-dimensional mixture model setting. We then study this phenomenon in 11 different state-of-the-art pretrained encoders and show how our proposed framework can be used to compare the quality of encoders, as well as evaluate and combine data augmentation stra
    
[^111]: 利用Koopman算子提取神经网络中的非线性并进行模型压缩

    Extraction of nonlinearity in neural networks and model compression with Koopman operator

    [https://arxiv.org/abs/2402.11740](https://arxiv.org/abs/2402.11740)

    本论文使用Koopman算子提取神经网络中的非线性，证明了受限非线性已足够进行手写数字分类，并提出了一种模型压缩方法，能在资源受限环境中处理大型网络。

    

    非线性在深度神经网络中起着至关重要的作用。在本文中，我们首先调查了神经网络的非线性对分类任务的关键程度。为此，我们采用Koopman算子、扩展动态模态分解和张量列车格式。结果表明，受限非线性已经足以进行手写数字的分类。接着，我们提出了一种用于深度神经网络的模型压缩方法，这对于在资源受限的环境中处理大型网络可能是有益的。利用Koopman算子，所提出的方法使我们可以在神经网络的内部处理中使用线性代数。数值结果表明，所提出的方法在高度压缩模型设置下在手写数字识别任务中的性能要么与传统方法相当，要么更好。

    arXiv:2402.11740v1 Announce Type: new  Abstract: Nonlinearity plays a crucial role in deep neural networks. In this paper, we first investigate the degree to which the nonlinearity of the neural network is essential. For this purpose, we employ the Koopman operator, extended dynamic mode decomposition, and the tensor-train format. The results imply that restricted nonlinearity is enough for the classification of handwritten numbers. Then, we propose a model compression method for deep neural networks, which could be beneficial to handling large networks in resource-constrained environments. Leveraging the Koopman operator, the proposed method enables us to use linear algebra in the internal processing of neural networks. We numerically show that the proposed method performs comparably or better than conventional methods in highly compressed model settings for the handwritten number recognition task.
    
[^112]: 一个用于神经网络动力系统模型的过渡系统抽象框架

    A Transition System Abstraction Framework for Neural Network Dynamical System Models

    [https://arxiv.org/abs/2402.11739](https://arxiv.org/abs/2402.11739)

    提出了一个过渡系统抽象框架，用于增强神经网络动力系统模型的可解释性，并通过人类手写动力学学习和验证应用进行验证。

    

    本文提出了一个用于神经网络动力系统模型的过渡系统抽象框架，以提高模型的可解释性，应用于复杂动力系统，如人类行为学习和验证。首先，将局部工作区域根据数据驱动的最大熵（ME）划分方法分割为多个局部分区。然后，基于神经网络的集值可达性分析获得过渡矩阵。最后，给出了应用于人类手写动力学学习和验证的应用，以验证我们提出的抽象框架，展示了增强黑盒模型可解释性的优势，即我们提出的框架能够将数据驱动的神经网络模型抽象为一个过渡系统，通过验证在计算中描述的规范使神经网络模型变得可解释。

    arXiv:2402.11739v1 Announce Type: cross  Abstract: This paper proposes a transition system abstraction framework for neural network dynamical system models to enhance the model interpretability, with applications to complex dynamical systems such as human behavior learning and verification. To begin with, the localized working zone will be segmented into multiple localized partitions under the data-driven Maximum Entropy (ME) partitioning method. Then, the transition matrix will be obtained based on the set-valued reachability analysis of neural networks. Finally, applications to human handwriting dynamics learning and verification are given to validate our proposed abstraction framework, which demonstrates the advantages of enhancing the interpretability of the black-box model, i.e., our proposed framework is able to abstract a data-driven neural network model into a transition system, making the neural network model interpretable through verifying specifications described in Computat
    
[^113]: 基于模型等效评估的前馈神经网络压缩修复方法

    Compression Repair for Feedforward Neural Networks Based on Model Equivalence Evaluation

    [https://arxiv.org/abs/2402.11737](https://arxiv.org/abs/2402.11737)

    提出了一种基于模型等效评估的前馈神经网络压缩修复方法，通过计算两个神经网络之间的输出差异，初始化新的训练集并进行重新训练来改进压缩网络性能

    

    本文提出了一种基于两个神经网络等效评估的方法，用于修复压缩的前馈神经网络（FNNs）。在修复框架中，开发了一种新颖的神经网络等效评估方法来计算两个神经网络之间的输出差异。输出差异可以定量表征压缩过程产生的输出差异。根据计算得到的输出差异，修复方法首先为压缩网络初始化一个新的训练集，以缩小两个神经网络间的差异并改进压缩网络的性能。然后，我们通过基于训练集的重新训练来修复压缩的 FNN。我们将所开发的方法应用于 MNIST 数据集，以展示我们提出的修复方法的有效性和优势。

    arXiv:2402.11737v1 Announce Type: cross  Abstract: In this paper, we propose a method of repairing compressed Feedforward Neural Networks (FNNs) based on equivalence evaluation of two neural networks. In the repairing framework, a novel neural network equivalence evaluation method is developed to compute the output discrepancy between two neural networks. The output discrepancy can quantitatively characterize the output difference produced by compression procedures. Based on the computed output discrepancy, the repairing method first initializes a new training set for the compressed networks to narrow down the discrepancy between the two neural networks and improve the performance of the compressed network. Then, we repair the compressed FNN by re-training based on the training set. We apply our developed method to the MNIST dataset to demonstrate the effectiveness and advantages of our proposed repair method.
    
[^114]: 基于核Gibbs测度的蒙特卡罗：概率随机放牧的保证

    Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding

    [https://arxiv.org/abs/2402.11736](https://arxiv.org/abs/2402.11736)

    该论文研究了一种联合概率分布，其支持趋于最小化最坏情况误差，证明了它在最坏情况积分误差集中不等式上优于i.i.d.蒙特卡罗。

    

    Kernel herding属于一类确定性的四位数法，旨在通过再生核希尔伯特空间（RKHS）上的最坏情况积分误差。尽管有很强的实验支持，但在通常情况下，即RKHS是无限维时，证明这种最坏情况误差以比标准积分节点数量的平方根更快的速率减少是困难的。在这篇理论论文中，我们研究了一个关于积分节点的联合概率分布，其支持趋于最小化与核放牧相同的最坏情况误差。我们证明它优于i.i.d.蒙特卡罗，意味着在最坏情况积分误差上具有更紧的集中不等式。尽管尚未提高速率，但这表明了研究Gibbs测度的数学工具可以帮助理解核放牧及其变体在计算上的改进程度

    arXiv:2402.11736v1 Announce Type: new  Abstract: Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS). In spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the RKHS is infinite-dimensional. In this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. We prove that it does outperform i.i.d. Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. While not improving the rate yet, this demonstrates that the mathematical tools of the study of Gibbs measures can help understand to what extent kernel herding and its variants improve on computation
    
[^115]: 随机遗忘对稳健泛化的效果

    The Effectiveness of Random Forgetting for Robust Generalization

    [https://arxiv.org/abs/2402.11733](https://arxiv.org/abs/2402.11733)

    FOMO引入了一种新的学习范式，通过随机遗忘部分权重来调节信息并强调学习可泛化的特征，从而显著减少神经网络在对抗攻击下出现的稳健过拟合问题。

    

    深度神经网络容易受到对抗攻击，这可能损害它们的性能和准确性。对抗训练（AT）已经成为保护神经网络免受此类攻击的常见方法。然而，AT的关键挑战之一是稳健过拟合，即网络对测试数据的稳健性能随着进一步训练而恶化，从而妨碍泛化。受大脑中主动遗忘的概念启发，我们引入了一种名为“遗忘来减轻过拟合（FOMO）”的新型学习范式。FOMO在遗忘阶段随机遗忘部分权重并通过权重重新初始化调节模型的信息，然后在重新学习阶段强调学习可泛化的特征。我们在基准数据集和对抗攻击上的实验表明，FOMO通过显著减少最佳结果和最终结果之间的差距，缓解了稳健过拟合。

    arXiv:2402.11733v1 Announce Type: cross  Abstract: Deep neural networks are susceptible to adversarial attacks, which can compromise their performance and accuracy. Adversarial Training (AT) has emerged as a popular approach for protecting neural networks against such attacks. However, a key challenge of AT is robust overfitting, where the network's robust performance on test data deteriorates with further training, thus hindering generalization. Motivated by the concept of active forgetting in the brain, we introduce a novel learning paradigm called "Forget to Mitigate Overfitting (FOMO)". FOMO alternates between the forgetting phase, which randomly forgets a subset of weights and regulates the model's information through weight reinitialization, and the relearning phase, which emphasizes learning generalizable features. Our experiments on benchmark datasets and adversarial attacks show that FOMO alleviates robust overfitting by significantly reducing the gap between the best and last
    
[^116]: Prospector Heads:大规模模型和数据的广义特征归因

    Prospector Heads: Generalized Feature Attribution for Large Models & Data

    [https://arxiv.org/abs/2402.11729](https://arxiv.org/abs/2402.11729)

    Prospector heads是一种高效且可解释的基于特征归因的替代方法，它可以应用于任何编码器和任何数据形态，并且通过对不同数据形态的实验，表现优越于传统方法。

    

    特征归因是一种定位输入数据中与分类相关的区域的能力，对于科学和生物医学领域的机器学习模型而言，这是一种重要的能力。当前的特征归因方法依赖于“解释”端到端分类器的预测，存在特征定位不精确以及由于计算挑战而无法在小样本尺寸和高维数据集上使用的问题。我们引入了探寻者头部（prospector heads），这是一种高效且可解释的基于特征归因的替代方法，可以应用于任何编码器和任何数据形态。通过对序列（文本）、图像（病理学）和图（蛋白质结构）的实验，探寻者头部在模态之间进行概括，表现优于基线归因方法，平均局部化AUPRC得分提升了高达49点。我们还演示了探寻者头部如何实现了改进的解释能力。

    arXiv:2402.11729v1 Announce Type: cross  Abstract: Feature attribution, the ability to localize regions of the input data that are relevant for classification, is an important capability for machine learning models in scientific and biomedical domains. Current methods for feature attribution, which rely on "explaining" the predictions of end-to-end classifiers, suffer from imprecise feature localization and are inadequate for use with small sample sizes and high-dimensional datasets due to computational challenges. We introduce prospector heads, an efficient and interpretable alternative to explanation-based methods for feature attribution that can be applied to any encoder and any data modality. Prospector heads generalize across modalities through experiments on sequences (text), images (pathology), and graphs (protein structures), outperforming baseline attribution methods by up to 49 points in mean localization AUPRC. We also demonstrate how prospector heads enable improved interpr
    
[^117]: 金融领域的数字化索赔检测：一个新的金融数据集、弱监督模型和市场分析

    Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis

    [https://arxiv.org/abs/2402.11728](https://arxiv.org/abs/2402.11728)

    本研究探讨了分析师报告和盈利电话中的索赔对金融市场回报的影响，并构建了一个新的金融数据集用于索赔检测任务。提出了一种融入主题专家知识的新型弱监督模型，通过构建一种新的度量“乐观主义”展示了模型的实际效用。

    

    在本文中，我们研究了分析师报告和盈利电话中的索赔对金融市场回报的影响，将它们视为上市公司重要的季度事件。为了进行全面的分析，我们构建了一个新的金融数据集，用于金融领域的索赔检测任务。我们在该数据集上对各种语言模型进行了基准测试，并提出了一种融入主题专家（SMEs）知识的新型弱监督模型，在聚合函数中超越了现有方法。此外，我们通过构建一种新的度量“乐观主义”展示了我们提出的模型的实际效用。我们还观察到盈利惊喜和回报对我们的乐观主义度量的依赖。我们的数据集、模型和代码将在GitHub和Hugging Face上公开（遵循CC BY 4.0许可）。

    arXiv:2402.11728v1 Announce Type: new  Abstract: In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.
    
[^118]: 可逆傅立叶神经算子处理前向和反问题

    Invertible Fourier Neural Operators for Tackling Both Forward and Inverse Problems

    [https://arxiv.org/abs/2402.11722](https://arxiv.org/abs/2402.11722)

    本文提出了可逆傅立叶神经算子(iFNO)，通过设计可逆傅立叶块和集成变分自动编码器，实现同时处理前向与反向问题的能力，为双向任务的学习提供有效的参数共享和信息交换，克服了不适定性、数据短缺和噪声等挑战。

    

    傅立叶神经算子（FNO）是一种流行的算子学习方法，已在许多任务中表现出色。然而，FNO主要用于前向预测，而许多应用程序依赖于解决反问题。在本文中，我们提出了一种可逆傅立叶神经算子（iFNO），旨在解决前向和反向问题。我们在潜在通道空间中设计了一系列可逆傅立叶块，以分享模型参数，有效交换信息，并相互正规化双向任务的学习。我们集成了变分自动编码器以捕获输入空间内在结构，并实现后验推断，以克服不适定性、数据短缺、噪声等挑战。我们开发了一个三步过程，用于预训练和微调以实现高效训练。对五个基准问题的评估已经证明...

    arXiv:2402.11722v1 Announce Type: new  Abstract: Fourier Neural Operator (FNO) is a popular operator learning method, which has demonstrated state-of-the-art performance across many tasks. However, FNO is mainly used in forward prediction, yet a large family of applications rely on solving inverse problems. In this paper, we propose an invertible Fourier Neural Operator (iFNO) that tackles both the forward and inverse problems. We designed a series of invertible Fourier blocks in the latent channel space to share the model parameters, efficiently exchange the information, and mutually regularize the learning for the bi-directional tasks. We integrated a variational auto-encoder to capture the intrinsic structures within the input space and to enable posterior inference so as to overcome challenges of illposedness, data shortage, noises, etc. We developed a three-step process for pre-training and fine tuning for efficient training. The evaluations on five benchmark problems have demonst
    
[^119]: 在广义朗之万方程中学习记忆核

    Learning Memory Kernels in Generalized Langevin Equations

    [https://arxiv.org/abs/2402.11705](https://arxiv.org/abs/2402.11705)

    提出一种学习广义朗之万方程中记忆核的新方法，通过正则化Prony方法估计相关函数并在Sobolev范数Loss函数和RKHS正则化下实现回归，在指数加权的$L^2$空间内获得改进性能，对比其他回归估计器展示了其优越性。

    

    我们引入了一种新颖的方法来学习广义朗之万方程中的记忆核。该方法最初利用正则化Prony方法从轨迹数据中估计相关函数，然后通过基于Sobolev范数的回归和RKHS正则化来进行回归。我们的方法保证在指数加权的$L^2$空间内获得了改进的性能，核估计误差受控于估计相关函数的误差。我们通过数值示例展示了我们的估计器相对于依赖于$L^2$损失函数的其他回归估计器以及从逆拉普拉斯变换推导出的估计器的优越性，这些示例突显了我们的估计器在各种权重参数选择上的持续优势。此外，我们提供了包括力和漂移项在方程中的应用示例。

    arXiv:2402.11705v1 Announce Type: cross  Abstract: We introduce a novel approach for learning memory kernels in Generalized Langevin Equations. This approach initially utilizes a regularized Prony method to estimate correlation functions from trajectory data, followed by regression over a Sobolev norm-based loss function with RKHS regularization. Our approach guarantees improved performance within an exponentially weighted $L^2$ space, with the kernel estimation error controlled by the error in estimated correlation functions. We demonstrate the superiority of our estimator compared to other regression estimators that rely on $L^2$ loss functions and also an estimator derived from the inverse Laplace transform, using numerical examples that highlight its consistent advantage across various weight parameter selections. Additionally, we provide examples that include the application of force and drift terms in the equation.
    
[^120]: ChatGPT是否能支持开发者？对大型语言模型用于代码生成的实证评估

    Can ChatGPT Support Developers? An Empirical Evaluation of Large Language Models for Code Generation

    [https://arxiv.org/abs/2402.11702](https://arxiv.org/abs/2402.11702)

    大型语言模型在代码生成方面表现出显著能力，但目前主要用于展示概念或提供示例，需要进一步改进才能实现生产就绪代码。

    

    大型语言模型（LLMs）已经展示出在代码生成方面的显著能力，许多先前的研究显示它们在各种开发场景中具有很大潜力。然而，这些研究主要提供了在研究环境中的评估，这在理解LLM在实际世界中能有效支持开发者方面留下了显著的空白。为了解决这个问题，我们对DevGPT中的对话进行了实证分析，这是从开发者与ChatGPT的对话中收集的数据集（通过GitHub等平台上的Share Link功能捕获）。我们的实证结果表明，目前使用LLM生成的代码的实践通常仅限于展示高层概念或提供文档中的示例，而不是作为可用于生产的代码。这些发现表明，在LLM代码生成方面还需要大量未来工作才能使其成为不可或缺的组成部分。

    arXiv:2402.11702v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated notable proficiency in code generation, with numerous prior studies showing their promising capabilities in various development scenarios. However, these studies mainly provide evaluations in research settings, which leaves a significant gap in understanding how effectively LLMs can support developers in real-world. To address this, we conducted an empirical analysis of conversations in DevGPT, a dataset collected from developers' conversations with ChatGPT (captured with the Share Link feature on platforms such as GitHub). Our empirical findings indicate that the current practice of using LLM-generated code is typically limited to either demonstrating high-level concepts or providing examples in documentation, rather than to be used as production-ready code. These findings indicate that there is much future work needed to improve LLMs in code generation before they can be integral parts o
    
[^121]: 解释伊辛模型的机器学习解决方案

    Explaining the Machine Learning Solution of the Ising Model

    [https://arxiv.org/abs/2402.11701](https://arxiv.org/abs/2402.11701)

    展示了如何通过神经网络和模型哈密顿量的对称性，解释铁磁伊辛模型的机器学习解决方案策略。

    

    尽管机器学习（ML）技术在解决涉及大维数据的问题中非常强大，但解释从拟合参数得出的结果仍然是一项至关重要且具有挑战性的任务，特别是在物理应用中。本文展示了如何实现对铁磁伊辛模型的解释，这是近年来许多机器学习研究的重点对象。通过使用一个没有任何隐藏层的神经网络（NN）以及哈密顿量的对称性来找到模型连续相变的临界温度，找到了一种解释其策略的方法。这使得在对称性未知时可以预测解决问题所需的NN的最小扩展，这也是可以解释的。

    arXiv:2402.11701v1 Announce Type: cross  Abstract: As powerful as machine learning (ML) techniques are in solving problems involving data with large dimensionality, explaining the results from the fitted parameters remains a challenging task of utmost importance, especially in physics applications. Here it is shown how this can be accomplished for the ferromagnetic Ising model, the target of many ML studies in the last years. By using a neural network (NN) without any hidden layers and the symmetry of the Hamiltonian to find the critical temperature for the continuous phase transition of the model, an explanation of its strategy is found. This allows the prediction of the minimal extension of the NN to solve the problem when the symmetry is not known, which is also explainable.
    
[^122]: 评估量子神经网络模型窃取攻击和防御的有效性

    Evaluating Efficacy of Model Stealing Attacks and Defenses on Quantum Neural Networks

    [https://arxiv.org/abs/2402.11687](https://arxiv.org/abs/2402.11687)

    本研究评估了量子神经网络模型窃取攻击的有效性，并提出了硬件变化诱发扰动和硬件与架构变化诱发扰动作为防御方法。

    

    arXiv:2402.11687v1 通报类型：跨领域 摘要：通过云托管量子机器学习（QML）模型使其面临一系列漏洞，其中最重要的是模型窃取攻击。在本研究中，我们评估了在量子计算领域中此类攻击的有效性。我们对多个数据集以及多种QML模型架构进行了全面的实验。我们的研究发现，当使用Top-$1$和Top-$k$标签分别进行训练时，模型窃取攻击可以产生达到$0.9\times$和$0.99\times$克隆测试精度的克隆模型（$k:$ num\_classes）。为了防御这些攻击，我们利用当前嘈杂硬件的独特特性扰乱受害模型输出并阻碍攻击者的训练过程。具体而言，我们提出了：1）硬件变化诱发扰动（HVIP）和 2）硬件和架构变化诱发扰动（HAVIP）

    arXiv:2402.11687v1 Announce Type: cross  Abstract: Cloud hosting of quantum machine learning (QML) models exposes them to a range of vulnerabilities, the most significant of which is the model stealing attack. In this study, we assess the efficacy of such attacks in the realm of quantum computing. We conducted comprehensive experiments on various datasets with multiple QML model architectures. Our findings revealed that model stealing attacks can produce clone models achieving up to $0.9\times$ and $0.99\times$ clone test accuracy when trained using Top-$1$ and Top-$k$ labels, respectively ($k:$ num\_classes). To defend against these attacks, we leverage the unique properties of current noisy hardware and perturb the victim model outputs and hinder the attacker's training process. In particular, we propose: 1) hardware variation-induced perturbation (HVIP) and 2) hardware and architecture variation-induced perturbation (HAVIP). Although noise and architectural variability can provide u
    
[^123]: 学习离散动力系统的拓扑结构和行为

    Learning the Topology and Behavior of Discrete Dynamical Systems

    [https://arxiv.org/abs/2402.11686](https://arxiv.org/abs/2402.11686)

    本文研究了学习黑盒离散动力系统的行为和底层拓扑结构的问题，证明了这是一个计算上难以解决的问题，并提出了在某些条件下的高效学习方法。

    

    离散动力系统通常用于模拟真实世界网络上传染的传播。现有研究已经在PAC框架下研究了在已知底层网络的条件下学习系统行为的问题。本文着眼于一个更具挑战性的情境：学习一个黑盒系统的行为和底层拓扑结构。我们证明，一般情况下，这个学习问题在计算上是难以解决的。在积极的一面，我们在PAC模型下针对某些类属于的动力系统图提出了高效的学习方法。此外，我们考虑了一个放松的情境，其中未知系统的拓扑结构部分可观测。针对这种情况，我们开发了一个高效的PAC学习器来推断系统并确定样本复杂性。最后，我们对动力系统的假设类的表达能力进行了形式化分析，其中包含了行为和拓扑结构。

    arXiv:2402.11686v1 Announce Type: new  Abstract: Discrete dynamical systems are commonly used to model the spread of contagions on real-world networks. Under the PAC framework, existing research has studied the problem of learning the behavior of a system, assuming that the underlying network is known. In this work, we focus on a more challenging setting: to learn both the behavior and the underlying topology of a black-box system. We show that, in general, this learning problem is computationally intractable. On the positive side, we present efficient learning methods under the PAC model when the underlying graph of the dynamical system belongs to some classes. Further, we examine a relaxed setting where the topology of an unknown system is partially observed. For this case, we develop an efficient PAC learner to infer the system and establish the sample complexity. Lastly, we present a formal analysis of the expressive power of the hypothesis class of dynamical systems where both the
    
[^124]: 通过非交换性学习条件不变性

    Learning Conditional Invariances through Non-Commutativity

    [https://arxiv.org/abs/2402.11682](https://arxiv.org/abs/2402.11682)

    学习非交换性条件不变性是一种更严格的优化方法，可以通过将优化导向目标特定编码器来实现对目标风险的更严格限制。

    

    条件过滤器学习算法可以根据数据语义而非目标域在评估中仅基于的方法来过滤掉特定于域的随机变量作为干扰因素。我们展示了学习条件不变性的一种经过证明的最优和样本高效的方法是将不变性准则放宽为非交换性，指向目标域。在存在领域不对称性的情况下，即当目标领域包含源领域中不包含的语义相关信息时，跨领域平均最优编码器$\varphi^*$的风险严格地被目标特定最优编码器$\Phi^*_\tau$的风险下界限制。我们证明非交换性将优化导向$\Phi^*_\tau$而非$\varphi^*$，将领域之间的$\mathcal{H}$-散度降至零，从而导致对目标风险的更严格限制。我们的理论和实验均表明非交换性可以改进风险边界。

    arXiv:2402.11682v1 Announce Type: new  Abstract: Invariance learning algorithms that conditionally filter out domain-specific random variables as distractors, do so based only on the data semantics, and not the target domain under evaluation. We show that a provably optimal and sample-efficient way of learning conditional invariances is by relaxing the invariance criterion to be non-commutatively directed towards the target domain. Under domain asymmetry, i.e., when the target domain contains semantically relevant information absent in the source, the risk of the encoder $\varphi^*$ that is optimal on average across domains is strictly lower-bounded by the risk of the target-specific optimal encoder $\Phi^*_\tau$. We prove that non-commutativity steers the optimization towards $\Phi^*_\tau$ instead of $\varphi^*$, bringing the $\mathcal{H}$-divergence between domains down to zero, leading to a stricter bound on the target risk. Both our theory and experiments demonstrate that non-commu
    
[^125]: 一种快速模拟非线性电阻网络的算法

    A Fast Algorithm to Simulate Nonlinear Resistive Networks

    [https://arxiv.org/abs/2402.11674](https://arxiv.org/abs/2402.11674)

    提出一种新颖的方法用于模拟非线性电阻网络，通过将问题转化为具有线性不等式约束的二次规划问题，并利用快速、精确的坐标下降算法进行求解。

    

    在追求能效的人工智能系统中，电阻网络作为替代传统基于GPU的神经网络的一种方式备受关注。这些网络利用电路的物理特性进行推断，并可通过局部训练技术（如平衡传播）进行优化。尽管在功耗方面具有潜在优势，但高效模拟这些电阻网络的挑战一直是评估其可扩展性的重要瓶颈，目前的方法要么局限于线性网络，要么依赖于SPICE等现实但速度较慢的电路模拟器。在假定理想电路元件的情况下，我们提出了一种新颖的非线性电阻网络模拟方法，将其构建为一个具有线性不等式约束的二次规划问题，并使用快速、精确的坐标下降算法进行求解。我们的模拟方法显著

    arXiv:2402.11674v1 Announce Type: cross  Abstract: In the quest for energy-efficient artificial intelligence systems, resistor networks are attracting interest as an alternative to conventional GPU-based neural networks. These networks leverage the physics of electrical circuits for inference and can be optimized with local training techniques such as equilibrium propagation. Despite their potential advantage in terms of power consumption, the challenge of efficiently simulating these resistor networks has been a significant bottleneck to assess their scalability, with current methods either being limited to linear networks or relying on realistic, yet slow circuit simulators like SPICE. Assuming ideal circuit elements, we introduce a novel approach for the simulation of nonlinear resistive networks, which we frame as a quadratic programming problem with linear inequality constraints, and which we solve using a fast, exact coordinate descent algorithm. Our simulation methodology signif
    
[^126]: 挑战黑匣子：对在农业和林业中CNN应用的归因图进行全面评估

    Challenging the Black Box: A Comprehensive Evaluation of Attribution Maps of CNN Applications in Agriculture and Forestry

    [https://arxiv.org/abs/2402.11670](https://arxiv.org/abs/2402.11670)

    对于在农业和林业中CNN应用的归因图进行全面评估，发现这些图表往往未能准确突出关键特征，与领域专家认为重要的特征不一致，这引发了对于其在理解神经网络决策过程中实用性的重大问题。

    

    在这项研究中，我们探讨神经网络在农业和林业中的可解释性，特别是在肥料处理分类和木材识别方面。通过对最先进的归因图进行全面评估，也称为类激活图或显著性图，解决了这些模型的不透明性，通常被认为是“黑匣子”。我们对这些归因图进行了全面的定性和定量分析，揭示了关键实际局限性。研究结果表明，归因图经常无法一致地突出关键特征，并且经常与领域专家认为重要的特征不一致。这些差异引发了关于归因图在理解神经网络决策过程中实用性的重大问题。我们的研究为农业和林业部门内归因图的可信度和实用性提供了关键见解，从而促进了一种

    arXiv:2402.11670v1 Announce Type: cross  Abstract: In this study, we explore the explainability of neural networks in agriculture and forestry, specifically in fertilizer treatment classification and wood identification. The opaque nature of these models, often considered 'black boxes', is addressed through an extensive evaluation of state-of-the-art Attribution Maps (AMs), also known as class activation maps (CAMs) or saliency maps. Our comprehensive qualitative and quantitative analysis of these AMs uncovers critical practical limitations. Findings reveal that AMs frequently fail to consistently highlight crucial features and often misalign with the features considered important by domain experts. These discrepancies raise substantial questions about the utility of AMs in understanding the decision-making process of neural networks. Our study provides critical insights into the trustworthiness and practicality of AMs within the agriculture and forestry sectors, thus facilitating a be
    
[^127]: 通过多尺度时间分解进行可解释的短期负荷预测

    Interpretable Short-Term Load Forecasting via Multi-Scale Temporal Decomposition

    [https://arxiv.org/abs/2402.11664](https://arxiv.org/abs/2402.11664)

    本文提出了一种可解释的深度学习方法，通过学习每个神经网络关注一个输入时间特征的线性组合，以及提出了一种多尺度时间序列分解方法来处理复杂的时间模式，在电力负荷预测中取得了更好的准确性。

    

    机器学习和深度学习的快速发展使得在电力系统负荷预测中实现了广泛应用，例如单变量和多变量短期负荷预测。尽管已经实现了学习负荷模式非线性和高预测准确性的强大能力，但对于电力负荷预测的典型深度学习模型的可解释性研究较少。本文提出了一种可解释的深度学习方法，该方法学习了一个线性组合的神经网络，每个神经网络都关注一个输入时间特征。我们还提出了一种多尺度时间序列分解方法来处理复杂的时间模式。在比利时中央电网负荷数据集上进行了案例研究，并且提出的模型与常用基线模型相比展现出更好的准确性。

    arXiv:2402.11664v1 Announce Type: new  Abstract: Rapid progress in machine learning and deep learning has enabled a wide range of applications in the electricity load forecasting of power systems, for instance, univariate and multivariate short-term load forecasting. Though the strong capabilities of learning the non-linearity of the load patterns and the high prediction accuracy have been achieved, the interpretability of typical deep learning models for electricity load forecasting is less studied. This paper proposes an interpretable deep learning method, which learns a linear combination of neural networks that each attends to an input time feature. We also proposed a multi-scale time series decomposition method to deal with the complex time patterns. Case studies have been carried out on the Belgium central grid load dataset and the proposed model demonstrated better accuracy compared to the frequently applied baseline model. Specifically, the proposed multi-scale temporal decompo
    
[^128]: 分层主动推断中的动态规划

    Dynamic planning in hierarchical active inference

    [https://arxiv.org/abs/2402.11658](https://arxiv.org/abs/2402.11658)

    通过研究在动态规划领域中模拟工具使用的目标，我们深入探讨了主动推断中的动态规划，该领域考虑到生物目标导向行为的两个关键方面

    

    通过动态规划，我们指的是人类大脑推断和施加与认知决策相关的运动轨迹的能力。最近的一个范式，主动推断，为生物有机体适应带来了基本见解，不断努力最小化预测误差以将自己限制在与生命兼容的状态。在过去的几年里，许多研究表明人类和动物行为可以解释为主动推断过程，无论是作为离散决策还是连续运动控制，都激发了机器人技术和人工智能中的创新解决方案。然而，文献缺乏对如何有效地在变化环境中规划行动的全面展望。我们设定了对工具使用进行建模的目标，深入研究了主动推断中的动态规划主题，牢记两个生物目标导向行为的关键方面：理解……

    arXiv:2402.11658v1 Announce Type: new  Abstract: By dynamic planning, we refer to the ability of the human brain to infer and impose motor trajectories related to cognitive decisions. A recent paradigm, active inference, brings fundamental insights into the adaptation of biological organisms, constantly striving to minimize prediction errors to restrict themselves to life-compatible states. Over the past years, many studies have shown how human and animal behavior could be explained in terms of an active inferential process -- either as discrete decision-making or continuous motor control -- inspiring innovative solutions in robotics and artificial intelligence. Still, the literature lacks a comprehensive outlook on how to effectively plan actions in changing environments. Setting ourselves the goal of modeling tool use, we delve into the topic of dynamic planning in active inference, keeping in mind two crucial aspects of biological goal-directed behavior: the capacity to understand a
    
[^129]: 将预训练语言模型与物理层通信集成

    Integrating Pre-Trained Language Model with Physical Layer Communications

    [https://arxiv.org/abs/2402.11656](https://arxiv.org/abs/2402.11656)

    提出了一个集成了物理层通信功能的实用设备间人工智能通信框架，通过端到端训练结合信道噪声以增强韧性，采用VQ-VAE实现高效稳健的通信，利用预训练Transformer提升通用性能

    

    在设备间人工智能通信的新兴领域中，设备直接通过嵌入式基础模型（如语言模型）交换信息，需要强大、高效且通用的通信框架。然而，将这些框架与现有无线系统集成并有效管理噪声和比特误差都面临着重大挑战。在本研究中，我们介绍了一个实用的设备间人工智能通信框架，集成了物理层通信功能，并通过链路级模拟器展示了其性能。我们的框架通过端到端训练结合信道噪声以增强韧性，采用向量量化变分自动编码器（VQ-VAE）实现高效稳健的通信，利用预训练编码-解码Transformer提升通用性能。在各种通信场景的模拟中，我们的框架展现出

    arXiv:2402.11656v1 Announce Type: cross  Abstract: The burgeoning field of on-device AI communication, where devices exchange information directly through embedded foundation models, such as language models (LMs), requires robust, efficient, and generalizable communication frameworks. However, integrating these frameworks with existing wireless systems and effectively managing noise and bit errors pose significant challenges. In this work, we introduce a practical on-device AI communication framework, integrated with physical layer (PHY) communication functions, demonstrated through its performance on a link-level simulator. Our framework incorporates end-to-end training with channel noise to enhance resilience, incorporates vector quantized variational autoencoders (VQ-VAE) for efficient and robust communication, and utilizes pre-trained encoder-decoder transformers for improved generalization capabilities. Simulations, across various communication scenarios, reveal that our framework
    
[^130]: 无模型$\mu$-综合: 一个非光滑优化的视角

    Model-Free $\mu$-Synthesis: A Nonsmooth Optimization Perspective

    [https://arxiv.org/abs/2402.11654](https://arxiv.org/abs/2402.11654)

    本文基于一个策略优化视角，将基于子梯度的搜索方法扩展到无模型设置，并研究了两种无模型策略优化策略的有效性。

    

    在这篇论文中，我们重新审视了在一个重要的鲁棒控制基准上的无模型策略搜索，即$\mu$-综合。在一般的输出反馈设置中，这个问题不存在凸形式，因此不太可能存在全局最优性保证。Apkarian (2011)提出了一个非凸非光滑的策略优化方法来解决这个问题，并且通过使用基于子梯度的策略搜索算法在一个基于模型的方式中生成更新方向，实现了最先进的设计结果。尽管缺乏凸性和全局最优性保证，但这些基于子梯度的策略搜索方法在实践中取得了令人印象深刻的数值结果。建立在这样一个策略优化视角之上，我们的论文将这些基于子梯度的搜索方法扩展到了一个无模型的设定。具体地，我们研究了两种无模型策略优化策略的有效性: 无模型非导数的采样

    arXiv:2402.11654v1 Announce Type: cross  Abstract: In this paper, we revisit model-free policy search on an important robust control benchmark, namely $\mu$-synthesis. In the general output-feedback setting, there do not exist convex formulations for this problem, and hence global optimality guarantees are not expected. Apkarian (2011) presented a nonconvex nonsmooth policy optimization approach for this problem, and achieved state-of-the-art design results via using subgradient-based policy search algorithms which generate update directions in a model-based manner. Despite the lack of convexity and global optimality guarantees, these subgradient-based policy search methods have led to impressive numerical results in practice. Built upon such a policy optimization persepctive, our paper extends these subgradient-based search methods to a model-free setting. Specifically, we examine the effectiveness of two model-free policy optimization strategies: the model-free non-derivative samplin
    
[^131]: 因果潜在因子模型中的双重稳健推断

    Doubly Robust Inference in Causal Latent Factor Models

    [https://arxiv.org/abs/2402.11652](https://arxiv.org/abs/2402.11652)

    提出了一种双重稳健的估计量框架，可以在现代数据丰富的环境中估计存在未观察混杂因素下平均处理效应，具有良好的有限样本和渐近性质，并在参数速率下将其误差收敛为零均值高斯分布。

    

    本文介绍了一种在现代数据丰富环境中估计存在未观察混杂因素下的平均处理效应的新框架，该环境具有大量单位和结果。所提出的估计量是双重稳健的，结合了结果填补、倒数概率加权以及一种用于矩阵补全的新型交叉配对程序。我们推导了有限样本和渐近保证，并展示了新估计量的误差收敛到参数速率下的零均值高斯分布。模拟结果展示了本文分析的估计量的形式特性的实际相关性。

    arXiv:2402.11652v1 Announce Type: cross  Abstract: This article introduces a new framework for estimating average treatment effects under unobserved confounding in modern data-rich environments featuring large numbers of units and outcomes. The proposed estimator is doubly robust, combining outcome imputation, inverse probability weighting, and a novel cross-fitting procedure for matrix completion. We derive finite-sample and asymptotic guarantees, and show that the error of the new estimator converges to a mean-zero Gaussian distribution at a parametric rate. Simulation results demonstrate the practical relevance of the formal properties of the estimators analyzed in this article.
    
[^132]: 程序化强化学习的理论基础

    Theoretical foundations for programmatic reinforcement learning

    [https://arxiv.org/abs/2402.11650](https://arxiv.org/abs/2402.11650)

    本文旨在探讨程序化强化学习的理论基础，给出了对程序化强化学习中关键问题的初步回答

    

    强化学习领域致力于在未知的随机环境中学习最优策略的算法。程序化强化学习研究将策略表示为程序，即涉及控制循环等高阶构造。尽管在机器学习和形式方法交叉领域吸引了很多关注，但在程序化强化学习的理论方面知之甚少：什么样的程序化策略是好的？最优程序化策略有多大？我们如何学习它们？本文的目标是首次回答这些问题，启动对程序化强化学习的理论研究。

    arXiv:2402.11650v1 Announce Type: new  Abstract: The field of Reinforcement Learning (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments. Programmatic RL studies representations of policies as programs, meaning involving higher order constructs such as control loops. Despite attracting a lot of attention at the intersection of the machine learning and formal methods communities, very little is known on the theoretical front about programmatic RL: what are good classes of programmatic policies? How large are optimal programmatic policies? How can we learn them? The goal of this paper is to give first answers to these questions, initiating a theoretical study of programmatic RL.
    
[^133]: 从大规模语言模型的视角探索多功能图学习方法

    Towards Versatile Graph Learning Approach: from the Perspective of Large Language Models

    [https://arxiv.org/abs/2402.11641](https://arxiv.org/abs/2402.11641)

    本文提出了一种利用大规模语言模型设计多功能图学习方法的新概念原型，重点关注“在哪里”和“如何”的角度。

    

    图结构数据是常用的，并在现实世界中有广泛的应用场景。面对多样的学习任务、图领域和复杂的图学习过程，传统的设计多功能图学习方法对人类专家提出挑战。本文提出了一个新颖的概念原型，用于设计具有大规模语言模型（LLMs）的多功能图学习方法，特别关注“在哪里”和“如何”的角度。从“在哪里”的角度，我们总结了四个关键的图学习过程，包括任务定义、图数据特征工程、模型选择与优化、部署与服务。然后，我们探讨了LLMs在这些过程中的应用场景。在“如何”的角度，

    arXiv:2402.11641v1 Announce Type: new  Abstract: Graph-structured data are the commonly used and have wide application scenarios in the real world. For these diverse applications, the vast variety of learning tasks, graph domains, and complex graph learning procedures present challenges for human experts when designing versatile graph learning approaches. Facing these challenges, large language models (LLMs) offer a potential solution due to the extensive knowledge and the human-like intelligence. This paper proposes a novel conceptual prototype for designing versatile graph learning methods with LLMs, with a particular focus on the ``where'' and ``how'' perspectives. From the ``where'' perspective, we summarize four key graph learning procedures, including task definition, graph data feature engineering, model selection and optimization, deployment and serving. We then explore the application scenarios of LLMs in these procedures across a wider spectrum. In the ``how'' perspective, we
    
[^134]: 具有Transformer的上下文学习：Softmax注意力适应函数Lipschitz性质

    In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness

    [https://arxiv.org/abs/2402.11639](https://arxiv.org/abs/2402.11639)

    本文研究了在上下文学习框架中，Softmax注意力在适应预训练任务背景时的作用，发现注意力单元学会与Lipschitzness降低和标签噪声增加相关的窗口调整，以及在低维、线性问题上学会在推理前进行适当空间的投影。

    

    Transformer的一个显著特性是其能够进行上下文学习（ICL），在这种机器学习框架中，学习者在推理过程中通过某些数据隐式地被呈现一个新领域的背景，并被要求在该背景下进行预测。在这种情况下，学习者必须在没有额外训练的情况下适应背景。本文探讨了Softmax注意力在一个ICL设置中的作用，其中每个背景都编码了一个回归任务。我们展示了一个注意力单元学习一个窗口，用于实现一个适应于预训练任务的最近邻预测器。具体地，我们展示了这个窗口随着Lipschitzness的降低和标签噪声的增加而扩大。我们还展示了在低秩、线性问题上，注意力单元在推理之前学会了投影到适当的子空间。此外，我们还展示了这种适应性关键地依赖于softmax激活函数。

    arXiv:2402.11639v1 Announce Type: new  Abstract: A striking property of transformers is their ability to perform in-context learning (ICL), a machine learning framework in which the learner is presented with a novel context during inference implicitly through some data, and tasked with making a prediction in that context. As such that learner must adapt to the context without additional training. We explore the role of softmax attention in an ICL setting where each context encodes a regression task. We show that an attention unit learns a window that it uses to implement a nearest-neighbors predictor adapted to the landscape of the pretraining tasks. Specifically, we show that this window widens with decreasing Lipschitzness and increasing label noise in the pretraining tasks. We also show that on low-rank, linear problems, the attention unit learns to project onto the appropriate subspace before inference. Further, we show that this adaptivity relies crucially on the softmax activatio
    
[^135]: 用假用户对联邦推荐系统进行毒化

    Poisoning Federated Recommender Systems with Fake Users

    [https://arxiv.org/abs/2402.11637](https://arxiv.org/abs/2402.11637)

    本研究介绍了一种无需额外信息的新型基于假用户的毒化攻击方法，用于在联邦推荐系统中执行推广攻击。

    

    联邦推荐是联邦学习中一个显著的用例，但仍然容易受到各种攻击，从用户到服务器端的漏洞。毒化攻击在用户端攻击中特别引人注目，因为参与者上传恶意模型更新来欺骗全局模型，通常意图提升或降低特定目标项。本研究探讨了在联邦推荐系统中执行推广攻击的策略。当前对联邦推荐系统的毒化攻击通常依赖于额外信息，如真实用户的本地训练数据或物品流行度。然而，攻击者很难获得这些信息。因此，有必要开发一种攻击，除了从服务器获取的物品嵌入之外，不需要额外信息。在本文中，我们介绍了一种名为PoisonFRS的新型基于假用户的毒化攻击，用于促销

    arXiv:2402.11637v1 Announce Type: cross  Abstract: Federated recommendation is a prominent use case within federated learning, yet it remains susceptible to various attacks, from user to server-side vulnerabilities. Poisoning attacks are particularly notable among user-side attacks, as participants upload malicious model updates to deceive the global model, often intending to promote or demote specific targeted items. This study investigates strategies for executing promotion attacks in federated recommender systems.   Current poisoning attacks on federated recommender systems often rely on additional information, such as the local training data of genuine users or item popularity. However, such information is challenging for the potential attacker to obtain. Thus, there is a need to develop an attack that requires no extra information apart from item embeddings obtained from the server. In this paper, we introduce a novel fake user based poisoning attack named PoisonFRS to promote the
    
[^136]: 离散神经算法推理

    Discrete Neural Algorithmic Reasoning

    [https://arxiv.org/abs/2402.11628](https://arxiv.org/abs/2402.11628)

    这项工作提出了一种强制神经推理器维护执行轨迹作为有限预定义状态组合的方法，通过对算法状态转换的监督训练，使模型能够与原始算法完美对齐，并在基准测试中取得了完美的测试成绩。

    

    神经算法推理旨在通过学习模仿经典算法的执行来捕捉神经网络中的计算。尽管常见的架构足够表达正确的模型在权重空间中，但当前的神经推理器在处理超出分布数据时面临泛化困难。另一方面，经典计算不受分布变化的影响，因为它们可以描述为离散计算状态之间的转换。在这项工作中，我们提出强制神经推理器将执行轨迹作为有限预定义状态的组合进行维护。通过对算法状态转换的监督训练，这种模型能够与原始算法完美对齐。为了证明这一点，我们在SALSA-CLRS基准测试上评估我们的方法，在那里我们为所有任务获得了完美的测试成绩。此外，所提出的架构选择使我们能够证明...

    arXiv:2402.11628v1 Announce Type: new  Abstract: Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the 
    
[^137]: 逻辑闭环：揭示大型视觉-语言模型中的对象幻觉

    Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models

    [https://arxiv.org/abs/2402.11622](https://arxiv.org/abs/2402.11622)

    提出了一种基于逻辑闭环的框架（LogicCheckGPT），利用大型视觉-语言模型本身来检测和减轻对象幻觉。

    

    对象幻觉一直是阻碍大型视觉-语言模型（LVLMs）更广泛应用的软肋。对象幻觉是指LVLMs在图像中声称不存在的对象的现象。为了减轻对象幻觉，已经提出了指导调整和基于外部模型的检测方法，这两种方法要么需要大规模的计算资源，要么依赖于外部模型的检测结果。然而，仍然存在一个未深入探讨的领域，即利用LVLM本身来减轻对象幻觉。在这项工作中，我们采用了这样的直觉，即LVLM倾向于对存在的对象做出逻辑一致的反应，但对幻觉对象做出不一致的反应。因此，我们提出了基于逻辑闭环的对象幻觉检测和减轻框架，即LogicCheckGPT。具体来说，我们设计了逻辑一致性探测来提出具有逻辑性的问题。

    arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
    
[^138]: 自进化自动编码器嵌入的 Q 网络

    Self-evolving Autoencoder Embedded Q-Network

    [https://arxiv.org/abs/2402.11604](https://arxiv.org/abs/2402.11604)

    提出了一种将自进化自动编码器嵌入 Q 网络以增强强化学习代理探索能力的新方法

    

    在序贯决策任务领域，强化学习代理的探索能力对通过与环境的交互获得高奖励至关重要。为增强这一关键能力，我们提出了一种新方法 SAQN，在其中将一个自进化自动编码器（SA）嵌入到一个 Q 网络（QN）中。在 SAQN 中，自进化自动编码器架构随着代理探索环境而调整和进化。这种进化使得自动编码器能够捕捉各种原始观测并有效地在其潜在空间中表示它们。通过利用从编码器生成的潜在空间中提取的分解状态，QN 被训练以确定改善奖励的最佳行动。在自动编码器架构的进化过程中，采用偏差-方差调节策略来引导强化学习代理做出最佳反应。这个策略涉及两个关键组成部分：（i）促进

    arXiv:2402.11604v1 Announce Type: new  Abstract: In the realm of sequential decision-making tasks, the exploration capability of a reinforcement learning (RL) agent is paramount for achieving high rewards through interactions with the environment. To enhance this crucial ability, we propose SAQN, a novel approach wherein a self-evolving autoencoder (SA) is embedded with a Q-Network (QN). In SAQN, the self-evolving autoencoder architecture adapts and evolves as the agent explores the environment. This evolution enables the autoencoder to capture a diverse range of raw observations and represent them effectively in its latent space. By leveraging the disentangled states extracted from the encoder generated latent space, the QN is trained to determine optimal actions that improve rewards. During the evolution of the autoencoder architecture, a bias-variance regulatory strategy is employed to elicit the optimal response from the RL agent. This strategy involves two key components: (i) fost
    
[^139]: 在线机器学习中的超参数调优简化--spotRiverGUI

    Simplifying Hyperparameter Tuning in Online Machine Learning -- The spotRiverGUI

    [https://arxiv.org/abs/2402.11594](https://arxiv.org/abs/2402.11594)

    `spotRiverGUI`是一个为在线机器学习模型进行超参数调优的图形用户界面，简化了用户手动搜索最佳超参数设置的过程。

    

    批量机器学习(BML)在处理大量流数据时存在内存、数据流漂移处理和处理新的未知数据等方面的限制。在线机器学习(OML)是BML的替代方案，能够以顺序方式处理数据，特别适用于数据流。`river`包是一个Python OML库，提供了各种在线学习算法，包括分类、回归、聚类、异常检测等。`spotRiver`包为OML模型提供了超参数调优的框架。`spotRiverGUI`是`spotRiver`包的图形用户界面，为用户提供了从强大的算法中选择最优超参数设置的便利。

    arXiv:2402.11594v1 Announce Type: cross  Abstract: Batch Machine Learning (BML) reaches its limits when dealing with very large amounts of streaming data. This is especially true for available memory, handling drift in data streams, and processing new, unknown data. Online Machine Learning (OML) is an alternative to BML that overcomes the limitations of BML. OML is able to process data in a sequential manner, which is especially useful for data streams. The `river` package is a Python OML-library, which provides a variety of online learning algorithms for classification, regression, clustering, anomaly detection, and more. The `spotRiver` package provides a framework for hyperparameter tuning of OML models. The `spotRiverGUI` is a graphical user interface for the `spotRiver` package. The `spotRiverGUI` releases the user from the burden of manually searching for the optimal hyperparameter setting. After the data is provided, users can compare different OML algorithms from the powerful `
    
[^140]: 重新探讨零阶优化在内存高效LLM微调中的应用：一个基准研究

    Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark

    [https://arxiv.org/abs/2402.11592](https://arxiv.org/abs/2402.11592)

    本研究提出了一种不使用反向传播的零阶优化方法，用于降低LLM微调中的内存成本，通过全面的基准研究扩展了对不同的ZO优化技术的探索。

    

    在自然语言处理（NLP）领域的不断发展中，使用SGD和Adam等一阶（FO）优化器微调预训练的大型语言模型（LLMs）已成为标准。然而，随着LLMs体积的增长，由于FO梯度计算的反向传播（BP）带来的巨大内存开销构成了一个重大挑战。解决这个问题至关重要，尤其对于内存效率至关重要的设备端训练等应用。本文提出了一种转向不使用BP的零阶（ZO）优化的方法，用于在LLM微调过程中降低内存成本，构建在MeZO提出的概念基础上。与传统的ZO梯度下降方法不同，我们的工作将探索扩展到更广泛的ZO优化技术，通过全面的、首次推出的基准研究跨越五个LLM系列（Roberta，OPT，LLaMA，Vicuna，Mistral），三种任务复杂性和五种微调方案。

    arXiv:2402.11592v1 Announce Type: new  Abstract: In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes
    
[^141]: PolypNextLSTM：使用ConvNext和ConvLSTM的轻量级快速息肉视频分割网络

    PolypNextLSTM: A lightweight and fast polyp video segmentation network using ConvNext and ConvLSTM

    [https://arxiv.org/abs/2402.11585](https://arxiv.org/abs/2402.11585)

    PolypNextLSTM是一个轻量且快速的息肉视频分割网络，使用ConvNext和ConvLSTM，最大的创新在于参数最少且速度最快，性能超越了五种先进的基于图像和视频的深度学习模型。

    

    通常用于息肉分割的单图像UNet架构缺乏临床医生在诊断息肉时从视频数据中获得的时间洞察。为了更忠实地反映临床实践，我们提出的解决方案PolypNextLSTM利用基于视频的深度学习，利用时间信息实现了卓越的分割性能，参数开销最小，可能适用于边缘设备。PolypNextLSTM采用类似UNet的结构，ConvNext-Tiny作为其主干，策略性地省略最后两层以减少参数开销。我们的时间融合模块，一个卷积长短期记忆（ConvLSTM），有效地利用时间特征。我们的主要创新在于PolypNextLSTM，在参数上最瘦且速度最快，超越了五种最先进的基于图像和视频的深度学习模型的性能。SUN-SEG数据集的评估跨越了

    arXiv:2402.11585v1 Announce Type: cross  Abstract: Commonly employed in polyp segmentation, single image UNet architectures lack the temporal insight clinicians gain from video data in diagnosing polyps. To mirror clinical practices more faithfully, our proposed solution, PolypNextLSTM, leverages video-based deep learning, harnessing temporal information for superior segmentation performance with the least parameter overhead, making it possibly suitable for edge devices. PolypNextLSTM employs a UNet-like structure with ConvNext-Tiny as its backbone, strategically omitting the last two layers to reduce parameter overhead. Our temporal fusion module, a Convolutional Long Short Term Memory (ConvLSTM), effectively exploits temporal features. Our primary novelty lies in PolypNextLSTM, which stands out as the leanest in parameters and the fastest model, surpassing the performance of five state-of-the-art image and video-based deep learning models. The evaluation of the SUN-SEG dataset spans 
    
[^142]: 图上的持续学习：挑战、解决方案和机会

    Continual Learning on Graphs: Challenges, Solutions, and Opportunities

    [https://arxiv.org/abs/2402.11565](https://arxiv.org/abs/2402.11565)

    对图上持续学习进行了全面评估和分类，弥补了欧几里得数据上持续学习研究的不足。

    

    近来，图数据上的持续学习因旨在解决现有任务上的灾难性遗忘问题以及将顺序更新的模型适应新出现的图任务而引起了极大关注。虽然人们努力总结了在欧几里得数据（例如图像和文本）上持续学习研究的进展，但对于图上的持续学习，即持续图学习（CGL）或终身图学习，仍然需要进行系统性审查。图数据在数据结构和应用场景方面要复杂得多，这使得CGL任务设置、模型设计和应用变得极具挑战性。为了弥合差距，我们通过阐明不同的任务设置并根据特性对现有的持续图学习（CGL）算法进行分类，提供了对现有持续图学习（CGL）算法的全面评估。我们将CGL方法与传统的持续学习方法进行比较。

    arXiv:2402.11565v1 Announce Type: cross  Abstract: Continual learning on graph data has recently attracted paramount attention for its aim to resolve the catastrophic forgetting problem on existing tasks while adapting the sequentially updated model to newly emerged graph tasks. While there have been efforts to summarize progress on continual learning research over Euclidean data, e.g., images and texts, a systematic review of progress in continual learning on graphs, a.k.a, continual graph learning (CGL) or lifelong graph learning, is still demanding. Graph data are far more complex in terms of data structures and application scenarios, making CGL task settings, model designs, and applications extremely challenging. To bridge the gap, we provide a comprehensive review of existing continual graph learning (CGL) algorithms by elucidating the different task settings and categorizing the existing methods based on their characteristics. We compare the CGL methods with traditional continual
    
[^143]: 时间解耦对比扩散模型用于时空插补

    Temporal Disentangled Contrastive Diffusion Model for Spatiotemporal Imputation

    [https://arxiv.org/abs/2402.11558](https://arxiv.org/abs/2402.11558)

    基于深度学习的时间解耦对比扩散模型应用于时空插补，旨在通过生成模型来提高预测效能。

    

    arXiv:2402.11558v1 发布类型: 新内容 摘要: 时空数据分析在各个领域至关重要，包括交通、气象和医疗保健。然而，在现实场景中收集的数据往往因传感器故障和网络传输错误而不完整。时空插补旨在通过利用观测数据中存在的固有空间和时间依赖关系来预测缺失值。传统方法主要依赖于经典统计和机器学习技术，通常不足够，特别是当数据未能符合严格的分布假设时。相比之下，最近基于深度学习的方法，利用图形和循环神经网络，已经表现出增强的功效。然而，这些方法容易积累误差。生成模型越来越多地被采用，以规避依赖于潜在不准确的历史插补值进行未来预测的情况。

    arXiv:2402.11558v1 Announce Type: new  Abstract: Spatiotemporal data analysis is pivotal across various domains, including transportation, meteorology, and healthcare. However, the data collected in real-world scenarios often suffers incompleteness due to sensor malfunctions and network transmission errors. Spatiotemporal imputation endeavours to predict missing values by exploiting the inherent spatial and temporal dependencies present in the observed data. Traditional approaches, which rely on classical statistical and machine learning techniques, are often inadequate, particularly when the data fails to meet strict distributional assumptions. In contrast, recent deep learning-based methods, leveraging graph and recurrent neural networks, have demonstrated enhanced efficacy. Nonetheless, these approaches are prone to error accumulation. Generative models have been increasingly adopted to circumvent the reliance on potentially inaccurate historical imputed values for future prediction
    
[^144]: 基于样条拟插值的经验密度估计及其在Copulas聚类建模中的应用

    Empirical Density Estimation based on Spline Quasi-Interpolation with applications to Copulas clustering modeling

    [https://arxiv.org/abs/2402.11552](https://arxiv.org/abs/2402.11552)

    本文提出了使用样条拟插值进行单变量密度估计，并将其应用于聚类建模，为构建适用的多元分布提供了新方法。

    

    密度估计是各个领域内用于建模和理解数据基础分布的基本技术。密度估计的主要目标是估计随机变量的概率密度函数。在处理单变量或多变量数据时，这一过程尤为重要，对于聚类、异常检测和生成建模等任务至关重要。本文提出了使用样条拟插值来近似单变量密度，并将其应用于聚类建模的方法。所使用的聚类技术基于构建适用的多元分布，这取决于对单变量经验密度（边际密度）的估计。该逼近是通过使用提出的样条拟插值实现的，同时用于建模所寻求的聚类划分的联合分布。

    arXiv:2402.11552v1 Announce Type: cross  Abstract: Density estimation is a fundamental technique employed in various fields to model and to understand the underlying distribution of data. The primary objective of density estimation is to estimate the probability density function of a random variable. This process is particularly valuable when dealing with univariate or multivariate data and is essential for tasks such as clustering, anomaly detection, and generative modeling. In this paper we propose the mono-variate approximation of the density using spline quasi interpolation and we applied it in the context of clustering modeling. The clustering technique used is based on the construction of suitable multivariate distributions which rely on the estimation of the monovariate empirical densities (marginals). Such an approximation is achieved by using the proposed spline quasi-interpolation, while the joint distributions to model the sought clustering partition is constructed with the 
    
[^145]: PASCL: 带扰动增强的监督对比学习用于粒子衰变重建

    PASCL: Supervised Contrastive Learning with Perturbative Augmentation for Particle Decay Reconstruction

    [https://arxiv.org/abs/2402.11538](https://arxiv.org/abs/2402.11538)

    提出了PASCL算法，在高能物理学中使用带扰动增强的监督对比学习，利用图结构重建粒子衰变层次树。

    

    在高能物理学中，在碰撞事件中产生的粒子以层次树结构的形式衰变，只有最终的衰变产物可以通过探测器观测到。然而，可能的树结构巨大的排列空间使得在给定一组最终粒子的情况下重建实际衰变过程具有挑战性。为了更好地分析层次树结构，我们提出了一个基于图的深度学习模型来推断树结构以重建碰撞事件。具体地，我们使用一种紧凑的矩阵表示，称为最低公共祖先代数（LCAG）矩阵，来编码粒子衰变树结构。然后，我们引入了一种应用于节点特征的扰动增强技术，旨在模拟实验不确定性并增加数据多样性。此外，我们提出了一种监督图对比学习算法，利用来自多个粒子之间关系的信息

    arXiv:2402.11538v1 Announce Type: cross  Abstract: In high-energy physics, particles produced in collision events decay in a format of a hierarchical tree structure, where only the final decay products can be observed using detectors. However, the large combinatorial space of possible tree structures makes it challenging to recover the actual decay process given a set of final particles. To better analyse the hierarchical tree structure, we propose a graph-based deep learning model to infer the tree structure to reconstruct collision events. In particular, we use a compact matrix representation termed as lowest common ancestor generations (LCAG) matrix, to encode the particle decay tree structure. Then, we introduce a perturbative augmentation technique applied to node features, aiming to mimic experimental uncertainties and increase data diversity. We further propose a supervised graph contrastive learning algorithm to utilize the information of inter-particle relations from multiple 
    
[^146]: 用RLHF推进翻译偏好建模：迈向成本效益解决方案

    Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution

    [https://arxiv.org/abs/2402.11525](https://arxiv.org/abs/2402.11525)

    提出了一种利用强化学习和人类反馈（RLHF）来改进翻译质量的成本效益偏好学习策略，该策略通过优化奖励模型来区分人类和机器翻译，从而指导改进机器翻译。

    

    arXiv:2402.11525v1 公告类型：新 真实性、表达力和优雅是机器翻译中不断追求的目标。然而，传统的度量标准如BLEU并不严格符合人类对翻译质量的偏好。本文探讨了利用强化学习与人类反馈（RLHF）来提高翻译质量。收集人类对翻译之间的比较的大规模高质量数据集并不容易，尤其对于低资源语言。为解决这一问题，我们提出了一种成本效益的偏好学习策略，通过区分人类和机器翻译来优化奖励模型。通过这种方式，奖励模型学习机器翻译与人类之间的不足之处，并指导随后对机器翻译的改进。实验结果表明，RLHF可以有效地提升翻译质量，这种改进也有益于其他翻译方向。

    arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
    
[^147]: 大型语言模型驱动的异质信息网络中元结构的发现

    Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network

    [https://arxiv.org/abs/2402.11518](https://arxiv.org/abs/2402.11518)

    提出了一种利用大型语言模型驱动的元结构搜索框架，解决了手工设计元结构不易扩展以及忽视可解释性的问题

    

    异质信息网络（HIN）因能够捕捉不同类型节点之间复杂关系而日益受到青睐。元结构被提出用于识别HIN上的重要关系模式，已证明在提取丰富语义信息和促进图神经网络学习表达力表示方面有效。然而，手工设计的元结构在扩展性方面存在挑战，这引起了广泛关注，以发展自动元结构搜索算法。先前的研究集中于寻找具有良好经验预测性能的元结构，而忽视了可解释性。因此，他们往往产生易于过度拟合和人类难以理解的元结构。为了解决这个问题，我们从大型语言模型（LLM）新兴的推理能力中获取启示。我们提出了一种新颖的REasoning meta-STRUCTure search（ReStruct）框架

    arXiv:2402.11518v1 Announce Type: new  Abstract: Heterogeneous information networks (HIN) have gained increasing popularity for being able to capture complex relations between nodes of diverse types. Meta-structure was proposed to identify important patterns of relations on HIN, which has been proven effective for extracting rich semantic information and facilitating graph neural networks to learn expressive representations. However, hand-crafted meta-structures pose challenges for scaling up, which draws wide research attention for developing automatic meta-structure search algorithms. Previous efforts concentrate on searching for meta-structures with good empirical prediction performance, overlooking explainability. Thus, they often produce meta-structures prone to overfitting and incomprehensible to humans. To address this, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose a novel REasoning meta-STRUCTure search (ReStruct) framewor
    
[^148]: 深度强化学习在流体力学中主动流控制中的最佳并行化策略

    Optimal Parallelization Strategies for Active Flow Control in Deep Reinforcement Learning-Based Computational Fluid Dynamics

    [https://arxiv.org/abs/2402.11515](https://arxiv.org/abs/2402.11515)

    该研究专注于优化深度强化学习在流体力学中主动流控制中的并行设置，通过拆解DRL框架、进行扩展性基准测试、提出混合并行化配置并优化多环境DRL训练中的I/O操作，提出了有效的并行化策略。

    

    深度强化学习（DRL）已被证明是处理高动态和非线性主动流控制（AFC）问题的一种有前途的方法。然而，与训练DRL模型相关的计算成本构成了重要的性能瓶颈。为了应对这一挑战并在高性能计算架构上实现有效的扩展，本研究侧重于优化并行设置中的基于DRL的算法。我们验证了用于AFC问题的现有最先进的DRL框架，并讨论了其效率瓶颈。随后，通过拆解整体框架，并为各个组件进行广泛的可扩展性基准测试，我们研究了各种混合并行化配置，并提出了有效的并行化策略。此外，我们优化了多环境DRL训练中的输入/输出（I/O）操作，以解决与数据移动相关的关键开销。

    arXiv:2402.11515v1 Announce Type: new  Abstract: Deep Reinforcement Learning (DRL) has emerged as a promising approach for handling highly dynamic and nonlinear Active Flow Control (AFC) problems. However, the computational cost associated with training DRL models presents a significant performance bottleneck. To address this challenge and enable efficient scaling on high-performance computing architectures, this study focuses on optimizing DRL-based algorithms in parallel settings. We validate an existing state-of-the-art DRL framework used for AFC problems and discuss its efficiency bottlenecks. Subsequently, by deconstructing the overall framework and conducting extensive scalability benchmarks for individual components, we investigate various hybrid parallelization configurations and propose efficient parallelization strategies. Moreover, we refine input/output (I/O) operations in multi-environment DRL training to tackle critical overhead associated with data movement. Finally, we 
    
[^149]: URLBERT：一种用于URL分类的对比和对抗预训练模型

    URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification

    [https://arxiv.org/abs/2402.11495](https://arxiv.org/abs/2402.11495)

    URLBERT是第一个专门针对URL分类或检测任务的预训练模型，引入了自监督对比学习和虚拟对抗训练两种新颖的预训练任务，以加强模型对URL结构的理解和提高从URL中提取语义特征的鲁棒性。

    

    arXiv：2402.11495v1 发表类型：跨领域摘要：URL在理解和分类网络内容方面发挥着至关重要的作用，特别是在与安全控制和在线推荐相关的任务中。尽管预训练模型目前在各个领域占据主导地位，但URL分析领域仍缺乏专门的预训练模型。为填补这一空白，本文介绍了URLBERT，这是第一个应用于各种URL分类或检测任务的预训练表示学习模型。我们首先在数十亿个URL的语料库上训练了一个URL标记器，以解决URL数据的标记化问题。此外，我们提出了两种新颖的预训练任务：（1）自监督对比学习任务，通过区分相同URL的不同变体来增强模型对URL结构的理解和对类别差异的捕捉；（2）虚拟对抗训练，旨在提高模型从URL中提取语义特征的鲁棒性。最后，我们提出了

    arXiv:2402.11495v1 Announce Type: cross  Abstract: URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online recommendations. While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained representation learning model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data tokenization. Additionally, we propose two novel pre-training tasks: (1) self-supervised contrastive learning tasks, which strengthen the model's understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual adversarial training, aimed at improving the model's robustness in extracting semantic features from URLs. Finally, our proposed 
    
[^150]: 通过因果干预实现图形的离群分布泛化

    Graph Out-of-Distribution Generalization via Causal Intervention

    [https://arxiv.org/abs/2402.11494](https://arxiv.org/abs/2402.11494)

    GNN在离群分布泛化中的失败关键在于来自环境的潜在混杂偏差，因此引入了一个简单而原则性的方法来训练稳健GNN。

    

    离群分布（OOD）泛化在图形学习中引起了越来越多的关注，因为图神经网络（GNN）在分布转移时通常会表现出性能下降。本文从自下而上的数据生成角度出发，通过因果分析揭示了一个关键观察：GNN在OOD泛化中失败的关键在于来自环境的潜在混杂偏差。后者误导模型利用自我图特征和目标节点标签之间的环境敏感相关性，导致在新的未见节点上不良泛化。基于这一分析，我们引入了一个在节点级别分布转移下训练稳健GNN的概念简单而又原则性的方法，而不需要环境的先验知识。

    arXiv:2402.11494v1 Announce Type: new  Abstract: Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up data-generative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment
    
[^151]: LEIA: 利用基于实体的数据增强在语言模型中促进跨语言知识转移

    LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation

    [https://arxiv.org/abs/2402.11485](https://arxiv.org/abs/2402.11485)

    LEIA是一种语言适应调整方法，利用维基百科实体名称跨语言增强目标语言语料库，通过左到右的语言建模训练，显著提高了各种非英语语言的表现。

    

    将基于英语的大型语言模型（LLMs）适应其他语言的操作由于跨语言转移的效率和潜力而变得越来越受欢迎。然而，现有的语言适应方法常常忽视跨语言监督的好处。在本研究中，我们介绍LEIA，一种利用跨语言对齐的维基百科实体名称的语言适应调整方法。该方法涉及使用英语实体名称增强目标语言语料库，并使用从左到右的语言建模训练模型。我们在多样的问答数据集上评估LEIA，使用7B参数的LLMs，展示了在各种非英语语言上的显著性能增益。源代码可在https://github.com/studio-ousia/leia上获得。

    arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
    
[^152]: 基于图提示学习的药物相互作用事件预测：DDIPrompt

    DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning

    [https://arxiv.org/abs/2402.11472](https://arxiv.org/abs/2402.11472)

    基于图提示学习的DDIPrompt框架旨在解决药物相互作用事件预测中的高度不平衡事件分布和罕见事件标记数据稀缺性问题。

    

    最近，由于其在建模药物分子内部和之间原子和功能团之间复杂关联方面的熟练表现，图神经网络在预测药物相互作用事件（DDI）方面变得日益普遍。然而，它们仍然受到两个重大挑战的制约：（1）高度不平衡事件分布的问题，在医学数据集中这是一个常见但关键的问题，某些相互作用被广泛地低估。这种不平衡对实现准确可靠的DDI预测构成了重大障碍。（2）罕见事件标记数据的稀缺性，在医学领域是一个普遍问题，由于数据有限，往往忽视或研究不足的罕见但潜在关键的相互作用。为此，我们提出了DDIPrompt，这是一种受最近图提示学进展启发的创新良方。我们的框架旨在解决这些问题。

    arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
    
[^153]: 在搜索训练数据与Transformer文本模型对抗性稳健性之间的相关性时的一个有趣案例

    A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models

    [https://arxiv.org/abs/2402.11469](https://arxiv.org/abs/2402.11469)

    本文研究了训练数据与模型鲁棒性之间的相关性，并提出通过提取不同特征来预测Transformer文本模型的对抗性稳健性的方法。

    

    现有研究表明，经过微调的文本Transformer模型可以实现最先进的预测性能，但也容易受到对抗文本扰动的影响。传统的对抗性评估通常在对模型进行微调之后才进行，忽略了训练数据。本文旨在证明训练数据和模型鲁棒性之间也存在着强关联。为此，我们提取了代表广泛输入微调语料库属性的13种不同特征，并用它们来预测经过微调的模型的对抗性稳健性。我们主要关注仅编码器的Transformer模型BERT和RoBERTa，并附加了BART、ELECTRA和GPT2的其他结果，为我们的论点提供多样的证据。首先，经验证明，(a)提取的特征可与轻量级分类器（如随机森林）一起有效地预测攻击成功率。

    arXiv:2402.11469v1 Announce Type: cross  Abstract: Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate 
    
[^154]: 长期时间序列预测中的吸引子记忆：混沌视角

    Attractor Memory for Long-Term Time Series Forecasting: A Chaos Perspective

    [https://arxiv.org/abs/2402.11463](https://arxiv.org/abs/2402.11463)

    Attraos模型基于混沌理论，在长期时间序列预测中利用多尺度动态记忆单元和局部演化策略，表现优异于其他LTSF方法。

    

    在长期时间序列预测（LTSF）任务中，现有的深度学习模型忽视了离散时间序列源自潜在连续动态系统的关键特征，导致缺乏外推和演化能力。 鉴别真实世界数据的混沌性质，我们的模型\textbf{\textit{Attraos}}将混沌理论融入到LTSF中，将实际时间序列视为未知高维混沌动态系统的观测。 在吸引子不变性的概念下，Attraos利用提出的多尺度动态记忆单元来记忆历史动态结构，并通过频率增强的局部演化策略进行预测。 详细的理论分析和丰富的经验证据一致表明，Attraos在主流LTSF数据集和混沌数据集上的表现优于各种LTSF方法。

    arXiv:2402.11463v1 Announce Type: cross  Abstract: In long-term time series forecasting (LTSF) tasks, existing deep learning models overlook the crucial characteristic that discrete time series originate from underlying continuous dynamic systems, resulting in a lack of extrapolation and evolution capabilities. Recognizing the chaotic nature of real-world data, our model, \textbf{\textit{Attraos}}, incorporates chaos theory into LTSF, perceiving real-world time series as observations from unknown high-dimensional chaotic dynamic systems. Under the concept of attractor invariance, Attraos utilizes the proposed multi-scale dynamic memory unit to memorize historical dynamics structure and predicts by a frequency-enhanced local evolution strategy. Detailed theoretical analysis and abundant empirical evidence consistently show that Attraos outperforms various LTSF methods on mainstream LTSF datasets and chaotic datasets.
    
[^155]: Re-Dock: 朝向具有扩散桥的灵活和现实分子对接

    Re-Dock: Towards Flexible and Realistic Molecular Docking with Diffusion Bridge

    [https://arxiv.org/abs/2402.11459](https://arxiv.org/abs/2402.11459)

    提出了一种新颖的扩散桥生成模型 Re-Dock，用于灵活和现实的分子对接，通过能量到几何映射来共同建模结合能和构象，填补了对接中的实用性和构象预测方面的差距

    

    准确预测蛋白质-配体结合结构，即分子对接任务对于药物设计至关重要，但仍然具有挑战性。尽管深度学习显示出了潜力，但现有方法通常依赖于完整蛋白质结构（对接，且在现实任务中不可达）或忽略口袋侧链构象，导致有限的实用性和不切实际的构象预测。为填补这些差距，我们引入了一个未经探索的任务，命名为柔性对接，以同时预测配体和口袋侧链的姿势，并引入了一种扩展到几何流形的新型扩散桥生成模型 Re-Dock。具体而言，我们提出了受牛顿-欧拉方程启发的能量到几何映射，以共同建模结合能和构象，以反映能量约束对接生成过程。我们在设计的基准数据集上进行了全面的实验，包括apo-dock和cross-dock d

    arXiv:2402.11459v1 Announce Type: cross  Abstract: Accurate prediction of protein-ligand binding structures, a task known as molecular docking is crucial for drug design but remains challenging. While deep learning has shown promise, existing methods often depend on holo-protein structures (docked, and not accessible in realistic tasks) or neglect pocket sidechain conformations, leading to limited practical utility and unrealistic conformation predictions. To fill these gaps, we introduce an under-explored task, named flexible docking to predict poses of ligand and pocket sidechains simultaneously and introduce Re-Dock, a novel diffusion bridge generative model extended to geometric manifolds. Specifically, we propose energy-to-geometry mapping inspired by the Newton-Euler equation to co-model the binding energy and conformations for reflecting the energy-constrained docking generative process. Comprehensive experiments on designed benchmark datasets including apo-dock and cross-dock d
    
[^156]: InfuserKI：通过Infuser引导的知识集成增强大型语言模型与知识图谱

    InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration

    [https://arxiv.org/abs/2402.11441](https://arxiv.org/abs/2402.11441)

    提出了一种Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态有效地将未知知识集成到大型语言模型中，从而缓解知识遗忘问题。

    

    大型语言模型（LLMs）在各个领域展现出卓越的开放式生成能力，但在知识密集型任务中表现不佳。为了缓解这一问题，提出了知识集成方法，利用外部模块将领域特定知识图谱与LLMs结合起来。然而，它们存在数据效率低的问题，因为它们需要已知和未知的知识来进行微调。因此，我们研究了一个新颖的问题，即如何在不重复已知知识的情况下有效地将未知知识集成到LLMs中。注入新知识会导致遗忘先前获得的知识的风险。为了解决这个问题，我们提出了一种新颖的Infuser-Guided Knowledge Integration（InfuserKI）框架，利用transformer内部状态来确定是否应该增强原始LLM输出信息，从而有效地减轻知识遗忘问题。在UMLS-2.5k和MetaQ上进行了评估。

    arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
    
[^157]: 使用机器学习技术改善物联网应用的室内定位

    Improved Indoor Localization with Machine Learning Techniques for IoT applications

    [https://arxiv.org/abs/2402.11433](https://arxiv.org/abs/2402.11433)

    该研究利用机器学习算法改善基于RSSI的室内定位，引入加权最小二乘技术和伪线性解决方案方法以解决非线性RSSI测量方程的问题。

    

    互联网物联网和移动互联网应用的兴起推动了商业、军事和社会应用中基于位置的服务的兴趣。室内定位系统利用诸如Wi-Fi、ZigBee、蓝牙、UWB等无线技术，根据上下文进行选择。接收信号强度指示器（RSSI）技术以其准确性和简单性而被广泛采用。该研究在三个阶段利用机器学习算法：监督回归器、监督分类器和基于RSSI的室内定位的集成方法。此外，它引入了加权最小二乘技术和伪线性解决方案方法，通过用线性方程来近似非线性RSSI测量方程，来解决非线性RSSI测量方程的问题。

    arXiv:2402.11433v1 Announce Type: new  Abstract: The rise of the Internet of Things (IoT) and mobile internet applications has spurred interest in location-based services (LBS) for commercial, military, and social applications. While the global positioning system (GPS) dominates outdoor localization, its efficacy wanes indoors due to signal challenges. Indoor localization systems leverage wireless technologies like Wi-Fi, ZigBee, Bluetooth, UWB, selecting based on context. Received signal strength indicator (RSSI) technology, known for its accuracy and simplicity, is widely adopted. This study employs machine learning algorithms in three phases: supervised regressors, supervised classifiers, and ensemble methods for RSSI-based indoor localization. Additionally, it introduces a weighted least squares technique and pseudo-linear solution approach to address non-linear RSSI measurement equations by approximating them with linear equations. An experimental testbed, utilizing diverse wirele
    
[^158]: OptEx: 利用近似并行化迭代加速一阶优化

    OptEx: Expediting First-Order Optimization with Approximately Parallelized Iterations

    [https://arxiv.org/abs/2402.11427](https://arxiv.org/abs/2402.11427)

    OptEx是第一个通过利用并行计算来减轻一阶优化的迭代瓶颈并增强效率的框架，使用核化梯度估计实现迭代的并行化，提供理论保证。

    

    第一阶优化（FOO）算法在诸如机器学习和信号去噪等众多计算领域中至关重要。然而，将它们应用于神经网络训练等复杂任务往往导致显著的低效，因为需要许多顺序迭代以实现收敛。为此，我们引入了第一阶优化加速近似并行迭代（OptEx），这是第一个通过利用并行计算来减轻其迭代瓶颈而增强FOO效率的框架。OptEx采用核化梯度估计来利用梯度历史进行未来梯度预测，实现了迭代的并行化 -- 这是一种曾经被认为由于FOO中固有的迭代依赖而不切实际的策略。我们为我们的核化梯度估计的可靠性和基于SGD的OptEx的迭代复杂度提供理论保证，并确认了其可靠性。

    arXiv:2402.11427v1 Announce Type: cross  Abstract: First-order optimization (FOO) algorithms are pivotal in numerous computational domains such as machine learning and signal denoising. However, their application to complex tasks like neural network training often entails significant inefficiencies due to the need for many sequential iterations for convergence. In response, we introduce first-order optimization expedited with approximately parallelized iterations (OptEx), the first framework that enhances the efficiency of FOO by leveraging parallel computing to mitigate its iterative bottleneck. OptEx employs kernelized gradient estimation to make use of gradient history for future gradient prediction, enabling parallelization of iterations -- a strategy once considered impractical because of the inherent iterative dependency in FOO. We provide theoretical guarantees for the reliability of our kernelized gradient estimation and the iteration complexity of SGD-based OptEx, confirming t
    
[^159]: 在线局部虚发现率控制：一种资源分配方法

    Online Local False Discovery Rate Control: A Resource Allocation Approach

    [https://arxiv.org/abs/2402.11425](https://arxiv.org/abs/2402.11425)

    该研究提出了一种在线局部虚发现率控制的资源分配方法，实现了$O(\sqrt{T})$的后悔率，并指出这种后悔率在一般情况下是不可改进的。

    

    我们考虑在线局部虚发现率（FDR）控制问题，其中多个测试被顺序进行，目标是最大化总期望的发现次数。我们将问题形式化为一种在线资源分配问题，涉及接受/拒绝决策，从高层次来看，这可以被视为一个带有额外不确定性的在线背包问题，即随机预算补充。我们从一般的到达分布开始，并提出了一个简单的策略，实现了$O(\sqrt{T})$的后悔。我们通过展示这种后悔率在一般情况下是不可改进的来补充这一结果。然后我们将焦点转向离散到达分布。我们发现许多现有的在线资源分配文献中的重新解决启发式虽然在典型设置中实现了有界的损失，但可能会造成$\Omega(\sqrt{T})$甚至$\Omega(T)$的后悔。通过观察到典型策略往往太过

    arXiv:2402.11425v1 Announce Type: cross  Abstract: We consider the problem of online local false discovery rate (FDR) control where multiple tests are conducted sequentially, with the goal of maximizing the total expected number of discoveries. We formulate the problem as an online resource allocation problem with accept/reject decisions, which from a high level can be viewed as an online knapsack problem, with the additional uncertainty of random budget replenishment. We start with general arrival distributions and propose a simple policy that achieves a $O(\sqrt{T})$ regret. We complement the result by showing that such regret rate is in general not improvable. We then shift our focus to discrete arrival distributions. We find that many existing re-solving heuristics in the online resource allocation literature, albeit achieve bounded loss in canonical settings, may incur a $\Omega(\sqrt{T})$ or even a $\Omega(T)$ regret. With the observation that canonical policies tend to be too op
    
[^160]: LoRETTA: 低秩经济张量训练适应大型语言模型的超低参数微调

    LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models

    [https://arxiv.org/abs/2402.11417](https://arxiv.org/abs/2402.11417)

    LoRETTA是一个通过张量训练分解显著减少可训练参数的超低参数高效框架，在大型语言模型的微调中表现出与大多数PEFT方法相媲美甚至更好的性能。

    

    各种参数高效微调（PEFT）技术被提出以实现在保持模型性能的情况下进行计算高效的微调。然而，随着大型语言模型（LLMs）的快速部署，现有的PEFT方法仍然受到可训练参数数量增长的限制。为了解决这一挑战，我们提出了LoRETTA，这是一个超参数高效的框架，通过张量训练分解显著减少可训练参数。具体来说，我们提出了两种方法，分别命名为{LoRETTA}$_{adp}$和{LoRETTA}$_{rep}$。前者采用张量化适配器，为LLMs的微调提供了高性能且轻量级的方法。后者强调通过一组小张量因子进行权重参数化的微调。LoRETTA在LLaMA-2-7B模型上比大多数广泛使用的PEFT方法具有可比或更好的性能，并且参数少达到100倍。

    arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the LLaMA-2-7B models. Further
    
[^161]: 一种用于利用“Segment Anything Model (SAM)”进行自动图像分割的多光谱自动转移技术（MATT）

    A Multispectral Automated Transfer Technique (MATT) for machine-driven image labeling utilizing the Segment Anything Model (SAM)

    [https://arxiv.org/abs/2402.11413](https://arxiv.org/abs/2402.11413)

    介绍了一种名为多光谱自动转移技术（MATT）的方法，通过从RGB图像转置SAM分割掩模，实现了对多光谱图像的高精度和高效自动分割标记。

    

    arXiv:2402.11413v1 公告类型: 跨领域  摘要: Segment Anything Model (SAM)大大加快了自动分割和标记大型红绿蓝（RGB）图像数据集的速度和准确性。然而，SAM无法对可见光谱之外的图像进行分割和标记，例如，对于多光谱或高光谱图像。因此，本文概述了一种我们称之为多光谱自动转移技术（MATT）的方法。通过从RGB图像中转置SAM分割掩模，我们可以高精度和高效地自动分割和标记多光谱图像。例如，结果表明，利用MATT对一个2,400张图像数据集进行分割和标记，在开发训练模型方面节省了87.8%的时间，将大约20小时的手动标注减少到仅2.4小时。在通过MATT训练多光谱模型时，这种效率提高仅与整体平均精度（mAP）降低了6.7%有关。

    arXiv:2402.11413v1 Announce Type: cross  Abstract: Segment Anything Model (SAM) is drastically accelerating the speed and accuracy of automatically segmenting and labeling large Red-Green-Blue (RGB) imagery datasets. However, SAM is unable to segment and label images outside of the visible light spectrum, for example, for multispectral or hyperspectral imagery. Therefore, this paper outlines a method we call the Multispectral Automated Transfer Technique (MATT). By transposing SAM segmentation masks from RGB images we can automatically segment and label multispectral imagery with high precision and efficiency. For example, the results demonstrate that segmenting and labeling a 2,400-image dataset utilizing MATT achieves a time reduction of 87.8% in developing a trained model, reducing roughly 20 hours of manual labeling, to only 2.4 hours. This efficiency gain is associated with only a 6.7% decrease in overall mean average precision (mAP) when training multispectral models via MATT, co
    
[^162]: 通过偏好微调在视觉大语言模型中对齐模态

    Aligning Modalities in Vision Large Language Models via Preference Fine-tuning

    [https://arxiv.org/abs/2402.11411](https://arxiv.org/abs/2402.11411)

    本研究将幻觉问题视为对齐问题，并通过偏好调整解决，提出了POVID方法来生成反馈数据。

    

    指示跟随的视觉大语言模型（VLLMs）最近在各种任务上取得了显著进展。这些方法合并了强大的预训练视觉模型和大型语言模型（LLMs）。由于这些组件是分别训练的，所以需要通过联合训练额外的图像-语言对来对学习的表示进行对齐。这个过程并不完美，可能会导致模型产生幻觉-即使核心LLM非常客观，视觉支撑具有充分完整的表示，也会提供与图像不符合的答案。在这项工作中，我们将幻觉问题定义为一个对齐问题，通过偏好调整来解决。具体来说，我们提出了POVID来产生AI模型的反馈数据。我们使用地面真实指示作为首选响应，采用两阶段方法生成不受欢迎的数据。首先，我们提示GPT-4V注入合理的幻觉。

    arXiv:2402.11411v1 Announce Type: cross  Abstract: Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucin
    
[^163]: 获得$2\sqrt{T}$到校准的基本预测器

    An Elementary Predictor Obtaining $2\sqrt{T}$ Distance to Calibration

    [https://arxiv.org/abs/2402.11410](https://arxiv.org/abs/2402.11410)

    给出了一种简单、高效、确定性的算法，该算法的校准距离误差最多为$2\sqrt{T}$

    

    Blasiok等人[2023]提出了校准距离作为一种自然的校准误差度量，与预期的校准误差(ECE)不同，它是连续的。最近，Qiao和Zheng [2024]给出了一个非构造性的论证，建立了一种在线预测器的存在，该预测器可以在对抗设置中获得$O(\sqrt{T})$的校准距离，而对于ECE来说是不可能的。他们将找到一种明确的、高效的算法作为一个需要解决的问题。我们解决了这个问题，并给出了一个非常简单、高效、确定性的算法，该算法的校准距离误差最多为$2\sqrt{T}$。

    arXiv:2402.11410v1 Announce Type: new  Abstract: Blasiok et al. [2023] proposed distance to calibration as a natural measure of calibration error that unlike expected calibration error (ECE) is continuous. Recently, Qiao and Zheng [2024] gave a non-constructive argument establishing the existence of an online predictor that can obtain $O(\sqrt{T})$ distance to calibration in the adversarial setting, which is known to be impossible for ECE. They leave as an open problem finding an explicit, efficient algorithm. We resolve this problem and give an extremely simple, efficient, deterministic algorithm that obtains distance to calibration error at most $2\sqrt{T}$.
    
[^164]: 评估深度学习潜在特征空间的稳定性

    Evaluating the Stability of Deep Learning Latent Feature Spaces

    [https://arxiv.org/abs/2402.11404](https://arxiv.org/abs/2402.11404)

    评估深度学习潜在特征空间稳定性的新方法，引入了可以确保一致性和可靠性的稳定性评估工作流程，包括了三种稳定性类型和一套全面评估的度量标准。

    

    高维数据集在各个学科的统计建模中提出了重大挑战，需要有效的降维方法。深度学习方法以从复杂数据中提炼关键特征的能力而著称，通过降维的潜在特征空间实现建模、可视化和压缩，在生物信息学到地球科学等领域有广泛应用。本研究引入了一种新颖工作流程，用于评估这些潜在空间的稳定性，确保后续分析的一致性和可靠性。稳定性被定义为潜在空间对于微小数据、训练实现和参数扰动的不变性，是至关重要却经常被忽视的。我们提出的方法论界定了潜在空间中的三种稳定性类型，样本稳定性、结构稳定性和推断稳定性，并引入了一套用于全面评估的度量标准。我们实现了这个工作流程。

    arXiv:2402.11404v1 Announce Type: new  Abstract: High-dimensional datasets present substantial challenges in statistical modeling across various disciplines, necessitating effective dimensionality reduction methods. Deep learning approaches, notable for their capacity to distill essential features from complex data, facilitate modeling, visualization, and compression through reduced dimensionality latent feature spaces, have wide applications from bioinformatics to earth sciences. This study introduces a novel workflow to evaluate the stability of these latent spaces, ensuring consistency and reliability in subsequent analyses. Stability, defined as the invariance of latent spaces to minor data, training realizations, and parameter perturbations, is crucial yet often overlooked.   Our proposed methodology delineates three stability types, sample, structural, and inferential, within latent spaces, and introduces a suite of metrics for comprehensive evaluation. We implement this workflow
    
[^165]: GraphKD：探索知识蒸馏在文档目标检测中的应用，并通过结构化图创建实现

    GraphKD: Exploring Knowledge Distillation Towards Document Object Detection with Structured Graph Creation

    [https://arxiv.org/abs/2402.11401](https://arxiv.org/abs/2402.11401)

    提出了一种基于图的知识蒸馏框架，用于在文档图像中识别和定位文档对象，以减少大型模型在资源受限设备上的部署成本

    

    在文档中进行目标检测是自动化识别数字或扫描文档中的结构元素的关键步骤，通过理解不同元素之间的分层结构和关系。大型和复杂的模型虽然可以实现高准确性，但在计算上昂贵且占用内存，使其在资源受限设备上部署变得不切实际。知识蒸馏允许我们创建小型且更高效的模型，这些模型保留了大型模型的大部分性能。在这里，我们提出了一种基于图的知识蒸馏框架，以正确识别并定位文档图像中的文档对象。在这里，我们设计了一个具有建议级特征的结构化图，边表示不同建议区域之间的关系。此外，为了减少文本偏见，设计了一种自适应节点抽样策略来修剪权重。

    arXiv:2402.11401v1 Announce Type: cross  Abstract: Object detection in documents is a key step to automate the structural elements identification process in a digital or scanned document through understanding the hierarchical structure and relationships between different elements. Large and complex models, while achieving high accuracy, can be computationally expensive and memory-intensive, making them impractical for deployment on resource constrained devices. Knowledge distillation allows us to create small and more efficient models that retain much of the performance of their larger counterparts. Here we present a graph-based knowledge distillation framework to correctly identify and localize the document objects in a document image. Here, we design a structured graph with nodes containing proposal-level features and edges representing the relationship between the different proposal regions. Also, to reduce text bias an adaptive node sampling strategy is designed to prune the weight
    
[^166]: k-SemStamp：一种基于聚类的语义水印用于检测机器生成的文本

    k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text

    [https://arxiv.org/abs/2402.11399](https://arxiv.org/abs/2402.11399)

    k-SemStamp是一种简单而有效的语义水印方案，通过利用k均值聚类代替LSH来提高鲁棒性和抽样效率，同时保持生成质量，为机器生成文本检测提供了更有效的工具。

    

    最近的水印生成算法在语言生成过程中注入可检测的签名，以便进行事后检测。虽然基于标记级别的水印容易受到改写攻击，但SemStamp (Hou等人，2023)在句子的语义表示上应用水印，并展示出很好的鲁棒性。SemStamp利用局部敏感哈希（LSH）来利用任意超平面对语义空间进行分区，导致在鲁棒性和速度之间存在次优的权衡。我们提出k-SemStamp，这是SemStamp的一个简单而有效的增强版，利用k均值聚类作为局部敏感哈希的替代方案，以了解内在的语义结构来分区嵌入空间。实验结果表明，k-SemStamp显著提高了其鲁棒性和抽样效率，同时保持生成质量，推进了更有效的机器生成文本检测工具。

    arXiv:2402.11399v1 Announce Type: new  Abstract: Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
    
[^167]: 最佳逼近的随机投影神经网络：收敛理论和实际应用

    Random Projection Neural Networks of Best Approximation: Convergence theory and practical applications

    [https://arxiv.org/abs/2402.11397](https://arxiv.org/abs/2402.11397)

    RPNNs具有固定内部权重和偏置，通过选择外部权重，它们展现出指数收敛速率来逼近任意无穷可微函数，在函数逼近问题上表现出高效准确的潜力

    

    我们研究了前馈神经网络（FNN）的最佳逼近概念，并通过随机投影神经网络（RPNNs）的视角探讨它们的收敛性质。RPNNs具有预先确定且固定的内部权重和偏置，提供了计算效率。我们证明了对于任何一族具有非多项式无穷可微激活函数的RPNNs，存在外部权重的选择，展现出指数收敛速率来逼近任意无穷可微函数。为了说明，我们在五个基准函数逼近问题上测试了基于RPNN的函数逼近，选择了简洁的基函数。结果显示，RPNNs达到了与勒让德多项式等已建立方法相媲美的性能，突显了它们在高效准确函数逼近方面的潜力。

    arXiv:2402.11397v1 Announce Type: new  Abstract: We investigate the concept of Best Approximation for Feedforward Neural Networks (FNN) and explore their convergence properties through the lens of Random Projection (RPNNs). RPNNs have predetermined and fixed, once and for all, internal weights and biases, offering computational efficiency. We demonstrate that there exists a choice of external weights, for any family of such RPNNs, with non-polynomial infinitely differentiable activation functions, that exhibit an exponential convergence rate when approximating any infinitely differentiable function. For illustration purposes, we test the proposed RPNN-based function approximation, with parsimoniously chosen basis functions, across five benchmark function approximation problems. Results show that RPNNs achieve comparable performance to established methods such as Legendre Polynomials, highlighting their potential for efficient and accurate function approximation.
    
[^168]: 强化学习用于最大化风力发电

    Reinforcement learning to maximise wind turbine energy generation

    [https://arxiv.org/abs/2402.11384](https://arxiv.org/abs/2402.11384)

    提出了一种利用强化学习控制风力发电以最大化能量产生的方法，表现出在各种环境下比传统PID控制更优异，并在真实风条件下进行了对比验证。

    

    我们提出了一种强化学习策略，通过主动调整转子速度、转子偏航角和叶片倾角来控制风力发电。我们将双深度Q学习与优先经验重播代理与叶片元素动量模型耦合，并训练该代理以允许控制不断变化的风。该代理经过训练，可以在简单稳定风中决定最佳控制（速度、偏航、倾角），随后挑战真实的动态紊流风，表现良好。双深度Q学习与经典值迭代强化学习控制进行比较，在所有环境中都优于经典PID控制。此外，强化学习方法非常适合变化的环境，包括紊乱/阵风风，表现出很强的适应性。最后，我们将所有控制策略与真实风进行比较，并计算年度能源产量。

    arXiv:2402.11384v1 Announce Type: new  Abstract: We propose a reinforcement learning strategy to control wind turbine energy generation by actively changing the rotor speed, the rotor yaw angle and the blade pitch angle. A double deep Q-learning with a prioritized experience replay agent is coupled with a blade element momentum model and is trained to allow control for changing winds. The agent is trained to decide the best control (speed, yaw, pitch) for simple steady winds and is subsequently challenged with real dynamic turbulent winds, showing good performance. The double deep Q- learning is compared with a classic value iteration reinforcement learning control and both strategies outperform a classic PID control in all environments. Furthermore, the reinforcement learning approach is well suited to changing environments including turbulent/gusty winds, showing great adaptability. Finally, we compare all control strategies with real winds and compute the annual energy production. I
    
[^169]: 多任务逆强化学习用于常识奖励

    Multi Task Inverse Reinforcement Learning for Common Sense Reward

    [https://arxiv.org/abs/2402.11367](https://arxiv.org/abs/2402.11367)

    将奖励分解为任务特定奖励和常识奖励，探索如何从专家演示中学习常识奖励；研究发现，逆强化学习成功训练代理后，并不会学到有用的奖励函数。

    

    在将强化学习应用于复杂的现实环境中，一个挑战在于为agent提供足够详细的奖励函数。奖励与期望行为之间的不一致可能导致意外结果，如“奖励篡改”，agent通过意外行为最大化奖励。本文提出将奖励分解为两个明确部分：一个简单的任务特定奖励，概述了当前任务的细节；以及一个未知的常识奖励，指示agent在环境中的预期行为。我们探讨了如何从专家演示中学习这种常识奖励。我们首先展示，即使逆强化学习成功训练了一个代理，也不会学到一个有用的奖励函数。也就是说，使用学到的奖励训练新代理不会影响期望行为。

    arXiv:2402.11367v1 Announce Type: new  Abstract: One of the challenges in applying reinforcement learning in a complex real-world environment lies in providing the agent with a sufficiently detailed reward function. Any misalignment between the reward and the desired behavior can result in unwanted outcomes. This may lead to issues like "reward hacking" where the agent maximizes rewards by unintended behavior. In this work, we propose to disentangle the reward into two distinct parts. A simple task-specific reward, outlining the particulars of the task at hand, and an unknown common-sense reward, indicating the expected behavior of the agent within the environment. We then explore how this common-sense reward can be learned from expert demonstrations. We first show that inverse reinforcement learning, even when it succeeds in training an agent, does not learn a useful reward function. That is, training a new agent with the learned reward does not impair the desired behaviors. We then d
    
[^170]: 使用高斯过程的数据驱动随机交流优化潮流

    Data-Driven Stochastic AC-OPF using Gaussian Processes

    [https://arxiv.org/abs/2402.11365](https://arxiv.org/abs/2402.11365)

    该论文提出了一种基于高斯过程的数据驱动算法，用于解决随机交流（AC）概率约束（CC）最优潮流（OPF）问题，并通过多个IEEE测试案例展示了其实证效率。

    

    这篇论文聚焦于发展一种基于机器学习的数据驱动算法，用于解决随机交流（AC）概率约束（CC）最优潮流（OPF）问题。虽然AC CC-OPF问题在学术界取得了成功，但由于高度非线性和计算要求很高，限制了其实际影响。该方法旨在解决这一限制，并通过应用于多个IEEE测试案例来展示其实证效率。为了解决非凸和计算复杂的CC AC-OPF问题，该方法依赖于机器学习高斯过程回归（GPR）模型。完整高斯过程（GP）方法能够学习一个简单但非凸的数据驱动近似至AC潮流方程，能够纳入不确定输入。该方法使用各种近似来传播GP不确定性。

    arXiv:2402.11365v1 Announce Type: new  Abstract: The thesis focuses on developing a data-driven algorithm, based on machine learning, to solve the stochastic alternating current (AC) chance-constrained (CC) Optimal Power Flow (OPF) problem. Although the AC CC-OPF problem has been successful in academic circles, it is highly nonlinear and computationally demanding, which limits its practical impact. The proposed approach aims to address this limitation and demonstrate its empirical efficiency through applications to multiple IEEE test cases. To solve the non-convex and computationally challenging CC AC-OPF problem, the proposed approach relies on a machine learning Gaussian process regression (GPR) model. The full Gaussian process (GP) approach is capable of learning a simple yet non-convex data-driven approximation to the AC power flow equations that can incorporate uncertain inputs. The proposed approach uses various approximations for GP-uncertainty propagation. The full GP CC-OPF ap
    
[^171]: 利用T-范数进行自动驾驶中的深度学习

    Exploiting T-norms for Deep Learning in Autonomous Driving

    [https://arxiv.org/abs/2402.11362](https://arxiv.org/abs/2402.11362)

    本文提出了如何定义内存高效的T-范数损失，允许在自动驾驶中利用T-范数进行事件检测任务，并在实验中展示了在GPU上运行的可行性。

    

    深度学习一直是自动驾驶领域发展的核心，由于神经网络在发现原始数据中的模式并将其转化为准确预测方面取得的成功。此外，最近的神经符号化工作表明，通过T-范数在损失函数中合并关于手头问题的可用背景知识可以进一步提高深度学习模型的性能。然而，基于T-范数的损失可能具有非常高的内存需求，因此可能无法应用于像自动驾驶这样的复杂应用领域。在本文中，我们展示了如何定义内存高效的T-范数损失，从而允许利用T-范数来进行自动驾驶中的事件检测任务。我们在ROAD-R数据集上进行了广泛的实验分析，并展示了（i）我们的提议可以在具有小于25 GiB可用内存的GPU上实现和运行，而标准的t-

    arXiv:2402.11362v1 Announce Type: new  Abstract: Deep learning has been at the core of the autonomous driving field development, due to the neural networks' success in finding patterns in raw data and turning them into accurate predictions. Moreover, recent neuro-symbolic works have shown that incorporating the available background knowledge about the problem at hand in the loss function via t-norms can further improve the deep learning models' performance. However, t-norm-based losses may have very high memory requirements and, thus, they may be impossible to apply in complex application domains like autonomous driving. In this paper, we show how it is possible to define memory-efficient t-norm-based losses, allowing for exploiting t-norms for the task of event detection in autonomous driving. We conduct an extensive experimental analysis on the ROAD-R dataset and show (i) that our proposal can be implemented and run on GPUs with less than 25 GiB of available memory, while standard t-
    
[^172]: 改变了什么？将表征干预转化为自然语言

    What Changed? Converting Representational Interventions to Natural Language

    [https://arxiv.org/abs/2402.11355](https://arxiv.org/abs/2402.11355)

    将表征空间的反事实转化为自然语言，以分析和解释模型干预所引起的语言变化，并减轻分类中的偏见。

    

    针对语言模型（LMs）表征空间的干预方法已经被证明是影响模型行为的有效手段。这些方法被用来消除或改变模型表示中的人口统计信息（如性别）的编码，创建一个反事实的表示。然而，由于干预操作在表示空间内，准确理解它修改了哪些特征是一个挑战。我们展示了表征空间的反事实可以转化为自然语言的反事实。我们证明了这种方法使我们能够分析对应于给定表示空间干预的语言变化，并解释用于编码特定概念的特征。此外，由此产生的反事实可以用于减轻分类中的偏见。

    arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
    
[^173]: 基于概率路由的基于图的近似最近邻搜索

    Probabilistic Routing for Graph-Based Approximate Nearest Neighbor Search

    [https://arxiv.org/abs/2402.11354](https://arxiv.org/abs/2402.11354)

    该论文提出了一种基于概率路由的方法，通过引入PEOs有效识别图中需要考虑进行精确距离计算的邻居，从而显著提高了基于图的近似最近邻搜索的效率。

    

    arXiv：2402.11354v1 公告类型：交叉 摘要：在机器学习领域，高维空间中的近似最近邻搜索(ANNS)是一个重要挑战。近年来，基于图的方法已经成为ANNS的优越方法，建立了一种新的技术水平。尽管引入了各种基于图的ANNS优化方法，但它们主要依赖于缺乏正式理论支持的启发式方法。本文旨在通过引入一种方法来增强基于图的ANNS中的路由，该方法在探索图中节点的邻居时提供概率保证。我们将问题建模为概率路由，并通过结合局部敏感技术开发了两种基准策略。随后，我们介绍了PEOs，这是一种有效识别图中应考虑进行精确距离计算的邻居的新方法，从而在实践中显著提高了效率。我们的实验证明...

    arXiv:2402.11354v1 Announce Type: cross  Abstract: Approximate nearest neighbor search (ANNS) in high-dimensional spaces is a pivotal challenge in the field of machine learning. In recent years, graph-based methods have emerged as the superior approach to ANNS, establishing a new state of the art. Although various optimizations for graph-based ANNS have been introduced, they predominantly rely on heuristic methods that lack formal theoretical backing. This paper aims to enhance routing within graph-based ANNS by introducing a method that offers a probabilistic guarantee when exploring a node's neighbors in the graph. We formulate the problem as probabilistic routing and develop two baseline strategies by incorporating locality-sensitive techniques. Subsequently, we introduce PEOs, a novel approach that efficiently identifies which neighbors in the graph should be considered for exact distance computation, thus significantly improving efficiency in practice. Our experiments demonstrate 
    
[^174]: 变分熵搜索用于调整期望改进

    Variational Entropy Search for Adjusting Expected Improvement

    [https://arxiv.org/abs/2402.11345](https://arxiv.org/abs/2402.11345)

    本论文通过变分推断的方法，将期望改进（EI）视为最大值熵搜索（MES）的特殊情况，提出了变分熵搜索（VES）方法和 VES-Gamma 算法，成功调整 EI 并展示其在贝叶斯优化方面的实用性。

    

    Bayesian optimization 是一种广泛使用的优化黑盒函数的技术，期望改进（EI）是该领域中最常用的获取函数。虽然 EI 通常被视为与其他信息理论获取函数（如熵搜索（ES）和最大值熵搜索（MES））不同，但我们的工作揭示了，通过变分推断（VI）方法，EI 可以被视为 MES 的一种特殊情况。在这一背景下，我们开发了变分熵搜索（VES）方法和 VES-Gamma 算法，通过将信息理论概念的原则整合到 EI 中来调整 EI。VES-Gamma 的有效性在各种测试函数和真实数据集中得到了证明，突出了它在贝叶斯优化场景中的理论和实际用途。

    arXiv:2402.11345v1 Announce Type: cross  Abstract: Bayesian optimization is a widely used technique for optimizing black-box functions, with Expected Improvement (EI) being the most commonly utilized acquisition function in this domain. While EI is often viewed as distinct from other information-theoretic acquisition functions, such as entropy search (ES) and max-value entropy search (MES), our work reveals that EI can be considered a special case of MES when approached through variational inference (VI). In this context, we have developed the Variational Entropy Search (VES) methodology and the VES-Gamma algorithm, which adapts EI by incorporating principles from information-theoretic concepts. The efficacy of VES-Gamma is demonstrated across a variety of test functions and read datasets, highlighting its theoretical and practical utilities in Bayesian optimization scenarios.
    
[^175]: 使用堆叠自编码器进行特征选择的勒索软件检测

    Ransomware detection using stacked autoencoder for feature selection

    [https://arxiv.org/abs/2402.11342](https://arxiv.org/abs/2402.11342)

    提出并评估了一种使用堆叠自编码器进行特征选择的先进勒索软件检测和分类方法，通过结合堆叠自编码器和长短期记忆分类器，实现了提高勒索软件分层准确性的目标。

    

    本研究旨在提出并评估一种先进的勒索软件检测和分类方法，该方法将堆叠自编码器（SAE）与长短期记忆（LSTM）分类器结合，用于精确特征选择，以增强勒索软件分层准确性。该方法包括对 UGRansome 数据集进行彻底的预处理，训练一个无监督的 SAE 进行最佳特征选择，或通过监督学习进行微调，提高 LSTM 模型的分类能力。该研究详细分析了自编码器学习到的权重和激活，以识别用于将勒索软件家族与其他恶意软件区分开的重要特征，并为精确分类创建了精简的特征集。通过进行多达 400 个 epochs 和不同的学习率等广泛实验，优化模型的性能。结果表明，SAE-LSTM 模型表现出色。

    arXiv:2402.11342v1 Announce Type: new  Abstract: The aim of this study is to propose and evaluate an advanced ransomware detection and classification method that combines a Stacked Autoencoder (SAE) for precise feature selection with a Long Short Term Memory (LSTM) classifier to enhance ransomware stratification accuracy. The proposed approach involves thorough pre processing of the UGRansome dataset and training an unsupervised SAE for optimal feature selection or fine tuning via supervised learning to elevate the LSTM model's classification capabilities. The study meticulously analyzes the autoencoder's learned weights and activations to identify essential features for distinguishing ransomware families from other malware and creates a streamlined feature set for precise classification. Extensive experiments, including up to 400 epochs and varying learning rates, are conducted to optimize the model's performance. The results demonstrate the outstanding performance of the SAE-LSTM mod
    
[^176]: 通过超图对称性打破进行高阶链接预测

    Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking

    [https://arxiv.org/abs/2402.11339](https://arxiv.org/abs/2402.11339)

    通过引入预处理算法识别展现对称性的正则子超图，从而提高超图在高阶链接预测中的表达能力和区分能力。

    

    一种超图由一组节点以及称为超边的节点子集合组成。更高阶链接预测是预测一个超图中是否存在缺失的超边的任务。为高阶链接预测学习的超边表示在同构下不失去区分能力时具有完全表达性。许多现有的超图表示学习器受到广义Weisfeiler Lehman-1（GWL-1）算法的表达能力限制，它是Weisfeiler Lehman-1算法的推广。然而，GWL-1的表达能力有限。事实上，具有相同GWL-1值节点的诱导子超图是无法区分的。此外，在超图上进行消息传递可能已经在GPU内存上变得计算昂贵。为了解决这些限制，我们设计了一种可以识别出展现对称性的特定正则子超图的预处理算法。

    arXiv:2402.11339v1 Announce Type: new  Abstract: A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. Higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. A hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. Many existing hypergraph representation learners, are bounded in expressive power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact, induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Furthermore, message passing on hypergraphs can already be computationally expensive, especially on GPU memory. To address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. Our p
    
[^177]: 具有部分反馈的公平分类：一种基于探索的数据收集方法

    Fair Classification with Partial Feedback: An Exploration-Based Data-Collection Approach

    [https://arxiv.org/abs/2402.11338](https://arxiv.org/abs/2402.11338)

    该方法提出了一种基于探索的数据收集方法，能够在缺乏部分反馈信息的情况下训练分类器，并提供了一系列策略来确保所有子群体都被探索、防止错误分类、以及收敛到期望的分类器。

    

    在许多预测场景（例如信贷放款）中，只有过去被积极分类的样本才会观察到真实结果。这些过去的观察结果形成了用于训练分类器以进行未来预测的训练数据集。然而，这样的训练数据集缺乏关于过去（错误地）被负面分类的样本结果的信息，可能导致错误的分类器。我们提出了一种方法，利用可用数据训练分类器，并提供一系列探索策略来收集关于否则会被忽略的子群体的结果数据。对于任何探索策略，该方法都具有以下保证：（1）所有子群体都得到了探索，（2）假阳性的比例受到了限制，（3）训练的分类器收敛到一个“期望”的分类器。正确的探索策略取决于上下文；它可以选择以改善学习保证

    arXiv:2402.11338v1 Announce Type: cross  Abstract: In many predictive contexts (e.g., credit lending), true outcomes are only observed for samples that were positively classified in the past. These past observations, in turn, form training datasets for classifiers that make future predictions. However, such training datasets lack information about the outcomes of samples that were (incorrectly) negatively classified in the past and can lead to erroneous classifiers. We present an approach that trains a classifier using available data and comes with a family of exploration strategies to collect outcome data about subpopulations that otherwise would have been ignored. For any exploration strategy, the approach comes with guarantees that (1) all sub-populations are explored, (2) the fraction of false positives is bounded, and (3) the trained classifier converges to a "desired" classifier. The right exploration strategy is context-dependent; it can be chosen to improve learning guarantees 
    
[^178]: SpikeNAS: 一种面向脉冲神经网络系统的快速内存感知神经架构搜索框架

    SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems

    [https://arxiv.org/abs/2402.11322](https://arxiv.org/abs/2402.11322)

    SpikeNAS提出了一种快速内存感知神经架构搜索框架，旨在帮助脉冲神经网络系统快速找到在给定内存预算下高准确性的适当架构。

    

    脉冲神经网络（SNN）为解决机器学习任务提供了实现超低功耗计算的有前途的解决方案。目前，大多数SNN架构都源自人工神经网络，其神经元的架构和操作与SNN不同，或者在不考虑来自底层处理硬件的内存预算的情况下开发。这些限制阻碍了SNN在准确性和效率方面充分发挥潜力。为此，我们提出了SpikeNAS，一种新颖的内存感知神经架构搜索（NAS）框架，可在给定内存预算下快速找到一个具有高准确性的适当SNN架构。为实现这一目标，我们的SpikeNAS采用了几个关键步骤：分析网络操作对准确性的影响，增强网络架构以提高学习质量，并开发快速内存感知搜索算法。

    arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
    
[^179]: BiasBuster：使用偏倚位置数据准确估计人口统计数据的神经方法

    BiasBuster: a Neural Approach for Accurate Estimation of Population Statistics using Biased Location Data

    [https://arxiv.org/abs/2402.11318](https://arxiv.org/abs/2402.11318)

    该论文提出了一种应对使用偏倚位置数据计算人口统计数据造成的不准确性的方法，以解决这些数据集在敏感决策中可能产生的严重后果。

    

    虽然移动设备收集的位置数据非常有用（例如，用于COVID-19预测和政策制定、城市流动性分析和营销、以及获取商业洞见），但这些数据通常来自于一个带有偏向性的人口子集，某些社区在收集的数据中被过度或者低估了。因此，从这样的数据集计算的汇总统计数据（如Safegraph、Google和Facebook等各种公司所做的那样），如果忽视了偏见，会导致对人口统计数据的不准确表示。这样的统计数据不仅普遍不准确，而且错误将不成比例地影响不同的人口亚组（例如，因为它们忽视了被低估的社区）。这会带来严重后果，因为这些数据集被用于像COVID-19政策制定这样的敏感决策。本文解决了利用这种有偏差数据提供准确人口统计数据的问题。

    arXiv:2402.11318v1 Announce Type: new  Abstract: While extremely useful (e.g., for COVID-19 forecasting and policy-making, urban mobility analysis and marketing, and obtaining business insights), location data collected from mobile devices often contain data from a biased population subset, with some communities over or underrepresented in the collected datasets. As a result, aggregate statistics calculated from such datasets (as is done by various companies including Safegraph, Google, and Facebook), while ignoring the bias, leads to an inaccurate representation of population statistics. Such statistics will not only be generally inaccurate, but the error will disproportionately impact different population subgroups (e.g., because they ignore the underrepresented communities). This has dire consequences, as these datasets are used for sensitive decision-making such as COVID-19 policymaking. This paper tackles the problem of providing accurate population statistics using such biased da
    
[^180]: 针对非静态动态的快速在线调整的去偏置离线表示学习

    Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics

    [https://arxiv.org/abs/2402.11317](https://arxiv.org/abs/2402.11317)

    提出了一种名为DORA的新方法，通过信息瓶颈原理在离线设置中学习适应性策略，解决了动态编码与环境数据之间的互信息与与行为策略的互信息之间的难题

    

    开发能够适应非静态环境的策略对于现实世界的强化学习应用至关重要。然而，在仅有一组有限的预先收集的轨迹的离线设置中学习这种适应性策略存在显著挑战。为了解决这个问题，我们引入了一种名为去偏置离线表示快速在线调整（DORA）的新方法。DORA融入了信息瓶颈原理，最大化了动态编码与环境数据之间的互信息，同时最小化了动态编码与行为策略的互信息。我们提出了DORA的实际实现，利用

    arXiv:2402.11317v1 Announce Type: cross  Abstract: Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leverag
    
[^181]: 在虚拟化O-RAN平台中的公平资源分配

    Fair Resource Allocation in Virtualized O-RAN Platforms

    [https://arxiv.org/abs/2402.11285](https://arxiv.org/abs/2402.11285)

    该论文通过实验评估了O-Cloud的能耗及其与服务器硬件、容量和数据流量特性的关系，提出了一种计算策略和无线策略，平衡能源节约和性能，确保它们在服务器和用户之间公平分配。

    

    O-RAN系统及其在虚拟化通用计算平台（O-Cloud）中的部署构成了一个预计将带来前所未有的性能增益的范式转变。然而，这些架构带来了新的实施挑战，并威胁着加剧移动网络已经高能耗的问题。本文首先提出了一系列实验，评估了O-Cloud的能耗及其对服务器硬件、容量和数据流量特性的依赖性，这些特性通常会随时间改变。接下来，它提出了一个计算策略，以能效的方式将基站数据负载分配到O-Cloud服务器；以及一个无线策略，可实时确定每个用户的最小传输块大小，从而避免不必要的能源成本。这些策略平衡了能源节约和性能，并确保它们在服务器和用户之间分散公平。

    arXiv:2402.11285v1 Announce Type: cross  Abstract: O-RAN systems and their deployment in virtualized general-purpose computing platforms (O-Cloud) constitute a paradigm shift expected to bring unprecedented performance gains. However, these architectures raise new implementation challenges and threaten to worsen the already-high energy consumption of mobile networks. This paper presents first a series of experiments which assess the O-Cloud's energy costs and their dependency on the servers' hardware, capacity and data traffic properties which, typically, change over time. Next, it proposes a compute policy for assigning the base station data loads to O-Cloud servers in an energy-efficient fashion; and a radio policy that determines at near-real-time the minimum transmission block size for each user so as to avoid unnecessary energy costs. The policies balance energy savings with performance, and ensure that both of them are dispersed fairly across the servers and users, respectively. 
    
[^182]: 基于扩散模型和改进的MF-UNet方法的纹理协调MRI重建方法

    TC-DiffRecon: Texture coordination MRI reconstruction method based on diffusion model and modified MF-UNet method

    [https://arxiv.org/abs/2402.11274](https://arxiv.org/abs/2402.11274)

    提出了一种基于扩散模型的MRI重建方法TC-DiffRecon，旨在解决扩散模型导致的图像碎裂和不一致性以及生成图像过度平滑等问题。

    

    最近，扩散模型作为一种新型基于深度学习的生成方法引起了广泛关注。这些模型尝试从符合目标分布的高斯分布中对数据进行采样，并已成功地应用于MRI数据的重建。然而，作为一种无条件生成模型，扩散模型通常由于条件引导引入的一致数据投影而打破图像协调性。这经常导致图像的碎裂和不一致性。此外，扩散模型的固有局限性通常会导致生成图像过度平滑。在同一思路上，一些基于深度学习的模型经常表现出较差的泛化性能，意味着它们的有效性受到不同加速因子的极大影响。为了解决这些挑战，我们提出了一种名为TC-DiffRecon的基于扩散模型的MRI重建方法，

    arXiv:2402.11274v1 Announce Type: cross  Abstract: Recently, diffusion models have gained significant attention as a novel set of deep learning-based generative methods. These models attempt to sample data from a Gaussian distribution that adheres to a target distribution, and have been successfully adapted to the reconstruction of MRI data. However, as an unconditional generative model, the diffusion model typically disrupts image coordination because of the consistent projection of data introduced by conditional bootstrap. This often results in image fragmentation and incoherence. Furthermore, the inherent limitations of the diffusion model often lead to excessive smoothing of the generated images. In the same vein, some deep learning-based models often suffer from poor generalization performance, meaning their effectiveness is greatly affected by different acceleration factors. To address these challenges, we propose a novel diffusion model-based MRI reconstruction method, named TC-
    
[^183]: 镜像梯度：通过探索平缓局部最小值实现鲁棒的多模式推荐系统

    Mirror Gradient: Towards Robust Multimodal Recommender Systems via Exploring Flat Local Minima

    [https://arxiv.org/abs/2402.11262](https://arxiv.org/abs/2402.11262)

    本文从平缓局部最小值的角度分析多模态推荐系统，并提出了一种名为镜像梯度（MG）的梯度策略，可以增强模型的稳健性，缓解来自多模态信息输入的不稳定风险。

    

    多模态推荐系统利用各种信息来建模用户偏好和物品特征，帮助用户发现符合其兴趣的物品。在推荐系统中整合多模态信息可以缓解固有的挑战，例如数据稀疏问题和冷启动问题。然而，它同时会放大来自多模态信息输入的某些风险，如信息调整风险和固有噪声风险。这些风险对推荐模型的稳健性构成重要挑战。在本文中，我们通过平缓局部最小值的新颖视角分析多模态推荐系统，并提出一种简洁而有效的梯度策略，称为镜像梯度（MG）。这种策略可以在优化过程中隐式增强模型的稳健性，缓解由多模态信息输入引起的不稳定风险。

    arXiv:2402.11262v1 Announce Type: cross  Abstract: Multimodal recommender systems utilize various types of information to model user preferences and item features, helping users discover items aligned with their interests. The integration of multimodal information mitigates the inherent challenges in recommender systems, e.g., the data sparsity problem and cold-start issues. However, it simultaneously magnifies certain risks from multimodal information inputs, such as information adjustment risk and inherent noise risk. These risks pose crucial challenges to the robustness of recommendation models. In this paper, we analyze multimodal recommender systems from the novel perspective of flat local minima and propose a concise yet effective gradient strategy called Mirror Gradient (MG). This strategy can implicitly enhance the model's robustness during the optimization process, mitigating instability risks arising from multimodal information inputs. We also provide strong theoretical evide
    
[^184]: 通过基于政策的自我判断来对齐大型语言模型

    Aligning Large Language Models by On-Policy Self-Judgment

    [https://arxiv.org/abs/2402.11253](https://arxiv.org/abs/2402.11253)

    本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。

    

    为了使大型语言模型与人类偏好保持一致，现有研究要么利用单独的奖励模型（RM）执行基于政策的学习，要么通过放弃基于政策的学习和对独立RM的需求简化训练过程。在本文中，我们提出了一个新颖的对齐框架SELF-JUDGE，它既是(1) 基于政策的学习，又是(2) 参数高效的，因为它不需要额外的RM来评估样本进行基于政策的学习。为此，我们提出了增强式监督微调（JSFT）来训练一个单一模型，作为策略和评判器。具体来说，我们将一对一判断任务视为指导式任务的特殊情况，从响应对中选择更好的响应。因此，得到的模型可以评判当前策略的即时响应偏好，从自身初始化。实验结果显示了SELF-JUDGE的有效性，优于基线模型。

    arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
    
[^185]: 通过防止样本选择偏差来学习不平衡嘈杂数据

    Learning with Imbalanced Noisy Data by Preventing Bias in Sample Selection

    [https://arxiv.org/abs/2402.11242](https://arxiv.org/abs/2402.11242)

    提出了一种用于处理不平衡数据中嘈杂标签的方法，通过Class-Balance-based Sample Selection (CBS)防止忽视尾部类别样本，并通过Confidence-based Sample Augmentation (CSA)增强干净样本的可靠性。

    

    学习具有嘈杂标签的方法引起了越来越多的关注，因为现实场景中不可避免的不完美标签会严重影响深度模型的性能。最近的研究倾向于将低损失样本视为干净样本，丢弃高损失样本以减轻嘈杂标签的负面影响。然而，现实世界的数据集不仅包含嘈杂标签，还包含类别不平衡。不平衡问题容易导致损失较大的尾部类别的学习不足，从而产生高损失。因此，我们提出了一种简单而有效的方法来处理不平衡数据中的嘈杂标签。具体来说，我们提出了基于类平衡的样本选择（CBS）来防止在训练过程中忽视尾部类别样本。我们提出了基于置信度的样本增强（CSA）以加强所选干净样本在训练过程中的可靠性。

    arXiv:2402.11242v1 Announce Type: cross  Abstract: Learning with noisy labels has gained increasing attention because the inevitable imperfect labels in real-world scenarios can substantially hurt the deep model performance. Recent studies tend to regard low-loss samples as clean ones and discard high-loss ones to alleviate the negative impact of noisy labels. However, real-world datasets contain not only noisy labels but also class imbalance. The imbalance issue is prone to causing failure in the loss-based sample selection since the under-learning of tail classes also leans to produce high losses. To this end, we propose a simple yet effective method to address noisy labels in imbalanced datasets. Specifically, we propose Class-Balance-based sample Selection (CBS) to prevent the tail class samples from being neglected during training. We propose Confidence-based Sample Augmentation (CSA) for the chosen clean samples to enhance their reliability in the training process. To exploit sel
    
[^186]: 对抗深度学习中快捷方式的统一解决方案

    Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in Deep Learning

    [https://arxiv.org/abs/2402.11237](https://arxiv.org/abs/2402.11237)

    本文旨在通过利用拓扑数据分析提出一个统一的解决方案，检测深度学习中的快捷学习问题。

    

    深度神经网络(DNNs)容易受到快捷学习的影响：它们倾向于建立输入和输出之间无关的关系，而不是学习预期的任务。快捷学习在神经网络许多失败案例中普遍存在，这一现象的痕迹可见于其泛化问题、领域转移、对抗性脆弱性，甚至对多数群体的偏见。本文认为，各种DNN问题的共同原因为我们提供了一个重要机会，应该利用这一点找到对抗快捷学习的统一解决方案。为此，我们概述了拓扑数据分析(TDA)特别是持续同调(PH)方面的最新进展，为探测深度学习中快捷方式勾画了统一的路线图。我们通过研究DNNs中计算图的拓扑特征，使用无法学习的示例和偏见为两种情况，来证明我们的论点。

    arXiv:2402.11237v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than learning the intended task, they tend to draw inconclusive relationships between their inputs and outputs. Shortcut learning is ubiquitous among many failure cases of neural networks, and traces of this phenomenon can be seen in their generalizability issues, domain shift, adversarial vulnerability, and even bias towards majority groups. In this paper, we argue that this commonality in the cause of various DNN issues creates a significant opportunity that should be leveraged to find a unified solution for shortcut learning. To this end, we outline the recent advances in topological data analysis~(TDA), and persistent homology~(PH) in particular, to sketch a unified roadmap for detecting shortcuts in deep learning. We demonstrate our arguments by investigating the topological features of computational graphs in DNNs using two cases of unlearnable examples and bia
    
[^187]: ZeroG：探究图中跨数据集零射击可迁移性

    ZeroG: Investigating Cross-dataset Zero-shot Transferability in Graphs

    [https://arxiv.org/abs/2402.11235](https://arxiv.org/abs/2402.11235)

    ZeroG是一个新框架，旨在实现图中跨数据集零射击迁移，解决了特征不对齐、标签空间不匹配和负迁移等挑战

    

    随着如GPT-4这样的大型语言模型的发展，零射击迁移学习变得越来越重要。NLP模型的生成能力和CV模型的基于检索的方法突显了这一点，二者有效地弥合了已见数据和未见数据之间的差距。在图学习领域，新图的不断涌现和人类标注的挑战也加剧了零射击迁移学习的必要性，推动了探索能够在不需要特定数据集和标签特定微调的情况下泛化跨多样图数据的方法。在本研究中，我们通过引入ZeroG，一个旨在实现跨数据集泛化的新框架，将这样的范例扩展到了图中的零射击迁移性。解决诸如特征不对齐、不匹配的标签空间和负迁移等固有挑战。

    arXiv:2402.11235v1 Announce Type: new  Abstract: With the development of foundation models such as large language models, zero-shot transfer learning has become increasingly significant. This is highlighted by the generative capabilities of NLP models like GPT-4, and the retrieval-based approaches of CV models like CLIP, both of which effectively bridge the gap between seen and unseen data. In the realm of graph learning, the continuous emergence of new graphs and the challenges of human labeling also amplify the necessity for zero-shot transfer learning, driving the exploration of approaches that can generalize across diverse graph data without necessitating dataset-specific and label-specific fine-tuning. In this study, we extend such paradigms to zero-shot transferability in graphs by introducing ZeroG, a new framework tailored to enable cross-dataset generalization. Addressing the inherent challenges such as feature misalignment, mismatched label spaces, and negative transfer, we l
    
[^188]: 自适应分割平衡优化随机森林

    Adaptive Split Balancing for Optimal Random Forest

    [https://arxiv.org/abs/2402.11228](https://arxiv.org/abs/2402.11228)

    介绍了自适应分割平衡森林（ASBF），可在学习树表示的同时，在复杂情况下实现极小极优性，并提出了一个本地化版本，在H\"older类下达到最小极优性。

    

    尽管随机森林通常用于回归问题，但现有方法在复杂情况下缺乏适应性，或在简单、平滑情景下失去最优性。在本研究中，我们介绍了自适应分割平衡森林（ASBF），能够从数据中学习树表示，同时在Lipschitz类下实现极小极优性。为了利用更高阶的平滑性水平，我们进一步提出了一个本地化版本，该版本在任意$q \in \mathbb{N}$和$\beta \in (0,1]$的Hölder类$\mathcal{H}^{q,\beta}$下达到最小极优性。与广泛使用的随机特征选择不同，我们考虑了对现有方法的平衡修改。我们的结果表明，过度依赖辅助随机性可能会损害树模型的逼近能力，导致次优结果。相反，一个更平衡、更少随机的方法表现出最佳性能。

    arXiv:2402.11228v1 Announce Type: cross  Abstract: While random forests are commonly used for regression problems, existing methods often lack adaptability in complex situations or lose optimality under simple, smooth scenarios. In this study, we introduce the adaptive split balancing forest (ASBF), capable of learning tree representations from data while simultaneously achieving minimax optimality under the Lipschitz class. To exploit higher-order smoothness levels, we further propose a localized version that attains the minimax rate under the H\"older class $\mathcal{H}^{q,\beta}$ for any $q\in\mathbb{N}$ and $\beta\in(0,1]$. Rather than relying on the widely-used random feature selection, we consider a balanced modification to existing approaches. Our results indicate that an over-reliance on auxiliary randomness may compromise the approximation power of tree models, leading to suboptimal results. Conversely, a less random, more balanced approach demonstrates optimality. Additionall
    
[^189]: 关于相似性在检测伪装文件中的作用

    On the Role of Similarity in Detecting Masquerading Files

    [https://arxiv.org/abs/2402.11227](https://arxiv.org/abs/2402.11227)

    坏演员可以通过使用伪装样本绕过机器学习解决方案，研究探讨了数字签名和机器学习之间的相互作用，并提出了通过相似性和聚类来改进安全解决方案的方法。

    

    相似性已经应用于广泛的安全应用领域，通常用于机器学习模型。我们研究了由坏演员制作的伪装样本所提出的问题; 这些样本被制作得与合法样本相似或接近相同。我们发现这些样本可能会给机器学习解决方案带来重大问题。主要问题在于，坏演员可以通过使用伪装样本绕过机器学习解决方案。然后，我们研究了数字签名和机器学习解决方案之间的相互作用。特别是，我们专注于可执行文件和代码签名。我们提供了伪装文件的分类法。我们使用相似性和聚类的组合来查找伪装文件。我们利用在这个过程中收集到的见解来改进基于相似性和机器学习的安全解决方案。

    arXiv:2402.11227v1 Announce Type: cross  Abstract: Similarity has been applied to a wide range of security applications, typically used in machine learning models. We examine the problem posed by masquerading samples; that is samples crafted by bad actors to be similar or near identical to legitimate samples. We find that these samples potentially create significant problems for machine learning solutions. The primary problem being that bad actors can circumvent machine learning solutions by using masquerading samples.   We then examine the interplay between digital signatures and machine learning solutions. In particular, we focus on executable files and code signing. We offer a taxonomy for masquerading files. We use a combination of similarity and clustering to find masquerading files. We use the insights gathered in this process to offer improvements to similarity based and machine learning security solutions.
    
[^190]: 具有（低精度）多项式逼近的神经网络：准确性提高的新见解和技术

    Neural Networks with (Low-Precision) Polynomial Approximations: New Insights and Techniques for Accuracy Improvement

    [https://arxiv.org/abs/2402.11224](https://arxiv.org/abs/2402.11224)

    本文揭示了神经网络的多项式逼近作为一种独立对象的研究，发现PANN对某些类型的逼近误差...

    

    在神经网络中，用多项式逼近替换非多项式函数（例如非线性激活函数，如ReLU）是隐私保护机器学习中的标准做法。本文中称之为神经网络的多项式逼近（PANN）的结果神经网络与先进的密码系统兼容，实现隐私保护模型推断。利用“高精度”逼近，最先进的PANN提供了与基础骨干模型相似的推断准确性。然而，关于逼近的影响知之甚少，并且现有文献通常是通过实证确定所需的逼近精度。在本文中，我们开始对PANN进行独立对象的研究。具体来说，我们的贡献是双重的。首先，我们解释了PANN中近似误差的影响。特别是，我们发现PANN容易受到某种类型的...

    arXiv:2402.11224v1 Announce Type: new  Abstract: Replacing non-polynomial functions (e.g., non-linear activation functions such as ReLU) in a neural network with their polynomial approximations is a standard practice in privacy-preserving machine learning. The resulting neural network, called polynomial approximation of neural network (PANN) in this paper, is compatible with advanced cryptosystems to enable privacy-preserving model inference. Using ``highly precise'' approximation, state-of-the-art PANN offers similar inference accuracy as the underlying backbone model. However, little is known about the effect of approximation, and existing literature often determined the required approximation precision empirically. In this paper, we initiate the investigation of PANN as a standalone object. Specifically, our contribution is two-fold. Firstly, we provide an explanation on the effect of approximate error in PANN. In particular, we discovered that (1) PANN is susceptible to some type o
    
[^191]: HEAL：启发于大脑的高维度高效主动学习

    HEAL: Brain-inspired Hyperdimensional Efficient Active Learning

    [https://arxiv.org/abs/2402.11223](https://arxiv.org/abs/2402.11223)

    HEAL是一种专为HDC分类量身定制的新型主动学习框架，通过不确定性和多样性引导的获取主动为未标记的数据点进行注释，实现更高效的数据集注释和降低劳动成本。

    

    从人类大脑出色的学习能力中汲取灵感，超高维度计算（HDC）作为一种新颖的计算范式出现，利用高维向量表示和操作进行类似大脑的轻量级机器学习（ML）。HDC的实际部署显著提高了学习效率，与当前的深度ML方法相比，在广泛的应用领域取得了显著进展。然而，提高HDC分类器在监督学习中的数据效率仍然是一个悬而未决的问题。在本文中，我们介绍了Hyperdimensional Efficient Active Learning（HEAL），这是一种专为HDC分类量身定制的新型主动学习（AL）框架。 HEAL通过不确定性和多样性引导的获取主动为未标记的数据点进行注释，从而实现更高效的数据集注释和降低劳动成本。与仅支持基于深度神经网络的分类器的传统AL方法不同，

    arXiv:2402.11223v1 Announce Type: new  Abstract: Drawing inspiration from the outstanding learning capability of our human brains, Hyperdimensional Computing (HDC) emerges as a novel computing paradigm, and it leverages high-dimensional vector presentation and operations for brain-like lightweight Machine Learning (ML). Practical deployments of HDC have significantly enhanced the learning efficiency compared to current deep ML methods on a broad spectrum of applications. However, boosting the data efficiency of HDC classifiers in supervised learning remains an open question. In this paper, we introduce Hyperdimensional Efficient Active Learning (HEAL), a novel Active Learning (AL) framework tailored for HDC classification. HEAL proactively annotates unlabeled data points via uncertainty and diversity-guided acquisition, leading to a more efficient dataset annotation and lowering labor costs. Unlike conventional AL methods that only support classifiers built upon deep neural networks (D
    
[^192]: AdAdaGrad：自适应梯度方法的自适应批大小方案

    AdAdaGrad: Adaptive Batch Size Schemes for Adaptive Gradient Methods

    [https://arxiv.org/abs/2402.11215](https://arxiv.org/abs/2402.11215)

    AdAdaGrad和AdAdaGradNorm是一个自适应增加批大小的方法，在深度学习中引入了自适应批大小策略，证明AdaGradNorm以高概率在$O(1/K)$速度下收敛。

    

    随机梯度优化器中批量大小的选择对模型训练至关重要。然而，在训练过程中变化批大小的实践相对其他超参数较少探讨。我们研究了从自适应采样方法中导出的自适应批大小策略，传统上仅应用于随机梯度下降。考虑到学习速率和批大小之间的显著相互作用，以及自适应梯度方法在深度学习中的普及，我们强调在这些情境中需要自适应批大小策略。我们介绍了AdAdaGrad及其标量变体AdAdaGradNorm，它们在训练过程中逐渐增加批大小，同时使用AdaGrad和AdaGradNorm进行模型更新。我们证明了AdaGradNorm以高概率以$O(1/K)$的速度收敛，用于找到光滑非凸函数的一阶稳定点在$K$次迭代内。

    arXiv:2402.11215v1 Announce Type: new  Abstract: The choice of batch sizes in stochastic gradient optimizers is critical for model training. However, the practice of varying batch sizes throughout the training process is less explored compared to other hyperparameters. We investigate adaptive batch size strategies derived from adaptive sampling methods, traditionally applied only in stochastic gradient descent. Given the significant interplay between learning rates and batch sizes, and considering the prevalence of adaptive gradient methods in deep learning, we emphasize the need for adaptive batch size strategies in these contexts. We introduce AdAdaGrad and its scalar variant AdAdaGradNorm, which incrementally increase batch sizes during training, while model updates are performed using AdaGrad and AdaGradNorm. We prove that AdaGradNorm converges with high probability at a rate of $\mathscr{O}(1/K)$ for finding a first-order stationary point of smooth nonconvex functions within $K$ i
    
[^193]: 探索ChatGPT在下一代信息检索中的应用：机遇与挑战

    Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges

    [https://arxiv.org/abs/2402.11203](https://arxiv.org/abs/2402.11203)

    ChatGPT作为信息检索领域的关键技术，不断挑战传统范式，带来了新的机遇和挑战，同时超越了之前的GPT-3模型。

    

    人工智能（AI）的快速发展凸显了ChatGPT作为信息检索（IR）领域中的关键技术。与之前的模型不同，ChatGPT提供了显著的好处，吸引了行业和学术界的关注。一些人认为ChatGPT是一项开创性的创新，而另一些人将其成功归因于产品开发和市场策略的有效整合。ChatGPT的出现，以及与OpenAI的GPT-4一起，标志着生成式AI的新阶段，产生的内容与训练样本有所不同，并超越了以往的GPT-3模型的能力。与信息检索任务中的传统监督学习方法不同，ChatGPT挑战了现有的范式，带来了关于文本质量保证、模型偏差和效率方面的新挑战和机遇。本文旨在研究ChatGPT对信息检索任务的影响，并提供

    arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
    
[^194]: 在异步联邦学习中实现具有异构客户端的线性加速度

    Achieving Linear Speedup in Asynchronous Federated Learning with Heterogeneous Clients

    [https://arxiv.org/abs/2402.11198](https://arxiv.org/abs/2402.11198)

    提出了一种异步联邦学习(AFL)框架，旨在解决具有异构客户端的联邦学习中同步问题，实现线性加速。

    

    联邦学习(FL)是一种新兴的分布式训练范式，旨在学习一个通用的全局模型，而无需交换或传输存储在不同客户端本地的数据。基于Federated Averaging (FedAvg)的算法在FL中备受欢迎，以减少通信开销，其中每个客户端在与中央服务器通信之前进行多个本地化迭代。本文重点研究了具有不同计算和/或通信能力的客户端的FL。在这种情况下，FedAvg可能不那么高效，因为它要求参与全局聚合的所有客户端在一个回合中从最新的全局模型开始迭代，因此快速客户端和滞后客户端之间的同步会严重拖慢整个训练过程。为了解决这个问题，我们提出了一种称为高效异步联邦学习(AFL)框架

    arXiv:2402.11198v1 Announce Type: new  Abstract: Federated learning (FL) is an emerging distributed training paradigm that aims to learn a common global model without exchanging or transferring the data that are stored locally at different clients. The Federated Averaging (FedAvg)-based algorithms have gained substantial popularity in FL to reduce the communication overhead, where each client conducts multiple localized iterations before communicating with a central server. In this paper, we focus on FL where the clients have diverse computation and/or communication capabilities. Under this circumstance, FedAvg can be less efficient since it requires all clients that participate in the global aggregation in a round to initiate iterations from the latest global model, and thus the synchronization among fast clients and straggler clients can severely slow down the overall training process. To address this issue, we propose an efficient asynchronous federated learning (AFL) framework call
    
[^195]: 在连续学习中保持对抗性鲁棒性

    Maintaining Adversarial Robustness in Continuous Learning

    [https://arxiv.org/abs/2402.11196](https://arxiv.org/abs/2402.11196)

    提出了一种名为双梯度投影的方法，通过将梯度投影到两个关键子空间来实现持续鲁棒学习，有效地维持了神经网络对抗性鲁棒性。

    

    对抗性鲁棒性对于机器学习系统的安全性和可靠性至关重要。然而，通过复杂的防御算法获得的对抗性鲁棒性在神经网络不断演化以学习新任务时很容易被抹去。这种脆弱性可以通过培养一种新颖的神经网络能力来解决，称为持续鲁棒学习，它在连续学习过程中关注前期任务的(分类)性能和对抗性鲁棒性。为了实现持续鲁棒学习，我们提出了一种称为双梯度投影的方法，将用于权重更新的梯度正交投影到两个关键子空间上 -- 一个用于稳定平滑样本梯度，另一个用于稳定神经网络的最终输出。在四个基准测试上的实验结果表明，所提出的方法有效地维持了对强对抗性的持续鲁棒性。

    arXiv:2402.11196v1 Announce Type: cross  Abstract: Adversarial robustness is essential for security and reliability of machine learning systems. However, the adversarial robustness gained by sophisticated defense algorithms is easily erased as the neural network evolves to learn new tasks. This vulnerability can be addressed by fostering a novel capability for neural networks, termed continual robust learning, which focuses on both the (classification) performance and adversarial robustness on previous tasks during continuous learning. To achieve continuous robust learning, we propose an approach called Double Gradient Projection that projects the gradients for weight updates orthogonally onto two crucial subspaces -- one for stabilizing the smoothed sample gradients and another for stabilizing the final outputs of the neural network. The experimental results on four benchmarks demonstrate that the proposed approach effectively maintains continuous robustness against strong adversarial
    
[^196]: 自组织映射的最小监督拓扑投影用于飞行阶段识别

    Minimally Supervised Topological Projections of Self-Organizing Maps for Phase of Flight Identification

    [https://arxiv.org/abs/2402.11185](https://arxiv.org/abs/2402.11185)

    本研究提出了一种最小监督自组织映射方法，利用最近邻多数投票来进行类别估计，在飞行阶段识别方面表现出很好的效果，并且对类别不平衡更加稳健

    

    在通用航空领域，识别飞行阶段是重要的，因为了解从飞行器飞行数据记录器收集的数据处于哪个飞行阶段，可以帮助更有效地检测安全或危险事件。本文研究了一种新的最小监督自组织映射（MS-SOMs）方法，该方法利用SOM U-矩阵中的最近邻多数投票进行类估计。结果表明，所提出的方法可以达到或超过使用完整标记数据文件的朴素SOM方法，每个类别仅需要30个标记数据点。此外，最小监督SOM对类别不平衡更具鲁棒性。

    arXiv:2402.11185v1 Announce Type: new  Abstract: Identifying phases of flight is important in the field of general aviation, as knowing which phase of flight data is collected from aircraft flight data recorders can aid in the more effective detection of safety or hazardous events. General aviation flight data for phase of flight identification is usually per-second data, comes on a large scale, and is class imbalanced. It is expensive to manually label the data and training classification models usually faces class imbalance problems. This work investigates the use of a novel method for minimally supervised self-organizing maps (MS-SOMs) which utilize nearest neighbor majority votes in the SOM U-matrix for class estimation. Results show that the proposed method can reach or exceed a naive SOM approach which utilized a full data file of labeled data, with only 30 labeled datapoints per class. Additionally, the minimally supervised SOM is significantly more robust to the class imbalance
    
[^197]: 演化过程的图卷积神经网络模型的不确定性量化

    Uncertainty Quantification of Graph Convolution Neural Network Models of Evolving Processes

    [https://arxiv.org/abs/2402.11179](https://arxiv.org/abs/2402.11179)

    本研究比较了使用哈密顿蒙特卡洛和斯坦变分梯度下降等方法，对建模复杂时空过程的神经网络进行参数不确定性量化，特别是应用于图卷积神经网络模型的演化过程，展示了...

    

    近年来，神经网络模型在科学机器学习任务中的应用不断增加。特别是，神经网络模型已被证明在建模具有时空复杂性的过程方面非常擅长。然而，这些高度参数化的模型在能够在感兴趣的区域内产生具有量化误差界限的输出方面引起了怀疑。因此，有必要找到适用于神经网络的不确定性量化方法。在这项工作中，我们对使用哈密顿蒙特卡洛和斯坦变分梯度下降以及其投影变体对建模复杂时空过程的神经网络进行参数不确定性量化的比较进行了展示。具体来说，我们将这些方法应用于使用递归神经网络和神经常微分方程架构建模的演化系统的图卷积神经网络模型。我们展示了S

    arXiv:2402.11179v1 Announce Type: new  Abstract: The application of neural network models to scientific machine learning tasks has proliferated in recent years. In particular, neural network models have proved to be adept at modeling processes with spatial-temporal complexity. Nevertheless, these highly parameterized models have garnered skepticism in their ability to produce outputs with quantified error bounds over the regimes of interest. Hence there is a need to find uncertainty quantification methods that are suitable for neural networks. In this work we present comparisons of the parametric uncertainty quantification of neural networks modeling complex spatial-temporal processes with Hamiltonian Monte Carlo and Stein variational gradient descent and its projected variant. Specifically we apply these methods to graph convolutional neural network models of evolving systems modeled with recurrent neural network and neural ordinary differential equations architectures. We show that S
    
[^198]: 如何在隐私条件下使梯度变得更小：改进的差分隐私非凸优化速率

    How to Make the Gradients Small Privately: Improved Rates for Differentially Private Non-Convex Optimization

    [https://arxiv.org/abs/2402.11173](https://arxiv.org/abs/2402.11173)

    提出了一种设计具有差分隐私算法的简单灵活框架，用于寻找非凸损失函数的近似稳定点，并获得了改进和有时是最优的速率。

    

    我们提供了一个简单灵活的框架，用于设计具有差分隐私算法，以找到非凸损失函数的近似稳定点。我们的框架基于使用私有的近似风险最小化器来“热启动”另一个用于寻找稳定点的私有算法。我们利用这个框架来获得对几类非凸损失函数的改进甚至是最优速率。首先，我们改进了寻找平滑非凸经验损失函数稳定点的速率。其次，我们专门针对夸萨-凸函数，这种函数概括了星-凸函数，并在学习动态系统和训练一些神经网络时出现。我们为这个类别实现了最优速率。第三，我们提供了一种对满足Kurdyka-Lojasiewicz（KL）条件的函数寻找稳定点的最优算法。例如，超参数化神经网络经常满足这个条件。

    arXiv:2402.11173v1 Announce Type: new  Abstract: We provide a simple and flexible framework for designing differentially private algorithms to find approximate stationary points of non-convex loss functions. Our framework is based on using a private approximate risk minimizer to "warm start" another private algorithm for finding stationary points. We use this framework to obtain improved, and sometimes optimal, rates for several classes of non-convex loss functions. First, we obtain improved rates for finding stationary points of smooth non-convex empirical loss functions. Second, we specialize to quasar-convex functions, which generalize star-convex functions and arise in learning dynamical systems and training some neural nets. We achieve the optimal rate for this class. Third, we give an optimal algorithm for finding stationary points of functions satisfying the Kurdyka-Lojasiewicz (KL) condition. For example, over-parameterized neural networks often satisfy this condition. Fourth, 
    
[^199]: 基于信任区域的黑盒概率认证解释

    Trust Regions for Explanations via Black-Box Probabilistic Certification

    [https://arxiv.org/abs/2402.11168](https://arxiv.org/abs/2402.11168)

    通过黑盒概率认证解释的信任区域能够有效地洞察模型行为、保证解释的稳定性，并实现解释的重用

    

    由于机器学习模型的黑盒性质，人们开发了大量的可解释性方法来解析个别决策背后的因素。本文提出了一个新颖的黑盒（概率性）解释认证问题。我们提出了一个问题：给定一个黑盒模型，只有查询访问权，一个示例的解释以及一个质量度量（如逼真度、稳定性），我们是否能找到最大的超立方体（即 $\ell_{\infty}$ 球），以示例为中心，使得当解释被应用于超立方体内的所有示例时（高概率下）质量标准得到满足（比如逼真度高于某个值）？能够高效地找到这样一个信任区域有多重好处：i）洞察模型在一个区域内的行为，具有保证；ii）解释的稳定性得到保证；iii）解释的重用，可以节省时间、精力和金钱。

    arXiv:2402.11168v1 Announce Type: cross  Abstract: Given the black box nature of machine learning models, a plethora of explainability methods have been developed to decipher the factors behind individual decisions. In this paper, we introduce a novel problem of black box (probabilistic) explanation certification. We ask the question: Given a black box model with only query access, an explanation for an example and a quality metric (viz. fidelity, stability), can we find the largest hypercube (i.e., $\ell_{\infty}$ ball) centered at the example such that when the explanation is applied to all examples within the hypercube, (with high probability) a quality criterion is met (viz. fidelity greater than some value)? Being able to efficiently find such a \emph{trust region} has multiple benefits: i) insight into model behavior in a \emph{region}, with a \emph{guarantee}; ii) ascertained \emph{stability} of the explanation; iii) \emph{explanation reuse}, which can save time, energy and mone
    
[^200]: 高效的低秩矩阵估计、实验设计和基于Arm集的低秩赌博机

    Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits

    [https://arxiv.org/abs/2402.11156](https://arxiv.org/abs/2402.11156)

    提出一种新型低秩矩阵估计方法LowPopArt，通过最小化量B(Q)提供更紧密的恢复保证，同时提出了一种新颖的实验设计标准，以及两种适用于一般Arm集的低秩线性赌博算法。

    

    我们研究了低秩矩阵迹回归和相关的低秩矩阵赌博问题。假设可以访问协变量的分布，我们提出了一种名为LowPopArt的新型低秩矩阵估计方法，并提供了其依赖于一个新颖数量B(Q)的恢复保证，该数量表征了问题的难度，其中Q是测量分布的协方差矩阵。我们展示了我们的方法在几个问题中可以提供比经典的核范数惩罚最小二乘法（Koltchinskii等人，2011）更紧密的恢复保证。为了在从任意给定的测量集合A中进行有限测量的情况下执行高效估计，我们还提出了一种新颖的实验设计标准，该标准以计算效率最小化B(Q)。我们利用我们的新颖估计器和实验设计推导了两种适用于一般Arm集的低秩线性赌博算法，其享有改进的

    arXiv:2402.11156v1 Announce Type: cross  Abstract: We study low-rank matrix trace regression and the related problem of low-rank matrix bandits. Assuming access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called LowPopArt and provide its recovery guarantee that depends on a novel quantity denoted by B(Q) that characterizes the hardness of the problem, where Q is the covariance matrix of the measurement distribution. We show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (Koltchinskii et al., 2011) in several problems. To perform efficient estimation with a limited number of measurements from an arbitrarily given measurement set A, we also propose a novel experimental design criterion that minimizes B(Q) with computational efficiency. We leverage our novel estimator and design of experiments to derive two low-rank linear bandit algorithms for general arm sets that enjoy improved 
    
[^201]: 超越泛化：图上的分布适应性调查

    Beyond Generalization: A Survey of Out-Of-Distribution Adaptation on Graphs

    [https://arxiv.org/abs/2402.11153](https://arxiv.org/abs/2402.11153)

    该调查综述了图像上的分布适应问题，提供最新的图OOD适应方法的回顾，覆盖了训练时和测试时的两种主要问题场景，并提出了分类法。

    

    在图上的分布转移——即训练和测试图机器学习模型之间的数据分布差异，往往普遍且在现实场景中不可避免。这种转移可能严重恶化模型的性能，给可靠的图机器学习带来重大挑战。因此，图图像的Out-Of-Distribution（OOD）适应方法的研究激增，其旨在减轻分布转移，并将一个分布的知识适应到另一个分布。 在我们的调查中，我们提供了图OOD适应方法的最新和前瞻性回顾，涵盖两个主要问题场景，包括训练时和测试时的图OOD适应。 我们首先正式描述了这两个问题，然后讨论了图上的不同类型的分布转移。基于我们对图OOD适应的提出的分类法，我们系统地对现有的方法进行分类。

    arXiv:2402.11153v1 Announce Type: new  Abstract: Distribution shifts on graphs -- the data distribution discrepancies between training and testing a graph machine learning model, are often ubiquitous and unavoidable in real-world scenarios. Such shifts may severely deteriorate the performance of the model, posing significant challenges for reliable graph machine learning. Consequently, there has been a surge in research on graph Out-Of-Distribution (OOD) adaptation methods that aim to mitigate the distribution shifts and adapt the knowledge from one distribution to another. In our survey, we provide an up-to-date and forward-looking review of graph OOD adaptation methods, covering two main problem scenarios including training-time as well as test-time graph OOD adaptation. We start by formally formulating the two problems and then discuss different types of distribution shifts on graphs. Based on our proposed taxonomy for graph OOD adaptation, we systematically categorize the existing 
    
[^202]: 基于转换教师匹配的知识蒸馏

    Knowledge Distillation Based on Transformed Teacher Matching

    [https://arxiv.org/abs/2402.11148](https://arxiv.org/abs/2402.11148)

    通过放弃学生端的温度缩放，本文提出了一种名为转换教师匹配（TTM）的知识蒸馏变体，通过对温度缩放的重新解释，TTM在目标函数中引入了固有的Rényi熵项，从而实现了更好的学生泛化效果。

    

    作为连接逻辑匹配和概率分布匹配的技术，温度缩放在知识蒸馏（KD）中起着关键作用。传统上，在KD中，温度缩放被应用于教师的logits和学生的logits。受一些最近的研究启发，本文放弃了在学生端的温度缩放，系统地研究了由此产生的KD变体，称为转换教师匹配（TTM）。通过重新解释温度缩放作为概率分布的幂变换，我们表明，与原始的KD相比，TTM在其目标函数中具有固有的Rényi熵项，这充当了额外的正则化项。大量的实验结果表明，由于这种固有的正则化，TTM导致训练良好的学生比原始KD具有更好的泛化能力。

    arXiv:2402.11148v1 Announce Type: new  Abstract: As a technique to bridge logit matching and probability distribution matching, temperature scaling plays a pivotal role in knowledge distillation (KD). Conventionally, temperature scaling is applied to both teacher's logits and student's logits in KD. Motivated by some recent works, in this paper, we drop instead temperature scaling on the student side, and systematically study the resulting variant of KD, dubbed transformed teacher matching (TTM). By reinterpreting temperature scaling as a power transform of probability distribution, we show that in comparison with the original KD, TTM has an inherent R\'enyi entropy term in its objective function, which serves as an extra regularization term. Extensive experiment results demonstrate that thanks to this inherent regularization, TTM leads to trained students with better generalization than the original KD. To further enhance student's capability to match teacher's power transformed proba
    
[^203]: 为人类行为分析对话视频支持专家的多模式机器学习工具

    Supporting Experts with a Multimodal Machine-Learning-Based Tool for Human Behavior Analysis of Conversational Videos

    [https://arxiv.org/abs/2402.11145](https://arxiv.org/abs/2402.11145)

    Providence是一个基于视觉编程的多模式机器学习工具，旨在帮助专家捕获人类行为线索，无需编写代码，具有可取的可用性和令人满意的输出。

    

    跨学科对话的多模式场景搜索对解锁有价值的社会动态见解和增强沟通至关重要。我们开发了Providence，这是一个基于视觉编程的工具，旨在提供从专家研究中得出的设计考虑，使专家能够结合各种机器学习算法捕获人类行为线索，而无需编写代码。我们的研究表明，该工具具有可取的可用性和令人满意的输出，执行对话场景搜索任务时所施加的认知负荷较小，验证了其可定制性和透明性的重要性。

    arXiv:2402.11145v1 Announce Type: cross  Abstract: Multimodal scene search of conversations is essential for unlocking valuable insights into social dynamics and enhancing our communication. While experts in conversational analysis have their own knowledge and skills to find key scenes, a lack of comprehensive, user-friendly tools that streamline the processing of diverse multimodal queries impedes efficiency and objectivity. To solve it, we developed Providence, a visual-programming-based tool based on design considerations derived from a formative study with experts. It enables experts to combine various machine learning algorithms to capture human behavioral cues without writing code. Our study showed its preferable usability and satisfactory output with less cognitive load imposed in accomplishing scene search tasks of conversations, verifying the importance of its customizability and transparency. Furthermore, through the in-the-wild trial, we confirmed the objectivity and reusabi
    
[^204]: LiGNN: 领英上的图神经网络

    LiGNN: Graph Neural Networks at LinkedIn

    [https://arxiv.org/abs/2402.11139](https://arxiv.org/abs/2402.11139)

    本文介绍了在领英上开发和部署的LiGNN框架，包括对GNN表示学习的算法改进和大规模训练优化，为工作申请回复率、广告点击率和Feed每日活跃用户提高带来了约1%-2%的相对改善。

    

    在本文中，我们提出了LiGNN，一种已部署的大规模图神经网络（GNNs）框架。我们分享了在领英上开发和部署GNNs的见解。我们提出了一组算法改进，包括具有长期损失的时间图架构，通过图密集化实现的有效冷启动解决方案，ID嵌入和多跳邻居采样，以改进GNN表示学习的质量。我们解释了如何通过邻居的自适应采样，对训练数据批次进行分组和切片，专门的共享内存队列和本地梯度优化将LinkedIn图的大规模训练加快7倍。我们总结了从A/B测试实验中获得的部署经验和教训。本文介绍的技术已经为工作申请回复率的相对改善率约为1％，广告点击率提升2％，Feed每日活跃用户提高0.5％做出了贡献。

    arXiv:2402.11139v1 Announce Type: cross  Abstract: In this paper, we present LiGNN, a deployed large-scale Graph Neural Networks (GNNs) Framework. We share our insight on developing and deployment of GNNs at large scale at LinkedIn. We present a set of algorithmic improvements to the quality of GNN representation learning including temporal graph architectures with long term losses, effective cold start solutions via graph densification, ID embeddings and multi-hop neighbor sampling. We explain how we built and sped up by 7x our large-scale training on LinkedIn graphs with adaptive sampling of neighbors, grouping and slicing of training data batches, specialized shared-memory queue and local gradient optimization. We summarize our deployment lessons and learnings gathered from A/B test experiments. The techniques presented in this work have contributed to an approximate relative improvements of 1% of Job application hearing back rate, 2% Ads CTR lift, 0.5% of Feed engaged daily active 
    
[^205]: 对比指令调整

    Contrastive Instruction Tuning

    [https://arxiv.org/abs/2402.11138](https://arxiv.org/abs/2402.11138)

    提出了对比指令调整方法，通过最大化相似性来提高大型语言模型对未知任务指令的稳健性

    

    指令调整一直被用作改善大型语言模型（LLMs）在未知任务上的性能的一种有前途的方法。然而，当前的LLMs在面临未知指令时表现出有限的稳健性，当相同的指令以稍微变化的形式或语言风格提出时会产生不一致的输出。这种行为表明LLMs对文本变化的稳健性和对未知指令的泛化能力不足，可能会导致可信度问题。因此，我们提出了对比指令调整，该方法在最大化语义上等价的指令-实例对的隐藏表示之间的相似性的同时，最小化语义上不同的对之间的相似性。为了促进这种方法，我们通过释义任务指令，扩充现有的FLAN集合。在PromptBench基准测试上的实验表明，对比指令调整（CoIN）一直提高了LLMs对未知指令的稳健性

    arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
    
[^206]: TuneTables：可扩展先验数据拟合网络的上下文优化

    TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks

    [https://arxiv.org/abs/2402.11137](https://arxiv.org/abs/2402.11137)

    提出了TuneTables上下文优化技术，通过开发一种新的提示调整策略，将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。

    

    针对表格分类传统上依赖于从零开始训练的问题，最近提出了一个名为先验数据拟合网络（PFN）的突破性方法，挑战了这种方法。类似于大型语言模型，PFN利用预训练和上下文学习，在单次前向传递中在新任务上取得强大表现。然而，当前的PFN存在限制，阻碍了它们的广泛采用。特别是，TabPFN在小型表格数据集上取得非常强劲的性能，但并不适用于数据集大小大于1000的预测。在这项工作中，我们通过为PFN开发上下文优化技术，克服了这些限制，大幅提高了PFN的性能。具体来说，我们提出了TuneTables，一种将大型数据集压缩为较小学习上下文的新型提示调整策略。TuneTables将TabPFN扩展到与更大数据上的最先进表格分类方法相竞争。

    arXiv:2402.11137v1 Announce Type: new  Abstract: While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs by developing context optimization techniques for PFNs. Specifically, we propose TuneTables, a novel prompt-tuning strategy that compresses large datasets into a smaller learned context. TuneTables scales TabPFN to be competitive with state-of-the-art tabular classification methods on larger datas
    
[^207]: 推测式流式处理: 无需辅助模型的快速LLM推理

    Speculative Streaming: Fast LLM Inference without Auxiliary Models

    [https://arxiv.org/abs/2402.11131](https://arxiv.org/abs/2402.11131)

    提出了一种Speculative Streaming方法，将草稿模型融入目标模型，并通过将微调目标从下一个令牌预测更改为未来的n-gram预测，加速解码1.8-3.1倍，同时保持生成质量。

    

    推测式解码是一种突出的技术，可以提高基于辅助草稿模型预测的大型目标语言模型的推理速度。虽然在特定应用设置中有效，但通常需要微调草稿和目标模型以实现较高的接受率。随着下游任务数量的增加，这些草稿模型给推理系统增加了显著的复杂性。我们提出了Speculative Streaming，一种单模型的推测式解码方法，通过将草拟融入目标模型，将微调目标从下一个令牌预测对象更改为未来的n-gram预测。 Speculative Streaming在各种任务中加速解码1.8-3.1倍，如摘要、结构化查询和意义表达，同时不降低生成质量。此外，Speculative Streaming参数有效。它实现了与Medusa风格架构相媲美/更高的加速度

    arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u
    
[^208]: Kolmogorov n-宽度用于多任务物理信息机器学习（PIML）方法：朝向稳健指标

    Kolmogorov n-Widths for Multitask Physics-Informed Machine Learning (PIML) Methods: Towards Robust Metrics

    [https://arxiv.org/abs/2402.11126](https://arxiv.org/abs/2402.11126)

    本研究使用Kolmogorov n-宽度作为评估多任务PIML架构的有效性的方法，分析模型在各种PDE问题上学到的基函数。

    

    arXiv:2402.11126v1 声明类型：新摘要：作为解决偏微分方程（PDE）的手段，物理信息机器学习（PIML）在计算科学与工程（CS&E）领域引起了广泛关注。 这个话题涵盖了旨在解决单个或多个PDE问题的各种方法和模型，称为多任务学习。 PIML 的特点是在解决 PDE 问题时，将物理定律纳入机器学习模型的训练过程中，而不是大数据。 尽管这一系列方法的整体成功，但分析、基准测试和通常比较一种方法与另一种方法仍然非常困难。 我们使用 Kolmogorov n-宽度作为近似函数有效性的衡量标准，审慎地将这一指标应用于比较各种多任务 PIML 结构。 我们计算较低的准确度下界，并分析模型在各种 PDE 问题上学到的基函数。

    arXiv:2402.11126v1 Announce Type: new  Abstract: Physics-informed machine learning (PIML) as a means of solving partial differential equations (PDE) has garnered much attention in the Computational Science and Engineering (CS&E) world. This topic encompasses a broad array of methods and models aimed at solving a single or a collection of PDE problems, called multitask learning. PIML is characterized by the incorporation of physical laws into the training process of machine learning models in lieu of large data when solving PDE problems. Despite the overall success of this collection of methods, it remains incredibly difficult to analyze, benchmark, and generally compare one approach to another. Using Kolmogorov n-widths as a measure of effectiveness of approximating functions, we judiciously apply this metric in the comparison of various multitask PIML architectures. We compute lower accuracy bounds and analyze the model's learned basis functions on various PDE problems. This is the fi
    
[^209]: 通过开关变量在隐式因果模型中解开纠缠

    Disentanglement in Implicit Causal Models via Switch Variable

    [https://arxiv.org/abs/2402.11124](https://arxiv.org/abs/2402.11124)

    该论文通过软干预处理隐式潜在因果表征学习，在 Variational Autoencoder (VAE) 框架中引入了因果机制开关变量。

    

    从观测和干预数据中学习因果表征，在没有已知的地面真实图结构的情况下，需要隐式潜在因果表征学习。隐式学习因果机制通常涉及两类干预数据：硬干预和软干预。在现实世界场景中，软干预通常比硬干预更现实，因为后者需要完全受控的环境。与直接强制改变因果变量的硬干预不同，软干预通过影响因果机制间接地产生影响。本文通过软干预在变分自动编码器（VAE）框架中处理隐式潜在因果表征学习。我们的方法通过使用一个旨在在不同因果机制之间切换的因果机制开关变量来建模软干预效果。在我们的实验中，我们始终保持

    arXiv:2402.11124v1 Announce Type: new  Abstract: Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicitly learning causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. In this paper, we tackle implicit latent causal representation learning in a Variational Autoencoder (VAE) framework through soft interventions. Our approach models soft interventions effects by employing a causal mechanism switch variable designed to toggle between different causal mechanisms. In our experiments, we consistentl
    
[^210]: 使用情境臂研究优化华法林用量：一种离线策略学习和评估方法

    Optimizing Warfarin Dosing Using Contextual Bandit: An Offline Policy Learning and Evaluation Method

    [https://arxiv.org/abs/2402.11123](https://arxiv.org/abs/2402.11123)

    本研究提出了一种使用情境臂和离线策略学习的方法，通过观察数据建立个性化的华法林剂量策略，即使在缺乏基因型信息的情况下也可以超越基线方法。

    

    华法林是一种抗凝药物，旨在预防和治疗与异常血液凝结相关的疾病，是全球最常开处方的药物之一。然而，由于个体反应变化，确定合适的剂量仍具挑战性，错误的剂量可能导致严重后果。情境臂和强化学习在解决这一问题上显示出了潜力。鉴于历史政策的观察数据广泛可得且医疗决策的安全性，我们专注于使用历史政策作为演示的观测数据，通过离线策略学习和评估在情境臂环境中建立最佳个性化剂量策略。我们学习到的策略在没有基因型信息的情况下超越了这些基线方法，甚至在给定次优演示的情况下也表现出色，展示了巨大的应用潜力。

    arXiv:2402.11123v1 Announce Type: cross  Abstract: Warfarin, an anticoagulant medication, is formulated to prevent and address conditions associated with abnormal blood clotting, making it one of the most prescribed drugs globally. However, determining the suitable dosage remains challenging due to individual response variations, and prescribing an incorrect dosage may lead to severe consequences. Contextual bandit and reinforcement learning have shown promise in addressing this issue. Given the wide availability of observational data and safety concerns of decision-making in healthcare, we focused on using exclusively observational data from historical policies as demonstrations to derive new policies; we utilized offline policy learning and evaluation in a contextual bandit setting to establish the optimal personalized dosage strategy. Our learned policies surpassed these baseline approaches without genotype inputs, even when given a suboptimal demonstration, showcasing promising app
    
[^211]: DART: 一种面向对抗鲁棒的无监督领域自适应的原则性方法

    DART: A Principled Approach to Adversarially Robust Unsupervised Domain Adaptation

    [https://arxiv.org/abs/2402.11120](https://arxiv.org/abs/2402.11120)

    本文探讨了无监督领域自适应中对抗鲁棒性的问题，通过建立对抗目标损失的泛化界限来解决目标域标签缺失带来的挑战。

    

    分布转移和对抗样本是部署机器学习模型面临的两个主要挑战。虽然这些挑战已被分别研究，但它们的结合仍然是一个相对未被充分探索的重要主题。本文研究了在一个常见的分布转移设置下对抗鲁棒性的问题，即无监督领域自适应（UDA）。具体地，给定一个带标签的源域 $D_S$ 和一个带有相关但不同分布的未标记目标域 $D_T$，目标是为 $D_T$ 获得一个对抗鲁棒的模型。目标域标签的缺失提出了一个独特的挑战，因为传统的对抗鲁棒性防御不能直接应用于 $D_T$。为了解决这一挑战，我们首先建立了对抗目标损失的泛化界限，其中包括与数据损失相关的项和最坏情况域分歧的度量。

    arXiv:2402.11120v1 Announce Type: new  Abstract: Distribution shifts and adversarial examples are two major challenges for deploying machine learning models. While these challenges have been studied individually, their combination is an important topic that remains relatively under-explored. In this work, we study the problem of adversarial robustness under a common setting of distribution shift - unsupervised domain adaptation (UDA). Specifically, given a labeled source domain $D_S$ and an unlabeled target domain $D_T$ with related but different distributions, the goal is to obtain an adversarially robust model for $D_T$. The absence of target domain labels poses a unique challenge, as conventional adversarial robustness defenses cannot be directly applied to $D_T$. To address this challenge, we first establish a generalization bound for the adversarial target loss, which consists of (i) terms related to the loss on the data, and (ii) a measure of worst-case domain divergence. Motivat
    
[^212]: 私有PAC学习可能比在线学习更困难

    Private PAC Learning May be Harder than Online Learning

    [https://arxiv.org/abs/2402.11119](https://arxiv.org/abs/2402.11119)

    私有PAC学习和在线学习之间的转换并不总是可以保持计算效率，在一些情况下是困难的。

    

    我们继续研究不同ially private PAC学习的计算复杂性，以及它在机器学习基础中的定位。最近的一系列工作揭示了私有PAC模型与Littlestone的错误界在线学习模型之间的定性等价性，特别是展示了任何Littlestone维度为$d$的概念类都可以使用$\mathrm{poly}(d)$个样本来进行私有PAC学习。这引发了一个自然问题，即是否可能从在线学习者转换为私有PAC学习者，并且还能保持计算效率。我们在合理的加密假设下（大致来自于可以为所有电路构建不可区分混淆的假设）对这个问题给出了否定答案。我们展示了一个概念类，可以接受在多项式时间内运行且具有多项式错误界限的在线学习器，但对于这样一个概念类，不存在

    arXiv:2402.11119v1 Announce Type: new  Abstract: We continue the study of the computational complexity of differentially private PAC learning and how it is situated within the foundations of machine learning. A recent line of work uncovered a qualitative equivalence between the private PAC model and Littlestone's mistake-bounded model of online learning, in particular, showing that any concept class of Littlestone dimension $d$ can be privately PAC learned using $\mathrm{poly}(d)$ samples. This raises the natural question of whether there might be a generic conversion from online learners to private PAC learners that also preserves computational efficiency.   We give a negative answer to this question under reasonable cryptographic assumptions (roughly, those from which it is possible to build indistinguishability obfuscation for all circuits). We exhibit a concept class that admits an online learner running in polynomial time with a polynomial mistake bound, but for which there is no 
    
[^213]: 新西兰温室气体清单的动态现预测

    Dynamic nowcast of the New Zealand greenhouse gas inventory

    [https://arxiv.org/abs/2402.11107](https://arxiv.org/abs/2402.11107)

    通过机器学习方法，提出了一种动态预测新西兰国家温室气体排放的新方法，使得预测能提前至国家排放清单发布之前，并展现了较低误差的次年估计能力。

    

    随着减缓气候变化效果的努力加强，可靠和全面的温室气体排放报告对于衡量国际和国内减排目标的进展至关重要。目前，新西兰的国家排放清单报告滞后于15至27个月。我们提出了一种机器学习方法，通过仅有两个月的延迟（由于当前数据可用性），在新西兰国家排放清单发布之前现在预测国家温室气体排放。主要发现包括自2020年以来国家总排放量的估计减少0.2%（截至2022年7月）。我们的研究突显了动态视角对排放密集型活动的预测能力。这种方法论是一个概念验证，即机器学习方法可以通过较低的误差对各个部门的国家温室气体排放进行次年估计。

    arXiv:2402.11107v1 Announce Type: new  Abstract: As efforts to mitigate the effects of climate change grow, reliable and thorough reporting of greenhouse gas emissions are essential for measuring progress towards international and domestic emissions reductions targets. New Zealand's national emissions inventories are currently reported between 15 to 27 months out-of-date. We present a machine learning approach to nowcast (dynamically estimate) national greenhouse gas emissions in New Zealand in advance of the national emissions inventory's release, with just a two month latency due to current data availability. Key findings include an estimated 0.2% decrease in national gross emissions since 2020 (as at July 2022). Our study highlights the predictive power of a dynamic view of emissions intensive activities. This methodology is a proof of concept that a machine learning approach can make sub-annual estimates of national greenhouse gas emissions by sector with a relatively low error tha
    
[^214]: 通过在空间统计空间优化学习微观结构的潜变量表示

    Toward Learning Latent-Variable Representations of Microstructures by Optimizing in Spatial Statistics Space

    [https://arxiv.org/abs/2402.11103](https://arxiv.org/abs/2402.11103)

    通过在空间统计空间中最小化原始和重建图像之间的距离，实现学习微观结构的低维表示

    

    在材料科学中，材料开发涉及评估和优化材料的内部结构，一般称为微观结构。微观结构是随机的，类似于图像纹理。一个特定的微观结构可以通过其空间统计特征进行良好的表征，类似于图像纹理通过对傅里叶滤波器组的响应进行表征。材料设计将受益于微观结构的低维表示（Paulson等人，2017）。在本研究中，我们训练变分自动编码器（VAE）以生成保留原始纹理的空间统计的重建纹理，而不一定在数据空间中重建相同的图像。我们通过在成本函数中添加可微分项来实现这一点，以最小化原始图像和重建图像之间在空间统计空间中的距离。

    arXiv:2402.11103v1 Announce Type: new  Abstract: In Materials Science, material development involves evaluating and optimizing the internal structures of the material, generically referred to as microstructures. Microstructures structure is stochastic, analogously to image textures. A particular microstructure can be well characterized by its spatial statistics, analogously to image texture being characterized by the response to a Fourier-like filter bank. Material design would benefit from low-dimensional representation of microstructures Paulson et al. (2017).   In this work, we train a Variational Autoencoders (VAE) to produce reconstructions of textures that preserve the spatial statistics of the original texture, while not necessarily reconstructing the same image in data space. We accomplish this by adding a differentiable term to the cost function in order to minimize the distance between the original and the reconstruction in spatial statistics space.   Our experiments indicate
    
[^215]: 通过贝叶斯优化从钙钛矿实验中提取基于物理的材料参数

    Physics-based material parameters extraction from perovskite experiments via Bayesian optimization

    [https://arxiv.org/abs/2402.11101](https://arxiv.org/abs/2402.11101)

    使用贝叶斯优化开发了一个分析平台，可以从钙钛矿实验中提取多个基本材料参数，加速材料发现和半导体优化

    

    从定量实验分析中提取材料参数的能力对于合理设计和理论进步至关重要。然而，随着理论模型的复杂性和材料参数数量的增加，这种分析的难度显着增加。在这里，我们使用贝叶斯优化开发了一个分析平台，可以从瞬态光致发光实验中提取一个有机金属钙钛矿半导体的8个基本材料参数，基于一个包括载流子漂移扩散和动态缺陷占据的复杂全物理模型。热降解的一个示例研究表明，掺杂浓度和载流子迁移率的变化主导，而缺陷能级几乎保持不变。这个平台可以方便地应用于其他实验或实验组合，加速材料发现和半导体优化。

    arXiv:2402.11101v1 Announce Type: cross  Abstract: The ability to extract material parameters from quantitative experimental analysis is essential for rational design and theory advancement. However, the difficulty of this analysis increases significantly with the complexity of the theoretical model and the number of material parameters. Here we use Bayesian optimization to develop an analysis platform that can extract up to 8 fundamental material parameters of an organometallic perovskite semiconductor from a transient photoluminescence experiment, based on a complex full physics model that includes drift-diffusion of carriers and dynamic defect occupation. An example study of thermal degradation reveals that changes in doping concentration and carrier mobility dominate, while the defect energy level remains nearly unchanged. This platform can be conveniently applied to other experiments or to combinations of experiments, accelerating materials discovery and optimization of semiconduc
    
[^216]: 用于手写电路图像的模块化图形提取

    Modular Graph Extraction for Handwritten Circuit Diagram Images

    [https://arxiv.org/abs/2402.11093](https://arxiv.org/abs/2402.11093)

    本文描述了一种模块化的端到端手写电路图像的图形提取方法，以解决从栅格图像中提取电气图的问题。

    

    随着工程领域的数字化进展，电路图（也称为原理图）通常是在计算机辅助工程（CAE）系统中开发和维护的，从而实现自动化验证、模拟和在下游工程步骤中进一步加工。然而，除了印刷的传统原理图外，在教育领域中今天仍然使用手绘电路图，它们作为学员和学生学习绘制此类图表的一种容易获取的手段。此外，由于法律约束，手绘原理图通常用于考试。为了利用数字电路表示的功能，需要自动提取栅格图形中的电路图的方法。虽然文献中提出了相应的方法，但它们通常是在小型或未公开的数据集上进行的。本文描述了一种模块化的端到端s

    arXiv:2402.11093v1 Announce Type: cross  Abstract: As digitization in engineering progressed, circuit diagrams (also referred to as schematics) are typically developed and maintained in computer-aided engineering (CAE) systems, thus allowing for automated verification, simulation and further processing in downstream engineering steps. However, apart from printed legacy schematics, hand-drawn circuit diagrams are still used today in the educational domain, where they serve as an easily accessible mean for trainees and students to learn drawing this type of diagrams. Furthermore, hand-drawn schematics are typically used in examinations due to legal constraints. In order to harness the capabilities of digital circuit representations, automated means for extracting the electrical graph from raster graphics are required.   While respective approaches have been proposed in literature, they are typically conducted on small or non-disclosed datasets. This paper describes a modular end-to-end s
    
[^217]: 通过纯微调进行模型编辑

    Model Editing by Pure Fine-Tuning

    [https://arxiv.org/abs/2402.11078](https://arxiv.org/abs/2402.11078)

    纯微调通过优化条件似然、增加随机释义和事实的数据，在模型编辑中取得了不俗的表现。

    

    精细调整被认为在模型编辑中不够有效，因为相对更专业的方法而言，它的表现较差。然而，微调是简单的，不关心被编辑模型的体系结构细节，并且能够利用标准训练方法的不断进展（例如PEFT），使其成为模型编辑器的吸引选择。在本文中，我们展示了纯粹的微调可以是一种可行的模型编辑方法。我们提出了对朴素微调进行轻微修改的两个关键因素。第一，我们优化条件似然而非完整似然。第二，我们使用随机释义和事实来增加数据，以鼓励泛化和局部性。我们在ZsRE和CounterFact上的实验表明，这一简单修改使得微调通常可以与专业编辑器在编辑分数方面匹敌甚至超越。

    arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
    
[^218]: 通过金融时间序列聚类实现金融包容性信贷产品

    Towards Financially Inclusive Credit Products Through Financial Time Series Clustering

    [https://arxiv.org/abs/2402.11066](https://arxiv.org/abs/2402.11066)

    通过金融时间序列聚类，解决金融包容性信贷产品中客户分割的挑战

    

    金融包容性确保个人能够获得满足其需求的金融产品和服务。利用消费者交易数据进行客户分割是一种促进金融包容的常用策略，然而，对数据标注的需求通常难以获取，这导致了时间序列分类模型在基于领域专家知识进行客户分割时的挑战。

    arXiv:2402.11066v1 Announce Type: new  Abstract: Financial inclusion ensures that individuals have access to financial products and services that meet their needs. As a key contributing factor to economic growth and investment opportunity, financial inclusion increases consumer spending and consequently business development. It has been shown that institutions are more profitable when they provide marginalised social groups access to financial services. Customer segmentation based on consumer transaction data is a well-known strategy used to promote financial inclusion. While the required data is available to modern institutions, the challenge remains that segment annotations are usually difficult and/or expensive to obtain. This prevents the usage of time series classification models for customer segmentation based on domain expert knowledge. As a result, clustering is an attractive alternative to partition customers into homogeneous groups based on the spending behaviour encoded with
    
[^219]: 具有领域标签噪声的亚群体转移鲁棒性通过领域标注的正则化

    Robustness to Subpopulation Shift with Domain Label Noise via Regularized Annotation of Domains

    [https://arxiv.org/abs/2402.11039](https://arxiv.org/abs/2402.11039)

    提出了一种名为RAD的方法，通过领域标注的正则化来训练鲁棒的最后一层分类器，无需显式的领域标注，在具有领域标签噪声的情况下表现优越。

    

    现有针对最优组准确性(WGA)进行最后一层重新训练的方法在训练数据中过于依赖于良好标注的组。我们理论上和实践中展示了，基于注释的数据增强使用下采样或上加权用于WGA是容易受到领域标注噪声干扰，在高噪声情况下接近使用原始经验风险最小化训练的模型的WGA。我们引入了领域标注正则化(RAD)来训练具有鲁棒性的最后一层分类器，而无需明确的领域标注。我们的结果表明，RAD与其他最近提出的无领域标注技术具有竞争力。最重要的是，即使在训练数据中仅有5%的噪声，RAD也在几个公开可用数据集上胜过最先进的依赖注释的方法。

    arXiv:2402.11039v1 Announce Type: new  Abstract: Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA) rely heavily on well-annotated groups in the training data. We show, both in theory and practice, that annotation-based data augmentations using either downsampling or upweighting for WGA are susceptible to domain annotation noise, and in high-noise regimes approach the WGA of a model trained with vanilla empirical risk minimization. We introduce Regularized Annotation of Domains (RAD) in order to train robust last layer classifiers without the need for explicit domain annotations. Our results show that RAD is competitive with other recently proposed domain annotation-free techniques. Most importantly, RAD outperforms state-of-the-art annotation-reliant methods even with only 5% noise in the training data for several publicly available datasets.
    
[^220]: 具有遮挡鲁棒性的3D人体姿势估计

    Occlusion Resilient 3D Human Pose Estimation

    [https://arxiv.org/abs/2402.11036](https://arxiv.org/abs/2402.11036)

    通过将身体建模为时空图并引入一个精炼网络，来实现对遮挡具有鲁棒性的3D人体姿势估计。

    

    遮挡仍然是单摄像头视频序列中3D人体姿势估计的关键挑战之一。虽然时间一致性已被广泛用于减轻其影响，但文献中的现有算法并没有明确对其进行建模。在这里，我们通过将变形的身体表示为时空图来应用这一点。然后，我们引入一个精炼网络，该网络对该图执行图卷积以输出3D姿势。为了确保对遮挡具有鲁棒性，我们使用一组二进制掩码来训练这个网络，我们将其用于禁用一些边，如去除技术中。实际上，我们模拟了一些关节可能在一段时间内被隐藏的事实，并训练网络免疫于此。我们展示了与从单摄像头序列推断姿势的最先进技术相比，这种方法的有效性。

    arXiv:2402.11036v1 Announce Type: cross  Abstract: Occlusions remain one of the key challenges in 3D body pose estimation from single-camera video sequences. Temporal consistency has been extensively used to mitigate their impact but the existing algorithms in the literature do not explicitly model them.   Here, we apply this by representing the deforming body as a spatio-temporal graph. We then introduce a refinement network that performs graph convolutions over this graph to output 3D poses. To ensure robustness to occlusions, we train this network with a set of binary masks that we use to disable some of the edges as in drop-out techniques.   In effect, we simulate the fact that some joints can be hidden for periods of time and train the network to be immune to that. We demonstrate the effectiveness of this approach compared to state-of-the-art techniques that infer poses from single-camera sequences.
    
[^221]: 使用稀疏子空间变分推断训练贝叶斯神经网络

    Training Bayesian Neural Networks with Sparse Subspace Variational Inference

    [https://arxiv.org/abs/2402.11025](https://arxiv.org/abs/2402.11025)

    提出了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架

    

    贝叶斯神经网络（BNN）提供了不确定性量化，但代价是大幅增加训练和推断成本。稀疏BNN已被研究用于高效推断，通常通过在训练过程中逐渐引入稀疏性或通过后续对密集BNN进行压缩。然而，如何降低巨大的训练成本仍然是一个难题，特别是考虑到需要学习不确定性。为了解决这一挑战，我们引入了稀疏子空间变分推断（SSVI），这是第一个在训练和推断阶段始终保持高度稀疏的贝叶斯模型的全稀疏BNN框架。我们的方法从一个随机初始化的低维稀疏子空间开始，交替优化稀疏子空间基向量的选择以及相关参数。尽管基向量选择被描述为一个不可微分的问题，我们近似求解该问题。

    arXiv:2402.11025v1 Announce Type: new  Abstract: Bayesian neural networks (BNNs) offer uncertainty quantification but come with the downside of substantially increased training and inference costs. Sparse BNNs have been investigated for efficient inference, typically by either slowly introducing sparsity throughout the training or by post-training compression of dense BNNs. The dilemma of how to cut down massive training costs remains, particularly given the requirement to learn about the uncertainty. To solve this challenge, we introduce Sparse Subspace Variational Inference (SSVI), the first fully sparse BNN framework that maintains a consistently highly sparse Bayesian model throughout the training and inference phases. Starting from a randomly initialized low-dimensional sparse subspace, our approach alternately optimizes the sparse subspace basis selection and its associated parameters. While basis selection is characterized as a non-differentiable problem, we approximate the opti
    
[^222]: 使用真实世界语料库自动检测和分析数据实践

    Automated Detection and Analysis of Data Practices Using A Real-World Corpus

    [https://arxiv.org/abs/2402.11006](https://arxiv.org/abs/2402.11006)

    本文提出了一种使用真实世界语料库自动检测和分析隐私政策中数据实践的方法，并通过实验和案例研究证明了其有效性。

    

    隐私政策对于告知用户数据实践至关重要，然而其长度和复杂性经常阻止用户阅读。本文提出了一种自动化方法，用于识别和可视化隐私政策中不同细节级别的数据实践。利用来自ToS;DR平台的众包注释，我们尝试使用各种方法匹配政策摘录与预定义的数据实践描述。我们进一步进行了案例研究，评估我们的方法在真实政策中的效果，证明其在简化复杂政策方面的有效性。实验表明，我们的方法能够准确匹配数据实践描述和政策摘录，有助于向用户呈现简化的隐私信息。

    arXiv:2402.11006v1 Announce Type: cross  Abstract: Privacy policies are crucial for informing users about data practices, yet their length and complexity often deter users from reading them. In this paper, we propose an automated approach to identify and visualize data practices within privacy policies at different levels of detail. Leveraging crowd-sourced annotations from the ToS;DR platform, we experiment with various methods to match policy excerpts with predefined data practice descriptions. We further conduct a case study to evaluate our approach on a real-world policy, demonstrating its effectiveness in simplifying complex policies. Experiments show that our approach accurately matches data practice descriptions with policy excerpts, facilitating the presentation of simplified privacy information to users.
    
[^223]: 在上下文学习马尔可夫链的统计归纳头的演变

    The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains

    [https://arxiv.org/abs/2402.11004](https://arxiv.org/abs/2402.11004)

    研究了大型语言模型如何通过训练在上下文学习中准确预测下一个标记概率，并发现了一个多阶段的过程。

    

    大型语言模型能够生成模仿其输入模式的文本。我们引入了一个简单的马尔可夫链序列建模任务，以研究这种上下文学习（ICL）能力是如何出现的。在我们的设定中，每个例子是从一个从马尔可夫链先验分布中抽取的马尔可夫链中抽取的。在这个任务中训练的Transformer形成了计算给定上下文双字母统计的准确下一个标记概率的\emph{统计归纳头}。在训练过程中，模型经过多个阶段：在初始阶段，预测是均匀的；他们学会使用上下文单标记统计（一元组）进行次优预测；然后，快速过渡到正确的上下文双字母解决方案。我们对这个多阶段过程进行了经验和理论调查，显示了成功学习是如何来自于

    arXiv:2402.11004v1 Announce Type: new  Abstract: Large language models have the ability to generate text that mimics patterns in their inputs. We introduce a simple Markov Chain sequence modeling task in order to study how this in-context learning (ICL) capability emerges. In our setting, each example is sampled from a Markov chain drawn from a prior distribution over Markov chains. Transformers trained on this task form \emph{statistical induction heads} which compute accurate next-token probabilities given the bigram statistics of the context. During the course of training, models pass through multiple phases: after an initial stage in which predictions are uniform, they learn to sub-optimally predict using in-context single-token statistics (unigrams); then, there is a rapid phase transition to the correct in-context bigram solution. We conduct an empirical and theoretical investigation of this multi-phase process, showing how successful learning results from the interaction between
    
[^224]: 分析和预测2型糖尿病老年患者的多类分类死亡率

    Analysis and Mortality Prediction using Multiclass Classification for Older Adults with Type 2 Diabetes

    [https://arxiv.org/abs/2402.10999](https://arxiv.org/abs/2402.10999)

    设计治疗方案需注意患者剩余生命和合并症，研究利用大规模数据集构建多类分类模型来预测老年2型糖尿病患者的死亡率。

    

    设计合适的治疗方案来管理糖尿病要求医护人员注意患者剩余的生命以及影响他们的合并症。本研究利用了一个包含68个潜在死亡预测因子的结构化数据集，针对275,190名年龄在65岁或以上的美国退伍军人进行了研究。通过将两个原始目标变量组合起来创造了一个新的目标变量。通过离散化连续变量来处理异常值，对分类变量进行了虚拟编码。通过随机欠采样实现了类平衡。使用带LASSO的多项式逻辑回归建立了基准回归模型。采用卡方检验和信息增益作为基于过滤的特征选择技术。分类器包括多项式逻辑回归、随机森林、极端梯度提升（XGB）。

    arXiv:2402.10999v1 Announce Type: cross  Abstract: Designing proper treatment plans to manage diabetes requires health practitioners to pay heed to the individuals remaining life along with the comorbidities affecting them. Older adults with Type 2 Diabetes Mellitus (T2DM) are prone to experience premature death or even hypoglycaemia. The structured dataset utilized has 68 potential mortality predictors for 275,190 diabetic U.S. military Veterans aged 65 years or older. A new target variable is invented by combining the two original target variables. Outliers are handled by discretizing the continuous variables. Categorical variables have been dummy encoded. Class balancing is achieved by random under-sampling. A benchmark regression model is built using Multinomial Logistic Regression with LASSO. Chi-Squared and Information Gain are the filter-based feature selection techniques utilized. Classifiers such as Multinomial Logistic Regression, Random Forest, Extreme Gradient Boosting (XGB
    
[^225]: 通过差分动态逻辑确保神经网络控制器的安全性

    Provably Safe Neural Network Controllers via Differential Dynamic Logic

    [https://arxiv.org/abs/2402.10998](https://arxiv.org/abs/2402.10998)

    通过差分动态逻辑与神经网络验证相结合的VerSAILLE方法，实现了对神经网络控制系统在无限时间范围上的安全性证明。

    

    虽然神经网络（NN）作为面向目标的控制器在网络物理系统中具有巨大潜力，但验证基于神经网络的控制系统（NNCS）的安全性对于实际应用NN来说面临着重大挑战，特别是当需要对无界时间范围进行安全性验证时。我们引入了VerSAILLE（通过逻辑链接包验证的可验证安全人工智能）：这是差分动态逻辑（dL）和NN验证组合的第一种方法。通过合作，我们可以利用NN验证工具的效率，同时保留dL的严谨性。我们提出了一个控制器信封的安全性证明，以证明无限时间范围上具体NNCS的安全性。VerSAILLE导致的NN验证属性通常需要非线性算术，而高效的NN验证工具仅支持线性算术。

    arXiv:2402.10998v1 Announce Type: cross  Abstract: While neural networks (NNs) have a large potential as goal-oriented controllers for Cyber-Physical Systems, verifying the safety of neural network based control systems (NNCSs) poses significant challenges for the practical use of NNs -- especially when safety is needed for unbounded time horizons. One reason for this is the intractability of NN and hybrid system analysis. We introduce VerSAILLE (Verifiably Safe AI via Logically Linked Envelopes): The first approach for the combination of differential dynamic logic (dL) and NN verification. By joining forces, we can exploit the efficiency of NN verification tools while retaining the rigor of dL. We reflect a safety proof for a controller envelope in an NN to prove the safety of concrete NNCS on an infinite-time horizon. The NN verification properties resulting from VerSAILLE typically require nonlinear arithmetic while efficient NN verification tools merely support linear arithmetic. T
    
[^226]: 加速半异步联邦学习

    Accelerating Semi-Asynchronous Federated Learning

    [https://arxiv.org/abs/2402.10991](https://arxiv.org/abs/2402.10991)

    提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。

    

    联邦学习（FL）是一种分布式机器学习范例，允许客户端在保护隐私的同时在其数据上训练模型。现有的FL算法，如Federated Averaging（FedAvg）及其变种，在许多情况下已经被证明收敛良好。然而，这些方法需要客户端以同步方式将其本地更新上传至服务器，这在现实情况下可能会变得缓慢和不可靠。为了解决这个问题，研究人员开发了异步FL方法，允许客户端继续使用陈旧的全局模型对其本地数据进行训练。然而，大多数这些方法仅仅聚合了所有接收到的更新，而没有考虑其相对贡献，这可能导致收敛速度变慢。在本文中，我们提出了一种考虑贡献的异步FL方法，考虑了接收到的更新的陈旧程度和统计异质性。我们的方法动态调整

    arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
    
[^227]: 量子启发式分析神经网络漏洞：协变量在系统攻击中的作用

    Quantum-Inspired Analysis of Neural Network Vulnerabilities: The Role of Conjugate Variables in System Attacks

    [https://arxiv.org/abs/2402.10983](https://arxiv.org/abs/2402.10983)

    神经网络漏洞的量子启发式分析揭示了输入共轭在系统攻击中的作用，揭示了网络结构中的系统脆弱性，并展示了与量子物理不确定性原理之间的数学一致性，突显了神经网络系统的固有脆弱性和潜在的跨学科领域进展。

    

    神经网络对于微小的、非随机的扰动展示出固有的脆弱性，这些漏洞被称为对抗性攻击。这些攻击源自损失函数相对输入的梯度，被识别为输入共轭，揭示了网络结构内部的系统脆弱性。有趣的是，这种机制与量子物理的不确定性原理之间呈现出数学上的一致性，为此种跨学科性质投下了一道前所未有的光。神经网络系统内在的敏感性通常是固有的，不仅突显了这些网络的固有脆弱性，更暗示着在理解这些黑匣网络方面的跨学科领域的潜在进展。

    arXiv:2402.10983v1 Announce Type: new  Abstract: Neural networks demonstrate inherent vulnerability to small, non-random perturbations, emerging as adversarial attacks. Such attacks, born from the gradient of the loss function relative to the input, are discerned as input conjugates, revealing a systemic fragility within the network structure. Intriguingly, a mathematical congruence manifests between this mechanism and the quantum physics' uncertainty principle, casting light on a hitherto unanticipated interdisciplinarity. This inherent susceptibility within neural network systems is generally intrinsic, highlighting not only the innate vulnerability of these networks but also suggesting potential advancements in the interdisciplinary area for understanding these black-box networks.
    
[^228]: mshw，一种基于多季节Holt-Winters的预测库，用于预测短期电力需求

    mshw, a forecasting library to predict short-term electricity demand based on multiple seasonal Holt-Winters

    [https://arxiv.org/abs/2402.10982](https://arxiv.org/abs/2402.10982)

    这是一个用于预测短期电力需求的MATLAB工具箱，可以提供更准确的预测。

    

    输电系统运营商越来越需要更准确的电力需求预测。目前的电力系统在很大程度上需要需求预测，以便电力市场制定电力价格以及生产单元的编程。本文介绍了一个用于电力需求预测的MATLAB工具箱，实现了多

    arXiv:2402.10982v1 Announce Type: new  Abstract: Transmission system operators have a growing need for more accurate forecasting of electricity demand. Current electricity systems largely require demand forecasting so that the electricity market establishes electricity prices as well as the programming of production units. The companies that are part of the electrical system use exclusive software to obtain predictions, based on the use of time series and prediction tools, whether statistical or artificial intelligence. However, the most common form of prediction is based on hybrid models that use both technologies. In any case, it is software with a complicated structure, with a large number of associated variables and that requires a high computational load to make predictions. The predictions they can offer are not much better than those that simple models can offer. In this paper we present a MATLAB toolbox created for the prediction of electrical demand. The toolbox implements mul
    
[^229]: ReRAM神经形态电路阵列中的卡住故障及其通过机器学习的纠正

    Stuck-at Faults in ReRAM Neuromorphic Circuit Array and their Correction through Machine Learning

    [https://arxiv.org/abs/2402.10981](https://arxiv.org/abs/2402.10981)

    本文研究了ReRAM神经形态电路中卡住故障对推理准确性的影响，并发现卡住和卡住缺陷对推理准确性有类似的影响，但如果在列之间存在空间缺陷变化，则推理准确性会受到影响。

    

    本文研究了由于卡住故障（卡住、卡住和卡住在某一电阻值）导致的可编程阻性随机存取内存（ReRAM）神经形态电路的推理准确性。 使用Python执行了一个模拟框架，进行监督式机器学习（具有3个隐藏层、1个输入层和1个输出层的神经网络）来识别手写数字，并构建相应的完全模拟神经形态电路（由Spectre模拟的4个突触阵列）。 使用了一个通用的45nm工艺开发套件（PDK）。 研究了卡住和卡住缺陷导致的推理准确性降级的差异。 研究了各种缺陷模式，包括圆形、环形、行、列和圆形互补缺陷。 发现卡住和卡住缺陷对推理准确性具有类似的影响。 但还发现，如果在列之间存在空间缺陷变化，则推理准确性将受到影响。

    arXiv:2402.10981v1 Announce Type: cross  Abstract: In this paper, we study the inference accuracy of the Resistive Random Access Memory (ReRAM) neuromorphic circuit due to stuck-at faults (stuck-on, stuck-off, and stuck at a certain resistive value). A simulation framework using Python is used to perform supervised machine learning (neural network with 3 hidden layers, 1 input layer, and 1 output layer) of handwritten digits and construct a corresponding fully analog neuromorphic circuit (4 synaptic arrays) simulated by Spectre. A generic 45nm Process Development Kit (PDK) was used. We study the difference in the inference accuracy degradation due to stuck-on and stuck-off defects. Various defect patterns are studied including circular, ring, row, column, and circular-complement defects. It is found that stuck-on and stuck-off defects have a similar effect on inference accuracy. However, it is also found that if there is a spatial defect variation across the columns, the inference accu
    
[^230]: CHEMREASONER：使用量子化学反馈在大型语言模型的知识空间中进行启发式搜索

    CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback

    [https://arxiv.org/abs/2402.10980](https://arxiv.org/abs/2402.10980)

    通过将大型语言模型推理与量子化学反馈相结合，我们引入了一个AI引导的计算筛选框架，将催化剂发现形式化为一个不确定环境，从而实现高效催化剂的积极搜索

    

    arXiv:2402.10980v1 类型公告：跨领域 摘要：发现新的催化剂对于设计新的更高效的化学过程至关重要，以实现向可持续未来的过渡。我们引入了一种人工智能引导的计算筛选框架，将语言推理与基于量子化学的三维原子表示的反馈统一起来。我们的方法将催化剂发现构建为一个不确定环境，其中一个代理通过大型语言模型（LLM）推导的假设与基于原子图神经网络（GNN）的反馈的迭代组合，积极搜索高效催化剂。在中间搜索步骤确定的催化剂经过基于空间定向、反应途径和稳定性的结构评估。基于吸附能和势垒的评分函数引导在LLM的知识空间中向能量有利、高效的催化剂探索。我们引入了可以自动规划的方法

    arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
    
[^231]: 具有符合事实性保证的语言模型

    Language Models with Conformal Factuality Guarantees

    [https://arxiv.org/abs/2402.10978](https://arxiv.org/abs/2402.10978)

    提出了一种能够通过连接语言建模和符合预测为语言模型提供高概率正确性保证的框架。

    

    语言模型（LM）输出的正确性和事实性保证是一个重要的开放问题。在这项工作中，我们提出了符合事实性，这是一个框架，可以通过连接语言建模和符合预测，确保LM的高概率正确性保证。我们观察到，LM输出的正确性等价于一个不确定性量化问题，其中不确定性集被定义为LM输出的蕴含集。利用这种联系，我们表明，语言模型中的符合预测对应于一种后退算法，通过逐渐使LM输出变得不太具体（并扩大相关的不确定性集）提供高概率的正确性保证。这种方法适用于任何黑盒LM，并且需要很少的人工注释样本。我们在封闭书籍QA（FActScore，NaturalQuestions）和推理任务（MATH）上对我们的方法进行评估，结果表明我们的方法可以p

    arXiv:2402.10978v1 Announce Type: cross  Abstract: Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can p
    
[^232]: 生成人工智能与过程系统工程：下一个前沿

    Generative AI and Process Systems Engineering: The Next Frontier

    [https://arxiv.org/abs/2402.10977](https://arxiv.org/abs/2402.10977)

    新兴生成人工智能模型（如基础模型）在过程系统工程中的应用，提供了多功能的适应性，对合成与设计、优化与集成以及过程监控与控制等关键领域具有重要影响。

    

    本文探讨了新兴生成人工智能（GenAI）模型，如大型语言模型（LLMs），如何增强过程系统工程（PSE）中的解决方法。这些最前沿的GenAI模型，特别是基础模型（FMs），它们在广泛的通用数据集上预训练，为涉及查询响应、图像生成和复杂决策等广泛任务提供了多功能的适应性。鉴于PSE的进展与计算和系统技术的发展之间密切关系，探索GenAI和PSE之间的协同作用是至关重要的。我们从经典和新兴的GenAI模型，包括FMs的简要概述开始讨论，然后深入探讨它们在关键PSE领域内的应用：合成与设计、优化与集成，以及过程监控与控制。在每个领域中，我们探讨了GenAI模型如何可以促进

    arXiv:2402.10977v1 Announce Type: new  Abstract: This article explores how emerging generative artificial intelligence (GenAI) models, such as large language models (LLMs), can enhance solution methodologies within process systems engineering (PSE). These cutting-edge GenAI models, particularly foundation models (FMs), which are pre-trained on extensive, general-purpose datasets, offer versatile adaptability for a broad range of tasks, including responding to queries, image generation, and complex decision-making. Given the close relationship between advancements in PSE and developments in computing and systems technologies, exploring the synergy between GenAI and PSE is essential. We begin our discussion with a compact overview of both classic and emerging GenAI models, including FMs, and then dive into their applications within key PSE domains: synthesis and design, optimization and integration, and process monitoring and control. In each domain, we explore how GenAI models could pot
    
[^233]: 论机器学习在网络入侵检测中的跨数据集泛化能力

    On the Cross-Dataset Generalization of Machine Learning for Network Intrusion Detection

    [https://arxiv.org/abs/2402.10974](https://arxiv.org/abs/2402.10974)

    该研究通过在跨数据集框架中进行实验，对基于机器学习的网络入侵检测系统的泛化能力进行了综合分析，并提出了一个创新的数据集对其进行验证。

    

    网络入侵检测系统(NIDS)是网络安全中的基本工具。它们在不同网络中的泛化能力是其有效性的关键因素，也是实际应用的先决条件。在本研究中，我们通过在跨数据集框架中进行全面实验，对基于机器学习的NIDS的泛化能力进行了综合分析。我们采用了四种机器学习分类器，并利用了来自不同网络的四个数据集：CIC-IDS-2017、CSE-CIC-IDS2018、LycoS-IDS2017和LycoS-Unicas-IDS2018。值得注意的是，最后一个数据集是一个创新贡献，我们在这个数据集上基于LycoS-IDS2017的修正对知名的CSE-CIC-IDS2018数据集进行了应用。结果显示，当模型在同一数据集上进行训练和测试时，几乎完美的分类性能。然而，当以跨数据集的方式训练和测试模型时，分类准确度会大幅程度地降低。

    arXiv:2402.10974v1 Announce Type: cross  Abstract: Network Intrusion Detection Systems (NIDS) are a fundamental tool in cybersecurity. Their ability to generalize across diverse networks is a critical factor in their effectiveness and a prerequisite for real-world applications. In this study, we conduct a comprehensive analysis on the generalization of machine-learning-based NIDS through an extensive experimentation in a cross-dataset framework. We employ four machine learning classifiers and utilize four datasets acquired from different networks: CIC-IDS-2017, CSE-CIC-IDS2018, LycoS-IDS2017, and LycoS-Unicas-IDS2018. Notably, the last dataset is a novel contribution, where we apply corrections based on LycoS-IDS2017 to the well-known CSE-CIC-IDS2018 dataset. The results show nearly perfect classification performance when the models are trained and tested on the same dataset. However, when training and testing the models in a cross-dataset fashion, the classification accuracy is largel
    
[^234]: 慢性疾病症状事件准确及及时预测的建模方法论

    Modeling methodology for the accurate and prompt prediction of symptomatic events in chronic diseases

    [https://arxiv.org/abs/2402.10972](https://arxiv.org/abs/2402.10972)

    本研究提出了针对偏头痛等慢性疾病的预测模型建设方法，探讨了最大预测时间跨度及其对选定特征的依赖性。

    

    预测慢性疾病中的症状危机可以在症状发生之前做出决策，比如服用药物以避免症状或激活医疗警报。本文针对一种慢性病—偏头痛进行预测极限研究。为此，本研究开发了一种构建偏头痛预测模型并改善这些预测超出原始模型限制的方法论。分析了最大预测时间跨度，并研究了其对所选特征的依赖性。提出了一种模型选择策略，以解决对保守但稳健的预测模型和对精度较低但更大时间跨度的预测之间的权衡。得到的结果显示了一种预测

    arXiv:2402.10972v1 Announce Type: cross  Abstract: Prediction of symptomatic crises in chronic diseases allows to take decisions before the symptoms occur, such as the intake of drugs to avoid the symptoms or the activation of medical alarms. The prediction horizon is in this case an important parameter in order to fulfill the pharmacokinetics of medications, or the time response of medical services. This paper presents a study about the prediction limits of a chronic disease with symptomatic crises: the migraine. For that purpose, this work develops a methodology to build predictive migraine models and to improve these predictions beyond the limits of the initial models. The maximum prediction horizon is analyzed, and its dependency on the selected features is studied. A strategy for model selection is proposed to tackle the trade off between conservative but robust predictive models, with respect to less accurate predictions with higher horizons. The obtained results show a predictio
    
[^235]: 医疗AI中的泛化性能：临床大型语言模型的评估

    Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model

    [https://arxiv.org/abs/2402.10965](https://arxiv.org/abs/2402.10965)

    大型语言模型在医疗健康领域有着重要作用，然而它们的泛化效果取决于在不同临床环境和人群中的表现，对于泛化能力不足的原因进行了分析，发现在样本较少的医院和特定人群中存在挑战。

    

    大型语言模型（LLMs）的进展为医疗健康领域提供了新机遇，可以改善患者护理、临床决策以及提升医师和管理人员的工作流程。然而，这些模型的潜力重要取决于它们在临床环境和人群中有效泛化的能力，这是在早期开发中经常被低估的挑战。为了更好地理解这些挑战的原因并制定缓解方法，我们评估了ClinicLLM，这是一个在 [HOSPITAL] 的临床笔记上训练的LLM模型，对其在30天全因素再入院预测中的表现进行分析，关注跨医院和患者特征的变异性。我们发现在样本较少的医院、政府和未指定保险的患者、老年人以及高共病性患者中，泛化效果较差。为了了解泛化不彰的原因，我们调查了样本量

    arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
    
[^236]: 基于神经网络的机器学习中的最佳特征重缩放

    Optimal feature rescaling in machine learning based on neural networks

    [https://arxiv.org/abs/2402.10964](https://arxiv.org/abs/2402.10964)

    提出了一种通过遗传算法进行输入特征的最佳重缩放来改善神经网络训练效率和泛化性能的方法。

    

    这篇论文提出了一种新的方法，通过遗传算法（GA）进行输入特征的最佳重缩放（OFR）来改善前馈神经网络（FFNNs）的训练效率和泛化性能。OFR重新塑造了输入空间，改善了用于训练的基于梯度的算法的条件。此外，GA进行的比例因子探索和选择对应于每次训练尝试中第一层权重的不同初始化，从而实现了一个多起点全局搜索算法（尽管仅限于少量权重），从而促进了全局最小值的实现。该方法已在模拟实际工业过程（无心磨削）结果的FFNN上进行了测试。

    arXiv:2402.10964v1 Announce Type: new  Abstract: This paper proposes a novel approach to improve the training efficiency and the generalization performance of Feed Forward Neural Networks (FFNNs) resorting to an optimal rescaling of input features (OFR) carried out by a Genetic Algorithm (GA). The OFR reshapes the input space improving the conditioning of the gradient-based algorithm used for the training. Moreover, the scale factors exploration entailed by GA trials and selection corresponds to different initialization of the first layer weights at each training attempt, thus realizing a multi-start global search algorithm (even though restrained to few weights only) which fosters the achievement of a global minimum. The approach has been tested on a FFNN modeling the outcome of a real industrial process (centerless grinding).
    
[^237]: GLoRe: 何时、何地以及如何通过全局和局部的改进来提高LLM推理能力

    GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements

    [https://arxiv.org/abs/2402.10963](https://arxiv.org/abs/2402.10963)

    提出了Stepwise ORMs (SORMs)，它们在合成数据上训练，以近似预测最优策略的未来预期奖励

    

    最先进的语言模型在数学、科学或编码任务中展现出令人印象深刻的推理改进能力。然而，最近的研究表明，即使最好的模型也很难在没有外部反馈的情况下确定何时何地进行改进。基于结果的奖励模型(ORMs)，被训练来预测最终答案的正确性，指示何时进行改进，为决定何时进行改进提供了一种便利的解决方案。基于过程的奖励模型(PRMs)受过训练，用以预测中间步骤的正确性，然后可以用来指示何处进行改进。但它们很昂贵，需要大量的人工注释。在本文中，我们提出了逐步ORMs(SORMs)，它们只在合成数据上受过训练，以近似预测最优策略或$V^{\star}$的未来预期奖励。更具体地说，SORMs受训练来预测当取样时最终答案的正确性

    arXiv:2402.10963v1 Announce Type: new  Abstract: State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when samplin
    
[^238]: 在语言模型对话中测量和控制“人设”漂移

    Measuring and Controlling Persona Drift in Language Model Dialogs

    [https://arxiv.org/abs/2402.10962](https://arxiv.org/abs/2402.10962)

    提出了一种量化基准来测量语言模型对话中的“人设”漂移，并提出了一种称为split-softmax的轻量级方法来对抗注意力衰减和“人设”漂移

    

    提示是定制语言模型聊天机器人的标准工具，使其能够承担特定的“人设”。在使用提示时的一个隐含假设是，它们将是稳定的，因此聊天机器人将在整个对话过程中继续根据规定的“人设”生成文本。我们提出了一个量化基准来测试这一假设，通过两个个性化聊天机器人之间的自我对话来评估“人设”的稳定性。我们对流行模型如LLaMA2-chat-70B进行测试，发现在八轮对话中存在显著的“人设”漂移。对这一现象的实证和理论分析表明，由于长对话中的注意力衰减，变压器注意力机制起到了一定作用。为了对抗注意力衰减和“人设”漂移，我们提出了一种称为split-softmax的轻量级方法，与两个强基线方法相比表现优异。

    arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
    
[^239]: 相对优先权优化: 通过对相同和不同提示的对比响应增强LLM对齐

    Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts

    [https://arxiv.org/abs/2402.10958](https://arxiv.org/abs/2402.10958)

    提出了相对优先权优化（RPO）方法，通过区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应，扩展了模型的学习能力。

    

    在大型语言模型（LLM）领域，将模型与用户的多样化偏好相一致是一个关键挑战。直接优先权优化（DPO）在这一领域起到了关键作用。DPO通过使用从相同提示中派生的偏好对来工作，而无需额外的奖励模型。然而，DPO并不能完全反映人类学习的复杂性，这种学习往往涉及对不仅相同而且相似问题的对比响应的理解。为了克服这一不足，我们提出了相对优先权优化（RPO）。RPO旨在区分来自相同和相关提示的更受青睐的响应和更不受青睐的响应。它引入了对比加权机制，使LLMs能够使用更广泛的偏好数据进行调整，包括成对和不成对的数据集。这种方法扩展了模型的学习能力，使其能够利用更多的偏好数据进行优化。

    arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
    
[^240]: 类似于睡眠的无监督重播在数据有限或不平衡时改善性能

    Sleep-Like Unsupervised Replay Improves Performance when Data are Limited or Unbalanced

    [https://arxiv.org/abs/2402.10956](https://arxiv.org/abs/2402.10956)

    研究发现，通过类似于睡眠的无监督重播阶段，可以显著提高在有限数据情况下训练的人工神经网络的准确性，这为解决数据有限或不平衡时的性能问题提供了新方法。

    

    人工神经网络（ANNs）在训练数据有限或不平衡时性能下降。相反，人脑可以快速从少量示例中学习。本研究探讨了在MNIST和Fashion MNIST数据集上用有限数据训练的ANNs性能改善作用。睡眠被实现为具有本地Hebbian类型学习规则的无监督阶段。我们发现，在使用MNIST或Fashion MNIST数据集总量的0.5-10%有限数据训练模型后，经过睡眠阶段后准确性显著提高。当使用超过总数据量的10%时，仅睡眠对性能有轻微负面影响，但这可以通过对原始数据进行微调来纠正。该研究揭示了大脑在睡眠时利用的潜在突触权重动态策略，以增强在训练数据有限或不平衡情况下的记忆性能。

    arXiv:2402.10956v1 Announce Type: cross  Abstract: The performance of artificial neural networks (ANNs) degrades when training data are limited or imbalanced. In contrast, the human brain can learn quickly from just a few examples. Here, we investigated the role of sleep in improving the performance of ANNs trained with limited data on the MNIST and Fashion MNIST datasets. Sleep was implemented as an unsupervised phase with local Hebbian type learning rules. We found a significant boost in accuracy after the sleep phase for models trained with limited data in the range of 0.5-10% of total MNIST or Fashion MNIST datasets. When more than 10% of the total data was used, sleep alone had a slight negative impact on performance, but this was remedied by fine-tuning on the original data. This study sheds light on a potential synaptic weight dynamics strategy employed by the brain during sleep to enhance memory performance when training data are limited or imbalanced.
    
[^241]: DAEDRA：用于预测被动药物警戒报告结果的语言模型

    DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting

    [https://arxiv.org/abs/2402.10951](https://arxiv.org/abs/2402.10951)

    DAEDRA是一种旨在在被动报告中检测监管相关结果的大型语言模型，弥补了通用模型无法捕捉临床维度与专业模型在非专业报告上表现不佳的缺陷

    

    近年来，大型语言模型（LLMs）的出现导致了特定领域模型的激增，这些模型旨在反映来源领域的语言环境和内容的特殊性。本文详细介绍了DAEDRA的构思、设计、训练和评估，这是一个旨在检测被动报告（PR）中的监管相关结果（死亡、急诊就诊和住院）的LLM。虽然PR是一种高效的从广泛和多样化受众（通常不仅包括医生和医护人员，还包括患者、家庭成员和其他非专业利益相关者）那里获取信息的方式，但是这种多样性使PR语料库难以分析。通用语言模型可能无法捕捉复杂的临床维度，而特定的临床或生物医学模型可能在非专业报告上表现不佳。

    arXiv:2402.10951v1 Announce Type: new  Abstract: Over the recent years, the emergence of large language models (LLMs) has given rise to a proliferation of domain-specific models that are intended to reflect the particularities of linguistic context and content as a correlate of the originating domain. This paper details the conception, design, training and evaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes (mortality, ER attendance and hospitalisation) in adverse event reports elicited through passive reporting (PR). While PR is a highly cost-efficient way of eliciting information from a wide and diverse audience -- typically including not only physicians and healthcare providers but also patients, family members and other lay stakeholders --, this diversity makes PR corpora difficult to analyse. Generic language models may not capture the complex clinical dimensions while specific clinical or biomedical models may not perform well on lay reports. To evaluate t
    
[^242]: 异类自动提示的不合理有效性

    The Unreasonable Effectiveness of Eccentric Automatic Prompts

    [https://arxiv.org/abs/2402.10949](https://arxiv.org/abs/2402.10949)

    异类自动提示的不合理有效性研究了大型语言模型在处理各种提示时的表现，结果显示在大多数情况下，包括“积极思考”提示会对模型性能产生积极影响。

    

    大型语言模型（LLMs）展示了出色的问题解决和基本数学能力。然而，它们的功效高度依赖于提示的制定。本研究旨在量化将“积极思考”纳入系统提示消息的影响，然后将其与系统化提示优化进行比较。我们评估了60种系统消息片段的性能，分别使用和不使用Chain of Thought提示，跨三个参数范围从70亿到70亿个变量的模型，在GSM8K数据集上进行测试。我们的发现表明，结果并不在所有模型中普遍适用。在大多数情况下，包括“积极思考”提示会积极影响模型性能。然而，值得注意的是，Llama2-70B在不使用Chain of Thought时是个例外，因为发现最佳系统消息实际上是没有消息。考虑到组合复杂性，以及其导至的加# Truncated due to exceeding character limit.

    arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
    
[^243]: 将文化差异纳入大型语言模型的研究

    CultureLLM: Incorporating Cultural Differences into Large Language Models

    [https://arxiv.org/abs/2402.10946](https://arxiv.org/abs/2402.10946)

    提出了一种名为CultureLLM的成本效益高的解决方案，通过使用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强来将文化差异纳入大型语言模型中，成功微调得到了涵盖富裕和低资源语言的9种文化特定LLMs以及一个统一模型（CultureLLM-One）。

    

    大型语言模型（LLMs）被报道偏向于某些文化，因为训练数据主要来自英语语料库。由于多语种文化数据通常较难收集，现有的工作通过提示工程或特定文化的预训练来处理这一问题。然而，它们可能忽视了低资源文化的知识缺乏，并需要大量的计算资源。本文提出了CultureLLM，这是一个成本效益高的解决方案，可将文化差异纳入LLMs中。CultureLLM采用世界价值调查（WVS）作为种子数据，并通过提出的语义数据增强生成语义等效的训练数据。仅使用来自WVS的50个种子样本和增强数据，我们对9种包括富裕和低资源语言的文化特定LLMs和一个统一模型（CultureLLM-One）进行了微调。对60个与文化相关的数据集进行的大量实验表明，CultureLLM在增强LLM的文化特性方面取得了显著的成果。

    arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
    
[^244]: Text2Data：使用文本控制的低资源数据生成

    Text2Data: Low-Resource Data Generation with Textual Control

    [https://arxiv.org/abs/2402.10941](https://arxiv.org/abs/2402.10941)

    Text2Data提出了一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法，以解决低资源环境下缺乏文本标签的文本到数据任务中的挑战。

    

    自然语言作为人类与机器无缝交互的一种常见直接控制信号。意识到这一接口的重要性，机器学习社区正在投入大量精力生成与文本指令在语义上一致的数据。虽然在涵盖图像编辑、音频合成、视频生成等领域取得了进展，但低资源领域由于昂贵注释或复杂数据结构（如分子、运动动态和时序）等特点，往往缺乏文本标签。这种不足阻碍了监督学习，从而限制了将先进生成模型应用于文本到数据任务的可能性。为了应对低资源场景中的这些挑战，我们提出了Text2Data，这是一种利用未标记数据通过无监督扩散模型来理解基础数据分布的新方法。

    arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
    
[^245]: 临床程序代码的神经机器翻译用于医学诊断和不确定性量化

    Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification

    [https://arxiv.org/abs/2402.10940](https://arxiv.org/abs/2402.10940)

    研究引入了医学熵的概念，通过神经机器翻译基于ICD-9代码的患者预测结果，量化了不确定性。

    

    临床决策支持系统（CDSS）旨在通过将系统生成的建议与医学专业知识结合来增强临床医生的决策能力。本研究引入了医学熵的概念，通过基于手术ICD-9代码的神经机器翻译来量化患者预测结果中的不确定性。我们的实验结果不仅展示了程序代码与实际医疗结果之间的强相关性，

    arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
    
[^246]: 一种轻量级Inception增强U-Net神经网络用于布线可预测性

    A Lightweight Inception Boosted U-Net Neural Network for Routability Prediction

    [https://arxiv.org/abs/2402.10937](https://arxiv.org/abs/2402.10937)

    提出了一种轻量级Inception增强的U-Net神经网络模型，用于预测布线拥塞和设计规则检查热点，在实验中取得了显著的性能改进。

    

    随着现代CPU、GPU和NPU芯片设计复杂性和晶体管数量的不断增加，以及半导体技术节点不断缩小至近1纳米，布局和布线逐渐成为现代超大规模集成（VLSI）电路后端设计中最关键的两个过程。如何有效准确地在先期评估可路由性（在布局和全局布线阶段）已成为人工智能辅助电子设计自动化（EDA）领域的关键研究领域。本文提出了一种新颖的U-Net变体模型，通过嵌入Inception模块来预测路由拥塞（RC）和设计规则检查（DRC）热点。最近发布的CircuitNet数据集基准上的实验结果表明，我们提出的方法在Avg-NRMSE（平均归一化根均方误差）方面实现了高达5%（RC）和20%（DRC）的降低率。

    arXiv:2402.10937v1 Announce Type: cross  Abstract: As the modern CPU, GPU, and NPU chip design complexity and transistor counts keep increasing, and with the relentless shrinking of semiconductor technology nodes to nearly 1 nanometer, the placement and routing have gradually become the two most pivotal processes in modern very-large-scale-integrated (VLSI) circuit back-end design. How to evaluate routability efficiently and accurately in advance (at the placement and global routing stages) has grown into a crucial research area in the field of artificial intelligence (AI) assisted electronic design automation (EDA). In this paper, we propose a novel U-Net variant model boosted by an Inception embedded module to predict Routing Congestion (RC) and Design Rule Checking (DRC) hotspots. Experimental results on the recently published CircuitNet dataset benchmark show that our proposed method achieves up to 5% (RC) and 20% (DRC) rate reduction in terms of Avg-NRMSE (Average Normalized Root 
    
[^247]: ConSmax: 具有可学习参数的硬件友好型Softmax替代方案

    ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters

    [https://arxiv.org/abs/2402.10930](https://arxiv.org/abs/2402.10930)

    ConSmax是一种硬件友好型Softmax替代方案，通过引入可学习参数，在不影响性能的情况下实现了对原Softmax关键任务的高效处理。

    

    自注意机制将基于transformer的大型语言模型（LLM）与卷积和循环神经网络区分开来。尽管性能有所提升，但由于自注意中广泛使用Softmax，在硅上实现实时LLM推断仍具挑战性。为了解决这一挑战，我们提出了Constant Softmax（ConSmax），这是一种高效的Softmax替代方案，采用可微的规范化参数来消除Softmax中的最大搜索和分母求和，实现了大规模并行化。

    arXiv:2402.10930v1 Announce Type: cross  Abstract: The self-attention mechanism sets transformer-based large language model (LLM) apart from the convolutional and recurrent neural networks. Despite the performance improvement, achieving real-time LLM inference on silicon is challenging due to the extensively used Softmax in self-attention. Apart from the non-linearity, the low arithmetic intensity greatly reduces the processing parallelism, which becomes the bottleneck especially when dealing with a longer context. To address this challenge, we propose Constant Softmax (ConSmax), a software-hardware co-design as an efficient Softmax alternative. ConSmax employs differentiable normalization parameters to remove the maximum searching and denominator summation in Softmax. It allows for massive parallelization while performing the critical tasks of Softmax. In addition, a scalable ConSmax hardware utilizing a bitwidth-split look-up table (LUT) can produce lossless non-linear operation and 
    
[^248]: 物理信息神经网络及相关模型在物理信息机器学习中的数值分析

    Numerical analysis of physics-informed neural networks and related models in physics-informed machine learning

    [https://arxiv.org/abs/2402.10926](https://arxiv.org/abs/2402.10926)

    这项研究全面回顾了物理信息神经网络及相关模型在物理信息机器学习中的数值分析结果，提供了对PINNs近似PDE过程中误差成分的统一框架分析。

    

    物理信息神经网络（PINNs）及其变种近年来在偏微分方程正演和反问题的数值模拟中备受青睐。本文旨在全面回顾目前关于PINNs及其相关模型的数值分析结果，这些模型构成了物理信息机器学习的基础。我们提供了一个统一的框架，可以有效地分析PINNs在逼近PDE时产生的误差的各个组成部分。重点回顾了关于逼近误差、泛化误差和训练误差的可用结果，以及它们在对应PDE类型和底层域的维度方面的行为。特别地，阐明了解决方案的正则性及其对误差分析中的扰动的稳定性。数值结果也得到了展示。

    arXiv:2402.10926v1 Announce Type: cross  Abstract: Physics-informed neural networks (PINNs) and their variants have been very popular in recent years as algorithms for the numerical simulation of both forward and inverse problems for partial differential equations. This article aims to provide a comprehensive review of currently available results on the numerical analysis of PINNs and related models that constitute the backbone of physics-informed machine learning. We provide a unified framework in which analysis of the various components of the error incurred by PINNs in approximating PDEs can be effectively carried out. A detailed review of available results on approximation, generalization and training errors and their behavior with respect to the type of the PDE and the dimension of the underlying domain is presented. In particular, the role of the regularity of the solutions and their stability to perturbations in the error analysis is elucidated. Numerical results are also presen
    
[^249]: AM^2-EmoJE：通过联合嵌入学习实现对话中的自适应缺失模态情绪识别

    AM^2-EmoJE: Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning

    [https://arxiv.org/abs/2402.10921](https://arxiv.org/abs/2402.10921)

    通过自适应缺失模态情绪识别, 该模型包括查询自适应融合和多模态联合嵌入学习两大特点，旨在提高情绪识别的准确性和鲁棒性。

    

    人类情绪可以通过不同模式表达，例如音频、视频和文本。然而，每种模式在展示情绪时的贡献并不均匀。此外，在测试时，不一定总是能够保证完整的模式特定细节可用。在这项工作中，我们提出了一种名为AM^2-EmoJE的模型，通过联合嵌入学习模型，在对话中实现自适应缺失模态情绪识别，该模型基于两方面的贡献：首先，查询自适应融合可以自动学习其模式特定表示在查询特定方式下的相对重要性。通过这种方式，模型旨在优先考虑情绪模式的模式不变空间查询细节，同时在学习的多模式查询描述符中保留其独占模式方面。其次，多模态联合嵌入学习模块明确解决了测试时的各种缺失模态场景。

    arXiv:2402.10921v1 Announce Type: new  Abstract: Human emotion can be presented in different modes i.e., audio, video, and text. However, the contribution of each mode in exhibiting each emotion is not uniform. Furthermore, the availability of complete mode-specific details may not always be guaranteed in the test time. In this work, we propose AM^2-EmoJE, a model for Adaptive Missing-Modality Emotion Recognition in Conversation via Joint Embedding Learning model that is grounded on two-fold contributions: First, a query adaptive fusion that can automatically learn the relative importance of its mode-specific representations in a query-specific manner. By this the model aims to prioritize the mode-invariant spatial query details of the emotion patterns, while also retaining its mode-exclusive aspects within the learned multimodal query descriptor. Second the multimodal joint embedding learning module that explicitly addresses various missing modality scenarios in test-time. By this, th
    
[^250]: LLM辅助危机管理：构建用于有效应急响应和公众协作的先进LLM平台

    LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration

    [https://arxiv.org/abs/2402.10908](https://arxiv.org/abs/2402.10908)

    通过LLAMA2语言模型，建立了一个能够从社交媒体和紧急消息中识别和分类紧急情况的方法，可协助在全国范围内的紧急情况下公共安全话务员和大众，提供相关指导并通知政府机构。

    

    紧急情况和重大事件往往迅速发展，需要迅速有效的响应。本研究介绍了一种新方法，利用开源大型语言模型LLAMA2，从社交媒体帖子和直接的紧急消息中识别和分类紧急情况。旨在利用自然语言处理和机器学习的力量，协助公共安全话务员和大量民众在全国范围内的紧急情况中。我们的研究集中于开发一种语言模型，能够理解用户在911呼叫中描述自己的情况，使LLAMA2能够分析内容并为话务员提供相关指导，同时创建工作流程，在必要时将呼叫者信息通知政府机构。该语言模型提供的另一个好处是，当911系统不堪重负时，它能够在重大紧急事件中协助人们。

    arXiv:2402.10908v1 Announce Type: cross  Abstract: Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelme
    
[^251]: Hermite神经网络模拟解决二维薛定谔方程

    Hermite Neural Network Simulation for Solving the 2D Schrodinger Equation

    [https://arxiv.org/abs/2402.10649](https://arxiv.org/abs/2402.10649)

    使用混合神经网络结合Hermite函数根作为配点，提高了解决二维薛定谔方程的效率和精度，与其他神经网络和方法相比取得了优秀的结果。

    

    arXiv: 2402.10649v1 公告类型: 交叉摘要: 薛定谔方程是描述量子力学系统中波函数行为的数学方程。它是一个偏微分方程，提供了有关量子力学基本原理的宝贵见解。本文旨在通过使用混合神经网络与基于Hermite函数的配点方法来以足够的精度解决薛定谔方程。最初，Hermite函数的根被用作配点，提高了解决方案的效率。薛定谔方程在无限域中定义，使用Hermite函数作为激活函数导致了出色的精度。最后，提出的方法使用MATLAB的Simulink工具进行了模拟。然后将结果与使用基于物理信息的神经网络和所提出的方法获得的结果进行了比较。

    arXiv:2402.10649v1 Announce Type: cross  Abstract: The Schrodinger equation is a mathematical equation describing the wave function's behavior in a quantum-mechanical system. It is a partial differential equation that provides valuable insights into the fundamental principles of quantum mechanics. In this paper, the aim was to solve the Schrodinger equation with sufficient accuracy by using a mixture of neural networks with the collocation method base Hermite functions. Initially, the Hermite functions roots were employed as collocation points, enhancing the efficiency of the solution. The Schrodinger equation is defined in an infinite domain, the use of Hermite functions as activation functions resulted in excellent precision. Finally, the proposed method was simulated using MATLAB's Simulink tool. The results were then compared with those obtained using Physics-informed neural networks and the presented method.
    
[^252]: 基于上下文的奖励：基于动态偏好调整的多目标基础模型对齐

    Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment

    [https://arxiv.org/abs/2402.10207](https://arxiv.org/abs/2402.10207)

    本文介绍了Rewards-in-Context（RiC）方法，该方法通过多个奖励条件控制基础模型的响应，并应用有监督的微调进行对齐。它具有简单性和适应性，并支持在推理时动态调整用户偏好。

    

    我们考虑了基于人类偏好的基础模型多目标对齐问题，这是实现有益和无害的人工智能系统的关键步骤。然而，使用强化学习（RL）对大型基础模型进行微调通常是昂贵且不稳定的，并且人类偏好的多维度、异质性和冲突性进一步复杂化了对齐过程。在本文中，我们引入了Rewards-in-Context（RiC）方法，它使得基础模型的响应取决于其提示上下文中的多个奖励，并应用有监督的微调来进行对齐。RiC的显著特点是简单性和适应性，因为它只需要对单个基础模型进行有监督的微调，并支持在推理时动态调整用户偏好。受到抽象的凸优化问题的解析解的启发，我们提出了一种动态推理时调整方法。

    arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
    
[^253]: 使用锐度感知最小化和通道注意力解锁Transformer在时间序列预测中的潜力

    Unlocking the Potential of Transformers in Time Series Forecasting with Sharpness-Aware Minimization and Channel-Wise Attention

    [https://arxiv.org/abs/2402.10198](https://arxiv.org/abs/2402.10198)

    本文研究了Transformer在时间序列预测中的局限性，发现其注意力机制是泛化能力不足的原因。在此基础上，提出了一个浅层轻量级的Transformer模型SAMformer，通过锐度感知优化避免了陷入坏的局部最小值，并在常用时间序列数据集上超过了当前最先进的模型TSMixer。

    

    Transformer架构在自然语言处理和计算机视觉中取得了突破性的性能，但在多元长期预测方面，它们仍然不如更简单的线性基线。为了更好地理解这一现象，我们首先研究了一个玩具线性预测问题，展示了尽管Transformer具有高表达能力，但它们无法收敛到真正的解决方案。我们进一步确定Transformer的注意力是造成其低泛化能力的原因。基于这一认识，我们提出了一个浅层轻量级的Transformer模型，在锐度感知优化的情况下成功避免了坏的局部最小值。我们通过实验证明，这个结果适用于所有常用的实际多元时间序列数据集。特别是，相比当前最先进的模型TSMixer，SAMformer的平均性能提高了14.33%，并且参数数量减少了约4倍。

    arXiv:2402.10198v1 Announce Type: new  Abstract: Transformer-based architectures achieved breakthrough performance in natural language processing and computer vision, yet they remain inferior to simpler linear baselines in multivariate long-term forecasting. To better understand this phenomenon, we start by studying a toy linear forecasting problem for which we show that transformers are incapable of converging to their true solution despite their high expressive power. We further identify the attention of transformers as being responsible for this low generalization capacity. Building upon this insight, we propose a shallow lightweight transformer model that successfully escapes bad local minima when optimized with sharpness-aware optimization. We empirically demonstrate that this result extends to all commonly used real-world multivariate time series datasets. In particular, SAMformer surpasses the current state-of-the-art model TSMixer by 14.33% on average, while having ~4 times few
    
[^254]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^255]: 调谐：在临床设置中使用有限数据的音频分类器性能分析

    Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data

    [https://arxiv.org/abs/2402.10100](https://arxiv.org/abs/2402.10100)

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。

    

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，限制条件是以反映实际世界数据收集的小数据集为基础。我们分析了包括DenseNet和ConvNeXt在内的CNN模型，以及ViT、SWIN和AST等转换模型，并将它们与诸如YAMNet和VGGish的预训练音频模型进行比较。我们的方法强调了在特定临床数据上微调之前，在大数据集上进行预训练的好处。我们从卒中患者中新收集了两个前所未有的患者音频数据集。我们研究了各种预处理技术，发现基于它们从预训练中学习到的先验知识，RGB和灰度谱图转换对模型性能产生了不同的影响。我们的研究结果表明，CNN模型在小数据集环境中可以与转换模型相媲美或超越，其中DenseNet-Contrastive和AST模型表现突出。本研究强调了...

    arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
    
[^256]: GraphCBAL: 通过强化学习对图神经网络进行类平衡主动学习

    GraphCBAL: Class-Balanced Active Learning for Graph Neural Networks via Reinforcement Learning

    [https://arxiv.org/abs/2402.10074](https://arxiv.org/abs/2402.10074)

    本文提出了一种通过强化学习对图神经网络进行类平衡主动学习的框架GraphCBAL，该框架能够学习一种最佳策略，选择类平衡和信息丰富的节点进行注释，以最大化GNNs性能。

    

    最近，图神经网络（GNNs）已经取得了显著的成功。GNNs的主动学习旨在从未标记的数据中查询有价值的样本进行注释，以最大限度地降低成本并提高GNNs的性能。然而，对于GNNs中的强化主动学习，现有的大多数方法可能导致高度不平衡的类分布，尤其是在高度倾斜的类别场景下。这进一步对分类性能产生负面影响。为了解决这个问题，本文提出了一种新颖的增强类平衡主动学习框架GraphCBAL，用于GNNs。它学习一种最佳策略，以获取类平衡和信息丰富的节点进行注释，从而最大化选择的标记节点训练的GNNs的性能。GraphCBAL设计了类平衡感知状态和奖励函数，实现模型性能和类平衡之间的折衷。我们进一步改进了GraphCBAL，得到GraphCBAL++。

    arXiv:2402.10074v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have recently demonstrated significant success. Active learning for GNNs aims to query the valuable samples from the unlabeled data for annotation to maximize the GNNs' performance at a low cost. However, most existing methods for reinforced active learning in GNNs may lead to a highly imbalanced class distribution, especially in highly skewed class scenarios. This further adversely affects the classification performance. To tackle this issue, in this paper, we propose a novel reinforced class-balanced active learning framework for GNNs, namely, GraphCBAL. It learns an optimal policy to acquire class-balanced and informative nodes for annotation, maximizing the performance of GNNs trained with selected labeled nodes. GraphCBAL designs class-balance-aware states, as well as a reward function that achieves trade-off between model performance and class balance. We further upgrade GraphCBAL to GraphCBAL++ by intr
    
[^257]: LoraRetriever: 适应输入的LoRA检索与合成方法用于混合任务

    LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild

    [https://arxiv.org/abs/2402.09997](https://arxiv.org/abs/2402.09997)

    LoraRetriever提出了一种适应输入的LoRA检索与合成方法，用于弥合实际情况下大型语言模型接收到不同任务提示的差距。

    

    Low-Rank Adaptation (LoRA)为大型语言模型（LLM）的微调提供了一种有效而高效的解决方案。LoRA的模块化和即插即用的特性使得能够集成各种领域特定的LoRA，以增强LLM的能力。先前的研究要么专注于特定的隔离下游任务，要么在训练过程中固定LoRA的选择。然而，在实际情况中，LLM接收到涵盖不同任务的各种提示，并且候选LoRA的池经常动态更新。为了弥合这一差距，我们提出了LoraRetriever，一种根据输入提示自适应检索和合成多个LoRA的框架。LoraRetriever包含三个主要组成部分：首先，识别和检索与给定输入相关的LoRA；其次，制定有效整合检索到的LoRA的策略；最后，开发高效的方法用于实现LoRA的合成。

    arXiv:2402.09997v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing effici
    
[^258]: 制定良好提示还是提供出色的对话？关于基于上下文学习的角色生成对话的研究

    Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation

    [https://arxiv.org/abs/2402.09954](https://arxiv.org/abs/2402.09954)

    本研究通过对大型语言模型在基于角色生成对话方面进行实验，发现调整提示指令可以最直接有效且经济地提高生成质量，并且随机检索示范会取得最佳结果，而查询相同上下文的示范检索效果最差。即使破坏了示范中的多回合关联和单回合语义，对话生成仍然有效。

    

    先前关于上下文学习（ICL）的研究主要侧重于分类、机器翻译、文本到表格等任务，而对于ICL能否改进生成类似人类对话的研究很少。我们的工作通过在高质量的真实人类对话数据集上进行广泛的实验，系统地研究了大型语言模型（LLMs）在基于角色生成对话方面的ICL能力。根据实验结果，我们得出三个结论：1）调整提示指令是提高生成质量最直接、有效和经济的方法；2）随机检索示范可以取得最佳的结果，可能是因为具有更多样化和有效信息的原因；与查询相同上下文的示范检索结果最差；3）即使破坏了示范中的多回合关联和单回合语义，对话生成仍然可以实现较好的效果。

    arXiv:2402.09954v1 Announce Type: new  Abstract: Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demo
    
[^259]: 用深度学习增强金融行业的网络安全韧性，实现高级威胁检测

    Enhancing Cybersecurity Resilience in Finance with Deep Learning for Advanced Threat Detection

    [https://arxiv.org/abs/2402.09820](https://arxiv.org/abs/2402.09820)

    这项研究提出使用深度学习来增强金融行业的网络安全韧性，并实现高级威胁检测。目前的网络威胁检测方法往往基于规则和传统的机器学习方法，无法适用大规模数据应用，并且无法有效检测未知威胁。

    

    在互联网时代，人们的生活越来越依赖于今天的网络技术。然而，网络技术是一把双刃剑，给人们带来便利的同时也带来了许多安全挑战。保持网络安全和保护用户的合法利益是网络建设的核心。威胁检测是一个完整有效的防御系统的重要组成部分。在网络信息安全领域，网络攻击和网络防护的技术更新日益迅猛。如何有效地检测未知威胁是网络防护的关注焦点之一。目前，网络威胁检测通常基于规则和传统的机器学习方法，这些方法创建人工规则或提取常见的时空特征，不能应用于大规模数据应用，并且未知威胁的出现导致了系统的检测准确性降低。

    arXiv:2402.09820v1 Announce Type: cross  Abstract: In the age of the Internet, people's lives are increasingly dependent on today's network technology. However, network technology is a double-edged sword, bringing convenience to people but also posing many security challenges. Maintaining network security and protecting the legitimate interests of users is at the heart of network construction. Threat detection is an important part of a complete and effective defense system. In the field of network information security, the technical update of network attack and network protection is spiraling. How to effectively detect unknown threats is one of the concerns of network protection. Currently, network threat detection is usually based on rules and traditional machine learning methods, which create artificial rules or extract common spatiotemporal features, which cannot be applied to large-scale data applications, and the emergence of unknown threats causes the detection accuracy of the or
    
[^260]: 关于基于网络特征在欺诈检测中潜力的研究

    On the Potential of Network-Based Features for Fraud Detection

    [https://arxiv.org/abs/2402.09495](https://arxiv.org/abs/2402.09495)

    本文研究了基于网络特征在欺诈检测中的潜力，通过使用个性化的PageRank算法来捕捉欺诈的社会动态。实验结果表明，集成PPR可以提高模型的预测能力并提供独特有价值的信息。

    

    在线交易欺诈给企业和消费者带来了重大挑战，面临着重大的经济损失。传统的基于规则的系统难以跟上欺诈战术的演变，导致高误报率和漏报率。机器学习技术通过利用历史数据识别欺诈模式提供了一个有希望的解决方案。本文探讨使用个性化的PageRank（PPR）算法通过分析金融账户之间的关系来捕捉欺诈的社会动态。主要目标是比较传统特征与添加PPR在欺诈检测模型中的性能。结果表明，集成PPR可以提高模型的预测能力，超过基准模型。此外，PPR特征提供了独特而有价值的信息，通过其高特征重要性得分得以证明。特征稳定性分析证实了一致的结果。

    arXiv:2402.09495v1 Announce Type: cross  Abstract: Online transaction fraud presents substantial challenges to businesses and consumers, risking significant financial losses. Conventional rule-based systems struggle to keep pace with evolving fraud tactics, leading to high false positive rates and missed detections. Machine learning techniques offer a promising solution by leveraging historical data to identify fraudulent patterns. This article explores using the personalised PageRank (PPR) algorithm to capture the social dynamics of fraud by analysing relationships between financial accounts. The primary objective is to compare the performance of traditional features with the addition of PPR in fraud detection models. Results indicate that integrating PPR enhances the model's predictive power, surpassing the baseline model. Additionally, the PPR feature provides unique and valuable information, evidenced by its high feature importance score. Feature stability analysis confirms consist
    
[^261]: 人类中的即时概括与深度神经网络中的滞后概括——表示分歧的证据？

    Immediate generalisation in humans but a generalisation lag in deep neural networks$\unicode{x2014}$evidence for representational divergence?

    [https://arxiv.org/abs/2402.09303](https://arxiv.org/abs/2402.09303)

    研究对比了人类和深度神经网络在图像分类中的行为差异，发现人类具有即时概括能力，而DNNs存在滞后概括现象，这表明了表示分歧的存在。

    

    近期的研究在图像分类领域中对比了人类与深度神经网络（DNNs）的许多行为比较。通常，比较研究关注的是学习过程的最终结果，通过测量和比较目标类别表示的相似性。然而，这些表示如何形成即其过程——即在获取过程中观察到的行为变化和中间阶段——往往少有直接和实证的比较。在这里，我们报告了对人类观察者和不同经典与最新技术的DNNs中可转移表示是如何被获取的的详细调查。我们开发了一个受限的监督学习环境，该环境中我们对齐了学习相关的参数，如起始点、输入模式、可用输入数据以及提供的反馈。在整个学习过程中我们评估...

    arXiv:2402.09303v1 Announce Type: cross Abstract: Recent research has seen many behavioral comparisons between humans and deep neural networks (DNNs) in the domain of image classification. Often, comparison studies focus on the end-result of the learning process by measuring and comparing the similarities in the representations of object categories once they have been formed. However, the process of how these representations emerge$\unicode{x2014}$that is, the behavioral changes and intermediate stages observed during the acquisition$\unicode{x2014}$is less often directly and empirically compared.   Here we report a detailed investigation of how transferable representations are acquired in human observers and various classic and state-of-the-art DNNs. We develop a constrained supervised learning environment in which we align learning-relevant parameters such as starting point, input modality, available input data and the feedback provided. Across the whole learning process we evaluate 
    
[^262]: UR2M: 微控制器上的不确定性和资源感知事件检测

    UR2M: Uncertainty and Resource-Aware Event Detection on Microcontrollers

    [https://arxiv.org/abs/2402.09264](https://arxiv.org/abs/2402.09264)

    UR2M是一个新颖的不确定性和资源感知的事件检测框架，针对微控制器上的应用，通过评估模型输出的可靠性来解决传统机器学习技术在数据分布变化时产生不准确预测的问题。

    

    传统的机器学习技术在训练和测试阶段的数据分布发生变化时容易产生不准确的预测。这种脆弱性可能导致严重后果，特别是在移动医疗等应用中。不确定性估计有潜力通过评估模型输出的可靠性来缓解这个问题。然而，现有的不确定性估计技术通常需要大量的计算资源和内存，使它们在微控制器（MCU）上的实施变得不切实际。这个限制阻碍了许多重要的设备上可穿戴事件检测（WED）应用的可行性，如心脏病发作检测。在本文中，我们提出了UR2M，一个针对MCU的新颖的不确定性和资源感知事件检测框架。具体地，我们（i）基于证据理论开发了一种不确定性感知的WED，用于准确的事件检测。

    arXiv:2402.09264v1 Announce Type: new Abstract: Traditional machine learning techniques are prone to generating inaccurate predictions when confronted with shifts in the distribution of data between the training and testing phases. This vulnerability can lead to severe consequences, especially in applications such as mobile healthcare. Uncertainty estimation has the potential to mitigate this issue by assessing the reliability of a model's output. However, existing uncertainty estimation techniques often require substantial computational resources and memory, making them impractical for implementation on microcontrollers (MCUs). This limitation hinders the feasibility of many important on-device wearable event detection (WED) applications, such as heart attack detection.   In this paper, we present UR2M, a novel Uncertainty and Resource-aware event detection framework for MCUs. Specifically, we (i) develop an uncertainty-aware WED based on evidential theory for accurate event detection
    
[^263]: IMUOptimize: 一种基于数据驱动的方法，用于使用Transformer架构进行人体姿态估计的最佳IMU放置

    IMUOptimize: A Data-Driven Approach to Optimal IMU Placement for Human Pose Estimation with Transformer Architecture

    [https://arxiv.org/abs/2402.08923](https://arxiv.org/abs/2402.08923)

    本文提出了一种基于数据驱动的方法，用于使用Transformer架构进行人体姿态估计的最佳IMU放置。研究结果表明，该方法在姿态重建准确性方面优于传统的双向RNN模型，并且在使用只有6个IMU时，Transformer架构将24个IMU位置获取的数据与双向RNN具有相当的性能。这一优化选择的IMU放置策略结合了Transformer的并行性和性能优势，对基于IMU的姿态估计领域带来了显著的改进。

    

    本文提出了一种使用IMU数据预测人体姿态的新方法，不同于之前的研究DIP-IMU、IMUPoser和TransPose，它们使用最多6个IMU与双向RNN结合。我们引入了两个主要创新：一种基于数据驱动的最佳IMU放置策略和基于Transformer的时序分析模型架构。我们的研究结果表明，我们的方法不仅在传统的以6个IMU为基础的双向RNN模型上表现优越，而且使用只有6个IMU时，与双向RNN相比，Transformer架构显著提高了从24个IMU位置获取的数据的姿态重建准确性。我们优化选择的位置提供了显著的IMU姿态估计领域的改进，结合Transformer的并行性和性能优势。

    arXiv:2402.08923v1 Announce Type: new Abstract: This paper presents a novel approach for predicting human poses using IMU data, diverging from previous studies such as DIP-IMU, IMUPoser, and TransPose, which use up to 6 IMUs in conjunction with bidirectional RNNs. We introduce two main innovations: a data-driven strategy for optimal IMU placement and a transformer-based model architecture for time series analysis. Our findings indicate that our approach not only outperforms traditional 6 IMU-based biRNN models but also that the transformer architecture significantly enhances pose reconstruction from data obtained from 24 IMU locations, with equivalent performance to biRNNs when using only 6 IMUs. The enhanced accuracy provided by our optimally chosen locations, when coupled with the parallelizability and performance of transformers, provides significant improvements to the field of IMU-based pose estimation.
    
[^264]: 自动编码贝叶斯反向游戏

    Auto-Encoding Bayesian Inverse Games

    [https://arxiv.org/abs/2402.08902](https://arxiv.org/abs/2402.08902)

    本文研究了多个智能体在共同环境中的交互问题，并提出了一种贝叶斯方法来推断游戏参数。采用变分自动编码器（VAE）来构建游戏参数的后验分布，从而解决了现有方法中无法对不确定性进行定量化的问题。

    

    当多个智能体在共同环境中互动时，每个智能体的行动会影响其他智能体未来的决策，而非合作动态游戏自然地捕捉到了这种耦合。然而，在交互式运动规划中，智能体通常没有完整的游戏模型，例如由于其他玩家的目标是未知的。因此，我们考虑了逆向游戏问题，其中游戏的某些属性是先验未知的，必须根据观测结果进行推断。现有的最大似然估计（MLE）方法解决逆向游戏问题时仅提供未知参数的点估计而不对不确定性进行定量化，并且在许多参数值能解释观测行为时表现不佳。为了解决这些限制，我们采用贝叶斯观点构建了游戏参数的后验分布。为了使推断可行，我们使用了一个具有内嵌可微分游戏的变分自动编码器（VAE）。

    arXiv:2402.08902v1 Announce Type: cross Abstract: When multiple agents interact in a common environment, each agent's actions impact others' future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game
    
[^265]: 图马巴：面向基于状态空间模型的图学习

    Graph Mamba: Towards Learning on Graphs with State Space Models

    [https://arxiv.org/abs/2402.08678](https://arxiv.org/abs/2402.08678)

    本文提出了一种基于选择性SSMs的新类GNNs框架——图马巴网络（GMNs），通过不依赖于Transformer、复杂的消息传递和位置/结构编码（SE/PE），解决了传统GNNs的过度压缩和无法捕捉长程依赖的问题。

    

    图神经网络（GNNs）在图表示学习方面显示出了很大的潜力。大多数GNNs定义了一种局部消息传递机制，通过堆叠多个层在图上传播信息。然而，这些方法已知存在两个主要限制：过度压缩和无法捕捉长程依赖。最近，图转换器（GTs）作为消息传递神经网络（MPNNs）的一种强大替代方法出现。然而，GTs具有二次计算成本，在图结构上缺乏归纳偏差，并且依赖复杂的位置/结构编码（SE/PE）。在本文中，我们展示了在实践中，尽管Transformer、复杂的消息传递和SE/PE对于良好性能而言是足够的，但并非必需。受到最近的状态空间模型（SSMs）（例如Mamba）的成功启发，我们提出了图马巴网络（GMNs），这是一种基于选择性SSMs的新类GNNs的通用框架。我们讨论并对新的c进行分类。

    Graph Neural Networks (GNNs) have shown promising potential in graph representation learning. The majority of GNNs define a local message-passing mechanism, propagating information over the graph by stacking multiple layers. These methods, however, are known to suffer from two major limitations: over-squashing and poor capturing of long-range dependencies. Recently, Graph Transformers (GTs) emerged as a powerful alternative to Message-Passing Neural Networks (MPNNs). GTs, however, have quadratic computational cost, lack inductive biases on graph structures, and rely on complex Positional/Structural Encodings (SE/PE). In this paper, we show that while Transformers, complex message-passing, and SE/PE are sufficient for good performance in practice, neither is necessary. Motivated by the recent success of State Space Models (SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general framework for a new class of GNNs based on selective SSMs. We discuss and categorize the new c
    
[^266]: 使用分散的相位振荡器学习新生步态：关于观察、奖励和反馈的作用

    Learning Emergent Gaits with Decentralized Phase Oscillators: on the role of Observations, Rewards, and Feedback

    [https://arxiv.org/abs/2402.08662](https://arxiv.org/abs/2402.08662)

    该论文通过将相位观察、简单的相位奖励和局部反馈动力学相结合，提出了一种使用分散相位振荡器学习动物步态的模型，有效地实现了新生步态偏好的策略。

    

    我们提出了一个最小化相位振荡器模型来学习四足动物的运动。每个振荡器仅与自身和相应的腿之间通过地面反作用力的局部反馈相连，这可以被解释为一个观察者反馈增益。我们将振荡器本身解释为潜在的接触状态估计器。通过系统的消除研究，我们表明相位观察、简单的基于相位的奖励和局部反馈动力学的结合引导出展现新生步态偏好的策略，同时使用了一组简化的奖励，而没有预先指定特定的步态。代码是开源的，视频摘要可在https://youtu.be/1NKQ0rSV3jU处获取。

    We present a minimal phase oscillator model for learning quadrupedal locomotion. Each of the four oscillators is coupled only to itself and its corresponding leg through local feedback of the ground reaction force, which can be interpreted as an observer feedback gain. We interpret the oscillator itself as a latent contact state-estimator. Through a systematic ablation study, we show that the combination of phase observations, simple phase-based rewards, and the local feedback dynamics induces policies that exhibit emergent gait preferences, while using a reduced set of simple rewards, and without prescribing a specific gait. The code is open-source, and a video synopsis available at https://youtu.be/1NKQ0rSV3jU.
    
[^267]: SAGMAN: 用于图神经网络在流形上的稳定性分析的方法

    SAGMAN: Stability Analysis of Graph Neural Networks on the Manifolds

    [https://arxiv.org/abs/2402.08653](https://arxiv.org/abs/2402.08653)

    SAGMAN是一种用于检验图神经网络稳定性的谱框架，它通过评估非线性映射中的距离失真来衡量GNN的稳定性。为了进行有意义的稳定性分析，我们提出了一种距离保持的图降维方法。

    

    现代图神经网络（GNN）对输入图结构和节点特征的变化敏感，可能导致不可预测的行为和性能下降。本文引入了一种称为SAGMAN的谱框架，用于检验GNN的稳定性。该框架评估非线性映射中GNN在输入和输出流形之间引起的距离失真: 当输入流行中两个附近的节点（通过GNN模型）被映射到输出流行上的两个远离的节点时，意味着存在较大的距离失真，从而导致GNN的稳定性较差。我们提出了一种距离保持的图降维（GDR）方法，利用谱图嵌入和概率图模型（PGMs）来创建低维的输入/输出基于图的流形，以进行有意义的稳定性分析。我们的实证评估表明，SAGMAN能够有效评估每个节点在面对不同边缘或特征扰动时的稳定性。

    Modern graph neural networks (GNNs) can be sensitive to changes in the input graph structure and node features, potentially resulting in unpredictable behavior and degraded performance. In this work, we introduce a spectral framework known as SAGMAN for examining the stability of GNNs. This framework assesses the distance distortions that arise from the nonlinear mappings of GNNs between the input and output manifolds: when two nearby nodes on the input manifold are mapped (through a GNN model) to two distant ones on the output manifold, it implies a large distance distortion and thus a poor GNN stability. We propose a distance-preserving graph dimension reduction (GDR) approach that utilizes spectral graph embedding and probabilistic graphical models (PGMs) to create low-dimensional input/output graph-based manifolds for meaningful stability analysis. Our empirical evaluations show that SAGMAN effectively assesses the stability of each node when subjected to various edge or feature pe
    
[^268]: 在黑盒大型语言模型上进行知识编辑

    Knowledge Editing on Black-box Large Language Models

    [https://arxiv.org/abs/2402.08631](https://arxiv.org/abs/2402.08631)

    这项研究提出了在黑盒大型语言模型上进行知识编辑的方法，并引入了一种多角度评估框架和一种新的postEdit框架，以解决现有方法中的隐私和风格问题。

    

    知识编辑旨在高效、精确地修改大型语言模型的行为，以更新特定的知识，而不对其他知识产生负面影响。当前的研究主要集中在白盒语言模型编辑上，忽视了一个重要的场景：黑盒语言模型编辑，即通过接口访问语言模型，并仅可用文本输出。为了解决现有评估在黑盒语言模型编辑上不适用且缺乏全面性的局限性，我们提出了一种多角度评估框架，首次将风格保留的评估纳入其中。为了解决当前方法中的编辑数据隐私泄漏和风格过度编辑的问题，我们引入了一种新的postEdit框架，通过下游后处理解决隐私问题，并通过对原始回答进行细粒度编辑来保持文本风格一致性。在两个基准测试上的实验与分析表明，postEdit的性能超过了所有现有方法。

    Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
    
[^269]: ChatCell: 利用自然语言促进单细胞分析

    ChatCell: Facilitating Single-Cell Analysis with Natural Language

    [https://arxiv.org/abs/2402.08303](https://arxiv.org/abs/2402.08303)

    ChatCell是一个利用自然语言促进单细胞分析的工具，通过词汇适应和统一序列生成，它具备深厚的专业知识和适应各种分析任务的能力。

    

    随着大型语言模型(LLMs)的快速发展，它们在科学中的影响日益突出。LLMs在任务泛化和自由对话方面的新兴能力可以极大地推进化学和生物学等领域。然而，单细胞生物学这个构成生物体基础构件的领域仍面临一些挑战。当前方法在知识门槛和可扩展性方面存在限制，阻碍了LLMs在掌握单细胞数据方面的充分利用，影响了直接可访问和快速迭代的能力。为此，我们引入了ChatCell，通过利用词汇适应和统一序列生成，它在单细胞生物学领域获得了深厚的专业知识和适应各种分析任务的能力，标志着一种范式转变。

    As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
    
[^270]: LLaGA: 大型语言和图形助手

    LLaGA: Large Language and Graph Assistant

    [https://arxiv.org/abs/2402.08170](https://arxiv.org/abs/2402.08170)

    LLaGA是一个创新的模型，它有效地整合了大型语言模型（LLM）的能力，以处理图结构数据的复杂性。通过重新组织图节点以作为结构感知的序列，并通过一个多功能投影仪将其映射到标记嵌入空间中，LLaGA在多样性、泛化性和可解释性方面表现出色。

    

    图神经网络（GNN）已经推动了图结构数据分析的进步。最近，大型语言模型（LLM）如GPT-4的崛起预示着深度学习的一个新时代。然而，将它们应用于图数据还面临着独特的挑战，由于将图结构转化为文本的固有难度。为此，我们引入了一个创新模型——大型语言和图形助手（LLaGA），它有效地整合了LLM的能力，以处理图结构数据的复杂性。LLaGA保留了LLM的通用性，同时将图数据转化为与LLM输入兼容的格式。LLaGA通过重新组织图节点以作为结构感知的序列，然后通过一个多功能投影仪将其映射到标记嵌入空间中。LLaGA在多样性、泛化性和可解释性方面表现出色，使其能够在不同数据集和任务上表现出一致的良好性能。

    Graph Neural Networks (GNNs) have empowered the advance in graph-structured data analysis. Recently, the rise of Large Language Models (LLMs) like GPT-4 has heralded a new era in deep learning. However, their application to graph data poses distinct challenges due to the inherent difficulty of translating graph structures to language. To this end, we introduce the \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{G}raph \textbf{A}ssistant (\textbf{LLaGA}), an innovative model that effectively integrates LLM capabilities to handle the complexities of graph-structured data. LLaGA retains the general-purpose nature of LLMs while adapting graph data into a format compatible with LLM input. LLaGA achieves this by reorganizing graph nodes to structure-aware sequences and then mapping these into the token embedding space through a versatile projector. LLaGA excels in versatility, generalizability and interpretability, allowing it to perform consistently well across different datasets and 
    
[^271]: 具有一般价值函数的情境多项式逻辑回归赌博机

    Contextual Multinomial Logit Bandits with General Value Functions

    [https://arxiv.org/abs/2402.08126](https://arxiv.org/abs/2402.08126)

    本文研究了具有一般价值函数的情境多项式逻辑回归赌博机，并提出了一套算法来处理线性情况，这些算法具有计算效率高、无维度的遗憾界限以及处理完全对抗性环境和奖励的能力。

    

    情境多项式逻辑回归 (MNL) 赌博机可以解决许多实际中的推荐问题，比如在线零售/广告。然而，先前的研究仅考虑了（广义的）线性价值函数，这严重限制了其适用性。鉴于这一事实，在本研究中，我们考虑了包含真实情况的情境MNL赌博机，借鉴了最近对情境赌博机研究的趋势。具体而言，我们考虑了随机和对抗性环境，并提出了一套算法，每个算法在计算和遗憾之间有不同的权衡。当应用于线性情况时，我们的结果不仅是第一个不依赖于某个可能指数增长的问题相关常数的结果，还具有其他优势，如计算效率、无维度的遗憾界限以及处理完全对抗性环境和奖励的能力。

    Contextual multinomial logit (MNL) bandits capture many real-world assortment recommendation problems such as online retailing/advertising. However, prior work has only considered (generalized) linear value functions, which greatly limits its applicability. Motivated by this fact, in this work, we consider contextual MNL bandits with a general value function class that contains the ground truth, borrowing ideas from a recent trend of studies on contextual bandits. Specifically, we consider both the stochastic and the adversarial settings, and propose a suite of algorithms, each with different computation-regret trade-off. When applied to the linear case, our results not only are the first ones with no dependence on a certain problem-dependent constant that can be exponentially large, but also enjoy other advantages such as computational efficiency, dimension-free regret bounds, or the ability to handle completely adversarial contexts and rewards.
    
[^272]: 只有曲线形状有关：通过下一个曲线形状预测训练基础模型进行零样本多元时间序列预测

    Only the Curve Shape Matters: Training Foundation Models for Zero-Shot Multivariate Time Series Forecasting through Next Curve Shape Prediction

    [https://arxiv.org/abs/2402.07570](https://arxiv.org/abs/2402.07570)

    通过下一个曲线形状预测，我们提出了基于编码器的零样本多元时间序列预测模型GTT，通过预训练和通道级别的曲线形状预测，展现出优秀的预测能力，甚至超过了最先进的有监督模型。

    

    我们提出了General Time Transformer (GTT)，一种仅有编码器的基础模型，用于零样本多元时间序列预测。GTT在一个包含2亿个高质量时间序列样本的大型数据集上进行预训练，涵盖了不同领域。在我们提出的框架中，多元时间序列预测的任务被建模为一个逐通道的下一个曲线形状预测问题，其中每个时间序列样本表示为一系列非重叠的曲线形状，具有统一的数值大小。GTT在通道级别上通过预测过去曲线形状的窗口来预测下一个曲线形状。实验结果表明，GTT在未见时间序列数据集上展现出优秀的零样本多元预测能力，甚至超过了最先进的有监督基线模型。此外，我们还研究了GTT模型参数和训练数据集规模变化的影响，观察到在零样本多元预测的背景下，规模定律也成立。

    We present General Time Transformer (GTT), an encoder-only style foundation model for zero-shot multivariate time series forecasting. GTT is pretrained on a large dataset of 200M high-quality time series samples spanning diverse domains. In our proposed framework, the task of multivariate time series forecasting is formulated as a channel-wise next curve shape prediction problem, where each time series sample is represented as a sequence of non-overlapping curve shapes with a unified numerical magnitude. GTT is trained to predict the next curve shape based on a window of past curve shapes in a channel-wise manner. Experimental results demonstrate that GTT exhibits superior zero-shot multivariate forecasting capabilities on unseen time series datasets, even surpassing state-of-the-art supervised baselines. Additionally, we investigate the impact of varying GTT model parameters and training dataset scales, observing that the scaling law also holds in the context of zero-shot multivariate
    
[^273]: 从均场稳态分布中采样

    Sampling from the Mean-Field Stationary Distribution

    [https://arxiv.org/abs/2402.07355](https://arxiv.org/abs/2402.07355)

    本文研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，并提出了一种解耦的方法。该方法能够在多种情况下提供改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    

    我们研究了从均场随机微分方程 (SDE) 的稳态分布中采样的复杂性，或者等价地，即包含交互项的概率测度空间上的最小化函数的复杂性。我们的主要洞察是将这个问题的两个关键方面解耦：(1) 通过有限粒子系统逼近均场SDE，通过时间均匀传播混沌，和(2) 通过标准对数凹抽样器从有限粒子稳态分布中采样。我们的方法在概念上更简单，其灵活性允许结合用于算法和理论的最新技术。这导致在许多设置中提供了改进的保证，包括在均场区域优化某些双层神经网络的更好保证。

    We study the complexity of sampling from the stationary distribution of a mean-field SDE, or equivalently, the complexity of minimizing a functional over the space of probability measures which includes an interaction term.   Our main insight is to decouple the two key aspects of this problem: (1) approximation of the mean-field SDE via a finite-particle system, via uniform-in-time propagation of chaos, and (2) sampling from the finite-particle stationary distribution, via standard log-concave samplers. Our approach is conceptually simpler and its flexibility allows for incorporating the state-of-the-art for both algorithms and theory. This leads to improved guarantees in numerous settings, including better guarantees for optimizing certain two-layer neural networks in the mean-field regime.
    
[^274]: 领域知识和多模态对智能分子性质预测的影响：一项系统调查

    The Impact of Domain Knowledge and Multi-Modality on Intelligent Molecular Property Prediction: A Systematic Survey

    [https://arxiv.org/abs/2402.07249](https://arxiv.org/abs/2402.07249)

    本文通过系统调查，发现整合领域知识可以提高分子性质预测的准确性，同时利用多模态数据融合可以产生更精确的结果。

    

    准确预测分子性质对于药物开发尤其是虚拟筛选和化合物优化的进展至关重要。近年来引入了许多基于深度学习的方法，在增强分子性质预测（MPP）方面显示出显著潜力，特别是提高了准确性和对分子结构的洞察力。然而，有两个关键问题：领域知识的整合是否增强了分子性质预测的准确性，使用多模态数据融合是否比单一数据来源方法产生更精确的结果？为了探究这些问题，我们全面回顾和定量分析了基于各种基准的最新深度学习方法。我们发现，整合分子信息将分别提高MPP回归和分类任务的准确性，分别高达3.98％和1.72％。我们还发现，使用三维信息与一维和二维信息相结合会产生更好的结果。

    The precise prediction of molecular properties is essential for advancements in drug development, particularly in virtual screening and compound optimization. The recent introduction of numerous deep learning-based methods has shown remarkable potential in enhancing molecular property prediction (MPP), especially improving accuracy and insights into molecular structures. Yet, two critical questions arise: does the integration of domain knowledge augment the accuracy of molecular property prediction and does employing multi-modal data fusion yield more precise results than unique data source methods? To explore these matters, we comprehensively review and quantitatively analyze recent deep learning methods based on various benchmarks. We discover that integrating molecular information will improve both MPP regression and classification tasks by upto 3.98% and 1.72%, respectively. We also discover that the utilizing 3-dimensional information with 1-dimensional and 2-dimensional informati
    
[^275]: 动态图信息瓶颈

    Dynamic Graph Information Bottleneck

    [https://arxiv.org/abs/2402.06716](https://arxiv.org/abs/2402.06716)

    动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。

    

    动态图广泛存在于现实世界中，它们携带着复杂的时空特征模式，对于它们的表示学习提出了挑战。动态图神经网络（DGNNs）通过利用内在的动态性展示了令人印象深刻的预测能力。然而，DGNNs展示了有限的鲁棒性，易受对抗攻击。本文提出了一种新颖的动态图信息瓶颈（DGIB）框架来学习鲁棒且有区分性的表示。借助信息瓶颈（IB）原理，我们首先提出期望的最优表示应满足最小-全局-一致（MSC）条件。为了在潜在表示中压缩冗余信息和保留有价值的信息，DGIB迭代地引导和改进通过图快照传递的结构和特征信息流。为了满足MSC条件，我们将整体IB目标分解为DGIB$_{MS}$和DGIB$_C$，其中DGIB$_{MS}$通道的目标是...

    Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
    
[^276]: 内省规划：引导语言驱动的代理机器人改进自身的不确定性

    Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty

    [https://arxiv.org/abs/2402.06529](https://arxiv.org/abs/2402.06529)

    本文研究了内省规划的概念，作为一种引导语言驱动的代理机器人改进自身不确定性的系统方法。通过识别任务不确定性并主动寻求澄清，内省显著提高了机器人任务规划的成功率和安全性。

    

    大型语言模型（LLM）展示了先进的推理能力，使得机器人能够理解自然语言指令，并通过适当的基础塑造来策略性地进行高级行动规划。然而，LLM产生的幻觉可能导致机器人自信地执行与用户目标不符或在极端情况下不安全的计划。此外，自然语言指令中的固有歧义可能引发任务的不确定性，尤其是在存在多个有效选项的情况下。为了解决这个问题，LLMs必须识别此类不确定性并主动寻求澄清。本文探索了内省规划的概念，作为一种系统方法，引导LLMs在无需微调的情况下形成意识到不确定性的机器人任务执行计划。我们研究了任务级机器人规划中的不确定性量化，并证明与最先进的基于LLM的规划方法相比，内省显著提高了成功率和安全性。

    Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
    
[^277]: 使用大型语言模型的多模态临床试验结果预测

    Multimodal Clinical Trial Outcome Prediction with Large Language Models

    [https://arxiv.org/abs/2402.06512](https://arxiv.org/abs/2402.06512)

    本研究提出了一种名为LIFTED的多模态临床试验结果预测方法，通过将不同模态数据转化为自然语言描述来统一数据，并构建统一的抗噪声编码器进行信息提取。

    

    临床试验是一个关键且昂贵的过程，通常需要多年时间和大量财力资源。因此，开发临床试验结果预测模型旨在排除可能失败的药物，并具有显著的成本节约潜力。最近的数据驱动尝试利用深度学习方法整合多模态数据来预测临床试验结果。然而，这些方法依赖于手动设计的模态特定编码器，这限制了适应新模态的可扩展性和识别不同模态之间相似信息模式的能力。为了解决这些问题，我们提出了一种多模态专家混合（LIFTED）方法用于临床试验结果预测。具体而言，LIFTED通过将不同模态的数据转化为自然语言描述来统一不同模态数据。然后，LIFTED构建统一的抗噪声编码器，从模态特定的语言描述中提取信息。

    The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
    
[^278]: 使用PEAK进行窥探：多个数据流均值的顺序、非参数复合假设检验

    Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams

    [https://arxiv.org/abs/2402.06122](https://arxiv.org/abs/2402.06122)

    本论文提出了一种名为PEAK的新型非参数顺序复合假设检验方法，适用于多个数据流的均值检验。该方法基于测试即博弈的框架，在任何停止时间上提供了非渐进α水平的检验。PEAK能够有效拒绝在满足非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。与现有方法相比，该方法具有较高的计算效率。

    

    我们提出了一种新颖的非参数顺序复合假设检验方法，用于多个数据流的均值。我们的方法名为PEAK（基于期望平均资产的窥探），基于测试即博弈的框架，提供了一个在任何停止时间上的非渐进α水平测试。PEAK在计算上可行，并且能够有效拒绝在满足我们的非参数假设条件的所有潜在分布中错误的假设，从而实现对多个数据流的联合复合假设检验。我们在强化学习中的最佳臂识别和阈值识别任务中对我们的理论结果进行了数值验证，并展示了我们的方法在计算效率上优于现有的测试方法。

    We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-as-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. PEAK is computationally tractable and efficiently rejects hypotheses that are incorrect across all potential distributions that satisfy our nonparametric assumption, enabling joint composite hypothesis testing on multiple streams of data. We numerically validate our theoretical findings under the best arm identification and threshold identification in the bandit setting, illustrating the computational efficiency of our method against state-of-the-art testing methods.
    
[^279]: TASER: 时间自适应采样的快速准确动态图表示学习

    TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning

    [https://arxiv.org/abs/2402.05396](https://arxiv.org/abs/2402.05396)

    该论文提出了TASER方法，它是针对动态图表示学习的时间自适应采样技术，在准确性、效率和可扩展性方面进行了优化，解决了现实世界动态图中存在的噪声问题。

    

    最近，时间图神经网络（TGNN）在包括欺诈检测和内容推荐在内的各种重要应用中展示出了最先进的性能。尽管TGNN取得了成功，但它们容易受到现实世界动态图中普遍存在的噪声的影响，例如时间过时的链接和偏斜的交互分布。这些噪声导致两个关键问题，严重损害了TGNN的准确性：（1）模型受到较差交互的监督，（2）噪声输入导致聚合消息的高方差。然而，目前的TGNN去噪技术并未考虑每个节点的多样化和动态的噪声模式。此外，它们还面临着遍历更多邻居导致产生过多小批量的开销。我们相信快速准确的TGNN的解决方法在于时间自适应采样。在这项工作中，我们提出了TASER，这是第一个针对准确性、效率和可扩展性进行优化的TGNN自适应采样方法。

    Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and sc
    
[^280]: E(3)-等变Mesh神经网络

    E(3)-Equivariant Mesh Neural Networks

    [https://arxiv.org/abs/2402.04821](https://arxiv.org/abs/2402.04821)

    E(3)-等变Mesh神经网络通过扩展E(n)-等变图神经网络的更新方程以包括网格面信息，并通过层次化进一步改进以考虑长程相互作用，实现了在网格任务上的优越性能，具有快速运行时间和无需昂贵预处理的特点。

    

    三角网格被广泛用于表示三维物体。因此，许多最近的研究都致力于在3D网格上进行几何深度学习。然而，我们观察到，许多这些架构的复杂性与实际性能之间并没有直接关联，并且在实践中，简单的深度模型对于几何图表现竞争力。受到这一观察的启发，我们最小限度地扩展了E(n)-等变图神经网络（EGNNs）（Satorras等，2021）的更新方程，以包括网格面信息，并通过层次化进一步改进了该方程以考虑长程相互作用。由此得到的架构，即等变Mesh神经网络（EMNN），在网格任务上优于其他更复杂的等变方法，具有快速的运行时间和无需昂贵的预处理。

    Triangular meshes are widely used to represent three-dimensional objects. As a result, many recent works have address the need for geometric deep learning on 3D mesh. However, we observe that the complexities in many of these architectures does not translate to practical performance, and simple deep models for geometric graphs are competitive in practice. Motivated by this observation, we minimally extend the update equations of E(n)-Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) to incorporate mesh face information, and further improve it to account for long-range interactions through hierarchy. The resulting architecture, Equivariant Mesh Neural Network (EMNN), outperforms other, more complicated equivariant methods on mesh tasks, with a fast run-time and no expensive pre-processing.
    
[^281]: 光HGNN：将超图神经网络蒸馏成MLPs，推断速度提升100倍

    LightHGNN: Distilling Hypergraph Neural Networks into MLPs for $100\times$ Faster Inference

    [https://arxiv.org/abs/2402.04296](https://arxiv.org/abs/2402.04296)

    本论文介绍了一种光HGNN方法，将超图神经网络(HGNNs)转化为Multi-Layer Perceptron (MLPs)以提高推断速度。LightHGNN通过软标签将知识从teacher HGNN蒸馏到student MLPs，而LightHGNN$^+$则注入了可靠的高阶相关性。

    

    最近，由于其在高阶相关性建模方面的优势，超图神经网络(HGNNs)引起了广泛关注并展现了令人满意的性能。然而，我们注意到，超图的高阶建模能力也带来了增加的计算复杂性，这阻碍了其在实际工业部署中的应用。实际上，我们发现HGNNs高阶结构依赖在推断过程中是高效部署的一个关键障碍。在本文中，我们提出了将HGNNs和高效推断的多层感知器(MLPs)联系起来，以消除HGNNs的超图依赖性，从而降低计算复杂性并改善推断速度。具体而言，我们引入了LightHGNN和LightHGNN$^+$，以实现快速推断和低复杂性。LightHGNN通过软标签将知识直接从teacher HGNN蒸馏到student MLPs中，而LightHGNN$^+$则进一步显式地将可靠的高阶相关性注入其中。

    Hypergraph Neural Networks (HGNNs) have recently attracted much attention and exhibited satisfactory performance due to their superiority in high-order correlation modeling. However, it is noticed that the high-order modeling capability of hypergraph also brings increased computation complexity, which hinders its practical industrial deployment. In practice, we find that one key barrier to the efficient deployment of HGNNs is the high-order structural dependencies during inference. In this paper, we propose to bridge the gap between the HGNNs and inference-efficient Multi-Layer Perceptron (MLPs) to eliminate the hypergraph dependency of HGNNs and thus reduce computational complexity as well as improve inference speed. Specifically, we introduce LightHGNN and LightHGNN$^+$ for fast inference with low complexity. LightHGNN directly distills the knowledge from teacher HGNNs to student MLPs via soft labels, and LightHGNN$^+$ further explicitly injects reliable high-order correlations into 
    
[^282]: 通过基于排列的权重匹配分析线性模式连接性

    Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching

    [https://arxiv.org/abs/2402.04051](https://arxiv.org/abs/2402.04051)

    通过基于排列的权重匹配分析线性模式连接性，我们实验证明了通过权重匹配找到的排列可以改变权重矩阵奇异向量的方向，但不能改变奇异值。这一发现对于理解随机梯度下降的有效性及其在模型合并等领域的应用具有重要意义。

    

    最近，Ainsworth等人展示了使用权重匹配（WM）来最小化排列搜索模型参数中的$L_2$距离有效地识别满足线性模式连接性（LMC）的排列的方法，其中，在两个具有不同种子的独立训练模型之间的线性路径上的损失保持几乎恒定。本文通过WM提供了LMC的理论分析，这对于理解随机梯度下降的有效性及其在模型合并等领域的应用至关重要。我们首先通过实验和理论分析表明，WM找到的排列并不显着减少两个模型之间的$L_2$距离，而LMC的出现并不仅仅是由于WM本身的距离减小。然后，我们提供了理论洞见，表明排列可以改变每层权重矩阵的奇异向量的方向，但不能改变奇异值。这一发现表明，WM找到的排列主要改变了权重矩阵的方向，而不是奇异值。

    Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
    
[^283]: 图缩减的综合调研：稀疏化、粗化和浓缩

    A Comprehensive Survey on Graph Reduction: Sparsification, Coarsening, and Condensation

    [https://arxiv.org/abs/2402.03358](https://arxiv.org/abs/2402.03358)

    这篇综述调研了图缩减方法，包括稀疏化、粗化和浓缩，在解决大型图形数据分析和计算复杂性方面起到了重要作用。调研对这些方法的技术细节进行了系统的回顾，并强调了它们在实际应用中的关键性。同时，调研还提出了保证图缩减技术持续有效性的关键研究方向。

    

    许多真实世界的数据集可以自然地表示为图，涵盖了广泛的领域。然而，图数据集的复杂性和规模的增加为分析和计算带来了显著的挑战。为此，图缩减技术在保留关键属性的同时简化大型图形数据变得越来越受关注。在本调研中，我们旨在提供对图缩减方法的全面理解，包括图稀疏化、图粗化和图浓缩。具体而言，我们建立了这些方法的统一定义，并引入了一个分层分类法来分类这些方法所解决的挑战。我们的调研系统地回顾了这些方法的技术细节，并强调了它们在各种场景中的实际应用。此外，我们还概述了保证图缩减技术持续有效性的关键研究方向，并提供了一个详细的论文列表链接。

    Many real-world datasets can be naturally represented as graphs, spanning a wide range of domains. However, the increasing complexity and size of graph datasets present significant challenges for analysis and computation. In response, graph reduction techniques have gained prominence for simplifying large graphs while preserving essential properties. In this survey, we aim to provide a comprehensive understanding of graph reduction methods, including graph sparsification, graph coarsening, and graph condensation. Specifically, we establish a unified definition for these methods and introduce a hierarchical taxonomy to categorize the challenges they address. Our survey then systematically reviews the technical details of these methods and emphasizes their practical applications across diverse scenarios. Furthermore, we outline critical research directions to ensure the continued effectiveness of graph reduction techniques, as well as provide a comprehensive paper list at https://github.
    
[^284]: 基于微环的相干光学GEMM加速器的比较分析

    A Comparative Analysis of Microrings Based Incoherent Photonic GEMM Accelerators

    [https://arxiv.org/abs/2402.03149](https://arxiv.org/abs/2402.03149)

    本文详细分析了基于微环的相干光学GEMM加速器的组织，其通过分裂、聚合、调制、加权和求和等方式操作光信号以加速深度神经网络中的矩阵-矩阵乘法，提高吞吐量和能量效率。

    

    提出了几种基于微环谐振器（MRR）的模拟光学架构，以在深度神经网络中加速通用的矩阵-矩阵乘法（GEMM），具有出色的吞吐量和能量效率。为了实现GEMM功能，这些基于MRR的架构一般通过五种不同的方式操作光信号：（i）将多个光信号分裂（复制）以达到某种多分支，（ii）将多个光信号聚合（复用）以达到某种多输入，（iii）调制光信号以将输入值印置于模拟信号幅度上，（iv）对调制的光信号进行加权，以实现模拟输入权重相乘，（v）对光信号进行求和。MRR基于的GEMM加速器以任意顺序执行前四种信号操作，忽略了这些操作顺序对其性能的可能影响。本文对加速器组织进行了详细分析。

    Several microring resonator (MRR) based analog photonic architectures have been proposed to accelerate general matrix-matrix multiplications (GEMMs) in deep neural networks with exceptional throughput and energy efficiency. To implement GEMM functions, these MRR-based architectures, in general, manipulate optical signals in five different ways: (i) Splitting (copying) of multiple optical signals to achieve a certain fan-out, (ii) Aggregation (multiplexing) of multiple optical signals to achieve a certain fan-in, (iii) Modulation of optical signals to imprint input values onto analog signal amplitude, (iv) Weighting of modulated optical signals to achieve analog input-weight multiplication, (v) Summation of optical signals. The MRR-based GEMM accelerators undertake the first four ways of signal manipulation in an arbitrary order ignoring the possible impact of the order of these manipulations on their performance. In this paper, we conduct a detailed analysis of accelerator organization
    
[^285]: 文本引导的图像聚类

    Text-Guided Image Clustering

    [https://arxiv.org/abs/2402.02996](https://arxiv.org/abs/2402.02996)

    这篇论文提出了一种文本引导的图像聚类方法，使用图像字幕和视觉问答模型生成文本，然后对生成的文本进行聚类，并通过注入任务或领域知识来改进聚类结果。实验证明，获得的文本表示通常优于图像特征，而基于关键词的解释可以更好地描述聚类。

    

    图像聚类将一组图像分成有意义的组，通常通过人工给出的注释进行解释。这些注释通常以文本形式存在，引发了使用文本作为图像聚类的抽象的问题。然而，当前的图像聚类方法忽视了生成的文本描述的使用。因此，我们提出了一种文本引导的图像聚类方法，即使用图像字幕和视觉问答（VQA）模型生成文本，然后对生成的文本进行聚类。此外，我们还介绍了一种通过提示VQA模型来注入任务或领域知识用于聚类的新方法。在八个不同的图像聚类数据集上，我们的结果表明，获得的文本表示通常优于图像特征。此外，我们提出了一种基于计数的聚类可解释性方法。我们的评估结果表明，基于关键词的解释比相应的聚类准确性更好地描述了聚类。总的来说，

    Image clustering divides a collection of images into meaningful groups, typically interpreted post-hoc via human-given annotations. Those are usually in the form of text, begging the question of using text as an abstraction for image clustering. Current image clustering methods, however, neglect the use of generated textual descriptions. We, therefore, propose Text-Guided Image Clustering, i.e., generating text using image captioning and visual question-answering (VQA) models and subsequently clustering the generated text. Further, we introduce a novel approach to inject task- or domain knowledge for clustering by prompting VQA models. Across eight diverse image clustering datasets, our results show that the obtained text representations often outperform image features. Additionally, we propose a counting-based cluster explainability method. Our evaluations show that the derived keyword-based explanations describe clusters better than the respective cluster accuracy suggests. Overall, 
    
[^286]: TopoX: 一个用于拓扑域上的机器学习的Python软件包套件

    TopoX: A Suite of Python Packages for Machine Learning on Topological Domains

    [https://arxiv.org/abs/2402.02441](https://arxiv.org/abs/2402.02441)

    TopoX是一个用于在拓扑域上进行机器学习的Python软件包套件，包含了构建、计算和嵌入拓扑域的功能，并提供了一套全面的高阶消息传递功能工具箱。

    

    我们介绍了topox，一个提供可靠且用户友好的Python软件包套件，用于在拓扑域（扩展了图的领域）上进行计算和机器学习：超图、单纯、胞腔、路径和组合复合体。topox由三个软件包组成：toponetx用于构建和计算这些域，包括节点、边和高阶单元的处理；topoembedx提供了将拓扑域嵌入到向量空间的方法，类似于流行的基于图的嵌入算法，如node2vec；topomodelx建立在PyTorch之上，为拓扑域上的神经网络提供了一套全面的高阶消息传递功能工具箱。topox的源代码经过广泛的文档化和单元测试，并在https://github.com/pyt-team以MIT许可证的形式提供。

    We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
    
[^287]: 通过优化抽象的方式进行Slate Bandit策略的离策略评估

    Off-Policy Evaluation of Slate Bandit Policies via Optimizing Abstraction

    [https://arxiv.org/abs/2402.02171](https://arxiv.org/abs/2402.02171)

    我们提出了一种名为潜在IPS（LIPS）的新的Slate Bandit OPE估计器，通过在低维度的Slate抽象空间中定义重要性权重，并通过数据驱动的方式优化Slate抽象来减小偏差和方差。

    

    我们研究了Slate上下文强盗问题中的离策略评估（OPE），其中一个策略选择称为slates的多维动作。这个问题在推荐系统、搜索引擎、营销以及医疗应用中广泛存在，然而，由于动作空间大，典型的逆倾向评分（IPS）估计器存在较大的方差，使得有效的OPE成为一个重大挑战。伪逆（PI）估计器已被引入以减小方差问题，通过假设奖励函数线性，但这可能导致显著的偏差，因为这个假设在观测数据中很难验证并且经常会被实质性违反。为了解决之前估计器的局限性，我们开发了一种新的Slate Bandit OPE估计器，称为潜在IPS（LIPS），它在低维度的Slate抽象空间中定义了重要性权重，我们通过数据驱动的方式优化Slate抽象来最小化LIPS的偏差和方差。

    We study off-policy evaluation (OPE) in the problem of slate contextual bandits where a policy selects multi-dimensional actions known as slates. This problem is widespread in recommender systems, search engines, marketing, to medical applications, however, the typical Inverse Propensity Scoring (IPS) estimator suffers from substantial variance due to large action spaces, making effective OPE a significant challenge. The PseudoInverse (PI) estimator has been introduced to mitigate the variance issue by assuming linearity in the reward function, but this can result in significant bias as this assumption is hard-to-verify from observed data and is often substantially violated. To address the limitations of previous estimators, we develop a novel estimator for OPE of slate bandits, called Latent IPS (LIPS), which defines importance weights in a low-dimensional slate abstraction space where we optimize slate abstractions to minimize the bias and variance of LIPS in a data-driven way. By do
    
[^288]: 机器学习生成代码的质量和信任

    Quality and Trust in LLM-generated Code

    [https://arxiv.org/abs/2402.02047](https://arxiv.org/abs/2402.02047)

    本论文研究了机器学习生成代码的质量和信任问题，提出了校准的重要性，并探讨了如何确定模型生成代码的正确性。

    

    机器学习模型广泛应用，但常常会出错。用户需要可靠的指示，以确定给定模型的输出是否可信，从而可以做出理性决策是否使用该输出。例如，可以将输出与置信度相关联；如果置信度与正确性的可能性强相关，则称该模型为良好校准。在这种情况下，高置信度的输出可以安全接受，低置信度的输出可以拒绝。校准迄今主要在非生成性（例如分类）环境中进行研究，特别是在软件工程领域。然而，生成代码很容易出错：开发人员需要知道何时直接使用、经过仔细审查后使用或丢弃模型生成的代码，因此在生成环境中，校准非常重要。然而，生成代码的正确性概念并不简单，因此校准也是如此。

    Machine learning models are widely used but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated. In this case, for example, high-confidence outputs could be safely accepted, and low-confidence outputs rejected.   Calibration has so far been studied in non-generative (e.g., classification) settings, especially in Software Engineering. However, generated code can quite often be wrong: Developers need to know when they should e.g., directly use, use after careful review, or discard model-generated code; thus Calibration is vital in generative settings. However, the notion of correctness of generated code is non-trivial, and thus so is Calibration. I
    
[^289]: 用于神经密度比估计的$\alpha$-散度损失函数

    $\alpha$-Divergence Loss Function for Neural Density Ratio Estimation

    [https://arxiv.org/abs/2402.02041](https://arxiv.org/abs/2402.02041)

    本文提出了一种应用于神经密度比估计的$\alpha$-散度损失函数($\alpha$-Div)，通过简洁实现和稳定优化解决了现有方法中存在的优化问题。实验证明了这种损失函数的稳定性，并提出了对DRE任务的估计准确性的研究，同时给出了样本要求的解决方案。

    

    最近，神经网络在机器学习中的基础技术密度比估计(DRE)方面取得了最先进的结果。然而，现有方法因DRE的损失函数而出现了优化问题：KL散度需要大样本，训练损失梯度消失，损失函数梯度有偏。因此，本文提出了一种提供简洁实现和稳定优化的$\alpha$-散度损失函数($\alpha$-Div)。此外，还给出了对所提出的损失函数的技术验证。实验证明了所提出的损失函数的稳定性，并研究了DRE任务的估计准确性。此外，本研究还提出了使用所提出的损失函数进行DRE的样本要求，以$L_1$误差的上界联系起来，该上界将高维度DRE任务中的维度诅咒作为一个共同问题。

    Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
    
[^290]: 跨越进化算法和强化学习：一项全面调查

    Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey

    [https://arxiv.org/abs/2401.11963](https://arxiv.org/abs/2401.11963)

    通过整合进化算法与强化学习，进化强化学习（ERL）展现出卓越的性能提升，本综述呈现了ERL领域的各个研究分支，突出了EA辅助RL的优化、RL辅助EA的优化以及EA和RL的协同优化这三个主要研究方向。

    

    进化强化学习（ERL）将进化算法（EAs）和强化学习（RL）相结合进行优化，表现出卓越的性能提升。通过融合两种方法的优势，ERL已经成为一个有前景的研究方向。本调查综述了ERL中不同研究分支的全面概述。具体而言，我们系统总结了相关算法的最新进展，并确定了三个主要研究方向：EA辅助RL的优化，RL辅助EA的优化，以及EA和RL的协同优化。随后，我们深入分析了每个研究方向，组织了多个研究分支。我们阐明了每个分支致力于解决的问题，以及EA和RL的整合如何应对这些挑战。最后，我们讨论了潜在的挑战和未来的研究方向。

    arXiv:2401.11963v2 Announce Type: replace-cross  Abstract: Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions
    
[^291]: LightDiC: 一种简单而有效的大规模有向图表示学习方法

    LightDiC: A Simple yet Effective Approach for Large-scale Digraph Representation Learning

    [https://arxiv.org/abs/2401.11772](https://arxiv.org/abs/2401.11772)

    LightDiC是基于磁性拉普拉斯的可扩展有向图卷积方法，通过在离线预处理中进行拓扑相关计算，实现了可扩展性，适用于大规模数据库。

    

    大多数现有的图神经网络（GNN）局限于无向图，其捕捉关系信息的范围受限制，限制了其表达能力和在现实场景中的部署。 相较于无向图，有向图（有向图）更适合建模更复杂的拓扑系统的需求，通过捕捉节点之间更复杂的关系，如制定交通和金融网络。 虽然已经提出了一些有向GNN，但它们的灵感主要来自深度学习架构，这导致了冗余的复杂性和计算量，使其无法应用于大规模数据库。 为解决这些问题，我们提出了一种基于磁性拉普拉斯的可扩展变种有向图卷积，LightDiC。 由于拓扑相关的计算仅在离线预处理过程中进行，LightDiC实现了出色的可扩展性，从而实现了向下的...

    arXiv:2401.11772v2 Announce Type: replace-cross  Abstract: Most existing graph neural networks (GNNs) are limited to undirected graphs, whose restricted scope of the captured relational information hinders their expressive capabilities and deployments in real-world scenarios. Compared with undirected graphs, directed graphs (digraphs) fit the demand for modeling more complex topological systems by capturing more intricate relationships between nodes, such as formulating transportation and financial networks. While some directed GNNs have been introduced, their inspiration mainly comes from deep learning architectures, which lead to redundant complexity and computation, making them inapplicable to large-scale databases. To address these issues, we propose LightDiC, a scalable variant of the digraph convolution based on the magnetic Laplacian. Since topology-related computations are conducted solely during offline pre-processing, LightDiC achieves exceptional scalability, enabling downst
    
[^292]: PRewrite: 使用强化学习的提示重写

    PRewrite: Prompt Rewriting with Reinforcement Learning

    [https://arxiv.org/abs/2401.08189](https://arxiv.org/abs/2401.08189)

    本文提出了一种基于强化学习的自动化工具PRewrite，用于重写提示草案并生成高效的新提示，以解决提示工程中的挑战。

    

    arXiv:2401.08189v2 公告类型: 替换 摘要: 提示工程对于基于LLM的应用程序的开发至关重要。然而，通常以“试错”的方式手动完成。这种手动程序可能耗时，效果不佳，并且在许多情况下生成的提示都是次优的。即使对那些看似运作良好的提示，始终存在一个悬而未决的问题：是否可以通过进一步修改使提示变得更好呢？为了解决这些问题，在本文中，我们研究了提示工程自动化。我们考虑了一个特定的使用情景，即开发者/用户已经起草了初始提示，但缺乏时间/专业知识来优化它们。我们提出了PRewrite，一个自动化工具，可重写这些草案，并生成高效的新提示。PRewrite基于强化学习（RL）框架，允许端到端优化，我们的设计允许RL搜索在大动作空间中进行。

    arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
    
[^293]: 结合机器学习和本体论：系统文献综述

    Combining Machine Learning and Ontology: A Systematic Literature Review

    [https://arxiv.org/abs/2401.07744](https://arxiv.org/abs/2401.07744)

    通过系统文献综述研究了将机器学习和本体整合的过程，发现了增强型本体学习、语义数据挖掘和学习与推理系统这三种主要的混合类别，比较了不同研究中使用的机器学习算法。

    

    受对将归纳推理和演绎推理结合的探索过程的渴望驱使，我们对调查机器学习和本体一体化的文章进行了系统文献综述。其目标是识别涵盖归纳推理（由机器学习执行）和演绎推理（由本体执行）的多种技术以整合到人工智能系统中。我们的综述包括对128项研究的分析，使我们能够确定机器学习和本体之间的三个主要混合类别：增强型本体学习、语义数据挖掘以及学习与推理系统。我们对所有这些类别进行了全面检查，强调了研究中使用的各种机器学习算法。此外，我们将我们的分类与领域内类似的近期工作以及混合人工智能和神经符号方法进行了比较。

    arXiv:2401.07744v2 Announce Type: replace  Abstract: Motivated by the desire to explore the process of combining inductive and deductive reasoning, we conducted a systematic literature review of articles that investigate the integration of machine learning and ontologies. The objective was to identify diverse techniques that incorporate both inductive reasoning (performed by machine learning) and deductive reasoning (performed by ontologies) into artificial intelligence systems. Our review, which included the analysis of 128 studies, allowed us to identify three main categories of hybridization between machine learning and ontologies: learning-enhanced ontologies, semantic data mining, and learning and reasoning systems. We provide a comprehensive examination of all these categories, emphasizing the various machine learning algorithms utilized in the studies. Furthermore, we compared our classification with similar recent work in the field of hybrid AI and neuro-symbolic approaches.
    
[^294]: 图语言模型

    Graph Language Models

    [https://arxiv.org/abs/2401.07105](https://arxiv.org/abs/2401.07105)

    引入了一种新型的图语言模型（GLM），结合线性化和图神经网络的优点，解决了传统方法在处理结构化知识图谱时的弱点。

    

    虽然语言模型（LMs）是自然语言处理的主力军，它们与结构化知识图谱（KGs）的相互作用仍在积极研究中。当前用于编码这些图形的方法通常要么（i）将它们线性化以供LM嵌入--这样会低效利用结构信息，要么（ii）使用图神经网络（GNNs）来保留图结构--但GNNs无法像预训练的LM一样很好地表示文本特征。在我们的工作中，我们引入了一种新型LM类型，即图语言模型（GLM），它整合了两种方法的优点并减轻了它们的弱点。GLM参数从预训练的LM中初始化，以增强对个别图概念和三元组的理解。同时，我们设计GLM的架构以整合图偏差，从而促进图内的知识分布。这使GLM能够处理图形、文本以及两者的交织输入。实证

    arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
    
[^295]: 两层ReLU网络中的隐藏极小值

    Hidden Minima in Two-Layer ReLU Networks

    [https://arxiv.org/abs/2312.16819](https://arxiv.org/abs/2312.16819)

    本文研究了两层ReLU网络中的隐藏极小值现象，并提出方法来研究这些隐藏极小值的独特解析性质。

    

    本文考虑拟合具有$d$个输入、$k$个神经元以及由目标网络生成的标签的两层ReLU网络所涉及的优化问题。最近发现了两种无穷族的虚假极小值，每个$d$对应一个极小值。属于第一类的极小值的损失在$d$增加时收敛于零。在第二类中，损失保持远离于零。那么，如何避免属于后一类的极小值呢？幸运的是，这样的极小值从不会被标准优化方法检测到。受到此现象性质的问题的启发，我们开发了研究隐藏极小值独特解析性质的方法。根据现有的分析，两种类型的Hessian谱在$O(d^{-1/2})$项模意义下一致 -- 不太乐观。因此，我们的研究通过研究损失被最小化或最大化的曲线进行，通常称为切线。

    arXiv:2312.16819v2 Announce Type: replace  Abstract: The optimization problem associated to fitting two-layer ReLU networks having $d$~inputs, $k$~neurons, and labels generated by a target network, is considered. Two types of infinite families of spurious minima, giving one minimum per $d$, were recently found. The loss at minima belonging to the first type converges to zero as $d$ increases. In the second type, the loss remains bounded away from zero. That being so, how may one avoid minima belonging to the latter type? Fortunately, such minima are never detected by standard optimization methods. Motivated by questions concerning the nature of this phenomenon, we develop methods to study distinctive analytic properties of hidden minima.   By existing analyses, the Hessian spectrum of both types agree modulo $O(d^{-1/2})$-terms -- not promising. Thus, rather, our investigation proceeds by studying curves along which the loss is minimized or maximized, generally referred to as tangency 
    
[^296]: 线性情境赌博问题的两全其美算法

    Best-of-Both-Worlds Algorithms for Linear Contextual Bandits

    [https://arxiv.org/abs/2312.15433](https://arxiv.org/abs/2312.15433)

    该论文研究了线性情境赌博问题的两全其美算法，实现了在对抗性和随机情况下接近最优的遗憾界，其中包括了针对最小次优差距的多对数级别速率和在对抗性情况下的第一阶或第二阶界以及基于Shannon熵正则项的FTRL算法。

    

    我们研究了针对$K$臂线性情境赌博问题的两全其美算法。我们的算法在对抗性和随机情况下均具有接近最优的遗憾界，而无需关于环境的先验知识。在随机情况下，我们实现了多对数级别的速率$\frac{(dK)^2\mathrm{poly}\log(dKT)}{\Delta_{\min}}$，其中$\Delta_{\min}$是$d$维情境空间中的最小次优差距。在对抗性情况下，我们获得了第一阶$\widetilde{O}(dK\sqrt{L^*})$界或者第二阶$\widetilde{O}(dK\sqrt{\Lambda^*})$界，其中$L^*$是最佳操作的累积损失，$\Lambda^*$是算法产生的损失的累积二次矩的一种概念。此外，我们基于带有Shannon熵正则项的FTRL算法开发了一种不需要知道协方差矩阵逆的算法，并实现了多对数遗憾

    arXiv:2312.15433v2 Announce Type: replace  Abstract: We study best-of-both-worlds algorithms for $K$-armed linear contextual bandits. Our algorithms deliver near-optimal regret bounds in both the adversarial and stochastic regimes, without prior knowledge about the environment. In the stochastic regime, we achieve the polylogarithmic rate $\frac{(dK)^2\mathrm{poly}\log(dKT)}{\Delta_{\min}}$, where $\Delta_{\min}$ is the minimum suboptimality gap over the $d$-dimensional context space. In the adversarial regime, we obtain either the first-order $\widetilde{O}(dK\sqrt{L^*})$ bound, or the second-order $\widetilde{O}(dK\sqrt{\Lambda^*})$ bound, where $L^*$ is the cumulative loss of the best action and $\Lambda^*$ is a notion of the cumulative second moment for the losses incurred by the algorithm. Moreover, we develop an algorithm based on FTRL with Shannon entropy regularizer that does not require the knowledge of the inverse of the covariance matrix, and achieves a polylogarithmic regre
    
[^297]: 从教师那里少或多：利用三边几何进行知识蒸馏

    Less or More From Teacher: Exploiting Trilateral Geometry For Knowledge Distillation

    [https://arxiv.org/abs/2312.15112](https://arxiv.org/abs/2312.15112)

    该研究提出了一种新颖的自适应方法，利用教师和学生的正确性以及学生对教师模仿程度来学习每个样本的知识融合比率，从而引入了样本内三边几何关系。

    

    知识蒸馏旨在使用来自较大教师网络的软监督和来自真实数据的硬监督来训练一个紧凑的学生网络。然而，确定一个平衡这些监督信号的最佳知识融合比率仍然具有挑战性。在本研究中，我们介绍了一种新颖的自适应方法，用于学习每个样本的知识融合比率，利用教师和学生的正确性，以及学生在每个样本上模仿教师的程度。我们的方法自然地导致了学生预测($S$)、教师预测($T$)和地面真相($G$)之间的样本内三边几何关系。为了抵消异常值的影响，我们进一步扩展到样本间的关系，将教师的全局平均预测$\bar{T}$纳入考虑。

    arXiv:2312.15112v3 Announce Type: replace  Abstract: Knowledge distillation aims to train a compact student network using soft supervision from a larger teacher network and hard supervision from ground truths. However, determining an optimal knowledge fusion ratio that balances these supervisory signals remains challenging. Prior methods generally resort to a constant or heuristic-based fusion ratio, which often falls short of a proper balance. In this study, we introduce a novel adaptive method for learning a sample-wise knowledge fusion ratio, exploiting both the correctness of teacher and student, as well as how well the student mimics the teacher on each sample. Our method naturally leads to the intra-sample trilateral geometric relations among the student prediction ($S$), teacher prediction ($T$), and ground truth ($G$). To counterbalance the impact of outliers, we further extend to the inter-sample relations, incorporating the teacher's global average prediction $\bar{T}$ for sa
    
[^298]: 从高阶统计量中高效学习：假设检验、随机特征和神经网络

    Learning from higher-order statistics, efficiently: hypothesis tests, random features, and neural networks

    [https://arxiv.org/abs/2312.14922](https://arxiv.org/abs/2312.14922)

    神经网络在高维数据中发现统计模式，研究了如何高效地从高阶累积量中提取特征，并探讨了在尖峰累积量模型中的统计和计算限制。

    

    神经网络擅长发现高维数据集中的统计模式。在实践中，度量三个或更多变量间的非高斯相关性的高阶累积量对神经网络的性能特别重要。但神经网络有多有效地从高阶累积量中提取特征？我们在尖峰累积量模型中探讨了这个问题，这里统计学家需要从$d$维输入的阶-$p\ge 4$累积量中恢复出一个特权方向或“尖峰”。我们首先通过分析所需样本数$n$来表征恢复尖峰的基本统计和计算限制，以强烈区分来自尖峰累积量模型和各向同性高斯输入的输入。我们发现，统计上的可区分性需要$n\gtrsim d$个样本，而在多项式时间内区分这两个分布则需要

    arXiv:2312.14922v2 Announce Type: replace-cross  Abstract: Neural networks excel at discovering statistical patterns in high-dimensional data sets. In practice, higher-order cumulants, which quantify the non-Gaussian correlations between three or more variables, are particularly important for the performance of neural networks. But how efficient are neural networks at extracting features from higher-order cumulants? We study this question in the spiked cumulant model, where the statistician needs to recover a privileged direction or "spike" from the order-$p\ge 4$ cumulants of $d$-dimensional inputs. We first characterise the fundamental statistical and computational limits of recovering the spike by analysing the number of samples $n$ required to strongly distinguish between inputs from the spiked cumulant model and isotropic Gaussian inputs. We find that statistical distinguishability requires $n\gtrsim d$ samples, while distinguishing the two distributions in polynomial time require
    
[^299]: 通过基于提示生成的方法在文学作品中进行说话者识别

    SIG: Speaker Identification in Literature via Prompt-Based Generation

    [https://arxiv.org/abs/2312.14590](https://arxiv.org/abs/2312.14590)

    通过基于生成的方法SIG，在文学作品中实现了说话者识别任务，支持跨领域评估和开放世界分类范式。

    

    在文学分析中，识别叙述中引用的发言者是一项重要任务，挑战性情景包括对看不见发言者的跨领域推断，以及周围环境中没有提到发言者的非明确情况。本文提出了一种简单且有效的方法SIG，这是一种基于生成的方法，根据设计的提示模板对任务和引语输入进行语言化处理，还可以轻松集成其他进一步增强说话者识别性能的辅助任务。预测可以来自模型的直接生成，也可以由每个发言者候选人的最高生成概率确定。基于我们的方法设计，SIG支持跨领域评估，并实现了能够接受任何候选输入形式的开放世界分类范式。我们在PDNC上进行了跨领域评估和内领域评估。

    arXiv:2312.14590v2 Announce Type: replace  Abstract: Identifying speakers of quotations in narratives is an important task in literary analysis, with challenging scenarios including the out-of-domain inference for unseen speakers, and non-explicit cases where there are no speaker mentions in surrounding context. In this work, we propose a simple and effective approach SIG, a generation-based method that verbalizes the task and quotation input based on designed prompt templates, which also enables easy integration of other auxiliary tasks that further bolster the speaker identification performance. The prediction can either come from direct generation by the model, or be determined by the highest generation probability of each speaker candidate. Based on our approach design, SIG supports out-of-domain evaluation, and achieves open-world classification paradigm that is able to accept any forms of candidate input. We perform both cross-domain evaluation and in-domain evaluation on PDNC, t
    
[^300]: 双重扰动任务自由的持续学习

    Doubly Perturbed Task Free Continual Learning

    [https://arxiv.org/abs/2312.13027](https://arxiv.org/abs/2312.13027)

    提出了一种新颖的任务自由持续学习框架，在输入数据和决策制定上注入敌对扰动，提高了未来样本考虑的效果。

    

    Task Free online continual learning (TF-CL)是一个具有挑战性的问题，模型在没有显式任务信息的情况下逐步学习任务。尽管使用来自过去、现在以及未来的所有数据进行训练被认为是黄金标准，但在TF-CL中使用当前样本的幼稚方法可能会与未来样本的学习发生冲突，导致灾难性遗忘和可塑性不佳。因此，在TF-CL中积极考虑未来样本变得至关重要。受到这种直觉的启发，我们提出了一个考虑未来样本的新颖TF-CL框架，并展示在输入数据和决策制定上注入敌对扰动是有效的。然后，我们提出了一种名为Doubly Perturbed Continual Learning (DPCL)的新方法，以有效地实施这些输入和决策扰动。

    arXiv:2312.13027v2 Announce Type: replace  Abstract: Task Free online continual learning (TF-CL) is a challenging problem where the model incrementally learns tasks without explicit task information. Although training with entire data from the past, present as well as future is considered as the gold standard, naive approaches in TF-CL with the current samples may be conflicted with learning with samples in the future, leading to catastrophic forgetting and poor plasticity. Thus, a proactive consideration of an unseen future sample in TF-CL becomes imperative. Motivated by this intuition, we propose a novel TF-CL framework considering future samples and show that injecting adversarial perturbations on both input data and decision-making is effective. Then, we propose a novel method named Doubly Perturbed Continual Learning (DPCL) to efficiently implement these input and decision-making perturbations. Specifically, for input perturbation, we propose an approximate perturbation method th
    
[^301]: PPO-Clip实现全局最优性：更深入理解修剪技术

    PPO-Clip Attains Global Optimality: Towards Deeper Understandings of Clipping

    [https://arxiv.org/abs/2312.12065](https://arxiv.org/abs/2312.12065)

    该论文在PPO-Clip算法方面做出贡献，建立了其在表格和神经函数逼近设置中的全局收敛结果，特别突出了在神经函数逼近情境下的$O(1/\sqrt{T})$最小迭代收敛速率。

    

    在这篇论文中，我们首次建立了PPO-Clip变体在表格和神经函数逼近设置中具有全局收敛性结果。我们的发现特别突出了在神经函数逼近情境下的$O(1/\sqrt{T})$最小迭代收敛速率。通过引入PPO-Clip的广义版本，结合其与铰链损失的关系，采用熵镜像下降，我们为直接策略参数化的表格PPO-Clip建立了渐近收敛。受表格分析启发，

    arXiv:2312.12065v2 Announce Type: replace-cross  Abstract: Proximal Policy Optimization algorithm employing a clipped surrogate objective (PPO-Clip) is a prominent exemplar of the policy optimization methods. However, despite its remarkable empirical success, PPO-Clip lacks theoretical substantiation to date. In this paper, we contribute to the field by establishing the first global convergence results of a PPO-Clip variant in both tabular and neural function approximation settings. Our findings highlight the $O(1/\sqrt{T})$ min-iterate convergence rate specifically in the context of neural function approximation. We tackle the inherent challenges in analyzing PPO-Clip through three central concepts: (i) We introduce a generalized version of the PPO-Clip objective, illuminated by its connection with the hinge loss. (ii) Employing entropic mirror descent, we establish asymptotic convergence for tabular PPO-Clip with direct policy parameterization. (iii) Inspired by the tabular analysis,
    
[^302]: 超越自注意力的序列推荐中的关注归纳偏差

    An Attentive Inductive Bias for Sequential Recommendation beyond the Self-Attention

    [https://arxiv.org/abs/2312.10325](https://arxiv.org/abs/2312.10325)

    提出了一种名为BSARec的新方法，超越了自注意力，在序列推荐中注入了归纳偏差，并集成了低频和高频信息以减轻过度平滑问题

    

    基于Transformer的序列推荐（SR）模型取得了显著的成功。 Transformer的自注意机制在计算机视觉和自然语言处理中遇到了过度平滑问题，即隐藏表示变得类似于标记。 在SR领域，我们首次展示了相同问题的发生。 我们进行了开创性的研究，揭示了自注意在SR中的低通滤波特性，导致了过度平滑。 为此，我们提出了一种名为$\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation（BSARec）的新方法，利用傅里叶变换来 i）通过考虑细粒度的序列模式注入归纳偏差和 ii）集成低频和高频信息以减轻过度平滑。 我们的发现在SR领域显示了显著的进展，并有望搭起

    arXiv:2312.10325v2 Announce Type: replace-cross  Abstract: Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called $\textbf{B}$eyond $\textbf{S}$elf-$\textbf{A}$ttention for Sequential $\textbf{Rec}$ommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge t
    
[^303]: 具有条件建模的粒子加速器健壮的错误束预测

    Robust Errant Beam Prognostics with Conditional Modeling for Particle Accelerators

    [https://arxiv.org/abs/2312.10040](https://arxiv.org/abs/2312.10040)

    应用条件建模的方法，结合Siamese神经网络模型进行监督式异常检测，提高了粒子加速器错误束预测的健壮性和总体可用性。

    

    粒子加速器复杂且由成千上万个组件组成，许多设备运行在其峰值功率下。因此，粒子加速器可能因为多种原因出现故障并中止运行。这些故障会影响计划运行时粒子加速器的可用性，降低效率和整体科学产出。为了避免这些故障，我们应用异常检测技术来预测任何不寻常的行为，并采取预防性措施来提高粒子加速器的总体可用性。基于半监督机器学习（ML）的异常检测方法，如自编码器和变分自编码器，通常用于这些任务。然而，监督式ML技术，如Siamese神经网络（SNN）模型，可以通过利用标签信息优于无监督或半监督方法进行异常检测。其中一个特定的挑战是异常检测过程中的标记数据的不足。

    arXiv:2312.10040v2 Announce Type: replace-cross  Abstract: Particle accelerators are complex and comprise thousands of components, with many pieces of equipment running at their peak power. Consequently, particle accelerators can fault and abort operations for numerous reasons. These faults impact the availability of particle accelerators during scheduled run-time and hamper the efficiency and the overall science output. To avoid these faults, we apply anomaly detection techniques to predict any unusual behavior and perform preemptive actions to improve the total availability of particle accelerators. Semi-supervised Machine Learning (ML) based anomaly detection approaches such as autoencoders and variational autoencoders are often used for such tasks. However, supervised ML techniques such as Siamese Neural Network (SNN) models can outperform unsupervised or semi-supervised approaches for anomaly detection by leveraging the label information. One of the challenges specific to anomaly 
    
[^304]: 一种具有条件扩散建模框架的应用于蛋白设计中的基序支架

    A framework for conditional diffusion modelling with applications in motif scaffolding for protein design

    [https://arxiv.org/abs/2312.09236](https://arxiv.org/abs/2312.09236)

    该论文提出一种统一的条件扩散建模框架，基于Doob's h-transform，用于解决蛋白设计中的基序支架问题

    

    许多蛋白设计应用，如结合物或酶的设计，需要以高精度搭建具有结构基序的蛋白质。基于去噪扩散过程的生成建模范式已成为解决这一基序支架问题的主要候选方案，并在某些情况下显示出早期实验成功。在扩散范式中，基序支架被视为一种条件生成任务，并提出了几种条件生成协议或从计算机视觉文献中导入。然而，这些协议大多基于启发性动机，例如通过对朗之万动力学的类比，并缺乏统一的框架，使得不同方法之间的联系变得模糊。在这项工作中，我们在一个基于数学上理解良好的Doob's h-transform的共同框架下统一了条件训练和条件抽样程序。这种新的视角使我们能够在不同方法之间建立联系

    arXiv:2312.09236v2 Announce Type: replace  Abstract: Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between ex
    
[^305]: Math-Shepherd: 在不需要人工标注的情况下逐步验证和加强LLMs

    Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations

    [https://arxiv.org/abs/2312.08935](https://arxiv.org/abs/2312.08935)

    Math-Shepherd提出了一种新的数学奖励模型，通过自动生成的过程监督数据实现LLMs的逐步验证和加强，显著提高了数学问题解决的准确性。

    

    在这篇论文中，我们提出了一种名为Math-Shepherd的创新过程导向数学奖励模型，为数学问题解决的每一步分配奖励分数。Math-Shepherd的训练是使用自动构建的基于过程的监督数据完成的，打破了现有工作中对手动标注的严重依赖瓶颈。我们探讨了Math-Shepherd在两种场景中的有效性：1）\textit{验证}：利用Math-Shepherd对大型语言模型(LLMs)生成的多个输出进行重新排序；2）\textit{强化学习}：使用Math-Shepherd通过逐步的近端策略优化(PPO)加强LLMs。通过Math-Shepherd，一系列开源LLMs展现出卓越的性能。例如，使用Math-Shepherd的逐步PPO显著提高了Mistral-7B的准确率(GSM8K由77.9%提高到84.1%，MATH由28.6%提高到33.0%)

    arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K and 28.6\%$\to$33.0\% on MATH). The
    
[^306]: 通过无数据网格移动器实现更好的神经PDE求解器

    Better Neural PDE Solvers Through Data-Free Mesh Movers

    [https://arxiv.org/abs/2312.05583](https://arxiv.org/abs/2312.05583)

    提出了一种神经PDE求解器，通过无数据神经网格适配器（DMM）解决了神经PDE求解器需要昂贵网格数据和解决空间的自由度和拓扑结构变化的挑战。

    

    最近，神经网络已被广泛应用于解决物理系统建模中的偏微分方程（PDEs）。虽然主要研究集中在学习预定义静态网格离散化上的系统演化，但一些方法利用强化学习或监督学习技术来创建适应性和动态网格，由于这些系统的动态特性。然而，这些方法面临两个主要挑战：（1）需要昂贵的最佳网格数据，和（2）在网格细化过程中解决空间的自由度和拓扑结构的变化。为了解决这些挑战，本文提出了一种带有神经网格适配器的神经PDE求解器。首先，我们介绍了一种新颖的无数据神经网格适配器，称为Data-free Mesh Mover（DMM），具有两个主要创新点。首先，它是一个操作符，将解映射到自适应网格上，并使用Monge-Amp\`ere方程进行训练。

    arXiv:2312.05583v2 Announce Type: replace-cross  Abstract: Recently, neural networks have been extensively employed to solve partial differential equations (PDEs) in physical system modeling. While major studies focus on learning system evolution on predefined static mesh discretizations, some methods utilize reinforcement learning or supervised learning techniques to create adaptive and dynamic meshes, due to the dynamic nature of these systems. However, these approaches face two primary challenges: (1) the need for expensive optimal mesh data, and (2) the change of the solution space's degree of freedom and topology during mesh refinement. To address these challenges, this paper proposes a neural PDE solver with a neural mesh adapter. To begin with, we introduce a novel data-free neural mesh adaptor, called Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an operator that maps the solution to adaptive meshes and is trained using the Monge-Amp\`ere equation withou
    
[^307]: AI模型是否比基于物理的模型能够提供更好的天气预报？对风暴Ciarán的定量评估案例研究

    Do AI models produce better weather forecasts than physics-based models? A quantitative evaluation case study of Storm Ciar\'an

    [https://arxiv.org/abs/2312.02658](https://arxiv.org/abs/2312.02658)

    本研究对比了机器学习和数值天气预报模型对风暴Ciarán的预测，发现机器学习模型可以准确模拟高影响天气事件的大尺度结构和动力驱动器。

    

    近年来，使用机器学习技术进行操作性天气预报的潜力备受关注。随着它们成为天气预报工具箱的一部分，迫切需要了解当前的机器学习模型能够多好地模拟高影响天气事件。本研究比较了由机器学习和数值天气预报模型制作的风暴Ciarán的预报，这是一场引发北欧16人死亡并造成广泛破坏的欧洲风暴。考虑的四个机器学习模型（FourCastNet，Pangu-Weather，GraphCast和FourCastNet-v2）产生的预报能够准确捕捉气旋的大尺度结构，包括云头位置、暖区形状和暖气围带喷流位置，以及对快速风暴发展重要的大尺度动力驱动器，例如暴风相对于高层喷流出口的位置。

    arXiv:2312.02658v2 Announce Type: replace  Abstract: There has been huge recent interest in the potential of making operational weather forecasts using machine learning techniques. As they become a part of the weather forecasting toolbox, there is a pressing need to understand how well current machine learning models can simulate high-impact weather events. We compare forecasts of Storm Ciar\'an, a European windstorm that caused sixteen deaths and extensive damage in Northern Europe, made by machine learning and numerical weather prediction models. The four machine learning models considered (FourCastNet, Pangu-Weather, GraphCast and FourCastNet-v2) produce forecasts that accurately capture the synoptic-scale structure of the cyclone including the position of the cloud head, shape of the warm sector and location of warm conveyor belt jet, and the large-scale dynamical drivers important for the rapid storm development such as the position of the storm relative to the upper-level jet exi
    
[^308]: 你需要多少验证标签？探索标签高效模型排名的设计空间

    How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking

    [https://arxiv.org/abs/2312.01619](https://arxiv.org/abs/2312.01619)

    LEMR框架通过在未标记的验证集中策略性地标注实例，显著降低了模型选择中的标注成本，并在各种NLP任务中展现了其高效性。

    

    本文提出了LEMR（标签高效模型排名）并介绍了MoraBench基准测试。LEMR是一个新颖的框架，通过从未标记的验证集中策略性地标注实例，最大程度地减少了在模型选择中昂贵注释的需求。为了评估LEMR，我们利用了MoraBench基准测试，这是一个涵盖了多种场景的模型输出的综合收集。我们在半监督学习、弱监督和提示选择任务的23个不同自然语言处理任务上进行了广泛评估，结果表明LEMR在显著降低标注成本方面的有效性。关键发现突出了合适的集成方法、不确定性采样策略和模型委员会选择对提高模型排名准确性的影响。LEMR结合了MoraBench的见解，为模型选择提供了一种成本效益高且准确的解决方案，特别适用于资源受限环境。

    arXiv:2312.01619v3 Announce Type: replace  Abstract: This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark. LEMR is a novel framework that minimizes the need for costly annotations in model selection by strategically annotating instances from an unlabeled validation set. To evaluate LEMR, we leverage the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios. Our extensive evaluation across 23 different NLP tasks in semi-supervised learning, weak supervision, and prompt selection tasks demonstrates LEMR's effectiveness in significantly reducing labeling costs. Key findings highlight the impact of suitable ensemble methods, uncertainty sampling strategies, and model committee selection in enhancing model ranking accuracy. LEMR, supported by the insights from MoraBench, provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments.
    
[^309]: 曲率方向作为失去可塑性的解释

    Directions of Curvature as an Explanation for Loss of Plasticity

    [https://arxiv.org/abs/2312.00246](https://arxiv.org/abs/2312.00246)

    曲率方向的丧失被认为是导致神经网络可塑性丧失的一个重要原因，并且我们通过系统调查和在多个任务中的研究结果支持了这一观点。

    

    可塑性的丧失是神经网络丧失从新经验学习能力的现象。尽管在几种问题设置中经验上观察到，但对导致可塑性丧失的机制了解甚少。在本文中，我们提供了对可塑性丧失的一致解释：神经网络在训练过程中丧失了曲率方向，可将可塑性的丧失归因于这种曲率减少。为了支持这样的说法，我们对在MNIST、CIFAR-10和ImageNet中使用的不断学习任务中的可塑性丧失进行了系统调查。我们的研究结果表明，曲率方向的丧失与可塑性的丧失相吻合，同时还表明以前的解释不足以解释所有情况下的可塑性丧失。最后，我们展示了缓解可塑性丧失的正则化器也会保留曲率，促使采用简单的分布式正则化器。

    arXiv:2312.00246v2 Announce Type: replace  Abstract: Loss of plasticity is a phenomenon in which neural networks lose their ability to learn from new experience. Despite being empirically observed in several problem settings, little is understood about the mechanisms that lead to loss of plasticity. In this paper, we offer a consistent explanation for loss of plasticity: Neural networks lose directions of curvature during training and that loss of plasticity can be attributed to this reduction in curvature. To support such a claim, we provide a systematic investigation of loss of plasticity across continual learning tasks using MNIST, CIFAR-10 and ImageNet. Our findings illustrate that loss of curvature directions coincides with loss of plasticity, while also showing that previous explanations are insufficient to explain loss of plasticity in all settings. Lastly, we show that regularizers which mitigate loss of plasticity also preserve curvature, motivating a simple distributional reg
    
[^310]: 通过熵率最小化实现可预测的强化学习动态

    Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization

    [https://arxiv.org/abs/2311.18703](https://arxiv.org/abs/2311.18703)

    该论文提出了一种名为PA-RL的方法，通过最小化熵率来引导强化学习智能体展现可预测的行为。研究展示了如何利用平均替代奖励实现确定性策略，并在动态模型的基础上近似计算值函数。

    

    在强化学习中，智能体没有动机展示可预测的行为，通常通过策略熵正则化推动智能体在探索上随机化其行为。从人的角度来看，这使得强化学习智能体很难解释和预测；从安全角度来看，更难以进行形式化验证。我们提出了一种新的方法，称为可预测性感知强化学习（PA-RL），用于引导智能体展现可预测的行为，其利用状态序列熵率作为可预测性度量。我们展示了如何将熵率制定为平均奖励目标，并且由于其熵奖励函数依赖于策略，我们引入了一个动作相关的替代熵，以利用PG方法。我们证明了最小化平均替代奖励的确定性策略存在，并且最小化了实际熵率。我们还展示了如何在学习到的动态模型的基础上近似计算与值函数。

    In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
    
[^311]: 用于非结构稀疏恢复的特征矩阵

    Eigenmatrix for unstructured sparse recovery

    [https://arxiv.org/abs/2311.16609](https://arxiv.org/abs/2311.16609)

    本文提出了一种名为特征矩阵的数据驱动构造，用于解决非结构稀疏恢复问题，对于样本值中的噪声和样本位置的非结构性质具有很好的适应性。

    

    本文考虑了一般形式的非结构稀疏恢复问题，包括有理逼近、谱函数估计、傅里叶反演、拉普拉斯反演和稀疏反卷积等。主要挑战是样本值中的噪声和样本位置的非结构性质。本文提出了特征矩阵，一种具有所需近似特征值和特征向量的数据驱动构造，为这些稀疏恢复问题提供了一种新的方法。数值结果证明了所提方法的效率。

    This paper considers the unstructured sparse recovery problems in a general form. Examples include rational approximation, spectral function estimation, Fourier inversion, Laplace inversion, and sparse deconvolution. The main challenges are the noise in the sample values and the unstructured nature of the sample locations. This paper proposes the eigenmatrix, a data-driven construction with desired approximate eigenvalues and eigenvectors. The eigenmatrix offers a new way for these sparse recovery problems. Numerical results are provided to demonstrate the efficiency of the proposed method.
    
[^312]: 一般框架用于用户引导的贝叶斯优化

    A General Framework for User-Guided Bayesian Optimization

    [https://arxiv.org/abs/2311.14645](https://arxiv.org/abs/2311.14645)

    ColaBO是第一个贝叶斯原理框架，允许领域专家定制优化程序，整合先验信念以加速优化。

    

    昂贵的黑盒函数优化在各种科学学科中普遍存在。贝叶斯优化是一种自动、通用且样本高效的方法，可以在最小了解基础函数动态的情况下解决这些问题。然而，贝叶斯优化能够整合关于待优化函数的先验知识或信念以加速优化的能力有限，这降低了对具有预算紧迫知识渊博的实践者的吸引力。为了允许领域专家定制优化程序，我们提出了ColaBO，这是第一个贝叶斯原理框架，用于整合超出典型核结构的先验信念，如优化器的可能位置或最佳值。ColaBO的通用性使其适用于不同蒙特卡洛收获函数和用户信念的类型。我们经验性地展示了ColaBO的能力，

    arXiv:2311.14645v2 Announce Type: replace  Abstract: The optimization of expensive-to-evaluate black-box functions is prevalent in various scientific disciplines. Bayesian optimization is an automatic, general and sample-efficient method to solve these problems with minimal knowledge of the underlying function dynamics. However, the ability of Bayesian optimization to incorporate prior knowledge or beliefs about the function at hand in order to accelerate the optimization is limited, which reduces its appeal for knowledgeable practitioners with tight budgets. To allow domain experts to customize the optimization routine, we propose ColaBO, the first Bayesian-principled framework for incorporating prior beliefs beyond the typical kernel structure, such as the likely location of the optimizer or the optimal value. The generality of ColaBO makes it applicable across different Monte Carlo acquisition functions and types of user beliefs. We empirically demonstrate ColaBO's ability to substa
    
[^313]: 关于基于扩散的生成模型及其误差界限：完全收敛估计下的对数凹情况

    On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates

    [https://arxiv.org/abs/2311.13584](https://arxiv.org/abs/2311.13584)

    我们提出了对于基于扩散的生成模型在强对数凹数据分布假设下的完整收敛理论保证，获得了对于参数估计和采样算法的最优上限估计。

    

    我们在强对数凹数据分布的假设下为基于扩散的生成模型的收敛行为提供了完整的理论保证，而我们用于得分估计的逼近函数类由Lipschitz连续函数组成。我们通过一个激励性例子展示了我们方法的强大之处，即从具有未知均值的高斯分布中进行采样。在这种情况下，我们对相关的优化问题，即得分估计，提供了明确的估计，同时将其与相应的采样估计结合起来。因此，我们获得了最好的已知上限估计，涉及关键感兴趣的数量，如数据分布（具有未知均值的高斯分布）与我们的采样算法之间的Wasserstein-2距离的维度和收敛速率。

    arXiv:2311.13584v2 Announce Type: replace  Abstract: We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm.   Beyond the motivating example and in order to allow for the use of a diverse range o
    
[^314]: LLMs作为视觉解释器：通过不断演进的视觉描述提升图像分类

    LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions

    [https://arxiv.org/abs/2311.11904](https://arxiv.org/abs/2311.11904)

    提出了一种集成LLMs和VLMs的框架，以找到最佳的类别描述符，解决了图像分类中在精确构建文本表示和区分相似类别方面的挑战

    

    视觉语言模型（VLMs）通过比较图像与类别嵌入之间的相似性，为图像分类提供了一种有前途的范式。一个关键的挑战在于为类别名称构建精确的文本表示。本文提出了一种集成LLMs和VLMs的新框架，以找到最佳的类别描述符。

    arXiv:2311.11904v2 Announce Type: replace-cross  Abstract: Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We attribute this to two primary factors: 1) the reliance on single-turn textual interactions with LLMs, leading to a mismatch between generated text and visual concepts for VLMs; 2) the oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. In this paper, we propose a novel framework that integrates LLMs and VLMs to find the optimal class descriptors. Our training-free approach develops an LLM-based agent with an evolutionary optimization strategy to ite
    
[^315]: 基于困惑度量和上下文信息的令牌级对抗提示检测

    Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information

    [https://arxiv.org/abs/2311.11509](https://arxiv.org/abs/2311.11509)

    该论文引入了一种新的方法，在令牌级别检测对抗性提示，利用语言模型的能力预测下一个标记的概率。

    

    近年来，大型语言模型(LLM)已成为各种应用中的关键工具。然而，这些模型容易受到对抗性提示攻击，攻击者可以精心策划输入字符串，误导LLM生成不正确或不希望的输出。先前的研究揭示了利用离散优化的相对简单却有效的攻击方式可以生成绕过模型的调整和对齐的对抗性提示。对对抗性提示的脆弱性凸显了对LLM健壮性和可靠性的重要关注。我们的工作旨在通过引入一种新颖方法，在令牌级别检测对抗性提示，利用LLM预测下一个标记的概率能力。我们测量模型困惑度的程度，其中高概率预测的令牌被视为正常，而那些表现异常的则可能是对抗性提示。

    arXiv:2311.11509v3 Announce Type: replace  Abstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibi
    
[^316]: 使用可屏蔽股票表示的强化学习在可定制股票池中进行投资组合管理

    Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools

    [https://arxiv.org/abs/2311.10801](https://arxiv.org/abs/2311.10801)

    使用EarnMore方法，我们提出了一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。

    

    投资组合管理（PM）是一项基本的金融交易任务，探索定期将资金重新配置到不同股票中以追求长期利润。最近，强化学习（RL）显示出其潜力，通过与金融市场互动来训练具有盈利能力的PM代理。但是，现有工作主要集中在固定股票池上，这与投资者的实际需求不一致。为应对这一挑战，我们提出EarnMore，一种新的RL方法，可以允许RL代理与可定制股票池（CSPs）交互，而不需要重新训练。

    arXiv:2311.10801v3 Announce Type: replace-cross  Abstract: Portfolio management (PM) is a fundamental financial trading task, which explores the optimal periodical reallocation of capitals into different stocks to pursue long-term profits. Reinforcement learning (RL) has recently shown its potential to train profitable agents for PM through interacting with financial markets. However, existing work mostly focuses on fixed stock pools, which is inconsistent with investors' practical demand. Specifically, the target stock pool of different investors varies dramatically due to their discrepancy on market states and individual investors may temporally adjust stocks they desire to trade (e.g., adding one popular stocks), which lead to customizable stock pools (CSPs). Existing RL methods require to retrain RL agents even with a tiny change of the stock pool, which leads to high computational cost and unstable performance. To tackle this challenge, we propose EarnMore, a rEinforcement leARNin
    
[^317]: 利用函数空间聚合进行大规模联邦学习

    Leveraging Function Space Aggregation for Federated Learning at Scale

    [https://arxiv.org/abs/2311.10291](https://arxiv.org/abs/2311.10291)

    提出一种新算法FedFish，通过利用客户端学习函数的局部近似，并使用基于费舍尔信息的估计，实现了对联邦学习中的函数空间聚合，并在大规模交叉设备基准测试中表现出更好的鲁棒性和性能。

    

    联邦学习范式激发了将多个客户端更新聚合到全局服务器模型中的方法的发展，而无需共享客户端数据。许多联邦学习算法，包括经典的联邦平均（FedAvg），采用了对客户端参数更新的直接（可能加权）平均值，这是基于分布式优化结果的动机。在这项工作中，我们采用了函数空间角度，并提出了一种新算法FedFish，它聚合了客户端学习到的函数的局部近似，使用基于费舍尔信息的估计。我们在实际的大规模跨设备基准上评估了FedFish。虽然当客户端模型漂离时FedAvg的性能可能会受到影响，但我们证明FedFish对更长的局部训练更具鲁棒性。我们对图像和语言基准测试中的几个设置进行评估，结果显示FedFish在局部训练中优于FedAvg。

    arXiv:2311.10291v2 Announce Type: replace  Abstract: The federated learning paradigm has motivated the development of methods for aggregating multiple client updates into a global server model, without sharing client data. Many federated learning algorithms, including the canonical Federated Averaging (FedAvg), take a direct (possibly weighted) average of the client parameter updates, motivated by results in distributed optimization. In this work, we adopt a function space perspective and propose a new algorithm, FedFish, that aggregates local approximations to the functions learned by clients, using an estimate based on their Fisher information. We evaluate FedFish on realistic, large-scale cross-device benchmarks. While the performance of FedAvg can suffer as client models drift further apart, we demonstrate that FedFish is more robust to longer local training. Our evaluation across several settings in image and language benchmarks shows that FedFish outperforms FedAvg as local train
    
[^318]: 无需任务特定知识的自监督课程生成用于自主强化学习

    Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge

    [https://arxiv.org/abs/2311.09195](https://arxiv.org/abs/2311.09195)

    提出一种新颖的自主强化学习算法，无需任务特定知识即可生成适应智能体学习进展的课程

    

    在将当前强化学习算法应用于现实场景时，一个重要瓶颈是需要在每个回合之间重置环境。重置过程需要大量人工干预，使得智能体难以连续和自主学习。本文提出一种新颖的自主强化学习算法，可以生成适应智能体学习进展的课程，而无需任务特定知识。

    arXiv:2311.09195v2 Announce Type: replace  Abstract: A significant bottleneck in applying current reinforcement learning algorithms to real-world scenarios is the need to reset the environment between every episode. This reset process demands substantial human intervention, making it difficult for the agent to learn continuously and autonomously. Several recent works have introduced autonomous reinforcement learning (ARL) algorithms that generate curricula for jointly training reset and forward policies. While their curricula can reduce the number of required manual resets by taking into account the agent's learning progress, they rely on task-specific knowledge, such as predefined initial states or reset reward functions. In this paper, we propose a novel ARL algorithm that can generate a curriculum adaptive to the agent's learning progress without task-specific knowledge. Our curriculum empowers the agent to autonomously reset to diverse and informative initial states. To achieve thi
    
[^319]: 对抗偏好优化

    Adversarial Preference Optimization

    [https://arxiv.org/abs/2311.08045](https://arxiv.org/abs/2311.08045)

    提出了一种对抗偏好优化（APO）框架，实现了在没有额外注释的情况下，通过对抗学习自适应于生成分布差距。

    

    人类偏好调整是提高大型语言模型（LLMs）交互质量的关键。现有的对齐方法依赖于手动注释的偏好数据来指导LLM的优化方向。然而，在实践中，持续更新LLMs会导致模型生成样本与人类首选响应之间存在分布差距，这阻碍了模型微调的效率。为了缓解这个问题，先前的方法需要在生成的样本上额外进行偏好注释，以适应转移分布，这需要大量的注释资源。针对更高效的人类偏好优化，我们提出了一种对抗偏好优化（APO）框架，其中LLM代理和偏好模型通过极小-极大博弈交替更新。在没有额外注释的情况下，我们的APO方法可以通过对抗学习自适应于生成分布差距。

    arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
    
[^320]: ResMGCN：用于快速生物医学相互作用发现的残差消息图卷积网络

    ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering

    [https://arxiv.org/abs/2311.07632](https://arxiv.org/abs/2311.07632)

    提出了一种新颖的Residual Message Graph Convolution Network（ResMGCN），用于快速而准确地预测生物医学相互作用。

    

    生物医学信息图对于现代生物医学领域中的生物医学信息相互作用的发现至关重要，比如多样化分子相互作用的识别和药物发现，在生物医学、生物信息学和人类健康社区中引起了越来越多的关注。目前，越来越多的图神经网络被提出来学习生物医学信息实体，并通过最先进的结果准确揭示生物医学分子相互作用。然而，这些方法虽然弥补了远距离特征的衰减，但却以冗余的内存和时间为代价。在我们的论文中，我们提出了一种新颖的Residual Message Graph Convolution Network（ResMGCN），用于以一种不同的思路快速而准确地预测生物医学相互作用。具体来说，ResMGCN不是增强远程节点的消息，而是与下一轮高阶信息相结合。

    arXiv:2311.07632v2 Announce Type: replace-cross  Abstract: Biomedical information graphs are crucial for interaction discovering of biomedical information in modern age, such as identification of multifarious molecular interactions and drug discovery, which attracts increasing interests in biomedicine, bioinformatics, and human healthcare communities. Nowadays, more and more graph neural networks have been proposed to learn the entities of biomedical information and precisely reveal biomedical molecule interactions with state-of-the-art results. These methods remedy the fading of features from a far distance but suffer from remedying such problem at the expensive cost of redundant memory and time. In our paper, we propose a novel Residual Message Graph Convolution Network (ResMGCN) for fast and precise biomedical interaction prediction in a different idea. Specifically, instead of enhancing the message from far nodes, ResMGCN aggregates lower-order information with the next round highe
    
[^321]: 特征通过边界最大化的出现：代数任务案例研究

    Feature emergence via margin maximization: case studies in algebraic tasks

    [https://arxiv.org/abs/2311.07568](https://arxiv.org/abs/2311.07568)

    本文研究了神经网络在代数任务中学到的特征，发现边界最大化原则可以完全指定网络学到的特征。

    

    理解神经网络学习的内部表示是机器学习科学中的一个基石性挑战。虽然在某些情况下近期已经取得了重大进展，以了解神经网络如何实现特定的目标函数，但本文探讨了一个互补的问题——网络为何会采用特定的计算策略？我们的研究重点放在模块化加法、稀疏奇偶性和有限群操作的代数学习任务上。我们的主要理论发现通过分析的方法对这些代数任务的神经网络学到的特征进行了表征。值得注意的是，我们的主要技术展示了边界最大化原则如何单独用于完全指定网络学到的特征。具体来说，我们证明训练后的网络利用傅里叶特征执行模块化加法，并使用与不可约 gr

    arXiv:2311.07568v2 Announce Type: replace  Abstract: Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding how neural networks implement specific target functions, this paper explores a complementary question -- why do networks arrive at particular computational strategies? Our inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. Specifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible gr
    
[^322]: 通过正常结构规范化实现开放图异常检测

    Open-Set Graph Anomaly Detection via Normal Structure Regularisation

    [https://arxiv.org/abs/2311.06835](https://arxiv.org/abs/2311.06835)

    通过正常结构规范化方法，实现开放图异常检测模型对未知异常的广义检测能力

    

    本文考虑了一个重要的图异常检测（GAD）任务，即开放式GAD，旨在使用少量标记的训练正常节点和异常节点（称为已知异常）来检测异常节点，这些节点无法展示所有可能的推理时异常。已标记数据的可用性为GAD模型提供了关键的异常先验知识，可大大降低检测错误。然而，当前方法往往过分强调拟合已知异常，导致对未知异常（即未被标记的异常节点）的弱泛化能力。此外，它们被引入以处理欧几里德数据，未能有效捕捉GAD的重要非欧几里德特征。在这项工作中，我们提出了一种新颖的开放式GAD方法，即正常结构规范化（NSReg），以实现对未知异常的广义检测能力。

    arXiv:2311.06835v2 Announce Type: replace-cross  Abstract: This paper considers an important Graph Anomaly Detection (GAD) task, namely open-set GAD, which aims to detect anomalous nodes using a small number of labelled training normal and anomaly nodes (known as seen anomalies) that cannot illustrate all possible inference-time abnormalities. The availability of that labelled data provides crucial prior knowledge about abnormalities for GAD models, enabling substantially reduced detection errors. However, current methods tend to over-emphasise fitting the seen anomalies, leading to a weak generalisation ability to detect unseen anomalies, i.e., those that are not illustrated by the labelled anomaly nodes. Further, they were introduced to handle Euclidean data, failing to effectively capture important non-Euclidean features for GAD. In this work, we propose a novel open-set GAD approach, namely Normal Structure Regularisation (NSReg), to achieve generalised detection ability to unseen 
    
[^323]: 知识增强的大型语言模型用于个性化上下文查询建议

    Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion

    [https://arxiv.org/abs/2311.06318](https://arxiv.org/abs/2311.06318)

    提出一种新颖且通用的方法，通过从用户与搜索引擎的交互历史中提取相关上下文来个性化大型语言模型的输出，尤其适用于改进网络搜索体验。

    

    大型语言模型（LLMs）擅长解决各种自然语言任务。然而，由于重新训练或微调它们所涉及的成本巨大，它们仍然在很大程度上是静态的，并且难以个性化。尽管如此，许多应用程序可以从根据用户的偏好、目标和知识量定制的生成中受益。其中之一是网络搜索，了解用户试图做什么、关心什么以及他们知道什么可以提高搜索体验。在这项工作中，我们提出了一种新颖且通用的方法，该方法使用用户与搜索引擎的交互历史中的相关上下文来增强LLM以个性化其输出。具体而言，我们根据用户在网络上的搜索和浏览活动构建了每个用户的以实体为中心的知识存储，然后利用这些知识为LLM提供具有上下文相关性的提示增强。

    arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
    
[^324]: 犹豫模糊集及其应用于多强度智能分类器的基础理论

    Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers

    [https://arxiv.org/abs/2311.04256](https://arxiv.org/abs/2311.04256)

    本文提出了基于犹豫模糊集的多种包含关系定义、犹豫模糊信息系统的基础命题和基于多强度智能分类器的健康状态诊断方法。

    

    犹豫模糊集在某些不确定和犹豫的情况下被广泛使用。在集合中，包含关系是一个重要且基础的定义。因此，作为一种集合，犹豫模糊集需要一个明确的包含关系定义。基于离散形式的犹豫模糊隶属度，本文提出了几种适用于犹豫模糊集的包含关系。随后，介绍了一些犹豫模糊集的基础命题，以及犹豫模糊集族的命题。针对参数减少，提出了犹豫模糊信息系统的一些基础命题，并给出了一个示例和算法来说明参数减少的过程。最后，提出了一种多强度智能分类器，用于对复杂系统进行健康状态诊断。

    arXiv:2311.04256v3 Announce Type: replace  Abstract: Hesitant fuzzy sets are widely used in certain instances of uncertainty and hesitation. In sets, the inclusion relationship is an important and foundational definition. Thus, as a kind of set, hesitant fuzzy sets require an explicit definition of inclusion relationship. Based on the hesitant fuzzy membership degree of discrete form, several kinds of inclusion relationships for hesitant fuzzy sets are proposed in this work. Then, some foundational propositions of hesitant fuzzy sets are presented, along with propositions of families of hesitant fuzzy sets. Some foundational propositions of hesitant fuzzy information systems are proposed with respect to parameter reductions and an example and an algorithm are given to illustrate the processes of parameter reduction. Finally, a multi-strength intelligent classifier is proposed to make health state diagnoses for complex systems.
    
[^325]: DreamSmooth：通过奖励平滑改进基于模型的强化学习

    DreamSmooth: Improving Model-based Reinforcement Learning via Reward Smoothing

    [https://arxiv.org/abs/2311.01450](https://arxiv.org/abs/2311.01450)

    DreamSmooth通过学习预测时间平滑奖励而非精确奖励，优化了基于模型的强化学习，在稀疏奖励任务上表现出最先进的性能。

    

    摘要: 基于模型的强化学习（MBRL）因其以节约样本的方式学习复杂行为的能力而受到广泛关注：通过生成具有预测奖励的虚拟轨迹来规划动作。尽管取得了成功，但我们发现令人惊讶的是，奖励预测通常是MBRL的瓶颈，特别是对于难以预测的稀疏奖励。受到人类从粗糙奖励估计中学习的直觉启发，我们提出了一种简单而有效的奖励平滑方法DreamSmooth，它学习预测一个在给定时间步的奖励的时间平滑版本，而不是精确奖励。我们在实证上展示DreamSmooth在长视野稀疏奖励任务上取得了最先进的性能，既在样本效率和最终性能上，又不损失在常见基准测试上的性能，如Deepmind Control Suite和Atari基准测试。

    arXiv:2311.01450v2 Announce Type: replace-cross  Abstract: Model-based reinforcement learning (MBRL) has gained much attention for its ability to learn complex behaviors in a sample-efficient way: planning actions by generating imaginary trajectories with predicted rewards. Despite its success, we found that surprisingly, reward prediction is often a bottleneck of MBRL, especially for sparse rewards that are challenging (or even ambiguous) to predict. Motivated by the intuition that humans can learn from rough reward estimates, we propose a simple yet effective reward smoothing approach, DreamSmooth, which learns to predict a temporally-smoothed reward, instead of the exact reward at the given timestep. We empirically show that DreamSmooth achieves state-of-the-art performance on long-horizon sparse-reward tasks both in sample efficiency and final performance without losing performance on common benchmarks, such as Deepmind Control Suite and Atari benchmarks.
    
[^326]: 使用Syntheseus重新评估回溯合成算法

    Re-evaluating Retrosynthesis Algorithms with Syntheseus

    [https://arxiv.org/abs/2310.19796](https://arxiv.org/abs/2310.19796)

    使用Syntheseus建立的基准库重新评估了回溯合成算法，揭示了现有技术模型的系统性缺陷并提供了对未来工作的指导建议。

    

    过去几年，分子合成规划，也称为回溯合成，已经成为机器学习和化学界关注的焦点。尽管看似取得了稳定的进展，但我们认为存在不完善的基准和不一致的比较掩盖了现有技术的系统性缺陷。为了解决这个问题，我们提出了一个名为syntheseus的基准库，通过默认推广最佳实践，实现了对单步和多步回溯合成算法的一致而有意义的评估。我们使用syntheseus重新评估了若干先前的回溯合成算法，并发现在仔细评估时，现有技术模型的排名会发生变化。最后，我们给出了这一领域未来工作的指导建议。

    arXiv:2310.19796v2 Announce Type: replace-cross  Abstract: The planning of how to synthesize molecules, also known as retrosynthesis, has been a growing focus of the machine learning and chemistry communities in recent years. Despite the appearance of steady progress, we argue that imperfect benchmarks and inconsistent comparisons mask systematic shortcomings of existing techniques. To remedy this, we present a benchmarking library called syntheseus which promotes best practice by default, enabling consistent meaningful evaluation of single-step and multi-step retrosynthesis algorithms. We use syntheseus to re-evaluate a number of previous retrosynthesis algorithms, and find that the ranking of state-of-the-art models changes when evaluated carefully. We end with guidance for future works in this area.
    
[^327]: 具有通用函数逼近的耐腐败离线强化学习

    Corruption-Robust Offline Reinforcement Learning with General Function Approximation

    [https://arxiv.org/abs/2310.14550](https://arxiv.org/abs/2310.14550)

    设计了一种新的不确定性加权迭代过程以提高耐腐败的离线强化学习算法的效率，并在单策略覆盖和已知腐败水平的情况下实现了次优性界。

    

    我们研究了具有通用函数逼近的耐腐败离线强化学习中的腐败鲁棒性问题，其中对手可以破坏离线数据集中的每个样本，而腐败水平 $\zeta\geq0$ 量化了 $n$ 个周期和 $H$ 步中的累积破坏量。我们的目标是找到一种对此类破坏具有鲁棒性并最小化相对于未受损的马尔可夫决策过程（MDP）最优策略的次优差距的策略。受到鲁棒在线强化学习环境中的不确定性加权技术的启发，我们设计了一种新的不确定性加权迭代过程，可以高效地在批量样本上计算，并提出了一种耐腐败的离线强化学习算法。值得注意的是，在单策略覆盖和 $\zeta$ 知识假设的前提下，我们提出的算法达到了一个次优性界，该界是恶化的

    arXiv:2310.14550v3 Announce Type: replace  Abstract: We investigate the problem of corruption robustness in offline reinforcement learning (RL) with general function approximation, where an adversary can corrupt each sample in the offline dataset, and the corruption level $\zeta\geq0$ quantifies the cumulative corruption amount over $n$ episodes and $H$ steps. Our goal is to find a policy that is robust to such corruption and minimizes the suboptimality gap with respect to the optimal policy for the uncorrupted Markov decision processes (MDPs). Drawing inspiration from the uncertainty-weighting technique from the robust online RL setting \citep{he2022nearly,ye2022corruptionrobust}, we design a new uncertainty weight iteration procedure to efficiently compute on batched samples and propose a corruption-robust algorithm for offline RL. Notably, under the assumption of single policy coverage and the knowledge of $\zeta$, our proposed algorithm achieves a suboptimality bound that is worsen
    
[^328]: 变压器用于绿色语义通信：更少能量，更多语义

    Transformers for Green Semantic Communication: Less Energy, More Semantics

    [https://arxiv.org/abs/2310.07592](https://arxiv.org/abs/2310.07592)

    本研究提出了一种名为“Energy-Optimized Semantic Loss”（EOSL）的新型多目标损失函数，通过对变压器模型进行全面实验，实现了在推断过程中节省高达90%能量并提高44%语义相似性表现的目标。

    

    语义通信旨在传输有意义且有效的信息，而不是专注于单个符号或位。与传统通信相比，这样做带来了诸如降低延迟、带宽使用和更高吞吐量等好处。然而，语义通信面临着重大挑战，因为需要通用度量来衡量语义信息丢失和实际能量消耗的联合效果。本研究提出了一种称为“Energy-Optimized Semantic Loss”（EOSL）的新型多目标损失函数，解决了平衡语义信息丢失和能量消耗的挑战。通过对变压器模型进行全面实验，包括CPU和GPU能量使用，证明基于EOSL的编码器模型选择在该实验中推断过程中能节省高达90%的能量，同时在语义相似性性能方面实现了44%的改进。这项工作为

    arXiv:2310.07592v2 Announce Type: replace  Abstract: Semantic communication aims to transmit meaningful and effective information, rather than focusing on individual symbols or bits. This results in benefits like reduced latency, bandwidth usage, and higher throughput compared with traditional communication. However, semantic communication poses significant challenges due to the need for universal metrics to benchmark the joint effects of semantic information loss and practical energy consumption. This research presents a novel multi-objective loss function named "Energy-Optimized Semantic Loss" (EOSL), addressing the challenge of balancing semantic information loss and energy consumption. Through comprehensive experiments on transformer models, including CPU and GPU energy usage, it is demonstrated that EOSL-based encoder model selection can save up to 90% of energy while achieving a 44% improvement in semantic similarity performance during inference in this experiment. This work pave
    
[^329]: 谨慎平滑标签：标签平滑既可以作为隐私屏障，又可以成为模型反推攻击的催化剂

    Be Careful What You Smooth For: Label Smoothing Can Be a Privacy Shield but Also a Catalyst for Model Inversion Attacks

    [https://arxiv.org/abs/2310.06549](https://arxiv.org/abs/2310.06549)

    标签平滑方法在深度学习中发挥重要作用，既能提升模型泛化能力和校准性，又可能成为模型隐私泄露的因素。研究揭示了结合负因子进行平滑可有效阻止模型反推攻击，提升隐私保护效果，超越了当前最先进的防御技术。

    

    标签平滑——使用软化的标签而不是硬标签——是深度学习中被广泛采用的正则化方法，表现出增强泛化和校准等多样益处。然而，它对于保护模型隐私的影响仍然没有被探索。为了填补这一空白，我们调查了标签平滑对模型反推攻击（MIAs）的影响，这些攻击旨在通过利用分类器中编码的知识生成具有类代表性的样本，从而推断有关其训练数据的敏感信息。通过广泛的分析，我们发现传统标签平滑促进了MIAs，从而增加了模型的隐私泄露。更甚者，我们揭示了用负因子进行平滑可以抵制这一趋势，阻碍提取与类相关的信息，实现隐私保护，胜过最先进的防御方法。这确立了一种实用且强大的新的增强方式。

    arXiv:2310.06549v2 Announce Type: replace  Abstract: Label smoothing -- using softened labels instead of hard ones -- is a widely adopted regularization method for deep learning, showing diverse benefits such as enhanced generalization and calibration. Its implications for preserving model privacy, however, have remained unexplored. To fill this gap, we investigate the impact of label smoothing on model inversion attacks (MIAs), which aim to generate class-representative samples by exploiting the knowledge encoded in a classifier, thereby inferring sensitive information about its training data. Through extensive analyses, we uncover that traditional label smoothing fosters MIAs, thereby increasing a model's privacy leakage. Even more, we reveal that smoothing with negative factors counters this trend, impeding the extraction of class-related information and leading to privacy preservation, beating state-of-the-art defenses. This establishes a practical and powerful novel way for enhanc
    
[^330]: Outlier Weighed Layerwise Sparsity (OWL): 为剪枝LLMs达到高稀疏度提供缺失的秘密调味料

    Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity

    [https://arxiv.org/abs/2310.05175](https://arxiv.org/abs/2310.05175)

    本研究发现LLMs中的激活异常值与网络层稀疏度的非均匀性相关，并提出了Outlier Weighed Layerwise Sparsity（OWL）作为剪枝LLMs到高稀疏度的秘密调味料。

    

    大型语言模型（LLMs）以在各个领域展现出的卓越性能而闻名，在实际部署时由于模型庞大而面临挑战。为了解决这一挑战，人们努力将传统的网络剪枝技术应用于LLMs，发现可以在不影响性能的情况下一次性剪掉大量参数。现有的LLM剪枝策略一直坚持以等价稀疏度均匀剪裁所有层的做法，结果表现强劲。然而，这个观察结果与在视觉模型领域观察到的非均匀逐层稀疏的主流趋势相矛盾，后者通常会产生更好的结果。为了了解这种差异背后的原因，我们进行了全面研究，并发现与LLMs中异常值的出现强相关。

    arXiv:2310.05175v2 Announce Type: replace  Abstract: Large Language Models (LLMs), renowned for their remarkable performance across diverse domains, present a challenge when it comes to practical deployment due to their colossal model size. In response to this challenge, efforts have been directed toward the application of traditional network pruning techniques to LLMs, uncovering a massive number of parameters that can be pruned in one-shot without hurting performance. Prevailing LLM pruning strategies have consistently adhered to the practice of uniformly pruning all layers at equivalent sparsity, resulting in robust performance. However, this observation stands in contrast to the prevailing trends observed in the field of vision models, where non-uniform layerwise sparsity typically yields stronger results. To understand the underlying reasons for this disparity, we conduct a comprehensive study and discover a strong correlation with the emergence of activation outliers in LLMs. Ins
    
[^331]: CoDi: 条件扩散蒸馏，用于更高保真度和更快的图像生成

    CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation

    [https://arxiv.org/abs/2310.01407](https://arxiv.org/abs/2310.01407)

    CoDi引入了一种新的方法，通过适应预先训练的潜在扩散模型接受额外的图像条件输入，可以显著降低生成高质量结果所需的采样步骤。

    

    大型生成性扩散模型已经革新了文本到图像的生成，并为像图像增强、恢复、编辑和合成等条件生成任务提供了巨大潜力。然而，它们的广泛应用受到高计算成本的限制，这限制了它们在实时应用中的应用。为了解决这一挑战，我们引入了一种名为CoDi的新方法，它使一个预训练的潜在扩散模型能够接受额外的图像条件输入，同时显著减少了需要达到高质量结果所需的采样步骤。

    arXiv:2310.01407v2 Announce Type: replace-cross  Abstract: Large generative diffusion models have revolutionized text-to-image generation and offer immense potential for conditional generation tasks such as image enhancement, restoration, editing, and compositing. However, their widespread adoption is hindered by the high computational cost, which limits their real-time application. To address this challenge, we introduce a novel method dubbed CoDi, that adapts a pre-trained latent diffusion model to accept additional image conditioning inputs while significantly reducing the sampling steps required to achieve high-quality results. Our method can leverage architectures such as ControlNet to incorporate conditioning inputs without compromising the model's prior knowledge gained during large scale pre-training. Additionally, a conditional consistency loss enforces consistent predictions across diffusion steps, effectively compelling the model to generate high-quality images with conditio
    
[^332]: 赋能众多，偏袒少数：通过大型语言模型实现通用信用评分

    Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models

    [https://arxiv.org/abs/2310.00566](https://arxiv.org/abs/2310.00566)

    大型语言模型在信用评分任务中表现出强大的通用能力，通过首个开源框架和CALM模型，可以帮助解决传统信用评分方法所面临的挑战，并同时解决潜在的偏见问题。

    

    在金融行业，信用评分是一个基础要素，塑造着个人和企业的信贷准入，决定着贷款条件。本研究认为，大型语言模型（LLMs）在信用评分任务中具有巨大潜力，能够强大地跨多个任务进行泛化。为了系统地探索LLMs在信用评分中的应用，我们提出了第一个开源全面框架。我们收集了一个涵盖9个数据集、1.4K样本的新型基准，专门用于信用评估，并对LLMs内潜在偏见进行了重要检查，同时提供了超过45K样本的新型指导调整数据。然后，我们通过指导调整提出了首个信贷与风险评估大型语言模型（CALM），以针对不同的微妙需求

    arXiv:2310.00566v3 Announce Type: replace-cross  Abstract: In the financial industry, credit scoring is a fundamental element, shaping access to credit and determining the terms of loans for individuals and businesses alike. Traditional credit scoring methods, however, often grapple with challenges such as narrow knowledge scope and isolated evaluation of credit tasks. Our work posits that Large Language Models (LLMs) have great potential for credit scoring tasks, with strong generalization ability across multiple tasks. To systematically explore LLMs for credit scoring, we propose the first open-source comprehensive framework. We curate a novel benchmark covering 9 datasets with 14K samples, tailored for credit assessment and a critical examination of potential biases within LLMs, and the novel instruction tuning data with over 45k samples. We then propose the first Credit and Risk Assessment Large Language Model (CALM) by instruction tuning, tailored to the nuanced demands of various
    
[^333]: 从语言建模到指令跟随：理解指令调整后LLMs中行为的转变

    From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning

    [https://arxiv.org/abs/2310.00492](https://arxiv.org/abs/2310.00492)

    指令调整对LLMs产生了三个重要影响：1）使其能够识别用户提示中的指令部分；2）促进响应生成的不断调整

    

    大型语言模型（LLMs）已经取得了显著的成功，其中指令调整是将LLMs与用户意图对齐的关键步骤。在这项工作中，我们研究了指令调整如何调整经过预训练的模型，重点关注内在变化。具体来说，我们首先开发了几种本地和全局解释方法，包括一种基于梯度的输入输出归因方法，以及用于解释自注意力和前馈层中的模式和概念的技术。然后通过比较从预训练和指令调整模型中得出的解释来研究指令调整的影响。这种方法在人可理解的水平上提供了模型转变的内部视角。我们的研究发现了指令调整的三个重要影响：1）它使LLMs能够识别用户提示中的指令部分，并不断促进响应生成

    arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
    
[^334]: SINCERE: 监督信息噪声-对比估计再审

    SINCERE: Supervised Information Noise-Contrastive Estimation REvisited

    [https://arxiv.org/abs/2309.14277](https://arxiv.org/abs/2309.14277)

    SINCERE提出了一个理论上合理的监督扩展，避免了同一类别的图像相互排斥，通过更好地分离不同类别的嵌入，在保持竞争性分类准确性的同时实现了更好的效果。

    

    信息噪声对比估计（InfoNCE）损失函数由于其强大的实证结果和理论动机，为许多自监督深度学习方法提供了基础。先前的工作表明，监督对比（SupCon）损失可扩展InfoNCE以从可用类标签中学习。然而，在这项工作中，我们发现先前的SupCon损失公式存在疑问的理由，因为它可能会促使来自同一类别的某些图像在学习到的嵌入空间中相互排斥。我们提出了监督信息噪声-对比估计再审（SINCERE）损失，作为信息噪声对比估计的理论上合理的监督扩展，它永远不会导致来自同一类别的图像相互排斥。实验表明，SINCERE导致不同类别的嵌入更好地分离，同时对于监督和迁移学习提供具有竞争力的分类准确性。我们进一步展示了一个信息论上的下界

    arXiv:2309.14277v2 Announce Type: replace-cross  Abstract: The information noise-contrastive estimation (InfoNCE) loss function provides the basis of many self-supervised deep learning methods due to its strong empirical results and theoretic motivation. Previous work suggests a supervised contrastive (SupCon) loss to extend InfoNCE to learn from available class labels. However, in this work we find that the prior SupCon loss formulation has questionable justification because it can encourage some images from the same class to repel one another in the learned embedding space. We propose the Supervised InfoNCE REvisited (SINCERE) loss as a theoretically-justified supervised extension of InfoNCE that never causes images from the same class to repel one another. Experiments show that SINCERE leads to better separation of embeddings from different classes while delivering competitive classification accuracy for supervised and transfer learning. We further show an information-theoretic boun
    
[^335]: 使用类别排除的早期退出实现神经网络有效推理

    Early-Exit with Class Exclusion for Efficient Inference of Neural Networks

    [https://arxiv.org/abs/2309.13443](https://arxiv.org/abs/2309.13443)

    提出了一种基于类别排除的动态推理早期退出方法，通过在中间层利用学到的特征排除大量不相关类别，从而显著降低神经网络推理的计算成本。

    

    深度神经网络在各个领域已经被成功应用。在神经网络中，需要执行大量的乘法累加（MAC）运算，这在资源受限的平台（如边缘设备）中应用它们时会带来挑战。为了解决这个挑战，本文提出了一种基于类别的动态推理早期退出方法。我们利用这些层中学到的特征来排除尽可能多的不相关类别，使得后续层只需要在剩余类别中确定目标类别。当在某一层中仅剩下一个类别时，这个类别就是相应的分类结果。实验结果表明，提出的早期退出技术可以显著降低DNN在推理中的计算成本。

    arXiv:2309.13443v2 Announce Type: replace  Abstract: Deep neural networks (DNNs) have been successfully applied in various fields. In DNNs, a large number of multiply-accumulate (MAC) operations are required to be performed, posing critical challenges in applying them in resource-constrained platforms, e.g., edge devices. To address this challenge, in this paper, we propose a class-based early-exit for dynamic inference. Instead of pushing DNNs to make a dynamic decision at intermediate layers, we take advantage of the learned features in these layers to exclude as many irrelevant classes as possible, so that later layers only have to determine the target class among the remaining classes. When only one class remains at a layer, this class is the corresponding classification result. Experimental results demonstrate the computational cost of DNNs in inference can be reduced significantly with the proposed early-exit technique. The codes can be found at https://github.com/HWAI-TUDa/Early
    
[^336]: AFN: 自适应融合规范化在编码器-解码器框架中的应用

    AFN: Adaptive Fusion Normalization via an Encoder-Decoder Framework

    [https://arxiv.org/abs/2308.03321](https://arxiv.org/abs/2308.03321)

    该论文介绍了一种新的自适应融合规范化方法，通过编码器-解码器框架，在领域泛化和图像分类任务中表现优越。

    

    深度学习的成功与规范化层密不可分。研究人员提出了各种规范化功能，每种都有其优势和劣势。为此，人们致力于设计一个统一的规范化功能，将所有规范化过程结合起来并减轻它们的弱点。我们还提出了一种新的规范化功能，称为自适应融合规范化。通过实验证明，AFN在领域泛化和图像分类任务中优于先前的规范化技术。

    arXiv:2308.03321v4 Announce Type: replace  Abstract: The success of deep learning is inseparable from normalization layers. Researchers have proposed various normalization functions, and each of them has both advantages and disadvantages. In response, efforts have been made to design a unified normalization function that combines all normalization procedures and mitigates their weaknesses. We also proposed a new normalization function called Adaptive Fusion Normalization. Through experiments, we demonstrate AFN outperforms the previous normalization techniques in domain generalization and image classification tasks.
    
[^337]: CroSSL: 跨模态的自监督学习在时间序列上的应用通过隐藏掩码

    CroSSL: Cross-modal Self-Supervised Learning for Time-series through Latent Masking

    [https://arxiv.org/abs/2307.16847](https://arxiv.org/abs/2307.16847)

    CroSSL是一种跨模态的自监督学习方法，通过隐藏掩码和跨模态聚合器实现对时间序列的学习，无需负样本对和数据预处理。

    

    机器学习多模态时间序列的标注数据的有限可用性严重阻碍了该领域的进展。自监督学习（SSL）是一种无需依赖标签学习数据表示的有前景的方法。然而，现有的SSL方法需要计算昂贵的负样本对，并且通常仅适用于单模态，从而限制了它们的多功能性。我们引入了CroSSL（跨模态自监督学习），它提出了两个创新概念：通过模态特定编码器产生的中间嵌入的隐藏掩码，以及通过跨模态聚合器将其聚合为全局嵌入，可以提供给下游分类器。CroSSL允许处理缺失模态和端到端的跨模态学习，无需进行预处理以处理缺失输入或进行对比学习的负样本采样。我们对各种数据进行了评估，包括加速度计或陀螺仪等运动传感器和生物传感器。

    Limited availability of labeled data for machine learning on multimodal time-series extensively hampers progress in the field. Self-supervised learning (SSL) is a promising approach to learning data representations without relying on labels. However, existing SSL methods require expensive computations of negative pairs and are typically designed for single modalities, which limits their versatility. We introduce CroSSL (Cross-modal SSL), which puts forward two novel concepts: masking intermediate embeddings produced by modality-specific encoders, and their aggregation into a global embedding through a cross-modal aggregator that can be fed to down-stream classifiers. CroSSL allows for handling missing modalities and end-to-end cross-modal learning without requiring prior data preprocessing for handling missing inputs or negative-pair sampling for contrastive learning. We evaluate our method on a wide range of data, including motion sensors such as accelerometers or gyroscopes and biosi
    
[^338]: 分解扩散采样器用于加速大规模逆问题

    Decomposed Diffusion Sampler for Accelerating Large-Scale Inverse Problems

    [https://arxiv.org/abs/2303.05754](https://arxiv.org/abs/2303.05754)

    提出了一种将扩散采样和Krylov子空间方法协同结合的新型高效采样策略。

    

    Krylov子空间是通过将给定向量与线性变换矩阵及其连续幂相乘而生成的，广泛研究的经典优化文献中利用Krylov子空间设计算法以快速收敛大规模线性逆问题。本研究提出了一种新颖高效的扩散采样策略，将扩散采样与Krylov子空间方法协同结合起来。

    arXiv:2303.05754v3 Announce Type: replace-cross  Abstract: Krylov subspace, which is generated by multiplying a given vector by the matrix of a linear transformation and its successive powers, has been extensively studied in classical optimization literature to design algorithms that converge quickly for large linear inverse problems. For example, the conjugate gradient method (CG), one of the most popular Krylov subspace methods, is based on the idea of minimizing the residual error in the Krylov subspace. However, with the recent advancement of high-performance diffusion solvers for inverse problems, it is not clear how classical wisdom can be synergistically combined with modern diffusion models. In this study, we propose a novel and efficient diffusion sampling strategy that synergistically combines the diffusion sampling and Krylov subspace methods. Specifically, we prove that if the tangent space at a denoised sample by Tweedie's formula forms a Krylov subspace, then the CG initi
    
[^339]: 马尔可夫链的距离及其差异化

    Distances for Markov Chains, and Their Differentiation

    [https://arxiv.org/abs/2302.08621](https://arxiv.org/abs/2302.08621)

    本文提出了一个统一框架，用于生成马尔可夫链的距离，包括带节点属性的有向图，这个框架称为最优输运马尔可夫（OTM）距禄。

    

    （有向）带节点属性的图是各种应用中常见的一种数据类型，关于开发用于比较它们的度量标准和高效算法有大量文献。最近，在图学习和优化领域，针对带节点属性的图比较已经出现了一系列新方法，利用了最优输运（OT）和Weisfeiler-Lehman（WL）图同构测试等思想。两个最先进的代表是(O'Connor等人，2022)提出的OTC距离和(Chen等人，2022)提出的WL距离。有趣的是，尽管这两种距离基于不同的思想开发，我们观察到它们都将图视为马尔可夫链，并且有着深刻的联系。实际上，在本文中，我们提出了一个统一框架来生成马尔可夫链的距离（从而包括带节点属性的（有向）图），我们称之为最优输运马尔可夫（OTM）d

    arXiv:2302.08621v2 Announce Type: replace  Abstract: (Directed) graphs with node attributes are a common type of data in various applications and there is a vast literature on developing metrics and efficient algorithms for comparing them. Recently, in the graph learning and optimization communities, a range of new approaches have been developed for comparing graphs with node attributes, leveraging ideas such as the Optimal Transport (OT) and the Weisfeiler-Lehman (WL) graph isomorphism test. Two state-of-the-art representatives are the OTC distance proposed in (O'Connor et al., 2022) and the WL distance in (Chen et al., 2022). Interestingly, while these two distances are developed based on different ideas, we observe that they both view graphs as Markov chains, and are deeply connected. Indeed, in this paper, we propose a unified framework to generate distances for Markov chains (thus including (directed) graphs with node attributes), which we call the Optimal Transport Markov (OTM) d
    
[^340]: 深度学习从英国生物库眼底成像中预测帕金森病的现有和新发病例

    Deep Learning Predicts Prevalent and Incident Parkinson's Disease From UK Biobank Fundus Imaging

    [https://arxiv.org/abs/2302.06727](https://arxiv.org/abs/2302.06727)

    深度学习技术在英国生物库眼底成像中被用于诊断帕金森病，可在疾病症状发作之前实现准确筛查。

    

    帕金森病是世界上增长最快的神经系统疾病。研究阐明帕金森病的机制并自动进行诊断将极大地改善帕金森病患者的治疗。当前的诊断方法昂贵且供应有限。考虑到疾病的隐匿性和临床前期的发展，理想的筛查应该具有诊断准确性，甚至在症状发作之前就能允许医学干预。我们强调视网膜眼底成像作为帕金森病的诊断筛查模式，通常被称为大脑之窗。我们对传统机器学习和深度学习技术进行了系统评估，以从英国生物库眼底成像中对帕金森病进行分类。我们的结果表明，帕金森病患者可以与年龄和性别匹配的健康受试者区分开来，面积在曲线下

    arXiv:2302.06727v3 Announce Type: replace  Abstract: Parkinson's disease is the world's fastest-growing neurological disorder. Research to elucidate the mechanisms of Parkinson's disease and automate diagnostics would greatly improve the treatment of patients with Parkinson's disease. Current diagnostic methods are expensive and have limited availability. Considering the insidious and preclinical onset and progression of the disease, a desirable screening should be diagnostically accurate even before the onset of symptoms to allow medical interventions. We highlight retinal fundus imaging, often termed a window to the brain, as a diagnostic screening modality for Parkinson's disease. We conducted a systematic evaluation of conventional machine learning and deep learning techniques to classify Parkinson's disease from UK Biobank fundus imaging. Our results show that Parkinson's disease individuals can be differentiated from age and gender-matched healthy subjects with an Area Under the 
    
[^341]: 带有分布式量化流的GFlowNets

    Distributional GFlowNets with Quantile Flows

    [https://arxiv.org/abs/2302.05793](https://arxiv.org/abs/2302.05793)

    本文提出了一种带分布式量化流的GFlowNets模型，通过将流函数转化为分布，在训练过程中提供更多信息的学习信号。通过量化函数参数化每个边流，我们提出的算法可以学习风险敏感的策略，实现对风险不确定性场景的处理，并在现有基准上取得了显著改进。

    

    生成式流网络（GFlowNets）是一种新的概率采样器系列，其中代理通过一系列决策步骤学习生成复杂组合结构的随机策略。尽管受强化学习启发，当前的GFlowNet框架在适用性上相对有限，无法处理奖励函数中的随机性。在这项工作中，我们采用分布式范式来处理GFlowNets，将每个流函数转化为一个分布，从而在训练过程中提供更多信息的学习信号。通过通过量化函数对每个边流进行参数化，我们提出的“量化匹配” GFlowNet学习算法能够学习风险敏感的策略，这是处理风险不确定性场景的基本组成部分。此外，我们发现与之前的方法相比，分布式方法由于我们增强的训练算法，可以在现有基准上实现显着改进。

    Generative Flow Networks (GFlowNets) are a new family of probabilistic samplers where an agent learns a stochastic policy for generating complex combinatorial structure through a series of decision-making steps. Despite being inspired from reinforcement learning, the current GFlowNet framework is relatively limited in its applicability and cannot handle stochasticity in the reward function. In this work, we adopt a distributional paradigm for GFlowNets, turning each flow function into a distribution, thus providing more informative learning signals during training. By parameterizing each edge flow through their quantile functions, our proposed \textit{quantile matching} GFlowNet learning algorithm is able to learn a risk-sensitive policy, an essential component for handling scenarios with risk uncertainty. Moreover, we find that the distributional approach can achieve substantial improvement on existing benchmarks compared to prior methods due to our enhanced training algorithm, even i
    
[^342]: STLGRU：时空轻量级图GRU用于交通流预测

    STLGRU: Spatio-Temporal Lightweight Graph GRU for Traffic Flow Prediction

    [https://arxiv.org/abs/2212.04548](https://arxiv.org/abs/2212.04548)

    STLGRU是一种新型的交通预测模型，能够有效捕捉交通网络的动态局部和全局时空关系，适用于低功耗设备

    

    可靠的交通流预测需要对交通数据进行高效建模。由于动态交通网络中出现不同的相关性和影响，使得建模成为一项复杂的任务。现有文献提出了许多不同的方法来捕捉交通网络的复杂时空关系。然而，鉴于交通数据的异质性，始终捕捉空间和时间依赖性是一个重要挑战。此外，随着越来越多复杂方法的提出，模型变得越来越消耗内存，因此不适合低功耗设备。为此，我们提出了一种名为STLGRU的时空轻量级图GRU，这是一种用于准确预测交通流量的新型交通预测模型。具体而言，我们提出的STLGRU可以有效地捕获交通网络的动态局部和全局时空关系，利用增强记忆的注意机制

    arXiv:2212.04548v3 Announce Type: replace  Abstract: Reliable forecasting of traffic flow requires efficient modeling of traffic data. Indeed, different correlations and influences arise in a dynamic traffic network, making modeling a complicated task. Existing literature has proposed many different methods to capture traffic networks' complex underlying spatial-temporal relations. However, given the heterogeneity of traffic data, consistently capturing both spatial and temporal dependencies presents a significant challenge. Also, as more and more sophisticated methods are being proposed, models are increasingly becoming memory-heavy and, thus, unsuitable for low-powered devices. To this end, we propose Spatio-Temporal Lightweight Graph GRU, namely STLGRU, a novel traffic forecasting model for predicting traffic flow accurately. Specifically, our proposed STLGRU can effectively capture dynamic local and global spatial-temporal relations of traffic networks using memory-augmented attent
    
[^343]: 分治核函数的功能线性回归的统计优化性

    Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression

    [https://arxiv.org/abs/2211.10968](https://arxiv.org/abs/2211.10968)

    研究了在目标函数不一定包含在核空间中的情况下，分治核函数的功能线性回归算法的统计优化性。

    

    先前对于正则化的核函数空间中的功能线性回归的分析通常要求目标函数包含在这个核空间中。本文研究了在目标函数不一定驻留在基本RKHS中的情况下，分治估计器的收敛性能。作为一种基于分解的可扩展方法，功能线性回归的分治估计器可以大幅减少时间和内存中的算法复杂性。我们采用积分算子方法建立了针对分治估计器在解释变量和目标函数的各种正则性条件下的预测的尖锐有限样本上界。通过建立最小-最大下界，我们还证明了导出率的渐近最优性。最后，我们考虑了无噪声估计器的收敛性，并展示了这些率能够

    arXiv:2211.10968v3 Announce Type: replace  Abstract: Previous analysis of regularized functional linear regression in a reproducing kernel Hilbert space (RKHS) typically requires the target function to be contained in this kernel space. This paper studies the convergence performance of divide-and-conquer estimators in the scenario that the target function does not necessarily reside in the underlying RKHS. As a decomposition-based scalable approach, the divide-and-conquer estimators of functional linear regression can substantially reduce the algorithmic complexities in time and memory. We develop an integral operator approach to establish sharp finite sample upper bounds for prediction with divide-and-conquer estimators under various regularity conditions of explanatory variables and target function. We also prove the asymptotic optimality of the derived rates by building the mini-max lower bounds. Finally, we consider the convergence of noiseless estimators and show that the rates ca
    
[^344]: 从已记录数据中进行半监督批量学习

    Semi-supervised Batch Learning From Logged Data

    [https://arxiv.org/abs/2209.07148](https://arxiv.org/abs/2209.07148)

    本研究基于反事实风险最小化框架提出了一种半监督批量学习方法，解决了在已记录数据中反馈缺失的问题，提出了一个新的上界来处理这种学习问题。

    

    异策略学习方法旨在从已记录数据中学习策略，该数据包括每个样本点的环境、动作和反馈（成本或奖励）。在这项工作中，我们基于反事实风险最小化框架，该框架还假设能够访问概率得分。我们提出了针对一些样本缺失反馈的问题提出的学习方法，因此在已记录数据中有些样本有反馈，有些样本缺失反馈。我们将这种类型的学习称为从已记录数据中的半监督批量学习，这在广泛的应用领域中出现。为了解决这种学习问题，我们推导出了真实风险的新上界，采用倒数概率得分估计器。利用这个上界，我们提出了一种带有已记录数据的正则化半监督批量学习方法，其中正则化项与反馈无关，结果可以使用已记录的缺失反馈进行评估。

    arXiv:2209.07148v3 Announce Type: replace-cross  Abstract: Off-policy learning methods are intended to learn a policy from logged data, which includes context, action, and feedback (cost or reward) for each sample point. In this work, we build on the counterfactual risk minimization framework, which also assumes access to propensity scores. We propose learning methods for problems where feedback is missing for some samples, so there are samples with feedback and samples missing-feedback in the logged data. We refer to this type of learning as semi-supervised batch learning from logged data, which arises in a wide range of application domains. We derive a novel upper bound for the true risk under the inverse propensity score estimator to address this kind of learning problem. Using this bound, we propose a regularized semi-supervised batch learning method with logged data where the regularization term is feedback-independent and, as a result, can be evaluated using the logged missing-fe
    
[^345]: 对使用时间侧信道在深度神经网络中评估用户隐私的研究

    On the Evaluation of User Privacy in Deep Neural Networks using Timing Side Channel

    [https://arxiv.org/abs/2208.01113](https://arxiv.org/abs/2208.01113)

    在本研究中，我们发现并报告了DL实现中的一种新型数据依赖性时序侧信道泄漏，并展示了一个实用的推理时攻击。

    

    最近深度学习（DL）在解决复杂现实世界任务方面取得的进展导致其在实际应用中得到广泛采用。然而，这一机会伴随着重要的潜在风险，因为许多模型依赖于用于训练的涉及隐私的数据，在各种应用中使其成为过度暴露的隐私侵犯威胁面。此外，广泛使用基于云的机器学习即服务（MLaaS）以获取其强大的基础设施支持，将威胁面扩展到包括各种远程侧信道攻击。在本文中，我们首先识别并报告了一个新颖的数据相关的DL实现中的时序侧信道泄漏（称为类泄漏），源自于广泛使用的DL框架PyTorch中的非常量时间分支操作。我们进一步展示了一种实用的推理时攻击，其中拥有用户特权和硬标签黑盒访问权限的对手

    arXiv:2208.01113v3 Announce Type: replace-cross  Abstract: Recent Deep Learning (DL) advancements in solving complex real-world tasks have led to its widespread adoption in practical applications. However, this opportunity comes with significant underlying risks, as many of these models rely on privacy-sensitive data for training in a variety of applications, making them an overly-exposed threat surface for privacy violations. Furthermore, the widespread use of cloud-based Machine-Learning-as-a-Service (MLaaS) for its robust infrastructure support has broadened the threat surface to include a variety of remote side-channel attacks. In this paper, we first identify and report a novel data-dependent timing side-channel leakage (termed Class Leakage) in DL implementations originating from non-constant time branching operation in a widely used DL framework PyTorch. We further demonstrate a practical inference-time attack where an adversary with user privilege and hard-label black-box acces
    
[^346]: 具有融合套索的图中方差估计

    Variance estimation in graphs with the fused lasso

    [https://arxiv.org/abs/2207.12638](https://arxiv.org/abs/2207.12638)

    研究了在一般图结构问题中的方差估计，开发了线性时间估计器并提供了上界，允许推广到更广泛的分布类。

    

    我们研究了在一般图结构问题中的方差估计问题。首先，我们为同方差情况开发了一个线性时间估计器，可以在一般图中一致估计方差。我们证明了当均值信号具有总变化与标准尺度时，我们的估计器在链式图和二维网格图上达到最小最大率。此外，我们在一般图中提供了融合套索估计器的均方误差表现的一般上界，根据矩条件和误差尾部行为的界限。这些上界使我们能够推广到更广泛的分布类，例如亚指数分布，许多现有关于融合套索的结果仅在假设错误为亚高斯随机变量的情况下成立。利用我们的上界，我们随后研究了一个简单的总变差正则化估计器，用于估算方差信号。

    arXiv:2207.12638v3 Announce Type: replace-cross  Abstract: We study the problem of variance estimation in general graph-structured problems. First, we develop a linear time estimator for the homoscedastic case that can consistently estimate the variance in general graphs. We show that our estimator attains minimax rates for the chain and 2D grid graphs when the mean signal has total variation with canonical scaling. Furthermore, we provide general upper bounds on the mean squared error performance of the fused lasso estimator in general graphs under a moment condition and a bound on the tail behavior of the errors. These upper bounds allow us to generalize for broader classes of distributions, such as sub-exponential, many existing results on the fused lasso that are only known to hold with the assumption that errors are sub-Gaussian random variables. Exploiting our upper bounds, we then study a simple total variation regularization estimator for estimating the signal of variances in t
    
[^347]: 学习进度驱动的多智能体课程

    Learning Progress Driven Multi-Agent Curriculum

    [https://arxiv.org/abs/2205.10016](https://arxiv.org/abs/2205.10016)

    提出了自主式MARL（SPMARL）以解决当前多智能体强化学习中课程生成的问题，优先考虑基于任务的优先级。

    

    课程强化学习（CRL）旨在通过逐渐增加任务的难度（通常由可实现的预期回报量化）来加快学习速度。受CRL在单智能体环境中的成功启发，一些研究尝试将CRL应用于多智能体强化学习（MARL），使用智能体数量来控制任务难度。然而，现有的工作通常使用手动定义的课程，如线性方案。本文首先将最先进的单智能体自主式CRL应用于稀疏奖励MARL。虽然表现令人满意，但我们确定了现有基于奖励的CRL方法生成的课程存在两个潜在缺陷：（1）高回报的任务可能不提供信息量大的学习信号，（2）在多智能体产生更高回报的任务中，加剧了学分分配困难。因此，我们进一步提出了自主式MARL（SPMARL），以基于任务的优先级进行安排。

    arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
    
[^348]: 代数机器学习及其在化学中的应用

    Algebraic Machine Learning with an Application to Chemistry

    [https://arxiv.org/abs/2205.05795](https://arxiv.org/abs/2205.05795)

    本文开发了一种机器学习流程，能够捕获细粒度的几何信息，而无需依赖光滑性假设。

    

    随着科学应用中使用的数据集变得更加复杂，研究数据的几何和拓扑结构已成为数据分析过程中日益重要的一部分。这可以从对持久同调等拓扑工具日益增长的兴趣中看出。然而，一方面，拓扑工具本质上仅提供关于数据基本空间的粗略信息。另一方面，更多几何方法主要依赖于流形假设，即基本空间是光滑流形。这一假设对于许多包含奇点的物理模型是不成立的。本文中，我们开发了一种机器学习流程，能够捕获细粒度的几何信息，而无需依赖任何光滑性假设。我们的方法涉及代数几何和代数多项式的范围。

    arXiv:2205.05795v3 Announce Type: replace-cross  Abstract: As datasets used in scientific applications become more complex, studying the geometry and topology of data has become an increasingly prevalent part of the data analysis process. This can be seen for example with the growing interest in topological tools such as persistent homology. However, on the one hand, topological tools are inherently limited to providing only coarse information about the underlying space of the data. On the other hand, more geometric approaches rely predominately on the manifold hypothesis, which asserts that the underlying space is a smooth manifold. This assumption fails for many physical models where the underlying space contains singularities.   In this paper we develop a machine learning pipeline that captures fine-grain geometric information without having to rely on any smoothness assumptions. Our approach involves working within the scope of algebraic geometry and algebraic varieties instead of 
    
[^349]: 从生物时间序列数据中发现随机动力学方程

    Discovering stochastic dynamical equations from biological time series data

    [https://arxiv.org/abs/2205.02645](https://arxiv.org/abs/2205.02645)

    提出一种方程发现方法，结合传统随机微积分方法和最先进的方程发现技术，从时间序列数据中分析波动并输出可解释的SDE，在合成和真实数据集上验证了方法的普适性和适用性。

    

    随机微分方程（SDEs）是一个重要的框架，用于模拟具有随机性的动态，这在大多数生物系统中都很常见。将这些模型与实证数据进行整合的逆问题仍然是一个重大挑战。在这里，我们提出了一种方程发现方法，该方法将时间序列数据作为输入，分析细粒度波动并输出一个可以正确捕捉数据长时间动态的可解释SDE。我们通过将来自随机微积分文献的传统方法与最先进的方程发现技术相结合来实现这一点。我们在合成数据集上验证了我们的方法，并在两个时空尺度迥异的真实数据集上验证了该方法的普适性和适用性：（i）鱼群的集体运动，其中随机性起着关键作用，和（ii）单个细胞受限迁移，主要是遵循放松振荡。

    arXiv:2205.02645v5 Announce Type: replace-cross  Abstract: Stochastic differential equations (SDEs) are an important framework to model dynamics with randomness, as is common in most biological systems. The inverse problem of integrating these models with empirical data remains a major challenge. Here, we present an equation discovery methodology that takes time series data as an input, analyses fine scale fluctuations and outputs an interpretable SDE that can correctly capture long-time dynamics of data. We achieve this by combining traditional approaches from stochastic calculus literature with state-of-the-art equation discovery techniques. We validate our approach on synthetic datasets, and demonstrate the generality and applicability of the method on two real-world datasets of vastly different spatiotemporal scales: (i) collective movement of fish school where stochasticity plays a crucial role, and (ii) confined migration of a single cell, primarily following a relaxed oscillatio
    
[^350]: 《SGD和自适应学习规则学到的表示：变化稀疏性和选择性的神经网络条件》

    Representations learnt by SGD and Adaptive learning rules: Conditions that vary sparsity and selectivity in neural network

    [https://arxiv.org/abs/2201.11653](https://arxiv.org/abs/2201.11653)

    本文调查了导致神经网络稀疏性和选择性增加的各种条件。

    

    从人脑的角度来看，连续学习可以执行各种任务而互不干扰。减少互相干扰的有效方式可以在神经元的稀疏性和选择性中找到。根据Aljundi等人和Hadsell等人的观点，在表示水平施加稀疏性对连续学习是有利的，因为稀疏的神经元激活鼓励参数之间的少重叠，导致更少的干扰。同样，高度选择性的神经网络可能会引起较少的干扰，因为神经元中的特定响应将减少与其他参数的重叠机会。考虑到人脑在一生中执行连续学习，找到自然增加稀疏性和选择性的条件可能为了解大脑功能提供见解。本文调查了自然增加神经网络稀疏性和选择性的各种条件。

    arXiv:2201.11653v2 Announce Type: replace  Abstract: From the point of view of the human brain, continual learning can perform various tasks without mutual interference. An effective way to reduce mutual interference can be found in sparsity and selectivity of neurons. According to Aljundi et al. and Hadsell et al., imposing sparsity at the representational level is advantageous for continual learning because sparse neuronal activations encourage less overlap between parameters, resulting in less interference. Similarly, highly selective neural networks are likely to induce less interference since particular response in neurons will reduce the chance of overlap with other parameters. Considering that the human brain performs continual learning over the lifespan, finding conditions where sparsity and selectivity naturally arises may provide insight for understanding how the brain functions. This paper investigates various conditions that naturally increase sparsity and selectivity in a 
    
[^351]: 基于方差减少的经验回放用于策略优化

    Variance Reduction Based Experience Replay for Policy Optimization

    [https://arxiv.org/abs/2110.08902](https://arxiv.org/abs/2110.08902)

    引入了基于方差减少的经验回放框架，实现了选择性重复利用相关样本来改善策略梯度估计，并构建了高效的离策略算法PG-VRER。

    

    在复杂随机系统上进行强化学习时，有效利用历史样本中的信息以加速策略优化是很有必要的。传统的经验回放虽然有效，但是将所有观测都视为相同，忽略了它们的相对重要性。为了解决这一限制，我们引入了一种新颖的方差减少经验回放（VRER）框架，实现对相关样本的选择性重复利用，从而改善策略梯度估计。VRER作为一种适应性方法，可以无缝集成到不同的策略优化算法中，构建了我们高效的离策略算法Policy Optimization with VRER (PG-VRER)。此外，文献中对经验回放方法缺乏严格的理论理解，这促使我们引入一个新颖的理论框架，考虑样本依赖性。

    arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc
    
[^352]: HCR-Net：基于深度学习的脱机手写字符识别网络

    HCR-Net: A deep learning based script independent handwritten character recognition network

    [https://arxiv.org/abs/2108.06663](https://arxiv.org/abs/2108.06663)

    HCR-Net提出了一种基于迁移学习的脱机手写字符识别网络，通过部分利用预训练网络的特征提取层，实现了更快、更高效的训练，更好的性能和泛化能力。

    

    尽管经过几十年的研究，手写字符识别（HCR）仍然是一个具有挑战性的模式识别问题，并且缺乏脱机识别技术的研究。这主要是由于相似的字符结构、不同的手写风格、不同的书写系统、手工特征提取技术、数据和代码的不可用性，以及脚本特定的深度学习技术的发展。为了解决这些限制，我们提出了一个名为HCR-Net的脱机深度学习网络，为HCR研究开辟了新的研究方向。HCR-Net基于一种新颖的用于HCR的迁移学习方法，\textit{部分利用}了预训练网络的特征提取层。由于迁移学习和图像增强，HCR-Net提供了更快、更高效的训练，更好的性能和泛化能力，并且可以处理少量数据...

    arXiv:2108.06663v4 Announce Type: replace-cross  Abstract: Handwritten character recognition (HCR) remains a challenging pattern recognition problem despite decades of research, and lacks research on script independent recognition techniques. {\color{black}This is mainly because of similar character structures, different handwriting styles, diverse scripts, handcrafted feature extraction techniques, unavailability of data and code, and the development of script-specific deep learning techniques. To address these limitations, we have proposed a script independent deep learning network for HCR research, called HCR-Net, that sets a new research direction for the field. HCR-Net is based on a novel transfer learning approach for HCR, which \textit{partly utilizes} feature extraction layers of a pre-trained network.} Due to transfer learning and image augmentation, HCR-Net provides faster and computationally efficient training, better performance and generalizations, and can work with small 
    
[^353]: 基于群稀疏矩阵分解的词嵌入传递学习

    Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings

    [https://arxiv.org/abs/2104.08928](https://arxiv.org/abs/2104.08928)

    提出了一种基于群稀疏矩阵分解的方法，用于在新领域进行词嵌入的传递学习，以解决不同领域单词含义差异的挑战。

    

    非结构化文本为许多领域的决策者提供了丰富的数据源，涵盖范围从零售中的产品评论到医疗保健中的护理记录。为了利用这些信息，通常会通过无监督学习算法（如矩阵分解）将单词转换为词嵌入——编码单词之间语义关系的向量。然而，从具有有限训练数据的新领域学习单词嵌入可能具有挑战性，因为在新领域中，单词的含义/用法可能不同，例如，“positive”一词通常具有正面情绪，但在医疗记录中往往具有负面情绪，因为它可能意味着患者检测呈阳性。在实践中，我们预计只有少量领域特定单词可能具有新含义。我们提出了一个直观的两阶段估计器，通过群稀疏惩罚来有效地传递学习领域特定的新含义。

    arXiv:2104.08928v3 Announce Type: replace-cross  Abstract: Unstructured text provides decision-makers with a rich data source in many domains, ranging from product reviews in retail to nursing notes in healthcare. To leverage this information, words are typically translated into word embeddings -- vectors that encode the semantic relationships between words -- through unsupervised learning algorithms such as matrix factorization. However, learning word embeddings from new domains with limited training data can be challenging, because the meaning/usage may be different in the new domain, e.g., the word ``positive'' typically has positive sentiment, but often has negative sentiment in medical notes since it may imply that a patient tested positive for a disease. In practice, we expect that only a small number of domain-specific words may have new meanings. We propose an intuitive two-stage estimator that exploits this structure via a group-sparse penalty to efficiently transfer learn dom
    
[^354]: RadarScenes: 用于汽车应用的实际雷达点云数据集

    RadarScenes: A Real-World Radar Point Cloud Data Set for Automotive Applications

    [https://arxiv.org/abs/2104.02493](https://arxiv.org/abs/2104.02493)

    提出了一个新的汽车雷达数据集，用于开发基于机器学习的雷达感知算法，重点关注移动道路用户。

    

    提出了一个新的汽车雷达数据集，包括超过四个小时驾驶的测量数据和点级标注。来自一辆测试车上安装的四个系列雷达传感器提供的数据被记录下来，动态目标的每个检测都被手动分组到簇中，并随后进行标记。该数据集的目的是为了促进基于机器学习的新型雷达感知算法的开发，重点放在移动道路用户上。记录序列的图像是使用记录摄像机捕获的。为了评估未来的目标检测和分类算法，提出了得分计算的建议，以便研究人员可以在一个共同的基础上评估他们的算法。额外信息以及下载说明可以在数据集的网站上找到：www.radar-scenes.com。

    arXiv:2104.02493v2 Announce Type: replace  Abstract: A new automotive radar data set with measurements and point-wise annotations from more than four hours of driving is presented. Data provided by four series radar sensors mounted on one test vehicle were recorded and the individual detections of dynamic objects were manually grouped to clusters and labeled afterwards. The purpose of this data set is to enable the development of novel (machine learning-based) radar perception algorithms with the focus on moving road users. Images of the recorded sequences were captured using a documentary camera. For the evaluation of future object detection and classification algorithms, proposals for score calculation are made so that researchers can evaluate their algorithms on a common basis. Additional information as well as download instructions can be found on the website of the data set: www.radar-scenes.com.
    
[^355]: Deep-Lock: 深度神经网络的安全授权

    Deep-Lock: Secure Authorization for Deep Neural Networks

    [https://arxiv.org/abs/2008.05966](https://arxiv.org/abs/2008.05966)

    本文提出了一种名为Deep-Lock的通用和轻量级基于密钥的模型锁定方案，通过使用S-盒对训练完毕的DNN模型的每个参数进行加密，并确保只有在应用正确的秘密密钥时模型才能正确运行，从而防止了DNN模型的未经授权使用。

    

    训练完毕的深度神经网络（DNN）模型被视为多种商业模式中的有价值的知识产权（IP）。防止IP盗窃和未经授权使用这些DNN模型已被业界提出作为一个重大关注点。本文通过提出一种通用且轻量的基于密钥的模型锁定方案—Deep-Lock，解决了防止DNN模型未经授权使用的问题，确保被锁定的模型只有在应用正确的秘密密钥时才能正确运行。Deep-Lock方案利用具有良好安全性质的S-盒对经过训练的DNN模型的每个参数进行加密，使用主密钥通过密钥调度算法生成秘密密钥。由此产生的加密权重的密集网络被发现能够抵抗模型微调攻击。最后，Deep-Lock不需要对DNN模型的结构和训练进行任何干预，使其适用于所有已存在的模型。

    arXiv:2008.05966v2 Announce Type: replace  Abstract: Trained Deep Neural Network (DNN) models are considered valuable Intellectual Properties (IP) in several business models. Prevention of IP theft and unauthorized usage of such DNN models has been raised as of significant concern by industry. In this paper, we address the problem of preventing unauthorized usage of DNN models by proposing a generic and lightweight key-based model-locking scheme, which ensures that a locked model functions correctly only upon applying the correct secret key. The proposed scheme, known as Deep-Lock, utilizes S-Boxes with good security properties to encrypt each parameter of a trained DNN model with secret keys generated from a master key via a key scheduling algorithm. The resulting dense network of encrypted weights is found robust against model fine-tuning attacks. Finally, Deep-Lock does not require any intervention in the structure and training of the DNN models, making it applicable for all existin
    
[^356]: 通过分裂诊断实现随机优化的稳健学习率选择

    Robust Learning Rate Selection for Stochastic Optimization via Splitting Diagnostic

    [https://arxiv.org/abs/1910.08597](https://arxiv.org/abs/1910.08597)

    该方法提出了SplitSGD，通过简单而有效的稳态检测，在检测到稳态阶段时降低学习速率，使其适用于凸问题和训练神经网络，表现优于其他随机优化方法。

    

    这篇论文提出了SplitSGD，这是一种用于随机优化的新的动态学习率调度方法。该方法通过在检测到稳态阶段时降低学习速率，以更好地适应目标函数的局部几何结构，即当迭代处于局部最小值附近时可能会出现反弹。通过将单线程分成两个部分，并使用两个线程梯度的内积作为稳态度量来执行检测。基于这个简单但经过验证有效的稳态检测，SplitSGD易于实现，并且基本不会比标准SGD产生额外的计算成本。通过一系列广泛的实验，我们展示了该方法既适用于凸问题，也适用于训练（非凸）神经网络，表现比其他随机优化方法更好。重要的是，观察到该方法

    arXiv:1910.08597v5 Announce Type: replace-cross  Abstract: This paper proposes SplitSGD, a new dynamic learning rate schedule for stochastic optimization. This method decreases the learning rate for better adaptation to the local geometry of the objective function whenever a stationary phase is detected, that is, the iterates are likely to bounce at around a vicinity of a local minimum. The detection is performed by splitting the single thread into two and using the inner product of the gradients from the two threads as a measure of stationarity. Owing to this simple yet provably valid stationarity detection, SplitSGD is easy-to-implement and essentially does not incur additional computational cost than standard SGD. Through a series of extensive experiments, we show that this method is appropriate for both convex problems and training (non-convex) neural networks, with performance compared favorably to other stochastic optimization methods. Importantly, this method is observed to be v
    
[^357]: AI监督和人类错误：来自中心法庭的证据

    AI Oversight and Human Mistakes: Evidence from Centre Court. (arXiv:2401.16754v1 [cs.LG])

    [http://arxiv.org/abs/2401.16754](http://arxiv.org/abs/2401.16754)

    人工智能系统在纠正人类错误方面起到了积极作用，但此举也潜在导致心理成本，并影响人的决策。通过研究网球比赛中的Hawk-Eye审查系统，我们发现引入AI监督后，裁判员的错误率下降，心理成本导致他们更倾向于将球判为进界，从而产生了类型错判的转变。

    

    在机器学习算法不断提升的驱动下，人工智能（AI）系统已经开始在许多场合用于纠正人类错误。我们提供了首个实地证据，证明这种AI监督会产生心理成本，影响人的决策。我们调查了AI监督发生的最高可见性场景之一：顶级网球比赛中裁判的Hawk-Eye审查。我们发现，引入Hawk-Eye审查后，裁判的整体错误率降低，符合心理成本被AI否定的合理忽视现象。我们还发现，裁判增加了对球入内的判定率，从而产生了从II类错误（将球判为出界，实际上是进界）到I类错误（将球判为进界，实际上是出界）的转变。通过对理性不注意的裁判模型进行心理成本的结构估计，我们的结果表明，由于AI否定的心理成本，裁判员降低了错误判定的风险并提高了球入内的判定率。

    Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because 
    
[^358]: 有效的可控偏差缓解方法，利用门适配器进行分类和检索。(arXiv:2401.16457v1 [cs.LG])

    Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters. (arXiv:2401.16457v1 [cs.LG])

    [http://arxiv.org/abs/2401.16457](http://arxiv.org/abs/2401.16457)

    本文引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，可在推理时逐渐过渡从模型的偏向状态到完全去偏的版本，并通过实验证明了其在分类和检索任务中的性能。

    

    语言模型的偏差缓解已经成为许多研究的主题，最近关注的焦点是学习独立的模块，例如适配器进行按需去偏。除了优化模块化去偏模型外，在实践中通常需要在推理时控制偏差减少的程度，例如，为了在搜索结果中调整期望的性能-公平性权衡或在分类任务中控制去偏的强度。在本文中，我们引入了可控门适配器（ConGater），一种具有可调节敏感性参数的新颖模块化门机制，允许在推理时从模型的偏向状态逐渐过渡到完全去偏的版本。通过在三个分类任务上对三个不同模型进行对抗性去偏实验，并通过公平性列表正则化来减少搜索结果的偏差，我们展示了ConGater的性能。

    Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a
    
[^359]: LLM指令微调中的提示权重实验

    Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])

    [http://arxiv.org/abs/2401.13586](http://arxiv.org/abs/2401.13586)

    LLM指令微调中，对于短提示完成数据集，提示词标记分类损失加权（PLW）与性能呈负二次关系，而长提示完成数据集则不受PLW影响。

    

    我们进行了一项小型研究，分析了提示词标记分类损失加权（PLW）如何影响在指令任务上进行微调的7B大小的LLaMA模型的性能。我们使用多个指令数据集重现了斯坦福大学的Alpaca实验，其中包括LLaMA 1和LLaMA 2。我们发现，在我们的短提示完成数据集上微调的模型与PLW之间存在负二次关系，而在长提示完成数据集上微调的模型不受PLW的影响。

    We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
    
[^360]: 使用欠阻尼 Langevin Monte Carlo 加速近似 Thompson 采样

    Accelerating Approximate Thompson Sampling with Underdamped Langevin Monte Carlo. (arXiv:2401.11665v1 [stat.ML])

    [http://arxiv.org/abs/2401.11665](http://arxiv.org/abs/2401.11665)

    本文提出了一种使用欠阻尼 Langevin Monte Carlo 加速的近似 Thompson 采样策略，通过特定势函数的设计改善了高维问题中的样本复杂度，并在高维赌博机问题中进行了验证。

    

    使用欠阻尼 Langevin Monte Carlo 的近似 Thompson 采样方法扩展了其适用范围，从高斯后验采样扩展到更一般的平滑后验。然而，在高维问题中要求高准确性时，仍然面临可扩展性问题。为了解决这个问题，我们提出了一种近似 Thompson 采样策略，利用欠阻尼 Langevin Monte Carlo，后者是模拟高维后验的通用工具。基于标准的平滑性和对数凹性条件，我们研究了使用特定势函数的加速后验集中和采样。该设计改进了实现对数遗憾的样本复杂度，从$\mathcal{\tilde O}(d)$改进到$\mathcal{\tilde O}(\sqrt{d})$。我们还通过合成实验在高维赌博机问题中经验验证了我们算法的可扩展性和鲁棒性。

    Approximate Thompson sampling with Langevin Monte Carlo broadens its reach from Gaussian posterior sampling to encompass more general smooth posteriors. However, it still encounters scalability issues in high-dimensional problems when demanding high accuracy. To address this, we propose an approximate Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where the latter is the go-to workhorse for simulations of high-dimensional posteriors. Based on the standard smoothness and log-concavity conditions, we study the accelerated posterior concentration and sampling using a specific potential function. This design improves the sample complexity for realizing logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde O}(\sqrt{d})$. The scalability and robustness of our algorithm are also empirically validated through synthetic experiments in high-dimensional bandit problems.
    
[^361]: SymTC:一种用于腰椎MRI实例分割的共生Transformer-CNN网络

    SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v1 [eess.IV])

    [http://arxiv.org/abs/2401.09627](http://arxiv.org/abs/2401.09627)

    SymTC是一种新颖的腰椎MR图像分割模型，通过将Transformer和CNN相结合，并利用位置嵌入和自注意力模块，实现了更准确的实例分割。

    

    椎间盘疾病是一种常见病症，经常导致间歇性或持续性的腰背疼痛，对该疾病的诊断和评估依赖于腰椎MR图像中椎骨和椎间盘几何形状的准确测量。深度神经网络（DNN）模型可以帮助临床医生以更高效的方式自动化地对腰椎的个体实例（椎骨和椎间盘）进行图像分割，这被称为实例图像分割。在这项工作中，我们提出了SymTC，一种创新的腰椎MR图像分割模型，它结合了Transformer和卷积神经网络（CNN）的优势。具体而言，我们设计了一个并行的双路径架构来融合CNN层和Transformer层，并在Transformer的自注意力模块中集成了一种新颖的位置嵌入，增强了位置信息的利用以实现更准确的分割。为了进一步提高模型的性能，我们引入了一个新的定位系统进行模型优化。

    Intervertebral disc disease, a prevalent ailment, frequently leads to intermittent or persistent low back pain, and diagnosing and assessing of this disease rely on accurate measurement of vertebral bone and intervertebral disc geometries from lumbar MR images. Deep neural network (DNN) models may assist clinicians with more efficient image segmentation of individual instances (disks and vertebrae) of the lumbar spine in an automated way, which is termed as instance image segmentation. In this work, we proposed SymTC, an innovative lumbar spine MR image segmentation model that combines the strengths of Transformer and Convolutional Neural Network (CNN). Specifically, we designed a parallel dual-path architecture to merge CNN layers and Transformer layers, and we integrated a novel position embedding into the self-attention module of Transformer, enhancing the utilization of positional information for more accurate segmentation. To further improves model performance, we introduced a new
    
[^362]: 技术报告：关于节点不可访问情况下流言学习收敛性的研究

    Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])

    [http://arxiv.org/abs/2401.09498](http://arxiv.org/abs/2401.09498)

    本文研究了在动态网络拓扑下，不可访问节点对流言学习的收敛性的影响，并提供了理论分析。

    

    Gossip learning（GL）作为分散式学习的一种替代方法，更适用于资源受限的无线网络，如由无人机（UAV）组成的FANETs。GL能够显著提高UAV网络的效率并延长电池寿命。尽管具有这些优势，但GL的性能受数据分布、通信速度和网络连接性的影响较大。然而，这些因素如何影响GL的收敛性仍不清楚。现有研究基于虚拟数量来研究GL的收敛性，以方便性而忽略了当一些节点不可访问时网络的真实状态。在本文中，我们对动态网络拓扑下不可访问节点对GL的影响进行了建模和研究。首先，我们将权重发散分解为节点是否可访问的情况。然后，我们研究了在节点可访问性的动态下GL的收敛性，并在理论上提供了

    Gossip learning (GL), as a decentralized alternative to federated learning (FL), is more suitable for resource-constrained wireless networks, such as FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly enhance the efficiency and extend the battery life of UAV networks. Despite the advantages, the performance of GL is strongly affected by data distribution, communication speed, and network connectivity. However, how these factors influence the GL convergence is still unclear. Existing work studied the convergence of GL based on a virtual quantity for the sake of convenience, which fail to reflect the real state of the network when some nodes are inaccessible. In this paper, we formulate and investigate the impact of inaccessible nodes to GL under a dynamic network topology. We first decompose the weight divergence by whether the node is accessible or not. Then, we investigate the GL convergence under the dynamic of node accessibility and theoretically provide
    
[^363]: 从部分观测中进行空间和时间连续的物理模拟

    Space and Time Continuous Physics Simulation From Partial Observations. (arXiv:2401.09198v1 [cs.LG])

    [http://arxiv.org/abs/2401.09198](http://arxiv.org/abs/2401.09198)

    本研究提出了一种新颖的方法，可以从部分观测中进行连续的空间和时间物理模拟，并解决了基于固定支持网格的传统方法的缺点。这种方法通过在稀疏观测上进行训练，利用两个相互关联的动力系统在稀疏位置和连续域上进行预测和插值求解。

    

    现代物理模拟技术依赖于数值方案和网格细化方法来解决精度和复杂性之间的权衡，但这些手工解决方案繁琐且需要高计算能力。基于大规模机器学习的数据驱动方法通过更直接和高效地集成长距离依赖来实现高适应性。在这项工作中，我们主要关注流体动力学，并解决了大部分文献中存在的问题，即计算和预测形式为常规或非规则网格的固定支持。我们提出了一种新颖的设置，可以在连续的空间和时间域中进行预测，同时在稀疏观测上进行训练。我们将这个任务形式化为双观测问题，并提出了一个解决方案，其中在稀疏位置和连续域上定义了两个相互关联的动力系统，可以从初始条件进行预测和插值求解。

    Modern techniques for physical simulations rely on numerical schemes and mesh-refinement methods to address trade-offs between precision and complexity, but these handcrafted solutions are tedious and require high computational power. Data-driven methods based on large-scale machine learning promise high adaptivity by integrating long-range dependencies more directly and efficiently. In this work, we focus on fluid dynamics and address the shortcomings of a large part of the literature, which are based on fixed support for computations and predictions in the form of regular or irregular grids. We propose a novel setup to perform predictions in a continuous spatial and temporal domain while being trained on sparse observations. We formulate the task as a double observation problem and propose a solution with two interlinked dynamical systems defined on, respectively, the sparse positions and the continuous domain, which allows to forecast and interpolate a solution from the initial cond
    
[^364]: 评估用于AI辅助图像标注的符合预测集的效用

    Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image Labeling. (arXiv:2401.08876v1 [cs.HC])

    [http://arxiv.org/abs/2401.08876](http://arxiv.org/abs/2401.08876)

    本研究评估了符合预测集在AI辅助图像标注中的效用，发现对于简单图像，预测集与Top-1和Top-k显示的准确性相当，但在标记分布外图像时特别有效，尤其是集合大小较小时。

    

    随着深度神经网络在高风险领域中越来越常见，它们的缺乏可解释性使得不确定性量化变得具有挑战性。我们研究了用于表示AI辅助决策中的不确定性的符合预测集的效果。通过一项大型预注册实验，我们比较了符合预测集和显示Top-1和Top-k预测在AI辅助图像标注中的效用。我们发现，对于简单的图像，预测集的准确性与Top-1和Top-k显示相当或稍低，但在标记分布外（OOD）图像时，尤其是当集合大小较小时，预测集在辅助人类标注方面表现出色。我们的结果在实践中强调了符合预测集的实际挑战，并提供了相关建议。

    As deep neural networks are more commonly deployed in high-stakes domains, their lack of interpretability makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets$\unicode{x2013}$a method for generating valid confidence sets in distribution-free uncertainty quantification$\unicode{x2013}$to express uncertainty in AI-advised decision-making. Through a large pre-registered experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-k predictions for AI-advised image labeling. We find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-k displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images especially when the set size is small. Our results empirically pinpoint the practical challenges of conformal prediction sets and provide implications 
    
[^365]: 对知识蒸馏中参数选择的实证研究

    An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation. (arXiv:2401.06356v1 [cs.LG])

    [http://arxiv.org/abs/2401.06356](http://arxiv.org/abs/2401.06356)

    本文对知识蒸馏中参数选择的影响进行了大规模实证研究，揭示了不同选项对学生性能的整体影响，并找到了在各方面表现良好的单一配置。

    

    我们展示了一项关于参数配置选择对知识蒸馏性能影响的大规模实证研究。其中一个示例是教师和学生预测之间距离的度量，在此方面常见的选择包括均方误差（MSE）和KL散度。尽管已经进行了一些散乱的努力来理解这些选项之间的差异，但是知识蒸馏领域仍然缺乏一个对它们对学生性能的整体影响进行系统研究的工作。我们在这篇论文中采用实证方法来探索这个问题，试图找出这些选择在包括4个NLP任务和3种学生规模的13个数据集上对学生性能的影响程度。我们衡量了做出次优选择的代价，并确定了一个在各方面表现良好的单一配置。

    We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board.
    
[^366]: 迭代正则化与k支撑范数：稀疏恢复的重要补充

    Iterative Regularization with k-Support Norm: an Important Complement to Sparse Recovery. (arXiv:2401.05394v1 [eess.SP])

    [http://arxiv.org/abs/2401.05394](http://arxiv.org/abs/2401.05394)

    该论文介绍了一种新的迭代正则化算法IRKSN，它通过使用$k$支撑范数正则化实现稀疏恢复，并提供了条件。这是对基于$\ell_1$范数的迭代方法的一种重要补充。

    

    稀疏恢复在机器学习和信号处理中无处不在。由于稀疏恢复的NP困难性质，现有方法通常要么受限于适用条件（甚至未知），要么计算成本高。最近，迭代正则化方法作为一种快速方法出现，因为它们可以通过提前停止一次通过来实现稀疏恢复，而不是传统方法中繁琐的网格搜索。然而，大多数这些迭代方法都基于$\ell_1$范数，需要受限的适用条件，并且在许多情况下可能会失败。因此，迭代正则化方法在更广泛的条件下实现稀疏恢复仍需进一步探索。为了解决这个问题，我们提出了一种新的迭代正则化算法IRKSN，它基于$k$支撑范数正则化而不是$\ell_1$范数。我们提供了使用IRKSN进行稀疏恢复的条件，并进行了比较。

    Sparse recovery is ubiquitous in machine learning and signal processing. Due to the NP-hard nature of sparse recovery, existing methods are known to suffer either from restrictive (or even unknown) applicability conditions, or high computational cost. Recently, iterative regularization methods have emerged as a promising fast approach because they can achieve sparse recovery in one pass through early stopping, rather than the tedious grid-search used in the traditional methods. However, most of those iterative methods are based on the $\ell_1$ norm which requires restrictive applicability conditions and could fail in many cases. Therefore, achieving sparse recovery with iterative regularization methods under a wider range of conditions has yet to be further explored. To address this issue, we propose a novel iterative regularization algorithm, IRKSN, based on the $k$-support norm regularizer rather than the $\ell_1$ norm. We provide conditions for sparse recovery with IRKSN, and compar
    
[^367]: 人作为AI导师：增强人机协作强化学习以实现安全高效的自动驾驶

    Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])

    [http://arxiv.org/abs/2401.03160](http://arxiv.org/abs/2401.03160)

    本文提出了一种增强的人机协作强化学习方法，通过将人类智能注入到AI中实现混合交通编队中的安全高效自动驾驶。该方法将人类专家作为导师，允许代理在不确定环境中进行探索，同时在危险情况下接管控制以避免事故，并指导代理减小交通流干扰，优化交通流效果。

    

    尽管自动驾驶车辆（AVs）取得了重大进展，但确保AVs的安全性和交通流效率的驾驶策略的发展尚未得到充分探索。在本文中，我们提出了一种增强的人机协作强化学习方法，称为基于人作为AI导师的深度强化学习（HAIM-DRL）框架，以在混合交通编队中实现安全高效的自动驾驶。从人类学习过程中汲取灵感，我们首先引入了一种创新的学习范式，有效地将人类智能注入到AI中，称为人作为AI导师（HAIM）。在这个范式中，人类专家作为导师为AI代理提供帮助。在允许代理在不确定环境中进行充分探索的同时，人类专家可以在危险情况下接管控制，并展示正确的行动以避免潜在事故。另一方面，可以指导代理减小交通流干扰，从而优化交通流效果。

    Despite significant progress in autonomous vehicles (AVs), the development of driving policies that ensure both the safety of AVs and traffic flow efficiency has not yet been fully explored. In this paper, we propose an enhanced human-in-the-loop reinforcement learning method, termed the Human as AI mentor-based deep reinforcement learning (HAIM-DRL) framework, which facilitates safe and efficient autonomous driving in mixed traffic platoon. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves as a mentor to the AI agent. While allowing the agent to sufficiently explore uncertain environments, the human expert can take control in dangerous situations and demonstrate correct actions to avoid potential accidents. On the other hand, the agent could be guided to minimize traffic flow disturbance, thereby optimizi
    
[^368]: 扩散变分推断：扩散模型作为表达性变分后验

    Diffusion Variational Inference: Diffusion Models as Expressive Variational Posteriors. (arXiv:2401.02739v1 [cs.LG])

    [http://arxiv.org/abs/2401.02739](http://arxiv.org/abs/2401.02739)

    本文提出了去噪扩散变分推断（DDVI）算法，该算法使用扩散模型作为表达性变分后验，并通过反转加噪过程在潜空间中进行扩散。该方法易于实现，兼容黑盒变分推断，并在深度潜变量模型中的任务中表现优异。

    

    我们提出了去噪扩散变分推断（DDVI），一种用扩散模型作为表达性变分后验的潜变量模型的近似推断算法。我们的方法通过辅助潜变量增加了变分后验，从而得到一个表达性的模型类，通过反转用户指定的加噪过程在潜空间中进行扩散。我们通过优化一个受到觉醒-睡眠算法启发的边际似然新下界来拟合这些模型。我们的方法易于实现（它适配了正则化的ELBO扩展），与黑盒变分推断兼容，并且表现优于基于归一化流或对抗网络的替代近似后验类别。将我们的方法应用于深度潜变量模型时，我们的方法得到了去噪扩散变分自动编码器（DD-VAE）算法。我们将该算法应用于生物学中的一个激励任务 -- 从人类基因组中推断潜在血统 -- 超过了强基线模型。

    We propose denoising diffusion variational inference (DDVI), an approximate inference algorithm for latent variable models which relies on diffusion models as expressive variational posteriors. Our method augments variational posteriors with auxiliary latents, which yields an expressive class of models that perform diffusion in latent space by reversing a user-specified noising process. We fit these models by optimizing a novel lower bound on the marginal likelihood inspired by the wake-sleep algorithm. Our method is easy to implement (it fits a regularized extension of the ELBO), is compatible with black-box variational inference, and outperforms alternative classes of approximate posteriors based on normalizing flows or adversarial networks. When applied to deep latent variable models, our method yields the denoising diffusion VAE (DD-VAE) algorithm. We use this algorithm on a motivating task in biology -- inferring latent ancestry from human genomes -- outperforming strong baselines
    
[^369]: 强化学习中的消除学习

    Reinforcement Unlearning. (arXiv:2312.15910v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.15910](http://arxiv.org/abs/2312.15910)

    强化学习中的消除学习是一种新兴的研究领域，旨在解决环境所有者有权撤销智能体训练数据的隐私问题。该领域面临三个主要挑战。

    

    机器消除学习指的是根据数据所有者的请求，降低特定训练数据对机器学习模型的影响的过程。然而，在消除学习的研究中，一个重要的领域往往被忽视，那就是强化学习。强化学习旨在训练一个智能体在环境中做出最优决策以最大化累积奖励。在训练过程中，智能体往往会记忆环境的特征，这引发了一个重大的隐私问题。根据数据保护法规，环境的所有者有权撤销智能体的训练数据的访问权限，因此需要开展一个新颖且紧迫的研究领域，即“强化消除学习”。强化消除学习侧重于撤销整个环境而不是单个数据样本。这一独特特征带来了三个不同的挑战：1）如何提出消除学习方案

    Machine unlearning refers to the process of mitigating the influence of specific training data on machine learning models based on removal requests from data owners. However, one important area that has been largely overlooked in the research of unlearning is reinforcement learning. Reinforcement learning focuses on training an agent to make optimal decisions within an environment to maximize its cumulative rewards. During the training, the agent tends to memorize the features of the environment, which raises a significant concern about privacy. As per data protection regulations, the owner of the environment holds the right to revoke access to the agent's training data, thus necessitating the development of a novel and pressing research field, known as \emph{reinforcement unlearning}. Reinforcement unlearning focuses on revoking entire environments rather than individual data samples. This unique characteristic presents three distinct challenges: 1) how to propose unlearning schemes f
    
[^370]: LLMs能够修复安全问题吗？

    Can LLMs Patch Security Issues?. (arXiv:2312.00024v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2312.00024](http://arxiv.org/abs/2312.00024)

    本文提出了一种新的方法, Feedback-Driven Solution Synthesis (FDSS), 旨在通过将LLMs与静态代码分析工具Bandit结合，解决代码中的安全漏洞问题。该方法在现有方法的基础上有显著改进，并引入了一个新的数据集PythonSecurityEval。

    

    大型语言模型(LLMs)在代码生成方面显示出了令人印象深刻的能力。然而，类似于人类开发者，这些模型可能会生成包含安全漏洞和缺陷的代码。编写安全代码仍然是一个重大挑战，因为漏洞通常在程序与外部系统或服务（如数据库和操作系统）之间的交互过程中出现。在本文中，我们提出了一种新颖的方法，即基于反馈的解决方案合成（FDSS），旨在探索使用LLMs接收来自静态代码分析工具Bandit的反馈，然后LLMs生成潜在解决方案来解决安全漏洞。每个解决方案以及易受攻击的代码随后被送回LLMs进行代码完善。我们的方法在基线上表现出显著改进，并优于现有方法。此外，我们引入了一个新的数据集PythonSecurityEval，该数据集收集了来自Stack Overflow的真实场景数据。

    Large Language Models (LLMs) have shown impressive proficiency in code generation. Nonetheless, similar to human developers, these models might generate code that contains security vulnerabilities and flaws. Writing secure code remains a substantial challenge, as vulnerabilities often arise during interactions between programs and external systems or services, such as databases and operating systems. In this paper, we propose a novel approach, Feedback-Driven Solution Synthesis (FDSS), designed to explore the use of LLMs in receiving feedback from Bandit, which is a static code analysis tool, and then the LLMs generate potential solutions to resolve security vulnerabilities. Each solution, along with the vulnerable code, is then sent back to the LLM for code refinement. Our approach shows a significant improvement over the baseline and outperforms existing approaches. Furthermore, we introduce a new dataset, PythonSecurityEval, collected from real-world scenarios on Stack Overflow to e
    
[^371]: SalUn：通过基于梯度的权重显著性增强机器遗忘在图像分类和生成中的效果

    SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. (arXiv:2310.12508v1 [cs.LG])

    [http://arxiv.org/abs/2310.12508](http://arxiv.org/abs/2310.12508)

    这篇论文提出了一种名为SalUn的机器遗忘方法，通过引入"权重显著性"的概念，将关注点从整个模型引导到具体的模型权重上，提高了遗忘的效果和效率。这是第一个能够有效消除遗忘数据、类别或概念影响的有原则的机器遗忘方法。

    

    随着数据法规的不断发展，机器遗忘（MU）已成为增强当前AI模型的信任和安全性的重要工具。然而，现有的MU方法通常在遗忘精度、稳定性和跨领域适用性方面存在局限。为了解决这些挑战，我们引入了MU中的“权重显著性”概念，借鉴了模型解释中的输入显著性。这一创新将MU的关注点从整个模型引导到了具体的模型权重上，提高了其效果和效率。我们称之为显著性遗忘（SalUn）的方法将其与“精确”遗忘（在删除遗忘数据集后从头开始重新训练模型）的性能差距缩小。据我们所知，SalUn是第一个能够在图像分类和生成中有效消除遗忘数据、类别或概念影响的有原则的MU方法。例如，SalUn可在图片分类和生成任务中擦除遗忘数据、类别或概念。

    With evolving data regulations, machine unlearning (MU) has become an important tool for fostering trust and safety in today's AI models. However, existing MU methods focusing on data and/or weight perspectives often grapple with limitations in unlearning accuracy, stability, and cross-domain applicability. To address these challenges, we introduce the concept of 'weight saliency' in MU, drawing parallels with input saliency in model explanation. This innovation directs MU's attention toward specific model weights rather than the entire model, improving effectiveness and efficiency. The resultant method that we call saliency unlearning (SalUn) narrows the performance gap with 'exact' unlearning (model retraining from scratch after removing the forgetting dataset). To the best of our knowledge, SalUn is the first principled MU approach adaptable enough to effectively erase the influence of forgetting data, classes, or concepts in both image classification and generation. For example, Sa
    
[^372]: 加速策略梯度：关于应用Nesterov动量在强化学习中的论文

    Accelerated Policy Gradient: On the Nesterov Momentum for Reinforcement Learning. (arXiv:2310.11897v1 [cs.LG])

    [http://arxiv.org/abs/2310.11897](http://arxiv.org/abs/2310.11897)

    本文介绍了一种名为加速策略梯度的方法，通过应用Nesterov动量在强化学习中实现更快的全局收敛速度。通过在softmax策略参数化中收敛到最优策略，它以 $\tilde{O}(1/t^2)$ 的速率收敛。这是第一个基于Nesterov加速梯度在强化学习中全局收敛速率的研究。

    

    最近研究表明，策略梯度方法在非正则化表格softmax设置中以Θ(1/t)的速率全局收敛。因此，一个重要的研究问题是是否可以通过仅使用一阶更新进一步改进这种收敛速度。本文从动量的角度回答了上述问题，通过将著名的Nesterov加速梯度（NAG）方法应用于强化学习（RL），称之为 \textit{加速策略梯度}（APG）。为了展示APG在实现更快全局收敛方面的潜力，我们正式证明了使用真实梯度时，具有 softmax 策略参数化的APG以 $\tilde{O}(1/t^2)$ 的速率收敛到最优策略。据我们所知，这是NAG在RL领域中全局收敛速率的第一个表征。值得注意的是，我们的分析依赖于一个有趣的发现：不论初始化如何，APG最终可以达到近乎局部收敛的地方。

    Policy gradient methods have recently been shown to enjoy global convergence at a $\Theta(1/t)$ rate in the non-regularized tabular softmax setting. Accordingly, one important research question is whether this convergence rate can be further improved, with only first-order updates. In this paper, we answer the above question from the perspective of momentum by adapting the celebrated Nesterov's accelerated gradient (NAG) method to reinforcement learning (RL), termed \textit{Accelerated Policy Gradient} (APG). To demonstrate the potential of APG in achieving faster global convergence, we formally show that with the true gradient, APG with softmax policy parametrization converges to an optimal policy at a $\tilde{O}(1/t^2)$ rate. To the best of our knowledge, this is the first characterization of the global convergence rate of NAG in the context of RL. Notably, our analysis relies on one interesting finding: Regardless of the initialization, APG could end up reaching a locally nearly-con
    
[^373]: 关于强化学习中目标规范形式的表达能力

    On The Expressivity of Objective-Specification Formalisms in Reinforcement Learning. (arXiv:2310.11840v1 [cs.LG])

    [http://arxiv.org/abs/2310.11840](http://arxiv.org/abs/2310.11840)

    这项工作通过对强化学习中17种目标规范形式的表达能力进行全面比较，填补了现有文献中的空白。通过将这些形式化方法进行预排序，并呈现为哈斯图，我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。

    

    要解决强化学习任务，必须对该任务的目标进行形式化规定。尽管大多数强化学习算法要求将目标形式化为马尔可夫奖励函数，但已经开发出了其他替代方法（如线性时间逻辑和多目标强化学习）。此外，众所周知，其中一些形式化方法能够表达其他形式化方法无法表达的特定任务。然而，目前还没有对这些形式化方法在表达能力方面如何相互关联进行全面分析。在本研究中，我们通过对强化学习中17种目标规范形式的表达能力进行全面比较填补了现有文献中的空白。我们将这些形式化方法根据其表达能力进行预排序，并将该预排序呈现为哈斯图。我们发现不同形式化方法存在各种限制，并且没有一种形式化方法既具有主导性的表达能力又容易优化。

    To solve a task with reinforcement learning (RL), it is necessary to formally specify the goal of that task. Although most RL algorithms require that the goal is formalised as a Markovian reward function, alternatives have been developed (such as Linear Temporal Logic and Multi-Objective Reinforcement Learning). Moreover, it is well known that some of these formalisms are able to express certain tasks that other formalisms cannot express. However, there has not yet been any thorough analysis of how these formalisms relate to each other in terms of expressivity. In this work, we fill this gap in the existing literature by providing a comprehensive comparison of the expressivities of 17 objective-specification formalisms in RL. We place these formalisms in a preorder based on their expressive power, and present this preorder as a Hasse diagram. We find a variety of limitations for the different formalisms, and that no formalism is both dominantly expressive and straightforward to optimis
    
[^374]: 面向隐私保护推荐的联邦异构图神经网络

    Federated Heterogeneous Graph Neural Network for Privacy-preserving Recommendation. (arXiv:2310.11730v1 [cs.LG])

    [http://arxiv.org/abs/2310.11730](http://arxiv.org/abs/2310.11730)

    本文提出了一种联邦异构图神经网络（FedHGNN）的框架，能够在分布式的异构信息网络上协同训练推荐模型，同时保护用户隐私。

    

    异构信息网络（HIN）通过元路径描述丰富的语义，已成为缓解推荐系统数据稀疏性的强大工具。现有的基于HIN的推荐系统持有数据的集中存储假设，并进行集中式模型训练。然而，由于隐私问题，现实世界的数据往往以分布式方式存储，导致集中式HIN推荐无法实现。本文提出将HIN分为客户端存储的私有HIN和服务器端的共享HIN。在此设置下，我们提出了一种基于联邦异构图神经网络（FedHGNN）的框架，可以在分布式HIN上协作训练推荐模型，同时不泄露用户隐私。具体而言，我们首先针对基于HIN的联合推荐，基于差分隐私的光下确定了隐私定义，旨在保护私有HIN的用户-商品交互，以及用户的隐私信息。

    Heterogeneous information network (HIN), which contains rich semantics depicted by meta-paths, has become a powerful tool to alleviate data sparsity in recommender systems. Existing HIN-based recommendations hold the data centralized storage assumption and conduct centralized model training. However, the real-world data is often stored in a distributed manner for privacy concerns, resulting in the failure of centralized HIN-based recommendations. In this paper, we suggest the HIN is partitioned into private HINs stored in the client side and shared HINs in the server. Following this setting, we propose a federated heterogeneous graph neural network (FedHGNN) based framework, which can collaboratively train a recommendation model on distributed HINs without leaking user privacy. Specifically, we first formalize the privacy definition in the light of differential privacy for HIN-based federated recommendation, which aims to protect user-item interactions of private HIN as well as user's 
    
[^375]: 敏感性感知的摊销贝叶斯推断

    Sensitivity-Aware Amortized Bayesian Inference. (arXiv:2310.11122v1 [stat.ML])

    [http://arxiv.org/abs/2310.11122](http://arxiv.org/abs/2310.11122)

    本文提出了一种敏感性感知的摊销贝叶斯推断方法，通过权重共享和神经网络来进行似然和先验规范的训练，以及对数据扰动和预处理程序的敏感性评估。

    

    贝叶斯推断是在不确定性下进行概率推理和决策的强大框架。现代贝叶斯工作流程中的基本选择涉及似然函数和先验分布的规范、后验逼近器和数据。每个选择都可以显着影响基于模型的推断和后续决策，因此需要进行敏感性分析。在这项工作中，我们提出了一种多方面的方法，将敏感性分析整合到摊销贝叶斯推断（ABI，即基于神经网络的模拟推断）中。首先，我们利用权重共享在训练过程中编码替代似然和先验规范之间的结构相似性，以最小的计算开销。其次，我们利用神经网络的快速推断来评估对各种数据扰动或预处理程序的敏感性。与大多数其他贝叶斯方法相比，这两个步骤都避免了昂贵的计算。

    Bayesian inference is a powerful framework for making probabilistic inferences and decisions under uncertainty. Fundamental choices in modern Bayesian workflows concern the specification of the likelihood function and prior distributions, the posterior approximator, and the data. Each choice can significantly influence model-based inference and subsequent decisions, thereby necessitating sensitivity analysis. In this work, we propose a multifaceted approach to integrate sensitivity analyses into amortized Bayesian inference (ABI, i.e., simulation-based inference with neural networks). First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to various data perturbations or pre-processing procedures. In contrast to most other Bayesian approaches, both steps circumvent the costly
    
[^376]: 大型语言模型的去学习研究

    Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])

    [http://arxiv.org/abs/2310.10683](http://arxiv.org/abs/2310.10683)

    大型语言模型的去学习是一个研究的新领域，我们探索了三个场景，可以通过去学习让语言模型与人类偏好保持一致。去学习具有三个优势，只需要负面示例，计算效率高，特别对于知道具体导致不良行为的训练样本更为有效。

    

    我们研究了如何对大型语言模型（LLMs）进行去学习，即忘记不受欢迎的（非）行为。我们展示了至少三种情境可以从去学习中使LLMs与人类偏好保持一致：（1）删除有害回复，（2）按要求删除受版权保护的内容，以及（3）消除幻觉。作为对齐技术的一种，去学习具有三个优点：（1）只需要负面（例如有害）示例，这比在RLHF（基于人类反馈的强化学习）中所需的正面（例如有帮助且通常由人类编写）示例更容易和更便宜地收集（例如通过红队测试或用户报告）；（2）计算效率高；（3）当我们知道哪些训练样本导致了不良行为时，它特别有效。据我们所知，我们的工作是首次探索LLM去学习的工作之一。我们也是首次在LLM去学习中制定了设置、目标和评估。我们表明，如果从业者只有有限的

    We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
    
[^377]: 组合能力以乘法方式出现：在合成任务中探索扩散模型

    Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])

    [http://arxiv.org/abs/2310.09336](http://arxiv.org/abs/2310.09336)

    组合能力以乘法方式出现：研究了条件扩散模型在合成任务中的组合泛化能力，结果显示这种能力受到底层数据生成过程的结构影响，且模型在学习到更高级的组合时存在困难。

    

    现代生成模型展示出了产生极为逼真数据的前所未有的能力。然而，考虑到现实世界的自然组合性，这些模型在实际应用中可靠使用需要展示出能够组合新的概念集合以生成训练数据集中未见的输出的能力。先前的研究表明，最近的扩散模型确实表现出了有趣的组合泛化能力，但它们也会出现无法预测的失败。受此启发，我们在合成环境中进行了有控制性的研究，以了解条件扩散模型的组合泛化能力，我们变化了训练数据的不同属性并测量了模型生成越界样本的能力。我们的结果显示：（i）从一个概念生成样本的能力和将它们组合起来的能力的出现顺序受到了底层数据生成过程的结构的影响；（ii）在组合任务上的表现表明模型在学习到更高级的组合时存在困难。

    Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhib
    
[^378]: 理解RLHF对LLM泛化和多样性的影响

    Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.06452](http://arxiv.org/abs/2310.06452)

    本研究深入分析了强化学习从人类反馈中调整的大型语言模型每个阶段对超出分布泛化和输出多样性的影响。

    

    在最广泛使用的AI模型中，如OpenAI的ChatGPT或Anthropic的Claude，使用强化学习从人类反馈中调整的大型语言模型（LLM）。尽管在这些方法的开发方面有大量的研究，但我们对RLHF过程中每个阶段的利与弊的理解仍然有限。为了填补这一空白，我们对每个阶段（即监督微调（SFT），奖励建模和RLHF）如何影响两个关键属性进行了全面分析：超出分布的泛化和输出多样性。在这些模型被广泛应用于真实世界中的各种情景的背景下，超出分布的泛化非常重要，而输出多样性指的是模型生成各种不同输出的能力，对于各种用例来说都非常重要。我们在摘要和指令遵循任务中对两个基本模型进行了分析，后者非常相关。

    Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
    
[^379]: 关于使用LSTD和随机特征的强化学习中的双下降现象

    On Double-Descent in Reinforcement Learning with LSTD and Random Features. (arXiv:2310.05518v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05518](http://arxiv.org/abs/2310.05518)

    本文研究了在强化学习中网络大小和L2正则化对性能的影响，并观察到了双下降现象。通过使用随机特征和懒惰训练策略，在参数和状态数无限大的情况下研究了正则化的最小二乘时间差分算法，得出了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    

    时间差分算法在深度强化学习中被广泛使用，其性能受神经网络大小的影响。然而，在监督学习中过参数化和其带来的好处已经得到了很好的理解，但是在强化学习中情况则不太清楚。本文通过理论分析探讨了网络大小和L2正则化对性能的影响，并将参数个数与访问状态个数之比定义为关键因素，当该比值大于1时称为过参数化。此外，我们观察到了双下降现象，即在参数/状态比为1附近会突然性能下降。通过利用随机特征和懒惰训练策略，我们在无限大的参数和状态数下研究了正则化的最小二乘时间差分算法。我们推导了其收敛性和最优性，并阐述了双下降现象在该算法中的影响。

    Temporal Difference (TD) algorithms are widely used in Deep Reinforcement Learning (RL). Their performance is heavily influenced by the size of the neural network. While in supervised learning, the regime of over-parameterization and its benefits are well understood, the situation in RL is much less clear. In this paper, we present a theoretical analysis of the influence of network size and $l_2$-regularization on performance. We identify the ratio between the number of parameters and the number of visited states as a crucial factor and define over-parameterization as the regime when it is larger than one. Furthermore, we observe a double-descent phenomenon, i.e., a sudden drop in performance around the parameter/state ratio of one. Leveraging random features and the lazy training regime, we study the regularized Least-Square Temporal Difference (LSTD) algorithm in an asymptotic regime, as both the number of parameters and states go to infinity, maintaining a constant ratio. We derive 
    
[^380]: Entropy-MCMC: 轻松从平坦盆地进行采样

    Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05401](http://arxiv.org/abs/2310.05401)

    本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。

    

    贝叶斯深度学习依赖于对后验分布的质量估计。然而，深度神经网络的后验分布在性质上是高度多模态的，局部模式表现出不同的泛化性能。在有限的计算资源下，从原始后验分布中进行采样可能会导致次优性能，因为一些样本可能会陷入“坏”模式并出现过拟合。基于观察到低泛化误差的“好”模式通常存在于能量景观的平坦盆地中，我们提出通过偏置采样朝向这些平坦区域的后验。具体而言，我们引入了一个辅助引导变量，其稳态分布类似于平滑后验分布，并且没有尖锐的模态，以引导MCMC采样器在平坦的盆地中采样。通过将此引导变量与模型参数相结合，我们创建了一个简单的联合分布，可以在最小计算开销下实现高效采样。我们证明了我们的元算法的收敛性。

    Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
    
[^381]: "垃圾DNA假设：通过稀疏性对LLM预训练权重进行任务中心角度分析"

    Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity. (arXiv:2310.02277v1 [cs.LG])

    [http://arxiv.org/abs/2310.02277](http://arxiv.org/abs/2310.02277)

    本文研究通过稀疏性分析LLM预训练权重的任务中心角度，挑战了传统对于权重中冗余性的观点，并提出了"垃圾DNA假设"。

    

    传统对"垃圾DNA"的概念长期以来与人类基因组中的非编码片段相关联，占其组成的大约98%。然而，最近的研究揭示了一些这些看似无功能的DNA序列在细胞过程中起到的关键作用。有趣的是，深度神经网络中的权重与人类基因中观察到的冗余性有着显著的相似性。人们认为，庞大模型中的权重包含了过多的冗余，可以在不影响性能的情况下去除。本文通过提出一个令人信服的反论来挑战这个传统观点。我们使用稀疏性作为一种工具，来独立而准确地量化预训练大语言模型(LLM)中低幅度权重的细微重要性，从下游任务中心的角度理解它们包含的知识。我们提出了支持我们深入研究的"垃圾DNA假设"。

    The traditional notion of "Junk DNA" has long been linked to non-coding segments within the human genome, constituting roughly 98% of its composition. However, recent research has unveiled the critical roles some of these seemingly non-functional DNA sequences play in cellular processes. Intriguingly, the weights within deep neural networks exhibit a remarkable similarity to the redundancy observed in human genes. It was believed that weights in gigantic models contained excessive redundancy, and could be removed without compromising performance. This paper challenges this conventional wisdom by presenting a compelling counter-argument. We employ sparsity as a tool to isolate and quantify the nuanced significance of low-magnitude weights in pre-trained large language models (LLMs). Our study demonstrates a strong correlation between these weight magnitudes and the knowledge they encapsulate, from a downstream task-centric angle. we raise the "Junk DNA Hypothesis" backed by our in-depth
    
[^382]: 为什么自编码器起作用？

    Why do autoencoders work?. (arXiv:2310.02250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02250](http://arxiv.org/abs/2310.02250)

    自编码器是一种深度神经网络模型，通过调整参数实现输入数据和重构输出之间的最小差异，用于识别数据在高维空间中的内在维度，并且对于某些拓扑结构，存在难以找到完美重构网络的限制。

    

    深度神经网络自编码器被广泛用于模型压缩。它们可以识别数据在输入的欧几里德空间R^n中，位于k维子集K中的内在维度。其基本思想是获得一个将R^n映射为R^k的编码层（称为瓶颈层或潜变量空间），以及一个将R^k映射回R^n的解码层，使得在组合这两个映射时可以恢复来自集合K的输入数据。这通过调整网络中的参数（权重）来最小化输入和重构输出之间的差异来实现。由于神经网络（具有连续激活函数）计算连续映射，实现完美重构的网络的存在将意味着K在R^k中是一个k维子集的同胚，因此明显存在拓扑障碍来寻找这样的网络。

    Deep neural network autoencoders are routinely used computationally for model reduction. They allow recognizing the intrinsic dimension of data that lie in a $k$-dimensional subset $K$ of an input Euclidean space $\mathbb{R}^n$. The underlying idea is to obtain both an encoding layer that maps $\mathbb{R}^n$ into $\mathbb{R}^k$ (called the bottleneck layer or the space of latent variables) and a decoding layer that maps $\mathbb{R}^k$ back into $\mathbb{R}^n$, in such a way that the input data from the set $K$ is recovered when composing the two maps. This is achieved by adjusting parameters (weights) in the network to minimize the discrepancy between the input and the reconstructed output. Since neural networks (with continuous activation functions) compute continuous maps, the existence of a network that achieves perfect reconstruction would imply that $K$ is homeomorphic to a $k$-dimensional subset of $\mathbb{R}^k$, so clearly there are topological obstructions to finding such a ne
    
[^383]: 超越熟悉特征的深度异常检测

    Going Beyond Familiar Features for Deep Anomaly Detection. (arXiv:2310.00797v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00797](http://arxiv.org/abs/2310.00797)

    该论文提出了一种超越熟悉特征的深度异常检测方法，通过利用可解释性在输入空间中捕捉新颖特征来避免假阴性。该方法在广泛的异常基准测试中取得了强大的性能，并且消除了昂贵的背景模型和密集匹配的需求。

    

    异常检测（AD）是一项重要任务，涉及识别不符合已学习的正常模型的观察结果。之前的深度AD工作主要基于熟悉性假设，在预训练的嵌入空间中使用熟悉特征作为参考。虽然这种策略已被证明非常成功，但实际上，在异常包含未被预训练编码很好捕捉到的全新特征时，它会导致持续的假阴性。我们提出了一种新颖的AD方法，利用可解释性在输入空间中捕捉新颖特征作为未解释的观察结果。通过在混合方法中结合相似性和新颖性，我们在广泛的异常基准测试中取得了很强的性能。我们的方法在多个基准测试中建立了新的最新技术，处理多样化的异常类型，同时消除了昂贵的背景模型和密集匹配的需求。特别地，我们展示了通过考虑新颖特征，我们的方法能够解决先前的假阴性问题。

    Anomaly Detection (AD) is a critical task that involves identifying observations that do not conform to a learned model of normality. Prior work in deep AD is predominantly based on a familiarity hypothesis, where familiar features serve as the reference in a pre-trained embedding space. While this strategy has proven highly successful, it turns out that it causes consistent false negatives when anomalies consist of truly novel features that are not well captured by the pre-trained encoding. We propose a novel approach to AD using explainability to capture novel features as unexplained observations in the input space. We achieve strong performance across a wide range of anomaly benchmarks by combining similarity and novelty in a hybrid approach. Our approach establishes a new state-of-the-art across multiple benchmarks, handling diverse anomaly types while eliminating the need for expensive background models and dense matching. In particular, we show that by taking account of novel fea
    
[^384]: 大规模粒度：利用高分辨率正射影像和混合学习估计居民社区的福祉

    Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v1 [cs.CV])

    [http://arxiv.org/abs/2309.16808](http://arxiv.org/abs/2309.16808)

    本研究利用高分辨率影像和混合学习方法，通过图像数据中的特征提取和模式检测，估计了个别社区的人口密度、家庭收入中位数和教育程度。

    

    由于现有数据收集方法的局限性，世界上许多地区缺乏有关居民福祉的基本信息。通过遥感获取的高空影像，如卫星或飞机，可以作为窥视地面上生活状况的窗口，并帮助填补社区信息稀缺的地方，较小地理尺度的估计需要更高分辨率的传感器。随着传感器分辨率的提高，机器学习和计算机视觉的最新进展使得能够快速从图像数据中提取特征并检测模式，从而将这些特征与其他信息相关联。在这项工作中，我们探讨了两种方法（监督卷积神经网络和基于视觉词袋的半监督聚类）如何从公开可用的高分辨率影像中估计个别社区的人口密度、家庭收入中位数和教育程度。

    Many areas of the world are without basic information on the well-being of the residing population due to limitations in existing data collection methods. Overhead images obtained remotely, such as from satellite or aircraft, can help serve as windows into the state of life on the ground and help "fill in the gaps" where community information is sparse, with estimates at smaller geographic scales requiring higher resolution sensors. Concurrent with improved sensor resolutions, recent advancements in machine learning and computer vision have made it possible to quickly extract features from and detect patterns in image data, in the process correlating these features with other information. In this work, we explore how well two approaches, a supervised convolutional neural network and semi-supervised clustering based on bag-of-visual-words, estimate population density, median household income, and educational attainment of individual neighborhoods from publicly available high-resolution 
    
[^385]: 关于去噪中的后验分布：在不确定性量化中的应用

    On the Posterior Distribution in Denoising: Application to Uncertainty Quantification. (arXiv:2309.13598v1 [cs.CV])

    [http://arxiv.org/abs/2309.13598](http://arxiv.org/abs/2309.13598)

    该论文研究了去噪中的后验分布及其与后验均值之间的关系，并应用于预训练去噪器的不确定性量化。提出了一种高效计算后验分布主成分和近似边际分布的方法。不需要显式计算高阶矩张量或进行训练或微调。

    

    去噪算法在许多应用中起着核心作用，从降噪低级别成像传感器到提升基于评分的生成模型。后一类方法使用Tweedie公式，将高斯去噪的后验均值（即最小均方误差去噪器）与数据分布的评分链接起来。我们在这里推导了后验分布的高阶中心矩与后验均值的高阶导数之间的基本关系。我们利用这个结果进行预训练去噪器的不确定性量化。特别地，我们展示了如何高效计算图像任何所需区域的后验分布的主成分，以及如何近似沿这些（或任何其他）一维方向的完整边际分布。我们的方法快速且内存高效，因为它不需要显式计算或存储高阶矩张量，并且无需训练或微调。

    Denoisers play a central role in many applications, from noise suppression in low-grade imaging sensors, to empowering score-based generative models. The latter category of methods makes use of Tweedie's formula, which links the posterior mean in Gaussian denoising (i.e., the minimum MSE denoiser) with the score of the data distribution. Here, we derive a fundamental relation between the higher-order central moments of the posterior distribution, and the higher-order derivatives of the posterior mean. We harness this result for uncertainty quantification of pre-trained denoisers. Particularly, we show how to efficiently compute the principal components of the posterior distribution for any desired region of an image, as well as to approximate the full marginal distribution along those (or any other) one-dimensional directions. Our method is fast and memory efficient, as it does not explicitly compute or store the high-order moment tensors and it requires no training or fine tuning of t
    
[^386]: 通过3D-U-SAM网络进行少样本CBCT图像的牙齿分割

    3D-U-SAM Network For Few-shot Tooth Segmentation in CBCT Images. (arXiv:2309.11015v1 [eess.IV])

    [http://arxiv.org/abs/2309.11015](http://arxiv.org/abs/2309.11015)

    本文提出了一种新颖的3D-U-SAM网络，用于少样本CBCT图像的牙齿分割。通过使用预训练的SAM和卷积逼近方法，以及跳跃连接融合特征，本方法在解决小样本问题上表现出很好的效果。

    

    牙齿位置的准确表示在治疗中非常重要。3D牙齿图像分割是一种广泛使用的方法，然而标注的3D牙齿数据集是稀缺的资源，这导致了这个任务在许多情况下面临小样本问题。为了解决这个问题，本文采用了预训练的SAM，并提出了一种新颖的3D-U-SAM网络用于3D牙齿图像分割。具体而言，为了解决在3D数据集上使用2D预训练权重的问题，我们采用了卷积逼近方法；为了保留更多细节，我们设计了跳跃连接，以参考U-Net在所有层级上融合特征。通过消融实验、对比实验和样本大小实验证明了所提出方法的有效性。

    Accurate representation of tooth position is extremely important in treatment. 3D dental image segmentation is a widely used method, however labelled 3D dental datasets are a scarce resource, leading to the problem of small samples that this task faces in many cases. To this end, we address this problem with a pretrained SAM and propose a novel 3D-U-SAM network for 3D dental image segmentation. Specifically, in order to solve the problem of using 2D pre-trained weights on 3D datasets, we adopted a convolution approximation method; in order to retain more details, we designed skip connections to fuse features at all levels with reference to U-Net. The effectiveness of the proposed method is demonstrated in ablation experiments, comparison experiments, and sample size experiments.
    
[^387]: 解耦训练：令人沮丧的简单多领域学习的回归

    Decoupled Training: Return of Frustratingly Easy Multi-Domain Learning. (arXiv:2309.10302v1 [cs.LG])

    [http://arxiv.org/abs/2309.10302](http://arxiv.org/abs/2309.10302)

    这篇论文提出了一种称为解耦训练（D-Train）的令人沮丧的、无超参数的多领域学习方法。该方法采用了一种三阶段的训练策略，首先进行预训练，然后在每个领域上进行后训练，最后进行头部微调，实现解耦训练以获得更好的性能。

    

    多领域学习（MDL）旨在训练一个模型，在多个重叠但非相同的领域中具有最小的平均风险。为了解决数据集偏差和领域优势的挑战，从对齐分布减少领域差距的角度或通过实施领域特定的塔、门甚至专家来保留差异，已经提出了许多MDL方法。MDL模型变得越来越复杂，具有复杂的网络架构或损失函数，引入额外的参数并增加计算成本。在本文中，我们提出了一种令人沮丧的、无超参数的多领域学习方法，命名为解耦训练（D-Train）。D-Train是一种三阶段的从一般到特殊的训练策略，首先在所有领域上进行预训练以热身一个根模型，然后通过将其拆分为多个头部在每个领域上进行后训练，最后通过固定骨干进行头部微调，实现解耦训练以获得更好的性能。

    Multi-domain learning (MDL) aims to train a model with minimal average risk across multiple overlapping but non-identical domains. To tackle the challenges of dataset bias and domain domination, numerous MDL approaches have been proposed from the perspectives of seeking commonalities by aligning distributions to reduce domain gap or reserving differences by implementing domain-specific towers, gates, and even experts. MDL models are becoming more and more complex with sophisticated network architectures or loss functions, introducing extra parameters and enlarging computation costs. In this paper, we propose a frustratingly easy and hyperparameter-free multi-domain learning method named Decoupled Training(D-Train). D-Train is a tri-phase general-to-specific training strategy that first pre-trains on all domains to warm up a root model, then post-trains on each domain by splitting into multi heads, and finally fine-tunes the heads by fixing the backbone, enabling decouple training to ac
    
[^388]: RaTrack: 带有4D雷达点云的运动物体检测与跟踪

    RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09737](http://arxiv.org/abs/2309.09737)

    RaTrack是一种针对雷达跟踪的创新解决方案，通过运动分割和聚类以及运动估计模块，实现了对移动物体的精确跟踪，优于最先进性能。

    

    移动自主性依赖于对动态环境的精确感知。在3D世界中稳定地跟踪移动物体因此对于轨迹预测、避障和路径规划等应用起着关键作用。虽然大多数现有方法利用LiDAR或相机进行多目标跟踪（MOT），但4D成像雷达的能力仍然很少被探索。认识到4D雷达数据中的雷达噪声和点稀疏性所带来的挑战，我们介绍了RaTrack，这是一种专门针对基于雷达的跟踪的创新解决方案。我们的方法摒弃了对特定对象类型和3D边界框的依赖，而是专注于运动分割和聚类，并配以运动估计模块。在View-of-Delft数据集上进行评估时，RaTrack展示出了优于最先进性能的运动物体跟踪精度。

    Mobile autonomy relies on the precise perception of dynamic environments. Robustly tracking moving objects in 3D world thus plays a pivotal role for applications like trajectory prediction, obstacle avoidance, and path planning. While most current methods utilize LiDARs or cameras for Multiple Object Tracking (MOT), the capabilities of 4D imaging radars remain largely unexplored. Recognizing the challenges posed by radar noise and point sparsity in 4D radar data, we introduce RaTrack, an innovative solution tailored for radar-based tracking. Bypassing the typical reliance on specific object types and 3D bounding boxes, our method focuses on motion segmentation and clustering, enriched by a motion estimation module. Evaluated on the View-of-Delft dataset, RaTrack showcases superior tracking precision of moving objects, largely surpassing the performance of the state of the art.
    
[^389]: VERSE：具有实时推理能力的虚拟梯度感知流转学习

    VERSE: Virtual-Gradient Aware Streaming Lifelong Learning with Anytime Inference. (arXiv:2309.08227v1 [cs.LG])

    [http://arxiv.org/abs/2309.08227](http://arxiv.org/abs/2309.08227)

    这项研究提出了一种具有实时推理能力的流式终身学习方法，采用虚拟梯度进行连续表示学习，借助语义记忆来抑制灾难性遗忘，并在多样化的数据上进行了广泛实验。

    

    终身学习是指在训练AI代理的同时，防止其遗忘以前获得的知识的问题。现有的方法大多关注在静态环境下的终身学习，并且缺乏在快速变化的动态环境中减轻遗忘的能力。流式终身学习是终身学习中一个具有挑战性的设置，其目标是在动态的非平稳环境中进行连续学习而不遗忘。我们引入一种新颖的终身学习方法，该方法是流式的，仅需要对数据进行一次遍历，可以以类增量的方式学习，并且可以进行即时评估（实时推理）。为了实现这些，我们提出了用于连续表示学习的虚拟梯度，以防止灾难性遗忘，并借助基于指数移动平均的语义记忆进一步提高性能。我们在多样化的数据上进行了广泛的实验。

    Lifelong learning, also referred to as continual learning, is the problem of training an AI agent continuously while also preventing it from forgetting its previously acquired knowledge. Most of the existing methods primarily focus on lifelong learning within a static environment and lack the ability to mitigate forgetting in a quickly-changing dynamic environment. Streaming lifelong learning is a challenging setting of lifelong learning with the goal of continuous learning in a dynamic non-stationary environment without forgetting. We introduce a novel approach to lifelong learning, which is streaming, requires a single pass over the data, can learn in a class-incremental manner, and can be evaluated on-the-fly (anytime inference). To accomplish these, we propose virtual gradients for continual representation learning to prevent catastrophic forgetting and leverage an exponential-moving-average-based semantic memory to further enhance performance. Extensive experiments on diverse data
    
[^390]: DePT:分解提示调整以实现参数高效微调

    DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05173](http://arxiv.org/abs/2309.05173)

    DePT通过将软提示分解为较短的软提示和一对低秩矩阵，并用两个不同的学习率来优化，以解决提示调整对训练和推理时间以及内存使用的影响，从而实现更好的性能。

    

    提示调整（PT）是一种将可训练的少量软提示向量附加到语言模型（LM）输入中的参数高效微调（PEFT）方法，已在各种任务和模型中显示出了有希望的结果。 与其他PEFT方法相比，PT的竞争性能可以在可训练参数更少的情况下保持，并且随着模型规模的扩大，其参数并不会显著增加。 但是，PT引入了额外的软提示标记，导致输入序列变长，这对于Transformer的二次复杂度而言，在训练和推理时间以及内存使用方面会产生显著影响。 这对于面临大量每日查询的大型语言模型（LLMs）尤其令人担忧。

    Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
    
[^391]: 深度视频编码控制

    Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])

    [http://arxiv.org/abs/2308.16215](http://arxiv.org/abs/2308.16215)

    本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。

    

    丢失率视频压缩通常用于传输和存储视频数据。尽管存在进阶（神经）压缩方法，但统一视频编码器（如H.264或H.265）仍然是事实上的标准。在面对动态网络带宽条件的视频传输中，视频编码器需要适应非常不同的压缩强度。速率控制模块增强编解码器的压缩能力，以满足带宽限制并尽量减少视频失真。然而，标准视频编码器及其速率控制模块是为了最小化人类质量评估而开发的，却没有考虑保护深度视觉模型的下游性能。在本文中，我们提出了第一个端到端可学习的深度视频编码控制方法，考虑了带宽限制和下游视觉性能，并不破坏现有的标准化。我们针对两个常见的视觉任务（语义分割...

    Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
    
[^392]: 这不是一个苹果：多模态嵌入中的对抗幻觉

    Ceci n'est pas une pomme: Adversarial Illusions in Multi-Modal Embeddings. (arXiv:2308.11804v1 [cs.CR])

    [http://arxiv.org/abs/2308.11804](http://arxiv.org/abs/2308.11804)

    该论文研究了多模态嵌入中的对抗幻觉问题。对手可以扰动输入的任意模态，使其嵌入与其他模态的任意输入接近，从而实现任意图像与任意文本、任意文本与任意声音的对齐。该问题与下游任务无关，对生成和分类任务会产生误导。

    

    多模态编码器将图像、声音、文本、视频等映射到一个单一的嵌入空间中，通过对齐不同模态的表示（例如将一张狗的图像与一种叫声相关联）。我们展示了多模态嵌入可以受到一种我们称之为“对抗幻觉”的攻击。给定任意模态的输入，对手可以扰动它，使其嵌入接近于另一模态中任意对手选择的输入的嵌入。幻觉使对手能够将任意图像与任意文本、任意文本与任意声音等进行对齐。对抗幻觉利用了嵌入空间中的接近性，因此与下游任务无关。使用ImageBind嵌入，我们演示了在没有具体下游任务知识的情况下，通过对抗性对齐的输入如何误导图像生成、文本生成和零样例分类。

    Multi-modal encoders map images, sounds, texts, videos, etc. into a single embedding space, aligning representations across modalities (e.g., associate an image of a dog with a barking sound). We show that multi-modal embeddings can be vulnerable to an attack we call "adversarial illusions." Given an input in any modality, an adversary can perturb it so as to make its embedding close to that of an arbitrary, adversary-chosen input in another modality. Illusions thus enable the adversary to align any image with any text, any text with any sound, etc.  Adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks. Using ImageBind embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, and zero-shot classification.
    
[^393]: 基于对比损失的离线手写签名验证模型的白盒误报对抗攻击方法

    A White-Box False Positive Adversarial Attack Method on Contrastive Loss-Based Offline Handwritten Signature Verification Models. (arXiv:2308.08925v1 [cs.CV])

    [http://arxiv.org/abs/2308.08925](http://arxiv.org/abs/2308.08925)

    本文提出了一种针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的新方法，通过将攻击视为书写风格之间的风格转换，引入新的损失函数来生成欺骗性图像，实现了最先进的攻击成功率。

    

    本文针对基于对比损失的离线手写签名验证模型的白盒误报对抗攻击的挑战，提出了一种新颖的攻击方法，将攻击视为在密切相关但不同的书写风格之间进行风格转换。为了引导欺骗性图像的生成，我们引入了两个新的损失函数，通过扰动原始样本与合成样本的嵌入向量之间的欧氏距离来提高攻击成功率，同时通过减小生成图像与原始图像之间的差异来保证最小的扰动。通过实验证明，我们的方法在对比损失的离线手写签名验证模型的白盒攻击中表现出最先进的性能。本文的关键贡献包括了一种新颖的误报攻击方法、两个新的损失函数、有效的书写风格转换以及卓越的性能。

    In this paper, we tackle the challenge of white-box false positive adversarial attacks on contrastive loss-based offline handwritten signature verification models. We propose a novel attack method that treats the attack as a style transfer between closely related but distinct writing styles. To guide the generation of deceptive images, we introduce two new loss functions that enhance the attack success rate by perturbing the Euclidean distance between the embedding vectors of the original and synthesized samples, while ensuring minimal perturbations by reducing the difference between the generated image and the original image. Our method demonstrates state-of-the-art performance in white-box attacks on contrastive loss-based offline handwritten signature verification models, as evidenced by our experiments. The key contributions of this paper include a novel false positive attack method, two new loss functions, effective style transfer in handwriting styles, and superior performance in
    
[^394]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^395]: OCDaf: 有序因果推断与自回归流

    OCDaf: Ordered Causal Discovery with Autoregressive Flows. (arXiv:2308.07480v1 [cs.LG])

    [http://arxiv.org/abs/2308.07480](http://arxiv.org/abs/2308.07480)

    我们提出了一种新的有序因果推断方法，可以从观测数据中学习因果图，并在多种基准测试中展示出了最先进的性能。

    

    我们提出了一种新的基于顺序的方法，用于从观测数据中学习因果图。我们在多变量异方差噪声模型中建立了因果图的可识别性，这是对加性噪声模型的推广，允许非常数噪声方差。借鉴这些模型与仿射自回归归一互补规范流的结构相似性，我们引入了一种连续搜索算法来寻找因果结构。我们的实验结果表明，在Sachs和SynTReN基准测试中，在结构汉明距离（SHD）和结构干预距离（SID）方面具有最先进的性能。此外，我们验证了我们的可识别性理论在各种参数和非参数合成数据集上，并展示了与现有基线方法相比的卓越性能。

    We propose OCDaf, a novel order-based method for learning causal graphs from observational data. We establish the identifiability of causal graphs within multivariate heteroscedastic noise models, a generalization of additive noise models that allow for non-constant noise variances. Drawing upon the structural similarities between these models and affine autoregressive normalizing flows, we introduce a continuous search algorithm to find causal structures. Our experiments demonstrate state-of-the-art performance across the Sachs and SynTReN benchmarks in Structural Hamming Distance (SHD) and Structural Intervention Distance (SID). Furthermore, we validate our identifiability theory across various parametric and nonparametric synthetic datasets and showcase superior performance compared to existing baselines.
    
[^396]: 通过动态激活函数优化前馈和卷积神经网络的性能

    Optimizing Performance of Feedforward and Convolutional Neural Networks through Dynamic Activation Functions. (arXiv:2308.05724v1 [cs.LG])

    [http://arxiv.org/abs/2308.05724](http://arxiv.org/abs/2308.05724)

    该论文研究了使用动态激活函数优化前馈和卷积神经网络的性能。研究结果表明，在卷积神经网络和多层感知器中，复杂的分段线性激活函数比ReLU激活函数表现更好。

    

    近年来，深度学习训练算法在语音、文本、图像和视频等许多领域取得了巨大成功。人们提出了越来越深的网络层次结构，如具有约152层的ResNet结构。浅层卷积神经网络（CNN）仍然是一个活跃的研究领域，其中一些现象仍然没有得到解释。网络中使用的激活函数非常重要，因为它们为网络提供了非线性。ReLU是最常用的激活函数。我们在隐藏层使用了复杂的分段线性（PWL）激活函数。我们证明了这些PWL激活函数在我们的卷积神经网络和多层感知器中比ReLU激活函数的性能更好。我们还提供了在PyTorch中比较浅层和深度CNN的结果，以进一步证实我们的观点。

    Deep learning training training algorithms are a huge success in recent years in many fields including speech, text,image video etc. Deeper and deeper layers are proposed with huge success with resnet structures having around 152 layers. Shallow convolution neural networks(CNN's) are still an active research, where some phenomena are still unexplained. Activation functions used in the network are of utmost importance, as they provide non linearity to the networks. Relu's are the most commonly used activation function.We show a complex piece-wise linear(PWL) activation in the hidden layer. We show that these PWL activations work much better than relu activations in our networks for convolution neural networks and multilayer perceptrons. Result comparison in PyTorch for shallow and deep CNNs are given to further strengthen our case.
    
[^397]: CoRe优化器：机器学习的一体化解决方案

    CoRe Optimizer: An All-in-One Solution for Machine Learning. (arXiv:2307.15663v1 [cs.LG])

    [http://arxiv.org/abs/2307.15663](http://arxiv.org/abs/2307.15663)

    CoRe优化器是一种高性能的机器学习优化器，具有快速、平滑收敛、低计算需求和通用适用性的特点，在训练终身机器学习潜力方面表现出优势。

    

    优化算法及其超参数在机器学习应用中会显著影响训练速度和模型准确度。理想优化器的愿望清单包括快速、平滑地收敛到低误差、低计算需求和通用适用性。我们最近引入的持续弹性（CoRe）优化器在训练终身机器学习潜力方面比其他最先进的一阶梯度优化器表现出更好的性能。在这项工作中，我们对CoRe优化器进行了与其他九种优化算法的广泛性能对比，包括Adam优化器和弹性反向传播（RPROP）。我们分析了不同超参数的影响，并提供了通用适用的值。CoRe优化器在每个研究应用中都取得了最佳或竞争性的性能，只需要更改一个超参数，具体取决于小批量

    The optimization algorithm and its hyperparameters can significantly affect the training speed and resulting model accuracy in machine learning applications. The wish list for an ideal optimizer includes fast and smooth convergence to low error, low computational demand, and general applicability. Our recently introduced continual resilient (CoRe) optimizer has shown superior performance compared to other state-of-the-art first-order gradient-based optimizers for training lifelong machine learning potentials. In this work we provide an extensive performance comparison of the CoRe optimizer and nine other optimization algorithms including the Adam optimizer and resilient backpropagation (RPROP) for diverse machine learning tasks. We analyze the influence of different hyperparameters and provide generally applicable values. The CoRe optimizer yields best or competitive performance in every investigated application, while only one hyperparameter needs to be changed depending on mini-batch
    
[^398]: 具有确定性演化状态的强盗模型

    Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])

    [http://arxiv.org/abs/2307.11655](http://arxiv.org/abs/2307.11655)

    该论文提出了一种名为具有确定性演化状态的强盗模型，用于学习带有强盗反馈的推荐系统和在线广告。该模型考虑了状态演化的不同速率，能准确评估奖励与系统健康程度之间的关系。

    

    我们提出了一种学习与强盗反馈结合的模型，同时考虑到确定性演化和不可观测的状态，我们称之为具有确定性演化状态的强盗模型。我们的模型主要应用于推荐系统和在线广告的学习。在这两种情况下，算法在每一轮获得的奖励是选择行动的短期奖励和系统的“健康”程度（即通过其状态测量）的函数。例如，在推荐系统中，平台从用户对特定类型内容的参与中获得的奖励不仅取决于具体内容的固有特征，还取决于用户与平台上其他类型内容互动后其偏好的演化。我们的通用模型考虑了状态演化的不同速率λ∈[0,1]（例如，用户的偏好因先前内容消费而快速变化）。

    We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
    
[^399]: 在联邦学习中分享什么：模型效用、隐私泄露和通信效率的视角综述

    A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency. (arXiv:2307.10655v1 [cs.LG])

    [http://arxiv.org/abs/2307.10655](http://arxiv.org/abs/2307.10655)

    本文介绍了一篇系统综述，从新的视角探讨了在联邦学习中应该分享什么，注重模型效用、隐私泄露和通信效率。

    

    联邦学习（FL）已成为一种高效的隐私保护合作训练范式，可以在不暴露私有数据集的情况下，允许客户端共享隐私保护信息。这种方法不仅保证了增强的隐私保护，而且促进了多方之间更高效、更安全的合作。因此，FL引起了研究人员的广泛关注，推动了许多综述性文章对相关工作进行总结。然而，大多数综述集中于在训练过程中共享模型参数的方法，而忽视了共享其他形式的本地信息的潜力。本文从一种新的视角出发，即在FL中分享什么，重点关注模型效用、隐私泄露和通信效率，进行了系统综述。

    Federated learning (FL) has emerged as a highly effective paradigm for privacy-preserving collaborative training among different parties. Unlike traditional centralized learning, which requires collecting data from each party, FL allows clients to share privacy-preserving information without exposing private datasets. This approach not only guarantees enhanced privacy protection but also facilitates more efficient and secure collaboration among multiple participants. Therefore, FL has gained considerable attention from researchers, promoting numerous surveys to summarize the related works. However, the majority of these surveys concentrate on methods sharing model parameters during the training process, while overlooking the potential of sharing other forms of local information. In this paper, we present a systematic survey from a new perspective, i.e., what to share in FL, with an emphasis on the model utility, privacy leakage, and communication efficiency. This survey differs from pr
    
[^400]: 从随机游走到图形快跑：一种在连续时间动态图上具有低延迟的节点嵌入框架

    From random-walks to graph-sprints: a low-latency node embedding framework on continuous-time dynamic graphs. (arXiv:2307.08433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08433](http://arxiv.org/abs/2307.08433)

    这篇论文介绍了一种在连续时间动态图上具有低延迟的节点嵌入框架，通过提出流式低延迟的近似随机游走特征，计算时间感知节点嵌入以总结多跳信息。

    

    许多真实世界的数据集具有基础的动态图结构，其中实体和它们的相互作用随时间演变。机器学习模型应考虑这些动态因素，以在下游任务中充分发挥其潜力。以前用于图表示学习的方法要么侧重于抽样k-跳邻域，类似于广度优先搜索，要么侧重于随机游走，类似于深度优先搜索。然而，这些方法在实时动态图上进行低延迟推断是计算上昂贵且不适用的。为了克服这些限制，我们提出了图形快跑，这是一个适用于连续时间动态图（CTDGs）的通用特征提取框架，具有低延迟，并且与高延迟模型相比具有竞争力。为了实现这一点，我们提出了一种流式、低延迟的近似随机游走特征。在我们的框架中，使用仅单跳操作计算总结多跳信息的时间感知节点嵌入。

    Many real-world datasets have an underlying dynamic graph structure, where entities and their interactions evolve over time. Machine learning models should consider these dynamics in order to harness their full potential in downstream tasks. Previous approaches for graph representation learning have focused on either sampling k-hop neighborhoods, akin to breadth-first search, or random walks, akin to depth-first search. However, these methods are computationally expensive and unsuitable for real-time, low-latency inference on dynamic graphs. To overcome these limitations, we propose graph-sprints a general purpose feature extraction framework for continuous-time-dynamic-graphs (CTDGs) that has low latency and is competitive with state-of-the-art, higher latency models. To achieve this, a streaming, low latency approximation to the random-walk based features is proposed. In our framework, time-aware node embeddings summarizing multi-hop information are computed using only single-hop ope
    
[^401]: 强化学习中基于奖励机器抽象的上下文预规划以增强迁移学习

    Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])

    [http://arxiv.org/abs/2307.05209](http://arxiv.org/abs/2307.05209)

    我们提出了一种使用奖励机器抽象来表示当前任务，并在迁移学习中提升DRL代理的性能的方法，实验表明该方法能够提高样本效率并在多个领域中进行少样本迁移。

    

    最近的研究表明，深度强化学习（DRL）代理倾向于过拟合训练任务，并且无法适应轻微的环境变化。为了在转移到未见任务时加快学习，我们提出了一种使用奖励机器（RM）来表示当前任务的新方法，奖励机器是基于当前任务的奖励和动态生成子任务的状态机抽象。我们的方法为代理提供了当前抽象状态的符号表示，并奖励它们达成这些转换。这些表示在任务之间共享，使代理能够利用先前遇到的符号和转换的知识，从而增强迁移能力。我们的实证评估表明，我们的表示在各种领域中提高了样本效率和少样本迁移。

    Recent studies show that deep reinforcement learning (DRL) agents tend to overfit to the task on which they were trained and fail to adapt to minor environment changes. To expedite learning when transferring to unseen tasks, we propose a novel approach to representing the current task using reward machines (RM), state machine abstractions that induce subtasks based on the current task's rewards and dynamics. Our method provides agents with symbolic representations of optimal transitions from their current abstract state and rewards them for achieving these transitions. These representations are shared across tasks, allowing agents to exploit knowledge of previously encountered symbols and transitions, thus enhancing transfer. Our empirical evaluation shows that our representations improve sample efficiency and few-shot transfer in a variety of domains.
    
[^402]: 使用腰部佩戴的加速计在典型的行走和跑步速度范围内自动检测步态事件和行走距离

    Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])

    [http://arxiv.org/abs/2307.04866](http://arxiv.org/abs/2307.04866)

    该论文研究了使用腰部佩戴的加速计自动检测步态事件和行走距离的方法，通过分析市售智能手机加速计数据，实现了从广泛的步态速度范围中提取步态特征，可用于对Duchenne肌肉萎缩患儿和典型发育正常患者的评估。

    

    背景：估计步态（CFs）的时间空间临床特征，如步数和长度、步长、步频、步速和行走距离等，在使用可穿戴式加速计进行基于社区的移动性评估中是一个重要的组成部分。然而，由于设备复杂性和可用性、成本和分析方法学引起的挑战限制了此类工具的广泛应用。研究问题：能否使用市售智能手机的加速计数据来提取Duchenne肌肉萎缩（DMD）患儿和典型发育正常（TDs）患者在广泛步态速度范围内的步态CFs，并使用机器学习（ML）方法。方法：15名DMD患儿和15名TDs被要求在10MRW、25MRW、100MRW、6MWT和FW评估中以一系列步态速度进行监督性临床测试，同时佩戴手机基础加速计。

    Background: Estimation of temporospatial clinical features of gait (CFs), such as step count and length, step duration, step frequency, gait speed and distance traveled is an important component of community-based mobility evaluation using wearable accelerometers. However, challenges arising from device complexity and availability, cost and analytical methodology have limited widespread application of such tools. Research Question: Can accelerometer data from commercially-available smartphones be used to extract gait CFs across a broad range of attainable gait velocities in children with Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using machine learning (ML)-based methods Methods: Fifteen children with DMD and 15 TDs underwent supervised clinical testing across a range of gait speeds using 10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT) and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer at the wa
    
[^403]: 低维流形上过参数化卷积残差网络的非参数分类

    Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])

    [http://arxiv.org/abs/2307.01649](http://arxiv.org/abs/2307.01649)

    本文研究了卷积残差网络在非参数分类任务中的性能。研究表明，通过使用权重衰减的ConvResNeXts，可以隐含地实现对模块的稀疏性，从而使网络能够适应低维流形的平滑性和结构，并高效地学习函数。

    

    卷积残差神经网络(ConvResNets)虽然过参数化，但在实践中能够获得显著的预测性能，这不能被常规智慧很好地解释。为了弥合这一差距，我们从非参数分类的角度研究了使用权重衰减训练的ConvResNeXts（覆盖ConvResNets作为一种特殊情况）的性能。我们的分析允许ConvResNeXts中有无限多的构建模块，并显示权重衰减隐含地强制这些模块的稀疏性。具体而言，我们考虑在低维流形上的平滑目标函数，然后证明ConvResNeXts可以适应函数的平滑性和低维结构，并且能够高效地学习函数而不受维度诅咒的困扰。我们的发现部分证明了过参数化的ConvResNeXts相对于常规机器学习模型的优势。

    Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
    
[^404]: 通过训练概念影响理解不公平性

    Understanding Unfairness via Training Concept Influence. (arXiv:2306.17828v1 [cs.LG])

    [http://arxiv.org/abs/2306.17828](http://arxiv.org/abs/2306.17828)

    通过观察训练数据的作用，研究模型不公平性的来源和影响，并通过改变样本的属性来计算训练样本对模型的不公平性的影响。

    

    了解模型不公平性的原因有助于从业人员更好地理解他们的数据和算法。我们通过培训数据这一主要不公平来源的视角来研究这个问题。我们提出以下问题：如果在训练数据中有些样本（1）来自不同的（例如人口统计学）群体，（2）标记方式不同，或者（3）某些特征发生了变化，那么模型的公平性表现会发生怎样的变化？换句话说，我们通过反事实地对基于预定义概念的样本进行干预和改变，量化训练样本对模型的不公平性的影响。计算训练样本对模型相对于概念的不公平性的影响时，我们首先基于概念生成反事实版本的样本，即如果概念发生变化，样本的反事实版本。然后我们计算重新

    Knowing the causes of a model's unfairness helps practitioners better understand their data and algorithms. This is an important yet relatively unexplored task. We look into this problem through the lens of the training data - one of the major sources of unfairness. We ask the following questions: how would a model's fairness performance change if, in its training data, some samples (1) were collected from a different (e.g. demographic) group, (2) were labeled differently, or (3) some features were changed? In other words, we quantify the fairness influence of training samples by counterfactually intervening and changing samples based on predefined concepts, i.e. data attributes such as features (X), labels (Y), or sensitive attributes (A). To calculate a training sample's influence on the model's unfairness w.r.t a concept, we first generate counterfactual samples based on the concept, i.e. the counterfactual versions of the sample if the concept were changed. We then calculate the re
    
[^405]: 放置有自适应约束条件的传感器以创建核数字孪生体的最优方案

    Optimal Sensor Placement with Adaptive Constraints for Nuclear Digital Twins. (arXiv:2306.13637v1 [math.OC])

    [http://arxiv.org/abs/2306.13637](http://arxiv.org/abs/2306.13637)

    本文提出了一种数据驱动的技术，通过整合自适应的约束条件，用较少的传感器实现重建反应堆流场并创建核数字孪生体。

    

    鉴于反应堆的恶劣运行条件和物理限制，核应用不能将物理资产配备大量传感器。因此，需要仔细确定在给定的空间限制内传感器的放置位置，以实现反应堆流场的重建和核数字孪生体的创建。我们开发了一种数据驱动的技术，将约束条件整合到传感器放置优化过程中，旨在最小化重建误差。我们的方法采用贪心算法，在网格上优化传感器位置，遵循用户定义的约束条件。我们通过计算选择传感器位置的所有可能配置并将其与我们的方法得出的解进行比较，证明了我们算法的近乎最优性。我们发现，我们的方法可以显著提高性能，且所需传感器数量极少，因此为核数字孪生体的创建提供了更为经济高效的解决方案。

    Given harsh operating conditions and physical constraints in reactors, nuclear applications cannot afford to equip the physical asset with a large array of sensors. Therefore, it is crucial to carefully determine the placement of sensors within the given spatial limitations, enabling the reconstruction of reactor flow fields and the creation of nuclear digital twins. Various design considerations are imposed, such as predetermined sensor locations, restricted areas within the reactor, a fixed number of sensors allocated to a specific region, or sensors positioned at a designated distance from one another. We develop a data-driven technique that integrates constraints into an optimization procedure for sensor placement, aiming to minimize reconstruction errors. Our approach employs a greedy algorithm that can optimize sensor locations on a grid, adhering to user-defined constraints. We demonstrate the near optimality of our algorithm by computing all possible configurations for selectin
    
[^406]: 机器学习中的人类限制：利用土壤微生物数据预测植物表型

    Human Limits in Machine Learning: Prediction of Plant Phenotypes Using Soil Microbiome Data. (arXiv:2306.11157v1 [stat.ML])

    [http://arxiv.org/abs/2306.11157](http://arxiv.org/abs/2306.11157)

    本论文深入研究了机器学习模型在预测土壤与植物表型之间联系方面的潜力，证明加入土壤物理化学性质和微生物种群密度等环境特征可以提高预测准确性。

    

    保护土壤健康被认为是21世纪的主要挑战之一，因为它在农业、人类健康和生物多样性方面具有广泛（可能具有威胁性的）影响。本研究通过两种模型（随机森林和贝叶斯神经网络）探索了利用机器学习模型来理解土壤和生物表型之间联系的预测潜力。结果表明，在模型中加入土壤物理化学性质和微生物种群密度等环境特征可以提高预测准确性。此外，通过探索多种数据预处理策略，如归一化、零替换和数据增强，进一步提高了预测性能。

    The preservation of soil health has been identified as one of the main challenges of the XXI century given its vast (and potentially threatening) ramifications in agriculture, human health and biodiversity. Here, we provide the first deep investigation of the predictive potential of machine-learning models to understand the connections between soil and biological phenotypes. Indeed, we investigate an integrative framework performing accurate machine-learning-based prediction of plant phenotypes from biological, chemical and physical properties of the soil via two models: random forest and Bayesian neural network. We show that prediction is improved, as evidenced by higher weighted F1 scores, when incorporating into the models environmental features like soil physicochemical properties and microbial population density in addition to the microbiome information. Furthermore, by exploring multiple data preprocessing strategies such as normalization, zero replacement, and data augmentation,
    
[^407]: 机器学习传感器的数据表

    Datasheets for Machine Learning Sensors. (arXiv:2306.08848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08848](http://arxiv.org/abs/2306.08848)

    本研究提出了一种用于机器学习传感器的标准数据表模板，并讨论了其主要组成部分。这些数据表可以促进对传感器数据在机器学习应用中的理解和利用，并提供了客观的性能评估指标。

    

    机器学习（ML）传感器提供了一种新的感知范式，能够在边缘进行智能化，同时赋予终端用户更多对其数据的控制权。由于这些ML传感器在智能设备的发展中起着至关重要的作用，清晰地记录其规格、功能和限制非常关键。本文介绍了一种用于ML传感器的标准数据表模板，并讨论了其主要组成部分，包括系统的硬件、ML模型和数据集属性、端到端性能指标以及环境影响。我们提供了一个我们自己ML传感器的示例数据表，并详细讨论了每个部分。我们强调这些数据表如何促进对ML应用中传感器数据的更好理解和利用，并提供了客观的衡量系统性能的指标进行评估和比较。ML传感器及其数据表共同提供了更高的隐私、安全性、透明度、可解释性、可审计性和

    Machine learning (ML) sensors offer a new paradigm for sensing that enables intelligence at the edge while empowering end-users with greater control of their data. As these ML sensors play a crucial role in the development of intelligent devices, clear documentation of their specifications, functionalities, and limitations is pivotal. This paper introduces a standard datasheet template for ML sensors and discusses its essential components including: the system's hardware, ML model and dataset attributes, end-to-end performance metrics, and environmental impact. We provide an example datasheet for our own ML sensor and discuss each section in detail. We highlight how these datasheets can facilitate better understanding and utilization of sensor data in ML applications, and we provide objective measures upon which system performance can be evaluated and compared. Together, ML sensors and their datasheets provide greater privacy, security, transparency, explainability, auditability, and u
    
[^408]: 抓住意外收获：在离线策略演员-评论家中利用过去成功的价值(arXiv:2306.02865v2 [cs.LG]已更新)

    Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic. (arXiv:2306.02865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02865](http://arxiv.org/abs/2306.02865)

    该论文提出了 BEE 操作符，通过充分利用过去的成功经验，并保持探索乐观性，解决了离线策略演员-评论家中 Q 值高估与低估问题，提高了策略学习和样本效率。

    

    学习高质量的 Q 值函数在许多现代离线深度强化学习 (RL) 算法的成功中起着关键作用。之前的研究集中解决采用函数逼近器和离线学习所导致的值过高的问题。与这种普遍观点不同，我们观察到 Q 值在 RL 训练过程的后期实际上被低估了，主要是由于贝尔曼更新中，当前策略使用比回放缓冲区中更优的动作样本差。我们假设这个长期被忽视的现象可能阻碍了策略学习，降低了样本效率。我们的想法是在保持探索乐观性的同时，结合充分利用过去成功的经验。我们提出了混合利用和探索 (BEE) 操作符，这是一种简单而有效的方法，使用历史上表现最佳的动作和当前策略生成的动作来更新 Q 值。

    Learning high-quality Q-value functions plays a key role in the success of many modern off-policy deep reinforcement learning (RL) algorithms. Previous works focus on addressing the value overestimation issue, an outcome of adopting function approximators and off-policy learning. Deviating from the common viewpoint, we observe that Q-values are indeed underestimated in the latter stage of the RL training process, primarily related to the use of inferior actions from the current policy in Bellman updates as compared to the more optimal action samples in the replay buffer. We hypothesize that this long-neglected phenomenon potentially hinders policy learning and reduces sample efficiency. Our insight to address this issue is to incorporate sufficient exploitation of past successes while maintaining exploration optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a simple yet effective approach that updates Q-value using both historical best-performing actions and
    
[^409]: 适应性船舶辐射噪声识别与可学习微观小波变换

    Adaptive ship-radiated noise recognition with learnable fine-grained wavelet transform. (arXiv:2306.01002v1 [eess.AS])

    [http://arxiv.org/abs/2306.01002](http://arxiv.org/abs/2306.01002)

    本文提出了一种自适应通用识别系统AGNet，通过转换可学习的微观参数，学习了不同频率下水下声音的特性，以解决在可变水下环境中识别船舶辐射噪音的问题。

    

    分析海洋声学环境是一个棘手的任务。背景噪声和可变的信道传输环境使得准确的船舶辐射噪声识别变得复杂。现有的识别系统在处理可变水下环境方面较为薄弱，因此在实际应用中表现令人失望。为了保持识别系统在各种水下环境下的鲁棒性，本文提出了一种自适应通用识别系统——AGNet（自适应通用网络）。通过将固定的小波参数转换为可学习的微观参数，AGNet学习了不同频率下水下声音的特性。其灵活微观的设计有助于捕获更多背景声学信息（例如背景噪声、水下传输通道）。为了利用小波谱图中的隐式信息，AGNet采用并行卷积注意力卷积神经网络（Convolutional Neural Network with Parallel Convolution Attention M）。

    Analyzing the ocean acoustic environment is a tricky task. Background noise and variable channel transmission environment make it complicated to implement accurate ship-radiated noise recognition. Existing recognition systems are weak in addressing the variable underwater environment, thus leading to disappointing performance in practical application. In order to keep the recognition system robust in various underwater environments, this work proposes an adaptive generalized recognition system - AGNet (Adaptive Generalized Network). By converting fixed wavelet parameters into fine-grained learnable parameters, AGNet learns the characteristics of underwater sound at different frequencies. Its flexible and fine-grained design is conducive to capturing more background acoustic information (e.g., background noise, underwater transmission channel). To utilize the implicit information in wavelet spectrograms, AGNet adopts the convolutional neural network with parallel convolution attention m
    
[^410]: 使用自适应流采样平衡训练能量基模型

    Balanced Training of Energy-Based Models with Adaptive Flow Sampling. (arXiv:2306.00684v1 [cs.LG])

    [http://arxiv.org/abs/2306.00684](http://arxiv.org/abs/2306.00684)

    本文研究了能量基模型的训练算法，使用归一化流进行采样，提高了模型的统计精度和生成性能。

    

    能量基模型 (EBM) 是一种直接参数化未标准化对数密度的多功能密度估计模型。EBM 非常灵活，但缺乏模型的规范化常量，使模型的似然函数计算不可行。近年来，已经提出了许多近似采样器和变分推理技术来估计似然函数梯度进行训练。这些技术在生成样本方面表现出色，但对于估计密度的统计精度，例如确定数据集中不同类的相对重要性，却付出了很少的关注。在本文中，我们提出了一种新的最大似然训练算法，使用一种不同类型的生成模型，归一化流 (NF)，这种模型最近被提出以便于采样。我们的方法在训练过程中将 NF 拟合到 EBM 上，以便 NF 辅助下的采样方案能够始终为 EBM 提供准确的梯度，最终提高模型的统计精度。实验结果表明，与传统 EBM 训练技术相比，我们的方法产生了更高质量的样本和更好的生成性能。

    Energy-based models (EBMs) are versatile density estimation models that directly parameterize an unnormalized log density. Although very flexible, EBMs lack a specified normalization constant of the model, making the likelihood of the model computationally intractable. Several approximate samplers and variational inference techniques have been proposed to estimate the likelihood gradients for training. These techniques have shown promising results in generating samples, but little attention has been paid to the statistical accuracy of the estimated density, such as determining the relative importance of different classes in a dataset. In this work, we propose a new maximum likelihood training algorithm for EBMs that uses a different type of generative model, normalizing flows (NF), which have recently been proposed to facilitate sampling. Our method fits an NF to an EBM during training so that an NF-assisted sampling scheme provides an accurate gradient for the EBMs at all times, ultim
    
[^411]: 在雾环境中进行预测复制的时间运动预测

    Predicting Temporal Aspects of Movement for Predictive Replication in Fog Environments. (arXiv:2306.00575v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2306.00575](http://arxiv.org/abs/2306.00575)

    本文研究了时间预测在雾环境下预测复制中的应用，提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，可以在减少多余数据的同时，只有微小的数据可用性降低。

    

    充分利用雾环境的好处，有效地管理数据位置非常重要。盲目或反应式的数据复制无法充分利用雾计算的潜力，需要更先进的技术来预测客户端何时何地连接。虽然空间预测受到了相当多的关注，但时间预测仍未得到充分研究。本文通过研究将时间预测纳入现有空间预测模型的优势来填补这一空白。我们还在预测复制的背景下对时空预测模型（如深度神经网络和马尔可夫模型）进行全面分析。我们提出了一种利用Holder-Winter指数平滑法进行时间预测的新模型，利用顺序和周期性用户移动模式。在模拟真实用户轨迹的雾网络中，我们的模型在多余数据上实现了15％的降低，而数据可用性只有1％的微小降低。

    To fully exploit the benefits of the fog environment, efficient management of data locality is crucial. Blind or reactive data replication falls short in harnessing the potential of fog computing, necessitating more advanced techniques for predicting where and when clients will connect. While spatial prediction has received considerable attention, temporal prediction remains understudied.  Our paper addresses this gap by examining the advantages of incorporating temporal prediction into existing spatial prediction models. We also provide a comprehensive analysis of spatio-temporal prediction models, such as Deep Neural Networks and Markov models, in the context of predictive replication. We propose a novel model using Holt-Winter's Exponential Smoothing for temporal prediction, leveraging sequential and periodical user movement patterns. In a fog network simulation with real user trajectories our model achieves a 15% reduction in excess data with a marginal 1% decrease in data availabi
    
[^412]: 拓展水下声学目标识别信息视角的文本模板方法

    Underwater-Art: Expanding Information Perspectives With Text Templates For Underwater Acoustic Target Recognition. (arXiv:2305.19612v1 [cs.SD])

    [http://arxiv.org/abs/2305.19612](http://arxiv.org/abs/2305.19612)

    本文提出了一种基于文本模板的水下声学目标识别方法（UART），从不同视角整合相关信息，通过音频-频谱图-文本三模态对比学习框架，赋予UART用自然语言指导声学表示学习的能力，显著提高识别模型的可解释性和鲁棒性。

    

    水下声学目标识别是一项棘手的任务，由于复杂的声源特征和声波传播模式而变得十分困难。基于深度学习的识别模型，受限于数据量不足和狭窄信息视角，在实际水下场景中似乎远未达到满意程度。尽管水下声学信号受距离、水深或其他因素的严重影响，但相关信息的注释往往是不均匀的、不完整的、难以使用的。本文提出了一种基于文本模板的水下声学目标识别方法（UART），通过将来自不同视角的相关信息整合成自然语言来设计模板。UART采用音频-频谱图-文本三模态对比学习框架，赋予UART用自然语言指导声学表示学习的能力。实验结果表明，UART可以显著提高各种水下声学数据集上识别模型的可解释性和鲁棒性。

    Underwater acoustic target recognition is an intractable task due to the complex acoustic source characteristics and sound propagation patterns. Limited by insufficient data and narrow information perspective, recognition models based on deep learning seem far from satisfactory in practical underwater scenarios. Although underwater acoustic signals are severely influenced by distance, channel depth, or other factors, annotations of relevant information are often non-uniform, incomplete, and hard to use. In our work, we propose to implement Underwater Acoustic Recognition based on Templates made up of rich relevant information (hereinafter called "UART"). We design templates to integrate relevant information from different perspectives into descriptive natural language. UART adopts an audio-spectrogram-text tri-modal contrastive learning framework, which endows UART with the ability to guide the learning of acoustic representations by descriptive natural language. Our experiments reveal
    
[^413]: 一张图值得一比特的差异性：当图的对比学习遇到脉冲神经网络时。

    A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks. (arXiv:2305.19306v1 [cs.NE])

    [http://arxiv.org/abs/2305.19306](http://arxiv.org/abs/2305.19306)

    本文提出了SpikeGCL，一种用二值化1比特表示来提高效率和节约资源的图对比学习框架，实验结果表明可以以近32倍的表示存储压缩实现高效学习。

    

    虽然对比自监督学习已经成为图神经网络的事实上的学习范式，但对高任务准确性的追求需要大的隐藏维度来学习信息丰富、有区别性的全精度表示，这引发了对计算、存储和能源消耗负担（在现实世界应用中大多被忽略）的担忧。本文探索了一种有前途的方向，即用脉冲神经网络（SNNs）进行图的对比学习（GCL），利用稀疏和二元特性来学习更具生物可行性和紧凑性的表示。我们提出了SpikeGCL，一种学习图的二值化1比特表示的新型GCL框架，平衡了效率和性能之间的权衡。我们提供了理论保证，证明SpikeGCL在表达能力上与其全精度对应物具有可比性。实验结果表明，通过将表示存储压缩近32倍，SpikeGCL在保持高准确性的同时可以实现高效的学习。

    While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of high task accuracy requires a large hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This paper explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compressio
    
[^414]: 基于自编码器的条件神经过程用于表示学习

    Autoencoding Conditional Neural Processes for Representation Learning. (arXiv:2305.18485v1 [cs.LG])

    [http://arxiv.org/abs/2305.18485](http://arxiv.org/abs/2305.18485)

    本文提出了部分像素空间变分自编码器，结合了自编码器与条件神经过程，可以学习到一系列基本物理和文化概念的表示，并且可以提高上下文预测的准确性。

    

    条件神经过程(CNPs)是一种灵活高效的模型族群，可以从观测值中学习出一个随机过程。在视觉领域中，CNPs 在上下文图像补全中得到了特别的应用，即通过观察某些位置的像素值来预测其他未观察位置上的值的分布。然而，学习这样一个 CNP 的像素选择通常是随机的或者是通过一个简单的统计量(例如像素方差)导出的。本文将问题转变一下：一个 CNP 想要观察哪些像素？也就是说，哪些像素允许拟合 CNP，这样的像素能告诉我们一些关于潜在图像的信息吗？将提供给 CNP 的上下文视为固定大小的潜在表示，我们构建了一个一次性变分框架，部分像素空间变分自编码器(Partical Pixel Space VAE, PPS-VAE)，同时预测这个上下文，并学习一个 CNP。我们在一组视觉数据集上评估了 PPS-VAE，发现通过相对大小或变化预测像素的选择可以安排学习，且更准确地进行了上下文预测，并且可以对基本物理和文化概念进行有意义的表示。

    Conditional neural processes (CNPs) are a flexible and efficient family of models that learn to learn a stochastic process from observations. In the visual domain, they have seen particular application in contextual image completion - observing pixel values at some locations to predict a distribution over values at other unobserved locations. However, the choice of pixels in learning such a CNP is typically either random or derived from a simple statistical measure (e.g. pixel variance). Here, we turn the problem on its head and ask: which pixels would a CNP like to observe? That is, which pixels allow fitting CNP, and do such pixels tell us something about the underlying image? Viewing the context provided to the CNP as fixed-size latent representations, we construct an amortised variational framework, Partial Pixel Space Variational Autoencoder (PPS-VAE), for predicting this context simultaneously with learning a CNP. We evaluate PPS-VAE on a set of vision datasets, and find that not
    
[^415]: 序列建模的变压器网络的逼近理论

    Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])

    [http://arxiv.org/abs/2305.18475](http://arxiv.org/abs/2305.18475)

    本文证明了变压器假设空间的普遍逼近定理，并提出了一种新的规律概念用于精确逼近速率估计，揭示了变压器适用于逼近哪些类型的序列关系，并讨论了其与传统序列建模方法之间的结构偏差。

    

    变压器是序列建模应用中广泛应用的架构，但其工作原理的理论理解有限。在本文中，我们研究了变压器逼近序列关系的能力。我们首先证明了变压器假设空间的普遍逼近定理。通过推导，我们确定了一种新的规律概念，在此概念下，我们可以证明一个明确的逼近速率估计。这个估计揭示了变压器的关键结构特性，并暗示了变压器适用于逼近哪些类型的序列关系。特别地，它使我们能够具体地讨论变压器与传统序列建模方法（如循环神经网络）之间的结构偏差。我们的研究结果得到了数字实验的支持。

    The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.
    
[^416]: 离巢：超越本地损失函数的预测优化问题

    Leaving the Nest: Going Beyond Local Loss Functions for Predict-Then-Optimize. (arXiv:2305.16830v1 [cs.LG])

    [http://arxiv.org/abs/2305.16830](http://arxiv.org/abs/2305.16830)

    本文提出了一种避免限制性假设的解决方案，利用机器学习模型的特性来提高学习损失函数的样本效率，在预测优化问题中实现了最先进的结果。

    

    预测优化问题是一种使用机器学习在不确定性条件下进行决策制定的框架。它的中心研究问题是，“如何利用决策任务的结构来定制特定任务的机器学习模型？”为此，最近的研究已经提出了学习任务特定的损失函数来捕捉这种潜在的结构。然而，当前的方法对这些损失的形式和对机器学习模型行为的影响做出了限制性的假设。这些假设既导致了高计算成本的方法，也在实践中被违反时导致了性能下降。在本文中，我们提出了解决这些问题的解决方案，避免了上述假设，利用机器学习模型的特性来提高学习损失函数的样本效率。我们从文献中的四个领域实验证明了我们的方法取得了最先进的结果，通常需要比可比方法少一个数量级的样本。

    Predict-then-Optimize is a framework for using machine learning to perform decision-making under uncertainty. The central research question it asks is, "How can the structure of a decision-making task be used to tailor ML models for that specific task?" To this end, recent work has proposed learning task-specific loss functions that capture this underlying structure. However, current approaches make restrictive assumptions about the form of these losses and their impact on ML model behavior. These assumptions both lead to approaches with high computational cost, and when they are violated in practice, poor performance. In this paper, we propose solutions to these issues, avoiding the aforementioned assumptions and utilizing the ML model's features to increase the sample efficiency of learning loss functions. We empirically show that our method achieves state-of-the-art results in four domains from the literature, often requiring an order of magnitude fewer samples than comparable metho
    
[^417]: 通过安全层实现垂直联邦学习的高效安全聚合

    vFedSec: Efficient Secure Aggregation for Vertical Federated Learning via Secure Layer. (arXiv:2305.16794v1 [cs.CR])

    [http://arxiv.org/abs/2305.16794](http://arxiv.org/abs/2305.16794)

    vFedSec提出了一个用于垂直联邦学习的新型Secure Layer，旨在使用最先进的安全模块，实现安全高效的联合训练。实验结果表明，该方法在保护数据隐私效果显著，不会影响训练性能。

    

    隐私保护联邦学习主要关注横向划分的数据集，而在许多有趣的问题中，个体数据点分散在垂直的客户端/组织中。这种情况下的联邦学习需要参与者之间交换中间输出和梯度，若不考虑隐私和安全问题，可能会导致隐私泄露的风险。本文提出了vFedSec，通过创新性的安全层设计和最先进的安全模块，在保护数据隐私的同时，实现了垂直联邦学习的安全和高效。理论上证明了我们的方法不影响训练绩效，同时有效保护私人数据。实验结果也表明了我们的设计的应用性和保护能力。

    Most work in privacy-preserving federated learning (FL) has been focusing on horizontally partitioned datasets where clients share the same sets of features and can train complete models independently. However, in many interesting problems, individual data points are scattered across different clients/organizations in a vertical setting. Solutions for this type of FL require the exchange of intermediate outputs and gradients between participants, posing a potential risk of privacy leakage when privacy and security concerns are not considered. In this work, we present vFedSec - a novel design with an innovative Secure Layer for training vertical FL securely and efficiently using state-of-the-art security modules in secure aggregation. We theoretically demonstrate that our method does not impact the training performance while protecting private data effectively. Empirically results also show its applicability with extensive experiments that our design can achieve the protection with negl
    
[^418]: 使用基于文献的语境化学习生成新的科学方向

    Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.14259](http://arxiv.org/abs/2305.14259)

    本文介绍了一种基于文献的发现方法，通过上下文化的学习生成新的科学方向，克服了标准方法在预测关联、忽略上下文等方面的局限性。模型使用了引文和知识图关系的网络，并使用大型语言模型进行评估，发现GPT4在生成创新思想方面表现出色。

    

    基于文献的发现（LBD）旨在通过挖掘论文并生成假设来发现新的科学知识。标准的LBD仅限于预测离散概念之间的两两关系（例如，药物和疾病的关联）。LBD还忽略了关键的上下文，例如实验设置（例如，药物评估的特定患者群体）和人类科学家考虑的背景知识和动机（例如，找到没有特定副作用的药物候选）。我们通过一种新颖的上下文化LBD（C-LBD）表述来解决这些局限性：以自然语言生成科学假设，同时将它们联系到控制假设搜索空间的上下文中。我们提出了一个建模框架，使用获得的引文和知识图关系的异构网络中的“灵感”，并创建了一个从论文中派生的新数据集。我们使用强大的大型语言模型（LLM）进行评估，发现GPT4倾向于生成具有创新性的思想。

    Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
    
[^419]: 使用指令微调基础模型的多模态 Web 导航。

    Multimodal Web Navigation with Instruction-Finetuned Foundation Models. (arXiv:2305.11854v1 [cs.LG])

    [http://arxiv.org/abs/2305.11854](http://arxiv.org/abs/2305.11854)

    本文研究使用视觉语言基础模型进行数据驱动离线训练的 Web 代理，提出了一个指令跟随多模态代理WebGUM，将微调指令微调语言模型和视觉转换器，能够有效提高代理的基于视觉感知、HTML 理解和多步推理的能力。

    

    自主 Web 导航的进展受到了依赖数十亿次在线强化学习的探索性交互和具有领域特定模型设计的影响，这使得难以利用来自丰富领域外数据的泛化。在本工作中，我们研究了基于数据驱动的脱机训练，用于使用视觉语言基础模型的 Web 代理。我们提出了一个指令跟随多模态代理， WebGUM，它观察了网页截图和 HTML 页面，并输出 Web 导航操作，如单击和输入。WebGUM 是通过联合微调指令微调语言模型和视觉转换器在大量的演示语料库上训练的。我们凭经验证明，这种方法可以提高代理的基于视觉感知、HTML 理解和多步推理的能力，明显优于之前的工作。在 MiniWoB 基准测试中，我们超过之前最佳脱机方法 31.9% 以上，接近实现在线交互的表现。

    The progress of autonomous web navigation has been hindered by the dependence on billions of exploratory interactions via online reinforcement learning, and domain-specific model designs that make it difficult to leverage generalization from rich out-of-domain data. In this work, we study data-driven offline training for web agents with vision-language foundation models. We propose an instruction-following multimodal agent, WebGUM, that observes both webpage screenshots and HTML pages and outputs web navigation actions, such as click and type. WebGUM is trained by jointly finetuning an instruction-finetuned language model and a vision transformer on a large corpus of demonstrations. We empirically demonstrate this recipe improves the agent's ability of grounded visual perception, HTML comprehension and multi-step reasoning, outperforming prior works by a significant margin. On the MiniWoB benchmark, we improve over the previous best offline methods by more than 31.9%, being close to re
    
[^420]: DeformerNet: 学习三维可塑物体的双手操纵

    DeformerNet: Learning Bimanual Manipulation of 3D Deformable Objects. (arXiv:2305.04449v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04449](http://arxiv.org/abs/2305.04449)

    本论文介绍了一种名为DeformerNet的神经网络架构，通过学习三维可塑物体的低维表示来实现机器人对物体形状的操纵。这种方法不需要手工特征和物体特定的控制模型，可在仿真和真实机器人上进行演示和应用。

    

    从家庭护理到仓库配送再到外科手术助理等领域，应用需要机器人可靠地操纵三维可塑物体的形状。弹性三维可塑物体的分析模型需要大量参数来描述决定物体形状的可能无限自由度。以往的3D形状控制尝试依赖于手工特征来表示物体形状，并需要训练物体特定的控制模型。我们通过使用我们的新型DeformerNet神经网络架构来克服这些问题，该架构在被操纵物体的部分视图点云和目标形状的点云上运行，学习物体形状的低维表示。这个形状嵌入使机器人能够学习一种视觉伺服控制器，该控制器计算出所需的机器人末端执行器动作，将物体迭代地变形向目标形状。我们在仿真和真实机器人上演示了这一点。

    Applications in fields ranging from home care to warehouse fulfillment to surgical assistance require robots to reliably manipulate the shape of 3D deformable objects. Analytic models of elastic, 3D deformable objects require numerous parameters to describe the potentially infinite degrees of freedom present in determining the object's shape. Previous attempts at performing 3D shape control rely on hand-crafted features to represent the object shape and require training of object-specific control models. We overcome these issues through the use of our novel DeformerNet neural network architecture, which operates on a partial-view point cloud of the manipulated object and a point cloud of the goal shape to learn a low-dimensional representation of the object shape. This shape embedding enables the robot to learn a visual servo controller that computes the desired robot end-effector action to iteratively deform the object toward the target shape. We demonstrate both in simulation and on 
    
[^421]: Adam家族算法在无平滑优化中的收敛性保证研究

    Adam-family Methods for Nonsmooth Optimization with Convergence Guarantees. (arXiv:2305.03938v1 [math.OC])

    [http://arxiv.org/abs/2305.03938](http://arxiv.org/abs/2305.03938)

    本文提出了一种新的双时间尺度框架，证明了其在温和条件下收敛性，该框架包括了各种流行的Adam家族算法，用于训练无平滑神经网络和应对重尾噪声的需求，并通过实验表明了其效率和鲁棒性。

    

    本文对Adam家族算法在无平滑优化中的收敛性进行了全面研究，特别是在无平滑神经网络的训练中。我们提出了一种新的双时间尺度框架，采用双时间尺度更新方案，证明了其在温和条件下的收敛性。我们的框架包括了各种流行的Adam家族算法，在训练无平滑神经网络中提供了收敛性保证。此外，我们还开发了随机次梯度方法，结合梯度裁剪技术，用于训练具有重尾噪声的无平滑神经网络。通过我们的框架，我们展示了我们提出的方法甚至在仅假定评估噪声可积的情况下也会收敛。广泛的数值实验证明了我们提出的方法的高效性和稳健性。

    In this paper, we present a comprehensive study on the convergence properties of Adam-family methods for nonsmooth optimization, especially in the training of nonsmooth neural networks. We introduce a novel two-timescale framework that adopts a two-timescale updating scheme, and prove its convergence properties under mild assumptions. Our proposed framework encompasses various popular Adam-family methods, providing convergence guarantees for these methods in training nonsmooth neural networks. Furthermore, we develop stochastic subgradient methods that incorporate gradient clipping techniques for training nonsmooth neural networks with heavy-tailed noise. Through our framework, we show that our proposed methods converge even when the evaluation noises are only assumed to be integrable. Extensive numerical experiments demonstrate the high efficiency and robustness of our proposed methods.
    
[^422]: MLCopilot：释放大语言模型在解决机器学习任务中的能力

    MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks. (arXiv:2304.14979v1 [cs.LG])

    [http://arxiv.org/abs/2304.14979](http://arxiv.org/abs/2304.14979)

    本文介绍了一种新型框架MLCopilot，通过利用最先进的大语言模型，扩展其能力以理解结构化输入并进行深入推理以解决新型ML任务的能力，展示了MLCopilot在解决图像分类，文本分类和表格分类三项任务方面的巨大潜力。

    

    机器学习（ML）领域受到了广泛的应用，因此逐渐引发了将ML应用于特定场景的需求，但实现起来耗时且不易。 自动化解决ML任务（例如AutoML）的主要方法通常耗费时间且难以理解。 而与之相反，虽然人类工程师具有理解任务和推理解决方案的难以置信的能力，但他们的经验和知识往往不充分且难以借助定量方法利用。 在本文中，我们旨在通过引入一种新颖的框架MLCopilot来弥合机器智能和人类知识之间的差距，该框架利用最先进的LLM来开发新型任务的ML解决方案。 我们展示了扩展LLM的能力以理解结构化输入并进行深入推理以解决新型ML任务的可能性。 经过一些专门设计后，我们发现LLM可以（i）从人类编写的文件中观察现有知识，（ii）制定解决ML任务的具体步骤。 我们对图像分类，文本分类和表格分类三项任务进行了实验，证明了MLCopilot在解决实际ML问题方面的巨大潜力。

    The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the exi
    
[^423]: 自主预测和长期家庭能源负荷预测的自旋电子物理水库

    Spintronic Physical Reservoir for Autonomous Prediction and Long-Term Household Energy Load Forecasting. (arXiv:2304.03343v1 [cs.LG])

    [http://arxiv.org/abs/2304.03343](http://arxiv.org/abs/2304.03343)

    本研究展示了利用自旋电子物理水库进行自治型长期预测任务的方法，可以用于建模混沌时间序列和动态时间序列数据，是适合在边缘设备上进行实时学习的。这里提出的基于微旋磁隧穿结的涡旋子可以作为实现此种RC的原型。

    

    本研究利用自旋电子物理水库进行了自治型长期预测。由于磁化动力学的短期记忆特性，水库状态中产生了非线性，可用于使用简单线性回归进行在线训练的长期预测任务。在预测阶段，输出直接馈入水库的输入中进行自治型预测。我们将所提出的水库用于建模诸如Mackey-Glass等混沌时间序列和动态时间序列数据，如家庭建筑能耗。由于只有RC的最后一层需要使用线性回归进行训练，因此它非常适合在边缘设备上进行实时学习。本文展示了基于微旋磁隧穿结的涡旋子可能用作原型RC，但任何具有非线性磁化行为的纳米磁隧道结都可以实现此种RC。通过比较我们的自旋电子物理RC方法

    In this study, we have shown autonomous long-term prediction with a spintronic physical reservoir. Due to the short-term memory property of the magnetization dynamics, non-linearity arises in the reservoir states which could be used for long-term prediction tasks using simple linear regression for online training. During the prediction stage, the output is directly fed to the input of the reservoir for autonomous prediction. We employ our proposed reservoir for the modeling of the chaotic time series such as Mackey-Glass and dynamic time-series data, such as household building energy loads. Since only the last layer of a RC needs to be trained with linear regression, it is well suited for learning in real time on edge devices. Here we show that a skyrmion based magnetic tunnel junction can potentially be used as a prototypical RC but any nanomagnetic magnetic tunnel junction with nonlinear magnetization behavior can implement such a RC. By comparing our spintronic physical RC approach 
    
[^424]: 无碰撞运输图在流行学习中的应用

    Applications of No-Collision Transportation Maps in Manifold Learning. (arXiv:2304.00199v1 [cs.LG])

    [http://arxiv.org/abs/2304.00199](http://arxiv.org/abs/2304.00199)

    本文研究了在流形学习中应用无碰撞运输图的方法，其可以比OT图更便宜地计算距离，并提供单个概率测度的平移和伸缩的等距性。

    

    本文研究了引入于[Nurbekyan et al.，2020]的无碰撞运输图在图像数据的流形学习中的应用。近年来，在表示类似运动或变形现象的数据中，应用基于运输的距离和特征的研究大幅增加。事实上，固定位置比较强度通常无法显示数据结构。在[Nurbekyan et al.，2020]中开发的无碰撞图和距离类似于最优传输(OT)图的几何特征但由于无需优化，计算成本要便宜得多。本文证明无碰撞距离提供单个概率测度的平移(分别是伸缩)和装备欧几里得距离的平移(分别是伸缩)向量之间的等距性。此外，我们证明，无碰撞运输图以及OT和线性OT图，一般来说不能为旋转提供等距性。

    In this work, we investigate applications of no-collision transportation maps introduced in [Nurbekyan et. al., 2020] in manifold learning for image data. Recently, there has been a surge in applying transportation-based distances and features for data representing motion-like or deformation-like phenomena. Indeed, comparing intensities at fixed locations often does not reveal the data structure. No-collision maps and distances developed in [Nurbekyan et. al., 2020] are sensitive to geometric features similar to optimal transportation (OT) maps but much cheaper to compute due to the absence of optimization. In this work, we prove that no-collision distances provide an isometry between translations (respectively dilations) of a single probability measure and the translation (respectively dilation) vectors equipped with a Euclidean distance. Furthermore, we prove that no-collision transportation maps, as well as OT and linearized OT maps, do not in general provide an isometry for rotatio
    
[^425]: 朝黑盒参数估计迈进

    Towards black-box parameter estimation. (arXiv:2303.15041v1 [stat.ML])

    [http://arxiv.org/abs/2303.15041](http://arxiv.org/abs/2303.15041)

    本文提出了一种基于弱参数结构假设的黑盒程序，用于估计统计模型参数。该程序可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。

    

    深度学习算法最近已经被证明是估计统计模型参数的成功工具，模拟容易但似然计算具有挑战性。但这些方法的成功取决于模拟出可以充分复制观察数据的参数，并且目前缺乏有效的方法来产生这些模拟数据。我们开发了基于弱参数结构假设估计统计模型参数的新的黑盒程序。对于似然函数有较频繁出现的良好结构的情况，如时间序列，这是通过在广泛的模拟数据库上预训练深度神经网络来实现的，该数据库涵盖了各种数据大小的范围。对于其他类型的复杂依赖关系，则需要一个迭代的算法来指导多轮正确参数区域的模拟。这些方法可以成功地从具有复杂空间相关的非高斯模型中估计和量化参数的不确定性。

    Deep learning algorithms have recently shown to be a successful tool in estimating parameters of statistical models for which simulation is easy, but likelihood computation is challenging. But the success of these approaches depends on simulating parameters that sufficiently reproduce the observed data, and, at present, there is a lack of efficient methods to produce these simulations. We develop new black-box procedures to estimate parameters of statistical models based only on weak parameter structure assumptions. For well-structured likelihoods with frequent occurrences, such as in time series, this is achieved by pre-training a deep neural network on an extensive simulated database that covers a wide range of data sizes. For other types of complex dependencies, an iterative algorithm guides simulations to the correct parameter region in multiple rounds. These approaches can successfully estimate and quantify the uncertainty of parameters from non-Gaussian models with complex spatia
    
[^426]: AI生成的文本是否可靠地检测出来？

    Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2303.11156](http://arxiv.org/abs/2303.11156)

    本研究通过实证和理论分析表明，在实际场景中，几种AI文本检测器不可靠。改写攻击可以破解多种检测器，包括水印方案、神经网络检测器和零样本分类器。即使是最好的检测器，随着语言模型的进一步提升，性能也会下降。因此，AI生成的文本的可靠检测仍然是一个挑战。

    

    本文从实证和理论两个方面表明，在实际场景中，几种AI文本检测器并不可靠。从实践上来说，我们证明了轻量级的改写器应用在大型语言模型（LLM）上可以破解一系列的检测器，包括使用水印方案、神经网络检测器和零样本分类器。我们的实验表明，旨在躲避改写攻击的基于检索的检测器仍然容易受到递归改写的攻击。然后，我们提出了一个理论上的不可能结果，指出随着语言模型变得越来越复杂和更擅长模仿人类文本，在最好的检测器性能会下降。对于一个足够先进的语言模型来模仿人类文本，即使最佳的检测器的表现只比随机分类器好上一点点。我们的结果足够概括特定的场景，如改写攻击。

    In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
    
[^427]: 用于随机控制和博弈的机器学习方法的最新发展

    Recent Developments in Machine Learning Methods for Stochastic Control and Games. (arXiv:2303.10257v1 [math.OC])

    [http://arxiv.org/abs/2303.10257](http://arxiv.org/abs/2303.10257)

    本文回顾了基于机器学习的随机控制问题和博弈的计算方法，特别是着重介绍了使用深度学习算法解决高维度和非常复杂结构情况下问题的新方法。

    

    随机最优控制和博弈已经在金融、经济学、社会科学、机器人和能源管理等领域中找到了广泛的应用。许多真实世界的应用都涉及到复杂的模型，这推动了先进的数值方法的发展。最近，基于机器学习的计算方法已经发展用于随机控制问题和博弈。我们回顾这些方法，重点关注已经解锁了高维度和非常复杂结构情况下解决此类问题的可能性的深度学习算法，这是传统数值方法无法完成的。在这里，我们主要考虑连续时间和连续空间设置。许多新方法基于最近用于高维偏微分方程或反向随机微分方程的神经网络方法，或者基于无模型强化学习的马尔科夫决策过程，这导致了突破性的发展。

    Stochastic optimal control and games have found a wide range of applications, from finance and economics to social sciences, robotics and energy management. Many real-world applications involve complex models which have driven the development of sophisticated numerical methods. Recently, computational methods based on machine learning have been developed for stochastic control problems and games. We review such methods, with a focus on deep learning algorithms that have unlocked the possibility to solve such problems even when the dimension is high or when the structure is very complex, beyond what is feasible with traditional numerical methods. Here, we consider mostly the continuous time and continuous space setting. Many of the new approaches build on recent neural-network based methods for high-dimensional partial differential equations or backward stochastic differential equations, or on model-free reinforcement learning for Markov decision processes that have led to breakthrough 
    
[^428]: 金融时间序列的元对比标签校正方法

    Meta contrastive label correction for financial time series. (arXiv:2303.08103v1 [cs.LG])

    [http://arxiv.org/abs/2303.08103](http://arxiv.org/abs/2303.08103)

    本文针对股票价格预测中标记不准确的问题，提出了一种元对比标签校正方法。方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签，并提高分类性能。

    

    金融应用（如股票价格预测）通常面临标记不准确的问题，本文提出一种元对比标签校正方法，可以自动生成准确标签，并提高分类性能。该方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签。

    Financial applications such as stock price forecasting, usually face an issue that under the predefined labeling rules, it is hard to accurately predict the directions of stock movement. This is because traditional ways of labeling, taking Triple Barrier Method, for example, usually gives us inaccurate or even corrupted labels. To address this issue, we focus on two main goals. One is that our proposed method can automatically generate correct labels for noisy time series patterns, while at the same time, the method is capable of boosting classification performance on this new labeled dataset. Based on the aforementioned goals, our approach has the following three novelties: First, we fuse a new contrastive learning algorithm into the meta-learning framework to estimate correct labels iteratively when updating the classification model inside. Moreover, we utilize images generated from time series data through Gramian angular field and representative learning. Most important of all, we 
    
[^429]: Lumos: 针对分布式设备的异构感知联邦图学习

    Lumos: Heterogeneity-aware Federated Graph Learning over Decentralized Devices. (arXiv:2303.00492v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00492](http://arxiv.org/abs/2303.00492)

    Lumos是第一个支持节点级联邦图学习的框架，具有特征和度数保护功能。它采用了树构造器和去中心化节点聚合策略来提高图的表示能力。

    

    由于图神经网络(GNN)能够处理图结构数据，因此在现实世界的网络应用和系统中广泛使用。然而，对数据隐私的日益关注严重挑战了传统的集中式模型训练范式，其中服务器持有所有图信息。联邦学习是一种新兴的协作计算范式，可以在不集中数据的情况下进行模型训练。现有的联邦GNN研究主要集中在客户端持有不同的图或子图的系统上。然而，还没有对每个客户端仅知道其直接邻居的实际节点级联邦情况进行研究。在本文中，我们提出了第一个支持节点级联邦图的有监督和无监督学习的联邦GNN框架Lumos，它具有特征和度数保护功能。我们首先设计了一个树构造器，以提高在有限结构信息下的表示能力。我们还提出了一种基于邻居信息的去中心化节点聚合策略。

    Graph neural networks (GNN) have been widely deployed in real-world networked applications and systems due to their capability to handle graph-structured data. However, the growing awareness of data privacy severely challenges the traditional centralized model training paradigm, where a server holds all the graph information. Federated learning is an emerging collaborative computing paradigm that allows model training without data centralization. Existing federated GNN studies mainly focus on systems where clients hold distinctive graphs or sub-graphs. The practical node-level federated situation, where each client is only aware of its direct neighbors, has yet to be studied. In this paper, we propose the first federated GNN framework called Lumos that supports supervised and unsupervised learning with feature and degree protection on node-level federated graphs. We first design a tree constructor to improve the representation capability given the limited structural information. We fur
    
[^430]: 一种双层经验风险最小化算法的下界和近似最优算法

    A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08766](http://arxiv.org/abs/2302.08766)

    该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。

    

    双层最优化问题越来越多地应用于机器学习中。在许多实际情况下，上层和下层目标对应于经验风险最小化问题，并因此具有总和结构。在这个背景下，我们提出了一个著名的SARAH算法的双层扩展。我们证明了该算法需要$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$次梯度计算才能实现$\varepsilon$稳定性，其中$n+m$是样本总数，这比先前所有的双层算法都要好。此外，我们提供了一个下界，用于得到双层问题的目标函数的近似稳定点所需的oracle调用次数。这个下界正是我们的算法所达到的，因此在样本复杂度方面是最优的。

    Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
    
[^431]: 关于使用近似传输映射进行抽样的研究

    On Sampling with Approximate Transport Maps. (arXiv:2302.04763v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.04763](http://arxiv.org/abs/2302.04763)

    本研究探讨了两种基于传输映射的抽样方法，研究结果表明，基于流的提议可以处理多峰目标，在高维度和训练不良的情况下使用依赖于重新参数化的方法更加稳健。

    

    通过将分布转化为易于处理的分布，传输映射可以简化具有非平凡几何结构的分布的抽样。随着深度神经网络参数化的传统流（NF）的发展，这种方法的潜力不断提高。NF增强采样器最近提出了将马尔可夫链蒙特卡罗方法与（i）来自流的提议绘制或（ii）基于流的重新参数化相结合。在这两种情况下，学习到的传输的质量会影响性能。本研究首次阐明了这两种方法的相对优势和劣势。我们的研究得出结论：直到中等维度，可以可靠地使用基于流的提议处理多峰目标。相比之下，在高维度和训练不良的情况下，依赖于重新参数化的方法在多模式方面存在困难，但其他方面更为稳健。

    Transport maps can ease the sampling of distributions with non-trivial geometries by transforming them into distributions that are easier to handle. The potential of this approach has risen with the development of Normalizing Flows (NF) which are maps parameterized with deep neural networks trained to push a reference distribution towards a target. NF-enhanced samplers recently proposed blend (Markov chain) Monte Carlo methods with either (i) proposal draws from the flow or (ii) a flow-based reparametrization. In both cases, the quality of the learned transport conditions performance. The present work clarifies for the first time the relative strengths and weaknesses of these two approaches. Our study concludes that multimodal targets can be reliably handled with flow-based proposals up to moderately high dimensions. In contrast, methods relying on reparametrization struggle with multimodality but are more robust otherwise in high-dimensional settings and under poor training. To furthe
    
[^432]: 基于图的时间序列异常检测：综述(arXiv：2302.00058v2 [cs.LG]更新)

    Graph-based Time-Series Anomaly Detection: A Survey. (arXiv:2302.00058v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00058](http://arxiv.org/abs/2302.00058)

    本文综述了基于图的时间序列异常检测，主要探讨了图表示学习的潜力和最先进的图异常检测技术在时间序列中的应用。

    

    随着技术的发展，许多系统持续收集大量时间序列数据，如电子商务、网络安全、车辆维护和医疗监测等领域，时间序列异常检测已成为重要的任务。但由于需要同时考虑变量内部和变量间的依赖性，这一任务非常具有挑战性。近年来，基于图的方法在解决该领域的难题方面取得了重要进展。本综述全面而最新地回顾了基于图的时间序列异常检测(G-TSAD)。首先探讨了图表示学习在时间序列数据中的巨大潜力，然后在时间序列背景下回顾了最先进的图异常检测技术，并讨论了它们的优点和缺点。最后，讨论了这些技术如何应用于实际系统中。

    With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency and the inter-variable dependency, where a variable can be defined as an observation in time series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we explore the significant potential of graph representation learning for time-series data. Then, we review state-of-the-art graph anomaly detection techniques in the context of time series and discuss their strengths and drawbacks. Finally, we discuss the technic
    
[^433]: 逆可解性和安全性及其在联邦学习中的应用

    Inverse Solvability and Security with Applications to Federated Learning. (arXiv:2211.14115v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14115](http://arxiv.org/abs/2211.14115)

    介绍了逆可解性和安全性的概念，以及其在联邦学习中的应用。论文提供了模型示例，展示了如何通过增加用户数量来增加可解性和安全性。

    

    我们介绍了逆可解性和安全性的概念，适用于一般线性前向模型，并展示了如何将其应用于联邦学习中使用的模型。我们提供了这样的模型的示例，其逆可解性和安全性在本文中得到定义。我们还展示了如何利用参与给定迭代的大量用户来增加可解性和安全性。最后，我们讨论了所提出概念的可能扩展，包括非线性情况。

    We introduce the concepts of inverse solvability and security for a generic linear forward model and demonstrate how they can be applied to models used in federated learning. We provide examples of such models which differ in the resulting inverse solvability and security as defined in this paper. We also show how the large number of users participating in a given iteration of federated learning can be leveraged to increase both solvability and security. Finally, we discuss possible extensions of the presented concepts including the nonlinear case.
    
[^434]: 从去噪扩散到去噪马尔科夫模型

    From Denoising Diffusions to Denoising Markov Models. (arXiv:2211.03595v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.03595](http://arxiv.org/abs/2211.03595)

    本论文提出了一个统一的框架，将去噪扩散模型推广到广泛的空间中，并导致分数匹配的原始扩展，适用于各种应用程序。

    

    去噪扩散是展现出卓越实验性能的最先进的生成模型。他们通过将数据分布扩散到高斯分布，然后学习逆转这个噪声过程以获取合成数据点。去噪扩散依赖于使用分数匹配对噪声数据密度的对数导数的逼近。当只能从先验分布和似然函数中进行抽样时，这种模型也可用于执行近似后验模拟。我们提出了一个统一框架，将此方法推广到一类广泛的空间，并导致分数匹配的原始扩展。我们通过各种应用程序说明了所得模型。

    Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalising this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.
    
[^435]: 类别不平衡下的学习动态理论分析

    A Theoretical Analysis of the Learning Dynamics under Class Imbalance. (arXiv:2207.00391v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.00391](http://arxiv.org/abs/2207.00391)

    本文分析证明了数据不平衡对学习的负面影响，说明在使用梯度下降训练时，少数和多数类的学习曲线会遵循次优轨迹，同时提出对每种类别梯度做出贡献的归一化变体，以解决优化不同类别之间的竞争问题。

    

    数据不平衡是机器学习中常见的问题，会严重影响模型性能。虽然有各种解决方案，但它们对学习动态的收敛影响尚未被理解。本文阐明了数据不平衡对学习的显著负面影响，当使用梯度优化器进行训练时，少数类和多数类的学习曲线会遵循次优轨迹。这种放缓与不平衡比相关，可以追溯到优化不同类别之间的竞争。我们的主要贡献在于分析了全批次（GD）和随机梯度下降（SGD）的收敛和各种对每种类别梯度做出贡献的归一化变体。我们发现GD不能保证降低每个类别的损失，但可以通过执行各自归一化梯度来解决这个问题。使用SGD时, 类别不平衡会对算法产生额外的影响。

    Data imbalance is a common problem in machine learning that can have a critical effect on the performance of a model. Various solutions exist but their impact on the convergence of the learning dynamics is not understood. Here, we elucidate the significant negative impact of data imbalance on learning, showing that the learning curves for minority and majority classes follow sub-optimal trajectories when training with a gradient-based optimizer. This slowdown is related to the imbalance ratio and can be traced back to a competition between the optimization of different classes. Our main contribution is the analysis of the convergence of full-batch (GD) and stochastic gradient descent (SGD), and of variants that renormalize the contribution of each per-class gradient. We find that GD is not guaranteed to decrease the loss for each class but that this problem can be addressed by performing a per-class normalization of the gradient. With SGD, class imbalance has an additional effect on th
    
[^436]: 在复杂的多智能体场景中估计反事实治疗结果的时间变化

    Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2206.01900](http://arxiv.org/abs/2206.01900)

    本论文提出了一个可解释的反事实循环网络，用于在复杂的多智能体场景中估计干预效果。该模型考虑了时间变化的多智能体关系和协变量反事实预测的复杂结构，能够准确评估个体治疗效果，并提供解释性。

    

    在各种工程和科学领域中，评估多智能体系统中的干预行为（例如，人类何时应该干预自动驾驶系统，何时球员应该传给队友进行好射门）是一项具有挑战性的任务。使用反事实的长期预测来估计个体治疗效果（ITE）是评估此类干预措施的实用方法。然而，大多数传统框架没有考虑到多智能体关系的时间变化和协变量反事实预测的复杂结构，这可能导致ITE的错误评估和解释困难。在这里，我们提出了一个可解释的反事实循环网络，用于估计干预的效果。我们的模型利用图形变分循环神经网络和基于领域知识的计算来进行基于多智能体协变量和结果的长期预测的ITE估计框架，能够确认循环结构。

    Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
    

