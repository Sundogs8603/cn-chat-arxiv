# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [A Statistical Turing Test for Generative Models.](http://arxiv.org/abs/2309.08913) | 本研究提出了一个统计图灵测试的框架，用于量化人类和机器在给定评估环境下生成内容分布的差异，并演示了如何使用该框架评估生成模型在实现人类水平能力方面的进展。 |
| [^2] | [Efficient Methods for Non-stationary Online Learning.](http://arxiv.org/abs/2309.08911) | 这项工作提出了一种针对非平稳在线学习的高效方法，通过降低每轮投影的数量来优化动态遗憾和自适应遗憾的计算复杂性。 |
| [^3] | [Robust Online Covariance and Sparse Precision Estimation Under Arbitrary Data Corruption.](http://arxiv.org/abs/2309.08884) | 该论文提出了一种在任意数据损坏下鲁棒地在线估计协方差和稀疏精度的算法。 |
| [^4] | [Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems.](http://arxiv.org/abs/2309.08880) | 本文提出了一种实时高效的强化学习算法，可以解决线性离散时间系统的H-infinity控制问题。该算法降低了计算复杂度，完全无模型，且不需要初始稳定策略，能够收敛到闭式解。 |
| [^5] | [PDFTriage: Question Answering over Long, Structured Documents.](http://arxiv.org/abs/2309.08872) | PDFTriage是一种处理长篇结构化文档问答的方法，通过使用结构或内容来检索上下文，解决了大型语言模型在问答中遇到的问题。 |
| [^6] | [Rethinking Learning Rate Tuning in the Era of Large Language Models.](http://arxiv.org/abs/2309.08859) | 本文重新评估了在大语言模型时代中学习率调整的挑战和机遇，并提出了LRBench++来帮助研究者评估学习率策略。 |
| [^7] | [Intelligent machines work in unstructured environments by differential neural computing.](http://arxiv.org/abs/2309.08835) | 本研究提出了一种基于差分神经计算的智能机器方法，通过提取环境信息的主要特征并应用相应的编码刺激到记忆阻性器件，成功地实现了处理无结构环境信息的类人能力，并展现了良好的可扩展性和泛化性。该方法在物体抓取和自动驾驶等应用方面得到了验证。 |
| [^8] | [Distributionally Robust Post-hoc Classifiers under Prior Shifts.](http://arxiv.org/abs/2309.08825) | 研究了先验偏移下的分布鲁棒事后分类器的训练问题，通过在预训练模型的预测上进行缩放调整，以最小化目标分布周围的分布鲁棒损失。 |
| [^9] | [SHAPNN: Shapley Value Regularized Tabular Neural Network.](http://arxiv.org/abs/2309.08799) | SHAPNN是一种使用Shapley值正则化的表格型神经网络，能够提供有效的解释并改善模型的性能和连续学习能力。 |
| [^10] | [Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation.](http://arxiv.org/abs/2309.08793) | Fin-Fact是一个用于多模态金融事实核查和解释生成的基准数据集，通过提供专业的注释和证据，以及多模态信息源来增强事实性分析，从而打击金融领域的错误信息，促进透明度，并建立信任。 |
| [^11] | [BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials.](http://arxiv.org/abs/2309.08788) | 一个名为BioinspiredLLM的开源语言模型利用大量的文献进行微调，能够主动交互地回忆和评估生物材料的信息，提出新的问题和回答，并为生物材料设计提供合理的假设。 |
| [^12] | [Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata.](http://arxiv.org/abs/2309.08787) | 本研究讨论了在电影推荐系统中利用类型标签进行内容元数据分析的挑战，并提出了一种名为“类型频谱”的新方法来捕捉标题中的微妙类型。实验证实了该方法的有效性。此外，该研究还讨论了LLMs在增强内容元数据分析中的应用。 |
| [^13] | [Projected Task-Specific Layers for Multi-Task Reinforcement Learning.](http://arxiv.org/abs/2309.08776) | 本研究提出了一种新的架构，Projected Task-Specific Layers (PTSL)，通过任务特定的层来表达共享和可变的任务信息，成功解决了多任务强化学习中的推广和干扰问题。 |
| [^14] | [Enhance audio generation controllability through representation similarity regularization.](http://arxiv.org/abs/2309.08773) | 这篇论文介绍了一种创新的方法，通过在训练过程中强调音频和文本表示之间的对齐来增强音频生成的可控性。实验结果表明，在音乐和音频生成任务中，这种方法取得了良好的效果。 |
| [^15] | [Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures.](http://arxiv.org/abs/2309.08765) | 通过使用大型语言模型挖掘化学专利，我们得到了一个包含10万个分子及其功能标签的化学功能（CheF）数据集，这些功能标签经过验证是高质量的。 |
| [^16] | [Circular Clustering with Polar Coordinate Reconstruction.](http://arxiv.org/abs/2309.08757) | 本研究提出了一种新的循环数据聚类分析框架，通过使用极坐标重建和数学属性，能够在重构数据集中准确找出聚类结果。 |
| [^17] | [Diverse Neural Audio Embeddings -- Bringing Features back !.](http://arxiv.org/abs/2309.08751) | 本文通过在音频分类任务中学习多样化的特征表示，包括领域特定的音高、音色和神经表示，以及端到端架构，为学习稳健、多样化的表示铺平了道路，并显著提高了性能。 |
| [^18] | [Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits.](http://arxiv.org/abs/2309.08748) | 通过使用Wasserstein距离而不是KL散度，我们提出了一种新颖的分布保证优化方法，用于解决上下文乐队中实际环境不匹配和最坏情况下过度拟合的问题。 |
| [^19] | [AlbNER: A Corpus for Named Entity Recognition in Albanian.](http://arxiv.org/abs/2309.08741) | 本文介绍了一个用于阿尔巴尼亚命名实体识别的语料库AlbNER，该语料库由阿尔巴尼亚维基百科文章中收集的900个带有标记命名实体的句子组成。初步结果表明，模型大小对NER性能影响小，而语言迁移有着显著的影响。这些资源和结果为未来实验提供了基线。 |
| [^20] | [Concept explainability for plant diseases classification.](http://arxiv.org/abs/2309.08739) | 本研究提出了一种名为Testing with Concept Activation Vectors (TCAV)的方法，用于解决基于深度学习的植物疾病分类中的可解释性问题。 |
| [^21] | [Experimental Assessment of a Forward-Collision Warning System Fusing Deep Learning and Decentralized Radio Sensing.](http://arxiv.org/abs/2309.08737) | 该论文提出了一种基于分散式无线电感知方法的前向碰撞警告系统，利用深度学习模块分析波形信号上的多普勒特征，实现对前方来车的检测。实验评估表明该系统在高速公路上的检测性能良好。 |
| [^22] | [Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights.](http://arxiv.org/abs/2309.08731) | 本文提出了一种深度学习方法，通过学习到的ICP权重优化雷达-激光雷达的定位，从而改善了雷达测量对激光雷达地图的定位效果。这一方法在保持高质量地图定位性能的同时，提高了在降水和大雾等恶劣天气条件下的定位准确性。 |
| [^23] | [Clustered Multi-Agent Linear Bandits.](http://arxiv.org/abs/2309.08710) | 本文研究了集群化的多智能体线性赌博机问题，提出了一种新颖的算法，通过智能体之间的协作来加速优化问题。通过理论分析和实证评估，证明了算法在遗憾最小化和聚类质量上的有效性。 |
| [^24] | [Price of Safety in Linear Best Arm Identification.](http://arxiv.org/abs/2309.08709) | 该论文提出了一种具有线性反馈的安全最优臂识别框架，该框架通过利用线性结构来保证在每一轮中不违反阶段性安全约束，提出了一种基于间隙的算法来实现有意义的样本复杂性，并通过实验证明了算法的有效性。 |
| [^25] | [Wasserstein Distributionally Robust Control Barrier Function using Conditional Value-at-Risk with Differentiable Convex Programming.](http://arxiv.org/abs/2309.08700) | 本文提出了一种使用Wasserstein分布鲁棒控制屏障函数的方法，以实现在分布偏移下的韧性，并保持控制屏障函数的优势，通过估计条件风险来衡量分布偏移下的安全约束。 |
| [^26] | [Modelling Irregularly Sampled Time Series Without Imputation.](http://arxiv.org/abs/2309.08698) | SLAN是一种无需插值的方法，可以建模不规则采样时间序列，利用动态适应的LSTM架构来捕捉每个传感器的局部摘要，并在整个观测期间维持一个全局摘要状态。 |
| [^27] | [Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents.](http://arxiv.org/abs/2309.08695) | 本研究通过多语言探索，解决了法律文件中否定范围解析的挑战。实验结果表明，以往模型在处理多语言法律数据时表现不佳，因此我们发布了一套新的法庭判决标注数据用于改进解析效果，并取得了高达86.7％的标记级F1分。 |
| [^28] | [Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions.](http://arxiv.org/abs/2309.08678) | 本论文通过影响函数评估了本地差分隐私对效用损失的影响，并提出了一种方法，可以帮助数据管理员选择最适合其隐私-效用权衡的隐私参数值，而无需进行大量计算。 |
| [^29] | [Quantifying Credit Portfolio sensitivity to asset correlations with interpretable generative neural networks.](http://arxiv.org/abs/2309.08652) | 本研究提出了一种新方法，利用生成的金融相关矩阵量化信贷组合对资产相关性的敏感性。通过使用可解释的变分自动编码器（VAE）的潜在空间表示，揭示了影响投资组合多元化的关键因素。 |
| [^30] | [Cure the headache of Transformers via Collinear Constrained Attention.](http://arxiv.org/abs/2309.08646) | 通过引入共线约束注意力（CoCA）结构，解决Transformer模型中的头痛问题，实现了出色的外推性能和提高的计算效率。 |
| [^31] | [A Stochastic Online Forecast-and-Optimize Framework for Real-Time Energy Dispatch in Virtual Power Plants under Uncertainty.](http://arxiv.org/abs/2309.08642) | 本论文提出了一个适应不确定性的实时能源调度框架，该框架通过整合深度学习预测和随机优化，并通过在线数据增强和模型微调来解决数据波动、模型差异和环境扰动等不确定性问题。 |
| [^32] | [FedFNN: Faster Training Convergence Through Update Predictions in Federated Recommender Systems.](http://arxiv.org/abs/2309.08635) | FedFNN是一种在联邦推荐系统中加速训练的算法。通过预测未抽样用户的权重更新，使用已抽样集的更新，FedFNN实现了比其他方法快5倍的训练速度，同时保持或提高了准确性。 |
| [^33] | [Doubly High-Dimensional Contextual Bandits: An Interpretable Model for Joint Assortment-Pricing.](http://arxiv.org/abs/2309.08634) | 本论文提出了一种双高维上下文强化学习算法，用于解决联合组合-定价问题，通过简单而灵活的模型捕捉协变量和行为之间的相互作用，同时保持可解释性。该方法兼容多种结构化的线性强化学习和定价模型，提供了一种计算可行的流程。 |
| [^34] | [Large Language Models Can Infer Psychological Dispositions of Social Media Users.](http://arxiv.org/abs/2309.08631) | 大型语言模型能够通过分析社交媒体用户的数字足迹推断他们的心理倾向，具体表现为从Facebook状态更新中推断五大人格特质。研究发现，推断得分与自我报告得分之间存在相关性，但在性别和年龄方面存在偏见。 |
| [^35] | [PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions.](http://arxiv.org/abs/2309.08630) | 本研究提出了一种基于图形的喷注表示方法，并设计了一种名为PCN的图神经网络（GNN），利用切比雪夫图卷积（ChebConv）进行深度学习喷注标记，取得了显著的改进。 |
| [^36] | [Recovering from Privacy-Preserving Masking with Large Language Models.](http://arxiv.org/abs/2309.08628) | 本文利用大型语言模型（LLM）探索了替换标识信息的方法，并在下游语言建模任务上进行了评估。实验结果表明，使用混淆语料库训练的模型能够达到可比较的性能。 |
| [^37] | [Evaluating Dynamic Topic Models.](http://arxiv.org/abs/2309.08627) | 提出了一种评估动态主题模型的新方法，该方法分析了每个主题随时间变化的质量变化，并结合了模型的时间一致性。该方法在合成数据和已有DTMs数据上展示了实用性，并与人类判断具有良好的相关性。这些研究结果对于识别变化的主题、评估DTMs和指导未来研究具有重要意义。 |
| [^38] | [Balance Measures Derived from Insole Sensor Differentiate Prodromal Dementia with Lewy Bodies.](http://arxiv.org/abs/2309.08623) | 本研究利用鞋垫传感器获得的平衡测量，提出了一种基于机器学习的自动流程管道来辅助鉴别早期患有Lewy体痴呆的患者。实验证明，该模型能以78.0%的准确率将MCI-LB与其他组别区分开，比基于人口统计和临床神经心理测量的参考模型准确率提高了6.8%。 |
| [^39] | [Variance Reduction of Resampling for Sequential Monte Carlo.](http://arxiv.org/abs/2309.08620) | 本研究提出了一种重要性重采样方案，通过引入重复确定性区域和中位数遍历性的方法，实现了最低的方差，使得顺序蒙特卡洛方法在逼近隐马尔可夫模型时更快且准确。 |
| [^40] | [Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems.](http://arxiv.org/abs/2309.08617) | Drifter是一个高效的在线特征监控系统，通过敏捷、响应和适应性的数据质量监控，实时分析、检测和解决推荐系统中的数据问题，使得实时推荐系统的可靠性和性能得到显著提升。 |
| [^41] | [Multimodal Recommender Systems in the Prediction of Disease Comorbidity.](http://arxiv.org/abs/2309.08613) | 该研究探讨了在医疗领域中利用基于深度学习的推荐系统进行疾病并发症预测的方法。研究使用了NCF和DHF两种新颖的推荐系统，并利用了不同的数据集进行预测。研究结果显示NCF模型在准确率和命中率方面表现较差。 |
| [^42] | [Do the Frankenstein, or how to achieve better out-of-distribution performance with manifold mixing model soup.](http://arxiv.org/abs/2309.08610) | 提出了一种混合模型空间法，在微调模型后生成融合模型，该模型在分布之外表现更好（比最佳单个模型提高了3.5%），同时在原始数据集上也提供更好的准确性。 |
| [^43] | [Monitoring Urban Changes in Mariupol/Ukraine in 2022/23.](http://arxiv.org/abs/2309.08607) | 本文研究证明使用历史数据进行迁移学习是解决城市变化监测问题的可行方案，通过使用合成孔径雷达和光学多光谱观测数据，成功监测了乌克兰马里乌波尔市在俄乌冲突开始阶段的相关城市变化。 |
| [^44] | [CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation.](http://arxiv.org/abs/2309.08019) | CRYPTO-MINE是一种通过神经网络估计互信息的新方法，应用于选择明文攻击中明文和密文之间的互信息估计。该方法可用于分析密码系统的计算安全性和信息泄露与输入分布之间的关系。 |
| [^45] | [Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks.](http://arxiv.org/abs/2309.07937) | Voxtlm是一个统一的只解码模型，能够在语音识别、语音合成、文本生成和语音延续等任务上取得显著的改善。 |
| [^46] | [Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer.](http://arxiv.org/abs/2309.07929) | 本研究提出了一种使用声音提示进行分割的泛化音频-视觉源定位器，在零样本和少样本情况下实现音频-视觉定位和分割任务。通过引入编码器提示解码器范式、构建语义感知音频提示和相关适配器来解决数据稀缺性和不同数据分布的困境。 |
| [^47] | [Text Classification of Cancer Clinical Trial Eligibility Criteria.](http://arxiv.org/abs/2309.07812) | 本文研究了癌症临床试验中常见的排除标准，通过应用文本分类方法和预训练的BERT模型，证明了自动分类排除标准的可行性，并展示了专门为临床试验设计的预训练语言模型的价值。 |
| [^48] | [Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning.](http://arxiv.org/abs/2309.07383) | 本文研究了在强化学习中出现的值函数近似在特定本地空间中的收敛速度，提出了运用算子方程进行离线近似的方法，并通过有限维近似空间中的功率函数得到了值函数近似误差的上界。这些结果改进和细化了值函数近似的收敛性。 |
| [^49] | [Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models.](http://arxiv.org/abs/2309.06655) | 本文提出了一种通过嵌入领域知识的高斯过程状态空间模型，以实现未知领域的检测，并基于递归预测构建了在线的监视器。数值结果表明，带有领域信息的内核可以提供更好的回归质量。 |
| [^50] | [Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction.](http://arxiv.org/abs/2309.06584) | 这项研究提出了一种可解释的图神经网络方法来预测阿尔茨海默病和相关痴呆症的风险。通过将机器学习与索赔数据相结合，不仅能发现额外的风险因素，还能揭示不同医学代码之间的关联。通过评估关系重要性和其对风险预测的影响，该方法能提供全面的解释。 |
| [^51] | [PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis.](http://arxiv.org/abs/2309.05833) | 本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。 |
| [^52] | [Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity.](http://arxiv.org/abs/2309.04160) | 本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。 |
| [^53] | [Diffusion Generative Inverse Design.](http://arxiv.org/abs/2309.02040) | 本文介绍了一种使用去噪扩散模型（DDMs）高效解决反向设计问题的方法，并提出了一种粒子采样算法来进一步改进。 |
| [^54] | [Tropical Geometric Tools for Machine Learning: the TML package.](http://arxiv.org/abs/2309.01082) | TML软件包是第一个包含一套全面工具和方法的R软件包，用于处理与热带凸性相关的基本计算和可视化，以及使用热带度量进行监督和无监督学习模型的统计推断。 |
| [^55] | [Explainability for Large Language Models: A Survey.](http://arxiv.org/abs/2309.01029) | 本文调研了大型语言模型的可解释性问题，提出了一个解释技术的分类法，并介绍了基于Transformer的语言模型的解释方法。同时，讨论了评估生成解释的度量标准，以及如何利用解释来调试模型和提高性能。 |
| [^56] | [Image Hijacking: Adversarial Images can Control Generative Models at Runtime.](http://arxiv.org/abs/2309.00236) | 本研究发现对抗性图像能够在运行时控制生成模型，并提出了通用的方法来创建图像劫持。通过研究三种攻击类型，我们发现这些攻击对最新的视觉语言模型具有高达90％以上的成功率。该研究引发了对基础模型安全性的严重担忧。 |
| [^57] | [Deep Video Codec Control.](http://arxiv.org/abs/2308.16215) | 本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。 |
| [^58] | [LLaSM: Large Language and Speech Model.](http://arxiv.org/abs/2308.15930) | LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。 |
| [^59] | [Elucidating the Exposure Bias in Diffusion Models.](http://arxiv.org/abs/2308.15321) | 本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。 |
| [^60] | [3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking.](http://arxiv.org/abs/2308.15316) | 3D-MuPPET是一个用于估计和跟踪多只鸽子三维姿势的框架，通过多视角实时推测2D关键点并将其三角化到3D空间，同时使用动态匹配和2D跟踪器维持对应关系。相比最先进的3D姿势估计器，具有可比的准确性。该框架还能在使用单只鸽子数据训练的情况下应用于多只鸽子数据，简化领域转换。 |
| [^61] | [Fast Feedforward Networks.](http://arxiv.org/abs/2308.14711) | 快速前馈网络是一种对于前馈网络的改进架构，能够以更快的速度进行推理，并具有比专家混合模型更好的训练性能。在视觉转换器中，它们可以仅使用1%的层神经元进行推理，同时保持94.2%的预测性能。 |
| [^62] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^63] | [Exploration of Rashomon Set Assists Explanations for Medical Data.](http://arxiv.org/abs/2308.11446) | 本文提出了一种新的过程来探索和分析医疗数据中的拉舒蒙集合模型，从而超越传统单一模型选择的方法，并通过引入"拉舒蒙检测"算法识别出集合中最不同的模型。 |
| [^64] | [How Expressive are Graph Neural Networks in Recommendation?.](http://arxiv.org/abs/2308.11127) | 本文对图神经网络在推荐中的表达能力进行了理论分析，发现现有的表达能力度量标准可能无法有效评估模型在推荐中的能力，提出了一个全面的理论分析方法。 |
| [^65] | [Dyadic Reinforcement Learning.](http://arxiv.org/abs/2308.07843) | 该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。 |
| [^66] | [Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads.](http://arxiv.org/abs/2308.07013) | RusKey是一个基于强化学习的键值存储系统，通过在线编排LSM树结构来实现在动态工作负载下的稳健性能，并引入了FLSM树设计来高效转换不同压实策略，无需先验工作负载知识。 |
| [^67] | [Change Point Detection With Conceptors.](http://arxiv.org/abs/2308.06213) | 我们提出了一种使用概念器矩阵进行变点检测的方法，通过学习时间序列中的特征动态，并利用单变量量化来识别变点。该方法在条件和无条件的变点检测问题上进行了测试，可以提供潜在的需要进一步研究的感兴趣位置。 |
| [^68] | [Investigation of compressor cascade flow based on physics-informed neural networks.](http://arxiv.org/abs/2308.04501) | 本研究首次使用物理信息神经网络（PINNs）方法来预测压缩机叶栅的流场，结果表明PINNs在预测精度和处理反向问题方面比传统CFD方法具有明显优势。 |
| [^69] | [Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach.](http://arxiv.org/abs/2308.03887) | 本论文提出了一种使用时间对称的深度学习方法来提升细胞跟踪的准确性。该方法不依赖于连续帧跟踪，而是基于细胞的时空邻域进行跟踪，具有学习细胞运动模式的能力，并能处理具有严重伪影的大量视频帧。 |
| [^70] | [A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation.](http://arxiv.org/abs/2308.00287) | 这项研究关注于无监督领域自适应中的评估度量。通过将源准确率纳入度量中，并使用一种新的MLP分类器进行改进，该研究解决了原有度量的不足之处，并将其与数据增强相结合，改进了模型的质量评估。 |
| [^71] | [Fixed Integral Neural Networks.](http://arxiv.org/abs/2307.14439) | 本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。 |
| [^72] | [Efficient Estimation of the Local Robustness of Machine Learning Models.](http://arxiv.org/abs/2307.13885) | 本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。 |
| [^73] | [RED CoMETS: An ensemble classifier for symbolically represented multivariate time series.](http://arxiv.org/abs/2307.13679) | 本文介绍了一种名为RED CoMETS的集成分类器，用于处理符号化表示的多变量时间序列数据。它在多变量设置中展现出竞争力的准确性，并在'HandMovementDirection'数据集上实现了最高的报告准确性。 |
| [^74] | [Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition.](http://arxiv.org/abs/2307.09762) | 该论文提出了一种算法框架，通过将模式识别和随机滤波理论的技术结合起来，强化了基于POD的反应扩散复杂网络模型简化技术，在受扰动输入的情况下提高了代理模型的准确性。 |
| [^75] | [vONTSS: vMF based semi-supervised neural topic modeling with optimal transport.](http://arxiv.org/abs/2307.01226) | vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。 |
| [^76] | [Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis.](http://arxiv.org/abs/2306.17181) | 本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。 |
| [^77] | [Text-Driven Foley Sound Generation With Latent Diffusion Model.](http://arxiv.org/abs/2306.10359) | 本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。 |
| [^78] | [Fair Causal Feature Selection.](http://arxiv.org/abs/2306.10336) | 提出了一种公平因果特征选择算法FairCFS，通过构建局部因果图识别类和敏感变量的马尔可夫毯，阻止敏感信息的传输，从而实现公平的特征选择。经过大量实验证实，FairCFS在特征选择准确性方面与八种最先进的算法相媲美，同时具有更优越的公平性。 |
| [^79] | [Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning.](http://arxiv.org/abs/2306.09273) | 这篇论文提出了一种针对值函数算法和梯度算法的攻击方法，利用梯度反转重建状态、动作和监督信号，以解决嵌入式人工智能中的隐私泄露问题。 |
| [^80] | [Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning.](http://arxiv.org/abs/2306.06766) | 该论文提出了一种基于物理信息强化学习的零样本无线室内导航方法，通过样本高效学习和零样本泛化来提高导航效率。 |
| [^81] | [An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation.](http://arxiv.org/abs/2305.14704) | 本文推导和评估了四种贝叶斯批次赌博算法，用于自适应确定流量分配，全面评估了这些算法的可信度、敏感性和后悔度，仿真结果表明这些基于批次的贝叶斯算法是有效的。 |
| [^82] | [SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres.](http://arxiv.org/abs/2305.13617) | 这篇论文提出了一种称为SPEECH的模型，它使用能量建模来表示复杂的事件结构，并使用超球来表示事件类别。实验结果表明，SPEECH在事件检测和事件关系抽取任务中表现出卓越的性能。 |
| [^83] | [Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset.](http://arxiv.org/abs/2305.10775) | 该论文提出了一种新的几何变换方法，将X-Ray Microbeam数据集中的解剖标记物的X-Y坐标沿中矢状面映射到多个相对测量中，进而改进了测量的准确性。 |
| [^84] | [Edge Directionality Improves Learning on Heterophilic Graphs.](http://arxiv.org/abs/2305.10498) | 本文提出了一种新的有向图神经网络框架Dir-GNN，并在有向引用图上进行评估，结果表明它在预测缺失的引用链接方面优于现有的无向GNN和一些有向图模型。 |
| [^85] | [Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites.](http://arxiv.org/abs/2305.09820) | 这篇论文研究了机器生成文章在虚假新闻和主流新闻网站的普及程度，发现虚假新闻网站上合成文章的使用速度比主流网站上更快。 |
| [^86] | [Finite Expression Methods for Discovering Physical Laws from Data.](http://arxiv.org/abs/2305.08342) | 本文介绍了一种名为"有限表达法" (FEX) 的深度符号学习方法，通过学习动态数据中PDE解的导数，发现控制方程的解析表达式。相对于其他现有方法，FEX在多种问题上表现出更好的数值性能，包括时变的PDE问题和具有时变系数的非线性动力系统。 |
| [^87] | [Learning Human-Human Interactions in Images from Weak Textual Supervision.](http://arxiv.org/abs/2304.14104) | 本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。 |
| [^88] | [Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics.](http://arxiv.org/abs/2304.14094) | 本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。 |
| [^89] | [Node Feature Augmentation Vitaminizes Network Alignment.](http://arxiv.org/abs/2304.12751) | 本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。 |
| [^90] | [Fairness in Visual Clustering: A Novel Transformer Clustering Approach.](http://arxiv.org/abs/2304.07408) | 本文提出了一种新的Transformer聚类方法，通过引入聚类纯度作为指标，采用新的损失函数来维持聚类模型的公平性，同时引入Cross-attention机制提高聚类的纯度。 |
| [^91] | [Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning.](http://arxiv.org/abs/2304.07163) | 本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。 |
| [^92] | [counterfactuals: An R Package for Counterfactual Explanation Methods.](http://arxiv.org/abs/2304.06569) | 该论文介绍了一个统一且模块化的 R6 接口，用于具体实现反事实解释方法。通过实现三种方法并推广到不同的情境中，结合真实用例，此方法能够快速准确地得出有关如何更改单个观测值的特征值以获得所需预测的信息。 |
| [^93] | [Difficult Lessons on Social Prediction from Wisconsin Public Schools.](http://arxiv.org/abs/2304.06205) | 美国公立学校引入的预测算法（EWS）未能提高毕业率，EWS准确性高，但环境因素影响更大。 |
| [^94] | [Exact and Cost-Effective Automated Transformation of Neural Network Controllers to Decision Tree Controllers.](http://arxiv.org/abs/2304.06049) | 本文研究了将基于神经网络的控制器转换为等效软决策树控制器并提出了一种自动且节约成本的转换算法。该方法适用于包括ReLU激活函数在内的离散输出NN控制器，并能够提高形式验证的运行效率。 |
| [^95] | [A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control.](http://arxiv.org/abs/2304.04066) | 本文提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，针对强化学习控制现实世界系统时的安全稳定控制问题提出了一种解决方案。其中，基于重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。 |
| [^96] | [Blockwise Compression of Transformer-based Models without Retraining.](http://arxiv.org/abs/2304.01483) | 本论文提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，实现了降低部署门槛的目的。 |
| [^97] | [Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity.](http://arxiv.org/abs/2303.13634) | 本文提出了一种物理学指导的神经网络模型PIPN，它能利用稀疏标记数据同时预测所需偏微分方程在数百个不同的几何体上的解，有望在工业界进行快速的几何设计。 |
| [^98] | [Semantic segmentation of surgical hyperspectral images under geometric domain shifts.](http://arxiv.org/abs/2303.10972) | 外科高光谱图像的语义分割面临几何域转换挑战。我们提出了一种分析最新语义分割网络在几何分布转换数据上的方法，并通过“器官移植”增强技术解决了泛化性问题。 |
| [^99] | [Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament.](http://arxiv.org/abs/2303.07925) | 本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。 |
| [^100] | [Generalised Scale-Space Properties for Probabilistic Diffusion Models.](http://arxiv.org/abs/2303.07900) | 本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。 |
| [^101] | [RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback.](http://arxiv.org/abs/2303.07622) | RE-MOVE提出了一种基于语言反馈的自适应策略设计方法，可以使机器人适应实时环境变化，并从人类反馈中学习并适应之前未见过的对抗性场景。 |
| [^102] | [Virtual Guidance as a Mid-level Representation for Navigation.](http://arxiv.org/abs/2303.02731) | 该论文介绍了一种名为“虚拟导航”的新技术，通过在智能体的相机视图上叠加彩色路径或球的形式的视觉指引，以易于理解的导航指令传达抽象的导航信息。实验结果表明，在模拟和真实环境中，虚拟导航在遵循计划路径和避开障碍物等多个指标上优于现有方法。 |
| [^103] | [Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement.](http://arxiv.org/abs/2303.01388) | 本论文介绍了一种利用多智能体深度强化学习的标签放置方法，该方法在数据可视化中解决了标签重叠和可读性问题，与现有的手工设计算法相比具有显著的优势。 |
| [^104] | [Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications.](http://arxiv.org/abs/2303.01346) | 本文提出了一种通过共学习规划和控制策略来解决带有复杂逻辑约束的高维度机器人导航任务的强化学习方法。相比现有算法，这种方法通过降低样本复杂性来训练出高质量的策略，并且能够高效地生成长期的机器人运动路径。实验证明了该方法的有效性。 |
| [^105] | [Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach.](http://arxiv.org/abs/2302.13423) | 本文提出了一种基于共识的模拟与实际深度强化学习算法，该算法在机器人操纵中表现出可比较的性能，并发现了在模拟中的最佳策略不一定适用于模拟与实际训练，以及模拟智能体数量越多，模拟与实际训练效果越好。 |
| [^106] | [Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings.](http://arxiv.org/abs/2302.07729) | 该论文提出了一种使用指针生成网络和SciBERT嵌入来自动生成研究论文亮点的方法。在多个基准数据集上的实验证明，该模型在研究亮点生成方面具有最佳性能。 |
| [^107] | [Stop overkilling simple tasks with black-box models and use transparent models instead.](http://arxiv.org/abs/2302.02804) | 过度使用黑匣子模型会导致简单任务的浪费，透明模型可以提高效率和精度。 |
| [^108] | [Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach.](http://arxiv.org/abs/2302.02179) | 本文提出了基于技能的分层驾驶策略，通过设计和使用运动原语作为高层动作，大幅减少了训练时间，并且在合并场景中得到了更高性能的驾驶模型。 |
| [^109] | [3D-Spatiotemporal Forecasting the Expansion of Supernova Shells Using Deep Learning toward High-Resolution Galaxy Simulations.](http://arxiv.org/abs/2302.00026) | 本文开发了一个深度学习模型，3D-MIM，用于预测超新星爆炸后的壳层扩张，通过在平滑粒子流体动力学模拟中检测并预测超新星影响粒子所在的壳层形状，解决了高分辨率星系模拟中超新星积分时间步长问题。 |
| [^110] | [Combinatorial Causal Bandits without Graph Skeleton.](http://arxiv.org/abs/2301.13392) | 本文研究了在二值一般因果模型和BGLMs上不考虑图骨架的组合因果赌博机问题，提出了可在BGLMs上实现的无需图骨架的遗憾最小化算法，达到了与依赖于图结构的最先进算法相同的渐进遗憾率$O(\sqrt{T}\ln T)$。 |
| [^111] | [Neural Operator: Is data all you need to model the world? An insight into the impact of Physics Informed Machine Learning.](http://arxiv.org/abs/2301.13331) | 本文探讨了如何将数据驱动方法与传统技术相结合，以解决工程和物理问题，并指出了机器学习方法的一些主要问题。 |
| [^112] | [One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER.](http://arxiv.org/abs/2301.10410) | 本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。 |
| [^113] | [Tailor: Altering Skip Connections for Resource-Efficient Inference.](http://arxiv.org/abs/2301.07247) | 本文介绍了一种硬件-软件协同设计方法，通过逐渐去除或缩短神经网络中的跳跃连接来实现更高效的硬件推断。实验结果显示，该方法可以显著提高硬件资源利用率和性能，同时减少内存带宽的需求。 |
| [^114] | [A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends.](http://arxiv.org/abs/2301.05712) | 本综述论文从算法、应用和趋势的角度概述了自监督学习的多维视角。它介绍了SSL算法的动机、共性和差异，以及在图像处理、计算机视觉和自然语言处理等领域中的典型应用。 |
| [^115] | [Equivariant Representation Learning in the Presence of Stabilizers.](http://arxiv.org/abs/2301.05231) | 引入了一种称为EquIN的方法，用于学习在数据上具有一般群作用等变的表示。通过考虑稳定子，该方法可以提取数据的几何结构。 |
| [^116] | [Settling the Reward Hypothesis.](http://arxiv.org/abs/2212.10420) | 解决奖励假设，明确指明假设成立的目标和目的的隐含要求。 |
| [^117] | [Reasoning with Language Model Prompting: A Survey.](http://arxiv.org/abs/2212.09597) | 本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。 |
| [^118] | [Over-The-Air Federated Learning Over Scalable Cell-free Massive MIMO.](http://arxiv.org/abs/2212.06482) | 本论文研究了如何利用无蜂窝大规模MIMO支持联邦边缘学习，并提出了一种实际的OTA-FL实现方法，通过空中计算减轻了联邦学习在无线网络中的通信开销，并验证了无蜂窝大规模MIMO的优势。 |
| [^119] | [FedALA: Adaptive Local Aggregation for Personalized Federated Learning.](http://arxiv.org/abs/2212.01197) | FedALA是一种用于个性化联邦学习的方法，通过自适应局部聚合（ALA）模块来解决统计异质性问题，并在广泛的实验证明中超过了11种最先进的基准模型。 |
| [^120] | [Decentralized Matrix Factorization with Heterogeneous Differential Privacy.](http://arxiv.org/abs/2212.00306) | 本文提出了一种用于不可信推荐者的异构差分隐私矩阵分解算法，通过修改拉伸机制和重新缩放方案，实现了隐私和准确性的权衡。 |
| [^121] | [Pattern Attention Transformer with Doughnut Kernel.](http://arxiv.org/abs/2211.16961) | 本论文提出了一种新的模式注意力变换器(PAT)，它采用了新的圆环核设计，以解决图像分类中像素高分辨率的问题。 |
| [^122] | [AdsorbML: A Leap in Efficiency for Adsorption Energy Calculations using Generalizable Machine Learning Potentials.](http://arxiv.org/abs/2211.16486) | 本文展示了如何利用机器学习势函数更准确地、以更高效率地识别低能吸附物-表面配置。我们的算法在准确性和效率之间提供了一系列权衡，其中一个平衡选项在计算中发现最低能量结构的准确率为87.36％，同时计算速度提高了2000倍。 |
| [^123] | [A fermion neural network with efficient optimization and quantum applicability.](http://arxiv.org/abs/2211.05793) | 本文提出了一种费米子神经网络（FNN），它将输入作为初始层，输出物理特性，建立了一种高效的优化方法，可应用于具有相互作用的硬量子系统，而且能够精确地确定拓扑相和紧凑电荷序，其量子特性带来多种优势。 |
| [^124] | [Generative Knowledge Graph Construction: A Review.](http://arxiv.org/abs/2210.12714) | 本文综述了生成式知识图谱构建领域的最新进展，包括方法分类和优劣分析，并提出了未来的研究方向。 |
| [^125] | [Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction.](http://arxiv.org/abs/2210.10709) | 提出了一种以检索增强的架构感知参考作为提示的方法，可动态利用人类注释和弱监督数据所继承的架构和知识，指导生成具有更好语义连贯性和一致性的结构化知识，从而在数据效率和知识质量方面具有优越性。 |
| [^126] | [Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study.](http://arxiv.org/abs/2210.10678) | 本文针对低资源环境中的关系抽取进行了实证研究，并提出了三种方案来提高性能，包括使用提示方法、平衡方法和数据增强技术。通过对8个关系抽取数据集的广泛比较，实验结果表明，虽然基于提示的调整有益于低资源关系抽取，但仍有改进空间，尤其是跨句子上下文中的多个关系三元组的抽取。 |
| [^127] | [Deep learning extraction of band structure parameters from density of states: a case study on trilayer graphene.](http://arxiv.org/abs/2210.06310) | 本论文提出了一个深度学习方法，可以从实验数据中准确地推导出材料的带结构参数。通过研究三层石墨烯的穿透场电容测量，证明了该方法的有效性。 |
| [^128] | [Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing.](http://arxiv.org/abs/2210.02622) | 本论文提出了三种新的CMA-MAE变体，利用高效近似方法提高了其可扩展性，相较于ES基线在基准测试中表现更好，并且达到或超过最先进的深度强化学习算法的性能。 |
| [^129] | [Automatic Data Augmentation via Invariance-Constrained Learning.](http://arxiv.org/abs/2209.15031) | 本论文提出了一种通过在解决学习任务时自动适应数据增强的方法，将数据增强表述为基于不变性约束的学习问题，并利用蒙特卡洛采样来解决，提出的算法能够优化学习任务的解决方案。 |
| [^130] | [Understanding of the properties of neural network approaches for transient light curve approximations.](http://arxiv.org/abs/2209.07542) | 对于瞬变光曲线逼近，本文研究了基于神经网络的多种逼近方法，以生成时间步长规则的时间序列。 |
| [^131] | [Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine.](http://arxiv.org/abs/2209.06261) | 本文描述了一种基于可微物理引擎的真实世界到仿真世界转移的策略，该策略通过对真实机器人的有限数据进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。该策略在索驱动张力结构机器人上得到了测试，并证明了其有效性。 |
| [^132] | [TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second.](http://arxiv.org/abs/2207.01848) | TabPFN是一种可以在不到一秒钟内完成小型表格数据集的监督分类的Transformer，无需超参数调整，并且具有竞争力。它使用先验适应网络（PFN）逼近基于先验的贝叶斯推断，先验融合了因果推理的思想。 |
| [^133] | [Sum-of-Squares Relaxations for Information Theory and Variational Inference.](http://arxiv.org/abs/2206.13285) | 本论文研究了基于平方和松弛方法在信息论和变分推断中的应用。通过使用这种方法，我们提出了计算$f$-divergences的凸松弛算法，其中涉及到从非局部协方差矩阵计算这些divergences的问题。这些结果对于数据科学中的多个应用具有重要意义。 |
| [^134] | [Neural Moving Horizon Estimation for Robust Flight Control.](http://arxiv.org/abs/2206.10397) | 本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。 |
| [^135] | [Multiple Testing Framework for Out-of-Distribution Detection.](http://arxiv.org/abs/2206.09522) | 本研究提出了一个多重检验框架用于离群分布检测的问题，包括了定义OOD概念和提供强有力保证的方法，与之前的基于阈值的测试相比，在不同类型的OOD实例中表现更一致。 |
| [^136] | [An Empirical Study of Retrieval-enhanced Graph Neural Networks.](http://arxiv.org/abs/2206.00362) | 这项研究考察了检索增强的图神经网络在图数据集中的有效性，设计了一种称为GRAPHRETRIEVAL的检索增强方案，为图神经网络中学习的有用信息提供了增强。 |
| [^137] | [DeepCluE: Enhanced Image Clustering via Multi-layer Ensembles in Deep Neural Networks.](http://arxiv.org/abs/2206.00359) | 本文提出了DeepCluE方法，通过利用深度神经网络中的多个层次，桥接了深度聚类和集成聚类之间的差距，从而增强了图像聚类的性能。 |
| [^138] | [Addressing Strategic Manipulation Disparities in Fair Classification.](http://arxiv.org/abs/2205.10842) | 该论文研究了公平分类中存在的战略操纵差异问题，提出了一个受约束的优化框架来解决这个问题。 |
| [^139] | [A Comprehensive Survey on Model Quantization for Deep Neural Networks.](http://arxiv.org/abs/2205.07877) | 本文综述了深度神经网络中的模型量化，这是一种用低位宽存储全精度值以实现节约内存和操作成本的压缩方法。文章分类介绍了各种量化方法，并讨论了使用比例因子匹配数据范围和适当的训练方法的重要性。本文还回顾了模型量化的最新研究，并强调了其优缺点和当前的挑战和未来研究方向。 |
| [^140] | [Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion.](http://arxiv.org/abs/2205.02357) | 本文提出了一种混合Transformer与多级融合的方法，用于解决多模态知识图谱补全的问题。该方法通过统一的输入-输出架构适用于多样的任务，同时利用多级融合将视觉和文本表示集成起来。 |
| [^141] | [On-Device Learning: A Neural Network Based Field-Trainable Edge AI.](http://arxiv.org/abs/2203.01077) | 本研究介绍了一种基于神经网络的设备上学习方法，针对边缘人工智能应用中的环境因素对准确性造成的影响进行了解决。通过重新训练，在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。 |
| [^142] | [L4KDE: Learning for KinoDynamic Tree Expansion.](http://arxiv.org/abs/2203.00975) | L4KDE是一种用于运动规划的学习方法，通过使用神经网络来预测状态之间的过渡成本，解决了树扩展中选择低成本节点的问题。 |
| [^143] | [When AUC meets DRO: Optimizing Partial AUC for Deep Learning with Non-Convex Convergence Guarantee.](http://arxiv.org/abs/2203.00176) | 本文提出了一种基于梯度的方法，通过分布鲁棒优化（DRO）来最大化深度学习中的局部AUC（pAUC），并提出了准确和平滑的pAUC估计量。实验证明了该方法在各种数据集上的有效性。 |
| [^144] | [Self-Training: A Survey.](http://arxiv.org/abs/2202.12040) | 自主训练方法是一种半监督算法，无需额外假设数据分布，在低密度区域找到决策边界，并使用学习分类器的输出分数作为置信度，通过为无标签样本分配伪标签，迭代地学习分类器，从而丰富有标签训练数据。 |
| [^145] | [Adaptive and Robust Multi-Task Learning.](http://arxiv.org/abs/2202.05250) | 本文提出一系列自适应方法，能够同时处理多任务学习的相似性和差异性，并具有统计保证和鲁棒性。 |
| [^146] | [Reward-Respecting Subtasks for Model-Based Reinforcement Learning.](http://arxiv.org/abs/2202.03466) | 论文提出了一种基于模型的强化学习的方法，通过添加奖励加成的子任务来发现选项，从而解决了以前方法中忽略原始奖励的问题。 |
| [^147] | [DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population.](http://arxiv.org/abs/2201.03335) | DeepKE是一个基于深度学习的知识提取工具包，支持复杂的低资源、文档级和多模态场景，可用于自定义数据集和模型来从非结构化数据中提取信息。 |
| [^148] | [Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data.](http://arxiv.org/abs/2112.04351) | 这项研究通过分析大学社区中的Reddit数据，研究了COVID-19疫情对人们情绪和心理状态的影响，并提出了基于RoBERTa和GAT的情绪分类模型。 |
| [^149] | [PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization.](http://arxiv.org/abs/2112.03676) | 本文提出了一种逐层和逐通道的渐进式退出方法用于领域泛化，该方法比以前的层级特定退出方法具有更强的正则化效果，能够更充分地缓解源领域上的过拟合问题。 |
| [^150] | [Interpretable and Fair Boolean Rule Sets via Column Generation.](http://arxiv.org/abs/2111.08466) | 本文提出了一种通过列生成法来生成可解释和公平的布尔规则集合的方法。该方法在兼顾准确性和简单性的平衡方面优于常用的启发式规则挖掘方法，同时考虑了公平性设置，并扩展了模型以满足两种不同的分类公平性度量。使用近似列生成算法处理大规模数据集，并在实验中取得了良好的结果。 |
| [^151] | [Exact Pareto Optimal Search for Multi-Task Learning and Multi-Criteria Decision-Making.](http://arxiv.org/abs/2108.00597) | 提出了一种新的多目标优化（MOO）方法，EPO Search，用于解决多任务学习（MTL）和多标准决策制定（MCDM）中切比雪夫标量化方法在收敛性和计算效率方面的问题。 |
| [^152] | [Contextualizing Meta-Learning via Learning to Decompose.](http://arxiv.org/abs/2106.08112) | 通过学习解耦的方法，我们提出了一种上下文化的元学习策略，用于从支持集中推断目标模型。我们的方法能够捕捉到实例级的模糊相似性，从而提高了模型的性能和泛化能力。 |
| [^153] | [Privacy-Preserving Constrained Domain Generalization for Medical Image Classification.](http://arxiv.org/abs/2105.08511) | 该论文提出了一种隐私保护约束域泛化的方法，通过改进信息聚合过程来提高模型的泛化能力，并解决了由于数据隐私保护问题导致的医学影像分类的挑战。 |
| [^154] | [A Class of Dimension-free Metrics for the Convergence of Empirical Measures.](http://arxiv.org/abs/2104.12036) | 本文提出了一类无维度度量，用于高维情况下经验测度的收敛性，解决了维度灾难问题，具有重要的实际应用。 |
| [^155] | [KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction.](http://arxiv.org/abs/2104.07650) | 本文提出了一种名为KnowPrompt的知识感知提示调整方法，通过将关系标签中的潜在知识融入到提示构建中，并通过协同优化的方式，提高了关系抽取任务的性能。 |
| [^156] | [A General Framework for the Practical Disintegration of PAC-Bayesian Bounds.](http://arxiv.org/abs/2102.08649) | 该论文提出了一种新的PAC-Bayesian泛化界的框架，该框架能够提供解离界，相比现有框架在神经网络上有显著的实用改进。 |
| [^157] | [Lazy OCO: Online Convex Optimization on a Switching Budget.](http://arxiv.org/abs/2102.03803) | 本研究提出了一种懒惰型在线凸优化的算法，其在切换次数有限的情况下达到了近似最优的遗憾上界，并且在连续设置中呈现出高效的计算性能。 |
| [^158] | [Acting in Delayed Environments with Non-Stationary Markov Policies.](http://arxiv.org/abs/2101.11992) | 该论文介绍了在延迟环境中，学习和规划的马尔可夫决策过程(MDP)框架，证明了在延迟执行的情况下，原始状态空间中的非固定马尔可夫策略可以实现最大奖励，提出了一种解决延迟执行任务的非固定Q-learning风格算法。 |
| [^159] | [Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy.](http://arxiv.org/abs/2012.14098) | 本文首次尝试在平均奖励设置下，通过方差风险准则研究风险敏感的深度强化学习。我们提出了一个方差约束的策略优化问题，并设计了一种演员-评论家算法来解决该问题。 |
| [^160] | [A Survey of Privacy Attacks in Machine Learning.](http://arxiv.org/abs/2007.07646) | 该研究对机器学习中的隐私攻击进行了调查，提出了攻击分类和威胁模型，并分析了不同攻击的原因和防御方法。 |
| [^161] | [Advanced Capsule Networks via Context Awareness.](http://arxiv.org/abs/1903.07497) | 本研究通过增加池化层和重建层来改进胶囊网络（CN）的设计，以适应具有不同上下文的图像数据集，并与深度学习（DL）模型进行了性能对比。结果显示，CN在大大减少训练时间的同时表现出了与DL模型相当的性能。 |

# 详细

[^1]: 一个统计图灵测试用于生成模型

    A Statistical Turing Test for Generative Models. (arXiv:2309.08913v1 [cs.AI])

    [http://arxiv.org/abs/2309.08913](http://arxiv.org/abs/2309.08913)

    本研究提出了一个统计图灵测试的框架，用于量化人类和机器在给定评估环境下生成内容分布的差异，并演示了如何使用该框架评估生成模型在实现人类水平能力方面的进展。

    

    人工智能系统在文本、音频和视觉等领域的内容生成能力的出现催生了用于区分内容来源于人还是机器的分类器的发展。这些工作的隐含假设是人类的生成能力与机器的生成能力存在差异。本文提供了一个在统计模式识别语言中量化人类和机器生成内容分布差异的框架，并描述了如何在框架中评估生成模型在向人类水平能力方面的进展，涵盖了多个分析维度。

    The emergence of human-like abilities of AI systems for content generation in domains such as text, audio, and vision has prompted the development of classifiers to determine whether content originated from a human or a machine. Implicit in these efforts is an assumption that the generation properties of a human are different from that of the machine. In this work, we provide a framework in the language of statistical pattern recognition that quantifies the difference between the distributions of human and machine-generated content conditioned on an evaluation context. We describe current methods in the context of the framework and demonstrate how to use the framework to evaluate the progression of generative models towards human-like capabilities, among many axes of analysis.
    
[^2]: 非平稳在线学习的高效方法

    Efficient Methods for Non-stationary Online Learning. (arXiv:2309.08911v1 [cs.LG])

    [http://arxiv.org/abs/2309.08911](http://arxiv.org/abs/2309.08911)

    这项工作提出了一种针对非平稳在线学习的高效方法，通过降低每轮投影的数量来优化动态遗憾和自适应遗憾的计算复杂性。

    

    非平稳在线学习近年来引起了广泛关注。特别是在非平稳环境中，动态遗憾和自适应遗憾被提出作为在线凸优化的两个原则性性能度量。为了优化它们，通常采用两层在线集成，由于非平稳性的固有不确定性，其中维护一组基学习器，并采用元算法在运行过程中跟踪最佳学习器。然而，这种两层结构引发了关于计算复杂性的担忧 -这些方法通常同时维护$\mathcal{O}(\log T)$个基学习器，对于一个$T$轮在线游戏，因此每轮执行多次投影到可行域上，当域很复杂时，这成为计算瓶颈。在本文中，我们提出了优化动态遗憾和自适应遗憾的高效方法，将每轮的投影次数从$\mathcal{O}(\log T)$降低到...

    Non-stationary online learning has drawn much attention in recent years. In particular, dynamic regret and adaptive regret are proposed as two principled performance measures for online convex optimization in non-stationary environments. To optimize them, a two-layer online ensemble is usually deployed due to the inherent uncertainty of the non-stationarity, in which a group of base-learners are maintained and a meta-algorithm is employed to track the best one on the fly. However, the two-layer structure raises the concern about the computational complexity -- those methods typically maintain $\mathcal{O}(\log T)$ base-learners simultaneously for a $T$-round online game and thus perform multiple projections onto the feasible domain per round, which becomes the computational bottleneck when the domain is complicated. In this paper, we present efficient methods for optimizing dynamic regret and adaptive regret, which reduce the number of projections per round from $\mathcal{O}(\log T)$ t
    
[^3]: 在任意数据损坏下的鲁棒在线协方差和稀疏精度估计

    Robust Online Covariance and Sparse Precision Estimation Under Arbitrary Data Corruption. (arXiv:2309.08884v1 [cs.LG])

    [http://arxiv.org/abs/2309.08884](http://arxiv.org/abs/2309.08884)

    该论文提出了一种在任意数据损坏下鲁棒地在线估计协方差和稀疏精度的算法。

    

    高斯图模型被广泛用于表示实体间的相关性，但仍然容易受到数据损坏的影响。在这项工作中，我们引入了一种修改后的修剪内积算法，在任意和对抗性数据攻击存在的情况下，在在线场景中鲁棒地估计协方差。在每个时间步骤中，正常地独立同分布于多元高斯分布的数据点到来。然而，其中一定比例的数据点可能已被任意破坏。我们提出了一种在线算法，尽管存在数据损坏，可以估计稀疏逆协方差（即精度）矩阵。我们提供了我们算法下估计精度矩阵的误差界和收敛性质。

    Gaussian graphical models are widely used to represent correlations among entities but remain vulnerable to data corruption. In this work, we introduce a modified trimmed-inner-product algorithm to robustly estimate the covariance in an online scenario even in the presence of arbitrary and adversarial data attacks. At each time step, data points, drawn nominally independently and identically from a multivariate Gaussian distribution, arrive. However, a certain fraction of these points may have been arbitrarily corrupted. We propose an online algorithm to estimate the sparse inverse covariance (i.e., precision) matrix despite this corruption. We provide the error-bound and convergence properties of the estimates to the true precision matrix under our algorithms.
    
[^4]: 数据驱动的实时高效强化学习算法在H-infinity控制中的应用：自主移动需求系统

    Data-Driven H-infinity Control with a Real-Time and Efficient Reinforcement Learning Algorithm: An Application to Autonomous Mobility-on-Demand Systems. (arXiv:2309.08880v1 [eess.SY])

    [http://arxiv.org/abs/2309.08880](http://arxiv.org/abs/2309.08880)

    本文提出了一种实时高效的强化学习算法，可以解决线性离散时间系统的H-infinity控制问题。该算法降低了计算复杂度，完全无模型，且不需要初始稳定策略，能够收敛到闭式解。

    

    强化学习是一类人工智能算法，通过在线学习设计适应性最优控制器。本文提出了一种基于模型无关、实时、数据高效的Q-learning算法来解决线性离散时间系统的H-infinity控制问题。计算复杂度从文献中的O(q^3)降低到了提出的算法的O(q^2)，其中q是状态变量、控制输入和干扰的大小之和的二次项。通过在线学习，设计了自适应最优控制器，并学习了动作和评论网络的参数，不需要对系统动力学的了解，使得该算法完全无模型。此外，仅在第一次迭代中需要足够的扰动噪声，而不影响提出的算法。无需初始稳定策略，算法收敛到闭式解。

    Reinforcement learning (RL) is a class of artificial intelligence algorithms being used to design adaptive optimal controllers through online learning. This paper presents a model-free, real-time, data-efficient Q-learning-based algorithm to solve the H$_{\infty}$ control of linear discrete-time systems. The computational complexity is shown to reduce from $\mathcal{O}(\underline{q}^3)$ in the literature to $\mathcal{O}(\underline{q}^2)$ in the proposed algorithm, where $\underline{q}$ is quadratic in the sum of the size of state variables, control inputs, and disturbance. An adaptive optimal controller is designed and the parameters of the action and critic networks are learned online without the knowledge of the system dynamics, making the proposed algorithm completely model-free. Also, a sufficient probing noise is only needed in the first iteration and does not affect the proposed algorithm. With no need for an initial stabilizing policy, the algorithm converges to the closed-form 
    
[^5]: PDFTriage: 对长篇结构化文档进行问答

    PDFTriage: Question Answering over Long, Structured Documents. (arXiv:2309.08872v1 [cs.CL])

    [http://arxiv.org/abs/2309.08872](http://arxiv.org/abs/2309.08872)

    PDFTriage是一种处理长篇结构化文档问答的方法，通过使用结构或内容来检索上下文，解决了大型语言模型在问答中遇到的问题。

    

    大型语言模型在处理长篇文档的问答时存在问题，因为文档无法适应语言模型的上下文长度限制。为了解决这个问题，现有的大多数方法集中于从文档中检索相关的上下文，并将其表示为纯文本。然而，像PDF、网页和演示文稿这样的文档是有结构的，包括不同的页码、表格、章节等。将这样的结构化文档表示为纯文本与用户对这些具有丰富结构的文档的认知模型不符。当系统需要从文档中查询上下文时，这种不符会显现出来，甚至简单的问题也可能使问答系统出错。为了弥合处理结构化文档中的基本差距，我们提出了一种名为PDFTriage的方法，使模型能够根据结构或内容检索上下文。我们的实验证明了所提出的PDFTriage的有效性。

    Large Language Models (LLMs) have issues with document question answering (QA) in situations where the document is unable to fit in the small context length of an LLM. To overcome this issue, most existing works focus on retrieving the relevant context from the document, representing them as plain text. However, documents such as PDFs, web pages, and presentations are naturally structured with different pages, tables, sections, and so on. Representing such structured documents as plain text is incongruous with the user's mental model of these documents with rich structure. When a system has to query the document for context, this incongruity is brought to the fore, and seemingly trivial questions can trip up the QA system. To bridge this fundamental gap in handling structured documents, we propose an approach called PDFTriage that enables models to retrieve the context based on either structure or content. Our experiments demonstrate the effectiveness of the proposed PDFTriage-augmente
    
[^6]: 在大语言模型时代重新思考学习率调整

    Rethinking Learning Rate Tuning in the Era of Large Language Models. (arXiv:2309.08859v1 [cs.LG])

    [http://arxiv.org/abs/2309.08859](http://arxiv.org/abs/2309.08859)

    本文重新评估了在大语言模型时代中学习率调整的挑战和机遇，并提出了LRBench++来帮助研究者评估学习率策略。

    

    大语言模型（LLMs）代表了深度学习在实现了出色的人类预测性能方面的最新成功。鉴于LLM训练的昂贵费用，利用微调来适应各种实际应用已成为主流策略。学习率是LLM微调中最重要的超参数之一，直接影响微调效率和微调后的LLM质量。现有的学习率策略主要针对传统深度神经网络（DNNs）的训练而设计，可能在LLM微调方面效果不佳。我们重新评估了在大语言模型时代中学习率调整的研究挑战和机遇。本文做出了三个原创贡献。首先，我们重新审视现有的学习率策略，分析了在大语言模型时代中学习率调整的关键挑战。其次，我们提出了LRBench++来评估学习率策略并促进学习率调整的研究。

    Large Language Models (LLMs) represent the recent success of deep learning in achieving remarkable human-like predictive performance. It has become a mainstream strategy to leverage fine-tuning to adapt LLMs for various real-world applications due to the prohibitive expenses associated with LLM training. The learning rate is one of the most important hyperparameters in LLM fine-tuning with direct impacts on both fine-tuning efficiency and fine-tuned LLM quality. Existing learning rate policies are primarily designed for training traditional deep neural networks (DNNs), which may not work well for LLM fine-tuning. We reassess the research challenges and opportunities of learning rate tuning in the coming era of Large Language Models. This paper makes three original contributions. First, we revisit existing learning rate policies to analyze the critical challenges of learning rate tuning in the era of LLMs. Second, we present LRBench++ to benchmark learning rate policies and facilitate l
    
[^7]: 通过差分神经计算，智能机器在无结构环境中工作

    Intelligent machines work in unstructured environments by differential neural computing. (arXiv:2309.08835v1 [eess.SP])

    [http://arxiv.org/abs/2309.08835](http://arxiv.org/abs/2309.08835)

    本研究提出了一种基于差分神经计算的智能机器方法，通过提取环境信息的主要特征并应用相应的编码刺激到记忆阻性器件，成功地实现了处理无结构环境信息的类人能力，并展现了良好的可扩展性和泛化性。该方法在物体抓取和自动驾驶等应用方面得到了验证。

    

    希望智能机器能够在现实世界中高效地工作，需要一种新的方法来准确地理解未知环境中的无结构信息，具有良好的准确性、可扩展性和泛化性，就像人类一样。本文介绍了一种基于记忆阻性神经计算的感知信号差分处理和学习方法，通过提取环境信息的主要特征并应用相关编码刺激到记忆阻性器件，我们成功地获得了处理无结构环境信息的类人能力，如机械刺激的放大（>720%）和适应（<50%）。该方法还展现了良好的可扩展性和泛化性，在智能机器的两个典型应用中得到了验证：物体抓取和自动驾驶。在物体抓取方面，通过在1毫秒内使用单个记忆阻性器件学习未知物体特征（例如尖锐的角和光滑的表面），一个机器手实现了安全稳定的抓取。

    Expecting intelligent machines to efficiently work in real world requires a new method to understand unstructured information in unknown environments with good accuracy, scalability and generalization, like human. Here, a memristive neural computing based perceptual signal differential processing and learning method for intelligent machines is presented, via extracting main features of environmental information and applying associated encoded stimuli to memristors, we successfully obtain human-like ability in processing unstructured environmental information, such as amplification (>720%) and adaptation (<50%) of mechanical stimuli. The method also exhibits good scalability and generalization, validated in two typical applications of intelligent machines: object grasping and autonomous driving. In the former, a robot hand experimentally realizes safe and stable grasping, through learning unknown object features (e.g., sharp corner and smooth surface) with a single memristor in 1 ms. In
    
[^8]: 先验偏移下的分布鲁棒事后分类器

    Distributionally Robust Post-hoc Classifiers under Prior Shifts. (arXiv:2309.08825v1 [cs.LG])

    [http://arxiv.org/abs/2309.08825](http://arxiv.org/abs/2309.08825)

    研究了先验偏移下的分布鲁棒事后分类器的训练问题，通过在预训练模型的预测上进行缩放调整，以最小化目标分布周围的分布鲁棒损失。

    

    当测试分布偏离训练分布时，机器学习模型的泛化能力显著降低。我们研究了训练模型以应对由类先验或组先验分布变化引起的偏移的问题。存在偏斜的训练先验往往会导致模型对噪声特征过拟合。与现有方法不同，现有方法优化最差或平均性能，而我们的工作是出于对模型鲁棒性质更细粒度控制的需求。我们提出了一种极其轻量级的事后方法，通过对预训练模型的预测进行缩放调整，旨在最小化选择的目标分布周围的分布鲁棒损失。这些调整通过在验证集上求解约束优化问题来计算，并在测试时间应用于模型。

    The generalization ability of machine learning models degrades significantly when the test distribution shifts away from the training distribution. We investigate the problem of training models that are robust to shifts caused by changes in the distribution of class-priors or group-priors. The presence of skewed training priors can often lead to the models overfitting to spurious features. Unlike existing methods, which optimize for either the worst or the average performance over classes or groups, our work is motivated by the need for finer control over the robustness properties of the model. We present an extremely lightweight post-hoc approach that performs scaling adjustments to predictions from a pre-trained model, with the goal of minimizing a distributionally robust loss around a chosen target distribution. These adjustments are computed by solving a constrained optimization problem on a validation set and applied to the model during test time. Our constrained optimization obje
    
[^9]: SHAPNN: Shapley Value正则化的表格型神经网络

    SHAPNN: Shapley Value Regularized Tabular Neural Network. (arXiv:2309.08799v1 [cs.LG])

    [http://arxiv.org/abs/2309.08799](http://arxiv.org/abs/2309.08799)

    SHAPNN是一种使用Shapley值正则化的表格型神经网络，能够提供有效的解释并改善模型的性能和连续学习能力。

    

    我们提出了SHAPNN，一种用于监督学习的新型深度表格数据建模架构。我们的方法利用了Shapley值，这是一种用于解释黑盒模型的成熟技术。我们的神经网络使用标准的反向传播优化方法进行训练，并使用实时估计的Shapley值进行正则化。我们的方法具有多个优势，包括能够对数据实例和数据集提供有效的解释而不增加计算开销。此外，带有解释的预测作为一种正则化器，改善了模型的性能。此外，正则化的预测增强了模型的连续学习能力。我们在各种公开可用的数据集上评估了我们的方法，并将其与最先进的深度神经网络模型进行了比较，展示了SHAPNN在AUROC、透明性以及对流数据的鲁棒性方面的卓越性能。

    We present SHAPNN, a novel deep tabular data modeling architecture designed for supervised learning. Our approach leverages Shapley values, a well-established technique for explaining black-box models. Our neural network is trained using standard backward propagation optimization methods, and is regularized with realtime estimated Shapley values. Our method offers several advantages, including the ability to provide valid explanations with no computational overhead for data instances and datasets. Additionally, prediction with explanation serves as a regularizer, which improves the model's performance. Moreover, the regularized prediction enhances the model's capability for continual learning. We evaluate our method on various publicly available datasets and compare it with state-of-the-art deep neural network models, demonstrating the superior performance of SHAPNN in terms of AUROC, transparency, as well as robustness to streaming data.
    
[^10]: Fin-Fact:一种面向多模态金融事实核查和解释生成的基准数据集

    Fin-Fact: A Benchmark Dataset for Multimodal Financial Fact Checking and Explanation Generation. (arXiv:2309.08793v1 [cs.AI])

    [http://arxiv.org/abs/2309.08793](http://arxiv.org/abs/2309.08793)

    Fin-Fact是一个用于多模态金融事实核查和解释生成的基准数据集，通过提供专业的注释和证据，以及多模态信息源来增强事实性分析，从而打击金融领域的错误信息，促进透明度，并建立信任。

    

    金融领域的事实核查尚未充分探索，该领域缺乏高质量的数据集。本文提出了Fin-Fact，一种用于金融领域多模态事实核查的基准数据集。值得注意的是，它包括专业事实核查人员的注释和证据，提供专业知识和可信度。由于其多模态性质涵盖了文本和视觉内容，Fin-Fact提供了补充信息源，以增强事实性分析。其主要目标是在金融领域打击错误信息，促进透明度，并在财务报告和新闻传播中建立信任。通过提供有深度的解释，Fin-Fact使用户，包括领域专家和终端用户，能够理解事实核查决策的推理过程，验证声明的可信度，并促进对事实核查流程的信任。Fin-Fact数据集以及我们的实验代码可在https://github.com/IIT-DM/Fin-Fact/ 上找到。

    Fact-checking in financial domain is under explored, and there is a shortage of quality dataset in this domain. In this paper, we propose Fin-Fact, a benchmark dataset for multimodal fact-checking within the financial domain. Notably, it includes professional fact-checker annotations and justifications, providing expertise and credibility. With its multimodal nature encompassing both textual and visual content, Fin-Fact provides complementary information sources to enhance factuality analysis. Its primary objective is combating misinformation in finance, fostering transparency, and building trust in financial reporting and news dissemination. By offering insightful explanations, Fin-Fact empowers users, including domain experts and end-users, to understand the reasoning behind fact-checking decisions, validating claim credibility, and fostering trust in the fact-checking process. The Fin-Fact dataset, along with our experimental codes is available at https://github.com/IIT-DM/Fin-Fact/
    
[^11]: BioinspiredLLM: 生物和生物受启发材料力学的对话型大型语言模型

    BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials. (arXiv:2309.08788v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2309.08788](http://arxiv.org/abs/2309.08788)

    一个名为BioinspiredLLM的开源语言模型利用大量的文献进行微调，能够主动交互地回忆和评估生物材料的信息，提出新的问题和回答，并为生物材料设计提供合理的假设。

    

    生物材料和生物受启发材料科学的研究已经得到了很好的发展；然而，令人惊讶的是，很少有系统地将这些知识转化为工程解决方案。为了加快发现并引导洞察，报道了一个开源的自回归转换器大型语言模型BioinspiredLLM。该模型使用了一千多篇经过同行评审的结构生物学和生物受启发材料领域的文章进行了微调，可以被提示主动和交互地回忆信息，协助研究任务，并作为创造力的引擎。该模型通过示例证明了它不仅能够在查询时准确回忆有关生物材料的信息，还能够提出生物材料问题和答案来评估自己的性能。BioinspiredLLM还被证明能够对生物材料设计提出合理的假设，尤其是对于那些从未明确研究过的材料。

    The study of biological materials and bio-inspired materials science is well established; however, surprisingly little knowledge has been systematically translated to engineering solutions. To accelerate discovery and guide insights, an open-source autoregressive transformer large language model, BioinspiredLLM, is reported. The model was finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials and can be prompted to actively and interactively recall information, assist with research tasks, and function as an engine for creativity. The model has proven by example that it is not only able to accurately recall information about biological materials when queried but also formulate biomaterials questions and answers that can evaluate its own performance. BioinspiredLLM also has been shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly 
    
[^12]: 超越标签: 利用深度学习和LLMs进行内容元数据分析

    Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata. (arXiv:2309.08787v1 [cs.IR])

    [http://arxiv.org/abs/2309.08787](http://arxiv.org/abs/2309.08787)

    本研究讨论了在电影推荐系统中利用类型标签进行内容元数据分析的挑战，并提出了一种名为“类型频谱”的新方法来捕捉标题中的微妙类型。实验证实了该方法的有效性。此外，该研究还讨论了LLMs在增强内容元数据分析中的应用。

    

    内容元数据在电影推荐系统中扮演着非常重要的角色，它提供了有关电影的各个方面（如类型、演员、剧情概要、票房摘要等）的有价值信息。分析元数据可以帮助理解用户偏好，生成个性化推荐和解决项目冷启动问题。本文将重点关注一种特定类型的元数据——“类型”标签。电影或电视剧的类型标签有助于将一系列标题分类为不同主题，并相应地设置观众期望。我们提出了使用类型标签信息所涉及的一些挑战，并提出了一种称为“类型频谱”的新方法来研究类型信息。类型频谱有助于捕捉标题中的各种微妙类型，我们的离线和在线实验证实了该方法的有效性。此外，我们还讨论了LLMs在增强内容元数据分析方面的应用。

    Content metadata plays a very important role in movie recommender systems as it provides valuable information about various aspects of a movie such as genre, cast, plot synopsis, box office summary, etc. Analyzing the metadata can help understand the user preferences to generate personalized recommendations and item cold starting. In this talk, we will focus on one particular type of metadata - \textit{genre} labels. Genre labels associated with a movie or a TV series help categorize a collection of titles into different themes and correspondingly setting up the audience expectation. We present some of the challenges associated with using genre label information and propose a new way of examining the genre information that we call as the \textit{Genre Spectrum}. The Genre Spectrum helps capture the various nuanced genres in a title and our offline and online experiments corroborate the effectiveness of the approach. Furthermore, we also talk about applications of LLMs in augmenting con
    
[^13]: 多任务强化学习中的Projected Task-Specific Layers

    Projected Task-Specific Layers for Multi-Task Reinforcement Learning. (arXiv:2309.08776v1 [cs.LG])

    [http://arxiv.org/abs/2309.08776](http://arxiv.org/abs/2309.08776)

    本研究提出了一种新的架构，Projected Task-Specific Layers (PTSL)，通过任务特定的层来表达共享和可变的任务信息，成功解决了多任务强化学习中的推广和干扰问题。

    

    多任务强化学习可以使机器人在家庭和工作场所的各种操作任务中实现规模化。然而，从一个任务推广到另一个任务并减轻负面任务干扰仍然是一个挑战。成功地在任务之间共享信息并取得良好效果将取决于对任务底层结构的有效捕捉。在这项工作中，我们介绍了一种新的架构，即Projected Task-Specific Layers（PTSL），它通过任务特定的层，通过稠密的任务特定的修正来更好地表达共享和可变的任务信息。然后，我们展示了我们的模型在Meta-World的MT10和MT50基准中（包括Sawyer机器人臂上的10个和50个目标条件任务）的表现优于现有技术水平。

    Multi-task reinforcement learning could enable robots to scale across a wide variety of manipulation tasks in homes and workplaces. However, generalizing from one task to another and mitigating negative task interference still remains a challenge. Addressing this challenge by successfully sharing information across tasks will depend on how well the structure underlying the tasks is captured. In this work, we introduce our new architecture, Projected Task-Specific Layers (PTSL), that leverages a common policy with dense task-specific corrections through task-specific layers to better express shared and variable task information. We then show that our model outperforms the state of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10 and 50 goal-conditioned tasks for a Sawyer arm.
    
[^14]: 增强音频生成可控性的方法：通过表示相似性正则化

    Enhance audio generation controllability through representation similarity regularization. (arXiv:2309.08773v1 [cs.SD])

    [http://arxiv.org/abs/2309.08773](http://arxiv.org/abs/2309.08773)

    这篇论文介绍了一种创新的方法，通过在训练过程中强调音频和文本表示之间的对齐来增强音频生成的可控性。实验结果表明，在音乐和音频生成任务中，这种方法取得了良好的效果。

    

    本文提出了一种创新的方法，通过在模型训练期间强调音频和文本表示之间的对齐，增强对音频生成的控制能力。在基于语言模型的音频生成中，模型利用来自文本和音频标记表示的输入来预测后续的音频标记。然而，当前的配置缺乏明确的正则化来确保所选择的文本表示与语言模型的预测之间的对齐。我们的提议涉及音频和文本表示正则化的整合，特别是在无分类器引导（CFG）阶段，在语言模型训练过程中，文本条件被排除在跨注意力之外。我们提出的表示正则化的目的是最小化与同一训练批次中其他样本的音频和文本相似性差异。在音乐和音频生成任务上的实验结果表明，

    This paper presents an innovative approach to enhance control over audio generation by emphasizing the alignment between audio and text representations during model training. In the context of language model-based audio generation, the model leverages input from both textual and audio token representations to predict subsequent audio tokens. However, the current configuration lacks explicit regularization to ensure the alignment between the chosen text representation and the language model's predictions. Our proposal involves the incorporation of audio and text representation regularization, particularly during the classifier-free guidance (CFG) phase, where the text condition is excluded from cross attention during language model training. The aim of this proposed representation regularization is to minimize discrepancies in audio and text similarity compared to other samples within the same training batch. Experimental results on both music and audio generation tasks demonstrate that
    
[^15]: 使用大型语言模型挖掘专利展示了功能标签与化学结构的一致性

    Mining Patents with Large Language Models Demonstrates Congruence of Functional Labels and Chemical Structures. (arXiv:2309.08765v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.08765](http://arxiv.org/abs/2309.08765)

    通过使用大型语言模型挖掘化学专利，我们得到了一个包含10万个分子及其功能标签的化学功能（CheF）数据集，这些功能标签经过验证是高质量的。

    

    从结构预测化学功能是化学科学的一个主要目标，从发现和重用新型药物到创造新材料。最近，新的机器学习算法开辟了涵盖许多不同化学功能的通用预测模型的可能性。在这里，我们考虑将大型语言模型应用于化学专利的挑战，以整合和利用这些资源所捕捉的关于化学功能的信息。化学专利包含大量关于化学功能的知识，但由于提取高质量功能标签的不可行性，它们作为数据集的有用性历来被忽视。使用可扩展的ChatGPT辅助专利摘要和词嵌入标签清理流程，我们得到了一个化学功能（CheF）数据集，包含10万个分子及其专利衍生的功能标签。这些功能标签经过验证是高质量的，使我们能够

    Predicting chemical function from structure is a major goal of the chemical sciences, from the discovery and repurposing of novel drugs to the creation of new materials. Recently, new machine learning algorithms are opening up the possibility of general predictive models spanning many different chemical functions. Here, we consider the challenge of applying large language models to chemical patents in order to consolidate and leverage the information about chemical functionality captured by these resources. Chemical patents contain vast knowledge on chemical function, but their usefulness as a dataset has historically been neglected due to the impracticality of extracting high-quality functional labels. Using a scalable ChatGPT-assisted patent summarization and word-embedding label cleaning pipeline, we derive a Chemical Function (CheF) dataset, containing 100K molecules and their patent-derived functional labels. The functional labels were validated to be of high quality, allowing us 
    
[^16]: 使用极坐标重建的循环聚类

    Circular Clustering with Polar Coordinate Reconstruction. (arXiv:2309.08757v1 [cs.LG])

    [http://arxiv.org/abs/2309.08757](http://arxiv.org/abs/2309.08757)

    本研究提出了一种新的循环数据聚类分析框架，通过使用极坐标重建和数学属性，能够在重构数据集中准确找出聚类结果。

    

    对于在生物系统中发现的循环数据，越来越多的人对其进行特征化感兴趣。这些数据范围广泛，多种多样，从神经记录中的信号相位到圆形基因组中的核苷酸序列。传统的聚类算法往往由于其对周期性组成部分的差异有限，而显得不够适用。目前在极坐标系统中工作的聚类方案存在一些限制，比如仅专注于角度或缺乏通用性。为了克服这些限制，我们提出了一种新的分析框架，利用投影到柱状坐标系统来更好地表示极坐标系统中的对象。利用循环数据的数学属性，我们展示了我们的方法在重建的数据集中，只要数据具有足够的周期重复，总能找到正确的聚类结果。我们的方法具有普适性和可调性，并可以融入大多数最先进的聚类算法中。

    There is a growing interest in characterizing circular data found in biological systems. Such data are wide ranging and varied, from signal phase in neural recordings to nucleotide sequences in round genomes. Traditional clustering algorithms are often inadequate due to their limited ability to distinguish differences in the periodic component. Current clustering schemes that work in a polar coordinate system have limitations, such as being only angle-focused or lacking generality. To overcome these limitations, we propose a new analysis framework that utilizes projections onto a cylindrical coordinate system to better represent objects in a polar coordinate system. Using the mathematical properties of circular data, we show our approach always finds the correct clustering result within the reconstructed dataset, given sufficient periodic repetitions of the data. Our approach is generally applicable and adaptable and can be incorporated into most state-of-the-art clustering algorithms.
    
[^17]: 多样的神经音频嵌入 - 恢复特征！

    Diverse Neural Audio Embeddings -- Bringing Features back !. (arXiv:2309.08751v1 [cs.SD])

    [http://arxiv.org/abs/2309.08751](http://arxiv.org/abs/2309.08751)

    本文通过在音频分类任务中学习多样化的特征表示，包括领域特定的音高、音色和神经表示，以及端到端架构，为学习稳健、多样化的表示铺平了道路，并显著提高了性能。

    

    随着现代人工智能架构的出现，从端到端的架构开始流行。这种转变导致了神经架构在没有领域特定偏见/知识的情况下进行训练，根据任务进行优化。本文中，我们通过多样的特征表示（在本例中是领域特定的）学习音频嵌入。对于涉及数百种声音分类的情况，我们学习分别针对音高、音色和神经表示等多样的音频属性建立稳健的嵌入，同时也通过端到端架构进行学习。我们观察到手工制作的嵌入，例如基于音高和音色的嵌入，虽然单独使用时无法击败完全端到端的表示，但将这些嵌入与端到端嵌入相结合可以显著提高性能。这项工作将为在端到端模型中引入一些领域专业知识来学习稳健、多样化的表示铺平道路，并超越仅训练端到端模型的性能。

    With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training 
    
[^18]: Wasserstein分布保证的策略评估和学习在上下文乐队中

    Wasserstein Distributionally Robust Policy Evaluation and Learning for Contextual Bandits. (arXiv:2309.08748v1 [cs.LG])

    [http://arxiv.org/abs/2309.08748](http://arxiv.org/abs/2309.08748)

    通过使用Wasserstein距离而不是KL散度，我们提出了一种新颖的分布保证优化方法，用于解决上下文乐队中实际环境不匹配和最坏情况下过度拟合的问题。

    

    在没有与环境直接互动的情况下，数据收集的环境通常与学习的策略应用的环境不同。为了在学习和执行过程中考虑不同环境的影响，我们提出了一种使用Wasserstein距离的新型分布保证优化(DRO)方法，该方法在假设新环境的分布位于不确定集合内时，计算策略值的最坏情况下界。典型地，这个不确定集合是基于从日志数据集中计算的经验分布的KL散度定义的。然而，KL不确定集合无法包含具有不同支持的分布，也缺乏对分布支持的几何感知。结果，KL方法在解决实际环境不匹配和导致过度拟合最坏情况方面存在不足。为了克服这些限制，我们提出了一种使用Wasserstein距离的新型DRO方法。

    Without direct interaction with the environment. Often, the environment in which the data are collected differs from the environment in which the learned policy is applied. To account for the effect of different environments during learning and execution, distributionally robust optimization (DRO) methods have been developed that compute worst-case bounds on the policy values assuming that the distribution of the new environment lies within an uncertainty set. Typically, this uncertainty set is defined based on the KL divergence around the empirical distribution computed from the logging dataset. However, the KL uncertainty set fails to encompass distributions with varying support and lacks awareness of the geometry of the distribution support. As a result, KL approaches fall short in addressing practical environment mismatches and lead to over-fitting to worst-case scenarios. To overcome these limitations, we propose a novel DRO approach that employs the Wasserstein distance instead. 
    
[^19]: AlbNER：一个用于阿尔巴尼亚命名实体识别的语料库

    AlbNER: A Corpus for Named Entity Recognition in Albanian. (arXiv:2309.08741v1 [cs.CL])

    [http://arxiv.org/abs/2309.08741](http://arxiv.org/abs/2309.08741)

    本文介绍了一个用于阿尔巴尼亚命名实体识别的语料库AlbNER，该语料库由阿尔巴尼亚维基百科文章中收集的900个带有标记命名实体的句子组成。初步结果表明，模型大小对NER性能影响小，而语言迁移有着显著的影响。这些资源和结果为未来实验提供了基线。

    

    针对阿尔巴尼亚等资源匮乏的语言，如计算语言学和自然语言处理研究中存在着标注文本语料库的严重障碍。本文介绍了AlbNER，这是一个由阿尔巴尼亚维基百科文章中收集的900个带有标记命名实体的句子的语料库。用使用AlbNER数据进行细调和测试的BERT和RoBERTa变体的初步结果表明，模型大小对NER性能的影响很小，而语言迁移的影响很大。AlbNER语料库和这些结果应作为未来实验的基线。

    Scarcity of resources such as annotated text corpora for under-resourced languages like Albanian is a serious impediment in computational linguistics and natural language processing research. This paper presents AlbNER, a corpus of 900 sentences with labeled named entities, collected from Albanian Wikipedia articles. Preliminary results with BERT and RoBERTa variants fine-tuned and tested with AlbNER data indicate that model size has slight impact on NER performance, whereas language transfer has a significant one. AlbNER corpus and these obtained results should serve as baselines for future experiments.
    
[^20]: 植物疾病分类的概念可解释性

    Concept explainability for plant diseases classification. (arXiv:2309.08739v1 [cs.CV])

    [http://arxiv.org/abs/2309.08739](http://arxiv.org/abs/2309.08739)

    本研究提出了一种名为Testing with Concept Activation Vectors (TCAV)的方法，用于解决基于深度学习的植物疾病分类中的可解释性问题。

    

    植物疾病对食品安全和农业可持续性仍然是一个重大威胁。快速和早期识别这些疾病已成为一个重要关注点，这推动了几项研究依赖于全球数字化的增长和基于深度学习的计算机视觉的最新进展。事实上，基于深度卷积神经网络的植物疾病分类已经展现出了令人印象深刻的性能。然而，由于对它们的鲁棒性、透明性以及与人工专家对照相比缺乏解释性的担忧，这些方法尚未全球采用。已经提出了一些基于显著性的方法，将网络输出与输入像素的扰动相关联，以提供对这些算法的洞察。然而，它们对人类用户来说并不容易理解和直观，并且受到偏见的威胁。在这项工作中，我们使用了一种名为Testing with Concept Activation Vectors (TCAV)的方法，来改变这种情况。

    Plant diseases remain a considerable threat to food security and agricultural sustainability. Rapid and early identification of these diseases has become a significant concern motivating several studies to rely on the increasing global digitalization and the recent advances in computer vision based on deep learning. In fact, plant disease classification based on deep convolutional neural networks has shown impressive performance. However, these methods have yet to be adopted globally due to concerns regarding their robustness, transparency, and the lack of explainability compared with their human experts counterparts. Methods such as saliency-based approaches associating the network output to perturbations of the input pixels have been proposed to give insights into these algorithms. Still, they are not easily comprehensible and not intuitive for human users and are threatened by bias. In this work, we deploy a method called Testing with Concept Activation Vectors (TCAV) that shifts th
    
[^21]: 结合深度学习和分散式无线电感知的前向碰撞警告系统的实验评估

    Experimental Assessment of a Forward-Collision Warning System Fusing Deep Learning and Decentralized Radio Sensing. (arXiv:2309.08737v1 [cs.LG])

    [http://arxiv.org/abs/2309.08737](http://arxiv.org/abs/2309.08737)

    该论文提出了一种基于分散式无线电感知方法的前向碰撞警告系统，利用深度学习模块分析波形信号上的多普勒特征，实现对前方来车的检测。实验评估表明该系统在高速公路上的检测性能良好。

    

    本文提出了一种基于分散式无线电感知方法的自动前向碰撞警告系统的思想。在这个框架中，接收模式下的一辆车使用第二辆车发射的连续波形作为探测信号，以检测前方来车并提醒驾驶员可能会发生碰撞。这种连续波形可以容易地作为导频信号嵌入到当前多载波车载通信系统的数据帧中。通过深度学习模块分析快速接近的车辆所印刻在连续波形探测信号上的多普勒特征来实现对前方来车的检测。通过在一条双车道高速公路上进行的一系列现场试验收集的数据，对这种分散式连续波形无线电感知方法进行了实验评估。使用了两种不同的深度学习模型进行了检测性能评估：长短期记忆网络和卷积神经网络。实验结果表明

    This paper presents the idea of an automatic forward-collision warning system based on a decentralized radio sensing (RS) approach. In this framework, a vehicle in receiving mode employs a continuous waveform (CW) transmitted by a second vehicle as a probe signal to detect oncoming vehicles and warn the driver of a potential forward collision. Such a CW can easily be incorporated as a pilot signal within the data frame of current multicarrier vehicular communication systems. Detection of oncoming vehicles is performed by a deep learning (DL) module that analyzes the features of the Doppler signature imprinted on the CW probe signal by a rapidly approaching vehicle. This decentralized CW RS approach was assessed experimentally using data collected by a series of field trials conducted in a two-lanes high-speed highway. Detection performance was evaluated for two different DL models: a long short-term memory network and a convolutional neural network. The obtained results demonstrate the
    
[^22]: 指引的方法：利用学习到的ICP权重改进雷达-激光雷达定位

    Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights. (arXiv:2309.08731v1 [cs.RO])

    [http://arxiv.org/abs/2309.08731](http://arxiv.org/abs/2309.08731)

    本文提出了一种深度学习方法，通过学习到的ICP权重优化雷达-激光雷达的定位，从而改善了雷达测量对激光雷达地图的定位效果。这一方法在保持高质量地图定位性能的同时，提高了在降水和大雾等恶劣天气条件下的定位准确性。

    

    本文提出了一种基于深度学习的新方法，用于改进雷达测量对激光雷达地图的定位。虽然目前定位的技术水平是将激光雷达数据与激光雷达地图进行匹配，但是雷达被认为是一种有前途的替代方法，因为它对降水和大雾等恶劣天气具有更强的韧性。为了利用现有的高质量激光雷达地图，同时在恶劣天气下保持性能，将雷达数据与激光雷达地图进行匹配具有重要意义。然而，由于雷达测量中存在的独特伪影，雷达-激光雷达定位一直难以达到与激光雷达-激光雷达系统相媲美的性能，使其无法用于自动驾驶。本工作在基于ICP的雷达-激光雷达定位系统基础上，包括一个学习的预处理步骤，根据高层次的扫描信息对雷达点进行加权。将经过验证的分析方法与学习到的权重相结合，减小了雷达定位中的误差。

    This paper presents a novel deep-learning-based approach to improve localizing radar measurements against lidar maps. Although the state of the art for localization is matching lidar data to lidar maps, radar has been considered as a promising alternative, as it is potentially more resilient against adverse weather such as precipitation and heavy fog. To make use of existing high-quality lidar maps, while maintaining performance in adverse weather, matching radar data to lidar maps is of interest. However, owing in part to the unique artefacts present in radar measurements, radar-lidar localization has struggled to achieve comparable performance to lidar-lidar systems, preventing it from being viable for autonomous driving. This work builds on an ICP-based radar-lidar localization system by including a learned preprocessing step that weights radar points based on high-level scan information. Combining a proven analytical approach with a learned weight reduces localization errors in rad
    
[^23]: 集群化的多智能体线性赌博机

    Clustered Multi-Agent Linear Bandits. (arXiv:2309.08710v1 [cs.LG])

    [http://arxiv.org/abs/2309.08710](http://arxiv.org/abs/2309.08710)

    本文研究了集群化的多智能体线性赌博机问题，提出了一种新颖的算法，通过智能体之间的协作来加速优化问题。通过理论分析和实证评估，证明了算法在遗憾最小化和聚类质量上的有效性。

    

    本文针对多智能体线性随机赌博问题的一个特定实例，即集群化的多智能体线性赌博机进行了研究。在这个设置中，我们提出了一种新颖的算法，通过智能体之间的有效协作来加速整体优化问题。在这一贡献中，网络控制器负责估计网络的基本集群结构并优化同一组中智能体之间的经验分享。我们对遗憾最小化问题和聚类质量进行了理论分析。通过对合成数据和真实数据进行与最先进算法的实证评估，我们证明了我们方法的有效性：我们的算法显著改善了遗憾最小化，并成功恢复了真实的基本集群划分。

    We address in this paper a particular instance of the multi-agent linear stochastic bandit problem, called clustered multi-agent linear bandits. In this setting, we propose a novel algorithm leveraging an efficient collaboration between the agents in order to accelerate the overall optimization problem. In this contribution, a network controller is responsible for estimating the underlying cluster structure of the network and optimizing the experiences sharing among agents within the same groups. We provide a theoretical analysis for both the regret minimization problem and the clustering quality. Through empirical evaluation against state-of-the-art algorithms on both synthetic and real data, we demonstrate the effectiveness of our approach: our algorithm significantly improves regret minimization while managing to recover the true underlying cluster partitioning.
    
[^24]: 线性最优臂识别中的安全代价

    Price of Safety in Linear Best Arm Identification. (arXiv:2309.08709v1 [stat.ML])

    [http://arxiv.org/abs/2309.08709](http://arxiv.org/abs/2309.08709)

    该论文提出了一种具有线性反馈的安全最优臂识别框架，该框架通过利用线性结构来保证在每一轮中不违反阶段性安全约束，提出了一种基于间隙的算法来实现有意义的样本复杂性，并通过实验证明了算法的有效性。

    

    我们引入了具有线性反馈的安全最优臂识别框架，其中代理受到一些阶段性安全约束的限制，该限制线性地依赖于未知的参数向量。代理必须以保守的方式采取行动，以确保在每一轮中不会高概率违反安全约束。已经研究了利用线性结构来确保安全性的方法，但据我们所知，还没有研究在最优臂识别中应用该方法。我们提出了一种基于间隙的算法，可以在确保阶段性安全性的同时实现有意义的样本复杂性。我们证明由于额外的安全约束导致的强制探索阶段，我们在样本复杂性上付出了额外的代价。我们提供了实验说明，以验证我们算法的设计。

    We introduce the safe best-arm identification framework with linear feedback, where the agent is subject to some stage-wise safety constraint that linearly depends on an unknown parameter vector. The agent must take actions in a conservative way so as to ensure that the safety constraint is not violated with high probability at each round. Ways of leveraging the linear structure for ensuring safety has been studied for regret minimization, but not for best-arm identification to the best our knowledge. We propose a gap-based algorithm that achieves meaningful sample complexity while ensuring the stage-wise safety. We show that we pay an extra term in the sample complexity due to the forced exploration phase incurred by the additional safety constraint. Experimental illustrations are provided to justify the design of our algorithm.
    
[^25]: 使用具有可微凸规划的条件风险的Wasserstein分布鲁棒控制屏障函数

    Wasserstein Distributionally Robust Control Barrier Function using Conditional Value-at-Risk with Differentiable Convex Programming. (arXiv:2309.08700v1 [cs.RO])

    [http://arxiv.org/abs/2309.08700](http://arxiv.org/abs/2309.08700)

    本文提出了一种使用Wasserstein分布鲁棒控制屏障函数的方法，以实现在分布偏移下的韧性，并保持控制屏障函数的优势，通过估计条件风险来衡量分布偏移下的安全约束。

    

    控制屏障函数（CBFs）在设计安全控制器中吸引了广泛的关注，以在实际的安全关键系统中部署。然而，周围环境的感知通常受到随机性的影响，并可能进一步偏离名义值。在本文中，我们提出了分布鲁棒CBF（DR-CBF），以实现在分布偏移下的韧性，并保持CBF的优势，如计算效率和正向不变性。为了实现这个目标，我们首先提出了一个单层凸重组，用于估计分布偏移下用Wasserstein度量衡量的安全约束的条件风险（CVaR），这本质上是三层规划。另外，为了构建控制屏障条件以强制实现CVaR的正向不变性，我们应用了可微凸规划技术，以实现CVaR估计的优化层内的微分。

    Control Barrier functions (CBFs) have attracted extensive attention for designing safe controllers for their deployment in real-world safety-critical systems. However, the perception of the surrounding environment is often subject to stochasticity and further distributional shift from the nominal one. In this paper, we present distributional robust CBF (DR-CBF) to achieve resilience under distributional shift while keeping the advantages of CBF, such as computational efficacy and forward invariance.  To achieve this goal, we first propose a single-level convex reformulation to estimate the conditional value at risk (CVaR) of the safety constraints under distributional shift measured by a Wasserstein metric, which is by nature tri-level programming. Moreover, to construct a control barrier condition to enforce the forward invariance of the CVaR, the technique of differentiable convex programming is applied to enable differentiation through the optimization layer of CVaR estimation. We a
    
[^26]: 无需插值的建模不规则采样时间序列

    Modelling Irregularly Sampled Time Series Without Imputation. (arXiv:2309.08698v1 [cs.AI])

    [http://arxiv.org/abs/2309.08698](http://arxiv.org/abs/2309.08698)

    SLAN是一种无需插值的方法，可以建模不规则采样时间序列，利用动态适应的LSTM架构来捕捉每个传感器的局部摘要，并在整个观测期间维持一个全局摘要状态。

    

    模拟不规则采样时间序列（ISTS）是具有挑战性的，因为存在缺失值。大多数现有方法通过将不规则采样数据转换为规则采样数据来处理ISTS，但这些模型假设存在潜在的缺失机制，导致了不希望的偏差和次优性能。我们提出了SLAN（Switch LSTM Aggregate Network），该方法利用一组LSTM对ISTS进行建模，而无需插值，消除了任何潜在过程的假设。它根据测量传感器动态自适应地调整其架构。SLAN利用不规则性信息明确捕捉每个传感器的局部摘要，并在整个观测期间维持一个全局摘要状态。我们在公开可用的数据集上展示了SLAN的有效性，包括MIMIC-III、Physionet 2012和Physionet 2019。代码可在https://github.com/Rohit102497/SLAN找到。

    Modelling irregularly-sampled time series (ISTS) is challenging because of missing values. Most existing methods focus on handling ISTS by converting irregularly sampled data into regularly sampled data via imputation. These models assume an underlying missing mechanism leading to unwanted bias and sub-optimal performance. We present SLAN (Switch LSTM Aggregate Network), which utilizes a pack of LSTMs to model ISTS without imputation, eliminating the assumption of any underlying process. It dynamically adapts its architecture on the fly based on the measured sensors. SLAN exploits the irregularity information to capture each sensor's local summary explicitly and maintains a global summary state throughout the observational period. We demonstrate the efficacy of SLAN on publicly available datasets, namely, MIMIC-III, Physionet 2012 and Physionet 2019. The code is available at https://github.com/Rohit102497/SLAN.
    
[^27]: 解决法律术语：法律文件中否定范围解析的多语言探索

    Resolving Legalese: A Multilingual Exploration of Negation Scope Resolution in Legal Documents. (arXiv:2309.08695v1 [cs.CL])

    [http://arxiv.org/abs/2309.08695](http://arxiv.org/abs/2309.08695)

    本研究通过多语言探索，解决了法律文件中否定范围解析的挑战。实验结果表明，以往模型在处理多语言法律数据时表现不佳，因此我们发布了一套新的法庭判决标注数据用于改进解析效果，并取得了高达86.7％的标记级F1分。

    

    在句子中解析否定的范围是一项具有挑战性的自然语言处理任务。法律文本的复杂性以及缺乏经过注释的领域内否定语料库给最先进的模型在处理多语言法律数据上的否定范围解析时带来了挑战。我们的实验表明，预先未使用法律数据进行训练的模型在否定范围解析任务中表现不佳。我们的实验使用仅在文学文本和医学数据等领域进行了精细调整的语言模型，与之前的跨领域实验中记录的结果相比，效果较差。我们发布了一套德语、法语和意大利语的标注法院判决，并将其用于改进零摄取和多语言环境下的否定范围解析。在我们的零摄取跨语言实验中，我们的标记级F1分达到了86.7％，其中模型在我们的法律数据集的两种语言上进行训练，并在第三种语言上进行评估。

    Resolving the scope of a negation within a sentence is a challenging NLP task. The complexity of legal texts and the lack of annotated in-domain negation corpora pose challenges for state-of-the-art (SotA) models when performing negation scope resolution on multilingual legal data. Our experiments demonstrate that models pre-trained without legal data underperform in the task of negation scope resolution. Our experiments, using language models exclusively fine-tuned on domains like literary texts and medical data, yield inferior results compared to the outcomes documented in prior cross-domain experiments. We release a new set of annotated court decisions in German, French, and Italian and use it to improve negation scope resolution in both zero-shot and multilingual settings. We achieve token-level F1-scores of up to 86.7% in our zero-shot cross-lingual experiments, where the models are trained on two languages of our legal datasets and evaluated on the third. Our multilingual experim
    
[^28]: 通过影响函数评估本地差分隐私对效用损失的影响

    Evaluating the Impact of Local Differential Privacy on Utility Loss via Influence Functions. (arXiv:2309.08678v1 [cs.LG])

    [http://arxiv.org/abs/2309.08678](http://arxiv.org/abs/2309.08678)

    本论文通过影响函数评估了本地差分隐私对效用损失的影响，并提出了一种方法，可以帮助数据管理员选择最适合其隐私-效用权衡的隐私参数值，而无需进行大量计算。

    

    如何正确设置差分隐私中的隐私参数是自从2006年首次提出差分隐私以来一直存在的一个问题。在这项工作中，我们展示了影响函数的能力，可以揭示在基于随机响应的本地差分隐私设置下，特定隐私参数值将如何影响模型的测试损失。我们提出的方法允许数据管理员选择与其允许的隐私-效用权衡最符合的隐私参数值，而无需进行繁重的计算，如大量模型重训练和数据私有化。我们考虑了多种常见的随机化场景，例如对特征进行随机响应，对标签进行随机响应，以及通过应用基于类别的标签噪声校正方法来抵消随机化造成的噪声的更复杂的情况。此外，我们还详细讨论了我们提出的方法的计算复杂度，包括经验分析。

    How to properly set the privacy parameter in differential privacy (DP) has been an open question in DP research since it was first proposed in 2006. In this work, we demonstrate the ability of influence functions to offer insight into how a specific privacy parameter value will affect a model's test loss in the randomized response-based local DP setting. Our proposed method allows a data curator to select the privacy parameter best aligned with their allowed privacy-utility trade-off without requiring heavy computation such as extensive model retraining and data privatization. We consider multiple common randomization scenarios, such as performing randomized response over the features, and/or over the labels, as well as the more complex case of applying a class-dependent label noise correction method to offset the noise incurred by randomization. Further, we provide a detailed discussion over the computational complexity of our proposed approach inclusive of an empirical analysis. Thro
    
[^29]: 用可解释的生成神经网络量化信贷组合对资产相关性的敏感性

    Quantifying Credit Portfolio sensitivity to asset correlations with interpretable generative neural networks. (arXiv:2309.08652v1 [q-fin.RM])

    [http://arxiv.org/abs/2309.08652](http://arxiv.org/abs/2309.08652)

    本研究提出了一种新方法，利用生成的金融相关矩阵量化信贷组合对资产相关性的敏感性。通过使用可解释的变分自动编码器（VAE）的潜在空间表示，揭示了影响投资组合多元化的关键因素。

    

    在本研究中，我们提出了一种新的方法，利用深度学习模型生成的合成金融相关矩阵，量化信贷组合价值风险（VaR）对资产相关性的敏感性。之前的工作中，使用生成对抗网络（GANs）演示了生成能捕捉到资产收益的经验相关矩阵中观察到的基本特征的可信相关矩阵。我们使用变分自动编码器（VAE）而不是GANs，以实现更可解释的潜在空间表示。通过我们的分析，我们揭示了VAE潜在空间可以成为捕捉影响投资组合多元化的关键因素的有用工具，特别是与信贷组合敏感性和资产相关性变化相关的因素。

    In this research, we propose a novel approach for the quantification of credit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the use of synthetic financial correlation matrices generated with deep learning models. In previous work Generative Adversarial Networks (GANs) were employed to demonstrate the generation of plausible correlation matrices, that capture the essential characteristics observed in empirical correlation matrices estimated on asset returns. Instead of GANs, we employ Variational Autoencoders (VAE) to achieve a more interpretable latent space representation. Through our analysis, we reveal that the VAE latent space can be a useful tool to capture the crucial factors impacting portfolio diversification, particularly in relation to credit portfolio sensitivity to asset correlations changes.
    
[^30]: 通过共线约束注意力解决Transformer的头痛问题

    Cure the headache of Transformers via Collinear Constrained Attention. (arXiv:2309.08646v1 [cs.LG])

    [http://arxiv.org/abs/2309.08646](http://arxiv.org/abs/2309.08646)

    通过引入共线约束注意力（CoCA）结构，解决Transformer模型中的头痛问题，实现了出色的外推性能和提高的计算效率。

    

    随着基于大型语言模型的实际应用的快速进展，推断性能的外推变得在研究领域中变得越来越重要。在我们的研究中，我们发现了Transformer模型中的一个被之前忽视的异常行为，导致了最接近的标记之间的混乱，这些标记携带了最重要的信息。我们将这一发现称为“Transformer的头痛问题”。为了从根本上解决这个问题，我们引入了一种新的自注意结构，命名为Collinear Constrained Attention（CoCA）。这个结构可以无缝地与现有的推断、插值方法和其他针对传统Transformer模型设计的优化策略集成。我们在推断过程中实现了优秀的外推性能，即使是16到24倍的序列长度，而且没有对我们的模型进行任何微调。我们还增强了CoCA的计算和空间效率，以确保其实用性。我们计划...

    As the rapid progression of practical applications based on Large Language Models continues, the importance of extrapolating performance has grown exponentially in the research domain. In our study, we identified an anomalous behavior in Transformer models that had been previously overlooked, leading to a chaos around closest tokens which carried the most important information. We've coined this discovery the "headache of Transformers". To address this at its core, we introduced a novel self-attention structure named Collinear Constrained Attention (CoCA). This structure can be seamlessly integrated with existing extrapolation, interpolation methods, and other optimization strategies designed for traditional Transformer models. We have achieved excellent extrapolating performance even for 16 times to 24 times of sequence lengths during inference without any fine-tuning on our model. We have also enhanced CoCA's computational and spatial efficiency to ensure its practicality. We plan to
    
[^31]: 一种适用于虚拟电力厂中存在不确定性的实时能源调度的随机在线预测与优化框架

    A Stochastic Online Forecast-and-Optimize Framework for Real-Time Energy Dispatch in Virtual Power Plants under Uncertainty. (arXiv:2309.08642v1 [eess.SY])

    [http://arxiv.org/abs/2309.08642](http://arxiv.org/abs/2309.08642)

    本论文提出了一个适应不确定性的实时能源调度框架，该框架通过整合深度学习预测和随机优化，并通过在线数据增强和模型微调来解决数据波动、模型差异和环境扰动等不确定性问题。

    

    在电力系统中聚合分布式能源资源显著增加了不确定性，特别是由可再生能源产生的波动所引起的不确定性。这个问题驱使了在不确定性下广泛利用先进的预测控制技术，以确保长期经济和减碳。本文提出了一个实时的不确定性感知的能源调度框架，该框架由两个关键要素组成：(i) 一个混合的预测和优化顺序任务，将基于深度学习的预测和随机优化进行整合，这两个阶段通过多个时间分辨率的不确定性估计进行连接；(ii) 一种高效的在线数据增强方案，同时涉及模型预训练和在线微调阶段。通过这种方式，所提出的框架能够迅速适应实时数据分布，并针对数据漂移、模型差异和环境扰动引起的不确定性进行处理。

    Aggregating distributed energy resources in power systems significantly increases uncertainties, in particular caused by the fluctuation of renewable energy generation. This issue has driven the necessity of widely exploiting advanced predictive control techniques under uncertainty to ensure long-term economics and decarbonization. In this paper, we propose a real-time uncertainty-aware energy dispatch framework, which is composed of two key elements: (i) A hybrid forecast-and-optimize sequential task, integrating deep learning-based forecasting and stochastic optimization, where these two stages are connected by the uncertainty estimation at multiple temporal resolutions; (ii) An efficient online data augmentation scheme, jointly involving model pre-training and online fine-tuning stages. In this way, the proposed framework is capable to rapidly adapt to the real-time data distribution, as well as to target on uncertainties caused by data drift, model discrepancy and environment pertu
    
[^32]: FedFNN: 在联邦推荐系统中通过更新预测实现更快的训练收敛

    FedFNN: Faster Training Convergence Through Update Predictions in Federated Recommender Systems. (arXiv:2309.08635v1 [cs.IR])

    [http://arxiv.org/abs/2309.08635](http://arxiv.org/abs/2309.08635)

    FedFNN是一种在联邦推荐系统中加速训练的算法。通过预测未抽样用户的权重更新，使用已抽样集的更新，FedFNN实现了比其他方法快5倍的训练速度，同时保持或提高了准确性。

    

    联邦学习（FL）已成为分布式机器学习的关键方法，增强了在线个性化的同时确保用户数据的隐私。与传统方法将私有数据发送到中央服务器不同，FL将计算分散：设备在本地训练并与全局服务器共享更新。在这种情况下，主要挑战是实现快速和准确的模型训练，这对于推荐系统来说至关重要，因为延迟可能会损害用户参与度。本文介绍了FedFNN，一种加速分散式模型训练的算法。在FL中，每个训练周期仅涉及用户子集。FedFNN利用监督学习从未抽样的用户中预测权重更新，使用来自抽样集的更新。我们使用真实和合成数据进行了评估，结果显示：1. FedFNN的训练速度比领先方法快5倍，保持或提高准确性；2. 该算法的性能与客户端集群的变化无关。

    Federated Learning (FL) has emerged as a key approach for distributed machine learning, enhancing online personalization while ensuring user data privacy. Instead of sending private data to a central server as in traditional approaches, FL decentralizes computations: devices train locally and share updates with a global server. A primary challenge in this setting is achieving fast and accurate model training - vital for recommendation systems where delays can compromise user engagement. This paper introduces FedFNN, an algorithm that accelerates decentralized model training. In FL, only a subset of users are involved in each training epoch. FedFNN employs supervised learning to predict weight updates from unsampled users, using updates from the sampled set. Our evaluations, using real and synthetic data, show: 1. FedFNN achieves training speeds 5x faster than leading methods, maintaining or improving accuracy; 2. the algorithm's performance is consistent regardless of client cluster va
    
[^33]: 双高维上下文强化学习算法：用于联合组合-定价的可解释模型

    Doubly High-Dimensional Contextual Bandits: An Interpretable Model for Joint Assortment-Pricing. (arXiv:2309.08634v1 [stat.ML])

    [http://arxiv.org/abs/2309.08634](http://arxiv.org/abs/2309.08634)

    本论文提出了一种双高维上下文强化学习算法，用于解决联合组合-定价问题，通过简单而灵活的模型捕捉协变量和行为之间的相互作用，同时保持可解释性。该方法兼容多种结构化的线性强化学习和定价模型，提供了一种计算可行的流程。

    

    零售业务的关键挑战之一是如何选择要向消费者展示的产品（组合问题），以及如何定价产品（定价问题）以最大化收入或利润。我们提出了一种基于上下文强化学习的联合组合-定价方法，该方法同时考虑了组合和定价问题。我们的模型是双高维的，即上下文向量和行为都允许在高维空间中取值。为了克服维度灾难，我们提出了一个简单而灵活的模型，通过（近似）低秩表示矩阵来捕捉协变量和行为之间的相互作用。得到的模型类是相当表达力的，同时通过潜在因素保持可解释性，并包括不同结构化的线性强化学习和定价模型作为特殊情况。我们提出了一种计算可行的流程，将探索/利用协议与高效的低秩矩阵估计相结合。

    Key challenges in running a retail business include how to select products to present to consumers (the assortment problem), and how to price products (the pricing problem) to maximize revenue or profit. Instead of considering these problems in isolation, we propose a joint approach to assortment-pricing based on contextual bandits. Our model is doubly high-dimensional, in that both context vectors and actions are allowed to take values in high-dimensional spaces. In order to circumvent the curse of dimensionality, we propose a simple yet flexible model that captures the interactions between covariates and actions via a (near) low-rank representation matrix. The resulting class of models is reasonably expressive while remaining interpretable through latent factors, and includes various structured linear bandit and pricing models as particular cases. We propose a computationally tractable procedure that combines an exploration/exploitation protocol with an efficient low-rank matrix esti
    
[^34]: 大型语言模型能够推断社交媒体用户的心理倾向

    Large Language Models Can Infer Psychological Dispositions of Social Media Users. (arXiv:2309.08631v1 [cs.CL])

    [http://arxiv.org/abs/2309.08631](http://arxiv.org/abs/2309.08631)

    大型语言模型能够通过分析社交媒体用户的数字足迹推断他们的心理倾向，具体表现为从Facebook状态更新中推断五大人格特质。研究发现，推断得分与自我报告得分之间存在相关性，但在性别和年龄方面存在偏见。

    

    随着大型语言模型（LLMs）在各种自然语言处理（NLP）任务中展示出越来越接近人类的能力，而这些任务将成为个性化技术的重要组成部分，理解它们的能力和固有偏见至关重要。我们的研究调查了类似ChatGPT的LLMs从个人数字足迹中推断个人心理倾向的潜力。具体而言，我们评估了GPT-3.5和GPT-4在零样本学习场景下从用户的Facebook状态更新中推导出五大人格特质的能力。我们的结果显示LLM推断与自我报告得分之间的平均相关性为r = 0.29（范围为[0.22, 0.33]）。此外，我们的研究结果表明在性别和年龄方面存在个性推断的偏见：对于几个特质，推断得分在女性和年轻人中的误差较小，这表明可能存在来自底层训练数据或在线自我呈现的差异的系统性偏见。

    As Large Language Models (LLMs) demonstrate increasingly human-like abilities in various natural language processing (NLP) tasks that are bound to become integral to personalized technologies, understanding their capabilities and inherent biases is crucial. Our study investigates the potential of LLMs like ChatGPT to infer psychological dispositions of individuals from their digital footprints. Specifically, we assess the ability of GPT-3.5 and GPT-4 to derive the Big Five personality traits from users' Facebook status updates in a zero-shot learning scenario. Our results show an average correlation of r = .29 (range = [.22, .33]) between LLM-inferred and self-reported trait scores. Furthermore, our findings suggest biases in personality inferences with regard to gender and age: inferred scores demonstrated smaller errors for women and younger individuals on several traits, suggesting a potential systematic bias stemming from the underlying training data or differences in online self-e
    
[^35]: PCN：一种利用新颖的图构建方法和切比雪夫图卷积的深度学习方法进行喷注标记

    PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph Construction Methods and Chebyshev Graph Convolutions. (arXiv:2309.08630v1 [hep-ph])

    [http://arxiv.org/abs/2309.08630](http://arxiv.org/abs/2309.08630)

    本研究提出了一种基于图形的喷注表示方法，并设计了一种名为PCN的图神经网络（GNN），利用切比雪夫图卷积（ChebConv）进行深度学习喷注标记，取得了显著的改进。

    

    喷注标记是高能物理实验中的一个分类问题，旨在识别粒子碰撞产生的锥状喷注，并将其标记为发射粒子。喷注标记的进展为超出标准模型的新物理搜索提供了机会。目前的方法使用深度学习在复杂碰撞数据中寻找隐藏的模式。然而，将喷注表示为深度学习模型的输入的方法多种多样，并且通常会向模型隐藏有信息的特征。在这项研究中，我们提出了一种基于图形的喷注表示方法，以尽可能地编码最多的信息。为了从这种表示中最好地学习，我们设计了一种名为Particle Chebyshev Network（PCN）的图神经网络（GNN），并使用切比雪夫图卷积（ChebConv）。ChebConv已经被证明是GNN中的一种有效替代传统图卷积的方法，而在喷注标记中还没有被探索过。PCN取得了显著的改进。

    Jet tagging is a classification problem in high-energy physics experiments that aims to identify the collimated sprays of subatomic particles, jets, from particle collisions and tag them to their emitter particle. Advances in jet tagging present opportunities for searches of new physics beyond the Standard Model. Current approaches use deep learning to uncover hidden patterns in complex collision data. However, the representation of jets as inputs to a deep learning model have been varied, and often, informative features are withheld from models. In this study, we propose a graph-based representation of a jet that encodes the most information possible. To learn best from this representation, we design Particle Chebyshev Network (PCN), a graph neural network (GNN) using Chebyshev graph convolutions (ChebConv). ChebConv has been demonstrated as an effective alternative to classical graph convolutions in GNNs and has yet to be explored in jet tagging. PCN achieves a substantial improvemen
    
[^36]: 通过大型语言模型进行隐私保护掩码的恢复

    Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])

    [http://arxiv.org/abs/2309.08628](http://arxiv.org/abs/2309.08628)

    本文利用大型语言模型（LLM）探索了替换标识信息的方法，并在下游语言建模任务上进行了评估。实验结果表明，使用混淆语料库训练的模型能够达到可比较的性能。

    

    模型适应对于处理代理训练数据和实际用户数据之间的差异非常重要。为了有效地进行适应，用户的文本数据通常存储在服务器或本地设备上，下游的自然语言处理模型可以使用这些领域内的数据进行直接训练。然而，这可能会引起隐私和安全问题，因为存在向对手泄露用户信息的额外风险。最近，人们开始探索使用通用标记替换文本中的标识信息。在这项工作中，我们利用大型语言模型（LLM）来建议替换掩码标记的方法，并在下游语言建模任务上评估其效果。具体而言，我们提出了多种基于预训练和微调的LLM方法，并在不同数据集上进行实证研究以比较这些方法。实验结果表明，在混淆语料库上训练的模型能够达到可比较的性能。

    Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve compar
    
[^37]: 评估动态主题模型

    Evaluating Dynamic Topic Models. (arXiv:2309.08627v1 [cs.CL])

    [http://arxiv.org/abs/2309.08627](http://arxiv.org/abs/2309.08627)

    提出了一种评估动态主题模型的新方法，该方法分析了每个主题随时间变化的质量变化，并结合了模型的时间一致性。该方法在合成数据和已有DTMs数据上展示了实用性，并与人类判断具有良好的相关性。这些研究结果对于识别变化的主题、评估DTMs和指导未来研究具有重要意义。

    

    动态主题模型(DTMs)在评估主题随时间变化的进展方面缺乏定量指标。为了填补这一空白，我们提出了一种新颖的DTMs评估方法，该方法分析了每个主题随时间变化的质量变化。此外，我们还提出了结合主题质量和模型时间一致性的扩展方法。我们通过将该方法应用于合成数据和已有DTMs的数据来证明其实用性。我们还进行了人工评估，结果表明该方法与人类判断具有良好的相关性。我们的研究结果有助于识别变化的主题、评估不同的DTMs以及指导未来的研究领域。

    There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model's temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs. We also conducted a human evaluation, which indicates that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs, and guiding future research in this area.
    
[^38]: 源于鞋垫传感器的平衡测量能区分早期患有Lewy体痴呆的患者

    Balance Measures Derived from Insole Sensor Differentiate Prodromal Dementia with Lewy Bodies. (arXiv:2309.08623v1 [eess.SP])

    [http://arxiv.org/abs/2309.08623](http://arxiv.org/abs/2309.08623)

    本研究利用鞋垫传感器获得的平衡测量，提出了一种基于机器学习的自动流程管道来辅助鉴别早期患有Lewy体痴呆的患者。实验证明，该模型能以78.0%的准确率将MCI-LB与其他组别区分开，比基于人口统计和临床神经心理测量的参考模型准确率提高了6.8%。

    

    Lewy体痴呆是第二常见的神经退行性痴呆类型，早期识别，即由Lewy体引起的轻度认知功能障碍（MCI-LB），对于提供适当的护理非常重要。然而，由于其临床表现多样性以及与其他病症（如阿尔茨海默病引起的轻度认知功能障碍）的相似之处，MCI-LB经常被低估。在这项研究中，我们提出了一种基于机器学习的自动流程管道，通过利用鞋垫传感器在30秒站立任务期间获取的平衡测量来帮助识别MCI-LB。98名参与者（14名MCI-LB、38名MCI-AD、46名认知正常人）的实验结果显示，所得到的模型能够以高达78.0%的准确率（AUC: 0.681）将MCI-LB与其他组别进行区分，这比基于人口统计和临床神经心理测量的参考模型的准确率提高了6.8%。我们的发现可能为一种新的方法打开了大门。

    Dementia with Lewy bodies is the second most common type of neurodegenerative dementia, and identification at the prodromal stage$-$i.e., mild cognitive impairment due to Lewy bodies (MCI-LB)$-$is important for providing appropriate care. However, MCI-LB is often underrecognized because of its diversity in clinical manifestations and similarities with other conditions such as mild cognitive impairment due to Alzheimer's disease (MCI-AD). In this study, we propose a machine learning-based automatic pipeline that helps identify MCI-LB by exploiting balance measures acquired with an insole sensor during a 30-s standing task. An experiment with 98 participants (14 MCI-LB, 38 MCI-AD, 46 cognitively normal) showed that the resultant models could discriminate MCI-LB from the other groups with up to 78.0% accuracy (AUC: 0.681), which was 6.8% better than the accuracy of a reference model based on demographic and clinical neuropsychological measures. Our findings may open up a new approach for 
    
[^39]: 重要性重采样方差减小的研究

    Variance Reduction of Resampling for Sequential Monte Carlo. (arXiv:2309.08620v1 [stat.CO])

    [http://arxiv.org/abs/2309.08620](http://arxiv.org/abs/2309.08620)

    本研究提出了一种重要性重采样方案，通过引入重复确定性区域和中位数遍历性的方法，实现了最低的方差，使得顺序蒙特卡洛方法在逼近隐马尔可夫模型时更快且准确。

    

    重采样方案为顺序蒙特卡洛方法提供了一种通过更高权重的粒子来表示目标分布的方法。权重分布的方差越小，有效粒子的集中程度越高，对于非线性情况下的隐马尔可夫模型的逼近速度和准确性就越快且更准确。我们提出了一种重复确定性区域与中位数遍历性的重采样方法，并在与其他重采样方法的比较中实现了最低的方差。在确定性区域的大小$M\ll N$（粒子数量的大小）的情况下，考虑到实际的粒子数量，我们的算法比最先进的状态要更快，这一点通过在线性和非线性情况下隐马尔可夫模型的理论推导和实验证明了。

    A resampling scheme provides a way to switch low-weight particles for sequential Monte Carlo with higher-weight particles representing the objective distribution. The less the variance of the weight distribution is, the more concentrated the effective particles are, and the quicker and more accurate it is to approximate the hidden Markov model, especially for the nonlinear case. We propose a repetitive deterministic domain with median ergodicity for resampling and have achieved the lowest variances compared to the other resampling methods. As the size of the deterministic domain $M\ll N$ (the size of population), given a feasible size of particles, our algorithm is faster than the state of the art, which is verified by theoretical deduction and experiments of a hidden Markov model in both the linear and non-linear cases.
    
[^40]: Drifter: 大规模推荐系统中高效的在线特征监控以提高数据完整性

    Drifter: Efficient Online Feature Monitoring for Improved Data Integrity in Large-Scale Recommendation Systems. (arXiv:2309.08617v1 [cs.IR])

    [http://arxiv.org/abs/2309.08617](http://arxiv.org/abs/2309.08617)

    Drifter是一个高效的在线特征监控系统，通过敏捷、响应和适应性的数据质量监控，实时分析、检测和解决推荐系统中的数据问题，使得实时推荐系统的可靠性和性能得到显著提升。

    

    实际生产系统通常面临在大规模、动态流中维护数据质量的问题。我们介绍了Drifter，这是一个用于推荐系统中在线特征监控和验证的高效且轻量级的系统。Drifter通过提供敏捷、响应和适应性的数据质量监控，能够实时进行根本原因分析、漂移检测以及对有问题的生产事件的洞察。Drifter集成了最先进的稀疏数据在线特征排名和异常检测方法，具有高度可扩展性和资源效率性，每分钟处理数百万个实例仅需要两个线程和少于一GB的RAM。在真实数据集上的评估证明了Drifter在警报和缓解数据质量问题方面的有效性，大大提高了实时实况推荐系统的可靠性和性能。

    Real-world production systems often grapple with maintaining data quality in large-scale, dynamic streams. We introduce Drifter, an efficient and lightweight system for online feature monitoring and verification in recommendation use cases. Drifter addresses limitations of existing methods by delivering agile, responsive, and adaptable data quality monitoring, enabling real-time root cause analysis, drift detection and insights into problematic production events. Integrating state-of-the-art online feature ranking for sparse data and anomaly detection ideas, Drifter is highly scalable and resource-efficient, requiring only two threads and less than a gigabyte of RAM per production deployments that handle millions of instances per minute. Evaluation on real-world data sets demonstrates Drifter's effectiveness in alerting and mitigating data quality issues, substantially improving reliability and performance of real-time live recommender systems.
    
[^41]: 预测疾病并发症中的多模态推荐系统

    Multimodal Recommender Systems in the Prediction of Disease Comorbidity. (arXiv:2309.08613v1 [cs.IR])

    [http://arxiv.org/abs/2309.08613](http://arxiv.org/abs/2309.08613)

    该研究探讨了在医疗领域中利用基于深度学习的推荐系统进行疾病并发症预测的方法。研究使用了NCF和DHF两种新颖的推荐系统，并利用了不同的数据集进行预测。研究结果显示NCF模型在准确率和命中率方面表现较差。

    

    尽管基于深度学习的协同过滤推荐系统已经在其他领域的推荐中得到普遍应用，但在医疗领域的应用还很有限。除了建模用户-项目交互之外，我们还展示了基于深度学习的推荐系统可以用于建模主题-疾病码交互。我们利用神经协同过滤(NCF)和深度混合过滤(DHF)这两种基于深度学习的推荐系统在疾病诊断中进行了两种新颖的应用，基于已知的过去患者并发症来进行预测。我们使用了两个数据集，一个包含MIMIC-III数据库中的所有主题-疾病码对，另一个包含发生最常见的50种疾病。准确率和Hit Ratio@10被用作评估模型性能的指标。发现利用减少的“top 50” ICD-9码数据集的NCF模型的性能较低(准确率约为80%和Hit Ratio@10为...

    While deep-learning based recommender systems utilizing collaborative filtering have been commonly used for recommendation in other domains, their application in the medical domain have been limited. In addition to modeling user-item interactions, we show that deep-learning based recommender systems can be used to model subject-disease code interactions. Two novel applications of deep learning-based recommender systems using Neural Collaborative Filtering (NCF) and Deep Hybrid Filtering (DHF) were utilized for disease diagnosis based on known past patient comorbidities. Two datasets, one incorporating all subject-disease code pairs present in the MIMIC-III database, and the other incorporating the top 50 most commonly occurring diseases, were used for prediction. Accuracy and Hit Ratio@10 were utilized as metrics to estimate model performance. The performance of the NCF model making use of the reduced "top 50" ICD-9 code dataset was found to be lower (accuracy of ~80% and hit ratio@10 
    
[^42]: 进行弗兰肯斯坦法或如何通过混合模型空间来提升对分布之外性能的改进

    Do the Frankenstein, or how to achieve better out-of-distribution performance with manifold mixing model soup. (arXiv:2309.08610v1 [cs.LG])

    [http://arxiv.org/abs/2309.08610](http://arxiv.org/abs/2309.08610)

    提出了一种混合模型空间法，在微调模型后生成融合模型，该模型在分布之外表现更好（比最佳单个模型提高了3.5%），同时在原始数据集上也提供更好的准确性。

    

    在迁移学习中，使用的标准方法是在特定任务的数据集上微调预训练模型，使用不同的超参数设置，并选择在验证数据集上准确率最高的模型。然而，这会导致模型在分布转换时表现不佳，例如当模型接收输入的是图形化的物体草图而不是照片时。为了解决这个问题，我们提出了混合模型空间法，这是一种通过最佳方式混合多个微调模型的潜在空间流形来生成一个融合模型的算法。我们展示了，在对图像分类进行微调后，融合模型在分布之外性能方面显著优于最佳的单个模型（比最佳单个模型提高了3.5%）。此外，它还在进行微调的原始数据集上提供更好的准确性。

    The standard recipe applied in transfer learning is to finetune a pretrained model on the task-specific dataset with different hyperparameter settings and pick the model with the highest accuracy on the validation dataset. Unfortunately, this leads to models which do not perform well under distribution shifts, e.g. when the model is given graphical sketches of the object as input instead of photos. In order to address this, we propose the manifold mixing model soup, an algorithm which mixes together the latent space manifolds of multiple finetuned models in an optimal way in order to generate a fused model. We show that the fused model gives significantly better out-of-distribution performance (+3.5 % compared to best individual model) when finetuning a CLIP model for image classification. In addition, it provides also better accuracy on the original dataset where the finetuning has been done.
    
[^43]: 在2022/23年监测乌克兰马里乌波尔市的城市变化

    Monitoring Urban Changes in Mariupol/Ukraine in 2022/23. (arXiv:2309.08607v1 [cs.CY])

    [http://arxiv.org/abs/2309.08607](http://arxiv.org/abs/2309.08607)

    本文研究证明使用历史数据进行迁移学习是解决城市变化监测问题的可行方案，通过使用合成孔径雷达和光学多光谱观测数据，成功监测了乌克兰马里乌波尔市在俄乌冲突开始阶段的相关城市变化。

    

    不断监测城市变化的能力具有巨大的社会经济利益。之前的研究已经展示了使用深度神经网络（DNNs）和迁移学习在这一领域的方法。然而，它们未能展示在训练或迁移领域之外的时间尺度。本研究在现有研究的基础上，证明了使用历史数据进行迁移学习是可行的解决方案，仍然可以对以后的年份进行城市变化监测。我们考虑了一个对公共和免费高分辨率图像访问有限的情况来指导迁移。为了提供高时空分辨率，我们的监测方法的核心数据包括来自Sentinel 1（合成孔径雷达）和Sentinel 2（光学多光谱）的多模态合成孔径雷达和光学多光谱观测。我们选择了实际应用我们的方法来监测乌克兰马里乌波尔市与俄乌冲突开始时的相关城市变化。

    The ability to constantly monitor urban changes is of large socio-economic interest. Previous works have already shown approaches in this field with the use of Deep Neural Networks (DNNs) and transfer learning. However, they fell short in demonstrating temporal scale outside of either the training or transfer domain.  This work builds on existing research and proves that transfer learning with the use of historic data is a feasible solution, which still allows the urban change monitoring of later years. We considered a case with limited access to public and free Very High Resolution (VHR) imagery to guide the transfer. To provide a high temporal resolution, the core data of our monitoring method comprised multi-modal Synthetic Aperture Radar (SAR) and optical multispectral observations from Sentinel 1 and Sentinel 2, respectively.  We chose a practical application of our methods for monitoring urban-related changes in the city of Mariupol in Ukraine during the beginning of the Russo-Uk
    
[^44]: CRYPTO-MINE: 通过互信息神经估计进行密码分析

    CRYPTO-MINE: Cryptanalysis via Mutual Information Neural Estimation. (arXiv:2309.08019v1 [cs.CR])

    [http://arxiv.org/abs/2309.08019](http://arxiv.org/abs/2309.08019)

    CRYPTO-MINE是一种通过神经网络估计互信息的新方法，应用于选择明文攻击中明文和密文之间的互信息估计。该方法可用于分析密码系统的计算安全性和信息泄露与输入分布之间的关系。

    

    互信息（MI）作为评估密码系统效率的指标具有广泛的历史。然而，在高维空间中估计未知随机变量之间的互信息是具有挑战性的。机器学习的最新进展使得使用神经网络估计互信息成为可能。本文提出了互信息估计在密码学领域的新应用。我们建议将这种方法直接应用于选择明文攻击中明文和密文之间的估计互信息。如果有的话，加密中的泄露信息可能会被对手利用来破坏密码系统的计算安全性。我们通过对多个加密方案和基准方法进行经验分析来评估我们方法的效率。此外，我们还扩展了对提供个体保密性的基于网络编码的密码系统的分析，并研究了信息泄露和输入分布之间的关系。

    The use of Mutual Information (MI) as a measure to evaluate the efficiency of cryptosystems has an extensive history. However, estimating MI between unknown random variables in a high-dimensional space is challenging. Recent advances in machine learning have enabled progress in estimating MI using neural networks. This work presents a novel application of MI estimation in the field of cryptography. We propose applying this methodology directly to estimate the MI between plaintext and ciphertext in a chosen plaintext attack. The leaked information, if any, from the encryption could potentially be exploited by adversaries to compromise the computational security of the cryptosystem. We evaluate the efficiency of our approach by empirically analyzing multiple encryption schemes and baseline approaches. Furthermore, we extend the analysis to novel network coding-based cryptosystems that provide individual secrecy and study the relationship between information leakage and input distribution
    
[^45]: Voxtlm: 统一的只解码模型，用于合并语音识别/合成和语音/文本补充任务

    Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks. (arXiv:2309.07937v1 [eess.AS])

    [http://arxiv.org/abs/2309.07937](http://arxiv.org/abs/2309.07937)

    Voxtlm是一个统一的只解码模型，能够在语音识别、语音合成、文本生成和语音延续等任务上取得显著的改善。

    

    我们提出了一个只解码语言模型VoxtLM，能够执行四个任务：语音识别、语音合成、文本生成和语音延续。VoxtLM将文本词汇与自监督语音特征中的离散语音令牌进行整合，并使用特殊令牌实现多任务学习。与单任务模型相比，VoxtLM在语音合成方面显示了显著的改善，语音可理解性从28.9提高到5.6，客观质量从2.68提高到3.90。VoxtLM还改善了语音生成和语音识别性能。VoxtLM使用公开可用的数据进行训练，并将提供训练脚本和模型检查点的开源代码，以实现完全可复现的工作。

    We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. VoxtLM is trained with publicly available data and training recipes and model checkpoints will be open-sourced to make fully reproducible work.
    
[^46]: 使用声音提示进行分割的泛化音频-视觉源定位器

    Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer. (arXiv:2309.07929v1 [cs.CV])

    [http://arxiv.org/abs/2309.07929](http://arxiv.org/abs/2309.07929)

    本研究提出了一种使用声音提示进行分割的泛化音频-视觉源定位器，在零样本和少样本情况下实现音频-视觉定位和分割任务。通过引入编码器提示解码器范式、构建语义感知音频提示和相关适配器来解决数据稀缺性和不同数据分布的困境。

    

    在从未同时看到物体和听到其声音的情况下，模型是否仍然能够准确地从输入音频中定位其视觉位置？在这项工作中，我们关注零样本和少样本情况下的音频-视觉定位和分割任务。为了实现这个目标，我们引入了编码器提示解码器的范式，与现有方法不同，现有方法主要使用编码器融合解码器范式从融合音频-视觉特征中解码定位信息，我们旨在借助预训练模型的丰富知识来更好地适应数据稀缺性和不同数据分布的困境。具体地，我们首先提出构建语义感知音频提示（SAP）来帮助视觉基础模型关注有声对象，同时也鼓励视觉和音频模态之间的语义差距缩小。然后，我们开发了一个相关适配器（ColA）来保持最小的训练工作量并维持模型性能。

    Never having seen an object and heard its sound simultaneously, can the model still accurately localize its visual position from the input audio? In this work, we concentrate on the Audio-Visual Localization and Segmentation tasks but under the demanding zero-shot and few-shot scenarios. To achieve this goal, different from existing approaches that mostly employ the encoder-fusion-decoder paradigm to decode localization information from the fused audio-visual feature, we introduce the encoder-prompt-decoder paradigm, aiming to better fit the data scarcity and varying data distribution dilemmas with the help of abundant knowledge from pre-trained models. Specifically, we first propose to construct Semantic-aware Audio Prompt (SAP) to help the visual foundation model focus on sounding objects, meanwhile, the semantic gap between the visual and audio modalities is also encouraged to shrink. Then, we develop a Correlation Adapter (ColA) to keep minimal training efforts as well as maintain 
    
[^47]: 癌症临床试验资格标准的文本分类

    Text Classification of Cancer Clinical Trial Eligibility Criteria. (arXiv:2309.07812v1 [cs.CL])

    [http://arxiv.org/abs/2309.07812](http://arxiv.org/abs/2309.07812)

    本文研究了癌症临床试验中常见的排除标准，通过应用文本分类方法和预训练的BERT模型，证明了自动分类排除标准的可行性，并展示了专门为临床试验设计的预训练语言模型的价值。

    

    由于试验资格标准以自然语言形式陈述，因此自动确定患者是否符合试验资格是一项复杂的任务。解决该问题的一个潜在方法是使用文本分类方法对常见类型的资格标准进行处理。本研究关注癌症试验中的七个常见排除标准：先前恶性肿瘤、人类免疫缺陷病毒、乙肝病毒、丙肝病毒、精神疾病、药物/物质滥用和自身免疫疾病。我们的数据集包含764个带有这些排除标准注释的三期癌症试验。我们尝试了常见的transformer模型以及一个新的预训练的临床试验BERT模型。我们的结果表明，自动分类常见的排除标准是可行的。此外，我们展示了一种专门针对临床试验的预训练语言模型的价值，该模型在所有标准中表现出最高的平均性能。

    Automatic identification of clinical trials for which a patient is eligible is complicated by the fact that trial eligibility is stated in natural language. A potential solution to this problem is to employ text classification methods for common types of eligibility criteria. In this study, we focus on seven common exclusion criteria in cancer trials: prior malignancy, human immunodeficiency virus, hepatitis B, hepatitis C, psychiatric illness, drug/substance abuse, and autoimmune illness. Our dataset consists of 764 phase III cancer trials with these exclusions annotated at the trial level. We experiment with common transformer models as well as a new pre-trained clinical trial BERT model. Our results demonstrate the feasibility of automatically classifying common exclusion criteria. Additionally, we demonstrate the value of a pre-trained language model specifically for clinical trials, which yields the highest average performance across all criteria.
    
[^48]: 在强化学习中使用的近似的某些本地空间中的收敛速度

    Rates of Convergence in Certain Native Spaces of Approximations used in Reinforcement Learning. (arXiv:2309.07383v1 [eess.SY])

    [http://arxiv.org/abs/2309.07383](http://arxiv.org/abs/2309.07383)

    本文研究了在强化学习中出现的值函数近似在特定本地空间中的收敛速度，提出了运用算子方程进行离线近似的方法，并通过有限维近似空间中的功率函数得到了值函数近似误差的上界。这些结果改进和细化了值函数近似的收敛性。

    

    本文研究了在一组再生核希尔伯特空间（RKHS）$H(\Omega)$中出现的一些值函数近似的收敛速度。通过在特定类的本地空间中构建一个最优控制问题，得到了离线近似的算子方程的强收敛速度，这个算子方程出现在策略迭代中。利用有限维近似空间$H_N$在本地空间$H(\Omega)$中的功率函数$\Pwr_{H,N}$，得到了值函数近似误差的显式上界。这些上界具有几何性质，并对值函数近似的收敛性有了一些改进和细化。

    This paper studies convergence rates for some value function approximations that arise in a collection of reproducing kernel Hilbert spaces (RKHS) $H(\Omega)$. By casting an optimal control problem in a specific class of native spaces, strong rates of convergence are derived for the operator equation that enables offline approximations that appear in policy iteration. Explicit upper bounds on error in value function approximations are derived in terms of power function $\Pwr_{H,N}$ for the space of finite dimensional approximants $H_N$ in the native space $H(\Omega)$. These bounds are geometric in nature and refine some well-known, now classical results concerning convergence of approximations of value functions.
    
[^49]: 通过基于领域信息的高斯过程状态空间模型实现未知领域检测

    Out of Distribution Detection via Domain-Informed Gaussian Process State Space Models. (arXiv:2309.06655v1 [cs.RO])

    [http://arxiv.org/abs/2309.06655](http://arxiv.org/abs/2309.06655)

    本文提出了一种通过嵌入领域知识的高斯过程状态空间模型，以实现未知领域的检测，并基于递归预测构建了在线的监视器。数值结果表明，带有领域信息的内核可以提供更好的回归质量。

    

    为了使机器人能够使用基于学习的方法在未知情景中安全导航，准确地在线检测训练集之外的情况是非常重要的。近年来，高斯过程状态空间模型（GPSSM）已被证明是一种有效的方法，通过将不确定的观测与概率预测进行比较来区分它们。然而，模型能够正确区分训练集内外的观测取决于预测的准确性，主要受到GPSSM内核所能表示的函数类的影响。本文提出了一种新颖的方法，将现有领域知识嵌入内核中，并基于递归地预测构建了一个在线运行时的未知领域监视器。领域知识假设以在模拟中收集的数据集或使用名义模型。数值结果表明，对于较小的数据集，带有领域信息的内核提供了更好的回归质量，与标准内核相比。

    In order for robots to safely navigate in unseen scenarios using learning-based methods, it is important to accurately detect out-of-training-distribution (OoD) situations online. Recently, Gaussian process state-space models (GPSSMs) have proven useful to discriminate unexpected observations by comparing them against probabilistic predictions. However, the capability for the model to correctly distinguish between in- and out-of-training distribution observations hinges on the accuracy of these predictions, primarily affected by the class of functions the GPSSM kernel can represent. In this paper, we propose (i) a novel approach to embed existing domain knowledge in the kernel and (ii) an OoD online runtime monitor, based on receding-horizon predictions. Domain knowledge is assumed given as a dataset collected either in simulation or using a nominal model. Numerical results show that the informed kernel yields better regression quality with smaller datasets, as compared to standard ker
    
[^50]: 可解释的图神经网络用于阿尔茨海默病和相关痴呆症风险预测

    Explainable Graph Neural Network for Alzheimer's Disease And Related Dementias Risk Prediction. (arXiv:2309.06584v1 [cs.LG])

    [http://arxiv.org/abs/2309.06584](http://arxiv.org/abs/2309.06584)

    这项研究提出了一种可解释的图神经网络方法来预测阿尔茨海默病和相关痴呆症的风险。通过将机器学习与索赔数据相结合，不仅能发现额外的风险因素，还能揭示不同医学代码之间的关联。通过评估关系重要性和其对风险预测的影响，该方法能提供全面的解释。

    

    阿尔茨海默病和相关痴呆症（ADRD）在美国是第六大死亡原因，准确的ADRD风险预测具有重要意义。虽然最近在ADRD风险预测方面取得了一定进展，但大部分依赖于图像分析，而并非所有患者在ADRD诊断前都接受医学影像检查。将机器学习与索赔数据相结合可以揭示额外的风险因素并发现不同医学代码之间的相互关联。我们的目标是利用图神经网络（GNN）和索赔数据进行ADRD风险预测。为了解决这些预测背后缺乏可解释原因的问题，我们引入了一种创新方法来评估关系重要性及其对ADRD风险预测的影响，确保全面解释。我们使用变分正则化编码器-解码器图神经网络（VGNN）来估计ADRD可能性。我们创建了三种情景来评估模型的效率，使用了随机森林和轻梯度...

    Alzheimer's disease and related dementias (ADRD) ranks as the sixth leading cause of death in the US, underlining the importance of accurate ADRD risk prediction. While recent advancement in ADRD risk prediction have primarily relied on imaging analysis, yet not all patients undergo medical imaging before an ADRD diagnosis. Merging machine learning with claims data can reveal additional risk factors and uncover interconnections among diverse medical codes. Our goal is to utilize Graph Neural Networks (GNNs) with claims data for ADRD risk prediction. Addressing the lack of human-interpretable reasons behind these predictions, we introduce an innovative method to evaluate relationship importance and its influence on ADRD risk prediction, ensuring comprehensive interpretation.  We employed Variationally Regularized Encoder-decoder Graph Neural Network (VGNN) for estimating ADRD likelihood. We created three scenarios to assess the model's efficiency, using Random Forest and Light Gradient 
    
[^51]: PACE: 使用GPT-4进行云事件根本原因分析中的提示和增加以进行校准的置信度估计

    PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])

    [http://arxiv.org/abs/2309.05833](http://arxiv.org/abs/2309.05833)

    本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。

    

    近年来，IT行业向基于云的平台的转变强调了云事件根本原因分析的重要性，以确保服务的可靠性和维护客户信任。核心问题是有效确定根本原因，由于当代云基础设施的复杂性，这一任务变得具有挑战性。尽管出现了许多用于根本原因识别的基于AI的工具，但它们的适用性仍受到其输出质量不一致的限制。本文介绍了一种通过提示检索增强的大语言模型（LLM）来增强根本原因分析工具中置信度估计的方法。此方法分为两个阶段。首先，模型根据历史事件数据评估自身的置信度，考虑其对证据的评估强度。然后，模型审核由预测器生成的根本原因。然后，优化步骤将这些评估结合起来确定最终的置信度估计。

    In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
    
[^52]: 利用特征缺失感知校准的原型患者表示来缓解电子健康记录数据稀疏性问题

    Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration to Mitigate EHR Data Sparsity. (arXiv:2309.04160v1 [cs.LG])

    [http://arxiv.org/abs/2309.04160](http://arxiv.org/abs/2309.04160)

    本研究提出了一种利用原型患者表示和特征缺失感知校准的间接插补方法，以缓解电子健康记录数据稀疏性问题，通过获取更密集的嵌入来提高预测模型的有效性。

    

    电子健康记录（EHR）数据经常呈现稀疏特征，给预测建模带来挑战。当前的直接插补方法（如矩阵插补方法）依赖于参考类似行或列来完成原始缺失数据，不区分插补和实际值。因此，模型可能会无意中将与预测目标无关的或具有欺骗性的信息纳入其中，从而损害下游性能的效果。虽然一些方法尝试在直接插补后重新校准或增强EHR嵌入，但它们经常错误地优先考虑插补特征。这种优先错误可能会给模型引入偏见或不准确性。为了解决这些问题，我们的工作采用间接插补，利用类似患者的原型表示获取更密集的嵌入。认识到在衡量时通常将缺失特征与存在特征相同的限制时

    Electronic Health Record (EHR) data frequently exhibits sparse characteristics, posing challenges for predictive modeling. Current direct imputation such as matrix imputation approaches hinge on referencing analogous rows or columns to complete raw missing data and do not differentiate between imputed and actual values. As a result, models may inadvertently incorporate irrelevant or deceptive information with respect to the prediction objective, thereby compromising the efficacy of downstream performance. While some methods strive to recalibrate or augment EHR embeddings after direct imputation, they often mistakenly prioritize imputed features. This misprioritization can introduce biases or inaccuracies into the model. To tackle these issues, our work resorts to indirect imputation, where we leverage prototype representations from similar patients to obtain a denser embedding. Recognizing the limitation that missing features are typically treated the same as present ones when measurin
    
[^53]: 扩散生成反向设计

    Diffusion Generative Inverse Design. (arXiv:2309.02040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.02040](http://arxiv.org/abs/2309.02040)

    本文介绍了一种使用去噪扩散模型（DDMs）高效解决反向设计问题的方法，并提出了一种粒子采样算法来进一步改进。

    

    反向设计是指通过优化目标函数的输入来实现目标结果的问题。对于许多实际工程问题，目标函数采用模拟器的形式，预测系统状态随时间的演化，设计挑战是优化导致目标结果的初始条件。最近，学习模拟的发展表明图神经网络（GNN）可以用于准确、高效、可微分地估计模拟器动态，并支持具有梯度或基于采样的优化程序的高质量设计优化。然而，从头开始优化设计需要许多昂贵的模型查询，并且这些程序在非凸或高维问题上表现出基本失败。在这项工作中，我们展示了如何使用去噪扩散模型（DDMs）来高效地解决反向设计问题，并提出了一种粒子采样算法来进一步改进。

    Inverse design refers to the problem of optimizing the input of an objective function in order to enact a target outcome. For many real-world engineering problems, the objective function takes the form of a simulator that predicts how the system state will evolve over time, and the design challenge is to optimize the initial conditions that lead to a target outcome. Recent developments in learned simulation have shown that graph neural networks (GNNs) can be used for accurate, efficient, differentiable estimation of simulator dynamics, and support high-quality design optimization with gradient- or sampling-based optimization procedures. However, optimizing designs from scratch requires many expensive model queries, and these procedures exhibit basic failures on either non-convex or high-dimensional problems. In this work, we show how denoising diffusion models (DDMs) can be used to solve inverse design problems efficiently and propose a particle sampling algorithm for further improving
    
[^54]: 用于机器学习的热带几何工具：TML软件包

    Tropical Geometric Tools for Machine Learning: the TML package. (arXiv:2309.01082v1 [stat.ML])

    [http://arxiv.org/abs/2309.01082](http://arxiv.org/abs/2309.01082)

    TML软件包是第一个包含一套全面工具和方法的R软件包，用于处理与热带凸性相关的基本计算和可视化，以及使用热带度量进行监督和无监督学习模型的统计推断。

    

    在过去的十年中，热带几何学的发展提供了许多直接应用于统计学习问题的工具。TML软件包是第一个包含一套全面的工具和方法的R软件包，用于处理与热带凸性相关的基本计算、热带凸集的可视化，以及使用热带度量和热带投影环上的max-plus代数进行监督和无监督学习模型。主要的，TML软件包使用Hit and Run Markov chain Monte Carlo采样器与热带度量作为统计推断的主要工具。除了基本计算和热带HAR采样器的各种应用之外，我们还关注TML软件包中包含的几种监督和无监督方法，包括热带主成分分析、热带逻辑回归和热带核密度估计。

    In the last decade, developments in tropical geometry have provided a number of uses directly applicable to problems in statistical learning. The TML package is the first R package which contains a comprehensive set of tools and methods used for basic computations related to tropical convexity, visualization of tropically convex sets, as well as supervised and unsupervised learning models using the tropical metric under the max-plus algebra over the tropical projective torus. Primarily, the TML package employs a Hit and Run Markov chain Monte Carlo sampler in conjunction with the tropical metric as its main tool for statistical inference. In addition to basic computation and various applications of the tropical HAR sampler, we also focus on several supervised and unsupervised methods incorporated in the TML package including tropical principal component analysis, tropical logistic regression and tropical kernel density estimation.
    
[^55]: 大型语言模型的可解释性：一项调查

    Explainability for Large Language Models: A Survey. (arXiv:2309.01029v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01029](http://arxiv.org/abs/2309.01029)

    本文调研了大型语言模型的可解释性问题，提出了一个解释技术的分类法，并介绍了基于Transformer的语言模型的解释方法。同时，讨论了评估生成解释的度量标准，以及如何利用解释来调试模型和提高性能。

    

    大型语言模型（LLMs）在自然语言处理中展示出令人印象深刻的能力。然而，它们的内部机制仍然不明确，这种缺乏透明度为下游应用带来了不必要的风险。因此，理解和解释这些模型对于阐明它们的行为、限制和社会影响至关重要。在本文中，我们引入了一个可解释性技术的分类法，并提供了一种结构化的概述方法，用于解释基于Transformer的语言模型。我们根据LLMs的训练范式将技术进行分类：传统的微调范式和提示范式。对于每个范式，我们总结了生成个体预测的局部解释和整体模型知识的全局解释的目标和主要方法。我们还讨论了评估生成解释的度量标准，并讨论了如何利用解释来调试模型和提高性能。

    Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, 
    
[^56]: 图像劫持：对抗性图像能在运行时控制生成模型

    Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])

    [http://arxiv.org/abs/2309.00236](http://arxiv.org/abs/2309.00236)

    本研究发现对抗性图像能够在运行时控制生成模型，并提出了通用的方法来创建图像劫持。通过研究三种攻击类型，我们发现这些攻击对最新的视觉语言模型具有高达90％以上的成功率。该研究引发了对基础模型安全性的严重担忧。

    

    基础模型是否能够免受恶意行为者的攻击？本文研究了视觉语言模型（VLM）的图像输入。我们发现了图像劫持，即能够在运行时控制生成模型的对抗性图像。我们引入了一种名为“行为匹配”的通用方法来创建图像劫持，并用它来探索三种类型的攻击：具体字符串攻击可以生成任意被攻击者选择的输出；泄露上下文攻击可以将上下文窗口中的信息泄露到输出中；越狱攻击可以绕过模型的安全训练。我们对基于CLIP和LLaMA-2的最新VLM模型LLaVA-2进行了这些攻击的研究，并发现我们所有的攻击类型成功率均在90％以上。而且，我们的攻击是自动化的，只需要对图像进行小的扰动。这些发现对基础模型的安全性提出了严重的担忧。如果图像劫持与CIFAR-10中的对抗性样本一样难以防御，那么可能需要很多年才能找到解决方案。

    Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
    
[^57]: 深度视频编码控制

    Deep Video Codec Control. (arXiv:2308.16215v1 [eess.IV])

    [http://arxiv.org/abs/2308.16215](http://arxiv.org/abs/2308.16215)

    本文提出了第一个端到端可学习的深度视频编码控制方法，同时考虑了带宽限制和下游视觉性能，并在不破坏现有标准化的情况下实现了保护深度视觉模型的目标。

    

    丢失率视频压缩通常用于传输和存储视频数据。尽管存在进阶（神经）压缩方法，但统一视频编码器（如H.264或H.265）仍然是事实上的标准。在面对动态网络带宽条件的视频传输中，视频编码器需要适应非常不同的压缩强度。速率控制模块增强编解码器的压缩能力，以满足带宽限制并尽量减少视频失真。然而，标准视频编码器及其速率控制模块是为了最小化人类质量评估而开发的，却没有考虑保护深度视觉模型的下游性能。在本文中，我们提出了第一个端到端可学习的深度视频编码控制方法，考虑了带宽限制和下游视觉性能，并不破坏现有的标准化。我们针对两个常见的视觉任务（语义分割...

    Lossy video compression is commonly used when transmitting and storing video data. Unified video codecs (e.g., H.264 or H.265) remain the \emph{de facto} standard, despite the availability of advanced (neural) compression approaches. Transmitting videos in the face of dynamic network bandwidth conditions requires video codecs to adapt to vastly different compression strengths. Rate control modules augment the codec's compression such that bandwidth constraints are satisfied and video distortion is minimized. While, both standard video codes and their rate control modules are developed to minimize video distortion w.r.t. human quality assessment, preserving the downstream performance of deep vision models is not considered. In this paper, we present the first end-to-end learnable deep video codec control considering both bandwidth constraints and downstream vision performance, while not breaking existing standardization. We demonstrate for two common vision tasks (semantic segmentation 
    
[^58]: LLaSM: 大型语言和语音模型

    LLaSM: Large Language and Speech Model. (arXiv:2308.15930v1 [cs.CL])

    [http://arxiv.org/abs/2308.15930](http://arxiv.org/abs/2308.15930)

    LLaSM是一个大型语言和语音模型，具有跨模态对话能力，通过遵循语音和语言指令，提供了一种方便自然的人机交互方式。

    

    最近，多模态大型语言模型引起了广泛关注。然而，大部分研究都集中在视觉-语言多模态模型上，提供了强大的能力来遵循视觉和语言指令。然而，我们认为语音也是人类与世界互动的重要方式。因此，对于一个通用的助手来说，能够遵循多模态语音和语言指令是至关重要的。在这项工作中，我们提出了大型语言和语音模型（LLaSM）。LLaSM是一个端到端训练的大型多模态语音-语言模型，具有跨模态对话能力，能够遵循语音和语言指令。我们的初步实验表明，LLaSM展示了一种更方便自然的人机交互方式。为了支持研究，我们还发布了一个大型的语音指令数据集LLaSM-Audio-Instructions。代码和演示可在https://github.com/LinkSoul-AI/LLaSM和ht上查看

    Multi-modal large language models have garnered significant interest recently. Though, most of the works focus on vision-language multi-modal models providing strong capabilities in following vision-and-language instructions. However, we claim that speech is also an important modality through which humans interact with the world. Hence, it is crucial for a general-purpose assistant to be able to follow multi-modal speech-and-language instructions. In this work, we propose Large Language and Speech Model (LLaSM). LLaSM is an end-to-end trained large multi-modal speech-language model with cross-modal conversational abilities, capable of following speech-and-language instructions. Our early experiments show that LLaSM demonstrates a more convenient and natural way for humans to interact with artificial intelligence. Specifically, we also release a large Speech Instruction Following dataset LLaSM-Audio-Instructions. Code and demo are available at https://github.com/LinkSoul-AI/LLaSM and ht
    
[^59]: 阐明扩散模型中的曝光偏差问题

    Elucidating the Exposure Bias in Diffusion Models. (arXiv:2308.15321v1 [cs.LG])

    [http://arxiv.org/abs/2308.15321](http://arxiv.org/abs/2308.15321)

    本文系统地研究了扩散模型中的曝光偏差问题，并提出了一种名为Epsilon Scaling的免训练方法来减轻这一问题。实验结果验证了该方法的有效性。

    

    扩散模型展示了令人印象深刻的生成能力，但它们的“曝光偏差”问题，即训练和采样之间的输入不匹配，缺乏深入探索。本文通过首先对采样分布进行分析建模，然后将每个采样步骤的预测误差归因为曝光偏差问题的根本原因，系统地研究了扩散模型中的曝光偏差问题。此外，我们讨论了解决这个问题的潜在方法，并提出了一个直观的度量标准。除了阐明曝光偏差问题，我们提出了一种简单但有效的免训练方法，称为Epsilon Scaling，以减轻曝光偏差。我们展示了Epsilon Scaling通过缩小网络输出（Epsilon）明确地将采样轨迹移近训练阶段学习到的向量场，从而减轻了训练和采样之间的输入不匹配。在各种扩散框架上进行了实验。

    Diffusion models have demonstrated impressive generative capabilities, but their 'exposure bias' problem, described as the input mismatch between training and sampling, lacks in-depth exploration. In this paper, we systematically investigate the exposure bias problem in diffusion models by first analytically modelling the sampling distribution, based on which we then attribute the prediction error at each sampling step as the root cause of the exposure bias issue. Furthermore, we discuss potential solutions to this issue and propose an intuitive metric for it. Along with the elucidation of exposure bias, we propose a simple, yet effective, training-free method called Epsilon Scaling to alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the sampling trajectory closer to the vector field learned in the training phase by scaling down the network output (Epsilon), mitigating the input mismatch between training and sampling. Experiments on various diffusion framework
    
[^60]: 3D-MuPPET: 3D多鸽姿态估计与跟踪

    3D-MuPPET: 3D Multi-Pigeon Pose Estimation and Tracking. (arXiv:2308.15316v1 [cs.CV] CROSS LISTED)

    [http://arxiv.org/abs/2308.15316](http://arxiv.org/abs/2308.15316)

    3D-MuPPET是一个用于估计和跟踪多只鸽子三维姿势的框架，通过多视角实时推测2D关键点并将其三角化到3D空间，同时使用动态匹配和2D跟踪器维持对应关系。相比最先进的3D姿势估计器，具有可比的准确性。该框架还能在使用单只鸽子数据训练的情况下应用于多只鸽子数据，简化领域转换。

    

    近年来，对动物姿势跟踪的无标记方法已有所发展，但仍缺乏用于追踪大规模动物群体的三维框架和基准。为了弥补文献中的这一空白，我们提出了3D-MuPPET，一个使用多视角实时估计和跟踪多达10只鸽子的三维姿势的框架。我们训练了一个姿势估计器，用于推测多只鸽子的2D关键点和边界框，然后将关键点三角化到3D空间。对于匹配对应关系，我们首先动态地将2D检测结果与第一帧中的全局身份进行匹配，然后使用2D跟踪器在后续帧中维持对应关系。我们达到了与最先进的3D姿势估计器相当的准确性，即均方根误差（RMSE）和正确关键点百分比（PCK）。我们还展示了一个新颖的用例，即我们使用单只鸽子的数据训练模型并在包含多只鸽子的数据上得到可比较的结果，这可以简化对新场景的领域转换。

    Markerless methods for animal posture tracking have been developing recently, but frameworks and benchmarks for tracking large animal groups in 3D are still lacking. To overcome this gap in the literature, we present 3D-MuPPET, a framework to estimate and track 3D poses of up to 10 pigeons at interactive speed using multiple-views. We train a pose estimator to infer 2D keypoints and bounding boxes of multiple pigeons, then triangulate the keypoints to 3D. For correspondence matching, we first dynamically match 2D detections to global identities in the first frame, then use a 2D tracker to maintain correspondences accross views in subsequent frames. We achieve comparable accuracy to a state of the art 3D pose estimator for Root Mean Square Error (RMSE) and Percentage of Correct Keypoints (PCK). We also showcase a novel use case where our model trained with data of single pigeons provides comparable results on data containing multiple pigeons. This can simplify the domain shift to new sp
    
[^61]: 快速前馈网络

    Fast Feedforward Networks. (arXiv:2308.14711v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14711](http://arxiv.org/abs/2308.14711)

    快速前馈网络是一种对于前馈网络的改进架构，能够以更快的速度进行推理，并具有比专家混合模型更好的训练性能。在视觉转换器中，它们可以仅使用1%的层神经元进行推理，同时保持94.2%的预测性能。

    

    我们通过引入快速前馈(FFF)架构，打破了层大小与推理成本之间的线性关系，这是一种对于前馈网络的对数时间替代方法。我们证明FFF比前馈网络快高达220倍，比专家混合网络快高达6倍，并且由于无噪声条件执行而表现出比专家混合模型更好的训练性能。将FFF推到极限，我们展示了在视觉转换器中，它们可仅使用1%的层神经元进行推理，同时保持94.2%的预测性能。

    We break the linear link between the layer size and its inference cost by introducing the fast feedforward (FFF) architecture, a log-time alternative to feedforward networks. We demonstrate that FFFs are up to 220x faster than feedforward networks, up to 6x faster than mixture-of-experts networks, and exhibit better training properties than mixtures of experts thanks to noiseless conditional execution. Pushing FFFs to the limit, we show that they can use as little as 1% of layer neurons for inference in vision transformers while preserving 94.2% of predictive performance.
    
[^62]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^63]: 探索拉舒蒙集合有助于医疗数据的解释

    Exploration of Rashomon Set Assists Explanations for Medical Data. (arXiv:2308.11446v1 [cs.LG])

    [http://arxiv.org/abs/2308.11446](http://arxiv.org/abs/2308.11446)

    本文提出了一种新的过程来探索和分析医疗数据中的拉舒蒙集合模型，从而超越传统单一模型选择的方法，并通过引入"拉舒蒙检测"算法识别出集合中最不同的模型。

    

    机器学习建模过程通常以选择最大化某个性能指标的单一模型作为最终结果。然而，这种方法会导致对稍微差一些的模型进行更深入的分析被忽视。尤其在医疗和健康研究中，目标不仅仅是预测，还包括产生有价值的洞察，仅仅依赖性能指标可能会导致误导或不完整的结论。当处理一组性能接近最优的模型集合时，即所谓的"拉舒蒙集合"，这个问题尤为突出。这样的集合可能包含描述数据的不同方式的模型，需要进行全面的分析。本文引入了一种新的过程来探索拉舒蒙集合模型，扩展了传统建模方法。核心是通过引入的"拉舒蒙检测"算法来识别拉舒蒙集合中最不同的模型。

    The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorit
    
[^64]: 图神经网络在推荐中的表达能力有多强？

    How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])

    [http://arxiv.org/abs/2308.11127](http://arxiv.org/abs/2308.11127)

    本文对图神经网络在推荐中的表达能力进行了理论分析，发现现有的表达能力度量标准可能无法有效评估模型在推荐中的能力，提出了一个全面的理论分析方法。

    

    图神经网络（GNNs）在各种图学习任务中展示了优越的性能，包括利用图中的用户-物品协作过滤信号进行推荐。然而，尽管它们在最先进的推荐模型中的经验有效性，但对于它们的能力的理论表述非常稀少。最近的研究探讨了GNNs的一般表达能力，证明了消息传递GNNs至多与Weisfeiler-Lehman测试一样强大，并且与随机节点初始化相结合的GNNs是通用的。然而，GNNs的“表达能力”概念仍然定义模糊。大多数现有的工作采用图同构测试作为表达能力的度量标准，但这种图级任务可能不能有效评估模型在推荐中区分不同接近程度节点的能力。在本文中，我们对GNNs在推荐中的表达能力进行了全面的理论分析。

    Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
    
[^65]: Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG]) 该论文标题已翻译：二元强化学习。

    Dyadic Reinforcement Learning. (arXiv:2308.07843v1 [cs.LG])

    [http://arxiv.org/abs/2308.07843](http://arxiv.org/abs/2308.07843)

    该论文介绍了一个称为二元强化学习的在线算法，用于根据上下文因素和目标人与其照顾伴侣的过去反馈，个性化地提供干预措施。该算法是贝叶斯和层次的，并通过模拟展示了良好的实证效果。

    

    移动医疗旨在通过在个人日常生活中提供干预来提高健康结果。照顾伴侣和社会支持网络的参与经常在帮助个人管理繁重的医疗条件方面起着关键作用。这为移动医疗提供了机会，设计针对二元关系——目标人和其照顾伴侣之间关系——以提高社会支持的干预措施。在本文中，我们开发了二元强化学习（Dyadic RL），这是一种基于环境因素和目标人及其照顾伴侣的过去反馈个性化干预措施的在线强化学习算法。在这里，多组干预措施影响着二元关系在多个时间间隔内。开发的二元强化学习是贝叶斯和层次的。我们正式介绍了问题设定，开发了二元强化学习并确定了遗憾边界。通过模拟，我们展示了二元强化学习的实证效果。

    Mobile health aims to enhance health outcomes by delivering interventions to individuals as they go about their daily life. The involvement of care partners and social support networks often proves crucial in helping individuals managing burdensome medical conditions. This presents opportunities in mobile health to design interventions that target the dyadic relationship -- the relationship between a target person and their care partner -- with the aim of enhancing social support. In this paper, we develop dyadic RL, an online reinforcement learning algorithm designed to personalize intervention delivery based on contextual factors and past responses of a target person and their care partner. Here, multiple sets of interventions impact the dyad across multiple time intervals. The developed dyadic RL is Bayesian and hierarchical. We formally introduce the problem setup, develop dyadic RL and establish a regret bound. We demonstrate dyadic RL's empirical performance through simulation st
    
[^66]: 学习优化LSM树：面向动态工作负载的基于强化学习的键值存储系统

    Learning to Optimize LSM-trees: Towards A Reinforcement Learning based Key-Value Store for Dynamic Workloads. (arXiv:2308.07013v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2308.07013](http://arxiv.org/abs/2308.07013)

    RusKey是一个基于强化学习的键值存储系统，通过在线编排LSM树结构来实现在动态工作负载下的稳健性能，并引入了FLSM树设计来高效转换不同压实策略，无需先验工作负载知识。

    

    LSM树被广泛应用于键值存储的存储后端。然而，在动态工作负载下优化系统性能的问题在以前的工作中尚未得到充分研究或评估。为了填补这一空白，我们提出了RusKey，一个具有以下新特性的键值存储：（1）RusKey是第一个尝试在线编排LSM树结构，以实现在动态工作负载情况下的稳健性能；（2）RusKey是第一个使用强化学习（RL）来引导LSM树转换的研究；（3）RusKey包括一种新的LSM树设计，称为FLSM树，用于在不同的压实策略之间实现高效转换，这是动态键值存储的瓶颈。我们通过理论分析证明了新设计的优越性；（4）与最先进的技术相比，RusKey不需要先验的工作负载知识进行系统调整。实验证明，RusKey在多样的工作负载中表现出强大的性能稳健性，最高可达4倍的性能。

    LSM-trees are widely adopted as the storage backend of key-value stores. However, optimizing the system performance under dynamic workloads has not been sufficiently studied or evaluated in previous work. To fill the gap, we present RusKey, a key-value store with the following new features: (1) RusKey is a first attempt to orchestrate LSM-tree structures online to enable robust performance under the context of dynamic workloads; (2) RusKey is the first study to use Reinforcement Learning (RL) to guide LSM-tree transformations; (3) RusKey includes a new LSM-tree design, named FLSM-tree, for an efficient transition between different compaction policies -- the bottleneck of dynamic key-value stores. We justify the superiority of the new design with theoretical analysis; (4) RusKey requires no prior workload knowledge for system adjustment, in contrast to state-of-the-art techniques. Experiments show that RusKey exhibits strong performance robustness in diverse workloads, achieving up to 4
    
[^67]: 使用概念器进行变点检测

    Change Point Detection With Conceptors. (arXiv:2308.06213v1 [stat.ML])

    [http://arxiv.org/abs/2308.06213](http://arxiv.org/abs/2308.06213)

    我们提出了一种使用概念器矩阵进行变点检测的方法，通过学习时间序列中的特征动态，并利用单变量量化来识别变点。该方法在条件和无条件的变点检测问题上进行了测试，可以提供潜在的需要进一步研究的感兴趣位置。

    

    离线变点检测旨在识别时间序列中数据生成过程发生变化的点。对于单变量独立同分布数据，这个问题已经得到了较好的研究，但是随着维度和时间依赖性的增加，变得具有挑战性。针对至多一个变点的问题，我们提出使用概念器矩阵来学习时间序列中指定训练窗口的特征动态。相关的随机递归神经网络作为数据的特征提取器，并且通过计算特征化与代表性概念器矩阵所张成空间之间的距离的单变量量化来识别变点。这种模型无关的方法可以提示可能需要进一步研究的感兴趣的位置。我们证明，在温和的假设下，该方法提供了真实变点的一致估计，并通过对原始数据进行移动块自助法产生统计量的分位数估计。该方法在条件和无条件的变点检测问题上进行了测试。

    Offline change point detection seeks to identify points in a time series where the data generating process changes. This problem is well studied for univariate i.i.d. data, but becomes challenging with increasing dimension and temporal dependence. For the at most one change point problem, we propose the use of a conceptor matrix to learn the characteristic dynamics of a specified training window in a time series. The associated random recurrent neural network acts as a featurizer of the data, and change points are identified from a univariate quantification of the distance between the featurization and the space spanned by a representative conceptor matrix. This model agnostic method can suggest potential locations of interest that warrant further study. We prove that, under mild assumptions, the method provides a consistent estimate of the true change point, and quantile estimates for statistics are produced via a moving block bootstrap of the original data. The method is tested on si
    
[^68]: 基于物理信息神经网络的压缩机叶栅流场调查

    Investigation of compressor cascade flow based on physics-informed neural networks. (arXiv:2308.04501v1 [cs.LG])

    [http://arxiv.org/abs/2308.04501](http://arxiv.org/abs/2308.04501)

    本研究首次使用物理信息神经网络（PINNs）方法来预测压缩机叶栅的流场，结果表明PINNs在预测精度和处理反向问题方面比传统CFD方法具有明显优势。

    

    在本研究中，我们首次使用新兴的物理信息神经网络（PINNs）方法来预测压缩机叶栅的流场。该方法在一个二维问题上进行演示，同时结合正向和反向问题中的Navier-Stokes方程。在正向问题中，PINNs能够有效预测压缩机的流场。与深度神经网络（DNNs）相比，PINNs模型融合了相关量之间的物理关系，从而得到更精确的预测结果。在没有部分边界条件的情况下，PINNs在处理反向问题时显示出显著优势。PINNs仅基于部分速度向量和壁压信息成功重构了压缩机叶栅的流场。该研究提供了有力的证据，证明PINNs为涡轮机械设计人员提供了一种有前景的替代方法，取代了当前主导的CFD方法。

    In this study, we utilize the emerging Physics Informed Neural Networks (PINNs) approach for the first time to predict the flow field of a compressor cascade. The approach is demonstrated on a two-dimensional problem, incorporating Navier-Stokes equations in both the forward and inverse problems. In the forward problem, PINNs effectively predict the flow field of the compressor. The key advantage over Deep Neural Networks (DNNs) is that the PINNs model incorporates a physical relationship between the relevant quantities, resulting in more precise predictions. PINNs show obvious advantages over the traditional CFD approaches when dealing with inverse problems in the absence of partial boundary conditions. PINNs successfully reconstruct the flow field of the compressor cascade solely based on partial velocity vectors and wall pressure information. This research provides compelling evidence that PINNs offer turbomachinery designers a promising alternative to the current dominant CFD metho
    
[^69]: 用一种时间对称的深度学习方法提升细胞跟踪能力

    Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach. (arXiv:2308.03887v1 [eess.IV])

    [http://arxiv.org/abs/2308.03887](http://arxiv.org/abs/2308.03887)

    本论文提出了一种使用时间对称的深度学习方法来提升细胞跟踪的准确性。该方法不依赖于连续帧跟踪，而是基于细胞的时空邻域进行跟踪，具有学习细胞运动模式的能力，并能处理具有严重伪影的大量视频帧。

    

    使用视频显微镜记录准确跟踪活细胞仍然是目前流行的最先进图像处理技术方法的一个具有挑战性的任务。近年来，已有几个现有和新的应用尝试将基于深度学习的框架整合到该任务中，但大部分仍然严重依赖于嵌入其架构或其他前提条件中的连续帧跟踪，从而限制了广义学习。为了解决这个问题，我们旨在开发一种新的基于深度学习的跟踪方法，该方法仅依赖于细胞可以根据其时空邻域进行跟踪的假设，而非仅限于连续帧。所提出的方法的额外优点是细胞的运动模式可以完全由预测器在没有任何先验假设的情况下学习，并且具有处理大量具有严重伪影的视频帧的潜力。

    The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated thro
    
[^70]: 无监督评估度量用于实践和自动领域自适应的研究

    A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation. (arXiv:2308.00287v1 [cs.CV])

    [http://arxiv.org/abs/2308.00287](http://arxiv.org/abs/2308.00287)

    这项研究关注于无监督领域自适应中的评估度量。通过将源准确率纳入度量中，并使用一种新的MLP分类器进行改进，该研究解决了原有度量的不足之处，并将其与数据增强相结合，改进了模型的质量评估。

    

    无监督领域自适应（UDA）方法可以在没有标签的情况下将模型转移到目标领域。然而，这些方法需要一个标记的目标验证集用于超参数调优和模型选择。本文旨在寻找一种能够评估转移模型质量的评估度量，而无需访问目标验证标签。我们首先基于模型预测的互信息的度量。通过实证分析，我们发现了这个度量的三个普遍问题：1）它没有考虑源结构；2）它容易受到攻击；3）它无法检测到由于源和目标特征过度对齐导致的负迁移。为了解决前两个问题，我们将源准确率纳入度量中，并使用一种在训练过程中保持独立的新的多层感知器（MLP）分类器，从而显著改进了结果。为了解决最后一个问题，我们将这个增强的度量与数据增强相结合，得到一个n

    Unsupervised domain adaptation (UDA) methods facilitate the transfer of models to target domains without labels. However, these methods necessitate a labeled target validation set for hyper-parameter tuning and model selection. In this paper, we aim to find an evaluation metric capable of assessing the quality of a transferred model without access to target validation labels. We begin with the metric based on mutual information of the model prediction. Through empirical analysis, we identify three prevalent issues with this metric: 1) It does not account for the source structure. 2) It can be easily attacked. 3) It fails to detect negative transfer caused by the over-alignment of source and target features. To address the first two issues, we incorporate source accuracy into the metric and employ a new MLP classifier that is held out during training, significantly improving the result. To tackle the final issue, we integrate this enhanced metric with data augmentation, resulting in a n
    
[^71]: 固定积分神经网络

    Fixed Integral Neural Networks. (arXiv:2307.14439v1 [cs.LG])

    [http://arxiv.org/abs/2307.14439](http://arxiv.org/abs/2307.14439)

    本文介绍了一种方法来通过神经网络表示学习函数的解析积分，具有计算精确积分的能力，并能将约束直接应用于积分，而且还介绍了将学习函数约束为正的方法以及相关应用。

    

    将学习函数通过神经网络进行积分是非常有用的，但是这种积分通常是通过数值方法来计算的，因为解析计算积分过程复杂，尤其是对于神经网络这样的学习函数。本文介绍了一种表示学习函数 $f$ 解析积分的方法。这允许精确计算神经网络的积分，并且通过将约束直接应用于积分来对约束神经网络进行参数化。关键的是，我们还介绍了一种将 $f$ 约束为正的方法，这是许多应用（例如概率分布、距离度量等）所必需的条件。最后，我们介绍了几个可以利用我们的固定积分神经网络（FINN）的应用领域。

    It is often useful to perform integration over learned functions represented by neural networks. However, this integration is usually performed numerically, as analytical integration over learned functions (especially neural networks) is generally viewed as intractable. In this work, we present a method for representing the analytical integral of a learned function $f$. This allows the exact integral of a neural network to be computed, and enables constrained neural networks to be parametrised by applying constraints directly to the integral. Crucially, we also introduce a method to constrain $f$ to be positive, a necessary condition for many applications (e.g. probability distributions, distance metrics, etc). Finally, we introduce several applications where our fixed-integral neural network (FINN) can be utilised.
    
[^72]: 机器学习模型的局部鲁棒性的高效估计

    Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])

    [http://arxiv.org/abs/2307.13885](http://arxiv.org/abs/2307.13885)

    本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    

    机器学习模型通常需要对噪声输入数据具有鲁棒性。现实世界中的噪声（通常是随机的）对模型预测的影响可以通过模型的局部鲁棒性来捕捉，即在输入周围的局部区域内模型预测的一致性。然而，基于蒙特卡罗采样的计算局部鲁棒性的朴素方法在统计上是低效的，对于大规模应用而言计算成本高昂。在这项工作中，我们通过局部线性函数逼近和多元正态CDF开发了首个分析估计器，以高效计算多类别判别模型的局部鲁棒性。通过这些估计器的推导，我们展示了局部鲁棒性与随机平滑和softmax概率等概念的联系。我们还通过实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
    
[^73]: RED CoMETS: 一种用于符号化表示的多变量时间序列的集成分类器

    RED CoMETS: An ensemble classifier for symbolically represented multivariate time series. (arXiv:2307.13679v1 [cs.LG])

    [http://arxiv.org/abs/2307.13679](http://arxiv.org/abs/2307.13679)

    本文介绍了一种名为RED CoMETS的集成分类器，用于处理符号化表示的多变量时间序列数据。它在多变量设置中展现出竞争力的准确性，并在'HandMovementDirection'数据集上实现了最高的报告准确性。

    

    多变量时间序列分类是一个快速发展的研究领域，可在金融、医疗、工程等实际应用中使用。多变量时间序列数据的分类复杂性来自于其高维度、时间依赖性和长度不一致性。本文介绍了一种名为RED CoMETS（Random Enhanced Co-eye for Multivariate Time Series）的新型集成分类器，它解决了这些挑战。RED CoMETS基于Co-eye的成功，并将其能力扩展到处理多变量数据。使用UCR档案中的基准数据集对RED CoMETS的性能进行评估，在多变量设置中与最先进的技术相比，它显示出竞争力的准确性。值得注意的是，它在文献中对于'HandMovementDirection'数据集实现了最高的报告准确性。此外，该方法显著地改进了传统的Co-eye方法。

    Multivariate time series classification is a rapidly growing research field with practical applications in finance, healthcare, engineering, and more. The complexity of classifying multivariate time series data arises from its high dimensionality, temporal dependencies, and varying lengths. This paper introduces a novel ensemble classifier called RED CoMETS (Random Enhanced Co-eye for Multivariate Time Series), which addresses these challenges. RED CoMETS builds upon the success of Co-eye, an ensemble classifier specifically designed for symbolically represented univariate time series, and extends its capabilities to handle multivariate data. The performance of RED CoMETS is evaluated on benchmark datasets from the UCR archive, where it demonstrates competitive accuracy when compared to state-of-the-art techniques in multivariate settings. Notably, it achieves the highest reported accuracy in the literature for the 'HandMovementDirection' dataset. Moreover, the proposed method signific
    
[^74]: 通过随机滤波和模式识别强化基于POD的反应扩散复杂网络模型简化技术

    Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])

    [http://arxiv.org/abs/2307.09762](http://arxiv.org/abs/2307.09762)

    该论文提出了一种算法框架，通过将模式识别和随机滤波理论的技术结合起来，强化了基于POD的反应扩散复杂网络模型简化技术，在受扰动输入的情况下提高了代理模型的准确性。

    

    复杂网络被用于建模许多现实世界系统，然而这些系统的维度使得其分析变得困难。在这种情况下，可以使用POD等降维技术。然而，这些模型容易受输入数据扰动的影响。我们提出了一种算法框架，将模式识别和随机滤波理论的技术结合起来，以增强这些模型的输出。研究结果表明，我们的方法可以在受扰动输入的情况下提高代理模型的准确性。深度神经网络(DNNs)容易受到对抗性攻击，然而最近的研究发现，神经常微分方程(ODEs)在特定应用中表现出鲁棒性。我们将我们的算法框架与基于神经ODE的方法进行了基准比较。

    Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
    
[^75]: vONTSS：基于vMF和最优传输的半监督神经主题建模

    vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])

    [http://arxiv.org/abs/2307.01226](http://arxiv.org/abs/2307.01226)

    vONTSS是一种基于vMF和最优传输的半监督神经主题建模方法，它在分类准确率和多样性方面优于其他方法，并且支持无监督主题建模。实验证明，vONTSS比最近的NTM更快。

    

    最近，受变分自编码器启发的神经主题模型（NTM）引起了很多研究兴趣，然而，由于整合人类知识的挑战，这些方法在实际应用中受到了限制。本研究提出了一种半监督神经主题建模方法vONTSS，该方法利用基于von Mises-Fisher（vMF）的变分自编码器和最优传输。在半监督设置中，当提供每个主题的少量关键词时，vONTSS生成潜在主题并优化主题-关键词质量和主题分类。实验证明，vONTSS在分类准确率和多样性方面优于现有的半监督主题建模方法。vONTSS还支持无监督主题建模。定量和定性实验证明，vONTSS在无监督设置下在多个方面优于最近的NTM：vONTSS在基准数据集上发现高度聚类和连贯的主题。它也比现有-手法快得多。

    Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
    
[^76]: 使用生成对抗网络生成无监督文本嵌入空间用于文本合成

    Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis. (arXiv:2306.17181v1 [cs.CL])

    [http://arxiv.org/abs/2306.17181](http://arxiv.org/abs/2306.17181)

    本论文提出了一种使用生成对抗网络（GAN）生成连续文本嵌入空间的方法（TESGAN），以解决传统GAN在自然语言生成中的限制。这种方法通过引入连续的文本嵌入空间取代离散的标记，使得生成器在通过反向传播更新梯度时更加有效。

    

    生成对抗网络（GAN）是一种用于数据合成的模型，通过生成器和判别器的竞争来创建逼真的数据。尽管GAN在图像合成方面得到了广泛研究，但在自然语言生成方面存在固有的限制。因为自然语言由离散的标记组成，生成器在通过反向传播更新梯度时遇到困难；因此，大多数文本-GAN研究使用奖励系统以随机标记为基础生成句子。因此，先前研究中的生成器在对抗训练之前以自回归方式进行预训练，导致合成的句子重复训练数据。在本文中，我们使用类似原始GAN的框架来合成句子。更具体地说，我们提出了文本嵌入空间生成对抗网络（TESGAN），它生成连续的文本嵌入空间来解决梯度反向传播的问题。

    Generative Adversarial Networks (GAN) is a model for data synthesis, which creates plausible data through the competition of generator and discriminator. Although GAN application to image synthesis is extensively studied, it has inherent limitations to natural language generation. Because natural language is composed of discrete tokens, a generator has difficulty updating its gradient through backpropagation; therefore, most text-GAN studies generate sentences starting with a random token based on a reward system. Thus, the generators of previous studies are pre-trained in an autoregressive way before adversarial training, causing data memorization that synthesized sentences reproduce the training data. In this paper, we synthesize sentences using a framework similar to the original GAN. More specifically, we propose Text Embedding Space Generative Adversarial Networks (TESGAN) which generate continuous text embedding spaces instead of discrete tokens to solve the gradient backpropagat
    
[^77]: 基于潜在扩散模型的文本驱动Foley音效生成

    Text-Driven Foley Sound Generation With Latent Diffusion Model. (arXiv:2306.10359v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2306.10359](http://arxiv.org/abs/2306.10359)

    本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。我们通过迁移学习对系统进行微调，并引入可训练的层来改善文本嵌入，同时也改进了生成的波形。

    

    Foley音效生成旨在为多媒体内容生成背景音效。先前的模型通常使用大量有标签的开发集作为输入（例如，单个数字或one-hot向量）。本文提出了一种基于扩散模型的Foley音效生成系统，可进行文本条件的生成。为了缓解数据稀缺问题，我们的模型首先使用大规模数据集进行预训练，然后通过对比语言-音频配对（CLAP）技术进行迁移学习来对该任务进行微调。我们观察到，文本编码器提取的特征嵌入可以显著影响生成模型的性能。因此，我们在编码器之后引入可训练的层来改善编码器产生的文本嵌入。此外，我们通过同时生成多个候选音频片段并选择最佳片段来进一步改进生成的波形，最佳片段是根据嵌入之间相似性得分确定的。

    Foley sound generation aims to synthesise the background sound for multimedia content. Previous models usually employ a large development set with labels as input (e.g., single numbers or one-hot vector). In this work, we propose a diffusion model based system for Foley sound generation with text conditions. To alleviate the data scarcity issue, our model is initially pre-trained with large-scale datasets and fine-tuned to this task via transfer learning using the contrastive language-audio pertaining (CLAP) technique. We have observed that the feature embedding extracted by the text encoder can significantly affect the performance of the generation model. Hence, we introduce a trainable layer after the encoder to improve the text embedding produced by the encoder. In addition, we further refine the generated waveform by generating multiple candidate audio clips simultaneously and selecting the best one, which is determined in terms of the similarity score between the embedding of the 
    
[^78]: 公平因果特征选择

    Fair Causal Feature Selection. (arXiv:2306.10336v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10336](http://arxiv.org/abs/2306.10336)

    提出了一种公平因果特征选择算法FairCFS，通过构建局部因果图识别类和敏感变量的马尔可夫毯，阻止敏感信息的传输，从而实现公平的特征选择。经过大量实验证实，FairCFS在特征选择准确性方面与八种最先进的算法相媲美，同时具有更优越的公平性。

    

    最近，对于分类决策任务的公平特征选择引起了研究者的重视。然而，现有的公平特征选择算法在提供特征和敏感属性之间因果关系的全面解释方面存在不足，可能影响公平特征识别的准确性。为了解决这个问题，我们提出了一种公平因果特征选择算法，称为FairCFS。具体而言，FairCFS构建了一个局部因果图，识别了类和敏感变量的马尔可夫毯以阻止敏感信息的传输，以选择公平因果特征。对七个公共真实数据集进行的大量实验证实，FairCFS在与八种最先进的特征选择算法相比具有可比较的准确性，同时呈现出更优越的公平性。

    Fair feature selection for classification decision tasks has recently garnered significant attention from researchers. However, existing fair feature selection algorithms fall short of providing a full explanation of the causal relationship between features and sensitive attributes, potentially impacting the accuracy of fair feature identification. To address this issue, we propose a Fair Causal Feature Selection algorithm, called FairCFS. Specifically, FairCFS constructs a localized causal graph that identifies the Markov blankets of class and sensitive variables, to block the transmission of sensitive information for selecting fair causal features. Extensive experiments on seven public real-world datasets validate that FairCFS has comparable accuracy compared to eight state-of-the-art feature selection algorithms, while presenting more superior fairness.
    
[^79]: 你的房间不是私密的：关于强化学习的梯度反转攻击

    Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning. (arXiv:2306.09273v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.09273](http://arxiv.org/abs/2306.09273)

    这篇论文提出了一种针对值函数算法和梯度算法的攻击方法，利用梯度反转重建状态、动作和监督信号，以解决嵌入式人工智能中的隐私泄露问题。

    

    嵌入式人工智能的显著发展吸引了人们的极大关注，该技术使得机器人可以在虚拟环境中导航、感知和互动。由于计算机视觉和大型语言模型方面的显著进展，隐私问题在嵌入式人工智能领域变得至关重要，因为机器人可以访问大量个人信息。然而，关于强化学习算法中的隐私泄露问题，尤其是关于值函数算法和梯度算法的问题，在研究中尚未得到充分考虑。本文旨在通过提出一种攻击值函数算法和梯度算法的方法，利用梯度反转重建状态、动作和监督信号，来解决这一问题。选择使用梯度进行攻击是因为常用的联邦学习技术仅利用基于私人用户数据计算的梯度来优化模型，而不存储或传输用户数据。

    The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervision signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or trans
    
[^80]: 零样本无线室内导航通过物理信息强化学习

    Zero-Shot Wireless Indoor Navigation through Physics-Informed Reinforcement Learning. (arXiv:2306.06766v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.06766](http://arxiv.org/abs/2306.06766)

    该论文提出了一种基于物理信息强化学习的零样本无线室内导航方法，通过样本高效学习和零样本泛化来提高导航效率。

    

    针对室内机器人导航利用无线信号的不断关注，该论文提出了一种基于物理信息强化学习（PIRL）的方法，以实现高效的样本学习和零样本泛化。相对于基于启发式方法的传统方法，基于射频传播的方法直观且能够适应简单的场景，但在复杂环境下难以导航。而基于端到端深度强化学习（RL）的方法可以通过使用先进的计算机技术来探索整个状态空间，在面对复杂的无线环境时表现出令人惊讶的性能。然而，这种方法需要大量的训练样本，而且得到的策略在未进行微调的情况下无法在训练阶段未见过的新场景中有效导航。

    The growing focus on indoor robot navigation utilizing wireless signals has stemmed from the capability of these signals to capture high-resolution angular and temporal measurements. Prior heuristic-based methods, based on radio frequency propagation, are intuitive and generalizable across simple scenarios, yet fail to navigate in complex environments. On the other hand, end-to-end (e2e) deep reinforcement learning (RL), powered by advanced computing machinery, can explore the entire state space, delivering surprising performance when facing complex wireless environments. However, the price to pay is the astronomical amount of training samples, and the resulting policy, without fine-tuning (zero-shot), is unable to navigate efficiently in new scenarios unseen in the training phase. To equip the navigation agent with sample-efficient learning and {zero-shot} generalization, this work proposes a novel physics-informed RL (PIRL) where a distance-to-target-based cost (standard in e2e) is a
    
[^81]: 基于贝叶斯抽样算法的在线自适应流量实验的实用批次评估

    An Evaluation on Practical Batch Bayesian Sampling Algorithms for Online Adaptive Traffic Experimentation. (arXiv:2305.14704v1 [cs.LG])

    [http://arxiv.org/abs/2305.14704](http://arxiv.org/abs/2305.14704)

    本文推导和评估了四种贝叶斯批次赌博算法，用于自适应确定流量分配，全面评估了这些算法的可信度、敏感性和后悔度，仿真结果表明这些基于批次的贝叶斯算法是有效的。

    

    为了加速在线测试，多臂赌博算法通过自适应地收集数据而被作为固定时间A/B测试的重要补充方式不断提高。本文基于最近关于自适应收集数据的最佳臂识别和统计推断的研究，推导和评估了四种基于贝叶斯批次赌博算法（NB-TS，WB-TS，NB-TTTS，WB-TTTS），它们是两种加权批次（Naive Batch和Weighted Batch）和两种贝叶斯抽样策略（Thompson Sampling和Top-Two Thompson Sampling）的组合，用于自适应确定流量分配。本文提供的这些基于批次统计奖励度量的贝叶斯抽样算法在实践中得以应用，而本文提出的其中一个组合WB-TTTS似乎是最新讨论的。对这四种基于批次的贝叶斯抽样算法进行了全面评估，包括测试方法的可信度、敏感性和后悔度。此外，评估还考虑了批次内奖励度量的方差以及批次之间的相关性，这在以前的研究中尚未得到很好的解决。仿真结果表明，与现有的赌博算法（例如UCB1，TS和Exp3）相比，这些基于批次的贝叶斯算法是有效的。

    To speed up online testing, adaptive traffic experimentation through multi-armed bandit algorithms is rising as an essential complementary alternative to the fixed horizon A/B testing. Based on recent research on best arm identification and statistical inference with adaptively collected data, this paper derives and evaluates four Bayesian batch bandit algorithms (NB-TS, WB-TS, NB-TTTS, WB-TTTS), which are combinations of two ways of weighting batches (Naive Batch and Weighted Batch) and two Bayesian sampling strategies (Thompson Sampling and Top-Two Thompson Sampling) to adaptively determine traffic allocation. These derived Bayesian sampling algorithms are practically based on summary batch statistics of a reward metric for pilot experiments, where one of the combination WB-TTTS in this paper seems to be newly discussed. The comprehensive evaluation on the four Bayesian sampling algorithms covers trustworthiness, sensitivity and regret of a testing methodology. Moreover, the evaluati
    
[^82]: SPEECH: 基于能量的事件中心超球的结构化预测

    SPEECH: Structured Prediction with Energy-Based Event-Centric Hyperspheres. (arXiv:2305.13617v1 [cs.CL])

    [http://arxiv.org/abs/2305.13617](http://arxiv.org/abs/2305.13617)

    这篇论文提出了一种称为SPEECH的模型，它使用能量建模来表示复杂的事件结构，并使用超球来表示事件类别。实验结果表明，SPEECH在事件检测和事件关系抽取任务中表现出卓越的性能。

    

    事件中心的结构化预测涉及预测事件的结构化输出。在大多数自然语言处理情况下，事件结构都具有复杂的依赖关系，因此有效地表示这些复杂的事件结构是具有挑战性的。为了解决这些问题，我们提出了基于能量的事件中心超球的结构化预测 (SPEECH)。 SPEECH 使用基于能量的建模来模拟事件结构组件之间的复杂依赖关系，并使用简单但有效的超球来表示事件类别。在两个统一标注的事件数据集的实验中，结果表明SPEECH在事件检测和事件关系抽取任务中占优势。

    Event-centric structured prediction involves predicting structured outputs of events. In most NLP cases, event structures are complex with manifold dependency, and it is challenging to effectively represent these complicated structured events. To address these issues, we propose Structured Prediction with Energy-based Event-Centric Hyperspheres (SPEECH). SPEECH models complex dependency among event structured components with energy-based modeling, and represents event classes with simple but effective hyperspheres. Experiments on two unified-annotated event datasets indicate that SPEECH is predominant in event detection and event-relation extraction tasks.
    
[^83]: 采用X射线微束数据几何变换增强语音发音分析

    Enhancing Speech Articulation Analysis using a Geometric Transformation of the X-ray Microbeam Dataset. (arXiv:2305.10775v1 [eess.AS])

    [http://arxiv.org/abs/2305.10775](http://arxiv.org/abs/2305.10775)

    该论文提出了一种新的几何变换方法，将X-Ray Microbeam数据集中的解剖标记物的X-Y坐标沿中矢状面映射到多个相对测量中，进而改进了测量的准确性。

    

    准确分析语音发音对于语音分析至关重要。然而，声门的X-Y坐标严重依赖于发言者的解剖结构和颗粒位置的可变性，现有的X射线微束数据集（XRMB）中的解剖标志物映射方法无法捕捉到发音道的整个解剖学。在本文中，我们提出了一种新的几何变换，改进了这些测量的准确性。我们的变换将解剖标记物的X-Y坐标沿中矢状面映射到6个相对测量中：唇缝张度（LA）、唇部突出（LP）、舌体收缩位置（TTCL）、度数（TBCD）、舌尖收缩位置（TTCL）和度数（TTCD）。我们的创新贡献是将腭板追踪延伸到推测的咽喉前线，从而改善了舌体收缩的测量。

    Accurate analysis of speech articulation is crucial for speech analysis. However, X-Y coordinates of articulators strongly depend on the anatomy of the speakers and the variability of pellet placements, and existing methods for mapping anatomical landmarks in the X-ray Microbeam Dataset (XRMB) fail to capture the entire anatomy of the vocal tract. In this paper, we propose a new geometric transformation that improves the accuracy of these measurements. Our transformation maps anatomical landmarks' X-Y coordinates along the midsagittal plane onto six relative measures: Lip Aperture (LA), Lip Protusion (LP), Tongue Body Constriction Location (TTCL), Degree (TBCD), Tongue Tip Constriction Location (TTCL) and Degree (TTCD). Our novel contribution is the extension of the palate trace towards the inferred anterior pharyngeal line, which improves measurements of tongue body constriction.
    
[^84]: 边方向性提高了异质图上的学习能力

    Edge Directionality Improves Learning on Heterophilic Graphs. (arXiv:2305.10498v1 [cs.LG])

    [http://arxiv.org/abs/2305.10498](http://arxiv.org/abs/2305.10498)

    本文提出了一种新的有向图神经网络框架Dir-GNN，并在有向引用图上进行评估，结果表明它在预测缺失的引用链接方面优于现有的无向GNN和一些有向图模型。

    

    图神经网络（GNN）已成为建模关系数据的事实标准工具。然而，尽管许多真实世界的图是有向的，但今天大多数GNN模型都通过使图成为无向图来完全忽略这些信息。这样做的原因是历史性的：1）许多早期的谱GNN变体明确要求图是无向的，2）关于同类图的第一批基准测试并未发现使用方向性有明显的增益。在本文中，我们展示了在异类设置中，将图形视为有向图可以增加图的内在同质性，这表明了从正确使用方向性信息中可能得到的好处。为此，我们引入了Directed Graph Neural Network（Dir-GNN），这是一个新的面向有向图的深度学习通用框架。Dir-GNN可以用于扩展任何消息传递神经网络（MPNN），以通过对每个节点执行单独的进出消息聚合来考虑边方向性信息。我们在有向引用图上评估了Dir-GNN，并证明它在预测缺失的引用链接方面优于现有的无向GNN和一些有向图模型。我们的结果表明，方向性信息可以提高在异质图上的学习能力，Dir-GNN可以有效地利用这些信息。

    Graph Neural Networks (GNNs) have become the de-facto standard tool for modeling relational data. However, while many real-world graphs are directed, the majority of today's GNN models discard this information altogether by simply making the graph undirected. The reasons for this are historical: 1) many early variants of spectral GNNs explicitly required undirected graphs, and 2) the first benchmarks on homophilic graphs did not find significant gain from using direction. In this paper, we show that in heterophilic settings, treating the graph as directed increases the effective homophily of the graph, suggesting a potential gain from the correct use of directionality information. To this end, we introduce Directed Graph Neural Network (Dir-GNN), a novel general framework for deep learning on directed graphs. Dir-GNN can be used to extend any Message Passing Neural Network (MPNN) to account for edge directionality information by performing separate aggregations of the incoming and outg
    
[^85]: 机器制造的媒体：监测虚假新闻和主流新闻网站上机器生成文章的动向。

    Machine-Made Media: Monitoring the Mobilization of Machine-Generated Articles on Misinformation and Mainstream News Websites. (arXiv:2305.09820v1 [cs.CY])

    [http://arxiv.org/abs/2305.09820](http://arxiv.org/abs/2305.09820)

    这篇论文研究了机器生成文章在虚假新闻和主流新闻网站的普及程度，发现虚假新闻网站上合成文章的使用速度比主流网站上更快。

    

    随着像ChatGPT这样的生成式大型语言模型（LLM）日益流行，越来越多的新闻网站开始利用它们生成文章。然而，这些语言模型不仅可能在声誉良好的网站上产生事实不准确的文章，而且不良新闻网站也可以利用这些LLM批量生产虚假信息。为了开始理解这一现象，我们提出了首个大规模研究合成文章在线新闻媒体中普及率的研究。为此，我们训练了一个基于DeBERTa的合成新闻检测器，并对3074个虚假新闻和主流新闻网站的超过1291万篇文章进行分类。我们发现，在2022年1月1日至2023年4月1日期间，合成新闻文章的相对数量在主流网站上增加了79.4％，而在虚假信息网站上增加了342％。分析ChatGPT发布的影响，使用中断时间序列，我们发现，虽然它的发布导致合成文章的使用显著增加，但虚假信息网站上的合成文章使用速度比主流网站上的快。

    With the increasing popularity of generative large language models (LLMs) like ChatGPT, an increasing number of news websites have begun utilizing them to generate articles. However, not only can these language models produce factually inaccurate articles on reputable websites but disreputable news sites can utilize these LLMs to mass produce misinformation. To begin to understand this phenomenon, we present one of the first large-scale studies of the prevalence of synthetic articles within online news media. To do this, we train a DeBERTa-based synthetic news detector and classify over 12.91 million articles from 3,074 misinformation and mainstream news websites. We find that between January 1, 2022 and April 1, 2023, the relative number of synthetic news articles increased by 79.4% on mainstream websites while increasing by 342% on misinformation sites. Analyzing the impact of the release of ChatGPT using an interrupted-time-series, we show that while its release resulted in a marked
    
[^86]: 从数据中发现物理定律的有限表达方法

    Finite Expression Methods for Discovering Physical Laws from Data. (arXiv:2305.08342v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.08342](http://arxiv.org/abs/2305.08342)

    本文介绍了一种名为"有限表达法" (FEX) 的深度符号学习方法，通过学习动态数据中PDE解的导数，发现控制方程的解析表达式。相对于其他现有方法，FEX在多种问题上表现出更好的数值性能，包括时变的PDE问题和具有时变系数的非线性动力系统。

    

    非线性动力学是科学和工程领域中普遍存在的现象。然而，从有限数据中推导出描述非线性动力学的解析表达式仍然具有挑战性。本文将介绍一种新颖的深度符号学习方法，称为"有限表达法" (FEX)，通过学习动态数据中的偏微分方程（PDE）解的导数，利用FEX在包含有限集的解析表达式的函数空间中发现控制方程。我们的数值结果表明，相对于其他现有方法（如PDE-Net, SINDy, GP 和 SPL），我们的FEX在多种问题上表现出更好的数值性能，包括时变的PDE问题和具有时变系数的非线性动力系统。此外，结果突显了FEX的灵活性和鲁棒性。

    Nonlinear dynamics is a pervasive phenomenon observed in scientific and engineering disciplines. However, the task of deriving analytical expressions to describe nonlinear dynamics from limited data remains challenging. In this paper, we shall present a novel deep symbolic learning method called the "finite expression method" (FEX) to discover governing equations within a function space containing a finite set of analytic expressions, based on observed dynamic data. The key concept is to employ FEX to generate analytical expressions of the governing equations by learning the derivatives of partial differential equation (PDE) solutions through convolutions. Our numerical results demonstrate that our FEX surpasses other existing methods (such as PDE-Net, SINDy, GP, and SPL) in terms of numerical performance across a range of problems, including time-dependent PDE problems and nonlinear dynamical systems with time-varying coefficients. Moreover, the results highlight FEX's flexibility and
    
[^87]: 从弱文本监督中学习图像中的人际互动

    Learning Human-Human Interactions in Images from Weak Textual Supervision. (arXiv:2304.14104v1 [cs.CV])

    [http://arxiv.org/abs/2304.14104](http://arxiv.org/abs/2304.14104)

    本文提出了一种新的范式，从单一的静态图像中学习自由文本的形式来灵活建模人际互动。并通过知识蒸馏生成伪标签来训练一种字幕模型，用于有效理解图像中的人际互动，具有较高的预测文本和语义质量，并在此任务上优于SOTA的图像字幕和情境识别模型。

    

    人际互动是多样且依赖于上下文的，但先前的工作将它们视为分类，忽略了可能的互动的重尾。本文提出了一种新的学习人际互动的范式，将其作为自由文本从单一的静态图像中学习，从而允许对情况和人际关系的无限空间进行灵活建模。为了克服缺乏特定于此任务的标记数据的问题，我们使用知识蒸馏应用于由大型语言模型产生的合成字幕数据，以此生成伪标签。我们展示了通过这个过程产生的伪标签可以用于训练一种字幕模型，能有效理解图像中的人际互动，通过衡量我们预测的文本和语义质量与事实的基础性的各种指标来衡量。我们进一步展示了我们的方法在这个任务上的性能优于SOTA的图像字幕和情境识别模型。我们将公开我们的代码。

    Interactions between humans are diverse and context-dependent, but previous works have treated them as categorical, disregarding the heavy tail of possible interactions. We propose a new paradigm of learning human-human interactions as free text from a single still image, allowing for flexibility in modeling the unlimited space of situations and relationships between people. To overcome the absence of data labelled specifically for this task, we use knowledge distillation applied to synthetic caption data produced by a large language model without explicit supervision. We show that the pseudo-labels produced by this procedure can be used to train a captioning model to effectively understand human-human interactions in images, as measured by a variety of metrics that measure textual and semantic faithfulness and factual groundedness of our predictions. We further show that our approach outperforms SOTA image captioning and situation recognition models on this task. We will release our c
    
[^88]: 可解释人工智能的范畴基础：一种统一的结构和语义形式体系。

    Categorical Foundations of Explainable AI: A Unifying Formalism of Structures and Semantics. (arXiv:2304.14094v1 [cs.AI])

    [http://arxiv.org/abs/2304.14094](http://arxiv.org/abs/2304.14094)

    本文采用范畴理论的框架，提出了可解释AI的统一理论体系，为领域中所有重要术语提供了清晰的形式定义，并提供了遵循所提出结构的领域分类法。

    

    可解释人工智能（XAI）旨在回答与AI模型部署相关的伦理和法律问题。然而，相当数量的领域特定评论强调需要一个数学基础来定义领域中的关键概念，即使“解释”这个术语还缺乏精确定义。这些评论还主张建立一个健全而统一的可解释AI形式体系，以避免出现不良提出问题，帮助研究人员浏览一个快速增长的知识体系。据作者所知，该论文是填补该空白的首次尝试，通过形式化一个可解释AI的统一理论。采用范畴理论的框架，特别是反馈单调范畴，我们首先提供了可解释AI中所有重要术语的形式定义。然后，我们提出了一个遵循提出结构的领域分类法，展示了如何使用引入的理论来对当前研究的所有主要XAI系统类进行分类。

    Explainable AI (XAI) aims to answer ethical and legal questions associated with the deployment of AI models. However, a considerable number of domain-specific reviews highlight the need of a mathematical foundation for the key notions in the field, considering that even the term "explanation" still lacks a precise definition. These reviews also advocate for a sound and unifying formalism for explainable AI, to avoid the emergence of ill-posed questions, and to help researchers navigate a rapidly growing body of knowledge. To the authors knowledge, this paper is the first attempt to fill this gap by formalizing a unifying theory of XAI. Employing the framework of category theory, and feedback monoidal categories in particular, we first provide formal definitions for all essential terms in explainable AI. Then we propose a taxonomy of the field following the proposed structure, showing how the introduced theory can be used to categorize all the main classes of XAI systems currently studi
    
[^89]: 节点特征增强改进网络对齐

    Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])

    [http://arxiv.org/abs/2304.12751](http://arxiv.org/abs/2304.12751)

    本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。

    

    网络对齐（NA）是通过给定网络的拓扑和/或特征信息来发现多个网络之间的节点对应关系的任务。虽然NA方法在各种场景下取得了显著的成功，但其有效性并不总是有额外信息，如先前的锚点链接和/或节点特征。为了解决这个实际的挑战，我们提出了Grad-Align+，这是一种新颖的NA方法，建立在最近一种最先进的NA方法Grad-Align之上，Grad-Align+仅逐步发现部分节点对，直到找到所有节点对。在设计Grad-Align+时，我们考虑如何通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法。为了实现这个目标，我们开发了由三个关键组成部分组成的Grad-Align+：基于中心性的节点特征增强（CNFA）、图切片生成和优化节点嵌入特征（ONIFE）。

    Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
    
[^90]: 可靠的聚类算法:一种新的Transformer聚类方法

    Fairness in Visual Clustering: A Novel Transformer Clustering Approach. (arXiv:2304.07408v1 [cs.CV])

    [http://arxiv.org/abs/2304.07408](http://arxiv.org/abs/2304.07408)

    本文提出了一种新的Transformer聚类方法，通过引入聚类纯度作为指标，采用新的损失函数来维持聚类模型的公平性，同时引入Cross-attention机制提高聚类的纯度。

    

    在无监督聚类的情景下，为了减少人群偏差而增加深度聚类模型的公平性是一个具有挑战性的目标。本文从聚类纯度的角度评估了深度聚类模型中的人口偏差，聚类纯度是指聚类中正样本与它们的相关程度的比值。我们引入了一种新的损失函数来鼓励所有聚类的纯度一致性以维持学习到的聚类模型的公平性。此外, 我们提出了一种新的Cross-attention机制，用于测量多个聚类之间的相关性，在学习过程中加强远距离的正样本，提高聚类的纯度。在一个大规模的数据集上进行实验，包括多种属性设置。

    Promoting fairness for deep clustering models in unsupervised clustering settings to reduce demographic bias is a challenging goal. This is because of the limitation of large-scale balanced data with well-annotated labels for sensitive or protected attributes. In this paper, we first evaluate demographic bias in deep clustering models from the perspective of cluster purity, which is measured by the ratio of positive samples within a cluster to their correlation degree. This measurement is adopted as an indication of demographic bias. Then, a novel loss function is introduced to encourage a purity consistency for all clusters to maintain the fairness aspect of the learned clustering model. Moreover, we present a novel attention mechanism, Cross-attention, to measure correlations between multiple clusters, strengthening faraway positive samples and improving the purity of clusters during the learning process. Experimental results on a large-scale dataset with numerous attribute settings 
    
[^91]: 强化学习中基于Bandit方法的显式塑形外部建议算法的研究

    Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning. (arXiv:2304.07163v1 [cs.AI])

    [http://arxiv.org/abs/2304.07163](http://arxiv.org/abs/2304.07163)

    本文研究了如何基于Bandit方法将外部建议融入到强化学习中，并提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。实验结果表明这些算法在样本复杂度、学习速度和形状质量方面都取得了良好的效果。

    

    强化学习（RL）算法中的一个关键问题是如何将外部或专家的建议融入到学习当中。本文将将将此问题表述为一种多臂赌博机称为塑形赌博机（shaping-bandits）。我们提出了三种不同的塑形算法：UCB-PIES（UPIES）， Racing-PIES（RPIES）和Lazy PIES（LPIES）。通过在模拟环境和LQR和Atari环境中的实验，我们证明了这三种算法在样本复杂度、学习速度和形状质量方面的有效性。

    A key challenge for a reinforcement learning (RL) agent is to incorporate external/expert1 advice in its learning. The desired goals of an algorithm that can shape the learning of an RL agent with external advice include (a) maintaining policy invariance; (b) accelerating the learning of the agent; and (c) learning from arbitrary advice [3]. To address this challenge this paper formulates the problem of incorporating external advice in RL as a multi-armed bandit called shaping-bandits. The reward of each arm of shaping bandits corresponds to the return obtained by following the expert or by following a default RL algorithm learning on the true environment reward.We show that directly applying existing bandit and shaping algorithms that do not reason about the non-stationary nature of the underlying returns can lead to poor results. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES (LPIES) three different shaping algorithms built on different assumptions that reason a
    
[^92]: counterfactuals: 用于反事实解释方法的 R 包

    counterfactuals: An R Package for Counterfactual Explanation Methods. (arXiv:2304.06569v1 [stat.ML])

    [http://arxiv.org/abs/2304.06569](http://arxiv.org/abs/2304.06569)

    该论文介绍了一个统一且模块化的 R6 接口，用于具体实现反事实解释方法。通过实现三种方法并推广到不同的情境中，结合真实用例，此方法能够快速准确地得出有关如何更改单个观测值的特征值以获得所需预测的信息。

    

    反事实解释方法提供有关如何更改单个观测值的特征值以获得所需预测的信息。尽管研究中提出了越来越多的方法，但只有少数具有广泛变化的接口和要求的实现存在。在本文中，我们介绍 counterfactuals R 包，它提供了一个基于 R6 的模块化和统一的接口，用于反事实解释方法。我们已经实现了三种现有的反事实解释方法，并提出了一些可选的方法学扩展，以将这些方法推广到不同的场景并使其更具可比性。我们使用真实用例解释了包的结构和工作流程，并展示了如何将其他反事实解释方法集成到包中。此外，我们针对各种模型和数据集比较了实施的方法，以评估其反事实解释的质量和运行时行为。

    Counterfactual explanation methods provide information on how feature values of individual observations must be changed to obtain a desired prediction. Despite the increasing amount of proposed methods in research, only a few implementations exist whose interfaces and requirements vary widely. In this work, we introduce the counterfactuals R package, which provides a modular and unified R6-based interface for counterfactual explanation methods. We implemented three existing counterfactual explanation methods and propose some optional methodological extensions to generalize these methods to different scenarios and to make them more comparable. We explain the structure and workflow of the package using real use cases and show how to integrate additional counterfactual explanation methods into the package. In addition, we compared the implemented methods for a variety of models and datasets with regard to the quality of their counterfactual explanations and their runtime behavior.
    
[^93]: 威斯康星公立学校社交预测的困难教训

    Difficult Lessons on Social Prediction from Wisconsin Public Schools. (arXiv:2304.06205v1 [cs.CY])

    [http://arxiv.org/abs/2304.06205](http://arxiv.org/abs/2304.06205)

    美国公立学校引入的预测算法（EWS）未能提高毕业率，EWS准确性高，但环境因素影响更大。

    

    美国公立学校近期引入了预测算法（EWS）来提高毕业率。这些系统通过预测哪些学生可能退学，帮助对个体学生进行干预。虽然投资巨大，得到广泛应用，但对EWS有效性的理解仍然存在重大差距。本研究利用威斯康星全区的近十年数据，首次对EWS对毕业率的长期影响进行大规模评估。我们提供证据表明预测系统所做的风险评估非常准确，特别是对于来自边缘化背景的学生。尽管该系统准确性高并且得到广泛应用，但我们发现没有证据表明它会导致毕业率的提高。我们提供了一个强大的统计模式，可以解释为什么这些看似矛盾的见解存在，即环境因素，例如学生所在学校或社区的质量，淹没了EWS对毕业率可能产生的任何影响。

    Early warning systems (EWS) are prediction algorithms that have recently taken a central role in efforts to improve graduation rates in public schools across the US. These systems assist in targeting interventions at individual students by predicting which students are at risk of dropping out. Despite significant investments and adoption, there remain significant gaps in our understanding of the efficacy of EWS. In this work, we draw on nearly a decade's worth of data from a system used throughout Wisconsin to provide the first large-scale evaluation of the long-term impact of EWS on graduation outcomes.  We present evidence that risk assessments made by the prediction system are highly accurate, including for students from marginalized backgrounds. Despite the system's accuracy and widespread use, we find no evidence that it has led to improved graduation rates. We surface a robust statistical pattern that can explain why these seemingly contradictory insights hold. Namely, environmen
    
[^94]: 神经网络控制器到决策树控制器的精确且节约成本的自动转换

    Exact and Cost-Effective Automated Transformation of Neural Network Controllers to Decision Tree Controllers. (arXiv:2304.06049v1 [cs.LG])

    [http://arxiv.org/abs/2304.06049](http://arxiv.org/abs/2304.06049)

    本文研究了将基于神经网络的控制器转换为等效软决策树控制器并提出了一种自动且节约成本的转换算法。该方法适用于包括ReLU激活函数在内的离散输出NN控制器，并能够提高形式验证的运行效率。

    

    在过去的十年中，基于神经网络（NN）的控制器在各种决策任务中表现出了显着的功效。然而，它们的黑盒特性和意外行为和令人惊讶的结果的风险对于在具有正确性和安全性强保证的真实世界系统中的部署构成了挑战。我们通过调查将基于NN的控制器转换为等效的软决策树（SDT）控制器及其对可验证性的影响来解决这些限制。与以前的方法不同，我们专注于离散输出NN控制器，包括整流线性单元（ReLU）激活函数以及argmax操作。然后，我们设计了一种精确但节省成本的转换算法，因为它可以自动删除多余的分支。我们使用OpenAI Gym环境的两个基准测试来评估我们的方法。我们的结果表明，SDT转换可以使形式验证受益，显示运行时改进。

    Over the past decade, neural network (NN)-based controllers have demonstrated remarkable efficacy in a variety of decision-making tasks. However, their black-box nature and the risk of unexpected behaviors and surprising results pose a challenge to their deployment in real-world systems with strong guarantees of correctness and safety. We address these limitations by investigating the transformation of NN-based controllers into equivalent soft decision tree (SDT)-based controllers and its impact on verifiability. Differently from previous approaches, we focus on discrete-output NN controllers including rectified linear unit (ReLU) activation functions as well as argmax operations. We then devise an exact but cost-effective transformation algorithm, in that it can automatically prune redundant branches. We evaluate our approach using two benchmarks from the OpenAI Gym environment. Our results indicate that the SDT transformation can benefit formal verification, showing runtime improveme
    
[^95]: 基于屏障-李亚普诺夫Actor-Critic强化学习方法的安全稳定控制

    A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control. (arXiv:2304.04066v1 [eess.SY])

    [http://arxiv.org/abs/2304.04066](http://arxiv.org/abs/2304.04066)

    本文提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，针对强化学习控制现实世界系统时的安全稳定控制问题提出了一种解决方案。其中，基于重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。

    

    强化学习在视频游戏和机器人等领域展现出惊人的性能。然而，使用强化学习控制现实世界系统时，确保安全和稳定性仍然是一个重大挑战。在本文中，我们首先为强化学习系统提供安全和稳定性的定义，然后将控制屏障函数（CBF）和控制李亚普诺夫函数（CLF）方法与Actor-Critic方法相结合，提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，有助于保持系统的安全和稳定性。在该框架中，基于来自重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。此外，还引入了一个备用控制器，以防RL控制器无法提供稳定控制。

    Reinforcement learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the control barrier function (CBF) and control Lyapunov function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide
    
[^96]: 无需重新训练的Transformer模型分块压缩

    Blockwise Compression of Transformer-based Models without Retraining. (arXiv:2304.01483v1 [cs.CL])

    [http://arxiv.org/abs/2304.01483](http://arxiv.org/abs/2304.01483)

    本论文提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，实现了降低部署门槛的目的。

    

    基于Transformer模型的GPT-3、ChatGPT和GPT-4近年来备受关注，但它们的巨大计算资源和存储开销仍然是不可避免的挑战。为了解决这个问题，我们提出了一种名为BCT的分块压缩框架，可以对整个Transformer模型进行更细粒度的压缩，包括嵌入、矩阵乘法、GELU、Softmax、层规范化以及所有中间结果。我们对一个高效模型使用BCT进行了压缩并在多个GLUE数据集上进行了评估，结果显示在大多数任务中，BCT只会带来少于0.90%的准确率下降。

    Transformer-based models, represented by GPT-3, ChatGPT, and GPT-4, have recently attracted increasing interest, research enthusiasm, and business demand. However, their massive computation resources and huge memory footprint are inevitable challenges. To tackle this issue, we propose BCT, a framework of blockwise compression for transformers without retraining, to lower deployment thresholds. BCT achieves more fine-grained compression of the whole transformer, including embedding, matrix multiplication, GELU, Softmax, layer normalization, and all the intermediate results. As a case, we compress an efficient model with BCT and evaluate it on several General Language Understanding Evaluation (GLUE) datasets. The results show that BCT can achieve a less than 0.90% accuracy drop in most tasks.
    
[^97]: 物理学指导的PointNet：它能同时解决多少不规则几何体的反问题？以线弹性为例。

    Physics-informed PointNet: On how many irregular geometries can it solve an inverse problem simultaneously? Application to linear elasticity. (arXiv:2303.13634v1 [cs.LG])

    [http://arxiv.org/abs/2303.13634](http://arxiv.org/abs/2303.13634)

    本文提出了一种物理学指导的神经网络模型PIPN，它能利用稀疏标记数据同时预测所需偏微分方程在数百个不同的几何体上的解，有望在工业界进行快速的几何设计。

    

    常规的物理学指导的神经网络（PINN）利用稀疏标记数据预测偏微分方程的解，但只限于单一的域。相反，完全监督学习模型通常是首先在已知解（即标记数据）的几千个域上进行训练，然后预测在一些未知域上的解。物理学指导的PointNet（PIPN）主要旨在填补PINN（作为弱监督学习模型）和完全监督学习模型之间的差距。在本文中，我们展示了PIPN能够同时预测所需偏微分方程在数百个域上的解，而只使用稀疏标记数据。这个框架有助于在工业界进行快速的几何设计，尤其当只有稀疏标记数据可用时。特别地，我们展示了PIPN能够同时预测平面应力问题在500多个不同几何体上的解。

    Regular physics-informed neural networks (PINNs) predict the solution of partial differential equations using sparse labeled data but only over a single domain. On the other hand, fully supervised learning models are first trained usually over a few thousand domains with known solutions (i.e., labeled data) and then predict the solution over a few hundred unseen domains. Physics-informed PointNet (PIPN) is primarily designed to fill this gap between PINNs (as weakly supervised learning models) and fully supervised learning models. In this article, we demonstrate that PIPN predicts the solution of desired partial differential equations over a few hundred domains simultaneously, while it only uses sparse labeled data. This framework benefits fast geometric designs in the industry when only sparse labeled data are available. Particularly, we show that PIPN predicts the solution of a plane stress problem over more than 500 domains with different geometries, simultaneously. Moreover, we pio
    
[^98]: 外科高光谱图像的语义分割在几何域转换下

    Semantic segmentation of surgical hyperspectral images under geometric domain shifts. (arXiv:2303.10972v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.10972](http://arxiv.org/abs/2303.10972)

    外科高光谱图像的语义分割面临几何域转换挑战。我们提出了一种分析最新语义分割网络在几何分布转换数据上的方法，并通过“器官移植”增强技术解决了泛化性问题。

    

    鲁棒的术中图像数据的语义分割可以为自动手术场景理解和自主机器人手术铺平道路。然而，由于手术程序的变化或操作部位的遮挡，几何域转换在现实世界的开放手术中普遍存在，但在该领域仍然是一个未被广泛讨论的主题。为了填补文献中的空白，我们（1）在几何域的分布转换数据存在下，提出了对最新的语义分割网络进行分析，并（2）通过一种名为“器官移植”的专用增强技术来解决泛化性问题，该技术是我们从一般计算机视觉社区中改编的。通过对包括来自33只猪的600个RGB和高光谱成像数据集在内的六个不同分布转换数据集进行全面验证，我们证明了SOA器官分割网络在几何分布转换数据上表现出较大的性能下降。令人惊讶的是，这些网络在语义上注释了19个类别的高光谱成像（HSI）立方体的器官分割。

    Robust semantic segmentation of intraoperative image data could pave the way for automatic surgical scene understanding and autonomous robotic surgery. Geometric domain shifts, however, although common in real-world open surgeries due to variations in surgical procedures or situs occlusions, remain a topic largely unaddressed in the field. To address this gap in the literature, we (1) present the first analysis of state-of-the-art (SOA) semantic segmentation networks in the presence of geometric out-of-distribution (OOD) data, and (2) address generalizability with a dedicated augmentation technique termed "Organ Transplantation" that we adapted from the general computer vision community. According to a comprehensive validation on six different OOD data sets comprising 600 RGB and hyperspectral imaging (HSI) cubes from 33 pigs semantically annotated with 19 classes, we demonstrate a large performance drop of SOA organ segmentation networks applied to geometric OOD data. Surprisingly, th
    
[^99]: 通过 Numerai 数据科学竞赛案例，理解时间表格和多变量时间序列的模型复杂度

    Understanding Model Complexity for temporal tabular and multi-variate time series, case study with Numerai data science tournament. (arXiv:2303.07925v1 [cs.LG])

    [http://arxiv.org/abs/2303.07925](http://arxiv.org/abs/2303.07925)

    本文采用 Numerai 数据科学竞赛的数据，探究了多变量时间序列建模中不同特征工程和降维方法的应用；提出了一种新的集成方法，用于高维时间序列建模，该方法在通用性、鲁棒性和效率上优于一些深度学习模型。

    

    本文探究了在多变量时间序列建模中使用不同特征工程和降维方法的应用。利用从 Numerai 数据竞赛创建的特征目标交叉相关时间序列数据集，我们证明在过度参数化的情况下，不同特征工程方法的性能与预测会收敛到可由再生核希尔伯特空间刻画的相同平衡态。我们提出了一种新的集成方法，该方法结合了不同的随机非线性变换，随后采用岭回归模型进行高维时间序列建模。与一些常用的用于序列建模的深度学习模型（如 LSTM 和 transformer）相比，我们的方法更加鲁棒（在不同的随机种子下具有较低的模型方差，且对架构的选择不太敏感），并且更有效率。我们方法的另一个优势在于模型的简单性，因为没有必要使用复杂的深度学习框架。

    In this paper, we explore the use of different feature engineering and dimensionality reduction methods in multi-variate time-series modelling. Using a feature-target cross correlation time series dataset created from Numerai tournament, we demonstrate under over-parameterised regime, both the performance and predictions from different feature engineering methods converge to the same equilibrium, which can be characterised by the reproducing kernel Hilbert space. We suggest a new Ensemble method, which combines different random non-linear transforms followed by ridge regression for modelling high dimensional time-series. Compared to some commonly used deep learning models for sequence modelling, such as LSTM and transformers, our method is more robust (lower model variance over different random seeds and less sensitive to the choice of architecture) and more efficient. An additional advantage of our method is model simplicity as there is no need to use sophisticated deep learning frame
    
[^100]: 概率扩散模型的广义尺度空间特性

    Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])

    [http://arxiv.org/abs/2303.07900](http://arxiv.org/abs/2303.07900)

    本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。

    

    概率扩散模型在深度学习社区中越来越受欢迎。它们生成从学习图像分布的令人信服的样本，具有广泛的实际应用。这些方法最初是受漂移-扩散过程的启发，但在近期的实践导向的出版物中，这些起源得到了较少的关注。本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。此外，我们讨论了深度学习和基于模型的世界中漂移-扩散物理核心概念解释之间的相似性和差异。为此，我们考察了概率扩散与渗透滤波器之间的关系。

    Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
    
[^101]: RE-MOVE：一种基于语言反馈的动态环境自适应策略设计方法

    RE-MOVE: An Adaptive Policy Design Approach for Dynamic Environments via Language-Based Feedback. (arXiv:2303.07622v1 [cs.RO])

    [http://arxiv.org/abs/2303.07622](http://arxiv.org/abs/2303.07622)

    RE-MOVE提出了一种基于语言反馈的自适应策略设计方法，可以使机器人适应实时环境变化，并从人类反馈中学习并适应之前未见过的对抗性场景。

    

    连续控制机器人导航任务的强化学习策略经常无法在实时部署期间适应环境的变化，这可能导致灾难性的失败。为了解决这个问题，我们提出了一种名为RE-MOVE（请求帮助并移动）的新方法，它使用基于语言的反馈来调整经过训练的策略以适应环境的实时变化。在这项工作中，我们使经过训练的策略能够决定何时请求反馈并如何将反馈纳入训练好的策略中。RE-MOVE利用先验不确定性来确定请求人类反馈的最佳时间，并使用基于语言的反馈进行实时适应。我们进行了大量的合成和实际世界的评估，以展示我们提出的方法在多种测试时间动态导航场景中的好处。我们的方法使机器人能够从人类反馈中学习并适应之前未见过的对抗性场景。

    Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (\textbf{RE}quest help and \textbf{MOVE} on), which uses language-based feedback to adjust trained policies to real-time changes in the environment. In this work, we enable the trained policy to decide \emph{when to ask for feedback} and \emph{how to incorporate feedback into trained policies}. RE-MOVE incorporates epistemic uncertainty to determine the optimal time to request feedback from humans and uses language-based feedback for real-time adaptation. We perform extensive synthetic and real-world evaluations to demonstrate the benefits of our proposed approach in several test-time dynamic navigation scenarios. Our approach enable robots to learn from human feedback and adapt to previously unseen adversarial 
    
[^102]: 导航的中层表示——虚拟导航

    Virtual Guidance as a Mid-level Representation for Navigation. (arXiv:2303.02731v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02731](http://arxiv.org/abs/2303.02731)

    该论文介绍了一种名为“虚拟导航”的新技术，通过在智能体的相机视图上叠加彩色路径或球的形式的视觉指引，以易于理解的导航指令传达抽象的导航信息。实验结果表明，在模拟和真实环境中，虚拟导航在遵循计划路径和避开障碍物等多个指标上优于现有方法。

    

    在自主导航的背景下，有效地传达抽象的导航指引给动态环境中的智能体存在挑战，特别是当导航信息是多模态的时候。为了解决这个问题，本文引入了一种名为“虚拟导航”的新技术，旨在以视觉方式呈现非视觉指令信号。这些视觉指引以彩色路径或球的形式叠加在智能体的相机视图上，作为易于理解的导航指令。我们通过在模拟和真实环境中进行实验来评估我们提出的方法。在模拟环境中，我们的虚拟导航在多项指标上优于基线混合方法，包括遵循计划路径和避开障碍物。此外，我们将虚拟导航的概念扩展到将基于文本提示的指令转换为用于真实环境实验的直观视觉格式。我们的结果验证了虚拟导航的适应性。

    In the context of autonomous navigation, effectively conveying abstract navigational cues to agents in dynamic environments poses challenges, particularly when the navigation information is multimodal. To address this issue, the paper introduces a novel technique termed "Virtual Guidance," which is designed to visually represent non-visual instructional signals. These visual cues, rendered as colored paths or spheres, are overlaid onto the agent's camera view, serving as easily comprehensible navigational instructions. We evaluate our proposed method through experiments in both simulated and real-world settings. In the simulated environments, our virtual guidance outperforms baseline hybrid approaches in several metrics, including adherence to planned routes and obstacle avoidance. Furthermore, we extend the concept of virtual guidance to transform text-prompt-based instructions into a visually intuitive format for real-world experiments. Our results validate the adaptability of virtua
    
[^103]: 强化标签: 多智能体深度强化学习用于点特征标签放置

    Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement. (arXiv:2303.01388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01388](http://arxiv.org/abs/2303.01388)

    本论文介绍了一种利用多智能体深度强化学习的标签放置方法，该方法在数据可视化中解决了标签重叠和可读性问题，与现有的手工设计算法相比具有显著的优势。

    

    最近几年，结合深度学习技术的强化学习已成功应用于解决各个领域的复杂问题，包括机器人、自动驾驶汽车和金融等。本文将强化学习引入标签放置，这是数据可视化中一个复杂的任务，旨在找到最佳位置以避免重叠并确保可读性。我们提出了一种新颖的点特征标签放置方法，利用多智能体深度强化学习(MADRL)来学习标签放置策略，这是与现有手工设计的算法相对应的第一个机器学习驱动的标签放置方法。为了方便强化学习的学习过程，我们开发了一个环境，其中代理作为标签的代理，这些标签是一种增强可视化的短文本注释。我们的结果表明，通过我们的方法训练得到的策略明显优于未经训练的代理的随机策略以及由人类设计的比较方法。

    Over recent years, Reinforcement Learning combined with Deep Learning techniques has successfully proven to solve complex problems in various domains, including robotics, self-driving cars, and finance. In this paper, we are introducing Reinforcement Learning (RL) to label placement, a complex task in data visualization that seeks optimal positioning for labels to avoid overlap and ensure legibility. Our novel point-feature label placement method utilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the label placement strategy, which is the first machine-learning-driven labeling method in contrast to existing hand-crafted algorithms designed by human experts. To facilitate RL learning, we developed an environment where an agent acts as a proxy for a label, a short textual annotation that augments visualization. Our results show that the strategy trained by our method significantly outperforms the random strategy of an untrained agent and compared methods designed by human 
    
[^104]: 受可微分逻辑约束的共学习规划与控制策略

    Co-learning Planning and Control Policies Constrained by Differentiable Logic Specifications. (arXiv:2303.01346v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.01346](http://arxiv.org/abs/2303.01346)

    本文提出了一种通过共学习规划和控制策略来解决带有复杂逻辑约束的高维度机器人导航任务的强化学习方法。相比现有算法，这种方法通过降低样本复杂性来训练出高质量的策略，并且能够高效地生成长期的机器人运动路径。实验证明了该方法的有效性。

    

    在机器人技术中综合规划与控制策略是一项基本任务，但复杂的逻辑约束和高维度的机器人动力学使其变得更加复杂。本文提出了一种新颖的强化学习方法，通过共学习规划和控制策略来解决带有复杂逻辑约束的高维度机器人导航任务。值得注意的是，这种方法显著降低了训练的样本复杂性，相比现有的强化学习算法，我们可以用更少的样本训练出高质量的策略。此外，我们的方法简化了从地图图像中提取复杂规范并能够高效生成不同地图布局的长期机器人运动路径。此外，我们的方法还展示了在高维度控制和避免次优策略方面的能力。通过模拟高维机器人导航任务的实验验证了我们方法的有效性。

    Synthesizing planning and control policies in robotics is a fundamental task, further complicated by factors such as complex logic specifications and high-dimensional robot dynamics. This paper presents a novel reinforcement learning approach to solving high-dimensional robot navigation tasks with complex logic specifications by co-learning planning and control policies. Notably, this approach significantly reduces the sample complexity in training, allowing us to train high-quality policies with much fewer samples compared to existing reinforcement learning algorithms. In addition, our methodology streamlines complex specification extraction from map images and enables the efficient generation of long-horizon robot motion paths across different map layouts. Moreover, our approach also demonstrates capabilities for high-dimensional control and avoiding suboptimal policies via policy alignment. The efficacy of our approach is demonstrated through experiments involving simulated high-dim
    
[^105]: 模拟与实际强化学习在操纵中的应用：一种基于共识的方法

    Sim-and-Real Reinforcement Learning for Manipulation: A Consensus-based Approach. (arXiv:2302.13423v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.13423](http://arxiv.org/abs/2302.13423)

    本文提出了一种基于共识的模拟与实际深度强化学习算法，该算法在机器人操纵中表现出可比较的性能，并发现了在模拟中的最佳策略不一定适用于模拟与实际训练，以及模拟智能体数量越多，模拟与实际训练效果越好。

    

    模拟与实际训练是机器人操纵的一种有希望的替代方案。然而，当前的模拟与实际训练既不高效（即收敛到最优策略较慢），也不有效（即真实世界机器人数据较少）。考虑到有限的时间和硬件预算，模拟与实际训练的性能不尽如人意。本文提出一种基于共识的模拟与实际深度强化学习算法 (CSAR)，用于操纵器人的挑选和放置任务，该算法在模拟和实际世界中都表现出可比较的性能。在这个算法中，我们通过在模拟器和真实世界中训练智能体来获得模拟和实际世界的最优策略。我们发现了两个有趣的现象：（1）在模拟中的最佳策略并不是模拟与实际训练的最佳策略。（2）模拟智能体越多，模拟与实际训练效果越好。

    Sim-and-real training is a promising alternative to sim-to-real training for robot manipulations. However, the current sim-and-real training is neither efficient, i.e., slow convergence to the optimal policy, nor effective, i.e., sizeable real-world robot data. Given limited time and hardware budgets, the performance of sim-and-real training is not satisfactory. In this paper, we propose a Consensus-based Sim-And-Real deep reinforcement learning algorithm (CSAR) for manipulator pick-and-place tasks, which shows comparable performance in both sim-and-real worlds. In this algorithm, we train the agents in simulators and the real world to get the optimal policies for both sim-and-real worlds. We found two interesting phenomenons: (1) Best policy in simulation is not the best for sim-and-real training. (2) The more simulation agents, the better sim-and-real training. The experimental video is available at: https://youtu.be/mcHJtNIsTEQ.
    
[^106]: 使用指针生成网络和SciBERT嵌入生成研究论文的摘要

    Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.07729](http://arxiv.org/abs/2302.07729)

    该论文提出了一种使用指针生成网络和SciBERT嵌入来自动生成研究论文亮点的方法。在多个基准数据集上的实验证明，该模型在研究亮点生成方面具有最佳性能。

    

    如今，许多研究文章都以研究亮点作为前言，以总结论文的主要发现。亮点不仅帮助研究人员准确快速地识别论文的贡献，还通过搜索引擎增加了文章的可发现性。我们的目标是在给定研究论文的特定段落的情况下自动构建研究亮点。我们使用了一个具有覆盖机制和上下文嵌入层的指针生成网络，将输入标记编码为SciBERT嵌入。我们在基准数据集CSPubSum上测试了我们的模型，并且还提出了MixSub，一个用于自动生成研究亮点的新的跨学科论文语料库。对于CSPubSum和MixSub，我们观察到所提出的模型相对于相关变体和文献中提出的其他模型来说具有最佳性能。在CSPubSum数据集上，我们的模型在只使用论文的摘要作为输入时表现最佳。

    Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
    
[^107]: 不要再过度使用黑匣子模型进行简单任务，转而使用透明模型。

    Stop overkilling simple tasks with black-box models and use transparent models instead. (arXiv:2302.02804v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02804](http://arxiv.org/abs/2302.02804)

    过度使用黑匣子模型会导致简单任务的浪费，透明模型可以提高效率和精度。

    

    近年来，深度学习方法在人工智能领域中取得了重大突破。与传统机器学习模型不同，基于深度学习的方法能够自主从原始数据中提取特征。这允许跳过通常被认为是容易出错和烦琐的特征工程过程。此外，深度学习策略在精度方面通常优于传统模型。

    In recent years, the employment of deep learning methods has led to several significant breakthroughs in artificial intelligence. Different from traditional machine learning models, deep learning-based approaches are able to extract features autonomously from raw data. This allows for bypassing the feature engineering process, which is generally considered to be both error-prone and tedious. Moreover, deep learning strategies often outperform traditional models in terms of accuracy.
    
[^108]: 高效开发驾驶策略：基于技能的分层强化学习方法

    Developing Driving Strategies Efficiently: A Skill-Based Hierarchical Reinforcement Learning Approach. (arXiv:2302.02179v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02179](http://arxiv.org/abs/2302.02179)

    本文提出了基于技能的分层驾驶策略，通过设计和使用运动原语作为高层动作，大幅减少了训练时间，并且在合并场景中得到了更高性能的驾驶模型。

    

    在人类和自动驾驶车辆中驾驶汽车是一项具有挑战性的任务，需要高层次的规划和推理能力。人类驾驶员可以轻松完成这个任务，因此一直在努力模拟人类驾驶员的策略。这些策略可以用作开发自动驾驶算法或创建真实感模拟器的灵感。强化学习是建模驾驶策略的常用工具，但传统的模型训练可能计算成本高且耗时。为了解决这个问题，本文提出了"基于技能"的分层驾驶策略，其中运动原语（即技能）被设计并用作高层动作。这减少了需要具有不同行为的多个模型的应用的训练时间。合并场景的仿真结果表明，与基准强化学习相比，该方法提供了在较少训练次数下实现更高性能的驾驶模型。

    Driving in dense traffic with human and autonomous drivers is a challenging task that requires high-level planning and reasoning. Human drivers can achieve this task comfortably, and there has been many efforts to model human driver strategies. These strategies can be used as inspirations for developing autonomous driving algorithms or to create high-fidelity simulators. Reinforcement learning is a common tool to model driver policies, but conventional training of these models can be computationally expensive and time-consuming. To address this issue, in this paper, we propose ``skill-based" hierarchical driving strategies, where motion primitives, i.e. skills, are designed and used as high-level actions. This reduces the training time for applications that require multiple models with varying behavior. Simulation results in a merging scenario demonstrate that the proposed approach yields driver models that achieve higher performance with less training compared to baseline reinforcemen
    
[^109]: 使用深度学习预测超新星壳层扩张的3D时空预测方法，用于高分辨率星系模拟

    3D-Spatiotemporal Forecasting the Expansion of Supernova Shells Using Deep Learning toward High-Resolution Galaxy Simulations. (arXiv:2302.00026v2 [astro-ph.GA] UPDATED)

    [http://arxiv.org/abs/2302.00026](http://arxiv.org/abs/2302.00026)

    本文开发了一个深度学习模型，3D-MIM，用于预测超新星爆炸后的壳层扩张，通过在平滑粒子流体动力学模拟中检测并预测超新星影响粒子所在的壳层形状，解决了高分辨率星系模拟中超新星积分时间步长问题。

    

    超新星在星系形成和演化中起着重要作用。在使用大规模并行计算进行高分辨率星系模拟时，超新星的短积分时间步长成为严重瓶颈。为了解决这个问题，一种可能的解决方案是使用Hamiltonian分裂方法，即将需要短积分时间步长的区域与整个系统分开积分。为了将这种方法应用于平滑粒子流体动力学模拟中受超新星影响的粒子，我们需要在随后的全局步骤中提前检测到这些粒子所在的壳层的形状。本文中，我们开发了一个名为3D-MIM的深度学习模型来预测超新星爆炸后的壳层扩张。通过对带有粒子质量$m_{\rm gas}$ = 1 M$_\odot$的湍流云模拟进行训练，该模型能够准确地再现出各向异性的壳层形状，其中密度逐渐下降。

    Supernova (SN) plays an important role in galaxy formation and evolution. In high-resolution galaxy simulations using massively parallel computing, short integration timesteps for SNe are serious bottlenecks. This is an urgent issue that needs to be resolved for future higher-resolution galaxy simulations. One possible solution would be to use the Hamiltonian splitting method, in which regions requiring short timesteps are integrated separately from the entire system. To apply this method to the particles affected by SNe in a smoothed-particle hydrodynamics simulation, we need to detect the shape of the shell on and within which such SN-affected particles reside during the subsequent global step in advance. In this paper, we develop a deep learning model, 3D-MIM, to predict a shell expansion after a SN explosion. Trained on turbulent cloud simulations with particle mass $m_{\rm gas}$~=~1 M$_\odot$, the model accurately reproduces the anisotropic shell shape, where densities decrease by
    
[^110]: 不考虑图骨架的组合因果赌博机

    Combinatorial Causal Bandits without Graph Skeleton. (arXiv:2301.13392v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13392](http://arxiv.org/abs/2301.13392)

    本文研究了在二值一般因果模型和BGLMs上不考虑图骨架的组合因果赌博机问题，提出了可在BGLMs上实现的无需图骨架的遗憾最小化算法，达到了与依赖于图结构的最先进算法相同的渐进遗憾率$O(\sqrt{T}\ln T)$。

    

    在组合因果赌博机问题中，学习代理在每一轮选择一组变量进行干预，收集观测变量的反馈以最小化期望遗憾或样本复杂度。先前的工作研究了一般因果模型和二值广义线性模型（BGLMs）中的问题。但是，它们都需要先验知识来构建因果关系图。本文研究了在二值一般因果模型和BGLMs上不考虑图骨架的组合因果赌博机问题。我们首先在一般因果模型上提供了累积遗憾的指数下限。然后，我们设计了一种无需图骨架来实现BGLMs的遗憾最小化算法，表明它仍然达到$O(\sqrt{T}\ln T)$的期望遗憾。这个渐进的遗憾率与依赖于图结构的最先进算法相同。

    In combinatorial causal bandits (CCB), the learning agent chooses a subset of variables in each round to intervene and collects feedback from the observed variables to minimize expected regret or sample complexity. Previous works study this problem in both general causal models and binary generalized linear models (BGLMs). However, all of them require prior knowledge of causal graph structure. This paper studies the CCB problem without the graph structure on binary general causal models and BGLMs. We first provide an exponential lower bound of cumulative regrets for the CCB problem on general causal models. To overcome the exponentially large space of parameters, we then consider the CCB problem on BGLMs. We design a regret minimization algorithm for BGLMs even without the graph skeleton and show that it still achieves $O(\sqrt{T}\ln T)$ expected regret. This asymptotic regret is the same as the state-of-art algorithms relying on the graph structure. Moreover, we sacrifice the regret t
    
[^111]: 神经操作员：数据是否足以模拟世界？对物理启示机器学习影响的洞察

    Neural Operator: Is data all you need to model the world? An insight into the impact of Physics Informed Machine Learning. (arXiv:2301.13331v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.13331](http://arxiv.org/abs/2301.13331)

    本文探讨了如何将数据驱动方法与传统技术相结合，以解决工程和物理问题，并指出了机器学习方法的一些主要问题。

    

    常常使用偏微分方程（PDE）的数值近似来构建解决物理、工程和数学问题的方案，这些问题涉及到多个变量的函数，比如热传导或声音传播、流体流动、弹性、静电学、电动力学等。虽然这在解决许多复杂现象方面发挥了作用，但存在一些限制。常规方法如有限元法（FEM）和有限差分法（FDM）需要大量时间且计算成本高。相比之下，数据驱动的基于神经网络的方法提供了一种更快速、相对准确的替代方案，并具有离散不变性和分辨率不变性等优势。本文旨在深入了解数据驱动方法如何与传统技术相辅相成，解决工程和物理问题，同时指出机器学习方法的一些主要问题。

    Numerical approximations of partial differential equations (PDEs) are routinely employed to formulate the solution of physics, engineering and mathematical problems involving functions of several variables, such as the propagation of heat or sound, fluid flow, elasticity, electrostatics, electrodynamics, and more. While this has led to solving many complex phenomena, there are some limitations. Conventional approaches such as Finite Element Methods (FEMs) and Finite Differential Methods (FDMs) require considerable time and are computationally expensive. In contrast, data driven machine learning-based methods such as neural networks provide a faster, fairly accurate alternative, and have certain advantages such as discretization invariance and resolution invariance. This article aims to provide a comprehensive insight into how data-driven approaches can complement conventional techniques to solve engineering and physics problems, while also noting some of the major pitfalls of machine l
    
[^112]: 适用于所有领域的一个模型：基于协作域前缀调整的跨领域实体识别

    One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.10410](http://arxiv.org/abs/2301.10410)

    本论文提出了基于协作域前缀调整的跨领域实体识别，使用文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务，避免了先前的为每个领域结束一个全新的NER模型的问题。

    

    解决实际场景中低资源问题是跨领域实体识别的一个挑战性任务。先前典型的解决方案主要通过使用来自丰富资源领域的数据进行预训练语言模型(PLMs)获得NER模型并将其适应于目标领域。由于不同领域实体类型之间的不匹配问题，先前的方法通常调整所有PLMs的参数，从而为每个领域结束一个全新的NER模型。此外，当前的模型只关注于利用一个普通来源领域中的知识，而未能成功地将来自多个来源领域的知识转移到目标上。为了解决这些问题，我们基于文本到文本生成的PLM引入了协作域前缀调整跨领域NER(CP-NER)。具体来说，我们呈现了用于文本到文本生成的支撑领域相关指导来将知识转移至新域NER任务而无需结构修改。我们利用冻结的PLMs并进行协作域前缀调整。

    Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
    
[^113]: Tailor：为资源效率推断改变跳跃连接

    Tailor: Altering Skip Connections for Resource-Efficient Inference. (arXiv:2301.07247v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.07247](http://arxiv.org/abs/2301.07247)

    本文介绍了一种硬件-软件协同设计方法，通过逐渐去除或缩短神经网络中的跳跃连接来实现更高效的硬件推断。实验结果显示，该方法可以显著提高硬件资源利用率和性能，同时减少内存带宽的需求。

    

    深度神经网络使用跳跃连接来提高训练收敛性。然而，这些跳跃连接在硬件上很昂贵，需要额外的缓冲区，并增加了片上和片外存储器的利用和带宽需求。本文展示了通过硬件-软件协同设计方法来优化硬件上的跳跃连接。作者认为，尽管网络的跳跃连接对网络的学习是必要的，但可以在不损失精度的情况下去除或缩短跳跃连接，以提供更高效的硬件实现。作者引入了Tailor，这是一个代码设计工具，其硬件感知的训练算法逐渐去除或缩短一个完全训练好的网络的跳跃连接，从而降低它们的硬件代价。Tailor在片上数据流式体系结构中提高了34％的BRAM利用率，13％的FF利用率和16％的LUT利用率。Tailor提高了30％的性能，并降低了45％的内存带宽。

    Deep neural networks use skip connections to improve training convergence. However, these skip connections are costly in hardware, requiring extra buffers and increasing on- and off-chip memory utilization and bandwidth requirements. In this paper, we show that skip connections can be optimized for hardware when tackled with a hardware-software codesign approach. We argue that while a network's skip connections are needed for the network to learn, they can later be removed or shortened to provide a more hardware efficient implementation with minimal to no accuracy loss. We introduce Tailor, a codesign tool whose hardware-aware training algorithm gradually removes or shortens a fully trained network's skip connections to lower their hardware cost. Tailor improves resource utilization by up to 34% for BRAMs, 13% for FFs, and 16% for LUTs for on-chip, dataflow-style architectures. Tailor increases performance by 30% and reduces memory bandwidth by 45% for a 2D processing element array arc
    
[^114]: 自监督学习的多维视角综述: 算法、应用和未来趋势

    A Survey of Self-supervised Learning from Multiple Perspectives: Algorithms, Applications and Future Trends. (arXiv:2301.05712v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05712](http://arxiv.org/abs/2301.05712)

    本综述论文从算法、应用和趋势的角度概述了自监督学习的多维视角。它介绍了SSL算法的动机、共性和差异，以及在图像处理、计算机视觉和自然语言处理等领域中的典型应用。

    

    深度监督学习算法通常需要大量标记的样本来达到令人满意的性能。然而，收集和标记过多的样本可能成本高昂且耗时。作为无监督学习的子集，自监督学习（SSL）旨在从未标记的样本中学习有用的特征，而无需任何人工标注的标签。SSL最近引起了广泛关注，并且已经开发了许多相关算法。然而，目前很少有综述研究来解释不同的SSL变体之间的关系和演变。本文从算法、应用、三个主要趋势和待解问题的视角综述了各种SSL方法。首先，详细介绍了大多数SSL算法的动机，并比较了它们的共性和差异。其次，概述了SSL在图像处理和计算机视觉（CV）以及自然语言处理（NLP）等领域中的典型应用。

    Deep supervised learning algorithms generally require large numbers of labeled examples to achieve satisfactory performance. However, collecting and labeling too many examples can be costly and time-consuming. As a subset of unsupervised learning, self-supervised learning (SSL) aims to learn useful features from unlabeled examples without any human-annotated labels. SSL has recently attracted much attention and many related algorithms have been developed. However, there are few comprehensive studies that explain the connections and evolution of different SSL variants. In this paper, we provide a review of various SSL methods from the perspectives of algorithms, applications, three main trends, and open questions. First, the motivations of most SSL algorithms are introduced in detail, and their commonalities and differences are compared. Second, typical applications of SSL in domains such as image processing and computer vision (CV), as well as natural language processing (NLP), are dis
    
[^115]: 在稳定子存在的情况下的等变表示学习

    Equivariant Representation Learning in the Presence of Stabilizers. (arXiv:2301.05231v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05231](http://arxiv.org/abs/2301.05231)

    引入了一种称为EquIN的方法，用于学习在数据上具有一般群作用等变的表示。通过考虑稳定子，该方法可以提取数据的几何结构。

    

    我们引入了等变同构网络（EquIN）-一种用于学习与数据上的一般群作用等变的表示的方法。与现有的等变表示学习方式不同，EquIN适用于非自由群作用，即通过非平凡对称性稳定数据的情况。EquIN在群论中的轨道稳定子定理的理论基础上进行。这保证了理想的学习器仅通过等变性训练时推断出同构表示，并完全提取了数据的几何结构。我们对具有旋转对称性的图像数据集进行了实证研究，并证明考虑稳定子可以提高表示的质量。

    We introduce Equivariant Isomorphic Networks (EquIN) -- a method for learning representations that are equivariant with respect to general group actions over data. Differently from existing equivariant representation learners, EquIN is suitable for group actions that are not free, i.e., that stabilize data via nontrivial symmetries. EquIN is theoretically grounded in the orbit-stabilizer theorem from group theory. This guarantees that an ideal learner infers isomorphic representations while trained on equivariance alone and thus fully extracts the geometric structure of data. We provide an empirical investigation on image datasets with rotational symmetries and show that taking stabilizers into account improves the quality of the representations.
    
[^116]: 解决奖励假设

    Settling the Reward Hypothesis. (arXiv:2212.10420v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.10420](http://arxiv.org/abs/2212.10420)

    解决奖励假设，明确指明假设成立的目标和目的的隐含要求。

    

    奖励假设认为，“我们所说的目标和目的都可以想象为最大化接收到的标量信号（奖励）的累积总和的预期值。”我们的目标是完全解决这个假设。这将不仅仅是一个简单的肯定或否定，而是完全指明假设成立的目标和目的的隐含要求。

    The reward hypothesis posits that, "all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)." We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.
    
[^117]: 使用语言模型提示进行推理：一项调查

    Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09597](http://arxiv.org/abs/2212.09597)

    本文提供了使用语言模型提示进行推理的前沿研究综合调查。讨论了新兴推理能力出现的潜在原因，并提供系统资源帮助初学者。

    

    推理作为复杂问题解决的重要能力，可以为医疗诊断、谈判等各种实际应用提供后端支持。本文对使用语言模型提示进行推理的前沿研究进行了综合调查。我们介绍了研究成果的比较和总结，并提供了系统资源以帮助初学者。我们还讨论了新兴推理能力出现的潜在原因，并突出了未来的研究方向。资源可在 https://github.com/zjunlp/Prompt4ReasoningPapers 上获取（定期更新）。

    Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions. Resources are available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated periodically).
    
[^118]: 通过可扩展的无蜂窝大规模MIMO进行空中联邦学习（OTA-FL）

    Over-The-Air Federated Learning Over Scalable Cell-free Massive MIMO. (arXiv:2212.06482v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2212.06482](http://arxiv.org/abs/2212.06482)

    本论文研究了如何利用无蜂窝大规模MIMO支持联邦边缘学习，并提出了一种实际的OTA-FL实现方法，通过空中计算减轻了联邦学习在无线网络中的通信开销，并验证了无蜂窝大规模MIMO的优势。

    

    无蜂窝大规模MIMO作为一种有前景的未来无线通信技术，预计将与传统蜂窝系统相比，提供均匀覆盖和高频谱效率。本文研究了无蜂窝大规模MIMO如何支持联邦边缘学习。利用无线多址信道的可加性特性，通过空中计算的方式，客户端同时将他们的本地更新发送到相同的通信资源上。这种方法被称为空中联邦学习（OTA-FL），已经证明可以减轻在无线网络上进行联邦学习的通信开销。考虑到信道相关性和中央服务器上仅有的不完全信道状态信息，我们提出了一种在无蜂窝大规模MIMO上实现OTA-FL的实际方法。我们通过理论分析和实验研究了该实现的收敛性，验证了无蜂窝大规模MIMO的优势。

    Cell-free massive MIMO is emerging as a promising technology for future wireless communication systems, which is expected to offer uniform coverage and high spectral efficiency compared to classical cellular systems. We study in this paper how cell-free massive MIMO can support federated edge learning. Taking advantage of the additive nature of the wireless multiple access channel, over-the-air computation is exploited, where the clients send their local updates simultaneously over the same communication resource. This approach, known as over-the-air federated learning (OTA-FL), is proven to alleviate the communication overhead of federated learning over wireless networks. Considering channel correlation and only imperfect channel state information available at the central server, we propose a practical implementation of OTA-FL over cell-free massive MIMO. The convergence of the proposed implementation is studied analytically and experimentally, confirming the benefits of cell-free mas
    
[^119]: FedALA: 自适应局部聚合用于个性化联邦学习

    FedALA: Adaptive Local Aggregation for Personalized Federated Learning. (arXiv:2212.01197v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01197](http://arxiv.org/abs/2212.01197)

    FedALA是一种用于个性化联邦学习的方法，通过自适应局部聚合（ALA）模块来解决统计异质性问题，并在广泛的实验证明中超过了11种最先进的基准模型。

    

    联邦学习中的一个关键挑战是统计异质性，这会影响全局模型在每个客户端上的泛化能力。为了解决这个问题，我们提出了一种名为Federated learning with Adaptive Local Aggregation（FedALA）的方法，通过在个性化联邦学习中捕捉全局模型对客户端模型中所需的信息。FedALA的关键组成部分是自适应局部聚合（ALA）模块，它可以根据每个客户端上的局部目标自适应聚合下载的全局模型和本地模型以在每次迭代中初始化本地模型。为了评估FedALA的有效性，我们在计算机视觉和自然语言处理领域使用了五个基准数据集进行了大量的实验证明。FedALA在测试准确性方面比十一种最先进的基准模型取得了最多3.27%的改进。此外，我们还将ALA模块应用于其他联邦学习方法，并在测试准确性方面取得了最多24.19%的改进。

    A key challenge in federated learning (FL) is the statistical heterogeneity that impairs the generalization of the global model on each client. To address this, we propose a method Federated learning with Adaptive Local Aggregation (FedALA) by capturing the desired information in the global model for client models in personalized FL. The key component of FedALA is an Adaptive Local Aggregation (ALA) module, which can adaptively aggregate the downloaded global model and local model towards the local objective on each client to initialize the local model before training in each iteration. To evaluate the effectiveness of FedALA, we conduct extensive experiments with five benchmark datasets in computer vision and natural language processing domains. FedALA outperforms eleven state-of-the-art baselines by up to 3.27% in test accuracy. Furthermore, we also apply ALA module to other federated learning methods and achieve up to 24.19% improvement in test accuracy.
    
[^120]: 具有异构差分隐私的分散矩阵分解

    Decentralized Matrix Factorization with Heterogeneous Differential Privacy. (arXiv:2212.00306v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00306](http://arxiv.org/abs/2212.00306)

    本文提出了一种用于不可信推荐者的异构差分隐私矩阵分解算法，通过修改拉伸机制和重新缩放方案，实现了隐私和准确性的权衡。

    

    传统的矩阵分解依赖于对用户数据的集中收集来进行推荐，这可能会增加隐私泄露的风险，特别是在推荐者不可信的情况下。现有的差分隐私矩阵分解方法要么假设推荐者是可信的，要么只能为所有用户和物品提供统一级别的隐私保护，当推荐者不可信时。在本文中，我们提出了一种新颖的用于不可信推荐者的异构差分隐私矩阵分解算法（称为HDPMF）。据我们所知，我们是第一个在不可信推荐者场景下实现分布式矩阵分解的异构差分隐私的方法。具体而言，我们的框架使用改进的拉伸机制和创新的重新缩放方案，在隐私和准确性之间实现更好的权衡。同时，通过适当分配隐私预算，我们可以捕捉到用户内的同质隐私偏好。

    Conventional matrix factorization relies on centralized collection of users' data for recommendation, which might introduce an increased risk of privacy leakage especially when the recommender is untrusted. Existing differentially private matrix factorization methods either assume the recommender is trusted, or can only provide a uniform level of privacy protection for all users and items with untrusted recommender. In this paper, we propose a novel Heterogeneous Differentially Private Matrix Factorization algorithm (denoted as HDPMF) for untrusted recommender. To the best of our knowledge, we are the first to achieve heterogeneous differential privacy for decentralized matrix factorization in untrusted recommender scenario. Specifically, our framework uses modified stretching mechanism with an innovative rescaling scheme to achieve better trade off between privacy and accuracy. Meanwhile, by allocating privacy budget properly, we can capture homogeneous privacy preference within a use
    
[^121]: 带有圆环核的模式注意力变换器

    Pattern Attention Transformer with Doughnut Kernel. (arXiv:2211.16961v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.16961](http://arxiv.org/abs/2211.16961)

    本论文提出了一种新的模式注意力变换器(PAT)，它采用了新的圆环核设计，以解决图像分类中像素高分辨率的问题。

    

    本文介绍了一种新的体系结构，即Pattern Attention Transformer（PAT），该体系结构由新的圆环核组成。与NLP领域的标记不同，计算机视觉中的Transformer解决了处理图像中像素高分辨率的问题。在ViT中，图像被切成方形的补丁。作为ViT的后续，Swin Transformer提出了一个额外的移位步骤以减少固定边界的存在，这也导致“两个连接的Swin Transformer块”成为模型的最小单位。继承了补丁/窗口的想法，我们的圆环核进一步增强了补丁的设计。它用传感器和更新两种区域代替了线型边界，这是基于自我关注的理解（称为QKVA网格）。圆环核还带来了一个关于核形状的新话题，超越了方形。为了验证其在图像分类上的性能，PAT被设计为由定期八边形形状的Transformer块组成。

    We present in this paper a new architecture, the Pattern Attention Transformer (PAT), that is composed of the new doughnut kernel. Compared with tokens in the NLP field, Transformer in computer vision has the problem of handling the high resolution of pixels in images. In ViT, an image is cut into square-shaped patches. As the follow-up of ViT, Swin Transformer proposes an additional step of shifting to decrease the existence of fixed boundaries, which also incurs 'two connected Swin Transformer blocks' as the minimum unit of the model. Inheriting the patch/window idea, our doughnut kernel enhances the design of patches further. It replaces the line-cut boundaries with two types of areas: sensor and updating, which is based on the comprehension of self-attention (named QKVA grid). The doughnut kernel also brings a new topic about the shape of kernels beyond square. To verify its performance on image classification, PAT is designed with Transformer blocks of regular octagon shape doughn
    
[^122]: AdsorbML：使用可推广的机器学习势函数提高吸附能计算效率的一大飞跃

    AdsorbML: A Leap in Efficiency for Adsorption Energy Calculations using Generalizable Machine Learning Potentials. (arXiv:2211.16486v3 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2211.16486](http://arxiv.org/abs/2211.16486)

    本文展示了如何利用机器学习势函数更准确地、以更高效率地识别低能吸附物-表面配置。我们的算法在准确性和效率之间提供了一系列权衡，其中一个平衡选项在计算中发现最低能量结构的准确率为87.36％，同时计算速度提高了2000倍。

    

    在催化计算中，计算方法在设计各种应用的催化剂中扮演着越来越重要的角色。许多计算方法的常见任务是准确计算感兴趣的吸附物和催化剂表面的吸附能。传统上，识别低能吸附物-表面构型依赖于启发式方法和研究人员的直觉。随着高通量筛选的需求增加，仅靠启发式和直觉变得困难。在本文中，我们证明机器学习势函数可以更准确、高效地识别低能吸附物-表面配置。我们的算法在准确性和效率之间提供一系列权衡，其中一个平衡选项在计算中发现最低能量结构的准确率为87.36％，同时计算速度提高了2000倍。为了标准化基准测试，我们引入了开放催化剂密集数据集。

    Computational catalysis is playing an increasingly significant role in the design of catalysts across a wide range of applications. A common task for many computational methods is the need to accurately compute the adsorption energy for an adsorbate and a catalyst surface of interest. Traditionally, the identification of low energy adsorbate-surface configurations relies on heuristic methods and researcher intuition. As the desire to perform high-throughput screening increases, it becomes challenging to use heuristics and intuition alone. In this paper, we demonstrate machine learning potentials can be leveraged to identify low energy adsorbate-surface configurations more accurately and efficiently. Our algorithm provides a spectrum of trade-offs between accuracy and efficiency, with one balanced option finding the lowest energy configuration 87.36% of the time, while achieving a 2000x speedup in computation. To standardize benchmarking, we introduce the Open Catalyst Dense dataset con
    
[^123]: 一种具有高效优化和量子适用性的费米子神经网络

    A fermion neural network with efficient optimization and quantum applicability. (arXiv:2211.05793v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.05793](http://arxiv.org/abs/2211.05793)

    本文提出了一种费米子神经网络（FNN），它将输入作为初始层，输出物理特性，建立了一种高效的优化方法，可应用于具有相互作用的硬量子系统，而且能够精确地确定拓扑相和紧凑电荷序，其量子特性带来多种优势。

    

    经典人工神经网络已在机器学习领域取得了广泛成功。本文提出了一种费米子神经网络（FNN），其物理特性（例如局部态密度或条件电导）在输入作为初始层后作为输出。与反向传播类似，我们建立了一种高效优化方法，使FNN在具有挑战性的机器学习基准测试上表现出竞争性能。FNN也直接应用于量子系统，包括具有相互作用的硬系统，并在无预处理或假设的情况下提供原位分析。在机器学习之后，FNN精确地确定拓扑相和紧凑电荷序。它们的量子特性也带来了各种优势：量子相关性使网络连接更加通用，并且可以深入了解消失的梯度问题，量子纠缠则为可解释的机器学习打开了新的途径等。

    Classical artificial neural networks have witnessed widespread successes in machine-learning applications. Here, we propose fermion neural networks (FNNs) whose physical properties, such as local density of states or conditional conductance, serve as outputs, once the inputs are incorporated as an initial layer. Comparable to back-propagation, we establish an efficient optimization, which entitles FNNs to competitive performance on challenging machine-learning benchmarks. FNNs also directly apply to quantum systems, including hard ones with interactions, and offer in-situ analysis without preprocessing or presumption. Following machine learning, FNNs precisely determine topological phases and emergent charge orders. Their quantum nature also brings various advantages: quantum correlation entitles more general network connectivity and insight into the vanishing gradient problem, quantum entanglement opens up novel avenues for interpretable machine learning, etc.
    
[^124]: 生成式知识图谱构建综述

    Generative Knowledge Graph Construction: A Review. (arXiv:2210.12714v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.12714](http://arxiv.org/abs/2210.12714)

    本文综述了生成式知识图谱构建领域的最新进展，包括方法分类和优劣分析，并提出了未来的研究方向。

    

    生成式知识图谱构建（KGC）是指利用序列到序列框架构建灵活且可适用于广泛任务的知识图谱。本研究总结了生成式知识图谱构建领域中近期的重要进展，对不同的生成目标从理论和实证分析角度分别讨论了各种方法的优势和不足，并提出了未来有潜力的研究方向。我们的贡献有三个方面：（1）我们提供了生成式KGC方法的详细、完整的分类体系；（2）我们对生成式KGC方法进行了理论和实证分析；（3）我们提出了几个未来可以发展的研究方向。

    Generative Knowledge Graph Construction (KGC) refers to those methods that leverage the sequence-to-sequence framework for building knowledge graphs, which is flexible and can be adapted to widespread tasks. In this study, we summarize the recent compelling progress in generative knowledge graph construction. We present the advantages and weaknesses of each paradigm in terms of different generation targets and provide theoretical insight and empirical analysis. Based on the review, we suggest promising research directions for the future. Our contributions are threefold: (1) We present a detailed, complete taxonomy for the generative KGC methods; (2) We provide a theoretical and empirical analysis of the generative KGC methods; (3) We propose several research directions that can be developed in the future.
    
[^125]: 以架构感知参考作为提示提高了数据有效的知识图谱构建

    Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10709](http://arxiv.org/abs/2210.10709)

    提出了一种以检索增强的架构感知参考作为提示的方法，可动态利用人类注释和弱监督数据所继承的架构和知识，指导生成具有更好语义连贯性和一致性的结构化知识，从而在数据效率和知识质量方面具有优越性。

    

    随着预训练语言模型的发展，许多基于提示的方法被提出并在数据有效的知识图谱构建中取得了令人瞩目的表现。然而，现有的基于提示的学习方法仍存在几个潜在的限制：（i）自然语言和预定义模式的输出结构化知识之间的语义差距，这意味着模型无法充分利用受限模板的语义知识；（ii）基于局部个体实例的表示学习限制了性能，给定了不充足的特征，这些特征不能释放预先训练语言模型的潜在类比能力。受这些观察的启发，我们提出了一种检索增强的方法，使用检索得到的架构感知参考作为提示，提高了数据有效的知识图谱构建的语义连贯性和一致性。在两个标准数据集上的实验结果表明，相比现有的基于提示和非提示的方法，我们提出的方法在数据效率和知识质量方面具有优越性。

    With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
    
[^126]: 实现真实低资源关系抽取: 针对具有实证基准研究的论文

    Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study. (arXiv:2210.10678v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.10678](http://arxiv.org/abs/2210.10678)

    本文针对低资源环境中的关系抽取进行了实证研究，并提出了三种方案来提高性能，包括使用提示方法、平衡方法和数据增强技术。通过对8个关系抽取数据集的广泛比较，实验结果表明，虽然基于提示的调整有益于低资源关系抽取，但仍有改进空间，尤其是跨句子上下文中的多个关系三元组的抽取。

    

    本文提出了一项针对低资源环境中构建关系抽取系统的实证研究。基于最近的预训练语言模型，我们全面调查了三种方案来评估低资源环境下的性能：(i) 使用少量标记数据的不同类型的提示方法； (ii) 多样化的平衡方法来解决长尾分布问题； (iii) 数据增强技术和自训练来生成更多领域内标记数据。我们创建了一个包含8个关系抽取(RE) 数据集的基准，涵盖了不同的语言、领域和上下文，并对所提出的方案进行了广泛的比较。我们的实验证明：(i) 虽然基于提示的调整在低资源关系抽取中是有益的，但仍有很大的改进潜力，特别是在提取跨句子上下文中的多个关系三元组方面； (ii) 平衡方法并不总是有助于长尾分布的关系抽取。

    This paper presents an empirical study to build relation extraction systems in low-resource settings. Based upon recent pre-trained language models, we comprehensively investigate three schemes to evaluate the performance in low-resource settings: (i) different types of prompt-based methods with few-shot labeled data; (ii) diverse balancing methods to address the long-tailed distribution issue; (iii) data augmentation technologies and self-training to generate more labeled in-domain data. We create a benchmark with 8 relation extraction (RE) datasets covering different languages, domains and contexts and perform extensive comparisons over the proposed schemes with combinations. Our experiments illustrate: (i) Though prompt-based tuning is beneficial in low-resource RE, there is still much potential for improvement, especially in extracting relations from cross-sentence contexts with multiple relational triples; (ii) Balancing methods are not always helpful for RE with long-tailed distr
    
[^127]: 从态密度中提取带结构参数的深度学习方法：以三层石墨烯为案例研究

    Deep learning extraction of band structure parameters from density of states: a case study on trilayer graphene. (arXiv:2210.06310v2 [cond-mat.mes-hall] UPDATED)

    [http://arxiv.org/abs/2210.06310](http://arxiv.org/abs/2210.06310)

    本论文提出了一个深度学习方法，可以从实验数据中准确地推导出材料的带结构参数。通过研究三层石墨烯的穿透场电容测量，证明了该方法的有效性。

    

    二维材料的发展导致了一系列新颖、高质量的化合物，其复杂性逐渐增加。一个全面的定量理论的关键要求是准确确定这些材料的带结构参数。然而，由于复杂的带结构和实验探测方式的间接性质，这个任务具有挑战性。本研究引入了一个通用框架，利用深度神经网络从实验数据中推导带结构参数。我们将该方法应用于三层石墨烯的穿透场电容测量，这是一种有效探测其密度态的方式。首先，我们证明了经过训练的深度网络在紧束缚参数函数关系的穿透场电容预测方面具有准确性。接下来，我们利用训练网络的快速、准确的预测结果，直接从实验数据中自动确定紧束缚参数，提取参数。

    The development of two-dimensional materials has resulted in a diverse range of novel, high-quality compounds with increasing complexity. A key requirement for a comprehensive quantitative theory is the accurate determination of these materials' band structure parameters. However, this task is challenging due to the intricate band structures and the indirect nature of experimental probes. In this work, we introduce a general framework to derive band structure parameters from experimental data using deep neural networks. We applied our method to the penetration field capacitance measurement of trilayer graphene, an effective probe of its density of states. First, we demonstrate that a trained deep network gives accurate predictions for the penetration field capacitance as a function of tight-binding parameters. Next, we use the fast and accurate predictions from the trained network to automatically determine tight-binding parameters directly from experimental data, with extracted parame
    
[^128]: 通过缩放协方差矩阵自适应MAP模拟来训练多样化的高维度控制器

    Training Diverse High-Dimensional Controllers by Scaling Covariance Matrix Adaptation MAP-Annealing. (arXiv:2210.02622v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.02622](http://arxiv.org/abs/2210.02622)

    本论文提出了三种新的CMA-MAE变体，利用高效近似方法提高了其可扩展性，相较于ES基线在基准测试中表现更好，并且达到或超过最先进的深度强化学习算法的性能。

    

    在模拟中预先训练多样化的神经网络控制器使得机器人能够适应运动任务中的损伤。然而，寻找多样性高且性能优异的控制器需要昂贵的网络训练和大量的超参数调整。相比而言，基于进化策略（ES）的质量多样性算法“协方差矩阵自适应MAP模拟”没有这些限制，并在标准的QD基准测试中取得了最先进的性能。然而，由于其二次复杂度，CMA-MAE无法扩展到现代神经网络控制器。我们利用ES中的高效近似方法提出了三种新的CMA-MAE变体，这些变体能够扩展到高维度。我们的实验表明，这些变体在机器人运动任务基准测试中优于ES基线，同时可与最先进的基于深度强化学习的质量多样性算法媲美或超越其性能。

    Pre-training a diverse set of neural network controllers in simulation has enabled robots to adapt online to damage in robot locomotion tasks. However, finding diverse, high-performing controllers requires expensive network training and extensive tuning of a large number of hyperparameters. On the other hand, Covariance Matrix Adaptation MAP-Annealing (CMA-MAE), an evolution strategies (ES)-based quality diversity algorithm, does not have these limitations and has achieved state-of-the-art performance on standard QD benchmarks. However, CMA-MAE cannot scale to modern neural network controllers due to its quadratic complexity. We leverage efficient approximation methods in ES to propose three new CMA-MAE variants that scale to high dimensions. Our experiments show that the variants outperform ES-based baselines in benchmark robotic locomotion tasks, while being comparable with or exceeding state-of-the-art deep reinforcement learning-based quality diversity algorithms.
    
[^129]: 通过不变性约束学习实现自动数据增强

    Automatic Data Augmentation via Invariance-Constrained Learning. (arXiv:2209.15031v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15031](http://arxiv.org/abs/2209.15031)

    本论文提出了一种通过在解决学习任务时自动适应数据增强的方法，将数据增强表述为基于不变性约束的学习问题，并利用蒙特卡洛采样来解决，提出的算法能够优化学习任务的解决方案。

    

    基于底层数据结构（如对称性或变换不变性），经常被利用来改善学习任务的解决方案。然而，将这些属性嵌入到模型或学习算法中可能具有挑战性且计算密集。另一方面，数据增强通过对输入数据应用多个变换来在训练过程中引入这些对称性。尽管广泛应用，但其有效性取决于应用哪些变换，何时应用以及多久应用的选择。事实上，经验和理论证据都表明，不加选择地使用数据增强可能会引入偏见，超过其好处。本研究通过在解决学习任务时自动适应数据增强来解决这些问题。为此，它将数据增强表述为一个基于不变性约束的学习问题，并利用马尔可夫链蒙特卡洛（MCMC）采样来解决它。结果是一个实用算法，不仅能够自动适应数据增强，而且能够优化学习任务的解决方案。

    Underlying data structures, such as symmetries or invariances to transformations, are often exploited to improve the solution of learning tasks. However, embedding these properties in models or learning algorithms can be challenging and computationally intensive. Data augmentation, on the other hand, induces these symmetries during training by applying multiple transformations to the input data. Despite its ubiquity, its effectiveness depends on the choices of which transformations to apply, when to do so, and how often. In fact, there is both empirical and theoretical evidence that the indiscriminate use of data augmentation can introduce biases that outweigh its benefits. This work tackles these issues by automatically adapting the data augmentation while solving the learning task. To do so, it formulates data augmentation as an invariance-constrained learning problem and leverages Monte Carlo Markov Chain (MCMC) sampling to solve it. The result is a practical algorithm that not only
    
[^130]: 对于瞬变光曲线逼近的神经网络方法的性质理解

    Understanding of the properties of neural network approaches for transient light curve approximations. (arXiv:2209.07542v2 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2209.07542](http://arxiv.org/abs/2209.07542)

    对于瞬变光曲线逼近，本文研究了基于神经网络的多种逼近方法，以生成时间步长规则的时间序列。

    

    现代时域光度测量调查收集了大量各种天体物体的观测数据，大规模调查即将到来，将会提供更多关于它们性质的信息。对于超新星等瞬变天体来说，光谱追踪是特别关键的，但大部分这些天体还未经过这样的研究。} {流量时间序列被积极用作光度分类和表征的廉价替代方案，例如峰值标识和亮度衰减估计。然而，收集到的时间序列是多维的且采样不规则，并且包含异常值，没有明确定义的系统性不确定性。本文介绍了对于逼近经过时间和波长观测的光曲线的最佳方法的研究，目的是生成每个通道下具有规则时间步长的时间序列。

    Modern-day time-domain photometric surveys collect a lot of observations of various astronomical objects and the coming era of large-scale surveys will provide even more information on their properties. Spectroscopic follow-ups are especially crucial for transients such as supernovae and most of these objects have not been subject to such studies. }{Flux time series are actively used as an affordable alternative for photometric classification and characterization, for instance, peak identifications and luminosity decline estimations. However, the collected time series are multidimensional and irregularly sampled, while also containing outliers and without any well-defined systematic uncertainties. This paper presents a search for the best-performing methods to approximate the observed light curves over time and wavelength for the purpose of generating time series with regular time steps in each passband.}{We examined several light curve approximation methods based on neural networks su
    
[^131]: 基于可微物理引擎的索驱动机器人的真实世界到仿真世界的控制转移

    Real2Sim2Real Transfer for Control of Cable-driven Robots via a Differentiable Physics Engine. (arXiv:2209.06261v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.06261](http://arxiv.org/abs/2209.06261)

    本文描述了一种基于可微物理引擎的真实世界到仿真世界转移的策略，该策略通过对真实机器人的有限数据进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。该策略在索驱动张力结构机器人上得到了测试，并证明了其有效性。

    

    张力结构机器人由坚硬的杆和柔软的缆绳组成，具有高强度重量比和显著的变形能力，使其能够在非结构化的地形中航行并在严峻的撞击中存活。然而，由于维度高、动力复杂且耦合结构使得它们难以控制。基于物理的仿真是开发可以转移到实际机器人的运动策略的有前途途径。然而，由于实到虚之间的显著差距，对张力结构机器人进行建模是一个复杂的任务。为了解决这个问题，本文描述了一种基于不同iable物理引擎的张力结构机器人的真实世界到仿真世界的转移策略(R2S2R)。该策略基于一个可训练的可微物理引擎，通过对真实机器人的有限数据进行训练，并将包括物理属性的离线测量，如质量和几何体的各种机器人部件，以及使用随机控制策略的轨迹观察。利用来自真实机器人的数据，物理引擎可以进行迭代训练，以减少实到虚之间的差距并产生准确的仿真。这种R2S2R策略在索驱动张力结构机器人上得到了测试，并证明了使用可微物理引擎开发可以转移到实际机器人的控制策略的有效性。

    Tensegrity robots, composed of rigid rods and flexible cables, exhibit high strength-to-weight ratios and significant deformations, which enable them to navigate unstructured terrains and survive harsh impacts. They are hard to control, however, due to high dimensionality, complex dynamics, and a coupled architecture. Physics-based simulation is a promising avenue for developing locomotion policies that can be transferred to real robots. Nevertheless, modeling tensegrity robots is a complex task due to a substantial sim2real gap. To address this issue, this paper describes a Real2Sim2Real (R2S2R) strategy for tensegrity robots. This strategy is based on a differentiable physics engine that can be trained given limited data from a real robot. These data include offline measurements of physical properties, such as mass and geometry for various robot components, and the observation of a trajectory using a random control policy. With the data from the real robot, the engine can be iterativ
    
[^132]: TabPFN：在一秒内解决小型表格分类问题的Transformer

    TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second. (arXiv:2207.01848v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.01848](http://arxiv.org/abs/2207.01848)

    TabPFN是一种可以在不到一秒钟内完成小型表格数据集的监督分类的Transformer，无需超参数调整，并且具有竞争力。它使用先验适应网络（PFN）逼近基于先验的贝叶斯推断，先验融合了因果推理的思想。

    

    本文提出了TabPFN，一种经过训练的Transformer，可以在不到一秒钟的时间内完成小型表格数据集的监督分类，无需超参数调整，并且在分类方法的最新状态下具有竞争力。TabPFN完全包含在我们网络的权重中，接受训练和测试样本作为设置值输入，并在单个前向传递中为整个测试集提供预测。TabPFN是一种先验适应网络（PFN），只需要线下训练一次，即可逼近基于我们的先验的合成数据集上的贝叶斯推断。这个先验融合了因果推理的思想：它包括一个大的结构因果模型空间，偏好于简单结构。在OpenML-CC18套件的18个包含最多1000个训练数据点、最多100个纯数值特征且无缺失值、最多10个类别的数据集中，我们展示了我们的方法明显优于提升树，与复杂的最新AutoM方法表现相当。

    We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoM
    
[^133]: 信息论和变分推断中的基于平方和松弛的方法

    Sum-of-Squares Relaxations for Information Theory and Variational Inference. (arXiv:2206.13285v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2206.13285](http://arxiv.org/abs/2206.13285)

    本论文研究了基于平方和松弛方法在信息论和变分推断中的应用。通过使用这种方法，我们提出了计算$f$-divergences的凸松弛算法，其中涉及到从非局部协方差矩阵计算这些divergences的问题。这些结果对于数据科学中的多个应用具有重要意义。

    

    我们考虑了香农相对熵的扩展，称为$f$-divergences。这些divergences通常与三个经典的相关计算问题相关联：（a）从矩估计，（b）计算归一化积分，以及（c）概率模型的变分推断。这些问题通过凸对偶性相互关联，对于所有这些问题，都有许多数据科学中的应用，并且我们旨在提出能够保持原始问题特性（如潜在凸性或单调性）的计算上可行的近似算法。为了实现这一目标，我们从与给定特征向量相关的非局部协方差矩阵计算这些divergences的一系列凸松弛开始：从通常不易处理的最优下界开始，我们考虑了一个额外的基于“平方和”的松弛，它现在作为半定规划可以在多项式时间内计算。

    We consider extensions of the Shannon relative entropy, referred to as $f$-divergences.Three classical related computational problems are typically associated with these divergences: (a) estimation from moments, (b) computing normalizing integrals, and (c) variational inference in probabilistic models. These problems are related to one another through convex duality, and for all them, there are many applications throughout data science, and we aim for computationally tractable approximation algorithms that preserve properties of the original problem such as potential convexity or monotonicity. In order to achieve this, we derive a sequence of convex relaxations for computing these divergences from non-centered covariance matrices associated with a given feature vector: starting from the typically non-tractable optimal lower-bound, we consider an additional relaxation based on ``sums-of-squares'', which is is now computable in polynomial time as a semidefinite program. We also provide c
    
[^134]: 鲁棒飞行控制的神经移动视界估计

    Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.10397](http://arxiv.org/abs/2206.10397)

    本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    

    估计和应对干扰对于四旋翼飞行控制至关重要。现有的估计器通常需要对特定飞行场景进行大量调整，或者经过广泛的地面真实干扰数据训练，才能实现令人满意的性能。在本文中，我们提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整由神经网络建模的关键参数，并适应不同的飞行场景。我们通过推导与加权矩阵相关的MHE估计的解析梯度，实现了将MHE作为可学习层嵌入神经网络以实现高效学习的无缝融合。有趣的是，我们证明可以使用递归形式的卡尔曼滤波器高效地计算出梯度。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
    
[^135]: 多重检验框架用于离群分布检测

    Multiple Testing Framework for Out-of-Distribution Detection. (arXiv:2206.09522v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.09522](http://arxiv.org/abs/2206.09522)

    本研究提出了一个多重检验框架用于离群分布检测的问题，包括了定义OOD概念和提供强有力保证的方法，与之前的基于阈值的测试相比，在不同类型的OOD实例中表现更一致。

    

    我们研究离群分布（OOD）检测的问题，即在推理时检测学习算法的输出是否可信。尽管之前的工作中提出了一些OOD检测的测试方法，但缺乏一个形式化的框架来研究这个问题。我们提出了一个OOD概念的定义，包括输入分布和学习算法，这为构建强大的OOD检测测试提供了启示。我们提出了一种多重假设检验启发的过程，使用符合性p值系统地结合学习算法中的任意数量的不同统计量。我们进一步对将入群样本错误分类为OOD的概率提供了强有力的保证。在实验中，我们发现之前工作中提出的基于阈值的测试在特定场景下表现良好，但在不同类型的OOD实例中的表现并不一致。相比之下，我们提出的方法结合了m个不同统计量。

    We study the problem of Out-of-Distribution (OOD) detection, that is, detecting whether a learning algorithm's output can be trusted at inference time. While a number of tests for OOD detection have been proposed in prior work, a formal framework for studying this problem is lacking. We propose a definition for the notion of OOD that includes both the input distribution and the learning algorithm, which provides insights for the construction of powerful tests for OOD detection. We propose a multiple hypothesis testing inspired procedure to systematically combine any number of different statistics from the learning algorithm using conformal p-values. We further provide strong guarantees on the probability of incorrectly classifying an in-distribution sample as OOD. In our experiments, we find that threshold-based tests proposed in prior work perform well in specific settings, but not uniformly well across different types of OOD instances. In contrast, our proposed method that combines m
    
[^136]: 检索增强的图神经网络的实证研究

    An Empirical Study of Retrieval-enhanced Graph Neural Networks. (arXiv:2206.00362v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00362](http://arxiv.org/abs/2206.00362)

    这项研究考察了检索增强的图神经网络在图数据集中的有效性，设计了一种称为GRAPHRETRIEVAL的检索增强方案，为图神经网络中学习的有用信息提供了增强。

    

    图神经网络（GNNs）是图表示学习的有效工具。大多数GNNs依赖于递归的邻居聚合方案，称为消息传递，因此它们的理论表达能力限于一阶Weisfeiler-Lehman测试（1-WL）。解决这个挑战的一种有效方法是明确地检索用于增强GNN模型的一些已注释的示例。虽然在语言和视觉领域中已经证明了检索增强模型的有效性，但是当应用于图数据集时，检索增强的GNNs的有效性问题仍然是一个悬而未决的问题。出于这个目的，我们想探索检索思想如何帮助增强图神经网络中学习的有用信息，并设计了一个称为GRAPHRETRIEVAL的检索增强方案，该方案对于图神经网络模型的选择是不可知的。在GRAPHRETRIEVAL中，对于每个输入图，从现有数据库中检索出相似的图以及它们的真实标签。

    Graph Neural Networks (GNNs) are effective tools for graph representation learning. Most GNNs rely on a recursive neighborhood aggregation scheme, named message passing, thereby their theoretical expressive power is limited to the first-order Weisfeiler-Lehman test (1-WL). An effective approach to this challenge is to explicitly retrieve some annotated examples used to enhance GNN models. While retrieval-enhanced models have been proved to be effective in many language and vision domains, it remains an open question how effective retrieval-enhanced GNNs are when applied to graph datasets. Motivated by this, we want to explore how the retrieval idea can help augment the useful information learned in the graph neural networks, and we design a retrieval-enhanced scheme called GRAPHRETRIEVAL, which is agnostic to the choice of graph neural network models. In GRAPHRETRIEVAL, for each input graph, similar graphs together with their ground-true labels are retrieved from an existing database. 
    
[^137]: DeepCluE：通过深度神经网络中的多层集成增强图像聚类

    DeepCluE: Enhanced Image Clustering via Multi-layer Ensembles in Deep Neural Networks. (arXiv:2206.00359v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.00359](http://arxiv.org/abs/2206.00359)

    本文提出了DeepCluE方法，通过利用深度神经网络中的多个层次，桥接了深度聚类和集成聚类之间的差距，从而增强了图像聚类的性能。

    

    深度聚类近年来成为复杂数据聚类的一种有前景的技术。尽管有了相当大的进展，但以前的深度聚类工作大多只利用单层表示构建或学习最终的聚类，例如在最后一个全连接层上执行K-means聚类或将一些聚类损失与特定层相关联，忽视了联合利用多层表示以增强深度聚类性能的可能性。鉴于此，本文提出了一种通过集成（DeepCluE）的深度聚类方法，它通过利用深度神经网络中的多个层的力量来弥合深度聚类和集成聚类之间的差距。具体而言，我们利用具有权重共享的卷积神经网络作为骨干网络，该网络通过实例级对比学习（通过实例投影仪）和簇级对比学习（通过簇投影仪）进行训练

    Deep clustering has recently emerged as a promising technique for complex data clustering. Despite the considerable progress, previous deep clustering works mostly build or learn the final clustering by only utilizing a single layer of representation, e.g., by performing the K-means clustering on the last fully-connected layer or by associating some clustering loss to a specific layer, which neglect the possibilities of jointly leveraging multi-layer representations for enhancing the deep clustering performance. In view of this, this paper presents a Deep Clustering via Ensembles (DeepCluE) approach, which bridges the gap between deep clustering and ensemble clustering by harnessing the power of multiple layers in deep neural networks. In particular, we utilize a weight-sharing convolutional neural network as the backbone, which is trained with both the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector) i
    
[^138]: 解决公平分类中战略操纵的差异

    Addressing Strategic Manipulation Disparities in Fair Classification. (arXiv:2205.10842v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2205.10842](http://arxiv.org/abs/2205.10842)

    该论文研究了公平分类中存在的战略操纵差异问题，提出了一个受约束的优化框架来解决这个问题。

    

    在现实世界的分类环境中，如贷款申请评估或在线平台上的内容审查，个体通过战略性地更新其特征来增加其获得特定（积极）决策的可能性（以一定的成本）。然而，当不同人口群体具有不同的特征分布或支付不同的更新成本时，先前的研究表明，来自少数群体的个体付出更高的成本来更新其特征。公平分类旨在通过限制分类器满足统计公平性属性来解决此类分类器性能差异。然而，我们发现标准的公平性约束并不能确保受约束的分类器减少战略操纵成本的差异。为了解决战略环境中的这种偏差并为战略操纵提供平等机会，我们提出了一个受约束的优化框架，构建分类器。

    In real-world classification settings, such as loan application evaluation or content moderation on online platforms, individuals respond to classifier predictions by strategically updating their features to increase their likelihood of receiving a particular (positive) decision (at a certain cost). Yet, when different demographic groups have different feature distributions or pay different update costs, prior work has shown that individuals from minority groups often pay a higher cost to update their features. Fair classification aims to address such classifier performance disparities by constraining the classifiers to satisfy statistical fairness properties. However, we show that standard fairness constraints do not guarantee that the constrained classifier reduces the disparity in strategic manipulation cost. To address such biases in strategic settings and provide equal opportunities for strategic manipulation, we propose a constrained optimization framework that constructs classif
    
[^139]: 模型量化在深度神经网络中的应用：综述与分析

    A Comprehensive Survey on Model Quantization for Deep Neural Networks. (arXiv:2205.07877v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.07877](http://arxiv.org/abs/2205.07877)

    本文综述了深度神经网络中的模型量化，这是一种用低位宽存储全精度值以实现节约内存和操作成本的压缩方法。文章分类介绍了各种量化方法，并讨论了使用比例因子匹配数据范围和适当的训练方法的重要性。本文还回顾了模型量化的最新研究，并强调了其优缺点和当前的挑战和未来研究方向。

    

    深度神经网络在机器学习领域中的应用取得了重大进展，但是需要大量的参数存储和运算会带来硬件成本的增加和挑战。对此，提出了压缩方法以设计高效的加速器，其中最重要的方法是把全精度的值存储在低位宽中，这就可以节约内存同时用低成本的简单运算代替原本的操作。由于模型量化的灵活性和对设计高效硬件的影响，最近几年提出了许多深度神经网络量化方法，因此需要进行综合性的调查以更好地理解、分析和比较。本文提供了一份全面的综述，介绍了量化概念并从不同角度分类方法，讨论了使用比例因子匹配数据范围的重要性以及使用适当的训练方法避免精度损失的方法。我们还回顾了近年来对模型量化的研究，并强调其优点和缺点。最后，我们讨论了当前的挑战和未来的研究方向。

    Recent advances in machine learning by deep neural networks are significant. But using these networks has been accompanied by a huge number of parameters for storage and computations that leads to an increase in the hardware cost and posing challenges. Therefore, compression approaches have been proposed to design efficient accelerators. One important approach for deep neural network compression is quantization that full-precision values are stored in low bit-width. In this way, in addition to memory saving, the operations will be replaced by simple ones with low cost. Many methods are suggested for DNNs Quantization in recent years, because of flexibility and influence in designing efficient hardware. Therefore, an integrated report is essential for better understanding, analysis, and comparison. In this paper, we provide a comprehensive survey. We describe the quantization concepts and categorize the methods from different perspectives. We discuss using the scale factor to match the 
    
[^140]: 混合Transformer与多级融合用于多模态知识图谱补全

    Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion. (arXiv:2205.02357v5 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.02357](http://arxiv.org/abs/2205.02357)

    本文提出了一种混合Transformer与多级融合的方法，用于解决多模态知识图谱补全的问题。该方法通过统一的输入-输出架构适用于多样的任务，同时利用多级融合将视觉和文本表示集成起来。

    

    最近，多模态知识图谱（MKG）在信息检索、问答和推荐系统等任务中取得了成功，MKG组织了视觉-文本事实知识。然而，由于大多数MKG都不完整，因此提出了广泛的知识图谱补全研究，重点关注多模态实体、关系提取和链接预测。本文针对这些问题提出了一种混合Transformer与多级融合的方法。具体来说，我们利用一种混合Transformer架构和统一的输入-输出来完成多样的多模态知识图谱补全任务。此外，我们提出了多级融合，通过粗粒度前缀引导交互和细粒度相关感知将视觉和文本表示集成起来。

    Multimodal Knowledge Graphs (MKGs), which organize visual-text factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware f
    
[^141]: 设备上学习：基于神经网络的可训练边缘人工智能

    On-Device Learning: A Neural Network Based Field-Trainable Edge AI. (arXiv:2203.01077v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01077](http://arxiv.org/abs/2203.01077)

    本研究介绍了一种基于神经网络的设备上学习方法，针对边缘人工智能应用中的环境因素对准确性造成的影响进行了解决。通过重新训练，在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。

    

    在真实世界的边缘人工智能应用中，其准确性经常受到各种环境因素的影响，如噪声、传感器的位置/校准和时间相关的变化。本文介绍了一种基于神经网络的设备上学习方法，以解决这个问题而不需要深入了解。我们的方法与事实上的反向传播训练有很大区别，而是专为低端边缘设备量身定制。本文介绍了其算法和在由树莓派Pico和低功耗无线模块组成的无线传感器节点上的实现。通过使用旋转机器的振动模式进行实验证明，通过设备上学习的重新训练在嘈杂环境下显著提高了异常检测的准确性，同时节约了低功耗设备的计算和通信成本。

    In real-world edge AI applications, their accuracy is often affected by various environmental factors, such as noises, location/calibration of sensors, and time-related changes. This article introduces a neural network based on-device learning approach to address this issue without going deep. Our approach is quite different from de facto backpropagation based training but tailored for low-end edge devices. This article introduces its algorithm and implementation on a wireless sensor node consisting of Raspberry Pi Pico and low-power wireless module. Experiments using vibration patterns of rotating machines demonstrate that retraining by the on-device learning significantly improves an anomaly detection accuracy at a noisy environment while saving computation and communication costs for low power.
    
[^142]: L4KDE：学习用于运动规划的动态树扩展

    L4KDE: Learning for KinoDynamic Tree Expansion. (arXiv:2203.00975v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.00975](http://arxiv.org/abs/2203.00975)

    L4KDE是一种用于运动规划的学习方法，通过使用神经网络来预测状态之间的过渡成本，解决了树扩展中选择低成本节点的问题。

    

    我们提出了一种用于运动规划的学习动态树扩展（L4KDE）方法。基于树的规划方法，如快速随机树（RRT），是在连续状态空间运动规划中寻找全局最优路径的主要方法。 树的扩展是这些方法的核心步骤，即将新节点添加到不断扩展的树中。我们研究了基于树的运动规划的动力学变体，其中我们已知系统动力学和运动约束。现有方法通常无法优化以找到低成本过渡到采样坐标的节点，而是使用像坐标之间的欧几里德距离这样的指标作为选择候选节点连接到搜索树的启发式。我们提出L4KDE来解决这个问题。L4KDE使用神经网络来预测查询状态之间的过渡成本，从而可以高效地选择节点与样本坐标相连。

    We present the Learning for KinoDynamic Tree Expansion (L4KDE) method for kinodynamic planning. Tree-based planning approaches, such as rapidly exploring random tree (RRT), are the dominant approach to finding globally optimal plans in continuous state-space motion planning. Central to these approaches is tree-expansion, the procedure in which new nodes are added into an ever-expanding tree. We study the kinodynamic variants of tree-based planning, where we have known system dynamics and kinematic constraints. In the interest of quickly selecting nodes to connect newly sampled coordinates, existing methods typically cannot optimise to find nodes that have low cost to transition to sampled coordinates. Instead, they use metrics like Euclidean distance between coordinates as a heuristic for selecting candidate nodes to connect to the search tree. We propose L4KDE to address this issue. L4KDE uses a neural network to predict transition costs between queried states, which can be efficientl
    
[^143]: 当AUC遇上DRO：基于非凸收敛保证的深度学习局部AUC优化

    When AUC meets DRO: Optimizing Partial AUC for Deep Learning with Non-Convex Convergence Guarantee. (arXiv:2203.00176v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00176](http://arxiv.org/abs/2203.00176)

    本文提出了一种基于梯度的方法，通过分布鲁棒优化（DRO）来最大化深度学习中的局部AUC（pAUC），并提出了准确和平滑的pAUC估计量。实验证明了该方法在各种数据集上的有效性。

    

    本文提出了用于深度学习的一种系统且高效的基于梯度的一次性和二次性局部AUC（pAUC）最大化方法。我们通过使用分布鲁棒优化（DRO）来为每个单独的正数据定义损失，提出了pAUC替代目标的新公式。我们考虑了两种DRO的形式，一种基于条件风险值（CVaR），产生非平滑但准确的pAUC估计量；另一种基于KL散度正则化的DRO，产生不准确但平滑（软）的pAUC估计量。对于一次性和二次性pAUC最大化，我们分别提出了两种算法，并证明了它们对于优化各自的两种形式的收敛性。实验证明了提出的算法在各种数据集上对于深度学习中的pAUC最大化的有效性。

    In this paper, we propose systematic and efficient gradient-based methods for both one-way and two-way partial AUC (pAUC) maximization that are applicable to deep learning. We propose new formulations of pAUC surrogate objectives by using the distributionally robust optimization (DRO) to define the loss for each individual positive data. We consider two formulations of DRO, one of which is based on conditional-value-at-risk (CVaR) that yields a non-smooth but exact estimator for pAUC, and another one is based on a KL divergence regularized DRO that yields an inexact but smooth (soft) estimator for pAUC. For both one-way and two-way pAUC maximization, we propose two algorithms and prove their convergence for optimizing their two formulations, respectively. Experiments demonstrate the effectiveness of the proposed algorithms for pAUC maximization for deep learning on various datasets.
    
[^144]: 自主训练：一项综述

    Self-Training: A Survey. (arXiv:2202.12040v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.12040](http://arxiv.org/abs/2202.12040)

    自主训练方法是一种半监督算法，无需额外假设数据分布，在低密度区域找到决策边界，并使用学习分类器的输出分数作为置信度，通过为无标签样本分配伪标签，迭代地学习分类器，从而丰富有标签训练数据。

    

    半监督算法旨在从少量有标签观测和大量无标签观测中学习预测函数。由于这个框架在许多应用中是相关的，因此它们在学术界和工业界都受到了很多关注。在现有的技术中，自主训练方法在近年来无疑引起了更大的关注。这些模型旨在在低密度区域找到决策边界，而不对数据分布作出额外的假设，并使用学习分类器的无符号输出分数或其边界作为置信度的指标。自主训练算法的工作原理是通过给具有大于某个阈值的边界的无标签训练样本分配伪标签，迭代地学习分类器。然后，使用伪标记的示例来增强有标签训练数据，并与有标签训练集一起训练一个新的分类器。

    Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this
    
[^145]: 自适应鲁棒的多任务学习

    Adaptive and Robust Multi-Task Learning. (arXiv:2202.05250v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.05250](http://arxiv.org/abs/2202.05250)

    本文提出一系列自适应方法，能够同时处理多任务学习的相似性和差异性，并具有统计保证和鲁棒性。

    

    本论文研究了解决从不同来源收集的多个数据集并对每个数据集学习一个模型的多任务学习问题。我们提出了一系列自适应方法，自动利用任务之间的相似性，同时处理它们之间的差异。我们证明了这些方法的统计保证，并证明它们对异常任务具有鲁棒性。通过合成和实际数据集的数值实验，证明了我们的新方法的功效。

    We study the multi-task learning problem that aims to simultaneously analyze multiple datasets collected from different sources and learn one model for each of them. We propose a family of adaptive methods that automatically utilize possible similarities among those tasks while carefully handling their differences. We derive sharp statistical guarantees for the methods and prove their robustness against outlier tasks. Numerical experiments on synthetic and real datasets demonstrate the efficacy of our new methods.
    
[^146]: 基于模型的强化学习中遵循奖励的子任务

    Reward-Respecting Subtasks for Model-Based Reinforcement Learning. (arXiv:2202.03466v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03466](http://arxiv.org/abs/2202.03466)

    论文提出了一种基于模型的强化学习的方法，通过添加奖励加成的子任务来发现选项，从而解决了以前方法中忽略原始奖励的问题。

    

    为了实现人工智能的远大目标，强化学习必须包括对抽象状态和时间的世界模型的规划。深度学习在状态抽象方面取得了进展，但时间抽象却很少被使用，尽管基于选项框架已经广泛发展了理论。其中一个原因是可能的选项空间很大，以前提出的选项发现方法没有考虑到选项模型在规划中的使用方式。通常通过提出子任务（例如达到瓶颈状态或最大化除奖励外的感知信号的累积和）来发现选项。解决每个子任务以生成一个选项，然后学习选项的模型并使其可用于规划过程。在大多数以前的研究中，子任务忽略了原始问题上的奖励，而我们提出的子任务使用原始奖励加上基于某个特征的奖励加成。

    To achieve the ambitious goals of artificial intelligence, reinforcement learning must include planning with a model of the world that is abstract in state and time. Deep learning has made progress with state abstraction, but temporal abstraction has rarely been used, despite extensively developed theory based on the options framework. One reason for this is that the space of possible options is immense, and the methods previously proposed for option discovery do not take into account how the option models will be used in planning. Options are typically discovered by posing subsidiary tasks, such as reaching a bottleneck state or maximizing the cumulative sum of a sensory signal other than reward. Each subtask is solved to produce an option, and then a model of the option is learned and made available to the planning process. In most previous work, the subtasks ignore the reward on the original problem, whereas we propose subtasks that use the original reward plus a bonus based on a fe
    
[^147]: DeepKE: 一种基于深度学习的知识提取工具包用于知识库构建

    DeepKE: A Deep Learning Based Knowledge Extraction Toolkit for Knowledge Base Population. (arXiv:2201.03335v6 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2201.03335](http://arxiv.org/abs/2201.03335)

    DeepKE是一个基于深度学习的知识提取工具包，支持复杂的低资源、文档级和多模态场景，可用于自定义数据集和模型来从非结构化数据中提取信息。

    

    我们提出了一个开放源代码和可扩展的知识提取工具包DeepKE，支持知识库构建中的复杂低资源、文档级和多模态场景。DeepKE实现了各种信息提取任务，包括命名实体识别、关系提取和属性提取。通过统一的框架，DeepKE允许开发人员和研究人员根据自己的需求定制数据集和模型，从非结构化数据中提取信息。具体而言，DeepKE不仅为不同任务和场景提供各种功能模块和模型实现，还通过一致的框架组织所有组件，以保持足够的模块化和可扩展性。我们在https://github.com/zjunlp/DeepKE发布了源代码，并提供了适用于初学者的Google Colab教程和全面的文档。此外，我们还在http URL上提供了一个在线系统，用于实时提取各种任务，并提供了演示视频。

    We present an open-source and extensible knowledge extraction toolkit DeepKE, supporting complicated low-resource, document-level and multimodal scenarios in the knowledge base population. DeepKE implements various information extraction tasks, including named entity recognition, relation extraction and attribute extraction. With a unified framework, DeepKE allows developers and researchers to customize datasets and models to extract information from unstructured data according to their requirements. Specifically, DeepKE not only provides various functional modules and model implementation for different tasks and scenarios but also organizes all components by consistent frameworks to maintain sufficient modularity and extensibility. We release the source code at GitHub in https://github.com/zjunlp/DeepKE with Google Colab tutorials and comprehensive documents for beginners. Besides, we present an online system in this http URL for real-time extraction of various tasks, and a demo video
    
[^148]: 情绪分析和新冠疫情对大学社区中Reddit数据的影响研究

    Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data. (arXiv:2112.04351v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.04351](http://arxiv.org/abs/2112.04351)

    这项研究通过分析大学社区中的Reddit数据，研究了COVID-19疫情对人们情绪和心理状态的影响，并提出了基于RoBERTa和GAT的情绪分类模型。

    

    背景：COVID-19疫情以各种方式影响了我们的社会和人类福祉。在这项研究中，我们使用社交媒体的真实数据，调查了疫情如何与疫情前期相比影响了人们的情绪和心理状态。方法：我们收集了与八所大学相关的Reddit社交媒体数据，其中包括2019年（疫情前）和2020年（疫情期间）的数据。我们利用预训练的RoBERTa方法学习Reddit消息的文本嵌入，同时利用发布的消息之间的关系信息训练了一个图注意力网络（GAT）进行情绪分类。最后，我们将RoBERTa和GAT的预测概率进行模型堆叠，得出情绪最终分类结果。通过对收集到的数据进行模型预测的情绪标签，我们使用广义线性混合效应模型估计了疫情和情绪之间的效应。

    Background: The COVID-19 pandemic has affected our society and human well-being in various ways. In this study, we investigate how the pandemic has influenced people's emotions and psychological states compared to a pre-pandemic period using real-world data from social media.  Method: We collected Reddit social media data from 2019 (pre-pandemic) and 2020 (pandemic) from the subreddits communities associated with eight universities. We applied the pre-trained Robustly Optimized BERT pre-training approach (RoBERTa) to learn text embedding from the Reddit messages, and leveraged the relational information among posted messages to train a graph attention network (GAT) for sentiment classification. Finally, we applied model stacking to combine the prediction probabilities from RoBERTa and GAT to yield the final classification on sentiment. With the model-predicted sentiment labels on the collected data, we used a generalized linear mixed-effects model to estimate the effects of pandemic an
    
[^149]: PLACE退出：一种逐层和逐通道的渐进式退出方法用于领域泛化

    PLACE dropout: A Progressive Layer-wise and Channel-wise Dropout for Domain Generalization. (arXiv:2112.03676v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.03676](http://arxiv.org/abs/2112.03676)

    本文提出了一种逐层和逐通道的渐进式退出方法用于领域泛化，该方法比以前的层级特定退出方法具有更强的正则化效果，能够更充分地缓解源领域上的过拟合问题。

    

    领域泛化（DG）旨在从多个观测到的源领域中学习出一个泛化能力良好的通用模型，以适应任意未观测到的目标领域，而无需进一步训练。DG的主要挑战在于，由于源领域和目标领域之间的领域差异，模型必然面临严重的过拟合问题。为了缓解这个问题，一些基于退出的方法已经被提出，通过丢弃中间层的部分表示来抵抗过拟合。然而，我们观察到，大多数这些方法只在一些特定的层级上执行退出操作，导致对模型的正则化效果不足。我们认为，在多个层级上应用退出可以产生更强的正则化效果，相比以前的层级特定退出方法，这可以更充分地缓解源领域上的过拟合问题。本文中，我们开发了一种新的逐层和逐通道退出方法用于DG，该方法随机选择一层，然后执行退出操作。

    Domain generalization (DG) aims to learn a generic model from multiple observed source domains that generalizes well to arbitrary unseen target domains without further training. The major challenge in DG is that the model inevitably faces a severe overfitting issue due to the domain gap between source and target domains. To mitigate this problem, some dropout-based methods have been proposed to resist overfitting by discarding part of the representation of the intermediate layers. However, we observe that most of these methods only conduct the dropout operation in some specific layers, leading to an insufficient regularization effect on the model. We argue that applying dropout at multiple layers can produce stronger regularization effects, which could alleviate the overfitting problem on source domains more adequately than previous layer-specific dropout methods. In this paper, we develop a novel layer-wise and channel-wise dropout for DG, which randomly selects one layer and then ran
    
[^150]: 可解释和公平的布尔规则集合生成法

    Interpretable and Fair Boolean Rule Sets via Column Generation. (arXiv:2111.08466v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.08466](http://arxiv.org/abs/2111.08466)

    本文提出了一种通过列生成法来生成可解释和公平的布尔规则集合的方法。该方法在兼顾准确性和简单性的平衡方面优于常用的启发式规则挖掘方法，同时考虑了公平性设置，并扩展了模型以满足两种不同的分类公平性度量。使用近似列生成算法处理大规模数据集，并在实验中取得了良好的结果。

    

    本文考虑将布尔规则学习表示为析取范式（DNF，与决策规则集相等）的可解释的分类模型。提出了一个整数规划模型，以最优化分类准确性和规则简单性之间的平衡。同时，考虑了公平性设置，并扩展了模型，包括对两种分类公平性度量的显式约束：机会平等和均衡几率。采用列生成法（CG）高效搜索大量可能规则，而无需启发式规则挖掘。为了处理大规模数据集，提出了一种利用随机化的近似CG算法。与三种最近提出的替代算法相比，在16个数据集中，CG算法在8个数据集上兼顾了准确性和简单性的平衡。当以准确性为最大化目标时，CG算法与专为此目的设计的规则学习器具有相当的竞争力，有时能找到更简单但准确性不减的解决方案。

    This paper considers the learning of Boolean rules in disjunctive normal form (DNF, OR-of-ANDs, equivalent to decision rule sets) as an interpretable model for classification. An integer program is formulated to optimally trade classification accuracy for rule simplicity. We also consider the fairness setting and extend the formulation to include explicit constraints on two different measures of classification parity: equality of opportunity and equalized odds. Column generation (CG) is used to efficiently search over an exponential number of candidate rules without the need for heuristic rule mining. To handle large data sets, we propose an approximate CG algorithm using randomization. Compared to three recently proposed alternatives, the CG algorithm dominates the accuracy-simplicity trade-off in 8 out of 16 data sets. When maximized for accuracy, CG is competitive with rule learners designed for this purpose, sometimes finding significantly simpler solutions that are no less accurat
    
[^151]: 针对多任务学习和多标准决策制定的准确帕累托最优搜索

    Exact Pareto Optimal Search for Multi-Task Learning and Multi-Criteria Decision-Making. (arXiv:2108.00597v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.00597](http://arxiv.org/abs/2108.00597)

    提出了一种新的多目标优化（MOO）方法，EPO Search，用于解决多任务学习（MTL）和多标准决策制定（MCDM）中切比雪夫标量化方法在收敛性和计算效率方面的问题。

    

    给定多个非凸目标函数和目标特定权重，切比雪夫标量化（CS）是一种获得准确帕累托最优（EPO）的方法，即通过权重的倒数定义的射线与帕累托前沿（PF）相交的解决方案。使用CS公式寻找EPO解的一阶优化器会遇到震荡和停滞等实际问题，影响收敛性。此外，当使用PO解初始化时，它们不能保证完全位于PF上的受控轨迹。这些缺点导致利用CS进行基于非凸多目标优化（MOO）的多任务学习（MTL）和多标准决策制定（MCDM）方法的建模限制和计算效率低下。为了解决这些缺点，我们设计了一种新的MOO方法，EPO搜索。我们证明了EPO搜索收敛到一个EPO解，并通过实证说明了其计算效率和鲁棒性。

    Given multiple non-convex objective functions and objective-specific weights, Chebyshev scalarization (CS) is a well-known approach to obtain an Exact Pareto Optimal (EPO), i.e., a solution on the Pareto front (PF) that intersects the ray defined by the inverse of the weights. First-order optimizers that use the CS formulation to find EPO solutions encounter practical problems of oscillations and stagnation that affect convergence. Moreover, when initialized with a PO solution, they do not guarantee a controlled trajectory that lies completely on the PF. These shortcomings lead to modeling limitations and computational inefficiency in multi-task learning (MTL) and multi-criteria decision-making (MCDM) methods that utilize CS for their underlying non-convex multi-objective optimization (MOO). To address these shortcomings, we design a new MOO method, EPO Search. We prove that EPO Search converges to an EPO solution and empirically illustrate its computational efficiency and robustness t
    
[^152]: 通过学习解耦来对元学习进行上下文化

    Contextualizing Meta-Learning via Learning to Decompose. (arXiv:2106.08112v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.08112](http://arxiv.org/abs/2106.08112)

    通过学习解耦的方法，我们提出了一种上下文化的元学习策略，用于从支持集中推断目标模型。我们的方法能够捕捉到实例级的模糊相似性，从而提高了模型的性能和泛化能力。

    

    元学习已经成为一种基于支持集构建目标模型的高效方法。例如，元学习嵌入使得可以通过将实例拉近到同类邻居来构建特定任务的目标最近邻分类器。然而，单个实例可以从多个潜在属性进行注释，使得在支持集内部或跨支持集的看似相似的实例具有不同的标签和与其他实例的多样关系。因此，对于从支持集推断目标模型的统一元学习策略无法捕捉到实例级的模糊相似性。为此，我们提出了一种学习解耦网络（LeadNet），以在支持集中利用具有一种或混合潜在属性的实例的上下文对元学习的“支持到目标”策略进行上下文化。特别是，实例之间的比较关系在多个嵌入空间中进行解耦。LeadNet学习自动化地...（待完整翻译）

    Meta-learning has emerged as an efficient approach for constructing target models based on support sets. For example, the meta-learned embeddings enable the construction of target nearest-neighbor classifiers for specific tasks by pulling instances closer to their same-class neighbors. However, a single instance can be annotated from various latent attributes, making visually similar instances inside or across support sets have different labels and diverse relationships with others. Consequently, a uniform meta-learned strategy for inferring the target model from the support set fails to capture the instance-wise ambiguous similarity. To this end, we propose Learning to Decompose Network (LeadNet) to contextualize the meta-learned ``support-to-target'' strategy, leveraging the context of instances with one or mixed latent attributes in a support set. In particular, the comparison relationship between instances is decomposed w.r.t. multiple embedding spaces. LeadNet learns to automatica
    
[^153]: 面向医学图像分类的隐私保护约束域泛化

    Privacy-Preserving Constrained Domain Generalization for Medical Image Classification. (arXiv:2105.08511v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.08511](http://arxiv.org/abs/2105.08511)

    该论文提出了一种隐私保护约束域泛化的方法，通过改进信息聚合过程来提高模型的泛化能力，并解决了由于数据隐私保护问题导致的医学影像分类的挑战。

    

    深度神经网络在医学影像应用中取得了前所未有的成功。然而，由于数据集有限和对患者隐私保护的严格法律和伦理要求，由大规模训练数据驱动的深度神经网络在医学影像分类的广泛应用受到了很大阻碍。在这篇论文中，我们旨在通过开发隐私保护约束域泛化方法来解决这个问题，以在隐私保护条件下提高泛化能力。具体而言，我们提出了一种新的梯度对齐损失，以改善集中式服务器端的信息聚合过程，期望训练的模型可以更好地泛化到“未见过”的数据。

    Deep neural networks (DNN) have demonstrated unprecedented success for medical imaging applications. However, due to the issue of limited dataset availability and the strict legal and ethical requirements for patient privacy protection, the broad applications of medical imaging classification driven by DNN with large-scale training data have been largely hindered. For example, when training the DNN from one domain (e.g., with data only from one hospital), the generalization capability to another domain (e.g., data from another hospital) could be largely lacking. In this paper, we aim to tackle this problem by developing the privacy-preserving constrained domain generalization method, aiming to improve the generalization capability under the privacy-preserving condition. In particular, We propose to improve the information aggregation process on the centralized server-side with a novel gradient alignment loss, expecting that the trained model can be better generalized to the "unseen" bu
    
[^154]: 一类无维度度量用于经验测度的收敛性

    A Class of Dimension-free Metrics for the Convergence of Empirical Measures. (arXiv:2104.12036v4 [math.PR] UPDATED)

    [http://arxiv.org/abs/2104.12036](http://arxiv.org/abs/2104.12036)

    本文提出了一类无维度度量，用于高维情况下经验测度的收敛性，解决了维度灾难问题，具有重要的实际应用。

    

    本文研究高维情况下经验测度的收敛性。我们提出了一种新的概率度量类，证明在这种度量下，收敛性不受维度灾难的影响。这种特性对于高维分析至关重要，与经典度量（如Wasserstein度量）形成对比。所提出的度量属于积分概率度量的范畴，我们指定了测试函数空间的准则，以确保不受维度灾难影响。所选测试函数空间的例子包括再生核希尔伯特空间、Barron空间和流诱导函数空间。我们提供了所提出度量的三个应用：1. 在随机变量情况下的经验测度收敛性；2. n粒子系统收敛于McKean-Vlasov随机微分方程的解；3. 构造齐次n-player的ε-Nash均衡。

    This paper concerns the convergence of empirical measures in high dimensions. We propose a new class of probability metrics and show that under such metrics, the convergence is free of the curse of dimensionality (CoD). Such a feature is critical for high-dimensional analysis and stands in contrast to classical metrics ({\it e.g.}, the Wasserstein metric). The proposed metrics fall into the category of integral probability metrics, for which we specify criteria of test function spaces to guarantee the property of being free of CoD. Examples of the selected test function spaces include the reproducing kernel Hilbert spaces, Barron space, and flow-induced function spaces. Three applications of the proposed metrics are presented: 1. The convergence of empirical measure in the case of random variables; 2. The convergence of $n$-particle system to the solution to McKean-Vlasov stochastic differential equation; 3. The construction of an $\varepsilon$-Nash equilibrium for a homogeneous $n$-pl
    
[^155]: KnowPrompt：具有协同优化的知识感知提示调整在关系抽取中的应用

    KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v7 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2104.07650](http://arxiv.org/abs/2104.07650)

    本文提出了一种名为KnowPrompt的知识感知提示调整方法，通过将关系标签中的潜在知识融入到提示构建中，并通过协同优化的方式，提高了关系抽取任务的性能。

    

    最近，对于特定的少样本分类任务，使用提示调整方法取得了有希望的结果。提示调整的核心思想是将文本片段（即模板）插入输入，并将分类任务转化为掩码语言建模问题。然而，对于关系抽取，确定一个合适的提示模板需要领域专业知识，获取合适的标签词是繁琐且耗时的。此外，关系标签之间存在丰富的语义和先验知识，不容忽视。因此，我们的研究着眼于将关系标签之间的知识融入到关系抽取的提示调整中，并提出了一种具有协同优化的知识感知提示调整方法（KnowPrompt）。具体而言，我们利用可学习的虚拟类型词和答案词将关系标签中的潜在知识融入到提示构建中。然后，我们通过结构化约束协同优化它们的表示。

    Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and time-consuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Ex
    
[^156]: 一个通用框架用于PAC-Bayesian界的实用解读

    A General Framework for the Practical Disintegration of PAC-Bayesian Bounds. (arXiv:2102.08649v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.08649](http://arxiv.org/abs/2102.08649)

    该论文提出了一种新的PAC-Bayesian泛化界的框架，该框架能够提供解离界，相比现有框架在神经网络上有显著的实用改进。

    

    PAC-Bayesian界在研究随机分类器的泛化能力时已被证明紧凑而且有信息量。然而，当应用于一些确定性模型家族（如神经网络）时，它们需要松弛且昂贵的去随机化步骤。作为替代步骤，我们引入了新的PAC-Bayesian泛化界，这些界独具创新性，能够提供解离界，即它们能够对一个单一假设提供保证，而不是通常的平均分析。我们的界易于优化，并可用于设计学习算法。我们在神经网络上展示了这种行为，并展示了与现有框架相比的显著实用改进。

    PAC-Bayesian bounds are known to be tight and informative when studying the generalization ability of randomized classifiers. However, they require a loose and costly derandomization step when applied to some families of deterministic models such as neural networks. As an alternative to this step, we introduce new PAC-Bayesian generalization bounds that have the originality to provide disintegrated bounds, i.e., they give guarantees over one single hypothesis instead of the usual averaged analysis. Our bounds are easily optimizable and can be used to design learning algorithms. We illustrate this behavior on neural networks, and we show a significant practical improvement over the state-of-the-art framework.
    
[^157]: 懒惰型在线凸优化: 切换预算下的研究

    Lazy OCO: Online Convex Optimization on a Switching Budget. (arXiv:2102.03803v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.03803](http://arxiv.org/abs/2102.03803)

    本研究提出了一种懒惰型在线凸优化的算法，其在切换次数有限的情况下达到了近似最优的遗憾上界，并且在连续设置中呈现出高效的计算性能。

    

    我们研究了一种在线凸优化的变种，其中玩家在T轮中的期望切换决策不超过S次。之前的研究已经解决了离散决策设置中的类似问题，最近也在连续设置中使用自适应对手进行了研究。在本研究中，我们旨在填补这一空白，并在普遍存在的无知设置中提出计算有效的算法，为一般凸损失建立了O(T/S)的遗憾上界以及强凸损失的近似O(T/S^2)的遗憾上界。此外，对于随机独立同分布的损失，我们提出了一种简单的算法，在一般和强凸设置中遗憾仅有对数因子的乘法log T的情况下进行了log T次切换。最后，我们补充了与我们考虑的一些情况相匹配的下界来补充我们的算法。

    We study a variant of online convex optimization where the player is permitted to switch decisions at most $S$ times in expectation throughout $T$ rounds. Similar problems have been addressed in prior work for the discrete decision set setting, and more recently in the continuous setting but only with an adaptive adversary. In this work, we aim to fill the gap and present computationally efficient algorithms in the more prevalent oblivious setting, establishing a regret bound of $O(T/S)$ for general convex losses and $\widetilde O(T/S^2)$ for strongly convex losses. In addition, for stochastic i.i.d.~losses, we present a simple algorithm that performs $\log T$ switches with only a multiplicative $\log T$ factor overhead in its regret in both the general and strongly convex settings. Finally, we complement our algorithms with lower bounds that match our upper bounds in some of the cases we consider.
    
[^158]: 在延迟环境中以非固定马尔可夫策略行动

    Acting in Delayed Environments with Non-Stationary Markov Policies. (arXiv:2101.11992v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.11992](http://arxiv.org/abs/2101.11992)

    该论文介绍了在延迟环境中，学习和规划的马尔可夫决策过程(MDP)框架，证明了在延迟执行的情况下，原始状态空间中的非固定马尔可夫策略可以实现最大奖励，提出了一种解决延迟执行任务的非固定Q-learning风格算法。

    

    标准的马尔可夫决策过程(MDP)假设在选择动作后立即执行，但这种假设常常不切实际，会在机器人操纵、云计算和金融等应用中导致灾难性故障。我们引入了一个学习和计划的MDP框架，其中决策者选择的动作需要延迟$m$步才能执行。我们证明了在延迟执行的情况下，原始状态空间中的确定性马尔可夫策略足以实现最大奖励，但需要是非固定的。然而，我们还证明了固定的马尔可夫策略在一般情况下是次优的。因此，我们设计了一种非固定的基于模型的Q-learning风格算法，可以解决延迟执行任务。

    The standard Markov Decision Process (MDP) formulation hinges on the assumption that an action is executed immediately after it was chosen. However, assuming it is often unrealistic and can lead to catastrophic failures in applications such as robotic manipulation, cloud computing, and finance. We introduce a framework for learning and planning in MDPs where the decision-maker commits actions that are executed with a delay of $m$ steps. The brute-force state augmentation baseline where the state is concatenated to the last $m$ committed actions suffers from an exponential complexity in $m$, as we show for policy iteration. We then prove that with execution delay, deterministic Markov policies in the original state-space are sufficient for attaining maximal reward, but need to be non-stationary. As for stationary Markov policies, we show they are sub-optimal in general. Consequently, we devise a non-stationary Q-learning style model-based algorithm that solves delayed execution tasks wi
    
[^159]: 风险敏感的深度强化学习：方差约束的演员-评论家算法能够找到全局最优策略

    Risk-Sensitive Deep RL: Variance-Constrained Actor-Critic Provably Finds Globally Optimal Policy. (arXiv:2012.14098v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.14098](http://arxiv.org/abs/2012.14098)

    本文首次尝试在平均奖励设置下，通过方差风险准则研究风险敏感的深度强化学习。我们提出了一个方差约束的策略优化问题，并设计了一种演员-评论家算法来解决该问题。

    

    尽管深度强化学习在各种应用中取得了巨大成功，但大多数现有工作仅关注最大化总回报的期望值，从而忽略了其固有的随机性。这种随机性也被称为不确定性，并与风险的概念密切相关。本文首次尝试在平均奖励设置下，通过方差风险准则研究风险敏感的深度强化学习。具体而言，我们关注一个方差约束的策略优化问题，目标是找到一个策略，最大化长期平均奖励的期望值，并且使得长期平均奖励的方差上界不超过某个阈值。利用Lagrange和Fenchel对偶性，我们将原始问题转化为一个无约束的鞍点策略优化问题，并提出了一种迭代和高效更新策略的演员-评论家算法。

    While deep reinforcement learning has achieved tremendous successes in various applications, most existing works only focus on maximizing the expected value of total return and thus ignore its inherent stochasticity. Such stochasticity is also known as the aleatoric uncertainty and is closely related to the notion of risk. In this work, we make the first attempt to study risk-sensitive deep reinforcement learning under the average reward setting with the variance risk criteria. In particular, we focus on a variance-constrained policy optimization problem where the goal is to find a policy that maximizes the expected value of the long-run average reward, subject to a constraint that the long-run variance of the average reward is upper bounded by a threshold. Utilizing Lagrangian and Fenchel dualities, we transform the original problem into an unconstrained saddle-point policy optimization problem, and propose an actor-critic algorithm that iteratively and efficiently updates the policy,
    
[^160]: 机器学习中隐私攻击的调查

    A Survey of Privacy Attacks in Machine Learning. (arXiv:2007.07646v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2007.07646](http://arxiv.org/abs/2007.07646)

    该研究对机器学习中的隐私攻击进行了调查，提出了攻击分类和威胁模型，并分析了不同攻击的原因和防御方法。

    

    随着机器学习的广泛应用，研究其安全和隐私的影响变得更加紧迫。尽管在隐私方面的研究工作在过去几年里稳步增长，但与安全方面相比，对机器学习的隐私方面研究的关注度较低。本研究的贡献是对过去七年发表的与机器学习隐私攻击相关的40多篇论文进行分析。我们提出了一种攻击分类方法，并提出了威胁模型，可以根据对手知识和受攻击资产对不同攻击进行分类。我们首先对隐私泄露的原因进行了初步探讨，然后对不同的攻击进行了详细分析。最后，我们总结了最常见的防御方法，并讨论了我们分析过程中发现的开放问题和未来的研究方向。

    As machine learning becomes more widely used, the need to study its implications in security and privacy becomes more urgent. Although the body of work in privacy has been steadily growing over the past few years, research on the privacy aspects of machine learning has received less focus than the security aspects. Our contribution in this research is an analysis of more than 40 papers related to privacy attacks against machine learning that have been published during the past seven years. We propose an attack taxonomy, together with a threat model that allows the categorization of different attacks based on the adversarial knowledge, and the assets under attack. An initial exploration of the causes of privacy leaks is presented, as well as a detailed analysis of the different attacks. Finally, we present an overview of the most commonly proposed defenses and a discussion of the open problems and future directions identified during our analysis.
    
[^161]: 基于上下文感知的先进胶囊网络

    Advanced Capsule Networks via Context Awareness. (arXiv:1903.07497v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1903.07497](http://arxiv.org/abs/1903.07497)

    本研究通过增加池化层和重建层来改进胶囊网络（CN）的设计，以适应具有不同上下文的图像数据集，并与深度学习（DL）模型进行了性能对比。结果显示，CN在大大减少训练时间的同时表现出了与DL模型相当的性能。

    

    胶囊网络（CN）为深度学习（DL）社区提供了新的架构。尽管它的有效性已经在MNIST和smallNORB数据集中得到了证明，但是对于具有不同上下文的图像的数据集，该网络仍然面临挑战。在这项研究中，我们改进了CN（向量版本）的设计，具体来说，我们增加了更多的池化层来过滤图像背景，并增加了更多的重建层来实现更好的图像恢复。此外，我们进行了实验，比较了CN和DL模型的准确性和速度。在DL模型中，除了在强大的计算机上使用Inception V3和DenseNet V201外，我们还使用了NASNet、MobileNet V1和MobileNet V2来适用于小型和嵌入式设备。我们在美国手语（ASL）的手指拼写字母数据集上评估了我们的模型。结果表明，CN与DL模型相比，在大大减少训练时间的同时表现出了可比较的性能。我们还进行了演示，并提供了一个链接以进行说明。

    Capsule Networks (CN) offer new architectures for Deep Learning (DL) community. Though its effectiveness has been demonstrated in MNIST and smallNORB datasets, the networks still face challenges in other datasets for images with distinct contexts. In this research, we improve the design of CN (Vector version) namely we expand more Pooling layers to filter image backgrounds and increase Reconstruction layers to make better image restoration. Additionally, we perform experiments to compare accuracy and speed of CN versus DL models. In DL models, we utilize Inception V3 and DenseNet V201 for powerful computers besides NASNet, MobileNet V1 and MobileNet V2 for small and embedded devices. We evaluate our models on a fingerspelling alphabet dataset from American Sign Language (ASL). The results show that CNs perform comparably to DL models while dramatically reducing training time. We also make a demonstration and give a link for the purpose of illustration.
    

