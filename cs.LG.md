# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Semantic Multi-Resolution Communications.](http://arxiv.org/abs/2308.11604) | 本论文提出了一个基于深度学习的多分辨率联合源-信道编码框架，在数据重建方面取得了显著进展，并在多用户和/或多分辨率方式下具有优势。同时，该框架具有潜力实现语义通信，通过保持特定的语义属性来扩展其目标。 |
| [^2] | [Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model.](http://arxiv.org/abs/2308.11601) | Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。 |
| [^3] | [Quantization-based Optimization with Perspective of Quantum Mechanics.](http://arxiv.org/abs/2308.11594) | 基于量子力学视角的量化优化方法在全局优化中利用薛定谔方程推导的隧道效应，从而能够避免局部最小值。 |
| [^4] | [What's Race Got to do with it? Predicting Youth Depression Across Racial Groups Using Machine and Deep Learning.](http://arxiv.org/abs/2308.11591) | 本研究利用机器学习和深度学习方法，使用全国青少年风险行为监测系统（YRBSS）调查数据，对不同种族群体中的青少年抑郁症进行预测。研究发现种族亚组之间存在着相关因素的差异，并呼吁提供更多广泛和多样化的数据集。 |
| [^5] | [An alternative to SVM Method for Data Classification.](http://arxiv.org/abs/2308.11579) | 本文提出了一种新的方法来替代支持向量机(SVM)方法进行数据分类，在保持类似性能的同时，对SVM方法的一些缺点进行了改进和敏感处理。 |
| [^6] | [Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models.](http://arxiv.org/abs/2308.11578) | 本论文综合调查了大型语言模型（LLMs）在情绪识别中表现的各个方面，包括上下文学习、少样本学习和准确度等。LLMs的出现为情绪识别建模带来了新的潜力和机会。 |
| [^7] | [Low Tensor Rank Learning of Neural Dynamics.](http://arxiv.org/abs/2308.11567) | 研究发现通过学习过程中的张量秩演化来理解神经元连接在学习中的协调变化。研究表明训练过的递归神经网络的权重矩阵通常具有低秩结构，而这种结构在整个学习过程中保持在一个固定的低维子空间中。对真实权重进行低秩分解验证了这一观察结果。 |
| [^8] | [Multi-event Video-Text Retrieval.](http://arxiv.org/abs/2308.11551) | 本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。 |
| [^9] | [A free from local minima algorithm for training regressive MLP neural networks.](http://arxiv.org/abs/2308.11532) | 本文介绍了一种克服局部最小值问题的算法，用于训练回归MLP神经网络。 |
| [^10] | [ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm.](http://arxiv.org/abs/2308.11522) | 本文介绍了一种使用线性元胞自动机设计算法的储备计算方法，解决了线性元胞自动机规则选择的开放问题，并通过数学分析和大量实验验证，该方法具有较低的计算复杂度和误差。 |
| [^11] | [Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models.](http://arxiv.org/abs/2308.11521) | 这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。 |
| [^12] | [EM for Mixture of Linear Regression with Clustered Data.](http://arxiv.org/abs/2308.11518) | 本文研究了在分布式数据中利用集群结构改进学习方案的问题，针对带有集群数据的线性回归混合模型，应用了期望最大化（EM）方法进行参数估计。 |
| [^13] | [TrackFlow: Multi-Object Tracking with Normalizing Flows.](http://arxiv.org/abs/2308.11513) | 本论文提出了一种带有标准化流的多目标跟踪方法，通过融合异构信息，并解决了成本贡献、超参数调整和独立性等问题。 |
| [^14] | [Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models.](http://arxiv.org/abs/2308.11511) | 该论文探索了排列对齐模型的凸组合，并发现广泛的超立方体区域形成了低损失值的曲面，揭示了线性模式连通性的概念扩展到了更一般的模式可组合性现象。同时提出了一些关于线性模式连通性和模型重排基的新观察。研究还发现了模型组合具有传递性和鲁棒性质，并分析了功能和权重相似性的情况。 |
| [^15] | [Can Authorship Representation Learning Capture Stylistic Features?.](http://arxiv.org/abs/2308.11490) | 本论文研究了作者身份表征学习能否捕捉文体特征的问题，并通过实验验证了这些表征能够有效地捕捉写作风格的特征。 |
| [^16] | [Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions.](http://arxiv.org/abs/2308.11483) | 本文研究了大型语言模型对多选题选项顺序的敏感性。实验证明，当对回答选项进行重新排序时，大型语言模型的性能差距可以达到13%至75%。这种敏感性主要在大型语言模型对前两个/三个选项的预测不确定时出现。 |
| [^17] | [Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection.](http://arxiv.org/abs/2308.11480) | 这项研究对机器学习中分布外检测方法进行了评估，发现现有方法在检测未知类别方面表现出色，但在遇到其他类型的分布变化时性能不稳定。 |
| [^18] | [Revisiting column-generation-based matheuristic for learning classification trees.](http://arxiv.org/abs/2308.11477) | 该论文改进了基于列生成的启发式方法，以提高学习分类树的效果。通过减少子问题数量、使用数据依赖约束作为割平面以及生成违反约束的数据点，该方法提高了可伸缩性并适用于大型数据集。 |
| [^19] | [Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning.](http://arxiv.org/abs/2308.11464) | 提出了一种基于内部跨层梯度的联邦学习方法，通过混合浅层和深层的梯度，增强了深层的相似性，从而扩展了在处理系统异质性方面的能力。 |
| [^20] | [A Survey on Self-Supervised Representation Learning.](http://arxiv.org/abs/2308.11455) | 本综述论文全面回顾了无监督学习图像表示的方法，提出了一种分类法，并总结了最新的实验结果，为深入研究表示学习领域的人员提供了一个起点。 |
| [^21] | [Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding.](http://arxiv.org/abs/2308.11448) | 本研究基于自监督学习技术，在计算机视觉任务中评估了零样本分割的有效性，提出了一种基于提示补丁的评估协议。这项研究旨在实现在一般化和识别未见对象方面模拟人类的能力。 |
| [^22] | [Exploration of Rashomon Set Assists Explanations for Medical Data.](http://arxiv.org/abs/2308.11446) | 本文提出了一种新的过程来探索和分析医疗数据中的拉舒蒙集合模型，从而超越传统单一模型选择的方法，并通过引入"拉舒蒙检测"算法识别出集合中最不同的模型。 |
| [^23] | [TurboViT: Generating Fast Vision Transformers via Generative Architecture Search.](http://arxiv.org/abs/2308.11421) | TurboViT是通过生成式架构搜索生成的快速视觉变压器架构设计，它在准确性和计算效率之间取得了良好的平衡。 |
| [^24] | [Designing an attack-defense game: how to increase robustness of financial transaction models via a competition.](http://arxiv.org/abs/2308.11406) | 通过设计一款攻防游戏，我们研究了使用序列金融数据的神经网络模型的对抗攻击和防御的现状和动态，并且通过分析比赛动态，回答了隐藏模型免受恶意用户攻击的重要性以及需要多长时间才能破解模型的问题。 |
| [^25] | [Non-Redundant Combination of Hand-Crafted and Deep Learning Radiomics: Application to the Early Detection of Pancreatic Cancer.](http://arxiv.org/abs/2308.11389) | 本文解决了学习不与手工制作放射学冗余的深度学习放射学的问题，并通过结合两者的特征来预测早期胰腺癌标志物，取得了比基线方法更好的结果。 |
| [^26] | [Targeted Data Augmentation for bias mitigation.](http://arxiv.org/abs/2308.11386) | 本研究提出了一种新颖而高效的方法，通过利用数据增强技术来解决数据和模型中的偏见问题。与去除偏见不同，我们的方法建议在训练过程中插入偏见，从而提高了性能。我们还通过对两个不同的数据集进行标注来识别偏见，并发布了这些偏见标注，为未来的研究提供了有价值的资源。 |
| [^27] | [Interpretable Distribution-Invariant Fairness Measures for Continuous Scores.](http://arxiv.org/abs/2308.11375) | 对于连续评分，我们提出了一种基于Wasserstein距离的分布不变公平性度量方法，能够解释度量结果并适用于比较不同模型、数据集或时间点之间的偏差。 |
| [^28] | [How Much Temporal Long-Term Context is Needed for Action Segmentation?.](http://arxiv.org/abs/2308.11358) | 本文提出了一种基于transformer的模型，利用稀疏注意力捕捉视频的完整上下文，以回答时间行动分割需要多少长期时间上下文。通过与当前最先进的方法进行比较，在三个时间行动分割数据集上取得了良好的性能。 |
| [^29] | [Machine learning assisted exploration for affine Deligne-Lusztig varieties.](http://arxiv.org/abs/2308.11355) | 本研究利用机器学习辅助框架探索仿射Deligne-Lusztig变量的几何性质，加速纯数学研究，发现新猜想和有前景的研究方向。 |
| [^30] | [Careful at Estimation and Bold at Exploration.](http://arxiv.org/abs/2308.11348) | 本文中提出了一个基于双Q函数框架的新颖探索策略，以解决连续动作空间中基于策略的探索中的问题，并提出了贪婪的Q softmax更新方案来更新Q值。 |
| [^31] | [ProAgent: Building Proactive Cooperative AI with Large Language Models.](http://arxiv.org/abs/2308.11339) | ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。 |
| [^32] | [Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation.](http://arxiv.org/abs/2308.11333) | 通过数据审计和触发器图像过滤等机制，我们提出了一种无数据生成触发器的防御方法来保护联邦学习免受后门攻击。该方法利用后门攻击特征来学习触发器，并生成具有新学习知识的图像。 |
| [^33] | [Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices.](http://arxiv.org/abs/2308.11295) | 本论文通过拓扑数据分析方法，提出一种基于注意力机制的拓扑性质的不确定性估计方法，用于Transformer模型的预测，超越传统方法，开辟了注意力机制的新应用领域。 |
| [^34] | [Network Momentum across Asset Classes.](http://arxiv.org/abs/2308.11294) | 本文研究了跨资产类别的网络动量，通过观察资产间动量传递，提出了一种新的交易信号，并探索了不同类别的连续期货合约之间的动量特征的相互关系。 |
| [^35] | [Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation.](http://arxiv.org/abs/2308.11291) | 本文提出了一种通过木材外形预测内部缺陷位置的方法，利用卷积循环神经网络解决二分类分割任务，实现在廉价设备上进行推理，并在冷杉和云杉树种上验证了该方法的有效性。 |
| [^36] | [ShadowNet for Data-Centric Quantum System Learning.](http://arxiv.org/abs/2308.11290) | 本研究提出了一个数据为中心的量子系统学习范式，将神经网络和经典阴影相结合，以解决大型量子系统动力学的预测和泛化问题。 |
| [^37] | [Test Time Embedding Normalization for Popularity Bias Mitigation.](http://arxiv.org/abs/2308.11288) | 本文提出了一种名为“测试时间嵌入归一化”的策略来解决推荐系统中的热门偏见问题。该方法利用归一化的物品嵌入来控制嵌入大小，并通过与用户和物品嵌入的角度相似度区分受欢迎和不受欢迎的物品，从而有效减少了热门偏见的影响。 |
| [^38] | [CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation.](http://arxiv.org/abs/2308.11277) | 该论文描述了一个基于CNN的楔形符号检测方法，通过学习注释的三维渲染和映射照片，结合光照增强。研究团队创建了HeiCuBeDa和MaiCuBeDa数据集，并提供了映射工具以传递注释。符号定位方法使用RepPoints检测器来预测字符的位置。该方法可以应用于处理楔形文字的数字工具开发和研究。 |
| [^39] | [FoX: Formation-aware exploration in multi-agent reinforcement learning.](http://arxiv.org/abs/2308.11272) | FoX是一个新颖的多智能体强化学习框架，通过减少探索空间，并引导智能体在不同形成中访问有意义的状态，显著提高了在合作多智能体任务中的性能。 |
| [^40] | [Quantum-Inspired Machine Learning: a Survey.](http://arxiv.org/abs/2308.11269) | 量子启发式机器学习（QiML）是一个利用量子力学原理在经典计算框架中的新领域，在本调查中我们提供了对QiML的综合和全面的研究，展示了其多样化的研究领域、最新进展和实际应用，并为QiML建立了明确的定义。未来的研究将从量子力学、量子计算和经典机器学习中汲取经验，丰富QiML的领域。 |
| [^41] | [Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes.](http://arxiv.org/abs/2308.11267) | 本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。 |
| [^42] | [Efficient Last-iterate Convergence Algorithms in Solving Games.](http://arxiv.org/abs/2308.11256) | 该论文研究了求解博弈中高效收敛算法的问题，通过分析乐观梯度下降上升（OGDA）和乐观乘法权重更新（OMWU）算法，以及基于奖励转化（RT）框架的算法，提出了解决这些问题的方法。 |
| [^43] | [A survey on bias in machine learning research.](http://arxiv.org/abs/2308.11254) | 本文调查了机器学习研究中的偏差问题，提供了偏差和错误的分类，并分析了机器学习流程中超过四十种潜在的偏差来源，为每种情况提供了清晰的示例。通过理解和减轻机器学习中的偏差，可以开发出更公平、更透明、更准确的ML模型。 |
| [^44] | [Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes.](http://arxiv.org/abs/2308.11247) | 本文在化学过程的交叉领域故障诊断中，对单源和多源无监督领域适应算法进行了广泛比较。研究结果表明，即使没有进行适应，使用多个领域进行训练也具有积极影响。 |
| [^45] | [An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification.](http://arxiv.org/abs/2308.11241) | 本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。 |
| [^46] | [Minwise-Independent Permutations with Insertion and Deletion of Features.](http://arxiv.org/abs/2308.11240) | 这项研究介绍了一种针对动态插入和删除特征的minHash算法，用于近似测量高维二进制数据的Jaccard相似度。 |
| [^47] | [Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment.](http://arxiv.org/abs/2308.11220) | 本研究使用联邦学习方法，通过访问大量多样的患者数据并保护隐私，来预测多囊卵巢综合征患者的最佳治疗药物选项。 |
| [^48] | [Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models.](http://arxiv.org/abs/2308.11217) | 本论文提出了一种多模态联邦学习框架，利用私有领域数据协同训练大型模型，以实现跨场景的智能服务。在大模型时代，该框架解决了异构数据、模型聚合、性能和成本权衡、数据隐私以及激励机制等方面的挑战。 |
| [^49] | [Hamiltonian GAN.](http://arxiv.org/abs/2308.11216) | 这个论文提出了一种基于Hamiltonian的生成对抗网络（GAN）视频生成方法，通过学习配置空间映射和运动模型来生成合理的视频。训练过程中使用的物理启发式损失函数可以促进对最小配置空间的表示和提高可解释性。 |
| [^50] | [A Simple Framework for Multi-mode Spatial-Temporal Data Modeling.](http://arxiv.org/abs/2308.11204) | 本文提出了一个简单的多模态空间 - 时间数据建模框架，通过跨模态关系学习和多层感知器来有效地建立多模态之间的连接并捕捉时间依赖关系和通道相关性。 |
| [^51] | [SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting.](http://arxiv.org/abs/2308.11200) | SegRNN是一种针对长期时间序列预测任务的分段循环神经网络，通过两种新策略（分段迭代和并行多步预测）显著减少了循环迭代次数，提高了预测准确性和推理速度。与现有的基于Transformer模型的方法相比，SegRNN不仅表现更好，还大幅减少了运行时间和内存使用量。 |
| [^52] | [ConcatPlexer: Additional Dim1 Batching for Faster ViTs.](http://arxiv.org/abs/2308.11199) | 本文提出了一种名为ConcatPlexer的方法，通过在视觉识别中使用附加的Dim1批处理（即连接）来提高吞吐量，同时准确性受到的影响较小。 |
| [^53] | [Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation.](http://arxiv.org/abs/2308.11197) | 该研究提供了使用嵌套交叉验证方法的定量证据，并提出了基于机器学习分析进行功效分析的方法。通过对交叉验证方法、特征和模型维度之间的相互作用进行蒙特卡罗模拟，比较了不同交叉验证方法的统计功效和置信度。同时，确定了获得统计显著结果所需的最小样本容量。 |
| [^54] | [Automatic Task Parallelization of Dataflow Graphs in ML/DL models.](http://arxiv.org/abs/2308.11192) | 通过对ML数据流图进行关键路径线性聚类和任务并行化，我们提出了一种优化ML/DL模型的自动并行化方法。与其他工作不同的是，我们通过一个新的工具Ramiel生成可读和可执行的并行Pytorch+Python代码。 |
| [^55] | [Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries.](http://arxiv.org/abs/2308.11189) | 本文提出了一种基于回应多样性的大型语言模型错误量化指标，这些指标独立于领域特定信息，并与失败概率强相关。实证结果展示了这些指标在少样本提示、思维链推理和错误检测方面的应用。 |
| [^56] | [A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology.](http://arxiv.org/abs/2308.11179) | 本研究提出了一种自下而上的框架，用于数字癌症组织中多器官细胞核的同时语义分割、实例分割和分类。通过引入额外的解码器头部，并利用独立的加权损失来产生语义分割、边界提议和分类图，我们解决了语义分割的挑战，并扩展到了同时进行实例分割和分类的问题。 |
| [^57] | [A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks.](http://arxiv.org/abs/2308.11162) | 该研究初步探索了使用深度网络在WHO乳腺分类中进行肿瘤区分的搜索和匹配方法。研究采用了深度学习模型提取的深度特征对35种肿瘤类型在数字图谱中进行了可视化分析。 |
| [^58] | [xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium.](http://arxiv.org/abs/2308.11155) | 在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。 |
| [^59] | [Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning.](http://arxiv.org/abs/2308.11154) | 使用深度强化学习的移动感知计算卸载于群体机器人中的应用，可以减轻计算负担，满足延迟要求并保证计算精度，同时使用最小的机器人能量。 |
| [^60] | [Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing.](http://arxiv.org/abs/2308.11152) | 本研究通过应用节能的基于神经形态计算的机器学习模型来进行卫星通信中的局内无线资源管理。实验结果表明，在面对传统卷积神经网络的对比中，基于脉冲神经网络在能效方面表现出色。 |
| [^61] | [LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report).](http://arxiv.org/abs/2308.11148) | 本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。 |
| [^62] | [Exploring Unsupervised Cell Recognition with Prior Self-activation Maps.](http://arxiv.org/abs/2308.11144) | 该论文提出了一种无监督细胞识别方法，通过先验自激活图生成伪掩膜作为训练目标。在多个数据集上的实验证明，该方法在细胞分割和多类别细胞检测任务中优于其他监督和弱监督方法。 |
| [^63] | [Graph Encoding and Neural Network Approaches for Volleyball Analytics: From Game Outcome to Individual Play Predictions.](http://arxiv.org/abs/2308.11142) | 本研究采用了图编码和神经网络方法，通过丰富的排球数据集，提高了排球预测的准确性，并通过比较基准模型的性能分析排球局势的潜在关系。 |
| [^64] | [Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems.](http://arxiv.org/abs/2308.11137) | 本研究重新研究了使用评论数据集的交互式推荐系统实验，并发现简单的贪婪模型可以实现与基于RL的模型相媲美的性能。 |
| [^65] | [Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances.](http://arxiv.org/abs/2308.11129) | 本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。 |
| [^66] | [How Expressive are Graph Neural Networks in Recommendation?.](http://arxiv.org/abs/2308.11127) | 本文对图神经网络在推荐中的表达能力进行了理论分析，发现现有的表达能力度量标准可能无法有效评估模型在推荐中的能力，提出了一个全面的理论分析方法。 |
| [^67] | [Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection.](http://arxiv.org/abs/2308.11119) | 本文提出了一种利用CLIP作为数据源的零样本异常检测方法，通过随机词语数据增强的方式改善了训练效率，并应用prompt-guided分类进行图像的检测。该方法克服了以往需为每个对象类别训练模型的低效问题，有潜力应用于工业领域。 |
| [^68] | [Development of a Novel Quantum Pre-processing Filter to Improve Image Classification Accuracy of Neural Network Models.](http://arxiv.org/abs/2308.11112) | 本文提出了一种新型的量子预处理滤波器（QPF），通过应用该方法可以提高神经网络模型的图像分类准确性，实验结果在MNIST和EMNIST数据集上分别取得了显著的提升，然而在GTSRB数据集上效果有所下降。 |
| [^69] | [CAME: Contrastive Automated Model Evaluation.](http://arxiv.org/abs/2308.11111) | CAME是一个不依赖训练集的对比自动化模型评估框架，通过理论分析和实证验证，建立了模型性能与对比损失之间的可预测关系，并取得了新的SOTA结果。 |
| [^70] | [Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models.](http://arxiv.org/abs/2308.11103) | 本研究评估了大型语言模型在重新识别匿名个人方面的能力，并发现模型大小、输入长度和指令调整是最重要的决定因素。 |
| [^71] | [Explicability and Inexplicability in the Interpretation of Quantum Neural Networks.](http://arxiv.org/abs/2308.11098) | 本文探索了量子神经网络的可解释性，引入了不可解释性带的概念，为理解如何构建负责任且可追究的量子人工智能模型迈出了一步。 |
| [^72] | [Video OWL-ViT: Temporally-consistent open-world localization in video.](http://arxiv.org/abs/2308.11093) | 本论文提出了Video OWL-ViT模型，将预训练的开放世界图像模型应用于视频定位任务，通过添加变换器解码器实现时间上的连续传播，相比于传统的跟踪-by-detection方法具有更好的时间一致性。 |
| [^73] | [Addressing Fairness and Explainability in Image Classification Using Optimal Transport.](http://arxiv.org/abs/2308.11090) | 本论文提出了使用最优输运理论解决图像分类中公平性和可解释性问题的综合方法，通过发现和解释模型中偏见区域的原因和影响来提供细粒度的解释。 |
| [^74] | [Stress representations for tensor basis neural networks: alternative formulations to Finger-Rivlin-Ericksen.](http://arxiv.org/abs/2308.11080) | 本研究通过探索一系列张量基神经网络模型，研究不同的应力表示方法，以替代传统的Finger-Rivlin-Ericksen形式。通过比较基于势能和基于系数的方法，以及差异，进一步拓展了在有限变形情况下建模超弹性材料的可能性。 |
| [^75] | [Long-Term Prediction of Natural Video Sequences with Robust Video Predictors.](http://arxiv.org/abs/2308.11079) | 这篇论文提出了一种稳健的视频预测器（RoViPs），结合了深层感知和基于不确定性的重建损失，并利用基于注意力的跳过连接来提高预测性能。通过使预测器对自身的预测误差具有鲁棒性，能够产生非常长的、逼真的自然视频序列。 |
| [^76] | [A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model.](http://arxiv.org/abs/2308.11075) | 本研究探讨了重整化群和深度学习在伊辛模型中的联系，并通过开发新的重整化技术得到了有关群流的新发现。 |
| [^77] | [Neural Amortized Inference for Nested Multi-agent Reasoning.](http://arxiv.org/abs/2308.11071) | 本研究引入了一种新的方法，利用神经网络对高阶社会推理进行摊销，从而加快嵌套多智能体推理的计算速度，实验结果表明该方法在计算效率上表现出色，同时准确性降低最小化。 |
| [^78] | [Topological Graph Signal Compression.](http://arxiv.org/abs/2308.11068) | 这项研究提出了一种基于拓扑结构的图信号压缩方法，通过处理高阶交互、聚类和消息传递等步骤，相比于传统方法在压缩信号时具有更好的重建误差，能够更好地捕捉和利用空间和时间特征。 |
| [^79] | [UnLoc: A Unified Framework for Video Localization Tasks.](http://arxiv.org/abs/2308.11062) | UnLoc是一个用于视频定位任务的统一框架，通过使用预训练的图像和文本模型以及视频-文本融合模型，实现了Moment Retrieval、Temporal Localization和Action Segmentation三个定位任务的最先进结果。 |
| [^80] | [Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression.](http://arxiv.org/abs/2308.11053) | 本文提出了一种超级双路径压缩方法用于联合回声消除和噪声抑制，通过时间频率双路径压缩实现广泛的压缩比，同时降低计算成本，且在固定压缩比下能够进一步改善性能。 |
| [^81] | [Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI.](http://arxiv.org/abs/2308.11047) | 这项研究提出了一种用于脑部MRI的单次学习方法，通过神经风格迁移进行图像协调，避免了生成对抗网络中的幻觉现象。该方法在收集来自不同临床地点的医学成像数据时可以提高协调的效果。 |
| [^82] | [Spurious Correlations and Where to Find Them.](http://arxiv.org/abs/2308.11043) | 本论文研究了错误相关性的发生原因以及其对标准ERM基线的影响，并观察到了这些原因与模型设计选择之间的模式。 |
| [^83] | [Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances.](http://arxiv.org/abs/2308.11038) | 本研究基于K-Means和P-Median模型提出了一种混合方法，通过使用道路网络距离来优化在城市环境下物流集散地的位置布置，以减少配送距离和碳足迹。 |
| [^84] | [RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition.](http://arxiv.org/abs/2308.11029) | 提出了RBA-GCN模型用于情感识别。该模型通过引入关系双层聚合和图生成模块，解决了GCN模型中的节点信息冗余和远距离上下文信息捕获问题。 |
| [^85] | [Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics.](http://arxiv.org/abs/2308.11027) | 拆分学习可以实现在不同的、私有维护的健康数据集之间协作训练深度学习模型，同时保护隐私。在医疗相关任务上，通过拆分学习训练的模型能够实现与中央和联邦学习模型相似的性能，提高了计算效率。 |
| [^86] | [Extreme Multilabel Classification for Specialist Doctor Recommendation with Implicit Feedback and Limited Patient Metadata.](http://arxiv.org/abs/2308.11022) | 本论文研究了基于隐式反馈和有限患者元数据的专科医生推荐的极限多标签分类问题。通过将传统的推荐设置转换为多标签分类问题，我们提出了一个统一模型，利用患者历史来实现更好的推荐性能。 |
| [^87] | [Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations.](http://arxiv.org/abs/2308.11021) | 本文介绍了使用多任务超图进行半监督学习的方法，并将其应用于地球观测问题。通过形成集成教师并生成可靠伪标签，我们的方法在NASA NEO数据集上表现出相对强基线和最新工作的改进。此外，我们展示了超图可以自适应地无监督地适应逐渐的数据分布变化。 |
| [^88] | [Instance-based Learning with Prototype Reduction for Real-Time Proportional Myocontrol: A Randomized User Study Demonstrating Accuracy-preserving Data Reduction for Prosthetic Embedded Systems.](http://arxiv.org/abs/2308.11019) | 本研究提出了一种基于kNN的学习技术，用于肢体假肢控制中的手势检测。通过数据集缩减方法，通过实验验证了准确性和实时要求的可靠集成。决策面映射（DSM）在减少数据集时表现出良好的潜力。 |
| [^89] | [Personalized Event Prediction for Electronic Health Records.](http://arxiv.org/abs/2308.11013) | 该论文研究了个性化的电子健康记录事件预测，提出了多个新的预测模型和方法以更好地适应个体差异。 |
| [^90] | [Using language models in the implicit automated assessment of mathematical short answer items.](http://arxiv.org/abs/2308.11006) | 这项研究提出了一种使用语言模型的新方法来评估数学简答题的正确性和误解，并通过值识别流程提供针对性反馈。 |
| [^91] | [Autonomous Detection of Methane Emissions in Multispectral Satellite Data Using Deep Learning.](http://arxiv.org/abs/2308.11003) | 本研究利用深度学习方法的图像识别能力，实现了对多光谱卫星数据中甲烷泄漏的自动检测。 |
| [^92] | [Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT.](http://arxiv.org/abs/2308.11001) | 本文利用可解释的人工智能方法，分析了研究人员对ChatGPT在不同使用方面的情感。通过提出一种新的面向方面情感分析技术，使得这种分析不受文本数据长度限制，并提供了对新数据集的宝贵洞见。 |
| [^93] | [Eigenvalue-based Incremental Spectral Clustering.](http://arxiv.org/abs/2308.10999) | 本文介绍了一种基于特征值的增量谱聚类方法，通过将数据集划分为子集并进行聚类和合并，可以获得与聚类整个数据集相近的结果。 |
| [^94] | [SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models.](http://arxiv.org/abs/2308.10997) | 本文提出了一种使用马尔可夫随机场（MRF）模型的轻量级方法，用于实现图像不同区域的相容性，以降低生成文本到图像模型的计算成本。 |
| [^95] | [Deep Learning Techniques in Extreme Weather Events: A Review.](http://arxiv.org/abs/2308.10995) | 本综述提供了极端天气事件中深度学习技术的综合概述。我们探讨了深度学习在各种天气预测方面的潜力，并强调了其捕捉复杂模式和非线性关系的能力。此外，我们还讨论了当前方法的局限性和未来的发展方向。 |
| [^96] | [SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance.](http://arxiv.org/abs/2308.10973) | 使用Supervised Contrastive Learning和欧氏距离的简单方法（SupEuclid）在OoD检测中取得了最先进的结果，无需复杂模型或超参数调整，为进一步实验和分析提供了强大且易用的基线。 |
| [^97] | [MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer.](http://arxiv.org/abs/2308.10968) | 本论文通过使用神经风格转换进行正规化，实现了在有限数据条件下从低质量图像重建高质量图像的目标。实验结果验证了该方法在临床MRI扫描中的有效性和潜力。 |
| [^98] | [Artificial intelligence-driven antimicrobial peptide discovery.](http://arxiv.org/abs/2308.10921) | 人工智能在抗菌肽发现中起到了重要作用，通过区分和生成方法，可以帮助识别有前途的候选化合物，并生成具有所需性质的抗菌肽。 |
| [^99] | [Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge.](http://arxiv.org/abs/2308.10918) | 本文介绍了一种利用基于Metapath的半监督学习的新颖方法，用于图异常检测。通过在编码器和解码器中使用GCN层来有效传播上下文信息，以及特别设计的异常社区，该方法在结构和属性差异的学习中表现出优越性能。通过实验证明了该方法的有效性，为未来的研究提供了重要的思路和方向。 |
| [^100] | [PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model.](http://arxiv.org/abs/2308.10917) | 该研究提出了一种监督式多头注意力机制模型（SMA），通过学习多组学数据的全局和局部特征信息，成功分类了癌症亚型。通过融合模块深度融合Siamese的多头注意力编码器，提高了模型的参数丰富度。在模拟、单细胞和癌症多组学数据中，SMA模型实现了最高的准确性、F1宏观、F1加权和癌症亚型的准确分类。 |
| [^101] | [DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data.](http://arxiv.org/abs/2308.10915) | DiffPrep 提出了一种自动且高效地搜索数据预处理管道的方法，以最大化机器学习模型的性能。这通过将数据预处理管道搜索问题形式化为一个双层优化问题，并将离散的非可微搜索空间转化为连续空间来实现。 |
| [^102] | [Automated mapping of virtual environments with visual predictive coding.](http://arxiv.org/abs/2308.10913) | 本文提出了一种利用预测编码进行虚拟环境映射的算法，该算法可以根据感知数据构建空间地图，并在不同输入模态下具有泛化能力。 |
| [^103] | [Global Warming In Ghana's Major Cities Based on Statistical Analysis of NASA's POWER Over 3-Decades.](http://arxiv.org/abs/2308.10909) | 该研究使用NASA的POWER数据，通过统计分析和机器学习预测了加纳四个主要城市的长期温度趋势，结果显示工业城市阿克拉有明显的变暖趋势，并且XGBoost模型的有效性得到了证实。 |
| [^104] | [MLOps: A Review.](http://arxiv.org/abs/2308.10908) | 本研究综述了机器学习运维（MLOps）方法在解决问题中的可接受答案的重要性，并评估了22篇应用MLOps理念的论文。研究发现需要更加有效的MLOps方法以减少人类的参与。 |
| [^105] | [Analyzing Quantization in TVM.](http://arxiv.org/abs/2308.10905) | 该论文研究了在TVM中8位量化的性能问题，并讨论了兼容性和优化机会。 |
| [^106] | [Majorana Demonstrator Data Release for AI/ML Applications.](http://arxiv.org/abs/2308.10856) | 此数据发布包含Majorana示范器实验的校准数据子集，旨在支持人工智能和机器学习算法在该数据上的训练和测试。 |
| [^107] | [DynED: Dynamic Ensemble Diversification in Data Stream Classification.](http://arxiv.org/abs/2308.10807) | DynED是一种动态集成多样化方法，基于MRR结合了组件的多样性和预测准确性，在数据流环境中实现了更高的准确率。 |
| [^108] | [Stabilizing Unsupervised Environment Design with a Learned Adversary.](http://arxiv.org/abs/2308.10797) | 本论文研究了无监督环境设计（UED）中PAIRED方法存在的问题，并提出了解决方案，使其在实际性能上能够与或超过最先进的方法。 |
| [^109] | [FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly.](http://arxiv.org/abs/2308.10608) | FocalDreamer是一个基于文本驱动的3D编辑框架，能够通过焦点融合装配的方式实现精细化编辑，并且在几何和风格上能够保持整体一致性。实验证明了FocalDreamer在编辑能力上的优势。 |
| [^110] | [Information Theory-Guided Heuristic Progressive Multi-View Coding.](http://arxiv.org/abs/2308.10522) | 通过信息理论提出了一个新的多视图学习框架，并基于此框架构建了一种渐进式多视图编码方法。 |
| [^111] | [Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis.](http://arxiv.org/abs/2308.10511) | 本论文旨在通过使用基于掩膜区域卷积神经网络 (Mask-RCNN) 对孟加拉文档进行版面分析，提升性能。通过逐步的超参数调整，我们达到了0.889的良好dice分数。虽然在应用英文文档模型时遇到了一些挑战，但这表明每种语言都有其特定的问题需要解决。 |
| [^112] | [An Effective Method using Phrase Mechanism in Neural Machine Translation.](http://arxiv.org/abs/2308.10482) | 本论文介绍了一种使用短语机制的神经机器翻译方法，PhraseTransformer，在越南语-中文平行语料库上取得了较高的BLEU得分。 |
| [^113] | [Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting.](http://arxiv.org/abs/2308.10425) | 本研究提出了一种名为时空自适应嵌入的新组件，在普通的Transformer中实现了领先于其他方法的交通预测性能，通过捕捉交通时间序列中的时空关系和时间信息实现了优秀的结果。 |
| [^114] | [FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection.](http://arxiv.org/abs/2308.10236) | FedSIS是一种用于隐私保护的领域泛化的新框架，采用联邦式分割学习和中间表示采样的方法，解决了面部展示攻击检测算法泛化能力不足的问题。 |
| [^115] | [Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs.](http://arxiv.org/abs/2308.09895) | 本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。 |
| [^116] | [DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets.](http://arxiv.org/abs/2308.09878) | 该论文提出了一种新的方法来解决机器学习中数据不平衡的问题，通过使用深层感知嵌入和聚类来计算样本的似然性，并使用广义聚焦损失函数在训练过程中对样本进行不同的加权。实验验证了该方法的有效性。 |
| [^117] | [Taken by Surprise: Contrast effect for Similarity Scores.](http://arxiv.org/abs/2308.09765) | 提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。 |
| [^118] | [MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.](http://arxiv.org/abs/2308.09729) | 本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。 |
| [^119] | [PMET: Precise Model Editing in a Transformer.](http://arxiv.org/abs/2308.08742) | 该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。 |
| [^120] | [Consciousness in Artificial Intelligence: Insights from the Science of Consciousness.](http://arxiv.org/abs/2308.08708) | 本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。 |
| [^121] | [Deep learning for spike detection in deep brain stimulation surgery.](http://arxiv.org/abs/2308.05755) | 本文介绍了一种利用深度学习分析深脑刺激神经外科手术期间神经活动记录的方法，通过卷积神经网络对时间窗口内的神经活动进行分类，实现了高准确率的尖峰检测。 |
| [^122] | [Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI.](http://arxiv.org/abs/2308.05525) | 本文研究了三维点云的临界点与非分布样本之间的相互作用，并将临界点的概念推广为重要性度量方法。通过仅基于非重要点进行分类网络训练，可以提高鲁棒性，同时在干净数据集上会有些性能损失。建议使用标准化熵选择非临界点集合的自适应阈值。这种重要性度量方法计算速度极快，可以应用于可解释AI、离群值去除、不确定性估计、鲁棒分类和对抗性防御等多种应用。 |
| [^123] | [The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers.](http://arxiv.org/abs/2308.03945) | 本文研究了在大规模异构联邦学习中，通过Transformer模型实现泛化和个性化的前景，并通过广泛的比较实验证明了Transformer在大规模异构FL任务中相对于深度神经网络的优势，并通过分析中心核对齐（CKA）表示相似性来深入了解Transformer有前景的能力背后的原因。 |
| [^124] | [Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits.](http://arxiv.org/abs/2308.03212) | 本文研究表明平均困难注意力变换器和对数精度变换器都可以模拟常深度阈值电路，其中后者由于生成统一的电路族而更健壮。 |
| [^125] | [Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective.](http://arxiv.org/abs/2307.16889) | 本文提出了一种基于样本选择的噪声标签学习方法，该方法可以在真实场景下区分不同类型的噪声，并利用噪声的语义信息进行学习。通过构建原型向量和计算样本与原型向量之间的距离，该方法可以改进标签的准确性。实证评估结果表明了该方法的鲁棒性和有效性。 |
| [^126] | [Autonomous Payload Thermal Control.](http://arxiv.org/abs/2307.15438) | 该论文提出了一种基于深度强化学习的框架，利用软演员-评论家算法在卫星上学习热控制策略，以解决小型卫星中热控制的挑战。该框架在模拟环境和实际空间处理计算机上进行了评估，并证明能够辅助传统热控制系统，保持载荷温度在可操作范围内。 |
| [^127] | [Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space.](http://arxiv.org/abs/2307.14953) | 本文提出了一种基于字典学习和最优传输的MSDA框架，通过将每个域表示为字典原子的Wasserstein重心来缓解数据分布偏移。根据该字典，提出了两种新的MSDA方法，分别基于目标域标记样本的重构和在原子分布上学习的分类器的集成。在多个基准测试集上进行的实验证明，这些方法在分类任务上取得了显著的改进效果。 |
| [^128] | [Understanding Silent Failures in Medical Image Classification.](http://arxiv.org/abs/2307.14729) | 这项研究通过对医学应用中的分类系统进行分析，发现目前的置信度评分函数无法可靠地防止隐性失败，强调了对数据中失败根本原因的深入理解的重要性。引入了SF-Visuals，一个通过潜在空间聚类来可视化偏移和失败的交互式分析工具。 |
| [^129] | [Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging.](http://arxiv.org/abs/2307.03266) | 本文通过对前列腺成像中的新型基础模型UniverSeg进行了实证评估研究，并将其与传统方法进行了比较。结果表明基础模型可能是医学成像领域未来的方向。 |
| [^130] | [Variational Autoencoding Molecular Graphs with Denoising Diffusion Probabilistic Model.](http://arxiv.org/abs/2307.00623) | 这篇论文提出了一种新颖的分子深度生成模型，将分层结构融入概率潜在向量中，并通过去噪扩散概率模型来设计有效的分子潜在向量，用于分子性质预测。 |
| [^131] | [Maximum Entropy Heterogeneous-Agent Mirror Learning.](http://arxiv.org/abs/2306.10715) | 最大熵异质代理镜像学习(MEHAML)是一种新的理论框架，通过最大熵原理设计了最大熵MARL的演员-评论家算法，具有联合最大熵目标的单调改进和收敛至中位响应均衡(QRE)的期望特性，并通过扩展常用的强化学习算法HASAC来验证其实用性和在探索和稳健性方面的显著改进。 |
| [^132] | [Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis.](http://arxiv.org/abs/2306.10168) | 本研究提出了一种新的方法，通过比较神经网络系统的动力学特征来判断它们是否利用了相同的内部过程进行计算。 |
| [^133] | [Survey on Sociodemographic Bias in Natural Language Processing.](http://arxiv.org/abs/2306.08158) | 本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。 |
| [^134] | [On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks.](http://arxiv.org/abs/2306.05557) | 本文研究了GNN在测试时节点的本地同质性水平与其图的全局同质性水平偏离时的性能，并介绍一种新参数用于控制同质性，在生成的图中系统地研究本地同质性的影响。 |
| [^135] | [Blockwise Parallel Transformer for Long Context Large Models.](http://arxiv.org/abs/2305.19370) | 本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。 |
| [^136] | [FairDP: Certified Fairness with Differential Privacy.](http://arxiv.org/abs/2305.16474) | FairDP是一种同时确保差分隐私和公平性的新型机制，通过独立为不同的个体群体训练模型，在训练过程中逐步整合来自群体模型的知识，制定综合模型以平衡隐私、效用和公平性的下游任务。相比现有方法，FairDP展示了更好的模型效益、隐私和公平性的权衡。 |
| [^137] | [SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes.](http://arxiv.org/abs/2305.13998) | SMT 2.0是一个开源的代理模型工具包，引入了处理混合变量和层次变量的能力，并通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。 |
| [^138] | [Gibbs free energies via isobaric-isothermal flows.](http://arxiv.org/abs/2305.13233) | 采用机器学习模型利用等压等温流得到吉布斯自由能，并在单原子水的结晶中进行了测试，表现出优秀的性能。 |
| [^139] | [Causality-Aided Trade-off Analysis for Machine Learning Fairness.](http://arxiv.org/abs/2305.13057) | 本论文使用因果分析作为一种方法，通过分析机器学习流程中公平性参数与其他关键指标的权衡，提供了一种系统理解和决策基础，帮助开发者在提供公平机器学习服务时做出明智的决策。 |
| [^140] | [DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging Scheduling in Large-scale Networked Facilities.](http://arxiv.org/abs/2305.11195) | 本文提出了一种基于深度学习和近似算法技术的数据驱动优化框架DClEVerNet，可以优化大规模网络化的EV充电站的预约管理程序，最大化EV用户的总福利收益，同时考虑到网络的可用功率容量和站点的入住限制。 |
| [^141] | [Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage.](http://arxiv.org/abs/2305.09659) | 本论文提出了一个名为P2MPO的算法框架，用于解决基于鲁棒离线RL的问题。该框架结合了灵活的模型估计子例程和双重悲观的策略优化步骤，采用双重悲观性原则以克服模型偏移等问题。研究表明，在模型准确性的假设下，该框架在拥有良好的鲁棒部分覆盖数据的情况下是具备高效性的。 |
| [^142] | [Designing Discontinuities.](http://arxiv.org/abs/2305.08559) | 本文通过一种量化理论方法优化不连续变量的设计，以平衡效应大小的增益和损失，并开发了一种计算效率高的强化学习算法。 |
| [^143] | [On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code.](http://arxiv.org/abs/2305.04106) | 本文强调预训练语言模型需要对代码进行持续学习以适应变化的软件数据分布，使其具有更好的泛化能力。 |
| [^144] | [CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction.](http://arxiv.org/abs/2304.14633) | 研究团队提出了一种基于代价体的3D神经重建框架CVRecon，利用丰富的几何嵌入来促进3D几何特征学习。通过引入射线上下文补偿代价体（RCCV），有效提高了视角相关信息的完整性和鲁棒性，并在各种度量方面显着提高了重建质量。 |
| [^145] | [SAFE: Machine Unlearning With Shard Graphs.](http://arxiv.org/abs/2304.13169) | 本论文提出了一种使用 shard graph 进行机器遗忘的方法，以实现在最小化遗忘成本的情况下适应多样数据的大型模型，并取得了较高的准确性。 |
| [^146] | [Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning.](http://arxiv.org/abs/2304.11930) | 该研究描述了一种基于深度学习的新方法，用于模块化核探测器的无标记时间分析，其能够利用单个探测器内部时间相关性，实现有意义和准确的映射函数。 |
| [^147] | [Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning.](http://arxiv.org/abs/2304.03193) | 本文介绍了一种利用多视图融合及两步式迁移学习增强的自动内镜结石识别方法，可提高肾结石分类准确度达6%以上。 |
| [^148] | [Equivariant Networks for Porous Crystalline Materials.](http://arxiv.org/abs/2304.01628) | 本研究开发了一种模型，它在架构中合并了晶体的单元格对称性，并显式地建模了多孔结构，可更准确地预测多孔晶体材料的吸附热。 |
| [^149] | [Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers.](http://arxiv.org/abs/2303.16464) | 本文理论证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。本文的选择损失函数方针为Adam或AdamW优化算法的使用提供了指导。实验结果表明了Lipschitz常数较低且最大值较小的损失函数可以提高模型的泛化能力。 |
| [^150] | [Machine Learning for QoS Prediction in Vehicular Communication: Challenges and Solution Approaches.](http://arxiv.org/abs/2302.11966) | 本文探讨了机器学习在车联网通信中的QoS预测，重点讨论了采样过程、数据集特征分析、数据拆分效果和数据可用性等方面。这项研究对于提升最大吞吐量预测在汽车行业中的应用具有重要意义。 |
| [^151] | [DTAAD: Dual Tcn-Attention Networks for Anomaly Detection in Multivariate Time Series Data.](http://arxiv.org/abs/2302.10753) | 这项研究提出了一种基于Transformer和双重TCN-Attention网络的DTAAD模型，用于多变量时间序列数据的异常检测和诊断。通过集成设计和引入缩放方法和反馈机制，该模型实现了快速准确定位异常，并提高了预测精度和扩大了相关差异。 |
| [^152] | [Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization.](http://arxiv.org/abs/2302.04552) | 本论文研究了乐观的在线镜像下降算法在Stochastically Extended Adversarial (SEA)模型中的理论保证，对于凸和平滑的函数，其遗憾界限为O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，对于强凸和平滑的函数，其界限为O(sqrt(σ_{\max}^2) + sqrt(Σ_{\max}^2))。 |
| [^153] | [Truveta Mapper: A Zero-shot Ontology Alignment Framework.](http://arxiv.org/abs/2301.09767) | 提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。 |
| [^154] | [Distributed Black-box Attack against Image Classification Cloud Services.](http://arxiv.org/abs/2210.16371) | 本文研究了分布式黑盒攻击云服务的图像分类器，通过直接应用于云API而不是本地模型，避免了之前研究中的错误，并利用负载平衡实现了攻击时间的减少。 |
| [^155] | [Perceptual Grouping in Contrastive Vision-Language Models.](http://arxiv.org/abs/2210.09996) | 本文研究视觉语言模型是否能够理解物体在图像中的位置，并将视觉相关部分组合在一起。我们提出了一些修改，使模型独特地学习了语义和空间信息，并通过多个指标衡量了性能。 |
| [^156] | [Risk of re-identification for shared clinical speech recordings.](http://arxiv.org/abs/2210.09975) | 本研究探究了分享的临床语音记录的重新识别风险，发现风险与搜索空间大小和语音记录的性质有关。研究结果表明，使用说话人识别系统可以重新识别特定参与者。 |
| [^157] | [ISEE.U: Distributed online active target localization with unpredictable targets.](http://arxiv.org/abs/2210.09107) | 本文提出了一种分布式在线主动学习算法ISEE.U，用于不可预测目标的定位，无需参数调整且具有稳健性。通过局部估计的费舍尔信息矩阵，每个节点计算最大化整体目标位置精度的控制策略，与现有算法相比，在目标运动不规律时表现优秀且计算时间极少。 |
| [^158] | [CrowdGuard: Federated Backdoor Detection in Federated Learning.](http://arxiv.org/abs/2210.07714) | CrowdGuard是一种在联邦学习中有效防御后门攻击的新机制，通过利用客户对个别模型的反馈分析行为，克服了现有技术的不足。 |
| [^159] | [Sequence Learning Using Equilibrium Propagation.](http://arxiv.org/abs/2209.09626) | 本文提出了使用平衡传播（EP）进行序列学习的方法，EP是一种更符合生物可信性的学习框架，与传统的反向传播方法相比具有更广泛的应用性。文中利用现代Hopfield网络的进展，解决了基于EP的模型无法处理动态输入的问题，为复杂序列学习问题提供了解决方案。 |
| [^160] | [Discovering Conservation Laws using Optimal Transport and Manifold Learning.](http://arxiv.org/abs/2208.14995) | 本论文提出了一种使用最优传输和流形学习的非参数方法来发现复杂系统中的保守定律。通过测试在不同物理系统上，证明该方法能够确定保守定律的数量并提取其值，为分析系统动力学和构建稳定预测模型提供了新的可靠而解释性强的方法。 |
| [^161] | [Probable Domain Generalization via Quantile Risk Minimization.](http://arxiv.org/abs/2207.09944) | 该论文提出了一种通过Quantile Risk Minimization（QRM）方法实现可能的领域泛化的概率性框架。通过最小化预测器风险分布在不同领域上的分位数，该方法可以实现在测试时以高概率表现良好的预测器。 |
| [^162] | [Active Exploration for Inverse Reinforcement Learning.](http://arxiv.org/abs/2207.08645) | AceIRL提出了一种新的逆强化学习算法，通过主动探索来学习奖励函数和策略，在不需要环境生成模型的情况下，能够确定可行奖励函数的置信区间，并找到侧重于环境中最有信息的区域的探索策略。 |
| [^163] | [FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation.](http://arxiv.org/abs/2205.14900) | 本研究提出了一种名为FRAug的方法，用于解决联邦学习中具有非独立同分布特征的问题。该方法通过在嵌入空间中生成合成的客户端特定样本来增强客户端数据集。通过训练一个共享的生成模型，将来自不同特征分布的客户端的知识融合起来。 |
| [^164] | [Differential Privacy Amplification in Quantum and Quantum-inspired Algorithms.](http://arxiv.org/abs/2203.03604) | 本研究提供了量子和量子启发式算法的差分隐私放大界限。首次展示了通过量子编码的经典数据集的算法或量子启发式经典采样结果能够放大差分隐私。研究还证明了量子版本的差分隐私可以通过满足混合条件的量子通道组合来放大。 |
| [^165] | [Quantum Local Differential Privacy and Quantum Statistical Query Model.](http://arxiv.org/abs/2203.03591) | 本文将量子统计查询和量子差分隐私在局部模型下进行了等价建立，扩展了经典结果到量子领域。在局部差分隐私下推导了量子相对熵的强数据处理不等式，并将其应用于受限测量的不对称假设测试任务。此外，还讨论了在局部差分隐私下的量子多方计算任务。 |
| [^166] | [Self-Training: A Survey.](http://arxiv.org/abs/2202.12040) | 自主训练方法是一种半监督算法，无需额外假设数据分布，在低密度区域找到决策边界，并使用学习分类器的输出分数作为置信度，通过为无标签样本分配伪标签，迭代地学习分类器，从而丰富有标签训练数据。 |
| [^167] | [Information Theory-Guided Heuristic Progressive Multi-View Coding.](http://arxiv.org/abs/2109.02344) | 该论文从信息论的视角重新思考了现有的多视角学习范式，并提出了一种新的信息论框架，用于广义的多视角学习。在此框架的指导下，作者提出了一种三层递进的多视角编码方法。 |
| [^168] | [Calibrating and Improving Graph Contrastive Learning.](http://arxiv.org/abs/2101.11525) | 该论文提出了一种新的规则化方法，Contrast-Reg，通过应用期望校准误差（ECE）来校准和改进图对比学习算法，以提高在下游任务中的性能。 |
| [^169] | [MMD-regularized Unbalanced Optimal Transport.](http://arxiv.org/abs/2011.05001) | 本文研究了使用MMD正则化的非平衡最优输运问题，提出了基于Fenchel对偶性的新度量方法，还提出了基于有限样本的凸规划用于估算问题，证明了估计量的一致性和误差速率。 |

# 详细

[^1]: 语义多分辨率通信

    Semantic Multi-Resolution Communications. (arXiv:2308.11604v1 [cs.LG])

    [http://arxiv.org/abs/2308.11604](http://arxiv.org/abs/2308.11604)

    本论文提出了一个基于深度学习的多分辨率联合源-信道编码框架，在数据重建方面取得了显著进展，并在多用户和/或多分辨率方式下具有优势。同时，该框架具有潜力实现语义通信，通过保持特定的语义属性来扩展其目标。

    

    基于深度学习的联合源-信道编码在数据重建方面与分离的源-信道编码相比取得了显著的进展。这种优势在于在处理有限块长度数据时，分离的源-信道编码的非最优性。此外，分离的源-信道编码在多用户和/或多分辨率方式下重构数据时表现不佳，因为它只试图满足最差信道和/或最高质量数据。为了克服这些限制，我们提出了一个新颖的基于深度学习的多分辨率联合源-信道编码框架，受到多任务学习 (MTL) 的概念启发。该框架通过分层逐渐编码不同分辨率的数据，并通过利用当前和过去的编码数据层来有效解码。此外，该框架在语义通信方面有巨大潜力，其目标不仅仅是数据重建，还包括在整个通信过程中保持特定的语义属性。

    Deep learning based joint source-channel coding (JSCC) has demonstrated significant advancements in data reconstruction compared to separate source-channel coding (SSCC). This superiority arises from the suboptimality of SSCC when dealing with finite block-length data. Moreover, SSCC falls short in reconstructing data in a multi-user and/or multi-resolution fashion, as it only tries to satisfy the worst channel and/or the highest quality data. To overcome these limitations, we propose a novel deep learning multi-resolution JSCC framework inspired by the concept of multi-task learning (MTL). This proposed framework excels at encoding data for different resolutions through hierarchical layers and effectively decodes it by leveraging both current and past layers of encoded data. Moreover, this framework holds great potential for semantic communication, where the objective extends beyond data reconstruction to preserving specific semantic attributes throughout the communication process. Th
    
[^2]: Tryage: 实时智能路由用户提示到大型语言模型

    Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])

    [http://arxiv.org/abs/2308.11601](http://arxiv.org/abs/2308.11601)

    Tryage是一个上下文感知的路由系统，能够根据对个体输入提示的分析，从模型库中选择最佳的专家模型，以消除模型选择和定制化的负担，释放庞大的新兴模型库的巨大威力给最终用户。

    

    变压器架构和自注意机制的引入导致了在特定下游任务和数据领域训练的语言模型的爆炸性增长。在Hugging Face生态系统中有超过200,000个模型，用户在选择和优化模型以适应多方面的工作流程和数据领域的同时，还要解决计算、安全和时效性等问题。迫切需要机器学习框架来消除模型选择和定制化的负担，并释放庞大的新兴模型库的巨大威力给最终用户。在这里，我们提出了一个上下文感知的路由系统Tryage，它利用语言模型路由器根据对个体输入提示的分析，从模型库中选择最佳的专家模型。受大脑中的丘脑路由器启发，Tryage采用感知路由器来预测下游模型在提示上的性能，并根据目标做出路由决策。

    The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
    
[^3]: 基于量子力学视角的量化优化方法

    Quantization-based Optimization with Perspective of Quantum Mechanics. (arXiv:2308.11594v1 [quant-ph])

    [http://arxiv.org/abs/2308.11594](http://arxiv.org/abs/2308.11594)

    基于量子力学视角的量化优化方法在全局优化中利用薛定谔方程推导的隧道效应，从而能够避免局部最小值。

    

    基于热力学的统计和随机分析一直是随机全局优化的主要分析框架。最近，出现了用于全局优化的量子退火或量子隧道算法，我们需要一个新的研究框架来进行全局优化算法。在本文中，我们提供了基于薛定谔方程的量化优化的分析，以揭示量子力学中的哪些属性使全局优化成为可能。我们提出，薛定谔方程推导出的隧道效应使得量化优化能够逃离局部最小值。此外，我们确认这种隧道效应是包含在基于量子力学的全局优化中的相同属性。对标准多模态基准函数进行的实验表明了所提出的分析的有效性。

    Statistical and stochastic analysis based on thermodynamics has been the main analysis framework for stochastic global optimization. Recently, appearing quantum annealing or quantum tunneling algorithm for global optimization, we require a new researching framework for global optimization algorithms. In this paper, we provide the analysis for quantization-based optimization based on the Schr\"odinger equation to reveal what property in quantum mechanics enables global optimization. We present that the tunneling effect derived by the Schr\"odinger equation in quantization-based optimization enables to escape of a local minimum. Additionally, we confirm that this tunneling effect is the same property included in quantum mechanics-based global optimization. Experiments with standard multi-modal benchmark functions represent that the proposed analysis is valid.
    
[^4]: 种族与青少年抑郁症的预测：利用机器学习和深度学习跨种族群体的研究

    What's Race Got to do with it? Predicting Youth Depression Across Racial Groups Using Machine and Deep Learning. (arXiv:2308.11591v1 [cs.LG])

    [http://arxiv.org/abs/2308.11591](http://arxiv.org/abs/2308.11591)

    本研究利用机器学习和深度学习方法，使用全国青少年风险行为监测系统（YRBSS）调查数据，对不同种族群体中的青少年抑郁症进行预测。研究发现种族亚组之间存在着相关因素的差异，并呼吁提供更多广泛和多样化的数据集。

    

    抑郁症是一种常见但严重的心理障碍，每年影响数百万美国高中生。然而，准确的诊断和早期发现仍然是重大挑战。在公共卫生领域，研究表明神经网络在识别其他疾病如癌症和HIV方面产生了有希望的结果。本研究提出了类似的方法，利用机器学习和人工神经网络模型对学生的抑郁症进行分类。此外，本研究还强调了种族亚组的相关因素差异，并主张需要更广泛和多样化的数据集。模型在全国青少年风险行为监测系统（YRBSS）调查数据上进行训练，其中通过统计分析找到了抑郁症的最相关因素。该调查数据是一个包含15000个条目的结构化数据集，包括三个种族子集，每个子集包含900个条目。在分类方面，研究问题被建模为一个有监督的问题。

    Depression is a common yet serious mental disorder that affects millions of U.S. high schoolers every year. Still, accurate diagnosis and early detection remain significant challenges. In the field of public health, research shows that neural networks produce promising results in identifying other diseases such as cancer and HIV. This study proposes a similar approach, utilizing machine learning (ML) and artificial neural network (ANN) models to classify depression in a student. Additionally, the study highlights the differences in relevant factors for race subgroups and advocates the need for more extensive and diverse datasets. The models train on nationwide Youth Risk Behavior Surveillance System (YRBSS) survey data, in which the most relevant factors of depression are found with statistical analysis. The survey data is a structured dataset with 15000 entries including three race subsets each consisting of 900 entries. For classification, the research problem is modeled as a supervi
    
[^5]: SVM方法的一种替代方法用于数据分类

    An alternative to SVM Method for Data Classification. (arXiv:2308.11579v1 [cs.LG])

    [http://arxiv.org/abs/2308.11579](http://arxiv.org/abs/2308.11579)

    本文提出了一种新的方法来替代支持向量机(SVM)方法进行数据分类，在保持类似性能的同时，对SVM方法的一些缺点进行了改进和敏感处理。

    

    支持向量机(SVM)是一种广泛应用的核方法，用于数据分类，并在许多实际应用中证明了其效率。然而，该方法存在一些缺点，包括时间处理、高维情况下优化过程失败的风险、多类别、不平衡类别和动态分类的泛化能力。本文提出了一种新的方法，具有类似的性能，对上述缺点进行了敏感改进。这种新方法基于到包含映射原始类别的最优子空间的最小距离。

    Support vector machine (SVM), is a popular kernel method for data classification that demonstrated its efficiency for a large range of practical applications. The method suffers, however, from some weaknesses including; time processing, risk of failure of the optimization process for high dimension cases, generalization to multi-classes, unbalanced classes, and dynamic classification. In this paper an alternative method is proposed having a similar performance, with a sensitive improvement of the aforementioned shortcomings. The new method is based on a minimum distance to optimal subspaces containing the mapped original classes.
    
[^6]: 改变情绪识别建模方式:通用大型模型的出现

    Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])

    [http://arxiv.org/abs/2308.11578](http://arxiv.org/abs/2308.11578)

    本论文综合调查了大型语言模型（LLMs）在情绪识别中表现的各个方面，包括上下文学习、少样本学习和准确度等。LLMs的出现为情绪识别建模带来了新的潜力和机会。

    

    在情绪识别或情感计算的诞生之后，由于其广泛应用，它已经成为一个越来越活跃的研究课题。在过去的几十年里，情绪识别模型逐渐从统计浅层模型迁移到基于神经网络的深度模型，可以显著提升情绪识别模型的性能，并在不同的基准测试中始终取得最佳结果。因此，近年来，深度模型一直被视为情绪识别的首选。然而，大型语言模型（LLMs）的出现，如ChatGPT，由于它们具备的零/少样本学习、上下文学习、连贯思维等能力，在情绪识别方面引起了巨大的惊讶，而这些能力在以前的深度模型中从未出现。在本文中，我们全面调查了LLMs在情绪识别方面的表现，包括上下文学习、少样本学习、准确度等各方面。

    After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, acc
    
[^7]: 神经动力学的低阶张量秩学习

    Low Tensor Rank Learning of Neural Dynamics. (arXiv:2308.11567v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.11567](http://arxiv.org/abs/2308.11567)

    研究发现通过学习过程中的张量秩演化来理解神经元连接在学习中的协调变化。研究表明训练过的递归神经网络的权重矩阵通常具有低秩结构，而这种结构在整个学习过程中保持在一个固定的低维子空间中。对真实权重进行低秩分解验证了这一观察结果。

    

    学习依赖于神经元群体中的协调突触变化。因此，了解学习过程中突触连接的集体演化是神经科学和机器学习中的一个关键挑战。近期的研究表明，经过训练的递归神经网络（RNN）的权重矩阵通常是低秩的，但是这种低秩结构如何在学习过程中展开还不清楚。为了解决这个问题，我们研究了整个学习过程中由权重矩阵形成的3阶张量的秩。通过用不同秩的RNN拟合大规模神经记录的运动学习任务，我们发现推断的权重是低阶张量秩的，因此在整个学习过程中在一个固定的低维子空间中演化。接下来，我们通过在真实权重上直接进行低阶张量秩分解，并展示我们所使用的方法，验证了低阶张量秩学习的观察结论。

    Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applie
    
[^8]: 多事件视频文本检索

    Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])

    [http://arxiv.org/abs/2308.11551](http://arxiv.org/abs/2308.11551)

    本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。

    

    视频文本检索（VTR）是互联网上海量视频文本数据时代中一项关键的多模态任务。使用双流视觉-语言模型架构学习视频文本对的联合表示成为VTR任务中一种突出的方法。然而，这些模型在假设视频文本对应是双射的情况下运行，并忽视了更实际的情况，即视频内容通常涵盖多个事件，而用户查询或网页元数据等文本往往是具体的，并对应单个事件。这造成了之前的训练目标与实际应用之间的差距，在推理过程中可能导致早期模型的性能下降。本研究引入了多事件视频文本检索（MeVTR）任务，针对每个视频包含多个不同事件的场景，作为传统视频文本检索任务的一个利基场景。

    Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
    
[^9]: 一种克服局部最小值的算法用于训练回归MLP神经网络

    A free from local minima algorithm for training regressive MLP neural networks. (arXiv:2308.11532v1 [cs.LG])

    [http://arxiv.org/abs/2308.11532](http://arxiv.org/abs/2308.11532)

    本文介绍了一种克服局部最小值问题的算法，用于训练回归MLP神经网络。

    

    本文介绍了一种创新的方法用于训练回归MLP网络，该方法不易受到局部最小值的影响。误差反向传播算法在机器学习技术的发展中起到了重要作用，但是该算法存在局部最小值的问题。本文提出的算法避免了局部最小值的问题。

    In this article an innovative method for training regressive MLP networks is presented, which is not subject to local minima. The Error-Back-Propagation algorithm, proposed by William-Hinton-Rummelhart, has had the merit of favouring the development of machine learning techniques, which has permeated every branch of research and technology since the mid-1980s. This extraordinary success is largely due to the black-box approach, but this same factor was also seen as a limitation, as soon more challenging problems were approached. One of the most critical aspects of the training algorithms was that of local minima of the loss function, typically the mean squared error of the output on the training set. In fact, as the most popular training algorithms are driven by the derivatives of the loss function, there is no possibility to evaluate if a reached minimum is local or global. The algorithm presented in this paper avoids the problem of local minima, as the training is based on the proper
    
[^10]: ReLiCADA -- 使用线性元胞自动机设计算法的储备计算

    ReLiCADA -- Reservoir Computing using Linear Cellular Automata Design Algorithm. (arXiv:2308.11522v1 [cs.LG])

    [http://arxiv.org/abs/2308.11522](http://arxiv.org/abs/2308.11522)

    本文介绍了一种使用线性元胞自动机设计算法的储备计算方法，解决了线性元胞自动机规则选择的开放问题，并通过数学分析和大量实验验证，该方法具有较低的计算复杂度和误差。

    

    本文介绍了一种新颖的算法，用于优化应用于时间序列应用的储备计算的元胞自动机模型的设计。除了选择模型的超参数外，提出的算法特别解决了线性元胞自动机规则选择的开放问题。选择方法从指数增长的规则空间中预先选择仅有几个有前景的候选规则。当应用于相关的基准数据集时，所选择的规则达到了较低的误差，其中最佳规则位于整体规则空间的前5%。该算法基于对线性元胞自动机属性的数学分析开发，并通过近一百万次试验支持，总计计算时间将近一年。与其他最先进的时间序列模型相比较，所提出的使用元胞自动机模型的储备计算具有更低的计算复杂度，同时实现了更低的误差。

    In this paper, we present a novel algorithm to optimize the design of Reservoir Computing using Cellular Automata models for time series applications. Besides selecting the models' hyperparameters, the proposed algorithm particularly solves the open problem of linear Cellular Automaton rule selection. The selection method pre-selects only a few promising candidate rules out of an exponentially growing rule space. When applied to relevant benchmark datasets, the selected rules achieve low errors, with the best rules being among the top 5% of the overall rule space. The algorithm was developed based on mathematical analysis of linear Cellular Automaton properties and is backed by almost one million experiments, adding up to a computational runtime of nearly one year. Comparisons to other state-of-the-art time series models show that the proposed Reservoir Computing using Cellular Automata models have lower computational complexity, at the same time, achieve lower errors. Hence, our appro
    
[^11]: 自我欺骗：逆向破解大型语言模型的语义防火墙

    Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])

    [http://arxiv.org/abs/2308.11521](http://arxiv.org/abs/2308.11521)

    这篇论文研究了大型语言模型的越狱问题，并提出了一种自动越狱方法，介绍了语义防火墙的概念和三种技术实现方法。

    

    大型语言模型（LLM），如ChatGPT，具有接近人工通用智能的惊人能力。虽然为各种社会需求提供了便利，但LLM也降低了生成有害内容的成本。因此，LLM开发人员已经部署了语义级的防御机制，用于识别和拒绝可能导致不适当内容的提示。不幸的是，这些防御机制并不完全可靠，一些攻击者已经设计出了“越狱”提示，临时使LLM忘记内容防御规则并回答任何不适当的问题。迄今为止，业界和学术界尚无关于这些语义级攻击和防御原则的明确解释。本文研究了LLM越狱问题，并首次提出了一种自动越狱方法。我们提出了语义防火墙的概念，并提供了三种技术实现方法。

    Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
    
[^12]: EM算法在带有集群数据的线性回归混合模型中的应用

    EM for Mixture of Linear Regression with Clustered Data. (arXiv:2308.11518v1 [cs.LG])

    [http://arxiv.org/abs/2308.11518](http://arxiv.org/abs/2308.11518)

    本文研究了在分布式数据中利用集群结构改进学习方案的问题，针对带有集群数据的线性回归混合模型，应用了期望最大化（EM）方法进行参数估计。

    

    现代数据驱动和分布式学习框架处理由分布在异质环境中的客户端生成的各种大规模数据。数据的异质性是扩大许多分布式学习范例的一个主要瓶颈。然而，在许多情况下，异构数据可能以具有共享结构的集群形式生成，例如在联邦学习中，一个共同的潜变量控制着客户端生成的所有样本的分布。因此，自然会问在分布式数据中如何利用潜在的集群结构来改进学习方案。在本文中，我们以估计一个具有两个分量的线性回归混合模型问题的d维参数的特例为例，其中每个节点生成具有共享潜变量的n个样本。我们使用众所周知的期望最大化（EM）方法来从m个节点中估计最大似然参数。

    Modern data-driven and distributed learning frameworks deal with diverse massive data generated by clients spread across heterogeneous environments. Indeed, data heterogeneity is a major bottleneck in scaling up many distributed learning paradigms. In many settings however, heterogeneous data may be generated in clusters with shared structures, as is the case in several applications such as federated learning where a common latent variable governs the distribution of all the samples generated by a client. It is therefore natural to ask how the underlying clustered structures in distributed data can be exploited to improve learning schemes. In this paper, we tackle this question in the special case of estimating $d$-dimensional parameters of a two-component mixture of linear regressions problem where each of $m$ nodes generates $n$ samples with a shared latent variable. We employ the well-known Expectation-Maximization (EM) method to estimate the maximum likelihood parameters from $m$ b
    
[^13]: TrackFlow: 带有标准化流的多目标跟踪

    TrackFlow: Multi-Object Tracking with Normalizing Flows. (arXiv:2308.11513v1 [cs.CV])

    [http://arxiv.org/abs/2308.11513](http://arxiv.org/abs/2308.11513)

    本论文提出了一种带有标准化流的多目标跟踪方法，通过融合异构信息，并解决了成本贡献、超参数调整和独立性等问题。

    

    近年来，随着跟踪-检测方法的简洁性和强大先验条件使其摆脱了跟踪-注意力方法的复杂设计和麻烦，多目标跟踪领域对跟踪-检测方法重新产生了兴趣。在这种背景下，我们旨在将跟踪-检测方法扩展到多模态设置，其中需要从异构信息（例如2D运动线索、视觉外观和姿态估计）计算综合成本。具体而言，我们通过一个案例研究来融合具有粗略估计的三维信息和其他传统度量（例如IoU）。为了实现这一目标，最近的方法采用简单规则或复杂启发式方法来平衡每个成本的贡献。然而，它们需要在一个保留集上对定制超参数进行仔细调整，并且暗示这些成本是相互独立的，而这在现实中并不成立。我们通过建立一个优雅的概率模型来解决这些问题。

    The field of multi-object tracking has recently seen a renewed interest in the good old schema of tracking-by-detection, as its simplicity and strong priors spare it from the complex design and painful babysitting of tracking-by-attention approaches. In view of this, we aim at extending tracking-by-detection to multi-modal settings, where a comprehensive cost has to be computed from heterogeneous information e.g., 2D motion cues, visual appearance, and pose estimates. More precisely, we follow a case study where a rough estimate of 3D information is also available and must be merged with other traditional metrics (e.g., the IoU). To achieve that, recent approaches resort to either simple rules or complex heuristics to balance the contribution of each cost. However, i) they require careful tuning of tailored hyperparameters on a hold-out set, and ii) they imply these costs to be independent, which does not hold in reality. We address these issues by building upon an elegant probabilisti
    
[^14]: 模式可组合性：探索排列对齐模型的凸组合

    Mode Combinability: Exploring Convex Combinations of Permutation Aligned Models. (arXiv:2308.11511v1 [cs.LG])

    [http://arxiv.org/abs/2308.11511](http://arxiv.org/abs/2308.11511)

    该论文探索了排列对齐模型的凸组合，并发现广泛的超立方体区域形成了低损失值的曲面，揭示了线性模式连通性的概念扩展到了更一般的模式可组合性现象。同时提出了一些关于线性模式连通性和模型重排基的新观察。研究还发现了模型组合具有传递性和鲁棒性质，并分析了功能和权重相似性的情况。

    

    我们探索了两个大小为d的排列对齐神经网络参数向量Θ_A和Θ_B的逐元素凸组合。我们通过检查由超立方体[0,1]^d及其邻域的元素参数化的各种模型组合的分布进行了广泛的实验。我们的研究结果揭示出，超立方体的广泛区域形成了低损失值的曲面，这表明线性模式连通性的概念扩展到了一个更一般的现象，我们将其称为模式可组合性。我们还对线性模式连通性和模型重排基进行了几项新颖的观察。我们展示了一个传递性质：基于一个共同的第三个模型进行重新基准的两个模型也是线性模式连通的，并且具有鲁棒性质：即使神经元匹配发生了相当大的扰动，所得到的组合仍然形成一个工作模型。此外，我们还分析了模型组合的功能和权重相似性，并表明此类组合是。。。

    We explore element-wise convex combinations of two permutation-aligned neural network parameter vectors $\Theta_A$ and $\Theta_B$ of size $d$. We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube $[0,1]^{d}$ and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are
    
[^15]: 作者身份表征学习能够捕捉文体特征吗？

    Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])

    [http://arxiv.org/abs/2308.11490](http://arxiv.org/abs/2308.11490)

    本论文研究了作者身份表征学习能否捕捉文体特征的问题，并通过实验验证了这些表征能够有效地捕捉写作风格的特征。

    

    在计算语言学中，自动将作者的风格从其写作内容中分离出来是一个长期存在且可能不可解决的问题。同时，最近有大量带有作者标签的文本语料库可用，使得以纯数据驱动的方式学习作者身份表征成为可能，用于作者归属这一任务，该任务显然更多地依赖于编码写作风格而不是编码内容。然而，对这一替代任务的成功并不能确保这些表征能够捕捉写作风格，因为作者身份也可能与其他潜在变量（如主题）相关。为了更好地理解这些表征所传递的信息的本质，特别是为了验证其主要编码的是写作风格的假设，我们通过一系列有针对性的实验系统地检查了这些表征。这些实验的结果表明，为作者表示学习的表征能够有效地捕捉写作风格的特征。

    Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the 
    
[^16]: 大型语言模型对多选题选项顺序的敏感性

    Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])

    [http://arxiv.org/abs/2308.11483](http://arxiv.org/abs/2308.11483)

    本文研究了大型语言模型对多选题选项顺序的敏感性。实验证明，当对回答选项进行重新排序时，大型语言模型的性能差距可以达到13%至75%。这种敏感性主要在大型语言模型对前两个/三个选项的预测不确定时出现。

    

    大型语言模型在各种自然语言处理任务中展现了出色的能力。然而，先前的研究表明，这些模型对提示文字的敏感性以及少样本展示的顺序敏感性，给对这些模型的公正评估带来了挑战。随着这些模型变得更加强大，了解和解决这些局限性变得迫切。本文关注在多选题任务中，对大型语言模型对选项顺序的鲁棒性进行研究，这是研究大型语言模型推理和事实检索能力常用的任务。通过对大型语言模型在不同基准测试中在重新排序回答选项时的表现差距的调查，我们证明了在少样本情况下，大型语言模型的性能相差约13%至75%。通过详细分析，我们推测这种敏感性是在大型语言模型在前两个/三个选项之间的预测不确定时产生的。

    Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specif
    
[^17]: 对广泛的分布外检测的期望：期望之外的未知数据

    Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])

    [http://arxiv.org/abs/2308.11480](http://arxiv.org/abs/2308.11480)

    这项研究对机器学习中分布外检测方法进行了评估，发现现有方法在检测未知类别方面表现出色，但在遇到其他类型的分布变化时性能不稳定。

    

    提高部署的机器学习系统的可靠性通常涉及开发方法来检测分布外（OOD）的输入。然而，现有研究常常狭窄地关注训练集中缺失的类别样本，忽略了其他类型的可能分布变化。这种限制降低了这些方法在现实场景中的适用性，因为系统会遇到各种各样的异常输入。在本研究中，我们将五种不同类型的分布变化进行分类，并对最近的OOD检测方法在每一种分布变化上进行了关键评估。我们以BROAD（Benchmarking Resilience Over Anomaly Diversity）的名义公开发布我们的基准。我们的研究发现这些方法在检测未知类别方面表现出色，但在遇到其他类型的分布变化时性能不一致。换句话说，它们只能可靠地检测到它们特别设计来预期的意外输入。

    Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expec
    
[^18]: 重新审视基于列生成的启发式方法用于学习分类树

    Revisiting column-generation-based matheuristic for learning classification trees. (arXiv:2308.11477v1 [cs.LG])

    [http://arxiv.org/abs/2308.11477](http://arxiv.org/abs/2308.11477)

    该论文改进了基于列生成的启发式方法，以提高学习分类树的效果。通过减少子问题数量、使用数据依赖约束作为割平面以及生成违反约束的数据点，该方法提高了可伸缩性并适用于大型数据集。

    

    决策树是机器学习中解决分类问题的高度可解释性模型。传统的机器学习算法训练决策树快速但生成的树在准确性上不够优化。文献中其他离散优化模型解决了最优性问题但只在较小的数据集上表现良好。firat2020column提出了一种基于列生成的启发式方法来学习决策树。该方法提高了可伸缩性，并可以处理大型数据集。在这篇论文中，我们描述了对该列生成方法的改进。首先，我们修改了子问题模型以显著减少多类分类实例中的子问题数量。接下来，我们证明了主问题中的数据依赖约束是蕴含的，并将其用作割平面。此外，我们描述了一个分离模型来生成线性规划松弛解违反其对应的数据点。

    Decision trees are highly interpretable models for solving classification problems in machine learning (ML). The standard ML algorithms for training decision trees are fast but generate suboptimal trees in terms of accuracy. Other discrete optimization models in the literature address the optimality problem but only work well on relatively small datasets. \cite{firat2020column} proposed a column-generation-based heuristic approach for learning decision trees. This approach improves scalability and can work with large datasets. In this paper, we describe improvements to this column generation approach. First, we modify the subproblem model to significantly reduce the number of subproblems in multiclass classification instances. Next, we show that the data-dependent constraints in the master problem are implied, and use them as cutting planes. Furthermore, we describe a separation model to generate data points for which the linear programming relaxation solution violates their correspond
    
[^19]: 基于内部跨层梯度的联邦学习中的同质性到异质性的扩展

    Internal Cross-layer Gradients for Extending Homogeneity to Heterogeneity in Federated Learning. (arXiv:2308.11464v1 [cs.LG])

    [http://arxiv.org/abs/2308.11464](http://arxiv.org/abs/2308.11464)

    提出了一种基于内部跨层梯度的联邦学习方法，通过混合浅层和深层的梯度，增强了深层的相似性，从而扩展了在处理系统异质性方面的能力。

    

    联邦学习（FL）在实际场景中不可避免地面临系统异质性的挑战。为了增强大多数模型同质性FL方法处理系统异质性的能力，我们提出了一种训练方案，可以扩展它们应对这一挑战的能力。我们在本文中从详细探索同质性和异质性FL设置开始，发现了三个关键观察结果：（1）客户端性能与层之间的相似性呈正相关，（2）浅层比深层具有更高的相似性，（3）较为平滑的梯度分布指示了更高的层相似性。基于这些观察结果，我们提出了InCo Aggregation方法，利用内部跨层梯度，即服务器模型中来自浅层和深层的梯度混合，以增强深层的相似性，而无需额外的客户端通信。

    Federated learning (FL) inevitably confronts the challenge of system heterogeneity in practical scenarios. To enhance the capabilities of most model-homogeneous FL methods in handling system heterogeneity, we propose a training scheme that can extend their capabilities to cope with this challenge. In this paper, we commence our study with a detailed exploration of homogeneous and heterogeneous FL settings and discover three key observations: (1) a positive correlation between client performance and layer similarities, (2) higher similarities in the shallow layers in contrast to the deep layers, and (3) the smoother gradients distributions indicate the higher layer similarities. Building upon these observations, we propose InCo Aggregation that leverags internal cross-layer gradients, a mixture of gradients from shallow and deep layers within a server model, to augment the similarity in the deep layers without requiring additional communication between clients. Furthermore, our methods 
    
[^20]: 《自监督表示学习综述》

    A Survey on Self-Supervised Representation Learning. (arXiv:2308.11455v1 [cs.LG])

    [http://arxiv.org/abs/2308.11455](http://arxiv.org/abs/2308.11455)

    本综述论文全面回顾了无监督学习图像表示的方法，提出了一种分类法，并总结了最新的实验结果，为深入研究表示学习领域的人员提供了一个起点。

    

    在现代机器学习领域中，学习有意义的表示是许多任务的核心。最近引入了许多允许无监督学习图像表示的方法。这些表示可以应用于分类或物体检测等下游任务中。这些表示的质量接近于有监督学习，而不需要标记的图像。本综述论文以统一的符号表示对这些方法进行了全面的回顾，指出了这些方法的相似性和差异，并提出了一种分类法，将这些方法联系起来。此外，我们的综述通过元分析总结了文献中最新的实验结果。我们的综述旨在为希望深入研究表示学习领域的研究人员和实践者提供一个起点。

    Learning meaningful representations is at the heart of many tasks in the field of modern machine learning. Recently, a lot of methods were introduced that allow learning of image representations without supervision. These representations can then be used in downstream tasks like classification or object detection. The quality of these representations is close to supervised learning, while no labeled images are needed. This survey paper provides a comprehensive review of these methods in a unified notation, points out similarities and differences of these methods, and proposes a taxonomy which sets these methods in relation to each other. Furthermore, our survey summarizes the most-recent experimental results reported in the literature in form of a meta-study. Our survey is intended as a starting point for researchers and practitioners who want to dive into the field of representation learning.
    
[^21]: 基于掩码动量对比学习的零样本语义理解

    Masked Momentum Contrastive Learning for Zero-shot Semantic Understanding. (arXiv:2308.11448v1 [cs.CV])

    [http://arxiv.org/abs/2308.11448](http://arxiv.org/abs/2308.11448)

    本研究基于自监督学习技术，在计算机视觉任务中评估了零样本分割的有效性，提出了一种基于提示补丁的评估协议。这项研究旨在实现在一般化和识别未见对象方面模拟人类的能力。

    

    自监督预训练（SSP）已经成为机器学习中一种流行的技术，能够在没有标注数据的情况下提取有意义的特征表示。在计算机视觉领域中，预训练的视觉变换器（ViTs）在推动迁移学习方面发挥了关键作用。然而，由于模型规模的爆炸性增长，微调这些大模型的成本不断增加，这给人工智能带来了挑战。本研究旨在评估纯自监督学习（SSL）技术在计算机视觉任务中的有效性，消除了微调的需要，以实现在一般化和识别未见对象方面模拟人类的能力。为此，我们提出了一种基于提示补丁的零样本分割评估协议。给定目标对象上的一个点作为提示，算法计算选定补丁与其他补丁之间的相似度图，然后应用简单的阈值分割算法。

    Self-supervised pretraining (SSP) has emerged as a popular technique in machine learning, enabling the extraction of meaningful feature representations without labelled data. In the realm of computer vision, pretrained vision transformers (ViTs) have played a pivotal role in advancing transfer learning. Nonetheless, the escalating cost of finetuning these large models has posed a challenge due to the explosion of model size. This study endeavours to evaluate the effectiveness of pure self-supervised learning (SSL) techniques in computer vision tasks, obviating the need for finetuning, with the intention of emulating human-like capabilities in generalisation and recognition of unseen objects. To this end, we propose an evaluation protocol for zero-shot segmentation based on a prompting patch. Given a point on the target object as a prompt, the algorithm calculates the similarity map between the selected patch and other patches, upon that, a simple thresholding is applied to segment the 
    
[^22]: 探索拉舒蒙集合有助于医疗数据的解释

    Exploration of Rashomon Set Assists Explanations for Medical Data. (arXiv:2308.11446v1 [cs.LG])

    [http://arxiv.org/abs/2308.11446](http://arxiv.org/abs/2308.11446)

    本文提出了一种新的过程来探索和分析医疗数据中的拉舒蒙集合模型，从而超越传统单一模型选择的方法，并通过引入"拉舒蒙检测"算法识别出集合中最不同的模型。

    

    机器学习建模过程通常以选择最大化某个性能指标的单一模型作为最终结果。然而，这种方法会导致对稍微差一些的模型进行更深入的分析被忽视。尤其在医疗和健康研究中，目标不仅仅是预测，还包括产生有价值的洞察，仅仅依赖性能指标可能会导致误导或不完整的结论。当处理一组性能接近最优的模型集合时，即所谓的"拉舒蒙集合"，这个问题尤为突出。这样的集合可能包含描述数据的不同方式的模型，需要进行全面的分析。本文引入了一种新的过程来探索拉舒蒙集合模型，扩展了传统建模方法。核心是通过引入的"拉舒蒙检测"算法来识别拉舒蒙集合中最不同的模型。

    The machine learning modeling process conventionally culminates in selecting a single model that maximizes a selected performance metric. However, this approach leads to abandoning a more profound analysis of slightly inferior models. Particularly in medical and healthcare studies, where the objective extends beyond predictions to valuable insight generation, relying solely on performance metrics can result in misleading or incomplete conclusions. This problem is particularly pertinent when dealing with a set of models with performance close to maximum one, known as $\textit{Rashomon set}$. Such a set can be numerous and may contain models describing the data in a different way, which calls for comprehensive analysis. This paper introduces a novel process to explore Rashomon set models, extending the conventional modeling approach. The cornerstone is the identification of the most different models within the Rashomon set, facilitated by the introduced $\texttt{Rashomon_DETECT}$ algorit
    
[^23]: TurboViT: 通过生成式架构搜索生成快速视觉变压器

    TurboViT: Generating Fast Vision Transformers via Generative Architecture Search. (arXiv:2308.11421v1 [cs.CV])

    [http://arxiv.org/abs/2308.11421](http://arxiv.org/abs/2308.11421)

    TurboViT是通过生成式架构搜索生成的快速视觉变压器架构设计，它在准确性和计算效率之间取得了良好的平衡。

    

    近年来，视觉变压器在处理各种视觉感知任务方面展示出了前所未有的性能水平。然而，这种网络架构的结构和计算复杂性使得它们在具有高吞吐量和低内存要求的实际应用中难以部署。因此，最近在有效视觉变压器架构设计方面进行了重要的研究。在本研究中，我们通过生成式架构搜索（GAS）探索了快速视觉变压器架构设计的生成，以实现在准确性和架构计算效率之间的良好平衡。通过这个生成式架构搜索过程，我们创建了 TurboViT，这是一种基于掩码单元注意力和 Q-pooling 设计模式的高效分层视觉变压器架构设计。该结果表明，TurboViT 架构设计的架构计算复杂性显著降低（>2.47）

    Vision transformers have shown unprecedented levels of performance in tackling various visual perception tasks in recent years. However, the architectural and computational complexity of such network architectures have made them challenging to deploy in real-world applications with high-throughput, low-memory requirements. As such, there has been significant research recently on the design of efficient vision transformer architectures. In this study, we explore the generation of fast vision transformer architecture designs via generative architecture search (GAS) to achieve a strong balance between accuracy and architectural and computational efficiency. Through this generative architecture search process, we create TurboViT, a highly efficient hierarchical vision transformer architecture design that is generated around mask unit attention and Q-pooling design patterns. The resulting TurboViT architecture design achieves significantly lower architectural computational complexity (>2.47
    
[^24]: 设计一款攻防游戏：通过竞争来增加金融交易模型的鲁棒性

    Designing an attack-defense game: how to increase robustness of financial transaction models via a competition. (arXiv:2308.11406v1 [cs.LG])

    [http://arxiv.org/abs/2308.11406](http://arxiv.org/abs/2308.11406)

    通过设计一款攻防游戏，我们研究了使用序列金融数据的神经网络模型的对抗攻击和防御的现状和动态，并且通过分析比赛动态，回答了隐藏模型免受恶意用户攻击的重要性以及需要多长时间才能破解模型的问题。

    

    鉴于金融领域恶意攻击风险不断升级和由此引发的严重损害，对机器学习模型的对抗策略和鲁棒的防御机制有深入的理解至关重要。随着银行日益广泛采用更精确但潜在脆弱的神经网络，这一威胁变得更加严重。我们旨在调查使用序列金融数据作为输入的神经网络模型的对抗攻击和防御的当前状态和动态。为了实现这一目标，我们设计了一个比赛，允许对现代金融交易数据中的问题进行逼真而详细的研究。参与者直接竞争，因此可能的攻击和防御在接近真实条件下进行了检验。我们的主要贡献是分析比赛动态，回答了隐藏模型免受恶意用户攻击的重要性以及需要多长时间才能破解模型的问题。

    Given the escalating risks of malicious attacks in the finance sector and the consequential severe damage, a thorough understanding of adversarial strategies and robust defense mechanisms for machine learning models is critical. The threat becomes even more severe with the increased adoption in banks more accurate, but potentially fragile neural networks. We aim to investigate the current state and dynamics of adversarial attacks and defenses for neural network models that use sequential financial data as the input.  To achieve this goal, we have designed a competition that allows realistic and detailed investigation of problems in modern financial transaction data. The participants compete directly against each other, so possible attacks and defenses are examined in close-to-real-life conditions. Our main contributions are the analysis of the competition dynamics that answers the questions on how important it is to conceal a model from malicious users, how long does it take to break i
    
[^25]: 非冗余的手工制作和深度学习放射学的组合：应用于早期胰腺癌的检测

    Non-Redundant Combination of Hand-Crafted and Deep Learning Radiomics: Application to the Early Detection of Pancreatic Cancer. (arXiv:2308.11389v1 [eess.IV])

    [http://arxiv.org/abs/2308.11389](http://arxiv.org/abs/2308.11389)

    本文解决了学习不与手工制作放射学冗余的深度学习放射学的问题，并通过结合两者的特征来预测早期胰腺癌标志物，取得了比基线方法更好的结果。

    

    我们解决了学习不与手工制作放射学(HCR)冗余的深度学习放射学(DLR)的问题。我们使用VAE提取DLR特征，同时通过最小化它们的互信息来确保与HCR特征的独立性。得到的DLR特征可以与手工制作的特征相结合，并通过分类器预测癌症的早期标志物。我们在四种胰腺癌早期标志物上演示了我们的方法，并在一个大型独立测试集上进行了验证。我们的结果突出了非冗余的DLR和HCR特征的价值，与不解决冗余问题或仅依赖于HCR特征的基线方法相比，曲线下面积有所改善。

    We address the problem of learning Deep Learning Radiomics (DLR) that are not redundant with Hand-Crafted Radiomics (HCR). To do so, we extract DLR features using a VAE while enforcing their independence with HCR features by minimizing their mutual information. The resulting DLR features can be combined with hand-crafted ones and leveraged by a classifier to predict early markers of cancer. We illustrate our method on four early markers of pancreatic cancer and validate it on a large independent test set. Our results highlight the value of combining non-redundant DLR and HCR features, as evidenced by an improvement in the Area Under the Curve compared to baseline methods that do not address redundancy or solely rely on HCR features.
    
[^26]: 解决偏见的目标数据增强

    Targeted Data Augmentation for bias mitigation. (arXiv:2308.11386v1 [cs.LG])

    [http://arxiv.org/abs/2308.11386](http://arxiv.org/abs/2308.11386)

    本研究提出了一种新颖而高效的方法，通过利用数据增强技术来解决数据和模型中的偏见问题。与去除偏见不同，我们的方法建议在训练过程中插入偏见，从而提高了性能。我们还通过对两个不同的数据集进行标注来识别偏见，并发布了这些偏见标注，为未来的研究提供了有价值的资源。

    

    公正和道德的人工智能系统的发展需要对偏见的缓解进行谨慎考虑，这个领域经常被忽视或忽略。在本研究中，我们引入了一种被称为目标数据增强 (TDA) 的新颖而高效的方法来解决偏见问题，该方法利用经典的数据增强技术来解决数据和模型中的偏见问题。与去除偏见的繁琐任务不同，我们的方法提出在训练过程中插入偏见，从而提高了性能。我们通过对两个不同的数据集进行标注来识别偏见：一个是临床皮肤病变数据集，一个是男性和女性面孔数据集。这些偏见标注首次在本研究中发布，为未来的研究提供了有价值的资源。通过对事后偏见插入进行反事实分析，我们发现与镜框、尺子和眼镜相关的偏见对模型产生了重要影响。通过在训练中随机引入偏见，我们减轻了这些偏见并实现了显著的降低。

    The development of fair and ethical AI systems requires careful consideration of bias mitigation, an area often overlooked or ignored. In this study, we introduce a novel and efficient approach for addressing biases called Targeted Data Augmentation (TDA), which leverages classical data augmentation techniques to tackle the pressing issue of bias in data and models. Unlike the laborious task of removing biases, our method proposes to insert biases instead, resulting in improved performance. To identify biases, we annotated two diverse datasets: a dataset of clinical skin lesions and a dataset of male and female faces. These bias annotations are published for the first time in this study, providing a valuable resource for future research. Through Counterfactual Bias Insertion, we discovered that biases associated with the frame, ruler, and glasses had a significant impact on models. By randomly introducing biases during training, we mitigated these biases and achieved a substantial decr
    
[^27]: 可解释的分布不变公平性度量方法对于连续评分

    Interpretable Distribution-Invariant Fairness Measures for Continuous Scores. (arXiv:2308.11375v1 [stat.ML])

    [http://arxiv.org/abs/2308.11375](http://arxiv.org/abs/2308.11375)

    对于连续评分，我们提出了一种基于Wasserstein距离的分布不变公平性度量方法，能够解释度量结果并适用于比较不同模型、数据集或时间点之间的偏差。

    

    算法公平性度量通常在二元决策的背景下进行讨论。我们将这种方法扩展到连续评分。到目前为止，基于ROC的度量方法主要用于此目的。其他现有方法主要依赖于评分的分布，不适用于排名任务，或者它们的效果大小不可解释。在这里，我们提出了一种基于Wasserstein距离的连续评分的分布不变公平性度量方法，具有合理的解释。我们的度量方法易于计算，并适用于量化和解释群体差异的强度，以及比较不同模型、数据集或时间点之间的偏差。我们建立了现有评分公平性度量方法的不同族之间的联系，并表明所提出的分布不变公平性度量方法表现更好，因为它们更明确，并且可以量化显著的偏差，而ROC-based不能。

    Measures of algorithmic fairness are usually discussed in the context of binary decisions. We extend the approach to continuous scores. So far, ROC-based measures have mainly been suggested for this purpose. Other existing methods depend heavily on the distribution of scores, are unsuitable for ranking tasks, or their effect sizes are not interpretable. Here, we propose a distributionally invariant version of fairness measures for continuous scores with a reasonable interpretation based on the Wasserstein distance. Our measures are easily computable and well suited for quantifying and interpreting the strength of group disparities as well as for comparing biases across different models, datasets, or time points. We derive a link between the different families of existing fairness measures for scores and show that the proposed distributionally invariant fairness measures outperform ROC-based fairness measures because they are more explicit and can quantify significant biases that ROC-ba
    
[^28]: 行动分割需要多少长期时间上下文？

    How Much Temporal Long-Term Context is Needed for Action Segmentation?. (arXiv:2308.11358v1 [cs.CV])

    [http://arxiv.org/abs/2308.11358](http://arxiv.org/abs/2308.11358)

    本文提出了一种基于transformer的模型，利用稀疏注意力捕捉视频的完整上下文，以回答时间行动分割需要多少长期时间上下文。通过与当前最先进的方法进行比较，在三个时间行动分割数据集上取得了良好的性能。

    

    在视频中建模长期上下文对于许多细粒度任务包括时间行动分割至关重要。一个有趣的问题是，为了达到最佳性能，需要多少长期时间上下文仍然是一个未解之谜。虽然transformers可以对视频的长期上下文进行建模，但对于长视频，这在计算上是不可行的。因此，最近关于时间行动分割的研究结合了使用局部时间窗口计算出的自注意力的时间卷积网络。虽然这些方法显示出良好的结果，但它们的性能受到无法捕捉视频的完整上下文的限制。在这项工作中，我们通过引入基于transformer的模型并利用稀疏注意力来捕捉视频的完整上下文，试图回答需要多少长期时间上下文才能进行时间行动分割。我们将我们的模型与目前的三个数据集上的时间行动分割的最新技术水平进行比较，这三个数据集包括50Salads，Brea...

    Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Brea
    
[^29]: 机器学习辅助下的仿射Deligne-Lusztig变量探索

    Machine learning assisted exploration for affine Deligne-Lusztig varieties. (arXiv:2308.11355v1 [math.AG])

    [http://arxiv.org/abs/2308.11355](http://arxiv.org/abs/2308.11355)

    本研究利用机器学习辅助框架探索仿射Deligne-Lusztig变量的几何性质，加速纯数学研究，发现新猜想和有前景的研究方向。

    

    本文提出了一种新颖的交叉学科研究，利用机器学习辅助框架来探索仿射Deligne-Lusztig变量（ADLV）的几何性质。主要目标是研究ADLV的不空集模式、维度和不可约分量的枚举。我们提出的框架展示了数据生成、模型训练、模式分析和人工验证的递归流程，展示了机器学习与纯数学研究之间的复杂相互作用。值得注意的是，我们的数据生成过程是细致入微的，强调选择有意义的子集和适当的特征集。我们证明了这个框架有潜力加速纯数学研究，带来新猜想和有前景的研究方向的发现，否则可能需花费相当长时间才能揭示。我们重新发现了虚拟维数公式，并提供了关于某个新识别问题的完整数学证明。

    This paper presents a novel, interdisciplinary study that leverages a Machine Learning (ML) assisted framework to explore the geometry of affine Deligne-Lusztig varieties (ADLV). The primary objective is to investigate the nonemptiness pattern, dimension and enumeration of irreducible components of ADLV. Our proposed framework demonstrates a recursive pipeline of data generation, model training, pattern analysis, and human examination, presenting an intricate interplay between ML and pure mathematical research. Notably, our data-generation process is nuanced, emphasizing the selection of meaningful subsets and appropriate feature sets. We demonstrate that this framework has a potential to accelerate pure mathematical research, leading to the discovery of new conjectures and promising research directions that could otherwise take significant time to uncover. We rediscover the virtual dimension formula and provide a full mathematical proof of a newly identified problem concerning a certa
    
[^30]: 谨慎估计，大胆探索

    Careful at Estimation and Bold at Exploration. (arXiv:2308.11348v1 [cs.LG])

    [http://arxiv.org/abs/2308.11348](http://arxiv.org/abs/2308.11348)

    本文中提出了一个基于双Q函数框架的新颖探索策略，以解决连续动作空间中基于策略的探索中的问题，并提出了贪婪的Q softmax更新方案来更新Q值。

    

    由于无限的动作空间，连续动作空间中的探索策略往往是启发式的，这些方法无法得出一般性结论。在先前的研究中，已经展示了对确定性策略强化学习中的连续动作空间进行基于策略的探索的好处。然而，在确定性策略强化学习中，基于策略的探索存在两个主要问题：无目标的探索和策略发散，并且由于估计不准确，策略梯度对探索的帮助仅在某些情况下有效。基于双Q函数框架，我们引入了一种新颖的探索策略来缓解这些问题，与策略梯度分离。我们首先提出了贪婪的Q softmax更新方案来更新Q值。期望Q值通过在动作上加权求和保守Q值得到，权重为相应的贪婪Q值。贪婪Q取两个Q函数的最大值，保守Q取两个Q函数的最小值。

    Exploration strategies in continuous action space are often heuristic due to the infinite actions, and these kinds of methods cannot derive a general conclusion. In prior work, it has been shown that policy-based exploration is beneficial for continuous action space in deterministic policy reinforcement learning(DPRL). However, policy-based exploration in DPRL has two prominent issues: aimless exploration and policy divergence, and the policy gradient for exploration is only sometimes helpful due to inaccurate estimation. Based on the double-Q function framework, we introduce a novel exploration strategy to mitigate these issues, separate from the policy gradient. We first propose the greedy Q softmax update schema for Q value update. The expected Q value is derived by weighted summing the conservative Q value over actions, and the weight is the corresponding greedy Q value. Greedy Q takes the maximum value of the two Q functions, and conservative Q takes the minimum value of the two d
    
[^31]: ProAgent：利用大型语言模型构建主动合作的人工智能

    ProAgent: Building Proactive Cooperative AI with Large Language Models. (arXiv:2308.11339v1 [cs.AI])

    [http://arxiv.org/abs/2308.11339](http://arxiv.org/abs/2308.11339)

    ProAgent是一个利用大型语言模型构建的主动合作的AI框架，能够预测队友的决策并为自己制定增强计划，具有高度的模块化和可解释性。

    

    在AGI研究中，构建具有自适应行为的人工智能以进行人工智能和人类的合作成为一个关键关注点。目前，开发合作代理人的方法主要依赖于基于学习的方法，其中政策泛化严重依赖于与特定队友的过去互动。这些方法限制了代理人在面对新的队友时重新校准策略的能力。我们提出了ProAgent，这是一个新颖的框架，利用大型语言模型（LLMs）来创建一个具有预测队友未来决策能力和为自身制定增强计划能力的主动代理。ProAgent在合作推理方面表现出色，能够动态调整行为以增强与队友的协作努力。此外，ProAgent框架具有高度的模块化和可解释性，便于无缝集成，以应对各种协调场景。

    Building AIs with adaptive behaviors in human-AI cooperation stands as a pivotal focus in AGI research. Current methods for developing cooperative agents predominantly rely on learning-based methods, where policy generalization heavily hinges on past interactions with specific teammates. These approaches constrain the agent's capacity to recalibrate its strategy when confronted with novel teammates. We propose \textbf{ProAgent}, a novel framework that harnesses large language models (LLMs) to fashion a \textit{pro}active \textit{agent} empowered with the ability to anticipate teammates' forthcoming decisions and formulate enhanced plans for itself. ProAgent excels at cooperative reasoning with the capacity to dynamically adapt its behavior to enhance collaborative efforts with teammates. Moreover, the ProAgent framework exhibits a high degree of modularity and interpretability, facilitating seamless integration to address a wide array of coordination scenarios. Experimental evaluations
    
[^32]: 无数据生成触发器保护联邦学习免受后门攻击

    Protect Federated Learning Against Backdoor Attacks via Data-Free Trigger Generation. (arXiv:2308.11333v1 [cs.LG])

    [http://arxiv.org/abs/2308.11333](http://arxiv.org/abs/2308.11333)

    通过数据审计和触发器图像过滤等机制，我们提出了一种无数据生成触发器的防御方法来保护联邦学习免受后门攻击。该方法利用后门攻击特征来学习触发器，并生成具有新学习知识的图像。

    

    作为分布式机器学习范 paradigm，联邦学习 (FL) 可以使大规模客户端在不共享原始数据的情况下协同训练模型。然而，由于对不可信客户端的数据审计缺失，FL 易受污染攻击，特别是后门攻击。攻击者可以通过使用污染数据进行本地训练或直接更改模型参数，轻而易举地将后门注入模型，从而触发模型对图像中的目标模式进行错误分类。为解决这些问题，我们提出了一种基于两个后门攻击特征的新型无数据生成触发器防御方法：i) 触发器学习速度比普通知识更快，ii) 触发器模式对图像分类的影响大于普通类别模式。我们的方法通过识别旧和新全局模型之间的差异，生成具有新学习知识的图像，并通过评估方法过滤触发器图像。

    As a distributed machine learning paradigm, Federated Learning (FL) enables large-scale clients to collaboratively train a model without sharing their raw data. However, due to the lack of data auditing for untrusted clients, FL is vulnerable to poisoning attacks, especially backdoor attacks. By using poisoned data for local training or directly changing the model parameters, attackers can easily inject backdoors into the model, which can trigger the model to make misclassification of targeted patterns in images. To address these issues, we propose a novel data-free trigger-generation-based defense approach based on the two characteristics of backdoor attacks: i) triggers are learned faster than normal knowledge, and ii) trigger patterns have a greater effect on image classification than normal class patterns. Our approach generates the images with newly learned knowledge by identifying the differences between the old and new global models, and filters trigger images by evaluating the 
    
[^33]: 通过注意力矩阵的拓扑分析来估算Transformer模型预测的不确定性

    Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices. (arXiv:2308.11295v1 [cs.LG])

    [http://arxiv.org/abs/2308.11295](http://arxiv.org/abs/2308.11295)

    本论文通过拓扑数据分析方法，提出一种基于注意力机制的拓扑性质的不确定性估计方法，用于Transformer模型的预测，超越传统方法，开辟了注意力机制的新应用领域。

    

    在自然语言处理领域，确定深度学习模型预测的置信度是一个开放的问题。传统的不确定性估计方法对于文本分类模型并不有效。我们提出了一种基于Transformer架构的神经网络的不确定性估计任务。这种模型的一个关键特点是注意力机制，它支持神经网络中的令牌之间的信息流。我们利用拓扑数据分析方法探索内部表示之间的关系，并利用它们来预测模型的置信度。本文提出了一种基于注意力机制的拓扑性质的不确定性估计方法，并与传统方法进行了比较。结果表明，该算法在质量上超过了现有的方法，并开辟了注意力机制的新应用领域，但需要...

    Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires t
    
[^34]: 跨资产类别的网络动量

    Network Momentum across Asset Classes. (arXiv:2308.11294v1 [q-fin.PM])

    [http://arxiv.org/abs/2308.11294](http://arxiv.org/abs/2308.11294)

    本文研究了跨资产类别的网络动量，通过观察资产间动量传递，提出了一种新的交易信号，并探索了不同类别的连续期货合约之间的动量特征的相互关系。

    

    我们研究了网络动量概念，这是一种从资产间动量传递中得出的新型交易信号。起初，我们只在经济和基本联系中观察到这种现象，例如同一公司的股票-债券关系和通过供求链相连的股票，动量传递意味着动量风险溢价从一个资产传递到另一个资产。动量风险溢价的相似性，如共动性模式所示，已经在多个资产类别中被发现，包括商品、股票、债券和货币。然而，由于缺乏公司层面之外的共同特征或经济联系，研究动量传递的网络效应在这些类别之间一直面临挑战。在本文中，我们探索了跨越这四个类别的64个连续期货合约的动量特征之间的相互关系。我们利用了一个线性且可解释的图学习模型，通过最小化损失函数来学习网络中的动量特征。

    We investigate the concept of network momentum, a novel trading signal derived from momentum spillover across assets. Initially observed within the confines of pairwise economic and fundamental ties, such as the stock-bond connection of the same company and stocks linked through supply-demand chains, momentum spillover implies a propagation of momentum risk premium from one asset to another. The similarity of momentum risk premium, exemplified by co-movement patterns, has been spotted across multiple asset classes including commodities, equities, bonds and currencies. However, studying the network effect of momentum spillover across these classes has been challenging due to a lack of readily available common characteristics or economic ties beyond the company level. In this paper, we explore the interconnections of momentum features across a diverse range of 64 continuous future contracts spanning these four classes. We utilise a linear and interpretable graph learning model with minim
    
[^35]: 提高长径向特征传播中的木材原结预测

    Improving Knot Prediction in Wood Logs with Longitudinal Feature Propagation. (arXiv:2308.11291v1 [cs.CV])

    [http://arxiv.org/abs/2308.11291](http://arxiv.org/abs/2308.11291)

    本文提出了一种通过木材外形预测内部缺陷位置的方法，利用卷积循环神经网络解决二分类分割任务，实现在廉价设备上进行推理，并在冷杉和云杉树种上验证了该方法的有效性。

    

    木材行业中木材原结的质量主要取决于内外缺陷的存在，其中内部节疤是树枝生长的结果。目前，定位内部节疤需要使用昂贵的设备，如X射线扫描仪。本文解决了通过木材外形预测内部缺陷位置的任务。数据集通过利用X射线测量提取轮廓和节疤构建。我们提出了使用卷积循环神经网络解决这个二分类分割任务的方法。一旦神经网络训练完毕，可以使用廉价设备（如激光剖面仪）测量的外形进行推理。我们在冷杉和云杉树种上验证了我们方法的有效性，并对循环的重要性进行了消融实验证明。

    The quality of a wood log in the wood industry depends heavily on the presence of both outer and inner defects, including inner knots that are a result of the growth of tree branches. Today, locating the inner knots require the use of expensive equipment such as X-ray scanners. In this paper, we address the task of predicting the location of inner defects from the outer shape of the logs. The dataset is built by extracting both the contours and the knots with X-ray measurements. We propose to solve this binary segmentation task by leveraging convolutional recurrent neural networks. Once the neural network is trained, inference can be performed from the outer shape measured with cheap devices such as laser profilers. We demonstrate the effectiveness of our approach on fir and spruce tree species and perform ablation on the recurrence to demonstrate its importance.
    
[^36]: 数据为中心的量子系统学习的影子网络

    ShadowNet for Data-Centric Quantum System Learning. (arXiv:2308.11290v1 [quant-ph])

    [http://arxiv.org/abs/2308.11290](http://arxiv.org/abs/2308.11290)

    本研究提出了一个数据为中心的量子系统学习范式，将神经网络和经典阴影相结合，以解决大型量子系统动力学的预测和泛化问题。

    

    由于维度诅咒，理解大型量子系统的动力学变得困难。统计学习通过神经网络协议和经典阴影在这个领域提供了新的可能性，然而这两种方法都存在局限性：前者受到预测不确定性的困扰，后者缺乏泛化能力。在这里，我们提出了一个数据为中心的学习范式，结合了这两种方法的优势，以促进多样化的量子系统学习任务。特别地，我们的范式利用了经典阴影和其他易于获取的量子系统信息来创建训练数据集，然后通过神经网络来学习探索的量子系统学习问题的潜在映射规律。利用神经网络的泛化能力，这个范式可以在离线训练，并且在推理阶段能够优秀地预测之前未见过的系统，即使只有很少的状态副本。此外，它还继承了

    Understanding the dynamics of large quantum systems is hindered by the curse of dimensionality. Statistical learning offers new possibilities in this regime by neural-network protocols and classical shadows, while both methods have limitations: the former is plagued by the predictive uncertainty and the latter lacks the generalization ability. Here we propose a data-centric learning paradigm combining the strength of these two approaches to facilitate diverse quantum system learning (QSL) tasks. Particularly, our paradigm utilizes classical shadows along with other easily obtainable information of quantum systems to create the training dataset, which is then learnt by neural networks to unveil the underlying mapping rule of the explored QSL problem. Capitalizing on the generalization power of neural networks, this paradigm can be trained offline and excel at predicting previously unseen systems at the inference stage, even with few state copies. Besides, it inherits the characteristic 
    
[^37]: 测试时间嵌入归一化对热门偏见的缓解

    Test Time Embedding Normalization for Popularity Bias Mitigation. (arXiv:2308.11288v1 [cs.IR])

    [http://arxiv.org/abs/2308.11288](http://arxiv.org/abs/2308.11288)

    本文提出了一种名为“测试时间嵌入归一化”的策略来解决推荐系统中的热门偏见问题。该方法利用归一化的物品嵌入来控制嵌入大小，并通过与用户和物品嵌入的角度相似度区分受欢迎和不受欢迎的物品，从而有效减少了热门偏见的影响。

    

    热门偏见是推荐系统领域普遍存在的问题，其中热门物品倾向于主导推荐结果。在这项工作中，我们提出了“测试时间嵌入归一化”作为一种简单而有效的策略来缓解热门偏见，其性能超过了以往的缓解方法。我们的方法在推理阶段利用归一化的物品嵌入来控制嵌入的大小，而嵌入的大小与物品的流行度高度相关。通过大量实验证明，我们的方法结合采样softmax损失相比以前的方法更有效地减少了热门偏见的影响。我们进一步研究了用户和物品嵌入之间的关系，并发现嵌入之间的角度相似度可以区分受欢迎和不受欢迎的物品，而不考虑它们的流行程度。这一分析解释了我们方法成功的机制。

    Popularity bias is a widespread problem in the field of recommender systems, where popular items tend to dominate recommendation results. In this work, we propose 'Test Time Embedding Normalization' as a simple yet effective strategy for mitigating popularity bias, which surpasses the performance of the previous mitigation approaches by a significant margin. Our approach utilizes the normalized item embedding during the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity. Through extensive experiments, we show that our method combined with the sampled softmax loss effectively reduces popularity bias compare to previous approaches for bias mitigation. We further investigate the relationship between user and item embeddings and find that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity. The analysis explains the mechanism behind the success of our approac
    
[^38]: 基于CNN的基于注释的三维渲染和映射照片的楔形符号检测

    CNN based Cuneiform Sign Detection Learned from Annotated 3D Renderings and Mapped Photographs with Illumination Augmentation. (arXiv:2308.11277v1 [cs.CV])

    [http://arxiv.org/abs/2308.11277](http://arxiv.org/abs/2308.11277)

    该论文描述了一个基于CNN的楔形符号检测方法，通过学习注释的三维渲染和映射照片，结合光照增强。研究团队创建了HeiCuBeDa和MaiCuBeDa数据集，并提供了映射工具以传递注释。符号定位方法使用RepPoints检测器来预测字符的位置。该方法可以应用于处理楔形文字的数字工具开发和研究。

    

    在Digital Ancient Near Eastern Studies (DANES)社区面临的挑战的推动下，我们开发了用于处理楔形文字的数字工具，这是一种印在粘土板上的三维脚本，已有三千多年历史和至少八种主要语言。它由数千个随时间和空间变化的字符组成。照片是最常用的用于机器学习的表示方式，而墨水绘画则容易被解释。我们创建并使用了HeiCuBeDa和MaiCuBeDa数据集，这些数据集包含约500个带有注释的平板。对于我们的新型类似OCR的混合图像数据方法，我们提供了一种额外的映射工具，用于在3D渲染和照片之间传递注释。我们使用RepPoints检测器来预测字符的位置，以边界框的形式进行符号定位。我们使用来自GigaMesh的MSII（曲率）基于渲染的图像数据，以及Phong着色的3D数据。

    Motivated by the challenges of the Digital Ancient Near Eastern Studies (DANES) community, we develop digital tools for processing cuneiform script being a 3D script imprinted into clay tablets used for more than three millennia and at least eight major languages. It consists of thousands of characters that have changed over time and space. Photographs are the most common representations usable for machine learning, while ink drawings are prone to interpretation. Best suited 3D datasets that are becoming available. We created and used the HeiCuBeDa and MaiCuBeDa datasets, which consist of around 500 annotated tablets. For our novel OCR-like approach to mixed image data, we provide an additional mapping tool for transferring annotations between 3D renderings and photographs. Our sign localization uses a RepPoints detector to predict the locations of characters as bounding boxes. We use image data from GigaMesh's MSII (curvature, see https://gigamesh.eu) based rendering, Phong-shaded 3D 
    
[^39]: FoX:多智能体强化学习中的形成感知探索

    FoX: Formation-aware exploration in multi-agent reinforcement learning. (arXiv:2308.11272v1 [cs.LG])

    [http://arxiv.org/abs/2308.11272](http://arxiv.org/abs/2308.11272)

    FoX是一个新颖的多智能体强化学习框架，通过减少探索空间，并引导智能体在不同形成中访问有意义的状态，显著提高了在合作多智能体任务中的性能。

    

    最近，基于深度的多智能体强化学习 (MARL) 在各种合作多智能体任务中取得了显著的成功。然而，由于智能体的部分可观测性和随着智能体数量增加而指数增长的探索空间，探索仍然是MARL中的一个具有挑战性的问题。首先，为了解决探索空间的可扩展性问题，我们在探索空间上定义了一个基于形成的等价关系，并旨在通过仅探索不同形式的有意义状态来减少搜索空间。然后，我们提出了一种新颖的形成感知探索 (FoX) 框架，通过引导部分可观测智能体凭借自身观测信息充分了解其当前形成，鼓励他们访问不同形成中的状态。数值结果表明，所提出的FoX框架在Google Research Football上明显优于现有的MARL算法。

    Recently, deep multi-agent reinforcement learning (MARL) has gained significant popularity due to its success in various cooperative multi-agent tasks. However, exploration still remains a challenging problem in MARL due to the partial observability of the agents and the exploration space that can grow exponentially as the number of agents increases. Firstly, in order to address the scalability issue of the exploration space, we define a formation-based equivalence relation on the exploration space and aim to reduce the search space by exploring only meaningful states in different formations. Then, we propose a novel formation-aware exploration (FoX) framework that encourages partially observable agents to visit the states in diverse formations by guiding them to be well aware of their current formation solely based on their own observations. Numerical results show that the proposed FoX framework significantly outperforms the state-of-the-art MARL algorithms on Google Research Football
    
[^40]: 量子启发式机器学习：一项调查

    Quantum-Inspired Machine Learning: a Survey. (arXiv:2308.11269v1 [cs.LG])

    [http://arxiv.org/abs/2308.11269](http://arxiv.org/abs/2308.11269)

    量子启发式机器学习（QiML）是一个利用量子力学原理在经典计算框架中的新领域，在本调查中我们提供了对QiML的综合和全面的研究，展示了其多样化的研究领域、最新进展和实际应用，并为QiML建立了明确的定义。未来的研究将从量子力学、量子计算和经典机器学习中汲取经验，丰富QiML的领域。

    

    量子启发式机器学习（QiML）是一个不断发展的领域，因其在经典计算框架中利用量子力学原理的潜力而受到全球研究人员的关注。然而，目前的综述文献经常只对QiML进行表面探索，而更多关注广义的量子机器学习（QML）领域。为了弥补这一空白，本调查提供了对QiML的综合和全面的研究，探讨了QiML的多样化研究领域，包括张量网络模拟、非量子化算法等，展示了最新的进展、实际应用，并揭示了潜在的未来研究方向。此外，通过分析先前对QiML的各种解释及其内在的模糊性，建立了一个具体的QiML定义。随着QiML的不断发展，我们预计将有大量的未来发展从量子力学、量子计算和经典机器学习中汲取经验，丰富QiML的领域。

    Quantum-inspired Machine Learning (QiML) is a burgeoning field, receiving global attention from researchers for its potential to leverage principles of quantum mechanics within classical computational frameworks. However, current review literature often presents a superficial exploration of QiML, focusing instead on the broader Quantum Machine Learning (QML) field. In response to this gap, this survey provides an integrated and comprehensive examination of QiML, exploring QiML's diverse research domains including tensor network simulations, dequantized algorithms, and others, showcasing recent advancements, practical applications, and illuminating potential future research avenues. Further, a concrete definition of QiML is established by analyzing various prior interpretations of the term and their inherent ambiguities. As QiML continues to evolve, we anticipate a wealth of future developments drawing from quantum mechanics, quantum computing, and classical machine learning, enriching 
    
[^41]: 鲁棒拉格朗日和对抗性策略梯度在鲁棒约束马尔可夫决策过程中的应用

    Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])

    [http://arxiv.org/abs/2308.11267](http://arxiv.org/abs/2308.11267)

    本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。

    

    鲁棒约束马尔可夫决策过程（RCMDP）是一个最近应用于强化学习的任务建模框架，它通过使用不确定性集合在转移动态模型中提供了对错误的鲁棒性。模拟RCMDPs需要基于每个状态的值估计计算最坏情况下的动态，这种方法之前在鲁棒约束策略梯度（RCPG）中使用过。本文介绍了两种算法，分别称为具有鲁棒拉格朗日的RCPG和对抗性RCPG。具有鲁棒拉格朗日的RCPG通过使用拉格朗日而不是值或约束来计算最坏情况下的动态从而修改RCPG。对抗性RCPG也基于拉格朗日公式计算最坏情况下的动态，但是将其作为对抗策略直接和增量地学习。

    The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
    
[^42]: 在求解博弈中的高效收敛算法

    Efficient Last-iterate Convergence Algorithms in Solving Games. (arXiv:2308.11256v1 [cs.GT])

    [http://arxiv.org/abs/2308.11256](http://arxiv.org/abs/2308.11256)

    该论文研究了求解博弈中高效收敛算法的问题，通过分析乐观梯度下降上升（OGDA）和乐观乘法权重更新（OMWU）算法，以及基于奖励转化（RT）框架的算法，提出了解决这些问题的方法。

    

    无悔算法在学习两人零和标准型游戏和扩展型游戏的纳什均衡中很受欢迎。最近的许多研究考虑了最后一次迭代收敛的无悔算法。其中，最有名的两个算法是乐观梯度下降上升（OGDA）和乐观乘法权重更新（OMWU）。然而，OGDA的每次迭代复杂度很高。OMWU具有较低的每次迭代复杂度，但实验性能较差，并且它的收敛仅在纳什均衡唯一时成立。最近的研究提出了一种基于奖励转化（RT）框架用于MWU，它消除了唯一性条件，并且在与OMWU相同迭代次数的情况下实现了有竞争力的性能。不幸的是，基于RT的算法在相同迭代次数下表现不如OGDA，并且它们的收敛保证基于连续时间反馈假设，这在大多数情况下不成立。为了解决这些问题，我们对RT框架进行了更详细的分析。

    No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, w
    
[^43]: 机器学习研究中偏差的调查

    A survey on bias in machine learning research. (arXiv:2308.11254v1 [cs.LG])

    [http://arxiv.org/abs/2308.11254](http://arxiv.org/abs/2308.11254)

    本文调查了机器学习研究中的偏差问题，提供了偏差和错误的分类，并分析了机器学习流程中超过四十种潜在的偏差来源，为每种情况提供了清晰的示例。通过理解和减轻机器学习中的偏差，可以开发出更公平、更透明、更准确的ML模型。

    

    当前对机器学习中偏差的研究通常关注公平性，却忽视了偏差的根源或原因。然而，偏差最初被定义为“系统性错误”，通常是由研究过程中不同阶段的人类引起的。本文旨在通过提供偏差和数据模型中潜在偏差和错误的分类，弥补过去关于偏差研究的差距。该文重点研究机器学习流程中的偏差。调查分析了机器学习（ML）流程中超过四十种潜在的偏差来源，并为每种情况提供了清晰的示例。通过理解机器学习中偏差的来源和后果，可以开发出更好的方法来检测和减轻偏差，从而实现更公平、更透明、更准确的ML模型。

    Current research on bias in machine learning often focuses on fairness, while overlooking the roots or causes of bias. However, bias was originally defined as a "systematic error," often caused by humans at different stages of the research process. This article aims to bridge the gap between past literature on bias in research by providing taxonomy for potential sources of bias and errors in data and models. The paper focus on bias in machine learning pipelines. Survey analyses over forty potential sources of bias in the machine learning (ML) pipeline, providing clear examples for each. By understanding the sources and consequences of bias in machine learning, better methods can be developed for its detecting and mitigating, leading to fairer, more transparent, and more accurate ML models.
    
[^44]: 多源领域适应用于化学过程交叉领域故障诊断

    Multi-Source Domain Adaptation for Cross-Domain Fault Diagnosis of Chemical Processes. (arXiv:2308.11247v1 [cs.LG])

    [http://arxiv.org/abs/2308.11247](http://arxiv.org/abs/2308.11247)

    本文在化学过程的交叉领域故障诊断中，对单源和多源无监督领域适应算法进行了广泛比较。研究结果表明，即使没有进行适应，使用多个领域进行训练也具有积极影响。

    

    故障诊断是过程监视中的重要组成部分。机器学习的故障诊断系统基于传感器数据预测故障类型。然而，这些模型对数据分布的变化敏感，这些变化可能由于监测过程中的变化，如操作模式的改变，导致跨领域故障诊断的情况。本文在化学工业中广泛使用的田纳西-伊斯曼过程的背景下，提供了单源和多源无监督领域适应算法在交叉领域故障诊断中的广泛比较。研究结果表明，即使没有进行适应，使用多个领域进行训练也具有积极影响。因此，多源无监督领域适应的基准模型相对于单源无监督领域适应的基准模型有所改进。

    Fault diagnosis is an essential component in process supervision. Indeed, it determines which kind of fault has occurred, given that it has been previously detected, allowing for appropriate intervention. Automatic fault diagnosis systems use machine learning for predicting the fault type from sensor readings. Nonetheless, these models are sensible to changes in the data distributions, which may be caused by changes in the monitored process, such as changes in the mode of operation. This scenario is known as Cross-Domain Fault Diagnosis (CDFD). We provide an extensive comparison of single and multi-source unsupervised domain adaptation (SSDA and MSDA respectively) algorithms for CDFD. We study these methods in the context of the Tennessee-Eastmann Process, a widely used benchmark in the chemical industry. We show that using multiple domains during training has a positive effect, even when no adaptation is employed. As such, the MSDA baseline improves over the SSDA baseline classificati
    
[^45]: 一个有效的基于Transformer的上下文模型和时间门池化用于说话人识别

    An Effective Transformer-based Contextual Model and Temporal Gate Pooling for Speaker Identification. (arXiv:2308.11241v1 [cs.SD])

    [http://arxiv.org/abs/2308.11241](http://arxiv.org/abs/2308.11241)

    本文介绍了一种基于Transformer的上下文模型和时间门池化的有效方法，应用于说话人识别，并在准确率85.9%的情况下比较了其性能与wav2vec2方法。

    

    Wav2vec2在语音识别中应用Transformer架构和自监督学习取得了成功。最近，这些方法不仅用于语音识别，还用于整个语音处理。本文介绍了一种应用了基于Transformer的上下文模型的有效端到端说话人识别模型。我们探索了参数与性能之间的关系，以确定一个有效模型的结构。此外，我们提出了一种具有强大学习能力的池化方法，称为时间门池化(Temporal Gate Pooling)，用于说话人识别。我们将Conformer作为编码器，并利用BEST-RQ进行预训练，并使用VoxCeleb1的说话人识别进行了评估。该方法在仅有28.5M个参数的情况下，实现了85.9%的准确率，与具有317.7M个参数的wav2vec2相当。代码可在https://github.com/HarunoriKawano/speaker-identification-with-tgp获得。

    Wav2vec2 has achieved success in applying Transformer architecture and self-supervised learning to speech recognition. Recently, these have come to be used not only for speech recognition but also for the entire speech processing. This paper introduces an effective end-to-end speaker identification model applied Transformer-based contextual model. We explored the relationship between the parameters and the performance in order to discern the structure of an effective model. Furthermore, we propose a pooling method, Temporal Gate Pooling, with powerful learning ability for speaker identification. We applied Conformer as encoder and BEST-RQ for pre-training and conducted an evaluation utilizing the speaker identification of VoxCeleb1. The proposed method has achieved an accuracy of 85.9% with 28.5M parameters, demonstrating comparable precision to wav2vec2 with 317.7M parameters. Code is available at https://github.com/HarunoriKawano/speaker-identification-with-tgp.
    
[^46]: 具有特征插入和删除的最小不相关置换

    Minwise-Independent Permutations with Insertion and Deletion of Features. (arXiv:2308.11240v1 [cs.LG])

    [http://arxiv.org/abs/2308.11240](http://arxiv.org/abs/2308.11240)

    这项研究介绍了一种针对动态插入和删除特征的minHash算法，用于近似测量高维二进制数据的Jaccard相似度。

    

    在他们的开创性工作中，Broder等人引入了minHash算法，该算法计算高维二进制数据的低维草图，可以近似地衡量Jaccard相似性。自从它的发明以来，minHash已经在各种大数据应用中被从业者普遍使用。此外，在许多实际场景中，数据是动态的，其特征集会随时间而演变。我们考虑到当特征在数据集中动态插入和删除的情况。我们注意到，这个问题的一个简单解决方案是针对更新的维度重复计算minHash。然而，这是一个昂贵的任务，因为它需要生成新的随机置换。据我们所知，在特征的动态插入和删除的情况下，没有关于minHash的系统研究记录。在这项工作中，我们开始了这项研究，并提出了使minHash草图适应动态插入和删除的算法。

    In their seminal work, Broder \textit{et. al.}~\citep{BroderCFM98} introduces the $\mathrm{minHash}$ algorithm that computes a low-dimensional sketch of high-dimensional binary data that closely approximates pairwise Jaccard similarity. Since its invention, $\mathrm{minHash}$ has been commonly used by practitioners in various big data applications. Further, the data is dynamic in many real-life scenarios, and their feature sets evolve over time. We consider the case when features are dynamically inserted and deleted in the dataset. We note that a naive solution to this problem is to repeatedly recompute $\mathrm{minHash}$ with respect to the updated dimension. However, this is an expensive task as it requires generating fresh random permutations. To the best of our knowledge, no systematic study of $\mathrm{minHash}$ is recorded in the context of dynamic insertion and deletion of features. In this work, we initiate this study and suggest algorithms that make the $\mathrm{minHash}$ sket
    
[^47]: 使用患者数据的联邦学习以保护多囊卵巢综合征的隐私

    Federated Learning on Patient Data for Privacy-Protecting Polycystic Ovary Syndrome Treatment. (arXiv:2308.11220v1 [cs.LG])

    [http://arxiv.org/abs/2308.11220](http://arxiv.org/abs/2308.11220)

    本研究使用联邦学习方法，通过访问大量多样的患者数据并保护隐私，来预测多囊卵巢综合征患者的最佳治疗药物选项。

    

    妇科内分泌学领域在数据驱动的医疗解决方案方面落后，主要是因为对患者数据隐私的担忧。有关荷尔蒙水平或月经周期的有价值的数据点可能会暴露出患有合并症或终止妊娠的患者，侵犯其隐私。我们探讨了在预测多囊卵巢综合征（PCOS）患者的最佳药物方面应用联邦学习（FL）的方法。PCOS是一种影响全球数百万女性的严重激素失调疾病，但其研究受限于患者数据的缺乏。我们展示了各种联邦学习方法在合成的PCOS患者数据集上的成功应用。我们提出的联邦学习模型是一种访问大量多样数据并识别最有效治疗选项的工具，同时提供PCOS患者的隐私保证。

    The field of women's endocrinology has trailed behind data-driven medical solutions, largely due to concerns over the privacy of patient data. Valuable datapoints about hormone levels or menstrual cycling could expose patients who suffer from comorbidities or terminate a pregnancy, violating their privacy. We explore the application of Federated Learning (FL) to predict the optimal drug for patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal disorder impacting millions of women worldwide, yet it's poorly understood and its research is stunted by a lack of patient data. We demonstrate that a variety of FL approaches succeed on a synthetic PCOS patient dataset. Our proposed FL models are a tool to access massive quantities of diverse data and identify the most effective treatment option while providing PCOS patients with privacy guarantees.
    
[^48]: 大模型时代中的联邦学习：针对特定领域的多模态大模型

    Federated Learning in Big Model Era: Domain-Specific Multimodal Large Models. (arXiv:2308.11217v1 [cs.LG])

    [http://arxiv.org/abs/2308.11217](http://arxiv.org/abs/2308.11217)

    本论文提出了一种多模态联邦学习框架，利用私有领域数据协同训练大型模型，以实现跨场景的智能服务。在大模型时代，该框架解决了异构数据、模型聚合、性能和成本权衡、数据隐私以及激励机制等方面的挑战。

    

    多模态数据能够全面感知和识别物理世界，已成为通往通用人工智能的重要路径。然而，在公共数据集上训练的多模态大模型在特定工业领域的性能往往不理想。本文提出了一种多模态联邦学习框架，可以使多个企业利用私有领域数据协同训练大型模型，实现跨场景的智能服务。作者深入探讨了大模型时代联邦学习的智能基础和目标的战略转变，以及在异构数据、模型聚合、性能和成本权衡、数据隐私和激励机制方面面临的新挑战。本文详细介绍了领先企业在城市安全运营管理方面贡献多模态数据和专家知识的案例研究，包括分布式部署和高效性能的实现。

    Multimodal data, which can comprehensively perceive and recognize the physical world, has become an essential path towards general artificial intelligence. However, multimodal large models trained on public datasets often underperform in specific industrial domains. This paper proposes a multimodal federated learning framework that enables multiple enterprises to utilize private domain data to collaboratively train large models for vertical domains, achieving intelligent services across scenarios. The authors discuss in-depth the strategic transformation of federated learning in terms of intelligence foundation and objectives in the era of big model, as well as the new challenges faced in heterogeneous data, model aggregation, performance and cost trade-off, data privacy, and incentive mechanism. The paper elaborates a case study of leading enterprises contributing multimodal data and expert knowledge to city safety operation management , including distributed deployment and efficient 
    
[^49]: Hamiltonian GAN. (arXiv:2308.11216v1 [cs.LG])

    Hamiltonian GAN. (arXiv:2308.11216v1 [cs.LG])

    [http://arxiv.org/abs/2308.11216](http://arxiv.org/abs/2308.11216)

    这个论文提出了一种基于Hamiltonian的生成对抗网络（GAN）视频生成方法，通过学习配置空间映射和运动模型来生成合理的视频。训练过程中使用的物理启发式损失函数可以促进对最小配置空间的表示和提高可解释性。

    

    一系列越来越多的研究利用Hamiltonian形式主义作为物理上合理的神经网络视频生成的归纳偏见。Hamiltonian的结构确保了学习到的数量（如能量）的守恒，并在低维流形上给输入视频施加了一个相空间解释。虽然这种解释潜在地有助于在下游任务中整合学习到的表示，但现有方法在可行性上存在限制，因为它们需要在设计时对配置空间的结构先验知识。在这项工作中，我们提出了一种基于GAN的视频生成流程，其中包括了一个学习到的配置空间映射和Hamiltonian神经网络运动模型，以从数据中学习配置空间的表示。我们使用一种受物理启发的循环坐标损失函数来训练模型，这可以促进对配置空间的最小表示并提高可解释性。我们展示了这种方法的有效性。

    A growing body of work leverages the Hamiltonian formalism as an inductive bias for physically plausible neural network based video generation. The structure of the Hamiltonian ensures conservation of a learned quantity (e.g., energy) and imposes a phase-space interpretation on the low-dimensional manifold underlying the input video. While this interpretation has the potential to facilitate the integration of learned representations in downstream tasks, existing methods are limited in their applicability as they require a structural prior for the configuration space at design time. In this work, we present a GAN-based video generation pipeline with a learned configuration space map and Hamiltonian neural network motion model, to learn a representation of the configuration space from data. We train our model with a physics-inspired cyclic-coordinate loss function which encourages a minimal representation of the configuration space and improves interpretability. We demonstrate the effica
    
[^50]: 多模态时空数据建模的简单框架

    A Simple Framework for Multi-mode Spatial-Temporal Data Modeling. (arXiv:2308.11204v1 [cs.LG])

    [http://arxiv.org/abs/2308.11204](http://arxiv.org/abs/2308.11204)

    本文提出了一个简单的多模态空间 - 时间数据建模框架，通过跨模态关系学习和多层感知器来有效地建立多模态之间的连接并捕捉时间依赖关系和通道相关性。

    

    空间 - 时间数据建模旨在挖掘系统中对象之间的空间关系和时间依赖关系。然而，大多数现有方法集中于单模式下的空间 - 时间数据建模，缺乏对多模式的理解。尽管最近有很少的方法来学习多模式关系，但它们建立在复杂的组件上，使模型复杂性更高。在本文中，我们提出了一个简单的多模态空间 - 时间数据建模框架，以同时兼具效果和效率。具体而言，我们设计了一个通用的跨模态空间关系学习组件，以自适应地建立多模态之间的连接，并沿着学习到的连接传播信息。此外，我们使用多层感知器来捕捉时间依赖关系和通道相关性，这在概念上和技术上都很简洁。在三个真实数据集上的实验显示，我们的模型

    Spatial-temporal data modeling aims to mine the underlying spatial relationships and temporal dependencies of objects in a system. However, most existing methods focus on the modeling of spatial-temporal data in a single mode, lacking the understanding of multiple modes. Though very few methods have been presented to learn the multi-mode relationships recently, they are built on complicated components with higher model complexities. In this paper, we propose a simple framework for multi-mode spatial-temporal data modeling to bring both effectiveness and efficiency together. Specifically, we design a general cross-mode spatial relationships learning component to adaptively establish connections between multiple modes and propagate information along the learned connections. Moreover, we employ multi-layer perceptrons to capture the temporal dependencies and channel correlations, which are conceptually and technically succinct. Experiments on three real-world datasets show that our model 
    
[^51]: SegRNN: 长期时间序列预测的分段循环神经网络

    SegRNN: Segment Recurrent Neural Network for Long-Term Time Series Forecasting. (arXiv:2308.11200v1 [cs.LG])

    [http://arxiv.org/abs/2308.11200](http://arxiv.org/abs/2308.11200)

    SegRNN是一种针对长期时间序列预测任务的分段循环神经网络，通过两种新策略（分段迭代和并行多步预测）显著减少了循环迭代次数，提高了预测准确性和推理速度。与现有的基于Transformer模型的方法相比，SegRNN不仅表现更好，还大幅减少了运行时间和内存使用量。

    

    在长期时间序列预测（LTSF）领域中，基于循环神经网络（RNN）的方法在处理过长的回溯窗口和预测范围时面临挑战。因此，该领域的主导地位已经转向Transformer、MLP和CNN方法。RNN在LTSF中存在限制的根本原因是循环迭代的数量相当多。为了解决这些问题，我们提出了两种减少RNN在LTSF任务中迭代次数的新策略：分段迭代和并行多步预测（PMF）。将这些策略结合起来的SegRNN显著减少了LTSF所需的循环迭代次数，从而显著提高了预测准确性和推理速度。大量实验表明，SegRNN不仅优于现有的基于Transformer模型的方法，还将运行时间和内存使用量减少了超过78%。这些成果为RNN在LTSF任务中的出色表现提供了强有力的证据。

    RNN-based methods have faced challenges in the Long-term Time Series Forecasting (LTSF) domain when dealing with excessively long look-back windows and forecast horizons. Consequently, the dominance in this domain has shifted towards Transformer, MLP, and CNN approaches. The substantial number of recurrent iterations are the fundamental reasons behind the limitations of RNNs in LTSF. To address these issues, we propose two novel strategies to reduce the number of iterations in RNNs for LTSF tasks: Segment-wise Iterations and Parallel Multi-step Forecasting (PMF). RNNs that combine these strategies, namely SegRNN, significantly reduce the required recurrent iterations for LTSF, resulting in notable improvements in forecast accuracy and inference speed. Extensive experiments demonstrate that SegRNN not only outperforms SOTA Transformer-based models but also reduces runtime and memory usage by more than 78%. These achievements provide strong evidence that RNNs continue to excel in LTSF ta
    
[^52]: ConcatPlexer：通过附加Dim1批处理以加快ViTs速度

    ConcatPlexer: Additional Dim1 Batching for Faster ViTs. (arXiv:2308.11199v1 [cs.CV])

    [http://arxiv.org/abs/2308.11199](http://arxiv.org/abs/2308.11199)

    本文提出了一种名为ConcatPlexer的方法，通过在视觉识别中使用附加的Dim1批处理（即连接）来提高吞吐量，同时准确性受到的影响较小。

    

    Transformer不仅在自然语言处理领域，还在计算机视觉领域取得了巨大成功，引发了各种创新的方法和应用。然而，Transformer卓越的性能和建模灵活性带来了计算成本的严重增加，因此有几项工作提出了减少这种负担的方法。受语言模型的一种减少成本的方法Data Multiplexing (DataMUX)的启发，我们提出了一种用于高效视觉识别的新方法，它采用了附加的Dim1批处理（即连接），在保证准确性的基础上大大提高了吞吐量。我们首先为视觉模型引入了DataMux的一种天然适应方法，图像多路复用器（Image Multiplexer），并设计了新的组件来克服其缺点，进而形成了我们最终的模型ConcatPlexer，在推理速度和准确度之间找到了平衡点。ConcatPlexer在ImageNet1K和CIFAR100数据集上进行了训练。

    Transformers have demonstrated tremendous success not only in the natural language processing (NLP) domain but also the field of computer vision, igniting various creative approaches and applications. Yet, the superior performance and modeling flexibility of transformers came with a severe increase in computation costs, and hence several works have proposed methods to reduce this burden. Inspired by a cost-cutting method originally proposed for language models, Data Multiplexing (DataMUX), we propose a novel approach for efficient visual recognition that employs additional dim1 batching (i.e., concatenation) that greatly improves the throughput with little compromise in the accuracy. We first introduce a naive adaptation of DataMux for vision models, Image Multiplexer, and devise novel components to overcome its weaknesses, rendering our final model, ConcatPlexer, at the sweet spot between inference speed and accuracy. The ConcatPlexer was trained on ImageNet1K and CIFAR100 dataset and
    
[^53]: 在语音、语言和听力科学中建立通用的机器学习模型：功效分析和样本容量估计

    Toward Generalizable Machine Learning Models in Speech, Language, and Hearing Sciences: Power Analysis and Sample Size Estimation. (arXiv:2308.11197v1 [cs.LG])

    [http://arxiv.org/abs/2308.11197](http://arxiv.org/abs/2308.11197)

    该研究提供了使用嵌套交叉验证方法的定量证据，并提出了基于机器学习分析进行功效分析的方法。通过对交叉验证方法、特征和模型维度之间的相互作用进行蒙特卡罗模拟，比较了不同交叉验证方法的统计功效和置信度。同时，确定了获得统计显著结果所需的最小样本容量。

    

    该研究的第一个目的是提供定量证据，以激励研究人员改用更健壮的嵌套交叉验证方法。第二个目的是在研究设计过程中提出基于机器学习分析的功效分析方法和MATLAB代码。通过蒙特卡罗模拟，量化了所采用的交叉验证方法、特征的判别力、特征空间的维度和模型的维度之间的相互作用。基于机器学习模型的统计功效和统计置信度，比较了四种不同的交叉验证方法（单一留出法、10折交叉验证、训练-验证-测试法和嵌套10折交叉验证）。利用零假设和备择假设的分布确定了获得统计显著结果所需的最小样本容量（α=0.05，1-β=0.8）。模型的统计置信度被定义为正确特征的概率。

    This study's first purpose is to provide quantitative evidence that would incentivize researchers to instead use the more robust method of nested cross-validation. The second purpose is to present methods and MATLAB codes for doing power analysis for ML-based analysis during the design of a study. Monte Carlo simulations were used to quantify the interactions between the employed cross-validation method, the discriminative power of features, the dimensionality of the feature space, and the dimensionality of the model. Four different cross-validations (single holdout, 10-fold, train-validation-test, and nested 10-fold) were compared based on the statistical power and statistical confidence of the ML models. Distributions of the null and alternative hypotheses were used to determine the minimum required sample size for obtaining a statistically significant outcome ({\alpha}=0.05, 1-\b{eta}=0.8). Statistical confidence of the model was defined as the probability of correct features being 
    
[^54]: 自动并行化ML/DL模型中的数据流图

    Automatic Task Parallelization of Dataflow Graphs in ML/DL models. (arXiv:2308.11192v1 [cs.LG])

    [http://arxiv.org/abs/2308.11192](http://arxiv.org/abs/2308.11192)

    通过对ML数据流图进行关键路径线性聚类和任务并行化，我们提出了一种优化ML/DL模型的自动并行化方法。与其他工作不同的是，我们通过一个新的工具Ramiel生成可读和可执行的并行Pytorch+Python代码。

    

    现今存在多种方法用于加速机器学习(ML)或深度学习(DL)模型的训练和推断过程。然而，依赖于各种图形和运算符并行性方法的现代技术依赖于搜索空间优化，这在功耗和硬件使用方面代价高昂。特别是在推断过程中，当批量大小为1且在CPU上执行或用于功耗受限的边缘设备时，现有技术可能变得昂贵、复杂或不适用。为了改善这一问题，我们提出了一种基于关键路径的线性聚类方法，以利用ML数据流图中的内在并行路径。我们的任务并行化方法通过克隆优化图的结构，并通过常量传播和死代码消除对其进行修剪。与其他工作不同的是，我们通过一个名为Ramiel的新工具，从ONNX格式的输入ML模型生成可读和可执行的并行Pytorch+Python代码。这使我们能够...

    Several methods exist today to accelerate Machine Learning(ML) or Deep-Learning(DL) model performance for training and inference. However, modern techniques that rely on various graph and operator parallelism methodologies rely on search space optimizations which are costly in terms of power and hardware usage. Especially in the case of inference, when the batch size is 1 and execution is on CPUs or for power-constrained edge devices, current techniques can become costly, complicated or inapplicable. To ameliorate this, we present a Critical-Path-based Linear Clustering approach to exploit inherent parallel paths in ML dataflow graphs. Our task parallelization approach further optimizes the structure of graphs via cloning and prunes them via constant propagation and dead-code elimination. Contrary to other work, we generate readable and executable parallel Pytorch+Python code from input ML models in ONNX format via a new tool that we have built called {\bf Ramiel}. This allows us to be
    
[^55]: 多样性指标：语言模型查询中失败的领域无关代理

    Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])

    [http://arxiv.org/abs/2308.11189](http://arxiv.org/abs/2308.11189)

    本文提出了一种基于回应多样性的大型语言模型错误量化指标，这些指标独立于领域特定信息，并与失败概率强相关。实证结果展示了这些指标在少样本提示、思维链推理和错误检测方面的应用。

    

    大型语言模型中的错误预测通常依赖于领域特定的信息。本文提出了一种基于回应多样性的大型语言模型错误量化指标，因此独立于底层应用。我们描述了如何使用基于熵、基尼不纯度和质心距离的三个指标。我们进行了一系列的实验，涉及多个数据集和温度设置，证明这些指标与失败概率强相关。此外，我们还提出了实证结果，展示了如何将这些指标应用于少样本提示、思维链推理和错误检测。

    Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
    
[^56]: 一种用于数字癌症组织中多器官细胞核的同时语义分割、实例分割和分类的自下而上框架

    A three in one bottom-up framework for simultaneous semantic segmentation, instance segmentation and classification of multi-organ nuclei in digital cancer histology. (arXiv:2308.11179v1 [cs.CV])

    [http://arxiv.org/abs/2308.11179](http://arxiv.org/abs/2308.11179)

    本研究提出了一种自下而上的框架，用于数字癌症组织中多器官细胞核的同时语义分割、实例分割和分类。通过引入额外的解码器头部，并利用独立的加权损失来产生语义分割、边界提议和分类图，我们解决了语义分割的挑战，并扩展到了同时进行实例分割和分类的问题。

    

    数字组织学中细胞核的同时分割和分类在计算机辅助癌症诊断中起着重要作用，然而仍然具有挑战性。我们之前的工作解决了语义分割的问题，并在此基础上将模型扩展到同时进行实例分割和分类。我们引入了额外的解码器头部，利用独立的加权损失产生语义分割、边界提议和分类图。我们使用这三个头部模型的输出来进行综合分割和分类。

    Simultaneous segmentation and classification of nuclei in digital histology play an essential role in computer-assisted cancer diagnosis; however, it remains challenging. The highest achieved binary and multi-class Panoptic Quality (PQ) remains as low as 0.68 bPQ and 0.49 mPQ, respectively. It is due to the higher staining variability, variability across the tissue, rough clinical conditions, overlapping nuclei, and nuclear class imbalance. The generic deep-learning methods usually rely on end-to-end models, which fail to address these problems associated explicitly with digital histology. In our previous work, DAN-NucNet, we resolved these issues for semantic segmentation with an end-to-end model. This work extends our previous model to simultaneous instance segmentation and classification. We introduce additional decoder heads with independent weighted losses, which produce semantic segmentation, edge proposals, and classification maps. We use the outputs from the three-head model to
    
[^57]: 对使用深度网络在WHO乳腺分类中进行肿瘤区分的搜索和匹配的初步研究

    A Preliminary Investigation into Search and Matching for Tumour Discrimination in WHO Breast Taxonomy Using Deep Networks. (arXiv:2308.11162v1 [eess.IV])

    [http://arxiv.org/abs/2308.11162](http://arxiv.org/abs/2308.11162)

    该研究初步探索了使用深度网络在WHO乳腺分类中进行肿瘤区分的搜索和匹配方法。研究采用了深度学习模型提取的深度特征对35种肿瘤类型在数字图谱中进行了可视化分析。

    

    乳腺癌是影响全球女性最常见的癌症之一。它们包括一组具有各种生物学、临床和组织病理学特征的恶性新生物。有35多种不同的乳腺病变的组织学形式可根据细胞形态学、生长和组织构架模式进行组织学分类和诊断。近年来，在人工智能领域，深度学习对医学图像的计算表示引起了很多关注。可搜索的数字图谱可以提供给病理学家可以搜索明确诊断和治疗的档案案例的补丁匹配工具，这项技术可以被视为计算机的第二意见。在本研究中，我们对包括35种肿瘤类型的WHO乳腺分类进行了索引和分析。我们使用先进的深度学习模型提取的深度特征可视化了所有肿瘤类型。

    Breast cancer is one of the most common cancers affecting women worldwide. They include a group of malignant neoplasms with a variety of biological, clinical, and histopathological characteristics. There are more than 35 different histological forms of breast lesions that can be classified and diagnosed histologically according to cell morphology, growth, and architecture patterns. Recently, deep learning, in the field of artificial intelligence, has drawn a lot of attention for the computerized representation of medical images. Searchable digital atlases can provide pathologists with patch matching tools allowing them to search among evidently diagnosed and treated archival cases, a technology that may be regarded as computational second opinion. In this study, we indexed and analyzed the WHO breast taxonomy (Classification of Tumours 5th Ed.) spanning 35 tumour types. We visualized all tumour types using deep features extracted from a state-of-the-art deep learning model, pre-trained
    
[^58]: 通过超出平衡状态的扩展动力学性能评估神经力场

    xxMD: Benchmarking Neural Force Fields Using Extended Dynamics beyond Equilibrium. (arXiv:2308.11155v1 [cs.LG])

    [http://arxiv.org/abs/2308.11155](http://arxiv.org/abs/2308.11155)

    在神经力场模型中，常用的MD17数据集对于表示经历化学反应的系统不足。为了解决这一问题，我们引入了xxMD数据集，该数据集采样自扩展激发态分子动力学，包含了能量和力的信息。

    

    神经力场已成为计算化学中的重要模型，取代了从头算的分子动力学中的量子化学计算。目前对神经力场的主要评估基准是MD17数据集及其后续扩展。这些数据集主要包含来自基态势能面平衡区域的几何结构，采样自直接绝热动力学。然而，许多化学反应涉及到较大的分子变形，特别是键断裂。我们展示了MD17数据集中内坐标和能量的约束分布，凸显了其在表示经历化学反应的系统方面的不足。为了解决这种采样限制，我们引入了xxMD（扩展激发态分子动力学）数据集，从非绝热动力学中派生。该数据集包含了从多参考波函数理论和密度泛函中确定的能量和力。

    Neural force fields (NFFs) have gained prominence in computational chemistry as surrogate models, superseding quantum-chemistry calculations in ab initio molecular dynamics. The prevalent benchmark for NFFs has been the MD17 dataset and its subsequent extension. These datasets predominantly comprise geometries from the equilibrium region of the ground electronic state potential energy surface, sampling from direct adiabatic dynamics. However, many chemical reactions entail significant molecular deformations, notably bond breaking. We demonstrate the constrained distribution of internal coordinates and energies in the MD17 datasets, underscoring their inadequacy for representing systems undergoing chemical reactions. Addressing this sampling limitation, we introduce the xxMD (Extended Excited-state Molecular Dynamics) dataset, derived from non-adiabatic dynamics. This dataset encompasses energies and forces ascertained from both multireference wave function theory and density functional
    
[^59]: 使用深度强化学习的移动感知计算卸载于群体机器人中的应用

    Mobility-Aware Computation Offloading for Swarm Robotics using Deep Reinforcement Learning. (arXiv:2308.11154v1 [cs.RO])

    [http://arxiv.org/abs/2308.11154](http://arxiv.org/abs/2308.11154)

    使用深度强化学习的移动感知计算卸载于群体机器人中的应用，可以减轻计算负担，满足延迟要求并保证计算精度，同时使用最小的机器人能量。

    

    群体机器人被设想为能自动化进行大量肮脏、危险和乏味任务的机器人。然而，机器人能量、计算能力和通信资源有限。因此，目前的群体机器人只能提供有限的时空信息。在本文中，我们提出利用移动边缘计算来减轻计算负担。我们基于移动感知的深度强化学习模型在边缘服务器端开发了一种有效的解决方案，用于计算调度和资源分配。我们的结果表明，所提出的方法可以通过使用最小的机器人能量来满足延迟要求并保证计算精度。

    Swarm robotics is envisioned to automate a large number of dirty, dangerous, and dull tasks. Robots have limited energy, computation capability, and communication resources. Therefore, current swarm robotics have a small number of robots, which can only provide limited spatio-temporal information. In this paper, we propose to leverage the mobile edge computing to alleviate the computation burden. We develop an effective solution based on a mobility-aware deep reinforcement learning model at the edge server side for computing scheduling and resource. Our results show that the proposed approach can meet delay requirements and guarantee computation precision by using minimum robot energy.
    
[^60]: 卫星通信中基于神经形态计算的节能局内无线资源管理

    Energy-Efficient On-Board Radio Resource Management for Satellite Communications via Neuromorphic Computing. (arXiv:2308.11152v1 [eess.SY])

    [http://arxiv.org/abs/2308.11152](http://arxiv.org/abs/2308.11152)

    本研究通过应用节能的基于神经形态计算的机器学习模型来进行卫星通信中的局内无线资源管理。实验结果表明，在面对传统卷积神经网络的对比中，基于脉冲神经网络在能效方面表现出色。

    

    最新的卫星通信任务采用完全可重构的局内软件定义有效载荷，能够根据系统通信的时空变化来调整无线资源。由于优化算法的计算复杂度高且缺乏灵活性，基于机器学习的方法成为了有希望的替代方案。我们研究了应用基于脑启发式机器学习模型实现节能的局内无线资源管理。除了软件模拟外，我们还利用最近发布的Intel Loihi 2芯片进行了大量实验。为了评估所提出模型的性能，我们在Xilinx Versal VCK5000上实现了传统卷积神经网络(CNN)，并对不同通信需求下的准确性、精度、召回率和能效进行了详细比较。值得注意的是，对于相关工作负载，基于脉冲神经网络(SNNs)在Loihi芯片上的实现效果显著。

    The latest satellite communication (SatCom) missions are characterized by a fully reconfigurable on-board software-defined payload, capable of adapting radio resources to the temporal and spatial variations of the system traffic. As pure optimization-based solutions have shown to be computationally tedious and to lack flexibility, machine learning (ML)-based methods have emerged as promising alternatives. We investigate the application of energy-efficient brain-inspired ML models for on-board radio resource management. Apart from software simulation, we report extensive experimental results leveraging the recently released Intel Loihi 2 chip. To benchmark the performance of the proposed model, we implement conventional convolutional neural networks (CNN) on a Xilinx Versal VCK5000, and provide a detailed comparison of accuracy, precision, recall, and energy efficiency for different traffic demands. Most notably, for relevant workloads, spiking neural networks (SNNs) implemented on Loih
    
[^61]: LLaMA-Reviewer: 通过参数高效微调推进大型语言模型在代码审查自动化中的应用（实证研究）

    LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])

    [http://arxiv.org/abs/2308.11148](http://arxiv.org/abs/2308.11148)

    本文提出了LLaMA-Reviewer框架，通过参数高效微调方法，利用流行的大型语言模型LLaMA在代码审查领域能力，实现对代码审查任务的自动化。研究表明，即使仅使用不到1%的可训练参数，该框架仍能取得显著的成果。

    

    代码审查活动的自动化长期以来一直是软件工程领域的追求，主要通过许多领域特定的预训练模型来解决。尽管这些模型取得了一定的成功，但它们经常需要大量的资源从头开始进行预训练。相比之下，大型语言模型（LLMs）在补充领域特定知识的情况下展现出了令人着迷的潜力。然而，它们在自动化代码审查任务方面的潜力仍然很少被探索。为了填补这一研究空白，我们提出了LLaMA-Reviewer，这是一个创新的框架，它利用了流行的LLM——LLaMA在代码审查领域的能力。考虑到资源限制，该框架采用了参数高效微调（PEFT）方法，以极少的可训练参数提供高性能。我们对LLaMA-Reviewer进行了广泛的评估，使用了两个不同的公开数据集。值得注意的是，即使在只使用不到1%的可训练参数的情况下，它也取得了显著的成果。

    The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
    
[^62]: 无监督细胞识别中的先验自激活图探索

    Exploring Unsupervised Cell Recognition with Prior Self-activation Maps. (arXiv:2308.11144v1 [cs.CV])

    [http://arxiv.org/abs/2308.11144](http://arxiv.org/abs/2308.11144)

    该论文提出了一种无监督细胞识别方法，通过先验自激活图生成伪掩膜作为训练目标。在多个数据集上的实验证明，该方法在细胞分割和多类别细胞检测任务中优于其他监督和弱监督方法。

    

    目前在细胞识别任务上，监督深度学习模型的成功依赖于详细的注释。许多先前的研究已经成功减少了对标签的依赖性。然而，考虑到一个补丁中包含的大量细胞，昂贵而低效的标注仍然不可避免。为此，我们探索了无标签方法来进行细胞识别。我们提出了先验自激活图（PSM）来生成伪掩膜作为训练目标。具体而言，我们使用自监督学习来训练一个激活网络。网络的浅层中的梯度信息被聚合以生成先验自激活图。然后，我们引入了一个语义聚类模块作为管道，将PSMs转换为像素级语义伪掩膜，以供下游任务使用。我们在两个组织学数据集MoNuSeg（细胞分割）和BCData（多类别细胞检测）上评估了我们的方法。与其他全监督和弱监督方法相比，我们的方法表现更好。

    The success of supervised deep learning models on cell recognition tasks relies on detailed annotations. Many previous works have managed to reduce the dependency on labels. However, considering the large number of cells contained in a patch, costly and inefficient labeling is still inevitable. To this end, we explored label-free methods for cell recognition. Prior self-activation maps (PSM) are proposed to generate pseudo masks as training targets. To be specific, an activation network is trained with self-supervised learning. The gradient information in the shallow layers of the network is aggregated to generate prior self-activation maps. Afterward, a semantic clustering module is then introduced as a pipeline to transform PSMs to pixel-level semantic pseudo masks for downstream tasks. We evaluated our method on two histological datasets: MoNuSeg (cell segmentation) and BCData (multi-class cell detection). Compared with other fully-supervised and weakly-supervised methods, our metho
    
[^63]: 排球分析的图编码和神经网络方法：从比赛结果到个体战略预测

    Graph Encoding and Neural Network Approaches for Volleyball Analytics: From Game Outcome to Individual Play Predictions. (arXiv:2308.11142v1 [cs.LG])

    [http://arxiv.org/abs/2308.11142](http://arxiv.org/abs/2308.11142)

    本研究采用了图编码和神经网络方法，通过丰富的排球数据集，提高了排球预测的准确性，并通过比较基准模型的性能分析排球局势的潜在关系。

    

    本研究旨在提高复杂排球预测的准确性，并为教练和球员提供更有意义的见解。我们引入了一种专门的图编码技术，在已有排球数据集的基础上，无需额外的数据收集，为每次接触添加附加的排球上下文。我们展示了在这个丰富的数据集上使用图神经网络（GNN）进行三种不同的排球预测任务：回合结果预测、局位置预测和击球类型预测的潜在好处。我们比较了基于图的模型与基准模型的性能，并分析了结果，以更好地理解排球攻防中的潜在关系。我们的结果表明，在结合图编码的情况下使用GNN可明显提高数据的分析水平，从而总体上提高预测结果。我们还表明，通过简单的调整，如移除某些特征，能够显著改善这些基准任务。

    This research aims to improve the accuracy of complex volleyball predictions and provide more meaningful insights to coaches and players. We introduce a specialized graph encoding technique to add additional contact-by-contact volleyball context to an already available volleyball dataset without any additional data gathering. We demonstrate the potential benefits of using graph neural networks (GNNs) on this enriched dataset for three different volleyball prediction tasks: rally outcome prediction, set location prediction, and hit type prediction. We compare the performance of our graph-based models to baseline models and analyze the results to better understand the underlying relationships in a volleyball rally. Our results show that the use of GNNs with our graph encoding yields a much more advanced analysis of the data, which noticeably improves prediction results overall. We also show that these baseline tasks can be significantly improved with simple adjustments, such as removing 
    
[^64]: 面向交互式推荐系统中长期用户反馈验证的研究

    Towards Validating Long-Term User Feedbacks in Interactive Recommendation Systems. (arXiv:2308.11137v1 [cs.IR])

    [http://arxiv.org/abs/2308.11137](http://arxiv.org/abs/2308.11137)

    本研究重新研究了使用评论数据集的交互式推荐系统实验，并发现简单的贪婪模型可以实现与基于RL的模型相媲美的性能。

    

    交互式推荐系统（IRS）吸引了很多关注，因为它们能够模拟用户与推荐系统之间的交互过程。许多方法采用强化学习（RL）算法，因为这些算法可以直接最大化用户的累积奖励。在IRS中，研究人员通常使用公开的评论数据集来比较和评估算法。然而，公开数据集中提供的用户反馈只包括即时反应（例如，评分），没有包括延迟反应（例如停留时间和生命周期价值）。因此，问题在于这些评论数据集是否适合评估IRS的长期效果。在这项工作中，我们重新研究了使用评论数据集的IRS实验，并将基于RL的模型与一种简单的奖励模型进行了比较，后者以贪婪的方式推荐具有最高单步奖励的项目。经过广泛分析，我们得出了三个主要发现：首先，简单的贪婪模型可以实现与基于RL的模型相媲美的性能。

    Interactive Recommender Systems (IRSs) have attracted a lot of attention, due to their ability to model interactive processes between users and recommender systems. Numerous approaches have adopted Reinforcement Learning (RL) algorithms, as these can directly maximize users' cumulative rewards. In IRS, researchers commonly utilize publicly available review datasets to compare and evaluate algorithms. However, user feedback provided in public datasets merely includes instant responses (e.g., a rating), with no inclusion of delayed responses (e.g., the dwell time and the lifetime value). Thus, the question remains whether these review datasets are an appropriate choice to evaluate the long-term effects of the IRS. In this work, we revisited experiments on IRS with review datasets and compared RL-based models with a simple reward model that greedily recommends the item with the highest one-step reward. Following extensive analysis, we can reveal three main findings: First, a simple greedy
    
[^65]: 使用层次结构距离捕捉多层次图结构的变压器

    Transformers for Capturing Multi-level Graph Structure using Hierarchical Distances. (arXiv:2308.11129v1 [cs.LG])

    [http://arxiv.org/abs/2308.11129](http://arxiv.org/abs/2308.11129)

    本论文提出了一种层次距离结构编码（HDSE）方法，用于捕捉多层次图结构。经过在12个真实世界数据集上的实验，证明了该方法在10个基准数据集上实验效果达到了最先进水平。

    

    图变压器需要强大的归纳偏差来得出有意义的注意力分数。然而，当前的提议很少涉及捕捉更长距离、层次结构或社区结构的方法，而这些在分子、社交网络和引用网络等各种图形中都会出现。在本文中，我们提出了一种层次距离结构编码（HDSE）方法，用于建模图中节点之间的层次距离，重点关注其多层次、层次化的性质。特别是，这产生了一个可以灵活与现有图变压器集成的框架，可以与其他位置表示同时应用。通过在12个真实世界数据集上进行大量实验，我们证明了我们的HDSE方法成功提升了各种类型的基线变压器，在10个基准数据集上获得了最先进的实证性能。

    Graph transformers need strong inductive biases to derive meaningful attention scores. Yet, current proposals rarely address methods capturing longer ranges, hierarchical structures, or community structures, as they appear in various graphs such as molecules, social networks, and citation networks. In this paper, we propose a hierarchy-distance structural encoding (HDSE), which models a hierarchical distance between the nodes in a graph focusing on its multi-level, hierarchical nature. In particular, this yields a framework which can be flexibly integrated with existing graph transformers, allowing for simultaneous application with other positional representations. Through extensive experiments on 12 real-world datasets, we demonstrate that our HDSE method successfully enhances various types of baseline transformers, achieving state-of-the-art empirical performances on 10 benchmark datasets.
    
[^66]: 图神经网络在推荐中的表达能力有多强？

    How Expressive are Graph Neural Networks in Recommendation?. (arXiv:2308.11127v1 [cs.IR])

    [http://arxiv.org/abs/2308.11127](http://arxiv.org/abs/2308.11127)

    本文对图神经网络在推荐中的表达能力进行了理论分析，发现现有的表达能力度量标准可能无法有效评估模型在推荐中的能力，提出了一个全面的理论分析方法。

    

    图神经网络（GNNs）在各种图学习任务中展示了优越的性能，包括利用图中的用户-物品协作过滤信号进行推荐。然而，尽管它们在最先进的推荐模型中的经验有效性，但对于它们的能力的理论表述非常稀少。最近的研究探讨了GNNs的一般表达能力，证明了消息传递GNNs至多与Weisfeiler-Lehman测试一样强大，并且与随机节点初始化相结合的GNNs是通用的。然而，GNNs的“表达能力”概念仍然定义模糊。大多数现有的工作采用图同构测试作为表达能力的度量标准，但这种图级任务可能不能有效评估模型在推荐中区分不同接近程度节点的能力。在本文中，我们对GNNs在推荐中的表达能力进行了全面的理论分析。

    Graph Neural Networks (GNNs) have demonstrated superior performance on various graph learning tasks, including recommendation, where they leverage user-item collaborative filtering signals in graphs. However, theoretical formulations of their capability are scarce, despite their empirical effectiveness in state-of-the-art recommender models. Recently, research has explored the expressiveness of GNNs in general, demonstrating that message passing GNNs are at most as powerful as the Weisfeiler-Lehman test, and that GNNs combined with random node initialization are universal. Nevertheless, the concept of "expressiveness" for GNNs remains vaguely defined. Most existing works adopt the graph isomorphism test as the metric of expressiveness, but this graph-level task may not effectively assess a model's ability in recommendation, where the objective is to distinguish nodes of different closeness. In this paper, we provide a comprehensive theoretical analysis of the expressiveness of GNNs in 
    
[^67]: 使用CLIP进行随机词语数据增强的零样本异常检测方法

    Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection. (arXiv:2308.11119v1 [cs.CV])

    [http://arxiv.org/abs/2308.11119](http://arxiv.org/abs/2308.11119)

    本文提出了一种利用CLIP作为数据源的零样本异常检测方法，通过随机词语数据增强的方式改善了训练效率，并应用prompt-guided分类进行图像的检测。该方法克服了以往需为每个对象类别训练模型的低效问题，有潜力应用于工业领域。

    

    本文提出了一种新颖的方法，利用视觉-语言模型CLIP作为数据源进行零样本异常检测。由于潜在的工业应用，人们已经付出了大量的努力来开发异常检测器。考虑到获取各种异常样本以用于训练的困难，大多数现有方法仅用正常样本训练模型，并在推理过程中从正常样本的分布中测量其差异，这需要为每个对象类别训练一个模型。为了解决这种低效的训练需求，设计了一种基于CLIP的异常检测器，它以滑动窗口的方式对图像的每个部分应用prompt-guided分类。然而，该方法仍然受到以已知对象类别仔细组合提示的劳动力的影响。为了解决以上问题，我们提出利用CLIP作为训练的数据源。我们的方法使用CLI中的文本编码器生成文本嵌入。

    This paper presents a novel method that leverages a visual-language model, CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have been put towards developing anomaly detectors due to their potential industrial applications. Considering the difficulty in acquiring various anomalous samples for training, most existing methods train models with only normal samples and measure discrepancies from the distribution of normal samples during inference, which requires training a model for each object category. The problem of this inefficient training requirement has been tackled by designing a CLIP-based anomaly detector that applies prompt-guided classification to each part of an image in a sliding window manner. However, the method still suffers from the labor of careful prompt ensembling with known object categories. To overcome the issues above, we propose leveraging CLIP as a data source for training. Our method generates text embeddings with the text encoder in CLI
    
[^68]: 开发一种新型的量子预处理滤波器以提高神经网络模型的图像分类准确性

    Development of a Novel Quantum Pre-processing Filter to Improve Image Classification Accuracy of Neural Network Models. (arXiv:2308.11112v1 [quant-ph])

    [http://arxiv.org/abs/2308.11112](http://arxiv.org/abs/2308.11112)

    本文提出了一种新型的量子预处理滤波器（QPF），通过应用该方法可以提高神经网络模型的图像分类准确性，实验结果在MNIST和EMNIST数据集上分别取得了显著的提升，然而在GTSRB数据集上效果有所下降。

    

    本文提出了一种新型的量子预处理滤波器（QPF），以提高神经网络（NN）模型的图像分类准确性。在将数据传入全连接NN架构之前，应用了一个简单的四比特量子电路，该电路使用Y旋转门进行编码，并使用两个控制NOT门在比特之间创建相关性作为特征提取滤波器。通过应用QPF方法，结果显示基于MNIST（手写10位数字）和EMNIST（手写47类数字和字母）数据集的图像分类准确性可以提高，分别从92.5%提高到95.4%和从68.9%提高到75.9%。这些改进是在机器学习过程中没有引入额外的模型参数或优化的情况下获得的。然而，对于具有43个不同类别的真实交通标志图像的相对复杂的GTSRB数据集进行的测试显示出了分类准确性的降低。考虑到这一点，我们推测QPF方法对于复杂的图像数据可能不太适用。

    This paper proposes a novel quantum pre-processing filter (QPF) to improve the image classification accuracy of neural network (NN) models. A simple four qubit quantum circuit that uses Y rotation gates for encoding and two controlled NOT gates for creating correlation among the qubits is applied as a feature extraction filter prior to passing data into the fully connected NN architecture. By applying the QPF approach, the results show that the image classification accuracy based on the MNIST (handwritten 10 digits) and the EMNIST (handwritten 47 class digits and letters) datasets can be improved, from 92.5% to 95.4% and from 68.9% to 75.9%, respectively. These improvements were obtained without introducing extra model parameters or optimizations in the machine learning process. However, tests performed on the developed QPF approach against a relatively complex GTSRB dataset with 43 distinct class real-life traffic sign images showed a degradation in the classification accuracy. Consid
    
[^69]: CAME: 对比自动化模型评估

    CAME: Contrastive Automated Model Evaluation. (arXiv:2308.11111v1 [cs.CV])

    [http://arxiv.org/abs/2308.11111](http://arxiv.org/abs/2308.11111)

    CAME是一个不依赖训练集的对比自动化模型评估框架，通过理论分析和实证验证，建立了模型性能与对比损失之间的可预测关系，并取得了新的SOTA结果。

    

    自动化模型评估（AutoEval）框架探索了在没有标记的测试集的情况下评估训练好的机器学习模型的可能性。尽管有一些不错的结果和承诺，但现有的AutoEval方法主要依赖于计算未标记测试集与训练集之间的分布偏移。我们认为这种对训练集的依赖成为将这项技术应用于真实世界机器学习开发中的另一个障碍。在这项工作中，我们提出了对比自动模型评估（CAME）一个新的AutoEval框架，该框架不需要依赖训练集。CAME的核心思想基于理论分析，将模型性能与对比损失相联系。此外，通过大量的实证验证，我们成功建立了两者之间的可预测关系，只需在未标记/未见的测试集上进行推导。由此产生的框架CAME通过超越以前的工作建立了新的SOTA结果。

    The Automated Model Evaluation (AutoEval) framework entertains the possibility of evaluating a trained machine learning model without resorting to a labeled testing set. Despite the promise and some decent results, the existing AutoEval methods heavily rely on computing distribution shifts between the unlabelled testing set and the training set. We believe this reliance on the training set becomes another obstacle in shipping this technology to real-world ML development. In this work, we propose Contrastive Automatic Model Evaluation (CAME), a novel AutoEval framework that is rid of involving training set in the loop. The core idea of CAME bases on a theoretical analysis which bonds the model performance with a contrastive loss. Further, with extensive empirical validation, we manage to set up a predictable relationship between the two, simply by deducing on the unlabeled/unseen testing set. The resulting framework CAME establishes a new SOTA results for AutoEval by surpassing prior wo
    
[^70]: 大型语言模型的再识别能力：匿名面临风险吗？

    Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])

    [http://arxiv.org/abs/2308.11103](http://arxiv.org/abs/2308.11103)

    本研究评估了大型语言模型在重新识别匿名个人方面的能力，并发现模型大小、输入长度和指令调整是最重要的决定因素。

    

    在欧盟和瑞士，法院裁决中自然人和法人的匿名性是隐私保护的关键方面。随着大型语言模型（LLMs）的出现，对于匿名人员的大规模再识别的担忧日益增长。根据瑞士联邦最高法院的要求，我们通过使用来自瑞士联邦最高法院的实际法律数据构建了一个概念验证，来探讨LLMs重新识别法院裁决中个人的潜力。在最初的实验之后，我们构建了一个经过匿名化处理的维基百科数据集，作为一个更严格的测试场地来进一步研究研究结果。通过引入并应用文本中再识别人员的新任务，我们还引入了新的性能衡量指标。我们系统地分析了影响成功再识别的因素，确定模型大小、输入长度和指令调整是最重要的决定因素之一。尽管在匿名化处理后，LLMs在重新识别上的成功率很高，但在某些情况下仍然存在风险。

    Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
    
[^71]: 量子神经网络解释性与不可解释性的探索

    Explicability and Inexplicability in the Interpretation of Quantum Neural Networks. (arXiv:2308.11098v1 [quant-ph])

    [http://arxiv.org/abs/2308.11098](http://arxiv.org/abs/2308.11098)

    本文探索了量子神经网络的可解释性，引入了不可解释性带的概念，为理解如何构建负责任且可追究的量子人工智能模型迈出了一步。

    

    人工智能(AI)方法的可解释性，特别是深度神经网络的可解释性，引起了广泛关注，因为AI支持的系统往往具有不可解释的行为。解释这种模型的可解释性是构建可信系统的关键组成部分。许多方法用于解决这个问题，但它们不能明显地推广到量子环境中。在这里，我们使用量子和经典神经网络的局部模型无关解释性指标来探索量子神经网络的可解释性。我们引入了不可解释性带的概念，表示在该区域内的数据样本没有解释，很可能是不可避免的随机量子测量的受害者。我们将此视为理解如何构建负责任且可追究的量子人工智能模型的一步。

    Interpretability of artificial intelligence (AI) methods, particularly deep neural networks, is of great interest due to the widespread use of AI-backed systems, which often have unexplainable behavior. The interpretability of such models is a crucial component of building trusted systems. Many methods exist to approach this problem, but they do not obviously generalize to the quantum setting. Here we explore the interpretability of quantum neural networks using local model-agnostic interpretability measures of quantum and classical neural networks. We introduce the concept of the band of inexplicability, representing the interpretable region in which data samples have no explanation, likely victims of inherently random quantum measurements. We see this as a step toward understanding how to build responsible and accountable quantum AI models.
    
[^72]: Video OWL-ViT: 视频中具有时间一致性的开放世界定位

    Video OWL-ViT: Temporally-consistent open-world localization in video. (arXiv:2308.11093v1 [cs.CV])

    [http://arxiv.org/abs/2308.11093](http://arxiv.org/abs/2308.11093)

    本论文提出了Video OWL-ViT模型，将预训练的开放世界图像模型应用于视频定位任务，通过添加变换器解码器实现时间上的连续传播，相比于传统的跟踪-by-detection方法具有更好的时间一致性。

    

    我们提出了一种架构和训练方案，将预训练的开放世界图像模型应用于视频定位。理解开放的视觉世界（不受固定标签空间的限制）对于许多真实世界的视觉任务至关重要。在大型图像-文本数据集上进行对比预训练最近在图像级任务中取得了显著的改进。对于涉及对象定位的更结构化任务，应用预训练模型更具挑战性。对于视频任务来说尤其如此，因为任务特定的数据是有限的。我们通过在OWL-ViT开放词汇检测模型的基础上构建，并通过添加一个变换器解码器将其适应为视频，展示了开放世界模型的成功转移。解码器通过使用一个帧的输出标记作为下一帧的对象查询，以时间上的连续方式传播对象表示。我们的模型可以对视频数据进行端到端的训练，并且相比通过检测进行跟踪的方法，具有更好的时间一致性。

    We present an architecture and a training recipe that adapts pre-trained open-world image models to localization in videos. Understanding the open visual world (without being constrained by fixed label spaces) is crucial for many real-world vision tasks. Contrastive pre-training on large image-text datasets has recently led to significant improvements for image-level tasks. For more structured tasks involving object localization applying pre-trained models is more challenging. This is particularly true for video tasks, where task-specific data is limited. We show successful transfer of open-world models by building on the OWL-ViT open-vocabulary detection model and adapting it to video by adding a transformer decoder. The decoder propagates object representations recurrently through time by using the output tokens for one frame as the object queries for the next. Our model is end-to-end trainable on video data and enjoys improved temporal consistency compared to tracking-by-detection b
    
[^73]: 使用最优输运解决图像分类中的公平性和可解释性问题

    Addressing Fairness and Explainability in Image Classification Using Optimal Transport. (arXiv:2308.11090v1 [cs.CV])

    [http://arxiv.org/abs/2308.11090](http://arxiv.org/abs/2308.11090)

    本论文提出了使用最优输运理论解决图像分类中公平性和可解释性问题的综合方法，通过发现和解释模型中偏见区域的原因和影响来提供细粒度的解释。

    

    在诸如医疗保健和执法等领域中，算法公平性和对潜在不公平结果的解释是建立人工智能系统信任和问责性的关键。尽管在每个领域分别取得了重大进展，但在使用深度神经网络的领域中实现公平性应用的可解释性仍然具有挑战性。与此同时，伦理数据挖掘变得越来越重要，因为多次研究表明不关注公平性的算法会导致有偏差的结果。目前的方法主要关注在模型的输出中减轻偏见，但很少有尝试解释模型为什么存在偏见。为了弥补这一差距，我们提出了一种综合方法，利用最优输运理论来发现图像中有偏差区域的原因和影响，这种方法也可以轻松地扩展到表格数据。通过使用Wasserstein barycenters，我们可以以细粒度的方式解释模型的偏见。

    Algorithmic Fairness and the explainability of potentially unfair outcomes are crucial for establishing trust and accountability of Artificial Intelligence systems in domains such as healthcare and policing. Though significant advances have been made in each of the fields separately, achieving explainability in fairness applications remains challenging, particularly so in domains where deep neural networks are used. At the same time, ethical data-mining has become ever more relevant, as it has been shown countless times that fairness-unaware algorithms result in biased outcomes. Current approaches focus on mitigating biases in the outcomes of the model, but few attempts have been made to try to explain \emph{why} a model is biased. To bridge this gap, we propose a comprehensive approach that leverages optimal transport theory to uncover the causes and implications of biased regions in images, which easily extends to tabular data as well. Through the use of Wasserstein barycenters, we o
    
[^74]: 张量基神经网络的应力表示：Finger-Rivlin-Ericksen的替代公式

    Stress representations for tensor basis neural networks: alternative formulations to Finger-Rivlin-Ericksen. (arXiv:2308.11080v1 [cond-mat.soft])

    [http://arxiv.org/abs/2308.11080](http://arxiv.org/abs/2308.11080)

    本研究通过探索一系列张量基神经网络模型，研究不同的应力表示方法，以替代传统的Finger-Rivlin-Ericksen形式。通过比较基于势能和基于系数的方法，以及差异，进一步拓展了在有限变形情况下建模超弹性材料的可能性。

    

    基于神经网络和经典表示定理的数据驱动本构模型框架近年来引起了广泛关注，因为它们能够轻松地融入本构约束条件，并具有出色的泛化性能。在这些模型中，应力预测是基于不变性相关系数函数和已知张量基生成器的线性组合。然而，迄今为止，该公式仅限于基于经典Rivlin和Ericksen形式的应力表示，而其他替代公式的性能尚未得到研究。在本工作中，我们调查了一系列在有限变形情况下用于建模超弹性材料的张量基神经网络模型，包括一些迄今未探索的使用理论上等价的不变量和生成器的公式，以取代Finger-Rivlin-Ericksen。此外，我们比较了基于势能和基于系数的方法，以及差异。

    Data-driven constitutive modeling frameworks based on neural networks and classical representation theorems have recently gained considerable attention due to their ability to easily incorporate constitutive constraints and their excellent generalization performance. In these models, the stress prediction follows from a linear combination of invariant-dependent coefficient functions and known tensor basis generators. However, thus far the formulations have been limited to stress representations based on the classical Rivlin and Ericksen form, while the performance of alternative representations has yet to be investigated. In this work, we survey a variety of tensor basis neural network models for modeling hyperelastic materials in a finite deformation context, including a number of so far unexplored formulations which use theoretically equivalent invariants and generators to Finger-Rivlin-Ericksen. Furthermore, we compare potential-based and coefficient-based approaches, as well as dif
    
[^75]: 使用稳健的视频预测器进行自然视频序列的长期预测

    Long-Term Prediction of Natural Video Sequences with Robust Video Predictors. (arXiv:2308.11079v1 [cs.CV])

    [http://arxiv.org/abs/2308.11079](http://arxiv.org/abs/2308.11079)

    这篇论文提出了一种稳健的视频预测器（RoViPs），结合了深层感知和基于不确定性的重建损失，并利用基于注意力的跳过连接来提高预测性能。通过使预测器对自身的预测误差具有鲁棒性，能够产生非常长的、逼真的自然视频序列。

    

    预测高维视频序列是一个异常困难的问题。由于不确定性，给定视频序列的可能未来数量随着时间的推移呈指数增长。当试图从有限的世界快照中预测复杂的自然视频场景时，这一点尤为明显。固有的不确定性随着预测进入未来而累积，使得长期预测变得非常困难。在这项工作中，我们引入了一些对现有工作的改进，有助于创建稳健的视频预测器（RoViPs）。我们展示出，通过深层感知和基于不确定性的重建损失的组合，我们能够创建高质量的短期预测。利用基于注意力的跳过连接，可以实现输入特征的长程空间移动，进一步提高性能。最后，我们展示，通过使预测器对自身的预测误差具有鲁棒性，可以产生非常长的、逼真的自然视频序列。

    Predicting high dimensional video sequences is a curiously difficult problem. The number of possible futures for a given video sequence grows exponentially over time due to uncertainty. This is especially evident when trying to predict complicated natural video scenes from a limited snapshot of the world. The inherent uncertainty accumulates the further into the future you predict making long-term prediction very difficult. In this work we introduce a number of improvements to existing work that aid in creating Robust Video Predictors (RoViPs). We show that with a combination of deep Perceptual and uncertainty-based reconstruction losses we are able to create high quality short-term predictions. Attention-based skip connections are utilised to allow for long range spatial movement of input features to further improve performance. Finally, we show that by simply making the predictor robust to its own prediction errors, it is possible to produce very long, realistic natural video sequenc
    
[^76]: 关于重整化群和深度学习在伊辛模型中的联系的深入探讨

    A Deep Dive into the Connections Between the Renormalization Group and Deep Learning in the Ising Model. (arXiv:2308.11075v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2308.11075](http://arxiv.org/abs/2308.11075)

    本研究探讨了重整化群和深度学习在伊辛模型中的联系，并通过开发新的重整化技术得到了有关群流的新发现。

    

    重整化群是统计物理和量子场论中的一种重要技术，考虑物理理论的尺度不变性和参数在尺度变换下的变化。深度学习是一种强大的计算技术，利用多层神经网络解决各种复杂问题。先前的研究表明，无监督深度学习可能是一种重整化群流的形式，通过逐层粗粒化原始数据。我们以2D最近邻伊辛模型的卡丹诺夫块重整化为简单示例，通过受限玻尔兹曼机实现深度学习，对这种联系进行了更严格的基础上研究。我们为1D和2D伊辛模型开发了广泛的重整化技术，以提供基准进行比较。对于1D伊辛模型，我们成功地利用Adam优化和相关长度损失函数学习了群流yi

    The renormalization group (RG) is an essential technique in statistical physics and quantum field theory, which considers scale-invariant properties of physical theories and how these theories' parameters change with scaling. Deep learning is a powerful computational technique that uses multi-layered neural networks to solve a myriad of complicated problems. Previous research suggests the possibility that unsupervised deep learning may be a form of RG flow, by being a layer-by-layer coarse graining of the original data. We examined this connection on a more rigorous basis for the simple example of Kadanoff block renormalization of the 2D nearest-neighbor Ising model, with our deep learning accomplished via Restricted Boltzmann Machines (RBMs). We developed extensive renormalization techniques for the 1D and 2D Ising model to provide a baseline for comparison. For the 1D Ising model, we successfully used Adam optimization on a correlation length loss function to learn the group flow, yi
    
[^77]: 嵌套的多智能体推理的神经摊销推断

    Neural Amortized Inference for Nested Multi-agent Reasoning. (arXiv:2308.11071v1 [cs.AI])

    [http://arxiv.org/abs/2308.11071](http://arxiv.org/abs/2308.11071)

    本研究引入了一种新的方法，利用神经网络对高阶社会推理进行摊销，从而加快嵌套多智能体推理的计算速度，实验结果表明该方法在计算效率上表现出色，同时准确性降低最小化。

    

    多智能体之间的相互作用，如沟通、教学和虚张声势，通常依赖于高阶的社会推理，即理解他人如何推断自己。这种复杂的推理可以通过嵌套式多智能体推理来有效建模。然而，随着每个推理级别的增加，计算复杂性呈指数级增长，提出了重大挑战。然而，人类在日常生活中轻松地执行复杂的社会推理。为了弥合人类推理能力和计算限制之间的差距，我们提出了一种新的方法：利用神经网络对高阶社会推理进行摊销，从而加快嵌套多智能体推理。我们在两个具有挑战性的多智能体交互领域对我们的方法进行评估。实验结果表明，我们的方法在计算效率上表现出色，同时准确性降低最小化。

    Multi-agent interactions, such as communication, teaching, and bluffing, often rely on higher-order social inference, i.e., understanding how others infer oneself. Such intricate reasoning can be effectively modeled through nested multi-agent reasoning. Nonetheless, the computational complexity escalates exponentially with each level of reasoning, posing a significant challenge. However, humans effortlessly perform complex social inferences as part of their daily lives. To bridge the gap between human-like inference capabilities and computational limitations, we propose a novel approach: leveraging neural networks to amortize high-order social inference, thereby expediting nested multi-agent reasoning. We evaluate our method in two challenging multi-agent interaction domains. The experimental results demonstrate that our method is computationally efficient while exhibiting minimal degradation in accuracy.
    
[^78]: 基于拓扑结构的图信号压缩

    Topological Graph Signal Compression. (arXiv:2308.11068v1 [cs.LG])

    [http://arxiv.org/abs/2308.11068](http://arxiv.org/abs/2308.11068)

    这项研究提出了一种基于拓扑结构的图信号压缩方法，通过处理高阶交互、聚类和消息传递等步骤，相比于传统方法在压缩信号时具有更好的重建误差，能够更好地捕捉和利用空间和时间特征。

    

    最近出现的拓扑深度学习（TDL）方法旨在通过自然地处理高阶交互，超越由图表示定义的成对关系和局部邻域，从而扩展当前的图神经网络（GNN）。在本文中，我们提出了一种基于TDL的图信号压缩方法，包括两个主要步骤：首先，基于原始信号推断出不相交的高阶结构，通过将N个数据点聚类成K个集合；然后，基于拓扑启示的消息传递在这些多元素集合中获得信号的压缩表示。我们的结果表明，我们的框架在压缩来自两个真实的互联网服务提供商网络数据集的时间链路信号时，比标准的GNN和前馈架构具有更好的重建误差——在所有评估场景中，重建误差提高了从30%到90%。这表明它更好地捕捉和利用了空间和时间特征。

    Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and tempor
    
[^79]: UnLoc：用于视频定位任务的统一框架

    UnLoc: A Unified Framework for Video Localization Tasks. (arXiv:2308.11062v1 [cs.CV])

    [http://arxiv.org/abs/2308.11062](http://arxiv.org/abs/2308.11062)

    UnLoc是一个用于视频定位任务的统一框架，通过使用预训练的图像和文本模型以及视频-文本融合模型，实现了Moment Retrieval、Temporal Localization和Action Segmentation三个定位任务的最先进结果。

    

    虽然像CLIP这样的大规模图像-文本预训练模型已经被用于修剪视频上的多个视频级任务，但它们在未修剪视频的时间定位方面仍然是一个相对未被探索的任务。我们设计了一种称为UnLoc的新方法，它使用预训练的图像和文本模型，并将令牌提供给视频-文本融合模型。融合模块的输出然后用于构建一个特征金字塔，其中每个级别都连接到一个头部以预测每帧的相关性得分和开始/结束时间的偏移。与以前的工作不同，我们的架构使用一个单一阶段模型实现了Moment Retrieval、Temporal Localization和Action Segmentation，而无需行动建议、基于动作的预训练特征或表示掩码。与专门的模型不同，我们在所有三个不同的定位任务上都取得了最先进的结果，采用了统一的方法。代码可在\url{https://github.com/google-research/scenic}上找到。

    While large-scale image-text pretrained models such as CLIP have been used for multiple video-level tasks on trimmed videos, their use for temporal localization in untrimmed videos is still a relatively unexplored task. We design a new approach for this called UnLoc, which uses pretrained image and text towers, and feeds tokens to a video-text fusion model. The output of the fusion module are then used to construct a feature pyramid in which each level connects to a head to predict a per-frame relevancy score and start/end time displacements. Unlike previous works, our architecture enables Moment Retrieval, Temporal Localization, and Action Segmentation with a single stage model, without the need for action proposals, motion based pretrained features or representation masking. Unlike specialized models, we achieve state of the art results on all three different localization tasks with a unified approach. Code will be available at: \url{https://github.com/google-research/scenic}.
    
[^80]: 联合回声消除和噪声抑制的超级双路径压缩

    Ultra Dual-Path Compression For Joint Echo Cancellation And Noise Suppression. (arXiv:2308.11053v1 [eess.AS])

    [http://arxiv.org/abs/2308.11053](http://arxiv.org/abs/2308.11053)

    本文提出了一种超级双路径压缩方法用于联合回声消除和噪声抑制，通过时间频率双路径压缩实现广泛的压缩比，同时降低计算成本，且在固定压缩比下能够进一步改善性能。

    

    回声消除和噪声抑制对于全双工通信至关重要，然而大多数现有的神经网络在计算成本高和模型复杂度调整上不灵活。在本文中，我们引入时间频率双路径压缩，实现广泛的压缩比，同时降低计算成本。具体来说，在频率压缩方面，使用可训练滤波器代替手动设计的滤波器进行维度缩减。在时间压缩方面，仅使用帧跳预测会导致性能大幅下降，但通过具有完整序列建模的后处理网络可以缓解这个问题。我们发现，在固定压缩比的情况下，同时使用时间和频率方法进行双路径压缩将进一步改善性能，并且几乎不改变模型大小，压缩比覆盖范围从4x到32x。此外，所提出的模型与快速FullSubNet和DeepFilterNet相比具有竞争力的性能。可以访问演示页面。

    Echo cancellation and noise reduction are essential for full-duplex communication, yet most existing neural networks have high computational costs and are inflexible in tuning model complexity. In this paper, we introduce time-frequency dual-path compression to achieve a wide range of compression ratios on computational cost. Specifically, for frequency compression, trainable filters are used to replace manually designed filters for dimension reduction. For time compression, only using frame skipped prediction causes large performance degradation, which can be alleviated by a post-processing network with full sequence modeling. We have found that under fixed compression ratios, dual-path compression combining both the time and frequency methods will give further performance improvement, covering compression ratios from 4x to 32x with little model size change. Moreover, the proposed models show competitive performance compared with fast FullSubNet and DeepFilterNet. A demo page can be f
    
[^81]: 跨影像地点的协调（HAIL）：用于脑部MRI的单次学习

    Harmonization Across Imaging Locations(HAIL): One-Shot Learning for Brain MRI. (arXiv:2308.11047v1 [eess.IV])

    [http://arxiv.org/abs/2308.11047](http://arxiv.org/abs/2308.11047)

    这项研究提出了一种用于脑部MRI的单次学习方法，通过神经风格迁移进行图像协调，避免了生成对抗网络中的幻觉现象。该方法在收集来自不同临床地点的医学成像数据时可以提高协调的效果。

    

    对于基于机器学习的罕见疾病（如儿童脑肿瘤）的预后和诊断，需要收集来自可能使用不同设备和协议的多个临床地点的医学成像数据。基于深度学习的放射学图像协调依赖于生成对抗网络（GANs）。然而，GANs会生成原始训练数据中不存在的伪结构，这种现象被称为“幻觉”。为了防止医学成像（如脑部的磁共振图像）中的幻觉，我们提出了一种单次学习方法，其中我们利用神经风格迁移进行协调。在测试时间，该方法使用临床地点的一张图像生成与协作地点的强度比例匹配的图像。我们的方法结合了学习特征提取器、神经风格迁移和自适应实例归一化。我们进一步提出了一种评估图像协调效果的新策略。

    For machine learning-based prognosis and diagnosis of rare diseases, such as pediatric brain tumors, it is necessary to gather medical imaging data from multiple clinical sites that may use different devices and protocols. Deep learning-driven harmonization of radiologic images relies on generative adversarial networks (GANs). However, GANs notoriously generate pseudo structures that do not exist in the original training data, a phenomenon known as "hallucination". To prevent hallucination in medical imaging, such as magnetic resonance images (MRI) of the brain, we propose a one-shot learning method where we utilize neural style transfer for harmonization. At test time, the method uses one image from a clinical site to generate an image that matches the intensity scale of the collaborating sites. Our approach combines learning a feature extractor, neural style transfer, and adaptive instance normalization. We further propose a novel strategy to evaluate the effectiveness of image harmo
    
[^82]: 错误的相关性及其发现方法

    Spurious Correlations and Where to Find Them. (arXiv:2308.11043v1 [cs.LG])

    [http://arxiv.org/abs/2308.11043](http://arxiv.org/abs/2308.11043)

    本论文研究了错误相关性的发生原因以及其对标准ERM基线的影响，并观察到了这些原因与模型设计选择之间的模式。

    

    错误的相关性指的是模型从数据中学习到不可靠的特征，是数据驱动学习的一个已知缺陷。尽管已经有几种算法提出来减轻这个问题，但我们还没有能够共同推导出错误相关性的指标。因此，基于独立假设构建的解决方案无法击败简单的ERM基线。我们收集了一些常见的错误相关性出现背后的假设，并使用从因果图生成的合成数据集研究它们对标准ERM基线的影响。随后，我们观察到这些假设和模型设计选择之间的模式。

    Spurious correlations occur when a model learns unreliable features from the data and are a well-known drawback of data-driven learning. Although there are several algorithms proposed to mitigate it, we are yet to jointly derive the indicators of spurious correlations. As a result, the solutions built upon standalone hypotheses fail to beat simple ERM baselines. We collect some of the commonly studied hypotheses behind the occurrence of spurious correlations and investigate their influence on standard ERM baselines using synthetic datasets generated from causal graphs. Subsequently, we observe patterns connecting these hypotheses and model design choices.
    
[^83]: 物流集散地位置优化：一种基于K-Means和P-Median模型的混合方法，利用道路网络距离

    Logistics Hub Location Optimization: A K-Means and P-Median Model Hybrid Approach Using Road Network Distances. (arXiv:2308.11038v1 [math.OC])

    [http://arxiv.org/abs/2308.11038](http://arxiv.org/abs/2308.11038)

    本研究基于K-Means和P-Median模型提出了一种混合方法，通过使用道路网络距离来优化在城市环境下物流集散地的位置布置，以减少配送距离和碳足迹。

    

    物流集散地在最后一公里配送距离中起着关键作用；即使距离微小增加也会对电子商务行业的业务产生负面影响，同时还会增加其碳足迹。特别是在Covid-19之后，该行业的增长进一步加剧了在城市环境中优化资源分配的需求。在这项研究中，我们使用了一种混合方法来优化物流集散地的布置。该方法依次采用不同的技术。首先，根据它们的空间位置，使用K-Means对交付点进行聚类。聚类方法使用道路网络距离，而不是欧几里德距离。避免使用非基于道路网络的方法，因为它们会导致错误和误导性结果。最后，使用P-Median方法确定集散地的位置。P-Median方法还将交付数量和人口作为权重考虑在内。使用Muller和Phipps（M＆P）的实际交付数据

    Logistic hubs play a pivotal role in the last-mile delivery distance; even a slight increment in distance negatively impacts the business of the e-commerce industry while also increasing its carbon footprint. The growth of this industry, particularly after Covid-19, has further intensified the need for optimized allocation of resources in an urban environment. In this study, we use a hybrid approach to optimize the placement of logistic hubs. The approach sequentially employs different techniques. Initially, delivery points are clustered using K-Means in relation to their spatial locations. The clustering method utilizes road network distances as opposed to Euclidean distances. Non-road network-based approaches have been avoided since they lead to erroneous and misleading results. Finally, hubs are located using the P-Median method. The P-Median method also incorporates the number of deliveries and population as weights. Real-world delivery data from Muller and Phipps (M&P) is used to 
    
[^84]: RBA-GCN: 关系双层聚合图卷积网络用于情感识别

    RBA-GCN: Relational Bilevel Aggregation Graph Convolutional Network for Emotion Recognition. (arXiv:2308.11029v1 [cs.AI])

    [http://arxiv.org/abs/2308.11029](http://arxiv.org/abs/2308.11029)

    提出了RBA-GCN模型用于情感识别。该模型通过引入关系双层聚合和图生成模块，解决了GCN模型中的节点信息冗余和远距离上下文信息捕获问题。

    

    情感识别在对话中的应用受到了研究人员的关注，由于它具有广泛的应用。由于对话具有自然的图结构，很多基于图卷积网络（GCNs）的ERC模型方法取得了显著的结果。然而，传统GCNs的聚合方法存在节点信息冗余问题，导致节点辨别信息的丢失。此外，单层GCNs缺乏从图中捕获远距离上下文信息的能力。此外，大多数方法都是基于文本模态或将不同模态拼接在一起，导致捕捉模态间交互能力弱。为了解决这些问题，我们提出了关系双层聚合图卷积网络（RBA-GCN），它由三个模块组成：图生成模块（GGM）、基于相似性的簇构建模块（SCBM）和双层聚合模块。

    Emotion recognition in conversation (ERC) has received increasing attention from researchers due to its wide range of applications. As conversation has a natural graph structure, numerous approaches used to model ERC based on graph convolutional networks (GCNs) have yielded significant results. However, the aggregation approach of traditional GCNs suffers from the node information redundancy problem, leading to node discriminant information loss. Additionally, single-layer GCNs lack the capacity to capture long-range contextual information from the graph. Furthermore, the majority of approaches are based on textual modality or stitching together different modalities, resulting in a weak ability to capture interactions between modalities. To address these problems, we present the relational bilevel aggregation graph convolutional network (RBA-GCN), which consists of three modules: the graph generation module (GGM), similarity-based cluster building module (SCBM) and bilevel aggregation 
    
[^85]: 健康信息学中分布式协作训练深度学习模型的拆分学习

    Split Learning for Distributed Collaborative Training of Deep Learning Models in Health Informatics. (arXiv:2308.11027v1 [cs.LG])

    [http://arxiv.org/abs/2308.11027](http://arxiv.org/abs/2308.11027)

    拆分学习可以实现在不同的、私有维护的健康数据集之间协作训练深度学习模型，同时保护隐私。在医疗相关任务上，通过拆分学习训练的模型能够实现与中央和联邦学习模型相似的性能，提高了计算效率。

    

    深度学习在医疗预测任务中展现出了巨大的潜力，然而在不同医疗机构之间实现泛化的深度学习模型仍然具有挑战性。这部分是由于这些机构的孤立性质和患者隐私要求所致。为了解决这个问题，我们展示了如何利用拆分学习来实现深度学习模型的协作训练，跨越不同的、私有维护的健康数据集，同时保护原始记录和模型参数的隐私。我们引入了一种新的隐私保护分布式学习框架，相比传统的联邦学习，它提供了更高的隐私保护水平。我们使用了多个生物医学影像和电子健康记录 (EHR) 数据集，证明了通过拆分学习训练的深度学习模型能够达到与中央和联邦学习模型相似的性能，同时极大地提高了计算效率。

    Deep learning continues to rapidly evolve and is now demonstrating remarkable potential for numerous medical prediction tasks. However, realizing deep learning models that generalize across healthcare organizations is challenging. This is due, in part, to the inherent siloed nature of these organizations and patient privacy requirements. To address this problem, we illustrate how split learning can enable collaborative training of deep learning models across disparate and privately maintained health datasets, while keeping the original records and model parameters private. We introduce a new privacy-preserving distributed learning framework that offers a higher level of privacy compared to conventional federated learning. We use several biomedical imaging and electronic health record (EHR) datasets to show that deep learning models trained via split learning can achieve highly similar performance to their centralized and federated counterparts while greatly improving computational effi
    
[^86]: 基于隐式反馈和有限患者元数据的专科医生推荐的极限多标签分类

    Extreme Multilabel Classification for Specialist Doctor Recommendation with Implicit Feedback and Limited Patient Metadata. (arXiv:2308.11022v1 [cs.LG])

    [http://arxiv.org/abs/2308.11022](http://arxiv.org/abs/2308.11022)

    本论文研究了基于隐式反馈和有限患者元数据的专科医生推荐的极限多标签分类问题。通过将传统的推荐设置转换为多标签分类问题，我们提出了一个统一模型，利用患者历史来实现更好的推荐性能。

    

    推荐系统通常用于解决医生推荐的问题。然而，在现实场景中，患者反馈和医疗记录可能并不总是可用的。我们的研究聚焦于医生推荐并旨在预测新患者和有过咨询历史的患者在不同专科医生中的推荐。我们使用了常用于文本分类任务的极限多标签分类（XML）方法来编码可用的特征并探索不同的情景。虽然它在推荐任务中的潜力经常被提及，但在文献中并没有得到深入研究。受到医生推荐案例的启发，我们展示了如何将传统的推荐设置转化为当前XML方法可解决的多标签分类问题。此外，我们提出了一个统一模型，利用不同专科医生的患者历史。与现有最先进的推荐系统相比，我们的模型考虑了更多的因素，有更好的性能。

    Recommendation Systems (RS) are often used to address the issue of medical doctor referrals. However, these systems require access to patient feedback and medical records, which may not always be available in real-world scenarios. Our research focuses on medical referrals and aims to predict recommendations in different specialties of physicians for both new patients and those with a consultation history. We use Extreme Multilabel Classification (XML), commonly employed in text-based classification tasks, to encode available features and explore different scenarios. While its potential for recommendation tasks has often been suggested, this has not been thoroughly explored in the literature. Motivated by the doctor referral case, we show how to recast a traditional recommender setting into a multilabel classification problem that current XML methods can solve. Further, we propose a unified model leveraging patient history across different specialties. Compared to state-of-the-art RS us
    
[^87]: 多任务超图用于利用地球观测进行半监督学习

    Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations. (arXiv:2308.11021v1 [cs.CV])

    [http://arxiv.org/abs/2308.11021](http://arxiv.org/abs/2308.11021)

    本文介绍了使用多任务超图进行半监督学习的方法，并将其应用于地球观测问题。通过形成集成教师并生成可靠伪标签，我们的方法在NASA NEO数据集上表现出相对强基线和最新工作的改进。此外，我们展示了超图可以自适应地无监督地适应逐渐的数据分布变化。

    

    世界有许多不同的解释方式，它们之间高度相互依赖。我们利用这种复杂的依赖关系，引入了一个强大的多任务超图，其中每个节点是一个任务，通过超图中到达给定任务的不同路径形成无监督学习的教师，通过形成学习可靠伪标签的集成。每个超边是给定任务的一个集成教师的一部分，并且还是自监督超图系统的学生。我们将我们的模型应用于时下最重要的问题之一地球观测，这是一个高度多任务的问题，往往存在缺少地面真实数据的情况。通过在NASA NEO数据集上进行了为期22年的广泛实验证明了我们的多任务半监督方法的价值，通过相对强基线和最新工作的一贯改进。我们还展示了超图可以自适应地无监督地适应逐渐的数据分布变化。

    There are many ways of interpreting the world and they are highly interdependent. We exploit such complex dependencies and introduce a powerful multi-task hypergraph, in which every node is a task and different paths through the hypergraph reaching a given task become unsupervised teachers, by forming ensembles that learn to generate reliable pseudolabels for that task. Each hyperedge is part of an ensemble teacher for a given task and it is also a student of the self-supervised hypergraph system. We apply our model to one of the most important problems of our times, that of Earth Observation, which is highly multi-task and it often suffers from missing ground-truth data. By performing extensive experiments on the NASA NEO Dataset, spanning a period of 22 years, we demonstrate the value of our multi-task semi-supervised approach, by consistent improvements over strong baselines and recent work. We also show that the hypergraph can adapt unsupervised to gradual data distribution shifts 
    
[^88]: 实例化学习与原型缩减在实时比例肌肉控制中的应用：一个随机用户研究，展示保持准确性的数据缩减对植入系统的影响

    Instance-based Learning with Prototype Reduction for Real-Time Proportional Myocontrol: A Randomized User Study Demonstrating Accuracy-preserving Data Reduction for Prosthetic Embedded Systems. (arXiv:2308.11019v1 [cs.HC])

    [http://arxiv.org/abs/2308.11019](http://arxiv.org/abs/2308.11019)

    本研究提出了一种基于kNN的学习技术，用于肢体假肢控制中的手势检测。通过数据集缩减方法，通过实验验证了准确性和实时要求的可靠集成。决策面映射（DSM）在减少数据集时表现出良好的潜力。

    

    本文介绍了一种基于kNN方法的手势检测学习技术的设计、实现和验证。为了应对实时决策中高计算要求的问题，评估了数据集缩减方法，考虑到实时确定性，以便可靠地集成到电池供电的便携设备中。分析了参数化和不同比例方案的影响，利用八通道sEMG臂带。除了离线交叉验证准确率外，还确定了实时飞行实验（在线目标实现测试）的成功率。基于对嵌入式控制应用的特定数据集缩减技术在准确性和时间行为方面的评估，决策面映射（DSM）在应用于缩减集上的kNN中表现出良好的潜力。进行了一项随机、双盲的用户研究，评估了相应的方法（kNN和具有DSM缩减的kNN）。

    This work presents the design, implementation and validation of learning techniques based on the kNN scheme for gesture detection in prosthetic control. To cope with high computational demands in instance-based prediction, methods of dataset reduction are evaluated considering real-time determinism to allow for the reliable integration into battery-powered portable devices. The influence of parameterization and varying proportionality schemes is analyzed, utilizing an eight-channel-sEMG armband. Besides offline cross-validation accuracy, success rates in real-time pilot experiments (online target achievement tests) are determined. Based on the assessment of specific dataset reduction techniques' adequacy for embedded control applications regarding accuracy and timing behaviour, Decision Surface Mapping (DSM) proves itself promising when applying kNN on the reduced set. A randomized, double-blind user study was conducted to evaluate the respective methods (kNN and kNN with DSM-reduction
    
[^89]: 个性化的电子健康记录事件预测

    Personalized Event Prediction for Electronic Health Records. (arXiv:2308.11013v1 [cs.LG])

    [http://arxiv.org/abs/2308.11013](http://arxiv.org/abs/2308.11013)

    该论文研究了个性化的电子健康记录事件预测，提出了多个新的预测模型和方法以更好地适应个体差异。

    

    临床事件序列包含数百个临床事件，代表了患者在不同时间接受照护的记录。开发准确的预测模型对于支持各种模型来解释/分类当前患者状况或预测不良临床事件和结果非常重要，所有这些都旨在提高患者护理水平。学习临床序列的预测模型的一个重要挑战是它们的个体差异性。根据潜在的临床条件，每个患者的序列可能包含不同的临床事件集（观察，实验室结果，药物，程序）。因此，仅基于许多不同患者的事件序列学习的简单群体范围模型可能无法准确预测患者特定的事件序列动态和差异。为了解决这个问题，我们提出并研究了多个新的事件序列预测模型和方法，可以更好地调整预测结果。

    Clinical event sequences consist of hundreds of clinical events that represent records of patient care in time. Developing accurate predictive models of such sequences is of a great importance for supporting a variety of models for interpreting/classifying the current patient condition, or predicting adverse clinical events and outcomes, all aimed to improve patient care. One important challenge of learning predictive models of clinical sequences is their patient-specific variability. Based on underlying clinical conditions, each patient's sequence may consist of different sets of clinical events (observations, lab results, medications, procedures). Hence, simple population-wide models learned from event sequences for many different patients may not accurately predict patient-specific dynamics of event sequences and their differences. To address the problem, we propose and investigate multiple new event sequence prediction models and methods that let us better adjust the prediction for
    
[^90]: 在隐式数学简答题的自动评估中使用语言模型的方法

    Using language models in the implicit automated assessment of mathematical short answer items. (arXiv:2308.11006v1 [cs.CL])

    [http://arxiv.org/abs/2308.11006](http://arxiv.org/abs/2308.11006)

    这项研究提出了一种使用语言模型的新方法来评估数学简答题的正确性和误解，并通过值识别流程提供针对性反馈。

    

    我们提出了一种评估特定数学简答题构造回答的新方法。我们的方法使用一个流程来识别学生回答中指定的关键值。这使得我们可以确定回答的正确性，并识别任何误解。来自值识别流程的信息可以用来向教师和学生提供反馈。值识别流程由两个经过调优的语言模型组成。第一个模型判断一个值是否隐含在学生回答中。第二个模型识别关键值在回答中的位置。我们考虑了一个通用模型，可以用于任何提示和值，以及每个提示和值特定的模型。值识别流程比传统的基于评分标准的评估更准确和有信息量。它可以用来向学生提供更有针对性的反馈。

    We propose a new way to assess certain short constructed responses to mathematics items. Our approach uses a pipeline that identifies the key values specified by the student in their response. This allows us to determine the correctness of the response, as well as identify any misconceptions. The information from the value identification pipeline can then be used to provide feedback to the teacher and student. The value identification pipeline consists of two fine-tuned language models. The first model determines if a value is implicit in the student response. The second model identifies where in the response the key value is specified. We consider both a generic model that can be used for any prompt and value, as well as models that are specific to each prompt and value. The value identification pipeline is a more accurate and informative way to assess short constructed responses than traditional rubric-based scoring. It can be used to provide more targeted feedback to students, which
    
[^91]: 使用深度学习在多光谱卫星数据中自动检测甲烷排放

    Autonomous Detection of Methane Emissions in Multispectral Satellite Data Using Deep Learning. (arXiv:2308.11003v1 [physics.geo-ph])

    [http://arxiv.org/abs/2308.11003](http://arxiv.org/abs/2308.11003)

    本研究利用深度学习方法的图像识别能力，实现了对多光谱卫星数据中甲烷泄漏的自动检测。

    

    甲烷是最强效的温室气体之一，其短暂的大气半衰期使其成为迅速抑制全球变暖的主要目标。然而，目前的甲烷排放监测技术主要依赖于近似的排放因子或自我报告，这些方法往往严重低估了排放量。虽然初始设计用于监测地表特性，但卫星多光谱数据最近已成为分析大气内容的强大方法。然而，多光谱仪器的光谱分辨率较低，甲烷测量通常非常嘈杂。甲烷数据产品还对地表和其他大气气体（尤其是水蒸气）的吸收非常敏感，因此提供了嘈杂的潜在甲烷云图，通常需要大量人工分析。在这里，我们展示了深度学习方法的图像识别能力可以用于自动检测甲烷泄漏。

    Methane is one of the most potent greenhouse gases, and its short atmospheric half-life makes it a prime target to rapidly curb global warming. However, current methane emission monitoring techniques primarily rely on approximate emission factors or self-reporting, which have been shown to often dramatically underestimate emissions. Although initially designed to monitor surface properties, satellite multispectral data has recently emerged as a powerful method to analyze atmospheric content. However, the spectral resolution of multispectral instruments is poor, and methane measurements are typically very noisy. Methane data products are also sensitive to absorption by the surface and other atmospheric gases (water vapor in particular) and therefore provide noisy maps of potential methane plumes, that typically require extensive human analysis. Here, we show that the image recognition capabilities of deep learning methods can be leveraged to automatize the detection of methane leaks in 
    
[^92]: 利用可解释的人工智能来分析研究人员对ChatGPT的面向方面情感

    Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT. (arXiv:2308.11001v1 [cs.CL])

    [http://arxiv.org/abs/2308.11001](http://arxiv.org/abs/2308.11001)

    本文利用可解释的人工智能方法，分析了研究人员对ChatGPT在不同使用方面的情感。通过提出一种新的面向方面情感分析技术，使得这种分析不受文本数据长度限制，并提供了对新数据集的宝贵洞见。

    

    ChatGPT的创新性发明在各个领域引发了广泛的讨论。尽管庆祝其各种优点，但对其正确性和使用伦理产生了疑问。已经在努力捕捉用户对其的情感，但如何分析研究界关于ChatGPT不同使用方面的情感是一个值得探讨的问题。在标准的面向方面情感分析中，通常只应用于少量数据集，并且在短文本数据上仅获得有限的成功。我们提出了一种方法，利用可解释的人工智能来促进对研究数据的分析。我们的技术为在新数据集上拓展面向方面情感分析的最新技术提供了宝贵的洞见，使得这种分析不受文本数据长度的限制。

    The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
    
[^93]: 基于特征值的增量谱聚类

    Eigenvalue-based Incremental Spectral Clustering. (arXiv:2308.10999v1 [cs.LG])

    [http://arxiv.org/abs/2308.10999](http://arxiv.org/abs/2308.10999)

    本文介绍了一种基于特征值的增量谱聚类方法，通过将数据集划分为子集并进行聚类和合并，可以获得与聚类整个数据集相近的结果。

    

    我们之前的实验表明，（短）文档的子集合（包含几百个条目）在组合拉普拉斯特征值谱上有共同的归一化方式。基于这一洞察，我们提出了一种增量谱聚类的方法。该方法包括以下步骤：（1）将数据划分为可管理的子集，（2）对每个子集进行聚类，（3）基于特征值谱的相似性合并来自不同子集的聚类，形成整个数据集的聚类。这种方法可以特别适用于数据样本量大小发生强烈变化的聚类方法，例如典型的谱聚类。实验证明，实际上对子集进行聚类和合并可以得到与对整个数据集进行聚类相近的聚类结果。

    Our previous experiments demonstrated that subsets collections of (short) documents (with several hundred entries) share a common normalized in some way eigenvalue spectrum of combinatorial Laplacian. Based on this insight, we propose a method of incremental spectral clustering. The method consists of the following steps: (1) split the data into manageable subsets, (2) cluster each of the subsets, (3) merge clusters from different subsets based on the eigenvalue spectrum similarity to form clusters of the entire set. This method can be especially useful for clustering methods of complexity strongly increasing with the size of the data sample,like in case of typical spectral clustering. Experiments were performed showing that in fact the clustering and merging the subsets yields clusters close to clustering the entire dataset.
    
[^94]: SPEGTI: 结构预测用于高效生成文本到图像模型

    SPEGTI: Structured Prediction for Efficient Generative Text-to-Image Models. (arXiv:2308.10997v1 [cs.CV])

    [http://arxiv.org/abs/2308.10997](http://arxiv.org/abs/2308.10997)

    本文提出了一种使用马尔可夫随机场（MRF）模型的轻量级方法，用于实现图像不同区域的相容性，以降低生成文本到图像模型的计算成本。

    

    现代文本到图像生成模型能够生成高质量的图像，既逼真又与文本提示相符。然而，这种质量需要付出巨大的计算成本：几乎所有这些模型都是迭代式的，需要多次运行推断，并使用大模型。这种迭代过程是为了确保图像的不同区域不仅与文本提示对齐，还与其他区域相容。本文中，我们提出了一种轻量级的方法来实现图像不同区域的相容性，使用了马尔可夫随机场（MRF）模型。这种方法能够与最近提出的Muse模型配合使用。MRF编码了不同空间位置的图像标记之间的相容性，并且使我们能够显著减少所需的Muse预测步骤。使用MRF的推断成本大大降低，并且可以通过反向传播快速学习其参数，通过对MRF进行建模。

    Modern text-to-image generation models produce high-quality images that are both photorealistic and faithful to the text prompts. However, this quality comes at significant computational cost: nearly all of these models are iterative and require running inference multiple times with large models. This iterative process is needed to ensure that different regions of the image are not only aligned with the text prompt, but also compatible with each other. In this work, we propose a light-weight approach to achieving this compatibility between different regions of an image, using a Markov Random Field (MRF) model. This method is shown to work in conjunction with the recently proposed Muse model. The MRF encodes the compatibility among image tokens at different spatial locations and enables us to significantly reduce the required number of Muse prediction steps. Inference with the MRF is significantly cheaper, and its parameters can be quickly learned through back-propagation by modeling MR
    
[^95]: 极端天气事件中的深度学习技术：综述

    Deep Learning Techniques in Extreme Weather Events: A Review. (arXiv:2308.10995v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.10995](http://arxiv.org/abs/2308.10995)

    本综述提供了极端天气事件中深度学习技术的综合概述。我们探讨了深度学习在各种天气预测方面的潜力，并强调了其捕捉复杂模式和非线性关系的能力。此外，我们还讨论了当前方法的局限性和未来的发展方向。

    

    极端天气事件带来重大挑战，需要精确分析和精确预测的技术来减轻其影响。近年来，深度学习技术已经成为天气预报和理解极端天气动力学的一种有前途的方法。本综述旨在提供领域内最新的深度学习的综合概述。我们探索了深度学习架构在雷暴、闪电、降水、干旱、热浪、寒潮和热带气旋等天气预测方面的利用。我们强调了深度学习的潜力，如其捕捉复杂模式和非线性关系的能力。此外，我们讨论了当前方法的局限性，并强调了气象学领域发展的未来方向。从这个系统综述中获得的见解对科学界做出明智决策至关重要。

    Extreme weather events pose significant challenges, thereby demanding techniques for accurate analysis and precise forecasting to mitigate its impact. In recent years, deep learning techniques have emerged as a promising approach for weather forecasting and understanding the dynamics of extreme weather events. This review aims to provide a comprehensive overview of the state-of-the-art deep learning in the field. We explore the utilization of deep learning architectures, across various aspects of weather prediction such as thunderstorm, lightning, precipitation, drought, heatwave, cold waves and tropical cyclones. We highlight the potential of deep learning, such as its ability to capture complex patterns and non-linear relationships. Additionally, we discuss the limitations of current approaches and highlight future directions for advancements in the field of meteorology. The insights gained from this systematic review are crucial for the scientific community to make informed decision
    
[^96]: SupEuclid：使用监督对比学习和欧氏距离进行极简高质量OoD检测

    SupEuclid: Extremely Simple, High Quality OoD Detection with Supervised Contrastive Learning and Euclidean Distance. (arXiv:2308.10973v1 [cs.CV])

    [http://arxiv.org/abs/2308.10973](http://arxiv.org/abs/2308.10973)

    使用Supervised Contrastive Learning和欧氏距离的简单方法（SupEuclid）在OoD检测中取得了最先进的结果，无需复杂模型或超参数调整，为进一步实验和分析提供了强大且易用的基线。

    

    在过去几年中，OoD（Out-of-Distribution）检测取得了显著发展，现有方法在标准基准上接近甚至达到了完美的数据分离效果。这些结果通常涉及大型或复杂模型，预训练，暴露于OoD示例或额外的超参数调整。令人惊讶的是，使用一种非常简单的方法就可以超越这些最先进的方法。我们证明，使用监督对比学习（SCL）训练的ResNet18在近距离和远距离OoD检测基准上仅使用欧氏距离作为得分规则时，可以实现开箱即用的最先进结果。在某些情况下，这可能消除了对更复杂的方法或更大的模型的需求，并且至少提供了一个非常强大且易于使用的基线供进一步实验和分析使用。

    Out-of-Distribution (OoD) detection has developed substantially in the past few years, with available methods approaching, and in a few cases achieving, perfect data separation on standard benchmarks. These results generally involve large or complex models, pretraining, exposure to OoD examples or extra hyperparameter tuning. Remarkably, it is possible to achieve results that can exceed many of these state-of-the-art methods with a very simple method. We demonstrate that ResNet18 trained with Supervised Contrastive Learning (SCL) produces state-of-the-art results out-of-the-box on near and far OoD detection benchmarks using only Euclidean distance as a scoring rule. This may obviate the need in some cases for more sophisticated methods or larger models, and at the very least provides a very strong, easy to use baseline for further experimentation and analysis.
    
[^97]: 使用有限数据的MRI场转移重建：通过神经风格转换进行正规化

    MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer. (arXiv:2308.10968v1 [cs.CV])

    [http://arxiv.org/abs/2308.10968](http://arxiv.org/abs/2308.10968)

    本论文通过使用神经风格转换进行正规化，实现了在有限数据条件下从低质量图像重建高质量图像的目标。实验结果验证了该方法在临床MRI扫描中的有效性和潜力。

    

    最近的研究表明，使用基于深度学习模型的MRI重建取得了成功。然而，大多数报告的方法都需要在特定任务的大规模数据集上进行训练。通过降噪（RED）正规化是一种将降噪器作为图像重建先验的通用流程。RED的潜力已经在多个与图像相关的任务（如降噪、去模糊和超分辨率）中得到了证明。本文提出了一种通过神经风格转换（RNST）方法进行正规化的方法，进一步利用神经转移和降噪引擎的先验信息。这使得RNST能够从有噪声的低质量图像中重建出高质量图像，图像风格和有限数据不同。我们使用1.5T和3T的临床MRI扫描验证了RNST，并且显示RNST可以显著提高图像质量。我们的结果突显了RNST框架在MRI重建和有限数据重建任务中的能力。

    Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
    
[^98]: 基于人工智能的抗菌肽发现

    Artificial intelligence-driven antimicrobial peptide discovery. (arXiv:2308.10921v1 [q-bio.BM])

    [http://arxiv.org/abs/2308.10921](http://arxiv.org/abs/2308.10921)

    人工智能在抗菌肽发现中起到了重要作用，通过区分和生成方法，可以帮助识别有前途的候选化合物，并生成具有所需性质的抗菌肽。

    

    抗菌肽（AMPs）作为对抗抗菌耐药性的有希望的药物代替传统抗生素。通过区分和生成方法，人工智能（AI）革新了AMP发现。鉴别器通过预测关键的肽性质（如活性和毒性）来帮助识别有前途的候选化合物，而生成器学习肽的分布并能够采样新的AMP候选化合物，可以是全新的，也可以是原型肽的类似物。此外，通过鉴别器引导的过滤、仅针对正样本的学习、潜在空间采样以及条件和优化生成，实现了具有所需性质的AMP的可控生成。在这里，我们回顾了基于AI的AMP发现的最新成果，并突出了最令人兴奋的方向。

    Antimicrobial peptides (AMPs) emerge as promising agents against antimicrobial resistance, providing an alternative to conventional antibiotics. Artificial intelligence (AI) revolutionized AMP discovery through both discrimination and generation approaches. The discriminators aid the identification of promising candidates by predicting key peptide properties such as activity and toxicity, while the generators learn the distribution over peptides and enable sampling novel AMP candidates, either de novo, or as analogues of a prototype peptide. Moreover, the controlled generation of AMPs with desired properties is achieved by discriminator-guided filtering, positive-only learning, latent space sampling, as well as conditional and optimized generation. Here we review recent achievements in AI-driven AMP discovery, highlighting the most exciting directions.
    
[^99]: 基于Metapath的上下文知识的深度半监督异常检测

    Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge. (arXiv:2308.10918v1 [cs.LG])

    [http://arxiv.org/abs/2308.10918](http://arxiv.org/abs/2308.10918)

    本文介绍了一种利用基于Metapath的半监督学习的新颖方法，用于图异常检测。通过在编码器和解码器中使用GCN层来有效传播上下文信息，以及特别设计的异常社区，该方法在结构和属性差异的学习中表现出优越性能。通过实验证明了该方法的有效性，为未来的研究提供了重要的思路和方向。

    

    图异常检测近年来引起了广泛的关注。本文介绍了一种新颖的方法，利用基于Metapath的半监督学习，解决了之前方法的局限性。我们提出了一种新的框架，基于Metapath的半监督异常检测（MSAD），在编码器和解码器中都使用GCN层来有效地传播异常和正常节点之间的上下文信息。基于Metapath的上下文信息的设计和特别精心设计的异常社区增强了全局和局部结构和属性差异的学习过程。通过在七个真实网络上进行的一系列综合实验，本文证明了MSAD方法相对于最先进技术的优越性。本研究的有希望的结果为未来的研究铺平了道路，重点是优化和分析Metapath模式以进一步提高方法的效果。

    Graph anomaly detection has attracted considerable attention in recent years. This paper introduces a novel approach that leverages metapath-based semi-supervised learning, addressing the limitations of previous methods. We present a new framework, Metapath-based Semi-supervised Anomaly Detection (MSAD), incorporating GCN layers in both the encoder and decoder to efficiently propagate context information between abnormal and normal nodes. The design of metapath-based context information and a specifically crafted anomaly community enhance the process of learning differences in structures and attributes, both globally and locally. Through a comprehensive set of experiments conducted on seven real-world networks, this paper demonstrates the superiority of the MSAD method compared to state-of-the-art techniques. The promising results of this study pave the way for future investigations, focusing on the optimization and analysis of metapath patterns to further enhance the effectiveness of 
    
[^100]: PACS：基于多头注意力机制模型的多组学数据癌症亚型预测与分析

    PACS: Prediction and analysis of cancer subtypes from multi-omics data based on a multi-head attention mechanism model. (arXiv:2308.10917v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.10917](http://arxiv.org/abs/2308.10917)

    该研究提出了一种监督式多头注意力机制模型（SMA），通过学习多组学数据的全局和局部特征信息，成功分类了癌症亚型。通过融合模块深度融合Siamese的多头注意力编码器，提高了模型的参数丰富度。在模拟、单细胞和癌症多组学数据中，SMA模型实现了最高的准确性、F1宏观、F1加权和癌症亚型的准确分类。

    

    由于癌症的高异质性和临床特征，不同癌症亚型之间的多组学数据和临床特征存在显著差异。因此，准确分类癌症亚型可以帮助医生选择最合适的治疗方案，改善治疗效果，并提供更准确的患者生存预测。本研究提出了一种监督式多头注意力机制模型（SMA），成功分类了癌症亚型。SMA模型的注意力机制和特征共享模块可以成功学习多组学数据的全局和局部特征信息。其次，通过融合模块，深度融合了Siamese的多头注意力编码器，丰富了模型的参数。通过广泛的实验证明，SMA模型在模拟、单细胞和癌症多组学数据中实现了最高的准确性、F1宏观、F1加权和癌症亚型的准确分类。

    Due to the high heterogeneity and clinical characteristics of cancer, there are significant differences in multi-omic data and clinical characteristics among different cancer subtypes. Therefore, accurate classification of cancer subtypes can help doctors choose the most appropriate treatment options, improve treatment outcomes, and provide more accurate patient survival predictions. In this study, we propose a supervised multi-head attention mechanism model (SMA) to classify cancer subtypes successfully. The attention mechanism and feature sharing module of the SMA model can successfully learn the global and local feature information of multi-omics data. Second, it enriches the parameters of the model by deeply fusing multi-head attention encoders from Siamese through the fusion module. Validated by extensive experiments, the SMA model achieves the highest accuracy, F1 macroscopic, F1 weighted, and accurate classification of cancer subtypes in simulated, single-cell, and cancer multio
    
[^101]: DiffPrep: 可微分数据预处理管道搜索用于表格数据的学习

    DiffPrep: Differentiable Data Preprocessing Pipeline Search for Learning over Tabular Data. (arXiv:2308.10915v1 [cs.DB])

    [http://arxiv.org/abs/2308.10915](http://arxiv.org/abs/2308.10915)

    DiffPrep 提出了一种自动且高效地搜索数据预处理管道的方法，以最大化机器学习模型的性能。这通过将数据预处理管道搜索问题形式化为一个双层优化问题，并将离散的非可微搜索空间转化为连续空间来实现。

    

    数据预处理是机器学习过程中的关键步骤，它将原始数据转化为对下游机器学习模型更可用的格式。然而，这个过程往往耗时且代价高昂，通常需要领域专家的专门知识。现有的自动化机器学习 (AutoML) 框架声称可以自动化数据预处理。然而，它们通常使用了受限的数据预处理管道搜索空间，这限制了潜在的性能提升，并且由于需要多次训练机器学习模型，它们通常速度过慢。在本文中，我们提出了 DiffPrep，一种可以自动且高效地搜索给定表格数据集和可微分机器学习模型的数据预处理管道的方法，以最大化机器学习模型的性能。我们将数据预处理管道搜索问题形式化为一个双层优化问题。为了高效解决这个问题，我们将离散的非可微搜索空间转化并放松为连续的空间。

    Data preprocessing is a crucial step in the machine learning process that transforms raw data into a more usable format for downstream ML models. However, it can be costly and time-consuming, often requiring the expertise of domain experts. Existing automated machine learning (AutoML) frameworks claim to automate data preprocessing. However, they often use a restricted search space of data preprocessing pipelines which limits the potential performance gains, and they are often too slow as they require training the ML model multiple times. In this paper, we propose DiffPrep, a method that can automatically and efficiently search for a data preprocessing pipeline for a given tabular dataset and a differentiable ML model such that the performance of the ML model is maximized. We formalize the problem of data preprocessing pipeline search as a bi-level optimization problem. To solve this problem efficiently, we transform and relax the discrete, non-differential search space into a continuo
    
[^102]: 自动利用视觉预测编码进行虚拟环境映射

    Automated mapping of virtual environments with visual predictive coding. (arXiv:2308.10913v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.10913](http://arxiv.org/abs/2308.10913)

    本文提出了一种利用预测编码进行虚拟环境映射的算法，该算法可以根据感知数据构建空间地图，并在不同输入模态下具有泛化能力。

    

    人类根据感知输入直接构建对环境的内部认知地图，而不需要具有明确坐标或距离测量的系统。虽然机器学习算法如SLAM利用专门的视觉推理过程从视觉和里程计数据中识别视觉特征并构建空间地图，但大脑中认知地图的一般性表明可以使用统一的映射算法策略来泛化到听觉、触觉和语言输入。在这里，我们展示了预测编码提供了一种自然且多功能的神经网络算法，可以使用感知数据构建空间地图。我们介绍了一个框架，其中代理在虚拟环境中导航，并使用具有自我注意力的卷积神经网络进行视觉预测编码。在学习下一个图像预测任务的同时，代理会自动构建一个内部对环境的表示，定量地反映出距离等信息。

    Humans construct internal cognitive maps of their environment directly from sensory inputs without access to a system of explicit coordinates or distance measurements. While machine learning algorithms like SLAM utilize specialized visual inference procedures to identify visual features and construct spatial maps from visual and odometry data, the general nature of cognitive maps in the brain suggests a unified mapping algorithmic strategy that can generalize to auditory, tactile, and linguistic inputs. Here, we demonstrate that predictive coding provides a natural and versatile neural network algorithm for constructing spatial maps using sensory data. We introduce a framework in which an agent navigates a virtual environment while engaging in visual predictive coding using a self-attention-equipped convolutional neural network. While learning a next image prediction task, the agent automatically constructs an internal representation of the environment that quantitatively reflects dist
    
[^103]: 根据NASA的POWER数据在加纳主要城市中基于统计分析研究全球变暖的影响

    Global Warming In Ghana's Major Cities Based on Statistical Analysis of NASA's POWER Over 3-Decades. (arXiv:2308.10909v1 [physics.ao-ph])

    [http://arxiv.org/abs/2308.10909](http://arxiv.org/abs/2308.10909)

    该研究使用NASA的POWER数据，通过统计分析和机器学习预测了加纳四个主要城市的长期温度趋势，结果显示工业城市阿克拉有明显的变暖趋势，并且XGBoost模型的有效性得到了证实。

    

    全球变暖对世界各地高温的影响引起了人们的关注。本研究调查了代表不同气候区域的加纳四个主要城市的长期温度趋势。使用NASA的Prediction of Worldwide Energy Resource (POWER)数据，通过统计分析评估了当地气候变暖及其影响。线性回归趋势分析和eXtreme Gradient Boosting (XGBoost)机器学习预测温度变化。通过RSLab平台生成的地表温度（LST）剖面图提高了准确性。结果显示了当地的变暖趋势，特别是在工业城市阿克拉。人口因素不显著。XGBoost模型的低均方根误差（RMSE）得分显示了捕捉温度模式的有效性。瓦镇意外地拥有最高的平均温度。预计2023年中期的平均温度为：阿克拉27.86℃，库马西27.15℃，凯特克拉奇29.39℃，瓦镇30.76℃。

    Global warming's impact on high temperatures in various parts of the world has raised concerns. This study investigates long-term temperature trends in four major Ghanaian cities representing distinct climatic zones. Using NASA's Prediction of Worldwide Energy Resource (POWER) data, statistical analyses assess local climate warming and its implications. Linear regression trend analysis and eXtreme Gradient Boosting (XGBoost) machine learning predict temperature variations. Land Surface Temperature (LST) profile maps generated from the RSLab platform enhance accuracy. Results reveal local warming trends, particularly in industrialized Accra. Demographic factors aren't significant. XGBoost model's low Root Mean Square Error (RMSE) scores demonstrate effectiveness in capturing temperature patterns. Wa unexpectedly has the highest mean temperature. Estimated mean temperatures for mid-2023 are: Accra 27.86{\deg}C, Kumasi 27.15{\deg}C, Kete-Krachi 29.39{\deg}C, and Wa 30.76{\deg}C. These fin
    
[^104]: MLOps：一项综述研究

    MLOps: A Review. (arXiv:2308.10908v1 [cs.LG])

    [http://arxiv.org/abs/2308.10908](http://arxiv.org/abs/2308.10908)

    本研究综述了机器学习运维（MLOps）方法在解决问题中的可接受答案的重要性，并评估了22篇应用MLOps理念的论文。研究发现需要更加有效的MLOps方法以减少人类的参与。

    

    最近，机器学习（ML）已成为一种广泛接受的方法，用于快速发展的重要进展。由于它采用计算方法来教导机器并产生可接受的答案。本研究考察了机器学习运维（MLOps）方法的重要性，这些方法可以为这类问题提供可接受的答案。为了帮助创建易于使用的软件，作者研究了MLOps方法。为了选择适用于特定项目的最佳工具结构，作者还评估了各种MLOps方法的特性和可操作性。总共评估了22篇试图应用MLOps理念的论文。最后，作者承认了基于这样的进展可以通过限制人类参与来自我调节的完全有效的MLOps方法的稀缺性。

    Recently, Machine Learning (ML) has become a widely accepted method for significant progress that is rapidly evolving. Since it employs computational methods to teach machines and produce acceptable answers. The significance of the Machine Learning Operations (MLOps) methods, which can provide acceptable answers for such problems, is examined in this study. To assist in the creation of software that is simple to use, the authors research MLOps methods. To choose the best tool structure for certain projects, the authors also assess the features and operability of various MLOps methods. A total of 22 papers were assessed that attempted to apply the MLOps idea. Finally, the authors admit the scarcity of fully effective MLOps methods based on which advancements can self-regulate by limiting human engagement.
    
[^105]: 在TVM中分析量化

    Analyzing Quantization in TVM. (arXiv:2308.10905v1 [cs.LG])

    [http://arxiv.org/abs/2308.10905](http://arxiv.org/abs/2308.10905)

    该论文研究了在TVM中8位量化的性能问题，并讨论了兼容性和优化机会。

    

    在深度学习模型中对权重张量进行量化以减少推理延迟和内存占用的研究已经有很多。TVM也具备支持低比特计算和量化权重的能力。尽管通常期望通过量化来提高推理时间，在TVM中，8位量化的性能却不能满足期望。通常在将8位量化应用于深度学习模型时，通常期望达到全精度推理时间的50%左右。然而，在这种特殊情况下，量化版本不仅未能实现所期望的性能提升，而且实际上性能更差，导致推理时间约为非量化版本的两倍慢。在这个项目中，我们深入探讨了性能不佳的原因，评估了8位量化在TVM中的兼容性和优化机会。我们讨论了两种不同类型的优化方法

    There has been many papers in academic literature on quantizing weight tensors in deep learning models to reduce inference latency and memory footprint. TVM also has the ability to quantize weights and support low-bit computations. Although quantization is typically expected to improve inference time, in TVM, the performance of 8-bit quantization does not meet the expectations. Typically, when applying 8-bit quantization to a deep learning model, it is usually expected to achieve around 50% of the full-precision inference time. However, in this particular case, not only does the quantized version fail to achieve the desired performance boost, but it actually performs worse, resulting in an inference time that is about 2 times as slow as the non-quantized version. In this project, we thoroughly investigate the reasons behind the underperformance and assess the compatibility and optimization opportunities of 8-bit quantization in TVM. We discuss the optimization of two different types of
    
[^106]: 用于人工智能和机器学习应用的Majorana示范器数据发布

    Majorana Demonstrator Data Release for AI/ML Applications. (arXiv:2308.10856v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10856](http://arxiv.org/abs/2308.10856)

    此数据发布包含Majorana示范器实验的校准数据子集，旨在支持人工智能和机器学习算法在该数据上的训练和测试。

    

    此数据发布包含Majorana示范器实验的校准数据子集。每个Majorana事件都有原始的锗探测器波形、脉冲形状识别切割和校准后的最终能量，所有这些数据以HDF5文件格式与相关元数据一同分享。此发布旨在支持对我们的数据进行人工智能（AI）和机器学习（ML）算法的训练和测试。

    The enclosed data release consists of a subset of the calibration data from the Majorana Demonstrator experiment. Each Majorana event is accompanied by raw Germanium detector waveforms, pulse shape discrimination cuts, and calibrated final energies, all shared in an HDF5 file format along with relevant metadata. This release is specifically designed to support the training and testing of Artificial Intelligence (AI) and Machine Learning (ML) algorithms upon our data. This document is structured as follows. Section I provides an overview of the dataset's content and format; Section II outlines the location of this dataset and the method for accessing it; Section III presents the NPML Machine Learning Challenge associated with this dataset; Section IV contains a disclaimer from the Majorana collaboration regarding the use of this dataset; Appendix A contains technical details of this data release. Please direct questions about the material provided within this release to liaobo77@ucsd.ed
    
[^107]: DynED: 数据流分类中的动态集成多样化

    DynED: Dynamic Ensemble Diversification in Data Stream Classification. (arXiv:2308.10807v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2308.10807](http://arxiv.org/abs/2308.10807)

    DynED是一种动态集成多样化方法，基于MRR结合了组件的多样性和预测准确性，在数据流环境中实现了更高的准确率。

    

    鉴于数据分布的突变性变化，也称为概念漂移，在数据流环境中实现高准确度是一项具有挑战性的任务。在这种情况下，集合方法被广泛应用于分类，因为它们具有出色的性能。 在集合内部的更大多样性已被证明可以提高预测准确性。尽管集合内组件的多样性很高，但并不是所有组件都像预期的那样对整体性能有所贡献。这需要一种方法来选择展现出高性能和多样性的组件。我们提出了一种基于MMR（最大边际相关性）的新型集合构建和维护方法，在组合集合的过程中动态地结合了组件的多样性和预测准确性。在四个真实和11个合成数据集上的实验结果表明，所提出的方法（DynED）相比于五种最先进的基准方法提供了更高的平均准确率

    Ensemble methods are commonly used in classification due to their remarkable performance. Achieving high accuracy in a data stream environment is a challenging task considering disruptive changes in the data distribution, also known as concept drift. A greater diversity of ensemble components is known to enhance prediction accuracy in such settings. Despite the diversity of components within an ensemble, not all contribute as expected to its overall performance. This necessitates a method for selecting components that exhibit high performance and diversity. We present a novel ensemble construction and maintenance approach based on MMR (Maximal Marginal Relevance) that dynamically combines the diversity and prediction accuracy of components during the process of structuring an ensemble. The experimental results on both four real and 11 synthetic datasets demonstrate that the proposed approach (DynED) provides a higher average mean accuracy compared to the five state-of-the-art baselines
    
[^108]: 使用学习对手稳定无监督环境设计

    Stabilizing Unsupervised Environment Design with a Learned Adversary. (arXiv:2308.10797v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10797](http://arxiv.org/abs/2308.10797)

    本论文研究了无监督环境设计（UED）中PAIRED方法存在的问题，并提出了解决方案，使其在实际性能上能够与或超过最先进的方法。

    

    训练具备普遍能力的智能体面临的主要挑战是设计训练任务，以促进广泛泛化和对环境变化的鲁棒性。这个挑战驱动了无监督环境设计（UED）的问题设置，其中学生智能体在由教师智能体提出的自适应任务分布上进行训练。UED的先驱方法是PAIRED，它使用强化学习（RL）训练教师策略从头开始设计任务，这样可以直接生成适应智能体当前能力的任务。尽管其有很强的理论支持，但PAIRED存在一些挑战，妨碍了其实际性能。因此，目前的最先进方法依赖于策划和变异，而不是生成新任务。在这项工作中，我们研究了PAIRED的几个关键不足之处，并提出了解决方案。因此，我们使PAIRED能够与或超过最先进的方法。

    A key challenge in training generally-capable agents is the design of training tasks that facilitate broad generalization and robustness to environment variations. This challenge motivates the problem setting of Unsupervised Environment Design (UED), whereby a student agent trains on an adaptive distribution of tasks proposed by a teacher agent. A pioneering approach for UED is PAIRED, which uses reinforcement learning (RL) to train a teacher policy to design tasks from scratch, making it possible to directly generate tasks that are adapted to the agent's current capabilities. Despite its strong theoretical backing, PAIRED suffers from a variety of challenges that hinder its practical performance. Thus, state-of-the-art methods currently rely on curation and mutation rather than generation of new tasks. In this work, we investigate several key shortcomings of PAIRED and propose solutions for each shortcoming. As a result, we make it possible for PAIRED to match or exceed state-of-the-a
    
[^109]: FocalDreamer:基于文本驱动的焦点融合装配的3D编辑

    FocalDreamer: Text-driven 3D Editing via Focal-fusion Assembly. (arXiv:2308.10608v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.10608](http://arxiv.org/abs/2308.10608)

    FocalDreamer是一个基于文本驱动的3D编辑框架，能够通过焦点融合装配的方式实现精细化编辑，并且在几何和风格上能够保持整体一致性。实验证明了FocalDreamer在编辑能力上的优势。

    

    虽然文本驱动的3D编辑在利用分数蒸馏采样方面取得了显著进展，但新兴方法仍然无法提供内容创作所必需的可分离、精确和一致的结果。为了解决这个问题，我们介绍了FocalDreamer，这是一个根据文本提示将基本形状与可编辑部分合并的框架，用于对所需区域进行细粒度编辑。具体来说，FocalDreamer使用几何联合和双路径渲染将独立的3D部分组装成完整的对象，方便实例重用和部分控制。我们提出了几何焦点损失和风格一致性正则化，促进焦点融合和一致的整体外观。此外，FocalDreamer生成与广泛使用的图形引擎兼容的高保真度的几何和PBR纹理。大量实验证明了FocalDreamer在定量和定性评估中的优秀编辑能力。

    While text-3D editing has made significant strides in leveraging score distillation sampling, emerging approaches still fall short in delivering separable, precise and consistent outcomes that are vital to content creation. In response, we introduce FocalDreamer, a framework that merges base shape with editable parts according to text prompts for fine-grained editing within desired regions. Specifically, equipped with geometry union and dual-path rendering, FocalDreamer assembles independent 3D parts into a complete object, tailored for convenient instance reuse and part-wise control. We propose geometric focal loss and style consistency regularization, which encourage focal fusion and congruent overall appearance. Furthermore, FocalDreamer generates high-fidelity geometry and PBR textures which are compatible with widely-used graphics engines. Extensive experiments have highlighted the superior editing capabilities of FocalDreamer in both quantitative and qualitative evaluations.
    
[^110]: 信息理论引导的启发式渐进式多视图编码

    Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2308.10522v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.10522](http://arxiv.org/abs/2308.10522)

    通过信息理论提出了一个新的多视图学习框架，并基于此框架构建了一种渐进式多视图编码方法。

    

    多视图表示学习旨在从多个视图中捕获共享上下文的全面信息。最近的研究直观地将对比学习应用于不同视图的成对方式，这仍然可扩展：学习视图共享表示时未过滤视图特定的噪声；虚假的负对中，负项实际上在同一类别中与正项相同，并且真正的负对被同等对待；均匀地衡量术语之间的相似性可能干扰优化。重要的是，很少有工作研究广义自监督多视图学习的理论框架，特别是针对多于两个视图的情况。为此，我们从信息理论的视角重新思考了现有的多视图学习范式，然后提出了一个新的信息理论框架，用于广义多视图学习。在其指导下，我们构建了一个具有三层渐进式结构的多视图编码方法，即In

    Multi-view representation learning aims to capture comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning to different views in a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; evenly measuring the similarities between terms might interfere with optimization. Importantly, few works study the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the perspective of information theory and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive architecture, namely In
    
[^111]: 基于掩膜区域卷积神经网络 (Mask-RCNN) 的孟加拉文档版面分析性能增强

    Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis. (arXiv:2308.10511v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.10511](http://arxiv.org/abs/2308.10511)

    本论文旨在通过使用基于掩膜区域卷积神经网络 (Mask-RCNN) 对孟加拉文档进行版面分析，提升性能。通过逐步的超参数调整，我们达到了0.889的良好dice分数。虽然在应用英文文档模型时遇到了一些挑战，但这表明每种语言都有其特定的问题需要解决。

    

    理解数字文档就像解决一个谜题，尤其是历史文档。文档版面分析(DLA)通过将文档划分为段落、图片和表格等部分，帮助解决这个谜题。这对于机器来阅读和理解这些文档非常重要。在DL Sprint 2.0竞赛中，我们致力于理解孟加拉文档。我们使用了一个名为BaDLAD的数据集，其中包含很多例子。我们训练了一个特殊的模型，称为Mask R-CNN，来帮助理解。通过逐步的超参数调整，我们改进了这个模型，并取得了0.889的好的dice分数。然而，并不是一切都进行得非常完美。我们尝试使用训练好的英文文档模型，但它与孟加拉文不太匹配。这向我们展示了每种语言都有自己的挑战。我们对DL Sprint 2.0的解决方案可以在https://www.kaggle.com/competitions/dlsprint2/discussion/432201公开获得，其中包括notebooks、权重和推理笔记本。

    Understanding digital documents is like solving a puzzle, especially historical ones. Document Layout Analysis (DLA) helps with this puzzle by dividing documents into sections like paragraphs, images, and tables. This is crucial for machines to read and understand these documents. In the DL Sprint 2.0 competition, we worked on understanding Bangla documents. We used a dataset called BaDLAD with lots of examples. We trained a special model called Mask R-CNN to help with this understanding. We made this model better by step-by-step hyperparameter tuning, and we achieved a good dice score of 0.889. However, not everything went perfectly. We tried using a model trained for English documents, but it didn't fit well with Bangla. This showed us that each language has its own challenges. Our solution for the DL Sprint 2.0 is publicly available at https://www.kaggle.com/competitions/dlsprint2/discussion/432201 along with notebooks, weights, and inference notebook.
    
[^112]: 一种使用短语机制的神经机器翻译有效方法

    An Effective Method using Phrase Mechanism in Neural Machine Translation. (arXiv:2308.10482v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10482](http://arxiv.org/abs/2308.10482)

    本论文介绍了一种使用短语机制的神经机器翻译方法，PhraseTransformer，在越南语-中文平行语料库上取得了较高的BLEU得分。

    

    机器翻译是自然语言处理中的基本任务之一，广泛应用于现实生活，并对NLP研究社区的其他任务做出了贡献。最近，基于Transformer的方法在此领域吸引了许多研究人员，并在大多数语言对中取得了最先进的结果。在本文中，我们报告了一种使用短语机制的有效方法，PhraseTransformer，用于改进基线模型Transformer构建平行语料库越南语-中文的神经机器翻译（NMT）系统。我们在VLSP 2022竞赛的MT数据集上，实现了越南语到中文的35.3 BLEU得分和中文到越南语的33.2 BLEU得分。我们的代码可在https://github.com/phuongnm94/PhraseTransformer获得。

    Machine Translation is one of the essential tasks in Natural Language Processing (NLP), which has massive applications in real life as well as contributing to other tasks in the NLP research community. Recently, Transformer -based methods have attracted numerous researchers in this domain and achieved state-of-the-art results in most of the pair languages. In this paper, we report an effective method using a phrase mechanism, PhraseTransformer, to improve the strong baseline model Transformer in constructing a Neural Machine Translation (NMT) system for parallel corpora Vietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022 competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2 BLEU scores on Chinese to Vietnamese data. Our code is available at https://github.com/phuongnm94/PhraseTransformer.
    
[^113]: 基于时空自适应嵌入，普通Transformer在交通预测中领先于其他方法

    Spatio-Temporal Adaptive Embedding Makes Vanilla Transformer SOTA for Traffic Forecasting. (arXiv:2308.10425v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10425](http://arxiv.org/abs/2308.10425)

    本研究提出了一种名为时空自适应嵌入的新组件，在普通的Transformer中实现了领先于其他方法的交通预测性能，通过捕捉交通时间序列中的时空关系和时间信息实现了优秀的结果。

    

    随着智能交通系统（ITS）的快速发展，准确的交通预测已成为一个关键挑战。其核心瓶颈在于捕捉复杂的时空交通模式。近年来，已经提出了许多具有复杂架构的神经网络来解决这个问题。然而，网络架构的进步遇到了性能收益降低的问题。在本研究中，我们提出了一种称为时空自适应嵌入的新组件，可以在普通的Transformer中获得出色的结果。我们的提出的Spatio-Temporal Adaptive Embedding transformer（STAEformer）在五个真实世界的交通预测数据集上实现了最先进的性能。进一步的实验表明，时空自适应嵌入通过有效捕捉交通时间序列中的内在时空关系和时间信息，在交通预测中起到了关键作用。

    With the rapid development of the Intelligent Transportation System (ITS), accurate traffic forecasting has emerged as a critical challenge. The key bottleneck lies in capturing the intricate spatio-temporal traffic patterns. In recent years, numerous neural networks with complicated architectures have been proposed to address this issue. However, the advancements in network architectures have encountered diminishing performance gains. In this study, we present a novel component called spatio-temporal adaptive embedding that can yield outstanding results with vanilla transformers. Our proposed Spatio-Temporal Adaptive Embedding transformer (STAEformer) achieves state-of-the-art performance on five real-world traffic forecasting datasets. Further experiments demonstrate that spatio-temporal adaptive embedding plays a crucial role in traffic forecasting by effectively capturing intrinsic spatio-temporal relations and chronological information in traffic time series.
    
[^114]: FedSIS: 用于隐私保护的分布式分割学习和中间表示采样的联邦式面部展示攻击检测

    FedSIS: Federated Split Learning with Intermediate Representation Sampling for Privacy-preserving Generalized Face Presentation Attack Detection. (arXiv:2308.10236v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2308.10236](http://arxiv.org/abs/2308.10236)

    FedSIS是一种用于隐私保护的领域泛化的新框架，采用联邦式分割学习和中间表示采样的方法，解决了面部展示攻击检测算法泛化能力不足的问题。

    

    大多数面部展示攻击检测（FacePAD）算法的致命缺点是对未见过的领域/攻击缺乏泛化能力。现有的增强FacePAD解决方案泛化能力的尝试假设可以通过单一实体收集多个源领域的数据来进行集中式训练。实际上，不同源领域的数据可能由不同的实体收集，由于法律和隐私限制，他们往往无法共享自己的数据。虽然联合学习范式（如联邦学习）可以克服这个问题，但标准的联邦学习方法不适合领域泛化，因为它们在训练过程中很难处理非独立同分布的客户端数据分布，并在推理过程中泛化到未见过的领域。本文介绍了一种新颖的框架，称为联邦式分割学习和中间表示采样（FedSIS），用于隐私保护的领域泛化。

    Lack of generalization to unseen domains/attacks is the Achilles heel of most face presentation attack detection (FacePAD) algorithms. Existing attempts to enhance the generalizability of FacePAD solutions assume that data from multiple source domains are available with a single entity to enable centralized training. In practice, data from different source domains may be collected by diverse entities, who are often unable to share their data due to legal and privacy constraints. While collaborative learning paradigms such as federated learning (FL) can overcome this problem, standard FL methods are ill-suited for domain generalization because they struggle to surmount the twin challenges of handling non-iid client data distributions during training and generalizing to unseen domains during inference. In this work, a novel framework called Federated Split learning with Intermediate representation Sampling (FedSIS) is introduced for privacy-preserving domain generalization. In FedSIS, a 
    
[^115]: 从高资源语言到低资源编程语言的知识转移用于代码LLMs

    Knowledge Transfer from High-Resource to Low-Resource Programming Languages for Code LLMs. (arXiv:2308.09895v1 [cs.PL])

    [http://arxiv.org/abs/2308.09895](http://arxiv.org/abs/2308.09895)

    本文介绍了一种有效的方法，通过使用半合成数据来提升代码LLMs在低资源语言上的性能。方法名为MultiPL-T，通过将高资源语言的训练数据转化为低资源语言的训练数据，生成高质量的低资源语言数据集。

    

    在过去几年中，代码LLMs（大规模语言模型）开始对编程实践产生重大影响。代码LLMs还成为编程语言和软件工程研究的重要组成部分。然而，代码LLMs生成的代码质量在不同编程语言之间存在显著差异。代码LLMs对训练数据充分的编程语言（如Java、Python或JavaScript）产生令人印象深刻的结果，但在像OCaml和Racket这样的低资源语言上表现困难。本文提出了一种有效的方法，通过使用半合成数据提高代码LLMs在低资源语言上的性能。我们的方法生成了高质量的低资源语言数据集，并可用于微调任何预训练的代码LLMs。我们的方法称为MultiPL-T，它将高资源语言的训练数据转化为低资源语言的训练数据。我们将该方法应用于生成十个数据集。

    Over the past few years, Large Language Models of Code (Code LLMs) have started to have a significant impact on programming practice. Code LLMs are also emerging as a building block for research in programming languages and software engineering. However, the quality of code produced by a Code LLM varies significantly by programming languages. Code LLMs produce impressive results on programming languages that are well represented in their training data (e.g., Java, Python, or JavaScript), but struggle with low-resource languages, like OCaml and Racket.  This paper presents an effective approach for boosting the performance of Code LLMs on low-resource languages using semi-synthetic data. Our approach generates high-quality datasets for low-resource languages, which can then be used to fine-tune any pretrained Code LLM. Our approach, called MultiPL-T, translates training data from high-resource languages into training data for low-resource languages. We apply our approach to generate ten
    
[^116]: DatasetEquity: 所有样本是否平等？在数据集中追求公平性。

    DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets. (arXiv:2308.09878v1 [cs.CV])

    [http://arxiv.org/abs/2308.09878](http://arxiv.org/abs/2308.09878)

    该论文提出了一种新的方法来解决机器学习中数据不平衡的问题，通过使用深层感知嵌入和聚类来计算样本的似然性，并使用广义聚焦损失函数在训练过程中对样本进行不同的加权。实验验证了该方法的有效性。

    

    数据不平衡是机器学习领域中一个众所周知的问题，归因于数据收集的成本、标签的困难以及数据的地理分布。在计算机视觉中，图像外观引起的数据分布偏差仍未得到很好的研究。与使用类别标签的分类分布相比，图像外观展示了超出类别标签所提供的对象之间的复杂关系。通过对从原始像素中提取的深层感知特征进行聚类，可以获得更丰富的数据表示。本文提出了一种新的解决机器学习中数据不平衡问题的方法。该方法使用基于图像外观的深层感知嵌入和聚类计算样本的似然性，然后利用这些似然性在训练过程中对样本进行不同的加权，使用提出的\textbf{广义聚焦损失}函数。该损失函数可以很容易地与深度学习算法集成。实验验证了该方法的有效性。

    Data imbalance is a well-known issue in the field of machine learning, attributable to the cost of data collection, the difficulty of labeling, and the geographical distribution of the data. In computer vision, bias in data distribution caused by image appearance remains highly unexplored. Compared to categorical distributions using class labels, image appearance reveals complex relationships between objects beyond what class labels provide. Clustering deep perceptual features extracted from raw pixels gives a richer representation of the data. This paper presents a novel method for addressing data imbalance in machine learning. The method computes sample likelihoods based on image appearance using deep perceptual embeddings and clustering. It then uses these likelihoods to weigh samples differently during training with a proposed \textbf{Generalized Focal Loss} function. This loss can be easily integrated with deep learning algorithms. Experiments validate the method's effectiveness a
    
[^117]: 受冷落: 相似度分数的反差效应

    Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])

    [http://arxiv.org/abs/2308.09765](http://arxiv.org/abs/2308.09765)

    提出了一种新的相似度度量方法，称为“惊喜分数”，该方法能够考虑对象的上下文信息并显著提高零样本和少样本文档分类任务的性能。

    

    准确评估物体向量嵌入的相似度对于自然语言处理、信息检索和分类任务至关重要。流行的相似度分数（如余弦相似度）基于嵌入向量对，并忽略了从中提取对象的分布。人类对物体相似度的感知显著取决于对象出现的上下文。在这项工作中，我们提出了“惊喜分数”，这是一个对整体进行归一化的相似度度量，包括了人类感知的反差效应，并显著提高了零样本和少样本文档分类任务的性能。此分数量化了在两个元素之间找到给定相似度的惊喜，相对于成对的整体相似度。我们在零样本/少样本分类和聚类任务上评估了这个度量，通常发现与原始余弦相似度相比，性能提高了10-15\%。我们的代码...

    Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
    
[^118]: MindMap：知识图谱激发大型语言模型的思维图思考方法

    MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])

    [http://arxiv.org/abs/2308.09729](http://arxiv.org/abs/2308.09729)

    本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。

    

    通常，大型语言模型存在无法整合新知识、产生幻觉和决策过程不透明等限制。本文探讨了如何利用知识图谱（KG）来激发大型语言模型，以解决整合最新知识和引发模型思维路径的问题。具体来说，我们构建了一个提示管道，使大型语言模型能够理解KG输入并利用隐含知识和检索到的外部知识进行推理。此外，我们研究了引发大型语言模型执行推理和生成答案的思维导图。研究发现，生成的思维导图基于知识的本体论，展示了大型语言模型的推理路径，从而为生产环境中的推理提供了探索和评估的可能性。对三个问答数据集的实验证明，MindMap提示方法带来了显著的实证增益。

    LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
    
[^119]: PMET: 在Transformer中的精确模型编辑

    PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])

    [http://arxiv.org/abs/2308.08742](http://arxiv.org/abs/2308.08742)

    该论文通过分析Transformer模型中的隐藏状态，发现多头自注意力编码了某些通用知识提取模式，因此在进行模型编辑时，不需要更新多头自注意力的权重。

    

    模型编辑技术可以以较低的成本修改大型语言模型中的少量知识，并且已经取得了显著的成功。现有方法假设Transformer层隐藏状态是前馈网络的键值内存的值。它们通常优化Transformer层隐藏状态来记忆目标知识，并将其用于更新大型语言模型中前馈网络的权重。然而，Transformer层隐藏状态的信息流来自三个部分：多头自注意力、前馈网络和残差连接。现有方法忽视了Transformer层隐藏状态包含了前馈网络特别需要的信息这一事实。因此，模型编辑的性能下降。为了实现更精确的模型编辑，我们分析了多头自注意力和前馈网络的隐藏状态，发现多头自注意力编码了某些通用知识提取模式。这意味着当引入新知识时，多头自注意力的权重不需要更新。

    Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
    
[^120]: 人工智能中的意识：来自意识科学的洞见

    Consciousness in Artificial Intelligence: Insights from the Science of Consciousness. (arXiv:2308.08708v1 [cs.AI])

    [http://arxiv.org/abs/2308.08708](http://arxiv.org/abs/2308.08708)

    本论文提出了一种严谨的方法，通过对当前的人工智能系统进行详细评估来探讨人工智能的意识问题。研究中对几种科学意识理论进行概述，并通过计算方法推导出意识的“指示性特征”。分析结果表明目前的人工智能系统尚不具备意识，但建立具有意识的人工智能系统并无明显的障碍。

    

    当前或近期的人工智能系统是否能具有意识成为科学界关注的话题，也引起了公众的担忧。本报告提出并举例了一种严谨且经验基础的人工智能意识方法：根据我们目前最可信的神经科学理论对现有的人工智能系统进行详细评估。我们概述了几种广泛认可的科学意识理论，包括循环处理理论、全局工作空间理论、高阶理论、预测处理理论和注意模式理论。从这些理论中，我们推导出一些意识的“指示性特征”，并通过计算方法来评估人工智能系统是否具备这些特征。我们利用这些指示性特征来评估了几个近期的人工智能系统，并讨论了未来系统如何实现这些特征。我们的分析表明，目前没有现有的人工智能系统具有意识，但同时也显示出没有明显的建立具有意识的人工智能系统的障碍。

    Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also shows that there are no obvious barriers to building conscious AI systems.
    
[^121]: 深度学习用于深脑刺激手术中的尖峰检测

    Deep learning for spike detection in deep brain stimulation surgery. (arXiv:2308.05755v1 [eess.SP] CROSS LISTED)

    [http://arxiv.org/abs/2308.05755](http://arxiv.org/abs/2308.05755)

    本文介绍了一种利用深度学习分析深脑刺激神经外科手术期间神经活动记录的方法，通过卷积神经网络对时间窗口内的神经活动进行分类，实现了高准确率的尖峰检测。

    

    深脑刺激（DBS）是一种成功用于治疗帕金森病等疾病的神经外科手术。通过将电极植入脑部的特定区域进行电刺激，可以显著减轻疾病症状。本文介绍了一种利用深度学习分析DBS神经外科手术期间获得的神经活动记录的方法。我们测试了使用卷积神经网络（CNN）进行这一目的。基于时间窗口，分类器评估神经活动（尖峰）是否存在。分类器的最大准确率为98.98％，接收器工作特性曲线下面积（AUC）为0.9898。该方法使得在不使用数据预处理的情况下获得分类结果。

    Deep brain stimulation (DBS) is a neurosurgical procedure successfully used to treat conditions such as Parkinson's disease. Electrostimulation, carried out by implanting electrodes into an identified focus in the brain, makes it possible to reduce the symptoms of the disease significantly. In this paper, a method for analyzing recordings of neuronal activity acquired during DBS neurosurgery using deep learning is presented. We tested using a convolutional neural network (CNN) for this purpose. Based on the time window, the classifier assesses whether neuronal activity (spike) is present. The maximum accuracy value for the classifier was 98.98%, and the area under the receiver operating characteristic curve (AUC) was 0.9898. The method made it possible to obtain a classification without using data preprocessing.
    
[^122]: 临界点++：一种用于鲁棒分类、对抗性防御和可解释AI的敏捷点云重要性度量方法

    Critical Points ++: An Agile Point Cloud Importance Measure for Robust Classification, Adversarial Defense and Explainable AI. (arXiv:2308.05525v1 [cs.CV])

    [http://arxiv.org/abs/2308.05525](http://arxiv.org/abs/2308.05525)

    本文研究了三维点云的临界点与非分布样本之间的相互作用，并将临界点的概念推广为重要性度量方法。通过仅基于非重要点进行分类网络训练，可以提高鲁棒性，同时在干净数据集上会有些性能损失。建议使用标准化熵选择非临界点集合的自适应阈值。这种重要性度量方法计算速度极快，可以应用于可解释AI、离群值去除、不确定性估计、鲁棒分类和对抗性防御等多种应用。

    

    在真实世界的安全需求应用中，准确且快速地处理非分布样本是至关重要的。本文首先研究了三维点云的临界点与非分布样本之间的相互作用。我们的研究发现，常见的数据损坏和离群点往往会被解释为临界点。我们将临界点的概念推广为重要性度量。我们证明，仅基于非重要点的分类网络训练，可以大大提高鲁棒性，而在干净数据集上会稍微损失一些性能。我们观察到，标准化熵对于数据损坏分析非常有信息量。建议基于标准化熵选择非临界点集合的自适应阈值。我们提出的重要性度量计算极其快速。我们展示了它可以用于多种应用，例如可解释AI(XAI)，离群值去除，不确定性估计，鲁棒分类和对抗性防御。

    The ability to cope accurately and fast with Out-Of-Distribution (OOD) samples is crucial in real-world safety demanding applications. In this work we first study the interplay between critical points of 3D point clouds and OOD samples. Our findings are that common corruptions and outliers are often interpreted as critical points. We generalize the notion of critical points into importance measures. We show that training a classification network based only on less important points dramatically improves robustness, at a cost of minor performance loss on the clean set. We observe that normalized entropy is highly informative for corruption analysis. An adaptive threshold based on normalized entropy is suggested for selecting the set of uncritical points. Our proposed importance measure is extremely fast to compute. We show it can be used for a variety of applications, such as Explainable AI (XAI), Outlier Removal, Uncertainty Estimation, Robust Classification and Adversarial Defense. We 
    
[^123]: 通过Transformer增强大规模异构联邦学习的前景

    The Prospect of Enhancing Large-Scale Heterogeneous Federated Learning with Transformers. (arXiv:2308.03945v1 [cs.LG])

    [http://arxiv.org/abs/2308.03945](http://arxiv.org/abs/2308.03945)

    本文研究了在大规模异构联邦学习中，通过Transformer模型实现泛化和个性化的前景，并通过广泛的比较实验证明了Transformer在大规模异构FL任务中相对于深度神经网络的优势，并通过分析中心核对齐（CKA）表示相似性来深入了解Transformer有前景的能力背后的原因。

    

    联邦学习（FL）通过实现跨分布式数据所有者的AI模型的协作训练来解决数据隐私问题。FL的广泛采用面临着数据异构和涉及到的数据所有者规模庞大的根本挑战。本文研究了在这种情况下，基于Transformer的FL模型在实现泛化和个性化方面的前景。我们在不同场景下进行了广泛的比较实验，涉及到了FL与Transformer、ResNet和个性化ResNet-based FL方法。这些实验考虑了不同数量的数据所有者，以展示Transformer在大规模异构FL任务中相对于深度神经网络的优势。此外，我们通过比较不同层和FL模型之间的中心核对齐（CKA）表示相似性来分析Transformer的出色性能，以深入了解其有前景的能力背后的原因。

    Federated learning (FL) addresses data privacy concerns by enabling collaborative training of AI models across distributed data owners. Wide adoption of FL faces the fundamental challenges of data heterogeneity and the large scale of data owners involved. In this paper, we investigate the prospect of Transformer-based FL models for achieving generalization and personalization in this setting. We conduct extensive comparative experiments involving FL with Transformers, ResNet, and personalized ResNet-based FL approaches under various scenarios. These experiments consider varying numbers of data owners to demonstrate Transformers' advantages over deep neural networks in large-scale heterogeneous FL tasks. In addition, we analyze the superior performance of Transformers by comparing the Centered Kernel Alignment (CKA) representation similarity across different layers and FL models to gain insight into the reasons behind their promising capabilities.
    
[^124]: 平均困难注意力变换器是常深度均匀阈值电路

    Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.03212](http://arxiv.org/abs/2308.03212)

    本文研究表明平均困难注意力变换器和对数精度变换器都可以模拟常深度阈值电路，其中后者由于生成统一的电路族而更健壮。

    

    转换器已成为各种自然语言处理任务中广泛使用的神经网络模型。先前的研究探索了它们与常深度阈值电路的关系，做出了两个假设：平均困难注意力和相对于输入长度的对数精度的内部计算。Merrill等人证明了平均困难注意力变换器可以识别属于复杂性类别TC0的语言，该类别表示可以由常深度多项式大小的阈值电路识别的语言集合。同样地，Merrill和Sabharwal证明了对数精度的变换器可以识别统一的TC0类语言。这表明这两种转换器模型都可以通过常深度阈值电路来模拟，而后者由于生成统一的电路族而更健壮。我们的论文表明，第一个结果也可以延伸到产生统一电路。

    Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
    
[^125]: 从噪声类型角度重新思考真实场景下的有噪标注学习问题

    Rethinking Noisy Label Learning in Real-world Annotation Scenarios from the Noise-type Perspective. (arXiv:2307.16889v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16889](http://arxiv.org/abs/2307.16889)

    本文提出了一种基于样本选择的噪声标签学习方法，该方法可以在真实场景下区分不同类型的噪声，并利用噪声的语义信息进行学习。通过构建原型向量和计算样本与原型向量之间的距离，该方法可以改进标签的准确性。实证评估结果表明了该方法的鲁棒性和有效性。

    

    本文研究了在真实场景下学习具有噪声标签的问题，其中噪声可以分为两种类型：事实性噪声和模糊性噪声。为了更好地区分这些噪声类型并利用其语义，我们提出了一种基于样本选择的噪声标签学习方法，称为Proto-semi。Proto-semi通过预热将所有样本划分为自信和不自信的数据集。通过利用自信数据集，构建原型向量以捕捉类别特征。然后，计算不自信样本与原型向量之间的距离以促进噪声分类。根据这些距离，对标签进行修正或保留，从而改进自信和不自信数据集。最后，我们引入了一种半监督学习方法来增强训练。对一个真实标注数据集进行的实证评估证实了Proto-semi的鲁棒性。

    In this paper, we investigate the problem of learning with noisy labels in real-world annotation scenarios, where noise can be categorized into two types: factual noise and ambiguity noise. To better distinguish these noise types and utilize their semantics, we propose a novel sample selection-based approach for noisy label learning, called Proto-semi. Proto-semi initially divides all samples into the confident and unconfident datasets via warm-up. By leveraging the confident dataset, prototype vectors are constructed to capture class characteristics. Subsequently, the distances between the unconfident samples and the prototype vectors are calculated to facilitate noise classification. Based on these distances, the labels are either corrected or retained, resulting in the refinement of the confident and unconfident datasets. Finally, we introduce a semi-supervised learning method to enhance training. Empirical evaluations on a real-world annotated dataset substantiate the robustness of
    
[^126]: 自主载荷热控制

    Autonomous Payload Thermal Control. (arXiv:2307.15438v1 [cs.LG])

    [http://arxiv.org/abs/2307.15438](http://arxiv.org/abs/2307.15438)

    该论文提出了一种基于深度强化学习的框架，利用软演员-评论家算法在卫星上学习热控制策略，以解决小型卫星中热控制的挑战。该框架在模拟环境和实际空间处理计算机上进行了评估，并证明能够辅助传统热控制系统，保持载荷温度在可操作范围内。

    

    在小型卫星中，热控制设备、科学仪器和电子部件的空间较小。此外，电子设备的近距离使得功耗散热困难，存在无法适当控制温度、降低部件寿命和任务性能的风险。为了应对这一挑战，利用卫星上逐渐增加的智能，提出了一种基于深度强化学习的框架，使用软演员-评论家算法来学习机载热控制策略。该框架在一个简单的模拟环境和未来将运往ISS并在IMAGIN-e任务中进行边缘计算的真实空间处理计算机中进行了评估。实验结果表明，所提出的框架能够学习控制载荷处理功率，以保持温度在操作范围内，补充传统热控制系统。

    In small satellites there is less room for heat control equipment, scientific instruments, and electronic components. Furthermore, the near proximity of the electronics makes power dissipation difficult, with the risk of not being able to control the temperature appropriately, reducing component lifetime and mission performance. To address this challenge, taking advantage of the advent of increasing intelligence on board satellites, a deep reinforcement learning based framework that uses Soft Actor-Critic algorithm is proposed for learning the thermal control policy onboard. The framework is evaluated both in a naive simulated environment and in a real space edge processing computer that will be shipped in the future IMAGIN-e mission and hosted in the ISS. The experiment results show that the proposed framework is able to learn to control the payload processing power to maintain the temperature under operational ranges, complementing traditional thermal control systems.
    
[^127]: 在Wasserstein空间中通过数据集字典学习进行多源域自适应

    Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])

    [http://arxiv.org/abs/2307.14953](http://arxiv.org/abs/2307.14953)

    本文提出了一种基于字典学习和最优传输的MSDA框架，通过将每个域表示为字典原子的Wasserstein重心来缓解数据分布偏移。根据该字典，提出了两种新的MSDA方法，分别基于目标域标记样本的重构和在原子分布上学习的分类器的集成。在多个基准测试集上进行的实验证明，这些方法在分类任务上取得了显著的改进效果。

    

    本文旨在解决多源域自适应（MSDA）问题，该问题旨在在从多个标记的源域转移知识到未标记的目标域时缓解数据分布偏移。我们提出了一种基于字典学习和最优传输的新型MSDA框架。我们将MSDA中的每个域解释为经验分布。因此，我们将每个域表达为字典原子的Wasserstein重心，这些原子是经验分布。我们提出了一种新的通过小批量学习的算法DaDiL：（i）原子分布；（ii）重心坐标矩阵。根据我们的字典，我们提出了两种新的MSDA方法：DaDiL-R，基于目标域标记样本的重构；DaDiL-E，基于在原子分布上学习的分类器的集成。我们在3个基准测试集中评估了我们的方法：Caltech-Office、Office 31和CRWU，在分类上改进了以前的最先进技术3.15％、2.29％和7.71％。

    This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims to mitigate data distribution shifts when transferring knowledge from multiple labeled source domains to an unlabeled target domain. We propose a novel MSDA framework based on dictionary learning and optimal transport. We interpret each domain in MSDA as an empirical distribution. As such, we express each domain as a Wasserstein barycenter of dictionary atoms, which are empirical distributions. We propose a novel algorithm, DaDiL, for learning via mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates. Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based on the reconstruction of labeled samples in the target domain, and DaDiL-E, based on the ensembling of classifiers learned on atom distributions. We evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU, where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in classification 
    
[^128]: 理解医学图像分类中的隐性失败

    Understanding Silent Failures in Medical Image Classification. (arXiv:2307.14729v1 [eess.IV])

    [http://arxiv.org/abs/2307.14729](http://arxiv.org/abs/2307.14729)

    这项研究通过对医学应用中的分类系统进行分析，发现目前的置信度评分函数无法可靠地防止隐性失败，强调了对数据中失败根本原因的深入理解的重要性。引入了SF-Visuals，一个通过潜在空间聚类来可视化偏移和失败的交互式分析工具。

    

    为了确保分类系统在医学应用中的可靠性使用，防止隐性失败至关重要。可以通过设计足够稳健的分类器以避免首次失败，或通过使用置信度评分函数（CSFs）检测剩余的失败来实现。图像分类中失败的主要源头是训练数据和部署数据之间的分布偏移。为了理解医学图像中防止隐性失败的当前状态，我们进行了第一次全面分析，比较了四个生物医学任务和各种分布偏移下的各种CSFs。根据结果发现，没有一个被基准化的CSF能够可靠地防止隐性失败，我们认为需要对数据中失败的根本原因进行更深入的理解。为了促进这一点，我们引入了SF-Visuals，一个交互式分析工具，利用潜在空间聚类来可视化偏移和失败。

    To ensure the reliable use of classification systems in medical applications, it is crucial to prevent silent failures. This can be achieved by either designing classifiers that are robust enough to avoid failures in the first place, or by detecting remaining failures using confidence scoring functions (CSFs). A predominant source of failures in image classification is distribution shifts between training data and deployment data. To understand the current state of silent failure prevention in medical imaging, we conduct the first comprehensive analysis comparing various CSFs in four biomedical tasks and a diverse range of distribution shifts. Based on the result that none of the benchmarked CSFs can reliably prevent silent failures, we conclude that a deeper understanding of the root causes of failures in the data is required. To facilitate this, we introduce SF-Visuals, an interactive analysis tool that uses latent space clustering to visualize shifts and failures. On the basis of va
    
[^129]: 前列腺成像中分割基础模型的实证分析

    Empirical Analysis of a Segmentation Foundation Model in Prostate Imaging. (arXiv:2307.03266v1 [eess.IV])

    [http://arxiv.org/abs/2307.03266](http://arxiv.org/abs/2307.03266)

    本文通过对前列腺成像中的新型基础模型UniverSeg进行了实证评估研究，并将其与传统方法进行了比较。结果表明基础模型可能是医学成像领域未来的方向。

    

    大多数医学图像分割的最先进技术依赖于深度学习模型。然而，这些模型通常在狭义任务上以监督的方式进行训练，需要昂贵的标记数据集。最近，在自然语言生成等多个机器学习领域取得的进展已经证明了构建基础模型的可行性和实用性，这些模型可以在几乎没有标记数据的情况下为各种不同的下游任务定制。这可能代表了医学成像的范式转变，我们预计基础模型可能塑造该领域的未来。本文考虑了一个最近开发的应用于医学图像分割的基础模型UniverSeg。我们在前列腺成像的背景下进行了经验评估研究，并将其与传统的训练任务特定分割模型的方法进行了比较。我们的结果和讨论突出了几个重要因素，这些因素在基础模型开发中可能非常重要。

    Most state-of-the-art techniques for medical image segmentation rely on deep-learning models. These models, however, are often trained on narrowly-defined tasks in a supervised fashion, which requires expensive labeled datasets. Recent advances in several machine learning domains, such as natural language generation have demonstrated the feasibility and utility of building foundation models that can be customized for various downstream tasks with little to no labeled data. This likely represents a paradigm shift for medical imaging, where we expect that foundation models may shape the future of the field. In this paper, we consider a recently developed foundation model for medical image segmentation, UniverSeg. We conduct an empirical evaluation study in the context of prostate imaging and compare it against the conventional approach of training a task-specific segmentation model. Our results and discussion highlight several important factors that will likely be important in the develo
    
[^130]: 使用去噪扩散概率模型对分子图进行变分自动编码

    Variational Autoencoding Molecular Graphs with Denoising Diffusion Probabilistic Model. (arXiv:2307.00623v1 [cs.LG])

    [http://arxiv.org/abs/2307.00623](http://arxiv.org/abs/2307.00623)

    这篇论文提出了一种新颖的分子深度生成模型，将分层结构融入概率潜在向量中，并通过去噪扩散概率模型来设计有效的分子潜在向量，用于分子性质预测。

    

    在数据驱动的药物发现中，设计分子描述符是一个非常重要的任务。变分自动编码器(VAEs)等深度生成模型通过设计由分子结构导出的概率潜在向量作为描述符，提供了潜在的解决方案。这些模型可以在只有分子结构的大型数据集上进行训练，并应用于迁移学习。然而，通常VAE的潜在向量的近似后验分布假设为简单的多元高斯分布，而且协方差为零，这可能限制了表示潜在特征的性能。为了克服这个限制，我们提出了一种新颖的分子深度生成模型，将分层结构融入概率潜在向量中。我们通过去噪扩散概率模型(DDPM)实现了这一目标。通过一些实验证明了我们模型可以为分子性质预测设计出有效的分子潜在向量。

    In data-driven drug discovery, designing molecular descriptors is a very important task. Deep generative models such as variational autoencoders (VAEs) offer a potential solution by designing descriptors as probabilistic latent vectors derived from molecular structures. These models can be trained on large datasets, which have only molecular structures, and applied to transfer learning. Nevertheless, the approximate posterior distribution of the latent vectors of the usual VAE assumes a simple multivariate Gaussian distribution with zero covariance, which may limit the performance of representing the latent features. To overcome this limitation, we propose a novel molecular deep generative model that incorporates a hierarchical structure into the probabilistic latent vectors. We achieve this by a denoising diffusion probabilistic model (DDPM). We demonstrate that our model can design effective molecular latent vectors for molecular property prediction from some experiments by small dat
    
[^131]: 最大熵异质代理镜像学习

    Maximum Entropy Heterogeneous-Agent Mirror Learning. (arXiv:2306.10715v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2306.10715](http://arxiv.org/abs/2306.10715)

    最大熵异质代理镜像学习(MEHAML)是一种新的理论框架，通过最大熵原理设计了最大熵MARL的演员-评论家算法，具有联合最大熵目标的单调改进和收敛至中位响应均衡(QRE)的期望特性，并通过扩展常用的强化学习算法HASAC来验证其实用性和在探索和稳健性方面的显著改进。

    

    多智能体强化学习(MARL)在合作博弈中表现出有效性。然而，现有的最先进方法面临样本效率低、超参数脆弱性和收敛于次优纳什均衡的风险等挑战。为了解决这些问题，本文提出了一种新的理论框架，命名为最大熵异质代理镜像学习(MEHAML)，利用最大熵原理设计了最大熵MARL的演员-评论家算法。我们证明了从MEHAML框架导出的算法具有联合最大熵目标的单调改进和收敛至中位响应均衡(QRE)的期望特性。MEHAML的实用性通过开发广泛使用的强化学习算法HASAC的MEHAML扩展来展示，在三个具有挑战性的基准测试上展示出了探索和稳健性的显著提升。

    Multi-agent reinforcement learning (MARL) has been shown effective for cooperative games in recent years. However, existing state-of-the-art methods face challenges related to sample inefficiency, brittleness regarding hyperparameters, and the risk of converging to a suboptimal Nash Equilibrium. To resolve these issues, in this paper, we propose a novel theoretical framework, named Maximum Entropy Heterogeneous-Agent Mirror Learning (MEHAML), that leverages the maximum entropy principle to design maximum entropy MARL actor-critic algorithms. We prove that algorithms derived from the MEHAML framework enjoy the desired properties of the monotonic improvement of the joint maximum entropy objective and the convergence to quantal response equilibrium (QRE). The practicality of MEHAML is demonstrated by developing a MEHAML extension of the widely used RL algorithm, HASAC (for soft actor-critic), which shows significant improvements in exploration and robustness on three challenging benchmark
    
[^132]: 超越几何：使用动力相似性分析比较神经回路计算中的计算时间结构

    Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis. (arXiv:2306.10168v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2306.10168](http://arxiv.org/abs/2306.10168)

    本研究提出了一种新的方法，通过比较神经网络系统的动力学特征来判断它们是否利用了相同的内部过程进行计算。

    

    我们如何判断两个神经网络是否在特定计算中利用了相同的内部过程？这个问题对神经科学和机器学习的多个子领域都很重要，包括神经人工智能、机械解释性和脑机接口。比较神经网络的标准方法注重潜在状态的空间几何。然而，在循环网络中，计算是在神经动力学的层面上实现的，它们与几何没有简单的一对一映射关系。为了弥合这个差距，我们引入了一种新的相似度度量方法，它在动力学的层面上比较两个系统。我们的方法包括两个组成部分：使用最近数据驱动的动力系统理论的发展，我们学习一个能够准确捕捉原始非线性动力学核心特征的高维线性系统。接下来，我们通过一种新的Procrustes分析的扩展方法比较这些线性近似，该方法考虑了向量场的影响。

    How can we tell whether two neural networks are utilizing the same internal processes for a particular computation? This question is pertinent for multiple subfields of both neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of neural dynamics, which do not have a simple one-to-one mapping with geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics. Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare these linear approximations via a novel extension of Procrustes Analysis that accounts for how vector fields c
    
[^133]: 自然语言处理中社会人口统计偏见的调查

    Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])

    [http://arxiv.org/abs/2306.08158](http://arxiv.org/abs/2306.08158)

    本文调查了209篇关于NLP模型偏见的论文，其中大部分涉及社会人口统计偏见。研究者提出了社会人口统计偏见的定义，并确定了NLP偏见研究的三个主要类别。当前去偏见技术只是隐藏了偏见而不是真正去除它，需要进一步改进。

    

    深度神经网络在训练过程中往往会学习到非预期的偏见，这在实际应用中可能会产生有害的影响。本文对209篇关于NLP模型中偏见的论文进行了调查，其中大部分论文涉及社会人口统计偏见。为了更好地理解偏见与真实世界的危害之间的区别，我们借鉴心理学和行为经济学的思想，提出了社会人口统计偏见的定义。我们确定了NLP偏见研究的三个主要类别：偏见类型、量化偏见和去偏见。我们认为当前对于量化偏见的方法存在可靠性问题，许多偏见度量并不涉及真实世界中的偏见，当前的去偏见技术是表面的，只是隐藏了偏见，而不是真正去除它。最后，我们提供了未来工作的建议。

    Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
    
[^134]: 关于图神经网络中本地同质性水平的性能差异

    On Performance Discrepancies Across Local Homophily Levels in Graph Neural Networks. (arXiv:2306.05557v1 [cs.SI])

    [http://arxiv.org/abs/2306.05557](http://arxiv.org/abs/2306.05557)

    本文研究了GNN在测试时节点的本地同质性水平与其图的全局同质性水平偏离时的性能，并介绍一种新参数用于控制同质性，在生成的图中系统地研究本地同质性的影响。

    

    GNN的研究强调高同质性（即相似类节点相互连接的倾向）与节点分类的强预测性能之间的关系。然而，最近的研究发现这种关系更加微妙，证明即使简单的GNN也可以在某些异质性环境中学习。为了弥合这些发现之间的差距，我们重新思考了先前作品中的假设，并确定数据集经常被视为在节点间具有恒定的同质性水平。为了更接近真实世界的数据集，我们理论上和实证地研究了GNN在测试时节点的本地同质性水平与其图的全局同质性水平偏离时的性能。为了帮助我们的理论分析，我们在同质性分析中常用的优先附加模型中引入了一个新参数，以控制生成的图中的本地同质性水平，从而实现系统的实证研究，探究本地同质性的影响。

    Research on GNNs has highlighted a relationship between high homophily (i.e., the tendency for nodes of a similar class to connect) and strong predictive performance in node classification. However, recent research has found the relationship to be more nuanced, demonstrating that even simple GNNs can learn in certain heterophilous settings. To bridge the gap between these findings, we revisit the assumptions made in previous works and identify that datasets are often treated as having a constant homophily level across nodes. To align closer to real-world datasets, we theoretically and empirically study the performance of GNNs when the local homophily level of a node deviates at test-time from the global homophily level of its graph. To aid our theoretical analysis, we introduce a new parameter to the preferential attachment model commonly used in homophily analysis to enable the control of local homophily levels in generated graphs, enabling a systematic empirical study on how local ho
    
[^135]: 大型长序列模型的块级并行Transformer

    Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])

    [http://arxiv.org/abs/2305.19370](http://arxiv.org/abs/2305.19370)

    本文提出了块级并行Transformer方法，以最小化内存成本，能够处理长序列，并且可以处理比先前的内存高效方法更长32倍的训练序列。

    

    Transformer已经成为最先进的自然语言处理模型的基石，在各种AI应用中展现出出色的性能。然而，Transformer中的自我注意机制和大型前馈网络所需的内存容量限制了它们处理长序列的能力，从而为涉及多个长序列或长期依赖的任务带来了挑战。我们提出了一种独特的方法，块级并行Transformer（BPT），它利用块级计算自我注意和前馈网络融合以最小化内存成本。通过在保持内存效率的同时处理更长的输入序列，BPT使训练序列的长度比原始的Transformer长32倍，比先前的内存高效方法长2到4倍。对语言建模和强化学习任务进行的大量实验证明了BPT在减少内存需求和提高性能方面的有效性。

    Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
    
[^136]: FairDP: 具有差分隐私认证的公平性保障

    FairDP: Certified Fairness with Differential Privacy. (arXiv:2305.16474v1 [cs.LG])

    [http://arxiv.org/abs/2305.16474](http://arxiv.org/abs/2305.16474)

    FairDP是一种同时确保差分隐私和公平性的新型机制，通过独立为不同的个体群体训练模型，在训练过程中逐步整合来自群体模型的知识，制定综合模型以平衡隐私、效用和公平性的下游任务。相比现有方法，FairDP展示了更好的模型效益、隐私和公平性的权衡。

    

    本文介绍了一种名为FairDP的新型机制，旨在同时确保差分隐私(DP)和公平性。FairDP通过独立为不同的个体群体训练模型，在使用组特定的剪裁项来评估和限制DP的差异影响的同时操作。在训练过程中，该机制逐步整合来自群体模型的知识，制定综合模型以平衡隐私、效用和公平性的下游任务。广泛的理论和实证分析验证了FairDP的功效，与现有方法相比，展示了更好的模型效益、隐私和公平性的权衡。

    This paper introduces FairDP, a novel mechanism designed to simultaneously ensure differential privacy (DP) and fairness. FairDP operates by independently training models for distinct individual groups, using group-specific clipping terms to assess and bound the disparate impacts of DP. Throughout the training process, the mechanism progressively integrates knowledge from group models to formulate a comprehensive model that balances privacy, utility, and fairness in downstream tasks. Extensive theoretical and empirical analyses validate the efficacy of FairDP, demonstrating improved trade-offs between model utility, privacy, and fairness compared with existing methods.
    
[^137]: SMT 2.0：一个关注层次和混合变量高斯过程的代理模型工具包

    SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and Mixed Variables Gaussian Processes. (arXiv:2305.13998v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13998](http://arxiv.org/abs/2305.13998)

    SMT 2.0是一个开源的代理模型工具包，引入了处理混合变量和层次变量的能力，并通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。

    

    Surrogate Modeling Toolbox (SMT)是一个开源的Python包，提供了一系列代理建模方法、采样技术和一套示例问题。本文介绍了SMT 2.0，这是SMT的一个重要新版本，引入了显著的升级和新功能。这个版本增加了处理混合变量代理模型和层次变量的能力。这些类型的变量在多个代理建模应用中变得越来越重要。SMT 2.0还通过扩展采样方法、添加新的代理模型以及计算Kriging的方差和核导数来改进了SMT。这个版本还包括了处理带噪声和使用多保真度数据的新函数。据我们所知，SMT 2.0是第一个提出层次和混合输入的开源代理库。这个开源软件采用New BSD许可证进行分发。

    The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.
    
[^138]: 通过等压等温流获得吉布斯自由能

    Gibbs free energies via isobaric-isothermal flows. (arXiv:2305.13233v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.13233](http://arxiv.org/abs/2305.13233)

    采用机器学习模型利用等压等温流得到吉布斯自由能，并在单原子水的结晶中进行了测试，表现出优秀的性能。

    

    我们提出了一种基于归一化流的机器学习模型，该模型经过训练可从等压等温（NPT）集合中进行采样。在我们的方法中，我们采用近似方法来得到完全灵活的三斜晶系统的联合分布和粒子坐标以达到所需的内部压力。我们对单原子水在立方和六角冰相中进行测试，并发现与已建立的基线相比，吉布斯自由能和其他可观测量的结果完全一致。

    We present a machine-learning model based on normalizing flows that is trained to sample from the isobaric-isothermal (NPT) ensemble. In our approach, we approximate the joint distribution of a fully-flexible triclinic simulation box and particle coordinates to achieve a desired internal pressure. We test our model on monatomic water in the cubic and hexagonal ice phases and find excellent agreement of Gibbs free energies and other observables compared with established baselines.
    
[^139]: 机器学习公平性的因果关联权衡分析

    Causality-Aided Trade-off Analysis for Machine Learning Fairness. (arXiv:2305.13057v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13057](http://arxiv.org/abs/2305.13057)

    本论文使用因果分析作为一种方法，通过分析机器学习流程中公平性参数与其他关键指标的权衡，提供了一种系统理解和决策基础，帮助开发者在提供公平机器学习服务时做出明智的决策。

    

    在增强机器学习公平性方面，越来越多的关注点涌现出来。尽管有越来越多的提升公平性的方法，但我们对应用这些方法时，在考虑因素之间的权衡缺乏系统性的理解。这种理解对于开发者做出关于提供公平机器学习服务的明智决策至关重要。然而，当存在多个公平性参数和其他关键指标，并且彼此之间存在耦合甚至冲突时，分析这些权衡是极其困难的。本文提出使用因果分析作为一种原则性方法，在机器学习流程中分析公平性参数与其他关键指标之间的权衡。为了实际有效地进行因果分析，我们提出了一组领域特定的优化方法，以促进准确的因果发现，并基于成熟的因果推断提出了一个统一的、新颖的权衡分析接口。

    There has been an increasing interest in enhancing the fairness of machine learning (ML). Despite the growing number of fairness-improving methods, we lack a systematic understanding of the trade-offs among factors considered in the ML pipeline when fairness-improving methods are applied. This understanding is essential for developers to make informed decisions regarding the provision of fair ML services. Nonetheless, it is extremely difficult to analyze the trade-offs when there are multiple fairness parameters and other crucial metrics involved, coupled, and even in conflict with one another.  This paper uses causality analysis as a principled method for analyzing trade-offs between fairness parameters and other crucial metrics in ML pipelines. To ractically and effectively conduct causality analysis, we propose a set of domain-specific optimizations to facilitate accurate causal discovery and a unified, novel interface for trade-off analysis based on well-established causal inferenc
    
[^140]: DClEVerNet: 深度组合学习优化大规模网络化充电设施的高效电动汽车充电调度

    DClEVerNet: Deep Combinatorial Learning for Efficient EV Charging Scheduling in Large-scale Networked Facilities. (arXiv:2305.11195v1 [cs.LG])

    [http://arxiv.org/abs/2305.11195](http://arxiv.org/abs/2305.11195)

    本文提出了一种基于深度学习和近似算法技术的数据驱动优化框架DClEVerNet，可以优化大规模网络化的EV充电站的预约管理程序，最大化EV用户的总福利收益，同时考虑到网络的可用功率容量和站点的入住限制。

    

    随着交通电气化，电动汽车（EV）的普及可能会显着增加配电网络的压力，导致其性能下降和稳定性受到威胁。为了以经济有效的方式容纳这些新负载，现代电力网络需要协调或“智能”充电策略，能够在一个可伸缩和高效的方式下优化EV充电调度。为此，本文重点研究大规模、网络化EV充电站的预约管理程序。我们制定了一个时耦合的二进制优化问题，最大化EV用户的总福利收益，同时考虑到网络的可用功率容量和站点的入住限制。为了在保持高解决质量的同时大规模解决问题，引入了一个基于深度学习和近似算法技术的数据驱动优化框架。该框架的关键因素是一种新颖的输入输出处理方案。

    With the electrification of transportation, the rising uptake of electric vehicles (EVs) might stress distribution networks significantly, leaving their performance degraded and stability jeopardized. To accommodate these new loads cost-effectively, modern power grids require coordinated or ``smart'' charging strategies capable of optimizing EV charging scheduling in a scalable and efficient fashion. With this in view, the present work focuses on reservation management programs for large-scale, networked EV charging stations. We formulate a time-coupled binary optimization problem that maximizes EV users' total welfare gain while accounting for the network's available power capacity and stations' occupancy limits. To tackle the problem at scale while retaining high solution quality, a data-driven optimization framework combining techniques from the fields of Deep Learning and Approximation Algorithms is introduced. The framework's key ingredient is a novel input-output processing schem
    
[^141]: 分布式鲁棒的离线强化学习：基于双重悲观性的通用算法和强健部分覆盖

    Double Pessimism is Provably Efficient for Distributionally Robust Offline Reinforcement Learning: Generic Algorithm and Robust Partial Coverage. (arXiv:2305.09659v1 [cs.LG])

    [http://arxiv.org/abs/2305.09659](http://arxiv.org/abs/2305.09659)

    本论文提出了一个名为P2MPO的算法框架，用于解决基于鲁棒离线RL的问题。该框架结合了灵活的模型估计子例程和双重悲观的策略优化步骤，采用双重悲观性原则以克服模型偏移等问题。研究表明，在模型准确性的假设下，该框架在拥有良好的鲁棒部分覆盖数据的情况下是具备高效性的。

    

    本文研究了分布式鲁棒的离线强化学习（鲁棒离线RL），其旨在从离线数据集中纯粹地找到一个能够在扰动环境中表现良好的最优强鲁棒策略。我们提出了一个名为P2MPO的算法框架，其中包含了灵活的模型估计子例程和双重悲观的策略优化步骤。双重悲观性原则对于克服由行为策略和目标策略家族之间的不匹配以及名义模型的扰动所引起的分布偏移至关重要。在对模型估计子例程进行一定准确性假设的情况下，我们证明了P2MPO算法在拥有良好的鲁棒部分覆盖数据的情况下是可证明有效的。

    We study distributionally robust offline reinforcement learning (robust offline RL), which seeks to find an optimal robust policy purely from an offline dataset that can perform well in perturbed environments. We propose a generic algorithm framework \underline{D}oubly \underline{P}essimistic \underline{M}odel-based \underline{P}olicy \underline{O}ptimization ($\texttt{P}^2\texttt{MPO}$) for robust offline RL, which features a novel combination of a flexible model estimation subroutine and a doubly pessimistic policy optimization step. The \emph{double pessimism} principle is crucial to overcome the distributional shift incurred by i) the mismatch between behavior policy and the family of target policies; and ii) the perturbation of the nominal model. Under certain accuracy assumptions on the model estimation subroutine, we show that $\texttt{P}^2\texttt{MPO}$ is provably efficient with \emph{robust partial coverage data}, which means that the offline dataset has good coverage of the d
    
[^142]: 设计不连续性

    Designing Discontinuities. (arXiv:2305.08559v1 [cs.IT])

    [http://arxiv.org/abs/2305.08559](http://arxiv.org/abs/2305.08559)

    本文通过一种量化理论方法优化不连续变量的设计，以平衡效应大小的增益和损失，并开发了一种计算效率高的强化学习算法。

    

    不连续性可以是相当任意的，但也会在社会系统中产生重大影响。事实上，它们的任意性是为什么它们被用于推断在许多情况下变量之间的因果关系。计量经济学中的回归不连续性假定存在一个不连续的变量，将人口分成不同的部分，以估计给定现象的因果效应。在这里，我们考虑为给定的不连续变量设计分区以优化以前使用回归不连续性研究过的某种效果。为此，我们提出了一种量化理论方法来优化感兴趣的效果，首先学习给定不连续变量的因果效应大小，然后应用动态规划来优化不连续性的量化设计，以平衡增益和损失的效应大小。我们还开发了一种计算效率高的强化学习算法，用于形成动态规划公式。

    Discontinuities can be fairly arbitrary but also cause a significant impact on outcomes in social systems. Indeed, their arbitrariness is why they have been used to infer causal relationships among variables in numerous settings. Regression discontinuity from econometrics assumes the existence of a discontinuous variable that splits the population into distinct partitions to estimate the causal effects of a given phenomenon. Here we consider the design of partitions for a given discontinuous variable to optimize a certain effect previously studied using regression discontinuity. To do so, we propose a quantization-theoretic approach to optimize the effect of interest, first learning the causal effect size of a given discontinuous variable and then applying dynamic programming for optimal quantization design of discontinuities that balance the gain and loss in the effect size. We also develop a computationally-efficient reinforcement learning algorithm for the dynamic programming formul
    
[^143]: 对预训练语言模型在非平稳环境下对代码进行持续学习以实现超出分布的泛化

    On the Usage of Continual Learning for Out-of-Distribution Generalization in Pre-trained Language Models of Code. (arXiv:2305.04106v1 [cs.SE])

    [http://arxiv.org/abs/2305.04106](http://arxiv.org/abs/2305.04106)

    本文强调预训练语言模型需要对代码进行持续学习以适应变化的软件数据分布，使其具有更好的泛化能力。

    

    学习预训练语言模型（PLMs）已成为深度学习代码中的普遍技术，利用两阶段的预训练和微调过程获取关于代码的通用知识并专门从事各种下游任务。然而，软件代码库的动态性对PLMs的有效性和鲁棒性构成挑战。本文强调需要调整适应代码的PLMs，适应分布会随时间变化的软件数据，这是之前的研究所忽视的一个关键问题。本文的动机是将PLM视为一个在非平稳环境下的模型，有助于模型在应对演化的软件数据时具有更好的泛化能力。

    Pre-trained language models (PLMs) have become a prevalent technique in deep learning for code, utilizing a two-stage pre-training and fine-tuning procedure to acquire general knowledge about code and specialize in a variety of downstream tasks. However, the dynamic nature of software codebases poses a challenge to the effectiveness and robustness of PLMs. In particular, world-realistic scenarios potentially lead to significant differences between the distribution of the pre-training and test data, i.e., distribution shift, resulting in a degradation of the PLM's performance on downstream tasks. In this paper, we stress the need for adapting PLMs of code to software data whose distribution changes over time, a crucial problem that has been overlooked in previous works. The motivation of this work is to consider the PLM in a non-stationary environment, where fine-tuning data evolves over time according to a software evolution scenario. Specifically, we design a scenario where the model 
    
[^144]: CVRecon: 重新思考神经重建的3D几何特征学习

    CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])

    [http://arxiv.org/abs/2304.14633](http://arxiv.org/abs/2304.14633)

    研究团队提出了一种基于代价体的3D神经重建框架CVRecon，利用丰富的几何嵌入来促进3D几何特征学习。通过引入射线上下文补偿代价体（RCCV），有效提高了视角相关信息的完整性和鲁棒性，并在各种度量方面显着提高了重建质量。

    

    最近使用图像序列进行神经重建的进展取得了显着进展。但是，由于缺乏深度信息，现有的基于体积的技术仅沿整个相机光线复制对象表面的2D图像特征。我们认为这种复制会在空洞和遮挡空间中引入噪声，从而产生高质量的3D几何体成形方面产生挑战。受传统多视角立体方法的启发，我们提出了一种端到端的3D神经重建框架CVRecon，旨在利用代价体中丰富的几何嵌入来促进3D几何特征学习。此外，我们提出了一种新颖的3D几何特征表示法——射线上下文补偿代价体（RCCV），它具有更好的完整性和鲁棒性，可以编码视角相关信息。通过全面的实验，我们证明了我们的方法在各种度量方面显着提高了重建质量，并恢复了清晰的

    Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
    
[^145]: SAFE: 使用 Shard Graphs 进行机器遗忘

    SAFE: Machine Unlearning With Shard Graphs. (arXiv:2304.13169v1 [cs.LG])

    [http://arxiv.org/abs/2304.13169](http://arxiv.org/abs/2304.13169)

    本论文提出了一种使用 shard graph 进行机器遗忘的方法，以实现在最小化遗忘成本的情况下适应多样数据的大型模型，并取得了较高的准确性。

    

    我们提出了 Synergy Aware Forgetting Ensemble（SAFE）方法，该方法可以在最小化从训练模型中消除训练样本影响的预期成本的同时，适应各种数据的大型模型。这个过程也被称为选择性遗忘或遗忘，通常是通过将数据集分成碎片，对每个碎片进行完全独立的模型训练，然后将所得模型合成来进行的。增加碎片的数量可以降低遗忘的预期成本，但同时也增加了推理成本，并降低了模型的最终准确性，因为独立的模型训练过程中失去了样本之间的协同信息。SAFE 引入了 shard graph 的概念，它允许在训练过程中从其他碎片中引入有限的信息，以牺牲一定的预期遗忘成本增加明显的准确性，同时仍然实现完全消除残留影响。

    We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt large models on a diverse collection of data while minimizing the expected cost to remove the influence of training samples from the trained model. This process, also known as selective forgetting or unlearning, is often conducted by partitioning a dataset into shards, training fully independent models on each, then ensembling the resulting models. Increasing the number of shards reduces the expected cost to forget but at the same time it increases inference cost and reduces the final accuracy of the model since synergistic information between samples is lost during the independent model training. Rather than treating each shard as independent, SAFE introduces the notion of a shard graph, which allows incorporating limited information from other shards during training, trading off a modest increase in expected forgetting cost with a significant increase in accuracy, all while still attaining complete removal of resi
    
[^146]: 无标记时间分析模块化核探测器的物理约束深度学习方法

    Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning. (arXiv:2304.11930v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2304.11930](http://arxiv.org/abs/2304.11930)

    该研究描述了一种基于深度学习的新方法，用于模块化核探测器的无标记时间分析，其能够利用单个探测器内部时间相关性，实现有意义和准确的映射函数。

    

    脉冲时间是核仪器学中重要的话题，具有从高能物理到辐射成像的广泛应用。尽管高速模数转换器越来越发展和易于使用，但它们在核探测器信号处理中的潜在用途和优点仍不确定，部分原因是相关的时间算法还没有得到充分理解和利用。在本文中，我们提出了一种基于深度学习的新方法，用于模块化核探测器的时间分析，无需对事件数据进行显式的标记。通过利用单个探测器内部时间相关性，形成一个无标记损失函数和一个经过特殊设计的正则化器，以监督神经网络的训练，以获得有意义和准确的映射函数。我们从数学上证明了所需方法的最优函数的存在，并提供了一个系统的算法来训练和校准模型。该方法的应用在核能谱学中表现出较好的效果。

    Pulse timing is an important topic in nuclear instrumentation, with far-reaching applications from high energy physics to radiation imaging. While high-speed analog-to-digital converters become more and more developed and accessible, their potential uses and merits in nuclear detector signal processing are still uncertain, partially due to associated timing algorithms which are not fully understood and utilized. In this paper, we propose a novel method based on deep learning for timing analysis of modularized nuclear detectors without explicit needs of labelling event data. By taking advantage of the inner time correlation of individual detectors, a label-free loss function with a specially designed regularizer is formed to supervise the training of neural networks towards a meaningful and accurate mapping function. We mathematically demonstrate the existence of the optimal function desired by the method, and give a systematic algorithm for training and calibration of the model. The pr
    
[^147]: 采用多视图融合及两步式迁移学习增强的自动内镜结石识别方法

    Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning. (arXiv:2304.03193v1 [eess.IV])

    [http://arxiv.org/abs/2304.03193](http://arxiv.org/abs/2304.03193)

    本文介绍了一种利用多视图融合及两步式迁移学习增强的自动内镜结石识别方法，可提高肾结石分类准确度达6%以上。

    

    本文提出了一种深度学习方法，用于提取和融合从不同视角获取的图像信息，旨在为鉴定内镜图像中所见的肾结石类型产生更具区分性的物体特征。本模型进一步应用了两步迁移学习方法和注意力块来调整学习的特征图。深度特征融合策略将肾结石分类准确度比单视图提取主干模型提高了6%以上。

    This contribution presents a deep-learning method for extracting and fusing image information acquired from different viewpoints, with the aim to produce more discriminant object features for the identification of the type of kidney stones seen in endoscopic images. The model was further improved with a two-step transfer learning approach and by attention blocks to refine the learned feature maps. Deep feature fusion strategies improved the results of single view extraction backbone models by more than 6% in terms of accuracy of the kidney stones classification.
    
[^148]: 多孔晶态材料的等变网络

    Equivariant Networks for Porous Crystalline Materials. (arXiv:2304.01628v1 [cs.LG])

    [http://arxiv.org/abs/2304.01628](http://arxiv.org/abs/2304.01628)

    本研究开发了一种模型，它在架构中合并了晶体的单元格对称性，并显式地建模了多孔结构，可更准确地预测多孔晶体材料的吸附热。

    

    高效地预测多孔晶体材料的性质具有加速开发新材料的高通量筛选过程的巨大潜力，因为使用第一原理模型进行的模拟往往是计算密集型的。为了有效地利用深度学习方法来建模这些材料，我们需要利用晶体中存在的对称性，这些对称性由它们的空间群定义。现有的晶体性质预测方法要么具有过于严格的对称性限制，要么仅包括单元格之间的对称性。此外，这些模型没有明确地建模晶体的多孔结构。在本文中，我们开发了一种模型，它在其架构中合并了晶体的单元格对称性，并显式地建模了多孔结构。我们通过预测不同构型的莫尔定石沸石的CO$_2$吸附热来评估我们的模型。我们的结果证实，我们的方法在准确性和效率方面优于现有的晶体性质预测模型。

    Efficiently predicting properties of porous crystalline materials has great potential to accelerate the high throughput screening process for developing new materials, as simulations carried out using first principles model are often computationally expensive. To effectively make use of Deep Learning methods to model these materials, we need to utilize the symmetries present in the crystals, which are defined by their space group. Existing methods for crystal property prediction either have symmetry constraints that are too restrictive or only incorporate symmetries between unit cells. In addition, these models do not explicitly model the porous structure of the crystal. In this paper, we develop a model which incorporates the symmetries of the unit cell of a crystal in its architecture and explicitly models the porous structure. We evaluate our model by predicting the heat of adsorption of CO$_2$ for different configurations of the mordenite zeolite. Our results confirm that our metho
    
[^149]: Adam和AdamW优化器训练的深度神经网络损失函数的Lipschitz效应对泛化性能的影响

    Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v1 [cs.LG])

    [http://arxiv.org/abs/2303.16464](http://arxiv.org/abs/2303.16464)

    本文理论证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。本文的选择损失函数方针为Adam或AdamW优化算法的使用提供了指导。实验结果表明了Lipschitz常数较低且最大值较小的损失函数可以提高模型的泛化能力。

    

    机器学习中一个主要关注点是深度神经网络的泛化性能与优化算法之间的关系。本文证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。这些结果可作为选择损失函数时优化算法为Adam或AdamW的指导方针。本文选择了计算机视觉中的人脸年龄评估问题来评估理论界限在实际环境下的表现。为了更好地评估泛化能力，训练集和测试集从不同分布中选择。实验结果表明，Lipschitz常数较低且最大值较小的损失函数可以提高Adam或AdamW训练的模型的泛化能力。

    The generalization performance of deep neural networks with regard to the optimization algorithm is one of the major concerns in machine learning. This performance can be affected by various factors. In this paper, we theoretically prove that the Lipschitz constant of a loss function is an important factor to diminish the generalization error of the output model obtained by Adam or AdamW. The results can be used as a guideline for choosing the loss function when the optimization algorithm is Adam or AdamW. In addition, to evaluate the theoretical bound in a practical setting, we choose the human age estimation problem in computer vision. For assessing the generalization better, the training and test datasets are drawn from different distributions. Our experimental evaluation shows that the loss function with lower Lipschitz constant and maximum value improves the generalization of the model trained by Adam or AdamW.
    
[^150]: 机器学习在车联网通信中的QoS预测：挑战与解决方案的探讨

    Machine Learning for QoS Prediction in Vehicular Communication: Challenges and Solution Approaches. (arXiv:2302.11966v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2302.11966](http://arxiv.org/abs/2302.11966)

    本文探讨了机器学习在车联网通信中的QoS预测，重点讨论了采样过程、数据集特征分析、数据拆分效果和数据可用性等方面。这项研究对于提升最大吞吐量预测在汽车行业中的应用具有重要意义。

    

    随着蜂窝网络进化到第六代，机器学习被视为提升网络能力的关键技术。机器学习提供了一种预测系统的方法，可以使网络变得主动。网络的主动行为可以用于维持特定的服务质量要求。有了预测性的服务质量，出现了各种新的使用案例，尤其是在汽车行业中，涉及到安全和娱乐等方面。因此，在这项工作中，我们考虑了提高最大吞吐量的预测，如流媒体或高清地图应用。我们讨论了整个机器学习工作流程，强调了一些不太被关注的方面，如详细的采样过程、数据集特征的深入分析、提供结果时的数据拆分效果以及数据可用性。可靠的机器学习模型需要面对许多挑战。

    As cellular networks evolve towards the 6th generation, machine learning is seen as a key enabling technology to improve the capabilities of the network. Machine learning provides a methodology for predictive systems, which can make networks become proactive. This proactive behavior of the network can be leveraged to sustain, for example, a specific quality of service requirement. With predictive quality of service, a wide variety of new use cases, both safety- and entertainment-related, are emerging, especially in the automotive sector. Therefore, in this work, we consider maximum throughput prediction enhancing, for example, streaming or high-definition mapping applications. We discuss the entire machine learning workflow highlighting less regarded aspects such as the detailed sampling procedures, the in-depth analysis of the dataset characteristics, the effects of splits in the provided results, and the data availability. Reliable machine learning models need to face a lot of challe
    
[^151]: DTAAD: 双重TCN-Attention网络用于多变量时间序列数据的异常检测

    DTAAD: Dual Tcn-Attention Networks for Anomaly Detection in Multivariate Time Series Data. (arXiv:2302.10753v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10753](http://arxiv.org/abs/2302.10753)

    这项研究提出了一种基于Transformer和双重TCN-Attention网络的DTAAD模型，用于多变量时间序列数据的异常检测和诊断。通过集成设计和引入缩放方法和反馈机制，该模型实现了快速准确定位异常，并提高了预测精度和扩大了相关差异。

    

    异常检测技术能够对多变量时间序列数据进行有效的异常检测和诊断，对当今工业应用具有重要意义。然而，由于缺乏异常标签、数据的高维复杂性、实际硬件的内存瓶颈以及快速推理的需求，建立一个能够迅速准确定位的异常检测系统是一个具有挑战性的问题。本文提出了一种基于Transformer和双重时间卷积网络（TCN）的异常检测和诊断模型--DTAAD。我们的整体模型是一个集成设计，在此基础上，自回归模型（AR）与自编码器（AE）结构相结合，并引入了缩放方法和反馈机制，以提高预测精度和扩大相关差异。我们构建的双重TCN-Attention网络（DTA）在基准实验中仅使用了单层Transformer编码器。

    Anomaly detection techniques enable effective anomaly detection and diagnosis in multi-variate time series data, which are of major significance for today's industrial applications. However, establishing an anomaly detection system that can be rapidly and accurately located is a challenging problem due to the lack of outlier tags, the high dimensional complexity of the data, memory bottlenecks in the actual hardware, and the need for fast reasoning. We have proposed an anomaly detection and diagnosis model -- DTAAD in this paper, based on Transformer, and Dual Temporal Convolutional Network(TCN). Our overall model will be an integrated design in which autoregressive model(AR) combines autoencoder(AE) structures, and scaling methods and feedback mechanisms are introduced to improve prediction accuracy and expand correlation differences. Constructed by us, the Dual TCN-Attention Network (DTA) only uses a single layer of Transformer encoder in our baseline experiment, that belongs to an u
    
[^152]: 乐观的在线镜像下降算法用于连接随机性和对抗性在线凸优化

    Optimistic Online Mirror Descent for Bridging Stochastic and Adversarial Online Convex Optimization. (arXiv:2302.04552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04552](http://arxiv.org/abs/2302.04552)

    本论文研究了乐观的在线镜像下降算法在Stochastically Extended Adversarial (SEA)模型中的理论保证，对于凸和平滑的函数，其遗憾界限为O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，对于强凸和平滑的函数，其界限为O(sqrt(σ_{\max}^2) + sqrt(Σ_{\max}^2))。

    

    Sachs等人介绍了Stochastically Extended Adversarial (SEA)模型，作为随机性和对抗性在线凸优化的插值方法。在光滑条件下，他们证明了乐观的Follow-the-Regularized-Leader (FTRL)算法的期望遗憾依赖于凸函数的累积随机方差和累积对抗变化。对于强凸函数，他们也给出了基于最大随机方差和最大对抗变化的稍弱界限。受到他们的工作的启发，我们研究了乐观的在线镜像下降算法在SEA模型中的理论保证。对于凸且平滑的函数，我们得到了相同的遗憾界限，即O(sqrt(σ_{1:T}^2) + sqrt(Σ_{1:T}^2))，而不需要个别函数的凸性要求。对于强凸且平滑的函数，我们建立了一个O(sqrt(σ_{\max}^2) + sqrt(Σ_{\max}^2))的界限。

    Stochastically Extended Adversarial (SEA) model is introduced by Sachs et al. [2022] as an interpolation between stochastic and adversarial online convex optimization. Under the smoothness condition, they demonstrate that the expected regret of optimistic follow-the-regularized-leader (FTRL) depends on the cumulative stochastic variance $\sigma_{1:T}^2$ and the cumulative adversarial variation $\Sigma_{1:T}^2$ for convex functions. They also provide a slightly weaker bound based on the maximal stochastic variance $\sigma_{\max}^2$ and the maximal adversarial variation $\Sigma_{\max}^2$ for strongly convex functions. Inspired by their work, we investigate the theoretical guarantees of optimistic online mirror descent (OMD) for the SEA model. For convex and smooth functions, we obtain the same $\mathcal{O}(\sqrt{\sigma_{1:T}^2}+\sqrt{\Sigma_{1:T}^2})$ regret bound, without the convexity requirement of individual functions. For strongly convex and smooth functions, we establish an $\mathc
    
[^153]: Truveta Mapper：一个零样本本体映射框架

    Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09767](http://arxiv.org/abs/2301.09767)

    提出了一个将无监督本体匹配或本体对齐视为翻译任务的新视角的Truveta Mapper框架，在零样本、统一和端到端的方式下执行多本体对齐。该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案，无需显式跨本体手动标注数据。

    

    本文提出了一种将无监督本体匹配(Ontology Matching, OM)或本体对齐(Ontology Alignment, OA)视为翻译任务的新视角。将本体表示为图形，在源本体图中的节点到目标本体图中的路径之间进行翻译。所提出的Truveta Mapper (TM)框架利用多任务序列到序列转换器模型，在零样本、统一和端到端的方式下执行多本体对齐。多任务使模型能够通过迁移学习来隐含地学习不同本体之间的关系，无需任何显式的跨本体手动标注数据。这也使得该框架能够在运行时间延迟和对齐质量方面胜过现有解决方案。模型仅在公开可用的文本语料库和内部本体数据上进行预训练和微调。该方案优于现有标准基准解决方案，如Edit-Similarity和MINTE+。

    In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
    
[^154]: 分布式黑盒攻击云服务的图像分类器

    Distributed Black-box Attack against Image Classification Cloud Services. (arXiv:2210.16371v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16371](http://arxiv.org/abs/2210.16371)

    本文研究了分布式黑盒攻击云服务的图像分类器，通过直接应用于云API而不是本地模型，避免了之前研究中的错误，并利用负载平衡实现了攻击时间的减少。

    

    黑盒对抗攻击可以欺骗图像分类器，在不需要访问模型结构和权重的情况下对图像进行错误分类。最近的研究报告了超过95%的攻击成功率，查询次数少于1000次。然后产生了一个问题，黑盒攻击是否已经成为依赖云API实现图像分类的物联网设备的真正威胁。为了解决这个问题，值得注意的是之前的研究主要集中在提高成功率和减少查询次数。然而，对于黑盒攻击云API而言，攻击所需的时间也是一个关键因素。本文将黑盒攻击直接应用于云API，而不是本地模型，从而避免了之前研究中在图像编码和预处理之前应用扰动造成的错误。此外，我们利用负载平衡实现了分布式黑盒攻击，可以将攻击时间缩短约五倍。

    Black-box adversarial attacks can fool image classifiers into misclassifying images without requiring access to model structure and weights. Recent studies have reported attack success rates of over 95% with less than 1,000 queries. The question then arises of whether black-box attacks have become a real threat against IoT devices that rely on cloud APIs to achieve image classification. To shed some light on this, note that prior research has primarily focused on increasing the success rate and reducing the number of queries. However, another crucial factor for black-box attacks against cloud APIs is the time required to perform the attack. This paper applies black-box attacks directly to cloud APIs rather than to local models, thereby avoiding mistakes made in prior research that applied the perturbation before image encoding and pre-processing. Further, we exploit load balancing to enable distributed black-box attacks that can reduce the attack time by a factor of about five for both
    
[^155]: 对比视觉语言模型中的感知分组

    Perceptual Grouping in Contrastive Vision-Language Models. (arXiv:2210.09996v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09996](http://arxiv.org/abs/2210.09996)

    本文研究视觉语言模型是否能够理解物体在图像中的位置，并将视觉相关部分组合在一起。我们提出了一些修改，使模型独特地学习了语义和空间信息，并通过多个指标衡量了性能。

    

    最近零样本图像识别的进展表明，视觉语言模型学习了高度语义信息的通用视觉表示，可以通过自然语言短语进行任意探索。然而，理解图像不仅仅是理解图像中包含的内容，更重要的是了解这些内容所在的位置。本文研究了视觉语言模型能否理解物体在图像中的位置，并将视觉相关部分组合在一起。我们展示了基于对比损失和大规模网络数据的现代视觉和语言表示学习模型在有限的目标定位信息上的局限性。我们提出了一组最小的修改，使模型独特地学习了语义和空间信息。我们通过零样本图像识别、无监督自下而上和自上而下的语义分割以及鲁棒的目标定位来衡量模型的性能。

    Recent advances in zero-shot image recognition suggest that vision-language models learn generic visual representations with a high degree of semantic information that may be arbitrarily probed with natural language phrases. Understanding an image, however, is not just about understanding what content resides within an image, but importantly, where that content resides. In this work we examine how well vision-language models are able to understand where objects reside within an image and group together visually related parts of the imagery. We demonstrate how contemporary vision and language representation learning models based on contrastive losses and large web-based data capture limited object localization information. We propose a minimal set of modifications that results in models that uniquely learn both semantic and spatial information. We measure this performance in terms of zero-shot image recognition, unsupervised bottom-up and top-down semantic segmentations, as well as robu
    
[^156]: 分享的临床语音记录的重新识别风险

    Risk of re-identification for shared clinical speech recordings. (arXiv:2210.09975v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.09975](http://arxiv.org/abs/2210.09975)

    本研究探究了分享的临床语音记录的重新识别风险，发现风险与搜索空间大小和语音记录的性质有关。研究结果表明，使用说话人识别系统可以重新识别特定参与者。

    

    在医疗保健中利用基于语音的工具需要大规模的、经过筛选的数据集。由于制作这些数据集的成本昂贵，因此对数据共享产生了增加的兴趣。由于语音可能识别说话者（即声纹），分享录音引起了隐私问题。我们使用最先进的说话人识别系统，研究了无需参考人口统计或元数据的语音记录的重新识别风险。我们证明了风险与对手必须考虑的比较次数（即搜索空间）呈反相关关系。对于较小的搜索空间，风险较高，但随着搜索空间的增大（<1×10^6次比较），风险下降（精确度>0.85； >3×10^6次比较时，精确度<0.5）。接下来，我们展示了语音记录的性质如何影响重新识别的风险，非连续语音（例如音素延长）更难识别。我们的研究结果表明，说话人识别系统可以用于重新识别特定参与者。

    Large, curated datasets are required to leverage speech-based tools in healthcare. These are costly to produce, resulting in increased interest in data sharing. As speech can potentially identify speakers (i.e., voiceprints), sharing recordings raises privacy concerns. We examine the re-identification risk for speech recordings, without reference to demographic or metadata, using a state-of-the-art speaker recognition system. We demonstrate that the risk is inversely related to the number of comparisons an adversary must consider, i.e., the search space. Risk is high for a small search space but drops as the search space grows ($precision >0.85$ for $<1*10^{6}$ comparisons, $precision <0.5$ for $>3*10^{6}$ comparisons). Next, we show that the nature of a speech recording influences re-identification risk, with non-connected speech (e.g., vowel prolongation) being harder to identify. Our findings suggest that speaker recognition systems can be used to re-identify participants in specifi
    
[^157]: ISEE.U：具有不可预测目标的分布式在线主动目标定位

    ISEE.U: Distributed online active target localization with unpredictable targets. (arXiv:2210.09107v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09107](http://arxiv.org/abs/2210.09107)

    本文提出了一种分布式在线主动学习算法ISEE.U，用于不可预测目标的定位，无需参数调整且具有稳健性。通过局部估计的费舍尔信息矩阵，每个节点计算最大化整体目标位置精度的控制策略，与现有算法相比，在目标运动不规律时表现优秀且计算时间极少。

    

    本文提出了一种在线主动学习算法ISEE.U，通过在每个节点上进行分布式、简单和快速的计算来实现目标定位，无需调整参数，且每个节点对目标位置的估计期望渐进地等于中心化的最大似然估计器。ISEE.U利用每个节点的噪声距离，寻找最大化定位精度的控制策略。我们不对目标动态做出特定假设，因此，我们的方法在面对不可预测的目标时具有稳健性。每个节点通过局部估计的费舍尔信息矩阵计算最大化整体目标位置精度的控制策略。我们将所提出的方法与现有算法进行比较，在目标运动不遵循预定轨迹时表现优秀，甚至在我们的方法运行在单个中央CPU时，计算时间仅为其100分之一。

    This paper addresses target localization with an online active learning algorithm defined by distributed, simple and fast computations at each node, with no parameters to tune and where the estimate of the target position at each agent is asymptotically equal in expectation to the centralized maximum-likelihood estimator. ISEE.U takes noisy distances at each agent and finds a control that maximizes localization accuracy. We do not assume specific target dynamics and, thus, our method is robust when facing unpredictable targets. Each agent computes the control that maximizes overall target position accuracy via a local estimate of the Fisher Information Matrix. We compared the proposed method with a state of the art algorithm outperforming it when the target movements do not follow a prescribed trajectory, with x100 less computation time, even when our method is running in one central CPU.
    
[^158]: CrowdGuard：联邦学习中的联邦后门检测

    CrowdGuard: Federated Backdoor Detection in Federated Learning. (arXiv:2210.07714v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.07714](http://arxiv.org/abs/2210.07714)

    CrowdGuard是一种在联邦学习中有效防御后门攻击的新机制，通过利用客户对个别模型的反馈分析行为，克服了现有技术的不足。

    

    联邦学习是一种有前途的方法，使多个客户端在不分享本地训练数据的情况下协作训练深度神经网络（DNN）。然而，联邦学习容易受到后门（或有针对性的毒化）攻击的影响。这些攻击是由恶意客户端发起的，他们试图通过引入特定行为到学习模型中来破坏学习过程，这些行为可以由精心设计的输入触发。现有的联邦学习安全防护措施存在各种限制：它们仅限于特定的数据分布，或者由于排除良性模型或添加噪音而降低全局模型精度，易受到具有适应性防御意识的对手的攻击，或要求服务器访问本地模型，从而容易受到数据推断攻击。本文提出了一种新颖的防御机制CrowdGuard，它有效地减轻了联邦学习中的后门攻击，并克服了现有技术的不足之处。它利用客户对个别模型的反馈，分析模型的行为。

    Federated Learning (FL) is a promising approach enabling multiple clients to train Deep Neural Networks (DNNs) collaboratively without sharing their local training data. However, FL is susceptible to backdoor (or targeted poisoning) attacks. These attacks are initiated by malicious clients who seek to compromise the learning process by introducing specific behaviors into the learned model that can be triggered by carefully crafted inputs. Existing FL safeguards have various limitations: They are restricted to specific data distributions or reduce the global model accuracy due to excluding benign models or adding noise, are vulnerable to adaptive defense-aware adversaries, or require the server to access local models, allowing data inference attacks.  This paper presents a novel defense mechanism, CrowdGuard, that effectively mitigates backdoor attacks in FL and overcomes the deficiencies of existing techniques. It leverages clients' feedback on individual models, analyzes the behavior 
    
[^159]: 使用平衡传播的序列学习

    Sequence Learning Using Equilibrium Propagation. (arXiv:2209.09626v4 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2209.09626](http://arxiv.org/abs/2209.09626)

    本文提出了使用平衡传播（EP）进行序列学习的方法，EP是一种更符合生物可信性的学习框架，与传统的反向传播方法相比具有更广泛的应用性。文中利用现代Hopfield网络的进展，解决了基于EP的模型无法处理动态输入的问题，为复杂序列学习问题提供了解决方案。

    

    平衡传播（EP）是一种强大且更符合生物可信性的学习框架，可替代传统的反向传播方法。EP的有效性源于它仅依赖于局部计算，并且在训练的两个阶段中只需要一种计算单元，因此在生物启发的神经形态计算等领域具有更广泛的应用性。EP模型的动力学受能量函数控制，模型的内部状态随之收敛到稳定状态，遵循由同一函数定义的状态转换规则。然而，根据定义，EP要求模型的输入（收敛的循环神经网络）在训练的两个阶段中都是静态的。因此，不可能使用类似LSTM或GRU的架构设计基于EP的序列分类模型。本文利用现代Hopfield网络的最新进展，进一步理解基于能量的模型，并为复杂序列学习问题提供解决方案。

    Equilibrium Propagation (EP) is a powerful and more bio-plausible alternative to conventional learning frameworks such as backpropagation. The effectiveness of EP stems from the fact that it relies only on local computations and requires solely one kind of computational unit during both of its training phases, thereby enabling greater applicability in domains such as bio-inspired neuromorphic computing. The dynamics of the model in EP is governed by an energy function and the internal states of the model consequently converge to a steady state following the state transition rules defined by the same. However, by definition, EP requires the input to the model (a convergent RNN) to be static in both the phases of training. Thus it is not possible to design a model for sequence classification using EP with an LSTM or GRU like architecture. In this paper, we leverage recent developments in modern hopfield networks to further understand energy based models and develop solutions for complex 
    
[^160]: 使用最优传输和流形学习发现保守定律

    Discovering Conservation Laws using Optimal Transport and Manifold Learning. (arXiv:2208.14995v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2208.14995](http://arxiv.org/abs/2208.14995)

    本论文提出了一种使用最优传输和流形学习的非参数方法来发现复杂系统中的保守定律。通过测试在不同物理系统上，证明该方法能够确定保守定律的数量并提取其值，为分析系统动力学和构建稳定预测模型提供了新的可靠而解释性强的方法。

    

    保守定律是理解、表征和建模非线性动力系统的关键理论和实际工具。然而，对于许多复杂系统，很难确定相应的保守量，从而难以分析它们的动力学和构建稳定的预测模型。当前发现保守定律的方法通常依赖于详细的动力学信息或依赖于黑盒参数化深度学习方法。相反，我们将这个任务重新定义为流形学习问题，并提出了一种非参数方法来发现保守量。我们在各种物理系统上测试了这种新方法，并证明我们的方法能够确定保守量的数量并提取它们的值。利用最优传输理论和流形学习工具，我们提出的方法提供了一种直接的几何方法来确定保守定律，既稳健又可解释，而无需额外的动力学信息。

    Conservation laws are key theoretical and practical tools for understanding, characterizing, and modeling nonlinear dynamical systems. However, for many complex systems, the corresponding conserved quantities are difficult to identify, making it hard to analyze their dynamics and build stable predictive models. Current approaches for discovering conservation laws often depend on detailed dynamical information or rely on black box parametric deep learning methods. We instead reformulate this task as a manifold learning problem and propose a non-parametric approach for discovering conserved quantities. We test this new approach on a variety of physical systems and demonstrate that our method is able to both identify the number of conserved quantities and extract their values. Using tools from optimal transport theory and manifold learning, our proposed method provides a direct geometric approach to identifying conservation laws that is both robust and interpretable without requiring an e
    
[^161]: 通过分位数风险最小化实现可能的领域泛化

    Probable Domain Generalization via Quantile Risk Minimization. (arXiv:2207.09944v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.09944](http://arxiv.org/abs/2207.09944)

    该论文提出了一种通过Quantile Risk Minimization（QRM）方法实现可能的领域泛化的概率性框架。通过最小化预测器风险分布在不同领域上的分位数，该方法可以实现在测试时以高概率表现良好的预测器。

    

    领域泛化（DG）通过利用来自多个相关训练领域的数据，寻找在未见测试分布上表现良好的预测器。为了实现这一目标，DG通常被描述为对可能的领域集合进行平均或最坏情况下的问题。然而，平均情况下表现良好的预测器缺乏鲁棒性，而在最坏情况下表现良好的预测器往往过于保守。为解决这个问题，我们提出了一个新的概率性框架来进行DG，目标是学习以高概率表现良好的预测器。我们的关键思想是在训练过程中观察到的分布变化应该能够告诉我们测试时可能的分布变化，我们通过将训练和测试领域明确地视为从同一基础元分布中抽取的实现这一目标。为了实现可能的DG，我们提出了一个称为Quantile Risk Minimization（QRM）的新优化问题。通过最小化预测器风险分布在领域上的α-分位数，QRM可以实现概率上的DG。

    Domain generalization (DG) seeks predictors which perform well on unseen test distributions by leveraging data drawn from multiple related training distributions or domains. To achieve this, DG is commonly formulated as an average- or worst-case problem over the set of possible domains. However, predictors that perform well on average lack robustness while predictors that perform well in the worst case tend to be overly-conservative. To address this, we propose a new probabilistic framework for DG where the goal is to learn predictors that perform well with high probability. Our key idea is that distribution shifts seen during training should inform us of probable shifts at test time, which we realize by explicitly relating training and test domains as draws from the same underlying meta-distribution. To achieve probable DG, we propose a new optimization problem called Quantile Risk Minimization (QRM). By minimizing the $\alpha$-quantile of predictor's risk distribution over domains, Q
    
[^162]: 逆强化学习的主动探索方法

    Active Exploration for Inverse Reinforcement Learning. (arXiv:2207.08645v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08645](http://arxiv.org/abs/2207.08645)

    AceIRL提出了一种新的逆强化学习算法，通过主动探索来学习奖励函数和策略，在不需要环境生成模型的情况下，能够确定可行奖励函数的置信区间，并找到侧重于环境中最有信息的区域的探索策略。

    

    逆强化学习（IRL）是从专家演示中推断奖励函数的强大范式。许多IRL算法需要已知的转移模型，有时甚至需要已知的专家策略，或者至少需要访问生成模型。但是，这些假设对于许多实际应用来说太强了，因为只能通过顺序交互来访问环境。我们提出了一种新的IRL算法：主动探索逆强化学习（AceIRL），它主动探索未知环境和专家策略，快速学习专家的奖励函数并识别出一个好的策略。AceIRL使用先前的观察结果构建置信区间来捕捉可行的奖励函数，并找到侧重于环境中最有信息的区域的探索策略。AceIRL是第一种具有样本复杂度界限且不需要环境生成模型的主动IRL方法。

    Inverse Reinforcement Learning (IRL) is a powerful paradigm for inferring a reward function from expert demonstrations. Many IRL algorithms require a known transition model and sometimes even a known expert policy, or they at least require access to a generative model. However, these assumptions are too strong for many real-world applications, where the environment can be accessed only through sequential interaction. We propose a novel IRL algorithm: Active exploration for Inverse Reinforcement Learning (AceIRL), which actively explores an unknown environment and expert policy to quickly learn the expert's reward function and identify a good policy. AceIRL uses previous observations to construct confidence intervals that capture plausible reward functions and find exploration policies that focus on the most informative regions of the environment. AceIRL is the first approach to active IRL with sample-complexity bounds that does not require a generative model of the environment. AceIRL 
    
[^163]: FRAug: 通过表示增强解决非独立同分布特征的联邦学习问题

    FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation. (arXiv:2205.14900v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14900](http://arxiv.org/abs/2205.14900)

    本研究提出了一种名为FRAug的方法，用于解决联邦学习中具有非独立同分布特征的问题。该方法通过在嵌入空间中生成合成的客户端特定样本来增强客户端数据集。通过训练一个共享的生成模型，将来自不同特征分布的客户端的知识融合起来。

    

    联邦学习是一种去中心化的学习范式，在其中多个客户端协同训练深度学习模型，而不集中其本地数据，从而保护数据隐私。实际应用通常涉及不同客户端数据集之间的分布偏移，这会损害客户端对来自各自数据分布的未见样本的泛化能力。本研究解决了最近提出的特征偏移问题，其中客户端具有不同的特征分布，而标签分布相同。我们提出了Federated Representation Augmentation (FRAug)来解决这个实际且具有挑战性的问题。我们的方法在嵌入空间中生成合成的客户端特定样本，以增强通常较小的客户端数据集。为此，我们训练一个共享的生成模型，来融合客户端从不同特征分布中学到的知识。这个生成器合成客户无关的嵌入向量。

    Federated Learning (FL) is a decentralized learning paradigm, in which multiple clients collaboratively train deep learning models without centralizing their local data, and hence preserve data privacy. Real-world applications usually involve a distribution shift across the datasets of the different clients, which hurts the generalization ability of the clients to unseen samples from their respective data distributions. In this work, we address the recently proposed feature shift problem where the clients have different feature distributions, while the label distribution is the same. We propose Federated Representation Augmentation (FRAug) to tackle this practical and challenging problem. Our approach generates synthetic client-specific samples in the embedding space to augment the usually small client datasets. For that, we train a shared generative model to fuse the clients knowledge learned from their different feature distributions. This generator synthesizes client-agnostic embedd
    
[^164]: 量子和量子启发式算法中的差分隐私放大

    Differential Privacy Amplification in Quantum and Quantum-inspired Algorithms. (arXiv:2203.03604v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.03604](http://arxiv.org/abs/2203.03604)

    本研究提供了量子和量子启发式算法的差分隐私放大界限。首次展示了通过量子编码的经典数据集的算法或量子启发式经典采样结果能够放大差分隐私。研究还证明了量子版本的差分隐私可以通过满足混合条件的量子通道组合来放大。

    

    差分隐私为处理关于n个用户的数据集提供了一个理论框架，以确保输出对任何单个用户的信息泄露最小化。通常，通过添加噪声和进行子采样、洗牌、迭代、混合和扩散等多个过程来实现这种隐私保护。本研究提供了量子和量子启发式算法的隐私放大界限。具体而言，我们首次展示了在量子编码的经典数据集上运行的算法或量子启发式经典采样的结果能够放大差分隐私。此外，我们证明了量子版本的差分隐私通过满足一些混合条件，可以通过量子通道的组合来放大。

    Differential privacy provides a theoretical framework for processing a dataset about $n$ users, in a way that the output reveals a minimal information about any single user. Such notion of privacy is usually ensured by noise-adding mechanisms and amplified by several processes, including subsampling, shuffling, iteration, mixing and diffusion. In this work, we provide privacy amplification bounds for quantum and quantum-inspired algorithms. In particular, we show for the first time, that algorithms running on quantum encoding of a classical dataset or the outcomes of quantum-inspired classical sampling, amplify differential privacy. Moreover, we prove that a quantum version of differential privacy is amplified by the composition of quantum channels, provided that they satisfy some mixing conditions.
    
[^165]: 量子局部差分隐私和量子统计查询模型

    Quantum Local Differential Privacy and Quantum Statistical Query Model. (arXiv:2203.03591v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2203.03591](http://arxiv.org/abs/2203.03591)

    本文将量子统计查询和量子差分隐私在局部模型下进行了等价建立，扩展了经典结果到量子领域。在局部差分隐私下推导了量子相对熵的强数据处理不等式，并将其应用于受限测量的不对称假设测试任务。此外，还讨论了在局部差分隐私下的量子多方计算任务。

    

    量子统计查询提供了一个理论框架，用于研究有限量子资源的学习者的计算能力。在当前的背景下，这个模型特别相关，因为可用的量子设备受到严重噪声和限制量子存储的影响。另一方面，量子差分隐私的框架表明，在某些情况下，噪声可能有益于计算，增强鲁棒性和统计安全性。在这项工作中，我们建立了量子统计查询和局部差分隐私之间的等价关系，将一个著名的经典结果推广到量子设置中。此外，我们在局部差分隐私下推导出了量子相对熵的强数据处理不等式，并将此结果应用于受限测量的不对称假设测试任务。最后，我们考虑了在局部差分隐私下的量子多方计算任务。

    Quantum statistical queries provide a theoretical framework for investigating the computational power of a learner with limited quantum resources. This model is particularly relevant in the current context, where available quantum devices are subject to severe noise and have limited quantum memory. On the other hand, the framework of quantum differential privacy demonstrates that noise can, in some cases, benefit the computation, enhancing robustness and statistical security. In this work, we establish an equivalence between quantum statistical queries and quantum differential privacy in the local model, extending a celebrated classical result to the quantum setting. Furthermore, we derive strong data processing inequalities for the quantum relative entropy under local differential privacy and apply this result to the task of asymmetric hypothesis testing with restricted measurements. Finally, we consider the task of quantum multi-party computation under local differential privacy. As 
    
[^166]: 自主训练：一项综述

    Self-Training: A Survey. (arXiv:2202.12040v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.12040](http://arxiv.org/abs/2202.12040)

    自主训练方法是一种半监督算法，无需额外假设数据分布，在低密度区域找到决策边界，并使用学习分类器的输出分数作为置信度，通过为无标签样本分配伪标签，迭代地学习分类器，从而丰富有标签训练数据。

    

    半监督算法旨在从少量有标签观测和大量无标签观测中学习预测函数。由于这个框架在许多应用中是相关的，因此它们在学术界和工业界都受到了很多关注。在现有的技术中，自主训练方法在近年来无疑引起了更大的关注。这些模型旨在在低密度区域找到决策边界，而不对数据分布作出额外的假设，并使用学习分类器的无符号输出分数或其边界作为置信度的指标。自主训练算法的工作原理是通过给具有大于某个阈值的边界的无标签训练样本分配伪标签，迭代地学习分类器。然后，使用伪标记的示例来增强有标签训练数据，并与有标签训练集一起训练一个新的分类器。

    Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this
    
[^167]: 信息论引导的启发式渐进式多视角编码

    Information Theory-Guided Heuristic Progressive Multi-View Coding. (arXiv:2109.02344v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2109.02344](http://arxiv.org/abs/2109.02344)

    该论文从信息论的视角重新思考了现有的多视角学习范式，并提出了一种新的信息论框架，用于广义的多视角学习。在此框架的指导下，作者提出了一种三层递进的多视角编码方法。

    

    多视角表示学习从共享上下文的多个视角中捕捉综合信息。最近的研究直观地将对比学习（CL）应用于表示学习，被认为是一种成对方式，但仍然可扩展：在学习视角共享表示时，不会过滤特定于视角的噪声；虚假的负对，其中负项实际上与正项属于同一类，以及真实的负对被等同对待；均匀地测量术语之间的相似性可能干扰优化。重要的是，很少有研究针对广义自监督多视角学习的理论框架，尤其是针对超过两个视角的情况。为此，我们从信息理论的角度重新思考现有的多视角学习范式，然后提出了一种新颖的信息理论框架，用于广义多视角学习。在其指导下，我们构建了一种具有三层递进的多视角编码方法。

    Multi-view representation learning captures comprehensive information from multiple views of a shared context. Recent works intuitively apply contrastive learning (CL) to learn representations, regarded as a pairwise manner, which is still scalable: view-specific noise is not filtered in learning view-shared representations; the fake negative pairs, where the negative terms are actually within the same class as the positive, and the real negative pairs are coequally treated; and evenly measuring the similarities between terms might interfere with optimization. Importantly, few works research the theoretical framework of generalized self-supervised multi-view learning, especially for more than two views. To this end, we rethink the existing multi-view learning paradigm from the information theoretical perspective and then propose a novel information theoretical framework for generalized multi-view learning. Guided by it, we build a multi-view coding method with a three-tier progressive 
    
[^168]: 校准和改进图对比学习

    Calibrating and Improving Graph Contrastive Learning. (arXiv:2101.11525v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.11525](http://arxiv.org/abs/2101.11525)

    该论文提出了一种新的规则化方法，Contrast-Reg，通过应用期望校准误差（ECE）来校准和改进图对比学习算法，以提高在下游任务中的性能。

    

    图对比学习算法在节点分类、链接预测和图聚类等各种应用中取得了显著的成功。然而，在无监督图对比学习中，一些对比对可能与下游任务中的真相相矛盾，因此在这些对比对上减少损失会不希望地影响下游任务的性能。为了评估这些对比对在下游任务中的预测与真相之间的差异，我们将期望校准误差（ECE）应用于图对比学习。ECE的分析激发我们提出了一种新的规则化方法，Contrast-Reg，以确保减少对比损失能够在下游任务中取得更好的性能。作为插件式规则化器，Contrast-Reg有效地提高了现有图对比学习算法的性能。我们提供了理论和实证结果来证明Contrast-Reg的有效性。

    Graph contrastive learning algorithms have demonstrated remarkable success in various applications such as node classification, link prediction, and graph clustering. However, in unsupervised graph contrastive learning, some contrastive pairs may contradict the truths in downstream tasks and thus the decrease of losses on these pairs undesirably harms the performance in the downstream tasks. To assess the discrepancy between the prediction and the ground-truth in the downstream tasks for these contrastive pairs, we adapt the expected calibration error (ECE) to graph contrastive learning. The analysis of ECE motivates us to propose a novel regularization method, Contrast-Reg, to ensure that decreasing the contrastive loss leads to better performance in the downstream tasks. As a plug-in regularizer, Contrast-Reg effectively improves the performance of existing graph contrastive learning algorithms. We provide both theoretical and empirical results to demonstrate the effectiveness of Con
    
[^169]: MMD正则化的非平衡最优输运问题

    MMD-regularized Unbalanced Optimal Transport. (arXiv:2011.05001v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.05001](http://arxiv.org/abs/2011.05001)

    本文研究了使用MMD正则化的非平衡最优输运问题，提出了基于Fenchel对偶性的新度量方法，还提出了基于有限样本的凸规划用于估算问题，证明了估计量的一致性和误差速率。

    

    本文研究了非平衡最优输运（UOT）问题，其中使用最大平均偏差（MMD）正则化来实现边际约束。我们的研究动机在于观察到，现有的UOT研究主要关注基于$\phi$-散度（例如KL）的正则化。MMD作为互补的积分概率度量（IPM）家族之一，在UOT上的作用似乎不太被理解。我们的主要结果基于Fenchel对偶性，利用它我们能够研究MMD正则化的UOT（MMD-UOT）的特性。这种对偶结果的一个有趣结果是MMD-UOT诱导了一种新的度量方法，也属于IPM家族。此外，我们提出了基于有限样本的凸规划，用于估算MMD-UOT和相应的重心。在温和的条件下，我们证明了我们基于凸规划的估计量是一致的，而且估计误差以$\mathcal{O}(m^{-1/2})$的速率衰减。

    We study the unbalanced optimal transport (UOT) problem, where the marginal constraints are enforced using Maximum Mean Discrepancy (MMD) regularization. Our study is motivated by the observation that existing works on UOT have mainly focused on regularization based on $\phi$-divergence (e.g., KL). The role of MMD, which belongs to the complementary family of integral probability metrics (IPMs), as a regularizer in the context of UOT seems to be less understood. Our main result is based on Fenchel duality, using which we are able to study the properties of MMD-regularized UOT (MMD-UOT). One interesting outcome of this duality result is that MMD-UOT induces a novel metric over measures, which again belongs to the IPM family. Further, we present finite-sample-based convex programs for estimating MMD-UOT and the corresponding barycenter. Under mild conditions, we prove that our convex-program-based estimators are consistent, and the estimation error decays at a rate $\mathcal{O}\left(m^{-
    

