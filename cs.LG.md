# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Generalizing Dataset Distillation via Deep Generative Prior.](http://arxiv.org/abs/2305.01649) | 该方法提出一种基于预训练深度生成模型的学习先验的数据集蒸馏方法，通过在生成模型的潜在空间中将大量图像蒸馏为少量中间特征向量，显著提高了在所有设置中的跨体系结构的泛化能力。 |
| [^2] | [Differentially Private In-Context Learning.](http://arxiv.org/abs/2305.01639) | 本文提出了DP-ICL，实现了在隐私保证下对新任务的适应性。经过四个基准测试，发现其性能与非私有ICL相当。 |
| [^3] | [Sequence Modeling with Multiresolution Convolutional Memory.](http://arxiv.org/abs/2305.01638) | 本论文提出了一种新的用于序列建模的构建块，称为MultiresLayer，通过多分辨率卷积捕获输入序列中的多尺度趋势，既具有卷积网络的计算优势，又具有小波分解的有理论基础的动机。 |
| [^4] | [The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers.](http://arxiv.org/abs/2305.01628) | 本文提出一种新颖的方法，利用语言模型层之间的对比来改进文本生成输出，解决模型在开放式生成中的不良行为问题，并显著提高生成文本的质量。 |
| [^5] | [ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation.](http://arxiv.org/abs/2305.01618) | 本研究提出了一种基于视觉遥操作的数据收集方法以及学习手物互动先验的新方法，从而能够在联结目标和手部姿态估计中实现更好的关键点定位性能。 |
| [^6] | [AutoColor: Learned Light Power Control for Multi-Color Holograms.](http://arxiv.org/abs/2305.01611) | AutoColor 是第一个学习正确照明多色全息图所需光源功率的方法，将优化多色全息图所需的步骤数从超过1000个降至70个迭代步骤。 |
| [^7] | [Finding Neurons in a Haystack: Case Studies with Sparse Probing.](http://arxiv.org/abs/2305.01610) | 本文通过训练$k$-稀疏线性分类器以预测输入特征是否存在，研究了大型语言模型（LLM）内部神经元激活的表示方式；对不同层次的神经元网络的研究表明，早期层利用神经元的稀疏组合来表示多种特征，中间层有特定的神经元表示高级上下文特征，增加规模使特征表示更加稀疏化。 |
| [^8] | [The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold.](http://arxiv.org/abs/2305.01604) | 本文展示了多个深度网络的训练过程探索相同的低维流形，这些网络包括不同体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化，并揭示了网络初始化位置、体系结构和大小对流形的影响。 |
| [^9] | [On the Impact of Data Quality on Image Classification Fairness.](http://arxiv.org/abs/2305.01595) | 本文探讨了图像分类中训练数据质量和模型公平性之间的关系，通过在不同噪声级别的数据上测量公平性指标，揭示了数据噪声对于模型公平性的影响。 |
| [^10] | [Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees.](http://arxiv.org/abs/2305.01588) | 本文提出了针对梯度剪切的收敛保证机制，不再需要特定的阈值和强噪声假设，同时可以独立于步长选择，从而提高了收敛的自由度。 |
| [^11] | [Molecular design method based on novel molecular representation and variational auto-encoder.](http://arxiv.org/abs/2305.01580) | 本论文提出了一种基于SELFIES分子表示和改进后的变分自编码器模型的分子设计方法，用于生成更多种类的分子。 |
| [^12] | [NELoRa-Bench: A Benchmark for Neural-enhanced LoRa Demodulation.](http://arxiv.org/abs/2305.01573) | 本文介绍了一种名为NELoRa的神经增强解码器，用于提高低功耗广域网络中LoRa的信号噪声比（SNR）性能。数据集显示，NELoRa相比标准LoRa解码器可以实现1.84-2.35 dB的SNR增益。 |
| [^13] | [How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?.](http://arxiv.org/abs/2305.01555) | 本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。 |
| [^14] | [Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy.](http://arxiv.org/abs/2305.01550) | 本论文提出了一种利用强化学习方法缓解语言模型近似记忆化问题的新框架，使用负相似度评分作为奖励信号来学习差异策略。该方法能够有效解决显式和隐式假设所导致的数据结构不完全的问题。 |
| [^15] | [Accelerating Neural Self-Improvement via Bootstrapping.](http://arxiv.org/abs/2305.01547) | 本研究通过引入自助元学习加速神经网络的自我提高，提高了神经网络在少样本学习中的表现。 |
| [^16] | [Jacobian-Scaled K-means Clustering for Physics-Informed Segmentation of Reacting Flows.](http://arxiv.org/abs/2305.01539) | 本研究提出了一种受物理学影响的聚类策略，名为Jacobian-Scaled K-means聚类方法，能够不修改输入数据集即生成能够捕获动力学相似性区域的聚类。本方法具有一定的应用前景和实用价值。 |
| [^17] | [Empowering AI drug discovery with explicit and implicit knowledge.](http://arxiv.org/abs/2305.01523) | DeepEIK是一个统一的深度学习框架，利用显式和隐式知识进行AI药物发现，通过特征融合和注意力机制来提高模型预测准确性，并在多个任务上显著优于现有方法。 |
| [^18] | [Safe Deployment for Counterfactual Learning to Rank with Exposure-Based Risk Minimization.](http://arxiv.org/abs/2305.01522) | 该论文提出了一种新的反事实学习排序方法，通过基于曝光的风险正则化对IPS估计进行调整，以确保学习排名模型与给定的安全模型接近，从而在部署过程中降低风险。 |
| [^19] | [Unlocking the Power of Representations in Long-term Novelty-based Exploration.](http://arxiv.org/abs/2305.01521) | 本论文介绍了一种名为RECODE的非参数新颖性探索方法，它在深度强化学习中跟踪状态访问计数，并与新颖的逆动力学损失相结合，实现了在具有挑战性的任务中的最新最佳表现。 |
| [^20] | [Conditional Graph Information Bottleneck for Molecular Relational Learning.](http://arxiv.org/abs/2305.01520) | 本文提出了一种新的关系学习框架，称为CGIB，通过检测其中的核心子图，预测对图对之间的相互作用行为。 |
| [^21] | [BCEdge: SLO-Aware DNN Inference Services with Adaptive Batching on Edge Platforms.](http://arxiv.org/abs/2305.01519) | BCEdge是一种新颖的学习-based调度框架，使用自适应批处理和DNN推断服务进行并发执行，通过最大化效用来同时实现高吞吐量和低延迟，且优于现有的最先进方法。 |
| [^22] | [Defining Replicability of Prediction Rules.](http://arxiv.org/abs/2305.01518) | 本文提出了一种定义预测规则可复制性的方法，通过多代理人框架定义可复制性，旨在为机器学习中更系统的可复制性评估提供指导。 |
| [^23] | [MTrainS: Improving DLRM training efficiency using heterogeneous memories.](http://arxiv.org/abs/2305.01515) | 本文旨在研究现实中部署的深度学习推荐模型中嵌入表的带宽需求和局部性，并通过使用异构内存提出MTrainS来提高DLRM训练效率。 |
| [^24] | [A Parameter-free Adaptive Resonance Theory-based Topological Clustering Algorithm Capable of Continual Learning.](http://arxiv.org/abs/2305.01507) | 本文提出一种无需预设参数的ART拓扑聚类算法，通过引入参数估计方法实现持续学习，并在实验中证明其比现有聚类算法更优。 |
| [^25] | [Discovering the Effectiveness of Pre-Training in a Large-scale Car-sharing Platform.](http://arxiv.org/abs/2305.01506) | 本文针对大规模共享汽车平台上的汽车图像分析任务，通过分析实验验证了预训练技术对于提高模型性能的有效性。 |
| [^26] | [Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement.](http://arxiv.org/abs/2305.01481) | 通过测量模型和一个基础模型的潜在空间之间的一致性来提高机器学习模型的可靠性。 |
| [^27] | [On the properties of Gaussian Copula Mixture Models.](http://arxiv.org/abs/2305.01479) | 本文研究了高斯Copula混合模型（GCMM）的性质，开发了基于扩展期望最大算法的参数估计方法，并表明GCMM相比于GMM可以更好地拟合数据并实现更深入的数据挖掘。 |
| [^28] | [Cancer-inspired Genomics Mapper Model for the Generation of Synthetic DNA Sequences with Desired Genomics Signatures.](http://arxiv.org/abs/2305.01475) | 通过结合遗传算法和深度学习方法，我们提出了基于肿瘤启发的基因组测序映射模型（CGMM），该模型基于生物学关系和约束条件来生成具有所需基因组标志的合成DNA序列。 |
| [^29] | [Efficient Sensitivity Analysis for Parametric Robust Markov Chains.](http://arxiv.org/abs/2305.01473) | 本文提出了一种针对参数鲁棒马尔科夫链的高效敏感性分析方法，可以选择具有最高偏导数的$k$个参数子集并应用于包含超过一百万状态和数千个参数的大型模型中。 |
| [^30] | [Stochastic Contextual Bandits with Graph-based Contexts.](http://arxiv.org/abs/2305.01470) | 本文提出了一种基于图上下文的随机情境赌博机问题，其中节点标签相同的节点共享相同的奖励分布。对于线图和树，我们提出了一种具有遗憾界的算法。 |
| [^31] | [Memory of recurrent networks: Do we compute it right?.](http://arxiv.org/abs/2305.01457) | 本文研究了线性回声状态网络的记忆容量计算问题。通过发现数值评估的不准确性主要源于数值方面的问题，提出了基于掩码矩阵MC相对于中立性的稳健数值方法，该方法可以解决数值评估中的误差问题。 |
| [^32] | [Forecast reconciliation for vaccine supply chain optimization.](http://arxiv.org/abs/2305.01455) | 本文尝试通过层次时间序列来预测疫苗销售数据，并使用协调方法解决了不同层次预测不一致的问题，结果表明最小迹和加权最小二乘与结构缩放的协调方法效果最佳，提高了预测的一致性并减少预测误差。 |
| [^33] | [Unsupervised Feature Based Algorithms for Time Series Extrinsic Regression.](http://arxiv.org/abs/2305.01429) | 本研究提出了两种新的TSER算法：FreshPRINCE和DrCIF，它们分别由一组汇总特征和多个条件推理树构成，能够更好地在时间序列外源回归的问题上预测响应变量，比起以前的评估中使用的基线算法表现更佳。 |
| [^34] | [From Local to Global: Navigating Linguistic Diversity in the African Context.](http://arxiv.org/abs/2305.01427) | 摘要介绍了如何应对非洲语言多样性的挑战，并提出了模型作为产品教学的教学工具的想法，该方法对于寻求改善非洲本土方言客户体验和产品开发的企业有重要影响。 |
| [^35] | [Absolute integrability of Mercer kernels is only sufficient for RKHS stability.](http://arxiv.org/abs/2305.01411) | 本文证明了Mercer核的绝对可积性仅仅是RKHS稳定性的充分条件。 |
| [^36] | [Stress and heat flux via automatic differentiation.](http://arxiv.org/abs/2305.01401) | 本文报道了一种自动微分方法，用于计算机器学习势能中的应力和热通量，该方法在测试中取得了良好的效果。 |
| [^37] | [Get Back Here: Robust Imitation by Return-to-Distribution Planning.](http://arxiv.org/abs/2305.01400) | 本文提出的算法POIR将行为克隆和规划器相结合，解决了模仿学习中分布偏移的问题，模型在机器人操纵任务中表现出强大的鲁棒性。 |
| [^38] | [Are demographically invariant models and representations in medical imaging fair?.](http://arxiv.org/abs/2305.01397) | 医学影像模型编码患者人口统计信息，引发有关潜在歧视的担忧。研究表明，不编码人口属性的模型容易损失预测性能，而考虑人口统计属性的反事实模型不变性存在复杂性。人口统计学编码可以被认为是优势。 |
| [^39] | [Efficient Federated Learning with Enhanced Privacy via Lottery Ticket Pruning in Edge Computing.](http://arxiv.org/abs/2305.01387) | 本文提出了一种新的联邦学习方法，名为Fed-LTP，该方法采用彩票卷积稀疏网络和零集中差分隐私算法，能够高效实现隐私保护以及资源约束的移动终端协作学习。 |
| [^40] | [Class based Influence Functions for Error Detection.](http://arxiv.org/abs/2305.01384) | 该论文提出了基于类别信息的影响函数来提高异常检测的稳定性和性能。 |
| [^41] | [Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees.](http://arxiv.org/abs/2305.01381) | 本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。 |
| [^42] | [LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees.](http://arxiv.org/abs/2305.01379) | 本文提出了一种新的图形学习模型LogSpecT及其实际公式rLogSpecT，以解决现有模型rSpecT敏感超参数选择、不可行的问题。本文提供了rLogSpecT的恢复保证，并提出了基于L-ADMM的高效算法。 |
| [^43] | [Random Function Descent.](http://arxiv.org/abs/2305.01377) | 本文提出了随机函数下降(RFD)算法，可以在随机环境中计算出步长并且与贝叶斯优化中的梯度下降算法相同。在合成基准测试中，RFD算法比未调整的Adam方法表现更好，提出的heuristic扩展可与调整后的Adam方法相媲美。 |
| [^44] | [Physics-Informed Learning Using Hamiltonian Neural Networks with Output Error Noise Models.](http://arxiv.org/abs/2305.01338) | 本文介绍了一种输出误差Hamiltonian神经网络（OE-HNN）建模方法，以解决具有输入和噪声状态测量的物理系统建模问题。OE-HNN通过使用嵌入式ODE求解器从带噪声的状态测量数据中学习动力学，并在三个物理系统上得到了准确的预测结果。 |
| [^45] | [Validation of massively-parallel adaptive testing using dynamic control matching.](http://arxiv.org/abs/2305.01334) | 本文提出了一种方法，使用随着测试而不断适应的匹配合成控制组，来区分动态并行测试中各种测试的因果效应。 |
| [^46] | [Projection-Free Online Convex Optimization with Stochastic Constraints.](http://arxiv.org/abs/2305.01333) | 该论文提出了一种用于处理带有随机约束的在线凸优化的无投影算法，其能够取得亚线性遗憾和约束违规的效果。 |
| [^47] | [An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework.](http://arxiv.org/abs/2305.01322) | 该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。 |
| [^48] | [An Improved Yaw Control Algorithm for Wind Turbines via Reinforcement Learning.](http://arxiv.org/abs/2305.01299) | 本文利用强化学习开发了一个偏航控制程序，以最小化偏航偏差，并优化重新分配偏航资源，使高速段得到优先处理，同时保持偏航使用率低，并成功降低了偏航偏差，增加了涡轮机额外利润，减少机械负载和延长了使用寿命。 |
| [^49] | [Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation.](http://arxiv.org/abs/2305.01281) | 本文提出了一种使用线性聚合的方法来解决无监督领域适应中的参数选择问题，并且展示了该算法的目标误差渐近不劣于未知最佳聚合的两倍，大规模实证研究表明该方法优于深度嵌入验证。 |
| [^50] | [DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning.](http://arxiv.org/abs/2305.01267) | 本文介绍了一种新的针对联邦学习系统的后门攻击模型：数据无关性的联邦学习服务器端后门攻击，即DABS。DABS通过直接修改全局模型来实现攻击，并在保持干净数据上的正常准确率的同时，实现了更高的攻击成功率。 |
| [^51] | [HTPS: Heterogeneous Transferring Prediction System for Healthcare Datasets.](http://arxiv.org/abs/2305.01252) | 本文提出了面向医疗数据集的异构预测系统HTPS，其通过特征工程和自动编码器实现特征嵌入和异构数据的知识转移。实验结果表明，HTPS在多种预测任务和数据集上的表现优于基准系统。 |
| [^52] | [MDENet: Multi-modal Dual-embedding Networks for Malware Open-set Recognition.](http://arxiv.org/abs/2305.01245) | 提出了多模态双嵌入网络（MDENet）用于恶意软件开放集识别，利用恶意软件图片和句子来增加恶意软件特征的多样性，以提升识别效果。通过引入新的训练策略，方法在两个公开数据集上表现优于现有最先进方法。 |
| [^53] | [Machine-Learned Invertible Coarse Graining for Multiscale Molecular Modeling.](http://arxiv.org/abs/2305.01243) | 本文提出了循环粗化（CCG）方法，通过一种统一方法解决了粗粒化模型构建和给定 CG 结构的细节恢复的问题，提供一种新的 CG 方法及无罕见事件的计算自由能的高效方法。 |
| [^54] | [AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis.](http://arxiv.org/abs/2305.01241) | 本文提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练并基于潜在空间的生成方法，从而实现高度逼真、富有表现力、并避免了伪像的手势生成。 |
| [^55] | [Dynamic Scheduling for Federated Edge Learning with Streaming Data.](http://arxiv.org/abs/2305.01238) | 本文提出了一种能够动态调度参与本地训练的设备子集的算法，以最大化其时间平均数据重要性，适用于联邦边缘学习系统，特别是当训练数据具有强时间相关性时 |
| [^56] | [CNS-Net: Conservative Novelty Synthesizing Network for Malware Recognition in an Open-set Scenario.](http://arxiv.org/abs/2305.01236) | CNS-Net是一种用于开放式场景下恶意软件识别的新颖神经网络，能够合成保守性新颖性来准确区分已知和未知的恶意软件家族。 |
| [^57] | [MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset.](http://arxiv.org/abs/2305.01211) | 本文介绍了一个多语言法律句子边界检测数据集MultiLegalSBD，包括6种语言的130,000个注释句子。在该数据集上，现有的SBD模型的表现不佳。作者训练了基于CRF、BiLSTM-CRF和transformers的单语和多语模型，并展示了在该领域中最先进的性能。他们的多语模型在零-shot测试中优于所有基线。 |
| [^58] | [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation.](http://arxiv.org/abs/2305.01210) | 本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。 |
| [^59] | [Exploration of Unranked Items in Safe Online Learning to Re-Rank.](http://arxiv.org/abs/2305.01202) | 本文提出了一种安全的在线学习排序算法，通过将当前排名中的一个项目与排名外的项目高效地交换来执行探索，并基于Kullback-Leibler上置信度界限（KL-UCB）对未排序项目进行乐观选择和安全的排序，以提高长期收益。 |
| [^60] | [Topic Shift Detection in Chinese Dialogues: Corpus and Benchmark.](http://arxiv.org/abs/2305.01195) | 本文注释了一个由1308个对话组成的中文自然话题对话语料库，以填补中文自然对话话题语料库的空白。并提出了一种基于分层对比学习的师生框架来预测没有回复的话题转移。 |
| [^61] | [Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data.](http://arxiv.org/abs/2305.01166) | 本研究提出了一种从带噪声数据中学习基于得分的生成式先验的方法，能够广泛应用于图像重建、图像修补、超分辨率等反问题，实现最先进的结果。 |
| [^62] | [Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels.](http://arxiv.org/abs/2305.01160) | 本论文提出了一种名为LC的新长尾识别方法，它能够更好地模拟真实标签分布，同时解决类别标签不平衡问题，从而在CIFAR-10，CIFAR-100和ImageNet基准数据集上显着优于现有方法。 |
| [^63] | [FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer.](http://arxiv.org/abs/2305.01154) | 本文介绍了一种名为FedAVO的算法，利用非洲秃鹫优化器选择最佳超参数来提高联邦学习中的通信效率，显著降低了与FL操作相关的通信成本。 |
| [^64] | [Early Classifying Multimodal Sequences.](http://arxiv.org/abs/2305.01151) | 本项研究针对多模态数据序列扩展了早期分类方法，实验证明其在AUC方面有高达8.7%的优势。 |
| [^65] | [Ripple Knowledge Graph Convolutional Networks For Recommendation Systems.](http://arxiv.org/abs/2305.01147) | 本文介绍了一种基于知识图谱的深度学习模型RKGCN，它能够动态分析用户的偏好并推荐出合适的物品。该模型在包括电影、书籍和音乐在内的三个真实世界的数据集上比5个基准模型表现更好。 |
| [^66] | [Understanding the Generalization Ability of Deep Learning Algorithms: A Kernelized Renyi's Entropy Perspective.](http://arxiv.org/abs/2305.01143) | 本论文提出了一种新的信息理论度量方法——基于核化Renyi熵，用于在不假设Lipschitz或凸性条件的前提下对SGD / SGLD等下降学习算法进行分析，旨在提高当前泛化误差界限的优化水平。 |
| [^67] | [Geometric Latent Diffusion Models for 3D Molecule Generation.](http://arxiv.org/abs/2305.01140) | 提出了三维分子生成的几何潜在扩散模型（GeoLDM），是分子几何领域的第一个潜在扩散模型，通过建立具有不变标量和等变张量的点结构潜在空间来捕捉其关键的旋转平移等变性约束，能够在多个分子生成基准测试中达到更好的性能。 |
| [^68] | [Stratified Adversarial Robustness with Rejection.](http://arxiv.org/abs/2305.01139) | 本文提出了一种新的防御方法——基于一致预测的拒绝对抗训练（CPR），用于构建鲁棒的选择性分类器。该方法可以在分层拒绝设置下进行对抗鲁棒分类，并且在实验中表现出很好的性能。 |
| [^69] | [PGrad: Learning Principal Gradients For Domain Generalization.](http://arxiv.org/abs/2305.01134) | 这篇论文提出了PGrad方法来学习主导梯度，提高模型在未知领域的泛化能力，可以忽略领域相关的噪声信号，该方法在DomainBed和WILDS基准测试中表现出较好的效果。 |
| [^70] | [Analysis of different temporal graph neural network configurations on dynamic graphs.](http://arxiv.org/abs/2305.01128) | 本文针对动态图上的时空依赖结构进行了分析，并比较了不同TGN模型在预测任务上的有效性。通过广泛的消融实验，探讨了不同设计选择对预测准确性的影响。 |
| [^71] | [Learning Controllable Adaptive Simulation for Multi-resolution Physics.](http://arxiv.org/abs/2305.01122) | 一种基于深度学习和图神经网络的可控自适应多分辨率物理模拟模型，通过优化适当的空间分辨率，将更多的计算资源分配给高度动态区域，降低计算成本并提高准确性。 |
| [^72] | [CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations.](http://arxiv.org/abs/2305.01118) | CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。 |
| [^73] | [Local and Global Contextual Features Fusion for Pedestrian Intention Prediction.](http://arxiv.org/abs/2305.01111) | 本研究提出了基于局部和全局上下文特征融合的行人意向预测方法，通过分析行人和交通上下文的视觉特征来提高自动驾驶汽车在道路上的安全性。 |
| [^74] | [Leveraging Language Representation for Material Recommendation, Ranking, and Exploration.](http://arxiv.org/abs/2305.01101) | 本文提出了一种新型材料发现框架，利用材料科学特定语言模型的自然语言嵌入作为材料的组成和结构特征进行表示，并且联合采用了表示相似性召回候选材料和基于多任务学习对候选材料进行目标属性排名的方案。通过这种方法，可以更好地探索广阔的材料搜索空间，并确定高性能候选材料。 |
| [^75] | [Logion: Machine Learning for Greek Philology.](http://arxiv.org/abs/2305.01099) | 该研究提出了基于机器学习的方法来解决希腊语语言学中的问题，成功利用BERT模型发现和纠正了抄写员在文本传递过程中未被发现的错误，并能在修复预现代手稿材料老化引起的信息缺失方面发挥作用。同时，在领域专家与模型合作时，最佳性能可以通过启示性建议实现。模型的注意力头似乎编码了预现代希腊语的选择性语法特征。 |
| [^76] | [A Novel Model for Driver Lane Change Prediction in Cooperative Adaptive Cruise Control Systems.](http://arxiv.org/abs/2305.01096) | 本文研究了合作自适应巡航控制系统中驾驶员变道预测问题。使用车到车通信技术，提供了多个车辆的加速度信息，通过LSTM模型的学习，当周围车辆数量增加时，准确性有所提高。 |
| [^77] | [LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application.](http://arxiv.org/abs/2305.01095) | 该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。 |
| [^78] | [Performative Prediction with Bandit Feedback: Learning through Reparameterization.](http://arxiv.org/abs/2305.01094) | 本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。 |
| [^79] | [Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems.](http://arxiv.org/abs/2305.01090) | 本文提出了一种自动编码器框架，结合implicit regularization，内部线性层和L2正则化（权重衰减）自动估计数据集的潜在维度，产生正交的流形坐标系，并提供环境空间和流形空间之间的映射函数，从而允许进行样本之外的投影。该方法在动力系统数据集中表现出了较好的低秩表示效果，为底层动态提供了物理洞见，并可以用于提高机器学习模型和控制策略的效果。 |
| [^80] | [Computing Expected Motif Counts for Exchangeable Graph Generative Models.](http://arxiv.org/abs/2305.01089) | 本文提出了一种可扩展的估计过程，用于生成混合模型中期望的模体计数。 |
| [^81] | [Contextual Multilingual Spellchecker for User Queries.](http://arxiv.org/abs/2305.01082) | 本文提出了一个上下文多语种用户查询拼写检查器，它非常快速、可扩展，并根据特定产品的需求调整其词汇表和拼写输出，以满足用户的需求。 |
| [^82] | [Personalized Federated Learning under Mixture of Distributions.](http://arxiv.org/abs/2305.01068) | 该论文提出了一种新方法FedGMM，利用高斯混合模型处理了协变量漂移问题，提高了个性化联邦学习的性能。 |
| [^83] | [Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making.](http://arxiv.org/abs/2305.01063) | 本研究通过引入专业知识树算法，解决了集体决策中专业知识水平不同的问题，并在多个问题上进行了验证。 |
| [^84] | [semantic neural model approach for face recognition from sketch.](http://arxiv.org/abs/2305.01058) | 本文提出了一种基于语义神经网络模型的方法，可同时解决草图合成和识别问题。对待识别的人脸进行正面姿态、正常光照、中性表情和无遮挡的要求。 |
| [^85] | [LooPy: A Research-Friendly Mix Framework for Music Information Retrieval on Electronic Dance Music.](http://arxiv.org/abs/2305.01051) | LooPy是一种Python软件包，为MIR提供了一种面向电子舞曲的基础设施，可用于自动生成EDM音频，并提供了框架来构建专业级的模板，使用户可以从指定的旋律和和弦中呈现出制作精良的歌曲轨道或仅通过使用符号性的旋律生成器，提供具有多种风格的音轨。 |
| [^86] | [SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation.](http://arxiv.org/abs/2305.01050) | 本文研究使用BERT模型来标注侮辱性文本中的注释者不一致性，并比较直接训练和聚合两种方法，结果发现聚合方法比直接训练有更好的效果。 |
| [^87] | [Model-agnostic Measure of Generalization Difficulty.](http://arxiv.org/abs/2305.01034) | 该论文提出了第一个无特定模型的、量化机器学习测试泛化难度的方法——归纳偏差复杂度度量。该方法量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差，通常需要在许多维度上泛化的任务比涉及更少维度但要求更多细节的任务要困难得多。 |
| [^88] | [Company classification using zero-shot learning.](http://arxiv.org/abs/2305.01028) | 本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。该方法可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。 |
| [^89] | [Deception Detection with Feature-Augmentation by soft Domain Transfer.](http://arxiv.org/abs/2305.01011) | 本文提出了一种基于中间层表示的特征增强方法，通过软域转移进行领域间的关联，提高欺骗检测的准确率，分析结果显示推文是检测假新闻和钓鱼电子邮件最有帮助的信息提供者，新闻在推特谣言检测中最有帮助。 |
| [^90] | [Towards a Phenomenological Understanding of Neural Networks: Data.](http://arxiv.org/abs/2305.00995) | 该研究尝试通过集体变量来建立神经网络（NN）的理论，探究NN的学习过程。研究者提出两个变量，熵和经验神经切向核（NTK）的迹线，并实证分析发现这些变量与模型性能有关。随机网络提炼（RND）被用于优化数据选择，发现选择RND数据集能胜过随机选取，且RND数据集相关的集体变量更大。 |
| [^91] | [A novel algorithm can generate data to train machine learning models in conditions of extreme scarcity of real world data.](http://arxiv.org/abs/2305.00987) | 该论文提出了一种新的算法，可以在极度缺乏真实数据的情况下生成大型人造数据集以用于训练机器学习模型。该算法基于遗传算法，通过对随机生成的数据集的突变来训练神经网络，并通过引入选择性压力来优化生成的数据集。 |
| [^92] | [Attention-based Spatial-Temporal Graph Neural ODE for Traffic Prediction.](http://arxiv.org/abs/2305.00985) | 这篇论文提出了一个基于注意力机制的图神经ODE模型（ASTGODE），该模型可以解决交通预测问题。实验结果表明，ASTGODE模型在准确性方面优于现有的GNN模型。 |
| [^93] | [Two-phase Dual COPOD Method for Anomaly Detection in Industrial Control System.](http://arxiv.org/abs/2305.00982) | 本文提出了一个双阶段基于双Copula的离群检测方法，该方法具有优异的透明度、可解释性和计算效率，并在实际工控系统数据集上具有卓越的性能。 |
| [^94] | [Generalization for slowly mixing processes.](http://arxiv.org/abs/2305.00977) | 该论文研究了慢混合过程的泛化能力，给出了对由平稳且phi混合过程生成的数据的不同种类损失类别的一种上界。 |
| [^95] | [Deep Ensembles to Improve Uncertainty Quantification of Statistical Downscaling Models under Climate Change Conditions.](http://arxiv.org/abs/2305.00975) | 本文提出了使用深度集成作为一种简单的方法来改善统计降尺度模型的不确定性量化，以提供更优越的极端天气事件规划。深度集成可提供更好的风险评估，是应对气候变化的部门应用所迫切需要的。 |
| [^96] | [On the use of Deep Generative Models for Perfect Prognosis Climate Downscaling.](http://arxiv.org/abs/2305.00974) | 研究提出了使用生成模型以改善气候降尺度的空间一致性问题，从而更好地应对气候变化。 |
| [^97] | [Towards the Flatter Landscape and Better Generalization in Federated Learning under Client-level Differential Privacy.](http://arxiv.org/abs/2305.00873) | 提出了一种名为DP-FedSAM的算法，它利用梯度扰动来减轻差分隐私的负面影响，并将锐度感知优化器整合到算法中，生成更平缓的损失曲面和更好的权重扰动鲁棒性，从而提高联邦学习的性能。 |
| [^98] | [Revisiting Robustness in Graph Machine Learning.](http://arxiv.org/abs/2305.00851) | GNNs在节点层面预测中可能会因图结构小的更改而导致不稳健。本文提出了对抗性图的概念，并发现所有评估的GNN都表现出过度鲁棒性，包括超出语义变化点。在推理的图中包括训练图的标签-结构可以避免过度鲁棒性。基于对抗性概念的鲁棒性测试与最终主动学习表现相关。 |
| [^99] | [SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation.](http://arxiv.org/abs/2305.00795) | SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。 |
| [^100] | [Dynamic Transfer Learning across Graphs.](http://arxiv.org/abs/2305.00664) | 该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。 |
| [^101] | [Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition.](http://arxiv.org/abs/2305.00654) | 本文提出了一种基于奇异值分解的自动表征学习模型，可以获得保留转换结构的表示形式并捕捉状态访问的相对频率。该方法不需要转移矩阵，可以利用深度网络，适用于部分可观察领域，并且在多任务设置中表现良好。 |
| [^102] | [Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding.](http://arxiv.org/abs/2305.00633) | 本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。 |
| [^103] | [Reconstructing seen images from human brain activity via guided stochastic search.](http://arxiv.org/abs/2305.00556) | 本研究使用条件生成扩散模型，改进过去的视觉重建算法，通过对一小组图像的采样和编码模型的选择，实现了从人脑活动中高质量、保留语义内容的重建结果，并发现了视觉皮层不同区域的重建时间差异。 |
| [^104] | [The R\'io Hortega University Hospital Glioblastoma dataset: a comprehensive collection of preoperative, early postoperative and recurrence MRI scans (RHUH-GBM).](http://arxiv.org/abs/2305.00005) | 这份数据集提供了包括MRI图像、容积评估、分子数据和生存细节在内的Glioblastoma患者相关数据，同时提供了专家纠正的肿瘤亚区划分，为发展术后和随访MRI扫描的算法提供了有价值的基准数据。 |
| [^105] | [Automatic Generation of Labeled Data for Video-Based Human Pose Analysis via NLP applied to YouTube Subtitles.](http://arxiv.org/abs/2304.14489) | 该研究利用NLP技术分析健身视频字幕数据自动生成人体姿势分析的标记数据集。 |
| [^106] | [JaxPruner: A concise library for sparsity research.](http://arxiv.org/abs/2304.14082) | 本文介绍了JaxPruner，一款用于研究稀疏神经网络的开源库。JaxPruner提供了流行的剪枝和稀疏训练算法的简明实现，最小化内存和延迟开销，并可轻松集成到现有的JAX库中。 |
| [^107] | [Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture.](http://arxiv.org/abs/2304.12985) | RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。 |
| [^108] | [In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion.](http://arxiv.org/abs/2304.08658) | 本研究利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力，可以实时预测每个沃克塞尔中的气孔存在，是一个重大飞跃。 |
| [^109] | [Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks.](http://arxiv.org/abs/2304.07073) | 本文基于深度神经网络集成学习方法，提出了一种减少预测不确定性并输出衡量值的车辆能效预测方法，可应用于降低碳足迹，并在测试中表现出高度的预测性能。 |
| [^110] | [Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark.](http://arxiv.org/abs/2304.03279) | 本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。 |
| [^111] | [Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models.](http://arxiv.org/abs/2303.10774) | 本文提出了一种跨GAN审核方法，在与已建立“参考”GAN进行比较后，联合识别新开发的GAN中可理解的属性，提供GAN之间相似性和差异性的直观评估。 |
| [^112] | [ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond.](http://arxiv.org/abs/2303.06562) | 本研究提出了一种新的规范化层——ContraNorm，针对图神经网络和变压器中的过度平滑问题，通过对比学习的方式在嵌入空间中破坏表示，缓解了完全塌陷和维度塌陷的现象，并在实验中表现出较高的精度。 |
| [^113] | [Physics-constrained neural differential equations for learning multi-ionic transport.](http://arxiv.org/abs/2303.04594) | 本文提出了物理约束下的神经微分方程模型来学习聚酰胺纳米孔中离子的传输行为。 |
| [^114] | [Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations.](http://arxiv.org/abs/2303.02304) | 本论文提出一种耦合多小波神经算子学习的方案，解决了处理耦合多变量映射问题的难点，能够显著提高解决耦合偏微分方程的准确性，并在实验中得到了验证。 |
| [^115] | [Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition.](http://arxiv.org/abs/2302.05881) | 本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。 |
| [^116] | [Undersampling and Cumulative Class Re-decision Methods to Improve Detection of Agitation in People with Dementia.](http://arxiv.org/abs/2302.03224) | 本文对样本不平衡及标记不精确问题进行改进，仅用正常行为数据的20％即可训练具竞争力的痴呆症患者情绪激动检测模型。 |
| [^117] | [Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling.](http://arxiv.org/abs/2302.01312) | 本文提出了一个正则化流（NF）集合来估计Epistemic不确定性和Aleatoric不确定性，通过固定的dropout掩码来创建集合，运用于各种实验并可以提供全面的基准线。 |
| [^118] | [Recurrences reveal shared causal drivers of complex time series.](http://arxiv.org/abs/2301.13516) | 该研究开发了一种新的无监督学习算法，能够使用时间序列测量中的复现逐渐重构未被观察到的驱动信号，从而可靠地推断共享因果驱动者，其已在多个示例中进行验证。 |
| [^119] | [A Machine Learning Approach for Player and Position Adjusted Expected Goals in Football (Soccer).](http://arxiv.org/abs/2301.13052) | 该篇论文使用机器学习方法对足球运动员的位置信息以及其他参数进行建模，成功预测了进球的概率值，并进一步解决了“球掉进了错误的人手里”的问题。 |
| [^120] | [Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data.](http://arxiv.org/abs/2301.12321) | 本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。 |
| [^121] | [Risk-Sensitive Reinforcement Learning with Exponential Criteria.](http://arxiv.org/abs/2212.09010) | 本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。 |
| [^122] | [LidarCLIP or: How I Learned to Talk to Point Clouds.](http://arxiv.org/abs/2212.06858) | LidarCLIP可以将文本和激光雷达数据联系起来，达到有效的检索效果，而且能够在不良传感器条件下实现对具有挑战性的检测场景的有针对性搜索。 |
| [^123] | [Numerical Stability of DeepGOPlus Inference.](http://arxiv.org/abs/2212.06361) | 这篇论文研究了用于预测蛋白质功能的 CNN DeepGOPlus 在推理过程中的数值不确定性和数值稳定性，并研究了使用降低精度浮点格式进行推理的可能性。 |
| [^124] | [Technical Report of Mixing Local Patterns.](http://arxiv.org/abs/2212.03654) | 本文旨在解决GNN在处理非同质图数据时性能不佳的问题，提出混合局部结构模式的概念，并从局部模式的随机性和近邻可聚合性两个方面深入研究，以实现更通用的GNN。 |
| [^125] | [Differentially Private Learning with Per-Sample Adaptive Clipping.](http://arxiv.org/abs/2212.00328) | 本文提出了一种差分隐私每个样本自适应裁剪算法DP-PSAC，该算法采用非单调自适应权重函数，根据梯度的历史敏感性自适应裁剪每个样本的梯度幅度，提高了模型的实用性，且保证了隐私。 |
| [^126] | [Interpretable Scientific Discovery with Symbolic Regression: A Review.](http://arxiv.org/abs/2211.10873) | 符号回归作为一种机器学习方法日益流行，可以从数据中学习简洁且可解释的数学表达式，本综述全面概述了该方法并讨论了其优点和局限性。 |
| [^127] | [RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure.](http://arxiv.org/abs/2211.05239) | RecD 是一种为 DLRM 训练提供去重功能的端到端基础设施优化，解决了由于特征重复造成的海量存储、预处理和训练开销，引入了新的张量格式 InverseKeyedJaggedTensors (IKJTs) 来去除特征值的重复，使 DLRM 模型架构能够更好地利用数据的重复性提高训练吞吐量。 |
| [^128] | [On Web-based Visual Corpus Construction for Visual Document Understanding.](http://arxiv.org/abs/2211.03256) | 这篇论文提出了一个基于Web的视觉语料库构建工具（Webvicob），用于从Wikipedia HTML转储文件中构建大规模的、多语言的视觉语料库，通过数据集的使用可以提升视觉文档理解模型的性能。 |
| [^129] | [Going In Style: Audio Backdoors Through Stylistic Transformations.](http://arxiv.org/abs/2211.03117) | 本文探讨了音频领域中通过吉他效果动态转换恶意样本来进行后门攻击的风格触发器。JingleBack 的提出使得风格触发器在音频领域得以应用，并取得了极高的攻击成功率。 |
| [^130] | [Training Neural Networks for Sequential Change-point Detection.](http://arxiv.org/abs/2210.17312) | 本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。 |
| [^131] | [On Many-Actions Policy Gradient.](http://arxiv.org/abs/2210.13011) | 本论文研究了具有多个动作样本的随机策略梯度的方差问题，并提出了一种基于动态模型的多动作采样方法，实现了更高的样本效率和更高的回报。 |
| [^132] | [Transformers Learn Shortcuts to Automata.](http://arxiv.org/abs/2210.10749) | Transformer模型通过重新参数化其循环动态，可以使用比推理步骤更少的层数执行任何有限状态自动机的计算。多项式大小的 $O(\log T)$ 深度解决方案始终存在，而且$O(1)$深度模拟器是非常普遍的。 |
| [^133] | [Conditional Feature Importance for Mixed Data.](http://arxiv.org/abs/2210.03047) | 本研究提出了一种针对混合数据的条件特征重要性框架，使用条件预测影响和顺序knockoff抽样结合，以解决很少讨论的条件和边缘度量之间的重要区别，并揭示出为测试条件FI，目前只有少数方法可用且过去从业者由于数据要求不匹配而受到严重限制。 |
| [^134] | [Graph Neural Networks for Link Prediction with Subgraph Sketching.](http://arxiv.org/abs/2209.15486) | 本研究提出了一种名为ELPH的全图GNN模型，它使用子图草图作为消息传递，以缓解LP任务中子图之间的冗余问题，并在多个基准数据集上取得了最先进的结果。 |
| [^135] | [Extremely Simple Activation Shaping for Out-of-Distribution Detection.](http://arxiv.org/abs/2209.09858) | 该论文提出了一种极其简单的后处理激活形状方法ASH，用于离群点检测，该方法不需要额外的训练步骤、额外的数据或对训练网络进行非常规修改，就能增强模型对未知情况的处理能力。 |
| [^136] | [Why Deep Learning's Performance Data Are Misleading.](http://arxiv.org/abs/2208.11228) | 本文解释了“深度学习”中的两种误导性行为“数据删除”和“在训练集上测试”，并建立了一个定理，即NNWT方法可以使用这些不当行为在任何验证集和测试集上达到零误差，但是这些方法并不具有可泛化性。 |
| [^137] | [A physics-based domain adaptation framework for modelling and forecasting building energy systems.](http://arxiv.org/abs/2208.09456) | 本文提出了一种基于物理学的领域自适应框架，将线性时不变状态空间模型与基于子空间的无监督降阶建模相结合，通过最小化模型在共享子空间中的表示差异，将在一个领域上训练过的模型适应到另一个领域，用于建筑能量系统的建模和预测，并在实验中表现出优异的预测性能。 |
| [^138] | [Boosted Off-Policy Learning.](http://arxiv.org/abs/2208.01148) | 我们提出了一种基于Boosting的离线策略学习算法，将基础学习器简化为监督学习，获得了广泛的实际效益；实验结果表明其应用能力优于深度神经网络的离线策略学习和简单回归方法。 |
| [^139] | [Efficient Learning of Accurate Surrogates for Simulations of Complex Systems.](http://arxiv.org/abs/2207.12855) | 本论文提出了一种在线学习方法，由优化器驱动的采样方法，该方法可以提高替代模型的预测能力，并构建了一个比目前使用的替代模型更准确的替代模型，为复杂系统模拟提供了更加计算上高效的替代模型方法。 |
| [^140] | [Neural Stein critics with staged $L^2$-regularization.](http://arxiv.org/abs/2207.03406) | 本论文研究了使用$L^2$正则化训练神经网络Stein评价器的方法，通过阶段性的权重调整训练过程，可以在高维统计测试中实现对未知概率分布与名义模型分布的区分。 |
| [^141] | [LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks.](http://arxiv.org/abs/2206.09333) | LogGENE采用分位数回归框架预测基因表达水平的完整条件分位数，从而为高通量基因组学提供了一种能提供解释和报告不确定性估计、鲁棒性强的推断方法。 |
| [^142] | [Know your audience: specializing grounded language models with listener subtraction.](http://arxiv.org/abs/2206.08349) | 本文介绍了一种利用多智能体图像参照游戏自适应不同听众的目标任务描述的方法，并通过微调 CLIP 视觉编码器和大型语言模型之间的适配器，在适应听众的语言上下文的情况下进行了自然语言专业化。 |
| [^143] | [Learning Physics between Digital Twins with Low-Fidelity Models and Physics-Informed Gaussian Processes.](http://arxiv.org/abs/2206.08201) | 本文提出了一种新方法，通过低保真模型和物理知识驱动高斯过程进行数字孪生之间的学习， 并开发了贝叶斯分层建模框架允许多个数字孪生之间共享信息。 |
| [^144] | [Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning.](http://arxiv.org/abs/2206.06719) | 本文提出了一种自适应探索的Stein变分目标生成方法，通过粒子建模并引入适当难度区域，提高了多目标强化学习中目标达成的成功覆盖率，同时在环境变化时表现出有用的恢复特性。 |
| [^145] | [Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning.](http://arxiv.org/abs/2206.04384) | 本论文提出了一种离线强化学习的方法，通过构建简单离散的世界模型（Value Memory Graph，VMG）来抽象原始复杂环境，从而简化策略学习。 |
| [^146] | [Improving adversarial robustness by putting more regularizations on less robust samples.](http://arxiv.org/abs/2206.03353) | 本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。 |
| [^147] | [Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics.](http://arxiv.org/abs/2206.02897) | 本文提出了一个全面的团体公平指标框架，将其与更多的分配公正理论联系起来，揭示了标准团体公平指标所涉及的规范选择并允许解释道德实质。 |
| [^148] | [A Justice-Based Framework for the Analysis of Algorithmic Fairness-Utility Trade-Offs.](http://arxiv.org/abs/2206.02891) | 本论文提出了一个公正与效用并重的算法决策框架，能够平衡决策制定者和决策对象的观点，其中公正性评估基于公正的分配理论和算法公正的文献研究，效用评估基于基数效用函数。 |
| [^149] | [Cardinality-Minimal Explanations for Monotonic Neural Networks.](http://arxiv.org/abs/2205.09901) | 本文研究基于单调神经网络的解释方法，假设激活函数连续可导，则可以通过贪心算法在多项式时间内寻找到最小输入特征集合，解决难以处理的决策问题。 |
| [^150] | [Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff for Resilient, Secure, and Private Computation.](http://arxiv.org/abs/2204.11168) | 该论文提出了广义拉格朗日编码计算（GLCC）代码，该代码可以提供鲁棒性、安全性和隐私保护的解决方案。通过将数据集分成多个组，使用插值多项式对数据集进行编码，分享编码数据点，可以在主节点上消除跨组的干扰计算结果。GLCC代码是现有拉格朗日编码计算（LCC）代码的一种特例，具有更灵活的折衷方案。 |
| [^151] | [Bayesian Model Selection, the Marginal Likelihood, and Generalization.](http://arxiv.org/abs/2202.11678) | 本文回顾和探讨了边缘似然在构造约束和假设测试方面的有用性，强调了使用边缘似然作为泛化的代理的问题，并展示了其如何与神经架构搜索相关，可能导致超参数学习中的欠拟合和过拟合。 |
| [^152] | [CD-ROM: Complemented Deep-Reduced Order Model.](http://arxiv.org/abs/2202.10746) | 本文介绍了一种基于深度学习的闭合建模方法CD-ROM，用于经典的POD-Galerkin降阶模型，该方法可以显著提高降阶模型的准确性和稳定性。 |
| [^153] | [End-to-End Training for Back-Translation with Categorical Reparameterization Trick.](http://arxiv.org/abs/2202.08465) | 本文提出了一种基于分类重新参数化技巧的回译端到端训练方法，来有效地减少两个神经机器翻译模型间离散属性的影响，从而实现端到端式的训练，获得了比以前基准测试更好的BLEU分数。 |
| [^154] | [Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule.](http://arxiv.org/abs/2201.11989) | 本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。 |
| [^155] | [MassFormer: Tandem Mass Spectrum Prediction for Small Molecules using Graph Transformers.](http://arxiv.org/abs/2111.04824) | MassFormer提出了一种新的方法来准确预测小分子的串联质谱。该模型使用图变换器架构来考虑分子中原子之间的远距离关系，并在多个数据集上击败其他方法，能够恢复关于碰撞能量对光谱的影响的先前知识。 |
| [^156] | [Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel.](http://arxiv.org/abs/2111.02827) | 本研究旨在通过提供一个智能体间的消息传递环境，使得智能体能够通过连续声学通道进行通讯并观察到新兴语言的产生与特点，结果表明：与离散型信号不同，声学讲话者学习使用冗余信息以提高侦听者的连贯性。 |
| [^157] | [Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function.](http://arxiv.org/abs/2107.08649) | 本文研究了非凸随机优化问题，提出了TUSLA算法在Wasserstein-1和Wasserstein-2距离上的非渐进误差界限，进而推导了期望过量风险的非渐进估计值。在ReLU神经网络中，理论和数值实验表明TUSLA算法能够高效且精确地解决此类优化问题。 |
| [^158] | [Word Embeddings: A Survey.](http://arxiv.org/abs/1901.09069) | 这篇综述介绍了一些主要的词向量构建策略，称为word embeddings，这些策略基于分布假设，编码了语法和语义信息，并被证明在很多NLP任务中是有用的额外特征。 |

# 详细

[^1]: 基于深度生成先验的数据集蒸馏方法的泛化

    Generalizing Dataset Distillation via Deep Generative Prior. (arXiv:2305.01649v1 [cs.CV])

    [http://arxiv.org/abs/2305.01649](http://arxiv.org/abs/2305.01649)

    该方法提出一种基于预训练深度生成模型的学习先验的数据集蒸馏方法，通过在生成模型的潜在空间中将大量图像蒸馏为少量中间特征向量，显著提高了在所有设置中的跨体系结构的泛化能力。

    

    数据集蒸馏旨在将整个数据集的知识蒸馏到几个合成图像中。其思想是合成少量的合成数据点，并将其作为训练数据提供给学习算法，以得到一个逼近原始数据训练的模型。尽管该领域最近取得了进展，但现有的数据集蒸馏方法无法推广到新的体系结构并扩展到高分辨率数据集。为了克服上述问题，我们建议使用预训练深度生成模型的学习先验来合成蒸馏的数据。为实现这一目的，我们提出了一种新的优化算法，在生成模型的潜在空间中将大量图像蒸馏为少量中间特征向量。我们的方法增强了现有技术，显著提高了在所有设置中的跨体系结构的泛化能力。

    Dataset Distillation aims to distill an entire dataset's knowledge into a few synthetic images. The idea is to synthesize a small number of synthetic data points that, when given to a learning algorithm as training data, result in a model approximating one trained on the original data. Despite recent progress in the field, existing dataset distillation methods fail to generalize to new architectures and scale to high-resolution datasets. To overcome the above issues, we propose to use the learned prior from pre-trained deep generative models to synthesize the distilled data. To achieve this, we present a new optimization algorithm that distills a large number of images into a few intermediate feature vectors in the generative model's latent space. Our method augments existing techniques, significantly improving cross-architecture generalization in all settings.
    
[^2]: 差分隐私下的上下文学习

    Differentially Private In-Context Learning. (arXiv:2305.01639v1 [cs.LG])

    [http://arxiv.org/abs/2305.01639](http://arxiv.org/abs/2305.01639)

    本文提出了DP-ICL，实现了在隐私保证下对新任务的适应性。经过四个基准测试，发现其性能与非私有ICL相当。

    

    在部署大型语言模型（LLM）时，一个重要的问题是如何使用私有数据增强LLM。我们提出了"DP-ICL"来实现对新任务的适应性，同时保持隐私保证。DP-ICL通过使用"report-noisy-max"机制在示例集合上建立嘈杂一致性来进行私有推断。我们在四个基准测试上评估了DP-ICL，发现其与非私有ICL相比具有可比性的性能(<2%降级)。

    An important question in deploying large language models (LLMs) is how to augment LLMs with private data. We propose Differentially Private In-context Learning (DP-ICL) to enable LLMs to adapt to new tasks while maintaining privacy guarantees. DP-ICL performs private inference by establishing noisy consensus over an ensemble of exemplars using the Report-Noisy-Max mechanism. We evaluate DP-ICL on four benchmarks and find that it achieves comparable performance (<2\% degradation) with non-private ICL.
    
[^3]: 多分辨率卷积记忆的序列建模

    Sequence Modeling with Multiresolution Convolutional Memory. (arXiv:2305.01638v1 [cs.LG])

    [http://arxiv.org/abs/2305.01638](http://arxiv.org/abs/2305.01638)

    本论文提出了一种新的用于序列建模的构建块，称为MultiresLayer，通过多分辨率卷积捕获输入序列中的多尺度趋势，既具有卷积网络的计算优势，又具有小波分解的有理论基础的动机。

    

    有效地捕捉对于某个任务（如分类和生成建模）显著的顺序数据源中的长程模式是一个基本挑战。我们从基于小波的多分辨率分析中获得灵感，定义了一个新的用于序列建模的构建块，称为MultiresLayer。我们模型的关键组成部分是多分辨率卷积，以捕获输入序列中的多尺度趋势。我们的MultiresConv可以通过在扩张的因果卷积树上使用共享过滤器来实现。因此，它既具有卷积网络的计算优势，又具有小波分解的有理论基础的动机。

    Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. 
    
[^4]: 坏建议的好处：模型层间自动对照解码

    The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers. (arXiv:2305.01628v1 [cs.CL])

    [http://arxiv.org/abs/2305.01628](http://arxiv.org/abs/2305.01628)

    本文提出一种新颖的方法，利用语言模型层之间的对比来改进文本生成输出，解决模型在开放式生成中的不良行为问题，并显著提高生成文本的质量。

    

    在自然语言处理任务中，应用语言模型通常依赖于最终模型层的表示，因为假设中间隐藏层的表示是不太有用的。本文认为由于模型层之间的渐进改进，可以从更高层和更低层之间的对比中获取额外信息。具体来说，在选择生成模型的下一个可能标记的预测时，可以使用较低层的预测来突出哪些候选项是最好避免的。我们提出了一种新颖的方法，利用层之间的对比来改进文本生成输出，并表明它可以缓解模型在开放式生成中的不良行为，显著提高生成的文本质量。此外，我们的结果表明，在推断时比较模型层之间可以对一些总体语言模型能力的方面产生实质性的好处。

    Applying language models to natural language processing tasks typically relies on the representations in the final model layer, as intermediate hidden layer representations are presumed to be less informative. In this work, we argue that due to the gradual improvement across model layers, additional information can be gleaned from the contrast between higher and lower layers during inference. Specifically, in choosing between the probable next token predictions of a generative model, the predictions of lower layers can be used to highlight which candidates are best avoided. We propose a novel approach that utilizes the contrast between layers to improve text generation outputs, and show that it mitigates degenerative behaviors of the model in open-ended generation, significantly improving the quality of generated texts. Furthermore, our results indicate that contrasting between model layers at inference time can yield substantial benefits to certain aspects of general language model ca
    
[^5]: ContactArt：学习类别级联结物体和手部姿态估计的三维交互先验

    ContactArt: Learning 3D Interaction Priors for Category-level Articulated Object and Hand Poses Estimation. (arXiv:2305.01618v1 [cs.CV])

    [http://arxiv.org/abs/2305.01618](http://arxiv.org/abs/2305.01618)

    本研究提出了一种基于视觉遥操作的数据收集方法以及学习手物互动先验的新方法，从而能够在联结目标和手部姿态估计中实现更好的关键点定位性能。

    

    我们提出了一个新的数据集和一种新方法，用于学习手部和联结目标姿态估计中的手物互动先验。我们首先使用视觉遥操作收集了一个数据集，其中人类操作员可以直接在物理模拟器中游戏来操纵联结对象。 我们记录数据并从模拟器获得有关目标姿态和接触信息的免费和准确注释。 我们的系统仅需要使用iPhone来记录人手运动，可以轻松扩展并大大降低数据和注释收集的成本。使用这些数据，我们学习了三维交互先验，包括捕获对象部件排列分布的鉴别器（在GAN中），以及生成联结对象上接触区域的扩散模型，以指导手势估计。这些结构和接触先验可以很容易地转移到现实世界数据，几乎没有任何领域差距。通过使用我们的数据和学习的先验，我们的方法显著提高了关键点定位性能。

    We propose a new dataset and a novel approach to learning hand-object interaction priors for hand and articulated object pose estimation. We first collect a dataset using visual teleoperation, where the human operator can directly play within a physical simulator to manipulate the articulated objects. We record the data and obtain free and accurate annotations on object poses and contact information from the simulator. Our system only requires an iPhone to record human hand motion, which can be easily scaled up and largely lower the costs of data and annotation collection. With this data, we learn 3D interaction priors including a discriminator (in a GAN) capturing the distribution of how object parts are arranged, and a diffusion model which generates the contact regions on articulated objects, guiding the hand pose estimation. Such structural and contact priors can easily transfer to real-world data with barely any domain gap. By using our data and learned priors, our method signific
    
[^6]: AutoColor: 针对多色全息图的学习光功率控制

    AutoColor: Learned Light Power Control for Multi-Color Holograms. (arXiv:2305.01611v1 [cs.CV])

    [http://arxiv.org/abs/2305.01611](http://arxiv.org/abs/2305.01611)

    AutoColor 是第一个学习正确照明多色全息图所需光源功率的方法，将优化多色全息图所需的步骤数从超过1000个降至70个迭代步骤。

    

    多色全息图需要多个光源同时照射才能形成图形。这种全息图可以比传统单色全息图更好地利用光源，并提高全息显示的动态范围。我们引入了\projectname，这是一种学习方法，用于估计照明多色全息图所需的最佳光源功率。为此，我们使用合成图像和其深度信息建立了第一个多色全息图数据集。我们使用一种流行的流程结合生成模型、大语言和单眼深度估计模型来生成这些合成图像。最后，我们使用数据集训练我们的学习模型，并实验性地证明，\projectname可以将优化多色全息图所需的步骤数从超过1000个降至70个迭代步骤，而不会影响图像质量。

    Multi-color holograms rely on simultaneous illumination from multiple light sources. These multi-color holograms could utilize light sources better than conventional single-color holograms and can improve the dynamic range of holographic displays. In this letter, we introduce \projectname, the first learned method for estimating the optimal light source powers required for illuminating multi-color holograms. For this purpose, we establish the first multi-color hologram dataset using synthetic images and their depth information. We generate these synthetic images using a trending pipeline combining generative, large language, and monocular depth estimation models. Finally, we train our learned model using our dataset and experimentally demonstrate that \projectname significantly decreases the number of steps required to optimize multi-color holograms from $>1000$ to $70$ iteration steps without compromising image quality.
    
[^7]: 在稀疏探测中寻找海量神经元: 实例研究

    Finding Neurons in a Haystack: Case Studies with Sparse Probing. (arXiv:2305.01610v1 [cs.LG])

    [http://arxiv.org/abs/2305.01610](http://arxiv.org/abs/2305.01610)

    本文通过训练$k$-稀疏线性分类器以预测输入特征是否存在，研究了大型语言模型（LLM）内部神经元激活的表示方式；对不同层次的神经元网络的研究表明，早期层利用神经元的稀疏组合来表示多种特征，中间层有特定的神经元表示高级上下文特征，增加规模使特征表示更加稀疏化。

    

    尽管大型语言模型(LLM)的应用和部署迅速增加，但这些模型的内部计算仍然不透明且难以理解。本文旨在了解高级可解释特征在LLM内部神经元激活中的表示方式。我们使用$k$-稀疏线性分类器(探针)来训练这些内部激活值，并预测输入的特征是否存在；通过改变$k$值，我们研究了学习表示的稀疏性以及随着模型规模的变化而变化的情况。当$k=1$时，我们定位某个特定特征非常相关的单个神经元，并进行了大量案例研究，以说明LLM的一般性质。特别是，我们展示了早期层利用神经元的稀疏组合来表示许多特征，中间层似乎具有专门的神经元来表示更高级的上下文特征，而增加的规模则导致表示稀疏性增加。

    Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train $k$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of $k$ we study the sparsity of learned representations and how this varies with model scale. With $k=1$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to
    
[^8]: 多个深度网络的训练过程探索相同的低维流形

    The Training Process of Many Deep Networks Explores the Same Low-Dimensional Manifold. (arXiv:2305.01604v1 [cs.LG])

    [http://arxiv.org/abs/2305.01604](http://arxiv.org/abs/2305.01604)

    本文展示了多个深度网络的训练过程探索相同的低维流形，这些网络包括不同体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化，并揭示了网络初始化位置、体系结构和大小对流形的影响。

    

    我们开发了信息几何技术来分析深度网络训练过程中预测轨迹。通过检查底层高维概率模型，我们揭示了训练过程探索的有效低维流形。具有各种体系结构、大小、使用不同优化方法、正则化技术、数据增强技术和权重初始化的网络在预测空间内位于同一流形上。我们研究了这种流形的细节，发现具有不同体系结构的网络遵循可区分的轨迹，但其他因素影响极小; 更大的网络沿着与较小的网络相似的流形训练，只是更快; 不同部分的初始化网络在相似的流形上向解决方案收敛。

    We develop information-geometric techniques to analyze the trajectories of the predictions of deep networks during training. By examining the underlying high-dimensional probabilistic models, we reveal that the training process explores an effectively low-dimensional manifold. Networks with a wide range of architectures, sizes, trained using different optimization methods, regularization techniques, data augmentation techniques, and weight initializations lie on the same manifold in the prediction space. We study the details of this manifold to find that networks with different architectures follow distinguishable trajectories but other factors have a minimal influence; larger networks train along a similar manifold as that of smaller networks, just faster; and networks initialized at very different parts of the prediction space converge to the solution along a similar manifold.
    
[^9]: 论数据质量对图像分类公平性的影响

    On the Impact of Data Quality on Image Classification Fairness. (arXiv:2305.01595v1 [cs.CV])

    [http://arxiv.org/abs/2305.01595](http://arxiv.org/abs/2305.01595)

    本文探讨了图像分类中训练数据质量和模型公平性之间的关系，通过在不同噪声级别的数据上测量公平性指标，揭示了数据噪声对于模型公平性的影响。

    

    随着算法决策的普及，对这些系统的关注越来越多。本文探讨了在监督分类的情况下，训练数据的质量和使用此类数据训练的模型的整体公平性之间的关系。我们在多个图像分类数据集上测量关键的公平性指标，其中包含标签和训练数据本身噪声水平不同的情况。我们将标签中的噪声描述为训练集中数据标记的不准确性，将数据中的噪声描述为数据中的扭曲，也是训练集中的内容。通过向原始数据集添加噪声，我们可以探索训练数据的质量与在该数据上训练的模型输出的公平性之间的关系。

    With the proliferation of algorithmic decision-making, increased scrutiny has been placed on these systems. This paper explores the relationship between the quality of the training data and the overall fairness of the models trained with such data in the context of supervised classification. We measure key fairness metrics across a range of algorithms over multiple image classification datasets that have a varying level of noise in both the labels and the training data itself. We describe noise in the labels as inaccuracies in the labelling of the data in the training set and noise in the data as distortions in the data, also in the training set. By adding noise to the original datasets, we can explore the relationship between the quality of the training data and the fairness of the output of the models trained on that data.
    
[^10]: 重新审视梯度剪切：随机偏差和紧密收敛性保证。

    Revisiting Gradient Clipping: Stochastic bias and tight convergence guarantees. (arXiv:2305.01588v1 [cs.LG])

    [http://arxiv.org/abs/2305.01588](http://arxiv.org/abs/2305.01588)

    本文提出了针对梯度剪切的收敛保证机制，不再需要特定的阈值和强噪声假设，同时可以独立于步长选择，从而提高了收敛的自由度。

    

    梯度剪切是标准（随机）梯度下降的一种流行修改方法，每次迭代将梯度范数限制在某个值c>0。它被广泛用于稳定深度学习模型的训练( Goodfellow et al., 2016 )或强制实施差分隐私( Abadi et al., 2016 )。尽管剪切机制受欢迎且简单，但其收敛保证通常需要特定的$c$值和强噪声假设。在本文中，我们给出了收敛保证，显示了对任意剪辑阈值的精确依赖，并且表明我们的保证在确定性和随机梯度下都是紧密的。特别地，我们表明(i)对于确定性的梯度下降，剪辑阈值仅影响收敛的高阶项，(ii)在随机设置中，即使对于任意小的步长，也不能保证收敛到真正的最优解在标准的噪声假设下，我们给出了机器学习特定的随机噪声假设，在此假设下，收敛是保证的，剪切阈值$c$可以独立于步长选择。

    Gradient clipping is a popular modification to standard (stochastic) gradient descent, at every iteration limiting the gradient norm to a certain value $c >0$. It is widely used for example for stabilizing the training of deep learning models (Goodfellow et al., 2016), or for enforcing differential privacy (Abadi et al., 2016). Despite popularity and simplicity of the clipping mechanism, its convergence guarantees often require specific values of $c$ and strong noise assumptions.  In this paper, we give convergence guarantees that show precise dependence on arbitrary clipping thresholds $c$ and show that our guarantees are tight with both deterministic and stochastic gradients. In particular, we show that (i) for deterministic gradient descent, the clipping threshold only affects the higher-order terms of convergence, (ii) in the stochastic setting convergence to the true optimum cannot be guaranteed under the standard noise assumption, even under arbitrary small step-sizes. We give ma
    
[^11]: 基于新型分子表示与变分自编码器的分子设计方法

    Molecular design method based on novel molecular representation and variational auto-encoder. (arXiv:2305.01580v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.01580](http://arxiv.org/abs/2305.01580)

    本论文提出了一种基于SELFIES分子表示和改进后的变分自编码器模型的分子设计方法，用于生成更多种类的分子。

    

    本论文在传统的变分自编码器的基础上采用最新的分子表示方法SELFIES，提出一种改进后的神经网络模型用于生成新的分子，该模型结合了多层卷积网络、费舍尔信息和长短期记忆神经网络，通过聚合数据特征和指导编码过程等手段，有效解决了原始变分自编码器模型的问题。实验结果表明，使用SELFIES和新的变分自编码器模型可以生成更多种类的分子，且生成的分子相似度较高。

    Based on the traditional VAE, a novel neural network model is presented, with the latest molecular representation, SELFIES, to improve the effect of generating new molecules. In this model, multi-layer convolutional network and Fisher information are added to the original encoding layer to learn the data characteristics and guide the encoding process, which makes the features of the data hiding layer more aggregated, and integrates the Long Short Term Memory neural network (LSTM) into the decoding layer for better data generation, which effectively solves the degradation phenomenon generated by the encoding layer and decoding layer of the original VAE model. Through experiments on zinc molecular data sets, it is found that the similarity in the new VAE is 8.47% higher than that of the original ones. SELFIES are better at generating a variety of molecules than the traditional molecular representation, SELFIES. Experiments have shown that using SELFIES and the new VAE model presented in 
    
[^12]: NELoRa-Bench：一种神经增强LoRa解调基准测试

    NELoRa-Bench: A Benchmark for Neural-enhanced LoRa Demodulation. (arXiv:2305.01573v1 [cs.NI])

    [http://arxiv.org/abs/2305.01573](http://arxiv.org/abs/2305.01573)

    本文介绍了一种名为NELoRa的神经增强解码器，用于提高低功耗广域网络中LoRa的信号噪声比（SNR）性能。数据集显示，NELoRa相比标准LoRa解码器可以实现1.84-2.35 dB的SNR增益。

    

    低功耗广域网络（LPWANs）是一种标志性的物联网范例，具有低功耗和远距离通信的特点。其中，LoRa因其独特的特性和开源技术而得到广泛应用。通过采用啁啾扩频（CSS）调制，LoRa可以实现低信噪比（SNR）通信。标准LoRa解调方法将整个啁啾信号的功率累加成频域中的一个能量峰值。这样，即使SNR低于-15 dB，它也可以支持通信。此外，我们提出了一种名为NELoRa的神经增强解码器，它利用多维信息实现了显著的SNR增益。本文介绍了用于训练/测试NELoRa的数据集，其中包括27,329个LoRa符号，其扩展因子从7到10，以进一步提高神经增强LoRa解调性能。数据集显示，NELoRa相比标准LoRa解码器可以实现1.84-2.35 dB的SNR增益。该数据集

    Low-Power Wide-Area Networks (LPWANs) are an emerging Internet-of-Things (IoT) paradigm marked by low-power and long-distance communication. Among them, LoRa is widely deployed for its unique characteristics and open-source technology. By adopting the Chirp Spread Spectrum (CSS) modulation, LoRa enables low signal-to-noise ratio (SNR) communication. The standard LoRa demodulation method accumulates the chirp power of the whole chirp into an energy peak in the frequency domain. In this way, it can support communication even when SNR is lower than -15 dB. Beyond that, we proposed NELoRa, a neural-enhanced decoder that exploits multi-dimensional information to achieve significant SNR gain. This paper presents the dataset used to train/test NELoRa, which includes 27,329 LoRa symbols with spreading factors from 7 to 10, for further improvement of neural-enhanced LoRa demodulation. The dataset shows that NELoRa can achieve 1.84-2.35 dB SNR gain over the standard LoRa decoder. The dataset and
    
[^13]: 如何发挥大语言模型在少样本关系抽取中的能力？

    How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?. (arXiv:2305.01555v1 [cs.CL])

    [http://arxiv.org/abs/2305.01555](http://arxiv.org/abs/2305.01555)

    本文通过使用GPT-3.5模型在少样本关系抽取中，实现在四个不同数据集上的新的最优性能，并提出了与任务相关的指导说明和约束模式下的数据生成方法。

    

    语言模型的扩展已经彻底改变了广泛的自然语言处理任务，但是使用大型语言模型进行少样本关系抽取还没有得到全面探索。本文通过详细实验，研究了使用GPT-3.5进行少样本关系抽取的基本方法——上下文学习和数据生成。为了增强少样本性能，我们进一步提出了与任务相关的指导说明和约束模式下的数据生成。我们观察到，在上下文学习的情况下，可以实现与以前的提示学习方法相当的性能，而使用大型语言模型的数据生成可以推动以前的解决方案以在四个广泛研究的关系抽取数据集上获得新的最先进的少样本结果。我们希望我们的工作可以激发未来对大型语言模型在少样本关系抽取中的能力的研究。代码可以在 \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm} 中找到。

    Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in \url{https://github.com/zjunlp/DeepKE/tree/main/example/llm.
    
[^14]: 通过学习的差异策略缓解语言模型中的近似记忆化问题

    Mitigating Approximate Memorization in Language Models via Dissimilarity Learned Policy. (arXiv:2305.01550v1 [cs.CL])

    [http://arxiv.org/abs/2305.01550](http://arxiv.org/abs/2305.01550)

    本论文提出了一种利用强化学习方法缓解语言模型近似记忆化问题的新框架，使用负相似度评分作为奖励信号来学习差异策略。该方法能够有效解决显式和隐式假设所导致的数据结构不完全的问题。

    

    大型语言模型（LLMs）是通过大量数据进行训练的，其中可能包含会危害个人隐私的敏感信息。LLMs被证明会记忆训练数据的部分内容并在遇到对应提示时直接输出该数据。先前的研究主要集中在数据预处理和差分隐私技术上以解决记忆化问题，但这些方法都依赖于对要保护数据结构的显式和隐式假设，导致问题解决不完全。为了解决这个问题，我们提出了一种新的框架，利用强化学习方法（PPO）微调LLMs以缓解近似记忆化问题。我们的方法使用负相似度评分（例如BERTScore或SacreBLEU）作为奖励信号来学习差异策略。我们的结果表明，这种框架有效地缓解了近似记忆化问题。

    Large Language models (LLMs) are trained on large amounts of data, which can include sensitive information that may compromise per- sonal privacy. LLMs showed to memorize parts of the training data and emit those data verbatim when an adversary prompts appropriately. Previous research has primarily focused on data preprocessing and differential privacy techniques to address memorization or prevent verbatim memorization exclusively, which can give a false sense of privacy. However, these methods rely on explicit and implicit assumptions about the structure of the data to be protected, which often results in an incomplete solution to the problem. To address this, we propose a novel framework that utilizes a reinforcement learning approach (PPO) to fine-tune LLMs to mitigate approximate memorization. Our approach utilizes a negative similarity score, such as BERTScore or SacreBLEU, as a reward signal to learn a dissimilarity policy. Our results demonstrate that this framework effectively 
    
[^15]: 通过自助模型引导加速神经网络的自我提高

    Accelerating Neural Self-Improvement via Bootstrapping. (arXiv:2305.01547v1 [cs.LG])

    [http://arxiv.org/abs/2305.01547](http://arxiv.org/abs/2305.01547)

    本研究通过引入自助元学习加速神经网络的自我提高，提高了神经网络在少样本学习中的表现。

    

    在大型语言模型的背景下，序列处理神经网络（NN）的少样本学习最近引起了新的关注。在标准的N类K样本学习环境中，NN被明确优化为学习通过观察序列NK标记示例来对未标记的输入进行分类，从而迫使NN学习实现最佳性能的学习算法，考虑到有限的训练示例。在这里，我们研究了一种辅助损失函数，通过将最近提出的自助元学习应用于NN少样本学习器，鼓励进一步加速少样本学习：我们通过仅仅利用NK个样本，将K样本学习器的性能优化到其可以观察到多于NK个样本时的性能，从而获得了有希望的结果，在标准的Mini-ImageNet数据集上测试，我们的代码是公开的。

    Few-shot learning with sequence-processing neural networks (NNs) has recently attracted a new wave of attention in the context of large language models. In the standard N-way K-shot learning setting, an NN is explicitly optimised to learn to classify unlabelled inputs by observing a sequence of NK labelled examples. This pressures the NN to learn a learning algorithm that achieves optimal performance, given the limited number of training examples. Here we study an auxiliary loss that encourages further acceleration of few-shot learning, by applying recently proposed bootstrapped meta-learning to NN few-shot learners: we optimise the K-shot learner to match its own performance achievable by observing more than NK examples, using only NK examples. Promising results are obtained on the standard Mini-ImageNet dataset. Our code is public.
    
[^16]: Jacobian-Scaled K-means聚类用于受物理学影响的反应流分割

    Jacobian-Scaled K-means Clustering for Physics-Informed Segmentation of Reacting Flows. (arXiv:2305.01539v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.01539](http://arxiv.org/abs/2305.01539)

    本研究提出了一种受物理学影响的聚类策略，名为Jacobian-Scaled K-means聚类方法，能够不修改输入数据集即生成能够捕获动力学相似性区域的聚类。本方法具有一定的应用前景和实用价值。

    

    本文介绍了Jacobian-Scaled K-means (JSK-means)聚类方法，该方法是基于K-means框架的受物理学影响的聚类策略。该方法通过距离函数的修改将潜在的物理知识注入到聚类过程中：JSK-means聚类过程不使用传统的欧氏距离向量，而是使用从集群质心处求得的动态系统Jacobian矩阵缩放的距离向量。本文的目标是展示如何通过JSK-means算法--而不是修改输入数据集--生成能够捕获动力学相似性区域的聚类，即聚类是向高敏感区域在相空间中重新分布并由样本的源项的相似性描述而非样本本身。该算法在复杂的反应流模拟数据集(通道爆轰配置)上进行演示，其中动态性质是

    This work introduces Jacobian-scaled K-means (JSK-means) clustering, which is a physics-informed clustering strategy centered on the K-means framework. The method allows for the injection of underlying physical knowledge into the clustering procedure through a distance function modification: instead of leveraging conventional Euclidean distance vectors, the JSK-means procedure operates on distance vectors scaled by matrices obtained from dynamical system Jacobians evaluated at the cluster centroids. The goal of this work is to show how the JSK-means algorithm -- without modifying the input dataset -- produces clusters that capture regions of dynamical similarity, in that the clusters are redistributed towards high-sensitivity regions in phase space and are described by similarity in the source terms of samples instead of the samples themselves. The algorithm is demonstrated on a complex reacting flow simulation dataset (a channel detonation configuration), where the dynamics in the the
    
[^17]: 用显式和隐式知识推动AI药物发现

    Empowering AI drug discovery with explicit and implicit knowledge. (arXiv:2305.01523v1 [cs.LG])

    [http://arxiv.org/abs/2305.01523](http://arxiv.org/abs/2305.01523)

    DeepEIK是一个统一的深度学习框架，利用显式和隐式知识进行AI药物发现，通过特征融合和注意力机制来提高模型预测准确性，并在多个任务上显著优于现有方法。

    

    近年来，独立利用知识图谱中的显式知识或生物医学文献中的隐式知识进行AI药物发现的研究迅速增长。这些方法极大地提高了多个下游任务上AI模型的预测准确性。然而，独立地整合显式和隐式知识会阻碍对分子的理解。本研究提出了DeepEIK，这是一个统一的深度学习框架，结合了显式和隐式知识来进行AI药物发现。

    Motivation: Recently, research on independently utilizing either explicit knowledge from knowledge graphs or implicit knowledge from biomedical literature for AI drug discovery has been growing rapidly. These approaches have greatly improved the prediction accuracy of AI models on multiple downstream tasks. However, integrating explicit and implicit knowledge independently hinders their understanding of molecules. Results: We propose DeepEIK, a unified deep learning framework that incorporates both explicit and implicit knowledge for AI drug discovery. We adopt feature fusion to process the multi-modal inputs, and leverage the attention mechanism to denoise the text information. Experiments show that DeepEIK significantly outperforms state-of-the-art methods on crucial tasks in AI drug discovery including drug-target interaction prediction, drug property prediction and protein-protein interaction prediction. Further studies show that benefiting from explicit and implicit knowledge, our
    
[^18]: 带有基于曝光的风险最小化的反事实学习排序安全部署

    Safe Deployment for Counterfactual Learning to Rank with Exposure-Based Risk Minimization. (arXiv:2305.01522v1 [cs.IR])

    [http://arxiv.org/abs/2305.01522](http://arxiv.org/abs/2305.01522)

    该论文提出了一种新的反事实学习排序方法，通过基于曝光的风险正则化对IPS估计进行调整，以确保学习排名模型与给定的安全模型接近，从而在部署过程中降低风险。

    

    反事实学习排序（CLTR）依赖于基于曝光的倒数概率评分（IPS），一种LTR特定的自适应IPS来纠正位置偏差。虽然IPS可以提供无偏和一致的估计，但通常会受到高方差的影响。尤其是当可用点击数据很少时，这种方差可能会导致CLTR学习次优的排名行为。因此，现有的CLTR方法带来了重大风险，因为简单地部署它们的模型可能会导致用户体验非常差。我们引入了一种新的风险感知CLTR方法，具有安全部署的理论保证。我们对LTR的IPS估计应用了一种新的基于曝光的风险正则化概念。我们的风险正则化惩罚学习模型的排名行为与给定安全模型的不匹配。因此，在IPS估计存在高度不确定性时，它确保学习排名模型与可信模型保持接近，从而大大降低了部署过程中的风险。

    Counterfactual learning to rank (CLTR) relies on exposure-based inverse propensity scoring (IPS), a LTR-specific adaptation of IPS to correct for position bias. While IPS can provide unbiased and consistent estimates, it often suffers from high variance. Especially when little click data is available, this variance can cause CLTR to learn sub-optimal ranking behavior. Consequently, existing CLTR methods bring significant risks with them, as naively deploying their models can result in very negative user experiences. We introduce a novel risk-aware CLTR method with theoretical guarantees for safe deployment. We apply a novel exposure-based concept of risk regularization to IPS estimation for LTR. Our risk regularization penalizes the mismatch between the ranking behavior of a learned model and a given safe model. Thereby, it ensures that learned ranking models stay close to a trusted model, when there is high uncertainty in IPS estimation, which greatly reduces the risks during deployme
    
[^19]: 揭秘长期基于新颖性探索中表示方法的威力

    Unlocking the Power of Representations in Long-term Novelty-based Exploration. (arXiv:2305.01521v1 [cs.LG])

    [http://arxiv.org/abs/2305.01521](http://arxiv.org/abs/2305.01521)

    本论文介绍了一种名为RECODE的非参数新颖性探索方法，它在深度强化学习中跟踪状态访问计数，并与新颖的逆动力学损失相结合，实现了在具有挑战性的任务中的最新最佳表现。

    

    我们介绍了一种名为RECODE（基于聚类的在线密度估计强化学习方法）的非参数新颖性探索方法，其根据在所选择的嵌入空间中的相似性聚合状态并估计其访问次数。通过将经典聚类方法适应于深度强化学习的非平稳性环境，RECODE可在数千个回合中有效地跟踪状态访问计数。我们进一步提出了一种新颖的逆动力学损失的泛化形式，它利用掩码变压器结构进行多步预测。RECODE与此相结合，在DM-Hard-8的一系列具有挑战性的3D探索任务中实现了最新的最佳表现。在困难的Atari游戏中，RECODE也创造了新的最佳表现，并成为首个成功通关"Pitfall!"的代理。

    We introduce Robust Exploration via Clustering-based Online Density Estimation (RECODE), a non-parametric method for novelty-based exploration that estimates visitation counts for clusters of states based on their similarity in a chosen embedding space. By adapting classical clustering to the nonstationary setting of Deep RL, RECODE can efficiently track state visitation counts over thousands of episodes. We further propose a novel generalization of the inverse dynamics loss, which leverages masked transformer architectures for multi-step prediction; which in conjunction with RECODE achieves a new state-of-the-art in a suite of challenging 3D-exploration tasks in DM-Hard-8. RECODE also sets new state-of-the-art in hard exploration Atari games, and is the first agent to reach the end screen in "Pitfall!".
    
[^20]: 分子关系学习的条件图信息瓶颈

    Conditional Graph Information Bottleneck for Molecular Relational Learning. (arXiv:2305.01520v1 [q-bio.MN])

    [http://arxiv.org/abs/2305.01520](http://arxiv.org/abs/2305.01520)

    本文提出了一种新的关系学习框架，称为CGIB，通过检测其中的核心子图，预测对图对之间的相互作用行为。

    

    分子关系学习是指学习分子之间的相互作用行为，因其广泛应用于分子科学而引起极大兴趣。近年来，图神经网络通过将分子建模为图结构，并考虑两个分子之间的原子级相互作用，在分子关系学习中取得了巨大成功。然而，现有的分子关系学习方法往往忽略了化学的本质。即化合物由多个互相作用的子结构组成，这些子结构会引起独特的化学反应。本文提出了一种新的关系学习框架，称为CGIB，通过检测其中的核心子图，预测对图对之间的相互作用行为。其主要思想是，在给定一对图的情况下，从一个图中找到包含有关所需任务的最小充分信息的子图，以此来预测这对图之间的信息瓶颈。

    Molecular relational learning, whose goal is to learn the interaction behavior between molecular pairs, got a surge of interest in molecular sciences due to its wide range of applications. Recently, graph neural networks have recently shown great success in molecular relational learning by modeling a molecule as a graph structure, and considering atom-level interactions between two molecules. Despite their success, existing molecular relational learning methods tend to overlook the nature of chemistry, i.e., a chemical compound is composed of multiple substructures such as functional groups that cause distinctive chemical reactions. In this work, we propose a novel relational learning framework, called CGIB, that predicts the interaction behavior between a pair of graphs by detecting core subgraphs therein. The main idea is, given a pair of graphs, to find a subgraph from a graph that contains the minimal sufficient information regarding the task at hand conditioned on the paired graph
    
[^21]: BCEdge：适应边缘平台上的自适应批处理和SLO感知的DNN推断服务

    BCEdge: SLO-Aware DNN Inference Services with Adaptive Batching on Edge Platforms. (arXiv:2305.01519v1 [cs.LG])

    [http://arxiv.org/abs/2305.01519](http://arxiv.org/abs/2305.01519)

    BCEdge是一种新颖的学习-based调度框架，使用自适应批处理和DNN推断服务进行并发执行，通过最大化效用来同时实现高吞吐量和低延迟，且优于现有的最先进方法。

    

    随着深度神经网络(DNN)应用于各种边缘智能应用，边缘推断平台具有高吞吐量和低延迟对于提高服务质量非常关键。对于具有多个DNN模型的边缘平台，这种情况给调度程序设计带来了新的挑战。首先，每个请求的服务水平目标(SLOs)都不同，以提高服务质量(QoS)。其次，边缘平台应该能够有效地调度多个异构DNN模型，以提高系统利用率。为了达到这两个目标，本文提出了BCEdge，一种新颖的学习-based调度框架，采用自适应批处理和DNN推断服务在边缘平台上进行并发执行。我们定义了一个实用函数来评估吞吐量和延迟之间的权衡。BCEdge中的调度程序利用了基于最大熵的深度强化学习(DRL)来通过1)共同优化批大小和2)基于SLOs的同时执行数量以最大化效用。实验结果表明，BCEdge在吞吐量和SLO达成率方面优于现有的最先进方法。

    As deep neural networks (DNNs) are being applied to a wide range of edge intelligent applications, it is critical for edge inference platforms to have both high-throughput and low-latency at the same time. Such edge platforms with multiple DNN models pose new challenges for scheduler designs. First, each request may have different service level objectives (SLOs) to improve quality of service (QoS). Second, the edge platforms should be able to efficiently schedule multiple heterogeneous DNN models so that system utilization can be improved. To meet these two goals, this paper proposes BCEdge, a novel learning-based scheduling framework that takes adaptive batching and concurrent execution of DNN inference services on edge platforms. We define a utility function to evaluate the trade-off between throughput and latency. The scheduler in BCEdge leverages maximum entropy-based deep reinforcement learning (DRL) to maximize utility by 1) co-optimizing batch size and 2) the number of concurren
    
[^22]: 定义预测规则的可复制性

    Defining Replicability of Prediction Rules. (arXiv:2305.01518v1 [stat.ME])

    [http://arxiv.org/abs/2305.01518](http://arxiv.org/abs/2305.01518)

    本文提出了一种定义预测规则可复制性的方法，通过多代理人框架定义可复制性，旨在为机器学习中更系统的可复制性评估提供指导。

    

    本文提出了一种定义预测规则可复制性的方法。受最近一份美国国家科学院报告的启发，文章从一个角度出发，认为可复制性是在适合回答同一预测问题的多个研究中获得一致结果，每个研究都有自己的数据。然后讨论了定义该语句关键部分的概念和问题，并专注于典型利用环境中“一致结果”的含义，提出了一个多代理人框架来定义可复制性，代理人既不是合作伙伴也不是对手。其中一些普遍实用的方法成为特例。希望为机器学习中更系统的可复制性评估提供指导。

    In this article I propose an approach for defining replicability for prediction rules. Motivated by a recent NAS report, I start from the perspective that replicability is obtaining consistent results across studies suitable to address the same prediction question, each of which has obtained its own data. I then discuss concept and issues in defining key elements of this statement. I focus specifically on the meaning of "consistent results" in typical utilization contexts, and propose a multi-agent framework for defining replicability, in which agents are neither partners nor adversaries. I recover some of the prevalent practical approaches as special cases. I hope to provide guidance for a more systematic assessment of replicability in machine learning.
    
[^23]: MTrainS: 使用异构内存提高DLRM训练效率

    MTrainS: Improving DLRM training efficiency using heterogeneous memories. (arXiv:2305.01515v1 [cs.IR])

    [http://arxiv.org/abs/2305.01515](http://arxiv.org/abs/2305.01515)

    本文旨在研究现实中部署的深度学习推荐模型中嵌入表的带宽需求和局部性，并通过使用异构内存提出MTrainS来提高DLRM训练效率。

    

    推荐模型非常庞大，在训练时需要使用几TB的内存。为了获得更好的质量，模型的大小和复杂度随着时间的推移而增长，这需要更多的训练数据以避免过拟合。这种模型增长要求数据中心大量资源。因此，训练效率变得越来越重要，以保持数据中心的功率需求可控。在深度学习推荐模型(DLRM)中，通过嵌入表捕捉分类输入的稀疏特征是模型大小的主要贡献者，并且需要高内存带宽。在本文中，我们研究了现实中部署模型中嵌入表的带宽需求和局部性。我们观察到，不同表的带宽要求不均匀，并且嵌入表显示出高时序局部性。然后，我们设计了MTrainS，利用异构内存，包括字节和块可寻址存储类内存，用于DLRM的分层训练。

    Recommendation models are very large, requiring terabytes (TB) of memory during training. In pursuit of better quality, the model size and complexity grow over time, which requires additional training data to avoid overfitting. This model growth demands a large number of resources in data centers. Hence, training efficiency is becoming considerably more important to keep the data center power demand manageable. In Deep Learning Recommendation Models (DLRM), sparse features capturing categorical inputs through embedding tables are the major contributors to model size and require high memory bandwidth. In this paper, we study the bandwidth requirement and locality of embedding tables in real-world deployed models. We observe that the bandwidth requirement is not uniform across different tables and that embedding tables show high temporal locality. We then design MTrainS, which leverages heterogeneous memory, including byte and block addressable Storage Class Memory for DLRM hierarchicall
    
[^24]: 一种无需预设参数的自适应共振理论拓扑聚类算法，实现持续学习

    A Parameter-free Adaptive Resonance Theory-based Topological Clustering Algorithm Capable of Continual Learning. (arXiv:2305.01507v1 [cs.NE])

    [http://arxiv.org/abs/2305.01507](http://arxiv.org/abs/2305.01507)

    本文提出一种无需预设参数的ART拓扑聚类算法，通过引入参数估计方法实现持续学习，并在实验中证明其比现有聚类算法更优。

    

    一般来说，在自适应共振理论（ART）算法中，节点学习过程中的相似度阈值（即警觉参数）对聚类性能有重大影响。此外，拓扑聚类算法中的边缘删除阈值在自组织过程中生成互相分离的聚类中起重要作用。在本文中，我们提出了一种新的无需预设参数的ART拓扑聚类算法，通过引入参数估计方法实现持续学习。针对合成数据集和真实世界数据集的实验结果表明，所提算法在无预设参数的情况下具有比现有聚类算法更优的聚类性能。

    In general, a similarity threshold (i.e., a vigilance parameter) for a node learning process in Adaptive Resonance Theory (ART)-based algorithms has a significant impact on clustering performance. In addition, an edge deletion threshold in a topological clustering algorithm plays an important role in adaptively generating well-separated clusters during a self-organizing process. In this paper, we propose a new parameter-free ART-based topological clustering algorithm capable of continual learning by introducing parameter estimation methods. Experimental results with synthetic and real-world datasets show that the proposed algorithm has superior clustering performance to the state-of-the-art clustering algorithms without any parameter pre-specifications.
    
[^25]: 在大规模共享汽车平台中发现预训练的效果

    Discovering the Effectiveness of Pre-Training in a Large-scale Car-sharing Platform. (arXiv:2305.01506v1 [cs.CV])

    [http://arxiv.org/abs/2305.01506](http://arxiv.org/abs/2305.01506)

    本文针对大规模共享汽车平台上的汽车图像分析任务，通过分析实验验证了预训练技术对于提高模型性能的有效性。

    

    深度学习的最新进展赋予了各种智能交通应用以力量，特别是在共享汽车平台上。传统的共享汽车服务运营高度依赖于车队管理中的人类参与，现代共享汽车平台则允许用户在使用前后上传汽车图像，以检查汽车而无需实地访问。为了自动化上述检查任务，之前的方法利用了深度神经网络并普遍采用了预训练技术以在有限的标记数据集下建立有效的模型。由于处理汽车图像的候选从业者很可能会遭受标记数据集的缺乏，因此我们对预训练的有效性进行了复杂的类比分析。然而，之前的研究主要集中于预训练的效果缺乏深入剖析。鉴于上述分析缺乏，我们的研究介绍了一系列分析，以探究预训练在大规模共享汽车平台中的有效性。我们在大规模汽车图像数据集上尝试了不同的预训练策略，并将已预训练的模型的性能与从头开始训练的传统模型进行比较。我们的发现表明，预训练技术确实提高了汽车图像分析的性能，无论是在准确性还是效率方面。我们的研究为在共享汽车平台应用中探究预训练的有效性提供了洞见。

    Recent progress of deep learning has empowered various intelligent transportation applications, especially in car-sharing platforms. While the traditional operations of the car-sharing service highly relied on human engagements in fleet management, modern car-sharing platforms let users upload car images before and after their use to inspect the cars without a physical visit. To automate the aforementioned inspection task, prior approaches utilized deep neural networks. They commonly employed pre-training, a de-facto technique to establish an effective model under the limited number of labeled datasets. As candidate practitioners who deal with car images would presumably get suffered from the lack of a labeled dataset, we analyzed a sophisticated analogy into the effectiveness of pre-training is important. However, prior studies primarily shed a little spotlight on the effectiveness of pre-training. Motivated by the aforementioned lack of analysis, our study proposes a series of analys
    
[^26]: 模型之间的潜在一致性：提高机器学习模型可靠性的方法

    Great Models Think Alike: Improving Model Reliability via Inter-Model Latent Agreement. (arXiv:2305.01481v1 [cs.LG])

    [http://arxiv.org/abs/2305.01481](http://arxiv.org/abs/2305.01481)

    通过测量模型和一个基础模型的潜在空间之间的一致性来提高机器学习模型的可靠性。

    

    机器学习模型的可靠应用对于深度学习算法的实际部署至关重要。但是由于过度自信，模型经常是不可靠的。本文提出通过测量一个模型的潜在空间与另一个基础模型的潜在空间之间的一致性来估计模型的可靠性。然而，由于它们的不连贯性，即任意旋转和不同的维度，两个不同的潜在空间之间的一致性很难衡量。为了解决这个不连贯性问题，我们设计了一种潜在空间之间的“邻域一致性度量”，并发现这种一致性与模型预测的可靠性有惊人的相关性。此外，我们证明在后续的方式中将邻域一致性融入模型的预测置信度可以显著提高其可靠性。在各种数据集上的故障检测的理论分析和广泛实验验证了这种方法的有效性。

    Reliable application of machine learning is of primary importance to the practical deployment of deep learning methods. A fundamental challenge is that models are often unreliable due to overconfidence. In this paper, we estimate a model's reliability by measuring \emph{the agreement between its latent space, and the latent space of a foundation model}. However, it is challenging to measure the agreement between two different latent spaces due to their incoherence, \eg, arbitrary rotations and different dimensionality. To overcome this incoherence issue, we design a \emph{neighborhood agreement measure} between latent spaces and find that this agreement is surprisingly well-correlated with the reliability of a model's predictions. Further, we show that fusing neighborhood agreement into a model's predictive confidence in a post-hoc way significantly improves its reliability. Theoretical analysis and extensive experiments on failure detection across various datasets verify the effective
    
[^27]: 高斯Copula混合模型的性质研究

    On the properties of Gaussian Copula Mixture Models. (arXiv:2305.01479v1 [cs.LG])

    [http://arxiv.org/abs/2305.01479](http://arxiv.org/abs/2305.01479)

    本文研究了高斯Copula混合模型（GCMM）的性质，开发了基于扩展期望最大算法的参数估计方法，并表明GCMM相比于GMM可以更好地拟合数据并实现更深入的数据挖掘。

    

    高斯Copula混合模型（GCMM）是使用Copula概念的高斯混合模型的推广。本文给出了其数学定义，并研究了似然函数的性质。基于这些属性，我们开发了扩展期望最大算法，用于估计混合Copula的参数，而每个组件对应的边际分布则使用单独的非参数统计方法进行估计。实验表明，相比于GMM，GCMM在相同数量的聚类情况下可以实现更好的拟合；此外，GCMM可以利用每个维度上的不同步数据实现更深入的数据挖掘。

    Gaussian copula mixture models (GCMM) are the generalization of Gaussian Mixture models using the concept of copula. Its mathematical definition is given and the properties of likelihood function are studied in this paper. Based on these properties, extended Expectation Maximum algorithms are developed for estimating parameters for the mixture of copulas while marginal distributions corresponding to each component is estimated using separate nonparametric statistical methods. In the experiment, GCMM can achieve better goodness-of-fitting given the same number of clusters as GMM; furthermore, GCMM can utilize unsynchronized data on each dimension to achieve deeper mining of data.
    
[^28]: 基于肿瘤启发的基因组测序映射模型，用于生成具有所需基因组标志的合成DNA序列

    Cancer-inspired Genomics Mapper Model for the Generation of Synthetic DNA Sequences with Desired Genomics Signatures. (arXiv:2305.01475v1 [q-bio.GN])

    [http://arxiv.org/abs/2305.01475](http://arxiv.org/abs/2305.01475)

    通过结合遗传算法和深度学习方法，我们提出了基于肿瘤启发的基因组测序映射模型（CGMM），该模型基于生物学关系和约束条件来生成具有所需基因组标志的合成DNA序列。

    

    基因组数据在现代医学中至关重要，为诊断和治疗提供了重要的潜力。得益于技术进步，已经对许多百万的健康和患有疾病的基因组进行了测序；然而，对于具体研究和特别是验证研究来说，获取最合适的数据仍然具有规模和可访问性上的挑战。因此，已经提出了基于计算机模拟的基因组序列发生器作为一种可能的解决方案。然而，目前的生成器使用的数据大都较差，主要是使用浅层（随机）连接在训练数据中用有限的计算复杂性检测出来的。这意味着它们没有考虑到最初导致观察到的连接的合适的生物关系和约束条件。为解决这个问题，我们提出了基于肿瘤启发的基因组映射模型（CGMM），它结合了遗传算法（GA）和深度学习（DL）方法来解决这个问题。CGMM模拟了产生基因组序列的过程，以便生成具有所需基因组标志的合成DNA序列。

    Genome data are crucial in modern medicine, offering significant potential for diagnosis and treatment. Thanks to technological advancements, many millions of healthy and diseased genomes have already been sequenced; however, obtaining the most suitable data for a specific study, and specifically for validation studies, remains challenging with respect to scale and access. Therefore, in silico genomics sequence generators have been proposed as a possible solution. However, the current generators produce inferior data using mostly shallow (stochastic) connections, detected with limited computational complexity in the training data. This means they do not take the appropriate biological relations and constraints, that originally caused the observed connections, into consideration. To address this issue, we propose cancer-inspired genomics mapper model (CGMM), that combines genetic algorithm (GA) and deep learning (DL) methods to tackle this challenge. CGMM mimics processes that generate 
    
[^29]: 参数鲁棒马尔科夫链的高效敏感性分析

    Efficient Sensitivity Analysis for Parametric Robust Markov Chains. (arXiv:2305.01473v1 [cs.LG])

    [http://arxiv.org/abs/2305.01473](http://arxiv.org/abs/2305.01473)

    本文提出了一种针对参数鲁棒马尔科夫链的高效敏感性分析方法，可以选择具有最高偏导数的$k$个参数子集并应用于包含超过一百万状态和数千个参数的大型模型中。

    

    我们提供了一种敏感性分析参数鲁棒马尔科夫链的新方法。这些模型包括参数和概率分布集合，以减轻精确概率可用的不现实假设。我们根据与不确定转移概率相关的措施（如期望奖励）的偏导数来衡量敏感性。作为我们的主要贡献，我们提出了一种计算这些偏导数的高效方法。为了扩展我们的方法以适用于具有数千个参数的模型，我们提出了一种该方法的扩展，可以选择具有最高偏导数的$k$个参数子集。我们的方法基于线性规划，并在给定参数值附近对这些程序进行微分。实验表明，我们的方法适用于具有超过一百万状态和数千个参数的模型。此外，我们将结果嵌入到迭代学习方案中，使得我们可以更快地获取对参数的响应并用于改进模型。

    We provide a novel method for sensitivity analysis of parametric robust Markov chains. These models incorporate parameters and sets of probability distributions to alleviate the often unrealistic assumption that precise probabilities are available. We measure sensitivity in terms of partial derivatives with respect to the uncertain transition probabilities regarding measures such as the expected reward. As our main contribution, we present an efficient method to compute these partial derivatives. To scale our approach to models with thousands of parameters, we present an extension of this method that selects the subset of $k$ parameters with the highest partial derivative. Our methods are based on linear programming and differentiating these programs around a given value for the parameters. The experiments show the applicability of our approach on models with over a million states and thousands of parameters. Moreover, we embed the results within an iterative learning scheme that profi
    
[^30]: 带有基于图的上下文的随机情境赌博机问题

    Stochastic Contextual Bandits with Graph-based Contexts. (arXiv:2305.01470v1 [cs.LG])

    [http://arxiv.org/abs/2305.01470](http://arxiv.org/abs/2305.01470)

    本文提出了一种基于图上下文的随机情境赌博机问题，其中节点标签相同的节点共享相同的奖励分布。对于线图和树，我们提出了一种具有遗憾界的算法。

    

    本文将在线图预测问题自然地推广到了随机情境赌博问题的一个版本，其中上下文是图中的节点，而图的结构提供了有关上下文相似性的信息。具体而言，我们给出一个图$G = (V，E)$，其节点集$V$表示具有未知顶点标签$y$的上下文。在我们的随机情境赌博机设置中，具有相同标签的顶点共享同一奖励分布。在图标签预测中，标准的实例难度概念是割大小$f$，即有不同标签结束点的边数。对于线图和树，我们提出了一种具有遗憾界的算法$O(T^{2/3}K^{1/3}f^{1/3})$，其中$K$是手臂数量。我们的算法依赖于Zimmert和Seldin~[AISTAT'19，JMLR'21]的最优随机赌徒算法。当最佳手臂的表现优于其他手臂时，遗憾界将改善为$\tilde{O}(\sqrt{KT\cdot f})$。

    We naturally generalize the on-line graph prediction problem to a version of stochastic contextual bandit problems where contexts are vertices in a graph and the structure of the graph provides information on the similarity of contexts. More specifically, we are given a graph $G=(V,E)$, whose vertex set $V$ represents contexts with {\em unknown} vertex label $y$. In our stochastic contextual bandit setting, vertices with the same label share the same reward distribution. The standard notion of instance difficulties in graph label prediction is the cutsize $f$ defined to be the number of edges whose end points having different labels. For line graphs and trees we present an algorithm with regret bound of $\tilde{O}(T^{2/3}K^{1/3}f^{1/3})$ where $K$ is the number of arms. Our algorithm relies on the optimal stochastic bandit algorithm by Zimmert and Seldin~[AISTAT'19, JMLR'21]. When the best arm outperforms the other arms, the regret improves to $\tilde{O}(\sqrt{KT\cdot f})$. The regret 
    
[^31]: 循环神经网络的记忆：我们计算得对吗？

    Memory of recurrent networks: Do we compute it right?. (arXiv:2305.01457v1 [cs.LG])

    [http://arxiv.org/abs/2305.01457](http://arxiv.org/abs/2305.01457)

    本文研究了线性回声状态网络的记忆容量计算问题。通过发现数值评估的不准确性主要源于数值方面的问题，提出了基于掩码矩阵MC相对于中立性的稳健数值方法，该方法可以解决数值评估中的误差问题。

    

    文献中对于循环神经网络的记忆容量（MC）的数值评估常常与已经建立的理论界限相矛盾。本文研究了线性回声状态网络的情况，对应的Kalman可控矩阵的秩已被证明等于总记忆容量。我们揭示了关于记忆不准确的数值评估的各种原因，并表明这些问题是纯粹数值方面上的，往往在近期文献中被忽视。更明确地说，我们证明了当线性MC的Krylov结构被忽略时，理论MC和它的经验值之间会存在差距。解决这一问题的方法是，利用MC相对于输入掩码矩阵的中立性，开发出稳健的数值方法。模拟结果显示，我们提出的方法得到的记忆曲线与理论完全一致。

    Numerical evaluations of the memory capacity (MC) of recurrent neural networks reported in the literature often contradict well-established theoretical bounds. In this paper, we study the case of linear echo state networks, for which the total memory capacity has been proven to be equal to the rank of the corresponding Kalman controllability matrix. We shed light on various reasons for the inaccurate numerical estimations of the memory, and we show that these issues, often overlooked in the recent literature, are of an exclusively numerical nature. More explicitly, we prove that when the Krylov structure of the linear MC is ignored, a gap between the theoretical MC and its empirical counterpart is introduced. As a solution, we develop robust numerical approaches by exploiting a result of MC neutrality with respect to the input mask matrix. Simulations show that the memory curves that are recovered using the proposed methods fully agree with the theory.
    
[^32]: 疫苗供应链优化的预测协调

    Forecast reconciliation for vaccine supply chain optimization. (arXiv:2305.01455v1 [cs.LG])

    [http://arxiv.org/abs/2305.01455](http://arxiv.org/abs/2305.01455)

    本文尝试通过层次时间序列来预测疫苗销售数据，并使用协调方法解决了不同层次预测不一致的问题，结果表明最小迹和加权最小二乘与结构缩放的协调方法效果最佳，提高了预测的一致性并减少预测误差。

    

    疫苗供应链优化通过按类型或地点分组的层次时间序列预测来获益。然而，当高层次的预测结果与低层次预测结果之和不匹配时，不同层次的预测变得不一致，这可以通过协调方法进行解决。本文通过将2010年至2021年GSK公司的销售数据建模为层次时间序列，解决了疫苗销售预测问题。在使用多个ARIMA模型预测未来值后，我们系统地比较了各种协调方法的性能，并使用统计检验进行了比较。我们还比较了COVID之前和之后的预测性能。结果突出了最小迹和加权最小二乘与结构缩放的协调方法，这些方法提供了一致的预测并减少了基线ARIMA的预测误差。

    Vaccine supply chain optimization can benefit from hierarchical time series forecasting, when grouping the vaccines by type or location. However, forecasts of different hierarchy levels become incoherent when higher levels do not match the sum of the lower levels forecasts, which can be addressed by reconciliation methods.  In this paper, we tackle the vaccine sale forecasting problem by modeling sales data from GSK between 2010 and 2021 as a hierarchical time series. After forecasting future values with several ARIMA models, we systematically compare the performance of various reconciliation methods, using statistical tests. We also compare the performance of the forecast before and after COVID. The results highlight Minimum Trace and Weighted Least Squares with Structural scaling as the best performing methods, which provided a coherent forecast while reducing the forecast error of the baseline ARIMA.
    
[^33]: 时间序列外源回归的无监督特征算法

    Unsupervised Feature Based Algorithms for Time Series Extrinsic Regression. (arXiv:2305.01429v1 [cs.LG])

    [http://arxiv.org/abs/2305.01429](http://arxiv.org/abs/2305.01429)

    本研究提出了两种新的TSER算法：FreshPRINCE和DrCIF，它们分别由一组汇总特征和多个条件推理树构成，能够更好地在时间序列外源回归的问题上预测响应变量，比起以前的评估中使用的基线算法表现更佳。

    

    时间序列外源回归（TSER）涉及使用一组训练时间序列来形成一个连续响应变量的预测模型，该变量与回归器序列没有直接关系。TSER存档用于比较算法于2022年发布，包括19个问题。我们将此存档的大小增加到63个问题，并重现以前算法的基准比较。然后，我们扩展比较，包括更广泛的标准回归器和以前研究中使用的最新版本的TSER模型。我们表明，以前评估的回归器都不能胜过标准分类器旋转森林的回归适应。我们引入了两个新的TSER算法，这些算法是从时间序列分类的相关工作中开发而来。FreshPRINCE是一个管道估计器，包括转换到各种汇总特征，然后是一个旋转森林回归器。DrCIF是一个树集合，它根据时间序列的随机增量创建汇总统计信息特征，然后使用多个条件推理树来预测响应变量。在扩展的TSER问题集上，FreshPRINCE和DrCIF都始终优于基准算法。

    Time Series Extrinsic Regression (TSER) involves using a set of training time series to form a predictive model of a continuous response variable that is not directly related to the regressor series. The TSER archive for comparing algorithms was released in 2022 with 19 problems. We increase the size of this archive to 63 problems and reproduce the previous comparison of baseline algorithms. We then extend the comparison to include a wider range of standard regressors and the latest versions of TSER models used in the previous study. We show that none of the previously evaluated regressors can outperform a regression adaptation of a standard classifier, rotation forest. We introduce two new TSER algorithms developed from related work in time series classification. FreshPRINCE is a pipeline estimator consisting of a transform into a wide range of summary features followed by a rotation forest regressor. DrCIF is a tree ensemble that creates features from summary statistics over random i
    
[^34]: 从本地到全球：应对非洲语言多样性的挑战

    From Local to Global: Navigating Linguistic Diversity in the African Context. (arXiv:2305.01427v1 [cs.CL])

    [http://arxiv.org/abs/2305.01427](http://arxiv.org/abs/2305.01427)

    摘要介绍了如何应对非洲语言多样性的挑战，并提出了模型作为产品教学的教学工具的想法，该方法对于寻求改善非洲本土方言客户体验和产品开发的企业有重要影响。

    

    本文探讨了自然语言处理（NLP）中存在的有关非洲大陆上语言多样性的关键问题，特别是非洲的地方方言和鲜为人知的阿拉伯方言。我们评估了各种方法，展示了它们的有效性，同时强调了所提出方法对于寻求改进非洲本土方言的客户体验和产品开发的企业可能产生的潜在影响。使用该模型作为产品教学的教学工具的想法也很有趣，因为这可能会激发学习者的兴趣并引发科技创业。总的来说，我们的改进方法提供了一个应对非洲本土方言挑战的有前途的分析。特别是鲜为人知的阿拉伯方言，这对于寻求改进客户体验和产品开发的企业可能会产生重大影响。

    The focus is on critical problems in NLP related to linguistic diversity and variation across the African continent, specifically with regards to African local di- alects and Arabic dialects that have received little attention. We evaluated our various approaches, demonstrating their effectiveness while highlighting the potential impact of the proposed approach on businesses seeking to improve customer experience and product development in African local dialects. The idea of using the model as a teaching tool for product-based instruction is interesting, as it could potentially stimulate interest in learners and trigger techno entrepreneurship. Overall, our modified approach offers a promising analysis of the challenges of dealing with African local dialects. Particularly Arabic dialects, which could have a significant impact on businesses seeking to improve customer experience and product development.
    
[^35]: Mercer核的绝对可积性仅仅是RKHS稳定性的充分条件

    Absolute integrability of Mercer kernels is only sufficient for RKHS stability. (arXiv:2305.01411v1 [eess.SY])

    [http://arxiv.org/abs/2305.01411](http://arxiv.org/abs/2305.01411)

    本文证明了Mercer核的绝对可积性仅仅是RKHS稳定性的充分条件。

    

    再生核希尔伯特空间（RKHS）是一种与称为核的正定映射一一对应的特殊希尔伯特空间。它们被广泛应用于机器学习中，用于从稀疏嘈杂的数据中重建未知的函数。在过去的二十年中，在线性系统识别的背景下也引入了稳定RKHS的子类。稳定RKHS仅包含正实数线上的绝对可积脉冲响应。因此，它们可以作为假设空间用于从输入输出数据中估计线性、时不变和BIBO稳定的动态系统。RKHS稳定性的必要和充分条件已经在文献中得到了证明，并且已知核的绝对可积性意味着稳定性。最近的一项工作中，我们通过在离散时间中工作，证明了这个条件只是充分条件。在连续时间中工作，本文旨在证明同样的结果也适用于Mercer核。

    Reproducing kernel Hilbert spaces (RKHSs) are special Hilbert spaces in one-to-one correspondence with positive definite maps called kernels. They are widely employed in machine learning to reconstruct unknown functions from sparse and noisy data. In the last two decades, a subclass known as stable RKHSs has been also introduced in the setting of linear system identification. Stable RKHSs contain only absolutely integrable impulse responses over the positive real line. Hence, they can be adopted as hypothesis spaces to estimate linear, time-invariant and BIBO stable dynamic systems from input-output data. Necessary and sufficient conditions for RKHS stability are available in the literature and it is known that kernel absolute integrability implies stability. Working in discrete-time, in a recent work we have proved that this latter condition is only sufficient. Working in continuous-time, it is the purpose of this note to prove that the same result holds also for Mercer kernels.
    
[^36]: 通过自动微分计算应力和热通量

    Stress and heat flux via automatic differentiation. (arXiv:2305.01401v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2305.01401](http://arxiv.org/abs/2305.01401)

    本文报道了一种自动微分方法，用于计算机器学习势能中的应力和热通量，该方法在测试中取得了良好的效果。

    

    机器学习势能可以高效准确地近似描述BOPES（Born-Oppenheimer势能面），该势能决定了许多材料特性。模拟技术通常需要梯度，尤其是分子动力学中的力和应力以及热学输运中的热通量。最近开发的势能特征具有高的相互作用能量和包括通过消息传递机制的等变半局部相互作用。由于它们的复杂功能形式，它们依赖于自动微分（AD），从而克服了手动实现或有限差分方案来计算梯度的需要。这项研究展示了一种统一的AD方法，用于获取这种势能的力、应力和热通量，并提供了一种与模型无关的实现。该方法在Lennard-Jones势能上进行了测试，然后应用于使用等变mes模型预测锡硒化物的内聚特性和热导率。

    Machine-learning potentials provide computationally efficient and accurate approximations of the Born-Oppenheimer potential energy surface. This potential determines many materials properties and simulation techniques usually require its gradients, in particular forces and stress for molecular dynamics, and heat flux for thermal transport properties. Recently developed potentials feature high body order and can include equivariant semi-local interactions through message-passing mechanisms. Due to their complex functional forms, they rely on automatic differentiation (AD), overcoming the need for manual implementations or finite-difference schemes to evaluate gradients. This study demonstrates a unified AD approach to obtain forces, stress, and heat flux for such potentials, and provides a model-independent implementation. The method is tested on the Lennard-Jones potential, and then applied to predict cohesive properties and thermal conductivity of tin selenide using an equivariant mes
    
[^37]: 返回分布规划：鲁棒性模仿学习

    Get Back Here: Robust Imitation by Return-to-Distribution Planning. (arXiv:2305.01400v1 [cs.RO])

    [http://arxiv.org/abs/2305.01400](http://arxiv.org/abs/2305.01400)

    本文提出的算法POIR将行为克隆和规划器相结合，解决了模仿学习中分布偏移的问题，模型在机器人操纵任务中表现出强大的鲁棒性。

    

    我们考虑模仿学习的设置，即专家数据不是在实际部署环境下收集的，而是在另一个版本上收集的。为了解决由此导致的分布偏移问题，我们将行为克隆（BC）与一个规划器相结合，规划器的任务是在代理程序偏离演示分布时，将代理程序带回到专家访问的状态。得到的算法POIR可以离线训练，并利用在线交互来有效地优化其规划器，以逐步提高性能。我们在现实的机器人操纵模拟器中对POIR进行测试，使用各种人类生成的操作演示，并展示了学习策略对不同初始状态分布和嘈杂动态的鲁棒性。

    We consider the Imitation Learning (IL) setup where expert data are not collected on the actual deployment environment but on a different version. To address the resulting distribution shift, we combine behavior cloning (BC) with a planner that is tasked to bring the agent back to states visited by the expert whenever the agent deviates from the demonstration distribution. The resulting algorithm, POIR, can be trained offline, and leverages online interactions to efficiently fine-tune its planner to improve performance over time. We test POIR on a variety of human-generated manipulation demonstrations in a realistic robotic manipulation simulator and show robustness of the learned policy to different initial state distributions and noisy dynamics.
    
[^38]: 医学影像中的人口统计学不变模型和表示是否公平？

    Are demographically invariant models and representations in medical imaging fair?. (arXiv:2305.01397v1 [cs.LG])

    [http://arxiv.org/abs/2305.01397](http://arxiv.org/abs/2305.01397)

    医学影像模型编码患者人口统计信息，引发有关潜在歧视的担忧。研究表明，不编码人口属性的模型容易损失预测性能，而考虑人口统计属性的反事实模型不变性存在复杂性。人口统计学编码可以被认为是优势。

    

    研究表明，医学成像模型在其潜在表示中编码了有关患者人口统计学信息（年龄、种族、性别），这引发了有关其潜在歧视的担忧。在这里，我们询问是否可行和值得训练不编码人口属性的模型。我们考虑不同类型的与人口统计学属性的不变性，即边际、类条件和反事实模型不变性，并说明它们与算法公平的标准概念的等价性。根据现有理论，我们发现边际和类条件的不变性可被认为是实现某些公平概念的过度限制方法，导致显著的预测性能损失。关于反事实模型不变性，我们注意到对于人口统计学属性，定义医学图像反事实存在复杂性。最后，我们认为人口统计学编码甚至可以被认为是优势。

    Medical imaging models have been shown to encode information about patient demographics (age, race, sex) in their latent representation, raising concerns about their potential for discrimination. Here, we ask whether it is feasible and desirable to train models that do not encode demographic attributes. We consider different types of invariance with respect to demographic attributes marginal, class-conditional, and counterfactual model invariance - and lay out their equivalence to standard notions of algorithmic fairness. Drawing on existing theory, we find that marginal and class-conditional invariance can be considered overly restrictive approaches for achieving certain fairness notions, resulting in significant predictive performance losses. Concerning counterfactual model invariance, we note that defining medical image counterfactuals with respect to demographic attributes is fraught with complexities. Finally, we posit that demographic encoding may even be considered advantageou
    
[^39]: 边缘计算领域实现增强隐私性的高效联邦学习方法

    Efficient Federated Learning with Enhanced Privacy via Lottery Ticket Pruning in Edge Computing. (arXiv:2305.01387v1 [cs.DC])

    [http://arxiv.org/abs/2305.01387](http://arxiv.org/abs/2305.01387)

    本文提出了一种新的联邦学习方法，名为Fed-LTP，该方法采用彩票卷积稀疏网络和零集中差分隐私算法，能够高效实现隐私保护以及资源约束的移动终端协作学习。

    

    联邦学习（FL）是一种协作学习范式，用于分散的移动终端（MT）中的私有数据。然而，它在通信、MT资源和隐私方面存在问题。现有的隐私保护FL方法通常采用实例级差分隐私（DP），它提供了严格的隐私保证，但存在性能下降、传输开销和边缘设备（如MT）的资源约束等几个瓶颈。为了克服这些缺点，我们提出了Fed-LTP，这是一种高效且增强隐私的FL框架，采用彩票卷积稀疏网络和零集中DP（zCDP）算法。它在服务器上生成修剪的全局模型，并在客户端上使用zCDP进行网络训练。服务器端提出了两种修剪方案：（i）基于权重的修剪（LTH）决定修剪全局模型所需的非零模型元素数量和位置；（ii）基于模式的修剪（PTH）修剪全局模型,使其与MT的模式相关。

    Federated learning (FL) is a collaborative learning paradigm for decentralized private data from mobile terminals (MTs). However, it suffers from issues in terms of communication, resource of MTs, and privacy. Existing privacy-preserving FL methods usually adopt the instance-level differential privacy (DP), which provides a rigorous privacy guarantee but with several bottlenecks: severe performance degradation, transmission overhead, and resource constraints of edge devices such as MTs. To overcome these drawbacks, we propose Fed-LTP, an efficient and privacy-enhanced FL framework with \underline{\textbf{L}}ottery \underline{\textbf{T}}icket \underline{\textbf{H}}ypothesis (LTH) and zero-concentrated D\underline{\textbf{P}} (zCDP). It generates a pruned global model on the server side and conducts sparse-to-sparse training from scratch with zCDP on the client side. On the server side, two pruning schemes are proposed: (i) the weight-based pruning (LTH) determines the pruned global mode
    
[^40]: 基于类的影响函数用于误差检测

    Class based Influence Functions for Error Detection. (arXiv:2305.01384v1 [cs.CL])

    [http://arxiv.org/abs/2305.01384](http://arxiv.org/abs/2305.01384)

    该论文提出了基于类别信息的影响函数来提高异常检测的稳定性和性能。

    

    影响函数(IFs)是在大规模数据集中检测异常样本的强大工具。然而，当应用于深度网络时，它们不稳定。本文解释了IFs不稳定的原因，并提出了解决这个问题的方法。我们表明，在两个数据点属于两个不同类别时，IFs是不可靠的。我们的解决方法利用类别信息来改进IFs的稳定性。大量实验证明，我们的修改显著提高了IFs的性能和稳定性，同时不增加任何计算成本。

    Influence functions (IFs) are a powerful tool for detecting anomalous examples in large scale datasets. However, they are unstable when applied to deep networks. In this paper, we provide an explanation for the instability of IFs and develop a solution to this problem. We show that IFs are unreliable when the two data points belong to two different classes. Our solution leverages class information to improve the stability of IFs. Extensive experiments show that our modification significantly improves the performance and stability of IFs while incurring no additional computational cost.
    
[^41]: 基于LTL规范的样本有效无模型强化学习与优化保证

    Sample Efficient Model-free Reinforcement Learning from LTL Specifications with Optimality Guarantees. (arXiv:2305.01381v1 [cs.LG])

    [http://arxiv.org/abs/2305.01381](http://arxiv.org/abs/2305.01381)

    本文提出了一种基于LTL规范的无模型强化学习方法，该方法结合乘积MDP、奖励结构和折扣机制有效地学习并优化未知随机系统最大化满足LTL规范的概率的最优策略。

    

    线性时间逻辑（LTL）广泛用于指定系统策略的高级目标，自主系统学习相对于这样的规范的最优策略是非常理想的。 但是，从LTL规范中学习最优策略并不轻松。我们提出了一种无模型强化学习（RL）方法，该方法可以有效地学习未知随机系统的最优策略，其中使用马尔可夫决策过程（MDP）进行建模。我们提出了一种新颖且更通用的乘积MDP、奖励结构和折扣机制，当与现成的无模型RL算法结合使用时，能够高效地学习最大化给定LTL规范满足概率的最优策略，并提供了更好的有关选择RL中关键参数以保证最优性的理论结果。为了直接评估学习策略，我们采用概率模型检查器PRISM来计算LTL规范的满足概率。

    Linear Temporal Logic (LTL) is widely used to specify high-level objectives for system policies, and it is highly desirable for autonomous systems to learn the optimal policy with respect to such specifications. However, learning the optimal policy from LTL specifications is not trivial. We present a model-free Reinforcement Learning (RL) approach that efficiently learns an optimal policy for an unknown stochastic system, modelled using Markov Decision Processes (MDPs). We propose a novel and more general product MDP, reward structure and discounting mechanism that, when applied in conjunction with off-the-shelf model-free RL algorithms, efficiently learn the optimal policy that maximizes the probability of satisfying a given LTL specification with optimality guarantees. We also provide improved theoretical results on choosing the key parameters in RL to ensure optimality. To directly evaluate the learned policy, we adopt probabilistic model checker PRISM to compute the probability of 
    
[^42]: LogSpecT: 从平稳信号中学习可行的图形学习模型并具备恢复保证

    LogSpecT: Feasible Graph Learning Model from Stationary Signals with Recovery Guarantees. (arXiv:2305.01379v1 [stat.ML])

    [http://arxiv.org/abs/2305.01379](http://arxiv.org/abs/2305.01379)

    本文提出了一种新的图形学习模型LogSpecT及其实际公式rLogSpecT，以解决现有模型rSpecT敏感超参数选择、不可行的问题。本文提供了rLogSpecT的恢复保证，并提出了基于L-ADMM的高效算法。

    

    信号图形学习是图形信号处理（GSP）中的核心任务。学习平稳信号图形最常用的模型之一是SpecT。然而，它的实际公式rSpecT被认为对超参数选择敏感，并且更糟的是，容易无法实现。在本文中，我们首次给出保证rSpecT无法实现的条件，并设计了一种新模型（LogSpecT）及其实际公式（rLogSpecT）来解决这个问题。与rSpecT不同，新的实用模型rLogSpecT始终是可行的。此外，我们还提供了rLogSpecT的恢复保证，这些保证来自于与epi-converg​​ence相关的现代优化工具。这些工具对于各种学习问题都具有独立的利益和重要性。为了展示rLogSpecT在实践中的优点，我们提出了一种基于线性化交替方向乘子方法（L-ADMM）的高效算法。L-ADMM的子问题

    Graph learning from signals is a core task in Graph Signal Processing (GSP). One of the most commonly used models to learn graphs from stationary signals is SpecT. However, its practical formulation rSpecT is known to be sensitive to hyperparameter selection and, even worse, to suffer from infeasibility. In this paper, we give the first condition that guarantees the infeasibility of rSpecT and design a novel model (LogSpecT) and its practical formulation (rLogSpecT) to overcome this issue. Contrary to rSpecT, the novel practical model rLogSpecT is always feasible. Furthermore, we provide recovery guarantees of rLogSpecT, which are derived from modern optimization tools related to epi-convergence. These tools could be of independent interest and significant for various learning problems. To demonstrate the advantages of rLogSpecT in practice, a highly efficient algorithm based on the linearized alternating direction method of multipliers (L-ADMM) is proposed. The subproblems of L-ADMM a
    
[^43]: 随机函数下降法

    Random Function Descent. (arXiv:2305.01377v1 [math.OC])

    [http://arxiv.org/abs/2305.01377](http://arxiv.org/abs/2305.01377)

    本文提出了随机函数下降(RFD)算法，可以在随机环境中计算出步长并且与贝叶斯优化中的梯度下降算法相同。在合成基准测试中，RFD算法比未调整的Adam方法表现更好，提出的heuristic扩展可与调整后的Adam方法相媲美。

    

    虽然梯度下降方法在机器学习中十分常见，但是选择正确的步长经常需要进行“超参数调整”。这是因为回溯程序如Armijo's准则依赖于每个步骤中的质量评估，而这些评估在随机情况下不可用。由于优化方案可以用Taylor逼近来解释，我们将Taylor逼近替换为条件期望（最佳的$L^2$估计），提出了“随机函数下降”（RFD）。 在Bayesian优化中常见的一些轻微假设的情况下，我们证明了RFD与梯度下降算法是相同的，但是在随机情况下具有可计算的步长。我们在合成基准测试中比未调整的Adam方法表现更好。为了缩小与调整后的Adam算法之间的性能差距，我们提出了一种启发式扩展，可与调整后的Adam方法相媲美。

    While gradient based methods are ubiquitous in machine learning, selecting the right step size often requires "hyperparameter tuning". This is because backtracking procedures like Armijo's rule depend on quality evaluations in every step, which are not available in a stochastic context. Since optimization schemes can be motivated using Taylor approximations, we replace the Taylor approximation with the conditional expectation (the best $L^2$ estimator) and propose "Random Function Descent" (RFD). Under light assumptions common in Bayesian optimization, we prove that RFD is identical to gradient descent, but with calculable step sizes, even in a stochastic context. We beat untuned Adam in synthetic benchmarks. To close the performance gap to tuned Adam, we propose a heuristic extension competitive with tuned Adam.
    
[^44]: 带输出误差噪声模型的Hamiltonian神经网络物理学知识驱动学习研究

    Physics-Informed Learning Using Hamiltonian Neural Networks with Output Error Noise Models. (arXiv:2305.01338v1 [eess.SY])

    [http://arxiv.org/abs/2305.01338](http://arxiv.org/abs/2305.01338)

    本文介绍了一种输出误差Hamiltonian神经网络（OE-HNN）建模方法，以解决具有输入和噪声状态测量的物理系统建模问题。OE-HNN通过使用嵌入式ODE求解器从带噪声的状态测量数据中学习动力学，并在三个物理系统上得到了准确的预测结果。

    

    为了使物理系统数据驱动模型的可解释性和可靠性，必须将先前的物理知识融入到建模框架中。Hamiltonian神经网络(HNN)将哈密顿理论实现在深度学习中，形成了用于建模自治能量守恒系统的综合框架。本文介绍了一种输出误差Hamiltonian神经网络（OE-HNN）建模方法，以解决具有输入和噪声状态测量的物理系统建模的挑战。此外，它不需要已知状态导数。相反，OE-HNN在训练过程中利用嵌入式ODE求解器，使其能够从带噪声的状态测量数据中学习动力学。该方法在三个物理系统上进行了演示：阻尼谐振子，混沌系统和机械臂模型。结果表明，OE-HNN提供了带有噪声测量的系统动态的准确预测，并在解释性和可靠性方面优于传统数据驱动模型。

    In order to make data-driven models of physical systems interpretable and reliable, it is essential to include prior physical knowledge in the modeling framework. Hamiltonian Neural Networks (HNNs) implement Hamiltonian theory in deep learning and form a comprehensive framework for modeling autonomous energy-conservative systems. Despite being suitable to estimate a wide range of physical system behavior from data, classical HNNs are restricted to systems without inputs and require noiseless state measurements and information on the derivative of the state to be available. To address these challenges, this paper introduces an Output Error Hamiltonian Neural Network (OE-HNN) modeling approach to address the modeling of physical systems with inputs and noisy state measurements. Furthermore, it does not require the state derivatives to be known. Instead, the OE-HNN utilizes an ODE-solver embedded in the training process, which enables the OE-HNN to learn the dynamics from noisy state meas
    
[^45]: 通过动态控制匹配验证大规模自适应测试

    Validation of massively-parallel adaptive testing using dynamic control matching. (arXiv:2305.01334v1 [cs.LG])

    [http://arxiv.org/abs/2305.01334](http://arxiv.org/abs/2305.01334)

    本文提出了一种方法，使用随着测试而不断适应的匹配合成控制组，来区分动态并行测试中各种测试的因果效应。

    

    A/B测试是市场优化中广泛使用的一种范例，因为它承诺能够识别因果效应，并且它已经在大多数消息传递软件平台中实现。然而，现代企业经常同时并行运行多个A/B/n测试，并将许多内容变化打包到相同的消息中，其中并不是所有内容变化都是明确测试的一部分。无论是由于许多团队同时测试，还是作为更复杂的强化学习（RL）方法的一部分，该方法不断根据以前的结果调整测试和测试条件分配，动态并行测试不能像传统的A/B测试一样评估。本文提出了一种方法，使用随着测试而不断适应的匹配合成控制组，在连续测试适应的条件下区分各种测试的因果效应。

    A/B testing is a widely-used paradigm within marketing optimization because it promises identification of causal effects and because it is implemented out of the box in most messaging delivery software platforms. Modern businesses, however, often run many A/B/n tests at the same time and in parallel, and package many content variations into the same messages, not all of which are part of an explicit test. Whether as the result of many teams testing at the same time, or as part of a more sophisticated reinforcement learning (RL) approach that continuously adapts tests and test condition assignment based on previous results, dynamic parallel testing cannot be evaluated the same way traditional A/B tests are evaluated. This paper presents a method for disentangling the causal effects of the various tests under conditions of continuous test adaptation, using a matched-synthetic control group that adapts alongside the tests.
    
[^46]: 无投影在线凸优化与随机约束

    Projection-Free Online Convex Optimization with Stochastic Constraints. (arXiv:2305.01333v1 [math.OC])

    [http://arxiv.org/abs/2305.01333](http://arxiv.org/abs/2305.01333)

    该论文提出了一种用于处理带有随机约束的在线凸优化的无投影算法，其能够取得亚线性遗憾和约束违规的效果。

    

    该论文提出了一种用于带有随机约束的在线凸优化的无投影算法。我们设计了一个在线原始-对偶无投影框架，该框架可以采用任何用于在线凸优化的无长期约束的无投影算法。使用该模板，我们推导出各种情况下的亚线性遗憾和约束违规边界。此外，对于损失函数和约束函数都光滑的情况，我们开发了一种原始-对偶条件梯度方法，实现了$O(\sqrt{T})$的遗憾和 $O(T^{3/4})$的约束违规。此外，对于损失函数和约束函数都是随机的情况，并且与关联的离线随机优化问题存在强对偶性的情况，我们证明了约束违规可以被减少到与遗憾有相同的渐近增长。

    This paper develops projection-free algorithms for online convex optimization with stochastic constraints. We design an online primal-dual projection-free framework that can take any projection-free algorithms developed for online convex optimization with no long-term constraint. With this general template, we deduce sublinear regret and constraint violation bounds for various settings. Moreover, for the case where the loss and constraint functions are smooth, we develop a primal-dual conditional gradient method that achieves $O(\sqrt{T})$ regret and $O(T^{3/4})$ constraint violations. Furthermore, for the setting where the loss and constraint functions are stochastic and strong duality holds for the associated offline stochastic optimization problem, we prove that the constraint violation can be reduced to have the same asymptotic growth as the regret.
    
[^47]: 基于Option框架的多模式探索自主非单体智能体

    An Autonomous Non-monolithic Agent with Multi-mode Exploration based on Options Framework. (arXiv:2305.01322v1 [cs.AI])

    [http://arxiv.org/abs/2305.01322](http://arxiv.org/abs/2305.01322)

    该论文关注强化学习中的探索研究，提出了一个能够自主管理探索策略的多模式智能体非单体探索方法，并通过实验结果展示了该方法的优越性能。

    

    强化学习领域的探索研究主要关注“如何探索”的探索方式，而“何时探索”的探索研究一直没有成为重点。典型的探索行为通常将探索行为与智能体的开发利用行为绑定在一起。最近出现了非单体探索行为的研究，以研究人类和动物的模式切换行为。本研究的最终目的是使智能体能够自主决定何时探索或利用。我们在Option框架中描述了自主多模式探索的初始研究。通过比较实验结果，我们展示了我们的方法相对于现有的非单体探索方法的更高性能。

    Most exploration research on reinforcement learning (RL) has paid attention to `the way of exploration', which is `how to explore'. The other exploration research, `when to explore', has not been the main focus of RL exploration research. \textcolor{black}{The issue of `when' of a monolithic exploration in the usual RL exploration behaviour binds an exploratory action to an exploitational action of an agent. Recently, a non-monolithic exploration research has emerged to examine the mode-switching exploration behaviour of humans and animals.} The ultimate purpose of our research is to enable an agent to decide when to explore or exploit autonomously. We describe the initial research of an autonomous multi-mode exploration of non-monolithic behaviour in an options framework. The higher performance of our method is shown against the existing non-monolithic exploration method through comparative experimental results.
    
[^48]: 一种基于强化学习的风力涡轮机偏航控制方法的改进

    An Improved Yaw Control Algorithm for Wind Turbines via Reinforcement Learning. (arXiv:2305.01299v1 [cs.LG])

    [http://arxiv.org/abs/2305.01299](http://arxiv.org/abs/2305.01299)

    本文利用强化学习开发了一个偏航控制程序，以最小化偏航偏差，并优化重新分配偏航资源，使高速段得到优先处理，同时保持偏航使用率低，并成功降低了偏航偏差，增加了涡轮机额外利润，减少机械负载和延长了使用寿命。

    

    风向与风轮转向机孔位之间的偏差会对风力涡轮机及其整个风电场的功率输出、安全性和寿命产生影响。本文利用强化学习开发了一个偏航控制程序，以最小化偏航偏差，并优化重新分配偏航资源，重点优化高速段，同时保持偏航使用率低。为实现这一目标，我们仔细设计和测试了奖励标准，以平衡偏航使用率与偏航校准之间（与功率产量成比例）的权衡，并基于来自 REpower MM82 2MW 涡轮机的实际风速记录创建了一个新的模拟器（环境）。该算法的两个模拟实验中，将偏航偏差分别降低了 5.5% 和 11.2%，相对于传统的主动偏航控制算法。相应地，平均净能量增益分别为 0.31% 和 0.33%，相对于传统偏航控制算法。在单个 2MW 涡轮机上，这意味着增加了大约 4500 美元的额外利润，同时减少了机械负载和延长了其使用寿命。

    Yaw misalignment, measured as the difference between the wind direction and the nacelle position of a wind turbine, has consequences on the power output, the safety and the lifetime of the turbine and its wind park as a whole. We use reinforcement learning to develop a yaw control agent to minimise yaw misalignment and optimally reallocate yaw resources, prioritising high-speed segments, while keeping yaw usage low. To achieve this, we carefully crafted and tested the reward metric to trade-off yaw usage versus yaw alignment (as proportional to power production), and created a novel simulator (environment) based on real-world wind logs obtained from a REpower MM82 2MW turbine. The resulting algorithm decreased the yaw misalignment by 5.5% and 11.2% on two simulations of 2.7 hours each, compared to the conventional active yaw control algorithm. The average net energy gain obtained was 0.31% and 0.33% respectively, compared to the traditional yaw control algorithm. On a single 2MW turbin
    
[^49]: 通过聚合解决无监督领域适应中的参数选择问题

    Addressing Parameter Choice Issues in Unsupervised Domain Adaptation by Aggregation. (arXiv:2305.01281v1 [stat.ML])

    [http://arxiv.org/abs/2305.01281](http://arxiv.org/abs/2305.01281)

    本文提出了一种使用线性聚合的方法来解决无监督领域适应中的参数选择问题，并且展示了该算法的目标误差渐近不劣于未知最佳聚合的两倍，大规模实证研究表明该方法优于深度嵌入验证。

    

    本文研究了在无监督领域适应中选择算法超参数的问题，即在源域中有标记数据，在目标域中有来自不同输入分布的未标记数据。我们采用计算使用不同超参数的几个模型的策略，然后计算模型的线性聚合。虽然存在几个遵循这种策略的启发式方法，但是仍然缺少依赖于限制目标误差的彻底理论的方法。因此，我们提出了一种方法，将加权最小二乘法扩展到向量值函数（例如深度神经网络）。我们展示了所提出算法的目标误差渐近不劣于未知最佳聚合的两倍。我们还进行了大规模实证比较研究，包括文本、图像、脑电图、身体传感器信号和手机信号等多个数据集。我们的方法优于深度嵌入验证。

    We study the problem of choosing algorithm hyper-parameters in unsupervised domain adaptation, i.e., with labeled data in a source domain and unlabeled data in a target domain, drawn from a different input distribution. We follow the strategy to compute several models using different hyper-parameters, and, to subsequently compute a linear aggregation of the models. While several heuristics exist that follow this strategy, methods are still missing that rely on thorough theories for bounding the target error. In this turn, we propose a method that extends weighted least squares to vector-valued functions, e.g., deep neural networks. We show that the target error of the proposed algorithm is asymptotically not worse than twice the error of the unknown optimal aggregation. We also perform a large scale empirical comparative study on several datasets, including text, images, electroencephalogram, body sensor signals and signals from mobile phones. Our method outperforms deep embedded valid
    
[^50]: DABS：基于数据无关性的联邦学习服务器端后门攻击

    DABS: Data-Agnostic Backdoor attack at the Server in Federated Learning. (arXiv:2305.01267v1 [cs.CR])

    [http://arxiv.org/abs/2305.01267](http://arxiv.org/abs/2305.01267)

    本文介绍了一种新的针对联邦学习系统的后门攻击模型：数据无关性的联邦学习服务器端后门攻击，即DABS。DABS通过直接修改全局模型来实现攻击，并在保持干净数据上的正常准确率的同时，实现了更高的攻击成功率。

    

    联邦学习（FL）试图通过在中央服务器的协调下聚合来自分布式设备的局部模型来训练全局模型。然而，大量异构设备的存在使得FL容易受到各种攻击的影响，尤其是隐蔽的后门攻击。现有的作品侧重于客户端攻击，试图通过修改本地数据集来污染全局模型。在本文中，我们提出了一种新的FL攻击模型，即基于数据无关性的联邦学习服务器端后门攻击（DABS），其中服务器直接修改全局模型来实现FL系统的后门攻击。广泛的模拟结果表明，与基准方法相比，这种攻击方案在保持干净数据上的正常准确性的同时，实现了更高的攻击成功率。

    Federated learning (FL) attempts to train a global model by aggregating local models from distributed devices under the coordination of a central server. However, the existence of a large number of heterogeneous devices makes FL vulnerable to various attacks, especially the stealthy backdoor attack. Backdoor attack aims to trick a neural network to misclassify data to a target label by injecting specific triggers while keeping correct predictions on original training data. Existing works focus on client-side attacks which try to poison the global model by modifying the local datasets. In this work, we propose a new attack model for FL, namely Data-Agnostic Backdoor attack at the Server (DABS), where the server directly modifies the global model to backdoor an FL system. Extensive simulation results show that this attack scheme achieves a higher attack success rate compared with baseline methods while maintaining normal accuracy on the clean data.
    
[^51]: HTPS: 面向医疗数据集的异构预测系统

    HTPS: Heterogeneous Transferring Prediction System for Healthcare Datasets. (arXiv:2305.01252v1 [cs.LG])

    [http://arxiv.org/abs/2305.01252](http://arxiv.org/abs/2305.01252)

    本文提出了面向医疗数据集的异构预测系统HTPS，其通过特征工程和自动编码器实现特征嵌入和异构数据的知识转移。实验结果表明，HTPS在多种预测任务和数据集上的表现优于基准系统。

    

    医疗物联网带来医疗服务的革命性改进，也称为智能医疗。利用大量医疗数据，数据挖掘和机器学习可以协助健康管理和智能诊断，并实现P4医学。然而，医疗数据具有高稀疏性和异质性。在本文中，我们提出了一个异构转移预测系统（HTPS）。特征工程机制将数据集转换为稀疏和密集的特征矩阵，嵌入网络中的自动编码器不仅嵌入特征，还从异构数据集中传输知识。实验结果表明，所提出的HTPS在多种预测任务和数据集上优于基准系统，消融研究表明了每个设计机制的有效性。实验结果展示了异构数据对基准系统的负面影响以及所提出的HTPS的高可转移性。

    Medical internet of things leads to revolutionary im- provements in medical services, also known as smart healthcare. With the big healthcare data, data mining and machine learning can assist wellness management and intelligent diagnosis, and achieve the P4-medicine. However, healthcare data has high sparsity and heterogeneity. In this paper, we propose a Heterogeneous Transferring Prediction System (HTPS). Feature engineering mechanism transforms the dataset into sparse and dense feature matrices, and autoencoders in the embedding networks not only embed features but also transfer knowledge from heterogeneous datasets. Experimental results show that the proposed HTPS outperforms the benchmark systems on various prediction tasks and datasets, and ablation studies present the effectiveness of each designed mechanism. Experimental results demonstrate the negative impact of heterogeneous data on benchmark systems and the high transferability of the proposed HTPS.
    
[^52]: MDENet: 多模态双嵌入网络用于恶意软件开放集识别

    MDENet: Multi-modal Dual-embedding Networks for Malware Open-set Recognition. (arXiv:2305.01245v1 [cs.CR])

    [http://arxiv.org/abs/2305.01245](http://arxiv.org/abs/2305.01245)

    提出了多模态双嵌入网络（MDENet）用于恶意软件开放集识别，利用恶意软件图片和句子来增加恶意软件特征的多样性，以提升识别效果。通过引入新的训练策略，方法在两个公开数据集上表现优于现有最先进方法。

    

    恶意软件开放集识别（MOSR）旨在同时对来自已知家族的恶意软件样本进行分类并检测来自新未知家族的样本。现有的方法大多依赖于经过良好训练的分类器，考虑每个已知家族的预测概率并使用基于阈值的检测来实现MOSR。但是，我们的观察结果表明，恶意软件样本的特征分布非常相似，甚至在已知和未知家族之间也是如此。因此，获得的分类器可能会在向已知家族测试未知样本时产生过高的概率，从而降低模型性能。在本文中，我们提出了多模态双嵌入网络，称为MDENet，利用来自不同模态的综合恶意软件特征（即恶意软件图片和恶意软件句子）来增强恶意软件特征空间的多样性，这更具代表性和区分性，以供下游识别使用。最后，为了进一步保证MDENet的稳健性，我们引入了一种新的训练策略，生成更强大的未知样本作为真实未知家族的代理。在两个公开可用的数据集上的实验结果表明，所提出的MDENet方法优于现有最先进的方法。

    Malware open-set recognition (MOSR) aims at jointly classifying malware samples from known families and detect the ones from novel unknown families, respectively. Existing works mostly rely on a well-trained classifier considering the predicted probabilities of each known family with a threshold-based detection to achieve the MOSR. However, our observation reveals that the feature distributions of malware samples are extremely similar to each other even between known and unknown families. Thus the obtained classifier may produce overly high probabilities of testing unknown samples toward known families and degrade the model performance. In this paper, we propose the Multi-modal Dual-Embedding Networks, dubbed MDENet, to take advantage of comprehensive malware features (i.e., malware images and malware sentences) from different modalities to enhance the diversity of malware feature space, which is more representative and discriminative for down-stream recognition. Last, to further guara
    
[^53]: 机器学习可逆粗粒化多尺度分子建模

    Machine-Learned Invertible Coarse Graining for Multiscale Molecular Modeling. (arXiv:2305.01243v1 [physics.comp-ph])

    [http://arxiv.org/abs/2305.01243](http://arxiv.org/abs/2305.01243)

    本文提出了循环粗化（CCG）方法，通过一种统一方法解决了粗粒化模型构建和给定 CG 结构的细节恢复的问题，提供一种新的 CG 方法及无罕见事件的计算自由能的高效方法。

    

    多尺度分子建模被广泛应用于研究分子在大时间和长度尺度下的性质。这篇论文解决多尺度建模中的两个挑战：粗粒化（CG）模型的构建和给定 CG 结构的细节恢复。文章提出了循环粗化（CCG）方法，解决了这两个问题。在 CCG 中，可以通过易处理的优化过程实现重构，从 CG 模拟中恢复细节，进而提供一种新的 CG 方法及无罕见事件的计算自由能的高效方法。

    Multiscale molecular modeling is widely applied in scientific research of molecular properties over large time and length scales. Two specific challenges are commonly present in multiscale modeling, provided that information between the coarse and fine representations of molecules needs to be properly exchanged: One is to construct coarse grained (CG) models by passing information from the fine to coarse levels; the other is to restore finer molecular details given CG configurations. Although these two problems are commonly addressed independently, in this work, we present a theory connecting them, and develop a methodology called Cycle Coarse Graining (CCG) to solve both problems in a unified manner. In CCG, reconstruction can be achieved via a tractable optimization process, leading to a general method to retrieve fine details from CG simulations, which in turn, delivers a new solution to the CG problem, yielding an efficient way to calculate free energies in a rare-event-free manner
    
[^54]: AQ-GT:一种时间对齐并量化的GRU-Transformer，用于共语手势合成

    AQ-GT: a Temporally Aligned and Quantized GRU-Transformer for Co-Speech Gesture Synthesis. (arXiv:2305.01241v1 [cs.HC])

    [http://arxiv.org/abs/2305.01241](http://arxiv.org/abs/2305.01241)

    本文提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练并基于潜在空间的生成方法，从而实现高度逼真、富有表现力、并避免了伪像的手势生成。

    

    在创建多模式人工智能代理时，生成逼真且与上下文相关的共语手势是一项具有挑战性但越来越重要的任务。以往的方法主要集中于学习共语手势表示和实际动作之间的直接对应关系，这种方法通常会在人类评估中产生似乎自然但常常不令人信服的手势。我们提出了一种使用生成对抗网络和量化流水线对部分手势序列进行预训练的方法。所得到的码本向量既是我们框架中的输入，也是输出，构成手势的生成和重构的基础。通过学习潜在空间表示的映射，而不是将其直接映射到矢量表示，该框架促进了高度逼真且富有表现力的手势生成，这些手势与人类的运动和行为非常相似，同时避免了生成过程中的伪像。我们评估了我们的模型在两个数据集上的效果，并发现它比以前的方法产生了更逼真和丰富的手势。

    The generation of realistic and contextually relevant co-speech gestures is a challenging yet increasingly important task in the creation of multimodal artificial agents. Prior methods focused on learning a direct correspondence between co-speech gesture representations and produced motions, which created seemingly natural but often unconvincing gestures during human assessment. We present an approach to pre-train partial gesture sequences using a generative adversarial network with a quantization pipeline. The resulting codebook vectors serve as both input and output in our framework, forming the basis for the generation and reconstruction of gestures. By learning the mapping of a latent space representation as opposed to directly mapping it to a vector representation, this framework facilitates the generation of highly realistic and expressive gestures that closely replicate human movement and behavior, while simultaneously avoiding artifacts in the generation process. We evaluate ou
    
[^55]: 带有流数据的联邦边缘学习的动态调度

    Dynamic Scheduling for Federated Edge Learning with Streaming Data. (arXiv:2305.01238v1 [cs.LG])

    [http://arxiv.org/abs/2305.01238](http://arxiv.org/abs/2305.01238)

    本文提出了一种能够动态调度参与本地训练的设备子集的算法，以最大化其时间平均数据重要性，适用于联邦边缘学习系统，特别是当训练数据具有强时间相关性时

    

    本文考虑了一个联邦边缘学习（FEEL）系统，其中训练数据在具有长期能源约束的分布式边缘设备集上随机生成，由于通信资源和时延要求的限制，每次迭代只有一个设备子集可以参加本地培训过程。我们制定了一个随机网络优化问题，设计了一种动态调度策略，以最大化被调度用户集的时间平均数据重要性，同时受到能源消耗和时延约束的限制。我们提出的基于Lyapunov优化框架的算法在不考虑时变数据重要性的情况下优于其他方法，特别是当训练数据的生成显示出强时间相关性时。

    In this work, we consider a Federated Edge Learning (FEEL) system where training data are randomly generated over time at a set of distributed edge devices with long-term energy constraints. Due to limited communication resources and latency requirements, only a subset of devices is scheduled for participating in the local training process in every iteration. We formulate a stochastic network optimization problem for designing a dynamic scheduling policy that maximizes the time-average data importance from scheduled user sets subject to energy consumption and latency constraints. Our proposed algorithm based on the Lyapunov optimization framework outperforms alternative methods without considering time-varying data importance, especially when the generation of training data shows strong temporal correlation.
    
[^56]: CNS-Net: 用于开放式场景下恶意软件识别的保守性新颖合成网络

    CNS-Net: Conservative Novelty Synthesizing Network for Malware Recognition in an Open-set Scenario. (arXiv:2305.01236v1 [cs.CR])

    [http://arxiv.org/abs/2305.01236](http://arxiv.org/abs/2305.01236)

    CNS-Net是一种用于开放式场景下恶意软件识别的新颖神经网络，能够合成保守性新颖性来准确区分已知和未知的恶意软件家族。

    

    本文研究了恶意软件开放集识别(MOSR)的挑战性任务，即在已知和未知恶意软件家族中进行识别。以前的工作通常假设分类器已知恶意软件家族处于关闭集场景中，即测试家族是训练家族的子集或最多与之相同。然而，新的未知恶意软件家族经常出现在实际应用中，因此需要在开放集场景中对恶意软件实例进行识别，即测试集中也包含一些未知家族。MOSR的一个实用解决方案可能会考虑通过单一分类器(例如，神经网络)从已知家族预测概率分布的方差来同时对已知和未知恶意软件家族进行分类和检测。然而，传统的良好训练的分类器通常倾向于获得过高的识别概率输出，从而难以区分已知和未知的恶意软件家族。为了解决这个问题，我们提出了CNS-Net，一种新的神经网络架构，它合成了保守性新颖性，旨在准确地分类已知和未知的恶意软件家族。

    We study the challenging task of malware recognition on both known and novel unknown malware families, called malware open-set recognition (MOSR). Previous works usually assume the malware families are known to the classifier in a close-set scenario, i.e., testing families are the subset or at most identical to training families. However, novel unknown malware families frequently emerge in real-world applications, and as such, require to recognize malware instances in an open-set scenario, i.e., some unknown families are also included in the test-set, which has been rarely and non-thoroughly investigated in the cyber-security domain. One practical solution for MOSR may consider jointly classifying known and detecting unknown malware families by a single classifier (e.g., neural network) from the variance of the predicted probability distribution on known families. However, conventional well-trained classifiers usually tend to obtain overly high recognition probabilities in the outputs,
    
[^57]: MultiLegalSBD：一个多语言法律句子边界检测数据集

    MultiLegalSBD: A Multilingual Legal Sentence Boundary Detection Dataset. (arXiv:2305.01211v1 [cs.CL])

    [http://arxiv.org/abs/2305.01211](http://arxiv.org/abs/2305.01211)

    本文介绍了一个多语言法律句子边界检测数据集MultiLegalSBD，包括6种语言的130,000个注释句子。在该数据集上，现有的SBD模型的表现不佳。作者训练了基于CRF、BiLSTM-CRF和transformers的单语和多语模型，并展示了在该领域中最先进的性能。他们的多语模型在零-shot测试中优于所有基线。

    

    句子边界检测是自然语言处理的基础之一，不正确的分割会严重影响下游任务的输出质量。对于算法来说是一个具有挑战性的任务，尤其对于法律领域，因为使用的复杂句子结构各不相同。本文精心策划了一个多语种法律数据集，包括6种语言的130,000个注释句子。我们的实验结果表明，现有的SBD模型在多语种法律数据上的性能表现不佳。我们训练和测试了基于CRF、BiLSTM-CRF和transformers的单语和多语模型，展示了最先进的性能。我们还展示了我们的多语模型在葡萄牙语测试集的零-shot设置中优于所有基线。为了鼓励社区进一步的研究和发展，我们已经公开了我们的数据集、模型和代码。

    Sentence Boundary Detection (SBD) is one of the foundational building blocks of Natural Language Processing (NLP), with incorrectly split sentences heavily influencing the output quality of downstream tasks. It is a challenging task for algorithms, especially in the legal domain, considering the complex and different sentence structures used. In this work, we curated a diverse multilingual legal dataset consisting of over 130'000 annotated sentences in 6 languages. Our experimental results indicate that the performance of existing SBD models is subpar on multilingual legal data. We trained and tested monolingual and multilingual models based on CRF, BiLSTM-CRF, and transformers, demonstrating state-of-the-art performance. We also show that our multilingual models outperform all baselines in the zero-shot setting on a Portuguese test set. To encourage further research and development by the community, we have made our dataset, models, and code publicly available.
    
[^58]: ChatGPT生成的代码真的正确吗？对大型语言模型在代码生成方面的严格评估

    Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation. (arXiv:2305.01210v1 [cs.SE])

    [http://arxiv.org/abs/2305.01210](http://arxiv.org/abs/2305.01210)

    本论文提出了一个严格的代码综合基准评估框架EvalPlus，用于评估利用大型语言模型生成的代码的功能正确性。

    

    程序综合一直以来都是被长期研究的领域，最近的方法集中于直接利用大型语言模型(LLMs)根据自然语言中用户的意图生成代码。代码评估数据集，包含策划好的综合问题和各种输入/输出测试用例，被用来衡量各种LLMs在代码综合上的性能。然而，这些数据集中的测试用例在完全评估生成代码的功能正确性方面，数量和质量都可能有所限制。这种现有基准中的限制引出了以下问题：在LLMs时代，生成的代码真的正确吗？为了回答这个问题，我们提出了EvalPlus——一个评估LLM-synthesized代码功能正确性的严格基准评估框架。EvalPlus接受基础评估数据集，并利用自动输入生成步骤，使用LLM-based和基于变异的方法生成和多样化大量新的测试输入。

    Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code according to user intent written in natural language. Code evaluation datasets, containing curated synthesis problems with input/output test-cases, are used to measure the performance of various LLMs on code synthesis. However, test-cases in these datasets can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis benchmarking framework to rigorously evaluate the functional correctness of LLM-synthesized code. In short, EvalPlus takes in the base evaluation dataset and uses an automatic input generation step to produce and diversify large amounts of new test inputs using both LLM-based and mutation-based
    
[^59]: 安全在线学习中未排序项目的探索与重新排序

    Exploration of Unranked Items in Safe Online Learning to Re-Rank. (arXiv:2305.01202v1 [cs.IR])

    [http://arxiv.org/abs/2305.01202](http://arxiv.org/abs/2305.01202)

    本文提出了一种安全的在线学习排序算法，通过将当前排名中的一个项目与排名外的项目高效地交换来执行探索，并基于Kullback-Leibler上置信度界限（KL-UCB）对未排序项目进行乐观选择和安全的排序，以提高长期收益。

    

    在线学习排序问题的贝叶斯算法经常试图利用用户反馈最大化长期收益。然而，从实际角度考虑，这种算法由于过于激进的探索而具有损害用户体验的高风险。因此，近年来，对安全探索的需求不断增加。本文提出了一种安全的在线学习排序算法，它通过将当前排名中的一个项目与排名外的项目（即未排序项目）高效地交换来执行探索并逐步提高原始排名的质量。我们基于Kullback-Leibler上置信度界限（KL-UCB）乐观地选择一个未排名项目进行探索，并安全地对包括所选项目在内的项目进行重新排序。通过实验，我们证明了所提出的算法可以在没有任何安全违规的情况下提高长期悔恨。

    Bandit algorithms for online learning to rank (OLTR) problems often aim to maximize long-term revenue by utilizing user feedback. From a practical point of view, however, such algorithms have a high risk of hurting user experience due to their aggressive exploration. Thus, there has been a rising demand for safe exploration in recent years. One approach to safe exploration is to gradually enhance the quality of an original ranking that is already guaranteed acceptable quality. In this paper, we propose a safe OLTR algorithm that efficiently exchanges one of the items in the current ranking with an item outside the ranking (i.e., an unranked item) to perform exploration. We select an unranked item optimistically to explore based on Kullback-Leibler upper confidence bounds (KL-UCB) and safely re-rank the items including the selected one. Through experiments, we demonstrate that the proposed algorithm improves long-term regret from baselines without any safety violation.
    
[^60]: 中文对话中的话题转移检测：语料库与基准

    Topic Shift Detection in Chinese Dialogues: Corpus and Benchmark. (arXiv:2305.01195v1 [cs.CL])

    [http://arxiv.org/abs/2305.01195](http://arxiv.org/abs/2305.01195)

    本文注释了一个由1308个对话组成的中文自然话题对话语料库，以填补中文自然对话话题语料库的空白。并提出了一种基于分层对比学习的师生框架来预测没有回复的话题转移。

    

    对话话题转移检测是指检测对话中正在进行的话题是否转移或应该转移。任务可分为已知回复任务和未知回复任务。目前，只有少数针对后者进行了研究，因为在没有回复信息的情况下预测话题转移仍然是一项挑战。本文首先注释了一个由1308个对话组成的中文自然话题对话（CNTD）语料库，以填补中文自然会话话题语料库的空白。然后，我们专注于未知回复任务，并提出了一种基于分层对比学习的师生框架来预测没有回复的话题转移。具体而言，在高级师生响应中引入对比学习，用于建立响应和上下文之间的对比学习，而在低级学生中构建标签对比学习。在我们的中文CNTD和英文TIAGE上的实验结果表明了其有效性。

    Dialogue topic shift detection is to detect whether an ongoing topic has shifted or should shift in a dialogue, which can be divided into two categories, i.e., response-known task and response-unknown task. Currently, only a few investigated the latter, because it is still a challenge to predict the topic shift without the response information. In this paper, we first annotate a Chinese Natural Topic Dialogue (CNTD) corpus consisting of 1308 dialogues to fill the gap in the Chinese natural conversation topic corpus. And then we focus on the response-unknown task and propose a teacher-student framework based on hierarchical contrastive learning to predict the topic shift without the response. Specifically, the response at high-level teacher-student is introduced to build the contrastive learning between the response and the context, while the label contrastive learning is constructed at low-level student. The experimental results on our Chinese CNTD and English TIAGE show the effectiven
    
[^61]: 从带噪声数据中学习基于得分的生成式先验来解决反问题

    Solving Inverse Problems with Score-Based Generative Priors learned from Noisy Data. (arXiv:2305.01166v1 [cs.LG])

    [http://arxiv.org/abs/2305.01166](http://arxiv.org/abs/2305.01166)

    本研究提出了一种从带噪声数据中学习基于得分的生成式先验的方法，能够广泛应用于图像重建、图像修补、超分辨率等反问题，实现最先进的结果。

    

    我们提出了SURE-Score：一种利用加性高斯噪声所污染的训练样本来学习得分-based生成模型的方法。当有一个大型干净样本的训练集可用时，最近已经证明，通过在下面的完全采样数据分布上训练得分-based（扩散）生成模型来解决反问题，胜过一步到位的监督式深度学习。在实践中，这样一个庞大的训练数据集可能在一开始获取时是难以负担的。在这项工作中，我们提出了一种方法，从噪声训练数据中大约学习干净分布的基于得分的生成模型。我们制定并证明了一种新颖的损失函数，利用Stein的无偏风险估计来通过除噪得分匹配连续地除去数据和学习得分函数，同时仅使用噪声样本。我们通过学习先验并应用后验采样到图像重建、图像修补和超分辨率等反问题上展示了SURE-Score的普遍性。我们的方法在医学成像、显微镜和摄影等反问题的一系列领域中实现了最先进的结果。

    We present SURE-Score: an approach for learning score-based generative models using training samples corrupted by additive Gaussian noise. When a large training set of clean samples is available, solving inverse problems via score-based (diffusion) generative models trained on the underlying fully-sampled data distribution has recently been shown to outperform end-to-end supervised deep learning. In practice, such a large collection of training data may be prohibitively expensive to acquire in the first place. In this work, we present an approach for approximately learning a score-based generative model of the clean distribution, from noisy training data. We formulate and justify a novel loss function that leverages Stein's unbiased risk estimate to jointly denoise the data and learn the score function via denoising score matching, while using only the noisy samples. We demonstrate the generality of SURE-Score by learning priors and applying posterior sampling to ill-posed inverse prob
    
[^62]: 最大化潜在特征和真实标签之间的互信息实现长尾识别

    Long-Tailed Recognition by Mutual Information Maximization between Latent Features and Ground-Truth Labels. (arXiv:2305.01160v1 [cs.LG])

    [http://arxiv.org/abs/2305.01160](http://arxiv.org/abs/2305.01160)

    本论文提出了一种名为LC的新长尾识别方法，它能够更好地模拟真实标签分布，同时解决类别标签不平衡问题，从而在CIFAR-10，CIFAR-100和ImageNet基准数据集上显着优于现有方法。

    

    尽管对比学习方法在各种表示学习任务中表现出了优越的性能，但当训练数据集是长尾分布时，它们会遇到困难。许多研究人员已经将对比学习和逻辑斯蒂调整技术相结合来解决这个问题，但这些组合是临时的，并没有提供理论背景。本文的目的是提供背景并进一步提高性能。首先，我们证明了对比学习方法在长尾任务中遇到困难的根本原因是它们试图最大化潜在特征和输入数据之间的互信息最大化。由于不考虑真实标签的最大化，它们无法解决类别标签之间的不平衡问题。相反，我们将长尾识别任务解释为潜在特征和真实标签之间的互信息最大化。这种方法以一种有原则的方式集成了对比学习和逻辑斯蒂调整技术。其次，我们提出了一种新方法，称为潜在类别（LC）方法，它明确地模拟了真实标签的分布，并联合最大化潜在特征和真实标签之间的互信息。对包括CIFAR-10，CIFAR-100和ImageNet在内的基准数据集进行的大量实验表明，我们提出的方法在长尾识别任务上显着优于现有方法。

    Although contrastive learning methods have shown prevailing performance on a variety of representation learning tasks, they encounter difficulty when the training dataset is long-tailed. Many researchers have combined contrastive learning and a logit adjustment technique to address this problem, but the combinations are done ad-hoc and a theoretical background has not yet been provided. The goal of this paper is to provide the background and further improve the performance. First, we show that the fundamental reason contrastive learning methods struggle with long-tailed tasks is that they try to maximize the mutual information maximization between latent features and input data. As ground-truth labels are not considered in the maximization, they are not able to address imbalances between class labels. Rather, we interpret the long-tailed recognition task as a mutual information maximization between latent features and ground-truth labels. This approach integrates contrastive learning a
    
[^63]: FedAVO：利用非洲秃鹫优化器提高联邦学习中的通信效率

    FedAVO: Improving Communication Efficiency in Federated Learning with African Vultures Optimizer. (arXiv:2305.01154v1 [cs.LG])

    [http://arxiv.org/abs/2305.01154](http://arxiv.org/abs/2305.01154)

    本文介绍了一种名为FedAVO的算法，利用非洲秃鹫优化器选择最佳超参数来提高联邦学习中的通信效率，显著降低了与FL操作相关的通信成本。

    

    近年来，分布式机器学习技术联邦学习（FL）由于强调用户数据隐私保护而变得越来越受欢迎。然而，FL的分布式计算可能导致通信受限并且学习过程变得拖延，需要对客户-服务器通信成本进行优化。选择的客户比例和本地训练循环次数是对FL性能有重大影响的两个超参数。由于在各种应用程序中存在不同的训练偏好，因此FL从业者很难手动选择这些超参数。在我们的研究论文中，我们介绍了FedAVO，这是一种新的FL算法，通过利用非洲秃鹫优化器（AVO）选择最佳超参数来增强通信效率。我们的研究表明，采用AVO进行FL超参数调整可以大大减少与FL操作相关的通信成本。

    Federated Learning (FL), a distributed machine learning technique has recently experienced tremendous growth in popularity due to its emphasis on user data privacy. However, the distributed computations of FL can result in constrained communication and drawn-out learning processes, necessitating the client-server communication cost optimization. The ratio of chosen clients and the quantity of local training passes are two hyperparameters that have a significant impact on FL performance. Due to different training preferences across various applications, it can be difficult for FL practitioners to manually select such hyperparameters. In our research paper, we introduce FedAVO, a novel FL algorithm that enhances communication effectiveness by selecting the best hyperparameters leveraging the African Vulture Optimizer (AVO). Our research demonstrates that the communication costs associated with FL operations can be substantially reduced by adopting AVO for FL hyperparameter adjustment. Th
    
[^64]: 早期分类多模态数据序列研究

    Early Classifying Multimodal Sequences. (arXiv:2305.01151v1 [cs.LG])

    [http://arxiv.org/abs/2305.01151](http://arxiv.org/abs/2305.01151)

    本项研究针对多模态数据序列扩展了早期分类方法，实验证明其在AUC方面有高达8.7%的优势。

    

    通常，人们会随着时间的推移逐步收集信息。什么时候收集到足够的信息来进行分类？为了在决策的确定性和等待时间之间取得平衡，早期分类问题已成为适应更动态环境的分类方法的关键所在。然而，迄今为止的结果仅限于单一模态序列。在这项初步研究中，我们通过结合现有方法，扩展了早期分类多模态序列的研究。我们的新方法的实验AUC优势高达8.7%。

    Often pieces of information are received sequentially over time. When did one collect enough such pieces to classify? Trading wait time for decision certainty leads to early classification problems that have recently gained attention as a means of adapting classification to more dynamic environments. However, so far results have been limited to unimodal sequences. In this pilot study, we expand into early classifying multimodal sequences by combining existing methods. We show our new method yields experimental AUC advantages of up to 8.7%.
    
[^65]: 基于知识图谱的卷积神经网络在推荐系统中的应用

    Ripple Knowledge Graph Convolutional Networks For Recommendation Systems. (arXiv:2305.01147v1 [cs.IR])

    [http://arxiv.org/abs/2305.01147](http://arxiv.org/abs/2305.01147)

    本文介绍了一种基于知识图谱的深度学习模型RKGCN，它能够动态分析用户的偏好并推荐出合适的物品。该模型在包括电影、书籍和音乐在内的三个真实世界的数据集上比5个基准模型表现更好。

    

    最近已经证明，使用知识图谱来辅助深度学习模型进行推荐决策能有效提高模型的可解释性和准确性。本文介绍了一种端到端的深度学习模型，命名为RKGCN，它动态分析每个用户的偏好，并推荐出合适的物品。它在物品和用户双方面利用知识图谱来丰富它们的表示，最大化知识图谱中丰富的信息的利用。 RKGCN能够在三种不同的场景下提供更个性化和相关的推荐。实验结果表明，在包括电影、书籍和音乐在内的三个真实世界的数据集上，我们的模型比5个基准模型更有效。

    Using knowledge graphs to assist deep learning models in making recommendation decisions has recently been proven to effectively improve the model's interpretability and accuracy. This paper introduces an end-to-end deep learning model, named RKGCN, which dynamically analyses each user's preferences and makes a recommendation of suitable items. It combines knowledge graphs on both the item side and user side to enrich their representations to maximize the utilization of the abundant information in knowledge graphs. RKGCN is able to offer more personalized and relevant recommendations in three different scenarios. The experimental results show the superior effectiveness of our model over 5 baseline models on three real-world datasets including movies, books, and music.
    
[^66]: 深度学习算法的泛化能力理解：基于核化Renyi熵的视角

    Understanding the Generalization Ability of Deep Learning Algorithms: A Kernelized Renyi's Entropy Perspective. (arXiv:2305.01143v1 [stat.ML])

    [http://arxiv.org/abs/2305.01143](http://arxiv.org/abs/2305.01143)

    本论文提出了一种新的信息理论度量方法——基于核化Renyi熵，用于在不假设Lipschitz或凸性条件的前提下对SGD / SGLD等下降学习算法进行分析，旨在提高当前泛化误差界限的优化水平。

    

    最近，信息理论分析已成为理解深度神经网络泛化行为的流行框架。它允许对随机梯度/ Langevin下降（SGD / SGLD）学习算法进行直接分析，而无需诸如Lipschitz或凸性条件等强假设。然而，在这个框架内的当前泛化误差界限仍然远非最优，而对这些界限的实质性改进由于高维信息量的不可计算性而相当具有挑战性。为解决这个问题，我们首先提出了一种新的信息理。论衡量：基于核化Renyi熵，利用希尔伯特空间中的算子表示。它继承了香农熵的属性，可通过简单的随机抽样进行有效计算，同时保持独立于输入维度。然后，我们在核化Renyi熵下建立了SGD / SGLD的泛化误差界限，其中相互信息...

    Recently, information theoretic analysis has become a popular framework for understanding the generalization behavior of deep neural networks. It allows a direct analysis for stochastic gradient/Langevin descent (SGD/SGLD) learning algorithms without strong assumptions such as Lipschitz or convexity conditions. However, the current generalization error bounds within this framework are still far from optimal, while substantial improvements on these bounds are quite challenging due to the intractability of high-dimensional information quantities. To address this issue, we first propose a novel information theoretical measure: kernelized Renyi's entropy, by utilizing operator representation in Hilbert space. It inherits the properties of Shannon's entropy and can be effectively calculated via simple random sampling, while remaining independent of the input dimension. We then establish the generalization error bounds for SGD/SGLD under kernelized Renyi's entropy, where the mutual informati
    
[^67]: 三维分子生成的几何潜在扩散模型

    Geometric Latent Diffusion Models for 3D Molecule Generation. (arXiv:2305.01140v1 [cs.LG])

    [http://arxiv.org/abs/2305.01140](http://arxiv.org/abs/2305.01140)

    提出了三维分子生成的几何潜在扩散模型（GeoLDM），是分子几何领域的第一个潜在扩散模型，通过建立具有不变标量和等变张量的点结构潜在空间来捕捉其关键的旋转平移等变性约束，能够在多个分子生成基准测试中达到更好的性能。

    

    生成模型，特别是扩散模型（DMs），已经在生成富有特征的几何图形和推进分子设计等基础科学问题方面取得了有希望的结果。受到潜在扩散模型稳定性的最新巨大成功的启发，我们提出了一种新的、有原则的三维分子生成方法，称为几何潜在扩散模型（GeoLDM）。GeoLDM是分子几何领域的第一个潜在DM模型，由将结构编码为连续潜在码的自编码器和在潜在空间中操作的DM组成。我们的关键创新在于对于三维分子几何建模，我们通过建立具有不变标量和等变张量的点结构潜在空间来捕捉其关键的旋转平移等变性约束。广泛的实验表明，GeoLDM在多个分子生成基准测试中都能够稳定地实现更好的性能，其中在大型双较差分子的有效百分比方面提高了7％。

    Generative models, especially diffusion models (DMs), have achieved promising results for generating feature-rich geometries and advancing foundational science problems such as molecule design. Inspired by the recent huge success of Stable (latent) Diffusion models, we propose a novel and principled method for 3D molecule generation named Geometric Latent Diffusion Models (GeoLDM). GeoLDM is the first latent DM model for the molecular geometry domain, composed of autoencoders encoding structures into continuous latent codes and DMs operating in the latent space. Our key innovation is that for modeling the 3D molecular geometries, we capture its critical roto-translational equivariance constraints by building a point-structured latent space with both invariant scalars and equivariant tensors. Extensive experiments demonstrate that GeoLDM can consistently achieve better performance on multiple molecule generation benchmarks, with up to 7\% improvement for the valid percentage of large bi
    
[^68]: 分层对抗鲁棒性与拒绝

    Stratified Adversarial Robustness with Rejection. (arXiv:2305.01139v1 [cs.LG])

    [http://arxiv.org/abs/2305.01139](http://arxiv.org/abs/2305.01139)

    本文提出了一种新的防御方法——基于一致预测的拒绝对抗训练（CPR），用于构建鲁棒的选择性分类器。该方法可以在分层拒绝设置下进行对抗鲁棒分类，并且在实验中表现出很好的性能。

    

    最近出现了一种对分类器进行有选择性地训练的对抗性方法——拒绝预测，用于增强对抗鲁棒性。虽然在许多应用中，拒绝预测会带来一定的成本，但现有研究通常将被扰动的输入的拒绝与零成本相关联，这可能会导致拒绝大量可以被正确分类的轻度扰动输入。本文研究了在分层拒绝设置下的对抗鲁棒分类，并且通过拒绝损失函数在扰动幅度上单调不减地建模来模拟拒绝成本。我们从理论上分析了分层拒绝设置，并提出了一种新的防御方法——基于一致预测的拒绝对抗训练（CPR）——用于构建鲁棒的选择性分类器。针对图像数据集进行的实验表明，所提出的方法在强适应性下明显优于现有方法。

    Recently, there is an emerging interest in adversarially training a classifier with a rejection option (also known as a selective classifier) for boosting adversarial robustness. While rejection can incur a cost in many applications, existing studies typically associate zero cost with rejecting perturbed inputs, which can result in the rejection of numerous slightly-perturbed inputs that could be correctly classified. In this work, we study adversarially-robust classification with rejection in the stratified rejection setting, where the rejection cost is modeled by rejection loss functions monotonically non-increasing in the perturbation magnitude. We theoretically analyze the stratified rejection setting and propose a novel defense method -- Adversarial Training with Consistent Prediction-based Rejection (CPR) -- for building a robust selective classifier. Experiments on image datasets demonstrate that the proposed method significantly outperforms existing methods under strong adaptiv
    
[^69]: PGrad: 学习主导梯度以进行领域泛化

    PGrad: Learning Principal Gradients For Domain Generalization. (arXiv:2305.01134v1 [cs.LG])

    [http://arxiv.org/abs/2305.01134](http://arxiv.org/abs/2305.01134)

    这篇论文提出了PGrad方法来学习主导梯度，提高模型在未知领域的泛化能力，可以忽略领域相关的噪声信号，该方法在DomainBed和WILDS基准测试中表现出较好的效果。

    

    机器学习模型在面对超出分布范围（OOD）的领域时表现不佳，这是一项具有挑战性的任务，称为领域泛化（DG）。在这项工作中，我们开发了一种新的DG训练策略，称为PGrad，以学习一个强健的梯度方向，提高模型在未知领域的泛化能力。所提出的梯度聚合了一个采样的roll-out优化轨迹的主导方向，该轨迹测量了在所有训练领域中的训练动态。PGrad梯度设计强制DG训练忽略与领域相关的噪声信号，并用覆盖参数动态主要组成部分的强健方向更新所有训练领域。我们通过双射基于计算的精炼和基于方向加长度的校准进一步改进了PGrad。我们的理论证明将PGrad与训练神经网络中Hessian的谱分析相连接。DomainBed和WILDS基准测试的实验证明，我们的方法有效地实现了强健的DG。

    Machine learning models fail to perform when facing out-of-distribution (OOD) domains, a challenging task known as domain generalization (DG). In this work, we develop a novel DG training strategy, we call PGrad, to learn a robust gradient direction, improving models' generalization ability on unseen domains. The proposed gradient aggregates the principal directions of a sampled roll-out optimization trajectory that measures the training dynamics across all training domains. PGrad's gradient design forces the DG training to ignore domain-dependent noise signals and updates all training domains with a robust direction covering main components of parameter dynamics. We further improve PGrad via bijection-based computational refinement and directional plus length-based calibrations. Our theoretical proof connects PGrad to the spectral analysis of Hessian in training neural networks. Experiments on DomainBed and WILDS benchmarks demonstrate that our approach effectively enables robust DG o
    
[^70]: 不同时间图神经网络配置在动态图上的分析。

    Analysis of different temporal graph neural network configurations on dynamic graphs. (arXiv:2305.01128v1 [cs.LG])

    [http://arxiv.org/abs/2305.01128](http://arxiv.org/abs/2305.01128)

    本文针对动态图上的时空依赖结构进行了分析，并比较了不同TGN模型在预测任务上的有效性。通过广泛的消融实验，探讨了不同设计选择对预测准确性的影响。

    

    近年来，人们对使用图神经网络（GNN）分析动态图（随时间演变的图）的兴趣越来越大。然而，不同的时间图神经网络（TGN）配置如何影响动态图上的预测准确性仍存在不足之处。此外，对于这些TGN模型的基准数据集的搜索仍在进行中。近期，Pytorch Geometric Temporal提供了一些基准数据集，但这些数据集大多数还没有用不同的TGN模型进行分析以建立最先进的状况。因此，本项目旨在通过对动态图上的时空依赖结构学习进行定性分析，并比较选定TGN模型在节点和边预测任务上的有效性，以及对最佳表现TGN模型的不同变体进行广泛的消融实验，以探索不同设计选择对预测准确性的影响。

    In recent years, there has been an increasing interest in the use of graph neural networks (GNNs) for analyzing dynamic graphs, which are graphs that evolve over time. However, there is still a lack of understanding of how different temporal graph neural network (TGNs) configurations can impact the accuracy of predictions on dynamic graphs. Moreover, the hunt for benchmark datasets for these TGNs models is still ongoing. Up until recently, Pytorch Geometric Temporal came up with a few benchmark datasets but most of these datasets have not been analyzed with different TGN models to establish the state-of-the-art. Therefore, this project aims to address this gap in the literature by performing a qualitative analysis of spatial-temporal dependence structure learning on dynamic graphs, as well as a comparative study of the effectiveness of selected TGNs on node and edge prediction tasks. Additionally, an extensive ablation study will be conducted on different variants of the best-performin
    
[^71]: 学习可控自适应多分辨率物理模拟

    Learning Controllable Adaptive Simulation for Multi-resolution Physics. (arXiv:2305.01122v1 [cs.LG])

    [http://arxiv.org/abs/2305.01122](http://arxiv.org/abs/2305.01122)

    一种基于深度学习和图神经网络的可控自适应多分辨率物理模拟模型，通过优化适当的空间分辨率，将更多的计算资源分配给高度动态区域，降低计算成本并提高准确性。

    

    在许多科学和工程问题中，模拟物理系统的时间演化至关重要。模拟这些系统的一个难题是它们的多分辨率动态特性：系统的一小部分非常动态，需要非常细粒度的分辨率，而大多数系统变化缓慢，可以用较粗的空间尺度来建模。典型的基于学习的替代模型使用统一的空间尺度，需要解析到最细的尺度，并且可能浪费大量计算资源来达到所需的精度。在这项工作中，我们引入了一种名为学习可控自适应多分辨率物理模拟（LAMP）的模型，它是第一个完全基于深度学习的替代模型，同时学习演化模型并优化适当的空间分辨率，将更多的计算资源分配给高度动态区域。LAMP由用于学习正向演化的图神经网络（GNN）和基于GNN的演员-评论家用于学习空间分辨率控制策略两部分组成。我们在三个不同的物理系统上展示了LAMP的有效性，它优于统一分辨率替代模型同时减少计算成本。

    Simulating the time evolution of physical systems is pivotal in many scientific and engineering problems. An open challenge in simulating such systems is their multi-resolution dynamics: a small fraction of the system is extremely dynamic, and requires very fine-grained resolution, while a majority of the system is changing slowly and can be modeled by coarser spatial scales. Typical learning-based surrogate models use a uniform spatial scale, which needs to resolve to the finest required scale and can waste a huge compute to achieve required accuracy. In this work, we introduce Learning controllable Adaptive simulation for Multi-resolution Physics (LAMP) as the first full deep learning-based surrogate model that jointly learns the evolution model and optimizes appropriate spatial resolutions that devote more compute to the highly dynamic regions. LAMP consists of a Graph Neural Network (GNN) for learning the forward evolution, and a GNN-based actor-critic for learning the policy of sp
    
[^72]: CSP：针对地理空间视觉表示的自监督对比空间预训练

    CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations. (arXiv:2305.01118v1 [cs.CV])

    [http://arxiv.org/abs/2305.01118](http://arxiv.org/abs/2305.01118)

    CSP提出了一个自监督对比学习框架，通过利用地理信息，学习地理位置的有效表示，进一步提高了模型在大规模地理标记图像上的表现。

    

    大量的地理标记图像公开可用，而对象类别等标签则相对稀缺且收集成本高昂。同时，对比学习在各种自然图像和语言任务中取得了巨大成功，仅需很少的带标签数据。然而，现有的方法未能充分利用地理空间信息，这可能是区分视觉上相似的对象的关键。为了在预训练、微调和推理阶段直接利用与图像相关的丰富地理空间信息，我们提出了针对地理标记图像的自监督学习框架 Contrastive Spatial Pre-Training（CSP）。我们使用双编码器分别对图像及其对应的地理位置进行编码，利用对比目标从图像中学习有效的位置表示，这些表示可以转移到下游监督任务，例如图像分类。实验证明，CSP可以提高模型在大规模地理标记图像上的性能。

    Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged im- ages. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on b
    
[^73]: 行人意向预测的局部和全局上下文特征融合

    Local and Global Contextual Features Fusion for Pedestrian Intention Prediction. (arXiv:2305.01111v1 [cs.CV])

    [http://arxiv.org/abs/2305.01111](http://arxiv.org/abs/2305.01111)

    本研究提出了基于局部和全局上下文特征融合的行人意向预测方法，通过分析行人和交通上下文的视觉特征来提高自动驾驶汽车在道路上的安全性。

    

    自动驾驶汽车正在成为未来交通不可或缺的一部分。但是，安全挑战和缺乏可靠性限制了它们在实际环境中的部署。与行人的交互包括“预测行人的穿越意向”值得广泛研究，以提高自动驾驶汽车在道路上的可靠性。本文通过提取和分析行人和交通上下文的时空视觉特征进行有效的意向预测学习。

    Autonomous vehicles (AVs) are becoming an indispensable part of future transportation. However, safety challenges and lack of reliability limit their real-world deployment. Towards boosting the appearance of AVs on the roads, the interaction of AVs with pedestrians including "prediction of the pedestrian crossing intention" deserves extensive research. This is a highly challenging task as involves multiple non-linear parameters. In this direction, we extract and analyse spatio-temporal visual features of both pedestrian and traffic contexts. The pedestrian features include body pose and local context features that represent the pedestrian's behaviour. Additionally, to understand the global context, we utilise location, motion, and environmental information using scene parsing technology that represents the pedestrian's surroundings, and may affect the pedestrian's intention. Finally, these multi-modality features are intelligently fused for effective intention prediction learning. The 
    
[^74]: 利用语言表示进行材料推荐、排名和探索

    Leveraging Language Representation for Material Recommendation, Ranking, and Exploration. (arXiv:2305.01101v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2305.01101](http://arxiv.org/abs/2305.01101)

    本文提出了一种新型材料发现框架，利用材料科学特定语言模型的自然语言嵌入作为材料的组成和结构特征进行表示，并且联合采用了表示相似性召回候选材料和基于多任务学习对候选材料进行目标属性排名的方案。通过这种方法，可以更好地探索广阔的材料搜索空间，并确定高性能候选材料。

    

    利用机器学习的新兴技术，数据驱动的材料发现和设计已经得到了加速。虽然在学习材料结构与性质关系方面取得了巨大进展，但能够有效探索广阔的材料搜索空间并确定高性能候选材料的方法仍然十分有限。本文介绍了一种材料发现框架，它利用从材料科学特定语言模型中得出的自然语言嵌入作为组成和结构特征的表示。该发现框架由一个联合方案组成，给定一个查询材料，首先基于表示相似性召回候选材料，再通过多任务学习对候选材料进行目标属性排名。语言表示中编码的上下文知识被发现可以传达有关材料性质和结构的信息，使得相似性分析和高性能材料发现变得更加可行。

    Data-driven approaches for material discovery and design have been accelerated by emerging efforts in machine learning. While there is enormous progress towards learning the structure to property relationship of materials, methods that allow for general representations of crystals to effectively explore the vast material search space and identify high-performance candidates remain limited. In this work, we introduce a material discovery framework that uses natural language embeddings derived from material science-specific language models as representations of compositional and structural features. The discovery framework consists of a joint scheme that, given a query material, first recalls candidates based on representational similarity, and ranks the candidates based on target properties through multi-task learning. The contextual knowledge encoded in language representations is found to convey information about material properties and structures, enabling both similarity analysis fo
    
[^75]: Logion：基于机器学习的希腊语语言学

    Logion: Machine Learning for Greek Philology. (arXiv:2305.01099v1 [cs.CL])

    [http://arxiv.org/abs/2305.01099](http://arxiv.org/abs/2305.01099)

    该研究提出了基于机器学习的方法来解决希腊语语言学中的问题，成功利用BERT模型发现和纠正了抄写员在文本传递过程中未被发现的错误，并能在修复预现代手稿材料老化引起的信息缺失方面发挥作用。同时，在领域专家与模型合作时，最佳性能可以通过启示性建议实现。模型的注意力头似乎编码了预现代希腊语的选择性语法特征。

    

    本篇论文提出了一种基于机器学习的方法来解决希腊语语言学中的各种问题。首先，我们在迄今为止用于此目的的最大预现代希腊语数据集上训练了BERT模型，发现并纠正了抄写员在文本传递过程中未被发现的错误。此外，我们展示了该模型在填补由于预现代手稿材料老化导致的缺口方面的能力，并将其性能与领域专家的表现进行了比较。我们发现，当领域专家得到模型的启示性建议时，才能达到最佳性能。考虑到这种人与计算机的合作，我们探讨了模型的可解释性，并发现一些注意力头似乎编码了预现代希腊语的选择性语法特征。

    This paper presents machine-learning methods to address various problems in Greek philology. After training a BERT model on the largest premodern Greek dataset used for this purpose to date, we identify and correct previously undetected errors made by scribes in the process of textual transmission, in what is, to our knowledge, the first successful identification of such errors via machine learning. Additionally, we demonstrate the model's capacity to fill gaps caused by material deterioration of premodern manuscripts and compare the model's performance to that of a domain expert. We find that best performance is achieved when the domain expert is provided with model suggestions for inspiration. With such human-computer collaborations in mind, we explore the model's interpretability and find that certain attention heads appear to encode select grammatical features of premodern Greek.
    
[^76]: 合作自适应巡航控制系统中的驾驶员变道预测新模型

    A Novel Model for Driver Lane Change Prediction in Cooperative Adaptive Cruise Control Systems. (arXiv:2305.01096v1 [cs.RO])

    [http://arxiv.org/abs/2305.01096](http://arxiv.org/abs/2305.01096)

    本文研究了合作自适应巡航控制系统中驾驶员变道预测问题。使用车到车通信技术，提供了多个车辆的加速度信息，通过LSTM模型的学习，当周围车辆数量增加时，准确性有所提高。

    

    准确的变道预测可以减少潜在事故，提高道路安全性。自适应巡航控制（ACC），车道偏离警告（LDA）和车道保持辅助（LKA）是一些高级驾驶员辅助系统（ADAS）的传统模块。有了车到车通信（V2V），车辆可以与周围车辆共享交通信息，实现合作自适应巡航控制（CACC）。本文比较了驾驶员变道预测所需的信息类型（位置、速度、加速度）和周围车辆数量。我们使用了HighD数据集，训练了一个LSTM（长短期记忆）模型来预测变道意图。结果表明，随着周围车辆数量的增加，准确性有了显著提高。

    Accurate lane change prediction can reduce potential accidents and contribute to higher road safety. Adaptive cruise control (ACC), lane departure avoidance (LDA), and lane keeping assistance (LKA) are some conventional modules in advanced driver assistance systems (ADAS). Thanks to vehicle-to-vehicle communication (V2V), vehicles can share traffic information with surrounding vehicles, enabling cooperative adaptive cruise control (CACC). While ACC relies on the vehicle's sensors to obtain the position and velocity of the leading vehicle, CACC also has access to the acceleration of multiple vehicles through V2V communication. This paper compares the type of information (position, velocity, acceleration) and the number of surrounding vehicles for driver lane change prediction. We trained an LSTM (Long Short-Term Memory) on the HighD dataset to predict lane change intention. Results indicate a significant improvement in accuracy with an increase in the number of surrounding vehicles and 
    
[^77]: 一种基于LSTM的自适应巡航控制器，在车道变换时可以预测先前车辆的行为

    LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application. (arXiv:2305.01095v1 [cs.RO])

    [http://arxiv.org/abs/2305.01095](http://arxiv.org/abs/2305.01095)

    该论文提出了一种基于LSTM的ACC系统，可以学习过去的驾驶经验，适应和预测新情况，并且在模拟驾驶环境中表现良好。

    

    自适应巡航控制（ACC）系统的发展旨在通过自动调节车速来确保与前车安全距离，从而增强车辆的安全性和舒适性。然而，传统的ACC系统无法适应不断变化的驾驶条件和驾驶者的行为。为了解决这个问题，我们提出了一种基于LSTM的ACC系统，可以从以往的驾驶经验中学习，并实时地适应和预测新的情况。该模型是基于实际的高速公路高D数据集构建的，该数据集是利用装备有摄像头的无人机在德国高速公路上获取的。我们在侧面车道前车剪切并强制目标驾驶员减速的激进车道变化时评估了ACC系统。为此，将所提出的系统在模拟驾驶环境中进行了评估，并与馈送前人工神经网络（ANN）模型和模型预测控制（MPC）模型进行了比较。

    The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The 
    
[^78]: 通过重新参数化学习实现在线反馈的实现式预测

    Performative Prediction with Bandit Feedback: Learning through Reparameterization. (arXiv:2305.01094v1 [cs.LG])

    [http://arxiv.org/abs/2305.01094](http://arxiv.org/abs/2305.01094)

    本文提出一种新的在线反馈的实现式预测框架，解决了在模型部署自身改变数据分布的情况下优化准确性的问题。

    

    本文提出了在数据分布由模型部署自身改变的情形下预测的一个框架——实现式预测。现有研究的重点在于优化准确性，但是其假设往往难以在实践中得到满足。本文针对这类问题，提出了一种两层零阶优化算法，通过重新参数化实现式预测目标，从而将非凸的目标转化为凸的目标。

    Performative prediction, as introduced by Perdomo et al. (2020), is a framework for studying social prediction in which the data distribution itself changes in response to the deployment of a model. Existing work on optimizing accuracy in this setting hinges on two assumptions that are easily violated in practice: that the performative risk is convex over the deployed model, and that the mapping from the model to the data distribution is known to the model designer in advance. In this paper, we initiate the study of tractable performative prediction problems that do not require these assumptions. To tackle this more challenging setting, we develop a two-level zeroth-order optimization algorithm, where one level aims to compute the distribution map, and the other level reparameterizes the performative prediction objective as a function of the induced data distribution. Under mild conditions, this reparameterization allows us to transform the non-convex objective into a convex one and ac
    
[^79]: 自动编码器用于发现复杂动力系统数据中的流形维度和坐标

    Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems. (arXiv:2305.01090v1 [cs.LG])

    [http://arxiv.org/abs/2305.01090](http://arxiv.org/abs/2305.01090)

    本文提出了一种自动编码器框架，结合implicit regularization，内部线性层和L2正则化（权重衰减）自动估计数据集的潜在维度，产生正交的流形坐标系，并提供环境空间和流形空间之间的映射函数，从而允许进行样本之外的投影。该方法在动力系统数据集中表现出了较好的低秩表示效果，为底层动态提供了物理洞见，并可以用于提高机器学习模型和控制策略的效果。

    

    尽管物理和工程学中的许多现象在形式上是高维的，但它们的长时间动态往往生活在较低维的流形上。本文介绍了一种自动编码器框架，将隐式正则化与内部线性层和$L_2$正则化（权重衰减）相结合，自动估计数据集的潜在维度、产生正交的流形坐标系，并提供环境空间和流形空间之间的映射函数，从而允许进行样本之外的投影。我们验证了我们的框架估计流形维度的能力，针对多种复杂度的动力系统数据集进行比较，并与其他最先进的估计器进行对比。我们分析网络的训练动态，以了解低秩学习的机制，并发现每个隐式正则化层共同构成了低秩表示，甚至在训练过程中进行了自我纠正。学习的坐标系的分析可以为底层动态提供物理洞见，也可以用于提高机器学习模型和控制策略的效果。

    While many phenomena in physics and engineering are formally high-dimensional, their long-time dynamics often live on a lower-dimensional manifold. The present work introduces an autoencoder framework that combines implicit regularization with internal linear layers and $L_2$ regularization (weight decay) to automatically estimate the underlying dimensionality of a data set, produce an orthogonal manifold coordinate system, and provide the mapping functions between the ambient space and manifold space, allowing for out-of-sample projections. We validate our framework's ability to estimate the manifold dimension for a series of datasets from dynamical systems of varying complexities and compare to other state-of-the-art estimators. We analyze the training dynamics of the network to glean insight into the mechanism of low-rank learning and find that collectively each of the implicit regularizing layers compound the low-rank representation and even self-correct during training. Analysis o
    
[^80]: 计算可交换图生成模型的期望模体计数

    Computing Expected Motif Counts for Exchangeable Graph Generative Models. (arXiv:2305.01089v1 [cs.LG])

    [http://arxiv.org/abs/2305.01089](http://arxiv.org/abs/2305.01089)

    本文提出了一种可扩展的估计过程，用于生成混合模型中期望的模体计数。

    

    估计图形统计量的期望值是使用和学习图模型的重要推断任务。本文介绍了一种可扩展的估计过程，用于期望模体计数，这是一种广泛使用的图统计量类型。该程序适用于生成混合模型，这种模型常用于神经网络和贝叶斯方法中的图数据。

    Estimating the expected value of a graph statistic is an important inference task for using and learning graph models. This note presents a scalable estimation procedure for expected motif counts, a widely used type of graph statistic. The procedure applies for generative mixture models of the type used in neural and Bayesian approaches to graph data.
    
[^81]: 上下文多语种用户查询拼写检查器

    Contextual Multilingual Spellchecker for User Queries. (arXiv:2305.01082v1 [cs.CL])

    [http://arxiv.org/abs/2305.01082](http://arxiv.org/abs/2305.01082)

    本文提出了一个上下文多语种用户查询拼写检查器，它非常快速、可扩展，并根据特定产品的需求调整其词汇表和拼写输出，以满足用户的需求。

    

    拼写检查是最基本和广泛使用的搜索功能之一。纠正拼写错误的用户查询不仅增强了用户体验，而且用户也期望能够实现。然而，大多数广泛可用的拼写检查解决方案要么比最新的解决方案精度低，要么速度太慢，无法用于延迟是关键要求的搜索用例。此外，大多数最新的创新架构集中在英语上，并且没有以多语言方式进行培训，并且是针对较长文本的拼写纠正进行培训，这是与对用户查询的拼写纠正不同的范式，其中上下文很少(大多数查询只有1-2个单词)。最后，由于大多数企业有独特的词汇，例如产品名称，现成的拼写解决方案无法满足用户的需求。在这项工作中，我们构建了一个多语言拼写检查器，它非常快速和可扩展，并根据特定产品的需求调整其词汇表和拼写输出。

    Spellchecking is one of the most fundamental and widely used search features. Correcting incorrectly spelled user queries not only enhances the user experience but is expected by the user. However, most widely available spellchecking solutions are either lower accuracy than state-of-the-art solutions or too slow to be used for search use cases where latency is a key requirement. Furthermore, most innovative recent architectures focus on English and are not trained in a multilingual fashion and are trained for spell correction in longer text, which is a different paradigm from spell correction for user queries, where context is sparse (most queries are 1-2 words long). Finally, since most enterprises have unique vocabularies such as product names, off-the-shelf spelling solutions fall short of users' needs. In this work, we build a multilingual spellchecker that is extremely fast and scalable and that adapts its vocabulary and hence speller output based on a specific product's needs. Fu
    
[^82]: 混合分布下的个性化联邦学习

    Personalized Federated Learning under Mixture of Distributions. (arXiv:2305.01068v1 [cs.LG])

    [http://arxiv.org/abs/2305.01068](http://arxiv.org/abs/2305.01068)

    该论文提出了一种新方法FedGMM，利用高斯混合模型处理了协变量漂移问题，提高了个性化联邦学习的性能。

    

    近年来，针对每个客户训练定制模型并保持数据隐私的个性化联邦学习(PFL)趋势引起了重视。然而，当前的PFL技术主要关注条件分布异质性(即概念漂移)，当客户之间输入数据分布发散(即协变量漂移)时，可能导致性能不佳。此外，这些技术通常缺乏适应未见数据的能力，进一步限制了它们在现实场景中的有效性。为了解决这些局限性，我们提出了一种新方法FedGMM，它利用高斯混合模型(GMM)有效地适应多样化客户端的输入数据分布。模型参数通过最大似然估计，利用联邦期望最大化算法解决，该算法在闭合形式下解决且不假设渐变相似性。此外，FedGMM同时模型化条件和协变量分布异质性，提高了个性化联邦学习的性能。

    The recent trend towards Personalized Federated Learning (PFL) has garnered significant attention as it allows for the training of models that are tailored to each client while maintaining data privacy. However, current PFL techniques primarily focus on modeling the conditional distribution heterogeneity (i.e. concept shift), which can result in suboptimal performance when the distribution of input data across clients diverges (i.e. covariate shift). Additionally, these techniques often lack the ability to adapt to unseen data, further limiting their effectiveness in real-world scenarios. To address these limitations, we propose a novel approach, FedGMM, which utilizes Gaussian mixture models (GMM) to effectively fit the input data distributions across diverse clients. The model parameters are estimated by maximum likelihood estimation utilizing a federated Expectation-Maximization algorithm, which is solved in closed form and does not assume gradient similarity. Furthermore, FedGMM po
    
[^83]: 专业知识树在集体决策中解决知识局限性问题

    Expertise Trees Resolve Knowledge Limitations in Collective Decision-Making. (arXiv:2305.01063v1 [cs.AI])

    [http://arxiv.org/abs/2305.01063](http://arxiv.org/abs/2305.01063)

    本研究通过引入专业知识树算法，解决了集体决策中专业知识水平不同的问题，并在多个问题上进行了验证。

    

    向决策者提供建议的专家往往会显示出随问题实例变化而变化的专业知识水平。在实践中，这可能导致针对少数情况的次优或歧视性决策。在本文中，我们将这种知识深度和广度的变化建模为将问题空间划分为不同专业知识区域。我们提供了一些新算法，它们明确考虑并适应问题实例与专家知识之间的关系。我们首先提出并强调了一种基于最近邻查询的天真方法的缺点。为了解决这些问题，我们引入了一种新的算法——专业知识树，它构建决策树，使学习者能够选择适当的模型。我们提供了理论见解，并在一系列现有方法被证明不足以解决问题的问题上进行了经验证实了我们的新方法的改进性能。

    Experts advising decision-makers are likely to display expertise which varies as a function of the problem instance. In practice, this may lead to sub-optimal or discriminatory decisions against minority cases. In this work we model such changes in depth and breadth of knowledge as a partitioning of the problem space into regions of differing expertise. We provide here new algorithms that explicitly consider and adapt to the relationship between problem instances and experts' knowledge. We first propose and highlight the drawbacks of a naive approach based on nearest neighbor queries. To address these drawbacks we then introduce a novel algorithm - expertise trees - that constructs decision trees enabling the learner to select appropriate models. We provide theoretical insights and empirically validate the improved performance of our novel approach on a range of problems for which existing methods proved to be inadequate.
    
[^84]: 基于语义神经模型的草图人脸识别方法

    semantic neural model approach for face recognition from sketch. (arXiv:2305.01058v1 [cs.CV])

    [http://arxiv.org/abs/2305.01058](http://arxiv.org/abs/2305.01058)

    本文提出了一种基于语义神经网络模型的方法，可同时解决草图合成和识别问题。对待识别的人脸进行正面姿态、正常光照、中性表情和无遮挡的要求。

    

    在法律执法中，人脸草图合成和识别有着广泛的应用，但当前的研究往往将其视为独立的任务。为了同时解决草图合成和识别问题，本文提出了一种基于语义神经网络模型的方法。我们假设待识别的人脸处于正面姿态，有着正常的光照和中性的表情，并且没有遮挡。为了合成草图/图像，将人脸区域划分为重叠的补丁进行学习，补丁的大小决定要学习的局部人脸结构的尺度。

    Face sketch synthesis and reputation have wide range of packages in law enforcement. Despite the amazing progresses had been made in faces cartoon and reputation, maximum current researches regard them as separate responsibilities. On this paper, we propose a semantic neural version approach so that you can address face caricature synthesis and recognition concurrently. We anticipate that faces to be studied are in a frontal pose, with regular lighting and neutral expression, and have no occlusions. To synthesize caricature/image photos, the face vicinity is divided into overlapping patches for gaining knowledge of. The size of the patches decides the scale of local face systems to be found out.
    
[^85]: LooPy: 一种针对电子舞曲的研究友好型混合框架的音乐信息检索

    LooPy: A Research-Friendly Mix Framework for Music Information Retrieval on Electronic Dance Music. (arXiv:2305.01051v1 [cs.SD])

    [http://arxiv.org/abs/2305.01051](http://arxiv.org/abs/2305.01051)

    LooPy是一种Python软件包，为MIR提供了一种面向电子舞曲的基础设施，可用于自动生成EDM音频，并提供了框架来构建专业级的模板，使用户可以从指定的旋律和和弦中呈现出制作精良的歌曲轨道或仅通过使用符号性的旋律生成器，提供具有多种风格的音轨。

    

    近年来，随着深度学习技术的发展，音乐信息检索(MIR)得到了爆炸性的发展。然而，诸如电子舞曲等音乐类型相对于其他类型一直较少研究。考虑到其广泛的应用，我们提出了一种Python软件包，用于自动化EDM音频生成，并作为EDM歌曲MIR的基础设施，以缓解获取标注数据的难度。它是一个方便的工具，可以轻松连接到许多符号音乐生成管道的末端。在这个软件包中，我们提供了一个框架来构建专业水平的模板，可以从指定的旋律和和弦中呈现出一个制作精良的轨道，或者仅通过我们的概率符号旋律生成器，提供具有各种风格的音轨。实验证明，我们的混音在主观和客观评估方面与由世界著名艺术家制作的原始参考歌曲具有相同的质量。

    Music information retrieval (MIR) has gone through an explosive development with the advancement of deep learning in recent years. However, music genres like electronic dance music (EDM) has always been relatively less investigated compared to others. Considering its wide range of applications, we present a Python package for automated EDM audio generation as an infrastructure for MIR for EDM songs, to mitigate the difficulty of acquiring labelled data. It is a convenient tool that could be easily concatenated to the end of many symbolic music generation pipelines. Inside this package, we provide a framework to build professional-level templates that could render a well-produced track from specified melody and chords, or produce massive tracks given only a specific key by our probabilistic symbolic melody generator. Experiments show that our mixes could achieve the same quality of the original reference songs produced by world-famous artists, with respect to both subjective and objecti
    
[^86]: SemEval-2023 任务11中的SafeWebUH：学习侮辱性文本的注释者不一致性： 直接训练与聚合的比较。

    SafeWebUH at SemEval-2023 Task 11: Learning Annotator Disagreement in Derogatory Text: Comparison of Direct Training vs Aggregation. (arXiv:2305.01050v1 [cs.CL])

    [http://arxiv.org/abs/2305.01050](http://arxiv.org/abs/2305.01050)

    本文研究使用BERT模型来标注侮辱性文本中的注释者不一致性，并比较直接训练和聚合两种方法，结果发现聚合方法比直接训练有更好的效果。

    

    主观性和不同意见是关键的社会现象，考虑到这一点在注释和检测侮辱性文本内容的过程中至关重要。本文使用SemEval-2023任务11提供的四个数据集，对BERT模型进行微调，以捕捉注释中的不一致性。我们发现个体注释者建模和聚合将交叉熵得分平均降低了0.21，而与直接训练软标签相比。我们的研究进一步证明了注释者元数据对平均交叉熵分数的0.029降低有所贡献。

    Subjectivity and difference of opinion are key social phenomena, and it is crucial to take these into account in the annotation and detection process of derogatory textual content. In this paper, we use four datasets provided by SemEval-2023 Task 11 and fine-tune a BERT model to capture the disagreement in the annotation. We find individual annotator modeling and aggregation lowers the Cross-Entropy score by an average of 0.21, compared to the direct training on the soft labels. Our findings further demonstrate that annotator metadata contributes to the average 0.029 reduction in the Cross-Entropy score.
    
[^87]: 无特定模型泛化难度度量

    Model-agnostic Measure of Generalization Difficulty. (arXiv:2305.01034v1 [cs.LG])

    [http://arxiv.org/abs/2305.01034](http://arxiv.org/abs/2305.01034)

    该论文提出了第一个无特定模型的、量化机器学习测试泛化难度的方法——归纳偏差复杂度度量。该方法量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差，通常需要在许多维度上泛化的任务比涉及更少维度但要求更多细节的任务要困难得多。

    

    机器学习算法的度量是其可以执行的任务难度，足够困难的任务是强大机器学习模型的关键驱动因素。然而，量化机器学习测试的泛化难度一直是具有挑战性的。我们提出了据我们所知的第一个对任务固有泛化难度的无特定模型的度量。我们的归纳偏差复杂度度量量化了在任务上良好泛化所需的总信息量与数据提供的信息量之差。通过测量适合训练数据的假设在任务中泛化的分数占据的容积，来实现这一点。它与模型必须泛化的空间的内在维数成指数比例，但仅在每个维度的分辨率上呈多项式比例，表明需要在许多维度上泛化的任务比涉及更少维度的更多细节的任务要困难得多。

    The measure of a machine learning algorithm is the difficulty of the tasks it can perform, and sufficiently difficult tasks are critical drivers of strong machine learning models. However, quantifying the generalization difficulty of machine learning benchmarks has remained challenging. We propose what is to our knowledge the first model-agnostic measure of the inherent generalization difficulty of tasks. Our inductive bias complexity measure quantifies the total information required to generalize well on a task minus the information provided by the data. It does so by measuring the fractional volume occupied by hypotheses that generalize on a task given that they fit the training data. It scales exponentially with the intrinsic dimensionality of the space over which the model must generalize but only polynomially in resolution per dimension, showing that tasks which require generalizing over many dimensions are drastically more difficult than tasks involving more detail in fewer dimen
    
[^88]: 零样本学习在公司分类中的应用

    Company classification using zero-shot learning. (arXiv:2305.01028v1 [cs.CL])

    [http://arxiv.org/abs/2305.01028](http://arxiv.org/abs/2305.01028)

    本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。该方法可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。

    

    近年来，自然语言处理在许多商业应用中变得越来越重要，包括情感分析、文本分类和命名实体识别。本文提出了一种利用自然语言处理和零样本学习的方法来进行公司分类的方法。我们的方法利用预训练的Transformer模型从公司描述中提取特征，然后应用零样本学习将公司分类到相关类别，无需为每个类别提供特定的训练数据。我们在公开可用的公司文本描述数据集上评估我们的方法，并证明它可以简化公司分类过程，从而减少传统方法如全球产业分类标准（GICS）所需的时间和资源。结果表明，该方法具有自动化公司分类的潜力，是未来研究的一个有前途的方向。

    In recent years, natural language processing (NLP) has become increasingly important in a variety of business applications, including sentiment analysis, text classification, and named entity recognition. In this paper, we propose an approach for company classification using NLP and zero-shot learning. Our method utilizes pre-trained transformer models to extract features from company descriptions, and then applies zero-shot learning to classify companies into relevant categories without the need for specific training data for each category. We evaluate our approach on publicly available datasets of textual descriptions of companies, and demonstrate that it can streamline the process of company classification, thereby reducing the time and resources required in traditional approaches such as the Global Industry Classification Standard (GICS). The results show that this method has potential for automation of company classification, making it a promising avenue for future research in thi
    
[^89]: 基于软域转移的特征增强欺骗检测

    Deception Detection with Feature-Augmentation by soft Domain Transfer. (arXiv:2305.01011v1 [cs.CL])

    [http://arxiv.org/abs/2305.01011](http://arxiv.org/abs/2305.01011)

    本文提出了一种基于中间层表示的特征增强方法，通过软域转移进行领域间的关联，提高欺骗检测的准确率，分析结果显示推文是检测假新闻和钓鱼电子邮件最有帮助的信息提供者，新闻在推特谣言检测中最有帮助。

    

    在信息爆炸的这个时代，欺骗者利用不同的信息领域或媒介来利用用户，比如新闻、电子邮件和推文等。尽管已经进行了大量的研究来检测这些领域中的欺骗，但新事件中信息的短缺需要这些领域相互关联来对抗欺骗。为了形成这种关联，我们提出了一种通过利用神经模型的中间层表示进行特征增强的方法。我们的方法比自身领域基线模型提高了多达6.60%的准确率。我们发现推文是检测假新闻和钓鱼电子邮件最有帮助的信息提供者，而新闻在推特谣言检测中最有帮助。我们的分析提供了对于域知识转移的有用洞见，可以帮助建立比现有文献更强大的欺骗检测系统。

    In this era of information explosion, deceivers use different domains or mediums of information to exploit the users, such as News, Emails, and Tweets. Although numerous research has been done to detect deception in all these domains, information shortage in a new event necessitates these domains to associate with each other to battle deception. To form this association, we propose a feature augmentation method by harnessing the intermediate layer representation of neural models. Our approaches provide an improvement over the self-domain baseline models by up to 6.60%. We find Tweets to be the most helpful information provider for Fake News and Phishing Email detection, whereas News helps most in Tweet Rumor detection. Our analysis provides a useful insight for domain knowledge transfer which can help build a stronger deception detection system than the existing literature.
    
[^90]: 朝向对神经网络的现象学理解：数据

    Towards a Phenomenological Understanding of Neural Networks: Data. (arXiv:2305.00995v1 [cs.LG])

    [http://arxiv.org/abs/2305.00995](http://arxiv.org/abs/2305.00995)

    该研究尝试通过集体变量来建立神经网络（NN）的理论，探究NN的学习过程。研究者提出两个变量，熵和经验神经切向核（NTK）的迹线，并实证分析发现这些变量与模型性能有关。随机网络提炼（RND）被用于优化数据选择，发现选择RND数据集能胜过随机选取，且RND数据集相关的集体变量更大。

    

    如果建立在集体变量之上的神经网络（NN）理论将为科学家提供更好地理解每个阶段的学习过程的工具。在这项工作中，我们介绍了两个这样的变量，即基于传递给模型的训练数据构建的经验神经切向核（NTK）的熵和迹线。我们在这些变量的背景下对NN性能进行了实证分析，并发现起始熵、NTK的迹线与训练完成后计算的模型推广之间存在相关性。然后，将此框架应用于NN训练的最佳数据选择问题。为此，使用随机网络提炼（RND）作为选择训练数据的手段，然后与随机选取数据进行比较。结果表明，RND选择的数据集不仅能够胜过随机选取，而且与RND数据集相关联的集体变量比随机选取的要大。

    A theory of neural networks (NNs) built upon collective variables would provide scientists with the tools to better understand the learning process at every stage. In this work, we introduce two such variables, the entropy and the trace of the empirical neural tangent kernel (NTK) built on the training data passed to the model. We empirically analyze the NN performance in the context of these variables and find that there exists correlation between the starting entropy, the trace of the NTK, and the generalization of the model computed after training is complete. This framework is then applied to the problem of optimal data selection for the training of NNs. To this end, random network distillation (RND) is used as a means of selecting training data which is then compared with random selection of data. It is shown that not only does RND select data-sets capable of outperforming random selection, but that the collective variables associated with the RND data-sets are larger than those o
    
[^91]: 一种新的算法可以在极度缺乏真实数据的情况下生成用于训练机器学习模型的数据

    A novel algorithm can generate data to train machine learning models in conditions of extreme scarcity of real world data. (arXiv:2305.00987v1 [cs.LG])

    [http://arxiv.org/abs/2305.00987](http://arxiv.org/abs/2305.00987)

    该论文提出了一种新的算法，可以在极度缺乏真实数据的情况下生成大型人造数据集以用于训练机器学习模型。该算法基于遗传算法，通过对随机生成的数据集的突变来训练神经网络，并通过引入选择性压力来优化生成的数据集。

    

    训练机器学习模型需要大量数据集，但收集、整理和操纵大型复杂的真实数据集会带来成本、伦理和法律问题以及数据可用性的问题。在这里，我们提出了一种新的算法，可以在极度缺乏真实数据的情况下生成大型人造数据集，以用于训练机器学习模型。该算法基于遗传算法，通过突变随机生成的数据集来训练神经网络。训练后，神经网络在一批真实数据上的表现被视为用于其训练的生成数据集的适应性代理。通过对生成的数据集的种群施加选择性压力，淘汰不适宜的个体，通过代际遗传，适应性最强的个体的适应性得到提高。

    Training machine learning models requires large datasets. However, collecting, curating, and operating large and complex sets of real world data poses problems of costs, ethical and legal issues, and data availability. Here we propose a novel algorithm to generate large artificial datasets to train machine learning models in conditions of extreme scarcity of real world data. The algorithm is based on a genetic algorithm, which mutates randomly generated datasets subsequently used for training a neural network. After training, the performance of the neural network on a batch of real world data is considered a surrogate for the fitness of the generated dataset used for its training. As selection pressure is applied to the population of generated datasets, unfit individuals are discarded, and the fitness of the fittest individuals increases through generations. The performance of the data generation algorithm was measured on the Iris dataset and on the Breast Cancer Wisconsin diagnostic d
    
[^92]: 基于注意力机制的时空图神经ODE用于交通预测

    Attention-based Spatial-Temporal Graph Neural ODE for Traffic Prediction. (arXiv:2305.00985v1 [cs.LG])

    [http://arxiv.org/abs/2305.00985](http://arxiv.org/abs/2305.00985)

    这篇论文提出了一个基于注意力机制的图神经ODE模型（ASTGODE），该模型可以解决交通预测问题。实验结果表明，ASTGODE模型在准确性方面优于现有的GNN模型。

    

    交通预测是智能交通系统中的一个重要问题。图神经网络（GNN）是一种有效的深度学习模型，可以捕捉交通数据的复杂时空依赖性，并实现理想的预测性能。在本文中，我们提出了基于注意力机制的图神经ODE（ASTGODE），明确学习交通系统的动态，使我们的机器学习模型的预测更加可解释。我们的模型聚合了不同周期的交通模式，并在两个真实世界的交通数据集上表现出令人满意的性能。实验结果表明，我们的模型在均方根误差度量的准确性上，是所有现有GNN模型中表现最好的。

    Traffic forecasting is an important issue in intelligent traffic systems (ITS). Graph neural networks (GNNs) are effective deep learning models to capture the complex spatio-temporal dependency of traffic data, achieving ideal prediction performance. In this paper, we propose attention-based graph neural ODE (ASTGODE) that explicitly learns the dynamics of the traffic system, which makes the prediction of our machine learning model more explainable. Our model aggregates traffic patterns of different periods and has satisfactory performance on two real-world traffic data sets. The results show that our model achieves the highest accuracy of the root mean square error metric among all the existing GNN models in our experiments.
    
[^93]: 工控系统中针对异常检测的双阶段双Copula方法

    Two-phase Dual COPOD Method for Anomaly Detection in Industrial Control System. (arXiv:2305.00982v1 [cs.LG])

    [http://arxiv.org/abs/2305.00982](http://arxiv.org/abs/2305.00982)

    本文提出了一个双阶段基于双Copula的离群检测方法，该方法具有优异的透明度、可解释性和计算效率，并在实际工控系统数据集上具有卓越的性能。

    

    水处理设施和电站等关键基础设施依赖于工业控制系统（ICS）进行监控和控制，这使它们容易受到网络攻击和系统故障的影响。传统的ICS异常检测方法缺乏透明度和可解释性，这使实践者难以理解和信任结果。本文提出了一种基于两个阶段的双Copula离群检测方法来应对这些挑战。

    Critical infrastructures like water treatment facilities and power plants depend on industrial control systems (ICS) for monitoring and control, making them vulnerable to cyber attacks and system malfunctions. Traditional ICS anomaly detection methods lack transparency and interpretability, which make it difficult for practitioners to understand and trust the results. This paper proposes a two-phase dual Copula-based Outlier Detection (COPOD) method that addresses these challenges. The first phase removes unwanted outliers using an empirical cumulative distribution algorithm, and the second phase develops two parallel COPOD models based on the output data of phase 1. The method is based on empirical distribution functions, parameter-free, and provides interpretability by quantifying each feature's contribution to an anomaly. The method is also computationally and memory-efficient, suitable for low- and high-dimensional datasets. Experimental results demonstrate superior performance in 
    
[^94]: 慢混合过程的泛化能力研究

    Generalization for slowly mixing processes. (arXiv:2305.00977v1 [cs.LG])

    [http://arxiv.org/abs/2305.00977](http://arxiv.org/abs/2305.00977)

    该论文研究了慢混合过程的泛化能力，给出了对由平稳且phi混合过程生成的数据的不同种类损失类别的一种上界。

    

    该论文给出了针对由平稳且phi混合过程生成的数据的各种损失类别的一种上界，其中混合时间（获得近似独立所需的时间）仅以加法方式进入样本复杂度。对于慢速混合过程而言，这可以是其优势，因为其与混合时间的乘法依赖性相比要好。允许的损失类别包括具有指定Lipschitz归一化或平滑度参数的函数。该上界还可以应用于对无限制损失类别的统一性研究，其取决于样本路径上函数的局部Lipschitz性质。

    A bound uniform over various loss-classes is given for data generated by stationary and phi-mixing processes, where the mixing time (the time needed to obtain approximate independence) enters the sample complexity only in an additive way. For slowly mixing processes this can be a considerable advantage over results with multiplicative dependence on the mixing time. The admissible loss-classes include functions with prescribed Lipschitz norms or smoothness parameters. The bound can also be applied to be uniform over unconstrained loss-classes, where it depends on local Lipschitz properties of the function on the sample path.
    
[^95]: 深度集成用于改进气候变化条件下统计降尺度模型的不确定性量化

    Deep Ensembles to Improve Uncertainty Quantification of Statistical Downscaling Models under Climate Change Conditions. (arXiv:2305.00975v1 [cs.LG])

    [http://arxiv.org/abs/2305.00975](http://arxiv.org/abs/2305.00975)

    本文提出了使用深度集成作为一种简单的方法来改善统计降尺度模型的不确定性量化，以提供更优越的极端天气事件规划。深度集成可提供更好的风险评估，是应对气候变化的部门应用所迫切需要的。

    

    最近，深度学习已成为一种有望解决统计降尺度问题的工具，统计降尺度指的是从粗糙的低分辨率变量生成高分辨率气候场的方法集。然而，由于其基于稳定性假设，其在气候变化条件下推广的能力仍然存在疑问。我们提出使用深度集成作为一种简单的方法来改善统计降尺度模型的不确定性量化。通过更好地捕捉不确定性，统计降尺度模型可以提供更优越的极端天气事件规划，这是各种负面社会和经济影响的来源。由于不存在观测到的未来数据，我们依靠伪实验来评估深度集成对气候变化预测的不确定性量化是否适用。深度集成可提供更好的风险评估，这是应对气候变化的部门应用所迫切需要的。

    Recently, deep learning has emerged as a promising tool for statistical downscaling, the set of methods for generating high-resolution climate fields from coarse low-resolution variables. Nevertheless, their ability to generalize to climate change conditions remains questionable, mainly due to the stationarity assumption. We propose deep ensembles as a simple method to improve the uncertainty quantification of statistical downscaling models. By better capturing uncertainty, statistical downscaling models allow for superior planning against extreme weather events, a source of various negative social and economic impacts. Since no observational future data exists, we rely on a pseudo reality experiment to assess the suitability of deep ensembles for quantifying the uncertainty of climate change projections. Deep ensembles allow for a better risk assessment, highly demanded by sectoral applications to tackle climate change.
    
[^96]: 关于使用深度生成模型进行完美预测气候降尺度的研究

    On the use of Deep Generative Models for Perfect Prognosis Climate Downscaling. (arXiv:2305.00974v1 [cs.LG])

    [http://arxiv.org/abs/2305.00974](http://arxiv.org/abs/2305.00974)

    研究提出了使用生成模型以改善气候降尺度的空间一致性问题，从而更好地应对气候变化。

    

    深度学习最近已经成为一种完美的预测降尺度技术，可以从大规模粗粒度的大气数据中计算出高分辨率的场。尽管这些技术有望复制观测到的局部变异，但它们是基于在每个位置估计独立分布，这导致空间结构不足，特别是在下降雨量时。本研究建议使用生成模型来改善高分辨率场的空间一致性，这在一些部门应用（例如水文学）中对应对气候变化非常必需。

    Deep Learning has recently emerged as a perfect prognosis downscaling technique to compute high-resolution fields from large-scale coarse atmospheric data. Despite their promising results to reproduce the observed local variability, they are based on the estimation of independent distributions at each location, which leads to deficient spatial structures, especially when downscaling precipitation. This study proposes the use of generative models to improve the spatial consistency of the high-resolution fields, very demanded by some sectoral applications (e.g., hydrology) to tackle climate change.
    
[^97]: 支持客户端差分隐私的联邦学习中更平的梯度图和更好的泛化能力

    Towards the Flatter Landscape and Better Generalization in Federated Learning under Client-level Differential Privacy. (arXiv:2305.00873v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.00873](http://arxiv.org/abs/2305.00873)

    提出了一种名为DP-FedSAM的算法，它利用梯度扰动来减轻差分隐私的负面影响，并将锐度感知优化器整合到算法中，生成更平缓的损失曲面和更好的权重扰动鲁棒性，从而提高联邦学习的性能。

    

    为了保护隐私和减少信息泄露，采用客户端级别差分隐私的联邦学习方法已经成为保护用户隐私的标准。然而，目前的方案往往导致尖锐的损失图像和权重扰动的鲁棒性较差，从而导致性能下降。为了缓解这些问题，我们提出了一种新的客户端级别差分隐私联邦学习算法DP-FedSAM，该算法利用梯度扰动来减轻差分隐私的负面影响。DP-FedSAM将锐度感知优化器（SAM）整合到算法中，生成局部平坦模型，从而提高了稳定性和权重扰动鲁棒性，结果产生了局部更新的小范数和对DP噪声的鲁棒性，从而提高了性能。为了进一步减少随机噪声的幅度同时实现更好的性能，我们提出了DP-FedSAM-top_k，采用局部更新稀疏性的方法。

    To defend the inference attacks and mitigate the sensitive information leakages in Federated Learning (FL), client-level Differentially Private FL (DPFL) is the de-facto standard for privacy protection by clipping local updates and adding random noise. However, existing DPFL methods tend to make a sharp loss landscape and have poor weight perturbation robustness, resulting in severe performance degradation. To alleviate these issues, we propose a novel DPFL algorithm named DP-FedSAM, which leverages gradient perturbation to mitigate the negative impact of DP. Specifically, DP-FedSAM integrates Sharpness Aware Minimization (SAM) optimizer to generate local flatness models with improved stability and weight perturbation robustness, which results in the small norm of local updates and robustness to DP noise, thereby improving the performance. To further reduce the magnitude of random noise while achieving better performance, we propose DP-FedSAM-$top_k$ by adopting the local update sparsi
    
[^98]: 重访图机器学习的鲁棒性

    Revisiting Robustness in Graph Machine Learning. (arXiv:2305.00851v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.00851](http://arxiv.org/abs/2305.00851)

    GNNs在节点层面预测中可能会因图结构小的更改而导致不稳健。本文提出了对抗性图的概念，并发现所有评估的GNN都表现出过度鲁棒性，包括超出语义变化点。在推理的图中包括训练图的标签-结构可以避免过度鲁棒性。基于对抗性概念的鲁棒性测试与最终主动学习表现相关。

    

    许多研究表明，图神经网络（GNN）的节点层面预测对图结构的微小更改——通常称为对抗性更改——不稳健。然而，由于手动检查图形困难，不清楚研究的扰动是否总是保留对抗性示例的核心假设:即不改变语义内容。为解决这个问题，我们引入了一种更基本的对抗性图形概念，它知道语义内容的改变。通过使用语境随机块模型（CSBMs）和真实世界的图表，我们的结果发现：i）对于大多数节点，主要的扰动模型包括大量扰动图违反了未改变的语义假设；ii）令人惊讶的是，所有评估的GNN都表现出过度鲁棒性——即超出语义变化点的鲁棒性。我们发现这是对抗性示例的补充现象，并显示将训练图的标签-结构包括在推理的图中可以避免这种过度鲁棒性。最后，提出了一种基于所提出的对抗性概念的鲁棒性测试，并表明它与最终的主动学习性能相关。

    Many works show that node-level predictions of Graph Neural Networks (GNNs) are unrobust to small, often termed adversarial, changes to the graph structure. However, because manual inspection of a graph is difficult, it is unclear if the studied perturbations always preserve a core assumption of adversarial examples: that of unchanged semantic content. To address this problem, we introduce a more principled notion of an adversarial graph, which is aware of semantic content change. Using Contextual Stochastic Block Models (CSBMs) and real-world graphs, our results uncover: $i)$ for a majority of nodes the prevalent perturbation models include a large fraction of perturbed graphs violating the unchanged semantics assumption; $ii)$ surprisingly, all assessed GNNs show over-robustness - that is robustness beyond the point of semantic change. We find this to be a complementary phenomenon to adversarial examples and show that including the label-structure of the training graph into the infer
    
[^99]: SelfDocSeg: 一种自我监督视觉文档分割方法

    SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation. (arXiv:2305.00795v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.00795](http://arxiv.org/abs/2305.00795)

    SelfDocSeg提出了一种完全基于视觉的自我监督文档分割方法，通过生成伪布局训练图像编码器学习文档对象的表示和定位，克服了标注数据稀缺的挑战。

    

    文档布局分析是一个众所周知的问题，已经被广泛探索，涵盖了从文本挖掘、识别到基于图形的表示、视觉特征提取等多种解决方案。然而，现有的大部分工作都忽略了标注数据的稀缺性这一关键事实。我们使用自我监督来解决这一挑战，并且与现有的使用文本挖掘和文本标签的自我监督文档分割方法不同，我们使用了完全基于视觉的方法在预训练中生成伪布局，以在没有任何真实标签或其导出物的情况下训练图像编码器，以自我监督的框架中学习文档对象的表示和定位。

    Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tun
    
[^100]: 跨图动态迁移学习

    Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])

    [http://arxiv.org/abs/2305.00664](http://arxiv.org/abs/2305.00664)

    该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。

    

    在许多高风险领域中，跨图传输知识起着关键作用，包括运输网络、电子商务网络、神经科学和金融领域。我们提出了一个新问题：在动态设置下，考虑已观察到的具有标签的源图和标签稀疏的目标图，如何有效地表征不断变化的领域偏差，并优化目标域在下一个时间戳的泛化性能？为了回答这个问题，我们首次提出了跨图动态迁移学习设置下的一般化界限，这意味着泛化性能由领域演化控制。

    Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
    
[^101]: 使用奇异值分解的深度强化学习中的表征学习和探索

    Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition. (arXiv:2305.00654v1 [cs.LG])

    [http://arxiv.org/abs/2305.00654](http://arxiv.org/abs/2305.00654)

    本文提出了一种基于奇异值分解的自动表征学习模型，可以获得保留转换结构的表示形式并捕捉状态访问的相对频率。该方法不需要转移矩阵，可以利用深度网络，适用于部分可观察领域，并且在多任务设置中表现良好。

    

    表现学习和探索是任何深度强化学习代理所面临的关键挑战。本文提供了一种基于奇异值分解的方法，可以用来获得保留域中潜在转换结构的表示形式。有趣的是，我们发现这些表示形式还捕捉了状态访问的相对频率，从而免费提供了伪计数的估计。为了将这种分解方法推广到大规模域，我们提供了一种不需要建立转移矩阵，可以利用深度网络，也允许小批量训练的算法。此外，我们从预测状态表示中吸取灵感，并扩展了我们的分解方法到部分可观察的环境。通过对部分可观察领域的多任务设置进行实验，我们展示了提出的方法不仅可以在DM-Lab-30环境中学习有用的表示形式。

    Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on DM-Lab-30 environments (that have inputs
    
[^102]: 分解增强推理的自我评估引导解码

    Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])

    [http://arxiv.org/abs/2305.00633](http://arxiv.org/abs/2305.00633)

    本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。

    

    我们提出了一种有效的提示方法，通过随机束搜索结合自我评估引导。我们的方法使用经过校准的自动标准探索推理搜索空间。这使得有效搜索能够产生更高质量的最终预测结果。使用自我评估引导的随机束搜索，我们在产生推理链的质量和多样性之间平衡权衡，从而能够适应多数投票，并在GSM8K、AQUA和StrategyQA基准测试中以少量示例准确性分别超越对应的Codex-backboned基线$6.34\%$、$9.56\%$和$5.46\%$。对我们的分解式推理分析发现，它可以指出逻辑错误并导致更高的一致性和鲁棒性。

    We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
    
[^103]: 通过引导随机搜索从人脑活动中重建视觉图像

    Reconstructing seen images from human brain activity via guided stochastic search. (arXiv:2305.00556v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.00556](http://arxiv.org/abs/2305.00556)

    本研究使用条件生成扩散模型，改进过去的视觉重建算法，通过对一小组图像的采样和编码模型的选择，实现了从人脑活动中高质量、保留语义内容的重建结果，并发现了视觉皮层不同区域的重建时间差异。

    

    视觉重建算法是一种将脑活动映射到像素的解释工具。过去的重建算法采用大规模库的暴力搜索来选择候选图像，这些图像通过编码模型可以准确地预测脑活动。本研究使用条件生成扩散模型来扩展和改进这种基于搜索的策略。我们在大部分视觉皮层的体素中从人脑活动（7T fMRI）解码出语义描述符，然后使用扩散模型在此描述符的条件下对一小组图像进行采样。我们将每个样本通过编码模型，选择最能准确预测脑活动的图像，然后使用这些图像来种子另一个库。我们展示了这个过程通过在迭代中细化低级图像细节，同时保留语义内容而收敛到高质量的重建结果。有趣的是，收敛所需的时间在视觉皮层中有系统差异，表明了脑区的高层抽象概念需要更长的时间来集成和反映。

    Visual reconstruction algorithms are an interpretive tool that map brain activity to pixels. Past reconstruction algorithms employed brute-force search through a massive library to select candidate images that, when passed through an encoding model, accurately predict brain activity. Here, we use conditional generative diffusion models to extend and improve this search-based strategy. We decode a semantic descriptor from human brain activity (7T fMRI) in voxels across most of visual cortex, then use a diffusion model to sample a small library of images conditioned on this descriptor. We pass each sample through an encoding model, select the images that best predict brain activity, and then use these images to seed another library. We show that this process converges on high-quality reconstructions by refining low-level image details while preserving semantic content across iterations. Interestingly, the time-to-convergence differs systematically across visual cortex, suggesting a succi
    
[^104]: "Río Hortega University Hospital Glioblastoma Dataset: 一份包含术前、术后早期和复发MRI扫描的综合性数据集 (RHUH-GBM)"

    The R\'io Hortega University Hospital Glioblastoma dataset: a comprehensive collection of preoperative, early postoperative and recurrence MRI scans (RHUH-GBM). (arXiv:2305.00005v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.00005](http://arxiv.org/abs/2305.00005)

    这份数据集提供了包括MRI图像、容积评估、分子数据和生存细节在内的Glioblastoma患者相关数据，同时提供了专家纠正的肿瘤亚区划分，为发展术后和随访MRI扫描的算法提供了有价值的基准数据。

    

    高度侵袭性的原发性脑肿瘤Glioblastoma与患者不良后果相关，MRI在诊断、特征描述和预测Glioblastoma进展方面扮演重要角色。然而，公共MRI资料库存在严重问题，包括术后和随访研究不足，以及专家肿瘤分割不足等。为了解决这些问题，我们提出了“Río Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)”，一个包含多参量MRI图像、容积评估、分子数据和对进行全面或近全增强肿瘤切除的Glioblastoma患者的生存细节的数据集。该数据集具有专家纠正的肿瘤亚区划分，为发展术后和随访MRI扫描的算法提供了有价值的基准数据。RHUH-GBM数据集的公开发布在Glioblastoma研究中做出了显著贡献，使科学界能够更好地理解此病并开展深入的研究。

    Glioblastoma, a highly aggressive primary brain tumor, is associated with poor patient outcomes. Although magnetic resonance imaging (MRI) plays a critical role in diagnosing, characterizing, and forecasting glioblastoma progression, public MRI repositories present significant drawbacks, including insufficient postoperative and follow-up studies as well as expert tumor segmentations. To address these issues, we present the "R\'io Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)," a collection of multiparametric MRI images, volumetric assessments, molecular data, and survival details for glioblastoma patients who underwent total or near-total enhancing tumor resection. The dataset features expert-corrected segmentations of tumor subregions, offering valuable ground truth data for developing algorithms for postoperative and follow-up MRI scans. The public release of the RHUH-GBM dataset significantly contributes to glioblastoma research, enabling the scientific community to st
    
[^105]: 通过应用NLP技术分析YouTube的字幕数据，自动生成视频人体姿势分析的标记数据集

    Automatic Generation of Labeled Data for Video-Based Human Pose Analysis via NLP applied to YouTube Subtitles. (arXiv:2304.14489v1 [cs.CV])

    [http://arxiv.org/abs/2304.14489](http://arxiv.org/abs/2304.14489)

    该研究利用NLP技术分析健身视频字幕数据自动生成人体姿势分析的标记数据集。

    

    随着计算机视觉和机器学习技术的不断提升，基于视频的居家锻炼评估系统已成为目前研究的热门话题。然而，性能很大程度上取决于可用的训练数据量。由于专门针对运动的标记数据集很少，我们提出了一种利用在线健身视频丰富资源的方法。具体来说，我们利用视频通常不仅展示练习内容，还提供语言信息作为额外的信息源的优势。以俯卧撑为例，我们展示了通过NLP技术分析字幕数据，可以创建一个包含与姿势分析相关信息的标记数据集（无关紧要，相关正确，相关不正确）。特别地，我们展示了无关剪辑（$n=332$）具有与相关剪辑（$n=298$）显著不同的关节可见性值。检查聚类中心也展现了有用的信息。

    With recent advancements in computer vision as well as machine learning (ML), video-based at-home exercise evaluation systems have become a popular topic of current research. However, performance depends heavily on the amount of available training data. Since labeled datasets specific to exercising are rare, we propose a method that makes use of the abundance of fitness videos available online. Specifically, we utilize the advantage that videos often not only show the exercises, but also provide language as an additional source of information. With push-ups as an example, we show that through the analysis of subtitle data using natural language processing (NLP), it is possible to create a labeled (irrelevant, relevant correct, relevant incorrect) dataset containing relevant information for pose analysis. In particular, we show that irrelevant clips ($n=332$) have significantly different joint visibility values compared to relevant clips ($n=298$). Inspecting cluster centroids also show
    
[^106]: JaxPruner：一个用于稀疏性研究的简明库

    JaxPruner: A concise library for sparsity research. (arXiv:2304.14082v1 [cs.LG])

    [http://arxiv.org/abs/2304.14082](http://arxiv.org/abs/2304.14082)

    本文介绍了JaxPruner，一款用于研究稀疏神经网络的开源库。JaxPruner提供了流行的剪枝和稀疏训练算法的简明实现，最小化内存和延迟开销，并可轻松集成到现有的JAX库中。

    

    本文介绍了JaxPruner，这是一个基于JAX的开源剪枝和稀疏训练库，旨在通过提供流行的剪枝和稀疏训练算法的简明实现，最小化内存和延迟开销，加速稀疏神经网络的研究。JaxPruner实现的算法使用通用API，并与流行的优化库Optax无缝协作，从而使其能轻松集成到现有的JAX库中。我们通过在四个不同的代码库中提供示例并在流行的基准测试中提供基准实验来展示这种集成的便捷性。

    This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
    
[^107]: Rubik光学神经网络：具有物理感知旋转结构的多任务学习

    Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])

    [http://arxiv.org/abs/2304.12985](http://arxiv.org/abs/2304.12985)

    RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。

    

    最近，有越来越多的研究工作在推进光学神经网络（ONNs），在功率效率，并行性和计算速度方面，ONNs带来了机器学习（ML）方面的显着优势。

    Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
    
[^108]: 多模式传感器融合技术在DED打印SS316L部件中的原位表面孔隙率预测

    In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion. (arXiv:2304.08658v1 [physics.app-ph])

    [http://arxiv.org/abs/2304.08658](http://arxiv.org/abs/2304.08658)

    本研究利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力，可以实时预测每个沃克塞尔中的气孔存在，是一个重大飞跃。

    

    本研究旨在将声发射（AE）等多模式传感器数据中的时频模式与DED过程中的孔隙率形成进行高空间（0.5mm）和时间（<1ms）的关联。通过采用可解释的AI方法中的LIME（局部可解释性非特定性解释），将AE中的某些高频波形特征归因于DED过程中的两个主要孔隙形成途径：飞溅事件和低热量输入下相邻打印轨迹的不充分熔合。该方法为实时预测每个沃克塞尔（0.5mm）中的气孔存在提供了令人兴奋的可能性，这是与先前努力相比的一个重大飞跃。在打印并随后加工SS316L材料样品时，同步采集了包括力，AE，振动和温度在内的多模式传感器数据。深度卷积神经网络分类器用于识别两种孔隙形成途径的AE特征，然后使用可解释AI方法进一步分析这些特征。结果表明，利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力。

    This study aims to relate the time-frequency patterns of acoustic emission (AE) and other multi-modal sensor data collected in a hybrid directed energy deposition (DED) process to the pore formations at high spatial (0.5 mm) and time (< 1ms) resolutions. Adapting an explainable AI method in LIME (Local Interpretable Model-Agnostic Explanations), certain high-frequency waveform signatures of AE are to be attributed to two major pathways for pore formation in a DED process, namely, spatter events and insufficient fusion between adjacent printing tracks from low heat input. This approach opens an exciting possibility to predict, in real-time, the presence of a pore in every voxel (0.5 mm in size) as they are printed, a major leap forward compared to prior efforts. Synchronized multimodal sensor data including force, AE, vibration and temperature were gathered while an SS316L material sample was printed and subsequently machined. A deep convolution neural network classifier was used to ide
    
[^109]: 基于神经网络集成的不确定性感知车辆能效预测

    Uncertainty-Aware Vehicle Energy Efficiency Prediction using an Ensemble of Neural Networks. (arXiv:2304.07073v1 [cs.LG])

    [http://arxiv.org/abs/2304.07073](http://arxiv.org/abs/2304.07073)

    本文基于深度神经网络集成学习方法，提出了一种减少预测不确定性并输出衡量值的车辆能效预测方法，可应用于降低碳足迹，并在测试中表现出高度的预测性能。

    

    运输部门约占全球温室气体排放的25％。因此，在交通部门提高能效是减少碳足迹的关键。能效通常以每行驶距离的能源消耗来衡量，例如每公里的燃油升数。影响能效的主要因素包括车辆类型，环境，驾驶员行为和天气条件。这些不同的因素引入了估计车辆能效的不确定性。本文提出了一种基于深度神经网络集成学习方法，旨在减少预测不确定性并输出这种不确定性的衡量值。本研究使用公开可得的车辆能源数据集（VED）进行了评估，并将其与每辆车和能源类型的几个基线进行了比较。结果表明，该方法具有高度的预测性能，并且能够输出预测不确定性的衡量值。

    The transportation sector accounts for about 25% of global greenhouse gas emissions. Therefore, an improvement of energy efficiency in the traffic sector is crucial to reducing the carbon footprint. Efficiency is typically measured in terms of energy use per traveled distance, e.g. liters of fuel per kilometer. Leading factors that impact the energy efficiency are the type of vehicle, environment, driver behavior, and weather conditions. These varying factors introduce uncertainty in estimating the vehicles' energy efficiency. We propose in this paper an ensemble learning approach based on deep neural networks (ENN) that is designed to reduce the predictive uncertainty and to output measures of such uncertainty. We evaluated it using the publicly available Vehicle Energy Dataset (VED) and compared it with several baselines per vehicle and energy type. The results showed a high predictive performance and they allowed to output a measure of predictive uncertainty.
    
[^110]: 奖励是否合理？在 MACHIAVELLI 基准测试中衡量奖励与道德行为之间的权衡

    Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. (arXiv:2304.03279v1 [cs.LG])

    [http://arxiv.org/abs/2304.03279](http://arxiv.org/abs/2304.03279)

    本文介绍了 MACHIAVELLI 基准测试，用于衡量人工智能代理是否表现出马基雅维利行为，发现了最大化奖励和行为的道德性之间存在权衡，并探索了基于语言模型的方法来减轻这种权衡。

    

    传统上，人工智能代理被训练成最大化奖励，这可能会激励追求权力和欺骗行为，类似于语言模型中的下一个标记预测可能会激励有害行为。那么代理是否自然而然地学会了马基雅维利行为？我们如何在 GPT-4 等通用模型中衡量这些行为呢？为回答这些问题，我们引入了 MACHIAVELLI 基准测试，该测试涵盖了超过一百万个多样化的情景，重点关注社会决策制定，用于衡量人工代理是否表现出马基雅维利行为。我们数学化了数十种有害行为，并使用我们的注释来评估代理倾向于追求权力，造成功能不良和违反伦理的倾向。我们观察到最大化奖励和行为的道德性之间存在一些紧张关系。为了改善这种权衡，我们研究了基于语言模型的方法，以使代理趋向于采取更少的有害行为。我们的结果显示，MACHIAVELLI 是评估人工代理马基雅维利行为水平的有用基准测试。

    Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results sh
    
[^111]: 跨GAN审核：无监督识别预训练生成模型之间的属性级相似性和差异

    Cross-GAN Auditing: Unsupervised Identification of Attribute Level Similarities and Differences between Pretrained Generative Models. (arXiv:2303.10774v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.10774](http://arxiv.org/abs/2303.10774)

    本文提出了一种跨GAN审核方法，在与已建立“参考”GAN进行比较后，联合识别新开发的GAN中可理解的属性，提供GAN之间相似性和差异性的直观评估。

    

    生成对抗网络（GAN）特别是在复杂分布和有限数据情况下，训练难度极大。这促使了对已训练网络进行审计的需求，以便以人类可理解的格式识别偏见或确保公平性。现有的GAN审计工具仅限于基于总结统计信息（如FID或召回率）的粗粒度模型-数据比较。本文提出了一种新的方法，即与之前的基线GAN相比较的途径。为此，我们引入了跨GAN审核（xGA），该方法联合识别出可理解的属性，这些属性可以是两个GAN共有的，是客户端GAN的新颖属性，或者是客户端GAN缺少的属性。这为用户和模型开发人员提供了直观的GAN相似性和差异性评估。我们引入了新的指标来评估基于属性的GAN审核方法，并使用这些指标来比较现有的审计方法。

    Generative Adversarial Networks (GANs) are notoriously difficult to train especially for complex distributions and with limited data. This has driven the need for tools to audit trained networks in human intelligible format, for example, to identify biases or ensure fairness. Existing GAN audit tools are restricted to coarse-grained, model-data comparisons based on summary statistics such as FID or recall. In this paper, we propose an alternative approach that compares a newly developed GAN against a prior baseline. To this end, we introduce Cross-GAN Auditing (xGA) that, given an established "reference" GAN and a newly proposed "client" GAN, jointly identifies intelligible attributes that are either common across both GANs, novel to the client GAN, or missing from the client GAN. This provides both users and model developers an intuitive assessment of similarity and differences between GANs. We introduce novel metrics to evaluate attribute-based GAN auditing approaches and use these m
    
[^112]: ContraNorm: 对于过度平滑的对比学习视角和更多的研究

    ContraNorm: A Contrastive Learning Perspective on Oversmoothing and Beyond. (arXiv:2303.06562v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06562](http://arxiv.org/abs/2303.06562)

    本研究提出了一种新的规范化层——ContraNorm，针对图神经网络和变压器中的过度平滑问题，通过对比学习的方式在嵌入空间中破坏表示，缓解了完全塌陷和维度塌陷的现象，并在实验中表现出较高的精度。

    

    过度平滑现象在各种图神经网络和变压器中普遍存在，当层数增加时，其性能会变差。我们从维度折叠的一个更一般的视角来描述过度平滑的现象，表示会聚到一个狭窄的锥形空间中，而不是表示会聚到一个点上。受到对抗性学习在防止维度折叠方面的有效性启发，我们提出了一种新的规范化层——ContraNorm。直观上，ContraNorm会在嵌入空间中隐式破坏表示，导致更均匀的分布和轻微的维度折叠。在理论分析中，我们证明了在某些条件下，ContraNorm可以缓解完全塌陷和维度塌陷的情况。我们提出的规范化层可以轻松地集成到GNNs和Transformers中，且参数开销很小。实验结果表明，我们的提议可以提高GNNs和Transformers的精度。

    Oversmoothing is a common phenomenon in a wide range of Graph Neural Networks (GNNs) and Transformers, where performance worsens as the number of layers increases. Instead of characterizing oversmoothing from the view of complete collapse in which representations converge to a single point, we dive into a more general perspective of dimensional collapse in which representations lie in a narrow cone. Accordingly, inspired by the effectiveness of contrastive learning in preventing dimensional collapse, we propose a novel normalization layer called ContraNorm. Intuitively, ContraNorm implicitly shatters representations in the embedding space, leading to a more uniform distribution and a slighter dimensional collapse. On the theoretical analysis, we prove that ContraNorm can alleviate both complete collapse and dimensional collapse under certain conditions. Our proposed normalization layer can be easily integrated into GNNs and Transformers with negligible parameter overhead. Experiments o
    
[^113]: 物理约束下的神经微分方程学习多离子传输

    Physics-constrained neural differential equations for learning multi-ionic transport. (arXiv:2303.04594v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04594](http://arxiv.org/abs/2303.04594)

    本文提出了物理约束下的神经微分方程模型来学习聚酰胺纳米孔中离子的传输行为。

    

    离子通过聚酰胺纳米孔的连续模型需要通过复杂的孔隙几何形状求解偏微分方程。在这个长度和时间尺度上解决时空特征可能会使解决这些方程的计算复杂。此外，机制模型经常需要在纳米限制下离子相互作用参数之间的功能关系，这通常是实验上太具挑战性或难以知晓的。在本研究中，我们开发了第一个物理约束边界下的深度神经网络模型来学习聚酰胺纳米孔中的离子传输行为。所提出的体系结构结合了神经微分方程和经典闭合模型作为归纳偏差，直接编码到神经网络框架中。神经微分方程通过模拟连续模型的数据进行预训练，并在独立的实验数据上进行微调，以学习离子排斥行为。从高斯噪声增强进行数据增强。

    Continuum models for ion transport through polyamide nanopores require solving partial differential equations (PDEs) through complex pore geometries. Resolving spatiotemporal features at this length and time-scale can make solving these equations computationally intractable. In addition, mechanistic models frequently require functional relationships between ion interaction parameters under nano-confinement, which are often too challenging to measure experimentally or know a priori. In this work, we develop the first physics-informed deep learning model to learn ion transport behaviour across polyamide nanopores. The proposed architecture leverages neural differential equations in conjunction with classical closure models as inductive biases directly encoded into the neural framework. The neural differential equations are pre-trained on simulated data from continuum models and fine-tuned on independent experimental data to learn ion rejection behaviour. Gaussian noise augmentations from
    
[^114]: 针对耦合偏微分方程的耦合多小波神经算子学习

    Coupled Multiwavelet Neural Operator Learning for Coupled Partial Differential Equations. (arXiv:2303.02304v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02304](http://arxiv.org/abs/2303.02304)

    本论文提出一种耦合多小波神经算子学习的方案，解决了处理耦合多变量映射问题的难点，能够显著提高解决耦合偏微分方程的准确性，并在实验中得到了验证。

    

    耦合偏微分方程是描述许多物理过程复杂动态的关键任务。最近，神经算子已经展示出通过在傅里叶/小波空间直接学习积分核来解决PDE的能力。对于耦合PDE的解决方法，难点在于处理函数之间的耦合映射。为此，我们提出了一种耦合多小波神经算子（CMWNO）学习方案，通过在小波空间中进行多小波分解和重构过程中解耦合积分核。在解决Gray-Scott（GS）方程和非局部均场博弈（MFG）问题等耦合PDE方面，所提出的模型相对于先前基于学习的求解器实现了显著提高的准确性。根据我们的实验结果，所提出的模型相对于最先进模型的$L^2$误差表现出了$2\times \sim 4\times$的改进。

    Coupled partial differential equations (PDEs) are key tasks in modeling the complex dynamics of many physical processes. Recently, neural operators have shown the ability to solve PDEs by learning the integral kernel directly in Fourier/Wavelet space, so the difficulty for solving the coupled PDEs depends on dealing with the coupled mappings between the functions. Towards this end, we propose a \textit{coupled multiwavelets neural operator} (CMWNO) learning scheme by decoupling the coupled integral kernels during the multiwavelet decomposition and reconstruction procedures in the Wavelet space. The proposed model achieves significantly higher accuracy compared to previous learning-based solvers in solving the coupled PDEs including Gray-Scott (GS) equations and the non-local mean field game (MFG) problem. According to our experimental results, the proposed model exhibits a $2\times \sim 4\times$ improvement relative $L$2 error compared to the best results from the state-of-the-art mode
    
[^115]: 探索基于数值先验的广义CP分解低秩张量补全算法

    Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition. (arXiv:2302.05881v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05881](http://arxiv.org/abs/2302.05881)

    本文提出了一种新的方法框架GCDTC，利用数值先验和广义CP分解实现了更高的低秩张量补全精度；同时介绍了一个算法SPTC，作为该框架的一个实现。在实验中，该方法表现出比现有技术更好的性能。

    

    张量补全在计算机视觉、数据分析和信号处理等领域中具有重要意义。最近，低秩张量补全这一类别的方法得到了广泛研究，对补全张量施加低秩结构。虽然这些方法取得了巨大成功，但尚未考虑到张量元素的数值先验信息。忽略数值先验将导致丢失关于数据的重要信息，因此阻止算法达到最优精度。本研究试图构建一个新的方法框架，名为GCDTC（广义CP分解张量补全），以利用数值先验并实现更高的张量补全精度。在这个新引入的框架中，将广义的CP分解应用于低秩张量补全。本文还提出了一种名为SPTC（平滑泊松张量补全）的算法，用于非负整数张量补全，作为GCDTC框架的一个实现。通过对合成和真实世界数据集的大量实验，证明所提出的方法相比于现有技术具有更优的张量补全性能。

    Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion has recently been studied extensively. While such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the G
    
[^116]: 用于改善痴呆症患者情绪激动检测的下采样和累积类决策方法

    Undersampling and Cumulative Class Re-decision Methods to Improve Detection of Agitation in People with Dementia. (arXiv:2302.03224v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03224](http://arxiv.org/abs/2302.03224)

    本文对样本不平衡及标记不精确问题进行改进，仅用正常行为数据的20％即可训练具竞争力的痴呆症患者情绪激动检测模型。

    

    情绪激动是痴呆症患者最常见的症状之一，可能会对他们和看护者的安全造成威胁。开发客观的情绪激动检测方法对支持在居住环境中生活的痴呆症患者的健康和安全非常重要。本文旨在通过实现不同的下采样方法消除样本不平衡问题，并设计加权下采样方法来评估手动标记机制的有效性，以改善痴呆症患者情绪激动检测的问题。研究结果表明，只有正常行为数据的20％足以训练具有竞争力的情绪激动检测模型。

    Agitation is one of the most prevalent symptoms in people with dementia (PwD) that can place themselves and the caregiver's safety at risk. Developing objective agitation detection approaches is important to support health and safety of PwD living in a residential setting. In a previous study, we collected multimodal wearable sensor data from 17 participants for 600 days and developed machine learning models for predicting agitation in one-minute windows. However, there are significant limitations in the dataset, such as imbalance problem and potential imprecise labels as the occurrence of agitation is much rarer in comparison to the normal behaviours. In this paper, we first implement different undersampling methods to eliminate the imbalance problem, and come to the conclusion that only 20\% of normal behaviour data are adequate to train a competitive agitation detection model. Then, we design a weighted undersampling method to evaluate the manual labeling mechanism given the ambiguo
    
[^117]: 正则化流集合用于丰富的Aleatoric和Epistemic不确定性建模

    Normalizing Flow Ensembles for Rich Aleatoric and Epistemic Uncertainty Modeling. (arXiv:2302.01312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01312](http://arxiv.org/abs/2302.01312)

    本文提出了一个正则化流（NF）集合来估计Epistemic不确定性和Aleatoric不确定性，通过固定的dropout掩码来创建集合，运用于各种实验并可以提供全面的基准线。

    

    本文展示了如何可靠地估计Epistemic不确定性，同时保持捕捉复杂Aleatoric分布所需的灵活性。为此，我们提出了一个正则化流（NF）集合，这是建模Aleatoric不确定性的最先进技术。集合是通过固定的dropout掩码集合创建的，比创建单独的NF模型更加经济。我们演示了如何利用NF的独特结构——基础分布——来估计Aleatoric不确定性，而无需依赖样本，提供了全面的基准线，并推导出无偏的微分熵估计值。该方法被应用于各种用于基准测试Aleatoric和Epistemic不确定性估计的实验中：1D正弦数据，2D有风格网格世界（$\it{Wet Chicken}$），$\it{Pendulum}$和$\it{Hopper}$。在这些实验中，我们建立了一个主动学习框架，并评估了每个模型在测量Aleatoric和Epistemic不确定性方面的能力。

    In this work, we demonstrate how to reliably estimate epistemic uncertainty while maintaining the flexibility needed to capture complicated aleatoric distributions. To this end, we propose an ensemble of Normalizing Flows (NF), which are state-of-the-art in modeling aleatoric uncertainty. The ensembles are created via sets of fixed dropout masks, making them less expensive than creating separate NF models. We demonstrate how to leverage the unique structure of NFs, base distributions, to estimate aleatoric uncertainty without relying on samples, provide a comprehensive set of baselines, and derive unbiased estimates for differential entropy. The methods were applied to a variety of experiments, commonly used to benchmark aleatoric and epistemic uncertainty estimation: 1D sinusoidal data, 2D windy grid-world ($\it{Wet Chicken}$), $\it{Pendulum}$, and $\it{Hopper}$. In these experiments, we setup an active learning framework and evaluate each model's capability at measuring aleatoric and
    
[^118]: 循环揭示了复杂时间序列的共享因果驱动者

    Recurrences reveal shared causal drivers of complex time series. (arXiv:2301.13516v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13516](http://arxiv.org/abs/2301.13516)

    该研究开发了一种新的无监督学习算法，能够使用时间序列测量中的复现逐渐重构未被观察到的驱动信号，从而可靠地推断共享因果驱动者，其已在多个示例中进行验证。

    

    许多实验时间序列测量共享未被观察到的因果驱动器。例如，受转录因子靶向的基因、受大尺度大气环流影响的海洋流动，以及被下降神经元控制的电机电路。可靠地推断这种看不见的驱动力是必要的，以了解不同生物和工程系统中自上而下控制方案的间歇性本质。在这里，我们介绍了一种新的无监督学习算法，该算法使用时间序列测量中的复现逐渐重构未被观察到的驱动信号。借助于斜积动力系统的数学理论，我们确定了响应时间序列间共享的复现事件，这些事件隐含地定义了一个具有玻璃状结构的复现图。随着观察到的数据量或质量的改善，该复现图经历了一个渗流转变，表现为随机行走在诱导的景观上出现微弱的遍历性破裂 - 暴露出共享驱动信号作为少数参数的函数。我们在几个合成示例中展示了算法的性能。最后，我们将我们的方法应用于现实生态数据集，并确定了厄尔尼诺、季节性鱼类迁移和果蝇种群动态的驱动因素。

    Many experimental time series measurements share unobserved causal drivers. Examples include genes targeted by transcription factors, ocean flows influenced by large-scale atmospheric currents, and motor circuits steered by descending neurons. Reliably inferring this unseen driving force is necessary to understand the intermittent nature of top-down control schemes in diverse biological and engineered systems. Here, we introduce a new unsupervised learning algorithm that uses recurrences in time series measurements to gradually reconstruct an unobserved driving signal. Drawing on the mathematical theory of skew-product dynamical systems, we identify recurrence events shared across response time series, which implicitly define a recurrence graph with glass-like structure. As the amount or quality of observed data improves, this recurrence graph undergoes a percolation transition manifesting as weak ergodicity breaking for random walks on the induced landscape -- revealing the shared dri
    
[^119]: 联合球员与位置信息的足球（soccer）期望进球的机器学习方法

    A Machine Learning Approach for Player and Position Adjusted Expected Goals in Football (Soccer). (arXiv:2301.13052v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13052](http://arxiv.org/abs/2301.13052)

    该篇论文使用机器学习方法对足球运动员的位置信息以及其他参数进行建模，成功预测了进球的概率值，并进一步解决了“球掉进了错误的人手里”的问题。

    

    足球是一个非常结果驱动的行业，进球比大多数体育项目都更为罕见，因此，进一步评估团队和个人表现的参数非常关键。期望进球（xG）比仅有进球比分更能提供见解。为了解决足球中进一步分析的需求，本文使用机器学习应用程序开发并应用于足球事件数据。从该概念中，创建了一个二元分类问题，通过逻辑回归和基于梯度提升的方法输出概率估计值。该模型成功预测了基于15,575次射门的足球运动员的xG概率值。所提出的解决方案利用StatsBomb作为数据提供商，并将行业基准用于调整模型。提出的xG的机器学习解决方案进一步用于解决“球掉进了错误的人手里”的老话题。该模型的开发用于调整并获得更现实的价值。

    Football is a very result-driven industry, with goals being rarer than in most sports, so having further parameters to judge the performance of teams and individuals is key. Expected Goals (xG) allow further insight than just a scoreline. To tackle the need for further analysis in football, this paper uses machine learning applications that are developed and applied to Football Event data. From the concept, a Binary Classification problem is created whereby a probabilistic valuation is outputted using Logistic Regression and Gradient Boosting based approaches. The model successfully predicts xGs probability values for football players based on 15,575 shots. The proposed solution utilises StatsBomb as the data provider and an industry benchmark to tune the models in the right direction. The proposed ML solution for xG is further used to tackle the age-old cliche of: 'the ball has fallen to the wrong guy there'. The development of the model is used to adjust and gain more realistic value
    
[^120]: 神经关系图：识别标签噪音和异常数据的统一框架

    Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data. (arXiv:2301.12321v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12321](http://arxiv.org/abs/2301.12321)

    本研究提出了一种利用数据的关系结构识别标签错误和异常数据的统一方法，并提供了可视化工具，在大规模图像、语音和语言领域任务中表现良好。

    

    诊断和清理数据是构建健壮的机器学习系统的关键步骤。但是，由于存在复杂问题，如标签错误、欠表示和异常值，因此在具有真实世界分布的大规模数据集中识别问题具有挑战性。在本文中，我们提出了一种利用特征嵌入空间中数据的关系结构这一被忽视的信息来源，来识别有问题的数据的统一方法。为此，我们提出了基于数据的关系图结构来检测标签错误和异常数据的可扩展和有效的算法。我们进一步引入了一种可视化工具，提供特征嵌入空间中数据点的上下文信息，作为交互式诊断数据的有效工具。我们在大规模图像、语音和语言领域任务中评估了我们方法的标签错误和离群值/分布外（OOD）检测性能。

    Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, inc
    
[^121]: 风险敏感的强化学习算法：指数标准的应用

    Risk-Sensitive Reinforcement Learning with Exponential Criteria. (arXiv:2212.09010v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2212.09010](http://arxiv.org/abs/2212.09010)

    本文介绍了一种风险敏感的强化学习算法，使用指数判据来提高其系统抗干扰性和实用性。作者进行了在模拟和实际机器人上的实验验证，表明该算法能够有效地提高样本效率和执行效果。

    

    尽管风险中性的强化学习已经在很多应用中得到了实验成功，但是这种方法容易受到噪声和系统参数扰动的影响而不够稳健。因此,对风险敏感的强化学习算法进行了研究，以提高其系统抗干扰性，样本效率和实用性。本文介绍了一种新型的无模型风险敏感学习算法，将广泛使用的策略梯度算法进行变体，其实现过程类似。具体来说，本文研究了指数标准对强化学习代理的策略风险敏感性的影响，并开发了蒙特卡罗策略梯度算法和在线(时间差分)演员-评论家算法的变体。分析结果表明，指数标准的使用能够推广常用的特定正则化方法。作者在摆动杆和摆摆杆任务上进行了测试，验证了所提出的算法的实现性能和稳健性。

    While risk-neutral reinforcement learning has shown experimental success in a number of applications, it is well-known to be non-robust with respect to noise and perturbations in the parameters of the system. For this reason, risk-sensitive reinforcement learning algorithms have been studied to introduce robustness and sample efficiency, and lead to better real-life performance. In this work, we introduce new model-free risk-sensitive reinforcement learning algorithms as variations of widely-used Policy Gradient algorithms with similar implementation properties. In particular, we study the effect of exponential criteria on the risk-sensitivity of the policy of a reinforcement learning agent, and develop variants of the Monte Carlo Policy Gradient algorithm and the online (temporal-difference) Actor-Critic algorithm. Analytical results showcase that the use of exponential criteria generalize commonly used ad-hoc regularization approaches. The implementation, performance, and robustness 
    
[^122]: LidarCLIP：如何与点云交互

    LidarCLIP or: How I Learned to Talk to Point Clouds. (arXiv:2212.06858v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.06858](http://arxiv.org/abs/2212.06858)

    LidarCLIP可以将文本和激光雷达数据联系起来，达到有效的检索效果，而且能够在不良传感器条件下实现对具有挑战性的检测场景的有针对性搜索。

    

    最近，将文本和图像联系起来的研究取得了几项重大突破，例如 CLIP、DALL-E 2 和 Stable Diffusion 等模型。然而，文本与其他视觉模态(如激光雷达数据)之间的联系却没有得到足够的关注，这是由于缺少文本-激光雷达数据集。在这项工作中，我们提出了 LidarCLIP，它将汽车点云映射到预先存在的 CLIP 嵌入空间中。使用图像-点云对，我们使用图像 CLIP 嵌入监督点云编码器，有效地将文本和激光雷达数据与图像域作为中介联系起来。我们展示了 LidarCLIP 的有效性，通过演示基于激光雷达的检索一般与基于图像的检索相当，但具有互补的优点和缺点。通过结合图像和激光雷达特征，我们改进了单模态方法，实现了针对在不良传感器条件下具有挑战性的检测场景的有针对性搜索。我们还探讨了零样本分类，并展示了...

    Research connecting text and images has recently seen several breakthroughs, with models like CLIP, DALL-E 2, and Stable Diffusion. However, the connection between text and other visual modalities, such as lidar data, has received less attention, prohibited by the lack of text-lidar datasets. In this work, we propose LidarCLIP, a mapping from automotive point clouds to a pre-existing CLIP embedding space. Using image-lidar pairs, we supervise a point cloud encoder with the image CLIP embeddings, effectively relating text and lidar data with the image domain as an intermediary. We show the effectiveness of LidarCLIP by demonstrating that lidar-based retrieval is generally on par with image-based retrieval, but with complementary strengths and weaknesses. By combining image and lidar features, we improve upon both single-modality methods and enable a targeted search for challenging detection scenarios under adverse sensor conditions. We also explore zero-shot classification and show that
    
[^123]: DeepGOPlus 推理的数值稳定性研究

    Numerical Stability of DeepGOPlus Inference. (arXiv:2212.06361v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06361](http://arxiv.org/abs/2212.06361)

    这篇论文研究了用于预测蛋白质功能的 CNN DeepGOPlus 在推理过程中的数值不确定性和数值稳定性，并研究了使用降低精度浮点格式进行推理的可能性。

    

    卷积神经网络 (CNNs) 是目前最广泛使用的神经网络之一，在许多问题上都取得了最先进的性能。虽然最初应用于计算机视觉任务，但 CNNs 与具有空间关系的任何数据都能很好地配合使用，并已应用于不同领域。然而，最近的研究强调了 CNNs，与其他深度学习模型一样，对噪声注入的敏感性可能会危及其性能。本文量化了 DeepGOPlus 的浮点精度不确定性以确定其数值稳定性，DeepGOPlus 是一种用于预测蛋白质功能的 CNN。此外，本文研究了使用降低精度浮点格式进行 DeepGOPlus 推理以减少内存消耗和延迟的可能性。这是通过 Monte Carlo Arithmetic 实现的，该技术实验性地量化了浮点运算错误和 VPR。

    Convolutional neural networks (CNNs) are currently among the most widely-used neural networks available and achieve state-of-the-art performance for many problems. While originally applied to computer vision tasks, CNNs work well with any data with a spatial relationship, besides images, and have been applied to different fields. However, recent works have highlighted how CNNs, like other deep learning models, are sensitive to noise injection which can jeopardise their performance. This paper quantifies the numerical uncertainty of the floating point arithmetic inaccuracies of the inference stage of DeepGOPlus, a CNN that predicts protein function, in order to determine its numerical stability. In addition, this paper investigates the possibility to use reduced-precision floating point formats for DeepGOPlus inference to reduce memory consumption and latency. This is achieved with Monte Carlo Arithmetic, a technique that experimentally quantifies floating point operation errors and VPR
    
[^124]: 混合局部模式技术报告

    Technical Report of Mixing Local Patterns. (arXiv:2212.03654v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.03654](http://arxiv.org/abs/2212.03654)

    本文旨在解决GNN在处理非同质图数据时性能不佳的问题，提出混合局部结构模式的概念，并从局部模式的随机性和近邻可聚合性两个方面深入研究，以实现更通用的GNN。

    

    图神经网络（GNN）在同质图数据上表现出卓越的性能，但在处理非同质图数据时却远远不如同质图数据，这是由于GNN的固有低通滤波特性所致。在面对分析具有不同同质性属性的复杂现实世界图表时，不应忽略图中潜在的混合局部结构模式。因此，必须充分考虑上述两个问题，即（\textbf{Q1}）和（\textbf{Q2}），以实现更通用的GNN。为此，我们尝试从两个方面深入了解它们，分别是\textbf{（A1）：局部模式的随机性}和\textbf{（A2）：近邻可聚合性}。

    Graph neural networks (GNNs) have shown remarkable performance on homophilic graph data while being far less impressive when handling non-homophilic graph data due to the inherent low-pass filtering property of GNNs. In the face of analyzing complex real-world graphs with different homophily properties, the latent mixed local structural patterns in graphs should not be neglected. Therefore, the two questions, i.e., (\textbf{Q1}) and (\textbf{Q2}) as motioned above, should be well considered on the way to implementing a more generic GNN. For this purpose, we attempt to get deeper insights into them from two points, respectively, \textbf{(A1): Randomness of local patterns}, and \textbf{(A2): Aggregability of near-neighbors}.
    
[^125]: 带每个样本自适应裁剪的差分隐私学习

    Differentially Private Learning with Per-Sample Adaptive Clipping. (arXiv:2212.00328v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00328](http://arxiv.org/abs/2212.00328)

    本文提出了一种差分隐私每个样本自适应裁剪算法DP-PSAC，该算法采用非单调自适应权重函数，根据梯度的历史敏感性自适应裁剪每个样本的梯度幅度，提高了模型的实用性，且保证了隐私。

    

    近年来，AI中的隐私问题一直是吸引研究者和公众关注的话题。差分隐私学习作为一种实现隐私保护AI的方式，可以使AI模型使用差分隐私。现有算法通常通过常量裁剪来限制梯度幅度以实现学习过程的差分隐私。为了解决裁剪常数对模型性能的巨大影响，最新的作品NSGD和Auto-S提出了使用归一化来替代裁剪的创新方法。然而，NSGD和Auto-S等基于归一化的方法依赖于单调权重函数，对小梯度样本施加过量权重并引入额外偏差。本文提出了一种基于非单调自适应权重函数的差分隐私每个样本自适应裁剪（DP-PSAC）算法，它保证了隐私而不牺牲模型性能。我们的DP-PSAC算法根据该样本的历史敏感性自适应裁剪每个样本的梯度幅度，可以显著减少裁剪梯度的方差并提高模型的效用。实验结果表明，与现有方法相比，我们的DP-PSAC算法在真实数据集上实现了竞争性的实用性。

    Privacy in AI remains a topic that draws attention from researchers and the general public in recent years. As one way to implement privacy-preserving AI, differentially private learning is a framework that enables AI models to use differential privacy (DP). To achieve DP in the learning process, existing algorithms typically limit the magnitude of gradients with a constant clipping, which requires carefully tuned due to its significant impact on model performance. As a solution to this issue, latest works NSGD and Auto-S innovatively propose to use normalization instead of clipping to avoid hyperparameter tuning. However, normalization-based approaches like NSGD and Auto-S rely on a monotonic weight function, which imposes excessive weight on small gradient samples and introduces extra deviation to the update. In this paper, we propose a Differentially Private Per-Sample Adaptive Clipping (DP-PSAC) algorithm based on a non-monotonic adaptive weight function, which guarantees privacy w
    
[^126]: 用符号回归实现可解释的科学发现：综述（arXiv:2211.10873v2 [cs.LG] UPDATED）

    Interpretable Scientific Discovery with Symbolic Regression: A Review. (arXiv:2211.10873v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.10873](http://arxiv.org/abs/2211.10873)

    符号回归作为一种机器学习方法日益流行，可以从数据中学习简洁且可解释的数学表达式，本综述全面概述了该方法并讨论了其优点和局限性。

    

    符号回归作为从数据中学习简洁且可解释的数学表达式的一种机器学习方法日益流行。虽然传统上采用遗传编程的方式解决这个问题，但近年来，它在深度学习中引起了越来越多的关注，作为一种数据驱动的模型发现方法，在从基础科学到应用科学的各个领域取得了重大进展。本文综述了符号回归方法的结构和全面的概述，并讨论了它们的优点和局限性。

    Symbolic regression is emerging as a promising machine learning method for learning succinct underlying interpretable mathematical expressions directly from data. Whereas it has been traditionally tackled with genetic programming, it has recently gained a growing interest in deep learning as a data-driven model discovery method, achieving significant advances in various application domains ranging from fundamental to applied sciences. This survey presents a structured and comprehensive overview of symbolic regression methods and discusses their strengths and limitations.
    
[^127]: RecD：为端到端深度学习推荐模型训练基础设施提供去重功能

    RecD: Deduplication for End-to-End Deep Learning Recommendation Model Training Infrastructure. (arXiv:2211.05239v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05239](http://arxiv.org/abs/2211.05239)

    RecD 是一种为 DLRM 训练提供去重功能的端到端基础设施优化，解决了由于特征重复造成的海量存储、预处理和训练开销，引入了新的张量格式 InverseKeyedJaggedTensors (IKJTs) 来去除特征值的重复，使 DLRM 模型架构能够更好地利用数据的重复性提高训练吞吐量。

    

    我们提出了 RecD（推荐去重），它是一组针对深度学习推荐模型 (DLRM) 训练流程的端到端基础设施优化。RecD解决了由于特征重复造成的海量存储、预处理和训练开销，这是大规模 DLRM 训练数据集内在的问题，因为 DLRM 数据集是从交互中生成的。我们展示了 RecD 如何利用此属性来优化生产数据的流程，减少数据集存储和预处理需求，并最大限度地在训练批次中重复。RecD 引入了一种新的张量格式 InverseKeyedJaggedTensors (IKJTs)，以在每个批次中去除特征值的重复。我们展示了 DLRM 模型架构如何利用 IKJTs 来显著提高训练吞吐量。

    We present RecD (Recommendation Deduplication), a suite of end-to-end infrastructure optimizations across the Deep Learning Recommendation Model (DLRM) training pipeline. RecD addresses immense storage, preprocessing, and training overheads caused by feature duplication inherent in industry-scale DLRM training datasets. Feature duplication arises because DLRM datasets are generated from interactions. While each user session can generate multiple training samples, many features' values do not change across these samples. We demonstrate how RecD exploits this property, end-to-end, across a deployed training pipeline. RecD optimizes data generation pipelines to decrease dataset storage and preprocessing resource demands and to maximize duplication within a training batch. RecD introduces a new tensor format, InverseKeyedJaggedTensors (IKJTs), to deduplicate feature values in each batch. We show how DLRM model architectures can leverage IKJTs to drastically increase training throughput. Re
    
[^128]: 基于Web的视觉语料库构建用于视觉文档理解

    On Web-based Visual Corpus Construction for Visual Document Understanding. (arXiv:2211.03256v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.03256](http://arxiv.org/abs/2211.03256)

    这篇论文提出了一个基于Web的视觉语料库构建工具（Webvicob），用于从Wikipedia HTML转储文件中构建大规模的、多语言的视觉语料库，通过数据集的使用可以提升视觉文档理解模型的性能。

    

    近年来，关于视觉文档理解（VDU）的研究取得了显著的进展，特别是在自监督学习方法的发展方面。然而，在这个领域面临的一个重要挑战是可公开获取的视觉语料库的有限性，尤其是对于非拉丁语言或资源短缺的语言而言，大规模的图像集合与详细文本注释难以得到。为了解决这一挑战，我们提出了基于Web的视觉语料库构建工具（Webvicob），该工具能够从原始的Wikipedia HTML转储文件中构建大规模的多语言视觉语料库。我们的实验表明，Webvicob生成的数据可用于训练稳健的VDU模型，并在各种下游任务（例如DocVQA和后OCR解析）中表现良好。此外，当使用由Webvicob生成的100万张图像数据集时，我们观察到在DocVQA任务3上相对于COCO-Text数据集的1100万张图像数据集，有超过13％的提升。

    In recent years, research on visual document understanding (VDU) has grown significantly, with a particular emphasis on the development of self-supervised learning methods. However, one of the significant challenges faced in this field is the limited availability of publicly accessible visual corpora or extensive collections of images with detailed text annotations, particularly for non-Latin or resource-scarce languages. To address this challenge, we propose Web-based Visual Corpus Builder (Webvicob), a dataset generator engine capable of constructing large-scale, multilingual visual corpora from raw Wikipedia HTML dumps. Our experiments demonstrate that the data generated by Webvicob can be used to train robust VDU models that perform well on various downstream tasks, such as DocVQA and post-OCR parsing. Furthermore, when using a dataset of 1 million images generated by Webvicob, we observed an improvement of over 13% on the DocVQA Task 3 compared to a dataset of 11 million images fr
    
[^129]: Going In Style：通过风格转换进行音频后门攻击

    Going In Style: Audio Backdoors Through Stylistic Transformations. (arXiv:2211.03117v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.03117](http://arxiv.org/abs/2211.03117)

    本文探讨了音频领域中通过吉他效果动态转换恶意样本来进行后门攻击的风格触发器。JingleBack 的提出使得风格触发器在音频领域得以应用，并取得了极高的攻击成功率。

    

    本文探讨了音频领域中通过吉他效果动态转换恶意样本来进行后门攻击的风格触发器。首先，我们规范化了风格触发器——这是当前文献中缺失的。其次，我们探讨了如何通过提出 JingleBack 来在音频领域开发风格触发器。我们的实验证实了攻击的有效性，达到了 96% 的攻击成功率。我们的代码可在 https://github.com/skoffas/going-in-style 中找到。

    This work explores stylistic triggers for backdoor attacks in the audio domain: dynamic transformations of malicious samples through guitar effects. We first formalize stylistic triggers - currently missing in the literature. Second, we explore how to develop stylistic triggers in the audio domain by proposing JingleBack. Our experiments confirm the effectiveness of the attack, achieving a 96% attack success rate. Our code is available in https://github.com/skoffas/going-in-style.
    
[^130]: 训练神经网络用于时序变点检测

    Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17312](http://arxiv.org/abs/2210.17312)

    本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。

    

    检测数据流中的突变分布转换，即所谓的变点检测，是统计学和机器学习中的一个基本问题。我们引入了一种新颖的方法，使用神经网络进行在线变点检测。具体而言，我们的方法是训练神经网络来逐步计算检测统计量的累积和，当发生变点时，该量会显著变化。我们使用合成和真实世界数据证明了所提出的方法在检测变点方面的优越性和潜力。

    Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
    
[^131]: 论多动作策略梯度

    On Many-Actions Policy Gradient. (arXiv:2210.13011v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13011](http://arxiv.org/abs/2210.13011)

    本论文研究了具有多个动作样本的随机策略梯度的方差问题，并提出了一种基于动态模型的多动作采样方法，实现了更高的样本效率和更高的回报。

    

    我们研究了具有多个动作样本的随机策略梯度（SPG）的方差。我们得出了一个多动作最优性条件，它决定了当与比例扩展轨迹的单动作代理相比，多动作SPG产生比较低的方差。我们提出了一种称为基于模型的多动作（MBMA）的方法，在SPG背景下利用动态模型进行多动作采样，以解决现有多动作SPG实现所涉及的问题，并在模型模拟的回合中提供与SPG相当的偏差和方差。我们发现，MBMA的偏差和方差结构与理论预测的相匹配。因此，在一系列连续动作环境中，MBMA与无模型，多动作和基于模型的策略梯度基线相比，实现了更高的样本效率和更高的回报。

    We study the variance of stochastic policy gradients (SPGs) with many action samples per state. We derive a many-actions optimality condition, which determines when many-actions SPG yields lower variance as compared to a single-action agent with proportionally extended trajectory. We propose Model-Based Many-Actions (MBMA), an approach leveraging dynamics models for many-actions sampling in the context of SPG. MBMA addresses issues associated with existing implementations of many-actions SPG and yields lower bias and comparable variance to SPG estimated from states in model-simulated rollouts. We find that MBMA bias and variance structure matches that predicted by theory. As a result, MBMA achieves improved sample efficiency and higher returns on a range of continuous action environments as compared to model-free, many-actions, and model-based on-policy SPG baselines.
    
[^132]: Transformers学会了自动机的快捷方式

    Transformers Learn Shortcuts to Automata. (arXiv:2210.10749v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10749](http://arxiv.org/abs/2210.10749)

    Transformer模型通过重新参数化其循环动态，可以使用比推理步骤更少的层数执行任何有限状态自动机的计算。多项式大小的 $O(\log T)$ 深度解决方案始终存在，而且$O(1)$深度模拟器是非常普遍的。

    

    算法推理需要计算模型的循环能力，如图灵机等。然而，Transformer模型虽然缺乏循环能力，但能够使用比推理步骤更少的层数执行此类推理。这引发了一个问题：这些浅层次和非循环模型学到了什么解决方案？我们发现，低深度Transformer可以通过逐层重新参数化其循环动态，表示任何有限状态自动机（因此，任何有界内存算法）的计算。我们的理论结果表征了快捷解决方案，其中具有 $o(T)$ 层的Transformer可以精确复制自动机在长度为 $T$ 的输入序列上的计算。我们发现，多项式大小的 $O(\log T)$ 深度解决方案始终存在；此外，$O(1)$ 深度模拟器非常普遍，可以使用从 Krohn-Rhodes 理论和电路复杂度理论中的工具来理解。实证

    Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with $o(T)$ layers can exactly replicate the computation of an automaton on an input sequence of length $T$. We find that polynomial-sized $O(\log T)$-depth solutions always exist; furthermore, $O(1)$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirical
    
[^133]: 混合数据的条件特征重要性

    Conditional Feature Importance for Mixed Data. (arXiv:2210.03047v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.03047](http://arxiv.org/abs/2210.03047)

    本研究提出了一种针对混合数据的条件特征重要性框架，使用条件预测影响和顺序knockoff抽样结合，以解决很少讨论的条件和边缘度量之间的重要区别，并揭示出为测试条件FI，目前只有少数方法可用且过去从业者由于数据要求不匹配而受到严重限制。

    

    尽管特征重要性（FI）在可解释的机器学习中很受欢迎，但这些方法的统计充分性很少被讨论。从统计角度看，一个主要区别是在调整协变量之前和之后分析变量的重要性，即“边缘”和“条件”度量之间。我们的工作引起了这种很少被承认但至关重要的区别的注意，并展示了其影响。此外，我们揭示了测试条件FI时只有少数方法可用，而从业者过去由于数据要求不匹配而受到严重限制。大多数现实世界的数据都表现出复杂的特征依赖性，并包含连续和分类数据（混合数据）。这些属性通常被条件FI度量所忽略。为了填补这一空白，我们提出将条件预测影响（CPI）框架与顺序knockoff抽样相结合。

    Despite the popularity of feature importance (FI) measures in interpretable machine learning, the statistical adequacy of these methods is rarely discussed. From a statistical perspective, a major distinction is between analyzing a variable's importance before and after adjusting for covariates i.e., between $\textit{marginal}$ and $\textit{conditional}$ measures. Our work draws attention to this rarely acknowledged, yet crucial distinction and showcases its implications. Further, we reveal that for testing conditional FI, only few methods are available and practitioners have hitherto been severely restricted in method application due to mismatching data requirements. Most real-world data exhibits complex feature dependencies and incorporates both continuous and categorical data (mixed data). Both properties are oftentimes neglected by conditional FI measures. To fill this gap, we propose to combine the conditional predictive impact (CPI) framework with sequential knockoff sampling. 
    
[^134]: 基于子图草图的图神经网络用于链路预测

    Graph Neural Networks for Link Prediction with Subgraph Sketching. (arXiv:2209.15486v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15486](http://arxiv.org/abs/2209.15486)

    本研究提出了一种名为ELPH的全图GNN模型，它使用子图草图作为消息传递，以缓解LP任务中子图之间的冗余问题，并在多个基准数据集上取得了最先进的结果。

    

    许多图神经网络在链路预测任务中表现不佳，这是由于表达能力的限制，例如无法计算三角形（大多数LP启发式算法的骨干），以及不能区分同构节点（具有相同结构角色的节点）。这两种表达问题可以通过学习链路（而不是节点）表示，并结合三角形计数等结构特征来缓解。由于显式链路表示通常代价高昂，因此最近的研究采用了基于子图的方法，这些方法在LP方面取得了最先进的性能，但由于子图之间存在高度冗余，效率较低。我们分析了子图GNN（SGNN）方法的组件，基于我们的分析，提出了一种名为ELPH（高效哈希链路预测）的新型全图GNN，将子图草图作为消息传递以近似全图上的转换。ELPH在多个基准数据集上实现了最先进的结果，同时需要比现有基于子图的方法更少的计算资源。

    Many Graph Neural Networks (GNNs) perform poorly compared to simple heuristics on Link Prediction (LP) tasks. This is due to limitations in expressive power such as the inability to count triangles (the backbone of most LP heuristics) and because they can not distinguish automorphic nodes (those having identical structural roles). Both expressiveness issues can be alleviated by learning link (rather than node) representations and incorporating structural features such as triangle counts. Since explicit link representations are often prohibitively expensive, recent works resorted to subgraph-based methods, which have achieved state-of-the-art performance for LP, but suffer from poor efficiency due to high levels of redundancy between subgraphs. We analyze the components of subgraph GNN (SGNN) methods for link prediction. Based on our analysis, we propose a novel full-graph GNN called ELPH (Efficient Link Prediction with Hashing) that passes subgraph sketches as messages to approximate t
    
[^135]: 极其简单的激活形状法用于识别离群点

    Extremely Simple Activation Shaping for Out-of-Distribution Detection. (arXiv:2209.09858v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.09858](http://arxiv.org/abs/2209.09858)

    该论文提出了一种极其简单的后处理激活形状方法ASH，用于离群点检测，该方法不需要额外的训练步骤、额外的数据或对训练网络进行非常规修改，就能增强模型对未知情况的处理能力。

    

    机器学习模型在训练和部署之间的区别意味着无法预测部署中可能遇到的所有情况，因此仅依靠训练的进步具有其局限性。本文提出了一种极其简单的后处理激活形状方法ASH，可以应用于模型训练后的网络，用于离群点检测。该方法在推理时应用，不需要从训练数据中计算任何统计数据。实验表明，这种简单的处理可以增强正常数据和异常数据的区分能力。

    The separation between training and deployment of machine learning models implies that not all scenarios encountered in deployment can be anticipated during training, and therefore relying solely on advancements in training has its limits. Out-of-distribution (OOD) detection is an important area that stress-tests a model's ability to handle unseen situations: Do models know when they don't know? Existing OOD detection methods either incur extra training steps, additional data or make nontrivial modifications to the trained network. In contrast, in this work, we propose an extremely simple, post-hoc, on-the-fly activation shaping method, ASH, where a large portion (e.g. 90%) of a sample's activation at a late layer is removed, and the rest (e.g. 10%) simplified or lightly adjusted. The shaping is applied at inference time, and does not require any statistics calculated from training data. Experiments show that such a simple treatment enhances in-distribution and out-of-distribution dist
    
[^136]: 深度学习性能数据为何误导人

    Why Deep Learning's Performance Data Are Misleading. (arXiv:2208.11228v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11228](http://arxiv.org/abs/2208.11228)

    本文解释了“深度学习”中的两种误导性行为“数据删除”和“在训练集上测试”，并建立了一个定理，即NNWT方法可以使用这些不当行为在任何验证集和测试集上达到零误差，但是这些方法并不具有可泛化性。

    

    这篇论文是AIEE 2023年同名会议的主题演讲的附带论文，属于一篇理论性文章。文章解释了AI项目中所谓的“深度学习”给出了印象深刻的性能表现，但这些表现数据实际上被错误地夸大了。文章阐明了“数据删除”和“在训练集上测试”的误导性，并定义了一种简单的分类方法“最近邻加阈值”（NNWT）。文章建立了一个定理，即在具有有限但无限的存储空间和训练时间（与许多深度学习方法类似）的情况下，NNWT方法使用这两种不当行为可以在任何验证集和任何测试集上达到零误差，但是许多深度学习方法，包括NNWT方法在内，并不具有可泛化性。

    This is a theoretical paper, as a companion paper of the keynote talk at the same conference AIEE 2023. In contrast to conscious learning, many projects in AI have employed so-called "deep learning" many of which seemed to give impressive performance. This paper explains that such performance data are deceptively inflated due to two misconducts: "data deletion" and "test on training set". This paper clarifies "data deletion" and "test on training set" in deep learning and why they are misconducts. A simple classification method is defined, called Nearest Neighbor With Threshold (NNWT). A theorem is established that the NNWT method reaches a zero error on any validation set and any test set using the two misconducts, as long as the test set is in the possession of the author and both the amount of storage space and the time of training are finite but unbounded like with many deep learning methods. However, many deep learning methods, like the NNWT method, are all not generalizable since
    
[^137]: 基于物理学的领域自适应框架用于建筑能量系统建模和预测

    A physics-based domain adaptation framework for modelling and forecasting building energy systems. (arXiv:2208.09456v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09456](http://arxiv.org/abs/2208.09456)

    本文提出了一种基于物理学的领域自适应框架，将线性时不变状态空间模型与基于子空间的无监督降阶建模相结合，通过最小化模型在共享子空间中的表示差异，将在一个领域上训练过的模型适应到另一个领域，用于建筑能量系统的建模和预测，并在实验中表现出优异的预测性能。

    

    现代的基于机器学习的模型在建筑能源行为建模和预测方面已经成为一个流行的选择。然而，它们的结构通常并不具有与物理现象相关的机械结构相对应。因此，它们成功地泛化为未观测到的时间步取决于所观察到的系统动态在数据中表现的代表性，在数字孪生控制和能源管理等真实世界的工程问题中很难得到保证。为了解决这一问题，本文提出了一个框架，将线性时不变（LTI）状态空间模型（SSM）的成批参数模型与基于子空间的无监督降阶建模相结合，形成了一个子空间导向的领域适应（SDA）框架。SDA是一种转移学习方法，旨在通过最小化共享子空间中的模型表示的差异来将在一个领域上训练过的模型适应到另一个领域。在一个建筑能量系统案例研究中展示了我们提出的基于物理学的领域自适应框架，证明了它在短期和长期能量预测方面优于现有的基于机器学习的模型。

    State-of-the-art machine-learning-based models are a popular choice for modeling and forecasting energy behavior in buildings because given enough data, they are good at finding spatiotemporal patterns and structures even in scenarios where the complexity prohibits analytical descriptions. However, their architecture typically does not hold physical correspondence to mechanistic structures linked with governing physical phenomena. As a result, their ability to successfully generalize for unobserved timesteps depends on the representativeness of the dynamics underlying the observed system in the data, which is difficult to guarantee in real-world engineering problems such as control and energy management in digital twins. In response, we present a framework that combines lumped-parameter models in the form of linear time-invariant (LTI) state-space models (SSMs) with unsupervised reduced-order modeling in a subspace-based domain adaptation (SDA) framework. SDA is a type of transfer-lear
    
[^138]: 基于Boosting的离线策略学习算法

    Boosted Off-Policy Learning. (arXiv:2208.01148v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.01148](http://arxiv.org/abs/2208.01148)

    我们提出了一种基于Boosting的离线策略学习算法，将基础学习器简化为监督学习，获得了广泛的实际效益；实验结果表明其应用能力优于深度神经网络的离线策略学习和简单回归方法。

    

    我们提出了一种针对来自记录式赌博反馈的离线策略学习的Boosting算法。与现有的监督学习的Boosting方法不同，我们的算法直接优化了策略预期收益的估计。我们对该算法进行了分析，并证明如果基础学习器满足“弱”学习条件，那么每一轮Boosting都会减小过量经验风险（可能是指数级）。我们进一步展示了如何将基础学习器简化为监督学习，从而打开了广泛的基础学习器源，如决策树等，具有实际益处。实验结果表明，我们的算法继承了许多基于决策树的Boosting算法的优良性质（例如对特征缩放和超参数调整的鲁棒性），并且可以胜过基于深度神经网络的离线策略学习和只是回归观察到的奖励的方法。

    We propose the first boosting algorithm for off-policy learning from logged bandit feedback. Unlike existing boosting methods for supervised learning, our algorithm directly optimizes an estimate of the policy's expected reward. We analyze this algorithm and prove that the excess empirical risk decreases (possibly exponentially fast) with each round of boosting, provided a ''weak'' learning condition is satisfied by the base learner. We further show how to reduce the base learner to supervised learning, which opens up a broad range of readily available base learners with practical benefits, such as decision trees. Experiments indicate that our algorithm inherits many desirable properties of tree-based boosting algorithms (e.g., robustness to feature scaling and hyperparameter tuning), and that it can outperform off-policy learning with deep neural networks as well as methods that simply regress on the observed rewards.
    
[^139]: 用于复杂系统模拟的有效学习准确替代模型的方法

    Efficient Learning of Accurate Surrogates for Simulations of Complex Systems. (arXiv:2207.12855v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12855](http://arxiv.org/abs/2207.12855)

    本论文提出了一种在线学习方法，由优化器驱动的采样方法，该方法可以提高替代模型的预测能力，并构建了一个比目前使用的替代模型更准确的替代模型，为复杂系统模拟提供了更加计算上高效的替代模型方法。

    

    机器学习方法越来越被用于构建计算上廉价的复杂物理模型的替代模型。当数据不稳定、稀疏或时间相关时，这些替代模型的预测能力会下降。为了找到一种可以提供任何潜在未来模型评估的有效预测的替代模型，我们介绍了一种由优化器驱动的在线学习方法。该方法有两个优点，首先它确保在训练数据中包含模型响应曲面上的所有拐点。其次，在任何新的模型评估之后，如果“得分”下降到下限，则会进行机器学习模型的“重新训练”（更新）。在基准函数上的测试表明，与传统的采样方法相比，由优化器引导的采样通常在局部极值附近的准确性方面表现优越，即使评分指标支持整体准确性。我们将该方法应用于核物质的模拟中，以证明它处理嘈杂、稀疏和时间相关的数据的能力。我们建立的替代模型比同样数量的训练数据构建的目前使用的替代模型更准确。我们的在线方法提供了一种计算上高效的替代模型，而传统的替代模型则需要大量的训练数据。

    Machine learning methods are increasingly used to build computationally inexpensive surrogates for complex physical models. The predictive capability of these surrogates suffers when data are noisy, sparse, or time-dependent. As we are interested in finding a surrogate that provides valid predictions of any potential future model evaluations, we introduce an online learning method empowered by optimizer-driven sampling. The method has two advantages over current approaches. First, it ensures that all turning points on the model response surface are included in the training data. Second, after any new model evaluations, surrogates are tested and "retrained" (updated) if the "score" drops below a validity threshold. Tests on benchmark functions reveal that optimizer-directed sampling generally outperforms traditional sampling methods in terms of accuracy around local extrema, even when the scoring metric favors overall accuracy. We apply our method to simulations of nuclear matter to dem
    
[^140]: 采用阶段$L^2$正则化的神经Stein评价器

    Neural Stein critics with staged $L^2$-regularization. (arXiv:2207.03406v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2207.03406](http://arxiv.org/abs/2207.03406)

    本论文研究了使用$L^2$正则化训练神经网络Stein评价器的方法，通过阶段性的权重调整训练过程，可以在高维统计测试中实现对未知概率分布与名义模型分布的区分。

    

    学习区分模型分布和观测数据的分布是统计学和机器学习中的一个基本问题，高维数据对这些问题仍然是一个具有挑战性的场景。量化概率分布差异的度量，例如Stein差异，在高维统计测试中扮演着重要角色。本文研究了在训练神经网络Stein评价器时采用$L^2$正则化的作用，以区分从未知概率分布中采样的数据和名义模型分布。通过与神经切向核(NTK)理论的联系，我们开发了一种新的权重正则化的阶段过程，利用了在早期时期高度正则化训练的优势。从理论上证明，当$L^2$正则化权重较大时，训练动态的近似性质通过核优化即实现了懒惰(“lazy training”)的训练。

    Learning to differentiate model distributions from observed data is a fundamental problem in statistics and machine learning, and high-dimensional data remains a challenging setting for such problems. Metrics that quantify the disparity in probability distributions, such as the Stein discrepancy, play an important role in high-dimensional statistical testing. In this paper, we investigate the role of $L^2$ regularization in training a neural network Stein critic so as to distinguish between data sampled from an unknown probability distribution and a nominal model distribution. Making a connection to the Neural Tangent Kernel (NTK) theory, we develop a novel staging procedure for the weight of regularization over training time, which leverages the advantages of highly-regularized training at early times. Theoretically, we prove the approximation of the training dynamic by the kernel optimization, namely the ``lazy training'', when the $L^2$ regularization weight is large, and training o
    
[^141]: LogGENE: 一种用于深度医疗推理任务的平滑检查损失替代方法

    LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks. (arXiv:2206.09333v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09333](http://arxiv.org/abs/2206.09333)

    LogGENE采用分位数回归框架预测基因表达水平的完整条件分位数，从而为高通量基因组学提供了一种能提供解释和报告不确定性估计、鲁棒性强的推断方法。

    

    在可靠的深度学习中，挖掘大型数据集并从中获得校准的预测具有即时相关性和实用性。本研究开发了基于深度神经网络的推断方法，适用于基因表达等大型数据集。然而，与典型的深度学习方法不同的是，我们的推断技术在准确性方面实现了最先进的性能，同时还能提供解释和报告不确定性估计。我们采用分位数回归框架来预测给定一组基因表达的完整条件分位数。条件分位数除了有助于提供预测的丰富解释外，还能够抵抗测量噪声。我们的技术在高通量基因组学中特别重要，这是一个正在引领个性化医疗、靶向药物设计和传递的新时代。然而，用于驱动估计过程的检查损失，在分位数回归中并无不同之处。

    Mining large datasets and obtaining calibrated predictions from tem is of immediate relevance and utility in reliable deep learning. In our work, we develop methods for Deep neural networks based inferences in such datasets like the Gene Expression. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of housekeeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. Our technique is particularly consequential in High-throughput Genomics, an area which is ushering a new era in personalized health care, and targeted drug design and delivery. However, check loss, used in quantile regression to drive the estimation process is not diffe
    
[^142]: 了解你的听众：用听众减法专门化基于上下文的语言模型

    Know your audience: specializing grounded language models with listener subtraction. (arXiv:2206.08349v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.08349](http://arxiv.org/abs/2206.08349)

    本文介绍了一种利用多智能体图像参照游戏自适应不同听众的目标任务描述的方法，并通过微调 CLIP 视觉编码器和大型语言模型之间的适配器，在适应听众的语言上下文的情况下进行了自然语言专业化。

    

    有效的沟通需要适应每个交际情境的特殊性，比如与每个交互伙伴分享的共同语境。本研究通过借鉴对话游戏 Dixit 的思想设计了一个多智能体图像参照游戏，训练一个说话者模型来描述一个目标图像，使得一个听者能够在干扰项中正确地识别出目标图像，而另一个听者则不能。这要求说话者利用它与不同听者的共同知识差异进行适应。本研究还展示了在这种对比、多智能体的语境下微调 CLIP 视觉编码器和大型语言模型之间的注意力适配器会自然地产生上下文依赖的自然语言专业化，且只需要通过奖励而无需直接监督来实现。通过控制实验，本研究证明了用两个听者来训练说话者的有效性。

    Effective communication requires adapting to the idiosyncrasies of each communicative context--such as the common ground shared with each partner. Humans demonstrate this ability to specialize to their audience in many contexts, such as the popular game Dixit. We take inspiration from Dixit to formulate a multi-agent image reference game where a (trained) speaker model is rewarded for describing a target image such that one (pretrained) listener model can correctly identify it among distractors, but another listener cannot. To adapt, the speaker must exploit differences in the knowledge it shares with the different listeners. We show that finetuning an attention-based adapter between a CLIP vision encoder and a large language model in this contrastive, multi-agent setting gives rise to context-dependent natural language specialization from rewards only, without direct supervision. Through controlled experiments, we show that training a speaker with two listeners that perceive different
    
[^143]: 通过低保真模型和物理知识驱动高斯过程学习数字孪生之间的物理关系

    Learning Physics between Digital Twins with Low-Fidelity Models and Physics-Informed Gaussian Processes. (arXiv:2206.08201v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.08201](http://arxiv.org/abs/2206.08201)

    本文提出了一种新方法，通过低保真模型和物理知识驱动高斯过程进行数字孪生之间的学习， 并开发了贝叶斯分层建模框架允许多个数字孪生之间共享信息。

    

    数字孪生是一种代表个体的计算机模型，例如组件、患者或过程。 在许多情况下，我们想从数据中获取有关个体的知识，同时结合不完美的物理知识，并从其他个体的数据中学习。 本文介绍了一种全贝叶斯方法，用于在每个个体的物理参数引起兴趣的情况下学习数字孪生之间的关系。 在个性化模型的模型公式中引入了模型差异项，以解释低保真模型中缺失的物理现象。 为了允许个体之间共享信息，我们介绍了一个贝叶斯分层建模框架，其中通过新层将个体模型连接起来。 我们的方法在两个案例研究中进行了演示，一个是先前在文献中使用的玩具示例，扩展到更多个体的情况，一个是与治疗高血压相关的心血管模型。

    A digital twin is a computer model that represents an individual, for example, a component, a patient or a process. In many situations, we want to gain knowledge about an individual from its data while incorporating imperfect physical knowledge and also learn from data from other individuals. In this paper, we introduce a fully Bayesian methodology for learning between digital twins in a setting where the physical parameters of each individual are of interest. A model discrepancy term is incorporated in the model formulation of each personalized model to account for the missing physics of the low-fidelity model. To allow sharing of information between individuals, we introduce a Bayesian Hierarchical modelling framework where the individual models are connected through a new level in the hierarchy. Our methodology is demonstrated in two case studies, a toy example previously used in the literature extended to more individuals and a cardiovascular model relevant for the treatment of Hyp
    
[^144]: 自适应探索的Stein变分目标生成在多目标强化学习中的应用

    Stein Variational Goal Generation for adaptive Exploration in Multi-Goal Reinforcement Learning. (arXiv:2206.06719v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.06719](http://arxiv.org/abs/2206.06719)

    本文提出了一种自适应探索的Stein变分目标生成方法，通过粒子建模并引入适当难度区域，提高了多目标强化学习中目标达成的成功覆盖率，同时在环境变化时表现出有用的恢复特性。

    

    在多目标强化学习中，智能体可以在相关的训练任务之间共享经验，从而在测试时实现更好的泛化。但是，在目标空间具有不连续性且奖励稀疏的情况下，大多数目标难以达成。在这种情况下，目标的课程有助于智能体通过适应其当前能力来学习。本文提出了一种名为Stein Variational Goal Generation (SVGG)的方法，通过利用其目标到达能力的学习预测模型，为智能体采样中等难度的目标。目标分布采用粒子来建模，并利用Stein变分梯度下降将粒子吸引到适当难度的区域。我们展示SVGG在难度较高的探索问题中胜过最先进的多目标强化学习方法，并证明它在环境发生变化时具有有用的恢复特性。

    In multi-goal Reinforcement Learning, an agent can share experience between related training tasks, resulting in better generalization for new tasks at test time. However, when the goal space has discontinuities and the reward is sparse, a majority of goals are difficult to reach. In this context, a curriculum over goals helps agents learn by adapting training tasks to their current capabilities. In this work we propose Stein Variational Goal Generation (SVGG), which samples goals of intermediate difficulty for the agent, by leveraging a learned predictive model of its goal reaching capabilities. The distribution of goals is modeled with particles that are attracted in areas of appropriate difficulty using Stein Variational Gradient Descent. We show that SVGG outperforms state-of-the-art multi-goal Reinforcement Learning methods in terms of success coverage in hard exploration problems, and demonstrate that it is endowed with a useful recovery property when the environment changes.
    
[^145]: 基于图结构的世界模型：离线强化学习中的价值记忆图

    Value Memory Graph: A Graph-Structured World Model for Offline Reinforcement Learning. (arXiv:2206.04384v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04384](http://arxiv.org/abs/2206.04384)

    本论文提出了一种离线强化学习的方法，通过构建简单离散的世界模型（Value Memory Graph，VMG）来抽象原始复杂环境，从而简化策略学习。

    

    强化学习（RL）方法通常直接应用于环境来学习策略。在一些具有连续状态-动作空间、稀疏奖励和/或长时间间隔的复杂环境中，在原始环境中学习一个好的策略是困难的。在离线RL设置中，我们旨在构建一个简单且离散的世界模型，以抽象出原始环境。RL方法应用于我们的世界模型而非环境数据进行简化的策略学习。我们的世界模型称为价值记忆图（VMG），它被设计为基于有向图的马尔可夫决策过程（MDP），其中顶点和有向边分别代表图状态和图动作。由于VMG的状态空间和动作空间相对于原始环境而言是有限且相对较小的，因此我们可以直接在VMG上应用价值迭代算法来估算图状态值并确定最佳的图动作。VMG是从离线数据中训练和构建的。

    Reinforcement Learning (RL) methods are typically applied directly in environments to learn policies. In some complex environments with continuous state-action spaces, sparse rewards, and/or long temporal horizons, learning a good policy in the original environments can be difficult. Focusing on the offline RL setting, we aim to build a simple and discrete world model that abstracts the original environment. RL methods are applied to our world model instead of the environment data for simplified policy learning. Our world model, dubbed Value Memory Graph (VMG), is designed as a directed-graph-based Markov decision process (MDP) of which vertices and directed edges represent graph states and graph actions, separately. As state-action spaces of VMG are finite and relatively small compared to the original environment, we can directly apply the value iteration algorithm on VMG to estimate graph state values and figure out the best graph actions. VMG is trained from and built on the offline
    
[^146]: 在不稳健样本上施加更多正则化以提高对抗性鲁棒性

    Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03353](http://arxiv.org/abs/2206.03353)

    本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。

    

    对抗性训练是提高对抗攻击鲁棒性的一种方法，在人类视觉无法察觉的数据扰动下，使给定的深度神经网络产生误判。本文提出了一种新的对抗训练算法，它在理论上得到很好的证明，并且在实践中表现优于其他现有的算法。该算法的一个新的特点是：对于容易受到对抗攻击的数据，比其他现有的正则化算法更多地应用正则化。理论上，我们证明了我们的算法可以被理解为一个最小化经验风险的正则化算法，它来自一个新的鲁棒风险上界的动机。数值实验表明，我们提出的算法同时提高了泛化性能(在例子上的准确性)和鲁棒性(在对抗攻击上的准确性)，达到了最先进的性能水平。

    Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
    
[^147]: 分布公正作为公平机器学习的基础前提：团体公平指标的统一、拓展和解释

    Distributive Justice as the Foundational Premise of Fair ML: Unification, Extension, and Interpretation of Group Fairness Metrics. (arXiv:2206.02897v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2206.02897](http://arxiv.org/abs/2206.02897)

    本文提出了一个全面的团体公平指标框架，将其与更多的分配公正理论联系起来，揭示了标准团体公平指标所涉及的规范选择并允许解释道德实质。

    

    团体公平指标是评估基于预测的决策系统公平性的一种已经确立的方法。然而，这些指标仍然不足以与哲学理论联系起来，并且它们的道德含义经常不明确。在本文中，我们提出了一个全面的团体公平指标框架，将它们与更多的分配公正理论联系起来。不同的团体公平指标在衡量决策对受影响人员的利益或伤害以及所假设的道德权利方面有所不同。我们的统一框架揭示了标准团体公平指标所涉及的规范选择，并允许解释它们的道德实质。此外，这个更广泛的视角为我们在文献中找到的标准公平指标的扩展提供了一个结构。这个扩展允许解决标准团体公平指标的若干批评，特别是它们是基于平等的，即它们...

    Group fairness metrics are an established way of assessing the fairness of prediction-based decision-making systems. However, these metrics are still insufficiently linked to philosophical theories, and their moral meaning is often unclear. In this paper, we propose a comprehensive framework for group fairness metrics, which links them to more theories of distributive justice. The different group fairness metrics differ in their choices about how to measure the benefit or harm of a decision for the affected individuals, and what moral claims to benefits are assumed. Our unifying framework reveals the normative choices associated with standard group fairness metrics and allows an interpretation of their moral substance. In addition, this broader view provides a structure for the expansion of standard fairness metrics that we find in the literature. This expansion allows addressing several criticisms of standard group fairness metrics, specifically: (1) they are parity-based, i.e., they 
    
[^148]: 基于公正的算法权衡-效用交易分析框架

    A Justice-Based Framework for the Analysis of Algorithmic Fairness-Utility Trade-Offs. (arXiv:2206.02891v3 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2206.02891](http://arxiv.org/abs/2206.02891)

    本论文提出了一个公正与效用并重的算法决策框架，能够平衡决策制定者和决策对象的观点，其中公正性评估基于公正的分配理论和算法公正的文献研究，效用评估基于基数效用函数。

    

    在基于预测的决策系统中，不同的观点常常存在矛盾：决策制定者的短期商业目标与决策对象希望得到公正待遇的愿望往往相冲突。平衡这两种观点是一个关乎价值观的问题。但是，这些价值观常常隐藏在决策系统实施的技术细节中。本文提出了一个框架，使这些价值含义明确可见。我们专注于一个我们想找到能够平衡决策制定者和决策对象观点的决策规则的设置。我们提供了一个方法来形式化和评估决策制定者和决策对象的效用和公正性。对于公正性评估，我们基于公正的分配理论和算法公正的文献研究。对于效用评估，我们提出了一种基于基数效用函数的方法。我们通过将其应用于机器学习中公平性文献研究中的一个著名例子来说明我们的框架。

    In prediction-based decision-making systems, different perspectives can be at odds: The short-term business goals of the decision makers are often in conflict with the decision subjects' wish to be treated fairly. Balancing these two perspectives is a question of values. However, these values are often hidden in the technicalities of the implementation of the decision-making system. In this paper, we propose a framework to make these value-laden choices clearly visible. We focus on a setting in which we want to find decision rules that balance the perspective of the decision maker and of the decision subjects. We provide an approach to formalize both perspectives, i.e., to assess the utility of the decision maker and the fairness towards the decision subjects. In both cases, the idea is to elicit values from decision makers and decision subjects that are then turned into something measurable. For the fairness evaluation, we build on well-known theories of distributive justice and on th
    
[^149]: 基于单调神经网络的最小基数解释方法

    Cardinality-Minimal Explanations for Monotonic Neural Networks. (arXiv:2205.09901v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09901](http://arxiv.org/abs/2205.09901)

    本文研究基于单调神经网络的解释方法，假设激活函数连续可导，则可以通过贪心算法在多项式时间内寻找到最小输入特征集合，解决难以处理的决策问题。

    

    近年来，对于神经网络模型预测的解释方法越来越受到关注，其中最为准确且形式化的方法包括寻找对于预测结果至关重要的最小输入特征集合，以及改变预测结果的最小输入特征集合。但这些相应的决策问题在计算上是难以处理的。本文研究了单调函数的神经模型在此方面的适用性，发现如果假设激活函数在各处均连续且几乎无处不可导，则可以通过贪心算法在多项式时间内解决相关的难题。我们通过实验验证了该算法的性能。

    In recent years, there has been increasing interest in explanation methods for neural model predictions that offer precise formal guarantees. These include abductive (respectively, contrastive) methods, which aim to compute minimal subsets of input features that are sufficient for a given prediction to hold (respectively, to change a given prediction). The corresponding decision problems are, however, known to be intractable. In this paper, we investigate whether tractability can be regained by focusing on neural models implementing a monotonic function. Although the relevant decision problems remain intractable, we can show that they become solvable in polynomial time by means of greedy algorithms if we additionally assume that the activation functions are continuous everywhere and differentiable almost everywhere. Our experiments suggest favourable performance of our algorithms.
    
[^150]: 广义拉格朗日编码计算：用于具有鲁棒性、安全性和隐私保护的计算通信折衷的灵活解决方案

    Generalized Lagrange Coded Computing: A Flexible Computation-Communication Tradeoff for Resilient, Secure, and Private Computation. (arXiv:2204.11168v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2204.11168](http://arxiv.org/abs/2204.11168)

    该论文提出了广义拉格朗日编码计算（GLCC）代码，该代码可以提供鲁棒性、安全性和隐私保护的解决方案。通过将数据集分成多个组，使用插值多项式对数据集进行编码，分享编码数据点，可以在主节点上消除跨组的干扰计算结果。GLCC代码是现有拉格朗日编码计算（LCC）代码的一种特例，具有更灵活的折衷方案。

    

    我们考虑在包含多个输入的大规模数据集上评估任意多元多项式的问题，使用一个主节点和多个工作节点的分布式计算系统。提出了广义拉格朗日编码计算（GLCC）代码，同时提供针对不及时返回计算结果的落后者的鲁棒性，针对恶意工人故意修改结果以获取好处的安全性，以及在工人可能共谋的情况下维护数据集的信息理论隐私。GLCC代码首先将数据集分成多个组，然后使用精心设计的插值多项式对数据集进行编码，并将多个编码数据点分享给每个工作器，使得可以在主节点上消除跨组的干扰计算结果。特别地，GLCC代码包括现有最先进的拉格朗日编码计算（LCC）代码作为一种特例，显示出更灵活的折衷方案。

    We consider the problem of evaluating arbitrary multivariate polynomials over a massive dataset containing multiple inputs, on a distributed computing system with a master node and multiple worker nodes. Generalized Lagrange Coded Computing (GLCC) codes are proposed to simultaneously provide resiliency against stragglers who do not return computation results in time, security against adversarial workers who deliberately modify results for their benefit, and information-theoretic privacy of the dataset amidst possible collusion of workers. GLCC codes are constructed by first partitioning the dataset into multiple groups, then encoding the dataset using carefully designed interpolation polynomials, and sharing multiple encoded data points to each worker, such that interference computation results across groups can be eliminated at the master. Particularly, GLCC codes include the state-of-the-art Lagrange Coded Computing (LCC) codes as a special case, and exhibit a more flexible tradeoff 
    
[^151]: 贝叶斯模型选择、边际似然和泛化

    Bayesian Model Selection, the Marginal Likelihood, and Generalization. (arXiv:2202.11678v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.11678](http://arxiv.org/abs/2202.11678)

    本文回顾和探讨了边缘似然在构造约束和假设测试方面的有用性，强调了使用边缘似然作为泛化的代理的问题，并展示了其如何与神经架构搜索相关，可能导致超参数学习中的欠拟合和过拟合。

    

    如何比较与观测完全一致的假设之间的区别？边际似然（亦称为贝叶斯证据）作为生成由先验得到观测结果的概率，为解决这个问题提供了一个独特的方法，自动编码奥卡姆剃刀原理。尽管已经观察到边际似然可能过拟合并且对先验假设很敏感，但其在超参数学习和离散模型比较方面的局限性尚未得到彻底研究。本文首先重温了边际似然的吸引人的特点，包括学习约束和假设测试。然后，我们强调了使用边际似然作为泛化的代理存在的概念和实际问题。我们展示了边际似然如何与泛化呈负相关，并对神经架构搜索产生影响，并且在超参数学习中可能导致欠拟合和过拟合。

    How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka Bayesian evidence), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor. Although it has been observed that the marginal likelihood can overfit and is sensitive to prior assumptions, its limitations for hyperparameter learning and discrete model comparison have not been thoroughly investigated. We first revisit the appealing properties of the marginal likelihood for learning constraints and hypothesis testing. We then highlight the conceptual and practical issues in using the marginal likelihood as a proxy for generalization. Namely, we show how marginal likelihood can be negatively correlated with generalization, with implications for neural architecture search, and can lead to both underfitting and overfitting in hyperparameter learning. We also re
    
[^152]: CD-ROM：补充深度减少阶模型

    CD-ROM: Complemented Deep-Reduced Order Model. (arXiv:2202.10746v4 [physics.flu-dyn] UPDATED)

    [http://arxiv.org/abs/2202.10746](http://arxiv.org/abs/2202.10746)

    本文介绍了一种基于深度学习的闭合建模方法CD-ROM，用于经典的POD-Galerkin降阶模型，该方法可以显著提高降阶模型的准确性和稳定性。

    

    通过POD-Galerkin方法进行模型阶数约简可以极大地提高求解物理问题的计算效率。然而，该方法在处理非线性高维动力系统如Navier-Stokes方程方面的适用性受到限制，产生不准确且有时不稳定的模型。本文提出了一种基于深度学习的闭合建模方法，用于经典的POD-Galerkin降阶模型(ROM)。所提出的方法在理论上是有基础的，使用神经网络来逼近研究得当的运算符。与大多数以前的工作相比，本文中的CD-ROM方法是基于可解释的连续记忆形式，由关于部分观测到的动力系统行为的简单假设导出。因此，修正后的模型可以使用大多数经典的时间步进模式进行模拟。CD-ROM方法的能力在计算流体力学的两个经典案例中得到了证明，表明它可以显著提高降阶模型的准确性和稳定性。

    Model order reduction through the POD-Galerkin method can lead to dramatic gains in terms of computational efficiency in solving physical problems. However, the applicability of the method to non linear high-dimensional dynamical systems such as the Navier-Stokes equations has been shown to be limited, producing inaccurate and sometimes unstable models. This paper proposes a deep learning based closure modeling approach for classical POD-Galerkin reduced order models (ROM). The proposed approach is theoretically grounded, using neural networks to approximate well studied operators. In contrast with most previous works, the present CD-ROM approach is based on an interpretable continuous memory formulation, derived from simple hypotheses on the behavior of partially observed dynamical systems. The final corrected models can hence be simulated using most classical time stepping schemes. The capabilities of the CD-ROM approach are demonstrated on two classical examples from Computational F
    
[^153]: 基于分类重新参数化技巧的回译端到端训练

    End-to-End Training for Back-Translation with Categorical Reparameterization Trick. (arXiv:2202.08465v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08465](http://arxiv.org/abs/2202.08465)

    本文提出了一种基于分类重新参数化技巧的回译端到端训练方法，来有效地减少两个神经机器翻译模型间离散属性的影响，从而实现端到端式的训练，获得了比以前基准测试更好的BLEU分数。

    

    回译是一种在神经机器翻译中有效的半监督学习框架。预先训练的神经机器翻译模型翻译单语句子并生成合成的双语句对以训练另一个神经机器翻译模型，反之亦然。将两个神经机器翻译模型分别理解为推理和生成模型。以往的研究采用了变分自动编码器（VAE）的培训框架。但是，由于翻译句子的离散属性使得梯度信息无法在两个NMT模型之间流动。本文提出了一种分类重新参数化技巧，使得神经机器翻译模型能够生成可微分的句子，使得VAE的训练框架可以以端到端方式工作。我们的实验表明，我们的方法有效地训练了NMT模型，并在WMT翻译任务的数据集上取得比以前基准测试更好的BLEU分数。

    Back-translation is an effective semi-supervised learning framework in neural machine translation (NMT). A pre-trained NMT model translates monolingual sentences and makes synthetic bilingual sentence pairs for the training of the other NMT model, and vice versa. Understanding the two NMT models as inference and generation models, respectively, previous works applied the training framework of variational auto-encoder (VAE). However, the discrete property of translated sentences prevents gradient information from flowing between the two NMT models. In this paper, we propose a categorical reparameterization trick that makes NMT models generate differentiable sentences so that the VAE's training framework can work in the end-to-end fashion. Our experiments demonstrate that our method effectively trains the NMT models and achieves better BLEU scores than the previous baseline on the datasets of the WMT translation task.
    
[^154]: 两时间尺度更新规则训练生成式对抗网络中的关键批次大小的存在和估计

    Existence and Estimation of Critical Batch Size for Training Generative Adversarial Networks with Two Time-Scale Update Rule. (arXiv:2201.11989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11989](http://arxiv.org/abs/2201.11989)

    本文研究了使用两时间尺度更新规则（TTUR）训练生成式对抗网络（GAN）时批次大小与训练所需步骤数量之间的关系，理论上证明了为了找到稳定点，随着批次大小的增加所需步骤数量会减少并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。

    

    先前的研究表明，在理论和实践中，使用不同的学习率，如不同的恒定率或不同的衰减率等，使用两时间尺度更新规则（TTUR）有助于训练生成式对抗网络（GAN）。此外，批次大小对于使用TTUR训练GANs也很重要，两者都影响了训练所需的步骤数量。本文基于恒定学习率研究了批次大小与使用TTUR训练GANs所需步骤数量之间的关系。我们理论上表明，对于具有恒定学习率的TTUR，为了找到鉴别器和生成器损失函数的稳定点，所需步骤数随着批次大小的增加而减少，并且存在一个最小化随机一阶预言机（SFO）复杂度的关键批次大小。然后，我们使用Fr'echet Inception Distance（FID）作为训练的性能测量，并提供了...

    Previous results have shown that a two time-scale update rule (TTUR) using different learning rates, such as different constant rates or different decaying rates, is useful for training generative adversarial networks (GANs) in theory and in practice. Moreover, not only the learning rate but also the batch size is important for training GANs with TTURs and they both affect the number of steps needed for training. This paper studies the relationship between batch size and the number of steps needed for training GANs with TTURs based on constant learning rates. We theoretically show that, for a TTUR with constant learning rates, the number of steps needed to find stationary points of the loss functions of both the discriminator and generator decreases as the batch size increases and that there exists a critical batch size minimizing the stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet inception distance (FID) as the performance measure for training and provide nu
    
[^155]: MassFormer：使用图变换器预测小分子串联质谱

    MassFormer: Tandem Mass Spectrum Prediction for Small Molecules using Graph Transformers. (arXiv:2111.04824v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.04824](http://arxiv.org/abs/2111.04824)

    MassFormer提出了一种新的方法来准确预测小分子的串联质谱。该模型使用图变换器架构来考虑分子中原子之间的远距离关系，并在多个数据集上击败其他方法，能够恢复关于碰撞能量对光谱的影响的先前知识。

    

    串联质谱能够捕捉分子的碎片模式，提供关于分子结构的关键信息。尽管质谱法在许多领域中应用，但绝大多数小分子缺乏实验参考谱。七十多年来，谱预测一直是该领域的关键挑战。现有的深度学习方法没有利用分子的全局结构，可能导致难以推广到新数据。在这项工作中，我们提出了一个新模型MassFormer，用于准确预测串联质谱。MassFormer使用图变换器架构来模拟分子中原子之间的远距离关系。变形器模块使用化学的预训练任务获得的参数进行初始化，然后在谱数据上进行微调。MassFormer在多组数据集上优于竞争方法，能够恢复关于碰撞能量对光谱的影响的先前知识。

    Tandem mass spectra capture fragmentation patterns that provide key structural information about a molecule. Although mass spectrometry is applied in many areas, the vast majority of small molecules lack experimental reference spectra. For over seventy years, spectrum prediction has remained a key challenge in the field. Existing deep learning methods do not leverage global structure in the molecule, potentially resulting in difficulties when generalizing to new data. In this work we propose a new model, MassFormer, for accurately predicting tandem mass spectra. MassFormer uses a graph transformer architecture to model long-distance relationships between atoms in the molecule. The transformer module is initialized with parameters obtained through a chemical pre-training task, then fine-tuned on spectral data. MassFormer outperforms competing approaches for spectrum prediction on multiple datasets, and is able to recover prior knowledge about the effect of collision energy on the spectr
    
[^156]: 通过连续声学通道进行多智能体通讯学习听说能力的实现

    Towards Learning to Speak and Hear Through Multi-Agent Communication over a Continuous Acoustic Channel. (arXiv:2111.02827v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.02827](http://arxiv.org/abs/2111.02827)

    本研究旨在通过提供一个智能体间的消息传递环境，使得智能体能够通过连续声学通道进行通讯并观察到新兴语言的产生与特点，结果表明：与离散型信号不同，声学讲话者学习使用冗余信息以提高侦听者的连贯性。

    

    多智能体强化学习已成为研究智能体间新兴通讯的有效手段，但对于连续声学通讯却鲜有研究。这更类似于人类获得语言的方式；人类婴儿主要通过与看护者的连续信号交互来习得语言。因此，我们现在的目标是提供一个平台，以开始填补人类和智能体通信之间的差距，让我们能够分析连续信号以及它们产生的方式，它们的特征以及它们与人类语言习得的关系。

    Multi-agent reinforcement learning has been used as an effective means to study emergent communication between agents, yet little focus has been given to continuous acoustic communication. This would be more akin to human language acquisition; human infants acquire language in large part through continuous signalling with their caregivers. We therefore ask: Are we able to observe emergent language between agents with a continuous communication channel? Our goal is to provide a platform to begin bridging the gap between human and agent communication, allowing us to analyse continuous signals, how they emerge, their characteristics, and how they relate to human language acquisition. We propose a messaging environment where a Speaker agent needs to convey a set of attributes to a Listener over a noisy acoustic channel. Using DQN to train our agents, we show that: (1) unlike the discrete case, the acoustic Speaker learns redundancy to improve Listener coherency, (2) the acoustic Speaker de
    
[^157]: 非凸学习中TUSLA算法的非渐进估计及其在ReLU神经网络中的应用

    Non-asymptotic estimates for TUSLA algorithm for non-convex learning with applications to neural networks with ReLU activation function. (arXiv:2107.08649v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2107.08649](http://arxiv.org/abs/2107.08649)

    本文研究了非凸随机优化问题，提出了TUSLA算法在Wasserstein-1和Wasserstein-2距离上的非渐进误差界限，进而推导了期望过量风险的非渐进估计值。在ReLU神经网络中，理论和数值实验表明TUSLA算法能够高效且精确地解决此类优化问题。

    

    本文考虑目标函数具有超线性增加和不连续随机梯度的非凸随机优化问题。针对这种情况，我们对Lovas等人（2020年）引入的tamed unadjusted stochastic Langevin algorithm（TUSLA）进行了非渐进性分析。特别地，我们在Wasserstein-1和Wasserstein-2距离上建立了TUSLA算法的非渐进误差界限。后一结果使我们能够进一步推导期望过量风险的非渐进估计值。为了说明主要结果的适用性，我们考虑了一个包含ReLU神经网络的迁移学习示例，该示例代表机器学习中的一个关键范例。我们为上述示例呈现了数值实验，支持了我们的理论发现。因此，在这种情况下，我们在理论和数值上都证明了TUSLA算法能够高效且精确地解决涉及ReLU激活函数的神经网络优化问题。

    We consider non-convex stochastic optimization problems where the objective functions have super-linearly growing and discontinuous stochastic gradients. In such a setting, we provide a non-asymptotic analysis for the tamed unadjusted stochastic Langevin algorithm (TUSLA) introduced in Lovas et al. (2020). In particular, we establish non-asymptotic error bounds for the TUSLA algorithm in Wasserstein-1 and Wasserstein-2 distances. The latter result enables us to further derive non-asymptotic estimates for the expected excess risk. To illustrate the applicability of the main results, we consider an example from transfer learning with ReLU neural networks, which represents a key paradigm in machine learning. Numerical experiments are presented for the aforementioned example which support our theoretical findings. Hence, in this setting, we demonstrate both theoretically and numerically that the TUSLA algorithm can solve the optimization problem involving neural networks with ReLU activati
    
[^158]: 词向量：一项综述

    Word Embeddings: A Survey. (arXiv:1901.09069v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1901.09069](http://arxiv.org/abs/1901.09069)

    这篇综述介绍了一些主要的词向量构建策略，称为word embeddings，这些策略基于分布假设，编码了语法和语义信息，并被证明在很多NLP任务中是有用的额外特征。

    

    这项工作列出并描述了近期主要的策略，基于分布假设，用于构建单词的固定长度、密集和分布式表示。 这些表示现在通常被称为词向量，并且除了编码出令人惊讶的语法和语义信息外，在许多下游NLP任务中已被证明是有用的额外特征。

    This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.
    

