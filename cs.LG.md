# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Differentiable and accelerated wavelet transforms on the sphere and ball](https://rss.arxiv.org/abs/2402.01282) | 本研究设计了新的高度可分布和自动可微分的方向小波变换，在球面和球上的信号处理中取得了显著的加速效果，同时保持高精度，具有重要的实际应用价值。 |
| [^2] | [Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models](https://arxiv.org/abs/2403.09635) | 提出了一个统一的信号传播理论，提供了控制transformer模型信号传播的公式，提出了DeepScaleLM初始化和缩放方案，使得可以训练非常深的模型，并发现深层模型在多个任务和数据集上胜过浅层模型。 |
| [^3] | [Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking](https://arxiv.org/abs/2403.09629) | Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力 |
| [^4] | [Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation](https://arxiv.org/abs/2403.09625) | Make-Your-3D是一种新颖的3D定制方法，可以在5分钟内从单个图像和文本描述中实现高保真、一致的3D内容生成，通过协调不同模型的分布来实现这一目标 |
| [^5] | [Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning](https://arxiv.org/abs/2403.09621) | 研究提出了最小化最优和计算高效的算法，为鲁棒离线强化学习中的函数逼近带来新颖视角，并展示了其与标准离线强化学习中函数逼近的区别。 |
| [^6] | [Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training](https://arxiv.org/abs/2403.09613) | 在结构化环境中依次微调的LLMs表现出预期行为，能够从遗忘中恢复，揭示了在过参数化网络中进行训练的新见解 |
| [^7] | [Compute-first optical detection for noise-resilient visual perception](https://arxiv.org/abs/2403.09612) | 光学信号处理能够提高视觉感知任务的检测噪声鲁棒性，通过空间重新分配光学信号来增强性能，为解决噪声和弱信号环境下的神经计算任务性能瓶颈提供了新思路。 |
| [^8] | [MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training](https://arxiv.org/abs/2403.09611) | 通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。 |
| [^9] | [Optimistic Verifiable Training by Controlling Hardware Nondeterminism](https://arxiv.org/abs/2403.09603) | 提出了一种方法，结合了在比目标模型更高精度下进行训练、在中间计算步骤后进行四舍五入，并基于自适应阈值存储四舍五入决策，以应对硬件非确定性对训练过程的影响。 |
| [^10] | [Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds](https://arxiv.org/abs/2403.09598) | 该论文提出了利用混合Mixup方法来解决稀有无尾蛙声多标签分类中的挑战，实验证明这种方法在处理类别不平衡和多标签示例方面具有有效性。 |
| [^11] | [Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation](https://arxiv.org/abs/2403.09588) | 提出了一种受数据库启发的在线数据流回归模型，通过迭代遗忘过时数据粒子，以保持只有最近的相关信息，从而实现低延迟预测。 |
| [^12] | [Algorithmic syntactic causal identification](https://arxiv.org/abs/2403.09580) | 通过替换传统概率论为对称单调范畴的替代基础，可以扩展因果识别技术到更多因果设置中。 |
| [^13] | [uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures](https://arxiv.org/abs/2403.09579) | uaMix-MAE提出了一种高效的ID调整策略，利用无监督音频混合对预训练的MAEs进行对比调整，从而有效地适应特定任务语义。 |
| [^14] | [Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis](https://arxiv.org/abs/2403.09571) | 提出了一个框架，通过监视车辆的行为和状态信息来自动识别自动驾驶车辆，无需车辆主动通知。 |
| [^15] | [Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search](https://arxiv.org/abs/2403.09570) | 本文引入了一种新颖的信息理论获取函数，用于平衡在连续的优化任务中获得最优值或解信息的需求。 |
| [^16] | [Self-Consistency Training for Hamiltonian Prediction](https://arxiv.org/abs/2403.09560) | 提出了一种基于自洽原理的哈密顿量预测训练方法，无需标记数据，能够在大量未标记数据上训练，极大地增强了泛化能力，并提高了训练效率。 |
| [^17] | [Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields](https://arxiv.org/abs/2403.09549) | 将去噪方法推广到非平衡结构，从而改进等变力场的性能，提高了对原子间相互作用的理解以及在分子动力学和催化剂设计等领域的应用。 |
| [^18] | [Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability](https://arxiv.org/abs/2403.09548) | 本研究使用梯度提升算法对乳腺癌进行分类，关注提高召回率，以实现更好的检测和预测效果。 |
| [^19] | [How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions](https://arxiv.org/abs/2403.09547) | 本研究通过对GitHub上185个开源项目的分析，发现机器学习项目通常需要更长的构建时间，中等规模的项目测试覆盖率较低，并呈现出构建时间趋势增长的特点。 |
| [^20] | [Explorations in Texture Learning](https://arxiv.org/abs/2403.09543) | 论文探索了纹理学习的关键，通过纹理-对象关联揭示了卷积神经网络中纹理和对象类别之间关系的新见解，并提出纹理学习对于解释性方法的新可能性和发现意想不到偏见的潜力。 |
| [^21] | [Logits of API-Protected LLMs Leak Proprietary Information](https://arxiv.org/abs/2403.09539) | 大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能 |
| [^22] | [Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information](https://arxiv.org/abs/2403.09516) | 通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。 |
| [^23] | [On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study](https://arxiv.org/abs/2403.09509) | STPA在汽车领域的应用面临可追溯性挑战，本文探讨了针对汽车行业的STPA指南。 |
| [^24] | [Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition](https://arxiv.org/abs/2403.09506) | 本研究提出了一种名为运动一致增强（MCA）的数据增强方法，通过引入外观变化来鼓励模型优先考虑视频中的运动信息，而不是静态外观。 |
| [^25] | [EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning](https://arxiv.org/abs/2403.09502) | 这项研究提出了一种利用等变性进行音频-视觉对比学习的新框架，通过一个共享的基于注意力的变换预测器来实现特征聚合和嵌入表示，有效提供了强大的监督，且计算开销最小。 |
| [^26] | [A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning](https://arxiv.org/abs/2403.09499) | 该研究提出了一种基于Q学习的算法，以实现在奶牛养殖中整合可再生能源，以改善电池管理，应对电能消耗波动和能源价格波动的挑战。 |
| [^27] | [On using Machine Learning Algorithms for Motorcycle Collision Detection](https://arxiv.org/abs/2403.09491) | 本文研究了机器学习算法在摩托车碰撞检测中的应用，通过数据收集和训练分类器，探讨了可靠地检测即将发生的碰撞的挑战。 |
| [^28] | [Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks](https://arxiv.org/abs/2403.09479) | 本文研究了语言模型从基本技能到复杂推理任务的泛化能力，并提出了分级课程学习训练策略，成功改进了模型在复杂推理任务上的性能。 |
| [^29] | [VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields](https://arxiv.org/abs/2403.09477) | VIRUS-NeRF是基于视觉、红外和超声波的神经辐射场，通过整合超声波和红外传感器的深度测量数据，实现了在自主移动机器人中达到与LiDAR点云相媲美的映射性能。 |
| [^30] | [Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision](https://arxiv.org/abs/2403.09472) | 通过从更简单的任务学习，实现对更难推理任务的有效泛化，提出了一种可扩展对齐方法。 |
| [^31] | [Outlier Robust Multivariate Polynomial Regression](https://arxiv.org/abs/2403.09465) | 将一维的解法推广到多维，提出了一种在多元多项式回归中对异常值具有鲁棒性的算法。 |
| [^32] | [Machine learning for structural design models of continuous beam systems via influence zones](https://arxiv.org/abs/2403.09454) | 通过影响区域概念，该研究提出了一个非迭代的机器学习结构设计模型，能够准确预测连续梁系统的截面要求，并在测试中表现出很好的泛化能力。 |
| [^33] | [Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk](https://arxiv.org/abs/2403.09450) | 微调扩散模型可能会增加生成隐私风险，甚至使最先进的成员推理攻击对扩散模型的攻击效果提高5.4％，并可将提取的私有样本数量从几乎0个增加到平均16.3个。 |
| [^34] | [Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency](https://arxiv.org/abs/2403.09441) | 本研究探讨了对压缩神经网络进行对抗微调对提高鲁棒性和效率的影响。 |
| [^35] | [Variational Inference with Sequential Sample-Average Approximations](https://arxiv.org/abs/2403.09429) | VISA方法通过顺序样本均值逼近在计算密集型模型中实现近似推断，能够在保守选择学习率的情况下以较小的计算成本达到与标准方法相当的逼近精度。 |
| [^36] | [Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity](https://arxiv.org/abs/2403.09428) | 本文提出了一种检索增强的上下文学习框架，旨在解决多模态学习中缺失模态和数据稀缺问题。 |
| [^37] | [User Identification via Free Roaming Eye Tracking Data](https://arxiv.org/abs/2403.09415) | 通过自由漫游眼动数据，本研究在非实验室环境中进行了用户识别研究，并在径向基函数网络分类器的支持下取得了相对较高的识别准确率。 |
| [^38] | [LM2D: Lyrics- and Music-Driven Dance Synthesis](https://arxiv.org/abs/2403.09407) | 提出了LM2D，一种结合了多模态扩散模型和一致性蒸馏的新颖概率架构，旨在在音乐和歌词条件下进行舞蹈生成；介绍了第一个涵盖音乐和歌词的3D舞蹈运动数据集。 |
| [^39] | [Learning to optimize with convergence guarantees using nonlinear system theory](https://arxiv.org/abs/2403.09389) | 基于非线性系统理论，提出了一种对于平滑非凸目标函数的所有收敛算法的无约束参数化方法 |
| [^40] | [Pantypes: Diverse Representatives for Self-Explainable Models](https://arxiv.org/abs/2403.09383) | 引入了pantypes，一种旨在通过一组稀疏对象捕获输入分布的全部多样性的原型对象家族，可以大大增强原型自解释模型。 |
| [^41] | [BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences](https://arxiv.org/abs/2403.09347) | BurstAttention是一种用于优化内存访问和通信操作的分布式注意力框架，适用于处理极长序列。 |
| [^42] | [A Hierarchical Fused Quantum Fuzzy Neural Network for Image Classification](https://arxiv.org/abs/2403.09318) | 提出了一种新颖的层级融合量子模糊神经网络（HQFNN），通过使用量子神经网络学习模糊成员函数，在不确定数据分类任务中表现出色。 |
| [^43] | [Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective](https://arxiv.org/abs/2403.09303) | 该研究从理论角度为医学异常检测中基于自编码器的重建方法提供了基础，揭示了改进AE在异常检测中的关键在于最小化信息。 |
| [^44] | [StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images](https://arxiv.org/abs/2403.09302) | StainFuser提出了一种新颖的条件潜在扩散架构，将染色标准化问题视为风格迁移任务，无需手工制作颜色组分，在2百万多个组织学图像上训练的结果优于当前最先进的方法。 |
| [^45] | [Recursive Causal Discovery](https://arxiv.org/abs/2403.09300) | 可移除变量的概念允许递归方法进行因果发现，通过逐步减少问题规模来帮助解决因果发现中的主要挑战。 |
| [^46] | [More than words: Advancements and challenges in speech recognition for singing](https://arxiv.org/abs/2403.09298) | 本文讨论了歌唱语音识别中的挑战和进展，探索了音素识别、歌曲中的语言识别、关键词识别和完整歌词转录等关键领域，并介绍了深度学习和大规模数据集在该领域推动进展的最新发展。 |
| [^47] | [SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival](https://arxiv.org/abs/2403.09290) | 该论文介绍了一种名为SELECTOR的异构图网络，利用卷积掩码自编码器进行癌症患者生存的鲁棒多模态预测 |
| [^48] | [DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning](https://arxiv.org/abs/2403.09284) | 提出了一种新颖的动态关联的个性化联邦学习模型（DA-PFL），采用互补的关联度量和动态聚合策略，在每轮动态聚合客户端以减少类不平衡风险。 |
| [^49] | [Deep Limit Order Book Forecasting](https://arxiv.org/abs/2403.09267) | 该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。 |
| [^50] | [To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation](https://arxiv.org/abs/2403.09259) | 提出了一种用于神经机器翻译的混合主动学习策略HUDS，结合了不确定性和多样性，用于领域自适应的句子选择。 |
| [^51] | [Uncertainty Quantification for cross-subject Motor Imagery classification](https://arxiv.org/abs/2403.09228) | 本研究针对跨受试者的运动想象分类，引入不确定性量化方法，验证了深度集成在分类性能和跨受试者不确定性量化性能上的优越表现，同时发现标准CNN方法在某些情况下表现更好。 |
| [^52] | [MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer](https://arxiv.org/abs/2403.09223) | 提出了混合通道策略，结合了通道独立策略的数据扩展优势，能对抗跨通道相关性忘却，实现多变量时间序列预测的创新模型MCformer。 |
| [^53] | [On the Laplace Approximation as Model Selection Criterion for Gaussian Processes](https://arxiv.org/abs/2403.09215) | 通过引入基于Laplace逼近的多种度量标准，本研究解决了高斯过程模型选择中遇到的性能不佳和运行时间问题，实验结果表明这些新的标准在质量上与黄金标准动态嵌套抽样相当，同时保持了计算速度。 |
| [^54] | [LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection](https://arxiv.org/abs/2403.09209) | 该论文的贡献是提出了一个名为LAN的框架，能够实时在活动级别进行内部威胁检测，并学习活动序列内的时间依赖关系和活动之间的关系。 |
| [^55] | [Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM](https://arxiv.org/abs/2403.09206) | 本文在三层线性结构的部分CBM中揭示了贝叶斯概化错误的上界，进一步证明部分CBM优于朴素CBM。 |
| [^56] | [Are Vision Language Models Texture or Shape Biased and Can We Steer Them?](https://arxiv.org/abs/2403.09193) | 本文研究了广泛应用的视觉语言模型中的纹理与形状偏见，发现这些模型通常比视觉编码器更偏向形状，暗示视觉偏见在一定程度上会受到文本的调节 |
| [^57] | [Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study](https://arxiv.org/abs/2403.09188) | 提出了一种名为基础投影层（BPL）的DL模块，通过将稀疏数据转换为密集表示来优化深度学习模型的训练过程。 |
| [^58] | [Generalized Relevance Learning Grassmann Quantization](https://arxiv.org/abs/2403.09183) | 该论文扩展了广义相关性学习向量量化的应用，以处理格拉斯曼流形，提出模型返回一组原型子空间和相关性向量，为分类任务提供重要见解。 |
| [^59] | [ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks](https://arxiv.org/abs/2403.09171) | ADEdgeDrop提出了一种敌对边缘删除方法，通过引入敌对边缘预测器指导边缘删除，从而提高了图神经网络的稳健性。 |
| [^60] | [Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices](https://arxiv.org/abs/2403.09141) | 研究探索了在支持AI的边缘设备中实现分布式数据处理的方法，重点解决了在独立代理遇到的数据集的空间和时间变异性中确定学习结果的置信水平挑战。 |
| [^61] | [Optimal Top-Two Method for Best Arm Identification and Fluid Analysis](https://arxiv.org/abs/2403.09123) | 提出了解决最佳臂识别问题中的样本复杂度挑战的最优Top-Two算法。 |
| [^62] | [Randomized Principal Component Analysis for Hyperspectral Image Classification](https://arxiv.org/abs/2403.09117) | 本研究探讨了针对高光谱图像分类的主成分分析（PCA）和随机主成分分析（R-PCA），实验证明PCA在支持向量机（SVM）方面优于R-PCA，但在轻量级梯度提升机（LightGBM）方面表现接近准确度数值。 |
| [^63] | [AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning](https://arxiv.org/abs/2403.09113) | AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。 |
| [^64] | [SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning](https://arxiv.org/abs/2403.09110) | 将稀疏识别的非线性动力学（SINDy）与深度强化学习（DRL）结合，提出了SINDy-RL框架，用于创建高效解释性还有在低数据制度下创建高效且可解释的数据驱动模型。 |
| [^65] | [S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering](https://arxiv.org/abs/2403.09107) | S^2MVTC提出了一种简单而高效的可扩展多视图张量聚类方法，重点是学习跨视图内部和视图间的嵌入特征之间的相关性。 |
| [^66] | [Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement](https://arxiv.org/abs/2403.09101) | 本文提出了一种自导标签细化方法，通过在对抗训练中自我细化更准确和信息丰富的标签分布，并动态将自我蒸馏模型的知识纳入当前模型来消除对抗鲁棒性的过拟合问题。 |
| [^67] | [Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning](https://arxiv.org/abs/2403.09100) | 首次实现了对不加标记组织进行虚拟双折射成像和虚拟刚果红染色，通过深度学习将自发荧光图像转换为明场和偏振光图像。 |
| [^68] | [Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization](https://arxiv.org/abs/2403.09090) | DGDA方法通过在GDA更新中引入耗散项，解决了极小-极大优化问题中的振荡行为，实现了超越其他方法的更优越收敛速度。 |
| [^69] | [Learning from straggler clients in federated learning](https://arxiv.org/abs/2403.09086) | 现有联邦学习算法难以从严重延迟的客户端学习，为了改进这种情况，引入了两种新算法FARe-DUST和FeAST-on-MSG。 |
| [^70] | [Hyperparameters in Continual Learning: a Reality Check](https://arxiv.org/abs/2403.09066) | 超参数对于连续学习的重要性被强调，提出了一个涉及超参数调整和评估阶段的评估协议。 |
| [^71] | [Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference](https://arxiv.org/abs/2403.09054) | 本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。 |
| [^72] | [Towards a theory of model distillation](https://arxiv.org/abs/2403.09053) | 提出了模型蒸馏的一般理论，通过PAC-蒸馏定义，提出了抽取神经网络训练权重知识的新算法，并证明了蒸馏比从头学习更便宜且有助于理解其复杂性。 |
| [^73] | [Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains](https://arxiv.org/abs/2403.09048) | 引入FedPLVM通过建立方差感知的双层原型聚类和使用新型$\alpha$-稀疏原型损失，以减少跨领域特征表示差异。 |
| [^74] | [Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs](https://arxiv.org/abs/2403.09039) | 提出了一种空间-时间记忆增强图自编码器（STRIPE）用于动态图中的异常检测，通过结合图神经网络和门控时间卷积层来提取空间特征和时间特征。 |
| [^75] | [DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers](https://arxiv.org/abs/2403.09035) | 本文提出了一种新颖的DNN训练和推断框架DiTMoS，通过选择器-分类器架构和多样性策略，构建小型/弱模型并提高准确性上限。 |
| [^76] | [CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences](https://arxiv.org/abs/2403.09032) | 介绍了 CodeUltraFeedback 数据集，通过 AI 反馈使 14 种不同的 LLMs 对 10,000 个复杂指令生成响应，并使用 LLM-as-a-Judge 方法评估它们与五种编程偏好的对齐情况，同时提出了用于评估 LLM 对编程偏好对齐的基准 CODAL-Bench。 |
| [^77] | [An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals](https://arxiv.org/abs/2403.09030) | 该研究开发了一个深度学习模型，通过声学信号对风力涡轮机发电机中的轴承故障进行分类，取得了在准确性和泛化能力方面的出色成果。 |
| [^78] | [Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications](https://arxiv.org/abs/2403.08980) | 神经网络推断在高数据速率、低延迟科学应用中的架构要求：所有参数存储在芯片上，需要协同设计自定义/可重构逻辑以满足极端的延迟和带宽约束 |
| [^79] | [AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents](https://arxiv.org/abs/2403.08978) | AutoGuide通过提取嵌入在离线数据中的知识，生成一组状态感知指南，从而弥合大型语言模型中的知识差距，为代理的决策过程提供有用的知识。 |
| [^80] | [The Full-scale Assembly Simulation Testbed (FAST) Dataset](https://arxiv.org/abs/2403.08969) | 提出了一个新的用于机器学习目的的VR数据集，包括108名参与者在VR中学习组装两种不同全尺寸结构的数据，并探讨了未来研究人员如何利用这个数据集。 |
| [^81] | [Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory](https://arxiv.org/abs/2403.08965) | 通过深度学习和库普曼理论，提出了一种数据驱动框架，可以同时识别“两体问题”和“圆限制三体问题”的动力学，并将其全局线性化成线性时不变系统。 |
| [^82] | [Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis](https://arxiv.org/abs/2403.08955) | 本文对风险敏感策略梯度方法进行了迭代复杂度分析，发现其能够通过使用指数效用函数达到较低的迭代复杂度。 |
| [^83] | [Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era](https://arxiv.org/abs/2403.08946) | 在大型语言模型时代，为了适应其复杂性和先进能力，我们引入了可用的XAI概念，通过积极增强LLMs在实际环境中的生产力和适用性，实现XAI方法论的重大转变。 |
| [^84] | [Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders](https://arxiv.org/abs/2403.08941) | 最近的研究表明，为了确保高质量的推断模型，可以通过迭代训练最大化与推断模型相关的目标函数，以解决变分自编码器中推断模型近似不准确导致的局部最优解问题。 |
| [^85] | [FogGuard: guarding YOLO against fog using perceptual loss](https://arxiv.org/abs/2403.08939) | FogGuard提出了一种针对雾天气条件挑战的新型雾感知目标检测网络，通过微调数据收集的方法来提高目标检测算法在恶劣天气条件下的可靠性。 |
| [^86] | [A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator](https://arxiv.org/abs/2403.08938) | 该论文证明了对于满足一定核特征值分解的谱和集中性质的一般类问题，具有一种非渐近确定性近似的等价性。 |
| [^87] | [Efficiently Computing Similarities to Private Datasets](https://arxiv.org/abs/2403.08917) | 提出了一种能够高效计算与私有数据集相似性的方法，改进了先前工作的理论结果，提供了更好的隐私-效用权衡和更快的查询时间。 |
| [^88] | [A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty](https://arxiv.org/abs/2403.08901) | 提出了一个名为Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)的框架，通过基于神经网络的代理模型，采用层次贝叶斯推理和模型验证测试来评估代理模型的可信度和预测可靠性。 |
| [^89] | [One-Shot Averaging for Distributed TD($\lambda$) Under Markov Sampling](https://arxiv.org/abs/2403.08896) | 在分布式强化学习中，通过一次性平均化的方法，每个agent独立进行TD($\lambda$)运算，并最终在结果上进行平均，实现了相对于以往工作更少的通信量要求的线性加速。 |
| [^90] | [REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values](https://arxiv.org/abs/2403.08880) | 本文介绍了特征重选择的问题，使得特征可以根据次要模型性能特征进行选择，从而纠正与负责任人工智能相关的模型特征。 |
| [^91] | [Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning](https://arxiv.org/abs/2403.08879) | 提出了一种在智能交通系统环境下进行多目标、多智能体强化学习的算法，具有高学习效率和低计算要求。 |
| [^92] | [Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling](https://arxiv.org/abs/2403.08854) | 提出了一种称为Moment Pooling的新方法，通过将Deep Sets中的求和泛化为任意的多变量矩，显著降低机器学习网络的潜在空间维度，在固定的潜在维度下实现更高的有效潜在维度，从而可以直接可视化和解释内部表示。 |
| [^93] | [PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models](https://arxiv.org/abs/2403.08851) | 该研究提出了PAPERCLIP方法，通过将天文观测与自然语言关联起来，利用预训练的CLIP模型进行微调，实现了观测和自然语言之间的有意义的联合表示。 |
| [^94] | [JAXbind: Bind any function to JAX](https://arxiv.org/abs/2403.08847) | JAXbind旨在大幅减少将其他编程语言中实现的自定义函数绑定到JAX所需的工作量，提供易于使用的Python接口定义自定义JAX原语。 |
| [^95] | [Bifurcated Attention for Single-Context Large-Batch Sampling](https://arxiv.org/abs/2403.08845) | 分叉注意力是针对语言模型推断中单上下文批量抽样环境开发的方法，通过将注意力机制分成两个独立的操作来减少冗余内存IO成本，提高效率并降低延迟。 |
| [^96] | [Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows](https://arxiv.org/abs/2403.08839) | 该论文提出了一种将机器学习整合到大邻域搜索中的方法，称为学习增强的邻域选择（LENS），用于在解决车辆路径问题时辅助确定解决方案中应该破坏和修复的部分。 |
| [^97] | [Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation](https://arxiv.org/abs/2403.08838) | 提出了一种基于分层轨迹表示的船舶行为预测聚类方法，通过使用预测聚类和潜在编码，可以同时改善聚类和预测，并在实验证明其相对于现有方法的优越性。 |
| [^98] | [Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks](https://arxiv.org/abs/2403.08837) | 提出循环数据并行性，通过将微批量执行从同时改为顺序执行，以解决数据并行化中激活内存峰值和梯度平均的问题，同时还能减少所需GPU数量。 |
| [^99] | [Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring](https://arxiv.org/abs/2403.08836) | 本文在医学过程监测中提出了一种基于变压器的预测性过程监测方法，通过图形位置编码技术融入本体领域特定知识，获得了令人鼓舞的实验结果。 |
| [^100] | [Stacking-based deep neural network for player scouting in football 1](https://arxiv.org/abs/2403.08835) | 本研究提出了一种基于堆叠的深度学习模型，在足球领域中用于检测高潜力球员，相比传统统计方法取得了显著更好的结果。 |
| [^101] | [Predictive Analysis of Tuberculosis Treatment Outcomes Using Machine Learning: A Karnataka TB Data Study at a Scale](https://arxiv.org/abs/2403.08834) | 该研究探索了如何利用机器学习和表格数据更准确地预测结核病治疗结果，并在验证集上取得了优异的性能。 |
| [^102] | [Majority-of-Three: The Simplest Optimal Learner?](https://arxiv.org/abs/2403.08831) | 该论文研究了可能最优的最简单算法：返回三个ERM分类器的多数投票，证明其实现了错误的期望最优边界，并得出近乎最优概率边界。 |
| [^103] | [Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News](https://arxiv.org/abs/2403.08829) | 该研究探讨了在假新闻中个人和社会偏见对人类决策的影响，发现了偏见对集体决策的渗透，并评估了自适应聚合算法在提高判断准确性方面的有效性。 |
| [^104] | [A Dataset for the Validation of Truth Inference Algorithms Suitable for Online Deployment](https://arxiv.org/abs/2403.08826) | 介绍了一个大规模真实众包标注数据集，弥补了先前真相推断算法验证数据集的局限性，为长期和在线真相推断提供了实用性。 |
| [^105] | [LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2403.08822) | LoRA-SP利用随机半选择参数冻结的新颖方法，在微调大型语言模型时有效平衡预训练知识的保留和任务特定优化的适应性，显著降低了计算和内存需求，同时实现了竞争性性能。 |
| [^106] | [Effective Gradient Sample Size via Variation Estimation for Accelerating Sharpness aware Minimization](https://arxiv.org/abs/2403.08821) | 本文提出了一种简单而高效的抽样方法，通过基于PSF的变化来显著加速Sharpness-aware Minimization（SAM），实现了与SAM相当的最先进准确性。 |
| [^107] | [Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns](https://arxiv.org/abs/2403.08820) | 该研究建立了一个大规模多方面的膳食数据集，提出了Diet-ODIN框架，旨在探索膳食模式与阿片类药物误用之间的关联。 |
| [^108] | [Thermometer: Towards Universal Calibration for Large Language Models](https://arxiv.org/abs/2403.08819) | 提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。 |
| [^109] | [Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM](https://arxiv.org/abs/2403.08818) | 提出了一个名为MINGLE的新框架，通过两级注入策略将医学概念语义和临床笔记语义融合到超图中，有效地整合了结构和语义的电子健康记录数据。 |
| [^110] | [Federated Deep Q-Learning and 5G load balancing](https://arxiv.org/abs/2403.08813) | 本研究提出并分析了一个联合深度Q学习负载平衡系统，利用Open-RAN xAPP框架和近实时射频接口控制器（near-RT RIC）实施。我们的模拟结果表明，与目前UE使用的最大信噪比（MAX-SINR）方法相比，我们提出的深度Q学习模型可以持续提供更好的负载平衡性能。 |
| [^111] | [Gore Diffusion LoRA Model](https://arxiv.org/abs/2403.08812) | 本文审查了“Gore扩散LoRA模型”，这是一种擅长生成超逼真暴力和流血视觉效果的创新AI模型，强调在其创作与实施中需要认真讨论AI、艺术和暴力的融合，主张负责任的发展和道德部署这些强大技术。 |
| [^112] | [Comparison of edge computing methods in Internet of Things architectures for efficient estimation of indoor environmental parameters with Machine Learning](https://arxiv.org/abs/2403.08810) | 本研究提出了两种基于低成本边缘物联网架构的方法，用于实现估计室内环境质量参数的轻量级机器学习模型，为物联网架构中进行高效数据处理提供了新思路。 |
| [^113] | [Adversarially Robust Deepfake Detection via Adversarial Feature Similarity Learning](https://arxiv.org/abs/2403.08806) | 通过对抗特征相似性学习来提升对抗性强的Deepfake检测，以区分真假实例并最大化对抗性扰动和未扰动示例之间的相似性。 |
| [^114] | [Forward Direct Feedback Alignment for Online Gradient Estimates of Spiking Neural Networks](https://arxiv.org/abs/2403.08804) | 提出了一种新颖的神经形态算法SFDFA，描述了如何在线计算准确的局部梯度，并推导出神经形态硬件的动力系统。 |
| [^115] | [Governance of Generative Artificial Intelligence for Companies](https://arxiv.org/abs/2403.08802) | 本综述填补了有关企业中生成式人工智能（GenAI）治理的研究空白，提出了一个框架，旨在利用业务机会并减轻与GenAI整合相关风险。 |
| [^116] | [Neural Loss Function Evolution for Large-Scale Image Classifier Convolutional Neural Networks](https://arxiv.org/abs/2403.08793) | 通过神经损失函数搜索，本研究发现了三种新的损失函数NeuroLoss1、NeuroLoss2和NeuroLoss3，这些损失函数能够在大规模图像分类器卷积神经网络中发挥作用。 |
| [^117] | [Realtime Facial Expression Recognition: Neuromorphic Hardware vs. Edge AI Accelerators](https://arxiv.org/abs/2403.08792) | 本研究比较了神经形态硬件与边缘AI加速器在实时面部表情识别中的性能，结果显示Loihi相较于Coral TPU具有更低的功耗和能耗，并保持了可比较的准确性水平。 |
| [^118] | [Bridging Human Concepts and Computer Vision for Explainable Face Verification](https://arxiv.org/abs/2403.08789) | 本文通过结合计算机和人类视觉的方法，提高了人脸验证算法解释的可解释性 |
| [^119] | [Multi-view Subspace Clustering via An Adaptive Consensus Graph Filter](https://arxiv.org/abs/2403.08787) | 本文提出一种基于自适应一致性图滤波器的多视角子空间聚类方法，通过整合一致性重构系数矩阵和来自不同视角的重构系数矩阵，实现对多视角数据集的子空间结构建模。 |
| [^120] | [Efficient Combinatorial Optimization via Heat Diffusion](https://arxiv.org/abs/2403.08757) | 通过热扩散实现了高效的组合优化，克服了现有方法在搜索全局最优时效率有限的问题。 |
| [^121] | [Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records](https://arxiv.org/abs/2403.08664) | 这项研究探讨了利用合成数据生成医疗记录的方法，特别是通过零样本和少样本提示策略，避免了在训练过程中使用真实患者数据的隐私问题。 |
| [^122] | [On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors](https://arxiv.org/abs/2403.08609) | 本文研究如何将自适应步长引入蒙特卡罗采样算法，以实现从神经网络后验分布中生成样本，并证明了这些方法可以收敛到正确的分布。 |
| [^123] | [Consistent Prompting for Rehearsal-Free Continual Learning](https://arxiv.org/abs/2403.08568) | 提出了一种新颖的一致提示（CPrompt）方法，通过训练期间所有现有分类器接受提示训练，实现更加对齐的训练和测试。 |
| [^124] | [HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback](https://arxiv.org/abs/2403.08309) | 提出了HRLAIF方法来改善开放域强化学习中的模型响应帮助性，通过增强AI注释响应的准确性来提高模型在训练过程中的鲁棒性 |
| [^125] | [Random Search as a Baseline for Sparse Neural Network Architecture Search](https://arxiv.org/abs/2403.08265) | 论文提出了一种评估方法和基于随机搜索的基线方法，用于发现高质量的稀疏神经网络配置，以解决当前缺乏可靠比较和可重现性的问题。 |
| [^126] | [KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction](https://arxiv.org/abs/2403.07969) | 本文提出了KnowCoder，一个通过代码生成执行普适信息提取的大型语言模型，引入了代码风格的模式表示方法和两阶段学习框架，以提高LLMs对结构化知识的准确提取能力 |
| [^127] | [A Neural-Evolutionary Algorithm for Autonomous Transit Network Design](https://arxiv.org/abs/2403.07917) | 提出了一种神经进化算法用于自动公交网络设计，该算法通过训练图神经网络模型作为策略，并将其用作进化算法中的变异操作符，在公交网络设计基准集上优于单独学习策略和简单进化算法方法。 |
| [^128] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^129] | [Knowledge Graph Large Language Model (KG-LLM) for Link Prediction](https://arxiv.org/abs/2403.07311) | 该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。 |
| [^130] | [Better than classical? The subtle art of benchmarking quantum machine learning models](https://arxiv.org/abs/2403.07059) | 通过开发开源软件包进行大规模研究，研究发现总体而言，开箱即用的经典机器学习模型胜过量子分类器，还发现将量子模型中的纠缠去除通常会导致同样或更好的性能。 |
| [^131] | [Semantic Residual Prompts for Continual Learning](https://arxiv.org/abs/2403.06870) | 通过引入语义剩余提示，作者提出了一种稳定的选择策略，利用两级适应机制来在持续学习中解决提示冲突的问题。 |
| [^132] | [Probabilistic Contrastive Learning for Long-Tailed Visual Recognition](https://arxiv.org/abs/2403.06726) | 提出了一种新颖的概率对比（ProCo）学习算法，该算法估计特征空间中每个类别样本的数据分布，并对比对进行采样 |
| [^133] | [On the Diminishing Returns of Width for Continual Learning](https://arxiv.org/abs/2403.06398) | 增加神经网络宽度以减少遗忘会带来递减的回报，并且在先前研究中尚未探索的宽度范围内进行了实证验证。 |
| [^134] | [Statistical Efficiency of Distributional Temporal Difference](https://arxiv.org/abs/2403.05811) | 该论文分析了分布式时间差分的统计效率和有限样本性能。 |
| [^135] | [Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking](https://arxiv.org/abs/2403.05693) | 本文基于线性时态逻辑（LTL）形式化了航天器任务和安全要求，并提出了自动构建奖励函数以实现有效训练的方法。同时探讨了从安全LTL规范构建航天器屏蔽的方法，提出了三种可以提供概率保证的设计，并通过实验展示了这些屏蔽与不同策略的互动和奖励结构的灵活性。 |
| [^136] | [Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces](https://arxiv.org/abs/2403.02524) | 本文提出了一种基于装配再生核希尔伯特空间内在结构和jets几何概念的估计Koopman算子的新方法JetDMD，通过明确的误差界和收敛率证明其优越性，为Koopman算子的数值估计提供了更精确的方法，同时在装配希尔伯特空间框架内提出了扩展Koopman算子的概念，有助于深入理解估计的Koopman特征函数。 |
| [^137] | [Diffusion-TS: Interpretable Diffusion for General Time Series Generation](https://arxiv.org/abs/2403.01742) | 提出了一种新颖的基于扩散的框架 Diffusion-TS，结合了编码器-解码器变压器和解耦时间表示，通过直接重建样本而非噪声生成高质量的多变量时间序列样本，旨在实现时间序列的解释性和真实性。 |
| [^138] | [VBART: The Turkish LLM](https://arxiv.org/abs/2403.01308) | VBART是第一个土耳其序列到序列大语言模型，通过与BART和mBART模型结合形成了紧凑型LLM，并在多个任务中表现出色，为土耳其自然语言处理研究开辟了新的可能性。 |
| [^139] | [End-to-end Graph-Sequential Representation Learning for Accurate Recommendations](https://arxiv.org/abs/2403.00895) | 本文提出了一个新颖的多重表示学习框架，有效地结合了基于序列和基于图的推荐方法，显著改善了推荐性能。 |
| [^140] | [Smooth Tchebycheff Scalarization for Multi-Objective Optimization](https://arxiv.org/abs/2402.19078) | 通过光滑 Tchebycheff 标量化方法，本文提出了一种轻量级的方法，用于梯度型多目标优化，具有更低的计算复杂性但仍能找到所有帕累托解。 |
| [^141] | [MMSR: Symbolic Regression is a Multimodal Task](https://arxiv.org/abs/2402.18603) | 符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。 |
| [^142] | [QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations](https://arxiv.org/abs/2402.17516) | QUCE方法旨在通过减少路径不确定性来量化和缓解基于路径的不确定性，从而改善对抗性反事实解释的表现。 |
| [^143] | [Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning](https://arxiv.org/abs/2402.13897) | 提出了一个两块式的方法来解决长文档中信息检索领域的挑战，并实现了双向交互 |
| [^144] | [The Price of Adaptivity in Stochastic Convex Optimization](https://arxiv.org/abs/2402.10898) | 该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。 |
| [^145] | [World Model on Million-Length Video And Language With RingAttention](https://arxiv.org/abs/2402.08268) | 该论文介绍了一个使用百万长度的视频和语言序列进行联合建模的环形注意力世界模型。该模型通过利用视频序列中的时间信息和语言的文本知识以及逐渐增加上下文大小的方法提高了AI辅助人类的能力。 |
| [^146] | [G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering](https://arxiv.org/abs/2402.07630) | 本论文提出了G-Retriever模型，该模型用于文本图理解和问题解答。该模型能够将用户的问题转化为文本回复，并突出显示图形的相关部分。与现有的方法不同，该模型适用于真实世界的文本图形，并可应用于不同的任务，包括场景图理解、常识推理和知识图推理。 |
| [^147] | [FedImpro: Measuring and Improving Client Update in Federated Learning](https://arxiv.org/abs/2402.07011) | 本文提出了FedImpro方法，通过生成改进的本地模型来减轻联邦学习中的客户漂移问题。该方法通过分析本地训练的泛化贡献，并利用类似的条件分布进行训练，增强了泛化贡献并减小了梯度的差异性。 |
| [^148] | [Classification of Volatile Organic Compounds by Differential Mobility Spectrometry Based on Continuity of Alpha Curves](https://arxiv.org/abs/2401.07066) | 差动迁移光谱法结合Alpha曲线连续性，首次将分散图解释为序列测量，可能为挥发性有机化合物的分类提供新的方法 |
| [^149] | [Plug-and-Play Regularization on Magnitude with Deep Priors for 3D Near-Field MIMO Imaging](https://arxiv.org/abs/2312.16024) | 本文提出了一种基于深度先验的近场3D MIMO成像中振幅正则化问题的插入和播放正则化方法，通过解决复值去噪问题实现了高效的重建方法。 |
| [^150] | [NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks](https://arxiv.org/abs/2312.10112) | NM-FlowGAN是一种利用生成对抗网络和正规化流的混合方法，旨在更准确地建模sRGB噪声，弥补了单一生成模型固有特性所带来的性能限制。 |
| [^151] | [A framework for conditional diffusion modelling with applications in motif scaffolding for protein design](https://arxiv.org/abs/2312.09236) | 该论文提出一种统一的条件扩散建模框架，基于Doob's h-transform，用于解决蛋白设计中的基序支架问题 |
| [^152] | [A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary Analysis](https://arxiv.org/abs/2312.05930) | 这项研究提出了一份全面的指甲皱纹毛细管数据集，并开发了一个自动化分析流水线，能够准确检测和测量各种指甲毛细管的特征，表现出色的精度。 |
| [^153] | [Innovations in Agricultural Forecasting: A Multivariate Regression Study on Global Crop Yield Prediction](https://arxiv.org/abs/2312.02254) | 通过实施六个回归模型并使用关键训练参数，该研究提出了一个随机森林回归模型，用于预测37个发展中国家27年的农作物产量，取得了0.94的确定系数，并探讨了不同因素对产量的影响。 |
| [^154] | [Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers](https://arxiv.org/abs/2311.17717) | Receler提出了一种可靠概念擦除方法，通过轻量级橡皮擦实现对文本到图像扩散模型的概念擦除，具备鲁棒性和局部性，实验证明其优越性。 |
| [^155] | [Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging](https://arxiv.org/abs/2311.11642) | 该论文提出了一个新颖的合成视频数据集，设计了基线架构来验证其有效性，并开发了针对视频重龄化技术的时间一致性的新颖评估指标。 |
| [^156] | [Mind the map! Accounting for existing map information when estimating online HDMaps from sensor](https://arxiv.org/abs/2311.10517) | 在估算HD地图时考虑现有地图信息是本文的创新，提出了三种有用的现有地图类型，并引入了MapEX框架，通过编码地图元素和优化匹配算法来实现，在nuScenes数据集上取得了显著改进。 |
| [^157] | [Plum: Prompt Learning using Metaheuristic](https://arxiv.org/abs/2311.08364) | 提出了使用元启发式的提示学习方法，通过测试六种典型的元启发式方法，在大型语言模型的提示优化任务中取得了有效性。 |
| [^158] | [Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers](https://arxiv.org/abs/2311.04744) | 该论文研究了基于欧几里得、射影和共形代数的Geometric Algebra Transformer架构，发现共形代数和改进的射影代数定义了功能强大、性能良好的变换器架构。 |
| [^159] | [LILO: Learning Interpretable Libraries by Compressing and Documenting Code](https://arxiv.org/abs/2310.19791) | LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。 |
| [^160] | [Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning](https://arxiv.org/abs/2310.12921) | 使用预训练的视觉语言模型作为零样本奖励模型，在强化学习中指定任务，提高训练效率。 |
| [^161] | [VeCLIP: Improving CLIP Training via Visual-enriched Captions](https://arxiv.org/abs/2310.07699) | 本研究提出了一种通过将视觉概念融入标题中的方式来改进CLIP训练的方法，名为VeCLIP，该方法在大规模网络爬取数据集上展示了良好的性能。 |
| [^162] | [iTransformer: Inverted Transformers Are Effective for Time Series Forecasting](https://arxiv.org/abs/2310.06625) | iTransformer通过重新利用Transformer架构，在时间序列预测中简单应用注意力和馈送，提高了性能并克服了其他模型在处理具有更大回溯窗口的系列时面临的挑战 |
| [^163] | [Think before you speak: Training Language Models With Pause Tokens](https://arxiv.org/abs/2310.02226) | 引入暂停标记的语言模型训练方法可以让模型在输出标记前处理更多隐藏向量，取得了较好的实验结果 |
| [^164] | [Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy](https://arxiv.org/abs/2310.01334) | 本文旨在探讨如何通过合并专家信息来制定出更紧凑但更具知识的SMoE模型，因为传统的模型合并方法并不适用于SMoE的专家合并。 |
| [^165] | [Making Language Models Better Tool Learners with Execution Feedback](https://arxiv.org/abs/2305.13068) | 这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。 |
| [^166] | [Energy Disaggregation & Appliance Identification in a Smart Home: Transfer Learning enables Edge Computing](https://arxiv.org/abs/2301.03018) | 迁移学习结合边缘计算，提出了新的深度学习方法解决非侵入式负载监视问题和电器识别问题，利用改进的CNN模型和预训练模型获得更准确的结果。 |
| [^167] | [Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models](https://arxiv.org/abs/2212.08399) | Transformer编码器模型在处理分类任务时，受到序列长度学习问题影响，可能导致模型过度依赖序列长度作为预测特征而非文本信息，本文提出了一些方法来减少这种影响。 |
| [^168] | [COMET: A Comprehensive Cluster Design Methodology for Distributed Deep Learning Training](https://arxiv.org/abs/2211.16648) | COMET提出了一种全面的集群设计方法论，用于研究并行化策略和关键集群资源配置对分布式DL训练性能的影响 |
| [^169] | [Plug and Play Active Learning for Object Detection](https://arxiv.org/abs/2211.11612) | PPAL是一个简单而有效的对象检测主动学习策略，通过两阶段的不确定性和多样性抽样相结合，克服了专门对象检测器架构集成困难的挑战。 |
| [^170] | [SVD-PINNs: Transfer Learning of Physics-Informed Neural Networks via Singular Value Decomposition](https://arxiv.org/abs/2211.08760) | 本文提出了一种通过奇异值分解实现物理信息神经网络的迁移学习方法（SVD-PINNs），有效解决了一类具有不同但相似右端项的高维PDEs。 |
| [^171] | [Learning New Tasks from a Few Examples with Soft-Label Prototypes](https://arxiv.org/abs/2210.17437) | 本研究提出了一种新的极端少样本学习方法，利用软标签原型从少量示例中学习新任务，在大型、高维和现实世界数据集上表现出色。 |
| [^172] | [DPAR: Decoupled Graph Neural Networks with Node-Level Differential Privacy](https://arxiv.org/abs/2210.04442) | 本研究提出了一种名为DPAR的分离图神经网络，能实现对GNNs进行节点级差分隐私，从而保护节点及其边缘。 |
| [^173] | [Deep Adaptation of Adult-Child Facial Expressions by Fusing Landmark Features](https://arxiv.org/abs/2209.08614) | 提出了通过领域自适应和融合LANDMARK特征来实现成人和儿童面部表情的深度调整，以便在教育、医疗保健、娱乐等领域中进行准确分类。 |
| [^174] | [Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation](https://arxiv.org/abs/2207.14000) | 提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。 |
| [^175] | [Stochastic Approximation with Decision-Dependent Distributions: Asymptotic Normality and Optimality](https://arxiv.org/abs/2207.04173) | 该论文分析了一种针对决策相关问题的随机逼近算法，在渐近意义下，算法的平均迭代与解之间的偏差是正态的，并且算法的性能在局部达到了最优水平。 |
| [^176] | [SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations](https://arxiv.org/abs/2206.14464) | SPI-GAN使用直线路径插值定义的增强型GAN去噪方法，能够在极大程度上减少采样时间，同时实现与SGMs相同的高采样质量和多样性。 |
| [^177] | [OpenXAI: Towards a Transparent Evaluation of Model Explanations](https://arxiv.org/abs/2206.11104) | OpenXAI 是一个开源框架，旨在评估和基准测试后续解释方法，提供了灵活的数据生成器、多种数据集和评估指标，用户可轻松扩展和比较不同解释方法。 |
| [^178] | [A semi-agnostic ansatz with variable structure for quantum machine learning](https://arxiv.org/abs/2103.06712) | 提出了一种 VAns (Variable Ansatz) 可变结构方法来构建 VQAs 的假设，通过在优化过程中以一种熟悉的方式增长和移除量子门来成功缓解了可训练性和噪音相关问题。 |
| [^179] | [Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness.](http://arxiv.org/abs/2401.15127) | 本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。 |
| [^180] | [Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations.](http://arxiv.org/abs/2401.11792) | 本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。 |
| [^181] | [LDReg: Local Dimensionality Regularized Self-Supervised Learning.](http://arxiv.org/abs/2401.10474) | 本文提出了一种叫做LDReg的本地维度正则化方法，用于解决自监督学习中的维度坍缩问题。通过增加局部内在维度，LDReg能够改善表示的性能。 |
| [^182] | [Most discriminative stimuli for functional cell type identification.](http://arxiv.org/abs/2401.05342) | 本文提出了一种使用最具区分性刺激物的优化聚类方法，成功地识别了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能细胞类型。 |
| [^183] | [Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features.](http://arxiv.org/abs/2401.03195) | 我们提出了一种使用迁移学习和时空特征的比特率和复杂度高效预测方法，通过利用预训练深度神经网络的特征图和预测的最小比特率来改进比特率梯度的效率，从而在视频行业中提供高质量视频并保持比特率的效率。 |
| [^184] | [Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning.](http://arxiv.org/abs/2311.00860) | 本文提出了一种用于物理约束操作学习的新型自动微分算法，通过零坐标移动（ZCS）的技巧，将所需导数的复杂度从“多根多叶”简化为“一根多叶”，从而显著提高了性能。 |
| [^185] | [Vanishing Gradients in Reinforcement Finetuning of Language Models.](http://arxiv.org/abs/2310.20703) | 本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。 |
| [^186] | [Brain decoding: toward real-time reconstruction of visual perception.](http://arxiv.org/abs/2310.19812) | 本研究提出了一种基于脑磁图（MEG）的脑解码方法，通过训练一个具有预训练嵌入、MEG模块和图像生成器的模型，在实时应用中实现了对视觉知觉的高时间分辨率解码，并在图像检索上取得了7倍的改进。 |
| [^187] | [Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation.](http://arxiv.org/abs/2310.18235) | 本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。 |
| [^188] | [The statistical thermodynamics of generative diffusion models.](http://arxiv.org/abs/2310.17467) | 本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。 |
| [^189] | [Stable Nonconvex-Nonconcave Training via Linear Interpolation.](http://arxiv.org/abs/2310.13459) | 本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。 |
| [^190] | [CacheGen: Fast Context Loading for Language Model Applications.](http://arxiv.org/abs/2310.07240) | CacheGen是一种用于语言模型应用的技术，通过对上下文进行压缩来减少LLM的网络获取和处理延迟。 |
| [^191] | [Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions.](http://arxiv.org/abs/2310.07174) | 本文提出了一种广义神经排序网络，其中采用了具有无误差且可微分的交换函数，同时使用了置换等变Transformer网络来捕捉输入之间的依赖关系。实验证明，该方法在各种排序基准上表现优于或与基准方法相当。 |
| [^192] | [Amortizing intractable inference in large language models.](http://arxiv.org/abs/2310.04363) | 本论文提出了一种使用摊销的贝叶斯推理从难以处理的后验分布中进行抽样的方法，并利用生成流网络来实现大规模语言模型的微调，从而解决了在这些模型中处理推理问题的限制。 |
| [^193] | [Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective.](http://arxiv.org/abs/2310.03556) | 本文介绍了一种使用Leave-One-Out最大对数似然目标稳定训练概率模型的方法，通过自适应核密度估计模型和留一法最大对数似然准则，解决了数据密度不均匀困难，并通过分配可学习权重扩展模型，加速了训练过程。 |
| [^194] | [Expected flow networks in stochastic environments and two-player zero-sum games.](http://arxiv.org/abs/2310.02779) | 该论文提出了预期流网络（EFlowNets）和对抗流网络（AFlowNets），分别应用于随机环境和双人零和游戏中。在随机任务中，EFlowNets表现优于其他方法，在双人游戏中，AFlowNets在自我对弈中找到了80%以上的最佳动作，并在竞赛中超过了AlphaZero。 |
| [^195] | [Delta-AI: Local objectives for amortized inference in sparse graphical models.](http://arxiv.org/abs/2310.02423) | Delta-AI算法提出了一种基于稀疏图模型的摊还推理方法，通过局部信用分配和离策略训练加快了训练速度。 |
| [^196] | [GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking.](http://arxiv.org/abs/2310.01794) | 本研究通过基准测试系统评估了基于扰动的GNN解释性方法，发现帕累托最优方法在噪声存在的情况下表现出卓越效力和稳定性。 |
| [^197] | [Graph Neural Architecture Search with GPT-4.](http://arxiv.org/abs/2310.01436) | 本文将GPT-4集成到图神经网络架构搜索（GNAS）中，提出了一种新的GPT-4基于的GNAS方法（GPT4GNAS），通过设计新的提示来引导GPT-4生成更准确的图神经网络，实验证明嵌入GPT-4到GNAS中优于现有方法。 |
| [^198] | [DyVal: Graph-informed Dynamic Evaluation of Large Language Models.](http://arxiv.org/abs/2309.17167) | DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。 |
| [^199] | [Era Splitting.](http://arxiv.org/abs/2309.14496) | 本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。 |
| [^200] | [K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling.](http://arxiv.org/abs/2309.11093) | 研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。 |
| [^201] | [Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers.](http://arxiv.org/abs/2309.10639) | 本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。 |
| [^202] | [Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks.](http://arxiv.org/abs/2309.07030) | 本文提出了两种基于最优输运的距离度量，用于比较有向图，并通过仿真图数据和单细胞RNA-seq数据推断的实际细胞间通讯图对其相对表现进行了评估。 |
| [^203] | [ConR: Contrastive Regularizer for Deep Imbalanced Regression.](http://arxiv.org/abs/2309.06651) | ConR是一种对比正则化器，通过建模全局和局部标签相似性，防止少数样本的特征被折叠到其多数邻居中，有效地处理深度不平衡回归问题。 |
| [^204] | [EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation.](http://arxiv.org/abs/2309.03244) | EGIC是一种增强的低位速率生成图像压缩方法，通过语义分割提供指导。它在失真感知和失真方向基线方法上表现优越，并具有较小的模型参数和优秀的插值特性。 |
| [^205] | [SLiMe: Segment Like Me.](http://arxiv.org/abs/2309.03179) | 基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。 |
| [^206] | [PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates.](http://arxiv.org/abs/2309.02014) | 本文介绍了一套基于草图的预条件随机梯度算法套件PROMISE，用于解决大规模凸优化问题，相比传统方法，在默认超参数下在机器学习问题上表现更优。 |
| [^207] | [Instruction Tuning for Large Language Models: A Survey.](http://arxiv.org/abs/2308.10792) | 本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。 |
| [^208] | [Precision and Recall Reject Curves for Classification.](http://arxiv.org/abs/2308.08381) | 该论文提出了一种在分类问题中评估精确度和召回率的拒绝曲线方法。使用感知量化的原型分类器来验证了该方法在不平衡数据集和医学实际数据上的有效性。 |
| [^209] | [Deep projection networks for learning time-homogeneous dynamical systems.](http://arxiv.org/abs/2307.09912) | 这篇论文介绍了一种利用深度投影网络学习时间齐次动力系统的有意义表示的方法。通过优化类似于经典相关分析的目标函数，避免了矩阵求逆的稳定性问题，并通过两种正则化方案进一步增强学习效果。 |
| [^210] | [milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing.](http://arxiv.org/abs/2306.17010) | milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。 |
| [^211] | [Kernelized Reinforcement Learning with Order Optimal Regret Bounds.](http://arxiv.org/abs/2306.07745) | 该论文提出了一种称为$\pi$-KRVI的乐观修改方法，并使用核岭回归进行强化学习中的非线性函数逼近。论文证明了在一般设置下第一个最优遗憾保证，并相对于现有最优结果实现了显着的多项式低差距。 |
| [^212] | [Controlling Text-to-Image Diffusion by Orthogonal Finetuning.](http://arxiv.org/abs/2306.07280) | 本文介绍了一种名为正交微调（OFT）的方法，可以有效地引导和控制大型文本到图像扩散模型，以执行不同的下游任务。我们还提出了约束正交微调（COFT），来提高微调的稳定性。这些方法能够保持语义生成能力并生成特定主题的图像。 |
| [^213] | [Expressive Losses for Verified Robustness via Convex Combinations.](http://arxiv.org/abs/2305.13991) | 通过基于凸组合的表达性损失，可以提高网络的对抗鲁棒性，最新的算法可以获得最先进的结果；这种方法通过对抗性攻击和IBP边界之间的简单凸组合进行实现。 |
| [^214] | [Model-free Reinforcement Learning of Semantic Communication by Stochastic Policy Gradient.](http://arxiv.org/abs/2305.03571) | 本论文利用随机策略梯度（SPG）强化学习，成功设计了一种无需通道模型的语义通信系统，能够传输意义而非精确版本，达到了信息速率节省的目的。 |
| [^215] | [A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems.](http://arxiv.org/abs/2304.14907) | 本文提出了一种用于求解光滑有界约束优化问题的内点算法。它使用基于随机梯度估计的搜索方向和内部邻域，能够在确定性和随机性设置下具有收敛保证，并且在实验中表现出优于传统方法的性能。 |
| [^216] | [Learning Melanocytic Cell Masks from Adjacent Stained Tissue.](http://arxiv.org/abs/2211.00646) | 本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。 |
| [^217] | [Log-linear Guardedness and its Implications.](http://arxiv.org/abs/2210.10012) | 本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。 |
| [^218] | [Kernelized Concept Erasure.](http://arxiv.org/abs/2201.12191) | 通过核化线性极小极大博弈，防止特定非线性对手预测概念，但无法彻底解决非线性编码的概念擦除问题。 |

# 详细

[^1]: 在球面和球上的可微分和加速小波变换

    Differentiable and accelerated wavelet transforms on the sphere and ball

    [https://rss.arxiv.org/abs/2402.01282](https://rss.arxiv.org/abs/2402.01282)

    本研究设计了新的高度可分布和自动可微分的方向小波变换，在球面和球上的信号处理中取得了显著的加速效果，同时保持高精度，具有重要的实际应用价值。

    

    方向小波字典是一种层次结构的表示方法，可以高效地捕捉和分割各种尺度、位置和方向上的信息。这种表示方法对于物理信号具有特殊的亲和性，因为物理信号通常具有高各向异性、局部多尺度结构。许多重要的物理信号在球形域上观测，例如宇宙学中的天空。借助最近在计算谐波分析方面的进展，我们设计了新的高度可分布和自动可微分的方向小波变换，应用于二维球面S^2和三维球体B^3=R^+ x S^2（通过将球面与径向半线结合而形成的空间）。与现有软件相比，我们观察到球面和球上信号的加速比分别高达300倍和21800倍，同时保持64位机器精度。这些算法不仅在加速领域取得了显著的进展，同时具有可微分性质。

    Directional wavelet dictionaries are hierarchical representations which efficiently capture and segment information across scale, location and orientation. Such representations demonstrate a particular affinity to physical signals, which often exhibit highly anisotropic, localised multiscale structure. Many physically important signals are observed over spherical domains, such as the celestial sky in cosmology. Leveraging recent advances in computational harmonic analysis, we design new highly distributable and automatically differentiable directional wavelet transforms on the $2$-dimensional sphere $\mathbb{S}^2$ and $3$-dimensional ball $\mathbb{B}^3 = \mathbb{R}^+ \times \mathbb{S}^2$ (the space formed by augmenting the sphere with the radial half-line). We observe up to a $300$-fold and $21800$-fold acceleration for signals on the sphere and ball, respectively, compared to existing software, whilst maintaining 64-bit machine precision. Not only do these algorithms dramatically acce
    
[^2]: Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

    Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models

    [https://arxiv.org/abs/2403.09635](https://arxiv.org/abs/2403.09635)

    提出了一个统一的信号传播理论，提供了控制transformer模型信号传播的公式，提出了DeepScaleLM初始化和缩放方案，使得可以训练非常深的模型，并发现深层模型在多个任务和数据集上胜过浅层模型。

    

    尽管transformer模型取得了巨大的成功，但在深度方面仍然很难扩展。本研究提出了一个统一的信号传播理论，并提供了控制transformer模型前向和反向信号矩的公式。我们的框架可以用于理解和缓解与高注意力分数相关的梯度消失/爆炸、秩坍缩和不稳定性。我们还提出了DeepScaleLM，一种初始化和缩放方案，通过该方案能够在模型中保持单位输出/梯度矩，从而使训练具有100多层的非常深模型成为可能。我们发现，transformer模型可以更深 - 我们的深层模型在语言建模、语音翻译和图像分类方面表现优异，包括仅编码器、仅解码器和编码器-解码器变体，适用于Pre-LN和Post-LN transformers，适用于多个数据集和模型大小。

    arXiv:2403.09635v1 Announce Type: cross  Abstract: In spite of their huge success, transformer models remain difficult to scale in depth. In this work, we develop a unified signal propagation theory and provide formulae that govern the moments of the forward and backward signal through the transformer model. Our framework can be used to understand and mitigate vanishing/exploding gradients, rank collapse, and instability associated with high attention scores. We also propose DeepScaleLM, an initialization and scaling scheme that conserves unit output/gradient moments throughout the model, enabling the training of very deep models with 100s of layers. We find that transformer models could be much deeper - our deep models with fewer parameters outperform shallow models in Language Modeling, Speech Translation, and Image Classification, across Encoder-only, Decoder-only and Encoder-Decoder variants, for both Pre-LN and Post-LN transformers, for multiple datasets and model sizes. These imp
    
[^3]: Quiet-STaR: 语言模型可以自己学会思考后再说话

    Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking

    [https://arxiv.org/abs/2403.09629](https://arxiv.org/abs/2403.09629)

    Quiet-STaR提出了一种新的泛化版本，在每个标记处生成解释未来文本的思考过程，从而改善预测能力

    

    写作和交谈时，人们有时会停下来思考。尽管以推理为重点的作品通常将推理框定为回答问题或完成代理任务的方法，但推理几乎都隐含在所有书面文本中。例如，这适用于证明中未明确说明的步骤，以及支撑对话的心智理论。在自学习推理者（STaR，Zelikman等，2022）中，通过从少量示例中推断来自问答中有用的思考，并学习那些导致正确答案的思考。这是一个高度受限制的环境--理想情况下, 一个语言模型可以学会从任意文本中推断未明确说明的思考。我们提出Quiet-STaR，这是STaR的一个泛化版本，其中语言模型学会在每个标记处生成解释未来文本的思考过程，从而改善其预测。我们解决了一些关键挑战，包括1）生成连续的计算成本

    arXiv:2403.09629v1 Announce Type: cross  Abstract: When writing and talking, people sometimes pause to think. Although reasoning-focused works have often framed reasoning as a method of answering questions or completing agentic tasks, reasoning is implicit in almost all written text. For example, this applies to the steps not stated between the lines of a proof or to the theory of mind underlying a conversation. In the Self-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned by inferring rationales from few-shot examples in question-answering and learning from those that lead to a correct answer. This is a highly constrained setting -- ideally, a language model could instead learn to infer unstated rationales in arbitrary text. We present Quiet-STaR, a generalization of STaR in which LMs learn to generate rationales at each token to explain future text, improving their predictions. We address key challenges, including 1) the computational cost of generating continu
    
[^4]: Make-Your-3D: 快速并一致的主体驱动3D内容生成

    Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation

    [https://arxiv.org/abs/2403.09625](https://arxiv.org/abs/2403.09625)

    Make-Your-3D是一种新颖的3D定制方法，可以在5分钟内从单个图像和文本描述中实现高保真、一致的3D内容生成，通过协调不同模型的分布来实现这一目标

    

    近年来，强调3D生成模型的强大力量，通过允许用户通过单个图像或自然语言引导3D内容生成过程，提供了新水平的创造灵活性。然而，现有的3D生成方法仍然面临着跨不同提示创建主体驱动3D内容的挑战。本文介绍了一种名为Make-Your-3D的新颖3D定制方法，可以从仅一张主题的单个图像和文本描述中个性化地生成高保真、一致的3D内容，仅需5分钟。我们的关键见解是协调多视扩散模型和特定身份的2D生成模型的分布，将它们与所需的3D主体的分布对齐。具体地，我们设计了一个共同进化框架来减少分布的差异，其中每个模型通过身份感知优化互相学习的过程。

    arXiv:2403.09625v1 Announce Type: cross  Abstract: Recent years have witnessed the strong power of 3D generation models, which offer a new level of creative flexibility by allowing users to guide the 3D content generation process through a single image or natural language. However, it remains challenging for existing 3D generation methods to create subject-driven 3D content across diverse prompts. In this paper, we introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 minutes. Our key insight is to harmonize the distributions of a multi-view diffusion model and an identity-specific 2D generative model, aligning them with the distribution of the desired 3D subject. Specifically, we design a co-evolution framework to reduce the variance of distributions, where each model undergoes a process of learning from the other through identity-aware optimizatio
    
[^5]: 最小化最优和计算高效的分布鲁棒离线强化学习算法

    Minimax Optimal and Computationally Efficient Algorithms for Distributionally Robust Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.09621](https://arxiv.org/abs/2403.09621)

    研究提出了最小化最优和计算高效的算法，为鲁棒离线强化学习中的函数逼近带来新颖视角，并展示了其与标准离线强化学习中函数逼近的区别。

    

    分布鲁棒离线强化学习（RL）寻求针对环境扰动的鲁棒策略训练，通过建模动态不确定性来调用函数逼近，当面对庞大的状态-动作空间时，这种RL需要考虑到动态不确定性，引入了基本的非线性和计算负担，这给分析和实际应用函数逼近提出了独特挑战。在基本设置下，提议最小化最优和计算高效的算法，实现函数逼近，并在鲁棒离线RL的背景下启动对实例相关次优性分析的研究。我们的结果揭示了鲁棒离线RL中的函数逼近本质上与标准离线RL中的函数逼近有明显区别，可能更加困难。我们的算法和理论结果至关重要地依赖于

    arXiv:2403.09621v1 Announce Type: cross  Abstract: Distributionally robust offline reinforcement learning (RL), which seeks robust policy training against environment perturbation by modeling dynamics uncertainty, calls for function approximations when facing large state-action spaces. However, the consideration of dynamics uncertainty introduces essential nonlinearity and computational burden, posing unique challenges for analyzing and practically employing function approximation. Focusing on a basic setting where the nominal model and perturbed models are linearly parameterized, we propose minimax optimal and computationally efficient algorithms realizing function approximation and initiate the study on instance-dependent suboptimality analysis in the context of robust offline RL. Our results uncover that function approximation in robust offline RL is essentially distinct from and probably harder than that in standard offline RL. Our algorithms and theoretical results crucially depen
    
[^6]: 通过结构化训练重新唤醒知识：从灾难性干扰中进行预期性恢复

    Reawakening knowledge: Anticipatory recovery from catastrophic interference via structured training

    [https://arxiv.org/abs/2403.09613](https://arxiv.org/abs/2403.09613)

    在结构化环境中依次微调的LLMs表现出预期行为，能够从遗忘中恢复，揭示了在过参数化网络中进行训练的新见解

    

    我们探讨了神经网络在一个结构化的非独立同分布设置中的训练动态，其中文档以固定重复序列的方式呈现。通常情况下，在一系列文档上训练时，网络会遭受灾难性干扰；然而，我们发现在这种设置下依次微调的LLMs表现出一种奇特且卓越的特性：它们表现出预期的行为，在再次遇到之前的文档时从遗忘中恢复过来。这种行为在架构扩展其参数数量时逐渐出现并变得更加稳健。通过全面的实验和可视化，我们揭示了在结构化环境中训练超参数网络的新见解。

    arXiv:2403.09613v1 Announce Type: cross  Abstract: We explore the training dynamics of neural networks in a structured non-IID setting where documents are presented cyclically in a fixed, repeated sequence. Typically, networks suffer from catastrophic interference when training on a sequence of documents; however, we discover a curious and remarkable property of LLMs fine-tuned sequentially in this setting: they exhibit anticipatory behavior, recovering from the forgetting on documents before encountering them again. The behavior emerges and becomes more robust as the architecture scales up its number of parameters. Through comprehensive experiments and visualizations, we uncover new insights into training over-parameterized networks in structured environments.
    
[^7]: 先算法计算的光学检测用于噪声鲁棒的视觉感知

    Compute-first optical detection for noise-resilient visual perception

    [https://arxiv.org/abs/2403.09612](https://arxiv.org/abs/2403.09612)

    光学信号处理能够提高视觉感知任务的检测噪声鲁棒性，通过空间重新分配光学信号来增强性能，为解决噪声和弱信号环境下的神经计算任务性能瓶颈提供了新思路。

    

    在视觉感知的背景下，场景中的光学信号通过探测器转换为电子领域中的图像数据，然后进行处理以提取视觉信息。然而，在噪声和弱信号环境（如夜视应用中的热成像）中，神经计算任务的性能面临着一个显著的瓶颈，原因是在嘈杂检测时数据质量固有的降级。在这里，我们提出了一种在检测之前进行光学信号处理的概念来解决这个问题。我们证明通过适当设计的线性变换器空间重新分配光学信号可以增强视觉感知任务的检测噪声鲁棒性，以MNIST分类为基准进行了评估。我们的想法得到了量化分析的支持，详细说明了信号集中与噪声鲁棒性之间的关系，以及其实际实施。

    arXiv:2403.09612v1 Announce Type: cross  Abstract: In the context of visual perception, the optical signal from a scene is transferred into the electronic domain by detectors in the form of image data, which are then processed for the extraction of visual information. In noisy and weak-signal environments such as thermal imaging for night vision applications, however, the performance of neural computing tasks faces a significant bottleneck due to the inherent degradation of data quality upon noisy detection. Here, we propose a concept of optical signal processing before detection to address this issue. We demonstrate that spatially redistributing optical signals through a properly designed linear transformer can enhance the detection noise resilience of visual perception tasks, as benchmarked with the MNIST classification. Our idea is supported by a quantitative analysis detailing the relationship between signal concentration and noise robustness, as well as its practical implementatio
    
[^8]: MM1：多模式LLM预训练的方法、分析与见解

    MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training

    [https://arxiv.org/abs/2403.09611](https://arxiv.org/abs/2403.09611)

    通过详细研究图像编码器、视觉语言连接器和预训练数据选择的重要性，确定了对于实现多个基准测试中最新潮的少样本结果至关重要的关键设计经验。

    

    在这项工作中，我们讨论了构建高性能的多模式大型语言模型（MLLMs）。具体来说，我们研究了各种架构组件和数据选择的重要性。通过对图像编码器、视觉语言连接器和各种预训练数据选择进行仔细和全面的消融实验，我们确定了几个关键的设计经验。例如，我们展示了对大规模多模式预训练使用仔细混合的图像标题、交替图像文本和仅文本数据对于在多个基准测试中实现最新潮（SOTA）的少样本结果至关重要，与其他已发表的预训练结果相比。此外，我们表明图像编码器连同图像分辨率和图像标记计数具有重要影响，而视觉语言连接器设计相对重要性较小。通过扩大所提出的方法，我们构建了MM1，一个多模式模型系列。

    arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
    
[^9]: 控制硬件非确定性进行乐观可验证训练

    Optimistic Verifiable Training by Controlling Hardware Nondeterminism

    [https://arxiv.org/abs/2403.09603](https://arxiv.org/abs/2403.09603)

    提出了一种方法，结合了在比目标模型更高精度下进行训练、在中间计算步骤后进行四舍五入，并基于自适应阈值存储四舍五入决策，以应对硬件非确定性对训练过程的影响。

    

    AI系统日益增加的计算需求导致了为缺乏必要资源的客户进行模型训练的服务的出现。然而，确保训练的正确性并防范潜在的训练时攻击，例如数据毒化，都带来了挑战。现有的关于可验证训练的工作主要分为两类：基于证明的系统，由于需要加密技术而难以扩展，以及考虑到一个可信第三方审计员复制训练过程的“乐观”方法。 后者的一个关键挑战是，在训练期间GPU类型之间的硬件非确定性阻止审计员精确复制训练过程，因此这样的方案不够健壮。我们提出了一种方法，将训练在比目标模型更高的精度下进行，中间计算步骤后四舍五入，基于自适应阈值存储四舍五入决策。

    arXiv:2403.09603v1 Announce Type: cross  Abstract: The increasing compute demands of AI systems has led to the emergence of services that train models on behalf of clients lacking necessary resources. However, ensuring correctness of training and guarding against potential training-time attacks, such as data poisoning, poses challenges. Existing works on verifiable training largely fall into two classes: proof-based systems, which struggle to scale due to requiring cryptographic techniques, and "optimistic" methods that consider a trusted third-party auditor who replicates the training process. A key challenge with the latter is that hardware nondeterminism between GPU types during training prevents an auditor from replicating the training process exactly, and such schemes are therefore non-robust. We propose a method that combines training in a higher precision than the target model, rounding after intermediate computation steps, and storing rounding decisions based on an adaptive thr
    
[^10]: 混合Mixup用于稀有无尾蛙声多标签分类

    Mixture of Mixups for Multi-label Classification of Rare Anuran Sounds

    [https://arxiv.org/abs/2403.09598](https://arxiv.org/abs/2403.09598)

    该论文提出了利用混合Mixup方法来解决稀有无尾蛙声多标签分类中的挑战，实验证明这种方法在处理类别不平衡和多标签示例方面具有有效性。

    

    多标签不平衡分类在机器学习中是一个重要挑战，特别在生物声学中尤为明显，动物声音经常同时出现，而某些声音比其他声音要少得多。本文针对使用包含类别不平衡和多标签示例的数据集AnuraSet，专注于分类无尾目物种声音的特定情况。为了解决这些挑战，我们引入了Mixture of Mixups (Mix2)，这是一个利用混合正则化方法Mixup、Manifold Mixup和MultiMix的框架。实验结果表明，这些方法单独使用可能导致次优结果；然而，当随机应用它们时，每次训练迭代选取一个方法，它们在解决提到的挑战方面表现出有效性，特别是对于发生次数较少的稀有类别。进一步分析表明，Mix2在跨各种类别同时出现水平上也能有效分类声音。

    arXiv:2403.09598v1 Announce Type: cross  Abstract: Multi-label imbalanced classification poses a significant challenge in machine learning, particularly evident in bioacoustics where animal sounds often co-occur, and certain sounds are much less frequent than others. This paper focuses on the specific case of classifying anuran species sounds using the dataset AnuraSet, that contains both class imbalance and multi-label examples. To address these challenges, we introduce Mixture of Mixups (Mix2), a framework that leverages mixing regularization methods Mixup, Manifold Mixup, and MultiMix. Experimental results show that these methods, individually, may lead to suboptimal results; however, when applied randomly, with one selected at each training iteration, they prove effective in addressing the mentioned challenges, particularly for rare classes with few occurrences. Further analysis reveals that Mix2 is also proficient in classifying sounds across various levels of class co-occurrences
    
[^11]: 迭代遗忘：使用数据库启发的自适应粒化进行在线数据流回归

    Iterative Forgetting: Online Data Stream Regression Using Database-Inspired Adaptive Granulation

    [https://arxiv.org/abs/2403.09588](https://arxiv.org/abs/2403.09588)

    提出了一种受数据库启发的在线数据流回归模型，通过迭代遗忘过时数据粒子，以保持只有最近的相关信息，从而实现低延迟预测。

    

    许多现代系统，如金融、交通和电信系统，在时间上敏感，要求实时决策需要低延迟的预测。这些系统通常必须处理连续的无界数据流以及概念漂移，这是传统回归技术无法解决的挑战性需求。我们提出了一种受数据库启发的数据流回归模型，它（a）借鉴了R*-树的灵感，从传入的数据流中创建粒子，以保留相关信息，（b）迭代性地遗忘被认为已过时的粒子，从而维护仅包含最近的相关粒子列表，（c）使用最近的数据和粒子提供低延迟预测。这种受R*-树启发的方法还使算法具有扩展性

    arXiv:2403.09588v1 Announce Type: new  Abstract: Many modern systems, such as financial, transportation, and telecommunications systems, are time-sensitive in the sense that they demand low-latency predictions for real-time decision-making. Such systems often have to contend with continuous unbounded data streams as well as concept drift, which are challenging requirements that traditional regression techniques are unable to cater to. There exists a need to create novel data stream regression methods that can handle these scenarios. We present a database-inspired datastream regression model that (a) uses inspiration from R*-trees to create granules from incoming datastreams such that relevant information is retained, (b) iteratively forgets granules whose information is deemed to be outdated, thus maintaining a list of only recent, relevant granules, and (c) uses the recent data and granules to provide low-latency predictions. The R*-tree-inspired approach also makes the algorithm amen
    
[^12]: 算法句法因果识别

    Algorithmic syntactic causal identification

    [https://arxiv.org/abs/2403.09580](https://arxiv.org/abs/2403.09580)

    通过替换传统概率论为对称单调范畴的替代基础，可以扩展因果识别技术到更多因果设置中。

    

    在因果贝叶斯网络（CBN）中进行因果识别是因果推断中的一项重要工具，允许从理论上可能的情况下的观测分布推导干预分布。然而，大多数现有的因果识别形式，如使用d分离和do-演算的技术都是在CBN上利用经典概率论的数学语言表达的。然而，在许多因果设置中，概率论和因此目前的因果识别技术不适用，如关系数据库、数据流程序（例如硬件描述语言）、分布式系统和大多数现代机器学习算法。我们表明，可以通过用对称单调范畴的替代公理基础来消除这种限制。在这种替代公理化中，我们展示了如何获得一个明确且清晰的

    arXiv:2403.09580v1 Announce Type: new  Abstract: Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean
    
[^13]: uaMix-MAE: 使用无监督音频混合高效调整预训练音频Transformer

    uaMix-MAE: Efficient Tuning of Pretrained Audio Transformers with Unsupervised Audio Mixtures

    [https://arxiv.org/abs/2403.09579](https://arxiv.org/abs/2403.09579)

    uaMix-MAE提出了一种高效的ID调整策略，利用无监督音频混合对预训练的MAEs进行对比调整，从而有效地适应特定任务语义。

    

    掩码自编码器（MAEs）能够从无标注数据中学习丰富的低级表示，但需要大量有标注数据才能有效地适应下游任务。而实例区分（ID）则强调高级语义，为减轻MAEs中的注释需求提供了潜在解决方案。本文介绍了一种称为uaMix-MAE的高效ID调整策略，利用无监督音频混合。通过对比调整，uaMix-MAE使预训练MAEs的表示对齐，从而促进有效地适应特定任务语义。为了在小量无标注数据优化模型，我们提出了一种音频混合技术，可以在输入和虚拟标签中操作音频样本。

    arXiv:2403.09579v1 Announce Type: cross  Abstract: Masked Autoencoders (MAEs) learn rich low-level representations from unlabeled data but require substantial labeled data to effectively adapt to downstream tasks. Conversely, Instance Discrimination (ID) emphasizes high-level semantics, offering a potential solution to alleviate annotation requirements in MAEs. Although combining these two approaches can address downstream tasks with limited labeled data, naively integrating ID into MAEs leads to extended training times and high computational costs. To address this challenge, we introduce uaMix-MAE, an efficient ID tuning strategy that leverages unsupervised audio mixtures. Utilizing contrastive tuning, uaMix-MAE aligns the representations of pretrained MAEs, thereby facilitating effective adaptation to task-specific semantics. To optimize the model with small amounts of unlabeled data, we propose an audio mixing technique that manipulates audio samples in both input and virtual label 
    
[^14]: 你是机器人吗？从行为分析中检测自动驾驶车辆

    Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis

    [https://arxiv.org/abs/2403.09571](https://arxiv.org/abs/2403.09571)

    提出了一个框架，通过监视车辆的行为和状态信息来自动识别自动驾驶车辆，无需车辆主动通知。

    

    自动驾驶技术的巨大热潮急切地呼唤新兴和创新技术，以支持先进的移动性使用案例。随着汽车制造商不断开发SAE 3级及以上系统来提高乘客的安全性和舒适性，交通管理机构需要建立新程序来管理从人工驾驶到完全自动驾驶车辆的过渡，并提供一个反馈机制来微调设想的自动驾驶系统。因此，自动对自动驾驶车辆进行自动分析并将其与人工驾驶车辆区分开是必要的。本文提出了一个完整的框架，通过监视使用摄像头图像和状态信息的活动车辆，以确定车辆是否为自动驾驶，而无需车辆主动通知。基本上，它依赖车辆之间的合作，这些车辆共享在道路上获取的数据，供机器学习使用。

    arXiv:2403.09571v1 Announce Type: cross  Abstract: The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learn
    
[^15]: 基于多保真度的贝叶斯优化方法及跨任务可转移的最大值熵搜索

    Multi-Fidelity Bayesian Optimization With Across-Task Transferable Max-Value Entropy Search

    [https://arxiv.org/abs/2403.09570](https://arxiv.org/abs/2403.09570)

    本文引入了一种新颖的信息理论获取函数，用于平衡在连续的优化任务中获得最优值或解信息的需求。

    

    在许多应用中，设计者面临一系列优化任务，任务的目标是昂贵评估的黑盒函数形式。本文介绍了一种新的信息理论获取函数，用于平衡需要获取不同任务的最优值或解的信息和通过参数的转移传递。

    arXiv:2403.09570v1 Announce Type: new  Abstract: In many applications, ranging from logistics to engineering, a designer is faced with a sequence of optimization tasks for which the objectives are in the form of black-box functions that are costly to evaluate. For example, the designer may need to tune the hyperparameters of neural network models for different learning tasks over time. Rather than evaluating the objective function for each candidate solution, the designer may have access to approximations of the objective functions, for which higher-fidelity evaluations entail a larger cost. Existing multi-fidelity black-box optimization strategies select candidate solutions and fidelity levels with the goal of maximizing the information accrued about the optimal value or solution for the current task. Assuming that successive optimization tasks are related, this paper introduces a novel information-theoretic acquisition function that balances the need to acquire information about the 
    
[^16]: 自洽训练用于哈密顿量预测

    Self-Consistency Training for Hamiltonian Prediction

    [https://arxiv.org/abs/2403.09560](https://arxiv.org/abs/2403.09560)

    提出了一种基于自洽原理的哈密顿量预测训练方法，无需标记数据，能够在大量未标记数据上训练，极大地增强了泛化能力，并提高了训练效率。

    

    arXiv:2403.09560v1 公告类型:新 提要: 哈密顿量预测是一种利用机器学习解决分子科学问题的多功能公式。然而，其适用性受到训练数据不足的限制。在这项工作中，我们强调哈密顿量预测具有自洽原理，基于此我们提出了一种不需要标记数据的精确训练方法。这一优点解决了数据稀缺困难，并将该任务与其他具有独特优势的属性预测公式区分开：（1）自洽训练使模型能够在大量未标记数据上训练，因此极大地增强了泛化能力；（2）自洽训练比使用DFT标记数据进行监督训练更有效，因为它是对一组分子结构上的DFT计算的摊销。我们通过实验证明，在数据稀缺和分布之外的情况下有更好的泛化能力。

    arXiv:2403.09560v1 Announce Type: new  Abstract: Hamiltonian prediction is a versatile formulation to leverage machine learning for solving molecular science problems. Yet, its applicability is limited by insufficient labeled data for training. In this work, we highlight that Hamiltonian prediction possesses a self-consistency principle, based on which we propose an exact training method that does not require labeled data. This merit addresses the data scarcity difficulty, and distinguishes the task from other property prediction formulations with unique benefits: (1) self-consistency training enables the model to be trained on a large amount of unlabeled data, hence substantially enhances generalization; (2) self-consistency training is more efficient than labeling data with DFT for supervised training, since it is an amortization of DFT calculation over a set of molecular structures. We empirically demonstrate the better generalization in data-scarce and out-of-distribution scenarios
    
[^17]: 将去噪推广到非平衡结构以改进等变力场

    Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields

    [https://arxiv.org/abs/2403.09549](https://arxiv.org/abs/2403.09549)

    将去噪方法推广到非平衡结构，从而改进等变力场的性能，提高了对原子间相互作用的理解以及在分子动力学和催化剂设计等领域的应用。

    

    理解原子间的相互作用，如3D原子体系中的力，对于许多应用如分子动力学和催化剂设计至关重要。然而，模拟这些相互作用需要计算密集的从头算计算，因此训练神经网络的数据有限。本文提出使用去噪非平衡结构（DeNS）作为辅助任务，以更好地利用训练数据并提高性能。在使用DeNS进行训练时，我们首先通过向其3D坐标添加噪声来破坏3D结构，然后预测噪声。不同于以往仅限于平衡结构的去噪工作，所提出的方法将去噪泛化到更大范围的非平衡结构。主要区别在于非平衡结构不对应于局部能量最小值，具有非零力，因此可能具有许多可能的原子位置。

    arXiv:2403.09549v1 Announce Type: cross  Abstract: Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamental to many applications like molecular dynamics and catalyst design. However, simulating these interactions requires compute-intensive ab initio calculations and thus results in limited data for training neural networks. In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to better leverage training data and improve performance. For training with DeNS, we first corrupt a 3D structure by adding noise to its 3D coordinates and then predict the noise. Different from previous works on denoising, which are limited to equilibrium structures, the proposed method generalizes denoising to a much larger set of non-equilibrium structures. The main difference is that a non-equilibrium structure does not correspond to local energy minima and has non-zero forces, and therefore it can have many possible atomic posit
    
[^18]: 使用梯度提升算法对乳腺癌进行分类，重点减少假阴性和使用 SHAP 进行解释性研究

    Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability

    [https://arxiv.org/abs/2403.09548](https://arxiv.org/abs/2403.09548)

    本研究使用梯度提升算法对乳腺癌进行分类，关注提高召回率，以实现更好的检测和预测效果。

    

    癌症是世界上夺走最多女性生命的疾病之一，其中乳腺癌占据了癌症病例和死亡人数最高的位置。然而，通过早期检测可以预防乳腺癌，从而进行早期治疗。许多研究关注的是在癌症预测中具有高准确性的模型，但有时仅依靠准确性可能并非始终可靠。本研究对使用提升技术基于不同机器学习算法预测乳腺癌的性能进行了调查性研究，重点关注召回率指标。提升机器学习算法已被证明是检测医学疾病的有效工具。利用加州大学尔湾分校 (UCI) 数据集对训练和测试模型分类器进行训练，其中包含各自属性。

    arXiv:2403.09548v1 Announce Type: new  Abstract: Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. Th
    
[^19]: 机器学习项目如何使用持续集成实践？GitHub Actions 上的经验性研究

    How do Machine Learning Projects use Continuous Integration Practices? An Empirical Study on GitHub Actions

    [https://arxiv.org/abs/2403.09547](https://arxiv.org/abs/2403.09547)

    本研究通过对GitHub上185个开源项目的分析，发现机器学习项目通常需要更长的构建时间，中等规模的项目测试覆盖率较低，并呈现出构建时间趋势增长的特点。

    

    持续集成（CI）是传统软件开发中一个成熟的实践，但在机器学习（ML）项目领域中的细节仍然相对未被探索。考虑到ML开发的独特性，了解CI实践在这一背景下的采用对于调整有效方法至关重要。本研究对GitHub上的185个开源项目进行全面分析（93个ML项目和92个非ML项目）。我们的调查涵盖了定量和定性维度，旨在揭示ML和非ML项目之间CI采用的差异。我们的发现表明，ML项目通常需要更长的构建时间，并且中等规模的ML项目与非ML项目相比具有较低的测试覆盖率。此外，小型和中等规模的ML项目显示出比其非ML对应项目更多的增长构建时间趋势的普遍性。

    arXiv:2403.09547v1 Announce Type: cross  Abstract: Continuous Integration (CI) is a well-established practice in traditional software development, but its nuances in the domain of Machine Learning (ML) projects remain relatively unexplored. Given the distinctive nature of ML development, understanding how CI practices are adopted in this context is crucial for tailoring effective approaches. In this study, we conduct a comprehensive analysis of 185 open-source projects on GitHub (93 ML and 92 non-ML projects). Our investigation comprises both quantitative and qualitative dimensions, aiming to uncover differences in CI adoption between ML and non-ML projects. Our findings indicate that ML projects often require longer build durations, and medium-sized ML projects exhibit lower test coverage compared to non-ML projects. Moreover, small and medium-sized ML projects show a higher prevalence of increasing build duration trends compared to their non-ML counterparts. Additionally, our qualita
    
[^20]: 纹理学习探索

    Explorations in Texture Learning

    [https://arxiv.org/abs/2403.09543](https://arxiv.org/abs/2403.09543)

    论文探索了纹理学习的关键，通过纹理-对象关联揭示了卷积神经网络中纹理和对象类别之间关系的新见解，并提出纹理学习对于解释性方法的新可能性和发现意想不到偏见的潜力。

    

    在这项工作中，我们研究了\textit{纹理学习}：即通过对象分类模型学习到的纹理的识别，以及它们在多大程度上依赖这些纹理。我们建立了纹理-对象关联，揭示了卷积神经网络中纹理和对象类别之间关系的新见解，并发现了三类结果：强大且预期的关联、强大但意想不到的关联，以及预期但不存在的关联。我们的分析表明，对纹理学习的探究使得解释性方面的新方法成为可能，并有潜力发现意想不到的偏见。

    arXiv:2403.09543v1 Announce Type: cross  Abstract: In this work, we investigate \textit{texture learning}: the identification of textures learned by object classification models, and the extent to which they rely on these textures. We build texture-object associations that uncover new insights about the relationships between texture and object classes in CNNs and find three classes of results: associations that are strong and expected, strong and not expected, and expected but not present. Our analysis demonstrates that investigations in texture learning enable new methods for interpretability and have the potential to uncover unexpected biases.
    
[^21]: API保护的LLMs的标志泄露专有信息

    Logits of API-Protected LLMs Leak Proprietary Information

    [https://arxiv.org/abs/2403.09539](https://arxiv.org/abs/2403.09539)

    大多数现代LLM受到softmax瓶颈影响，可以以较低成本获取API保护的LLM的非公开信息和解锁多种功能

    

    大型语言模型（LLMs）的商业化导致了高级API-only接入专有模型的常见实践。在这项工作中，我们展示了即使对于模型架构有保守的假设，也可以从相对较少的API查询中学习关于API保护的LLM的大量非公开信息（例如，使用OpenAI的gpt-3.5-turbo仅花费不到1000美元）。我们的发现集中在一个关键观察上：大多数现代LLM受到了softmax瓶颈的影响，这限制了模型输出到完整输出空间的线性子空间。我们表明，这导致了一个模型图像或模型签名，从而以较低的成本解锁了几种功能：有效发现LLM的隐藏大小，获取完整词汇输出，检测和消除不同模型更新，识别给定单个完整LLM输出的源LLM，以及...

    arXiv:2403.09539v1 Announce Type: cross  Abstract: The commercialization of large language models (LLMs) has led to the common practice of high-level API-only access to proprietary models. In this work, we show that even with a conservative assumption about the model architecture, it is possible to learn a surprisingly large amount of non-public information about an API-protected LLM from a relatively small number of API queries (e.g., costing under $1,000 for OpenAI's gpt-3.5-turbo). Our findings are centered on one key observation: most modern LLMs suffer from a softmax bottleneck, which restricts the model outputs to a linear subspace of the full output space. We show that this lends itself to a model image or a model signature which unlocks several capabilities with affordable cost: efficiently discovering the LLM's hidden size, obtaining full-vocabulary outputs, detecting and disambiguating different model updates, identifying the source LLM given a single full LLM output, and eve
    
[^22]: 利用典型表示减轻社会偏见而不使用人口统计信息

    Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information

    [https://arxiv.org/abs/2403.09516](https://arxiv.org/abs/2403.09516)

    通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。

    

    减轻社会偏见通常需要识别与每个数据样本相关联的社会群体。在本文中，我们提出了DAFair，一种新颖的方法来解决语言模型中的社会偏见问题。与依赖显式人口统计标签的传统方法不同，我们的方法不需要任何此类信息。相反，我们利用预定义的人口统计典型文本，并在微调过程中加入一个正则化项来减轻模型表示中的偏见。我们在两个任务和两个模型上的实证结果展示了我们的方法相对于之前不依赖标记数据的方法的有效性。此外，即使使用有限的人口统计标注数据，我们的方法也优于常见的去偏见方法。

    arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
    
[^23]: 在安全自主驾驶分布式开发中的STPA：一项访谈研究

    On STPA for Distributed Development of Safe Autonomous Driving: An Interview Study

    [https://arxiv.org/abs/2403.09509](https://arxiv.org/abs/2403.09509)

    STPA在汽车领域的应用面临可追溯性挑战，本文探讨了针对汽车行业的STPA指南。

    

    安全分析用于在安全相关功能的设计阶段识别危险并建立知识。这在复杂的AI-enabled和软件密集型系统如自主驾驶中尤为重要。系统论过程分析 (STPA) 是一种新颖方法，应用于防御和航空航天等安全相关领域，也在汽车行业中日益流行。然而，STPA 假设在具有分布式系统开发和多抽象设计层次的汽车系统工程中并非完全有效。这将阻碍软件开发人员使用STPA分析他们的软件作为更大系统的一部分，导致缺乏可追溯性。这可以看作是持续开发和部署中的可维护性挑战 (DevOps)。本文首先比较了面向汽车行业的STPA的不同指南，例如J31887/ISO21448/STPA手册。

    arXiv:2403.09509v1 Announce Type: cross  Abstract: Safety analysis is used to identify hazards and build knowledge during the design phase of safety-relevant functions. This is especially true for complex AI-enabled and software intensive systems such as Autonomous Drive (AD). System-Theoretic Process Analysis (STPA) is a novel method applied in safety-related fields like defense and aerospace, which is also becoming popular in the automotive industry. However, STPA assumes prerequisites that are not fully valid in the automotive system engineering with distributed system development and multi-abstraction design levels. This would inhibit software developers from using STPA to analyze their software as part of a bigger system, resulting in a lack of traceability. This can be seen as a maintainability challenge in continuous development and deployment (DevOps). In this paper, STPA's different guidelines for the automotive industry, e.g. J31887/ISO21448/STPA handbook, are firstly compare
    
[^24]: 不要以外表判断: 用于视频识别的运动一致增强

    Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition

    [https://arxiv.org/abs/2403.09506](https://arxiv.org/abs/2403.09506)

    本研究提出了一种名为运动一致增强（MCA）的数据增强方法，通过引入外观变化来鼓励模型优先考虑视频中的运动信息，而不是静态外观。

    

    当前目标识别中的训练流程在数据增强时忽略了色调抖动，因为它不仅会带来对分类有害的外观变化，而且在实践中实现也是低效的。本研究探讨了色调变化在视频识别中的影响，并发现这种变化是有益的，因为对于包含运动信息的视频来说，静态外观不是那么重要。基于这一观察结果，我们提出了一种用于视频识别的数据增强方法，名为运动一致增强（MCA），它在视频中引入外观变化，隐式鼓励模型优先考虑运动模式，而不是静态外观。具体来说，我们提出了一种名为SwapMix的操作，用于高效修改视频样本的外观，并引入了变异对齐（VA）来解决SwapMix引起的分布偏移，迫使模型去学习

    arXiv:2403.09506v1 Announce Type: cross  Abstract: Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to le
    
[^25]: EquiAV: 利用等变性进行音频-视觉对比学习

    EquiAV: Leveraging Equivariance for Audio-Visual Contrastive Learning

    [https://arxiv.org/abs/2403.09502](https://arxiv.org/abs/2403.09502)

    这项研究提出了一种利用等变性进行音频-视觉对比学习的新框架，通过一个共享的基于注意力的变换预测器来实现特征聚合和嵌入表示，有效提供了强大的监督，且计算开销最小。

    

    自我监督的音频-视觉表示学习最近取得了重大进展，展示出捕捉丰富综合表示的潜力。然而，尽管数据增强在许多学习方法中已经得到验证，音频-视觉学习仍然很难充分利用这些优势，因为增强可能会轻易破坏输入对之间的对应关系。为了解决这一限制，我们引入了EquiAV，一种利用等变性进行音频-视觉对比学习的新框架。我们的方法从扩展等变性开始进行音频-视觉学习，通过一个共享的基于注意力的变换预测器来促进。它使得来自不同增强的特征能够聚合到一个代表性的嵌入中，提供强大的监督。值得注意的是，这是在最小计算开销的情况下实现的。大量消融研究和定性结果验证了我们方法的有效性。

    arXiv:2403.09502v1 Announce Type: cross  Abstract: Recent advancements in self-supervised audio-visual representation learning have demonstrated its potential to capture rich and comprehensive representations. However, despite the advantages of data augmentation verified in many learning methods, audio-visual learning has struggled to fully harness these benefits, as augmentations can easily disrupt the correspondence between input pairs. To address this limitation, we introduce EquiAV, a novel framework that leverages equivariance for audio-visual contrastive learning. Our approach begins with extending equivariance to audio-visual learning, facilitated by a shared attention-based transformation predictor. It enables the aggregation of features from diverse augmentations into a representative embedding, providing robust supervision. Notably, this is achieved with minimal computational overhead. Extensive ablation studies and qualitative results verify the effectiveness of our method. 
    
[^26]: 使用Q学习的奶牛养殖场电池管理的强化学习方法

    A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning

    [https://arxiv.org/abs/2403.09499](https://arxiv.org/abs/2403.09499)

    该研究提出了一种基于Q学习的算法，以实现在奶牛养殖中整合可再生能源，以改善电池管理，应对电能消耗波动和能源价格波动的挑战。

    

    奶牛养殖消耗大量能源，是农业中一个能源密集型的部门。将可再生能源集成到奶牛养殖中可以帮助应对这一挑战。有效的电池管理对于整合可再生能源发电至关重要。管理电池的充电和放电由于电能消耗的波动、可再生能源发电的间歇性以及能源价格的波动而面临重大挑战。人工智能（AI）有潜力显著改善奶牛养殖中可再生能源的利用，然而在这一特定领域中进行的研究有限。本研究以爱尔兰作为案例研究，以实现其以可再生能源利用为核心的2030年能源战略。这项研究提出了一种基于Q学习的算法，用于安排电池的充电和放电。

    arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
    
[^27]: 利用机器学习算法进行摩托车碰撞检测

    On using Machine Learning Algorithms for Motorcycle Collision Detection

    [https://arxiv.org/abs/2403.09491](https://arxiv.org/abs/2403.09491)

    本文研究了机器学习算法在摩托车碰撞检测中的应用，通过数据收集和训练分类器，探讨了可靠地检测即将发生的碰撞的挑战。

    

    全球范围内，摩托车吸引了大量不同类型的用户。然而，由于摩托车事故中严重伤亡率远远超过客车事故，人们一直致力于增加被动安全系统。碰撞模拟显示，如果摩托车配备了诸如气囊和安全带等被动安全措施，那么在摩托车与汽车碰撞时，严重受伤或死亡的风险可以大大降低。为了使被动安全系统被激活，必须在毫秒内检测到碰撞，涉及各种碰撞配置，但绝不能误触发。为了可靠地检测即将发生的碰撞，本文介绍了一项关于机器学习算法适用性的研究。首先，引入了一系列事故和驾驶操作模拟来收集数据，以用于训练机器学习分类器。

    arXiv:2403.09491v1 Announce Type: new  Abstract: Globally, motorcycles attract vast and varied users. However, since the rate of severe injury and fatality in motorcycle accidents far exceeds passenger car accidents, efforts have been directed toward increasing passive safety systems. Impact simulations show that the risk of severe injury or death in the event of a motorcycle-to-car impact can be greatly reduced if the motorcycle is equipped with passive safety measures such as airbags and seat belts. For the passive safety systems to be activated, a collision must be detected within milliseconds for a wide variety of impact configurations, but under no circumstances may it be falsely triggered. For the challenge of reliably detecting impending collisions, this paper presents an investigation towards the applicability of machine learning algorithms. First, a series of simulations of accidents and driving operation is introduced to collect data to train machine learning classification m
    
[^28]: 从基本技能到复杂推理任务：先打好基础再说？

    Laying the Foundation First? Investigating the Generalization from Atomic Skills to Complex Reasoning Tasks

    [https://arxiv.org/abs/2403.09479](https://arxiv.org/abs/2403.09479)

    本文研究了语言模型从基本技能到复杂推理任务的泛化能力，并提出了分级课程学习训练策略，成功改进了模型在复杂推理任务上的性能。

    

    当前的语言模型已经展示了它们发展基本推理的能力，但在需要结合诸如算术和单位转换等基本技能的更复杂推理任务中往往困难重重。本文首先提出了一个探测框架，来研究基本技能能否自发地推广到复杂推理任务。然后，我们引入了一个分级课程学习训练策略，以实现更好的技能推广。在实验中，我们发现基本技能不能自发地推广到合成任务。通过利用分层课程学习，我们成功诱导了推广，显著改善了开源语言模型在复杂推理任务上的表现。

    arXiv:2403.09479v1 Announce Type: new  Abstract: Current language models have demonstrated their capability to develop basic reasoning, but struggle in more complicated reasoning tasks that require a combination of atomic skills, such as math word problem requiring skills like arithmetic and unit conversion. Previous methods either do not improve the inherent atomic skills of models or not attempt to generalize the atomic skills to complex reasoning tasks. In this paper, we first propose a probing framework to investigate whether the atomic skill can spontaneously generalize to complex reasoning tasks. Then, we introduce a hierarchical curriculum learning training strategy to achieve better skill generalization. In our experiments, we find that atomic skills can not spontaneously generalize to compositional tasks. By leveraging hierarchical curriculum learning, we successfully induce generalization, significantly improve the performance of open-source LMs on complex reasoning tasks. Pr
    
[^29]: 基于视觉、红外和超声波的神经辐射场——VIRUS-NeRF

    VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance Fields

    [https://arxiv.org/abs/2403.09477](https://arxiv.org/abs/2403.09477)

    VIRUS-NeRF是基于视觉、红外和超声波的神经辐射场，通过整合超声波和红外传感器的深度测量数据，实现了在自主移动机器人中达到与LiDAR点云相媲美的映射性能。

    

    自主移动机器人在现代工厂和仓库操作中起着越来越重要的作用。障碍物检测、回避和路径规划是关键的安全相关任务，通常使用昂贵的LiDAR传感器和深度摄像头来解决。我们提出使用成本效益的低分辨率测距传感器，如超声波和红外时间飞行传感器，通过开发基于视觉、红外和超声波的神经辐射场(VIRUS-NeRF)来解决这一问题。VIRUS-NeRF构建在瞬时神经图形基元与多分辨率哈希编码(Instant-NGP)的基础上，融合了超声波和红外传感器的深度测量数据，并利用它们来更新用于光线跟踪的占据网格。在2D实验评估中，VIRUS-NeRF实现了与LiDAR点云相媲美的映射性能，尤其在小型环境中，其准确性与LiDAR测量相符。

    arXiv:2403.09477v1 Announce Type: cross  Abstract: Autonomous mobile robots are an increasingly integral part of modern factory and warehouse operations. Obstacle detection, avoidance and path planning are critical safety-relevant tasks, which are often solved using expensive LiDAR sensors and depth cameras. We propose to use cost-effective low-resolution ranging sensors, such as ultrasonic and infrared time-of-flight sensors by developing VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance Fields. Building upon Instant Neural Graphics Primitives with a Multiresolution Hash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from ultrasonic and infrared sensors and utilizes them to update the occupancy grid used for ray marching. Experimental evaluation in 2D demonstrates that VIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds regarding coverage. Notably, in small environments, its accuracy aligns with that of LiDAR measurements, whi
    
[^30]: 易于难的泛化：超越人类监督的可扩展对齐

    Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision

    [https://arxiv.org/abs/2403.09472](https://arxiv.org/abs/2403.09472)

    通过从更简单的任务学习，实现对更难推理任务的有效泛化，提出了一种可扩展对齐方法。

    

    当前人工智能对齐方法依赖于人类提供的演示或判断，由于这种方法，AI系统学习到的能力将受到人类能力的上界限制。这就带来了一个具有挑战性的研究问题：当系统的能力超过人类水平时，我们如何继续改进这些系统？本文在解决难度推理任务（如4-5级数学问题）的背景下回答了这个问题，通过从更简单的任务（如1-3级数学问题）中学习人类注释，我们将其称为“易于难的泛化”。我们的关键观点是，一个在更简单任务的监督下训练的评估器（奖励模型）可以有效地用于评分更难任务的候选解决方案，从而促进在不同难度任务间的易于难的泛化。基于这一观点，我们提出了一种新的可扩展对齐方法，首先训练处理督导

    arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise
    
[^31]: 异常值鲁棒的多元多项式回归

    Outlier Robust Multivariate Polynomial Regression

    [https://arxiv.org/abs/2403.09465](https://arxiv.org/abs/2403.09465)

    将一维的解法推广到多维，提出了一种在多元多项式回归中对异常值具有鲁棒性的算法。

    

    我们研究了异常值鲁棒的多元多项式回归问题：设$p\colon\mathbb{R}^n\to\mathbb{R}$是一个未知的在每个变量上至多为$d$次的$n$元多项式。给定一组随机样本$(\mathbf{x}_i,y_i) \in [-1,1]^n \times \mathbb{R}$，它们是$(\mathbf{x}_i,p(\mathbf{x}_i))$的带噪声版本。更具体地，每个$\mathbf{x}_i$独立地从$[-1,1]^n$上的某个分布$\chi$中采样，对每个$i$，$y_i$以概率至多$\rho < 1/2$是任意的（即异常值），否则满足$|y_i-p(\mathbf{x}_i)|\leq\sigma$。目标是输出一个多元多项式$\hat{p}$，在每个变量上至多为$d$次，在$\ell_\infty$范数下与$p$的距离至多为$O(\sigma)$。

    arXiv:2403.09465v1 Announce Type: cross  Abstract: We study the problem of robust multivariate polynomial regression: let $p\colon\mathbb{R}^n\to\mathbb{R}$ be an unknown $n$-variate polynomial of degree at most $d$ in each variable. We are given as input a set of random samples $(\mathbf{x}_i,y_i) \in [-1,1]^n \times \mathbb{R}$ that are noisy versions of $(\mathbf{x}_i,p(\mathbf{x}_i))$. More precisely, each $\mathbf{x}_i$ is sampled independently from some distribution $\chi$ on $[-1,1]^n$, and for each $i$ independently, $y_i$ is arbitrary (i.e., an outlier) with probability at most $\rho < 1/2$, and otherwise satisfies $|y_i-p(\mathbf{x}_i)|\leq\sigma$. The goal is to output a polynomial $\hat{p}$, of degree at most $d$ in each variable, within an $\ell_\infty$-distance of at most $O(\sigma)$ from $p$.   Kane, Karmalkar, and Price [FOCS'17] solved this problem for $n=1$. We generalize their results to the $n$-variate setting, showing an algorithm that achieves a sample complexity 
    
[^32]: 通过影响区域的机器学习实现连续梁系统的结构设计模型

    Machine learning for structural design models of continuous beam systems via influence zones

    [https://arxiv.org/abs/2403.09454](https://arxiv.org/abs/2403.09454)

    通过影响区域概念，该研究提出了一个非迭代的机器学习结构设计模型，能够准确预测连续梁系统的截面要求，并在测试中表现出很好的泛化能力。

    

    这项工作从逆问题的角度为连续梁系统开发了一个机器学习的结构设计模型。在划分前向、优化和逆向机器学习算子之后，本研究提出了一种基于最近发展的影响区域概念的新方法，与传统的结构设计方法相比，这代表了一种基本的方法变革。该方法的目标是构思一个非迭代的结构设计模型，用于预测任意系统大小的连续梁系统的截面要求。在生成已知解的数据集之后，确定了适当的神经网络架构，并对其进行训练，然后针对未知数据进行测试。结果显示，对于截面属性预测，平均绝对百分比测试误差为1.6%，并且神经网络表现出良好的泛化能力，能够很好地适用于不同规模的结构系统。

    arXiv:2403.09454v1 Announce Type: new  Abstract: This work develops a machine learned structural design model for continuous beam systems from the inverse problem perspective. After demarcating between forward, optimisation and inverse machine learned operators, the investigation proposes a novel methodology based on the recently developed influence zone concept which represents a fundamental shift in approach compared to traditional structural design methods. The aim of this approach is to conceptualise a non-iterative structural design model that predicts cross-section requirements for continuous beam systems of arbitrary system size. After generating a dataset of known solutions, an appropriate neural network architecture is identified, trained, and tested against unseen data. The results show a mean absolute percentage testing error of 1.6% for cross-section property predictions, along with a good ability of the neural network to generalise well to structural systems of variable si
    
[^33]: 震动泄密：微调扩散模型可能增加生成隐私风险

    Shake to Leak: Fine-tuning Diffusion Models Can Amplify the Generative Privacy Risk

    [https://arxiv.org/abs/2403.09450](https://arxiv.org/abs/2403.09450)

    微调扩散模型可能会增加生成隐私风险，甚至使最先进的成员推理攻击对扩散模型的攻击效果提高5.4％，并可将提取的私有样本数量从几乎0个增加到平均16.3个。

    

    虽然扩散模型最近在生成逼真图像方面取得了显著进展，但也存在隐私风险：已发布的模型或API可能生成训练图像，从而泄露涉及隐私的训练信息。本文揭示了一种新风险，即Shake-to-Leak (S2L)，即使用操纵数据微调预训练模型可能会增加现有的隐私风险。我们证明了S2L可能发生在各种标准的扩散模型微调策略中，包括概念注入方法（DreamBooth和Textual Inversion）和参数高效方法（LoRA和Hypernetwork），以及它们的组合。在最糟糕的情况下，S2L可以将扩散模型上的最新成员推理攻击（MIA）的AUC提高5.4％（绝对差异），并且可以将每个目标域的提取私有样本从几乎0个样本增加到平均16.3个样本。这一发现强调了

    arXiv:2403.09450v1 Announce Type: new  Abstract: While diffusion models have recently demonstrated remarkable progress in generating realistic images, privacy risks also arise: published models or APIs could generate training images and thus leak privacy-sensitive training information. In this paper, we reveal a new risk, Shake-to-Leak (S2L), that fine-tuning the pre-trained models with manipulated data can amplify the existing privacy risks. We demonstrate that S2L could occur in various standard fine-tuning strategies for diffusion models, including concept-injection methods (DreamBooth and Textual Inversion) and parameter-efficient methods (LoRA and Hypernetwork), as well as their combinations. In the worst case, S2L can amplify the state-of-the-art membership inference attack (MIA) on diffusion models by $5.4\%$ (absolute difference) AUC and can increase extracted private samples from almost $0$ samples to $16.3$ samples on average per target domain. This discovery underscores that
    
[^34]: 对压缩神经网络进行对抗微调，共同提高鲁棒性和效率

    Adversarial Fine-tuning of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency

    [https://arxiv.org/abs/2403.09441](https://arxiv.org/abs/2403.09441)

    本研究探讨了对压缩神经网络进行对抗微调对提高鲁棒性和效率的影响。

    

    随着深度学习（DL）模型越来越多地融入我们的日常生活中，确保它们的安全性，使其对抗对抗性攻击具有鲁棒性变得越来越关键。我们在这项研究中探讨了两种不同的模型压缩方法 -- 结构化权重剪枝和量化对抗鲁棒性的影响。我们特别研究了对压缩模型进行微调的效果，并提出了一种同时提高鲁棒性和效率的方法。

    arXiv:2403.09441v1 Announce Type: new  Abstract: As deep learning (DL) models are increasingly being integrated into our everyday lives, ensuring their safety by making them robust against adversarial attacks has become increasingly critical. DL models have been found to be susceptible to adversarial attacks which can be achieved by introducing small, targeted perturbations to disrupt the input data. Adversarial training has been presented as a mitigation strategy which can result in more robust models. This adversarial robustness comes with additional computational costs required to design adversarial attacks during training. The two objectives -- adversarial robustness and computational efficiency -- then appear to be in conflict of each other. In this work, we explore the effects of two different model compression methods -- structured weight pruning and quantization -- on adversarial robustness. We specifically explore the effects of fine-tuning on compressed models, and present th
    
[^35]: 具有顺序样本均值逼近的变分推断

    Variational Inference with Sequential Sample-Average Approximations

    [https://arxiv.org/abs/2403.09429](https://arxiv.org/abs/2403.09429)

    VISA方法通过顺序样本均值逼近在计算密集型模型中实现近似推断，能够在保守选择学习率的情况下以较小的计算成本达到与标准方法相当的逼近精度。

    

    我们提出了一种具有顺序样本均值逼近（VISA）的变分推断方法，用于在计算密集型模型中进行近似推断，例如基于数值模拟的模型。VISA通过采用一系列样本均值逼近来扩展重要性加权的前向KL变分推断，这些逼近在信任区域内被视为有效。这使得可以在多个梯度步骤中重复使用模型评估，从而降低计算成本。我们在高维高斯分布、Lotka-Volterra动力学和Pickover吸引子上进行实验，结果表明，VISA可以在选择保守的学习率的情况下，以两倍或更高的计算节约达到与标准重要性加权前向KL变分推断相当的逼近精度。

    arXiv:2403.09429v1 Announce Type: cross  Abstract: We present variational inference with sequential sample-average approximation (VISA), a method for approximate inference in computationally intensive models, such as those based on numerical simulations. VISA extends importance-weighted forward-KL variational inference by employing a sequence of sample-average approximations, which are considered valid inside a trust region. This makes it possible to reuse model evaluations across multiple gradient steps, thereby reducing computational cost. We perform experiments on high-dimensional Gaussians, Lotka-Volterra dynamics, and a Pickover attractor, which demonstrate that VISA can achieve comparable approximation accuracy to standard importance-weighted forward-KL variational inference with computational savings of a factor two or more for conservatively chosen learning rates.
    
[^36]: 与邻居借宝：用于多模态学习的上下文学习，解决缺失模态和数据稀缺问题

    Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity

    [https://arxiv.org/abs/2403.09428](https://arxiv.org/abs/2403.09428)

    本文提出了一种检索增强的上下文学习框架，旨在解决多模态学习中缺失模态和数据稀缺问题。

    

    缺失模态的多模态机器学习是一个越来越相关的挑战，在各种应用中如医疗保健。本文将现有关于缺失模态的研究拓展到低数据情境，即一个下游任务既存在缺失模态又存在样本数量有限的问题。我们建议使用检索增强的上下文学习来解决这两个关键问题，释放变压器在上下文学习能力方面的潜力。与现有方法不同，我们的工作利用现有的全模态数据的价值，提供了解决挑战的新视角。

    arXiv:2403.09428v1 Announce Type: new  Abstract: Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited sample size issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training samples. We propose to use retrieval-augmented in-context learning to address these two crucial issues by unleashing the potential of a transformer's in-context learning ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training samples, our work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a high
    
[^37]: 通过自由漫游眼动数据进行用户识别

    User Identification via Free Roaming Eye Tracking Data

    [https://arxiv.org/abs/2403.09415](https://arxiv.org/abs/2403.09415)

    通过自由漫游眼动数据，本研究在非实验室环境中进行了用户识别研究，并在径向基函数网络分类器的支持下取得了相对较高的识别准确率。

    

    我们提出了一个新的数据集“自由漫游”(FR)和“定向漫游”(TR)：41名参与者被要求在大学校园里四处走动(FR)，或者被要求在图书馆内找到特定房间(TR)。使用Pupil Labs Neon眼动设备记录眼动。在这个数据集上，我们研究了使用以前已知的机器学习流程进行用户识别的准确性，其中使用径向基函数网络(RBFN)作为分类器。我们在FR和TR中获得的最高准确率分别为87.3%和89.4%。我们的结果应与我们所知道的最高准确率95.3%进行比较(该准确率是在实验室环境中使用BioEye 2015竞赛数据集的“RAN”刺激获得的)。据我们所知，我们的研究结果是第一个在非实验室环境中研究用户识别的结果；这种环境通常比实验室环境更具可行性，可能包含更多的进展。

    arXiv:2403.09415v1 Announce Type: new  Abstract: We present a new dataset of "free roaming" (FR) and "targeted roaming" (TR): a pool of 41 participants is asked to walk around a university campus (FR) or is asked to find a particular room within a library (TR). Eye movements are recorded using a commodity wearable eye tracker (Pupil Labs Neon at 200Hz). On this dataset we investigate the accuracy of user identification using a previously known machine learning pipeline where a Radial Basis Function Network (RBFN) is used as classifier. Our highest accuracies are 87.3% for FR and 89.4% for TR. This should be compared to 95.3% which is the (corresponding) highest accuracy we are aware of (achieved in a laboratory setting using the "RAN" stimulus of the BioEye 2015 competition dataset). To the best of our knowledge, our results are the first that study user identification in a non laboratory setting; such settings are often more feasible than laboratory settings and may include further ad
    
[^38]: LM2D：以歌词和音乐驱动的舞蹈合成

    LM2D: Lyrics- and Music-Driven Dance Synthesis

    [https://arxiv.org/abs/2403.09407](https://arxiv.org/abs/2403.09407)

    提出了LM2D，一种结合了多模态扩散模型和一致性蒸馏的新颖概率架构，旨在在音乐和歌词条件下进行舞蹈生成；介绍了第一个涵盖音乐和歌词的3D舞蹈运动数据集。

    

    舞蹈通常涉及专业编舞，包含按照音乐节奏进行的复杂动作，还可能受到歌词内容的影响。将歌词整合到听觉维度之外，丰富了基础音色，并使运动生成更易于语义含义。然而，现有的舞蹈合成方法往往只建模于音频信号。在这项工作中，我们做出了两点贡献，首先，我们提出了LM2D，这是一个结合了多模态扩散模型和一致性蒸馏的新颖概率架构，旨在通过一次扩散生成步骤在音乐和歌词的条件下创建舞蹈。其次，我们介绍了第一个涵盖音乐和歌词的3D舞蹈运动数据集，通过姿势估计技术获得。我们通过客观指标和人类评价将我们的模型与仅有音乐的基线模型进行评估。

    arXiv:2403.09407v1 Announce Type: cross  Abstract: Dance typically involves professional choreography with complex movements that follow a musical rhythm and can also be influenced by lyrical content. The integration of lyrics in addition to the auditory dimension, enriches the foundational tone and makes motion generation more amenable to its semantic meanings. However, existing dance synthesis methods tend to model motions only conditioned on audio signals. In this work, we make two contributions to bridge this gap. First, we propose LM2D, a novel probabilistic architecture that incorporates a multimodal diffusion model with consistency distillation, designed to create dance conditioned on both music and lyrics in one diffusion generation step. Second, we introduce the first 3D dance-motion dataset that encompasses both music and lyrics, obtained with pose estimation technologies. We evaluate our model against music-only baseline models with objective metrics and human evaluations, i
    
[^39]: 使用非线性系统理论学习具有收敛保证的优化

    Learning to optimize with convergence guarantees using nonlinear system theory

    [https://arxiv.org/abs/2403.09389](https://arxiv.org/abs/2403.09389)

    基于非线性系统理论，提出了一种对于平滑非凸目标函数的所有收敛算法的无约束参数化方法

    

    越来越多地依赖于数字方法来控制动态系统和训练机器学习模型，突显了需要设计可靠且高效地遍历复杂优化空间的算法。传统的梯度下降方法对凸问题提供了强大的理论保证；然而，对于非凸问题则需要精细调整超参数。学习优化(L2O)的新兴范式自动发现具有优化性能的算法，利用学习模型和数据，但缺乏分析所学算法的收敛性和稳健性的理论框架。本文通过利用非线性系统理论填补了这一空白。具体来说，我们提出了对于平滑非凸目标函数的所有收敛算法的无约束参数化。值得注意的是，我们的框架与自动微分工具直接兼容，确保...

    arXiv:2403.09389v1 Announce Type: cross  Abstract: The increasing reliance on numerical methods for controlling dynamical systems and training machine learning models underscores the need to devise algorithms that dependably and efficiently navigate complex optimization landscapes. Classical gradient descent methods offer strong theoretical guarantees for convex problems; however, they demand meticulous hyperparameter tuning for non-convex ones. The emerging paradigm of learning to optimize (L2O) automates the discovery of algorithms with optimized performance leveraging learning models and data - yet, it lacks a theoretical framework to analyze convergence and robustness of the learned algorithms. In this paper, we fill this gap by harnessing nonlinear system theory. Specifically, we propose an unconstrained parametrization of all convergent algorithms for smooth non-convex objective functions. Notably, our framework is directly compatible with automatic differentiation tools, ensurin
    
[^40]: Pantypes: 代表各种类型的自解释模型

    Pantypes: Diverse Representatives for Self-Explainable Models

    [https://arxiv.org/abs/2403.09383](https://arxiv.org/abs/2403.09383)

    引入了pantypes，一种旨在通过一组稀疏对象捕获输入分布的全部多样性的原型对象家族，可以大大增强原型自解释模型。

    

    随着对可解释人工智能系统日益增长的需求，原型自解释分类器已经出现。这些分类器旨在通过基于与学习的原型对象的相似性进行推理，以在决策中融入高透明度。虽然这些模型在设计时考虑了多样性，但学习的原型对象通常并不足以充分代表所有输入分布的方面，尤其是低密度区域中的方面。这种不足的数据表示，即表示偏差，已经与与机器学习多样性和公平性相关的各种有害特性联系起来。鉴于此，我们引入了pantypes，一种旨在通过一组稀疏对象捕获输入分布的全部多样性的原型对象家族。我们展示pantypes可以通过占据潜在空间中的不同区域来增强原型自解释模型

    arXiv:2403.09383v1 Announce Type: cross  Abstract: Prototypical self-explainable classifiers have emerged to meet the growing demand for interpretable AI systems. These classifiers are designed to incorporate high transparency in their decisions by basing inference on similarity with learned prototypical objects. While these models are designed with diversity in mind, the learned prototypes often do not sufficiently represent all aspects of the input distribution, particularly those in low density regions. Such lack of sufficient data representation, known as representation bias, has been associated with various detrimental properties related to machine learning diversity and fairness. In light of this, we introduce pantypes, a new family of prototypical objects designed to capture the full diversity of the input distribution through a sparse set of objects. We show that pantypes can empower prototypical self-explainable models by occupying divergent regions of the latent space and thu
    
[^41]: BurstAttention：一种用于处理极长序列的高效分布式注意力框架

    BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences

    [https://arxiv.org/abs/2403.09347](https://arxiv.org/abs/2403.09347)

    BurstAttention是一种用于优化内存访问和通信操作的分布式注意力框架，适用于处理极长序列。

    

    有效的注意力模块在基于Transformer的大型语言模型（LLMs）的成功中起着关键作用，但这些注意力模块的二次时间和内存复杂度在处理长序列时也构成了挑战。本文提出了一种名为“BurstAttention”的分布式注意力框架，以优化全局集群和本地设备级别的内存访问和通信操作。

    arXiv:2403.09347v1 Announce Type: cross  Abstract: Effective attention modules have played a crucial role in the success of Transformer-based large language models (LLMs), but the quadratic time and memory complexities of these attention modules also pose a challenge when processing long sequences. One potential solution for the long sequence problem is to utilize distributed clusters to parallelize the computation of attention modules across multiple devices (e.g., GPUs). However, adopting a distributed approach inevitably introduces extra memory overheads to store local attention results and incurs additional communication costs to aggregate local results into global ones. In this paper, we propose a distributed attention framework named ``BurstAttention'' to optimize memory access and communication operations at both the global cluster and local device levels. In our experiments, we compare BurstAttention with other competitive distributed attention solutions for long sequence proce
    
[^42]: 一种用于图像分类的层级融合量子模糊神经网络

    A Hierarchical Fused Quantum Fuzzy Neural Network for Image Classification

    [https://arxiv.org/abs/2403.09318](https://arxiv.org/abs/2403.09318)

    提出了一种新颖的层级融合量子模糊神经网络（HQFNN），通过使用量子神经网络学习模糊成员函数，在不确定数据分类任务中表现出色。

    

    神经网络是大数据时代数据特征学习的强大学习范式，然而大多数神经网络模型是忽略数据不确定性的确定性模型。模糊神经网络被提出来解决这个问题。FDNN是层级深度神经网络，从模糊和神经表示中提取信息，然后融合这些表示形成待分类的表示。FDNN在不确定数据分类任务上表现良好。在本文中，我们提出了一种新颖的层级融合量子模糊神经网络（HQFNN）。与经典的FDNN不同，HQFNN使用量子神经网络来学习模糊神经网络中的模糊成员函数。我们在两种数据集（Dirty-MNIST和15-Scene）上进行了模拟实验，结果显示所提出的模型可以胜过几种现有方法。此外，我们展示了所提出的模型的鲁棒性。

    arXiv:2403.09318v1 Announce Type: cross  Abstract: Neural network is a powerful learning paradigm for data feature learning in the era of big data. However, most neural network models are deterministic models that ignore the uncertainty of data. Fuzzy neural networks are proposed to address this problem. FDNN is a hierarchical deep neural network that derives information from both fuzzy and neural representations, the representations are then fused to form representation to be classified. FDNN perform well on uncertain data classification tasks. In this paper, we proposed a novel hierarchical fused quantum fuzzy neural network (HQFNN). Different from classical FDNN, HQFNN uses quantum neural networks to learn fuzzy membership functions in fuzzy neural network. We conducted simulated experiment on two types of datasets (Dirty-MNIST and 15-Scene), the results show that the proposed model can outperform several existing methods. In addition, we demonstrate the robustness of the proposed q
    
[^43]: 用理论视角重新思考医学异常检测中的自编码器

    Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical Perspective

    [https://arxiv.org/abs/2403.09303](https://arxiv.org/abs/2403.09303)

    该研究从理论角度为医学异常检测中基于自编码器的重建方法提供了基础，揭示了改进AE在异常检测中的关键在于最小化信息。

    

    医学异常检测旨在仅使用正常训练数据识别异常发现，对健康筛查和识别罕见疾病至关重要。基于重建的方法，特别是利用自编码器（AEs）的方法在这一领域占主导地位。它们基于这样的假设工作：仅使用正常数据训练的AEs不能很好地重建看不见的异常区域，从而实现基于重建错误的异常检测。然而，由于重建训练目标与异常检测任务目标之间的不匹配，这一假设并不总是成立，使得这些方法在理论上不够合理。该研究侧重于为基于AE的重建方法在异常检测中提供理论基础。通过利用信息论，我们阐明了这些方法的原则，并揭示了改进AE在异常检测中的关键在于最小化信息。

    arXiv:2403.09303v1 Announce Type: new  Abstract: Medical anomaly detection aims to identify abnormal findings using only normal training data, playing a crucial role in health screening and recognizing rare diseases. Reconstruction-based methods, particularly those utilizing autoencoders (AEs), are dominant in this field. They work under the assumption that AEs trained on only normal data cannot reconstruct unseen abnormal regions well, thereby enabling the anomaly detection based on reconstruction errors. However, this assumption does not always hold due to the mismatch between the reconstruction training objective and the anomaly detection task objective, rendering these methods theoretically unsound. This study focuses on providing a theoretical foundation for AE-based reconstruction methods in anomaly detection. By leveraging information theory, we elucidate the principles of these methods and reveal that the key to improving AE in anomaly detection lies in minimizing the informati
    
[^44]: StainFuser：在多吉加像素组织学图像中控制扩散以加快神经风格迁移

    StainFuser: Controlling Diffusion for Faster Neural Style Transfer in Multi-Gigapixel Histology Images

    [https://arxiv.org/abs/2403.09302](https://arxiv.org/abs/2403.09302)

    StainFuser提出了一种新颖的条件潜在扩散架构，将染色标准化问题视为风格迁移任务，无需手工制作颜色组分，在2百万多个组织学图像上训练的结果优于当前最先进的方法。

    

    染色标准化算法旨在将源多吉加像素组织学图像的颜色和强度特征转换为与目标图像相匹配，从而减轻图像中用于突出显示细胞组分的染色剂外观的不一致性。我们提出了一种新方法，StainFuser，将这个问题视为一个风格迁移任务，使用一种新颖的条件潜在扩散架构，消除了手工制作颜色组分的需要。通过这种方法，我们为高质量转换筛选了迄今为止包含超过200万个组织学图像的最大染色标准化数据集SPI-2M，并进行了神经风格迁移训练。在这些数据上训练后，StainFuser在质量上优于当前最先进的GAN和手工制作方法的标准化图像。此外，与现有方法相比，在用作te时，它改善了细胞核实例分割和分类模型的性能

    arXiv:2403.09302v1 Announce Type: cross  Abstract: Stain normalization algorithms aim to transform the color and intensity characteristics of a source multi-gigapixel histology image to match those of a target image, mitigating inconsistencies in the appearance of stains used to highlight cellular components in the images. We propose a new approach, StainFuser, which treats this problem as a style transfer task using a novel Conditional Latent Diffusion architecture, eliminating the need for handcrafted color components. With this method, we curate SPI-2M the largest stain normalization dataset to date of over 2 million histology images with neural style transfer for high-quality transformations. Trained on this data, StainFuser outperforms current state-of-the-art GAN and handcrafted methods in terms of the quality of normalized images. Additionally, compared to existing approaches, it improves the performance of nuclei instance segmentation and classification models when used as a te
    
[^45]: 递归因果发现

    Recursive Causal Discovery

    [https://arxiv.org/abs/2403.09300](https://arxiv.org/abs/2403.09300)

    可移除变量的概念允许递归方法进行因果发现，通过逐步减少问题规模来帮助解决因果发现中的主要挑战。

    

    arXiv:2403.09300v1 公告类型：新摘要：因果发现，即从数据中学习因果图，通常是识别和估计因果效应的第一步，这是许多科学领域的关键要求。因果发现面临两个主要挑战：有限的数据导致统计检验错误，学习任务的计算复杂性令人望而却步。本文基于并扩展了我们先前发表的四篇论文（Mokhtarian等，2021年；Akbari等，2021年；Mokhtarian等，2022年，2023a年）。这些作品引入了可移除变量的概念，这些变量是唯一可以递归移除用于因果发现的变量。可移除变量的存在和识别允许因果发现的递归方法，这是一个有前途的解决方案，通过逐步减少问题规模来帮助解决前述挑战。这种缩减不仅在每个条件设置中最小化了条件集。

    arXiv:2403.09300v1 Announce Type: new  Abstract: Causal discovery, i.e., learning the causal graph from data, is often the first step toward the identification and estimation of causal effects, a key requirement in numerous scientific domains. Causal discovery is hampered by two main challenges: limited data results in errors in statistical testing and the computational complexity of the learning task is daunting. This paper builds upon and extends four of our prior publications (Mokhtarian et al., 2021; Akbari et al., 2021; Mokhtarian et al., 2022, 2023a). These works introduced the concept of removable variables, which are the only variables that can be removed recursively for the purpose of causal discovery. Presence and identification of removable variables allow recursive approaches for causal discovery, a promising solution that helps to address the aforementioned challenges by reducing the problem size successively. This reduction not only minimizes conditioning sets in each con
    
[^46]: 超越言语：歌唱语音识别中的进展与挑战

    More than words: Advancements and challenges in speech recognition for singing

    [https://arxiv.org/abs/2403.09298](https://arxiv.org/abs/2403.09298)

    本文讨论了歌唱语音识别中的挑战和进展，探索了音素识别、歌曲中的语言识别、关键词识别和完整歌词转录等关键领域，并介绍了深度学习和大规模数据集在该领域推动进展的最新发展。

    

    本文讨论了歌唱语音识别中的挑战和进展，这是一个与标准语音识别完全不同的领域。歌唱包含独特的挑战，包括广泛的音高变化、多样化的声乐风格以及背景音乐干扰。我们探讨了诸如音素识别、歌曲中的语言识别、关键词识别和完整歌词转录等关键领域。我将描述一些我在这些任务上进行研究时的经历，就在它们开始崭露头角的时候，但也会展示深度学习和大规模数据集的最新进展如何推动了这一领域的进步。我的目标是阐明将语音识别应用于歌唱时的复杂性，评估当前的能力，并概述未来的研究方向。

    arXiv:2403.09298v1 Announce Type: cross  Abstract: This paper addresses the challenges and advancements in speech recognition for singing, a domain distinctly different from standard speech recognition. Singing encompasses unique challenges, including extensive pitch variations, diverse vocal styles, and background music interference. We explore key areas such as phoneme recognition, language identification in songs, keyword spotting, and full lyrics transcription. I will describe some of my own experiences when performing research on these tasks just as they were starting to gain traction, but will also show how recent developments in deep learning and large-scale datasets have propelled progress in this field. My goal is to illuminate the complexities of applying speech recognition to singing, evaluate current capabilities, and outline future research directions.
    
[^47]: SELECTOR：具有卷积掩码自编码器的异构图网络，用于癌症生存鲁棒预测

    SELECTOR: Heterogeneous graph network with convolutional masked autoencoder for multimodal robust prediction of cancer survival

    [https://arxiv.org/abs/2403.09290](https://arxiv.org/abs/2403.09290)

    该论文介绍了一种名为SELECTOR的异构图网络，利用卷积掩码自编码器进行癌症患者生存的鲁棒多模态预测

    

    准确预测癌症患者的生存率对于帮助临床医生制定适当的治疗方案，降低与癌症相关的医疗费用，并显著提高患者的生活质量至关重要。癌症患者生存的多模预测提供了一种更全面、更精确的方法。然而，现有方法仍然面临着与缺失的多模态数据和模态内信息交互相关的挑战。本文介绍了SELECTOR，一种基于卷积掩码编码器的异构图感知网络，用于癌症患者生存的鲁棒多模态预测。SELECTOR包括特征边重构、卷积掩码编码器、特征交叉融合和多模态生存预测模块。首先，我们构建一个多模态异构图，并采用元路径方法进行特征边重构，确保特征信息的全面整合。

    arXiv:2403.09290v1 Announce Type: cross  Abstract: Accurately predicting the survival rate of cancer patients is crucial for aiding clinicians in planning appropriate treatment, reducing cancer-related medical expenses, and significantly enhancing patients' quality of life. Multimodal prediction of cancer patient survival offers a more comprehensive and precise approach. However, existing methods still grapple with challenges related to missing multimodal data and information interaction within modalities. This paper introduces SELECTOR, a heterogeneous graph-aware network based on convolutional mask encoders for robust multimodal prediction of cancer patient survival. SELECTOR comprises feature edge reconstruction, convolutional mask encoder, feature cross-fusion, and multimodal survival prediction modules. Initially, we construct a multimodal heterogeneous graph and employ the meta-path method for feature edge reconstruction, ensuring comprehensive incorporation of feature informatio
    
[^48]: DA-PFL: 用于个性化联邦学习的动态关联聚合

    DA-PFL: Dynamic Affinity Aggregation for Personalized Federated Learning

    [https://arxiv.org/abs/2403.09284](https://arxiv.org/abs/2403.09284)

    提出了一种新颖的动态关联的个性化联邦学习模型（DA-PFL），采用互补的关联度量和动态聚合策略，在每轮动态聚合客户端以减少类不平衡风险。

    

    个性化联邦学习成为一个热门的研究课题，可以为每个客户端学习个性化的学习模型。现有的个性化联邦学习模型倾向于聚合数据分布相似的客户端以提高学习模型的性能。然而，基于相似性的个性化联邦学习方法可能加剧类不平衡问题。本文提出了一种新颖的基于动态关联的个性化联邦学习模型（DA-PFL），以减轻联邦学习中的类不平衡问题。具体而言，我们从一个互补的角度构建了一种关联度量来引导哪些客户端应该被聚合。然后，我们设计了一种动态聚合策略，根据每一轮的关联度量动态聚合客户端，以减少类不平衡风险。大量实验证明，所提出的DA-PFL模型可以显著提高每个客户端的准确性。

    arXiv:2403.09284v1 Announce Type: new  Abstract: Personalized federated learning becomes a hot research topic that can learn a personalized learning model for each client. Existing personalized federated learning models prefer to aggregate similar clients with similar data distribution to improve the performance of learning models. However, similaritybased personalized federated learning methods may exacerbate the class imbalanced problem. In this paper, we propose a novel Dynamic Affinity-based Personalized Federated Learning model (DA-PFL) to alleviate the class imbalanced problem during federated learning. Specifically, we build an affinity metric from a complementary perspective to guide which clients should be aggregated. Then we design a dynamic aggregation strategy to dynamically aggregate clients based on the affinity metric in each round to reduce the class imbalanced risk. Extensive experiments show that the proposed DA-PFL model can significantly improve the accuracy of each
    
[^49]: 深度限价订单簿预测

    Deep Limit Order Book Forecasting

    [https://arxiv.org/abs/2403.09267](https://arxiv.org/abs/2403.09267)

    该研究利用深度学习方法预测纳斯达克交易所股票的限价订单簿中间价格变动，提出了一个创新的操作框架来评估预测的实用性。

    

    我们利用尖端的深度学习方法探索了在纳斯达克交易所上交易的一组异质股票的高频限价订单簿中间价格变动的可预测性。在此过程中，我们发布了“LOBFrame”，一个开源代码库，可以高效处理大规模限价订单簿数据，并定量评估最先进的深度学习模型的预测能力。我们的结果是双重的。我们证明股票的微观结构特征影响深度学习方法的有效性，并且它们的高预测能力不一定对应可操作的交易信号。我们认为传统的机器学习指标未能充分评估限价订单簿环境中预测的质量。作为替代，我们提出了一个创新的操作框架，通过专注于准确预测的概率来评估预测的实用性。

    arXiv:2403.09267v1 Announce Type: cross  Abstract: We exploit cutting-edge deep learning methodologies to explore the predictability of high-frequency Limit Order Book mid-price changes for a heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we release `LOBFrame', an open-source code base, to efficiently process large-scale Limit Order Book data and quantitatively assess state-of-the-art deep learning models' forecasting capabilities. Our results are twofold. We demonstrate that the stocks' microstructural characteristics influence the efficacy of deep learning methods and that their high forecasting power does not necessarily correspond to actionable trading signals. We argue that traditional machine learning metrics fail to adequately assess the quality of forecasts in the Limit Order Book context. As an alternative, we propose an innovative operational framework that assesses predictions' practicality by focusing on the probability of accurately forecasting com
    
[^50]: 是否给数据贴标签：神经机器翻译的混合主动学习

    To Label or Not to Label: Hybrid Active Learning for Neural Machine Translation

    [https://arxiv.org/abs/2403.09259](https://arxiv.org/abs/2403.09259)

    提出了一种用于神经机器翻译的混合主动学习策略HUDS，结合了不确定性和多样性，用于领域自适应的句子选择。

    

    主动学习技术通过从未标记数据中选择更小的代表性子集进行注释，降低了训练神经机器翻译（NMT）模型的标记成本。我们提出了HUDS，这是一种用于NMT领域自适应的混合主动学习策略，将不确定性和多样性相结合，以进行句子选择。

    arXiv:2403.09259v1 Announce Type: new  Abstract: Active learning (AL) techniques reduce labeling costs for training neural machine translation (NMT) models by selecting smaller representative subsets from unlabeled data for annotation. Diversity sampling techniques select heterogeneous instances, while uncertainty sampling methods select instances with the highest model uncertainty. Both approaches have limitations - diversity methods may extract varied but trivial examples, while uncertainty sampling can yield repetitive, uninformative instances. To bridge this gap, we propose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines uncertainty and diversity for sentence selection. HUDS computes uncertainty scores for unlabeled sentences and subsequently stratifies them. It then clusters sentence embeddings within each stratum using k-MEANS and computes diversity scores by distance to the centroid. A weighted hybrid score that combines uncertainty and diversity is then us
    
[^51]: 跨受试者运动想象分类的不确定性量化

    Uncertainty Quantification for cross-subject Motor Imagery classification

    [https://arxiv.org/abs/2403.09228](https://arxiv.org/abs/2403.09228)

    本研究针对跨受试者的运动想象分类，引入不确定性量化方法，验证了深度集成在分类性能和跨受试者不确定性量化性能上的优越表现，同时发现标准CNN方法在某些情况下表现更好。

    

    不确定性量化旨在确定机器学习模型的预测何时可能出错。计算机视觉研究探索了确定认知不确定性（也称为模型不确定性）的方法，这应与泛化误差相对应。这些方法理论上可用于预测由受试者间变异性引起的错分。我们应用了各种不确定性量化方法来预测脑机接口中的错分。深度集成在分类性能和跨受试者不确定性量化性能方面表现最好。然而，我们发现具有Softmax输出的标准CNN优于一些更先进的方法。

    arXiv:2403.09228v1 Announce Type: new  Abstract: Uncertainty Quantification aims to determine when the prediction from a Machine Learning model is likely to be wrong. Computer Vision research has explored methods for determining epistemic uncertainty (also known as model uncertainty), which should correspond with generalisation error. These methods theoretically allow to predict misclassifications due to inter-subject variability. We applied a variety of Uncertainty Quantification methods to predict misclassifications for a Motor Imagery Brain Computer Interface. Deep Ensembles performed best, both in terms of classification performance and cross-subject Uncertainty Quantification performance. However, we found that standard CNNs with Softmax output performed better than some of the more advanced methods.
    
[^52]: MCformer: 混合通道Transformer实现多变量时间序列预测

    MCformer: Multivariate Time Series Forecasting with Mixed-Channels Transformer

    [https://arxiv.org/abs/2403.09223](https://arxiv.org/abs/2403.09223)

    提出了混合通道策略，结合了通道独立策略的数据扩展优势，能对抗跨通道相关性忘却，实现多变量时间序列预测的创新模型MCformer。

    

    大规模物联网设备的时间序列数据大量生成，需要探索更有效的多变量时间序列预测模型。当前最先进的模型主要依赖通道独立（CI）策略，但是CI策略面临着跨通道相关性忘却的挑战。为了解决这个问题，我们提出了创新的混合通道策略，结合了CI策略的数据扩展优势和对抗跨通道相关性忘却的能力。基于这个策略，我们引入了MCformer，一个...

    arXiv:2403.09223v1 Announce Type: new  Abstract: The massive generation of time-series data by largescale Internet of Things (IoT) devices necessitates the exploration of more effective models for multivariate time-series forecasting. In previous models, there was a predominant use of the Channel Dependence (CD) strategy (where each channel represents a univariate sequence). Current state-of-the-art (SOTA) models primarily rely on the Channel Independence (CI) strategy. The CI strategy treats all channels as a single channel, expanding the dataset to improve generalization performance and avoiding inter-channel correlation that disrupts long-term features. However, the CI strategy faces the challenge of interchannel correlation forgetting. To address this issue, we propose an innovative Mixed Channels strategy, combining the data expansion advantages of the CI strategy with the ability to counteract inter-channel correlation forgetting. Based on this strategy, we introduce MCformer, a 
    
[^53]: Laplace逼近作为高斯过程模型选择准则的研究

    On the Laplace Approximation as Model Selection Criterion for Gaussian Processes

    [https://arxiv.org/abs/2403.09215](https://arxiv.org/abs/2403.09215)

    通过引入基于Laplace逼近的多种度量标准，本研究解决了高斯过程模型选择中遇到的性能不佳和运行时间问题，实验结果表明这些新的标准在质量上与黄金标准动态嵌套抽样相当，同时保持了计算速度。

    

    模型选择旨在找到在准确性、可解释性或简易性方面最佳的模型，最好是三者兼顾。本研究着重于评估高斯过程模型的模型性能，即找到一个提供所有这些准则之间最佳权衡的度量标准。我们引入了基于Laplace逼近的多种度量标准来应对以往工作中存在的性能不佳或具有严重运行时间问题的情况，从而极大地限制了适用性。实验表明，我们的度量标准在质量上与黄金标准动态嵌套抽样相当，而又不会影响计算速度。我们的模型选择标准使得高斯过程模型的模型选择能够更快速地且生成质量更高的结果。

    arXiv:2403.09215v1 Announce Type: cross  Abstract: Model selection aims to find the best model in terms of accuracy, interpretability or simplicity, preferably all at once. In this work, we focus on evaluating model performance of Gaussian process models, i.e. finding a metric that provides the best trade-off between all those criteria. While previous work considers metrics like the likelihood, AIC or dynamic nested sampling, they either lack performance or have significant runtime issues, which severely limits applicability. We address these challenges by introducing multiple metrics based on the Laplace approximation, where we overcome a severe inconsistency occuring during naive application of the Laplace approximation. Experiments show that our metrics are comparable in quality to the gold standard dynamic nested sampling without compromising for computational speed. Our model selection criteria allow significantly faster and high quality model selection of Gaussian process models.
    
[^54]: 学习自适应邻居以实时检测内部威胁

    LAN: Learning Adaptive Neighbors for Real-Time Insider Threat Detection

    [https://arxiv.org/abs/2403.09209](https://arxiv.org/abs/2403.09209)

    该论文的贡献是提出了一个名为LAN的框架，能够实时在活动级别进行内部威胁检测，并学习活动序列内的时间依赖关系和活动之间的关系。

    

    企业和组织面临来自内部员工可能导致严重后果的潜在威胁。先前关于内部威胁检测（ITD）的研究主要集中在检测异常用户或异常时间段（例如，一周或一天）。然而，用户可能在日志中有数十万条活动，即使在一天内，一个用户也可能存在数千条活动，这需要高昂的调查预算来验证异常用户或活动。另一方面，现有作品主要是事后方法而不是实时检测，无法及时报告内部威胁在引起损失之前。在本文中，我们进行了针对活动级别实时ITD的第一项研究，并提出了一个细粒度和高效的框架LAN。具体而言，LAN同时学习活动序列内的时间依赖关系和活动之间的关系。

    arXiv:2403.09209v1 Announce Type: cross  Abstract: Enterprises and organizations are faced with potential threats from insider employees that may lead to serious consequences. Previous studies on insider threat detection (ITD) mainly focus on detecting abnormal users or abnormal time periods (e.g., a week or a day). However, a user may have hundreds of thousands of activities in the log, and even within a day there may exist thousands of activities for a user, requiring a high investigation budget to verify abnormal users or activities given the detection results. On the other hand, existing works are mainly post-hoc methods rather than real-time detection, which can not report insider threats in time before they cause loss. In this paper, we conduct the first study towards real-time ITD at activity level, and present a fine-grained and efficient framework LAN. Specifically, LAN simultaneously learns the temporal dependencies within an activity sequence and the relationships between ac
    
[^55]: 贝叶斯概化错误在部分概念瓶颈模型中的上界：部分CBM胜过朴素CBM

    Upper Bound of Bayesian Generalization Error in Partial Concept Bottleneck Model (CBM): Partial CBM outperforms naive CBM

    [https://arxiv.org/abs/2403.09206](https://arxiv.org/abs/2403.09206)

    本文在三层线性结构的部分CBM中揭示了贝叶斯概化错误的上界，进一步证明部分CBM优于朴素CBM。

    

    arXiv：2403.09206v1 类型通告：交叉摘要：概念瓶颈模型（CBM）是解释神经网络的方法。在CBM中，对应于输出原因的概念被插入到最后一个中间层作为观察值。人们预期我们可以解释输出和概念之间的关系，类似于线性回归。然而，这种解释需要观察所有概念，并且降低了神经网络的泛化性能。部分CBM（PCBM）使用部分观察到的概念，旨在解决这些困难。尽管一些数值实验表明PCBM的泛化性能几乎与原始神经网络一样高，但由于PCBM是奇异的统计模型，其泛化错误的理论行为尚未明确。在本文中，我们揭示了具有三层线性架构的PCBM中的贝叶斯泛化错误。

    arXiv:2403.09206v1 Announce Type: cross  Abstract: Concept Bottleneck Model (CBM) is a methods for explaining neural networks. In CBM, concepts which correspond to reasons of outputs are inserted in the last intermediate layer as observed values. It is expected that we can interpret the relationship between the output and concept similar to linear regression. However, this interpretation requires observing all concepts and decreases the generalization performance of neural networks. Partial CBM (PCBM), which uses partially observed concepts, has been devised to resolve these difficulties. Although some numerical experiments suggest that the generalization performance of PCBMs is almost as high as that of the original neural networks, the theoretical behavior of its generalization error has not been yet clarified since PCBM is singular statistical model. In this paper, we reveal the Bayesian generalization error in PCBM with a three-layered and linear architecture. The result indcates t
    
[^56]: 视觉语言模型是纹理偏见还是形状偏见，我们可以引导它们吗？

    Are Vision Language Models Texture or Shape Biased and Can We Steer Them?

    [https://arxiv.org/abs/2403.09193](https://arxiv.org/abs/2403.09193)

    本文研究了广泛应用的视觉语言模型中的纹理与形状偏见，发现这些模型通常比视觉编码器更偏向形状，暗示视觉偏见在一定程度上会受到文本的调节

    

    arXiv:2403.09193v1 公告类型: 跨领域 摘要: 视觉语言模型（VLMs）在短短几年内彻底改变了计算机视觉模型的格局，开启了一系列新的应用，从零样本图像分类到图像字幕生成，再到视觉问答。与纯视觉模型不同，它们提供了通过语言提示访问视觉内容的直观方式。这种模型的广泛适用性引发我们思考它们是否也与人类视觉一致 - 具体来说，它们在多模态融合中有多大程度地采用了人类引导的视觉偏见，或者它们是否只是从纯视觉模型中继承了偏见。其中一个重要的视觉偏见是纹理与形状偏见，即局部信息的主导地位。在本文中，我们研究了一系列流行的VLMs中的这种偏见。有趣的是，我们发现VLMs通常比它们的视觉编码器更偏向于形状，这表明视觉偏见在一定程度上通过文本进行调节。

    arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text
    
[^57]: 使用GC-MS光谱作为案例研究，在深度学习训练中设计用于稀疏数据集的基础投影层

    Design of an basis-projected layer for sparse datasets in deep learning training using gc-ms spectra as a case study

    [https://arxiv.org/abs/2403.09188](https://arxiv.org/abs/2403.09188)

    提出了一种名为基础投影层（BPL）的DL模块，通过将稀疏数据转换为密集表示来优化深度学习模型的训练过程。

    

    深度学习（DL）模型包含数百万甚至数十亿的参数，并从大数据中学习复杂模式。然而，并非所有数据最初都以适当的形式存储，以有效地训练DL模型，例如气相色谱-质谱（GC-MS）光谱和DNA序列。这些数据集通常包含许多零值，稀疏数据形式导致了优化DL模型的困难。提出了一种名为基础投影层（BPL）的DL模块，通过将稀疏数据转换为密集表示来缓解这个问题。预期转换后的数据将有助于DL训练过程中的梯度计算和微调过程。数据集中包含了从GC-MS检测到的362种特殊咖啡气味光谱作为稀疏数据集的示例。BPL层位于DL模型的开头。该层中的可调参数是可学习的投影轴，它们是一个新的基础。

    arXiv:2403.09188v1 Announce Type: new  Abstract: Deep learning (DL) models encompass millions or even billions of parameters and learn complex patterns from big data. However, not all data are initially stored in a suitable formation to effectively train a DL model, e.g., gas chromatography-mass spectrometry (GC-MS) spectra and DNA sequence. These datasets commonly contain many zero values, and the sparse data formation causes difficulties in optimizing DL models. A DL module called the basis-projected layer (BPL) was proposed to mitigate the issue by transforming the sparse data into a dense representation. The transformed data is expected to facilitate the gradient calculation and finetuned process in a DL training process. The dataset, example of a sparse dataset, contained 362 specialty coffee odorant spectra detected from GC-MS. The BPL layer was placed at the beginning of the DL model. The tunable parameters in the layer were learnable projected axes that were the bases of a new 
    
[^58]: 广义相关性学习格拉斯曼量化

    Generalized Relevance Learning Grassmann Quantization

    [https://arxiv.org/abs/2403.09183](https://arxiv.org/abs/2403.09183)

    该论文扩展了广义相关性学习向量量化的应用，以处理格拉斯曼流形，提出模型返回一组原型子空间和相关性向量，为分类任务提供重要见解。

    

    由于数字相机的进步，容易在不同条件下收集来自对象的多个图像（或视频）。因此，图像集分类引起了更多关注，并提出了不同的解决方案来对其进行建模。建模图像集的一种流行方式是子空间，它们形成称为格拉斯曼流形的流形。在这篇论文中，我们将广义相关性学习向量量化的应用拓展到处理格拉斯曼流形。所提出的模型返回一组原型子空间和一个相关性向量。原型模型了类内的典型行为，而相关性因子指定了分类任务中最具辨别性的主要向量（或图像）。它们通过突出影响分类预测的重要图像和像素，为模型的决策提供了见解。此外，由于学习了原型，新方法在推理过程中的模型复杂性也得到了控制。

    arXiv:2403.09183v1 Announce Type: cross  Abstract: Due to advancements in digital cameras, it is easy to gather multiple images (or videos) from an object under different conditions. Therefore, image-set classification has attracted more attention, and different solutions were proposed to model them. A popular way to model image sets is subspaces, which form a manifold called the Grassmann manifold. In this contribution, we extend the application of Generalized Relevance Learning Vector Quantization to deal with Grassmann manifold. The proposed model returns a set of prototype subspaces and a relevance vector. While prototypes model typical behaviours within classes, the relevance factors specify the most discriminative principal vectors (or images) for the classification task. They both provide insights into the model's decisions by highlighting influential images and pixels for predictions. Moreover, due to learning prototypes, the model complexity of the new method during inference 
    
[^59]: ADEdgeDrop：用于强健图神经网络的敌对边缘删除

    ADEdgeDrop: Adversarial Edge Dropping for Robust Graph Neural Networks

    [https://arxiv.org/abs/2403.09171](https://arxiv.org/abs/2403.09171)

    ADEdgeDrop提出了一种敌对边缘删除方法，通过引入敌对边缘预测器指导边缘删除，从而提高了图神经网络的稳健性。

    

    尽管图神经网络（GNNs）通过各种消息传递机制展示了从邻近节点收集图结构信息的强大能力，但由于嘈杂和冗余的图数据造成的差的泛化和脆弱的稳健性限制了GNNs的性能。在Graph Augmentation Learning（GAL）中，边缘删除方法是一种有效的技术，可以提高GNNs的稳健性。然而，随机删除边缘通常会绕过关键边缘，从而削弱消息传递的效果。本文提出了一种新颖的敌对边缘删除方法（ADEdgeDrop），利用敌对边缘预测器引导边缘删除，可以灵活地整合到不同的GNN主干中。

    arXiv:2403.09171v1 Announce Type: cross  Abstract: Although Graph Neural Networks (GNNs) have exhibited the powerful ability to gather graph-structured information from neighborhood nodes via various message-passing mechanisms, the performance of GNNs is limited by poor generalization and fragile robustness caused by noisy and redundant graph data. As a prominent solution, Graph Augmentation Learning (GAL) has recently received increasing attention. Among prior GAL approaches, edge-dropping methods that randomly remove edges from a graph during training are effective techniques to improve the robustness of GNNs. However, randomly dropping edges often results in bypassing critical edges, consequently weakening the effectiveness of message passing. In this paper, we propose a novel adversarial edge-dropping method (ADEdgeDrop) that leverages an adversarial edge predictor guiding the removal of edges, which can be flexibly incorporated into diverse GNN backbones. Employing an adversarial 
    
[^60]: 在支持AI的边缘设备中的多智能体分布式学习中的不确定性估计

    Uncertainty Estimation in Multi-Agent Distributed Learning for AI-Enabled Edge Devices

    [https://arxiv.org/abs/2403.09141](https://arxiv.org/abs/2403.09141)

    研究探索了在支持AI的边缘设备中实现分布式数据处理的方法，重点解决了在独立代理遇到的数据集的空间和时间变异性中确定学习结果的置信水平挑战。

    

    最初被认为是具有有限自主处理能力的低功率单元，边缘物联网设备随着FPGA和AI加速器的引入而发生了范式转变。这一进步极大地增强了它们的计算能力，突显了边缘AI的实用性。这种进步引入了新挑战，即如何针对边缘计算环境中能源和网络资源的限制优化AI任务。我们的研究探讨了通过支持AI的边缘设备实现分布式数据处理的方法，增强了协作学习能力。我们研究的重点之一是解决确定学习结果的置信水平的挑战，考虑独立代理遇到的数据集的空间和时间变异性。为了解决这个问题，我们研究了贝叶斯神经网络的应用，提出了一种在分布式学习环境中管理不确定性的新方法。

    arXiv:2403.09141v1 Announce Type: cross  Abstract: Initially considered as low-power units with limited autonomous processing, Edge IoT devices have seen a paradigm shift with the introduction of FPGAs and AI accelerators. This advancement has vastly amplified their computational capabilities, emphasizing the practicality of edge AI. Such progress introduces new challenges of optimizing AI tasks for the limitations of energy and network resources typical in Edge computing environments. Our study explores methods that enable distributed data processing through AI-enabled edge devices, enhancing collaborative learning capabilities. A key focus of our research is the challenge of determining confidence levels in learning outcomes, considering the spatial and temporal variability of data sets encountered by independent agents. To address this issue, we investigate the application of Bayesian neural networks, proposing a novel approach to manage uncertainty in distributed learning environme
    
[^61]: 最佳臂识别和流体分析的最优Top-Two方法

    Optimal Top-Two Method for Best Arm Identification and Fluid Analysis

    [https://arxiv.org/abs/2403.09123](https://arxiv.org/abs/2403.09123)

    提出了解决最佳臂识别问题中的样本复杂度挑战的最优Top-Two算法。

    

    Top-2方法在解决最佳臂识别（BAI）问题中变得流行。该方法通过一个算法识别最佳臂，即在有限数量臂中具有最大均值的臂，该算法在任何顺序步骤中独立地以固定概率 β 拉动经验最佳臂，并在其他情况下拉动最佳挑战者臂。选择错误的概率保证在指定的δ >0以下。对于BAI问题，已知信息理论下界的样本复杂度，并在δ → 0时与计算要求高的插件方法渐近匹配。 对于任何 β ∈（0,1）的上述Top 2算法的样本复杂度始终保持在下界的常数范围内。然而，确定与下界匹配的最佳 β 已被证明困难。在本文中，我们解决了这个问题并提出了一个最优的Top-2类型算法。我们考虑分配锚点的一个函数。

    arXiv:2403.09123v1 Announce Type: new  Abstract: Top-$2$ methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\beta$, and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified $\delta >0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\delta \rightarrow 0$ by computationally demanding plug-in methods. The above top 2 algorithm for any $\beta \in (0,1)$ has sample complexity within a constant of the lower bound. However, determining the optimal $\beta$ that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchor
    
[^62]: 针对高光谱图像分类的随机主成分分析

    Randomized Principal Component Analysis for Hyperspectral Image Classification

    [https://arxiv.org/abs/2403.09117](https://arxiv.org/abs/2403.09117)

    本研究探讨了针对高光谱图像分类的主成分分析（PCA）和随机主成分分析（R-PCA），实验证明PCA在支持向量机（SVM）方面优于R-PCA，但在轻量级梯度提升机（LightGBM）方面表现接近准确度数值。

    

    高光谱图像的高维特征空间给高光谱数据集的处理和分析带来了重大挑战。在这种情况下，降维是必要的以减小计算复杂性。随机投影为降低维度提供了新的途径，尤其适用于大数据集。本文探讨了使用支持向量机（SVM）和轻量级梯度提升机（LightGBM）对高光谱图像进行分类的主成分分析（PCA）和随机主成分分析（R-PCA）。在这项实验研究中，将特征数减少到20和30以进行两个高光谱数据集（Indian Pines和Pavia University）的分类。实验结果表明，对于两个数据集，PCA在SVM方面优于R-PCA，但在LightGBM方面获得了接近的准确度数值。

    arXiv:2403.09117v1 Announce Type: cross  Abstract: The high-dimensional feature space of the hyperspectral imagery poses major challenges to the processing and analysis of the hyperspectral data sets. In such a case, dimensionality reduction is necessary to decrease the computational complexity. The random projections open up new ways of dimensionality reduction, especially for large data sets. In this paper, the principal component analysis (PCA) and randomized principal component analysis (R-PCA) for the classification of hyperspectral images using support vector machines (SVM) and light gradient boosting machines (LightGBM) have been investigated. In this experimental research, the number of features was reduced to 20 and 30 for classification of two hyperspectral datasets (Indian Pines and Pavia University). The experimental results demonstrated that PCA outperformed R-PCA for SVM for both datasets, but received close accuracy values for LightGBM. The highest classification accurac
    
[^63]: AutoLoRA：基于元学习的自动调整矩阵秩在低秩适应中的应用

    AutoLoRA: Automatically Tuning Matrix Ranks in Low-Rank Adaptation Based on Meta Learning

    [https://arxiv.org/abs/2403.09113](https://arxiv.org/abs/2403.09113)

    AutoLoRA提出了一个基于元学习的框架，自动识别每个LoRA层的最佳秩，以解决LoRA中秩分配和秩搜索的问题，进而提高微调性能。

    

    大规模预训练之后进行任务特定微调在各种自然语言处理任务中取得了巨大成功。然而，对于大型预训练模型的所有参数进行微调存在着巨大的计算和内存挑战，因此研发了几种高效的微调方法。其中，低秩适应（LoRA）通过在冻结的预训练权重之上微调低秩增量更新矩阵，被证明特别有效。然而，LoRA在所有层中均匀分配秩，并且依赖于穷举搜索来找到最佳秩，导致了高计算成本和微调性能不佳。为了解决这些限制，我们引入了AutoLoRA，这是一个基于元学习的框架，用于自动识别每个LoRA层的最佳秩。AutoLoRA将低秩更新矩阵中的每个秩为1的矩阵与选择变量相关联，该变量决定了秩为1的矩阵是否应该被...

    arXiv:2403.09113v1 Announce Type: cross  Abstract: Large-scale pretraining followed by task-specific finetuning has achieved great success in various NLP tasks. Since finetuning all parameters of large pretrained models poses substantial computational and memory challenges, several efficient finetuning methods have been developed. Among them, low-rank adaptation (LoRA), which finetunes low-rank incremental update matrices on top of frozen pretrained weights, has proven particularly effective. Nonetheless, LoRA's uniform rank assignment across all layers, along with its reliance on an exhaustive search to find the best rank, leads to high computation costs and suboptimal finetuning performance. To address these limitations, we introduce AutoLoRA, a meta learning based framework for automatically identifying the optimal rank of each LoRA layer. AutoLoRA associates each rank-1 matrix in a low-rank update matrix with a selection variable, which determines whether the rank-1 matrix should b
    
[^64]: SINDy-RL: 可解释和高效的基于模型的强化学习

    SINDy-RL: Interpretable and Efficient Model-Based Reinforcement Learning

    [https://arxiv.org/abs/2403.09110](https://arxiv.org/abs/2403.09110)

    将稀疏识别的非线性动力学（SINDy）与深度强化学习（DRL）结合，提出了SINDy-RL框架，用于创建高效解释性还有在低数据制度下创建高效且可解释的数据驱动模型。

    

    深度强化学习（DRL）已显示出在与复杂动态环境中相互作用的复杂控制策略中具有显著潜力，例如稳定托卡马克聚变反应堆的磁流体动力学或使物体在流体流动中受到的阻力最小化。然而，这些算法需要大量的训练样本，对许多应用而言成本可能过高。另外，依赖深度神经网络往往会导致难以解释的黑盒策略，可能在某些嵌入式系统中使用时计算成本过高。最近的稀疏字典学习方法的进展，如稀疏非线性动力学的稀疏识别（SINDy），显示出在低数据制度下创建高效且可解释的数据驱动模型的前景。在这项工作中，我们介绍SINDy-RL，这是一个结合SINDy和DRL的统一框架，用于创建高效的可解释的模型。

    arXiv:2403.09110v1 Announce Type: new  Abstract: Deep reinforcement learning (DRL) has shown significant promise for uncovering sophisticated control policies that interact in environments with complicated dynamics, such as stabilizing the magnetohydrodynamics of a tokamak fusion reactor or minimizing the drag force exerted on an object in a fluid flow. However, these algorithms require an abundance of training examples and may become prohibitively expensive for many applications. In addition, the reliance on deep neural networks often results in an uninterpretable, black-box policy that may be too computationally expensive to use with certain embedded systems. Recent advances in sparse dictionary learning, such as the sparse identification of nonlinear dynamics (SINDy), have shown promise for creating efficient and interpretable data-driven models in the low-data regime. In this work we introduce SINDy-RL, a unifying framework for combining SINDy and DRL to create efficient, interpret
    
[^65]: S^2MVTC：简单而高效的可扩展多视图张量聚类

    S^2MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering

    [https://arxiv.org/abs/2403.09107](https://arxiv.org/abs/2403.09107)

    S^2MVTC提出了一种简单而高效的可扩展多视图张量聚类方法，重点是学习跨视图内部和视图间的嵌入特征之间的相关性。

    

    arXiv:2403.09107v1 公告类型：新摘要：基于锚点的大规模多视图聚类因其在处理海量数据集时的有效性而受到了广泛关注。然而，当前方法主要通过探索锚图或投影矩阵之间的全局相关性来寻求用于聚类的共识嵌入特征。在本文中，我们提出了一种简单而高效的可扩展多视图张量聚类（S^2MVTC）方法，我们的重点是学习跨视图内部和视图间的嵌入特征之间的相关性。具体来说，我们首先通过将不同视图的嵌入特征叠加成张量并对其进行旋转来构建嵌入特征张量。此外，我们构建了一种新颖的张量低频近似（TLFA）算子，将图相似性融入到嵌入特征学习中，高效地实现了不同视图内部嵌入特征的平滑表示。此外，一致性约束被应用于嵌入特征。

    arXiv:2403.09107v1 Announce Type: new  Abstract: Anchor-based large-scale multi-view clustering has attracted considerable attention for its effectiveness in handling massive datasets. However, current methods mainly seek the consensus embedding feature for clustering by exploring global correlations between anchor graphs or projection matrices.In this paper, we propose a simple yet efficient scalable multi-view tensor clustering (S^2MVTC) approach, where our focus is on learning correlations of embedding features within and across views. Specifically, we first construct the embedding feature tensor by stacking the embedding features of different views into a tensor and rotating it. Additionally, we build a novel tensor low-frequency approximation (TLFA) operator, which incorporates graph similarity into embedding feature learning, efficiently achieving smooth representation of embedding features within different views. Furthermore, consensus constraints are applied to embedding featur
    
[^66]: 软化以保卫：通过自导标签细化实现对抗鲁棒性

    Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement

    [https://arxiv.org/abs/2403.09101](https://arxiv.org/abs/2403.09101)

    本文提出了一种自导标签细化方法，通过在对抗训练中自我细化更准确和信息丰富的标签分布，并动态将自我蒸馏模型的知识纳入当前模型来消除对抗鲁棒性的过拟合问题。

    

    对抗训练（AT）目前是获得深度神经网络对抗攻击鲁棒性的最有效方法之一。然而，大多数AT方法遭受鲁棒过拟合，即在训练和测试曲线之间存在显著的对抗鲁棒性泛化差距。本文首次从梯度范数的角度识别了鲁棒过拟合与AT中嘈杂标签过度记忆之间的联系。鉴于此类标签噪声主要是由分布不匹配和不恰当的标签分配导致的，我们提出了一种用于AT的标签细化方法。具体来说，我们的自导标签细化首先从过于自信的硬标签中自我细化出更准确和信息丰富的标签分布，然后通过动态将自我蒸馏模型的知识纳入当前模型从而无需外部教师来校准训练。

    arXiv:2403.09101v1 Announce Type: new  Abstract: Adversarial training (AT) is currently one of the most effective ways to obtain the robustness of deep neural networks against adversarial attacks. However, most AT methods suffer from robust overfitting, i.e., a significant generalization gap in adversarial robustness between the training and testing curves. In this paper, we first identify a connection between robust overfitting and the excessive memorization of noisy labels in AT from a view of gradient norm. As such label noise is mainly caused by a distribution mismatch and improper label assignments, we are motivated to propose a label refinement approach for AT. Specifically, our Self-Guided Label Refinement first self-refines a more accurate and informative label distribution from over-confident hard labels, and then it calibrates the training by dynamically incorporating knowledge from self-distilled models into the current model and thus requiring no external teachers. Empirica
    
[^67]: 使用自发荧光显微镜和深度学习实现对不加标记组织中淀粉样沉积的虚拟双折射成像和组织学染色

    Virtual birefringence imaging and histological staining of amyloid deposits in label-free tissue using autofluorescence microscopy and deep learning

    [https://arxiv.org/abs/2403.09100](https://arxiv.org/abs/2403.09100)

    首次实现了对不加标记组织进行虚拟双折射成像和虚拟刚果红染色，通过深度学习将自发荧光图像转换为明场和偏振光图像。

    

    系统性淀粉样变性病是一组疾病，其特征是各种器官和组织中错折蛋白的沉积，导致器官功能逐渐受损和衰竭。刚果红染色是在组织切片中可视化淀粉样沉积的金标准化学染色方法，因为它与错折蛋白形成复合物，在偏振光显微镜下呈现双折射图案。然而，刚果红染色是费时且昂贵的，并且容易出现误诊，因为淀粉量、染色质量和专家解释受到变化，需要通过对组织在偏振显微镜下的手动检查来进行。本文报道了首次演示了对不加标记人类组织的虚拟双折射成像和虚拟刚果红染色，表明单个经过训练的神经网络可以快速地将不加标记组织切片的自发荧光图像转换为明场和偏振光图像。

    arXiv:2403.09100v1 Announce Type: cross  Abstract: Systemic amyloidosis is a group of diseases characterized by the deposition of misfolded proteins in various organs and tissues, leading to progressive organ dysfunction and failure. Congo red stain is the gold standard chemical stain for the visualization of amyloid deposits in tissue sections, as it forms complexes with the misfolded proteins and shows a birefringence pattern under polarized light microscopy. However, Congo red staining is tedious and costly to perform, and prone to false diagnoses due to variations in the amount of amyloid, staining quality and expert interpretation through manual examination of tissue under a polarization microscope. Here, we report the first demonstration of virtual birefringence imaging and virtual Congo red staining of label-free human tissue to show that a single trained neural network can rapidly transform autofluorescence images of label-free tissue sections into brightfield and polarized lig
    
[^68]: 耗散梯度下降方法：一种受控制论启发的极小-极大优化算法

    Dissipative Gradient Descent Ascent Method: A Control Theory Inspired Algorithm for Min-max Optimization

    [https://arxiv.org/abs/2403.09090](https://arxiv.org/abs/2403.09090)

    DGDA方法通过在GDA更新中引入耗散项，解决了极小-极大优化问题中的振荡行为，实现了超越其他方法的更优越收敛速度。

    

    梯度下降上升(GDA)方法用于极小-极大优化问题通常会产生可能导致不稳定性的振荡行为，例如在双线性设置中。为了解决这个问题，我们在GDA更新中引入了一个耗散项，以减弱这些振荡。所提出的耗散梯度下降（DGDA）方法可以被视为在一个状态增广和正则化的鞍点函数上执行标准的GDA，而不严格引入额外的凸性/凹性。我们在双线性和强凸-强凹设置中理论上展示了DGDA的线性收敛性，并通过将DGDA与其他方法（如GDA、Extra-Gradient (EG) 和乐观性GDA）进行比较来评估其性能。我们的研究结果表明，DGDA超越了这些方法，实现了更优越的收敛速度。我们通过两个数值示例支持我们的说法，展示了DGDA在解决鞍点问题中的有效性。

    arXiv:2403.09090v1 Announce Type: cross  Abstract: Gradient Descent Ascent (GDA) methods for min-max optimization problems typically produce oscillatory behavior that can lead to instability, e.g., in bilinear settings. To address this problem, we introduce a dissipation term into the GDA updates to dampen these oscillations. The proposed Dissipative GDA (DGDA) method can be seen as performing standard GDA on a state-augmented and regularized saddle function that does not strictly introduce additional convexity/concavity. We theoretically show the linear convergence of DGDA in the bilinear and strongly convex-strongly concave settings and assess its performance by comparing DGDA with other methods such as GDA, Extra-Gradient (EG), and Optimistic GDA. Our findings demonstrate that DGDA surpasses these methods, achieving superior convergence rates. We support our claims with two numerical examples that showcase DGDA's effectiveness in solving saddle point problems.
    
[^69]: 从联邦学习中学习迟到的客户端

    Learning from straggler clients in federated learning

    [https://arxiv.org/abs/2403.09086](https://arxiv.org/abs/2403.09086)

    现有联邦学习算法难以从严重延迟的客户端学习，为了改进这种情况，引入了两种新算法FARe-DUST和FeAST-on-MSG。

    

    现有的联邦学习算法有多大程度上能够从返回具有显著时间延迟的模型更新的客户设备中进行学习？学习效果是否可能受到客户端在被安排后几分钟、几小时或几天才报告回来的影响？通过开发由实际应用指导的蒙特卡洛模拟客户端延迟，我们回答了这些问题。我们研究了像FedAvg和FedAdam这样的同步优化算法，以及异步FedBuff算法，并观察到所有这些现有方法都难以从严重延迟的客户端中学习。为了改善这种情况，我们尝试了修改，包括蒸馏正则化和模型权重的指数移动平均值。最后，我们介绍了两种新算法，FARe-DUST和FeAST-on-MSG，分别基于蒸馏和平均化。对EMNIST、CIFAR-100和StackOverflow基准联邦学习进行了实验

    arXiv:2403.09086v1 Announce Type: new  Abstract: How well do existing federated learning algorithms learn from client devices that return model updates with a significant time delay? Is it even possible to learn effectively from clients that report back minutes, hours, or days after being scheduled? We answer these questions by developing Monte Carlo simulations of client latency that are guided by real-world applications. We study synchronous optimization algorithms like FedAvg and FedAdam as well as the asynchronous FedBuff algorithm, and observe that all these existing approaches struggle to learn from severely delayed clients. To improve upon this situation, we experiment with modifications, including distillation regularization and exponential moving averages of model weights. Finally, we introduce two new algorithms, FARe-DUST and FeAST-on-MSG, based on distillation and averaging, respectively. Experiments with the EMNIST, CIFAR-100, and StackOverflow benchmark federated learning
    
[^70]: Continual Learning中的超参数：现实检验

    Hyperparameters in Continual Learning: a Reality Check

    [https://arxiv.org/abs/2403.09066](https://arxiv.org/abs/2403.09066)

    超参数对于连续学习的重要性被强调，提出了一个涉及超参数调整和评估阶段的评估协议。

    

    不同的连续学习（CL）算法旨在在CL过程中有效地缓解稳定性和可塑性之间的权衡，为了实现这一目标，调整每种算法的适当超参数是必不可少的。本文主张现行的评估协议既不切实际，也无法有效评估连续学习算法的能力。

    arXiv:2403.09066v1 Announce Type: new  Abstract: Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datase
    
[^71]: Keyformer：通过关键标记选择减少KV缓存以实现高效的生成推断

    Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference

    [https://arxiv.org/abs/2403.09054](https://arxiv.org/abs/2403.09054)

    本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。

    

    Transformer已经成为大型语言模型(LLMs)的基础架构。在生成语言模型中，推断过程涉及两个主要阶段：提示处理和标记生成。标记生成，构成了大部分计算工作量，主要涉及向量-矩阵乘法和与键-值(KV)缓存交互。由于从存储系统传输权重和KV缓存值到计算单元的开销，这一阶段受到内存带宽的限制。这种内存瓶颈在需要长上下文和大量文本生成的应用中尤为突出，这两者对LLMs越来越重要。  本文介绍了一种创新的推断时间方法“Keyformer”，以缓解与KV缓存大小和内存带宽利用相关的挑战。Keyformer利用了这样的观察结果，大约90

    arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
    
[^72]: 走向模型蒸馏理论

    Towards a theory of model distillation

    [https://arxiv.org/abs/2403.09053](https://arxiv.org/abs/2403.09053)

    提出了模型蒸馏的一般理论，通过PAC-蒸馏定义，提出了抽取神经网络训练权重知识的新算法，并证明了蒸馏比从头学习更便宜且有助于理解其复杂性。

    

    蒸馏是将复杂的机器学习模型替换为简化模型来近似原模型的任务。尽管有许多实际应用，关于模型蒸馏的程度、所需运行时间和数据量的基本问题仍然大多未解。为了研究这些问题，我们开始了蒸馏的一般理论，以类似的方式定义了PAC-蒸馏 [Val84]，提出了提取训练权重中存储的知识的新算法，展示了如何通过使用“线性表示假设”将神经网络高效地蒸馏成简明明了的决策树表示，还证明了蒸馏可以比从头开始学习便宜得多，并在表征其复杂性方面取得了进展。

    arXiv:2403.09053v1 Announce Type: cross  Abstract: Distillation is the task of replacing a complicated machine learning model with a simpler model that approximates the original [BCNM06,HVD15]. Despite many practical applications, basic questions about the extent to which models can be distilled, and the runtime and amount of data needed to distill, remain largely open.   To study these questions, we initiate a general theory of distillation, defining PAC-distillation in an analogous way to PAC-learning [Val84]. As applications of this theory: (1) we propose new algorithms to extract the knowledge stored in the trained weights of neural networks -- we show how to efficiently distill neural networks into succinct, explicit decision tree representations when possible by using the ``linear representation hypothesis''; and (2) we prove that distillation can be much cheaper than learning from scratch, and make progress on characterizing its complexity.
    
[^73]: 驯服异构数据域中联邦原型学习中的跨领域表示差异

    Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains

    [https://arxiv.org/abs/2403.09048](https://arxiv.org/abs/2403.09048)

    引入FedPLVM通过建立方差感知的双层原型聚类和使用新型$\alpha$-稀疏原型损失，以减少跨领域特征表示差异。

    

    联邦学习（FL）允许在不共享私人数据的情况下进行协作机器学习训练。虽然大多数FL方法假设客户端之间具有相同的数据领域，但现实场景中通常涉及异构数据领域。联邦原型学习（FedPL）解决了这个问题，使用平均特征向量作为原型来增强模型泛化能力。然而，现有的FedPL方法为每个客户端创建相同数量的原型，导致跨领域性能差距，并使数据分布不同的客户端存在差异。为了减轻跨领域特征表示差异，我们引入了FedPLVM，它建立了方差感知的双层原型聚类，并采用了一种新颖的$\alpha$-稀疏原型损失。

    arXiv:2403.09048v1 Announce Type: new  Abstract: Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel $\alpha$-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and pres
    
[^74]: 空间-时间记忆增强图自编码器用于动态图中的异常检测

    Spatial-temporal Memories Enhanced Graph Autoencoder for Anomaly Detection in Dynamic Graphs

    [https://arxiv.org/abs/2403.09039](https://arxiv.org/abs/2403.09039)

    提出了一种空间-时间记忆增强图自编码器（STRIPE）用于动态图中的异常检测，通过结合图神经网络和门控时间卷积层来提取空间特征和时间特征。

    

    动态图中的异常检测面临较大挑战，因为图结构和属性的时间演变。为了解决这一问题，我们提出了一种新颖的空间-时间记忆增强图自编码器（STRIPE）。STRIPE利用图神经网络（GNNs）和门控时间卷积层分别提取空间特征和时间特征。

    arXiv:2403.09039v1 Announce Type: cross  Abstract: Anomaly detection in dynamic graphs presents a significant challenge due to the temporal evolution of graph structures and attributes. The conventional approaches that tackle this problem typically employ an unsupervised learning framework, capturing normality patterns with exclusive normal data during training and identifying deviations as anomalies during testing. However, these methods face critical drawbacks: they either only depend on proxy tasks for general representation without directly pinpointing normal patterns, or they neglect to differentiate between spatial and temporal normality patterns, leading to diminished efficacy in anomaly detection. To address these challenges, we introduce a novel Spatial-Temporal memories-enhanced graph autoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs) and gated temporal convolution layers to extract spatial features and temporal features, respectively. Then STRIPE in
    
[^75]: 探究微控制器上多样微小模型选择

    DiTMoS: Delving into Diverse Tiny-Model Selection on Microcontrollers

    [https://arxiv.org/abs/2403.09035](https://arxiv.org/abs/2403.09035)

    本文提出了一种新颖的DNN训练和推断框架DiTMoS，通过选择器-分类器架构和多样性策略，构建小型/弱模型并提高准确性上限。

    

    在微控制器上实现高效准确的深度神经网络（DNN）推断并不容易，因为芯片资源受限。本文从构建小型/弱模型并提高其准确性的反向角度重新思考这个问题。因此，我们引入了DiTMoS，一种新颖的DNN训练和推断框架，具有选择器-分类器架构，其中选择器将每个输入样本定位到适当的分类器以进行分类。DiTMoS建立在一个关键洞见上：弱模型的组合可以表现出高多样性，它们的联合可以显着提升准确性上限。为了接近这个上限，DiTMoS引入了包括增加分类器多样性的多样训练数据拆分在内的三种策略，对抗选择器-分类器训练

    arXiv:2403.09035v1 Announce Type: new  Abstract: Enabling efficient and accurate deep neural network (DNN) inference on microcontrollers is non-trivial due to the constrained on-chip resources. Current methodologies primarily focus on compressing larger models yet at the expense of model accuracy. In this paper, we rethink the problem from the inverse perspective by constructing small/weak models directly and improving their accuracy. Thus, we introduce DiTMoS, a novel DNN training and inference framework with a selector-classifiers architecture, where the selector routes each input sample to the appropriate classifier for classification. DiTMoS is grounded on a key insight: a composition of weak models can exhibit high diversity and the union of them can significantly boost the accuracy upper bound. To approach the upper bound, DiTMoS introduces three strategies including diverse training data splitting to increase the classifiers' diversity, adversarial selector-classifiers training 
    
[^76]: CodeUltraFeedback：一种用于将大型语言模型与编程偏好对齐的LLM作为法官数据集

    CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences

    [https://arxiv.org/abs/2403.09032](https://arxiv.org/abs/2403.09032)

    介绍了 CodeUltraFeedback 数据集，通过 AI 反馈使 14 种不同的 LLMs 对 10,000 个复杂指令生成响应，并使用 LLM-as-a-Judge 方法评估它们与五种编程偏好的对齐情况，同时提出了用于评估 LLM 对编程偏好对齐的基准 CODAL-Bench。

    

    评估大型语言模型（LLMs）与用户定义的编程偏好的对齐性是一项具有挑战性的工作，需要评估复杂文本LLMs的输出。现有基准仰赖自动化指标和静态分析工具，未能评估用户指令和LLM输出中的微妙之处，突显了对LLM偏好对齐的大规模数据集和基准的需求。在本文中，我们介绍了CodeUltraFeedback，一个包含10,000个复杂指令的偏好数据集，通过AI反馈来调整和对齐LLMs与编程偏好。我们使用14种不同的LLMs对这些指令生成响应，然后根据它们与五种编程偏好的对齐情况进行注释，使用GPT-3.5的LLM作为法官方法产生数字和文本反馈。我们还提出了CODAL-Bench，一个用于评估LLM与这些编程偏好对齐的基准。我们的结果显示C

    arXiv:2403.09032v1 Announce Type: cross  Abstract: Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that C
    
[^77]: 一种基于人工智能的方法用于从声学信号诊断风力涡轮机轴承故障

    An AI-Driven Approach to Wind Turbine Bearing Fault Diagnosis from Acoustic Signals

    [https://arxiv.org/abs/2403.09030](https://arxiv.org/abs/2403.09030)

    该研究开发了一个深度学习模型，通过声学信号对风力涡轮机发电机中的轴承故障进行分类，取得了在准确性和泛化能力方面的出色成果。

    

    该研究旨在开发一个深度学习模型，用于通过声学信号对风力涡轮机发电机中的轴承故障进行分类。通过使用来自五种预定义故障类型的音频数据进行训练和验证成功构建和训练了一个卷积LSTM模型。为了创建数据集，收集并处理原始音频信号数据，将其处理成帧以捕获时间和频率域信息。该模型在训练样本上表现出色，并在验证过程中展现出极佳的泛化能力，表明其具有出色的泛化能力。在测试样本上，该模型取得了显著的分类性能，整体准确率超过99.5％，正常状态的假阳性率低于1％。该研究结果为风力涡轮机发电机中轴承故障的诊断和维护提供了重要支持，具有潜在的...

    arXiv:2403.09030v1 Announce Type: cross  Abstract: This study aimed to develop a deep learning model for the classification of bearing faults in wind turbine generators from acoustic signals. A convolutional LSTM model was successfully constructed and trained by using audio data from five predefined fault types for both training and validation. To create the dataset, raw audio signal data was collected and processed in frames to capture time and frequency domain information. The model exhibited outstanding accuracy on training samples and demonstrated excellent generalization ability during validation, indicating its proficiency of generalization capability. On the test samples, the model achieved remarkable classification performance, with an overall accuracy exceeding 99.5%, and a false positive rate of less than 1% for normal status. The findings of this study provide essential support for the diagnosis and maintenance of bearing faults in wind turbine generators, with the potential
    
[^78]: 神经网络推断对高数据速率、低延迟科学应用的架构影响

    Architectural Implications of Neural Network Inference for High Data-Rate, Low-Latency Scientific Applications

    [https://arxiv.org/abs/2403.08980](https://arxiv.org/abs/2403.08980)

    神经网络推断在高数据速率、低延迟科学应用中的架构要求：所有参数存储在芯片上，需要协同设计自定义/可重构逻辑以满足极端的延迟和带宽约束

    

    随着越来越多的科学领域依赖神经网络（NNs）处理极高吞吐量和延迟的数据，开发所有参数存储在芯片上的神经网络是至关重要的。在许多应用中，没有足够的时间去从芯片外检索权重。此外，例如DRAM的芯片外内存没有足够的带宽来按照数据产生的速度快速处理这些神经网络（例如，每25纳秒）。因此，这些极端的延迟和带宽要求对旨在运行这些神经网络的硬件有架构上的影响：1）所有神经网络参数必须适合在芯片上，2）通常需要协同设计自定义/可重构逻辑以满足这些延迟和带宽约束。在我们的工作中，我们展示了许多科学神经网络应用必须完全在芯片上运行，极端情况下需要定制芯片来满足如此严格的约束。

    arXiv:2403.08980v1 Announce Type: new  Abstract: With more scientific fields relying on neural networks (NNs) to process data incoming at extreme throughputs and latencies, it is crucial to develop NNs with all their parameters stored on-chip. In many of these applications, there is not enough time to go off-chip and retrieve weights. Even more so, off-chip memory such as DRAM does not have the bandwidth required to process these NNs as fast as the data is being produced (e.g., every 25 ns). As such, these extreme latency and bandwidth requirements have architectural implications for the hardware intended to run these NNs: 1) all NN parameters must fit on-chip, and 2) codesigning custom/reconfigurable logic is often required to meet these latency and bandwidth constraints. In our work, we show that many scientific NN applications must run fully on chip, in the extreme case requiring a custom chip to meet such stringent constraints.
    
[^79]: AutoGuide: 大型语言模型代理的自动生成和选择状态感知指南

    AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents

    [https://arxiv.org/abs/2403.08978](https://arxiv.org/abs/2403.08978)

    AutoGuide通过提取嵌入在离线数据中的知识，生成一组状态感知指南，从而弥合大型语言模型中的知识差距，为代理的决策过程提供有用的知识。

    

    大型语言模型（LLMs）的主要局限性是它们对世界的理解受限。这给基于LLMs的代理带来了重大困难，特别是在预训练的LLMs缺乏足够知识的领域。在本文中，我们介绍了一个名为AutoGuide的新框架，通过利用离线经验中的隐含知识来弥合预训练LLMs中的知识差距。具体而言，AutoGuide通过提取一组状态感知指南有效地提取嵌入在离线数据中的知识。每个状态感知指南以简洁的自然语言表达，并遵循条件结构，清晰描述适用的状态。因此，由此产生的指南为向代理当前的决策过程提供有用的知识提供了一种原则性的方法。我们展示了我们的方法在顺序任务中大幅领先于竞争的基于LLMs的基线。

    arXiv:2403.08978v1 Announce Type: new  Abstract: The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential
    
[^80]: 全尺寸装配模拟测试台（FAST）数据集

    The Full-scale Assembly Simulation Testbed (FAST) Dataset

    [https://arxiv.org/abs/2403.08969](https://arxiv.org/abs/2403.08969)

    提出了一个新的用于机器学习目的的VR数据集，包括108名参与者在VR中学习组装两种不同全尺寸结构的数据，并探讨了未来研究人员如何利用这个数据集。

    

    近年来，许多研究人员已经开始研究虚拟现实（VR）跟踪和交互数据如何用于各种机器学习目的，包括用户识别、预测网络晕动症和估算学习增益。本文介绍了一个新的公开数据集，该数据集是使用我们的基于VR的全尺寸装配模拟测试台（FAST）捕获的。这个数据集包括从108名参与者（50名女性，56名男性，2名非二元性别）学习如何在VR中组装两种不同全尺寸结构时收集的数据。除了解释数据集是如何收集的并描述包含的数据外，我们还讨论了未来研究人员如何使用这个数据集。

    arXiv:2403.08969v1 Announce Type: cross  Abstract: In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.
    
[^81]: 基于深度学习和库普曼理论的轨道问题动力学识别与线性化

    Deep Learning Based Dynamics Identification and Linearization of Orbital Problems using Koopman Theory

    [https://arxiv.org/abs/2403.08965](https://arxiv.org/abs/2403.08965)

    通过深度学习和库普曼理论，提出了一种数据驱动框架，可以同时识别“两体问题”和“圆限制三体问题”的动力学，并将其全局线性化成线性时不变系统。

    

    航空航天工程和科学领域中对“两体问题”和“圆限制三体问题”的研究非常重要，因为它们有助于描述天体和人造卫星的运动。随着对卫星和卫星编队飞行的需求日益增长，对这些系统进行快速有效的控制变得越来越重要。我们提出了一个数据驱动框架，通过基于深度学习的库普曼理论实现“两体问题”和“圆限制三体问题”的同时系统识别和全局线性化，即通过纯数据驱动训练深度神经网络来发现线性库普曼算子，并将其全局线性化为线性时不变系统（LTI）系统。

    arXiv:2403.08965v1 Announce Type: cross  Abstract: The study of the Two-Body and Circular Restricted Three-Body Problems in the field of aerospace engineering and sciences is deeply important because they help describe the motion of both celestial and artificial satellites. With the growing demand for satellites and satellite formation flying, fast and efficient control of these systems is becoming ever more important. Global linearization of these systems allows engineers to employ methods of control in order to achieve these desired results. We propose a data-driven framework for simultaneous system identification and global linearization of both the Two-Body Problem and Circular Restricted Three-Body Problem via deep learning-based Koopman Theory, i.e., a framework that can identify the underlying dynamics and globally linearize it into a linear time-invariant (LTI) system. The linear Koopman operator is discovered through purely data-driven training of a Deep Neural Network with a 
    
[^82]: 朝向高效的风险敏感策略梯度：一个迭代复杂度分析

    Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis

    [https://arxiv.org/abs/2403.08955](https://arxiv.org/abs/2403.08955)

    本文对风险敏感策略梯度方法进行了迭代复杂度分析，发现其能够通过使用指数效用函数达到较低的迭代复杂度。

    

    强化学习在各种应用中表现出色，使得自主智能体能够通过与环境的互动学习最佳策略。然而，传统的强化学习框架在迭代复杂度和鲁棒性方面经常面临挑战。风险敏感强化学习平衡了期望回报和风险，具有产生概率鲁棒策略的潜力，但其迭代复杂度分析尚未得到充分探讨。在本研究中，我们针对风险敏感策略梯度方法进行了彻底的迭代复杂度分析，重点关注REINFORCE算法并采用指数效用函数。我们获得了一个$\mathcal{O}(\epsilon^{-2})$的迭代复杂度，以达到$\epsilon$-近似的一阶稳定点（FOSP）。我们研究了风险敏感算法是否可以比风险中性算法实现更好的迭代复杂度。

    arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
    
[^83]: 可用的XAI：在LLM时代利用可解释性的10个策略

    Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era

    [https://arxiv.org/abs/2403.08946](https://arxiv.org/abs/2403.08946)

    在大型语言模型时代，为了适应其复杂性和先进能力，我们引入了可用的XAI概念，通过积极增强LLMs在实际环境中的生产力和适用性，实现XAI方法论的重大转变。

    

    可解释人工智能（XAI）指的是提供人类可理解的洞见，揭示人工智能模型的运作方式的技术。最近，XAI的重点正被扩展到常常因为不透明而备受批评的大型语言模型（LLMs）。这一拓展需要对XAI方法论进行显著转变，因为有两个原因。首先，许多现有的XAI方法无法直接应用于LLMs，因为它们的复杂性和先进能力。其次，随着LLMs越来越广泛地应用于不同行业应用中，XAI的角色从仅仅打开“黑匣子”转变为积极增强LLMs在实际环境中的生产力和适用性。与此同时，不同于传统机器学习模型仅作为XAI洞见的被动接受者，LLMs的独特能力能够相互增强XAI。因此，在本文中，我们通过分析（1）...

    arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
    
[^84]: 面向模型无关后验逼近的快速准确变分自编码器

    Towards Model-Agnostic Posterior Approximation for Fast and Accurate Variational Autoencoders

    [https://arxiv.org/abs/2403.08941](https://arxiv.org/abs/2403.08941)

    最近的研究表明，为了确保高质量的推断模型，可以通过迭代训练最大化与推断模型相关的目标函数，以解决变分自编码器中推断模型近似不准确导致的局部最优解问题。

    

    变分自编码器（VAEs）的推断包括学习两个模型：（1）生成模型，将潜在空间上的简单分布转换为观测数据分布，以及（2）推断模型，近似给定数据的潜在编码后验。这两个组件通过对生成模型对数边际似然的下界进行联合学习。在联合训练的早期阶段，推断模型很差地近似了潜在编码后验。最近的研究表明，这导致优化陷入局部最优解，对学习到的生成模型造成负面影响。因此，最近的研究建议通过迭代训练确保高质量的推断模型：相对于生成模型的每次更新之前最大化与推断模型相关的目标函数。不幸的是，迭代训练效率低，需要启发式标准来从迭代中恢复。

    arXiv:2403.08941v1 Announce Type: cross  Abstract: Inference for Variational Autoencoders (VAEs) consists of learning two models: (1) a generative model, which transforms a simple distribution over a latent space into the distribution over observed data, and (2) an inference model, which approximates the posterior of the latent codes given data. The two components are learned jointly via a lower bound to the generative model's log marginal likelihood. In early phases of joint training, the inference model poorly approximates the latent code posteriors. Recent work showed that this leads optimization to get stuck in local optima, negatively impacting the learned generative model. As such, recent work suggests ensuring a high-quality inference model via iterative training: maximizing the objective function relative to the inference model before every update to the generative model. Unfortunately, iterative training is inefficient, requiring heuristic criteria for reverting from iterative
    
[^85]: FogGuard: 使用感知损失保护YOLO免受雾霾影响

    FogGuard: guarding YOLO against fog using perceptual loss

    [https://arxiv.org/abs/2403.08939](https://arxiv.org/abs/2403.08939)

    FogGuard提出了一种针对雾天气条件挑战的新型雾感知目标检测网络，通过微调数据收集的方法来提高目标检测算法在恶劣天气条件下的可靠性。

    

    在本文中，我们提出了一种新颖的雾感知目标检测网络，称为FogGuard，旨在解决雾天气条件带来的挑战。自动驾驶系统严重依赖准确的目标检测算法，但恶劣的天气条件会显著影响深度神经网络（DNN）的可靠性。现有方法可分为两类，1）图像增强（如IA-YOLO）和2）基于领域适应的方法。图像增强技术试图生成无雾图像，然而，从有雾图像中恢复无雾图像比在有雾图像中检测对象要困难得多。另一方面，基于领域适应的方法没有利用目标领域中的标记数据集。这两类方法都在尝试解决问题的更难版本。我们的方法建立在对原始标注数据的微调之上。

    arXiv:2403.08939v1 Announce Type: cross  Abstract: In this paper, we present a novel fog-aware object detection network called FogGuard, designed to address the challenges posed by foggy weather conditions. Autonomous driving systems heavily rely on accurate object detection algorithms, but adverse weather conditions can significantly impact the reliability of deep neural networks (DNNs).   Existing approaches fall into two main categories, 1) image enhancement such as IA-YOLO 2) domain adaptation based approaches. Image enhancement based techniques attempt to generate fog-free image. However, retrieving a fogless image from a foggy image is a much harder problem than detecting objects in a foggy image. Domain-adaptation based approaches, on the other hand, do not make use of labelled datasets in the target domain. Both categories of approaches are attempting to solve a harder version of the problem. Our approach builds over fine-tuning on the   Our framework is specifically designed t
    
[^86]: Kernel Ridge Regression的非渐近理论：确定性等价，测试误差和GCV估计

    A non-asymptotic theory of Kernel Ridge Regression: deterministic equivalents, test error, and GCV estimator

    [https://arxiv.org/abs/2403.08938](https://arxiv.org/abs/2403.08938)

    该论文证明了对于满足一定核特征值分解的谱和集中性质的一般类问题，具有一种非渐近确定性近似的等价性。

    

    我们考虑使用核岭回归（KRR）学习未知目标函数$f_*$，给定i.i.d.数据$(u_i,y_i)$，$i\leq n$，其中$u_i \in U$是一个协变量向量，$y_i = f_* (u_i) +\varepsilon_i \in \mathbb{R}$。最近的研究连续表明，KRR的测试误差可以通过一个从核算子的谱依赖的等价序列模型导出的封闭形式估计很好地近似。然而，对于此等价性的理论证明迄今为止要么依赖于限制性假设--如次高斯独立本征函数--，要么对高维具体核进行渐近推导。

    arXiv:2403.08938v1 Announce Type: cross  Abstract: We consider learning an unknown target function $f_*$ using kernel ridge regression (KRR) given i.i.d. data $(u_i,y_i)$, $i\leq n$, where $u_i \in U$ is a covariate vector and $y_i = f_* (u_i) +\varepsilon_i \in \mathbb{R}$. A recent string of work has empirically shown that the test error of KRR can be well approximated by a closed-form estimate derived from an `equivalent' sequence model that only depends on the spectrum of the kernel operator. However, a theoretical justification for this equivalence has so far relied either on restrictive assumptions -- such as subgaussian independent eigenfunctions -- , or asymptotic derivations for specific kernels in high dimensions.   In this paper, we prove that this equivalence holds for a general class of problems satisfying some spectral and concentration properties on the kernel eigendecomposition. Specifically, we establish in this setting a non-asymptotic deterministic approximation for 
    
[^87]: 高效计算与私有数据集的相似性

    Efficiently Computing Similarities to Private Datasets

    [https://arxiv.org/abs/2403.08917](https://arxiv.org/abs/2403.08917)

    提出了一种能够高效计算与私有数据集相似性的方法，改进了先前工作的理论结果，提供了更好的隐私-效用权衡和更快的查询时间。

    

    许多差分私有模型训练方法依赖于计算查询点（如公共数据或合成数据）与私有数据之间的相似性。我们将这个常见子例程抽象出来，并研究以下基本算法问题：给定一个相似性函数$f$和一个大的高维私有数据集$X \subset \mathbb{R}^d$，输出一个差分私有（DP）数据结构，用于近似计算任何查询$y$的$\sum_{x \in X} f(x,y)$。我们考虑$f$为核函数的情况，例如$f(x,y) = e^{-\|x-y\|_2^2/\sigma^2}$（也称为差分私有核密度估计），或者距离函数的情况，如$f(x,y) = \|x-y\|_2$等。我们的理论结果改进了先前的工作，并为一系列核函数和距离函数提供了更好的隐私-效用权衡和更快的查询时间。我们结果背后的统一方法是利用“低维结构”。

    arXiv:2403.08917v1 Announce Type: cross  Abstract: Many methods in differentially private model training rely on computing the similarity between a query point (such as public or synthetic data) and private data. We abstract out this common subroutine and study the following fundamental algorithmic problem: Given a similarity function $f$ and a large high-dimensional private dataset $X \subset \mathbb{R}^d$, output a differentially private (DP) data structure which approximates $\sum_{x \in X} f(x,y)$ for any query $y$. We consider the cases where $f$ is a kernel function, such as $f(x,y) = e^{-\|x-y\|_2^2/\sigma^2}$ (also known as DP kernel density estimation), or a distance function such as $f(x,y) = \|x-y\|_2$, among others.   Our theoretical results improve upon prior work and give better privacy-utility trade-offs as well as faster query times for a wide range of kernels and distance functions. The unifying approach behind our results is leveraging `low-dimensional structures' pre
    
[^88]: 一个用于在不确定性下发现可信神经网络代理模型的战略框架

    A Framework for Strategic Discovery of Credible Neural Network Surrogate Models under Uncertainty

    [https://arxiv.org/abs/2403.08901](https://arxiv.org/abs/2403.08901)

    提出了一个名为Occam Plausibility Algorithm for surrogate models (OPAL-surrogate)的框架，通过基于神经网络的代理模型，采用层次贝叶斯推理和模型验证测试来评估代理模型的可信度和预测可靠性。

    

    深度神经网络在开发复杂物理系统高保真仿真的数据驱动代理模型中的广泛整合，凸显了稳健不确定性量化技术和可信度评估方法的重要性，确保代理模型可可靠地用于重要决策。本研究提出了用于代理模型的Occam Plausibility Algorithm（OPAL-surrogate），提供了一个系统框架，以在大量潜在模型（包括各种神经网络类别以及架构和超参数选择）中揭示具有预测能力的基于神经网络的代理模型。框架基于层次贝叶斯推理，并采用模型验证测试来评估在不确定性下代理模型的可信度和预测可靠性。通过利用这些原则，OPAL-surrogate引入了一种系统性和

    arXiv:2403.08901v1 Announce Type: cross  Abstract: The widespread integration of deep neural networks in developing data-driven surrogate models for high-fidelity simulations of complex physical systems highlights the critical necessity for robust uncertainty quantification techniques and credibility assessment methodologies, ensuring the reliable deployment of surrogate models in consequential decision-making. This study presents the Occam Plausibility Algorithm for surrogate models (OPAL-surrogate), providing a systematic framework to uncover predictive neural network-based surrogate models within the large space of potential models, including various neural network classes and choices of architecture and hyperparameters. The framework is grounded in hierarchical Bayesian inferences and employs model validation tests to evaluate the credibility and prediction reliability of the surrogate models under uncertainty. Leveraging these principles, OPAL-surrogate introduces a systematic and
    
[^89]: 一次性平均化在马尔可夫采样下的分布式TD($\lambda$)

    One-Shot Averaging for Distributed TD($\lambda$) Under Markov Sampling

    [https://arxiv.org/abs/2403.08896](https://arxiv.org/abs/2403.08896)

    在分布式强化学习中，通过一次性平均化的方法，每个agent独立进行TD($\lambda$)运算，并最终在结果上进行平均，实现了相对于以往工作更少的通信量要求的线性加速。

    

    我们考虑一种分布式强化学习设置，每个agent都拥有相同的马尔可夫决策过程副本，但是转换是独立地从相应的马尔可夫链中由每个agent采样的。我们展示在这种设置下，我们可以实现对于TD($\lambda$)的线性加速，这是一系列流行的用于策略评估的方法，即若目标精度足够小，$N$个agents可以以$N$倍速度评估一个策略。值得注意的是，这种加速是通过“一次性平均化”实现的，即agent们独立地使用马尔可夫采样运行TD($\lambda$)，并且仅在最后一步之后对他们的结果进行平均。相对于以前的工作，这显著减少了实现线性加速所需的通信量。

    arXiv:2403.08896v1 Announce Type: new  Abstract: We consider a distributed setup for reinforcement learning, where each agent has a copy of the same Markov Decision Process but transitions are sampled from the corresponding Markov chain independently by each agent. We show that in this setting, we can achieve a linear speedup for TD($\lambda$), a family of popular methods for policy evaluation, in the sense that $N$ agents can evaluate a policy $N$ times faster provided the target accuracy is small enough. Notably, this speedup is achieved by ``one shot averaging,'' a procedure where the agents run TD($\lambda$) with Markov sampling independently and only average their results after the final step. This significantly reduces the amount of communication required to achieve a linear speedup relative to previous work.
    
[^90]: REFRESH: 由SHAP值指导的负责任和高效的特征重选择

    REFRESH: Responsible and Efficient Feature Reselection Guided by SHAP Values

    [https://arxiv.org/abs/2403.08880](https://arxiv.org/abs/2403.08880)

    本文介绍了特征重选择的问题，使得特征可以根据次要模型性能特征进行选择，从而纠正与负责任人工智能相关的模型特征。

    

    特征选择是构建机器学习模型中至关重要的一步。这个过程通常以准确性为目标，对于大规模数据集来说可能既烦琐又计算密集。模型性能特征，如公正性和稳健性，也对模型开发至关重要。随着法规推动对更加可信模型的需求，部署模型需要纠正与负责任人工智能相关的模型特征。当特征选择是针对一个模型性能特征（例如准确性）时，以其他模型性能特征（如公正性和稳健性）为目标的特征选择将需要从头开始进行计算密集型选择过程。在本文中，我们引入了特征重选择的问题，以便可以根据次要

    arXiv:2403.08880v1 Announce Type: new  Abstract: Feature selection is a crucial step in building machine learning models. This process is often achieved with accuracy as an objective, and can be cumbersome and computationally expensive for large-scale datasets. Several additional model performance characteristics such as fairness and robustness are of importance for model development. As regulations are driving the need for more trustworthy models, deployed models need to be corrected for model characteristics associated with responsible artificial intelligence. When feature selection is done with respect to one model performance characteristic (eg. accuracy), feature selection with secondary model performance characteristics (eg. fairness and robustness) as objectives would require going through the computationally expensive selection process from scratch. In this paper, we introduce the problem of feature \emph{reselection}, so that features can be selected with respect to secondary 
    
[^91]: 使用自适应分布式强化学习进行多目标优化

    Multi-Objective Optimization Using Adaptive Distributed Reinforcement Learning

    [https://arxiv.org/abs/2403.08879](https://arxiv.org/abs/2403.08879)

    提出了一种在智能交通系统环境下进行多目标、多智能体强化学习的算法，具有高学习效率和低计算要求。

    

    智能交通系统（ITS）环境被认为是动态和分布式的，参与者（车辆用户，运营商等）具有多个、不断变化且可能相互冲突的目标。虽然强化学习（RL）算法通常用于优化ITS应用，如资源管理和卸载，但大多数RL算法专注于单一目标。在许多情况下，将多目标问题转化为单一目标是不可能的、棘手的或不足的，这使得这种RL算法不适用。我们提出了一种具有高学习效率和低计算要求的多目标多代理强化学习（MARL）算法，该算法在动态、分布式和嘈杂的环境中自动触发自适应的少样本学习，并具有稀疏和延迟奖励。我们在具有边缘云计算的ITS环境中测试了我们的算法。实证结果表明

    arXiv:2403.08879v1 Announce Type: cross  Abstract: The Intelligent Transportation System (ITS) environment is known to be dynamic and distributed, where participants (vehicle users, operators, etc.) have multiple, changing and possibly conflicting objectives. Although Reinforcement Learning (RL) algorithms are commonly applied to optimize ITS applications such as resource management and offloading, most RL algorithms focus on single objectives. In many situations, converting a multi-objective problem into a single-objective one is impossible, intractable or insufficient, making such RL algorithms inapplicable. We propose a multi-objective, multi-agent reinforcement learning (MARL) algorithm with high learning efficiency and low computational requirements, which automatically triggers adaptive few-shot learning in a dynamic, distributed and noisy environment with sparse and delayed reward. We test our algorithm in an ITS environment with edge cloud computing. Empirical results show that
    
[^92]: 清晰瞬间：使用Moment Pooling简化机器学习中的潜在空间

    Moments of Clarity: Streamlining Latent Spaces in Machine Learning using Moment Pooling

    [https://arxiv.org/abs/2403.08854](https://arxiv.org/abs/2403.08854)

    提出了一种称为Moment Pooling的新方法，通过将Deep Sets中的求和泛化为任意的多变量矩，显著降低机器学习网络的潜在空间维度，在固定的潜在维度下实现更高的有效潜在维度，从而可以直接可视化和解释内部表示。

    

    许多机器学习应用涉及学习数据的潜在表示，通常是高维且难以直接解释。在这项工作中，我们提出了“Moment Pooling”，这是Deep Sets网络的一个自然延伸，可大幅减少这些网络的潜在空间维度，同时维持甚至提高性能。Moment Pooling将Deep Sets中的求和泛化为任意的多变量矩，使模型能够在固定的潜在维度下实现更高的有效潜在维度。我们将Moment Pooling应用于夸克/胶子喷注分类的对撞机物理任务，通过将Energy Flow Networks（EFNs）扩展为Moment EFNs。我们发现，具有小至1的潜在维度的Moment EFNs表现与具有较高潜在维度的普通EFNs类似。这种小潜在维度使内部表示可以直接可视化和解释。

    arXiv:2403.08854v1 Announce Type: cross  Abstract: Many machine learning applications involve learning a latent representation of data, which is often high-dimensional and difficult to directly interpret. In this work, we propose "Moment Pooling", a natural extension of Deep Sets networks which drastically decrease latent space dimensionality of these networks while maintaining or even improving performance. Moment Pooling generalizes the summation in Deep Sets to arbitrary multivariate moments, which enables the model to achieve a much higher effective latent dimensionality for a fixed latent dimension. We demonstrate Moment Pooling on the collider physics task of quark/gluon jet classification by extending Energy Flow Networks (EFNs) to Moment EFNs. We find that Moment EFNs with latent dimensions as small as 1 perform similarly to ordinary EFNs with higher latent dimension. This small latent dimension allows for the internal representation to be directly visualized and interpreted, w
    
[^93]: PAPERCLIP：使用多模态模型将天文观测和自然语言关联起来

    PAPERCLIP: Associating Astronomical Observations and Natural Language with Multi-Modal Models

    [https://arxiv.org/abs/2403.08851](https://arxiv.org/abs/2403.08851)

    该研究提出了PAPERCLIP方法，通过将天文观测与自然语言关联起来，利用预训练的CLIP模型进行微调，实现了观测和自然语言之间的有意义的联合表示。

    

    我们提出了PAPERCLIP（Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training），一种使用神经网络模型将由望远镜成像的天文观测与自然语言关联起来的方法。该模型是从经过预训练的对比语言-图像预训练（CLIP）模型微调而来，使用成功的观测提案摘要和相应的下游观测，其中摘要可选择通过使用大型语言模型（LLMs）进行引导生成来进行总结。以哈勃空间望远镜（HST）的观测为例，我们展示了微调的模型通过针对图像检索（即使用自然语言查询找到最相关的观测）和描述检索（即查询与天文物体类别和用例最相关的内容）的测试，体现了观测和自然语言之间的有意义的联合表示。

    arXiv:2403.08851v1 Announce Type: cross  Abstract: We present PAPERCLIP (Proposal Abstracts Provide an Effective Representation for Contrastive Language-Image Pre-training), a method which associates astronomical observations imaged by telescopes with natural language using a neural network model. The model is fine-tuned from a pre-trained Contrastive Language-Image Pre-training (CLIP) model using successful observing proposal abstracts and corresponding downstream observations, with the abstracts optionally summarized via guided generation using large language models (LLMs). Using observations from the Hubble Space Telescope (HST) as an example, we show that the fine-tuned model embodies a meaningful joint representation between observations and natural language through tests targeting image retrieval (i.e., finding the most relevant observations using natural language queries) and description retrieval (i.e., querying for astrophysical object classes and use cases most relevant to a 
    
[^94]: JAXbind: 将任何函数绑定到JAX

    JAXbind: Bind any function to JAX

    [https://arxiv.org/abs/2403.08847](https://arxiv.org/abs/2403.08847)

    JAXbind旨在大幅减少将其他编程语言中实现的自定义函数绑定到JAX所需的工作量，提供易于使用的Python接口定义自定义JAX原语。

    

    JAX被广泛应用在机器学习和科学计算中，后者经常依赖于现有的高性能代码，我们希望能够将其纳入JAX中。重新在JAX中实现现有代码通常是不现实的，而JAX中用于绑定自定义代码的现有接口需要对JAX及其C++后端有深入了解。JAXbind的目标是大大减少将其他编程语言中实现的自定义函数绑定到JAX所需的工作量。具体而言，JAXbind提供了一个易于使用的Python接口，用于定义支持任意JAX转换的自定义所谓JAX原语。

    arXiv:2403.08847v1 Announce Type: cross  Abstract: JAX is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into JAX. Reimplementing the existing code in JAX is often impractical and the existing interface in JAX for binding custom code requires deep knowledge of JAX and its C++ backend. The goal of JAXbind is to drastically reduce the effort required to bind custom functions implemented in other programming languages to JAX. Specifically, JAXbind provides an easy-to-use Python interface for defining custom so-called JAX primitives that support arbitrary JAX transformations.
    
[^95]: 单上下文大批量抽样的分叉注意力

    Bifurcated Attention for Single-Context Large-Batch Sampling

    [https://arxiv.org/abs/2403.08845](https://arxiv.org/abs/2403.08845)

    分叉注意力是针对语言模型推断中单上下文批量抽样环境开发的方法，通过将注意力机制分成两个独立的操作来减少冗余内存IO成本，提高效率并降低延迟。

    

    在我们的研究中，我们提出了一种称为分叉注意力的方法，用于单上下文批量抽样环境下的语言模型推断。这种方法旨在减少冗余的内存IO成本，这是高批量大小和长上下文长度的延迟的重要因素。分叉注意力通过在增量解码期间将注意力机制划分为两个不同的GEMM操作，分别专注于来自预填充的KV缓存以及解码过程，从而实现这一目标。该方法确保了精确的计算，并维持常规注意力机制的计算负载（FLOPs），但减少了内存IO。分叉注意力还与减少KV缓存内存IO已知的多查询注意力机制兼容，进一步实现更高的批量大小和上下文长度。由此带来的效率导致更低的延迟，改善了实时应用的适用性，例如实现大规模并行的答案生成。

    arXiv:2403.08845v1 Announce Type: cross  Abstract: In our study, we present bifurcated attention, a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation 
    
[^96]: 学习增强的邻域选择用于带时间窗的车辆路径问题

    Learning-Enhanced Neighborhood Selection for the Vehicle Routing Problem with Time Windows

    [https://arxiv.org/abs/2403.08839](https://arxiv.org/abs/2403.08839)

    该论文提出了一种将机器学习整合到大邻域搜索中的方法，称为学习增强的邻域选择（LENS），用于在解决车辆路径问题时辅助确定解决方案中应该破坏和修复的部分。

    

    大邻域搜索（LNS）是一种通用方法，广泛适用且在实践中已被证明在解决优化问题方面非常高效。我们提出将机器学习（ML）整合到LNS中，以帮助决定在每次LNS迭代中应破坏和修复哪些解决方案的部分。我们将新方法称为学习增强的邻域选择（LENS）。我们的方法是普适的，即可应用于任何LNS算法以增强破坏算法的作用。在本文中，我们展示了LENS在基本的带时间窗的车辆路径问题（VRPTW）上的潜力。我们为VRPTW实现了一个LNS算法，并收集了从众所周知、广泛使用的基准数据集衍生出的新颖训练实例的数据。我们用这些数据训练了我们的LENS方法，并将我们的方法的实验结果与两个进行了对比。

    arXiv:2403.08839v1 Announce Type: new  Abstract: Large Neighborhood Search (LNS) is a universal approach that is broadly applicable and has proven to be highly efficient in practice for solving optimization problems. We propose to integrate machine learning (ML) into LNS to assist in deciding which parts of the solution should be destroyed and repaired in each iteration of LNS. We refer to our new approach as Learning-Enhanced Neighborhood Selection (LENS for short). Our approach is universally applicable, i.e., it can be applied to any LNS algorithm to amplify the workings of the destroy algorithm. In this paper, we demonstrate the potential of LENS on the fundamental Vehicle Routing Problem with Time Windows (VRPTW). We implemented an LNS algorithm for VRPTW and collected data on generated novel training instances derived from well-known, extensively utilized benchmark datasets. We trained our LENS approach with this data and compared the experimental results of our approach with two
    
[^97]: 基于分层轨迹表示的船舶行为预测聚类

    Predictive Clustering of Vessel Behavior Based on Hierarchical Trajectory Representation

    [https://arxiv.org/abs/2403.08838](https://arxiv.org/abs/2403.08838)

    提出了一种基于分层轨迹表示的船舶行为预测聚类方法，通过使用预测聚类和潜在编码，可以同时改善聚类和预测，并在实验证明其相对于现有方法的优越性。

    

    船舶轨迹聚类旨在寻找相似的轨迹模式，在海上应用中被广泛应用。大多数传统方法使用预定义的规则和阈值来识别离散的船舶行为，但存在无法表示演变过程的问题。为解决这一问题，本文提出了一种基于分层船舶行为预测聚类（PC-HiV）的方法。PC-HiV首先使用分层表示将每条轨迹转换为行为序列，然后基于这些表示在每个时间戳预测演化。通过应用预测聚类和潜在编码，PC-HiV可以同时改善聚类和预测。在真实AIS数据集上的实验证明了PC-HiV相对于现有方法的优越性，展示了其在捕捉船舶行为模式方面的有效性。

    arXiv:2403.08838v1 Announce Type: cross  Abstract: Vessel trajectory clustering, which aims to find similar trajectory patterns, has been widely leveraged in overwater applications. Most traditional methods use predefined rules and thresholds to identify discrete vessel behaviors. They aim for high-quality clustering and conduct clustering on entire sequences, whether the original trajectory or its sub-trajectories, failing to represent their evolution. To resolve this problem, we propose a Predictive Clustering of Hierarchical Vessel Behavior (PC-HiV). PC-HiV first uses hierarchical representations to transform every trajectory into a behavioral sequence. Then, it predicts evolution at each timestamp of the sequence based on the representations. By applying predictive clustering and latent encoding, PC-HiV improves clustering and predictions simultaneously. Experiments on real AIS datasets demonstrate PC-HiV's superiority over existing methods, showcasing its effectiveness in capturin
    
[^98]: 用于深度神经网络高效并行化的循环数据并行性

    Cyclic Data Parallelism for Efficient Parallelism of Deep Neural Networks

    [https://arxiv.org/abs/2403.08837](https://arxiv.org/abs/2403.08837)

    提出循环数据并行性，通过将微批量执行从同时改为顺序执行，以解决数据并行化中激活内存峰值和梯度平均的问题，同时还能减少所需GPU数量。

    

    训练大型深度学习模型需要并行化技术以扩展规模。在现有方法中，如数据并行性或ZeRO-DP，微批量数据被并行处理，这产生了两个缺点：在前向传递结束时模型激活所需的总内存峰值，并且梯度必须在反向传播步骤结束时同时平均。我们提出了循环数据并行性，这是一种新颖的范式，将微批量的执行从同时变为顺序执行，带有均匀的延迟。以略微梯度延迟为代价，激活所占的总内存是恒定的，并且梯度通信在训练步骤期间是平衡的。通过模型并行性，我们的技术减少了所需的GPU数量，通过在微批量之间共享GPU。在ZeRO-DP框架内，我们的技术允许使用点对点操作进行模型状态的通信，而非 t

    arXiv:2403.08837v1 Announce Type: cross  Abstract: Training large deep learning models requires parallelization techniques to scale. In existing methods such as Data Parallelism or ZeRO-DP, micro-batches of data are processed in parallel, which creates two drawbacks: the total memory required to store the model's activations peaks at the end of the forward pass, and gradients must be simultaneously averaged at the end of the backpropagation step. We propose Cyclic Data Parallelism, a novel paradigm shifting the execution of the micro-batches from simultaneous to sequential, with a uniform delay. At the cost of a slight gradient delay, the total memory taken by activations is constant, and the gradient communications are balanced during the training step. With Model Parallelism, our technique reduces the number of GPUs needed, by sharing GPUs across micro-batches. Within the ZeRO-DP framework, our technique allows communication of the model states with point-to-point operations rather t
    
[^99]: 基于结构位置编码的变压器在医疗过程监测中的知识整合

    Structural Positional Encoding for knowledge integration in transformer-based medical process monitoring

    [https://arxiv.org/abs/2403.08836](https://arxiv.org/abs/2403.08836)

    本文在医学过程监测中提出了一种基于变压器的预测性过程监测方法，通过图形位置编码技术融入本体领域特定知识，获得了令人鼓舞的实验结果。

    

    预测性过程监测是一项旨在预测正在运行的过程跟踪信息的过程挖掘任务，例如最正确的下一个要执行的活动。在医学领域，预测性过程监测可以在非典型和非平凡情况下提供有价值的决策支持。本文提出了一种依赖于变压器的预测性过程监测方法，该方法基于注意机制的深度学习架构。我们的工作的一个重要贡献在于通过图形位置编码技术融入本体领域特定知识。文章介绍并讨论了我们在医学过程监测中正在收集的令人鼓舞的实验结果。

    arXiv:2403.08836v1 Announce Type: cross  Abstract: Predictive process monitoring is a process mining task aimed at forecasting information about a running process trace, such as the most correct next activity to be executed. In medical domains, predictive process monitoring can provide valuable decision support in atypical and nontrivial situations. Decision support and quality assessment in medicine cannot ignore domain knowledge, in order to be grounded on all the available information (which is not limited to data) and to be really acceptable by end users.   In this paper, we propose a predictive process monitoring approach relying on the use of a {\em transformer}, a deep learning architecture based on the attention mechanism. A major contribution of our work lies in the incorporation of ontological domain-specific knowledge, carried out through a graph positional encoding technique. The paper presents and discusses the encouraging experimental result we are collecting in the domai
    
[^100]: 基于堆叠的深度神经网络在足球球员搜寻中的应用

    Stacking-based deep neural network for player scouting in football 1

    [https://arxiv.org/abs/2403.08835](https://arxiv.org/abs/2403.08835)

    本研究提出了一种基于堆叠的深度学习模型，在足球领域中用于检测高潜力球员，相比传统统计方法取得了显著更好的结果。

    

    DataScouting是专业体育界最知名的数据应用之一，特别是在足球领域。它的目标是分析庞大的球员数据库，以便检测潜力巨大的球员，然后由人类球探进行进一步考虑。本文提出了一种基于堆叠的深度学习模型，用于检测高潜力的足球球员。在开源数据库上应用，我们的模型取得了比传统统计方法显著更好的结果。

    arXiv:2403.08835v1 Announce Type: cross  Abstract: Datascouting is one of the most known data applications in professional sport, and specifically football. Its objective is to analyze huge database of players in order to detect high potentials that can be then individually considered by human scouts. In this paper, we propose a stacking-based deep learning model to detect high potential football players. Applied on open-source database, our model obtains significantly better results that classical statistical methods.
    
[^101]: 使用机器学习进行结核病治疗结果的预测分析：卡纳塔克邦规模的结核病数据研究

    Predictive Analysis of Tuberculosis Treatment Outcomes Using Machine Learning: A Karnataka TB Data Study at a Scale

    [https://arxiv.org/abs/2403.08834](https://arxiv.org/abs/2403.08834)

    该研究探索了如何利用机器学习和表格数据更准确地预测结核病治疗结果，并在验证集上取得了优异的性能。

    

    结核病(TB)仍然是全球健康威胁之一，在全球致死原因中排名靠前。在这种背景下，机器学习(ML)已经成为一股变革性力量，为与TB治疗相关的复杂性提供创新解决方案。本研究探讨了如何利用机器学习，特别是利用表格数据，更准确地预测结核病(TB)治疗结果。它将这个预测任务转化为一个二元分类问题，从印度的国家结核病控制项目NIKSHAY中获取的患者数据生成风险分数，该项目包括超过50万患者记录。数据预处理是研究的关键组成部分，该模型在包含2万名患者记录的验证集上实现了98%的召回率和0.95的AUC-ROC分数。我们还探讨了自然语言处理(NLP)在改进模型学习中的应用。我们的结果经过了各种考虑，

    arXiv:2403.08834v1 Announce Type: cross  Abstract: Tuberculosis (TB) remains a global health threat, ranking among the leading causes of mortality worldwide. In this context, machine learning (ML) has emerged as a transformative force, providing innovative solutions to the complexities associated with TB treatment.This study explores how machine learning, especially with tabular data, can be used to predict Tuberculosis (TB) treatment outcomes more accurately. It transforms this prediction task into a binary classification problem, generating risk scores from patient data sourced from NIKSHAY, India's national TB control program, which includes over 500,000 patient records.   Data preprocessing is a critical component of the study, and the model achieved an recall of 98% and an AUC-ROC score of 0.95 on the validation set, which includes 20,000 patient records.We also explore the use of Natural Language Processing (NLP) for improved model learning. Our results, corroborated by various m
    
[^102]: 多数三者：最简单的最优学习器？

    Majority-of-Three: The Simplest Optimal Learner?

    [https://arxiv.org/abs/2403.08831](https://arxiv.org/abs/2403.08831)

    该论文研究了可能最优的最简单算法：返回三个ERM分类器的多数投票，证明其实现了错误的期望最优边界，并得出近乎最优概率边界。

    

    在实现数据设置下发展最佳的PAC学习算法是学习理论中几十年来的一个重大开放性问题，其中经验风险最小化（ERM）是次优的。几年前，Hanneke终于解决了这个问题。不幸的是，Hanneke的算法相当复杂，因为它返回许多经过精心选择的数据子集上训练的ERM分类器的多数投票。因此，最自然的目标是确定最简单的最优算法。在这项工作中，我们研究了可能最优的最简单算法：返回三个ERM分类器的多数投票。我们展示了这个算法实现了其错误的期望最优边界，这显然是单个ERM分类器无法达到的。此外，我们证明了该算法错误的近乎最优概率边界。我们推测更好的分析将证明这个算法实际上在高

    arXiv:2403.08831v1 Announce Type: cross  Abstract: Developing an optimal PAC learning algorithm in the realizable setting, where empirical risk minimization (ERM) is suboptimal, was a major open problem in learning theory for decades. The problem was finally resolved by Hanneke a few years ago. Unfortunately, Hanneke's algorithm is quite complex as it returns the majority vote of many ERM classifiers that are trained on carefully selected subsets of the data. It is thus a natural goal to determine the simplest algorithm that is optimal. In this work we study the arguably simplest algorithm that could be optimal: returning the majority vote of three ERM classifiers. We show that this algorithm achieves the optimal in-expectation bound on its error which is provably unattainable by a single ERM classifier. Furthermore, we prove a near-optimal high-probability bound on this algorithm's error. We conjecture that a better analysis will prove that this algorithm is in fact optimal in the hig
    
[^103]: 缓解集体决策中的偏见：在面对假新闻时提升表现

    Mitigating Biases in Collective Decision-Making: Enhancing Performance in the Face of Fake News

    [https://arxiv.org/abs/2403.08829](https://arxiv.org/abs/2403.08829)

    该研究探讨了在假新闻中个人和社会偏见对人类决策的影响，发现了偏见对集体决策的渗透，并评估了自适应聚合算法在提高判断准确性方面的有效性。

    

    个人和社会偏见削弱了人类顾问的效力，因为它导致判断错误，这可能对受保护的群体造成不利。本文研究了这些偏见在假新闻这一普遍问题中可能产生的影响，通过评估人类参与者识别虚假标题的能力。我们聚焦涉及敏感特征的标题，收集了一个全面的数据集，探讨人类反应如何受他们的偏见所影响。我们的分析揭示了不断出现的个人偏见及其渗透入集体决策中。我们展示了人口因素、标题类别以及信息呈现方式显著影响人类判断中的错误。然后，我们将收集的数据作为基准问题，评估自适应聚合算法的有效性。除了提高的准确性外，我们的结果突出了新算法崛起时的相互作用。

    arXiv:2403.08829v1 Announce Type: cross  Abstract: Individual and social biases undermine the effectiveness of human advisers by inducing judgment errors which can disadvantage protected groups. In this paper, we study the influence these biases can have in the pervasive problem of fake news by evaluating human participants' capacity to identify false headlines. By focusing on headlines involving sensitive characteristics, we gather a comprehensive dataset to explore how human responses are shaped by their biases. Our analysis reveals recurring individual biases and their permeation into collective decisions. We show that demographic factors, headline categories, and the manner in which information is presented significantly influence errors in human judgment. We then use our collected data as a benchmark problem on which we evaluate the efficacy of adaptive aggregation algorithms. In addition to their improved accuracy, our results highlight the interactions between the emergence of c
    
[^104]: 适用于在线部署的真相推断算法验证数据集

    A Dataset for the Validation of Truth Inference Algorithms Suitable for Online Deployment

    [https://arxiv.org/abs/2403.08826](https://arxiv.org/abs/2403.08826)

    介绍了一个大规模真实众包标注数据集，弥补了先前真相推断算法验证数据集的局限性，为长期和在线真相推断提供了实用性。

    

    为了实现高效和具有成本效益的大规模数据标注，越来越多地利用众包。为了保证数据标注的质量，需要为每个数据样本收集多个注释，并开发了真相推断算法来准确推断真实标签。尽管先前的研究已经发布了用于评估真相推断算法有效性的公共数据集，但这些数据集通常集中在单一类型的众包任务，并忽略了与工作者注释活动相关的时间信息。这些限制严重限制了这些算法的实际适用性，特别是在长期和在线真相推断的背景下。本文介绍了从一家真实众包平台收集的大量众包标注数据集。该数据集包括约两千名工作者、一百万个任务、一

    arXiv:2403.08826v1 Announce Type: cross  Abstract: For the purpose of efficient and cost-effective large-scale data labeling, crowdsourcing is increasingly being utilized. To guarantee the quality of data labeling, multiple annotations need to be collected for each data sample, and truth inference algorithms have been developed to accurately infer the true labels. Despite previous studies having released public datasets to evaluate the efficacy of truth inference algorithms, these have typically focused on a single type of crowdsourcing task and neglected the temporal information associated with workers' annotation activities. These limitations significantly restrict the practical applicability of these algorithms, particularly in the context of long-term and online truth inference. In this paper, we introduce a substantial crowdsourcing annotation dataset collected from a real-world crowdsourcing platform. This dataset comprises approximately two thousand workers, one million tasks, a
    
[^105]: LoRA-SP：用于资源高效微调大型语言模型的简化部分参数适应

    LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models

    [https://arxiv.org/abs/2403.08822](https://arxiv.org/abs/2403.08822)

    LoRA-SP利用随机半选择参数冻结的新颖方法，在微调大型语言模型时有效平衡预训练知识的保留和任务特定优化的适应性，显著降低了计算和内存需求，同时实现了竞争性性能。

    

    在解决大型语言模型（LLMs）微调的计算和内存需求方面，我们提出了 LoRA-SP（简化部分参数适应），这是一种新颖的方法，利用低秩适应（LoRA）框架内的随机半选择参数冻结。该方法有效地平衡了预训练知识的保留和任务特定优化的适应性。通过随机机制，LoRA-SP 确定要更新或冻结哪些参数，显著降低了计算和内存需求，而不会影响模型性能。我们在几个基准 NLP 任务中评估了 LoRA-SP，展示了它与传统的全参数微调和其他参数高效技术相比，能够以大大降低的资源消耗实现竞争性性能。LoRA-SP 的创新方法不仅有助于在资源有限的情况下部署先进的 NLP 模型。

    arXiv:2403.08822v1 Announce Type: cross  Abstract: In addressing the computational and memory demands of fine-tuning Large Language Models(LLMs), we propose LoRA-SP(Streamlined Partial Parameter Adaptation), a novel approach utilizing randomized half-selective parameter freezing within the Low-Rank Adaptation(LoRA)framework. This method efficiently balances pre-trained knowledge retention and adaptability for task-specific optimizations. Through a randomized mechanism, LoRA-SP determines which parameters to update or freeze, significantly reducing computational and memory requirements without compromising model performance. We evaluated LoRA-SP across several benchmark NLP tasks, demonstrating its ability to achieve competitive performance with substantially lower resource consumption compared to traditional full-parameter fine-tuning and other parameter-efficient techniques. LoRA-SP innovative approach not only facilitates the deployment of advanced NLP models in resource-limited sett
    
[^106]: 通过变化估计实现有效的梯度样本大小，加速敏锐度感知最小化

    Effective Gradient Sample Size via Variation Estimation for Accelerating Sharpness aware Minimization

    [https://arxiv.org/abs/2403.08821](https://arxiv.org/abs/2403.08821)

    本文提出了一种简单而高效的抽样方法，通过基于PSF的变化来显著加速Sharpness-aware Minimization（SAM），实现了与SAM相当的最先进准确性。

    

    近期提出了“敏锐度感知最小化”（SAM）以改善模型的泛化能力。然而，SAM在每次优化步骤中都会计算梯度两次，从而使计算成本比随机梯度下降（SGD）增加一倍。本文提出了一种简单而高效的抽样方法，显著加速SAM。具体而言，我们发现SAM的梯度是SGD梯度和第一阶梯度上的二阶梯度矩阵投影（PSF）的组合。PSF在训练过程中表现出逐渐增加的变化频率。为了利用这一观察结果，我们基于PSF的变化提出了一种自适应抽样方法，并将已采样的PSF重用于非抽样迭代。大量实证结果表明，所提出的方法在各种网络架构上达到了与SAM相当的最先进准确性。

    arXiv:2403.08821v1 Announce Type: cross  Abstract: Sharpness-aware Minimization (SAM) has been proposed recently to improve model generalization ability. However, SAM calculates the gradient twice in each optimization step, thereby doubling the computation costs compared to stochastic gradient descent (SGD). In this paper, we propose a simple yet efficient sampling method to significantly accelerate SAM. Concretely, we discover that the gradient of SAM is a combination of the gradient of SGD and the Projection of the Second-order gradient matrix onto the First-order gradient (PSF). PSF exhibits a gradually increasing frequency of change during the training process. To leverage this observation, we propose an adaptive sampling method based on the variation of PSF, and we reuse the sampled PSF for non-sampling iterations. Extensive empirical results illustrate that the proposed method achieved state-of-the-art accuracies comparable to SAM on diverse network architectures.
    
[^107]: Diet-ODIN：一种新型框架，用于解释性膳食模式下的阿片类药物误用检测

    Diet-ODIN: A Novel Framework for Opioid Misuse Detection with Interpretable Dietary Patterns

    [https://arxiv.org/abs/2403.08820](https://arxiv.org/abs/2403.08820)

    该研究建立了一个大规模多方面的膳食数据集，提出了Diet-ODIN框架，旨在探索膳食模式与阿片类药物误用之间的关联。

    

    阿片类药物危机一直是美国社会最关键的问题之一。尽管药物辅助治疗（MAT）被认为是阿片类药物误用和成瘾最有效的治疗方法，但各种副作用可能引发阿片类药物再次滥用。除MAT外，膳食营养干预在防止和康复阿片类药物误用中发挥了重要作用。然而，有关膳食模式与阿片类药物误用之间令人担忧的关联的研究仍未得到充分探讨。为弥补这一空白，本文首次建立了一个与阿片类药物使用者相关的大规模多方面膳食基准数据集，并开发了一个新颖框架，即解释性膳食模式下的阿片类药物误用检测（Diet-ODIN），用于连接异质图（HG）和大型语言模型（LLM），以识别阿片类药物误用者并解释其行为。

    arXiv:2403.08820v1 Announce Type: cross  Abstract: The opioid crisis has been one of the most critical society concerns in the United States. Although the medication assisted treatment (MAT) is recognized as the most effective treatment for opioid misuse and addiction, the various side effects can trigger opioid relapse. In addition to MAT, the dietary nutrition intervention has been demonstrated its importance in opioid misuse prevention and recovery. However, research on the alarming connections between dietary patterns and opioid misuse remain under-explored. In response to this gap, in this paper, we first establish a large-scale multifaceted dietary benchmark dataset related to opioid users at the first attempt and then develop a novel framework - i.e., namely Opioid Misuse Detection with Interpretable Dietary Patterns (Diet-ODIN) - to bridge heterogeneous graph (HG) and large language model (LLM) for the identification of users with opioid misuse and the interpretation of their a
    
[^108]: 温度计：面向大型语言模型的通用校准

    Thermometer: Towards Universal Calibration for Large Language Models

    [https://arxiv.org/abs/2403.08819](https://arxiv.org/abs/2403.08819)

    提出了一种针对大型语言模型的校准方法THERMOMETER，通过学习来自多个任务数据的辅助模型，实现了计算效率高、准确性保持并产生更好校准响应的目标。

    

    我们考虑大型语言模型（LLM）中的校准问题。最近的研究发现，常见的干预措施如指令调整通常会导致校准不佳的LLMs。尽管校准在传统应用中得到了很好的探讨，但对LLMs进行校准具有独特挑战。这些挑战不仅来自LLMs的严格计算要求，也来自它们的多功能性，使它们可以应用于各种任务。为了解决这些挑战，我们提出了一个针对LLMs的校准方法THERMOMETER。THERMOMETER通过学习来自多个任务的数据的辅助模型，用于校准LLM。它在计算上效率高，保持了LLM的准确性，并为新任务产生了更好的校准响应。对各种基准的广泛实证评估显示了所提方法的有效性。

    arXiv:2403.08819v1 Announce Type: cross  Abstract: We consider the issue of calibration in large language models (LLM). Recent studies have found that common interventions such as instruction tuning often result in poorly calibrated LLMs. Although calibration is well-explored in traditional applications, calibrating LLMs is uniquely challenging. These challenges stem as much from the severe computational requirements of LLMs as from their versatility, which allows them to be applied to diverse tasks. Addressing these challenges, we propose THERMOMETER, a calibration approach tailored to LLMs. THERMOMETER learns an auxiliary model, given data from multiple tasks, for calibrating a LLM. It is computationally efficient, preserves the accuracy of the LLM, and produces better-calibrated responses for new tasks. Extensive empirical evaluations across various benchmarks demonstrate the effectiveness of the proposed method.
    
[^109]: 结构和语义的电子健康记录多模态融合：将临床记录和笔记与超图和LLM集成

    Multimodal Fusion of EHR in Structures and Semantics: Integrating Clinical Records and Notes with Hypergraph and LLM

    [https://arxiv.org/abs/2403.08818](https://arxiv.org/abs/2403.08818)

    提出了一个名为MINGLE的新框架，通过两级注入策略将医学概念语义和临床笔记语义融合到超图中，有效地整合了结构和语义的电子健康记录数据。

    

    电子健康记录（EHRs）在近几十年来越来越受欢迎，支持临床决策和医疗保健。EHRs通常包含异构信息，如表格形式的结构化数据和文本笔记中的非结构化数据。EHRs中的不同类型信息可以相互补充，提供患者健康状态的更完整图片。尽管对结构化EHR数据的表示学习进行了大量研究，但不同类型EHR数据的融合（多模态融合）尚未得到充分研究。这主要是由于医疗编码系统的复杂性和书面笔记中存在的噪音和冗余。在这项工作中，我们提出了一个名为MINGLE的新框架，有效地将EHR中的结构和语义结合起来。我们的框架使用两级注入策略将医学概念语义和临床笔记语义融合到超图中。

    arXiv:2403.08818v1 Announce Type: cross  Abstract: Electronic Health Records (EHRs) have become increasingly popular to support clinical decision-making and healthcare in recent decades. EHRs usually contain heterogeneous information, such as structural data in tabular form and unstructured data in textual notes. Different types of information in EHRs can complement each other and provide a more complete picture of the health status of a patient. While there has been a lot of research on representation learning of structured EHR data, the fusion of different types of EHR data (multimodal fusion) is not well studied. This is mostly because of the complex medical coding systems used and the noise and redundancy present in the written notes. In this work, we propose a new framework called MINGLE, which integrates both structures and semantics in EHR effectively. Our framework uses a two-level infusion strategy to combine medical concept semantics and clinical note semantics into hypergrap
    
[^110]: 联合深度Q学习与5G负载平衡

    Federated Deep Q-Learning and 5G load balancing

    [https://arxiv.org/abs/2403.08813](https://arxiv.org/abs/2403.08813)

    本研究提出并分析了一个联合深度Q学习负载平衡系统，利用Open-RAN xAPP框架和近实时射频接口控制器（near-RT RIC）实施。我们的模拟结果表明，与目前UE使用的最大信噪比（MAX-SINR）方法相比，我们提出的深度Q学习模型可以持续提供更好的负载平衡性能。

    

    尽管蜂窝网络技术取得了进展，基站（BS）负载平衡仍然是一个持久性问题。虽然集中资源分配方法可以解决负载平衡问题，但仍然是一个NP难问题。本研究研究了如何利用联合深度Q学习来通知每个用户设备（UE）各个BS的负载情况。联合深度Q学习的负载平衡使得智能UE可以独立选择最佳BS，同时限制了向网络暴露的私人信息量。

    arXiv:2403.08813v1 Announce Type: cross  Abstract: Despite advances in cellular network technology, base station (BS) load balancing remains a persistent problem. Although centralized resource allocation methods can address the load balancing problem, it still remains an NP-hard problem. In this research, we study how federated deep Q learning can be used to inform each user equipment (UE) of the each BS's load conditions. Federated deep Q learning's load balancing enables intelligent UEs to independently select the best BS while also limiting the amount of private information exposed to the network.   In this study, we propose and analyze a federated deep Q learning load balancing system, which is implemented using the Open-RAN xAPP framework and the near-Real Time Radio Interface Controller (near-RT RIC). Our simulation results indicate that compared to the maximum Signal-To-Noise-Ratio (MAX-SINR) method currently used by UEs, our proposed deep Q learning model can consistently provi
    
[^111]: Gore扩散LoRA模型

    Gore Diffusion LoRA Model

    [https://arxiv.org/abs/2403.08812](https://arxiv.org/abs/2403.08812)

    本文审查了“Gore扩散LoRA模型”，这是一种擅长生成超逼真暴力和流血视觉效果的创新AI模型，强调在其创作与实施中需要认真讨论AI、艺术和暴力的融合，主张负责任的发展和道德部署这些强大技术。

    

    arXiv:2403.08812v1 公告类型: 跨界 摘要: 人工智能（AI）的出现显著影响了我们与暴力的互动，引发了关于算法创作暴力图像的伦理讨论。本文审查了“Gore扩散LoRA模型”，这是一种创新的AI模型，擅长生成表现激烈暴力和流血的超逼真视觉效果。我们的探讨涵盖了该模型的技术复杂性、可行应用以及其利用中固有的伦理困境。我们认为，这种模型的创作和实施需要认真讨论AI、艺术和暴力的融合。此外，我们主张建立一个结构化框架，倡导负责任的发展和道德部署这些强大技术。

    arXiv:2403.08812v1 Announce Type: cross  Abstract: The Emergence of Artificial Intelligence (AI) has significantly impacted our engagement with violence, sparking ethical deliberations regarding the algorithmic creation of violent imagery. This paper scrutinizes the "Gore Diffusion LoRA Model," an innovative AI model proficient in generating hyper-realistic visuals portraying intense violence and bloodshed. Our exploration encompasses the model's technical intricacies, plausible applications, and the ethical quandaries inherent in its utilization. We contend that the creation and implementation of such models warrant a meticulous discourse concerning the convergence of AI, art, and violence. Furthermore, we advocate for a structured framework advocating responsible development and ethical deployment of these potent technologies.
    
[^112]: 在物联网架构中比较边缘计算方法，用于高效估计室内环境参数与机器学习

    Comparison of edge computing methods in Internet of Things architectures for efficient estimation of indoor environmental parameters with Machine Learning

    [https://arxiv.org/abs/2403.08810](https://arxiv.org/abs/2403.08810)

    本研究提出了两种基于低成本边缘物联网架构的方法，用于实现估计室内环境质量参数的轻量级机器学习模型，为物联网架构中进行高效数据处理提供了新思路。

    

    互联网物联设备数量的大幅增加已经彻底改变了数据处理方式，加上目前从云计算转向边缘计算的趋势迫使我们需要使用能源高效的设备在数据源附近进行高效可靠的数据处理。文章提出了两种基于低成本边缘物联网架构的方法，用于实现轻量级机器学习模型，用以估计室内环境质量参数，比如多层感知器类型的人工神经网络。这些方法基于集中式和分布式并行物联网架构实现，通过无线连接，共享商用模块进行数据采集和传感，比如温度、湿度、照度、CO2和其他气体传感器。集中式方法使用图形处理单元和消息队列遥测传输协议，而分布式方法

    arXiv:2403.08810v1 Announce Type: cross  Abstract: The large increase in the number of Internet of Things (IoT) devices have revolutionised the way data is processed, which added to the current trend from cloud to edge computing has resulted in the need for efficient and reliable data processing near the data sources using energy-efficient devices. Two methods based on low-cost edge-IoT architectures are proposed to implement lightweight Machine Learning (ML) models that estimate indoor environmental quality (IEQ) parameters, such as Artificial Neural Networks of Multilayer Perceptron type. Their implementation is based on centralised and distributed parallel IoT architectures, connected via wireless, which share commercial off-the-self modules for data acquisition and sensing, such as sensors for temperature, humidity, illuminance, CO2, and other gases. The centralised method uses a Graphics Processing Unit and the Message Queuing Telemetry Transport protocol, but the distributed meth
    
[^113]: 通过对抗特征相似性学习来提升对抗性强的Deepfake检测

    Adversarially Robust Deepfake Detection via Adversarial Feature Similarity Learning

    [https://arxiv.org/abs/2403.08806](https://arxiv.org/abs/2403.08806)

    通过对抗特征相似性学习来提升对抗性强的Deepfake检测，以区分真假实例并最大化对抗性扰动和未扰动示例之间的相似性。

    

    Deepfake技术引发了对数字内容真实性的担忧，迫切需要开发有效的检测方法。然而，广泛传播的Deepfake技术也带来了对抗性攻击的新挑战，对手可以通过微小的、察觉不到的扰动来操纵Deepfake视频，欺骗检测模型产生错误输出。为了解决这一关键问题，我们引入了Adversarial Feature Similarity Learning（AFSL），该方法将三种基本的深度特征学习范式整合在一起。通过优化样本和权重向量之间的相似性，我们的方法旨在区分真实和虚假实例。此外，我们旨在最大化对抗性扰动示例和未经扰动示例之间的相似性，而不管它们是真实的还是虚假的。此外，我们引入了一种正则化技术，其最大化了

    arXiv:2403.08806v1 Announce Type: cross  Abstract: Deepfake technology has raised concerns about the authenticity of digital content, necessitating the development of effective detection methods. However, the widespread availability of deepfakes has given rise to a new challenge in the form of adversarial attacks. Adversaries can manipulate deepfake videos with small, imperceptible perturbations that can deceive the detection models into producing incorrect outputs. To tackle this critical issue, we introduce Adversarial Feature Similarity Learning (AFSL), which integrates three fundamental deep feature learning paradigms. By optimizing the similarity between samples and weight vectors, our approach aims to distinguish between real and fake instances. Additionally, we aim to maximize the similarity between both adversarially perturbed examples and unperturbed examples, regardless of their real or fake nature. Moreover, we introduce a regularization technique that maximizes the dissimil
    
[^114]: 针对尖峰神经网络的在线梯度估计的前向直接反馈对齐

    Forward Direct Feedback Alignment for Online Gradient Estimates of Spiking Neural Networks

    [https://arxiv.org/abs/2403.08804](https://arxiv.org/abs/2403.08804)

    提出了一种新颖的神经形态算法SFDFA，描述了如何在线计算准确的局部梯度，并推导出神经形态硬件的动力系统。

    

    存在一种找到比当前最先进的神经网络训练算法更节能选择的兴趣。尖峰神经网络是一种有前途的方法，因为它们可以在神经形态硬件平台上以高效节能的方式进行模拟。然而，这些平台在训练算法的设计上存在限制。最重要的是，反向传播不能在那些平台上实现。我们提出了一种新颖的神经形态算法，即\textit{尖锐前向直接反馈对齐}（SFDFA）算法，这是对\textit{前向直接反馈对齐}的改进，用于训练SNN。SFDFA将输出和隐藏神经元之间的权重估计为反馈连接。本文的主要贡献在于描述如何准确计算在线方式中的尖峰局部梯度，同时考虑到神经元内的后突触尖峰之间的依赖关系，并推导出神经形态硬件的动力系统。

    arXiv:2403.08804v1 Announce Type: cross  Abstract: There is an interest in finding energy efficient alternatives to current state of the art neural network training algorithms. Spiking neural network are a promising approach, because they can be simulated energy efficiently on neuromorphic hardware platforms. However, these platforms come with limitations on the design of the training algorithm. Most importantly, backpropagation cannot be implemented on those. We propose a novel neuromorphic algorithm, the \textit{Spiking Forward Direct Feedback Alignment} (SFDFA) algorithm, an adaption of \textit{Forward Direct Feedback Alignment} to train SNNs. SFDFA estimates the weights between output and hidden neurons as feedback connections. The main contribution of this paper is to describe how exact local gradients of spikes can be computed in an online manner while taking into account the intra-neuron dependencies between post-synaptic spikes and derive a dynamical system for neuromorphic har
    
[^115]: 企业中生成式人工智能的治理

    Governance of Generative Artificial Intelligence for Companies

    [https://arxiv.org/abs/2403.08802](https://arxiv.org/abs/2403.08802)

    本综述填补了有关企业中生成式人工智能（GenAI）治理的研究空白，提出了一个框架，旨在利用业务机会并减轻与GenAI整合相关风险。

    

    生成式人工智能（GenAI），特别是像ChatGPT这样的大型语言模型，已迅速进入企业，但缺乏充分的治理，带来机遇和挑战。尽管对GenAI具有变革性质和监管措施的广泛讨论，但有限的研究涉及组织治理，包括技术和业务视角。本综述填补了这一空白，调查了最近的研究。它不仅仅是总结，还通过制定适用于企业内的GenAI治理框架来进行。我们的框架详细描述了范围、目标和治理机制，旨在利用业务机会并减轻与GenAI整合相关风险。该研究提供了一种专注于GenAI治理的方法，为企业在负责任的AI采用挑战中提供了实用见解。对于技术人员来说，也有助于拓宽他们的视角。

    arXiv:2403.08802v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. This review paper fills this gap by surveying recent works. It goes beyond mere summarization by developing a framework for GenAI governance within companies. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities and mitigate risks associated with GenAI integration. This research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of responsible AI adoption. It is also valuable for a technical audience to broaden their perspective as inc
    
[^116]: 大规模图像分类器卷积神经网络的神经损失函数进化

    Neural Loss Function Evolution for Large-Scale Image Classifier Convolutional Neural Networks

    [https://arxiv.org/abs/2403.08793](https://arxiv.org/abs/2403.08793)

    通过神经损失函数搜索，本研究发现了三种新的损失函数NeuroLoss1、NeuroLoss2和NeuroLoss3，这些损失函数能够在大规模图像分类器卷积神经网络中发挥作用。

    

    在分类任务中，神经网络通常通过最小化交叉熵来学习，但是却使用准确度进行评估和比较。 这种差异性促使了神经损失函数搜索（NLFS），即寻找可以替代神经网络交叉熵损失函数的新损失函数。 我们将NLFS应用于图像分类器卷积神经网络。 我们提出了一个新的NLFS搜索空间，鼓励探索更多不同的损失函数，并提出了一个可以准确转换到大规模卷积神经网络的替代函数。 我们使用正则化进化进行空间搜索，这是一种仅通过变异的年龄遗传算法。 经过进化和提出的损失函数消除方案后，我们将最终得到的损失函数在多个架构、数据集和图像增强技术之间传递，以评估泛化性。最终，我们发现了三种新的损失函数，分别称为NeuroLoss1、NeuroLoss2和NeuroLoss3。

    arXiv:2403.08793v1 Announce Type: cross  Abstract: For classification, neural networks typically learn by minimizing cross-entropy, but are evaluated and compared using accuracy. This disparity suggests neural loss function search (NLFS), the search for a drop-in replacement loss function of cross-entropy for neural networks. We apply NLFS to image classifier convolutional neural networks. We propose a new search space for NLFS that encourages more diverse loss functions to be explored, and a surrogate function that accurately transfers to large-scale convolutional neural networks. We search the space using regularized evolution, a mutation-only aging genetic algorithm. After evolution and a proposed loss function elimination protocol, we transferred the final loss functions across multiple architectures, datasets, and image augmentation techniques to assess generalization. In the end, we discovered three new loss functions, called NeuroLoss1, NeuroLoss2, and NeuroLoss3 that were able 
    
[^117]: 实时面部表情识别：神经形态硬件与边缘AI加速器

    Realtime Facial Expression Recognition: Neuromorphic Hardware vs. Edge AI Accelerators

    [https://arxiv.org/abs/2403.08792](https://arxiv.org/abs/2403.08792)

    本研究比较了神经形态硬件与边缘AI加速器在实时面部表情识别中的性能，结果显示Loihi相较于Coral TPU具有更低的功耗和能耗，并保持了可比较的准确性水平。

    

    本文关注实时面部表情识别（FER）系统，作为社交机器人等各种实际应用中的重要组成部分。我们研究了在边缘部署FER机器学习（ML）模型的两种硬件选项：神经形态硬件和边缘AI加速器。研究包括对Intel Loihi神经形态处理器和四个不同边缘平台（Raspberry Pi-4、Intel神经网络计算棒（NSC）、Jetson Nano和Coral TPU）进行详尽实验，提供比较分析。结果显示，相对于Coral TPU，Loihi可以实现大约两个数量级的功耗降低和一个数量级的节能，而 Coral TPU则是功耗最低、能耗最少的边缘AI加速器。在此降低功耗和能耗的同时，神经形态解决方案保持了可比较的准确性水平。

    arXiv:2403.08792v1 Announce Type: cross  Abstract: The paper focuses on real-time facial expression recognition (FER) systems as an important component in various real-world applications such as social robotics. We investigate two hardware options for the deployment of FER machine learning (ML) models at the edge: neuromorphic hardware versus edge AI accelerators. Our study includes exhaustive experiments providing comparative analyses between the Intel Loihi neuromorphic processor and four distinct edge platforms: Raspberry Pi-4, Intel Neural Compute Stick (NSC), Jetson Nano, and Coral TPU. The results obtained show that Loihi can achieve approximately two orders of magnitude reduction in power dissipation and one order of magnitude energy savings compared to Coral TPU which happens to be the least power-intensive and energy-consuming edge AI accelerator. These reductions in power and energy are achieved while the neuromorphic solution maintains a comparable level of accuracy with the
    
[^118]: 将人类概念与计算机视觉相结合，实现可解释的人脸验证

    Bridging Human Concepts and Computer Vision for Explainable Face Verification

    [https://arxiv.org/abs/2403.08789](https://arxiv.org/abs/2403.08789)

    本文通过结合计算机和人类视觉的方法，提高了人脸验证算法解释的可解释性

    

    人工智能(AI)影响了敏感应用程序(如人脸验证)的决策过程，因此确保决策的透明性、公平性和问责制非常重要。尽管存在可解释的人工智能(XAI)技术用于澄清AI决策，但同样重要的是为人类提供这些决策的可解释性。本文提出了一种方法，结合计算机和人类视觉，增加了人脸验证算法解释的可解释性。具体而言，我们受到人类感知过程的启发，以了解机器在人脸比较任务期间如何感知人类语义区域。我们使用Mediapipe提供的分割技术，可以识别出不同的人类语义面部区域，从而实现机器的感知分析。此外，我们还调整了两种模型不可知算法，以提供人类可解释性

    arXiv:2403.08789v1 Announce Type: cross  Abstract: With Artificial Intelligence (AI) influencing the decision-making process of sensitive applications such as Face Verification, it is fundamental to ensure the transparency, fairness, and accountability of decisions. Although Explainable Artificial Intelligence (XAI) techniques exist to clarify AI decisions, it is equally important to provide interpretability of these decisions to humans. In this paper, we present an approach to combine computer and human vision to increase the explanation's interpretability of a face verification algorithm. In particular, we are inspired by the human perceptual process to understand how machines perceive face's human-semantic areas during face comparison tasks. We use Mediapipe, which provides a segmentation technique that identifies distinct human-semantic facial regions, enabling the machine's perception analysis. Additionally, we adapted two model-agnostic algorithms to provide human-interpretable i
    
[^119]: 多视角子空间聚类方法：一种自适应一致性图滤波器

    Multi-view Subspace Clustering via An Adaptive Consensus Graph Filter

    [https://arxiv.org/abs/2403.08787](https://arxiv.org/abs/2403.08787)

    本文提出一种基于自适应一致性图滤波器的多视角子空间聚类方法，通过整合一致性重构系数矩阵和来自不同视角的重构系数矩阵，实现对多视角数据集的子空间结构建模。

    

    多视角子空间聚类（MVSC）近年来引起了越来越多的关注。大多数现有的MVSC方法首先从不同视角收集互补信息，然后导出一致性重构系数矩阵以指示多视角数据集的子空间结构。本文首先假设存在一致性重构系数矩阵，然后使用它构建一致性图滤波器。在每个视角中，滤波器用于平滑数据并为重构系数矩阵设计一个正则化器。最后，从不同视角获得的重构系数矩阵被用来为一致性重构系数矩阵创建约束。因此，在提出的方法中，一致性重构系数矩阵、一致性图滤波器和来自不同视角的重构系数矩阵被整合在一起。

    arXiv:2403.08787v1 Announce Type: cross  Abstract: Multiview subspace clustering (MVSC) has attracted an increasing amount of attention in recent years. Most existing MVSC methods first collect complementary information from different views and consequently derive a consensus reconstruction coefficient matrix to indicate the subspace structure of a multi-view data set. In this paper, we initially assume the existence of a consensus reconstruction coefficient matrix and then use it to build a consensus graph filter. In each view, the filter is employed for smoothing the data and designing a regularizer for the reconstruction coefficient matrix. Finally, the obtained reconstruction coefficient matrices from different views are used to create constraints for the consensus reconstruction coefficient matrix. Therefore, in the proposed method, the consensus reconstruction coefficient matrix, the consensus graph filter, and the reconstruction coefficient matrices from different views are inte
    
[^120]: 通过热扩散实现高效的组合优化

    Efficient Combinatorial Optimization via Heat Diffusion

    [https://arxiv.org/abs/2403.08757](https://arxiv.org/abs/2403.08757)

    通过热扩散实现了高效的组合优化，克服了现有方法在搜索全局最优时效率有限的问题。

    

    论文探讨了通过热扩散来实现高效的组合优化。针对现有方法只能在每次迭代中访问解空间的一小部分这一限制，提出了一种框架来解决一般的组合优化问题，并且在一系列最具挑战性和广泛遇到的组合优化中展现出卓越性能。

    arXiv:2403.08757v1 Announce Type: cross  Abstract: Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature.The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing rec
    
[^121]: 用于人工临床记录的零样本和少样本生成策略

    Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records

    [https://arxiv.org/abs/2403.08664](https://arxiv.org/abs/2403.08664)

    这项研究探讨了利用合成数据生成医疗记录的方法，特别是通过零样本和少样本提示策略，避免了在训练过程中使用真实患者数据的隐私问题。

    

    通过使用合成医疗记录的创新方法，本研究评估了Llama 2 LLM的能力，利用零样本和少样本提示策略生成可准确反映真实患者信息的合成医疗记录，与需要在训练过程中使用敏感患者数据的微调方法进行对比。

    arXiv:2403.08664v1 Announce Type: new  Abstract: The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, 
    
[^122]: 深度贝叶斯神经网络后验的局部自适应和可扩展扩散采样方法的收敛问题

    On the Convergence of Locally Adaptive and Scalable Diffusion-Based Sampling Methods for Deep Bayesian Neural Network Posteriors

    [https://arxiv.org/abs/2403.08609](https://arxiv.org/abs/2403.08609)

    本文研究如何将自适应步长引入蒙特卡罗采样算法，以实现从神经网络后验分布中生成样本，并证明了这些方法可以收敛到正确的分布。

    

    实现深度神经网络的鲁棒不确定性量化在许多深度学习的实际应用中具有重要意义，如医学成像中需要评估神经网络预测的可靠性。贝叶斯神经网络是对深度神经网络中不确定性建模的一种有前途的方法。然而，从神经网络后验分布中生成样本是一个主要挑战。朝着这个方向的一个重要进展将是将类似于现代神经网络优化器的自适应步长纳入马尔可夫链蒙特卡罗采样算法，而不会显著增加计算需求。过去几年中，一些论文提出了具有实现这一属性的采样算法。然而，它们是否确实收敛到正确的分布呢？在本文中，我们证明了这些方法可以达到这一目标。

    arXiv:2403.08609v1 Announce Type: new  Abstract: Achieving robust uncertainty quantification for deep neural networks represents an important requirement in many real-world applications of deep learning such as medical imaging where it is necessary to assess the reliability of a neural network's prediction. Bayesian neural networks are a promising approach for modeling uncertainties in deep neural networks. Unfortunately, generating samples from the posterior distribution of neural networks is a major challenge. One significant advance in that direction would be the incorporation of adaptive step sizes, similar to modern neural network optimizers, into Monte Carlo Markov chain sampling algorithms without significantly increasing computational demand. Over the past years, several papers have introduced sampling algorithms with claims that they achieve this property. However, do they indeed converge to the correct distribution? In this paper, we demonstrate that these methods can have a 
    
[^123]: 无需复习的一致提示用于持续学习

    Consistent Prompting for Rehearsal-Free Continual Learning

    [https://arxiv.org/abs/2403.08568](https://arxiv.org/abs/2403.08568)

    提出了一种新颖的一致提示（CPrompt）方法，通过训练期间所有现有分类器接受提示训练，实现更加对齐的训练和测试。

    

    持续学习使模型能够自主适应不断变化的环境或数据流，而不会忘记旧知识。基于提示的方法建立在冻结的预训练模型上，以高效地学习任务特定的提示和分类器。现有的基于提示的方法在训练和测试之间存在不一致，从而限制了它们的有效性。这种不一致性揭示了两种类型。测试预测是从所有分类器中进行的，而训练只关注当前任务分类器而没有进行整体对齐，导致分类器的不一致性。提示的不一致性表示测试期间选择的提示可能与训练期间与该任务关联的提示不对应。在本文中，我们提出了一种新颖的基于提示的方法，一致提示（CPrompt），用于更加对齐的训练和测试。具体来说，所有现有的分类器都接受提示训练，从而形成分类

    arXiv:2403.08568v1 Announce Type: cross  Abstract: Continual learning empowers models to adapt autonomously to the ever-changing environment or data streams without forgetting old knowledge. Prompt-based approaches are built on frozen pre-trained models to learn the task-specific prompts and classifiers efficiently. Existing prompt-based methods are inconsistent between training and testing, limiting their effectiveness. Two types of inconsistency are revealed. Test predictions are made from all classifiers while training only focuses on the current task classifier without holistic alignment, leading to Classifier inconsistency. Prompt inconsistency indicates that the prompt selected during testing may not correspond to the one associated with this task during training. In this paper, we propose a novel prompt-based method, Consistent Prompting (CPrompt), for more aligned training and testing. Specifically, all existing classifiers are exposed to prompt training, resulting in classifie
    
[^124]: HRLAIF: 通过AI反馈改进开放域强化学习中的帮助性和无害性

    HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback

    [https://arxiv.org/abs/2403.08309](https://arxiv.org/abs/2403.08309)

    提出了HRLAIF方法来改善开放域强化学习中的模型响应帮助性，通过增强AI注释响应的准确性来提高模型在训练过程中的鲁棒性

    

    强化学习从AI反馈（RLAIF）相比从人类反馈中学习（RLHF）具有更短的注释周期和更低的成本优势，使其在大型语言模型（LLM）训练的快速策略迭代阶段非常高效。使用ChatGPT作为标注员，在RLAIF训练中为开放域提示提供反馈，我们观察到人类评估者对模型响应的偏好胜率增加，但评估者的满意度下降。分析表明，满意度下降主要是因为一些响应变得不够有帮助，特别是在正确性和真实性方面，突显了基本RLAIF的实际局限性。在本文中，我们提出了混合强化学习从AI反馈（HRLAIF）。该方法增强了AI注释响应的准确性，在训练过程中使模型的帮助性更加稳健。

    arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, 
    
[^125]: 随机搜索作为稀疏神经网络架构搜索的基准线

    Random Search as a Baseline for Sparse Neural Network Architecture Search

    [https://arxiv.org/abs/2403.08265](https://arxiv.org/abs/2403.08265)

    论文提出了一种评估方法和基于随机搜索的基线方法，用于发现高质量的稀疏神经网络配置，以解决当前缺乏可靠比较和可重现性的问题。

    

    稀疏神经网络在参数效率更高的情况下展现出与密集网络类似甚至更好的泛化性能，这促使许多工作学习、诱导或搜索性能高的稀疏网络。然而，尽管质量或效率的提升值得注意，但标准基线缺乏，因此妨碍了方法之间的可靠比较和可重现性。在这项工作中，我们提供了一种评估方法和一个简单的随机搜索基线方法，用于发现良好的稀疏配置。我们在过度参数化网络的节点空间上应用随机搜索，目标是找到在损失景观中位置更有优势的更好初始化的稀疏子网络。我们记录了不同稀疏程度下稀疏网络的训练后性能，并与它们的完全连接父网络以及随机稀疏配置进行比较。

    arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa
    
[^126]: KnowCoder：将结构化知识编码到LLMs中用于普适信息提取

    KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction

    [https://arxiv.org/abs/2403.07969](https://arxiv.org/abs/2403.07969)

    本文提出了KnowCoder，一个通过代码生成执行普适信息提取的大型语言模型，引入了代码风格的模式表示方法和两阶段学习框架，以提高LLMs对结构化知识的准确提取能力

    

    在本文中，我们提出了KnowCoder，这是一个大型语言模型（LLM），用于通过代码生成进行普适信息提取（UIE）。KnowCoder旨在开发一种统一的模式表示，使LLMs能够轻松理解，并且提出了一种有效的学习框架，鼓励LLMs遵循模式并准确提取结构化知识。为了实现这一目标，KnowCoder引入了一种代码风格的模式表示方法，将不同的模式统一转换为Python类，从而可以以LLM友好的方式捕捉UIE中任务之间的约束等复杂模式信息。我们进一步构建了一个包含超过30,000种知识类型的代码风格模式库，据我们所知，这是UIE中最大的库。为了简化LLMs的学习过程，KnowCoder包含一个通过代码预训练增强其模式理解能力的两阶段学习框架。

    arXiv:2403.07969v1 Announce Type: cross  Abstract: In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its 
    
[^127]: 用于自主公交网络设计的神经进化算法

    A Neural-Evolutionary Algorithm for Autonomous Transit Network Design

    [https://arxiv.org/abs/2403.07917](https://arxiv.org/abs/2403.07917)

    提出了一种神经进化算法用于自动公交网络设计，该算法通过训练图神经网络模型作为策略，并将其用作进化算法中的变异操作符，在公交网络设计基准集上优于单独学习策略和简单进化算法方法。

    

    规划公共交通网络是一个具有挑战性的优化问题，但是为了实现自动驾驶公交车的好处是至关重要的。我们提出了一种新颖的算法，用于规划自动驾驶公交车的路线网络。我们首先训练一个图神经网络模型作为构建路线网络的策略，然后将该策略用作进化算法中的多个变异操作符之一。我们在标准的公交网络设计基准集上评估这种算法，并发现它在现实基准实例上的表现比单独学习的策略高出高达20\%，比简单的进化算法方法高出高达53%。

    arXiv:2403.07917v1 Announce Type: cross  Abstract: Planning a public transit network is a challenging optimization problem, but essential in order to realize the benefits of autonomous buses. We propose a novel algorithm for planning networks of routes for autonomous buses. We first train a graph neural net model as a policy for constructing route networks, and then use the policy as one of several mutation operators in a evolutionary algorithm. We evaluate this algorithm on a standard set of benchmarks for transit network design, and find that it outperforms the learned policy alone by up to 20\% and a plain evolutionary algorithm approach by up to 53\% on realistic benchmark instances.
    
[^128]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^129]: 知识图谱大型语言模型（KG-LLM）用于链接预测

    Knowledge Graph Large Language Model (KG-LLM) for Link Prediction

    [https://arxiv.org/abs/2403.07311](https://arxiv.org/abs/2403.07311)

    该论文提出了知识图谱大型语言模型框架（KG-LLM），利用思维链提示和上下文学习等NLP范例，以增强知识图谱中的多跳链接预测，并展示了框架在微调大型语言模型和零次尝试能力方面的有效性。

    

    在知识图谱分析领域，预测知识图谱（KGs）内多个链接的任务是一个挑战，由于自然语言处理（NLP）和知识图嵌入技术的进步，这一挑战变得越来越可解决。本文介绍了一种新的方法，即知识图谱大型语言模型框架（KG-LLM），该框架利用关键的NLP范例，包括思维链提示（CoT）和上下文学习（ICL），以增强知识图谱中的多跳链接预测。通过将KG转换为CoT提示，我们的框架旨在识别并学习实体及其相互关系的潜在表示。为了展示KG-LLM框架的有效性，我们在该框架内微调了三种主要的大型语言模型（LLMs），同时采用了非ICL和ICL任务进行全面评估。此外，我们探讨了该框架为LLMs提供零次尝试能力的潜力。

    arXiv:2403.07311v1 Announce Type: new  Abstract: The task of predicting multiple links within knowledge graphs (KGs) stands as a challenge in the field of knowledge graph analysis, a challenge increasingly resolvable due to advancements in natural language processing (NLP) and KG embedding techniques. This paper introduces a novel methodology, the Knowledge Graph Large Language Model Framework (KG-LLM), which leverages pivotal NLP paradigms, including chain-of-thought (CoT) prompting and in-context learning (ICL), to enhance multi-hop link prediction in KGs. By converting the KG to a CoT prompt, our framework is designed to discern and learn the latent representations of entities and their interrelations. To show the efficacy of the KG-LLM Framework, we fine-tune three leading Large Language Models (LLMs) within this framework, employing both non-ICL and ICL tasks for a comprehensive evaluation. Further, we explore the framework's potential to provide LLMs with zero-shot capabilities f
    
[^130]: 优于经典？量子机器学习模型基准测试的微妙艺术

    Better than classical? The subtle art of benchmarking quantum machine learning models

    [https://arxiv.org/abs/2403.07059](https://arxiv.org/abs/2403.07059)

    通过开发开源软件包进行大规模研究，研究发现总体而言，开箱即用的经典机器学习模型胜过量子分类器，还发现将量子模型中的纠缠去除通常会导致同样或更好的性能。

    

    通过经典模拟来进行基准测试是在没有无噪声硬件可用之前评估量子机器学习思想的主要方式之一。然而，实验设计对结果的巨大影响，当前可达到的小规模，以及受量子技术商业化影响的叙述使得难以获得稳健的见解。为了促进更好的决策，我们开发了一个基于PennyLane软件框架的开源软件包，并使用它进行了大规模研究，系统地测试了12种流行的量子机器学习模型在用于创建160个单独数据集的6个二元分类任务上。我们发现，总体而言，开箱即用的经典机器学习模型胜过量子分类器。此外，从量子模型中移除纠缠往往会导致同样或更好的性能，这表明“量子特性”可能并非关键成分。

    arXiv:2403.07059v1 Announce Type: cross  Abstract: Benchmarking models via classical simulations is one of the main ways to judge ideas in quantum machine learning before noise-free hardware is available. However, the huge impact of the experimental design on the results, the small scales within reach today, as well as narratives influenced by the commercialisation of quantum technologies make it difficult to gain robust insights. To facilitate better decision-making we develop an open-source package based on the PennyLane software framework and use it to conduct a large-scale study that systematically tests 12 popular quantum machine learning models on 6 binary classification tasks used to create 160 individual datasets. We find that overall, out-of-the-box classical machine learning models outperform the quantum classifiers. Moreover, removing entanglement from a quantum model often results in as good or better performance, suggesting that "quantumness" may not be the crucial ingredi
    
[^131]: 语义剩余提示用于持续学习

    Semantic Residual Prompts for Continual Learning

    [https://arxiv.org/abs/2403.06870](https://arxiv.org/abs/2403.06870)

    通过引入语义剩余提示，作者提出了一种稳定的选择策略，利用两级适应机制来在持续学习中解决提示冲突的问题。

    

    持续学习（CL）的提示调整方法冻结了一个大型预训练模型，并侧重于训练一些称为提示的参数向量。这些方法中的大多数将这些向量组织在一个键-值对池中，并使用输入图像作为查询来检索提示（值）。然而，随着任务的进行，由于键是学习的，提示选择策略本身也会面临灾难性遗忘，这是现有方法经常忽视的问题。为了使选择策略更加稳定，我们请求一个基础模型（CLIP）来在两级适应机制中选择我们的提示。具体而言，第一级利用标准文本提示来调整CLIP文本编码器，形成稳定的类原型。而第二级则将这些原型与查询图像一起用作键来索引一个s

    arXiv:2403.06870v1 Announce Type: new  Abstract: Prompt-tuning methods for Continual Learning (CL) freeze a large pre-trained model and focus training on a few parameter vectors termed prompts. Most of these methods organize these vectors in a pool of key-value pairs, and use the input image as query to retrieve the prompts (values). However, as keys are learned while tasks progress, the prompting selection strategy is itself subject to catastrophic forgetting, an issue often overlooked by existing approaches. For instance, prompts introduced to accommodate new tasks might end up interfering with previously learned prompts. To make the selection strategy more stable, we ask a foundational model (CLIP) to select our prompt within a two-level adaptation mechanism. Specifically, the first level leverages standard textual prompts for the CLIP textual encoder, leading to stable class prototypes. The second level, instead, uses these prototypes along with the query image as keys to index a s
    
[^132]: 长尾视觉识别的概率对比学习

    Probabilistic Contrastive Learning for Long-Tailed Visual Recognition

    [https://arxiv.org/abs/2403.06726](https://arxiv.org/abs/2403.06726)

    提出了一种新颖的概率对比（ProCo）学习算法，该算法估计特征空间中每个类别样本的数据分布，并对比对进行采样

    

    长尾分布经常出现在现实世界的数据中，其中大量少数类别包含有限数量的样本。这种不平衡问题严重影响了标准监督学习算法的性能，这些算法主要设计用于平衡的训练集。最近的研究表明，监督对比学习在缓解数据不平衡方面表现出有希望的潜力。然而，监督对比学习的性能受到固有挑战的困扰：它需要足够大批次的训练数据来构建涵盖所有类别的对比对，然而在类别不平衡数据的情况下很难满足这一要求。为克服这一障碍，我们提出了一种新颖的概率对比（ProCo）学习算法，该算法估计特征空间中每个类别样本的数据分布，并对比对进行采样。

    arXiv:2403.06726v1 Announce Type: new  Abstract: Long-tailed distributions frequently emerge in real-world data, where a large number of minority categories contain a limited number of samples. Such imbalance issue considerably impairs the performance of standard supervised learning algorithms, which are mainly designed for balanced training sets. Recent investigations have revealed that supervised contrastive learning exhibits promising potential in alleviating the data imbalance. However, the performance of supervised contrastive learning is plagued by an inherent challenge: it necessitates sufficiently large batches of training data to construct contrastive pairs that cover all categories, yet this requirement is difficult to meet in the context of class-imbalanced data. To overcome this obstacle, we propose a novel probabilistic contrastive (ProCo) learning algorithm that estimates the data distribution of the samples from each class in the feature space, and samples contrastive pa
    
[^133]: 关于持续学习中宽度递减回报的研究

    On the Diminishing Returns of Width for Continual Learning

    [https://arxiv.org/abs/2403.06398](https://arxiv.org/abs/2403.06398)

    增加神经网络宽度以减少遗忘会带来递减的回报，并且在先前研究中尚未探索的宽度范围内进行了实证验证。

    

    深度神经网络在各种设置中展示了突破性的性能，但这些模型在按顺序训练新任务时经常出现“灾难性遗忘”。 一些研究已经经验性地证明增加神经网络宽度会导致灾难性遗忘减少，但尚未准确刻画宽度和持续学习之间的确切关系。我们设计了其中一个最早的框架来分析持续学习理论，并证明宽度与前馈网络（FFN）中的遗忘直接相关。 具体来说，我们证明增加网络宽度以减少遗忘会带来递减的回报。我们在先前研究中尚未探索的宽度上经验性验证了我们的论断，结果显示递减回报如我们的理论所预测的那样清晰可见。

    arXiv:2403.06398v1 Announce Type: cross  Abstract: While deep neural networks have demonstrated groundbreaking performance in various settings, these models often suffer from \emph{catastrophic forgetting} when trained on new tasks in sequence. Several works have empirically demonstrated that increasing the width of a neural network leads to a decrease in catastrophic forgetting but have yet to characterize the exact relationship between width and continual learning. We design one of the first frameworks to analyze Continual Learning Theory and prove that width is directly related to forgetting in Feed-Forward Networks (FFN). Specifically, we demonstrate that increasing network widths to reduce forgetting yields diminishing returns. We empirically verify our claims at widths hitherto unexplored in prior studies where the diminishing returns are clearly observed as predicted by our theory.
    
[^134]: 分布式时间差分的统计效率

    Statistical Efficiency of Distributional Temporal Difference

    [https://arxiv.org/abs/2403.05811](https://arxiv.org/abs/2403.05811)

    该论文分析了分布式时间差分的统计效率和有限样本性能。

    

    分布式强化学习(DRL)关注的是返回的完整分布，而不仅仅是均值，在各个领域取得了经验成功。领域DRL中的核心任务之一是分布式策略评估，涉及估计给定策略pi的返回分布η^pi。相应地提出了分布时间差分(TD)算法，这是经典RL文献中时间差分算法的延伸。在表格案例中，citet{rowland2018analysis}和citet{rowland2023analysis}分别证明了两个分布式TD实例即分类时间差分算法(CTD)和分位数时间差分算法(QTD)的渐近收敛。在这篇论文中，我们进一步分析了分布式TD的有限样本性能。为了促进理论分析，我们提出了一个非参数的 dis

    arXiv:2403.05811v1 Announce Type: cross  Abstract: Distributional reinforcement learning (DRL), which cares about the full distribution of returns instead of just the mean, has achieved empirical success in various domains. One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$. A distributional temporal difference (TD) algorithm has been accordingly proposed, which is an extension of the temporal difference algorithm in the classic RL literature. In the tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis} proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference algorithm (CTD) and quantile temporal difference algorithm (QTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric dis
    
[^135]: 复杂航天器任务的屏蔽深度强化学习

    Shielded Deep Reinforcement Learning for Complex Spacecraft Tasking

    [https://arxiv.org/abs/2403.05693](https://arxiv.org/abs/2403.05693)

    本文基于线性时态逻辑（LTL）形式化了航天器任务和安全要求，并提出了自动构建奖励函数以实现有效训练的方法。同时探讨了从安全LTL规范构建航天器屏蔽的方法，提出了三种可以提供概率保证的设计，并通过实验展示了这些屏蔽与不同策略的互动和奖励结构的灵活性。

    

    自主航天器控制通过屏蔽深度强化学习（SDRL）已成为快速增长的研究领域。然而，目前对屏蔽的构建和任务的定义仍不够正式，导致策略无法保证安全并给RL代理设定了模棱两可的目标。本文首先探讨了使用形式语言，即线性时态逻辑（LTL），来形式化航天器任务和安全要求。然后定义了一种自动从co-safe LTL规范构建奖励函数以有效训练SDRL框架的方式。我们还研究了为航天器应用从安全LTL规范构建屏蔽的方法，并提出了三种可以提供概率保证的设计。通过几个实验展示了这些屏蔽与不同策略的互动以及奖励结构的灵活性。

    arXiv:2403.05693v1 Announce Type: new  Abstract: Autonomous spacecraft control via Shielded Deep Reinforcement Learning (SDRL) has become a rapidly growing research area. However, the construction of shields and the definition of tasking remains informal, resulting in policies with no guarantees on safety and ambiguous goals for the RL agent. In this paper, we first explore the use of formal languages, namely Linear Temporal Logic (LTL), to formalize spacecraft tasks and safety requirements. We then define a manner in which to construct a reward function from a co-safe LTL specification automatically for effective training in SDRL framework. We also investigate methods for constructing a shield from a safe LTL specification for spacecraft applications and propose three designs that provide probabilistic guarantees. We show how these shields interact with different policies and the flexibility of the reward structure through several experiments.
    
[^136]: 在装配再生核希尔伯特空间中具有内在可观测性的Koopman算子

    Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces

    [https://arxiv.org/abs/2403.02524](https://arxiv.org/abs/2403.02524)

    本文提出了一种基于装配再生核希尔伯特空间内在结构和jets几何概念的估计Koopman算子的新方法JetDMD，通过明确的误差界和收敛率证明其优越性，为Koopman算子的数值估计提供了更精确的方法，同时在装配希尔伯特空间框架内提出了扩展Koopman算子的概念，有助于深入理解估计的Koopman特征函数。

    

    本文提出了一种新颖的方法，用于估计装配再生核希尔伯特空间（RKHS）上定义的Koopman算子及其谱。我们提出了一种估计方法，称为Jet Dynamic Mode Decomposition（JetDMD），利用RKHS的内在结构和称为jets的几何概念来增强Koopman算子的估计。该方法在精确度上优化了传统的扩展动态模态分解（EDMD），特别是在特征值的数值估计方面。本文通过明确的误差界和特殊正定内核的收敛率证明了JetDMD的优越性，为其性能提供了坚实的理论基础。我们还深入探讨了Koopman算子的谱分析，在装配希尔伯特空间框架内提出了扩展Koopman算子的概念。这个概念有助于更深入地理解估计的Koopman特征函数并捕捉

    arXiv:2403.02524v1 Announce Type: cross  Abstract: This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and captu
    
[^137]: 可解释的扩散用于通用时间序列生成

    Diffusion-TS: Interpretable Diffusion for General Time Series Generation

    [https://arxiv.org/abs/2403.01742](https://arxiv.org/abs/2403.01742)

    提出了一种新颖的基于扩散的框架 Diffusion-TS，结合了编码器-解码器变压器和解耦时间表示，通过直接重建样本而非噪声生成高质量的多变量时间序列样本，旨在实现时间序列的解释性和真实性。

    

    Denoising diffusion probabilistic models (DDPMs)正逐渐成为生成模型的主流范式，最近已在音频合成、时间序列填补和预测等领域取得突破。本文提出了Diffusion-TS，一种新颖的基于扩散的框架，通过使用具有解耦时间表示的编码器-解码器变压器生成高质量的多变量时间序列样本，其中分解技术指导Diffusion-TS捕获时间序列的语义含义，而变压器从嘈杂的模型输入中挖掘详细的序列信息。与现有的基于扩散的方法不同，我们训练模型直接重建样本而不是在每个扩散步骤中重建噪声，并结合了基于Fourier的损失项。预期Diffusion-TS可以生成既具有解释性又真实性的时间序列。此外，还表明了所提出的Diffusion

    arXiv:2403.01742v1 Announce Type: cross  Abstract: Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-T
    
[^138]: VBART: 土耳其LLM

    VBART: The Turkish LLM

    [https://arxiv.org/abs/2403.01308](https://arxiv.org/abs/2403.01308)

    VBART是第一个土耳其序列到序列大语言模型，通过与BART和mBART模型结合形成了紧凑型LLM，并在多个任务中表现出色，为土耳其自然语言处理研究开辟了新的可能性。

    

    我们提出了VBART，这是第一个土耳其序列到序列大语言模型（LLMs），在一个大语料库上从头开始进行预训练。VBART是基于BART和mBART模型的好思路构建的紧凑型LLMs，分为Large和XLarge两个尺寸。微调后的VBART模型在提取性文本摘要、标题生成、文本改写、问答和问题生成等任务中超越了先前的最先进结果。它们允许为未来的文本生成任务和数据集进行微调，为土耳其自然语言处理（NLP）研究开辟了新路径。我们的工作表明，拥有为土耳其进行预训练的LLM比多语言模型提高了最多3倍，改进了现有结果，并为训练和推理提供了高效的模型。此外，我们展示了我们的单语分词器比OpenAI的多语言分词器更高效7倍。最后但同样重要的是，我们介绍了一种扩展现有预训

    arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
    
[^139]: 精确推荐的端到端图-序列表示学习

    End-to-end Graph-Sequential Representation Learning for Accurate Recommendations

    [https://arxiv.org/abs/2403.00895](https://arxiv.org/abs/2403.00895)

    本文提出了一个新颖的多重表示学习框架，有效地结合了基于序列和基于图的推荐方法，显著改善了推荐性能。

    

    近年来推荐系统的许多新进展集中在开发基于序列和基于图的方法上。这两种方法在建模行为数据中的复杂关系方面都证明了其有效性，从而在个性化排名和下一个推荐任务中取得了有益的成果，同时保持了良好的可扩展性。然而，它们从数据中捕捉到的信号截然不同。前者直接通过与最近物品的有序交互来表示用户，而后者旨在捕捉交互图中的间接依赖关系。本文提出了一个新颖的多重表示学习框架，利用这两种范式之间的协同作用。我们在几个数据集上的实证评估表明，利用所提出的框架相互训练序列和图组件显著改善了推荐性能。

    arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
    
[^140]: 光滑 Tchebycheff 标量化用于多目标优化

    Smooth Tchebycheff Scalarization for Multi-Objective Optimization

    [https://arxiv.org/abs/2402.19078](https://arxiv.org/abs/2402.19078)

    通过光滑 Tchebycheff 标量化方法，本文提出了一种轻量级的方法，用于梯度型多目标优化，具有更低的计算复杂性但仍能找到所有帕累托解。

    

    多目标优化问题在许多现实世界应用中都能找到，在这些问题中，目标经常相互冲突，不能通过单个解进行优化。在过去的几十年中，已经提出了许多方法来找到帕累托解，这些解代表了对于给定问题的不同最佳权衡。然而，这些现有方法可能具有较高的计算复杂性，或者可能不能具备解决一般可微分多目标优化问题的良好理论属性。在本项工作中，通过利用光滑优化技术，我们提出了一种新颖且轻量的光滑 Tchebycheff 标量化方法，用于基于梯度的多目标优化。它对于找到所有帕累托解具有良好的理论属性，同时相对于其他方法具有显着较低的计算复杂性。在各种实验结果上

    arXiv:2402.19078v1 Announce Type: cross  Abstract: Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on variou
    
[^141]: MMSR：符号回归是一个多模态任务

    MMSR: Symbolic Regression is a Multimodal Task

    [https://arxiv.org/abs/2402.18603](https://arxiv.org/abs/2402.18603)

    符号回归被视为一个多模态任务，研究人员将数据到表达式的映射视为翻译问题，引入大规模预训练模型。

    

    数学公式是探索自然规律几千年来人类智慧的结晶。用简洁的数学公式描述复杂的自然规律是科学家不断追求的目标，也是人工智能面临的重大挑战。这一领域被称为符号回归。在本文中，研究人员将从数据到表达式的映射视为翻译问题，并引入了相应的大规模预训练模型。

    arXiv:2402.18603v1 Announce Type: cross  Abstract: Mathematical formulas are the crystallization of human wisdom in exploring the laws of nature for thousands of years. Describing the complex laws of nature with a concise mathematical formula is a constant pursuit of scientists and a great challenge for artificial intelligence. This field is called symbolic regression. Symbolic regression was originally formulated as a combinatorial optimization problem, and GP and reinforcement learning algorithms were used to solve it. However, GP is sensitive to hyperparameters, and these two types of algorithms are inefficient. To solve this problem, researchers treat the mapping from data to expressions as a translation problem. And the corresponding large-scale pre-trained model is introduced. However, the data and expression skeletons do not have very clear word correspondences as the two languages do. Instead, they are more like two modalities (e.g., image and text). Therefore, in this paper, w
    
[^142]: QUCE: 减少和量化基于路径的不确定性以生成对抗性反事实解释

    QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations

    [https://arxiv.org/abs/2402.17516](https://arxiv.org/abs/2402.17516)

    QUCE方法旨在通过减少路径不确定性来量化和缓解基于路径的不确定性，从而改善对抗性反事实解释的表现。

    

    arXiv:2402.17516v1 公告类型：跨学科 深度神经网络（DNNs）作为机器学习领域最突出的方法之一。DNNs的有效性随着最近计算能力的增加而激增，使得这些方法能够扩展到处理大数据中的重要复杂性以应对预测挑战。然而，随着DNN模型复杂性的提高，可解释性降低。针对这一挑战，诸如对抗梯度整合（AGI）这样的可解释模型利用DNN提供的基于路径的梯度来阐明它们的决策。然而，当梯度在越界路径遍历期间表现出不规则性时，基于路径的解释器的性能可能会受到损害。在这种情况下，我们介绍了Quantified Uncertainty Counterfactual Explanations（QUCE），这是一种旨在减少路径不确定性的方法，以缓解越界遍历。 QUCE不仅在提出解释时量化不确定性

    arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
    
[^143]: 科学检查者再度升级：透明度和逻辑推理的双向范式

    Science Checker Reloaded: A Bidirectional Paradigm for Transparency and Logical Reasoning

    [https://arxiv.org/abs/2402.13897](https://arxiv.org/abs/2402.13897)

    提出了一个两块式的方法来解决长文档中信息检索领域的挑战，并实现了双向交互

    

    信息检索是一个快速发展的领域。然而，它仍然面临着在科学和工业的海量信息中的诸多限制，比如语义分歧和检索中的词汇差距、语义搜索中的低精度和缺乏可解释性，或者生成模型中的幻觉和过时信息。在本文中，我们提出了一个两块式的方法来解决长文档的这些障碍。第一个模块通过查询扩展增强了在稀疏检索中的语言理解，以检索相关文档。第二个模块通过只使用长文档中传播的信息，为复杂问题提供全面和信息丰富的答案来加深结果，实现双向交互。在管道的各个阶段，向用户呈现中间结果以促进对系统推理的理解。我们相信这种双向方法带来了

    arXiv:2402.13897v1 Announce Type: cross  Abstract: Information retrieval is a rapidly evolving field. However it still faces significant limitations in the scientific and industrial vast amounts of information, such as semantic divergence and vocabulary gaps in sparse retrieval, low precision and lack of interpretability in semantic search, or hallucination and outdated information in generative models. In this paper, we introduce a two-block approach to tackle these hurdles for long documents. The first block enhances language understanding in sparse retrieval by query expansion to retrieve relevant documents. The second block deepens the result by providing comprehensive and informative answers to the complex question using only the information spread in the long document, enabling bidirectional engagement. At various stages of the pipeline, intermediate results are presented to users to facilitate understanding of the system's reasoning. We believe this bidirectional approach brings
    
[^144]: 随机凸优化中适应性的代价

    The Price of Adaptivity in Stochastic Convex Optimization

    [https://arxiv.org/abs/2402.10898](https://arxiv.org/abs/2402.10898)

    该论文证明了在非光滑随机凸优化中，适应性的代价是无法避免的，并且给出了关于不确定性参数的次优性乘法增加的下界。

    

    我们证明了在非光滑随机凸优化中适应性的不可能性结果。给定一组我们希望适应的问题参数，我们定义了“适应性的代价”（PoA），粗略地说，它衡量了由于这些参数的不确定性而导致的次优性的乘法增加。当初始距离最优解未知但梯度范数有界时，我们证明PoA至少对于期望次优性是对数级别，对于中位数次优性是双对数级别。当距离和梯度范数都存在不确定性时，我们表明PoA必须是与不确定性水平多项式相关的。我们的下界几乎与现有的上界相匹配，并且确定了没有无参数午餐的结论。

    arXiv:2402.10898v1 Announce Type: cross  Abstract: We prove impossibility results for adaptivity in non-smooth stochastic convex optimization. Given a set of problem parameters we wish to adapt to, we define a "price of adaptivity" (PoA) that, roughly speaking, measures the multiplicative increase in suboptimality due to uncertainty in these parameters. When the initial distance to the optimum is unknown but a gradient norm bound is known, we show that the PoA is at least logarithmic for expected suboptimality, and double-logarithmic for median suboptimality. When there is uncertainty in both distance and gradient norm, we show that the PoA must be polynomial in the level of uncertainty. Our lower bounds nearly match existing upper bounds, and establish that there is no parameter-free lunch.
    
[^145]: 百万长度视频和语言的环形注意力世界模型

    World Model on Million-Length Video And Language With RingAttention

    [https://arxiv.org/abs/2402.08268](https://arxiv.org/abs/2402.08268)

    该论文介绍了一个使用百万长度的视频和语言序列进行联合建模的环形注意力世界模型。该模型通过利用视频序列中的时间信息和语言的文本知识以及逐渐增加上下文大小的方法提高了AI辅助人类的能力。

    

    当前的语言模型在理解难以用文字描述的世界方面表现不佳，并且在处理复杂的长篇任务时遇到困难。视频序列提供了只有语言和静态图像所不具备的宝贵时间信息，因此它们在与语言进行联合建模时具有吸引力。这种模型可以对人类的文本知识和物理世界进行理解，为辅助人类提供更广泛的人工智能能力。然而，从百万个标记的视频和语言序列中学习面临着记忆约束、计算复杂性和数据有限性的挑战。为了应对这些挑战，我们策划了一个包含多样化视频和书籍的大型数据集，利用环形注意力技术对长序列进行可扩展的训练，逐渐增加上下文大小从4K到1M个标记。本文的贡献如下：

    Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language seq
    
[^146]: G-Retriever: 用于文本图理解和问题解答的检索增强生成模型

    G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering

    [https://arxiv.org/abs/2402.07630](https://arxiv.org/abs/2402.07630)

    本论文提出了G-Retriever模型，该模型用于文本图理解和问题解答。该模型能够将用户的问题转化为文本回复，并突出显示图形的相关部分。与现有的方法不同，该模型适用于真实世界的文本图形，并可应用于不同的任务，包括场景图理解、常识推理和知识图推理。

    

    在给定具有文本属性的图形的情况下，我们使用户能够使用对话界面向图形提出问题。针对用户的问题，我们的方法提供文本回复并突出显示图形的相关部分。与现有的方法将大型语言模型(LLM)和图神经网络(GNN)以各种方式整合起来不同，它们大多集中在传统图任务(如节点、边和图分类)或者在小型或合成图上回答简单的图查询。相比之下，我们开发了一个灵活的问题回答框架，针对真实世界的文本图形，适用于多个应用，包括场景图理解、常识推理和知识图推理。为实现这个目标，我们首先用来自不同任务的数据开发了我们的图问题回答(GraphQA)基准测试。然后，我们提出了我们的G-Retriever方法，它集成了GNN和LLM的优势。

    Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop our Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our G-Retriever approach, which integrates the strengths of GNNs, LLMs, 
    
[^147]: FedImpro: 测量和改善联邦学习中的客户更新

    FedImpro: Measuring and Improving Client Update in Federated Learning

    [https://arxiv.org/abs/2402.07011](https://arxiv.org/abs/2402.07011)

    本文提出了FedImpro方法，通过生成改进的本地模型来减轻联邦学习中的客户漂移问题。该方法通过分析本地训练的泛化贡献，并利用类似的条件分布进行训练，增强了泛化贡献并减小了梯度的差异性。

    

    联邦学习模型通常会受到异构数据引起的客户漂移的影响，其中数据的分布在不同的客户之间存在差异。为了解决这个问题，先进的研究主要关注于操作现有的梯度，以实现更一致的客户模型。在本文中，我们从另一个角度分析了客户漂移，并旨在通过生成改进的本地模型来减轻这种漂移。首先，我们分析了本地训练的泛化贡献，并得出结论，这种泛化贡献受到不同客户的数据分布之间的条件Wasserstein距离的限制。然后，我们提出了FedImpro，用于构建类似的条件分布进行本地训练。具体而言，FedImpro将模型分解为高层和低层组件，并对重构特征分布上的高层部分进行训练。这种方法增强了泛化贡献，并减小了联邦学习中梯度的差异性。

    Federated Learning (FL) models often experience client drift caused by heterogeneous data, where the distribution of data differs across clients. To address this issue, advanced research primarily focuses on manipulating the existing gradients to achieve more consistent client models. In this paper, we present an alternative perspective on client drift and aim to mitigate it by generating improved local models. First, we analyze the generalization contribution of local training and conclude that this generalization contribution is bounded by the conditional Wasserstein distance between the data distribution of different clients. Then, we propose FedImpro, to construct similar conditional distributions for local training. Specifically, FedImpro decouples the model into high-level and low-level components, and trains the high-level portion on reconstructed feature distributions. This approach enhances the generalization contribution and reduces the dissimilarity of gradients in FL. Exper
    
[^148]: 基于Alpha曲线连续性的差动迁移光谱法对挥发性有机化合物进行分类

    Classification of Volatile Organic Compounds by Differential Mobility Spectrometry Based on Continuity of Alpha Curves

    [https://arxiv.org/abs/2401.07066](https://arxiv.org/abs/2401.07066)

    差动迁移光谱法结合Alpha曲线连续性，首次将分散图解释为序列测量，可能为挥发性有机化合物的分类提供新的方法

    

    arXiv:2401.07066v2 公告类型:更新 摘要: 背景：挥发性有机化合物（VOCs）的分类在许多领域中受到关注。示例包括但不限于医学、爆炸物检测和食品质量控制。使用电子鼻采集的测量数据可用于VOCs的分类和分析。近年来，一种电子鼻的类型，即差动迁移光谱法（DMS），取得了相当大的发展。DMS产生的测量结果被可视化为包含曲线轨迹的分散图，也被称为Alpha曲线。目前用于分析DMS分散图的方法通常不利用这些曲线连续性中存储的信息，这表明应该研究替代方法。 结果：在这项工作中，首次将分散图解释为一系列按顺序演变的测量。因此，假设可以使用时间序列分类算法对其进行分类

    arXiv:2401.07066v2 Announce Type: replace  Abstract: Background: Classification of volatile organic compounds (VOCs) is of interest in many fields. Examples include but are not limited to medicine, detection of explosives, and food quality control. Measurements collected with electronic noses can be used for classification and analysis of VOCs. One type of electronic noses that has seen considerable development in recent years is Differential Mobility Spectrometry (DMS). DMS yields measurements that are visualized as dispersion plots that contain traces, also known as alpha curves. Current methods used for analyzing DMS dispersion plots do not usually utilize the information stored in the continuity of these traces, which suggests that alternative approaches should be investigated.   Results: In this work, for the first time, dispersion plots were interpreted as a series of measurements evolving sequentially. Thus, it was hypothesized that time-series classification algorithms can be e
    
[^149]: 基于深度先验的近场3D MIMO成像中的振幅插入和播放正则化

    Plug-and-Play Regularization on Magnitude with Deep Priors for 3D Near-Field MIMO Imaging

    [https://arxiv.org/abs/2312.16024](https://arxiv.org/abs/2312.16024)

    本文提出了一种基于深度先验的近场3D MIMO成像中振幅正则化问题的插入和播放正则化方法，通过解决复值去噪问题实现了高效的重建方法。

    

    近场雷达成像系统被广泛应用于隐蔽武器检测和医学诊断等各种应用。本文考虑通过对振幅施加正则化来重建近场场景的三维复值反射率分布的问题。我们利用交替方向乘子方法（ADMM）框架来解决这个逆问题。为此，我们提供了一个与这种正则化功能相关联的近似映射的一般表达式。这等效于解决一个涉及振幅正则化的复值去噪问题。通过利用这个表达式，我们开发了一种新颖高效的插入和播放（PnP）重建方法，包含简单的更新步骤。由于数据自适应深度先验在成像中的成功，我们还训练了一个3D深度降噪器以在其中加以利用。

    arXiv:2312.16024v2 Announce Type: replace-cross  Abstract: Near-field radar imaging systems are used in a wide range of applications such as concealed weapon detection and medical diagnosis. In this paper, we consider the problem of reconstructing the three-dimensional (3D) complex-valued reflectivity distribution of the near-field scene by enforcing regularization on its magnitude. We solve this inverse problem by using the alternating direction method of multipliers (ADMM) framework. For this, we provide a general expression for the proximal mapping associated with such regularization functionals. This equivalently corresponds to the solution of a complex-valued denoising problem which involves regularization on the magnitude. By utilizing this expression, we develop a novel and efficient plug-and-play (PnP) reconstruction method that consists of simple update steps. Due to the success of data-adaptive deep priors in imaging, we also train a 3D deep denoiser to exploit within the dev
    
[^150]: NM-FlowGAN: 基于正规化流和生成对抗网络的混合方法对sRGB噪声进行建模

    NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks

    [https://arxiv.org/abs/2312.10112](https://arxiv.org/abs/2312.10112)

    NM-FlowGAN是一种利用生成对抗网络和正规化流的混合方法，旨在更准确地建模sRGB噪声，弥补了单一生成模型固有特性所带来的性能限制。

    

    建模和合成真实的sRGB噪声对于各种低级别视觉任务至关重要，例如构建用于训练图像去噪系统的数据集。真实sRGB噪声的分布极为复杂，并受多种因素影响，使得其准确建模极具挑战性。因此，最近的研究提出了采用数据驱动生成模型，如生成对抗网络（GAN）和正规化流的方法。这些研究相比传统的噪声建模方法实现了对sRGB噪声的更准确建模。然而，由于每种生成模型的固有特性，存在性能限制。为了解决这个问题，我们提出了NM-FlowGAN，这是一种利用GAN和正规化流的优势的混合方法。我们同时采用基于正规化流的像素级噪声建模网络，以及基于GAN的空间相关性建模网络。

    arXiv:2312.10112v2 Announce Type: replace-cross  Abstract: Modeling and synthesizing real sRGB noise is crucial for various low-level vision tasks, such as building datasets for training image denoising systems. The distribution of real sRGB noise is highly complex and affected by a multitude of factors, making its accurate modeling extremely challenging. Therefore, recent studies have proposed methods that employ data-driven generative models, such as generative adversarial networks (GAN) and Normalizing Flows. These studies achieve more accurate modeling of sRGB noise compared to traditional noise modeling methods. However, there are performance limitations due to the inherent characteristics of each generative model. To address this issue, we propose NM-FlowGAN, a hybrid approach that exploits the strengths of both GAN and Normalizing Flows. We simultaneously employ a pixel-wise noise modeling network based on Normalizing Flows, and spatial correlation modeling networks based on GAN
    
[^151]: 一种具有条件扩散建模框架的应用于蛋白设计中的基序支架

    A framework for conditional diffusion modelling with applications in motif scaffolding for protein design

    [https://arxiv.org/abs/2312.09236](https://arxiv.org/abs/2312.09236)

    该论文提出一种统一的条件扩散建模框架，基于Doob's h-transform，用于解决蛋白设计中的基序支架问题

    

    许多蛋白设计应用，如结合物或酶的设计，需要以高精度搭建具有结构基序的蛋白质。基于去噪扩散过程的生成建模范式已成为解决这一基序支架问题的主要候选方案，并在某些情况下显示出早期实验成功。在扩散范式中，基序支架被视为一种条件生成任务，并提出了几种条件生成协议或从计算机视觉文献中导入。然而，这些协议大多基于启发性动机，例如通过对朗之万动力学的类比，并缺乏统一的框架，使得不同方法之间的联系变得模糊。在这项工作中，我们在一个基于数学上理解良好的Doob's h-transform的共同框架下统一了条件训练和条件抽样程序。这种新的视角使我们能够在不同方法之间建立联系

    arXiv:2312.09236v2 Announce Type: replace  Abstract: Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between ex
    
[^152]: 一份全面的指甲皱纹毛细管分析数据集和自动化流水线

    A Comprehensive Dataset and Automated Pipeline for Nailfold Capillary Analysis

    [https://arxiv.org/abs/2312.05930](https://arxiv.org/abs/2312.05930)

    这项研究提出了一份全面的指甲皱纹毛细管数据集，并开发了一个自动化分析流水线，能够准确检测和测量各种指甲毛细管的特征，表现出色的精度。

    

    指甲皱纹毛细管镜检验广泛用于评估健康状况，突出了对自动化指甲皱纹毛细管分析系统的迫切需求。在这项研究中，我们提出了一个开创性的工作，构建了一个全面的指甲皱纹毛细管数据集-来自68名受试者的321张图像，219段视频，附有临床报告和专家注释-这作为训练深度学习模型的关键资源。利用这个数据集，我们通过专家注释作为监督标签对三个深度学习模型进行微调，并将它们整合到一个新颖的端到端指甲皱纹毛细管分析流水线中。该流水线在自动检测和测量指甲皱纹毛细管的各种大小因素、形态特征和动态方面方面表现出色。我们将结果与临床报告进行了比较。实验结果表明，我们的自动化流水线在测量精度方面达到了亚像素级别，并且有89.9%的

    arXiv:2312.05930v2 Announce Type: replace-cross  Abstract: Nailfold capillaroscopy is widely used in assessing health conditions, highlighting the pressing need for an automated nailfold capillary analysis system. In this study, we present a pioneering effort in constructing a comprehensive nailfold capillary dataset-321 images, 219 videos from 68 subjects, with clinic reports and expert annotations-that serves as a crucial resource for training deep-learning models. Leveraging this dataset, we finetuned three deep learning models with expert annotations as supervised labels and integrated them into a novel end-to-end nailfold capillary analysis pipeline. This pipeline excels in automatically detecting and measuring a wide range of size factors, morphological features, and dynamic aspects of nailfold capillaries. We compared our outcomes with clinical reports. Experiment results showed that our automated pipeline achieves an average of sub-pixel level precision in measurements and 89.9
    
[^153]: 农业预测的创新：全球作物产量预测的多元回归研究

    Innovations in Agricultural Forecasting: A Multivariate Regression Study on Global Crop Yield Prediction

    [https://arxiv.org/abs/2312.02254](https://arxiv.org/abs/2312.02254)

    通过实施六个回归模型并使用关键训练参数，该研究提出了一个随机森林回归模型，用于预测37个发展中国家27年的农作物产量，取得了0.94的确定系数，并探讨了不同因素对产量的影响。

    

    农作物产量的全球预测是农业研究中一个至关重要的目标。因此，该研究实施了6个回归模型（线性、决策树、梯度下降、梯度提升、K最近邻和随机森林），以预测37个发展中国家27年的农作物产量。在给定4个关键的训练参数（杀虫剂量（吨）、降雨量（mm）、温度（摄氏度）和产量（hg/ha））的情况下，发现我们的随机森林回归模型达到了0.94的确定系数（r2），误差边际（ME）为0.03。这些模型使用联合国粮食和农业组织的数据以及世界银行气候变化数据目录进行训练和测试。此外，分析了每个参数，以了解不同因素如何影响总体产量。我们使用了与通常使用的深度学习（DL）和机器学习（ML）模型相反的非传统模型。

    arXiv:2312.02254v2 Announce Type: replace-cross  Abstract: The prediction of crop yields internationally is a crucial objective in agricultural research. Thus, this study implements 6 regression models (Linear, Tree, Gradient Descent, Gradient Boosting, K Nearest Neighbors, and Random Forest) to predict crop yields in 37 developing countries over 27 years. Given 4 key training parameters, insecticides (tonnes), rainfall (mm), temperature (Celsius), and yield (hg/ha), it was found that our Random Forest Regression model achieved a determination coefficient (r2) of 0.94, with a margin of error (ME) of .03. The models were trained and tested using the Food and Agricultural Organization of the United Nations data, along with the World Bank Climate Change Data Catalog. Furthermore, each parameter was analyzed to understand how varying factors could impact overall yield. We used unconventional models, contrary to generally used Deep Learning (DL) and Machine Learning (ML) models, combined wi
    
[^154]: Receler: 通过轻量级橡皮擦可靠地擦除文本到图像扩散模型中的概念

    Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers

    [https://arxiv.org/abs/2311.17717](https://arxiv.org/abs/2311.17717)

    Receler提出了一种可靠概念擦除方法，通过轻量级橡皮擦实现对文本到图像扩散模型的概念擦除，具备鲁棒性和局部性，实验证明其优越性。

    

    在文本到图像扩散模型中，概念擦除旨在禁用预训练的扩散模型生成与目标概念相关的图像。为了实现可靠的概念擦除，希望具备鲁棒性和局部性的属性。前者阻止模型为任何释义或学习提示生成与目标概念相关的图像，而后者保持其生成具有非目标概念的图像的能力。在本文中，我们提出了通过轻量级橡皮擦（Receler）来实现可靠的概念擦除。它学习了一个轻量级的橡皮擦来进行概念擦除，同时通过提出的概念定位正则化和对抗提示学习方案满足上述理想特性。通过对各种概念的全面实验验证了Receler相对于先前方法的优越性。我们的代码将在接受后提供。

    arXiv:2311.17717v2 Announce Type: replace-cross  Abstract: Concept erasure in text-to-image diffusion models aims to disable pre-trained diffusion models from generating images related to a target concept. To perform reliable concept erasure, the properties of robustness and locality are desirable. The former refrains the model from producing images associated with the target concept for any paraphrased or learned prompts, while the latter preserves its ability in generating images with non-target concepts. In this paper, we propose Reliable Concept Erasing via Lightweight Erasers (Receler). It learns a lightweight Eraser to perform concept erasing while satisfying the above desirable properties by proposed concept-localized regularization and adversarial prompt learning schemes. Comprehensive experiments with various concepts verify the superiority of Receler over previous methods. Our code will be available upon acceptance.
    
[^155]: 视频人脸重龄化：迈向时间一致的人脸重龄化

    Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging

    [https://arxiv.org/abs/2311.11642](https://arxiv.org/abs/2311.11642)

    该论文提出了一个新颖的合成视频数据集，设计了基线架构来验证其有效性，并开发了针对视频重龄化技术的时间一致性的新颖评估指标。

    

    视频人脸重龄化涉及在视频中将一个人的外观年龄改变到目标年龄。这个问题很具挑战性，因为缺乏在身份和年龄上保持时间一致性的配对视频数据集。大多数重龄化方法处理每个图像时都没有考虑视频的时间一致性。虽然一些现有工作通过在潜在空间中进行视频面部属性操作来解决时间一致问题，但它们通常在年龄转换方面表现不尽人意。为了解决这些问题，我们提出了（1）一个新颖的合成视频数据集，涵盖了各种年龄组的对象；（2）一个基线架构，旨在验证我们提出的数据集的有效性；以及（3）针对评估视频重龄化技术的时间一致性而设计的新颖度量标准。我们在公共数据集上进行了全面实验，包括

    arXiv:2311.11642v3 Announce Type: replace-cross  Abstract: Video face re-aging deals with altering the apparent age of a person to the target age in videos. This problem is challenging due to the lack of paired video datasets maintaining temporal consistency in identity and age. Most re-aging methods process each image individually without considering the temporal consistency of videos. While some existing works address the issue of temporal coherence through video facial attribute manipulation in latent space, they often fail to deliver satisfactory performance in age transformation. To tackle the issues, we propose (1) a novel synthetic video dataset that features subjects across a diverse range of age groups; (2) a baseline architecture designed to validate the effectiveness of our proposed dataset, and (3) the development of novel metrics tailored explicitly for evaluating the temporal consistency of video re-aging techniques. Our comprehensive experiments on public datasets, inclu
    
[^156]: 谨记地图！在从传感器估算在线HD地图时考虑现有地图信息

    Mind the map! Accounting for existing map information when estimating online HDMaps from sensor

    [https://arxiv.org/abs/2311.10517](https://arxiv.org/abs/2311.10517)

    在估算HD地图时考虑现有地图信息是本文的创新，提出了三种有用的现有地图类型，并引入了MapEX框架，通过编码地图元素和优化匹配算法来实现，在nuScenes数据集上取得了显著改进。

    

    尽管高清地图是自动驾驶的关键组成部分，但它们的获取和维护成本很高。因此，从传感器估算这些地图承诺显著降低成本。然而，这些估算忽视了现有的高清地图，当前方法最多只是将低质量地图地理定位，或考虑已知地图的通用数据库。在本文中，我们建议在估算HD地图时考虑所研究情况的现有地图。我们确定了3种合理的有用现有地图类型（简约型、嘈杂型和过时型）。我们还介绍了MapEX，这是一个新颖的在线HD地图估算框架，考虑了现有地图。MapEX通过将地图元素编码为查询标记，并通过优化用于训练经典基于查询的地图估算模型的匹配算法来实现这一点。我们证明MapEX在nuScenes数据集上带来了显著的改进。

    arXiv:2311.10517v2 Announce Type: replace  Abstract: While HDMaps are a crucial component of autonomous driving, they are expensive to acquire and maintain. Estimating these maps from sensors therefore promises to significantly lighten costs. These estimations however overlook existing HDMaps, with current methods at most geolocalizing low quality maps or considering a general database of known maps. In this paper, we propose to account for existing maps of the precise situation studied when estimating HDMaps. We identify 3 reasonable types of useful existing maps (minimalist, noisy, and outdated). We also introduce MapEX, a novel online HDMap estimation framework that accounts for existing maps. MapEX achieves this by encoding map elements into query tokens and by refining the matching algorithm used to train classic query based map estimation models. We demonstrate that MapEX brings significant improvements on the nuScenes dataset. For instance, MapEX - given noisy maps - improves by
    
[^157]: Plum: 使用元启发式的提示学习

    Plum: Prompt Learning using Metaheuristic

    [https://arxiv.org/abs/2311.08364](https://arxiv.org/abs/2311.08364)

    提出了使用元启发式的提示学习方法，通过测试六种典型的元启发式方法，在大型语言模型的提示优化任务中取得了有效性。

    

    自从大型语言模型出现以来，提示学习已成为优化和定制这些模型的一种流行方法。特殊提示，如“思维链”，甚至揭示了这些模型内部先前未知的推理能力。然而，发现有效提示的进展缓慢，促使人们渴望一种通用的提示优化方法。不幸的是，现有的提示学习方法中很少有满足“通用”的标准，即同时具备自动、离散、黑盒、无梯度和可解释性。在本文中，我们引入元启发式，作为一种有希望的提示学习方法的离散非凸优化方法分支，拥有100多种选项。在我们的范式中，我们测试了六种典型方法：爬山、模拟退火、遗传算法（带/不带交叉）、禁忌搜索和和谐搜索，展示了它们在白盒模式下的有效性。

    arXiv:2311.08364v2 Announce Type: replace-cross  Abstract: Since the emergence of large language models, prompt learning has become a popular method for optimizing and customizing these models. Special prompts, such as Chain-of-Thought, have even revealed previously unknown reasoning capabilities within these models. However, the progress of discovering effective prompts has been slow, driving a desire for general prompt optimization methods. Unfortunately, few existing prompt learning methods satisfy the criteria of being truly "general", i.e., automatic, discrete, black-box, gradient-free, and interpretable all at once. In this paper, we introduce metaheuristics, a branch of discrete non-convex optimization methods with over 100 options, as a promising approach to prompt learning. Within our paradigm, we test six typical methods: hill climbing, simulated annealing, genetic algorithms with/without crossover, tabu search, and harmony search, demonstrating their effectiveness in white-b
    
[^158]: 欧几里得、射影、共形：为等变换器选择几何代数

    Euclidean, Projective, Conformal: Choosing a Geometric Algebra for Equivariant Transformers

    [https://arxiv.org/abs/2311.04744](https://arxiv.org/abs/2311.04744)

    该论文研究了基于欧几里得、射影和共形代数的Geometric Algebra Transformer架构，发现共形代数和改进的射影代数定义了功能强大、性能良好的变换器架构。

    

    Geometric Algebra Transformer（GATr）是一种基于射影几何代数的通用几何深度学习架构。我们将这种架构概括为一个蓝图，允许一个根据任何几何（或克利福德）代数来构建可扩展的变换器架构。我们研究了欧几里得、射影和共形代数版本的这种架构，它们都适合表示3D数据，并在理论和实践中进行了评估。最简单的欧几里得架构在计算上廉价，但对称群较小且不够样本高效，而射影模型表达能力不够。共形代数和改进版本的射影代数都定义了功能强大、性能良好的架构。

    arXiv:2311.04744v2 Announce Type: replace-cross  Abstract: The Geometric Algebra Transformer (GATr) is a versatile architecture for geometric deep learning based on projective geometric algebra. We generalize this architecture into a blueprint that allows one to construct a scalable transformer architecture given any geometric (or Clifford) algebra. We study versions of this architecture for Euclidean, projective, and conformal algebras, all of which are suited to represent 3D data, and evaluate them in theory and practice. The simplest Euclidean architecture is computationally cheap, but has a smaller symmetry group and is not as sample-efficient, while the projective model is not sufficiently expressive. Both the conformal algebra and an improved version of the projective algebra define powerful, performant architectures.
    
[^159]: LILO：通过压缩和文档化代码学习可解释库

    LILO: Learning Interpretable Libraries by Compressing and Documenting Code

    [https://arxiv.org/abs/2310.19791](https://arxiv.org/abs/2310.19791)

    LILO是一种神经符号框架，通过迭代地合成、压缩和文档化代码来构建可解释且适用于特定问题领域的程序库。在其中，LILO结合了大型语言模型引导的程序合成和程序自动重构的算法进展，并且通过自动文档过程使得代码抽象可解释并提升性能。

    

    尽管大型语言模型（LLMs）在代码生成方面表现出色，但软件开发的关键方面是重构的艺术：将代码整合到可重用和可读的程序库中。本文介绍了一种名为LILO的神经符号框架，它通过迭代地合成、压缩和文档化代码来构建适合特定问题领域的库。LILO将LLM引导的程序合成与Stitch自动重构的近期算法进展相结合：Stitch是一个符号压缩系统，可以高效地识别大型代码语料库中的最佳lambda抽象。为了使这些抽象可解释，我们引入了一种自动文档（AutoDoc）过程，它根据上下文中的使用示例推断出自然语言名称和文档字符串。除了提高人类可读性外，我们发现AutoDoc通过帮助LILO的合成器解释和部署学习到的抽象来提高性能。我们对LILO进行了三个归纳式程序综合的评估。

    While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synth
    
[^160]: 视觉语言模型是强化学习的零样本奖励模型

    Vision-Language Models are Zero-Shot Reward Models for Reinforcement Learning

    [https://arxiv.org/abs/2310.12921](https://arxiv.org/abs/2310.12921)

    使用预训练的视觉语言模型作为零样本奖励模型，在强化学习中指定任务，提高训练效率。

    

    强化学习（RL）要求手动指定奖励函数，这通常是不可行的，或者通过大量人类反馈学习奖励模型，这通常是非常昂贵的。本文研究了一种更加样本高效的替代方案：使用预训练的视觉语言模型（VLM）作为零样本奖励模型（RM），通过自然语言指定任务。我们提出了一种自然和通用的使用VLM作为奖励模型的方法，称为VLM-RMs。我们使用基于CLIP的VLM-RMs来训练MuJoCo人形模型学习复杂任务，而无需手动指定奖励函数，例如跪下、劈叉和盘腿坐。对于每个任务，我们仅提供一个描述所需任务的单个句子文本提示，减少提示工程。我们提供训练代理的视频链接：https://sites.google.com/view/vlm-rm。我们可以通过提供第二个“基准”提示来改善性能。

    arXiv:2310.12921v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) requires either manually specifying a reward function, which is often infeasible, or learning a reward model from a large amount of human feedback, which is often very expensive. We study a more sample-efficient alternative: using pretrained vision-language models (VLMs) as zero-shot reward models (RMs) to specify tasks via natural language. We propose a natural and general approach to using VLMs as reward models, which we call VLM-RMs. We use VLM-RMs based on CLIP to train a MuJoCo humanoid to learn complex tasks without a manually specified reward function, such as kneeling, doing the splits, and sitting in a lotus position. For each of these tasks, we only provide a single sentence text prompt describing the desired task with minimal prompt engineering. We provide videos of the trained agents at: https://sites.google.com/view/vlm-rm. We can improve performance by providing a second "baseline" prom
    
[^161]: VeCLIP：通过富含视觉信息的标题改进CLIP训练

    VeCLIP: Improving CLIP Training via Visual-enriched Captions

    [https://arxiv.org/abs/2310.07699](https://arxiv.org/abs/2310.07699)

    本研究提出了一种通过将视觉概念融入标题中的方式来改进CLIP训练的方法，名为VeCLIP，该方法在大规模网络爬取数据集上展示了良好的性能。

    

    大规模网络爬取数据集对于预训练视觉-语言模型（如CLIP）的成功至关重要。然而，网络爬取的AltTexts存在固有的噪音和潜在的不相关性，造成了精确的图像-文字对齐方面的挑战。本研究引入了一种适用于嘈杂标题重写的可扩展流程。与利用大型语言模型（LLMs）进行标题重写的现有方法在小型策划数据集（如CC3M和CC12M）上已经显示出了希望。我们强调将视觉概念融入标题中，称为富含视觉信息的标题（VeCap），以确保数据多样性。为了优化AltTexts与新生成的VeCap的利用，我们提出了一种新颖的混合训练方案。我们展示了该方法在大规模网络爬取数据集上训练CLIP的适应性，称为VeCLIP。通过使用这种经济有效的流程，我们轻松扩展了我们的实验。

    arXiv:2310.07699v2 Announce Type: replace-cross  Abstract: Large-scale web-crawled datasets are fundamental for the success of pre-training vision-language models, such as CLIP. However, the inherent noise and potential irrelevance of web-crawled AltTexts pose challenges in achieving precise image-text alignment. Existing methods utilizing large language models (LLMs) for caption rewriting have shown promise on small, curated datasets like CC3M and CC12M. This study introduces a scalable pipeline for noisy caption rewriting. Unlike recent LLM rewriting techniques, we emphasize the incorporation of visual concepts into captions, termed as Visual-enriched Captions (VeCap). To ensure data diversity, we propose a novel mixed training scheme that optimizes the utilization of AltTexts alongside newly generated VeCap. We showcase the adaptation of this method for training CLIP on large-scale web-crawled datasets, termed VeCLIP. Employing this cost-effective pipeline, we effortlessly scale our
    
[^162]: iTransformer: 反转Transformer在时间序列预测中是有效的

    iTransformer: Inverted Transformers Are Effective for Time Series Forecasting

    [https://arxiv.org/abs/2310.06625](https://arxiv.org/abs/2310.06625)

    iTransformer通过重新利用Transformer架构，在时间序列预测中简单应用注意力和馈送，提高了性能并克服了其他模型在处理具有更大回溯窗口的系列时面临的挑战

    

    最近线性预测模型的兴起对基于Transformer的预测器的架构修改的持续热情提出了质疑。这些预测器利用Transformer来模拟对时间序列的时间标记的全局依赖关系，每个时间标记由相同时间戳的多个变量组成。然而，由于性能下降和计算爆炸，Transformer在预测具有更大回溯窗口的系列时受到挑战。此外，每个时间标记的嵌入融合了代表潜在延迟事件和不同物理测量的多个变量，这可能会导致无法学习变量-centric表示并导致无意义的注意力映射。在这项工作中，我们反思了Transformer组件的能力，并重新利用了Transformer架构，而没有修改基本组件。我们提出了iTransformer，它简单地应用了注意力和馈送-

    arXiv:2310.06625v3 Announce Type: replace  Abstract: The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-
    
[^163]: 谨言慎行：使用暂停标记训练语言模型

    Think before you speak: Training Language Models With Pause Tokens

    [https://arxiv.org/abs/2310.02226](https://arxiv.org/abs/2310.02226)

    引入暂停标记的语言模型训练方法可以让模型在输出标记前处理更多隐藏向量，取得了较好的实验结果

    

    语言模型通过立即连续生成一系列标记来生成响应: 第$(K+1)^{th}$个标记是通过操作每层的$K$个隐藏向量得到的，每个向量对应一个前面的标记。如果我们让模型在输出第$(K+1)^{th}$个标记之前操作更多的隐藏向量，比如说$K+10$个呢？我们通过在语言模型上进行训练和推断，引入了一个（可学习的）$\textit{pause}$标记，这一系列标记附加到输入前缀上。然后我们延迟提取模型的输出，直到最后一个暂停标记被看到，从而允许模型在做出答案之前进行额外的计算处理。我们在拥有1B和130M参数的仅解码器模型上进行了$\textit{pause-training}$的实证评估，在C4上进行了因果预训练，并在涵盖推理、问答、普遍理解和事实回忆等下游任务上进行了评估。我们的主要发现是，infer

    arXiv:2310.02226v2 Announce Type: replace-cross  Abstract: Language models generate responses by producing a series of tokens in immediate succession: the $(K+1)^{th}$ token is an outcome of manipulating $K$ hidden vectors per layer, one vector per preceding token. What if instead we were to let the model manipulate say, $K+10$ hidden vectors, before it outputs the $(K+1)^{th}$ token? We operationalize this idea by performing training and inference on language models with a (learnable) $\textit{pause}$ token, a sequence of which is appended to the input prefix. We then delay extracting the model's outputs until the last pause token is seen, thereby allowing the model to process extra computation before committing to an answer. We empirically evaluate $\textit{pause-training}$ on decoder-only models of 1B and 130M parameters with causal pretraining on C4, and on downstream tasks covering reasoning, question-answering, general understanding and fact recall. Our main finding is that infer
    
[^164]: 合并，然后压缩：从其路由策略中揭示高效的SMoE技术提示

    Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy

    [https://arxiv.org/abs/2310.01334](https://arxiv.org/abs/2310.01334)

    本文旨在探讨如何通过合并专家信息来制定出更紧凑但更具知识的SMoE模型，因为传统的模型合并方法并不适用于SMoE的专家合并。

    

    稀疏激活的专家混合模型（SMoE）显示出扩展神经网络学习能力的潜力，然而，它们存在诸如（a）高内存使用的问题，由于网络层的重复成为多个专家的副本；以及（b）专家中的冗余，因为常规基于学习的路由策略容易出现表示性崩溃。因此，传统SMoE模型在内存效率和可伸缩性方面效率低下，尤其对于资源受限的下游场景。在本文中，我们提出了一个问题：我们能否通过合并专家信息来制定一个紧凑的SMoE模型？如何将多个专家合并为更少但更有知识的专家的最佳方法？我们的初步调查显示，传统的模型合并方法对于SMoE的专家合并并不有效。潜在原因是：（1）冗余信息掩盖了关键专家；（2）为每个专家选择适当的神经元排列方式会丢失

    arXiv:2310.01334v2 Announce Type: replace-cross  Abstract: Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is miss
    
[^165]: 通过执行反馈使语言模型成为更好的工具学习者

    Making Language Models Better Tool Learners with Execution Feedback

    [https://arxiv.org/abs/2305.13068](https://arxiv.org/abs/2305.13068)

    这篇论文提出了一个名为TRICE的框架，通过执行反馈实现语言模型的工具学习，使其能够学会何时以及如何有效地使用工具。

    

    工具作为关键的界面，使人类能够理解和改变环境。随着基础模型的出现，AI系统可以利用工具扩展其能力并与真实世界互动。现有的工具学习方法包括监督微调和提示工程方法，通常使大型语言模型不加选择地利用工具，因为复杂任务往往超出了它们自身的能力。然而，为简单任务引入工具（模型本身可以轻松解决的任务），可能会无意间传播错误而不是提高性能。因此，研究问题是：我们能否教会语言模型何时以及如何使用工具？为满足这个需求，我们提出了Tool leaRning wIth exeCution fEedback (TRICE)，这是一个两阶段的端到端框架，使模型能够通过从工具执行中得到的反馈不断学习，从而学会何时以及如何有效地使用工具。

    Tools serve as pivotal interfaces that enable humans to understand and reshape the environment. With the advent of foundation models, AI systems can utilize tools to expand their capabilities and interact with the real world. Existing tool learning methodologies, encompassing supervised fine-tuning and prompt engineering approaches, often induce large language models to utilize tools indiscriminately, as complex tasks often exceed their own competencies. However, introducing tools for simple tasks, which the models themselves can readily resolve, can inadvertently propagate errors rather than enhance performance. This leads to the research question: can we teach language models when and how to use tools? To meet this need, we propose Tool leaRning wIth exeCution fEedback (TRICE), a two-stage end-to-end framework that enables the model to continually learn through feedback derived from tool execution, thereby learning when and how to use tools effectively. Experimental results, backed b
    
[^166]: 智能家居中的能量分解与电器识别：迁移学习实现边缘计算

    Energy Disaggregation & Appliance Identification in a Smart Home: Transfer Learning enables Edge Computing

    [https://arxiv.org/abs/2301.03018](https://arxiv.org/abs/2301.03018)

    迁移学习结合边缘计算，提出了新的深度学习方法解决非侵入式负载监视问题和电器识别问题，利用改进的CNN模型和预训练模型获得更准确的结果。

    

    非侵入式负载监视（NILM）或能量分解旨在在智能家居的总负载配置文件的基础上提取单个消费电子设备的负载配置文件。这项工作提出了一种新颖的深度学习和边缘计算方法来解决NILM问题以及一些相关问题。1）我们在著名的seq2-point卷积神经网络（CNN）模型的基础上构建了提出的seq2-[3]-point CNN模型，以解决（家庭）NILM问题和站点NILM问题（基本上是在较小规模上的NILM）。2）我们通过借鉴最先进的（预训练）2D-CNN模型，即AlexNet、ResNet-18和DenseNet-121，解决了电器识别的相关问题，这些模型被微调以适应两个自定义数据集，这些数据集包含电器基于小波变换和短时傅里叶变换（STFT）的2D电特征。3）最后，我们进行了一些基本的定性推断。

    arXiv:2301.03018v2 Announce Type: replace-cross  Abstract: Non-intrusive load monitoring (NILM) or energy disaggregation aims to extract the load profiles of individual consumer electronic appliances, given an aggregate load profile of the mains of a smart home. This work proposes a novel deep-learning and edge computing approach to solve the NILM problem and a few related problems as follows. 1) We build upon the reputed seq2-point convolutional neural network (CNN) model to come up with the proposed seq2-[3]-point CNN model to solve the (home) NILM problem and site-NILM problem (basically, NILM at a smaller scale). 2) We solve the related problem of appliance identification by building upon the state-of-the-art (pre-trained) 2D-CNN models, i.e., AlexNet, ResNet-18, and DenseNet-121, which are fine-tuned two custom datasets that consist of Wavelets and short-time Fourier transform (STFT)-based 2D electrical signatures of the appliances. 3) Finally, we do some basic qualitative inferen
    
[^167]: 评估序列长度学习对Transformer编码器模型分类任务的影响

    Assessing the Impact of Sequence Length Learning on Classification Tasks for Transformer Encoder Models

    [https://arxiv.org/abs/2212.08399](https://arxiv.org/abs/2212.08399)

    Transformer编码器模型在处理分类任务时，受到序列长度学习问题影响，可能导致模型过度依赖序列长度作为预测特征而非文本信息，本文提出了一些方法来减少这种影响。

    

    使用Transformer架构的分类算法在观测来自不同类的序列具有不同长度分布时可能受到序列长度学习问题的影响。这个问题导致模型将序列长度作为一个预测特征，而不是依赖于重要的文本信息。虽然大多数公共数据集不受此问题影响，但对于医学和保险等领域的私人拥有语料库可能存在这种数据偏差。利用这种序列长度特征会在整个价值链中带来挑战，因为这些机器学习模型可以用于关键应用。在本文中，我们从经验上揭示了这个问题，并提出了减少其影响的方法。

    arXiv:2212.08399v2 Announce Type: replace  Abstract: Classification algorithms using Transformer architectures can be affected by the sequence length learning problem whenever observations from different classes have a different length distribution. This problem causes models to use sequence length as a predictive feature instead of relying on important textual information. Although most public datasets are not affected by this problem, privately owned corpora for fields such as medicine and insurance may carry this data bias. The exploitation of this sequence length feature poses challenges throughout the value chain as these machine learning models can be used in critical applications. In this paper, we empirically expose this problem and present approaches to minimize its impacts.
    
[^168]: COMET：用于分布式深度学习训练的全面集群设计方法论

    COMET: A Comprehensive Cluster Design Methodology for Distributed Deep Learning Training

    [https://arxiv.org/abs/2211.16648](https://arxiv.org/abs/2211.16648)

    COMET提出了一种全面的集群设计方法论，用于研究并行化策略和关键集群资源配置对分布式DL训练性能的影响

    

    现代深度学习（DL）模型已经发展到需要大规模专门的、高端节点进行训练的大小。设计这样的集群以最大限度地提高性能和利用率--以摊销其高昂成本--是一项具有挑战性的任务，需要仔细平衡计算、内存和网络资源。此外，每个模型的众多调整旋钮极大地影响性能，最佳值往往取决于底层集群的特征，这要求进行复杂的集群-工作负载协同设计过程。为了促进这些大规模DL训练集群的设计空间探索，我们引入了COMET，这是一种综合的集群设计方法和工作流程，用于共同研究并行化策略和关键集群资源配置对分布式DL训练性能的影响。我们开发了一个逐步的过程，建立一种可重用和灵活的方法论，并加以演示。

    arXiv:2211.16648v2 Announce Type: replace-cross  Abstract: Modern Deep Learning (DL) models have grown to sizes requiring massive clusters of specialized, high-end nodes to train. Designing such clusters to maximize both performance and utilization--to amortize their steep cost--is a challenging task requiring careful balance of compute, memory, and network resources. Moreover, a plethora of each model's tuning knobs drastically affect the performance, with optimal values often depending on the underlying cluster's characteristics, which necessitates a complex cluster-workload co-design process. To facilitate the design space exploration of such massive DL training clusters, we introduce COMET, a holistic cluster design methodology and workflow to jointly study the impact of parallelization strategies and key cluster resource provisioning on the performance of distributed DL training. We develop a step-by-step process to establish a reusable and flexible methodology, and demonstrate it
    
[^169]: 对象检测的即插即用主动学习

    Plug and Play Active Learning for Object Detection

    [https://arxiv.org/abs/2211.11612](https://arxiv.org/abs/2211.11612)

    PPAL是一个简单而有效的对象检测主动学习策略，通过两阶段的不确定性和多样性抽样相结合，克服了专门对象检测器架构集成困难的挑战。

    

    为对象检测标注数据集是一项昂贵且耗时的工作。为了最大限度地减少这一负担，采用主动学习（AL）技术来在限定的“标注预算”内选择信息量最大的样本进行标注。传统的AL策略通常依赖于模型的不确定性或样本的多样性进行查询抽样，而更先进的方法则专注于开发AL特定的对象检测器架构以提高性能。然而，这些专门化方法由于集成所需的大量工程工作，不容易适应不同的对象检测器。为了克服这一挑战，我们引入了即插即用主动学习（PPAL），这是一种简单而有效的对象检测AL策略。PPAL是一个包含基于不确定性和多样性抽样阶段的两阶段方法。在第一阶段，我们的难度校准不确定性抽样...

    arXiv:2211.11612v2 Announce Type: replace-cross  Abstract: Annotating datasets for object detection is an expensive and time-consuming endeavor. To minimize this burden, active learning (AL) techniques are employed to select the most informative samples for annotation within a constrained "annotation budget". Traditional AL strategies typically rely on model uncertainty or sample diversity for query sampling, while more advanced methods have focused on developing AL-specific object detector architectures to enhance performance. However, these specialized approaches are not readily adaptable to different object detectors due to the significant engineering effort required for integration. To overcome this challenge, we introduce Plug and Play Active Learning (PPAL), a simple and effective AL strategy for object detection. PPAL is a two-stage method comprising uncertainty-based and diversity-based sampling phases. In the first stage, our Difficulty Calibrated Uncertainty Sampling leverage
    
[^170]: SVD-PINNs: 通过奇异值分解实现物理信息神经网络的迁移学习

    SVD-PINNs: Transfer Learning of Physics-Informed Neural Networks via Singular Value Decomposition

    [https://arxiv.org/abs/2211.08760](https://arxiv.org/abs/2211.08760)

    本文提出了一种通过奇异值分解实现物理信息神经网络的迁移学习方法（SVD-PINNs），有效解决了一类具有不同但相似右端项的高维PDEs。

    

    物理信息神经网络（PINNs）近年来受到了广泛关注，因为它们缓解了传统方法中出现的维度诅咒，适用于求解偏微分方程（PDEs）。然而，PINNs的最大缺点是一个神经网络对应一个PDE。在实践中，我们通常需要解决一类PDEs，而不仅仅是一个。随着深度学习的爆炸性增长，许多通用深度学习任务中的有用技术也适用于PINNs。迁移学习方法可以减少PINNs在解决一类PDEs时的成本。本文提出了一种通过保留奇异向量和优化奇异值的PINNs的迁移学习方法（即SVD-PINNs）。对高维PDEs（10维线性抛物线方程和10维Allen-Cahn方程）的数值实验证明，SVD-PINNs可用于解决一类具有不同但相似右端项的PDEs。

    arXiv:2211.08760v2 Announce Type: replace  Abstract: Physics-informed neural networks (PINNs) have attracted significant attention for solving partial differential equations (PDEs) in recent years because they alleviate the curse of dimensionality that appears in traditional methods. However, the most disadvantage of PINNs is that one neural network corresponds to one PDE. In practice, we usually need to solve a class of PDEs, not just one. With the explosive growth of deep learning, many useful techniques in general deep learning tasks are also suitable for PINNs. Transfer learning methods may reduce the cost for PINNs in solving a class of PDEs. In this paper, we proposed a transfer learning method of PINNs via keeping singular vectors and optimizing singular values (namely SVD-PINNs). Numerical experiments on high dimensional PDEs (10-d linear parabolic equations and 10-d Allen-Cahn equations) show that SVD-PINNs work for solving a class of PDEs with different but close right-hand-s
    
[^171]: 用软标签原型从少量示例中学习新任务

    Learning New Tasks from a Few Examples with Soft-Label Prototypes

    [https://arxiv.org/abs/2210.17437](https://arxiv.org/abs/2210.17437)

    本研究提出了一种新的极端少样本学习方法，利用软标签原型从少量示例中学习新任务，在大型、高维和现实世界数据集上表现出色。

    

    自然语言处理中的少样本学习现有方法依赖于大型语言模型和对其微调，以在分布外数据上进行泛化。在这项工作中，我们提出了一种简单但强大的“极端”少样本学习方法，其中模型只需接触每个类别至少4个示例，这些示例基于软标签原型，这些软标签原型共同捕获了输入域空间中不同类别的分布。受到先前关于一元或简单多元（合成）数据（Sucholutsky等人，2021）的工作的启发，我们提出了一种在大型、高维和现实世界数据集上有效的新方法。我们在神经框架（DeepSLP）中学习软标签原型，并在实验中展示，它在31/48个测试任务和少样本设置上表现优异，同时在其他任务上与强基线模型的性能相匹配。我们专注于从v中学习以前未见过的NLP任务

    arXiv:2210.17437v3 Announce Type: replace-cross  Abstract: Existing approaches to few-shot learning in NLP rely on large language models and fine-tuning of these to generalise on out-of-distribution data. In this work, we propose a simple yet powerful approach to "extreme" few-shot learning, wherein models are exposed to as little as 4 examples per class, based on soft-label prototypes that collectively capture the distribution of different classes across the input domain space. Inspired by previous work (Sucholutsky et al., 2021) on univariate or simple multivariate (synthetic) data, we propose a novel approach that is effective on large, high-dimensional and real-world datasets. We learn soft-label prototypes within a neural framework (DeepSLP) and we experimentally demonstrate that it achieves superior performance on 31/48 tested tasks and few-shot settings while closely matching the performance of strong baselines on the rest. We focus on learning previously unseen NLP tasks from v
    
[^172]: DPAR: 具有节点级差分隐私的分离图神经网络

    DPAR: Decoupled Graph Neural Networks with Node-Level Differential Privacy

    [https://arxiv.org/abs/2210.04442](https://arxiv.org/abs/2210.04442)

    本研究提出了一种名为DPAR的分离图神经网络，能实现对GNNs进行节点级差分隐私，从而保护节点及其边缘。

    

    图神经网络（GNNs）在学习图结构数据方面取得了巨大成功。 还提出了对训练模型的隐私问题，这可能暴露图的敏感信息，包括节点特征和结构信息。 本文旨在实现对GNNs进行节点级差分隐私（DP），以保护节点及其边缘。 GNNs的节点DP在本质上是困难的，因为所有直接和多跳邻居通过逐层消息传递参与每个节点的梯度计算，并且节点可以具有多少直接和多跳邻居，因此现有的DP方法将导致很高的隐私成本或由于节点敏感性高而效用不佳。 我们提出了具有差异性私人化调整页面排名（DPAR）的\textbf{D}ecoupled GNN，用于训练带有增强隐私效用

    arXiv:2210.04442v2 Announce Type: replace  Abstract: Graph Neural Networks (GNNs) have achieved great success in learning with graph-structured data. Privacy concerns have also been raised for the trained models which could expose the sensitive information of graphs including both node features and the structure information. In this paper, we aim to achieve node-level differential privacy (DP) for training GNNs so that a node and its edges are protected. Node DP is inherently difficult for GNNs because all direct and multi-hop neighbors participate in the calculation of gradients for each node via layer-wise message passing and there is no bound on how many direct and multi-hop neighbors a node can have, so existing DP methods will result in high privacy cost or poor utility due to high node sensitivity. We propose a \textbf{D}ecoupled GNN with Differentially \textbf{P}rivate \textbf{A}pproximate Personalized Page\textbf{R}ank (DPAR) for training GNNs with an enhanced privacy-utility t
    
[^173]: 通过融合地标特征实现成人-儿童面部表情的深度调整

    Deep Adaptation of Adult-Child Facial Expressions by Fusing Landmark Features

    [https://arxiv.org/abs/2209.08614](https://arxiv.org/abs/2209.08614)

    提出了通过领域自适应和融合LANDMARK特征来实现成人和儿童面部表情的深度调整，以便在教育、医疗保健、娱乐等领域中进行准确分类。

    

    面部情感成像可用于测量儿童到成人阶段的心理生理特征，应用于教育、医疗保健、娱乐等领域。深度卷积神经网络在成人面部表情分类中展现出有希望的结果。然而，用成人基准数据训练的分类器模型不适合学习儿童表情，因为心理物理发展存在差异。同样地，用儿童数据训练的模型在成人表情分类上表现不佳。我们提出领域自适应，同时将成人和儿童表情的分布对齐到共享的潜在空间，以便对任何一个领域进行健壮分类。此外，虽然年龄变化在面部图像中受到年龄不变人脸识别的研究，但在成人-儿童表情分类中仍未得到利用。我们从多个领域汲取灵感，提出深度自适应LANDMARK特征融合以实现精准的成人-儿童面部表情分类。

    arXiv:2209.08614v2 Announce Type: replace-cross  Abstract: Imaging of facial affects may be used to measure psychophysiological attributes of children through their adulthood for applications in education, healthcare, and entertainment, among others. Deep convolutional neural networks show promising results in classifying facial expressions of adults. However, classifier models trained with adult benchmark data are unsuitable for learning child expressions due to discrepancies in psychophysical development. Similarly, models trained with child data perform poorly in adult expression classification. We propose domain adaptation to concurrently align distributions of adult and child expressions in a shared latent space for robust classification of either domain. Furthermore, age variations in facial images are studied in age-invariant face recognition yet remain unleveraged in adult-child expression classification. We take inspiration from multiple fields and propose deep adaptive FACial
    
[^174]: 自然语言上的多步演绎推理：基于超领域泛化的实证研究

    Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study on Out-of-Distribution Generalisation

    [https://arxiv.org/abs/2207.14000](https://arxiv.org/abs/2207.14000)

    提出了IMA-GloVe-GA，一个用于自然语言表达的多步推理的迭代神经推理网络，在超领域泛化方面具有更好的性能表现。

    

    将深度学习与符号逻辑推理结合起来，旨在充分利用这两个领域的成功，并引起了越来越多的关注。受DeepLogic启发，该模型经过端到端训练，用于执行逻辑程序推理，我们介绍了IMA-GloVe-GA，这是一个用自然语言表达的多步推理的迭代神经推理网络。在我们的模型中，推理是使用基于RNN的迭代内存神经网络进行的，其中包含一个门关注机制。我们在PARARULES、CONCEPTRULES V1和CONCEPTRULES V2三个数据集上评估了IMA-GloVe-GA。实验结果表明，带有门关注机制的DeepLogic比DeepLogic和其他RNN基线模型能够实现更高的测试准确性。我们的模型在规则被打乱时比RoBERTa-Large实现了更好的超领域泛化性能。此外，为了解决当前多步推理数据集中推理深度不平衡的问题

    arXiv:2207.14000v2 Announce Type: replace-cross  Abstract: Combining deep learning with symbolic logic reasoning aims to capitalize on the success of both fields and is drawing increasing attention. Inspired by DeepLogic, an end-to-end model trained to perform inference on logic programs, we introduce IMA-GloVe-GA, an iterative neural inference network for multi-step reasoning expressed in natural language. In our model, reasoning is performed using an iterative memory neural network based on RNN with a gate attention mechanism. We evaluate IMA-GloVe-GA on three datasets: PARARULES, CONCEPTRULES V1 and CONCEPTRULES V2. Experimental results show DeepLogic with gate attention can achieve higher test accuracy than DeepLogic and other RNN baseline models. Our model achieves better out-of-distribution generalisation than RoBERTa-Large when the rules have been shuffled. Furthermore, to address the issue of unbalanced distribution of reasoning depths in the current multi-step reasoning datase
    
[^175]: 具有决策相关分布的随机逼近：渐近正态性和最优性

    Stochastic Approximation with Decision-Dependent Distributions: Asymptotic Normality and Optimality

    [https://arxiv.org/abs/2207.04173](https://arxiv.org/abs/2207.04173)

    该论文分析了一种针对决策相关问题的随机逼近算法，在渐近意义下，算法的平均迭代与解之间的偏差是正态的，并且算法的性能在局部达到了最优水平。

    

    我们分析了一种针对决策相关问题的随机逼近算法，其中算法使用的数据分布沿着迭代序列演变。这类问题的主要例子出现在执行性预测及其多人游戏扩展中。我们展示在温和假设下，算法的平均迭代与解之间的偏差在渐近意义下是正态的，协方差清晰地分解了梯度噪声和分布变化的影响。此外，基于H\'ajek和Le Cam的工作，我们展示了带有平均的算法的渐近性能在局部是局部最小化最优的。

    arXiv:2207.04173v3 Announce Type: replace-cross  Abstract: We analyze a stochastic approximation algorithm for decision-dependent problems, wherein the data distribution used by the algorithm evolves along the iterate sequence. The primary examples of such problems appear in performative prediction and its multiplayer extensions. We show that under mild assumptions, the deviation between the average iterate of the algorithm and the solution is asymptotically normal, with a covariance that clearly decouples the effects of the gradient noise and the distributional shift. Moreover, building on the work of H\'ajek and Le Cam, we show that the asymptotic performance of the algorithm with averaging is locally minimax optimal.
    
[^176]: 使用直线路径插值对Denoising Diffusion GANs的SPI-GAN方法

    SPI-GAN: Denoising Diffusion GANs with Straight-Path Interpolations

    [https://arxiv.org/abs/2206.14464](https://arxiv.org/abs/2206.14464)

    SPI-GAN使用直线路径插值定义的增强型GAN去噪方法，能够在极大程度上减少采样时间，同时实现与SGMs相同的高采样质量和多样性。

    

    基于分数的生成模型（SGMs）展示了最先进的采样质量和多样性。然而，它们的训练/采样复杂性由于高度复杂的前向/后向过程而极高，因此不适合资源有限的环境。为了解决这个问题，目前对学习更简单过程的关注度正逐渐增加。我们提出了一种增强的基于GAN的去噪方法，称为SPI-GAN，使用我们提出的直线路径插值定义。为此，我们提出了一种GAN架构，通过直线路径去噪，并且由连续映射神经网络表征，以模仿去噪路径。这种方法大大减少了采样时间，同时实现了与SGMs相同的高采样质量和多样性。因此，SPI-GAN是CIFAR-10和CelebA-HQ-256中采样质量、多样性和时间中最均衡的模型之一。

    arXiv:2206.14464v3 Announce Type: replace-cross  Abstract: Score-based generative models (SGMs) show the state-of-the-art sampling quality and diversity. However, their training/sampling complexity is notoriously high due to the highly complicated forward/reverse processes, so they are not suitable for resource-limited settings. To solving this problem, learning a simpler process is gathering much attention currently. We present an enhanced GAN-based denoising method, called SPI-GAN, using our proposed straight-path interpolation definition. To this end, we propose a GAN architecture i) denoising through the straight-path and ii) characterized by a continuous mapping neural network for imitating the denoising path. This approach drastically reduces the sampling time while achieving as high sampling quality and diversity as SGMs. As a result, SPI-GAN is one of the best-balanced models among the sampling quality, diversity, and time for CIFAR-10, and CelebA-HQ-256.
    
[^177]: OpenXAI: 迈向透明评估模型解释

    OpenXAI: Towards a Transparent Evaluation of Model Explanations

    [https://arxiv.org/abs/2206.11104](https://arxiv.org/abs/2206.11104)

    OpenXAI 是一个开源框架，旨在评估和基准测试后续解释方法，提供了灵活的数据生成器、多种数据集和评估指标，用户可轻松扩展和比较不同解释方法。

    

    虽然最近文献中提出了几种后续解释方法，但对这些方法进行系统性基准测试的工作非常少。在这里，我们介绍了OpenXAI，一个全面且可扩展的开源框架，用于评估和基准测试后续解释方法。OpenXAI包括以下关键组件：（i）灵活的合成数据生成器和各种真实世界数据集、预训练模型和最先进特征归属方法的集合，以及（ii）用于评估解释方法忠实度、稳定性（鲁棒性）和公平性的十一种量化度量标准的开源实现，从而提供了对多种度量标准、模型和数据集上几种解释方法的比较。OpenXAI易于扩展，用户可以轻松评估自定义解释方法并将其纳入我们的排行榜中。

    arXiv:2206.11104v4 Announce Type: replace-cross  Abstract: While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, and (ii) open-source implementations of eleven quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, in turn providing comparisons of several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. Overall, OpenXAI 
    
[^178]: 一种具有可变结构的半不可知假设用于量子机器学习

    A semi-agnostic ansatz with variable structure for quantum machine learning

    [https://arxiv.org/abs/2103.06712](https://arxiv.org/abs/2103.06712)

    提出了一种 VAns (Variable Ansatz) 可变结构方法来构建 VQAs 的假设，通过在优化过程中以一种熟悉的方式增长和移除量子门来成功缓解了可训练性和噪音相关问题。

    

    Quantum machine learning -- and specifically Variational Quantum Algorithms (VQAs) -- offers a powerful, flexible paradigm for programming near-term quantum computers, with applications in chemistry, metrology, materials science, data science, and mathematics. Here, one trains an ansatz, in the form of a parameterized quantum circuit, to accomplish a task of interest. However, challenges have recently emerged suggesting that deep ansatzes are difficult to train, due to flat training landscapes caused by randomness or by hardware noise. This motivates our work, where we present a variable structure approach to build ansatzes for VQAs. Our approach, called VAns (Variable Ansatz), applies a set of rules to both grow and (crucially) remove quantum gates in an informed manner during the optimization. Consequently, VAns is ideally suited to mitigate trainability and noise-related issues by keeping the ansatz shallow. We employ VAns i

    arXiv:2103.06712v4 Announce Type: replace-cross  Abstract: Quantum machine learning -- and specifically Variational Quantum Algorithms (VQAs) -- offers a powerful, flexible paradigm for programming near-term quantum computers, with applications in chemistry, metrology, materials science, data science, and mathematics. Here, one trains an ansatz, in the form of a parameterized quantum circuit, to accomplish a task of interest. However, challenges have recently emerged suggesting that deep ansatzes are difficult to train, due to flat training landscapes caused by randomness or by hardware noise. This motivates our work, where we present a variable structure approach to build ansatzes for VQAs. Our approach, called VAns (Variable Ansatz), applies a set of rules to both grow and (crucially) remove quantum gates in an informed manner during the optimization. Consequently, VAns is ideally suited to mitigate trainability and noise-related issues by keeping the ansatz shallow. We employ VAns i
    
[^179]: 评估用于基于OSINT的网络威胁意识的LLM聊天机器人

    Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness. (arXiv:2401.15127v1 [cs.CR])

    [http://arxiv.org/abs/2401.15127](http://arxiv.org/abs/2401.15127)

    本研究评估了LLM聊天机器人在基于OSINT的网络威胁意识中的应用能力，并发现聊天机器人在网络安全的二分类和命名实体识别任务方面表现出良好的性能。

    

    在快速发展的网络安全领域中，关于新兴威胁的知识共享至关重要，并构成了网络威胁情报的基础。在这个背景下，大型语言模型在网络安全领域越来越重要，提供了广泛的机遇。本研究探讨了ChatGPT、GPT4all、Dolly、Stanford Alpaca、Alpaca-LoRA和Falcon等聊天机器人在识别开源情报中与网络安全相关的文本方面的能力。我们评估了现有聊天机器人模型在自然语言处理任务中的能力。我们考虑了二分类和命名实体识别作为任务。本研究分析了从Twitter收集的经过充分验证的数据，该数据来源于以往的研究工作。在网络安全的二分类问题方面，商业模型Chatbot GPT-4实现了可接受的F1分数0.94，而开源模型GPT4all实现了F1分数0.90。然而，就网络安全实体识别而言，

    Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence. In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study explores the capability of chatbots such as ChatGPT, GPT4all, Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify cybersecurity-related text within Open Source Intelligence. We assess the capabilities of existing chatbot models for Natural Language Processing tasks. We consider binary classification and Named Entity Recognition as tasks. This study analyzes well-established data collected from Twitter, derived from previous research efforts. Regarding cybersecurity binary classification, Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94, and the open-source GPT4all model achieved an F1-score of 0.90. However, concerning cybersecurity entity re
    
[^180]: 安全且广义的端到端自主驾驶系统：基于强化学习和示范的研究

    Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2401.11792](http://arxiv.org/abs/2401.11792)

    本文介绍了一种安全且广义的端到端自主驾驶系统 (SGADS)，使用强化学习和示范相结合的方法解决了现有方法的低安全性、泛化能力差和采样效率低的问题，同时引入了变分推理和归一化流以准确预测驾驶轨迹，并提出了鲁棒性安全约束的制定方法。

    

    一个智能驾驶系统应该能够根据当前环境和车辆状态动态制定适当的驾驶策略，同时确保系统的安全性和可靠性。然而，基于强化学习和模仿学习的现有方法存在安全性低、泛化能力差和采样效率低的问题。此外，它们无法准确预测未来的驾驶轨迹，而准确预测未来的驾驶轨迹是做出最优决策的前提。为了解决这些问题，本文引入了一种复杂而多样场景下的安全且广义的端到端自主驾驶系统 (SGADS)。我们的SGADS与变分推理和归一化流结合，使智能车辆能够准确预测未来的驾驶轨迹。此外，我们提出了鲁棒性安全约束的制定。此外，我们将强化学习与示范相结合进行增强学习。

    An intelligent driving system should be capable of dynamically formulating appropriate driving strategies based on the current environment and vehicle status, while ensuring the security and reliability of the system. However, existing methods based on reinforcement learning and imitation learning suffer from low safety, poor generalization, and inefficient sampling. Additionally, they cannot accurately predict future driving trajectories, and the accurate prediction of future driving trajectories is a precondition for making optimal decisions. To solve these problems, in this paper, we introduce a Safe and Generalized end-to-end Autonomous Driving System (SGADS) for complex and various scenarios. Our SGADS incorporates variational inference with normalizing flows, enabling the intelligent vehicle to accurately predict future driving trajectories. Moreover, we propose the formulation of robust safety constraints. Furthermore, we combine reinforcement learning with demonstrations to aug
    
[^181]: LDReg: 本地维度正则化的自监督学习

    LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])

    [http://arxiv.org/abs/2401.10474](http://arxiv.org/abs/2401.10474)

    本文提出了一种叫做LDReg的本地维度正则化方法，用于解决自监督学习中的维度坍缩问题。通过增加局部内在维度，LDReg能够改善表示的性能。

    

    通过自监督学习（SSL）学习的表示可能容易出现维度坍缩，其中学习的表示子空间维度极低，因此无法表示完整的数据分布和模态。维度坍缩也被称为“填充不足”现象，是下游任务性能下降的主要原因之一。之前的工作在全局层面上研究了SSL的维度坍缩问题。在本文中，我们证明表示可以在全局上覆盖高维空间，但在局部上会坍缩。为了解决这个问题，我们提出了一种称为“本地维度正则化（LDReg）”的方法。我们的公式是基于Fisher-Rao度量的推导，用于比较和优化每个数据点在渐进小半径处的局部距离分布。通过增加局部内在维度，我们通过一系列实验证明LDReg可以改善表示。

    Representations learned via self-supervised learning (SSL) can be susceptible to dimensional collapse, where the learned representation subspace is of extremely low dimensionality and thus fails to represent the full data distribution and modalities. Dimensional collapse also known as the "underfilling" phenomenon is one of the major causes of degraded performance on downstream tasks. Previous work has investigated the dimensional collapse problem of SSL at a global level. In this paper, we demonstrate that representations can span over high dimensional space globally, but collapse locally. To address this, we propose a method called $\textit{local dimensionality regularization (LDReg)}$. Our formulation is based on the derivation of the Fisher-Rao metric to compare and optimize local distance distributions at an asymptotically small radius for each data point. By increasing the local intrinsic dimensionality, we demonstrate through a range of experiments that LDReg improves the repres
    
[^182]: 最具区分性的刺激物用于功能细胞类型的识别

    Most discriminative stimuli for functional cell type identification. (arXiv:2401.05342v1 [q-bio.NC])

    [http://arxiv.org/abs/2401.05342](http://arxiv.org/abs/2401.05342)

    本文提出了一种使用最具区分性刺激物的优化聚类方法，成功地识别了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能细胞类型。

    

    识别细胞类型并理解其功能特性对揭示感知和认知机制至关重要。在视网膜中，可以通过精心选择的刺激物来识别功能类型，但这需要专业领域知识，并会对以前已知的细胞类型产生偏见。在视觉皮层中，仍然不知道存在什么功能类型以及如何识别它们。因此，需要新的方法来对视网膜和视觉皮层中的功能细胞类型进行无偏见的识别。在这里，我们提出了一种基于优化的聚类方法，使用最具区分性的刺激物（MDS）来获得神经元的功能聚类。我们的方法通过刺激物的优化和聚类重新分配之间的交替进行，类似于期望最大化算法。该算法成功恢复了小鼠视网膜、恒河猴视网膜和猕猴V4视觉区的功能聚类。这证明了我们的方法可以成功地进行功能细胞类型的识别。

    Identifying cell types and understanding their functional properties is crucial for unraveling the mechanisms underlying perception and cognition. In the retina, functional types can be identified by carefully selected stimuli, but this requires expert domain knowledge and biases the procedure towards previously known cell types. In the visual cortex, it is still unknown what functional types exist and how to identify them. Thus, for unbiased identification of the functional cell types in retina and visual cortex, new approaches are needed. Here we propose an optimization-based clustering approach using deep predictive models to obtain functional clusters of neurons using Most Discriminative Stimuli (MDS). Our approach alternates between stimulus optimization with cluster reassignment akin to an expectation-maximization algorithm. The algorithm recovers functional clusters in mouse retina, marmoset retina and macaque visual area V4. This demonstrates that our approach can successfully 
    
[^183]: 使用迁移学习和时空特征构建高效的比特率梯度

    Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features. (arXiv:2401.03195v1 [cs.MM])

    [http://arxiv.org/abs/2401.03195](http://arxiv.org/abs/2401.03195)

    我们提出了一种使用迁移学习和时空特征的比特率和复杂度高效预测方法，通过利用预训练深度神经网络的特征图和预测的最小比特率来改进比特率梯度的效率，从而在视频行业中提供高质量视频并保持比特率的效率。

    

    在视频行业里，提供高质量的视频并保持比特率的效率是一个主要的挑战。传统的一刀切比特率梯度方案效率低下，并且由于需要进行大量的编码，使得计算最佳内容感知决策成为不现实。为了缓解这个问题，我们提出了一种使用迁移学习和时空特征的比特率和复杂度高效预测方法。我们提出了以下方法：（1）使用来自知名预训练深度神经网络的特征图来预测比特率质量行为，并限制训练数据的数量；（2）通过预测顶部质量的最小比特率来改进最高质量的梯度效率，并将其应用于顶部step。该方法在102个视频场景上进行了测试，结果显示，在1.71%的BD-Rate开销下，与蛮力方法相比，复杂度降低了94.1%。此外，我们还通过四个网络和消融研究深入研究了迁移学习。

    Providing high-quality video with efficient bitrate is a main challenge in video industry. The traditional one-size-fits-all scheme for bitrate ladders is inefficient and reaching the best content-aware decision computationally impractical due to extensive encodings required. To mitigate this, we propose a bitrate and complexity efficient bitrate ladder prediction method using transfer learning and spatio-temporal features. We propose: (1) using feature maps from well-known pre-trained DNNs to predict rate-quality behavior with limited training data; and (2) improving highest quality rung efficiency by predicting minimum bitrate for top quality and using it for the top rung. The method tested on 102 video scenes demonstrates 94.1% reduction in complexity versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning was thoroughly studied through four networks and ablation studies.
    
[^184]: 零坐标移动：针对物理约束操作学习的优化自动微分方法

    Zero Coordinate Shift: Whetted Automatic Differentiation for Physics-informed Operator Learning. (arXiv:2311.00860v1 [cs.LG])

    [http://arxiv.org/abs/2311.00860](http://arxiv.org/abs/2311.00860)

    本文提出了一种用于物理约束操作学习的新型自动微分算法，通过零坐标移动（ZCS）的技巧，将所需导数的复杂度从“多根多叶”简化为“一根多叶”，从而显著提高了性能。

    

    自动微分（AD）是物理约束机器学习中的关键步骤，用于计算网络输出相对于坐标的高阶导数。本文提出了一种新颖且轻量级的算法，用于进行针对物理约束操作学习的自动微分，称为零坐标移动（ZCS）的技巧。ZCS引入了一个标量值的叶变量，用于每个空间或时间维度，通过将所需导数从“多根多叶”简化为“一根多叶”，从而实现了性能的巨大提升。ZCS很容易在当前的深度学习库中实现；我们使用DeepXDE软件包进行了自己的实现。我们进行了全面的基准分析和多个案例研究，训练物理约束的DeepONets来解决无数据的偏微分方程（PDE）。结果表明，ZCS一直通过降低GPU内存消耗提供了改进效果。

    Automatic differentiation (AD) is a critical step in physics-informed machine learning, required for computing the high-order derivatives of network output w.r.t. coordinates. In this paper, we present a novel and lightweight algorithm to conduct such AD for physics-informed operator learning, as we call the trick of Zero Coordinate Shift (ZCS). Instead of making all sampled coordinates leaf variables, ZCS introduces only one scalar-valued leaf variable for each spatial or temporal dimension, leading to a game-changing performance leap by simplifying the wanted derivatives from "many-roots-many-leaves" to "one-root-many-leaves". ZCS is easy to implement with current deep learning libraries; our own implementation is by extending the DeepXDE package. We carry out a comprehensive benchmark analysis and several case studies, training physics-informed DeepONets to solve partial differential equations (PDEs) without data. The results show that ZCS has persistently brought down GPU memory co
    
[^185]: 强化微调语言模型中的梯度消失问题

    Vanishing Gradients in Reinforcement Finetuning of Language Models. (arXiv:2310.20703v1 [cs.LG])

    [http://arxiv.org/abs/2310.20703](http://arxiv.org/abs/2310.20703)

    本研究发现在强化微调（RFT）中存在梯度消失的问题，当模型下奖励的标准差较小时，输入的期望梯度会消失，导致奖励最大化缓慢。初始监督微调（SFT）阶段是克服这个问题的最有希望的方法。

    

    预训练的语言模型通过强化微调（RFT）与人类偏好和下游任务对齐，即使用策略梯度算法最大化（可能是学习得到的）奖励函数。本研究发现了RFT中的一个基本的优化障碍：我们证明了当模型下的奖励标准差较小时，输入的期望梯度会消失，即使期望奖励远离最优解。通过在RFT基准和控制环境中进行实验，以及理论分析，我们证明了由于小的奖励标准差导致的梯度消失问题普遍存在且有害，导致奖励最大化极其缓慢。最后，我们探索了克服RFT中梯度消失的方法。我们发现初始监督微调（SFT）阶段是最有希望的候选方法，并且揭示了它在RFT流程中的重要性。此外，我们还表明相对较小的训练数据集的SFT阶段可以有效克服梯度消失问题。

    Pretrained language models are commonly aligned with human preferences and downstream tasks via reinforcement finetuning (RFT), which entails maximizing a (possibly learned) reward function using policy gradient algorithms. This work highlights a fundamental optimization obstacle in RFT: we prove that the expected gradient for an input vanishes when its reward standard deviation under the model is small, even if the expected reward is far from optimal. Through experiments on an RFT benchmark and controlled environments, as well as a theoretical analysis, we then demonstrate that vanishing gradients due to small reward standard deviation are prevalent and detrimental, leading to extremely slow reward maximization. Lastly, we explore ways to overcome vanishing gradients in RFT. We find the common practice of an initial supervised finetuning (SFT) phase to be the most promising candidate, which sheds light on its importance in an RFT pipeline. Moreover, we show that a relatively small num
    
[^186]: 脑解码：走向实时重建视觉知觉

    Brain decoding: toward real-time reconstruction of visual perception. (arXiv:2310.19812v1 [eess.IV])

    [http://arxiv.org/abs/2310.19812](http://arxiv.org/abs/2310.19812)

    本研究提出了一种基于脑磁图（MEG）的脑解码方法，通过训练一个具有预训练嵌入、MEG模块和图像生成器的模型，在实时应用中实现了对视觉知觉的高时间分辨率解码，并在图像检索上取得了7倍的改进。

    

    在过去的五年中，生成式和基础性人工智能系统的使用极大地提高了对大脑活动的解码能力。特别是对于视觉知觉，现在可以从功能性磁共振成像（fMRI）中解码出令人瞩目的准确度。然而，这种神经影像技术的时间分辨率有限（约为0.5 Hz），因此在实时应用方面存在根本性的限制。在这里，我们提出了一种基于脑磁图（MEG）的替代方法，这是一种能够以高时间分辨率（约为5000 Hz）测量脑活动的神经影像设备。为此，我们开发了一个MEG解码模型，该模型通过对比和回归目标进行训练，并由三个模块组成：i）从图像中获得的预训练嵌入、ii）端到端训练的MEG模块以及iii）预训练的图像生成器。我们的结果有三个方面：首先，我们的MEG解码器在经典线性解码器上显示出7倍的图像检索改进。其次，后期脑部

    In the past five years, the use of generative and foundational AI systems has greatly improved the decoding of brain activity. Visual perception, in particular, can now be decoded from functional Magnetic Resonance Imaging (fMRI) with remarkable fidelity. This neuroimaging technique, however, suffers from a limited temporal resolution ($\approx$0.5 Hz) and thus fundamentally constrains its real-time usage. Here, we propose an alternative approach based on magnetoencephalography (MEG), a neuroimaging device capable of measuring brain activity with high temporal resolution ($\approx$5,000 Hz). For this, we develop an MEG decoding model trained with both contrastive and regression objectives and consisting of three modules: i) pretrained embeddings obtained from the image, ii) an MEG module trained end-to-end and iii) a pretrained image generator. Our results are threefold: Firstly, our MEG decoder shows a 7X improvement of image-retrieval over classic linear decoders. Second, late brain 
    
[^187]: Davidsonian场景图：改进文本-图像生成的细粒度评估的可靠性

    Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])

    [http://arxiv.org/abs/2310.18235](http://arxiv.org/abs/2310.18235)

    本论文提出了Davidsonian场景图（DSG）的评估框架，解决了现有文本-图像生成模型评估中的可靠性挑战，包括QG问题的准确性和VQA答案的一致性。

    

    评估文本到图像模型一直是困难的。最近一种用于评估文本-图像忠实度的强大方法是基于QG/A（问题生成和回答），它使用预训练的基础模型自动生成一组问题和答案，并基于这些答案与基于提示的答案在视觉问题回答模型中提取的一致性对输出图像进行评分。这种评估自然上取决于底层QG和QA模型的质量。我们确定并解决了现有QG/A工作中的几个可靠性挑战：（a）QG问题应尊重提示（避免幻觉、重复和遗漏）和（b）VQA答案应一致（不会在图像中宣称没有摩托车，同时声称摩托车是蓝色）。我们通过Davidsonian场景图（DSG），这个受形式语义启发的实证评估框架，解决了这些问题。

    Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
    
[^188]: 生成性扩散模型的统计热力学

    The statistical thermodynamics of generative diffusion models. (arXiv:2310.17467v1 [stat.ML])

    [http://arxiv.org/abs/2310.17467](http://arxiv.org/abs/2310.17467)

    本文通过在生成性扩散模型中应用平衡统计力学的工具，揭示了这些模型中的二阶相变现象，并且认为这种稳定性形式是生成能力的关键。

    

    生成性扩散模型在生成建模的许多领域取得了惊人的表现。虽然这些模型的基本思想来自非平衡物理学，但本文中我们表明，可以用平衡统计力学的工具来理解这些模型的许多方面。利用这种重构，我们展示了生成性扩散模型经历了与对称性破缺现象相对应的二阶相变。我们认为，这导致了一种稳定性形式，它是生成能力的核心，并可以用一组平均场临界指数来描述。最后，我们根据热力学的公式分析了将扩散模型与关联记忆网络连接的最近研究。

    Generative diffusion models have achieved spectacular performance in many areas of generative modeling. While the fundamental ideas behind these models come from non-equilibrium physics, in this paper we show that many aspects of these models can be understood using the tools of equilibrium statistical mechanics. Using this reformulation, we show that generative diffusion models undergo second-order phase transitions corresponding to symmetry breaking phenomena. We argue that this lead to a form of instability that lies at the heart of their generative capabilities and that can be described by a set of mean field critical exponents. We conclude by analyzing recent work connecting diffusion models and associative memory networks in view of the thermodynamic formulations.
    
[^189]: 稳定的非凸-非凹训练通过线性插值

    Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])

    [http://arxiv.org/abs/2310.13459](http://arxiv.org/abs/2310.13459)

    本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。

    

    本文提出了一种关于线性插值的理论分析，作为一种稳定（大规模）神经网络训练的方法。我们认为优化过程中的不稳定性通常是由损失函数的非单调性引起的，并展示了线性插值如何通过利用“非扩张算子”的理论来帮助解决这个问题。我们构建了一种新的优化方案，称为松弛近似近端点（RAPP），这是第一个明确的方法，能够实现完整范围内的协调部分单调问题的最后迭代收敛速率。该构造可扩展到约束和正则化设置。通过替换RAPP中的内部优化器，我们重新发现了Lookahead算法族，我们证明了这些算法在协调部分单调问题中的收敛性，即使基本优化器采用梯度下降升级算法。通过利用Lookahead继承性质，我们进一步扩展了Lookahead在协调部分单调问题中收敛的范围。

    This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
    
[^190]: CacheGen：用于语言模型应用的快速上下文加载

    CacheGen: Fast Context Loading for Language Model Applications. (arXiv:2310.07240v1 [cs.NI])

    [http://arxiv.org/abs/2310.07240](http://arxiv.org/abs/2310.07240)

    CacheGen是一种用于语言模型应用的技术，通过对上下文进行压缩来减少LLM的网络获取和处理延迟。

    

    随着大型语言模型（LLM）承担越来越复杂的任务，其输入将整合更长的上下文，以应对需要领域知识或用户特定的对话历史的问题。然而，使用长上下文对于响应式的LLM系统来说是一个挑战，因为在所有上下文被获取和LLM处理之前，无法生成任何内容。现有系统仅通过优化上下文处理的计算延迟（例如，通过缓存文本上下文的中间键值特征）来解决问题，但往往会导致上下文获取的网络延迟更长（例如，键值特征消耗的带宽比文本上下文大几个数量级）。本文介绍了CacheGen，以最小化LLM上下文获取和处理的延迟。CacheGen通过将长上下文的键值（KV）特征压缩为更紧凑的比特流表示，减少了传输所需的带宽。编码器结合了自适应量化和......

    As large language models (LLMs) take on more complex tasks, their inputs incorporate longer contexts to respond to questions that require domain knowledge or user-specific conversational histories. Yet, using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until all the contexts are fetched to and processed by the LLM. Existing systems optimize only the computation delay in context processing (e.g., by caching intermediate key-value features of the text context) but often cause longer network delays in context fetching (e.g., key-value features consume orders of magnitude larger bandwidth than the text context).  This paper presents CacheGen to minimize the delays in fetching and processing contexts for LLMs. CacheGen reduces the bandwidth needed for transmitting long contexts' key-value (KV) features through a novel encoder that compresses KV features into more compact bitstream representations. The encoder combines adaptive quantization with a 
    
[^191]: 具有无误差的可微分交换函数的广义神经排序网络

    Generalized Neural Sorting Networks with Error-Free Differentiable Swap Functions. (arXiv:2310.07174v1 [cs.LG])

    [http://arxiv.org/abs/2310.07174](http://arxiv.org/abs/2310.07174)

    本文提出了一种广义神经排序网络，其中采用了具有无误差且可微分的交换函数，同时使用了置换等变Transformer网络来捕捉输入之间的依赖关系。实验证明，该方法在各种排序基准上表现优于或与基准方法相当。

    

    排序是所有计算机系统的基本操作，一直是一个长期的重要研究课题。除了传统排序算法的问题表述，我们通过神经排序网络考虑了更抽象但具有表达力的输入，例如多位数字图像和图像片段。为了学习从高维输入到次序变量的映射，需要保证排序网络的可微分性。在本文中，我们通过可微分的交换函数定义一个柔化误差，并开发了一个无误差的交换函数，该函数满足非减和可微分的条件。此外，采用了具有多头注意力机制的置换等变Transformer网络，以捕捉给定输入之间的依赖关系，并利用其自注意力的模型能力。在多样的排序基准上进行的实验证明，我们的方法优于或与基准方法相当。

    Sorting is a fundamental operation of all computer systems, having been a long-standing significant research topic. Beyond the problem formulation of traditional sorting algorithms, we consider sorting problems for more abstract yet expressive inputs, e.g., multi-digit images and image fragments, through a neural sorting network. To learn a mapping from a high-dimensional input to an ordinal variable, the differentiability of sorting networks needs to be guaranteed. In this paper we define a softening error by a differentiable swap function, and develop an error-free swap function that holds non-decreasing and differentiability conditions. Furthermore, a permutation-equivariant Transformer network with multi-head attention is adopted to capture dependency between given inputs and also leverage its model capacity with self-attention. Experiments on diverse sorting benchmarks show that our methods perform better than or comparable to baseline methods.
    
[^192]: 在大规模语言模型中摊销难以处理的推理问题

    Amortizing intractable inference in large language models. (arXiv:2310.04363v1 [cs.LG])

    [http://arxiv.org/abs/2310.04363](http://arxiv.org/abs/2310.04363)

    本论文提出了一种使用摊销的贝叶斯推理从难以处理的后验分布中进行抽样的方法，并利用生成流网络来实现大规模语言模型的微调，从而解决了在这些模型中处理推理问题的限制。

    

    自回归的大规模语言模型通过下一个词条件分布来压缩其训练数据中的知识，这限制了对该知识的可处理查询仅限于从头到尾的自回归抽样。然而，许多感兴趣的任务，包括序列延续、填充和其他形式的受约束生成，都涉及从难以处理的后验分布中进行抽样。我们通过使用摊销的贝叶斯推理从这些难以处理的后验分布中进行抽样来解决这个限制。这种摊销通过通过寻求多样性的强化学习算法 - 生成流网络 (GFlowNets) 来微调 LLMs 实现。我们凭经验证明，LLM微调的这种分布匹配范式可以作为最大似然训练和奖励最大化策略优化的有效替代方法。作为一个重要应用，我们将思维链推理解释为潜变量建模问题，并证明了...

    Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest -- including sequence continuation, infilling, and other forms of constrained generation -- involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate 
    
[^193]: 使用Leave-One-Out最大对数似然目标稳定训练概率模型

    Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective. (arXiv:2310.03556v1 [stat.ML])

    [http://arxiv.org/abs/2310.03556](http://arxiv.org/abs/2310.03556)

    本文介绍了一种使用Leave-One-Out最大对数似然目标稳定训练概率模型的方法，通过自适应核密度估计模型和留一法最大对数似然准则，解决了数据密度不均匀困难，并通过分配可学习权重扩展模型，加速了训练过程。

    

    电力系统运行和规划过程的概率建模依赖于数据驱动方法，这需要足够大的数据集。当历史数据不足时，希望将潜在的数据生成机制建模为概率分布，以评估数据质量并生成更多数据。基于核密度估计（KDE）的模型是这一任务的常用选择，但它们无法适应密度不均匀的数据区域。在本文中，采用自适应KDE模型来解决这个问题，模型中的每个核函数具有独立的带宽。提出了一种留一法最大对数似然（LOO-MLL）准则，以防止常规的最大对数似然准则产生奇异解，并证明LOO-MLL可以防止这种情况。在此保证的鲁棒性基础上，通过为核函数分配可学习权重扩展了模型。此外，使用改进的期望最大化算法来加速训练过程。

    Probabilistic modelling of power systems operation and planning processes depends on data-driven methods, which require sufficiently large datasets. When historical data lacks this, it is desired to model the underlying data generation mechanism as a probability distribution to assess the data quality and generate more data, if needed. Kernel density estimation (KDE) based models are popular choices for this task, but they fail to adapt to data regions with varying densities. In this paper, an adaptive KDE model is employed to circumvent this, where each kernel in the model has an individual bandwidth. The leave-one-out maximum log-likelihood (LOO-MLL) criterion is proposed to prevent the singular solutions that the regular MLL criterion gives rise to, and it is proven that LOO-MLL prevents these. Relying on this guaranteed robustness, the model is extended by assigning learnable weights to the kernels. In addition, a modified expectation-maximization algorithm is employed to accelerat
    
[^194]: 随机环境中的预期流网络和双人零和游戏

    Expected flow networks in stochastic environments and two-player zero-sum games. (arXiv:2310.02779v1 [cs.LG])

    [http://arxiv.org/abs/2310.02779](http://arxiv.org/abs/2310.02779)

    该论文提出了预期流网络（EFlowNets）和对抗流网络（AFlowNets），分别应用于随机环境和双人零和游戏中。在随机任务中，EFlowNets表现优于其他方法，在双人游戏中，AFlowNets在自我对弈中找到了80%以上的最佳动作，并在竞赛中超过了AlphaZero。

    

    生成流网络（GFlowNets）是训练用于匹配给定分布的序列采样模型。GFlowNets已成功应用于各种结构对象生成任务，能够迅速采样出多样化且高回报的对象。我们提出了预期流网络（EFlowNets），将GFlowNets扩展到随机环境中。我们展示了EFlowNets在随机任务（如蛋白质设计）中优于其他GFlowNet方案。然后，我们将EFlowNets的概念扩展到对抗环境中，为双人零和游戏提出了对抗流网络（AFlowNets）。我们展示了AFlowNets通过自我对弈在Connect-4游戏中能找到80%以上的最佳动作，并在竞赛中优于AlphaZero。

    Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments.
    
[^195]: Delta-AI: 稀疏图模型的摊还推理中的局部目标

    Delta-AI: Local objectives for amortized inference in sparse graphical models. (arXiv:2310.02423v1 [cs.LG])

    [http://arxiv.org/abs/2310.02423](http://arxiv.org/abs/2310.02423)

    Delta-AI算法提出了一种基于稀疏图模型的摊还推理方法，通过局部信用分配和离策略训练加快了训练速度。

    

    我们提出了一种新的算法，用于稀疏概率图模型（PGMs）的摊还推理，我们称之为Delta-AI。我们的方法基于这样的观察：当PGM中的变量采样被视为一个代理人采取的动作序列时，PGM的稀疏性使得代理人的策略学习目标能够进行局部信用分配。这导致了一个局部约束，可以转化为类似生成流网络（GFlowNets）中的局部损失，从而实现了离策略训练，但避免了每个参数更新需要实例化所有随机变量的需求，从而大大加快了训练速度。Delta-AI目标与一个可计算的学习采样器中的变量给定其马尔可夫毯子的条件分布相匹配，该采样器的结构类似于贝叶斯网络，在目标PGM下具有相同的条件分布。因此，训练后的采样器可以恢复感兴趣变量的边际分布和条件分布。

    We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call $\Delta$-amortized inference ($\Delta$-AI). Our approach is based on the observation that when the sampling of variables in a PGM is seen as a sequence of actions taken by an agent, sparsity of the PGM enables local credit assignment in the agent's policy learning objective. This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update, thus speeding up training considerably. The $\Delta$-AI objective matches the conditional distribution of a variable given its Markov blanket in a tractable learned sampler, which has the structure of a Bayesian network, with the same conditional distribution under the target PGM. As such, the trained sampler recovers marginals and conditional distributions of intere
    
[^196]: GNNX-BENCH: 通过深度基准测试揭示基于扰动的GNN解释器的实用性

    GNNX-BENCH: Unravelling the Utility of Perturbation-based GNN Explainers through In-depth Benchmarking. (arXiv:2310.01794v1 [cs.LG])

    [http://arxiv.org/abs/2310.01794](http://arxiv.org/abs/2310.01794)

    本研究通过基准测试系统评估了基于扰动的GNN解释性方法，发现帕累托最优方法在噪声存在的情况下表现出卓越效力和稳定性。

    

    已经提出了许多解释性方法来揭示GNN的内部工作方式。尽管所有提出的算法都包含了实证评估，但这些评估的询问方面缺乏多样性。因此，关于GNN解释性的各个方面，如对事实求证推理器的比较分析、它们对不同GNN架构、噪声、非凸损失表面中的随机性、在领域约束条件下的可行性等等，尚未得到正式的研究。受此需求的激发，我们在基于扰动的GNN解释性方法上进行了基准测试研究，旨在系统评估和比较各种解释性技术。在我们的研究的关键发现中，我们确定了在噪声存在的情况下表现出卓越效力和稳定性的帕累托最优方法。然而，我们的研究揭示了所有算法都受到稳定性的影响。

    Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stabi
    
[^197]: 使用GPT-4的图神经网络架构搜索

    Graph Neural Architecture Search with GPT-4. (arXiv:2310.01436v1 [cs.LG])

    [http://arxiv.org/abs/2310.01436](http://arxiv.org/abs/2310.01436)

    本文将GPT-4集成到图神经网络架构搜索（GNAS）中，提出了一种新的GPT-4基于的GNAS方法（GPT4GNAS），通过设计新的提示来引导GPT-4生成更准确的图神经网络，实验证明嵌入GPT-4到GNAS中优于现有方法。

    

    图神经网络架构搜索（GNAS）在自动设计图神经网络方面取得了有希望的结果。然而，GNAS仍然需要大量的人工劳动和丰富的领域知识来设计搜索空间和搜索策略。本文将GPT-4集成到GNAS中，提出了一种基于GPT-4的图神经网络架构搜索方法（简称为GPT4GNAS）。我们的方法的基本思想是为GPT-4设计一类新的提示，以指导GPT-4进行图神经网络架构的生成任务。这些提示包括GNAS的搜索空间、搜索策略和搜索反馈的描述。通过迭代地运行具有提示的GPT-4，GPT4GNAS能够生成更准确的图神经网络，并快速收敛。实验结果表明，嵌入GPT-4到GNAS中优于现有最先进的GNAS方法。

    Graph Neural Architecture Search (GNAS) has shown promising results in automatically designing graph neural networks. However, GNAS still requires intensive human labor with rich domain knowledge to design the search space and search strategy. In this paper, we integrate GPT-4 into GNAS and propose a new GPT-4 based Graph Neural Architecture Search method (GPT4GNAS for short). The basic idea of our method is to design a new class of prompts for GPT-4 to guide GPT-4 toward the generative task of graph neural architectures. The prompts consist of descriptions of the search space, search strategy, and search feedback of GNAS. By iteratively running GPT-4 with the prompts, GPT4GNAS generates more accurate graph neural networks with fast convergence. Experimental results show that embedding GPT-4 into GNAS outperforms the state-of-the-art GNAS methods.
    
[^198]: DyVal: 基于图形信息的大型语言模型动态评估

    DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])

    [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167)

    DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。

    

    大型语言模型在各种评估基准中取得了显著的性能。然而，人们对它们的性能提出了担忧，因为它们庞大的训练语料库中可能存在数据污染。此外，当前基准的静态性质和固定复杂性可能无法充分衡量LLM的进步能力。在本文中，我们介绍了DyVal，一种新颖、通用且灵活的动态评估LLM的协议。基于我们提出的动态评估框架，我们利用有向无环图的结构优势构建了基于图形信息的DyVal，以动态生成具有可控复杂性的评估样本。DyVal生成了包括数学、逻辑推理和算法问题在内的具有挑战性的推理任务的评估集。我们评估了从Flan-T5-large到ChatGPT和GPT4的各种LLM。实验表明，LLM在DyVal生成的评估样本上表现更差

    Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
    
[^199]: Era Splitting.（arXiv:2309.14496v1 [cs.LG]）

    Era Splitting. (arXiv:2309.14496v1 [cs.LG])

    [http://arxiv.org/abs/2309.14496](http://arxiv.org/abs/2309.14496)

    本研究提出了两种新的分裂准则，使得决策树模型能够利用时代信息进行优化，从而将超分布泛化研究中的思想应用于决策树模型。

    

    现实生活中的机器学习问题在时间和空间上会呈现出数据的分布变化。这种行为超出了传统的经验风险最小化范式的范围，该范式假设数据在时间和地点上是独立同分布的。新兴的超分布泛化领域通过将环境或时代信息融入算法中，来应对这个现实。迄今为止，大部分研究都集中在线性模型和/或神经网络上。在本研究中，我们针对决策树模型，包括随机森林和梯度提升决策树，开发了两种新的分裂准则，使得树模型能够利用与每个数据点相关的时代信息，来找到在数据的所有不相交时代中都是最优的切分点，从而将超分布泛化研究中的思想应用于决策树模型。

    Real life machine learning problems exhibit distributional shifts in the data from one time to another or from on place to another. This behavior is beyond the scope of the traditional empirical risk minimization paradigm, which assumes i.i.d. distribution of data over time and across locations. The emerging field of out-of-distribution (OOD) generalization addresses this reality with new theory and algorithms which incorporate environmental, or era-wise information into the algorithms. So far, most research has been focused on linear models and/or neural networks. In this research we develop two new splitting criteria for decision trees, which allow us to apply ideas from OOD generalization research to decision tree models, including random forest and gradient-boosting decision trees. The new splitting criteria use era-wise information associated with each data point to allow tree-based models to find split points that are optimal across all disjoint eras in the data, instead of optim
    
[^200]: K-pop歌词翻译：数据集、分析与神经建模

    K-pop Lyric Translation: Dataset, Analysis, and Neural-Modelling. (arXiv:2309.11093v1 [cs.CL])

    [http://arxiv.org/abs/2309.11093](http://arxiv.org/abs/2309.11093)

    研究者介绍了一种新颖的K-pop歌词翻译数据集，该数据集揭示了K-pop歌词翻译的独特特征，并构建了一个神经歌词翻译模型，强调了专用数据集的重要性。

    

    歌词翻译作为一个研究了一个世纪的领域，如今吸引着计算语言学研究者的注意。我们在以往研究中发现了两个限制。首先，在歌词翻译研究中，尽管K-pop非常受欢迎，但主要关注的是西方流派和语言，没有研究集中在K-pop上。其次，歌词翻译领域缺乏可公开获得的数据集；据我们所知，目前尚无此类数据集。为了拓宽歌词翻译研究的流派和语言范围，我们引入了一种新颖的可唱歌词翻译数据集，其中约89%为K-pop歌词。该数据集通过逐行和逐节对齐了韩语和英语歌词。我们利用该数据集揭示了K-pop歌词翻译的独特特征，与其他广泛研究的流派区分开，并构建了一个神经歌词翻译模型，从而强调了专用数据集的重要性。

    Lyric translation, a field studied for over a century, is now attracting computational linguistics researchers. We identified two limitations in previous studies. Firstly, lyric translation studies have predominantly focused on Western genres and languages, with no previous study centering on K-pop despite its popularity. Second, the field of lyric translation suffers from a lack of publicly available datasets; to the best of our knowledge, no such dataset exists. To broaden the scope of genres and languages in lyric translation studies, we introduce a novel singable lyric translation dataset, approximately 89\% of which consists of K-pop song lyrics. This dataset aligns Korean and English lyrics line-by-line and section-by-section. We leveraged this dataset to unveil unique characteristics of K-pop lyric translation, distinguishing it from other extensively studied genres, and to construct a neural lyric translation model, thereby underscoring the importance of a dedicated dataset for
    
[^201]: 深度学习网络的几何结构和全局${\mathcal L}^2$最小化器的构建

    Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])

    [http://arxiv.org/abs/2309.10639](http://arxiv.org/abs/2309.10639)

    本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。

    

    本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\geq1$的输入和输出空间${\mathbb R}^Q$。隐藏层也定义在${\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。

    In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
    
[^202]: 有向加权图的最优输运距离：以细胞间通讯网络为案例研究

    Optimal transport distances for directed, weighted graphs: a case study with cell-cell communication networks. (arXiv:2309.07030v1 [cs.LG])

    [http://arxiv.org/abs/2309.07030](http://arxiv.org/abs/2309.07030)

    本文提出了两种基于最优输运的距离度量，用于比较有向图，并通过仿真图数据和单细胞RNA-seq数据推断的实际细胞间通讯图对其相对表现进行了评估。

    

    近年来，比较最优输运图引起了相当大的关注，因为最优输运引起的距离既提供了图之间的合理度量，又通过输运计划的可解释描述了图之间相关变化。然而，由于缺乏对称性，通常考虑的最优输运距离主要用于无向图。本文提出了两种基于最优输运变体的距离度量来比较有向图：（i）地球移动距离（Wasserstein）和（ii）Gromov-Wasserstein（GW）距离。我们评估了这两种距离，并讨论了它们在仿真图数据和基于单细胞RNA-seq数据推断的实际有向细胞间通讯图上的相对表现。

    Comparing graphs of optimal transport has recently gained significant attention, as the distances induced by optimal transport provide both a principled metric between graphs as well as an interpretable description of the associated changes between graphs in terms of a transport plan. As the lack of symmetry introduces challenges in the typically considered formulations, optimal transport distances for graphs have mostly been developed for undirected graphs. Here, we propose two distance measures to compare directed graphs based on variants of optimal transport: (i) an earth movers distance (Wasserstein) and (ii) a Gromov-Wasserstein (GW) distance. We evaluate these two distances and discuss their relative performance for both simulated graph data and real-world directed cell-cell communication graphs, inferred from single-cell RNA-seq data.
    
[^203]: ConR: 用于深度不平衡回归的对比正则化器

    ConR: Contrastive Regularizer for Deep Imbalanced Regression. (arXiv:2309.06651v1 [cs.LG])

    [http://arxiv.org/abs/2309.06651](http://arxiv.org/abs/2309.06651)

    ConR是一种对比正则化器，通过建模全局和局部标签相似性，防止少数样本的特征被折叠到其多数邻居中，有效地处理深度不平衡回归问题。

    

    不平衡分布在现实世界的数据中很常见。它们对深度神经网络提出了约束，以表示少数类别标签并避免对多数类别的偏见。大量的不平衡方法处理了分类标签空间，但在连续标签空间的回归问题上未能有效应用。相反，连续标签之间的局部和全局关联为在特征空间中有效建模关系提供了有价值的见解。在这项工作中，我们提出了ConR，一种对比正则化器，它在特征空间中建模全局和局部标签相似性，防止少数样本的特征被折叠到它们的多数邻居中。通过将预测的相似性作为特征相似性的指示器，ConR区分了标签空间和特征空间之间的不一致，并对这些不一致施加惩罚。ConR通过两个主要策略关注标签空间的连续性。

    Imbalanced distributions are ubiquitous in real-world data. They create constraints on Deep Neural Networks to represent the minority labels and avoid bias towards majority labels. The extensive body of imbalanced approaches address categorical label spaces but fail to effectively extend to regression problems where the label space is continuous. Conversely, local and global correlations among continuous labels provide valuable insights towards effectively modelling relationships in feature space. In this work, we propose ConR, a contrastive regularizer that models global and local label similarities in feature space and prevents the features of minority samples from being collapsed into their majority neighbours. Serving the similarities of the predictions as an indicator of feature similarities, ConR discerns the dissagreements between the label space and feature space and imposes a penalty on these disagreements. ConR minds the continuous nature of label space with two main strategi
    
[^204]: EGIC:增强的低位速率生成图像压缩方法在语义分割的指导下

    EGIC: Enhanced Low-Bit-Rate Generative Image Compression Guided by Semantic Segmentation. (arXiv:2309.03244v1 [eess.IV])

    [http://arxiv.org/abs/2309.03244](http://arxiv.org/abs/2309.03244)

    EGIC是一种增强的低位速率生成图像压缩方法，通过语义分割提供指导。它在失真感知和失真方向基线方法上表现优越，并具有较小的模型参数和优秀的插值特性。

    

    我们引入了一种新颖的生成图像压缩方法EGIC，它允许从一个单一模型有效地遍历失真感知曲线。具体而言，我们提出了一种隐式编码的图像插值变体，用于预测在MSE优化和GAN优化解码器输出之间的残差。在接收端，用户可以控制残差对基于GAN的重建的影响。结合改进的基于GAN的构建块，EGIC在感知导向和失真导向的基线方法（包括HiFiC，MRIC和DIRAC）上表现优于大多数方法，在失真端与VTM-20.0几乎相当。EGIC实现简单，非常轻量级（与HiFiC相比，模型参数只有0.18倍），并提供优异的插值特性，这使得它成为针对低位范围的实际应用的有希望的候选方法。

    We introduce EGIC, a novel generative image compression method that allows traversing the distortion-perception curve efficiently from a single model. Specifically, we propose an implicitly encoded variant of image interpolation that predicts the residual between a MSE-optimized and GAN-optimized decoder output. On the receiver side, the user can then control the impact of the residual on the GAN-based reconstruction. Together with improved GAN-based building blocks, EGIC outperforms a wide-variety of perception-oriented and distortion-oriented baselines, including HiFiC, MRIC and DIRAC, while performing almost on par with VTM-20.0 on the distortion end. EGIC is simple to implement, very lightweight (e.g. 0.18x model parameters compared to HiFiC) and provides excellent interpolation characteristics, which makes it a promising candidate for practical applications targeting the low bit range.
    
[^205]: SLiMe: 像我一样进行分割

    SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])

    [http://arxiv.org/abs/2309.03179](http://arxiv.org/abs/2309.03179)

    基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。

    

    使用大型视觉语言模型（如稳定扩散SD），在诸多下游任务（包括图像编辑、图像对应和3D形状生成）方面取得了显著进展。受到这些进展的启发，我们探索利用这些广泛的视觉语言模型，通过提出SLiMe，以尽可能少的标注样本对图像进行任意细粒度的分割。SLiMe将这个问题作为一个优化任务来进行。具体而言，给定一张训练图像及其分割掩膜，我们首先从SD先验中提取注意力图，包括我们的新颖的“加权累积自注意力图”。然后，利用提取的注意力图，优化稳定扩散的文本嵌入，使得每个嵌入只学习训练图像中的一个分割区域。这些学习到的嵌入然后在注意力图中突出显示分割区域，从而可以生成分割图。这使得SLiMe可以进行图像分割。

    Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
    
[^206]: PROMISE: 通过引入可扩展曲率估计的预条件随机优化方法

    PROMISE: Preconditioned Stochastic Optimization Methods by Incorporating Scalable Curvature Estimates. (arXiv:2309.02014v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2309.02014](http://arxiv.org/abs/2309.02014)

    本文介绍了一套基于草图的预条件随机梯度算法套件PROMISE，用于解决大规模凸优化问题，相比传统方法，在默认超参数下在机器学习问题上表现更优。

    

    本文介绍了PROMISE（通过引入可扩展曲率估计的预条件随机优化方法），这是一套基于草图的预条件随机梯度算法套件，用于解决机器学习中出现的大规模凸优化问题。PROMISE包括SVRG、SAGA和Katyusha的预条件版本；每个算法都有强大的理论分析和有效的默认超参数值。相比之下，传统的随机梯度方法需要仔细调节超参数才能成功，在机器学习中普遍存在的病态条件下性能会下降。实验证明，通过使用默认超参数值，所提出的算法在由基准机器学习问题汇编的51个岭回归和逻辑回归问题上优于或与流行的调整后的随机梯度优化器相匹配。

    This paper introduces PROMISE ($\textbf{Pr}$econditioned Stochastic $\textbf{O}$ptimization $\textbf{M}$ethods by $\textbf{I}$ncorporating $\textbf{S}$calable Curvature $\textbf{E}$stimates), a suite of sketching-based preconditioned stochastic gradient algorithms for solving large-scale convex optimization problems arising in machine learning. PROMISE includes preconditioned versions of SVRG, SAGA, and Katyusha; each algorithm comes with a strong theoretical analysis and effective default hyperparameter values. In contrast, traditional stochastic gradient methods require careful hyperparameter tuning to succeed, and degrade in the presence of ill-conditioning, a ubiquitous phenomenon in machine learning. Empirically, we verify the superiority of the proposed algorithms by showing that, using default hyperparameter values, they outperform or match popular tuned stochastic gradient optimizers on a test bed of $51$ ridge and logistic regression problems assembled from benchmark machine l
    
[^207]: 大型语言模型的指令调优：一项调研

    Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10792](http://arxiv.org/abs/2308.10792)

    本文调查了指令调优这一关键技术在增强大型语言模型能力和可控性方面的研究工作，包括方法、数据集构建、模型训练和应用，以及对结果影响的分析。同时回顾了可能的问题和批评，并指出了目前的不足。

    

    本文调查了指令调优（IT）这一快速发展的领域中的研究工作，这是一种增强大型语言模型（LLM）能力和可控性的关键技术。指令调优是指以监督方式在包含“指令-输出”对的数据集上进一步训练LLM，这将LLM的下一个词预测目标与用户希望LLM遵守人类指令的目标之间的差距。本文对IT的常规方法、IT数据集的构建、IT模型的训练以及应用于不同模态、领域和应用的情况进行了系统的文献综述，并对影响IT结果的各个方面进行了分析（例如，指令输出的生成、指令数据集的大小等）。我们还回顾了IT的潜在问题以及针对其的批评，以及指出当前不足的努力。

    This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
    
[^208]: 分类问题中的精确度和召回率拒绝曲线

    Precision and Recall Reject Curves for Classification. (arXiv:2308.08381v1 [cs.LG])

    [http://arxiv.org/abs/2308.08381](http://arxiv.org/abs/2308.08381)

    该论文提出了一种在分类问题中评估精确度和召回率的拒绝曲线方法。使用感知量化的原型分类器来验证了该方法在不平衡数据集和医学实际数据上的有效性。

    

    在某些分类场景中，只使用模型高度确定的分类实例是可取的。为了获得这些高度确定的实例，先前的研究提出了准确度拒绝曲线。拒绝曲线允许评估和比较不同的确定度度量在接受或拒绝分类的一系列阈值上的性能。然而，准确度可能并不适合所有应用程序的评估指标，相反，精确度或召回率可能更可取。这是因为在存在类别分布不平衡的数据中，例如，在不平衡的基准数据和医学实际数据中，我们提出了评估精确度和召回率的拒绝曲线：召回率-拒绝曲线和精确度-拒绝曲线。通过学习向量量化中的原型分类器，我们首先在人工基准数据上将提出的曲线与准确度拒绝曲线作为基准进行验证。然后，我们在存在类别分布不平衡的基准数据和医学实际数据上进行展示。

    For some classification scenarios, it is desirable to use only those classification instances that a trained model associates with a high certainty. To obtain such high-certainty instances, previous work has proposed accuracy-reject curves. Reject curves allow to evaluate and compare the performance of different certainty measures over a range of thresholds for accepting or rejecting classifications. However, the accuracy may not be the most suited evaluation metric for all applications, and instead precision or recall may be preferable. This is the case, for example, for data with imbalanced class distributions. We therefore propose reject curves that evaluate precision and recall, the recall-reject curve and the precision-reject curve. Using prototype-based classifiers from learning vector quantization, we first validate the proposed curves on artificial benchmark data against the accuracy reject curve as a baseline. We then show on imbalanced benchmarks and medical, real-world data 
    
[^209]: 用于学习时间齐次动力系统的深度投影网络

    Deep projection networks for learning time-homogeneous dynamical systems. (arXiv:2307.09912v1 [cs.LG])

    [http://arxiv.org/abs/2307.09912](http://arxiv.org/abs/2307.09912)

    这篇论文介绍了一种利用深度投影网络学习时间齐次动力系统的有意义表示的方法。通过优化类似于经典相关分析的目标函数，避免了矩阵求逆的稳定性问题，并通过两种正则化方案进一步增强学习效果。

    

    我们考虑了一般的时间齐次动力系统，包括离散和连续的，并研究了从观测数据中学习状态的有意义表示的问题。这对于学习系统的前向传输算子至关重要，该算子可以用于预测未来的状态或可观测量。表示通常通过神经网络参数化，与投影算子相关联，并通过优化类似于经典相关分析（CCA）的目标函数来学习。然而，与CCA不同，我们的目标函数避免了矩阵求逆，因此通常更稳定且适用于具有挑战性的场景。我们的目标函数是CCA的一个紧松弛，我们进一步通过提出两种正则化方案来增强它，一种鼓励表示的分量正交，而另一种利用了 Chapman-Kolmogorov 方程。我们将我们的方法应用于具有挑战性的离散动力系统

    We consider the general class of time-homogeneous dynamical systems, both discrete and continuous, and study the problem of learning a meaningful representation of the state from observed data. This is instrumental for the task of learning a forward transfer operator of the system, that in turn can be used for forecasting future states or observables. The representation, typically parametrized via a neural network, is associated with a projection operator and is learned by optimizing an objective function akin to that of canonical correlation analysis (CCA). However, unlike CCA, our objective avoids matrix inversions and therefore is generally more stable and applicable to challenging scenarios. Our objective is a tight relaxation of CCA and we further enhance it by proposing two regularization schemes, one encouraging the orthogonality of the components of the representation while the other exploiting Chapman-Kolmogorov's equation. We apply our method to challenging discrete dynamical
    
[^210]: milliFlow：用于人体运动感知的毫米波雷达点云场景流估计

    milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])

    [http://arxiv.org/abs/2306.17010](http://arxiv.org/abs/2306.17010)

    milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。

    

    随着普适计算时代的到来，人体运动感知在智能系统中起着关键作用，用于决策、用户交互和个性化服务。在传统方法中，人体跟踪、姿势估计、手势识别和活动识别等方面进行了大量研究，这些方法主要基于摄像机。然而，摄像机的侵入性特点限制了它们在智能家居应用中的使用。为了解决这个问题，毫米波雷达由于其保护隐私的特点而受到欢迎。在这项工作中，我们提出了一种新颖的深度学习方法milliFlow，用于对毫米波雷达点云进行场景流估计，作为中间层的特征，直接受益于下游的人体运动感知任务。实验结果表明，我们的方法具有优越的性能，平均3D端点误差为4.6cm，明显超过竞争方法。此外，通过结合...

    Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
    
[^211]: 核化强化学习及其近似方法的优化

    Kernelized Reinforcement Learning with Order Optimal Regret Bounds. (arXiv:2306.07745v1 [cs.LG])

    [http://arxiv.org/abs/2306.07745](http://arxiv.org/abs/2306.07745)

    该论文提出了一种称为$\pi$-KRVI的乐观修改方法，并使用核岭回归进行强化学习中的非线性函数逼近。论文证明了在一般设置下第一个最优遗憾保证，并相对于现有最优结果实现了显着的多项式低差距。

    

    强化学习（RL）在各种具有复杂模型和大状态-行为空间的实际场景中显示出了实证的成功。但是，现有的分析结果通常集中于具有少量状态-行为或简单模型（例如线性建模状态-行为值函数）的设置。 为了推导有效处理更广泛值函数的大状态-行为空间的RL策略，一些最新工作考虑使用核岭回归进行非线性函数逼近。 我们提出了称为$\pi$-KRVI的方法，它是最小二乘值迭代的一种乐观修改，当状态-行为值函数由RKHS表示时。我们证明了在一般设置下第一个最优遗憾保证。我们的结果显示，在许多具有高度非光滑内核（例如神经切向内核或某些Mat\'ern内核）的情况下，相对于现有最优结果，存在显着的多项式低差距。

    Reinforcement learning (RL) has shown empirical success in various real world settings with complex models and large state-action spaces. The existing analytical results, however, typically focus on settings with a small number of state-actions or simple models such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have considered nonlinear function approximation using kernel ridge regression. We propose $\pi$-KRVI, an optimistic modification of least-squares value iteration, when the state-action value function is represented by an RKHS. We prove the first order-optimal regret guarantees under a general setting. Our results show a significant polynomial in the number of episodes improvement over the state of the art. In particular, with highly non-smooth kernels (such as Neural Tangent kernel or some Mat\'ern kernels) the existing results lead to trivial (superl
    
[^212]: 通过正交微调控制文本到图像的扩散

    Controlling Text-to-Image Diffusion by Orthogonal Finetuning. (arXiv:2306.07280v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.07280](http://arxiv.org/abs/2306.07280)

    本文介绍了一种名为正交微调（OFT）的方法，可以有效地引导和控制大型文本到图像扩散模型，以执行不同的下游任务。我们还提出了约束正交微调（COFT），来提高微调的稳定性。这些方法能够保持语义生成能力并生成特定主题的图像。

    

    大型文本到图像扩散模型在生成真实感图像方面有很强的能力。如何有效地引导或控制这些强大的模型以执行不同的下游任务成为一个重要的开放性问题。为了解决这个挑战，我们引入了一种基于原则的微调方法——正交微调（OFT），用于将文本到图像扩散模型调整到下游任务中。与现有方法不同，OFT可以证明地保持特征对神经元在单位超球面上的关系所表征的超球形能量。我们发现，这种属性对于保持文本到图像扩散模型的语义生成能力非常关键。为了提高微调的稳定性，我们进一步提出了约束正交微调（COFT），它对超球面施加了额外的半径约束。具体来说，我们考虑了两个重要的微调文本到图像任务：主题驱动生成，目标是生成特定主题的图像

    Large text-to-image diffusion models have impressive capabilities in generating photorealistic images from text prompts. How to effectively guide or control these powerful models to perform different downstream tasks becomes an important open problem. To tackle this challenge, we introduce a principled finetuning method -- Orthogonal Finetuning (OFT), for adapting text-to-image diffusion models to downstream tasks. Unlike existing methods, OFT can provably preserve hyperspherical energy which characterizes the pairwise neuron relationship on the unit hypersphere. We find that this property is crucial for preserving the semantic generation ability of text-to-image diffusion models. To improve finetuning stability, we further propose Constrained Orthogonal Finetuning (COFT) which imposes an additional radius constraint to the hypersphere. Specifically, we consider two important finetuning text-to-image tasks: subject-driven generation where the goal is to generate subject-specific images
    
[^213]: 基于凸组合的表达性损失可以提高网络的对抗鲁棒性

    Expressive Losses for Verified Robustness via Convex Combinations. (arXiv:2305.13991v1 [cs.LG])

    [http://arxiv.org/abs/2305.13991](http://arxiv.org/abs/2305.13991)

    通过基于凸组合的表达性损失，可以提高网络的对抗鲁棒性，最新的算法可以获得最先进的结果；这种方法通过对抗性攻击和IBP边界之间的简单凸组合进行实现。

    

    先前的工作通常通过（扰动区域的子集）的最坏情况下限，或在对抗训练之上引入可验证性来训练具有已验证鲁棒性的网络。最先进性能的关键在于所使用的损失函数的表达能力，它应该能够匹配训练后要使用的验证器的紧密度。我们形式化定义了表达力，并表明它可以通过对抗性攻击和IBP边界之间的简单凸组合来满足。然后，我们展示了所得到的算法，命名为CC-IBP和MTL-IBP，在各种设置中均可以产生最先进的结果，尽管其概念上是简单的。特别地，在TinyImageNet和缩小的ImageNet上，对于半径为$ \frac{1} {255} $的$ \ell_ \infty $扰动，MTL-IBP可以将文献中最佳标准和验证准确性从$1.98\%$提高到$3.92\%$，同时仅依赖于单步自适应优化。

    In order to train networks for verified adversarial robustness, previous work typically over-approximates the worst-case loss over (subsets of) perturbation regions or induces verifiability on top of adversarial training. The key to state-of-the-art performance lies in the expressivity of the employed loss function, which should be able to match the tightness of the verifiers to be employed post-training. We formalize a definition of expressivity, and show that it can be satisfied via simple convex combinations between adversarial attacks and IBP bounds. We then show that the resulting algorithms, named CC-IBP and MTL-IBP, yield state-of-the-art results across a variety of settings in spite of their conceptual simplicity. In particular, for $\ell_\infty$ perturbations of radius $\frac{1}{255}$ on TinyImageNet and downscaled ImageNet, MTL-IBP improves on the best standard and verified accuracies from the literature by from $1.98\%$ to $3.92\%$ points while only relying on single-step ad
    
[^214]: 基于随机策略梯度的模型无关语义通信强化学习

    Model-free Reinforcement Learning of Semantic Communication by Stochastic Policy Gradient. (arXiv:2305.03571v1 [eess.SP])

    [http://arxiv.org/abs/2305.03571](http://arxiv.org/abs/2305.03571)

    本论文利用随机策略梯度（SPG）强化学习，成功设计了一种无需通道模型的语义通信系统，能够传输意义而非精确版本，达到了信息速率节省的目的。

    

    受机器学习工具在无线通信方面的成功启发，韦弗（Weaver）于1949年提出的语义通信概念引起了人们的关注。它打破了香农经典的设计范例，旨在传输消息的意义，即语义，而不是精确版本，从而实现信息速率节省。在这项工作中，我们应用了随机策略梯度（SPG）来设计一种基于强化学习的语义通信系统，不需要已知或可微分通道模型，这是实际部署的关键步骤。此外，我们从最大化接收和目标变量之间的互信息出发，激发了将SPG用于经典和语义通信的动机。数值结果表明，我们的方法达到了与基于重新参数化技巧的模型感知方法相当的性能，尽管收敛速度有所降低。

    Motivated by the recent success of Machine Learning tools in wireless communications, the idea of semantic communication by Weaver from 1949 has gained attention. It breaks with Shannon's classic design paradigm by aiming to transmit the meaning, i.e., semantics, of a message instead of its exact version, allowing for information rate savings. In this work, we apply the Stochastic Policy Gradient (SPG) to design a semantic communication system by reinforcement learning, not requiring a known or differentiable channel model a crucial step towards deployment in practice. Further, we motivate the use of SPG for both classic and semantic communication from the maximization of the mutual information between received and target variables. Numerical results show that our approach achieves comparable performance to a model-aware approach based on the reparametrization trick, albeit with a decreased convergence rate.
    
[^215]: 用基于随机梯度下降的内点算法求解光滑有界约束优化问题

    A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])

    [http://arxiv.org/abs/2304.14907](http://arxiv.org/abs/2304.14907)

    本文提出了一种用于求解光滑有界约束优化问题的内点算法。它使用基于随机梯度估计的搜索方向和内部邻域，能够在确定性和随机性设置下具有收敛保证，并且在实验中表现出优于传统方法的性能。

    

    本文提出并分析了一种基于随机梯度估计的内点算法，用于求解存在约束的连续可微非凸目标函数最小化问题，并通过实验结果进行了演示。该算法在解决光滑（非凸）优化问题时与其他内点方法不同之处在于搜索方向是通过随机梯度估计计算得到的。它在使用可行域的内部邻域（由正且消失的邻域参数序列定义）的过程中也很独特，通过将迭代强制保留在该邻域内。实验结果表明，通过精心平衡屏障、步长和邻域序列，该算法能够满足确定性和随机性设置下的收敛保证。在两种设置下，数值实验的结果表明，该算法可以优于投影-（随机）梯度方法。

    A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.
    
[^216]: 从相邻的染色组织学片中学习黑色素细胞掩膜

    Learning Melanocytic Cell Masks from Adjacent Stained Tissue. (arXiv:2211.00646v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2211.00646](http://arxiv.org/abs/2211.00646)

    本文提出了一种从相邻染色组织学片中训练深度神经网络进行黑色素细胞分割的方法，实现了0.64的平均IOU，尽管存在不完美的标签。

    

    黑色素瘤是最具侵袭性的皮肤癌之一，导致大部分皮肤癌死亡。然而，病理学家对黑色素瘤的诊断可靠性较低。由于黑色素瘤是黑色素细胞的肿瘤，需要开发一种与病理学家的差异无关并能自动进行像素级注释的黑色素细胞分割工具。然而，大规模病理学家标注是不现实的。在本文中，我们提出了一种方法，使用邻近组织切片上的偶联免疫组织化学（IHC）染色片，训练深度神经网络进行黑色素细胞分割，虽然很难有完美的标签，但达到了0.64的平均IOU。

    Melanoma is one of the most aggressive forms of skin cancer, causing a large proportion of skin cancer deaths. However, melanoma diagnoses by pathologists shows low interrater reliability. As melanoma is a cancer of the melanocyte, there is a clear need to develop a melanocytic cell segmentation tool that is agnostic to pathologist variability and automates pixel-level annotation. Gigapixel-level pathologist labeling, however, is impractical. Herein, we propose a means to train deep neural networks for melanocytic cell segmentation from hematoxylin and eosin (H&E) stained slides using paired immunohistochemical (IHC) slides of adjacent tissue sections, achieving a mean IOU of 0.64 despite imperfect ground-truth labels.
    
[^217]: 对数线性保护性及其影响的研究

    Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10012](http://arxiv.org/abs/2210.10012)

    本研究介绍了对数线性保护性及其对下游分类器行为的影响。在二元情况下，下游对数线性模型无法恢复被删除的概念，但在某些情况下，可以通过构建多类对数线性模型间接恢复概念。这些结果揭示了线性删除方法的局限性，并强调了进一步研究的需求。

    

    已经发现，在假设可线性的神经表示中，从中删除可人解释的概念的方法是可行和有用的。然而，这种删除对于基于修改后表示进行训练的下游分类器行为的影响尚未完全理解。在这项工作中，我们正式定义了对数线性保护性的概念，即对手无法直接从表示中预测概念的能力，并研究其影响。我们证明，在二元情况下，在某些假设下，下游对数线性模型无法恢复被删除的概念。然而，我们证明，在某些情况下，可以构建一个多类对数线性模型，间接恢复概念，这指出了对数线性保护性作为下游偏差缓解技术的内在局限性。这些发现揭示了线性删除方法的理论限制，并强调了进一步研究可解释神经表示与分类器之间的联系的需要。

    Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
    
[^218]: Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)

    Kernelized Concept Erasure. (arXiv:2201.12191v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.12191](http://arxiv.org/abs/2201.12191)

    通过核化线性极小极大博弈，防止特定非线性对手预测概念，但无法彻底解决非线性编码的概念擦除问题。

    

    文本数据的神经模型的表示空间在训练过程中以无监督的方式出现。理解这些表示如何编码可解释的人类概念是一个基本问题。识别神经表示中的概念的一种明显方法是搜索一个线性子空间，其擦除会阻止从表示中预测概念。然而，尽管许多线性擦除算法是可处理和可解释的，但神经网络未必以线性方式表示概念。为了识别非线性编码的概念，我们提出了一个核化的概念擦除线性极小极大博弈。我们证明了可以防止特定的非线性对手预测概念。然而，这种保护不会转移到不同的非线性对手。因此，彻底地擦除非线性编码的概念仍然是一个待解决的问题。

    The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.
    

