# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model.](http://arxiv.org/abs/2304.15010) | 本文提出了LLaMA-Adapter V2，这是一个参数高效的视觉指令模型，通过解锁更多可学习的参数，早期融合策略和联合训练策略，能够更好地处理视觉输入和精确地执行开放式视觉指令。 |
| [^2] | [Are Emergent Abilities of Large Language Models a Mirage?.](http://arxiv.org/abs/2304.15004) | 研究指出大型语言模型所谓的新兴技能是研究者分析的产物，不是模型行为的基本变化。研究还展示了度量标准选择和可能研究人员的偏见，可能导致这种新兴技能的出现。 |
| [^3] | [Towards Automated Circuit Discovery for Mechanistic Interpretability.](http://arxiv.org/abs/2304.14997) | 该论文提出了一种新算法 ACDC，可以自动识别网络中的重要单元，从而实现机制可解释性自动电路发现。 |
| [^4] | [A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks.](http://arxiv.org/abs/2304.14994) | 本文提出了一种用神经网络求解初值偏微分方程的稳定可扩展方法，解决了在全局最小化神经网络参数中的 PDE 残差中遇到的灾难性遗忘和 ODE 方法中随着数量呈立方级别扩展等问题。 |
| [^5] | [Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards.](http://arxiv.org/abs/2304.14989) | 本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。 |
| [^6] | [Hierarchical and Decentralised Federated Learning.](http://arxiv.org/abs/2304.14982) | 分层联邦学习将传统的联邦学习扩展到更高效的模型聚合，平衡云-边缘计算的处理优势，将成为各种应用的关键支撑。 |
| [^7] | [MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks.](http://arxiv.org/abs/2304.14979) | 本文介绍了一种新型框架MLCopilot，通过利用最先进的大语言模型，扩展其能力以理解结构化输入并进行深入推理以解决新型ML任务的能力，展示了MLCopilot在解决图像分类，文本分类和表格分类三项任务方面的巨大潜力。 |
| [^8] | [Quality-Adaptive Split-Federated Learning for Segmenting Medical Images with Inaccurate Annotations.](http://arxiv.org/abs/2304.14976) | 本文提出了一种数据质量自适应的平均策略QA-SplitFed，能够有效应对客户端标签的偏见、不准确和不一致性，从而实现在不准确注释下的医学图像分割任务。实验结果表明，该方法在处理多个不准确客户端的情况下表现出更好的稳定性。 |
| [^9] | [Flow Away your Differences: Conditional Normalizing Flows as an Improvement to Reweighting.](http://arxiv.org/abs/2304.14963) | 本论文提出了一种新的方法，使用条件归一化流来修改分布，不依赖于分组选择和密度比的估计，从而更好地匹配目标分布。 |
| [^10] | [PAO: A general particle swarm algorithm with exact dynamics and closed-form transition densities.](http://arxiv.org/abs/2304.14956) | 本文提出了一种高度通用、可解释的PSO变体算法，即粒子吸引子算法（PAO），并设计了带有精确动力学和闭式转移密度。 |
| [^11] | [An Empirical Study of Multimodal Model Merging.](http://arxiv.org/abs/2304.14933) | 本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。 |
| [^12] | [Uncertainty Aware Neural Network from Similarity and Sensitivity.](http://arxiv.org/abs/2304.14925) | 本文提出了一种神经网络训练方法，可以考虑相似样本与敏感性感知，有效地计算神经网络中的不确定性。 |
| [^13] | [An EEG Channel Selection Framework for Driver Drowsiness Detection via Interpretability Guidance.](http://arxiv.org/abs/2304.14920) | 本文提出了一种解释性引导的通道选择框架（ICS）用于司机疲劳检测任务，通过逐步选择关键的贡献通道，在提高司机疲劳检测准确性、可解释性和效率方面具有显著的效果。 |
| [^14] | [ScatterFormer: Locally-Invariant Scattering Transformer for Patient-Independent Multispectral Detection of Epileptiform Discharges.](http://arxiv.org/abs/2304.14919) | 本论文提出了一个名为ScatterFormer的模型，其通过局部不变的散射变换与解耦的频率感知注意力（FAA）来提取EEG模式特征，从而实现病人无关的多光谱癫痫放电检测。该模型展示了其在两个癫痫形放电检测任务上的有效性。 |
| [^15] | ["Can't Take the Pressure?": Examining the Challenges of Blood Pressure Estimation via Pulse Wave Analysis.](http://arxiv.org/abs/2304.14916) | 本文分析了脉搏波分析预测血压的任务，发现许多论文常常出现数据泄漏和对任务及预处理步骤的不切实际限制。提出了新的工具来确定输入信号（如PPG）是否能预测所需的量度。 |
| [^16] | [Human Activity Recognition Using Self-Supervised Representations of Wearable Data.](http://arxiv.org/abs/2304.14912) | 本文提出了一个基于自监督表示法识别人类活动的模型，该模型在大量未标记数据集的基础上获得了较强性能，并在真实世界数据集上展现出了很好的效果。 |
| [^17] | [A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems.](http://arxiv.org/abs/2304.14907) | 本文提出了一种用于求解光滑有界约束优化问题的内点算法。它使用基于随机梯度估计的搜索方向和内部邻域，能够在确定性和随机性设置下具有收敛保证，并且在实验中表现出优于传统方法的性能。 |
| [^18] | [Enhancing Supply Chain Resilience: A Machine Learning Approach for Predicting Product Availability Dates Under Disruption.](http://arxiv.org/abs/2304.14902) | 该论文以通用电气 (GE) 燃气动力公司的气体和蒸汽涡轮服务和制造业务为例，研究了基于机器学习的产品可用性日期预测方法。研究结果显示，梯度提升机 (GBM) 模型表现最佳，平均绝对误差为 4.93 天，均方根误差为 6.47 天。研究还通过特征重要性分析确定了影响产品可用性日期的关键因素。 |
| [^19] | [The Power of Typed Affine Decision Structures: A Case Study.](http://arxiv.org/abs/2304.14888) | TADS是一种新的神经网络表示方法，可用于生成有关神经网络属性的证明或者误差特征，本文通过一个案例研究表明TADS可以用于提供关于神经网络鲁棒性错误的精确诊断。 |
| [^20] | [The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share & Requests.](http://arxiv.org/abs/2304.14882) | ACM Multimedia 2023 计算语言学挑战赛涉及情感共享和请求检测，提供了基线特征提取和分类器方法。 |
| [^21] | [Deep Stock: training and trading scheme using deep learning.](http://arxiv.org/abs/2304.14870) | 本文提出了一种使用深度学习进行训练和交易的方案，DeepStock通过查看股票价格的过去数据，并使用Resnet和logits来预测股票价格在未来D天内是否会升降一定百分比，并在韩国和美国市场上取得了超过市场回报的利润。 |
| [^22] | [Evaluating the Stability of Semantic Concept Representations in CNNs for Robust Explainability.](http://arxiv.org/abs/2304.14864) | 本文提出了一种用于评估语义概念表示在CNN中稳定性的方法，为实现强大的可解释性提供了基础。本文关注计算机视觉CNN中概念表示的稳定性：概念检索稳定性和概念归属稳定性。在此基础上，本文提出了一种新的度量标准以解决概念检索稳定性的问题。 |
| [^23] | [Wasserstein Dictionaries of Persistence Diagrams.](http://arxiv.org/abs/2304.14852) | 本文提出了一种基于Wasserstein字典的持久图的紧凑编码方法，并在实验中证明了其有效性，可以应用于数据降维和压缩。 |
| [^24] | [Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem.](http://arxiv.org/abs/2304.14848) | 本文提出了一种将音乐人声分离的感知任务建模为多轨迹跟踪问题的方法，通过预测音符之间的链接将不同的人声进行分离，从而鼓励单声部（人声）轨迹的产生。 |
| [^25] | [Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption.](http://arxiv.org/abs/2304.14836) | 本论文提出一种新的HE友好模型训练方法，成功演示了在ResNet和ConvNeXt等经典和现代CNN上运行加密样本，并以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测，是一种可行的隐私保护机器学习解决方案。 |
| [^26] | [Earning Extra Performance from Restrictive Feedbacks.](http://arxiv.org/abs/2304.14831) | 本文提出了一个名为EXPECTED的挑战，解决模型调整问题，模型提供者可以通过来自本地用户的反馈多次访问候选模型的操作性能，从而优化模型，同时不需要依赖目标数据。 |
| [^27] | [A noise-robust acoustic method for recognition of foraging activities of grazing cattle.](http://arxiv.org/abs/2304.14824) | 本研究提出了一种抗噪声的声学方法，能够分析与吃草和反刍相关的鉴定下颚运动事件的固定长度段，用于识别牛的觅食活动，并在环境和自然噪声方面具有鲁棒性。 |
| [^28] | [Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation.](http://arxiv.org/abs/2304.14807) | 本文提出了一种基于深度学习辅助微波等离子体相互作用技术的等离子体密度估计方法，通过测量微波散射引起的电场模式来估计密度剖面。 |
| [^29] | [ResiDual: Transformer with Dual Residual Connections.](http://arxiv.org/abs/2304.14802) | 本文提出了具有Pre-Post-LN双重残差连接的新型Transformer架构ResiDual，解决了Post-LN和Pre-LN存在的问题，并具有优越的性能表现。 |
| [^30] | [MCPrioQ: A lock-free algorithm for online sparse markov-chains.](http://arxiv.org/abs/2304.14801) | 本文提出了MCPrioQ算法，一种无锁稀疏Markov链数据结构，用于在线和连续学习，特别适用于推荐系统，时间复杂度为$O(1)$用于更新和$O(CDF^{-1}(t))$用于推理。 |
| [^31] | [Learning Graph Neural Networks using Exact Compression.](http://arxiv.org/abs/2304.14793) | 本文研究了利用精确压缩来减少在大型图上学习图神经网络的内存需求，并提出了一种证明等效的压缩图神经网络学习问题的方法。 |
| [^32] | [Multisample Flow Matching: Straightening Flows with Minibatch Couplings.](http://arxiv.org/abs/2304.14772) | 该论文提出了一种多样本流匹配算法，在满足正确的边缘约束的条件下，利用小批量耦合将流进行矫正，从而使生成模型的训练更加高效，并获得更高质量、更低维代价的运输图。 |
| [^33] | [Hyperparameter Optimization through Neural Network Partitioning.](http://arxiv.org/abs/2304.14766) | 本文提出了一种将训练数据和神经网络模型分区的方法，将每个分区与特定的数据片段关联并进行优化，通过优化这些分区的子网络的“训练之外的样本”损失，实现了在单次训练运行中降低超参数优化代价的效果。 |
| [^34] | [Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy.](http://arxiv.org/abs/2304.14762) | 本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。 |
| [^35] | [A New Class of Explanations for Classifiers with Non-Binary Features.](http://arxiv.org/abs/2304.14760) | 本文提出了一种适用于具有非二元特征的分类器的新型解释方法，可以提供更多关于决策和基础分类器的信息。 |
| [^36] | [Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics.](http://arxiv.org/abs/2304.14738) | 本研究提出了代价敏感自训练（CSST）框架，可以更好地利用未标记的数据优化不可分解指标，为处理具有复杂目标的实际机器学习系统提供了实用的解决方案。 |
| [^37] | [Benchmarking Automated Machine Learning Methods for Price Forecasting Applications.](http://arxiv.org/abs/2304.14735) | 本研究展示了将自动化机器学习解决方案与企业领域知识相结合，以替代手动创建的ML管道，为中小型企业实现自动化价格预测提供可能。 |
| [^38] | [A Federated Reinforcement Learning Framework for Link Activation in Multi-link Wi-Fi Networks.](http://arxiv.org/abs/2304.14720) | 本文提出了一种使用联邦强化学习框架来协同优化多链路Wi-Fi网络中的链路激活的方法。 |
| [^39] | [X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation.](http://arxiv.org/abs/2304.14698) | 本论文提出了一种基于强化学习的方法，X-RLflow，用于替换神经网络的子图，以求得更优的计算图结构，可在各种深度学习模型和基准测试中超越现有技术的超优化系统。 |
| [^40] | [Graph Neural Networks on Factor Graphs for Robust, Fast, and Scalable Linear State Estimation with PMUs.](http://arxiv.org/abs/2304.14680) | 本文通过在因子图上实现图神经网络，从PMU电压和电流测量中学习复杂的母线电压估计，提高了线性状态估计的鲁棒性和可扩展性。 |
| [^41] | [Client Recruitment for Federated Learning in ICU Length of Stay Prediction.](http://arxiv.org/abs/2304.14663) | 本研究提出了一种新的基于ICU住院时间预测的联邦学习客户端招募策略，通过准则、相似性和聚类，可以降低通信开销和训练成本，同时保持预测性能。 |
| [^42] | [Segment Anything Model for Medical Images?.](http://arxiv.org/abs/2304.14660) | “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。 |
| [^43] | [From Explicit Communication to Tacit Cooperation:A Novel Paradigm for Cooperative MARL.](http://arxiv.org/abs/2304.14656) | 该论文提出了一种从显性沟通到隐性合作的新协作多智能体学习范式，通过在代理之间分享信息和使用本地轨迹重建信息来促进协作，并逐渐减少显式传达信息的比例。实验结果表明，这种范式在具有挑战性的情景下比传统的 CTDE 范式表现更好。 |
| [^44] | [An Adaptive Policy to Employ Sharpness-Aware Minimization.](http://arxiv.org/abs/2304.14647) | 本研究提出了一种自适应策略，基于损失函数的形状，用于设计高效的锐度感知优化算法SAM。采用AE-SAM和AE-LookSAM两种算法，理论上证明AE-SAM具有与SAM相同的收敛速度。实验结果表明这一策略的高效和有效。 |
| [^45] | [On Underdamped Nesterov's Acceleration.](http://arxiv.org/abs/2304.14642) | 该论文研究了欠阻尼Nesterov加速，基于高分辨率微分方程框架构建了新的Lyapunov函数，证明了最小梯度范数平方和目标值的收敛速率。 |
| [^46] | [CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction.](http://arxiv.org/abs/2304.14633) | 研究团队提出了一种基于代价体的3D神经重建框架CVRecon，利用丰富的几何嵌入来促进3D几何特征学习。通过引入射线上下文补偿代价体（RCCV），有效提高了视角相关信息的完整性和鲁棒性，并在各种度量方面显着提高了重建质量。 |
| [^47] | [MUDiff: Unified Diffusion for Complete Molecule Generation.](http://arxiv.org/abs/2304.14621) | MUDiff 是一种新的分子数据生成模型，它通过组合离散和连续扩散过程来生成全面的分子表示。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。我们的模型还包括一个新颖的图形转换器架构，用于去噪扩散过程。 |
| [^48] | [Recognizable Information Bottleneck.](http://arxiv.org/abs/2304.14618) | 本文提出了一种可识别信息瓶颈（RIB），其通过可识别性评论家来规范表示的可识别性，从而实现了一种有效的数据推广方式，相比现有的信息瓶颈能够更好地保证在实际应用中的有效性。 |
| [^49] | [Counterfactual Explanation with Missing Values.](http://arxiv.org/abs/2304.14606) | 本论文提出了一种新的反事实解释方法（CEPIA），可以处理数据中的缺失值，使得用户可以获得有效的行动建议并了解缺失值对建议的影响。 |
| [^50] | [Augmented balancing weights as linear regression.](http://arxiv.org/abs/2304.14545) | 本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。 |
| [^51] | [Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data.](http://arxiv.org/abs/2304.14541) | 该论文提出了一种新颖的算法Deep Spatiotemporal Clustering (DSC) ，用于使用无监督深度学习方法进行高维时空数据的时间聚类，DSC利用自动编码器集成CNN-RNN层学习时空数据的潜在表示，并优化聚类损失和数据重建损失以改善聚类分配和非线性重建能力。 |
| [^52] | [Optimal partition of feature using Bayesian classifier.](http://arxiv.org/abs/2304.14537) | 本文通过提出一种名为“共单调独立分类器”(CIBer)的新技术，专注于特征的最优分区，旨在克服朴素贝叶斯方法带来的挑战，并且证明该技术在不同数据集上具有更高的准确率和更低的错误率。 |
| [^53] | [Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization.](http://arxiv.org/abs/2304.14535) | 本文Survey了基于DTL的ASR框架，并介绍了如何使用实际数据集进行深度迁移学习以达到更好的泛化性能。 |
| [^54] | [Adversarial Policy Optimization in Deep Reinforcement Learning.](http://arxiv.org/abs/2304.14533) | 本文提出了一种新的强化学习算法，其中一个扰动网络通过最大化智能体执行不同动作的概率，同时最小化状态的扭曲，以减轻数据过拟合的影响。 |
| [^55] | [It is all about where you start: Text-to-image generation with seed selection.](http://arxiv.org/abs/2304.14530) | 该论文研究了文本到图像生成中训练数据不平衡对模型的影响，并提出了一种高效的方法：在噪声空间中选择适当的生成种子。该方法能够正确生成罕见的概念，而不需要重新训练模型。 |
| [^56] | [Multivariate Representation Learning for Information Retrieval.](http://arxiv.org/abs/2304.14522) | 本论文提出一种多元分布模型的信息检索表示学习框架，可无缝集成到现有近似最近邻算法中以实现高效检索。 |
| [^57] | [Understanding Shared Speech-Text Representations.](http://arxiv.org/abs/2304.14514) | 本文研究了将文本整合进入端到端语音模型中训练的方法，通过研究无语音域自适应和激活的相似性，发现持续模型对共享语音-文本表示很重要，共享编码器学习了一个比单模态更紧凑重叠的语音-文本表示，这部分解释了Maestro共享的语音-文本表示有效的原因。 |
| [^58] | [3D Brainformer: 3D Fusion Transformer for Brain Tumor Segmentation.](http://arxiv.org/abs/2304.14508) | 本文提出了一种名为3D Brainformer的三维融合Transformer结构，可以更准确地对脑MRI图像中的肿瘤进行分割。 |
| [^59] | [Transformer-based interpretable multi-modal data fusion for skin lesion classification.](http://arxiv.org/abs/2304.14505) | 本文提出了一种基于Transformer的可解释多模态数据融合算法，用于帮助皮肤疾病的诊断。 |
| [^60] | [Hybrid Deepfake Detection Utilizing MLP and LSTM.](http://arxiv.org/abs/2304.14504) | 本文提出了一种使用深度学习算法MLP和LSTM的深度伪造检测模式，旨在检测并防止虚假信息的传播。 |
| [^61] | [MWaste: A Deep Learning Approach to Manage Household Waste.](http://arxiv.org/abs/2304.14498) | MWaste是一款基于深度学习的移动应用，能够有效地将垃圾材料分类为垃圾、塑料、纸张、金属、玻璃或硬纸板，可帮助应对气候变化。 |
| [^62] | [Restoring Original Signal From Pile-up Signal using Deep Learning.](http://arxiv.org/abs/2304.14496) | 本研究采用深度学习方法还原叠加信号的原始信号，提高了物理数据的能量和时间分辨率，适用于类似问题的解决。 |
| [^63] | [Symmetry and Complexity in Object-Centric Deep Active Inference Models.](http://arxiv.org/abs/2304.14493) | 本文研究了深度主动推理下的物体中心表示中的对称性，以生成最简洁而准确的模型，从而学习和预测新的物体视图。 |
| [^64] | [Automatic Generation of Labeled Data for Video-Based Human Pose Analysis via NLP applied to YouTube Subtitles.](http://arxiv.org/abs/2304.14489) | 该研究利用NLP技术分析健身视频字幕数据自动生成人体姿势分析的标记数据集。 |
| [^65] | [Adversary Aware Continual Learning.](http://arxiv.org/abs/2304.14483) | 本文提出了一种新的防御性框架，针对对抗性后门攻击，利用可感知模式压倒攻击者的不可感知模式，提高了模型的稳健性。 |
| [^66] | [ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger.](http://arxiv.org/abs/2304.14475) | 本论文研究了黑盒生成模型作为后门攻击工具的角色以及相应的防御策略。其中，我们发现通过BGMAttack攻击文本分类器可以有效地实现后门攻击，而且输入的数据没有明显变形。 |
| [^67] | [Learning a Diffusion Prior for NeRFs.](http://arxiv.org/abs/2304.14473) | 本文提出了一种使用扩散模型生成编码在规则网格上的NeRFs的方法，该方法可以采样出逼真的NeRFs，并允许有条件的生成，给定某个观察作为指导。 |
| [^68] | [Nordic Vehicle Dataset (NVD): Performance of vehicle detectors using newly captured NVD from UAV in different snowy weather conditions.](http://arxiv.org/abs/2304.14466) | 本研究提供了无人机捕捉不同雪天气候下的车辆数据集，填补了检测大雪天气中车辆的数据缺失问题，对于车辆在复杂天气条件下的检测具有实际应用价值。 |
| [^69] | [Moccasin: Efficient Tensor Rematerialization for Neural Networks.](http://arxiv.org/abs/2304.14463) | 本文提出了一种名为Moccasin的新型约束编程形式，用于实现在内存预算下最小化计算图的执行时间，相较于最近的研究，该方法显著提高了效率，并成功应用于神经网络的高效张量重算。 |
| [^70] | [Robust and Fast Vehicle Detection using Augmented Confidence Map.](http://arxiv.org/abs/2304.14462) | 提出了一种基于增强置信度图的鲁棒快速车辆检测方法，通过组合多分辨率分析和极值稳定区域，生成增强置信度图，再通过快速CNN生成候选区域，有效地减少了不同速度、形状、结构和单张图像中多个车辆的影响，并采用粗糙集和基于模糊的模型实现了鲁棒的车辆检测。 |
| [^71] | [Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection.](http://arxiv.org/abs/2304.14460) | 对于域增量3D物体检测，GMIR提出了一种基于梯度的最大干扰恢复策略，可在微调时定期从以前的领域数据集中检索样本。 |
| [^72] | [Machine Learning for Detection and Mitigation of Web Vulnerabilities and Web Attacks.](http://arxiv.org/abs/2304.14451) | 本文调研了使用经典和先进的机器学习技术进行防御XSS和CSRF的研究，并总结出关键要点，为探讨该研究方向提供了参考。 |
| [^73] | [Learning Environment for the Air Domain (LEAD).](http://arxiv.org/abs/2304.14423) | 本文介绍了针对空中作战行为的学习环境（LEAD）系统。该系统整合了Gymnasium编程库和接口，可应用现成的机器学习算法，并可以与第三方模拟软件通信。 |
| [^74] | [MINN: Learning the dynamics of differential-algebraic equations and application to battery modeling.](http://arxiv.org/abs/2304.14422) | 本文提出了一种体系结构，生成模型集成神经网络（MINN）以允许在学习系统物理动态方面进行整合，应用于锂离子电池的电化学动力学建模，并展示了所提出的模型在解释性、精度和计算效率方面的优势。 |
| [^75] | [One-Step Distributional Reinforcement Learning.](http://arxiv.org/abs/2304.14421) | 本文提出一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性，提供了统一的理论，具有更好的表现。 |
| [^76] | [Network Cascade Vulnerability using Constrained Bayesian Optimization.](http://arxiv.org/abs/2304.14420) | 本研究基于约束贝叶斯优化，以修改输电线路保护设置为敌对攻击的候选方案，探讨了最大化级联网络退化的保护设置规律，发现将所有电网线路的保护设置最大失配并不会导致最多的级联。 |
| [^77] | [SSTM: Spatiotemporal Recurrent Transformers for Multi-frame Optical Flow Estimation.](http://arxiv.org/abs/2304.14418) | 该论文提出了一种基于学习的多帧光流估计方法，通过理解具有多于两帧的更长序列中时间场景的动态，推广复杂的运动模式，从而提高光流准确性。 |
| [^78] | [Do SSL Models Have D\'ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning.](http://arxiv.org/abs/2304.13850) | 自监督学习（SSL）算法会意外地记忆单个训练样本中的特定部分，称为“似曾相识”记忆，该现象是普遍存在的，不能被传统的评估方法检测。 |
| [^79] | [Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning.](http://arxiv.org/abs/2304.11930) | 该研究描述了一种基于深度学习的新方法，用于模块化核探测器的无标记时间分析，其能够利用单个探测器内部时间相关性，实现有意义和准确的映射函数。 |
| [^80] | [To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review.](http://arxiv.org/abs/2304.09355) | 本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。 |
| [^81] | [Long-term Forecasting with TiDE: Time-series Dense Encoder.](http://arxiv.org/abs/2304.08424) | TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。 |
| [^82] | [Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value.](http://arxiv.org/abs/2304.07718) | Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。 |
| [^83] | [The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges.](http://arxiv.org/abs/2304.05351) | 本研究对ChatGPT在股票预测方面进行了零样本分析，结果表明其预测股票移动的表现不如最先进和传统方法，需要进一步改进。 |
| [^84] | [A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation.](http://arxiv.org/abs/2304.00471) | 该研究开发了一种轻量级的语音驱动动态生成脸部特征模型，并通过从基于生成对抗网络的原始模型中移除残差块和减少通道宽度以及应用端到端的压缩技术实现了高效的推理和占用更少内存。 |
| [^85] | [Ensemble weather forecast post-processing with a flexible probabilistic neural network approach.](http://arxiv.org/abs/2303.17610) | 本论文提出了一种神经网络和归一化流相结合的方法，可联合预测所有位置和提前期，从而放宽了许多传统后处理方法的分布假设，并通过EUPPBench基准测试证明了其超越性能。 |
| [^86] | [The Hidden-Manifold Hopfield Model and a learning phase transition.](http://arxiv.org/abs/2303.16880) | 提出一种称为隐藏流形Hopfield模型的广义Hopfield模型，重点研究了在复杂数据上的应用，发现其中存在着一种学习相变的现象。 |
| [^87] | [Topological Reconstruction of Particle Physics Processes using Graph Neural Networks.](http://arxiv.org/abs/2303.13937) | Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。 |
| [^88] | [DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion.](http://arxiv.org/abs/2303.12743) | 该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。 |
| [^89] | [DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network.](http://arxiv.org/abs/2303.02165) | DeepMAD是一种基于数学的新框架，能够在系统化的方式中设计出高性能的CNN模型，解决了CNN网络设计过程中的挑战性问题。 |
| [^90] | [Explainable Contextual Anomaly Detection using Quantile Regression Forests.](http://arxiv.org/abs/2302.11239) | 该论文提出了一种可解释的上下文异常检测方法，运用分位数回归森林来模拟特征之间的依赖关系，能够更准确和可解释地识别偏离类似对象上下文的其他对象。 |
| [^91] | [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat.](http://arxiv.org/abs/2302.10289) | 本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。 |
| [^92] | [Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning.](http://arxiv.org/abs/2302.05326) | 本文提出了两个限制使得实时循环学习算法具有可扩展性，分别是将网络分解为独立模块或逐步学习网络。与其他可扩展算法不同的是，这些算法不会向梯度估计添加噪声或偏差，而是通过权衡网络的功能能力以实现可扩展学习。 |
| [^93] | [Linear Optimal Partial Transport Embedding.](http://arxiv.org/abs/2302.03232) | 本文提出了线性最优偏移嵌入技术（LOPT），它扩展了（局部）线性化技术到OPT问题上，提高了正测度对之间的计算速度。并且在点云内插和PCA分析中进行了应用。 |
| [^94] | [A novel framework for medium-term wind power prediction based on temporal attention mechanisms.](http://arxiv.org/abs/2302.01222) | 本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架（TPE-VMD-TFT），用于24小时和48小时之前的风电功率预测。在法国电力公司Engie的风能数据集上，所提出的方法表现良好。 |
| [^95] | [CMLCompiler: A Unified Compiler for Classical Machine Learning.](http://arxiv.org/abs/2301.13441) | 本文提出了一个名为CMLCompiler的统一编译器框架，用于经典机器学习（CML）推理。通过提出两种统一的抽象：运算符表示和扩展计算图，CMLCompiler框架可以将用于CML的已优化计算图转换并输出到DL编译器或框架中，实现了卓越的可移植性和性能提升。 |
| [^96] | [Simplifying Subgraph Representation Learning for Scalable Link Prediction.](http://arxiv.org/abs/2301.12562) | 提出了一种新的可扩展简化子图表示学习（S3GRL）框架，通过简化每个链接子图中的消息传递和聚合操作实现更快的训练和推理，并可适应各种子图采样策略和扩散操作符以模拟计算代价高的子图表示学习。大量实验证明了S3GRL模型可以扩展SGRL而不会显著降低性能。 |
| [^97] | [Convolution-enhanced Evolving Attention Networks.](http://arxiv.org/abs/2212.08330) | 本文提出了一种新颖且通用的卷积增强进化注意力机制，通过一系列残差卷积模块直接模拟标记间关系的演变，在不同层次之间促进信息流动。 |
| [^98] | [Automating Rigid Origami Design.](http://arxiv.org/abs/2211.13219) | 这篇论文介绍了一种离散优化问题-刚性折纸游戏，该游戏可以扩展刚性折纸的潜力，使其得到针对应用特定的折痕图案，从而可以得到日常物品的新颖、可折叠和实用的设计。 |
| [^99] | [A Generic Approach for Reproducible Model Distillation.](http://arxiv.org/abs/2211.12631) | 本文提出了一种用中心极限定理为基础的通用稳定模型蒸馏方法，能够从候选学生模型中搜索与老师模型相一致的合理模型，使用一个多重检验框架来选择一组足够大的伪数据集，以选出一致的学生模型。 |
| [^100] | [PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels.](http://arxiv.org/abs/2211.08604) | 本文提出了一种基于图注意力网络和PU损失函数的欺诈检测方法PU GNN，通过改进的GraphSMOTE算法来处理P2E MMORPGs欺诈检测数据集中的标签分布不平衡问题，实验证明该方法在欺诈检测方面具有良好的性能表现。 |
| [^101] | [Total Variation Graph Neural Networks.](http://arxiv.org/abs/2211.06218) | 本论文提出一种新型的GNN模型，通过基于图总变差的松弛来计算集群分配，并通过最小化相邻顶点之间特征的$\ell_1$距离来实现锐利转换，实现顶点聚类和图池化。 |
| [^102] | [Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming.](http://arxiv.org/abs/2211.01317) | 本文提出了一种基于神经模型重新编程的迁移学习方法，并针对复杂输入数据提出了输入依赖NMR范式，能够有效地进行音乐风格分类。 |
| [^103] | [Certified Robustness of Quantum Classifiers against Adversarial Examples through Quantum Noise.](http://arxiv.org/abs/2211.00887) | 本文首次提出添加量子随机旋转噪声可以提高量子分类器对抗攻击的鲁棒性，并通过实验支持了一种证明性的鲁棒性边界。 |
| [^104] | [Training Neural Networks for Sequential Change-point Detection.](http://arxiv.org/abs/2210.17312) | 本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。 |
| [^105] | [Improving Hyperspectral Adversarial Robustness Under Multiple Attacks.](http://arxiv.org/abs/2210.16346) | 该论文提出了一种Adversarial Discriminator Ensemble Network（ADE-Net）来增强高光谱图像模型的鲁棒性，针对不同攻击类型使用相应的攻击专家集合网络。 |
| [^106] | [Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition.](http://arxiv.org/abs/2210.08415) | 本文引入了统一加倍条件，以确保在训练DNN期间准确性的稳定性，无论使用绝对值激活函数还是ReLU激活函数。在统一加倍条件下，准确率有很高的概率稳定，并提供了具体估计。 |
| [^107] | [Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks.](http://arxiv.org/abs/2210.04476) | 本论文提出了一种使用演示和自然语言指令相结合的机器人任务学习方法，其基于两种模式相互消除歧义来有效地指定和教授机器人复杂任务。此方法可以减少教师工作量并实现更好的泛化性能。 |
| [^108] | [The SZ flux-mass ($Y$-$M$) relation at low halo masses: improvements with symbolic regression and strong constraints on baryonic feedback.](http://arxiv.org/abs/2209.02075) | 本文通过使用液体动力学模拟和机器学习工具对低质量晕的SZ通量质量（$Y$-$M$）关系进行了全面研究，发现通过简单地将$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$可以使该关系显著自相似，这对于低质量星团和星系群是一种稳健的多波长质量代理。 |
| [^109] | [Linguistically inspired roadmap for building biologically reliable protein language models.](http://arxiv.org/abs/2207.00982) | 研究提供了一个基于语言学的蛋白质语言模型建立的路线图，以帮助建立更可解释的蛋白质LM，这将有助于发现相关领域特定的规则，从而促进基于规则的生物治疗药物的开发。 |
| [^110] | [Learning Distribution Grid Topologies: A Tutorial.](http://arxiv.org/abs/2206.10837) | 本教程总结了针对电力分配网络最新的拓扑识别和检测方案，重点介绍了克服分布式电网中测量设备有限的方法，利用前馈的功率流物理方程和结构特性来增强拓扑估计。 |
| [^111] | [LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks.](http://arxiv.org/abs/2206.09333) | LogGENE采用分位数回归框架预测基因表达水平的完整条件分位数，从而为高通量基因组学提供了一种能提供解释和报告不确定性估计、鲁棒性强的推断方法。 |
| [^112] | [Learning Soft Constraints From Constrained Expert Demonstrations.](http://arxiv.org/abs/2206.01311) | 本文提出了一种反向强化学习的方法，能够从专家数据中学习软约束，恢复每个周期内代理平均满足的累积约束。方法通过调整约束函数实现。 |
| [^113] | [Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning.](http://arxiv.org/abs/2205.14842) | 本文通过设计一种通用的奖励污染框架展示了现有深度强化学习算法内在的漏洞，提出了两种新攻击方式，成功污染多种最先进DRL算法的智能体在多个环境中学习的效率，给深度强化学习提供了新的安全风险。 |
| [^114] | [Transformer for Partial Differential Equations' Operator Learning.](http://arxiv.org/abs/2205.13671) | 本文提出一种基于注意力机制的数据驱动算子学习框架OFormer，它可以广泛适用于不同的偏微分方程，并且具有与传统方法相当的准确性，甚至在某些情况下具有更好的表现。 |
| [^115] | [Decision Models for Selecting Federated Learning Architecture Patterns.](http://arxiv.org/abs/2204.13291) | 本文提出了一套选择联邦机器学习架构设计模式的决策模型，映射功能需求和非功能需求到一组模式上，以协助对联邦机器学习知识有限的设计师和架构师。 |
| [^116] | [TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs.](http://arxiv.org/abs/2112.02052) | 本文提出了TC-GNN，这是第一个基于GPU张量核心单元（TCUs）的GNN加速框架。采用稀疏图翻译技术来协调“稀疏”GNN计算与高性能的“密集”TCUs，实现了GNN计算效率的提升。 |
| [^117] | [A Fast Hybrid Cascade Network for Voxel-based 3D Object Classification.](http://arxiv.org/abs/2011.04522) | 本文提出了一种用于基于体素的三维物体分类的快速混合级联网络，包含三个阶段分别处理易、中和难的三维模型，通过给每个体素赋予有符号的距离值来提高精度，并大大减少了平均推理时间。 |
| [^118] | [Multimodal Affective States Recognition Based on Multiscale CNNs and Biologically Inspired Decision Fusion Model.](http://arxiv.org/abs/1911.12918) | 本文提出了一种基于多尺度 CNN 和仿生决策融合模型的多模态情感状态识别方法，该方法对 EEG 和各个外周生理信号分别进行预测，并通过信号的可靠性进行决策。 |
| [^119] | [Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization.](http://arxiv.org/abs/1903.02140) | 本文介绍了利用规范空间证明学习大规模神经网络的目标函数在收敛到全局最小值时是凸优化问题。使用点线性转换的方法建立原始NN模型空间和规范空间之间的关系，证明了使用梯度下降方法，只要差异矩阵保持完整秩，就一定能收敛到零损失的全局最小值。大规模NN具有奇异的差异矩阵的概率非常小。 |

# 详细

[^1]: LLaMA-Adapter V2: 参数高效的视觉指令模型

    LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])

    [http://arxiv.org/abs/2304.15010](http://arxiv.org/abs/2304.15010)

    本文提出了LLaMA-Adapter V2，这是一个参数高效的视觉指令模型，通过解锁更多可学习的参数，早期融合策略和联合训练策略，能够更好地处理视觉输入和精确地执行开放式视觉指令。

    

    近期的研究方向是如何将大型语言模型（LLMs）高效地转化为指令跟随者，而为多模态推理训练LLM的研究仍然较少。虽然最近的LLaMA-Adapter证明了用LLM处理视觉输入的潜力，但它仍然不能很好地推广到开放式视觉指令，并且落后于GPT-4。本文提出了LLaMA-Adapter V2，这是一个参数高效的视觉指令模型。

    How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This 
    
[^2]: 大型语言模型所表现的新兴技能是否为幻觉？

    Are Emergent Abilities of Large Language Models a Mirage?. (arXiv:2304.15004v1 [cs.AI])

    [http://arxiv.org/abs/2304.15004](http://arxiv.org/abs/2304.15004)

    研究指出大型语言模型所谓的新兴技能是研究者分析的产物，不是模型行为的基本变化。研究还展示了度量标准选择和可能研究人员的偏见，可能导致这种新兴技能的出现。

    

    最近的研究声称，大型语言模型展示了新兴技能，这些技能在更小规模的模型中不存在，但在更大规模的模型中存在。新兴技能让人感到困惑的是两方面：它们的清晰度，似乎瞬间从不存在到存在，以及它们的不可预测性，似乎在不可预见的模型规模下出现。本文提出了新兴技能的另一种解释，即对于特定任务和模型族，当分析固定的模型输出时，可以选择导致推断出新兴技能或不导致推断出新兴技能的度量标准。因此，我们的解释表明，现有的新兴技能声明是研究人员分析的产物，而不是特定任务中模型行为的基本变化。我们在一个简单的数学模型中提出了我们的解释，然后通过三种互补的方式进行了测试：我们(1)制作、测试并验证了关于报告的新兴技能的度量选择的三个预测效应；(2)展示了模型架构和训练程序的简单变化会在一个已经确定的任务中产生大的新兴能力差异；(3)展示所谓的新兴技能可以通过有意优化所选择的评估指标来实现。总的来说，我们认为目前大型语言模型中新兴能力的声明很可能并不是真实存在的，而是度量标准任意选择和可能的研究人员偏见的产物。

    Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, one can choose a metric which leads to the inference of an emergent ability or another metric which does not. Thus, our alternative suggests that existing claims of emergent abilities are creations of the researcher's analyses, not fundamental changes in model behavior on specific tasks with scale. We present our explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metri
    
[^3]: 实现机制可解释性自动电路发现

    Towards Automated Circuit Discovery for Mechanistic Interpretability. (arXiv:2304.14997v1 [cs.LG])

    [http://arxiv.org/abs/2304.14997](http://arxiv.org/abs/2304.14997)

    该论文提出了一种新算法 ACDC，可以自动识别网络中的重要单元，从而实现机制可解释性自动电路发现。

    

    最近的机制可解释性研究倒推了变形金刚模型的非平凡行为。这些发现需要大量的工作和研究者的直觉，这使得应用相同的方法来了解当前模型所展示的复杂行为变得困难。然而，这些发现的核心工作流程非常相似。研究人员创建一个数据集和度量，诱发所需的模型行为，将网络分为适当的抽象单元，替换这些单元的激活以确定哪些参与了行为，然后解释这些单元实施的功能。通过改变数据集、度量和待研究的单元，研究人员可以理解每个神经网络区域的功能和它们组成的电路。本文提出了一种新算法，自动电路发现（ACDC），以自动识别网络中的重要单元。

    Recent work in mechanistic interpretability has reverse-engineered nontrivial behaviors of transformer models. These contributions required considerable effort and researcher intuition, which makes it difficult to apply the same methods to understand the complex behavior that current models display. At their core however, the workflow for these discoveries is surprisingly similar. Researchers create a data set and metric that elicit the desired model behavior, subdivide the network into appropriate abstract units, replace activations of those units to identify which are involved in the behavior, and then interpret the functions that these units implement. By varying the data set, metric, and units under investigation, researchers can understand the functionality of each neural network region and the circuits they compose. This work proposes a novel algorithm, Automatic Circuit DisCovery (ACDC), to automate the identification of the important units in the network. Given a model's comput
    
[^4]: 用神经网络求解初值偏微分方程的稳定可扩展方法

    A Stable and Scalable Method for Solving Initial Value PDEs with Neural Networks. (arXiv:2304.14994v1 [cs.LG])

    [http://arxiv.org/abs/2304.14994](http://arxiv.org/abs/2304.14994)

    本文提出了一种用神经网络求解初值偏微分方程的稳定可扩展方法，解决了在全局最小化神经网络参数中的 PDE 残差中遇到的灾难性遗忘和 ODE 方法中随着数量呈立方级别扩展等问题。

    

    与传统的网格和基于网格的方法不同，神经网络有可能打破维数灾难，在使用经典求解器困难或不可能的问题中提供近似解。全局最小化神经网络参数中的 PDE 残差对于边界值问题效果良好，但是灾难性忘却损害了这种方法对于初值问题的适用性。在替代的局部时间方法中，可以将优化问题转化为网络参数上的常微分方程（ODE），并将解向前传播。然而，我们证明了目前基于这种方法的方法存在两个关键问题。首先，遵循 ODE 会导致问题条件增长无法控制，最终导致不可接受的大数值误差。其次，随着 ODE 方法随着 m 的数量呈立方级别扩展。

    Unlike conventional grid and mesh based methods for solving partial differential equations (PDEs), neural networks have the potential to break the curse of dimensionality, providing approximate solutions to problems where using classical solvers is difficult or impossible. While global minimization of the PDE residual over the network parameters works well for boundary value problems, catastrophic forgetting impairs the applicability of this approach to initial value problems (IVPs). In an alternative local-in-time approach, the optimization problem can be converted into an ordinary differential equation (ODE) on the network parameters and the solution propagated forward in time; however, we demonstrate that current methods based on this approach suffer from two key issues. First, following the ODE produces an uncontrolled growth in the conditioning of the problem, ultimately leading to unacceptably large numerical errors. Second, as the ODE methods scale cubically with the number of m
    
[^5]: Kullback-Leibler Maillard采样在有界奖励的多臂赌博机问题中的应用

    Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])

    [http://arxiv.org/abs/2304.14989](http://arxiv.org/abs/2304.14989)

    本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。

    

    本文研究了奖励分布集中在区间$[0,1]$内的$K$臂数臂赌博机问题。本文提出了一种名为Kullback-Leibler Maillard Sampling (KL-MS)的新算法，它是Maillard采样在KL空间的自然扩展。实验表明，KL-MS在Bernoulli奖励时具有渐近最优性能，其最坏情况遗憾度上界为$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$，其中$\mu^*$是最优臂的期望奖励，$T$是时段长度。

    We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
    
[^6]: 分层和去中心化的联邦学习

    Hierarchical and Decentralised Federated Learning. (arXiv:2304.14982v1 [cs.LG])

    [http://arxiv.org/abs/2304.14982](http://arxiv.org/abs/2304.14982)

    分层联邦学习将传统的联邦学习扩展到更高效的模型聚合，平衡云-边缘计算的处理优势，将成为各种应用的关键支撑。

    

    联邦学习已经显示出了在分布式环境下训练机器学习模型并降低通信成本和保护数据隐私方面的巨大潜力。然而，复杂的物联网等网络系统的出现带来了传统联邦学习方法无法应对的新挑战。分层联邦学习将传统联邦学习扩展到更高效的模型聚合，以充分考虑应用需求或部署环境的特征（例如，资源能力和/或网络连接性），并平衡云-边缘计算的处理优势。分层联邦学习很可能成为各种应用（如智能农业和智能能源管理）的关键支撑，因为它可以提高性能并降低成本，同时也可以在传统联邦学习不适合的环境中部署联邦学习工作流程。模型聚合算法、软件框架等技术都在不断地演进。

    Federated learning has shown enormous promise as a way of training ML models in distributed environments while reducing communication costs and protecting data privacy. However, the rise of complex cyber-physical systems, such as the Internet-of-Things, presents new challenges that are not met with traditional FL methods. Hierarchical Federated Learning extends the traditional FL process to enable more efficient model aggregation based on application needs or characteristics of the deployment environment (e.g., resource capabilities and/or network connectivity). It illustrates the benefits of balancing processing across the cloud-edge continuum. Hierarchical Federated Learning is likely to be a key enabler for a wide range of applications, such as smart farming and smart energy management, as it can improve performance and reduce costs, whilst also enabling FL workflows to be deployed in environments that are not well-suited to traditional FL. Model aggregation algorithms, software fra
    
[^7]: MLCopilot：释放大语言模型在解决机器学习任务中的能力

    MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks. (arXiv:2304.14979v1 [cs.LG])

    [http://arxiv.org/abs/2304.14979](http://arxiv.org/abs/2304.14979)

    本文介绍了一种新型框架MLCopilot，通过利用最先进的大语言模型，扩展其能力以理解结构化输入并进行深入推理以解决新型ML任务的能力，展示了MLCopilot在解决图像分类，文本分类和表格分类三项任务方面的巨大潜力。

    

    机器学习（ML）领域受到了广泛的应用，因此逐渐引发了将ML应用于特定场景的需求，但实现起来耗时且不易。 自动化解决ML任务（例如AutoML）的主要方法通常耗费时间且难以理解。 而与之相反，虽然人类工程师具有理解任务和推理解决方案的难以置信的能力，但他们的经验和知识往往不充分且难以借助定量方法利用。 在本文中，我们旨在通过引入一种新颖的框架MLCopilot来弥合机器智能和人类知识之间的差距，该框架利用最先进的LLM来开发新型任务的ML解决方案。 我们展示了扩展LLM的能力以理解结构化输入并进行深入推理以解决新型ML任务的可能性。 经过一些专门设计后，我们发现LLM可以（i）从人类编写的文件中观察现有知识，（ii）制定解决ML任务的具体步骤。 我们对图像分类，文本分类和表格分类三项任务进行了实验，证明了MLCopilot在解决实际ML问题方面的巨大潜力。

    The field of machine learning (ML) has gained widespread adoption, leading to a significant demand for adapting ML to specific scenarios, which is yet expensive and non-trivial. The predominant approaches towards the automation of solving ML tasks (e.g., AutoML) are often time consuming and hard to understand for human developers. In contrast, though human engineers have the incredible ability to understand tasks and reason about solutions, their experience and knowledge are often sparse and difficult to utilize by quantitative approaches. In this paper, we aim to bridge the gap between machine intelligence and human knowledge by introducing a novel framework MLCopilot, which leverages the state-of-the-art LLMs to develop ML solutions for novel tasks. We showcase the possibility of extending the capability of LLMs to comprehend structured inputs and perform thorough reasoning for solving novel ML tasks. And we find that, after some dedicated design, the LLM can (i) observe from the exi
    
[^8]: 不准确注释下的质量自适应分裂式联邦学习在医学图像分割中的应用

    Quality-Adaptive Split-Federated Learning for Segmenting Medical Images with Inaccurate Annotations. (arXiv:2304.14976v1 [cs.CV])

    [http://arxiv.org/abs/2304.14976](http://arxiv.org/abs/2304.14976)

    本文提出了一种数据质量自适应的平均策略QA-SplitFed，能够有效应对客户端标签的偏见、不准确和不一致性，从而实现在不准确注释下的医学图像分割任务。实验结果表明，该方法在处理多个不准确客户端的情况下表现出更好的稳定性。

    

    SplitFed Learning是去中心化机器学习领域中最近的发展之一，是联邦学习和分裂式学习的组合。在本文中，我们提出了一种基于数据质量的自适应平均策略，命名为QA-SplitFed。该策略与5种现有的模型平均方法在胚胎图像分割任务上进行了比较。实验结果表明，QA-SplitFed在处理多个不准确客户端的情况下表现出更好的稳定性。

    SplitFed Learning, a combination of Federated and Split Learning (FL and SL), is one of the most recent developments in the decentralized machine learning domain. In SplitFed learning, a model is trained by clients and a server collaboratively. For image segmentation, labels are created at each client independently and, therefore, are subject to clients' bias, inaccuracies, and inconsistencies. In this paper, we propose a data quality-based adaptive averaging strategy for SplitFed learning, called QA-SplitFed, to cope with the variation of annotated ground truth (GT) quality over multiple clients. The proposed method is compared against five state-of-the-art model averaging methods on the task of learning human embryo image segmentation. Our experiments show that all five baseline methods fail to maintain accuracy as the number of corrupted clients increases. QA-SplitFed, however, copes effectively with corruption as long as there is at least one uncorrupted client.
    
[^9]: 论文标题：条件归一化流作为重加权的改进方法

    Flow Away your Differences: Conditional Normalizing Flows as an Improvement to Reweighting. (arXiv:2304.14963v1 [hep-ph])

    [http://arxiv.org/abs/2304.14963](http://arxiv.org/abs/2304.14963)

    本论文提出了一种新的方法，使用条件归一化流来修改分布，不依赖于分组选择和密度比的估计，从而更好地匹配目标分布。

    

    本文提出了一种不同于重重新加权技术的选择，用于修改分布以考虑基础条件分布的期望变化。我们使用条件归一化流来学习从中采样新事件的完整条件概率分布，以其目标分布为条件值产生所需的修改分布。与常见的重新加权技术相比，此过程不依赖于分组选择，并且不依赖于两个分布之间密度比的估计。

    We present an alternative to reweighting techniques for modifying distributions to account for a desired change in an underlying conditional distribution, as is often needed to correct for mis-modelling in a simulated sample. We employ conditional normalizing flows to learn the full conditional probability distribution from which we sample new events for conditional values drawn from the target distribution to produce the desired, altered distribution. In contrast to common reweighting techniques, this procedure is independent of binning choice and does not rely on an estimate of the density ratio between two distributions.  In several toy examples we show that normalizing flows outperform reweighting approaches to match the distribution of the target.We demonstrate that the corrected distribution closes well with the ground truth, and a statistical uncertainty on the training dataset can be ascertained with bootstrapping. In our examples, this leads to a statistical precision up to th
    
[^10]: PAO：一种带有精确动力学和闭式转移密度的通用粒子群算法

    PAO: A general particle swarm algorithm with exact dynamics and closed-form transition densities. (arXiv:2304.14956v1 [cs.NE])

    [http://arxiv.org/abs/2304.14956](http://arxiv.org/abs/2304.14956)

    本文提出了一种高度通用、可解释的PSO变体算法，即粒子吸引子算法（PAO），并设计了带有精确动力学和闭式转移密度。

    

    在元启发式优化方法的考虑中，已经进行了大量的研究，这些方法能够在传统梯度优化器难以处理的情况下找到全局最优解。其中，所谓的粒子群优化（PSO）方法在许多应用领域已被证明非常有效。鉴于PSO领域的成熟，新颖的PSO算法的性能只能带来微小的收益。相反，在追求具有其他有用性质的算法方面，研究工作更好被放置。在这项工作中，提出了一种高度通用、可解释的PSO变体算法——粒子吸引子算法（PAO）。此外，该算法的设计是为了使粒子从一个全局最优解到另一个全局最优解的运动过程能够描述出来。

    A great deal of research has been conducted in the consideration of meta-heuristic optimisation methods that are able to find global optima in settings that gradient based optimisers have traditionally struggled. Of these, so-called particle swarm optimisation (PSO) approaches have proven to be highly effective in a number of application areas. Given the maturity of the PSO field, it is likely that novel variants of the PSO algorithm stand to offer only marginal gains in terms of performance -- there is, after all, no free lunch. Instead of only chasing performance on suites of benchmark optimisation functions, it is argued herein that research effort is better placed in the pursuit of algorithms that also have other useful properties. In this work, a highly-general, interpretable variant of the PSO algorithm -- particle attractor algorithm (PAO) -- is proposed. Furthermore, the algorithm is designed such that the transition densities (describing the motions of the particles from one g
    
[^11]: 一项多模态模型融合的实证研究

    An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])

    [http://arxiv.org/abs/2304.14933](http://arxiv.org/abs/2304.14933)

    本研究通过融合在不同模态上训练的transformer进行多模态模型融合，并提出一种参数有效的模态不可知架构，形成有效的训练配方。

    

    模型融合（例如插值或任务算术）将在不同任务上训练的多个模型合并以生成多任务解决方案。该技术在先前的研究中已经被证明成功，其中模型是在相似的任务和相同的初始化下训练的。在本文中，我们通过将在不同模态上训练的transformer进行融合，将此概念扩展到多模态设置。此外，我们针对一个新颖的目标进行研究，在该目标中，我们可以将视觉、语言和跨模态的transformer合并到特定模态的架构中，以创建一个参数有效的模态不可知架构。通过全面的实验，我们系统地研究了影响模型融合后性能的关键因素，包括初始化、融合机制和模型架构。我们的分析得出了一个有效的训练配方，可以通过模型融合来匹配模态不可知基线的性能（即从头开始预训练）。我们的代码可供使用。

    Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
    
[^12]: 基于相似性和敏感性的不确定性感知神经网络

    Uncertainty Aware Neural Network from Similarity and Sensitivity. (arXiv:2304.14925v1 [cs.LG])

    [http://arxiv.org/abs/2304.14925](http://arxiv.org/abs/2304.14925)

    本文提出了一种神经网络训练方法，可以考虑相似样本与敏感性感知，有效地计算神经网络中的不确定性。

    

    计算神经网络（NN）不确定性的研究已有多种方法被提出。然而，大多数的方法都是基于强假设的，并且在某些输入领域中表现不佳，而表现不佳的原因尚不明确。因此，本文提出了一种考虑相似样本与敏感性感知的神经网络训练方法。在这种方法中，我们首先训练一个浅层神经网络进行点预测。然后，我们计算预测值和目标值之间的绝对差值，并训练另一个神经网络来预测这些绝对差值或者绝对误差。平均绝对误差较高的领域表示高度不确定性。在接下来的步骤中，我们逐个选择训练集中的每个样本，并计算预测和误差的敏感性。然后我们选择敏感性考虑的相似样本并保存相似样本的索引。

    Researchers have proposed several approaches for neural network (NN) based uncertainty quantification (UQ). However, most of the approaches are developed considering strong assumptions. Uncertainty quantification algorithms often perform poorly in an input domain and the reason for poor performance remains unknown. Therefore, we present a neural network training method that considers similar samples with sensitivity awareness in this paper. In the proposed NN training method for UQ, first, we train a shallow NN for the point prediction. Then, we compute the absolute differences between prediction and targets and train another NN for predicting those absolute differences or absolute errors. Domains with high average absolute errors represent a high uncertainty. In the next step, we select each sample in the training set one by one and compute both prediction and error sensitivities. Then we select similar samples with sensitivity consideration and save indexes of similar samples. The ra
    
[^13]: 一种通过解释性引导进行司机疲劳检测的脑电信号通道选择框架

    An EEG Channel Selection Framework for Driver Drowsiness Detection via Interpretability Guidance. (arXiv:2304.14920v1 [eess.SP])

    [http://arxiv.org/abs/2304.14920](http://arxiv.org/abs/2304.14920)

    本文提出了一种解释性引导的通道选择框架（ICS）用于司机疲劳检测任务，通过逐步选择关键的贡献通道，在提高司机疲劳检测准确性、可解释性和效率方面具有显著的效果。

    

    疲劳驾驶对行车安全有着至关重要的影响，促使人们迫切需要进行司机疲劳检测。脑电图（EEG）信号能够准确反映精神疲劳状态，因此在疲劳监测方面得到了广泛研究。然而，原始的EEG数据本质上是嘈杂且冗余的，这被现有的研究忽视了，这些研究仅使用单通道EEG数据或全头通道EEG数据进行模型训练，导致司机疲劳检测性能有限。本文首次提出了一种解释性引导的通道选择框架（ICS）用于司机疲劳检测任务。具体而言，我们设计了一个两阶段的训练策略，通过解释性引导逐步选择关键的贡献通道。我们首先在第一阶段使用全头通道EEG数据训练一个教师网络，然后对训练好的教师模型应用类激活映射（CAM）来突出显示对于疲劳检测任务有高贡献的通道。基于选择的关键通道，我们在第二阶段训练一个更轻量级的学生网络，它使用的通道数量少得多，同时实现了与最先进方法相当的性能。在两个公共数据集上的实验结果表明，我们的ICS框架在提高司机疲劳检测的准确性、可解释性和效率方面具有显著的效果。

    Drowsy driving has a crucial influence on driving safety, creating an urgent demand for driver drowsiness detection. Electroencephalogram (EEG) signal can accurately reflect the mental fatigue state and thus has been widely studied in drowsiness monitoring. However, the raw EEG data is inherently noisy and redundant, which is neglected by existing works that just use single-channel EEG data or full-head channel EEG data for model training, resulting in limited performance of driver drowsiness detection. In this paper, we are the first to propose an Interpretability-guided Channel Selection (ICS) framework for the driver drowsiness detection task. Specifically, we design a two-stage training strategy to progressively select the key contributing channels with the guidance of interpretability. We first train a teacher network in the first stage using full-head channel EEG data. Then we apply the class activation mapping (CAM) to the trained teacher model to highlight the high-contributing
    
[^14]: ScatterFormer: 一种局部不变的散射Transformer用于病人无关的多光谱癫痫放电检测

    ScatterFormer: Locally-Invariant Scattering Transformer for Patient-Independent Multispectral Detection of Epileptiform Discharges. (arXiv:2304.14919v1 [eess.SP])

    [http://arxiv.org/abs/2304.14919](http://arxiv.org/abs/2304.14919)

    本论文提出了一个名为ScatterFormer的模型，其通过局部不变的散射变换与解耦的频率感知注意力（FAA）来提取EEG模式特征，从而实现病人无关的多光谱癫痫放电检测。该模型展示了其在两个癫痫形放电检测任务上的有效性。

    

    基于连续脑电图（cEEG）的视觉光谱表示具有广泛的应用于诊断癫痫的病人无关的癫痫活动检测，但由于受到受试者、通道和时间点之间微妙变异的影响，精确检测仍然是一个巨大的挑战。因此，捕捉细粒度、有辨别力的EEG模式特征，尤其是与高频纹理信息相关的特征，仍然需要解决。在本文中，我们提出了Scattering Transformer（ScatterFormer），一种基于不变散射变换的层级Transformer，专门关注微妙的特征。特别地，我们提出了解耦的频率感知注意力（FAA）来捕捉临床有信息量的高频成分，提供基于多通道EEG信号的视觉编码的新型临床可解释性。对癫痫形放电检测的两个不同任务的评估证明了我们方法的有效性。

    Patient-independent detection of epileptic activities based on visual spectral representation of continuous EEG (cEEG) has been widely used for diagnosing epilepsy. However, precise detection remains a considerable challenge due to subtle variabilities across subjects, channels and time points. Thus, capturing fine-grained, discriminative features of EEG patterns, which is associated with high-frequency textural information, is yet to be resolved. In this work, we propose Scattering Transformer (ScatterFormer), an invariant scattering transform-based hierarchical Transformer that specifically pays attention to subtle features. In particular, the disentangled frequency-aware attention (FAA) enables the Transformer to capture clinically informative high-frequency components, offering a novel clinical explainability based on visual encoding of multichannel EEG signals. Evaluations on two distinct tasks of epileptiform detection demonstrate the effectiveness our method. Our proposed model 
    
[^15]: 通过脉搏波分析估计血压的挑战：能否承受？

    "Can't Take the Pressure?": Examining the Challenges of Blood Pressure Estimation via Pulse Wave Analysis. (arXiv:2304.14916v1 [eess.SP])

    [http://arxiv.org/abs/2304.14916](http://arxiv.org/abs/2304.14916)

    本文分析了脉搏波分析预测血压的任务，发现许多论文常常出现数据泄漏和对任务及预处理步骤的不切实际限制。提出了新的工具来确定输入信号（如PPG）是否能预测所需的量度。

    

    利用可穿戴传感器数据（如光电容积波形图[PPG]）推导健康量度（如葡萄糖水平或血压）是目前非常热门的研究领域。该技术可以对健康筛查、慢性病管理和远程监测产生重大影响。本文分析了从PPG脉搏波分析中预测血压的任务。我们的研究发现，许多论文容易出现数据泄漏以及对任务和预处理步骤的不切实际限制。因此，我们提出了一套工具来确定输入信号（如PPG）是否能够良好地预测所需的量度。

    The use of observed wearable sensor data (e.g., photoplethysmograms [PPG]) to infer health measures (e.g., glucose level or blood pressure) is a very active area of research. Such technology can have a significant impact on health screening, chronic disease management and remote monitoring. A common approach is to collect sensor data and corresponding labels from a clinical grade device (e.g., blood pressure cuff), and train deep learning models to map one to the other. Although well intentioned, this approach often ignores a principled analysis of whether the input sensor data has enough information to predict the desired metric. We analyze the task of predicting blood pressure from PPG pulse wave analysis. Our review of the prior work reveals that many papers fall prey data leakage, and unrealistic constraints on the task and the preprocessing steps. We propose a set of tools to help determine if the input signal in question (e.g., PPG) is indeed a good predictor of the desired label
    
[^16]: 使用自监督表示法识别穿戴数据的人类活动

    Human Activity Recognition Using Self-Supervised Representations of Wearable Data. (arXiv:2304.14912v1 [eess.SP])

    [http://arxiv.org/abs/2304.14912](http://arxiv.org/abs/2304.14912)

    本文提出了一个基于自监督表示法识别人类活动的模型，该模型在大量未标记数据集的基础上获得了较强性能，并在真实世界数据集上展现出了很好的效果。

    

    利用佩戴传感器进行自动化和准确的人类活动识别（HAR）可以实现实用和经济效益的日常生活活动（ADL）的远程监测，这些活动已被证明可提供跨多个治疗领域的临床洞见。准确地识别人类活动（HAR）的算法的发展受到缺乏大型真实世界标记数据集的阻碍。此外，算法很少能够超越它们原型上的特定传感器，引发了有关是否可能基于加速度计的HAR的争议[Tong等人，2020]。在这里，我们开发了一个六类HAR模型，当在训练期间未见过的真实世界数据集上进行评估时，具有较强的性能。我们的模型基于在大型未标记数据集上学习的冻结自监督表示法，结合具有时间平滑的浅层多层感知器。该模型在Capture24数据集[$\kappa$=0.86]内部数据集达到最先进的性能水平。分布外（OOD）表现

    Automated and accurate human activity recognition (HAR) using body-worn sensors enables practical and cost efficient remote monitoring of Activity of DailyLiving (ADL), which are shown to provide clinical insights across multiple therapeutic areas. Development of accurate algorithms for human activity recognition(HAR) is hindered by the lack of large real-world labeled datasets. Furthermore, algorithms seldom work beyond the specific sensor on which they are prototyped, prompting debate about whether accelerometer-based HAR is even possible [Tong et al., 2020]. Here we develop a 6-class HAR model with strong performance when evaluated on real-world datasets not seen during training. Our model is based on a frozen self-supervised representation learned on a large unlabeled dataset, combined with a shallow multi-layer perceptron with temporal smoothing. The model obtains in-dataset state-of-the art performance on the Capture24 dataset ($\kappa= 0.86$). Out-of-distribution (OOD) performan
    
[^17]: 用基于随机梯度下降的内点算法求解光滑有界约束优化问题

    A Stochastic-Gradient-based Interior-Point Algorithm for Solving Smooth Bound-Constrained Optimization Problems. (arXiv:2304.14907v1 [math.OC])

    [http://arxiv.org/abs/2304.14907](http://arxiv.org/abs/2304.14907)

    本文提出了一种用于求解光滑有界约束优化问题的内点算法。它使用基于随机梯度估计的搜索方向和内部邻域，能够在确定性和随机性设置下具有收敛保证，并且在实验中表现出优于传统方法的性能。

    

    本文提出并分析了一种基于随机梯度估计的内点算法，用于求解存在约束的连续可微非凸目标函数最小化问题，并通过实验结果进行了演示。该算法在解决光滑（非凸）优化问题时与其他内点方法不同之处在于搜索方向是通过随机梯度估计计算得到的。它在使用可行域的内部邻域（由正且消失的邻域参数序列定义）的过程中也很独特，通过将迭代强制保留在该邻域内。实验结果表明，通过精心平衡屏障、步长和邻域序列，该算法能够满足确定性和随机性设置下的收敛保证。在两种设置下，数值实验的结果表明，该算法可以优于投影-（随机）梯度方法。

    A stochastic-gradient-based interior-point algorithm for minimizing a continuously differentiable objective function (that may be nonconvex) subject to bound constraints is presented, analyzed, and demonstrated through experimental results. The algorithm is unique from other interior-point methods for solving smooth (nonconvex) optimization problems since the search directions are computed using stochastic gradient estimates. It is also unique in its use of inner neighborhoods of the feasible region -- defined by a positive and vanishing neighborhood-parameter sequence -- in which the iterates are forced to remain. It is shown that with a careful balance between the barrier, step-size, and neighborhood sequences, the proposed algorithm satisfies convergence guarantees in both deterministic and stochastic settings. The results of numerical experiments show that in both settings the algorithm can outperform a projected-(stochastic)-gradient method.
    
[^18]: 提高供应链弹性：基于机器学习的预测受干扰影响下产品可用性日期的方法研究

    Enhancing Supply Chain Resilience: A Machine Learning Approach for Predicting Product Availability Dates Under Disruption. (arXiv:2304.14902v1 [cs.LG])

    [http://arxiv.org/abs/2304.14902](http://arxiv.org/abs/2304.14902)

    该论文以通用电气 (GE) 燃气动力公司的气体和蒸汽涡轮服务和制造业务为例，研究了基于机器学习的产品可用性日期预测方法。研究结果显示，梯度提升机 (GBM) 模型表现最佳，平均绝对误差为 4.93 天，均方根误差为 6.47 天。研究还通过特征重要性分析确定了影响产品可用性日期的关键因素。

    

    新冠疫情和持续存在的政治和地区冲突对全球供应链产生了极为不利的影响，导致物流操作和国际货运中出现了显著的延误。其中最迫切的问题之一是产品可用性日期的不确定性，这是公司制定有效物流和运输计划的关键信息。因此，准确地预测可用性日期在执行成功的物流操作中发挥着关键作用，最终可以将总运输和库存成本最小化。我们研究通用电气（GE）燃气动力公司的气体和蒸汽涡轮服务和制造业务的货物可用性日期预测，利用数字特征和类别特征。我们评估了几种回归模型，包括简单回归、套索回归、岭回归、弹性网络、随机森林（RF）、梯度提升机（GBM）和神经网络（NM）并比较它们的性能。我们的结果表明，GBM模型表现优于其他模型，实现了4.93天的平均绝对误差和6.47天的均方根误差。此外，我们进行了特征重要性分析，并确定了影响产品可用性日期的关键因素。我们的研究为通过准确的产品可用性预测提高供应链弹性提供了见解，这可以扩展到其他行业和环境中。

    The COVID 19 pandemic and ongoing political and regional conflicts have a highly detrimental impact on the global supply chain, causing significant delays in logistics operations and international shipments. One of the most pressing concerns is the uncertainty surrounding the availability dates of products, which is critical information for companies to generate effective logistics and shipment plans. Therefore, accurately predicting availability dates plays a pivotal role in executing successful logistics operations, ultimately minimizing total transportation and inventory costs. We investigate the prediction of product availability dates for General Electric (GE) Gas Power's inbound shipments for gas and steam turbine service and manufacturing operations, utilizing both numerical and categorical features. We evaluate several regression models, including Simple Regression, Lasso Regression, Ridge Regression, Elastic Net, Random Forest (RF), Gradient Boosting Machine (GBM), and Neural 
    
[^19]: 有类型仿射决策结构的威力：一项案例研究

    The Power of Typed Affine Decision Structures: A Case Study. (arXiv:2304.14888v1 [cs.LG])

    [http://arxiv.org/abs/2304.14888](http://arxiv.org/abs/2304.14888)

    TADS是一种新的神经网络表示方法，可用于生成有关神经网络属性的证明或者误差特征，本文通过一个案例研究表明TADS可以用于提供关于神经网络鲁棒性错误的精确诊断。

    

    TADS是一种新颖、简洁的神经网络白盒表示。本文将TADS应用于神经网络验证问题，使用TADS生成有关神经网络属性的证明或简洁误差特征。在一个案例研究中，我们考虑了神经网络对抗攻击（即输入微小变化导致神经网络知觉大幅度改变）的鲁棒性，并展示了TADS可以用于提供关于鲁棒性错误如何及其发生在何处的精确诊断。

    TADS are a novel, concise white-box representation of neural networks. In this paper, we apply TADS to the problem of neural network verification, using them to generate either proofs or concise error characterizations for desirable neural network properties. In a case study, we consider the robustness of neural networks to adversarial attacks, i.e., small changes to an input that drastically change a neural networks perception, and show that TADS can be used to provide precise diagnostics on how and where robustness errors a occur. We achieve these results by introducing Precondition Projection, a technique that yields a TADS describing network behavior precisely on a given subset of its input space, and combining it with PCA, a traditional, well-understood dimensionality reduction technique. We show that PCA is easily compatible with TADS. All analyses can be implemented in a straightforward fashion using the rich algebraic properties of TADS, demonstrating the utility of the TADS fr
    
[^20]: ACM Multimedia 2023 计算语言学挑战赛：情感共享与请求

    The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share & Requests. (arXiv:2304.14882v1 [cs.SD])

    [http://arxiv.org/abs/2304.14882](http://arxiv.org/abs/2304.14882)

    ACM Multimedia 2023 计算语言学挑战赛涉及情感共享和请求检测，提供了基线特征提取和分类器方法。

    

    ACM Multimedia 2023 计算语言学挑战赛首次在明确定义的条件下解决了两个不同的问题：在情感共享子挑战中需要对语音进行回归，而在请求子挑战中需要检测请求和抱怨。我们描述了子挑战、基线特征提取和分类器，其中包括通常的 ComPaRE 特征、auDeep 工具包和使用预训练 CNN 的深度特征提取以及 wav2vec2 模型的使用。

    The ACM Multimedia 2023 Computational Paralinguistics Challenge addresses two different problems for the first time in a research competition under well-defined conditions: In the Emotion Share Sub-Challenge, a regression on speech has to be made; and in the Requests Sub-Challenges, requests and complaints need to be detected. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the usual ComPaRE features, the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the DeepSpectRum toolkit; in addition, wav2vec2 models are used.
    
[^21]: Deep Stock: 使用深度学习进行训练和交易的方案

    Deep Stock: training and trading scheme using deep learning. (arXiv:2304.14870v1 [q-fin.ST])

    [http://arxiv.org/abs/2304.14870](http://arxiv.org/abs/2304.14870)

    本文提出了一种使用深度学习进行训练和交易的方案，DeepStock通过查看股票价格的过去数据，并使用Resnet和logits来预测股票价格在未来D天内是否会升降一定百分比，并在韩国和美国市场上取得了超过市场回报的利润。

    

    尽管有效市场假说存在，但许多研究表明股票市场存在失灵现象，导致出现了一些能够获得超过市场回报的技术，即alpha。近几十年来，系统性交易已经取得了重大进展，深度学习作为分析和预测市场行为的强大工具已经开始崭露头角。本文中，我们提出了一种受专业交易员启发的模型，该模型查看先前的600天的股票价格，并预测股票价格在接下来D天内是否会升降一定百分比。我们的模型称为DeepStock，使用Resnet的跳跃连接和logits来增加模型在交易方案中的概率。我们在韩国和美国股票市场上测试了我们的模型，并在韩国市场上获得了N％的利润，超过市场回报M％，并在美国市场上获得了A％的利润，超过市场回报B％。

    Despite the efficient market hypothesis, many studies suggest the existence of inefficiencies in the stock market, leading to the development of techniques to gain above-market returns, known as alpha. Systematic trading has undergone significant advances in recent decades, with deep learning emerging as a powerful tool for analyzing and predicting market behavior. In this paper, we propose a model inspired by professional traders that look at stock prices of the previous 600 days and predicts whether the stock price rises or falls by a certain percentage within the next D days. Our model, called DeepStock, uses Resnet's skip connections and logits to increase the probability of a model in a trading scheme. We test our model on both the Korean and US stock markets and achieve a profit of N\% on Korea market, which is M\% above the market return, and profit of A\% on US market, which is B\% above the market return.
    
[^22]: 评估CNN中语义概念表示的稳定性，以实现强大的可解释性

    Evaluating the Stability of Semantic Concept Representations in CNNs for Robust Explainability. (arXiv:2304.14864v1 [cs.AI])

    [http://arxiv.org/abs/2304.14864](http://arxiv.org/abs/2304.14864)

    本文提出了一种用于评估语义概念表示在CNN中稳定性的方法，为实现强大的可解释性提供了基础。本文关注计算机视觉CNN中概念表示的稳定性：概念检索稳定性和概念归属稳定性。在此基础上，本文提出了一种新的度量标准以解决概念检索稳定性的问题。

    

    在可解释的人工智能（XAI）中，分析卷积神经网络（CNNs）中语义概念的表示是一种广泛使用的方法。其动机是因为各个领域如自动驾驶等安全关键的基于AI的系统需要透明度。然而，要将这些概念表示用于安全相关目的，例如检查或错误检索，这些表示必须具有高质量，特别是稳定性。本文关注计算机视觉CNN中概念表示的稳定性：概念检索稳定性和概念归属稳定性。以目标检测（OD）CNN的事后可解释性框架为指导目标，成功地将现有的概念分析（CA）方法应用于其上。为了解决概念检索的稳定性问题，我们提出了一种新的度量标准，考虑概念分离和一致性，与层和概念表示无关。

    Analysis of how semantic concepts are represented within Convolutional Neural Networks (CNNs) is a widely used approach in Explainable Artificial Intelligence (XAI) for interpreting CNNs. A motivation is the need for transparency in safety-critical AI-based systems, as mandated in various domains like automated driving. However, to use the concept representations for safety-relevant purposes, like inspection or error retrieval, these must be of high quality and, in particular, stable. This paper focuses on two stability goals when working with concept representations in computer vision CNNs: stability of concept retrieval and of concept attribution. The guiding use-case is a post-hoc explainability framework for object detection (OD) CNNs, towards which existing concept analysis (CA) methods are successfully adapted. To address concept retrieval stability, we propose a novel metric that considers both concept separation and consistency, and is agnostic to layer and concept representati
    
[^23]: 基于Wasserstein字典的持久图的紧凑编码

    Wasserstein Dictionaries of Persistence Diagrams. (arXiv:2304.14852v1 [cs.LG])

    [http://arxiv.org/abs/2304.14852](http://arxiv.org/abs/2304.14852)

    本文提出了一种基于Wasserstein字典的持久图的紧凑编码方法，并在实验中证明了其有效性，可以应用于数据降维和压缩。

    

    本文提出了一个计算框架，用于以原子图字典的加权Wasserstein barycenters [99]，[101] 的形式对一组持久图进行简洁编码。我们引入了一种多尺度梯度下降方法，用于有效解决相应的最小化问题，该方法将Barycenter权重的优化与Atom图的优化交错进行。我们的方法利用了两个子问题梯度的解析表达式以确保快速迭代，并且还利用了共享内存并行性。对公共合奏的广泛实验证明了我们方法的有效性，最大示例的Wasserstein字典计算时间在数分钟之内。我们在两个应用中展示了我们的贡献的效用。首先，我们将Wassserstein字典应用于数据降维，并通过仅用其重量来紧凑地表示Persistence图来可靠地压缩它们。

    This paper presents a computational framework for the concise encoding of an ensemble of persistence diagrams, in the form of weighted Wasserstein barycenters [99], [101] of a dictionary of atom diagrams. We introduce a multi-scale gradient descent approach for the efficient resolution of the corresponding minimization problem, which interleaves the optimization of the barycenter weights with the optimization of the atom diagrams. Our approach leverages the analytic expressions for the gradient of both sub-problems to ensure fast iterations and it additionally exploits shared-memory parallelism. Extensive experiments on public ensembles demonstrate the efficiency of our approach, with Wasserstein dictionary computations in the orders of minutes for the largest examples. We show the utility of our contributions in two applications. First, we apply Wassserstein dictionaries to data reduction and reliably compress persistence diagrams by concisely representing them with their weights in t
    
[^24]: 将音乐人声分离建模为链路预测：将音乐感知任务建模为多轨迹跟踪问题

    Musical Voice Separation as Link Prediction: Modeling a Musical Perception Task as a Multi-Trajectory Tracking Problem. (arXiv:2304.14848v1 [cs.SD])

    [http://arxiv.org/abs/2304.14848](http://arxiv.org/abs/2304.14848)

    本文提出了一种将音乐人声分离的感知任务建模为多轨迹跟踪问题的方法，通过预测音符之间的链接将不同的人声进行分离，从而鼓励单声部（人声）轨迹的产生。

    

    本文旨在解决在多声部音乐中分离不同人声（即单声部旋律流）的感知问题。我们针对符号音乐，即明确编码音符，将此任务建模为从离散观测（即音高-时间空间中的音符）中的多轨迹跟踪问题。我们的方法通过为每个音符创建一个节点来构建音乐片段的图形，并通过预测两个音符之间的链接来分离旋律轨迹，如果它们在同一声部/流中连续。这种局部，贪心的预测是由异质图神经网络创建的节点嵌入所实现的，该节点嵌入可以捕捉轨迹之间和轨迹内的信息。此外，我们提出了一种新的正则化损失，鼓励输出遵守每个节点最多有一个入口和一个出口链接的多轨迹跟踪前提，支持单声部（人声）轨迹；这种损失函数在其他生成序列设置中也可能有用。

    This paper targets the perceptual task of separating the different interacting voices, i.e., monophonic melodic streams, in a polyphonic musical piece. We target symbolic music, where notes are explicitly encoded, and model this task as a Multi-Trajectory Tracking (MTT) problem from discrete observations, i.e., notes in a pitch-time space. Our approach builds a graph from a musical piece, by creating one node for every note, and separates the melodic trajectories by predicting a link between two notes if they are consecutive in the same voice/stream. This kind of local, greedy prediction is made possible by node embeddings created by a heterogeneous graph neural network that can capture inter- and intra-trajectory information. Furthermore, we propose a new regularization loss that encourages the output to respect the MTT premise of at most one incoming and one outgoing link for every node, favouring monophonic (voice) trajectories; this loss function might also be useful in other gener
    
[^25]: 使用同态加密对大规模CNN进行敏感调整以进行端到端安全预测

    Sensitive Tuning of Large Scale CNNs for E2E Secure Prediction using Homomorphic Encryption. (arXiv:2304.14836v1 [cs.LG])

    [http://arxiv.org/abs/2304.14836](http://arxiv.org/abs/2304.14836)

    本论文提出一种新的HE友好模型训练方法，成功演示了在ResNet和ConvNeXt等经典和现代CNN上运行加密样本，并以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测，是一种可行的隐私保护机器学习解决方案。

    

    隐私保护的机器学习解决方案近来受到了广泛关注。其中一种有前途的研究趋势是使用同态加密（HE），这是一种在加密数据上执行计算的方法。这种方法的一个主要挑战是训练适用于HE的加密或未加密的深层CNN，以实现良好的准确性。我们提出了一种新的HE友好模型训练方法，并在基本和现代CNN上进行了演示，例如ResNet和ConvNeXt。训练后，我们使用HELayers SDK运行加密样本来评估我们的模型，并证明它们产生了所需的结果。在ImageNet数据集上运行时，我们的ResNet-18/50/101实现仅需要7、31和57分钟，这表明这个解决方案是实用的。此外，我们提供了一些关于在HE下处理激活函数和跳跃连接的见解。最后，我们以前所未有的方式演示了如何使用CLIP GPT-2模型进行零知识安全预测。

    Privacy-preserving machine learning solutions have recently gained significant attention. One promising research trend is using Homomorphic Encryption (HE), a method for performing computation over encrypted data. One major challenge in this approach is training HE-friendly, encrypted or unencrypted, deep CNNs with decent accuracy. We propose a novel training method for HE-friendly models, and demonstrate it on fundamental and modern CNNs, such as ResNet and ConvNeXt. After training, we evaluate our models by running encrypted samples using HELayers SDK and proving that they yield the desired results. When running on a GPU over the ImageNet dataset, our ResNet-18/50/101 implementations take only 7, 31 and 57 minutes, respectively, which shows that this solution is practical. Furthermore, we present several insights on handling the activation functions and skip-connections under HE. Finally, we demonstrate in an unprecedented way how to perform secure zero-shot prediction using a CLIP m
    
[^26]: 从限制性反馈中提高模型性能

    Earning Extra Performance from Restrictive Feedbacks. (arXiv:2304.14831v1 [cs.LG])

    [http://arxiv.org/abs/2304.14831](http://arxiv.org/abs/2304.14831)

    本文提出了一个名为EXPECTED的挑战，解决模型调整问题，模型提供者可以通过来自本地用户的反馈多次访问候选模型的操作性能，从而优化模型，同时不需要依赖目标数据。

    

    许多机器学习应用程序面临这样一种情况：模型提供者需要进一步改进先前训练的模型以满足本地用户的特定需求。如果可以将目标数据传递给模型，那么这个问题就转化为标准的模型调整范例。然而，在许多实际应用中，目标数据并不共享给模型提供者，而只是一些关于模型的评估可供访问。本文提出了一个名为EXPECTED（Earning eXtra PerformancE from restriCTive feEDdbacks）的挑战，正式描述了这种形式的模型调整问题，允许模型提供者通过来自本地用户（或一组用户）的反馈多次访问候选模型的操作性能。模型提供者的目标是通过利用反馈最终向本地用户提供令人满意的模型。与现有的模型调整方法不同，EXPECTED在不依赖于目标数据的情况下实现了模型优化。

    Many machine learning applications encounter a situation where model providers are required to further refine the previously trained model so as to gratify the specific need of local users. This problem is reduced to the standard model tuning paradigm if the target data is permissibly fed to the model. However, it is rather difficult in a wide range of practical cases where target data is not shared with model providers but commonly some evaluations about the model are accessible. In this paper, we formally set up a challenge named \emph{Earning eXtra PerformancE from restriCTive feEDdbacks} (EXPECTED) to describe this form of model tuning problems. Concretely, EXPECTED admits a model provider to access the operational performance of the candidate model multiple times via feedback from a local user (or a group of users). The goal of the model provider is to eventually deliver a satisfactory model to the local user(s) by utilizing the feedbacks. Unlike existing model tuning methods wher
    
[^27]: 一种抗噪声的声学方法用于识别牛的觅食活动

    A noise-robust acoustic method for recognition of foraging activities of grazing cattle. (arXiv:2304.14824v1 [cs.LG])

    [http://arxiv.org/abs/2304.14824](http://arxiv.org/abs/2304.14824)

    本研究提出了一种抗噪声的声学方法，能够分析与吃草和反刍相关的鉴定下颚运动事件的固定长度段，用于识别牛的觅食活动，并在环境和自然噪声方面具有鲁棒性。

    

    为了在不断增长的乳制品市场中保持竞争力，农民必须不断改进他们的畜牧生产系统。精确畜牧业技术提供了商业农场动物个体化监测，优化畜牧生产。连续的声学监测是一种广泛接受的感应技术，用于估计自由放牧牛的日反刍和吃草时间预算。然而，牧场上的典型环境和自然噪声明显影响当前声学方法的性能和泛化。在本研究中，我们提出了一种声学方法，称为抗噪声觅食活动识别器 (NRFAR)。该方法通过分析与吃草和反刍相关的鉴定下颚运动事件的固定长度段，确定觅食活动的突发。NRFAR 的加性噪声鲁棒性使用静态高斯白噪声和四种不同的非静态自然噪声进行评估。

    To stay competitive in the growing dairy market, farmers must continuously improve their livestock production systems. Precision livestock farming technologies provide individualised monitoring of animals on commercial farms, optimising livestock production. Continuous acoustic monitoring is a widely accepted sensing technique used to estimate the daily rumination and grazing time budget of free-ranging cattle. However, typical environmental and natural noises on pasture noticeably affect the performance and generalisation of current acoustic methods. In this study, we present an acoustic method called Noise-Robust Foraging Activity Recognizer (NRFAR). The proposed method determines foraging activity bouts by analysing fixed-length segments of identified jaw movement events associated with grazing and rumination. The additive noise robustness of NRFAR was evaluated for several signal-to-noise ratios, using stationary Gaussian white noise and four different non-stationary natural noise 
    
[^28]: 一种基于深度学习辅助微波等离子体相互作用技术的等离子体密度估计方法

    Deep Learning assisted microwave-plasma interaction based technique for plasma density estimation. (arXiv:2304.14807v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2304.14807](http://arxiv.org/abs/2304.14807)

    本文提出了一种基于深度学习辅助微波等离子体相互作用技术的等离子体密度估计方法，通过测量微波散射引起的电场模式来估计密度剖面。

    

    电子密度是表征任何等离子体的关键参数。低温等离子体领域的大部分应用和研究都基于等离子体密度和等离子体温度。传统的电子密度测量方法针对给定线性低温等离子体设备提供轴向和径向剖面。这些方法存在操作范围较小、仪器沉重以及数据分析过程复杂等主要缺点。为了应对这些实际问题，本文提出了一种新颖的机器学习（ML）辅助微波等离子体相互作用的策略，该策略能够确定等离子体内电子密度剖面。通过测量微波散射引起的电场模式来估计密度剖面。该策略针对一个模拟的训练数据集进行了概念验证，其中包括低温、非磁化和碰撞性等离子体的不同类型的高斯形状密度剖面范围内。

    The electron density is a key parameter to characterize any plasma. Most of the plasma applications and research in the area of low-temperature plasmas (LTPs) is based on plasma density and plasma temperature. The conventional methods for electron density measurements offer axial and radial profiles for any given linear LTP device. These methods have major disadvantages of operational range (not very wide), cumbersome instrumentation, and complicated data analysis procedures. To address such practical concerns, the article proposes a novel machine learning (ML) assisted microwave-plasma interaction based strategy which is capable enough to determine the electron density profile within the plasma. The electric field pattern due to microwave scattering is measured to estimate the density profile. The proof of concept is tested for a simulated training data set comprising a low-temperature, unmagnetized, collisional plasma. Different types of Gaussian-shaped density profiles, in the range
    
[^29]: ResiDual：具有双重残差连接的Transformer

    ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])

    [http://arxiv.org/abs/2304.14802](http://arxiv.org/abs/2304.14802)

    本文提出了具有Pre-Post-LN双重残差连接的新型Transformer架构ResiDual，解决了Post-LN和Pre-LN存在的问题，并具有优越的性能表现。

    

    由于其卓越的性能，Transformer网络已成为许多任务的首选架构。然而，如何最优化地实现Transformer中的残差连接仍存在争议，而这些残差连接对于有效训练是必不可少的。两个广泛使用的变体是Post-Layer-Normalization(Post-LN)和Pre-Layer-Normalization(Pre-LN) Transformers，它们分别在每个残差块的输出之后或输入之前应用层规范化。尽管两种变体都有它们的优点，但也存在严重的局限性：Post-LN会导致梯度消失问题，从而阻碍训练深层Transformer，而Pre-LN会导致表示崩溃问题，限制模型容量。本文提出了一种新颖的Transformer架构ResiDual，具有Pre-Post-LN(PPLN)，它将Post-LN和Pre-LN中的连接融合在一起，继承了它们的优点，同时避免了它们的局限性。我们进行了理论分析和实证评估，表明ResiDual比现有方法优越，尤其是训练非常深的Transformer。

    Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empiric
    
[^30]: MCPrioQ: 一种用于在线稀疏Markov链的无锁算法

    MCPrioQ: A lock-free algorithm for online sparse markov-chains. (arXiv:2304.14801v1 [cs.LG])

    [http://arxiv.org/abs/2304.14801](http://arxiv.org/abs/2304.14801)

    本文提出了MCPrioQ算法，一种无锁稀疏Markov链数据结构，用于在线和连续学习，特别适用于推荐系统，时间复杂度为$O(1)$用于更新和$O(CDF^{-1}(t))$用于推理。

    

    在高性能系统中，构建内存和计算效率高的大型图形有时很困难。本文提出了一种名为Markov-chain-priority-queue(MCPrioQ)的数据结构，它是一种无锁稀疏Markov链，可以实现在线和连续学习，时间复杂度为$O（1）$用于更新和$O(CDF^{-1}(t))$用于推理。MCPrioQ特别适用于推荐系统，以降序概率顺序查找n个项目。并发更新是通过哈希表和原子指令实现的，查找是通过一种新颖的优先级队列实现的，即使在并发更新期间，也可以得到近似正确的结果。近似正确和无锁定属性由读取-复制-更新方案维护，但语义已略微更新，以允许元素的交换而不是传统的弹出-插入方案。

    In high performance systems it is sometimes hard to build very large graphs that are efficient both with respect to memory and compute. This paper proposes a data structure called Markov-chain-priority-queue (MCPrioQ), which is a lock-free sparse markov-chain that enables online and continuous learning with time-complexity of $O(1)$ for updates and $O(CDF^{-1}(t))$ inference. MCPrioQ is especially suitable for recommender-systems for lookups of $n$-items in descending probability order. The concurrent updates are achieved using hash-tables and atomic instructions and the lookups are achieved through a novel priority-queue which allows for approximately correct results even during concurrent updates. The approximatly correct and lock-free property is maintained by a read-copy-update scheme, but where the semantics have been slightly updated to allow for swap of elements rather than the traditional pop-insert scheme.
    
[^31]: 利用精确压缩学习图神经网络

    Learning Graph Neural Networks using Exact Compression. (arXiv:2304.14793v1 [cs.LG])

    [http://arxiv.org/abs/2304.14793](http://arxiv.org/abs/2304.14793)

    本文研究了利用精确压缩来减少在大型图上学习图神经网络的内存需求，并提出了一种证明等效的压缩图神经网络学习问题的方法。

    

    图神经网络是一种深度学习技术，可以对图结构数据进行广泛的机器学习应用。学习这种网络，然而，对于内存受限的设备（如GPU）来说是一大挑战。本文研究了利用精确压缩来减少在大型图上学习图神经网络的内存需求。具体而言，我们采用了一种形式化压缩方法，并提出了一种方法，将图神经网络学习问题转化为证明等效的压缩图神经网络学习问题。在初步的实验评估中，我们洞察了真实世界图上可以获得的压缩比，并将我们的方法应用于现有的一个图神经网络基准测试。

    Graph Neural Networks (GNNs) are a form of deep learning that enable a wide range of machine learning applications on graph-structured data. The learning of GNNs, however, is known to pose challenges for memory-constrained devices such as GPUs. In this paper, we study exact compression as a way to reduce the memory requirements of learning GNNs on large graphs. In particular, we adopt a formal approach to compression and propose a methodology that transforms GNN learning problems into provably equivalent compressed GNN learning problems. In a preliminary experimental evaluation, we give insights into the compression ratios that can be obtained on real-world graphs and apply our methodology to an existing GNN benchmark.
    
[^32]: 多样本流匹配：利用小批量耦合将流进行矫正

    Multisample Flow Matching: Straightening Flows with Minibatch Couplings. (arXiv:2304.14772v1 [cs.LG])

    [http://arxiv.org/abs/2304.14772](http://arxiv.org/abs/2304.14772)

    该论文提出了一种多样本流匹配算法，在满足正确的边缘约束的条件下，利用小批量耦合将流进行矫正，从而使生成模型的训练更加高效，并获得更高质量、更低维代价的运输图。

    

    无需模拟的连续时间生成模型训练方法构建了从噪声分布到单个数据样本的概率路径。最近的作品，如流匹配，导出了最适合每个数据样本的路径。然而，这些算法依赖于独立的数据和噪声样本，并且不利用数据分布中的基础结构来构建概率路径。我们提出了多样本流匹配，这是一个更通用的框架，使用数据和噪声样本之间的非平凡耦合，同时满足正确的边缘约束。在非常小的开销下，这种泛化使我们能够(i) 在训练过程中降低梯度方差，(ii) 获得更加直接的流，这使我们可以使用更少的函数评估生成高质量的样本，(iii) 获得更低维代价的运输图，这在生成模型之外也有应用。重要的是，我们以概念上简单的方式实现了这一点，基于一种新颖的小批量耦合层。

    Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. At very small overhead costs, this generalization allows us to (i) reduce gradient variance during training, (ii) obtain straighter flows for the learned vector field, which allows us to generate high-quality samples using fewer function evaluations, and (iii) obtain transport maps with lower cost in high dimensions, which has applications beyond generative modeling. Importantly, we do so in a c
    
[^33]: 通过神经网络分割进行超参数优化

    Hyperparameter Optimization through Neural Network Partitioning. (arXiv:2304.14766v1 [cs.LG])

    [http://arxiv.org/abs/2304.14766](http://arxiv.org/abs/2304.14766)

    本文提出了一种将训练数据和神经网络模型分区的方法，将每个分区与特定的数据片段关联并进行优化，通过优化这些分区的子网络的“训练之外的样本”损失，实现了在单次训练运行中降低超参数优化代价的效果。

    

    调整恰当的超参对于获得神经网络中良好的泛化行为至关重要。它们可以强制适当的归纳偏差，正则化模型并提高性能，特别是在有限的数据情况下。在本文中，我们提出了一种简单而有效的方法来优化超参数，该方法受边缘似然的启发，这是一种不需要验证数据的优化目标。我们的方法将训练数据和神经网络模型分为 K 个数据分片和参数分区。每个分区仅与特定的数据片段关联并进行优化。将这些分区组合成子网络使我们能够将子网络的“训练之外的样本”损失定义为超参数优化的目标，即在子网络看不到的数据片段上计算损失。我们证明，我们可以将这个目标应用到单次训练运行中，同时显着降低超参数优化的代价。

    Well-tuned hyperparameters are crucial for obtaining good generalization behavior in neural networks. They can enforce appropriate inductive biases, regularize the model and improve performance -- especially in the presence of limited data. In this work, we propose a simple and efficient way for optimizing hyperparameters inspired by the marginal likelihood, an optimization objective that requires no validation data. Our method partitions the training data and a neural network model into $K$ data shards and parameter partitions, respectively. Each partition is associated with and optimized only on specific data shards. Combining these partitions into subnetworks allows us to define the ``out-of-training-sample" loss of a subnetwork, i.e., the loss on data shards unseen by the subnetwork, as the objective for hyperparameter optimization. We demonstrate that we can apply this objective to optimize a variety of different hyperparameters in a single training run while being significantly c
    
[^34]: 利用扰动来改善基于核化斯坦距的拟合优度检验

    Using Perturbation to Improve Goodness-of-Fit Tests based on Kernelized Stein Discrepancy. (arXiv:2304.14762v1 [stat.ML])

    [http://arxiv.org/abs/2304.14762](http://arxiv.org/abs/2304.14762)

    本文提出了一种通过在样本中引入扰动，改进基于核化斯坦距的拟合优度检验方法的方法，以解决在同质但混合比例不同的情况下低功率的问题，并展示实验证据证明了该方法的功效。

    

    核化斯坦距（KSD）是一种广泛用于拟合优度检验的基于得分的差异度量。即使目标分布具有未知的标准化因子，例如在贝叶斯分析中，也可以应用它。我们理论上和实验证明，当目标分布和替代分布具有相同且相距较远的模式但在混合比例上有所不同时，KSD检验可能会出现低功率问题。我们提出通过马尔科夫转移核对观测样本进行扰动，使其相对于目标分布不变。这使我们可以在扰动样本上使用KSD检验。我们提供的数值证据表明，使用适当选择的核时，所提出的方法可以比KSD检验具有更高的功率。

    Kernelized Stein discrepancy (KSD) is a score-based discrepancy widely used in goodness-of-fit tests. It can be applied even when the target distribution has an unknown normalising factor, such as in Bayesian analysis. We show theoretically and empirically that the KSD test can suffer from low power when the target and the alternative distribution have the same well-separated modes but differ in mixing proportions. We propose to perturb the observed sample via Markov transition kernels, with respect to which the target distribution is invariant. This allows us to then employ the KSD test on the perturbed sample. We provide numerical evidence that with suitably chosen kernels the proposed approach can lead to a substantially higher power than the KSD test.
    
[^35]: 具有非二元特征的分类器的新型解释方法

    A New Class of Explanations for Classifiers with Non-Binary Features. (arXiv:2304.14760v1 [cs.AI])

    [http://arxiv.org/abs/2304.14760](http://arxiv.org/abs/2304.14760)

    本文提出了一种适用于具有非二元特征的分类器的新型解释方法，可以提供更多关于决策和基础分类器的信息。

    

    近来，当分析分类器决策时，已经有两种类型的解释受到了文献中的重视。第一种解释是为决策提供充分理由的解释，即缩写为PI解释的诱导式解释；第二种解释是为何不做出其他决策的解释，即对照式或反事实解释的必要理由。这些解释是为二元、离散和在某些情况下为连续特征的分类器定义的。我们展示了当存在非二元特征时，这些解释可以得到显著的改进，从而导致了一类新的解释方法，可以提供更多关于决策和基础分类器的信息。必要和充分原因也被证明是完整原因的主要蕴含项和被蕴含项，可以使用量化算子获得。我们的结果表明，我们改进的必要和充分原因的概念比现有方法更好地适用于具有非二元特征的分类器。

    Two types of explanations have received significant attention in the literature recently when analyzing the decisions made by classifiers. The first type explains why a decision was made and is known as a sufficient reason for the decision, also an abductive or PI-explanation. The second type explains why some other decision was not made and is known as a necessary reason for the decision, also a contrastive or counterfactual explanation. These explanations were defined for classifiers with binary, discrete and, in some cases, continuous features. We show that these explanations can be significantly improved in the presence of non-binary features, leading to a new class of explanations that relay more information about decisions and the underlying classifiers. Necessary and sufficient reasons were also shown to be the prime implicates and implicants of the complete reason for a decision, which can be obtained using a quantification operator. We show that our improved notions of necessa
    
[^36]: 面向优化不可分解指标的代价敏感自训练

    Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics. (arXiv:2304.14738v1 [cs.LG])

    [http://arxiv.org/abs/2304.14738](http://arxiv.org/abs/2304.14738)

    本研究提出了代价敏感自训练（CSST）框架，可以更好地利用未标记的数据优化不可分解指标，为处理具有复杂目标的实际机器学习系统提供了实用的解决方案。

    

    基于自训练的半监督学习算法使得仅使用少量标记数据就能学习到高精度的深度神经网络。然而，大多数自训练的工作都集中在提高精度的目标上，而实际的机器学习系统可能具有不可分解的复杂目标（例如，最大化不同类别召回率的最小值等）。在这项工作中，我们引入了代价敏感自训练（CSST）框架，该框架推广了用于优化不可分解指标的基于自训练的方法。我们证明了我们的框架可以更好地利用未标记的数据优化所需的不可分解指标，假设数据分布与自我训练的分析所做的一样。使用所提出的CSST框架，我们使用深度神经网络获得了针对不同不可分解指标的实际自训练方法（用于视觉和NLP任务）。我们的结果证明了CSST的有效性和可行性。

    Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy, whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training. Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks. Our results demonstrate th
    
[^37]: 基于自动机器学习方法的价格预测应用的基准测试

    Benchmarking Automated Machine Learning Methods for Price Forecasting Applications. (arXiv:2304.14735v1 [cs.LG])

    [http://arxiv.org/abs/2304.14735](http://arxiv.org/abs/2304.14735)

    本研究展示了将自动化机器学习解决方案与企业领域知识相结合，以替代手动创建的ML管道，为中小型企业实现自动化价格预测提供可能。

    

    由于空间和时间的价格波动，在二手建筑设备的价格预测方面是一项具有挑战性的任务。因此，基于当前市场数据自动化预测过程具有极高的兴趣。即使将机器学习（ML）应用于这些数据是预测某些工具残值的一个有前途的方法，但由于中小型企业的ML专业知识不足，因此很难实现。为此，我们展示了用自动化机器学习（AutoML）解决方案代替手动创建的ML管道的可能性，这些解决方案可以自动生成底层管道。我们将AutoML方法与公司的领域知识相结合。基于CRISP-DM过程，我们将手动ML管道分为机器学习部分和非机器学习部分。为了考虑所有复杂的工业要求并展示我们新方法的适用性，我们设计了一种新的名为“方法评估”的度量标准。

    Price forecasting for used construction equipment is a challenging task due to spatial and temporal price fluctuations. It is thus of high interest to automate the forecasting process based on current market data. Even though applying machine learning (ML) to these data represents a promising approach to predict the residual value of certain tools, it is hard to implement for small and medium-sized enterprises due to their insufficient ML expertise. To this end, we demonstrate the possibility of substituting manually created ML pipelines with automated machine learning (AutoML) solutions, which automatically generate the underlying pipelines. We combine AutoML methods with the domain knowledge of the companies. Based on the CRISP-DM process, we split the manual ML pipeline into a machine learning and non-machine learning part. To take all complex industrial requirements into account and to demonstrate the applicability of our new approach, we designed a novel metric named method evalua
    
[^38]: 非联网强化学习框架在多链路Wi-Fi网络中的链路激活中的应用

    A Federated Reinforcement Learning Framework for Link Activation in Multi-link Wi-Fi Networks. (arXiv:2304.14720v1 [cs.NI])

    [http://arxiv.org/abs/2304.14720](http://arxiv.org/abs/2304.14720)

    本文提出了一种使用联邦强化学习框架来协同优化多链路Wi-Fi网络中的链路激活的方法。

    

    下一代Wi-Fi网络正在引入新功能，如多链路操作（MLO），以实现更高的吞吐量和更低的延迟。然而，由于可用信道数量有限，一组竞争基本服务集（BSS）的多个链路的使用可能会导致更高的干扰和信道争用，从而可能导致性能和可靠性降低。在这种情况下，如果所有竞争的BSS都使用更少的链路以减少信道访问争用，则可能更好。最近，强化学习（RL）已经证明其在无线网络资源分配优化方面的潜力。然而，每个无线网络的独立运行使得每个单独网络学习一个好的配置变得困难 - 如果不是几乎不可能的话。为了解决这个问题，本文提出了使用联合机器学习方法--联邦强化学习（FRL）框架的方法。

    Next-generation Wi-Fi networks are looking forward to introducing new features like multi-link operation (MLO) to both achieve higher throughput and lower latency. However, given the limited number of available channels, the use of multiple links by a group of contending Basic Service Sets (BSSs) can result in higher interference and channel contention, thus potentially leading to lower performance and reliability. In such a situation, it could be better for all contending BSSs to use less links if that contributes to reduce channel access contention. Recently, reinforcement learning (RL) has proven its potential for optimizing resource allocation in wireless networks. However, the independent operation of each wireless network makes difficult -- if not almost impossible -- for each individual network to learn a good configuration. To solve this issue, in this paper, we propose the use of a Federated Reinforcement Learning (FRL) framework, i.e., a collaborative machine learning approac
    
[^39]: X-RLflow：面向神经网络子图转换的图形增强学习

    X-RLflow: Graph Reinforcement Learning for Neural Network Subgraphs Transformation. (arXiv:2304.14698v1 [cs.LG])

    [http://arxiv.org/abs/2304.14698](http://arxiv.org/abs/2304.14698)

    本论文提出了一种基于强化学习的方法，X-RLflow，用于替换神经网络的子图，以求得更优的计算图结构，可在各种深度学习模型和基准测试中超越现有技术的超优化系统。

    

    张量图超优化系统通过神经网络的一系列子图替换来找到最优的计算图结构。这个图转换过程自然而然地落入了序列决策框架中, 现有系统通常采用贪心搜索方法，无法探索整个搜索空间，因为它不能容忍临时的性能损失。本文提出了一种基于强化学习 (RL) 的替代搜索方法来解决张量图超优化问题。我们提出的方法，X-RLflow，可以学习执行神经网络数据流图重写，一次替换一个子图。X-RLflow 基于一种无模型 RL 代理，使用图神经网络 (GNN) 对目标计算图进行编码，并迭代输出转换后的计算图。我们证明，我们的方法可以在各种深度学习模型和基准测试中超越现有技术的超优化系统。

    Tensor graph superoptimisation systems perform a sequence of subgraph substitution to neural networks, to find the optimal computation graph structure. Such a graph transformation process naturally falls into the framework of sequential decision-making, and existing systems typically employ a greedy search approach, which cannot explore the whole search space as it cannot tolerate a temporary loss of performance. In this paper, we address the tensor graph superoptimisation problem by exploring an alternative search approach, reinforcement learning (RL). Our proposed approach, X-RLflow, can learn to perform neural network dataflow graph rewriting, which substitutes a subgraph one at a time. X-RLflow is based on a model-free RL agent that uses a graph neural network (GNN) to encode the target computation graph and outputs a transformed computation graph iteratively. We show that our approach can outperform state-of-the-art superoptimisation systems over a range of deep learning models an
    
[^40]: 基于因子图的图神经网络在PMU线性状态估计中的鲁棒、快速和可扩展性研究

    Graph Neural Networks on Factor Graphs for Robust, Fast, and Scalable Linear State Estimation with PMUs. (arXiv:2304.14680v1 [cs.LG])

    [http://arxiv.org/abs/2304.14680](http://arxiv.org/abs/2304.14680)

    本文通过在因子图上实现图神经网络，从PMU电压和电流测量中学习复杂的母线电压估计，提高了线性状态估计的鲁棒性和可扩展性。

    

    随着相量测量设备(PMU)在输电系统中的应用越来越广泛，需要一种能够利用它们高采样率的快速状态估计（SE）算法。为此，我们提出了一种使用图神经网络（GNN）从PMU电压和电流测量中学习复杂的母线电压估计的方法。我们提出了一种原始的GNN实现，将其应用于电力系统的因子图上，以简化对电力系统母线和分支上各种类型和数量的测量的集成。此外，我们增强了因子图，以提高GNN预测的鲁棒性。该模型非常高效、可扩展，其计算复杂度与电力系统节点数成线性关系。通过随机采样电力系统测量集生成训练和测试示例，并用带有PMU的线性SE的确切解进行注释。数值结果表明，GNN模型提供了高精度的预测性能。

    As phasor measurement units (PMUs) become more widely used in transmission power systems, a fast state estimation (SE) algorithm that can take advantage of their high sample rates is needed. To accomplish this, we present a method that uses graph neural networks (GNNs) to learn complex bus voltage estimates from PMU voltage and current measurements. We propose an original implementation of GNNs over the power system's factor graph to simplify the integration of various types and quantities of measurements on power system buses and branches. Furthermore, we augment the factor graph to improve the robustness of GNN predictions. This model is highly efficient and scalable, as its computational complexity is linear with respect to the number of nodes in the power system. Training and test examples were generated by randomly sampling sets of power system measurements and annotated with the exact solutions of linear SE with PMUs. The numerical results demonstrate that the GNN model provides 
    
[^41]: 基于ICU住院时间预测的联邦学习客户端招募研究

    Client Recruitment for Federated Learning in ICU Length of Stay Prediction. (arXiv:2304.14663v1 [cs.LG])

    [http://arxiv.org/abs/2304.14663](http://arxiv.org/abs/2304.14663)

    本研究提出了一种新的基于ICU住院时间预测的联邦学习客户端招募策略，通过准则、相似性和聚类，可以降低通信开销和训练成本，同时保持预测性能。

    

    近年来，医疗和保健领域的机器学习和深度学习方法取得了显著的进展和改进。这些方法需要大量的训练数据，医疗领域中有大量的这样的数据，但是这些数据是分散的。联邦学习技术非常适合应对这些挑战。本文针对联邦学习中的客户端选择问题，提出了一种新的基于ICU住院时间预测的联邦学习客户端招募策略。我们的策略包括几个阶段：（i）基于准则的客户端预选；（ii）基于相似性的客户端选择；和（iii）客户端聚类。实验结果表明，我们的策略可以降低通信开销和训练成本，同时保持预测性能。

    Machine and deep learning methods for medical and healthcare applications have shown significant progress and performance improvement in recent years. These methods require vast amounts of training data which are available in the medical sector, albeit decentralized. Medical institutions generate vast amounts of data for which sharing and centralizing remains a challenge as the result of data and privacy regulations. The federated learning technique is well-suited to tackle these challenges. However, federated learning comes with a new set of open problems related to communication overhead, efficient parameter aggregation, client selection strategies and more. In this work, we address the step prior to the initiation of a federated network for model training, client recruitment. By intelligently recruiting clients, communication overhead and overall cost of training can be reduced without sacrificing predictive performance. Client recruitment aims at pre-excluding potential clients fro
    
[^42]: 医学图像的“Segment Anything Model”模型？

    Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])

    [http://arxiv.org/abs/2304.14660](http://arxiv.org/abs/2304.14660)

    “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。

    

    “Segment Anything Model”（SAM）是第一个适用于常规图像分割的基础模型。它设计了一种新颖的可推广分割任务，通过自动和手动两种模式实现了使用预训练模型进行零样本图像分割。SAM在各种自然图像分割任务中取得了显着的成果。然而，由于复杂的模态、细微的解剖结构、不确定的复杂对象边界和广泛的对象尺度，医学图像分割（MIS）更具挑战性。SAM在各种自然图像分割任务中取得了显着的成果。同时，零样本和高效的MIS可以很好地减少注释时间并促进医学图像分析的发展。因此，SAM似乎是一个潜在的工具，并且其在大型医学数据集上的表现应该进一步验证。我们收集和整理了52个开源数据集，并建立了一个具有16个模态和68个对象的大型医学分割数据集。

    The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
    
[^43]: 从显性沟通到隐性合作：一种新的协作多智能体学习范式

    From Explicit Communication to Tacit Cooperation:A Novel Paradigm for Cooperative MARL. (arXiv:2304.14656v1 [cs.MA])

    [http://arxiv.org/abs/2304.14656](http://arxiv.org/abs/2304.14656)

    该论文提出了一种从显性沟通到隐性合作的新协作多智能体学习范式，通过在代理之间分享信息和使用本地轨迹重建信息来促进协作，并逐渐减少显式传达信息的比例。实验结果表明，这种范式在具有挑战性的情景下比传统的 CTDE 范式表现更好。

    

    集中培训与分散执行（CTDE）是一种广泛使用的学习范式，在复杂任务方面取得了显着成功。然而，局部可观察性问题和代理之间缺乏有效共享信号的存在经常限制了它在促进协作方面的有效性。虽然通信可以解决这一挑战，但同时也降低了算法的实用性。受人类团队合作学习的启发，我们提出了一种新的范式，以促进从显性通信到隐性合作的逐渐转变。在初始训练阶段，我们通过在代理之间分享相关信息和同时使用每个代理的本地轨迹重建该信息来促进协作。然后，我们将显式传达的信息与重建的信息相结合，得到混合信息。在整个训练过程中，我们通过学习到的隐含合作逐渐减少显式传达信息的比例。

    Centralized training with decentralized execution (CTDE) is a widely-used learning paradigm that has achieved significant success in complex tasks. However, partial observability issues and the absence of effectively shared signals between agents often limit its effectiveness in fostering cooperation. While communication can address this challenge, it simultaneously reduces the algorithm's practicality. Drawing inspiration from human team cooperative learning, we propose a novel paradigm that facilitates a gradual shift from explicit communication to tacit cooperation. In the initial training stage, we promote cooperation by sharing relevant information among agents and concurrently reconstructing this information using each agent's local trajectory. We then combine the explicitly communicated information with the reconstructed information to obtain mixed information. Throughout the training process, we progressively reduce the proportion of explicitly communicated information, facilit
    
[^44]: 一种自适应策略用于利用SAM算法进行锐度感知型优化

    An Adaptive Policy to Employ Sharpness-Aware Minimization. (arXiv:2304.14647v1 [cs.LG])

    [http://arxiv.org/abs/2304.14647](http://arxiv.org/abs/2304.14647)

    本研究提出了一种自适应策略，基于损失函数的形状，用于设计高效的锐度感知优化算法SAM。采用AE-SAM和AE-LookSAM两种算法，理论上证明AE-SAM具有与SAM相同的收敛速度。实验结果表明这一策略的高效和有效。

    

    研究表明，利用锐度感知型优化(SAM)通过最小化过程中的锐度寻求平坦极小值，在改善模型泛化能力方面具有一定的实用性。但是，与标准经验风险最小化(ERM)相比，每次SAM更新需要计算两个梯度，其计算成本和训练时间都增加了一倍。最近的方法通过随机或定期在SAM更新和ERM更新之间进行切换，从而减少SAM更新的比例，加速SAM。本文设计了一种自适应策略，基于损失函数的几何形状来应用SAM。提出了两种高效的算法，AE-SAM和AE-LookSAM。我们理论上证明了AE-SAM具有与SAM相同的收敛速度。在各种数据集和架构上的实验结果表明了自适应策略的效率和有效性。

    Sharpness-aware minimization (SAM), which searches for flat minima by min-max optimization, has been shown to be useful in improving model generalization. However, since each SAM update requires computing two gradients, its computational cost and training time are both doubled compared to standard empirical risk minimization (ERM). Recent state-of-the-arts reduce the fraction of SAM updates and thus accelerate SAM by switching between SAM and ERM updates randomly or periodically. In this paper, we design an adaptive policy to employ SAM based on the loss landscape geometry. Two efficient algorithms, AE-SAM and AE-LookSAM, are proposed. We theoretically show that AE-SAM has the same convergence rate as SAM. Experimental results on various datasets and architectures demonstrate the efficiency and effectiveness of the adaptive policy.
    
[^45]: 关于欠阻尼Nesterov加速的研究

    On Underdamped Nesterov's Acceleration. (arXiv:2304.14642v1 [math.OC])

    [http://arxiv.org/abs/2304.14642](http://arxiv.org/abs/2304.14642)

    该论文研究了欠阻尼Nesterov加速，基于高分辨率微分方程框架构建了新的Lyapunov函数，证明了最小梯度范数平方和目标值的收敛速率。

    

    这篇论文证明了高分辨率微分方程框架非常适合Nesterov的加速梯度下降法（\texttt{NAG}）以及它的近端对应物——更快的迭代阈值收缩算法（FISTA）。但是，该理论体系仍然不完整，因为欠阻尼情况（$r<2$）尚未包括在内。本文基于高分辨率微分方程框架，构建了欠阻尼情况下的新的Lyapunov函数，激发于时间$t^{\gamma}$或迭代$k^{\gamma}$的混合项的影响。当动量参数$r$为$2$时，新的Lyapunov函数与之前的函数相同。这些新的证明不仅包括了以前根据低分辨率微分方程框架得出的目标值的收敛速率，还表征了最小梯度范数平方的收敛速率。

    The high-resolution differential equation framework has been proven to be tailor-made for Nesterov's accelerated gradient descent method~(\texttt{NAG}) and its proximal correspondence -- the class of faster iterative shrinkage thresholding algorithms (FISTA). However, the systems of theories is not still complete, since the underdamped case ($r < 2$) has not been included. In this paper, based on the high-resolution differential equation framework, we construct the new Lyapunov functions for the underdamped case, which is motivated by the power of the time $t^{\gamma}$ or the iteration $k^{\gamma}$ in the mixed term. When the momentum parameter $r$ is $2$, the new Lyapunov functions are identical to the previous ones. These new proofs do not only include the convergence rate of the objective value previously obtained according to the low-resolution differential equation framework but also characterize the convergence rate of the minimal gradient norm square. All the convergence rates o
    
[^46]: CVRecon: 重新思考神经重建的3D几何特征学习

    CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction. (arXiv:2304.14633v1 [cs.CV])

    [http://arxiv.org/abs/2304.14633](http://arxiv.org/abs/2304.14633)

    研究团队提出了一种基于代价体的3D神经重建框架CVRecon，利用丰富的几何嵌入来促进3D几何特征学习。通过引入射线上下文补偿代价体（RCCV），有效提高了视角相关信息的完整性和鲁棒性，并在各种度量方面显着提高了重建质量。

    

    最近使用图像序列进行神经重建的进展取得了显着进展。但是，由于缺乏深度信息，现有的基于体积的技术仅沿整个相机光线复制对象表面的2D图像特征。我们认为这种复制会在空洞和遮挡空间中引入噪声，从而产生高质量的3D几何体成形方面产生挑战。受传统多视角立体方法的启发，我们提出了一种端到端的3D神经重建框架CVRecon，旨在利用代价体中丰富的几何嵌入来促进3D几何特征学习。此外，我们提出了一种新颖的3D几何特征表示法——射线上下文补偿代价体（RCCV），它具有更好的完整性和鲁棒性，可以编码视角相关信息。通过全面的实验，我们证明了我们的方法在各种度量方面显着提高了重建质量，并恢复了清晰的

    Recent advances in neural reconstruction using posed image sequences have made remarkable progress. However, due to the lack of depth information, existing volumetric-based techniques simply duplicate 2D image features of the object surface along the entire camera ray. We contend this duplication introduces noise in empty and occluded spaces, posing challenges for producing high-quality 3D geometry. Drawing inspiration from traditional multi-view stereo methods, we propose an end-to-end 3D neural reconstruction framework CVRecon, designed to exploit the rich geometric embedding in the cost volumes to facilitate 3D geometric feature learning. Furthermore, we present Ray-contextual Compensated Cost Volume (RCCV), a novel 3D geometric feature representation that encodes view-dependent information with improved integrity and robustness. Through comprehensive experiments, we demonstrate that our approach significantly improves the reconstruction quality in various metrics and recovers clear
    
[^47]: MUDiff: 统一扩散生成完整分子

    MUDiff: Unified Diffusion for Complete Molecule Generation. (arXiv:2304.14621v1 [cs.LG])

    [http://arxiv.org/abs/2304.14621](http://arxiv.org/abs/2304.14621)

    MUDiff 是一种新的分子数据生成模型，它通过组合离散和连续扩散过程来生成全面的分子表示。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。我们的模型还包括一个新颖的图形转换器架构，用于去噪扩散过程。

    

    我们提出了一种新的分子数据生成模型，通过组合离散和连续扩散过程来生成分子数据。我们的模型生成了分子的全面表示，包括原子特征、二维离散分子结构和三维连续分子坐标。使用扩散过程可以捕捉分子过程的概率本质，并探索不同因素对分子结构和性质的影响。此外，我们提出了一种新颖的图形转换器架构，用于去噪扩散过程。转换器对欧几里得变换是等变的，使其能够学习不变的原子和边界表示，同时保持原子坐标的等变性。这种转换器可以用于学习对几何变换具有鲁棒性的分子表示。我们通过实验和与现有方法的比较评估了我们模型的性能，展示了其生成更稳定和有效的分子结构的能力。

    We present a new model for generating molecular data by combining discrete and continuous diffusion processes. Our model generates a comprehensive representation of molecules, including atom features, 2D discrete molecule structures, and 3D continuous molecule coordinates. The use of diffusion processes allows for capturing the probabilistic nature of molecular processes and the ability to explore the effect of different factors on molecular structures and properties. Additionally, we propose a novel graph transformer architecture to denoise the diffusion process. The transformer is equivariant to Euclidean transformations, allowing it to learn invariant atom and edge representations while preserving the equivariance of atom coordinates. This transformer can be used to learn molecular representations robust to geometric transformations. We evaluate the performance of our model through experiments and comparisons with existing methods, showing its ability to generate more stable and val
    
[^48]: 可识别信息瓶颈

    Recognizable Information Bottleneck. (arXiv:2304.14618v1 [cs.LG])

    [http://arxiv.org/abs/2304.14618](http://arxiv.org/abs/2304.14618)

    本文提出了一种可识别信息瓶颈（RIB），其通过可识别性评论家来规范表示的可识别性，从而实现了一种有效的数据推广方式，相比现有的信息瓶颈能够更好地保证在实际应用中的有效性。

    

    信息瓶颈（IB）通过信息压缩学习表示，从而推广到未见过的数据。然而，由于虚无的推广边界，现有的IB在实际场景中无法保证推广性。最近的PAC-Bayes IB使用信息复杂度而不是信息压缩来建立与相互信息推广界限的联系。然而，它需要计算昂贵的二阶曲率，阻碍了其实际应用。本文建立了表示的可识别性与近期的函数条件互信息（f-CMI）推广边界之间的联系，该边界的估计要简单得多。在此基础上，我们提出了一种可识别信息瓶颈（RIB），通过基于Bregman散度下的密度比匹配优化的可识别性评论家来规范表示的可识别性。在多个标准数据集上进行了广泛实验验证。

    Information Bottlenecks (IBs) learn representations that generalize to unseen data by information compression. However, existing IBs are practically unable to guarantee generalization in real-world scenarios due to the vacuous generalization bound. The recent PAC-Bayes IB uses information complexity instead of information compression to establish a connection with the mutual information generalization bound. However, it requires the computation of expensive second-order curvature, which hinders its practical application. In this paper, we establish the connection between the recognizability of representations and the recent functional conditional mutual information (f-CMI) generalization bound, which is significantly easier to estimate. On this basis we propose a Recognizable Information Bottleneck (RIB) which regularizes the recognizability of representations through a recognizability critic optimized by density ratio matching under the Bregman divergence. Extensive experiments on sev
    
[^49]: 缺失值下的反事实解释方法

    Counterfactual Explanation with Missing Values. (arXiv:2304.14606v1 [cs.LG])

    [http://arxiv.org/abs/2304.14606](http://arxiv.org/abs/2304.14606)

    本论文提出了一种新的反事实解释方法（CEPIA），可以处理数据中的缺失值，使得用户可以获得有效的行动建议并了解缺失值对建议的影响。

    

    反事实解释是一种提供扰动以改变分类器预测结果的事后解释方法。现有方法需要完整信息的输入，但实际情况中往往会有缺失值。本文提出一种新的CE框架（称为CEPIA），使用户可以在有缺失值的情况下获得有效的操作，并阐明缺失值对操作的影响。

    Counterfactual Explanation (CE) is a post-hoc explanation method that provides a perturbation for altering the prediction result of a classifier. Users can interpret the perturbation as an "action" to obtain their desired decision results. Existing CE methods require complete information on the features of an input instance. However, we often encounter missing values in a given instance, and the previous methods do not work in such a practical situation. In this paper, we first empirically and theoretically show the risk that missing value imputation methods affect the validity of an action, as well as the features that the action suggests changing. Then, we propose a new framework of CE, named Counterfactual Explanation by Pairs of Imputation and Action (CEPIA), that enables users to obtain valid actions even with missing values and clarifies how actions are affected by imputation of the missing values. Specifically, our CEPIA provides a representative set of pairs of an imputation ca
    
[^50]: 自动去偏重重配作为线性回归

    Augmented balancing weights as linear regression. (arXiv:2304.14545v1 [stat.ME])

    [http://arxiv.org/abs/2304.14545](http://arxiv.org/abs/2304.14545)

    本文探索了自动去偏重重配的新颖特征描述，并将其等同于基于内核岭回归的单个欠平滑岭回归，进一步将这种方法推广到特定的结果和重配模型选择上。

    

    我们提供了对于自动去偏重重配(AutoDML)的新颖特征描述。这些估算器将结果建模与重配相结合，直接估计反向倾向积分权重。当结果与权重模型都是某些（可能是无限的）基础中的线性时，我们表明增强的估算器等同于具有将原始结果模型系数和OLS相结合的系数的单个线性模型；在许多设置中，增强估算器合并为仅使用OLS. 然后，我们将这些结果扩展到特定的结果和重配模型选择上。我们首先表明，使用(内核)岭回归作为结果和重配模型的联合估算器等同于单个、欠平滑(内核)岭回归；当考虑到渐近速率时，这一结果也成立。当代替权重模型为套索回归时，我们给出了特殊情况的解析表达式并且演示了…

    We provide a novel characterization of augmented balancing weights, also known as Automatic Debiased Machine Learning (AutoDML). These estimators combine outcome modeling with balancing weights, which estimate inverse propensity score weights directly. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the original outcome model coefficients and OLS; in many settings, the augmented estimator collapses to OLS alone. We then extend these results to specific choices of outcome and weighting models. We first show that the combined estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression; this also holds when considering asymptotic rates. When the weighting model is instead lasso regression, we give closed-form expressions for special cases and demonstrate
    
[^51]: 深度时空聚类：多维气候数据的临时聚类方法

    Deep Spatiotemporal Clustering: A Temporal Clustering Approach for Multi-dimensional Climate Data. (arXiv:2304.14541v1 [cs.LG])

    [http://arxiv.org/abs/2304.14541](http://arxiv.org/abs/2304.14541)

    该论文提出了一种新颖的算法Deep Spatiotemporal Clustering (DSC) ，用于使用无监督深度学习方法进行高维时空数据的时间聚类，DSC利用自动编码器集成CNN-RNN层学习时空数据的潜在表示，并优化聚类损失和数据重建损失以改善聚类分配和非线性重建能力。

    

    使用无监督方法对高维时空数据进行聚类是许多数据驱动应用程序中的一个具有挑战性的问题。现有的无监督聚类的最先进方法使用不同的相似性和距离函数，但集中于数据的空间或时间特征。我们集中于使用空间和时间特征的联合深度表示学习，提出了Deep Spatiotemporal Clustering (DSC)，这是一种新颖的算法，用于使用无监督深度学习方法进行高维时空数据的时间聚类。受U-net架构启发，DSC利用自动编码器集成CNN-RNN层学习时空数据的潜在表示。 DSC还包括一个独特的潜在表示层，用于使用学生t分布进行聚类分配。通过同时优化聚类损失和数据重建损失，该算法逐渐改善聚类分配和非线性重建能力。

    Clustering high-dimensional spatiotemporal data using an unsupervised approach is a challenging problem for many data-driven applications. Existing state-of-the-art methods for unsupervised clustering use different similarity and distance functions but focus on either spatial or temporal features of the data. Concentrating on joint deep representation learning of spatial and temporal features, we propose Deep Spatiotemporal Clustering (DSC), a novel algorithm for the temporal clustering of high-dimensional spatiotemporal data using an unsupervised deep learning method. Inspired by the U-net architecture, DSC utilizes an autoencoder integrating CNN-RNN layers to learn latent representations of the spatiotemporal data. DSC also includes a unique layer for cluster assignment on latent representations that uses the Student's t-distribution. By optimizing the clustering loss and data reconstruction loss simultaneously, the algorithm gradually improves clustering assignments and the nonlinea
    
[^52]: 基于贝叶斯分类器的特征最优分区研究

    Optimal partition of feature using Bayesian classifier. (arXiv:2304.14537v1 [cs.LG])

    [http://arxiv.org/abs/2304.14537](http://arxiv.org/abs/2304.14537)

    本文通过提出一种名为“共单调独立分类器”(CIBer)的新技术，专注于特征的最优分区，旨在克服朴素贝叶斯方法带来的挑战，并且证明该技术在不同数据集上具有更高的准确率和更低的错误率。

    

    朴素贝叶斯分类器是一种应用贝叶斯原理的流行分类方法，尽管输入变量之间的条件依赖关系听起来很好，但实际上会导致大多数投票风格的行为。朴素贝叶斯算法中的某些特征被称为独立特征，因为在预测分类时它们没有条件相关性或依赖性。本文通过提出一种名为“共单调独立分类器”(CIBer)的新技术，专注于特征的最优分区，旨在克服朴素贝叶斯方法带来的挑战。在不同的数据集上，我们明确证明了我们的技术的有效性，在错误率更低、准确率更高或相当的情况下，与随机森林和XGBoost等模型相比。

    The Naive Bayesian classifier is a popular classification method employing the Bayesian paradigm. The concept of having conditional dependence among input variables sounds good in theory but can lead to a majority vote style behaviour. Achieving conditional independence is often difficult, and they introduce decision biases in the estimates. In Naive Bayes, certain features are called independent features as they have no conditional correlation or dependency when predicting a classification. In this paper, we focus on the optimal partition of features by proposing a novel technique called the Comonotone-Independence Classifier (CIBer) which is able to overcome the challenges posed by the Naive Bayes method. For different datasets, we clearly demonstrate the efficacy of our technique, where we achieve lower error rates and higher or equivalent accuracy compared to models such as Random Forests and XGBoost.
    
[^53]: 基于深度迁移学习的自动语音识别：迈向更好的泛化

    Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])

    [http://arxiv.org/abs/2304.14535](http://arxiv.org/abs/2304.14535)

    本文Survey了基于DTL的ASR框架，并介绍了如何使用实际数据集进行深度迁移学习以达到更好的泛化性能。

    

    最近，深度学习在自动语音识别（ASR）方面面临着一个重要的挑战，这需要大规模的训练数据集和高计算和存储资源。此外，机器学习（ML）方法和深度学习技术通常假设训练和测试数据来自相同的域，具有相同的输入特征空间和数据分布特性。然而，在一些现实世界的人工智能（AI）应用中，这种假设是无法适用的。DTL被引入来克服这些问题，它有助于使用实际数据集开发高性能的模型，这些实际数据集即使很小或稍有不同，但与训练数据相关。本文提出了基于DTL的ASR框架的全面调查，以阐明最新的发展。

    Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which can not meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments 
    
[^54]: 深度强化学习中的对抗策略优化

    Adversarial Policy Optimization in Deep Reinforcement Learning. (arXiv:2304.14533v1 [cs.LG])

    [http://arxiv.org/abs/2304.14533](http://arxiv.org/abs/2304.14533)

    本文提出了一种新的强化学习算法，其中一个扰动网络通过最大化智能体执行不同动作的概率，同时最小化状态的扭曲，以减轻数据过拟合的影响。

    

    由于神经网络表示的策略可以过度拟合观测中的表面特征，这会妨碍强化学习智能体学习有效的策略。在高维状态下，这个问题变得更加严重，智能体难以学习有用的策略。数据增强可以通过减轻过拟合的影响来提供性能提升。然而，这样的数据增强是一种先验知识，如果在环境中简单地应用它们可能会降低智能体的性能。在本文中，我们提出了一种新的强化学习算法，以减轻上述问题并提高学习策略的效率。我们的方法包括一个博弈理论目标，在这个目标中，扰动网络修改状态，以最大化智能体执行不同动作的概率，同时最小化状态的扭曲。相反，策略网络更新其参数，以最小化扰动效果，同时最大化未来奖励的期望值。

    The policy represented by the deep neural network can overfit the spurious features in observations, which hamper a reinforcement learning agent from learning effective policy. This issue becomes severe in high-dimensional state, where the agent struggles to learn a useful policy. Data augmentation can provide a performance boost to RL agents by mitigating the effect of overfitting. However, such data augmentation is a form of prior knowledge, and naively applying them in environments might worsen an agent's performance. In this paper, we propose a novel RL algorithm to mitigate the above issue and improve the efficiency of the learned policy. Our approach consists of a max-min game theoretic objective where a perturber network modifies the state to maximize the agent's probability of taking a different action while minimizing the distortion in the state. In contrast, the policy network updates its parameters to minimize the effect of perturbation while maximizing the expected future r
    
[^55]: 文本到图像生成中种子选择的重要性

    It is all about where you start: Text-to-image generation with seed selection. (arXiv:2304.14530v1 [cs.CV])

    [http://arxiv.org/abs/2304.14530](http://arxiv.org/abs/2304.14530)

    该论文研究了文本到图像生成中训练数据不平衡对模型的影响，并提出了一种高效的方法：在噪声空间中选择适当的生成种子。该方法能够正确生成罕见的概念，而不需要重新训练模型。

    

    文本到图像扩散模型可以在新的组合和场景中合成大量的概念。然而，它们仍然在生成不常见的概念、罕见的不寻常组合或结构化概念（如手掌）方面有困难。它们的限制部分是由于训练数据的长尾性：网络爬取的数据集严重不平衡，导致模型在分布尾部的概念上表现不足。在这里，我们表征了不平衡训练数据对文本到图像模型的影响，并提出了一个解决方案。我们展示了通过在噪声空间中精心选择适当的生成种子，可以正确生成罕见的概念，这一技术被称为SeedSelect。SeedSelect是高效的，不需要重新训练扩散模型。我们在一系列问题上评估了SeedSelect的效益。首先，在少样本语义数据增强中，我们为少样本和长尾基准生成了语义正确的图像。我们展示了分类

    Text-to-image diffusion models can synthesize a large variety of concepts in new compositions and scenarios. However, they still struggle with generating uncommon concepts, rare unusual combinations, or structured concepts like hand palms. Their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. Here we characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, a technique that we call SeedSelect. SeedSelect is efficient and does not require retraining the diffusion model. We evaluate the benefit of SeedSelect on a series of problems. First, in few-shot semantic data augmentation, where we generate semantically correct images for few-shot and long-tail benchmarks. We show classificati
    
[^56]: 信息检索的多元表示学习

    Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])

    [http://arxiv.org/abs/2304.14522](http://arxiv.org/abs/2304.14522)

    本论文提出一种多元分布模型的信息检索表示学习框架，可无缝集成到现有近似最近邻算法中以实现高效检索。

    

    稠密检索模型使用双编码器网络架构来学习查询和文档的表示形式，这些表示形式通常采用向量表示，它们的相似性通常使用点积函数计算。本文提出一种新的稠密检索表示学习框架。我们的框架不是学习每个查询和文档的向量，而是学习多元分布，并使用负多元KL散度计算分布之间的相似性。为了简化和提高效率，我们假设这些分布是多维正态分布，然后训练大型语言模型来生成这些分布的均值和方差向量。我们为所提出的框架提供了理论基础，并展示了它可以无缝地集成到现有的近似最近邻算法中以实现高效检索。我们进行了广泛的实验，覆盖了各种不同的基准数据集和评估指标。

    Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range 
    
[^57]: 理解共享的语音-文本表示

    Understanding Shared Speech-Text Representations. (arXiv:2304.14514v1 [cs.CL])

    [http://arxiv.org/abs/2304.14514](http://arxiv.org/abs/2304.14514)

    本文研究了将文本整合进入端到端语音模型中训练的方法，通过研究无语音域自适应和激活的相似性，发现持续模型对共享语音-文本表示很重要，共享编码器学习了一个比单模态更紧凑重叠的语音-文本表示，这部分解释了Maestro共享的语音-文本表示有效的原因。

    

    最近出现了许多将文本整合到端到端模型中训练语音模型的方法，其中Maestro推进了自动语音识别（ASR）和语音翻译（ST）的最新进展。本文通过两类分析扩展了我们对产生的共享语音-文本表示的理解。首先，我们研究了无语音域自适应的极限，发现为了学习共享的语音-文本表示，具有语音-文本对齐的特定语料库持续模型是最重要的组成部分。其次，我们检查了单模态编码器（语音或文本）的激活与共享编码器的激活之间的相似之处。我们发现，共享编码器学习了一个比单模态编码器更紧凑和重叠的语音-文本表示。我们假设这部分解释了Maestro共享的语音-文本表示的有效性。

    Recently, a number of approaches to train speech models by incorpo-rating text into end-to-end models have been developed, with Mae-stro advancing state-of-the-art automatic speech recognition (ASR)and Speech Translation (ST) performance. In this paper, we expandour understanding of the resulting shared speech-text representationswith two types of analyses. First we examine the limits of speech-free domain adaptation, finding that a corpus-specific duration modelfor speech-text alignment is the most important component for learn-ing a shared speech-text representation. Second, we inspect the sim-ilarities between activations of unimodal (speech or text) encodersas compared to the activations of a shared encoder. We find that theshared encoder learns a more compact and overlapping speech-textrepresentation than the uni-modal encoders. We hypothesize that thispartially explains the effectiveness of the Maestro shared speech-textrepresentations.
    
[^58]: 3D Brainformer：用于脑肿瘤分割的三维融合Transformer

    3D Brainformer: 3D Fusion Transformer for Brain Tumor Segmentation. (arXiv:2304.14508v1 [eess.IV])

    [http://arxiv.org/abs/2304.14508](http://arxiv.org/abs/2304.14508)

    本文提出了一种名为3D Brainformer的三维融合Transformer结构，可以更准确地对脑MRI图像中的肿瘤进行分割。

    

    磁共振成像技术在科学研究和临床研究中都是至关重要的脑部映射工具。脑肿瘤的精确分割有助于临床诊断，评估和手术计划。基于深度学习的方法近年来在大脑肿瘤分割方面取得了显著进展。然而，由于卷积网络的固有限制，其仅能表示MRI图像中区域内局部像素的空间依赖性。相反，我们提出了一种名为3D Brainformer的三维融合Transformer结构，以提供更准确的MRI图像中脑肿瘤分割结果。实验结果表明，3D Brainformer在BraTS 2019和2020数据集中优于现有的方法。

    Magnetic resonance imaging (MRI) is critically important for brain mapping in both scientific research and clinical studies. Precise segmentation of brain tumors facilitates clinical diagnosis, evaluations, and surgical planning. Deep learning has recently emerged to improve brain tumor segmentation and achieved impressive results. Convolutional architectures are widely used to implement those neural networks. By the nature of limited receptive fields, however, those architectures are subject to representing long-range spatial dependencies of the voxel intensities in MRI images. Transformers have been leveraged recently to address the above limitations of convolutional networks. Unfortunately, the majority of current Transformers-based methods in segmentation are performed with 2D MRI slices, instead of 3D volumes. Moreover, it is difficult to incorporate the structures between layers because each head is calculated independently in the Multi-Head Self-Attention mechanism (MHSA). In th
    
[^59]: 基于Transformer的可解释多模态数据融合用于皮肤病分类

    Transformer-based interpretable multi-modal data fusion for skin lesion classification. (arXiv:2304.14505v1 [eess.IV])

    [http://arxiv.org/abs/2304.14505](http://arxiv.org/abs/2304.14505)

    本文提出了一种基于Transformer的可解释多模态数据融合算法，用于帮助皮肤疾病的诊断。

    

    当今许多深度学习（DL）研究主要集中在提高定量指标方面，而忽略了其他因素。在人类中心的应用领域，如皮肤病分类在皮肤科中，仍处于其初级阶段的DL驱动的临床决策支持系统，由于其决策过程的透明度有限。此外，缺乏能够解释训练的DL算法行为的程序几乎没有得到临床医师的信任。为诊断皮肤病变，皮肤科医生依靠疾病的视觉评估和患者病史收集的数据。处理多模态数据的数据驱动算法受限于卷积结构所需的特征级和决策级融合程序的分离。为解决这个问题，我们通过基于Transformer的架构的注意机制实现单阶段多模态数据融合，以帮助诊断皮肤疾病。

    A lot of deep learning (DL) research these days is mainly focused on improving on quantitative metrics regardless of other factors. In human centered applications, like skin lesion classification in dermatology, DL-driven clinical decision support systems are still in their infancy due to the limited transparency of their decision-making process. Moreover, the lack of procedures that can explain the behavior of trained DL algorithms leads to almost no trust from the clinical physicians. To diagnose skin lesions, dermatologists rely on both visual assessment of the disease and the data gathered from the anamnesis of the patient. Data-driven algorithms dealing with multi-modal data are limited by the separation of feature-level and decision-level fusion procedures required by convolutional architectures. To address this issue, we enable single-stage multi-modal data fusion via the attention mechanism of transformer-based architectures to aid in the diagnosis of skin diseases. Our method 
    
[^60]: 混合MLP和LSTM模型的深度欺诈检测

    Hybrid Deepfake Detection Utilizing MLP and LSTM. (arXiv:2304.14504v1 [cs.CV])

    [http://arxiv.org/abs/2304.14504](http://arxiv.org/abs/2304.14504)

    本文提出了一种使用深度学习算法MLP和LSTM的深度伪造检测模式，旨在检测并防止虚假信息的传播。

    

    随着社会对社交媒体上真实信息的依赖不断增长，利用深度伪造（deepfake）欺骗用户的方法越来越受欢迎。深度伪造技术利用最新技术发明，使网络用户能够将自己的面孔替换为许多重要政治和文化人物的计算机生成的合成面孔，从而传播大量虚假信息。为了防止虚假信息的传播，现在非常需要能够检测这些深度伪造图像和视频的模型。本文提出了一种新的深度伪造检测模式，采用两个深度学习算法：长短期记忆和多层感知器。我们使用一个名为140k Real and Fake的公共数据集来评估我们的模型。

    The growing reliance of society on social media for authentic information has done nothing but increase over the past years. This has only raised the potential consequences of the spread of misinformation. One of the growing methods in popularity is to deceive users using a deepfake. A deepfake is an invention that has come with the latest technological advancements, which enables nefarious online users to replace their face with a computer generated, synthetic face of numerous powerful members of society. Deepfake images and videos now provide the means to mimic important political and cultural figures to spread massive amounts of false information. Models that can detect these deepfakes to prevent the spread of misinformation are now of tremendous necessity. In this paper, we propose a new deepfake detection schema utilizing two deep learning algorithms: long short term memory and multilayer perceptron. We evaluate our model using a publicly available dataset named 140k Real and Fake
    
[^61]: MWaste：管理家庭垃圾的深度学习方法

    MWaste: A Deep Learning Approach to Manage Household Waste. (arXiv:2304.14498v1 [cs.CV])

    [http://arxiv.org/abs/2304.14498](http://arxiv.org/abs/2304.14498)

    MWaste是一款基于深度学习的移动应用，能够有效地将垃圾材料分类为垃圾、塑料、纸张、金属、玻璃或硬纸板，可帮助应对气候变化。

    

    计算机视觉方法已被证明在分类垃圾处理的回收分类方面很有效，但现有方法成本高、不精确且不清晰。为解决这个问题，我们介绍MWaste，一款移动应用程序，利用计算机视觉和深度学习技术将垃圾材料分类为垃圾、塑料、纸张、金属、玻璃或硬纸板。其有效性已在各种神经网络架构和真实世界图像上进行了测试，在测试集上达到了92％的平均精度。该应用程序可以通过使垃圾处理更有效并减少因不正确的垃圾处理而引起的温室气体排放来帮助应对气候变化。

    Computer vision methods have shown to be effective in classifying garbage into recycling categories for waste processing, existing methods are costly, imprecise, and unclear. To tackle this issue, we introduce MWaste, a mobile application that uses computer vision and deep learning techniques to classify waste materials as trash, plastic, paper, metal, glass or cardboard. Its effectiveness was tested on various neural network architectures and real-world images, achieving an average precision of 92\% on the test set. This app can help combat climate change by enabling efficient waste processing and reducing the generation of greenhouse gases caused by incorrect waste disposal.
    
[^62]: 使用深度学习技术还原叠加信号的原始信号

    Restoring Original Signal From Pile-up Signal using Deep Learning. (arXiv:2304.14496v1 [physics.ins-det])

    [http://arxiv.org/abs/2304.14496](http://arxiv.org/abs/2304.14496)

    本研究采用深度学习方法还原叠加信号的原始信号，提高了物理数据的能量和时间分辨率，适用于类似问题的解决。

    

    在实验物理学领域，叠加信号经常被产生，导致物理数据不准确，带有高不确定性并引起各种问题。因此，必须关键地矫正叠加信号。在本研究中，我们采用深度学习方法来还原叠加信号的原始信号。我们展示了深度学习模型可以准确地重建叠加波形中的原始信号波形。通过用模型预测的原始信号替换叠加信号，数据的能量和时间分辨率得到显着提高。该模型实现显著提高了粒子识别图和粒子轨迹的质量。该方法适用于类似问题，例如分离多个信号或纠正不同类型的噪声和背景下的叠加信号。

    Pile-up signals are frequently produced in experimental physics. They create inaccurate physics data with high uncertainty and cause various problems. Therefore, the correction to pile-up signals is crucially required. In this study, we implemented a deep learning method to restore the original signals from the pile-up signals. We showed that a deep learning model could accurately reconstruct the original signal waveforms from the pile-up waveforms. By substituting the pile-up signals with the original signals predicted by the model, the energy and timing resolutions of the data are notably enhanced. The model implementation significantly improved the quality of the particle identification plot and particle tracks. This method is applicable to similar problems, such as separating multiple signals or correcting pile-up signals with other types of noises and backgrounds.
    
[^63]: 对象中心的深度主动推理模型中的对称性与复杂性

    Symmetry and Complexity in Object-Centric Deep Active Inference Models. (arXiv:2304.14493v1 [cs.CV])

    [http://arxiv.org/abs/2304.14493](http://arxiv.org/abs/2304.14493)

    本文研究了深度主动推理下的物体中心表示中的对称性，以生成最简洁而准确的模型，从而学习和预测新的物体视图。

    

    人类每天要感知和互动上百个物体。为此，他们需要使用这些物体的心理模型，并经常利用物体形状和外观的对称性来学习通用和可转移的技能。主动推理是理解和建模有感知能力的代理的一种基本方法。它认为代理人在环境中产生模型，通过最小化上限的惊奇（即自由能）来学习和行动。自由能分解为准确性和复杂性项，这意味着代理倾向于选择最简单的模型，能够准确地解释他们的感觉观察结果。本文研究了特定物体天生对称性在深度主动推理下生成模型的潜在状态空间中也表现为对称性。具体而言，我们关注对象中心的表示，其从像素中训练以预测新的物体视图而年龄。

    Humans perceive and interact with hundreds of objects every day. In doing so, they need to employ mental models of these objects and often exploit symmetries in the object's shape and appearance in order to learn generalizable and transferable skills. Active inference is a first principles approach to understanding and modeling sentient agents. It states that agents entertain a generative model of their environment, and learn and act by minimizing an upper bound on their surprisal, i.e. their Free Energy. The Free Energy decomposes into an accuracy and complexity term, meaning that agents favor the least complex model, that can accurately explain their sensory observations. In this paper, we investigate how inherent symmetries of particular objects also emerge as symmetries in the latent state space of the generative model learnt under deep active inference. In particular, we focus on object-centric representations, which are trained from pixels to predict novel object views as the age
    
[^64]: 通过应用NLP技术分析YouTube的字幕数据，自动生成视频人体姿势分析的标记数据集

    Automatic Generation of Labeled Data for Video-Based Human Pose Analysis via NLP applied to YouTube Subtitles. (arXiv:2304.14489v1 [cs.CV])

    [http://arxiv.org/abs/2304.14489](http://arxiv.org/abs/2304.14489)

    该研究利用NLP技术分析健身视频字幕数据自动生成人体姿势分析的标记数据集。

    

    随着计算机视觉和机器学习技术的不断提升，基于视频的居家锻炼评估系统已成为目前研究的热门话题。然而，性能很大程度上取决于可用的训练数据量。由于专门针对运动的标记数据集很少，我们提出了一种利用在线健身视频丰富资源的方法。具体来说，我们利用视频通常不仅展示练习内容，还提供语言信息作为额外的信息源的优势。以俯卧撑为例，我们展示了通过NLP技术分析字幕数据，可以创建一个包含与姿势分析相关信息的标记数据集（无关紧要，相关正确，相关不正确）。特别地，我们展示了无关剪辑（$n=332$）具有与相关剪辑（$n=298$）显著不同的关节可见性值。检查聚类中心也展现了有用的信息。

    With recent advancements in computer vision as well as machine learning (ML), video-based at-home exercise evaluation systems have become a popular topic of current research. However, performance depends heavily on the amount of available training data. Since labeled datasets specific to exercising are rare, we propose a method that makes use of the abundance of fitness videos available online. Specifically, we utilize the advantage that videos often not only show the exercises, but also provide language as an additional source of information. With push-ups as an example, we show that through the analysis of subtitle data using natural language processing (NLP), it is possible to create a labeled (irrelevant, relevant correct, relevant incorrect) dataset containing relevant information for pose analysis. In particular, we show that irrelevant clips ($n=332$) have significantly different joint visibility values compared to relevant clips ($n=298$). Inspecting cluster centroids also show
    
[^65]: 对抗感知的迭代学习

    Adversary Aware Continual Learning. (arXiv:2304.14483v1 [cs.LG])

    [http://arxiv.org/abs/2304.14483](http://arxiv.org/abs/2304.14483)

    本文提出了一种新的防御性框架，针对对抗性后门攻击，利用可感知模式压倒攻击者的不可感知模式，提高了模型的稳健性。

    

    类别增量学习方法非常有用，因为它们帮助模型按顺序学习新信息（类别），同时保留之前获得的信息（类别）。然而，这样的方法极易受到对抗性后门攻击的影响，在训练期间，智能对手可以通过引入少量的信息误导模型，从而在测试时故意忘记特定的任务或类别。在这项工作中，我们提出了一种新的防御性框架来反击这种潜在攻击。我们利用攻击者的主要优势--使后门模式对人不可感知--并提议在训练期间学习一个可以压倒攻击者的可感知模式以抵消对抗性攻击者的模式。通过各种常用的Replay-based（两者都

    Class incremental learning approaches are useful as they help the model to learn new information (classes) sequentially, while also retaining the previously acquired information (classes). However, it has been shown that such approaches are extremely vulnerable to the adversarial backdoor attacks, where an intelligent adversary can introduce small amount of misinformation to the model in the form of imperceptible backdoor pattern during training to cause deliberate forgetting of a specific task or class at test time. In this work, we propose a novel defensive framework to counter such an insidious attack where, we use the attacker's primary strength-hiding the backdoor pattern by making it imperceptible to humans-against it, and propose to learn a perceptible (stronger) pattern (also during the training) that can overpower the attacker's imperceptible (weaker) pattern. We demonstrate the effectiveness of the proposed defensive mechanism through various commonly used Replay-based (both 
    
[^66]: ChatGPT作为攻击工具：通过黑盒生成模型触发的隐蔽文本后门攻击

    ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox Generative Model Trigger. (arXiv:2304.14475v1 [cs.CR])

    [http://arxiv.org/abs/2304.14475](http://arxiv.org/abs/2304.14475)

    本论文研究了黑盒生成模型作为后门攻击工具的角色以及相应的防御策略。其中，我们发现通过BGMAttack攻击文本分类器可以有效地实现后门攻击，而且输入的数据没有明显变形。

    

    文本后门攻击威胁着现有系统的安全性，因为攻击者可以将难以感知的触发器插入输入数据并操纵训练数据集的标签来破坏模型。随着尖端的生成模型（如GPT-4）将重写推向了前所未有的高度，这种攻击变得更加难以检测。本文全面调查了黑盒生成模型作为后门攻击工具的角色，并强调了研究相关防御策略的重要性。我们揭示了所提出的基于生成模型的攻击BGMAttack可以有效地欺骗文本分类器。相较于传统攻击方法，BGMAttack通过利用最先进的生成型号使后门触发器不太显眼。我们在五个数据集上进行了广泛的攻击有效性评估，并辅以三个不同的人类认知评估，发现BGMAttack的表现相当且输入数据没有明显的变形。我们的工作强调了进一步研究和开发对这种隐蔽文本后门攻击的防御机制的必要性。

    Textual backdoor attacks pose a practical threat to existing systems, as they can compromise the model by inserting imperceptible triggers into inputs and manipulating labels in the training dataset. With cutting-edge generative models such as GPT-4 pushing rewriting to extraordinary levels, such attacks are becoming even harder to detect. We conduct a comprehensive investigation of the role of black-box generative models as a backdoor attack tool, highlighting the importance of researching relative defense strategies. In this paper, we reveal that the proposed generative model-based attack, BGMAttack, could effectively deceive textual classifiers. Compared with the traditional attack methods, BGMAttack makes the backdoor trigger less conspicuous by leveraging state-of-the-art generative models. Our extensive evaluation of attack effectiveness across five datasets, complemented by three distinct human cognition assessments, reveals that Figure 4 achieves comparable attack performance w
    
[^67]: 学习扩散先验用于NeRFs

    Learning a Diffusion Prior for NeRFs. (arXiv:2304.14473v1 [cs.CV])

    [http://arxiv.org/abs/2304.14473](http://arxiv.org/abs/2304.14473)

    本文提出了一种使用扩散模型生成编码在规则网格上的NeRFs的方法，该方法可以采样出逼真的NeRFs，并允许有条件的生成，给定某个观察作为指导。

    

    神经辐射场（NeRFs）已成为从2D数据派生出的物体和场景的强大神经3D表示。然而，在许多情况下，生成NeRFs仍然是困难的。例如，只使用少量视图作为监督训练NeRFs仍然具有挑战性，因为它是一个欠参数问题。在这种情况下，需要一些归纳先验来过滤不良局部最小值。引入这样的归纳先验的一种方法是学习NeRFs的生成模型，该模型建模某类场景。在本文中，我们提出使用扩散模型生成编码在规则网格上的NeRFs。我们表明，我们的模型可以采样出逼真的NeRFs，并同时允许有条件的生成，给定某个观察作为指导。

    Neural Radiance Fields (NeRFs) have emerged as a powerful neural 3D representation for objects and scenes derived from 2D data. Generating NeRFs, however, remains difficult in many scenarios. For instance, training a NeRF with only a small number of views as supervision remains challenging since it is an under-constrained problem. In such settings, it calls for some inductive prior to filter out bad local minima. One way to introduce such inductive priors is to learn a generative model for NeRFs modeling a certain class of scenes. In this paper, we propose to use a diffusion model to generate NeRFs encoded on a regularized grid. We show that our model can sample realistic NeRFs, while at the same time allowing conditional generations, given a certain observation as guidance.
    
[^68]: 北欧车辆数据集（NVD）：使用新捕获的不同雪天气候下无人机捕捉的NVD评估车辆检测器的性能

    Nordic Vehicle Dataset (NVD): Performance of vehicle detectors using newly captured NVD from UAV in different snowy weather conditions. (arXiv:2304.14466v1 [cs.CV])

    [http://arxiv.org/abs/2304.14466](http://arxiv.org/abs/2304.14466)

    本研究提供了无人机捕捉不同雪天气候下的车辆数据集，填补了检测大雪天气中车辆的数据缺失问题，对于车辆在复杂天气条件下的检测具有实际应用价值。

    

    无人机图像中的车辆检测和识别是一个复杂的问题，已用于不同的安全目的。这些图像的主要挑战是以斜角度捕获，并带来了许多挑战，如不均匀的照明效果，退化，模糊，遮挡，能见度丧失等。此外，天气条件对导致安全问题起着至关重要的作用，并向收集的数据添加了另一个高级别的挑战。过去几十年间，采用了各种技术来检测和跟踪不同天气条件下的车辆。然而，在大雪天气下检测车辆仍处于初级阶段，因为缺乏可用数据。此外，没有任何研究使用无人机实际捕捉到的图像来检测雪天气中的车辆。本研究旨在通过提供捕捉在不同设置和各种雪覆盖条件下的车辆数据来填补这一空白，以服务于科学社区。

    Vehicle detection and recognition in drone images is a complex problem that has been used for different safety purposes. The main challenge of these images is captured at oblique angles and poses several challenges like non-uniform illumination effect, degradations, blur, occlusion, loss of visibility, etc. Additionally, weather conditions play a crucial role in causing safety concerns and add another high level of challenge to the collected data. Over the past few decades, various techniques have been employed to detect and track vehicles in different weather conditions. However, detecting vehicles in heavy snow is still in the early stages because of a lack of available data. Furthermore, there has been no research on detecting vehicles in snowy weather using real images captured by unmanned aerial vehicles (UAVs). This study aims to address this gap by providing the scientific community with data on vehicles captured by UAVs in different settings and under various snow cover conditi
    
[^69]: Moccasin：神经网络的高效张量重算技术

    Moccasin: Efficient Tensor Rematerialization for Neural Networks. (arXiv:2304.14463v1 [cs.LG])

    [http://arxiv.org/abs/2304.14463](http://arxiv.org/abs/2304.14463)

    本文提出了一种名为Moccasin的新型约束编程形式，用于实现在内存预算下最小化计算图的执行时间，相较于最近的研究，该方法显著提高了效率，并成功应用于神经网络的高效张量重算。

    

    在边缘计算设备上部署和训练神经网络面临许多挑战，其中较低的内存是部署大型神经网络模型时经常遇到的最大限制因素之一。张量重算是解决神经网络训练和推理所需高内存需求的一种方式。本文考虑在内存预算下最小化计算图的执行时间问题。具体来说，我们开发了一种新的约束编程形式，称为Moccasin，其中只有$O(n)$个整数变量，$n$是计算图中节点的数量。这相对于最近文献中提出的具有$O(n^2)$布尔变量的公式提出了显着的改进。我们展示了数值研究结果，表明我们的方法在大规模图上比最近的工作快一个数量级。

    The deployment and training of neural networks on edge computing devices pose many challenges. The low memory nature of edge devices is often one of the biggest limiting factors encountered in the deployment of large neural network models. Tensor rematerialization or recompute is a way to address high memory requirements for neural network training and inference. In this paper we consider the problem of execution time minimization of compute graphs subject to a memory budget. In particular, we develop a new constraint programming formulation called \textsc{Moccasin} with only $O(n)$ integer variables, where $n$ is the number of nodes in the compute graph. This is a significant improvement over the works in the recent literature that propose formulations with $O(n^2)$ Boolean variables. We present numerical studies that show that our approach is up to an order of magnitude faster than recent work especially for large-scale graphs.
    
[^70]: 基于增强置信度图的鲁棒快速车辆检测

    Robust and Fast Vehicle Detection using Augmented Confidence Map. (arXiv:2304.14462v1 [cs.CV])

    [http://arxiv.org/abs/2304.14462](http://arxiv.org/abs/2304.14462)

    提出了一种基于增强置信度图的鲁棒快速车辆检测方法，通过组合多分辨率分析和极值稳定区域，生成增强置信度图，再通过快速CNN生成候选区域，有效地减少了不同速度、形状、结构和单张图像中多个车辆的影响，并采用粗糙集和基于模糊的模型实现了鲁棒的车辆检测。

    

    实时场景下的车辆检测由于时间限制和多种类型的车辆（速度、形状、结构等）的存在而具有挑战性。本文提出了一种基于生成置信度图的新方法，用于实现鲁棒快速的车辆检测。为了减少不同速度、形状、结构和单张图像中多个车辆的不良影响，我们引入了增强的概念，强调包含车辆的感兴趣区域。增强的地图是通过探索多分辨率分析和极值稳定区域（MR-MSER）的组合来生成的。 MR-MSER的输出输入快速CNN以生成置信度图，从而得出候选区域。此外，与现有模型实现复杂模型的车辆检测不同，我们探索粗糙集和基于模糊的模型的组合，以实现鲁棒的车辆检测。

    Vehicle detection in real-time scenarios is challenging because of the time constraints and the presence of multiple types of vehicles with different speeds, shapes, structures, etc. This paper presents a new method relied on generating a confidence map-for robust and faster vehicle detection. To reduce the adverse effect of different speeds, shapes, structures, and the presence of several vehicles in a single image, we introduce the concept of augmentation which highlights the region of interest containing the vehicles. The augmented map is generated by exploring the combination of multiresolution analysis and maximally stable extremal regions (MR-MSER). The output of MR-MSER is supplied to fast CNN to generate a confidence map, which results in candidate regions. Furthermore, unlike existing models that implement complicated models for vehicle detection, we explore the combination of a rough set and fuzzy-based models for robust vehicle detection. To show the effectiveness of the pro
    
[^71]: 基于梯度的最大干扰恢复的域增量3D物体检测

    Gradient-based Maximally Interfered Retrieval for Domain Incremental 3D Object Detection. (arXiv:2304.14460v1 [cs.CV])

    [http://arxiv.org/abs/2304.14460](http://arxiv.org/abs/2304.14460)

    对于域增量3D物体检测，GMIR提出了一种基于梯度的最大干扰恢复策略，可在微调时定期从以前的领域数据集中检索样本。

    

    在所有天气条件下实现准确的3D物体检测仍然是实现自主车辆广泛部署的关键挑战，因为目前大部分工作是在清晰的天气数据上进行的。为了推广到逆境天气条件，监督方法在所有天气数据上从头开始训练表现得最好，而不是在清晰天气数据上微调预训练模型。但是，从所有数据开始训练最终会因数据集不断增长并包含了所有可能的天气条件而变得不可行和昂贵。而在不同天气领域的原始数据上进行简单的微调可能会导致对先前学习领域的灾难性遗忘。受回放式连续学习方法的成功启发，我们提出了基于梯度的最大干扰恢复（GMIR），这是一种对回放进行梯度采样的策略。在微调过程中，GMIR会定期从以前的领域数据集中检索样本。

    Accurate 3D object detection in all weather conditions remains a key challenge to enable the widespread deployment of autonomous vehicles, as most work to date has been performed on clear weather data. In order to generalize to adverse weather conditions, supervised methods perform best if trained from scratch on all weather data instead of finetuning a model pretrained on clear weather data. Training from scratch on all data will eventually become computationally infeasible and expensive as datasets continue to grow and encompass the full extent of possible weather conditions. On the other hand, naive finetuning on data from a different weather domain can result in catastrophic forgetting of the previously learned domain. Inspired by the success of replay-based continual learning methods, we propose Gradient-based Maximally Interfered Retrieval (GMIR), a gradient based sampling strategy for replay. During finetuning, GMIR periodically retrieves samples from the previous domain dataset
    
[^72]: 机器学习用于检测和缓解Web漏洞和Web攻击

    Machine Learning for Detection and Mitigation of Web Vulnerabilities and Web Attacks. (arXiv:2304.14451v1 [cs.CR])

    [http://arxiv.org/abs/2304.14451](http://arxiv.org/abs/2304.14451)

    本文调研了使用经典和先进的机器学习技术进行防御XSS和CSRF的研究，并总结出关键要点，为探讨该研究方向提供了参考。

    

    在Web安全领域中，检测和缓解跨站脚本（XSS）和跨站请求伪造（CSRF）等关键Web漏洞和攻击一直是一个重要的问题。这些Web攻击不断演变，越来越难以检测。近年来，研究人员已经开始使用机器学习技术来防御XSS和CSRF，由于取得的积极结果，可以得出结论，这是一个有前途的研究方向。本文的目标是简要介绍已经发表的研究工作，这些研究工作采用经典和先进的机器学习来识别和预防XSS和CSRF。提供这份调查的目的是为了探讨已经实施的不同机器学习方法，了解其中的关键要点。

    Detection and mitigation of critical web vulnerabilities and attacks like cross-site scripting (XSS), and cross-site request forgery (CSRF) have been a great concern in the field of web security. Such web attacks are evolving and becoming more challenging to detect. Several ideas from different perspectives have been put forth that can be used to improve the performance of detecting these web vulnerabilities and preventing the attacks from happening. Machine learning techniques have lately been used by researchers to defend against XSS and CSRF, and given the positive findings, it can be concluded that it is a promising research direction. The objective of this paper is to briefly report on the research works that have been published in this direction of applying classical and advanced machine learning to identify and prevent XSS and CSRF. The purpose of providing this survey is to address different machine learning approaches that have been implemented, understand the key takeaway of 
    
[^73]: 针对空域的学习环境（LEAD）设计

    Learning Environment for the Air Domain (LEAD). (arXiv:2304.14423v1 [cs.LG])

    [http://arxiv.org/abs/2304.14423](http://arxiv.org/abs/2304.14423)

    本文介绍了针对空中作战行为的学习环境（LEAD）系统。该系统整合了Gymnasium编程库和接口，可应用现成的机器学习算法，并可以与第三方模拟软件通信。

    

    战斗机飞行员的训练中很大一部分是基于模拟的，并涉及由预定义行为模型控制的计算机生成的力量。这些行为模型通常是通过从经验丰富的飞行员中获取知识手动创建的，这是一个耗时的过程。尽管花费了很多精力，但由于它们可预测的性质和缺乏适应性，行为模型通常是不尽如人意的，迫使教练花费时间手动监控和控制它们。强化学习和模仿学习成为手工制作模型的替代方案。本文介绍了针对空中作战行为的学习环境（LEAD），一种用于在军事模拟中创建和集成智能空中作战行为的系统。通过整合流行的编程库和接口Gymnasium，LEAD允许用户应用现成的机器学习算法。此外，LEAD可以通过分布式仿真协议与第三方模拟软件进行通信。

    A substantial part of fighter pilot training is simulation-based and involves computer-generated forces controlled by predefined behavior models. The behavior models are typically manually created by eliciting knowledge from experienced pilots, which is a time-consuming process. Despite the work put in, the behavior models are often unsatisfactory due to their predictable nature and lack of adaptivity, forcing instructors to spend time manually monitoring and controlling them. Reinforcement and imitation learning pose as alternatives to handcrafted models. This paper presents the Learning Environment for the Air Domain (LEAD), a system for creating and integrating intelligent air combat behavior in military simulations. By incorporating the popular programming library and interface Gymnasium, LEAD allows users to apply readily available machine learning algorithms. Additionally, LEAD can communicate with third-party simulation software through distributed simulation protocols, which al
    
[^74]: MINN：学习微分代数方程的动态和应用于电池建模

    MINN: Learning the dynamics of differential-algebraic equations and application to battery modeling. (arXiv:2304.14422v1 [cs.LG])

    [http://arxiv.org/abs/2304.14422](http://arxiv.org/abs/2304.14422)

    本文提出了一种体系结构，生成模型集成神经网络（MINN）以允许在学习系统物理动态方面进行整合，应用于锂离子电池的电化学动力学建模，并展示了所提出的模型在解释性、精度和计算效率方面的优势。

    

    整合基于物理和基于数据的方法已经成为建模可持续能源系统的常见方法。但是，现有的文献主要集中在生成用于替代基于物理模型的数据驱动替代模型上。这些模型通常以速度为代价换取精度，但缺乏基于物理模型的泛化性、适应性和可解释性，而这些特点在优化和控制实际动态系统的建模中通常是不可或缺的。在本文中，我们提出了一种新的体系结构来生成模型集成神经网络（MINN），以允许在学习系统物理动态方面进行整合。获得的混合模型解决了控制导向建模中一个尚未解决的研究问题，即如何同时获得物理洞察力、数字精度和计算可行性的最优简化模型。我们将所提出的神经网络架构应用于锂离子电池的电化学动力学建模，并展示了所提出的模型在解释性、精度和计算效率方面的优势。

    The concept of integrating physics-based and data-driven approaches has become popular for modeling sustainable energy systems. However, the existing literature mainly focuses on the data-driven surrogates generated to replace physics-based models. These models often trade accuracy for speed but lack the generalisability, adaptability, and interpretability inherent in physics-based models, which are often indispensable in the modeling of real-world dynamic systems for optimization and control purposes. In this work, we propose a novel architecture for generating model-integrated neural networks (MINN) to allow integration on the level of learning physics-based dynamics of the system. The obtained hybrid model solves an unsettled research problem in control-oriented modeling, i.e., how to obtain an optimally simplified model that is physically insightful, numerically accurate, and computationally tractable simultaneously. We apply the proposed neural network architecture to model the el
    
[^75]: 一步分布式强化学习

    One-Step Distributional Reinforcement Learning. (arXiv:2304.14421v1 [cs.LG])

    [http://arxiv.org/abs/2304.14421](http://arxiv.org/abs/2304.14421)

    本文提出一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性，提供了统一的理论，具有更好的表现。

    

    强化学习（Reinforcement Learning，RL）允许一个能代理与环境进行连续交互的系统最大化预期收益。在分布式RL（DistrRL）范式下，代理不仅局限于期望值，而是捕捉跨越所有时间步骤的回报概率分布。DistrRL算法的集合提高了经验性能，但DistrRL的理论仍未完全理解，尤其在控制案例中。本文提出了简单的一步分布式强化学习（OS-DistrRL）框架，仅涵盖环境一步动态引入的随机性。与DistrRL相反，我们证明了我们的方法针对策略评估和控制都具有统一的理论。我们提出了两种OS-DistrRL算法，并提供了几乎确定的收敛分析。所提出的方法在多种环境中比分类DistrRL好。

    Reinforcement learning (RL) allows an agent interacting sequentially with an environment to maximize its long-term expected return. In the distributional RL (DistrRL) paradigm, the agent goes beyond the limit of the expected value, to capture the underlying probability distribution of the return across all time steps. The set of DistrRL algorithms has led to improved empirical performance. Nevertheless, the theory of DistrRL is still not fully understood, especially in the control case. In this paper, we present the simpler one-step distributional reinforcement learning (OS-DistrRL) framework encompassing only the randomness induced by the one-step dynamics of the environment. Contrary to DistrRL, we show that our approach comes with a unified theory for both policy evaluation and control. Indeed, we propose two OS-DistrRL algorithms for which we provide an almost sure convergence analysis. The proposed approach compares favorably with categorical DistrRL on various environments.
    
[^76]: 使用约束贝叶斯优化的网络级联漏洞研究

    Network Cascade Vulnerability using Constrained Bayesian Optimization. (arXiv:2304.14420v1 [cs.SI])

    [http://arxiv.org/abs/2304.14420](http://arxiv.org/abs/2304.14420)

    本研究基于约束贝叶斯优化，以修改输电线路保护设置为敌对攻击的候选方案，探讨了最大化级联网络退化的保护设置规律，发现将所有电网线路的保护设置最大失配并不会导致最多的级联。

    

    评估电网的脆弱性常常是通过敌手能够对网络造成的损害量来衡量的。然而，这样攻击的级联影响通常被忽视，尽管级联是大规模停电的主要原因之一。本文探讨了将输电线路保护设置修改为敌对攻击的候选方案，只要网络平衡状态不改变，攻击就可以保持不被检测到。这构成了贝叶斯优化过程中的一个黑盒子函数基础，其目标是找到最大化级联网络退化的保护设置。广泛的实验表明，与常识相反，将所有网络线路的保护设置最大失配并不会导致最多的级联。更令人惊讶的是，即使在资源受限的情况下，仍然可以找到能够产生与实例相当严重的级联的设置。

    Measures of power grid vulnerability are often assessed by the amount of damage an adversary can exact on the network. However, the cascading impact of such attacks is often overlooked, even though cascades are one of the primary causes of large-scale blackouts. This paper explores modifications of transmission line protection settings as candidates for adversarial attacks, which can remain undetectable as long as the network equilibrium state remains unaltered. This forms the basis of a black-box function in a Bayesian optimization procedure, where the objective is to find protection settings that maximize network degradation due to cascading. Extensive experiments reveal that, against conventional wisdom, maximally misconfiguring the protection settings of all network lines does not cause the most cascading. More surprisingly, even when the degree of misconfiguration is resource constrained, it is still possible to find settings that produce cascades comparable in severity to instanc
    
[^77]: SSTM：用于多帧光流估计的时空循环变压器

    SSTM: Spatiotemporal Recurrent Transformers for Multi-frame Optical Flow Estimation. (arXiv:2304.14418v1 [cs.CV])

    [http://arxiv.org/abs/2304.14418](http://arxiv.org/abs/2304.14418)

    该论文提出了一种基于学习的多帧光流估计方法，通过理解具有多于两帧的更长序列中时间场景的动态，推广复杂的运动模式，从而提高光流准确性。

    

    光流估计算法的两个主要限制是在被遮挡区域内和靠近边界区域内的不准确的光流估计。最近的最先进的光流估计算法是基于两帧的方法，其中光流在序列中的每个连续图像对上被顺序估计。虽然这种方法可以给出良好的光流估计，但由于场景中移动元素的受限局部证据有限，它无法推广到被遮挡区域中的光流。在这项工作中，我们提出了一种基于学习的多帧光流估计方法，该方法从多帧图像序列中并行地估计两个或更多个连续光流。我们的基本假设是通过理解具有多于两帧的更长序列中的时间场景动态，我们可以在更大的时空域中表征像素级依赖性，推广复杂的运动模式，从而提高光流的准确性。

    Inaccurate optical flow estimates in and near occluded regions, and out-of-boundary regions are two of the current significant limitations of optical flow estimation algorithms. Recent state-of-the-art optical flow estimation algorithms are two-frame based methods where optical flow is estimated sequentially for each consecutive image pair in a sequence. While this approach gives good flow estimates, it fails to generalize optical flows in occluded regions mainly due to limited local evidence regarding moving elements in a scene. In this work, we propose a learning-based multi-frame optical flow estimation method that estimates two or more consecutive optical flows in parallel from multi-frame image sequences. Our underlying hypothesis is that by understanding temporal scene dynamics from longer sequences with more than two frames, we can characterize pixel-wise dependencies in a larger spatiotemporal domain, generalize complex motion patterns and thereby improve the accuracy of optica
    
[^78]: SSL模型是否感到“似曾相识”？自监督学习中的意外记忆

    Do SSL Models Have D\'ej\`a Vu? A Case of Unintended Memorization in Self-supervised Learning. (arXiv:2304.13850v1 [cs.CV])

    [http://arxiv.org/abs/2304.13850](http://arxiv.org/abs/2304.13850)

    自监督学习（SSL）算法会意外地记忆单个训练样本中的特定部分，称为“似曾相识”记忆，该现象是普遍存在的，不能被传统的评估方法检测。

    

    自监督学习（SSL）算法可以通过学习将自然图像的不同部分相互关联来产生有用的图像表示。然而，当被推向极端时，SSL模型会意外地记忆单个训练样本中的特定部分，而不是学习有意义的语义关联。在本研究中，我们对SSL模型中的意外记忆现象进行了系统研究，我们称之为“似曾相识”记忆。具体而言，我们展示了在给定训练模型和一个仅包含背景（如水、天空、草地）的训练图像裁剪后，可以高精度或甚至视觉重构地推断出前景对象。此外，我们展示了“似曾相识”记忆是不同SSL算法的普遍现象，并且会因某些设计选择而恶化，而且不能通过传统的评估表示质量的技术来检测。“似曾相识”记忆的研究

    Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another. However, when taken to the extreme, SSL models can unintendedly memorize specific parts in individual training samples rather than learning semantically meaningful associations. In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models -- which we refer to as d\'ej\`a vu memorization. Concretely, we show that given the trained model and a crop of a training image containing only the background (e.g., water, sky, grass), it is possible to infer the foreground object with high accuracy or even visually reconstruct it. Furthermore, we show that d\'ej\`a vu memorization is common to different SSL algorithms, is exacerbated by certain design choices, and cannot be detected by conventional techniques for evaluating representation quality. Our study of d\'ej\`a vu memorizatio
    
[^79]: 无标记时间分析模块化核探测器的物理约束深度学习方法

    Label-free timing analysis of modularized nuclear detectors with physics-constrained deep learning. (arXiv:2304.11930v2 [physics.ins-det] UPDATED)

    [http://arxiv.org/abs/2304.11930](http://arxiv.org/abs/2304.11930)

    该研究描述了一种基于深度学习的新方法，用于模块化核探测器的无标记时间分析，其能够利用单个探测器内部时间相关性，实现有意义和准确的映射函数。

    

    脉冲时间是核仪器学中重要的话题，具有从高能物理到辐射成像的广泛应用。尽管高速模数转换器越来越发展和易于使用，但它们在核探测器信号处理中的潜在用途和优点仍不确定，部分原因是相关的时间算法还没有得到充分理解和利用。在本文中，我们提出了一种基于深度学习的新方法，用于模块化核探测器的时间分析，无需对事件数据进行显式的标记。通过利用单个探测器内部时间相关性，形成一个无标记损失函数和一个经过特殊设计的正则化器，以监督神经网络的训练，以获得有意义和准确的映射函数。我们从数学上证明了所需方法的最优函数的存在，并提供了一个系统的算法来训练和校准模型。该方法的应用在核能谱学中表现出较好的效果。

    Pulse timing is an important topic in nuclear instrumentation, with far-reaching applications from high energy physics to radiation imaging. While high-speed analog-to-digital converters become more and more developed and accessible, their potential uses and merits in nuclear detector signal processing are still uncertain, partially due to associated timing algorithms which are not fully understood and utilized. In this paper, we propose a novel method based on deep learning for timing analysis of modularized nuclear detectors without explicit needs of labelling event data. By taking advantage of the inner time correlation of individual detectors, a label-free loss function with a specially designed regularizer is formed to supervise the training of neural networks towards a meaningful and accurate mapping function. We mathematically demonstrate the existence of the optimal function desired by the method, and give a systematic algorithm for training and calibration of the model. The pr
    
[^80]: 压缩与否——自监督学习与信息论:一篇综述

    To Compress or Not to Compress -- Self-Supervised Learning and Information Theory: A Review. (arXiv:2304.09355v1 [cs.LG])

    [http://arxiv.org/abs/2304.09355](http://arxiv.org/abs/2304.09355)

    本文从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。此外，讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    

    深度神经网络在监督学习任务中表现出了卓越的性能，但需要大量的标注数据。自监督学习提供了一个替代范例，使得模型可以在没有明确标签的情况下学习。信息论在理解和优化深度神经网络方面起着关键作用。特别地，信息瓶颈原则被应用于在监督设置中优化压缩和相关信息保存之间的权衡。然而，自监督学习中的最佳信息目标仍然不清楚。在本文中，我们从信息论的角度回顾了各种自监督学习方法，并提出了一个正式的“自监督信息理论学习问题”统一框架。我们将现有研究融合成一个一致的框架，研究了最近的自监督方法，并确定了研究机会和挑战。此外，我们还讨论了压缩性和压缩算法在自监督学习中的作用，并凸显了潜在的未来方向。

    Deep neural networks have demonstrated remarkable performance in supervised learning tasks but require large amounts of labeled data. Self-supervised learning offers an alternative paradigm, enabling the model to learn from data without explicit labels. Information theory has been instrumental in understanding and optimizing deep neural networks. Specifically, the information bottleneck principle has been applied to optimize the trade-off between compression and relevant information preservation in supervised settings. However, the optimal information objective in self-supervised learning remains unclear. In this paper, we review various approaches to self-supervised learning from an information-theoretic standpoint and present a unified framework that formalizes the \textit{self-supervised information-theoretic learning problem}. We integrate existing research into a coherent framework, examine recent self-supervised methods, and identify research opportunities and challenges. Moreove
    
[^81]: 用TiDE进行长期预测：时间序列稠密编码器

    Long-term Forecasting with TiDE: Time-series Dense Encoder. (arXiv:2304.08424v1 [stat.ML])

    [http://arxiv.org/abs/2304.08424](http://arxiv.org/abs/2304.08424)

    TiDE是一种基于MLP的编码器-解码器模型，用于长期时间序列预测。它既具备线性模型的简单性和速度，又能处理协变量和非线性依赖，相较于最佳的Transformer模型，速度快5-10倍。

    

    最近的研究表明，相比于基于Transformer的方法，简单的线性模型在长期时间序列预测中表现更好。鉴于此，我们提出了一种基于多层感知机(MLP)的编码器-解码器模型，即时间序列稠密编码器(TiDE)，用于长期时间序列预测。它既享有线性模型的简单性和速度，又能处理协变量和非线性依赖。从理论上讲，我们证明了我们模型的最简线性类比在一些假设下可以达到线性动态系统(LDS)的近乎最优误差率。实证上，我们表明，我们的方法可以在流行的长期时间序列预测基准测试中匹配或胜过以前的方法，同时比最佳的基于Transformer的模型快5-10倍。

    Recent work has shown that simple linear models can outperform several Transformer based approaches in long term time-series forecasting. Motivated by this, we propose a Multi-layer Perceptron (MLP) based encoder-decoder model, Time-series Dense Encoder (TiDE), for long-term time-series forecasting that enjoys the simplicity and speed of linear models while also being able to handle covariates and non-linear dependencies. Theoretically, we prove that the simplest linear analogue of our model can achieve near optimal error rate for linear dynamical systems (LDS) under some assumptions. Empirically, we show that our method can match or outperform prior approaches on popular long-term time-series forecasting benchmarks while being 5-10x faster than the best Transformer based model.
    
[^82]: Data-OOB:以无需额外计算的Out-of-bag估计为准的数据价值估计方法

    Data-OOB: Out-of-bag Estimate as a Simple and Efficient Data Value. (arXiv:2304.07718v1 [cs.LG])

    [http://arxiv.org/abs/2304.07718](http://arxiv.org/abs/2304.07718)

    Data-OOB是一种新的数据价值估计方法，它利用out-of-bag估计，并可以在计算上高效处理大型数据集。

    

    数据评估是一个强大的框架，可以为模型训练提供统计洞察力，以区分哪些数据对于模型训练是有益的，哪些是有害的。纵观各种下游任务，许多以Shapley为基础的数据价值评估方法均显示出了很有前途的结果。然而，由于这需要训练大量的模型，因此众所周知，这是具有挑战性的。因此，将此应用于大型数据集是不可行的。为了解决这个问题，我们提出了Data-OOB，这是一种新的数据价值估计方法，针对bagging模型，它利用了out-of-bag估计。所提出的方法在计算上是高效的，可以通过重复使用训练好的弱学习器来扩展到数百万个数据。具体而言，当评估100个输入维度且存在$10^6$个样本时，Data-OOB仅需要在单个CPU处理器上执行不到2.25个小时。此外，Data-OOB在理论上有坚实的解释，当两个离差值函数相同时，其识别具有相同重要性的数据点。

    Data valuation is a powerful framework for providing statistical insights into which data are beneficial or detrimental to model training. Many Shapley-based data valuation methods have shown promising results in various downstream tasks, however, they are well known to be computationally challenging as it requires training a large number of models. As a result, it has been recognized as infeasible to apply to large datasets. To address this issue, we propose Data-OOB, a new data valuation method for a bagging model that utilizes the out-of-bag estimate. The proposed method is computationally efficient and can scale to millions of data by reusing trained weak learners. Specifically, Data-OOB takes less than 2.25 hours on a single CPU processor when there are $10^6$ samples to evaluate and the input dimension is 100. Furthermore, Data-OOB has solid theoretical interpretations in that it identifies the same important data point as the infinitesimal jackknife influence function when two d
    
[^83]: ChatGPT在多模态股票预测挑战中的零样本分析：华尔街新手？

    The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])

    [http://arxiv.org/abs/2304.05351](http://arxiv.org/abs/2304.05351)

    本研究对ChatGPT在股票预测方面进行了零样本分析，结果表明其预测股票移动的表现不如最先进和传统方法，需要进一步改进。

    

    最近，如ChatGPT这样的大型语言模型在各种自然语言处理任务中展示了惊人的性能。然而，在预测股市走势方面，它们的有效性仍然有待探索。本文通过三个推文和历史股票价格数据集的广泛零样本分析，探讨了ChatGPT在多模态股票移动预测方面的能力。我们的研究表明，ChatGPT是一个“华尔街新手”，在预测股票移动方面的成功有限，不仅不如最先进的方法，而且不如使用价格特征的线性回归这样的传统方法。尽管思维链提示策略和推文的包含具有潜在的优势，ChatGPT的表现仍然不佳。此外，我们观察到它的可解释性和稳定性存在局限性，需要更专业的训练或微调。这项研究提供了有关ChatGPT在股票预测方面的见解。

    Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights i
    
[^84]: 一种有效的语音驱动动态生成脸部特征的统一压缩框架

    A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation. (arXiv:2304.00471v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2304.00471](http://arxiv.org/abs/2304.00471)

    该研究开发了一种轻量级的语音驱动动态生成脸部特征模型，并通过从基于生成对抗网络的原始模型中移除残差块和减少通道宽度以及应用端到端的压缩技术实现了高效的推理和占用更少内存。

    

    虚拟人类已经引起了许多行业的关注，例如娱乐和电子商务。人们积极研究利用生成对抗网络从目标语音和面部身份合成逼真的人脸。尽管现代生成模型的结果显着，但它们往往需要大量计算，限制了它们的有效部署。这项研究旨在开发一种轻量级语音驱动的动态生成脸部特征模型。我们通过从流行的动态生成脸部特征模型Wav2Lip中移除残差块和减少通道宽度来构建一个紧凑的生成器。我们还提出了一种知识蒸馏方案，以稳定而有效地训练小容量生成器而不需要对抗性学习。我们将参数数量和MAC的数量减少了28倍，同时保留了原始模型的性能。此外，为了缓解将整个生成器转换为图像信号处理器时性能严重下降的问题，我们提出了一个端到端的压缩框架，该框架包括ISP感知修剪，量化和Huffman编码。在一个大规模公开数据集上进行的实验表明，我们提出的框架以显著更快的速度生成高质量的动态生成脸部特征，并占用更少的内存。

    Virtual humans have gained considerable attention in numerous industries, e.g., entertainment and e-commerce. As a core technology, synthesizing photorealistic face frames from target speech and facial identity has been actively studied with generative adversarial networks. Despite remarkable results of modern talking-face generation models, they often entail high computational burdens, which limit their efficient deployment. This study aims to develop a lightweight model for speech-driven talking-face synthesis. We build a compact generator by removing the residual blocks and reducing the channel width from Wav2Lip, a popular talking-face generator. We also present a knowledge distillation scheme to stably yet effectively train the small-capacity generator without adversarial learning. We reduce the number of parameters and MACs by 28$\times$ while retaining the performance of the original model. Moreover, to alleviate a severe performance drop when converting the whole generator to I
    
[^85]: 采用灵活概率神经网络方法的集合天气预报后处理

    Ensemble weather forecast post-processing with a flexible probabilistic neural network approach. (arXiv:2303.17610v1 [cs.LG])

    [http://arxiv.org/abs/2303.17610](http://arxiv.org/abs/2303.17610)

    本论文提出了一种神经网络和归一化流相结合的方法，可联合预测所有位置和提前期，从而放宽了许多传统后处理方法的分布假设，并通过EUPPBench基准测试证明了其超越性能。

    

    集合预报后处理是生成准确概率预报的必要步骤。传统的后处理方法是根据每个位置或每个提前期估计参数统计分布。我们提出了一种基于神经网络的新方法，它可以联合预测所有位置和提前期。为了放宽许多后处理方法的分布假设，我们的方法采用归一化流作为灵活的参数分布估计器。这使我们能够以数学确切的方式模拟不同的预测分布。我们在EUPPBench基准测试中展示了我们方法的有效性，在该测试中，我们对西欧地区子区域的站点进行了温度预报后处理。我们展示了我们的新方法在基准测试中表现出卓越的性能，超越了我们之前的表现良好的成绩。此外，通过提供详细的比较，我们证明了我们方法的优越性。

    Ensemble forecast post-processing is a necessary step in producing accurate probabilistic forecasts. Conventional post-processing methods operate by estimating the parameters of a parametric distribution, frequently on a per-location or per-lead-time basis. We propose a novel, neural network-based method, which produces forecasts for all locations and lead times, jointly. To relax the distributional assumption of many post-processing methods, our approach incorporates normalizing flows as flexible parametric distribution estimators. This enables us to model varying forecast distributions in a mathematically exact way. We demonstrate the effectiveness of our method in the context of the EUPPBench benchmark, where we conduct temperature forecast post-processing for stations in a sub-region of western Europe. We show that our novel method exhibits state-of-the-art performance on the benchmark, outclassing our previous, well-performing entry. Additionally, by providing a detailed compariso
    
[^86]: 隐藏流形Hopfield模型及其学习相变

    The Hidden-Manifold Hopfield Model and a learning phase transition. (arXiv:2303.16880v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2303.16880](http://arxiv.org/abs/2303.16880)

    提出一种称为隐藏流形Hopfield模型的广义Hopfield模型，重点研究了在复杂数据上的应用，发现其中存在着一种学习相变的现象。

    

    Hopfield模型在统计物理学中有着悠久的传统，是少数具有理论基础的神经网络之一。通过将Hopfield模型的理论拓展到相关数据上，可以帮助我们理解深度神经网络的成功，例如描述它们如何从数据中提取特征。出于这个动机，我们提出并研究了一种广义的Hopfield模型，称为隐藏流形Hopfield模型：我们使用称为因子的$D=\alpha_D N$个随机向量的非线性变换，使用来自$P=\alpha N$个示例的Hebb规则生成耦合，其中$N$是神经元的数量。使用重复方法，我们获得了该模型的相图，显示出一个相变，其中在示例中隐藏的因子成为动态学的吸引子；这种相存在于关键的$\alpha$值以上和$\alpha_D$关键的值以下。我们将这种行为称为学习相变。

    The Hopfield model has a long-standing tradition in statistical physics, being one of the few neural networks for which a theory is available. Extending the theory of Hopfield models for correlated data could help understand the success of deep neural networks, for instance describing how they extract features from data. Motivated by this, we propose and investigate a generalized Hopfield model that we name Hidden-Manifold Hopfield Model: we generate the couplings from $P=\alpha N$ examples with the Hebb rule using a non-linear transformation of $D=\alpha_D N$ random vectors that we call factors, with $N$ the number of neurons. Using the replica method, we obtain a phase diagram for the model that shows a phase transition where the factors hidden in the examples become attractors of the dynamics; this phase exists above a critical value of $\alpha$ and below a critical value of $\alpha_D$. We call this behaviour learning transition.
    
[^87]: 使用图神经网络重建粒子物理过程的拓扑结构

    Topological Reconstruction of Particle Physics Processes using Graph Neural Networks. (arXiv:2303.13937v1 [hep-ph])

    [http://arxiv.org/abs/2303.13937](http://arxiv.org/abs/2303.13937)

    Topograph是一种利用图神经网络和粒子衰变自然规律的拓扑结构重建方法，不仅解决了观测到的末态对象组合指派问题，还预测了中间粒子的性质及其后续衰变，比标准方法效果更好，与现代机器学习技术表现相当。

    

    我们提出了一种新的方法，称为Topograph，它利用粒子物理衰变的本质和信息传递图神经网络的灵活性，重建了包括中介粒子在内的底层物理过程。Topograph不仅解决了观测到的末态对象的组合指派问题，将它们与它们原来的母粒子关联起来，而且直接预测了硬散射过程中中间粒子的性质及其后续衰变。与标准的组合方法或现代图神经网络方法相比，它的复杂度与重构对象的数量成线性关系。我们应用Topograph于全强子衰变模式下的顶夸克对产生问题，相对标准方法，我们的方法表现更优，与最先进的机器学习技术相当。

    We present a new approach, the Topograph, which reconstructs underlying physics processes, including the intermediary particles, by leveraging underlying priors from the nature of particle physics decays and the flexibility of message passing graph neural networks. The Topograph not only solves the combinatoric assignment of observed final state objects, associating them to their original mother particles, but directly predicts the properties of intermediate particles in hard scatter processes and their subsequent decays. In comparison to standard combinatoric approaches or modern approaches using graph neural networks, which scale exponentially or quadratically, the complexity of Topographs scales linearly with the number of reconstructed objects.  We apply Topographs to top quark pair production in the all hadronic decay channel, where we outperform the standard approach and match the performance of the state-of-the-art machine learning technique.
    
[^88]: DR.CPO：通过迭代构建、随机放置和 HPR 遮蔽实现的多样化和逼真的三维增强

    DR.CPO: Diversified and Realistic 3D Augmentation via Iterative Construction, Random Placement, and HPR Occlusion. (arXiv:2303.12743v1 [cs.CV])

    [http://arxiv.org/abs/2303.12743](http://arxiv.org/abs/2303.12743)

    该论文提出了一种多样化和逼真的增强方法，可以创建整体对象并灵活地定位和旋转对象，并相应地应用自遮挡和外遮挡。通过迭代构建多个对象来提高整体对象构造的多样性，构造的对象可以在训练帧中随机放置和旋转。

    

    在自动驾驶中，数据增强常用于改进三维物体检测。最基本的方法包括插入复制对象和旋转和缩放整个训练帧。也已经开发了许多变体。然而，现有方法与现实世界的可能性相比相当有限。在这项工作中，我们开发了一种多样化和逼真增强方法，可以灵活地构造整体对象，自由地定位和旋转对象，并相应地应用自遮挡和外遮挡。为了提高整体对象构造的多样性，我们开发了一种迭代方法，将从现实世界观察到的多个对象随机组合成单个对象。与现有增强方法不同的是，构造的对象可以随机放置和旋转在训练帧中，因为适当的遮挡可以反映在最终整体对象中。最后，为了防止过度增强导致过拟合，我们介绍了一种分层遮挡概率设置，通过对象的位置和大小调整遮挡强度。

    In autonomous driving, data augmentation is commonly used for improving 3D object detection. The most basic methods include insertion of copied objects and rotation and scaling of the entire training frame. Numerous variants have been developed as well. The existing methods, however, are considerably limited when compared to the variety of the real world possibilities. In this work, we develop a diversified and realistic augmentation method that can flexibly construct a whole-body object, freely locate and rotate the object, and apply self-occlusion and external-occlusion accordingly. To improve the diversity of the whole-body object construction, we develop an iterative method that stochastically combines multiple objects observed from the real world into a single object. Unlike the existing augmentation methods, the constructed objects can be randomly located and rotated in the training frame because proper occlusions can be reflected to the whole-body objects in the final step. Fina
    
[^89]: DeepMAD: 基于数学的深度卷积神经网络架构设计

    DeepMAD: Mathematical Architecture Design for Deep Convolutional Neural Network. (arXiv:2303.02165v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.02165](http://arxiv.org/abs/2303.02165)

    DeepMAD是一种基于数学的新框架，能够在系统化的方式中设计出高性能的CNN模型，解决了CNN网络设计过程中的挑战性问题。

    

    近期视觉Transformer（ViT）的快速发展使其在各种视觉任务中刷新了最先进的性能, 超过了传统的基于卷积神经网络的模型。这引发了一些最近在卷积神经网络领域的反击性研究，表明当仔细调整时，纯CNN模型可以实现与ViT模型一样出色的性能。然而，设计这样高性能的CNN模型是具有挑战性的，需要对网络设计具有非平凡的先验知识。为此，提出了一种新的框架，称为深度卷积神经网络的数学架构设计（DeepMAD）, 以系统的方式设计高性能的CNN模型。在DeepMAD中，CNN网络被建模为一个信息处理系统，其表现力和效果可以通过结构参数进行分析和计算。然后，提出了一个约束的数学规划（MP）问题来优化这些结构参数。MP问题可以通过现成的MP求解器在CPU上轻松解决，并具有较小的记忆量。

    The rapid advances in Vision Transformer (ViT) refresh the state-of-the-art performances in various vision tasks, overshadowing the conventional CNN-based models. This ignites a few recent striking-back research in the CNN world showing that pure CNN models can achieve as good performance as ViT models when carefully tuned. While encouraging, designing such high-performance CNN models is challenging, requiring non-trivial prior knowledge of network design. To this end, a novel framework termed Mathematical Architecture Design for Deep CNN (DeepMAD) is proposed to design high-performance CNN models in a principled way. In DeepMAD, a CNN network is modeled as an information processing system whose expressiveness and effectiveness can be analytically formulated by their structural parameters. Then a constrained mathematical programming (MP) problem is proposed to optimize these structural parameters. The MP problem can be easily solved by off-the-shelf MP solvers on CPUs with a small memo
    
[^90]: 使用分位数回归森林的可解释上下文异常检测

    Explainable Contextual Anomaly Detection using Quantile Regression Forests. (arXiv:2302.11239v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11239](http://arxiv.org/abs/2302.11239)

    该论文提出了一种可解释的上下文异常检测方法，运用分位数回归森林来模拟特征之间的依赖关系，能够更准确和可解释地识别偏离类似对象上下文的其他对象。

    

    传统的异常检测方法通过平等对待所有特征来识别偏离大多数其他对象的对象。相比之下，上下文异常检测方法通过将特征划分为上下文特征和行为特征，旨在检测偏离类似对象上下文的其他对象。在本文中，我们建立了依赖于传统的基于依赖的异常检测方法和上下文异常检测方法之间的联系。基于由此获得的见解，我们提出了一种新颖的方法，采用分位数回归森林来模拟特征之间的依赖关系，实现内在的可解释上下文异常检测。各种合成和真实世界数据集上的广泛实验表明，我们的方法在识别上下文异常方面的准确性和可解释性方面优于现有的状态-of-art异常检测方法。

    Traditional anomaly detection methods aim to identify objects that deviate from most other objects by treating all features equally. In contrast, contextual anomaly detection methods aim to detect objects that deviate from other objects within a context of similar objects by dividing the features into contextual features and behavioral features. In this paper, we develop connections between dependency-based traditional anomaly detection methods and contextual anomaly detection methods. Based on resulting insights, we propose a novel approach to inherently interpretable contextual anomaly detection that uses Quantile Regression Forests to model dependencies between features. Extensive experiments on various synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art anomaly detection methods in identifying contextual anomalies in terms of accuracy and interpretability.
    
[^91]: 将黑匣子分解为可解释模型的混合物：路线规划，解释，重复。

    Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10289](http://arxiv.org/abs/2302.10289)

    本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。

    

    机器学习模型设计要么从解释性模型开始，要么从黑盒开始并事后解释。黑盒模型灵活但难以解释，而解释性模型本质上是可解释的。然而，解释性模型需要广泛的机器学习知识，并且往往比它们的黑盒变体不够灵活和表现不佳。本文旨在模糊黑盒的事后解释和构建可解释模型之间的界限。我们从黑盒开始，迭代地Carve出一种混合解释模型（MoIE）和一个残余网络。每个可解释模型专门处理一个样本子集，并使用一阶逻辑(FOL)对其进行解释，从黑盒中提供基本推理概念。我们通过灵活的残差路由其余的样本。我们在残转网络上重复该方法，直到所有可解释模型解释所需比例的数据。我们进行了大量实验，结果表明我们的路线规划，解释和重复方法在各种数据集上优于目前几种黑匣子模型解释方法，并产生高度可解释的模型。

    ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
    
[^92]: 使用稀疏连接和选择性学习的可扩展实时循环学习

    Scalable Real-Time Recurrent Learning Using Sparse Connections and Selective Learning. (arXiv:2302.05326v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05326](http://arxiv.org/abs/2302.05326)

    本文提出了两个限制使得实时循环学习算法具有可扩展性，分别是将网络分解为独立模块或逐步学习网络。与其他可扩展算法不同的是，这些算法不会向梯度估计添加噪声或偏差，而是通过权衡网络的功能能力以实现可扩展学习。

    

    从感知观察中构建状态是强化学习代理的重要组成部分。一种用于状态构建的解决方案是使用循环神经网络。 BPTT和实时循环学习（RTRL）是两种流行的基于梯度的循环学习方法。 BPTT在计算梯度之前需要完整的观察序列，不适合在线实时更新。 RTRL可以进行在线更新，但不适用于大型网络。 在本文中，我们提出了两个限制，使RTRL具有可扩展性。我们表明，通过将网络分解为独立模块或逐步学习网络，我们可以使RTRL与参数数量呈线性比例关系。与先前的可扩展梯度估计算法（例如UORO和Truncated-BPTT）不同，我们的算法不会向梯度估计添加噪声或偏差。相反，它们权衡了网络的功能能力以实现可扩展学习。

    State construction from sensory observations is an important component of a reinforcement learning agent. One solution for state construction is to use recurrent neural networks. Back-propagation through time (BPTT), and real-time recurrent learning (RTRL) are two popular gradient-based methods for recurrent learning. BPTT requires the complete sequence of observations before computing gradients and is unsuitable for online real-time updates. RTRL can do online updates but scales poorly to large networks. In this paper, we propose two constraints that make RTRL scalable. We show that by either decomposing the network into independent modules, or learning the network incrementally, we can make RTRL scale linearly with the number of parameters. Unlike prior scalable gradient estimation algorithms, such as UORO and Truncated-BPTT, our algorithms do not add noise or bias to the gradient estimate. Instead, they trade-off the functional capacity of the network to achieve scalable learning. W
    
[^93]: 线性最优偏移嵌入

    Linear Optimal Partial Transport Embedding. (arXiv:2302.03232v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03232](http://arxiv.org/abs/2302.03232)

    本文提出了线性最优偏移嵌入技术（LOPT），它扩展了（局部）线性化技术到OPT问题上，提高了正测度对之间的计算速度。并且在点云内插和PCA分析中进行了应用。

    

    最优传输（OT）由于在机器学习、统计学和信号处理等领域中的广泛应用而变得越来越受欢迎。然而，平衡质量限制了它在实际问题中的性能。为了解决这些问题，已经提出了OT问题的变体，包括不平衡OT，最优偏移传输（OPT）和Hellinger Kantorovich（HK）。在本文中，我们提出了线性最优偏移（LOPT）嵌入技术，它将OT和HK上的（局部）线性化技术扩展到OPT问题上。所提出的嵌入技术提高了正测度对之间的LOPT距离的计算速度。除了我们的理论贡献，我们还展示了LOPT嵌入技术在点云内插和PCA分析中的应用。

    Optimal transport (OT) has gained popularity due to its various applications in fields such as machine learning, statistics, and signal processing. However, the balanced mass requirement limits its performance in practical problems. To address these limitations, variants of the OT problem, including unbalanced OT, Optimal partial transport (OPT), and Hellinger Kantorovich (HK), have been proposed. In this paper, we propose the Linear optimal partial transport (LOPT) embedding, which extends the (local) linearization technique on OT and HK to the OPT problem. The proposed embedding allows for faster computation of OPT distance between pairs of positive measures. Besides our theoretical contributions, we demonstrate the LOPT embedding technique in point-cloud interpolation and PCA analysis.
    
[^94]: 基于时间注意机制的中期风电功率预测新框架

    A novel framework for medium-term wind power prediction based on temporal attention mechanisms. (arXiv:2302.01222v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01222](http://arxiv.org/abs/2302.01222)

    本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架（TPE-VMD-TFT），用于24小时和48小时之前的风电功率预测。在法国电力公司Engie的风能数据集上，所提出的方法表现良好。

    

    风能是一种广泛分布、可再生和环保的能源，对缓解全球变暖和能源短缺具有重要作用。然而，由于其不确定性和波动性，大规模风电系统的网格集成具有挑战性。中期风电功率预测可以为能量调度提供基本依据，因此精确的风电功率预测至关重要。本文提出了一种基于树状Parzen估计器（TPE）和分解算法的新框架。该框架基于变分模式分解（VMD）和时间融合变压器（TFT）定义了24小时和48小时之前的风电功率预测的TPE-VMD-TFT方法。在法国电力公司Engie的风能数据集上，结果表明所提出的方法优于其他方法。

    Wind energy is a widely distributed, recyclable and environmentally friendly energy source that plays an important role in mitigating global warming and energy shortages. Wind energy's uncertainty and fluctuating nature makes grid integration of large-scale wind energy systems challenging. Medium-term wind power forecasts can provide an essential basis for energy dispatch, so accurate wind power forecasts are essential. Much research has yielded excellent results in recent years. However, many of them require additional experimentation and analysis when applied to other data. In this paper, we propose a novel short-term forecasting framework by tree-structured parzen estimator (TPE) and decomposition algorithms. This framework defines the TPE-VMD-TFT method for 24-h and 48-h ahead wind power forecasting based on variational mode decomposition (VMD) and time fusion transformer (TFT). In the Engie wind dataset from the electricity company in France, the results show that the proposed met
    
[^95]: CMLCompiler：一个面向经典机器学习的统一编译器

    CMLCompiler: A Unified Compiler for Classical Machine Learning. (arXiv:2301.13441v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13441](http://arxiv.org/abs/2301.13441)

    本文提出了一个名为CMLCompiler的统一编译器框架，用于经典机器学习（CML）推理。通过提出两种统一的抽象：运算符表示和扩展计算图，CMLCompiler框架可以将用于CML的已优化计算图转换并输出到DL编译器或框架中，实现了卓越的可移植性和性能提升。

    

    经典机器学习（CML）几乎占据了生产应用中近一半的机器学习流程。然而，它未能充分利用最先进的设备，性能表现较差。在没有统一框架的情况下，深度学习（DL）和CML的混合部署也会遇到严重的性能和可移植性问题。本文提出了一种统一的编译器框架CMLCompiler，用于CML推理。我们提出了两种统一的抽象：运算符表示和扩展计算图。CMLCompiler框架基于这两种统一的抽象执行转换和图优化，然后将优化后的计算图输出到DL编译器或框架中。我们在TVM上实现CMLCompiler。评估表明，CMLCompiler具有卓越的可移植性和性能。与最先进的解决方案（scikit-learn，XGBoost等）相比，它在CPU上实现了多达4.38倍的加速，GPU上实现了3.31倍的加速，IoT设备上实现了5.09倍的加速。

    Classical machine learning (CML) occupies nearly half of machine learning pipelines in production applications. Unfortunately, it fails to utilize the state-of-the-practice devices fully and performs poorly. Without a unified framework, the hybrid deployments of deep learning (DL) and CML also suffer from severe performance and portability issues. This paper presents the design of a unified compiler, called CMLCompiler, for CML inference. We propose two unified abstractions: operator representations and extended computational graphs. The CMLCompiler framework performs the conversion and graph optimization based on two unified abstractions, then outputs an optimized computational graph to DL compilers or frameworks. We implement CMLCompiler on TVM. The evaluation shows CMLCompiler's portability and superior performance. It achieves up to 4.38$\times$ speedup on CPU, 3.31$\times$ speedup on GPU, and 5.09$\times$ speedup on IoT devices, compared to the state-of-the-art solutions -- scikit
    
[^96]: 针对可扩展性的子图表示学习简化以进行可扩展的链接预测

    Simplifying Subgraph Representation Learning for Scalable Link Prediction. (arXiv:2301.12562v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12562](http://arxiv.org/abs/2301.12562)

    提出了一种新的可扩展简化子图表示学习（S3GRL）框架，通过简化每个链接子图中的消息传递和聚合操作实现更快的训练和推理，并可适应各种子图采样策略和扩散操作符以模拟计算代价高的子图表示学习。大量实验证明了S3GRL模型可以扩展SGRL而不会显著降低性能。

    

    图上的链接预测是一个基本问题。子图表示学习方法通过将链接预测转化为在链接周围子图上的图分类来实现了最先进的链接预测。然而，子图表示学习方法计算代价高，并且由于子图水平操作的代价而不适用于大规模图形。我们提出了一种新的子图表示学习类，称为可扩展简化子图表示学习（S3GRL），旨在实现更快的训练和推理。S3GRL简化了每个链接子图中的消息传递和聚合操作。作为可扩展性框架，S3GRL适应各种子图采样策略和扩散运算符来模拟计算代价高的子图表示学习方法。我们提出了多个S3GRL实例，并在小到大规模的图形上进行了实证研究。我们广泛的实验表明，所提出的S3GRL模型可以扩展SGRL而不会显著降低性能。

    Link prediction on graphs is a fundamental problem. Subgraph representation learning approaches (SGRLs), by transforming link prediction to graph classification on the subgraphs around the links, have achieved state-of-the-art performance in link prediction. However, SGRLs are computationally expensive, and not scalable to large-scale graphs due to expensive subgraph-level operations. To unlock the scalability of SGRLs, we propose a new class of SGRLs, that we call Scalable Simplified SGRL (S3GRL). Aimed at faster training and inference, S3GRL simplifies the message passing and aggregation operations in each link's subgraph. S3GRL, as a scalability framework, accommodates various subgraph sampling strategies and diffusion operators to emulate computationally-expensive SGRLs. We propose multiple instances of S3GRL and empirically study them on small to large-scale graphs. Our extensive experiments demonstrate that the proposed S3GRL models scale up SGRLs without significant performance 
    
[^97]: 卷积增强的不断进化的注意力网络

    Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08330](http://arxiv.org/abs/2212.08330)

    本文提出了一种新颖且通用的卷积增强进化注意力机制，通过一系列残差卷积模块直接模拟标记间关系的演变，在不同层次之间促进信息流动。

    

    基于注意力机制的神经网络，比如Transformers，在许多应用中变得无处不在，包括计算机视觉、自然语言处理和时间序列分析。在所有种类的注意力网络中，注意力图是至关重要的，因为它们编码了输入标记之间的语义依赖关系。然而，大多数现有的注意力网络是基于表示进行建模或推理的，不同层次的注意力图在学习时是分别学习而不是进行显式交互。本文提出了一种新颖且通用的进化注意力机制，该机制通过一系列残差卷积模块直接模拟标记间关系的演变。其主要动机有两方面。一方面，不同层次的注意力图分享可转移知识，因此添加残差连接可以促进标记之间关系在不同层次之间的信息流动。另一方面，自然存在着演化趋势。

    Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations , wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend a
    
[^98]: 自动化刚性折纸设计

    Automating Rigid Origami Design. (arXiv:2211.13219v2 [cs.GR] UPDATED)

    [http://arxiv.org/abs/2211.13219](http://arxiv.org/abs/2211.13219)

    这篇论文介绍了一种离散优化问题-刚性折纸游戏，该游戏可以扩展刚性折纸的潜力，使其得到针对应用特定的折痕图案，从而可以得到日常物品的新颖、可折叠和实用的设计。

    

    刚性折纸在很多实际应用中展现出了其潜力，然而目前的刚性折纸褶皱图案设计主要依赖于已知的拼贴。这严重限制了可以创建的图案的多样性和新颖性。在这项工作中，我们在最近开发的三单元原则的基础上，将刚性折纸设计成一种离散优化问题，即刚性折纸游戏。我们的实现允许简单定义多样的目标，从而进一步扩展刚性折纸的潜力，得到针对应用特定的折痕图案。我们通过多种搜索方法在几个案例研究中展示了我们公式的灵活性。我们不仅能够构造出近似给定目标形状的各种图案，而且还能够指定基于函数的抽象奖励，从而得到日常物品的新颖、可折叠和实用的设计。

    Rigid origami has shown potential in large diversity of practical applications. However, current rigid origami crease pattern design mostly relies on known tessellations. This strongly limits the diversity and novelty of patterns that can be created. In this work, we build upon the recently developed principle of three units method to formulate rigid origami design as a discrete optimization problem, the rigid origami game. Our implementation allows for a simple definition of diverse objectives and thereby expands the potential of rigid origami further to optimized, application-specific crease patterns. We showcase the flexibility of our formulation through use of a diverse set of search methods in several illustrative case studies. We are not only able to construct various patterns that approximate given target shapes, but to also specify abstract, function-based rewards which result in novel, foldable and functional designs for everyday objects.
    
[^99]: 可复现模型蒸馏的通用方法

    A Generic Approach for Reproducible Model Distillation. (arXiv:2211.12631v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.12631](http://arxiv.org/abs/2211.12631)

    本文提出了一种用中心极限定理为基础的通用稳定模型蒸馏方法，能够从候选学生模型中搜索与老师模型相一致的合理模型，使用一个多重检验框架来选择一组足够大的伪数据集，以选出一致的学生模型。

    

    模型蒸馏是一种流行的生成可解释机器学习模型的方法。它使用一个可解释的“学生”模型去模仿黑盒“老师”模型产生的预测结果。然而，当学生模型对于用于训练的数据集的变异性敏感时，关于学生模型的解释就不可靠了。现有的稳定模型蒸馏策略通过检查是否生成了足够大的伪数据来稳定模型蒸馏，但目前为止，这些方法只适用于特定的学生模型。在本文中，我们提出了一种基于中心极限定理的通用稳定模型蒸馏方法。我们首先从候选的学生模型集合开始，搜索与老师模型相一致的合理候选模型。然后，我们构建一个多重检验框架，选择一组可以使得一致的学生模型被选中的伪数据集。

    Model distillation has been a popular method for producing interpretable machine learning. It uses an interpretable "student" model to mimic the predictions made by the black box "teacher" model. However, when the student model is sensitive to the variability of the data sets used for training even when keeping the teacher fixed, the corresponded interpretation is not reliable. Existing strategies stabilize model distillation by checking whether a large enough corpus of pseudo-data is generated to reliably reproduce student models, but methods to do so have so far been developed for a specific student model. In this paper, we develop a generic approach for stable model distillation based on central limit theorem for the average loss. We start with a collection of candidate student models and search for candidates that reasonably agree with the teacher. Then we construct a multiple testing framework to select a corpus size such that the consistent student model would be selected under d
    
[^100]: 基于图注意力网络与不平衡PU标签的P2E MMORPGs欺诈检测方法

    PU GNN: Chargeback Fraud Detection in P2E MMORPGs via Graph Attention Networks with Imbalanced PU Labels. (arXiv:2211.08604v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08604](http://arxiv.org/abs/2211.08604)

    本文提出了一种基于图注意力网络和PU损失函数的欺诈检测方法PU GNN，通过改进的GraphSMOTE算法来处理P2E MMORPGs欺诈检测数据集中的标签分布不平衡问题，实验证明该方法在欺诈检测方面具有良好的性能表现。

    

    最近，在大型多人在线角色扮演游戏中，游戏点卡能够直接转换为比特币、以太坊或Klaytn等加密货币，因此play-to-earn（P2E）系统的出现使得游戏物品与现实世界的价值交换比以往更加频繁。本文提出了一种新的欺诈检测方法PU GNN，该方法采用图注意力网络和PU损失函数捕捉玩家的游戏行为、P2E代币交易模式，同时采用改进的GraphSMOTE算法处理欺诈检测数据集中的标签分布不平衡问题。该方法在三个实际的P2E MMORPGs数据集上进行实验证明，取得了最新的欺诈检测性能。

    The recent advent of play-to-earn (P2E) systems in massively multiplayer online role-playing games (MMORPGs) has made in-game goods interchangeable with real-world values more than ever before. The goods in the P2E MMORPGs can be directly exchanged with cryptocurrencies such as Bitcoin, Ethereum, or Klaytn via blockchain networks. Unlike traditional in-game goods, once they had been written to the blockchains, P2E goods cannot be restored by the game operation teams even with chargeback fraud such as payment fraud, cancellation, or refund. To tackle the problem, we propose a novel chargeback fraud prediction method, PU GNN, which leverages graph attention networks with PU loss to capture both the players' in-game behavior with P2E token transaction patterns. With the adoption of modified GraphSMOTE, the proposed model handles the imbalanced distribution of labels in chargeback fraud datasets. The conducted experiments on three real-world P2E MMORPG datasets demonstrate that PU GNN achi
    
[^101]: 总变分图神经网络

    Total Variation Graph Neural Networks. (arXiv:2211.06218v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06218](http://arxiv.org/abs/2211.06218)

    本论文提出一种新型的GNN模型，通过基于图总变差的松弛来计算集群分配，并通过最小化相邻顶点之间特征的$\ell_1$距离来实现锐利转换，实现顶点聚类和图池化。

    

    最近提出的基于图的神经网络(GNN)用于顶点聚类是通过一个无监督的最小割目标进行训练的，这个目标是通过谱聚类(SC)松弛来近似的。然而，SC松弛是宽松的，虽然它提供了一个闭式解，但它也产生了过于平滑的集群分配，无法很好地将顶点分隔开。在本文中，我们提出了一个GNN模型，通过优化基于图总变差(GTV)的最小割的一个更紧密的松弛来计算集群分配。这些集群分配可以直接用于执行顶点聚类或在图分类框架中实现图池化。我们的模型包含两个核心组件：i)一层消息传递，该层最小化相邻顶点之间特征的$\ell_1$距离，这对实现集群之间的锐利转换至关重要；ii)一种无监督的损失函数，最小化了集群分配的GTV，同时确保了平衡的分区。

    Recently proposed Graph Neural Networks (GNNs) for vertex clustering are trained with an unsupervised minimum cut objective, approximated by a Spectral Clustering (SC) relaxation. However, the SC relaxation is loose and, while it offers a closed-form solution, it also yields overly smooth cluster assignments that poorly separate the vertices. In this paper, we propose a GNN model that computes cluster assignments by optimizing a tighter relaxation of the minimum cut based on graph total variation (GTV). The cluster assignments can be used directly to perform vertex clustering or to implement graph pooling in a graph classification framework. Our model consists of two core components: i) a message-passing layer that minimizes the $\ell_1$ distance in the features of adjacent vertices, which is key to achieving sharp transitions between clusters; ii) an unsupervised loss function that minimizes the GTV of the cluster assignments while ensuring balanced partitions. Experimental results sh
    
[^102]: 基于跨模态神经模型重新编程的低资源音乐风格分类

    Low-Resource Music Genre Classification with Cross-Modal Neural Model Reprogramming. (arXiv:2211.01317v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.01317](http://arxiv.org/abs/2211.01317)

    本文提出了一种基于神经模型重新编程的迁移学习方法，并针对复杂输入数据提出了输入依赖NMR范式，能够有效地进行音乐风格分类。

    

    迁移学习方法在处理训练数据有限的任务时展现出了很好的效果。然而，微调预训练的神经网络来处理目标领域数据通常需要大量的内存和计算资源。本文提出了一种基于神经模型重新编程 (NMR) 的新方法，用于利用预训练模型进行低资源音乐分类。NMR旨在通过修改冻结的预训练模型的输入，将预训练模型从源域重新调整用于目标域。除了已知的与输入无关的重新编程方法外，我们还提出了一种先进的重新编程范式：输入依赖NMR，以增加对复杂输入数据（如音频）的适应性。实验结果表明，使用这种重新编程方法，基于大规模数据集预训练的神经模型成功地进行音乐风格分类。所提出的两种输入相关的NMR迁移学习方法表现优于传统的迁移学习方法。

    Transfer learning (TL) approaches have shown promising results when handling tasks with limited training data. However, considerable memory and computational resources are often required for fine-tuning pre-trained neural networks with target domain data. In this work, we introduce a novel method for leveraging pre-trained models for low-resource (music) classification based on the concept of Neural Model Reprogramming (NMR). NMR aims at re-purposing a pre-trained model from a source domain to a target domain by modifying the input of a frozen pre-trained model. In addition to the known, input-independent, reprogramming method, we propose an advanced reprogramming paradigm: Input-dependent NMR, to increase adaptability to complex input data such as musical audio. Experimental results suggest that a neural model pre-trained on large-scale datasets can successfully perform music genre classification by using this reprogramming method. The two proposed Input-dependent NMR TL methods outpe
    
[^103]: 通过量子噪声实现量子分类器对抗样本的可靠性认证

    Certified Robustness of Quantum Classifiers against Adversarial Examples through Quantum Noise. (arXiv:2211.00887v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.00887](http://arxiv.org/abs/2211.00887)

    本文首次提出添加量子随机旋转噪声可以提高量子分类器对抗攻击的鲁棒性，并通过实验支持了一种证明性的鲁棒性边界。

    

    最近发现量子分类器容易受到对抗攻击，导致被不能察觉的噪声欺骗，从而造成分类错误。本文提出了第一个理论研究，证明添加量子随机旋转噪声可以提高量子分类器对抗攻击的鲁棒性。我们建立了差分私密性的定义，并展示了在自然存在加性噪声的情况下训练的量子分类器的差分私密性。最后，我们得出了一种证明性的鲁棒性边界，用IBM 7量子位设备产生的噪声进行了实验模拟支持。

    Recently, quantum classifiers have been found to be vulnerable to adversarial attacks, in which quantum classifiers are deceived by imperceptible noises, leading to misclassification. In this paper, we propose the first theoretical study demonstrating that adding quantum random rotation noise can improve robustness in quantum classifiers against adversarial attacks. We link the definition of differential privacy and show that the quantum classifier trained with the natural presence of additive noise is differentially private. Finally, we derive a certified robustness bound to enable quantum classifiers to defend against adversarial examples, supported by experimental results simulated with noises from IBM's 7-qubits device.
    
[^104]: 训练神经网络用于时序变点检测

    Training Neural Networks for Sequential Change-point Detection. (arXiv:2210.17312v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17312](http://arxiv.org/abs/2210.17312)

    本文介绍了一种使用神经网络进行在线变点检测的方法，通过训练神经网络逐步计算检测统计量的累积和来检测变点，并在合成和真实数据上证明了该方法的优越性和潜力。

    

    检测数据流中的突变分布转换，即所谓的变点检测，是统计学和机器学习中的一个基本问题。我们引入了一种新颖的方法，使用神经网络进行在线变点检测。具体而言，我们的方法是训练神经网络来逐步计算检测统计量的累积和，当发生变点时，该量会显著变化。我们使用合成和真实世界数据证明了所提出的方法在检测变点方面的优越性和潜力。

    Detecting an abrupt distributional shift of a data stream, known as change-point detection, is a fundamental problem in statistics and machine learning. We introduce a novel approach for online change-point detection using neural networks. To be specific, our approach is training neural networks to compute the cumulative sum of a detection statistic sequentially, which exhibits a significant change when a change-point occurs. We demonstrated the superiority and potential of the proposed method in detecting change-point using both synthetic and real-world data.
    
[^105]: 在多次攻击下提高高光谱对抗鲁棒性

    Improving Hyperspectral Adversarial Robustness Under Multiple Attacks. (arXiv:2210.16346v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16346](http://arxiv.org/abs/2210.16346)

    该论文提出了一种Adversarial Discriminator Ensemble Network（ADE-Net）来增强高光谱图像模型的鲁棒性，针对不同攻击类型使用相应的攻击专家集合网络。

    

    对高光谱图像进行语义分割的模型容易受到对抗样本的攻击。传统的对抗鲁棒性方法主要集中在对单个网络进行训练或重新训练，但是在遇到多种攻击时，这些方法的性能会下降，比单独训练每个网络都要差。为了解决这个问题，我们提出了一种Adversarial Discriminator Ensemble Network（ADE-Net），该网络专注于攻击类型检测和鲁棒性，在一个统一的模型下保留最佳的每个数据类型权重，同时增强整个网络的鲁棒性。在所提出的方法中，使用鉴别器网络将数据按攻击类型进行分离，进而针对特定攻击类型使用相应的攻击专家集合网络。

    Semantic segmentation models classifying hyperspectral images (HSI) are vulnerable to adversarial examples. Traditional approaches to adversarial robustness focus on training or retraining a single network on attacked data, however, in the presence of multiple attacks these approaches decrease in performance compared to networks trained individually on each attack. To combat this issue we propose an Adversarial Discriminator Ensemble Network (ADE-Net) which focuses on attack type detection and adversarial robustness under a unified model to preserve per data-type weight optimally while robustifiying the overall network. In the proposed method, a discriminator network is used to separate data by attack type into their specific attack-expert ensemble network.
    
[^106]: 通过统一加倍条件稳定神经网络训练的准确性

    Stability of Accuracy for the Training of DNNs Via the Uniform Doubling Condition. (arXiv:2210.08415v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.08415](http://arxiv.org/abs/2210.08415)

    本文引入了统一加倍条件，以确保在训练DNN期间准确性的稳定性，无论使用绝对值激活函数还是ReLU激活函数。在统一加倍条件下，准确率有很高的概率稳定，并提供了具体估计。

    

    本文研究深度神经网络（DNNs）训练过程中准确性的稳定性。在这个背景下，DNNs的训练通过最小化交叉熵损失函数来进行，其性能指标是准确性（正确分类的对象比例）。虽然训练会导致损失减少，但是准确性不一定随着过程增加，有时甚至会下降。实现准确性稳定性的目标是确保如果初始时准确性很高，在整个训练过程中保持高水平。本文引入了训练数据的加倍条件，以确保使用绝对值激活函数的DNN的训练期间的准确性稳定。对于在$R^n$中的训练数据，这种加倍条件使用$R^n$中的板块进行制定，并且取决于板块的选择。本文的目标是两方面。首先，通过引入更简单、更通用的条件——统一加倍条件，使加倍条件方法更易于理解。其次，利用这个新条件证明在训练过程中，无论是使用绝对值激活函数还是ReLU激活函数的DNN，其准确性都保持不变。我们的主要结果是，在统一加倍条件下，准确性在训练期间有很高的概率保持稳定，并提供了具体的概率估计。

    We study the stability of accuracy during the training of deep neural networks (DNNs). In this context, the training of a DNN is performed via the minimization of a cross-entropy loss function, and the performance metric is accuracy (the proportion of objects that are classified correctly). While training results in a decrease of loss, the accuracy does not necessarily increase during the process and may sometimes even decrease. The goal of achieving stability of accuracy is to ensure that if accuracy is high at some initial time, it remains high throughout training.  A recent result by Berlyand, Jabin, and Safsten introduces a doubling condition on the training data, which ensures the stability of accuracy during training for DNNs using the absolute value activation function. For training data in $\mathbb{R}^n$, this doubling condition is formulated using slabs in $\mathbb{R}^n$ and depends on the choice of the slabs. The goal of this paper is twofold. First, to make the doubling cond
    
[^107]: 使用演示和自然语言指令高效学习机器人任务

    Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks. (arXiv:2210.04476v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.04476](http://arxiv.org/abs/2210.04476)

    本论文提出了一种使用演示和自然语言指令相结合的机器人任务学习方法，其基于两种模式相互消除歧义来有效地指定和教授机器人复杂任务。此方法可以减少教师工作量并实现更好的泛化性能。

    

    演示和自然语言指令是指定和教授机器人新任务的两种常见方式。然而，对于许多复杂任务，单独使用演示或语言指令会存在歧义，导致任务无法清晰地被指定。在这种情况下，演示和指令的组合可以更简明地有效地向机器人传达任务。为了演示这个问题设置，我们在几百个挑战性的机器人拾取放置任务上训练了一个多任务策略，并提出了一种名为 DeL-TaCo（联合演示 - 语言任务调节）的方法，用于将机器人策略调节为由两个组成部分组成的任务嵌入：视觉演示和语言指令。通过允许这两种模式在新任务规范时相互消除歧义和澄清彼此，DeL-TaCo（1）大大降低了指定新任务所需的教师工作量，并且（2）实现了更好的泛化性能。

    Demonstrations and natural language instructions are two common ways to specify and teach robots novel tasks. However, for many complex tasks, a demonstration or language instruction alone contains ambiguities, preventing tasks from being specified clearly. In such cases, a combination of both a demonstration and an instruction more concisely and effectively conveys the task to the robot than either modality alone. To instantiate this problem setting, we train a single multi-task policy on a few hundred challenging robotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task Conditioning), a method for conditioning a robotic policy on task embeddings comprised of two components: a visual demonstration and a language instruction. By allowing these two modalities to mutually disambiguate and clarify each other during novel task specification, DeL-TaCo (1) substantially decreases the teacher effort needed to specify a new task and (2) achieves better generalization performa
    
[^108]: 在低质量晕的情况下，符号回归和强约束对SZ通量质量（$Y$-$M$）关系的改进：对重子反馈的全面研究

    The SZ flux-mass ($Y$-$M$) relation at low halo masses: improvements with symbolic regression and strong constraints on baryonic feedback. (arXiv:2209.02075v2 [astro-ph.CO] UPDATED)

    [http://arxiv.org/abs/2209.02075](http://arxiv.org/abs/2209.02075)

    本文通过使用液体动力学模拟和机器学习工具对低质量晕的SZ通量质量（$Y$-$M$）关系进行了全面研究，发现通过简单地将$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$可以使该关系显著自相似，这对于低质量星团和星系群是一种稳健的多波长质量代理。

    

    来自活动星系核和超新星的反馈会影响CMB调查中晕的集成SZ通量（$Y_\mathrm{SZ}$）的测量结果，并导致其与晕质量（$Y_\mathrm{SZ}-M$）的关系偏离维里定理的自相似幂律预测。我们使用具有广泛反馈方案变化的液体动力学模拟套房CAMELS对这种偏离进行了全面研究。我们使用两种机器学习工具（随机森林和符号回归）的组合来搜索$Y-M$关系的类比物，这些类比物对低质量（$M \lesssim 10 ^ {14} \，h ^ {-1} \，M_\odot$）的反馈过程更加稳健。我们发现，在关系中简单地将$Y \rightarrow Y(1 + M_*/M_\mathrm{gas})$可使其显著自相似，这可以用作低质量星团和星系群的稳健多波长质量代理。我们的方法论还可以普遍用于改进其他天体物理的有效范围。

    Feedback from active galactic nuclei (AGN) and supernovae can affect measurements of integrated SZ flux of halos ($Y_\mathrm{SZ}$) from CMB surveys, and cause its relation with the halo mass ($Y_\mathrm{SZ}-M$) to deviate from the self-similar power-law prediction of the virial theorem. We perform a comprehensive study of such deviations using CAMELS, a suite of hydrodynamic simulations with extensive variations in feedback prescriptions. We use a combination of two machine learning tools (random forest and symbolic regression) to search for analogues of the $Y-M$ relation which are more robust to feedback processes for low masses ($M\lesssim 10^{14}\, h^{-1} \, M_\odot$); we find that simply replacing $Y\rightarrow Y(1+M_*/M_\mathrm{gas})$ in the relation makes it remarkably self-similar. This could serve as a robust multiwavelength mass proxy for low-mass clusters and galaxy groups. Our methodology can also be generally useful to improve the domain of validity of other astrophysical 
    
[^109]: 基于语言学的建立生物可靠的蛋白质语言模型的路线图

    Linguistically inspired roadmap for building biologically reliable protein language models. (arXiv:2207.00982v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2207.00982](http://arxiv.org/abs/2207.00982)

    研究提供了一个基于语言学的蛋白质语言模型建立的路线图，以帮助建立更可解释的蛋白质LM，这将有助于发现相关领域特定的规则，从而促进基于规则的生物治疗药物的开发。

    

    基于深度神经网络的语言模型（LM）被越来越多地应用于大规模蛋白质序列数据以预测蛋白质功能。然而，由于它们大多是黑盒模型，因此难以解释，目前的蛋白质LM方法并没有对序列-功能映射的基本理解作出贡献，从而阻碍了基于规则的生物治疗药物的开发。我们认为，从语言学中得出的指导可以帮助建立更可解释的蛋白质LM，这些模型更有可能学习相关的领域特定规则。与自然语言数据不同，蛋白质序列数据需要比自然语言LM提供更多领域特定知识的整合。在这里，我们提供了一个基于语言学的蛋白质LM流程选择的路线图，涉及培训数据、标记化、标记嵌入、序列嵌入和模型解释。

    Deep neural-network-based language models (LMs) are increasingly applied to large-scale protein sequence data to predict protein function. However, being largely black-box models and thus challenging to interpret, current protein LM approaches do not contribute to a fundamental understanding of sequence-function mappings, hindering rule-based biotherapeutic drug development. We argue that guidance drawn from linguistics, a field specialized in analytical rule extraction from natural language data, can aid with building more interpretable protein LMs that are more likely to learn relevant domain-specific rules. Differences between protein sequence data and linguistic sequence data require the integration of more domain-specific knowledge in protein LMs compared to natural language LMs. Here, we provide a linguistics-based roadmap for protein LM pipeline choices with regard to training data, tokenization, token embedding, sequence embedding, and model interpretation. Incorporating lingui
    
[^110]: 学习分布式电网拓扑结构：一份教程

    Learning Distribution Grid Topologies: A Tutorial. (arXiv:2206.10837v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2206.10837](http://arxiv.org/abs/2206.10837)

    本教程总结了针对电力分配网络最新的拓扑识别和检测方案，重点介绍了克服分布式电网中测量设备有限的方法，利用前馈的功率流物理方程和结构特性来增强拓扑估计。

    

    揭示数据中的馈线拓扑结构对于推进智能资源在电力分配网络中的运用和情境感知至关重要。本教程总结、对比和建立有用的链接，介绍了针对电力分配网络提出的拓扑识别和检测方案的最新研究成果。重点是强调克服分布式电网中测量设备有限的方法，利用前馈的功率流物理方程和结构特性来增强拓扑估计。可以通过传统的被动方式收集相量测量单元或智能电表的电网数据，也可以在激活电网资源并测量馈线电压响应时主动收集。在不同的计量设备放置情况下，回顾了馈线识别和检测的分析性质。这样的拓扑结构学习方法可以完全或近似地达到预期效果。

    Unveiling feeder topologies from data is of paramount importance to advance situational awareness and proper utilization of smart resources in power distribution grids. This tutorial summarizes, contrasts, and establishes useful links between recent works on topology identification and detection schemes that have been proposed for power distribution grids. The primary focus is to highlight methods that overcome the limited availability of measurement devices in distribution grids, while enhancing topology estimates using conservation laws of power-flow physics and structural properties of feeders. Grid data from phasor measurement units or smart meters can be collected either passively in the traditional way, or actively, upon actuating grid resources and measuring the feeder's voltage response. Analytical claims on feeder identifiability and detectability are reviewed under disparate meter placement scenarios. Such topology learning claims can be attained exactly or approximately so v
    
[^111]: LogGENE: 一种用于深度医疗推理任务的平滑检查损失替代方法

    LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks. (arXiv:2206.09333v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09333](http://arxiv.org/abs/2206.09333)

    LogGENE采用分位数回归框架预测基因表达水平的完整条件分位数，从而为高通量基因组学提供了一种能提供解释和报告不确定性估计、鲁棒性强的推断方法。

    

    在可靠的深度学习中，挖掘大型数据集并从中获得校准的预测具有即时相关性和实用性。本研究开发了基于深度神经网络的推断方法，适用于基因表达等大型数据集。然而，与典型的深度学习方法不同的是，我们的推断技术在准确性方面实现了最先进的性能，同时还能提供解释和报告不确定性估计。我们采用分位数回归框架来预测给定一组基因表达的完整条件分位数。条件分位数除了有助于提供预测的丰富解释外，还能够抵抗测量噪声。我们的技术在高通量基因组学中特别重要，这是一个正在引领个性化医疗、靶向药物设计和传递的新时代。然而，用于驱动估计过程的检查损失，在分位数回归中并无不同之处。

    Mining large datasets and obtaining calibrated predictions from tem is of immediate relevance and utility in reliable deep learning. In our work, we develop methods for Deep neural networks based inferences in such datasets like the Gene Expression. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of housekeeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. Our technique is particularly consequential in High-throughput Genomics, an area which is ushering a new era in personalized health care, and targeted drug design and delivery. However, check loss, used in quantile regression to drive the estimation process is not diffe
    
[^112]: 从约束专家演示中学习软约束

    Learning Soft Constraints From Constrained Expert Demonstrations. (arXiv:2206.01311v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01311](http://arxiv.org/abs/2206.01311)

    本文提出了一种反向强化学习的方法，能够从专家数据中学习软约束，恢复每个周期内代理平均满足的累积约束。方法通过调整约束函数实现。

    

    反向强化学习方法假设专家数据是由优化某些奖励函数的代理生成的。然而，在许多情况下，代理可能会优化受某些限制的奖励函数，其中这些限制引导代理行为的表达可能更为困难。我们考虑奖励函数已知，但约束未知的情况，并提出了一种能够令代理数据满足约束的方法。以IRL的方式，我们的方法通过迭代调整约束函数，直到代理行为与专家行为匹配来解决这个问题。我们的方法能够恢复每个周期内代理平均满足的累积软约束，而先前的工作集中于恢复硬约束。我们在合成环境、机器人环境和实际的高速公路驾驶环境中展示了我们的方法。

    Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. We demonstrate our approach on synthetic environments, robotics environments and real world highway driving 
    
[^113]: 在线深度强化学习上的高效奖励污染攻击

    Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning. (arXiv:2205.14842v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14842](http://arxiv.org/abs/2205.14842)

    本文通过设计一种通用的奖励污染框架展示了现有深度强化学习算法内在的漏洞，提出了两种新攻击方式，成功污染多种最先进DRL算法的智能体在多个环境中学习的效率，给深度强化学习提供了新的安全风险。

    

    本文研究了在线深度强化学习中的奖励污染攻击，攻击者对智能体使用的学习算法和环境动态一无所知。我们通过设计一种称为对抗MDP攻击的通用黑盒奖励污染框架，展示了现有深度强化学习算法内在的漏洞。我们将该框架实例化为两种新攻击，只破坏了总训练时间步数的少量奖励，并使代理学习低效策略。我们对攻击效率进行了理论分析，并进行了广泛的实证评估。结果表明我们的攻击能有效地污染使用多种最先进DRL算法（如DQN、PPO、SAC等）来学习的智能体，在多个受欢迎的经典控制和MuJoCo环境中具有较高效率。

    We study reward poisoning attacks on online deep reinforcement learning (DRL), where the attacker is oblivious to the learning algorithm used by the agent and the dynamics of the environment. We demonstrate the intrinsic vulnerability of state-of-the-art DRL algorithms by designing a general, black-box reward poisoning framework called adversarial MDP attacks. We instantiate our framework to construct two new attacks which only corrupt the rewards for a small fraction of the total training timesteps and make the agent learn a low-performing policy. We provide a theoretical analysis of the efficiency of our attack and perform an extensive empirical evaluation. Our results show that our attacks efficiently poison agents learning in several popular classical control and MuJoCo environments with a variety of state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc.
    
[^114]: 用Transformer进行偏微分方程算子学习

    Transformer for Partial Differential Equations' Operator Learning. (arXiv:2205.13671v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.13671](http://arxiv.org/abs/2205.13671)

    本文提出一种基于注意力机制的数据驱动算子学习框架OFormer，它可以广泛适用于不同的偏微分方程，并且具有与传统方法相当的准确性，甚至在某些情况下具有更好的表现。

    

    数据驱动的偏微分方程解算子学习近年来已成为一种有前途的范式，用于近似基础解。解算子通常由基于问题特定归纳偏见的深度学习模型参数化。例如，卷积或图神经网络利用了函数值被采样的本地网格结构。另一方面，注意力机制提供了一种灵活的方式来隐式利用输入中的模式，以及任意查询位置和输入之间的关系。在本研究中，我们提出了一种基于注意力的数据驱动算子学习框架，称为Operator Transformer (OFormer)。我们的框架建立在自注意、交叉注意和一组逐点多层感知机(MLP)之上，因此在输入函数或查询位置的采样模式上做出了很少的假设。我们展示了所提出的框架能够广泛适用于不同的偏微分方程，并且具有与先前基于卷积和图神经网络的方法相当的准确性，并且在某些情况下具有更好的表现。

    Data-driven learning of partial differential equations' solution operators has recently emerged as a promising paradigm for approximating the underlying solutions. The solution operators are usually parameterized by deep learning models that are built upon problem-specific inductive biases. An example is a convolutional or a graph neural network that exploits the local grid structure where functions' values are sampled. The attention mechanism, on the other hand, provides a flexible way to implicitly exploit the patterns within inputs, and furthermore, relationship between arbitrary query locations and inputs. In this work, we present an attention-based framework for data-driven operator learning, which we term Operator Transformer (OFormer). Our framework is built upon self-attention, cross-attention, and a set of point-wise multilayer perceptrons (MLPs), and thus it makes few assumptions on the sampling pattern of the input function or query locations. We show that the proposed frame
    
[^115]: 选择联邦学习架构模式的决策模型。

    Decision Models for Selecting Federated Learning Architecture Patterns. (arXiv:2204.13291v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.13291](http://arxiv.org/abs/2204.13291)

    本文提出了一套选择联邦机器学习架构设计模式的决策模型，映射功能需求和非功能需求到一组模式上，以协助对联邦机器学习知识有限的设计师和架构师。

    

    联邦机器学习作为解决机器学习中的数据饥饿和隐私问题的解决方案，在学术界和工业界迅速发展。作为一个广泛分布的系统，联邦机器学习需要各种系统设计思考。为了更好地设计联邦机器学习系统，研究人员介绍了多种模式和策略，涵盖了各种系统设计方面。然而，模式的多样性使设计师对何时采用哪种模式感到困惑。本文在对联邦机器学习进行系统文献综述的基础上，提出了一套选择联邦机器学习架构设计模式的决策模型，以协助对联邦机器学习知识有限的设计师和架构师。每个决策模型将联邦机器学习系统的功能需求和非功能需求映射到一组模式上。我们还澄清了这些模式的缺点。我们评估了这些模式的性能和可扩展性，以及它们在现实世界中的适用性。

    Federated machine learning is growing fast in academia and industries as a solution to solve data hungriness and privacy issues in machine learning. Being a widely distributed system, federated machine learning requires various system design thinking. To better design a federated machine learning system, researchers have introduced multiple patterns and tactics that cover various system design aspects. However, the multitude of patterns leaves the designers confused about when and which pattern to adopt. In this paper, we present a set of decision models for the selection of patterns for federated machine learning architecture design based on a systematic literature review on federated machine learning, to assist designers and architects who have limited knowledge of federated machine learning. Each decision model maps functional and non-functional requirements of federated machine learning systems to a set of patterns. We also clarify the drawbacks of the patterns. We evaluated the de
    
[^116]: TC-GNN：在GPU上连接稀疏GNN计算与密集Tensor Cores的桥梁

    TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs. (arXiv:2112.02052v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.02052](http://arxiv.org/abs/2112.02052)

    本文提出了TC-GNN，这是第一个基于GPU张量核心单元（TCUs）的GNN加速框架。采用稀疏图翻译技术来协调“稀疏”GNN计算与高性能的“密集”TCUs，实现了GNN计算效率的提升。

    

    最近，作为基于图的机器学习的骨干，图神经网络（GNN）展示了在各个领域（如电子商务）的巨大成功。然而，由于高度稀疏和不规则的基于图的操作，GNN的性能通常不尽如人意。为此，我们提出了TC-GNN，基于GPU张量核心单元（TCUs）的第一个GNN加速框架。其核心思想是通过将“稀疏”GNN计算与高性能的“密集”TCUs协调一致，实现GNN计算效率的提升。具体来说，我们对主流GNN计算框架中的稀疏操作进行了深入分析。我们引入了一种新颖的稀疏图翻译技术，以便TCU处理稀疏的GNN工作负载。我们实现了一种有效的CUDA核心和TCU协作设计，充分利用GPU资源。我们将TC-GNN与PyTorch框架集成，以实现高可编程性。严格的实验表明，在各种模型和数据集上，相比于最先进的DGL框架，平均加速了1.70倍。

    Recently, graph neural networks (GNNs), as the backbone of graph-based machine learning, demonstrate great success in various domains (e.g., e-commerce). However, the performance of GNNs is usually unsatisfactory due to the highly sparse and irregular graph-based operations. To this end, we propose TC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units (TCUs). The core idea is to reconcile the "Sparse" GNN computation with the high-performance "Dense" TCUs. Specifically, we conduct an in-depth analysis of the sparse operations in mainstream GNN computing frameworks. We introduce a novel sparse graph translation technique to facilitate TCU processing of the sparse GNN workload. We implement an effective CUDA core and TCU collaboration design to fully utilize GPU resources. We integrate TC-GNN with the PyTorch framework for high programmability. Rigorous experiments show an average of 1.70X speedup over the state-of-the-art DGL framework across various models and dat
    
[^117]: 一种用于基于体素的三维物体分类的快速混合级联网络

    A Fast Hybrid Cascade Network for Voxel-based 3D Object Classification. (arXiv:2011.04522v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2011.04522](http://arxiv.org/abs/2011.04522)

    本文提出了一种用于基于体素的三维物体分类的快速混合级联网络，包含三个阶段分别处理易、中和难的三维模型，通过给每个体素赋予有符号的距离值来提高精度，并大大减少了平均推理时间。

    

    近年来，基于体素的三维物体分类已经得到了深入研究。大多数先前的方法将经典的二维卷积转换为三维形式，然后应用于具有二进制体素表示的物体进行分类。然而，在许多情况下，二进制体素表示对于三维卷积来说并不是很有效。在本文中，我们提出了一种用于基于体素的三维物体分类的混合级联体系结构。它由三个阶段组成，包括全连接和卷积层，分别处理易、中和难的三维模型。我们的方法在精度和速度方面都能够达到平衡。通过给每个体素赋予一个有符号的距离值，可以观察到明显的精度提升。此外，与最先进的点云和基于体素的方法相比，平均推理时间可以大大加快。

    Voxel-based 3D object classification has been thoroughly studied in recent years. Most previous methods convert the classic 2D convolution into a 3D form that will be further applied to objects with binary voxel representation for classification. However, the binary voxel representation is not very effective for 3D convolution in many cases. In this paper, we propose a hybrid cascade architecture for voxel-based 3D object classification. It consists of three stages composed of fully connected and convolutional layers, dealing with easy, moderate, and hard 3D models respectively. Both accuracy and speed can be balanced in our proposed method. By giving each voxel a signed distance value, an obvious gain regarding the accuracy can be observed. Besides, the mean inference time can be speeded up hugely compared with the state-of-the-art point cloud and voxel based methods.
    
[^118]: 基于多尺度CNN和仿生决策融合模型的多模态情感状态识别

    Multimodal Affective States Recognition Based on Multiscale CNNs and Biologically Inspired Decision Fusion Model. (arXiv:1911.12918v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/1911.12918](http://arxiv.org/abs/1911.12918)

    本文提出了一种基于多尺度 CNN 和仿生决策融合模型的多模态情感状态识别方法，该方法对 EEG 和各个外周生理信号分别进行预测，并通过信号的可靠性进行决策。

    

    近年来，基于单模态信号（如脑电信号或外周生理信号）的情绪状态识别模型已取得了令人鼓舞的进展。然而，基于多模态生理信号的情感状态识别方法尚未被充分开发。在本文中，我们提出了一种基于多尺度卷积神经网络（Multiscale CNNs）和仿生决策融合模型的多模态情感状态识别方法。首先，原始信号被与基线信号进行预处理。然后，Multiscale CNNs 中的高尺度CNN和低尺度CNN分别用于 EEG 和每个外周生理信号的情感状态输出概率预测。最后，融合模型通过各种类标签之间的欧氏距离和来自 Multiscale CNNs 的分类概率计算单模态信号的可靠性，并根据其中更可靠的信号进行决策。

    There has been an encouraging progress in the affective states recognition models based on the single-modality signals as electroencephalogram (EEG) signals or peripheral physiological signals in recent years. However, multimodal physiological signals-based affective states recognition methods have not been thoroughly exploited yet. Here we propose Multiscale Convolutional Neural Networks (Multiscale CNNs) and a biologically inspired decision fusion model for multimodal affective states recognition. Firstly, the raw signals are pre-processed with baseline signals. Then, the High Scale CNN and Low Scale CNN in Multiscale CNNs are utilized to predict the probability of affective states output for EEG and each peripheral physiological signal respectively. Finally, the fusion model calculates the reliability of each single-modality signals by the Euclidean distance between various class labels and the classification probability from Multiscale CNNs, and the decision is made by the more rel
    
[^119]: 大规模神经网络学习为什么行为类似于凸优化

    Why Learning of Large-Scale Neural Networks Behaves Like Convex Optimization. (arXiv:1903.02140v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1903.02140](http://arxiv.org/abs/1903.02140)

    本文介绍了利用规范空间证明学习大规模神经网络的目标函数在收敛到全局最小值时是凸优化问题。使用点线性转换的方法建立原始NN模型空间和规范空间之间的关系，证明了使用梯度下降方法，只要差异矩阵保持完整秩，就一定能收敛到零损失的全局最小值。大规模NN具有奇异的差异矩阵的概率非常小。

    

    本文提出了一些理论工作，以解释为什么简单的梯度下降方法在解决学习大规模神经网络的非凸优化问题中如此成功。在介绍了一种称为规范空间的数学工具之后，我们证明了规范模型空间中的学习NN目标函数是凸的。我们进一步阐明了原始NN模型空间和规范空间之间的梯度之间的关系是通过所谓的差异矩阵表示的逐点线性变换相关的。此外，我们已经证明，如果差异矩阵保持完整秩，梯度下降方法一定会收敛到零损失的全局最小值。如果这个完整秩条件成立，在NN的学习中的行为与正常的凸优化相同。最后，我们证明，大规模NN具有奇异的差异矩阵的概率非常小。特别是，当超参数化的NN是...

    In this paper, we present some theoretical work to explain why simple gradient descent methods are so successful in solving non-convex optimization problems in learning large-scale neural networks (NN). After introducing a mathematical tool called canonical space, we have proved that the objective functions in learning NNs are convex in the canonical model space. We further elucidate that the gradients between the original NN model space and the canonical space are related by a pointwise linear transformation, which is represented by the so-called disparity matrix. Furthermore, we have proved that gradient descent methods surely converge to a global minimum of zero loss provided that the disparity matrices maintain full rank. If this full-rank condition holds, the learning of NNs behaves in the same way as normal convex optimization. At last, we have shown that the chance to have singular disparity matrices is extremely slim in large NNs. In particular, when over-parameterized NNs are 
    

