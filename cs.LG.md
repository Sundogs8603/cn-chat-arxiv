# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following.](http://arxiv.org/abs/2309.00615) | Point-Bind和Point-LLM是用于3D理解、生成和指导跟随的多模态点云对齐模型，能实现任意到3D生成、3D嵌入算术和3D开放世界的理解，并且Point-LLM能实现3D和多模态问答功能。 |
| [^2] | [Baseline Defenses for Adversarial Attacks Against Aligned Language Models.](http://arxiv.org/abs/2309.00614) | 这篇论文研究了对齐语言模型面临的对抗攻击问题，通过评估基线防御策略的效果，探讨了各种策略的可行性和有效性，并对鲁棒性和性能进行了讨论。 |
| [^3] | [Iterative Multi-granular Image Editing using Diffusion Models.](http://arxiv.org/abs/2309.00613) | 本研究提出了一种名为EMILIE的迭代多粒度图像编辑器，通过重新利用预训练的扩散模型来实现迭代编辑，并引入梯度控制操作来实现对图像编辑的粒度控制。 |
| [^4] | [Bayesian deep learning for cosmic volumes with modified gravity.](http://arxiv.org/abs/2309.00612) | 该研究利用贝叶斯深度学习的方法，从修正引力模拟中提取宇宙学参数，并对不确定性进行了评估。 |
| [^5] | [Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair.](http://arxiv.org/abs/2309.00608) | 这篇论文提出了一种框架，利用完成引擎来进一步支持大型语言模型在自动化程序修复中合成更多有效的修补程序。 |
| [^6] | [Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms.](http://arxiv.org/abs/2309.00591) | 本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。 |
| [^7] | [PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer.](http://arxiv.org/abs/2309.00585) | PolyGET是一个新的聚合物力场框架，使用等变换器模型来捕捉复杂的量子相互作用，并在不同聚合物家族上进行泛化。它通过优化力场模型来提高模拟的准确性和效率。 |
| [^8] | [Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion.](http://arxiv.org/abs/2309.00584) | Laminar是一个新的无服务器流式框架，通过语义代码搜索和代码自动完成增强了框架的功能，简化了流式计算的执行，更高效地管理数据流，并为研究人员和实践者提供了有价值的工具。 |
| [^9] | [Geometry-Informed Neural Operator for Large-Scale 3D PDEs.](http://arxiv.org/abs/2309.00583) | 我们提出了一种基于几何信息的神经算子（GINO），用于学习大规模偏微分方程的解算符。GINO使用符号距离函数和点云表示来处理不同几何形状，并通过图和傅里叶结构的神经算子实现离散一致性。在实验中，我们验证了GINO在大规模模拟中的性能。 |
| [^10] | [Consistency of Lloyd's Algorithm Under Perturbations.](http://arxiv.org/abs/2309.00578) | 该论文研究了Lloyd算法在扰动样本上的一致性，证明了在适当初始化和扰动相对于亚高斯噪声较小的假设下，算法在O(log(n))次迭代后的错聚类率在指数下界受限。 |
| [^11] | [Mechanism of feature learning in convolutional neural networks.](http://arxiv.org/abs/2309.00570) | 本论文研究了卷积神经网络从图像数据中学习特征的机制，并提出了卷积神经特征假设。通过实证分析和理论证明，论文展示了滤波器的协方差与输入补丁的平均梯度外积之间的高度相关性，以及通过基于补丁的平均梯度外积实现深度特征学习的普遍性。 |
| [^12] | [Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer's Disease.](http://arxiv.org/abs/2309.00569) | 通过使用图像翻译模型，本研究证明了从结构 MRI 中合成具有准确定量能力的 amyloid-beta PET 图像的可行性，为仅依靠 MRI 获得 amyloid-beta 信息提供了一种新途径。 |
| [^13] | [Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data.](http://arxiv.org/abs/2309.00564) | 本文研究了高维线性回归在解释方面的挑战，发现空间零值和正则化对回归系数产生重要影响，并提出了一种优化公式来比较回归系数与物理工程知识得到的系数，从而实现解释性的回归结果。 |
| [^14] | [Interactive and Concentrated Differential Privacy for Bandits.](http://arxiv.org/abs/2309.00557) | 本文研究了在交互学习和推荐系统中隐私保护的Bandit问题，并引入了集中差分隐私的概念。通过提供关于有限臂和线性Bandit问题遗憾的下界，我们揭示了不同隐私预算下的难度区域，并发现集中差分隐私可以比全局差分隐私更有效地保护隐私，我们提出了两种相应的算法。 |
| [^15] | [Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare.](http://arxiv.org/abs/2309.00543) | 提出了一种方法来筛选自然对立示例的数据集，以评估模型的鲁棒性，并通过自动弱监督标注获得的概率标签来实现这一方法。 |
| [^16] | [Adaptive function approximation based on the Discrete Cosine Transform (DCT).](http://arxiv.org/abs/2309.00530) | 本文研究了基于离散余弦变换的自适应函数逼近方法，提出了一种简单而高效的监督学习技术，可以用于近似无记忆单变量连续函数，在学习质量与复杂性方面表现出色。 |
| [^17] | [Online Distributed Learning over Random Networks.](http://arxiv.org/abs/2309.00520) | 本文提出了在随机网络上进行在线分布式学习的DOT-ADMM算法，通过解决在线学习、异步计算、不可靠通信和不精确计算等挑战，在一大类凸学习问题上获得了线性收敛速度。 |
| [^18] | [Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks.](http://arxiv.org/abs/2309.00508) | 本论文通过分析两层神经网络在全局最小值附近的结构和梯度动力学，揭示了其泛化能力较强的原因。 |
| [^19] | [Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems.](http://arxiv.org/abs/2309.00498) | 该论文应用深度学习方法来提高电力系统监测与优化算法，具体包括使用图神经网络增强系统状态估计，以及利用强化学习进行动态配电网重构。 |
| [^20] | [Multi-stage Deep Learning Artifact Reduction for Computed Tomography.](http://arxiv.org/abs/2309.00494) | 本论文提出了一种多阶段深度学习伪影减少方法，用于提高计算机断层扫描的图像质量。传统方法通常在重建之后进行处理，而本方法能够根据不同的图像域进行多步骤去伪影，使得相对困难去除的伪影也能够有效消除。 |
| [^21] | [How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?.](http://arxiv.org/abs/2309.00489) | 本文研究了时间序列预测对在O-RAN切片中使用的DRL技术的收敛性的影响。 |
| [^22] | [Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction.](http://arxiv.org/abs/2309.00483) | 本研究提出了一种几何感知的线图转换器（Galformer）预训练方法，用于增强分子表示学习。该方法结合2D和3D模态编码分子的拓扑和几何信息，并设计了互补的预训练任务。 |
| [^23] | [New metrics for analyzing continual learners.](http://arxiv.org/abs/2309.00462) | 该论文提出了分析持续学习者的新指标，针对现有指标的局限性进行了分析，并提出解决了分类任务逐渐增加难度的问题。 |
| [^24] | [A Locality-based Neural Solver for Optical Motion Capture.](http://arxiv.org/abs/2309.00428) | 该论文提出了一种基于局部性的神经求解器，用于清理和求解光学运动捕捉数据。论文通过构建异构图神经网络和使用特定的图卷积操作，提取标记和关节的局部特征，并转化为更准确的动作。通过研究标记的动作与其相邻标记的相关性，论文能够高效地填补缺失的标记，并通过分析加速度轮廓识别跟踪错误引起的异常标记。此外，论文还提出了基于表示学习和数据增强的训练机制，以进一步提高模型的准确性。 |
| [^25] | [Declarative Reasoning on Explanations Using Constraint Logic Programming.](http://arxiv.org/abs/2309.00422) | 这项研究提出了使用约束逻辑编程进行解释性推理的方法，可以为决策树提供声明性、交互式的解释，克服了当前解释方法中对背景知识的不充分结合和与用户的缺乏抽象和互动的问题。 |
| [^26] | [Area-norm COBRA on Conditional Survival Prediction.](http://arxiv.org/abs/2309.00417) | 本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。 |
| [^27] | [Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond.](http://arxiv.org/abs/2309.00416) | 本研究在个性化、隐私保证和公平性之间解决了联邦学习模型的三元交互作用。差分隐私及其变体被应用为提供正式隐私保证的前沿标准。在多样化的数据集中寻求公平性变得重要。 |
| [^28] | [Selective Scene Text Removal.](http://arxiv.org/abs/2309.00410) | 本论文提出了一种选择性场景文本去除（SSTR）任务，该任务可以根据用户指定的目标词汇来去除场景图像中的文本，实验结果表明所提出的方法有效地实现了目标词汇的去除。 |
| [^29] | [Boosting AND/OR-Based Computational Protein Design: Dynamic Heuristics and Generalizable UFO.](http://arxiv.org/abs/2309.00408) | 该研究专注于提升蛋白质重设计算法AOBB-K*的扩展性，通过引入增强版、带有动态启发式和带有下溢优化的新版本，显著提升了其可扩展性。 |
| [^30] | [Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds.](http://arxiv.org/abs/2309.00380) | 本文提出了一种用于多模态数据的深度潜变量模型，并开发了更灵活的编码特征聚合方案，能够紧密地下界数据对数似然。 |
| [^31] | [Anomaly detection with semi-supervised classification based on risk estimators.](http://arxiv.org/abs/2309.00379) | 基于风险估计的半监督分类异常检测方法克服了单类分类异常检测中对未标记训练数据只包含正常实例的假设，并提出了两种新型检测方法，实验证明了其有效性。 |
| [^32] | [Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark.](http://arxiv.org/abs/2309.00367) | 本文对长程图表基准（LRGB）进行了重新评估，通过严格的实证分析发现，先前的报告性能差距被高估了，而经过基本超参数优化后，差距完全消失。此外，我们还讨论了特征归一化的缺失和链接预测度量的虚假实现对LRGB的影响。 |
| [^33] | [Explainable Active Learning for Preference Elicitation.](http://arxiv.org/abs/2309.00356) | 本研究关注于冷启动问题中的偏好获取，在该问题中，推荐系统缺乏用户存在或访问其他用户数据受限。我们采用可解释的主动学习方法，通过最小化用户工作量最大化信息获取，并在偏好获取过程中采用无监督、半监督和监督机器学习方法。 |
| [^34] | [Bespoke Nanoparticle Synthesis and Chemical Knowledge Discovery Via Autonomous Experimentations.](http://arxiv.org/abs/2309.00349) | 该论文报道了一种自动实验平台，用于定制设计具有目标光学性质的纳米颗粒。通过贝叶斯优化器，能够在仅200次迭代内高效地合成具有所需吸收光谱的银纳米颗粒，并揭示了涉及柠檬酸盐效应的新化学反应。 |
| [^35] | [Multitask Deep Learning for Accurate Risk Stratification and Prediction of Next Steps for Coronary CT Angiography Patients.](http://arxiv.org/abs/2309.00330) | 本研究提出了一种多任务深度学习模型，可以支持冠状动脉CT血管造影患者的风险分层和下一步测试选择。在真实世界的数据集上，该模型在风险分层和预测下游测试方面取得了较高的准确性。 |
| [^36] | [Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI's Whisper.](http://arxiv.org/abs/2309.00329) | Mi-Go is a testing framework that uses YouTube as a data source to evaluate speech recognition models like OpenAI's Whisper. It leverages diverse real-world scenarios, multiple languages and accents, and compares machine-generated transcriptions with human-made subtitles to identify potential misuse of YouTube subtitles. |
| [^37] | [Multi-fidelity reduced-order surrogate modeling.](http://arxiv.org/abs/2309.00325) | 本论文提出了一种结合降维和多保真度神经网络代理的数据驱动策略，以解决在高保真度数据有限或稀缺的情况下，低保真度模型无法准确捕捉到高保真度模型中观察到的不稳定性和临界瞬变的问题。 |
| [^38] | [Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties.](http://arxiv.org/abs/2309.00305) | 本研究通过应用机器学习技术，开发并研究了两个不同领域的数据集，用于高效预测材料科学中的微结构属性。 |
| [^39] | [End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing.](http://arxiv.org/abs/2309.00296) | 本研究提出了一种基于激光雷达的端到端强化学习方法，用于解决在汽车赛车领域中复杂环境下的自主导航问题，并通过实验验证了该方法的可行性和潜力。 |
| [^40] | [RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.](http://arxiv.org/abs/2309.00267) | RLAIF是一种新的强化学习方法，利用AI反馈代替人类标注偏好，相比强化学习从人类反馈中学习（RLHF），在摘要任务上取得了类似的改进效果，并且在人类评估中得到了相同的认可。这提供了一种有潜力解决RLHF的可扩展性限制的解决方案。 |
| [^41] | [Leveraging Learning Metrics for Improved Federated Learning.](http://arxiv.org/abs/2309.00257) | 本论文将联邦学习与学习度量方法相结合，提出了首个联邦学习度量聚合方法，并证明了有效秩适用于联邦问题，并通过开发新型权重聚合方案取得了优于基准的联邦平均效果。 |
| [^42] | [SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks.](http://arxiv.org/abs/2309.00255) | SortedNet是一种广义解决方案，通过排序训练和概率方式，在深度神经网络的各个维度上实现高效动态推断。这种方法允许在模型推断过程中灵活适应计算负载，并且可以将子网络的数量扩展到数百个。 |
| [^43] | [Why do universal adversarial attacks work on large language models?: Geometry might be the answer.](http://arxiv.org/abs/2309.00254) | 本研究从几何视角解释了对大型语言模型的通用对抗性攻击，发现通用对抗性触发器可能是嵌入向量，仅仅近似了其对抗训练区域中的语义信息。 |
| [^44] | [Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care.](http://arxiv.org/abs/2309.00252) | 本文综述了最近ViT的进展以及对其决策过程的解释性方法，为医学诊断应用提供透明度。 |
| [^45] | [NeuroSurgeon: A Toolkit for Subnetwork Analysis.](http://arxiv.org/abs/2309.00244) | NeuroSurgeon是一个用于在Huggingface Transformers库中发现和操作模型子网络的Python工具包，可以推进对神经网络学习算法的理解。 |
| [^46] | [Image Hijacking: Adversarial Images can Control Generative Models at Runtime.](http://arxiv.org/abs/2309.00236) | 本研究发现对抗性图像能够在运行时控制生成模型，并提出了通用的方法来创建图像劫持。通过研究三种攻击类型，我们发现这些攻击对最新的视觉语言模型具有高达90％以上的成功率。该研究引发了对基础模型安全性的严重担忧。 |
| [^47] | [Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods.](http://arxiv.org/abs/2309.00203) | 本文研究了一种简单的数据驱动方法，通过学习投影矩阵来降低高维线性规划问题的维数，实现更快的求解速度。基于“数据驱动算法设计”，提出了泛化保证的数据量与性能指标的伪维度的上界和下界。 |
| [^48] | [Subjectivity in Unsupervised Machine Learning Model Selection.](http://arxiv.org/abs/2309.00201) | 无监督机器学习模型选择具有主观性，模型选择结果受模型构建者偏好的影响，并可能导致选择的不一致性。需要对模型选择过程进行更加深入的研究和标准化。 |
| [^49] | [Diffusion Model with Clustering-based Conditioning for Food Image Generation.](http://arxiv.org/abs/2309.00199) |  |
| [^50] | [Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches.](http://arxiv.org/abs/2309.00197) | 本论文提出基于深度学习模型的早期修复方法，通过训练模型为整数变量提供值并早期修复这些变量，从而将原问题简化成了线性规划问题。实验结果表明，有监督方法和弱监督方法均能有效地解决气举油生产优化问题。 |
| [^51] | [RepCodec: A Speech Representation Codec for Speech Tokenization.](http://arxiv.org/abs/2309.00169) | RepCodec是一种新型的语音表示编码器，通过重构语音表示并学习矢量量化码书，将语音波形转换为语义标记。实验证明，RepCodec在语音理解和生成方面明显优于传统的k-means聚类方法。 |
| [^52] | [Information Fusion for Assistance Systems in Production Assessment.](http://arxiv.org/abs/2309.00157) | 本文提出了一种将不同信息源进行融合的方法，使用证据理论构建了一个通用框架，并在生产评估辅助系统中进行了实证验证，同时解决了数据漂移问题。 |
| [^53] | [TurboGP: A flexible and advanced python based GP library.](http://arxiv.org/abs/2309.00149) | TurboGP是一个基于Python的灵活和先进的GP库，具有岛和细胞人群方案、不同类型的遗传操作和对不同类型的GP节点的本地支持，适用于处理各种数据源。 |
| [^54] | [Multi Agent DeepRL based Joint Power and Subchannel Allocation in IAB networks.](http://arxiv.org/abs/2309.00144) | 这项研究提出了一种基于多Agent DeepRL算法的框架，用于在IAB网络中联合优化功率和子通道分配，以最大化下行数据速率。 |
| [^55] | [Improving vision-inspired keyword spotting using dynamic module skipping in streaming conformer encoder.](http://arxiv.org/abs/2309.00140) | 本论文提出了一种使用动态模块跳过的流conformer编码器的视觉启发关键词检测方法，通过在输入音频上动态跳过网络模块，提高了检测和定位准确性，同时保持了小的内存占用。该方法在含背景噪声的Google语音指令数据集上表现更加突出，对于始终开启的关键词检测器特别有意义。 |
| [^56] | [Predicting Financial Market Trends using Time Series Analysis and Natural Language Processing.](http://arxiv.org/abs/2309.00136) | 本研究通过分析推文中表达的情绪，使用时间序列分析和自然语言处理预测了特斯拉、苹果等主要公司股票价格的波动。结果表明，积极性、消极性和主观性是股票价格波动的主要决定因素。 |
| [^57] | [FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning.](http://arxiv.org/abs/2309.00127) | FTA提出了一种新的灵活触发器、隐蔽且强健的联邦学习后门攻击方法，通过生成学习灵活触发器模式来操作良性样本，同时保留攻击者选择的标签的最重要隐藏特征。通过填充可区分的差异，使攻击具有隐蔽性。验证了该方法的有效性和可靠性。 |
| [^58] | [Differentially Private Functional Summaries via the Independent Component Laplace Process.](http://arxiv.org/abs/2309.00125) | 本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。 |
| [^59] | [Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures Market.](http://arxiv.org/abs/2309.00088) | 本研究评估了一种名为Deep SAD的深度半监督异常检测技术在高频金融数据中检测诈骗的效果。使用了来自TMX交易所的独家限价委托薄数据，并利用一小部分的标记数据。 |
| [^60] | [RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability.](http://arxiv.org/abs/2309.00082) | 本文提出了RePo算法，通过正则化后验可预测性的方式，增强了视觉模型基础强化学习方法的弹性。该方法通过学习一个对冗余和伪变化具有弹性的潜在表示，提高了方法对视觉干扰的鲁棒性，使其能够在动态环境中运行。 |
| [^61] | [On the Implicit Bias of Adam.](http://arxiv.org/abs/2309.00079) | 本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。 |
| [^62] | [Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction.](http://arxiv.org/abs/2309.00073) | 本论文提出了一种利用扩散变分自编码器来解决多步回归股票价格预测中的随机性问题的方法。 |
| [^63] | [YaRN: Efficient Context Window Extension of Large Language Models.](http://arxiv.org/abs/2309.00071) | YaRN是一种高效的上下文窗口扩展方法，可以在大型语言模型中有效利用和推断比原始预训练允许的上下文长度更长的上下文，同时超越了之前的最新研究成果。 |
| [^64] | [Efficient Multi-View Graph Clustering with Local and Global Structure Preservation.](http://arxiv.org/abs/2309.00024) | 提出了一种新的基于锚点的多视图图聚类方法，通过统一的框架将优化的全局结构和局部结构相结合，同时考虑局部相似度和全局相似度，以提高聚类性能。 |
| [^65] | [Continual Learning From a Stream of APIs.](http://arxiv.org/abs/2309.00023) | 本文介绍了基于API流的持续学习方法，包括数据有效的持续学习和无数据的持续学习。通过查询API生成伪数据，将API流中的知识蒸馏到持续学习模型中，从而解决了无法获取完整的原始数据、未知的模型参数、异构模型和灾难性遗忘等挑战。 |
| [^66] | [An Energy-Aware Approach to Design Self-Adaptive AI-based Applications on the Edge.](http://arxiv.org/abs/2309.00022) | 本文提出了一种能源感知方法，用于设计和部署能够在应用目标与能源消耗之间平衡的自适应人工智能应用。 |
| [^67] | [Unsupervised discovery of Interpretable Visual Concepts.](http://arxiv.org/abs/2309.00018) | 本文提出了两种方法（MAGE和Ms-IV），用于解释深度学习模型的决策，提高全局可解释性。MAGE可以发现形成语义含义的特征组合，将其称为概念，并通过聚类分组为“概念”，然后通过Ms-IV进行可视化。这一方法受到阻断和敏感性分析的启发，并使用一种新的指标（CaOC）全局评估模型最重要的图像区域。 |
| [^68] | [Physics-Based Trajectory Design for Cellular-Connected UAV in Rainy Environments Based on Deep Reinforcement Learning.](http://arxiv.org/abs/2309.00017) | 本文提出了一种基于物理的轨迹设计方法，用于在多雨环境中对细胞连接的无人机进行轨迹优化。该方法利用物理模拟器考虑了详细的环境信息和雨水对信号传播的影响。 |
| [^69] | [Large-Scale Public Data Improves Differentially Private Image Generation Quality.](http://arxiv.org/abs/2309.00008) | 本研究通过使用大规模公共数据，提出了一种改进的方法来提高差分隐私图像生成的质量。评估结果表明，与现有方法相比，我们的方法在FID得分和其他指标上取得了最先进的成果，并能以差分隐私的方式生成高质量、逼真的图像。 |
| [^70] | [When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning.](http://arxiv.org/abs/2309.00007) | 本文提出了一种面向Top-k多标签学习的不可察觉的对抗性扰动方法，既能够欺骗人眼的视觉感知，又能够避开度量监测。 |
| [^71] | [High Spectral Spatial Resolution Synthetic HyperSpectral Dataset form multi-source fusion.](http://arxiv.org/abs/2309.00005) | 本研究提供了一种合成高光谱数据集，通过融合多源数据，实现了对观测场景或对象的综合、准确和详细的表示。 |
| [^72] | [GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields.](http://arxiv.org/abs/2308.16891) | GNFactor是一个用于多任务机器人操作的代理方法，它利用可泛化神经特征场和Perceiver Transformer模块，以及深度三维体素表示来实现对真实世界环境中的操作任务的执行。它通过将视觉和语义信息纳入三维表示来提高场景的理解能力，并在多个任务上进行了验证。 |
| [^73] | [FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout.](http://arxiv.org/abs/2308.16835) | 本文提出了一种通过差分参数丢弃实现通信高效的联邦学习方案（FedDD）。这种方案避免了频繁交换模型参数的通信延迟问题，并通过模型参数丢弃而不是客户端选择来优化全局模型泛化能力。 |
| [^74] | [Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network.](http://arxiv.org/abs/2308.16818) | 该论文提出了一种基于异步时空图卷积网络的不规则交通时间序列预测方法，用于解决智能交叉口产生的异步空间依赖、不规则时间依赖和可变长度序列预测等挑战。 |
| [^75] | [Robust Networked Federated Learning for Localization.](http://arxiv.org/abs/2308.16737) | 本文提出了一种鲁棒的网络化联邦学习方法，通过采用$L_1$-范数鲁棒性和分布式次梯度框架，解决了在分布式环境中定位问题中的异常数据干扰和算法收敛挑战。 |
| [^76] | [Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction.](http://arxiv.org/abs/2308.16259) | 本研究提出了一种名为材料信息学变压器（MatInFormer）的语言模型，通过学习晶体学语法和引入MOFs数据，实现了对材料性能的准确预测，并通过注意力可视化揭示了模型的关键特征。 |
| [^77] | [Calibrated Explanations for Regression.](http://arxiv.org/abs/2308.16245) | 本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。 |
| [^78] | [Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search.](http://arxiv.org/abs/2308.15734) | 该论文提出了一种高效且可解释的图神经网络架构搜索方法，名为ExGNAS。它包括适应各种图形的简单搜索空间和能解释决策过程的搜索算法。通过蒙特卡洛树搜索高效地搜索最佳GNN架构。 |
| [^79] | [On Reward Structures of Markov Decision Processes.](http://arxiv.org/abs/2308.14919) | 该论文研究了马尔可夫决策过程中的奖励结构，提出了一种估计器用于估计单个状态值，并通过根据奖励代替常用的基于转移的常数，提供了对强化学习中技巧的理论解释。 |
| [^80] | [Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning.](http://arxiv.org/abs/2308.14705) | 本文提出了一个新的自助培训体制，利用独立子网络的集成和新的损失函数来提高自助的鲁棒性表示学习的效率和多样性。 |
| [^81] | [FAM: fast adaptive meta-learning.](http://arxiv.org/abs/2308.13970) | 本论文提出了一个快速自适应联邦元学习（FAM）框架，可以协作学习一个全局模型，并在个别客户端上进行个性化。这解决了数据分布发散和隐私限制的问题，并且适用于需要在不同客户端之间进行个性化的领域转变。 |
| [^82] | [Stochastic Configuration Machines for Industrial Artificial Intelligence.](http://arxiv.org/abs/2308.13570) | 本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。 |
| [^83] | [Human Comprehensible Active Learning of Genome-Scale Metabolic Networks.](http://arxiv.org/abs/2308.12740) | 这项研究介绍了一种人类可理解的基因组规模代谢网络的主动学习方法，基于归纳逻辑编程(ILP)框架进行逻辑推理，并通过从实验中学习新的逻辑结构，以有效探索假设空间和指导实验设计。 |
| [^84] | [Less is More -- Towards parsimonious multi-task models using structured sparsity.](http://arxiv.org/abs/2308.12114) | 该论文研究了将结构稀疏性引入多任务学习框架，开发了一种简洁的模型，可以用较少的参数有效地处理多个任务，并在性能上与密集模型相当或更优。通过在训练期间对模型进行稀疏化，可以减少内存占用、计算需求和预测时间。具体而言，该研究通过在卷积神经网络的共享层中使用通道级l1/l2组稀疏，同时消除多余的组（通道）并对权重施加惩罚，提高了所有任务的学习能力。 |
| [^85] | [Efficient Benchmarking (of Language Models).](http://arxiv.org/abs/2308.11696) | 本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。 |
| [^86] | [Test Time Embedding Normalization for Popularity Bias Mitigation.](http://arxiv.org/abs/2308.11288) | 本文提出了一种名为“测试时间嵌入归一化”的策略来解决推荐系统中的热门偏见问题。该方法利用归一化的物品嵌入来控制嵌入大小，并通过与用户和物品嵌入的角度相似度区分受欢迎和不受欢迎的物品，从而有效减少了热门偏见的影响。 |
| [^87] | [A Comprehensive Empirical Evaluation on Online Continual Learning.](http://arxiv.org/abs/2308.10328) | 这项综合实证评估了解决在线持续学习问题的各种方法，发现大多数方法存在稳定性和欠拟合问题，但所学习的表示与独立同分布的训练相当。 |
| [^88] | [Activation Addition: Steering Language Models Without Optimization.](http://arxiv.org/abs/2308.10248) | 这项研究探讨了一种在推理时通过改变激活来预测性地改变语言模型行为的方法，并且相比于传统方法具有更低的计算和实施成本，并且能够保持模型性能。 |
| [^89] | [Neural-network quantum state study of the long-range antiferromagnetic Ising chain.](http://arxiv.org/abs/2308.09709) | 在这项研究中，我们使用变分蒙特卡洛方法和约束玻尔兹曼机作为试验波函数参数化，研究了具有代数衰减长程反铁磁相互作用的横向场伊辛链中的量子相变。通过有限大小缩放分析，我们发现中心荷的值与衰变指数$\alpha_\mathrm{LR}$不同，而临界指数则保持接近短程伊辛模型的值，这支持了先前提出的共形不变性破缺场景。 |
| [^90] | [Graph Structural Residuals: A Learning Approach to Diagnosis.](http://arxiv.org/abs/2308.06961) | 本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合，通过数据学习系统的底层结构并提供动态观察。研究通过重新定义系统表示、观察和故障的构建，引入自监督图结构学习模型以及在耦合振荡器系统上的实验，展示了数据驱动的诊断方法的潜力。 |
| [^91] | [Tango: rethinking quantization for graph neural network training on GPUs.](http://arxiv.org/abs/2308.00890) | Tango是一个重新思考在GPU上对图神经网络训练的量化方法的研究，通过引入新的规则来保持准确度，设计并实现量化感知的基本操作和基本操作之间的优化，以加速GNN训练。 |
| [^92] | [Rational kernel-based interpolation for complex-valued frequency response functions.](http://arxiv.org/abs/2307.13484) | 本论文提出了一种基于有理核的复频率响应函数插值方法，通过引入新的复值函数再生核希尔伯特空间，并结合低阶有理函数进行自适应插值，解决了频率响应函数拟合过程中标准核方法表现不佳的问题。 |
| [^93] | [CLIPAG: Towards Generator-Free Text-to-Image Generation.](http://arxiv.org/abs/2306.16805) | 本文将感知对齐梯度（PAG）的研究扩展到视觉-语言架构，并通过对 CLIP 进行鲁棒性调整，展示了在视觉-语言生成任务中集成 CLIPAG 可以实现显著改进，并实现了无生成器的文本到图像生成。 |
| [^94] | [Fairness in Preference-based Reinforcement Learning.](http://arxiv.org/abs/2306.09995) | 该论文提出了一种名为FPbRL的新的公平偏好强化学习方法，旨在通过广义Gini福利函数最大化策略学习来实现多目标优化并处理每个目标的公平性。 |
| [^95] | [Causal Policy Gradient for Whole-Body Mobile Manipulation.](http://arxiv.org/abs/2305.04866) | 本文提出了一种新框架——因果MoMa，可以训练适用于典型MoMa任务的策略，在此框架下，机动和交互自由度可以同时组合，并且不需要人类领域知识来划分动作空间或将动作部分与子目标匹配。 |
| [^96] | [Multi-granulariy Time-based Transformer for Knowledge Tracing.](http://arxiv.org/abs/2304.05257) | 本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。 |
| [^97] | [Euler Characteristic Tools For Topological Data Analysis.](http://arxiv.org/abs/2303.14040) | 本文研究了欧拉特征技术在拓扑数据分析中的应用，利用点运算欧拉特征得到欧拉特征轮廓，在监督和无监督任务中实现了最先进性能，并提供了欧拉轮廓及其混合变换捕捉信息的启发式方法。 |
| [^98] | [Model Stitching: Looking For Functional Similarity Between Representations.](http://arxiv.org/abs/2303.11277) | 本研究扩展了模型拼接方法，使其能够比较不同神经网络架构中不同形状层学习到的表示。我们发现在小ResNets中使用基于卷积的拼接，在第一个网络中较晚出现的层与第二个网络中对应层距离较远的情况下也能达到较高的准确度。 |
| [^99] | [Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models.](http://arxiv.org/abs/2303.08440) | 本论文提出了一种使用两个预训练的垂直二维扩散模型来解决三维逆问题的方法，将三维数据分布建模为在不同方向上切片的二维分布的乘积，解决了维数的灾难性问题，并以高效的方式用于医学图像重建任务。 |
| [^100] | [Interpretable Outlier Summarization.](http://arxiv.org/abs/2303.06261) | STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。 |
| [^101] | [Task Aware Dreamer for Task Generalization in Reinforcement Learning.](http://arxiv.org/abs/2303.05092) | 本文提出了一种名为Task Aware Dreamer（TAD）的方法用于强化学习中的任务泛化。通过量化任务分布的相关性，TAD能够将历史信息编码到策略中，以便区分不同任务，并在泛化到未见任务时具有较好的性能。 |
| [^102] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^103] | [SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics.](http://arxiv.org/abs/2302.11055) | 该论文研究了随机梯度下降(SGD)算法在神经网络上的学习时间复杂度，提出了一种复杂度度量称为跃迁，证明了在高斯各向同性数据和二层神经网络上的研究结果，并展示了训练过程中函数支持的动态学习方法。 |
| [^104] | [LEVER: Learning to Verify Language-to-Code Generation with Execution.](http://arxiv.org/abs/2302.08468) | 提出了一种使用执行结果来验证生成的程序的简单方法LEVER，通过训练验证器根据自然语言输入、程序本身和执行结果来确定程序的正确性，从而改进了语言到代码生成的过程。 |
| [^105] | [Improving Differentiable Architecture Search via Self-Distillation.](http://arxiv.org/abs/2302.05629) | 本文提出了自我蒸馏可微分神经架构搜索（SD-DARTS）方法，通过从超网的先前步骤中蒸馏知识来指导其训练，有效降低了超网损失的尖锐度，从而缓解了离散化差距。 |
| [^106] | [SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms.](http://arxiv.org/abs/2301.12865) | 本文提出了一种动态批处理策略，采用基于GPU的批处理服务队列进行建模，通过半马尔可夫决策过程的方法最小化平均响应时间和功耗。 |
| [^107] | [Domain-Agnostic Molecular Generation with Self-feedback.](http://arxiv.org/abs/2301.11259) | MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。 |
| [^108] | [Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning.](http://arxiv.org/abs/2212.01168) | 通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。 |
| [^109] | [C3: Cross-instance guided Contrastive Clustering.](http://arxiv.org/abs/2211.07136) | C3是一种引入了交叉实例关系的对比聚类方法，通过考虑实例之间的关系增加了正对数，从而提高了聚类性能。 |
| [^110] | [A Decentralized Alternating Gradient Method for Communication-Efficient Bilevel Programming.](http://arxiv.org/abs/2211.04088) | 本文提出了一种通信高效的分散式交替梯度法解决双层规划问题，相较于其他方法，该算法具有更低的通信成本和更高的隐私性。 |
| [^111] | [Neural Augmented Kalman Filtering with Bollinger Bands for Pairs Trading.](http://arxiv.org/abs/2210.15448) | 本研究提出了KalmenNet辅助的Bollinger带配对交易策略（KBPT），通过深度学习增强了KF-BB交易操作。KBPT通过建立一个扩展的空间状态模型来近似配对交易的关系，并利用基于卡尔曼滤波的神经网络提高了交易效果。 |
| [^112] | [Fair admission risk prediction with proportional multicalibration.](http://arxiv.org/abs/2209.14613) | 提出了一种称为比例多校准的方法来实现公平校准，在限制校准误差的同时保持整体校准。通过比例多校准可以避免模型对特定群体的预测信任或不信任的问题。 |
| [^113] | [Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks.](http://arxiv.org/abs/2208.03160) | 本文提出了一种新技术，可以构建具有小Lipschitz常数的高效通用Lipschitz网络。该技术具有形式化保证、易于实现和高效运行的优点，并且可以与任何训练目标和优化方法相结合。主要贡献是通过重新缩放的权重矩阵参数化，确保每个网络层的Lipschitz常数最大为1，并且导致学习的权重矩阵接近正交。 |
| [^114] | [Simple and Efficient Heterogeneous Graph Neural Network.](http://arxiv.org/abs/2207.02547) | SeHGNN是一个简单高效的异构图神经网络，通过轻量级的均值聚合器预先计算邻居聚合来捕捉结构信息，采用单层结构和长浏览路径来更好地利用语义信息。 |
| [^115] | [LaserMix for Semi-Supervised LiDAR Semantic Segmentation.](http://arxiv.org/abs/2207.00026) | 本文提出了一种名为LaserMix的半监督学习方法，利用LiDAR点云的强空间线索更好地利用未标记数据，具有通用性、统计基础和有效性，并在SemanticKITTI上取得了最优性能。 |
| [^116] | [Feature Extractor Stacking for Cross-domain Few-shot Meta-learning.](http://arxiv.org/abs/2205.05831) | 这篇论文提出了一种新的跨领域少样本元学习方法，称为特征提取器叠加(FES)。FES通过叠加多个主干的信息，可以利用异构预训练的主干，而且不需要维护一个需要重新计算的通用模型。 |
| [^117] | [Mapping the landscape of histomorphological cancer phenotypes using self-supervised learning on unlabeled, unannotated pathology slides.](http://arxiv.org/abs/2205.01931) | 本研究提出了一种名为组织形态学表型学习的自监督方法，可以在未标记、未注释的病理切片上绘制出癌症组织的地图，从而揭示出不同表型间的特征和转变过程。 |
| [^118] | [Multi-Label Clinical Time-Series Generation via Conditional GAN.](http://arxiv.org/abs/2204.04797) | 本论文提出了一种通过条件GAN生成多标签临床时间序列的方法，用于解决EHR数据生成中的挑战，包括时间序列数据和不平衡的罕见疾病。生成器使用门控循环单元和平滑条件矩阵生成序列和罕见疾病，评论家使用Wasserstein距离评分来区分真实样本和合成样本。 |
| [^119] | [A New Multifractal-based Deep Learning Model for Text Mining.](http://arxiv.org/abs/2111.13861) | 该论文介绍了一个基于多重分形的深度学习模型，用于解读文本中隐藏的多重分形属性，并结合提出的激活函数，在神经网络结构中实现非线性信息传输。 |
| [^120] | [The Rich Get Richer: Disparate Impact of Semi-Supervised Learning.](http://arxiv.org/abs/2110.06282) | 本文研究了半监督学习的不平等影响，发现那些在不使用SSL时具有更高基准准确性的子族群更容易从SSL中获益，而那些基准准确性较低的子族群在添加SSL模块后可能会观察到性能下降。 |
| [^121] | [Best Practices for Noise-Based Augmentation to Improve the Performance of Deployable Speech-Based Emotion Recognition Systems.](http://arxiv.org/abs/2104.08806) | 在该论文中，作者验证了噪声的存在确实会改变注释标签，并展示了忽视这一知识会对情感识别模型的性能评估产生影响。 |
| [^122] | [Simulation comparisons between Bayesian and de-biased estimators in low-rank matrix completion.](http://arxiv.org/abs/2103.11749) | 本文通过模拟比较了贝叶斯方法和去偏估计器在低秩矩阵补全问题中的性能。结果表明去偏估计器与贝叶斯估计器一样好，在样本较小的情况下贝叶斯方法更稳定。 |
| [^123] | [Topology-aware Tensor Decomposition for Meta-graph Learning.](http://arxiv.org/abs/2101.01078) | 本论文提出了一种基于拓扑感知张量分解的方法，用于学习元图。这种方法不仅解释了现有方法的局限性，还提出了一种反映有向无环图结构的拓扑感知张量分解方法。 |
| [^124] | [STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos.](http://arxiv.org/abs/2003.08429) | 本文提出了一种适用于视频实例分割的新方法，通过将视频剪辑建模为一个时空体，在一个阶段内对空间和时间进行实例分割和跟踪，从而避免了多阶段管道和特定任务的需求。我们引入了时空嵌入的概念，并使用增强特征表示的混合函数来聚类像素，以实现更好的实例分割效果。 |
| [^125] | [Communication-Efficient Distributed Deep Learning: A Comprehensive Survey.](http://arxiv.org/abs/2003.06307) | 本文综述了通信高效的分布式深度学习的研究进展，重点探讨了系统级和算法级的优化方法，并比较了不同算法的收敛速度。 |

# 详细

[^1]: Point-Bind和Point-LLM：用于3D理解、生成和指导跟随的多模态点云对齐

    Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. (arXiv:2309.00615v1 [cs.CV])

    [http://arxiv.org/abs/2309.00615](http://arxiv.org/abs/2309.00615)

    Point-Bind和Point-LLM是用于3D理解、生成和指导跟随的多模态点云对齐模型，能实现任意到3D生成、3D嵌入算术和3D开放世界的理解，并且Point-LLM能实现3D和多模态问答功能。

    

    我们引入了Point-Bind，一个将点云与2D图像、语言、音频和视频对齐的3D多模态模型。在ImageBind的指导下，我们构建了一个将3D和多模态嵌入空间进行结合的模型，实现了许多有前景的应用，例如任意到3D生成、3D嵌入算术和3D开放世界的理解。在此基础上，我们进一步提出了Point-LLM，第一个遵循3D多模态指令的大型语言模型（LLM）。通过参数高效调优技术，Point-LLM将Point-Bind的语义注入到预训练的LLMs中，例如LLaMA，不需要3D指令数据但展现出卓越的3D和多模态问答能力。我们希望我们的工作能为将3D点云扩展到多模态应用的研究社区提供启示。代码可在https://github.com/ZiyuGuo99/Point-Bind_Point-LLM找到。

    We introduce Point-Bind, a 3D multi-modality model aligning point clouds with 2D image, language, audio, and video. Guided by ImageBind, we construct a joint embedding space between 3D and multi-modalities, enabling many promising applications, e.g., any-to-3D generation, 3D embedding arithmetic, and 3D open-world understanding. On top of this, we further present Point-LLM, the first 3D large language model (LLM) following 3D multi-modal instructions. By parameter-efficient fine-tuning techniques, Point-LLM injects the semantics of Point-Bind into pre-trained LLMs, e.g., LLaMA, which requires no 3D instruction data, but exhibits superior 3D and multi-modal question-answering capacity. We hope our work may cast a light on the community for extending 3D point clouds to multi-modality applications. Code is available at https://github.com/ZiyuGuo99/Point-Bind_Point-LLM.
    
[^2]: 面向对齐语言模型的对抗攻击的基线防御

    Baseline Defenses for Adversarial Attacks Against Aligned Language Models. (arXiv:2309.00614v1 [cs.LG])

    [http://arxiv.org/abs/2309.00614](http://arxiv.org/abs/2309.00614)

    这篇论文研究了对齐语言模型面临的对抗攻击问题，通过评估基线防御策略的效果，探讨了各种策略的可行性和有效性，并对鲁棒性和性能进行了讨论。

    

    随着大型语言模型的普及，其安全漏洞变得至关重要。最近的研究表明，文本优化器可以生成绕过审查和对齐的越狱提示。借鉴对抗机器学习的丰富研究成果，我们从三个问题入手：在这个领域中什么样的威胁模型是实用的？基线防御技术在这个新领域中表现如何？LLM安全性与计算机视觉有何不同？我们对主导对抗LLM攻击的几种基线防御策略进行评估，讨论了每种策略在各种设置下的可行性和有效性。特别是，我们关注三种类型的防御：检测（基于困惑度）、输入预处理（改写和重新标记化）和对抗训练。我们讨论了白盒和灰盒设置，并讨论了每种考虑的防御策略在鲁棒性和性能之间的权衡。

    As Large Language Models quickly become ubiquitous, their security vulnerabilities are critical to understand. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision?  We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. Surprisingly, we find much more succ
    
[^3]: 使用扩散模型的多粒度迭代图像编辑

    Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])

    [http://arxiv.org/abs/2309.00613](http://arxiv.org/abs/2309.00613)

    本研究提出了一种名为EMILIE的迭代多粒度图像编辑器，通过重新利用预训练的扩散模型来实现迭代编辑，并引入梯度控制操作来实现对图像编辑的粒度控制。

    

    最近，文本引导的图像合成的进展极大地改变了创意专业人员生成艺术和审美上令人愉悦的视觉资产的方式。为了充分支持这样的创意努力，该过程应具备以下能力：1）迭代地编辑生成的图像，2）控制所需变化的空间范围（全局、局部或介于两者之间）。我们将这个实用的问题设定正式化为迭代多粒度编辑。虽然在图像合成和编辑方面，基于扩散模型取得了重大进展，但它们都是一次性操作（即没有迭代编辑能力），并且不能自然产生多粒度控制（即涵盖从局部到全局编辑的全谱）。为了克服这些缺点，我们提出了EMILIE：迭代多粒度图像编辑器。EMILIE引入了一种新颖的潜在迭代策略，利用预训练的扩散模型来促进迭代编辑。同时，还引入了梯度控制操作来实现对所需变化的粒度控制。

    Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control opera
    
[^4]: 基于贝叶斯深度学习的宇宙尺度中的修正引力研究

    Bayesian deep learning for cosmic volumes with modified gravity. (arXiv:2309.00612v1 [astro-ph.CO])

    [http://arxiv.org/abs/2309.00612](http://arxiv.org/abs/2309.00612)

    该研究利用贝叶斯深度学习的方法，从修正引力模拟中提取宇宙学参数，并对不确定性进行了评估。

    

    新一代的星系调查将提供前所未有的数据，使我们能够在宇宙尺度上测试引力。对大尺度结构的健壮宇宙学分析需要利用编码在宇宙网中的非线性信息。机器学习技术提供了这样的工具，然而却不能提供先验的不确定性评估。本研究旨在通过具有不确定性估计的深度神经网络从修正引力（MG）模拟中提取宇宙学参数。我们使用贝叶斯神经网络（BNNs）实现了一个丰富的近似后验分布，分别考虑了一个带有单一贝叶斯最后一层（BLL）的情况，和一个在所有层面上都具有贝叶斯层（FullB）的情况。我们使用实空间密度场和一套2000个仅包含暗物质粒子网格$ N $-体模拟的功率谱对这两个BNN进行训练，这些模拟包括基于MG-PICOLA的修正引力模型，覆盖了边长为256 $h^{-1}$ Mpc的立方体体积，其中包含128$。

    The new generation of galaxy surveys will provide unprecedented data allowing us to test gravity at cosmological scales. A robust cosmological analysis of the large-scale structure demands exploiting the nonlinear information encoded in the cosmic web. Machine Learning techniques provide such tools, however, do not provide a priori assessment of uncertainties. This study aims at extracting cosmological parameters from modified gravity (MG) simulations through deep neural networks endowed with uncertainty estimations. We implement Bayesian neural networks (BNNs) with an enriched approximate posterior distribution considering two cases: one with a single Bayesian last layer (BLL), and another one with Bayesian layers at all levels (FullB). We train both BNNs with real-space density fields and power-spectra from a suite of 2000 dark matter only particle mesh $N$-body simulations including modified gravity models relying on MG-PICOLA covering 256 $h^{-1}$ Mpc side cubical volumes with 128$
    
[^5]: Copiloting the Copilots: 将大型语言模型与完成引擎融合用于自动化程序修复

    Copiloting the Copilots: Fusing Large Language Models with Completion Engines for Automated Program Repair. (arXiv:2309.00608v1 [cs.SE])

    [http://arxiv.org/abs/2309.00608](http://arxiv.org/abs/2309.00608)

    这篇论文提出了一种框架，利用完成引擎来进一步支持大型语言模型在自动化程序修复中合成更多有效的修补程序。

    

    在自动化程序修复中，对于通用编程语言中的实际系统合成正确的修补程序可能具有挑战性。最近的大型语言模型（LLMs）已被证明对开发人员在各种编码任务中具有帮助，并且已直接应用于修补程序的合成。然而，大多数LLMs将程序视为令牌序列，这意味着它们对目标编程语言的底层语义约束一无所知。这导致生成了大量静态无效的修补程序，阻碍了该技术的实用性。因此，我们提出了Repilot，一种在修复过程中通过合成更多有效修补程序从而进一步支持AI“副驾驶员”（即LLMs）的框架。我们的关键见解是，许多LLMs以自回归方式生成输出（即逐个令牌生成），类似于人类编写程序，这可以通过完成引擎显著提升和引导。Repilot协同合成了修补程序。

    During Automated Program Repair (APR), it can be challenging to synthesize correct patches for real-world systems in general-purpose programming languages. Recent Large Language Models (LLMs) have been shown to be helpful "copilots" in assisting developers with various coding tasks, and have also been directly applied for patch synthesis. However, most LLMs treat programs as sequences of tokens, meaning that they are ignorant of the underlying semantics constraints of the target programming language. This results in plenty of statically invalid generated patches, impeding the practicality of the technique. Therefore, we propose Repilot, a framework to further copilot the AI "copilots" (i.e., LLMs) by synthesizing more valid patches during the repair process. Our key insight is that many LLMs produce outputs autoregressively (i.e., token by token), resembling human writing programs, which can be significantly boosted and guided through a Completion Engine. Repilot synergistically synthe
    
[^6]: 快速和遗憾最小的最佳臂识别：基本限制和低复杂度算法

    Fast and Regret Optimal Best Arm Identification: Fundamental Limits and Low-Complexity Algorithms. (arXiv:2309.00591v1 [cs.LG])

    [http://arxiv.org/abs/2309.00591](http://arxiv.org/abs/2309.00591)

    本文提出了一种名为 ROBAI 的算法，旨在快速识别并选择最佳臂，并在一系列连续回合中最大化奖励。该算法在预定停止时间和自适应停止时间要求下均实现了渐进最优遗憾，并且在预定停止时间下仅需 $\mathcal{O}(\log T)$ 回合即可选择最佳臂，在自适应停止时间下仅需 $\mathcal{O}(\log^2 T)$ 回合即可选择最佳臂。

    

    本文考虑具有双重目标的随机多臂老虎机(MAB)问题：(i) 快速识别并选择最佳臂，以及(ii) 在一系列T个连续回合中最大化奖励。尽管每个目标都已经得到了独立的深入研究，即(i)的最佳臂识别和(ii)的遗憾最小化，但是同时实现这两个目标仍然是一个开放的问题，尽管它在实践中非常重要。本文引入了“遗憾最小化的最佳臂识别”(ROBAI)，旨在实现这两个双重目标。为了解决具有预定停止时间和自适应停止时间要求的ROBAI，我们分别提出了$\mathsf{EOCP}$算法及其变体，不仅在高斯老虎机和一般老虎机中达到了渐进最优遗憾，而且在预定停止时间下，在$\mathcal{O}(\log T)$回合内选择了最佳臂，在自适应停止时间下，选择了最佳臂在$\mathcal{O}(\log^2 T)$回合内。

    This paper considers a stochastic multi-armed bandit (MAB) problem with dual objectives: (i) quick identification and commitment to the optimal arm, and (ii) reward maximization throughout a sequence of $T$ consecutive rounds. Though each objective has been individually well-studied, i.e., best arm identification for (i) and regret minimization for (ii), the simultaneous realization of both objectives remains an open problem, despite its practical importance. This paper introduces \emph{Regret Optimal Best Arm Identification} (ROBAI) which aims to achieve these dual objectives. To solve ROBAI with both pre-determined stopping time and adaptive stopping time requirements, we present the $\mathsf{EOCP}$ algorithm and its variants respectively, which not only achieve asymptotic optimal regret in both Gaussian and general bandits, but also commit to the optimal arm in $\mathcal{O}(\log T)$ rounds with pre-determined stopping time and $\mathcal{O}(\log^2 T)$ rounds with adaptive stopping ti
    
[^7]: PolyGET: 使用等变换器进行准确通用的力场加速聚合物模拟

    PolyGET: Accelerating Polymer Simulations by Accurate and Generalizable Forcefield with Equivariant Transformer. (arXiv:2309.00585v1 [cs.LG])

    [http://arxiv.org/abs/2309.00585](http://arxiv.org/abs/2309.00585)

    PolyGET是一个新的聚合物力场框架，使用等变换器模型来捕捉复杂的量子相互作用，并在不同聚合物家族上进行泛化。它通过优化力场模型来提高模拟的准确性和效率。

    

    聚合物模拟同时具备准确性和效率是一项具有挑战性的任务。已经开发了机器学习(ML)力场以实现原子论方法的准确性和经验力场的效率。然而，现有的ML力场通常仅限于单分子模拟，并且它们的模拟不够稳健。在本文中，我们提出了PolyGET，一个新的聚合物力场通用等变换器框架。PolyGET使用称为等变换器的深度学习模型来捕捉原子之间的复杂量子相互作用，并跨不同聚合物家族进行泛化。我们提出了一种新的训练范式，专注于优化力，与现有的同时优化能量和力的方法不同。这个简单的以力为中心的目标函数避免了能量和力之间的竞争目标，从而允许在不同的聚合物上学习统一的力场ML模型。

    Polymer simulation with both accuracy and efficiency is a challenging task. Machine learning (ML) forcefields have been developed to achieve both the accuracy of ab initio methods and the efficiency of empirical force fields. However, existing ML force fields are usually limited to single-molecule settings, and their simulations are not robust enough. In this paper, we present PolyGET, a new framework for Polymer Forcefields with Generalizable Equivariant Transformers. PolyGET is designed to capture complex quantum interactions between atoms and generalize across various polymer families, using a deep learning model called Equivariant Transformers. We propose a new training paradigm that focuses exclusively on optimizing forces, which is different from existing methods that jointly optimize forces and energy. This simple force-centric objective function avoids competing objectives between energy and forces, thereby allowing for learning a unified forcefield ML model over different poly
    
[^8]: Laminar:一种具有语义代码搜索和代码自动完成的新型无服务器流式框架

    Laminar: A New Serverless Stream-based Framework with Semantic Code Search and Code Completion. (arXiv:2309.00584v1 [cs.DC])

    [http://arxiv.org/abs/2309.00584](http://arxiv.org/abs/2309.00584)

    Laminar是一个新的无服务器流式框架，通过语义代码搜索和代码自动完成增强了框架的功能，简化了流式计算的执行，更高效地管理数据流，并为研究人员和实践者提供了有价值的工具。

    

    本文介绍了Laminar，这是一种基于dispel4py的新型无服务器框架，它是一个并行流式数据流库。Laminar通过专用的注册表高效地管理流程工作流和组件，提供了无缝的无服务器体验。利用大型语言模型，Laminar通过语义代码搜索、代码概述和代码自动完成提升了框架的功能。这项贡献通过简化流式计算的执行、更高效地管理数据流，并为研究人员和实践者提供了有价值的工具，增强了无服务器计算的能力。

    This paper introduces Laminar, a novel serverless framework based on dispel4py, a parallel stream-based dataflow library. Laminar efficiently manages streaming workflows and components through a dedicated registry, offering a seamless serverless experience. Leveraging large lenguage models, Laminar enhances the framework with semantic code search, code summarization, and code completion. This contribution enhances serverless computing by simplifying the execution of streaming computations, managing data streams more efficiently, and offering a valuable tool for both researchers and practitioners.
    
[^9]: 基于几何信息的神经算子用于大规模三维偏微分方程

    Geometry-Informed Neural Operator for Large-Scale 3D PDEs. (arXiv:2309.00583v1 [cs.LG])

    [http://arxiv.org/abs/2309.00583](http://arxiv.org/abs/2309.00583)

    我们提出了一种基于几何信息的神经算子（GINO），用于学习大规模偏微分方程的解算符。GINO使用符号距离函数和点云表示来处理不同几何形状，并通过图和傅里叶结构的神经算子实现离散一致性。在实验中，我们验证了GINO在大规模模拟中的性能。

    

    我们提出了一种基于几何信息的神经算子（GINO），它是一种高效的方法，用于学习具有不同几何形状的大规模偏微分方程的解算符。GINO使用符号距离函数和输入形状的点云表示，以及基于图和傅里叶结构的神经算子来学习解算符。图神经算子处理不规则网格，并将其转换成可以有效应用傅里叶神经算子的规则潜在网格。GINO是离散一致的，这意味着训练模型可以应用于连续域的任意离散化，并在离散化细化时收敛到连续算子。为了经验性地验证我们的方法在大规模模拟上的性能，我们生成了包含最高500万雷诺数的3D车辆几何行业标准空气动力学数据集。

    We propose the geometry-informed neural operator (GINO), a highly efficient approach to learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function and point-cloud representations of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. GINO is discretization-convergent, meaning the trained model can be applied to arbitrary discretization of the continuous domain and it converges to the continuum operator as the discretization is refined. To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numeri
    
[^10]: Lloyd算法在扰动下的一致性

    Consistency of Lloyd's Algorithm Under Perturbations. (arXiv:2309.00578v1 [cs.LG])

    [http://arxiv.org/abs/2309.00578](http://arxiv.org/abs/2309.00578)

    该论文研究了Lloyd算法在扰动样本上的一致性，证明了在适当初始化和扰动相对于亚高斯噪声较小的假设下，算法在O(log(n))次迭代后的错聚类率在指数下界受限。

    

    在无监督学习的背景下，Lloyd算法是最常用的聚类算法之一。它启发了大量的工作，研究了算法在不同设置下对地面真实聚类的正确性。特别是在2016年，卢和周表明，在正确初始化算法的前提下，Lloyd算法在从亚高斯混合中独立抽取的n个样本上的错聚类率在O(log(n))次迭代后指数下界受限。然而，在许多应用中，真实样本是未观测到的，需要通过预处理流水线（如合适的数据矩阵上的谱方法）从数据中学习。我们展示了在适当初始化和扰动相对于亚高斯噪声较小的假设下，Lloyd算法在从亚高斯混合中扰动样本上的错聚类率在O(log(n))次迭代后同样指数下界受限。

    In the context of unsupervised learning, Lloyd's algorithm is one of the most widely used clustering algorithms. It has inspired a plethora of work investigating the correctness of the algorithm under various settings with ground truth clusters. In particular, in 2016, Lu and Zhou have shown that the mis-clustering rate of Lloyd's algorithm on $n$ independent samples from a sub-Gaussian mixture is exponentially bounded after $O(\log(n))$ iterations, assuming proper initialization of the algorithm. However, in many applications, the true samples are unobserved and need to be learned from the data via pre-processing pipelines such as spectral methods on appropriate data matrices. We show that the mis-clustering rate of Lloyd's algorithm on perturbed samples from a sub-Gaussian mixture is also exponentially bounded after $O(\log(n))$ iterations under the assumptions of proper initialization and that the perturbation is small relative to the sub-Gaussian noise. In canonical settings with g
    
[^11]: 卷积神经网络中特征学习的机制

    Mechanism of feature learning in convolutional neural networks. (arXiv:2309.00570v1 [stat.ML])

    [http://arxiv.org/abs/2309.00570](http://arxiv.org/abs/2309.00570)

    本论文研究了卷积神经网络从图像数据中学习特征的机制，并提出了卷积神经特征假设。通过实证分析和理论证明，论文展示了滤波器的协方差与输入补丁的平均梯度外积之间的高度相关性，以及通过基于补丁的平均梯度外积实现深度特征学习的普遍性。

    

    理解卷积神经网络如何从图像数据中学习特征是机器学习和计算机视觉中的一个基本问题。在这项工作中，我们确定了这样一个机制。我们提出了卷积神经特征假设，它指出任何卷积层中滤波器的协方差与该层输入的补丁的平均梯度外积成正比。我们对我们的假设提供了大量的实证证据，包括在标准神经网络架构（如AlexNet，VGG和在ImageNet上预训练的ResNets）的卷积层中，发现滤波器的协方差与基于补丁的平均梯度外积之间存在高度相关性。我们还提供了支持理论的证据。然后，我们通过使用基于补丁的平均梯度外积来展示我们结果的普遍性，实现了在卷积核机器中的深度特征学习。我们将得到的算法称为(Deep) ConvRFM，并展示了我们的算法能够恢复出类似的特征。

    Understanding the mechanism of how convolutional neural networks learn features from image data is a fundamental problem in machine learning and computer vision. In this work, we identify such a mechanism. We posit the Convolutional Neural Feature Ansatz, which states that covariances of filters in any convolutional layer are proportional to the average gradient outer product (AGOP) taken with respect to patches of the input to that layer. We present extensive empirical evidence for our ansatz, including identifying high correlation between covariances of filters and patch-based AGOPs for convolutional layers in standard neural architectures, such as AlexNet, VGG, and ResNets pre-trained on ImageNet. We also provide supporting theoretical evidence. We then demonstrate the generality of our result by using the patch-based AGOP to enable deep feature learning in convolutional kernel machines. We refer to the resulting algorithm as (Deep) ConvRFM and show that our algorithm recovers simil
    
[^12]: 从结构 MRI 中合成 Amyloid-Beta 轴向平面 PET：用于筛查阿尔茨海默病的图像翻译方法

    Amyloid-Beta Axial Plane PET Synthesis from Structural MRI: An Image Translation Approach for Screening Alzheimer's Disease. (arXiv:2309.00569v1 [eess.IV])

    [http://arxiv.org/abs/2309.00569](http://arxiv.org/abs/2309.00569)

    通过使用图像翻译模型，本研究证明了从结构 MRI 中合成具有准确定量能力的 amyloid-beta PET 图像的可行性，为仅依靠 MRI 获得 amyloid-beta 信息提供了一种新途径。

    

    本研究采用图像翻译模型，从结构 MRI 中产生合成的定量准确的 amyloid-beta PET 图像。使用了 amyloid-beta PET 和结构 MRI 的图像对来训练模型。结果表明，合成的 PET 图像在形状、对比度和整体的 SSIM 和 PSNR 上与真实图像具有很高的相似度。该研究表明，通过结构到定量图像的翻译，可以从仅有的 MRI 中获得 amyloid-beta 信息。

    In this work, an image translation model is implemented to produce synthetic amyloid-beta PET images from structural MRI that are quantitatively accurate. Image pairs of amyloid-beta PET and structural MRI were used to train the model. We found that the synthetic PET images could be produced with a high degree of similarity to truth in terms of shape, contrast and overall high SSIM and PSNR. This work demonstrates that performing structural to quantitative image translation is feasible to enable the access amyloid-beta information from only MRI.
    
[^13]: 高维线性回归的解释：空间零值和正则化在电池数据上的影响的演示

    Interpretation of High-Dimensional Linear Regression: Effects of Nullspace and Regularization Demonstrated on Battery Data. (arXiv:2309.00564v1 [stat.ML])

    [http://arxiv.org/abs/2309.00564](http://arxiv.org/abs/2309.00564)

    本文研究了高维线性回归在解释方面的挑战，发现空间零值和正则化对回归系数产生重要影响，并提出了一种优化公式来比较回归系数与物理工程知识得到的系数，从而实现解释性的回归结果。

    

    高维线性回归在许多科学领域中非常重要。本文考虑到从化学或生物系统中经常得到的基础平滑潜在过程的离散测量数据。在高维度中解释是具有挑战性的，因为空间零值及其与正则化的相互作用会塑造回归系数。数据的空间零值包含所有满足$\mathbf{Xw}=\mathbf{0}$的系数，从而允许非常不同的系数产生相同的预测。我们开发了一种优化公式来比较回归系数和通过物理工程知识得到的系数，以了解系数差异的哪些部分接近于空间零值。这种空间零值方法在一个合成示例和锂离子电池数据上进行了测试。案例研究表明，如果根据先前的物理知识选择合适的正则化和z-score处理，可以得到可解释的回归结果。

    High-dimensional linear regression is important in many scientific fields. This article considers discrete measured data of underlying smooth latent processes, as is often obtained from chemical or biological systems. Interpretation in high dimensions is challenging because the nullspace and its interplay with regularization shapes regression coefficients. The data's nullspace contains all coefficients that satisfy $\mathbf{Xw}=\mathbf{0}$, thus allowing very different coefficients to yield identical predictions. We developed an optimization formulation to compare regression coefficients and coefficients obtained by physical engineering knowledge to understand which part of the coefficient differences are close to the nullspace. This nullspace method is tested on a synthetic example and lithium-ion battery data. The case studies show that regularization and z-scoring are design choices that, if chosen corresponding to prior physical knowledge, lead to interpretable regression results. 
    
[^14]: 交互式和集中式差分隐私在Bandit问题中的应用

    Interactive and Concentrated Differential Privacy for Bandits. (arXiv:2309.00557v1 [stat.ML])

    [http://arxiv.org/abs/2309.00557](http://arxiv.org/abs/2309.00557)

    本文研究了在交互学习和推荐系统中隐私保护的Bandit问题，并引入了集中差分隐私的概念。通过提供关于有限臂和线性Bandit问题遗憾的下界，我们揭示了不同隐私预算下的难度区域，并发现集中差分隐私可以比全局差分隐私更有效地保护隐私，我们提出了两种相应的算法。

    

    Bandit问题在交互式学习方案和现代推荐系统中起着至关重要的作用。然而，这些系统通常依赖于敏感的用户数据，因此隐私是一个重要问题。本文通过交互式差分隐私的视角研究了基于可信集中式决策者的Bandit问题的隐私性。虽然已经对纯ε-全局差分隐私的Bandit问题进行了广泛研究，但我们在理解零集中差分隐私(zCDP)的Bandit问题方面做出了贡献。针对有限臂和线性Bandit问题，我们提供了关于遗憾的最小最大和问题相关下界，从而量化了这些情况下ρ-全局zCDP的代价。这些下界揭示了基于隐私预算ρ的两个困难区域，并表明ρ-全局zCDP比纯ε-全局差分隐私产生的遗憾更小。我们提出了两种有限臂和线性Bandit问题的ρ-全局zCDP算法，即AdaC-UCB和AdaC-GOPE。这两个算法都使用了高斯机制的共同策略。

    Bandits play a crucial role in interactive learning schemes and modern recommender systems. However, these systems often rely on sensitive user data, making privacy a critical concern. This paper investigates privacy in bandits with a trusted centralized decision-maker through the lens of interactive Differential Privacy (DP). While bandits under pure $\epsilon$-global DP have been well-studied, we contribute to the understanding of bandits under zero Concentrated DP (zCDP). We provide minimax and problem-dependent lower bounds on regret for finite-armed and linear bandits, which quantify the cost of $\rho$-global zCDP in these settings. These lower bounds reveal two hardness regimes based on the privacy budget $\rho$ and suggest that $\rho$-global zCDP incurs less regret than pure $\epsilon$-global DP. We propose two $\rho$-global zCDP bandit algorithms, AdaC-UCB and AdaC-GOPE, for finite-armed and linear bandits respectively. Both algorithms use a common recipe of Gaussian mechanism 
    
[^15]: 为了构建可信赖的医疗AI系统，筛选天然对立数据集

    Curating Naturally Adversarial Datasets for Trustworthy AI in Healthcare. (arXiv:2309.00543v1 [cs.LG])

    [http://arxiv.org/abs/2309.00543](http://arxiv.org/abs/2309.00543)

    提出了一种方法来筛选自然对立示例的数据集，以评估模型的鲁棒性，并通过自动弱监督标注获得的概率标签来实现这一方法。

    

    深度学习模型在时间序列的医疗应用中展示出了有希望的预测准确性。然而，确保这些模型的鲁棒性对于构建可信赖的AI系统至关重要。现有研究主要关注于对合成对立示例的鲁棒性，这些示例是通过向清洁输入数据添加难以察觉的扰动而制作出来的。然而，这些合成对立示例并不能准确反映最具挑战性的现实场景，特别是在医疗数据的背景下。因此，对合成对立示例的鲁棒性未必能够转化为对自然产生的对立示例的鲁棒性，而这对于可信赖的AI而言是非常重要的。我们提出了一种筛选由自然对立示例组成的数据集来评估模型鲁棒性的方法。该方法依赖于通过自动弱监督标注获得的概率标签，这种标签结合了嘈杂且易获得的标注启发式方法。

    Deep learning models have shown promising predictive accuracy for time-series healthcare applications. However, ensuring the robustness of these models is vital for building trustworthy AI systems. Existing research predominantly focuses on robustness to synthetic adversarial examples, crafted by adding imperceptible perturbations to clean input data. However, these synthetic adversarial examples do not accurately reflect the most challenging real-world scenarios, especially in the context of healthcare data. Consequently, robustness to synthetic adversarial examples may not necessarily translate to robustness against naturally occurring adversarial examples, which is highly desirable for trustworthy AI. We propose a method to curate datasets comprised of natural adversarial examples to evaluate model robustness. The method relies on probabilistic labels obtained from automated weakly-supervised labeling that combines noisy and cheap-to-obtain labeling heuristics. Based on these labels
    
[^16]: 基于离散余弦变换的自适应函数逼近研究

    Adaptive function approximation based on the Discrete Cosine Transform (DCT). (arXiv:2309.00530v1 [eess.SP])

    [http://arxiv.org/abs/2309.00530](http://arxiv.org/abs/2309.00530)

    本文研究了基于离散余弦变换的自适应函数逼近方法，提出了一种简单而高效的监督学习技术，可以用于近似无记忆单变量连续函数，在学习质量与复杂性方面表现出色。

    

    本文研究了余弦作为基函数用于近似无记忆单变量连续函数的方法。本文使用了监督学习来获取逼近系数，而不是使用离散余弦变换（DCT）。由于余弦基函数的有限动态和正交性，简单的梯度算法，如归一化最小均方（NLMS），可以从中受益，并呈现出可控和可预测的收敛时间和误差不匹配。由于其简单性，该技术在学习质量与复杂性方面位列前茅，并被提出作为更复杂的监督学习系统中使用的一种有吸引力的技术。模拟实验验证了该方法的性能。本文庆祝了1973年Nasir Ahmed发表DCT的50周年。

    This paper studies the cosine as basis function for the approximation of univariate and continuous functions without memory. This work studies a supervised learning to obtain the approximation coefficients, instead of using the Discrete Cosine Transform (DCT). Due to the finite dynamics and orthogonality of the cosine basis functions, simple gradient algorithms, such as the Normalized Least Mean Squares (NLMS), can benefit from it and present a controlled and predictable convergence time and error misadjustment. Due to its simplicity, the proposed technique ranks as the best in terms of learning quality versus complexity, and it is presented as an attractive technique to be used in more complex supervised learning systems. Simulations illustrate the performance of the approach. This paper celebrates the 50th anniversary of the publication of the DCT by Nasir Ahmed in 1973.
    
[^17]: 在随机网络上的在线分布式学习

    Online Distributed Learning over Random Networks. (arXiv:2309.00520v1 [math.OC])

    [http://arxiv.org/abs/2309.00520](http://arxiv.org/abs/2309.00520)

    本文提出了在随机网络上进行在线分布式学习的DOT-ADMM算法，通过解决在线学习、异步计算、不可靠通信和不精确计算等挑战，在一大类凸学习问题上获得了线性收敛速度。

    

    最近，在各种场景中部署的多智能体系统使得在分布式环境下解决学习问题成为可能。在这种情况下，智能体的任务是收集本地数据，然后合作训练模型，而不直接共享数据。虽然分布式学习在保护智能体隐私方面具有优势，但在设计和分析适当的算法方面也存在一些挑战。本文特别关注以下由实际实施所驱动的挑战：（i）在线学习，其中本地数据随时间变化；（ii）异步智能体计算；（iii）不可靠和有限的通信；（iv）不精确的本地计算。为了应对这些挑战，我们介绍了分布式操作理论（DOT）版本的交替方向乘子法（ADMM），称之为DOT-ADMM算法。我们证明了它在大类凸学习问题上具有线性收敛速度。

    The recent deployment of multi-agent systems in a wide range of scenarios has enabled the solution of learning problems in a distributed fashion. In this context, agents are tasked with collecting local data and then cooperatively train a model, without directly sharing the data. While distributed learning offers the advantage of preserving agents' privacy, it also poses several challenges in terms of designing and analyzing suitable algorithms. This work focuses specifically on the following challenges motivated by practical implementation: (i) online learning, where the local data change over time; (ii) asynchronous agent computations; (iii) unreliable and limited communications; and (iv) inexact local computations. To tackle these challenges, we introduce the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), which we call the DOT-ADMM Algorithm. We prove that it converges with a linear rate for a large class of convex learning 
    
[^18]: 两层神经网络全局最小值附近的结构和梯度动力学

    Structure and Gradient Dynamics Near Global Minima of Two-layer Neural Networks. (arXiv:2309.00508v1 [cs.LG])

    [http://arxiv.org/abs/2309.00508](http://arxiv.org/abs/2309.00508)

    本论文通过分析两层神经网络在全局最小值附近的结构和梯度动力学，揭示了其泛化能力较强的原因。

    

    在温和的假设下，我们研究了两层神经网络在全局最小值附近的损失函数表面的结构，确定了能够实现完美泛化的参数集，并完整描述了其周围的梯度流动态。通过新颖的技术，我们揭示了复杂的损失函数表面的一些简单方面，并揭示了模型、目标函数、样本和初始化对训练动力学的不同影响。基于这些结果，我们还解释了为什么（过度参数化的）神经网络可以很好地泛化。

    Under mild assumptions, we investigate the structure of loss landscape of two-layer neural networks near global minima, determine the set of parameters which give perfect generalization, and fully characterize the gradient flows around it. With novel techniques, our work uncovers some simple aspects of the complicated loss landscape and reveals how model, target function, samples and initialization affect the training dynamics differently. Based on these results, we also explain why (overparametrized) neural networks could generalize well.
    
[^19]: 深度学习方法在电力系统监测与优化中的应用

    Application of Deep Learning Methods in Monitoring and Optimization of Electric Power Systems. (arXiv:2309.00498v1 [cs.LG])

    [http://arxiv.org/abs/2309.00498](http://arxiv.org/abs/2309.00498)

    该论文应用深度学习方法来提高电力系统监测与优化算法，具体包括使用图神经网络增强系统状态估计，以及利用强化学习进行动态配电网重构。

    

    本博士论文全面研究了利用深度学习技术来提高电力系统监测与优化算法的应用。该论文的第一个重要贡献是应用图神经网络来增强电力系统状态估计。该论文的第二个关键方面是利用强化学习进行动态配电网重构。通过大量的实验和仿真验证了所提出方法的有效性。

    This PhD thesis thoroughly examines the utilization of deep learning techniques as a means to advance the algorithms employed in the monitoring and optimization of electric power systems. The first major contribution of this thesis involves the application of graph neural networks to enhance power system state estimation. The second key aspect of this thesis focuses on utilizing reinforcement learning for dynamic distribution network reconfiguration. The effectiveness of the proposed methods is affirmed through extensive experimentation and simulations.
    
[^20]: 计算机断层扫描的多阶段深度学习伪影减少

    Multi-stage Deep Learning Artifact Reduction for Computed Tomography. (arXiv:2309.00494v1 [eess.IV])

    [http://arxiv.org/abs/2309.00494](http://arxiv.org/abs/2309.00494)

    本论文提出了一种多阶段深度学习伪影减少方法，用于提高计算机断层扫描的图像质量。传统方法通常在重建之后进行处理，而本方法能够根据不同的图像域进行多步骤去伪影，使得相对困难去除的伪影也能够有效消除。

    

    在计算机断层扫描中，通过一系列获取的投影图像计算出物体内部结构的图像。这些重建图像的质量对于准确分析至关重要，但是这种质量可能会被各种成像伪影降低。为了提高重建质量，获取的投影图像通常通过由多个去伪影步骤组成的流程进行处理，这些步骤应用于不同的图像域（例如，投影图像的异常值去除和重建图像的去噪）。这些伪影去除方法利用了某些伪影在特定域相对于其他域更容易去除的事实。最近，深度学习方法在计算机断层扫描伪影去除方面取得了有希望的结果。然而，大多数现有的计算机断层扫描深度学习方法都是在重建之后作为后处理方法应用的。因此，在重建域相对困难去除的伪影可能无法有效去除。

    In Computed Tomography (CT), an image of the interior structure of an object is computed from a set of acquired projection images. The quality of these reconstructed images is essential for accurate analysis, but this quality can be degraded by a variety of imaging artifacts. To improve reconstruction quality, the acquired projection images are often processed by a pipeline consisting of multiple artifact-removal steps applied in various image domains (e.g., outlier removal on projection images and denoising of reconstruction images). These artifact-removal methods exploit the fact that certain artifacts are easier to remove in a certain domain compared with other domains.  Recently, deep learning methods have shown promising results for artifact removal for CT images. However, most existing deep learning methods for CT are applied as a post-processing method after reconstruction. Therefore, artifacts that are relatively difficult to remove in the reconstruction domain may not be effec
    
[^21]: 预测对DRL技术在O-RAN切片中的收敛性有何影响？

    How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN Slicing?. (arXiv:2309.00489v1 [cs.NI])

    [http://arxiv.org/abs/2309.00489](http://arxiv.org/abs/2309.00489)

    本文研究了时间序列预测对在O-RAN切片中使用的DRL技术的收敛性的影响。

    

    沉浸式应用（如虚拟现实游戏和元宇宙服务）的成功取决于低延迟和可靠的连接。为了提供无缝的用户体验，开放式无线接入网络（O-RAN）架构和6G网络被期望发挥关键作用。O-RAN范式的关键组成部分之一，RAN切片可以根据沉浸式服务的需求分配网络资源，在单个物理基础设施上创建多个虚拟网络。在O-RAN文献中，深度强化学习（DRL）算法通常用于优化资源分配。然而，DRL在实际部署中的采用速度较慢。主要原因是DRL代理在初始部署时和网络条件发生显著变化时的收敛缓慢和性能不稳定。本文研究了时间序列预测对交通需求的收敛影响。

    The success of immersive applications such as virtual reality (VR) gaming and metaverse services depends on low latency and reliable connectivity. To provide seamless user experiences, the open radio access network (O-RAN) architecture and 6G networks are expected to play a crucial role. RAN slicing, a critical component of the O-RAN paradigm, enables network resources to be allocated based on the needs of immersive services, creating multiple virtual networks on a single physical infrastructure. In the O-RAN literature, deep reinforcement learning (DRL) algorithms are commonly used to optimize resource allocation. However, the practical adoption of DRL in live deployments has been sluggish. This is primarily due to the slow convergence and performance instabilities suffered by the DRL agents both upon initial deployment and when there are significant changes in network conditions. In this paper, we investigate the impact of time series forecasting of traffic demands on the convergence
    
[^22]: 几何感知的线图转换器预训练用于分子性质预测

    Geometry-aware Line Graph Transformer Pre-training for Molecular Property Prediction. (arXiv:2309.00483v1 [cs.LG])

    [http://arxiv.org/abs/2309.00483](http://arxiv.org/abs/2309.00483)

    本研究提出了一种几何感知的线图转换器（Galformer）预训练方法，用于增强分子表示学习。该方法结合2D和3D模态编码分子的拓扑和几何信息，并设计了互补的预训练任务。

    

    过去几年来，使用深度学习进行分子性质预测已经引起了广泛关注。由于标记分子的稀缺性，对于从无标签数据中学习泛化分子表示的自监督学习方法越来越受到关注。通常将分子视为二维拓扑图来建模，但是已经发现分子的三维几何对确定分子功能非常重要。在本文中，我们提出了几何感知的线图转换器（Galformer）预训练，这是一个新颖的自监督学习框架，旨在通过2D和3D模态增强分子表示学习。具体而言，我们首先设计了一个双模态线图转换器主干来编码分子的拓扑和几何信息。设计的主干结合了有效的结构编码，从两种模态捕捉图结构。然后，我们设计了两个互补的预训练任务

    Molecular property prediction with deep learning has gained much attention over the past years. Owing to the scarcity of labeled molecules, there has been growing interest in self-supervised learning methods that learn generalizable molecular representations from unlabeled data. Molecules are typically treated as 2D topological graphs in modeling, but it has been discovered that their 3D geometry is of great importance in determining molecular functionalities. In this paper, we propose the Geometry-aware line graph transformer (Galformer) pre-training, a novel self-supervised learning framework that aims to enhance molecular representation learning with 2D and 3D modalities. Specifically, we first design a dual-modality line graph transformer backbone to encode the topological and geometric information of a molecule. The designed backbone incorporates effective structural encodings to capture graph structures from both modalities. Then we devise two complementary pre-training tasks at 
    
[^23]: 分析持续学习者的新指标

    New metrics for analyzing continual learners. (arXiv:2309.00462v1 [cs.LG])

    [http://arxiv.org/abs/2309.00462](http://arxiv.org/abs/2309.00462)

    该论文提出了分析持续学习者的新指标，针对现有指标的局限性进行了分析，并提出解决了分类任务逐渐增加难度的问题。

    

    深度神经网络在从固定的类别集合中进行独立和同分布的数据训练时表现出了显著的性能。然而，在现实世界的场景中，对于连续流的数据进行模型训练可能是有益的，其中多个分类任务按顺序呈现。这种情况被称为持续学习（CL），对于标准学习算法来说，它们在学习新任务的同时难以保持旧任务的知识。这种稳定性与可塑性的困境仍然是持续学习的核心问题，已经提出了多种指标来充分衡量稳定性和可塑性。然而，没有一个指标考虑到分类任务的逐渐增加的难度，这从本质上导致任何模型的性能下降。在这方面，我们分析了当前指标的一些限制，并确定了设置引起的遗忘的存在。因此，我们提出了考虑任务逐渐增加难度的新指标。

    Deep neural networks have shown remarkable performance when trained on independent and identically distributed data from a fixed set of classes. However, in real-world scenarios, it can be desirable to train models on a continuous stream of data where multiple classification tasks are presented sequentially. This scenario, known as Continual Learning (CL) poses challenges to standard learning algorithms which struggle to maintain knowledge of old tasks while learning new ones. This stability-plasticity dilemma remains central to CL and multiple metrics have been proposed to adequately measure stability and plasticity separately. However, none considers the increasing difficulty of the classification task, which inherently results in performance loss for any model. In that sense, we analyze some limitations of current metrics and identify the presence of setup-induced forgetting. Therefore, we propose new metrics that account for the task's increasing difficulty. Through experiments on 
    
[^24]: 光学运动捕捉的基于局部性的神经求解器

    A Locality-based Neural Solver for Optical Motion Capture. (arXiv:2309.00428v1 [cs.GR])

    [http://arxiv.org/abs/2309.00428](http://arxiv.org/abs/2309.00428)

    该论文提出了一种基于局部性的神经求解器，用于清理和求解光学运动捕捉数据。论文通过构建异构图神经网络和使用特定的图卷积操作，提取标记和关节的局部特征，并转化为更准确的动作。通过研究标记的动作与其相邻标记的相关性，论文能够高效地填补缺失的标记，并通过分析加速度轮廓识别跟踪错误引起的异常标记。此外，论文还提出了基于表示学习和数据增强的训练机制，以进一步提高模型的准确性。

    

    我们提出了一种新的基于局部性的学习方法来清理和求解光学运动捕捉数据。给定噪声标记数据，我们提出了一种新的异构图神经网络，将标记和关节视为不同类型的节点，并使用图卷积操作提取标记和关节的局部特征，并将其转化为清晰的动作。为了处理异常标记（例如遮挡或具有较大的跟踪误差），关键洞察力在于标记的动作与其直接相邻的标记的动作之间存在很强的相关性，但与其他标记的相关性较小，即局部性，这使得我们能够有效地填补缺失的标记（例如由于遮挡引起的）。此外，我们还通过研究加速度轮廓来识别由于跟踪错误引起的异常标记。最后，我们提出了一个基于表示学习和数据增强的训练机制，通过在具有屏蔽的数据上训练模型。屏蔽方案旨在模拟遮挡和噪声等情况。

    We present a novel locality-based learning method for cleaning and solving optical motion capture data. Given noisy marker data, we propose a new heterogeneous graph neural network which treats markers and joints as different types of nodes, and uses graph convolution operations to extract the local features of markers and joints and transform them to clean motions. To deal with anomaly markers (e.g. occluded or with big tracking errors), the key insight is that a marker's motion shows strong correlations with the motions of its immediate neighboring markers but less so with other markers, a.k.a. locality, which enables us to efficiently fill missing markers (e.g. due to occlusion). Additionally, we also identify marker outliers due to tracking errors by investigating their acceleration profiles. Finally, we propose a training regime based on representation learning and data augmentation, by training the model on data with masking. The masking schemes aim to mimic the occluded and nois
    
[^25]: 使用约束逻辑编程进行解释性推理

    Declarative Reasoning on Explanations Using Constraint Logic Programming. (arXiv:2309.00422v1 [cs.AI])

    [http://arxiv.org/abs/2309.00422](http://arxiv.org/abs/2309.00422)

    这项研究提出了使用约束逻辑编程进行解释性推理的方法，可以为决策树提供声明性、交互式的解释，克服了当前解释方法中对背景知识的不充分结合和与用户的缺乏抽象和互动的问题。

    

    解释不透明的机器学习模型是一个日益重要的问题。当前的解释方法在人工智能（XAI）中存在一些缺点，包括对背景知识的不充分结合，以及与用户的缺乏抽象和互动。我们提出了基于约束逻辑编程（CLP）的REASONX解释方法。REASONX可以为决策树提供声明性的交互式解释，这些决策树可以是分析的机器学习模型或任何黑盒模型的全局/局部替代模型。用户可以使用线性约束和基于事实和对比实例的特征的MILP优化来表达背景知识或常识，并通过约束投影在不同抽象级别上与答案约束进行交互。我们在这里介绍了REASONX的架构，它由接近用户的Python层和CLP层组成。REASONX的核心执行引擎是一个具有声明性语义的Prolog元编程。

    Explaining opaque Machine Learning (ML) models is an increasingly relevant problem. Current explanation in AI (XAI) methods suffer several shortcomings, among others an insufficient incorporation of background knowledge, and a lack of abstraction and interactivity with the user. We propose REASONX, an explanation method based on Constraint Logic Programming (CLP). REASONX can provide declarative, interactive explanations for decision trees, which can be the ML models under analysis or global/local surrogate models of any black-box model. Users can express background or common sense knowledge using linear constraints and MILP optimization over features of factual and contrastive instances, and interact with the answer constraints at different levels of abstraction through constraint projection. We present here the architecture of REASONX, which consists of a Python layer, closer to the user, and a CLP layer. REASONX's core execution engine is a Prolog meta-program with declarative seman
    
[^26]: 条件生存预测中的面积规范COBRA

    Area-norm COBRA on Conditional Survival Prediction. (arXiv:2309.00417v1 [cs.LG])

    [http://arxiv.org/abs/2309.00417](http://arxiv.org/abs/2309.00417)

    本文提出了一种基于组合回归的条件生存预测方法，其中使用面积作为相似度度量，通过选择最重要的变量来提高模型性能。

    

    本文探讨了一种不同的组合回归策略来计算条件生存函数。我们使用基于回归的弱学习器来创建所提出的集成技术。所提出的组合回归策略使用相似度度量作为两个生存曲线之间的面积。所提出的模型表明其表现优于随机生存森林。本文讨论了一种在组合回归设置中选择最重要变量的新技术。我们进行了一项模拟研究，表明我们对变量相关性的提议效果很好。我们还使用了三个真实数据集来说明该模型。

    The paper explores a different variation of combined regression strategy to calculate the conditional survival function. We use regression based weak learners to create the proposed ensemble technique. The proposed combined regression strategy uses proximity measure as area between two survival curves. The proposed model shows a construction which ensures that it performs better than the Random Survival Forest. The paper discusses a novel technique to select the most important variable in the combined regression setup. We perform a simulation study to show that our proposition for finding relevance of the variables works quite well. We also use three real-life datasets to illustrate the model.
    
[^27]: 推进个性化联邦学习：团体隐私、公平性等方面的突破

    Advancing Personalized Federated Learning: Group Privacy, Fairness, and Beyond. (arXiv:2309.00416v1 [cs.LG])

    [http://arxiv.org/abs/2309.00416](http://arxiv.org/abs/2309.00416)

    本研究在个性化、隐私保证和公平性之间解决了联邦学习模型的三元交互作用。差分隐私及其变体被应用为提供正式隐私保证的前沿标准。在多样化的数据集中寻求公平性变得重要。

    

    联邦学习 (FL) 是一种在分布式和协作方式下训练机器学习模型的框架。在训练过程中，一组参与的客户端处理本地存储的数据，仅共享通过最小化其本地输入的成本函数获得的模型更新。FL被提出作为隐私保护机器学习的一种途径，但已被证明易受私人信息泄露、模型个性化缺失以及可能导致某些群体比其他群体更公平的训练模型等问题的影响。在本文中，我们解决了在FL框架中训练的模型在个性化、隐私保证和公平性之间的三元交互作用。差分隐私及其变体已被研究和应用为提供正式隐私保证的前沿标准。然而，FL中的客户端往往拥有非常多样化的数据集，代表着异质的社区，这使得保证公平性变得重要。

    Federated learning (FL) is a framework for training machine learning models in a distributed and collaborative manner. During training, a set of participating clients process their data stored locally, sharing only the model updates obtained by minimizing a cost function over their local inputs. FL was proposed as a stepping-stone towards privacy-preserving machine learning, but it has been shown vulnerable to issues such as leakage of private information, lack of personalization of the model, and the possibility of having a trained model that is fairer to some groups than to others. In this paper, we address the triadic interaction among personalization, privacy guarantees, and fairness attained by models trained within the FL framework. Differential privacy and its variants have been studied and applied as cutting-edge standards for providing formal privacy guarantees. However, clients in FL often hold very diverse datasets representing heterogeneous communities, making it important 
    
[^28]: 选择性场景文本去除

    Selective Scene Text Removal. (arXiv:2309.00410v1 [cs.CV])

    [http://arxiv.org/abs/2309.00410](http://arxiv.org/abs/2309.00410)

    本论文提出了一种选择性场景文本去除（SSTR）任务，该任务可以根据用户指定的目标词汇来去除场景图像中的文本，实验结果表明所提出的方法有效地实现了目标词汇的去除。

    

    场景文本去除（Scene text removal，STR）是一种图像转换任务，用于去除场景图像中的文本区域。传统的STR方法会删除所有的场景文本。这意味着现有的方法无法选择要删除的文本。本文提出了一种名为选择性场景文本去除（Selective scene text removal，SSTR）的新任务设置，只删除用户指定的目标词汇。虽然SSTR比STR更复杂，但是我们提出的多模块结构使得SSTR的训练更加高效。实验结果表明，所提出的方法可以如预期地去除目标词汇。

    Scene text removal (STR) is the image transformation task to remove text regions in scene images. The conventional STR methods remove all scene text. This means that the existing methods cannot select text to be removed. In this paper, we propose a novel task setting named selective scene text removal (SSTR) that removes only target words specified by the user. Although SSTR is a more complex task than STR, the proposed multi-module structure enables efficient training for SSTR. Experimental results show that the proposed method can remove target words as expected.
    
[^29]: 提升基于AND/OR的计算蛋白质设计：动态启发式和可泛化的UFO

    Boosting AND/OR-Based Computational Protein Design: Dynamic Heuristics and Generalizable UFO. (arXiv:2309.00408v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.00408](http://arxiv.org/abs/2309.00408)

    该研究专注于提升蛋白质重设计算法AOBB-K*的扩展性，通过引入增强版、带有动态启发式和带有下溢优化的新版本，显著提升了其可扩展性。

    

    科学计算在神经网络等技术的推动下取得了飞速发展。然而，某些重要任务对于这些技术来说并不适用，因此需要对传统推理方案进行创新。其中一个任务就是蛋白质重设计。最近引入了一种新的重设计算法AOBB-K*，在小规模蛋白质重设计问题上与最先进的BBK*算法具有竞争力。然而，AOBB-K*在扩展性上表现不佳。在本研究中，我们专注于扩展AOBB-K*算法，并引入了三个新版本：AOBB-K*-b（增强版）、AOBB-K*-DH（带有动态启发式）和AOBB-K*-UFO（带有下溢优化），显著提升了扩展性。

    Scientific computing has experienced a surge empowered by advancements in technologies such as neural networks. However, certain important tasks are less amenable to these technologies, benefiting from innovations to traditional inference schemes. One such task is protein re-design. Recently a new re-design algorithm, AOBB-K*, was introduced and was competitive with state-of-the-art BBK* on small protein re-design problems. However, AOBB-K* did not scale well. In this work we focus on scaling up AOBB-K* and introduce three new versions: AOBB-K*-b (boosted), AOBB-K*-DH (with dynamic heuristics), and AOBB-K*-UFO (with underflow optimization) that significantly enhance scalability.
    
[^30]: 用排序不变的编码器和更紧的变分边界学习多模态生成模型

    Learning multi-modal generative models with permutation-invariant encoders and tighter variational bounds. (arXiv:2309.00380v1 [stat.ML])

    [http://arxiv.org/abs/2309.00380](http://arxiv.org/abs/2309.00380)

    本文提出了一种用于多模态数据的深度潜变量模型，并开发了更灵活的编码特征聚合方案，能够紧密地下界数据对数似然。

    

    设计用于多模态数据的深度潜变量模型一直是机器学习研究中的一个重要主题。多模态变分自编码器 (VAE) 是一种常用的生成模型类别，它学习能够共同解释多种模态的潜在表示。各种客观函数已被提出用于这样的模型，往往以多模态数据对数似然的下界以及信息论方面的考虑为动机。为了对不同模态子集进行编码，我们经常使用并展示了产品型专家 (PoE) 或者混合型专家 (MoE) 聚合方案，这些方案在生成质量或者多模态一致性等方面具有不同的权衡。在本研究中，我们考虑了一个能够紧密地下界数据对数似然的变分边界。我们通过将不同模态的编码特征组合起来，开发了更灵活的聚合方案，这些方案推广了 PoE 或者 MoE 方法。

    Devising deep latent variable models for multi-modal data has been a long-standing theme in machine learning research. Multi-modal Variational Autoencoders (VAEs) have been a popular generative model class that learns latent representations which jointly explain multiple modalities. Various objective functions for such models have been suggested, often motivated as lower bounds on the multi-modal data log-likelihood or from information-theoretic considerations. In order to encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely used and shown to yield different trade-offs, for instance, regarding their generative quality or consistency across multiple modalities. In this work, we consider a variational bound that can tightly lower bound the data log-likelihood. We develop more flexible aggregation schemes that generalise PoE or MoE approaches by combining encoded features from different modali
    
[^31]: 基于风险估计的半监督分类异常检测方法

    Anomaly detection with semi-supervised classification based on risk estimators. (arXiv:2309.00379v1 [cs.LG])

    [http://arxiv.org/abs/2309.00379](http://arxiv.org/abs/2309.00379)

    基于风险估计的半监督分类异常检测方法克服了单类分类异常检测中对未标记训练数据只包含正常实例的假设，并提出了两种新型检测方法，实验证明了其有效性。

    

    单类分类异常检测方法的一个显著限制是其依赖于未标记的训练数据只包含正常实例的假设。为了克服这个不现实的假设，我们提出了两种基于分类的新型异常检测方法。首先，我们介绍了一种基于无偏风险估计器的半监督浅层异常检测方法。其次，我们提出了一种基于非负（有偏）风险估计器的半监督深层异常检测方法。我们建立了对于两个风险最小化算法的估计误差界和超出风险界。此外，我们提出了选择适当的正则化参数的技术，以确保浅层模型下经验风险的非负性在特定损失函数下成立。我们广泛的实验提供了对于基于风险的异常检测方法有效性的强有力证据。

    A significant limitation of one-class classification anomaly detection methods is their reliance on the assumption that unlabeled training data only contains normal instances. To overcome this impractical assumption, we propose two novel classification-based anomaly detection methods. Firstly, we introduce a semi-supervised shallow anomaly detection method based on an unbiased risk estimator. Secondly, we present a semi-supervised deep anomaly detection method utilizing a nonnegative (biased) risk estimator. We establish estimation error bounds and excess risk bounds for both risk minimizers. Additionally, we propose techniques to select appropriate regularization parameters that ensure the nonnegativity of the empirical risk in the shallow model under specific loss functions. Our extensive experiments provide strong evidence of the effectiveness of the risk-based anomaly detection methods.
    
[^32]: 长程图表基准的重新评估：差距去哪儿了？

    Where Did the Gap Go? Reassessing the Long-Range Graph Benchmark. (arXiv:2309.00367v1 [cs.LG])

    [http://arxiv.org/abs/2309.00367](http://arxiv.org/abs/2309.00367)

    本文对长程图表基准（LRGB）进行了重新评估，通过严格的实证分析发现，先前的报告性能差距被高估了，而经过基本超参数优化后，差距完全消失。此外，我们还讨论了特征归一化的缺失和链接预测度量的虚假实现对LRGB的影响。

    

    最近的长程图表基准(LRGB，Dwivedi等，2022)引入了一组与顶点之间的长程相互作用密切相关的图表学习任务。经验证据表明，在这些任务中，图形变换器明显优于消息传递GNN（MPGNN）。在本文中，我们对LRGB上的多个MPGNN基线以及图形变换器GPS（Ramp\'a\v{s}ek等，2022）进行了仔细重新评估。通过严格的实证分析，我们证明了由于子优超参数选择不当而高估了报告的性能差距。值得注意的是，在基本超参数优化后，跨多个数据集，性能差距完全消失。此外，我们还讨论了LRGB的视觉数据集缺乏特征归一化的影响，并突出了LRGB的链接预测度量的虚假实现。我们的论文的主要目标是在图机器学习社区建立更高的实证严谨性标准。

    The recent Long-Range Graph Benchmark (LRGB, Dwivedi et al. 2022) introduced a set of graph learning tasks strongly dependent on long-range interaction between vertices. Empirical evidence suggests that on these tasks Graph Transformers significantly outperform Message Passing GNNs (MPGNNs). In this paper, we carefully reevaluate multiple MPGNN baselines as well as the Graph Transformer GPS (Ramp\'a\v{s}ek et al. 2022) on LRGB. Through a rigorous empirical analysis, we demonstrate that the reported performance gap is overestimated due to suboptimal hyperparameter choices. It is noteworthy that across multiple datasets the performance gap completely vanishes after basic hyperparameter optimization. In addition, we discuss the impact of lacking feature normalization for LRGB's vision datasets and highlight a spurious implementation of LRGB's link prediction metric. The principal aim of our paper is to establish a higher standard of empirical rigor within the graph machine learning commun
    
[^33]: 可解释的主动学习用于偏好获取

    Explainable Active Learning for Preference Elicitation. (arXiv:2309.00356v1 [cs.LG])

    [http://arxiv.org/abs/2309.00356](http://arxiv.org/abs/2309.00356)

    本研究关注于冷启动问题中的偏好获取，在该问题中，推荐系统缺乏用户存在或访问其他用户数据受限。我们采用可解释的主动学习方法，通过最小化用户工作量最大化信息获取，并在偏好获取过程中采用无监督、半监督和监督机器学习方法。

    

    深入了解新用户的偏好，并随后个性化推荐，需要智能地处理用户交互，即提出相关问题以有效获取有价值的信息。在本研究中，我们关注的是冷启动问题的特定情景，在该情景中，推荐系统缺乏足够的用户存在或访问其他用户数据受限，阻碍了利用系统中现有数据的用户建模方法。我们采用主动学习(AL)来解决这个问题，目标是在最小用户工作量的情况下最大化信息获取。AL从一个大型无标签集合中选择信息丰富的数据向询问预测标签，并最终更新机器学习(ML)模型。我们在解释性偏好获取过程中采用了无监督、半监督和监督ML的集成过程。它利用用户对系统返回推荐的反馈（给予系统的注意或喜好）和用户对问题的反馈向他们解释和辅助保持用户满意度和参与度。

    Gaining insights into the preferences of new users and subsequently personalizing recommendations necessitate managing user interactions intelligently, namely, posing pertinent questions to elicit valuable information effectively. In this study, our focus is on a specific scenario of the cold-start problem, where the recommendation system lacks adequate user presence or access to other users' data is restricted, obstructing employing user profiling methods utilizing existing data in the system. We employ Active Learning (AL) to solve the addressed problem with the objective of maximizing information acquisition with minimal user effort. AL operates for selecting informative data from a large unlabeled set to inquire an oracle to label them and eventually updating a machine learning (ML) model. We operate AL in an integrated process of unsupervised, semi-supervised, and supervised ML within an explanatory preference elicitation process. It harvests user feedback (given for the system's 
    
[^34]: 私人定制纳米颗粒合成与化学知识发现的自动实验

    Bespoke Nanoparticle Synthesis and Chemical Knowledge Discovery Via Autonomous Experimentations. (arXiv:2309.00349v1 [physics.chem-ph])

    [http://arxiv.org/abs/2309.00349](http://arxiv.org/abs/2309.00349)

    该论文报道了一种自动实验平台，用于定制设计具有目标光学性质的纳米颗粒。通过贝叶斯优化器，能够在仅200次迭代内高效地合成具有所需吸收光谱的银纳米颗粒，并揭示了涉及柠檬酸盐效应的新化学反应。

    

    优化使用多个合成变量的纳米材料合成被认为是一项极其繁重的任务，因为传统的组合探索方法非常昂贵。在这项工作中，我们报道了一个自动实验平台，用于定制设计具有目标光学性质的纳米颗粒（NPs）。该平台在纳米颗粒批量合成模块和UV-Vis光谱模块之间以闭环方式运行，根据AI优化建模的反馈。以银（Ag）纳米颗粒为代表性示例，我们证明了实现早停止准则的贝叶斯优化器能够在仅200次迭代（在五种合成试剂中进行优化）内高效地产生具有所需吸收光谱的Ag纳米颗粒。除了材料开发效率卓越之外，对合成变量的分析还揭示了涉及柠檬酸盐效应的新化学反应。

    The optimization of nanomaterial synthesis using numerous synthetic variables is considered to be extremely laborious task because the conventional combinatorial explorations are prohibitively expensive. In this work, we report an autonomous experimentation platform developed for the bespoke design of nanoparticles (NPs) with targeted optical properties. This platform operates in a closed-loop manner between a batch synthesis module of NPs and a UV- Vis spectroscopy module, based on the feedback of the AI optimization modeling. With silver (Ag) NPs as a representative example, we demonstrate that the Bayesian optimizer implemented with the early stopping criterion can efficiently produce Ag NPs precisely possessing the desired absorption spectra within only 200 iterations (when optimizing among five synthetic reagents). In addition to the outstanding material developmental efficiency, the analysis of synthetic variables further reveals a novel chemistry involving the effects of citrate
    
[^35]: 冠状动脉CT血管造影患者风险分层和下一步预测的多任务深度学习

    Multitask Deep Learning for Accurate Risk Stratification and Prediction of Next Steps for Coronary CT Angiography Patients. (arXiv:2309.00330v1 [cs.LG])

    [http://arxiv.org/abs/2309.00330](http://arxiv.org/abs/2309.00330)

    本研究提出了一种多任务深度学习模型，可以支持冠状动脉CT血管造影患者的风险分层和下一步测试选择。在真实世界的数据集上，该模型在风险分层和预测下游测试方面取得了较高的准确性。

    

    诊断性调查对疑似和已确诊冠状动脉疾病（CAD）的患者的风险分层和临床决策具有重要作用。然而，现有工具大多仅专注于选择门诊测试，而只有少数系统包含有关下游测试或治疗的信息。我们提出了一种多任务深度学习模型，用于支持冠状动脉计算机断层扫描（CCTA）患者的风险分层和下游测试选择。分析包括在2006年至2017年间进行CCTA检查的14,021名患者。我们的新型多任务深度学习框架扩展了最先进的感知器模型，以处理真实世界的CCTA报告数据。我们的模型在CAD风险分层中实现了0.76的接收器操作特征曲线下面积（AUC），在预测下游测试中实现了0.72的AUC。我们提出的深度学习模型可以准确估计患者需要进行下游测试的可能性。

    Diagnostic investigation has an important role in risk stratification and clinical decision making of patients with suspected and documented Coronary Artery Disease (CAD). However, the majority of existing tools are primarily focused on the selection of gatekeeper tests, whereas only a handful of systems contain information regarding the downstream testing or treatment. We propose a multi-task deep learning model to support risk stratification and down-stream test selection for patients undergoing Coronary Computed Tomography Angiography (CCTA). The analysis included 14,021 patients who underwent CCTA between 2006 and 2017. Our novel multitask deep learning framework extends the state-of-the art Perceiver model to deal with real-world CCTA report data. Our model achieved an Area Under the receiver operating characteristic Curve (AUC) of 0.76 in CAD risk stratification, and 0.72 AUC in predicting downstream tests. Our proposed deep learning model can accurately estimate the likelihood o
    
[^36]: Mi-Go: 使用YouTube作为数据源的测试框架，用于评估像OpenAI的Whisper这样的语音识别模型

    Mi-Go: Test Framework which uses YouTube as Data Source for Evaluating Speech Recognition Models like OpenAI's Whisper. (arXiv:2309.00329v1 [cs.SD])

    [http://arxiv.org/abs/2309.00329](http://arxiv.org/abs/2309.00329)

    Mi-Go is a testing framework that uses YouTube as a data source to evaluate speech recognition models like OpenAI's Whisper. It leverages diverse real-world scenarios, multiple languages and accents, and compares machine-generated transcriptions with human-made subtitles to identify potential misuse of YouTube subtitles.

    

    本文介绍了Mi-Go，一种新颖的测试框架，旨在评估通用语音识别机器学习模型在多样真实场景中的性能和适应性。该框架利用YouTube作为丰富且持续更新的数据源，涵盖多种语言、口音、方言、说话风格和音频质量水平。为了证明该框架的有效性，本文以OpenAI开发的Whisper模型作为测试对象。测试涉及使用124个YouTube视频来测试所有Whisper模型的版本。结果强调了YouTube作为语音识别模型宝贵的测试平台的实用性，确保其对各种语言和声学条件的健壮性、准确性和适应性。此外，通过对比机器生成的转录与人工制作的字幕，Mi-Go框架可以帮助发现可能的YouTube字幕滥用，如搜索引擎优化。

    This article introduces Mi-Go, a novel testing framework aimed at evaluating the performance and adaptability of general-purpose speech recognition machine learning models across diverse real-world scenarios. The framework leverages YouTube as a rich and continuously updated data source, accounting for multiple languages, accents, dialects, speaking styles, and audio quality levels. To demonstrate the effectiveness of the framework, the Whisper model, developed by OpenAI, was employed as a test object. The tests involve using a total of 124 YouTube videos to test all Whisper model versions. The results underscore the utility of YouTube as a valuable testing platform for speech recognition models, ensuring their robustness, accuracy, and adaptability to diverse languages and acoustic conditions. Additionally, by contrasting the machine-generated transcriptions against human-made subtitles, the Mi-Go framework can help pinpoint potential misuse of YouTube subtitles, like Search Engine Op
    
[^37]: 多层次减小阶次代理建模

    Multi-fidelity reduced-order surrogate modeling. (arXiv:2309.00325v1 [cs.LG])

    [http://arxiv.org/abs/2309.00325](http://arxiv.org/abs/2309.00325)

    本论文提出了一种结合降维和多保真度神经网络代理的数据驱动策略，以解决在高保真度数据有限或稀缺的情况下，低保真度模型无法准确捕捉到高保真度模型中观察到的不稳定性和临界瞬变的问题。

    

    在计算预算有限的情况下，高保真度的部分微分方程数值模拟可以显著限制所考虑的参数配置数量和/或对给定系统建模的时间窗口评估。多层次代理建模旨在利用计算成本低廉但不太准确的低保真度模型，以提高在高保真度数据有限或稀缺的情况下的预测准确性。然而，低保真度模型通常无法准确捕捉到高保真度模型中观察到的不稳定性和临界瞬变的发生，使其作为代理模型不实用。为了解决这个问题，我们提出了一种新的数据驱动策略，将降维和多保真度神经网络代理相结合。关键思想是通过将经典的适应值分解（POD）应用于高保真度模型来生成空间基准。

    High-fidelity numerical simulations of partial differential equations (PDEs) given a restricted computational budget can significantly limit the number of parameter configurations considered and/or time window evaluated for modeling a given system. Multi-fidelity surrogate modeling aims to leverage less accurate, lower-fidelity models that are computationally inexpensive in order to enhance predictive accuracy when high-fidelity data are limited or scarce. However, low-fidelity models, while often displaying important qualitative spatio-temporal features, fail to accurately capture the onset of instability and critical transients observed in the high-fidelity models, making them impractical as surrogate models. To address this shortcoming, we present a new data-driven strategy that combines dimensionality reduction with multi-fidelity neural network surrogates. The key idea is to generate a spatial basis by applying the classical proper orthogonal decomposition (POD) to high-fidelity s
    
[^38]: 高效的材料科学模拟代理模型：基于机器学习的微结构属性预测

    Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties. (arXiv:2309.00305v1 [cs.LG])

    [http://arxiv.org/abs/2309.00305](http://arxiv.org/abs/2309.00305)

    本研究通过应用机器学习技术，开发并研究了两个不同领域的数据集，用于高效预测材料科学中的微结构属性。

    

    在化学、生物学、气象学、物理学、工程学和材料科学等许多科学领域中，确定、理解和预测所谓的结构-性质关系是一项重要任务。结构指的是物质、材料或物质的空间分布，而性质是一种结果特性，通常以非平凡的方式取决于结构的空间细节。传统上，这些任务通常采用正向模拟模型进行。最近，在这些科学领域中，已经应用了几种机器学习算法来增强和加速模拟模型，或作为代理模型。在本研究中，我们开发和研究了基于两个不同材料科学领域的数据集的六种机器学习技术的应用：用于预测磁性域形成的二维灰度模型的数据，以及表示双相微结构演变的数据。

    Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures fro
    
[^39]: 基于激光雷达的端到端强化学习用于自主赛车

    End-to-end Lidar-Driven Reinforcement Learning for Autonomous Racing. (arXiv:2309.00296v1 [cs.RO])

    [http://arxiv.org/abs/2309.00296](http://arxiv.org/abs/2309.00296)

    本研究提出了一种基于激光雷达的端到端强化学习方法，用于解决在汽车赛车领域中复杂环境下的自主导航问题，并通过实验验证了该方法的可行性和潜力。

    

    强化学习已经成为自动化和机器人领域的一种革命性方法，为传统方法难以解决的复杂问题提供了强大的解决方案。在问题定义模糊且难以量化的情况下，学习为基础的解决方案如强化学习尤其有价值。本研究旨在开发和训练一个利用前馈原始激光雷达和速度数据在模拟环境中进行自主导航的强化学习智能体。经过在模拟环境中训练后，该智能体的性能在真实赛车场景中进行实验评估。这项研究突出了强化学习算法在增强自主赛车性能方面的可行性和潜在好处，特别是在先前地图信息有限的环境中。

    Reinforcement Learning (RL) has emerged as a transformative approach in the domains of automation and robotics, offering powerful solutions to complex problems that conventional methods struggle to address. In scenarios where the problem definitions are elusive and challenging to quantify, learning-based solutions such as RL become particularly valuable. One instance of such complexity can be found in the realm of car racing, a dynamic and unpredictable environment that demands sophisticated decision-making algorithms. This study focuses on developing and training an RL agent to navigate a racing environment solely using feedforward raw lidar and velocity data in a simulated context. The agent's performance, trained in the simulation environment, is then experimentally evaluated in a real-world racing scenario. This exploration underlines the feasibility and potential benefits of RL algorithm enhancing autonomous racing performance, especially in the environments where prior map inform
    
[^40]: RLAIF: 使用AI反馈来扩展强化学习从人类反馈中学习

    RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v1 [cs.CL])

    [http://arxiv.org/abs/2309.00267](http://arxiv.org/abs/2309.00267)

    RLAIF是一种新的强化学习方法，利用AI反馈代替人类标注偏好，相比强化学习从人类反馈中学习（RLHF），在摘要任务上取得了类似的改进效果，并且在人类评估中得到了相同的认可。这提供了一种有潜力解决RLHF的可扩展性限制的解决方案。

    

    从人类反馈中进行强化学习（RLHF）对于将大型语言模型（LLMs）与人类偏好相一致是有效的，但是收集高质量的人类偏好标签是一个关键瓶颈。我们比较了RLHF和利用现成的LLM进行标记的RL from AI Feedback (RLAIF)技术，并发现它们都能获得类似的改善效果。在摘要任务上，人类评估者在约70%的案例中都更喜欢RLAIF和RLHF产生的文本，而不是基准的监督微调模型。此外，当被要求评估RLAIF和RLHF的摘要时，人类以相同的比率更喜欢两者。这些结果表明，RLAIF可以达到人类水平的性能，为克服RLHF的可扩展性限制提供了潜在的解决方案。

    Reinforcement learning from human feedback (RLHF) is effective at aligning large language models (LLMs) to human preferences, but gathering high quality human preference labels is a key bottleneck. We conduct a head-to-head comparison of RLHF vs. RL from AI Feedback (RLAIF) - a technique where preferences are labeled by an off-the-shelf LLM in lieu of humans, and we find that they result in similar improvements. On the task of summarization, human evaluators prefer generations from both RLAIF and RLHF over a baseline supervised fine-tuned model in ~70% of cases. Furthermore, when asked to rate RLAIF vs. RLHF summaries, humans prefer both at equal rates. These results suggest that RLAIF can yield human-level performance, offering a potential solution to the scalability limitations of RLHF.
    
[^41]: 利用学习度量改进联邦学习

    Leveraging Learning Metrics for Improved Federated Learning. (arXiv:2309.00257v1 [cs.LG])

    [http://arxiv.org/abs/2309.00257](http://arxiv.org/abs/2309.00257)

    本论文将联邦学习与学习度量方法相结合，提出了首个联邦学习度量聚合方法，并证明了有效秩适用于联邦问题，并通过开发新型权重聚合方案取得了优于基准的联邦平均效果。

    

    目前在联邦学习环境中，没有学习方案利用可解释的人工智能（XAI）中的新学习度量，尤其是有助于确定模型学习情况的新学习度量。其中一种新的学习度量被称为“有效秩”（ER），它衡量矩阵奇异值的香农熵，从而确定层的映射效果。通过结合联邦学习和学习度量方法，本研究将(1)提供第一个联邦学习度量聚合方法，(2)证明有效秩适用于联邦问题，优于基准的联邦平均方法，(3)开发一种依赖有效秩的新型权重聚合方案。

    Currently in the federated setting, no learning schemes leverage the emerging research of explainable artificial intelligence (XAI) in particular the novel learning metrics that help determine how well a model is learning. One of these novel learning metrics is termed `Effective Rank' (ER) which measures the Shannon Entropy of the singular values of a matrix, thus enabling a metric determining how well a layer is mapping. By joining federated learning and the learning metric, effective rank, this work will \textbf{(1)} give the first federated learning metric aggregation method \textbf{(2)} show that effective rank is well-suited to federated problems by out-performing baseline Federated Averaging \cite{konevcny2016federated} and \textbf{(3)} develop a novel weight-aggregation scheme relying on effective rank.
    
[^42]: SortedNet，每个网络都有自己的位置：面向训练多对一神经网络的广义解决方案

    SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])

    [http://arxiv.org/abs/2309.00255](http://arxiv.org/abs/2309.00255)

    SortedNet是一种广义解决方案，通过排序训练和概率方式，在深度神经网络的各个维度上实现高效动态推断。这种方法允许在模型推断过程中灵活适应计算负载，并且可以将子网络的数量扩展到数百个。

    

    随着深度学习模型的规模不断增大，如何在内存和计算约束下找到最优模型变得越来越重要。虽然神经网络的架构和组成部分通常允许以模块化的方式使用，但它们的训练过程并不意识到这种模块化。因此，传统的神经网络训练缺乏在推断过程中适应模型计算负载的灵活性。本文提出了SortedNet，这是一种广义且可扩展的解决方案，用于利用深度神经网络在各个维度上的内在模块化特性，实现高效的动态推断。我们的训练方法采用了一种嵌套结构的子模型和主模型共享参数的方式，并以排序和概率的方式训练它们。这种子网络的排序训练使我们能够在一轮训练中将子网络的数量扩展到数百个。我们利用一种新颖的更新方案在推断过程中动态调整子网络的计算负载。

    As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during 
    
[^43]: 为什么通用对抗性攻击对大型语言模型有效：几何可能是答案

    Why do universal adversarial attacks work on large language models?: Geometry might be the answer. (arXiv:2309.00254v1 [cs.LG])

    [http://arxiv.org/abs/2309.00254](http://arxiv.org/abs/2309.00254)

    本研究从几何视角解释了对大型语言模型的通用对抗性攻击，发现通用对抗性触发器可能是嵌入向量，仅仅近似了其对抗训练区域中的语义信息。

    

    基于Transformer的大型语言模型具有新发能力，在社会中越来越普遍。然而，在对抗攻击的背景下，理解和解释它们的内部工作仍然基本未解决。已经证明基于梯度的通用对抗性攻击对大型语言模型非常有效，由于它们对输入不敏感的特性，可能具有潜在的危险。本研究提出了一个新颖的几何视角，解释了对大型语言模型的通用对抗性攻击。通过对攻击117M参数的GPT-2模型，我们发现证据表明通用对抗性触发器可能是嵌入向量，仅仅近似了其对抗训练区域中的语义信息。这个假设得到了通过白盒模型分析的支持，包括对隐藏表示的降维和相似度测量。我们相信这种关于驱动通用对抗性攻击的潜在机制的新几何视角。

    Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving univer
    
[^44]: 使用自注意力变换器的可解释医学图像诊断：一个关于可解释人工智能在医疗保健中的综述

    Interpretable Medical Imagery Diagnosis with Self-Attentive Transformers: A Review of Explainable AI for Health Care. (arXiv:2309.00252v1 [cs.CV])

    [http://arxiv.org/abs/2309.00252](http://arxiv.org/abs/2309.00252)

    本文综述了最近ViT的进展以及对其决策过程的解释性方法，为医学诊断应用提供透明度。

    

    最近人工智能的进展促使其在初级医疗服务中得到广泛应用，解决了医疗保健中的供需不平衡问题。视觉变换器（ViT）作为最先进的计算机视觉模型出现，受益于自注意力模块。然而，与传统的机器学习方法相比，深度学习模型复杂且常常被看作是一个“黑盒子”，这可能导致对其运作方式的不确定性。可解释的人工智能（XAI）是指解释和解读机器学习模型内部运作方式和决策过程的方法，这在医学领域尤为重要，以指导医疗决策过程。本综述总结了最近ViT的进展和解释性方法，以理解ViT的决策过程，实现医学诊断应用的透明化。

    Recent advancements in artificial intelligence (AI) have facilitated its widespread adoption in primary medical services, addressing the demand-supply imbalance in healthcare. Vision Transformers (ViT) have emerged as state-of-the-art computer vision models, benefiting from self-attention modules. However, compared to traditional machine-learning approaches, deep-learning models are complex and are often treated as a "black box" that can cause uncertainty regarding how they operate. Explainable Artificial Intelligence (XAI) refers to methods that explain and interpret machine learning models' inner workings and how they come to decisions, which is especially important in the medical domain to guide the healthcare decision-making process. This review summarises recent ViT advancements and interpretative approaches to understanding the decision-making process of ViT, enabling transparency in medical diagnosis applications.
    
[^45]: NeuroSurgeon: 一种用于子网络分析的工具包

    NeuroSurgeon: A Toolkit for Subnetwork Analysis. (arXiv:2309.00244v1 [cs.LG])

    [http://arxiv.org/abs/2309.00244](http://arxiv.org/abs/2309.00244)

    NeuroSurgeon是一个用于在Huggingface Transformers库中发现和操作模型子网络的Python工具包，可以推进对神经网络学习算法的理解。

    

    尽管在可解释性领域取得了一些进展，但我们对神经网络学习表示的算法仍知之甚少。最近的研究尝试通过将已训练模型分解为功能电路来理解它们(参考Csord\'as等人的研究，2020；Lepori等人，2023)。为了推进这项研究，我们开发了NeuroSurgeon，这是一个Python库，可以用于发现和操作Huggingface Transformers库中的模型中的子网络(Wolf等人，2019)。NeuroSurgeon可以在https://github.com/mlepori1/NeuroSurgeon 免费获取。

    Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord\'as et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.
    
[^46]: 图像劫持：对抗性图像能在运行时控制生成模型

    Image Hijacking: Adversarial Images can Control Generative Models at Runtime. (arXiv:2309.00236v1 [cs.LG])

    [http://arxiv.org/abs/2309.00236](http://arxiv.org/abs/2309.00236)

    本研究发现对抗性图像能够在运行时控制生成模型，并提出了通用的方法来创建图像劫持。通过研究三种攻击类型，我们发现这些攻击对最新的视觉语言模型具有高达90％以上的成功率。该研究引发了对基础模型安全性的严重担忧。

    

    基础模型是否能够免受恶意行为者的攻击？本文研究了视觉语言模型（VLM）的图像输入。我们发现了图像劫持，即能够在运行时控制生成模型的对抗性图像。我们引入了一种名为“行为匹配”的通用方法来创建图像劫持，并用它来探索三种类型的攻击：具体字符串攻击可以生成任意被攻击者选择的输出；泄露上下文攻击可以将上下文窗口中的信息泄露到输出中；越狱攻击可以绕过模型的安全训练。我们对基于CLIP和LLaMA-2的最新VLM模型LLaVA-2进行了这些攻击的研究，并发现我们所有的攻击类型成功率均在90％以上。而且，我们的攻击是自动化的，只需要对图像进行小的扰动。这些发现对基础模型的安全性提出了严重的担忧。如果图像劫持与CIFAR-10中的对抗性样本一样难以防御，那么可能需要很多年才能找到解决方案。

    Are foundation models secure from malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control generative models at runtime. We introduce Behavior Matching, a general method for creating image hijacks, and we use it to explore three types of attacks. Specific string attacks generate arbitrary output of the adversary's choosing. Leak context attacks leak information from the context window into the output. Jailbreak attacks circumvent a model's safety training. We study these attacks against LLaVA-2, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all our attack types have above a 90\% success rate. Moreover, our attacks are automated and require only small image perturbations. These findings raise serious concerns about the security of foundation models. If image hijacks are as difficult to defend against as adversarial examples in CIFAR-10, then it might be many years before a s
    
[^47]: 数据驱动的线性规划降维方法：泛化界限和学习方法

    Data-Driven Projection for Reducing Dimensionality of Linear Programs: Generalization Bound and Learning Methods. (arXiv:2309.00203v1 [cs.LG])

    [http://arxiv.org/abs/2309.00203](http://arxiv.org/abs/2309.00203)

    本文研究了一种简单的数据驱动方法，通过学习投影矩阵来降低高维线性规划问题的维数，实现更快的求解速度。基于“数据驱动算法设计”，提出了泛化保证的数据量与性能指标的伪维度的上界和下界。

    

    本文研究了一种简单的数据驱动方法来处理高维线性规划问题（LP）。给定过去的$n$维LP数据，我们学习一个$n\times k$的“投影矩阵”（$n > k$），将维数从$n$降低到$k$。然后，我们通过解决$k$维LP问题并通过乘以投影矩阵来恢复$n$维的解决方案来处理未来的LP实例。这个思想与任何用户首选的LP求解器兼容，因此是一种通用的加速LP求解的方法。一个自然的问题是：需要多少数据才能确保恢复的解决方案的质量？我们基于“数据驱动算法设计”的思想来回答这个问题，它将足够进行泛化保证的数据量与性能指标的“伪维度”联系起来。我们给出了伪维度的$\tilde{\mathrm{O}}(nk^2)$上界（$\tilde{\mathrm{O}}$压缩了对数因子），并通过一个$\Omega(nk)$下界来补充它，

    This paper studies a simple data-driven approach to high-dimensional linear programs (LPs). Given data of past $n$-dimensional LPs, we learn an $n\times k$ \textit{projection matrix} ($n > k$), which reduces the dimensionality from $n$ to $k$. Then, we address future LP instances by solving $k$-dimensional LPs and recovering $n$-dimensional solutions by multiplying the projection matrix. This idea is compatible with any user-preferred LP solvers, hence a versatile approach to faster LP solving. One natural question is: how much data is sufficient to ensure the recovered solutions' quality? We address this question based on the idea of \textit{data-driven algorithm design}, which relates the amount of data sufficient for generalization guarantees to the \textit{pseudo-dimension} of performance metrics. We present an $\tilde{\mathrm{O}}(nk^2)$ upper bound on the pseudo-dimension ($\tilde{\mathrm{O}}$ compresses logarithmic factors) and complement it by an $\Omega(nk)$ lower bound, hence 
    
[^48]: 无监督机器学习模型选择中的主观性

    Subjectivity in Unsupervised Machine Learning Model Selection. (arXiv:2309.00201v1 [cs.LG])

    [http://arxiv.org/abs/2309.00201](http://arxiv.org/abs/2309.00201)

    无监督机器学习模型选择具有主观性，模型选择结果受模型构建者偏好的影响，并可能导致选择的不一致性。需要对模型选择过程进行更加深入的研究和标准化。

    

    模型选择是无监督机器学习中必要的步骤。尽管有很多标准和指标，但模型选择仍然存在主观性。高度主观性可能会对各种机器学习研究的重复性和可再现性产生疑问，并对实际部署的模型的稳健性产生怀疑。然而，模型选择结果中模型构建者的偏好影响的影响尚未得到充分探索。本研究以隐马尔可夫模型为例，调查了模型选择中的主观性。我们邀请了33位参与者和三个大型语言模型（LLMs），在三个场景中进行模型选择。结果显示，无论是参与者还是LLMs的选择都存在变异性和不一致性，尤其是当不同的标准和指标存在分歧时。主观性来源包括对不同标准和指标重要性的不同意见，对模型应该有多简洁的不同看法，以及对数据规模的大小的看法。

    Model selection is a necessary step in unsupervised machine learning. Despite numerous criteria and metrics, model selection remains subjective. A high degree of subjectivity may lead to questions about repeatability and reproducibility of various machine learning studies and doubts about the robustness of models deployed in the real world. Yet, the impact of modelers' preferences on model selection outcomes remains largely unexplored. This study uses the Hidden Markov Model as an example to investigate the subjectivity involved in model selection. We asked 33 participants and three Large Language Models (LLMs) to make model selections in three scenarios. Results revealed variability and inconsistencies in both the participants' and the LLMs' choices, especially when different criteria and metrics disagree. Sources of subjectivity include varying opinions on the importance of different criteria and metrics, differing views on how parsimonious a model should be, and how the size of a da
    
[^49]: 基于聚类的条件扩散模型用于食物图像生成

    Diffusion Model with Clustering-based Conditioning for Food Image Generation. (arXiv:2309.00199v1 [cs.CV])

    [http://arxiv.org/abs/2309.00199](http://arxiv.org/abs/2309.00199)

    

    

    基于图像的膳食评估是使用进食场合图像作为输入的高效准确的记录和分析营养摄入的解决方案。基于深度学习的技术通常用于进行食物分类、分割和份量估计等图像分析任务，这些任务依赖于大量带有注释的食物图像进行训练。然而，这种数据依赖性对于真实世界应用来说存在重大障碍，因为获取大量丰富多样且平衡的食物图像集可能具有挑战性。一种潜在的解决方案是使用合成的食物图像进行数据增强。尽管现有的研究已经探索了基于生成对抗网络（GAN）的结构进行生成，但合成食物图像的质量仍然不理想。此外，尽管扩散式生成模型已经在一般图像生成任务中显示出了良好的结果，但生成食物图像可能具有挑战性，因为要生成真实的食物外观和细节是困难的。

    Image-based dietary assessment serves as an efficient and accurate solution for recording and analyzing nutrition intake using eating occasion images as input. Deep learning-based techniques are commonly used to perform image analysis such as food classification, segmentation, and portion size estimation, which rely on large amounts of food images with annotations for training. However, such data dependency poses significant barriers to real-world applications, because acquiring a substantial, diverse, and balanced set of food images can be challenging. One potential solution is to use synthetic food images for data augmentation. Although existing work has explored the use of generative adversarial networks (GAN) based structures for generation, the quality of synthetic food images still remains subpar. In addition, while diffusion-based generative models have shown promising results for general image generation tasks, the generation of food images can be challenging due to the substan
    
[^50]: 基于深度学习的气举油生产优化的早期修复方法：有监督和弱监督方法

    Deep-learning-based Early Fixing for Gas-lifted Oil Production Optimization: Supervised and Weakly-supervised Approaches. (arXiv:2309.00197v1 [cs.LG])

    [http://arxiv.org/abs/2309.00197](http://arxiv.org/abs/2309.00197)

    本论文提出基于深度学习模型的早期修复方法，通过训练模型为整数变量提供值并早期修复这些变量，从而将原问题简化成了线性规划问题。实验结果表明，有监督方法和弱监督方法均能有效地解决气举油生产优化问题。

    

    将深度学习模型训练用于为所有整数变量提供值，早期修复整数变量并将原问题简化为线性规划(LP)，从而减少依赖昂贵的精确方法或一般近似方法的成本。提出了两种学习启发式方法：有监督学习方法和弱监督学习方法。有监督方法需要训练集中原问题的多个实例的最优整数值，而弱监督方法只需要早期修复的线性问题的解以及整数变量的随机赋值。

    Maximizing oil production from gas-lifted oil wells entails solving Mixed-Integer Linear Programs (MILPs). As the parameters of the wells, such as the basic-sediment-to-water ratio and the gas-oil ratio, are updated, the problems must be repeatedly solved. Instead of relying on costly exact methods or the accuracy of general approximate methods, in this paper, we propose a tailor-made heuristic solution based on deep learning models trained to provide values to all integer variables given varying well parameters, early-fixing the integer variables and, thus, reducing the original problem to a linear program (LP). We propose two approaches for developing the learning-based heuristic: a supervised learning approach, which requires the optimal integer values for several instances of the original problem in the training set, and a weakly-supervised learning approach, which requires only solutions for the early-fixed linear problems with random assignments for the integer variables. Our res
    
[^51]: RepCodec:一种用于语音标记的语音表示编码器

    RepCodec: A Speech Representation Codec for Speech Tokenization. (arXiv:2309.00169v1 [eess.AS])

    [http://arxiv.org/abs/2309.00169](http://arxiv.org/abs/2309.00169)

    RepCodec是一种新型的语音表示编码器，通过重构语音表示并学习矢量量化码书，将语音波形转换为语义标记。实验证明，RepCodec在语音理解和生成方面明显优于传统的k-means聚类方法。

    

    随着大型语言模型（LLMs）的快速增长，离散语音标记在将语音注入LLMs中发挥了重要作用。然而，这种离散化导致了信息的丢失，从而损害了整体性能。为了提高这些离散语音标记的性能，我们提出了RepCodec，一种用于语义语音标记的新型语音表示编码器。与重新构建原始音频的音频编解码器不同，RepCodec通过从语音编码器（如HuBERT或data2vec）重构语音表示来学习矢量量化码书。语音编码器、编解码器和矢量量化码书共同构成一个将语音波形转换为语义标记的流水线。广泛的实验证明，由于其增强的信息保留能力，RepCodec在语音理解和生成方面显著优于广泛使用的k-means聚类方法。

    With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore
    
[^52]: 信息融合在生产评估辅助系统中的应用

    Information Fusion for Assistance Systems in Production Assessment. (arXiv:2309.00157v1 [cs.LG])

    [http://arxiv.org/abs/2309.00157](http://arxiv.org/abs/2309.00157)

    本文提出了一种将不同信息源进行融合的方法，使用证据理论构建了一个通用框架，并在生产评估辅助系统中进行了实证验证，同时解决了数据漂移问题。

    

    我们提出了一种新的方法来定义依赖信息融合的辅助系统，该系统能够结合不同的信息源并提供评估。本文的主要贡献是提供了一个使用证据理论来融合n个信息源的通用框架。融合提供了更可靠的预测和相关的不确定性，可以用于评估预测的可能性。此外，我们提供了一种用于信息融合的机器数据和专家中心模型两个主要来源的方法。我们使用工业设备的数据演示了信息融合方法，从而完善了本研究的应用部分。此外，我们通过提出一种基于证据理论的方法来更新基于数据的模型，解决了数据漂移的问题。我们使用Benchmark Tennessee Eastman进行了验证，并对模型更新进行了割除性研究。

    We propose a novel methodology to define assistance systems that rely on information fusion to combine different sources of information while providing an assessment. The main contribution of this paper is providing a general framework for the fusion of n number of information sources using the evidence theory. The fusion provides a more robust prediction and an associated uncertainty that can be used to assess the prediction likeliness. Moreover, we provide a methodology for the information fusion of two primary sources: an ensemble classifier based on machine data and an expert-centered model. We demonstrate the information fusion approach using data from an industrial setup, which rounds up the application part of this research. Furthermore, we address the problem of data drift by proposing a methodology to update the data-based models using an evidence theory approach. We validate the approach using the Benchmark Tennessee Eastman while doing an ablation study of the model update p
    
[^53]: TurboGP：一个灵活而先进的基于Python的GP库

    TurboGP: A flexible and advanced python based GP library. (arXiv:2309.00149v1 [cs.NE])

    [http://arxiv.org/abs/2309.00149](http://arxiv.org/abs/2309.00149)

    TurboGP是一个基于Python的灵活和先进的GP库，具有岛和细胞人群方案、不同类型的遗传操作和对不同类型的GP节点的本地支持，适用于处理各种数据源。

    

    我们介绍了TurboGP，一个完全用Python编写的遗传编程（GP）库，专门设计用于机器学习任务。TurboGP实现了其他GP实现中不存在的现代特性，如岛和细胞人群方案、不同类型的遗传操作（迁移、受保护的交叉等）、在线学习等特性。TurboGP最独特的特点是其对不同类型的GP节点的本地支持，以允许不同的抽象级别，这使得TurboGP在处理各种数据源时特别有用。

    We introduce TurboGP, a Genetic Programming (GP) library fully written in Python and specifically designed for machine learning tasks. TurboGP implements modern features not available in other GP implementations, such as island and cellular population schemes, different types of genetic operations (migration, protected crossovers), online learning, among other features. TurboGP's most distinctive characteristic is its native support for different types of GP nodes to allow different abstraction levels, this makes TurboGP particularly useful for processing a wide variety of data sources.
    
[^54]: 多Agent DeepRL算法在IAB网络中的联合功率和子通道分配

    Multi Agent DeepRL based Joint Power and Subchannel Allocation in IAB networks. (arXiv:2309.00144v1 [eess.SY])

    [http://arxiv.org/abs/2309.00144](http://arxiv.org/abs/2309.00144)

    这项研究提出了一种基于多Agent DeepRL算法的框架，用于在IAB网络中联合优化功率和子通道分配，以最大化下行数据速率。

    

    集成接入和回传（IAB）是满足未来几代对更高数据速率需求的一种可行方法，作为密集光纤连接的经济有效替代方案。具有约束条件的这种网络设计通常会导致非凸性和组合性的优化问题。在这些情况下，为联合子通道分配和功率分配（SAPA）问题获得最优策略是具有挑战性的。本文提出了一种基于多Agent Deep Reinforcement Learning（DeepRL）的框架，用于在IAB网络中联合优化功率和子通道分配，以最大化下行数据速率。使用DDQN（Double Deep Q-Learning Network）的SAPA可以处理与多个用户和节点相关联的具有巨大行动空间的计算密集型问题。与传统方法（如博弈论，分数规划和凸优化）不同，这些方法在实践中需要越来越精确的网络。

    Integrated Access and Backhauling (IAB) is a viable approach for meeting the unprecedented need for higher data rates of future generations, acting as a cost-effective alternative to dense fiber-wired links. The design of such networks with constraints usually results in an optimization problem of non-convex and combinatorial nature. Under those situations, it is challenging to obtain an optimal strategy for the joint Subchannel Allocation and Power Allocation (SAPA) problem. In this paper, we develop a multi-agent Deep Reinforcement Learning (DeepRL) based framework for joint optimization of power and subchannel allocation in an IAB network to maximize the downlink data rate. SAPA using DDQN (Double Deep Q-Learning Network) can handle computationally expensive problems with huge action spaces associated with multiple users and nodes. Unlike the conventional methods such as game theory, fractional programming, and convex optimization, which in practice demand more and more accurate net
    
[^55]: 使用动态模块跳过在流conformer编码器中改进了基于视觉启发的关键词检测的方法

    Improving vision-inspired keyword spotting using dynamic module skipping in streaming conformer encoder. (arXiv:2309.00140v1 [cs.SD])

    [http://arxiv.org/abs/2309.00140](http://arxiv.org/abs/2309.00140)

    本论文提出了一种使用动态模块跳过的流conformer编码器的视觉启发关键词检测方法，通过在输入音频上动态跳过网络模块，提高了检测和定位准确性，同时保持了小的内存占用。该方法在含背景噪声的Google语音指令数据集上表现更加突出，对于始终开启的关键词检测器特别有意义。

    

    在使用基于视觉启发的关键词检测框架中，我们提出了一个具有输入依赖的动态深度的架构，能够处理流式音频。具体来说，我们使用可训练的二进制门扩展了一个conformer编码器，从而允许我们根据输入音频动态跳过网络模块。我们的方法在使用Librispeech前1000个最常见单词的连续语音上提高了检测和定位的准确性，同时保持了较小的内存占用。门的加入还减少了平均处理量，而不影响整体性能。将谷歌语音命令数据集放置在背景噪声上，这些优势更加显著，非语音输入上的处理量可减少高达97%，因此我们的方法对于始终开启的关键词检测器尤为有趣。

    Using a vision-inspired keyword spotting framework, we propose an architecture with input-dependent dynamic depth capable of processing streaming audio. Specifically, we extend a conformer encoder with trainable binary gates that allow us to dynamically skip network modules according to the input audio. Our approach improves detection and localization accuracy on continuous speech using Librispeech top-1000 most frequent words while maintaining a small memory footprint. The inclusion of gates also reduces the average amount of processing without affecting the overall performance. These benefits are shown to be even more pronounced using the Google speech commands dataset placed over background noise where up to 97% of the processing is skipped on non-speech inputs, therefore making our method particularly interesting for an always-on keyword spotter.
    
[^56]: 使用时间序列分析和自然语言处理预测金融市场趋势

    Predicting Financial Market Trends using Time Series Analysis and Natural Language Processing. (arXiv:2309.00136v1 [q-fin.ST])

    [http://arxiv.org/abs/2309.00136](http://arxiv.org/abs/2309.00136)

    本研究通过分析推文中表达的情绪，使用时间序列分析和自然语言处理预测了特斯拉、苹果等主要公司股票价格的波动。结果表明，积极性、消极性和主观性是股票价格波动的主要决定因素。

    

    通过时间序列分析和自然语言处理预测金融市场趋势是一项复杂而具有挑战性的任务，因为许多变量可以影响股票价格。这些变量涵盖了一系列经济和政治事件，以及当前的公众态度。最近的研究表明，社交媒体平台上公众情绪的表达（如Twitter）可能对股票价格的确定产生重要影响。本研究旨在评估Twitter情绪作为预测特斯拉、苹果等主要公司股票价格的工具的可行性。我们的研究发现，推文中传达的情绪与股票价格的波动之间存在强有力的关联。我们的研究结果表明，积极性、消极性和主观性是股票价格波动的主要决定因素。数据使用了长短期记忆神经网络（LSTM）模型进行分析。

    Forecasting financial market trends through time series analysis and natural language processing poses a complex and demanding undertaking, owing to the numerous variables that can influence stock prices. These variables encompass a spectrum of economic and political occurrences, as well as prevailing public attitudes. Recent research has indicated that the expression of public sentiments on social media platforms such as Twitter may have a noteworthy impact on the determination of stock prices. The objective of this study was to assess the viability of Twitter sentiments as a tool for predicting stock prices of major corporations such as Tesla, Apple. Our study has revealed a robust association between the emotions conveyed in tweets and fluctuations in stock prices. Our findings indicate that positivity, negativity, and subjectivity are the primary determinants of fluctuations in stock prices. The data was analyzed utilizing the Long-Short Term Memory neural network (LSTM) model, whi
    
[^57]: FTA: 具有灵活触发器的隐蔽且强健的联邦学习后门攻击

    FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning. (arXiv:2309.00127v1 [cs.LG])

    [http://arxiv.org/abs/2309.00127](http://arxiv.org/abs/2309.00127)

    FTA提出了一种新的灵活触发器、隐蔽且强健的联邦学习后门攻击方法，通过生成学习灵活触发器模式来操作良性样本，同时保留攻击者选择的标签的最重要隐藏特征。通过填充可区分的差异，使攻击具有隐蔽性。验证了该方法的有效性和可靠性。

    

    当前针对联邦学习（FL）的后门攻击在很大程度上依赖于通用触发器或语义模式，这可以被某些防御机制（如范数剪裁，比较局部更新之间的参数差异）轻易检测和过滤。在这项工作中，我们提出了一种新的隐蔽且强健的具有灵活触发器的联邦学习后门攻击。为了实现这一点，我们构建了一个生成触发器函数，可以学习使用一个几乎不可察觉的灵活触发器模式来操作良性样本，并同时让触发器模式包含攻击者选择的标签的最重要的隐藏特征。此外，我们的触发器生成器可以在不同轮次中不断学习和适应，从而使其能够调整到全局模型的变化。通过填充可区分的差异（触发器模式和目标标签之间的映射），我们使我们的攻击具有自然的隐蔽性。对真实数据集的大量实验验证了我们攻击的有效性和可靠性。

    Current backdoor attacks against federated learning (FL) strongly rely on universal triggers or semantic patterns, which can be easily detected and filtered by certain defense mechanisms such as norm clipping, comparing parameter divergences among local updates. In this work, we propose a new stealthy and robust backdoor attack with flexible triggers against FL defenses. To achieve this, we build a generative trigger function that can learn to manipulate the benign samples with an imperceptible flexible trigger pattern and simultaneously make the trigger pattern include the most significant hidden features of the attacker-chosen label. Moreover, our trigger generator can keep learning and adapt across different rounds, allowing it to adjust to changes in the global model. By filling the distinguishable difference (the mapping between the trigger pattern and target label), we make our attack naturally stealthy. Extensive experiments on real-world datasets verify the effectiveness and st
    
[^58]: 通过独立分量拉普拉斯过程实现差分隐私的函数性摘要

    Differentially Private Functional Summaries via the Independent Component Laplace Process. (arXiv:2309.00125v1 [stat.ML])

    [http://arxiv.org/abs/2309.00125](http://arxiv.org/abs/2309.00125)

    本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。

    

    在这项工作中，我们提出了一种称为独立分量拉普拉斯过程（ICLP）机制的差分隐私函数性摘要的新机制。通过将感兴趣的函数性摘要视为真正无限维对象，并使用ICLP噪声来扰动它们，该新机制放宽了关于数据轨迹的假设，并相对于文献中的经典有限维子空间嵌入方法保留了更高的效用。我们在多个函数空间中验证了所提出机制的可行性。我们考虑了几个统计估计问题，并通过轻微过平滑摘要来证明隐私成本不会主导统计误差，并且在渐近情况下可以忽略。对合成和真实数据集的数值实验证明了所提出机制的有效性。

    In this work, we propose a new mechanism for releasing differentially private functional summaries called the Independent Component Laplace Process, or ICLP, mechanism. By treating the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over-smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism.
    
[^59]: 深度半监督异常检测在期货市场诈骗发现中的应用

    Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures Market. (arXiv:2309.00088v1 [cs.LG])

    [http://arxiv.org/abs/2309.00088](http://arxiv.org/abs/2309.00088)

    本研究评估了一种名为Deep SAD的深度半监督异常检测技术在高频金融数据中检测诈骗的效果。使用了来自TMX交易所的独家限价委托薄数据，并利用一小部分的标记数据。

    

    现代金融电子交易所是一个令人兴奋且快节奏的市场，每天交易额达数十亿美元。然而，市场上存在着操纵和诈骗活动。检测此类活动是一个重大任务，过去一直是由人类来完成。最近，更多的研究和资源集中在通过机器学习和人工智能来自动化这些过程上。欺诈检测主要与异常检测领域有关，通常通过无监督学习技术来进行，因为缺少有监督学习所需的标记数据。然而，通常还有少量可用的有标签数据。本研究旨在评估一种名为Deep SAD的深度半监督异常检测技术，在高频金融数据中检测诈骗的效果。我们使用了来自蒙特利尔TMX交易所的独家限价委托薄数据，配以少量的标记数据。

    Modern financial electronic exchanges are an exciting and fast-paced marketplace where billions of dollars change hands every day. They are also rife with manipulation and fraud. Detecting such activity is a major undertaking, which has historically been a job reserved exclusively for humans. Recently, more research and resources have been focused on automating these processes via machine learning and artificial intelligence. Fraud detection is overwhelmingly associated with the greater field of anomaly detection, which is usually performed via unsupervised learning techniques because of the lack of labeled data needed for supervised learning. However, a small quantity of labeled data does often exist. This research article aims to evaluate the efficacy of a deep semi-supervised anomaly detection technique, called Deep SAD, for detecting fraud in high-frequency financial data. We use exclusive proprietary limit order book data from the TMX exchange in Montr\'eal, with a small set of tr
    
[^60]: RePo: 通过正则化后验可预测性增强弹性模型基础强化学习

    RePo: Resilient Model-Based Reinforcement Learning by Regularizing Posterior Predictability. (arXiv:2309.00082v1 [cs.LG])

    [http://arxiv.org/abs/2309.00082](http://arxiv.org/abs/2309.00082)

    本文提出了RePo算法，通过正则化后验可预测性的方式，增强了视觉模型基础强化学习方法的弹性。该方法通过学习一个对冗余和伪变化具有弹性的潜在表示，提高了方法对视觉干扰的鲁棒性，使其能够在动态环境中运行。

    

    视觉模型基础强化学习方法通常将图像观测编码为低维表示方式，这种方式未能消除冗余信息。这使得这些方法容易受到伪变化的影响，即与任务无关的组成部分的变化，如背景干扰因素或光照条件的变化。本文提出了一种视觉模型基础强化学习方法，该方法学习到了一种对这种伪变化具有弹性的潜在表示。我们的训练目标鼓励该表示在动力学和奖励预测方面具有最大的预测性，同时限制了观测到潜在表示的信息流。我们证明了这一目标极大增强了视觉模型基础强化学习方法对视觉干扰的弹性，使其能够在动态环境中运行。然后我们展示了虽然学习到的编码器对伪变化具有弹性，但在显著分布变化下并没有不变性。为了解决这个问题，我们提出了一个简单的奖励方案。

    Visual model-based RL methods typically encode image observations into low-dimensional representations in a manner that does not eliminate redundant information. This leaves them susceptible to spurious variations -- changes in task-irrelevant components such as background distractors or lighting conditions. In this paper, we propose a visual model-based RL method that learns a latent representation resilient to such spurious variations. Our training objective encourages the representation to be maximally predictive of dynamics and reward, while constraining the information flow from the observation to the latent representation. We demonstrate that this objective significantly bolsters the resilience of visual model-based RL methods to visual distractors, allowing them to operate in dynamic environments. We then show that while the learned encoder is resilient to spirious variations, it is not invariant under significant distribution shift. To address this, we propose a simple reward-f
    
[^61]: 关于Adam的隐式偏差

    On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])

    [http://arxiv.org/abs/2309.00079](http://arxiv.org/abs/2309.00079)

    本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。

    

    在以前的文献中，后向误差分析被用来找到近似梯度下降轨迹的常微分方程（ODEs）。发现有限步长会隐式地规范化解决方案，因为出现在ODE中的项会惩罚损失梯度的二范数。我们证明了RMSProp和Adam中是否存在类似的隐式规范化取决于它们的超参数和训练阶段，但涉及的“范数”不同：对应的ODE项要么惩罚（扰动的）损失梯度的一范数，要么相反地阻止其减小（后一种情况是典型的）。我们还进行了数值实验，并讨论了这些证明事实如何影响泛化。

    In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
    
[^62]: 利用扩散变分自编码器解决多步回归股票价格预测中的随机性问题

    Diffusion Variational Autoencoder for Tackling Stochasticity in Multi-Step Regression Stock Price Prediction. (arXiv:2309.00073v1 [q-fin.ST])

    [http://arxiv.org/abs/2309.00073](http://arxiv.org/abs/2309.00073)

    本论文提出了一种利用扩散变分自编码器来解决多步回归股票价格预测中的随机性问题的方法。

    

    长期内的多步股票价格预测对于预测波动性至关重要，使得金融机构能够定价和对冲衍生品，并让银行量化其交易簿中的风险。此外，大多数金融监管机构还要求机构投资者有几天的流动性期限从其风险资产中退出，以避免对市场价格产生实质性影响。然而，由于股票数据具有高度随机性，多步股票价格预测的任务很具挑战性。目前解决这个问题的方法主要针对单步基于分类的预测，并且在表征表达力方面有限。随着目标价格序列的引入，这个问题在测试时变得越来越困难，因为目标价格序列中也包含随机噪声，降低了泛化能力。为了解决这些问题，我们结合了深层分层变分自编码器(VAE)和扩散概率技术。

    Multi-step stock price prediction over a long-term horizon is crucial for forecasting its volatility, allowing financial institutions to price and hedge derivatives, and banks to quantify the risk in their trading books. Additionally, most financial regulators also require a liquidity horizon of several days for institutional investors to exit their risky assets, in order to not materially affect market prices. However, the task of multi-step stock price prediction is challenging, given the highly stochastic nature of stock data. Current solutions to tackle this problem are mostly designed for single-step, classification-based predictions, and are limited to low representation expressiveness. The problem also gets progressively harder with the introduction of the target price sequence, which also contains stochastic noise and reduces generalizability at test-time. To tackle these issues, we combine a deep hierarchical variational-autoencoder (VAE) and diffusion probabilistic techniques
    
[^63]: YaRN: 大型语言模型的高效上下文窗口扩展方法

    YaRN: Efficient Context Window Extension of Large Language Models. (arXiv:2309.00071v1 [cs.CL])

    [http://arxiv.org/abs/2309.00071](http://arxiv.org/abs/2309.00071)

    YaRN是一种高效的上下文窗口扩展方法，可以在大型语言模型中有效利用和推断比原始预训练允许的上下文长度更长的上下文，同时超越了之前的最新研究成果。

    

    旋转位置嵌入（RoPE）已被证明可以有效地编码transformer-based语言模型中的位置信息。然而，这些模型在超过它们训练的序列长度时无法泛化。我们提出了YaRN（Yet another RoPE extensioN method），一种计算高效的方法来扩展这些模型的上下文窗口，需要的tokens数量和训练步骤少于之前的方法的10倍和2.5倍。使用YaRN，我们展示了LLaMA模型可以有效地利用和推断比原始预训练允许的上下文长度更长的上下文，并且在上下文窗口扩展方面超过了之前的最新研究成果。此外，我们还展示了YaRN具有超越微调数据集有限上下文的能力。我们在https://github.com/jquesnelle/yarn上发布了使用64k和128k上下文窗口进行Fine-tuning的Llama 2 7B/13B的检查点。

    Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. We publish the checkpoints of Llama 2 7B/13B fine-tuned using YaRN with 64k and 128k context windows at https://github.com/jquesnelle/yarn
    
[^64]: 保持局部和全局结构的高效多视图图聚类

    Efficient Multi-View Graph Clustering with Local and Global Structure Preservation. (arXiv:2309.00024v1 [cs.LG])

    [http://arxiv.org/abs/2309.00024](http://arxiv.org/abs/2309.00024)

    提出了一种新的基于锚点的多视图图聚类方法，通过统一的框架将优化的全局结构和局部结构相结合，同时考虑局部相似度和全局相似度，以提高聚类性能。

    

    基于锚点的多视图图聚类（AMVGC）由于其高效性和捕捉多个视图之间互补结构信息的能力而受到广泛关注。直观地说，高质量的锚图在AMVGC的成功中起着关键作用。然而，现有的AMVGC方法只考虑单一的结构信息，即局部或全局结构，这为学习任务提供了不足的信息。具体而言，过于分散的全局结构导致学习到的锚点不能很好地描述聚类划分。相反，具有不恰当相似度度量的局部结构导致潜在的不准确的锚点分配，最终导致次优的聚类性能。为了解决这个问题，我们提出了一种新的基于锚点的多视图图聚类框架，称为保持局部和全局结构的高效多视图图聚类（EMVGC-LG）。具体而言，我们将优化的全局结构和局部结构相结合，通过统一的框架进行优化，同时考虑局部相似度和全局相似度，从而提高聚类性能。

    Anchor-based multi-view graph clustering (AMVGC) has received abundant attention owing to its high efficiency and the capability to capture complementary structural information across multiple views. Intuitively, a high-quality anchor graph plays an essential role in the success of AMVGC. However, the existing AMVGC methods only consider single-structure information, i.e., local or global structure, which provides insufficient information for the learning task. To be specific, the over-scattered global structure leads to learned anchors failing to depict the cluster partition well. In contrast, the local structure with an improper similarity measure results in potentially inaccurate anchor assignment, ultimately leading to sub-optimal clustering performance. To tackle the issue, we propose a novel anchor-based multi-view graph clustering framework termed Efficient Multi-View Graph Clustering with Local and Global Structure Preservation (EMVGC-LG). Specifically, a unified framework with
    
[^65]: 从API流中进行持续学习

    Continual Learning From a Stream of APIs. (arXiv:2309.00023v1 [cs.LG])

    [http://arxiv.org/abs/2309.00023](http://arxiv.org/abs/2309.00023)

    本文介绍了基于API流的持续学习方法，包括数据有效的持续学习和无数据的持续学习。通过查询API生成伪数据，将API流中的知识蒸馏到持续学习模型中，从而解决了无法获取完整的原始数据、未知的模型参数、异构模型和灾难性遗忘等挑战。

    

    持续学习旨在学习新任务而不忘记以前的任务。然而，现有的持续学习方法需要大量原始数据，由于版权考虑和隐私风险，这些数据通常不可用。相反，利益相关者通常通过API释放预训练的机器学习模型作为服务（MLaaS），用户可以通过API访问。本文考虑了两种实用但新颖的持续学习设置：数据有效的持续学习（DECL-APIs）和无数据的持续学习（DFCL-APIs），通过部分或无原始数据从API流中实现持续学习。在这两种新设置下进行持续学习面临几个挑战：无法获取完整的原始数据，未知的模型参数，任意架构和规模的异构模型以及对以前API的灾难性遗忘。为了克服这些问题，我们提出了一种新颖的无数据合作持续蒸馏学习框架，通过查询API生成伪数据，将API流中的知识蒸馏到持续学习模型中。

    Continual learning (CL) aims to learn new tasks without forgetting previous tasks. However, existing CL methods require a large amount of raw data, which is often unavailable due to copyright considerations and privacy risks. Instead, stakeholders usually release pre-trained machine learning models as a service (MLaaS), which users can access via APIs. This paper considers two practical-yet-novel CL settings: data-efficient CL (DECL-APIs) and data-free CL (DFCL-APIs), which achieve CL from a stream of APIs with partial or no raw data. Performing CL under these two new settings faces several challenges: unavailable full raw data, unknown model parameters, heterogeneous models of arbitrary architecture and scale, and catastrophic forgetting of previous APIs. To overcome these issues, we propose a novel data-free cooperative continual distillation learning framework that distills knowledge from a stream of APIs into a CL model by generating pseudo data, just by querying APIs. Specifically
    
[^66]: 设计边缘上自适应人工智能应用的节能方法

    An Energy-Aware Approach to Design Self-Adaptive AI-based Applications on the Edge. (arXiv:2309.00022v1 [cs.SE])

    [http://arxiv.org/abs/2309.00022](http://arxiv.org/abs/2309.00022)

    本文提出了一种能源感知方法，用于设计和部署能够在应用目标与能源消耗之间平衡的自适应人工智能应用。

    

    专用于机器学习任务的边缘设备的出现使得能够高效处理和分类物联网中资源受限设备获取到的数据的人工智能应用能够执行。这些应用的普及（例如智能城市中的关键监控）要求我们从能源角度也要考虑这些系统的可持续性。本文提出了一种能源感知方法，用于设计和部署能够在应用目标（例如物体检测的准确性和帧处理速率）与能源消耗之间平衡的自适应人工智能应用。我们通过一种元启发式搜索过程来解决确定可用于系统自适应的配置集合的问题，该过程仅需要少量经验样本。最终的配置集合是通过加权灰色关联分析选择的，并映射到自适应系统的操作模式中。

    The advent of edge devices dedicated to machine learning tasks enabled the execution of AI-based applications that efficiently process and classify the data acquired by the resource-constrained devices populating the Internet of Things. The proliferation of such applications (e.g., critical monitoring in smart cities) demands new strategies to make these systems also sustainable from an energetic point of view.  In this paper, we present an energy-aware approach for the design and deployment of self-adaptive AI-based applications that can balance application objectives (e.g., accuracy in object detection and frames processing rate) with energy consumption. We address the problem of determining the set of configurations that can be used to self-adapt the system with a meta-heuristic search procedure that only needs a small number of empirical samples. The final set of configurations are selected using weighted gray relational analysis, and mapped to the operation modes of the self-adapt
    
[^67]: 无监督发现可解释的视觉概念

    Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v1 [cs.CV])

    [http://arxiv.org/abs/2309.00018](http://arxiv.org/abs/2309.00018)

    本文提出了两种方法（MAGE和Ms-IV），用于解释深度学习模型的决策，提高全局可解释性。MAGE可以发现形成语义含义的特征组合，将其称为概念，并通过聚类分组为“概念”，然后通过Ms-IV进行可视化。这一方法受到阻断和敏感性分析的启发，并使用一种新的指标（CaOC）全局评估模型最重要的图像区域。

    

    深度学习模型的可解释性对于非专家用户非常重要，但是在实际应用中，提供给用户的模型解释性是一项具有挑战性的任务。诸如集成梯度等可解释性方法产生了包含大量信息但难以解释的归因映射。本文提出了两种方法，最大激活组提取（MAGE）和多尺度可解释性可视化（Ms-IV），用于解释模型的决策，提高全局可解释性。MAGE可以找到给定CNN中形成语义含义的特征组合，我们将其称为概念，并通过聚类将这些相似特征模式分组为“概念”，然后通过Ms-IV进行可视化。这一方法受到阻断和敏感性分析的启发（包括因果关系），并使用一种新的指标，称为类别感知顺序相关性（CaOC），全局评估根据模型预测结果最重要的图像区域。

    Providing interpretability of deep-learning models to non-experts, while fundamental for a responsible real-world usage, is challenging. Attribution maps from xAI techniques, such as Integrated Gradients, are a typical example of a visualization technique containing a high level of information, but with difficult interpretation. In this paper, we propose two methods, Maximum Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization (Ms-IV), to explain the model's decision, enhancing global interpretability. MAGE finds, for a given CNN, combinations of features which, globally, form a semantic meaning, that we call concepts. We group these similar feature patterns by clustering in ``concepts'', that we visualize through Ms-IV. This last method is inspired by Occlusion and Sensitivity analysis (incorporating causality), and uses a novel metric, called Class-aware Order Correlation (CaOC), to globally evaluate the most important image regions according to the model's 
    
[^68]: 基于深度强化学习的基于物理的细胞连接无人机在多雨环境中的轨迹设计

    Physics-Based Trajectory Design for Cellular-Connected UAV in Rainy Environments Based on Deep Reinforcement Learning. (arXiv:2309.00017v1 [eess.SY])

    [http://arxiv.org/abs/2309.00017](http://arxiv.org/abs/2309.00017)

    本文提出了一种基于物理的轨迹设计方法，用于在多雨环境中对细胞连接的无人机进行轨迹优化。该方法利用物理模拟器考虑了详细的环境信息和雨水对信号传播的影响。

    

    基于现有的细胞基础设施，细胞连接的无人机（UAV）通过可靠的通信增强了传统的UAV能力，因此得到了越来越多的关注。它们已经被用于各种应用，包括天气预测和搜救行动。然而，在极端天气条件下，如雨天，细胞UAV的轨迹设计具有挑战性，由于天空中的覆盖区域较弱，UAV的飞行时间限制以及雨滴引起的信号衰减。为此，本文提出了一种基于物理的轨迹设计方法，用于细胞连接的UAV在多雨环境中。采用基于物理的电磁模拟器考虑了详细的环境信息和雨水对无线电波传播的影响。轨迹优化问题被规划为同时考虑无人机飞行时间和信号干扰的问题。

    Cellular-connected unmanned aerial vehicles (UAVs) have gained increasing attention due to their potential to enhance conventional UAV capabilities by leveraging existing cellular infrastructure for reliable communications between UAVs and base stations. They have been used for various applications, including weather forecasting and search and rescue operations. However, under extreme weather conditions such as rainfall, it is challenging for the trajectory design of cellular UAVs, due to weak coverage regions in the sky, limitations of UAV flying time, and signal attenuation caused by raindrops. To this end, this paper proposes a physics-based trajectory design approach for cellular-connected UAVs in rainy environments. A physics-based electromagnetic simulator is utilized to take into account detailed environment information and the impact of rain on radio wave propagation. The trajectory optimization problem is formulated to jointly consider UAV flying time and signal-to-interferenc
    
[^69]: 大规模公共数据提高差分隐私图像生成质量

    Large-Scale Public Data Improves Differentially Private Image Generation Quality. (arXiv:2309.00008v1 [cs.CV])

    [http://arxiv.org/abs/2309.00008](http://arxiv.org/abs/2309.00008)

    本研究通过使用大规模公共数据，提出了一种改进的方法来提高差分隐私图像生成的质量。评估结果表明，与现有方法相比，我们的方法在FID得分和其他指标上取得了最先进的成果，并能以差分隐私的方式生成高质量、逼真的图像。

    

    公共数据经常被用来改善差分隐私机器学习的隐私-准确性权衡，但是先前的工作主要假设这些数据与私有数据来自相同的分布。在本研究中，我们研究了如何使用通用的大规模公共数据来提高生成对抗网络（GANs）中差分隐私图像生成的质量，并提供了一种有效利用公共数据的改进方法。我们的方法基于这样一个假设，即公共数据分布的支持集包含了私有数据的支持集；一个例子是公共数据来自通用型的互联网规模图像源，而私有数据由特定类型的图像组成。详细评估结果显示，与使用公共数据的现有方法相比，我们的方法在FID得分和其他指标上达到了最先进水平，并能够以差分隐私的方式生成高质量、逼真的图像。

    Public data has been frequently used to improve the privacy-accuracy trade-off of differentially private machine learning, but prior work largely assumes that this data come from the same distribution as the private. In this work, we look at how to use generic large-scale public data to improve the quality of differentially private image generation in Generative Adversarial Networks (GANs), and provide an improved method that uses public data effectively. Our method works under the assumption that the support of the public data distribution contains the support of the private; an example of this is when the public data come from a general-purpose internet-scale image source, while the private data consist of images of a specific type. Detailed evaluations show that our method achieves SOTA in terms of FID score and other metrics compared with existing methods that use public data, and can generate high-quality, photo-realistic images in a differentially private manner.
    
[^70]: 当度量不可靠时：朝着Top-k多标签学习的不可察觉的对抗性扰动

    When Measures are Unreliable: Imperceptible Adversarial Perturbations toward Top-$k$ Multi-Label Learning. (arXiv:2309.00007v1 [cs.CV])

    [http://arxiv.org/abs/2309.00007](http://arxiv.org/abs/2309.00007)

    本文提出了一种面向Top-k多标签学习的不可察觉的对抗性扰动方法，既能够欺骗人眼的视觉感知，又能够避开度量监测。

    

    随着深度神经网络的巨大成功，对抗性学习在各种研究中得到了广泛关注，涵盖了从多类别学习到多标签学习的范围。然而，现有的对抗性攻击方法只关注传统的视觉不可察觉性，而忽视了来自度量的新的可感知问题，例如Precision@k和mAP@k。具体而言，当一个训练良好的多标签分类器在某些样本上的表现远低于预期时，受害者可以很容易地意识到这种性能退化源于攻击，而不是模型本身。因此，理想的多标签对抗性攻击应该能够欺骗视觉感知，并且避开度量的监测。为此，本文首先引入了度量不可察觉性的概念。然后，设计了一种新的损失函数来生成这种既能够实现视觉不可察觉性又能够避开度量监测的对抗性扰动。

    With the great success of deep neural networks, adversarial learning has received widespread attention in various studies, ranging from multi-class learning to multi-label learning. However, existing adversarial attacks toward multi-label learning only pursue the traditional visual imperceptibility but ignore the new perceptible problem coming from measures such as Precision@$k$ and mAP@$k$. Specifically, when a well-trained multi-label classifier performs far below the expectation on some samples, the victim can easily realize that this performance degeneration stems from attack, rather than the model itself. Therefore, an ideal multi-labeling adversarial attack should manage to not only deceive visual perception but also evade monitoring of measures. To this end, this paper first proposes the concept of measure imperceptibility. Then, a novel loss function is devised to generate such adversarial perturbations that could achieve both visual and measure imperceptibility. Furthermore, a
    
[^71]: 多源融合高光谱高空间分辨率的合成高光谱数据集

    High Spectral Spatial Resolution Synthetic HyperSpectral Dataset form multi-source fusion. (arXiv:2309.00005v1 [cs.CV])

    [http://arxiv.org/abs/2309.00005](http://arxiv.org/abs/2309.00005)

    本研究提供了一种合成高光谱数据集，通过融合多源数据，实现了对观测场景或对象的综合、准确和详细的表示。

    

    本研究介绍了一种合成高光谱数据集，通过将高光谱和高空间分辨率成像相结合，实现对观测场景或对象的综合、准确和详细的表示。在依赖单一相机时，获得这种理想的品质是具有挑战性的。所提出的数据集通过利用三种模态（RGB、推扫式可见光高光谱相机和快照式红外高光谱相机）来解决这个问题，每种模态都具有不同的空间和光谱分辨率。不同的相机系统具有不同的光度学特性，导致空间和光谱分辨率之间的权衡。RGB相机通常具有高空间分辨率但光谱分辨率有限，而高光谱相机在牺牲空间分辨率的情况下具有高光谱分辨率。此外，高光谱相机本身采用不同的捕获技术和光谱范围，进一步增加了综合获取的复杂性。

    This research paper introduces a synthetic hyperspectral dataset that combines high spectral and spatial resolution imaging to achieve a comprehensive, accurate, and detailed representation of observed scenes or objects. Obtaining such desirable qualities is challenging when relying on a single camera. The proposed dataset addresses this limitation by leveraging three modalities: RGB, push-broom visible hyperspectral camera, and snapshot infrared hyperspectral camera, each offering distinct spatial and spectral resolutions. Different camera systems exhibit varying photometric properties, resulting in a trade-off between spatial and spectral resolution. RGB cameras typically offer high spatial resolution but limited spectral resolution, while hyperspectral cameras possess high spectral resolution at the expense of spatial resolution. Moreover, hyperspectral cameras themselves employ different capturing techniques and spectral ranges, further complicating the acquisition of comprehensive
    
[^72]: GNFactor：具有可泛化神经特征场的多任务真实机器人学习

    GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields. (arXiv:2308.16891v1 [cs.RO])

    [http://arxiv.org/abs/2308.16891](http://arxiv.org/abs/2308.16891)

    GNFactor是一个用于多任务机器人操作的代理方法，它利用可泛化神经特征场和Perceiver Transformer模块，以及深度三维体素表示来实现对真实世界环境中的操作任务的执行。它通过将视觉和语义信息纳入三维表示来提高场景的理解能力，并在多个任务上进行了验证。

    

    在无结构的现实世界环境中，从视觉观察中开发能够执行多样化操作任务的代理机器人一直是机器人学中的一个长期问题。为了实现这个目标，机器人需要全面理解场景的三维结构和语义。在这项工作中，我们提出了GNFactor，一种用于多任务机器人操作的可视行为克隆代理，它利用可泛化神经特征场（GNF）作为重建模块，Perceiver Transformer作为决策模块，共享深度三维体素表示。为了将语义纳入三维表示，重建模块利用视觉语言基础模型（例如，稳定扩散）将丰富的语义信息提取到深度三维体素中。我们在3个真实机器人任务上评估了GNFactor，并对10个RLBench任务进行了详细的消融实验，只使用了有限数量的数据。

    It is a long-standing problem in robotics to develop agents capable of executing diverse manipulation tasks from visual observations in unstructured real-world environments. To achieve this goal, the robot needs to have a comprehensive understanding of the 3D structure and semantics of the scene. In this work, we present $\textbf{GNFactor}$, a visual behavior cloning agent for multi-task robotic manipulation with $\textbf{G}$eneralizable $\textbf{N}$eural feature $\textbf{F}$ields. GNFactor jointly optimizes a generalizable neural field (GNF) as a reconstruction module and a Perceiver Transformer as a decision-making module, leveraging a shared deep 3D voxel representation. To incorporate semantics in 3D, the reconstruction module utilizes a vision-language foundation model ($\textit{e.g.}$, Stable Diffusion) to distill rich semantic information into the deep 3D voxel. We evaluate GNFactor on 3 real robot tasks and perform detailed ablations on 10 RLBench tasks with a limited number of
    
[^73]: FedDD: 通过差分参数丢弃实现通信高效的联邦学习

    FedDD: Toward Communication-efficient Federated Learning with Differential Parameter Dropout. (arXiv:2308.16835v1 [cs.LG])

    [http://arxiv.org/abs/2308.16835](http://arxiv.org/abs/2308.16835)

    本文提出了一种通过差分参数丢弃实现通信高效的联邦学习方案（FedDD）。这种方案避免了频繁交换模型参数的通信延迟问题，并通过模型参数丢弃而不是客户端选择来优化全局模型泛化能力。

    

    联邦学习（FL）需要频繁交换模型参数，这导致了长时间的通信延迟，尤其是当客户端的网络环境差异很大时。此外，参数服务器需要等待最慢的客户端（即straggler，可能具有最大的模型大小、最低的计算能力或最差的网络条件）上传参数，这可能会严重影响通信效率。常用的客户端选择方法，如部分客户端选择，会导致计算资源的浪费并削弱全局模型的泛化能力。为了解决这个问题，在本文中，我们提出了一种不同的方法，即使用模型参数丢弃而不是客户端选择，并据此提出了一种新的差分参数丢弃的联邦学习方案（FedDD）框架。FedDD包括两个关键模块：丢弃率分配和上传参数选择，将优化模型参数。

    Federated Learning (FL) requires frequent exchange of model parameters, which leads to long communication delay, especially when the network environments of clients vary greatly. Moreover, the parameter server needs to wait for the slowest client (i.e., straggler, which may have the largest model size, lowest computing capability or worst network condition) to upload parameters, which may significantly degrade the communication efficiency. Commonly-used client selection methods such as partial client selection would lead to the waste of computing resources and weaken the generalization of the global model. To tackle this problem, along a different line, in this paper, we advocate the approach of model parameter dropout instead of client selection, and accordingly propose a novel framework of Federated learning scheme with Differential parameter Dropout (FedDD). FedDD consists of two key modules: dropout rate allocation and uploaded parameter selection, which will optimize the model par
    
[^74]: 基于异步时空图卷积网络的不规则交通时间序列预测

    Irregular Traffic Time Series Forecasting Based on Asynchronous Spatio-Temporal Graph Convolutional Network. (arXiv:2308.16818v1 [cs.LG])

    [http://arxiv.org/abs/2308.16818](http://arxiv.org/abs/2308.16818)

    该论文提出了一种基于异步时空图卷积网络的不规则交通时间序列预测方法，用于解决智能交叉口产生的异步空间依赖、不规则时间依赖和可变长度序列预测等挑战。

    

    准确预测智能交通信号控制系统中受智能交叉口控制的交叉口的交通流量对于提升交通出行效率至关重要。然而，由于智能交叉口产生的交通时间序列不规则，交通流量预测任务变得更加困难，并且面临三个主要挑战：1）异步的空间依赖性，2）交通数据的不规则时间依赖性，3) 需要预测的可变长度序列，严重影响了当前交通流量预测方法的性能。为此，我们提出了一种异步时空图卷积网络(ASeer)来预测智能交叉口进入车道的交通状态。具体而言，通过在交通扩散图上连接车道，我们首先提出了一种异步图扩散网络来模拟车道的异步空间依赖性。

    Accurate traffic forecasting at intersections governed by intelligent traffic signals is critical for the advancement of an effective intelligent traffic signal control system. However, due to the irregular traffic time series produced by intelligent intersections, the traffic forecasting task becomes much more intractable and imposes three major new challenges: 1) asynchronous spatial dependency, 2) irregular temporal dependency among traffic data, and 3) variable-length sequence to be predicted, which severely impede the performance of current traffic forecasting methods. To this end, we propose an Asynchronous Spatio-tEmporal graph convolutional nEtwoRk (ASeer) to predict the traffic states of the lanes entering intelligent intersections in a future time window. Specifically, by linking lanes via a traffic diffusion graph, we first propose an Asynchronous Graph Diffusion Network to model the asynchronous spatial dependency between the time-misaligned traffic state measurements of la
    
[^75]: 鲁棒的网络化联邦学习在定位中的应用

    Robust Networked Federated Learning for Localization. (arXiv:2308.16737v1 [cs.LG])

    [http://arxiv.org/abs/2308.16737](http://arxiv.org/abs/2308.16737)

    本文提出了一种鲁棒的网络化联邦学习方法，通过采用$L_1$-范数鲁棒性和分布式次梯度框架，解决了在分布式环境中定位问题中的异常数据干扰和算法收敛挑战。

    

    本文解决了在数据分布在多设备上的联邦环境中，本质上是非凸非光滑的定位问题。由于联邦环境的分散性质，分布式学习成为可伸缩性和适应性的关键。此外，这些环境经常受到异常数据的干扰，使得传统方法在维护估计精度和确保算法收敛方面面临重大挑战。为了解决这些挑战，我们提出了一种采用分布式次梯度框架中$L_1$-范数鲁棒性的方法，专门设计用于处理这些障碍。我们的方法以原始形式解决问题，而不是采用迭代简化或近似方法，从而提高计算效率和估计精度。我们证明了我们的方法收敛到一个稳定点，突出了其有效性。

    This paper addresses the problem of localization, which is inherently non-convex and non-smooth in a federated setting where the data is distributed across a multitude of devices. Due to the decentralized nature of federated environments, distributed learning becomes essential for scalability and adaptability. Moreover, these environments are often plagued by outlier data, which presents substantial challenges to conventional methods, particularly in maintaining estimation accuracy and ensuring algorithm convergence. To mitigate these challenges, we propose a method that adopts an $L_1$-norm robust formulation within a distributed sub-gradient framework, explicitly designed to handle these obstacles. Our approach addresses the problem in its original form, without resorting to iterative simplifications or approximations, resulting in enhanced computational efficiency and improved estimation accuracy. We demonstrate that our method converges to a stationary point, highlighting its effec
    
[^76]: 材料信息学变压器：一种用于可解释材料性能预测的语言模型

    Materials Informatics Transformer: A Language Model for Interpretable Materials Properties Prediction. (arXiv:2308.16259v1 [cs.LG])

    [http://arxiv.org/abs/2308.16259](http://arxiv.org/abs/2308.16259)

    本研究提出了一种名为材料信息学变压器（MatInFormer）的语言模型，通过学习晶体学语法和引入MOFs数据，实现了对材料性能的准确预测，并通过注意力可视化揭示了模型的关键特征。

    

    最近，大型语言模型(LLMs)在自然语言处理、计算机视觉和分子建模等多个研究领域展示了显著的能力。我们通过引入我们的模型材料信息学变压器(MatInFormer)，将LLMs的这种范式扩展到材料性能预测领域。具体而言，我们引入了一种新颖的方法，通过对相关的空间群信息进行标记化，学习了晶体学的语法。我们进一步展示了MatInFormer的适应性，通过 incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs)。通过注意力可视化，我们揭示了模型在属性预测过程中优先考虑的关键特征。我们的提议模型在14个不同的数据集中经过实证验证，从而强调了其在通过准确的材料属性预测进行高通量筛选方面的潜力。

    Recently, the remarkable capabilities of large language models (LLMs) have been illustrated across a variety of research domains such as natural language processing, computer vision, and molecular modeling. We extend this paradigm by utilizing LLMs for material property prediction by introducing our model Materials Informatics Transformer (MatInFormer). Specifically, we introduce a novel approach that involves learning the grammar of crystallography through the tokenization of pertinent space group information. We further illustrate the adaptability of MatInFormer by incorporating task-specific data pertaining to Metal-Organic Frameworks (MOFs). Through attention visualization, we uncover the key features that the model prioritizes during property prediction. The effectiveness of our proposed model is empirically validated across 14 distinct datasets, hereby underscoring its potential for high throughput screening through accurate material property prediction.
    
[^77]: 回归问题的校准解释

    Calibrated Explanations for Regression. (arXiv:2308.16245v1 [cs.LG])

    [http://arxiv.org/abs/2308.16245](http://arxiv.org/abs/2308.16245)

    本文介绍了一种针对回归问题的特征重要性解释方法的扩展，可以量化特征重要性的不确定性。

    

    人工智能（AI）通常是现代决策支持系统（DSS）的一部分。在基于AI的DSS中使用的最佳预测模型缺乏透明度。可解释的人工智能（XAI）旨在创建能够向人类用户解释其理由的AI系统。XAI中的局部解释可以提供关于个别预测的原因的信息，即特征重要性。然而，现有局部解释方法的一个关键缺点是无法量化与特征重要性相关的不确定性。本文介绍了特征重要性解释方法Calibrated Explanations（CE）的扩展，之前只支持分类，现在支持标准回归和概率回归，即目标超过任意阈值的概率。回归问题的扩展保留了CE的所有优点，例如将底层模型的预测与置信度校准。

    Artificial Intelligence (AI) is often an integral part of modern decision support systems (DSSs). The best-performing predictive models used in AI-based DSSs lack transparency. Explainable Artificial Intelligence (XAI) aims to create AI systems that can explain their rationale to human users. Local explanations in XAI can provide information about the causes of individual predictions in terms of feature importance. However, a critical drawback of existing local explanation methods is their inability to quantify the uncertainty associated with a feature's importance. This paper introduces an extension of a feature importance explanation method, Calibrated Explanations (CE), previously only supporting classification, with support for standard regression and probabilistic regression, i.e., the probability that the target is above an arbitrary threshold. The extension for regression keeps all the benefits of CE, such as calibration of the prediction from the underlying model with confidenc
    
[^78]: 高效且可解释的图神经网络架构搜索通过蒙特卡洛树搜索

    Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search. (arXiv:2308.15734v1 [cs.LG])

    [http://arxiv.org/abs/2308.15734](http://arxiv.org/abs/2308.15734)

    该论文提出了一种高效且可解释的图神经网络架构搜索方法，名为ExGNAS。它包括适应各种图形的简单搜索空间和能解释决策过程的搜索算法。通过蒙特卡洛树搜索高效地搜索最佳GNN架构。

    

    图神经网络（GNNs）是在各个领域进行数据科学任务的强大工具。尽管我们在广泛的应用场景中使用GNNs，但对研究人员和实践者来说，在不同的图中设计/选择最佳GNN架构是一项费力的任务。为了节省人力和计算成本，已经使用图神经网络架构搜索（Graph NAS）来搜索结合现有组件的次优GNN架构。然而，目前没有现有的Graph NAS方法能够同时满足可解释性、高效性和适应多样化图形的要求。因此，我们提出了一种高效且可解释的Graph NAS方法，称为ExGNAS，它包括（i）一个可以适应各种图形的简单搜索空间和（ii）一个能够解释决策过程的搜索算法。搜索空间仅包含可以处理同质和异质图的基本函数。搜索算法通过蒙特卡洛树搜索高效地搜索最佳GNN架构。

    Graph neural networks (GNNs) are powerful tools for performing data science tasks in various domains. Although we use GNNs in wide application scenarios, it is a laborious task for researchers and practitioners to design/select optimal GNN rchitectures in diverse graphs. To save human efforts and computational costs, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, there are no existing Graph NAS methods that satisfy explainability, efficiency, and adaptability to various graphs. Therefore, we propose an efficient and explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm that makes the decision process explainable. The search space includes only fundamental functions that can handle homophilic and heterophilic graphs. The search algorithm efficiently searches for the best GNN architecture via M
    
[^79]: 论马尔可夫决策过程的奖励结构

    On Reward Structures of Markov Decision Processes. (arXiv:2308.14919v1 [cs.LG])

    [http://arxiv.org/abs/2308.14919](http://arxiv.org/abs/2308.14919)

    该论文研究了马尔可夫决策过程中的奖励结构，提出了一种估计器用于估计单个状态值，并通过根据奖励代替常用的基于转移的常数，提供了对强化学习中技巧的理论解释。

    

    马尔可夫决策过程可以通过转移核与奖励函数参数化。这两个因素在强化学习研究中起着重要作用，正如它们在贝尔曼方程中的存在所证明的那样。针对机器人应用中的需求，我们研究了与强化学习相关的各种"成本"，奖励是理解马尔可夫决策过程结构的核心，奖励中心概念可以阐明强化学习中的重要概念。具体而言，我们研究了策略评估的样本复杂性，并开发了一种新的估计器，其实例特定的误差界为$\tilde{O}(\sqrt{\frac{\tau_s}{n}})$，用于估计单个状态值。在在线遗憾最小化设置下，我们将基于转移的MDP常数，直径，改进为基于奖励的常数，最大预期到达成本，并通过该常数为一种广为人知的技术，基于潜力的奖励形状提供了理论解释。

    A Markov decision process can be parameterized by a transition kernel and a reward function. Both play essential roles in the study of reinforcement learning as evidenced by their presence in the Bellman equations. In our inquiry of various kinds of ``costs'' associated with reinforcement learning inspired by the demands in robotic applications, rewards are central to understanding the structure of a Markov decision process and reward-centric notions can elucidate important concepts in reinforcement learning. Specifically, we studied the sample complexity of policy evaluation and developed a novel estimator with an instance-specific error bound of $\tilde{O}(\sqrt{\frac{\tau_s}{n}})$ for estimating a single state value. Under the online regret minimization setting, we refined the transition-based MDP constant, diameter, into a reward-based constant, maximum expected hitting cost, and with it, provided a theoretical explanation for how a well-known technique, potential-based reward shap
    
[^80]: 自助的鲁棒性表示学习的独立子网络多样化集成

    Diversified Ensemble of Independent Sub-Networks for Robust Self-Supervised Representation Learning. (arXiv:2308.14705v1 [stat.ML])

    [http://arxiv.org/abs/2308.14705](http://arxiv.org/abs/2308.14705)

    本文提出了一个新的自助培训体制，利用独立子网络的集成和新的损失函数来提高自助的鲁棒性表示学习的效率和多样性。

    

    集成神经网络是提高模型性能、估计不确定性和改善深度有监督学习鲁棒性的广泛承认方法。然而，深层集成通常具有高计算成本和内存需求。此外，深度集成的效率与集成成员之间的多样性有关，这对于大型的过参数化深度神经网络来说是具有挑战性的。而且，集成学习尚未得到如此广泛的采用，并且对于自助的或无监督的表示学习来说仍然是一项具有挑战性的工作。在这些挑战的推动下，我们提出了一个新颖的自助培训体制，利用独立子网络的集成，辅以一个旨在鼓励多样性的新损失函数。我们的方法以高多样性实现了高效的子模型集成，从而获得了良好校准的模型不确定性估计，并与传统方法相比，计算开销最小。

    Ensembling a neural network is a widely recognized approach to enhance model performance, estimate uncertainty, and improve robustness in deep supervised learning. However, deep ensembles often come with high computational costs and memory demands. In addition, the efficiency of a deep ensemble is related to diversity among the ensemble members which is challenging for large, over-parameterized deep neural networks. Moreover, ensemble learning has not yet seen such widespread adoption, and it remains a challenging endeavor for self-supervised or unsupervised representation learning. Motivated by these challenges, we present a novel self-supervised training regime that leverages an ensemble of independent sub-networks, complemented by a new loss function designed to encourage diversity. Our method efficiently builds a sub-model ensemble with high diversity, leading to well-calibrated estimates of model uncertainty, all achieved with minimal computational overhead compared to traditional
    
[^81]: FAM：快速自适应元学习

    FAM: fast adaptive meta-learning. (arXiv:2308.13970v1 [cs.LG])

    [http://arxiv.org/abs/2308.13970](http://arxiv.org/abs/2308.13970)

    本论文提出了一个快速自适应联邦元学习（FAM）框架，可以协作学习一个全局模型，并在个别客户端上进行个性化。这解决了数据分布发散和隐私限制的问题，并且适用于需要在不同客户端之间进行个性化的领域转变。

    

    在这项工作中，我们提出了一个快速自适应联邦元学习（FAM）框架，用于协作学习一个单一全局模型，然后可以在个别客户端上个性化。联邦学习使多个客户端能够协作训练模型而不共享数据。参与联邦学习的客户端由于数据不足或数据多样性导致学习受到影响。然而，当数据分布发散时，学习会受到困扰。有必要学习一个可以使用客户端特定信息进行自适应的全局模型，并在客户端上创建个性化模型。MRI数据存在这个问题，第一，由于数据采集挑战，在某个地点的本地数据足以训练准确的模型，第二，由于隐私问题有数据共享限制，第三，由于客户端站点之间的领域转变，需要对学习的共享全局模型进行个性化。

    In this work, we propose a fast adaptive federated meta-learning (FAM) framework for collaboratively learning a single global model, which can then be personalized locally on individual clients. Federated learning enables multiple clients to collaborate to train a model without sharing data. Clients with insufficient data or data diversity participate in federated learning to learn a model with superior performance. Nonetheless, learning suffers when data distributions diverge. There is a need to learn a global model that can be adapted using client's specific information to create personalised models on clients is required. MRI data suffers from this problem, wherein, one, due to data acquisition challenges, local data at a site is sufficient for training an accurate model and two, there is a restriction of data sharing due to privacy concerns and three, there is a need for personalization of a learnt shared global model on account of domain shift across client sites. The global model
    
[^82]: 工业人工智能中的随机配置机

    Stochastic Configuration Machines for Industrial Artificial Intelligence. (arXiv:2308.13570v1 [cs.LG])

    [http://arxiv.org/abs/2308.13570](http://arxiv.org/abs/2308.13570)

    本文提出了一种新颖的随机学习器模型，称为随机配置机（SCMs），其基于随机配置网络（SCNs），旨在强调工业人工智能中的有效建模和节约数据大小。SCMs通过压缩模型存储，并保持有利的预测性能，具有在工业应用中很大的潜力。

    

    在工业人工智能（IAI）中，需要实时、准确的预测建模，神经网络在其中起到关键作用。工业人工智能中的神经网络需要强大的高性能计算设备来处理大量的浮点数据。本文基于随机配置网络（SCNs），提出了一种新的随机学习器模型，称为随机配置机（SCMs），以强调对于工业应用非常有用和有价值的有效建模和节约数据大小。与具有二值化实现的随机向量功能链接（RVFL）网络相比，SCMs的模型存储可以显著压缩，同时保持有利的预测性能。除了SCM学习器模型的架构和学习算法，作为本文的重要部分，我们还通过分析模型的复杂性提供了SCMs的学习能力的理论基础。实验研究也进行了。

    Real-time predictive modelling with desired accuracy is highly expected in industrial artificial intelligence (IAI), where neural networks play a key role. Neural networks in IAI require powerful, high-performance computing devices to operate a large number of floating point data. Based on stochastic configuration networks (SCNs), this paper proposes a new randomized learner model, termed stochastic configuration machines (SCMs), to stress effective modelling and data size saving that are useful and valuable for industrial applications. Compared to SCNs and random vector functional-link (RVFL) nets with binarized implementation, the model storage of SCMs can be significantly compressed while retaining favourable prediction performance. Besides the architecture of the SCM learner model and its learning algorithm, as an important part of this contribution, we also provide a theoretical basis on the learning capacity of SCMs by analysing the model's complexity. Experimental studies are ca
    
[^83]: 人类可理解的基因组规模代谢网络的主动学习

    Human Comprehensible Active Learning of Genome-Scale Metabolic Networks. (arXiv:2308.12740v1 [cs.AI])

    [http://arxiv.org/abs/2308.12740](http://arxiv.org/abs/2308.12740)

    这项研究介绍了一种人类可理解的基因组规模代谢网络的主动学习方法，基于归纳逻辑编程(ILP)框架进行逻辑推理，并通过从实验中学习新的逻辑结构，以有效探索假设空间和指导实验设计。

    

    合成生物学的一个重要应用是将宿主细胞系统工程化以产生有用的产品。然而，宿主系统规模的增加导致设计空间巨大，并需要大量高昂的验证试验。为了宿主细胞系统的设计-构建-测试-学习（Design-Build-Test-Learn，DBTL）周期，迫切需要一种能有效探索假设空间并指导实验设计的可理解的机器学习方法。我们引入了一种基于归纳逻辑编程（ILP）的新型机器学习框架ILP-iML1515，它通过诱导逻辑推理和从训练实例中积极学习来执行说明性的逻辑推理。与数值模型不同，ILP-iML1515建立在对基因组规模代谢模型的可理解的逻辑表示上，并可以通过从缺乏营养的突变体试验中学习新的逻辑结构来更新模型。ILP-iML1515框架具有高通量模拟能力，并能主动选择实验。

    An important application of Synthetic Biology is the engineering of the host cell system to yield useful products. However, an increase in the scale of the host system leads to huge design space and requires a large number of validation trials with high experimental costs. A comprehensible machine learning approach that efficiently explores the hypothesis space and guides experimental design is urgently needed for the Design-Build-Test-Learn (DBTL) cycle of the host cell system. We introduce a novel machine learning framework ILP-iML1515 based on Inductive Logic Programming (ILP) that performs abductive logical reasoning and actively learns from training examples. In contrast to numerical models, ILP-iML1515 is built on comprehensible logical representations of a genome-scale metabolic model and can update the model by learning new logical structures from auxotrophic mutant trials. The ILP-iML1515 framework 1) allows high-throughput simulations and 2) actively selects experiments that 
    
[^84]: 简约多任务模型-使用结构稀疏性实现简洁的多任务模型

    Less is More -- Towards parsimonious multi-task models using structured sparsity. (arXiv:2308.12114v1 [cs.CV])

    [http://arxiv.org/abs/2308.12114](http://arxiv.org/abs/2308.12114)

    该论文研究了将结构稀疏性引入多任务学习框架，开发了一种简洁的模型，可以用较少的参数有效地处理多个任务，并在性能上与密集模型相当或更优。通过在训练期间对模型进行稀疏化，可以减少内存占用、计算需求和预测时间。具体而言，该研究通过在卷积神经网络的共享层中使用通道级l1/l2组稀疏，同时消除多余的组（通道）并对权重施加惩罚，提高了所有任务的学习能力。

    

    机器学习中的组稀疏性鼓励更简单、更可解释的模型，具有较少的活跃参数组。本研究旨在将结构化组稀疏性纳入多任务学习框架的共享参数，开发简洁的模型，它可以有效地处理多个任务，同时减少参数，而保持与密集模型相当或更高的性能。在训练过程中对模型进行稀疏化有助于减少模型的内存占用、计算需求和预测时间。我们在卷积神经网络的共享层中使用通道级l1/l2组稀疏。此方法不仅有助于消除多余的组（通道），还对权重施加惩罚，从而增强所有任务的学习能力。我们在两个公开可用的多任务学习数据集NYU-v2和CelebAMask-HQ上比较了组稀疏性下单任务和多任务实验的结果。

    Group sparsity in Machine Learning (ML) encourages simpler, more interpretable models with fewer active parameter groups. This work aims to incorporate structured group sparsity into the shared parameters of a Multi-Task Learning (MTL) framework, to develop parsimonious models that can effectively address multiple tasks with fewer parameters while maintaining comparable or superior performance to a dense model. Sparsifying the model during training helps decrease the model's memory footprint, computation requirements, and prediction time during inference. We use channel-wise l1/l2 group sparsity in the shared layers of the Convolutional Neural Network (CNN). This approach not only facilitates the elimination of extraneous groups (channels) but also imposes a penalty on the weights, thereby enhancing the learning of all tasks. We compare the outcomes of single-task and multi-task experiments under group sparsity on two publicly available MTL datasets, NYU-v2 and CelebAMask-HQ. We also i
    
[^85]: 有效的语言模型基准测试

    Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])

    [http://arxiv.org/abs/2308.11696](http://arxiv.org/abs/2308.11696)

    本研究提出了一种名为"Efficient Benchmarking"的问题，旨在智能地减少语言模型评估的计算成本而不降低可靠性，并使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估决策的可靠性。通过HELM基准测试的案例研究，发现只需删除一个低排名模型即可改变领先者，并仅需少量示例即可得到正确的基准测试排名。

    

    语言模型的多功能性增加导致了一类全面评估广泛能力的基准测试的出现。这些基准测试与大规模计算成本相关，每个模型需要数千个GPU小时。然而，关于评估效率方面的问题在文献中讨论较少。本文提出了一种名为"Efficient Benchmarking"的问题，即在不损害可靠性的情况下智能地减少语言模型评估的计算成本。通过使用HELM基准测试作为示例，我们研究了不同基准测试设计选择如何影响计算-可靠性权衡。我们提出使用一种名为Decision Impact on Reliability（DIoR）的新度量来评估这些决策的可靠性。例如，我们发现仅通过从基准测试中删除一个低排名模型，当前在HELM上的领先者可能会改变，并且观察到只需一小部分示例即可获得正确的基准测试排名。

    The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
    
[^86]: 测试时间嵌入归一化对热门偏见的缓解

    Test Time Embedding Normalization for Popularity Bias Mitigation. (arXiv:2308.11288v1 [cs.IR])

    [http://arxiv.org/abs/2308.11288](http://arxiv.org/abs/2308.11288)

    本文提出了一种名为“测试时间嵌入归一化”的策略来解决推荐系统中的热门偏见问题。该方法利用归一化的物品嵌入来控制嵌入大小，并通过与用户和物品嵌入的角度相似度区分受欢迎和不受欢迎的物品，从而有效减少了热门偏见的影响。

    

    热门偏见是推荐系统领域普遍存在的问题，其中热门物品倾向于主导推荐结果。在这项工作中，我们提出了“测试时间嵌入归一化”作为一种简单而有效的策略来缓解热门偏见，其性能超过了以往的缓解方法。我们的方法在推理阶段利用归一化的物品嵌入来控制嵌入的大小，而嵌入的大小与物品的流行度高度相关。通过大量实验证明，我们的方法结合采样softmax损失相比以前的方法更有效地减少了热门偏见的影响。我们进一步研究了用户和物品嵌入之间的关系，并发现嵌入之间的角度相似度可以区分受欢迎和不受欢迎的物品，而不考虑它们的流行程度。这一分析解释了我们方法成功的机制。

    Popularity bias is a widespread problem in the field of recommender systems, where popular items tend to dominate recommendation results. In this work, we propose 'Test Time Embedding Normalization' as a simple yet effective strategy for mitigating popularity bias, which surpasses the performance of the previous mitigation approaches by a significant margin. Our approach utilizes the normalized item embedding during the inference stage to control the influence of embedding magnitude, which is highly correlated with item popularity. Through extensive experiments, we show that our method combined with the sampled softmax loss effectively reduces popularity bias compare to previous approaches for bias mitigation. We further investigate the relationship between user and item embeddings and find that the angular similarity between embeddings distinguishes preferable and non-preferable items regardless of their popularity. The analysis explains the mechanism behind the success of our approac
    
[^87]: 在线持续学习的综合实证评估

    A Comprehensive Empirical Evaluation on Online Continual Learning. (arXiv:2308.10328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10328](http://arxiv.org/abs/2308.10328)

    这项综合实证评估了解决在线持续学习问题的各种方法，发现大多数方法存在稳定性和欠拟合问题，但所学习的表示与独立同分布的训练相当。

    

    在线持续学习旨在通过在数据流上直接学习，处理时间变化的分布，并仅存储一小部分数据，以更接近实时学习体验。在这个实证评估中，我们评估了文献中解决在线持续学习问题的各种方法。具体而言，我们在图像分类的类增量设置下，从数据流中逐步学习新的类别。我们在Split-CIFAR100和Split-TinyImagenet基准上比较了这些方法，并测量它们的平均准确性、遗忘率、稳定性和表示质量，以评估算法在整个训练过程中的多个方面。我们发现大多数方法都存在稳定性和欠拟合问题。然而，在相同的计算预算下，所学习的表示与独立同分布的训练相当。没有明确的优胜者从重新评估后的结果中出现。

    Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the re
    
[^88]: 激活添加: 无需优化即可操纵语言模型

    Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10248](http://arxiv.org/abs/2308.10248)

    这项研究探讨了一种在推理时通过改变激活来预测性地改变语言模型行为的方法，并且相比于传统方法具有更低的计算和实施成本，并且能够保持模型性能。

    

    可靠地控制大型语言模型的行为是一个紧迫的开放性问题。现有的方法包括有监督微调、根据人类反馈进行强化学习、提示工程和引导解码。我们相反，研究了激活工程：在推理时修改激活以可预测地改变模型行为。特别地，我们通过自然语言隐式指定了一个添加的“导向向量”来偏置前向传播。与以前学习这些导向向量的工作不同，我们的激活添加（ActAdd）方法通过计算来自提示对的激活差异来计算它们。我们在OpenWebText和ConceptNet上展示了ActAdd在GPT-2上的应用。我们的推理时方法控制了输出的高级属性并保持了非目标模型的性能。它所需的计算和实施工作比微调要少得多，允许用户提供自然语言的规范，并且其开销与模型规模自然地扩展。

    Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
    
[^89]: 神经网络量子态研究长程反铁磁伊辛链

    Neural-network quantum state study of the long-range antiferromagnetic Ising chain. (arXiv:2308.09709v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2308.09709](http://arxiv.org/abs/2308.09709)

    在这项研究中，我们使用变分蒙特卡洛方法和约束玻尔兹曼机作为试验波函数参数化，研究了具有代数衰减长程反铁磁相互作用的横向场伊辛链中的量子相变。通过有限大小缩放分析，我们发现中心荷的值与衰变指数$\alpha_\mathrm{LR}$不同，而临界指数则保持接近短程伊辛模型的值，这支持了先前提出的共形不变性破缺场景。

    

    我们利用变分蒙特卡洛方法研究具有代数衰减长程反铁磁相互作用的横向场伊辛链中的量子相变，采用约束玻尔兹曼机作为试验波函数参数化。在有限大小缩放分析中，通过序参数和第二个Renyi熵，我们发现中心荷与小的衰变指数$\alpha_\mathrm{LR}$不同，偏离了1/2，而临界指数则无论受到$\alpha_\mathrm{LR}$的影响如何，都保持非常接近短程（SR）伊辛模型的值，支持先前提出的共形不变性破缺场景。为了确定伊辛宇宙性和共形对称性的临界点，我们对普适Binder比和相关函数的共形场论（CFT）描述进行了两项附加测试。结果发现，在$\alpha_\mat

    We investigate quantum phase transitions in the transverse field Ising chain with algebraically decaying long-range antiferromagnetic interactions by using the variational Monte Carlo method with the restricted Boltzmann machine being employed as a trial wave function ansatz. In the finite-size scaling analysis with the order parameter and the second R\'enyi entropy, we find that the central charge deviates from 1/2 at a small decay exponent $\alpha_\mathrm{LR}$ in contrast to the critical exponents staying very close to the short-range (SR) Ising values regardless of $\alpha_\mathrm{LR}$ examined, supporting the previously proposed scenario of conformal invariance breakdown. To identify the threshold of the Ising universality and the conformal symmetry, we perform two additional tests for the universal Binder ratio and the conformal field theory (CFT) description of the correlation function. It turns out that both indicate a noticeable deviation from the SR Ising class at $\alpha_\mat
    
[^90]: 图结构残差：一种诊断的学习方法

    Graph Structural Residuals: A Learning Approach to Diagnosis. (arXiv:2308.06961v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2308.06961](http://arxiv.org/abs/2308.06961)

    本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合，通过数据学习系统的底层结构并提供动态观察。研究通过重新定义系统表示、观察和故障的构建，引入自监督图结构学习模型以及在耦合振荡器系统上的实验，展示了数据驱动的诊断方法的潜力。

    

    传统的基于模型的诊断依赖于构建明确的系统模型，这个过程可能费时且需要专业知识。本文提出了一种新颖的框架，将模型诊断的概念与深度图结构学习相结合。这种数据驱动的方法利用数据学习系统的底层结构，并提供由两个不同的图邻接矩阵表示的动态观察。我们的工作通过三个主要贡献实现了图结构学习与模型诊断的无缝集成：(i)重新定义系统表示、观察和故障的构建、(ii)引入两种不同版本的自监督图结构学习模型架构、(iii)通过耦合振荡器系统的实验展示了我们数据驱动的诊断方法的潜力。

    Traditional model-based diagnosis relies on constructing explicit system models, a process that can be laborious and expertise-demanding. In this paper, we propose a novel framework that combines concepts of model-based diagnosis with deep graph structure learning. This data-driven approach leverages data to learn the system's underlying structure and provide dynamic observations, represented by two distinct graph adjacency matrices. Our work facilitates a seamless integration of graph structure learning with model-based diagnosis by making three main contributions: (i) redefining the constructs of system representation, observations, and faults (ii) introducing two distinct versions of a self-supervised graph structure learning model architecture and (iii) demonstrating the potential of our data-driven diagnostic method through experiments on a system of coupled oscillators.
    
[^91]: Tango: 重新思考在GPU上对图神经网络训练的量化方法

    Tango: rethinking quantization for graph neural network training on GPUs. (arXiv:2308.00890v1 [cs.LG])

    [http://arxiv.org/abs/2308.00890](http://arxiv.org/abs/2308.00890)

    Tango是一个重新思考在GPU上对图神经网络训练的量化方法的研究，通过引入新的规则来保持准确度，设计并实现量化感知的基本操作和基本操作之间的优化，以加速GNN训练。

    

    图神经网络（GNN）因其在关键的图相关任务中表现出色而越来越受欢迎。虽然量化被广泛用于加速GNN计算，但量化训练面临前所未有的挑战。当前的量化GNN训练系统往往比其全精度对应物有更长的训练时间，原因有二：（一）解决准确度问题导致了过多的开销，（二）没有充分发挥量化所展示的优化潜力。本文介绍了Tango，它重新思考了在GPU上进行图神经网络训练的量化挑战和机会，并做出了三个贡献：首先，我们引入了有效的规则来在量化GNN训练过程中保持准确度。其次，我们设计并实现了量化感知的基本操作和基本操作之间的优化，可以加速GNN训练。最后，我们将Tango与流行的Deep Graph Library（DGL）系统集成，并展示其

    Graph Neural Networks (GNNs) are becoming increasingly popular due to their superior performance in critical graph-related tasks. While quantization is widely used to accelerate GNN computation, quantized training faces unprecedented challenges. Current quantized GNN training systems often have longer training times than their full-precision counterparts for two reasons: (i) addressing the accuracy challenge leads to excessive overhead, and (ii) the optimization potential exposed by quantization is not adequately leveraged. This paper introduces Tango which re-thinks quantization challenges and opportunities for graph neural network training on GPUs with three contributions: Firstly, we introduce efficient rules to maintain accuracy during quantized GNN training. Secondly, we design and implement quantization-aware primitives and inter-primitive optimizations that can speed up GNN training. Finally, we integrate Tango with the popular Deep Graph Library (DGL) system and demonstrate its
    
[^92]: 基于有理核的复频率响应函数插值

    Rational kernel-based interpolation for complex-valued frequency response functions. (arXiv:2307.13484v1 [cs.CE])

    [http://arxiv.org/abs/2307.13484](http://arxiv.org/abs/2307.13484)

    本论文提出了一种基于有理核的复频率响应函数插值方法，通过引入新的复值函数再生核希尔伯特空间，并结合低阶有理函数进行自适应插值，解决了频率响应函数拟合过程中标准核方法表现不佳的问题。

    

    本论文研究了基于核的逼近方法在复值函数数据中的应用，其中特别关注频域中的偏微分方程的频率响应函数。在这个设置中，核方法越来越常用，然而标准的核方法表现不佳。此外，在复值情况下，底层核对的数学含义和数学推导尚待解决。我们引入了复值函数的再生核希尔伯特空间，并将带有核对的复值插值问题转化为这些空间中的最小范数插值问题。此外，我们将插值器与低阶有理函数结合，其中阶数根据一种新的模型选择准则自适应选择。来自不同领域（包括电磁学和声学）的例子的数值结果说明了该方法的性能。

    This work is concerned with the kernel-based approximation of a complex-valued function from data, where the frequency response function of a partial differential equation in the frequency domain is of particular interest. In this setting, kernel methods are employed more and more frequently, however, standard kernels do not perform well. Moreover, the role and mathematical implications of the underlying pair of kernels, which arises naturally in the complex-valued case, remain to be addressed. We introduce new reproducing kernel Hilbert spaces of complex-valued functions, and formulate the problem of complex-valued interpolation with a kernel pair as minimum norm interpolation in these spaces. Moreover, we combine the interpolant with a low-order rational function, where the order is adaptively selected based on a new model selection criterion. Numerical results on examples from different fields, including electromagnetics and acoustic examples, illustrate the performance of the metho
    
[^93]: CLIPAG: 走向无需生成器的文本到图像生成

    CLIPAG: Towards Generator-Free Text-to-Image Generation. (arXiv:2306.16805v1 [cs.CV])

    [http://arxiv.org/abs/2306.16805](http://arxiv.org/abs/2306.16805)

    本文将感知对齐梯度（PAG）的研究扩展到视觉-语言架构，并通过对 CLIP 进行鲁棒性调整，展示了在视觉-语言生成任务中集成 CLIPAG 可以实现显著改进，并实现了无生成器的文本到图像生成。

    

    感知对齐梯度 (Perceptually Aligned Gradients, PAG) 是在健壮的图像分类模型中观察到的一种有趣属性，其中它们的输入渐变与人类感知对齐并具有语义意义。虽然这一现象引起了显着的研究关注，但仅仅在单模态纯视觉架构的背景下进行了研究。在本研究中，我们将 PAG 的研究扩展到视觉-语言架构，这是多样化的图像-文本任务和应用的基础。通过对 CLIP 进行对抗性鲁棒微调，我们证明了鲁棒的视觉-语言模型相对于其基准模型表现出了 PAG。这项工作展示了 CLIPAG 在几种视觉-语言生成任务中的优势。值得注意的是，我们展示了无缝集成 CLIPAG 的 "即插即用" 方式显著改进了视觉-语言生成应用。此外，利用其 PAG 属性，CLIPAG 实现了无生成器的文本到图像生成。

    Perceptually Aligned Gradients (PAG) refer to an intriguing property observed in robust image classification models, wherein their input gradients align with human perception and pose semantic meanings. While this phenomenon has gained significant research attention, it was solely studied in the context of unimodal vision-only architectures. In this work, we extend the study of PAG to Vision-Language architectures, which form the foundations for diverse image-text tasks and applications. Through an adversarial robustification finetuning of CLIP, we demonstrate that robust Vision-Language models exhibit PAG in contrast to their vanilla counterparts. This work reveals the merits of CLIP with PAG (CLIPAG) in several vision-language generative tasks. Notably, we show that seamlessly integrating CLIPAG in a "plug-n-play" manner leads to substantial improvements in vision-language generative applications. Furthermore, leveraging its PAG property, CLIPAG enables text-to-image generation witho
    
[^94]: 基于偏好的强化学习中的公平性

    Fairness in Preference-based Reinforcement Learning. (arXiv:2306.09995v1 [cs.LG])

    [http://arxiv.org/abs/2306.09995](http://arxiv.org/abs/2306.09995)

    该论文提出了一种名为FPbRL的新的公平偏好强化学习方法，旨在通过广义Gini福利函数最大化策略学习来实现多目标优化并处理每个目标的公平性。

    

    本文研究了在多目标情况下偏好强化学习(PbRL)中的公平性问题。主要目标是设计控制策略，既能够优化多个目标，又能够公平地处理每个目标。为实现这一目标，我们设计了一种新的公平偏好强化学习(FPbRL)方法。FPbRL的主要思想是通过新的福利偏好而不是PbRL中的基于奖励的偏好来学习与多目标关联的向量奖励函数，并通过最大化广义Gini福利函数进行策略学习。最后，在三个不同的环境上进行实验研究，展示了所提出的FPbRL方法能够实现有效和公平的控制策略的学习。

    In this paper, we address the issue of fairness in preference-based reinforcement learning (PbRL) in the presence of multiple objectives. The main objective is to design control policies that can optimize multiple objectives while treating each objective fairly. Toward this objective, we design a new fairness-induced preference-based reinforcement learning or FPbRL. The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL, coupled with policy learning via maximizing a generalized Gini welfare function. Finally, we provide experiment studies on three different environments to show that the proposed FPbRL approach can achieve both efficiency and equity for learning effective and fair policies.
    
[^95]: 移动机器人全身操作的因果策略梯度

    Causal Policy Gradient for Whole-Body Mobile Manipulation. (arXiv:2305.04866v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2305.04866](http://arxiv.org/abs/2305.04866)

    本文提出了一种新框架——因果MoMa，可以训练适用于典型MoMa任务的策略，在此框架下，机动和交互自由度可以同时组合，并且不需要人类领域知识来划分动作空间或将动作部分与子目标匹配。

    

    开发下一代家庭机器人助手需要结合机动和交互能力，即通常所说的移动操作。由于机器人的大动作空间和任务常见的多目标性质，例如能够有效地达到目标且避免障碍，移动操作任务很难。目前的方法通常根据人工匹配动作空间的部分到移动操作子目标（例如用于移动目标的基础动作和用于操作的手臂动作）将任务分为不带操作的导航和不带机动的固定操作。此解决方案防止了机动和交互自由度的同时组合，并且需要人类领域知识来划分动作空间并将动作部分与子目标匹配。在本文中，我们介绍了一种新的框架——因果MoMa，该框架用于训练典型MoMa任务的策略。

    Developing the next generation of household robot helpers requires combining locomotion and interaction capabilities, which is generally referred to as mobile manipulation (MoMa). MoMa tasks are difficult due to the large action space of the robot and the common multi-objective nature of the task, e.g., efficiently reaching a goal while avoiding obstacles. Current approaches often segregate tasks into navigation without manipulation and stationary manipulation without locomotion by manually matching parts of the action space to MoMa sub-objectives (e.g. base actions for locomotion objectives and arm actions for manipulation). This solution prevents simultaneous combinations of locomotion and interaction degrees of freedom and requires human domain knowledge for both partitioning the action space and matching the action parts to the sub-objectives. In this paper, we introduce Causal MoMa, a new framework to train policies for typical MoMa tasks that makes use of the most favorable subsp
    
[^96]: 多粒度时间变换器用于知识追踪

    Multi-granulariy Time-based Transformer for Knowledge Tracing. (arXiv:2304.05257v1 [cs.LG])

    [http://arxiv.org/abs/2304.05257](http://arxiv.org/abs/2304.05257)

    本文提出了一种基于Transformer的架构用于准确地预测学生在标准化测试中的表现。该模型考虑了学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，并在解码器输入中使用了多个时间特征粒度以显著提高模型性能。与LightGBM相比，该方法更加准确，为教育领域的AI发展提供了一个可伸缩和准确的预测学生成果的工具。

    

    本文提出了一种基于Transformer的架构，用于预测标准化测试中学生的表现。具体来说，我们利用学生的历史数据，包括他们以往的考试成绩、学习习惯和其他相关信息，为每个学生创建一个个性化的模型。然后，我们使用这些模型来预测学生在给定测试中的未来表现。将该模型应用于RIIID数据集，我们证明使用多个时间特征粒度作为解码器输入可以显着提高模型性能。我们的结果还表明了我们方法的有效性，相对于LightGBM方法有很大的改进。我们的工作为教育领域的AI发展做出了贡献，提供了一个可伸缩和准确的预测学生成果的工具。

    In this paper, we present a transformer architecture for predicting student performance on standardized tests. Specifically, we leverage students historical data, including their past test scores, study habits, and other relevant information, to create a personalized model for each student. We then use these models to predict their future performance on a given test. Applying this model to the RIIID dataset, we demonstrate that using multiple granularities for temporal features as the decoder input significantly improve model performance. Our results also show the effectiveness of our approach, with substantial improvements over the LightGBM method. Our work contributes to the growing field of AI in education, providing a scalable and accurate tool for predicting student outcomes.
    
[^97]: 拓扑数据分析中的欧拉特征工具

    Euler Characteristic Tools For Topological Data Analysis. (arXiv:2303.14040v1 [cs.LG])

    [http://arxiv.org/abs/2303.14040](http://arxiv.org/abs/2303.14040)

    本文研究了欧拉特征技术在拓扑数据分析中的应用，利用点运算欧拉特征得到欧拉特征轮廓，在监督和无监督任务中实现了最先进性能，并提供了欧拉轮廓及其混合变换捕捉信息的启发式方法。

    

    本文研究了拓扑数据分析中的欧拉特征技术。从数据构建的一族单纯复合体的点运算欧拉特征，得到所谓的欧拉特征轮廓。我们展示了这个简单描述符以极低的计算成本在监督任务中实现了最先进性能。受信号分析的启发，我们计算欧拉特征轮廓的混合变换。这些积分变换将欧拉特征技术与勒贝格积分混合，提供高效压缩拓扑信号。因此，它们在无监督设置中表现出卓越的性能。在定性方面，我们提供了关于欧拉轮廓及其混合变换所捕捉的拓扑和几何信息的众多启发式方法。最后，我们证明了这些描述符的稳定性结果以及在随机设置下的渐近保证。

    In this article, we study Euler characteristic techniques in topological data analysis. Pointwise computing the Euler characteristic of a family of simplicial complexes built from data gives rise to the so-called Euler characteristic profile. We show that this simple descriptor achieve state-of-the-art performance in supervised tasks at a very low computational cost. Inspired by signal analysis, we compute hybrid transforms of Euler characteristic profiles. These integral transforms mix Euler characteristic techniques with Lebesgue integration to provide highly efficient compressors of topological signals. As a consequence, they show remarkable performances in unsupervised settings. On the qualitative side, we provide numerous heuristics on the topological and geometric information captured by Euler profiles and their hybrid transforms. Finally, we prove stability results for these descriptors as well as asymptotic guarantees in random settings.
    
[^98]: 模型拼接：寻找表示间的功能相似性

    Model Stitching: Looking For Functional Similarity Between Representations. (arXiv:2303.11277v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.11277](http://arxiv.org/abs/2303.11277)

    本研究扩展了模型拼接方法，使其能够比较不同神经网络架构中不同形状层学习到的表示。我们发现在小ResNets中使用基于卷积的拼接，在第一个网络中较晚出现的层与第二个网络中对应层距离较远的情况下也能达到较高的准确度。

    

    模型拼接是一种比较不同神经网络表示的有效方法，因为它允许我们衡量它们可以互换的程度。我们在Bansal, Nakkiran & Barak的先前工作基础上进行了扩展，该工作使用模型拼接来比较不同结构的神经网络学习到的相同形状的表示。我们的贡献使我们能够比较不同结构的神经网络中不同形状层学习到的表示。我们随后揭示了模型拼接的意外行为。具体来说，我们发现基于卷积的拼接对于小ResNets来说，如果这些层在第一个（发送者）网络中出现的位置晚于第二个（接收者）网络中的位置，即使这些层相距很远，也可以达到很高的准确度。

    Model stitching (Lenc & Vedaldi 2015) is a compelling methodology to compare different neural network representations, because it allows us to measure to what degree they may be interchanged. We expand on a previous work from Bansal, Nakkiran & Barak which used model stitching to compare representations of the same shapes learned by differently seeded and/or trained neural networks of the same architecture. Our contribution enables us to compare the representations learned by layers with different shapes from neural networks with different architectures. We subsequently reveal unexpected behavior of model stitching. Namely, we find that stitching, based on convolutions, for small ResNets, can reach high accuracy if those layers come later in the first (sender) network than in the second (receiver), even if those layers are far apart.
    
[^99]: 利用预训练的垂直二维扩散模型改进三维成像

    Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models. (arXiv:2303.08440v1 [eess.IV])

    [http://arxiv.org/abs/2303.08440](http://arxiv.org/abs/2303.08440)

    本论文提出了一种使用两个预训练的垂直二维扩散模型来解决三维逆问题的方法，将三维数据分布建模为在不同方向上切片的二维分布的乘积，解决了维数的灾难性问题，并以高效的方式用于医学图像重建任务。

    

    由于其众多的优点，扩散模型已成为图像生成和重构的流行方法。 然而，大多数基于扩散的逆问题求解方法仅处理二维图像，即使是最近发布的三维方法也没有充分利用三维先验分布。 为了解决这个问题，我们提出了一种新方法，使用两个相互垂直的预训练二维扩散模型来解决三维逆问题。 通过将三维数据分布建模为在不同方向上切片的二维分布的乘积，我们的方法有效地解决了维数的灾难性问题。 我们的实验结果证明，我们的方法对于三维医学图像重建任务非常有效，包括MRI Z轴超分辨率，压缩感知MRI和稀疏视图CT。 我们的方法可以生成适用于医学应用的高质量体素体积。

    Diffusion models have become a popular approach for image generation and reconstruction due to their numerous advantages. However, most diffusion-based inverse problem-solving methods only deal with 2D images, and even recently published 3D methods do not fully exploit the 3D distribution prior. To address this, we propose a novel approach using two perpendicular pre-trained 2D diffusion models to solve the 3D inverse problem. By modeling the 3D data distribution as a product of 2D distributions sliced in different directions, our method effectively addresses the curse of dimensionality. Our experimental results demonstrate that our method is highly effective for 3D medical image reconstruction tasks, including MRI Z-axis super-resolution, compressed sensing MRI, and sparse-view CT. Our method can generate high-quality voxel volumes suitable for medical applications.
    
[^100]: 可解释的异常值汇总

    Interpretable Outlier Summarization. (arXiv:2303.06261v1 [cs.LG])

    [http://arxiv.org/abs/2303.06261](http://arxiv.org/abs/2303.06261)

    STAIR提出了一种可解释的异常值汇总方法，通过学习一组紧凑的人类可理解规则，以汇总和解释异常检测结果，具有强大的可解释性，以准确地总结检测结果。

    STAIR proposes an interpretable outlier summarization method by learning a compact set of human understandable rules to summarize and explain the anomaly detection results, which has strong interpretability to accurately summarize the detection results.

    异常值检测在实际应用中是至关重要的，以防止金融欺诈、防御网络入侵或检测即将发生的设备故障。为了减少人力评估异常值检测结果的工作量，并有效地将异常值转化为可操作的见解，用户通常希望系统自动产生可解释的异常值检测结果的子组的汇总。然而，到目前为止，没有这样的系统存在。为了填补这一空白，我们提出了STAIR，它学习了一组紧凑的人类可理解规则，以汇总和解释异常检测结果。STAIR不使用经典的决策树算法来产生这些规则，而是提出了一个新的优化目标，以产生少量规则，具有最小的复杂性，因此具有强大的可解释性，以准确地总结检测结果。STAIR的学习算法通过迭代分割大规则来产生规则集，并在每个i中最大化这个目标，是最优的。

    Outlier detection is critical in real applications to prevent financial fraud, defend network intrusions, or detecting imminent device failures. To reduce the human effort in evaluating outlier detection results and effectively turn the outliers into actionable insights, the users often expect a system to automatically produce interpretable summarizations of subgroups of outlier detection results. Unfortunately, to date no such systems exist. To fill this gap, we propose STAIR which learns a compact set of human understandable rules to summarize and explain the anomaly detection results. Rather than use the classical decision tree algorithms to produce these rules, STAIR proposes a new optimization objective to produce a small number of rules with least complexity, hence strong interpretability, to accurately summarize the detection results. The learning algorithm of STAIR produces a rule set by iteratively splitting the large rules and is optimal in maximizing this objective in each i
    
[^101]: Task Aware Dreamer用于强化学习中的任务泛化

    Task Aware Dreamer for Task Generalization in Reinforcement Learning. (arXiv:2303.05092v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05092](http://arxiv.org/abs/2303.05092)

    本文提出了一种名为Task Aware Dreamer（TAD）的方法用于强化学习中的任务泛化。通过量化任务分布的相关性，TAD能够将历史信息编码到策略中，以便区分不同任务，并在泛化到未见任务时具有较好的性能。

    

    强化学习的一个长期目标是获得能够在训练任务上学习并且在不同奖励函数下可以很好地泛化到未见任务的代理。一个通用的挑战是定量地衡量这些不同任务之间的相似性，这对于分析任务分布并进一步设计具有更强泛化能力的算法至关重要。为了解决这个问题，我们提出了一种新的度量方法，名为任务分布相关性（TDR），通过不同任务的最优Q函数来量化任务分布的相关性。在具有高TDR的任务情况下，即任务之间显著不同，我们发现马尔可夫策略无法区分它们，导致性能较差。基于这一观察，我们将所有历史信息编码到策略中以区分不同任务，并提出了Task Aware Dreamer（TAD），它将世界模型扩展为我们的奖励感知世界模型以捕捉任务的相关性。

    A long-standing goal of reinforcement learning is to acquire agents that can learn on training tasks and generalize well on unseen tasks that may share a similar dynamic but with different reward functions. A general challenge is to quantitatively measure the similarities between these different tasks, which is vital for analyzing the task distribution and further designing algorithms with stronger generalization. To address this, we present a novel metric named Task Distribution Relevance (TDR) via optimal Q functions of different tasks to capture the relevance of the task distribution quantitatively. In the case of tasks with a high TDR, i.e., the tasks differ significantly, we show that the Markovian policies cannot differentiate them, leading to poor performance. Based on this insight, we encode all historical information into policies for distinguishing different tasks and propose Task Aware Dreamer (TAD), which extends world models into our reward-informed world models to capture
    
[^102]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^103]: SGD学习神经网络: 跃迁复杂度与鞍到鞍动力学

    SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics. (arXiv:2302.11055v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11055](http://arxiv.org/abs/2302.11055)

    该论文研究了随机梯度下降(SGD)算法在神经网络上的学习时间复杂度，提出了一种复杂度度量称为跃迁，证明了在高斯各向同性数据和二层神经网络上的研究结果，并展示了训练过程中函数支持的动态学习方法。

    

    我们研究了随机梯度下降(SGD)学习具有各向同性数据的全连接神经网络的时间复杂度。我们提出了一种复杂度度量——跃迁，用来度量目标函数的"层级"程度。对于$d$维均匀布尔或各向同性高斯数据，我们的主要猜想是学习一个低维支持函数$f$的时间复杂度为$\tilde\Theta (d^{\max(\mathrm{Leap}(f),2)})$。我们在额外对SGD的技术假设下，证明了这个猜想在高斯各向同性数据和二层神经网络上的一个版本。我们展示出训练过程以鞍点到鞍点的动态方式逐渐学习了函数的支持。与[Abbe et al. 2022]不同，我们的结果超越了跃迁1(合并阶梯函数)的限制，并超越了均场和梯度流近似，在这里可以获得完全的复杂度控制。最后，我们指出这给出了完整训练的SGD复杂度。

    We investigate the time complexity of SGD learning on fully-connected neural networks with isotropic data. We put forward a complexity measure -- the leap -- which measures how "hierarchical" target functions are. For $d$-dimensional uniform Boolean or isotropic Gaussian data, our main conjecture states that the time complexity to learn a function $f$ with low-dimensional support is $\tilde\Theta (d^{\max(\mathrm{Leap}(f),2)})$. We prove a version of this conjecture for a class of functions on Gaussian isotropic data and 2-layer neural networks, under additional technical assumptions on how SGD is run. We show that the training sequentially learns the function support with a saddle-to-saddle dynamic. Our result departs from [Abbe et al. 2022] by going beyond leap 1 (merged-staircase functions), and by going beyond the mean-field and gradient flow approximations that prohibit the full complexity control obtained here. Finally, we note that this gives an SGD complexity for the full train
    
[^104]: LEVER: 使用执行进行语言到代码生成的学习验证

    LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08468](http://arxiv.org/abs/2302.08468)

    提出了一种使用执行结果来验证生成的程序的简单方法LEVER，通过训练验证器根据自然语言输入、程序本身和执行结果来确定程序的正确性，从而改进了语言到代码生成的过程。

    

    训练在代码上的大型语言模型（code LLMs）的出现，已经在语言到代码生成方面取得了显著进展。此领域的最新方法将LLM解码与使用测试用例或基于执行结果的启发式方法的样本修剪和重新排序相结合。然而，对于许多现实世界的语言到代码应用来说，获取测试用例是具有挑战性的，而启发式方法不能很好地捕捉执行结果的语义特征，比如数据类型和值范围，这往往表明程序的正确性。在这项工作中，我们提出了LEVER，一种通过学习使用执行结果来验证生成的程序，从而改进语言到代码生成的简单方法。具体地说，我们训练验证器根据自然语言输入、程序本身和执行结果来确定从LLM中抽样的程序是否正确。通过将验证分数与LLM生成分数相结合，对抽样的程序进行重新排序。

    The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generati
    
[^105]: 通过自我蒸馏改进可微分架构搜索方法

    Improving Differentiable Architecture Search via Self-Distillation. (arXiv:2302.05629v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.05629](http://arxiv.org/abs/2302.05629)

    本文提出了自我蒸馏可微分神经架构搜索（SD-DARTS）方法，通过从超网的先前步骤中蒸馏知识来指导其训练，有效降低了超网损失的尖锐度，从而缓解了离散化差距。

    

    可微分架构搜索（DARTS）是一种简单且高效的神经架构搜索（NAS）方法。DARTS在搜索阶段通过联合优化架构参数和网络参数来训练超网。在评估阶段，DARTS将超网离散化，从而得到基于架构参数的最优架构。然而，最近的研究表明，在训练过程中，超网往往会收敛到尖锐的极小值点而不是平坦的极小值点。这体现在超网损失曲面的尖锐程度较高，最终导致超网与最优架构之间存在性能差距。本文提出了自我蒸馏可微分神经架构搜索（SD-DARTS）来缓解离散化差距。我们利用自我蒸馏从超网的先前步骤中蒸馏知识，引导其在当前步骤中的训练，有效降低了超网损失的尖锐度。

    Differentiable Architecture Search (DARTS) is a simple yet efficient Neural Architecture Search (NAS) method. During the search stage, DARTS trains a supernet by jointly optimizing architecture parameters and network parameters. During the evaluation stage, DARTS discretizes the supernet to derive the optimal architecture based on architecture parameters. However, recent research has shown that during the training process, the supernet tends to converge towards sharp minima rather than flat minima. This is evidenced by the higher sharpness of the loss landscape of the supernet, which ultimately leads to a performance gap between the supernet and the optimal architecture. In this paper, we propose Self-Distillation Differentiable Neural Architecture Search (SD-DARTS) to alleviate the discretization gap. We utilize self-distillation to distill knowledge from previous steps of the supernet to guide its training in the current step, effectively reducing the sharpness of the supernet's loss
    
[^106]: 基于SMDP的GPU动态批处理优化推断效率

    SMDP-Based Dynamic Batching for Efficient Inference on GPU-Based Platforms. (arXiv:2301.12865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12865](http://arxiv.org/abs/2301.12865)

    本文提出了一种动态批处理策略，采用基于GPU的批处理服务队列进行建模，通过半马尔可夫决策过程的方法最小化平均响应时间和功耗。

    

    在云计算或边缘计算平台上，批处理是提供高效和经济服务的重要技术，本文提出了一种动态批处理策略，旨在在效率和延迟之间取得平衡。将基于GPU的推断服务建模为批处理服务队列，并将其设计为一个连续时间平均成本问题，制定了一个半马尔可夫决策过程（SMDP），并以最小化平均响应时间和平均功耗之和为目标。最优策略通过解决相关的离散时间贝尔曼方程获得。

    In up-to-date machine learning (ML) applications on cloud or edge computing platforms, batching is an important technique for providing efficient and economical services at scale. In particular, parallel computing resources on the platforms, such as graphics processing units (GPUs), have higher computational and energy efficiency with larger batch sizes. However, larger batch sizes may also result in longer response time, and thus it requires a judicious design. This paper aims to provide a dynamic batching policy that strikes a balance between efficiency and latency. The GPU-based inference service is modeled as a batch service queue with batch-size dependent processing time. Then, the design of dynamic batching is a continuous-time average-cost problem, and is formulated as a semi-Markov decision process (SMDP) with the objective of minimizing the weighted sum of average response time and average power consumption. The optimal policy is acquired by solving an associated discrete-time
    
[^107]: 领域无关的分子生成与自我反馈

    Domain-Agnostic Molecular Generation with Self-feedback. (arXiv:2301.11259v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11259](http://arxiv.org/abs/2301.11259)

    MolGen是一个专注于分子生成的预训练语言模型，使用了领域无关的分子前缀调整和自我反馈的范式，实现了化学有效性、多样性、新颖性和复杂性的突破，在分子生成领域表现出了出色的性能。

    

    分子的生成已经受到极大的关注，其革新了科学家设计分子结构的方式，并为化学和药物设计提供了宝贵的支持。然而，尽管在分子生成中使用语言模型具有潜力，但它们面临着许多挑战，比如生成语法或化学存在缺陷的分子，狭窄的领域专注以及由于缺乏注释数据或外部分子数据库而限制了生成多样性和可行性。因此，我们引入了MolGen，它是一个专门用于分子生成的预训练分子语言模型。MolGen通过重构一亿多个分子SELFIES获得了固有的结构和语法概念，并通过领域无关的分子前缀调整促进了不同领域之间的知识传递。此外，我们提出了一种自我反馈范式，启发预训练模型与最终下游目标对齐，有助于更稳健和高效的分子生成。我们在基准数据集上的实验表明，MolGen在化学有效性，多样性，新颖性和复杂性方面优于现有技术。

    The generation of molecules with desired properties has gained tremendous popularity, revolutionizing the way scientists design molecular structures and providing valuable support for chemical and drug design. However, despite the potential of language models in molecule generation, they face numerous challenges such as the generation of syntactically or chemically flawed molecules, narrow domain focus, and limitations in creating diverse and directionally feasible molecules due to a dearth of annotated data or external molecular databases. To this end, we introduce MolGen, a pre-trained molecular language model tailored specifically for molecule generation. MolGen acquires intrinsic structural and grammatical insights by reconstructing over 100 million molecular SELFIES, while facilitating knowledge transfer between different domains through domain-agnostic molecular prefix tuning. Moreover, we present a self-feedback paradigm that inspires the pre-trained model to align with the ulti
    
[^108]: 通过元学习在哈密顿流形中识别普遍的神经表示

    Identifying Generalized Neural Representation Across Hamiltonian Manifolds via Meta-learning. (arXiv:2212.01168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01168](http://arxiv.org/abs/2212.01168)

    通过元学习方法，在哈密顿流形中识别出普遍的神经表示，实现了对不同物理系统的快速适应能力。

    

    最近物理学中深度学习的进展集中在通过将物理先验或归纳偏见引入神经网络来发现目标系统的共享表示。然而，这些方法特定于系统，不允许轻松适应由不同物理法则驱动的新物理系统。例如，训练于质点弹簧系统的神经网络无法准确预测双体系统或任何具有不同物理法则的系统的行为。在本研究中，我们使用图神经网络模拟我们的系统，并采用元学习算法使模型在一系列任务中积累经验，并使其适应新的物理系统。我们的方法旨在学习跨各种哈密顿流形的通用表示，这是哈密顿系统数据分布的共同特征。我们使用由不同系统组成的数据集训练模型，每个系统都有其自身固有的动力学，并评估其性能。

    Recent advancements in deep learning for physics have focused on discovering shared representations of target systems by incorporating physics priors or inductive biases into neural networks. However, these approaches are system-specific and do not allow for easy adaptation to new physical systems governed by different laws. For example, a neural network trained on a mass-spring system cannot accurately predict the behavior of a two-body system or any other system with different governing physics. In this work, we model our system with a graph neural network and employ a meta-learning algorithm to enable the model to gain experience over a distribution of tasks and make it adapt to new physics. Our approach aims to learn a general representation across the various Hamiltonian manifolds, which is a common feature of the data distribution of Hamiltonian systems. We train our model using a dataset of different physical systems, each governed by its own inherent dynamics, and evaluate its 
    
[^109]: C3: 跨实例引导对比聚类

    C3: Cross-instance guided Contrastive Clustering. (arXiv:2211.07136v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07136](http://arxiv.org/abs/2211.07136)

    C3是一种引入了交叉实例关系的对比聚类方法，通过考虑实例之间的关系增加了正对数，从而提高了聚类性能。

    

    聚类是将相似的数据样本聚集到无需预定义标签的簇中的任务。它在机器学习文献中得到了广泛研究，最近深度学习的进展重新引起了人们对该领域的兴趣。对比聚类（CC）模型是深度聚类的一个重要组成部分，通过数据增强生成每个数据实例的正负对。CC模型的目标是学习一个将正对实例的实例级和聚类级表示分组在一起的特征空间。尽管提高了SOTA，但这些算法忽略了携带有改进聚类性能的交叉实例模式。这增加了模型的错误负对数，同时降低了它的真正正对数。在本文中，我们提出了一种新颖的对比聚类方法，Cross-instance guided Contrastive Clustering (C3)，考虑了样本之间的关系以增加正对数。

    Clustering is the task of gathering similar data samples into clusters without using any predefined labels. It has been widely studied in machine learning literature, and recent advancements in deep learning have revived interest in this field. Contrastive clustering (CC) models are a staple of deep clustering in which positive and negative pairs of each data instance are generated through data augmentation. CC models aim to learn a feature space where instance-level and cluster-level representations of positive pairs are grouped together. Despite improving the SOTA, these algorithms ignore the cross-instance patterns, which carry essential information for improving clustering performance. This increases the false-negative-pair rate of the model while decreasing its true-positive-pair rate. In this paper, we propose a novel contrastive clustering method, Cross-instance guided Contrastive Clustering (C3), that considers the cross-sample relationships to increase the number of positive p
    
[^110]: 一种通信高效的分散式交替梯度法用于双层规划

    A Decentralized Alternating Gradient Method for Communication-Efficient Bilevel Programming. (arXiv:2211.04088v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.04088](http://arxiv.org/abs/2211.04088)

    本文提出了一种通信高效的分散式交替梯度法解决双层规划问题，相较于其他方法，该算法具有更低的通信成本和更高的隐私性。

    

    双层规划近期引起了学术界的广泛关注，因为它有许多应用，包括强化学习和超参数优化。然而，现有的解决方案通常采用单机或联邦学习的方式，并存在通信成本高和隐私泄露风险等问题。本文提出了一种基于惩罚函数的分散式算法来解决这类优化问题，改进了现有的方法并且在理论上得到了保证。

    Bilevel programming has recently received attention in the literature, due to a wide range of applications, including reinforcement learning and hyper-parameter optimization. However, it is widely assumed that the underlying bilevel optimization problem is solved either by a single machine or in the case of multiple machines connected in a star-shaped network, i.e., federated learning setting. The latter approach suffers from a high communication cost on the central node (e.g., parameter server) and exhibits privacy vulnerabilities. Hence, it is of interest to develop methods that solve bilevel optimization problems in a communication-efficient decentralized manner. To that end, this paper introduces a penalty function based decentralized algorithm with theoretical guarantees for this class of optimization problems. Specifically, a distributed alternating gradient-type algorithm for solving consensus bilevel programming over a decentralized network is developed. A key feature of the pr
    
[^111]: 使用Bollinger带增强的神经增强卡尔曼滤波进行配对交易

    Neural Augmented Kalman Filtering with Bollinger Bands for Pairs Trading. (arXiv:2210.15448v2 [q-fin.TR] UPDATED)

    [http://arxiv.org/abs/2210.15448](http://arxiv.org/abs/2210.15448)

    本研究提出了KalmenNet辅助的Bollinger带配对交易策略（KBPT），通过深度学习增强了KF-BB交易操作。KBPT通过建立一个扩展的空间状态模型来近似配对交易的关系，并利用基于卡尔曼滤波的神经网络提高了交易效果。

    

    配对交易是一类交易技术，其策略是基于监测资产对之间的关系。一种常见的配对交易方法是将资产对之间的关系描述为带有高斯噪声的线性空间状态模型。这种表示方法利用卡尔曼滤波器提取具有低复杂性和延迟的金融指标，然后使用经典策略（如Bollinger带）进行处理。然而，这种空间状态模型本质上是近似和不匹配的，经常会降低收益。在本文中，我们提出了KalmenNet辅助的Bollinger带配对交易（KBPT），这是一种借助深度学习的策略，用于增强KF-BB交易的操作。KBPT通过构建一个扩展的空间状态模型来近似配对交易的关系，将其视为持有部分协整。这个空间状态模型被应用于一个交易策略，该策略利用基于卡尔曼滤波的神经网络增强KF-BB交易。

    Pairs trading is a family of trading techniques that determine their policies based on monitoring the relationships between pairs of assets. A common pairs trading approach relies on describing the pair-wise relationship as a linear Space State (SS) model with Gaussian noise. This representation facilitates extracting financial indicators with low complexity and latency using a Kalman Filter (KF), that are then processed using classic policies such as Bollinger Bands (BB). However, such SS models are inherently approximated and mismatched, often degrading the revenue. In this work, we propose KalmenNet-aided Bollinger bands Pairs Trading (KBPT), a deep learning aided policy that augments the operation of KF-aided BB trading. KBPT is designed by formulating an extended SS model for pairs trading that approximates their relationship as holding partial co-integration. This SS model is utilized by a trading policy that augments KF-BB trading with a dedicated neural network based on the Kal
    
[^112]: 具有比例多校准的公平入学风险预测

    Fair admission risk prediction with proportional multicalibration. (arXiv:2209.14613v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14613](http://arxiv.org/abs/2209.14613)

    提出了一种称为比例多校准的方法来实现公平校准，在限制校准误差的同时保持整体校准。通过比例多校准可以避免模型对特定群体的预测信任或不信任的问题。

    

    公平校准是风险预测环境中广泛需要的公平标准之一。实现公平校准的一种方法是采用多校准。多校准通过在灵活定义的子群体之间限制校准误差来实现整体校准。然而，多校准模型在低基础率群体中可能会表现出较高的校准误差百分比，而在高基础率群体中则较低。因此，决策者可能会学习信任或不信任特定群体的模型预测。为了缓解这个问题，我们提出了“比例多校准”标准，该标准限制了群体和预测区间内的校准误差百分比。我们证明满足比例多校准不仅限制了模型的多校准，而且还限制了模型作为一个直接衡量模型近似充分性的公平标准的“差分校准”。因此，比例校准模型限制了模型的能力。

    Fair calibration is a widely desirable fairness criteria in risk prediction contexts. One way to measure and achieve fair calibration is with multicalibration. Multicalibration constrains calibration error among flexibly-defined subpopulations while maintaining overall calibration. However, multicalibrated models can exhibit a higher percent calibration error among groups with lower base rates than groups with higher base rates. As a result, it is possible for a decision-maker to learn to trust or distrust model predictions for specific groups. To alleviate this, we propose \emph{proportional multicalibration}, a criteria that constrains the percent calibration error among groups and within prediction bins. We prove that satisfying proportional multicalibration bounds a model's multicalibration as well its \emph{differential calibration}, a fairness criteria that directly measures how closely a model approximates sufficiency. Therefore, proportionally calibrated models limit the abilit
    
[^113]: 高效通用的Lipschitz网络的近正交层

    Almost-Orthogonal Layers for Efficient General-Purpose Lipschitz Networks. (arXiv:2208.03160v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.03160](http://arxiv.org/abs/2208.03160)

    本文提出了一种新技术，可以构建具有小Lipschitz常数的高效通用Lipschitz网络。该技术具有形式化保证、易于实现和高效运行的优点，并且可以与任何训练目标和优化方法相结合。主要贡献是通过重新缩放的权重矩阵参数化，确保每个网络层的Lipschitz常数最大为1，并且导致学习的权重矩阵接近正交。

    

    对于深度网络来说，能够对小的输入变化具有鲁棒性是非常理想的属性。实现这个属性的一种常见方法是设计具有较小的Lipschitz常数的网络。在本文中，我们提出了一种构建这样的Lipschitz网络的新技术，它具有多个优点：可以应用于任何线性网络层（全连接或卷积），对Lipschitz常数提供形式化的保证，易于实现和高效运行，并且可以与任何训练目标和优化方法相结合。事实上，我们的技术是文献中第一个同时实现所有这些属性的技术。我们的主要贡献是基于重新缩放的权重矩阵参数化，保证每个网络层的Lipschitz常数最大为1，并且导致学习的权重矩阵接近正交。因此，我们将这样的层称为几乎正交Lipschitz（AOL）。实验证明和消融研究表明...

    It is a highly desirable property for deep networks to be robust against small input changes. One popular way to achieve this property is by designing networks with a small Lipschitz constant. In this work, we propose a new technique for constructing such Lipschitz networks that has a number of desirable properties: it can be applied to any linear network layer (fully-connected or convolutional), it provides formal guarantees on the Lipschitz constant, it is easy to implement and efficient to run, and it can be combined with any training objective and optimization method. In fact, our technique is the first one in the literature that achieves all of these properties simultaneously. Our main contribution is a rescaling-based weight matrix parametrization that guarantees each network layer to have a Lipschitz constant of at most 1 and results in the learned weight matrices to be close to orthogonal. Hence we call such layers almost-orthogonal Lipschitz (AOL). Experiments and ablation stu
    
[^114]: 简单高效的异构图神经网络

    Simple and Efficient Heterogeneous Graph Neural Network. (arXiv:2207.02547v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.02547](http://arxiv.org/abs/2207.02547)

    SeHGNN是一个简单高效的异构图神经网络，通过轻量级的均值聚合器预先计算邻居聚合来捕捉结构信息，采用单层结构和长浏览路径来更好地利用语义信息。

    

    异构图神经网络（HGNNs）能够将异构图的丰富结构和语义信息嵌入节点表示中。现有的HGNNs从同质图神经网络中继承了许多机制，尤其是注意力机制和多层结构。然而很少有研究探讨这些机制是否在异构图上真正有效。本文进行了深入而详细的研究，并提出了简单高效的异构图神经网络（SeHGNN）。为了轻松捕捉结构信息，SeHGNN使用轻量级的均值聚合器预先计算邻居聚合，通过去除过度使用的邻居注意力和避免在每个训练周期中重复进行邻居聚合来降低复杂性。为了更好地利用语义信息，SeHGNN采用具有长浏览路径的单层结构来扩展感受野。

    Heterogeneous graph neural networks (HGNNs) have powerful capability to embed rich structural and semantic information of a heterogeneous graph into node representations. Existing HGNNs inherit many mechanisms from graph neural networks (GNNs) over homogeneous graphs, especially the attention mechanism and the multi-layer structure. These mechanisms bring excessive complexity, but seldom work studies whether they are really effective on heterogeneous graphs. This paper conducts an in-depth and detailed study of these mechanisms and proposes Simple and Efficient Heterogeneous Graph Neural Network (SeHGNN). To easily capture structural information, SeHGNN pre-computes the neighbor aggregation using a light-weight mean aggregator, which reduces complexity by removing overused neighbor attention and avoiding repeated neighbor aggregation in every training epoch. To better utilize semantic information, SeHGNN adopts the single-layer structure with long metapaths to extend the receptive fiel
    
[^115]: 半监督LiDAR语义分割的LaserMix方法

    LaserMix for Semi-Supervised LiDAR Semantic Segmentation. (arXiv:2207.00026v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2207.00026](http://arxiv.org/abs/2207.00026)

    本文提出了一种名为LaserMix的半监督学习方法，利用LiDAR点云的强空间线索更好地利用未标记数据，具有通用性、统计基础和有效性，并在SemanticKITTI上取得了最优性能。

    

    密集标注LiDAR点云耗费巨大，限制了完全监督学习方法的可扩展性。本文研究了LiDAR分割中未被充分探索的半监督学习。我们的核心想法是利用LiDAR点云的强空间线索更好地利用未标记数据。我们提出了LaserMix方法，将来自不同LiDAR扫描的激光束混合，然后促使模型在混合前后做出一致且自信的预测。我们的框架具有三个吸引人的特点：1）通用性：LaserMix与LiDAR表示（例如，视角和体素）无关，因此我们的半监督框架可以普遍应用。2）统计基础：我们提供了详细的分析，从理论上解释了所提出框架的适用性。3）有效性：对流行的LiDAR分割数据集（nuScenes，SemanticKITTI和ScribbleKITTI）的全面实验分析证明了我们的有效性和优越性。值得注意的是，我们在SemanticKITTI上取得了显著优势的最优性能。

    Densely annotating LiDAR point clouds is costly, which restrains the scalability of fully-supervised learning methods. In this work, we study the underexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core idea is to leverage the strong spatial cues of LiDAR point clouds to better exploit unlabeled data. We propose LaserMix to mix laser beams from different LiDAR scans, and then encourage the model to make consistent and confident predictions before and after mixing. Our framework has three appealing properties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g., range view and voxel), and hence our SSL framework can be universally applied. 2) Statistically grounded: We provide a detailed analysis to theoretically explain the applicability of the proposed framework. 3) Effective: Comprehensive experimental analysis on popular LiDAR segmentation datasets (nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and superiority. Notably, we
    
[^116]: 跨领域少样本元学习中的特征提取器叠加

    Feature Extractor Stacking for Cross-domain Few-shot Meta-learning. (arXiv:2205.05831v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.05831](http://arxiv.org/abs/2205.05831)

    这篇论文提出了一种新的跨领域少样本元学习方法，称为特征提取器叠加(FES)。FES通过叠加多个主干的信息，可以利用异构预训练的主干，而且不需要维护一个需要重新计算的通用模型。

    

    跨领域少样本元学习(CDFSML)解决了需要将知识从多个源领域转移到一个实例稀缺的目标领域，而目标领域的数据分布明显不同的学习问题。最近发布的CDFSML方法通常构建一个通用模型，将多个源领域的知识合并到一个主干特征提取器中。这样可以实现高效推理，但需要在添加新的源领域时重新计算主干。其中一些方法还与异构源领域主干架构不兼容。我们提出了特征提取器叠加(FES)，一种将来自一组主干的信息进行组合的新CDFSML方法，它可以直接使用异构预训练的主干，并且不需要维护一个需要在主干集合更新时重新计算的通用模型。我们提出了基本的FES算法，它受经典叠加方法元学习的启发。

    Cross-domain few-shot meta-learning (CDFSML) addresses learning problems where knowledge needs to be transferred from several source domains into an instance-scarce target domain with an explicitly different distribution. Recently published CDFSML methods generally construct a universal model that combines knowledge of multiple source domains into one backbone feature extractor. This enables efficient inference but necessitates re-computation of the backbone whenever a new source domain is added. Some of these methods are also incompatible with heterogeneous source domain backbone architectures. We propose feature extractor stacking (FES), a new CDFSML method for combining information from a collection of backbones, which can utilise heterogeneous pretrained backbones out of the box, and does not maintain a universal model that needs to be re-computed when its backbone collection is updated. We present the basic FES algorithm, which is inspired by the classic stacking approach to meta-
    
[^117]: 使用自监督学习在未标记、未注释的病理切片上绘制组织形态学癌症表型的地图

    Mapping the landscape of histomorphological cancer phenotypes using self-supervised learning on unlabeled, unannotated pathology slides. (arXiv:2205.01931v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2205.01931](http://arxiv.org/abs/2205.01931)

    本研究提出了一种名为组织形态学表型学习的自监督方法，可以在未标记、未注释的病理切片上绘制出癌症组织的地图，从而揭示出不同表型间的特征和转变过程。

    

    癌症的确诊和治疗依赖于病理学家从显微镜图像中提取信息。这些图像包含复杂的信息，需要耗时的专家人工解读，存在人为偏见的问题。虽然监督深度学习方法在分类任务中表现出强大的能力，但它们在训练模型时受到注释成本和质量的固有限制。为了解决监督方法的这种限制，我们开发了组织形态学表型学习 (Histomorphological Phenotype Learning, HPL) 方法，这是一种完全基于自监督的方法，不需要专家标签或注释，并通过自动发现小图像块中的区分性图像特征来进行操作。图像块被分组成形态相似的聚类，构成组织形态学表型的库，揭示了从良性到恶性组织经过炎症和反应性表型的轨迹。这些聚类具有独特的特征，可以被识别出来。

    Definitive cancer diagnosis and management depend upon the extraction of information from microscopy images by pathologists. These images contain complex information requiring time-consuming expert human interpretation that is prone to human bias. Supervised deep learning approaches have proven powerful for classification tasks, but they are inherently limited by the cost and quality of annotations used for training these models. To address this limitation of supervised methods, we developed Histomorphological Phenotype Learning (HPL), a fully blue{self-}supervised methodology that requires no expert labels or annotations and operates via the automatic discovery of discriminatory image features in small image tiles. Tiles are grouped into morphologically similar clusters which constitute a library of histomorphological phenotypes, revealing trajectories from benign to malignant tissue via inflammatory and reactive phenotypes. These clusters have distinct features which can be identifie
    
[^118]: 通过条件GAN生成多标签临床时间序列

    Multi-Label Clinical Time-Series Generation via Conditional GAN. (arXiv:2204.04797v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.04797](http://arxiv.org/abs/2204.04797)

    本论文提出了一种通过条件GAN生成多标签临床时间序列的方法，用于解决EHR数据生成中的挑战，包括时间序列数据和不平衡的罕见疾病。生成器使用门控循环单元和平滑条件矩阵生成序列和罕见疾病，评论家使用Wasserstein距离评分来区分真实样本和合成样本。

    

    近年来，深度学习在与电子健康记录（EHR）相关的各种应用中取得了成功，如表示学习和临床事件预测。然而，由于隐私约束，对EHR的有限访问成为深度学习研究的瓶颈。为了缓解这些问题，生成对抗网络（GANs）已被成功用于生成EHR数据。然而，高质量EHR生成仍面临挑战，包括生成时间序列EHR数据和不平衡的罕见疾病。在这项工作中，我们提出了一个多标签时间序列GAN（MTGAN），用于生成EHR并同时提高罕见疾病生成的质量。MTGAN的生成器使用带有平滑条件矩阵的门控循环单元（GRU）生成序列和罕见疾病。评论家使用Wasserstein距离给出得分，通过考虑数据和时间特征来识别真实样本和合成样本。

    In recent years, deep learning has been successfully adopted in a wide range of applications related to electronic health records (EHRs) such as representation learning and clinical event prediction. However, due to privacy constraints, limited access to EHR becomes a bottleneck for deep learning research. To mitigate these concerns, generative adversarial networks (GANs) have been successfully used for generating EHR data. However, there are still challenges in high-quality EHR generation, including generating time-series EHR data and imbalanced uncommon diseases. In this work, we propose a Multi-label Time-series GAN (MTGAN) to generate EHR and simultaneously improve the quality of uncommon disease generation. The generator of MTGAN uses a gated recurrent unit (GRU) with a smooth conditional matrix to generate sequences and uncommon diseases. The critic gives scores using Wasserstein distance to recognize real samples from synthetic samples by considering both data and temporal featu
    
[^119]: 一个基于多重分形的深度学习模型用于文本挖掘

    A New Multifractal-based Deep Learning Model for Text Mining. (arXiv:2111.13861v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.13861](http://arxiv.org/abs/2111.13861)

    该论文介绍了一个基于多重分形的深度学习模型，用于解读文本中隐藏的多重分形属性，并结合提出的激活函数，在神经网络结构中实现非线性信息传输。

    

    在这个充满不确定性的世界中，存在的纹理编织出复杂的模式，多重分形成为洞察力的标志，照亮它们。当我们深入探索构成各种自然语言处理应用和智能服务的文本挖掘领域时，我们意识到在文本的面纱后面隐藏着人类思想和认知的表现，与复杂性紧密相互交织。在将文本视为复杂系统的基础上，本研究致力于揭示其中隐藏的宝藏，借助提出的多重分形方法解读嵌入在文本景观中的多重分形属性。这一努力最终孕育出我们的新颖模型，该模型还利用了提出的激活函数的力量，在其神经网络结构中实现非线性信息传输。

    In this world full of uncertainty, where the fabric of existence weaves patterns of complexity, multifractal emerges as beacons of insight, illuminating them. As we delve into the realm of text mining that underpins various natural language processing applications and powers a range of intelligent services, we recognize that behind the veil of text lies a manifestation of human thought and cognition, intricately intertwined with the complexities. Building upon the foundation of perceiving text as a complex system, this study embarks on a journey to unravel the hidden treasures within, armed with the proposed multifractal method that deciphers the multifractal attributes embedded within the text landscape. This endeavor culminates in the birth of our novel model, which also harnesses the power of the proposed activation function to facilitate nonlinear information transmission within its neural network architecture. The success on experiments anchored in real-world technical reports cov
    
[^120]: 富者愈富: 半监督学习的不平等影响

    The Rich Get Richer: Disparate Impact of Semi-Supervised Learning. (arXiv:2110.06282v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.06282](http://arxiv.org/abs/2110.06282)

    本文研究了半监督学习的不平等影响，发现那些在不使用SSL时具有更高基准准确性的子族群更容易从SSL中获益，而那些基准准确性较低的子族群在添加SSL模块后可能会观察到性能下降。

    

    半监督学习（SSL）在高质量监督数据严重有限的情况下，已经展示了提高模型准确性的潜力。尽管往往可以确立整个数据族群的平均准确性得到改善，但SSL在不同子族群中的表现尚不清楚。当我们希望公平对待的人口群体由人口统计学分组定义时，理解上述问题具有重要的公平性影响。在本文中，我们揭示了部署SSL的不平等影响：那些在不使用SSL时具有更高基准准确性的子族群（"富裕"子族群）往往从SSL中获益更多；而那些在基准准确性较低（"贫穷"）的子族群在添加SSL模块后甚至可能观察到性能下降。我们在一类广泛的SSL算法上在理论上和实证上证实了上述观察，这些算法明确或隐含地使用了一个...

    Semi-supervised learning (SSL) has demonstrated its potential to improve the model accuracy for a variety of learning tasks when the high-quality supervised data is severely limited. Although it is often established that the average accuracy for the entire population of data is improved, it is unclear how SSL fares with different sub-populations. Understanding the above question has substantial fairness implications when different sub-populations are defined by the demographic groups that we aim to treat fairly. In this paper, we reveal the disparate impacts of deploying SSL: the sub-population who has a higher baseline accuracy without using SSL (the "rich" one) tends to benefit more from SSL; while the sub-population who suffers from a low baseline accuracy (the "poor" one) might even observe a performance drop after adding the SSL module. We theoretically and empirically establish the above observation for a broad family of SSL algorithms, which either explicitly or implicitly use a
    
[^121]: 基于噪声增强的最佳实践，以提高可部署语音情感识别系统的性能

    Best Practices for Noise-Based Augmentation to Improve the Performance of Deployable Speech-Based Emotion Recognition Systems. (arXiv:2104.08806v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2104.08806](http://arxiv.org/abs/2104.08806)

    在该论文中，作者验证了噪声的存在确实会改变注释标签，并展示了忽视这一知识会对情感识别模型的性能评估产生影响。

    

    语音情感识别是任何以人为中心的系统的重要组成部分。但一个人产生和感知的语音特征可能受到多种原因的影响，包括可取的情感和不可取的噪声。为了训练稳健的情感识别模型，我们需要一个大而真实的数据分布，但情感数据集往往很小，因此需要增加噪声。通常，噪声增强会做出一个重要的假设，即在有或无噪声的情况下，预测标签应保持不变。这对于自动语音识别来说是正确的，但对于基于感知的任务来说却未必正确。在本文中，我们做出了三个创新贡献。我们通过众包验证了噪声的存在确实会改变注释标签，从而可能改变原始的真实标签。然后，我们展示了忽视这一知识并假设真实标签的一致性会对机器学习模型的下游评估产生影响。

    Speech emotion recognition is an important component of any human centered system. But speech characteristics produced and perceived by a person can be influenced by a multitude of reasons, both desirable such as emotion, and undesirable such as noise. To train robust emotion recognition models, we need a large, yet realistic data distribution, but emotion datasets are often small and hence are augmented with noise. Often noise augmentation makes one important assumption, that the prediction label should remain the same in presence or absence of noise, which is true for automatic speech recognition but not necessarily true for perception based tasks. In this paper we make three novel contributions. We validate through crowdsourcing that the presence of noise does change the annotation label and hence may alter the original ground truth label. We then show how disregarding this knowledge and assuming consistency in ground truth labels propagates to downstream evaluation of ML models, bo
    
[^122]: 贝叶斯估计器与去偏估计器在低秩矩阵补全中的模拟比较

    Simulation comparisons between Bayesian and de-biased estimators in low-rank matrix completion. (arXiv:2103.11749v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2103.11749](http://arxiv.org/abs/2103.11749)

    本文通过模拟比较了贝叶斯方法和去偏估计器在低秩矩阵补全问题中的性能。结果表明去偏估计器与贝叶斯估计器一样好，在样本较小的情况下贝叶斯方法更稳定。

    

    本文研究低秩矩阵补全问题，这是一类机器学习问题，旨在预测部分观察矩阵中的缺失条目。这类问题在协同过滤、图像处理和基因型插补等多个具有挑战性的应用中出现。我们比较了贝叶斯方法和最近引入的去偏估计器，该估计器提供了一种有用的方式来构建感兴趣的置信区间。从理论角度来看，去偏估计器具有尖锐的极小最优估计误差速率，而贝叶斯方法通过额外的对数因子达到了这一速率。我们的模拟研究显示了有趣的结果，即去偏估计器与贝叶斯估计器一样好。此外，贝叶斯方法更加稳定，在样本较小的情况下可以优于去偏估计器。此外，我们还发现置信区间的经验覆盖率

    In this paper, we study the low-rank matrix completion problem, a class of machine learning problems, that aims at the prediction of missing entries in a partially observed matrix. Such problems appear in several challenging applications such as collaborative filtering, image processing, and genotype imputation. We compare the Bayesian approaches and a recently introduced de-biased estimator which provides a useful way to build confidence intervals of interest. From a theoretical viewpoint, the de-biased estimator comes with a sharp minimax-optimal rate of estimation error whereas the Bayesian approach reaches this rate with an additional logarithmic factor. Our simulation studies show originally interesting results that the de-biased estimator is just as good as the Bayesian estimators. Moreover, Bayesian approaches are much more stable and can outperform the de-biased estimator in the case of small samples. In addition, we also find that the empirical coverage rate of the confidence 
    
[^123]: 基于拓扑感知的张量分解用于元图学习

    Topology-aware Tensor Decomposition for Meta-graph Learning. (arXiv:2101.01078v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.01078](http://arxiv.org/abs/2101.01078)

    本论文提出了一种基于拓扑感知张量分解的方法，用于学习元图。这种方法不仅解释了现有方法的局限性，还提出了一种反映有向无环图结构的拓扑感知张量分解方法。

    

    异构图通常指具有不同类型的节点和边的图。从异构图中提取有用信息的常见方法是使用元图，可以看作是与异构图具有相同节点和边类型的特殊有向无环图（DAG）。然而，如何设计合适的元图是具有挑战性的。最近，有许多关于从异构图中学习适当的元图的研究。现有方法通常引入独立于彼此的边的连续权重，忽略了元图的拓扑结构，可能效果不佳。为解决这个问题，我们提出了一个基于张量的新视角来学习元图。这个视角不仅有助于解释现有工作中CANDECOMP/PARAFAC（CP）分解的局限性，而且激发了我们提出了一种反映DAG结构的拓扑感知张量分解方法，称为TENSUS。

    Heterogeneous graphs generally refers to graphs with different types of nodes and edges. A common approach for extracting useful information from heterogeneous graphs is to use meta-graphs, which can be seen as a special kind of directed acyclic graph (DAG) with same node and edge types as the heterogeneous graph. However, how to design proper meta-graphs is challenging. Recently, there have been many works on learning suitable meta-graphs from a heterogeneous graph. Existing methods generally introduce continuous weights for edges that are independent of each other, which ignores the topological stucture of meta-graphs and can be ineffective. To address this issue, we propose a new viewpoint from tensor on learning meta-graphs. Such a viewpoint not only helps interpret the limitation of existing works by CANDECOMP/PARAFAC (CP) decomposition, but also inspires us to propose a topology-aware tensor decomposition, called TENSUS, that reflects the structure of DAGs. The proposed topology-
    
[^124]: STEm-Seg: 用于视频实例分割的时空嵌入

    STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos. (arXiv:2003.08429v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2003.08429](http://arxiv.org/abs/2003.08429)

    本文提出了一种适用于视频实例分割的新方法，通过将视频剪辑建模为一个时空体，在一个阶段内对空间和时间进行实例分割和跟踪，从而避免了多阶段管道和特定任务的需求。我们引入了时空嵌入的概念，并使用增强特征表示的混合函数来聚类像素，以实现更好的实例分割效果。

    

    现有的视频实例分割方法通常涉及多阶段管道，遵循跟踪-检测范式，并将视频剪辑建模为图像序列。多个网络用于检测单个帧中的对象，然后在时间上关联这些检测结果。因此，这些方法通常不是端到端可训练的，并且高度适应特定任务。在本文中，我们提出了一种不同的方法，适用于涉及视频中的实例分割的各种任务。特别是，我们将视频剪辑建模为一个单一的3D时空体，提出了一种新颖的方法，在一个阶段内对空间和时间进行实例分割和跟踪。我们的问题形式化围绕时空嵌入的思想，这些嵌入被训练为在整个视频剪辑中聚类属于特定对象实例的像素。为此，我们引入了增强时空特征表示的新型混合函数。

    Existing methods for instance segmentation in videos typically involve multi-stage pipelines that follow the tracking-by-detection paradigm and model a video clip as a sequence of images. Multiple networks are used to detect objects in individual frames, and then associate these detections over time. Hence, these methods are often non-end-to-end trainable and highly tailored to specific tasks. In this paper, we propose a different approach that is well-suited to a variety of tasks involving instance segmentation in videos. In particular, we model a video clip as a single 3D spatio-temporal volume, and propose a novel approach that segments and tracks instances across space and time in a single stage. Our problem formulation is centered around the idea of spatio-temporal embeddings which are trained to cluster pixels belonging to a specific object instance over an entire video clip. To this end, we introduce (i) novel mixing functions that enhance the feature representation of spatio-te
    
[^125]: 通信高效的分布式深度学习：综述

    Communication-Efficient Distributed Deep Learning: A Comprehensive Survey. (arXiv:2003.06307v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2003.06307](http://arxiv.org/abs/2003.06307)

    本文综述了通信高效的分布式深度学习的研究进展，重点探讨了系统级和算法级的优化方法，并比较了不同算法的收敛速度。

    

    近年来，利用多个计算设备（如GPU/TPU）进行分布式深度学习已经成为减少训练时间的一种普遍方法，这是因为模型和数据集越来越大。然而，由于通信成为性能瓶颈，系统的可扩展性受到限制。解决这个通信问题已经成为一个突出的研究课题。本文提供了对通信高效的分布式训练算法的综述，重点关注系统级和算法级的优化。首先，我们提出了一个将通信同步、系统架构、压缩技术和通信和计算任务并行化等四个主要维度纳入的数据并行分布式训练算法分类方法。然后，我们研究了这四个方面的最新研究，并比较了不同算法的收敛速度以了解其收敛速度。此外，我们还对比了不同算法的收敛速度以了解其收敛速度。

    Distributed deep learning (DL) has become prevalent in recent years to reduce training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to larger models and datasets. However, system scalability is limited by communication becoming the performance bottleneck. Addressing this communication issue has become a prominent research topic. In this paper, we provide a comprehensive survey of the communication-efficient distributed training algorithms, focusing on both system-level and algorithmic-level optimizations. We first propose a taxonomy of data-parallel distributed training algorithms that incorporates four primary dimensions: communication synchronization, system architectures, compression techniques, and parallelism of communication and computing tasks. We then investigate state-of-the-art studies that address problems in these four dimensions. We also compare the convergence rates of different algorithms to understand their convergence speed. Additionally, we cond
    

