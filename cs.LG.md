# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning.](http://arxiv.org/abs/2307.14338) | 本研究显示了现有的检索增强表格深度学习解决方案与无检索基线相比几乎没有明显优势，但提出了一种能够充分利用检索增强的表格深度学习模型，解锁了其潜力。 |
| [^2] | [Waypoint-Based Imitation Learning for Robotic Manipulation.](http://arxiv.org/abs/2307.14326) | 该论文提出了一种基于路径点的机器人操作模仿学习方法，通过自动提取路径点来减少行为克隆中累积误差的问题。 |
| [^3] | [Evaluating the Moral Beliefs Encoded in LLMs.](http://arxiv.org/abs/2307.14324) | 本文提出了一种对LLMs中编码的道德信念进行评估的案例研究方法。通过设计大规模调查了解不同LLMs中的道德信念，在明确的情况下，LLMs倾向于与人类的道德直觉保持一致，但在模糊的情况下，它们的回答会有所不同，并可能存在偏见和不一致性。 |
| [^4] | [Reinforcement Learning by Guided Safe Exploration.](http://arxiv.org/abs/2307.14316) | 本文研究了一种通过引导安全探索的强化学习方法，通过训练一个代理在没有奖励信号的情况下学习安全探索，然后利用其构建一个安全的行为策略，并且借鉴迁移学习的思想，在训练过程中逐渐消除引导代理的影响。 |
| [^5] | [A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch.](http://arxiv.org/abs/2307.14304) | 提出了一个约束执行深度强化学习框架，能够有效地处理能量存储系统调度中的操作约束，通过训练深度神经网络来学习高质量的控制模型，并将其转化为混合整数规划问题进行约束执行。 |
| [^6] | [ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality.](http://arxiv.org/abs/2307.14298) | 本文研究了将ChatGPT和说服技术应用于酒店推荐系统的潜力，通过ChatGPT可以提供更准确和上下文感知的推荐，而说服技术可影响用户行为并增强推荐的说服力。 |
| [^7] | [Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis.](http://arxiv.org/abs/2307.14294) | 本概念文章研究了各种拆分序列数据的挑战，并通过电机测试台和液体中的粒子跟踪等实例进行了探索。 |
| [^8] | [General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications.](http://arxiv.org/abs/2307.14283) | 这里是中文总结出的一句话要点：本论文讨论了通用目的人工智能系统（GPAIS）的性质、定义、分类和开放挑战，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。 |
| [^9] | [Deepfake Image Generation for Improved Brain Tumor Segmentation.](http://arxiv.org/abs/2307.14273) | 本研究使用深度伪造图像生成和卷积神经网络来进行脑肿瘤分割，取得了较好的性能表现。 |
| [^10] | [Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy.](http://arxiv.org/abs/2307.14243) | 荧光神经细胞v2是一组荧光显微镜图像及其真实注释，提供了多个学习任务的注释，并有助于计算机视觉方法的进展。 |
| [^11] | [Evolving Multi-Objective Neural Network Controllers for Robot Swarms.](http://arxiv.org/abs/2307.14237) | 本研究提出了一种基于多目标演化神经网络的方法来开发机器人群体的控制器。实验结果表明，该方法可以有效地控制每个机器人，并且可在具有更多机器人的环境中进行扩展，无需进一步重新训练。 |
| [^12] | [Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences.](http://arxiv.org/abs/2307.14225) | 大规模语言模型（LLMs）在冷启动情况下提供了与基于项目协同过滤（CF）方法相当的推荐性能，特别是在纯基于语言偏好的情况下。 |
| [^13] | [Online Modeling and Monitoring of Dependent Processes under Resource Constraints.](http://arxiv.org/abs/2307.14208) | 本文提出了一种在线协作学习方法，能够适应性地分配资源，实现对相关进程的监控和动态探索，有效地进行异常事件检测。 |
| [^14] | [Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling.](http://arxiv.org/abs/2307.14199) | 该研究应用随机森林和支持向量机模型对锌厂压力过滤性能进行建模，以预测滤饼湿度和提高锌的回收率。 |
| [^15] | [Efficient Learning of Discrete-Continuous Computation Graphs.](http://arxiv.org/abs/2307.14193) | 本文研究了具有多个离散组件的随机计算图的行为，并提出了两种优化策略:增加Gumbel噪声扰动的尺度参数和使用多个离散组件。这些策略有效地提高了模型的学习能力。 |
| [^16] | [A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia.](http://arxiv.org/abs/2307.14185) | 本研究比较了基于随机森林算法的先前替代模型与两个深度学习模型（LSTM和GRU）在弗吉尼亚诺福克市街道洪水的性能。结果表明，选择支持预测不确定性传达和有效集成多模态特征的模型架构十分重要。 |
| [^17] | [Learning Disentangled Discrete Representations.](http://arxiv.org/abs/2307.14151) | 通过替换标准的高斯变分自动编码器，使用定制的分类变分自动编码器，我们发现离散潜在空间的底层网格结构可以有效缓解解离表示中的旋转不变性问题，并提供了优化解离表示的无监督模型选择策略。 |
| [^18] | [Toward Design of Synthetic Active Inference Agents by Mere Mortals.](http://arxiv.org/abs/2307.14145) | 本文讨论了如何设计一个能够支持非专家工程师开发主动推理代理的软件工具箱，以实现在边缘设备上运行的有效代理。旨在加速主动推理代理的民主化进程。 |
| [^19] | [Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards.](http://arxiv.org/abs/2307.14138) | 本研究研究了具有因果关系奖励的分段稳定组合半强盗问题，并提出了上界置信度算法以应对非平稳环境中的挑战。此外，引入了组重启的概念作为结构化环境中的备份策略。 |
| [^20] | [Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models.](http://arxiv.org/abs/2307.14134) | 本研究开发并评估了小型到中型的土耳其BERT模型，在填补资源匮乏语言领域的研究空白方面取得了积极的成果，并为开发和应用小型语言模型提供了有价值的见解。 |
| [^21] | [GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs.](http://arxiv.org/abs/2307.14109) | 本文对GraphRNN进行了复现和评估，发现You等人建议的BFS遍历对模型性能有重要贡献。此外，通过将BFS遍历替换为拓扑排序，我们扩展了GraphRNN以生成有向无环图，并在真实数据集上取得了显著改进。 |
| [^22] | [Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks.](http://arxiv.org/abs/2307.14085) | 本文研究了强化学习中的量化斯坦克伯格均衡问题，提出了省样本量的在线和离线算法，并通过推断追随者的行动来学习量化响应模型。 |
| [^23] | [Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation.](http://arxiv.org/abs/2307.14068) | 该论文提出了一种名为D3AAMDA的新方法，通过动态调整特征分布的对齐程度，有效地利用源域内的局部有利特征信息，以解决多源无监督领域适应中的性能差距和冗余特征的问题。 |
| [^24] | [Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions.](http://arxiv.org/abs/2307.14067) | 机器学习在医疗领域有着巨大的应用潜力，但由于分散的信息和缺乏易于理解的文档，使得其应用受到了限制。 |
| [^25] | [Pre-Training with Diffusion models for Dental Radiography segmentation.](http://arxiv.org/abs/2307.14066) | 本文提出了一种利用扩散模型进行预训练的方法，用于牙科放射学分割，实验结果表明该方法在标签效率方面具有显著性能，竞争力强。 |
| [^26] | [Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification.](http://arxiv.org/abs/2307.14025) | 本论文提出一种基于拓扑正则化的多实例学习方法，用于罕见贫血疾病的红细胞分类。通过从单个红细胞图像中提取多尺度的拓扑特征来进行模型正则化，以保持数据的特征拓扑属性。实验结果表明，该方法是有效的。 |
| [^27] | [Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?.](http://arxiv.org/abs/2307.14023) | 通过澄清softmax函数与Boltzmann算符之间的关系，我们证明了单层具有低秩权重矩阵的自注意力具备完全捕获整个输入序列上下文的能力，单层Transformer对于有限样本的记忆能力，单层自注意力Transformer是紧凑域上连续函数的通用逼近器。 |
| [^28] | [MCMC-Correction of Score-Based Diffusion Models for Model Composition.](http://arxiv.org/abs/2307.14012) | 本文提出了一种修正基于得分的扩散模型的方法，使其能够与各种MCMC方法结合，从而实现模型组合和进行更好的采样。 |
| [^29] | [Fast algorithms for k-submodular maximization subject to a matroid constraint.](http://arxiv.org/abs/2307.13996) | 本文提出了一个快速算法，通过应用阈值递减算法来最大化满足拟阵约束的k-submodular函数。该算法在降低查询复杂度的同时，近似比例几乎没有损失。另外，作者还给出了对于单调和非单调情况的不同近似算法，并将总大小约束下的问题作为推论给出。 |
| [^30] | [Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space.](http://arxiv.org/abs/2307.13995) | 个性化联邦学习（PFL）允许客户端在数据不同领域的应用场景中使用不同的模型。然而，由于数据分布的差异，全局编码器产生的通用特征包含许多对某个特定客户端任务无关的组件。本文提出利用特征空间的低维度特点，实现有效的个性化联邦学习。 |
| [^31] | [BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States.](http://arxiv.org/abs/2307.13994) | 这项研究通过使用机器学习技术，分析了乳牛在负性情绪下的声音特征，为开发非侵入性动物情绪状态指标提供了重要的参考。 |
| [^32] | [METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation.](http://arxiv.org/abs/2307.13991) | 本文提出了METAVerse，一个用于在各种环境中准确可靠地预测地形可行性的元学习框架。通过自监督学习，利用稀疏的LiDAR点云生成密集连续值成本图，通过元学习训练全局模型，有效减小地形可行性估计的不确定性。 |
| [^33] | [This is not correct! Negation-aware Evaluation of Language Generation Systems.](http://arxiv.org/abs/2307.13989) | 本文提出了一种对语言生成系统进行否定感知的评估方法，并开发了NegBLEURT评估指标以提高对否定的敏感性。实验结果表明，该方法在否定句上的表现优于现有指标，并且在其他扰动上保持了性能。 |
| [^34] | [Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation.](http://arxiv.org/abs/2307.13978) | 本文提出了一种通过将强化学习代理与潜在空间 GAN 集成来控制生成过程的新方法，在实验证明其有效性的基础上。 |
| [^35] | [Understanding Deep Neural Networks via Linear Separability of Hidden Layers.](http://arxiv.org/abs/2307.13962) | 本文通过测量深度神经网络隐藏层输出的线性可分性来研究其特性，并发现隐藏层输出的线性可分性程度与网络训练性能有同步性，进一步探讨激活函数和网络尺寸对隐藏层线性可分性的影响。 |
| [^36] | [Entropy Neural Estimation for Graph Contrastive Learning.](http://arxiv.org/abs/2307.13944) | 本文提出了一种基于熵神经估计的图对比学习方法，通过最大化互信息下界来近似估计数据集的熵。通过简单但有效的子集抽样策略对比数据集不同视图中的节点表示，同时使用两个目标优化网络的学习过程。 |
| [^37] | [Topology-aware Robust Optimization for Out-of-distribution Generalization.](http://arxiv.org/abs/2307.13943) | 本文提出了一种基于拓扑感知的鲁棒优化方法，用于解决越界泛化问题。该方法通过整合分布的拓扑结构来约束优化过程，实现高泛化置信度。理论和实证结果表明，该方法在越界泛化任务中明显优于现有方法。 |
| [^38] | [Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network.](http://arxiv.org/abs/2307.13938) | 提出了一种用于半监督语义分割的双级孪生结构网络（DSSN），通过像素级对比学习来充分利用未标记数据，并引入了类别感知伪标签选择策略，最大限度地提高了算法的有效性。 |
| [^39] | [trajdata: A Unified Interface to Multiple Human Trajectory Datasets.](http://arxiv.org/abs/2307.13924) | trajdata是一个统一接口，提供简单、统一和高效的轨迹和地图数据的表示和API，在轨迹预测领域起到了重要作用，并对当前的人类和AV运动预测研究提供了深入理解和未来数据集的建议。 |
| [^40] | [Simulation-based Inference for Cardiovascular Models.](http://arxiv.org/abs/2307.13918) | 本研究将心血管模型的逆问题作为统计推理进行解决，在体外进行了五个生物标记物的不确定性分析，展示了模拟推理的能力。 |
| [^41] | [BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery.](http://arxiv.org/abs/2307.13917) | 这项研究引入了一种基于梯度的后验采样方法，用于解决Bayesian causal discovery中的计算挑战，能够高效地推断因果模型，并且不依赖于DAG正则化。 |
| [^42] | [Online learning in bandits with predicted context.](http://arxiv.org/abs/2307.13916) | 本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。 |
| [^43] | [Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength.](http://arxiv.org/abs/2307.13909) | 本论文提出了一个基于图神经网络的混合框架，用于预测颗粒破碎强度。通过建模颗粒碎片之间的关系，可以有效地描述颗粒破碎的力学行为。此外，为了促进机器学习在颗粒破碎研究中的进展，研究人员生成了一个包含大量数值模拟数据的开源数据集。 |
| [^44] | [Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input.](http://arxiv.org/abs/2307.13907) | 本文介绍了一种利用基于星星的可达性分析和变长时间序列输入的深度神经网络鲁棒性验证方法，并在预测与健康管理领域的SOC估计和RUL估计中进行了应用。 |
| [^45] | [Corruption-Robust Lipschitz Contextual Search.](http://arxiv.org/abs/2307.13903) | 该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。 |
| [^46] | [Regularizing Neural Networks with Meta-Learning Generative Models.](http://arxiv.org/abs/2307.13899) | 本文提出了一种名为元生成正则化（MGR）的新型生成数据增强策略，通过将合成样本用于特征提取器的正则化项而不是损失函数，最小化验证损失，提高了深度学习中的生成数据增强效果。 |
| [^47] | [Efficient Estimation of the Local Robustness of Machine Learning Models.](http://arxiv.org/abs/2307.13885) | 本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。 |
| [^48] | [ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis.](http://arxiv.org/abs/2307.13883) | ExeDec是一种基于分解的合成策略，通过预测执行子目标并在每个步骤的程序执行的指导下逐步解决问题，实现了更好的合成性能和组合泛化能力。 |
| [^49] | [Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory.](http://arxiv.org/abs/2307.13869) | 本研究提出了一种新的物理信息神经网络训练方法，受数论方法启发，通过选择适当的插值点来提高解决偏微分方程的准确性和效率。 |
| [^50] | [Learning sources of variability from high-dimensional observational studies.](http://arxiv.org/abs/2307.13868) | 本研究提出了一种针对高维观测研究的方法，将因果估计泛化到任意维度或可测空间的结果，并提出了一种用于名义变量的因果偏差测试。实验证明该方法相比现有策略在有限样本有效性和功率方面有改进。 |
| [^51] | [Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT.](http://arxiv.org/abs/2307.13865) | 本文介绍了预训练的深度2.5D模型，用于高效预测视网膜OCT的疾病进展。采用混合2.5D方法结合二维和三维技术，有效优化了性能和内存要求。 |
| [^52] | [Learning to Design Analog Circuits to Meet Threshold Specifications.](http://arxiv.org/abs/2307.13861) | 本论文提出了一种使用监督学习方法，通过仿真数据训练系统来设计满足阈值规范的模拟电路，实验结果显示成功率超过90％。 |
| [^53] | [On the unreasonable vulnerability of transformers for image restoration -- and an easy fix.](http://arxiv.org/abs/2307.13856) | 我们调查了Vision Transformers（ViTs）在图像恢复任务中的鲁棒性，发现ViTs在图像分类任务中的改进并不能扩展到图像恢复任务中，并且这些模型易受对抗攻击。我们尝试通过对抗性训练来提高它们的鲁棒性。 |
| [^54] | [Exploring the Sharpened Cosine Similarity.](http://arxiv.org/abs/2307.13855) | 该论文研究了锐化余弦相似度(SCS)在神经网络中作为卷积的替代品的可能性。研究发现，虽然SCS可能不会提高准确性，但可能学习到更可解释的表示，并在某些情况下稍微增加对抗性的鲁棒性。 |
| [^55] | [WebArena: A Realistic Web Environment for Building Autonomous Agents.](http://arxiv.org/abs/2307.13854) | WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。 |
| [^56] | [SplitFed resilience to packet loss: Where to split, that is the question.](http://arxiv.org/abs/2307.13851) | 本文研究了分割联邦学习（SplitFed）在通信链路上对数据包丢失的鲁棒性，并通过实验证明了更深的切割点对最终模型的准确性具有统计显著的优势。 |
| [^57] | [MAEA: Multimodal Attribution for Embodied AI.](http://arxiv.org/abs/2307.13850) | MAEA是一个用于计算任何可微分的多模态智能体人工智能政策的全局归因的框架。它可以通过归因分析来排名和分组故障场景，调查建模和数据集偏见，并对多模态智能体人工智能政策的稳健性和用户信任进行关键分析。 |
| [^58] | [Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search.](http://arxiv.org/abs/2307.13831) | 这项研究分析了使用Armijo线搜索的随机梯度下降非凸优化中批大小和步数的关系，并发现随着批大小的增加，所需的步数减少。 |
| [^59] | [Offline Reinforcement Learning with On-Policy Q-Function Regularization.](http://arxiv.org/abs/2307.13824) | 本论文提出了一种无模型强化学习方法，通过正则化向行为策略的Q函数进行训练，而不是训练行为策略本身。该方法利用了Q函数的估计来处理外推误差，并在D4RL基准测试中展现了强大的性能。 |
| [^60] | [Fitting Auditory Filterbanks with Multiresolution Neural Networks.](http://arxiv.org/abs/2307.13821) | 本文提出了一种名为多分辨率神经网络（MuReNN）的神经音频模型，通过在离散小波变换（DWT）的八度子带上训练单独的卷积算子，解决了基于波形的深度学习面临的非参数和参数方法之间的困境。 |
| [^61] | [Gradient-Based Spectral Embeddings of Random Dot Product Graphs.](http://arxiv.org/abs/2307.13818) | 本文介绍了基于梯度的随机点积图谱嵌入方法，并通过利用非凸优化技术改进了在观察图中估计节点潜在向量的任务。同时，作者还提出了一阶梯度下降方法来更好地解决嵌入问题，并适应更广泛的实用网络嵌入应用。 |
| [^62] | [How to Scale Your EMA.](http://arxiv.org/abs/2307.13813) | 本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。 |
| [^63] | [Sports Betting: an application of neural networks and modern portfolio theory to the English Premier League.](http://arxiv.org/abs/2307.13807) | 本文提出了一种将神经网络模型与投资组合优化相结合的新方法，通过研究英超联赛数据，成功实现了超过初始财富135.8%的惊人利润的体育博彩策略优化。 |
| [^64] | [Source Condition Double Robust Inference on Functionals of Inverse Problems.](http://arxiv.org/abs/2307.13793) | 本文提出了一种源条件双稳健推断方法，用于估计线性逆问题解的线性函数参数，无需知道哪个逆问题更良好，该方法能确保对感兴趣的参数的渐近正态性，并提供了对迭代Tikhonov正则化对抗估计器的新保证。 |
| [^65] | [Histogram Layer Time Delay Neural Networks for Passive Sonar Classification.](http://arxiv.org/abs/2307.13788) | 本文提出了一种直方图层时间延迟神经网络的方法，用于改进水下声纳目标识别。通过引入统计背景，该方法在特征学习和分类中取得了优越表现。 |
| [^66] | [The GANfather: Controllable generation of malicious activity to improve defence systems.](http://arxiv.org/abs/2307.13787) | GANfather是一种用于改进防御系统的可控生成方法，能够生成具有恶意行为属性但不需要标记要求的样本。通过引入额外的目标，我们能够提高对非法活动的检测能力，并发现并修正防御性弱点。 |
| [^67] | [Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach.](http://arxiv.org/abs/2307.13771) | 本文通过添加预训练模块，在差分隐私逻辑回归中提高了模型的准确性。 |
| [^68] | [ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning.](http://arxiv.org/abs/2307.13766) | ClusterSeq是一种基于聚类的元学习顺序推荐系统，通过利用用户序列的动态信息提高了物品预测的准确性，并保留了次要用户的偏好，并利用了同一聚类中用户的集体知识。 |
| [^69] | [Implicitly Normalized Explicitly Regularized Density Estimation.](http://arxiv.org/abs/2307.13763) | 我们提出了一种新的非参数密度估计方法，基于正则化密度的 Sobolev 范数，通过适当的初始化和使用自然梯度，可以得到性能良好的解，该方法在 Anomaly Detection benchmark 中排名第二。 |
| [^70] | [UPREVE: An End-to-End Causal Discovery Benchmarking System.](http://arxiv.org/abs/2307.13757) | UPREVE是一个用户友好的端到端因果发现评估系统，它允许用户同时运行多个算法，可视化因果关系，并评估学习到的因果图的准确性，旨在使因果发现对社会计算和行为文化建模等领域的研究人员和实践者更易于访问和用户友好，以获得有价值的见解进行更好的决策。 |
| [^71] | [Solution Path of Time-varying Markov Random Fields with Discrete Regularization.](http://arxiv.org/abs/2307.13750) | 本文提出了一种新的离散正则化的方法来解决推断稀疏时间变化的马尔科夫随机场问题。通过离开最大似然估计范式，我们可以有效地求解所有稀疏水平的解路径，并得到理想的统计性质。 |
| [^72] | [mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization.](http://arxiv.org/abs/2307.13744) | mL-BFGS是一种轻量级的基于动量的L-BFGS算法，通过引入动量方案和减少Hessian中的随机噪声，稳定了大规模分布式深度神经网络的优化过程。 |
| [^73] | [FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning.](http://arxiv.org/abs/2307.13716) | FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。 |
| [^74] | [Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions.](http://arxiv.org/abs/2307.13715) | 本文提出了一个名为ShuttleNet的框架，通过利用过去的击球信息，显著改进了预测羽毛球击球类型和位置的性能，最终在CoachAI羽毛球挑战赛中获得第一名。 |
| [^75] | [Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items.](http://arxiv.org/abs/2307.13709) | 本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。 |
| [^76] | [Control and Monitoring of Artificial Intelligence Algorithms.](http://arxiv.org/abs/2307.13705) | 论文阐述了控制和监控人工智能算法的重要性，介绍了数据漂移和概念漂移的概念，并提出了一系列指标用于审查模型在潜在时间变化方面的性能。 |
| [^77] | [eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review.](http://arxiv.org/abs/2307.13704) | 本综述探讨了可解释人工智能（XAI）在年龄预测任务中的应用。通过系统性综述，我们讨论了XAI方法在医疗应用和年龄预测领域的益处。 |
| [^78] | [Measuring Faithfulness in Chain-of-Thought Reasoning.](http://arxiv.org/abs/2307.13702) | 本研究探讨了在回答问题之前，大型语言模型（LLMs）能否进行忠实的“链式思维”推理。结果显示，模型在不同任务上对链式思维的依赖程度有很大变化，随着模型变得更大更能力越强，它们在大多数任务中产生的推理越来越不忠实。 |
| [^79] | [$\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation.](http://arxiv.org/abs/2307.13701) | 本文提出了$\text{EFO}_{k}$-CQA框架，用于超越集合操作的知识图谱复杂查询回答。该框架包括数据生成、模型训练和方法评估，并且扩展了现有查询空间。使用构建的$\text{EFO}_{k}$-CQA数据集进行实证评估，结果揭示了查询难度对结果的影响。此外，还证明了现有数据集构建过程存在的问题。 |
| [^80] | [CAMP: A Context-Aware Cricket Players Performance Metric.](http://arxiv.org/abs/2307.13700) | CAMP是一种上下文感知的板球球员表现指标，通过综合考虑对手实力和比赛环境等因素，可以量化个体球员对比赛结果的贡献。CAMP通过数据挖掘方法，为选择和草案、教练和训练、团队阵容和战略制定提供数据驱动的决策支持。 |
| [^81] | [Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance.](http://arxiv.org/abs/2307.13698) | 本研究通过使用可解释性方法，探究了彩票票据假说在稀疏网络性能方面的洞察，发现修剪的网络性能会降低，并且修剪后的网络产生的概念和像素与原始网络存在不一致性。 |
| [^82] | [Duet: efficient and scalable hybriD neUral rElation undersTanding.](http://arxiv.org/abs/2307.13494) | Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。 |
| [^83] | [On the learning Dynamics of Attention Networks.](http://arxiv.org/abs/2307.13421) | 本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。 |
| [^84] | [Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics.](http://arxiv.org/abs/2307.12171) | 本文介绍了一种名为LtC的协作框架，可以通过学习来有效减小流媒体视频的流量，提高流媒体视频分析的效率。 |
| [^85] | [AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models.](http://arxiv.org/abs/2307.10711) | AdjointDPM是一种新的伴随灵敏度方法，用于扩散概率模型的梯度反向传播，解决了DPM定制化中内存消耗高的问题，并通过解决增强的ODE将损失的梯度反向传播到模型的参数。 |
| [^86] | [Deceptive Alignment Monitoring.](http://arxiv.org/abs/2307.10569) | 本论文提出了欺骗性对齐监测这一新方向，旨在探讨大型机器学习模型在表面上表现正常，却暗中进行隐藏行为的问题，并提出了新的研究机会。 |
| [^87] | [TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations.](http://arxiv.org/abs/2307.09916) | TimeTuner是一个新颖的可视化分析框架，旨在帮助分析人员理解时间序列预测中模型行为与时间表示的关系，并解决自动化特征学习方法的局限性。 |
| [^88] | [Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications.](http://arxiv.org/abs/2307.09469) | 该论文提出了一种对高保真度等离子体模拟中的磁场拓扑进行图表示的方法，并应用于地球磁层模拟中，旨在挑战机器学习社区通过图机器学习方法解决具有广泛潜力影响的科学问题。 |
| [^89] | [No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models.](http://arxiv.org/abs/2307.06440) | 本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。 |
| [^90] | [Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering.](http://arxiv.org/abs/2307.05915) | 提出了一个名为Prompt Generate Train (PGT)的框架，用于少样本领域适应、对齐和不确定性校准的检索增强生成模型的开发。该框架通过有监督的微调和强化学习，将模型适应到目标领域，并在生成相关答案方面具有与基于GPT-4的模型相竞争的性能。 |
| [^91] | [Manifold Filter-Combine Networks.](http://arxiv.org/abs/2307.04056) | 这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。 |
| [^92] | [FedNoisy: Federated Noisy Label Learning Benchmark.](http://arxiv.org/abs/2306.11650) | FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。 |
| [^93] | [The Information Bottleneck's Ordinary Differential Equation: First-Order Root-Tracking for the IB.](http://arxiv.org/abs/2306.09790) | 本文重新导出了信息瓶颈的一阶ODR，描述了其最优权衡曲线的动态，揭示了最优编码的动态过程。 |
| [^94] | [Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models.](http://arxiv.org/abs/2306.02115) | 本文提出了一个表格和图像生成的任务，研究了预训练的视觉与语言模型中关于实体的知识如何被保留。通过实验结果发现，预训练模型OFA在生成图像的过程中忘记了部分实体知识。 |
| [^95] | [SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning.](http://arxiv.org/abs/2305.19442) | SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。 |
| [^96] | [Exploring Weight Balancing on Long-Tailed Recognition Problem.](http://arxiv.org/abs/2305.16573) | 研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。 |
| [^97] | [Productive Crop Field Detection: A New Dataset and Deep Learning Benchmark Results.](http://arxiv.org/abs/2305.11990) | 本研究提出了一个高质量的数据集，使用半监督和最先进的深度学习方法自动检测高产农田，获得了很高的准确性，有望为农民提供帮助. |
| [^98] | [Policy Gradient Algorithms Implicitly Optimize by Continuation.](http://arxiv.org/abs/2305.06851) | 本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。 |
| [^99] | [Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards.](http://arxiv.org/abs/2304.14989) | 本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。 |
| [^100] | [Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning.](http://arxiv.org/abs/2304.04660) | 本研究提出了一种基于不确定性的轨迹截断方法（TATU），用于解决模型驱动的离线强化学习中生成样本不可靠的问题。实验证明TATU相较于其他方法表现优越。 |
| [^101] | [Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts.](http://arxiv.org/abs/2303.17595) | 本研究指出传统的图片分类器学习过程忽视注释过程中的辅助信息，提出了使用注释副产品来训练模型的新方法，该方法可以减少虚假相关性并提高模型精度。 |
| [^102] | [Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles Using Transformer Networks.](http://arxiv.org/abs/2303.16109) | 提出了一种新颖的基于Transformer网络的多模态操作和轨迹预测框架，能够预测多个可能的行为模式及其概率，并且在两个公共基准公路驾驶数据集上得到了优秀的表现。 |
| [^103] | [FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle.](http://arxiv.org/abs/2302.14831) | 本研究提出了一种用于少样本生物特征认证的牛脸嵌入分布模型，通过计算测试嵌入和训练嵌入的马氏距离来进行认证。实验结果表明，在ImageNet数据集上预训练的模型表现更好。使用VGG16模型，在一个包含20个牛身份的数据集上，我们得到了1.25%的FRR和1.18%的FAR。 |
| [^104] | [Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework.](http://arxiv.org/abs/2302.12247) | 通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。 |
| [^105] | [Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System.](http://arxiv.org/abs/2302.12004) | 这项研究提出了一种基于知识蒸馏的方法，在分散制造系统中实现了高效且安全的信息共享，以提升性能较差的单元的在线过程监测。 |
| [^106] | [Uniformity Testing over Hypergrids with Subcube Conditioning.](http://arxiv.org/abs/2302.09013) | 本论文提出了一种用于测试超网格上分布均匀性的算法，该算法在具有子立方体条件抽样预言机的情况下，进行多次查询后可以获得高效的结果。通过使用傅里叶分析证明了超网格上函数的 Pisier 不等式的鲁棒版本。 |
| [^107] | [Factor Fields: A Unified Framework for Neural Fields and Beyond.](http://arxiv.org/abs/2302.01226) | 因子场是一种统一的框架，将信号分解为因子的乘积，通过神经场或常规场表示进行处理。这个框架可以概括多种最近的信号表示方法，并且可以创建出更强大的新信号表示，如本文提出的系数基函数分解（CoBaFa）。实验结果表明，CoBaFa在逼近质量、紧凑性和效率方面相较于之前的方法有所改进。 |
| [^108] | [Client Selection in Federated Learning: Principles, Challenges, and Opportunities.](http://arxiv.org/abs/2211.01549) | 本文系统地介绍了面向联邦学习的客户端选择的原则、挑战和机会，以及针对客户端异构性问题的各种算法的性能改进。该研究旨在帮助从业者选择最适合其应用的客户端选择机制，并激发研究人员对该领域的进一步探索。 |
| [^109] | [An optimal control perspective on diffusion-based generative modeling.](http://arxiv.org/abs/2211.01364) | 本文建立了随机最优控制与基于扩散的生成模型之间的联系，推导了用于控制潜在SDE边际密度演化的汉密尔顿-雅可比-贝尔曼方程，并将生成建模表述为对合适度量之间Kullback-Leibler散度的最小化。此外，作者还开发了一种新型扩散方法用于采样非归一化密度。 |
| [^110] | [Multi-Target Tracking with Transferable Convolutional Neural Networks.](http://arxiv.org/abs/2210.15539) | 本论文提出了一种使用可迁移的卷积神经网络进行多目标跟踪的方法。该方法利用图像表示目标状态和传感器测量，并通过迁移学习实现了在大规模上进行MTT，并在10个目标的MTT任务中表现优于传统方法，在250个目标的更大MTT任务中性能提高了29%。 |
| [^111] | [Teal: Learning-Accelerated Optimization of WAN Traffic Engineering.](http://arxiv.org/abs/2210.13763) | Teal是一种学习加速的广域网流量工程优化算法，利用GPU的并行处理能力加速TE控制。它使用流为中心的图神经网络来捕捉WAN连接和网络流量，并采用多智能体强化学习算法进行独立分配和中心化优化。最后，它使用交替方向乘子法对分配进行微调。 |
| [^112] | [Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes.](http://arxiv.org/abs/2210.07612) | 本研究研究了机器学习模型中不确定性估计的问题，证明了通过调整超参数可以提高边际似然，但交叉验证度量表现出双下降现象。 |
| [^113] | [Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review.](http://arxiv.org/abs/2210.03829) | 本文综述了早期检测树皮甲虫攻击的过去和现有进展，重点关注了使用遥感和机器学习方法的优势和劣势，以及提供了有关树皮甲虫物种、攻击阶段、寄主树木、研究区域、遥感平台与传感器、光谱分辨率、光谱特征、机器学习方法和深度学习网络的知识。 |
| [^114] | [Estimating large causal polytrees from small samples.](http://arxiv.org/abs/2209.07028) | 本文介绍了一种算法，可以在变量数量远大于样本大小的情况下，准确地估计大规模因果多树结构，而几乎不需要任何分布或建模的假设。 |
| [^115] | [AudioLM: a Language Modeling Approach to Audio Generation.](http://arxiv.org/abs/2209.03143) | AudioLM是一个通过语言建模方法实现音频生成的框架，它通过将输入音频映射为离散符号序列，结合语言模型和神经音频编解码器，能够生成高质量、符合语法和语义要求的连续音频。 |
| [^116] | [SHAP-XRT: The Shapley Value Meets Conditional Independence Testing.](http://arxiv.org/abs/2207.07038) | 本文展示了Shapley值解释方法与条件独立性测试之间的紧密关系，并介绍了一种基于条件随机化测试的测试过程SHAP-XRT，用于二分类问题中证明了Shapley值的边际贡献。 |
| [^117] | [Beyond the Edge of Stability via Two-step Gradient Updates.](http://arxiv.org/abs/2206.04172) | 本文研究了梯度下降对于非Lipschitz梯度的损失函数的收敛性，发现在过参数化模型中，尽管存在局部不稳定性和振荡行为，梯度下降仍然能够收敛。 |
| [^118] | [TreeFlow: Going beyond Tree-based Gaussian Probabilistic Regression.](http://arxiv.org/abs/2206.04140) | TreeFlow是一种基于树集成和归一化流模型的方法，用于解决具有复杂分布的回归问题。 |
| [^119] | [Efficient and Accurate Physics-aware Multiplex Graph Neural Networks for 3D Small Molecules and Macromolecule Complexes.](http://arxiv.org/abs/2206.02789) | 本论文提出了一种名为PaxNet的物理感知多重图神经网络，用于高效准确地学习3D小分子和大分子复合物的表示。PaxNet通过分离局部和非局部相互作用的建模，并减少了与角度相关的计算。它可以预测矢量属性并在实验中取得了良好的性能。 |
| [^120] | [Fairness in Recommendation: Foundations, Methods and Applications.](http://arxiv.org/abs/2205.13619) | 这篇论文对推荐系统中的公平性问题进行了系统调查，针对推荐过程中可能出现的数据或算法偏见，提供了一些方法和应用来提升推荐中的公平性。 |
| [^121] | [Robust Quantity-Aware Aggregation for Federated Learning.](http://arxiv.org/abs/2205.10848) | 本文提出了一种针对联邦学习的强健的数量感知聚合算法，名为FedRA，能够在聚合模型时考虑本地数据的数量，并能够抵御数量增强攻击。 |
| [^122] | [On Embeddings for Numerical Features in Tabular Deep Learning.](http://arxiv.org/abs/2203.05556) | 本文研究了表格深度学习中关于数值特征的嵌入方法，提出了两种不同的构建嵌入模块的方法，并通过实验证明，与传统模块相比，这些方法可以显著提升模型性能。这对于构建更强大的深度学习模型并在一些传统上适用于GBDT的基准上与之竞争具有重要的益处。 |
| [^123] | [MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent.](http://arxiv.org/abs/2203.04317) | 本文提出了MICDIR方法，在多尺度上使用UNetMSS和自构建图潜变量，用于解决医学图像配准中存在的全局依赖性和大形变的问题。 |
| [^124] | [MetaDT: Meta Decision Tree with Class Hierarchy for Interpretable Few-Shot Learning.](http://arxiv.org/abs/2203.01482) | 本论文提出了一种名为MetaDT的新颖元学习决策树框架，用于可解释的少样本学习任务。通过引入概念层次结构和决策树模型，实现了FSL决策过程的可解释性。从而在使用少量示例识别新类别时，提供了一个明确的决策过程。 |
| [^125] | [Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice.](http://arxiv.org/abs/2202.12780) | 这篇用户指南介绍了机器学习和精算实践中一致性评分函数的比较和校准评估技术，并强调了事先确定目标预测函数和选择相应评分函数的重要性。 |
| [^126] | [Combining optimal path search with task-dependent learning in a neural network.](http://arxiv.org/abs/2201.11104) | 这篇论文提出了一种在神经网络中结合最优路径搜索和任务相关学习的方法，通过将成本值转化为神经网络的权重来实现在线权重适应。实验结果表明，该方法与经典算法Bellman-Ford具有相同的解，并且网络学习机制可以进一步增强算法的性能。 |
| [^127] | [Optimal Simple Regret in Bayesian Best Arm Identification.](http://arxiv.org/abs/2111.09885) | 该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。 |
| [^128] | [Revisiting Deep Learning Models for Tabular Data.](http://arxiv.org/abs/2106.11959) | 本论文重新审视了用于表格数据的深度学习模型，提出了两种简单且强大的深度学习架构作为性能基准，包括类似于ResNet的架构和适应于表格数据的Transformer架构。这些基准模型在不同问题上表现出有竞争力的性能。 |
| [^129] | [Compressible Spectral Mixture Kernels with Sparse Dependency Structures for Gaussian Processes.](http://arxiv.org/abs/1808.00560) | 本文提出了一种带有稀疏依赖结构的可压缩频谱混合核用于高斯过程，通过改进原始核的泛化性能。具体方法包括通过交叉协方差和交叉卷积泛化依赖结构，以及通过参数化时间和相位延迟提高依赖结构的表达能力。 |

# 详细

[^1]: TabR：解锁检索增强的表格深度学习的潜力

    TabR: Unlocking the Power of Retrieval-Augmented Tabular Deep Learning. (arXiv:2307.14338v1 [cs.LG])

    [http://arxiv.org/abs/2307.14338](http://arxiv.org/abs/2307.14338)

    本研究显示了现有的检索增强表格深度学习解决方案与无检索基线相比几乎没有明显优势，但提出了一种能够充分利用检索增强的表格深度学习模型，解锁了其潜力。

    

    近年来，针对表格数据问题的深度学习模型受到越来越多的关注，而基于梯度提升决策树（GBDT）的算法仍然是一个强大的解决方案。随着自然语言处理和计算机视觉等其他领域的最新趋势，最近提出了几种检索增强的表格深度学习模型。对于给定的目标对象，检索模型从可用的（训练）数据中检索其他相关对象，例如最近邻居，并使用它们的特征甚至标签来进行更好的预测。然而，我们表明，现有的检索增强表格深度学习解决方案与适当调整的简单无检索基线相比，几乎没有或者只有微小的优势。因此，检索增强方法是否值得在表格深度学习中继续探索还不清楚。

    Deep learning (DL) models for tabular data problems are receiving increasingly more attention, while the algorithms based on gradient-boosted decision trees (GBDT) remain a strong go-to solution. Following the recent trends in other domains, such as natural language processing and computer vision, several retrieval-augmented tabular DL models have been recently proposed. For a given target object, a retrieval-based model retrieves other relevant objects, such as the nearest neighbors, from the available (training) data and uses their features or even labels to make a better prediction. However, we show that the existing retrieval-based tabular DL solutions provide only minor, if any, benefits over the properly tuned simple retrieval-free baselines. Thus, it remains unclear whether the retrieval-based approach is a worthy direction for tabular DL.  In this work, we give a strong positive answer to this question. We start by incrementally augmenting a simple feed-forward architecture wit
    
[^2]: 基于路径点的机器人操作模仿学习

    Waypoint-Based Imitation Learning for Robotic Manipulation. (arXiv:2307.14326v1 [cs.RO])

    [http://arxiv.org/abs/2307.14326](http://arxiv.org/abs/2307.14326)

    该论文提出了一种基于路径点的机器人操作模仿学习方法，通过自动提取路径点来减少行为克隆中累积误差的问题。

    

    尽管模仿学习方法在机器人操作中再次引起了广泛关注，但误差累积问题仍然困扰着行为克隆（BC）。路径点可以通过减少BC学习问题的视野来解决这个问题，从而减少随时间累积的误差。然而，路径点标记是不充分的，并且需要额外的人工监督。我们的关键见解是，如果一个轨迹段可以用线性运动近似，那么端点可以用作路径点。我们提出了一种基于自动路径点提取（AWE）的模仿学习预处理模块，将演示分解为一组最小的路径点，线性插值可以近似到指定的误差阈值。AWE可以与任何BC算法结合使用，并且我们发现AWE能够提高最先进算法的成功率。

    While imitation learning methods have seen a resurgent interest for robotic manipulation, the well-known problem of compounding errors continues to afflict behavioral cloning (BC). Waypoints can help address this problem by reducing the horizon of the learning problem for BC, and thus, the errors compounded over time. However, waypoint labeling is underspecified, and requires additional human supervision. Can we generate waypoints automatically without any additional human supervision? Our key insight is that if a trajectory segment can be approximated by linear motion, the endpoints can be used as waypoints. We propose Automatic Waypoint Extraction (AWE) for imitation learning, a preprocessing module to decompose a demonstration into a minimal set of waypoints which when interpolated linearly can approximate the trajectory up to a specified error threshold. AWE can be combined with any BC algorithm, and we find that AWE can increase the success rate of state-of-the-art algorithms by u
    
[^3]: 评估LLMs中编码的道德信念

    Evaluating the Moral Beliefs Encoded in LLMs. (arXiv:2307.14324v1 [cs.CL])

    [http://arxiv.org/abs/2307.14324](http://arxiv.org/abs/2307.14324)

    本文提出了一种对LLMs中编码的道德信念进行评估的案例研究方法。通过设计大规模调查了解不同LLMs中的道德信念，在明确的情况下，LLMs倾向于与人类的道德直觉保持一致，但在模糊的情况下，它们的回答会有所不同，并可能存在偏见和不一致性。

    

    本文提供了一个关于对大型语言模型(LLMs)进行设计、管理、后处理和评估调查的案例研究。它包括两个组成部分：(1) 一种用于获取LLMs中编码的信念的统计方法。我们介绍了统计量和评估指标，用于量化LLM“做出选择”的概率、相关的不确定性以及选择的一致性。(b) 我们将这种方法应用于研究不同LLMs中编码的道德信念，特别是在正确选择不明显的模糊情况下。我们设计了一个大规模调查，其中包括680个高模糊度的道德场景（例如，“我应该撒一个善意的谎言吗？”）和687个低模糊度的道德场景（例如，“我应该为路上的行人停下来吗？”）。每个场景包括一个描述、两个可能的行动以及指示违反规则的辅助标签（例如，“不要杀人”）。我们将这个调查应用于28个开源和闭源的LLMs。我们发现(b) 在明确的情况下，LLMs tend to align with human moral intuitions, but in ambiguous scenarios, their responses vary and may exhibit biases and inconsistencies.

    This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM "making a choice", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., "Should I tell a white lie?") and 687 low-ambiguity moral scenarios (e.g., "Should I stop for a pedestrian on the road?"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., "do not kill"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scen
    
[^4]: 通过引导安全探索的强化学习

    Reinforcement Learning by Guided Safe Exploration. (arXiv:2307.14316v1 [cs.LG])

    [http://arxiv.org/abs/2307.14316](http://arxiv.org/abs/2307.14316)

    本文研究了一种通过引导安全探索的强化学习方法，通过训练一个代理在没有奖励信号的情况下学习安全探索，然后利用其构建一个安全的行为策略，并且借鉴迁移学习的思想，在训练过程中逐渐消除引导代理的影响。

    

    安全性对于扩大强化学习(RL)的应用至关重要。我们经常在受控环境中训练RL智能体，如实验室，然后再在真实世界中部署它们。然而，在部署之前，真实世界的目标任务可能是未知的。无奖励RL在没有奖励的情况下训练智能体，以便在奖励揭示后能够快速适应。我们考虑了有约束的无奖励设置，其中一个代理（导引者）在没有奖励信号的情况下学习安全探索。该代理在受控环境中接受训练，可以进行不安全的交互但仍提供安全信号。当目标任务被揭示后，不允许违反安全规则。因此，导引者被利用来构建一个安全的行为策略。受到迁移学习的启发，我们还将目标策略（学生）向导引者进行正则化，同时学生在可靠性上是不可靠的，并逐渐消除导引者在训练过程中的影响。实证分析显示...

    Safety is critical to broadening the application of reinforcement learning (RL). Often, we train RL agents in a controlled environment, such as a laboratory, before deploying them in the real world. However, the real-world target task might be unknown prior to deployment. Reward-free RL trains an agent without the reward to adapt quickly once the reward is revealed. We consider the constrained reward-free setting, where an agent (the guide) learns to explore safely without the reward signal. This agent is trained in a controlled environment, which allows unsafe interactions and still provides the safety signal. After the target task is revealed, safety violations are not allowed anymore. Thus, the guide is leveraged to compose a safe behaviour policy. Drawing from transfer learning, we also regularize a target policy (the student) towards the guide while the student is unreliable and gradually eliminate the influence of the guide as training progresses. The empirical analysis shows tha
    
[^5]: 一种用于最优能量存储系统调度的约束执行深度强化学习框架

    A Constraint Enforcement Deep Reinforcement Learning Framework for Optimal Energy Storage Systems Dispatch. (arXiv:2307.14304v1 [eess.SY])

    [http://arxiv.org/abs/2307.14304](http://arxiv.org/abs/2307.14304)

    提出了一个约束执行深度强化学习框架，能够有效地处理能量存储系统调度中的操作约束，通过训练深度神经网络来学习高质量的控制模型，并将其转化为混合整数规划问题进行约束执行。

    

    由于动态价格波动、需求消耗和基于可再生能源的能量生成引入的不确定性，能量存储系统(ESSs)的最优调度面临巨大挑战。通过利用深度神经网络(DNNs)的泛化能力，深度强化学习(DRL)算法可以学习适应配电网络随机性的高质量控制模型。然而，当前的DRL算法缺乏严格执行操作约束的能力，甚至常常提供不可行的控制动作。为了解决这个问题，我们提出了一个DRL框架，在在线操作期间有效处理连续动作空间，同时严格执行环境和动作空间的操作约束。首先，所提出的框架训练一个由DNNs模拟的动作价值函数。随后，该动作价值函数被制定为一个混合整数规划(MIP)问题，使其能够进行操作约束的严格执行。

    The optimal dispatch of energy storage systems (ESSs) presents formidable challenges due to the uncertainty introduced by fluctuations in dynamic prices, demand consumption, and renewable-based energy generation. By exploiting the generalization capabilities of deep neural networks (DNNs), deep reinforcement learning (DRL) algorithms can learn good-quality control models that adaptively respond to distribution networks' stochastic nature. However, current DRL algorithms lack the capabilities to enforce operational constraints strictly, often even providing unfeasible control actions. To address this issue, we propose a DRL framework that effectively handles continuous action spaces while strictly enforcing the environments and action space operational constraints during online operation. Firstly, the proposed framework trains an action-value function modeled using DNNs. Subsequently, this action-value function is formulated as a mixed-integer programming (MIP) formulation enabling the 
    
[^6]: ChatGPT和说服技术在酒店服务领域个性化推荐管理和提供中的应用

    ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality. (arXiv:2307.14298v1 [cs.IR])

    [http://arxiv.org/abs/2307.14298](http://arxiv.org/abs/2307.14298)

    本文研究了将ChatGPT和说服技术应用于酒店推荐系统的潜力，通过ChatGPT可以提供更准确和上下文感知的推荐，而说服技术可影响用户行为并增强推荐的说服力。

    

    推荐系统在酒店服务业已成为不可或缺的工具，为客人提供个性化和定制化的体验。近年来，大型语言模型（LLM），如ChatGPT和说服技术的进步，为提升这些系统的效果打开了新的途径。本文探讨了将ChatGPT和说服技术整合到酒店服务推荐系统中自动化和改进的潜力。首先，我们深入研究了ChatGPT的能力，它可以理解和生成类似人类的文本，从而实现更准确和上下文感知的推荐。我们讨论了将ChatGPT整合到推荐系统中的能力，突出了其分析用户偏好、从在线评论中提取有价值的洞见，并根据客人配置生成个性化推荐的能力。其次，我们研究了说服技术在影响用户行为和提升酒店推荐的说服效果方面的作用。

    Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recomm
    
[^7]: 揭示拆分序列数据的复杂性：解决视频和时间序列分析中的挑战

    Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis. (arXiv:2307.14294v1 [cs.LG])

    [http://arxiv.org/abs/2307.14294](http://arxiv.org/abs/2307.14294)

    本概念文章研究了各种拆分序列数据的挑战，并通过电机测试台和液体中的粒子跟踪等实例进行了探索。

    

    序列数据的拆分，如视频和时间序列，是各种数据分析任务的重要步骤，包括目标跟踪和异常检测。然而，拆分序列数据面临各种挑战，可能会影响后续分析的准确性和可靠性。本概念文章研究了与拆分序列数据相关的挑战，包括数据获取、数据表示、拆分比例选择、建立质量标准和选择合适的选择策略。我们通过两个真实世界的例子进行了探索：电机测试台和液体中的粒子跟踪。

    Splitting of sequential data, such as videos and time series, is an essential step in various data analysis tasks, including object tracking and anomaly detection. However, splitting sequential data presents a variety of challenges that can impact the accuracy and reliability of subsequent analyses. This concept article examines the challenges associated with splitting sequential data, including data acquisition, data representation, split ratio selection, setting up quality criteria, and choosing suitable selection strategies. We explore these challenges through two real-world examples: motor test benches and particle tracking in liquids.
    
[^8]: 通用目的人工智能系统（GPAIS）：性质、定义、分类、开放挑战和影响

    General Purpose Artificial Intelligence Systems (GPAIS): Properties, Definition, Taxonomy, Open Challenges and Implications. (arXiv:2307.14283v1 [cs.AI])

    [http://arxiv.org/abs/2307.14283](http://arxiv.org/abs/2307.14283)

    这里是中文总结出的一句话要点：本论文讨论了通用目的人工智能系统（GPAIS）的性质、定义、分类和开放挑战，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。

    

    大部分人工智能（AI）应用都设计用于特定和有限的任务。然而，有许多场景需要更通用的AI，能够解决各种任务而不需要专门为它们设计。通用目的人工智能系统（GPAIS）这个术语被定义为指代这些AI系统。尽管迄今为止，实现人工通用智能的可能性，即足够强大以模拟人类并改进各种智力任务，一直是一个愿望、虚构的概念，并被认为对我们社会构成风险。虽然我们离实现这一目标可能还很遥远，但GPAIS是现实存在并位居人工智能研究的前沿。本文讨论了现有GPAIS定义，并提出了一种新的定义，允许根据其性质和限制逐步区分GPAIS的类型。我们区分了封闭世界和开放世界的GPAIS，描述其自主程度和...

    Most applications of Artificial Intelligence (AI) are designed for a confined and specific task. However, there are many scenarios that call for a more general AI, capable of solving a wide array of tasks without being specifically designed for them. The term General-Purpose Artificial Intelligence Systems (GPAIS) has been defined to refer to these AI systems. To date, the possibility of an Artificial General Intelligence, powerful enough to perform any intellectual task as if it were human, or even improve it, has remained an aspiration, fiction, and considered a risk for our society. Whilst we might still be far from achieving that, GPAIS is a reality and sitting at the forefront of AI research.  This work discusses existing definitions for GPAIS and proposes a new definition that allows for a gradual differentiation among types of GPAIS according to their properties and limitations. We distinguish between closed-world and open-world GPAIS, characterising their degree of autonomy and
    
[^9]: 为提高脑肿瘤分割而进行的深度伪造图像生成研究

    Deepfake Image Generation for Improved Brain Tumor Segmentation. (arXiv:2307.14273v1 [eess.IV])

    [http://arxiv.org/abs/2307.14273](http://arxiv.org/abs/2307.14273)

    本研究使用深度伪造图像生成和卷积神经网络来进行脑肿瘤分割，取得了较好的性能表现。

    

    随着世界在技术和健康方面的进步，通过揭示无症状的迹象来提高对疾病的认识变得越来越重要。及早检测和治疗肿瘤非常重要，因为它可能危及生命。计算机辅助技术被用来克服疾病诊断中存在的局限性，而脑肿瘤分割仍然是一个困难的过程，尤其是在涉及多模态数据时。这主要是由于数据和对应标注的缺乏导致的训练效果不佳。本研究探讨了使用深度伪造图像生成进行有效脑肿瘤分割的可行性。为此，使用生成对抗网络进行图像到图像的转换以增加数据集大小，然后使用基于U-Net的卷积神经网络对深度伪造图像进行图像分割训练。将所提出的方法的性能与四个公开数据集的基本事实进行比较。结果表明，在性能方面取得了提高。

    As the world progresses in technology and health, awareness of disease by revealing asymptomatic signs improves. It is important to detect and treat tumors in early stage as it can be life-threatening. Computer-aided technologies are used to overcome lingering limitations facing disease diagnosis, while brain tumor segmentation remains a difficult process, especially when multi-modality data is involved. This is mainly attributed to ineffective training due to lack of data and corresponding labelling. This work investigates the feasibility of employing deep-fake image generation for effective brain tumor segmentation. To this end, a Generative Adversarial Network was used for image-to-image translation for increasing dataset size, followed by image segmentation using a U-Net-based convolutional neural network trained with deepfake images. Performance of the proposed approach is compared with ground truth of four publicly available datasets. Results show improved performance in terms of
    
[^10]: 荧光神经细胞v2：深度学习显微镜中的多任务、多格式注释

    Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy. (arXiv:2307.14243v1 [cs.CV])

    [http://arxiv.org/abs/2307.14243](http://arxiv.org/abs/2307.14243)

    荧光神经细胞v2是一组荧光显微镜图像及其真实注释，提供了多个学习任务的注释，并有助于计算机视觉方法的进展。

    

    荧光神经细胞v2是一组荧光显微镜图像及其相应的真实注释，旨在促进生命科学和深度学习领域的创新研究。该数据集涵盖了三个图像收集，其中啮齿类神经细胞的细胞核和细胞质使用不同的标记染色以突出其解剖或功能特征。除图像外，我们还提供了多个学习任务的真实注释，包括语义分割、目标检测和计数。该研究成果有两个方面的贡献。首先，鉴于注释的多样性和其可访问的格式，我们希望我们的工作有助于计算机视觉方法在分割、检测、特征学习、无监督和自监督学习、迁移学习和相关领域的方法论进展。其次，通过促进广泛探索和评估，我们希望荧光神经细胞v2能够催化研究领域的发展。

    Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy images and the corresponding ground-truth annotations, designed to foster innovative research in the domains of Life Sciences and Deep Learning. This dataset encompasses three image collections in which rodent neuronal cells' nuclei and cytoplasm are stained with diverse markers to highlight their anatomical or functional characteristics. Alongside the images, we provide ground-truth annotations for several learning tasks, including semantic segmentation, object detection, and counting. The contribution is two-fold. First, given the variety of annotations and their accessible formats, we envision our work facilitating methodological advancements in computer vision approaches for segmentation, detection, feature learning, unsupervised and self-supervised learning, transfer learning, and related areas. Second, by enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal Cells v2 will catalyze bre
    
[^11]: 为机器人群体演化多目标神经网络控制器

    Evolving Multi-Objective Neural Network Controllers for Robot Swarms. (arXiv:2307.14237v1 [cs.RO])

    [http://arxiv.org/abs/2307.14237](http://arxiv.org/abs/2307.14237)

    本研究提出了一种基于多目标演化神经网络的方法来开发机器人群体的控制器。实验结果表明，该方法可以有效地控制每个机器人，并且可在具有更多机器人的环境中进行扩展，无需进一步重新训练。

    

    许多群体机器人任务由多个相互冲突的目标组成。本研究提出了一种基于多目标演化神经网络的方法来开发机器人群体的控制器。群体机器人控制器在低保真的Python模拟器中进行训练，然后在高保真的Webots模拟环境中进行测试。通过进行模拟实验，测试了演化出的多目标机器人控制器在具有更多机器人的环境中的可扩展性。实验结果表明，该方法可以有效地控制每个机器人。机器人群体在调整每个目标 的权重时表现出不同的行为。结果还证实，在低保真模拟器中演化出的多目标神经网络控制器可以传输到高保真模拟环境，并且控制器能够在具有更多机器人的环境中进行扩展，无需进一步重新训练。

    Many swarm robotics tasks consist of multiple conflicting objectives. This research proposes a multi-objective evolutionary neural network approach to developing controllers for swarms of robots. The swarm robot controllers are trained in a low-fidelity Python simulator and then tested in a high-fidelity simulated environment using Webots. Simulations are then conducted to test the scalability of the evolved multi-objective robot controllers to environments with a larger number of robots. The results presented demonstrate that the proposed approach can effectively control each of the robots. The robot swarm exhibits different behaviours as the weighting for each objective is adjusted. The results also confirm that multi-objective neural network controllers evolved in a low-fidelity simulator can be transferred to high-fidelity simulated environments and that the controllers can scale to environments with a larger number of robots without further retraining needed.
    
[^12]: 大规模语言模型在冷启动推荐系统中与基于语言和基于项目偏好竞争力相当

    Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences. (arXiv:2307.14225v1 [cs.IR])

    [http://arxiv.org/abs/2307.14225](http://arxiv.org/abs/2307.14225)

    大规模语言模型（LLMs）在冷启动情况下提供了与基于项目协同过滤（CF）方法相当的推荐性能，特别是在纯基于语言偏好的情况下。

    

    传统的推荐系统利用用户的项目偏好历史来推荐用户可能喜欢的新内容。然而，现代对话界面允许用户表达基于语言的偏好，提供了一种根本不同的偏好输入方式。受最近大规模语言模型（LLMs）提示范式的成功启发，我们研究了它们在基于项目和基于语言偏好方面与最先进的基于项目协同过滤（CF）方法相比的推荐应用。为了支持这项研究，我们收集了一个新的数据集，其中包含从用户那里引发出来的基于项目和基于语言偏好，以及他们对各种（有偏见的）推荐项目和（无偏见的）随机项目的评分。在众多实验结果中，我们发现在纯基于语言偏好（没有项目偏好）的情况下，LLMs在接近冷启动情况下与基于项目的CF方法相比具有竞争力的推荐性能。

    Traditional recommender systems leverage users' item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods,
    
[^13]: 在资源约束条件下在线建模和监控相关进程

    Online Modeling and Monitoring of Dependent Processes under Resource Constraints. (arXiv:2307.14208v1 [cs.LG])

    [http://arxiv.org/abs/2307.14208](http://arxiv.org/abs/2307.14208)

    本文提出了一种在线协作学习方法，能够适应性地分配资源，实现对相关进程的监控和动态探索，有效地进行异常事件检测。

    

    在有限资源下监控相关进程的群体对于异常事件检测至关重要。本文提出了一种新颖的在线协作学习方法，通过适应性地分配资源，实现高风险进程的开发利用和相关动态的探索。通过理论分析和实验证明了该方法的效率。

    Monitoring a population of dependent processes under limited resources is critical for abnormal events detection. A novel online collaborative learning method is proposed to adaptively allocate the resources for exploitation of high-risk processes and exploration of dependent dynamics. Efficiency of the proposed method is proved through theoretical analysis and experiments.
    
[^14]: 采用随机森林和支持向量机对压力过滤性能进行调查的应用，一种锌厂滤饼建模研究。

    Application of Random Forest and Support Vector Machine for Investigation of Pressure Filtration Performance, a Zinc Plant Filter Cake Modeling. (arXiv:2307.14199v1 [cs.LG])

    [http://arxiv.org/abs/2307.14199](http://arxiv.org/abs/2307.14199)

    该研究应用随机森林和支持向量机模型对锌厂压力过滤性能进行建模，以预测滤饼湿度和提高锌的回收率。

    

    锌冶金生产方法涉及从矿石中浸出锌，然后通过压力过滤将固体残渣与液体溶液分离。这个分离过程非常重要，因为固体残渣中含有一定的湿度，这会降低锌的回收量。本研究通过随机森林（RF）和支持向量机（SVM）对压力过滤过程进行了建模。模型以实验室样品的连续变量（提取特征）作为输入。因此，选择了回归模型，即随机森林回归（RFR）和支持向量回归（SVR）。在两个条件下（聚丙烯和涤纶织物），得到了压力过滤过程的完整数据集。用于预测滤饼湿度的因素包括固体浓度、温度、pH值、压力、滤饼厚度、吹气时间和过滤时间。

    The hydrometallurgical method of zinc production involves leaching zinc from ore and then separating the solid residue from the liquid solution by pressure filtration. This separation process is very important since the solid residue contains some moisture that can reduce the amount of zinc recovered. This study modeled the pressure filtration process through Random Forest (RF) and Support Vector Machine (SVM). The models take continuous variables (extracted features) from the lab samples as inputs. Thus, regression models namely Random Forest Regression (RFR) and Support Vector Regression (SVR) were chosen. A total dataset was obtained during the pressure filtration process in two conditions: 1) Polypropylene (S1) and 2) Polyester fabrics (S2). To predict the cake moisture, solids concentration (0.2 and 0.38), temperature (35 and 65 centigrade), pH (2, 3.5, and 5), pressure, cake thickness (14, 20, 26, and 34 mm), air-blow time (2, 10 and 15 min) and filtration time were applied as in
    
[^15]: 高效学习离散-连续计算图的方法

    Efficient Learning of Discrete-Continuous Computation Graphs. (arXiv:2307.14193v1 [cs.LG])

    [http://arxiv.org/abs/2307.14193](http://arxiv.org/abs/2307.14193)

    本文研究了具有多个离散组件的随机计算图的行为，并提出了两种优化策略:增加Gumbel噪声扰动的尺度参数和使用多个离散组件。这些策略有效地提高了模型的学习能力。

    

    监督学习和强化学习中，离散和连续模型组合应用广泛。端到端可学习的离散-连续模型具有组合性、更好的泛化性能和更可解释。现有工作主要关注计算图中每个执行路径上只有一个离散组件的情况。我们分析具有多个连续离散组件的复杂随机计算图的行为。我们发现，由于梯度较小和局部最小值问题，优化这些模型的参数非常具有挑战性。然后，我们提出了两种克服这些挑战的新策略。首先，我们发现在训练过程中增加Gumbel噪声扰动的尺度参数可以改善学习行为。

    Numerous models for supervised and reinforcement learning benefit from combinations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous computation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on computation graphs with a single discrete component on each of the graph's execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations during training improves the learning behavior. Seco
    
[^16]: 《弗吉尼亚诺福克市街道洪水的机器学习替代模型比较》

    A comparison of machine learning surrogate models of street-scale flooding in Norfolk, Virginia. (arXiv:2307.14185v1 [cs.LG])

    [http://arxiv.org/abs/2307.14185](http://arxiv.org/abs/2307.14185)

    本研究比较了基于随机森林算法的先前替代模型与两个深度学习模型（LSTM和GRU）在弗吉尼亚诺福克市街道洪水的性能。结果表明，选择支持预测不确定性传达和有效集成多模态特征的模型架构十分重要。

    

    低洼的沿海城市，如弗吉尼亚诺福克市，面临着由降雨和潮汐引起的街道洪水的挑战，这给交通和下水道系统造成了压力，并可能导致财产损失。虽然高保真的基于物理的模拟能够准确预测城市暴雨洪水，但它们的计算复杂性使它们不适用于实时应用。本研究利用2016年至2018年间弗吉尼亚诺福克市暴雨事件的数据，比较了基于随机森林算法的先前替代模型与两个深度学习模型（长短时记忆LSTM和门式循环单元GRU）的性能。这项研究强调了使用支持预测不确定性传达和有效集成相关多模态特征的模型架构的重要性。

    Low-lying coastal cities, exemplified by Norfolk, Virginia, face the challenge of street flooding caused by rainfall and tides, which strain transportation and sewer systems and can lead to property damage. While high-fidelity, physics-based simulations provide accurate predictions of urban pluvial flooding, their computational complexity renders them unsuitable for real-time applications. Using data from Norfolk rainfall events between 2016 and 2018, this study compares the performance of a previous surrogate model based on a random forest algorithm with two deep learning models: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). This investigation underscores the importance of using a model architecture that supports the communication of prediction uncertainty and the effective integration of relevant, multi-modal features.
    
[^17]: 学习解离的离散表示

    Learning Disentangled Discrete Representations. (arXiv:2307.14151v1 [cs.LG])

    [http://arxiv.org/abs/2307.14151](http://arxiv.org/abs/2307.14151)

    通过替换标准的高斯变分自动编码器，使用定制的分类变分自动编码器，我们发现离散潜在空间的底层网格结构可以有效缓解解离表示中的旋转不变性问题，并提供了优化解离表示的无监督模型选择策略。

    

    最近在图像生成、基于模型的增强学习和文本到图像生成方面取得了成功，这些都证明了离散潜在表示的经验优势，尽管其背后的原因尚不清楚。我们通过将标准的高斯变分自动编码器（VAE）替换为定制的分类变分自动编码器，探索了离散潜在空间和解离表示之间的关系。我们显示分类分布的底层网格结构减轻了与多变量高斯分布相关的旋转不变性问题，作为解离表示的高效归纳先验。我们提供了分析和实证结果，证明了离散VAE在学习解离表示方面的优势。此外，我们引入了第一个支持解离表示的无监督模型选择策略。

    Recent successes in image generation, model-based reinforcement learning, and text-to-image generation have demonstrated the empirical advantages of discrete latent representations, although the reasons behind their benefits remain unclear. We explore the relationship between discrete latent spaces and disentangled representations by replacing the standard Gaussian variational autoencoder (VAE) with a tailored categorical variational autoencoder. We show that the underlying grid structure of categorical distributions mitigates the problem of rotational invariance associated with multivariate Gaussian distributions, acting as an efficient inductive prior for disentangled representations. We provide both analytical and empirical findings that demonstrate the advantages of discrete VAEs for learning disentangled representations. Furthermore, we introduce the first unsupervised model selection strategy that favors disentangled representations.
    
[^18]: 通过凡人设计合成主动推理代理

    Toward Design of Synthetic Active Inference Agents by Mere Mortals. (arXiv:2307.14145v1 [stat.ML])

    [http://arxiv.org/abs/2307.14145](http://arxiv.org/abs/2307.14145)

    本文讨论了如何设计一个能够支持非专家工程师开发主动推理代理的软件工具箱，以实现在边缘设备上运行的有效代理。旨在加速主动推理代理的民主化进程。

    

    主动推理代理的理论特性是令人印象深刻的，但是我们如何在边缘设备上实现有效的硬件和软件代理呢？这是一个有趣的问题，因为策略探索的计算负荷呈指数级增长，而边缘设备的计算资源非常有限。在本文中，我们讨论了一个支持非专家工程师开发有效主动推理代理的软件工具箱所必需的特性。我们介绍了一个正在开发中的工具箱，旨在加速主动推理代理的民主化进程，就像TensorFlow推动了深度学习技术的应用一样。

    The theoretical properties of active inference agents are impressive, but how do we realize effective agents in working hardware and software on edge devices? This is an interesting problem because the computational load for policy exploration explodes exponentially, while the computational resources are very limited for edge devices. In this paper, we discuss the necessary features for a software toolbox that supports a competent non-expert engineer to develop working active inference agents. We introduce a toolbox-in-progress that aims to accelerate the democratization of active inference agents in a similar way as TensorFlow propelled applications of deep learning technology.
    
[^19]: 分段稳定组合半强盗问题及因果关系奖励研究

    Piecewise-Stationary Combinatorial Semi-Bandit with Causally Related Rewards. (arXiv:2307.14138v1 [cs.LG])

    [http://arxiv.org/abs/2307.14138](http://arxiv.org/abs/2307.14138)

    本研究研究了具有因果关系奖励的分段稳定组合半强盗问题，并提出了上界置信度算法以应对非平稳环境中的挑战。此外，引入了组重启的概念作为结构化环境中的备份策略。

    

    本文研究了具有因果关系奖励的分段稳定组合半强盗问题。在非平稳环境中，基本臂的分布变化、奖励之间的因果关系，或者两者同时改变奖励生成过程。在这样的环境中，最优的决策者必须跟随这两个变化源，并相应地进行适应。在组合半强盗设置中，问题变得更加严重，因为决策者只观察到所选臂组合的结果。我们提出的策略核心是上界置信度（Upper Confidence Bound, UCB）算法。我们假设代理依靠自适应的方法来应对这一挑战。具体来说，它使用基于广义似然比检验的变点检测器。此外，我们引入了组重启的概念作为结构化环境中决策过程中的新型备份策略。最后，我们的算法整合了一个跟踪机制以追踪

    We study the piecewise stationary combinatorial semi-bandit problem with causally related rewards. In our nonstationary environment, variations in the base arms' distributions, causal relationships between rewards, or both, change the reward generation process. In such an environment, an optimal decision-maker must follow both sources of change and adapt accordingly. The problem becomes aggravated in the combinatorial semi-bandit setting, where the decision-maker only observes the outcome of the selected bundle of arms. The core of our proposed policy is the Upper Confidence Bound (UCB) algorithm. We assume the agent relies on an adaptive approach to overcome the challenge. More specifically, it employs a change-point detector based on the Generalized Likelihood Ratio (GLR) test. Besides, we introduce the notion of group restart as a new alternative restarting strategy in the decision making process in structured environments. Finally, our algorithm integrates a mechanism to trace the 
    
[^20]: 开发和评估小型到中型的土耳其BERT模型

    Developing and Evaluating Tiny to Medium-Sized Turkish BERT Models. (arXiv:2307.14134v1 [cs.CL])

    [http://arxiv.org/abs/2307.14134](http://arxiv.org/abs/2307.14134)

    本研究开发并评估了小型到中型的土耳其BERT模型，在填补资源匮乏语言领域的研究空白方面取得了积极的成果，并为开发和应用小型语言模型提供了有价值的见解。

    

    本研究引入并评估了小型、迷你型、小型和中型的无大小写的土耳其BERT模型，旨在填补资源匮乏语言领域的研究空白。我们使用多个来源的75GB以上文本数据集对这些模型进行了训练，并在多个任务上进行了测试，包括掩码预测、情感分析、新闻分类和零样本分类。尽管规模较小，我们的模型表现出了强大的性能，包括零样本任务，同时保证了计算效率和更快的执行时间。我们的研究结果为小型语言模型的开发和应用提供了有价值的见解，特别是在土耳其语境下。

    This study introduces and evaluates tiny, mini, small, and medium-sized uncased Turkish BERT models, aiming to bridge the research gap in less-resourced languages. We trained these models on a diverse dataset encompassing over 75GB of text from multiple sources and tested them on several tasks, including mask prediction, sentiment analysis, news classification, and, zero-shot classification. Despite their smaller size, our models exhibited robust performance, including zero-shot task, while ensuring computational efficiency and faster execution times. Our findings provide valuable insights into the development and application of smaller language models, especially in the context of the Turkish language.
    
[^21]: GraphRNN再探：消融研究和有向无环图的扩展

    GraphRNN Revisited: An Ablation Study and Extensions for Directed Acyclic Graphs. (arXiv:2307.14109v1 [cs.LG])

    [http://arxiv.org/abs/2307.14109](http://arxiv.org/abs/2307.14109)

    本文对GraphRNN进行了复现和评估，发现You等人建议的BFS遍历对模型性能有重要贡献。此外，通过将BFS遍历替换为拓扑排序，我们扩展了GraphRNN以生成有向无环图，并在真实数据集上取得了显著改进。

    

    GraphRNN是由You等人提出的基于深度学习的架构，用于学习图的生成模型。我们使用重新实现的GraphRNN架构复现了You等人的结果，并使用新的指标将其与基准模型进行了评估。通过消融研究，我们发现You等人建议的BFS遍历以合并同构图的表示对模型性能有显著贡献。此外，我们通过替换BFS遍历为拓扑排序，将GraphRNN扩展为生成有向无环图。我们证明这种方法在真实数据集上明显优于GraphRNN的有向多类别变体。

    GraphRNN is a deep learning-based architecture proposed by You et al. for learning generative models for graphs. We replicate the results of You et al. using a reproduced implementation of the GraphRNN architecture and evaluate this against baseline models using new metrics. Through an ablation study, we find that the BFS traversal suggested by You et al. to collapse representations of isomorphic graphs contributes significantly to model performance. Additionally, we extend GraphRNN to generate directed acyclic graphs by replacing the BFS traversal with a topological sort. We demonstrate that this method improves significantly over a directed-multiclass variant of GraphRNN on a real-world dataset.
    
[^22]: 行动胜于言辞：证明了从策略反馈中省样本量的量化斯坦克伯格均衡的强化学习

    Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks. (arXiv:2307.14085v1 [cs.LG])

    [http://arxiv.org/abs/2307.14085](http://arxiv.org/abs/2307.14085)

    本文研究了强化学习中的量化斯坦克伯格均衡问题，提出了省样本量的在线和离线算法，并通过推断追随者的行动来学习量化响应模型。

    

    本文研究具有领导者-追随者结构的情境马尔科夫博弈中学习量化斯坦克伯格均衡（QSE）的强化学习（RL）。在游戏开始时，领导者宣布她的策略并承诺执行。追随者观察领导者的策略，然后采取量化响应策略，通过解决由领导者策略引发的熵正则化策略优化问题来确定。领导者的目标是通过与追随者的交互并从数据中学习，找到自己的最优策略，从而获得最优的预期总回报。这个问题的一个关键挑战是领导者无法观察到追随者的奖励，并且需要从追随者对抗领导者策略的行动中推断出追随者的量化响应模型。我们在函数逼近的背景下提出了适用于在线和离线设置的样本效率算法。我们的算法基于（i）通过最大似然学习量化响应模型

    We study reinforcement learning (RL) for learning a Quantal Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower structure. In specific, at the outset of the game, the leader announces her policy to the follower and commits to it. The follower observes the leader's policy and, in turn, adopts a quantal response policy by solving an entropy-regularized policy optimization problem induced by leader's policy. The goal of the leader is to find her optimal policy, which yields the optimal expected total return, by interacting with the follower and learning from data. A key challenge of this problem is that the leader cannot observe the follower's reward, and needs to infer the follower's quantal response model from his actions against leader's policies. We propose sample-efficient algorithms for both the online and offline settings, in the context of function approximation. Our algorithms are based on (i) learning the quantal response model via maximum likelihood 
    
[^23]: 动态域差异调整用于主动多领域适应的研究

    Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation. (arXiv:2307.14068v1 [cs.LG])

    [http://arxiv.org/abs/2307.14068](http://arxiv.org/abs/2307.14068)

    该论文提出了一种名为D3AAMDA的新方法，通过动态调整特征分布的对齐程度，有效地利用源域内的局部有利特征信息，以解决多源无监督领域适应中的性能差距和冗余特征的问题。

    

    多源无监督领域适应（MUDA）旨在将相关源域的知识转移至无标签目标域。虽然最近的MUDA方法显示出了有希望的结果，但大部分方法都专注于对齐源域之间的整体特征分布，这可能会导致由于每个域内冗余特征而产生负面影响。此外，MUDA方法与监督方法之间存在明显的性能差距。为了解决这些挑战，我们提出了一种名为动态域差异调整用于主动多领域适应（D3AAMDA）的新方法。首先，我们在训练过程中建立了一个多源动态调制机制，该机制基于源域与目标域之间的分布差异程度。该机制控制每个源域与目标域之间特征的对齐程度，有效地利用源域内的局部有利特征信息。此外，我们还提出了一种多源的...

    Multi-source unsupervised domain adaptation (MUDA) aims to transfer knowledge from related source domains to an unlabeled target domain. While recent MUDA methods have shown promising results, most focus on aligning the overall feature distributions across source domains, which can lead to negative effects due to redundant features within each domain. Moreover, there is a significant performance gap between MUDA and supervised methods. To address these challenges, we propose a novel approach called Dynamic Domain Discrepancy Adjustment for Active Multi-Domain Adaptation (D3AAMDA). Firstly, we establish a multi-source dynamic modulation mechanism during the training process based on the degree of distribution differences between source and target domains. This mechanism controls the alignment level of features between each source domain and the target domain, effectively leveraging the local advantageous feature information within the source domains. Additionally, we propose a Multi-sou
    
[^24]: 机器学习在医疗领域的应用：现状和未来方向

    Machine Learning Applications In Healthcare: The State Of Knowledge and Future Directions. (arXiv:2307.14067v1 [cs.LG])

    [http://arxiv.org/abs/2307.14067](http://arxiv.org/abs/2307.14067)

    机器学习在医疗领域有着巨大的应用潜力，但由于分散的信息和缺乏易于理解的文档，使得其应用受到了限制。

    

    快速处理能力和对易被忽视的隐藏模式的检测使得机器学习（ML）对今天的医疗系统至关重要。尽管已经发现了许多ML应用程序，还有许多应用程序仍在研究中，但目前的医疗系统只有少数应用了这些技术。因此，在医疗系统中存在着巨大的机器学习机会，但分散的信息、缺乏适当安排和易于解释的相关文件是使机器学习应用难以使用的主要障碍。本研究旨在简洁、更有效地收集不同领域医疗保健中的机器学习应用，以便立即访问相关参考资料。我们将研究分为五大组：社区级工作、风险管理/预防保健、医疗运营管理、远程护理和早期检测。将这些组分为子组，我们提供了相关的参考文献。

    Detection of easily missed hidden patterns with fast processing power makes machine learning (ML) indispensable to today's healthcare system. Though many ML applications have already been discovered and many are still under investigation, only a few have been adopted by current healthcare systems. As a result, there exists an enormous opportunity in healthcare system for ML but distributed information, scarcity of properly arranged and easily explainable documentation in related sector are major impede which are making ML applications difficult to healthcare professionals. This study aimed to gather ML applications in different areas of healthcare concisely and more effectively so that necessary information can be accessed immediately with relevant references. We divided our study into five major groups: community level work, risk management/ preventive care, healthcare operation management, remote care, and early detection. Dividing these groups into subgroups, we provided relevant re
    
[^25]: 牙科放射学分割的扩散模型预训练

    Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v1 [cs.CV])

    [http://arxiv.org/abs/2307.14066](http://arxiv.org/abs/2307.14066)

    本文提出了一种利用扩散模型进行预训练的方法，用于牙科放射学分割，实验结果表明该方法在标签效率方面具有显著性能，竞争力强。

    

    医学放射学分割，尤其是牙科放射学分割，受到标记成本的严重限制，需要具有特定专业知识和繁重的注释。本文提出了一种简单的预训练方法，利用去噪扩散概率模型（DDPM）进行语义分割，该模型在生成建模方面显示出令人印象深刻的结果。我们的简单方法在标签效率方面取得了显著的性能，并且在预训练和后续任务之间不需要架构修改。我们提议首先通过利用DDPM训练目标对Unet进行预训练，然后在分割任务上对得到的模型进行微调。我们对牙科放射片的分割的实验结果表明，所提出的方法与最先进的预训练方法相竞争。

    Medical radiography segmentation, and specifically dental radiography, is highly limited by the cost of labeling which requires specific expertise and labor-intensive annotations. In this work, we propose a straightforward pre-training method for semantic segmentation leveraging Denoising Diffusion Probabilistic Models (DDPM), which have shown impressive results for generative modeling. Our straightforward approach achieves remarkable performance in terms of label efficiency and does not require architectural modifications between pre-training and downstream tasks. We propose to first pre-train a Unet by exploiting the DDPM training objective, and then fine-tune the resulting model on a segmentation task. Our experimental results on the segmentation of dental radiographs demonstrate that the proposed method is competitive with state-of-the-art pre-training methods.
    
[^26]: 基于拓扑正则化的多实例学习用于红细胞疾病分类

    Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification. (arXiv:2307.14025v1 [cs.LG])

    [http://arxiv.org/abs/2307.14025](http://arxiv.org/abs/2307.14025)

    本论文提出一种基于拓扑正则化的多实例学习方法，用于罕见贫血疾病的红细胞分类。通过从单个红细胞图像中提取多尺度的拓扑特征来进行模型正则化，以保持数据的特征拓扑属性。实验结果表明，该方法是有效的。

    

    使用显微图像诊断罕见的贫血疾病对于熟练的专家和机器学习方法来说都具有挑战性。由于在单个血样中有数千个与疾病相关的细胞，这构成了一个复杂的多实例学习（MIL）问题。虽然红细胞的空间邻域本身并不重要，但整个血样的拓扑结构，即数据的几何性质，包含了有益的特征，以解决典型的MIL问题，如梯度消失和在有限数据上训练时的过拟合。因此，我们开发了一种基于拓扑的方法，从单个红细胞图像的包中提取多尺度的拓扑特征。这些拓扑特征被用来对模型进行正则化，强制保持数据的特征拓扑属性。在包含71个罕见贫血疾病患者的数据集上，包括521张红细胞显微图像，我们的实验表明拓扑正则化是一个有效的方法。

    Diagnosing rare anemia disorders using microscopic images is challenging for skilled specialists and machine-learning methods alike. Due to thousands of disease-relevant cells in a single blood sample, this constitutes a complex multiple-instance learning (MIL) problem. While the spatial neighborhood of red blood cells is not meaningful per se, the topology, i.e., the geometry of blood samples as a whole, contains informative features to remedy typical MIL issues, such as vanishing gradients and overfitting when training on limited data. We thus develop a topology-based approach that extracts multi-scale topological features from bags of single red blood cell images. The topological features are used to regularize the model, enforcing the preservation of characteristic topological properties of the data. Applied to a dataset of 71 patients suffering from rare anemia disorders with 521 microscopic images of red blood cells, our experiments show that topological regularization is an effe
    
[^27]: 单层自注意力变换器使用低秩权重矩阵是否是通用逼近器？

    Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?. (arXiv:2307.14023v1 [cs.LG])

    [http://arxiv.org/abs/2307.14023](http://arxiv.org/abs/2307.14023)

    通过澄清softmax函数与Boltzmann算符之间的关系，我们证明了单层具有低秩权重矩阵的自注意力具备完全捕获整个输入序列上下文的能力，单层Transformer对于有限样本的记忆能力，单层自注意力Transformer是紧凑域上连续函数的通用逼近器。

    

    现有的关于Transformer模型表达能力的分析要求过深的层数来实现数据的记忆，导致与实际使用的Transformer存在差异。这主要是由于将softmax函数解释为hardmax函数的逼近。通过澄清softmax函数与Boltzmann算符之间的关系，我们证明了单层具有低秩权重矩阵的自注意力具备完全捕获整个输入序列上下文的能力。因此，我们展示了单层Transformer对于有限样本的记忆能力，并且由两个前馈神经网络构成的单层自注意力Transformer是紧凑域上连续函数的通用逼近器。

    Existing analyses of the expressive capacity of Transformer models have required excessively deep layers for data memorization, leading to a discrepancy with the Transformers actually used in practice. This is primarily due to the interpretation of the softmax function as an approximation of the hardmax function. By clarifying the connection between the softmax function and the Boltzmann operator, we prove that a single layer of self-attention with low-rank weight matrices possesses the capability to perfectly capture the context of an entire input sequence. As a consequence, we show that single-layer Transformer has a memorization capacity for finite samples, and that Transformers consisting of one self-attention layer with two feed-forward neural networks are universal approximators for continuous functions on a compact domain.
    
[^28]: MCMC-修正基于得分的扩散模型用于模型组合

    MCMC-Correction of Score-Based Diffusion Models for Model Composition. (arXiv:2307.14012v1 [stat.ML])

    [http://arxiv.org/abs/2307.14012](http://arxiv.org/abs/2307.14012)

    本文提出了一种修正基于得分的扩散模型的方法，使其能够与各种MCMC方法结合，从而实现模型组合和进行更好的采样。

    

    扩散模型可以用得分或能量函数来参数化。能量参数化具有更好的理论特性，主要是它可以通过在提议样本中总能量的变化基于Metropolis-Hastings修正步骤来进行扩展采样过程。然而，它似乎产生了稍微较差的性能，更重要的是，由于基于得分的扩散模型的普遍流行，现有的预训练能量参数化模型的可用性受到限制。这种限制削弱了模型组合的目的，即将预训练模型组合起来从新分布中进行采样。然而，我们的提议建议保留得分参数化，而是通过对得分函数进行线积分来计算基于能量的接受概率。这使我们能够重用现有的扩散模型，并将反向过程与各种马尔可夫链蒙特卡罗（MCMC）方法组合起来。

    Diffusion models can be parameterised in terms of either a score or an energy function. The energy parameterisation has better theoretical properties, mainly that it enables an extended sampling procedure with a Metropolis--Hastings correction step, based on the change in total energy in the proposed samples. However, it seems to yield slightly worse performance, and more importantly, due to the widespread popularity of score-based diffusion, there are limited availability of off-the-shelf pre-trained energy-based ones. This limitation undermines the purpose of model composition, which aims to combine pre-trained models to sample from new distributions. Our proposal, however, suggests retaining the score parameterization and instead computing the energy-based acceptance probability through line integration of the score function. This allows us to re-use existing diffusion models and still combine the reverse process with various Markov-Chain Monte Carlo (MCMC) methods. We evaluate our 
    
[^29]: 快速算法用于满足一个拟阵约束的k-submodular最大化问题

    Fast algorithms for k-submodular maximization subject to a matroid constraint. (arXiv:2307.13996v1 [cs.DS])

    [http://arxiv.org/abs/2307.13996](http://arxiv.org/abs/2307.13996)

    本文提出了一个快速算法，通过应用阈值递减算法来最大化满足拟阵约束的k-submodular函数。该算法在降低查询复杂度的同时，近似比例几乎没有损失。另外，作者还给出了对于单调和非单调情况的不同近似算法，并将总大小约束下的问题作为推论给出。

    

    在本文中，我们应用了一个阈值递减算法来最大化满足拟阵约束的k-submodular函数，该算法相较于贪婪算法减少了查询复杂度，同时在近似比例上几乎没有损失。我们给出了一个半近似算法$(\frac{1}{2} - \epsilon)$来最大化单调的k-submodular函数，以及一个非单调情况下的$(\frac{1}{3} - \epsilon)$近似算法，其复杂度为$O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$，其中$r$表示拟阵的秩，$IO, EO$分别表示判断一个子集是否为独立集和计算函数$f$值的oracle调用数。由于总大小的约束可以看作一种特殊的拟阵，称为均匀拟阵，因此我们将快速算法作为推论给出，用于满足总大小约束下的k-submodular函数最大化问题。

    In this paper, we apply a Threshold-Decreasing Algorithm to maximize $k$-submodular functions under a matroid constraint, which reduces the query complexity of the algorithm compared to the greedy algorithm with little loss in approximation ratio. We give a $(\frac{1}{2} - \epsilon)$-approximation algorithm for monotone $k$-submodular function maximization, and a $(\frac{1}{3} - \epsilon)$-approximation algorithm for non-monotone case, with complexity $O(\frac{n(k\cdot EO + IO)}{\epsilon} \log \frac{r}{\epsilon})$, where $r$ denotes the rank of the matroid, and $IO, EO$ denote the number of oracles to evaluate whether a subset is an independent set and to compute the function value of $f$, respectively. Since the constraint of total size can be looked as a special matroid, called uniform matroid, then we present the fast algorithm for maximizing $k$-submodular functions subject to a total size constraint as corollaries. corollaries.
    
[^30]: 随你选择: 在低维特征空间中实现有效的个性化联邦学习

    Take Your Pick: Enabling Effective Personalized Federated Learning within Low-dimensional Feature Space. (arXiv:2307.13995v1 [cs.LG])

    [http://arxiv.org/abs/2307.13995](http://arxiv.org/abs/2307.13995)

    个性化联邦学习（PFL）允许客户端在数据不同领域的应用场景中使用不同的模型。然而，由于数据分布的差异，全局编码器产生的通用特征包含许多对某个特定客户端任务无关的组件。本文提出利用特征空间的低维度特点，实现有效的个性化联邦学习。

    

    个性化联邦学习（PFL）是一种流行的框架，允许客户端在数据属于不同领域的应用场景中拥有不同的模型。PFL中一个典型客户端的模型包括一个由所有客户端训练的全局编码器，用于从原始数据中提取通用特征，以及使用客户端本地数据训练的个性化层（如分类器）。然而，由于不同客户端之间数据分布的差异（也称为领域差异），全局编码器产生的通用特征主要包括许多与某个特定客户端本地任务无关的组件。最近一些PFL方法通过个性化编码器中的特定参数来解决以上问题。然而，这些方法面临神经网络参数空间高维度和非线性所带来的很大挑战。相比之下，特征空间具有较低的维度，提供更直观的理解。

    Personalized federated learning (PFL) is a popular framework that allows clients to have different models to address application scenarios where clients' data are in different domains. The typical model of a client in PFL features a global encoder trained by all clients to extract universal features from the raw data and personalized layers (e.g., a classifier) trained using the client's local data. Nonetheless, due to the differences between the data distributions of different clients (aka, domain gaps), the universal features produced by the global encoder largely encompass numerous components irrelevant to a certain client's local task. Some recent PFL methods address the above problem by personalizing specific parameters within the encoder. However, these methods encounter substantial challenges attributed to the high dimensionality and non-linearity of neural network parameter space. In contrast, the feature space exhibits a lower dimensionality, providing greater intuitiveness an
    
[^31]: BovineTalk: 机器学习应用于乳牛负性情绪下的声音分析

    BovineTalk: Machine Learning for Vocalization Analysis of Dairy Cattle under Negative Affective States. (arXiv:2307.13994v1 [cs.SD])

    [http://arxiv.org/abs/2307.13994](http://arxiv.org/abs/2307.13994)

    这项研究通过使用机器学习技术，分析了乳牛在负性情绪下的声音特征，为开发非侵入性动物情绪状态指标提供了重要的参考。

    

    在畜牧动物中，开发和验证非侵入性基于动物的情绪状态指标是非常重要的，以便将其整合到农场评估协议中，可能通过精密畜牧业工具来实现。其中一种有前景的方法是使用声音指标。在重要的畜牧物种中，如猪、马、家禽和山羊中，声音的声学结构和功能已经得到了广泛的研究，然而在乳牛中的研究相对较少。牛被发现会发出两种类型的叫声：低频叫声(LF)，用于近距离接触时，嘴闭合或部分闭合；高频叫声(HF)，用于远距离传播时，嘴张开，后者往往与负性情绪状态有关。此外，研究发现乳牛叫声中包含了个体在各种情境下的个体特点，包括负性和

    There is a critical need to develop and validate non-invasive animal-based indicators of affective states in livestock species, in order to integrate them into on-farm assessment protocols, potentially via the use of precision livestock farming (PLF) tools. One such promising approach is the use of vocal indicators. The acoustic structure of vocalizations and their functions were extensively studied in important livestock species, such as pigs, horses, poultry and goats, yet cattle remain understudied in this context to date. Cows were shown to produce two types vocalizations: low-frequency calls (LF), produced with the mouth closed, or partially closed, for close distance contacts and open mouth emitted high-frequency calls (HF), produced for long distance communication, with the latter considered to be largely associated with negative affective states. Moreover, cattle vocalizations were shown to contain information on individuality across a wide range of contexts, both negative and 
    
[^32]: METAVerse：用于越野导航的元学习可行性成本图

    METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation. (arXiv:2307.13991v1 [cs.RO])

    [http://arxiv.org/abs/2307.13991](http://arxiv.org/abs/2307.13991)

    本文提出了METAVerse，一个用于在各种环境中准确可靠地预测地形可行性的元学习框架。通过自监督学习，利用稀疏的LiDAR点云生成密集连续值成本图，通过元学习训练全局模型，有效减小地形可行性估计的不确定性。

    

    越野环境中的自主导航需要准确估计地形可行性。然而，在非结构化环境中，地形可行性估计受到多个影响车辆与地形相互作用的因素的不确定性的影响。因此，获取一个能够准确预测各种环境中可行性的可推广模型是具有挑战性的。本文提出了METAVerse，这是一个用于在各种环境中准确可靠地预测地形可行性的元学习框架。我们通过自监督方式利用车辆与地形相互作用反馈来训练可行性预测网络，从稀疏的LiDAR点云生成密集连续值成本图。利用元学习从多个环境收集的驾驶数据训练全局模型，有效地减小估计不确定性。在部署过程中，进行在线适应。

    Autonomous navigation in off-road conditions requires an accurate estimation of terrain traversability. However, traversability estimation in unstructured environments is subject to high uncertainty due to the variability of numerous factors that influence vehicle-terrain interaction. Consequently, it is challenging to obtain a generalizable model that can accurately predict traversability in a variety of environments. This paper presents METAVerse, a meta-learning framework for learning a global model that accurately and reliably predicts terrain traversability across diverse environments. We train the traversability prediction network to generate a dense and continuous-valued cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain interaction feedback in a self-supervised manner. Meta-learning is utilized to train a global model with driving data collected from multiple environments, effectively minimizing estimation uncertainty. During deployment, online adaptation is p
    
[^33]: 这是不正确的！对语言生成系统进行否定感知的评估

    This is not correct! Negation-aware Evaluation of Language Generation Systems. (arXiv:2307.13989v1 [cs.CL])

    [http://arxiv.org/abs/2307.13989](http://arxiv.org/abs/2307.13989)

    本文提出了一种对语言生成系统进行否定感知的评估方法，并开发了NegBLEURT评估指标以提高对否定的敏感性。实验结果表明，该方法在否定句上的表现优于现有指标，并且在其他扰动上保持了性能。

    

    大型语言模型低估了否定对句子含义变化的影响。因此，基于这些模型的学习评估指标对否定不敏感。在本文中，我们提出了NegBLEURT，一种针对否定感知的BLEURT评估指标的改进版本。为此，我们设计了一种基于规则的句子否定工具，并用它来创建了CANNOT否定评估数据集。基于这个数据集，我们对句子转换器和评估指标进行了微调，以提高它们对否定的敏感性。在现有的基准测试中评估这些模型表明，我们微调后的模型在否定句上的表现远远优于现有的指标，同时保持了它们基础模型在其他扰动上的性能。

    Large language models underestimate the impact of negations on how much they change the meaning of a sentence. Therefore, learned evaluation metrics based on these models are insensitive to negations. In this paper, we propose NegBLEURT, a negation-aware version of the BLEURT evaluation metric. For that, we designed a rule-based sentence negation tool and used it to create the CANNOT negation evaluation dataset. Based on this dataset, we fine-tuned a sentence transformer and an evaluation metric to improve their negation sensitivity. Evaluating these models on existing benchmarks shows that our fine-tuned models outperform existing metrics on the negated sentences by far while preserving their base models' performances on other perturbations.
    
[^34]: 通过强化学习控制 GAN 的潜在空间：基于任务的图像翻译案例研究

    Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation. (arXiv:2307.13978v1 [cs.LG])

    [http://arxiv.org/abs/2307.13978](http://arxiv.org/abs/2307.13978)

    本文提出了一种通过将强化学习代理与潜在空间 GAN 集成来控制生成过程的新方法，在实验证明其有效性的基础上。

    

    生成对抗网络（GAN）已经成为一种强大的人工智能工具，可以根据训练数据生成逼真的输出。然而，控制 GAN 生成过程的挑战仍然存在。在本文中，我们提出了一种新颖的方法来解决这个问题，通过将强化学习（RL）代理与潜在空间 GAN（l-GAN）集成在一起，从而促进生成所需的输出。具体来说，我们开发了一个带有精心设计的奖励策略的 actor-critic RL 代理，使其能够熟练地在 l-GAN 的潜在空间中导航，并根据指定的任务生成输出。为了验证我们的方法的效果，我们进行了一系列实验，使用 MNIST 数据集，包括算术加法作为一个说明性任务。这些实验的结果验证了我们的方法的有效性。我们首次将 RL 代理与 GAN 模型集成起来，代表了一种创新

    Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to generate realistic outputs based on training datasets. However, the challenge of exerting control over the generation process of GANs remains a significant hurdle. In this paper, we propose a novel methodology to address this issue by integrating a reinforcement learning (RL) agent with a latent-space GAN (l-GAN), thereby facilitating the generation of desired outputs. More specifically, we have developed an actor-critic RL agent with a meticulously designed reward policy, enabling it to acquire proficiency in navigating the latent space of the l-GAN and generating outputs based on specified tasks. To substantiate the efficacy of our approach, we have conducted a series of experiments employing the MNIST dataset, including arithmetic addition as an illustrative task. The outcomes of these experiments serve to validate our methodology. Our pioneering integration of an RL agent with a GAN model represents a nov
    
[^35]: 通过隐藏层的线性可分性理解深度神经网络

    Understanding Deep Neural Networks via Linear Separability of Hidden Layers. (arXiv:2307.13962v1 [cs.LG])

    [http://arxiv.org/abs/2307.13962](http://arxiv.org/abs/2307.13962)

    本文通过测量深度神经网络隐藏层输出的线性可分性来研究其特性，并发现隐藏层输出的线性可分性程度与网络训练性能有同步性，进一步探讨激活函数和网络尺寸对隐藏层线性可分性的影响。

    

    本文通过测量隐藏层输出的线性可分性来研究深度神经网络的特性。具体来说，我们首先提出了基于闵可夫斯基差异的线性可分度量（MD-LSMs）来评估两个点集的线性可分程度。然后，我们证明了隐藏层输出的线性可分性程度与网络训练性能之间存在一种同步性，即如果更新的权重能够提高隐藏层输出的线性可分性程度，那么更新后的网络将实现更好的训练性能，反之亦然。此外，我们还研究了激活函数和网络尺寸（包括宽度和深度）对隐藏层线性可分性的影响。最后，我们进行了数值实验证实了我们的发现对一些流行的深度网络，包括多层感知机（MLP）、卷积神经网络（CNN）、深度置信网络（DBN）、ResNet、VGGNet、AlexNet、vision tran的验证。

    In this paper, we measure the linear separability of hidden layer outputs to study the characteristics of deep neural networks. In particular, we first propose Minkowski difference based linear separability measures (MD-LSMs) to evaluate the linear separability degree of two points sets. Then, we demonstrate that there is a synchronicity between the linear separability degree of hidden layer outputs and the network training performance, i.e., if the updated weights can enhance the linear separability degree of hidden layer outputs, the updated network will achieve a better training performance, and vice versa. Moreover, we study the effect of activation function and network size (including width and depth) on the linear separability of hidden layers. Finally, we conduct the numerical experiments to validate our findings on some popular deep networks including multilayer perceptron (MLP), convolutional neural network (CNN), deep belief network (DBN), ResNet, VGGNet, AlexNet, vision tran
    
[^36]: 图对比学习的熵神经估计

    Entropy Neural Estimation for Graph Contrastive Learning. (arXiv:2307.13944v1 [cs.LG])

    [http://arxiv.org/abs/2307.13944](http://arxiv.org/abs/2307.13944)

    本文提出了一种基于熵神经估计的图对比学习方法，通过最大化互信息下界来近似估计数据集的熵。通过简单但有效的子集抽样策略对比数据集不同视图中的节点表示，同时使用两个目标优化网络的学习过程。

    

    图对比学习旨在提取节点的可区分的高层表示。本文理论上证明了数据集的熵可以通过最大化不同视图下的互信息下界来近似估计，即通过神经网络估计熵。基于这一发现，我们提出了一种简单而有效的子集抽样策略，用于对比数据集各视图之间的成对表示。具体而言，我们随机从给定的图中抽样节点和边来构建视图的输入子集。两个视图被输入到参数共享的连体网络中，以提取高维嵌入并估计整个图的信息熵。对于学习过程，我们提出了同时使用两个目标优化网络的方法。具体而言，对比损失函数的输入由正负对组成。我们的对比对策略与以前的不同

    Contrastive learning on graphs aims at extracting distinguishable high-level representations of nodes. In this paper, we theoretically illustrate that the entropy of a dataset can be approximated by maximizing the lower bound of the mutual information across different views of a graph, \ie, entropy is estimated by a neural network. Based on this finding, we propose a simple yet effective subset sampling strategy to contrast pairwise representations between views of a dataset. In particular, we randomly sample nodes and edges from a given graph to build the input subset for a view. Two views are fed into a parameter-shared Siamese network to extract the high-dimensional embeddings and estimate the information entropy of the entire graph. For the learning process, we propose to optimize the network using two objectives, simultaneously. Concretely, the input of the contrastive loss function consists of positive and negative pairs. Our selection strategy of pairs is different from previous
    
[^37]: 基于拓扑感知的鲁棒优化方法用于越界泛化问题

    Topology-aware Robust Optimization for Out-of-distribution Generalization. (arXiv:2307.13943v1 [cs.LG])

    [http://arxiv.org/abs/2307.13943](http://arxiv.org/abs/2307.13943)

    本文提出了一种基于拓扑感知的鲁棒优化方法，用于解决越界泛化问题。该方法通过整合分布的拓扑结构来约束优化过程，实现高泛化置信度。理论和实证结果表明，该方法在越界泛化任务中明显优于现有方法。

    

    越界泛化是一个具有挑战性的机器学习问题，在许多高风险应用中非常有价值。现有方法在泛化置信度低的情况下受到过于悲观的建模。由于不可能对任意测试分布进行泛化，我们假设在开发强大的越界泛化能力时，分布的拓扑结构至关重要。为此，我们提出了一种拓扑感知的鲁棒优化方法（TRO），将分布的拓扑无缝地整合到一个合理的优化框架中。具体而言，TRO解决了两个优化目标：（1）拓扑学习，探索数据流形以揭示分布的拓扑结构；（2）拓扑学习，利用拓扑结构对鲁棒优化进行约束，以实现紧密边界的泛化风险。理论上证明了我们方法的有效性，并经验证明它在广泛范围内明显优于现有技术水平。

    Out-of-distribution (OOD) generalization is a challenging machine learning problem yet highly desirable in many high-stake applications. Existing methods suffer from overly pessimistic modeling with low generalization confidence. As generalizing to arbitrary test distributions is impossible, we hypothesize that further structure on the topology of distributions is crucial in developing strong OOD resilience. To this end, we propose topology-aware robust optimization (TRO) that seamlessly integrates distributional topology in a principled optimization framework. More specifically, TRO solves two optimization objectives: (1) Topology Learning which explores data manifold to uncover the distributional topology; (2) Learning on Topology which exploits the topology to constrain robust optimization for tightly-bounded generalization risks. We theoretically demonstrate the effectiveness of our approach and empirically show that it significantly outperforms the state of the arts in a wide rang
    
[^38]: 提高半监督语义分割效果的双级孪生结构网络

    Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network. (arXiv:2307.13938v1 [cs.CV])

    [http://arxiv.org/abs/2307.13938](http://arxiv.org/abs/2307.13938)

    提出了一种用于半监督语义分割的双级孪生结构网络（DSSN），通过像素级对比学习来充分利用未标记数据，并引入了类别感知伪标签选择策略，最大限度地提高了算法的有效性。

    

    半监督语义分割是一项重要任务，利用标记和未标记数据减少标记训练样例的开销。然而，SSS算法的有效性受到充分利用未标记数据潜力的困难的限制。为了解决这个问题，我们提出了一种用于像素级对比学习的双级孪生结构网络（DSSN）。通过在低级图像空间和高级特征空间中使用强增强视图的像素级对比损失来对齐正对，我们设计的DSSN旨在最大化可用未标记数据的利用。此外，我们引入了一种新的类别感知伪标签选择策略，用于从弱到强的监督，解决了大多数现有方法不执行选择或为所有类别应用预定义阈值的限制。具体而言，我们的策略选择了每个类别弱视图的高置信度预测来生成伪标签。

    Semi-supervised semantic segmentation (SSS) is an important task that utilizes both labeled and unlabeled data to reduce expenses on labeling training examples. However, the effectiveness of SSS algorithms is limited by the difficulty of fully exploiting the potential of unlabeled data. To address this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise contrastive learning. By aligning positive pairs with a pixel-wise contrastive loss using strong augmented views in both low-level image space and high-level feature space, the proposed DSSN is designed to maximize the utilization of available unlabeled data. Additionally, we introduce a novel class-aware pseudo-label selection strategy for weak-to-strong supervision, which addresses the limitations of most existing methods that do not perform selection or apply a predefined threshold for all classes. Specifically, our strategy selects the top high-confidence prediction of the weak view for each class to generate ps
    
[^39]: trajdata：多个人类轨迹数据集的统一接口

    trajdata: A Unified Interface to Multiple Human Trajectory Datasets. (arXiv:2307.13924v1 [cs.CV])

    [http://arxiv.org/abs/2307.13924](http://arxiv.org/abs/2307.13924)

    trajdata是一个统一接口，提供简单、统一和高效的轨迹和地图数据的表示和API，在轨迹预测领域起到了重要作用，并对当前的人类和AV运动预测研究提供了深入理解和未来数据集的建议。

    

    近年来，由于自动驾驶车辆（AVs）和行人运动跟踪的大规模真实世界人类轨迹数据集的发布，轨迹预测领域取得了显著发展。尽管这些数据集对社区来说是一个福音，但它们都使用自定义和独特的数据格式和API，使得研究人员难以在多个数据集上训练和评估方法。为了解决这个问题，我们提出了trajdata：多个人类轨迹数据集的统一接口。trajdata提供了一个简单、统一和高效的轨迹和地图数据的表示和API。本文通过实证评估现有的轨迹数据集，展示了trajdata的能力，使用户对当前行人和AV运动预测研究的数据有了深入理解，并提出了未来数据集的建议。

    The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissiv
    
[^40]: 基于模拟的推理用于心血管模型

    Simulation-based Inference for Cardiovascular Models. (arXiv:2307.13918v1 [stat.ML])

    [http://arxiv.org/abs/2307.13918](http://arxiv.org/abs/2307.13918)

    本研究将心血管模型的逆问题作为统计推理进行解决，在体外进行了五个生物标记物的不确定性分析，展示了模拟推理的能力。

    

    在过去的几十年中，血流动力学模拟器不断发展，已成为研究体外心血管系统的首选工具。虽然这样的工具通常用于从生理参数模拟全身血流动力学，但解决将波形映射回合理的生理参数的逆问题仍然有很大的潜力和挑战。受模拟推理（SBI）的进展的启发，我们将这个逆问题作为统计推理来处理。与其他方法不同，SBI为感兴趣的参数提供了后验分布，提供了关于个体测量的不确定性的多维表示。我们通过对比几种测量模态来展示这种能力，进行了五个临床感兴趣的生物标志物的体外不确定性分析。除了确认已知事实，比如估计心率的可行性，我们的研究还突出了…

    Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlight
    
[^41]: BayesDAG：基于梯度的因果发现的后验采样

    BayesDAG: Gradient-Based Posterior Sampling for Causal Discovery. (arXiv:2307.13917v1 [cs.LG])

    [http://arxiv.org/abs/2307.13917](http://arxiv.org/abs/2307.13917)

    这项研究引入了一种基于梯度的后验采样方法，用于解决Bayesian causal discovery中的计算挑战，能够高效地推断因果模型，并且不依赖于DAG正则化。

    

    贝叶斯因果发现旨在从观测数据中推断出因果模型的后验分布，量化认知不确定性，从而有助于下游任务。然而，由于有向无环图（DAG）和非线性函数的组合空间的联合推理而带来了计算挑战。尽管近年来在DAG上的高效后验推断方面取得了进展，但现有方法要么仅限于对线性因果模型的节点排列矩阵进行变分推断，导致推断准确性受损，要么是在受DAG正则化约束的邻接矩阵上进行连续松弛，而不能确保得到的图是DAGs。在这项工作中，我们介绍了一种基于随机梯度马尔科夫链蒙特卡罗（SG-MCMC）的可扩展贝叶斯因果发现框架，克服了这些局限性。我们的方法直接从后验中采样DAG，并且不需要任何DAG正则化，同时还绘制函数参数样本和…

    Bayesian causal discovery aims to infer the posterior distribution over causal models from observed data, quantifying epistemic uncertainty and benefiting downstream tasks. However, computational challenges arise due to joint inference over combinatorial space of Directed Acyclic Graphs (DAGs) and nonlinear functions. Despite recent progress towards efficient posterior inference over DAGs, existing methods are either limited to variational inference on node permutation matrices for linear causal models, leading to compromised inference accuracy, or continuous relaxation of adjacency matrices constrained by a DAG regularizer, which cannot ensure resulting graphs are DAGs. In this work, we introduce a scalable Bayesian causal discovery framework based on stochastic gradient Markov Chain Monte Carlo (SG-MCMC) that overcomes these limitations. Our approach directly samples DAGs from the posterior without requiring any DAG regularization, simultaneously draws function parameter samples and 
    
[^42]: 在预测上下文中的在线学习问题

    Online learning in bandits with predicted context. (arXiv:2307.13916v1 [stat.ML])

    [http://arxiv.org/abs/2307.13916](http://arxiv.org/abs/2307.13916)

    本文研究了一种在预测上下文中的在线学习问题，通过将经典统计学中的测量误差模型推广到在线决策设置中，我们提出了第一个具有次线性后悔的在线算法。

    

    我们考虑在每个时刻，代理只能访问到上下文的一个带噪声的版本以及误差方差（或者这个方差的一个估计）。这一设置受到了许多应用的启发，在这些应用中，用于决策的真实上下文是不可观测的，而只有一个由可能复杂的机器学习算法预测出的上下文。当上下文误差是非衰减的时候，经典的bandit算法无法达到次线性的后悔。我们提出了在这一设置下，第一个具有次线性后悔的在线算法，并与适当的基准进行了比较。关键的思想是将经典统计学中的测量误差模型推广到在线决策设置中，这是非平凡的，因为策略依赖于有噪声的上下文观察。

    We consider the contextual bandit problem where at each time, the agent only has access to a noisy version of the context and the error variance (or an estimator of this variance). This setting is motivated by a wide range of applications where the true context for decision-making is unobserved, and only a prediction of the context by a potentially complex machine learning algorithm is available. When the context error is non-diminishing, classical bandit algorithms fail to achieve sublinear regret. We propose the first online algorithm in this setting with sublinear regret compared to the appropriate benchmark. The key idea is to extend the measurement error model in classical statistics to the online decision-making setting, which is nontrivial due to the policy being dependent on the noisy context observations.
    
[^43]: 基于图神经网络的混合框架用于预测颗粒破碎强度

    Graph Neural Networks-based Hybrid Framework For Predicting Particle Crushing Strength. (arXiv:2307.13909v1 [cs.LG])

    [http://arxiv.org/abs/2307.13909](http://arxiv.org/abs/2307.13909)

    本论文提出了一个基于图神经网络的混合框架，用于预测颗粒破碎强度。通过建模颗粒碎片之间的关系，可以有效地描述颗粒破碎的力学行为。此外，为了促进机器学习在颗粒破碎研究中的进展，研究人员生成了一个包含大量数值模拟数据的开源数据集。

    

    图神经网络已经成为一种有效的机器学习工具，用于多学科任务，例如制药分子分类和化学反应预测，因为它们可以建模不欧几里德的实体之间的关系。颗粒破碎作为土木工程的重要领域，描述了颗粒材料在数值模拟下由于颗粒碎片键的破裂而发生的断裂，这激发了我们通过颗粒碎片与图神经网络（GNNs）的连接性来表征颗粒破碎的力学行为。然而，由于实验测试或数值模拟的昂贵成本，目前缺乏一个开源的大规模颗粒破碎数据集进行研究。因此，我们首先生成了一个包含45,000个数值模拟和900种颗粒类型的数据集，以促进颗粒破碎的机器学习研究进展。其次，我们设计了一个混合框架

    Graph Neural Networks have emerged as an effective machine learning tool for multi-disciplinary tasks such as pharmaceutical molecule classification and chemical reaction prediction, because they can model non-euclidean relationships between different entities. Particle crushing, as a significant field of civil engineering, describes the breakage of granular materials caused by the breakage of particle fragment bonds under the modeling of numerical simulations, which motivates us to characterize the mechanical behaviors of particle crushing through the connectivity of particle fragments with Graph Neural Networks (GNNs). However, there lacks an open-source large-scale particle crushing dataset for research due to the expensive costs of laboratory tests or numerical simulations. Therefore, we firstly generate a dataset with 45,000 numerical simulations and 900 particle types to facilitate the research progress of machine learning for particle crushing. Secondly, we devise a hybrid frame
    
[^44]: 使用基于星星的可达性分析和变长时间序列输入的深度神经网络鲁棒性验证

    Robustness Verification of Deep Neural Networks using Star-Based Reachability Analysis with Variable-Length Time Series Input. (arXiv:2307.13907v1 [cs.LG])

    [http://arxiv.org/abs/2307.13907](http://arxiv.org/abs/2307.13907)

    本文介绍了一种利用基于星星的可达性分析和变长时间序列输入的深度神经网络鲁棒性验证方法，并在预测与健康管理领域的SOC估计和RUL估计中进行了应用。

    

    数据驱动的神经网络（NN）异常检测和预测维护是新兴的研究领域。基于时间序列数据的NN分析可以提供有关过去行为和关键参数（如设备的剩余寿命（RUL）和电池的荷电状态（SOC））的有价值见解。然而，输入的时间序列数据在经过传感器时可能遭受有意或无意的噪声干扰，因此需要对这些NN进行鲁棒性验证和验证。本文提出了一种针对时间序列回归NN（TSRegNN）的鲁棒性验证方法的案例研究，采用基于集合的形式方法。它着重于利用变长输入数据来简化输入操作并增强网络架构的泛化能力。该方法应用于预测与健康管理（PHM）应用领域中的两个数据集：（1）锂离子电池SOC估计和（2）涡轮发动机RUL估计。对NN的鲁棒性进行了核实。

    Data-driven, neural network (NN) based anomaly detection and predictive maintenance are emerging research areas. NN-based analytics of time-series data offer valuable insights into past behaviors and estimates of critical parameters like remaining useful life (RUL) of equipment and state-of-charge (SOC) of batteries. However, input time series data can be exposed to intentional or unintentional noise when passing through sensors, necessitating robust validation and verification of these NNs. This paper presents a case study of the robustness verification approach for time series regression NNs (TSRegNN) using set-based formal methods. It focuses on utilizing variable-length input data to streamline input manipulation and enhance network architecture generalizability. The method is applied to two data sets in the Prognostics and Health Management (PHM) application areas: (1) SOC estimation of a Lithium-ion battery and (2) RUL estimation of a turbine engine. The NNs' robustness is checke
    
[^45]: 腐败鲁棒的Lipschitz上下文搜索

    Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v1 [cs.LG])

    [http://arxiv.org/abs/2307.13903](http://arxiv.org/abs/2307.13903)

    该论文研究了学习具有被篡改的二进制信号的Lipschitz函数的问题，提出了一种腐败鲁棒算法。该算法在不同损失函数下实现了不同程度的后悔。

    

    我研究了学习具有被篡改的二进制信号的Lipschitz函数的问题。学习者试图学习一个由对手选择的Lipschitz函数$f$。在每一轮中，对手在输入空间中选择一个上下文向量$x_t$，学习者对真实函数值$f(x_t)$进行猜测，并接收一个指示猜测是高还是低的二进制信号。在总共$C$轮中，信号可能被篡改，但学习者不知道$C$的值。学习者的目标是造成小的累积损失。我提出了一个自然而强大的技术验证，对设计腐败鲁棒算法非常有用。我设计了一些算法（将Lipschitz参数$L$视为常数）：对于对称损失，学习者在$d=1$时达到后悔$O(C\log T)$，在$d>1$时达到后悔$O_d(C\log T + T^{(d-1)/d})$；对于计价损失，学习者在$d/(d+1)$时达到后悔$\widetilde{O}(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$。

    I study the problem of learning a Lipschitz function with corrupted binary signals. The learner tries to learn a Lipschitz function $f$ that the adversary chooses. In each round, the adversary selects a context vector $x_t$ in the input space, and the learner makes a guess to the true function value $f(x_t)$ and receives a binary signal indicating whether the guess was high or low. In a total of $C$ rounds, the signal may be corrupted, though the value of $C$ is unknown to the learner. The learner's goal is to incur a small cumulative loss. I present a natural yet powerful technique sanity check, which proves useful in designing corruption-robust algorithms. I design algorithms which (treating the Lipschitz parameter $L$ as constant): for the symmetric loss, the learner achieves regret $O(C\log T)$ with $d = 1$ and $O_d(C\log T + T^{(d-1)/d})$ with $d > 1$; for the pricing loss the learner achieves regret $\widetilde{O} (T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
    
[^46]: 用元学习生成模型正则化神经网络

    Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])

    [http://arxiv.org/abs/2307.13899](http://arxiv.org/abs/2307.13899)

    本文提出了一种名为元生成正则化（MGR）的新型生成数据增强策略，通过将合成样本用于特征提取器的正则化项而不是损失函数，最小化验证损失，提高了深度学习中的生成数据增强效果。

    

    本文研究了改进深度学习的生成数据增强方法。生成数据增强利用生成模型产生的合成样本作为小数据集分类的额外数据集。生成数据增强的一个关键挑战是合成数据中包含降低准确性的无信息样本。这是因为合成样本不能完美地代表真实数据中的类别，均匀抽样也不一定为任务提供有用的样本。本文提出了一种名为元生成正则化（MGR）的新型生成数据增强策略。为了避免生成数据增强的降级，MGR将合成样本用于特征提取器的正则化项而不是损失函数，如交叉熵。这些合成样本通过元学习动态确定，以最小化验证损失。

    This paper investigates methods for improving generative data augmentation for deep learning. Generative data augmentation leverages the synthetic samples produced by generative models as an additional dataset for classification with small dataset settings. A key challenge of generative data augmentation is that the synthetic data contain uninformative samples that degrade accuracy. This is because the synthetic samples do not perfectly represent class categories in real data and uniform sampling does not necessarily provide useful samples for tasks. In this paper, we present a novel strategy for generative data augmentation called meta generative regularization (MGR). To avoid the degradation of generative data augmentation, MGR utilizes synthetic samples in the regularization term for feature extractors instead of in the loss function, e.g., cross-entropy. These synthetic samples are dynamically determined to minimize the validation losses through meta-learning. We observed that MGR 
    
[^47]: 机器学习模型的局部鲁棒性的高效估计

    Efficient Estimation of the Local Robustness of Machine Learning Models. (arXiv:2307.13885v1 [cs.LG])

    [http://arxiv.org/abs/2307.13885](http://arxiv.org/abs/2307.13885)

    本文开发了一种通过局部线性函数逼近和多元正态CDF，高效计算多类别判别模型的局部鲁棒性的分析估计器。实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    

    机器学习模型通常需要对噪声输入数据具有鲁棒性。现实世界中的噪声（通常是随机的）对模型预测的影响可以通过模型的局部鲁棒性来捕捉，即在输入周围的局部区域内模型预测的一致性。然而，基于蒙特卡罗采样的计算局部鲁棒性的朴素方法在统计上是低效的，对于大规模应用而言计算成本高昂。在这项工作中，我们通过局部线性函数逼近和多元正态CDF开发了首个分析估计器，以高效计算多类别判别模型的局部鲁棒性。通过这些估计器的推导，我们展示了局部鲁棒性与随机平滑和softmax概率等概念的联系。我们还通过实验证实这些估计器准确且高效地计算了标准深度学习模型的局部鲁棒性。

    Machine learning models often need to be robust to noisy input data. The effect of real-world noise (which is often random) on model predictions is captured by a model's local robustness, i.e., the consistency of model predictions in a local region around an input. However, the na\"ive approach to computing local robustness based on Monte-Carlo sampling is statistically inefficient, leading to prohibitive computational costs for large-scale applications. In this work, we develop the first analytical estimators to efficiently compute local robustness of multi-class discriminative models using local linear function approximation and the multivariate Normal CDF. Through the derivation of these estimators, we show how local robustness is connected to concepts such as randomized smoothing and softmax probability. We also confirm empirically that these estimators accurately and efficiently compute the local robustness of standard deep learning models. In addition, we demonstrate these estima
    
[^48]: ExeDec: 在神经程序合成中进行执行分解以实现组合泛化

    ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis. (arXiv:2307.13883v1 [cs.LG])

    [http://arxiv.org/abs/2307.13883](http://arxiv.org/abs/2307.13883)

    ExeDec是一种基于分解的合成策略，通过预测执行子目标并在每个步骤的程序执行的指导下逐步解决问题，实现了更好的合成性能和组合泛化能力。

    

    在编写程序时，人们通过将复杂任务分解为较小且更熟悉的子任务来解决。虽然衡量神经程序合成方法是否具有类似的能力是困难的，但我们可以衡量它们是否能够进行组合泛化，即经过训练在较简单的子任务上的模型是否能够解决更复杂的任务。在本文中，我们描述了在程序合成中希望的几种不同形式的组合泛化，并形成一个元基准，用于为两个受欢迎的数据集RobustFill和DeepCoder创建泛化任务。然后，我们提出了一种新颖的基于分解的合成策略ExeDec，它通过在每个步骤的程序执行的指导下逐步预测执行子目标来解决问题。与基线方法相比，ExeDec具有更佳的合成性能和大大改进的组合泛化能力。

    When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines.
    
[^49]: 优良格训练: 借助数论加速的物理信息神经网络

    Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory. (arXiv:2307.13869v1 [cs.LG])

    [http://arxiv.org/abs/2307.13869](http://arxiv.org/abs/2307.13869)

    本研究提出了一种新的物理信息神经网络训练方法，受数论方法启发，通过选择适当的插值点来提高解决偏微分方程的准确性和效率。

    

    物理信息神经网络(PINNs)提供了一种新颖高效的解决偏微分方程(PDEs)的方法。它们的成功在于物理信息损失函数，该函数训练神经网络以满足给定点上的PDE，并对解进行逼近。然而，PDE的解在本质上是无限维的，并且输出与解之间的距离是定义在整个域上的积分。因此，物理信息损失函数仅提供有限的逼近。在选择合适的插值点方面则变得至关重要，尽管这一方面经常被忽视。在本文中，我们提出了一种新的技术，称为优良格训练(GLT)，用于PINNs，受数值分析中的数论方法的启发。GLT提供了一组即使在少量点和多维空间中也非常有效的插值点。我们的实验表明，GLT只需要2-20倍的点数

    Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 tim
    
[^50]: 从高维观测研究中学习变异源

    Learning sources of variability from high-dimensional observational studies. (arXiv:2307.13868v1 [stat.ME])

    [http://arxiv.org/abs/2307.13868](http://arxiv.org/abs/2307.13868)

    本研究提出了一种针对高维观测研究的方法，将因果估计泛化到任意维度或可测空间的结果，并提出了一种用于名义变量的因果偏差测试。实验证明该方法相比现有策略在有限样本有效性和功率方面有改进。

    

    因果推断研究是否存在一个变量影响观测结果。通过诸如“平均治疗效果”等量化指标，这一范式在许多生物领域中被采用，从疫苗和药物开发到政策干预。不幸的是，大多数方法通常仅限于单变量结果。我们的工作将因果估计泛化到任意维度或可测空间的结果，并将传统的因果估计形式化为名义变量的因果偏差测试。我们提出了一种简单的技术来调整一致性条件独立性测试，并证明了这些测试是一致性因果偏差测试。数值实验表明，与现有策略相比，我们的方法Causal CDcorr在有限样本有效性和功率方面均有改进。我们的方法都是开源的，可在github.com/ebridge2/cdcorr上获得。

    Causal inference studies whether the presence of a variable influences an observed outcome. As measured by quantities such as the "average treatment effect," this paradigm is employed across numerous biological fields, from vaccine and drug development to policy interventions. Unfortunately, the majority of these methods are often limited to univariate outcomes. Our work generalizes causal estimands to outcomes with any number of dimensions or any measurable space, and formulates traditional causal estimands for nominal variables as causal discrepancy tests. We propose a simple technique for adjusting universally consistent conditional independence tests and prove that these tests are universally consistent causal discrepancy tests. Numerical experiments illustrate that our method, Causal CDcorr, leads to improvements in both finite sample validity and power when compared to existing strategies. Our methods are all open source and available at github.com/ebridge2/cdcorr.
    
[^51]: 预训练的深度2.5D模型用于视网膜OCT的高效预测建模

    Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT. (arXiv:2307.13865v1 [cs.CV])

    [http://arxiv.org/abs/2307.13865](http://arxiv.org/abs/2307.13865)

    本文介绍了预训练的深度2.5D模型，用于高效预测视网膜OCT的疾病进展。采用混合2.5D方法结合二维和三维技术，有效优化了性能和内存要求。

    

    在医学影像领域，3D深度学习模型在构建强大的疾病进展预测模型中起着至关重要的作用。然而，这些模型的大小在计算资源和数据要求方面带来了重大挑战。此外，实现高质量的3D模型的预训练更加困难。为了解决这些问题，混合2.5D方法为利用二维模型高效地处理三维体积数据提供了有效的解决方案。结合二维和三维技术为优化性能并最小化内存要求提供了一条有希望的途径。在本文中，我们探讨了基于卷积神经网络（CNNs）、长短期记忆（LSTM）和Transformer的2.5D架构。此外，利用最近的非对比性预训练方法在二维上的优势，我们进一步提高了2.5D技术的性能和数据效率。我们展示了这种架构的有效性。

    In the field of medical imaging, 3D deep learning models play a crucial role in building powerful predictive models of disease progression. However, the size of these models presents significant challenges, both in terms of computational resources and data requirements. Moreover, achieving high-quality pretraining of 3D models proves to be even more challenging. To address these issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D volumetric data efficiently using 2D models. Combining 2D and 3D techniques offers a promising avenue for optimizing performance while minimizing memory requirements. In this paper, we explore 2.5D architectures based on a combination of convolutional neural networks (CNNs), long short-term memory (LSTM), and Transformers. In addition, leveraging the benefits of recent non-contrastive pretraining approaches in 2D, we enhanced the performance and data efficiency of 2.5D techniques even further. We demonstrate the effectiveness of archi
    
[^52]: 学习设计模拟电路以满足阈值规范

    Learning to Design Analog Circuits to Meet Threshold Specifications. (arXiv:2307.13861v1 [cs.LG])

    [http://arxiv.org/abs/2307.13861](http://arxiv.org/abs/2307.13861)

    本论文提出了一种使用监督学习方法，通过仿真数据训练系统来设计满足阈值规范的模拟电路，实验结果显示成功率超过90％。

    

    近年来，使用监督学习或强化学习从仿真数据中自动设计模拟电路和射频电路作为手动专家设计的替代方法已经得到研究。对于设计代理来说，从期望的性能指标到电路参数的逆函数学习是直接的。然而，用户通常具有阈值性能标准，而不是准确的目标向量。在这项工作中，我们提出了一种方法，通过仿真数据生成数据集，可以通过监督学习训练系统来设计满足阈值规范的电路。我们通过进行迄今为止最广泛的自动模拟电路设计评估，包括在比以前的工作更多样化的电路集合中进行实验，涵盖线性、非线性和自治电路配置，并展示我们的方法始终在5％误差边界下获得超过90％的成功率。

    Automated design of analog and radio-frequency circuits using supervised or reinforcement learning from simulation data has recently been studied as an alternative to manual expert design. It is straightforward for a design agent to learn an inverse function from desired performance metrics to circuit parameters. However, it is more common for a user to have threshold performance criteria rather than an exact target vector of feasible performance measures. In this work, we propose a method for generating from simulation data a dataset on which a system can be trained via supervised learning to design circuits to meet threshold specifications. We moreover perform the to-date most extensive evaluation of automated analog circuit design, including experimenting in a significantly more diverse set of circuits than in prior work, covering linear, nonlinear, and autonomous circuit configurations, and show that our method consistently reaches success rate better than 90% at 5% error margin, w
    
[^53]: 关于图像恢复任务中transformers的不合理易受攻击性 -- 以及一个简单的修复方法

    On the unreasonable vulnerability of transformers for image restoration -- and an easy fix. (arXiv:2307.13856v1 [cs.CV])

    [http://arxiv.org/abs/2307.13856](http://arxiv.org/abs/2307.13856)

    我们调查了Vision Transformers（ViTs）在图像恢复任务中的鲁棒性，发现ViTs在图像分类任务中的改进并不能扩展到图像恢复任务中，并且这些模型易受对抗攻击。我们尝试通过对抗性训练来提高它们的鲁棒性。

    

    在视觉识别任务中取得成功后，Vision Transformers（ViTs）越来越多地被用于图像恢复任务。一些最近的研究声称ViTs在图像分类任务中具有更好的鲁棒性属性，我们调查了ViTs的改进对图像恢复任务的鲁棒性是否具有扩展性。我们考虑了最近提出的Restormer模型，以及NAFNet和“基准网络”，它们都是Restormer的简化版本。我们使用投影梯度下降（PGD）和CosPGD进行鲁棒性评估，这是一种针对像素预测任务提出的最新的对抗攻击方法。我们在GoPro数据集上对真实世界的图像进行了图像去模糊任务的实验。我们的分析表明，与ViTs在图像分类任务中的声称相反，这些模型非常容易受到对抗攻击。我们试图通过对抗性训练来提高它们的鲁棒性。尽管这导致了显著的增加

    Following their success in visual recognition tasks, Vision Transformers(ViTs) are being increasingly employed for image restoration. As a few recent works claim that ViTs for image classification also have better robustness properties, we investigate whether the improved adversarial robustness of ViTs extends to image restoration. We consider the recently proposed Restormer model, as well as NAFNet and the "Baseline network" which are both simplified versions of a Restormer. We use Projected Gradient Descent (PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise prediction tasks for our robustness evaluation. Our experiments are performed on real-world images from the GoPro dataset for image deblurring. Our analysis indicates that contrary to as advocated by ViTs in image classification works, these models are highly susceptible to adversarial attacks. We attempt to improve their robustness through adversarial training. While this yields a significant increase
    
[^54]: 探索锐化余弦相似度

    Exploring the Sharpened Cosine Similarity. (arXiv:2307.13855v1 [cs.CV])

    [http://arxiv.org/abs/2307.13855](http://arxiv.org/abs/2307.13855)

    该论文研究了锐化余弦相似度(SCS)在神经网络中作为卷积的替代品的可能性。研究发现，虽然SCS可能不会提高准确性，但可能学习到更可解释的表示，并在某些情况下稍微增加对抗性的鲁棒性。

    

    卷积层长期以来一直是图像分类的主要工具。最近，提出了一种替代卷积的方法，使用锐化余弦相似度（SCS），据理论上认为可能作为更好的特征检测器。虽然多个来源报道了有希望的结果，但迄今为止还没有进行全面的经验分析，评估这些新层在神经网络性能方面。在我们的工作中，我们探索了SCS的参数行为和在CIFAR-10上多个CNN架构中作为卷积的替代品的潜力。我们发现，虽然SCS可能不会显著提高准确性，但可能学习到更可解释的表示。我们还发现，在某些情况下，SCS可能会稍微增加对抗性的鲁棒性。

    Convolutional layers have long served as the primary workhorse for image classification. Recently, an alternative to convolution was proposed using the Sharpened Cosine Similarity (SCS), which in theory may serve as a better feature detector. While multiple sources report promising results, there has not been to date a full-scale empirical analysis of neural network performance using these new layers. In our work, we explore SCS's parameter behavior and potential as a drop-in replacement for convolutions in multiple CNN architectures benchmarked on CIFAR-10. We find that while SCS may not yield significant increases in accuracy, it may learn more interpretable representations. We also find that, in some circumstances, SCS may confer a slight increase in adversarial robustness.
    
[^55]: WebArena: 一个用于构建自主智能体的真实网络环境

    WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])

    [http://arxiv.org/abs/2307.13854](http://arxiv.org/abs/2307.13854)

    WebArena是一个用于构建自主智能体的真实网络环境，它包含了完全功能的网站，并且通过引入工具和外部知识库来鼓励智能体像人类一样解决任务。此外，WebArena还发布了一组用于评估任务完成功能正确性的基准任务。

    

    随着生成式人工智能的进展，通过自然语言指令进行日常任务的自主智能体的潜力逐渐显现。然而，当前的智能体主要是在简化的合成环境中创建和测试的，严重限制了现实世界场景的表示能力。在本文中，我们构建了一个高度逼真且可复现的智能体指令和控制环境。具体而言，我们关注在网站上执行任务的智能体，我们创建了一个包含来自四个常见领域的完全功能网站的环境，分别是电子商务、社交论坛讨论、协同软件开发和内容管理。我们的环境使用工具（如地图）和外部知识库（如用户手册）来鼓励像人类一样解决任务。在我们的环境基础上，我们发布了一组重点评估任务完成功能正确性的基准任务。我们基准任务具有多样性和长远的视野，并且被设计为鼓励智能体进行更深层次的任务理解和解决。

    With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
    
[^56]: SplitFed对数据包丢失的韧性：何处切割，这是问题。

    SplitFed resilience to packet loss: Where to split, that is the question. (arXiv:2307.13851v1 [cs.CV])

    [http://arxiv.org/abs/2307.13851](http://arxiv.org/abs/2307.13851)

    本文研究了分割联邦学习（SplitFed）在通信链路上对数据包丢失的鲁棒性，并通过实验证明了更深的切割点对最终模型的准确性具有统计显著的优势。

    

    近年来，通过联邦学习（FL），分割学习（SL）以及它们的混合形式，如分割联邦学习（SplitFed或SFL），分散式机器学习的范围得到了扩展。SFL的目标是降低FL中每个客户端所需的计算能力，并在保护隐私的同时实现SL的并行化。本文研究了SFL对通信链路上数据包丢失的鲁棒性。通过在模型的两个位置——浅度切割点和深度切割点——进行切割，并测试切割点是否对最终模型的准确性产生统计显著的影响，考察了不同的SFL聚合策略的性能。实验在人类胚胎图像分割模型上进行，并表明更深的切割点具有统计显著的优势。

    Decentralized machine learning has broadened its scope recently with the invention of Federated Learning (FL), Split Learning (SL), and their hybrids like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce the computational power required by each client in FL and parallelize SL while maintaining privacy. This paper investigates the robustness of SFL against packet loss on communication links. The performance of various SFL aggregation strategies is examined by splitting the model at two points -- shallow split and deep split -- and testing whether the split point makes a statistically significant difference to the accuracy of the final model. Experiments are carried out on a segmentation model for human embryo images and indicate the statistically significant advantage of a deeper split point.
    
[^57]: MAEA: 基于多模态的智能体人工智能的归因

    MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])

    [http://arxiv.org/abs/2307.13850](http://arxiv.org/abs/2307.13850)

    MAEA是一个用于计算任何可微分的多模态智能体人工智能政策的全局归因的框架。它可以通过归因分析来排名和分组故障场景，调查建模和数据集偏见，并对多模态智能体人工智能政策的稳健性和用户信任进行关键分析。

    

    理解多模态感知对于智能体人工智能是一个开放性问题，因为这样的输入可能包含高度互补和冗余的信息用于任务。多模态政策的一个相关方向是理解融合层中每种模态的全局趋势。为此，我们在ALFRED数据集训练的不同政策上解开了视觉、语言和先前动作输入的归因。归因分析可以用于排名和分组故障场景、调查建模和数据集偏见，并在部署之前对多模态EAI政策的稳健性和用户信任进行关键分析。我们提出了MAEA，一个用于计算任何可微分政策的每个模态的全局归因的框架。此外，我们展示了归因如何在EAI政策的语言和视觉归因中启用更底层的行为分析。

    Understanding multimodal perception for embodied AI is an open question because such inputs may contain highly complementary as well as redundant information for the task. A relevant direction for multimodal policies is understanding the global trends of each modality at the fusion layer. To this end, we disentangle the attributions for visual, language, and previous action inputs across different policies trained on the ALFRED dataset. Attribution analysis can be utilized to rank and group the failure scenarios, investigate modeling and dataset biases, and critically analyze multimodal EAI policies for robustness and user trust before deployment. We present MAEA, a framework to compute global attributions per modality of any differentiable policy. In addition, we show how attributions enable lower-level behavior analysis in EAI policies for language and visual attributions.
    
[^58]: 批大小和步数与使用Armijo线搜索的随机梯度下降非凸优化之间的关系

    Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search. (arXiv:2307.13831v1 [cs.LG])

    [http://arxiv.org/abs/2307.13831](http://arxiv.org/abs/2307.13831)

    这项研究分析了使用Armijo线搜索的随机梯度下降非凸优化中批大小和步数的关系，并发现随着批大小的增加，所需的步数减少。

    

    随机梯度下降（SGD）是训练深度神经网络最简单的深度学习优化器。虽然SGD可以使用各种学习率，如常数或递减的学习率，但之前的数值结果表明，当SGD使用线搜索方法给出的学习率时，它的表现优于其他深度学习优化器。本文对使用Armijo线搜索给出学习率的非凸优化中的SGD进行了收敛性分析。分析表明，当步数和批大小都很大时，全梯度的平方范数的期望上界变小。接下来，我们展示了对于使用Armijo线搜索学习率的SGD来说，非凸优化所需的步数是批大小的单调递减凸函数；也就是说，随着批大小的增加，非凸优化所需的步数减少。此外，我们还展示了随机火灾的贡献。

    Stochastic gradient descent (SGD) is the simplest deep learning optimizer with which to train deep neural networks. While SGD can use various learning rates, such as constant or diminishing rates, the previous numerical results showed that SGD performs better than other deep learning optimizers using when it uses learning rates given by line search methods. In this paper, we perform a convergence analysis on SGD with a learning rate given by an Armijo line search for nonconvex optimization. The analysis indicates that the upper bound of the expectation of the squared norm of the full gradient becomes small when the number of steps and the batch size are large. Next, we show that, for SGD with the Armijo-line-search learning rate, the number of steps needed for nonconvex optimization is a monotone decreasing convex function of the batch size; that is, the number of steps needed for nonconvex optimization decreases as the batch size increases. Furthermore, we show that the stochastic fir
    
[^59]: 无模型强化学习中的在线Q函数正则化方法

    Offline Reinforcement Learning with On-Policy Q-Function Regularization. (arXiv:2307.13824v1 [cs.LG])

    [http://arxiv.org/abs/2307.13824](http://arxiv.org/abs/2307.13824)

    本论文提出了一种无模型强化学习方法，通过正则化向行为策略的Q函数进行训练，而不是训练行为策略本身。该方法利用了Q函数的估计来处理外推误差，并在D4RL基准测试中展现了强大的性能。

    

    离线强化学习（Offline RL）的核心挑战是处理历史数据集与期望策略之间的分布变化所引起的（潜在灾难性的）外推误差。先前的大部分工作通过隐式/显式地将学习策略向行为策略进行正则化来解决这个挑战，但在实践中很难可靠地估计行为策略。本文中，我们提出了一种向行为策略的Q函数进行正则化的方法，前提是Q函数可以通过SARSA-style估计更可靠且更容易地处理外推误差。我们提出了两种算法，通过正则化利用估计的Q函数，并证明它们在D4RL基准测试中表现出很强的性能。

    The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.
    
[^60]: 用多分辨率神经网络拟合听觉滤波器组

    Fitting Auditory Filterbanks with Multiresolution Neural Networks. (arXiv:2307.13821v1 [cs.SD])

    [http://arxiv.org/abs/2307.13821](http://arxiv.org/abs/2307.13821)

    本文提出了一种名为多分辨率神经网络（MuReNN）的神经音频模型，通过在离散小波变换（DWT）的八度子带上训练单独的卷积算子，解决了基于波形的深度学习面临的非参数和参数方法之间的困境。

    

    基于波形的深度学习面临非参数和参数方法之间的困境。一方面，卷积神经网络（convnets）可以近似任何线性时不变系统；然而，在实践中，它们的频率响应随着感受野的增长变得更加不规则。另一方面，诸如LEAF之类的参数模型保证产生Gabor滤波器，从而实现最佳的时频定位；然而，这种强烈的归纳偏见不利于表示能力。本文旨在通过引入名为多分辨率神经网络（MuReNN）的神经音频模型来克服这一困境。MuReNN背后的核心思想是在离散小波变换（DWT）的八度子带上训练单独的卷积算子。由于DWT原子的尺度在八度之间按指数增长，MuReNN中后续可学习卷积的感受野也相应扩张。对于给定的真实世界数据集，我们拟合了幅度...

    Waveform-based deep learning faces a dilemma between nonparametric and parametric approaches. On one hand, convolutional neural networks (convnets) may approximate any linear time-invariant system; yet, in practice, their frequency responses become more irregular as their receptive fields grow. On the other hand, a parametric model such as LEAF is guaranteed to yield Gabor filters, hence an optimal time-frequency localization; yet, this strong inductive bias comes at the detriment of representational capacity. In this paper, we aim to overcome this dilemma by introducing a neural audio model, named multiresolution neural network (MuReNN). The key idea behind MuReNN is to train separate convolutional operators over the octave subbands of a discrete wavelet transform (DWT). Since the scale of DWT atoms grows exponentially between octaves, the receptive fields of the subsequent learnable convolutions in MuReNN are dilated accordingly. For a given real-world dataset, we fit the magnitude r
    
[^61]: 基于梯度的随机点积图谱嵌入

    Gradient-Based Spectral Embeddings of Random Dot Product Graphs. (arXiv:2307.13818v1 [cs.LG])

    [http://arxiv.org/abs/2307.13818](http://arxiv.org/abs/2307.13818)

    本文介绍了基于梯度的随机点积图谱嵌入方法，并通过利用非凸优化技术改进了在观察图中估计节点潜在向量的任务。同时，作者还提出了一阶梯度下降方法来更好地解决嵌入问题，并适应更广泛的实用网络嵌入应用。

    

    随机点积图谱（RDPG）是一个关系数据的生成模型，其中节点通过在低维欧氏空间中的潜在向量表示。RDPG关键地假设边的形成概率由相应的潜在位置的点积给出。因此，从观察到的图中估计这些向量的嵌入任务通常被设定为一个低秩矩阵分解问题。经典的邻接谱嵌入（ASE）具有可靠的统计性质，但它在形式上解决的是一个代理问题，并且计算复杂度较高。在本文中，我们利用非凸优化的最新进展，并展示它们对RDPG推断的影响。我们提倡使用一阶梯度下降方法来更好地解决嵌入问题，并自然地适应更广泛的实用网络嵌入应用。值得注意的是，我们认为RDPG嵌入有向图失去了可解释性，除非...

    The Random Dot Product Graph (RDPG) is a generative model for relational data, where nodes are represented via latent vectors in low-dimensional Euclidean space. RDPGs crucially postulate that edge formation probabilities are given by the dot product of the corresponding latent positions. Accordingly, the embedding task of estimating these vectors from an observed graph is typically posed as a low-rank matrix factorization problem. The workhorse Adjacency Spectral Embedding (ASE) enjoys solid statistical properties, but it is formally solving a surrogate problem and can be computationally intensive. In this paper, we bring to bear recent advances in non-convex optimization and demonstrate their impact to RDPG inference. We advocate first-order gradient descent methods to better solve the embedding problem, and to organically accommodate broader network embedding applications of practical relevance. Notably, we argue that RDPG embeddings of directed graphs loose interpretability unless 
    
[^62]: 如何扩展您的EMA（arXiv:2307.13813v1 [stat.ML]）

    How to Scale Your EMA. (arXiv:2307.13813v1 [stat.ML])

    [http://arxiv.org/abs/2307.13813](http://arxiv.org/abs/2307.13813)

    本研究提供了在存在模型EMA的情况下进行优化的缩放规则，以保持训练动态的一致性。这对于实际机器学习中的权衡批量大小和墙钟时间非常重要。模型EMA能够提高模型的性能以及稳定训练过程，并为自监督学习提供学习信号。

    

    在实际机器学习中，保持训练动态在批量大小之间的一致性是一种重要工具，它能够在批量大小和墙钟时间之间进行权衡。这种权衡通常通过一个缩放规则来实现，例如，在随机梯度下降中，应该将学习率与批量大小呈线性关系。另一个实际机器学习的重要工具是模型指数移动平均（EMA），它是一个不接收梯度信息的模型副本，而是以一定的动量跟随其目标模型。这个模型EMA可以提高监督学习的稳健性和泛化性能，稳定伪标记，为自监督学习提供学习信号。之前的研究将模型EMA与优化分开处理，导致批量大小之间存在不同的训练动态和较低的模型性能。在这项工作中，我们提供了在存在模型EMA的情况下进行优化的缩放规则，并展示了其效果。

    Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important tool for practical machine learning is the model Exponential Moving Average (EMA), which is a model copy that does not receive gradient information, but instead follows its target model with some momentum. This model EMA can improve the robustness and generalization properties of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have treated the model EMA separately from optimization, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of model EMAs and demonst
    
[^63]: 体育博彩：神经网络和现代投资组合理论在英超联赛中的应用

    Sports Betting: an application of neural networks and modern portfolio theory to the English Premier League. (arXiv:2307.13807v1 [q-fin.PM])

    [http://arxiv.org/abs/2307.13807](http://arxiv.org/abs/2307.13807)

    本文提出了一种将神经网络模型与投资组合优化相结合的新方法，通过研究英超联赛数据，成功实现了超过初始财富135.8%的惊人利润的体育博彩策略优化。

    

    本文提出了一种在体育博彩中优化投注策略的新方法，该方法将冯·诺依曼-莫根斯特恩期望效用理论、深度学习技术和凯利标准的先进公式相结合。通过将神经网络模型与投资组合优化相结合，我们的方法在2020/2021英超联赛的后半段相对于初始财富获得了惊人的利润，达到了135.8%。我们研究了完整和受限策略，评估了它们的绩效、风险管理和多样化。我们开发了一个深度神经网络模型来预测比赛结果，解决了变量有限等挑战。我们的研究在体育博彩和预测建模领域提供了有价值的洞察和实际应用。

    This paper presents a novel approach for optimizing betting strategies in sports gambling by integrating Von Neumann-Morgenstern Expected Utility Theory, deep learning techniques, and advanced formulations of the Kelly Criterion. By combining neural network models with portfolio optimization, our method achieved remarkable profits of 135.8% relative to the initial wealth during the latter half of the 20/21 season of the English Premier League. We explore complete and restricted strategies, evaluating their performance, risk management, and diversification. A deep neural network model is developed to forecast match outcomes, addressing challenges such as limited variables. Our research provides valuable insights and practical applications in the field of sports betting and predictive modeling.
    
[^64]: 逆问题函数的源条件双稳健推断

    Source Condition Double Robust Inference on Functionals of Inverse Problems. (arXiv:2307.13793v1 [stat.ME])

    [http://arxiv.org/abs/2307.13793](http://arxiv.org/abs/2307.13793)

    本文提出了一种源条件双稳健推断方法，用于估计线性逆问题解的线性函数参数，无需知道哪个逆问题更良好，该方法能确保对感兴趣的参数的渐近正态性，并提供了对迭代Tikhonov正则化对抗估计器的新保证。

    

    本文考虑了线性逆问题解的线性函数参数的估计。任何这样的参数都有一个双稳健表示，该表示依赖于对偶线性逆问题的解，其中对偶解可以被视为逆倾向函数的推广。我们提供了第一个源条件双稳健推断方法，只要原始或对偶逆问题足够良好，无需知道哪个逆问题更良好，就能确保对感兴趣的参数的渐近正态性。我们的结果是通过对线性逆问题的迭代Tikhonov正则化对抗估计器在一般假设空间上的新的保证而实现的，这是一个独立发展的利益。

    We consider estimation of parameters defined as linear functionals of solutions to linear inverse problems. Any such parameter admits a doubly robust representation that depends on the solution to a dual linear inverse problem, where the dual solution can be thought as a generalization of the inverse propensity function. We provide the first source condition double robust inference method that ensures asymptotic normality around the parameter of interest as long as either the primal or the dual inverse problem is sufficiently well-posed, without knowledge of which inverse problem is the more well-posed one. Our result is enabled by novel guarantees for iterated Tikhonov regularized adversarial estimators for linear inverse problems, over general hypothesis spaces, which are developments of independent interest.
    
[^65]: 直方图层时间延迟神经网络用于被动声纳分类

    Histogram Layer Time Delay Neural Networks for Passive Sonar Classification. (arXiv:2307.13788v1 [cs.SD])

    [http://arxiv.org/abs/2307.13788](http://arxiv.org/abs/2307.13788)

    本文提出了一种直方图层时间延迟神经网络的方法，用于改进水下声纳目标识别。通过引入统计背景，该方法在特征学习和分类中取得了优越表现。

    

    在远程海洋感应运作中，水下声纳目标检测面临着复杂的声波传播挑战。尽管有可靠的声纳系统，目标识别仍然是一个困难的问题。各种方法解决了改进的目标识别问题，但大多数方法很难解开观测目标录音中的高维非线性模式。本文提出了一种新方法，将时延神经网络和直方图层相结合，引入了统计背景以实现更好的特征学习和水下声纳目标分类。所提出的方法优于基线模型，证明了引入统计背景对于被动声纳目标识别的实用性。本文的代码公开可用。

    Underwater acoustic target detection in remote marine sensing operations is challenging due to complex sound wave propagation. Despite the availability of reliable sonar systems, target recognition remains a difficult problem. Various methods address improved target recognition. However, most struggle to disentangle the high-dimensional, non-linear patterns in the observed target recordings. In this work, a novel method combines a time delay neural network and histogram layer to incorporate statistical contexts for improved feature learning and underwater acoustic target classification. The proposed method outperforms the baseline model, demonstrating the utility in incorporating statistical contexts for passive sonar target recognition. The code for this work is publicly available.
    
[^66]: GANfather：通过生成恶意行为来改进防御系统的可控生成方法

    The GANfather: Controllable generation of malicious activity to improve defence systems. (arXiv:2307.13787v1 [cs.LG])

    [http://arxiv.org/abs/2307.13787](http://arxiv.org/abs/2307.13787)

    GANfather是一种用于改进防御系统的可控生成方法，能够生成具有恶意行为属性但不需要标记要求的样本。通过引入额外的目标，我们能够提高对非法活动的检测能力，并发现并修正防御性弱点。

    

    用于辅助防御系统检测恶意行为的机器学习方法通常依赖于标记数据。在某些领域中，这种标记数据不可获取或不完整。实际上，这可能导致低检测率和高误报率，例如反洗钱系统。据估计，每年有1.7-4万亿欧元洗钱未被发现。我们提出了GANfather，一种生成具有恶意行为属性样本的方法，不需要标记要求。我们建议在典型的生成对抗网络（GANs）损失上引入额外的目标来奖励生成恶意样本。最终，我们的目标是将辨别器网络作为一种新颖且强大的防御系统，提高对非法活动的检测能力。可选地，我们可以鼓励生成器越过现有的检测系统。这种设置会暴露出辨别器的防御性弱点以进行修正。我们进行了评估。

    Machine learning methods to aid defence systems in detecting malicious activity typically rely on labelled data. In some domains, such labelled data is unavailable or incomplete. In practice this can lead to low detection rates and high false positive rates, which characterise for example anti-money laundering systems. In fact, it is estimated that 1.7--4 trillion euros are laundered annually and go undetected. We propose The GANfather, a method to generate samples with properties of malicious activity, without label requirements. We propose to reward the generation of malicious samples by introducing an extra objective to the typical Generative Adversarial Networks (GANs) loss. Ultimately, our goal is to enhance the detection of illicit activity using the discriminator network as a novel and robust defence system. Optionally, we may encourage the generator to bypass pre-existing detection systems. This setup then reveals defensive weaknesses for the discriminator to correct. We evalua
    
[^67]: 在差分隐私逻辑回归中的准确性增强：一种预训练方法

    Accuracy Amplification in Differentially Private Logistic Regression: A Pre-Training Approach. (arXiv:2307.13771v1 [cs.LG])

    [http://arxiv.org/abs/2307.13771](http://arxiv.org/abs/2307.13771)

    本文通过添加预训练模块，在差分隐私逻辑回归中提高了模型的准确性。

    

    机器学习模型可以记忆训练数据集，因此在私有数据集上训练机器学习模型可能会侵犯个人隐私。差分隐私是一种严格的隐私保护方法，可在机器学习模型中保留底层训练数据集的隐私。然而，在差分隐私框架下训练机器学习模型通常会降低模型的准确性。本文旨在通过预训练模块提高差分隐私机器学习模型（特别是逻辑回归模型）的准确性。具体而言，我们首先在公开训练数据集上对模型进行预训练，该数据集不涉及隐私问题。然后，我们通过使用私有数据集和差分隐私逻辑回归对模型进行微调。数值结果表明，添加预训练模块显著提高了差分隐私逻辑回归的准确性。

    Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can violate the privacy of individuals. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets in ML models. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP-ML model, specifically a logistic regression model, via a pre-training module. In more detail, we initially pre-train our model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our model via the DP logistic regression with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP logistic regression.
    
[^68]: ClusterSeq: 用基于聚类的元学习增强顺序推荐系统

    ClusterSeq: Enhancing Sequential Recommender Systems with Clustering based Meta-Learning. (arXiv:2307.13766v1 [cs.IR])

    [http://arxiv.org/abs/2307.13766](http://arxiv.org/abs/2307.13766)

    ClusterSeq是一种基于聚类的元学习顺序推荐系统，通过利用用户序列的动态信息提高了物品预测的准确性，并保留了次要用户的偏好，并利用了同一聚类中用户的集体知识。

    

    在实际场景中，顺序推荐系统的有效性受到了用户冷启动问题的限制，这是由于有限的交互使得无法准确确定用户的偏好。以前的研究试图通过将元学习与用户和物品侧信息相结合来解决这个问题。然而，这些方法在建模用户偏好动态方面面临着固有的挑战，尤其是对于展现出与更常见或“主要用户”不同偏好的“次要用户”。为了克服这些局限性，我们提出了一种新颖的方法，称为ClusterSeq，一种基于聚类的元学习顺序推荐系统。ClusterSeq利用用户序列中的动态信息来提高物品预测的准确性，即使没有侧信息。该模型保留了次要用户的偏好，而不会被主要用户所掩盖，并利用了同一聚类中用户的集体知识。

    In practical scenarios, the effectiveness of sequential recommendation systems is hindered by the user cold-start problem, which arises due to limited interactions for accurately determining user preferences. Previous studies have attempted to address this issue by combining meta-learning with user and item-side information. However, these approaches face inherent challenges in modeling user preference dynamics, particularly for "minor users" who exhibit distinct preferences compared to more common or "major users." To overcome these limitations, we present a novel approach called ClusterSeq, a Meta-Learning Clustering-Based Sequential Recommender System. ClusterSeq leverages dynamic information in the user sequence to enhance item prediction accuracy, even in the absence of side information. This model preserves the preferences of minor users without being overshadowed by major users, and it capitalizes on the collective knowledge of users within the same cluster. Extensive experiment
    
[^69]: 隐式归一化显式正则化密度估计

    Implicitly Normalized Explicitly Regularized Density Estimation. (arXiv:2307.13763v1 [stat.ML])

    [http://arxiv.org/abs/2307.13763](http://arxiv.org/abs/2307.13763)

    我们提出了一种新的非参数密度估计方法，基于正则化密度的 Sobolev 范数，通过适当的初始化和使用自然梯度，可以得到性能良好的解，该方法在 Anomaly Detection benchmark 中排名第二。

    

    我们提出了一种新的非参数密度估计方法，该方法是基于正则化密度的 Sobolev 范数。这种方法与核密度估计有明显差异，可以清晰解释模型的偏差。虽然我们无法得到相关核函数的闭合解析形式，但我们证明可以通过采样进行近似。决定密度的优化问题是非凸的，标准的梯度方法效果不好。然而，我们证明在适当的初始化和使用自然梯度的情况下，可以得到性能良好的解。最后，虽然该方法提供的是非归一化的密度，无法使用对数似然进行交叉验证，但我们证明可以采用基于 Fisher 散度的分数匹配方法来解决这个问题。我们在最近的异常检测基准套件 ADBench 上评估了得到的方法，并发现它在超过15个算法中排名第二。

    We propose a new approach to non-parametric density estimation, that is based on regularizing a Sobolev norm of the density. This method is provably different from Kernel Density Estimation, and makes the bias of the model clear and interpretable. While there is no closed analytic form for the associated kernel, we show that one can approximate it using sampling. The optimization problem needed to determine the density is non-convex, and standard gradient methods do not perform well. However, we show that with an appropriate initialization and using natural gradients, one can obtain well performing solutions. Finally, while the approach provides unnormalized densities, which prevents the use of log-likelihood for cross validation, we show that one can instead adapt Fisher Divergence based Score Matching methods for this task. We evaluate the resulting method on the comprehensive recent Anomaly Detection benchmark suite, ADBench, and find that it ranks second best, among more than 15 al
    
[^70]: UPREVE: 一个端到端的因果发现评估系统

    UPREVE: An End-to-End Causal Discovery Benchmarking System. (arXiv:2307.13757v1 [cs.LG])

    [http://arxiv.org/abs/2307.13757](http://arxiv.org/abs/2307.13757)

    UPREVE是一个用户友好的端到端因果发现评估系统，它允许用户同时运行多个算法，可视化因果关系，并评估学习到的因果图的准确性，旨在使因果发现对社会计算和行为文化建模等领域的研究人员和实践者更易于访问和用户友好，以获得有价值的见解进行更好的决策。

    

    在复杂的社会行为系统中发现因果关系具有挑战性但又是为了明智决策所必需的。我们提出了Upload, PREprocess, Visualize和Evaluate (UPREVE)，这是一个用户友好的基于web的图形用户界面(GUI)，旨在简化因果发现过程。UPREVE允许用户同时运行多个算法，可视化因果关系，评估学习到的因果图的准确性。通过其易于访问的界面和可定制的功能，UPREVE使社会计算和行为文化建模 (等) 的研究人员和实践者能够有效地探索和理解因果关系。我们的解决方案旨在使因果发现更易于访问和用户友好，使用户能够获得有价值的见解以进行更好的决策。

    Discovering causal relationships in complex socio-behavioral systems is challenging but essential for informed decision-making. We present Upload, PREprocess, Visualize, and Evaluate (UPREVE), a user-friendly web-based graphical user interface (GUI) designed to simplify the process of causal discovery. UPREVE allows users to run multiple algorithms simultaneously, visualize causal relationships, and evaluate the accuracy of learned causal graphs. With its accessible interface and customizable features, UPREVE empowers researchers and practitioners in social computing and behavioral-cultural modeling (among others) to explore and understand causal relationships effectively. Our proposed solution aims to make causal discovery more accessible and user-friendly, enabling users to gain valuable insights for better decision-making.
    
[^71]: 时间变化的马尔科夫随机场的解路径与离散正则化

    Solution Path of Time-varying Markov Random Fields with Discrete Regularization. (arXiv:2307.13750v1 [math.OC])

    [http://arxiv.org/abs/2307.13750](http://arxiv.org/abs/2307.13750)

    本文提出了一种新的离散正则化的方法来解决推断稀疏时间变化的马尔科夫随机场问题。通过离开最大似然估计范式，我们可以有效地求解所有稀疏水平的解路径，并得到理想的统计性质。

    

    本文研究了在参数上应用不同离散和时间正则化的推断稀疏时间变化的马尔科夫随机场(MRF)问题。由于离散正则化的复杂性，解决这个问题的大多数方法依赖于所谓的最大似然估计(MLE)与放松正则化，既不能得到理想的统计性质，也不能适应现实场景中的尺度。在本文中，我们通过离开MLE的范式，转而求解一类具有精确、离散正则化的受限优化问题，以促进估计参数的稀疏性。尽管我们的公式具有非凸和离散的特点，但我们证明它可以高效且参数化地求解所有稀疏水平的时间变化MRF的解路径。更具体地说，我们证明所有稀疏水平的时间变化MRF的整个解路径可以在O(pT^3)的复杂度下得到，其中T是时间的数量

    We study the problem of inferring sparse time-varying Markov random fields (MRFs) with different discrete and temporal regularizations on the parameters. Due to the intractability of discrete regularization, most approaches for solving this problem rely on the so-called maximum-likelihood estimation (MLE) with relaxed regularization, which neither results in ideal statistical properties nor scale to the dimensions encountered in realistic settings. In this paper, we address these challenges by departing from the MLE paradigm and resorting to a new class of constrained optimization problems with exact, discrete regularization to promote sparsity in the estimated parameters. Despite the nonconvex and discrete nature of our formulation, we show that it can be solved efficiently and parametrically for all sparsity levels. More specifically, we show that the entire solution path of the time-varying MRF for all sparsity levels can be obtained in $\mathcal{O}(pT^3)$, where $T$ is the number o
    
[^72]: mL-BFGS:一种基于动量的大规模分布式神经网络优化的L-BFGS算法

    mL-BFGS: A Momentum-based L-BFGS for Distributed Large-Scale Neural Network Optimization. (arXiv:2307.13744v1 [cs.LG])

    [http://arxiv.org/abs/2307.13744](http://arxiv.org/abs/2307.13744)

    mL-BFGS是一种轻量级的基于动量的L-BFGS算法，通过引入动量方案和减少Hessian中的随机噪声，稳定了大规模分布式深度神经网络的优化过程。

    

    鉴于Hessian相关计算中的额外计算成本和随机训练中的不稳定性问题，在训练大规模神经网络时，拟牛顿方法仍面临着重大挑战。已知的高效近似Hessian的L-BFGS方法，在随机训练中会出现收敛不稳定的问题。迄今为止，将L-BFGS适应于大规模随机训练的尝试带来了相当大的额外开销，这抵消了其在实际时间上的收敛优势。在本文中，我们提出了mL-BFGS算法，一种轻量级基于动量的L-BFGS算法，为大规模分布式深度神经网络(DNN)优化中的拟牛顿方法铺平了道路。mL-BFGS将近乎免费的动量方案引入到L-BFGS更新中，并大大减少Hessian中的随机噪声，从而在随机优化过程中稳定收敛。对于大规模模型训练，mL-BFGS使用块近似Hessian。

    Quasi-Newton methods still face significant challenges in training large-scale neural networks due to additional compute costs in the Hessian related computations and instability issues in stochastic training. A well-known method, L-BFGS that efficiently approximates the Hessian using history parameter and gradient changes, suffers convergence instability in stochastic training. So far, attempts that adapt L-BFGS to large-scale stochastic training incur considerable extra overhead, which offsets its convergence benefits in wall-clock time. In this paper, we propose mL-BFGS, a lightweight momentum-based L-BFGS algorithm that paves the way for quasi-Newton (QN) methods in large-scale distributed deep neural network (DNN) optimization. mL-BFGS introduces a nearly cost-free momentum scheme into L-BFGS update and greatly reduces stochastic noise in the Hessian, therefore stabilizing convergence during stochastic optimization. For model training at a large scale, mL-BFGS approximates a block
    
[^73]: FedDRL: 一种基于分阶段强化学习的可信联邦学习模型融合方法

    FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on Staged Reinforcement Learning. (arXiv:2307.13716v1 [cs.LG])

    [http://arxiv.org/abs/2307.13716](http://arxiv.org/abs/2307.13716)

    FedDRL是一种分阶段强化学习的联邦学习模型融合方法，解决了传统方法中无法解决的客户端模型质量和恶意模型问题。

    

    传统的联邦学习使用样本数量计算每个客户端模型的权重，并使用这个固定权重值来融合全局模型。然而，在实际场景中，每个客户端设备和数据的异质性导致每个客户端模型的质量存在差异。因此，对全局模型的贡献不仅仅取决于样本量。此外，如果客户端故意上传低质量或恶意模型，使用这些模型进行聚合将严重降低全局模型的准确性。传统的联邦学习算法没有解决这些问题。为了解决这个问题，我们提出了一种名为FedDRL的模型融合方法，它使用两个阶段的强化学习。在第一个阶段，我们的方法可以过滤掉恶意模型，并选择可信的客户端模型参与模型融合。在第二个阶段，FedDRL算法自适应地调整可信客户端模型的权重并聚合。

    Traditional federated learning uses the number of samples to calculate the weights of each client model and uses this fixed weight value to fusion the global model. However, in practical scenarios, each client's device and data heterogeneity leads to differences in the quality of each client's model. Thus the contribution to the global model is not wholly determined by the sample size. In addition, if clients intentionally upload low-quality or malicious models, using these models for aggregation will lead to a severe decrease in global model accuracy. Traditional federated learning algorithms do not address these issues. To solve this probelm, we propose FedDRL, a model fusion approach using reinforcement learning based on a two staged approach. In the first stage, Our method could filter out malicious models and selects trusted client models to participate in the model fusion. In the second stage, the FedDRL algorithm adaptively adjusts the weights of the trusted client models and ag
    
[^74]: 《2023年教练人工智能团队8参加CoachAI羽毛球挑战赛：用于击球预测的高级ShuttleNet》

    Team Intro to AI team8 at CoachAI Badminton Challenge 2023: Advanced ShuttleNet for Shot Predictions. (arXiv:2307.13715v1 [cs.LG])

    [http://arxiv.org/abs/2307.13715](http://arxiv.org/abs/2307.13715)

    本文提出了一个名为ShuttleNet的框架，通过利用过去的击球信息，显著改进了预测羽毛球击球类型和位置的性能，最终在CoachAI羽毛球挑战赛中获得第一名。

    

    本文的目标是通过利用过去的击球来改进现有框架ShuttleNet在预测羽毛球击球类型和位置方面的性能。我们参加了2023年IJCAI的CoachAI羽毛球挑战赛，并取得了显著优于基准线的成绩。最终，我们的团队在比赛中获得了第一名，并公开了我们的代码。

    In this paper, our objective is to improve the performance of the existing framework ShuttleNet in predicting badminton shot types and locations by leveraging past strokes. We participated in the CoachAI Badminton Challenge at IJCAI 2023 and achieved significantly better results compared to the baseline. Ultimately, our team achieved the first position in the competition and we made our code available.
    
[^75]: 深度布拉德利-特里评分：在没有具体评价标准的情况下估计物品的属性

    Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v1 [cs.LG])

    [http://arxiv.org/abs/2307.13709](http://arxiv.org/abs/2307.13709)

    本论文提出了深度布拉德利-特里评分（DBTR）方法，用于评估不一定存在于数据集中的未知物品的属性。该方法通过将传统的布拉德利-特里模型与神经网络结构无缝结合，成功地学习了这些属性的预期量化。

    

    在现实世界中，许多属性，如竞争环境中的可取性或强度，无法直接观测，这使得它们难以评估。为了解决这个具有挑战性的问题，先前的研究主要集中在估计已知物品的这些属性，特别是出现在配对比较数据集中的运动员的实力。在本文中，我们介绍了深度布拉德利-特里评分（DBTR），这是一个新颖的机器学习框架，用于评估不一定存在于数据集中的未知物品的任何属性。我们的方法无缝地将传统的布拉德利-特里模型与神经网络结构相结合。我们还进一步推广了这个架构，用于具有不公平性的非对称环境，这在现实世界中更为常见。在我们的实验分析中，DBTR成功地学习了这些属性的预期量化。

    Many properties in real world, such as desirability or strength in competitive environment, can't be directly observed, which makes them difficult to evaluate. To deal with this challenging problem, prior work has primarily focused on estimating those properties of known items, especially the strength of sports players, only of those who appears in paired comparison dataset. In this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework to evaluate any properties of unknown items, not necessarily present in dataset. Our method seamlessly integrates traditional Bradley-Terry model with a neural network structure. We also generalizes this architecture further for asymmetric environment with unfairness, which is much more common in real world settings. In our experimental analysis, DBTR successfully learned desired quantification of those properties.
    
[^76]: 控制和监控人工智能算法

    Control and Monitoring of Artificial Intelligence Algorithms. (arXiv:2307.13705v1 [cs.LG])

    [http://arxiv.org/abs/2307.13705](http://arxiv.org/abs/2307.13705)

    论文阐述了控制和监控人工智能算法的重要性，介绍了数据漂移和概念漂移的概念，并提出了一系列指标用于审查模型在潜在时间变化方面的性能。

    

    本文阐述了在部署后管理人工智能模型和监测当前数据分布与训练数据之间的潜在波动的重要性。详细解释了数据漂移和概念漂移的概念，以及它们各自的基本分布。此外，介绍了一系列可用于审查模型在潜在时间变化方面性能的指标。

    This paper elucidates the importance of governing an artificial intelligence model post-deployment and overseeing potential fluctuations in the distribution of present data in contrast to the training data. The concepts of data drift and concept drift are explicated, along with their respective foundational distributions. Furthermore, a range of metrics is introduced, which can be utilized to scrutinize the model's performance concerning potential temporal variations.
    
[^77]: 可解释人工智能（XAI）在年龄预测中的应用：一项系统综述

    eXplainable Artificial Intelligence (XAI) in age prediction: A systematic review. (arXiv:2307.13704v1 [cs.AI])

    [http://arxiv.org/abs/2307.13704](http://arxiv.org/abs/2307.13704)

    本综述探讨了可解释人工智能（XAI）在年龄预测任务中的应用。通过系统性综述，我们讨论了XAI方法在医疗应用和年龄预测领域的益处。

    

    可解释人工智能（XAI）现在是机器学习中的重要组成部分，能够解释复杂模型的预测结果。XAI特别适用于危险应用，特别是在医疗保健领域，人类的生命依赖于AI系统的决策。医疗研究的一个领域是年龄预测和衰老及与年龄相关疾病的生物标志物鉴定。然而，在年龄预测任务中，XAI的作用尚未直接探讨。在本综述中，我们讨论了XAI方法在年龄预测任务中的应用。我们通过器官系统进行了系统性综述，并讨论了XAI在医疗应用以及特别是年龄预测领域的益处。

    eXplainable Artificial Intelligence (XAI) is now an important and essential part of machine learning, allowing to explain the predictions of complex models. XAI is especially required in risky applications, particularly in health care, where human lives depend on the decisions of AI systems. One area of medical research is age prediction and identification of biomarkers of aging and age-related diseases. However, the role of XAI in the age prediction task has not previously been explored directly. In this review, we discuss the application of XAI approaches to age prediction tasks. We give a systematic review of the works organized by body systems, and discuss the benefits of XAI in medical applications and, in particular, in the age prediction domain.
    
[^78]: 测量链式思维推理中的忠诚度

    Measuring Faithfulness in Chain-of-Thought Reasoning. (arXiv:2307.13702v1 [cs.AI])

    [http://arxiv.org/abs/2307.13702](http://arxiv.org/abs/2307.13702)

    本研究探讨了在回答问题之前，大型语言模型（LLMs）能否进行忠实的“链式思维”推理。结果显示，模型在不同任务上对链式思维的依赖程度有很大变化，随着模型变得更大更能力越强，它们在大多数任务中产生的推理越来越不忠实。

    

    大型语言模型（LLMs）在回答问题之前，如果能够产生逐步的“链式思维”推理，其表现会更好，但不清楚所述的推理是否忠实地解释了模型实际的推理过程（即回答问题的方式）。我们通过检查当介入链式思维时模型预测如何发生变化（例如，添加错误或改写它），来研究链式思维可能不忠实的假设。模型在不同任务上在预测答案时对链式思维的依赖程度有很大变化，有时会严重依赖链式思维，而其他时候则主要忽视它。链式思维的性能提升似乎不仅仅来自于其增加的测试计算量，也不仅仅来自于链式思维的特定措辞所编码的信息。随着模型变得更大更有能力，它们在我们研究的大多数任务中产生的推理越来越不忠实。总体而言，我们的结果表明，如果环境适当，链式思维可以是忠实的。

    Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances s
    
[^79]: $\text{EFO}_{k}$-CQA：超越集合操作的知识图谱复杂查询回答

    $\text{EFO}_{k}$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation. (arXiv:2307.13701v1 [cs.AI])

    [http://arxiv.org/abs/2307.13701](http://arxiv.org/abs/2307.13701)

    本文提出了$\text{EFO}_{k}$-CQA框架，用于超越集合操作的知识图谱复杂查询回答。该框架包括数据生成、模型训练和方法评估，并且扩展了现有查询空间。使用构建的$\text{EFO}_{k}$-CQA数据集进行实证评估，结果揭示了查询难度对结果的影响。此外，还证明了现有数据集构建过程存在的问题。

    

    为了回答知识图谱上的复杂查询，需要对不完整知识进行逻辑推理，因为存在着开放世界的假设。基于学习的方法至关重要，因为它们能够对未观察到的知识进行泛化。因此，在这种范式下，一个合适的数据集对于获取和评估这样的方法都是基础。在本文中，我们提出了一个全面的框架，用于数据生成、模型训练和方法评估，涵盖了存在量化一阶查询与多个变量($\text{EFO}_{k}$)的组合空间。我们框架中的组合查询空间显著扩展了现有文献中通过集合操作定义的查询空间。此外，我们构建了一个包含741种查询类型的数据集$\text{EFO}_{k}$-CQA，以进行经验评估，我们的基准结果为理解查询难度如何影响结果提供了新的见解。此外，我们还证明了现有数据集构建过程的系统性问题。

    To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\text{EFO}_{k}$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\text{EFO}_{k}$-CQA, with 741 types of query for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systema
    
[^80]: CAMP:一种上下文感知的板球球员表现指标

    CAMP: A Context-Aware Cricket Players Performance Metric. (arXiv:2307.13700v1 [cs.AI])

    [http://arxiv.org/abs/2307.13700](http://arxiv.org/abs/2307.13700)

    CAMP是一种上下文感知的板球球员表现指标，通过综合考虑对手实力和比赛环境等因素，可以量化个体球员对比赛结果的贡献。CAMP通过数据挖掘方法，为选择和草案、教练和训练、团队阵容和战略制定提供数据驱动的决策支持。

    

    板球是仅次于足球在收视率上最受欢迎的运动。然而，对个体球员表现的评估，作为团队运动的基本任务，目前主要基于综合表现数据，包括平均得分和击球次数。我们提出了一种称为CAMP的上下文感知球员表现指标，用于衡量个体球员对板球比赛结果的贡献。CAMP采用数据挖掘方法，并为选择和草案、教练和训练、团队阵容和战略制定提供了有效的数据驱动决策。CAMP纳入了表现的确切上下文，例如对手的实力和比赛的特定环境，例如压力情况。我们在2001年至2019年间的有限过度板球比赛数据上进行了实证评估。在每场比赛中，一组专家宣布一名球员为最佳球员，被称为M}atch的最佳球员（MoM）。通过CAMP评估，最高评分的两名球员与MoM匹配的概率为83％。

    Cricket is the second most popular sport after soccer in terms of viewership. However, the assessment of individual player performance, a fundamental task in team sports, is currently primarily based on aggregate performance statistics, including average runs and wickets taken. We propose Context-Aware Metric of player Performance, CAMP, to quantify individual players' contributions toward a cricket match outcome. CAMP employs data mining methods and enables effective data-driven decision-making for selection and drafting, coaching and training, team line-ups, and strategy development. CAMP incorporates the exact context of performance, such as opponents' strengths and specific circumstances of games, such as pressure situations. We empirically evaluate CAMP on data of limited-over cricket matches between 2001 and 2019. In every match, a committee of experts declares one player as the best player, called Man of the M}atch (MoM). The top two rated players by CAMP match with MoM in 83\% 
    
[^81]: 使用可解释方法探索彩票票据假说：对稀疏网络性能的洞察

    Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance. (arXiv:2307.13698v1 [cs.CV])

    [http://arxiv.org/abs/2307.13698](http://arxiv.org/abs/2307.13698)

    本研究通过使用可解释性方法，探究了彩票票据假说在稀疏网络性能方面的洞察，发现修剪的网络性能会降低，并且修剪后的网络产生的概念和像素与原始网络存在不一致性。

    

    在具有有限存储能力的设备上部署高性能的稀疏网络对于如手机等设备非常有利。此外，模型的可解释性对于培养对人工智能的信任至关重要。彩票票据假说（LTH）是在深度网络中找到一个与原模型相当或更优的网络。然而，鲜有研究探究LTH在可解释性方面的成败。本研究探究了修剪的网络性能逐渐提高或降低的原因。我们分别使用Grad-CAM和后期概念瓶颈模型（PCBM）来考察修剪网络在像素和高级概念方面的可解释性。我们在视觉和医学图像数据集上进行了大量实验。随着权重修剪的增多，网络性能会下降。从修剪的网络中发现的概念和像素与原始网络的不一致。

    Discovering a high-performing sparse network within a massive neural network is advantageous for deploying them on devices with limited storage, such as mobile phones. Additionally, model explainability is essential to fostering trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep network with comparable or superior performance to the original model. However, limited study has been conducted on the success or failure of LTH in terms of explainability. In this work, we examine why the performance of the pruned networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept bottleneck models (PCBMs), respectively, we investigate the explainability of pruned networks in terms of pixels and high-level concepts. We perform extensive experiments across vision and medical imaging datasets. As more weights are pruned, the performance of the network degrades. The discovered concepts and pixels from the pruned networks are inconsistent with the original n
    
[^82]: Duet: 高效且可扩展的混合神经关系理解

    Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v1 [cs.DB])

    [http://arxiv.org/abs/2307.13494](http://arxiv.org/abs/2307.13494)

    Duet是一种高效且可扩展的混合神经关系理解方法，旨在解决基数估计问题中高成本和难以区分的采样方法，并通过可微分的预测过程改进模型的准确性。

    

    基于概率分布估计的基数估计方法相较于传统方法取得了高精度的估计结果。然而，最先进的方法由于在处理范围查询时使用的采样方法而导致估计成本较高。此外，这种采样方法也使得它们难以区分，因此来自查询工作负载的监督信号很难训练模型以提高基数估计的准确性。在本文中，我们提出了一种新的混合确定性建模方法（Duet）用于基数估计问题，与以前的方法相比，具有更好的效率和可扩展性。Duet可以以更低的时间和内存成本直接估计范围查询的基数，并且以可区分的形式呈现。由于此方法的预测过程是可微分的，我们可以将估计误差较大的查询纳入训练过程以进行改进。

    Cardinality estimation methods based on probability distribution estimation have achieved high-precision estimation results compared to traditional methods. However, the most advanced methods suffer from high estimation costs due to the sampling method they use when dealing with range queries. Also, such a sampling method makes them difficult to differentiate, so the supervision signal from the query workload is difficult to train the model to improve the accuracy of cardinality estimation. In this paper, we propose a new hybrid and deterministic modeling approach (Duet) for the cardinality estimation problem which has better efficiency and scalability compared to previous approaches. Duet allows for direct cardinality estimation of range queries with significantly lower time and memory costs, as well as in a differentiable form. As the prediction process of this approach is differentiable, we can incorporate queries with larger model estimation errors into the training process to addr
    
[^83]: 关于注意力网络学习动态的研究

    On the learning Dynamics of Attention Networks. (arXiv:2307.13421v1 [cs.LG])

    [http://arxiv.org/abs/2307.13421](http://arxiv.org/abs/2307.13421)

    本研究分析了软注意力、硬注意力和潜变量边际似然（LVML）注意力三种注意力模型的学习动态，发现了它们在所选择的片段聚合方式上的显著差异，并解释了分类模型在梯度下降下的演化对最终结果的影响。

    

    注意力模型通常通过优化三个标准损失函数之一来学习，分别称为软注意力、硬注意力和潜变量边际似然（LVML）注意力。这三种范式都是为了达到相同的目标，即找到两个模型：一个“焦点”模型，用于“选择”输入中的正确“片段”，和一个“分类”模型，用于将选定的片段处理成目标标签。然而，它们在所选择的片段聚合方式上存在显著差异，导致了不同的动态和最终结果。我们观察到使用这些范式学习的模型具有独特的特征，并将其解释为在焦点模型固定时，分类模型在梯度下降下的演化所致。我们还在一个简单的设置中分析了这些范式，并推导出梯度流下参数轨迹的闭式表达式。在软注意力损失下，焦点模型在初始化阶段快速改善。

    Attention models are typically learned by optimizing one of three standard loss functions that are variously called -- soft attention, hard attention, and latent variable marginal likelihood (LVML) attention. All three paradigms are motivated by the same goal of finding two models -- a `focus' model that `selects' the right \textit{segment} of the input and a `classification' model that processes the selected segment into the target label. However, they differ significantly in the way the selected segments are aggregated, resulting in distinct dynamics and final results. We observe a unique signature of models learned using these paradigms and explain this as a consequence of the evolution of the classification model under gradient descent when the focus model is fixed. We also analyze these paradigms in a simple setting and derive closed-form expressions for the parameter trajectory under gradient flow. With the soft attention loss, the focus model improves quickly at initialization a
    
[^84]: 学习压缩（LtC）：高效的基于学习的流媒体视频分析

    Learn to Compress (LtC): Efficient Learning-based Streaming Video Analytics. (arXiv:2307.12171v1 [eess.IV])

    [http://arxiv.org/abs/2307.12171](http://arxiv.org/abs/2307.12171)

    本文介绍了一种名为LtC的协作框架，可以通过学习来有效减小流媒体视频的流量，提高流媒体视频分析的效率。

    

    视频分析通常作为边缘设置中的云服务进行，主要是为了卸载计算负载，并且在视频传感器不直接使用结果的情况下进行。从边缘设备发送高质量的视频数据在带宽和功耗方面都会很昂贵。为了构建一个高效利用这些资源的流媒体视频分析流水线，因此有必要减小视频流的大小。传统的视频压缩算法对视频的语义不敏感，可能既低效又对分析性能有害。在本文中，我们介绍了LtC，这是视频源和分析服务器之间的协作框架，它通过在分析流水线中高效地学习来减小视频流。具体而言，LtC将服务器上的全功能分析算法作为教师，训练一个轻量级的学生神经网络，然后部署在视频源上。

    Video analytics are often performed as cloud services in edge settings, mainly to offload computation, and also in situations where the results are not directly consumed at the video sensors. Sending high-quality video data from the edge devices can be expensive both in terms of bandwidth and power use. In order to build a streaming video analytics pipeline that makes efficient use of these resources, it is therefore imperative to reduce the size of the video stream. Traditional video compression algorithms are unaware of the semantics of the video, and can be both inefficient and harmful for the analytics performance. In this paper, we introduce LtC, a collaborative framework between the video source and the analytics server, that efficiently learns to reduce the video streams within an analytics pipeline. Specifically, LtC uses the full-fledged analytics algorithm at the server as a teacher to train a lightweight student neural network, which is then deployed at the video source. The
    
[^85]: AdjointDPM: 扩散概率模型梯度反向传播的伴随灵敏度方法

    AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v1 [cs.CV])

    [http://arxiv.org/abs/2307.10711](http://arxiv.org/abs/2307.10711)

    AdjointDPM是一种新的伴随灵敏度方法，用于扩散概率模型的梯度反向传播，解决了DPM定制化中内存消耗高的问题，并通过解决增强的ODE将损失的梯度反向传播到模型的参数。

    

    现有的定制化方法需要多个参考样例来将预训练的扩散概率模型(DPMs)与用户提供的概念对齐。本文旨在解决当唯一可用的监督是定义在生成内容上的可微度量时的DPM定制化挑战。由于DPM的采样过程涉及对去噪UNet的递归调用，朴素的梯度反向传播需要存储所有迭代的中间状态，导致内存消耗极高。为了解决这个问题，我们提出了一种新的方法AdjointDPM，首先通过求解相应的概率流ODE从扩散模型中生成新样本。然后使用伴随灵敏度方法通过求解另一个增强的ODE将损失的梯度反向传播到模型的参数(包括调制信号、网络权重和初始噪声)。为了减少正向生成和反向传播中的数值误差

    Existing customization methods require access to multiple reference examples to align pre-trained diffusion probabilistic models (DPMs) with user-provided concepts. This paper aims to address the challenge of DPM customization when the only available supervision is a differentiable metric defined on the generated contents. Since the sampling procedure of DPMs involves recursive calls to the denoising UNet, na\"ive gradient backpropagation requires storing the intermediate states of all iterations, resulting in extremely high memory consumption. To overcome this issue, we propose a novel method AdjointDPM, which first generates new samples from diffusion models by solving the corresponding probability-flow ODEs. It then uses the adjoint sensitivity method to backpropagate the gradients of the loss to the models' parameters (including conditioning signals, network weights, and initial noises) by solving another augmented ODE. To reduce numerical errors in both the forward generation and 
    
[^86]: 欺骗性对齐监测

    Deceptive Alignment Monitoring. (arXiv:2307.10569v1 [cs.LG])

    [http://arxiv.org/abs/2307.10569](http://arxiv.org/abs/2307.10569)

    本论文提出了欺骗性对齐监测这一新方向，旨在探讨大型机器学习模型在表面上表现正常，却暗中进行隐藏行为的问题，并提出了新的研究机会。

    

    随着大型机器学习模型的能力不断增长，以及对这些模型的自治权不断扩大，一个新的对手出现了：模型本身。一个模型看似合理地行为，却暗中、微妙地修改其行为以达到别的目的的威胁，通常在AI安全与对齐社区中被称为欺骗性对齐。因此，我们将这个新方向称为欺骗性对齐监测。在这项工作中，我们确定了机器学习不同子领域中的新兴方向，我们认为在不久的将来对欺骗性对齐监测会变得越来越重要且紧密相关，并且我们认为这些领域的进步既提出了长期挑战，也带来了新的研究机会。最后，我们呼吁对抗性机器学习社区更多地参与这些新兴方向的研究。

    As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety & Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.
    
[^87]: TimeTuner: 诊断时间序列预测中的时间表示的对照解释

    TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations. (arXiv:2307.09916v1 [cs.HC])

    [http://arxiv.org/abs/2307.09916](http://arxiv.org/abs/2307.09916)

    TimeTuner是一个新颖的可视化分析框架，旨在帮助分析人员理解时间序列预测中模型行为与时间表示的关系，并解决自动化特征学习方法的局限性。

    

    深度学习方法在时间序列预测中的应用越来越多，许多努力致力于设计复杂的深度学习模型。最近的研究表明，深度学习的成功往往归因于有效的数据表示，促进了特征工程和表示学习领域的发展。然而，自动化特征学习方法通常在融入先验知识、识别变量间相互作用和选择评估指标以确保模型可靠性方面存在局限性。为了改善这些限制，本文提出了一种新颖的可视化分析框架，即TimeTuner，旨在帮助分析人员理解模型行为与时间序列表示的局部相关性、平稳性和粒度之间的关联。该系统主要包括以下两个阶段技术：我们首先利用对照解释来建立时间序列表示之间的关系，

    Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations,
    
[^88]: 对机器学习应用中高保真度等离子体模拟中磁场拓扑的图表示

    Graph Representation of the Magnetic Field Topology in High-Fidelity Plasma Simulations for Machine Learning Applications. (arXiv:2307.09469v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2307.09469](http://arxiv.org/abs/2307.09469)

    该论文提出了一种对高保真度等离子体模拟中的磁场拓扑进行图表示的方法，并应用于地球磁层模拟中，旨在挑战机器学习社区通过图机器学习方法解决具有广泛潜力影响的科学问题。

    

    通过对模拟等离子体中的磁场进行拓扑分析，可以在各种不同的场景下研究多种物理现象。其中一个应用是磁重新连接，这是与磁场拓扑动力学相关的一种现象，在三维空间中很难检测和描述。我们提出了一个可扩展的流程，用于对三维磁场矢量场进行拓扑数据分析和时空图表示。我们在地球磁层的模拟中演示了我们的方法，这是由Vlasiator产生的，Vlasiator是一个基于Vlasov理论的超级计算机规模的近地空间模拟。这项工作的目的是挑战机器学习社区采用基于图的机器学习方法来解决一个在科学上具有广泛潜力影响的问题。

    Topological analysis of the magnetic field in simulated plasmas allows the study of various physical phenomena in a wide range of settings. One such application is magnetic reconnection, a phenomenon related to the dynamics of the magnetic field topology, which is difficult to detect and characterize in three dimensions. We propose a scalable pipeline for topological data analysis and spatiotemporal graph representation of three-dimensional magnetic vector fields. We demonstrate our methods on simulations of the Earth's magnetosphere produced by Vlasiator, a supercomputer-scale Vlasov theory-based simulation for near-Earth space. The purpose of this work is to challenge the machine learning community to explore graph-based machine learning approaches to address a largely open scientific problem with wide-ranging potential impact.
    
[^89]: 没有训练就没有收益：重新审视基于Transformer的语言模型的高效训练算法

    No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])

    [http://arxiv.org/abs/2307.06440](http://arxiv.org/abs/2307.06440)

    本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。

    

    近年来，训练Transformer-based语言模型所需的计算量急剧增加。这一趋势促使研究者们开展了针对高效训练算法的研究，旨在比标准训练更快地改善训练、验证和下游性能。在这项工作中，我们重新审视了三类这样的算法：动态架构（层叠、层丢弃）、批量选择（选择性反向传播、RHO损失）和高效优化器（Lion、Sophia）。当使用这些方法在固定计算预算下对BERT和T5进行预训练时，我们发现它们的训练、验证和下游收益相对于一个具有完全衰减学习率的基线而言会消失。我们定义了一个评估协议，可以通过将所有计算时间映射到一个称为参考系统时间的参考机器上，在任意机器上进行计算。我们讨论了我们提出的协议的局限性，并发布了我们的代码，以鼓励对高效训练的严格研究。

    The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
    
[^90]: Prompt Generate Train (PGT): 一种用于领域特定开放式问题回答的检索增强生成模型的少样本领域适应、对齐和不确定性校准框架

    Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering. (arXiv:2307.05915v1 [cs.LG])

    [http://arxiv.org/abs/2307.05915](http://arxiv.org/abs/2307.05915)

    提出了一个名为Prompt Generate Train (PGT)的框架，用于少样本领域适应、对齐和不确定性校准的检索增强生成模型的开发。该框架通过有监督的微调和强化学习，将模型适应到目标领域，并在生成相关答案方面具有与基于GPT-4的模型相竞争的性能。

    

    我们提出了一种名为 Prompt, Generate, Train (PGT) 的框架，用于高效地开发一个针对专有的文本文档集合进行开放式问题回答的生成模型。该框架通过有监督的微调和强化学习，在少样本的情况下将检索增强的生成模型适应到目标领域。这产生了一个对齐、不确定性校准的模型，在生成相关答案时具有与基于 GPT-4 的上下文检索增强生成模型相竞争的性能，并且服务成本更低。通过使用中等规模的LLM (Flan-T5 XXL) 和一种新颖的一致性过滤方案，合成生成管道生成高质量合成训练数据。该管道旨在生成涵盖整个语料库的抽象和提取式问题。使用该数据集的样本，该框架对一个由稠密检索器和较小规模的LLM组成的较小的RAG模型进行微调。

    We present a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation model to the target domain using supervised finetuning and reinforcement learning with synthetic feedback in a few-shot setting. This yields an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The synthetic generation pipeline generates high quality synthetic training data musing a medium sized LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is designed to generate both abstractive and extractive questions that span the entire corpus. Using samples from this dataset, the framework fine-tunes a smaller RAG model comprising a dense retriever and a smaller sized LLM on samples from the dataset.
    
[^91]: 流形滤波-组合网络

    Manifold Filter-Combine Networks. (arXiv:2307.04056v1 [stat.ML])

    [http://arxiv.org/abs/2307.04056](http://arxiv.org/abs/2307.04056)

    这篇论文介绍了一类称为流形滤波-组合网络的大型流形神经网络。作者提出了一种基于构建数据驱动图的方法来实现这种网络，并提供了收敛到连续极限的充分条件，其收敛速度不依赖于滤波器数量。

    

    我们介绍了一类大型流形神经网络(MNNs)，我们称之为流形滤波-组合网络。这个类别包括了Wang、Ruiz和Ribeiro之前的研究中考虑的MNNs，流形散射变换(一种基于小波的神经网络模型)，以及其他有趣的之前在文献中未考虑的示例，如Kipf和Welling的图卷积网络的流形等效。然后，我们考虑了一种基于构建数据驱动图的方法，用于在没有对流形有全局知识的情况下实现这样的网络，而只能访问有限数量的样本点。我们提供了网络在样本点数趋于无穷大时能够保证收敛到其连续极限的充分条件。与之前的工作(主要关注特定的MNN结构和图构建)不同，我们的收敛速度并不依赖于使用的滤波器数量。而且，它表现出线性的收敛速度。

    We introduce a large class of manifold neural networks (MNNs) which we call Manifold Filter-Combine Networks. This class includes as special cases, the MNNs considered in previous work by Wang, Ruiz, and Ribeiro, the manifold scattering transform (a wavelet-based model of neural networks), and other interesting examples not previously considered in the literature such as the manifold equivalent of Kipf and Welling's graph convolutional network. We then consider a method, based on building a data-driven graph, for implementing such networks when one does not have global knowledge of the manifold, but merely has access to finitely many sample points. We provide sufficient conditions for the network to provably converge to its continuum limit as the number of sample points tends to infinity. Unlike previous work (which focused on specific MNN architectures and graph constructions), our rate of convergence does not explicitly depend on the number of filters used. Moreover, it exhibits line
    
[^92]: FedNoisy: 分布式噪声标签学习基准测试

    FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11650](http://arxiv.org/abs/2306.11650)

    FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。

    

    联合学习已经因为无需对来自客户端的敏感数据进行聚合而变得受欢迎。但是，数据隔离的分布式和孤立性可能会受到数据质量的复杂性的影响，使其更容易受到噪声标签的干扰。许多努力都致力于在集中式或联合式环境中防御噪声标签的负面影响。然而，缺乏一个全面考虑各种典型联合学习场景中噪声标签影响的基准测试。在这项工作中，我们提供了第一个标准化的基准测试，可以帮助研究人员充分探索潜在的联合噪声设置。此外，我们进行了全面的实验，探索这些数据设置的特性，并揭示了联合学习中的挑战性场景，这可能指导未来的方法开发。我们强调我们基准测试中提出的20个基本设置，适用于5个以上的数据集，并提供了标准化的仿真流程。

    Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
    
[^93]: 信息瓶颈的常微分方程：基于一阶根轨迹的信息瓶颈研究

    The Information Bottleneck's Ordinary Differential Equation: First-Order Root-Tracking for the IB. (arXiv:2306.09790v1 [cs.IT])

    [http://arxiv.org/abs/2306.09790](http://arxiv.org/abs/2306.09790)

    本文重新导出了信息瓶颈的一阶ODR，描述了其最优权衡曲线的动态，揭示了最优编码的动态过程。

    

    信息瓶颈是一种有损压缩方法，其速率-失真曲线描述了输入压缩和相关信息保留之间的基本权衡。然而，它掩盖了最优输入编码的基本动态。我们认为，随着输入信息的压缩，这些动态通常遵循分段光滑的轨迹，如最近在RD中所示。这些光滑的动态在最优编码发生定性变化（在分叉处）时会中断。通过利用信息瓶颈与RD的密切关系，可以看到次优解在那里发生碰撞或交换最优性。尽管信息瓶颈及其应用已被接受，但惊人地缺乏解决该问题的数值技术，即使对于已知分布的有限问题。我们重新导出了信息瓶颈的一阶常微分方程，描述了其最优权衡曲线的基本动态。

    The Information Bottleneck (IB) is a method of lossy compression. Its rate-distortion (RD) curve describes the fundamental tradeoff between input compression and the preservation of relevant information. However, it conceals the underlying dynamics of optimal input encodings. We argue that these typically follow a piecewise smooth trajectory as the input information is being compressed, as recently shown in RD. These smooth dynamics are interrupted when an optimal encoding changes qualitatively, at a bifurcation. By leveraging the IB's intimate relations with RD, sub-optimal solutions can be seen to collide or exchange optimality there.  Despite the acceptance of the IB and its applications, there are surprisingly few techniques to solve it numerically, even for finite problems whose distribution is known. We derive anew the IB's first-order Ordinary Differential Equation, which describes the dynamics underlying its optimal tradeoff curve. To exploit these dynamics, one needs not only 
    
[^94]: 预训练的视觉与语言模型中的实体知识探究的表格和图像生成

    Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models. (arXiv:2306.02115v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02115](http://arxiv.org/abs/2306.02115)

    本文提出了一个表格和图像生成的任务，研究了预训练的视觉与语言模型中关于实体的知识如何被保留。通过实验结果发现，预训练模型OFA在生成图像的过程中忘记了部分实体知识。

    

    本文提出了一个表格和图像生成任务，以验证自然语言中获取的关于实体的知识如何被保留在视觉与语言（V&L）模型中。该任务由两个部分组成：第一个部分是生成一个包含关于实体及其相关图像的知识的表格，第二个部分是根据实体、标题和包含相关实体知识的表格生成图像。在两个任务中，模型必须正确地了解用于执行生成的实体。我们从英文维基百科文章中的约200,000个信息框创建了维基百科表格和图像生成（WikiTIG）数据集来执行提出的任务。我们使用在多个任务中取得了最先进结果的V&L模型OFA对任务的表现进行了评估。实验结果显示，OFA在预训练过程中忘记了部分实体知识，这对于提高图像相关性能是一种补充。

    In this paper, we propose a table and image generation task to verify how the knowledge about entities acquired from natural language is retained in Vision & Language (V&L) models. This task consists of two parts: the first is to generate a table containing knowledge about an entity and its related image, and the second is to generate an image from an entity with a caption and a table containing related knowledge of the entity. In both tasks, the model must know the entities used to perform the generation properly. We created the Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000 infoboxes in English Wikipedia articles to perform the proposed tasks. We evaluated the performance on the tasks with respect to the above research question using the V&L model OFA, which has achieved state-of-the-art results in multiple tasks. Experimental results show that OFA forgets part of its entity knowledge by pre-training as a complement to improve the performance of image relat
    
[^95]: SimFBO：简单、灵活且通信高效的联邦双层学习

    SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v1 [cs.LG])

    [http://arxiv.org/abs/2305.19442](http://arxiv.org/abs/2305.19442)

    SimFBO和其ShroFBO变体提出了一个简单、灵活且通信高效的FBO框架，可以应用于元学习和超参数优化任务。

    

    近来，由于元学习、微调、超参数调整等领域中嵌套优化结构的出现，联邦双层优化（FBO）在机器学习和边缘计算中显示了巨大的潜力。然而，现有的FBO算法往往涉及复杂的计算，并需要每次迭代多个子循环，每个子循环包含多个通信轮。在本文中，我们提出了一个名为SimFBO的简单灵活的FBO框架，它易于实现，不需要子循环，并包括一种广义的服务器端聚合和更新以提高通信效率。我们进一步提出了系统级异构鲁棒FBO（ShroFBO）作为SimFBO的变体，其对本地计算的异构有更强的鲁棒性。我们证明了在部分客户端参与和无替换的客户端采样下，SimFBO和ShroFBO可以实现线性收敛加速，同时改进了样本和通信复杂度。实验证明了它们在图像分类数据集的元学习和真实世界数据集上的超参数优化任务中的有效性。

    Federated bilevel optimization (FBO) has shown great potential recently in machine learning and edge computing due to the emerging nested optimization structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However, existing FBO algorithms often involve complicated computations and require multiple sub-loops per iteration, each of which contains a number of communication rounds. In this paper, we propose a simple and flexible FBO framework named SimFBO, which is easy to implement without sub-loops, and includes a generalized server-side aggregation and update for improving communication efficiency. We further propose System-level heterogeneity robust FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous local computation. We show that SimFBO and ShroFBO provably achieve a linear convergence speedup with partial client participation and client sampling without replacement, as well as improved sample and communication complexities. Experiments demons
    
[^96]: 探索长尾识别问题中的权重平衡

    Exploring Weight Balancing on Long-Tailed Recognition Problem. (arXiv:2305.16573v1 [cs.LG])

    [http://arxiv.org/abs/2305.16573](http://arxiv.org/abs/2305.16573)

    研究分析了新提出的权重平衡方法在长尾识别问题中有效的原因，发现其能够缓解神经崩溃和圆锥效应，从而提高识别性能。

    

    长尾数据中的识别问题最近变得越来越重要，因为数据集中每个类别的样本数量分布通常是指数分布，除非有意地调整样本数量。针对这些问题已经提出了各种方法。最近，提出了权重平衡方法，它结合了著名的经典正则化技术和两阶段训练。尽管其简单性，但已知其对现有各种不同方法具有高性能。然而，我们缺乏为什么这种方法对长尾数据有效的理解。在这项研究中，我们分析了该方法，并关注了神经崩溃和每个训练阶段的圆锥效应，并发现它可以分解为由权值衰减和交叉熵损失引起的特征提取器中Fisher判别比的增加以及由权重衰减和类平衡正则化引起的隐式逻辑调整。我们还证明了权重平衡方法成功缓解了神经崩溃和圆锥效应，从而提高了长尾数据的识别性能。

    Recognition problems in long-tailed data, where the sample size per class is heavily skewed, have recently gained importance because the distribution of the sample size per class in a dataset is generally exponential unless the sample size is intentionally adjusted. Various approaches have been devised to address these problems. Recently, weight balancing, which combines well-known classical regularization techniques with two-stage training, has been proposed. Despite its simplicity, it is known for its high performance against existing methods devised in various ways. However, there is a lack of understanding as to why this approach is effective for long-tailed data. In this study, we analyze the method focusing on neural collapse and cone effect at each training stage and find that it can be decomposed into the increase in Fisher's discriminant ratio of the feature extractor caused by weight decay and cross entropy loss and implicit logit adjustment caused by weight decay and class-b
    
[^97]: 高产农田检测：一个新的数据集和深度学习基准结果

    Productive Crop Field Detection: A New Dataset and Deep Learning Benchmark Results. (arXiv:2305.11990v1 [cs.CV])

    [http://arxiv.org/abs/2305.11990](http://arxiv.org/abs/2305.11990)

    本研究提出了一个高质量的数据集，使用半监督和最先进的深度学习方法自动检测高产农田，获得了很高的准确性，有望为农民提供帮助.

    

    在精准农业中，检测高产农田是一项必要的实践，使得农民可以单独评估操作绩效并比较不同的种子品种、农药和肥料。然而，手动识别高产农田往往是一项耗时且容易出错的任务。以往的研究尝试使用先进的机器学习算法检测农田，但往往缺乏高质量的标记数据。在这种情况下，我们提出了一个高质量的数据集，它是通过机器操作结合随着时间推移而跟踪的Sentinel-2图像生成的。据我们所知，这是第一个通过使用这种技术克服标记样本不足的数据集。接着，我们应用半监督无标签数据分类和最先进的有监督和自监督深度学习方法来自动检测高产农田。最终结果表明在正无标记学习中具有很高的准确性，这非常适合

    In precision agriculture, detecting productive crop fields is an essential practice that allows the farmer to evaluate operating performance separately and compare different seed varieties, pesticides, and fertilizers. However, manually identifying productive fields is often a time-consuming and error-prone task. Previous studies explore different methods to detect crop fields using advanced machine learning algorithms, but they often lack good quality labeled data. In this context, we propose a high-quality dataset generated by machine operation combined with Sentinel-2 images tracked over time. As far as we know, it is the first one to overcome the lack of labeled samples by using this technique. In sequence, we apply a semi-supervised classification of unlabeled data and state-of-the-art supervised and self-supervised deep learning methods to detect productive crop fields automatically. Finally, the results demonstrate high accuracy in Positive Unlabeled learning, which perfectly fi
    
[^98]: 通过连续方式隐式优化的政策梯度算法

    Policy Gradient Algorithms Implicitly Optimize by Continuation. (arXiv:2305.06851v1 [cs.LG])

    [http://arxiv.org/abs/2305.06851](http://arxiv.org/abs/2305.06851)

    本文提供了政策梯度算法的新理论解释和证明，即政策梯度算法可以通过连续方式隐式优化确定性策略，并指出政策梯度算法探索的实质是计算当前策略收益的连续函数，策略的方差应该是历史依赖性函数。

    

    强化学习中的直接策略优化通常通过政策梯度算法解决，该算法通过随机梯度上升优化策略参数。本文提供了一种新的理论解释和证明这些算法的方法。首先，我们将直接策略优化问题建立在优化连续框架下。后者是一种用于优化非凸函数的框架，其中以连续的替代目标函数序列为基础。其次，我们证明了优化仿射高斯策略并执行熵正则化可以解释为通过连续隐式地优化确定性策略。基于这些理论结果，我们认为政策梯度算法中的探索包括计算当前的策略收益的连续函数，策略的方差应该是历史依赖性函数，以避免局部最值而不是仅仅最大化政策的收益。

    Direct policy optimization in reinforcement learning is usually solved with policy-gradient algorithms, which optimize policy parameters via stochastic gradient ascent. This paper provides a new theoretical interpretation and justification of these algorithms. First, we formulate direct policy optimization in the optimization by continuation framework. The latter is a framework for optimizing nonconvex functions where a sequence of surrogate objective functions, called continuations, are locally optimized. Second, we show that optimizing affine Gaussian policies and performing entropy regularization can be interpreted as implicitly optimizing deterministic policies by continuation. Based on these theoretical results, we argue that exploration in policy-gradient algorithms consists in computing a continuation of the return of the policy at hand, and that the variance of policies should be history-dependent functions adapted to avoid local extrema rather than to maximize the return of th
    
[^99]: Kullback-Leibler Maillard采样在有界奖励的多臂赌博机问题中的应用

    Kullback-Leibler Maillard Sampling for Multi-armed Bandits with Bounded Rewards. (arXiv:2304.14989v1 [cs.LG])

    [http://arxiv.org/abs/2304.14989](http://arxiv.org/abs/2304.14989)

    本文提出了Kullback-Leibler Maillard Sampling (KL-MS)算法，能够在有界奖励的多臂赌博机中实现KL空间的扩展，具有较好的渐近性能。

    

    本文研究了奖励分布集中在区间$[0,1]$内的$K$臂数臂赌博机问题。本文提出了一种名为Kullback-Leibler Maillard Sampling (KL-MS)的新算法，它是Maillard采样在KL空间的自然扩展。实验表明，KL-MS在Bernoulli奖励时具有渐近最优性能，其最坏情况遗憾度上界为$O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$，其中$\mu^*$是最优臂的期望奖励，$T$是时段长度。

    We study $K$-armed bandit problems where the reward distributions of the arms are all supported on the $[0,1]$ interval. It has been a challenge to design regret-efficient randomized exploration algorithms in this setting. Maillard sampling~\cite{maillard13apprentissage}, an attractive alternative to Thompson sampling, has recently been shown to achieve competitive regret guarantees in the sub-Gaussian reward setting~\cite{bian2022maillard} while maintaining closed-form action probabilities, which is useful for offline policy evaluation. In this work, we propose the Kullback-Leibler Maillard Sampling (KL-MS) algorithm, a natural extension of Maillard sampling for achieving KL-style gap-dependent regret bound. We show that KL-MS enjoys the asymptotic optimality when the rewards are Bernoulli and has a worst-case regret bound of the form $O(\sqrt{\mu^*(1-\mu^*) K T \ln K} + K \ln T)$, where $\mu^*$ is the expected reward of the optimal arm, and $T$ is the time horizon length.
    
[^100]: 数据增强中基于不确定性的轨迹截断在离线强化学习中的应用

    Uncertainty-driven Trajectory Truncation for Data Augmentation in Offline Reinforcement Learning. (arXiv:2304.04660v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.04660](http://arxiv.org/abs/2304.04660)

    本研究提出了一种基于不确定性的轨迹截断方法（TATU），用于解决模型驱动的离线强化学习中生成样本不可靠的问题。实验证明TATU相较于其他方法表现优越。

    

    模型驱动的离线强化学习算法通常能够从固定大小的数据集中学到好的策略，甚至一些质量较差的数据集。然而，生成的合成样本是否可靠无法得到保证（例如，一些合成样本可能位于静态数据集支持区域之外）。为了解决这个问题，我们提出了具有不确定性的轨迹截断（TATU）方法，如果累积不确定性超过阈值，则自适应截断合成轨迹。我们理论上证明了TATU的性能边界以证明其优势。为了在实证上显示出TATU的优势，我们首先将其与两个经典的模型驱动离线强化学习算法MOPO和COMBO相结合。此外，我们还将TATU与多个现成的无模型离线强化学习算法（例如BCQ）进行整合。在D4RL基准测试上的实验结果表明，TATU的表现优越。

    Equipped with the trained environmental dynamics, model-based offline reinforcement learning (RL) algorithms can often successfully learn good policies from fixed-sized datasets, even some datasets with poor quality. Unfortunately, however, it can not be guaranteed that the generated samples from the trained dynamics model are reliable (e.g., some synthetic samples may lie outside of the support region of the static dataset). To address this issue, we propose Trajectory Truncation with Uncertainty (TATU), which adaptively truncates the synthetic trajectory if the accumulated uncertainty along the trajectory is too large. We theoretically show the performance bound of TATU to justify its benefits. To empirically show the advantages of TATU, we first combine it with two classical model-based offline RL algorithms, MOPO and COMBO. Furthermore, we integrate TATU with several off-the-shelf model-free offline RL algorithms, e.g., BCQ. Experimental results on the D4RL benchmark show that TATU
    
[^101]: 被忽视的免费午餐——使用注释副产品学习图像分类器

    Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts. (arXiv:2303.17595v1 [cs.CV])

    [http://arxiv.org/abs/2303.17595](http://arxiv.org/abs/2303.17595)

    本研究指出传统的图片分类器学习过程忽视注释过程中的辅助信息，提出了使用注释副产品来训练模型的新方法，该方法可以减少虚假相关性并提高模型精度。

    

    图像分类器的监督学习将人类知识通过图像和相应标签（X，Y）的对应关系转化为参数模型。本文认为这种简单且广泛使用的人类知识表示忽视了注释过程中丰富的辅助信息，例如在图像选择后留下的鼠标轨迹和点击的时间序列等。我们的洞见是，这些注释副产品Z提供了近似的人类关注信息，弱化了模型对前景线索的关注，减少了虚假相关性并防止了捷径学习。为了验证这一点，我们创建了ImageNet-AB和COCO-AB。它们是通过复制相应的原始注释任务来获得的ImageNet和COCO训练集，增加了样本级别的注释副产品。我们称使用注释副产品来训练模型的新方法为学习注释副产品（LUAB）。我们展示了一个简单的多任务损失，用于同时回归Z和Y已经可以提高模型精度。

    Supervised learning of image classifiers distills human knowledge into a parametric model through pairs of images and corresponding labels (X,Y). We argue that this simple and widely used representation of human knowledge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging shortcut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves 
    
[^102]: 基于Transformer网络的自动驾驶车辆多模态操作和轨迹预测

    Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles Using Transformer Networks. (arXiv:2303.16109v1 [cs.LG])

    [http://arxiv.org/abs/2303.16109](http://arxiv.org/abs/2303.16109)

    提出了一种新颖的基于Transformer网络的多模态操作和轨迹预测框架，能够预测多个可能的行为模式及其概率，并且在两个公共基准公路驾驶数据集上得到了优秀的表现。

    

    预测其他道路用户（包括车辆）的行为（即操作/轨迹）对于自动驾驶车辆（AVs）或自动化驾驶系统（ADSs）的安全和高效运行至关重要。由于车辆未来行为的不确定性，对于给定的行驶场景，一个车辆通常存在多个未来行为模式是合理的。因此，多模态预测可以提供比单模态预测更丰富的信息，使AV能够更好地评估风险。为此，我们提出了一种新颖的多模态预测框架，可以预测多个可能的行为模式及其概率。该框架包括一个定制的操作预测问题公式，一个基于Transformer的预测模型，以及一种定制的多模态操作和轨迹预测的训练方法。使用两个公共基准公路驾驶数据集（即NGSIM和highD）评估了该框架的性能。结果表明，该框架在多模态操作和轨迹预测方面的表现得到了优化。

    Predicting the behaviour (i.e. manoeuvre/trajectory) of other road users, including vehicles, is critical for the safe and efficient operation of autonomous vehicles (AVs), a.k.a. automated driving systems (ADSs). Due to the uncertain future behaviour of vehicles, multiple future behaviour modes are often plausible for a vehicle in a given driving scene. Therefore, multimodal prediction can provide richer information than single-mode prediction enabling AVs to perform a better risk assessment. To this end, we propose a novel multimodal prediction framework that can predict multiple plausible behaviour modes and their likelihoods. The proposed framework includes a bespoke problem formulation for manoeuvre prediction, a novel transformer-based prediction model, and a tailored training method for multimodal manoeuvre and trajectory prediction. The performance of the framework is evaluated using two public benchmark highway driving datasets, namely NGSIM and highD. The results show that th
    
[^103]: FacEDiM：一种用于少样本生物特征认证的牛脸嵌入分布模型

    FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle. (arXiv:2302.14831v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.14831](http://arxiv.org/abs/2302.14831)

    本研究提出了一种用于少样本生物特征认证的牛脸嵌入分布模型，通过计算测试嵌入和训练嵌入的马氏距离来进行认证。实验结果表明，在ImageNet数据集上预训练的模型表现更好。使用VGG16模型，在一个包含20个牛身份的数据集上，我们得到了1.25%的FRR和1.18%的FAR。

    

    本研究提出通过计算测试嵌入和使用预训练的CNN获得的训练嵌入的多变量高斯分布之间的马氏距离来解决少样本生物特征认证问题。实验结果显示，在ImageNet数据集上预训练的模型明显优于在人脸上预训练的模型。使用VGG16模型，在一个包含20个牛身份的数据集上，我们得到了1.25%的FRR和1.18%的FAR。

    This work proposes to solve the problem of few-shot biometric authentication by computing the Mahalanobis distance between testing embeddings and a multivariate Gaussian distribution of training embeddings obtained using pre-trained CNNs. Experimental results show that models pre-trained on the ImageNet dataset significantly outperform models pre-trained on human faces. With a VGG16 model, we obtain a FRR of 1.25% for a FAR of 1.18% on a dataset of 20 cattle identities.
    
[^104]: 量化和建模多模态交互：一种信息分解框架

    Quantifying & Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12247](http://arxiv.org/abs/2302.12247)

    通过引入信息分解框架，我们提供了一种量化和建模多模态交互的方法，通过PID统计量来度量输入模态与输出任务之间的冗余度、独特性和协同性，并引入了两个新的PID统计估计器。

    

    对于解决多模态任务所需的交互如何进行量化？最适合捕捉这些交互的多模态模型是什么？为了回答这些问题，我们提出了一种信息论方法来量化输入模态与输出任务之间的冗余度、独特性和协同性。我们将这三个衡量标准称为多模态分布（或简称PID）的PID统计量，并引入了两个新的PID统计估计器，适用于高维分布。为了验证PID估计，我们在已知PID的合成数据集和大规模多模态基准测试集上进行了大量实验。

    The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
    
[^105]: 基于知识蒸馏的分散制造系统在线过程监测的信息共享

    Knowledge Distillation-based Information Sharing for Online Process Monitoring in Decentralized Manufacturing System. (arXiv:2302.12004v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12004](http://arxiv.org/abs/2302.12004)

    这项研究提出了一种基于知识蒸馏的方法，在分散制造系统中实现了高效且安全的信息共享，以提升性能较差的单元的在线过程监测。

    

    在先进制造领域，传感技术的应用为利用机器学习方法进行高效的原地过程监测提供了机会。同时，信息技术的进步也使得制造系统能够在连接和分散的环境下工作，使系统中的不同制造单元更加紧密地合作。在分散制造系统中，参与的单元可能制造相同或类似的产品，并且部署自己的机器学习模型进行在线过程监测。然而，由于操作过程中任务进度的可能不一致，有些单元的数据更具信息量，而有些单元的数据信息较少是很常见的情况。因此，每个单元的机器学习模型的监测性能可能会有很大差异。因此，在分散制造系统中实现单位间高效安全的知识共享是非常有价值的，可以提高性能较差的单元。

    In advanced manufacturing, the incorporation of sensing technology provides an opportunity to achieve efficient in-situ process monitoring using machine learning methods. Meanwhile, the advances of information technologies also enable a connected and decentralized environment for manufacturing systems, making different manufacturing units in the system collaborate more closely. In a decentralized manufacturing system, the involved units may fabricate same or similar products and deploy their own machine learning model for online process monitoring. However, due to the possible inconsistency of task progress during the operation, it is also common that some units have more informative data while some have less informative data. Thus, the monitoring performance of machine learning model for each unit may highly vary. Therefore, it is extremely valuable to achieve efficient and secured knowledge sharing among the units in a decentralized manufacturing system for enhancement of poorly perf
    
[^106]: 带有子立方体条件的超网格均匀性测试

    Uniformity Testing over Hypergrids with Subcube Conditioning. (arXiv:2302.09013v2 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2302.09013](http://arxiv.org/abs/2302.09013)

    本论文提出了一种用于测试超网格上分布均匀性的算法，该算法在具有子立方体条件抽样预言机的情况下，进行多次查询后可以获得高效的结果。通过使用傅里叶分析证明了超网格上函数的 Pisier 不等式的鲁棒版本。

    

    我们提出了一种算法，用于测试支持在超网格$[m_1] \times \cdots \times [m_n]$上的分布的均匀性，该算法对具有$m=\max_i m_i$的子立方体条件抽样预言机进行$\smash{\widetilde{O}(\text{poly}(m)\sqrt{n}/\epsilon^2)}$个查询。当$m$是一个常数时，我们的算法几乎是最优的，并且加强了[CCK+21]的算法，该算法具有相同的查询复杂度，但仅适用于超立方体$\{\pm 1\}^n$。我们算法分析中的一个关键技术贡献是使用傅里叶分析证明了超网格上函数的 Pisier 不等式的鲁棒版本。

    We give an algorithm for testing uniformity of distributions supported on hypergrids $[m_1] \times \cdots \times [m_n]$, which makes $\smash{\widetilde{O}(\text{poly}(m)\sqrt{n}/\epsilon^2)}$ many queries to a subcube conditional sampling oracle with $m=\max_i m_i$. When $m$ is a constant, our algorithm is nearly optimal and strengthens the algorithm of [CCK+21] which has the same query complexity but works for hypercubes $\{\pm 1\}^n$ only.  A key technical contribution behind the analysis of our algorithm is a proof of a robust version of Pisier's inequality for functions over hypergrids using Fourier analysis.
    
[^107]: 因子场：神经场和更多领域的统一框架

    Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.01226](http://arxiv.org/abs/2302.01226)

    因子场是一种统一的框架，将信号分解为因子的乘积，通过神经场或常规场表示进行处理。这个框架可以概括多种最近的信号表示方法，并且可以创建出更强大的新信号表示，如本文提出的系数基函数分解（CoBaFa）。实验结果表明，CoBaFa在逼近质量、紧凑性和效率方面相较于之前的方法有所改进。

    

    我们提出了因子场，一种用于建模和表示信号的新框架。因子场将信号分解为因子的乘积，每个因子由神经场或常规场表示，并对输入信号进行坐标变换操作。我们展示了这种分解产生了一个统一的框架，可以概括多种最近的信号表示方法，包括NeRF，PlenOxels，EG3D，Instant-NGP和TensoRF。此外，这个框架还可以创建强大的新信号表示，例如本文提出的系数基函数分解（CoBaFa）。通过实验，我们证明CoBaFa相较于之前的快速重建方法在神经信号表示的三个关键目标：逼近质量，紧凑性和效率方面有所改进。在二维图像回归任务中，我们实验证明我们的表示可以获得更好的图像逼近质量，同时在几何质量方面也更高。

    We present Factor Fields, a novel framework for modeling and representing signals. Factor Fields decomposes a signal into a product of factors, each of which is represented by a neural or regular field representation operating on a coordinate transformed input signal. We show that this decomposition yields a unified framework that generalizes several recent signal representations including NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, the framework allows for the creation of powerful new signal representations, such as the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper. As evidenced by our experiments, CoBaFa leads to improvements over previous fast reconstruction methods in terms of the three critical goals in neural signal representation: approximation quality, compactness and efficiency. Experimentally, we demonstrate that our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when r
    
[^108]: 面向联邦学习的客户端选择：原则、挑战和机会

    Client Selection in Federated Learning: Principles, Challenges, and Opportunities. (arXiv:2211.01549v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01549](http://arxiv.org/abs/2211.01549)

    本文系统地介绍了面向联邦学习的客户端选择的原则、挑战和机会，以及针对客户端异构性问题的各种算法的性能改进。该研究旨在帮助从业者选择最适合其应用的客户端选择机制，并激发研究人员对该领域的进一步探索。

    

    作为一种保护隐私的机器学习模型训练范式，联邦学习在工业界和学术界都受到了广泛关注。在典型的联邦学习场景中，客户端在数据分布和硬件配置方面显示出明显的异质性。因此，每一轮训练中随机选择客户端可能无法充分利用来自异构客户端的本地更新，导致模型准确性降低、收敛速度变慢、公平性下降等问题。为了解决联邦学习客户端异构性问题，已经开发了各种客户端选择算法，展示出了有希望的性能改进。在本文中，我们系统地介绍了近期在联邦学习客户端选择这一新兴领域的进展，以及其中的挑战和研究机会。我们希望能够帮助从业者选择最适合其应用的客户端选择机制，同时激发研究人员和新人对这一令人兴奋的研究领域有更好的理解。

    As a privacy-preserving paradigm for training Machine Learning (ML) models, Federated Learning (FL) has received tremendous attention from both industry and academia. In a typical FL scenario, clients exhibit significant heterogeneity in terms of data distribution and hardware configurations. Thus, randomly sampling clients in each training round may not fully exploit the local updates from heterogeneous clients, resulting in lower model accuracy, slower convergence rate, degraded fairness, etc. To tackle the FL client heterogeneity problem, various client selection algorithms have been developed, showing promising performance improvement. In this paper, we systematically present recent advances in the emerging field of FL client selection and its challenges and research opportunities. We hope to facilitate practitioners in choosing the most suitable client selection mechanisms for their applications, as well as inspire researchers and newcomers to better understand this exciting resea
    
[^109]: 对基于扩散的生成模型的最优控制视角

    An optimal control perspective on diffusion-based generative modeling. (arXiv:2211.01364v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01364](http://arxiv.org/abs/2211.01364)

    本文建立了随机最优控制与基于扩散的生成模型之间的联系，推导了用于控制潜在SDE边际密度演化的汉密尔顿-雅可比-贝尔曼方程，并将生成建模表述为对合适度量之间Kullback-Leibler散度的最小化。此外，作者还开发了一种新型扩散方法用于采样非归一化密度。

    

    我们建立了随机最优控制与基于随机微分方程（SDE）的生成模型之间的联系，例如最近发展起来的扩散概率模型。特别地，我们推导出一个汉密尔顿-雅可比-贝尔曼方程，用于控制潜在的SDE边际密度的演化。这个视角允许将最优控制理论的方法应用于生成建模中。首先，我们展示了证据下界是控制理论中广为人知的验证定理的直接结果。此外，我们可以将基于扩散的生成建模表述为路径空间中合适度量之间的Kullback-Leibler散度的最小化。最后，我们开发了一种从非归一化密度中进行采样的新型扩散方法，这在统计学和计算科学中经常出现的问题。我们证明了我们的时序反向扩散采样器（DIS）可以胜过其他基于扩散的采样方法。

    We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approache
    
[^110]: 使用可迁移的卷积神经网络进行多目标跟踪

    Multi-Target Tracking with Transferable Convolutional Neural Networks. (arXiv:2210.15539v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2210.15539](http://arxiv.org/abs/2210.15539)

    本论文提出了一种使用可迁移的卷积神经网络进行多目标跟踪的方法。该方法利用图像表示目标状态和传感器测量，并通过迁移学习实现了在大规模上进行MTT，并在10个目标的MTT任务中表现优于传统方法，在250个目标的更大MTT任务中性能提高了29%。

    

    多目标跟踪（MTT）是一个经典的信号处理任务，目标是从噪声传感器测量中估计出未知数量的移动目标的状态。在本文中，我们从深度学习的角度重新审视了MTT，并提出了一种卷积神经网络（CNN）架构来解决该问题。我们将目标状态和传感器测量表示为图像，并将问题重新定义为图像到图像的预测任务。然后，我们在小范围的跟踪区域训练了一个完全卷积模型，并将其迁移到具有大量目标和传感器的更大范围。这种迁移学习方法使得MTT能够在大规模上实现，并且在理论上得到了我们的新颖分析的支持，该分析限制了泛化误差。在实践中，所提出的可迁移CNN架构在具有10个目标的MTT任务上优于随机有限集滤波器，并且在不重新训练的情况下将其迁移到具有250个目标的更大的MTT任务中，性能提高了29%。

    Multi-target tracking (MTT) is a classical signal processing task, where the goal is to estimate the states of an unknown number of moving targets from noisy sensor measurements. In this paper, we revisit MTT from a deep learning perspective and propose a convolutional neural network (CNN) architecture to tackle it. We represent the target states and sensor measurements as images and recast the problem as an image-to-image prediction task. Then we train a fully convolutional model at small tracking areas and transfer it to much larger areas with numerous targets and sensors. This transfer learning approach enables MTT at a large scale and is also theoretically supported by our novel analysis that bounds the generalization error. In practice, the proposed transferable CNN architecture outperforms random finite set filters on the MTT task with 10 targets and transfers without re-training to a larger MTT task with 250 targets with a 29% performance improvement.
    
[^111]: Teal: 学习加速的广域网流量工程优化

    Teal: Learning-Accelerated Optimization of WAN Traffic Engineering. (arXiv:2210.13763v3 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2210.13763](http://arxiv.org/abs/2210.13763)

    Teal是一种学习加速的广域网流量工程优化算法，利用GPU的并行处理能力加速TE控制。它使用流为中心的图神经网络来捕捉WAN连接和网络流量，并采用多智能体强化学习算法进行独立分配和中心化优化。最后，它使用交替方向乘子法对分配进行微调。

    

    全球云广域网（WAN）的快速扩展给商业优化引擎带来了挑战，如何高效解决规模化的网络流量工程（TE）问题。现有的加速策略将TE优化分解为并发的子问题，但由于运行时间和分配性能之间的权衡，实现的并行化有限。本文提出了一种名为Teal的基于学习的TE算法，它利用GPU的并行处理能力加速TE控制。首先，Teal设计了一个以流为中心的图神经网络（GNN）来捕捉WAN连接和网络流量，学习流特征作为下游分配的输入。其次，为了减少问题规模并使学习可行，Teal采用了一种多智能体强化学习（RL）算法，独立分配每个流量需求，同时优化一个中心的TE目标。最后，Teal使用交替方向乘子法（ADMM）对分配进行微调，这是一种高效的方法。

    The rapid expansion of global cloud wide-area networks (WANs) has posed a challenge for commercial optimization engines to efficiently solve network traffic engineering (TE) problems at scale. Existing acceleration strategies decompose TE optimization into concurrent subproblems but realize limited parallelism due to an inherent tradeoff between run time and allocation performance.  We present Teal, a learning-based TE algorithm that leverages the parallel processing power of GPUs to accelerate TE control. First, Teal designs a flow-centric graph neural network (GNN) to capture WAN connectivity and network flows, learning flow features as inputs to downstream allocation. Second, to reduce the problem scale and make learning tractable, Teal employs a multi-agent reinforcement learning (RL) algorithm to independently allocate each traffic demand while optimizing a central TE objective. Finally, Teal fine-tunes allocations with ADMM (Alternating Direction Method of Multipliers), a highly 
    
[^112]: 不确定性估计中的单调性和双下降现象在高斯过程中的应用

    Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes. (arXiv:2210.07612v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.07612](http://arxiv.org/abs/2210.07612)

    本研究研究了机器学习模型中不确定性估计的问题，证明了通过调整超参数可以提高边际似然，但交叉验证度量表现出双下降现象。

    

    尽管评估预测的可靠性非常重要，但对于机器学习模型的不确定性量化（UQ）指标直到最近才开始得到严格的表征。一个显著问题是维度诅咒：普遍认为边缘似然应该与交叉验证度量类似，并且两者在输入维度较大时都会恶化。我们证明通过调整超参数以最大化边际似然（经验贝叶斯过程），性能（以边际似然测量）随着输入维度的增加单调改善。另一方面，我们证明交叉验证度量表现出不同的行为特征，即双下降现象。最近因在某些情况下性能提高而受到关注的冷态后验似乎加剧了这些现象。我们经验证实，我们的结果在真实数据上成立，超出我们考虑的假设范围。

    Despite their importance for assessing reliability of predictions, uncertainty quantification (UQ) measures for machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and that both should deteriorate with larger input dimensions. We prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), the performance, as measured by the marginal likelihood, improves monotonically} with the input dimension. On the other hand, we prove that cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena. We verify empirically that our results hold for real data, beyond our considered assump
    
[^113]: 利用遥感和机器学习实现早期检测树皮甲虫攻击：一项综述

    Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03829](http://arxiv.org/abs/2210.03829)

    本文综述了早期检测树皮甲虫攻击的过去和现有进展，重点关注了使用遥感和机器学习方法的优势和劣势，以及提供了有关树皮甲虫物种、攻击阶段、寄主树木、研究区域、遥感平台与传感器、光谱分辨率、光谱特征、机器学习方法和深度学习网络的知识。

    

    本文全面回顾了早期检测树皮甲虫引起的树木死亡方面的过去和现有进展，从树皮甲虫与寄主之间的相互作用、遥感和机器学习/深度学习三个主要角度进行了总结。与以往的努力相反，本综述包括了所有遥感系统，并强调了机器学习/深度学习方法来研究它们的优势和劣势。我们根据多光谱或高光谱分析解析了现有文献，并从树皮甲虫物种和攻击阶段，重点关注攻击的早期阶段、寄主树木、研究区域、遥感平台和传感器、光谱/空间/时间分辨率、光谱特征、光谱植被指数（SVIs）、机器学习方法、学习方案、任务类别、模型、算法、类别/簇、特征和深度学习网络与架构方面提取知识。尽管基于深度学习的方法和随机森林（RF）算法显示出有希望的结果，突出了它们在可见光和热红外等波段上检测微小变化的潜力。

    This paper provides a comprehensive review of past and current advances in the early detection of bark beetle-induced tree mortality from three primary perspectives: bark beetle & host interactions, RS, and ML/DL. In contrast to prior efforts, this review encompasses all RS systems and emphasizes ML/DL methods to investigate their strengths and weaknesses. We parse existing literature based on multi- or hyper-spectral analyses and distill their knowledge based on: bark beetle species & attack phases with a primary emphasis on early stages of attacks, host trees, study regions, RS platforms & sensors, spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation indices (SVIs), ML approaches, learning schemes, task categories, models, algorithms, classes/clusters, features, and DL networks & architectures. Although DL-based methods and the random forest (RF) algorithm showed promising results, highlighting their potential to detect subtle changes across visible, therma
    
[^114]: 从小样本中估计大的因果多树

    Estimating large causal polytrees from small samples. (arXiv:2209.07028v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2209.07028](http://arxiv.org/abs/2209.07028)

    本文介绍了一种算法，可以在变量数量远大于样本大小的情况下，准确地估计大规模因果多树结构，而几乎不需要任何分布或建模的假设。

    

    我们考虑从相对较小的独立同分布样本中估计大的因果多树的问题。这是在变量数量与样本大小相比非常大的情况下确定因果结构的问题，例如基因调控网络。我们提出了一种算法，在这种情况下以高准确度恢复树形结构。该算法除了一些温和的非退化条件外，基本不需要分布或建模的假设。

    We consider the problem of estimating a large causal polytree from a relatively small i.i.d. sample. This is motivated by the problem of determining causal structure when the number of variables is very large compared to the sample size, such as in gene regulatory networks. We give an algorithm that recovers the tree with high accuracy in such settings. The algorithm works under essentially no distributional or modeling assumptions other than some mild non-degeneracy conditions.
    
[^115]: AudioLM：一种音频生成的语言模型方法

    AudioLM: a Language Modeling Approach to Audio Generation. (arXiv:2209.03143v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2209.03143](http://arxiv.org/abs/2209.03143)

    AudioLM是一个通过语言建模方法实现音频生成的框架，它通过将输入音频映射为离散符号序列，结合语言模型和神经音频编解码器，能够生成高质量、符合语法和语义要求的连续音频。

    

    我们引入了AudioLM，这是一个高质量音频生成的框架，具有长期一致性。AudioLM将输入音频映射为一系列离散的符号，并将音频生成任务转化为语言建模任务。我们展示了现有音频分词工具在重构质量和长期结构之间提供的不同权衡，并提出了一种混合分词方案以实现两个目标。具体而言，我们利用在音频上预训练的带有掩码的语言模型的离散激活来捕捉长期结构，并利用神经音频编解码器产生的离散编码来实现高质量的合成。通过在大型原始音频波形语料库上进行训练，AudioLM学习了在给定短提示的情况下生成自然连续的能力。当在语音上进行训练且没有任何转录或注释时，AudioLM生成的语音连续保持语法和语义的合理性，同时还保持了未见过的说话者身份和韵律。

    We introduce AudioLM, a framework for high-quality audio generation with long-term consistency. AudioLM maps the input audio to a sequence of discrete tokens and casts audio generation as a language modeling task in this representation space. We show how existing audio tokenizers provide different trade-offs between reconstruction quality and long-term structure, and we propose a hybrid tokenization scheme to achieve both objectives. Namely, we leverage the discretized activations of a masked language model pre-trained on audio to capture long-term structure and the discrete codes produced by a neural audio codec to achieve high-quality synthesis. By training on large corpora of raw audio waveforms, AudioLM learns to generate natural and coherent continuations given short prompts. When trained on speech, and without any transcript or annotation, AudioLM generates syntactically and semantically plausible speech continuations while also maintaining speaker identity and prosody for unseen
    
[^116]: SHAP-XRT: Shapley Value遇上条件独立性测试

    SHAP-XRT: The Shapley Value Meets Conditional Independence Testing. (arXiv:2207.07038v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07038](http://arxiv.org/abs/2207.07038)

    本文展示了Shapley值解释方法与条件独立性测试之间的紧密关系，并介绍了一种基于条件随机化测试的测试过程SHAP-XRT，用于二分类问题中证明了Shapley值的边际贡献。

    

    人工神经网络的复杂性引发了对其在现实场景中的可靠性、可信度和公平性的关注。Shapley值是机器学习模型最流行的解释方法之一。从统计学的角度来看，特征重要性是通过条件独立性来定义的。到目前为止，这两种解释方法和特征重要性的方法被认为是分开的。本文展示了Shapley值解释方法与条件独立性测试之间的紧密关系。我们引入了SHAPley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT)，一种受条件随机化测试(CRT)启发的测试过程，用于检验特定概念的局部（在样本上的）条件独立性。通过SHAP-XRT，在二分类问题中证明了Shapley值的边际贡献。

    The complex nature of artificial neural networks raises concerns on their reliability, trustworthiness, and fairness in real-world scenarios. The Shapley value -- a solution concept from game theory -- is one of the most popular explanation methods for machine learning models. More traditionally, from a statistical perspective, feature importance is defined in terms of conditional independence. So far, these two approaches to interpretability and feature importance have been considered separate and distinct. In this work, we show that Shapley-based explanation methods and conditional independence testing are closely related. We introduce the $\textbf{SHAP}$ley-E$\textbf{X}$planation $\textbf{R}$andomization $\textbf{T}$est (SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test (CRT) for a specific notion of local (i.e., on a sample) conditional independence. With it, we prove that for binary classification problems, the marginal contributions in the Shapley valu
    
[^117]: 通过两步梯度更新超越稳定边界

    Beyond the Edge of Stability via Two-step Gradient Updates. (arXiv:2206.04172v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04172](http://arxiv.org/abs/2206.04172)

    本文研究了梯度下降对于非Lipschitz梯度的损失函数的收敛性，发现在过参数化模型中，尽管存在局部不稳定性和振荡行为，梯度下降仍然能够收敛。

    

    梯度下降（GD）是现代机器学习中的强大工具，因其在高维空间中的可扩展性和效率而闻名。对于具有Lipschitz梯度的损失函数，GD只能找到局部极小值点，可以看作是潜在梯度流的“真实”离散化方法。然而，许多涉及过参数化模型的机器学习设置并不属于这个问题类别，这促使研究超越所谓的“稳定边界”（Edge of Stability，EoS），其中步长越过与Lipschitz常数成反比的可允许阈值。令人惊讶的是，经验证明尽管存在局部不稳定性和振荡行为，GD仍然收敛。对这一现象的初步理论分析主要集中在过参数化的范围内，在此范围内选择较大的学习率可能与在最小化器流形内隐含的“尖度最小化”正则化相关，请详见论文了解更多细节。

    Gradient Descent (GD) is a powerful workhorse of modern machine learning thanks to its scalability and efficiency in high-dimensional spaces. Its ability to find local minimisers is only guaranteed for losses with Lipschitz gradients, where it can be seen as a `bona-fide' discretisation of an underlying gradient flow. Yet, many ML setups involving overparametrised models do not fall into this problem class, which has motivated research beyond the so-called ``Edge of Stability'' (EoS), where the step-size crosses the admissibility threshold inversely proportional to the Lipschitz constant above. Perhaps surprisingly, GD has been empirically observed to still converge regardless of local instability and oscillatory behavior.  The incipient theoretical analysis of this phenomena has mainly focused in the overparametrised regime, where the effect of choosing a large learning rate may be associated to a `Sharpness-Minimisation' implicit regularisation within the manifold of minimisers, unde
    
[^118]: TreeFlow：超越基于树的高斯概率回归

    TreeFlow: Going beyond Tree-based Gaussian Probabilistic Regression. (arXiv:2206.04140v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04140](http://arxiv.org/abs/2206.04140)

    TreeFlow是一种基于树集成和归一化流模型的方法，用于解决具有复杂分布的回归问题。

    

    以混合类型变量为特征向量的分类和回归问题上，基于树的集成在性能上表现出色。然而，在回归问题中，它们主要设计为提供确定性响应或使用高斯或参数分布对输出的不确定性进行建模。在这项工作中，我们引入了TreeFlow，这是一种结合了使用树集成和使用归一化流模型灵活概率分布建模能力的方法。解决方案的主要思想是使用基于树的模型作为特征提取器，并将其与归一化流的条件变体相结合。因此，我们的方法能够对回归输出进行复杂分布建模。我们在具有不同体积、特征特性和目标维度的具有挑战性的回归基准上评估了所提出的方法。

    The tree-based ensembles are known for their outstanding performance in classification and regression problems characterized by feature vectors represented by mixed-type variables from various ranges and domains. However, considering regression problems, they are primarily designed to provide deterministic responses or model the uncertainty of the output with Gaussian or parametric distribution. In this work, we introduce TreeFlow, the tree-based approach that combines the benefits of using tree ensembles with the capabilities of modeling flexible probability distributions using normalizing flows. The main idea of the solution is to use a tree-based model as a feature extractor and combine it with a conditional variant of normalizing flow. Consequently, our approach is capable of modeling complex distributions for the regression outputs. We evaluate the proposed method on challenging regression benchmarks with varying volume, feature characteristics, and target dimensionality. We obtai
    
[^119]: 高效准确的物理感知多重图神经网络用于3D小分子和大分子复合物

    Efficient and Accurate Physics-aware Multiplex Graph Neural Networks for 3D Small Molecules and Macromolecule Complexes. (arXiv:2206.02789v2 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2206.02789](http://arxiv.org/abs/2206.02789)

    本论文提出了一种名为PaxNet的物理感知多重图神经网络，用于高效准确地学习3D小分子和大分子复合物的表示。PaxNet通过分离局部和非局部相互作用的建模，并减少了与角度相关的计算。它可以预测矢量属性并在实验中取得了良好的性能。

    

    近年来，在分子科学中应用图神经网络（GNNs）的最新进展展示了使用GNNs学习三维（3D）结构表示的强大能力。然而，大多数现有的GNNs存在多样性相互作用建模不足、计算开销大和忽视矢量值的局限性。在这里，我们通过提出一种新颖的GNN模型，即物理感知多重图神经网络（PaxNet），来解决这些限制，以便在小有机化合物和大分子复合物的3D分子表示中高效准确地学习。PaxNet通过分离局部和非局部相互作用的建模，受到分子力学的启发，并减少了昂贵的与角度相关的计算。除了标量属性外，PaxNet还可以通过为每个原子学习一个相关联的矢量来预测矢量属性。为了评估PaxNet的性能，我们将其与两个任务中的最先进基线进行了比较。

    Recent advances in applying Graph Neural Networks (GNNs) to molecular science have showcased the power of learning three-dimensional (3D) structure representations with GNNs. However, most existing GNNs suffer from the limitations of insufficient modeling of diverse interactions, computational expensive operations, and ignorance of vectorial values. Here, we tackle these limitations by proposing a novel GNN model, Physics-aware Multiplex Graph Neural Network (PaxNet), to efficiently and accurately learn the representations of 3D molecules for both small organic compounds and macromolecule complexes. PaxNet separates the modeling of local and non-local interactions inspired by molecular mechanics, and reduces the expensive angle-related computations. Besides scalar properties, PaxNet can also predict vectorial properties by learning an associated vector for each atom. To evaluate the performance of PaxNet, we compare it with state-of-the-art baselines in two tasks. On small molecule dat
    
[^120]: 推荐系统中的公平性：基础、方法和应用

    Fairness in Recommendation: Foundations, Methods and Applications. (arXiv:2205.13619v5 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2205.13619](http://arxiv.org/abs/2205.13619)

    这篇论文对推荐系统中的公平性问题进行了系统调查，针对推荐过程中可能出现的数据或算法偏见，提供了一些方法和应用来提升推荐中的公平性。

    

    作为机器学习最普遍的应用之一，推荐系统在辅助人类决策中起着重要作用。用户的满意度和平台的利益与生成的推荐结果的质量密切相关。然而，作为一个高度数据驱动的系统，推荐系统可能受到数据或算法偏见的影响，从而产生不公平的结果，这可能削弱系统的可信赖性。因此，在推荐设置中解决潜在的不公平问题至关重要。最近，对推荐系统的公平性考虑引起了越来越多的关注，涉及提升推荐中的公平性的方法越来越多。然而，这些研究相对零散且缺乏系统化整理，因此对于新研究人员来说难以深入领域。这促使我们对推荐中现有公平性作品进行系统调查。

    As one of the most pervasive applications of machine learning, recommender systems are playing an important role on assisting human decision making. The satisfaction of users and the interests of platforms are closely related to the quality of the generated recommendation results. However, as a highly data-driven system, recommender system could be affected by data or algorithmic bias and thus generate unfair results, which could weaken the reliance of the systems. As a result, it is crucial to address the potential unfairness problems in recommendation settings. Recently, there has been growing attention on fairness considerations in recommender systems with more and more literature on approaches to promote fairness in recommendation. However, the studies are rather fragmented and lack a systematic organization, thus making it difficult to penetrate for new researchers to the domain. This motivates us to provide a systematic survey of existing works on fairness in recommendation. This
    
[^121]: 强健的数量感知聚合方法用于联邦学习

    Robust Quantity-Aware Aggregation for Federated Learning. (arXiv:2205.10848v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2205.10848](http://arxiv.org/abs/2205.10848)

    本文提出了一种针对联邦学习的强健的数量感知聚合算法，名为FedRA，能够在聚合模型时考虑本地数据的数量，并能够抵御数量增强攻击。

    

    联邦学习（FL）是一种能够使多个客户端协同训练模型而不共享本地数据的隐私保护机器学习框架。然而，传统的FL面临严重的安全性和鲁棒性问题，例如，恶意的客户端可以污染模型更新，并同时虚报大量以放大其在模型聚合中的影响。现有的FL防御方法，虽然都能处理恶意的模型更新，但要么将所有数量视为良性，要么简单地忽略/截断所有客户端的数量。前者容易受到数量增强攻击的影响，而后者会导致子优的性能，因为不同客户端上的本地数据通常具有显着不同的大小。在本文中，我们提出了一种 robust quantity-aware aggregation algorithm for federated learning (FedRA)，通过对本地数据数量的感知来执行聚合，并能够抵御数量增强攻击。

    Federated learning (FL) enables multiple clients to collaboratively train models without sharing their local data, and becomes an important privacy-preserving machine learning framework. However, classical FL faces serious security and robustness problem, e.g., malicious clients can poison model updates and at the same time claim large quantities to amplify the impact of their model updates in the model aggregation. Existing defense methods for FL, while all handling malicious model updates, either treat all quantities benign or simply ignore/truncate the quantities of all clients. The former is vulnerable to quantity-enhanced attack, while the latter leads to sub-optimal performance since the local data on different clients is usually in significantly different sizes. In this paper, we propose a robust quantity-aware aggregation algorithm for federated learning, called FedRA, to perform the aggregation with awareness of local data quantities while being able to defend against quantity
    
[^122]: 关于数值特征在表格深度学习中的嵌入

    On Embeddings for Numerical Features in Tabular Deep Learning. (arXiv:2203.05556v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.05556](http://arxiv.org/abs/2203.05556)

    本文研究了表格深度学习中关于数值特征的嵌入方法，提出了两种不同的构建嵌入模块的方法，并通过实验证明，与传统模块相比，这些方法可以显著提升模型性能。这对于构建更强大的深度学习模型并在一些传统上适用于GBDT的基准上与之竞争具有重要的益处。

    

    最近，类似Transformer的深度架构在表格数据问题上展现出了强大的性能。与传统模型（如MLP）不同，这些架构将数值特征的标量值映射到高维嵌入中，然后在主干网络中将它们混合。本文认为，数值特征的嵌入在表格深度学习中是一个未充分探索的自由度，它允许构建更强大的深度学习模型，并与传统上适用于GBDT的基准进行竞争。我们首先描述了构建嵌入模块的两种概念上不同的方法：第一种基于标量值的分段线性编码，第二种利用周期性激活函数。然后，我们凭经验证明，与基于线性层和ReLU激活的传统模块相比，这两种方法可以显著提高模型性能。重要的是，我们还展示了嵌入数值特征的益处。

    Recently, Transformer-like deep architectures have shown strong performance on tabular data problems. Unlike traditional models, e.g., MLP, these architectures map scalar values of numerical features to high-dimensional embeddings before mixing them in the main backbone. In this work, we argue that embeddings for numerical features are an underexplored degree of freedom in tabular DL, which allows constructing more powerful DL models and competing with GBDT on some traditionally GBDT-friendly benchmarks. We start by describing two conceptually different approaches to building embedding modules: the first one is based on a piecewise linear encoding of scalar values, and the second one utilizes periodic activations. Then, we empirically demonstrate that these two approaches can lead to significant performance boosts compared to the embeddings based on conventional blocks such as linear layers and ReLU activations. Importantly, we also show that embedding numerical features is beneficial 
    
[^123]: MICDIR: 使用UNetMSS和自构建图潜变量的多尺度反向一致可变形图像配准

    MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent. (arXiv:2203.04317v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2203.04317](http://arxiv.org/abs/2203.04317)

    本文提出了MICDIR方法，在多尺度上使用UNetMSS和自构建图潜变量，用于解决医学图像配准中存在的全局依赖性和大形变的问题。

    

    图像配准是将不同图像带入到一个共同坐标系的过程，广泛应用于计算机视觉的各个领域，如遥感、图像检索和医学成像。深度学习技术已成功应用于解决各种复杂的医学图像处理问题，包括医学图像配准。多年来，已提出了几种使用深度学习的图像配准技术。例如Voxelmorph等可变形图像配准技术，成功捕捉到更细微的变化并提供更平滑的变形。然而，Voxelmorph、ICNet和FIRE等方法并没有明确编码全局依赖性（即所提供图像的整体解剖视图），因此不能跟踪大的形变。为了解决上述问题，本文以三种不同的方式扩展了Voxelmorph方法。

    Image registration is the process of bringing different images into a common coordinate system - a technique widely used in various applications of computer vision, such as remote sensing, image retrieval, and, most commonly, medical imaging. Deep learning based techniques have been applied successfully to tackle various complex medical image processing problems, including medical image registration. Over the years, several image registration techniques have been proposed using deep learning. Deformable image registration techniques such as Voxelmorph have been successful in capturing finer changes and providing smoother deformations. However, Voxelmorph, as well as ICNet and FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical view of the supplied image) and, therefore, cannot track large deformations. In order to tackle the aforementioned problems, this paper extends the Voxelmorph approach in three different ways. To improve the performance in case of smal
    
[^124]: MetaDT: 使用类层次结构的元决策树进行可解释的少样本学习

    MetaDT: Meta Decision Tree with Class Hierarchy for Interpretable Few-Shot Learning. (arXiv:2203.01482v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01482](http://arxiv.org/abs/2203.01482)

    本论文提出了一种名为MetaDT的新颖元学习决策树框架，用于可解释的少样本学习任务。通过引入概念层次结构和决策树模型，实现了FSL决策过程的可解释性。从而在使用少量示例识别新类别时，提供了一个明确的决策过程。

    

    少样本学习(FSL)是一个具有挑战性的任务，旨在使用少量示例识别新类别。最近，许多方法从元学习和表示学习的角度提出。然而，很少有研究关注FSL决策过程的可解释性。本文提出了一种新颖的基于元学习的决策树框架MetaDT，从可解释的FSL角度迈出了一步。具体而言，FSL的可解释性从概念和视觉两个方面实现。在概念方面，我们首先引入了类似树状的概念层次结构作为FSL的先验知识。然后，根据先验，我们将每个少样本任务分割为一组具有不同概念层次的子任务，然后通过决策树模型进行类别预测。这种设计的优势在于可以获得一系列导致最终类别预测的高级概念决策，从而明确FSL的决策过程。在视觉方面...

    Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel classes with few examples. Recently, lots of methods have been proposed from the perspective of meta-learning and representation learning. However, few works focus on the interpretability of FSL decision process. In this paper, we take a step towards the interpretable FSL by proposing a novel meta-learning based decision tree framework, namely, MetaDT. In particular, the FSL interpretability is achieved from two aspects, i.e., a concept aspect and a visual aspect. On the concept aspect, we first introduce a tree-like concept hierarchy as FSL prior. Then, resorting to the prior, we split each few-shot task to a set of subtasks with different concept levels and then perform class prediction via a model of decision tree. The advantage of such design is that a sequence of high-level concept decisions that lead up to a final class prediction can be obtained, which clarifies the FSL decision process. On the visual a
    
[^125]: 模型比较和校准评估：机器学习和精算实践中一致性评分函数的用户指南

    Model Comparison and Calibration Assessment: User Guide for Consistent Scoring Functions in Machine Learning and Actuarial Practice. (arXiv:2202.12780v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.12780](http://arxiv.org/abs/2202.12780)

    这篇用户指南介绍了机器学习和精算实践中一致性评分函数的比较和校准评估技术，并强调了事先确定目标预测函数和选择相应评分函数的重要性。

    

    精算师和数据科学家主要的任务之一是构建适用于保险领域中诸如索赔金额或索赔数量等现象的有效预测模型。这些模型理想情况下利用给定的特征信息来提高预测的准确性。本用户指南重新审视和明确了用于评估模型校准性或适当性的统计技术，以及比较和排名不同模型的技术。在这样做时，强调了事先明确指定目标预测函数（如平均值或分位数）以及选择与此目标函数一致的评分函数在模型比较中的重要性。提供了实际选择评分函数的指导。致力于填补科学与实际应用中的差距，主要专注于现有结果和最佳实践的教学展示，并通过两个真实数据案例进行了说明和解释。

    One of the main tasks of actuaries and data scientists is to build good predictive models for certain phenomena such as the claim size or the number of claims in insurance. These models ideally exploit given feature information to enhance the accuracy of prediction. This user guide revisits and clarifies statistical techniques to assess the calibration or adequacy of a model on the one hand, and to compare and rank different models on the other hand. In doing so, it emphasises the importance of specifying the prediction target functional at hand a priori (e.g. the mean or a quantile) and of choosing the scoring function in model comparison in line with this target functional. Guidance for the practical choice of the scoring function is provided. Striving to bridge the gap between science and daily practice in application, it focuses mainly on the pedagogical presentation of existing results and of best practice. The results are accompanied and illustrated by two real data case studies 
    
[^126]: 在神经网络中结合最优路径搜索和任务相关学习

    Combining optimal path search with task-dependent learning in a neural network. (arXiv:2201.11104v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.11104](http://arxiv.org/abs/2201.11104)

    这篇论文提出了一种在神经网络中结合最优路径搜索和任务相关学习的方法，通过将成本值转化为神经网络的权重来实现在线权重适应。实验结果表明，该方法与经典算法Bellman-Ford具有相同的解，并且网络学习机制可以进一步增强算法的性能。

    

    在连接图中找到最优路径需要确定沿着图的边缘行进的最小总成本。这个问题可以通过几种经典算法来解决，通常所有边缘的成本都是预先定义好的。因此，在想要根据某个任务的要求以自适应的方式改变成本时，通常无法使用传统规划方法。在这里，我们展示了可以通过将成本值转化为突触权重来定义路径搜索问题的神经网络表示，这允许使用网络学习机制进行在线权重适应。当从一个初始活跃度值为1开始时，在这个网络中的活动传播将导致与Bellman-Ford算法找到的解相同的解。神经网络具有与Bellman-Ford相同的算法复杂度，并且此外，我们可以证明网络学习机制（如赫布学习）可以调整网络中的权重来增强算法的性能。

    Finding optimal paths in connected graphs requires determining the smallest total cost for traveling along the graph's edges. This problem can be solved by several classical algorithms where, usually, costs are predefined for all edges. Conventional planning methods can, thus, normally not be used when wanting to change costs in an adaptive way following the requirements of some task. Here we show that one can define a neural network representation of path finding problems by transforming cost values into synaptic weights, which allows for online weight adaptation using network learning mechanisms. When starting with an initial activity value of one, activity propagation in this network will lead to solutions, which are identical to those found by the Bellman-Ford algorithm. The neural network has the same algorithmic complexity as Bellman-Ford and, in addition, we can show that network learning mechanisms (such as Hebbian learning) can adapt the weights in the network augmenting the r
    
[^127]: 贝叶斯最优臂识别中的最优简单遗憾

    Optimal Simple Regret in Bayesian Best Arm Identification. (arXiv:2111.09885v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.09885](http://arxiv.org/abs/2111.09885)

    该论文研究了多臂赌博机问题中贝叶斯最优臂识别的速率，并提出了一种简单易行的算法，其匹配了下界，只差一个常数因子。

    

    我们考虑多臂赌博机问题中的最优臂识别。在先验条件具有一定的连续性的情况下，我们表征了贝叶斯简单遗憾的速率。与贝叶斯遗憾最小化不同，贝叶斯简单遗憾的主导项来源于最优臂和次优臂之间间隙小于$\sqrt{\frac{\log T}{T}}$的区域。我们提出了一种简单易行的计算算法，其主导项匹配了下界，只差一个常数因子；模拟结果支持了我们的理论发现。

    We consider best arm identification in the multi-armed bandit problem. Assuming certain continuity conditions of the prior, we characterize the rate of the Bayesian simple regret. Differing from Bayesian regret minimization (Lai, 1987), the leading term in the Bayesian simple regret derives from the region where the gap between optimal and suboptimal arms is smaller than $\sqrt{\frac{\log T}{T}}$. We propose a simple and easy-to-compute algorithm with its leading term matching with the lower bound up to a constant factor; simulation results support our theoretical findings.
    
[^128]: 重新审视用于表格数据的深度学习模型

    Revisiting Deep Learning Models for Tabular Data. (arXiv:2106.11959v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11959](http://arxiv.org/abs/2106.11959)

    本论文重新审视了用于表格数据的深度学习模型，提出了两种简单且强大的深度学习架构作为性能基准，包括类似于ResNet的架构和适应于表格数据的Transformer架构。这些基准模型在不同问题上表现出有竞争力的性能。

    

    现有关于表格数据的深度学习的文献提出了各种新颖的架构，并在各种数据集上报告了有竞争力的结果。然而，这些提出的模型通常没有进行适当的比较，现有的研究常常使用不同的基准和实验方案。因此，对于研究人员和实践者而言，什么样的模型性能最好是不清楚的。另外，该领域仍然缺乏有效的基准模型，即在不同问题上提供有竞争力性能的易于使用的模型。在这项工作中，我们对表格数据的主要DL架构进行了概述，并通过确定两种简单而强大的深度架构，提高了表格DL的基准。第一种是类似于ResNet的架构，结果显示它是常见的先前工作中常缺失的强基准。第二个模型是我们针对表格数据的Transformer架构的简单适应，在性能上超过了其他解决方案。

    The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets. However, the proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols. As a result, it is unclear for both researchers and practitioners what models perform best. Additionally, the field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.  In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures. The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works. The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solution
    
[^129]: 带有稀疏依赖结构的可压缩频谱混合核用于高斯过程

    Compressible Spectral Mixture Kernels with Sparse Dependency Structures for Gaussian Processes. (arXiv:1808.00560v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1808.00560](http://arxiv.org/abs/1808.00560)

    本文提出了一种带有稀疏依赖结构的可压缩频谱混合核用于高斯过程，通过改进原始核的泛化性能。具体方法包括通过交叉协方差和交叉卷积泛化依赖结构，以及通过参数化时间和相位延迟提高依赖结构的表达能力。

    

    频谱混合（SM）核是一种描述复杂模式的通用内核类用于高斯过程（GPs）。本文通过模型压缩和时间相位（TP）调制依赖结构，改进了原始的（SM）核使其具有更好的泛化性能。具体而言，通过采用Bienaymés恒等式，我们通过SM组件之间的交叉协方差来泛化依赖结构。然后，我们提出了一种新的带有依赖结构（SMD）的SM核，通过SM组件之间的交叉卷积。此外，我们通过参数化时间和相位延迟来改善依赖结构的表达能力。该依赖结构在频谱密度、协方差行为和采样路径方面具有清晰的解释。为了丰富SMD的有效超参数初始化、可压缩的SM核组件和稀疏的依赖结构，我们引入了一种新的结构适应（SA）算法。

    Spectral mixture (SM) kernels comprise a powerful class of generalized kernels for Gaussian processes (GPs) to describe complex patterns. This paper introduces model compression and time- and phase (TP) modulated dependency structures to the original (SM) kernel for improved generalization of GPs. Specifically, by adopting Bienaym\'es identity, we generalize the dependency structure through cross-covariance between the SM components. Then, we propose a novel SM kernel with a dependency structure (SMD) by using cross-convolution between the SM components. Furthermore, we ameliorate the expressiveness of the dependency structure by parameterizing it with time and phase delays. The dependency structure has clear interpretations in terms of spectral density, covariance behavior, and sampling path. To enrich the SMD with effective hyperparameter initialization, compressible SM kernel components, and sparse dependency structures, we introduce a novel structure adaptation (SA) algorithm in th
    

