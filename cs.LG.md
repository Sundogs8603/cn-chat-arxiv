# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series](https://arxiv.org/abs/2403.15360) | SiMBA是一种引入Einstein FFT进行通道建模并使用Mamba块进行序列建模的新架构，在图像和时间序列基准上优于现有的SSMs。 |
| [^2] | [Auditing Fairness under Unobserved Confounding](https://arxiv.org/abs/2403.14713) | 在未观测混杂因素的情况下，本文展示了即使在放宽或甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以给出对高风险个体分配率的信息丰富的界限。 |
| [^3] | [CleanAgent: Automating Data Standardization with LLM-based Agents](https://arxiv.org/abs/2403.08291) | 提出了一个具有声明性、统一API的Python库，通过简洁的API调用简化LLM的代码生成流程 |
| [^4] | [Generative Probabilistic Forecasting with Applications in Market Operations](https://arxiv.org/abs/2403.05743) | 提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。 |
| [^5] | [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218) | WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。 |
| [^6] | [NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function](https://arxiv.org/abs/2403.02411) | 提出了一种新的计算块，称为NiNformer，具有令牌混合生成门控功能，以解决注意机制在深度学习中的计算成本高昂和数据集要求大的缺点。 |
| [^7] | [FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class Classification in Flow Cytometry Benchmarking](https://arxiv.org/abs/2403.00024) | 本文介绍了FlowCyt项目，提出了针对流式细胞术数据中多类别单细胞分类的全面基准测试，其中图神经网络（GNNs）通过利用图编码数据中的空间关系展现出卓越的性能。 |
| [^8] | [TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations](https://arxiv.org/abs/2402.17660) | TorchMD-Net 2.0是在神经网络势模型方面取得的重要进展，通过引入TensorNet等尖端结构，显著提高了计算效率，使得在计算能量和力时获得了2到10倍的性能提升。 |
| [^9] | [A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference](https://arxiv.org/abs/2402.07314) | 本论文从理论层面分析了一种关于一般偏好下纳什学习从人类反馈中的方法，通过对两个竞争的LLM进行博弈来找到一种一致生成响应的策略。 |
| [^10] | [VerAs: Verify then Assess STEM Lab Reports](https://arxiv.org/abs/2402.05224) | VerAs是一个端到端的神经架构，用于验证和评估STEM实验报告。它通过利用多个维度的分析评估标准，以及针对学生提供详细反馈，帮助他们提高科学写作技巧。 |
| [^11] | [Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks](https://arxiv.org/abs/2402.04248) | 本研究评估了Mamba在上下文学习任务中的性能，并与Transformer模型进行比较。结果显示，SSMs在标准回归任务中与Transformer性能相当，在稀疏奇偶学习等任务中表现优异，但在涉及非标准检索功能的任务中表现不佳。为了解决这些局限性，引入了一种混合模型。 |
| [^12] | [PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems.](http://arxiv.org/abs/2401.08903) | 本文提出了一种名为PPR的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。该方法利用对抗样本修剪，并通过嵌入对抗扰动来增强对抗人脸样本的躲避性能。 |
| [^13] | [On the Convergence of Semi Unsupervised Calibration through Prior Adaptation Algorithm.](http://arxiv.org/abs/2401.03051) | 本文研究了一种半无监督校准算法，使用动力系统的观点证明了该算法的收敛性质，并通过实验验证了所提出的结果。 |
| [^14] | [Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space.](http://arxiv.org/abs/2310.13572) | 本研究通过对学得表示的特征空间进行全面分析，揭示出双谷现象是在使用噪声数据训练的不完美模型中出现的，并提出了双谷现象的解释：模型首先学习噪声数据直到插值点，然后通过超参数化的隐式正则化来将信息与噪声分离。 |
| [^15] | [Lemur: Integrating Large Language Models in Automated Program Verification.](http://arxiv.org/abs/2310.04870) | 本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。 |
| [^16] | [FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data.](http://arxiv.org/abs/2308.12388) | FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。 |
| [^17] | [Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective.](http://arxiv.org/abs/2308.00994) | 本文提出了一个简单而有效的基准线SYNAuG，利用合成数据来解决数据不平衡问题，并在多个数据集上取得了令人印象深刻的性能，超过了现有方法。 |
| [^18] | [Does Full Waveform Inversion Benefit from Big Data?.](http://arxiv.org/abs/2307.15388) | 本文研究了大数据对全波形反演深度学习模型的影响，并通过实证研究证明了更大的数据集能够提高模型性能和泛化能力，同时强调模型容量需要根据数据大小进行扩展以取得最佳改进效果。 |
| [^19] | [Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations.](http://arxiv.org/abs/2307.12062) | 这篇论文提出了一种新的鲁棒强化学习方法，通过将时间耦合的鲁棒强化学习问题视为两人零和游戏来处理问题，并通过找到近似均衡来确保代理对时间耦合干扰的鲁棒性。实验结果显示，该方法在各种连续控制任务中相比基准方法表现出显著的鲁棒性优势。 |
| [^20] | [Semantic Positive Pairs for Enhancing Contrastive Instance Discrimination.](http://arxiv.org/abs/2306.16122) | 提出了一种名为语义正向对集合（SPPS）的方法，可以在表示学习过程中识别具有相似语义内容的图像，并将它们视为正向实例，从而减少丢弃重要特征的风险。在多个实验数据集上的实验证明了该方法的有效性。 |
| [^21] | [Structure in Reinforcement Learning: A Survey and Open Problems.](http://arxiv.org/abs/2306.16021) | 这项调查研究了强化学习中结构的角色和重要性，并介绍了各个子领域在提高强化学习的性能方面所做的工作。 |
| [^22] | [Brainformers: Trading Simplicity for Efficiency.](http://arxiv.org/abs/2306.00008) | Brainformers 是一个新的深度神经网络模型，它通过使用多样层级的结构完善了 Transformer 的设计缺陷，具有更高效的训练收敛和更快的步骤时间，表现出更优秀的性能。 |
| [^23] | [IDEA: Invariant Causal Defense for Graph Adversarial Robustness.](http://arxiv.org/abs/2305.15792) | IDEA提出了一种通过学习具有强预测性和跨攻击不变性的因果特征来实现图形对抗鲁棒性的不变因果防御方法，相对于其他方法具有更高的效果和泛化性。 |
| [^24] | [Retrieving Texts based on Abstract Descriptions.](http://arxiv.org/abs/2305.12517) | 本研究针对语义检索问题，提出了一种基于摘要描述的文本检索模型，通过改进当前的文本嵌入方法，在标准最近邻搜索中取得了显著性能提升。 |
| [^25] | [Exact Recovery for System Identification with More Corrupt Data than Clean Data.](http://arxiv.org/abs/2305.10506) | 本文研究了在敌对环境下线性离散时间系统的系统识别问题。在周期性注入攻击时，系统动态可精确恢复样本复杂度为O(n)；当攻击以概率p进行时，精确恢复样本复杂度为O(log(n)p/(1-p)^2) 。即使有超过一半的数据受损，估计器仍可学习。 |
| [^26] | [Interpolation property of shallow neural networks.](http://arxiv.org/abs/2304.10552) | 本文证明了浅层神经网络可以插值任何数据集，即损失函数具有全局最小值为零的性质，此外还给出了该全局最小值处的惯性矩阵的表征，并提供了一种实用的概率方法来寻找插值点。 |
| [^27] | [Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning.](http://arxiv.org/abs/2304.06768) | 本文介绍了一种通过坐标变换来加速梯度优化算法、改善荒原高原和局部最小值影响的通用策略，有效提高了多种量子机器学习算法的性能。 |
| [^28] | [Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE.](http://arxiv.org/abs/2304.00083) | 本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。 |
| [^29] | [GADFormer: An Attention-based Model for Group Anomaly Detection on Trajectories.](http://arxiv.org/abs/2303.09841) | 本文提出了一种基于BERT模型的GADFormer模型，能够在轨迹上执行基于注意力的群体异常检测，相比其他深度序列模型，GADFormer表现更优，在多个公开数据集上实验表明GADFormer在检测异常群体方面取得了显著的提高。 |
| [^30] | [Digital Over-the-Air Federated Learning in Multi-Antenna Systems.](http://arxiv.org/abs/2302.14648) | 本文研究了在多天线系统中采用数字调制和空中计算的情况下，联邦学习的性能优化问题。通过结合数字调制和AirComp，提出了一种改进的联邦平均算法，以解决无线信道衰落导致的总体失真问题。 |
| [^31] | [Bagging Provides Assumption-free Stability.](http://arxiv.org/abs/2301.12600) | 本文证明了Bagging技术可提供无偏差稳定性，适用于各种数据分布和算法，具有良好的实证效果。 |
| [^32] | [A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics.](http://arxiv.org/abs/2301.04900) | 本文介绍了一种行为良好的图神经网络近似复杂动力学的方法，包括必要的偏置和适当的神经网络结构，并提出了评估泛化能力和推断时预测置信度的方法。 |
| [^33] | [Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction.](http://arxiv.org/abs/2206.09821) | 本论文提出了一种基于回归的超标概率预测方法，用于预测显著波高，通过利用预测来估计超标概率，取得了更好的效果。 |

# 详细

[^1]: SiMBA：用于视觉和多变量时间序列的简化Mamba架构

    SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time series

    [https://arxiv.org/abs/2403.15360](https://arxiv.org/abs/2403.15360)

    SiMBA是一种引入Einstein FFT进行通道建模并使用Mamba块进行序列建模的新架构，在图像和时间序列基准上优于现有的SSMs。

    

    Transformers广泛采用了注意力网络进行序列混合和MLPs进行通道混合，在各个领域取得了突破性进展。然而，最近的文献强调了关于注意力网络的问题，包括对输入序列长度的低归纳偏差和二次复杂度。状态空间模型（SSMs）如S4和其他模型（Hippo，Global Convolutions，liquid S4，LRU，Mega和Mamba），已经出现来解决以上问题，以帮助处理更长的序列长度。Mamba是当前最先进的SSM，但在扩展到大型计算机视觉数据集时存在稳定性问题。我们提出了SiMBA，一种新的架构，通过特定的特征值计算引入Einstein FFT（EinFFT）来进行通道建模，并使用Mamba块进行序列建模。对图像和时间序列基准的广泛性能研究表明，SiMBA优于现有的SSMs，架起了

    arXiv:2403.15360v1 Announce Type: cross  Abstract: Transformers have widely adopted attention networks for sequence mixing and MLPs for channel mixing, playing a pivotal role in achieving breakthroughs across domains. However, recent literature highlights issues with attention networks, including low inductive bias and quadratic complexity concerning input sequence length. State Space Models (SSMs) like S4 and others (Hippo, Global Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address the above issues to help handle longer sequence lengths. Mamba, while being the state-of-the-art SSM, has a stability issue when scaled to large networks for computer vision datasets. We propose SiMBA, a new architecture that introduces Einstein FFT (EinFFT) for channel modeling by specific eigenvalue computations and uses the Mamba block for sequence modeling. Extensive performance studies across image and time-series benchmarks demonstrate that SiMBA outperforms existing SSMs, bridging
    
[^2]: 在未观测混杂因素下审计公平性

    Auditing Fairness under Unobserved Confounding

    [https://arxiv.org/abs/2403.14713](https://arxiv.org/abs/2403.14713)

    在未观测混杂因素的情况下，本文展示了即使在放宽或甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以给出对高风险个体分配率的信息丰富的界限。

    

    决策系统中的一个基本问题是跨越人口统计线存在不公平性。然而，不公平性可能难以量化，特别是如果我们对公平性的理解依赖于难以衡量的风险等观念（例如，对于那些没有其治疗就会死亡的人平等获得治疗）。审计这种不公平性需要准确测量个体风险，而在未观测混杂的现实环境中，难以估计。在这些未观测到的因素“解释”明显差异的情况下，我们可能低估或高估不公平性。在本文中，我们展示了即使在放宽或（令人惊讶地）甚至在排除所有相关风险因素被观测到的假设的情况下，仍然可以对高风险个体的分配率给出信息丰富的界限。我们利用了在许多实际环境中（例如引入新型治疗）我们拥有在任何分配之前的数据的事实。

    arXiv:2403.14713v1 Announce Type: cross  Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any alloc
    
[^3]: CleanAgent：基于LLM代理自动化数据标准化

    CleanAgent: Automating Data Standardization with LLM-based Agents

    [https://arxiv.org/abs/2403.08291](https://arxiv.org/abs/2403.08291)

    提出了一个具有声明性、统一API的Python库，通过简洁的API调用简化LLM的代码生成流程

    

    数据标准化是数据科学生命周期中至关重要的一部分。虽然诸如Pandas之类的工具提供了强大的功能，但它们的复杂性以及需要定制代码以适应不同列类型的手动操作带来了重大挑战。尽管大型语言模型（LLMs）如ChatGPT已经展现出通过自然语言理解和代码生成自动化此过程的潜力，但仍需要专业程度的编程知识和持续互动以进行及时的完善。为了解决这些挑战，我们的关键想法是提出一个具有声明性、统一API的Python库，用于标准化列类型，通过简洁的API调用简化LLM的代码生成流程。我们首先提出了Dataprep.Clean，作为Dataprep库的一个组件，通过一行代码实现特定列类型的标准化，极大降低了复杂性。然后我们介绍了CleanAgen

    arXiv:2403.08291v1 Announce Type: cross  Abstract: Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgen
    
[^4]: 具有市场运营应用的生成式概率预测

    Generative Probabilistic Forecasting with Applications in Market Operations

    [https://arxiv.org/abs/2403.05743](https://arxiv.org/abs/2403.05743)

    提出了一种基于Wiener-Kallianpur创新表示的生成式概率预测方法，包括自编码器和新颖的深度学习算法，具有渐近最优性和结构收敛性质，适用于实时市场运营中的高动态和波动时间序列。

    

    本文提出了一种新颖的生成式概率预测方法，该方法源自于非参数时间序列的Wiener-Kallianpur创新表示。在生成人工智能的范式下，所提出的预测架构包括一个自编码器，将非参数多变量随机过程转化为规范的创新序列，从中根据过去样本生成未来时间序列样本，条件是它们的概率分布取决于过去样本。提出了一种新的深度学习算法，将潜在过程限制为具有匹配自编码器输入-输出条件概率分布的独立同分布序列。建立了所提出的生成式预测方法的渐近最优性和结构收敛性质。该方法在实时市场运营中涉及高度动态和波动时间序列的三个应用方面。

    arXiv:2403.05743v1 Announce Type: cross  Abstract: This paper presents a novel generative probabilistic forecasting approach derived from the Wiener-Kallianpur innovation representation of nonparametric time series. Under the paradigm of generative artificial intelligence, the proposed forecasting architecture includes an autoencoder that transforms nonparametric multivariate random processes into canonical innovation sequences, from which future time series samples are generated according to their probability distributions conditioned on past samples. A novel deep-learning algorithm is proposed that constrains the latent process to be an independent and identically distributed sequence with matching autoencoder input-output conditional probability distributions. Asymptotic optimality and structural convergence properties of the proposed generative forecasting approach are established. Three applications involving highly dynamic and volatile time series in real-time market operations a
    
[^5]: WMDP基准：通过遗忘测量和减少恶意使用

    The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning

    [https://arxiv.org/abs/2403.03218](https://arxiv.org/abs/2403.03218)

    WMDP基准是一个公开发布的数据集，包含4157个多项选择问题，用作生物安全、网络安全和化学安全危险知识的代理测量。

    

    arXiv:2403.03218v1 公告类型：交叉领域 摘要：白宫关于人工智能的行政命令强调了大型语言模型(LLMs)赋予恶意行为者开发生物、网络和化学武器的风险。为了衡量这些恶意使用的风险，政府机构和主要人工智能实验室正在开发LLMs的危险能力评估。然而，当前的评估是私人的，阻碍了进一步研究如何减少风险。此外，它们仅专注于几条高度特定的恶意使用途径。为了填补这些空白，我们公开发布了大规模杀伤性武器代理（WMDP）基准，这是一个包含4157个多项选择问题的数据集，作为生物安全、网络安全和化学安全危险知识的代理测量。WMDP由一组学术界和技术顾问联合开发，并在公开发布前严格过滤以消除敏感信息。WMDP有两个服务

    arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
    
[^6]: NiNformer: 一种具有令牌混合生成门控功能的网络中网络变压器

    NiNformer: A Network in Network Transformer with Token Mixing Generated Gating Function

    [https://arxiv.org/abs/2403.02411](https://arxiv.org/abs/2403.02411)

    提出了一种新的计算块，称为NiNformer，具有令牌混合生成门控功能，以解决注意机制在深度学习中的计算成本高昂和数据集要求大的缺点。

    

    注意机制是Transformer架构的主要组件，自引入以来，在深度学习领域取得了显著进展，跨越了许多领域和多个任务。该机制在计算机视觉中被应用为Vision Transformer ViT，并且其用途已扩展到视觉领域的许多任务，如分类、分割、目标检测和图像生成。尽管该机制非常具有表现力和能力，但其缺点是计算成本高昂，需要大规模数据集来有效优化。为了解决这些缺点，文献中提出了许多设计来减轻计算负担和缓解数据大小要求。在视觉领域的一些尝试的例子包括MLP-Mixer、Conv-Mixer、Perciver-IO等。本文介绍了一种新的计算块，作为一种

    arXiv:2403.02411v1 Announce Type: cross  Abstract: The Attention mechanism is the main component of the Transformer architecture, and since its introduction, it has led to significant advancements in Deep Learning that span many domains and multiple tasks. The Attention Mechanism was utilized in Computer Vision as the Vision Transformer ViT, and its usage has expanded into many tasks in the vision domain, such as classification, segmentation, object detection, and image generation. While this mechanism is very expressive and capable, it comes with the drawback of being computationally expensive and requiring datasets of considerable size for effective optimization. To address these shortcomings, many designs have been proposed in the literature to reduce the computational burden and alleviate the data size requirements. Examples of such attempts in the vision domain are the MLP-Mixer, the Conv-Mixer, the Perciver-IO, and many more. This paper introduces a new computational block as an 
    
[^7]: FlowCyt: 流式细胞术中多类别分类的深度学习方法比较研究

    FlowCyt: A Comparative Study of Deep Learning Approaches for Multi-Class Classification in Flow Cytometry Benchmarking

    [https://arxiv.org/abs/2403.00024](https://arxiv.org/abs/2403.00024)

    本文介绍了FlowCyt项目，提出了针对流式细胞术数据中多类别单细胞分类的全面基准测试，其中图神经网络（GNNs）通过利用图编码数据中的空间关系展现出卓越的性能。

    

    本文介绍了FlowCyt，这是第一个针对流式细胞术数据中多类别单细胞分类的全面基准测试。数据集包括来自30名患者的骨髓样本，每个细胞由十二个标记特征。地面真实标签识别了五种血液细胞类型：T淋巴细胞、B淋巴细胞、单核细胞、肥大细胞和造血干/祖细胞（HSPCs）。实验利用了有监督的归纳学习和半监督的转导学习，每名患者最多使用了100万个细胞。基线方法包括高斯混合模型、XGBoost、随机森林、深度神经网络和图神经网络（GNNs）。GNNs通过利用图编码数据中的空间关系展现出卓越的性能。该基准测试允许标准化评估临床相关的分类任务，以及探索性分析以获得对血液细胞表型的洞察。这代表了一项重要的工作。

    arXiv:2403.00024v1 Announce Type: new  Abstract: This paper presents FlowCyt, the first comprehensive benchmark for multi-class single-cell classification in flow cytometry data. The dataset comprises bone marrow samples from 30 patients, with each cell characterized by twelve markers. Ground truth labels identify five hematological cell types: T lymphocytes, B lymphocytes, Monocytes, Mast cells, and Hematopoietic Stem/Progenitor Cells (HSPCs). Experiments utilize supervised inductive learning and semi-supervised transductive learning on up to 1 million cells per patient. Baseline methods include Gaussian Mixture Models, XGBoost, Random Forests, Deep Neural Networks, and Graph Neural Networks (GNNs). GNNs demonstrate superior performance by exploiting spatial relationships in graph-encoded data. The benchmark allows standardized evaluation of clinically relevant classification tasks, along with exploratory analyses to gain insights into hematological cell phenotypes. This represents th
    
[^8]: TorchMD-Net 2.0: 分子模拟中的快速神经网络势

    TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations

    [https://arxiv.org/abs/2402.17660](https://arxiv.org/abs/2402.17660)

    TorchMD-Net 2.0是在神经网络势模型方面取得的重要进展，通过引入TensorNet等尖端结构，显著提高了计算效率，使得在计算能量和力时获得了2到10倍的性能提升。

    

    arXiv:2402.17660v1 声明类型：新的 摘要：在分子模拟中实现计算速度、预测准确性和通用适用性之间的平衡一直是一个持久的挑战。本文介绍了TorchMD-Net软件的重大进展，这是从传统力场转向基于神经网络的势的重要一步。TorchMD-Net演变成一个更全面和更多样化的框架，引入了TensorNet等尖端体系结构。通过模块化设计方法实现了这种转变，鼓励科学界内部的定制应用。最显着的增强是在计算效率方面显著改进，在TensorNet模型的能量和力计算中实现了非常显著的加速，性能提升范围从前几个版本的2倍到10倍。其他增强功能包括高度优化的邻居搜索

    arXiv:2402.17660v1 Announce Type: new  Abstract: Achieving a balance between computational speed, prediction accuracy, and universal applicability in molecular simulations has been a persistent challenge. This paper presents substantial advancements in the TorchMD-Net software, a pivotal step forward in the shift from conventional force fields to neural network-based potentials. The evolution of TorchMD-Net into a more comprehensive and versatile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a modular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a significant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2-fold to 10-fold over previous iterations. Other enhancements include highly optimized neighbor sear
    
[^9]: 一种关于一般KL正则化偏好下纳什学习从人类反馈中的理论分析

    A Theoretical Analysis of Nash Learning from Human Feedback under General KL-Regularized Preference

    [https://arxiv.org/abs/2402.07314](https://arxiv.org/abs/2402.07314)

    本论文从理论层面分析了一种关于一般偏好下纳什学习从人类反馈中的方法，通过对两个竞争的LLM进行博弈来找到一种一致生成响应的策略。

    

    来自人类反馈的强化学习（RLHF）从一个概率偏好模型提供的偏好信号中学习，该模型以一个提示和两个响应作为输入，并产生一个分数，表示对一个响应相对于另一个响应的偏好程度。迄今为止，最流行的RLHF范式是基于奖励的，它从奖励建模的初始步骤开始，然后使用构建的奖励为后续的奖励优化阶段提供奖励信号。然而，奖励函数的存在是一个强假设，基于奖励的RLHF在表达能力上有局限性，不能捕捉到真实世界中复杂的人类偏好。在这项工作中，我们为最近提出的学习范式Nash学习从人类反馈（NLHF）提供了理论洞察力，该学习范式考虑了一个一般的偏好模型，并将对齐过程定义为两个竞争的LLM之间的博弈。学习目标是找到一个一致生成响应的策略。

    Reinforcement Learning from Human Feedback (RLHF) learns from the preference signal provided by a probabilistic preference model, which takes a prompt and two responses as input, and produces a score indicating the preference of one response against another. So far, the most popular RLHF paradigm is reward-based, which starts with an initial step of reward modeling, and the constructed reward is then used to provide a reward signal for the subsequent reward optimization stage. However, the existence of a reward function is a strong assumption and the reward-based RLHF is limited in expressivity and cannot capture the real-world complicated human preference.   In this work, we provide theoretical insights for a recently proposed learning paradigm, Nash learning from human feedback (NLHF), which considered a general preference model and formulated the alignment process as a game between two competitive LLMs. The learning objective is to find a policy that consistently generates responses
    
[^10]: VerAs: 验证然后评估STEM实验报告

    VerAs: Verify then Assess STEM Lab Reports

    [https://arxiv.org/abs/2402.05224](https://arxiv.org/abs/2402.05224)

    VerAs是一个端到端的神经架构，用于验证和评估STEM实验报告。它通过利用多个维度的分析评估标准，以及针对学生提供详细反馈，帮助他们提高科学写作技巧。

    

    随着STEM教育对批判性思维能力的日益关注，科学写作在注重探究技能的课程中发挥着越来越重要的作用。最近发布的一份数据集是基于一套探究型物理课程的两组大学水平的实验报告，依赖于利用多个维度的分析评估标准，指定学科知识和优秀解释的一般组成部分。每个分析维度都以6分制进行评估，以提供详细反馈，帮助学生提高科学写作技巧。手动评估可能较慢，并且在大班中对所有学生进行一致性校准可能很困难。尽管在STEM学科的开放性问题的自动评估上已经有很多工作，但在实验报告等长篇写作中的工作要少得多。我们提出了一个端到端的神经架构，其中包括独立的验证器和评估模块，灵感来源于开放领域问题回答的方法。

    With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
    
[^11]: Mamba能学习如何学习吗？对上下文学习任务进行比较研究

    Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks

    [https://arxiv.org/abs/2402.04248](https://arxiv.org/abs/2402.04248)

    本研究评估了Mamba在上下文学习任务中的性能，并与Transformer模型进行比较。结果显示，SSMs在标准回归任务中与Transformer性能相当，在稀疏奇偶学习等任务中表现优异，但在涉及非标准检索功能的任务中表现不佳。为了解决这些局限性，引入了一种混合模型。

    

    状态空间模型（SSMs）被提出作为语言建模中替代Transformer网络的选择，通过引入门控、卷积和基于输入的令牌选择来缓解多头注意力的二次成本。虽然SSMs表现出竞争性能，但与Transformer相比，它们在上下文学习（ICL）能力方面仍然是未充分探索的。在这项研究中，我们评估了SSMs的ICL性能，着重研究了Mamba在各种任务中与Transformer模型的比较。我们的结果表明，在标准回归ICL任务中，SSMs的表现与Transformer相当，而在稀疏奇偶学习等任务中超过了Transformer。然而，在涉及非标准检索功能的任务中，SSMs表现不佳。为了解决这些局限性，我们引入了一种混合模型\variant，它将Mamba与注意力库结合起来。

    State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \variant, that combines Mamba with attention bloc
    
[^12]: PPR: 在维持冒名顶替攻击的同时增强躲避攻击对人脸识别系统的影响

    PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks on Face Recognition Systems. (arXiv:2401.08903v1 [cs.CV])

    [http://arxiv.org/abs/2401.08903](http://arxiv.org/abs/2401.08903)

    本文提出了一种名为PPR的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。该方法利用对抗样本修剪，并通过嵌入对抗扰动来增强对抗人脸样本的躲避性能。

    

    人脸识别系统上的对抗攻击可以分为两种类型：冒名顶替攻击和躲避攻击。我们观察到，在黑盒设置中成功进行冒名顶替攻击并不一定能保证在人脸识别系统上成功进行躲避攻击。本文提出了一种名为预训练修剪恢复攻击（PPR）的新型攻击方法，旨在增强躲避攻击的性能同时避免冒名顶替攻击的降级。我们的方法利用对抗样本修剪，可以将一部分对抗扰动设为零，并倾向于保持攻击性能。通过利用对抗样本修剪，我们可以修剪预训练的对抗样本，并有选择性地释放某些对抗扰动。然后，我们将对抗扰动嵌入到修剪区域，从而增强对抗人脸样本的躲避性能。通过实验证明了我们提出的攻击方法的有效性。

    Adversarial Attacks on Face Recognition (FR) encompass two types: impersonation attacks and evasion attacks. We observe that achieving a successful impersonation attack on FR does not necessarily ensure a successful dodging attack on FR in the black-box setting. Introducing a novel attack method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance the performance of dodging attacks whilst avoiding the degradation of impersonation attacks. Our method employs adversarial example pruning, enabling a portion of adversarial perturbations to be set to zero, while tending to maintain the attack performance. By utilizing adversarial example pruning, we can prune the pre-trained adversarial examples and selectively free up certain adversarial perturbations. Thereafter, we embed adversarial perturbations in the pruned area, which enhances the dodging performance of the adversarial face examples. The effectiveness of our proposed attack method is demonstrated through our experim
    
[^13]: 半无监督校准通过先验适应算法的收敛性研究

    On the Convergence of Semi Unsupervised Calibration through Prior Adaptation Algorithm. (arXiv:2401.03051v1 [cs.LG])

    [http://arxiv.org/abs/2401.03051](http://arxiv.org/abs/2401.03051)

    本文研究了一种半无监督校准算法，使用动力系统的观点证明了该算法的收敛性质，并通过实验验证了所提出的结果。

    

    校准是机器学习中的一个关键任务。Semi Unsupervised Calibration through Prior Adaptation (SUCPA)是一种校准算法，适用于大规模语言模型，其定义为一个一阶差分方程组。该方程组导出的映射具有非双曲性特征，并且存在一组非孤立不界定的固定点。本文从动力系统的视角证明了该算法的多个收敛性质。对于二类分类问题，可以证明该算法总是收敛，更具体地说，该映射是全局渐近稳定的，轨道收敛到一条固定点直线上。最后，我们对实际应用进行了数值实验，以支持所提出的结果。实验代码可在网上找到。

    Calibration is an essential key in machine leaning. Semi Unsupervised Calibration through Prior Adaptation (SUCPA) is a calibration algorithm used in (but not limited to) large-scale language models defined by a {system of first-order difference equation. The map derived by this system} has the peculiarity of being non-hyperbolic {with a non-bounded set of non-isolated fixed points}. In this work, we prove several convergence properties of this algorithm from the perspective of dynamical systems. For a binary classification problem, it can be shown that the algorithm always converges, {more precisely, the map is globally asymptotically stable, and the orbits converge} to a single line of fixed points. Finally, we perform numerical experiments on real-world application to support the presented results. Experiment codes are available online.
    
[^14]: 解开双谷之谜：透过学得特征空间的深入分析

    Unraveling the Enigma of Double Descent: An In-depth Analysis through the Lens of Learned Feature Space. (arXiv:2310.13572v1 [cs.LG])

    [http://arxiv.org/abs/2310.13572](http://arxiv.org/abs/2310.13572)

    本研究通过对学得表示的特征空间进行全面分析，揭示出双谷现象是在使用噪声数据训练的不完美模型中出现的，并提出了双谷现象的解释：模型首先学习噪声数据直到插值点，然后通过超参数化的隐式正则化来将信息与噪声分离。

    

    双谷现象在机器学习领域中呈现出一种逆直觉的特征，并且研究人员在各种模型和任务中观察到其表现。虽然对该现象在特定背景下提出了一些理论解释，但仍没有确立用于解释深度学习中出现双谷的公认理论。在本研究中，我们重新审视了双谷现象，并展示了其出现受到噪声数据影响的强烈影响。通过对学得表示的特征空间进行全面分析，我们揭示出在使用噪声数据训练的不完美模型中会出现双谷现象。我们认为双谷是模型首先学习噪声数据直到插值点，然后通过超参数化的隐式正则化来获取将信息与噪声分离的能力。我们假设双谷现象不应该在良好正则化的模型中发生。

    Double descent presents a counter-intuitive aspect within the machine learning domain, and researchers have observed its manifestation in various models and tasks. While some theoretical explanations have been proposed for this phenomenon in specific contexts, an accepted theory to account for its occurrence in deep learning remains yet to be established. In this study, we revisit the phenomenon of double descent and demonstrate that its occurrence is strongly influenced by the presence of noisy data. Through conducting a comprehensive analysis of the feature space of learned representations, we unveil that double descent arises in imperfect models trained with noisy data. We argue that double descent is a consequence of the model first learning the noisy data until interpolation and then adding implicit regularization via over-parameterization acquiring therefore capability to separate the information from the noise. We postulate that double descent should never occur in well-regulari
    
[^15]: Lemur：在自动程序验证中集成大型语言模型

    Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2310.04870](http://arxiv.org/abs/2310.04870)

    本论文提出了一种将LLMs和自动推理器结合起来进行自动程序验证的通用方法，并证明了其完备性。这个方法在一些合成和竞争基准上取得了实际的改进。

    

    LLMs在代码理解能力上的展示引发了一个问题：它们是否可以用于自动程序验证，这是一个通常需要高级抽象推理的任务，对于验证工具来说是具有挑战性的。我们提出了一种将LLMs的能力和自动推理器结合起来进行自动程序验证的通用方法。我们正式描述了这种方法论，将其作为推导规则的集合进行论证其完备性。我们将计算机推理形成为一个完备的自动验证过程，这在一组合成和竞争基准上带来了实际的改进。

    The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
    
[^16]: FOSA: 全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法

    FOSA: Full Information Maximum Likelihood (FIML) Optimized Self-Attention Imputation for Missing Data. (arXiv:2308.12388v1 [cs.LG])

    [http://arxiv.org/abs/2308.12388](http://arxiv.org/abs/2308.12388)

    FOSA是一种全信息最大似然 (FIML) 优化的自注意力缺失数据补全方法，通过融合FIML估计和自注意力机制，实现了在准确性、计算效率和适应不同数据结构方面的显著优势。

    

    数据补全中，有效地处理缺失值尤为重要，特别是在复杂的数据集中。本论文深入研究了FIML优化自注意力（FOSA）框架，这是一种融合了全信息最大似然（FIML）估计和自注意力神经网络能力的创新方法。我们的方法首先通过FIML对缺失值进行初始估计，然后通过利用自注意力机制来进一步提炼这些估计值。我们在模拟数据集和真实数据集上的全面实验证明了FOSA相对于传统的FIML技术在准确性、计算效率和适应不同数据结构方面的显著优势。有趣的是，即使在结构方程模型（SEM）可能错误规定导致子优的FIML估计的情况下，FOSA自注意力组件的稳健架构能够灵活地纠正和优化补全结果。

    In data imputation, effectively addressing missing values is pivotal, especially in intricate datasets. This paper delves into the FIML Optimized Self-attention (FOSA) framework, an innovative approach that amalgamates the strengths of Full Information Maximum Likelihood (FIML) estimation with the capabilities of self-attention neural networks. Our methodology commences with an initial estimation of missing values via FIML, subsequently refining these estimates by leveraging the self-attention mechanism. Our comprehensive experiments on both simulated and real-world datasets underscore FOSA's pronounced advantages over traditional FIML techniques, encapsulating facets of accuracy, computational efficiency, and adaptability to diverse data structures. Intriguingly, even in scenarios where the Structural Equation Model (SEM) might be mis-specified, leading to suboptimal FIML estimates, the robust architecture of FOSA's self-attention component adeptly rectifies and optimizes the imputati
    
[^17]: 利用合成数据解决数据不平衡问题：基于数据角度的基准线

    Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])

    [http://arxiv.org/abs/2308.00994](http://arxiv.org/abs/2308.00994)

    本文提出了一个简单而有效的基准线SYNAuG，利用合成数据来解决数据不平衡问题，并在多个数据集上取得了令人印象深刻的性能，超过了现有方法。

    

    我们生活在一个庞大的数据海洋中，深度神经网络也不例外。然而，这些数据表现出一种固有的不平衡现象。这种不平衡给深度神经网络产生偏见的预测带来了风险，可能导致严重的道德和社会后果。为了应对这些挑战，我们认为利用生成模型是一种有前景的方法，鉴于最近扩散模型在生成高质量图像方面的显著进展。在这项工作中，我们提出了一个简单而有效的基准线SYNAuG，利用合成数据作为解决数据不平衡问题之前使用的任务特定算法的初步步骤。这种直接的方法在CIFAR100-LT、ImageNet100-LT、UTKFace和Waterbird等数据集上取得了令人印象深刻的性能，超过了现有的任务特定方法的性能。虽然我们并不声称我们的方法作为问题的完整解决方案

    We live in a vast ocean of data, and deep neural networks are no exception to this. However, this data exhibits an inherent phenomenon of imbalance. This imbalance poses a risk of deep neural networks producing biased predictions, leading to potentially severe ethical and social consequences. To address these challenges, we believe that the use of generative models is a promising approach for comprehending tasks, given the remarkable advancements demonstrated by recent diffusion models in generating high-quality images. In this work, we propose a simple yet effective baseline, SYNAuG, that utilizes synthetic data as a preliminary step before employing task-specific algorithms to address data imbalance problems. This straightforward approach yields impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT, UTKFace, and Waterbird, surpassing the performance of existing task-specific methods. While we do not claim that our approach serves as a complete solution to the problem
    
[^18]: 全波形反演是否受益于大数据？

    Does Full Waveform Inversion Benefit from Big Data?. (arXiv:2307.15388v1 [cs.LG])

    [http://arxiv.org/abs/2307.15388](http://arxiv.org/abs/2307.15388)

    本文研究了大数据对全波形反演深度学习模型的影响，并通过实证研究证明了更大的数据集能够提高模型性能和泛化能力，同时强调模型容量需要根据数据大小进行扩展以取得最佳改进效果。

    

    本文研究了大数据对全波形反演（FWI）深度学习模型的影响。尽管众所周知，大数据可以提升深度学习模型在许多任务中的性能，但对于FWI的有效性尚未得到验证。为了填补这一空白，我们进行了一项实证研究，研究了在最近发布的大型、多结构数据集OpenFWI上训练的FWI深度学习模型的行为。特别地，我们在OpenFWI的10个2D子集的组合上训练和评估了FWI模型，这些子集总共包含了470K个数据对。我们的实验证明，更大的数据集能够提高FWI深度学习模型的性能和泛化能力。我们进一步证明，模型容量需要根据数据大小进行相应的扩展以获得最佳的改进效果。

    This paper investigates the impact of big data on deep learning models for full waveform inversion (FWI). While it is well known that big data can boost the performance of deep learning models in many tasks, its effectiveness has not been validated for FWI. To address this gap, we present an empirical study that investigates how deep learning models in FWI behave when trained on OpenFWI, a collection of large-scale, multi-structural datasets published recently. Particularly, we train and evaluate the FWI models on a combination of 10 2D subsets in OpenFWI that contain 470K data pairs in total. Our experiments demonstrate that larger datasets lead to better performance and generalization of deep learning models for FWI. We further demonstrate that model capacity needs to scale in accordance with data size for optimal improvement.
    
[^19]: 游戏理论的鲁棒强化学习处理时间耦合的干扰

    Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled Perturbations. (arXiv:2307.12062v1 [cs.LG])

    [http://arxiv.org/abs/2307.12062](http://arxiv.org/abs/2307.12062)

    这篇论文提出了一种新的鲁棒强化学习方法，通过将时间耦合的鲁棒强化学习问题视为两人零和游戏来处理问题，并通过找到近似均衡来确保代理对时间耦合干扰的鲁棒性。实验结果显示，该方法在各种连续控制任务中相比基准方法表现出显著的鲁棒性优势。

    

    鲁棒强化学习旨在训练能够在环境干扰或对抗攻击下表现良好的策略。现有方法通常假设可能干扰的空间在各个时间步骤保持不变。然而，在许多情况下，给定时间步骤上可能干扰的空间取决于过去的干扰。我们正式引入时间耦合干扰，对现有的鲁棒强化学习方法提出了新的挑战。为了应对这个挑战，我们提出了GRAD，一种新的游戏理论方法，将时间耦合鲁棒强化学习问题视为部分可观察的两人零和游戏。通过在这个游戏中找到一个近似均衡，GRAD确保了代理的对时间耦合干扰的鲁棒性。对各种连续控制任务的实证实验表明，我们提出的方法相比基准方法在标准和时间耦合干扰下具有显著的鲁棒性优势。

    Robust reinforcement learning (RL) seeks to train policies that can perform well under environment perturbations or adversarial attacks. Existing approaches typically assume that the space of possible perturbations remains the same across timesteps. However, in many settings, the space of possible perturbations at a given timestep depends on past perturbations. We formally introduce temporally-coupled perturbations, presenting a novel challenge for existing robust RL methods. To tackle this challenge, we propose GRAD, a novel game-theoretic approach that treats the temporally-coupled robust RL problem as a partially-observable two-player zero-sum game. By finding an approximate equilibrium in this game, GRAD ensures the agent's robustness against temporally-coupled perturbations. Empirical experiments on a variety of continuous control tasks demonstrate that our proposed approach exhibits significant robustness advantages compared to baselines against both standard and temporally-coupl
    
[^20]: 增强对比实例区分的语义正向对

    Semantic Positive Pairs for Enhancing Contrastive Instance Discrimination. (arXiv:2306.16122v1 [cs.CV])

    [http://arxiv.org/abs/2306.16122](http://arxiv.org/abs/2306.16122)

    提出了一种名为语义正向对集合（SPPS）的方法，可以在表示学习过程中识别具有相似语义内容的图像，并将它们视为正向实例，从而减少丢弃重要特征的风险。在多个实验数据集上的实验证明了该方法的有效性。

    

    基于实例区分的自监督学习算法有效地防止表示坍缩，并在表示学习中产生了有希望的结果。然而，在嵌入空间中吸引正向对（即相同实例的两个视图）并排斥所有其他实例（即负向对），无论它们的类别，可能导致丢弃重要的特征。为了解决这个问题，我们提出了一种方法来识别具有相似语义内容的图像，并将它们视为正向实例，命名为语义正向对集合（SPPS），从而在表示学习中减少了丢弃重要特征的风险。我们的方法可以与任何对比实例区分框架（如SimCLR或MOCO）一起使用。我们在三个数据集（ImageNet、STL-10和CIFAR-10）上进行实验证明了我们的方法的有效性。实验结果表明，我们的方法始终优于基线方法vanilla。

    Self-supervised learning algorithms based on instance discrimination effectively prevent representation collapse and produce promising results in representation learning. However, the process of attracting positive pairs (i.e., two views of the same instance) in the embedding space and repelling all other instances (i.e., negative pairs) irrespective of their categories could result in discarding important features. To address this issue, we propose an approach to identifying those images with similar semantic content and treating them as positive instances, named semantic positive pairs set (SPPS), thereby reducing the risk of discarding important features during representation learning. Our approach could work with any contrastive instance discrimination framework such as SimCLR or MOCO. We conduct experiments on three datasets: ImageNet, STL-10 and CIFAR-10 to evaluate our approach. The experimental results show that our approach consistently outperforms the baseline method vanilla 
    
[^21]: 强化学习中的结构：调查与开放问题

    Structure in Reinforcement Learning: A Survey and Open Problems. (arXiv:2306.16021v1 [cs.LG])

    [http://arxiv.org/abs/2306.16021](http://arxiv.org/abs/2306.16021)

    这项调查研究了强化学习中结构的角色和重要性，并介绍了各个子领域在提高强化学习的性能方面所做的工作。

    

    强化学习（RL）借助深度神经网络（DNN）在函数逼近方面的表达能力，已经在许多应用中取得了相当大的成功。然而，在应对多样且不可预测的动态、嘈杂信号以及庞大的状态和动作空间等各种真实场景时，其实用性仍然有限。这个限制源于诸如数据效率低、泛化能力有限、缺少安全保证和不可解释性等问题。为了克服这些挑战并在这些关键指标上提高性能，一个有前途的途径是将问题的附加结构信息纳入强化学习的学习过程中。强化学习的各个子领域已经提出了许多方法来纳入这样的归纳偏差。我们将这些多样化的方法统一到一个框架下，揭示结构在学习问题中的作用。

    Reinforcement Learning (RL), bolstered by the expressive capabilities of Deep Neural Networks (DNNs) for function approximation, has demonstrated considerable success in numerous applications. However, its practicality in addressing a wide range of real-world scenarios, characterized by diverse and unpredictable dynamics, noisy signals, and large state and action spaces, remains limited. This limitation stems from issues such as poor data efficiency, limited generalization capabilities, a lack of safety guarantees, and the absence of interpretability, among other factors. To overcome these challenges and improve performance across these crucial metrics, one promising avenue is to incorporate additional structural information about the problem into the RL learning process. Various sub-fields of RL have proposed methods for incorporating such inductive biases. We amalgamate these diverse methodologies under a unified framework, shedding light on the role of structure in the learning prob
    
[^22]: Brainformers：以效率换取简洁性

    Brainformers: Trading Simplicity for Efficiency. (arXiv:2306.00008v1 [cs.LG])

    [http://arxiv.org/abs/2306.00008](http://arxiv.org/abs/2306.00008)

    Brainformers 是一个新的深度神经网络模型，它通过使用多样层级的结构完善了 Transformer 的设计缺陷，具有更高效的训练收敛和更快的步骤时间，表现出更优秀的性能。

    

    Transformer 是自然语言处理和计算机视觉的最近成功的核心技术。Transformer 具有一个几乎统一的骨架，其中层次在前馈和自注意力之间交替以建立深度网络。在本文中，我们研究了这种设计选择，并发现更复杂的块可以更高效地完成任务。根据这个发现，我们提出了一个复杂的块，称为 Brainformer，它由各种形式的层归一化和激活函数、稀疏门控前馈层、密集前馈层、注意力层等多样层级组成。在质量和效率方面，Brainformer 总是优于现有的稠密和稀疏 Transformer。一个具有 80 亿个每个标记激活参数的 Brainformer 模型，相比于其 GLaM 对应物，表现出 2 倍更快的训练收敛和 5 倍更快的步骤时间。在下游任务评估中，Brainformer 也表现得更优秀。

    Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also de
    
[^23]: IDEA：图形对抗鲁棒性的不变因果防御

    IDEA: Invariant Causal Defense for Graph Adversarial Robustness. (arXiv:2305.15792v1 [cs.LG])

    [http://arxiv.org/abs/2305.15792](http://arxiv.org/abs/2305.15792)

    IDEA提出了一种通过学习具有强预测性和跨攻击不变性的因果特征来实现图形对抗鲁棒性的不变因果防御方法，相对于其他方法具有更高的效果和泛化性。

    

    图神经网络在各种任务中取得了显著的成功，然而，它们对于对抗性攻击的脆弱性引起了真实世界应用的关注。现有的防御方法可以抵抗一些攻击，但在其他未知攻击下会遭受难以承受的性能下降。这是由于它们依赖于有限的观察到的对抗性示例来进行优化（对抗性训练）或特定启发式来改变图形或模型结构（图纯化或鲁棒聚合）。本文提出了一种不变因果防御方法来对抗对抗性攻击（IDEA），为解决这个问题提供了一个新的视角。该方法旨在学习具有强预测标签的因果特征，并且跨攻击具有不变的预测能力，以实现图形对抗鲁棒性。通过对图形对抗攻击中的因果关系进行建模和分析，我们设计了两个不变性目标来学习因果特征。在各种数据集上进行了广泛的模拟实验和综合比较，证明了IDEA的有效性和泛化性。

    Graph neural networks (GNNs) have achieved remarkable success in various tasks, however, their vulnerability to adversarial attacks raises concerns for the real-world applications. Existing defense methods can resist some attacks, but suffer unbearable performance degradation under other unknown attacks. This is due to their reliance on either limited observed adversarial examples to optimize (adversarial training) or specific heuristics to alter graph or model structures (graph purification or robust aggregation). In this paper, we propose an Invariant causal DEfense method against adversarial Attacks (IDEA), providing a new perspective to address this issue. The method aims to learn causal features that possess strong predictability for labels and invariant predictability across attacks, to achieve graph adversarial robustness. Through modeling and analyzing the causal relationships in graph adversarial attacks, we design two invariance objectives to learn the causal features. Extens
    
[^24]: 基于摘要描述的文本检索

    Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12517](http://arxiv.org/abs/2305.12517)

    本研究针对语义检索问题，提出了一种基于摘要描述的文本检索模型，通过改进当前的文本嵌入方法，在标准最近邻搜索中取得了显著性能提升。

    

    虽然针对文本的信息提取，指令优化的大型语言模型表现优异，但对于在大规模文档集合中定位符合给定描述的文本（语义检索）并不适用。基于嵌入向量的相似度搜索可以通过查询执行检索，但嵌入中的相似度定义不明确且不一致，并且对于许多用例来说都是次优的。那么，什么是有效检索的好的查询表示？我们确定了根据内容的摘要描述检索句子的明确定义且一致的任务。我们展示了当前文本嵌入的不足，并提出了一种替代模型，在标准最近邻搜索中的表现显著提升。该模型使用通过提示LLM获得的正负样本对进行训练。虽然很容易从LLM中获得训练材料，但LLM无法直接执行检索任务。

    While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
    
[^25]: 更多脏数据下的系统识别精确恢复

    Exact Recovery for System Identification with More Corrupt Data than Clean Data. (arXiv:2305.10506v1 [cs.LG])

    [http://arxiv.org/abs/2305.10506](http://arxiv.org/abs/2305.10506)

    本文研究了在敌对环境下线性离散时间系统的系统识别问题。在周期性注入攻击时，系统动态可精确恢复样本复杂度为O(n)；当攻击以概率p进行时，精确恢复样本复杂度为O(log(n)p/(1-p)^2) 。即使有超过一半的数据受损，估计器仍可学习。

    

    本文研究了在敌对环境下线性离散时间系统的系统识别问题，并分析了两种Lasso估计器的渐近和非渐近特性，涉及到对于攻击时刻的确定性和随机性两种不同场景。由于收集的样本相关，现有的Lasso结果不适用。我们发现，当系统稳定且攻击以周期性注入时，系统动态的精确恢复的样本复杂度为O(n)，其中n是状态的维度。当攻击在每个时间实例中以概率p进行时，精确恢复所需的样本复杂度将按O(log (n)p / (1-p)^2)进行缩放。该结果在渐近状态下意味着几乎肯定收敛于真实系统动态。作为副产品，即使超过一半的数据受损，我们的估计仍然能够学习。

    In this paper, we study the system identification problem for linear discrete-time systems under adversaries and analyze two lasso-type estimators. We study both asymptotic and non-asymptotic properties of these estimators in two separate scenarios, corresponding to deterministic and stochastic models for the attack times. Since the samples collected from the system are correlated, the existing results on lasso are not applicable. We show that when the system is stable and the attacks are injected periodically, the sample complexity for the exact recovery of the system dynamics is O(n), where n is the dimension of the states. When the adversarial attacks occur at each time instance with probability p, the required sample complexity for the exact recovery scales as O(\log(n)p/(1-p)^2). This result implies the almost sure convergence to the true system dynamics under the asymptotic regime. As a by-product, even when more than half of the data is compromised, our estimators still learn th
    
[^26]: 浅层神经网络的插值性质

    Interpolation property of shallow neural networks. (arXiv:2304.10552v1 [cs.LG])

    [http://arxiv.org/abs/2304.10552](http://arxiv.org/abs/2304.10552)

    本文证明了浅层神经网络可以插值任何数据集，即损失函数具有全局最小值为零的性质，此外还给出了该全局最小值处的惯性矩阵的表征，并提供了一种实用的概率方法来寻找插值点。

    

    我们研究了超参数化神经网络的损失函数全局最小值的几何性质。在大多数优化问题中，损失函数是凸函数，这种情况下我们只有一个全局最小值，或者是非凸函数，在这种情况下我们有一个有限的全局最小值。在本文中，我们证明了在超参数化范围内，对于非小次数多项式的激活函数，浅层神经网络可以插值任何数据集，即损失函数具有全局最小值为零的性质。此外，如果存在这样的全局最小值，则全局最小值的轮廓有无穷多个点。此外，我们给出了在全局最小值处求解损失函数的海塞矩阵的表征，并在最后一节中，我们提供了一种实用的概率方法来寻找插值点。

    We study the geometry of global minima of the loss landscape of overparametrized neural networks. In most optimization problems, the loss function is convex, in which case we only have a global minima, or nonconvex, with a discrete number of global minima. In this paper, we prove that in the overparametrized regime, a shallow neural network can interpolate any data set, i.e. the loss function has a global minimum value equal to zero as long as the activation function is not a polynomial of small degree. Additionally, if such a global minimum exists, then the locus of global minima has infinitely many points. Furthermore, we give a characterization of the Hessian of the loss function evaluated at the global minima, and in the last section, we provide a practical probabilistic method of finding the interpolation point.
    
[^27]: 通过坐标变换改进梯度方法：应用于量子机器学习

    Improving Gradient Methods via Coordinate Transformations: Applications to Quantum Machine Learning. (arXiv:2304.06768v1 [quant-ph])

    [http://arxiv.org/abs/2304.06768](http://arxiv.org/abs/2304.06768)

    本文介绍了一种通过坐标变换来加速梯度优化算法、改善荒原高原和局部最小值影响的通用策略，有效提高了多种量子机器学习算法的性能。

    

    无论是经典的还是量子的机器学习算法，都大量依赖基于梯度的优化算法，如梯度下降等。总体性能取决于局部最小值和荒原高原的出现，这会减缓计算速度并导致非最优解。实际应用中，这会导致人工智能应用的计算和能源成本激增。本文介绍了一种通用策略，以加速和改善这些方法的总体性能，从而减轻了荒原高原和局部最小值的影响。我们的方法基于坐标变换，有点类似于变分旋转，在参数空间中添加了额外的方向，这些方向取决于成本函数本身，并且允许更有效地探索配置景观。我们已通过增强多种量子机器学习算法来测试我们的方法，得到了非常显着的改进。

    Machine learning algorithms, both in their classical and quantum versions, heavily rely on optimization algorithms based on gradients, such as gradient descent and alike. The overall performance is dependent on the appearance of local minima and barren plateaus, which slow-down calculations and lead to non-optimal solutions. In practice, this results in dramatic computational and energy costs for AI applications. In this paper we introduce a generic strategy to accelerate and improve the overall performance of such methods, allowing to alleviate the effect of barren plateaus and local minima. Our method is based on coordinate transformations, somehow similar to variational rotations, adding extra directions in parameter space that depend on the cost function itself, and which allow to explore the configuration landscape more efficiently. The validity of our method is benchmarked by boosting a number of quantum machine learning algorithms, getting a very significant improvement in their
    
[^28]: Fides：一种利用安全执行环境对机器学习工作负载进行结果验证的生成框架

    Fides: A Generative Framework for Result Validation of Outsourced Machine Learning Workloads via TEE. (arXiv:2304.00083v1 [cs.CR])

    [http://arxiv.org/abs/2304.00083](http://arxiv.org/abs/2304.00083)

    本文提出了一种名为Fides的框架，用于实时验证外协的机器学习工作负载。该框架采用贪心蒸馏迁移学习技术，可动态蒸馏并优化验证模型，以验证对应的服务模型。此外，该框架还能在可信执行环境中运行，以提供更高的安全性和隐私性保证。

    

    机器学习在敏感领域的部署导致了对其安全性和隐私性的重视，现有解决方案，如多方计算和基于证明的系统，给实时应用带来了很大的计算开销。本文提出了一个名为Fides的框架，用于实时验证外协的机器学习工作负载，其中采用新颖且高效的贪心蒸馏迁移学习技术，实现一种实时验证模型来较少地消耗空间和计算能力，同时运行在可信执行环境中。

    The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as autonomous driving, integrity verification of the outsourced ML workload is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time validation of outsourced ML workloads. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergenc
    
[^29]: GADFormer:一种基于注意力机制的轨迹群体异常检测模型

    GADFormer: An Attention-based Model for Group Anomaly Detection on Trajectories. (arXiv:2303.09841v1 [cs.LG])

    [http://arxiv.org/abs/2303.09841](http://arxiv.org/abs/2303.09841)

    本文提出了一种基于BERT模型的GADFormer模型，能够在轨迹上执行基于注意力的群体异常检测，相比其他深度序列模型，GADFormer表现更优，在多个公开数据集上实验表明GADFormer在检测异常群体方面取得了显著的提高。

    

    群体异常检测(GAD)可以揭示由多个成员实例组成的群体中异常行为，然而随着群体成员数量和异构性的增加，实际上的异常群体变得越来越难以检测，尤其是在无监督或半监督设定下。因此，本文提出了一种基于BERT体系结构的GAD专用模型GADFormer，它能够在轨迹上执行基于注意力的群体异常检测,并在公开数据集上展示了其在检测高精度率下的显著改进。

    Group Anomaly Detection (GAD) reveals anomalous behavior among groups consisting of multiple member instances, which are, individually considered, not necessarily anomalous. This task is of major importance across multiple disciplines, in which also sequences like trajectories can be considered as a group. However, with increasing amount and heterogenity of group members, actual abnormal groups get harder to detect, especially in an unsupervised or semi-supervised setting. Recurrent Neural Networks are well established deep sequence models, but recent works have shown that their performance can decrease with increasing sequence lengths. Hence, we introduce with this paper GADFormer, a GAD specific BERT architecture, capable to perform attention-based Group Anomaly Detection on trajectories in an unsupervised and semi-supervised setting. We show formally and experimentally how trajectory outlier detection can be realized as an attention-based Group Anomaly Detection problem. Furthermore
    
[^30]: 多天线系统中数字无线协作联邦学习的研究

    Digital Over-the-Air Federated Learning in Multi-Antenna Systems. (arXiv:2302.14648v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2302.14648](http://arxiv.org/abs/2302.14648)

    本文研究了在多天线系统中采用数字调制和空中计算的情况下，联邦学习的性能优化问题。通过结合数字调制和AirComp，提出了一种改进的联邦平均算法，以解决无线信道衰落导致的总体失真问题。

    

    本文研究了在实际的无线多输入多输出（MIMO）通信系统中，采用数字调制和空中计算（AirComp）的情况下，联邦学习（FL）的性能优化。特别考虑了一个MIMO系统，在该系统中，边缘设备使用波束形成将其本地收集的数据训练的本地FL模型传输给参数服务器（PS），以最大化可调度传输的设备数量。PS作为中央控制器，使用接收到的本地FL模型生成一个全局FL模型并广播给所有设备。由于无线网络中的带宽有限，采用了AirComp以实现高效的无线数据聚合。然而，无线信道的衰落会产生基于AirComp的FL方案中的总体失真。为了克服这一挑战，我们提出了一种改进的联邦平均（FedAvg）算法，将数字调制与AirComp相结合以减轻失真问题。

    In this paper, the performance optimization of federated learning (FL), when deployed over a realistic wireless multiple-input multiple-output (MIMO) communication system with digital modulation and over-the-air computation (AirComp) is studied. In particular, a MIMO system is considered in which edge devices transmit their local FL models (trained using their locally collected data) to a parameter server (PS) using beamforming to maximize the number of devices scheduled for transmission. The PS, acting as a central controller, generates a global FL model using the received local FL models and broadcasts it back to all devices. Due to the limited bandwidth in a wireless network, AirComp is adopted to enable efficient wireless data aggregation. However, fading of wireless channels can produce aggregate distortions in an AirComp-based FL scheme. To tackle this challenge, we propose a modified federated averaging (FedAvg) algorithm that combines digital modulation with AirComp to mitigate
    
[^31]: Bagging提供无偏差稳定性。

    Bagging Provides Assumption-free Stability. (arXiv:2301.12600v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.12600](http://arxiv.org/abs/2301.12600)

    本文证明了Bagging技术可提供无偏差稳定性，适用于各种数据分布和算法，具有良好的实证效果。

    

    Bagging是稳定机器学习模型的一个重要技术。在本文中，我们针对任何模型的稳定性推导了一个有限样本保证。我们的结果不对数据分布、基本算法的属性或协变量的维数进行任何假设。我们的保证适用于多种变体的Bagging，并且是最优的常数。实证结果验证了我们的发现，表明Bagging成功稳定了即使是高度不稳定的基本算法。

    Bagging is an important technique for stabilizing machine learning models. In this paper, we derive a finite-sample guarantee on the stability of bagging for any model. Our result places no assumptions on the distribution of the data, on the properties of the base algorithm, or on the dimensionality of the covariates. Our guarantee applies to many variants of bagging and is optimal up to a constant. Empirical results validate our findings, showing that bagging successfully stabilizes even highly unstable base algorithms.
    
[^32]: 一种行为良好的图神经近似复杂动力学的方法

    A Recipe for Well-behaved Graph Neural Approximations of Complex Dynamics. (arXiv:2301.04900v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2301.04900](http://arxiv.org/abs/2301.04900)

    本文介绍了一种行为良好的图神经网络近似复杂动力学的方法，包括必要的偏置和适当的神经网络结构，并提出了评估泛化能力和推断时预测置信度的方法。

    

    数据驱动的常微分方程近似提供了一种有前景的方法来发现动力系统模型，特别是对于缺乏明确原理的复杂系统。本文着重研究了一类由网络邻接矩阵耦合的常微分方程系统描述的复杂系统。许多现实世界中的系统，包括金融、社交和神经系统，属于这类动力学模型。我们提出了使用神经网络近似这种动力系统的关键要素，包括必要的偏置和适当的神经网络结构。强调与静态监督学习的区别，我们提倡在统计学习理论的经典假设之外评估泛化能力。为了在推断时估计预测的置信度，我们引入了一个专用的空模型。通过研究各种复杂网络动力学，我们展示了神经网络的能力。

    Data-driven approximations of ordinary differential equations offer a promising alternative to classical methods in discovering a dynamical system model, particularly in complex systems lacking explicit first principles. This paper focuses on a complex system whose dynamics is described with a system of ordinary differential equations, coupled via a network adjacency matrix. Numerous real-world systems, including financial, social, and neural systems, belong to this class of dynamical models. We propose essential elements for approximating such dynamical systems using neural networks, including necessary biases and an appropriate neural architecture. Emphasizing the differences from static supervised learning, we advocate for evaluating generalization beyond classical assumptions of statistical learning theory. To estimate confidence in prediction during inference time, we introduce a dedicated null model. By studying various complex network dynamics, we demonstrate the neural network'
    
[^33]: 基于回归的超标概率预测方法用于显著波高预测

    Exceedance Probability Forecasting via Regression for Significant Wave Height Prediction. (arXiv:2206.09821v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.09821](http://arxiv.org/abs/2206.09821)

    本论文提出了一种基于回归的超标概率预测方法，用于预测显著波高，通过利用预测来估计超标概率，取得了更好的效果。

    

    显著波高预测是海洋数据分析中的一个关键问题。预测显著波高对于估计波能产生是至关重要的。此外，及时预测大浪的到来对于确保航海作业的安全很重要。我们将预测显著波高的极端值作为超标概率预测问题。因此，我们旨在估计显著波高将超过预定义阈值的概率。通常使用概率二分类模型来解决这个任务。相反，我们提出了一种基于预测模型的新方法。该方法利用未来观测的预测来根据累积分布函数估计超标概率。我们使用来自加拿大哈利法克斯海岸的浮标数据进行了实验。结果表明，所提出的方法更好。

    Significant wave height forecasting is a key problem in ocean data analytics. Predicting the significant wave height is crucial for estimating the energy production from waves. Moreover, the timely prediction of large waves is important to ensure the safety of maritime operations, e.g. passage of vessels. We frame the task of predicting extreme values of significant wave height as an exceedance probability forecasting problem. Accordingly, we aim at estimating the probability that the significant wave height will exceed a predefined threshold. This task is usually solved using a probabilistic binary classification model. Instead, we propose a novel approach based on a forecasting model. The method leverages the forecasts for the upcoming observations to estimate the exceedance probability according to the cumulative distribution function. We carried out experiments using data from a buoy placed in the coast of Halifax, Canada. The results suggest that the proposed methodology is better
    

