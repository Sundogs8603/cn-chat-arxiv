# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting](https://rss.arxiv.org/abs/2402.01440) | 本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。 |
| [^2] | [Twisting Lids Off with Two Hands](https://arxiv.org/abs/2403.02338) | 深度强化学习结合仿真到真实世界的转移为解决物体操纵问题提供了有力支持 |
| [^3] | [Gradient Correlation Subspace Learning against Catastrophic Forgetting](https://arxiv.org/abs/2403.02334) | GCSL是一种用于减少灾难性遗忘的新颖方法，通过检测和利用不受以前任务影响的权重子空间来训练新任务。 |
| [^4] | [COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks](https://arxiv.org/abs/2403.02329) | 提出了第一个针对多传感器融合系统的鲁棒性认证框架COMMIT，以确保其针对语义攻击的稳健性。 |
| [^5] | [Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training](https://arxiv.org/abs/2403.02325) | 引入了对比区域引导（CRG）方法，实现了在视觉-语言模型中无需训练即可使模型响应视觉提示并取得显著改进。 |
| [^6] | [Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve](https://arxiv.org/abs/2403.02310) | 引入了一种有效的LLM推理调度程序Sarathi-Serve，通过分块预装填技术平衡了GPU计算饱和和单个标记处理的挑战，实现了高吞吐量和低延迟。 |
| [^7] | [Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation](https://arxiv.org/abs/2403.02302) | 本研究评估了多模态大型语言模型（MLLMs）在年龄和性别估计中的能力，对不同模型进行了比较，揭示了它们在特定任务上的优势和劣势。 |
| [^8] | [Statistical Query Lower Bounds for Learning Truncated Gaussians](https://arxiv.org/abs/2403.02300) | 本研究的主要发现是对于学习截断高斯分布估计的问题，我们证明了存在一个统计查询下界，表明在这个任务中存在着超多项式信息-计算差距。 |
| [^9] | [A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends](https://arxiv.org/abs/2403.02292) | 通过分析谷歌应用商店上1200万条隐私相关评论，研究了十年间隐私评论的大规模趋势，发现隐私评论呈现持续增长，探讨了热门和逐渐减少的隐私话题，以及不同国家用户对隐私问题看法的差异。 |
| [^10] | [Koopman-Assisted Reinforcement Learning](https://arxiv.org/abs/2403.02290) | 该论文利用Koopman算子技术将非线性系统提升到新坐标系，在其中动力学变得近似线性，从而构建两种新的强化学习算法，以解决高维状态和非线性系统中传统方程难以解决的问题。 |
| [^11] | [Physics-Informed Neural Networks with Skip Connections for Modeling and Control of Gas-Lifted Oil Wells](https://arxiv.org/abs/2403.02289) | 通过在PINC网络中引入跳跃连接并完善ODE中的某些项，该研究改进了物理信息神经网络，提高了气举油井建模的准确性和性能 |
| [^12] | [NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction](https://arxiv.org/abs/2403.02274) | NatSGD是一个包含语音、手势和演示的数据集，旨在帮助机器人通过多模式人类命令理解任务，并强调了共同考虑语音和手势的重要性。 |
| [^13] | [RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models](https://arxiv.org/abs/2403.02271) | 通过训练少样本释义模型并在训练和测试时用释义丰富数据，可以提高语言模型的性能，超出仅通过参数高效微调的效果。 |
| [^14] | [KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection](https://arxiv.org/abs/2403.02253) | 提出了一个自动化知识收集流水线，发布了一个包含20k品牌的大规模多模态品牌知识库KnowPhish，可用于加强现有基于参考的网络钓鱼检测器的性能 |
| [^15] | [A prediction rigidity formalism for low-cost uncertainties in trained neural networks](https://arxiv.org/abs/2403.02251) | 通过解决受限优化问题，提出了“预测刚性”作为一种获得任意预先训练回归器不确定性的方法，扩展了方法应用于神经网络，并在多种回归任务上展示了其有效性。 |
| [^16] | [Better Schedules for Low Precision Training of Deep Neural Networks](https://arxiv.org/abs/2403.02243) | 该研究发现了用于低精度训练的循环精度训练调度的更好选择，进一步提高了训练效率 |
| [^17] | [Neural Redshift: Random Networks are not Random Functions](https://arxiv.org/abs/2403.02241) | 本论文研究了未经训练的随机权重网络，发现即使简单的MLPs也具有强烈的归纳偏见，不同于传统观点的是，NNs并不具有固有的“简单偏见”，而是依赖于组件的作用。 |
| [^18] | [Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling](https://arxiv.org/abs/2403.02233) | 本文提供了关于使用MIM自监督预训练学习transformers的首个端到端理论，揭示了transformers如何学习到在具有空间结构的数据分布上突显特征-位置相关性的本地和多样化注意模式 |
| [^19] | [Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection](https://arxiv.org/abs/2403.02232) | 该研究通过机器学习技术全面评估了恶意软件检测，发现集成方法（如随机森林和XGBoost）相较于其他方法在恶意软件检测中表现出更高的准确性、精确度和召回率。 |
| [^20] | [TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models](https://arxiv.org/abs/2403.02221) | TPLLM提出了基于预训练大语言模型的交通预测框架，能够在历史交通数据有限的地区实现准确预测和良好泛化能力 |
| [^21] | [Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming](https://arxiv.org/abs/2403.02215) | 通过可微分编程，本研究提出了一种新框架，能够联合估计和量化物理参数以及机器学习参数化，实现了高维参数空间内的在线训练和有效贝叶斯推断。 |
| [^22] | [Mutual Information Estimation via Normalizing Flows](https://arxiv.org/abs/2403.02187) | 通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。 |
| [^23] | [Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance](https://arxiv.org/abs/2403.02185) | ChatGPT被应用于金融领域，通过知识蒸馏和迁移学习融合的训练方法，生成轻量级主题和情感分类模型，成功应用于量化投资场景。 |
| [^24] | [Not all Layers of LLMs are Necessary during Inference](https://arxiv.org/abs/2403.02181) | 推理过程中，根据输入实例的不同难易程度，本文提出了一种名为AdaInfer的算法，可以自适应地使用浅层和深层，从而节省了计算资源。 |
| [^25] | [Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models](https://arxiv.org/abs/2403.02178) | 引入对输入的扰动，通过随机掩盖思维链中的某些标记，可显著提高语言模型在推理任务中的学习效果 |
| [^26] | [Predicting large scale cosmological structure evolution with GAN-based autoencoders](https://arxiv.org/abs/2403.02171) | 使用基于GAN的自动编码器在宇宙学模拟中尝试预测结构演化，发现在2D模拟中能够很好地预测暗物质场的结构演化，但在3D模拟中表现更差，提供速度场作为输入后结果显著改善。 |
| [^27] | [Recency-Weighted Temporally-Segmented Ensemble for Time-Series Modeling](https://arxiv.org/abs/2403.02150) | 引入了Recency-Weighted Temporally-Segmented（ReWTS）集成模型，利用块状方法进行多步预测，可以专门化模型并优化预测效果。 |
| [^28] | [Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks](https://arxiv.org/abs/2403.02116) | Inf2Guard提出了一种信息论防御框架，应对三种主要类型的推断攻击，其中包括两个相互信息目标，分别用于隐私保护和效用保留。 |
| [^29] | [Iterated $Q$-Network: Beyond the One-Step Bellman Operator](https://arxiv.org/abs/2403.02107) | 引入了迭代$Q$-网络（iQN）方法，通过一次考虑多次迭代的贝尔曼算子来改进值基强化学习方法，在理论上可行，并在实验中展示其在游戏和控制环境中的优势。 |
| [^30] | [Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations](https://arxiv.org/abs/2403.02090) | 提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。 |
| [^31] | [Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection and Classification in Low Signal-to-Noise Ratio](https://arxiv.org/abs/2403.02080) | 当信噪比较低时，混合量子神经网络在雷达检测和分类问题中表现优越。 |
| [^32] | [Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations](https://arxiv.org/abs/2403.02051) | 在重尾扰动下，噪声SGD实现了差分隐私保证，适用于广泛的损失函数类，特别是非凸函数。 |
| [^33] | [Deep Neural Network for Constraint Acquisition through Tailored Loss Function](https://arxiv.org/abs/2403.02042) | 这项工作介绍了一种基于深度神经网络（DNN）和符号回归的新方法，通过设置适当的损失函数，可以直接从数据集中提取约束。 |
| [^34] | [Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes with Point Singularities](https://arxiv.org/abs/2403.02035) | 证明了ReLU$^k$神经网络在具有点奇异性的Gevrey类问题上具有指数表达能力，中间结果表明可以使用ReLU和ReLU$^2$激活组合的神经网络精确模拟高阶有限元。 |
| [^35] | [Active Learning of Mealy Machines with Timers](https://arxiv.org/abs/2403.02019) | 这篇论文提出了一种用于查询学习具有定时器的Mealy机器的算法，在实现上明显比已有算法更有效率。 |
| [^36] | [Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks](https://arxiv.org/abs/2403.02011) | 本研究提出了一种公平潜在表示的二分图变分自动编码器方法，以解决生态网络中的抽样偏差问题，通过在损失函数中引入额外的HSIC惩罚项，确保了潜在空间结构与连续变量的独立性。 |
| [^37] | [Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities](https://arxiv.org/abs/2403.02004) | 证明了粒子梯度下降算法对于一般化的log-Sobolev和Polyak-Lojasiewicz不等式模型的收敛速度，以及推广了Bakry-Emery定理。 |
| [^38] | [On Fractional Moment Estimation from Polynomial Chaos Expansion](https://arxiv.org/abs/2403.01948) | 本文提出了一种通过多项式混沌展开直接估计分数矩的新方法，可用于不确定性量化中的概率分布估计，实验结果表明该方法性能优越。 |
| [^39] | [A Generative Model of Symmetry Transformations](https://arxiv.org/abs/2403.01946) | 通过构建一种生成模型来明确捕捉数据中的对称变换，从而提高模型的泛化能力和稳健性。 |
| [^40] | [Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification](https://arxiv.org/abs/2403.01944) | 提出了辅助傅立叶基增强（AFA）的方法，通过在频域进行增强，填补了视觉增强遗留的增强差距，提高了模型的鲁棒性。 |
| [^41] | [Mitigating Label Noise on Graph via Topological Sample Selection](https://arxiv.org/abs/2403.01942) | 提出了一种通过利用图数据的拓扑信息来增强信息选择过程的$“\textit{拓扑样本选择}$”（TSS）方法。 |
| [^42] | [FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization](https://arxiv.org/abs/2403.01922) | 本研究将线性量化应用于FPGA-based soft sensors中，显著提高了神经网络模型精度，通过硬件优化实现了降低均方误差和提升推理速度，为实时流体流量估计提供了高效、精确的替代方案 |
| [^43] | [Matrix Completion with Convex Optimization and Column Subset Selection](https://arxiv.org/abs/2403.01919) | 该方法结合了列子集选择和低秩矩阵完成问题的理论基础，提出使用凸优化解决矩阵恢复问题，同时通过实验验证了算法的正确性和性能。 |
| [^44] | [Towards Continuous Assurance Case Creation for ADS with the Evidential Tool Bus](https://arxiv.org/abs/2403.01918) | 基于证据工具总线的自适应驾驶系统连续保障案例创建，实现了从预定义模式构建和持续维护保障案例，有效解决了系统需求变化和组件恶化带来的保障案例维护复杂性。 |
| [^45] | [Capacity of the Hebbian-Hopfield network associative memory](https://arxiv.org/abs/2403.01907) | Hopfield提出了一种Hebbian学习规则的神经网络模型，研究了关联记忆的容量，指出网络的容量与模式大小线性相关，提出了容量预测值，并使用两个著名模式的吸引盆地来探讨相关问题。 |
| [^46] | [Universality of reservoir systems with recurrent neural networks](https://arxiv.org/abs/2403.01900) | 讨论了具有循环神经网络的储层系统的逼近能力和统一强普适性，可以通过并行串联RNN储层构建这种类型的系统 |
| [^47] | [Robustness Bounds on the Successful Adversarial Examples: Theory and Practice](https://arxiv.org/abs/2403.01896) | 本文提出了一个新的成功对抗样本概率上限的理论界限，取决于扰动范数、核函数以及训练数据集中最接近的不同标签对之间的距离，并且实验证明了该理论结果的有效性。 |
| [^48] | [Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series](https://arxiv.org/abs/2403.01895) | 提出了一种无监督距离度量学习方法FCM-wDTW，用于多变量时间序列异常检测，通过将原始数据编码成潜在空间并通过聚类中心揭示正常维度关系，在实验中表现出竞争力的准确性和效率。 |
| [^49] | [Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks](https://arxiv.org/abs/2403.01888) | 通过引入用户友好的Python软件包，研究提出了有效的并行HPO方法，避免长时间等待实现快速评估。 |
| [^50] | [ICLN: Input Convex Loss Network for Decision Focused Learning](https://arxiv.org/abs/2403.01875) | 提出了输入凸损失网络（ICLN），通过输入凸神经网络学习任务损失，为决策集中学习提供了全局替代损失。 |
| [^51] | [A Survey on Evaluation of Out-of-Distribution Generalization](https://arxiv.org/abs/2403.01874) | 该论文是针对外部分布泛化评估进行的首次努力，重点在于评估模型的OOD泛化能力以及泛化的表现好坏。 |
| [^52] | [Improving generalisation via anchor multivariate analysis](https://arxiv.org/abs/2403.01865) | 引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。 |
| [^53] | [RCoCo: Contrastive Collective Link Prediction across Multiplex Network in Riemannian Space](https://arxiv.org/abs/2403.01864) | 在传统欧氏空间之外的黎曼空间中，针对多重社交网络中的对比集体链接预测问题，本文提出了新方法。 |
| [^54] | [Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences](https://arxiv.org/abs/2403.01857) | 本文通过比较强化学习从人类反馈学习（RLHF）范式与最近提出的直接偏好优化（DPO）范式，对学习人类偏好进行了深入探讨，推导出了次优差距的统计界限，并提供了收敛速度的分析。 |
| [^55] | [One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models](https://arxiv.org/abs/2403.01849) | 本文研究了预训练视觉-语言模型的对抗鲁棒性，提出了一种通过学习强韧文本提示来改善对抗攻击韧性的方法，称为Adversarial Prompt Tuning（APT），并在多个数据集和数据稀疏方案上进行了全面实验验证。 |
| [^56] | [NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models](https://arxiv.org/abs/2403.01845) | NASH是一种将神经架构搜索应用于机器学习硬件的新方法，可以帮助硬件设计实现高吞吐量、低延迟和优越的准确性表现。 |
| [^57] | [Making Pre-trained Language Models Great on Tabular Prediction](https://arxiv.org/abs/2403.01841) | 提出了一种专门为表格数据预测而预训练的语言模型TP-BERTa，通过新颖的相对大小标记化方法和内部特征关注方法解决了预训练语言模型在数值特征值上的不兼容性问题 |
| [^58] | [Macroscopic auxiliary asymptotic preserving neural networks for the linear radiative transfer equations](https://arxiv.org/abs/2403.01820) | 该论文提出了一种宏观辅助渐近保持神经网络（MA-APNN）方法，用于解决具有多尺度性质和高维度的时变线性辐射传递方程，利用了物理信息神经网络框架，并设计了新的自适应指数加权渐近保持损失函数，有效性得到了数值示例的验证。 |
| [^59] | [Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator](https://arxiv.org/abs/2403.01805) | 本文介绍了Tsallis熵正则化方法，用于解决线性可解MDP和线性二次调节器问题，能够平衡探索和获得的控制法则的稀疏性。 |
| [^60] | [COLA: Cross-city Mobility Transformer for Human Trajectory Simulation](https://arxiv.org/abs/2403.01801) | 研究通过将强大的Transformer模型与外部移动数据相结合，探讨了跨城市人类轨迹模拟的问题，解决了知识转移中的适应性挑战。 |
| [^61] | [Towards Fair and Efficient Learning-based Congestion Control](https://arxiv.org/abs/2403.01798) | Astraea是一种新的学习型拥塞控制算法，通过多智能体深度强化学习框架，实现了快速收敛到公平性的稳定性。 |
| [^62] | [Hybrid data-driven and physics-informed regularized learning of cyclic plasticity with Neural Networks](https://arxiv.org/abs/2403.01776) | 提出了一种利用混合数据驱动和物理信息规范的神经网络学习循环塑性的方法，实现了在少量训练数据情况下的高精度和稳定性，并且简化高效了模型架构。 |
| [^63] | [Improving out-of-distribution generalization in graphs via hierarchical semantic environments](https://arxiv.org/abs/2403.01773) | 通过生成分层语义环境，本文提出了一种新方法来增强图的不变学习，以处理分布转移。 |
| [^64] | [A Safe Screening Rule with Bi-level Optimization of $\nu$ Support Vector Machine](https://arxiv.org/abs/2403.01769) | 提出了一种具有双层优化的安全筛选规则的$\nu$支持向量机方法，可以在训练前筛选出不活跃样本，降低计算成本，同时不损失预测准确性。 |
| [^65] | [Open-world Machine Learning: A Review and New Outlooks](https://arxiv.org/abs/2403.01759) | 通过研究未知拒绝、新类别发现和类别增量学习，本文拓展了开放世界机器学习领域，提出了未来研究的多个潜在方向 |
| [^66] | [AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network](https://arxiv.org/abs/2403.01758) | 利用反事实推理构建的 AFBT GAN 增强了对认知衰退的可解释性和诊断性能 |
| [^67] | [How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems](https://arxiv.org/abs/2403.01757) | 提出了一种使用多模态LLM进行优化的框架，能够更全面地理解优化问题，类似于人类认知过程，并且提供了更细致和有效的分析。 |
| [^68] | [Diffusion-TS: Interpretable Diffusion for General Time Series Generation](https://arxiv.org/abs/2403.01742) | 提出了一种新颖的基于扩散的框架 Diffusion-TS，结合了编码器-解码器变压器和解耦时间表示，通过直接重建样本而非噪声生成高质量的多变量时间序列样本，旨在实现时间序列的解释性和真实性。 |
| [^69] | [ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution](https://arxiv.org/abs/2403.01738) | ComS2T是一种互补的时空学习系统，通过将神经结构分为稳定的新皮质和动态海马体，实现模型对数据的自适应演化。 |
| [^70] | [Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy](https://arxiv.org/abs/2403.01734) | 提出了一种名为RbSL的新方法，用于解决具有不同目标的安全关键任务，克服了传统方法在复杂环境中处理多样约束时的限制。 |
| [^71] | [Statistical Mechanics of Dynamical System Identification](https://arxiv.org/abs/2403.01723) | 统计力学提供了一种稀疏方程发现算法的分析方法，通过两级贝叶斯推断问题来平衡数据拟合和简洁性，特别在低数据极限中能够量化不确定性。 |
| [^72] | [L0 Regularization of Field-Aware Factorization Machine through Ising Model](https://arxiv.org/abs/2403.01718) | 通过使用伊辛模型作为L0正则化方法，改进了场感知因子分解机（FFM）的泛化性能，并能同时确定多个群组的最佳特征组合。 |
| [^73] | [Soft-constrained Schrodinger Bridge: a Stochastic Control Approach](https://arxiv.org/abs/2403.01717) | 提出了软约束薛定谔桥(SSB)控制问题，在允许终端分布与预先指定分布不同的情况下，惩罚两者之间的Kullback-Leibler散度。理论上推导出了SSB解，显示最优控制过程的终端分布是μT和其他分布的几何混合，并将结果扩展到时间序列设置。 |
| [^74] | [Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study](https://arxiv.org/abs/2403.01709) | 本研究旨在探究利用大语言模型（LLMs）生成建筑设计决策的可行性，并尝试应用于架构决策记录（ADR）生成。 |
| [^75] | [DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling](https://arxiv.org/abs/2403.01695) | 介绍了DyCE，一个动态可配置的提前退出框架，将设计考虑从彼此和基础模型解耦 |
| [^76] | [CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables](https://arxiv.org/abs/2403.01673) | CATS通过构建辅助时间序列作为外生变量，有效地表示和整合多元时间序列之间的关系，提高了多元时间序列预测的效果，并且相较于之前的模型大幅减少了复杂性和参数。 |
| [^77] | [Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation](https://arxiv.org/abs/2403.01671) | 本文研究了如何在多元概率分布中测试排列不变性、估计排列不变密度以及分析排列不变函数类的度量熵，比较了它们与没有排列不变性的函数类的差异。 |
| [^78] | [Quantifying and Predicting Residential Building Flexibility Using Machine Learning Methods](https://arxiv.org/abs/2403.01669) | 本研究提出了两种灵活性指标，并利用机器学习模型来预测住宅建筑在不同时间范围内的灵活性，其中长短期记忆（LSTM）模型表现最佳。 |
| [^79] | [Improving Adversarial Energy-Based Model via Diffusion Process](https://arxiv.org/abs/2403.01666) | 通过将EBMs嵌入到扩散步骤中并引入变分后验分布，有效改进了对抗性能量模型的生成能力。 |
| [^80] | [Geometry and Stability of Supervised Learning Problems](https://arxiv.org/abs/2403.01660) | 引入了监督学习问题之间的风险距离概念，通过风险距离可以量化问题的稳定性变化，并探索了监督学习问题空间的几何结构。 |
| [^81] | [Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data](https://arxiv.org/abs/2403.01653) | 提出了两种基于深度学习的区域太阳能发电预测方法，可以同时考虑历史发电量和天气数据，克服了以往在区域预测中忽视地理位置信息或生成大量预测模型的限制。 |
| [^82] | [You Need to Pay Better Attention](https://arxiv.org/abs/2403.01643) | 提出了三种新的注意力机制，在效率和学习能力方面优于标准的多头注意力，提高了Transformer模型的性能和更广泛的部署能力。 |
| [^83] | [Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning](https://arxiv.org/abs/2403.01642) | 该研究提出了一种通过快速集成学习优化化学传感器阵列的策略，引入了蓝色和绿色两种工作模式，通过选择性激活关键传感器显著降低能耗而不影响检测准确性。 |
| [^84] | [Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models](https://arxiv.org/abs/2403.01639) | 本文首次在高斯混合模型背景下进行理论研究，证明了将扩散指导纳入不仅提升了分类置信度，而且减少了分布多样性。 |
| [^85] | [Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks](https://arxiv.org/abs/2403.01636) | 通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。 |
| [^86] | [Application of Neural Ordinary Differential Equations for Tokamak Plasma Dynamics Analysis](https://arxiv.org/abs/2403.01635) | 本研究引入了神经常微分方程（Neural ODEs）来模拟托卡马克等离子体内部的能量转移过程，通过从实验数据中推导扩散性参数，实现了精确的能量相互作用模拟，为提高托卡马克性能提供了新思路。 |
| [^87] | [Critical windows: non-asymptotic theory for feature emergence in diffusion models](https://arxiv.org/abs/2403.01633) | 我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。 |
| [^88] | [Improving LLM Code Generation with Grammar Augmentation](https://arxiv.org/abs/2403.01632) | SynCode是一个新框架，结合编程语言的语法和DFA mask store，在LLMs中生成代码过程中获得96.07%的句法错误降低，并展现出提高句法精度的重大影响。 |
| [^89] | [Recent Advances, Applications, and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2023 Symposium](https://arxiv.org/abs/2403.01628) | ML4H 2023研讨会通过研究圆桌讨论会促进了与会者和资深研究人员就健康领域的时事话题展开讨论，并总结了其中涉及的最新进展、应用和开放挑战。 |
| [^90] | [ML4PhySim : Machine Learning for Physical Simulations Challenge (The airfoil design)](https://arxiv.org/abs/2403.01623) | 本竞赛旨在推动发展新的ML技术，使用一致的评估框架解决物理问题，首个竞赛解决了利用ML-based代理方法改善计算效率与性能权衡的挑战。 |
| [^91] | [Machine Learning vs Deep Learning: The Generalization Problem](https://arxiv.org/abs/2403.01621) | 深度学习模型具备泛化到训练范围之外的固有能力，这对于现实世界中至关重要。 |
| [^92] | [Partial Federated Learning](https://arxiv.org/abs/2403.01615) | 提出了一种新算法部分联邦学习(PartialFL)，允许将数据模态或其中间表示的子集提供给服务器训练模型，解决了部分数据限制的联邦学习问题。 |
| [^93] | [Respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy](https://arxiv.org/abs/2403.01607) | 该研究评估了资源高效的在线RNN算法在放疗治疗过程中准确预测呼吸运动的能力。 |
| [^94] | [Towards Provable Log Density Policy Gradient](https://arxiv.org/abs/2403.01605) | 提出对数密度梯度方法来估计策略梯度，修正残差误差，有望改善强化学习方法的样本复杂度。 |
| [^95] | [SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos](https://arxiv.org/abs/2403.01599) | 通过研究步骤和状态之间的因果关系，本文提出了SCHEMA方法，将每个步骤显式表示为状态变化，并追踪教学视频中的状态变化。 |
| [^96] | [The Hidden Attention of Mamba Models](https://arxiv.org/abs/2403.01590) | Mamba模型可以被视为关注驱动的模型，这与变压器中的自注意力层有所不同，并且通过可解释性方法可以深入了解其内部工作。 |
| [^97] | [On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation](https://arxiv.org/abs/2403.01582) | 该论文提出了一种新的模型不可知的多源无监督领域自适应（MMDA）设置，允许多样化的源模型，解决了源模型选择问题。 |
| [^98] | [Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa](https://arxiv.org/abs/2403.01571) | 通过将Kullback-Leibler散度与Cohen's Kappa相关联，限制了分类性能的最大限度 |
| [^99] | [SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction](https://arxiv.org/abs/2403.01570) | 提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。 |
| [^100] | [Transformers for Supervised Online Continual Learning](https://arxiv.org/abs/2403.01554) | 本文提出了一种方法，利用变压器的上下文学习能力以及它们与元学习的关联，用于监督在线持续学习。 |
| [^101] | [In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://arxiv.org/abs/2403.01548) | 本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。 |
| [^102] | [Hyperspectral Image Analysis in Single-Modal and Multimodal setting using Deep Learning Techniques](https://arxiv.org/abs/2403.01546) | 该研究利用深度学习技术处理高光谱图像分析中的高维度和有限空间分辨率挑战，通过多模态学习整合不同数据，应用对抗学习和知识蒸馏来克服领域差异和缺失模态问题，并定制深度学习架构适配HSI数据特征。 |
| [^103] | [Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity](https://arxiv.org/abs/2403.01540) | 该算法结合了分层联邦学习中的梯度聚合和模型聚合，通过量化提高通信效率，表现出对统计异质性的鲁棒性。 |
| [^104] | [Mixed-Strategy Nash Equilibrium for Crowd Navigation](https://arxiv.org/abs/2403.01537) | 通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。 |
| [^105] | [Fast Ergodic Search with Kernel Functions](https://arxiv.org/abs/2403.01536) | 提出了一种使用核函数的快速遍历搜索方法，其在搜索空间维度上具有线性复杂度，可以推广到李群，并且通过数值测试展示比现有算法快两个数量级。 |
| [^106] | [Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models](https://arxiv.org/abs/2403.01535) | NGG是一个神经图生成器，通过潜在扩散模型实现了特征条件图生成，具有模拟复杂图模式和控制图生成过程的显著能力。 |
| [^107] | [Machine learning predicts long-term mortality after acute myocardial infarction using systolic time intervals and routinely collected clinical data](https://arxiv.org/abs/2403.01533) | 该研究利用机器学习模型和新生物标志物探索长期死亡率预测，为心脏病患者提供更准确的医疗决策支持。 |
| [^108] | [Data-driven local operator finding for reduced-order modelling of plasma systems: II. Application to parametric dynamics](https://arxiv.org/abs/2403.01532) | 本研究介绍了一种新的数据驱动局部算子查找算法Phi Method，展示了其在学习参数动力学中的有效性，并应用于预测系统在未见参数空间中的行为。 |
| [^109] | [Data-driven local operator finding for reduced-order modelling of plasma systems: I. Concept and verifications](https://arxiv.org/abs/2403.01523) | 引入了"Phi Method"，通过约束回归和候选项库发现离散化的微分方程系统，成功导出了可靠且稳健的简化等离子体模型，具有广泛的应用前景。 |
| [^110] | [Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models](https://arxiv.org/abs/2403.01518) | 在线适应可以将参数转变为时间变化状态，提供一种具有内存权重记忆的上下文长度扩展形式，更符合神经科学中记忆概念，且在提升整体预测性能时尤其有趣。 |
| [^111] | [Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network](https://arxiv.org/abs/2403.01501) | 本文研究了将GNNs应用于无监督方式识别特定类型网络流的方法，通过设计一个引入了图注意力机制的编码器来获取图嵌入。 |
| [^112] | [Normalising Flow-based Differentiable Particle Filters](https://arxiv.org/abs/2403.01499) | 本文提出了一种基于正规化流的可微粒子滤波器框架，能够实现有效的概率密度估计并在复杂环境中灵活学习这些模块。 |
| [^113] | [ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis](https://arxiv.org/abs/2403.01493) | ConvTimeNet是一种新颖的深度分层全卷积网络，通过自适应分段和全卷积块设计，有效捕捉了全局序列和跨变量的依赖性。 |
| [^114] | [Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection](https://arxiv.org/abs/2403.01485) | 本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法 |
| [^115] | [Representation Learning on Heterophilic Graph with Directional Neighborhood Attention](https://arxiv.org/abs/2403.01475) | 提出了具有方向性邻域注意力的Directional Graph Attention Network（DGAT），能够有效结合特征注意力和全局方向信息，通过新型拉普拉斯矩阵减少节点之间的扩散距离，并引入拓扑引导的邻域修剪和边添加机制来提升异质图的表示学习性能。 |
| [^116] | [WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection](https://arxiv.org/abs/2403.01472) | 本研究提出了一种名为WARDEN的新方法，通过在嵌入文本中加入多个可能的水印方向，增加了水印消除的难度，以应对EaaS中背门水印被移除的新威胁。 |
| [^117] | [Preserving correlations: A statistical method for generating synthetic data](https://arxiv.org/abs/2403.01471) | 提出了一种方法来生成具有统计代表性的合成数据，能够在合成数据集中保持原始数据集中特征之间的相关性，并提供可调整的隐私级别。 |
| [^118] | [Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation](https://arxiv.org/abs/2403.01467) | 本论文提出了一种名为GraphCTA的新方法，通过协作的模型适应和图适应来解决无源图领域自适应问题。 |
| [^119] | [One-Step Multi-View Clustering Based on Transition Probability](https://arxiv.org/abs/2403.01460) | 提出了一种基于转移概率的一步多视角聚类方法，通过学习从锚点到类别的转移概率，以及从样本到类别的转移概率，获得样本和锚点的软标签矩阵，增强了聚类的解释性。 |
| [^120] | [Enhancing Data Provenance and Model Transparency in Federated Learning Systems - A Database Approach](https://arxiv.org/abs/2403.01451) | 提出了一种增强联邦学习系统中数据溯源和模型透明性的方法，通过加密技术和高效的模型管理来追踪数据变换。 |
| [^121] | [Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting](https://arxiv.org/abs/2403.01438) | 提出了面向智能电网负荷预测的隐私保护协同分裂学习框架，通过将深度神经网络模型分裂为Grid Station（GS）和服务提供商（SP）部分，实现智能电表数据的隐私保护和负载预测的个性化模型训练。 |
| [^122] | [On Diffusion Process in SE(3)-invariant Space](https://arxiv.org/abs/2403.01430) | 通过数学方法揭示了在SE(3)-不变空间中的扩散机制，提出了准确且无需投影的扩散模型，可以提高生成路径的性能和速度，并为其他系统的SE(3)-不变性提供了有价值的见解。 |
| [^123] | [Introduction to Algogens](https://arxiv.org/abs/2403.01426) | Algogens是生成型人工智能与传统算法相结合的一种方法，可以更有效地解决复杂挑战，具有更好的适应性和效率。 |
| [^124] | [Collective Certified Robustness against Graph Injection Attacks](https://arxiv.org/abs/2403.01423) | 本文提出了针对图注入攻击的GNN的集体证书，通过二进制整数二次约束线性规划构建问题，并开发了定制的线性化技术，极大地提高了认证性能。 |
| [^125] | [The Implicit Bias of Heterogeneity towards Invariance and Causality](https://arxiv.org/abs/2403.01420) | 异质性对于回归任务中出现因果性的贡献解释了为何大型语言模型能够从关联性训练中揭示因果关联。 |
| [^126] | [Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional Model Aggregation](https://arxiv.org/abs/2403.01417) | Asyn2F是一种异步联邦学习框架，通过双向模型聚合实现了服务器异步聚合多个本地模型并得到新的全局模型，同时训练工作者可以在训练过程中将全局模型的新版本聚合到本地模型中，解决了过时信息问题。 |
| [^127] | [Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks](https://arxiv.org/abs/2403.01400) | 本文提出了一种用于整合多个图预训练任务的新颖的实例级框架Weigh And Select（WAS），其中通过解耦的连体网络组合了权衡和选择这两个协作过程 |
| [^128] | [Fusion of Gaussian Processes Predictions with Monte Carlo Sampling](https://arxiv.org/abs/2403.01389) | 本文提出了一种将高斯过程预测通过蒙特卡洛采样进行融合的新方法，旨在提高预测准确性。 |
| [^129] | [A Comprehensive Survey of Federated Transfer Learning: Challenges, Methods and Applications](https://arxiv.org/abs/2403.01387) | 联邦学习是一种分布式机器学习范例，联邦迁移学习则将迁移学习引入联邦学习中，以解决不同参与者数据特征空间和分布的差异所带来的挑战。 |
| [^130] | [On the Compressibility of Quantized Large Language Models](https://arxiv.org/abs/2403.01384) | 研究在内存受限设备上应用数据压缩技术以加速量化LLM推理过程的一项初步工作。 |
| [^131] | [Large-scale variational Gaussian state-space models](https://arxiv.org/abs/2403.01371) | 该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。 |
| [^132] | [A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement](https://arxiv.org/abs/2403.01369) | 本文研究了Wav2Vec2嵌入在单通道语音增强中的应用，发现SSL表示对增强任务几乎没有任何价值。 |
| [^133] | [Bandit Profit-maximization for Targeted Marketing](https://arxiv.org/abs/2403.01361) | 该论文研究了针对目标营销的强盗利润最大化问题，并提出了在敌对强盗情境下的近乎最优算法。 |
| [^134] | [a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification](https://arxiv.org/abs/2403.01355) | 提出了一种架构无关的检测成本函数（a-DCF），适用于评估抵御欺骗攻击的自动说话人验证（ASV）解决方案。 |
| [^135] | [Improving Uncertainty Sampling with Bell Curve Weight Function](https://arxiv.org/abs/2403.01352) | 使用钟形曲线权重函数改进了不确定性采样方法，提高了监督学习效率。 |
| [^136] | [SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization](https://arxiv.org/abs/2403.01348) | SANGRIA是一个基于堆叠自编码器神经网络与梯度提升树的室内定位框架，相比其他先进框架，能够实现更低的平均定位误差。 |
| [^137] | [Improve Cost Efficiency of Active Learning over Noisy Dataset](https://arxiv.org/abs/2403.01346) | 提出了一种新的采样函数，通过在更广泛的范围内进行采样，限制了嘈杂和正标签的选择，提高了成本效率。 |
| [^138] | [Mitigating the Bias in the Model for Continual Test-Time Adaptation](https://arxiv.org/abs/2403.01344) | 本文提出了一种方法来缓解持续测试时间自适应模型中的预测偏差问题，并通过制定类别特定的指数加权移动平均目标原型来改善CTA场景中的性能。 |
| [^139] | [Uniform $\mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric Functions, Embedding Dimensions, and Polynomial Representations](https://arxiv.org/abs/2403.01339) | 该论文提出了一种新的方法，可以通过$G$不变多项式一致逼近$G$不变函数，同时能够获得反对称函数的统一$\mathcal{C}^k$逼近。这种方法不仅可以应用于完全对称函数，还可以将嵌入维度独立于目标函数的正则性、逼近准确度和$k$。 |
| [^140] | [Chaining thoughts and LLMs to learn DNA structural biophysics](https://arxiv.org/abs/2403.01332) | 通用语言模型chatGPT 3.5-turbo的微调，在学习DNA结构生物物理学方面显示出新的潜力和优势。 |
| [^141] | [Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models](https://arxiv.org/abs/2403.01329) | 该论文引入了定制的非平稳（BNS）求解器，提高了扩散和流动模型的采样效率，具有小参数空间、快速优化、样本多样性，并且在低中 NFE 范围内接近标准精炼方法。 |
| [^142] | [High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media](https://arxiv.org/abs/2403.01318) | 提出了高维尾指数回归方法，利用正则化估计和去偏方法进行推断，支持理论的仿真研究，并在社交媒体病毒帖子文本分析中应用。 |
| [^143] | [Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits](https://arxiv.org/abs/2403.01317) | 提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。 |
| [^144] | [Near-optimal Per-Action Regret Bounds for Sleeping Bandits](https://arxiv.org/abs/2403.01315) | 该论文提出了针对睡眠臂决策问题的接近最优每次行动遗憾界，通过直接最小化每次行动遗憾，使用Generalized EXP3、EXP3-IX和Tsallis entropy下的FTRL方法，获得了较之现有方法更好的界。 |
| [^145] | [VNLP: Turkish NLP Package](https://arxiv.org/abs/2403.01309) | VNLP是首个专门针对土耳其语开发的自然语言处理工具包，包含多种NLP工具，其中的标记分类模型基于“上下文模型”，支持多种任务如情感分析、命名实体识别等。 |
| [^146] | [VBART: The Turkish LLM](https://arxiv.org/abs/2403.01308) | VBART是第一个土耳其序列到序列大语言模型，通过与BART和mBART模型结合形成了紧凑型LLM，并在多个任务中表现出色，为土耳其自然语言处理研究开辟了新的可能性。 |
| [^147] | [ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation](https://arxiv.org/abs/2403.01306) | 提出一种新的度量标准，图像描述具体性，用于评估标题文本的具体性和相关性，以帮助在多模态学习中隔离提供最强信号的最具体样本。 |
| [^148] | [Supplier Recommendation in Online Procurement](https://arxiv.org/abs/2403.01301) | 本研究提出了一个推荐系统，用于在道路货运在线采购中辅助进行供应商发现，能够提供个性化的供应商推荐，考虑到客户的需求和偏好。 |
| [^149] | [A Photonic Physically Unclonable Function's Resilience to Multiple-Valued Machine Learning Attacks](https://arxiv.org/abs/2403.01299) | 该论文研究了光子物理不可克隆函数对多值机器学习攻击的韧性，发现需要大量CRPs才能成功训练模型进行预测，从而展示了光子PUF对此类攻击的抵抗能力。 |
| [^150] | [NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention](https://arxiv.org/abs/2403.01273) | NoMAD-Attention提出了一种高效的注意力算法，通过在CPU上使用寄存器内查找取代MAD操作，以实现LLM推断的快速计算。 |
| [^151] | [Can a Confident Prior Replace a Cold Posterior?](https://arxiv.org/abs/2403.01272) | 探讨了将后验调整替换为增加信心的先验分布的可行性，引入了实用的“DirClip”先验和“信心先验”，提供了对信心先验的一般见解。 |
| [^152] | [Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach](https://arxiv.org/abs/2403.01268) | 该论文通过一种信息论方法，旨在保证联邦学习在面临数据重构攻击时具有强大的隐私保证。 |
| [^153] | [Dissecting Language Models: Machine Unlearning via Selective Pruning](https://arxiv.org/abs/2403.01267) | 介绍了一种针对大型语言模型的机器去学习方法，通过选择性修剪神经元来实现去学习，发现LLMs中的神经元在特定任务中具有不同的重要性。 |
| [^154] | [SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code](https://arxiv.org/abs/2403.01248) | SceneCraft是一个LLM代理，可将文本描述转换为Blender代码，实现渲染高达一百个三维资产的复杂场景，通过先建模空间关系再编写Python脚本，并借助视觉-语言基础模型进行场景优化和库学习来解决挑战。 |
| [^155] | [AcME-AD: Accelerated Model Explanations for Anomaly Detection](https://arxiv.org/abs/2403.01245) | AcME-AD是一种根植于可解释人工智能原则的新方法，通过提供局部特征重要性评分和假设分析等方法，超越了模型特定或资源密集型的可解释性技术的限制。 |
| [^156] | [Augmenting Automation: Intent-Based User Instruction Classification with Machine Learning](https://arxiv.org/abs/2403.01242) | 提出了一种通过引入基于意图的用户指令分类和机器学习技术的新颖方法，从而增强自动化系统的灵活性和适应性。 |
| [^157] | [Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings](https://arxiv.org/abs/2403.01234) | 本文提出了一种利用深度核学习（DKL）的活跃学习方法，通过与传统变分自动编码器（VAEs）的对比分析，创造了优先考虑分子功能性的潜在空间，并且通过迭代重新计算嵌入向量实现了更好组织的潜在空间。 |
| [^158] | [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://arxiv.org/abs/2403.01232) | Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。 |
| [^159] | [REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild](https://arxiv.org/abs/2403.01229) | 通过视频和可穿戴传感器数据训练的机器学习模型可以隐私保护地识别说话状态，解决了在野外获取个人录音困难的问题 |
| [^160] | [A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations](https://arxiv.org/abs/2403.01221) | 本文提出了一种两阶段算法，用于找到实例组以及成本有效的多实例反事实解释，填补了先前工作中未解决的空白。 |
| [^161] | [Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy](https://arxiv.org/abs/2403.01218) | 论文讨论了针对反学习环境的成员推断攻击的调整，并提出了现有U-MIA的分类，对每个示例实例化了专用攻击者。 |
| [^162] | [API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access](https://arxiv.org/abs/2403.01216) | 本研究提出了一种针对无需访问对数的API-only LLMs的整体预测方法，旨在最小化预测集大小并确保用户定义的覆盖范围的统计保证。 |
| [^163] | [Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise](https://arxiv.org/abs/2403.01204) | 我们提出了一种针对具有Massart噪声的线性和ReLU回归问题的随机梯度下降方法，具有新颖的近乎线性收敛保证，首次在流式设置中为鲁棒ReLU回归提供了收敛保证，并展示了其相比于以前的方法有改进的收敛速率。 |
| [^164] | [Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment](https://arxiv.org/abs/2403.01203) | 本研究提出了一种伪标签校准的半监督多模态实体对齐方法，通过引入互信息最大化来过滤模态特定噪音，增强模态不变性共性。 |
| [^165] | [A Composite Decomposition Method for Large-Scale Global Optimization](https://arxiv.org/abs/2403.01192) | 本文提出了一种复合分解方法，将差分分组和一般分组方法整合到一个框架中，通过逐步分解准确分解各种问题类型，降低了计算复杂性。 |
| [^166] | [Training Unbiased Diffusion Models From Biased Dataset](https://arxiv.org/abs/2403.01189) | 提出了一种基于时间的重要性重赋权方法，以减轻扩散模型的偏差，并通过时间依赖密度比实现更准确的重赋权，从而最小化生成学习中的误差传播。 |
| [^167] | [Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery](https://arxiv.org/abs/2403.01183) | 利用自监督学习技术，本文提出了一种能够安全高效处理儿童性虐待图像数据的场景识别方法。 |
| [^168] | [A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds](https://arxiv.org/abs/2403.01158) | 本研究介绍了一种基于贝叶斯委员会机制的 BCM 潜势，可用于预测含氧有机化合物，具有高效的可扩展结构和适应性，在处理大型数据集时表现出色。 |
| [^169] | [A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model](https://arxiv.org/abs/2403.01147) | 提出了一种结合Transformer和生成对抗网络的混合模型来提高交通事故检测的效果，并通过扩展数据集和实现平衡比例进行了验证 |
| [^170] | [LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization](https://arxiv.org/abs/2403.01136) | 这项研究提出了LLM-PQ系统，通过采用自适应模型量化和相位感知分区，在异构GPU集群上提高了LLM服务的效率。 |
| [^171] | [Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data](https://arxiv.org/abs/2403.01133) | 大型语言模型（LLMs）作为虚拟标注器，直接使用原始传感器数据进行标注，可能解决传统人机协作标注时间序列数据的一系列问题。 |
| [^172] | [MPIPN: A Multi Physics-Informed PointNet for solving parametric acoustic-structure systems](https://arxiv.org/abs/2403.01132) | 本文提出了基于深度学习的多物理信息PointNet（MPIPN）用于解决参数化声学-结构系统，通过增强的点云架构和提取局部全局特征来解决涉及显式和隐式物理量的问题。 |
| [^173] | [LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation](https://arxiv.org/abs/2403.01131) | LLaMoCo是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架，通过全面指令集和新颖的两阶段学习策略，实现了优越的性能。 |
| [^174] | [Sensitivity Analysis On Loss Landscape](https://arxiv.org/abs/2403.01128) | 利用一、二和三阶导数进行损失景观分析，发现了与Spearman秩相关系数类似可视化的信息，以及损失函数和激活函数结合带来的非线性模式。 |
| [^175] | [OpenGraph: Towards Open Graph Foundation Models](https://arxiv.org/abs/2403.01121) | 该论文旨在通过开发一个通用图基础模型，以解决现有图神经网络在泛化到与训练数据显著不同的未见图数据时遇到的困难。 |
| [^176] | [Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2403.01112) | 提出了用于合作多智能体强化学习的高效情节记忆利用（EMU），利用语义一致内存加速学习，有选择地促进理想的转换，避免陷入局部最优解。 |
| [^177] | [Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models](https://arxiv.org/abs/2403.01101) | 通过代理进行特征对齐，以解决预先计算特征无法区分标记样本类别和避免通过代理模型选择样本时牺牲宝贵预训练信息的问题。 |
| [^178] | [Pairwise Alignment Improves Graph Domain Adaptation](https://arxiv.org/abs/2403.01092) | Pair-Align方法通过一对一对齐来对抗图结构转移，使用边权重减轻条件结构转移（CSS）并调整分类损失以处理标签转移（LS），在处理图领域自适应中表现出优越性能。 |
| [^179] | [COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting](https://arxiv.org/abs/2403.01091) | 本文提出了一种名为COOL的Conjoint Spatio-Temporal图神经网络，旨在共同捕捉交通预测中的高阶关系。 |
| [^180] | [LAB: Large-Scale Alignment for ChatBots](https://arxiv.org/abs/2403.01081) | 介绍了一种名为LAB的方法，旨在克服大型语言模型训练中的可扩展性挑战，通过分类法指导的合成数据生成和多阶段调整框架，实现了对昂贵人工标注和GPT-4等专有模型依赖较少的大规模对齐，提供了一种可扩展、具有成本效益的解决方案，不会出现灾难性遗忘情况，进一步增强了LLM的训练效率。 |
| [^181] | [Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework](https://arxiv.org/abs/2403.01079) | 提出了一个新的三阶段多任务知识蒸馏框架，使用位置编码来捕捉位置信息，引入神经热核处理图数据，通过隐藏层输出匹配提高学生多层感知机的性能。 |
| [^182] | [$\Gamma$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data](https://arxiv.org/abs/2403.01078) | $\Gamma$-VAE通过正则化曲率来解决非线性降维技术中的两个限制，可以揭示高维数据中的新兴低维几何结构 |
| [^183] | [Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection](https://arxiv.org/abs/2403.01076) | 通过Uncertainty Quantification技术从预训练的视觉模型中提取可用预测，有效忽略不自信的预测，节省了高达80%被误分类的样本。 |
| [^184] | [GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations](https://arxiv.org/abs/2403.01071) | 提出了一种自条件图生成框架，通过自引导表示指导生成过程，明确建模和利用图分布，优于传统隐式捕获分布的方法。 |
| [^185] | [Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL)](https://arxiv.org/abs/2403.01059) | CMZ-DRIL是一种新颖的模仿学习方法，通过连续的零均值奖励函数和智能体集合之间的不确定性最小化，提高了只有少量专家演示的智能体性能。 |
| [^186] | [Neural Field Classifiers via Target Encoding and Classification Loss](https://arxiv.org/abs/2403.01058) | 本文提出了一个新颖的神经场分类器（NFC）框架，将现有的神经场方法规定为分类任务而不是回归任务，从而探讨了神经场方法中回归模型和分类模型的优劣。 |
| [^187] | [Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling](https://arxiv.org/abs/2403.01053) | 提出了一种通过几何限制概率建模处理方法来解决生物医学数据中存在的非 i.i.d. 数据分布、类别不平衡等问题。 |
| [^188] | [A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features](https://arxiv.org/abs/2403.01046) | 证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。 |
| [^189] | [Federated Learning via Lattice Joint Source-Channel Coding](https://arxiv.org/abs/2403.01023) | 通过新的联合源通道编码方案，实现了一种通用联邦学习框架，有效解决了设备信道信息不可知的问题，并在数字通信中能够进行空中计算。 |
| [^190] | [Autonomous Strike UAVs for Counterterrorism Missions: Challenges and Preliminary Solutions](https://arxiv.org/abs/2403.01022) | 本研究首次深入分析了自主无人机任务成功实施的挑战和初步解决方案，提出了用于克服挑战的技术方案，并描述了一个用于训练无人机的机器学习模型。 |
| [^191] | [A Case for Validation Buffer in Pessimistic Actor-Critic](https://arxiv.org/abs/2403.01014) | 提出了验证悲观学习（VPL）算法，通过使用小的验证缓冲区调整悲观水平，以最小化评论家目标的逼近误差，从而改善了样本效率和性能。 |
| [^192] | [Distributional Dataset Distillation with Subtask Decomposition](https://arxiv.org/abs/2403.00999) | 提出了基于子任务分解的分布式数据集提炼方法，通过使用最小的每类统计信息进行数据编码，并结合解码器，将数据集提炼成更节省内存的分布式表示形式，相比于传统基于原型的方法更高效。 |
| [^193] | [On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games](https://arxiv.org/abs/2403.00993) | 明确表示信息结构是分析和解决强化学习问题的重要组成部分。 |
| [^194] | [SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation](https://arxiv.org/abs/2403.00991) | SELFI提出了一种在线学习方法，通过将在线无模型强化学习与离线基于模型的学习相结合，实现了机器人行为的快速改进，并在避撞和社交遵从行为方面取得了显著进展。 |
| [^195] | [Merging Text Transformer Models from Different Initializations](https://arxiv.org/abs/2403.00986) | 研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。 |
| [^196] | [Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance](https://arxiv.org/abs/2403.00975) | 利用功能神经网络（FNN）和长短期记忆（LSTM）网络的集成方法来预测风力发电机功率输出，实现准确稳定的预测并检测性能恶化，以推动积极的维护策略和健康评估。 |
| [^197] | [Motif distribution and function of sparse deep neural networks](https://arxiv.org/abs/2403.00974) | 通过网络模式理论，研究了稀疏深度神经网络的连接结构，证明了尽管随机初始化参数，强制稀疏会导致这些网络收敛到相似的结构 |
| [^198] | [Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients](https://arxiv.org/abs/2403.00965) | 提出了一种新的数据增强技术 Binary Gaussian Copula Synthesis (BGCS)，用于解决基于机器学习的临床决策支持系统在早期预测慢性肾病患者透析需求中所面临的数据不平衡问题 |
| [^199] | [MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection](https://arxiv.org/abs/2403.00964) | 引入了数据增强流水线和投票集成，利用合成数据检测LLM幻觉。 |
| [^200] | [Tree-Regularized Tabular Embeddings](https://arxiv.org/abs/2403.00963) | 提出了一种通过树正则化表征来正则化表格输入的方法，将原始变量转换为单个向量或一系列标记，有效缩小了与树模型之间的性能差距。 |
| [^201] | [Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice](https://arxiv.org/abs/2403.00961) | 通过建立数据科学教育实践社区，分享最佳实践和经验教训，为将数据科学纳入大学物理课程的关键策略和挑战提供指导。 |
| [^202] | [MediSwift: Efficient Sparse Pre-trained Biomedical Language Models](https://arxiv.org/abs/2403.00952) | MediSwift在生物医学领域引入了高效稀疏预训练模型，通过75%的权重稀疏性实现了2-2.5倍的训练FLOPs减少，从而显著提高了效率。 |
| [^203] | [Fine-tuning with Very Large Dropout](https://arxiv.org/abs/2403.00946) | 通过使用非常高的dropout率进行微调，可以实现超出分布性能，这超出了集成和权重平均方法。 |
| [^204] | [Resilience of Entropy Model in Distributed Neural Networks](https://arxiv.org/abs/2403.00942) | 本文研究了分布式神经网络中熵模型对有意干扰和无意干扰的韧性，通过实验证明了熵模型的韧性。 |
| [^205] | [Transfer Learning for Security: Challenges and Future Directions](https://arxiv.org/abs/2403.00935) | 迁移学习在安全领域表现出巨大潜力，能够有效跨域传递知识以提高学习性能，减少数据标注工作，并本文旨在回顾和探讨其在安全领域的应用前景。 |
| [^206] | [Differentially Private Knowledge Distillation via Synthetic Text Generation](https://arxiv.org/abs/2403.00932) | 提出一种利用合成数据进行知识蒸馏的差分私密算法 |
| [^207] | [Scale-free Adversarial Reinforcement Learning](https://arxiv.org/abs/2403.00930) | 本文在马尔可夫决策过程中提出了首个无尺度对抗性学习算法框架SCB，在对抗性多臂赌博机和MDP设置中取得了关键突破。 |
| [^208] | [PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning](https://arxiv.org/abs/2403.00929) | PRIME是一个基于行为原语设计的框架，通过将任务分解为原语序列并学习高级控制策略，显著提高了多阶段操作任务的性能表现。 |
| [^209] | [The Algorithm Configuration Problem](https://arxiv.org/abs/2403.00898) | 本文深入研究了算法配置问题，提出了一个全面框架，结合机器学习模型和启发式策略，划分了不同的解决方法，以明确路径来理解和解决算法配置中的复杂性。 |
| [^210] | [VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction](https://arxiv.org/abs/2403.00897) | VisRec提出了一种模型-不可知的半监督学习方法，用于重建射电干扰数据，通过监督学习模块和无监督学习模块相结合，减少了对标记训练数据的需求，降低了射电天文学家的标注工作量 |
| [^211] | [End-to-end Graph-Sequential Representation Learning for Accurate Recommendations](https://arxiv.org/abs/2403.00895) | 本文提出了一个新颖的多重表示学习框架，有效地结合了基于序列和基于图的推荐方法，显著改善了推荐性能。 |
| [^212] | [PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems](https://arxiv.org/abs/2403.00892) | PowerFlowMultiNet是一种专门为不平衡三相功率网格设计的新颖多图GNN框架，能够有效捕捉不平衡网格中的不对称性，并引入了图嵌入机制来捕获电力系统网络内部的空间依赖关系。 |
| [^213] | [A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder](https://arxiv.org/abs/2403.00891) | 提出了一种基于正则化的信息抽取迁移学习方法，通过指导图解码器实现数据集之间通用知识的迁移 |
| [^214] | [Time-bound Contextual Bio-ID Generation for Minimalist Wearables](https://arxiv.org/abs/2403.00889) | 提出了一种名为Proteus的创新概念，实现了时限上下文生物身份的生成，可以有效解决极简可穿戴设备的实时认证挑战，促进设备协作和用户交互。 |
| [^215] | [Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification](https://arxiv.org/abs/2403.00888) | 该研究提出了一种基于边际差异的对抗训练方法，通过在多领域文本分类中进行理论分析和新的泛化界限的建立，解决了在MDTC算法设计中缺乏理论保证的挑战。 |
| [^216] | [SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech](https://arxiv.org/abs/2403.00887) | 本文提出了一种统一的方法来从语音中预测年龄、性别和情绪，通过深度学习模型探索了单一、多输出和顺序模型的比较，并提出了新颖的多输出学习架构。 |
| [^217] | [Evaluating and Correcting Performative Effects of Decision Support Systems via Causal Domain Shift](https://arxiv.org/abs/2403.00886) | 在高风险环境中部署决策支持系统时，本研究将其视为因果域转移，并提出新颖的跨域识别方法。 |
| [^218] | [FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission](https://arxiv.org/abs/2403.00881) | FedRDMA是一个基于分块RDMA传输的高效跨边缘联邦学习系统，通过优化技术提高了通信效率，相比传统系统可实现最多3.8倍的速度提升。 |
| [^219] | [Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation](https://arxiv.org/abs/2403.00877) | Disaggregated Multi-Tower提出了一种面向拓扑感知的建模技术，通过SPTT、TM和TP三个组件实现了高效的大规模推荐，加速性能提升了1.9倍。 |
| [^220] | [Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions](https://arxiv.org/abs/2403.00875) | 本文将图片和文本的数据增强技术扩展到蛋白领域，提出了两种新的蛋白语义级增强方法，并将这些增强方法集成到一个增强池中，构建了一个名为自动蛋白增强（APA）的简单而有效的框架。 |
| [^221] | [Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions](https://arxiv.org/abs/2403.00873) | 区块链技术被整合到联邦学习系统中以提供更强的安全性、公平性和可扩展性，但也引入了额外的网络、计算和存储资源需求。 |
| [^222] | [Teach LLMs to Phish: Stealing Private Information from Language Models](https://arxiv.org/abs/2403.00871) | 本研究提出了一种名为“神经钓鱼”的新型实用数据提取攻击，使对手能够成功地从大型语言模型中提取敏感信息，攻击成功率高达10%至50%。 |
| [^223] | [Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling](https://arxiv.org/abs/2403.00869) | 引入了CDAM和TAM模型以改进多元时间序列预测，通过最小化冗余信息并增强互信息，利用时间相关性。 |
| [^224] | [Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes](https://arxiv.org/abs/2403.00867) | 本文提出了一种名为Gradient Cuff的方法，通过探索拒绝损失地形图来检测对大语言模型的越狱攻击，成功设计了一种有效的两步检测策略。 |
| [^225] | [Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning](https://arxiv.org/abs/2403.00865) | 提出了一种新的基于遗传编程的元学习框架，通过局部搜索方法实现了任务和模型无关的损失函数学习，实验证实了该框架在各种监督学习任务上的多样性和性能。 |
| [^226] | [Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy, Survey and Insights](https://arxiv.org/abs/2403.00861) | 本文旨在研究如何利用深度生成模型（DGMs）重构现代零售供应链，通过提供DGMs的分类法、零售供应链中的应用案例回顾以及潜在利用DGMs解决零售问题的讨论。 |
| [^227] | [Parallel Algorithms for Exact Enumeration of Deep Neural Network Activation Regions](https://arxiv.org/abs/2403.00860) | 本研究提出了深度（和浅层）神经网络中精确枚举的并行算法，主要贡献包括新颖的算法框架和并行算法，实现了其中一种算法在多种网络架构上，并实验证明区域数量对运行时间的影响。 |
| [^228] | [Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs](https://arxiv.org/abs/2403.00858) | 通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。 |
| [^229] | [Speaker-Independent Dysarthria Severity Classification using Self-Supervised Transformers and Multi-Task Learning](https://arxiv.org/abs/2403.00854) | 提出了一种使用自监督变压器和多任务学习进行说话者无关的运动障碍严重度分类的方法，可自动评估运动障碍的严重程度。 |
| [^230] | [Distributed Momentum Methods Under Biased Gradient Estimations](https://arxiv.org/abs/2403.00853) | 本文在偏差梯度估计下建立了关于一般非凸和$\mu$-PL非凸问题的分布式动量方法的非渐近收敛界限，覆盖了一般分布式优化问题的分析，并揭示了梯度估计有偏时的特殊情况下的影响，即在元学习和梯度被压缩或剪切时。 |
| [^231] | [NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions](https://arxiv.org/abs/2403.00849) | 改进了FPGA加速神经网络推断任务的方法，提出将整个子网络映射到单个LUT中，使得神经网络拓扑和精度不再影响生成的查找表的大小。 |
| [^232] | [Improved Online Learning Algorithms for CTR Prediction in Ad Auctions](https://arxiv.org/abs/2403.00845) | 该研究致力于解决广告拍卖中的在线学习问题，提出了针对广告主两种不同战略行为模型的在线机制，其中一种在最坏情况下取得了紧致的遗憾界限，另一种则解决了更复杂的长期效用优化问题。 |
| [^233] | [Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation](https://arxiv.org/abs/2403.00844) | 提出了一种新的优化指标Lower-Left Partial AUC（LLPAUC），在计算效率上类似于AUC，但与Top-K排名指标强相关，能在推荐系统中有效提升性能。 |
| [^234] | [Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning](https://arxiv.org/abs/2403.00843) | 利用大型语言模型的规划能力来增强长期推荐，使模型在个性化推荐中更有效地理解和应用任务解决原则 |
| [^235] | [Offline Fictitious Self-Play for Competitive Games](https://arxiv.org/abs/2403.00841) | 本文介绍了Off-FSP，这是竞争游戏的第一个实用的无模型离线RL算法，通过调整固定数据集的权重，使用重要性抽样，模拟与各种对手的互动。 |
| [^236] | [Deep Learning Detection Method for Large Language Models-Generated Scientific Content](https://arxiv.org/abs/2403.00828) | 提出了一种新的ChatGPT生成科学文本检测方法AI-Catcher，该方法集成了多层感知器（MLP）和卷积神经网络（CNN），是一个多模态模型，用于检测大型语言模型生成的科学内容。 |
| [^237] | [Self-Refinement of Language Models from External Proxy Metrics Feedback](https://arxiv.org/abs/2403.00827) | 本文提出了Proxy Metric-based Self-Refinement (ProMiSe)方法，通过外部指标反馈指导语言模型在质量关键维度上进行自我完善，从而改进响应质量。 |
| [^238] | [LLMGuard: Guarding Against Unsafe LLM Behavior](https://arxiv.org/abs/2403.00826) | LLMGuard是一个监视用户与LLM应用程序互动的工具，可标记违背特定行为或对话主题的内容。 |
| [^239] | [Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer Medication Effects Using Natural Language Processing](https://arxiv.org/abs/2403.00821) | 本文利用自然语言处理分析推特数据，发展了一种基于Transformer的分类器来识别乳腺癌患者/幸存者，并设计了多层规则模型以研究乳腺癌疗法效果。 |
| [^240] | [DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models](https://arxiv.org/abs/2403.00818) | DenseSSM是一种新方法，通过密集连接增强了状态空间模型(SSM)，有效地提升了各层之间隐藏信息的流动，在保持训练并行性和推理效率的同时，取得了显著的性能提升。 |
| [^241] | [Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random](https://arxiv.org/abs/2403.00817) | 提出了双重校准估计器，通过校准插补和概率模型来解决推荐系统中缺失数据不随机的挑战 |
| [^242] | [Uncovering Customer Issues through Topological Natural Language Analysis](https://arxiv.org/abs/2403.00804) | 提出了一种利用自然语言技术和拓扑数据分析监控新兴和热门客户问题的机器学习算法。 |
| [^243] | [LiMAML: Personalization of Deep Recommender Models via Meta Learning](https://arxiv.org/abs/2403.00803) | 该论文介绍了一种通过元学习实现个性化深度推荐模型的创新解决方案，能够根据最新用户互动信号频繁更新模型，以确保向不同成员提供相关且更新的体验。 |
| [^244] | [Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes](https://arxiv.org/abs/2403.00800) | 通过模仿人类思维过程，在数学推理任务中提出的Brain方法实现了最先进的性能，并发现计划可以从自然语言、代码或形式语言中明确提取出来。 |
| [^245] | [An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning](https://arxiv.org/abs/2403.00799) | 通过确定最优路径集，本研究拓展了LLMs在数学推理任务中的能力边界，提出了一种监督数据策略，通过混合不同类型数据的最小最优集来累积增强模型能力，并实现了SOTA性能。 |
| [^246] | [Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization](https://arxiv.org/abs/2403.00798) | 本文从优化的角度探讨CTR预测问题，揭示了特征频率与最大Hessian特征值之间的强正相关性，提出频繁出现的特征会趋向于收敛到尖锐的局部最小值，从而导致次优性能。 |
| [^247] | [Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting](https://arxiv.org/abs/2403.00796) | 本论文通过使用高斯过程在金融预测中探索功能和增强数据结构，提供了一种能够预测整个概率分布并进行长期预测的方法，对于准确预测和决策制定具有重要意义 |
| [^248] | [Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models](https://arxiv.org/abs/2403.00794) | 利用大型语言模型生成合成数据，可以帮助改进幽默检测，特别是通过取消幽默元素来评估模型性能。 |
| [^249] | [Ad Recommendation in a Collapsed and Entangled World](https://arxiv.org/abs/2403.00793) | 该论文提出了一个行业广告推荐系统，重点关注学习适当表示的挑战和实践，采用多种方法处理特征表示中的关键挑战，包括嵌入的维度坍缩和跨任务或场景的兴趣纠缠。 |
| [^250] | [PRECISE Framework: GPT-based Text For Improved Readability, Reliability, and Understandability of Radiology Reports For Patient-Centered Care](https://arxiv.org/abs/2403.00788) | 本研究提出并评估了PRECISE框架，利用GPT-4技术提供更易读的胸部X射线报告，以进一步提高放射学报告的可读性、可靠性和可理解性，有助于推动以患者为中心的护理。 |
| [^251] | [Applying News and Media Sentiment Analysis for Generating Forex Trading Signals](https://arxiv.org/abs/2403.00785) | 情感分析在外汇交易中的运用对预测市场走势和制定交易信号非常有价值，其有效性在不同市场条件下保持一致。 |
| [^252] | [ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework](https://arxiv.org/abs/2403.00781) | 这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。 |
| [^253] | [Empirical and Experimental Insights into Data Mining Techniques for Crime Prediction: A Comprehensive Survey](https://arxiv.org/abs/2403.00780) | 这篇论文提供了对犯罪预测技术的全面分析，提出了一种细分犯罪预测算法的方法论分类法，并通过经验和实验评估来对这些技术进行排名。 |
| [^254] | [Combating Financial Crimes with Unsupervised Learning Techniques: Clustering and Dimensionality Reduction for Anti-Money Laundering](https://arxiv.org/abs/2403.00777) | 本研究探讨将聚类方法与四种降维技术相结合，以克服反洗钱数据中的高维问题，并提高聚类结果的有效性。 |
| [^255] | [Detecting Anomalous Events in Object-centric Business Processes via Graph Neural Networks](https://arxiv.org/abs/2403.00775) | 通过图神经网络和面向对象的流程挖掘，本研究提出了一种在业务流程中检测异常事件的新框架，避免了传统方法中的扁平化事件日志带来的人为异常问题。 |
| [^256] | [Misconduct in Post-Selections and Deep Learning](https://arxiv.org/abs/2403.00773) | 该论文讨论了深度学习和后选择中的不端行为，并提出了对于解决这一问题的新观点。 |
| [^257] | [Do Weibo platform experts perform better at predicting stock market?](https://arxiv.org/abs/2403.00772) | 使用神经网络结合BERT情感分类和LSTM时间序列模型在微博平台授权和未授权金融顾问的背景下进行股市预测 |
| [^258] | [XProspeCT: CT Volume Generation from Paired X-Rays](https://arxiv.org/abs/2403.00771) | 通过探索更大的数据集和多种模型结构，本研究将正交X射线图像转换为模拟的CT体积，采用了UNet架构、自定义连接、激活函数、损失函数、优化器和一种新颖的反投影方法。 |
| [^259] | [Text mining in education](https://arxiv.org/abs/2403.00769) | 本文系统概述了当前教育文本挖掘领域的现状，旨在回答教育环境中最常用的文本挖掘技术、最常用的教育资源以及主要的应用或教育目标，同时概述了结论和未来趋势。 |
| [^260] | [Towards Fair and Firm Real-Time Scheduling in DNN Multi-Tenant Multi-Accelerator Systems via Reinforcement Learning](https://arxiv.org/abs/2403.00766) | 通过强化学习实现了在DNN多租户多加速器系统中公平和稳定的实时调度，提出了针对不同租户的模型特定QoS管理的新方法。 |
| [^261] | [An Architecture for Unattended Containerized (Deep) Reinforcement Learning with Webots](https://arxiv.org/abs/2403.00765) | 该论文提出了一种用于Webots的无人监控容器化(深度)强化学习体系结构，针对机器人 Robotino 训练强化学习代理，同时强调模拟环境和数据科学家模型开发环境的分离这一不太被讨论的主题。 |
| [^262] | [Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network](https://arxiv.org/abs/2403.00033) | 通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。 |
| [^263] | [Theoretical Foundations of Deep Selective State-Space Models](https://arxiv.org/abs/2402.19047) | 随着GateLoop、Mamba和GLA等具有乘法交互的线性递归驱动下的深度SSM架构的出现，它们在准确性和效率上超越了基于注意力的文本训练的基础模型。 |
| [^264] | [Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation](https://arxiv.org/abs/2402.18919) | 通过组合方法改善模型对相关性转移的稳健性，解决了图像分类中伪相关性的问题。 |
| [^265] | [Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation](https://arxiv.org/abs/2402.18839) | 本文基于Flow Matching发展了条件生成理论，通过使用广义连续性方程的数学框架而非流匹配中的连续性方程，实现了一种新颖的流基条件分布生成方法。 |
| [^266] | [Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games](https://arxiv.org/abs/2402.18781) | 提出了一种具有假设在线学习（COL）的学习方案，针对通用AISG，结构化为一个先验预测者-演员-评论家（FAC）架构，利用一级信念和对手策略的主观预测，通过在线展开更新策略，并通过贝叶斯学习校准假设。 |
| [^267] | [Data Interpreter: An LLM Agent For Data Science](https://arxiv.org/abs/2402.18679) | 本研究引入了数据解释器，采用动态规划、工具集成和逻辑错误识别等关键技术，旨在增强数据科学中的问题解决能力。 |
| [^268] | [Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective](https://arxiv.org/abs/2402.18607) | 本文从对抗性的角度研究了分享扩散模型可能存在的隐私和公平风险，特别是探讨了在一方使用私人数据训练模型后提供给另一方黑盒访问权限的情况。 |
| [^269] | [Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning](https://arxiv.org/abs/2402.18064) | 结合了迁移学习和主动学习的方法，通过多任务高斯过程和基于信息的目标函数，可以在实时评估假设的数量之间的关系，从而提高规划效率。 |
| [^270] | [Mixer is more than just a model](https://arxiv.org/abs/2402.18007) | Mixer的创新之处在于将通道和令牌信息融合，代表了信息提取范式，还可以根据不同需求创建更适合特定任务的混合器。 |
| [^271] | [Certain and Approximately Certain Models for Statistical Learning](https://arxiv.org/abs/2402.17926) | 可以直接从带有缺失值的数据中学习准确模型，构建了检查数据填充必要性的高效算法，并在不需要填充的情况下返回准确模型，显著减少数据填充所需的时间和精力 |
| [^272] | [When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning](https://arxiv.org/abs/2402.17747) | RLHF在考虑部分观察性时可能导致策略欺骗性地夸大性能或过度辩护行为，我们提出了数学条件来解决这些问题，并警告不要盲目应用RLHF在部分可观测情况下。 |
| [^273] | [A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data](https://arxiv.org/abs/2402.16991) | 扩散模型在研究数据的分层生成模型中展示出了在阈值时间发生相变的特性，这影响了高级特征和低级特征的重建过程。 |
| [^274] | [Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values](https://arxiv.org/abs/2402.16388) | 针对异常检测系统中不确定性量化的需求，提出了一种新颖的框架，称为交叉一致异常检测，通过校准模型的不确定性提供统计保证。 |
| [^275] | [Fusion Encoder Networks](https://arxiv.org/abs/2402.15883) | FENs是一种神经网络算法，具有对数深度且可以在线性时间内处理序列，关键创新在于通过训练大致线性数量的常深度神经网络并行学习。 |
| [^276] | [OmniPred: Language Models as Universal Regressors](https://arxiv.org/abs/2402.14547) | 本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。 |
| [^277] | [Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation](https://arxiv.org/abs/2402.14264) | 采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性 |
| [^278] | [NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning](https://arxiv.org/abs/2402.14139) | NeuroFlux是一个为内存受限场景量身定制的CNN训练系统，提出了自适应辅助网络和块特定的自适应批处理大小的创新机遇。 |
| [^279] | [E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series](https://arxiv.org/abs/2402.14041) | E2USD提出了一种有效的无监督多元时间序列状态检测方法，利用了快速傅里叶变换和双视图嵌入模块进行编码，以及通过对抗学习方法消除假阴性，从而实现了SOTA准确性并显著降低了计算开销。 |
| [^280] | [SDXL-Lightning: Progressive Adversarial Diffusion Distillation](https://arxiv.org/abs/2402.13929) | 提出了一种结合渐进和对抗性蒸馏的扩散蒸馏方法，在文本到图像生成任务中取得了新的最先进结果，并开源了相应模型。 |
| [^281] | [DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning](https://arxiv.org/abs/2402.13711) | DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。 |
| [^282] | [Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2402.12728) | 提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。 |
| [^283] | [Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling](https://arxiv.org/abs/2402.12694) | 引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。 |
| [^284] | [Self-Guided Robust Graph Structure Refinement](https://arxiv.org/abs/2402.11837) | 本文提出了一个自主引导的GSR框架（SG-GSR），通过利用被攻击图中发现的干净子图，并提出了图增强和分组训练策略，以应对现有GSR方法在真实场景中受限的问题。 |
| [^285] | [Aligning Large Language Models by On-Policy Self-Judgment](https://arxiv.org/abs/2402.11253) | 本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。 |
| [^286] | [Accelerating Semi-Asynchronous Federated Learning](https://arxiv.org/abs/2402.10991) | 提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。 |
| [^287] | [Why are Sensitive Functions Hard for Transformers?](https://arxiv.org/abs/2402.09963) | 本文证明了在Transformer架构下，损失函数的空间受到输入敏感性的限制，从而解释了Transformer对敏感函数的困难。这一理论统一了关于Transformer学习能力和偏见的广泛观察。 |
| [^288] | [MC-DBN: A Deep Belief Network-Based Model for Modality Completion](https://arxiv.org/abs/2402.09782) | MC-DBN是一种基于深度信念网络的模态补全模型，利用完整数据的隐式特征来弥补附加不完整数据的差距，提高预测准确性。 |
| [^289] | [Sparse and Faithful Explanations Without Sparse Models](https://arxiv.org/abs/2402.09702) | 引入了稀疏解释值(SEV)，用于衡量机器学习模型的决策稀疏性。即使模型不是稀疏的，许多机器学习模型在SEV的衡量下仍具有低决策稀疏性。 |
| [^290] | [Forecasting high-impact research topics via machine learning on evolving knowledge graphs](https://arxiv.org/abs/2402.08640) | 通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。 |
| [^291] | [The Limits of Assumption-free Tests for Algorithm Performance](https://arxiv.org/abs/2402.07388) | 这项研究探讨了使用有限数据量回答算法性能问题的基本限制，证明了黑盒测试方法无法准确回答算法在不同训练集上的整体性能和特定模型的性能问题。 |
| [^292] | [Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data](https://arxiv.org/abs/2402.06104) | 该论文提出了一种名为FAR的方法，通过捕捉函数导数来更好、更高效地拟合底层真实函数。在合成数据集和八个真实世界任务中证明了该方法的有效性。 |
| [^293] | [The last Dance : Robust backdoor attack via diffusion models and bayesian approach](https://arxiv.org/abs/2402.05967) | 本文介绍了一种通过扩散模型和贝叶斯方法进行鲁棒后门攻击的方法，具体应用于音频Transformer模型，并证明了攻击的可行性。 |
| [^294] | [Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction](https://arxiv.org/abs/2402.05663) | 该论文介绍了一种在实时中尺度交通预测中具有最先进效果的深度预测方法SA-LSTM，通过将自注意力与长短期记忆结合，实现了对多步预测的改进，并在短期和长期预测之间取得了平衡。 |
| [^295] | [Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application](https://arxiv.org/abs/2402.05448) | 本文提出了一种用于Minecraft游戏应用的图像生成和编辑系统"Minecraft-ify"，能够生成针对3D虚拟角色的面部聚焦图像，并支持使用文本进行图像编辑，提供了更自由和优化的用户体验。 |
| [^296] | [SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models](https://arxiv.org/abs/2402.05044) | SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。 |
| [^297] | [Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching](https://arxiv.org/abs/2402.05011) | 本文通过连接先前被忽视的监督信号的方式，首次尝试实现无损图谱精简，以解决现有方法无法准确复制原始图谱的问题。 |
| [^298] | [RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback](https://arxiv.org/abs/2402.03681) | RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。 |
| [^299] | [Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective](https://arxiv.org/abs/2402.03496) | 移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。 |
| [^300] | [ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer](https://arxiv.org/abs/2402.02733) | 本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。 |
| [^301] | [A Truly Joint Neural Architecture for Segmentation and Parsing](https://arxiv.org/abs/2402.02564) | 本文通过引入一个联合神经网络架构，在形态丰富的语言中实现了同时进行形态分割和句法分析的任务。通过提供基于格子的表示法，保留了输入的所有形态模糊性，有效解决了以往基于神经网络的依存句法分析器的局限性。 |
| [^302] | [LQER: Low-Rank Quantization Error Reconstruction for LLMs](https://arxiv.org/abs/2402.02446) | LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。 |
| [^303] | [Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm](https://arxiv.org/abs/2402.02042) | 该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。 |
| [^304] | [Prompt-Driven LLM Safeguarding via Directed Representation Optimization](https://arxiv.org/abs/2401.18018) | 通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。 |
| [^305] | [Contributing Dimension Structure of Deep Feature for Coreset Selection](https://arxiv.org/abs/2401.16193) | 现有核心集选择方法未能有效解决深度特征维度结构中维度之间差异对最终相似性的贡献，导致多样性样本选择的结果次优。 |
| [^306] | [INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning](https://arxiv.org/abs/2401.11667) | INCPrompt采用自适应关键学习者和面向任务的提示，结合通用和任务特定知识，有效缓解灾难性遗忘，表现优越，对持续学习性能具有显著影响。 |
| [^307] | [Inferring Properties of Graph Neural Networks](https://arxiv.org/abs/2401.03790) | 提出了GNNInfer，这是用于GNN的首个自动属性推断技术，通过识别和转换代表性影响结构，捕捉并推广GNN特定属性，最终通过建立模型提高推断属性的正确性。 |
| [^308] | [Refining GPT-3 Embeddings with a Siamese Structure for Technical Post Duplicate Detection](https://arxiv.org/abs/2312.15068) | 本研究提出了使用孪生结构对GPT-3嵌入进行细化的方法，用于技术帖子的重复检测，解决了现有方法中存在的限制性问题。 |
| [^309] | [Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks](https://arxiv.org/abs/2312.14499) | 介绍了 Hutchinson 迹估计（HTE），通过将整个 Hessian 矩阵的计算转换为 Hessian 矢量乘积（HVP），解决了 PINNs 处理高维和高阶 PDE 的挑战。 |
| [^310] | [Layerwise complexity-matched learning yields an improved model of cortical area V2](https://arxiv.org/abs/2312.11436) | 通过分层复杂度匹配学习，我们开发了一种自下而上的自监督训练方法，最大化了特征相似性同时在不同位置的补丁上解除特征相关性。 |
| [^311] | [Distributional Bellman Operators over Mean Embeddings](https://arxiv.org/abs/2312.07358) | 提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。 |
| [^312] | [$t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence](https://arxiv.org/abs/2312.01133) | 通过引入学生t分布和幂分歧，提出了$t^3$VAE变分自动编码器框架，以更好地处理重尾数据，并推导出新的优化目标。 |
| [^313] | [A Posteriori Evaluation of a Physics-Constrained Neural Ordinary Differential Equations Approach Coupled with CFD Solver for Modeling Stiff Chemical Kinetics](https://arxiv.org/abs/2312.00038) | 扩展NeuralODE框架的研究通过在训练过程中直接将质量守恒约束纳入损失函数，有效地建模了严谨的化学动力学。 |
| [^314] | [Semiparametric Efficient Inference in Adaptive Experiments](https://arxiv.org/abs/2311.18274) | 自适应实验中提出了一种半参数高效推断方法，通过中心极限定理和置信序列实现了更紧凑的推断，具有更广泛的适用性。 |
| [^315] | [Interpretable Fine-Tuning for Graph Neural Network Surrogate Models](https://arxiv.org/abs/2311.07548) | 本论文引入了一种可解释的微调策略，通过应用于非结构网格化流体动力学建模的GNNs，增强了模型的预测能力，并通过识别可解释的物理空间区域及其对应的子图，帮助理解模型架构、优化目标和已知物理之间的关系。 |
| [^316] | [Neuro-GPT: Towards A Foundation Model for EEG](https://arxiv.org/abs/2311.03764) | Neuro-GPT是一个面向EEG数据的基础模型，通过自监督任务预训练和微调动作想象分类任务，显著改善分类性能，并展示了其泛化能力以解决EEG数据稀缺性和异质性挑战。 |
| [^317] | [On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics](https://arxiv.org/abs/2310.19527) | 通过应用期望效用假设，本文揭示了风险中性和风险感知RL目标实际上可以通过使用指数效用函数的期望效用最大化来解释，提出了双演员-评论家（DAC）算法，为风险感知的RL算法贡献了框架。 |
| [^318] | [DySurv: Dynamic Deep Learning Model for Survival Prediction in the ICU](https://arxiv.org/abs/2310.18681) | DySurv是一种新型的动态深度学习模型，结合静态和时间序列测量，用于估计ICU患者的死亡风险，在多个基准测试中表现优异，并在实际世界的ICU数据上进行了评估。 |
| [^319] | [Adversarial Attacks on Fairness of Graph Neural Networks](https://arxiv.org/abs/2310.13822) | 本文研究了对图神经网络公平性的对抗攻击问题，提出了G-FairAttack框架可以成功地攻击各种类型的公平性感知GNNs，并保持攻击的不可察觉性。 |
| [^320] | [Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World](https://arxiv.org/abs/2310.10207) | Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。 |
| [^321] | [An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Traveling Salesman Problems](https://arxiv.org/abs/2310.06543) | 提出了一种基于边缘感知的图自编码器模型，用于解决具有不同城市数量的旅行商问题，并通过学习不同规模样本来训练模型。 |
| [^322] | [Protecting Sensitive Data through Federated Co-Training](https://arxiv.org/abs/2310.05696) | 提出了使用联合协同训练方法来保护敏感数据，通过在公共未标记数据集上共享硬标签代替模型参数，形成伪标签以结合私有数据训练本地模型，提高隐私保护效果并获得与联邦学习相媲美的模型质量。 |
| [^323] | [Language Models Represent Space and Time](https://arxiv.org/abs/2310.02207) | 现代大型语言模型学习到了丰富的时空表征，包括学习到了空间和时间的线性表征以及个体的“空间神经元”和“时间神经元”。 |
| [^324] | [Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design](https://arxiv.org/abs/2309.13957) | 提出了光束枚举方法，从语言为基础的分子生成模型中详尽枚举最可能的子序列，提取分子亚结构，并通过强化学习实现自我条件生成，从而提升生成式设计的可解释性和样本效率。 |
| [^325] | [A Post-Training Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks](https://arxiv.org/abs/2309.01829) | 研究了用于减轻量子卷积神经网络过拟合的后训练方法，并发现将经典后训练方法神经元丢弃直接应用到量子设置中会导致成功概率显著下降，揭示了纠缠在QCNN中的关键作用和其对纠缠丢失的脆弱性。 |
| [^326] | [A Unifying Generator Loss Function for Generative Adversarial Networks](https://arxiv.org/abs/2308.07233) | 引入了一个统一的生成器损失函数，称为$\mathcal{L}_\alpha$-GAN，通过最小化Jensen-$f_\alpha$-散度来优化生成器，可以恢复出文献中的多个GAN问题。 |
| [^327] | [MATNet: Multi-Level Fusion Transformer-Based Model for Day-Ahead PV Generation Forecasting](https://arxiv.org/abs/2306.10356) | 提出了MATNet，结合了人工智能范式与光伏发电的物理先验知识，通过多级联合融合方法进行日前光伏发电预测 |
| [^328] | [Safe Offline Reinforcement Learning with Real-Time Budget Constraints](https://arxiv.org/abs/2306.00603) | 提出了一种名为TREBI的新方法，在离线设置下解决强化学习中实时预算约束的问题，通过轨迹分布建模和扩散模型规划来提供性能保证。 |
| [^329] | [A critical look at the evaluation of GNNs under heterophily: Are we really making progress?](https://arxiv.org/abs/2302.11640) | 挑战了异质图上特定模型评估的假设，揭示了标准数据集存在严重缺陷，移除重复节点对GNN性能有重大影响 |
| [^330] | [A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection](https://arxiv.org/abs/2302.11517) | 提出了一种全新的监督对比学习框架来优化硬渗出物分割，通过引入基于区域的密度对比方案和判别性边缘检查模块来处理硬渗出物的独特特征。 |
| [^331] | [Gaussian Process-Gated Hierarchical Mixtures of Experts](https://arxiv.org/abs/2302.04947) | 该论文提出了一种新颖的高斯过程门控的分层专家混合模型，通过使用GPs构建门控函数和专家，优于传统基于树的模型，同时在复杂性较低的情况下表现出良好性能，还提供了深层GPs和深度贝叶斯神经网络的可解释性。 |
| [^332] | [Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering](https://arxiv.org/abs/2301.12318) | 本文分析了植入后门模型在触发输入周围的变化率，揭示了现有攻击倾向于在触发输入周围注入具有低变化率特征的后门，易被梯度触发器反转捕获。 |
| [^333] | [Social-Aware Clustered Federated Learning with Customized Privacy Preservation](https://arxiv.org/abs/2212.13992) | 通过利用用户之间的社交关系，提出了SCFL，一种具有定制隐私保护的社交感知聚类联邦学习方案，实现了数据隐私和效率之间的平衡。 |
| [^334] | [Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints](https://arxiv.org/abs/2212.04672) | 提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。 |
| [^335] | [Discovering Latent Knowledge in Language Models Without Supervision](https://arxiv.org/abs/2212.03827) | 通过在语言模型的内部激活中直接发现潜在知识的方式，我们提出了一种纯粹无监督的方法，可以准确回答未标记模型激活的是非问题，并且在大型语言模型中恢复多样知识。 |
| [^336] | [Predicting Properties of Quantum Systems with Conditional Generative Models](https://arxiv.org/abs/2211.16943) | 使用条件生成模型同时表示一系列状态，从测量中学习不同量子态的共享结构，可以预测基态的任意局部性质，无需为新的可观测量进行进一步训练 |
| [^337] | [Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks](https://arxiv.org/abs/2211.10209) | 通过使用自适应阈值来考虑数据集中敏感属性类别不平衡，我们提出了一种实用且有效的属性推断攻击方法。 |
| [^338] | [A Generative Shape Compositional Framework to Synthesise Populations of Virtual Chimaeras](https://arxiv.org/abs/2210.01607) | 引入了一种能够合成完整复杂形状集合的生成模型，可用于构建虚拟奇美拉人群，有助于进行医疗器械的硅中试验证 |
| [^339] | [Multi-View Hypercomplex Learning for Breast Cancer Screening](https://arxiv.org/abs/2204.05798) | 本文提出了一种基于参数化超复数神经网络的多视图乳腺癌分类方法，能够模拟并利用乳房X光检查的不同视图之间的相关性，从而提高肿瘤识别效果。 |
| [^340] | [Settling the Sample Complexity of Model-Based Offline Reinforcement Learning](https://arxiv.org/abs/2204.05275) | 该论文展示了基于模型的（或“插件”）方法在标签化马尔可夫决策过程（MDPs）中实现了无烧录成本的极小极优样本复杂性。 |
| [^341] | [Enhancing Digital Health Services: A Machine Learning Approach to Personalized Exercise Goal Setting](https://arxiv.org/abs/2204.00961) | 本研究通过开发机器学习算法，动态更新个性化运动目标，填补了现有方法对用户动态行为和健康状况变化的忽视。 |
| [^342] | [CMGAN: Conformer-based Metric GAN for Speech Enhancement](https://arxiv.org/abs/2203.15149) | 本文提出了一种基于Conformer的度量生成对抗网络（CMGAN）用于时频域的语音增强，通过优化生成器以使得增强估计语音相对应的评估分数来进一步提高增强语音的质量。 |
| [^343] | [Pseudo-Differential Neural Operator: Generalized Fourier Neural Operator for Learning Solution Operators of Partial Differential Equations](https://arxiv.org/abs/2201.11967) | 提出了伪微分积分算子（PDIO）来分析和推广傅立叶神经算子（FNO）中的傅立叶积分算子，实现了神经网络符号的平滑化，并证明了PDIO是有界线性算子，连续作用于Sobolev空间。 |
| [^344] | [Assisted Learning for Organizations with Limited Imbalanced Data](https://arxiv.org/abs/2109.09307) | 提出了一个辅助学习框架，帮助组织改善学习性能，通过购买外部服务提供商的辅助服务来提高模型性能，针对有限和不平衡的数据提出了有效的随机训练算法 |
| [^345] | [Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks](https://arxiv.org/abs/2105.13937) | 提出了一种基于多边形未调整的朗之万算法的新类别算法，名为TH$\varepsilon$O POULA（或简称为TheoPouLa），通过稳定性、非渐进分析和实验表明其在神经网络优化中具有卓越性能。 |
| [^346] | [Adaptive Rational Activations to Boost Deep Reinforcement Learning](https://arxiv.org/abs/2102.09407) | 本研究提出了利用有理数作为适应性激活函数来改进深度强化学习，并展示了这种方法在Atari游戏中取得了一致的改进，特别是将简单的DQN提升为一个稳健的方法。 |
| [^347] | [Private Prediction Sets](https://arxiv.org/abs/2102.06202) | 该研究提出了一个基于符合性预测的框架，可以在保护个人隐私的同时返回可靠的不确定性量化的预测集。 |
| [^348] | [FetusMap: Fetal Pose Estimation in 3D Ultrasound](https://arxiv.org/abs/1910.04935) | 本文第一次提出了关于文献中胎儿3D姿势估计的工作，旨在提取整个胎儿的骨架并分配正确的躯干/肢体标签给不同的部分/关节。 |
| [^349] | [Agent with Warm Start and Active Termination for Plane Localization in 3D Ultrasound](https://arxiv.org/abs/1910.04331) | 提出了一种具有热启动和主动终止功能的代理器，用于在3D超声中自动定位胎儿脑标准平面 |
| [^350] | [Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound](https://arxiv.org/abs/1907.01743) | 本文提出了一种新型的3D深度神经网络，配备着关注模块，通过充分利用卷积神经网络不同层中编码的互补信息，实现了在经直肠超声图像中更好地前列腺分割，通过选择性地整合不同层级的特征来提高前列腺分割性能 |
| [^351] | [Deep Learning for Multi-Label Learning: A Comprehensive Survey.](http://arxiv.org/abs/2401.16549) | 深度学习在多标签学习中的综合调研，旨在审视深度学习在解决多标签分类中的挑战方面的最新进展。 |
| [^352] | [A Survey on Data Augmentation in Large Model Era.](http://arxiv.org/abs/2401.15422) | 这篇论文综述了大模型驱动的数据增强方法，包括图像增强、文本增强和配对数据增强。这些方法利用大模型的能力，有效提高了数据增强的效果，是解决大模型训练中数据质量不足的重要研究方向。 |
| [^353] | [On the generalization capacity of neural networks during generic multimodal reasoning.](http://arxiv.org/abs/2401.15030) | 本研究评估了不同神经网络架构在多模态泛化方面的能力，并发现具有多个注意力层或利用交叉注意机制的模型表现更好。 |
| [^354] | [FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design.](http://arxiv.org/abs/2401.14112) | FP6-LLM提出了一种支持六位量化的GPU算法-系统协同设计方案，实现了在大型语言模型中推断成本和模型质量之间的平衡。 |
| [^355] | [Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility.](http://arxiv.org/abs/2401.13782) | 本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。 |
| [^356] | [Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control.](http://arxiv.org/abs/2401.12624) | 这项工作通过将语言导向的语义通信与新兴通信相结合，通过知识蒸馏的方式，提出了一种面向多智能体远程控制的新框架，实现了更快的行程时间和更高的训练收敛速度。 |
| [^357] | [PartIR: Composing SPMD Partitioning Strategies for Machine Learning.](http://arxiv.org/abs/2401.11202) | PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。 |
| [^358] | [AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference.](http://arxiv.org/abs/2401.10652) | AutoChunk是一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。 |
| [^359] | [Solving the Discretised Multiphase Flow Equations with Interface Capturing on Structured Grids Using Machine Learning Libraries.](http://arxiv.org/abs/2401.06755) | 本文使用AI4PDEs方法对具有界面捕获的离散多相流动方程进行求解，并引入了一种新的基于残差公式的压缩代数体积流方法。 |
| [^360] | [Prediction of Cellular Identities from Trajectory and Cell Fate Information.](http://arxiv.org/abs/2401.06182) | 本研究提出了一种创新的方法，利用机器学习从早期C. elegans胚胎的图像序列中预测细胞身份。通过使用少量的细胞轨迹和细胞命运信息，我们的模型在有限数据条件下达到了超过90%的分类准确率。 |
| [^361] | [Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting.](http://arxiv.org/abs/2401.06040) | 本论文提出了一种基于小波启发的多尺度图卷积循环网络，用于交通预测。该方法将多尺度分析方法和深度学习方法相结合，对交通数据中的多尺度结构进行建模，并展现了较好的性能。 |
| [^362] | [VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition.](http://arxiv.org/abs/2401.05531) | 本研究提出了VI-PANN，利用转移学习和不确定性感知变分推理方法，在音频模式识别中取得了改进的泛化性能。 |
| [^363] | [T-PRIME: Transformer-based Protocol Identification for Machine-learning at the Edge.](http://arxiv.org/abs/2401.04837) | T-PRIME是一个基于Transformer的边缘机器学习协议识别方法，通过注意机制学习传输帧的结构设计，克服了传统方法的局限性。实验证明其在深度学习硬件限制下的实时可行性，并证明了其优于传统方法和最先进的神经网络。 |
| [^364] | [Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study.](http://arxiv.org/abs/2401.04331) | 本文详细研究了图神经分数阶微分方程模型的鲁棒性，通过实施分数阶微积分，模型在特征更新过程中考虑了长期记忆，对抗性条件下的性能仍未得到广泛探究。 |
| [^365] | [A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models.](http://arxiv.org/abs/2401.00873) | 该论文研究了用贝叶斯方法统一自监督聚类和能量模型，提出了一种标准化的推导方法，并设计了一个新的可靠地惩罚失败模式的下界。这个下界使得能够训练一个标准的骨架架构，而无需使用非对称元素。 |
| [^366] | [Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective.](http://arxiv.org/abs/2311.01047) | 本论文提出了一种基于通信理论的方法，通过神经竞争来增强神经网络层输出的信噪比，从而提高深度网络的稳健性。 |
| [^367] | [Hodge-Compositional Edge Gaussian Processes.](http://arxiv.org/abs/2310.19450) | 本论文提出了一种新的方法用于对边缘集合上的函数进行建模，该方法基于Hodge分解开发了适用于不同应用场景的无散度和无旋度的高斯过程，并通过组合它们来表示任意边缘函数。实验结果表明这种方法在流动数据推断中具有潜在的实际应用价值。 |
| [^368] | [Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs.](http://arxiv.org/abs/2310.15516) | 本论文提出了一个基于图注意力的深度强化学习方法来解决带有负载相关成本的中国邮递员问题。该方法将问题形式化为马尔可夫决策过程，引入了一个编码器和解码器的自回归模型来有效处理问题。 |
| [^369] | [From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers.](http://arxiv.org/abs/2310.11984) | 本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。 |
| [^370] | [Optimising Distributions with Natural Gradient Surrogates.](http://arxiv.org/abs/2310.11837) | 本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。 |
| [^371] | [Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning.](http://arxiv.org/abs/2310.11684) | 本研究探索了无限时域平均奖励强化学习中量子加速的潜力。我们提出了一种创新的量子框架，通过高效的量子均值估计技术，实现了指数级改进的遗憾保证。所提出的量子算法相较于经典算法，在遗憾界限上有显著改进。 |
| [^372] | [BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference.](http://arxiv.org/abs/2310.11142) | BayesDiff提出了一种像素级不确定性估计方法，用于扩散模型生成结果。该方法通过贝叶斯推断和不确定性迭代原则来实现高效的估计，并可在图像生成任务中过滤低质量图像，增强成功生成结果，并纠正失败生成结果中的伪影。 |
| [^373] | [Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?.](http://arxiv.org/abs/2310.10012) | 本文研究了对于文本到图像合成的扩散模型中的潜在滥用问题的安全措施的有效性，并提出了一个用于评估的新颖概念检索算法。我们引入了一个名为Ring-A-Bell的模型无关的红队工具，可以事先准备整个评估过程，而无需先验知识。 |
| [^374] | [Emergence of Latent Binary Encoding in Deep Neural Network Classifiers.](http://arxiv.org/abs/2310.08224) | 这篇论文观察到在深度神经网络分类器的潜在空间中出现了二进制编码，这种编码通过引入线性倒数第二层和指数增长的损失函数产生，并且加速了收敛和提高了分类准确率。 |
| [^375] | [Local Graph Clustering with Noisy Labels.](http://arxiv.org/abs/2310.08031) | 本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。 |
| [^376] | [Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning.](http://arxiv.org/abs/2310.07902) | "Unraveling the Single Tangent Space Fallacy"论文分析和澄清了在机器人学习中应用黎曼几何的误区，该误区是指将数据仅投影到单一切空间中的方法。 |
| [^377] | [CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping.](http://arxiv.org/abs/2310.07855) | CrIBo通过跨图像对象级引导进行自监督学习，可以提高密集视觉表示学习的性能，并在自然理解应用中表现出领先的性能。 |
| [^378] | [Federated Generalization via Information-Theoretic Distribution Diversification.](http://arxiv.org/abs/2310.07171) | 该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。 |
| [^379] | [Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction.](http://arxiv.org/abs/2310.05207) | 本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。 |
| [^380] | [Functional Interpolation for Relative Positions Improves Long Context Transformers.](http://arxiv.org/abs/2310.04418) | 这项研究提出了一种名为FIRE的函数相对位置编码与渐进插值方法，通过改进Transformer对更长上下文的泛化能力，并在零射击语言建模和长文本基准测试中进行了实证验证。 |
| [^381] | [Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization.](http://arxiv.org/abs/2310.03234) | 本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。 |
| [^382] | [Forecasting Tropical Cyclones with Cascaded Diffusion Models.](http://arxiv.org/abs/2310.01690) | 本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。 |
| [^383] | [Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities.](http://arxiv.org/abs/2309.16739) | 本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。 |
| [^384] | [Contrastive Continual Multi-view Clustering with Filtered Structural Fusion.](http://arxiv.org/abs/2309.15135) | 提出了一种名为对比度连续多视角聚类与过滤结构融合（CCMVC-FSF）的新方法，用于解决多视角聚类在实时数据收集中的困难。该方法旨在防止先前知识遗忘和利用数据相关性指导新视图的聚类过程。 |
| [^385] | [Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations.](http://arxiv.org/abs/2309.13150) | 本文提出了一种用于对抗相机运动扰动的像素级平滑框架，通过在二维像素空间中使用平滑分布来提高鲁棒性认证的效率，并完全上界投影误差。 |
| [^386] | [Optimizing Modular Robot Composition: A Lexicographic Genetic Algorithm Approach.](http://arxiv.org/abs/2309.08399) | 本论文提出了一种将遗传算法与词典式评估相结合的方法来优化模块化机器人的组合，以克服以往方法中存在的设计空间不足和适应复杂任务的问题，并证明了这种方法在比以往范围更大的搜索空间中表现出更好的性能。 |
| [^387] | [$G$-Mapper: Learning a Cover in the Mapper Construction.](http://arxiv.org/abs/2309.06634) | 本论文介绍了一种基于统计检验和聚类算法的优化Mapper图覆盖的方法，通过分割覆盖选择生成了保留数据集本质的Mapper图。 |
| [^388] | [SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models.](http://arxiv.org/abs/2309.05019) | 本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。 |
| [^389] | [ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation.](http://arxiv.org/abs/2309.03891) | ArtiGrasp是一种通过强化学习和物理模拟的方式，用一个统一的策略来合成双手灵巧抓握和关节表达的方法。 |
| [^390] | [A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism.](http://arxiv.org/abs/2309.03720) | 本文介绍了一个基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统，通过数据流处理，实现了多步 ahead 的预测和持续学习能力。在复杂的实际应用场景中，通过评估预测模型的性能，证明了该方法的有效性。 |
| [^391] | [SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks.](http://arxiv.org/abs/2309.00255) | SortedNet是一种广义解决方案，通过排序训练和概率方式，在深度神经网络的各个维度上实现高效动态推断。这种方法允许在模型推断过程中灵活适应计算负载，并且可以将子网络的数量扩展到数百个。 |
| [^392] | [Differentially Private Functional Summaries via the Independent Component Laplace Process.](http://arxiv.org/abs/2309.00125) | 本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。 |
| [^393] | [Neural Implicit Morphing of Face Images.](http://arxiv.org/abs/2308.13888) | 本论文提出了一种利用神经网络实现人脸图像变形和混合的方法，通过利用网络的平滑性和灵活性，结合经典方法中的能量函数，实现了高效、准确和多样化的人脸变形效果。 |
| [^394] | [KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods.](http://arxiv.org/abs/2308.11863) | 通过自监督预训练、课程进度微调和半监督学习利用无标签语音数据，该论文提出了一种改善基尼亚兰达语音识别的方法，实现了最先进的结果。 |
| [^395] | [MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.](http://arxiv.org/abs/2308.09729) | 本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。 |
| [^396] | [Noncompact uniform universal approximation.](http://arxiv.org/abs/2308.03812) | 这篇论文将通用逼近定理推广到非紧致输入空间，并确定了在有界激活函数条件下可以通过神经网络一致逼近的函数类别，并提出了代数结构的意外结果。 |
| [^397] | [Settling the Sample Complexity of Online Reinforcement Learning.](http://arxiv.org/abs/2307.13586) | 本文解决了在线强化学习的样本复杂度问题，提出了一种基于模型的算法，它可以在有限时间不均匀马尔可夫决策问题中实现极小后悔的最优性。 |
| [^398] | [Tackling the Curse of Dimensionality with Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.12306) | 本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。 |
| [^399] | [A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising.](http://arxiv.org/abs/2307.07932) | 一种新颖的多通道彩色图像去噪方法，通过双加权截断核范数减去截断Frobenius范数最小化，利用非局部自相似性来提取相似的结构进行去噪，模型兼顾了跨通道差异和噪声的空间变化。 |
| [^400] | [One-Versus-Others Attention: Scalable Multimodal Integration.](http://arxiv.org/abs/2307.05435) | 提出了一种可扩展的多模态集成方法，通过一对多（OvO）注意力机制解决了多模态学习中超过三个模态的注意力计算问题。 |
| [^401] | [When No-Rejection Learning is Optimal for Regression with Rejection.](http://arxiv.org/abs/2307.02932) | 本文研究了具有拒绝的回归问题，并调查了将其视为标准回归任务来学习预测器的无拒绝学习策略。 |
| [^402] | [Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information.](http://arxiv.org/abs/2307.02799) | 本文提出了一种使用张量回归进行少样本个性化显著性预测的方法，以保留个性化显著性图的结构全局信息。 |
| [^403] | [Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing.](http://arxiv.org/abs/2307.00494) | 使用基于图形平滑的Gibbs采样方法（GGS）优化蛋白质适应性，消除了突变距离的限制，同时提高了搜索效率。该方法在发现高适应性蛋白质方面达到了最先进水平。 |
| [^404] | [The curse of dimensionality in operator learning.](http://arxiv.org/abs/2306.15924) | 算子学习中存在维度诅咒，但对于由Hamilton-Jacobi方程定义的解算子可以克服维度诅咒。 |
| [^405] | [Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement.](http://arxiv.org/abs/2306.12803) | 本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。 |
| [^406] | [LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning.](http://arxiv.org/abs/2306.09910) | 本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。 |
| [^407] | [DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer.](http://arxiv.org/abs/2306.08175) | 提出了一种基于Conformer的新型动态上下文传递机制DCTX-Conformer，解决了流式识别性能差距的问题，相比于现有最优解，识别结果的误差率提高了25%，但对延迟影响可以忽略不计。 |
| [^408] | [Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models.](http://arxiv.org/abs/2306.08018) | Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。 |
| [^409] | [Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning.](http://arxiv.org/abs/2306.04815) | 本文通过研究SGD训练损失中的尖峰现象，提出了“投石机”优化现象，通过增加与真实预测器的平均梯度外积对齐来促进特征学习，并证明较小的批量大小可提高泛化性能。 |
| [^410] | [Designing Decision Support Systems Using Counterfactual Prediction Sets.](http://arxiv.org/abs/2306.03928) | 本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。 |
| [^411] | [Memorization Capacity of Multi-Head Attention in Transformers.](http://arxiv.org/abs/2306.02010) | 本文研究了多头注意力机制的记忆能力，发现在特定的假设下，注意力层可以记忆Ω(Hn)个示例序列，这对于理解transformers的记忆容量有重要意义。 |
| [^412] | [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming.](http://arxiv.org/abs/2305.18436) | 本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。 |
| [^413] | [Multi-Objective Genetic Algorithm for Multi-View Feature Selection.](http://arxiv.org/abs/2305.18352) | 多视角数据提高了预测模型的准确性，但也使得高维数据增加，影响模型泛化能力。研究者提出了一种多视角多目标特征选择遗传算法（MMFS-GA），用于从多视角数据中选择最优的特征子集以提高模型精度和可解释性。 |
| [^414] | [On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models.](http://arxiv.org/abs/2305.17583) | 本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。 |
| [^415] | [Learning Safety Constraints from Demonstrations with Unknown Rewards.](http://arxiv.org/abs/2305.16147) | CoCoRL是一种从不知道奖励的已知安全演示中推断约束的方法，可以用于Constrained Markov Decision Process（CMDP），并且对于几乎最优演示能够无误差收敛于真实的安全集。 |
| [^416] | [Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge.](http://arxiv.org/abs/2305.15086) | 本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。 |
| [^417] | [A principled deep learning approach for geological facies generation.](http://arxiv.org/abs/2305.13318) | 本研究使用基于深度学习原理的生成对抗网络和深度变分推理应用于地质岩相生成，针对地下渠道进行了有条件模拟，并且比传统地质统计模型具有更高水平的准确性和物理逼真性。 |
| [^418] | [Deep Temporal Graph Clustering.](http://arxiv.org/abs/2305.10738) | 提出通用框架TGC 用于 deep temporal graph clustering, 解决了时间图只能作为静态图处理的难题，实现了对动态信息的聚类。实验证明了 TGC 的优越性。 |
| [^419] | [A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit.](http://arxiv.org/abs/2303.15495) | 本文介绍了一种基于神经网络的数据驱动方法，可以跨所有公交线路集体预测公交车到达每个交通点的时间，解决公交运输中公交车到达时间不准确和可靠的问题。 |
| [^420] | [Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis.](http://arxiv.org/abs/2302.14186) | 本文提出了一种基于Fisher线性判别的领域自适应模型，该模型是两个假设的凸组合，可以在不访问任何单个源任务的直接信息的情况下计算最优分类器，并在基于EEG和ECG的分类设置中展示了其有效性。 |
| [^421] | [Retrosynthetic Planning with Dual Value Networks.](http://arxiv.org/abs/2301.13755) | PDVN是一种新的在线训练算法，它在逆向合成规划中利用双价值网络优化完整的路线，成功率和效率上均优于现有方法。 |
| [^422] | [On the Expressive Power of Geometric Graph Neural Networks.](http://arxiv.org/abs/2301.09308) | 本文提出了几何版本的Weisfeiler-Leman测试(GWL)，可以区分几何图形，揭示了关键设计选择如何影响几何GNN的表现力 |
| [^423] | [FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model.](http://arxiv.org/abs/2211.07160) | FedTracker是第一个为联邦学习模型提供所有权验证和追溯性的保护框架，采用双层保护方案，并利用持续学习原则提高保护性能。 |
| [^424] | [Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems.](http://arxiv.org/abs/2211.00617) | 本文研究了有限时间随机线性二次控制问题中策略梯度方法的全局线性收敛性，并提出了基于Fisher几何和Bures-Wasserstein几何的几何感知梯度下降算法，该算法以线性速率全局收敛到最优策略。 |
| [^425] | [AMPNet: Attention as Message Passing for Graph Neural Networks.](http://arxiv.org/abs/2210.09475) | AMPNet是一种用于图神经网络的基于注意力的消息传递层，能够对节点进行逐个特征编码，并通过跨节点注意力模型特征级别的交互。在实际生物系统上进行广泛基准测试表明，AMPNet在fMRI信号重建方面优于现有基准，并通过案例研究验证了其发现有意义的特征级别交互的能力。 |
| [^426] | [Boundary-Aware Uncertainty for Feature Attribution Explainers.](http://arxiv.org/abs/2210.02419) | 本文提出了一种名为高斯过程解释不确定性（GPEC）框架，它可以对复杂的黑盒分类器进行可靠的特征归因，该框架结合了决策边界感知不确定性和解释函数逼近不确定性，能够生成一个统一的不确定性估计。 |
| [^427] | [A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism.](http://arxiv.org/abs/2209.06356) | 本论文提出了一种在离散动作空间中构建同态映射的新方法，通过使用环境动力学的部分模型来推断相同状态的状态动作对，从而减小状态-动作空间的大小。 |
| [^428] | [Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond.](http://arxiv.org/abs/2209.06177) | 本论文研究了用于节点分类的图形数据集的特征化，发现目前常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。作者提出了一种新的同质性测量指标，称为调整同质性，该指标满足更多期望特性，并具有较少在图形机器学习文献中使用的特点。 |
| [^429] | [Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting.](http://arxiv.org/abs/2207.07827) | 本文提出了一种通用记忆驱动变压器，通过集成多个时间序列特征来驱动预测过程，逐步引入噪声以增强泛化能力，在多个数据集上实现了更优秀的预测性能。 |

# 详细

[^1]: 在图上的小样本学习：从元学习到预训练和提示

    Few-Shot Learning on Graphs: from Meta-learning to Pre-training and Prompting

    [https://rss.arxiv.org/abs/2402.01440](https://rss.arxiv.org/abs/2402.01440)

    本文综述了图上的小样本学习的最新发展，将现有的研究方法划分为元学习、预训练和混合方法三大类别，并对它们的优缺点进行了比较。还提出了未来的研究方向。

    

    图表示学习是图中心任务中的关键步骤，在这方面已经取得了重大进展。早期的技术通常在端到端的设置中运行，性能严重依赖于充足的标记数据的可用性。这个限制引发了图上的小样本学习的出现，其中每个任务只有少量的任务特定标签可用。鉴于这个领域的广泛文献，本综述试图综合最近的发展，提供比较性的见解，并确定未来的方向。我们将现有的研究系统地分为三个主要类别：元学习方法、预训练方法和混合方法，并在每个类别中进行细粒度的分类，以帮助读者进行方法选择。在每个类别中，我们分析这些方法之间的关系并比较它们的优缺点。最后，我们概述了图上的小样本学习未来的方向。

    Graph representation learning, a critical step in graph-centric tasks, has seen significant advancements. Earlier techniques often operate in an end-to-end setting, where performance heavily relies on the availability of ample labeled data. This constraint has spurred the emergence of few-shot learning on graphs, where only a few task-specific labels are available for each task. Given the extensive literature in this field, this survey endeavors to synthesize recent developments, provide comparative insights, and identify future directions. We systematically categorize existing studies into three major families: meta-learning approaches, pre-training approaches, and hybrid approaches, with a finer-grained classification in each family to aid readers in their method selection process. Within each category, we analyze the relationships among these methods and compare their strengths and limitations. Finally, we outline prospective future directions for few-shot learning on graphs to cata
    
[^2]: 用双手扭开盖子

    Twisting Lids Off with Two Hands

    [https://arxiv.org/abs/2403.02338](https://arxiv.org/abs/2403.02338)

    深度强化学习结合仿真到真实世界的转移为解决物体操纵问题提供了有力支持

    

    用两只多指手臂操纵物体一直是机器人领域的一项长期挑战，原因在于许多操纵任务的丰富接触性质以及协调高维度双手系统固有的复杂性。在这项工作中，我们考虑了使用两只手扭开各种瓶子盖的问题，并展示出使用深度强化学习在仿真中训练的策略可以有效地转移到现实世界。通过对物理建模、实时感知和奖励设计的新工程见解，该策略展示了一般化能力，能够贯穿各种看不见的物体，展示出动态和灵巧的行为。我们的发现证明了深度强化学习结合仿真到真实世界的转移仍然是解决前所未有复杂问题的操纵问题的一个有前途的方法。

    arXiv:2403.02338v1 Announce Type: cross  Abstract: Manipulating objects with two multi-fingered hands has been a long-standing challenge in robotics, attributed to the contact-rich nature of many manipulation tasks and the complexity inherent in coordinating a high-dimensional bimanual system. In this work, we consider the problem of twisting lids of various bottle-like objects with two hands, and demonstrate that policies trained in simulation using deep reinforcement learning can be effectively transferred to the real world. With novel engineering insights into physical modeling, real-time perception, and reward design, the policy demonstrates generalization capabilities across a diverse set of unseen objects, showcasing dynamic and dexterous behaviors. Our findings serve as compelling evidence that deep reinforcement learning combined with sim-to-real transfer remains a promising approach for addressing manipulation problems of unprecedented complexity.
    
[^3]: 梯度相关子空间学习抵抗灾难性遗忘

    Gradient Correlation Subspace Learning against Catastrophic Forgetting

    [https://arxiv.org/abs/2403.02334](https://arxiv.org/abs/2403.02334)

    GCSL是一种用于减少灾难性遗忘的新颖方法，通过检测和利用不受以前任务影响的权重子空间来训练新任务。

    

    在过去几年中，高效的持续学习技术一直是一个重要的研究课题。这样的学习面临的一个基本问题是之前学习的任务性能严重下降，也称为灾难性遗忘。本文介绍了一种新颖的方法，在增量类学习的情况下减少灾难性遗忘，名为梯度相关子空间学习（GCSL）。该方法检测到最不受以前任务影响的权重子空间，并将权重投影到该子空间中进行新任务的训练。该方法可以应用于给定网络架构的一个或多个层，并且所使用的子空间大小可以从层到层、任务到任务进行改变。

    arXiv:2403.02334v1 Announce Type: cross  Abstract: Efficient continual learning techniques have been a topic of significant research over the last few years. A fundamental problem with such learning is severe degradation of performance on previously learned tasks, known also as catastrophic forgetting. This paper introduces a novel method to reduce catastrophic forgetting in the context of incremental class learning called Gradient Correlation Subspace Learning (GCSL). The method detects a subspace of the weights that is least affected by previous tasks and projects the weights to train for the new task into said subspace. The method can be applied to one or more layers of a given network architectures and the size of the subspace used can be altered from layer to layer and task to task. Code will be available at \href{https://github.com/vgthengane/GCSL}{https://github.com/vgthengane/GCSL}
    
[^4]: COMMIT: 针对语义攻击对多传感器融合系统的鲁棒性认证

    COMMIT: Certifying Robustness of Multi-Sensor Fusion Systems against Semantic Attacks

    [https://arxiv.org/abs/2403.02329](https://arxiv.org/abs/2403.02329)

    提出了第一个针对多传感器融合系统的鲁棒性认证框架COMMIT，以确保其针对语义攻击的稳健性。

    

    多传感器融合系统（MSFs）在现代自动驾驶汽车（AVs）中作为感知模块发挥着至关重要的作用。因此，确保它们针对现实世界中常见且逼真的敌对语义转换（如旋转和位移）的稳健性对于AVs的安全至关重要。本文提出了第一个针对多传感器融合系统的鲁棒性认证框架COMMIT，以对抗语义攻击。

    arXiv:2403.02329v1 Announce Type: new  Abstract: Multi-sensor fusion systems (MSFs) play a vital role as the perception module in modern autonomous vehicles (AVs). Therefore, ensuring their robustness against common and realistic adversarial semantic transformations, such as rotation and shifting in the physical world, is crucial for the safety of AVs. While empirical evidence suggests that MSFs exhibit improved robustness compared to single-modal models, they are still vulnerable to adversarial semantic transformations. Despite the proposal of empirical defenses, several works show that these defenses can be attacked again by new adaptive attacks. So far, there is no certified defense proposed for MSFs. In this work, we propose the first robustness certification framework COMMIT certify robustness of multi-sensor fusion systems against semantic attacks. In particular, we propose a practical anisotropic noise mechanism that leverages randomized smoothing with multi-modal data and perfo
    
[^5]: 对比区域引导：在视觉-语言模型中提高定位准确性而无需训练

    Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training

    [https://arxiv.org/abs/2403.02325](https://arxiv.org/abs/2403.02325)

    引入了对比区域引导（CRG）方法，实现了在视觉-语言模型中无需训练即可使模型响应视觉提示并取得显著改进。

    

    通过突出图像中特别相关的区域，可以改善视觉-语言模型（VLMs）在各种视觉-语言（VL）任务上的性能，引导模型更密切地关注这些感兴趣的区域。我们引入了对比区域引导（CRG），这是一种无需训练的引导方法，可以使开源的VLMs响应视觉提示，并在各种VL任务中取得显著改进。

    arXiv:2403.02325v1 Announce Type: cross  Abstract: Highlighting particularly relevant regions of an image can improve the performance of vision-language models (VLMs) on various vision-language (VL) tasks by guiding the model to attend more closely to these regions of interest. For example, VLMs can be given a "visual prompt", where visual markers such as bounding boxes delineate key image regions. However, current VLMs that can incorporate visual guidance are either proprietary and expensive or require costly training on curated data that includes visual prompts. We introduce Contrastive Region Guidance (CRG), a training-free guidance method that enables open-source VLMs to respond to visual prompts. CRG contrasts model outputs produced with and without visual prompts, factoring out biases revealed by the model when answering without the information required to produce a correct answer (i.e., the model's prior). CRG achieves substantial improvements in a wide variety of VL tasks: When
    
[^6]: 在LLM推理中平衡吞吐量和延迟权衡的研究：Sarathi-Serve方法

    Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve

    [https://arxiv.org/abs/2403.02310](https://arxiv.org/abs/2403.02310)

    引入了一种有效的LLM推理调度程序Sarathi-Serve，通过分块预装填技术平衡了GPU计算饱和和单个标记处理的挑战，实现了高吞吐量和低延迟。

    

    每个LLM服务请求经历两个阶段。首先是prefill阶段，处理整个输入提示以生成一个输出标记；第二个是decode阶段，逐个生成其余的输出标记。Prefill迭代具有较高的延迟，但由于输入提示的并行处理，可以使GPU计算饱和。相比之下，decode迭代具有较低的延迟，但也仅使用较低的计算资源，因为每个请求只处理一个标记。这使得对解码来说批处理非常有效，因此对整体吞吐量也很有效。然而，批量处理多个请求会导致prefill和decode迭代交错进行，这使得在实现高吞吐量和低延迟之间的平衡变得具有挑战性。我们引入了一个高效的LLM推理调度程序Sarathi-Serve，灵感来自我们最初为优化Sarathi的吞吐量提出的技术。Sarathi-Serve利用了从Sarathi中引入的分块prefill技术。

    arXiv:2403.02310v1 Announce Type: new  Abstract: Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt to produce one output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.   We introduce an efficient LLM inference scheduler Sarathi-Serve inspired by the techniques we originally proposed for optimizing throughput in Sarathi. Sarathi-Serve leverages chunked-prefills from Sarathi 
    
[^7]: 超越专业化：评估MLLMs在年龄和性别估计中的能力

    Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation

    [https://arxiv.org/abs/2403.02302](https://arxiv.org/abs/2403.02302)

    本研究评估了多模态大型语言模型（MLLMs）在年龄和性别估计中的能力，对不同模型进行了比较，揭示了它们在特定任务上的优势和劣势。

    

    最近，多模态大型语言模型（MLLMs）变得异常流行。像ChatGPT-4V和Gemini这样功能强大的商用模型，以及像LLaVA这样的开源模型，本质上都是通用模型，应用于解决各种各样的任务，包括计算机视觉中的任务。这些神经网络具有如此强大的通用知识和推理能力，以至于它们已被证明能够处理甚至未经专门训练的任务。我们将迄今为止最强大的MLLMs的能力进行了比较：ShareGPT4V、ChatGPT、LLaVA-Next 进行了专门任务的年龄和性别估计，与我们的最新专业化模型MiVOLO进行了比较。我们还更新了MiVOLO，并在本文中提供了详细信息和新的指标。这种比较产生了一些有趣的结果和关于参与模型的优点和缺点的见解。此外，我们尝试了各种微调方法

    arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
    
[^8]: 对学习截断高斯分布估计的统计查询下界

    Statistical Query Lower Bounds for Learning Truncated Gaussians

    [https://arxiv.org/abs/2403.02300](https://arxiv.org/abs/2403.02300)

    本研究的主要发现是对于学习截断高斯分布估计的问题，我们证明了存在一个统计查询下界，表明在这个任务中存在着超多项式信息-计算差距。

    

    我们研究了在截断设置中估计均值的问题，其中截断集来自一个低复杂度集合$\mathcal{C}$。具体地，对于一个固定但未知的截断集$S \subseteq \mathbb{R}^d$，我们可以访问从分布$\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$截断到集合$S$的样本。目标是在$\ell_2$-范数内以精度$\epsilon>0$估计$\boldsymbol\mu$。我们的主要结果是一个统计查询（SQ）下界，表明了在这个任务中存在着超多项式信息-计算差距。具体而言，我们展示了对于这个问题的任何SQ算法的复杂度均为$d^{\mathrm{poly}(1/\epsilon)}$，即使类$\mathcal{C}$是简单的，所以信息论上$\mathrm{poly}(d/\epsilon)$个样本足够。具体地，我们的SQ下界适用于当$\mathcal{C}$是有界个数的并集时

    arXiv:2403.02300v1 Announce Type: cross  Abstract: We study the problem of estimating the mean of an identity covariance Gaussian in the truncated setting, in the regime when the truncation set comes from a low-complexity family $\mathcal{C}$ of sets. Specifically, for a fixed but unknown truncation set $S \subseteq \mathbb{R}^d$, we are given access to samples from the distribution $\mathcal{N}(\boldsymbol{ \mu}, \mathbf{ I})$ truncated to the set $S$. The goal is to estimate $\boldsymbol\mu$ within accuracy $\epsilon>0$ in $\ell_2$-norm. Our main result is a Statistical Query (SQ) lower bound suggesting a super-polynomial information-computation gap for this task. In more detail, we show that the complexity of any SQ algorithm for this problem is $d^{\mathrm{poly}(1/\epsilon)}$, even when the class $\mathcal{C}$ is simple so that $\mathrm{poly}(d/\epsilon)$ samples information-theoretically suffice. Concretely, our SQ lower bound applies when $\mathcal{C}$ is a union of a bounded num
    
[^9]: Android应用隐私相关评论的十年大规模趋势分析

    A Decade of Privacy-Relevant Android App Reviews: Large Scale Trends

    [https://arxiv.org/abs/2403.02292](https://arxiv.org/abs/2403.02292)

    通过分析谷歌应用商店上1200万条隐私相关评论，研究了十年间隐私评论的大规模趋势，发现隐私评论呈现持续增长，探讨了热门和逐渐减少的隐私话题，以及不同国家用户对隐私问题看法的差异。

    

    我们展示了对谷歌应用商店上1200万条隐私相关评论的分析结果，这些评论跨越了10年时间。通过应用最先进的自然语言处理技术，我们能够在时间、国家、应用类型、不同隐私主题以及多种情感维度上检视用户对隐私问题的看法。我们发现隐私相关评论持续增长，并探究了一些热门话题（如数据删除和数据窃取），以及一些逐渐减少的话题（如涉及敏感权限的隐私相关评论）。尽管隐私评论来自200多个国家，但有33个国家提供了90%的隐私评论。我们通过检查每个国家用户评论的隐私主题分布来进行跨国家比较，发现地理接近并不意味着附近国家有类似的隐私观点。

    arXiv:2403.02292v1 Announce Type: new  Abstract: We present an analysis of 12 million instances of privacy-relevant reviews publicly visible on the Google Play Store that span a 10 year period. By leveraging state of the art NLP techniques, we can examine what users have been writing about privacy along multiple dimensions: time, countries, app types, diverse privacy topics, and even across a spectrum of emotions. We find consistent growth of privacy-relevant reviews, and explore topics that are trending (such as Data Deletion and Data Theft), as well as those on the decline (such as privacy-relevant reviews on sensitive permissions). We find that although privacy reviews come from more than 200 countries, 33 countries provide 90% of privacy reviews. We conduct a comparison across countries by examining the distribution of privacy topics a country's users write about, and find that geographic proximity is not a reliable indicator that nearby countries have similar privacy perspectives.
    
[^10]: Koopman辅助强化学习

    Koopman-Assisted Reinforcement Learning

    [https://arxiv.org/abs/2403.02290](https://arxiv.org/abs/2403.02290)

    该论文利用Koopman算子技术将非线性系统提升到新坐标系，在其中动力学变得近似线性，从而构建两种新的强化学习算法，以解决高维状态和非线性系统中传统方程难以解决的问题。

    

    鲍曼方程及其连续形式，即哈密顿-雅可比-贝尔曼（HJB）方程，在强化学习（RL）和控制理论中无处不在。然而，对于具有高维状态和非线性的系统，这些方程很快变得难以解决。本文探讨了数据驱动的Koopman算子与马尔可夫决策过程（MDPs）之间的联系，从而开发出两种新的RL算法来解决这些限制。我们利用Koopman算子技术将非线性系统提升到新坐标系，其中动力学变得近似线性，HJB方法更易处理。特别地，Koopman算子能够通过提升到的坐标系中的线性动态来捕获给定系统值函数的时间演化的期望。通过用控制动作参数化Koopman算子，我们构建了一个“Koopman张量”，以便实现...

    arXiv:2403.02290v1 Announce Type: new  Abstract: The Bellman equation and its continuous form, the Hamilton-Jacobi-Bellman (HJB) equation, are ubiquitous in reinforcement learning (RL) and control theory. However, these equations quickly become intractable for systems with high-dimensional states and nonlinearity. This paper explores the connection between the data-driven Koopman operator and Markov Decision Processes (MDPs), resulting in the development of two new RL algorithms to address these limitations. We leverage Koopman operator techniques to lift a nonlinear system into new coordinates where the dynamics become approximately linear, and where HJB-based methods are more tractable. In particular, the Koopman operator is able to capture the expectation of the time evolution of the value function of a given system via linear dynamics in the lifted coordinates. By parameterizing the Koopman operator with the control actions, we construct a ``Koopman tensor'' that facilitates the es
    
[^11]: 具有跳跃连接的物理信息神经网络用于模拟与控制气举油井

    Physics-Informed Neural Networks with Skip Connections for Modeling and Control of Gas-Lifted Oil Wells

    [https://arxiv.org/abs/2403.02289](https://arxiv.org/abs/2403.02289)

    通过在PINC网络中引入跳跃连接并完善ODE中的某些项，该研究改进了物理信息神经网络，提高了气举油井建模的准确性和性能

    

    神经网络虽然强大，但通常缺乏可解释性。物理信息神经网络（PINNs）通过将物理定律纳入损失函数来解决这一限制，使其适用于解决常微分方程（ODEs）和偏微分方程（PDEs）。最近引入的PINC框架将PINNs扩展到控制应用，允许对动态系统进行开放式长距离预测和控制。在这项工作中，我们增强了PINC用于模拟高度非线性系统，如气举油井。通过在PINC网络中引入跳跃连接并完善ODE中的某些项，我们在训练过程中实现了更准确的梯度，从而为油井系统的有效建模过程。我们提出的改进版PINC表现出卓越的性能，在油井应用中平均将验证预测误差降低了67％，并显著增强了梯度

    arXiv:2403.02289v1 Announce Type: new  Abstract: Neural networks, while powerful, often lack interpretability. Physics-Informed Neural Networks (PINNs) address this limitation by incorporating physics laws into the loss function, making them applicable to solving Ordinary Differential Equations (ODEs) and Partial Differential Equations (PDEs). The recently introduced PINC framework extends PINNs to control applications, allowing for open-ended long-range prediction and control of dynamic systems. In this work, we enhance PINC for modeling highly nonlinear systems such as gas-lifted oil wells. By introducing skip connections in the PINC network and refining certain terms in the ODE, we achieve more accurate gradients during training, resulting in an effective modeling process for the oil well system. Our proposed improved PINC demonstrates superior performance, reducing the validation prediction error by an average of 67% in the oil well application and significantly enhancing gradient 
    
[^12]: NatSGD: 一个包含语音、手势和演示的数据集，用于自然人机交互中机器人学习

    NatSGD: A Dataset with Speech, Gestures, and Demonstrations for Robot Learning in Natural Human-Robot Interaction

    [https://arxiv.org/abs/2403.02274](https://arxiv.org/abs/2403.02274)

    NatSGD是一个包含语音、手势和演示的数据集，旨在帮助机器人通过多模式人类命令理解任务，并强调了共同考虑语音和手势的重要性。

    

    最近多模式人机交互(HRI)数据集的发展突出了语音和手势的融合，扩展了机器人吸收显式和隐式人机交互洞见的能力。然而，现有的语音-手势HRI数据集通常专注于基本任务，如物体指向和推动，揭示了在扩展到复杂领域和优先考虑人类命令数据而不是机器人行为记录方面的限制。为了弥合这些差距，我们引入了NatSGD，一个多模式HRI数据集，涵盖了通过语音和手势传达的自然人类命令，这些命令与机器人行为演示同步。NatSGD作为机器学习和HRI研究交叉点上的基础资源，并展示了训练机器人通过多模式人类命令理解任务的有效性，强调了共同考虑语音和手势的重要性。我们已发布我们的数据集、模拟器等。

    arXiv:2403.02274v1 Announce Type: cross  Abstract: Recent advancements in multimodal Human-Robot Interaction (HRI) datasets have highlighted the fusion of speech and gesture, expanding robots' capabilities to absorb explicit and implicit HRI insights. However, existing speech-gesture HRI datasets often focus on elementary tasks, like object pointing and pushing, revealing limitations in scaling to intricate domains and prioritizing human command data over robot behavior records. To bridge these gaps, we introduce NatSGD, a multimodal HRI dataset encompassing human commands through speech and gestures that are natural, synchronized with robot behavior demonstrations. NatSGD serves as a foundational resource at the intersection of machine learning and HRI research, and we demonstrate its effectiveness in training robots to understand tasks through multimodal human commands, emphasizing the significance of jointly considering speech and gestures. We have released our dataset, simulator, a
    
[^13]: RIFF: 学习为语言模型的少样本微调改写输入

    RIFF: Learning to Rephrase Inputs for Few-shot Fine-tuning of Language Models

    [https://arxiv.org/abs/2403.02271](https://arxiv.org/abs/2403.02271)

    通过训练少样本释义模型并在训练和测试时用释义丰富数据，可以提高语言模型的性能，超出仅通过参数高效微调的效果。

    

    预训练语言模型（PLMs）可以精确地为下游文本处理任务进行微调。最近，研究人员引入了几种参数高效的微调方法，优化输入提示或调整少量模型参数（例如 LoRA）。在本研究中，我们探讨了改变原始任务的输入文本与参数高效微调方法相结合的影响。为了最有效地重写输入文本，我们使用最大边际似然目标训练了一个少样本释义模型。使用六个少样本文本分类数据集，我们展示了在训练和测试时用释义丰富数据可以提高性能，超出了仅通过参数高效微调可以实现的性能。

    arXiv:2403.02271v1 Announce Type: new  Abstract: Pre-trained Language Models (PLMs) can be accurately fine-tuned for downstream text processing tasks. Recently, researchers have introduced several parameter-efficient fine-tuning methods that optimize input prompts or adjust a small number of model parameters (e.g LoRA). In this study, we explore the impact of altering the input text of the original task in conjunction with parameter-efficient fine-tuning methods. To most effectively rewrite the input text, we train a few-shot paraphrase model with a Maximum-Marginal Likelihood objective. Using six few-shot text classification datasets, we show that enriching data with paraphrases at train and test time enhances the performance beyond what can be achieved with parameter-efficient fine-tuning alone.
    
[^14]: KnowPhish：大型语言模型遇见多模态知识图谱以增强基于参考的网络钓鱼检测

    KnowPhish: Large Language Models Meet Multimodal Knowledge Graphs for Enhancing Reference-Based Phishing Detection

    [https://arxiv.org/abs/2403.02253](https://arxiv.org/abs/2403.02253)

    提出了一个自动化知识收集流水线，发布了一个包含20k品牌的大规模多模态品牌知识库KnowPhish，可用于加强现有基于参考的网络钓鱼检测器的性能

    

    网络钓鱼攻击已给个人和企业造成了重大损失，因此需要开发强大高效的自动网络钓鱼检测方法。基于参考的网络钓鱼检测器（RBPDs）已成为最先进的方法，它们比较目标网页上的标志与已知标志集。然而，现有RBPDs的主要局限是它们依赖于手动构建的品牌知识库，这使得无法扩展到大量品牌，导致由于知识库中品牌覆盖不足而出现假阴性错误。为了解决这个问题，我们提出了一个自动化知识收集流水线，采用该流水线我们收集并发布了一个大规模多模态品牌知识库KnowPhish，包含20k个品牌和每个品牌的丰富信息。KnowPhish可以用来以即插即用的方式提升现有RBPDs的性能。

    arXiv:2403.02253v1 Announce Type: cross  Abstract: Phishing attacks have inflicted substantial losses on individuals and businesses alike, necessitating the development of robust and efficient automated phishing detection approaches. Reference-based phishing detectors (RBPDs), which compare the logos on a target webpage to a known set of logos, have emerged as the state-of-the-art approach. However, a major limitation of existing RBPDs is that they rely on a manually constructed brand knowledge base, making it infeasible to scale to a large number of brands, which results in false negative errors due to the insufficient brand coverage of the knowledge base. To address this issue, we propose an automated knowledge collection pipeline, using which we collect and release a large-scale multimodal brand knowledge base, KnowPhish, containing 20k brands with rich information about each brand. KnowPhish can be used to boost the performance of existing RBPDs in a plug-and-play manner. A second 
    
[^15]: 一个用于训练神经网络中低成本不确定性的预测刚性形式主义

    A prediction rigidity formalism for low-cost uncertainties in trained neural networks

    [https://arxiv.org/abs/2403.02251](https://arxiv.org/abs/2403.02251)

    通过解决受限优化问题，提出了“预测刚性”作为一种获得任意预先训练回归器不确定性的方法，扩展了方法应用于神经网络，并在多种回归任务上展示了其有效性。

    

    回归方法对科学和技术应用至关重要。然而，拟合模型在其训练领域之外可能极不可靠，因此在许多应用中，量化其不确定性是至关重要的。基于受限优化问题的解，我们提出“预测刚性”作为一种获得任意预先训练回归器不确定性的方法。我们建立了我们的框架与贝叶斯推断之间的强连接，并开发了一个允许新方法应用于神经网络的最后一层逼近。这种扩展提供了不需要对神经网络本身或其训练过程进行任何修改的低成本不确定性。我们展示了我们的方法在从简单玩具模型到化学和气象学应用的广泛回归任务中的有效性。

    arXiv:2403.02251v1 Announce Type: cross  Abstract: Regression methods are fundamental for scientific and technological applications. However, fitted models can be highly unreliable outside of their training domain, and hence the quantification of their uncertainty is crucial in many of their applications. Based on the solution of a constrained optimization problem, we propose "prediction rigidities" as a method to obtain uncertainties of arbitrary pre-trained regressors. We establish a strong connection between our framework and Bayesian inference, and we develop a last-layer approximation that allows the new method to be applied to neural networks. This extension affords cheap uncertainties without any modification to the neural network itself or its training procedure. We show the effectiveness of our method on a wide range of regression tasks, ranging from simple toy models to applications in chemistry and meteorology.
    
[^16]: 深度神经网络低精度训练的更好调度

    Better Schedules for Low Precision Training of Deep Neural Networks

    [https://arxiv.org/abs/2403.02243](https://arxiv.org/abs/2403.02243)

    该研究发现了用于低精度训练的循环精度训练调度的更好选择，进一步提高了训练效率

    

    低精度训练可以显著减少训练深度神经网络(DNNs)的计算开销。尽管存在许多这样的技术，但循环精度训练(CPT)，根据循环调度通过动态调整精度的方式进行训练，实现了训练效率的显著提升，同时实际上提高了DNN的性能。现有的CPT实现采用常见的学习率调度（例如，周期余弦调度），并在低精度训练中使用它们，但未与其他调度选项进行充分比较。我们定义了一套多样化的CPT调度，并分析它们在各种DNN训练方案中的性能，其中一些在低精度训练文献中尚未探索（例如，使用图神经网络进行节点分类）。通过这些实验，我们发现了提供进一步提升训练效率的替代CPT调度。

    arXiv:2403.02243v1 Announce Type: cross  Abstract: Low precision training can significantly reduce the computational overhead of training deep neural networks (DNNs). Though many such techniques exist, cyclic precision training (CPT), which dynamically adjusts precision throughout train- ing according to a cyclic schedule, achieves particularly impressive improvements in training efficiency, while actually improving DNN performance. Existing CPT implementations take common learning rate schedules (e.g., cyclical cosine sched- ules) and use them for low precision training without adequate comparisons to alternative scheduling options. We define a diverse suite of CPT schedules and analyze their performance across a variety of DNN training regimes, some of which are unexplored in the low precision training literature (e.g., node classification with graph neural networks). From these experiments, we discover alternative CPT schedules that offer further improvements in training efficiency 
    
[^17]: 神经红移：随机网络并非随机函数

    Neural Redshift: Random Networks are not Random Functions

    [https://arxiv.org/abs/2403.02241](https://arxiv.org/abs/2403.02241)

    本论文研究了未经训练的随机权重网络，发现即使简单的MLPs也具有强烈的归纳偏见，不同于传统观点的是，NNs并不具有固有的“简单偏见”，而是依赖于组件的作用。

    

    我们对神经网络（NNs）的泛化能力的理解仍不完整。目前的解释基于梯度下降（GD）的隐含偏见，但无法解释梯度自由方法中模型的能力，也无法解释最近观察到的未经训练网络的简单偏见。本文寻找NNs中的其他泛化源。为了独立于GD理解体系结构提供的归纳偏见，我们研究未经训练的随机权重网络。即使是简单的MLPs也表现出强烈的归纳偏见：在权重空间中进行均匀抽样会产生一个非常偏向于复杂性的函数分布。但与常规智慧不同，NNs并不具有固有的“简单偏见”。这一特性取决于组件，如ReLU、残差连接和层归一化。可利用替代体系结构构建偏向于任何复杂性水平的偏见。Transformers也具有这一特性。

    arXiv:2403.02241v1 Announce Type: cross  Abstract: Our understanding of the generalization capabilities of neural networks (NNs) is still incomplete. Prevailing explanations are based on implicit biases of gradient descent (GD) but they cannot account for the capabilities of models from gradient-free methods nor the simplicity bias recently observed in untrained networks. This paper seeks other sources of generalization in NNs.   Findings. To understand the inductive biases provided by architectures independently from GD, we examine untrained, random-weight networks. Even simple MLPs show strong inductive biases: uniform sampling in weight space yields a very biased distribution of functions in terms of complexity. But unlike common wisdom, NNs do not have an inherent "simplicity bias". This property depends on components such as ReLUs, residual connections, and layer normalizations. Alternative architectures can be built with a bias for any level of complexity. Transformers also inher
    
[^18]: Transformers在Masked Image Modeling中能够证明学习特征-位置相关性

    Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling

    [https://arxiv.org/abs/2403.02233](https://arxiv.org/abs/2403.02233)

    本文提供了关于使用MIM自监督预训练学习transformers的首个端到端理论，揭示了transformers如何学习到在具有空间结构的数据分布上突显特征-位置相关性的本地和多样化注意模式

    

    Masked image modeling (MIM)是一种新兴的自监督视觉预训练方法，它从未屏蔽的图像中预测随机屏蔽的补丁。然而，对于基于transformers的MIM的理论理解相当有限。本文提供了有关使用MIM自监督预训练学习一层transformers的首个端到端理论。我们提出了transformers如何学习到在具有空间结构的数据分布上突显特征-位置相关性的本地和多样化注意模式的理论机制。

    arXiv:2403.02233v1 Announce Type: new  Abstract: Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the i
    
[^19]: 机器学习在恶意软件检测中对Mal-API-2019数据集的全面评估

    Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection

    [https://arxiv.org/abs/2403.02232](https://arxiv.org/abs/2403.02232)

    该研究通过机器学习技术全面评估了恶意软件检测，发现集成方法（如随机森林和XGBoost）相较于其他方法在恶意软件检测中表现出更高的准确性、精确度和召回率。

    

    这项研究使用机器学习技术对恶意软件检测进行了彻底的探讨，重点评估了使用Mal-API-2019数据集的各种分类模型。旨在通过更有效地识别和缓解威胁来推进网络安全能力。研究探讨了集成和非集成的机器学习方法，如随机森林、XGBoost、K最近邻（KNN）和神经网络。特别强调了数据预处理技术的重要性，特别是TF-IDF表示和主成分分析，以提高模型性能。结果显示，随机森林和XGBoost等集成方法相比其他方法具有更高的准确性、精确度和召回率，突显了它们在恶意软件检测中的有效性。论文还讨论了限制和潜在的未来方向，强调了需要不断适应的必要性。

    arXiv:2403.02232v1 Announce Type: cross  Abstract: This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation t
    
[^20]: TPLLM: 基于预训练大语言模型的交通预测框架

    TPLLM: A Traffic Prediction Framework Based on Pretrained Large Language Models

    [https://arxiv.org/abs/2403.02221](https://arxiv.org/abs/2403.02221)

    TPLLM提出了基于预训练大语言模型的交通预测框架，能够在历史交通数据有限的地区实现准确预测和良好泛化能力

    

    arXiv:2403.02221v1 公告类型: 新摘要: 交通预测构成智能交通系统（ITS）范围内的一个关键方面，高精度预测对于有效交通管理具有深远意义。当前基于深度学习的交通预测模型的精度通常随着训练数据量的增加而呈上升趋势。然而，获取全面的交通时空数据往往面临诸多挑战，主要源自于数据收集和保存的高昂成本。因此，在历史交通数据有限的地区实现准确预测和良好泛化能力的模型开发是一个具有挑战性的问题。近年来迅速发展的预训练大语言模型（LLMs）在跨模态知识传输和少样本学习方面表现出卓越的能力。

    arXiv:2403.02221v1 Announce Type: new  Abstract: Traffic prediction constitutes a pivotal facet within the purview of Intelligent Transportation Systems (ITS), and the attainment of highly precise predictions holds profound significance for efficacious traffic management. The precision of prevailing deep learning-driven traffic prediction models typically sees an upward trend with a rise in the volume of training data. However, the procurement of comprehensive spatiotemporal datasets for traffic is often fraught with challenges, primarily stemming from the substantial costs associated with data collection and retention. Consequently, developing a model that can achieve accurate predictions and good generalization ability in areas with limited historical traffic data is a challenging problem. It is noteworthy that the rapidly advancing pretrained Large Language Models (LLMs) of recent years have demonstrated exceptional proficiency in cross-modality knowledge transfer and few-shot learn
    
[^21]: 通过可微分编程实现带不确定性量化的联合参数和参数化推断

    Joint Parameter and Parameterization Inference with Uncertainty Quantification through Differentiable Programming

    [https://arxiv.org/abs/2403.02215](https://arxiv.org/abs/2403.02215)

    通过可微分编程，本研究提出了一种新框架，能够联合估计和量化物理参数以及机器学习参数化，实现了高维参数空间内的在线训练和有效贝叶斯推断。

    

    精确地表示数值模拟中未知和亚网格物理过程的参数化(或闭合)并对其不确定性进行量化对于解析许多问题的粗粒化偏微分方程非常关键，这些问题包括天气和气候预测以及湍流模拟。最近的进展看到机器学习（ML）越来越多地应用于对这些亚网格过程建模，导致了通过与数值求解器集成开发混合物理-ML模型。在这项工作中，我们介绍了一种通过联合估计和不确定性量化物理参数和机器学习参数化的新框架，利用了可微分编程。通过在线训练和高维参数空间内的有效贝叶斯推断实现，这种方法借助可微分编程的能力实现。

    arXiv:2403.02215v1 Announce Type: new  Abstract: Accurate representations of unknown and sub-grid physical processes through parameterizations (or closure) in numerical simulations with quantified uncertainty are critical for resolving the coarse-grained partial differential equations that govern many problems ranging from weather and climate prediction to turbulence simulations. Recent advances have seen machine learning (ML) increasingly applied to model these subgrid processes, resulting in the development of hybrid physics-ML models through the integration with numerical solvers. In this work, we introduce a novel framework for the joint estimation and uncertainty quantification of physical parameters and machine learning parameterizations in tandem, leveraging differentiable programming. Achieved through online training and efficient Bayesian inference within a high-dimensional parameter space, this approach is enabled by the capabilities of differentiable programming. This proof 
    
[^22]: 通过正则化流进行互信息估计

    Mutual Information Estimation via Normalizing Flows

    [https://arxiv.org/abs/2403.02187](https://arxiv.org/abs/2403.02187)

    通过引入基于正则化流的估计器，该方法能够实现对原始数据进行互信息估计，并且在高维数据方面表现出优势。

    

    我们提出了一种新颖的方法来解决互信息（MI）估计问题，即引入基于正则化流的估计器。该估计器将原始数据映射到具有已知互信息闭合形式表达式的目标分布。我们证明了我们的方法产生了原始数据的互信息估计。通过高维数据的实验结果展示了所提出估计器的优势。

    arXiv:2403.02187v1 Announce Type: new  Abstract: We propose a novel approach to the problem of mutual information (MI) estimation via introducing normalizing flows-based estimator. The estimator maps original data to the target distribution with known closed-form expression for MI. We demonstrate that our approach yields MI estimates for the original data. Experiments with high-dimensional data are provided to show the advantages of the proposed estimator.
    
[^23]: ChatGPT主题和情感建模在金融中的应用优化

    Distilled ChatGPT Topic & Sentiment Modeling with Applications in Finance

    [https://arxiv.org/abs/2403.02185](https://arxiv.org/abs/2403.02185)

    ChatGPT被应用于金融领域，通过知识蒸馏和迁移学习融合的训练方法，生成轻量级主题和情感分类模型，成功应用于量化投资场景。

    

    在这项研究中，利用ChatGPT创建了简化模型，生成易于解释的特征。这些特征用于评估从财报电话会议中得出的财务结果。我们详细介绍了一个融合了知识蒸馏和迁移学习的训练方法，得到了轻量级的主题和情感分类模型，而准确率损失不大。这些模型经过专家标注的数据集评估。论文还探讨了两个实际案例研究，突显了生成的特征如何在量化投资场景中有效利用。

    arXiv:2403.02185v1 Announce Type: cross  Abstract: In this study, ChatGPT is utilized to create streamlined models that generate easily interpretable features. These features are then used to evaluate financial outcomes from earnings calls. We detail a training approach that merges knowledge distillation and transfer learning, resulting in lightweight topic and sentiment classification models without significant loss in accuracy. These models are assessed through a dataset annotated by experts. The paper also delves into two practical case studies, highlighting how the generated features can be effectively utilized in quantitative investing scenarios.
    
[^24]: 推理过程中不是所有LLMs的层都是必要的

    Not all Layers of LLMs are Necessary during Inference

    [https://arxiv.org/abs/2403.02181](https://arxiv.org/abs/2403.02181)

    推理过程中，根据输入实例的不同难易程度，本文提出了一种名为AdaInfer的算法，可以自适应地使用浅层和深层，从而节省了计算资源。

    

    大型语言模型（LLMs）的推理阶段非常昂贵。理想的LLMs推理阶段可以利用更少的计算资源，同时仍保持其能力（例如泛化和上下文学习能力）。本文尝试回答一个问题：“在LLMs推理过程中，我们可以为简单实例使用浅层，并为难以处理的实例使用深层吗？”为了回答这个问题，我们首先通过统计分析跨任务激活的层来指出并非所有层在推理过程中都是必要的。然后，我们提出了一种简单的算法，名为AdaInfer，根据输入实例自适应地确定推理终止时刻。更重要的是，AdaInfer不改变LLMs参数，并在任务之间保持泛化能力。对知名LLMs（即Llama2系列和OPT）的实验证明，AdaInfer节省了平均14.8%的计算资源，甚至在情感方面高达50%。

    arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
    
[^25]: 掩面思想:简单地掩盖部分推理步骤可以提高语言模型对数学推理的学习

    Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models

    [https://arxiv.org/abs/2403.02178](https://arxiv.org/abs/2403.02178)

    引入对输入的扰动，通过随机掩盖思维链中的某些标记，可显著提高语言模型在推理任务中的学习效果

    

    在推理任务中，即使是一个轻微的错误也可能导致不准确的结果，从而导致大型语言模型在这些领域的性能不佳。我们提出的方法避免了外部资源，而是依赖于引入对输入的扰动。我们的训练方法随机掩盖了链式思维中的某些标记，这种技术对推理任务特别有效。

    arXiv:2403.02178v1 Announce Type: cross  Abstract: In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models in such domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a technique we found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K, this method achieved a 5% improvement in accuracy over standard supervised fine-tuning with a few codes modified and no additional labeling effort. Furthermore, it is complementary to existing methods. When integrated with related data augmentation methods, it leads to an average improvement of 3% im
    
[^26]: 使用基于GAN的自动编码器预测宇宙大尺度结构演化

    Predicting large scale cosmological structure evolution with GAN-based autoencoders

    [https://arxiv.org/abs/2403.02171](https://arxiv.org/abs/2403.02171)

    使用基于GAN的自动编码器在宇宙学模拟中尝试预测结构演化，发现在2D模拟中能够很好地预测暗物质场的结构演化，但在3D模拟中表现更差，提供速度场作为输入后结果显著改善。

    

    宇宙学模拟在从初始条件预测和理解大尺度结构形成中起着关键作用。我们利用基于GAN的自动编码器尝试预测模拟中的结构演化。自动编码器是在描述暗物质场演化的2D和3D N体模拟生成的图像和立方体上进行训练的。我们发现，虽然自动编码器可以很好地预测2D模拟暗物质场的结构演化，但在类似条件下，仅使用密度场作为输入情况下，在3D模拟中表现明显更差。然而，提供速度场作为输入能够大大改善结果，预测类似，而无论输入和目标之间的时间差异如何。

    arXiv:2403.02171v1 Announce Type: cross  Abstract: Cosmological simulations play a key role in the prediction and understanding of large scale structure formation from initial conditions. We make use of GAN-based Autoencoders (AEs) in an attempt to predict structure evolution within simulations. The AEs are trained on images and cubes issued from respectively 2D and 3D N-body simulations describing the evolution of the dark matter (DM) field. We find that while the AEs can predict structure evolution for 2D simulations of DM fields well, using only the density fields as input, they perform significantly more poorly in similar conditions for 3D simulations. However, additionally providing velocity fields as inputs greatly improves results, with similar predictions regardless of time-difference between input and target.
    
[^27]: 改进的时间分段集成方法用于时间序列建模

    Recency-Weighted Temporally-Segmented Ensemble for Time-Series Modeling

    [https://arxiv.org/abs/2403.02150](https://arxiv.org/abs/2403.02150)

    引入了Recency-Weighted Temporally-Segmented（ReWTS）集成模型，利用块状方法进行多步预测，可以专门化模型并优化预测效果。

    

    在工艺行业中，时间序列建模面对处理复杂、多方面和不断演变的数据特征的挑战。传统的单一模型方法往往难以捕捉多样动态之间的相互作用，导致预测不够优化。因此，我们引入了Recency-Weighted Temporally-Segmented（ReWTS，发音为`roots'）集成模型，这是一种新颖的基于块的多步预测方法。ReWTS模型的关键特征有两个：1）通过将训练数据划分为数据块并对每个块训练一个模型，有助于将模型专门化为不同的动态。2）在推断阶段，一个优化过程评估最近的过去中的每个模型，并选择活动模型，以便能够召回以前学习的动态的适当混合来预测未来。这种方法不仅捕捉了每个时期的细微差异，而且能够自适应。

    arXiv:2403.02150v1 Announce Type: cross  Abstract: Time-series modeling in process industries faces the challenge of dealing with complex, multi-faceted, and evolving data characteristics. Conventional single model approaches often struggle to capture the interplay of diverse dynamics, resulting in suboptimal forecasts. Addressing this, we introduce the Recency-Weighted Temporally-Segmented (ReWTS, pronounced `roots') ensemble model, a novel chunk-based approach for multi-step forecasting. The key characteristics of the ReWTS model are twofold: 1) It facilitates specialization of models into different dynamics by segmenting the training data into `chunks' of data and training one model per chunk. 2) During inference, an optimization procedure assesses each model on the recent past and selects the active models, such that the appropriate mixture of previously learned dynamics can be recalled to forecast the future. This method not only captures the nuances of each period, but also adapt
    
[^28]: Inf2Guard：一种信息论框架，用于学习防止推断攻击的隐私保护表示

    Inf2Guard: An Information-Theoretic Framework for Learning Privacy-Preserving Representations against Inference Attacks

    [https://arxiv.org/abs/2403.02116](https://arxiv.org/abs/2403.02116)

    Inf2Guard提出了一种信息论防御框架，应对三种主要类型的推断攻击，其中包括两个相互信息目标，分别用于隐私保护和效用保留。

    

    机器学习（ML）容易受到推断攻击（例如成员推断、属性推断和数据重构等）的威胁，这些攻击旨在推断训练数据或数据集的私人信息。现有的防御措施只针对一种特定类型的攻击设计，且会牺牲较大的效用，或很快被自适应攻击击破。我们提出了一种信息论防御框架，名为Inf2Guard，用于抵御三种主要类型的推断攻击。我们的框架受到表示学习成功的启发，认为学习共享表示不仅节省时间/成本，而且有益于许多下游任务。一般来说，Inf2Guard包括两个相互信息目标，分别用于隐私保护和效用保留。Inf2Guard具有许多优点：它促进了针对特定推断攻击的定制目标的设计；它提供了一种通用的...

    arXiv:2403.02116v1 Announce Type: new  Abstract: Machine learning (ML) is vulnerable to inference (e.g., membership inference, property inference, and data reconstruction) attacks that aim to infer the private information of training data or dataset. Existing defenses are only designed for one specific type of attack and sacrifice significant utility or are soon broken by adaptive attacks. We address these limitations by proposing an information-theoretic defense framework, called Inf2Guard, against the three major types of inference attacks. Our framework, inspired by the success of representation learning, posits that learning shared representations not only saves time/costs but also benefits numerous downstream tasks. Generally, Inf2Guard involves two mutual information objectives, for privacy protection and utility preservation, respectively. Inf2Guard exhibits many merits: it facilitates the design of customized objectives against the specific inference attack; it provides a gener
    
[^29]: 迭代$Q$-网络：超越单步贝尔曼算子

    Iterated $Q$-Network: Beyond the One-Step Bellman Operator

    [https://arxiv.org/abs/2403.02107](https://arxiv.org/abs/2403.02107)

    引入了迭代$Q$-网络（iQN）方法，通过一次考虑多次迭代的贝尔曼算子来改进值基强化学习方法，在理论上可行，并在实验中展示其在游戏和控制环境中的优势。

    

    值基强化学习（RL）方法依赖于贝尔曼算子的应用，该算子需要从样本中进行近似。大多数方法包括交替应用贝尔曼算子和随后投影步骤到考虑的函数空间的迭代方案。然而，我们观察到这些算法可以通过一次考虑多次迭代的贝尔曼算子来改进。因此，我们引入了迭代$Q$-网络（iQN），这是一种新颖的方法，它学习一系列$Q$函数逼近，其中每个$Q$函数都作为下一个函数链中的目标。我们证明了iQN在理论上是可行的，并展示了它如何可以无缝地用于值基和演员-评论方法。我们在Atari$2600$游戏和连续控制MuJoCo环境中在实验上展示了它的优势。

    arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
    
[^30]: 建模多模态社交互动：具有密集对齐表示的新挑战和基线

    Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations

    [https://arxiv.org/abs/2403.02090](https://arxiv.org/abs/2403.02090)

    提出了三个新的具有挑战性的任务来模拟多人之间的细粒度动态，并为社交推理游戏设置提供了广泛的数据注释；同时提出了一种新颖的多模态基线方法，利用密集对齐的语言-视觉表示。

    

    理解涉及言语和非言语线索的社交互动对有效解释社交情境至关重要。然而，大多数关于多模态社交线索的先前工作主要集中在单人行为上，或依赖于与多方环境中的话语密切对齐的整体视觉表示。它们在建模多方互动的复杂动态方面存在局限。在本文中，我们介绍了三个新的具有挑战性的任务，以建模多人之间的细粒度动态：话语目标识别、代词指代消解和提及玩家预测。我们为社交推理游戏设置中的这些新挑战提供了广泛的数据注释。此外，我们提出了一种新颖的多模态基线，通过将视觉特征与其对应的话语同步，利用密集对齐的语言-视觉表示，这有助于

    arXiv:2403.02090v1 Announce Type: cross  Abstract: Understanding social interactions involving both verbal and non-verbal cues is essential to effectively interpret social situations. However, most prior works on multimodal social cues focus predominantly on single-person behaviors or rely on holistic visual representations that are not densely aligned to utterances in multi-party environments. They are limited in modeling the intricate dynamics of multi-party interactions. In this paper, we introduce three new challenging tasks to model the fine-grained dynamics between multiple people: speaking target identification, pronoun coreference resolution, and mentioned player prediction. We contribute extensive data annotations to curate these new challenges in social deduction game settings. Furthermore, we propose a novel multimodal baseline that leverages densely aligned language-visual representations by synchronizing visual features with their corresponding utterances. This facilitates
    
[^31]: 低信噪比下以雷达为基础的无人机检测与分类的混合量子神经网络优势

    Hybrid Quantum Neural Network Advantage for Radar-Based Drone Detection and Classification in Low Signal-to-Noise Ratio

    [https://arxiv.org/abs/2403.02080](https://arxiv.org/abs/2403.02080)

    当信噪比较低时，混合量子神经网络在雷达检测和分类问题中表现优越。

    

    在本文中，我们研究了使用混合量子神经网络(HQNN)和可比较的经典卷积神经网络(CNN)解决雷达检测和分类问题的性能。具体地，我们采用了从电磁理论中得出的相当复杂的雷达时间序列模型，即Martin-Mulgrew模型，这个模型被用来模拟带有旋转叶片的物体（如无人机）的雷达返回。我们发现，当信噪比（SNR）较高时，CNN在检测和分类方面优于HQNN。然而，在低SNR范围（这在实践中最为关注）中，HQNN的性能被发现优于类似架构的CNN。

    arXiv:2403.02080v1 Announce Type: cross  Abstract: In this paper, we investigate the performance of a Hybrid Quantum Neural Network (HQNN) and a comparable classical Convolution Neural Network (CNN) for detection and classification problem using a radar. Specifically, we take a fairly complex radar time-series model derived from electromagnetic theory, namely the Martin-Mulgrew model, that is used to simulate radar returns of objects with rotating blades, such as drones. We find that when that signal-to-noise ratio (SNR) is high, CNN outperforms the HQNN for detection and classification. However, in the low SNR regime (which is of greatest interest in practice) the performance of HQNN is found to be superior to that of the CNN of a similar architecture.
    
[^32]: 在重尾扰动下噪声(S)GD的差分隐私

    Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations

    [https://arxiv.org/abs/2403.02051](https://arxiv.org/abs/2403.02051)

    在重尾扰动下，噪声SGD实现了差分隐私保证，适用于广泛的损失函数类，特别是非凸函数。

    

    将重尾噪声注入随机梯度下降(SGD)的迭代中已经引起越来越多的关注。尽管对导致的算法的各种理论性质进行了分析，主要来自学习理论和优化视角，但它们的隐私保护性质尚未建立。为了弥补这一缺口，我们为噪声SGD提供差分隐私(DP)保证，当注入的噪声遵循$\alpha$-稳定分布时，该分布包括一系列重尾分布(具有无限方差)以及高斯分布。考虑$(\epsilon,\delta)$-DP框架，我们表明带有重尾扰动的SGD实现了$(0,\tilde{\mathcal{O}}(1/n))$-DP的广泛损失函数类，这些函数可以是非凸的，这里$n$是数据点的数量。作为一项显着的副产品，与以往的工作相反，该工作要求有界se

    arXiv:2403.02051v1 Announce Type: cross  Abstract: Injecting heavy-tailed noise to the iterates of stochastic gradient descent (SGD) has received increasing attention over the past few years. While various theoretical properties of the resulting algorithm have been analyzed mainly from learning theory and optimization perspectives, their privacy preservation properties have not yet been established. Aiming to bridge this gap, we provide differential privacy (DP) guarantees for noisy SGD, when the injected noise follows an $\alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the Gaussian distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that SGD with heavy-tailed perturbations achieves $(0, \tilde{\mathcal{O}}(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded se
    
[^33]: 通过量身定制的损失函数进行约束获取的深度神经网络

    Deep Neural Network for Constraint Acquisition through Tailored Loss Function

    [https://arxiv.org/abs/2403.02042](https://arxiv.org/abs/2403.02042)

    这项工作介绍了一种基于深度神经网络（DNN）和符号回归的新方法，通过设置适当的损失函数，可以直接从数据集中提取约束。

    

    从数据中学习约束的重要性被其在实际问题解决中的潜在应用所强调。尽管约束在建模和解决中很受欢迎，但从数据中学习约束的方法仍然相对稀缺。此外，建模的复杂任务需要专业知识，容易出错，因此约束获取方法通过从示例或解决方案和非解决方案的行为中学到的约束自动化该过程提供了解决方案。本工作引入了一种基于符号回归的深度神经网络（DNN）的创新方法，通过设置适当的损失函数，可以直接从数据集中提取约束。利用当前方法，直接制定约束得以实现。此外，考虑到DNN的广泛预先开发的架构和功能，与其他框架的连接和扩展可以预见到。

    arXiv:2403.02042v1 Announce Type: new  Abstract: The significance of learning constraints from data is underscored by its potential applications in real-world problem-solving. While constraints are popular for modeling and solving, the approaches to learning constraints from data remain relatively scarce. Furthermore, the intricate task of modeling demands expertise and is prone to errors, thus constraint acquisition methods offer a solution by automating this process through learnt constraints from examples or behaviours of solutions and non-solutions. This work introduces a novel approach grounded in Deep Neural Network (DNN) based on Symbolic Regression that, by setting suitable loss functions, constraints can be extracted directly from datasets. Using the present approach, direct formulation of constraints was achieved. Furthermore, given the broad pre-developed architectures and functionalities of DNN, connections and extensions with other frameworks could be foreseen.
    
[^34]: ReLU$^k$神经网络在具有点奇异性的Gevrey类问题上的指数表达能力

    Exponential Expressivity of ReLU$^k$ Neural Networks on Gevrey Classes with Point Singularities

    [https://arxiv.org/abs/2403.02035](https://arxiv.org/abs/2403.02035)

    证明了ReLU$^k$神经网络在具有点奇异性的Gevrey类问题上具有指数表达能力，中间结果表明可以使用ReLU和ReLU$^2$激活组合的神经网络精确模拟高阶有限元。

    

    我们分析了在有界的多面体域$\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$中具有点奇异性的平滑函数的深度神经网络模拟速率。我们证明了在Sobolev空间中的指数模拟速率，这与神经元的数量以及Gevrey正则解类内的非零系数的数量有关，这些解类是根据$\mathrm{D}$中加权Sobolev标度定义的，包括I.M. Babu\v{s}ka和B.Q. Guo的可数范数空间。作为中间结果，我们证明了在任意正则的简单形域$\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$上的连续、分段多项式高阶(``$p$-version'')有限元，其元素多项式度为$p\in\mathbb{N}$，可以被ReLU和ReLU$^2$激活组合的神经网络精确模拟。在形状正则的多面体域$\mathrm{D}$的简单形域划分上，神经元的数量以及t

    arXiv:2403.02035v1 Announce Type: cross  Abstract: We analyze deep Neural Network emulation rates of smooth functions with point singularities in bounded, polytopal domains $\mathrm{D} \subset \mathbb{R}^d$, $d=2,3$. We prove exponential emulation rates in Sobolev spaces in terms of the number of neurons and in terms of the number of nonzero coefficients for Gevrey-regular solution classes defined in terms of weighted Sobolev scales in $\mathrm{D}$, comprising the countably-normed spaces of I.M. Babu\v{s}ka and B.Q. Guo.   As intermediate result, we prove that continuous, piecewise polynomial high order (``$p$-version'') finite elements with elementwise polynomial degree $p\in\mathbb{N}$ on arbitrary, regular, simplicial partitions of polyhedral domains $\mathrm{D} \subset \mathbb{R}^d$, $d\geq 2$ can be exactly emulated by neural networks combining ReLU and ReLU$^2$ activations. On shape-regular, simplicial partitions of polytopal domains $\mathrm{D}$, both the number of neurons and t
    
[^35]: 具有定时器的Mealy机器的主动学习

    Active Learning of Mealy Machines with Timers

    [https://arxiv.org/abs/2403.02019](https://arxiv.org/abs/2403.02019)

    这篇论文提出了一种用于查询学习具有定时器的Mealy机器的算法，在实现上明显比已有算法更有效率。

    

    我们在黑盒环境中提出了第一个用于查询学习一般类别的具有定时器的Mealy机器（MMTs）的算法。我们的算法是Vaandrager等人的L＃算法对定时设置的扩展。类似于Waga提出的用于学习定时自动机的算法，我们的算法受到Maler＆Pnueli思想的启发。我们的算法和Waga的算法都使用符号查询进行基础语言学习，然后使用有限数量的具体查询进行实现。然而，Waga需要指数级的具体查询来实现单个符号查询，而我们只需要多项式数量。这是因为要学习定时自动机，学习者需要确定每个转换的确切卫兵和重置（有指数多种可能性），而要学习MMT，学习者只需要弄清楚哪些先前的转换导致超时。正如我们之前的工作所示，

    arXiv:2403.02019v1 Announce Type: cross  Abstract: We present the first algorithm for query learning of a general class of Mealy machines with timers (MMTs) in a black-box context. Our algorithm is an extension of the L# algorithm of Vaandrager et al. to a timed setting. Like the algorithm for learning timed automata proposed by Waga, our algorithm is inspired by ideas of Maler & Pnueli. Based on the elementary languages of, both Waga's and our algorithm use symbolic queries, which are then implemented using finitely many concrete queries. However, whereas Waga needs exponentially many concrete queries to implement a single symbolic query, we only need a polynomial number. This is because in order to learn a timed automaton, a learner needs to determine the exact guard and reset for each transition (out of exponentially many possibilities), whereas for learning an MMT a learner only needs to figure out which of the preceding transitions caused a timeout. As shown in our previous work, 
    
[^36]: 公平潜在表示的二分图变分自动编码器，以解决生态网络中的抽样偏差问题

    Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks

    [https://arxiv.org/abs/2403.02011](https://arxiv.org/abs/2403.02011)

    本研究提出了一种公平潜在表示的二分图变分自动编码器方法，以解决生态网络中的抽样偏差问题，通过在损失函数中引入额外的HSIC惩罚项，确保了潜在空间结构与连续变量的独立性。

    

    我们提出一种方法，使用图嵌入来表示二分网络，以解决研究生态网络所面临的挑战，比如连接植物和传粉者等网络，需考虑许多协变量，尤其要控制抽样偏差。我们将变分图自动编码器方法调整为二分情况，从而能够在潜在空间中生成嵌入，其中两组节点的位置基于它们的连接概率。我们将在社会学中常考虑的公平性框架转化为生态学中的抽样偏差问题。通过在损失函数中添加Hilbert-Schmidt独立准则（HSIC）作为额外惩罚项，我们确保潜在空间结构与连续变量（与抽样过程相关）无关。最后，我们展示了我们的方法如何改变我们对生态网络的理解。

    arXiv:2403.02011v1 Announce Type: cross  Abstract: We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological n
    
[^37]: 粒子梯度下降的误差界限，以及log-Sobolev和Talagrand不等式的推广

    Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities

    [https://arxiv.org/abs/2403.02004](https://arxiv.org/abs/2403.02004)

    证明了粒子梯度下降算法对于一般化的log-Sobolev和Polyak-Lojasiewicz不等式模型的收敛速度，以及推广了Bakry-Emery定理。

    

    我们证明了粒子梯度下降(PGD)~(Kuntz等人，2023)的非渐近误差界限，这是一种最大似然估计的算法，用于离散化自由能梯度流获得的大型潜变量模型。我们首先展示了对于满足一般化log-Sobolev和Polyak-Lojasiewicz不等式（LSI和PLI）的模型，流以指数速度收敛到自由能的极小化集合。我们通过将最优输运文献中众所周知的结果（LSI意味着Talagrand不等式）及其在优化文献中的对应物（PLI意味着所谓的二次增长条件）扩展并应用到我们的新设置，来实现这一点。我们还推广了Bakry-Emery定理，并展示了对于具有强凹对数似然的模型，LSI/PLI的概括成立。

    arXiv:2403.02004v1 Announce Type: new  Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD)~(Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that, for models satisfying a condition generalizing both the log-Sobolev and the Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively), the flow converges exponentially fast to the set of minimizers of the free energy. We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\L}I implies the so-called quadratic growth condition), and applying it to our new setting. We also generalize the Bakry--\'Emery Theorem and show that the LSI/P{\L}I generalization holds for models with strongly concave log-likelihoods. For such m
    
[^38]: 关于多项式混沌展开中的分数矩估计

    On Fractional Moment Estimation from Polynomial Chaos Expansion

    [https://arxiv.org/abs/2403.01948](https://arxiv.org/abs/2403.01948)

    本文提出了一种通过多项式混沌展开直接估计分数矩的新方法，可用于不确定性量化中的概率分布估计，实验结果表明该方法性能优越。

    

    分数统计矩被用于不确定性量化的各种任务，包括概率分布的估计。然而，通过统计抽样估计昂贵数学模型的分数统计矩是具有挑战的，因为由于计算能力的限制通常无法创建大型实验设计。本文提出了一种新颖的方法，通过多项式混沌展开直接估计分数矩。具体而言，从确定性PCE系数获得的前四个统计矩被用于通过H\"{o}lder不等式估计任意分数矩。所提出的方法用于估计三个不断复杂的数值示例中的统计矩和概率分布。实验结果表明，所提出的方法取得了卓越的性能。

    arXiv:2403.01948v1 Announce Type: cross  Abstract: Fractional statistical moments are utilized for various tasks of uncertainty quantification, including the estimation of probability distributions. However, an estimation of fractional statistical moments of costly mathematical models by statistical sampling is challenging since it is typically not possible to create a large experimental design due to limitations in computing capacity. This paper presents a novel approach for the analytical estimation of fractional moments, directly from polynomial chaos expansions. Specifically, the first four statistical moments obtained from the deterministic PCE coefficients are used for an estimation of arbitrary fractional moments via H\"{o}lder's inequality. The proposed approach is utilized for an estimation of statistical moments and probability distributions in three numerical examples of increasing complexity. Obtained results show that the proposed approach achieves a superior performance i
    
[^39]: 一种对称变换生成模型

    A Generative Model of Symmetry Transformations

    [https://arxiv.org/abs/2403.01946](https://arxiv.org/abs/2403.01946)

    通过构建一种生成模型来明确捕捉数据中的对称变换，从而提高模型的泛化能力和稳健性。

    

    准确捕捉数据的对称变换可以导致具有强大泛化能力的高效模型，尽管涉及对称性的方法通常需要先验知识。最近在直接从数据集中学习这些对称性方面已取得了进展，但其中大部分工作集中在判别设置上。本文构建了一个生成模型，明确旨在捕捉数据中的对称性，从而产生一个以可解释方式学习数据中存在哪些对称性的模型。我们提供了一个简单的算法来有效学习我们的生成模型，并展示了其在仿射和颜色变换下捕捉对称性的能力。将我们的对称模型与现有的生成模型相结合，可以实现更高的边际测试对数似然和对数据稀疏性的稳健性。

    arXiv:2403.01946v1 Announce Type: new  Abstract: Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we construct a generative model that explicitly aims to capture symmetries in the data, resulting in a model that learns which symmetries are present in an interpretable way. We provide a simple algorithm for efficiently learning our generative model and demonstrate its ability to capture symmetries under affine and color transformations. Combining our symmetry model with existing generative models results in higher marginal test-log-likelihoods and robustness to data sparsification.
    
[^40]: 用傅立叶基函数弥合增强差距：重新思考图像分类中的频率增强

    Fourier-basis Functions to Bridge Augmentation Gap: Rethinking Frequency Augmentation in Image Classification

    [https://arxiv.org/abs/2403.01944](https://arxiv.org/abs/2403.01944)

    提出了辅助傅立叶基增强（AFA）的方法，通过在频域进行增强，填补了视觉增强遗留的增强差距，提高了模型的鲁棒性。

    

    计算机视觉模型通常在部署到现实场景中时性能下降，这是由于训练过程中未考虑到的输入出现了意外变化。数据增强通常用于解决这个问题，因为它旨在增加数据变化性并减少训练和测试数据之间的分布差距。然而，常见的视觉增强可能无法保证计算机视觉模型的广泛鲁棒性。在本文中，我们提出了辅助傅立叶基增强（AFA），这是一种针对频率域增强的补充技术，填补了视觉增强遗留的增强差距。我们通过简单高效的对抗设置展示了傅立叶基加性噪声增强的效用。我们的结果表明，AFA有助于模型对常见损坏、OOD泛化以及模型性能随着性能增强的一致性。

    arXiv:2403.01944v1 Announce Type: cross  Abstract: Computer vision models normally witness degraded performance when deployed in real-world scenarios, due to unexpected changes in inputs that were not accounted for during training. Data augmentation is commonly used to address this issue, as it aims to increase data variety and reduce the distribution gap between training and test data. However, common visual augmentations might not guarantee extensive robustness of computer vision models. In this paper, we propose Auxiliary Fourier-basis Augmentation (AFA), a complementary technique targeting augmentation in the frequency domain and filling the augmentation gap left by visual augmentations. We demonstrate the utility of augmentation via Fourier-basis additive noise in a straightforward and efficient adversarial setting. Our results show that AFA benefits the robustness of models against common corruptions, OOD generalization, and consistency of performance of models against increasing
    
[^41]: 通过拓扑样本选择减轻图中的标签噪音

    Mitigating Label Noise on Graph via Topological Sample Selection

    [https://arxiv.org/abs/2403.01942](https://arxiv.org/abs/2403.01942)

    提出了一种通过利用图数据的拓扑信息来增强信息选择过程的$“\textit{拓扑样本选择}$”（TSS）方法。

    

    尽管精心注释的基准测试取得了成功，但当现实世界的图数据带有噪声标签时，现有图神经网络（GNNs）的有效性在实践中可能会受到相当大的影响。以往在样本选择方面的探索已被证明是一种有效的应对噪声标签的鲁棒学习方法，然而，传统研究侧重于i.i.d数据，当转向非独立同分布的图数据和GNNs时，仍然存在两个值得关注的挑战：(1) 位于拓扑类边界附近的节点对分类非常有信息量，但无法通过启发式样本选择成功区分。(2) 没有可用的衡量标准考虑图的拓扑信息以促进图中的样本选择。为了解决这一困境，我们提出了一种$“\textit{拓扑样本选择}$（TSS）”方法，通过利用拓扑信息来提升图中信息丰富的样本选择过程。

    arXiv:2403.01942v1 Announce Type: new  Abstract: Despite the success of the carefully-annotated benchmarks, the effectiveness of existing graph neural networks (GNNs) can be considerably impaired in practice when the real-world graph data is noisily labeled. Previous explorations in sample selection have been demonstrated as an effective way for robust learning with noisy labels, however, the conventional studies focus on i.i.d data, and when moving to non-iid graph data and GNNs, two notable challenges remain: (1) nodes located near topological class boundaries are very informative for classification but cannot be successfully distinguished by the heuristic sample selection. (2) there is no available measure that considers the graph topological information to promote sample selection in a graph. To address this dilemma, we propose a $\textit{Topological Sample Selection}$ (TSS) method that boosts the informative sample selection process in a graph by utilising topological information.
    
[^42]: FlowPrecision: 利用线性量化推进基于FPGA的实时流体流量估计

    FlowPrecision: Advancing FPGA-Based Real-Time Fluid Flow Estimation with Linear Quantization

    [https://arxiv.org/abs/2403.01922](https://arxiv.org/abs/2403.01922)

    本研究将线性量化应用于FPGA-based soft sensors中，显著提高了神经网络模型精度，通过硬件优化实现了降低均方误差和提升推理速度，为实时流体流量估计提供了高效、精确的替代方案

    

    在工业和环境监测中，实现实时和精确的流体流量测量仍然是一个重要挑战。本研究将线性量化应用于基于FPGA的软传感器中，用于流体流量估计，通过克服传统定点量化的局限性，显著增强了神经网络模型的精度。我们的方法通过针对性的硬件优化，实现了均方误差的最多减少10.10％，推理速度显著提高了9.39％。经过多个数据集的验证，我们的研究结果表明，优化后的基于FPGA的量化模型可以提供高效、精确的实时推理，为普遍自主系统中的云处理提供了一种可行的替代方案。

    arXiv:2403.01922v1 Announce Type: new  Abstract: In industrial and environmental monitoring, achieving real-time and precise fluid flow measurement remains a critical challenge. This study applies linear quantization in FPGA-based soft sensors for fluid flow estimation, significantly enhancing Neural Network model precision by overcoming the limitations of traditional fixed-point quantization. Our approach achieves up to a 10.10% reduction in Mean Squared Error and a notable 9.39% improvement in inference speed through targeted hardware optimizations. Validated across multiple data sets, our findings demonstrate that the optimized FPGA-based quantized models can provide efficient, accurate real-time inference, offering a viable alternative to cloud-based processing in pervasive autonomous systems.
    
[^43]: 使用凸优化和列子集选择的矩阵完成

    Matrix Completion with Convex Optimization and Column Subset Selection

    [https://arxiv.org/abs/2403.01919](https://arxiv.org/abs/2403.01919)

    该方法结合了列子集选择和低秩矩阵完成问题的理论基础，提出使用凸优化解决矩阵恢复问题，同时通过实验验证了算法的正确性和性能。

    

    我们介绍了一种用于矩阵恢复问题的两步方法。我们的方法结合了列子集选择和低秩矩阵完成问题的理论基础。提出的方法在每一步中解决一个凸优化任务。我们提出了两种实现我们的列选择矩阵完成（CSMC）方法的算法，每种算法针对不同规模的问题。我们对所提出的方法进行了正式分析，在分析中我们阐明了必要的假设和找到正确解的概率。在论文的第二部分，我们展示了实验工作的结果。数值实验验证了算法的正确性和性能。为了研究矩阵大小、秩和缺失元素比例对解的质量和计算时间的影响，我们在合成数据上进行了实验。所提出的方法被应用于两个真实世界的例子。

    arXiv:2403.01919v1 Announce Type: new  Abstract: We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-li
    
[^44]: 基于证据工具总线的自适应驾驶系统连续保障案例创建

    Towards Continuous Assurance Case Creation for ADS with the Evidential Tool Bus

    [https://arxiv.org/abs/2403.01918](https://arxiv.org/abs/2403.01918)

    基于证据工具总线的自适应驾驶系统连续保障案例创建，实现了从预定义模式构建和持续维护保障案例，有效解决了系统需求变化和组件恶化带来的保障案例维护复杂性。

    

    保障案例已成为认证安全关键系统的一个重要组成部分。手动定义保障案例模式可能无法避免，保障案例模式的系统特定实例既昂贵又耗时。当系统需保障的系统需求发生变化，或者因系统组件恶化导致保障声明无效时，即使是执行学习启用组件时，维护系统的保障案例变得尤为复杂。 本文报告了我们利用工具集成框架Evidential Tool Bus (ETB)从预定义的保障案例模式构建和持续维护保障案例的初步经验。具体地，我们在汽车领域的工业自动泊车系统上演示了保障过程。我们介绍了所提供的保

    arXiv:2403.01918v1 Announce Type: cross  Abstract: An assurance case has become an integral component for the certification of safety-critical systems. While manually defining assurance case patterns can be not avoided, system-specific instantiations of assurance case patterns are both costly and time-consuming. It becomes especially complex to maintain an assurance case for a system when the requirements of the System-Under-Assurance change, or an assurance claim becomes invalid due to, e.g., degradation of a systems component, as common when deploying learning-enabled components. In this paper, we report on our preliminary experience leveraging the tool integration framework Evidential Tool Bus (ETB) for the construction and continuous maintenance of an assurance case from a predefined assurance case pattern. Specifically, we demonstrate the assurance process on an industrial Automated Valet Parking system from the automotive domain. We present the formalization of the provided assur
    
[^45]: Hebbian-Hopfield网络关联记忆的容量

    Capacity of the Hebbian-Hopfield network associative memory

    [https://arxiv.org/abs/2403.01907](https://arxiv.org/abs/2403.01907)

    Hopfield提出了一种Hebbian学习规则的神经网络模型，研究了关联记忆的容量，指出网络的容量与模式大小线性相关，提出了容量预测值，并使用两个著名模式的吸引盆地来探讨相关问题。

    

    在Hopfield的论文中，他提出了一种基于Hebbian学习规则的神经网络模型，并提出了它如何可以高效地作为一个关联记忆。在研究随机二进制模式时，他还发现，如果存储模式检索中容忍一小部分错误，网络的容量（最大记忆模式数，$m$）与每个模式的大小$n$呈线性关系。此外，他著名地预测了$\alpha_c=\lim_{n\rightarrow\infty}\frac{m}{n}\approx 0.14$。我们研究了这个非常相同的情景，使用了两种著名模式的吸引盆地：\textbf{\emph{(i)}}来自\cite{AmiGutSom85}的AGS；以及\textbf{\emph{(ii)}}来自\cite{Newman88,Louk94,Louk94a,Louk97,Tal98}的NLT。依赖于来自\cite{Stojnicflrdt23}的\emph{完全提升的随机对偶理论}（fl RDT），我们获得了在第一层提升上的以下明确容量特性描述：\begin{equation} \alpha ...

    arXiv:2403.01907v1 Announce Type: cross  Abstract: In \cite{Hop82}, Hopfield introduced a \emph{Hebbian} learning rule based neural network model and suggested how it can efficiently operate as an associative memory. Studying random binary patterns, he also uncovered that, if a small fraction of errors is tolerated in the stored patterns retrieval, the capacity of the network (maximal number of memorized patterns, $m$) scales linearly with each pattern's size, $n$. Moreover, he famously predicted $\alpha_c=\lim_{n\rightarrow\infty}\frac{m}{n}\approx 0.14$. We study this very same scenario with two famous pattern's basins of attraction: \textbf{\emph{(i)}} The AGS one from \cite{AmiGutSom85}; and \textbf{\emph{(ii)}} The NLT one from \cite{Newman88,Louk94,Louk94a,Louk97,Tal98}. Relying on the \emph{fully lifted random duality theory} (fl RDT) from \cite{Stojnicflrdt23}, we obtain the following explicit capacity characterizations on the first level of lifting:   \begin{equation}   \alpha
    
[^46]: 具有循环神经网络的储层系统的普适性

    Universality of reservoir systems with recurrent neural networks

    [https://arxiv.org/abs/2403.01900](https://arxiv.org/abs/2403.01900)

    讨论了具有循环神经网络的储层系统的逼近能力和统一强普适性，可以通过并行串联RNN储层构建这种类型的系统

    

    讨论了储层系统的逼近能力，其中储层是循环神经网络（RNN）。在我们的问题设定中，储层系统通过调整其线性输出来逼近一组函数，而储层保持不变。我们将展示我们所称的一类函数的RNN储层系统的统一强普适性。这意味着，对于任意正数，我们可以构建一个足够大的RNN储层系统，其对该类函数中每个函数的逼近误差都被正数从上方限定。这样的RNN储层系统是通过并行串联RNN储层构建的。

    arXiv:2403.01900v1 Announce Type: cross  Abstract: Approximation capability of reservoir systems whose reservoir is a recurrent neural network (RNN) is discussed. In our problem setting, a reservoir system approximates a set of functions just by adjusting its linear readout while the reservoir is fixed. We will show what we call uniform strong universality of a family of RNN reservoir systems for a certain class of functions to be approximated. This means that, for any positive number, we can construct a sufficiently large RNN reservoir system whose approximation error for each function in the class of functions to be approximated is bounded from above by the positive number. Such RNN reservoir systems are constructed via parallel concatenation of RNN reservoirs.
    
[^47]: 成功对抗样本的强鲁棒性界限：理论与实践

    Robustness Bounds on the Successful Adversarial Examples: Theory and Practice

    [https://arxiv.org/abs/2403.01896](https://arxiv.org/abs/2403.01896)

    本文提出了一个新的成功对抗样本概率上限的理论界限，取决于扰动范数、核函数以及训练数据集中最接近的不同标签对之间的距离，并且实验证明了该理论结果的有效性。

    

    对抗样本（AE）是一种针对机器学习的攻击方法，通过对数据添加不可感知的扰动来诱使错分。本文基于高斯过程（GP）分类，研究了成功AE的概率上限。我们证明了一个新的上界，取决于AE的扰动范数、GP中使用的核函数以及训练数据集中具有不同标签的最接近对之间的距离。令人惊讶的是，该上限不受样本数据集分布的影响。我们通过使用ImageNet的实验验证了我们的理论结果。此外，我们展示了改变核函数参数会导致成功AE概率上限的变化。

    arXiv:2403.01896v1 Announce Type: new  Abstract: Adversarial example (AE) is an attack method for machine learning, which is crafted by adding imperceptible perturbation to the data inducing misclassification. In the current paper, we investigated the upper bound of the probability of successful AEs based on the Gaussian Process (GP) classification. We proved a new upper bound that depends on AE's perturbation norm, the kernel function used in GP, and the distance of the closest pair with different labels in the training dataset. Surprisingly, the upper bound is determined regardless of the distribution of the sample dataset. We showed that our theoretical result was confirmed through the experiment using ImageNet. In addition, we showed that changing the parameters of the kernel function induces a change of the upper bound of the probability of successful AEs.
    
[^48]: 无监督距离度量学习用于多变量时间序列异常检测

    Unsupervised Distance Metric Learning for Anomaly Detection Over Multivariate Time Series

    [https://arxiv.org/abs/2403.01895](https://arxiv.org/abs/2403.01895)

    提出了一种无监督距离度量学习方法FCM-wDTW，用于多变量时间序列异常检测，通过将原始数据编码成潜在空间并通过聚类中心揭示正常维度关系，在实验中表现出竞争力的准确性和效率。

    

    基于距离的时间序列异常检测方法由于其相对非参数化的特性和可解释性而广泛存在。然而，常用的欧几里得距离对噪声敏感。虽然现有工作已经探讨了动态时间规整（DTW）以增强其稳健性，但它们仅支持多变量时间序列（MTS）上的监督任务，缺乏无监督方法。在这项工作中，我们提出了FCM-wDTW，一种用于多变量时间序列异常检测的无监督距离度量学习方法，它将原始数据编码成潜在空间，并通过聚类中心揭示正常维度关系。FCM-wDTW将局部加权DTW引入到模糊C均值聚类中，并有效地学习最佳的潜在空间，从而通过数据重建实现异常识别。针对11种不同类型的基准进行的实验表明，我们的方法具有竞争力的准确性和效率。

    arXiv:2403.01895v1 Announce Type: cross  Abstract: Distance-based time series anomaly detection methods are prevalent due to their relative non-parametric nature and interpretability. However, the commonly used Euclidean distance is sensitive to noise. While existing works have explored dynamic time warping (DTW) for its robustness, they only support supervised tasks over multivariate time series (MTS), leaving a scarcity of unsupervised methods. In this work, we propose FCM-wDTW, an unsupervised distance metric learning method for anomaly detection over MTS, which encodes raw data into latent space and reveals normal dimension relationships through cluster centers. FCM-wDTW introduces locally weighted DTW into fuzzy C-means clustering and learns the optimal latent space efficiently, enabling anomaly identification via data reconstruction. Experiments with 11 different types of benchmarks demonstrate our method's competitive accuracy and efficiency.
    
[^49]: 零成本基准上异步多保真度优化的快速基准测试

    Fast Benchmarking of Asynchronous Multi-Fidelity Optimization on Zero-Cost Benchmarks

    [https://arxiv.org/abs/2403.01888](https://arxiv.org/abs/2403.01888)

    通过引入用户友好的Python软件包，研究提出了有效的并行HPO方法，避免长时间等待实现快速评估。

    

    尽管深度学习取得了许多成功，但其结果往往取决于超参数的精心选择。然而，深度学习训练的耗时性使得超参数优化(HPO)是一项昂贵的工作，拖慢了高效HPO工具的开发。本工作通过引入一个用户友好的Python软件包，来解决这一挑战，促进零成本基准下高效的并行HPO。我们的方法根据存储在文件系统中的信息计算精确的返回顺序，消除了长时间的等待，实现了更快的HPO评估。

    arXiv:2403.01888v1 Announce Type: new  Abstract: While deep learning has celebrated many successes, its results often hinge on the meticulous selection of hyperparameters (HPs). However, the time-consuming nature of deep learning training makes HP optimization (HPO) a costly endeavor, slowing down the development of efficient HPO tools. While zero-cost benchmarks, which provide performance and runtime without actual training, offer a solution for non-parallel setups, they fall short in parallel setups as each worker must communicate its queried runtime to return its evaluation in the exact order. This work addresses this challenge by introducing a user-friendly Python package that facilitates efficient parallel HPO with zero-cost benchmarks. Our approach calculates the exact return order based on the information stored in file system, eliminating the need for long waiting times and enabling much faster HPO evaluations. We first verify the correctness of our approach through extensive t
    
[^50]: ICLN：输入凸损失网络用于决策集中学习

    ICLN: Input Convex Loss Network for Decision Focused Learning

    [https://arxiv.org/abs/2403.01875](https://arxiv.org/abs/2403.01875)

    提出了输入凸损失网络（ICLN），通过输入凸神经网络学习任务损失，为决策集中学习提供了全局替代损失。

    

    在不确定性条件下的决策问题中，预测未知参数通常被认为与优化部分无关。决策集中学习（DFL）是一个面向任务的框架，通过调整预测模型以为相应任务提供更好的决策来整合预测和优化。本文提出了输入凸损失网络（ICLN），这是一种新颖的全局替代损失，可以在一般的DFL范式中实现。ICLN通过输入凸神经网络学习任务损失，已经被保证为某些情况下是凸的。

    arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
    
[^51]: 对外分布泛化评估的调查

    A Survey on Evaluation of Out-of-Distribution Generalization

    [https://arxiv.org/abs/2403.01874](https://arxiv.org/abs/2403.01874)

    该论文是针对外部分布泛化评估进行的首次努力，重点在于评估模型的OOD泛化能力以及泛化的表现好坏。

    

    机器学习模型虽然越来越先进，但严重依赖IID假设，而由于不可避免的分布转换，在实践中往往无法满足此假设。这使得它们在风险敏感应用的部署中容易受到影响并且不可信。这种重要问题因此衍生出各种致力于开发能够进行外部分布（OOD）泛化的算法的分支。尽管有这些努力，但对OOD泛化的评估却受到的关注较少，这也是一个复杂且基本的问题。其目标不仅在于评估模型的OOD泛化能力是否强大，还要评估模型泛化良好或不佳之处。这涉及描述模型能够有效解决的分布转换类型，并确定给定模型的安全和风险输入区域。这篇论文是首次努力进行的

    arXiv:2403.01874v1 Announce Type: new  Abstract: Machine learning models, while progressively advanced, rely heavily on the IID assumption, which is often unfulfilled in practice due to inevitable distribution shifts. This renders them susceptible and untrustworthy for deployment in risk-sensitive applications. Such a significant problem has consequently spawned various branches of works dedicated to developing algorithms capable of Out-of-Distribution (OOD) generalization. Despite these efforts, much less attention has been paid to the evaluation of OOD generalization, which is also a complex and fundamental problem. Its goal is not only to assess whether a model's OOD generalization capability is strong or not, but also to evaluate where a model generalizes well or poorly. This entails characterizing the types of distribution shifts that a model can effectively address, and identifying the safe and risky input regions given a model. This paper serves as the first effort to conduct a 
    
[^52]: 通过锚多元分析改善泛化能力

    Improving generalisation via anchor multivariate analysis

    [https://arxiv.org/abs/2403.01865](https://arxiv.org/abs/2403.01865)

    引入因果正则化扩展到锚回归（AR）中，提出了与锚框架相匹配的损失函数确保稳健性，各种多元分析算法均在锚框架内，简单正则化增强了OOD设置中的稳健性，验证了锚正则化的多功能性和对因果推断方法论的推进。

    

    我们在锚回归（AR）中引入因果正则化扩展，以改善超出分布（OOD）的泛化能力。我们提出了与锚框架相匹配的损失函数，以确保对分布转移的稳健性。各种多元分析（MVA）算法，如（正交化）PLS、RRR和MLR，均在锚框架内。我们观察到简单的正则化增强了OOD设置中的稳健性。在合成和真实的气候科学问题中，为所选算法提供了估计器，展示了其一致性和有效性。经验验证突显了锚正则化的多功能性，强调其与MVA方法的兼容性，并强调其在增强可复制性的同时抵御分布转移中的作用。扩展的AR框架推进了因果推断方法论，解决了可靠OOD泛化的需求。

    arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
    
[^53]: RCoCo：黎曼空间中多重网络中的对比集体链接预测

    RCoCo: Contrastive Collective Link Prediction across Multiplex Network in Riemannian Space

    [https://arxiv.org/abs/2403.01864](https://arxiv.org/abs/2403.01864)

    在传统欧氏空间之外的黎曼空间中，针对多重社交网络中的对比集体链接预测问题，本文提出了新方法。

    

    链接预测通常研究的是在单个社交网络中观察到的节点之间未来相互连接的概率。现实场景往往呈现为具有在多个社交网络中活跃的共同（锚定）用户的多重网络。然而，在文献中，大多数现有研究要么研究单个网络中的内部链接预测，要么研究网络之间的外部链接预测（即网络对齐），并认为这两个学习任务是独立的，这仍然远离事实。在表示空间上，绝大多数现有方法是建立在传统的欧氏空间之上，忽视了社交网络的固有几何性。第三个问题是稀缺的锚定用户。注释锚定用户是费时费力且昂贵的，因此使用大量锚定用户是不切实际的。鉴于上述问题，我们提议研究一个具有挑战性的...

    arXiv:2403.01864v1 Announce Type: cross  Abstract: Link prediction typically studies the probability of future interconnection among nodes with the observation in a single social network. More often than not, real scenario is presented as a multiplex network with common (anchor) users active in multiple social networks. In the literature, most existing works study either the intra-link prediction in a single network or inter-link prediction among networks (a.k.a. network alignment), and consider two learning tasks are independent from each other, which is still away from the fact. On the representation space, the vast majority of existing methods are built upon the traditional Euclidean space, unaware of the inherent geometry of social networks. The third issue is on the scarce anchor users. Annotating anchor users is laborious and expensive, and thus it is impractical to work with quantities of anchor users. Herein, in light of the issues above, we propose to study a challenging yet p
    
[^54]: 奖励模型学习与直接策略优化：从人类偏好学习的比较分析

    Reward Model Learning vs. Direct Policy Optimization: A Comparative Analysis of Learning from Human Preferences

    [https://arxiv.org/abs/2403.01857](https://arxiv.org/abs/2403.01857)

    本文通过比较强化学习从人类反馈学习（RLHF）范式与最近提出的直接偏好优化（DPO）范式，对学习人类偏好进行了深入探讨，推导出了次优差距的统计界限，并提供了收敛速度的分析。

    

    本文通过系统比较从人类偏好学习的强化学习（RLHF）范式与最近提出的直接偏好优化（DPO）范式，迈向对学习人类偏好的更深入理解。我们以对数线性策略参数化和线性奖励函数的类为重点。为了比较这两种范式，我们首先对由RLHF和DPO引起的次优差距推导出极小-最大统计界限，假设可以访问确切解决优化问题的预言。我们就相对比较两种范式进行了详细讨论，同时考虑样本大小、策略和奖励类维数以及正则化温度。此外，我们将分析扩展到近似优化设置，并为RLHF和DPO分别推导出指数衰减的收敛速度。

    arXiv:2403.01857v1 Announce Type: new  Abstract: In this paper, we take a step towards a deeper understanding of learning from human preferences by systematically comparing the paradigm of reinforcement learning from human feedback (RLHF) with the recently proposed paradigm of direct preference optimization (DPO). We focus our attention on the class of loglinear policy parametrization and linear reward functions. In order to compare the two paradigms, we first derive minimax statistical bounds on the suboptimality gap induced by both RLHF and DPO, assuming access to an oracle that exactly solves the optimization problems. We provide a detailed discussion on the relative comparison between the two paradigms, simultaneously taking into account the sample size, policy and reward class dimensions, and the regularization temperature. Moreover, we extend our analysis to the approximate optimization setting and derive exponentially decaying convergence rates for both RLHF and DPO. Next, we an
    
[^55]: 一个提示词就足以提升预训练视觉-语言模型的对抗鲁棒性

    One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models

    [https://arxiv.org/abs/2403.01849](https://arxiv.org/abs/2403.01849)

    本文研究了预训练视觉-语言模型的对抗鲁棒性，提出了一种通过学习强韧文本提示来改善对抗攻击韧性的方法，称为Adversarial Prompt Tuning（APT），并在多个数据集和数据稀疏方案上进行了全面实验验证。

    

    像CLIP这样的大型预训练视觉-语言模型（VLMs），尽管具有显着的泛化能力，但对于对抗样本非常脆弱。本文从文本提示的新颖视角而非传统研究的模型权重（在本文中冻结）研究了VLMs的对抗鲁棒性。我们首先展示了对抗攻击和防御的有效性对使用的文本提示敏感。在此启发下，我们提出了一种通过学习VLMs的强韧文本提示来提高对抗攻击韧性的方法。提出的方法，称为Adversarial Prompt Tuning（APT），在计算和数据效率方面都非常有效。通过在15个数据集和4种数据稀疏方案（从1-shot到完全训练数据设置）上进行了大量实验，以展示APT相对于手工设计提示和其他最先进的适应方法的优越性。APT表现出色

    arXiv:2403.01849v1 Announce Type: cross  Abstract: Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent 
    
[^56]: NASH：用于硬件优化机器学习模型的神经架构搜索

    NASH: Neural Architecture Search for Hardware-Optimized Machine Learning Models

    [https://arxiv.org/abs/2403.01845](https://arxiv.org/abs/2403.01845)

    NASH是一种将神经架构搜索应用于机器学习硬件的新方法，可以帮助硬件设计实现高吞吐量、低延迟和优越的准确性表现。

    

    随着机器学习（ML）算法在越来越多的应用中部署，这些算法需要在高准确性、高吞吐量和低延迟之间取得更好的权衡。本文介绍了一种名为NASH的新方法，将神经架构搜索应用于机器学习硬件。使用NASH，硬件设计不仅可以实现高吞吐量和低延迟，还可以实现优越的准确性表现。本文提出了四个版本的NASH策略，所有这些策略显示出比原始模型更高的准确性。该策略可以应用于各种卷积神经网络，从众多模型操作中选择特定操作，引导训练过程朝向更高的准确性。实验结果显示，在 ResNet18 或 ResNet34 上应用NASH，与非NASH版本相比，可使Top1准确率提高高达3.1%，Top5准确率提高高达2.2%。

    arXiv:2403.01845v1 Announce Type: cross  Abstract: As machine learning (ML) algorithms get deployed in an ever-increasing number of applications, these algorithms need to achieve better trade-offs between high accuracy, high throughput and low latency. This paper introduces NASH, a novel approach that applies neural architecture search to machine learning hardware. Using NASH, hardware designs can achieve not only high throughput and low latency but also superior accuracy performance. We present four versions of the NASH strategy in this paper, all of which show higher accuracy than the original models. The strategy can be applied to various convolutional neural networks, selecting specific model operations among many to guide the training process toward higher accuracy. Experimental results show that applying NASH on ResNet18 or ResNet34 achieves a top 1 accuracy increase of up to 3.1% and a top 5 accuracy increase of up to 2.2% compared to the non-NASH version when tested on the Imag
    
[^57]: 在表格预测上优化预训练语言模型的方法

    Making Pre-trained Language Models Great on Tabular Prediction

    [https://arxiv.org/abs/2403.01841](https://arxiv.org/abs/2403.01841)

    提出了一种专门为表格数据预测而预训练的语言模型TP-BERTa，通过新颖的相对大小标记化方法和内部特征关注方法解决了预训练语言模型在数值特征值上的不兼容性问题

    

    深度神经网络（DNN）的可迁移性在图像和语言处理领域取得了显著进展。然而，由于表格之间的异质性，这种DNN的优势在表格数据预测（例如回归或分类任务）上仍未得到充分利用。本文提出了TP-BERTa，这是一种专门为表格数据预测而预训练的语言模型。具体而言，一种新颖的相对大小标记化方法将标量数值特征值转换为离散度高、高维度的标记，并且一种内部特征关注方法整合了特征名称和数值特征值。

    arXiv:2403.01841v1 Announce Type: new  Abstract: The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM model for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates fe
    
[^58]: 宏观辅助渐近保持神经网络用于线性辐射传递方程

    Macroscopic auxiliary asymptotic preserving neural networks for the linear radiative transfer equations

    [https://arxiv.org/abs/2403.01820](https://arxiv.org/abs/2403.01820)

    该论文提出了一种宏观辅助渐近保持神经网络（MA-APNN）方法，用于解决具有多尺度性质和高维度的时变线性辐射传递方程，利用了物理信息神经网络框架，并设计了新的自适应指数加权渐近保持损失函数，有效性得到了数值示例的验证。

    

    我们开发了一种宏观辅助渐近保持神经网络（MA-APNN）方法来解决具有多尺度性质和高维度的时变线性辐射传递方程（LRTEs）。为实现这一目标，我们利用了物理信息神经网络（PINNs）框架，并设计了一种新的自适应指数加权渐近保持（AP）损失函数，该函数包含了直接从原始传递方程导出的宏观辅助方程，并明确地包含扩散极限方程的信息。因此，随着尺度参数趋向于零，损失函数逐渐从输运态过渡到扩散极限态。此外，初始数据、边界条件和守恒定律作为损失的正则化项。我们提供了几个数值示例来展示MA-APNN的有效性。

    arXiv:2403.01820v1 Announce Type: cross  Abstract: We develop a Macroscopic Auxiliary Asymptotic-Preserving Neural Network (MA-APNN) method to solve the time-dependent linear radiative transfer equations (LRTEs), which have a multi-scale nature and high dimensionality. To achieve this, we utilize the Physics-Informed Neural Networks (PINNs) framework and design a new adaptive exponentially weighted Asymptotic-Preserving (AP) loss function, which incorporates the macroscopic auxiliary equation that is derived from the original transfer equation directly and explicitly contains the information of the diffusion limit equation. Thus, as the scale parameter tends to zero, the loss function gradually transitions from the transport state to the diffusion limit state. In addition, the initial data, boundary conditions, and conservation laws serve as the regularization terms for the loss. We present several numerical examples to demonstrate the effectiveness of MA-APNNs.
    
[^59]: Tsallis熵正则化用于线性可解MDP和线性二次调节器

    Tsallis Entropy Regularization for Linearly Solvable MDP and Linear Quadratic Regulator

    [https://arxiv.org/abs/2403.01805](https://arxiv.org/abs/2403.01805)

    本文介绍了Tsallis熵正则化方法，用于解决线性可解MDP和线性二次调节器问题，能够平衡探索和获得的控制法则的稀疏性。

    

    Shannon熵正则化被广泛应用于最优控制中，因为它能促进探索并增强鲁棒性，例如，Soft Actor-Critic中采用的最大熵强化学习。本文使用Tsallis熵（Shannon熵的单参数扩展）来正则化线性可解MDP和线性二次调节器。我们推导出这些问题的解，并展示了它在平衡探索和获得的控制法则的稀疏性方面的实用性。

    arXiv:2403.01805v1 Announce Type: cross  Abstract: Shannon entropy regularization is widely adopted in optimal control due to its ability to promote exploration and enhance robustness, e.g., maximum entropy reinforcement learning known as Soft Actor-Critic. In this paper, Tsallis entropy, which is a one-parameter extension of Shannon entropy, is used for the regularization of linearly solvable MDP and linear quadratic regulators. We derive the solution for these problems and demonstrate its usefulness in balancing between exploration and sparsity of the obtained control law.
    
[^60]: COLA: 跨城市移动性转换器用于人类轨迹模拟

    COLA: Cross-city Mobility Transformer for Human Trajectory Simulation

    [https://arxiv.org/abs/2403.01801](https://arxiv.org/abs/2403.01801)

    研究通过将强大的Transformer模型与外部移动数据相结合，探讨了跨城市人类轨迹模拟的问题，解决了知识转移中的适应性挑战。

    

    由日常移动设备产生的人类轨迹数据在城市规划和疫情防控等重要领域证明了其实用性。针对个人隐私问题，人类轨迹模拟引起了研究人员越来越多的关注，旨在为下游任务提供大量逼真的移动数据。然而，数据稀缺的普遍问题无疑降低了现有深度学习模型的可靠性。本文旨在探讨跨城市移动性转移的问题，把握人类轨迹的普遍模式，为Transformer模型增加外部移动数据。在跨城市知识转移中出现了两个关键挑战：1）如何使Transformer适应领域的异质性；2）如何校准Transformer以适应细微不同的长尾频率分布。

    arXiv:2403.01801v1 Announce Type: cross  Abstract: Human trajectory data produced by daily mobile devices has proven its usefulness in various substantial fields such as urban planning and epidemic prevention. In terms of the individual privacy concern, human trajectory simulation has attracted increasing attention from researchers, targeting at offering numerous realistic mobility data for downstream tasks. Nevertheless, the prevalent issue of data scarcity undoubtedly degrades the reliability of existing deep learning models. In this paper, we are motivated to explore the intriguing problem of mobility transfer across cities, grasping the universal patterns of human trajectories to augment the powerful Transformer with external mobility data. There are two crucial challenges arising in the knowledge transfer across cities: 1) how to transfer the Transformer to adapt for domain heterogeneity; 2) how to calibrate the Transformer to adapt for subtly different long-tail frequency distrib
    
[^61]: 朝着公平高效的基于学习的拥塞控制迈进

    Towards Fair and Efficient Learning-based Congestion Control

    [https://arxiv.org/abs/2403.01798](https://arxiv.org/abs/2403.01798)

    Astraea是一种新的学习型拥塞控制算法，通过多智能体深度强化学习框架，实现了快速收敛到公平性的稳定性。

    

    近年来，出现了大量基于学习的拥塞控制（CC）解决方案，表现优于传统的TCP方案。然而，它们未能提供一致良好的收敛性能，包括公平性、快速收敛和稳定性，原因在于它们的目标函数与这些性能之间存在不匹配。尽管直觉上这些性能在现有基于学习的CC中进行整合具有挑战性，因为：1）它们的训练环境是为了单流性能优化而设计的，无法进行合作多流优化；2）缺乏直接可度量的指标来将这些性能表示到训练目标函数中。我们提出了Astraea，一种新的基于学习的拥塞控制，确保以稳定性快速收敛到公平性。Astraea的核心是一个多智能体深度强化学习框架。

    arXiv:2403.01798v1 Announce Type: cross  Abstract: Recent years have witnessed a plethora of learning-based solutions for congestion control (CC) that demonstrate better performance over traditional TCP schemes. However, they fail to provide consistently good convergence properties, including {\em fairness}, {\em fast convergence} and {\em stability}, due to the mismatch between their objective functions and these properties. Despite being intuitive, integrating these properties into existing learning-based CC is challenging, because: 1) their training environments are designed for the performance optimization of single flow but incapable of cooperative multi-flow optimization, and 2) there is no directly measurable metric to represent these properties into the training objective function.   We present Astraea, a new learning-based congestion control that ensures fast convergence to fairness with stability. At the heart of Astraea is a multi-agent deep reinforcement learning framework 
    
[^62]: 利用混合数据驱动和物理信息规范学习神经网络的循环塑性

    Hybrid data-driven and physics-informed regularized learning of cyclic plasticity with Neural Networks

    [https://arxiv.org/abs/2403.01776](https://arxiv.org/abs/2403.01776)

    提出了一种利用混合数据驱动和物理信息规范的神经网络学习循环塑性的方法，实现了在少量训练数据情况下的高精度和稳定性，并且简化高效了模型架构。

    

    提出了一种可扩展、高效且可解释的机器学习方法，用以表示循环塑性并替代基于径向回归映射算法的传统材料模型。通过实现物理信息规范和背应力信息，实现了在少量训练数据的情况下高精度和稳定性。神经网络的卸载被最大程度地应用。所提出的模型架构与文献中的现有解决方案相比更简单、更高效，同时代表了一个完整的三维材料模型。通过用Armstrong-Frederick运动硬化模型获取的代理数据对该方法进行了验证。采用均方误差作为损失函数，规定了几项限制条件：内部变量的偏差特性、符合流动规则、弹性区域的区分等。

    arXiv:2403.01776v1 Announce Type: cross  Abstract: An extendable, efficient and explainable Machine Learning approach is proposed to represent cyclic plasticity and replace conventional material models based on the Radial Return Mapping algorithm. High accuracy and stability by means of a limited amount of training data is achieved by implementing physics-informed regularizations and the back stress information. The off-loading of the Neural Network is applied to the maximal extent. The proposed model architecture is simpler and more efficient compared to existing solutions from the literature, while representing a complete three-dimensional material model. The validation of the approach is carried out by means of surrogate data obtained with the Armstrong-Frederick kinematic hardening model. The Mean Squared Error is assumed as the loss function which stipulates several restrictions: deviatoric character of internal variables, compliance with the flow rule, the differentiation of elas
    
[^63]: 通过分层语义环境改善图中的超出分布泛化

    Improving out-of-distribution generalization in graphs via hierarchical semantic environments

    [https://arxiv.org/abs/2403.01773](https://arxiv.org/abs/2403.01773)

    通过生成分层语义环境，本文提出了一种新方法来增强图的不变学习，以处理分布转移。

    

    在图领域中，由于复杂的分布转移和缺乏环境背景，图的超出分布（OOD）泛化具有挑战性。最近的方法尝试通过生成平面环境来增强图的OOD泛化。然而，这种平面环境存在固有的局限性，无法捕捉更复杂的数据分布。因此，针对包含各种训练环境（如骨架、大小等）的DrugOOD数据集，平面环境无法充分解决其高异质性。因此，提出了一个新的挑战，即生成更具语义丰富的环境，以增强图的不变学习以处理分布转移。在本文中，我们提出了一种新颖的方法，为每个图生成分层语义环境。首先，给定输入图，我们明确地提取输入图中的变体子图，以在本地环境上生成代理预测。然后，随机注意...

    arXiv:2403.01773v1 Announce Type: cross  Abstract: Out-of-distribution (OOD) generalization in the graph domain is challenging due to complex distribution shifts and a lack of environmental contexts. Recent methods attempt to enhance graph OOD generalization by generating flat environments. However, such flat environments come with inherent limitations to capture more complex data distributions. Considering the DrugOOD dataset, which contains diverse training environments (e.g., scaffold, size, etc.), flat contexts cannot sufficiently address its high heterogeneity. Thus, a new challenge is posed to generate more semantically enriched environments to enhance graph invariant learning for handling distribution shifts. In this paper, we propose a novel approach to generate hierarchical semantic environments for each graph. Firstly, given an input graph, we explicitly extract variant subgraphs from the input graph to generate proxy predictions on local environments. Then, stochastic attent
    
[^64]: 具有双层优化的$\nu$支持向量机安全筛选规则

    A Safe Screening Rule with Bi-level Optimization of $\nu$ Support Vector Machine

    [https://arxiv.org/abs/2403.01769](https://arxiv.org/abs/2403.01769)

    提出了一种具有双层优化的安全筛选规则的$\nu$支持向量机方法，可以在训练前筛选出不活跃样本，降低计算成本，同时不损失预测准确性。

    

    支持向量机（SVM）在机器学习中取得了许多成功，尤其是在小样本问题上。作为传统SVM的一个著名扩展，$\nu$支持向量机（$\nu$-SVM）由于其出色的模型可解释性而表现卓越。然而，对于大规模问题，它仍面临训练开销的挑战。为了解决这个问题，我们提出了一种具有双层优化的$\nu$-SVM安全筛选规则（SRBO-$\nu$-SVM），可以在训练之前筛选出不活跃的样本，并降低计算成本，而不会牺牲预测准确性。我们的SRBO-$\nu$-SVM严格地通过整合KKT条件、凸问题的变分不等式和$\nu$属性推导而来。此外，我们开发了一种高效的对偶坐标下降方法（DCDM）来进一步提高计算速度。最后，提出了一个用于SRBO的统一框架。

    arXiv:2403.01769v1 Announce Type: cross  Abstract: Support vector machine (SVM) has achieved many successes in machine learning, especially for a small sample problem. As a famous extension of the traditional SVM, the $\nu$ support vector machine ($\nu$-SVM) has shown outstanding performance due to its great model interpretability. However, it still faces challenges in training overhead for large-scale problems. To address this issue, we propose a safe screening rule with bi-level optimization for $\nu$-SVM (SRBO-$\nu$-SVM) which can screen out inactive samples before training and reduce the computational cost without sacrificing the prediction accuracy. Our SRBO-$\nu$-SVM is strictly deduced by integrating the Karush-Kuhn-Tucker (KKT) conditions, the variational inequalities of convex problems and the $\nu$-property. Furthermore, we develop an efficient dual coordinate descent method (DCDM) to further improve computational speed. Finally, a unified framework for SRBO is proposed to ac
    
[^65]: 开放世界机器学习：回顾与新展望

    Open-world Machine Learning: A Review and New Outlooks

    [https://arxiv.org/abs/2403.01759](https://arxiv.org/abs/2403.01759)

    通过研究未知拒绝、新类别发现和类别增量学习，本文拓展了开放世界机器学习领域，提出了未来研究的多个潜在方向

    

    机器学习在许多应用中取得了显著成功。然而，现有研究主要基于封闭世界假设，即假定环境是静态的，模型一旦部署就是固定的。在许多现实应用中，这种基本且相当幼稚的假设可能不成立，因为开放环境复杂、动态且充满未知。在这种情况下，拒绝未知、发现新奇点，然后逐步学习，可以使模型像生物系统一样安全地并持续进化。本文通过研究未知拒绝、新类别发现和类别增量学习在统一范式中，提供了对开放世界机器学习的整体观点。详细讨论了当前方法的挑战、原则和局限性。最后，我们讨论了几个未来研究的潜在方向。本文旨在提供一份综述

    arXiv:2403.01759v1 Announce Type: new  Abstract: Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then incrementally learning them, could enable models to be safe and evolve continually as biological systems do. This paper provides a holistic view of open-world machine learning by investigating unknown rejection, novel class discovery, and class-incremental learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Finally, we discuss several potential directions for future research. This paper aims to provide a compr
    
[^66]: AFBT GAN: 通过反事实生成对抗网络增强对认知衰退的可解释性和诊断性能

    AFBT GAN: enhanced explainability and diagnostic performance for cognitive decline by counterfactual generative adversarial network

    [https://arxiv.org/abs/2403.01758](https://arxiv.org/abs/2403.01758)

    利用反事实推理构建的 AFBT GAN 增强了对认知衰退的可解释性和诊断性能

    

    现有的功能连接（FC）的解释结果通常是通过使用分类结果标签和诸如Pearson相关性或梯度反推等相关分析方法生成的。然而，诊断模型仍然是在黑盒模型上训练的，在训练过程中可能缺乏对重要区域FC的关注。为了增强可解释性和提高诊断性能，在诊断模型中提供关于神经退行性相关区域的先验知识，特别是当健康受试者（HC）发展为主观认知衰退（SCD）和轻度认知障碍（MCI）时，这是一个关键步骤。为了更好地确定神经退行性相关区域，我们采用反事实推理来生成源标签FC派生的目标标签FC矩阵，然后将源标签FC减去目标标签FC。自适应前向和后向转换构成了反事实推理架构。

    arXiv:2403.01758v1 Announce Type: cross  Abstract: Existing explanation results of functional connectivity (FC) are normally generated by using classification result labels and correlation analysis methods such as Pearson's correlation or gradient backward. However, the diagnostic model is still trained on the black box model and might lack the attention of FCs in important regions during the training. To enhance the explainability and improve diagnostic performance, providing prior knowledge on neurodegeneration-related regions when healthy subjects (HC) develop into subject cognitive decline (SCD) and mild cognitive impairment (MCI) for the diagnostic model is a key step. To better determine the neurodegeneration-related regions, we employ counterfactual reasoning to generate the target label FC matrices derived from source label FC and then subtract source label FC with target label FC. The counterfactual reasoning architecture is constructed by adaptive forward and backward transfo
    
[^67]: 多模态集成如何提升LLM在优化中的性能：以容量车辆路径问题为案例研究

    How Multimodal Integration Boost the Performance of LLM for Optimization: Case Study on Capacitated Vehicle Routing Problems

    [https://arxiv.org/abs/2403.01757](https://arxiv.org/abs/2403.01757)

    提出了一种使用多模态LLM进行优化的框架，能够更全面地理解优化问题，类似于人类认知过程，并且提供了更细致和有效的分析。

    

    最近，大型语言模型（LLMs）已明显地将它们定位为解决复杂优化挑战的工具。尽管被认可，现有基于LLM的优化方法的一个主要限制是，在仅依赖于数字文本提示时，尤其是在高维问题中，难以捕捉决策变量之间的关系。鉴于此，我们首先提出使用多模态LLM来增强优化性能，它能够处理文本和视觉提示，深入了解处理的优化问题。这种集成允许更全面地理解优化问题，类似于人类认知过程。我们开发了一个基于多模态LLM的优化框架，模拟人类解决问题的工作流程，从而提供更细致和有效的分析。该方法的有效性通过扩展进行评估。

    arXiv:2403.01757v1 Announce Type: new  Abstract: Recently, large language models (LLMs) have notably positioned them as capable tools for addressing complex optimization challenges. Despite this recognition, a predominant limitation of existing LLM-based optimization methods is their struggle to capture the relationships among decision variables when relying exclusively on numerical text prompts, especially in high-dimensional problems. Keeping this in mind, we first propose to enhance the optimization performance using multimodal LLM capable of processing both textual and visual prompts for deeper insights of the processed optimization problem. This integration allows for a more comprehensive understanding of optimization problems, akin to human cognitive processes. We have developed a multimodal LLM-based optimization framework that simulates human problem-solving workflows, thereby offering a more nuanced and effective analysis. The efficacy of this method is evaluated through exten
    
[^68]: 可解释的扩散用于通用时间序列生成

    Diffusion-TS: Interpretable Diffusion for General Time Series Generation

    [https://arxiv.org/abs/2403.01742](https://arxiv.org/abs/2403.01742)

    提出了一种新颖的基于扩散的框架 Diffusion-TS，结合了编码器-解码器变压器和解耦时间表示，通过直接重建样本而非噪声生成高质量的多变量时间序列样本，旨在实现时间序列的解释性和真实性。

    

    Denoising diffusion probabilistic models (DDPMs)正逐渐成为生成模型的主流范式，最近已在音频合成、时间序列填补和预测等领域取得突破。本文提出了Diffusion-TS，一种新颖的基于扩散的框架，通过使用具有解耦时间表示的编码器-解码器变压器生成高质量的多变量时间序列样本，其中分解技术指导Diffusion-TS捕获时间序列的语义含义，而变压器从嘈杂的模型输入中挖掘详细的序列信息。与现有的基于扩散的方法不同，我们训练模型直接重建样本而不是在每个扩散步骤中重建噪声，并结合了基于Fourier的损失项。预期Diffusion-TS可以生成既具有解释性又真实性的时间序列。此外，还表明了所提出的Diffusion

    arXiv:2403.01742v1 Announce Type: cross  Abstract: Denoising diffusion probabilistic models (DDPMs) are becoming the leading paradigm for generative models. It has recently shown breakthroughs in audio synthesis, time series imputation and forecasting. In this paper, we propose Diffusion-TS, a novel diffusion-based framework that generates multivariate time series samples of high quality by using an encoder-decoder transformer with disentangled temporal representations, in which the decomposition technique guides Diffusion-TS to capture the semantic meaning of time series while transformers mine detailed sequential information from the noisy model input. Different from existing diffusion-based approaches, we train the model to directly reconstruct the sample instead of the noise in each diffusion step, combining a Fourier-based loss term. Diffusion-TS is expected to generate time series satisfying both interpretablity and realness. In addition, it is shown that the proposed Diffusion-T
    
[^69]: ComS2T：一种用于数据自适应模型演化的互补时空学习系统

    ComS2T: A complementary spatiotemporal learning system for data-adaptive model evolution

    [https://arxiv.org/abs/2403.01738](https://arxiv.org/abs/2403.01738)

    ComS2T是一种互补的时空学习系统，通过将神经结构分为稳定的新皮质和动态海马体，实现模型对数据的自适应演化。

    

    arXiv:2403.01738v1 通告类型: 新的 摘要: 时空（ST）学习已经成为实现智能城市和可持续城市发展的关键技术。当前的时空学习模型通过各种空间卷积和时间演化块捕获了异质性。然而，快速的城市化导致城市数据和城市结构在短期内呈波动分布，导致现有方法普遍存在泛化和数据适应性问题。尽管已经做出努力，但现有方法无法处理新观测到的数据，具有泛化能力的方法在重复训练方面也存在局限。受到神经科学中互补学习的启发，我们引入了一种名为ComS2T的及时互补时空学习，以增强模型的演化，实现数据适应。ComS2T将神经结构分成稳定的新皮质用于巩固历史记忆以及动态海马体用于更新新知识。我们首先进行了

    arXiv:2403.01738v1 Announce Type: new  Abstract: Spatiotemporal (ST) learning has become a crucial technique to enable smart cities and sustainable urban development. Current ST learning models capture the heterogeneity via various spatial convolution and temporal evolution blocks. However, rapid urbanization leads to fluctuating distributions in urban data and city structures over short periods, resulting in existing methods suffering generalization and data adaptation issues. Despite efforts, existing methods fail to deal with newly arrived observations and those methods with generalization capacity are limited in repeated training. Motivated by complementary learning in neuroscience, we introduce a prompt-based complementary spatiotemporal learning termed ComS2T, to empower the evolution of models for data adaptation. ComS2T partitions the neural architecture into a stable neocortex for consolidating historical memory and a dynamic hippocampus for new knowledge update. We first dise
    
[^70]: 离线目标条件强化学习在具有恢复策略的安全关键任务中的应用

    Offline Goal-Conditioned Reinforcement Learning for Safety-Critical Tasks with Recovery Policy

    [https://arxiv.org/abs/2403.01734](https://arxiv.org/abs/2403.01734)

    提出了一种名为RbSL的新方法，用于解决具有不同目标的安全关键任务，克服了传统方法在复杂环境中处理多样约束时的限制。

    

    离线目标条件强化学习（GCRL）旨在通过离线数据集解决具有稀疏奖励的目标达成任务。本文研究了约束下的离线GCRL问题，并提出了一种名为基于恢复的监督学习（RbSL）的新方法，用于完成具有不同目标的安全关键任务。

    arXiv:2403.01734v1 Announce Type: cross  Abstract: Offline goal-conditioned reinforcement learning (GCRL) aims at solving goal-reaching tasks with sparse rewards from an offline dataset. While prior work has demonstrated various approaches for agents to learn near-optimal policies, these methods encounter limitations when dealing with diverse constraints in complex environments, such as safety constraints. Some of these approaches prioritize goal attainment without considering safety, while others excessively focus on safety at the expense of training efficiency. In this paper, we study the problem of constrained offline GCRL and propose a new method called Recovery-based Supervised Learning (RbSL) to accomplish safety-critical tasks with various goals. To evaluate the method performance, we build a benchmark based on the robot-fetching environment with a randomly positioned obstacle and use expert or random policies to generate an offline dataset. We compare RbSL with three offline GC
    
[^71]: 动力学系统识别的统计力学

    Statistical Mechanics of Dynamical System Identification

    [https://arxiv.org/abs/2403.01723](https://arxiv.org/abs/2403.01723)

    统计力学提供了一种稀疏方程发现算法的分析方法，通过两级贝叶斯推断问题来平衡数据拟合和简洁性，特别在低数据极限中能够量化不确定性。

    

    从观测到的噪声数据中恢复动力学方程是系统识别的核心挑战。我们发展了一种统计力学方法来分析稀疏方程发现算法，这些算法通常通过对超参数的试错选择平衡数据拟合和简洁性。在这个框架中，统计力学提供了分析复杂性和适应性之间相互作用的工具，类似于熵与能量之间的分析。为了建立这种类比，我们将优化过程定义为一个将变量选择与系数值分开的两级贝叶斯推断问题，并使得后验参数分布可以以闭式形式计算。采用统计力学概念（如自由能和配分函数）的一个关键优势在于在低数据极限中量化不确定性，这在真实世界应用中经常遇到。

    arXiv:2403.01723v1 Announce Type: cross  Abstract: Recovering dynamical equations from observed noisy data is the central challenge of system identification. We develop a statistical mechanical approach to analyze sparse equation discovery algorithms, which typically balance data fit and parsimony through a trial-and-error selection of hyperparameters. In this framework, statistical mechanics offers tools to analyze the interplay between complexity and fitness, in analogy to that done between entropy and energy. To establish this analogy, we define the optimization procedure as a two-level Bayesian inference problem that separates variable selection from coefficient values and enables the computation of the posterior parameter distribution in closed form. A key advantage of employing statistical mechanical concepts, such as free energy and the partition function, is in the quantification of uncertainty, especially in in the low-data limit; frequently encountered in real-world applicati
    
[^72]: 通过伊辛模型进行场感知因子分解机的L0正则化

    L0 Regularization of Field-Aware Factorization Machine through Ising Model

    [https://arxiv.org/abs/2403.01718](https://arxiv.org/abs/2403.01718)

    通过使用伊辛模型作为L0正则化方法，改进了场感知因子分解机（FFM）的泛化性能，并能同时确定多个群组的最佳特征组合。

    

    我们研究了使用伊辛模型作为场感知因子分解机（FFM）的L0正则化方法。这种方法提高了泛化性能，并具有同时确定每个群组的最佳特征组合的优势。我们可以通过每个群组选择的特征的相似性和差异加深对模型的解释和理解。

    arXiv:2403.01718v1 Announce Type: new  Abstract: We examined the use of the Ising model as an L0 regularization method for field-aware factorization machines (FFM). This approach improves generalization performance and has the advantage of simultaneously determining the best feature combinations for each of several groups. We can deepen the interpretation and understanding of the model from the similarities and differences in the features selected in each group.
    
[^73]: 软约束薛定谔桥：一种随机控制方法

    Soft-constrained Schrodinger Bridge: a Stochastic Control Approach

    [https://arxiv.org/abs/2403.01717](https://arxiv.org/abs/2403.01717)

    提出了软约束薛定谔桥(SSB)控制问题，在允许终端分布与预先指定分布不同的情况下，惩罚两者之间的Kullback-Leibler散度。理论上推导出了SSB解，显示最优控制过程的终端分布是μT和其他分布的几何混合，并将结果扩展到时间序列设置。

    

    薛定谔桥可以被视为一个连续时间的随机控制问题，目标是找到一个最优控制扩散过程，其具有预先指定的终端分布μT。我们提出通过允许终端分布与μT不同但惩罚两个分布之间的Kullback-Leibler散度来泛化这个随机控制问题。我们将这个新的控制问题称为软约束薛定谔桥(SSB)。这项工作的主要贡献是对SSB解的理论推导，表明最优控制过程的终端分布是μT和另一些分布的几何混合。这个结果进一步扩展到时间序列设置。SSB的一个应用是鲁棒生成扩散模型的开发。我们提出了一个基于分数匹配的算法来从几何混合中进行抽样，并展示了其用途

    arXiv:2403.01717v1 Announce Type: cross  Abstract: Schr\"{o}dinger bridge can be viewed as a continuous-time stochastic control problem where the goal is to find an optimally controlled diffusion process with a pre-specified terminal distribution $\mu_T$. We propose to generalize this stochastic control problem by allowing the terminal distribution to differ from $\mu_T$ but penalizing the Kullback-Leibler divergence between the two distributions. We call this new control problem soft-constrained Schr\"{o}dinger bridge (SSB). The main contribution of this work is a theoretical derivation of the solution to SSB, which shows that the terminal distribution of the optimally controlled process is a geometric mixture of $\mu_T$ and some other distribution. This result is further extended to a time series setting. One application of SSB is the development of robust generative diffusion models. We propose a score matching-based algorithm for sampling from geometric mixtures and showcase its us
    
[^74]: LLMs能否生成建筑设计决策？-一项探索性实证研究

    Can LLMs Generate Architectural Design Decisions? -An Exploratory Empirical study

    [https://arxiv.org/abs/2403.01709](https://arxiv.org/abs/2403.01709)

    本研究旨在探究利用大语言模型（LLMs）生成建筑设计决策的可行性，并尝试应用于架构决策记录（ADR）生成。

    

    建筑知识管理（AKM）涉及对项目或组织中与建筑决策和设计相关信息的有组织处理。AKM的一个重要产物是架构决策记录（ADR），它记录关键设计决策。 ADR是捕捉决策背景、已做出的决策以及与设计决策相关的各个方面的文件，从而促进透明度、协作和理解。 尽管它们有益处，但由于时间限制和参与度不一致等挑战，ADR在软件开发中的采用速度较慢。 大语言模型（LLMs）的最新进展可能有助于弥合这种采用差距，通过促进ADR的生成。 但是，LLM用于ADR生成或理解的效果尚未得到探究。 因此，在这项工作中，我们进行了一项旨在调查使用LLM进行的可行性的探索性研究

    arXiv:2403.01709v1 Announce Type: cross  Abstract: Architectural Knowledge Management (AKM) involves the organized handling of information related to architectural decisions and design within a project or organization. An essential artifact of AKM is the Architecture Decision Records (ADR), which documents key design decisions. ADRs are documents that capture decision context, decision made and various aspects related to a design decision, thereby promoting transparency, collaboration, and understanding. Despite their benefits, ADR adoption in software development has been slow due to challenges like time constraints and inconsistent uptake. Recent advancements in Large Language Models (LLMs) may help bridge this adoption gap by facilitating ADR generation. However, the effectiveness of LLM for ADR generation or understanding is something that has not been explored. To this end, in this work, we perform an exploratory study that aims to investigate the feasibility of using LLM for the 
    
[^75]: DyCE：用于深度学习压缩和扩展的动态可配置退出

    DyCE: Dynamic Configurable Exiting for Deep Learning Compression and Scaling

    [https://arxiv.org/abs/2403.01695](https://arxiv.org/abs/2403.01695)

    介绍了DyCE，一个动态可配置的提前退出框架，将设计考虑从彼此和基础模型解耦

    

    现代深度学习（DL）模型需要在资源受限环境中有效部署时，使用缩放和压缩技术。大多数现有技术，如修剪和量化，通常是静态的。另一方面，动态压缩方法（如提前退出）通过识别输入样本的困难程度并根据需要分配计算来降低复杂性。动态方法，尽管具有更高的灵活性和与静态方法共存的潜力，但在实现上面临重大挑战，因为动态部分的任何变化都会影响后续过程。此外，大多数当前的动态压缩设计都是单片的，与基础模型紧密集成，从而使其难以适应新颖基础模型。本文介绍了DyCE，一种动态可配置的提前退出框架，从而使设计考虑相互解耦以及与基础模型

    arXiv:2403.01695v1 Announce Type: cross  Abstract: Modern deep learning (DL) models necessitate the employment of scaling and compression techniques for effective deployment in resource-constrained environments. Most existing techniques, such as pruning and quantization are generally static. On the other hand, dynamic compression methods, such as early exits, reduce complexity by recognizing the difficulty of input samples and allocating computation as needed. Dynamic methods, despite their superior flexibility and potential for co-existing with static methods, pose significant challenges in terms of implementation due to any changes in dynamic parts will influence subsequent processes. Moreover, most current dynamic compression designs are monolithic and tightly integrated with base models, thereby complicating the adaptation to novel base models. This paper introduces DyCE, an dynamic configurable early-exit framework that decouples design considerations from each other and from the 
    
[^76]: CATS：通过构建辅助时间序列作为外生变量增强多元时间序列预测

    CATS: Enhancing Multivariate Time Series Forecasting by Constructing Auxiliary Time Series as Exogenous Variables

    [https://arxiv.org/abs/2403.01673](https://arxiv.org/abs/2403.01673)

    CATS通过构建辅助时间序列作为外生变量，有效地表示和整合多元时间序列之间的关系，提高了多元时间序列预测的效果，并且相较于之前的模型大幅减少了复杂性和参数。

    

    对于多元时间序列预测（MTSF），最近的深度学习应用显示，单变量模型经常优于多元模型。为了解决多元模型的不足，我们引入了一种方法，即构建辅助时间序列（CATS），它类似于2D时间上下文关注机制，从原始时间序列（OTS）生成辅助时间序列（ATS），以有效表示和整合系列间关系用于预测。ATS的关键原则-连续性，稀疏性和变异性-通过不同模块进行识别和实现。即使是基本的2层MLP作为核心预测器，CATS也取得了最先进的成果，相对于先前的多元模型，它显著减少了复杂性和参数，使其成为高效且可转移的MTSF解决方案。

    arXiv:2403.01673v1 Announce Type: cross  Abstract: For Multivariate Time Series Forecasting (MTSF), recent deep learning applications show that univariate models frequently outperform multivariate ones. To address the difficiency in multivariate models, we introduce a method to Construct Auxiliary Time Series (CATS) that functions like a 2D temporal-contextual attention mechanism, which generates Auxiliary Time Series (ATS) from Original Time Series (OTS) to effectively represent and incorporate inter-series relationships for forecasting. Key principles of ATS - continuity, sparsity, and variability - are identified and implemented through different modules. Even with a basic 2-layer MLP as core predictor, CATS achieves state-of-the-art, significantly reducing complexity and parameters compared to previous multivariate models, marking it an efficient and transferable MTSF solution.
    
[^77]: 排列不变函数：统计检验、度量熵中的降维和估计

    Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation

    [https://arxiv.org/abs/2403.01671](https://arxiv.org/abs/2403.01671)

    本文研究了如何在多元概率分布中测试排列不变性、估计排列不变密度以及分析排列不变函数类的度量熵，比较了它们与没有排列不变性的函数类的差异。

    

    排列不变性是机器学习中可以利用来简化复杂问题的最常见的对称性之一。近年来关于构建排列不变的机器学习架构的研究活动激增。然而，在多元概率分布中的变量如何统计测试排列不变性却鲜有研究，其中样本量允许随着维数的增长。此外，在统计理论方面，关于排列不变性如何帮助估计中降维的知识甚少。本文通过研究几个基本问题，回顾并探讨这些问题：（i）测试多元分布排列不变性的假设；（ii）估计排列不变密度；（iii）分析光滑排列不变函数类的度量熵，并将其与未强加排列不变性的对应函数类进行比较。

    arXiv:2403.01671v1 Announce Type: new  Abstract: Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the sample size. Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without impos
    
[^78]: 使用机器学习方法量化和预测住宅建筑的灵活性

    Quantifying and Predicting Residential Building Flexibility Using Machine Learning Methods

    [https://arxiv.org/abs/2403.01669](https://arxiv.org/abs/2403.01669)

    本研究提出了两种灵活性指标，并利用机器学习模型来预测住宅建筑在不同时间范围内的灵活性，其中长短期记忆（LSTM）模型表现最佳。

    

    住宅建筑占到2022年美国总用电量的35\%，随着建筑中分布式能源资源的增加，它们提供灵活性给电网的潜力也在增加。为了利用建筑所提供的灵活性，聚合商或系统操作者需要量化和预测灵活性。本文提出了两种互补的灵活性指标（即功率和能量灵活性），并探讨了几种主流的基于机器学习的模型，用于预测四小时和24小时预测时段下住宅建筑的时间变化和零星灵活性。长短期记忆（LSTM）模型表现最佳，可预测24小时内的功率灵活性，最多可提前24小时进行预测。

    arXiv:2403.01669v1 Announce Type: new  Abstract: Residential buildings account for a significant portion (35\%) of the total electricity consumption in the U.S. as of 2022. As more distributed energy resources are installed in buildings, their potential to provide flexibility to the grid increases. To tap into that flexibility provided by buildings, aggregators or system operators need to quantify and forecast flexibility. Previous works in this area primarily focused on commercial buildings, with little work on residential buildings. To address the gap, this paper first proposes two complementary flexibility metrics (i.e., power and energy flexibility) and then investigates several mainstream machine learning-based models for predicting the time-variant and sporadic flexibility of residential buildings at four-hour and 24-hour forecast horizons. The long-short-term-memory (LSTM) model achieves the best performance and can predict power flexibility for up to 24 hours ahead with the ave
    
[^79]: 通过扩散过程改进对抗能量模型

    Improving Adversarial Energy-Based Model via Diffusion Process

    [https://arxiv.org/abs/2403.01666](https://arxiv.org/abs/2403.01666)

    通过将EBMs嵌入到扩散步骤中并引入变分后验分布，有效改进了对抗性能量模型的生成能力。

    

    生成模型展示了强大的生成能力，而高效的似然度估计却鲜为人知。基于能量的模型（EBMs）定义了一个灵活的能量函数，以有效地参数化未标准化的密度，但训练难度很大。对抗性的EBMs引入了一个生成器形成一个极小极大训练游戏，以避免传统EBMs中使用昂贵的MCMC采样，但对抗性的EBMs与其他强大的生成模型之间仍然存在明显差距。受扩散模型的启发，我们将EBMs嵌入到每个去噪步骤中，将一个长生成过程分成几个较小的步骤。此外，我们使用对称的Jeffrey散度并引入一个变分后验分布来训练生成器，以解决对抗性EBMs中存在的主要挑战。我们的实验结果显示，与现有的对抗性EBMs相比，在生成方面取得了显著的改进，同时提供了一个u

    arXiv:2403.01666v1 Announce Type: new  Abstract: Generative models have shown strong generation ability while efficient likelihood estimation is less explored. Energy-based models~(EBMs) define a flexible energy function to parameterize unnormalized densities efficiently but are notorious for being difficult to train. Adversarial EBMs introduce a generator to form a minimax training game to avoid expensive MCMC sampling used in traditional EBMs, but a noticeable gap between adversarial EBMs and other strong generative models still exists. Inspired by diffusion-based models, we embedded EBMs into each denoising step to split a long-generated process into several smaller steps. Besides, we employ a symmetric Jeffrey divergence and introduce a variational posterior distribution for the generator's training to address the main challenges that exist in adversarial EBMs. Our experiments show significant improvement in generation compared to existing adversarial EBMs, while also providing a u
    
[^80]: 监督学习问题的几何和稳定性

    Geometry and Stability of Supervised Learning Problems

    [https://arxiv.org/abs/2403.01660](https://arxiv.org/abs/2403.01660)

    引入了监督学习问题之间的风险距离概念，通过风险距离可以量化问题的稳定性变化，并探索了监督学习问题空间的几何结构。

    

    我们引入了一种监督学习问题之间的距离概念，我们称之为风险距离。这种以最优传输为灵感的距离促进了稳定性结果；我们可以通过限制这些修改可以将问题移动多少来量化诸如采样偏差、噪声、有限数据和逼近等问题在风险距离下如何改变给定问题。在建立了距离之后，我们探索了产生的监督学习问题空间的几何结构，提供了明确的测地线并证明分类问题集在更大类的问题中是密集的。我们还提供了风险距离的两个变体：一个在问题的预测变量上结合了指定的权重，另一个对问题的风险景观轮廓更为敏感。

    arXiv:2403.01660v1 Announce Type: new  Abstract: We introduce a notion of distance between supervised learning problems, which we call the Risk distance. This optimal-transport-inspired distance facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the geometry of the resulting space of supervised learning problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem's predictors, and one that is more sensitive to the contours of a problem's risk landscape.
    
[^81]: 基于历史发电量和天气数据的分层时间卷积神经网络的日前区域太阳能发电预测

    Day-ahead regional solar power forecasting with hierarchical temporal convolutional neural networks using historical power generation and weather data

    [https://arxiv.org/abs/2403.01653](https://arxiv.org/abs/2403.01653)

    提出了两种基于深度学习的区域太阳能发电预测方法，可以同时考虑历史发电量和天气数据，克服了以往在区域预测中忽视地理位置信息或生成大量预测模型的限制。

    

    区域太阳能发电预测涉及预测一个地区所有屋顶光伏系统的总发电量，对于能源行业的各方利益相关者具有重要意义。然而，需要在预测过程中考虑来自地理上分散位置的大量太阳能发电和天气时间序列，这使得准确的区域预测具有挑战性。因此，先前的工作要么将重点限制在预测单个时间序列（即聚合时间序列），即一个地区所有太阳能发电时间序列的总和，忽略了特定位置的天气效应，要么独立使用特定位置的天气数据来预测每个光伏站点的太阳能发电时间序列（即独立时间序列），导致数量庞大的预测模型。在这项工作中，我们提出了两种基于深度学习的区域预测方法。

    arXiv:2403.01653v1 Announce Type: new  Abstract: Regional solar power forecasting, which involves predicting the total power generation from all rooftop photovoltaic systems in a region holds significant importance for various stakeholders in the energy sector. However, the vast amount of solar power generation and weather time series from geographically dispersed locations that need to be considered in the forecasting process makes accurate regional forecasting challenging. Therefore, previous work has limited the focus to either forecasting a single time series (i.e., aggregated time series) which is the addition of all solar generation time series in a region, disregarding the location-specific weather effects or forecasting solar generation time series of each PV site (i.e., individual time series) independently using location-specific weather data, resulting in a large number of forecasting models. In this work, we propose two deep-learning-based regional forecasting methods that 
    
[^82]: 您需要更好地关注付费

    You Need to Pay Better Attention

    [https://arxiv.org/abs/2403.01643](https://arxiv.org/abs/2403.01643)

    提出了三种新的注意力机制，在效率和学习能力方面优于标准的多头注意力，提高了Transformer模型的性能和更广泛的部署能力。

    

    我们引入了三种新的注意力机制，这些机制在效率和学习能力方面胜过标准的多头注意力，从而提高了Transformer模型的性能和更广泛的部署能力。我们的第一个贡献是优化注意力，其性能与标准注意力相似，但参数数量少了四分之三，每个头部少了一个矩阵乘法。接下来，我们引入了高效注意力，其性能与标准注意力相当，但参数数量减少了一半，每个头部减少了两个矩阵乘法，并且比标准注意力快两倍。最后，我们介绍了超级注意力，在视觉和自然语言处理任务中明显超越了标准注意力，同时具有更少的参数和矩阵乘法。除了提供严格的数学比较，我们在MN中评估了所提出的注意力机制

    arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
    
[^83]: 通过快速集成学习实现的蓝绿模式高能效化化学传感器阵列

    Blue and Green-Mode Energy-Efficient Chemiresistive Sensor Array Realized by Rapid Ensemble Learning

    [https://arxiv.org/abs/2403.01642](https://arxiv.org/abs/2403.01642)

    该研究提出了一种通过快速集成学习优化化学传感器阵列的策略，引入了蓝色和绿色两种工作模式，通过选择性激活关键传感器显著降低能耗而不影响检测准确性。

    

    arXiv:2403.01642v1 公告类型: 新的 摘要: 物联网的快速发展需要开发既高效又能胜任的优化化学传感器阵列。本研究引入一种新颖的优化策略，采用快速集成学习模型委员会方法来实现这些目标。利用弹性网回归、随机森林、XGBoost等机器学习模型，该策略识别出在化学传感器阵列中对准确分类最具影响力的传感器：引入加权投票机制来聚合传感器选择中的模型意见，从而建立了两种不同的工作模式，称为“蓝色”和“绿色”。蓝色模式利用所有传感器进行最大检测能力，而绿色模式仅选择性激活关键传感器，从而显著降低能耗而不影响检测准确性。该策略通过理论验证。

    arXiv:2403.01642v1 Announce Type: new  Abstract: The rapid advancement of Internet of Things (IoT) necessitates the development of optimized Chemiresistive Sensor (CRS) arrays that are both energy-efficient and capable. This study introduces a novel optimization strategy that employs a rapid ensemble learning-based model committee approach to achieve these goals. Utilizing machine learning models such as Elastic Net Regression, Random Forests, and XGBoost, among others, the strategy identifies the most impactful sensors in a CRS array for accurate classification: A weighted voting mechanism is introduced to aggregate the models' opinions in sensor selection, thereby setting up wo distinct working modes, termed "Blue" and "Green". The Blue mode operates with all sensors for maximum detection capability, while the Green mode selectively activates only key sensors, significantly reducing energy consumption without compromising detection accuracy. The strategy is validated through theoreti
    
[^84]: Gaussian Mixture Models的扩散指导的理论洞见

    Theoretical Insights for Diffusion Guidance: A Case Study for Gaussian Mixture Models

    [https://arxiv.org/abs/2403.01639](https://arxiv.org/abs/2403.01639)

    本文首次在高斯混合模型背景下进行理论研究，证明了将扩散指导纳入不仅提升了分类置信度，而且减少了分布多样性。

    

    扩散模型受益于将特定任务信息注入评分函数以引导样本生成朝向所需属性。这种信息被称为指导。例如，在文本到图像合成中，文本输入被编码为指导以生成语义对齐的图像。适当的指导输入与扩散模型的性能密切相关。一个常见的观察是，强有力的指导促进了与任务特定信息的紧密对齐，同时减少了生成样本的多样性。本文首次在高斯混合模型背景下提供了对理解指导对扩散模型影响的理论研究。在温和的条件下，我们证明了将扩散指导纳入不仅提升了分类置信度，而且减少了分布多样性，导致输出分布的差异熵减少。

    arXiv:2403.01639v1 Announce Type: new  Abstract: Diffusion models benefit from instillation of task-specific information into the score function to steer the sample generation towards desired properties. Such information is coined as guidance. For example, in text-to-image synthesis, text input is encoded as guidance to generate semantically aligned images. Proper guidance inputs are closely tied to the performance of diffusion models. A common observation is that strong guidance promotes a tight alignment to the task-specific information, while reducing the diversity of the generated samples. In this paper, we provide the first theoretical study towards understanding the influence of guidance on diffusion models in the context of Gaussian mixture models. Under mild conditions, we prove that incorporating diffusion guidance not only boosts classification confidence but also diminishes distribution diversity, leading to a reduction in the differential entropy of the output distribution.
    
[^85]: 通过多任务强化学习实现高效的短视探索

    Sample Efficient Myopic Exploration Through Multitask Reinforcement Learning with Diverse Tasks

    [https://arxiv.org/abs/2403.01636](https://arxiv.org/abs/2403.01636)

    通过研究发现，当代理在多样化任务上进行训练时，具有短视探索设计的通用策略共享算法可以在多任务强化学习中显著提高样本效率。

    

    多任务强化学习（MTRL）方法在许多重要的强化学习（RL）任务中应用广泛，但近期MTRL理论的进展主要集中在通过假设任务间共享结构来提高统计效率，对于RL中至关重要的探索这一关键方面却大多被忽视。本文通过展示，当代理在足够多样化的任务集上训练时，具有短视探索设计（如$\epsilon$-贪心）的通用策略共享算法可以在MTRL中具有高样本效率，从我们所知，这是对“探索收益”在MTRL中的首次理论证明，也有助于解释短视探索在实践中应用广泛的成功。为了验证多样性的作用，我们在合成机器人控制任务上进行了实验证明。

    arXiv:2403.01636v1 Announce Type: cross  Abstract: Multitask Reinforcement Learning (MTRL) approaches have gained increasing attention for its wide applications in many important Reinforcement Learning (RL) tasks. However, while recent advancements in MTRL theory have focused on the improved statistical efficiency by assuming a shared structure across tasks, exploration--a crucial aspect of RL--has been largely overlooked. This paper addresses this gap by showing that when an agent is trained on a sufficiently diverse set of tasks, a generic policy-sharing algorithm with myopic exploration design like $\epsilon$-greedy that are inefficient in general can be sample-efficient for MTRL. To the best of our knowledge, this is the first theoretical demonstration of the "exploration benefits" of MTRL. It may also shed light on the enigmatic success of the wide applications of myopic exploration in practice. To validate the role of diversity, we conduct experiments on synthetic robotic control
    
[^86]: 神经常微分方程在托卡马克等离子体动力学分析中的应用

    Application of Neural Ordinary Differential Equations for Tokamak Plasma Dynamics Analysis

    [https://arxiv.org/abs/2403.01635](https://arxiv.org/abs/2403.01635)

    本研究引入了神经常微分方程（Neural ODEs）来模拟托卡马克等离子体内部的能量转移过程，通过从实验数据中推导扩散性参数，实现了精确的能量相互作用模拟，为提高托卡马克性能提供了新思路。

    

    在追求受控热核聚变的过程中，托卡马克在理解燃烧等离子体动力学方面面临着复杂挑战。本研究引入了一个多区域多时间尺度传输模型，利用神经常微分方程（Neural ODEs）来模拟托卡马克内部复杂的能量转移过程。我们的方法利用神经常微分方程从DIII-D托卡马克的实验数据中数值推导扩散性参数，实现对电子和离子在核心、边缘和刮削层等各个区域之间能量相互作用的精确建模。这些区域被概念化为不同的节点，捕捉辐射和传输过程的关键时间尺度，从而对托卡马克高效运行至关重要。通过在各种辅助加热条件下验证模型与DIII-D等离子体的匹配性，最终揭示了增强托卡马克性能的方法。

    arXiv:2403.01635v1 Announce Type: cross  Abstract: In the quest for controlled thermonuclear fusion, tokamaks present complex challenges in understanding burning plasma dynamics. This study introduces a multi-region multi-timescale transport model, employing Neural Ordinary Differential Equations (Neural ODEs) to simulate the intricate energy transfer processes within tokamaks. Our methodology leverages Neural ODEs for the numerical derivation of diffusivity parameters from DIII-D tokamak experimental data, enabling the precise modeling of energy interactions between electrons and ions across various regions, including the core, edge, and scrape-off layer. These regions are conceptualized as distinct nodes, capturing the critical timescales of radiation and transport processes essential for efficient tokamak operation. Validation against DIII-D plasmas under various auxiliary heating conditions demonstrates the model's effectiveness, ultimately shedding light on ways to enhance tokamak
    
[^87]: 批判性窗口：扩散模型中特征出现的非渐进理论

    Critical windows: non-asymptotic theory for feature emergence in diffusion models

    [https://arxiv.org/abs/2403.01633](https://arxiv.org/abs/2403.01633)

    我们开发了一个理论框架来研究扩散模型中的“批判性窗口”，并展示了针对强对数凹密度混合数据，这些窗口是可以明确地受到一定的分离度量约束的。

    

    我们发展理论来理解图像生成扩散模型中一个有趣的属性，我们称之为批判性窗口。实证上观察到在采样过程中存在狭窄的时间间隔，在此期间会出现最终图像的特定特征，例如图像类别或背景颜色。而这种特性对于解释性是有利的，因为意味着可以将生成的特性定位到轨迹的一个小片段，但这似乎与扩散的连续性质相矛盾。我们提出了一个形式化框架来研究这些窗口，并表明对于来自混合强对数凹密度分布的数据，这些窗口可以用一定的跨组和组内分离度量来显式地约束。我们还为诸如良条件G的具体示例实例化了这些界限。

    arXiv:2403.01633v1 Announce Type: new  Abstract: We develop theory to understand an intriguing property of diffusion models for image generation that we term critical windows. Empirically, it has been observed that there are narrow time intervals in sampling during which particular features of the final image emerge, e.g. the image class or background color (Ho et al., 2020b; Georgiev et al., 2023; Raya & Ambrogioni, 2023; Sclocchi et al., 2024; Biroli et al., 2024). While this is advantageous for interpretability as it implies one can localize properties of the generation to a small segment of the trajectory, it seems at odds with the continuous nature of the diffusion. We propose a formal framework for studying these windows and show that for data coming from a mixture of strongly log-concave densities, these windows can be provably bounded in terms of certain measures of inter- and intra-group separation. We also instantiate these bounds for concrete examples like well-conditioned G
    
[^88]: 通过语法增强改进LLM代码生成

    Improving LLM Code Generation with Grammar Augmentation

    [https://arxiv.org/abs/2403.01632](https://arxiv.org/abs/2403.01632)

    SynCode是一个新框架，结合编程语言的语法和DFA mask store，在LLMs中生成代码过程中获得96.07%的句法错误降低，并展现出提高句法精度的重大影响。

    

    我们提出了 SynCode，一个用于高效和通用地解码大型语言模型（LLMs）代码的新框架。SynCode利用编程语言的语法，利用离线构建的基于语言语法终结符的高效查找表DFA mask store。我们展示了SynCode在给定编程语言的上下文无关文法（CFG）的完备性和正确性，展示其在保留语义上有效令牌的同时拒绝无效令牌的能力。该框架与由CFG定义的任何语言无缝集成，验证了针对Python和Go的CFG实验。结果突出了当SynCode与最先进的LLMs结合时，语法错误减少96.07%，彰显了其对提高代码生成中的句法精度的重大影响。

    arXiv:2403.01632v1 Announce Type: new  Abstract: We present SynCode a novel framework for efficient and general syntactical decoding of code with large language models (LLMs). SynCode leverages the grammar of a programming language, utilizing an offline-constructed efficient lookup table called DFA mask store based on language grammar terminals. We demonstrate SynCode's soundness and completeness given the context-free grammar (CFG) of the programming language, presenting its ability to retain syntactically valid tokens while rejecting invalid ones. The framework seamlessly integrates with any language defined by CFG, as evidenced by experiments on CFGs for Python and Go. The results underscore the significant reduction of 96.07% of syntax errors achieved when SynCode is combined with state-of-the-art LLMs, showcasing its substantial impact on enhancing syntactical precision in code generation.   Our code is available at https://github.com/uiuc-focal-lab/syncode.
    
[^89]: 机器学习在健康领域的最新进展、应用和开放挑战：来自ML4H 2023研讨会的反思

    Recent Advances, Applications, and Open Challenges in Machine Learning for Health: Reflections from Research Roundtables at ML4H 2023 Symposium

    [https://arxiv.org/abs/2403.01628](https://arxiv.org/abs/2403.01628)

    ML4H 2023研讨会通过研究圆桌讨论会促进了与会者和资深研究人员就健康领域的时事话题展开讨论，并总结了其中涉及的最新进展、应用和开放挑战。

    

    第三届ML4H研讨会于2023年12月10日在美国路易斯安那州新奥尔良举行。研讨会包括研究圆桌讨论会，旨在促进与ML4H社区相关主题的参与者和资深研究人员之间的讨论。会议组织了11个现场圆桌和4个虚拟圆桌，涉及了17位资深主席和19位初级主席。每个圆桌讨论会都包括受邀的资深主席（在该领域拥有丰富经验）、初级主席（负责促进讨论）以及对该主题感兴趣的来自不同背景的与会者。在这里，我们详细介绍了会议的组织过程，并整理了这些研讨会讨论的收获，包括最新进展、应用和开放挑战。

    arXiv:2403.01628v1 Announce Type: new  Abstract: The third ML4H symposium was held in person on December 10, 2023, in New Orleans, Louisiana, USA. The symposium included research roundtable sessions to foster discussions between participants and senior researchers on timely and relevant topics for the \ac{ML4H} community. Encouraged by the successful virtual roundtables in the previous year, we organized eleven in-person roundtables and four virtual roundtables at ML4H 2022. The organization of the research roundtables at the conference involved 17 Senior Chairs and 19 Junior Chairs across 11 tables. Each roundtable session included invited senior chairs (with substantial experience in the field), junior chairs (responsible for facilitating the discussion), and attendees from diverse backgrounds with interest in the session's topic. Herein we detail the organization process and compile takeaways from these roundtable discussions, including recent advances, applications, and open challe
    
[^90]: ML4PhySim：物理模拟挑战中的机器学习（翼型设计）

    ML4PhySim : Machine Learning for Physical Simulations Challenge (The airfoil design)

    [https://arxiv.org/abs/2403.01623](https://arxiv.org/abs/2403.01623)

    本竞赛旨在推动发展新的ML技术，使用一致的评估框架解决物理问题，首个竞赛解决了利用ML-based代理方法改善计算效率与性能权衡的挑战。

    

    机器学习（ML）技术被认为是解决复杂物理问题的一种有前途的方法。然而，评估这些学习得到的物理模型仍然是工业应用中的一个重要问题。本次比赛的目的是鼓励开发新的ML技术，利用最近提出的一致性评估框架Learning Industrial Physical Simulations (LIPS) 来解决物理问题。我们提出利用一个名为AirfRANS的数据集学习代表着一个众所周知的物理用例：翼型设计模拟。对于每一个提交的解决方案计算的全局评分基于三个主要类别的标准，涵盖不同方面，即：与ML相关的标准，分布外标准和物理符合性标准。据我们所知，这是第一个解决利用ML-based代理方法改善计算效率与性能权衡的竞赛。

    arXiv:2403.01623v1 Announce Type: new  Abstract: The use of machine learning (ML) techniques to solve complex physical problems has been considered recently as a promising approach. However, the evaluation of such learned physical models remains an important issue for industrial use. The aim of this competition is to encourage the development of new ML techniques to solve physical problems using a unified evaluation framework proposed recently, called Learning Industrial Physical Simulations (LIPS). We propose learning a task representing a well-known physical use case: the airfoil design simulation, using a dataset called AirfRANS. The global score calculated for each submitted solution is based on three main categories of criteria covering different aspects, namely: ML-related, Out-Of-Distribution, and physical compliance criteria. To the best of our knowledge, this is the first competition addressing the use of ML-based surrogate approaches to improve the trade-off computational cos
    
[^91]: 机器学习与深度学习：泛化问题对比研究

    Machine Learning vs Deep Learning: The Generalization Problem

    [https://arxiv.org/abs/2403.01621](https://arxiv.org/abs/2403.01621)

    深度学习模型具备泛化到训练范围之外的固有能力，这对于现实世界中至关重要。

    

    能够推广到超出训练数据范围的能力是一个重要挑战，通常与模型的效用和鲁棒性密切相关。本研究调查了传统机器学习（ML）模型和深度学习（DL）算法在外推方面的比较能力——这是泛化的更具挑战性的方面，因为它要求模型对不在其训练域之外的数据点进行推断。

    arXiv:2403.01621v1 Announce Type: cross  Abstract: The capacity to generalize beyond the range of training data is a pivotal challenge, often synonymous with a model's utility and robustness. This study investigates the comparative abilities of traditional machine learning (ML) models and deep learning (DL) algorithms in terms of extrapolation -- a more challenging aspect of generalization because it requires the model to make inferences about data points that lie outside the domain it has been trained on. We present an empirical analysis where both ML and DL models are trained on an exponentially growing function and then tested on values outside the training domain. The choice of this function allows us to distinctly showcase the divergence in performance when models are required to predict beyond the scope of their training data. Our findings suggest that deep learning models possess inherent capabilities to generalize beyond the training scope, an essential feature for real-world a
    
[^92]: 部分联邦学习

    Partial Federated Learning

    [https://arxiv.org/abs/2403.01615](https://arxiv.org/abs/2403.01615)

    提出了一种新算法部分联邦学习(PartialFL)，允许将数据模态或其中间表示的子集提供给服务器训练模型，解决了部分数据限制的联邦学习问题。

    

    联邦学习(FL)是一种流行的算法，用于在用户数据受限于边缘设备(例如，移动电话)上训练机器学习模型，这是由于隐私问题。通常，FL在训练时假设用户数据的任何部分都不能从边缘流出。然而，在许多生产环境中，特定的数据模态/元数据被限制在设备上，而其他数据不受限制。例如，在商业SLU系统中，通常希望防止将生物识别信号(例如输入提示的音频记录)传输到云端，但可能将本地(即在边缘设备上)转录的文本传输到云端。在这项工作中，我们提出了一种名为部分联邦学习(PartialFL)的新算法，该算法使用数据训练机器学习模型，其中数据模态的子集或其中间表示可以向服务器提供。

    arXiv:2403.01615v1 Announce Type: new  Abstract: Federated Learning (FL) is a popular algorithm to train machine learning models on user data constrained to edge devices (for example, mobile phones) due to privacy concerns. Typically, FL is trained with the assumption that no part of the user data can be egressed from the edge. However, in many production settings, specific data-modalities/meta-data are limited to be on device while others are not. For example, in commercial SLU systems, it is typically desired to prevent transmission of biometric signals (such as audio recordings of the input prompt) to the cloud, but egress of locally (i.e. on the edge device) transcribed text to the cloud may be possible. In this work, we propose a new algorithm called Partial Federated Learning (PartialFL), where a machine learning model is trained using data where a subset of data modalities or their intermediate representations can be made available to the server. We further restrict our model tr
    
[^93]: 针对外部导向放疗中安全增强的呼吸运动预测与在线学习的循环神经网络

    Respiratory motion forecasting with online learning of recurrent neural networks for safety enhancement in externally guided radiotherapy

    [https://arxiv.org/abs/2403.01607](https://arxiv.org/abs/2403.01607)

    该研究评估了资源高效的在线RNN算法在放疗治疗过程中准确预测呼吸运动的能力。

    

    在肺部放疗中，红外摄像头可以记录胸部反射物体的位置，以推断由于呼吸而移动的肿瘤位置，但治疗系统的延迟影响了放射束精度。实时循环学习（RTRL）是一个潜在的解决方案，因为它可以学习非平稳呼吸数据中的模式，但具有较高的复杂性。本研究评估了资源高效的在线RNN算法，即无偏在线循环优化（UORO）、稀疏-1步逼近（SnAp-1）和解耦神经接口（DNI），以准确预测放疗治疗过程中的呼吸运动。我们使用了健康受试者胸部外部标记物的包含3D位置的时间序列。我们提出了基于影响和即时雅可比矩阵的压缩以及线性系数的准确更新用于信用分配的SnAp-1和DNI的高效实现。

    arXiv:2403.01607v1 Announce Type: new  Abstract: In lung radiotherapy, infrared cameras can record the location of reflective objects on the chest to infer the position of the tumor moving due to breathing, but treatment system latencies hinder radiation beam precision. Real-time recurrent learning (RTRL), is a potential solution as it can learn patterns within non-stationary respiratory data but has high complexity. This study assesses the capabilities of resource-efficient online RNN algorithms, namely unbiased online recurrent optimization (UORO), sparse-1 step approximation (SnAp-1), and decoupled neural interfaces (DNI) to forecast respiratory motion during radiotherapy treatment accurately. We use time series containing the 3D position of external markers on the chest of healthy subjects. We propose efficient implementations for SnAp-1 and DNI based on compression of the influence and immediate Jacobian matrices and an accurate update of the linear coefficients used in credit ass
    
[^94]: 朝向可证明的对数密度策略梯度

    Towards Provable Log Density Policy Gradient

    [https://arxiv.org/abs/2403.01605](https://arxiv.org/abs/2403.01605)

    提出对数密度梯度方法来估计策略梯度，修正残差误差，有望改善强化学习方法的样本复杂度。

    

    策略梯度方法是现代强化学习成功的关键要素。现代策略梯度方法虽然成功，但在梯度估计中引入了一个残差误差。本文认为这个残差项很重要，纠正它有可能改善强化学习方法的样本复杂度。为此，我们提出了对数密度梯度来估计策略梯度，可以纠正这个残差误差项。对数密度梯度方法通过利用状态-动作折扣分布形式来计算策略梯度。我们首先给出了准确找到标签马尔可夫决策过程（MDPs）的对数密度梯度所需的方程式。对于更复杂的环境，我们提出了一种利用后向即时（TD）方法来近似计算对数密度梯度的方法，通过利用后向的同策略样本。由于从马尔可夫链中进行后向采样是高度

    arXiv:2403.01605v1 Announce Type: cross  Abstract: Policy gradient methods are a vital ingredient behind the success of modern reinforcement learning. Modern policy gradient methods, although successful, introduce a residual error in gradient estimation. In this work, we argue that this residual term is significant and correcting for it could potentially improve sample-complexity of reinforcement learning methods. To that end, we propose log density gradient to estimate the policy gradient, which corrects for this residual error term. Log density gradient method computes policy gradient by utilising the state-action discounted distributional formulation. We first present the equations needed to exactly find the log density gradient for a tabular Markov Decision Processes (MDPs). For more complex environments, we propose a temporal difference (TD) method that approximates log density gradient by utilizing backward on-policy samples. Since backward sampling from a Markov chain is highly 
    
[^95]: SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos

    SCHEMA: State CHangEs MAtter for Procedure Planning in Instructional Videos

    [https://arxiv.org/abs/2403.01599](https://arxiv.org/abs/2403.01599)

    通过研究步骤和状态之间的因果关系，本文提出了SCHEMA方法，将每个步骤显式表示为状态变化，并追踪教学视频中的状态变化。

    

    我们研究了在教学视频中的程序规划问题，旨在给出根据部分视觉状态观察生成目标导向的动作步骤序列。这个问题的动机是为了学习一个结构化且可规划的状态和动作空间。我们指出，状态变化对于教学视频中的程序规划很重要，旨在通过研究步骤和状态之间的因果关系建立更为结构化的状态空间。具体地，我们将每个步骤显式地表示为状态变化，并跟踪程序中的状态变化。

    arXiv:2403.01599v1 Announce Type: cross  Abstract: We study the problem of procedure planning in instructional videos, which aims to make a goal-oriented sequence of action steps given partial visual state observations. The motivation of this problem is to learn a structured and plannable state and action space. Recent works succeeded in sequence modeling of steps with only sequence-level annotations accessible during training, which overlooked the roles of states in the procedures. In this work, we point out that State CHangEs MAtter (SCHEMA) for procedure planning in instructional videos. We aim to establish a more structured state space by investigating the causal relations between steps and states in procedures. Specifically, we explicitly represent each step as state changes and track the state changes in procedures. For step representation, we leveraged the commonsense knowledge in large language models (LLMs) to describe the state changes of steps via our designed chain-of-thoug
    
[^96]: Mamba模型的隐藏关注

    The Hidden Attention of Mamba Models

    [https://arxiv.org/abs/2403.01590](https://arxiv.org/abs/2403.01590)

    Mamba模型可以被视为关注驱动的模型，这与变压器中的自注意力层有所不同，并且通过可解释性方法可以深入了解其内部工作。

    

    Mamba层提供了一种高效的选择性状态空间模型(SSM)，在建模多个领域包括NLP、长距离序列处理和计算机视觉方面非常有效。选择性SSMs被视为双重模型，其中一个通过IO-aware并行扫描在整个序列上进行并行训练，并以自回归方式部署。我们添加了第三个视角，并展示这样的模型可以被视为关注驱动的模型。这一新视角使我们能够将底层机制与变压器中的自注意力层进行比较，并让我们通过可解释性方法窥探Mamba模型的内部工作。我们的代码可公开获取。

    arXiv:2403.01590v1 Announce Type: new  Abstract: The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains including NLP, long-range sequences processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.
    
[^97]: 在模型不可知的多源无监督领域自适应上

    On the Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation

    [https://arxiv.org/abs/2403.01582](https://arxiv.org/abs/2403.01582)

    该论文提出了一种新的模型不可知的多源无监督领域自适应（MMDA）设置，允许多样化的源模型，解决了源模型选择问题。

    

    多源无监督领域自适应（MSFDA）旨在通过使用源模型而非源数据，从多个良好标记的源域传递知识到一个未标记的目标域。现有的MSFDA方法局限于每个源域仅提供单一模型，并且具有统一结构。本文介绍了一种新的MSFDA设置：模型不可知的多源无监督领域自适应（MMDA），允许具有不同架构的多样化源模型，而不受定量限制。虽然MMDA具有良好的潜力，但合并大量源模型会增加包含不想要的模型的风险，从而突显出源模型选择问题。为了解决这个问题，我们首先对该问题进行了理论分析。我们揭示了两个基本选择原则：可转移性原则和多样性原则，并介绍了一个整合它们的选择算法。

    arXiv:2403.01582v1 Announce Type: new  Abstract: Multi-Source-Free Unsupervised Domain Adaptation (MSFDA) aims to transfer knowledge from multiple well-labeled source domains to an unlabeled target domain, using source models instead of source data. Existing MSFDA methods limited that each source domain provides only a single model, with a uniform structure. This paper introduces a new MSFDA setting: Model-Agnostic Multi-Source-Free Unsupervised Domain Adaptation (MMDA), allowing diverse source models with varying architectures, without quantitative restrictions. While MMDA holds promising potential, incorporating numerous source models poses a high risk of including undesired models, which highlights the source model selection problem. To address it, we first provide a theoretical analysis of this problem. We reveal two fundamental selection principles: transferability principle and diversity principle, and introduce a selection algorithm to integrate them. Then, considering the measu
    
[^98]: 将Kullback-Leibler散度与Cohen's Kappa相关联，限制分类性能

    Limits to classification performance by relating Kullback-Leibler divergence to Cohen's Kappa

    [https://arxiv.org/abs/2403.01571](https://arxiv.org/abs/2403.01571)

    通过将Kullback-Leibler散度与Cohen's Kappa相关联，限制了分类性能的最大限度

    

    机器学习分类算法的性能是通过估计指标来评估的，通常是从混淆矩阵中使用训练数据和交叉验证得出的。然而，这些并不证明已经实现了最佳性能。可以使用信息距离度量来估计错误率的基本限制。为此，混淆矩阵已被制定为符合Chernoff-Stein引理。这将错误率与描述两个类别的概率密度函数之间的Kullback-Leibler散度相关联。这导致了一个关键结果，将Cohen's Kappa与电阻器平均距离联系起来，这是两个Kullback-Leibler散度的并联电阻器组合。电阻器平均距离具有比特单位，可以从同一训练数据中使用分类算法估计的KullBack-Leibler散度的kNN估计中得出。

    arXiv:2403.01571v1 Announce Type: cross  Abstract: The performance of machine learning classification algorithms are evaluated by estimating metrics, often from the confusion matrix, using training data and cross-validation. However, these do not prove that the best possible performance has been achieved. Fundamental limits to error rates can be estimated using information distance measures. To this end, the confusion matrix has been formulated to comply with the Chernoff-Stein Lemma. This links the error rates to the Kullback-Leibler divergences between the probability density functions describing the two classes. This leads to a key result that relates Cohen's Kappa to the Resistor Average Distance which is the parallel resistor combination of the two Kullback-Leibler divergences. The Resistor Average Distance has units of bits and is estimated from the same training data used by the classification algorithm, using kNN estimates of the KullBack-Leibler divergences. The classification
    
[^99]: SERVAL：垂直模型和LLM之间的协同学习，实现零-shot级别的医学预测

    SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction

    [https://arxiv.org/abs/2403.01570](https://arxiv.org/abs/2403.01570)

    提出SERVAL，一个协同学习流水线，可以通过相互增强，实现LLMs和小模型的垂直能力无监督开发，从而改善领域特定垂直问题的零-shot预测能力。

    

    近期大型语言模型（LLMs）的发展展示出对通用和常识问题卓越的零-shot能力。然而，LLMs在领域特定垂直问题上的应用仍然落后，主要是由于垂直知识方面的问题和不足。此外，垂直数据注释过程通常需要劳动密集型的专家参与，因此增加了增强模型垂直能力的额外挑战。在本文中，我们提出了SERVAL，一个协同学习流水线，旨在通过相互增强，对LLMs和小模型的垂直能力进行无监督开发。具体来说，SERVAL利用LLMs的零-shot输出作为注释，利用其置信度来从头开始教授一个强大的垂直模型。反过来，训练有素的垂直模型引导LLM微调，以增强其零-shot能力，逐步改进两者。

    arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
    
[^100]: 用于监督在线持续学习的变压器

    Transformers for Supervised Online Continual Learning

    [https://arxiv.org/abs/2403.01554](https://arxiv.org/abs/2403.01554)

    本文提出了一种方法，利用变压器的上下文学习能力以及它们与元学习的关联，用于监督在线持续学习。

    

    变压器已成为序列建模任务（如自然语言处理或音频处理）的主导架构，甚至被考虑用于非自然顺序任务，如图像分类。它们能够关注和处理一组标记作为上下文，使其能够发展出上下文少样本学习的能力。然而，它们在在线持续学习中的潜力仍然相对未被探究。在线持续学习中，模型必须适应非静态数据流，最小化累积的下一步预测损失。

    arXiv:2403.01554v1 Announce Type: new  Abstract: Transformers have become the dominant architecture for sequence modeling tasks such as natural language processing or audio processing, and they are now even considered for tasks that are not naturally sequential such as image classification. Their ability to attend to and to process a set of tokens as context enables them to develop in-context few-shot learning abilities. However, their potential for online continual learning remains relatively unexplored. In online continual learning, a model must adapt to a non-stationary stream of data, minimizing the cumulative nextstep prediction loss. We focus on the supervised online continual learning setting, where we learn a predictor $x_t \rightarrow y_t$ for a sequence of examples $(x_t, y_t)$. Inspired by the in-context learning capabilities of transformers and their connection to meta-learning, we propose a method that leverages these strengths for online continual learning. Our approach e
    
[^101]: 基于内部表征的上下文锐度作为警报：减少幻觉的一个视角

    In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation

    [https://arxiv.org/abs/2403.01548](https://arxiv.org/abs/2403.01548)

    本研究从内部表征角度深入探讨了大型语言模型幻觉的机制，发现了幻觉的一个显著模式，即在上下文标记的隐藏状态中，正确生成具有更清晰的上下文激活。我们提出了一种基于熵的度量方法，将“锐度”纳入解码过程中，制定了一种受限解码方法，实验证明其在知识寻求和幻觉任务上的有效性。

    

    大型语言模型（LLMs）经常会产生幻觉并产生事实错误，然而我们对它们为什么会犯这些错误的理解仍然有限。在本研究中，我们从内部表征的角度深入探讨LLM幻觉的潜在机制，并发现与幻觉相关的一个突出模式：正确的生成在上下文标记的隐藏状态中具有更清晰的上下文激活，而不正确的生成则没有。利用这一见解，我们提出了一种基于熵的度量来量化上下文隐藏状态之间的“锐度”，并将其纳入解码过程中以制定一种受限解码方法。在各种知识寻求和幻觉基准测试上的实验证明了我们方法的一致有效性，例如，在TruthfulQA上实现了高达8.6点的改进。我们相信这项研究可以提高我们对幻觉的理解。

    arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
    
[^102]: 使用深度学习技术在单模和多模设置下进行高光谱图像分析

    Hyperspectral Image Analysis in Single-Modal and Multimodal setting using Deep Learning Techniques

    [https://arxiv.org/abs/2403.01546](https://arxiv.org/abs/2403.01546)

    该研究利用深度学习技术处理高光谱图像分析中的高维度和有限空间分辨率挑战，通过多模态学习整合不同数据，应用对抗学习和知识蒸馏来克服领域差异和缺失模态问题，并定制深度学习架构适配HSI数据特征。

    

    高光谱成像由于其出色的光谱分辨率，为土地利用和覆盖提供了精准的分类。然而，高维度和有限的空间分辨率等挑战影响了其效果。本研究通过应用深度学习技术，以集成化方式高效处理、提取特征和分类数据，来解决这些挑战。为了增强空间分辨率，我们通过多模态学习整合来自LiDAR和SAR数据等互补模态的信息。此外，利用对抗学习和知识蒸馏来克服因领域差异和缺失模态而产生的问题。我们还针对HSI数据的独特特征量身定制了深度学习架构，利用一维卷积和循环神经网络来处理其连续光谱维度。架构内的视觉注意力和反馈连接等技术

    arXiv:2403.01546v1 Announce Type: cross  Abstract: Hyperspectral imaging provides precise classification for land use and cover due to its exceptional spectral resolution. However, the challenges of high dimensionality and limited spatial resolution hinder its effectiveness. This study addresses these challenges by employing deep learning techniques to efficiently process, extract features, and classify data in an integrated manner. To enhance spatial resolution, we integrate information from complementary modalities such as LiDAR and SAR data through multimodal learning. Moreover, adversarial learning and knowledge distillation are utilized to overcome issues stemming from domain disparities and missing modalities. We also tailor deep learning architectures to suit the unique characteristics of HSI data, utilizing 1D convolutional and recurrent neural networks to handle its continuous spectral dimension. Techniques like visual attention and feedback connections within the architecture
    
[^103]: 分层量化联邦学习：统计异质性的一种强大方法

    Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity

    [https://arxiv.org/abs/2403.01540](https://arxiv.org/abs/2403.01540)

    该算法结合了分层联邦学习中的梯度聚合和模型聚合，通过量化提高通信效率，表现出对统计异质性的鲁棒性。

    

    本文介绍了一种新颖的分层联邦学习算法，该算法在多个集合中结合了量化以提高通信效率，并展示了对于统计异质性的弹性。与传统的分层联邦学习算法不同，我们的方法在集合内迭代中结合了梯度聚合和集合间迭代中的模型聚合。我们提供了一个全面的分析框架来评估其最优性差距和收敛速度，将这些方面与传统算法进行了比较。此外，我们开发了一个问题表述，以导出封闭形式的最优系统参数解。我们的研究结果表明，我们的算法在一系列参数上始终实现高学习精度，并且在具有异构数据分布的场景中明显优于其他分层算法。

    arXiv:2403.01540v1 Announce Type: new  Abstract: This paper presents a novel hierarchical federated learning algorithm within multiple sets that incorporates quantization for communication-efficiency and demonstrates resilience to statistical heterogeneity. Unlike conventional hierarchical federated learning algorithms, our approach combines gradient aggregation in intra-set iterations with model aggregation in inter-set iterations. We offer a comprehensive analytical framework to evaluate its optimality gap and convergence rate, comparing these aspects with those of conventional algorithms. Additionally, we develop a problem formulation to derive optimal system parameters in a closed-form solution. Our findings reveal that our algorithm consistently achieves high learning accuracy over a range of parameters and significantly outperforms other hierarchical algorithms, particularly in scenarios with heterogeneous data distributions.
    
[^104]: 混合策略纳什均衡用于人群导航

    Mixed-Strategy Nash Equilibrium for Crowd Navigation

    [https://arxiv.org/abs/2403.01537](https://arxiv.org/abs/2403.01537)

    通过简单的迭代贝叶斯更新方案和基于数据驱动的框架，我们证明了混合策略纳什均衡模型为人群导航提供了实时且可扩展的决策制定方法。

    

    我们解决了针对人群导航找到混合策略纳什均衡的问题。混合策略纳什均衡为机器人提供了一个严谨的模型，使其能够预测人群中不确定但合作的人类行为，但计算成本通常太高，无法进行可扩展和实时的决策制定。在这里，我们证明了一个简单的迭代贝叶斯更新方案收敛于混合策略社交导航游戏的纳什均衡。此外，我们提出了一个基于数据驱动的框架，通过将代理策略初始化为从人类数据集学习的高斯过程，来构建该游戏。基于所提出的混合策略纳什均衡模型，我们开发了一个基于采样的人群导航框架，可以集成到现有导航方法中，并可在笔记本电脑 CPU 上实时运行。我们通过模拟环境和真实世界的非结构化环境中人类数据集对我们的框架进行了评估。

    arXiv:2403.01537v1 Announce Type: cross  Abstract: We address the problem of finding mixed-strategy Nash equilibrium for crowd navigation. Mixed-strategy Nash equilibrium provides a rigorous model for the robot to anticipate uncertain yet cooperative human behavior in crowds, but the computation cost is often too high for scalable and real-time decision-making. Here we prove that a simple iterative Bayesian updating scheme converges to the Nash equilibrium of a mixed-strategy social navigation game. Furthermore, we propose a data-driven framework to construct the game by initializing agent strategies as Gaussian processes learned from human datasets. Based on the proposed mixed-strategy Nash equilibrium model, we develop a sampling-based crowd navigation framework that can be integrated into existing navigation methods and runs in real-time on a laptop CPU. We evaluate our framework in both simulated environments and real-world human datasets in unstructured environments. Our framework
    
[^105]: 使用核函数的快速遍历搜索

    Fast Ergodic Search with Kernel Functions

    [https://arxiv.org/abs/2403.01536](https://arxiv.org/abs/2403.01536)

    提出了一种使用核函数的快速遍历搜索方法，其在搜索空间维度上具有线性复杂度，可以推广到李群，并且通过数值测试展示比现有算法快两个数量级。

    

    遍历搜索使得对信息分布进行最佳探索成为可能，同时保证了对搜索空间的渐近覆盖。然而，当前的方法通常在搜索空间维度上具有指数计算复杂度，并且局限于欧几里得空间。我们引入了一种计算高效的遍历搜索方法。我们的贡献是双重的。首先，我们开发了基于核的遍历度量，并将其从欧几里得空间推广到李群上。我们正式证明了所建议的度量与标准遍历度量一致，同时保证了在搜索空间维度上具有线性复杂度。其次，我们推导了非线性系统的核遍历度量的一阶最优性条件，这使得轨迹优化变得更加高效。全面的数值基准测试表明，所提出的方法至少比现有最先进的算法快两个数量级。

    arXiv:2403.01536v1 Announce Type: cross  Abstract: Ergodic search enables optimal exploration of an information distribution while guaranteeing the asymptotic coverage of the search space. However, current methods typically have exponential computation complexity in the search space dimension and are restricted to Euclidean space. We introduce a computationally efficient ergodic search method. Our contributions are two-fold. First, we develop a kernel-based ergodic metric and generalize it from Euclidean space to Lie groups. We formally prove the proposed metric is consistent with the standard ergodic metric while guaranteeing linear complexity in the search space dimension. Secondly, we derive the first-order optimality condition of the kernel ergodic metric for nonlinear systems, which enables efficient trajectory optimization. Comprehensive numerical benchmarks show that the proposed method is at least two orders of magnitude faster than the state-of-the-art algorithm. Finally, we d
    
[^106]: 神经图生成器：使用潜在扩散模型的特征条件图生成

    Neural Graph Generator: Feature-Conditioned Graph Generation using Latent Diffusion Models

    [https://arxiv.org/abs/2403.01535](https://arxiv.org/abs/2403.01535)

    NGG是一个神经图生成器，通过潜在扩散模型实现了特征条件图生成，具有模拟复杂图模式和控制图生成过程的显著能力。

    

    图生成已经成为机器学习中的一个关键任务，面临着生成能够准确反映特定属性的图形的重大挑战。本文介绍了神经图生成器（NGG）这一新颖方法，它利用条件潜在扩散模型进行图生成。NGG展示了对复杂图模式进行建模的显著能力，提供了对图生成过程的控制。NGG利用变分图自动编码器进行图压缩，利用在潜在向量空间中的扩散过程，由总结图统计信息的向量指导。我们展示了NGG在各种图生成任务中的通用性，展示了其捕捉期望图属性并推广到未见图形的能力。

    arXiv:2403.01535v1 Announce Type: new  Abstract: Graph generation has emerged as a crucial task in machine learning, with significant challenges in generating graphs that accurately reflect specific properties. Existing methods often fall short in efficiently addressing this need as they struggle with the high-dimensional complexity and varied nature of graph properties. In this paper, we introduce the Neural Graph Generator (NGG), a novel approach which utilizes conditioned latent diffusion models for graph generation. NGG demonstrates a remarkable capacity to model complex graph patterns, offering control over the graph generation process. NGG employs a variational graph autoencoder for graph compression and a diffusion process in the latent vector space, guided by vectors summarizing graph statistics. We demonstrate NGG's versatility across various graph generation tasks, showing its capability to capture desired graph properties and generalize to unseen graphs. This work signifies 
    
[^107]: 机器学习利用收缩时间间隔和日常收集的临床数据预测急性心肌梗死后的长期死亡率

    Machine learning predicts long-term mortality after acute myocardial infarction using systolic time intervals and routinely collected clinical data

    [https://arxiv.org/abs/2403.01533](https://arxiv.org/abs/2403.01533)

    该研究利用机器学习模型和新生物标志物探索长期死亡率预测，为心脏病患者提供更准确的医疗决策支持。

    

    精确估计心脏患者当前和未来的合并症是优先考虑连续生理监测和新疗法的重要因素。机器学习模型在预测患有心脏病的患者短期死亡率方面表现良好，但在长期预测方面的效用有限。本研究旨在研究基于树的机器学习模型在长期死亡率预测上的性能，以及两个最近引入的生物标志物对长期死亡率的影响。本研究利用了台湾中国卫生福利部CCHIA的公开可用数据。医疗记录用于收集包括年龄、性别、BMI、经皮冠状动脉介入（PCI）状态和高血压、血脂异常、ST段抬高型心肌梗死（STEMI）和非STEMI等合并症在内的人口统计学和临床数据。

    arXiv:2403.01533v1 Announce Type: cross  Abstract: Precise estimation of cardiac patients' current and future comorbidities is an important factor in prioritizing continuous physiological monitoring and new therapies. ML models have shown satisfactory performance in short-term mortality prediction of patients with heart disease, while their utility in long-term predictions is limited. This study aims to investigate the performance of tree-based ML models on long-term mortality prediction and the effect of two recently introduced biomarkers on long-term mortality. This study utilized publicly available data from CCHIA at the Ministry of Health and Welfare, Taiwan, China. Medical records were used to gather demographic and clinical data, including age, gender, BMI, percutaneous coronary intervention (PCI) status, and comorbidities such as hypertension, dyslipidemia, ST-segment elevation myocardial infarction (STEMI), and non-STEMI. Using medical and demographic records as well as two rec
    
[^108]: 基于数据驱动的局部算子查找用于等离子体系统简化建模：II. 应用于参数动力学

    Data-driven local operator finding for reduced-order modelling of plasma systems: II. Application to parametric dynamics

    [https://arxiv.org/abs/2403.01532](https://arxiv.org/abs/2403.01532)

    本研究介绍了一种新的数据驱动局部算子查找算法Phi Method，展示了其在学习参数动力学中的有效性，并应用于预测系统在未见参数空间中的行为。

    

    真实世界中的系统通常表现出受各种参数影响的动态，这些参数可以是固有的也可以是外部可控的，因此需要能够可靠捕捉这些参数行为的模型。等离子体技术是这种系统的典型例子。例如，主导霍尔推进器（一种航天器推进技术）全局动态的现象会随着各种参数变化，比如“自维持电场”。在本文第二部分中，继续在第一部分中介绍了我们的新颖数据驱动局部算子查找算法Phi Method后，我们展示了该方法在学习参数动力学方面的有效性，以预测系统在未见参数空间中的行为。我们提出了两种改进：参数化Phi Method和集合Phi Method，并通过二维流体-穿过圆柱和一维霍尔推进器等离子放电问题进行了演示。在流体案例中与参数化OPT-DMD进行了比较评估。

    arXiv:2403.01532v1 Announce Type: cross  Abstract: Real-world systems often exhibit dynamics influenced by various parameters, either inherent or externally controllable, necessitating models capable of reliably capturing these parametric behaviors. Plasma technologies exemplify such systems. For example, phenomena governing global dynamics in Hall thrusters (a spacecraft propulsion technology) vary with various parameters, such as the "self-sustained electric field". In this Part II, following on the introduction of our novel data-driven local operator finding algorithm, Phi Method, in Part I, we showcase the method's effectiveness in learning parametric dynamics to predict system behavior across unseen parameter spaces. We present two adaptations: the "parametric Phi Method" and the "ensemble Phi Method", which are demonstrated through 2D fluid-flow-past-a-cylinder and 1D Hall-thruster-plasma-discharge problems. Comparative evaluation against parametric OPT-DMD in the fluid case demo
    
[^109]: 基于数据驱动的等离子体系统简化建模中的本地算子查找：I. 概念和验证

    Data-driven local operator finding for reduced-order modelling of plasma systems: I. Concept and verifications

    [https://arxiv.org/abs/2403.01523](https://arxiv.org/abs/2403.01523)

    引入了"Phi Method"，通过约束回归和候选项库发现离散化的微分方程系统，成功导出了可靠且稳健的简化等离子体模型，具有广泛的应用前景。

    

    被广泛追求但难以找到的简化等离子体模型可以高效地预测不同设置和配置下的等离子体行为。由于其潜力促进科学研究并加快等离子体技术的发展，过去十年对这类模型的需求急剧增加。本文介绍了“Phi Method”，该方法结合了计算能力和数据驱动方法的进展。第一部分介绍了这一新颖算法，该算法利用受数值离散方案启发的候选项库上的约束回归来发现离散化的微分方程系统。我们展示了Phi Method在三个测试案例中（Lorenz吸引子、圆柱体流动以及代表性的一维Hall推进器等离子体）导出可靠且稳健的简化模型（ROMs）的有效性。第二部分将深入探讨该方法在参数动态发现中的应用。

    arXiv:2403.01523v1 Announce Type: cross  Abstract: Reduced-order plasma models that can efficiently predict plasma behavior across various settings and configurations are highly sought after yet elusive. The demand for such models has surged in the past decade due to their potential to facilitate scientific research and expedite the development of plasma technologies. In line with the advancements in computational power and data-driven methods, we introduce the "Phi Method" in this two-part article. Part I presents this novel algorithm, which employs constrained regression on a candidate term library informed by numerical discretization schemes to discover discretized systems of differential equations. We demonstrate Phi Method's efficacy in deriving reliable and robust reduced-order models (ROMs) for three test cases: the Lorenz attractor, flow past a cylinder, and a 1D Hall-thruster-representative plasma. Part II will delve into the method's application for parametric dynamics discov
    
[^110]: 重新审视动态评估: 大型语言模型的在线调整

    Revisiting Dynamic Evaluation: Online Adaptation for Large Language Models

    [https://arxiv.org/abs/2403.01518](https://arxiv.org/abs/2403.01518)

    在线适应可以将参数转变为时间变化状态，提供一种具有内存权重记忆的上下文长度扩展形式，更符合神经科学中记忆概念，且在提升整体预测性能时尤其有趣。

    

    我们考虑在线微调语言模型参数的问题，也即称为动态评估。虽然一般认为这种方法可以提高整体的预测性能，特别是在考虑训练和评估数据之间的分布转移时，我们在这里强调在线调整将参数转变为时间变化状态，并提供了一种具有内存权重记忆的上下文长度扩展形式，更符合神经科学中记忆概念的思路。我们特别关注适应速度（以样本效率衡量）、对整体分布性漂移的敏感性以及执行梯度计算和参数更新的计算负担。我们的实证研究提供了关于何时在线调整尤为有趣的见解。我们强调，在线调整使得上下文长度和内存在概念上的区分模糊化。

    arXiv:2403.01518v1 Announce Type: new  Abstract: We consider the problem of online fine tuning the parameters of a language model at test time, also known as dynamic evaluation. While it is generally known that this approach improves the overall predictive performance, especially when considering distributional shift between training and evaluation data, we here emphasize the perspective that online adaptation turns parameters into temporally changing states and provides a form of context-length extension with memory in weights, more in line with the concept of memory in neuroscience. We pay particular attention to the speed of adaptation (in terms of sample efficiency),sensitivity to the overall distributional drift, and the computational overhead for performing gradient computations and parameter updates. Our empirical study provides insights on when online adaptation is particularly interesting. We highlight that with online adaptation the conceptual distinction between in-context l
    
[^111]: 将自监督学习应用于利用图神经网络进行网络流量入侵检测

    Applying Self-supervised Learning to Network Intrusion Detection for Network Flows with Graph Neural Network

    [https://arxiv.org/abs/2403.01501](https://arxiv.org/abs/2403.01501)

    本文研究了将GNNs应用于无监督方式识别特定类型网络流的方法，通过设计一个引入了图注意力机制的编码器来获取图嵌入。

    

    图神经网络（GNNs）由于其适合表示网络流量而引起人们的广泛关注，尤其是在网络入侵检测系统（NIDS）中。然而，大多数现有的基于GNN的NIDS方法是有监督或半监督的。网络流需要手动注释为监督标签，这个过程耗时，甚至不可能，导致NIDS难以适应潜在复杂的攻击，特别是在大规模实际场景中。现有基于GNN的自监督方法着重于将网络流分类为良性或非良性，难以揭示实际攻击类型。本文研究了以无监督方式应用GNNs识别特定类型网络流的方法。我们首先设计了一个编码器来获取图嵌入，引入了图注意力机制，并将边信息视为唯一基本因素。

    arXiv:2403.01501v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have garnered intensive attention for Network Intrusion Detection System (NIDS) due to their suitability for representing the network traffic flows. However, most present GNN-based methods for NIDS are supervised or semi-supervised. Network flows need to be manually annotated as supervisory labels, a process that is time-consuming or even impossible, making NIDS difficult to adapt to potentially complex attacks, especially in large-scale real-world scenarios. The existing GNN-based self-supervised methods focus on the binary classification of network flow as benign or not, and thus fail to reveal the types of attack in practice. This paper studies the application of GNNs to identify the specific types of network flows in an unsupervised manner. We first design an encoder to obtain graph embedding, that introduces the graph attention mechanism and considers the edge information as the only essential factor. Th
    
[^112]: 基于正规化流的可微粒子滤波器

    Normalising Flow-based Differentiable Particle Filters

    [https://arxiv.org/abs/2403.01499](https://arxiv.org/abs/2403.01499)

    本文提出了一种基于正规化流的可微粒子滤波器框架，能够实现有效的概率密度估计并在复杂环境中灵活学习这些模块。

    

    最近，人们对将神经网络引入粒子滤波器中的兴趣日益增加，例如可微粒子滤波器，以实现在复杂环境中对非线性非高斯状态空间模型进行联合顺序状态估计和模型学习。现有的可微粒子滤波器主要由普通神经网络构建，不允许密度估计。因此，它们要么受限于自举粒子滤波框架，要么采用预定义的分布系列（例如高斯分布），限制了它们在更复杂的现实世界场景中的性能。本文提出了一种使用（条件）正规化流构建动态模型、提议分布和测量模型的可微粒子滤波器框架。这不仅使其能够产生有效的概率密度，还使所提出的方法能够灵活地学习这些模块。

    arXiv:2403.01499v1 Announce Type: new  Abstract: Recently, there has been a surge of interest in incorporating neural networks into particle filters, e.g. differentiable particle filters, to perform joint sequential state estimation and model learning for non-linear non-Gaussian state-space models in complex environments. Existing differentiable particle filters are mostly constructed with vanilla neural networks that do not allow density estimation. As a result, they are either restricted to a bootstrap particle filtering framework or employ predefined distribution families (e.g. Gaussian distributions), limiting their performance in more complex real-world scenarios. In this paper we present a differentiable particle filtering framework that uses (conditional) normalising flows to build its dynamic model, proposal distribution, and measurement model. This not only enables valid probability densities but also allows the proposed method to adaptively learn these modules in a flexible w
    
[^113]: ConvTimeNet: 一种用于多变量时间序列分析的深度分层全卷积模型

    ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis

    [https://arxiv.org/abs/2403.01493](https://arxiv.org/abs/2403.01493)

    ConvTimeNet是一种新颖的深度分层全卷积网络，通过自适应分段和全卷积块设计，有效捕捉了全局序列和跨变量的依赖性。

    

    本文介绍了ConvTimeNet，这是一种新颖的深度分层全卷积网络，旨在作为时间序列分析的通用模型。该网络的关键设计是为了克服传统卷积网络的局限性。首先，我们提出了将时间序列划分为子序列级补丁的自适应分段，将其视为基本建模单元。这种设置避免了与原始点级时间步长相关联的稀疏语义。其次，我们设计了一个全卷积块，通过巧妙地集成深度和点卷积操作，遵循Transformer编码器中采用的先进构建块风格。这个骨干网络能够有效捕捉全局序列和跨变量依赖性，因为它不仅融合了Transformer架构的先进性，还继承了卷积的固有属性。

    arXiv:2403.01493v1 Announce Type: new  Abstract: This paper introduces ConvTimeNet, a novel deep hierarchical fully convolutional network designed to serve as a general-purpose model for time series analysis. The key design of this network is twofold, designed to overcome the limitations of traditional convolutional networks. Firstly, we propose an adaptive segmentation of time series into sub-series level patches, treating these as fundamental modeling units. This setting avoids the sparsity semantics associated with raw point-level time steps. Secondly, we design a fully convolutional block by skillfully integrating deepwise and pointwise convolution operations, following the advanced building block style employed in Transformer encoders. This backbone network allows for the effective capture of both global sequence and cross-variable dependence, as it not only incorporates the advancements of Transformer architecture but also inherits the inherent properties of convolution. Furtherm
    
[^114]: 对深度生成模型的费舍尔信息度量进行近似用于检测离群分布

    Approximations to the Fisher Information Metric of Deep Generative Models for Out-Of-Distribution Detection

    [https://arxiv.org/abs/2403.01485](https://arxiv.org/abs/2403.01485)

    本文分析了一种使用数据点关于深度生成模型参数的梯度进行离群分布检测的方法，基于对OOD数据应具有更大梯度范数的简单直觉，通过近似费舍尔信息度量实现该方法

    

    基于概率似然的深度生成模型，如基于评分的扩散模型和变分自动编码器，是近年来用于拟合高维数据分布（如图像、文本或音频）的先进机器学习模型之一。它们可以自然地应用于许多下游任务之一，即离群分布（OOD）检测。然而，Nalisnick等人的开创性工作表明，深度生成模型始终为OOD数据推断出比它们训练过的数据更高的对数似然，标志着一个悬而未决的问题。在这项工作中，我们分析了使用数据点对深度生成模型的参数梯度进行OOD检测，基于这样的简单直觉，即OOD数据的梯度范数应该大于训练数据。我们形式化地将梯度大小的度量量化为近似费舍尔信息度量。我们展示了费舍尔信息矩阵（FIM）具有较大的绝对值

    arXiv:2403.01485v1 Announce Type: cross  Abstract: Likelihood-based deep generative models such as score-based diffusion models and variational autoencoders are state-of-the-art machine learning models approximating high-dimensional distributions of data such as images, text, or audio. One of many downstream tasks they can be naturally applied to is out-of-distribution (OOD) detection. However, seminal work by Nalisnick et al. which we reproduce showed that deep generative models consistently infer higher log-likelihoods for OOD data than data they were trained on, marking an open problem. In this work, we analyse using the gradient of a data point with respect to the parameters of the deep generative model for OOD detection, based on the simple intuition that OOD data should have larger gradient norms than training data. We formalise measuring the size of the gradient as approximating the Fisher information metric. We show that the Fisher information matrix (FIM) has large absolute di
    
[^115]: 具有方向性邻域注意力的异质图表示学习

    Representation Learning on Heterophilic Graph with Directional Neighborhood Attention

    [https://arxiv.org/abs/2403.01475](https://arxiv.org/abs/2403.01475)

    提出了具有方向性邻域注意力的Directional Graph Attention Network（DGAT），能够有效结合特征注意力和全局方向信息，通过新型拉普拉斯矩阵减少节点之间的扩散距离，并引入拓扑引导的邻域修剪和边添加机制来提升异质图的表示学习性能。

    

    图注意力网络（GAT）是最受欢迎的图神经网络（GNN）架构之一，它采用注意力机制来学习边缘权重，在各种应用中展现了良好性能。然而，由于它只包含了来自即时邻域的信息，缺乏捕获远程和全局图信息的能力，导致在一些数据集上表现不佳，特别是在异质图上。为了解决这一局限性，我们在本文中提出了方向图注意力网络（DGAT）。DGAT能够将基于特征的注意力与从图拓扑中提取的全局方向信息结合起来。为此，提出了一种新型拉普拉斯矩阵，可以明显减少节点之间的扩散距离。基于新的拉普拉斯矩阵，提出了拓扑引导的邻域修剪和边添加机制，以消除噪声和限制性质的影响。

    arXiv:2403.01475v1 Announce Type: cross  Abstract: Graph Attention Network (GAT) is one of the most popular Graph Neural Network (GNN) architecture, which employs the attention mechanism to learn edge weights and has demonstrated promising performance in various applications. However, since it only incorporates information from immediate neighborhood, it lacks the ability to capture long-range and global graph information, leading to unsatisfactory performance on some datasets, particularly on heterophilic graphs. To address this limitation, we propose the Directional Graph Attention Network (DGAT) in this paper. DGAT is able to combine the feature-based attention with the global directional information extracted from the graph topology. To this end, a new class of Laplacian matrices is proposed which can provably reduce the diffusion distance between nodes. Based on the new Laplacian, topology-guided neighbour pruning and edge adding mechanisms are proposed to remove the noisy and cap
    
[^116]: WARDEN：多方向背门水印用于Embedding-as-a-Service版权保护

    WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection

    [https://arxiv.org/abs/2403.01472](https://arxiv.org/abs/2403.01472)

    本研究提出了一种名为WARDEN的新方法，通过在嵌入文本中加入多个可能的水印方向，增加了水印消除的难度，以应对EaaS中背门水印被移除的新威胁。

    

    Embedding as a Service（EaaS）已成为一种广泛采用的解决方案，为自然语言处理（NLP）中的各种下游任务提供特征提取能力。先前的研究表明，EaaS容易受到模型抽取攻击的威胁；然而，通过向文本嵌入添加背门水印，并随后验证攻击模型的发布后，可以缓解这一问题。通过对最近用于EaaS的水印策略EmbMarker的分析，我们设计了一种新颖的CSE（Cluster、Selection、Elimination）攻击，它能够移除背门水印同时保持嵌入的高效性，表明先前的水印方法是可以被突破的。针对这一新威胁，我们提出了一种新的协议，通过整合多种可能的水印方向使水印的移除变得更具挑战性。我们的防御方法WARDEN显著增加了水印消除的难度。

    arXiv:2403.01472v1 Announce Type: cross  Abstract: Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the ste
    
[^117]: 保持相关性：一种用于生成合成数据的统计方法

    Preserving correlations: A statistical method for generating synthetic data

    [https://arxiv.org/abs/2403.01471](https://arxiv.org/abs/2403.01471)

    提出了一种方法来生成具有统计代表性的合成数据，能够在合成数据集中保持原始数据集中特征之间的相关性，并提供可调整的隐私级别。

    

    我们提出了一种方法来生成具有统计代表性的合成数据。主要目标是在合成数据集中保持原始数据集中存在的特征之间的相关性，同时提供一个舒适的隐私级别，可以根据特定客户需求进行调整。我们详细描述了我们用于分析原始数据集和生成合成数据点的算法。我们使用了一个大型能源相关数据集进行了测试。我们在定性（例如通过可视化相关性图）和定量（以适当的$\ell^1$类型误差范数作为评估指标）方面获得了良好的结果。所提出的方法论是一般的，不依赖于使用的测试数据集。我们期望它可适用于比此处指示的更广泛的情境。

    arXiv:2403.01471v1 Announce Type: new  Abstract: We propose a method to generate statistically representative synthetic data. The main goal is to be able to maintain in the synthetic dataset the correlations of the features present in the original one, while offering a comfortable privacy level that can be eventually tailored on specific customer demands.   We describe in detail our algorithm used both for the analysis of the original dataset and for the generation of the synthetic data points. The approach is tested using a large energy-related dataset. We obtain good results both qualitatively (e.g. via vizualizing correlation maps) and quantitatively (in terms of suitable $\ell^1$-type error norms used as evaluation metrics).   The proposed methodology is general in the sense that it does not rely on the used test dataset. We expect it to be applicable in a much broader context than indicated here.
    
[^118]: 协作适应：通过双向适应实现无源图领域自适应

    Collaborate to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation

    [https://arxiv.org/abs/2403.01467](https://arxiv.org/abs/2403.01467)

    本论文提出了一种名为GraphCTA的新方法，通过协作的模型适应和图适应来解决无源图领域自适应问题。

    

    无监督图领域自适应（UGDA）已经成为将标记丰富的源图的知识转移到完全未标记的目标图的实际解决方案。然而，大多数方法需要标记丰富的源图提供监督信号，这在真实世界的情况下可能无法访问，原因是规定和隐私问题。在本文中，我们探讨了无源无监督图领域自适应的场景，试图解决领域适应问题而不使用标记的源图。具体来说，我们提出了一种称为GraphCTA的新范式，通过一系列程序协作地执行模型自适应和图自适应：（1）基于目标图中节点邻域预测进行模型自适应，考虑了局部和全局信息；（2）通过邻域对比性地更新图结构和节点属性来执行图自适应。

    arXiv:2403.01467v1 Announce Type: cross  Abstract: Unsupervised Graph Domain Adaptation (UGDA) has emerged as a practical solution to transfer knowledge from a label-rich source graph to a completely unlabelled target graph. However, most methods require a labelled source graph to provide supervision signals, which might not be accessible in the real-world settings due to regulations and privacy concerns. In this paper, we explore the scenario of source-free unsupervised graph domain adaptation, which tries to address the domain adaptation problem without accessing the labelled source graph. Specifically, we present a novel paradigm called GraphCTA, which performs model adaptation and graph adaptation collaboratively through a series of procedures: (1) conduct model adaptation based on node's neighborhood predictions in target graph considering both local and global information; (2) perform graph adaptation by updating graph structure and node attributes via neighborhood contrastive le
    
[^119]: 基于转移概率的一步多视角聚类

    One-Step Multi-View Clustering Based on Transition Probability

    [https://arxiv.org/abs/2403.01460](https://arxiv.org/abs/2403.01460)

    提出了一种基于转移概率的一步多视角聚类方法，通过学习从锚点到类别的转移概率，以及从样本到类别的转移概率，获得样本和锚点的软标签矩阵，增强了聚类的解释性。

    

    大规模多视角聚类算法基于锚图，显示出良好的性能和效率，并在近年来得到广泛探讨。尽管取得了成功，当前方法在聚类过程中缺乏解释性，且未充分考虑跨不同视角的补充信息。为解决这些缺点，我们引入了基于转移概率的一步多视角聚类（OSMVC-TP）。该方法采用概率方法，利用锚图，表示样本到锚点的转移概率。我们的方法直接学习从锚点到类别的转移概率，并计算从样本到类别的转移概率，从而获得样本和锚点的软标签矩阵，提高了聚类的解释性。

    arXiv:2403.01460v1 Announce Type: new  Abstract: The large-scale multi-view clustering algorithms, based on the anchor graph, have shown promising performance and efficiency and have been extensively explored in recent years. Despite their successes, current methods lack interpretability in the clustering process and do not sufficiently consider the complementary information across different views. To address these shortcomings, we introduce the One-Step Multi-View Clustering Based on Transition Probability (OSMVC-TP). This method adopts a probabilistic approach, which leverages the anchor graph, representing the transition probabilities from samples to anchor points. Our method directly learns the transition probabilities from anchor points to categories, and calculates the transition probabilities from samples to categories, thus obtaining soft label matrices for samples and anchor points, enhancing the interpretability of clustering. Furthermore, to maintain consistency in labels ac
    
[^120]: 在联邦学习系统中增强数据溯源和模型透明性 - 一个数据库的方法

    Enhancing Data Provenance and Model Transparency in Federated Learning Systems - A Database Approach

    [https://arxiv.org/abs/2403.01451](https://arxiv.org/abs/2403.01451)

    提出了一种增强联邦学习系统中数据溯源和模型透明性的方法，通过加密技术和高效的模型管理来追踪数据变换。

    

    联邦学习（FL）提供了一个在分布式边缘设备上训练机器学习模型的有前景的范例，同时保护数据隐私。然而，在这些分布式环境中确保数据的完整性和可追溯性仍然是一个关键挑战。本文提出了一种增强联邦学习系统中数据溯源和模型透明性的方法。我们的方法利用了一系列加密技术和高效的模型管理来跟踪FL过程中数据的变换。

    arXiv:2403.01451v1 Announce Type: cross  Abstract: Federated Learning (FL) presents a promising paradigm for training machine learning models across decentralized edge devices while preserving data privacy. Ensuring the integrity and traceability of data across these distributed environments, however, remains a critical challenge. The ability to create transparent artificial intelligence, such as detailing the training process of a machine learning model, has become an increasingly prominent concern due to the large number of sensitive (hyper)parameters it utilizes; thus, it is imperative to strike a reasonable balance between openness and the need to protect sensitive information.   In this paper, we propose one of the first approaches to enhance data provenance and model transparency in federated learning systems. Our methodology leverages a combination of cryptographic techniques and efficient model management to track the transformation of data throughout the FL process, and seeks 
    
[^121]: 面向智能电网负荷预测的隐私保护协同分裂学习框架

    Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting

    [https://arxiv.org/abs/2403.01438](https://arxiv.org/abs/2403.01438)

    提出了面向智能电网负荷预测的隐私保护协同分裂学习框架，通过将深度神经网络模型分裂为Grid Station（GS）和服务提供商（SP）部分，实现智能电表数据的隐私保护和负载预测的个性化模型训练。

    

    准确的负荷预测对能源管理、基础设施规划和供需平衡至关重要。智能电表数据的可用性导致了传感器数据驱动的负荷预测需求。传统机器学习允许使用来自多个智能电表的数据训练单个全局模型，这需要将数据传输到中央服务器，引发了对网络要求、隐私和安全性的担忧。我们提出了一种基于分裂学习的负荷预测框架，以缓解这一问题。我们将深度神经网络模型分为两个部分，一个用于每个Grid Station（GS），负责一个整个社区的智能电表；另一个用于服务提供商（SP）。客户智能电表不共享其数据，而是使用各自的GS模型拆分进行前向传递，只将其激活与GS共享。在这一框架下，每个GS负责为其各自的社区训练个性化模型分裂。

    arXiv:2403.01438v1 Announce Type: new  Abstract: Accurate load forecasting is crucial for energy management, infrastructure planning, and demand-supply balancing. Smart meter data availability has led to the demand for sensor-based load forecasting. Conventional ML allows training a single global model using data from multiple smart meters requiring data transfer to a central server, raising concerns for network requirements, privacy, and security. We propose a split learning-based framework for load forecasting to alleviate this issue. We split a deep neural network model into two parts, one for each Grid Station (GS) responsible for an entire neighbourhood's smart meters and the other for the Service Provider (SP). Instead of sharing their data, client smart meters use their respective GSs' model split for forward pass and only share their activations with the GS. Under this framework, each GS is responsible for training a personalized model split for their respective neighbourhoods,
    
[^122]: 在SE(3)-不变空间中的扩散过程

    On Diffusion Process in SE(3)-invariant Space

    [https://arxiv.org/abs/2403.01430](https://arxiv.org/abs/2403.01430)

    通过数学方法揭示了在SE(3)-不变空间中的扩散机制，提出了准确且无需投影的扩散模型，可以提高生成路径的性能和速度，并为其他系统的SE(3)-不变性提供了有价值的见解。

    

    使用基于扩散的模型对具有SE(3)-不变性的可行3D结构（例如分子和点云）进行采样，在各种现实应用中证明了其前景，其中SE(3)-不变性属性可以通过点之间的距离流形自然地表征。然而，由于非平凡的几何形状，我们仍然缺乏对这种SE(3)-不变空间内扩散机制的全面理解。本研究通过数学上描绘在SE(3)-不变性下的扩散机制，通过微分几何的视角深入分析坐标之间的相互作用行为和点之间的距离流形。在这一分析的基础上，我们相应地提出了准确且无需投影的扩散SDE和ODE。这些公式不仅能够提高生成路径的性能和速度，同时还为包含SE(3)-不变性的其他系统提供了宝贵的见解。

    arXiv:2403.01430v1 Announce Type: new  Abstract: Sampling viable 3D structures (e.g., molecules and point clouds) with SE(3)-invariance using diffusion-based models proved promising in a variety of real-world applications, wherein SE(3)-invariant properties can be naturally characterized by the inter-point distance manifold. However, due to the non-trivial geometry, we still lack a comprehensive understanding of the diffusion mechanism within such SE(3)-invariant space. This study addresses this gap by mathematically delineating the diffusion mechanism under SE(3)-invariance, via zooming into the interaction behavior between coordinates and the inter-point distance manifold through the lens of differential geometry. Upon this analysis, we propose accurate and projection-free diffusion SDE and ODE accordingly. Such formulations enable enhancing the performance and the speed of generation pathways; meanwhile offering valuable insights into other systems incorporating SE(3)-invariance.
    
[^123]: Algogens概论

    Introduction to Algogens

    [https://arxiv.org/abs/2403.01426](https://arxiv.org/abs/2403.01426)

    Algogens是生成型人工智能与传统算法相结合的一种方法，可以更有效地解决复杂挑战，具有更好的适应性和效率。

    

    该书介绍了Algogens的概念，这是生成型人工智能与传统算法相结合的一种前景广阔的方法，旨在提高跨各领域的问题解决技术。它提供了一种易于理解的概述，阐述了Algogens如何将人工智能的创新潜力与算法的可靠性相结合，以比单独使用其中任何一个更有效地解决复杂挑战。该文探讨了Algogens的基础知识、发展、应用和优势，例如更好的适应性和效率。通过例子和案例研究，读者将了解到Algogens在今天的实际用途以及它们在未来网络安全、医疗保健和环境科学创新中的潜力。

    arXiv:2403.01426v1 Announce Type: new  Abstract: This book introduces the concept of Algogens, a promising integration of generative AI with traditional algorithms aimed at improving problem-solving techniques across various fields. It provides an accessible overview of how Algogens combine AI's innovative potential with algorithms' reliability to tackle complex challenges more effectively than either could alone.   The text explores the basics of Algogens, their development, applications, and advantages, such as better adaptability and efficiency. Through examples and case studies, readers will learn about Algogens' practical uses today and their potential for future cybersecurity, healthcare, and environmental science innovation.   Acknowledging new technologies' challenges and ethical considerations, the book offers a balanced look at the prospects and obstacles facing Algogens. It invites a broad audience, including experts and newcomers, to engage with the topic and consider Algog
    
[^124]: 集体认证对抗图注入攻击的鲁棒性

    Collective Certified Robustness against Graph Injection Attacks

    [https://arxiv.org/abs/2403.01423](https://arxiv.org/abs/2403.01423)

    本文提出了针对图注入攻击的GNN的集体证书，通过二进制整数二次约束线性规划构建问题，并开发了定制的线性化技术，极大地提高了认证性能。

    

    我们研究了在图注入攻击下的GNN的认证鲁棒性。现有研究仅通过验证每个节点独立提供样本级证书，导致认证性能非常有限。本文提出了第一种集体证书，可以同时认证一组目标节点。为了实现这一目标，我们将问题建模为二进制整数二次约束线性规划（BQCLP）。我们进一步开发了一种定制的线性化技术，使我们能够将BQCLP松弛为可以高效求解的线性规划（LP）。通过全面的实验，我们证明了我们的集体认证方案显著提高了认证性能，而计算开销却很小。例如，在Citeseer数据集上在1分钟内解决LP，我们将认证比率从0.0%显著提高到81.2%。

    arXiv:2403.01423v1 Announce Type: cross  Abstract: We investigate certified robustness for GNNs under graph injection attacks. Existing research only provides sample-wise certificates by verifying each node independently, leading to very limited certifying performance. In this paper, we present the first collective certificate, which certifies a set of target nodes simultaneously. To achieve it, we formulate the problem as a binary integer quadratic constrained linear programming (BQCLP). We further develop a customized linearization technique that allows us to relax the BQCLP into linear programming (LP) that can be efficiently solved. Through comprehensive experiments, we demonstrate that our collective certification scheme significantly improves certification performance with minimal computational overhead. For instance, by solving the LP within 1 minute on the Citeseer dataset, we achieve a significant increase in the certified ratio from 0.0% to 81.2% when the injected node number
    
[^125]: 异质性对不变性和因果关系的隐性偏差

    The Implicit Bias of Heterogeneity towards Invariance and Causality

    [https://arxiv.org/abs/2403.01420](https://arxiv.org/abs/2403.01420)

    异质性对于回归任务中出现因果性的贡献解释了为何大型语言模型能够从关联性训练中揭示因果关联。

    

    从经验上观察到，使用来自互联网的大量语料库训练的大型语言模型（LLM），使用一种变体回归损失，可以在一定程度上揭示因果关联。这与传统智慧“关联不是因果”以及传统因果推断范式相反，传统因果推断范式认为先前的因果知识应谨慎地纳入到方法设计中。令人困惑的是，为何在追求关联的回归任务中能够从更高层次的理解中出现因果性。本文声称从面向关联的训练中出现因果性可以归因于源数据的异质性、训练算法的随机性和学习模型的超参数化的耦合效应。我们使用一个简单但有见地的模型来阐释这样的直觉，该模型使用回归损失学习不变性，一种准因果关系。

    arXiv:2403.01420v1 Announce Type: new  Abstract: It is observed empirically that the large language models (LLM), trained with a variant of regression loss using numerous corpus from the Internet, can unveil causal associations to some extent. This is contrary to the traditional wisdom that ``association is not causation'' and the paradigm of traditional causal inference in which prior causal knowledge should be carefully incorporated into the design of methods. It is a mystery why causality, in a higher layer of understanding, can emerge from the regression task that pursues associations. In this paper, we claim the emergence of causality from association-oriented training can be attributed to the coupling effects from the heterogeneity of the source data, stochasticity of training algorithms, and over-parameterization of the learning models. We illustrate such an intuition using a simple but insightful model that learns invariance, a quasi-causality, using regression loss. To be spec
    
[^126]: Asyn2F：一种具有双向模型聚合的异步联邦学习框架

    Asyn2F: An Asynchronous Federated Learning Framework with Bidirectional Model Aggregation

    [https://arxiv.org/abs/2403.01417](https://arxiv.org/abs/2403.01417)

    Asyn2F是一种异步联邦学习框架，通过双向模型聚合实现了服务器异步聚合多个本地模型并得到新的全局模型，同时训练工作者可以在训练过程中将全局模型的新版本聚合到本地模型中，解决了过时信息问题。

    

    在联邦学习中，模型可以同步或异步训练。许多研究工作专注于为服务器开发聚合方法，将多个本地模型聚合成性能更好的全局模型。他们忽略了训练工作者的异质性，这导致本地模型训练的延迟，进而导致过时信息问题。本文设计并开发了Asyn2F，一种具有双向模型聚合的异步联邦学习框架。通过双向模型聚合，Asyn2F一方面允许服务器异步聚合多个本地模型并得到新的全局模型。另一方面，它允许训练工作者将全局模型的新版本聚合到本地模型中，即使在训练时期中途也可以进行。我们开发Asyn2F时考虑了实际实施要求。

    arXiv:2403.01417v1 Announce Type: new  Abstract: In federated learning, the models can be trained synchronously or asynchronously. Many research works have focused on developing an aggregation method for the server to aggregate multiple local models into the global model with improved performance. They ignore the heterogeneity of the training workers, which causes the delay in the training of the local models, leading to the obsolete information issue. In this paper, we design and develop Asyn2F, an Asynchronous Federated learning Framework with bidirectional model aggregation. By bidirectional model aggregation, Asyn2F, on one hand, allows the server to asynchronously aggregate multiple local models and results in a new global model. On the other hand, it allows the training workers to aggregate the new version of the global model into the local model, which is being trained even in the middle of a training epoch. We develop Asyn2F considering the practical implementation requirements
    
[^127]: 解耦权衡和选择：用于整合多个图预训练任务的方法

    Decoupling Weighing and Selecting for Integrating Multiple Graph Pre-training Tasks

    [https://arxiv.org/abs/2403.01400](https://arxiv.org/abs/2403.01400)

    本文提出了一种用于整合多个图预训练任务的新颖的实例级框架Weigh And Select（WAS），其中通过解耦的连体网络组合了权衡和选择这两个协作过程

    

    近年来，图预训练在图表示学习中取得了巨大成功。伴随着数百种图预训练任务的提出，整合从多个预训练任务中获得的知识已成为一个热门研究课题。本文确定了此主题的两个重要协作过程：（1）选择：如何基于它们的兼容性从给定任务池中选择最佳任务组合，和（2）权衡：如何基于它们的重要性权衡所选任务。虽然目前有很多工作集中在权衡上，但相比之下，很少有工作致力于选择。本文提出了一种用于整合多个图预训练任务的新颖的实例级框架Weigh And Select（WAS），其中权衡和选择这两个协作过程通过解耦的连体网络进行组合。具体而言，它首先自适应地学习任务的最佳组合

    arXiv:2403.01400v1 Announce Type: cross  Abstract: Recent years have witnessed the great success of graph pre-training for graph representation learning. With hundreds of graph pre-training tasks proposed, integrating knowledge acquired from multiple pre-training tasks has become a popular research topic. In this paper, we identify two important collaborative processes for this topic: (1) select: how to select an optimal task combination from a given task pool based on their compatibility, and (2) weigh: how to weigh the selected tasks based on their importance. While there currently has been a lot of work focused on weighing, comparatively little effort has been devoted to selecting. This paper proposes a novel instance-level framework for integrating multiple graph pre-training tasks, Weigh And Select (WAS), where the two collaborative processes, weighing and selecting, are combined by decoupled siamese networks. Specifically, it first adaptively learns an optimal combination of task
    
[^128]: 高斯过程预测与蒙特卡洛采样的融合

    Fusion of Gaussian Processes Predictions with Monte Carlo Sampling

    [https://arxiv.org/abs/2403.01389](https://arxiv.org/abs/2403.01389)

    本文提出了一种将高斯过程预测通过蒙特卡洛采样进行融合的新方法，旨在提高预测准确性。

    

    在科学和工程领域，我们经常使用旨在准确预测感兴趣变量的模型。认识到这些模型是对现实的近似，我们希望将多个模型应用于相同的数据并整合它们的结果。本文在贝叶斯范式内运行，依赖于高斯过程作为我们的模型。这些模型生成预测概率密度函数（pdf），目标是系统地整合它们，使用线性和对数线性汇总。我们引入了对于对数线性汇总的新方法，确定了高斯过程预测pdf的输入相关权重。通过蒙特卡洛采样从其后验中抽取权重样本实现pdf的聚合。通过使用合成数据集展示了这些方法的性能，以及基于线性汇总的方法。

    arXiv:2403.01389v1 Announce Type: new  Abstract: In science and engineering, we often work with models designed for accurate prediction of variables of interest. Recognizing that these models are approximations of reality, it becomes desirable to apply multiple models to the same data and integrate their outcomes. In this paper, we operate within the Bayesian paradigm, relying on Gaussian processes as our models. These models generate predictive probability density functions (pdfs), and the objective is to integrate them systematically, employing both linear and log-linear pooling. We introduce novel approaches for log-linear pooling, determining input-dependent weights for the predictive pdfs of the Gaussian processes. The aggregation of the pdfs is realized through Monte Carlo sampling, drawing samples of weights from their posterior. The performance of these methods, as well as those based on linear pooling, is demonstrated using a synthetic dataset.
    
[^129]: 《联邦迁移学习综述：挑战、方法与应用》

    A Comprehensive Survey of Federated Transfer Learning: Challenges, Methods and Applications

    [https://arxiv.org/abs/2403.01387](https://arxiv.org/abs/2403.01387)

    联邦学习是一种分布式机器学习范例，联邦迁移学习则将迁移学习引入联邦学习中，以解决不同参与者数据特征空间和分布的差异所带来的挑战。

    

    arXiv:2403.01387v1 公告类型：新摘要：联邦学习（FL）是一种新颖的分布式机器学习范例，通过消除数据共享的要求，使参与者能够共同训练一个带有隐私保护的集中模型。在实践中，FL通常涉及多个参与者，并且需要第三方聚合全局信息以指导目标参与者的更新。因此，由于每个参与者的训练和测试数据可能不是从相同的特征空间和相同的基础分布中抽样得来，很多FL方法往往效果不佳。同时，他们本地设备的差异（系统异构性）、在线数据的持续涌入（增量数据）和标记数据的稀缺可能进一步影响这些方法的性能。为了解决这个问题，将迁移学习（TL）集成到FL中的联邦迁移学习（FTL）引起了许多研究人员的关注。

    arXiv:2403.01387v1 Announce Type: new  Abstract: Federated learning (FL) is a novel distributed machine learning paradigm that enables participants to collaboratively train a centralized model with privacy preservation by eliminating the requirement of data sharing. In practice, FL often involves multiple participants and requires the third party to aggregate global information to guide the update of the target participant. Therefore, many FL methods do not work well due to the training and test data of each participant may not be sampled from the same feature space and the same underlying distribution. Meanwhile, the differences in their local devices (system heterogeneity), the continuous influx of online data (incremental data), and labeled data scarcity may further influence the performance of these methods. To solve this problem, federated transfer learning (FTL), which integrates transfer learning (TL) into FL, has attracted the attention of numerous researchers. However, since F
    
[^130]: 关于量化大型语言模型的可压缩性

    On the Compressibility of Quantized Large Language Models

    [https://arxiv.org/abs/2403.01384](https://arxiv.org/abs/2403.01384)

    研究在内存受限设备上应用数据压缩技术以加速量化LLM推理过程的一项初步工作。

    

    部署大型语言模型（LLMs）到边缘或移动设备上具有显著优势，如增强数据隐私和实时处理能力。本文研究了将数据压缩技术应用于减少数据移动，从而加速内存受限设备上量化LLM的推理过程的初步步骤。

    arXiv:2403.01384v1 Announce Type: cross  Abstract: Deploying Large Language Models (LLMs) on edge or mobile devices offers significant benefits, such as enhanced data privacy and real-time processing capabilities. However, it also faces critical challenges due to the substantial memory requirement of LLMs. Quantization is an effective way of reducing the model size while maintaining good performance. However, even after quantization, LLMs may still be too big to fit entirely into the limited memory of edge or mobile devices and have to be partially loaded from the storage to complete the inference. In this case, the I/O latency of model loading becomes the bottleneck of the LLM inference latency. In this work, we take a preliminary step of studying applying data compression techniques to reduce data movement and thus speed up the inference of quantized LLM on memory-constrained devices. In particular, we discussed the compressibility of quantized LLMs, the trade-off between the compres
    
[^131]: 大规模变分高斯状态空间模型

    Large-scale variational Gaussian state-space models

    [https://arxiv.org/abs/2403.01371](https://arxiv.org/abs/2403.01371)

    该论文介绍了一种针对具有高斯噪声驱动非线性动力学的状态空间模型的大规模变分算法和结构化逼近方法，可以有效评估ELBO和获取低方差的随机梯度估计，通过利用低秩蒙特卡罗逼近和推断网络的精度矩阵更新，将近似平滑问题转化为近似滤波问题。

    

    我们介绍了一种用于状态空间模型的嵌套变分推断算法和结构化变分逼近方法，其中非线性动力学由高斯噪声驱动。值得注意的是，所提出的框架允许在没有采用对角高斯逼近的情况下有效地评估ELBO和低方差随机梯度估计，通过利用（i）通过动力学对隐状态进行边缘化的蒙特卡罗逼近的低秩结构，（ii）一个推断网络，该网络通过低秩精度矩阵更新来近似更新步骤，（iii）将当前和未来观测编码为伪观测--将近似平滑问题转换为（更简单的）近似滤波问题。整体而言，必要的统计信息和ELBO可以在$O（TL（Sr+S^2+r^2））$时间内计算，其中$T$是系列长度，$L$是状态空间维数，$S$是用于逼近的样本数量。

    arXiv:2403.01371v1 Announce Type: cross  Abstract: We introduce an amortized variational inference algorithm and structured variational approximation for state-space models with nonlinear dynamics driven by Gaussian noise. Importantly, the proposed framework allows for efficient evaluation of the ELBO and low-variance stochastic gradient estimates without resorting to diagonal Gaussian approximations by exploiting (i) the low-rank structure of Monte-Carlo approximations to marginalize the latent state through the dynamics (ii) an inference network that approximates the update step with low-rank precision matrix updates (iii) encoding current and future observations into pseudo observations -- transforming the approximate smoothing problem into an (easier) approximate filtering problem. Overall, the necessary statistics and ELBO can be computed in $O(TL(Sr + S^2 + r^2))$ time where $T$ is the series length, $L$ is the state-space dimensionality, $S$ are the number of samples used to app
    
[^132]: Wav2Vec2嵌入在设备端单通道语音增强中的探究

    A Closer Look at Wav2Vec2 Embeddings for On-Device Single-Channel Speech Enhancement

    [https://arxiv.org/abs/2403.01369](https://arxiv.org/abs/2403.01369)

    本文研究了Wav2Vec2嵌入在单通道语音增强中的应用，发现SSL表示对增强任务几乎没有任何价值。

    

    自监督学习模型被发现在某些语音任务中非常有效，比如自动语音识别、说话人识别、关键词识别等。本文探讨了SSL表示在具有挑战性条件下的单通道语音增强中的用途，并发现它们对增强任务几乎没有任何价值。

    arXiv:2403.01369v1 Announce Type: cross  Abstract: Self-supervised learned models have been found to be very effective for certain speech tasks such as automatic speech recognition, speaker identification, keyword spotting and others. While the features are undeniably useful in speech recognition and associated tasks, their utility in speech enhancement systems is yet to be firmly established, and perhaps not properly understood. In this paper, we investigate the uses of SSL representations for single-channel speech enhancement in challenging conditions and find that they add very little value for the enhancement task. Our constraints are designed around on-device real-time speech enhancement -- model is causal, the compute footprint is small. Additionally, we focus on low SNR conditions where such models struggle to provide good enhancement. In order to systematically examine how SSL representations impact performance of such enhancement models, we propose a variety of techniques to u
    
[^133]: 针对目标营销的强盗利润最大化

    Bandit Profit-maximization for Targeted Marketing

    [https://arxiv.org/abs/2403.01361](https://arxiv.org/abs/2403.01361)

    该论文研究了针对目标营销的强盗利润最大化问题，并提出了在敌对强盗情境下的近乎最优算法。

    

    我们研究了一个顺序利润最大化问题，优化价格和像营销支出这样的辅助变量。具体来说，我们旨在在一个任意序列的多个需求曲线上最大化利润，每个曲线依赖于一个不同的辅助变量，但共享相同的价格。一个典型的例子是针对营销，其中一家公司（卖方）希望在多个市场上销售产品。公司可以为不同市场投入不同的营销支出以优化客户获取，但必须在所有市场上保持相同的价格。此外，市场可能具有异质的需求曲线，每个需求曲线对价格和营销支出的响应方式不同。公司的目标是最大化毛利润，即总收入减去营销成本。

    arXiv:2403.01361v1 Announce Type: new  Abstract: We study a sequential profit-maximization problem, optimizing for both price and ancillary variables like marketing expenditures. Specifically, we aim to maximize profit over an arbitrary sequence of multiple demand curves, each dependent on a distinct ancillary variable, but sharing the same price. A prototypical example is targeted marketing, where a firm (seller) wishes to sell a product over multiple markets. The firm may invest different marketing expenditures for different markets to optimize customer acquisition, but must maintain the same price across all markets. Moreover, markets may have heterogeneous demand curves, each responding to prices and marketing expenditures differently. The firm's objective is to maximize its gross profit, the total revenue minus marketing costs.   Our results are near-optimal algorithms for this class of problems in an adversarial bandit setting, where demand curves are arbitrary non-adaptive seque
    
[^134]: a-DCF：一种与架构无关的度量，适用于抵御欺骗攻击的说话人验证

    a-DCF: an architecture agnostic metric with application to spoofing-robust speaker verification

    [https://arxiv.org/abs/2403.01355](https://arxiv.org/abs/2403.01355)

    提出了一种架构无关的检测成本函数（a-DCF），适用于评估抵御欺骗攻击的自动说话人验证（ASV）解决方案。

    

    欺骗检测目前是一个主流研究课题。标准度量可以用来评估孤立欺骗检测解决方案的性能，也有一些提出来支持它们在与说话人检测结合时的评估，但存在已知的缺陷或者限制了结合说话人和欺骗检测器的架构方法。本文提出了一种架构无关的检测成本函数（a-DCF）。作为广泛用于评估自动说话人验证（ASV）性能的原始DCF的推广，a-DCF旨在用于评估抵御欺骗攻击的ASV。与DCF类似，a-DCF从Bayes风险的角度反映了决策的代价，其中明确定义了类先验和检测成本模型。我们通过对架构异构的抵御欺骗攻击的ASV解决方案进行基准评估，展示了a-DCF的优点。

    arXiv:2403.01355v1 Announce Type: cross  Abstract: Spoofing detection is today a mainstream research topic. Standard metrics can be applied to evaluate the performance of isolated spoofing detection solutions and others have been proposed to support their evaluation when they are combined with speaker detection. These either have well-known deficiencies or restrict the architectural approach to combine speaker and spoof detectors. In this paper, we propose an architecture-agnostic detection cost function (a-DCF). A generalisation of the original DCF used widely for the assessment of automatic speaker verification (ASV), the a-DCF is designed for the evaluation of spoofing-robust ASV. Like the DCF, the a-DCF reflects the cost of decisions in a Bayes risk sense, with explicitly defined class priors and detection cost model. We demonstrate the merit of the a-DCF through the benchmarking evaluation of architecturally-heterogeneous spoofing-robust ASV solutions.
    
[^135]: 使用钟形曲线权重函数改进不确定性采样

    Improving Uncertainty Sampling with Bell Curve Weight Function

    [https://arxiv.org/abs/2403.01352](https://arxiv.org/abs/2403.01352)

    使用钟形曲线权重函数改进了不确定性采样方法，提高了监督学习效率。

    

    通常，通过随机选择未标记实例进行注释来训练监督学习模型。这种方法对于学习模型是有效的，但在获取标记实例昂贵的情况下可能成本高昂。因此，我们提出了使用钟形曲线权重函数改进不确定性采样，以提高监督学习的效率。

    arXiv:2403.01352v1 Announce Type: new  Abstract: Typically, a supervised learning model is trained using passive learning by randomly selecting unlabelled instances to annotate. This approach is effective for learning a model, but can be costly in cases where acquiring labelled instances is expensive. For example, it can be time-consuming to manually identify spam mails (labelled instances) from thousands of emails (unlabelled instances) flooding an inbox during initial data collection. Generally, we answer the above scenario with uncertainty sampling, an active learning method that improves the efficiency of supervised learning by using fewer labelled instances than passive learning. Given an unlabelled data pool, uncertainty sampling queries the labels of instances where the predicted probabilities, p, fall into the uncertainty region, i.e., $p \approx 0.5$. The newly acquired labels are then added to the existing labelled data pool to learn a new model. Nonetheless, the performance 
    
[^136]: SANGRIA：基于梯度提升的堆叠自编码器神经网络用于室内定位

    SANGRIA: Stacked Autoencoder Neural Networks with Gradient Boosting for Indoor Localization

    [https://arxiv.org/abs/2403.01348](https://arxiv.org/abs/2403.01348)

    SANGRIA是一个基于堆叠自编码器神经网络与梯度提升树的室内定位框架，相比其他先进框架，能够实现更低的平均定位误差。

    

    室内定位是许多嵌入式应用中的关键任务，例如资产跟踪、应急响应和实时导航。在本文中，我们提出了一种名为SANGRIA的基于指纹的室内定位框架，该框架使用了堆叠自编码器神经网络与梯度提升树。我们的方法旨在克服设备异构性挑战，该挑战可能导致用于定位的嵌入式设备之间的无线信号测量出现不确定性。我们将SANGRIA与几种最先进的框架进行了比较，并在不同室内场所和异构设备上展示了42.96%较低的平均定位误差。

    arXiv:2403.01348v1 Announce Type: cross  Abstract: Indoor localization is a critical task in many embedded applications, such as asset tracking, emergency response, and realtime navigation. In this article, we propose a novel fingerprintingbased framework for indoor localization called SANGRIA that uses stacked autoencoder neural networks with gradient boosted trees. Our approach is designed to overcome the device heterogeneity challenge that can create uncertainty in wireless signal measurements across embedded devices used for localization. We compare SANGRIA to several state-of-the-art frameworks and demonstrate 42.96% lower average localization error across diverse indoor locales and heterogeneous devices.
    
[^137]: 提高在嘈杂数据集上主动学习的成本效率

    Improve Cost Efficiency of Active Learning over Noisy Dataset

    [https://arxiv.org/abs/2403.01346](https://arxiv.org/abs/2403.01346)

    提出了一种新的采样函数，通过在更广泛的范围内进行采样，限制了嘈杂和正标签的选择，提高了成本效率。

    

    主动学习是一种学习策略，即机器学习算法主动识别和标记数据点以优化其学习过程。该策略在存在大量未标记数据的领域特别有效，但标记这些数据点的成本过高。本文考虑了二元分类的情况，其中获取正实例的成本明显高于负实例的成本。

    arXiv:2403.01346v1 Announce Type: new  Abstract: Active learning is a learning strategy whereby the machine learning algorithm actively identifies and labels data points to optimize its learning. This strategy is particularly effective in domains where an abundance of unlabeled data exists, but the cost of labeling these data points is prohibitively expensive. In this paper, we consider cases of binary classification, where acquiring a positive instance incurs a significantly higher cost compared to that of negative instances. For example, in the financial industry, such as in money-lending businesses, a defaulted loan constitutes a positive event leading to substantial financial loss. To address this issue, we propose a shifted normal distribution sampling function that samples from a wider range than typical uncertainty sampling. Our simulation underscores that our proposed sampling function limits both noisy and positive label selection, delivering between 20% and 32% improved cost 
    
[^138]: 缓解持续测试时间自适应模型中的偏差

    Mitigating the Bias in the Model for Continual Test-Time Adaptation

    [https://arxiv.org/abs/2403.01344](https://arxiv.org/abs/2403.01344)

    本文提出了一种方法来缓解持续测试时间自适应模型中的预测偏差问题，并通过制定类别特定的指数加权移动平均目标原型来改善CTA场景中的性能。

    

    持续测试时间自适应（CTA）是一项具有挑战性的任务，旨在将源预训练模型适应不断变化的目标域。在CTA设置中，模型不知道目标领域何时发生变化，因此在测试时间面临流输入分布的剧烈变化。关键挑战在于以在线方式持续地调整模型以适应不断变化的目标领域。我们发现，模型在不断适应目标数据的分布时会呈现高度偏倚的预测。它更频繁地预测某些类别，而这导致了错误的过度自信的预测。本文通过缓解这一问题来提高CTA场景中的性能。为了减轻偏差问题，我们利用可靠的目标样本制定以类别为基础的指数加权移动平均目标原型，并利用它们以类别方式对目标特征进行聚类。此外，我们还旨在对齐目标领域

    arXiv:2403.01344v1 Announce Type: new  Abstract: Continual Test-Time Adaptation (CTA) is a challenging task that aims to adapt a source pre-trained model to continually changing target domains. In the CTA setting, a model does not know when the target domain changes, thus facing a drastic change in the distribution of streaming inputs during the test-time. The key challenge is to keep adapting the model to the continually changing target domains in an online manner. We find that a model shows highly biased predictions as it constantly adapts to the chaining distribution of the target data. It predicts certain classes more often than other classes, making inaccurate over-confident predictions. This paper mitigates this issue to improve performance in the CTA scenario. To alleviate the bias issue, we make class-wise exponential moving average target prototypes with reliable target samples and exploit them to cluster the target features class-wisely. Moreover, we aim to align the target d
    
[^139]: $\mathcal{C}^k$一致逼近$G$-不变和反对称函数，嵌入维度和多项式表示

    Uniform $\mathcal{C}^k$ Approximation of $G$-Invariant and Antisymmetric Functions, Embedding Dimensions, and Polynomial Representations

    [https://arxiv.org/abs/2403.01339](https://arxiv.org/abs/2403.01339)

    该论文提出了一种新的方法，可以通过$G$不变多项式一致逼近$G$不变函数，同时能够获得反对称函数的统一$\mathcal{C}^k$逼近。这种方法不仅可以应用于完全对称函数，还可以将嵌入维度独立于目标函数的正则性、逼近准确度和$k$。

    

    对于对称群$\mathcal{S}_n$的任意子群$G$，我们提出了关于由$G$不变多项式逼近$G$不变函数的$\mathcal{C}^k$一致逼近结果。对于完全对称函数的情况($G = \mathcal{S}_n$)，我们展示了这导致了Zaheer等人(2018)的sum-decomposition Deep Sets方法，其中内部和外部函数都可以选择为光滑函数，并且内部函数可以选择与被逼近的目标函数无关。特别地，我们展示了所需的嵌入维度与目标函数的正则性、期望逼近的准确性以及$k$无关。接下来，我们展示了一个类似的过程可以让我们得到反对称函数的$\mathcal{C}^k$一致逼近作为$K$项的和，其中每一项均为光滑完全对称函数和光滑反对称函数之积。

    arXiv:2403.01339v1 Announce Type: new  Abstract: For any subgroup $G$ of the symmetric group $\mathcal{S}_n$ on $n$ symbols, we present results for the uniform $\mathcal{C}^k$ approximation of $G$-invariant functions by $G$-invariant polynomials. For the case of totally symmetric functions ($G = \mathcal{S}_n$), we show that this gives rise to the sum-decomposition Deep Sets ansatz of Zaheer et al. (2018), where both the inner and outer functions can be chosen to be smooth, and moreover, the inner function can be chosen to be independent of the target function being approximated. In particular, we show that the embedding dimension required is independent of the regularity of the target function, the accuracy of the desired approximation, as well as $k$. Next, we show that a similar procedure allows us to obtain a uniform $\mathcal{C}^k$ approximation of antisymmetric functions as a sum of $K$ terms, where each term is a product of a smooth totally symmetric function and a smooth antisy
    
[^140]: 将思维和LLMs串联起来学习DNA结构生物物理学

    Chaining thoughts and LLMs to learn DNA structural biophysics

    [https://arxiv.org/abs/2403.01332](https://arxiv.org/abs/2403.01332)

    通用语言模型chatGPT 3.5-turbo的微调，在学习DNA结构生物物理学方面显示出新的潜力和优势。

    

    未来发展AI科学家的一项重要发展是，一个能够整合各种实验数据并生成可验证假设的工具具有巨大潜力。到目前为止，定制的机器学习模型已被创建用于专门从事单一科学任务，但缺乏通用模型的灵活性。在这里，我们展示了一个通用的大型语言模型，chatGPT 3.5-turbo，可以被微调来学习DNA的结构生物物理学。我们发现，将模型微调为返回思维链式响应以及串联微调用于子任务的模型，具有增强的能力来分析和设计DNA序列及其结构。

    arXiv:2403.01332v1 Announce Type: cross  Abstract: The future development of an AI scientist, a tool that is capable of integrating a variety of experimental data and generating testable hypotheses, holds immense potential. So far, bespoke machine learning models have been created to specialize in singular scientific tasks, but otherwise lack the flexibility of a general purpose model. Here, we show that a general purpose large language model, chatGPT 3.5-turbo, can be fine-tuned to learn the structural biophysics of DNA. We find that both fine-tuning models to return chain-of-thought responses and chaining together models fine-tuned for subtasks have an enhanced ability to analyze and design DNA sequences and their structures.
    
[^141]: 为快速采样扩散和流动模型提供定制的非平稳求解器

    Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models

    [https://arxiv.org/abs/2403.01329](https://arxiv.org/abs/2403.01329)

    该论文引入了定制的非平稳（BNS）求解器，提高了扩散和流动模型的采样效率，具有小参数空间、快速优化、样本多样性，并且在低中 NFE 范围内接近标准精炼方法。

    

    本文介绍了定制的非平稳（BNS）求解器，这是一种解算精髓方法，旨在提高扩散和流动模型的样本效率。BNS 求解器基于一系列可证明包含现有数值 ODE 求解器的非平稳求解器家族，随之显著改进样本逼近度（PSNR）超过这些基线。与模型精炼相比，BNS 求解器具有微小参数空间（<200 参数）、快速优化（快两个数量级）、保持样本多样性，并且与以前的求解器精炼方法相反，几乎能在低中 NFE 范围内接近标准精炼方法，如 Progressive Distillation。例如，BNS 求解器在 class-conditional ImageNet-64 中使用 16 NFE 实现 45 PSNR / 1.76 FID。我们尝试了 BNS 求解器来进行有条件图像生成、文本到图像生成和文本到音频生成。

    arXiv:2403.01329v1 Announce Type: cross  Abstract: This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models. BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to model distillation, BNS solvers benefit from a tiny parameter space ($<$200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generat
    
[^142]: 高维尾指数回归：以社交媒体病毒帖子文本分析为例

    High-Dimensional Tail Index Regression: with An Application to Text Analyses of Viral Posts in Social Media

    [https://arxiv.org/abs/2403.01318](https://arxiv.org/abs/2403.01318)

    提出了高维尾指数回归方法，利用正则化估计和去偏方法进行推断，支持理论的仿真研究，并在社交媒体病毒帖子文本分析中应用。

    

    受社交媒体病毒帖子的点赞分布（如点赞数量）经验性幂律的启发，我们引入了高维尾指数回归及其参数的估计和推断方法。我们提出了一种正则化估计量，证明了它的一致性，并推导了其收敛速度。为了进行推断，我们提出了去偏正则化估计，证明了去偏估计量的渐近正态性。仿真研究支持了我们的理论。这些方法被应用于对涉及 LGBTQ+ 话题的 X（原 Twitter）病毒帖子的文本分析。

    arXiv:2403.01318v1 Announce Type: cross  Abstract: Motivated by the empirical power law of the distributions of credits (e.g., the number of "likes") of viral posts in social media, we introduce the high-dimensional tail index regression and methods of estimation and inference for its parameters. We propose a regularized estimator, establish its consistency, and derive its convergence rate. To conduct inference, we propose to debias the regularized estimate, and establish the asymptotic normality of the debiased estimator. Simulation studies support our theory. These methods are applied to text analyses of viral posts in X (formerly Twitter) concerning LGBTQ+.
    
[^143]: 少即是多：面向可扩展和通用学习的跳数图注意力在电路上的应用

    Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits

    [https://arxiv.org/abs/2403.01317](https://arxiv.org/abs/2403.01317)

    提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。

    

    虽然图神经网络（GNNs）在各种电子设计自动化（EDA）任务中学习电路表示方面变得流行，但当应用于大图时，它们面临可扩展性挑战，并且对新设计的泛化能力有限。这些限制使它们在解决大规模复杂电路问题时不太实用。在这项工作中，我们提出了HOGA，一种新颖的基于注意力的模型，用于以可扩展和通用的方式学习电路表示。HOGA首先在模型训练之前针对每个节点计算跳数特征。随后，跳数特征仅用于通过门控自注意力模块生成节点表示，该模块自适应地学习不同跳数之间的重要特征，而不涉及图拓扑。因此，HOGA能够适应不同电路之间的各种结构，并可以以分布式的方式高效训练。

    arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
    
[^144]: 睡眠臂决策问题中接近最优的每次行动遗憾界

    Near-optimal Per-Action Regret Bounds for Sleeping Bandits

    [https://arxiv.org/abs/2403.01315](https://arxiv.org/abs/2403.01315)

    该论文提出了针对睡眠臂决策问题的接近最优每次行动遗憾界，通过直接最小化每次行动遗憾，使用Generalized EXP3、EXP3-IX和Tsallis entropy下的FTRL方法，获得了较之现有方法更好的界。

    

    我们推导了针对睡眠臂决策问题的接近最优每次行动遗憾界，其中敌手选择每轮可用臂的集合和它们的损失。在每轮至多有 $A$ 个可用臂的 $K$ 个总臂的情况下，已知的最好上界为 $O(K\sqrt{TA\ln{K}})$，通过间接最小化内部睡眠遗憾获得。与极小值 $\Omega(\sqrt{TA})$ 下界相比，这个上界包含额外的乘数因子 $K\ln{K}$。我们通过直接最小化每次行动遗憾，使用EXP3、EXP3-IX和带有Tsallis熵的FTRL的推广版本，从而获得了顺序为 $O(\sqrt{TA\ln{K}})$ 和 $O(\sqrt{T\sqrt{AK}})$ 的接近最优界。我们将结果扩展到了从睡眠专家获得建议的臂决策问题设置，同时推广了EXP4。这为现有的多个自适应和跟踪遗憾界的新证明铺平了道路。

    arXiv:2403.01315v1 Announce Type: new  Abstract: We derive near-optimal per-action regret bounds for sleeping bandits, in which both the sets of available arms and their losses in every round are chosen by an adversary. In a setting with $K$ total arms and at most $A$ available arms in each round over $T$ rounds, the best known upper bound is $O(K\sqrt{TA\ln{K}})$, obtained indirectly via minimizing internal sleeping regrets. Compared to the minimax $\Omega(\sqrt{TA})$ lower bound, this upper bound contains an extra multiplicative factor of $K\ln{K}$. We address this gap by directly minimizing the per-action regret using generalized versions of EXP3, EXP3-IX and FTRL with Tsallis entropy, thereby obtaining near-optimal bounds of order $O(\sqrt{TA\ln{K}})$ and $O(\sqrt{T\sqrt{AK}})$. We extend our results to the setting of bandits with advice from sleeping experts, generalizing EXP4 along the way. This leads to new proofs for a number of existing adaptive and tracking regret bounds for 
    
[^145]: VNLP：土耳其自然语言处理工具包

    VNLP: Turkish NLP Package

    [https://arxiv.org/abs/2403.01309](https://arxiv.org/abs/2403.01309)

    VNLP是首个专门针对土耳其语开发的自然语言处理工具包，包含多种NLP工具，其中的标记分类模型基于“上下文模型”，支持多种任务如情感分析、命名实体识别等。

    

    在本文中，我们介绍了VNLP：第一个专门针对土耳其语的完整、开源、文档完备、轻量级、可投入生产使用的最先进的自然语言处理（NLP）工具包。它包含各种工具，从最简单的任务，如句子分割和文本规范化，到更高级的任务，如文本和标记分类模型。其标记分类模型基于“上下文模型”，这是一种既是编码器又是自回归模型的新颖架构。VNLP模型解决的NLP任务包括但不限于情感分析、命名实体识别、形态分析和消歧以及词性标注。此外，它配备了预训练的词嵌入和相应的SentencePiece Unigram标记器。VNLP具有开源的GitHub存储库、ReadtheDocs文档、方便安装的PyPi包、Python和逗号

    arXiv:2403.01309v1 Announce Type: cross  Abstract: In this work, we present VNLP: the first dedicated, complete, open-source, well-documented, lightweight, production-ready, state-of-the-art Natural Language Processing (NLP) package for the Turkish language. It contains a wide variety of tools, ranging from the simplest tasks, such as sentence splitting and text normalization, to the more advanced ones, such as text and token classification models. Its token classification models are based on "Context Model", a novel architecture that is both an encoder and an auto-regressive model. NLP tasks solved by VNLP models include but are not limited to Sentiment Analysis, Named Entity Recognition, Morphological Analysis \& Disambiguation and Part-of-Speech Tagging. Moreover, it comes with pre-trained word embeddings and corresponding SentencePiece Unigram tokenizers. VNLP has an open-source GitHub repository, ReadtheDocs documentation, PyPi package for convenient installation, Python and comma
    
[^146]: VBART: 土耳其LLM

    VBART: The Turkish LLM

    [https://arxiv.org/abs/2403.01308](https://arxiv.org/abs/2403.01308)

    VBART是第一个土耳其序列到序列大语言模型，通过与BART和mBART模型结合形成了紧凑型LLM，并在多个任务中表现出色，为土耳其自然语言处理研究开辟了新的可能性。

    

    我们提出了VBART，这是第一个土耳其序列到序列大语言模型（LLMs），在一个大语料库上从头开始进行预训练。VBART是基于BART和mBART模型的好思路构建的紧凑型LLMs，分为Large和XLarge两个尺寸。微调后的VBART模型在提取性文本摘要、标题生成、文本改写、问答和问题生成等任务中超越了先前的最先进结果。它们允许为未来的文本生成任务和数据集进行微调，为土耳其自然语言处理（NLP）研究开辟了新路径。我们的工作表明，拥有为土耳其进行预训练的LLM比多语言模型提高了最多3倍，改进了现有结果，并为训练和推理提供了高效的模型。此外，我们展示了我们的单语分词器比OpenAI的多语言分词器更高效7倍。最后但同样重要的是，我们介绍了一种扩展现有预训

    arXiv:2403.01308v1 Announce Type: new  Abstract: We present VBART, the first Turkish sequence-to-sequence Large Language Models (LLMs) pre-trained on a large corpus from scratch. VBART are compact LLMs based on good ideas leveraged from BART and mBART models and come in two sizes, Large and XLarge. Fine-tuned VBART models surpass the prior state-of-the-art results in abstractive text summarization, title generation, text paraphrasing, question answering and question generation tasks. They allow fine-tuning for future text generation tasks and datasets, carving a new path for Turkish Natural Language Processing (NLP) research. Our work shows that having a pre-trained LLM for Turkish outperforms up to 3x multilingual models, improving existing results and providing efficient models for training and inference. Moreover, we show that our monolingual tokenizer is 7x more efficient than OpenAI's multilingual tokenizer. Last but not least, we introduce a method to enlarge an existing pre-trai
    
[^147]: ICC：用于多模态数据集筛选的图像描述具体性量化

    ICC: Quantifying Image Caption Concreteness for Multimodal Dataset Curation

    [https://arxiv.org/abs/2403.01306](https://arxiv.org/abs/2403.01306)

    提出一种新的度量标准，图像描述具体性，用于评估标题文本的具体性和相关性，以帮助在多模态学习中隔离提供最强信号的最具体样本。

    

    arXiv:2403.01306v1 公告类型：新摘要：针对配对文本-图像数据的Web规模训练在多模态学习中变得越来越重要，但挑战在野外数据集的高噪声特性。标准数据过滤方法成功去除了不匹配的文本-图像对，但允许语义相关但非常抽象或主观的文本。这些方法缺乏细粒度的能力来隔离提供在嘈杂数据集中学习最强信号的最具体样本。在这项工作中，我们提出了一种新的度量标准，图像描述具体性，评估没有图像参考的标题文本以衡量其具体性和相关性，以供在多模态学习中使用。我们的方法利用了衡量视觉-语义信息损失的强基础模型来进行评估。我们证明了这与人类对单词和句子级文本具体性的评估高度相关。此外，我们展示了...

    arXiv:2403.01306v1 Announce Type: new  Abstract: Web-scale training on paired text-image data is becoming increasingly central to multimodal learning, but is challenged by the highly noisy nature of datasets in the wild. Standard data filtering approaches succeed in removing mismatched text-image pairs, but permit semantically related but highly abstract or subjective text. These approaches lack the fine-grained ability to isolate the most concrete samples that provide the strongest signal for learning in a noisy dataset. In this work, we propose a new metric, image caption concreteness, that evaluates caption text without an image reference to measure its concreteness and relevancy for use in multimodal learning. Our approach leverages strong foundation models for measuring visual-semantic information loss in multimodal representations. We demonstrate that this strongly correlates with human evaluation of concreteness in both single-word and sentence-level texts. Moreover, we show tha
    
[^148]: 在线采购中的供应商推荐

    Supplier Recommendation in Online Procurement

    [https://arxiv.org/abs/2403.01301](https://arxiv.org/abs/2403.01301)

    本研究提出了一个推荐系统，用于在道路货运在线采购中辅助进行供应商发现，能够提供个性化的供应商推荐，考虑到客户的需求和偏好。

    

    供应链优化对于健康和盈利的企业至关重要。许多公司使用在线采购系统与供应商签订合同。邀请最具竞争力的供应商竞标这些合同至关重要。在这项工作中，我们提出了一个推荐系统，以协助在道路货运在线采购中进行供应商发现。我们的系统能够提供个性化的供应商推荐，考虑到客户的需求和偏好。这是推荐系统的一种新颖应用，需要设计选择以符合在线采购的独特要求。我们使用真实数据进行的初步结果令人鼓舞。

    arXiv:2403.01301v1 Announce Type: cross  Abstract: Supply chain optimization is key to a healthy and profitable business. Many companies use online procurement systems to agree contracts with suppliers. It is vital that the most competitive suppliers are invited to bid for such contracts. In this work, we propose a recommender system to assist with supplier discovery in road freight online procurement. Our system is able to provide personalized supplier recommendations, taking into account customer needs and preferences. This is a novel application of recommender systems, calling for design choices that fit the unique requirements of online procurement. Our preliminary results, using real-world data, are promising.
    
[^149]: 一个光子物理不可克隆函数对多值机器学习攻击的韧性

    A Photonic Physically Unclonable Function's Resilience to Multiple-Valued Machine Learning Attacks

    [https://arxiv.org/abs/2403.01299](https://arxiv.org/abs/2403.01299)

    该论文研究了光子物理不可克隆函数对多值机器学习攻击的韧性，发现需要大量CRPs才能成功训练模型进行预测，从而展示了光子PUF对此类攻击的抵抗能力。

    

    物理不可克隆函数（PUFs）使用非线性相关的挑战-响应对（CRPs）来识别集成电路。理想情况下，挑战和相应响应之间的关系是不可预测的，即使已知某些CRPs的子集。先前的工作开发了一种光子PUF，相比非光学对应物提供了更好的安全性。在这里，我们调查了这种PUF对基于多值逻辑的机器学习攻击的敏感性。我们发现大约需要1,000个CRPs来训练模型，才能比随机猜测更好地预测响应位。考虑到从光子PUF获取大量CRPs的巨大挑战，我们的结果表明光子PUF对此类攻击具有韧性。

    arXiv:2403.01299v1 Announce Type: cross  Abstract: Physically unclonable functions (PUFs) identify integrated circuits using nonlinearly-related challenge-response pairs (CRPs). Ideally, the relationship between challenges and corresponding responses is unpredictable, even if a subset of CRPs is known. Previous work developed a photonic PUF offering improved security compared to non-optical counterparts. Here, we investigate this PUF's susceptibility to Multiple-Valued-Logic-based machine learning attacks. We find that approximately 1,000 CRPs are necessary to train models that predict response bits better than random chance. Given the significant challenge of acquiring a vast number of CRPs from a photonic PUF, our results demonstrate photonic PUF resilience against such attacks.
    
[^150]: NoMAD-Attention: 通过无MAD操作实现CPU上高效LLM推断

    NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention

    [https://arxiv.org/abs/2403.01273](https://arxiv.org/abs/2403.01273)

    NoMAD-Attention提出了一种高效的注意力算法，通过在CPU上使用寄存器内查找取代MAD操作，以实现LLM推断的快速计算。

    

    在中央处理单元（CPU）上进行大型语言模型推断具有挑战性，因为注意力计算中存在大量昂贵的MAD矩阵操作。本文认为现代CPU中的单指令多数据（SIMD）寄存器是一种珍贵的宝石，它允许在批处理中进行超低延迟查找。我们利用CPU的这一独特能力提出了NoMAD-Attention，这是一种高效的注意力算法，用于将MAD操作替换为寄存器内查找。通过硬件感知的算法设计，NoMAD-Attention实现了通过重复快速访问SIMD寄存器来计算注意力分数，尽管它们的大小非常有限。此外，NoMAD-Attention适用于预训练的基于注意力的LLM，无需对模型进行微调。实证评估表明，NoMAD-Attention很好地保持了原始LLM的质量，并加速了4位量化的LLaMA-7B-bas。

    arXiv:2403.01273v1 Announce Type: cross  Abstract: Large language model inference on Central Processing Units (CPU) is challenging due to the vast quantities of expensive Multiply-Add (MAD) matrix operations in the attention computations. In this paper, we argue that there is a rare gem in modern CPUs, Single-Instruction-Multiple-Data (SIMD) registers, which allow for ultra-low-latency lookups in batch. We leverage this unique capability of CPUs to propose NoMAD-Attention, an efficient attention algorithm that replaces MAD operations with in-register lookups. Through hardware-aware algorithmic designs, NoMAD-Attention achieves the computation of attention scores using repeated fast accesses to SIMD registers despite their highly limited sizes. Moreover, NoMAD-Attention works with pre-trained attention-based LLMs without model finetuning. Empirical evaluations demonstrate that NoMAD-Attention maintains the quality of the original LLMs well, and speeds up the 4-bit quantized LLaMA-7B-bas
    
[^151]: 可以用自信先验替代冷却后验吗？

    Can a Confident Prior Replace a Cold Posterior?

    [https://arxiv.org/abs/2403.01272](https://arxiv.org/abs/2403.01272)

    探讨了将后验调整替换为增加信心的先验分布的可行性，引入了实用的“DirClip”先验和“信心先验”，提供了对信心先验的一般见解。

    

    用于图像分类的基准数据集往往具有非常低的标签噪声水平。当贝叶斯神经网络在这些数据集上训练时，它们经常欠拟合，错误地表示数据的随机不确定性。一种常见的解决方案是调整后验概率，这样可以改善对训练数据的拟合，但从贝叶斯角度解释起来具有挑战性。我们探讨了后验调整是否可以被一种提高信心的先验分布替代。首先，我们引入了一个实用的采样“DirClip”先验，并且几乎与冷却后验的性能相匹配。其次，我们引入了一个“信心先验”，它在温度趋于零时直接近似于冷布局，但不能轻松抽样。最后，我们提供了一些关于提高信心的先验的一般见解，例如何时可能出现分歧以及如何通过微调来缓解数值不稳定性。

    arXiv:2403.01272v1 Announce Type: new  Abstract: Benchmark datasets used for image classification tend to have very low levels of label noise. When Bayesian neural networks are trained on these datasets, they often underfit, misrepresenting the aleatoric uncertainty of the data. A common solution is to cool the posterior, which improves fit to the training data but is challenging to interpret from a Bayesian perspective. We explore whether posterior tempering can be replaced by a confidence-inducing prior distribution. First, we introduce a "DirClip" prior that is practical to sample and nearly matches the performance of a cold posterior. Second, we introduce a "confidence prior" that directly approximates a cold likelihood in the limit of decreasing temperature but cannot be easily sampled. Lastly, we provide several general insights into confidence-inducing priors, such as when they might diverge and how fine-tuning can mitigate numerical instability.
    
[^152]: 防御联邦学习中的数据重构攻击：一种信息论方法

    Defending Against Data Reconstruction Attacks in Federated Learning: An Information Theory Approach

    [https://arxiv.org/abs/2403.01268](https://arxiv.org/abs/2403.01268)

    该论文通过一种信息论方法，旨在保证联邦学习在面临数据重构攻击时具有强大的隐私保证。

    

    联邦学习通过交换参数而非直接共享数据，在不同客户端之间训练一个黑匣子和高维模型，从而减少了由机器学习带来的隐私泄露。然而，联邦学习仍然容易受到成员推断攻击（MIA）或数据重构攻击（DRA）的影响。具体而言，攻击者可以通过构建DRA从本地数据集中提取信息，现有技术（如差分隐私）无法有效地阻止这种攻击。

    arXiv:2403.01268v1 Announce Type: new  Abstract: Federated Learning (FL) trains a black-box and high-dimensional model among different clients by exchanging parameters instead of direct data sharing, which mitigates the privacy leak incurred by machine learning. However, FL still suffers from membership inference attacks (MIA) or data reconstruction attacks (DRA). In particular, an attacker can extract the information from local datasets by constructing DRA, which cannot be effectively throttled by existing techniques, e.g., Differential Privacy (DP).   In this paper, we aim to ensure a strong privacy guarantee for FL under DRA. We prove that reconstruction errors under DRA are constrained by the information acquired by an attacker, which means that constraining the transmitted information can effectively throttle DRA. To quantify the information leakage incurred by FL, we establish a channel model, which depends on the upper bound of joint mutual information between the local dataset 
    
[^153]: 解剖语言模型：通过选择性修剪实现机器去学习

    Dissecting Language Models: Machine Unlearning via Selective Pruning

    [https://arxiv.org/abs/2403.01267](https://arxiv.org/abs/2403.01267)

    介绍了一种针对大型语言模型的机器去学习方法，通过选择性修剪神经元来实现去学习，发现LLMs中的神经元在特定任务中具有不同的重要性。

    

    本文引入了一种专门为大型语言模型（LLMs）设计的机器去学习方法。我们提出了一种针对LLMs的选择性修剪方法，根据神经元对特定能力的相对重要性来移除神经元，而非整体网络性能。该方法是一种高效的计算和数据方法，用于识别和删除能够实现特定行为的神经元。我们的研究发现，LLMs中的前馈神经元和注意力神经元是专门化的；也就是说，对于特定任务，某些神经元比其他神经元更为关键。

    arXiv:2403.01267v1 Announce Type: cross  Abstract: Understanding and shaping the behaviour of Large Language Models (LLMs) is increasingly important as applications become more powerful and more frequently adopted. This paper introduces a machine unlearning method specifically designed for LLMs. We introduce a selective pruning method for LLMs that removes neurons based on their relative importance on a targeted capability compared to overall network performance. This approach is a compute- and data-efficient method for identifying and removing neurons that enable specific behaviours. Our findings reveal that both feed-forward and attention neurons in LLMs are specialized; that is, for specific tasks, certain neurons are more crucial than others.
    
[^154]: SceneCraft：一个用于将文本描述合成为Blender代码的LLM代理

    SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code

    [https://arxiv.org/abs/2403.01248](https://arxiv.org/abs/2403.01248)

    SceneCraft是一个LLM代理，可将文本描述转换为Blender代码，实现渲染高达一百个三维资产的复杂场景，通过先建模空间关系再编写Python脚本，并借助视觉-语言基础模型进行场景优化和库学习来解决挑战。

    

    本文介绍了SceneCraft，一个大型语言模型（LLM）代理，将文本描述转换为Blender可执行的Python脚本，用于渲染高达一百个三维资产的复杂场景。该过程需要复杂的空间规划和布局。我们通过高级抽象、战略规划和库学习的组合来解决这些挑战。SceneCraft首先将场景图建模为蓝图，详细描述场景中各资产之间的空间关系。然后，SceneCraft根据这个图编写Python脚本，将关系转化为资产布局的数值约束。接下来，SceneCraft利用像GPT-V这样的视觉-语言基础模型的感知优势来分析渲染图像并迭代地优化场景。在这个过程之上，SceneCraft具备一个库学习机制，将常见的脚本函数编译为可重复使用的库，促进持续的自我

    arXiv:2403.01248v1 Announce Type: cross  Abstract: This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting text descriptions into Blender-executable Python scripts which render complex scenes with up to a hundred 3D assets. This process requires complex spatial planning and arrangement. We tackle these challenges through a combination of advanced abstraction, strategic planning, and library learning. SceneCraft first models a scene graph as a blueprint, detailing the spatial relationships among assets in the scene. SceneCraft then writes Python scripts based on this graph, translating relationships into numerical constraints for asset layout. Next, SceneCraft leverages the perceptual strengths of vision-language foundation models like GPT-V to analyze rendered images and iteratively refine the scene. On top of this process, SceneCraft features a library learning mechanism that compiles common script functions into a reusable library, facilitating continuous self
    
[^155]: AcME-AD: 用于异常检测的加速模型解释

    AcME-AD: Accelerated Model Explanations for Anomaly Detection

    [https://arxiv.org/abs/2403.01245](https://arxiv.org/abs/2403.01245)

    AcME-AD是一种根植于可解释人工智能原则的新方法，通过提供局部特征重要性评分和假设分析等方法，超越了模型特定或资源密集型的可解释性技术的限制。

    

    为了在异常检测中追求快速和稳健的解释性至关重要，传统异常检测方法在异常识别方面表现出色，但往往是黑盒子，提供对其决策过程的洞察有限。缺乏透明度影响了它们的可靠性，并阻碍了它们在需要理解异常检测原因至关重要的情况下的采用。同时，在实际场景中要迅速获得解释是至关重要的。为此，我们提出AcME-AD，一种根植于可解释人工智能原则的新方法，旨在澄清适用于表格数据的异常检测模型。 AcME-AD通过提供局部特征重要性评分和假设分析等方法，超越了模型特定或资源密集型的可解释性技术的限制，提供了一种面向模型的、高效的解决方案，以促进可解释性。

    arXiv:2403.01245v1 Announce Type: new  Abstract: Pursuing fast and robust interpretability in Anomaly Detection is crucial, especially due to its significance in practical applications. Traditional Anomaly Detection methods excel in outlier identification but are often black-boxes, providing scant insights into their decision-making process. This lack of transparency compromises their reliability and hampers their adoption in scenarios where comprehending the reasons behind anomaly detection is vital. At the same time, getting explanations quickly is paramount in practical scenarios. To bridge this gap, we present AcME-AD, a novel approach rooted in Explainable Artificial Intelligence principles, designed to clarify Anomaly Detection models for tabular data. AcME-AD transcends the constraints of model-specific or resource-heavy explainability techniques by delivering a model-agnostic, efficient solution for interoperability. It offers local feature importance scores and a what-if analy
    
[^156]: 用机器学习增强自动化：基于意图的用户指令分类

    Augmenting Automation: Intent-Based User Instruction Classification with Machine Learning

    [https://arxiv.org/abs/2403.01242](https://arxiv.org/abs/2403.01242)

    提出了一种通过引入基于意图的用户指令分类和机器学习技术的新颖方法，从而增强自动化系统的灵活性和适应性。

    

    电动自动化系统在控制电路和设备时提供了方便和效率。传统上，这些系统依赖预定义的命令进行控制，限制了灵活性和适应性。本文提出了一种新颖的方法，通过引入基于意图的用户指令分类和机器学习技术来增强自动化。我们的系统将用户指令表示为意图，允许在不依赖预定义命令的情况下动态控制电路。通过训练在标记的用户指令数据集上的机器学习模型，我们的系统可以从用户输入中对意图进行分类，从而实现更直观和可适应的控制方案。我们展示了基于意图的电动自动化系统的设计和实现，详细说明了用于意图分类的机器学习模型的开发。实验结果证明了我们方法的有效性。

    arXiv:2403.01242v1 Announce Type: cross  Abstract: Electric automation systems offer convenience and efficiency in controlling electrical circuits and devices. Traditionally, these systems rely on predefined commands for control, limiting flexibility and adaptability. In this paper, we propose a novel approach to augment automation by introducing intent-based user instruction classification using machine learning techniques. Our system represents user instructions as intents, allowing for dynamic control of electrical circuits without relying on predefined commands. Through a machine learning model trained on a labeled dataset of user instructions, our system classifies intents from user input, enabling a more intuitive and adaptable control scheme. We present the design and implementation of our intent-based electric automation system, detailing the development of the machine learning model for intent classification. Experimental results demonstrate the effectiveness of our approach i
    
[^157]: 活跃深度核学习分子功能性：实现动态结构嵌入

    Active Deep Kernel Learning of Molecular Functionalities: Realizing Dynamic Structural Embeddings

    [https://arxiv.org/abs/2403.01234](https://arxiv.org/abs/2403.01234)

    本文提出了一种利用深度核学习（DKL）的活跃学习方法，通过与传统变分自动编码器（VAEs）的对比分析，创造了优先考虑分子功能性的潜在空间，并且通过迭代重新计算嵌入向量实现了更好组织的潜在空间。

    

    探索分子空间对于推进我们对化学性质和反应的理解至关重要，从而在材料科学、医学和能源领域取得突破性创新。本文探讨了一种利用深度核学习（DKL）进行分子发现的主动学习方法，这是一种超越传统变分自动编码器（VAEs）限制的新方法。使用QM9数据集，我们将DKL与传统VAEs进行对比，后者基于相似性分析分子结构，揭示了由于潜在空间中的稀疏规律性而存在的局限性。然而，DKL通过将结构与性质相关联，创造了优先考虑分子功能性的潜在空间，提供了更全面的视角。这是通过迭代重新计算嵌入向量来实现的，与目标性质的实验可用性保持一致。由此产生的潜在空间不仅组织更好，而且具有独特特性。

    arXiv:2403.01234v1 Announce Type: new  Abstract: Exploring molecular spaces is crucial for advancing our understanding of chemical properties and reactions, leading to groundbreaking innovations in materials science, medicine, and energy. This paper explores an approach for active learning in molecular discovery using Deep Kernel Learning (DKL), a novel approach surpassing the limits of classical Variational Autoencoders (VAEs). Employing the QM9 dataset, we contrast DKL with traditional VAEs, which analyze molecular structures based on similarity, revealing limitations due to sparse regularities in latent spaces. DKL, however, offers a more holistic perspective by correlating structure with properties, creating latent spaces that prioritize molecular functionality. This is achieved by recalculating embedding vectors iteratively, aligning with the experimental availability of target properties. The resulting latent spaces are not only better organized but also exhibit unique characteri
    
[^158]: Polynormer: 多项式表达的线性时间图转换器

    Polynormer: Polynomial-Expressive Graph Transformer in Linear Time

    [https://arxiv.org/abs/2403.01232](https://arxiv.org/abs/2403.01232)

    Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。

    

    图转换器（GTs）已经成为一种有前途的架构，理论上它比消息传递图神经网络（GNNs）更具表现力。然而，典型的GT模型至少具有二次复杂度，因此无法扩展到大型图。虽然最近提出了几种线性GTs，但它们在几个热门图数据集上仍落后于GNN对应模型，这对于它们的实际表现力构成了一个重要关注点。为了平衡GTs的表现力和可扩展性之间的权衡，我们提出了Polynormer，一个具有线性复杂度的多项式表达GT模型。Polynormer构建在一个新颖的基础模型上，该模型在输入特征上学习高次多项式。为了使基础模型具有置换等变性，我们将其与图拓扑和节点特征分开集成，从而产生本地和全局等变关注模型。因此，Polynormer采用了线性的局部到全局关注方案。

    arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
    
[^159]: REWIND数据集：在野外多模态身体运动信号中隐私保护的语音状态分割

    REWIND Dataset: Privacy-preserving Speaking Status Segmentation from Multimodal Body Movement Signals in the Wild

    [https://arxiv.org/abs/2403.01229](https://arxiv.org/abs/2403.01229)

    通过视频和可穿戴传感器数据训练的机器学习模型可以隐私保护地识别说话状态，解决了在野外获取个人录音困难的问题

    

    识别人类的说话是理解社会互动的一个核心任务。通常情况下，会从个人录音中检测说话，就像之前为会议场景所做的那样。然而，在拥挤的聚会场景中，由于成本、后勤和隐私问题，很难获取个人录音。作为一种替代方案，通过训练在视频和可穿戴传感器数据上的机器学习模型可以实现通过检测其相关手势来识别语音，这种方式既不引人注目又保护隐私。然而，这些模型本身理想情况下应该使用从语音信号中获取的标签进行训练。然而，现有的聚会数据集中没有包含高质量的音频记录。相反，对说话状态的注释通常是通过人类标注者从视频中推断出来的，而没有对这种方法针对基于音频的地面真实性进行验证。本文重新审视了非音频说话状态标签

    arXiv:2403.01229v1 Announce Type: cross  Abstract: Recognizing speaking in humans is a central task towards understanding social interactions. Ideally, speaking would be detected from individual voice recordings, as done previously for meeting scenarios. However, individual voice recordings are hard to obtain in the wild, especially in crowded mingling scenarios due to cost, logistics, and privacy concerns. As an alternative, machine learning models trained on video and wearable sensor data make it possible to recognize speech by detecting its related gestures in an unobtrusive, privacy-preserving way. These models themselves should ideally be trained using labels obtained from the speech signal. However, existing mingling datasets do not contain high quality audio recordings. Instead, speaking status annotations have often been inferred by human annotators from video, without validation of this approach against audio-based ground truth. In this paper we revisit no-audio speaking statu
    
[^160]: 一种用于成本效率多实例反事实解释的两阶段算法

    A Two-Stage Algorithm for Cost-Efficient Multi-instance Counterfactual Explanations

    [https://arxiv.org/abs/2403.01221](https://arxiv.org/abs/2403.01221)

    本文提出了一种两阶段算法，用于找到实例组以及成本有效的多实例反事实解释，填补了先前工作中未解决的空白。

    

    反事实解释是分析黑盒系统预测结果的最流行方法之一，因为它可以推荐成本有效且可操作的输入更改，将不良系统输出转变为期望输出。大多数现有的反事实方法解释单个实例，但一些真实的用例（如客户满意度）需要识别能同时满足多个实例（例如客户）的单一反事实。在这项工作中，我们提出了一种灵活的两阶段算法，用于找到实例组以及成本有效的多实例反事实解释。这是因为在大多数先前的工作中，找到这样的实例组并未得到充分解决的。

    arXiv:2403.01221v1 Announce Type: cross  Abstract: Counterfactual explanations constitute among the most popular methods for analyzing the predictions of black-box systems since they can recommend cost-efficient and actionable changes to the input to turn an undesired system's output into a desired output. While most of the existing counterfactual methods explain a single instance, several real-world use cases, such as customer satisfaction, require the identification of a single counterfactual that can satisfy multiple instances (e.g. customers) simultaneously. In this work, we propose a flexible two-stage algorithm for finding groups of instances along with cost-efficient multi-instance counterfactual explanations. This is motivated by the fact that in most previous works the aspect of finding such groups is not addressed.
    
[^161]: 粗糙反学习需要更加谨慎的评估以避免虚假隐私感知

    Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy

    [https://arxiv.org/abs/2403.01218](https://arxiv.org/abs/2403.01218)

    论文讨论了针对反学习环境的成员推断攻击的调整，并提出了现有U-MIA的分类，对每个示例实例化了专用攻击者。

    

    模型训练的高成本使得开发反学习技术变得越来越有吸引力。这些技术旨在删除训练样本的影响，而无需从头重新训练模型。从直觉上讲，一旦模型完成反学习，与该模型交互的对手就不应再能够判断反学习的样本是否包含在模型的训练集中。在隐私领域，这被称为成员推断。在这项工作中，我们讨论了成员推断攻击（MIAs）对反学习设置的调整（导致它们的“U-MIA”对应）。我们提出了现有U-MIA的分类，将其分为“人口U-MIA”，其中同一攻击者适用于所有示例，和“每个示例U-MIA”，其中为每个示例实例化了专用攻击者。我们展示了后一类别，在这种情况下，攻击者为每个实例定制其成员预测。

    arXiv:2403.01218v1 Announce Type: new  Abstract: The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each ex
    
[^162]: API就够了：无需对数访问的大型语言模型的整体预测

    API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access

    [https://arxiv.org/abs/2403.01216](https://arxiv.org/abs/2403.01216)

    本研究提出了一种针对无需访问对数的API-only LLMs的整体预测方法，旨在最小化预测集大小并确保用户定义的覆盖范围的统计保证。

    

    本研究旨在解决无法访问对数时如何量化大型语言模型（LLMs）中的不确定性这一普遍挑战。整体预测（CP）以其与模型无关和无需分布的特点而闻名，是各种LLMs和数据分布的理想方法。然而，现有的LLMs整体预测方法通常假定可以访问对数，这对于一些仅支持API的LLMs来说是不可用的。此外，已知对数可能存在校准不准确的问题，可能导致整体预测性能下降。为了应对这些挑战，我们提出一种新颖的CP方法，（1）专为无需对数访问的API-only LLMs量身定制; (2) 最小化预测集的大小; 以及(3)确保用户定义的覆盖范围具有统计保证。该方法的核心思想是利用粗粒度（例如，样本频率）和细粒度不确定性概念（例如，语义相似性）来制定不一致性度量。实验结果表明，

    arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
    
[^163]: 具有Massart噪声的流式线性和修正线性系统的随机梯度下降

    Stochastic gradient descent for streaming linear and rectified linear systems with Massart noise

    [https://arxiv.org/abs/2403.01204](https://arxiv.org/abs/2403.01204)

    我们提出了一种针对具有Massart噪声的线性和ReLU回归问题的随机梯度下降方法，具有新颖的近乎线性收敛保证，首次在流式设置中为鲁棒ReLU回归提供了收敛保证，并展示了其相比于以前的方法有改进的收敛速率。

    

    我们提出了SGD-exp，一种用于线性和ReLU回归的随机梯度下降方法，在Massart噪声（对抗性半随机破坏模型）下，完全流式设置下。我们展示了SGD-exp对真实参数的近乎线性收敛保证，最高可达50%的Massart破坏率，在对称无忧破坏情况下，任意破坏率也有保证。这是流式设置中鲁棒ReLU回归的第一个收敛保证结果，它显示了相比于以前的鲁棒方法对于L1线性回归具有改进的收敛速率，这是由于选择了指数衰减步长，这在实践中已被证明是有效的。我们的分析基于离散随机过程的漂移分析，这本身也可能是有趣的。

    arXiv:2403.01204v1 Announce Type: new  Abstract: We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of SGD-exp to the true parameter with up to $50\%$ Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for $L_1$ linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete stochastic process, which could also be interesting on its own.
    
[^164]: 伪标签校准半监督多模态实体对齐

    Pseudo-Label Calibration Semi-supervised Multi-Modal Entity Alignment

    [https://arxiv.org/abs/2403.01203](https://arxiv.org/abs/2403.01203)

    本研究提出了一种伪标签校准的半监督多模态实体对齐方法，通过引入互信息最大化来过滤模态特定噪音，增强模态不变性共性。

    

    多模态实体对齐(MMEA)旨在识别两个多模态知识图之间的等价实体，以进行整合。不幸的是，之前的研究试图改进多模态信息的交互和融合，却忽视了模态特定噪音在半监督设置中标记和未标记数据的影响。在这项工作中，我们介绍了一种半监督的伪标签校准多模态实体对齐(PCMEA)方法。具体来说，为了生成全面的实体表示，我们首先设计了各种嵌入模块和注意机制来提取视觉、结构、关系和属性特征。接着，我们提出利用互信息最大化来过滤模态特定噪音并增强模态不变性共性。然后，我们将伪标签校准与基于动量的对比融合方法结合起来。

    arXiv:2403.01203v1 Announce Type: cross  Abstract: Multi-modal entity alignment (MMEA) aims to identify equivalent entities between two multi-modal knowledge graphs for integration. Unfortunately, prior arts have attempted to improve the interaction and fusion of multi-modal information, which have overlooked the influence of modal-specific noise and the usage of labeled and unlabeled data in semi-supervised settings. In this work, we introduce a Pseudo-label Calibration Multi-modal Entity Alignment (PCMEA) in a semi-supervised way. Specifically, in order to generate holistic entity representations, we first devise various embedding modules and attention mechanisms to extract visual, structural, relational, and attribute features. Different from the prior direct fusion methods, we next propose to exploit mutual information maximization to filter the modal-specific noise and to augment modal-invariant commonality. Then, we combine pseudo-label calibration with momentum-based contrastive
    
[^165]: 一种用于大规模全局优化的复合分解方法

    A Composite Decomposition Method for Large-Scale Global Optimization

    [https://arxiv.org/abs/2403.01192](https://arxiv.org/abs/2403.01192)

    本文提出了一种复合分解方法，将差分分组和一般分组方法整合到一个框架中，通过逐步分解准确分解各种问题类型，降低了计算复杂性。

    

    合作协同进化（CC）算法基于分而治之策略，已成为解决大规模全局优化（LSGO）问题的主要方法。分组阶段的效率和准确性显著影响优化过程的性能。一般可分离分组（GSG）方法克服了以往差分分组方法的局限性，使得非可加分离函数的分解成为可能，但其存在较高的计算复杂性。为了解决这一挑战，本文提出了一种复合可分离分组（CSG）方法，将DG和GSG无缝整合到一个问题分解框架中，以利用两种方法的优势。CSG引入了一个逐步分解框架，可以使用更少的计算资源准确分解各种问题类型。

    arXiv:2403.01192v1 Announce Type: cross  Abstract: Cooperative co-evolution (CC) algorithms, based on the divide-and-conquer strategy, have emerged as the predominant approach to solving large-scale global optimization (LSGO) problems. The efficiency and accuracy of the grouping stage significantly impact the performance of the optimization process. While the general separability grouping (GSG) method has overcome the limitation of previous differential grouping (DG) methods by enabling the decomposition of non-additively separable functions, it suffers from high computational complexity. To address this challenge, this article proposes a composite separability grouping (CSG) method, seamlessly integrating DG and GSG into a problem decomposition framework to utilize the strengths of both approaches. CSG introduces a step-by-step decomposition framework that accurately decomposes various problem types using fewer computational resources. By sequentially identifying additively, multiplic
    
[^166]: 从偏倚数据集中训练无偏扩散模型

    Training Unbiased Diffusion Models From Biased Dataset

    [https://arxiv.org/abs/2403.01189](https://arxiv.org/abs/2403.01189)

    提出了一种基于时间的重要性重赋权方法，以减轻扩散模型的偏差，并通过时间依赖密度比实现更准确的重赋权，从而最小化生成学习中的误差传播。

    

    随着扩散模型的重大进展，解决数据集偏差的潜在风险变得越来越重要。由于生成的输出直接受到数据集偏差的影响，减轻潜在偏差成为改善样本质量和比例的关键因素。本文提出了一种基于时间的重要性重赋权方法来减轻扩散模型的偏差。我们证明了时间依赖密度比比以前的方法更精确，从而最小化了生成学习中的误差传播。虽然直接将其应用于得分匹配是不可行的，但我们发现使用时间依赖密度比既进行重新赋权又进行得分校正可以导致一种可解的目标函数形式，来重新生成无偏数据密度。此外，我们在理论上建立了与传统得分匹配的联系，并证明了它收敛到一个无偏分布。

    arXiv:2403.01189v1 Announce Type: new  Abstract: With significant advancements in diffusion models, addressing the potential risks of dataset bias becomes increasingly important. Since generated outputs directly suffer from dataset bias, mitigating latent bias becomes a key factor in improving sample quality and proportion. This paper proposes time-dependent importance reweighting to mitigate the bias for the diffusion models. We demonstrate that the time-dependent density ratio becomes more precise than previous approaches, thereby minimizing error propagation in generative learning. While directly applying it to score-matching is intractable, we discover that using the time-dependent density ratio both for reweighting and score correction can lead to a tractable form of the objective function to regenerate the unbiased data density. Furthermore, we theoretically establish a connection with traditional score-matching, and we demonstrate its convergence to an unbiased distribution. The
    
[^167]: 利用自监督学习进行儿童性虐待图像场景识别

    Leveraging Self-Supervised Learning for Scene Recognition in Child Sexual Abuse Imagery

    [https://arxiv.org/abs/2403.01183](https://arxiv.org/abs/2403.01183)

    利用自监督学习技术，本文提出了一种能够安全高效处理儿童性虐待图像数据的场景识别方法。

    

    21世纪的犯罪分为虚拟和真实世界。然而，前者已经成为对后者人们福祉和安全构成全球威胁。它提出的挑战必须通过统一的全球合作来面对，我们必须比以往更加依赖自动化但值得信赖的工具来应对网络犯罪日益增长的本质。每年有超过1000万起儿童性虐待报告提交给美国国家失踪和被剥削儿童中心，超过80%来自网络来源。因此，调查中心和清除中心无法手动处理和正确调查所有图像。基于此，能够安全高效处理这些数据的可靠自动化工具至关重要。在这方面，场景识别任务寻找环境中的上下文线索，能够组织和分类儿童性虐待数据，而无需在敏感数据上进行训练。

    arXiv:2403.01183v1 Announce Type: cross  Abstract: Crime in the 21st century is split into a virtual and real world. However, the former has become a global menace to people's well-being and security in the latter. The challenges it presents must be faced with unified global cooperation, and we must rely more than ever on automated yet trustworthy tools to combat the ever-growing nature of online offenses. Over 10 million child sexual abuse reports are submitted to the US National Center for Missing & Exploited Children every year, and over 80% originated from online sources. Therefore, investigation centers and clearinghouses cannot manually process and correctly investigate all imagery. In light of that, reliable automated tools that can securely and efficiently deal with this data are paramount. In this sense, the scene recognition task looks for contextual cues in the environment, being able to group and classify child sexual abuse data without requiring to be trained on sensitive 
    
[^168]: 一种基于贝叶斯委员会机制的含氧有机化合物潜势

    A Bayesian Committee Machine Potential for Oxygen-containing Organic Compounds

    [https://arxiv.org/abs/2403.01158](https://arxiv.org/abs/2403.01158)

    本研究介绍了一种基于贝叶斯委员会机制的 BCM 潜势，可用于预测含氧有机化合物，具有高效的可扩展结构和适应性，在处理大型数据集时表现出色。

    

    理解含氧有机化合物在为生物体提供能量来源和促进蛋白质形成方面的关键作用对生物化学领域至关重要。本研究解决了理解蛋白质-蛋白质相互作用（PPI）和开发蛋白质和有机化合物的预测模型的挑战，特别关注量化它们的结合亲和力。在这里，我们介绍了一种特别设计用于预测八组CHO内含氧有机化合物的主动贝叶斯委员会机潜势（BCM）。BCM潜势采用基于委员会的方法来解决与核回归器相关的可扩展性问题，特别是在处理大型数据集时。其可适应性结构可以实现高效和具有成本效益的扩展，保持转移性和可扩展性。通过系统化基准测试，我们将稀疏的BCM潜势定位为

    arXiv:2403.01158v1 Announce Type: cross  Abstract: Understanding the pivotal role of oxygen-containing organic compounds in serving as an energy source for living organisms and contributing to protein formation is crucial in the field of biochemistry. This study addresses the challenge of comprehending protein-protein interactions (PPI) and developing predicitive models for proteins and organic compounds, with a specific focus on quantifying their binding affinity. Here, we introduce the active Bayesian Committee Machine (BCM) potential, specifically designed to predict oxygen-containing organic compounds within eight groups of CHO. The BCM potential adopts a committee-based approach to tackle scalability issues associated with kernel regressors, particularly when dealing with large datasets. Its adaptable structure allows for efficient and cost-effective expansion, maintaing both transferability and scalability. Through systematic benchmarking, we position the sparse BCM potential as 
    
[^169]: 基于生成对抗网络和Transformer模型的交通事故检测混合模型

    A Hybrid Model for Traffic Incident Detection based on Generative Adversarial Networks and Transformer Model

    [https://arxiv.org/abs/2403.01147](https://arxiv.org/abs/2403.01147)

    提出了一种结合Transformer和生成对抗网络的混合模型来提高交通事故检测的效果，并通过扩展数据集和实现平衡比例进行了验证

    

    除了增强交通安全并促进及时应急响应外，交通事故检测通过提供实时交通状态信息，在智能交通系统中起着不可或缺的作用。先前的研究发现，除了采用先进的算法模型外，检测的有效性还受到获取大型数据集和解决数据集不平衡等挑战的显著影响。提出了一种结合Transformer和生成对抗网络（GANs）的混合模型来解决这些挑战。实验证实了Transformer在交通事故检测中的优越性。此外，利用GANs扩展数据集，实现1:4、2:3和1:1的平衡比。该模型针对基准模型进行了评估。

    arXiv:2403.01147v1 Announce Type: cross  Abstract: In addition to enhancing traffic safety and facilitating prompt emergency response, traffic incident detection plays an indispensable role in intelligent transportation systems by providing real-time traffic status information. This enables the realization of intelligent traffic control and management. Previous research has identified that apart from employing advanced algorithmic models, the effectiveness of detection is also significantly influenced by challenges related to acquiring large datasets and addressing dataset imbalances. A hybrid model combining transformer and generative adversarial networks (GANs) is proposed to address these challenges. Experiments are conducted on four real datasets to validate the superiority of the transformer in traffic incident detection. Additionally, GANs are utilized to expand the dataset and achieve a balanced ratio of 1:4, 2:3, and 1:1. The proposed model is evaluated against the baseline mod
    
[^170]: 在具有相位感知分区和自适应量化的异构集群上提供LLM-PQ

    LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization

    [https://arxiv.org/abs/2403.01136](https://arxiv.org/abs/2403.01136)

    这项研究提出了LLM-PQ系统，通过采用自适应模型量化和相位感知分区，在异构GPU集群上提高了LLM服务的效率。

    

    最近大规模语言模型（LLMs）的突破性进展在各种任务上展示了令人印象深刻的性能。LLMs的巨大规模导致了非常高的资源需求和成本。尽管目前主要使用统一高性能GPU来服务这些模型，但利用一种混合可用高低容量GPU的异构集群可能会大幅降低服务成本。然而，目前缺乏支持使用异构集群高效提供LLM服务的设计，而当前的解决方案主要集中在模型分区和均匀压缩在同质设备之间。本文提出了LLM-PQ，这是一个倡导自适应模型量化和相位感知分区以提高异构GPU集群上LLM服务效率的系统。我们在分布式LLM服务中仔细选择了混合精度模型量化、相位感知模型分区和微批量大小。

    arXiv:2403.01136v1 Announce Type: cross  Abstract: Recent breakthroughs in Large-scale language models (LLMs) have demonstrated impressive performance on various tasks. The immense sizes of LLMs have led to very high resource demand and cost for running the models. Though the models are largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous cluster with a mix of available high- and low-capacity GPUs can potentially substantially reduce the serving cost. There is a lack of designs to support efficient LLM serving using a heterogeneous cluster, while the current solutions focus on model partition and uniform compression among homogeneous devices. This paper proposes LLM-PQ, a system that advocates adaptive model quantization and phase-aware partition to improve LLM serving efficiency on heterogeneous GPU clusters. We carefully decide on mixed-precision model quantization together with phase-aware model partition and micro-batch sizing in distributed LLM servin
    
[^171]: 评估大型语言模型作为时间序列物理感应数据的虚拟标注器

    Evaluating Large Language Models as Virtual Annotators for Time-series Physical Sensing Data

    [https://arxiv.org/abs/2403.01133](https://arxiv.org/abs/2403.01133)

    大型语言模型（LLMs）作为虚拟标注器，直接使用原始传感器数据进行标注，可能解决传统人机协作标注时间序列数据的一系列问题。

    

    传统的人机协作标注时间序列数据（如惯性数据）通常需要访问来自环境的视频或音频等其他模态。这些备用来源为人类标注者提供必要的信息，因为原始数字数据对于专家来说通常过于难以理解。然而，这种传统方法存在许多关于总体成本、效率、额外模态的存储、时间、可扩展性和隐私的问题。有趣的是，最近的大型语言模型（LLMs）也是通过大量公开可用的字母数字数据进行训练的，这使它们能够理解并在自然语言处理以外的任务上表现良好。自然地，这为探索LLMs作为虚拟标注器开辟了潜在途径，其中LLMs将直接提供原始传感器数据进行标注，而不依赖于任何备用模态。这可能有助于解决成本、效率、存储、时间、可扩展性和隐私等问题。

    arXiv:2403.01133v1 Announce Type: new  Abstract: Traditional human-in-the-loop-based annotation for time-series data like inertial data often requires access to alternate modalities like video or audio from the environment. These alternate sources provide the necessary information to the human annotator, as the raw numeric data is often too obfuscated even for an expert. However, this traditional approach has many concerns surrounding overall cost, efficiency, storage of additional modalities, time, scalability, and privacy. Interestingly, recent large language models (LLMs) are also trained with vast amounts of publicly available alphanumeric data, which allows them to comprehend and perform well on tasks beyond natural language processing. Naturally, this opens up a potential avenue to explore LLMs as virtual annotators where the LLMs will be directly provided the raw sensor data for annotation instead of relying on any alternate modality. Naturally, this could mitigate the problems 
    
[^172]: MPIPN: 用于求解参数声学-结构系统的多物理信息PointNet

    MPIPN: A Multi Physics-Informed PointNet for solving parametric acoustic-structure systems

    [https://arxiv.org/abs/2403.01132](https://arxiv.org/abs/2403.01132)

    本文提出了基于深度学习的多物理信息PointNet（MPIPN）用于解决参数化声学-结构系统，通过增强的点云架构和提取局部全局特征来解决涉及显式和隐式物理量的问题。

    

    机器学习被用于求解由一般非线性偏微分方程控制的物理系统。然而，诸如声学-结构耦合之类的复杂多物理系统通常由一系列包含变量物理量的偏微分方程描述，这被称为参数化系统。目前缺乏解决涉及显式和隐式量的参数化系统的PDE策略。本文提出了一种基于深度学习的多物理信息PointNet（MPIPN），用于解决参数化声学-结构系统。首先，MPIPN引入了增强的点云架构，包括计算域的显式物理量和几何特征。然后，MPIPN提取重构点云的局部和全局特征作为解决参数化系统的一部分标准。此外，隐式物理量被嵌入其中。

    arXiv:2403.01132v1 Announce Type: new  Abstract: Machine learning is employed for solving physical systems governed by general nonlinear partial differential equations (PDEs). However, complex multi-physics systems such as acoustic-structure coupling are often described by a series of PDEs that incorporate variable physical quantities, which are referred to as parametric systems. There are lack of strategies for solving parametric systems governed by PDEs that involve explicit and implicit quantities. In this paper, a deep learning-based Multi Physics-Informed PointNet (MPIPN) is proposed for solving parametric acoustic-structure systems. First, the MPIPN induces an enhanced point-cloud architecture that encompasses explicit physical quantities and geometric features of computational domains. Then, the MPIPN extracts local and global features of the reconstructed point-cloud as parts of solving criteria of parametric systems, respectively. Besides, implicit physical quantities are embe
    
[^173]: LLaMoCo：用于优化代码生成的大型语言模型指令调优

    LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation

    [https://arxiv.org/abs/2403.01131](https://arxiv.org/abs/2403.01131)

    LLaMoCo是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架，通过全面指令集和新颖的两阶段学习策略，实现了优越的性能。

    

    最近的研究探讨了使用大型语言模型（LLMs）进行优化，方法包括从LLMs迭代地寻找下一步解决方案，或直接提示LLMs以获取优化器。然而，这些方法存在固有限制，包括操作效率低、对提示设计敏感度高以及缺乏领域特定知识。我们介绍了LLaMoCo，这是第一个旨在以代码对代码方式调整LLMs以解决优化问题的指令调优框架。具体地，我们建立了一个包含清晰描述的问题提示和有效优化代码的全面指令集。然后我们开发了一种新颖的两阶段学习策略，在指令调优阶段之前，该策略整合了基于对比学习的热身过程，以增强模型微调期间的收敛行为。实验结果表明，通过我们的LLaMoCo精调的CodeGen（350M）模型达到了卓越的性能。

    arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
    
[^174]: 损失景观的灵敏度分析

    Sensitivity Analysis On Loss Landscape

    [https://arxiv.org/abs/2403.01128](https://arxiv.org/abs/2403.01128)

    利用一、二和三阶导数进行损失景观分析，发现了与Spearman秩相关系数类似可视化的信息，以及损失函数和激活函数结合带来的非线性模式。

    

    梯度可用于灵敏度分析。在这里，我们利用损失景观的优势，了解哪些自变量影响因变量。我们通过自动微分利用一阶、二阶和三阶导数来理解损失景观。我们知道Spearman秩相关系数可以检测两个变量之间的单调关系。然而，我发现在特定配置和参数下，二阶梯度提供的信息可以类似于Spearman的结果进行可视化。在我们的方法中，我们将损失函数与激活函数结合，导致非线性模式。通过重新训练对损失景观的每次探索都提供新的有价值信息。此外，一阶和三阶导数也很有用，因为它们显示了自变量对因变量的影响程度。

    arXiv:2403.01128v1 Announce Type: new  Abstract: Gradients can be employed for sensitivity analysis. Here, we leverage the advantages of the Loss Landscape to comprehend which independent variables impact the dependent variable. We seek to grasp the loss landscape by utilizing first, second, and third derivatives through automatic differentiation. we know that Spearman's rank correlation coefficient can detect the monotonic relationship between two variables. However, I have found that second-order gradients, with certain configurations and parameters, provide information that can be visualized similarly to Spearman's results.In our approach, we incorporate a loss function with an activation function, resulting in a non-linear pattern. Each exploration of the loss landscape through retraining yields new valuable information. Furthermore, the first and third derivatives are also beneficial, as they indicate the extent to which independent variables influence the dependent variable.
    
[^175]: OpenGraph: 迈向开放图基础模型

    OpenGraph: Towards Open Graph Foundation Models

    [https://arxiv.org/abs/2403.01121](https://arxiv.org/abs/2403.01121)

    该论文旨在通过开发一个通用图基础模型，以解决现有图神经网络在泛化到与训练数据显著不同的未见图数据时遇到的困难。

    

    arXiv:2403.01121v1 公告类型: 跨交互   摘要: 图学习已成为解释和利用各领域的关系数据的不可或缺部分，从推荐系统到社交网络分析。在这种背景下，各种GNN已经成为编码图的结构信息的有希望的方法论，通过有效地捕捉图的潜在结构，这些GNN已经展示出在增强图学习任务性能方面的巨大潜力，例如链接预测和节点分类。然而，尽管取得了成功，一个显著的挑战仍然存在: 这些先进方法通常在将显著不同于训练实例的未见图数据泛化时遇到困难。在这项工作中，我们的目标是通过开发一个通用图基础模型来推进图学习范式。该模型旨在理解多样图数据中存在的复杂拓扑模式，使其在零-shot情况下表现出色。

    arXiv:2403.01121v1 Announce Type: cross  Abstract: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot g
    
[^176]: 高效的合作多智能体强化学习的情节记忆利用

    Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning

    [https://arxiv.org/abs/2403.01112](https://arxiv.org/abs/2403.01112)

    提出了用于合作多智能体强化学习的高效情节记忆利用（EMU），利用语义一致内存加速学习，有选择地促进理想的转换，避免陷入局部最优解。

    

    在合作多智能体强化学习（MARL）中，智能体旨在实现共同目标，比如击败敌人或进球。现有的MARL算法虽然有效，但仍需要大量学习时间，通常会在复杂任务中陷入局部最优解，随后未能发现达成目标的策略。为了解决这一问题，我们引入了用于MARL的高效情节记忆利用（EMU），其两个主要目标是：（a）通过利用来自情节缓冲区的语义一致内存加速强化学习，以及（b）有选择地促进理想的转换以防止局部收敛。为实现（a），EMU在MARL旁引入了可训练的编码器/解码器结构，创建了有助于探索性内存回忆的连贯记忆嵌入。为实现（b），EMU引入了一种基于状态愿望性的新颖奖励结构，称为情节激励。这种奖励改善了TD

    arXiv:2403.01112v1 Announce Type: new  Abstract: In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD 
    
[^177]: 特征对齐：在预训练模型背景下通过代理思考高效主动学习

    Feature Alignment: Rethinking Efficient Active Learning via Proxy in the Context of Pre-trained Models

    [https://arxiv.org/abs/2403.01101](https://arxiv.org/abs/2403.01101)

    通过代理进行特征对齐，以解决预先计算特征无法区分标记样本类别和避免通过代理模型选择样本时牺牲宝贵预训练信息的问题。

    

    使用主动学习对预训练模型进行微调有望降低注释成本。然而，这种组合引入了显著的计算成本，尤其是随着预训练模型规模的增长。最近的研究提出了基于代理的主动学习，它预先计算特征以减少计算成本。然而，这种方法通常会在主动学习性能上造成重大损失，甚至可能超过计算成本节约。

    arXiv:2403.01101v1 Announce Type: cross  Abstract: Fine-tuning the pre-trained model with active learning holds promise for reducing annotation costs. However, this combination introduces significant computational costs, particularly with the growing scale of pre-trained models. Recent research has proposed proxy-based active learning, which pre-computes features to reduce computational costs. Yet, this approach often incurs a significant loss in active learning performance, which may even outweigh the computational cost savings. In this paper, we argue the performance drop stems not only from pre-computed features' inability to distinguish between categories of labeled samples, resulting in the selection of redundant samples but also from the tendency to compromise valuable pre-trained information when fine-tuning with samples selected through the proxy model. To address this issue, we propose a novel method called aligned selection via proxy to update pre-computed features while sele
    
[^178]: 一对一对齐改进图领域自适应

    Pairwise Alignment Improves Graph Domain Adaptation

    [https://arxiv.org/abs/2403.01092](https://arxiv.org/abs/2403.01092)

    Pair-Align方法通过一对一对齐来对抗图结构转移，使用边权重减轻条件结构转移（CSS）并调整分类损失以处理标签转移（LS），在处理图领域自适应中表现出优越性能。

    

    基于图的方法在许多现实世界应用中对相互连接的对象进行标签推断至关重要，然而，如果用于模型训练的图与用于测试的图有显著差异，往往会遇到泛化挑战。本工作深入探讨了图领域自适应（GDA）以解决图数据分布转移的独特复杂性，其中相互连接的数据点经历特征、标签和尤其是连接模式的转移。我们提出了一种新颖的、在理论上合理的方法，一对一对齐（Pair-Align），通过减轻条件结构转移（CSS）和标签转移（LS）来对抗图结构转移。Pair-Align使用边权重重新校准邻近节点之间的影响以处理CSS，并通过标签权重调整分类损失以处理LS。我们的方法在包括区域节点分类在内的现实世界应用中表现出优越的性能。

    arXiv:2403.01092v1 Announce Type: new  Abstract: Graph-based methods, pivotal for label inference over interconnected objects in many real-world applications, often encounter generalization challenges, if the graph used for model training differs significantly from the graph used for testing. This work delves into Graph Domain Adaptation (GDA) to address the unique complexities of distribution shifts over graph data, where interconnected data points experience shifts in features, labels, and in particular, connecting patterns. We propose a novel, theoretically principled method, Pairwise Alignment (Pair-Align) to counter graph structure shift by mitigating conditional structure shift (CSS) and label shift (LS). Pair-Align uses edge weights to recalibrate the influence among neighboring nodes to handle CSS and adjusts the classification loss with label weights to handle LS. Our method demonstrates superior performance in real-world applications, including node classification with region
    
[^179]: COOL：一种融合时空图神经网络用于交通预测的共同视角

    COOL: A Conjoint Perspective on Spatio-Temporal Graph Neural Network for Traffic Forecasting

    [https://arxiv.org/abs/2403.01091](https://arxiv.org/abs/2403.01091)

    本文提出了一种名为COOL的Conjoint Spatio-Temporal图神经网络，旨在共同捕捉交通预测中的高阶关系。

    

    本文研究交通预测，旨在根据历史情况预测交通的未来状态。鉴于其对多个场景的持续关注，并促进了许多下游应用程序的发展，例如城市规划和交通管理，该问题已经受到越来越多的关注。然而，由于现有方法倾向于独立地建模时空关系，因此未能充分考虑两者的复杂高阶互动，导致现有方法的效果不佳。此外，交通预测中的过渡模式的多样性使得现有方法难以捕捉，需要更深入地探索这种多样性。为此，本文提出了Conjoint Spatio-Temporal图神经网络（缩写为COOL），它从先前和后续信息中建模异构图，以共同捕捉高阶互动

    arXiv:2403.01091v1 Announce Type: cross  Abstract: This paper investigates traffic forecasting, which attempts to forecast the future state of traffic based on historical situations. This problem has received ever-increasing attention in various scenarios and facilitated the development of numerous downstream applications such as urban planning and transportation management. However, the efficacy of existing methods remains sub-optimal due to their tendency to model temporal and spatial relationships independently, thereby inadequately accounting for complex high-order interactions of both worlds. Moreover, the diversity of transitional patterns in traffic forecasting makes them challenging to capture for existing approaches, warranting a deeper exploration of their diversity. Toward this end, this paper proposes Conjoint Spatio-Temporal graph neural network (abbreviated as COOL), which models heterogeneous graphs from prior and posterior information to conjointly capture high-order sp
    
[^180]: LAB：针对ChatBots的大规模对齐

    LAB: Large-Scale Alignment for ChatBots

    [https://arxiv.org/abs/2403.01081](https://arxiv.org/abs/2403.01081)

    介绍了一种名为LAB的方法，旨在克服大型语言模型训练中的可扩展性挑战，通过分类法指导的合成数据生成和多阶段调整框架，实现了对昂贵人工标注和GPT-4等专有模型依赖较少的大规模对齐，提供了一种可扩展、具有成本效益的解决方案，不会出现灾难性遗忘情况，进一步增强了LLM的训练效率。

    

    这项工作介绍了LAB（ChatBots的大规模对齐），这是一种旨在克服大型语言模型（LLM）训练中指令调整阶段的可扩展性挑战的创新方法。通过利用基于分类法的合成数据生成过程和多阶段调整框架，LAB显著减少对昂贵的人类注释和诸如GPT-4之类的专有模型的依赖。我们证明，使用LAB训练的模型在几个基准测试中的性能可以与使用传统人类注释或GPT-4生成的合成数据训练的模型相比具有竞争力。因此，在不会出现灾难性遗忘的情况下，提供了一种可扩展、具有成本效益的解决方案，以增强LLM的能力和指令遵循行为，标志着在高效训练各种应用的LLM方面迈出了一步。

    arXiv:2403.01081v1 Announce Type: new  Abstract: This work introduces LAB (Large-scale Alignment for chatBots), a novel methodology designed to overcome the scalability challenges in the instruction-tuning phase of large language model (LLM) training. Leveraging a taxonomy-guided synthetic data generation process and a multi-phase tuning framework, LAB significantly reduces reliance on expensive human annotations and proprietary models like GPT-4. We demonstrate that LAB-trained models can achieve competitive performance across several benchmarks compared to models trained with traditional human-annotated or GPT-4 generated synthetic data. Thus offering a scalable, cost-effective solution for enhancing LLM capabilities and instruction-following behaviors without the drawbacks of catastrophic forgetting, marking a step forward in the efficient training of LLMs for a wide range of applications.
    
[^181]: 教授多层感知机更多图信息：三阶段多任务知识蒸馏框架

    Teaching MLP More Graph Information: A Three-stage Multitask Knowledge Distillation Framework

    [https://arxiv.org/abs/2403.01079](https://arxiv.org/abs/2403.01079)

    提出了一个新的三阶段多任务知识蒸馏框架，使用位置编码来捕捉位置信息，引入神经热核处理图数据，通过隐藏层输出匹配提高学生多层感知机的性能。

    

    我们研究了图神经网络在大规模图数据集上进行推理任务时面临的挑战：巨大的时间和内存消耗，并尝试通过减少对图结构的依赖来克服这一问题。尽管将图知识蒸馏到学生多层感知机是一个不错的想法，但它面临两个主要问题：位置信息丢失和泛化能力低。为了解决这些问题，我们提出了一种新的三阶段多任务蒸馏框架。具体地，我们使用位置编码来捕捉位置信息。此外，我们引入神经热核来负责图数据处理，在GNN中利用隐藏层输出匹配来提高学生多层感知机的性能。据我们所知，这是首次在图上引入隐藏层蒸馏用于学生多层感知机，并结合图位置编码和多层感知机。我们通过多种设置测试了其性能和稳健性，并得出结论......

    arXiv:2403.01079v1 Announce Type: cross  Abstract: We study the challenging problem for inference tasks on large-scale graph datasets of Graph Neural Networks: huge time and memory consumption, and try to overcome it by reducing reliance on graph structure. Even though distilling graph knowledge to student MLP is an excellent idea, it faces two major problems of positional information loss and low generalization. To solve the problems, we propose a new three-stage multitask distillation framework. In detail, we use Positional Encoding to capture positional information. Also, we introduce Neural Heat Kernels responsible for graph data processing in GNN and utilize hidden layer outputs matching for better performance of student MLP's hidden layers. To the best of our knowledge, it is the first work to include hidden layer distillation for student MLP on graphs and to combine graph Positional Encoding with MLP. We test its performance and robustness with several settings and draw the conc
    
[^182]: $\Gamma$-VAE: 曲率正则化变分自编码器，用于揭示高维数据中的新兴低维几何结构

    $\Gamma$-VAE: Curvature regularized variational autoencoders for uncovering emergent low dimensional geometric structure in high dimensional data

    [https://arxiv.org/abs/2403.01078](https://arxiv.org/abs/2403.01078)

    $\Gamma$-VAE通过正则化曲率来解决非线性降维技术中的两个限制，可以揭示高维数据中的新兴低维几何结构

    

    具有新兴行为的自然系统通常沿着高维空间中的低维子集进行组织。例如，尽管人类基因组中有数万个基因，但基因组学的原则研究富有成果，因为生物过程依赖于协调组织，从而产生较低维度的表型。为了揭示这种组织，许多非线性降维技术已成功地将高维数据嵌入到低维空间中，方法是保持数据点之间的局部相似性。然而，这些方法中的非线性性允许过多的曲率来保持跨多个非相邻数据集群的一般趋势，从而限制了它们对于超出分布数据的可解释性和泛化能力。在这里，我们通过规范化由变分自动编码器生成的流形的曲率来解决这两个限制，这一过程我们称之为“$\Gamma$-VAE”。

    arXiv:2403.01078v1 Announce Type: cross  Abstract: Natural systems with emergent behaviors often organize along low-dimensional subsets of high-dimensional spaces. For example, despite the tens of thousands of genes in the human genome, the principled study of genomics is fruitful because biological processes rely on coordinated organization that results in lower dimensional phenotypes. To uncover this organization, many nonlinear dimensionality reduction techniques have successfully embedded high-dimensional data into low-dimensional spaces by preserving local similarities between data points. However, the nonlinearities in these methods allow for too much curvature to preserve general trends across multiple non-neighboring data clusters, thereby limiting their interpretability and generalizability to out-of-distribution data. Here, we address both of these limitations by regularizing the curvature of manifolds generated by variational autoencoders, a process we coin ``$\Gamma$-VAE''.
    
[^183]: 通过不确定性量化从量化网络中提取可用预测以进行OOD检测

    Extracting Usable Predictions from Quantized Networks through Uncertainty Quantification for OOD Detection

    [https://arxiv.org/abs/2403.01076](https://arxiv.org/abs/2403.01076)

    通过Uncertainty Quantification技术从预训练的视觉模型中提取可用预测，有效忽略不自信的预测，节省了高达80%被误分类的样本。

    

    OOD检测随着网络设计的进步和任务复杂性的增加变得更加重要。确定给定网络误分类数据的部分变得与网络整体性能一样有价值。我们可以通过量化来压缩模型，但会有轻微性能损失。性能损失进一步需要从网络预测中推导出置信度估计。为此，我们引入了一种不确定性量化（UQ）技术，用于量化来自预先训练视觉模型的预测的不确定性。随后，我们利用这些信息提取有价值的预测，同时忽略不确定的预测。我们观察到，我们的技术可以将被误分类的样本的80%保存下来。同样的代码可在此处找到。

    arXiv:2403.01076v1 Announce Type: cross  Abstract: OOD detection has become more pertinent with advances in network design and increased task complexity. Identifying which parts of the data a given network is misclassifying has become as valuable as the network's overall performance. We can compress the model with quantization, but it suffers minor performance loss. The loss of performance further necessitates the need to derive the confidence estimate of the network's predictions. In line with this thinking, we introduce an Uncertainty Quantification(UQ) technique to quantify the uncertainty in the predictions from a pre-trained vision model. We subsequently leverage this information to extract valuable predictions while ignoring the non-confident predictions. We observe that our technique saves up to 80% of ignored samples from being misclassified. The code for the same is available here.
    
[^184]: GraphRCG: 通过自引导表示的自条件图生成

    GraphRCG: Self-conditioned Graph Generation via Bootstrapped Representations

    [https://arxiv.org/abs/2403.01071](https://arxiv.org/abs/2403.01071)

    提出了一种自条件图生成框架，通过自引导表示指导生成过程，明确建模和利用图分布，优于传统隐式捕获分布的方法。

    

    图生成通常旨在创建与特定图分布密切对齐的新图。现有研究往往通过生成器的优化隐式捕获这种分布，可能忽视分布本身的复杂性。此外，这些方法通常忽略了学习到的分布对图生成的见解。相比之下，在这项工作中，我们提出了一种新颖的自条件图生成框架，旨在明确建模图分布并利用这些分布来指导生成过程。我们首先进行自条件建模，通过将每个图样本转换为低维表示，并优化一个表示生成器来捕获图分布并生成反映学习分布的新表示。随后，我们利用这些自引导表示作为自条件指导来...

    arXiv:2403.01071v1 Announce Type: cross  Abstract: Graph generation generally aims to create new graphs that closely align with a specific graph distribution. Existing works often implicitly capture this distribution through the optimization of generators, potentially overlooking the intricacies of the distribution itself. Furthermore, these approaches generally neglect the insights offered by the learned distribution for graph generation. In contrast, in this work, we propose a novel self-conditioned graph generation framework designed to explicitly model graph distributions and employ these distributions to guide the generation process. We first perform self-conditioned modeling to capture the graph distributions by transforming each graph sample into a low-dimensional representation and optimizing a representation generator to create new representations reflective of the learned distribution. Subsequently, we leverage these bootstrapped representations as self-conditioned guidance f
    
[^185]: 连续零均值争议正则化模仿学习（CMZ-DRIL）

    Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL)

    [https://arxiv.org/abs/2403.01059](https://arxiv.org/abs/2403.01059)

    CMZ-DRIL是一种新颖的模仿学习方法，通过连续的零均值奖励函数和智能体集合之间的不确定性最小化，提高了只有少量专家演示的智能体性能。

    

    机器学习范式，如模仿学习和强化学习，可以在各种复杂环境中生成高性能智能体。然而，常用方法需要大量数据和/或已知的奖励函数。本文提出了一种称为连续零均值争议正则化模仿学习（CMZ-DRIL）的方法，它采用一种新颖的奖励结构来提高只有少量专家演示的模仿学习智能体的性能。CMZ-DRIL使用强化学习来最小化训练为模仿专家演示的智能体集合之间的不确定性。该方法不使用任何特定于环境的奖励，而是通过智能体集合的动作不一致性创建连续的零均值奖励函数。如在路径导航环境和两个MuJoCo环境中展示的，CMZ-DRIL可以生成具有优越性能的智能体。

    arXiv:2403.01059v1 Announce Type: new  Abstract: Machine-learning paradigms such as imitation learning and reinforcement learning can generate highly performant agents in a variety of complex environments. However, commonly used methods require large quantities of data and/or a known reward function. This paper presents a method called Continuous Mean-Zero Disagreement-Regularized Imitation Learning (CMZ-DRIL) that employs a novel reward structure to improve the performance of imitation-learning agents that have access to only a handful of expert demonstrations. CMZ-DRIL uses reinforcement learning to minimize uncertainty among an ensemble of agents trained to model the expert demonstrations. This method does not use any environment-specific rewards, but creates a continuous and mean-zero reward function from the action disagreement of the agent ensemble. As demonstrated in a waypoint-navigation environment and in two MuJoCo environments, CMZ-DRIL can generate performant agents that be
    
[^186]: 通过目标编码和分类损失的神经场分类器

    Neural Field Classifiers via Target Encoding and Classification Loss

    [https://arxiv.org/abs/2403.01058](https://arxiv.org/abs/2403.01058)

    本文提出了一个新颖的神经场分类器（NFC）框架，将现有的神经场方法规定为分类任务而不是回归任务，从而探讨了神经场方法中回归模型和分类模型的优劣。

    

    神经场方法在计算机视觉和计算机图形学中的各种长期任务中取得了巨大进展，包括新视图合成和几何重建。然而，现有神经场方法尝试预测一些基于坐标的连续目标值，如神经辐射场（NeRF）的RGB，所有这些方法都是回归模型，并通过一些回归损失进行优化。本文中，我们尝试从机器学习的视角讨论这个非常基本但被忽视的问题：对于神经场方法来说，回归模型真的比分类模型更好吗？我们成功地提出了一个新颖的神经场分类器（NFC）框架，将现有的神经场方法规定为分类任务而不是回归任务。所提出的NFC可以通过采用新颖的目标编码方法将任意神经场回归器（NFR）轻松转换为其分类变体。

    arXiv:2403.01058v1 Announce Type: cross  Abstract: Neural field methods have seen great progress in various long-standing tasks in computer vision and computer graphics, including novel view synthesis and geometry reconstruction. As existing neural field methods try to predict some coordinate-based continuous target values, such as RGB for Neural Radiance Field (NeRF), all of these methods are regression models and are optimized by some regression loss. However, are regression models really better than classification models for neural field methods? In this work, we try to visit this very fundamental but overlooked question for neural fields from a machine learning perspective. We successfully propose a novel Neural Field Classifier (NFC) framework which formulates existing neural field methods as classification tasks rather than regression tasks. The proposed NFC can easily transform arbitrary Neural Field Regressor (NFR) into its classification variant via employing a novel Target En
    
[^187]: 透过几何限制概率建模发现新生物医学概念

    Seeing Unseen: Discover Novel Biomedical Concepts via GeometryConstrained Probabilistic Modeling

    [https://arxiv.org/abs/2403.01053](https://arxiv.org/abs/2403.01053)

    提出了一种通过几何限制概率建模处理方法来解决生物医学数据中存在的非 i.i.d. 数据分布、类别不平衡等问题。

    

    arXiv:2403.01053v1 通告类型: 交叉  摘要: 机器学习以其数据驱动的特性，对科学发现的基本实践具有巨大的潜力改变。随着不断增加的研究数据收集，自动探索观测数据中的模式和见解，发现新的表型类别和概念将会变得更加吸引人。然而，在生物医学领域，累积数据中存在若干挑战，阻碍了新类发现的进展。非 i.i.d. 数据分布伴随着不同类别组之间的严重不平衡，本质上导致模糊和偏倚的语义表示。在这项工作中，我们提出了一种几何限制概率建模处理方法来解决所识别的问题。首先，我们建议将实例嵌入的近似后验参数化为边际 von Mises-Fisher 分布，以解决神经嵌入方案的模糊性与偏见性。

    arXiv:2403.01053v1 Announce Type: cross  Abstract: Machine learning holds tremendous promise for transforming the fundamental practice of scientific discovery by virtue of its data-driven nature. With the ever-increasing stream of research data collection, it would be appealing to autonomously explore patterns and insights from observational data for discovering novel classes of phenotypes and concepts. However, in the biomedical domain, there are several challenges inherently presented in the cumulated data which hamper the progress of novel class discovery. The non-i.i.d. data distribution accompanied by the severe imbalance among different groups of classes essentially leads to ambiguous and biased semantic representations. In this work, we present a geometry-constrained probabilistic modeling treatment to resolve the identified issues. First, we propose to parameterize the approximated posterior of instance embedding as a marginal von MisesFisher distribution to account for the int
    
[^188]: 一个镜子的库：低维深度神经网络是具有反射特征的凸Lasso模型

    A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex Lasso Models with Reflection Features

    [https://arxiv.org/abs/2403.01046](https://arxiv.org/abs/2403.01046)

    证明在1-D数据上训练神经网络等价于解决一个具有固定特征字典矩阵的凸Lasso问题，为全局最优网络和解空间提供了洞察。

    

    我们证明在1-D数据上训练神经网络等价于解决一个带有固定、明确定义的特征字典矩阵的凸Lasso问题。具体的字典取决于激活函数和深度。我们考虑具有分段线性激活函数的两层网络，深窄的ReLU网络最多有4层，以及具有符号激活和任意深度的矩形和树网络。有趣的是，在ReLU网络中，第四层创建代表训练数据关于自身的反射的特征。Lasso表示法揭示了全局最优网络和解空间的洞察。

    arXiv:2403.01046v1 Announce Type: cross  Abstract: We prove that training neural networks on 1-D data is equivalent to solving a convex Lasso problem with a fixed, explicitly defined dictionary matrix of features. The specific dictionary depends on the activation and depth. We consider 2-layer networks with piecewise linear activations, deep narrow ReLU networks with up to 4 layers, and rectangular and tree networks with sign activation and arbitrary depth. Interestingly in ReLU networks, a fourth layer creates features that represent reflections of training data about themselves. The Lasso representation sheds insight to globally optimal networks and the solution landscape.
    
[^189]: 通过格点联合源通道编码实现联邦学习

    Federated Learning via Lattice Joint Source-Channel Coding

    [https://arxiv.org/abs/2403.01023](https://arxiv.org/abs/2403.01023)

    通过新的联合源通道编码方案，实现了一种通用联邦学习框架，有效解决了设备信道信息不可知的问题，并在数字通信中能够进行空中计算。

    

    本文介绍了一种利用新的联合源通道编码方案，通过数字通信实现空中计算的通用联邦学习框架。该方案不依赖设备的信道状态信息，而是利用格点编码量化模型参数并利用来自设备的干扰。服务器处设计了一种新颖的两层接收机结构，用于可靠解码定量化模型参数的整数组合作为聚合的格点。数值实验验证了所提方案的有效性。即使面临信道条件和设备异构性带来的挑战，所提出的方案明显优于其他空中联邦学习策略。

    arXiv:2403.01023v1 Announce Type: cross  Abstract: This paper introduces a universal federated learning framework that enables over-the-air computation via digital communications, using a new joint source-channel coding scheme. Without relying on channel state information at devices, this scheme employs lattice codes to both quantize model parameters and exploit interference from the devices. A novel two-layer receiver structure at the server is designed to reliably decode an integer combination of the quantized model parameters as a lattice point for the purpose of aggregation. Numerical experiments validate the effectiveness of the proposed scheme. Even with the challenges posed by channel conditions and device heterogeneity, the proposed scheme markedly surpasses other over-the-air FL strategies.
    
[^190]: 自主打击无人机用于反恐任务：挑战与初步解决方案

    Autonomous Strike UAVs for Counterterrorism Missions: Challenges and Preliminary Solutions

    [https://arxiv.org/abs/2403.01022](https://arxiv.org/abs/2403.01022)

    本研究首次深入分析了自主无人机任务成功实施的挑战和初步解决方案，提出了用于克服挑战的技术方案，并描述了一个用于训练无人机的机器学习模型。

    

    无人机正在成为现代战争中至关重要的工具，主要是因为它们具有成本效益高、风险降低以及能够执行更广泛活动的特点。本研究专注于使用自主无人机针对高价值目标执行打击任务。由于账本技术、智能合约和机器学习的发展，以前由专业人员或远程飞行的无人机执行的这些活动现在是可行的。我们的研究首次深入分析了自主无人机任务成功实施的挑战和初步解决方案。具体来说，我们确定了必须克服的挑战，并提出了可能的技术解决方案。我们还推导出了自主无人机任务成功概率的分析表达式，并描述了一个用于训练无人机的机器学习模型。

    arXiv:2403.01022v1 Announce Type: cross  Abstract: Unmanned Aircraft Vehicles (UAVs) are becoming a crucial tool in modern warfare, primarily due to their cost-effectiveness, risk reduction, and ability to perform a wider range of activities. The use of autonomous UAVs to conduct strike missions against highly valuable targets is the focus of this research. Due to developments in ledger technology, smart contracts, and machine learning, such activities formerly carried out by professionals or remotely flown UAVs are now feasible. Our study provides the first in-depth analysis of challenges and preliminary solutions for successful implementation of an autonomous UAV mission. Specifically, we identify challenges that have to be overcome and propose possible technical solutions for the challenges identified. We also derive analytical expressions for the success probability of an autonomous UAV mission, and describe a machine learning model to train the UAV.
    
[^191]: 论证悲观演员-评论家算法中验证缓冲区的必要性

    A Case for Validation Buffer in Pessimistic Actor-Critic

    [https://arxiv.org/abs/2403.01014](https://arxiv.org/abs/2403.01014)

    提出了验证悲观学习（VPL）算法，通过使用小的验证缓冲区调整悲观水平，以最小化评论家目标的逼近误差，从而改善了样本效率和性能。

    

    在这篇论文中，我们研究了通过悲观时序差异目标更新的评论家网络中错误累积的问题。我们展示了评论家逼近误差可以通过类似贝尔曼值的递归不动点模型来近似。我们利用这种递归定义来找到悲观评论家无偏的条件。基于这些见解，我们提出了验证悲观学习（VPL）算法。VPL使用一个小的验证缓冲区来调整整个代理训练过程中的悲观水平，其中悲观设置为使评论家目标的逼近误差最小化。我们在各种运动和操纵任务上研究了所提出的方法，并报告了在样本效率和性能方面的改进。

    arXiv:2403.01014v1 Announce Type: new  Abstract: In this paper, we investigate the issue of error accumulation in critic networks updated via pessimistic temporal difference objectives. We show that the critic approximation error can be approximated via a recursive fixed-point model similar to that of the Bellman value. We use such recursive definition to retrieve the conditions under which the pessimistic critic is unbiased. Building on these insights, we propose Validation Pessimism Learning (VPL) algorithm. VPL uses a small validation buffer to adjust the levels of pessimism throughout the agent training, with the pessimism set such that the approximation error of the critic targets is minimized. We investigate the proposed approach on a variety of locomotion and manipulation tasks and report improvements in sample efficiency and performance.
    
[^192]: 基于子任务分解的分布式数据集提炼

    Distributional Dataset Distillation with Subtask Decomposition

    [https://arxiv.org/abs/2403.00999](https://arxiv.org/abs/2403.00999)

    提出了基于子任务分解的分布式数据集提炼方法，通过使用最小的每类统计信息进行数据编码，并结合解码器，将数据集提炼成更节省内存的分布式表示形式，相比于传统基于原型的方法更高效。

    

    神经网络在从特定任务数据集进行训练时学到了什么？综合这种知识是数据集提炼背后的中心思想，最近的研究表明可以将大型数据集压缩成一小组捕捉原始数据集关键方面的输入标签对 ($\textit{prototypes}$)。本文的关键观察是，现有的提取明确原型的方法往往是次优的，导致通过提炼标签而产生意外的存储成本。为此，我们提出了$\textit{分布式数据集提炼}$ (D3)，它使用最小的每类统计信息对数据进行编码，并与解码器配对，将数据集提炼成一种更节省内存的分布式表示形式，与基于原型的方法相比更高效。为了扩大学习这些表示形式的过程，我们提出了$\textit{联合提炼}$，它分解

    arXiv:2403.00999v1 Announce Type: new  Abstract: What does a neural network learn when training from a task-specific dataset? Synthesizing this knowledge is the central idea behind Dataset Distillation, which recent work has shown can be used to compress large datasets into a small set of input-label pairs ($\textit{prototypes}$) that capture essential aspects of the original dataset. In this paper, we make the key observation that existing methods distilling into explicit prototypes are very often suboptimal, incurring in unexpected storage cost from distilled labels. In response, we propose $\textit{Distributional Dataset Distillation}$ (D3), which encodes the data using minimal sufficient per-class statistics and paired with a decoder, we distill dataset into a compact distributional representation that is more memory-efficient compared to prototype-based methods. To scale up the process of learning these representations, we propose $\textit{Federated distillation}$, which decompose
    
[^193]: 论部分可观察序列团队和游戏中信息结构在强化学习中的作用

    On the Role of Information Structure in Reinforcement Learning for Partially-Observable Sequential Teams and Games

    [https://arxiv.org/abs/2403.00993](https://arxiv.org/abs/2403.00993)

    明确表示信息结构是分析和解决强化学习问题的重要组成部分。

    

    在顺序决策问题中，信息结构描述了系统中不同时刻事件如何相互影响。本文主张明确表示信息结构是分析和解决强化学习问题的重要组成部分，并提出具有明确信息结构表示的新型强化学习模型。

    arXiv:2403.00993v1 Announce Type: cross  Abstract: In a sequential decision-making problem, the information structure is the description of how events in the system occurring at different points in time affect each other. Classical models of reinforcement learning (e.g., MDPs, POMDPs, Dec-POMDPs, and POMGs) assume a very simple and highly regular information structure, while more general models like predictive state representations do not explicitly model the information structure. By contrast, real-world sequential decision-making problems typically involve a complex and time-varying interdependence of system variables, requiring a rich and flexible representation of information structure.   In this paper, we argue for the perspective that explicit representation of information structures is an important component of analyzing and solving reinforcement learning problems. We propose novel reinforcement learning models with an explicit representation of information structure, capturing 
    
[^194]: SELFI: 利用强化学习实现自主自我改进以进行社交导航

    SELFI: Autonomous Self-Improvement with Reinforcement Learning for Social Navigation

    [https://arxiv.org/abs/2403.00991](https://arxiv.org/abs/2403.00991)

    SELFI提出了一种在线学习方法，通过将在线无模型强化学习与离线基于模型的学习相结合，实现了机器人行为的快速改进，并在避撞和社交遵从行为方面取得了显著进展。

    

    自主自我改进的机器人通过与环境互动和经验积累来实现将是机器人系统在现实世界中投入使用的关键。本文提出了一种在线学习方法SELFI，利用在线机器人经验来快速高效地微调预训练的控制策略。SELFI将在线无模型强化学习应用于离线基于模型的学习之上，以发挥这两种学习范式的优点。具体来说，SELFI通过将离线预训练的模型学习目标与在线无模型强化学习中学习到的Q值相结合，稳定了在线学习过程。我们在多个现实环境中评估了SELFI，并报告了在避撞方面的改善，以及通过人类用户研究测量的更具社交遵从行为。SELFI使我们能够快速学习有用的机器人行为，减少了预先干预的人员干预。

    arXiv:2403.00991v1 Announce Type: cross  Abstract: Autonomous self-improving robots that interact and improve with experience are key to the real-world deployment of robotic systems. In this paper, we propose an online learning method, SELFI, that leverages online robot experience to rapidly fine-tune pre-trained control policies efficiently. SELFI applies online model-free reinforcement learning on top of offline model-based learning to bring out the best parts of both learning paradigms. Specifically, SELFI stabilizes the online learning process by incorporating the same model-based learning objective from offline pre-training into the Q-values learned with online model-free reinforcement learning. We evaluate SELFI in multiple real-world environments and report improvements in terms of collision avoidance, as well as more socially compliant behavior, measured by a human user study. SELFI enables us to quickly learn useful robotic behaviors with less human interventions such as pre-e
    
[^195]: 合并来自不同初始化的文本变换器模型

    Merging Text Transformer Models from Different Initializations

    [https://arxiv.org/abs/2403.00986](https://arxiv.org/abs/2403.00986)

    研究了合并不同初始化的Transformer模型的技术，提出了一种模型合并技术以研究这些模型极小值之间的关系，并发现与模型平均相比，通过我们的方法合并这些模型始终可以获得较低的损失障碍。

    

    最近关于一次性基于排列的模型合并的工作表明，不同初始化的模型之间存在令人印象深刻的低或零障碍模连接。然而，尽管Transformer架构在语言领域中占主导地位，但这一领域的研究尚未延伸到Transformer架构。因此，在这项工作中，我们调查了独立Transformer极小值学习类似特征的程度，并提出了一种模型合并技术，以研究损失景观中这些极小值之间的关系。架构的具体细节，如其残差连接、多头注意力和离散的顺序输入，需要特定的干预措施，以便计算留在相同功能等价类中的模型排列。通过我们的方法合并这些模型，我们发现与对几个在一个maske上训练的模型进行模型平均相比，最小值之间的损失障碍一直较低。

    arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
    
[^196]: 风力发电机性能的时间序列分析设备健康评估

    Equipment Health Assessment: Time Series Analysis for Wind Turbine Performance

    [https://arxiv.org/abs/2403.00975](https://arxiv.org/abs/2403.00975)

    利用功能神经网络（FNN）和长短期记忆（LSTM）网络的集成方法来预测风力发电机功率输出，实现准确稳定的预测并检测性能恶化，以推动积极的维护策略和健康评估。

    

    在这项研究中，我们利用来自不同风力发电机的SCADA数据，使用先进的时间序列方法，特别是功能神经网络（FNN）和长短期记忆（LSTM）网络来预测功率输出。关键创新在于FNN和LSTM模型的集成，利用它们的集体学习。这种集成方法胜过单个模型，确保稳定和准确的功率输出预测。此外，机器学习技术用于检测风力发电机性能恶化，实现积极的维护策略和健康评估。关键是，我们的分析揭示了每台风力发电机的独特性，需要为最佳预测定制模型。这些见解强调提供不同发电机自动化定制的重要性，以保持人力建模工作量低。重要的是，本分析中开发的方法不局限于风力发电机。

    arXiv:2403.00975v1 Announce Type: cross  Abstract: In this study, we leverage SCADA data from diverse wind turbines to predict power output, employing advanced time series methods, specifically Functional Neural Networks (FNN) and Long Short-Term Memory (LSTM) networks. A key innovation lies in the ensemble of FNN and LSTM models, capitalizing on their collective learning. This ensemble approach outperforms individual models, ensuring stable and accurate power output predictions. Additionally, machine learning techniques are applied to detect wind turbine performance deterioration, enabling proactive maintenance strategies and health assessment. Crucially, our analysis reveals the uniqueness of each wind turbine, necessitating tailored models for optimal predictions. These insight underscores the importance of providing automatized customization for different turbines to keep human modeling effort low. Importantly, the methodologies developed in this analysis are not limited to wind tu
    
[^197]: 稀疏深度神经网络的模式分布和功能

    Motif distribution and function of sparse deep neural networks

    [https://arxiv.org/abs/2403.00974](https://arxiv.org/abs/2403.00974)

    通过网络模式理论，研究了稀疏深度神经网络的连接结构，证明了尽管随机初始化参数，强制稀疏会导致这些网络收敛到相似的结构

    

    我们使用网络模式理论表征前馈式深度神经网络（DNNs）的连接结构。为了解特定模式分布是否是训练任务的特征，还是DNN的功能，我们比较了训练以模拟生物力学飞行控制系统的350个DNN的连接结构，这些DNN拥有不同的随机初始化参数。我们开发并实施了用于计算二阶和三阶模式并使用它们的Z分数计算其显着性的算法。这些DNN被训练来解决 Bustamante 等人（2022年）中的飞行动力学模型的反问题（即从初始和最终状态空间输入预测控制所需）。通过一个迭代修剪和重新训练算法 Zahn 等人（2022年），我们对它们进行了稀疏化处理。我们展示了，尽管网络参数随机初始化，但强制稀疏会导致DNN收敛到类似的结构。

    arXiv:2403.00974v1 Announce Type: new  Abstract: We characterize the connectivity structure of feed-forward, deep neural networks (DNNs) using network motif theory. To address whether a particular motif distribution is characteristic of the training task, or function of the DNN, we compare the connectivity structure of 350 DNNs trained to simulate a bio-mechanical flight control system with different randomly initialized parameters. We develop and implement algorithms for counting second- and third-order motifs and calculate their significance using their Z-score. The DNNs are trained to solve the inverse problem of the flight dynamics model in Bustamante, et al. (2022) (i.e., predict the controls necessary for controlled flight from the initial and final state-space inputs) and are sparsified through an iterative pruning and retraining algorithm Zahn, et al. (2022). We show that, despite random initialization of network parameters, enforced sparsity causes DNNs to converge to similar 
    
[^198]: 二值高斯Copula合成：一种新的数据增强技术，用于推进基于机器学习的临床决策支持系统，旨在早期预测慢性肾病患者的透析需求

    Binary Gaussian Copula Synthesis: A Novel Data Augmentation Technique to Advance ML-based Clinical Decision Support Systems for Early Prediction of Dialysis Among CKD Patients

    [https://arxiv.org/abs/2403.00965](https://arxiv.org/abs/2403.00965)

    提出了一种新的数据增强技术 Binary Gaussian Copula Synthesis (BGCS)，用于解决基于机器学习的临床决策支持系统在早期预测慢性肾病患者透析需求中所面临的数据不平衡问题

    

    谷歌学术：2403.00965v1  公告类型：跨界  摘要：美国疾病控制中心估计，超过3700万成年美国人患有慢性肾病（CKD），然而其中的9成患者由于早期没有症状而不知道自己的状况。早期预测透析需求至关重要，因为这可以显著改善患者预后，并帮助医疗提供者及时做出知情决策。然而，开发有效的基于机器学习（ML）的早期透析预测临床决策支持系统（CDSS）面临关键挑战，即数据的不平衡性。为了解决这一挑战，本研究评估了各种数据增强技术，以了解它们在现实世界数据集上的有效性。我们提出了一种名为二值高斯Copula合成（BGCS）的新方法，该方法针对二进制数据进行了优化。

    arXiv:2403.00965v1 Announce Type: cross  Abstract: The Center for Disease Control estimates that over 37 million US adults suffer from chronic kidney disease (CKD), yet 9 out of 10 of these individuals are unaware of their condition due to the absence of symptoms in the early stages. It has a significant impact on patients' quality of life, particularly when it progresses to the need for dialysis. Early prediction of dialysis is crucial as it can significantly improve patient outcomes and assist healthcare providers in making timely and informed decisions. However, developing an effective machine learning (ML)-based Clinical Decision Support System (CDSS) for early dialysis prediction poses a key challenge due to the imbalanced nature of data. To address this challenge, this study evaluates various data augmentation techniques to understand their effectiveness on real-world datasets. We propose a new approach named Binary Gaussian Copula Synthesis (BGCS). BGCS is tailored for binary me
    
[^199]: 在SemEval-2024任务6中的MALTO：利用合成数据检测LLM幻觉

    MALTO at SemEval-2024 Task 6: Leveraging Synthetic Data for LLM Hallucination Detection

    [https://arxiv.org/abs/2403.00964](https://arxiv.org/abs/2403.00964)

    引入了数据增强流水线和投票集成，利用合成数据检测LLM幻觉。

    

    在自然语言生成（NLG）中，当代大型语言模型（LLM）面临着一些挑战，例如生成流畅但不准确的输出以及依赖于流畅性为中心的度量。这经常导致神经网络出现“幻觉”。SHROOM挑战旨在自动识别生成文本中的这些幻觉。为了解决这些问题，我们引入了两个关键组件，一个数据增强流水线，包括LLM辅助的伪标记和句子改写，以及一个投票集成来自三个在自然语言推理（NLI）任务上预训练并在不同数据集上微调的模型。

    arXiv:2403.00964v1 Announce Type: new  Abstract: In Natural Language Generation (NLG), contemporary Large Language Models (LLMs) face several challenges, such as generating fluent yet inaccurate outputs and reliance on fluency-centric metrics. This often leads to neural networks exhibiting "hallucinations". The SHROOM challenge focuses on automatically identifying these hallucinations in the generated text. To tackle these issues, we introduce two key components, a data augmentation pipeline incorporating LLM-assisted pseudo-labelling and sentence rephrasing, and a voting ensemble from three models pre-trained on Natural Language Inference (NLI) tasks and fine-tuned on diverse datasets.
    
[^200]: 树正则化的表格嵌入

    Tree-Regularized Tabular Embeddings

    [https://arxiv.org/abs/2403.00963](https://arxiv.org/abs/2403.00963)

    提出了一种通过树正则化表征来正则化表格输入的方法，将原始变量转换为单个向量或一系列标记，有效缩小了与树模型之间的性能差距。

    

    表格神经网络（NN）引起了显着的关注，其最新进展逐渐缩小了与许多公共数据集上树模型之间的性能差距。虽然主流关注于校准NN以适应表格数据，但我们强调同质嵌入的重要性，并交替集中在通过监督预训练来正则化表格输入。具体来说，我们扩展了最近的一个工作（DeepTLF），并利用预训练树集合的结构将原始变量转换为单个向量（T2V）或一系列标记（T2T）。这些二元化嵌入可以通过带有全连接或基于注意力的构建块的经典表格NN消耗，而不会损失空间效率。通过对88个OpenML数据集进行数量实验，其中包括二元分类任务，我们验证了所提出的树正则化表征不仅缩小了与树模型之间的差距

    arXiv:2403.00963v1 Announce Type: new  Abstract: Tabular neural network (NN) has attracted remarkable attentions and its recent advances have gradually narrowed the performance gap with respect to tree-based models on many public datasets. While the mainstreams focus on calibrating NN to fit tabular data, we emphasize the importance of homogeneous embeddings and alternately concentrate on regularizing tabular inputs through supervised pretraining. Specifically, we extend a recent work (DeepTLF) and utilize the structure of pretrained tree ensembles to transform raw variables into a single vector (T2V), or an array of tokens (T2T). Without loss of space efficiency, these binarized embeddings can be consumed by canonical tabular NN with fully-connected or attention-based building blocks. Through quantitative experiments on 88 OpenML datasets with binary classification task, we validated that the proposed tree-regularized representation not only tapers the difference with respect to tree-
    
[^201]: 大学物理学中的数据科学教育：来自实践社区的经验教训

    Data Science Education in Undergraduate Physics: Lessons Learned from a Community of Practice

    [https://arxiv.org/abs/2403.00961](https://arxiv.org/abs/2403.00961)

    通过建立数据科学教育实践社区，分享最佳实践和经验教训，为将数据科学纳入大学物理课程的关键策略和挑战提供指导。

    

    随着各种数据集的日益丰富，从小规模实验数据点到大型复杂数据存储库和强大的数据分析工具，物理教育工作者越来越重视装备他们的学生有效处理数据的能力。然而，许多教育工作者可能缺乏教授这些技能所需的数据科学培训和专业知识。为了解决这一差距，我们建立了数据科学教育实践社区（DSECOP），汇集了不同院校和背景的研究生和物理教育工作者，分享整合数据科学到大学物理教育中的最佳实践和经验教训。在本文中，我们介绍了这个实践社区的见解和经验，突出了在将数据科学纳入初级物理课程中的关键策略和挑战。我们的目标是提供指导和ins

    arXiv:2403.00961v1 Announce Type: cross  Abstract: With the increasing availability of diverse datasets, ranging from small-scale experimental data points to large and complex data repositories and powerful data analysis tools, it is increasingly important that physics educators equip their students with the skills to work with data effectively. However, many educators may lack the necessary training and expertise in data science to teach these skills. To address this gap, we created the Data Science Education Community of Practice (DSECOP), bringing together graduate students and physics educators from different institutions and backgrounds to share best practices and lessons learned in integrating data science into undergraduate physics education. In this article, we present insights and experiences from this community of practice, highlighting key strategies and challenges in incorporating data science into the introductory physics curriculum. Our goal is to provide guidance and ins
    
[^202]: MediSwift：高效稀疏预训练生物医学语言模型

    MediSwift: Efficient Sparse Pre-trained Biomedical Language Models

    [https://arxiv.org/abs/2403.00952](https://arxiv.org/abs/2403.00952)

    MediSwift在生物医学领域引入了高效稀疏预训练模型，通过75%的权重稀疏性实现了2-2.5倍的训练FLOPs减少，从而显著提高了效率。

    

    大型语言模型（LLMs）通常在通用源数据上进行训练，用于各种领域，但最近领域特定的LLMs激增表明它们在领域特定任务（例如生物医学）中的潜力超过了通用型模型。虽然领域特定的预训练提高了效率并导致模型更小，但这些LLMs的训练计算成本仍然很高，构成了预算挑战。我们引入了MediSwift，一套利用领域特定生物医学文本数据上的稀疏预训练的生物医学LM。通过在预训练阶段引入高达75％的权重稀疏性，MediSwift在训练FLOPs方面实现了2-2.5倍的减少。值得注意的是，所有的稀疏预训练均在专门设计用于实现来自非结构化权重稀疏性的加速好处的Cerebras CS-2系统上进行，从而显着提高了MediSwift模型的效率。

    arXiv:2403.00952v1 Announce Type: new  Abstract: Large language models (LLMs) are typically trained on general source data for various domains, but a recent surge in domain-specific LLMs has shown their potential to outperform general-purpose models in domain-specific tasks (e.g., biomedicine). Although domain-specific pre-training enhances efficiency and leads to smaller models, the computational costs of training these LLMs remain high, posing budgeting challenges. We introduce MediSwift, a suite of biomedical LMs that leverage sparse pre-training on domain-specific biomedical text data. By inducing up to 75% weight sparsity during the pre-training phase, MediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse pre-training was performed on the Cerebras CS-2 system, which is specifically designed to realize the acceleration benefits from unstructured weight sparsity, thereby significantly enhancing the efficiency of the MediSwift models. Through subsequent dense f
    
[^203]: 使用非常大的Dropout进行微调

    Fine-tuning with Very Large Dropout

    [https://arxiv.org/abs/2403.00946](https://arxiv.org/abs/2403.00946)

    通过使用非常高的dropout率进行微调，可以实现超出分布性能，这超出了集成和权重平均方法。

    

    今天不可能假装机器学习实践与训练和测试数据遵循相同分布的观念是兼容的。该论文调查了使用非常高的丢弃率来获得这种丰富表示，尽管使用这样的丢弃率从头开始训练深度网络几乎是不可能的，但在这些条件下对大型预训练模型进行微调不仅是可能的，而且实现了超越集成和权重平均方法的超出分布性能。

    arXiv:2403.00946v1 Announce Type: new  Abstract: It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.   This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods
    
[^204]: 分布式神经网络中熵模型的韧性

    Resilience of Entropy Model in Distributed Neural Networks

    [https://arxiv.org/abs/2403.00942](https://arxiv.org/abs/2403.00942)

    本文研究了分布式神经网络中熵模型对有意干扰和无意干扰的韧性，通过实验证明了熵模型的韧性。

    

    分布式深度神经网络（DNNs）已经成为边缘计算系统中减少通信开销而不降低性能的关键技术。最近，熵编码被引入以进一步减少通信开销。其关键思想是将分布式DNN与熵模型联合训练，该模型在推断时间用作边信息，以自适应地将潜在表示编码为具有可变长度的比特流。据我们所知，熵模型的韧性尚未得到研究。因此，在本文中，我们制定并调查了熵模型对有意干扰（例如，对抗性攻击）和无意干扰（例如，天气变化和运动模糊）的韧性。通过对3种不同DNN架构、2个熵模型和4个速率失真权衡因子进行广泛的实验，我们证明了熵模型的韧性

    arXiv:2403.00942v1 Announce Type: cross  Abstract: Distributed deep neural networks (DNNs) have emerged as a key technique to reduce communication overhead without sacrificing performance in edge computing systems. Recently, entropy coding has been introduced to further reduce the communication overhead. The key idea is to train the distributed DNN jointly with an entropy model, which is used as side information during inference time to adaptively encode latent representations into bit streams with variable length. To the best of our knowledge, the resilience of entropy models is yet to be investigated. As such, in this paper we formulate and investigate the resilience of entropy models to intentional interference (e.g., adversarial attacks) and unintentional interference (e.g., weather changes and motion blur). Through an extensive experimental campaign with 3 different DNN architectures, 2 entropy models and 4 rate-distortion trade-off factors, we demonstrate that the entropy attacks
    
[^205]: 安全领域的迁移学习：挑战与未来方向

    Transfer Learning for Security: Challenges and Future Directions

    [https://arxiv.org/abs/2403.00935](https://arxiv.org/abs/2403.00935)

    迁移学习在安全领域表现出巨大潜力，能够有效跨域传递知识以提高学习性能，减少数据标注工作，并本文旨在回顾和探讨其在安全领域的应用前景。

    

    许多机器学习和数据挖掘算法都依赖于一个假设，即训练和测试数据共享相同的特征空间和分布。然而，这一假设并不总是成立。在某些情况下，我们需要对一个领域的数据进行分类，但我们只有来自另一个领域的充足训练数据可用。后者的数据可能遵循不同的分布。在这种情况下，成功地在领域之间传递知识可以显著提高学习性能，并减少对广泛数据标注工作的需求。迁移学习（TL）因此成为应对这一挑战的一个有前途的框架，特别是在安全相关任务中。本文旨在回顾利用TL技术来应用于安全领域的最新进展。本文还讨论了在安全领域应用TL中存在的研究差距，并探索了潜在的未来研究方向。

    arXiv:2403.00935v1 Announce Type: cross  Abstract: Many machine learning and data mining algorithms rely on the assumption that the training and testing data share the same feature space and distribution. However, this assumption may not always hold. For instance, there are situations where we need to classify data in one domain, but we only have sufficient training data available from a different domain. The latter data may follow a distinct distribution. In such cases, successfully transferring knowledge across domains can significantly improve learning performance and reduce the need for extensive data labeling efforts. Transfer learning (TL) has thus emerged as a promising framework to tackle this challenge, particularly in security-related tasks. This paper aims to review the current advancements in utilizing TL techniques for security. The paper includes a discussion of the existing research gaps in applying TL in the security domain, as well as exploring potential future researc
    
[^206]: 通过合成文本生成的差分私密知识蒸馏

    Differentially Private Knowledge Distillation via Synthetic Text Generation

    [https://arxiv.org/abs/2403.00932](https://arxiv.org/abs/2403.00932)

    提出一种利用合成数据进行知识蒸馏的差分私密算法

    

    大型语言模型(LLMs)在许多不同的下游任务中实现了最先进的性能。然而，数据隐私的增加紧迫性要求LLMs在私有数据上使用差分隐私(DP)进行训练。同时，还需要压缩LLMs以在资源受限的设备或延迟敏感的应用中进行真实部署。差分隐私和模型压缩通常必须在实现其目标的过程中权衡效用损失。此外，同时实现这两者可能导致更多的效用损失。为此，我们提出了一种新颖的差分私密知识蒸馏算法，该算法利用了由差分私密LLM生成的合成数据。教师模型的知识以两种方式转移到学生模型上：一种是来自合成数据本身的硬标签，另一种是通过在合成数据上评估的教师模型的输出分布。

    arXiv:2403.00932v1 Announce Type: cross  Abstract: Large Language models (LLMs) are achieving state-of-the-art performance in many different downstream tasks. However, the increasing urgency of data privacy requires LLMs to train with Differential Privacy (DP) on private data. Concurrently it is also necessary to compress LLMs for real-life deployments on resource-constrained devices or latency-sensitive applications. Differential privacy and model compression generally must trade off utility loss to achieve their objectives. Moreover, concurrently achieving both can result in even more utility loss. To this end, we propose a novel differentially private knowledge distillation algorithm that exploits synthetic data generated by a differentially private LLM. The knowledge of a teacher model is transferred onto the student in two ways: one way from the synthetic data itself, the hard labels, and the other way by the output distribution of the teacher model evaluated on the synthetic data
    
[^207]: 无尺度对抗性强化学习

    Scale-free Adversarial Reinforcement Learning

    [https://arxiv.org/abs/2403.00930](https://arxiv.org/abs/2403.00930)

    本文在马尔可夫决策过程中提出了首个无尺度对抗性学习算法框架SCB，在对抗性多臂赌博机和MDP设置中取得了关键突破。

    

    本文首次研究了马尔可夫决策过程（MDPs）中的无尺度学习，其奖励/损失的尺度为学习者所不知。我们设计了一个通用的算法框架，\underline{S}cale \underline{C}lipping \underline{B}ound（\texttt{SCB}），并将这一框架实例化到对抗性多臂赌博机（MAB）设置和对抗性MDP设置中。通过这个框架，在无尺度对抗性MABs中，我们实现了第一个最小值最优期望遗憾界和第一个高概率遗憾界，解决了\cite{hadiji2023adaptation}中提出的一个开放问题。在对抗性MDPs中，我们的框架还诞生了第一个带有$\tilde{\mathcal{O}}(\sqrt{T})$高概率遗憾保证的无尺度RL算法。

    arXiv:2403.00930v1 Announce Type: cross  Abstract: This paper initiates the study of scale-free learning in Markov Decision Processes (MDPs), where the scale of rewards/losses is unknown to the learner. We design a generic algorithmic framework, \underline{S}cale \underline{C}lipping \underline{B}ound (\texttt{SCB}), and instantiate this framework in both the adversarial Multi-armed Bandit (MAB) setting and the adversarial MDP setting. Through this framework, we achieve the first minimax optimal expected regret bound and the first high-probability regret bound in scale-free adversarial MABs, resolving an open problem raised in \cite{hadiji2023adaptation}. On adversarial MDPs, our framework also give birth to the first scale-free RL algorithm with a $\tilde{\mathcal{O}}(\sqrt{T})$ high-probability regret guarantee.
    
[^208]: 利用行为原语搭建任务的框架以提高数据效率的模仿学习

    PRIME: Scaffolding Manipulation Tasks with Behavior Primitives for Data-Efficient Imitation Learning

    [https://arxiv.org/abs/2403.00929](https://arxiv.org/abs/2403.00929)

    PRIME是一个基于行为原语设计的框架，通过将任务分解为原语序列并学习高级控制策略，显著提高了多阶段操作任务的性能表现。

    

    模仿学习已经显示出巨大潜力，可以让机器人学会复杂的操作行为。然而，在长期任务中，这些算法受到高样本复杂度的困扰，因为复合误差会在任务时段内累积。我们提出了PRIME（基于行为原语的数据效率模仿），这是一个基于行为原语的框架，旨在提高模仿学习的数据效率。PRIME通过将任务演示分解为原语序列来搭建机器人任务，然后通过模仿学习学习一个高级控制策略来对原语序列进行排序。我们的实验证明，PRIME在多阶段操作任务中实现了显著的性能提升，在模拟环境中的成功率比最先进的基线高出10-34％，在实际硬件上高出20-48％。

    arXiv:2403.00929v1 Announce Type: cross  Abstract: Imitation learning has shown great potential for enabling robots to acquire complex manipulation behaviors. However, these algorithms suffer from high sample complexity in long-horizon tasks, where compounding errors accumulate over the task horizons. We present PRIME (PRimitive-based IMitation with data Efficiency), a behavior primitive-based framework designed for improving the data efficiency of imitation learning. PRIME scaffolds robot tasks by decomposing task demonstrations into primitive sequences, followed by learning a high-level control policy to sequence primitives through imitation learning. Our experiments demonstrate that PRIME achieves a significant performance improvement in multi-stage manipulation tasks, with 10-34% higher success rates in simulation over state-of-the-art baselines and 20-48% on physical hardware.
    
[^209]: 算法配置问题

    The Algorithm Configuration Problem

    [https://arxiv.org/abs/2403.00898](https://arxiv.org/abs/2403.00898)

    本文深入研究了算法配置问题，提出了一个全面框架，结合机器学习模型和启发式策略，划分了不同的解决方法，以明确路径来理解和解决算法配置中的复杂性。

    

    算法优化领域随着自动配置算法参数方法的发展而显著进步。本文深入探讨了算法配置问题，旨在优化用于解决特定决策/优化问题实例的参数化算法。我们提出了一个全面的框架，不仅形式化了算法配置问题，还概述了利用机器学习模型和启发式策略解决该问题的不同方法。该文章将现有方法论划分为基于实例和基于问题的方法，区分离线和在线策略用于模型构建和部署。通过综合这些方法，我们旨在为理解和解决算法配置中固有复杂性提供清晰的路径。

    arXiv:2403.00898v1 Announce Type: new  Abstract: The field of algorithmic optimization has significantly advanced with the development of methods for the automatic configuration of algorithmic parameters. This article delves into the Algorithm Configuration Problem, focused on optimizing parametrized algorithms for solving specific instances of decision/optimization problems. We present a comprehensive framework that not only formalizes the Algorithm Configuration Problem, but also outlines different approaches for its resolution, leveraging machine learning models and heuristic strategies. The article categorizes existing methodologies into per-instance and per-problem approaches, distinguishing between offline and online strategies for model construction and deployment. By synthesizing these approaches, we aim to provide a clear pathway for both understanding and addressing the complexities inherent in algorithm configuration.
    
[^210]: VisRec:一种用于射电干涉数据重建的半监督方法

    VisRec: A Semi-Supervised Approach to Radio Interferometric Data Reconstruction

    [https://arxiv.org/abs/2403.00897](https://arxiv.org/abs/2403.00897)

    VisRec提出了一种模型-不可知的半监督学习方法，用于重建射电干扰数据，通过监督学习模块和无监督学习模块相结合，减少了对标记训练数据的需求，降低了射电天文学家的标注工作量

    

    射电望远镜产生关于天体对象的可见性数据，但这些数据稀疏且嘈杂。因此，基于原始可见性数据创建的图像质量较低。最近的研究使用深度学习模型重建可见性数据，以获得更清晰的图像。然而，这些方法依赖大量标记的训练数据，这需要射电天文学家大量的标注工作。针对这一挑战，我们提出了VisRec，一种面向模型的半监督学习方法，用于重建可见性数据。具体来说，VisRec包括监督学习模块和无监督学习模块。在监督学习模块中，我们引入一组数据增强函数来产生多样化的训练示例。相比之下，在VisRec中的无监督学习模块会增加未标记的数据，并使用来自非增强可见性数据的重建作为伪标签。

    arXiv:2403.00897v1 Announce Type: cross  Abstract: Radio telescopes produce visibility data about celestial objects, but these data are sparse and noisy. As a result, images created on raw visibility data are of low quality. Recent studies have used deep learning models to reconstruct visibility data to get cleaner images. However, these methods rely on a substantial amount of labeled training data, which requires significant labeling effort from radio astronomers. Addressing this challenge, we propose VisRec, a model-agnostic semi-supervised learning approach to the reconstruction of visibility data. Specifically, VisRec consists of both a supervised learning module and an unsupervised learning module. In the supervised learning module, we introduce a set of data augmentation functions to produce diverse training examples. In comparison, the unsupervised learning module in VisRec augments unlabeled data and uses reconstructions from non-augmented visibility data as pseudo-labels for t
    
[^211]: 精确推荐的端到端图-序列表示学习

    End-to-end Graph-Sequential Representation Learning for Accurate Recommendations

    [https://arxiv.org/abs/2403.00895](https://arxiv.org/abs/2403.00895)

    本文提出了一个新颖的多重表示学习框架，有效地结合了基于序列和基于图的推荐方法，显著改善了推荐性能。

    

    近年来推荐系统的许多新进展集中在开发基于序列和基于图的方法上。这两种方法在建模行为数据中的复杂关系方面都证明了其有效性，从而在个性化排名和下一个推荐任务中取得了有益的成果，同时保持了良好的可扩展性。然而，它们从数据中捕捉到的信号截然不同。前者直接通过与最近物品的有序交互来表示用户，而后者旨在捕捉交互图中的间接依赖关系。本文提出了一个新颖的多重表示学习框架，利用这两种范式之间的协同作用。我们在几个数据集上的实证评估表明，利用所提出的框架相互训练序列和图组件显著改善了推荐性能。

    arXiv:2403.00895v1 Announce Type: cross  Abstract: Many recent advancements in recommender systems have focused on developing sequence-based and graph-based approaches. Both approaches proved useful in modeling intricate relationships within behavioral data, leading to promising outcomes in personalized ranking and next-item recommendation tasks while maintaining good scalability. However, they capture very different signals from data. While the former approach represents users directly through ordered interactions with recent items, the latter one aims to capture indirect dependencies across the interactions graph. This paper presents a novel multi-representational learning framework that exploits the synergies between these two paradigms. Our empirical evaluation on several datasets demonstrates that mutual training of sequential and graph components with the proposed framework significantly improves recommendations performance.
    
[^212]: PowerFlowMultiNet：用于不平衡三相配电系统的多图神经网络

    PowerFlowMultiNet: Multigraph Neural Networks for Unbalanced Three-Phase Distribution Systems

    [https://arxiv.org/abs/2403.00892](https://arxiv.org/abs/2403.00892)

    PowerFlowMultiNet是一种专门为不平衡三相功率网格设计的新颖多图GNN框架，能够有效捕捉不平衡网格中的不对称性，并引入了图嵌入机制来捕获电力系统网络内部的空间依赖关系。

    

    高效解决配电网中不平衡的三相功率流问题对于网格分析和仿真至关重要。目前急需可处理大规模不平衡功率网格并能提供准确快速解决方案的可扩展算法。为解决这一问题，深度学习技术尤其是图神经网络（GNNs）应运而生。然而，现有文献主要集中在平衡网络上，缺乏支持不平衡三相功率网络的关键内容。本文介绍了PowerFlowMultiNet，这是一个专门为不平衡三相功率网格设计的新颖多图GNN框架。提出的方法在多图表示中分别对每个相进行建模，有效捕捉不平衡网格中固有的不对称性。引入了利用消息传递捕获电力系统网络内部空间依赖关系的图嵌入机制。

    arXiv:2403.00892v1 Announce Type: cross  Abstract: Efficiently solving unbalanced three-phase power flow in distribution grids is pivotal for grid analysis and simulation. There is a pressing need for scalable algorithms capable of handling large-scale unbalanced power grids that can provide accurate and fast solutions. To address this, deep learning techniques, especially Graph Neural Networks (GNNs), have emerged. However, existing literature primarily focuses on balanced networks, leaving a critical gap in supporting unbalanced three-phase power grids. This letter introduces PowerFlowMultiNet, a novel multigraph GNN framework explicitly designed for unbalanced three-phase power grids. The proposed approach models each phase separately in a multigraph representation, effectively capturing the inherent asymmetry in unbalanced grids. A graph embedding mechanism utilizing message passing is introduced to capture spatial dependencies within the power system network. PowerFlowMultiNet out
    
[^213]: 一种基于正则化的指导图解码器的信息抽取迁移学习方法

    A Regularization-based Transfer Learning Method for Information Extraction via Instructed Graph Decoder

    [https://arxiv.org/abs/2403.00891](https://arxiv.org/abs/2403.00891)

    提出了一种基于正则化的信息抽取迁移学习方法，通过指导图解码器实现数据集之间通用知识的迁移

    

    信息提取（IE）旨在从文本中提取复杂结构化信息。已为各种IE任务构建了大量数据集，导致耗时且劳动密集的数据标注。然而，大多数流行方法侧重于训练特定任务的模型，而不是明确对不同IE任务之间的通用知识进行建模。此外，相同短语可能在不同任务中具有不一致的标签，这对使用统一模型进行知识迁移构成了巨大挑战。在本研究中，我们提出了一种基于正则化的信息抽取（IE）的迁移学习方法，通过一个指导图解码器进行。具体而言，我们首先为所有著名IE任务的数据集构建一个指导池，然后提出一个指导图解码器，根据相应的指导将各种复杂结构均匀地解码为图。通过这种方式，与现有数据集共享的通用知识可以更好地用于迁移学习。

    arXiv:2403.00891v1 Announce Type: cross  Abstract: Information extraction (IE) aims to extract complex structured information from the text. Numerous datasets have been constructed for various IE tasks, leading to time-consuming and labor-intensive data annotations. Nevertheless, most prevailing methods focus on training task-specific models, while the common knowledge among different IE tasks is not explicitly modeled. Moreover, the same phrase may have inconsistent labels in different tasks, which poses a big challenge for knowledge transfer using a unified model. In this study, we propose a regularization-based transfer learning method for IE (TIE) via an instructed graph decoder. Specifically, we first construct an instruction pool for datasets from all well-known IE tasks, and then present an instructed graph decoder, which decodes various complex structures into a graph uniformly based on corresponding instructions. In this way, the common knowledge shared with existing datasets 
    
[^214]: 面向极简可穿戴设备的时限上下文生物身份生成

    Time-bound Contextual Bio-ID Generation for Minimalist Wearables

    [https://arxiv.org/abs/2403.00889](https://arxiv.org/abs/2403.00889)

    提出了一种名为Proteus的创新概念，实现了时限上下文生物身份的生成，可以有效解决极简可穿戴设备的实时认证挑战，促进设备协作和用户交互。

    

    随着可穿戴设备越来越小型化和强大，即时动态的设备对设备协作和人对设备交互的新机遇应运而生。然而，这一进展带来了独特的挑战：这些极简可穿戴设备缺乏实时身份验证机制，给数据隐私和整体安全带来了重大风险。为了解决这一问题，我们介绍了Proteus，实现了一种创新的时限上下文生物身份概念，这些生物身份是从设备传感器数据生成并嵌入到一个共同的潜在空间中。这些生物身份充当着时限的独特用户标识符，在特定上下文中用于识别佩戴者。Proteus实现了动态的上下文设备协作以及强大的人对设备交互。我们的评估证明了我们的方法的有效性，尤其在极简可穿戴设备的背景下。

    arXiv:2403.00889v1 Announce Type: cross  Abstract: As wearable devices become increasingly miniaturized and powerful, a new opportunity arises for instant and dynamic device-to-device collaboration and human-to-device interaction. However, this progress presents a unique challenge: these minimalist wearables lack inherent mechanisms for real-time authentication, posing significant risks to data privacy and overall security. To address this, we introduce Proteus that realizes an innovative concept of time-bound contextual bio-IDs, which are generated from on-device sensor data and embedded into a common latent space. These bio-IDs act as a time-bound unique user identifier that can be used to identify the wearer in a certain context. Proteus enables dynamic and contextual device collaboration as well as robust human-to-device interaction. Our evaluations demonstrate the effectiveness of our method, particularly in the context of minimalist wearables.
    
[^215]: 基于边际差异的多领域文本分类对抗训练

    Margin Discrepancy-based Adversarial Training for Multi-Domain Text Classification

    [https://arxiv.org/abs/2403.00888](https://arxiv.org/abs/2403.00888)

    该研究提出了一种基于边际差异的对抗训练方法，通过在多领域文本分类中进行理论分析和新的泛化界限的建立，解决了在MDTC算法设计中缺乏理论保证的挑战。

    

    多领域文本分类(MDTC)致力于利用相关领域的可用资源，提高目标领域的分类准确性。目前，大多数采用对抗训练和共享-私有范式的MDTC方法表现出尖端性能。然而，这些方法面临着一个不可忽视的挑战：在MDTC算法设计中缺乏理论保证。理论基础的缺乏给MDTC算法的发展造成了重大障碍。为了解决这一问题，我们首先通过将MDTC任务分解为多个领域自适应任务来提供MDTC的理论分析。我们将边际差异作为域差异的度量，并基于Rademacher复杂性建立了一个新的泛化界限。随后，我们提出了一种基于边际差异的对抗训练（MDAT）方法用于MDTC，符合我们的t

    arXiv:2403.00888v1 Announce Type: new  Abstract: Multi-domain text classification (MDTC) endeavors to harness available resources from correlated domains to enhance the classification accuracy of the target domain. Presently, most MDTC approaches that embrace adversarial training and the shared-private paradigm exhibit cutting-edge performance. Unfortunately, these methods face a non-negligible challenge: the absence of theoretical guarantees in the design of MDTC algorithms. The dearth of theoretical underpinning poses a substantial impediment to the advancement of MDTC algorithms. To tackle this problem, we first provide a theoretical analysis of MDTC by decomposing the MDTC task into multiple domain adaptation tasks. We incorporate the margin discrepancy as the measure of domain divergence and establish a new generalization bound based on Rademacher complexity. Subsequently, we propose a margin discrepancy-based adversarial training (MDAT) approach for MDTC, in accordance with our t
    
[^216]: SEGAA: 一种统一的方法来预测语音中的年龄、性别和情绪

    SEGAA: A Unified Approach to Predicting Age, Gender, and Emotion in Speech

    [https://arxiv.org/abs/2403.00887](https://arxiv.org/abs/2403.00887)

    本文提出了一种统一的方法来从语音中预测年龄、性别和情绪，通过深度学习模型探索了单一、多输出和顺序模型的比较，并提出了新颖的多输出学习架构。

    

    人类声音的解释在各种应用中都很重要。本研究探讨了从语音线索中预测年龄、性别和情绪，这是一个具有广泛应用的领域。声音分析技术的进展跨越各个领域，从改善客户互动到增强医疗保健和零售体验。辨识情绪有助于心理健康，而年龄和性别的检测在各种情境中至关重要。探索这些预测的深度学习模型涉及比较单一、多输出和顺序模型，这些模型在本文中得到了重点展示。寻找合适的数据提出了挑战，导致了CREMA-D和EMO-DB数据集的融合。以前的工作在个别预测方面表现出潜力，但有限的研究同时考虑了这三个变量。本文确定了个别模型方法中的缺陷，并倡导我们的新颖多输出学习架构Speech-based Emotion Gender。

    arXiv:2403.00887v1 Announce Type: cross  Abstract: The interpretation of human voices holds importance across various applications. This study ventures into predicting age, gender, and emotion from vocal cues, a field with vast applications. Voice analysis tech advancements span domains, from improving customer interactions to enhancing healthcare and retail experiences. Discerning emotions aids mental health, while age and gender detection are vital in various contexts. Exploring deep learning models for these predictions involves comparing single, multi-output, and sequential models highlighted in this paper. Sourcing suitable data posed challenges, resulting in the amalgamation of the CREMA-D and EMO-DB datasets. Prior work showed promise in individual predictions, but limited research considered all three variables simultaneously. This paper identifies flaws in an individual model approach and advocates for our novel multi-output learning architecture Speech-based Emotion Gender an
    
[^217]: 通过因果域转移评估和纠正决策支持系统的表现效果

    Evaluating and Correcting Performative Effects of Decision Support Systems via Causal Domain Shift

    [https://arxiv.org/abs/2403.00886](https://arxiv.org/abs/2403.00886)

    在高风险环境中部署决策支持系统时，本研究将其视为因果域转移，并提出新颖的跨域识别方法。

    

    当从特征X预测目标变量Y时，预测Y可能变得表现性强：一个代理可能根据这个预测采取行动，影响我们最终观察到的Y的值。 在高风险环境（例如医疗保健、法律、预测性执法或儿童福利筛选）部署决策支持系统（DSS）时，必须仔细评估DSS的表现效果。 我们提出将DSS的部署建模为因果域转移，并提供新颖的跨域识别结果。

    arXiv:2403.00886v1 Announce Type: new  Abstract: When predicting a target variable $Y$ from features $X$, the prediction $\hat{Y}$ can be performative: an agent might act on this prediction, affecting the value of $Y$ that we eventually observe. Performative predictions are deliberately prevalent in algorithmic decision support, where a Decision Support System (DSS) provides a prediction for an agent to affect the value of the target variable. When deploying a DSS in high-stakes settings (e.g. healthcare, law, predictive policing, or child welfare screening) it is imperative to carefully assess the performative effects of the DSS. In the case that the DSS serves as an alarm for a predicted negative outcome, naive retraining of the prediction model is bound to result in a model that underestimates the risk, due to effective workings of the previous model. In this work, we propose to model the deployment of a DSS as causal domain shift and provide novel cross-domain identification result
    
[^218]: FedRDMA：基于分块RDMA传输的少交流跨边缘联邦LLM通信高效系统

    FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission

    [https://arxiv.org/abs/2403.00881](https://arxiv.org/abs/2403.00881)

    FedRDMA是一个基于分块RDMA传输的高效跨边缘联邦学习系统，通过优化技术提高了通信效率，相比传统系统可实现最多3.8倍的速度提升。

    

    通信开销是联邦学习（FL）中的一个重要瓶颈，随着AI模型规模的增加，这一瓶颈越来越明显。本文提出了FedRDMA，一个通信高效的跨边缘FL系统，将RDMA集成到FL通信协议中。为了克服RDMA在广域网（WANs）中的限制，FedRDMA将更新后的模型分成块，并设计了一系列优化技术来改善基于RDMA的通信的效率和鲁棒性。我们在工业联邦学习框架上实现了FedRDMA，并在实际跨边缘FL场景中进行了评估。实验结果表明，与传统基于TCP/IP的FL系统相比，\sys的通信效率最多可以提高3.8倍。

    arXiv:2403.00881v1 Announce Type: new  Abstract: Communication overhead is a significant bottleneck in federated learning (FL), which has been exaggerated with the increasing size of AI models. In this paper, we propose FedRDMA, a communication-efficient cross-silo FL system that integrates RDMA into the FL communication protocol. To overcome the limitations of RDMA in wide-area networks (WANs), FedRDMA divides the updated model into chunks and designs a series of optimization techniques to improve the efficiency and robustness of RDMA-based communication. We implement FedRDMA atop the industrial federated learning framework and evaluate it on a real-world cross-silo FL scenario. The experimental results show that \sys can achieve up to 3.8$\times$ speedup in communication efficiency compared to traditional TCP/IP-based FL systems.
    
[^219]: Disaggregated Multi-Tower: 面向拓扑感知的高效大规模推荐建模技术

    Disaggregated Multi-Tower: Topology-aware Modeling Technique for Efficient Large-Scale Recommendation

    [https://arxiv.org/abs/2403.00877](https://arxiv.org/abs/2403.00877)

    Disaggregated Multi-Tower提出了一种面向拓扑感知的建模技术，通过SPTT、TM和TP三个组件实现了高效的大规模推荐，加速性能提升了1.9倍。

    

    我们研究了深度学习推荐模型的扁平架构、常见的分布式训练模式和分层数据中心拓扑之间的不匹配。为了解决相关的低效性，我们提出了Disaggregated Multi-Tower（DMT），这是一种建模技术，包括（1）语义保留的Tower Transform（SPTT），一个将单片全局嵌入查找过程分解为不相交塔以利用数据中心位置关系的新型训练模式；（2）Tower Module（TM），一个附加到每个塔的协同稠密组件，通过分层特征交互降低模型复杂性和通信量；和（3）Tower Partitioner（TP），一个特征分区器，系统地创建具有有意义特征交互和负载平衡分配的塔，通过学习的嵌入来保持模型质量和训练吞吐量。我们展示了DMT相比于最新的方法可以实现高达1.9倍的加速。

    arXiv:2403.00877v1 Announce Type: new  Abstract: We study a mismatch between the deep learning recommendation models' flat architecture, common distributed training paradigm and hierarchical data center topology. To address the associated inefficiencies, we propose Disaggregated Multi-Tower (DMT), a modeling technique that consists of (1) Semantic-preserving Tower Transform (SPTT), a novel training paradigm that decomposes the monolithic global embedding lookup process into disjoint towers to exploit data center locality; (2) Tower Module (TM), a synergistic dense component attached to each tower to reduce model complexity and communication volume through hierarchical feature interaction; and (3) Tower Partitioner (TP), a feature partitioner to systematically create towers with meaningful feature interactions and load balanced assignments to preserve model quality and training throughput via learned embeddings. We show that DMT can achieve up to 1.9x speedup compared to the state-of-th
    
[^220]: 通过蛋白数据增强增强蛋白预测模型：基准和新方向

    Enhancing Protein Predictive Models via Proteins Data Augmentation: A Benchmark and New Directions

    [https://arxiv.org/abs/2403.00875](https://arxiv.org/abs/2403.00875)

    本文将图片和文本的数据增强技术扩展到蛋白领域，提出了两种新的蛋白语义级增强方法，并将这些增强方法集成到一个增强池中，构建了一个名为自动蛋白增强（APA）的简单而有效的框架。

    

    数据增强是利用少量标记蛋白数据的有效替代方法。然而，大多数现有工作侧重于设计新的架构或预训练任务，对于蛋白的数据增强研究相对较少。本文将先前用于图像和文本的数据增强技术扩展到蛋白，然后在各种与蛋白相关的任务上对这些技术进行基准测试，提供了对蛋白增强的首次全面评估。此外，我们提出两种新的语义级蛋白增强方法，即集成梯度替换和回译替换，通过显著性检测和生物知识实现蛋白语义感知增强。最后，我们将扩展和提出的增强集成到一个增强池中，并提出了一个简单但有效的框架，即自动蛋白增强（APA），可对蛋白进行自动增强。

    arXiv:2403.00875v1 Announce Type: cross  Abstract: Augmentation is an effective alternative to utilize the small amount of labeled protein data. However, most of the existing work focuses on design-ing new architectures or pre-training tasks, and relatively little work has studied data augmentation for proteins. This paper extends data augmentation techniques previously used for images and texts to proteins and then benchmarks these techniques on a variety of protein-related tasks, providing the first comprehensive evaluation of protein augmentation. Furthermore, we propose two novel semantic-level protein augmentation methods, namely Integrated Gradients Substitution and Back Translation Substitution, which enable protein semantic-aware augmentation through saliency detection and biological knowledge. Finally, we integrate extended and proposed augmentations into an augmentation pool and propose a simple but effective framework, namely Automated Protein Augmentation (APA), which can a
    
[^221]: 区块链赋能的联邦学习: 好处、挑战和解决方案

    Blockchain-empowered Federated Learning: Benefits, Challenges, and Solutions

    [https://arxiv.org/abs/2403.00873](https://arxiv.org/abs/2403.00873)

    区块链技术被整合到联邦学习系统中以提供更强的安全性、公平性和可扩展性，但也引入了额外的网络、计算和存储资源需求。

    

    联邦学习(FL)是一种分布式机器学习方法，通过在客户端本地训练模型并在参数服务器上进行聚合来保护用户数据隐私。尽管在保护隐私方面有效，但FL系统面临单点故障、缺乏激励和不足的安全性等局限性。为了应对这些挑战，将区块链技术整合到FL系统中，以提供更强的安全性、公平性和可扩展性。然而，区块链赋能的FL(BC-FL)系统对网络、计算和存储资源提出了额外的需求。本调查全面审查了最近关于BC-FL系统的研究，分析了与区块链整合相关的好处和挑战。我们探讨了区块链为何适用于FL，如何实施以及整合的挑战和现有解决方案。此外，我们还提供了关于未来研究方向的见解。

    arXiv:2403.00873v1 Announce Type: cross  Abstract: Federated learning (FL) is a distributed machine learning approach that protects user data privacy by training models locally on clients and aggregating them on a parameter server. While effective at preserving privacy, FL systems face limitations such as single points of failure, lack of incentives, and inadequate security. To address these challenges, blockchain technology is integrated into FL systems to provide stronger security, fairness, and scalability. However, blockchain-empowered FL (BC-FL) systems introduce additional demands on network, computing, and storage resources. This survey provides a comprehensive review of recent research on BC-FL systems, analyzing the benefits and challenges associated with blockchain integration. We explore why blockchain is applicable to FL, how it can be implemented, and the challenges and existing solutions for its integration. Additionally, we offer insights on future research directions fo
    
[^222]: 教会大型语言模型进行钓鱼：从语言模型中窃取私人信息

    Teach LLMs to Phish: Stealing Private Information from Language Models

    [https://arxiv.org/abs/2403.00871](https://arxiv.org/abs/2403.00871)

    本研究提出了一种名为“神经钓鱼”的新型实用数据提取攻击，使对手能够成功地从大型语言模型中提取敏感信息，攻击成功率高达10%至50%。

    

    当大型语言模型在私人数据上训练时，它们可能会将敏感信息记忆并重复。本研究提出了一种新的实用数据提取攻击，称为“神经钓鱼”。这种攻击使对手能够从一个在用户数据上训练的模型中成功率高达10%甚至50%地提取敏感或可识别个人身份的信息，例如信用卡号。攻击仅假设对手可以将少量看似良性的句子插入训练数据集，仅使用对用户数据结构的模糊先验知识。

    arXiv:2403.00871v1 Announce Type: cross  Abstract: When large language models are trained on private data, it can be a significant privacy risk for them to memorize and regurgitate sensitive information. In this work, we propose a new practical data extraction attack that we call "neural phishing". This attack enables an adversary to target and extract sensitive or personally identifiable information (PII), e.g., credit card numbers, from a model trained on user data with upwards of 10% attack success rates, at times, as high as 50%. Our attack assumes only that an adversary can insert as few as 10s of benign-appearing sentences into the training dataset using only vague priors on the structure of the user data.
    
[^223]: 利用互信息驱动的跨变量和时间建模来增强多元时间序列预测

    Enhancing Multivariate Time Series Forecasting with Mutual Information-driven Cross-Variable and Temporal Modeling

    [https://arxiv.org/abs/2403.00869](https://arxiv.org/abs/2403.00869)

    引入了CDAM和TAM模型以改进多元时间序列预测，通过最小化冗余信息并增强互信息，利用时间相关性。

    

    最近的进展强调了深度学习技术对多元时间序列预测（MTSF）的影响。通常，这些技术被分为两类：通道独立和通道混合方法。虽然通道独立方法通常产生更好的结果，但通道混合理论上可以通过利用变量间的相关性来提供改进。然而，我们认为在通道混合方法中整合不相关信息可能会削弱MTSF模型性能的潜在增强。为了证实这一观点，我们介绍了用于通道混合方法的跨变量去相关感知特征建模（CDAM），旨在通过最小化通道间的冗余信息同时增强相关的互信息来改进通道混合。此外，我们引入了时序相关感知建模（TAM）来利用时间相关性，这是一个步骤

    arXiv:2403.00869v1 Announce Type: new  Abstract: Recent advancements have underscored the impact of deep learning techniques on multivariate time series forecasting (MTSF). Generally, these techniques are bifurcated into two categories: Channel-independence and Channel-mixing approaches. Although Channel-independence methods typically yield better results, Channel-mixing could theoretically offer improvements by leveraging inter-variable correlations. Nonetheless, we argue that the integration of uncorrelated information in channel-mixing methods could curtail the potential enhancement in MTSF model performance. To substantiate this claim, we introduce the Cross-variable Decorrelation Aware feature Modeling (CDAM) for Channel-mixing approaches, aiming to refine Channel-mixing by minimizing redundant information between channels while enhancing relevant mutual information. Furthermore, we introduce the Temporal correlation Aware Modeling (TAM) to exploit temporal correlations, a step be
    
[^224]: 梯度被罚：通过探索拒绝损失地形图来检测针对大语言模型的越狱攻击

    Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes

    [https://arxiv.org/abs/2403.00867](https://arxiv.org/abs/2403.00867)

    本文提出了一种名为Gradient Cuff的方法，通过探索拒绝损失地形图来检测对大语言模型的越狱攻击，成功设计了一种有效的两步检测策略。

    

    大型语言模型（LLMs）正成为一种突出的生成式AI工具，用户输入查询，LLM生成答案。为了减少伤害和滥用，人们通过使用先进的训练技术如来自人类反馈的强化学习（RLHF）来将这些LLMs与人类价值观保持一致。然而，最近的研究突显了LLMs对于试图颠覆嵌入的安全防护措施的对抗性越狱尝试的脆弱性。为了解决这一挑战，本文定义并调查了LLMs的拒绝损失，然后提出了一种名为Gradient Cuff的方法来检测越狱尝试。Gradient Cuff利用拒绝损失地形图中观察到的独特特性，包括功能值及其光滑性，设计了一种有效的两步检测策略。

    arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
    
[^225]: 基于遗传编程的损失函数学习的快速高效局部搜索

    Fast and Efficient Local Search for Genetic Programming Based Loss Function Learning

    [https://arxiv.org/abs/2403.00865](https://arxiv.org/abs/2403.00865)

    提出了一种新的基于遗传编程的元学习框架，通过局部搜索方法实现了任务和模型无关的损失函数学习，实验证实了该框架在各种监督学习任务上的多样性和性能。

    

    在本文中，我们深入探讨了损失函数学习的话题，这是一种新兴的元学习范式，旨在学习能显著改善经过其训练的模型性能的损失函数。具体来说，我们提出了一种新的元学习框架，通过混合搜索方法实现了任务和模型无关的损失函数学习。该框架首先使用遗传编程找到一组符号损失函数。其次，学习到的损失函数集合随后通过展开的微分进行参数化和优化。所提出的框架的多样性和性能经过实证验证，用于各种监督学习任务。实验结果表明，学习到的损失函数在表格、计算机视觉和自然语言处理问题上带来了改善的收敛性、样本效率和推理性能，使用各种特定任务的神经网络架构。

    arXiv:2403.00865v1 Announce Type: cross  Abstract: In this paper, we develop upon the topic of loss function learning, an emergent meta-learning paradigm that aims to learn loss functions that significantly improve the performance of the models trained under them. Specifically, we propose a new meta-learning framework for task and model-agnostic loss function learning via a hybrid search approach. The framework first uses genetic programming to find a set of symbolic loss functions. Second, the set of learned loss functions is subsequently parameterized and optimized via unrolled differentiation. The versatility and performance of the proposed framework are empirically validated on a diverse set of supervised learning tasks. Results show that the learned loss functions bring improved convergence, sample efficiency, and inference performance on tabulated, computer vision, and natural language processing problems, using a variety of task-specific neural network architectures.
    
[^226]: 用深度生成技术重构零售供应链：分类法、调研和洞见

    Pivoting Retail Supply Chain with Deep Generative Techniques: Taxonomy, Survey and Insights

    [https://arxiv.org/abs/2403.00861](https://arxiv.org/abs/2403.00861)

    本文旨在研究如何利用深度生成模型（DGMs）重构现代零售供应链，通过提供DGMs的分类法、零售供应链中的应用案例回顾以及潜在利用DGMs解决零售问题的讨论。

    

    生成式人工智能应用，如ChatGPT或DALL-E，展示了它们在生成类似人类文本或图像方面的令人印象深刻的能力。深入研究，这些AI应用的科学利益相关者是深度生成模型，即DGMs，旨在学习数据的潜在分布并生成与原始数据集在统计上相似的新数据点。一个关键问题是如何将DGMs应用于现代零售供应链领域？为了回答这个问题，本文旨在全面审查DGMs，并讨论它们在零售供应链中的现有和潜在用例，方法是(1)提供最先进的DGMs及其变体的分类法和概述，(2)从端到端的视角回顾现有DGM在零售供应链中的应用，以及(3)讨论关于如何进一步利用DGM解决零售问题的见解和潜在方向。

    arXiv:2403.00861v1 Announce Type: new  Abstract: Generative AI applications, such as ChatGPT or DALL-E, have shown the world their impressive capabilities in generating human-like text or image. Diving deeper, the science stakeholder for those AI applications are Deep Generative Models, a.k.a DGMs, which are designed to learn the underlying distribution of the data and generate new data points that are statistically similar to the original dataset. One critical question is raised: how can we leverage DGMs into morden retail supply chain realm? To address this question, this paper expects to provide a comprehensive review of DGMs and discuss their existing and potential usecases in retail supply chain, by (1) providing a taxonomy and overview of state-of-the-art DGMs and their variants, (2) reviewing existing DGM applications in retail supply chain from a end-to-end view of point, and (3) discussing insights and potential directions on how DGMs can be further utilized on solving retail 
    
[^227]: 深度神经网络激活区域的精确枚举并行算法

    Parallel Algorithms for Exact Enumeration of Deep Neural Network Activation Regions

    [https://arxiv.org/abs/2403.00860](https://arxiv.org/abs/2403.00860)

    本研究提出了深度（和浅层）神经网络中精确枚举的并行算法，主要贡献包括新颖的算法框架和并行算法，实现了其中一种算法在多种网络架构上，并实验证明区域数量对运行时间的影响。

    

    一种使用修正线性单元的前馈神经网络通过将其输入空间划分为一组凸区域来构建从输入到输出的映射，区域内的点共享单一仿射变换。为了理解神经网络的工作原理、失败原因以及与生物智能的比较，我们需要理解这些区域的组织和形成。本文介绍了精确枚举深度（和浅层）神经网络的并行算法。

    arXiv:2403.00860v1 Announce Type: cross  Abstract: A feedforward neural network using rectified linear units constructs a mapping from inputs to outputs by partitioning its input space into a set of convex regions where points within a region share a single affine transformation. In order to understand how neural networks work, when and why they fail, and how they compare to biological intelligence, we need to understand the organization and formation of these regions. Step one is to design and implement algorithms for exact region enumeration in networks beyond toy examples.   In this work, we present parallel algorithms for exact enumeration in deep (and shallow) neural networks. Our work has three main contributions: (1) we present a novel algorithm framework and parallel algorithms for region enumeration; (2) we implement one of our algorithms on a variety of network architectures and experimentally show how the number of regions dictates runtime; and (3) we show, using our algorit
    
[^228]: 直接与Chat-Fine-Tuned LLMs的草案模型对齐

    Direct Alignment of Draft Model for Speculative Decoding with Chat-Fine-Tuned LLMs

    [https://arxiv.org/abs/2403.00858](https://arxiv.org/abs/2403.00858)

    通过提出的框架，我们训练了一种用于Llama 2 Chat 7B或更大模型的草案模型，实现了加速推理，仅占原始大小的1.64％。

    

    文本生成与大型语言模型（LLMs）由于其自回归本质、巨大的参数数量和有限的内存带宽而被认为是内存密集型，通常导致低令牌速率。猜测解码已被提出作为LLM推理加速的解决方案。然而，在现代开源LLM系列中，例如Llama 2 7B，由于草案模型通常不可用，因此需要训练高质量的草案模型以通过猜测解码实现推理加速。在本文中，我们提出了一个简单的草案模型训练框架，用于直接与Chat-capable目标模型对齐。通过我们提出的框架，我们训练出Llama 2 Chat Drafter 115M，这是一个适用于Llama 2 Chat 7B或更大模型的草案模型，仅占原始大小的1.64％。我们的训练框架仅包括预训练、蒸馏数据集生成和使用知识蒸馏进行微调，没有额外的对齐步骤。

    arXiv:2403.00858v1 Announce Type: cross  Abstract: Text generation with Large Language Models (LLMs) is known to be memory bound due to the combination of their auto-regressive nature, huge parameter counts, and limited memory bandwidths, often resulting in low token rates. Speculative decoding has been proposed as a solution for LLM inference acceleration. However, since draft models are often unavailable in the modern open-source LLM families, e.g., for Llama 2 7B, training a high-quality draft model is required to enable inference acceleration via speculative decoding. In this paper, we propose a simple draft model training framework for direct alignment to chat-capable target models. With the proposed framework, we train Llama 2 Chat Drafter 115M, a draft model for Llama 2 Chat 7B or larger, with only 1.64\% of the original size. Our training framework only consists of pretraining, distillation dataset generation, and finetuning with knowledge distillation, with no additional align
    
[^229]: 使用自监督变压器和多任务学习进行说话者无关的运动障碍严重程度分类

    Speaker-Independent Dysarthria Severity Classification using Self-Supervised Transformers and Multi-Task Learning

    [https://arxiv.org/abs/2403.00854](https://arxiv.org/abs/2403.00854)

    提出了一种使用自监督变压器和多任务学习进行说话者无关的运动障碍严重度分类的方法，可自动评估运动障碍的严重程度。

    

    运动障碍是由于神经系统疾病导致言语肌肉控制能力受损而产生的一种状况，严重影响患者的沟通和生活质量。本研究提出了一种基于变压器的框架，可以从原始语音数据中自动评估运动障碍的严重程度。相较于传统需要人类专家评估的方法，它可以提供客观、可重复、可访问、标准化和具有成本效益的评估。

    arXiv:2403.00854v1 Announce Type: cross  Abstract: Dysarthria, a condition resulting from impaired control of the speech muscles due to neurological disorders, significantly impacts the communication and quality of life of patients. The condition's complexity, human scoring and varied presentations make its assessment and management challenging. This study presents a transformer-based framework for automatically assessing dysarthria severity from raw speech data. It can offer an objective, repeatable, accessible, standardised and cost-effective and compared to traditional methods requiring human expert assessors. We develop a transformer framework, called Speaker-Agnostic Latent Regularisation (SALR), incorporating a multi-task learning objective and contrastive learning for speaker-independent multi-class dysarthria severity classification. The multi-task framework is designed to reduce reliance on speaker-specific characteristics and address the intrinsic intra-class variability of d
    
[^230]: 在偏差梯度估计下的分布式动量方法

    Distributed Momentum Methods Under Biased Gradient Estimations

    [https://arxiv.org/abs/2403.00853](https://arxiv.org/abs/2403.00853)

    本文在偏差梯度估计下建立了关于一般非凸和$\mu$-PL非凸问题的分布式动量方法的非渐近收敛界限，覆盖了一般分布式优化问题的分析，并揭示了梯度估计有偏时的特殊情况下的影响，即在元学习和梯度被压缩或剪切时。

    

    分布式随机梯度方法在解决涉及分布在多个节点上的数据的大规模机器学习问题中日益受到重视。然而，在许多分布式机器学习应用中，获得无偏的随机梯度，这是大多数理论研究的重点，是具有挑战性的。梯度估计很容易变得有偏，例如，在梯度被压缩或剪切时，数据被洗牌时，以及在元学习和强化学习中。

    arXiv:2403.00853v1 Announce Type: new  Abstract: Distributed stochastic gradient methods are gaining prominence in solving large-scale machine learning problems that involve data distributed across multiple nodes. However, obtaining unbiased stochastic gradients, which have been the focus of most theoretical research, is challenging in many distributed machine learning applications. The gradient estimations easily become biased, for example, when gradients are compressed or clipped, when data is shuffled, and in meta-learning and reinforcement learning. In this work, we establish non-asymptotic convergence bounds on distributed momentum methods under biased gradient estimation on both general non-convex and $\mu$-PL non-convex problems. Our analysis covers general distributed optimization problems, and we work out the implications for special cases where gradient estimates are biased, i.e., in meta-learning and when the gradients are compressed or clipped. Our numerical experiments on 
    
[^231]: NeuraLUT: 在Boolean合成函数中隐藏神经网络密度

    NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable Functions

    [https://arxiv.org/abs/2403.00849](https://arxiv.org/abs/2403.00849)

    改进了FPGA加速神经网络推断任务的方法，提出将整个子网络映射到单个LUT中，使得神经网络拓扑和精度不再影响生成的查找表的大小。

    

    可编程门阵列（FPGA）加速器已经证明在处理延迟和资源关键的深度神经网络（DNN）推断任务方面取得了成功。神经网络中计算密集度最高的操作之一是特征和权重向量之间的点积。因此，一些先前的FPGA加速工作提出将具有量化输入和输出的神经元直接映射到查找表（LUTs）以进行硬件实现。在这些工作中，神经元的边界与LUTs的边界重合。我们建议放宽这些边界，将整个子网络映射到单个LUT。由于子网络被吸收到LUT中，分区内的神经网络拓扑和精度不会影响生成的查找表的大小。因此，我们在每个分区内使用具有浮点精度的全连接层，这些层受益于成为通用函数逼近器。

    arXiv:2403.00849v1 Announce Type: cross  Abstract: Field-Programmable Gate Array (FPGA) accelerators have proven successful in handling latency- and resource-critical deep neural network (DNN) inference tasks. Among the most computationally intensive operations in a neural network (NN) is the dot product between the feature and weight vectors. Thus, some previous FPGA acceleration works have proposed mapping neurons with quantized inputs and outputs directly to lookup tables (LUTs) for hardware implementation. In these works, the boundaries of the neurons coincide with the boundaries of the LUTs. We propose relaxing these boundaries and mapping entire sub-networks to a single LUT. As the sub-networks are absorbed within the LUT, the NN topology and precision within a partition do not affect the size of the lookup tables generated. Therefore, we utilize fully connected layers with floating-point precision inside each partition, which benefit from being universal function approximators, 
    
[^232]: 改进的在线学习算法用于广告拍卖中的点击率预测

    Improved Online Learning Algorithms for CTR Prediction in Ad Auctions

    [https://arxiv.org/abs/2403.00845](https://arxiv.org/abs/2403.00845)

    该研究致力于解决广告拍卖中的在线学习问题，提出了针对广告主两种不同战略行为模型的在线机制，其中一种在最坏情况下取得了紧致的遗憾界限，另一种则解决了更复杂的长期效用优化问题。

    

    在这项工作中，我们研究了广告拍卖中收入最大化的在线学习问题，其中卖方需要学习每个广告候选的点击率(CTR)，并通过按点击付费的方式收取获胜者的价格。我们关注广告主的两种战略行为模型。首先，我们假设广告主完全近视；即在每一轮中，他们只针对当前轮次最大化他们的效用。在这种情况下，我们基于置信上界开发了一个在线机制，在最坏情况下实现了严格的$O(\sqrt{T})$遗憾，当值在所有拍卖中静态并且最高期望值(即值乘以他们的CTR)与次高期望值广告之间存在差距时，存在负遗憾。接下来，我们假设广告主是非近视的，并关心他们的长期效用。这种设置要复杂得多，因为广告主有动机

    arXiv:2403.00845v1 Announce Type: cross  Abstract: In this work, we investigate the online learning problem of revenue maximization in ad auctions, where the seller needs to learn the click-through rates (CTRs) of each ad candidate and charge the price of the winner through a pay-per-click manner. We focus on two models of the advertisers' strategic behaviors. First, we assume that the advertiser is completely myopic; i.e.~in each round, they aim to maximize their utility only for the current round. In this setting, we develop an online mechanism based on upper-confidence bounds that achieves a tight $O(\sqrt{T})$ regret in the worst-case and negative regret when the values are static across all the auctions and there is a gap between the highest expected value (i.e.~value multiplied by their CTR) and second highest expected value ad. Next, we assume that the advertiser is non-myopic and cares about their long term utility. This setting is much more complex since an advertiser is incen
    
[^233]: 下-左部分AUC：一种用于推荐系统的有效和高效优化指标

    Lower-Left Partial AUC: An Effective and Efficient Optimization Metric for Recommendation

    [https://arxiv.org/abs/2403.00844](https://arxiv.org/abs/2403.00844)

    提出了一种新的优化指标Lower-Left Partial AUC（LLPAUC），在计算效率上类似于AUC，但与Top-K排名指标强相关，能在推荐系统中有效提升性能。

    

    优化指标对于构建大规模推荐系统至关重要。然而，一种实用的有效和高效指标仍然难以找到。尽管Top-K排名指标是优化的黄金标准，但它们存在着显着的计算开销。相比之下，更高效的准确性和AUC指标往往无法捕捉推荐任务的真正目标，导致性能亚优。为了克服这一困境，我们提出了一种新的优化指标，Lower-Left Partial AUC（LLPAUC），它在计算效率上类似于AUC，但与Top-K排名指标强相关。与AUC相比，LLPAUC仅考虑ROC曲线下方的局部区域，以将优化焦点放在Top-K上。我们提供了LLPAUC与Top-K排名指标之间的相关性的理论验证，并展示了其对嘈杂用户反馈的稳健性。

    arXiv:2403.00844v1 Announce Type: cross  Abstract: Optimization metrics are crucial for building recommendation systems at scale. However, an effective and efficient metric for practical use remains elusive. While Top-K ranking metrics are the gold standard for optimization, they suffer from significant computational overhead. Alternatively, the more efficient accuracy and AUC metrics often fall short of capturing the true targets of recommendation tasks, leading to suboptimal performance. To overcome this dilemma, we propose a new optimization metric, Lower-Left Partial AUC (LLPAUC), which is computationally efficient like AUC but strongly correlates with Top-K ranking metrics. Compared to AUC, LLPAUC considers only the partial area under the ROC curve in the Lower-Left corner to push the optimization focus on Top-K. We provide theoretical validation of the correlation between LLPAUC and Top-K ranking metrics and demonstrate its robustness to noisy user feedback. We further design an 
    
[^234]: 利用双层可学习大型语言模型规划增强长期推荐

    Enhancing Long-Term Recommendation with Bi-level Learnable Large Language Model Planning

    [https://arxiv.org/abs/2403.00843](https://arxiv.org/abs/2403.00843)

    利用大型语言模型的规划能力来增强长期推荐，使模型在个性化推荐中更有效地理解和应用任务解决原则

    

    传统推荐系统倾向于过分迎合用户的即时兴趣而忽视他们的长期参与。 为了解决这个问题，在推荐决策过程中合并规划能力是至关重要的，以开发能够同时考虑即时兴趣和长期参与的策略。本文提出利用大型语言模型（LLMs）对稀疏数据的显著规划能力用于长期推荐。关键在于使语言模型能够在个性化推荐场景中有效理解和应用任务解决原则，因为模型的预训练可能并未自然包含这些内容。

    arXiv:2403.00843v1 Announce Type: cross  Abstract: Traditional recommendation setting tends to excessively cater to users' immediate interests and neglect their long-term engagement. To address it, it is crucial to incorporate planning capabilities into the recommendation decision-making process to develop policies that take into account both immediate interests and long-term engagement. Despite Reinforcement Learning (RL) can learn planning capacity by maximizing cumulative reward, the scarcity of recommendation data presents challenges such as instability and susceptibility to overfitting when training RL models from scratch.   In this context, we propose to leverage the remarkable planning capabilities over sparse data of Large Language Models (LLMs) for long-term recommendation. The key lies in enabling a language model to understand and apply task-solving principles effectively in personalized recommendation scenarios, as the model's pre-training may not naturally encompass these 
    
[^235]: 竞争游戏的离线虚构自我对弈

    Offline Fictitious Self-Play for Competitive Games

    [https://arxiv.org/abs/2403.00841](https://arxiv.org/abs/2403.00841)

    本文介绍了Off-FSP，这是竞争游戏的第一个实用的无模型离线RL算法，通过调整固定数据集的权重，使用重要性抽样，模拟与各种对手的互动。

    

    离线强化学习（RL）因其在以前收集的数据集中改进策略而不需要在线交互的能力而受到重视。尽管在单一智能体设置中取得成功，但离线多智能体RL仍然是一个挑战，特别是在竞争游戏中。为了解决这些问题，本文介绍了Off-FSP，这是竞争游戏的第一个实用的无模型离线RL算法。我们首先通过调整固定数据集的权重，使用重要性抽样模拟与各种对手的互动。

    arXiv:2403.00841v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ
    
[^236]: 基于深度学习的大型语言模型生成科学内容的检测方法

    Deep Learning Detection Method for Large Language Models-Generated Scientific Content

    [https://arxiv.org/abs/2403.00828](https://arxiv.org/abs/2403.00828)

    提出了一种新的ChatGPT生成科学文本检测方法AI-Catcher，该方法集成了多层感知器（MLP）和卷积神经网络（CNN），是一个多模态模型，用于检测大型语言模型生成的科学内容。

    

    Large Language Models (LLMs), 如GPT-3和BERT，改变了文本内容的写作和传播方式。这些模型有潜力生成与人类写作无法区分的科学内容。因此，LLMs会给科学界带来严重后果，科学界依赖于出版物的完整性和可靠性。本研究提出了一种新颖的ChatGPT生成的科学文本检测方法，名为AI-Catcher。AI-Catcher集成了两个深度学习模型，多层感知器（MLP）和卷积神经网络（CNN）。MLP学习语言和统计特征的特征表示。CNN从文本内容中提取顺序模式的高级表示。AI-Catcher是一个多模态模型，融合了MLP和CNN导出的隐藏模式。此外，还收集了一个新的ChatGPT生成的科学文本数据集来增强AI生成的文本。

    arXiv:2403.00828v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual content is written and communicated. These models have the potential to generate scientific content that is indistinguishable from that written by humans. Hence, LLMs carry severe consequences for the scientific community, which relies on the integrity and reliability of publications. This research paper presents a novel ChatGPT-generated scientific text detection method, AI-Catcher. AI-Catcher integrates two deep learning models, multilayer perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the feature representations of the linguistic and statistical features. The CNN extracts high-level representations of the sequential patterns from the textual content. AI-Catcher is a multimodal model that fuses hidden patterns derived from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset is collected to enhance AI-generated tex
    
[^237]: 来自外部代理指标反馈的语言模型自我完善

    Self-Refinement of Language Models from External Proxy Metrics Feedback

    [https://arxiv.org/abs/2403.00827](https://arxiv.org/abs/2403.00827)

    本文提出了Proxy Metric-based Self-Refinement (ProMiSe)方法，通过外部指标反馈指导语言模型在质量关键维度上进行自我完善，从而改进响应质量。

    

    在文档为基础的响应生成中，期望代理响应不仅与用户的查询相关，还与给定的文档相关。本文引入了基于代理指标的自我完善（ProMiSe），使得大型语言模型能够沿着外部指标反馈引导的质量关键维度优化其初始响应，从而产生更好的最终响应。

    arXiv:2403.00827v1 Announce Type: cross  Abstract: It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning 
    
[^238]: LLMGuard：防范不安全的LLM行为

    LLMGuard: Guarding Against Unsafe LLM Behavior

    [https://arxiv.org/abs/2403.00826](https://arxiv.org/abs/2403.00826)

    LLMGuard是一个监视用户与LLM应用程序互动的工具，可标记违背特定行为或对话主题的内容。

    

    尽管大型语言模型(LLMs)在企业环境中的兴起带来了新的机遇和能力，但也带来了挑战，例如生成不当、偏倚或误导性内容的风险，该内容违反规定并可能涉及法律问题。为了缓解这一问题，我们提出了“LLMGuard”，这是一个工具，可监视用户与LLM应用程序的互动，并标记违背特定行为或对话主题的内容。为了做到这一点，LLMGuard采用了一组探测器。

    arXiv:2403.00826v1 Announce Type: new  Abstract: Although the rise of Large Language Models (LLMs) in enterprise settings brings new opportunities and capabilities, it also brings challenges, such as the risk of generating inappropriate, biased, or misleading content that violates regulations and can have legal concerns. To alleviate this, we present "LLMGuard", a tool that monitors user interactions with an LLM application and flags content against specific behaviours or conversation topics. To do this robustly, LLMGuard employs an ensemble of detectors.
    
[^239]: 社交媒体作为传感器：利用自然语言处理分析推特数据以研究乳腺癌药物效果

    Social Media as a Sensor: Analyzing Twitter Data for Breast Cancer Medication Effects Using Natural Language Processing

    [https://arxiv.org/abs/2403.00821](https://arxiv.org/abs/2403.00821)

    本文利用自然语言处理分析推特数据，发展了一种基于Transformer的分类器来识别乳腺癌患者/幸存者，并设计了多层规则模型以研究乳腺癌疗法效果。

    

    乳腺癌是一个重要的公共卫生问题，也是妇女癌症相关死亡的主要原因。尽管乳腺癌治疗取得了进展，药物不依从仍然是一个主要问题。由于电子健康记录通常不捕捉可能揭示关于药物相关经历的患者报告的结果，社交媒体为增进我们对患者治疗经历的理解提供了有吸引力的资源。本文开发了基于自然语言处理（NLP）的方法来研究社交媒体上自动策划的乳腺癌队列发布的信息。我们使用基于Transformer的分类器识别自我报告信息的乳腺癌患者/幸存者，我们从其个人资料中收集了纵向数据。然后，我们设计了一个多层规则模型来开发与乳腺癌疗法相关的sid

    arXiv:2403.00821v1 Announce Type: new  Abstract: Breast cancer is a significant public health concern and is the leading cause of cancer-related deaths among women. Despite advances in breast cancer treatments, medication non-adherence remains a major problem. As electronic health records do not typically capture patient-reported outcomes that may reveal information about medication-related experiences, social media presents an attractive resource for enhancing our understanding of the patients' treatment experiences. In this paper, we developed natural language processing (NLP) based methodologies to study information posted by an automatically curated breast cancer cohort from social media. We employed a transformer-based classifier to identify breast cancer patients/survivors on X (Twitter) based on their self-reported information, and we collected longitudinal data from their profiles. We then designed a multi-layer rule-based model to develop a breast cancer therapy-associated sid
    
[^240]: DenseMamba: 具有密集隐藏连接的状态空间模型，用于高效大型语言模型

    DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models

    [https://arxiv.org/abs/2403.00818](https://arxiv.org/abs/2403.00818)

    DenseSSM是一种新方法，通过密集连接增强了状态空间模型(SSM)，有效地提升了各层之间隐藏信息的流动，在保持训练并行性和推理效率的同时，取得了显著的性能提升。

    

    大型语言模型(LLMs)面临着由普遍使用的Transformer架构过高的计算和内存需求而带来的巨大挑战。而状态空间模型(SSM)是一种新型基础网络架构，具有较低的计算复杂度，但其性能尚未完全能与Transformer相媲美。本文引入了DenseSSM，一种增强SSMs中各层之间隐藏信息流动的新方法。通过有选择地将浅层隐藏状态集成到更深层，DenseSSM保留了对最终输出至关重要的细粒度信息。密集连接增强的DenseSSM仍保持了训练的并行性和推理效率。该方法可以广泛适用于RetNet和Mamba等各种SSM类型。在相似的模型大小下，DenseSSM取得了显著的改进，例如DenseRetNet比原始RetNet提高了高达5%的准确率。

    arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac
    
[^241]: 双重校准估计器用于缺失数据不随机推荐

    Doubly Calibrated Estimator for Recommendation on Data Missing Not At Random

    [https://arxiv.org/abs/2403.00817](https://arxiv.org/abs/2403.00817)

    提出了双重校准估计器，通过校准插补和概率模型来解决推荐系统中缺失数据不随机的挑战

    

    推荐系统往往受到选择偏差的影响，因为用户倾向于评价他们喜欢的物品。在这种条件下收集的数据集表现出不随机缺失的条目，因此不是代表目标人群的随机对照试验。为了解决这一挑战，提出了双重稳健估计器及其增强型变体，因为它们确保在提供准确的插补误差或预测概率时无偏。然而，我们认为现有的估计器依赖于错误校准的插补误差和概率分数，因为它们依赖于估计的基本模型。我们提供理论洞察，说明错误校准的插补和概率模型可能限制双重稳健估计器的有效性，并利用真实世界数据集验证我们的定理。基于此，我们提出了一个包括对插补和概率模型进行校准的双重校准估计器。

    arXiv:2403.00817v1 Announce Type: cross  Abstract: Recommender systems often suffer from selection bias as users tend to rate their preferred items. The datasets collected under such conditions exhibit entries missing not at random and thus are not randomized-controlled trials representing the target population. To address this challenge, a doubly robust estimator and its enhanced variants have been proposed as they ensure unbiasedness when accurate imputed errors or predicted propensities are provided. However, we argue that existing estimators rely on miscalibrated imputed errors and propensity scores as they depend on rudimentary models for estimation. We provide theoretical insights into how miscalibrated imputation and propensity models may limit the effectiveness of doubly robust estimators and validate our theorems using real-world datasets. On this basis, we propose a Doubly Calibrated Estimator that involves the calibration of both the imputation and propensity models. To achi
    
[^242]: 通过拓扑自然语言分析揭示客户问题

    Uncovering Customer Issues through Topological Natural Language Analysis

    [https://arxiv.org/abs/2403.00804](https://arxiv.org/abs/2403.00804)

    提出了一种利用自然语言技术和拓扑数据分析监控新兴和热门客户问题的机器学习算法。

    

    电子商务公司每天处理大量客户服务请求。尽管通常使用简单的注释系统来总结客户联系的主题，但深入探讨每个具体问题可能具有挑战性。为了解决这一挑战，我们提出了一种新颖的机器学习算法，利用自然语言技术和拓扑数据分析来监控新兴和热门客户问题。我们的方法涉及一种端到端的深度学习框架，同时标记每个客户对话记录的主要问题句，并生成句子嵌入向量。然后我们对嵌入向量进行白化处理，并使用它们构建一个无向图。然后，我们根据每个对话记录的拓扑特性来定义热门和新兴问题。

    arXiv:2403.00804v1 Announce Type: cross  Abstract: E-commerce companies deal with a high volume of customer service requests daily. While a simple annotation system is often used to summarize the topics of customer contacts, thoroughly exploring each specific issue can be challenging. This presents a critical concern, especially during an emerging outbreak where companies must quickly identify and address specific issues. To tackle this challenge, we propose a novel machine learning algorithm that leverages natural language techniques and topological data analysis to monitor emerging and trending customer issues. Our approach involves an end-to-end deep learning framework that simultaneously tags the primary question sentence of each customer's transcript and generates sentence embedding vectors. We then whiten the embedding vectors and use them to construct an undirected graph. From there, we define trending and emerging issues based on the topological properties of each transcript. W
    
[^243]: LiMAML: 通过元学习个性化深度推荐模型

    LiMAML: Personalization of Deep Recommender Models via Meta Learning

    [https://arxiv.org/abs/2403.00803](https://arxiv.org/abs/2403.00803)

    该论文介绍了一种通过元学习实现个性化深度推荐模型的创新解决方案，能够根据最新用户互动信号频繁更新模型，以确保向不同成员提供相关且更新的体验。

    

    在推荐系统领域，深度神经网络的普遍采用已经成为建模各种业务目标的主导范式。随着用户基数的持续增长，个性化和频繁的模型更新的必要性已经变得至关重要，以确保向各种成员提供相关且更新的体验。在这项工作中，我们介绍了一种创新的元学习解决方案，用于针对个人成员和其他实体的模型个性化，结合了根据最新用户互动信号进行频繁更新的功能。具体来说，我们利用了模型无关的元学习（MAML）算法，使用最近的用户互动数据来调整每个任务的子网络。考虑到在线推荐系统中生产原始MAML模型几乎不可行，我们提出了一种有效的策略来将元学习的子网络推广应用到生产中。

    arXiv:2403.00803v1 Announce Type: cross  Abstract: In the realm of recommender systems, the ubiquitous adoption of deep neural networks has emerged as a dominant paradigm for modeling diverse business objectives. As user bases continue to expand, the necessity of personalization and frequent model updates have assumed paramount significance to ensure the delivery of relevant and refreshed experiences to a diverse array of members. In this work, we introduce an innovative meta-learning solution tailored to the personalization of models for individual members and other entities, coupled with the frequent updates based on the latest user interaction signals. Specifically, we leverage the Model-Agnostic Meta Learning (MAML) algorithm to adapt per-task sub-networks using recent user interaction data. Given the near infeasibility of productionizing original MAML-based models in online recommendation systems, we propose an efficient strategy to operationalize meta-learned sub-networks in prod
    
[^244]: 借鉴人类思维过程的脑启发两阶段方法：通过模仿人类思维过程增强数学推理能力

    Brain-Inspired Two-Stage Approach: Enhancing Mathematical Reasoning by Imitating Human Thought Processes

    [https://arxiv.org/abs/2403.00800](https://arxiv.org/abs/2403.00800)

    通过模仿人类思维过程，在数学推理任务中提出的Brain方法实现了最先进的性能，并发现计划可以从自然语言、代码或形式语言中明确提取出来。

    

    虽然大型语言模型展示了在解决数学问题方面的新能力，但在复杂的多步数学推理任务中仍然存在挑战。为了提高模型在数学推理任务上的表现，先前的工作通过改进数据的质量和数量，在开源模型上进行了监督微调。在本文中，我们提出了一种名为Brain的新方法，通过使用前额叶模型生成计划，然后使用顶叶模型生成代码并执行以获得答案，来模仿人类思维过程以增强数学推理能力。首先，我们通过此方法与基于Code LLaMA 7B的模型相比实现了SOTA性能。其次，我们发现计划可以明确地从自然语言、代码或形式语言中提取出来。我们的代码和数据可以在https://github.com/cyzhh/Brain上公开获取。

    arXiv:2403.00800v1 Announce Type: cross  Abstract: Although large language models demonstrate emergent abilities in solving math word problems, there is a challenging task in complex multi-step mathematical reasoning tasks. To improve model performance on mathematical reasoning tasks, previous work has conducted supervised fine-tuning on open-source models by improving the quality and quantity of data. In this paper, we propose a novel approach, named Brain, to imitate human thought processes to enhance mathematical reasoning abilities, using the Frontal Lobe Model to generate plans, and then employing the Parietal Lobe Model to generate code and execute to obtain answers. First, we achieve SOTA performance in comparison with Code LLaMA 7B based models through this method. Secondly, we find that plans can be explicitly extracted from natural language, code, or formal language. Our code and data are publicly available at https://github.com/cyzhh/Brain.
    
[^245]: LLM在数学推理中数据能力边界的实证研究

    An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning

    [https://arxiv.org/abs/2403.00799](https://arxiv.org/abs/2403.00799)

    通过确定最优路径集，本研究拓展了LLMs在数学推理任务中的能力边界，提出了一种监督数据策略，通过混合不同类型数据的最小最优集来累积增强模型能力，并实现了SOTA性能。

    

    大型语言模型(LLMs)正在展示对数学推理任务的新兴能力，人们越来越关注通过监督微调（SFT）增强开源LLMs的能力。本文旨在探讨一个通用的监督数据策略，以帮助优化和拓展数学推理能力。首先，我们通过识别推理路径的最优路径集确定推理路径增强的能力边界。其次，我们验证模型不同能力可以通过相应类型数据的最小最优集混合来累积增强，而我们的模型MMOS在更低的构建成本下实现了系列基础模型的SOTA性能。此外，我们指出GSM-HARD并不真正困难，当今的LLMs不再缺乏数值稳健性。此外，我们提供一个用于稳健性测试和教育应用的自动问题生成器。我们的代码和数据可公开获取。

    arXiv:2403.00799v1 Announce Type: cross  Abstract: Large language models (LLMs) are displaying emergent abilities for math reasoning tasks,and there is a growing attention on enhancing the ability of open-source LLMs through supervised fine-tuning (SFT).In this paper, we aim to explore a general data strategy for supervised data to help optimize and expand math reasoning ability.Firstly, we determine the ability boundary of reasoning paths augmentation by identifying these paths' minimal optimal set.Secondly, we validate that different abilities of the model can be cumulatively enhanced by Mix of Minimal Optimal Sets of corresponding types of data, while our models MMOS achieve SOTA performance on series base models under much lower construction costs.Besides, we point out GSM-HARD is not really hard and today's LLMs no longer lack numerical robustness.Also, we provide an Auto Problem Generator for robustness testing and educational applications.Our code and data are publicly available
    
[^246]: Helen: 使用频率加权Hessian特征值正则化优化CTR预测模型

    Helen: Optimizing CTR Prediction Models with Frequency-wise Hessian Eigenvalue Regularization

    [https://arxiv.org/abs/2403.00798](https://arxiv.org/abs/2403.00798)

    本文从优化的角度探讨CTR预测问题，揭示了特征频率与最大Hessian特征值之间的强正相关性，提出频繁出现的特征会趋向于收敛到尖锐的局部最小值，从而导致次优性能。

    

    单击率(CTR)预测在在线广告和推荐场景中具有至关重要的意义。尽管最近CTR预测模型的广泛增加，但性能改进仍然有限，这一点可以从开源基准评估中得到证实。当前研究人员倾向于针对不同数据集和设置开发新模型，常常忽视一个关键问题：是什么真正使CTR预测如此具有挑战性？

    arXiv:2403.00798v1 Announce Type: cross  Abstract: Click-Through Rate (CTR) prediction holds paramount significance in online advertising and recommendation scenarios. Despite the proliferation of recent CTR prediction models, the improvements in performance have remained limited, as evidenced by open-source benchmark assessments. Current researchers tend to focus on developing new models for various datasets and settings, often neglecting a crucial question: What is the key challenge that truly makes CTR prediction so demanding?   In this paper, we approach the problem of CTR prediction from an optimization perspective. We explore the typical data characteristics and optimization statistics of CTR prediction, revealing a strong positive correlation between the top hessian eigenvalue and feature frequency. This correlation implies that frequently occurring features tend to converge towards sharp local minima, ultimately leading to suboptimal performance. Motivated by the recent advance
    
[^247]: 用高斯过程增强均值回归时间序列预测：金融预测中的功能和增强数据结构

    Enhancing Mean-Reverting Time Series Prediction with Gaussian Processes: Functional and Augmented Data Structures in Financial Forecasting

    [https://arxiv.org/abs/2403.00796](https://arxiv.org/abs/2403.00796)

    本论文通过使用高斯过程在金融预测中探索功能和增强数据结构，提供了一种能够预测整个概率分布并进行长期预测的方法，对于准确预测和决策制定具有重要意义

    

    在这篇论文中，我们探讨了利用高斯过程（GPs）来预测具有潜在结构的均值回归时间序列，使用相对未被探索的功能和增强数据结构。虽然许多传统的预测方法专注于时间序列数据的短期动态，但GPs提供了潜力，不仅可以预测平均预测值，还可以预测未来轨迹上整个概率分布。这在金融环境中特别有益，因为如果不正确的波动率评估导致资本损失，仅准确的预测可能不足够。此外，在交易选择中，GPs允许预测多个夏普比率，考虑交易成本后进行调整，有助于决策。本研究中使用的功能数据表示通过利用过去几年的信息使得可以进行更长期的预测，即使预测脱离了当前年份。

    arXiv:2403.00796v1 Announce Type: cross  Abstract: In this paper, we explore the application of Gaussian Processes (GPs) for predicting mean-reverting time series with an underlying structure, using relatively unexplored functional and augmented data structures. While many conventional forecasting methods concentrate on the short-term dynamics of time series data, GPs offer the potential to forecast not just the average prediction but the entire probability distribution over a future trajectory. This is particularly beneficial in financial contexts, where accurate predictions alone may not suffice if incorrect volatility assessments lead to capital losses. Moreover, in trade selection, GPs allow for the forecasting of multiple Sharpe ratios adjusted for transaction costs, aiding in decision-making. The functional data representation utilized in this study enables longer-term predictions by leveraging information from previous years, even as the forecast moves away from the current year
    
[^248]: 认真对待幽默：利用不风趣的大型语言模型构建幽默数据集

    Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models

    [https://arxiv.org/abs/2403.00794](https://arxiv.org/abs/2403.00794)

    利用大型语言模型生成合成数据，可以帮助改进幽默检测，特别是通过取消幽默元素来评估模型性能。

    

    幽默是人类认知和互动的基本要素。然而，尽管自然语言处理方面取得了近期进展，幽默检测仍然是一项具有挑战性的任务，这是因为幽默文本与类似非幽默文本的数据集稀缺。在我们的研究中，我们探讨了大型语言模型（LLMs）能否通过编辑文本生成用于幽默检测的合成数据。我们在现有人类数据集上对LLMs进行基准测试，并展示当前LLMs在“取消风趣”笑话方面显示出令人印象深刻的能力，这是由人类判断和幽默检测的下游任务衡量而得。我们将我们的方法扩展到了一个混合编码的英语-印地语幽默数据集，在那里我们发现GPT-4的合成数据被双语注释员高度评价，并为幽默分类器提供了具有挑战性的对抗性例子。

    arXiv:2403.00794v1 Announce Type: cross  Abstract: Humor is a fundamental facet of human cognition and interaction. Yet, despite recent advances in natural language processing, humor detection remains a challenging task that is complicated by the scarcity of datasets that pair humorous texts with similar non-humorous counterparts. In our work, we investigate whether large language models (LLMs), can generate synthetic data for humor detection via editing texts. We benchmark LLMs on an existing human dataset and show that current LLMs display an impressive ability to `unfun' jokes, as judged by humans and as measured on the downstream task of humor detection. We extend our approach to a code-mixed English-Hindi humor dataset, where we find that GPT-4's synthetic data is highly rated by bilingual annotators and provides challenging adversarial examples for humor classifiers.
    
[^249]: 在一个混乱而纠缠的世界中的广告推荐

    Ad Recommendation in a Collapsed and Entangled World

    [https://arxiv.org/abs/2403.00793](https://arxiv.org/abs/2403.00793)

    该论文提出了一个行业广告推荐系统，重点关注学习适当表示的挑战和实践，采用多种方法处理特征表示中的关键挑战，包括嵌入的维度坍缩和跨任务或场景的兴趣纠缠。

    

    在这篇论文中，我们提出了一个行业广告推荐系统，关注学习适当表示的挑战和实践。我们的研究从展示如何在对各种类型的特征进行嵌入表示时保留先验开始。具体来说，我们讨论了序列特征、数值特征、预训练嵌入特征以及稀疏ID特征。此外，我们深入探讨了与特征表示相关的两个关键挑战：嵌入的维度坍缩和跨多个任务或场景的兴趣纠缠。随后，我们提出了几种实用方法来有效应对这两个挑战。接着，我们探讨了几种训练技术，以促进模型优化，减少偏差并增强探索能力。此外，我们引入了三种分析工具，使我们能够全面研究特征相关性、维度坍缩等问题。

    arXiv:2403.00793v1 Announce Type: cross  Abstract: In this paper, we present an industry ad recommendation system, paying attention to the challenges and practices of learning appropriate representations. Our study begins by showcasing our approaches to preserving priors when encoding features of diverse types into embedding representations. Specifically, we address sequence features, numeric features, pre-trained embedding features, as well as sparse ID features. Moreover, we delve into two pivotal challenges associated with feature representation: the dimensional collapse of embeddings and the interest entanglement across various tasks or scenarios. Subsequently, we propose several practical approaches to effectively tackle these two challenges. We then explore several training techniques to facilitate model optimization, reduce bias, and enhance exploration. Furthermore, we introduce three analysis tools that enable us to comprehensively study feature correlation, dimensional collap
    
[^250]: PRECISE框架：基于GPT的文本以提高放射学报告的可读性、可靠性和可理解性，实现以患者为中心的护理

    PRECISE Framework: GPT-based Text For Improved Readability, Reliability, and Understandability of Radiology Reports For Patient-Centered Care

    [https://arxiv.org/abs/2403.00788](https://arxiv.org/abs/2403.00788)

    本研究提出并评估了PRECISE框架，利用GPT-4技术提供更易读的胸部X射线报告，以进一步提高放射学报告的可读性、可靠性和可理解性，有助于推动以患者为中心的护理。

    

    本研究介绍并评估了PRECISE框架，利用OpenAI的GPT-4来增强患者参与度，提供更清晰、更易读的六年级阅读水平的胸部X射线报告。该框架在500份报告上进行了测试，显示出在可读性、可靠性和可理解性方面的显著改进。统计分析证实了PRECISE方法的有效性，突显了其在促进健康决策中心的护理交付中的潜力。

    arXiv:2403.00788v1 Announce Type: cross  Abstract: This study introduces and evaluates the PRECISE framework, utilizing OpenAI's GPT-4 to enhance patient engagement by providing clearer and more accessible chest X-ray reports at a sixth-grade reading level. The framework was tested on 500 reports, demonstrating significant improvements in readability, reliability, and understandability. Statistical analyses confirmed the effectiveness of the PRECISE approach, highlighting its potential to foster patient-centric care delivery in healthcare decision-making.
    
[^251]: 应用新闻和媒体情感分析生成外汇交易信号

    Applying News and Media Sentiment Analysis for Generating Forex Trading Signals

    [https://arxiv.org/abs/2403.00785](https://arxiv.org/abs/2403.00785)

    情感分析在外汇交易中的运用对预测市场走势和制定交易信号非常有价值，其有效性在不同市场条件下保持一致。

    

    这项研究的目标是考察情感分析如何应用于生成外汇市场交易信号。作者运用基于词库的分析和朴素贝叶斯机器学习算法来评估社交媒体帖子和新闻文章中涉及美元（USD）的情感。研究结果表明，情感分析对于预测市场走势和制定交易信号非常有价值。值得注意的是，其有效性在不同的市场条件下保持一致。作者得出结论，通过分析新闻和社交媒体中表达的情感，交易员可以获取关于美元和其他相关国家当前市场情绪的见解，从而帮助决策交易。这项研究强调了将情感分析纳入交易策略作为预测市场动态的重要工具的重要性。

    arXiv:2403.00785v1 Announce Type: cross  Abstract: The objective of this research is to examine how sentiment analysis can be employed to generate trading signals for the Foreign Exchange (Forex) market. The author assessed sentiment in social media posts and news articles pertaining to the United States Dollar (USD) using a combination of methods: lexicon-based analysis and the Naive Bayes machine learning algorithm. The findings indicate that sentiment analysis proves valuable in forecasting market movements and devising trading signals. Notably, its effectiveness is consistent across different market conditions. The author concludes that by analyzing sentiment expressed in news and social media, traders can glean insights into prevailing market sentiments towards the USD and other pertinent countries, thereby aiding trading decision-making. This study underscores the importance of weaving sentiment analysis into trading strategies as a pivotal tool for predicting market dynamics.
    
[^252]: ChatDiet：通过LLM增强框架赋能个性化营养导向食品推荐聊天机器人

    ChatDiet: Empowering Personalized Nutrition-Oriented Food Recommender Chatbots through an LLM-Augmented Framework

    [https://arxiv.org/abs/2403.00781](https://arxiv.org/abs/2403.00781)

    这项研究介绍了ChatDiet，一个借助LLM技术构建的框架，能够帮助个性化营养导向食品推荐聊天机器人提供个性化和可解释的推荐。

    

    食物对健康的深远影响使得先进的营养导向食品推荐服务成为必要。传统方法往往缺乏个性化、可解释性和互动性等关键元素。虽然大型语言模型（LLMs）带来了解释性和可解释性，但它们单独的使用未能实现真正的个性化。本文介绍了ChatDiet，一种新颖的LLM驱动框架，专门设计用于个性化营养导向食品推荐聊天机器人。ChatDiet集成了个人和人群模型，辅以一个协调器，无缝检索和处理相关信息。其结果是动态提供个性化和可解释的食品推荐，根据个人用户喜好定制。我们对ChatDiet进行了评估，包括一个引人入胜的案例研究，在案例研究中建立了一个因果个人模型来估计个人营养效果。

    arXiv:2403.00781v1 Announce Type: cross  Abstract: The profound impact of food on health necessitates advanced nutrition-oriented food recommendation services. Conventional methods often lack the crucial elements of personalization, explainability, and interactivity. While Large Language Models (LLMs) bring interpretability and explainability, their standalone use falls short of achieving true personalization. In this paper, we introduce ChatDiet, a novel LLM-powered framework designed specifically for personalized nutrition-oriented food recommendation chatbots. ChatDiet integrates personal and population models, complemented by an orchestrator, to seamlessly retrieve and process pertinent information. The result is a dynamic delivery of personalized and explainable food recommendations, tailored to individual user preferences. Our evaluation of ChatDiet includes a compelling case study, where we establish a causal personal model to estimate individual nutrition effects. Our assessmen
    
[^253]: 对犯罪预测数据挖掘技术的实证和实验洞见：综合调查

    Empirical and Experimental Insights into Data Mining Techniques for Crime Prediction: A Comprehensive Survey

    [https://arxiv.org/abs/2403.00780](https://arxiv.org/abs/2403.00780)

    这篇论文提供了对犯罪预测技术的全面分析，提出了一种细分犯罪预测算法的方法论分类法，并通过经验和实验评估来对这些技术进行排名。

    

    这篇综述论文全面分析了犯罪预测方法论，探讨了在该领域中应用的各种技术和技术。该论文涵盖了用于分析犯罪数据的统计方法、机器学习算法和深度学习技术，同时还审视了它们的有效性和局限性。我们提出了一种将犯罪预测算法分类为特定技术的方法论分类法。该分类法分为四个层次，包括方法论类别、方法论子类别、方法论技术和方法论技术子类别。提供了经验和实验评估以对不同技术进行排名。经验评估根据四个标准评估了犯罪预测技术，而实验评估则对采用相同子技术的算法、采用相同技术的不同子技术、以及相同技术的不同算法进行排名。

    arXiv:2403.00780v1 Announce Type: cross  Abstract: This survey paper presents a comprehensive analysis of crime prediction methodologies, exploring the various techniques and technologies utilized in this area. The paper covers the statistical methods, machine learning algorithms, and deep learning techniques employed to analyze crime data, while also examining their effectiveness and limitations. We propose a methodological taxonomy that classifies crime prediction algorithms into specific techniques. This taxonomy is structured into four tiers, including methodology category, methodology sub-category, methodology techniques, and methodology sub-techniques. Empirical and experimental evaluations are provided to rank the different techniques. The empirical evaluation assesses the crime prediction techniques based on four criteria, while the experimental evaluation ranks the algorithms that employ the same sub-technique, the different sub-techniques that employ the same technique, the d
    
[^254]: 用无监督学习技术对抗金融犯罪：聚类和降维在反洗钱中的应用

    Combating Financial Crimes with Unsupervised Learning Techniques: Clustering and Dimensionality Reduction for Anti-Money Laundering

    [https://arxiv.org/abs/2403.00777](https://arxiv.org/abs/2403.00777)

    本研究探讨将聚类方法与四种降维技术相结合，以克服反洗钱数据中的高维问题，并提高聚类结果的有效性。

    

    反洗钱（AML）是确保金融系统完整性的关键任务。AML面临的一个主要挑战是基于行为识别高风险群体。无监督学习，特别是聚类，是解决这一任务的一种有前途的方法。然而，使用数百个特征描述行为会导致高维数据集，从而对聚类性能产生负面影响。本文研究了将聚类方法凝聚层次聚类与四种降维技术——独立成分分析（ICA）、核主成分分析（KPCA）、奇异值分解（SVD）、局部保持投影（LPP）相结合，以克服AML数据中的高维问题并提高聚类结果的有效性。本研究旨在揭示减少AML数据维度最有效的方法，并提高准确性。

    arXiv:2403.00777v1 Announce Type: cross  Abstract: Anti-Money Laundering (AML) is a crucial task in ensuring the integrity of financial systems. One keychallenge in AML is identifying high-risk groups based on their behavior. Unsupervised learning, particularly clustering, is a promising solution for this task. However, the use of hundreds of features todescribe behavior results in a highdimensional dataset that negatively impacts clustering performance.In this paper, we investigate the effectiveness of combining clustering method agglomerative hierarchicalclustering with four dimensionality reduction techniques -Independent Component Analysis (ICA), andKernel Principal Component Analysis (KPCA), Singular Value Decomposition (SVD), Locality Preserving Projections (LPP)- to overcome the issue of high-dimensionality in AML data and improve clusteringresults. This study aims to provide insights into the most effective way of reducing the dimensionality ofAML data and enhance the accuracy 
    
[^255]: 通过图神经网络在面向对象的业务流程中检测异常事件

    Detecting Anomalous Events in Object-centric Business Processes via Graph Neural Networks

    [https://arxiv.org/abs/2403.00775](https://arxiv.org/abs/2403.00775)

    通过图神经网络和面向对象的流程挖掘，本研究提出了一种在业务流程中检测异常事件的新框架，避免了传统方法中的扁平化事件日志带来的人为异常问题。

    

    检测异常对于识别业务流程中的低效率、错误或欺诈行为至关重要。传统的流程挖掘方法集中在分析基于单一案例概念的'扁平化'、顺序事件日志。然而，许多真实世界的流程执行展现出类似图形的结构，其中事件可以与多个案例相关联。将事件日志扁平化需要选择单个案例标识符，这导致与真实事件数据存在差距并在事件日志中人为引入异常。面向对象的流程挖掘通过允许事件与不同案例相关联来避免这些限制。该研究提出了一个利用图神经网络和面向对象的流程挖掘提供的增强信息进行业务流程异常检测的新框架。我们首先将面向对象的事件日志的过程依赖关系重建和表示为带属性的图，然后...

    arXiv:2403.00775v1 Announce Type: cross  Abstract: Detecting anomalies is important for identifying inefficiencies, errors, or fraud in business processes. Traditional process mining approaches focus on analyzing 'flattened', sequential, event logs based on a single case notion. However, many real-world process executions exhibit a graph-like structure, where events can be associated with multiple cases. Flattening event logs requires selecting a single case identifier which creates a gap with the real event data and artificially introduces anomalies in the event logs. Object-centric process mining avoids these limitations by allowing events to be related to different cases. This study proposes a novel framework for anomaly detection in business processes that exploits graph neural networks and the enhanced information offered by object-centric process mining. We first reconstruct and represent the process dependencies of the object-centric event logs as attributed graphs and then empl
    
[^256]: 后选择和深度学习中的不端行为

    Misconduct in Post-Selections and Deep Learning

    [https://arxiv.org/abs/2403.00773](https://arxiv.org/abs/2403.00773)

    该论文讨论了深度学习和后选择中的不端行为，并提出了对于解决这一问题的新观点。

    

    这是关于“深度学习”特别是一般后选择中不端行为的理论论文。作者所知，关于深度学习不端行为的第一篇同行评审论文是[32]，[37]和[36]。无论学习模式是监督，强化，对抗还是进化，几乎所有机器学习方法（除了少数训练单一系统的方法）都根源于同样的不端行为-作弊和隐藏-（1）在没有测试的情况下作弊以及（2）隐藏外观不佳的数据。在[32]，[37]，[36]中推理，作者必须至少报告所有已训练网络在验证集上的平均误差（本文中称为通用交叉验证）。最好还报告排名误差的五个百分比位置。从这里的新分析中，我们可以看到隐藏的罪魁祸首是后选择。对手动调整或搜索超参数的后选择也是如此。

    arXiv:2403.00773v1 Announce Type: new  Abstract: This is a theoretical paper on "Deep Learning" misconduct in particular and Post-Selection in general. As far as the author knows, the first peer-reviewed papers on Deep Learning misconduct are [32], [37], [36]. Regardless of learning modes, e.g., supervised, reinforcement, adversarial, and evolutional, almost all machine learning methods (except for a few methods that train a sole system) are rooted in the same misconduct -- cheating and hiding -- (1) cheating in the absence of a test and (2) hiding bad-looking data. It was reasoned in [32], [37], [36] that authors must report at least the average error of all trained networks, good and bad, on the validation set (called general cross-validation in this paper). Better, report also five percentage positions of ranked errors. From the new analysis here, we can see that the hidden culprit is Post-Selection. This is also true for Post-Selection on hand-tuned or searched hyperparameters, bec
    
[^257]: 微博平台专家在预测股市方面表现更好吗？

    Do Weibo platform experts perform better at predicting stock market?

    [https://arxiv.org/abs/2403.00772](https://arxiv.org/abs/2403.00772)

    使用神经网络结合BERT情感分类和LSTM时间序列模型在微博平台授权和未授权金融顾问的背景下进行股市预测

    

    情感分析可用于股市预测。然而，现有研究尚未研究用户的金融背景对基于情感的人工神经网络股市预测的影响。本文使用一种新颖的神经网络组合来评估基于人群金融背景的情感股市预测。采用最先进的语言处理模型BERT来分类情感，并使用长短期记忆（LSTM）模型进行基于时间序列的股市预测。评估时，使用微博社交网络平台作为情感数据收集来源。根据其背景，将微博用户（及其评论）分为授权金融顾问（AFA）和未授权金融顾问（UFA）两组。

    arXiv:2403.00772v1 Announce Type: cross  Abstract: Sentiment analysis can be used for stock market prediction. However, existing research has not studied the impact of a user's financial background on sentiment-based forecasting of the stock market using artificial neural networks. In this work, a novel combination of neural networks is used for the assessment of sentiment-based stock market prediction, based on the financial background of the population that generated the sentiment. The state-of-the-art language processing model Bidirectional Encoder Representations from Transformers (BERT) is used to classify the sentiment and a Long-Short Term Memory (LSTM) model is used for time-series based stock market prediction. For evaluation, the Weibo social networking platform is used as a sentiment data collection source. Weibo users (and their comments respectively) are divided into Authorized Financial Advisor (AFA) and Unauthorized Financial Advisor (UFA) groups according to their backg
    
[^258]: 从配对X射线生成CT体积的XProspeCT方法

    XProspeCT: CT Volume Generation from Paired X-Rays

    [https://arxiv.org/abs/2403.00771](https://arxiv.org/abs/2403.00771)

    通过探索更大的数据集和多种模型结构，本研究将正交X射线图像转换为模拟的CT体积，采用了UNet架构、自定义连接、激活函数、损失函数、优化器和一种新颖的反投影方法。

    

    计算机断层扫描（CT）是一种有益的诊断工具。CT扫描提供了关于患者内部解剖结构的详细信息，但与X射线成像相比，CT扫描具有更高的辐射剂量和费用。在本文中，我们在先前的研究基础上，通过探索更大的数据集和各种模型结构，将正交X射线图像转换为模拟的CT体积。显著的模型变化包括UNet架构、自定义连接、激活函数、损失函数、优化器和一种新颖的反投影方法。

    arXiv:2403.00771v1 Announce Type: cross  Abstract: Computed tomography (CT) is a beneficial imaging tool for diagnostic purposes. CT scans provide detailed information concerning the internal anatomic structures of a patient, but present higher radiation dose and costs compared to X-ray imaging. In this paper, we build on previous research to convert orthogonal X-ray images into simulated CT volumes by exploring larger datasets and various model structures. Significant model variations include UNet architectures, custom connections, activation functions, loss functions, optimizers, and a novel back projection approach.
    
[^259]: 在教育领域的文本挖掘

    Text mining in education

    [https://arxiv.org/abs/2403.00769](https://arxiv.org/abs/2403.00769)

    本文系统概述了当前教育文本挖掘领域的现状，旨在回答教育环境中最常用的文本挖掘技术、最常用的教育资源以及主要的应用或教育目标，同时概述了结论和未来趋势。

    

    在线教育环境的迅猛增长产生了大量数据，特别是来自论坛、聊天、社交网络、评估、论文等文本格式的数据。如何挖掘文本数据以找到对教育相关人员有用的知识，是一个激动人心的挑战。尽管最近已发表了越来越多应用文本挖掘于教育领域的文章，但我们尚未找到任何综述这些工作的论文。因此，本文对当前教育文本挖掘领域的现状进行了系统概述。我们的最终目标是回答三个主要研究问题：在教育环境中最常用的文本挖掘技术是什么？最常用的教育资源是什么？主要应用或教育目标是什么？最后，我们总结了结论和更有趣的未来趋势。

    arXiv:2403.00769v1 Announce Type: cross  Abstract: The explosive growth of online education environments is generating a massive volume of data, specially in text format from forums, chats, social networks, assessments, essays, among others. It produces exciting challenges on how to mine text data in order to find useful knowledge for educational stakeholders. Despite the increasing number of educational applications of text mining published recently, we have not found any paper surveying them. In this line, this work presents a systematic overview of the current status of the Educational Text Mining field. Our final goal is to answer three main research questions: Which are the text mining techniques most used in educational environments? Which are the most used educational resources? And which are the main applications or educational goals? Finally, we outline the conclusions and the more interesting future trends.
    
[^260]: 通过强化学习实现DNN多租户多加速器系统中的公平和稳定实时调度

    Towards Fair and Firm Real-Time Scheduling in DNN Multi-Tenant Multi-Accelerator Systems via Reinforcement Learning

    [https://arxiv.org/abs/2403.00766](https://arxiv.org/abs/2403.00766)

    通过强化学习实现了在DNN多租户多加速器系统中公平和稳定的实时调度，提出了针对不同租户的模型特定QoS管理的新方法。

    

    本文解决了云服务中管理服务质量（QoS）的关键挑战，重点关注个别租户期望和不同的服务水平指标（SLI）的细微之处。它引入了一种新颖的方法，利用深度强化学习来管理多租户、多加速器云环境中各自特定的QoS。所选择的SLI，截止时间命中率，允许客户为每个服务请求定制QoS。提出了一种面向多加速器系统的深度神经网络的在线调度算法，重点关注于在考虑实时约束的同时保证租户特定，模型特定的QoS水平。

    arXiv:2403.00766v1 Announce Type: cross  Abstract: This paper addresses the critical challenge of managing Quality of Service (QoS) in cloud services, focusing on the nuances of individual tenant expectations and varying Service Level Indicators (SLIs). It introduces a novel approach utilizing Deep Reinforcement Learning for tenant-specific QoS management in multi-tenant, multi-accelerator cloud environments. The chosen SLI, deadline hit rate, allows clients to tailor QoS for each service request. A novel online scheduling algorithm for Deep Neural Networks in multi-accelerator systems is proposed, with a focus on guaranteeing tenant-wise, model-specific QoS levels while considering real-time constraints.
    
[^261]: 一种用于Webots的无人监控容器化(深度)强化学习体系结构

    An Architecture for Unattended Containerized (Deep) Reinforcement Learning with Webots

    [https://arxiv.org/abs/2403.00765](https://arxiv.org/abs/2403.00765)

    该论文提出了一种用于Webots的无人监控容器化(深度)强化学习体系结构，针对机器人 Robotino 训练强化学习代理，同时强调模拟环境和数据科学家模型开发环境的分离这一不太被讨论的主题。

    

    随着数据科学应用在各行各业中得到采用，工具景观不断成熟，以促进这类应用的生命周期并提供解决方案，以应对涉及的挑战，以提高参与者的生产力。在3D世界中使用代理进行强化学习仍然可能面临挑战：使用模拟软件所需的知识以及在无人监控的训练管道中利用独立的模拟软件。在本文中，我们回顾了用于在3D世界中培训机器人的强化学习代理的工具和方法，针对机器人Robotino进行论述，并认为为虚拟世界的创建者分离模拟环境与数据科学家的模型开发环境并不是一个被很好涵盖的主题。通常二者相同，数据科学家需要了解模拟软件，直接与其API一起使用。此外，有时虚拟世界的创建者会......

    arXiv:2403.00765v1 Announce Type: cross  Abstract: As data science applications gain adoption across industries, the tooling landscape matures to facilitate the life cycle of such applications and provide solutions to the challenges involved to boost the productivity of the people involved. Reinforcement learning with agents in a 3D world could still face challenges: the knowledge required to use a simulation software as well as the utilization of a standalone simulation software in unattended training pipelines.   In this paper we review tools and approaches to train reinforcement learning agents for robots in 3D worlds with respect to the robot Robotino and argue that the separation of the simulation environment for creators of virtual worlds and the model development environment for data scientists is not a well covered topic. Often both are the same and data scientists require knowledge of the simulation software to work directly with their APIs. Moreover, sometimes creators of vir
    
[^262]: 通过高阶注意力大脑网络分析大麻使用者的静息态fMRI数据

    Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network

    [https://arxiv.org/abs/2403.00033](https://arxiv.org/abs/2403.00033)

    通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。

    

    大麻的持续使用明显影响人们的生活和健康。在这项研究中，我们提出了一个可解释的新框架，命名为HOGAB（High-Order Attention Graph Attention神经网络）模型，以分析两个数据集中慢性大麻用户的局部异常脑活动。HOGAB将动态内在功能网络与LSTM技术相结合，捕捉大麻用户fMRI时间序列中的时间模式。此外，我们使用高阶注意力模块来对邻域节点进行信息融合和消息传递，增强长期大麻用户的社区聚类分析。此外，我们通过融入注意力机制提高了模型的整体学习能力，在多图分类中实现了85.1%的AUC和80.7%的准确性。此外，我们比较了线性机器学习方法，并评估了我们提出的HODAB模型的有效性。

    arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
    
[^263]: 深度选择性状态空间模型的理论基础

    Theoretical Foundations of Deep Selective State-Space Models

    [https://arxiv.org/abs/2402.19047](https://arxiv.org/abs/2402.19047)

    随着GateLoop、Mamba和GLA等具有乘法交互的线性递归驱动下的深度SSM架构的出现，它们在准确性和效率上超越了基于注意力的文本训练的基础模型。

    

    结构化状态空间模型（SSM）如S4，源自Gu等人的开创性工作，作为建模序列数据的有效方法而日益受到青睐。深度SSM在各种领域展现出卓越的性能，相较于基于注意力的transformers，训练和推理成本降低。最近的研究表明，如果驱动SSM的线性递归允许输入和隐藏状态之间的乘法交互（如GateLoop，Mamba，GLA），那么所得到的架构可以在准确性和效率上超越基于注意力的文本训练的基础模型，参数规模达到十亿级。在本文中，我们使用Rough Path Theory的工具，为这一最近的发现提供了理论基础：我们表明，当随机线性递归配备简单的输入控制转换（选择性机制）时，隐藏状态可被证明是低维的投影。

    arXiv:2402.19047v1 Announce Type: new  Abstract: Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional proj
    
[^264]: Decompose-and-Compose: 一种组合方法来减轻伪相关性

    Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation

    [https://arxiv.org/abs/2402.18919](https://arxiv.org/abs/2402.18919)

    通过组合方法改善模型对相关性转移的稳健性，解决了图像分类中伪相关性的问题。

    

    尽管标准的经验风险最小化（ERM）训练已被证明在图像分类中的内分布数据上是有效的，但在外分布样本上表现不佳。图像分类中的一个主要分布转移来源是图像的组成性质。具体来说，除了确定标签的主要对象或组件外，通常还存在一些其他图像组件，这可能导致训练和测试环境之间的输入分布转移。更重要的是，这些组件可能与标签具有伪相关性。为了解决这个问题，我们提出了Decompose-and-Compose（DaC），通过基于组合图像元素的组合方法改善了对相关性转移的稳健性。根据我们的观察，使用ERM训练的模型通常高度关注要么是因果组件，要么是与标签具有高伪相关性的组件（尤其

    arXiv:2402.18919v1 Announce Type: cross  Abstract: While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especia
    
[^265]: 扩展流匹配：具有广义连续性方程的条件生成方法

    Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation

    [https://arxiv.org/abs/2402.18839](https://arxiv.org/abs/2402.18839)

    本文基于Flow Matching发展了条件生成理论，通过使用广义连续性方程的数学框架而非流匹配中的连续性方程，实现了一种新颖的流基条件分布生成方法。

    

    条件生成任务是生成模型中最重要的应用之一，迄今为止已经开发了许多基于著名扩散模型的方法，其中以基于引导的无分类器方法为首。然而，基于引导的方法的理论不仅要求用户微调“引导强度”，而且其目标向量场不一定对应于训练中使用的条件分布。本文基于流匹配发展了条件生成理论，流匹配是扩散方法的当前强大竞争者之一。受将概率路径解释为路径空间上的分布的启发，我们建立了一个新颖的流基条件分布生成理论，通过使用广义连续性方程的数学框架而不是流匹配中的连续性方程。这一理论自然地推导出一种方法

    arXiv:2402.18839v1 Announce Type: new  Abstract: The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the "guidance strength," but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method th
    
[^266]: 具有一级信念的假设在线学习在不对称信息随机博弈中的应用

    Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games

    [https://arxiv.org/abs/2402.18781](https://arxiv.org/abs/2402.18781)

    提出了一种具有假设在线学习（COL）的学习方案，针对通用AISG，结构化为一个先验预测者-演员-评论家（FAC）架构，利用一级信念和对手策略的主观预测，通过在线展开更新策略，并通过贝叶斯学习校准假设。

    

    随机博弈出现在许多复杂的社会技术系统中，如网络物理系统和IT基础设施，信息不对称为决策实体（玩家）的决策带来挑战。现有的不对称信息随机博弈（AISG）的计算方法主要是离线的，针对特殊类别的AISG，以避免信念层次，并且缺乏适应均衡偏差的在线能力。为了解决这一限制，我们提出了一种具有假设在线学习（COL）的学习方案，专门针对通用AISG。COL结构化为一个先验预测者-演员-评论家（FAC）架构，利用对隐藏状态的一级信念和对对手策略的主观预测。针对假设的对手，COL通过在线展开更新策略，并通过贝叶斯学习校准假设。我们证明了COL中的假设与t一致。

    arXiv:2402.18781v1 Announce Type: cross  Abstract: Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with t
    
[^267]: 数据解释器：用于数据科学的LLM代理

    Data Interpreter: An LLM Agent For Data Science

    [https://arxiv.org/abs/2402.18679](https://arxiv.org/abs/2402.18679)

    本研究引入了数据解释器，采用动态规划、工具集成和逻辑错误识别等关键技术，旨在增强数据科学中的问题解决能力。

    

    大型语言模型（LLM）代理已表现出显著的有效性。然而，在需要实时数据调整、优化专业知识以应对各种任务间复杂依赖性以及精确推理的逻辑错误识别的数据科学场景中，它们的性能可能会受到影响。本研究介绍了数据解释器，这是一个设计用于解决强调三种关键技术以增强数据科学中问题解决的方案的代码：1）具有分层图结构的动态规划，用于实时数据适应性；2）工具集成动态化，以增强代码执行过程中的熟练度，丰富必要的专业知识；3）在反馈中识别逻辑不一致性，并通过经验记录来提高效率。我们评估了数据解释器在各种数据科学和现实任务上的表现。与开源基线相比，它展现了s

    arXiv:2402.18679v1 Announce Type: new  Abstract: Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated s
    
[^268]: 在分享扩散模型中探讨隐私和公平风险：一种对抗性视角

    Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective

    [https://arxiv.org/abs/2402.18607](https://arxiv.org/abs/2402.18607)

    本文从对抗性的角度研究了分享扩散模型可能存在的隐私和公平风险，特别是探讨了在一方使用私人数据训练模型后提供给另一方黑盒访问权限的情况。

    

    扩散模型近年来在学术界和工业界引起了广泛关注，因为其在采样质量和分布覆盖方面表现出色。因此，提出了跨不同组织分享预训练扩散模型的建议，以提高数据利用率同时通过避免直接分享私人数据来增强隐私保护。然而，与这种方法相关的潜在风险尚未得到全面调查。本文从对抗性的角度探讨了与分享扩散模型相关的潜在隐私和公平风险。具体而言，我们调查了一方（分享者）使用私人数据训练扩散模型并向另一方（接收者）提供预训练模型的黑盒访问权限用于下游任务的情况。我们展示了分享者可以实行的行动

    arXiv:2402.18607v1 Announce Type: cross  Abstract: Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execut
    
[^269]: 通过主动迁移学习自动测试空间相关环境假设

    Automated Testing of Spatially-Dependent Environmental Hypotheses through Active Transfer Learning

    [https://arxiv.org/abs/2402.18064](https://arxiv.org/abs/2402.18064)

    结合了迁移学习和主动学习的方法，通过多任务高斯过程和基于信息的目标函数，可以在实时评估假设的数量之间的关系，从而提高规划效率。

    

    有效采样对户外信息收集应用至关重要，因为高昂的采样成本，如时间、能量，以及潜在的环境破坏。利用现有的先验数据可以是提高效率的强大工具。然而，这些数据与感兴趣的数量之间的关系通常事先未知，从而限制了利用此知识进行改进规划效率的能力。为此，这项工作通过多任务高斯过程和基于信息的目标函数结合了迁移学习和主动学习。通过这种组合，它可以研究假设的数量之间的关系空间，并即时评估这些假设，使此新知识能够立即为未来计划所利用。所提出方法的性能针对合成数据进行评估，并表明可以评估

    arXiv:2402.18064v1 Announce Type: cross  Abstract: The efficient collection of samples is an important factor in outdoor information gathering applications on account of high sampling costs such as time, energy, and potential destruction to the environment. Utilization of available a-priori data can be a powerful tool for increasing efficiency. However, the relationships of this data with the quantity of interest are often not known ahead of time, limiting the ability to leverage this knowledge for improved planning efficiency. To this end, this work combines transfer learning and active learning through a Multi-Task Gaussian Process and an information-based objective function. Through this combination it can explore the space of hypothetical inter-quantity relationships and evaluate these hypotheses in real-time, allowing this new knowledge to be immediately exploited for future plans. The performance of the proposed method is evaluated against synthetic data and is shown to evaluate 
    
[^270]: Mixer不仅仅是一个模型

    Mixer is more than just a model

    [https://arxiv.org/abs/2402.18007](https://arxiv.org/abs/2402.18007)

    Mixer的创新之处在于将通道和令牌信息融合，代表了信息提取范式，还可以根据不同需求创建更适合特定任务的混合器。

    

    最近，MLP结构重新受到关注，其中MLP-Mixer以其突出的表现脱颖而出。在计算机视觉领域，MLP-Mixer以从通道和令牌两个角度提取数据信息的能力而闻名，有效地作为通道信息和令牌信息的融合。事实上，Mixer代表了一种信息提取范式，将通道和令牌信息融合在一起。Mixer的精髓在于它能够从多元视角融合信息，典型地体现了在神经网络架构领域的“混合”真正概念。除了考虑通道和令牌以外，可以从各种角度创造更贴合特定任务需求的混合器。本研究专注于音频识别领域，引入一种名为带Roll-Time和Hermit FFT的音频频谱混合器(ASM-RH)的创新模型，该模型结合了对时间和频率的洞察。

    arXiv:2402.18007v1 Announce Type: cross  Abstract: Recently, MLP structures have regained popularity, with MLP-Mixer standing out as a prominent example. In the field of computer vision, MLP-Mixer is noted for its ability to extract data information from both channel and token perspectives, effectively acting as a fusion of channel and token information. Indeed, Mixer represents a paradigm for information extraction that amalgamates channel and token information. The essence of Mixer lies in its ability to blend information from diverse perspectives, epitomizing the true concept of "mixing" in the realm of neural network architectures. Beyond channel and token considerations, it is possible to create more tailored mixers from various perspectives to better suit specific task requirements. This study focuses on the domain of audio recognition, introducing a novel model named Audio Spectrogram Mixer with Roll-Time and Hermit FFT (ASM-RH) that incorporates insights from both time and freq
    
[^271]: 统计学习的确定性和近似确定性模型

    Certain and Approximately Certain Models for Statistical Learning

    [https://arxiv.org/abs/2402.17926](https://arxiv.org/abs/2402.17926)

    可以直接从带有缺失值的数据中学习准确模型，构建了检查数据填充必要性的高效算法，并在不需要填充的情况下返回准确模型，显著减少数据填充所需的时间和精力

    

    现实世界中的数据通常是不完整的，并且包含缺失值。为了在真实世界的数据集上训练准确的模型，用户需要花费大量时间和资源填充和找到缺失数据项的正确值。本文表明，对于某些训练数据和目标模型，可以直接从具有缺失值的数据中学习准确的模型。我们提出了一种统一的方法，可以检查数据填充的必要性，以便在各种广泛使用的机器学习范例中学习准确的模型。我们构建了具有理论保证的高效算法来检查此必要性，并在不需要填充的情况下返回准确的模型。我们广泛的实验证明，我们提出的算法显著减少了数据填充所需的时间和精力，而没有带来相当大的计算开销。

    arXiv:2402.17926v1 Announce Type: cross  Abstract: Real-world data is often incomplete and contains missing values. To train accurate models over real-world datasets, users need to spend a substantial amount of time and resources imputing and finding proper values for missing data items. In this paper, we demonstrate that it is possible to learn accurate models directly from data with missing values for certain training data and target models. We propose a unified approach for checking the necessity of data imputation to learn accurate models across various widely-used machine learning paradigms. We build efficient algorithms with theoretical guarantees to check this necessity and return accurate models in cases where imputation is unnecessary. Our extensive experiments indicate that our proposed algorithms significantly reduce the amount of time and effort needed for data imputation without imposing considerable computational overhead.
    
[^272]: 当你的AI欺骗你：在奖励学习中人类评估者部分可观测性的挑战

    When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning

    [https://arxiv.org/abs/2402.17747](https://arxiv.org/abs/2402.17747)

    RLHF在考虑部分观察性时可能导致策略欺骗性地夸大性能或过度辩护行为，我们提出了数学条件来解决这些问题，并警告不要盲目应用RLHF在部分可观测情况下。

    

    强化学习从人类反馈（RLHF）的过去分析假设人类完全观察到环境。当人类反馈仅基于部分观察时会发生什么？我们对两种失败情况进行了正式定义：欺骗和过度辩护。通过将人类建模为对轨迹信念的Boltzmann-理性，我们证明了RLHF保证会导致策略欺骗性地夸大其性能、为了留下印象而过度辩护或者两者兼而有之的条件。为了帮助解决这些问题，我们数学地刻画了环境部分可观测性如何转化为（缺乏）学到的回报函数中的模糊性。在某些情况下，考虑环境部分可观测性使得在理论上可能恢复回报函数和最优策略，而在其他情况下，存在不可减少的模糊性。我们警告不要盲目应用RLHF在部分可观测情况下。

    arXiv:2402.17747v1 Announce Type: cross  Abstract: Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observa
    
[^273]: 扩散模型中的相变揭示了数据的分层性质

    A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data

    [https://arxiv.org/abs/2402.16991](https://arxiv.org/abs/2402.16991)

    扩散模型在研究数据的分层生成模型中展示出了在阈值时间发生相变的特性，这影响了高级特征和低级特征的重建过程。

    

    理解真实数据的结构在推动现代深度学习方法方面至关重要。自然数据，如图像，被认为是由以层次和组合方式组织的特征组成的，神经网络在学习过程中捕捉到这些特征。最近的进展显示，扩散模型能够生成高质量的图像，暗示了它们捕捉到这种潜在结构的能力。我们研究了数据的分层生成模型中的这一现象。我们发现，在时间$t$后作用的反向扩散过程受到某个阈值时间处的相变控制，此时重建高级特征（如图像的类别）的概率突然下降。相反，低级特征（如图像的具体细节）的重建在整个扩散过程中平稳演变。这一结果暗示，在超出转变时间的时刻，类别已变化，但是基

    arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
    
[^274]: 具有交叉一致$p$-值的异常检测中的不确定性量化

    Uncertainty Quantification in Anomaly Detection with Cross-Conformal $p$-Values

    [https://arxiv.org/abs/2402.16388](https://arxiv.org/abs/2402.16388)

    针对异常检测系统中不确定性量化的需求，提出了一种新颖的框架，称为交叉一致异常检测，通过校准模型的不确定性提供统计保证。

    

    随着可靠、可信和可解释机器学习的重要性日益增加，对异常检测系统进行不确定性量化的要求变得愈发重要。在这种情况下，有效控制类型I错误率($\alpha$)而又不损害系统的统计功率($1-\beta$)可以建立信任，并减少与假发现相关的成本，特别是当后续程序昂贵时。利用符合预测原则的方法有望通过校准模型的不确定性为异常检测提供相应的统计保证。该工作引入了一个新颖的异常检测框架，称为交叉一致异常检测，建立在为预测任务设计的著名交叉一致方法之上。通过这种方法，他填补了在归纳一致异常检测环境中扩展先前研究的自然研究空白

    arXiv:2402.16388v1 Announce Type: cross  Abstract: Given the growing significance of reliable, trustworthy, and explainable machine learning, the requirement of uncertainty quantification for anomaly detection systems has become increasingly important. In this context, effectively controlling Type I error rates ($\alpha$) without compromising the statistical power ($1-\beta$) of these systems can build trust and reduce costs related to false discoveries, particularly when follow-up procedures are expensive. Leveraging the principles of conformal prediction emerges as a promising approach for providing respective statistical guarantees by calibrating a model's uncertainty. This work introduces a novel framework for anomaly detection, termed cross-conformal anomaly detection, building upon well-known cross-conformal methods designed for prediction tasks. With that, it addresses a natural research gap by extending previous works in the context of inductive conformal anomaly detection, rel
    
[^275]: 融合编码器网络

    Fusion Encoder Networks

    [https://arxiv.org/abs/2402.15883](https://arxiv.org/abs/2402.15883)

    FENs是一种神经网络算法，具有对数深度且可以在线性时间内处理序列，关键创新在于通过训练大致线性数量的常深度神经网络并行学习。

    

    在本文中，我们提出了一种名为融合编码器网络（FENs）的算法类：用于创建将固定长度序列映射到输出的神经网络。生成的神经网络仅具有对数深度（减轻数据在网络中传播时的退化），可以在线性时间内处理序列（或者在具有线性处理器数量的对数时间内）。FENs的关键属性是它们通过训练大致线性数量的常深度神经网络并行学习。这些网络具有常深度意味着反向传播效果良好。需要注意的是，目前FENs的性能仅仅是推测，因为我们尚未实现它们。

    arXiv:2402.15883v1 Announce Type: new  Abstract: In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.
    
[^276]: OmniPred：语言模型作为通用回归器

    OmniPred: Language Models as Universal Regressors

    [https://arxiv.org/abs/2402.14547](https://arxiv.org/abs/2402.14547)

    本文提出了OmniPred框架，用于训练语言模型作为通用的端到端回归器，实验证明，在多个任务上训练时，语言模型能够显著优于传统回归模型。

    

    在实验设计的广阔领域中，回归一直是一个强大的工具，可以准确预测系统或模型在给定一组参数的情况下的结果指标，但传统上只限于适用于特定任务的方法。在本文中，我们提出了OmniPred，这是一个用于训练语言模型作为通用端到端回归器的框架，使用来自多样真实世界实验的$(x,y)$评估数据。通过使用源自Google Vizier的数据，这是世界上最大的黑盒优化数据库之一，我们的大量实验表明，仅通过数学参数和值的文本表示，语言模型能够进行非常精确的数值回归，如果有机会训练多个任务，则可以显著优于传统的回归模型。

    arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
    
[^277]: 双稳健学习在处理效应估计中的结构不可知性最优性

    Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation

    [https://arxiv.org/abs/2402.14264](https://arxiv.org/abs/2402.14264)

    采用结构不可知的统计下界框架，证明了双稳健估计器在平均处理效应（ATE）和平均处理效应方面的统计最优性

    

    平均处理效应估计是因果推断中最核心的问题，应用广泛。虽然文献中提出了许多估计策略，最近还纳入了通用的机器学习估计器，但这些方法的统计最优性仍然是一个开放的研究领域。本文采用最近引入的统计下界结构不可知框架，该框架对干扰函数没有结构性质假设，除了访问黑盒估计器以达到小误差；当只愿意考虑使用非参数回归和分类神谕作为黑盒子过程的估计策略时，这一点尤其吸引人。在这个框架内，我们证明了双稳健估计器对于平均处理效应（ATE）和平均处理效应的统计最优性。

    arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
    
[^278]: NeuroFlux: 使用自适应局部学习进行高效CNN训练

    NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning

    [https://arxiv.org/abs/2402.14139](https://arxiv.org/abs/2402.14139)

    NeuroFlux是一个为内存受限场景量身定制的CNN训练系统，提出了自适应辅助网络和块特定的自适应批处理大小的创新机遇。

    

    在资源受限的移动和边缘环境中进行高效的设备内卷积神经网络（CNN）训练是一个挑战。本文介绍了NeuroFlux，这是一个为内存受限场景量身定制的CNN训练系统。我们提出了两个创新机遇：第一是采用可变数量滤波器的自适应辅助网络，以减少GPU内存的使用；第二是针对块特定的自适应批处理大小，既满足GPU内存限制，又加速训练过程。

    arXiv:2402.14139v1 Announce Type: new  Abstract: Efficient on-device convolutional neural network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in a substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments the CNNs into
    
[^279]: E2USD：用于多元时间序列的高效而有效的无监督状态检测

    E2USD: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series

    [https://arxiv.org/abs/2402.14041](https://arxiv.org/abs/2402.14041)

    E2USD提出了一种有效的无监督多元时间序列状态检测方法，利用了快速傅里叶变换和双视图嵌入模块进行编码，以及通过对抗学习方法消除假阴性，从而实现了SOTA准确性并显著降低了计算开销。

    

    我们提出了E2USD方法，能够实现高效而准确的无监督多元时间序列状态检测。E2USD利用基于快速傅立叶变换的时间序列压缩器(FFTCompress)和分解的双视图嵌入模块(DDEM)，一起以低计算开销对输入的多元时间序列进行编码。此外，我们提出了一种假阴性取消对比学习方法(FNCCLearning)，以抵消假阴性的影响，并实现更友好的簇嵌入空间。为了在流式设置中进一步减少计算开销，我们引入了自适应阈值检测(ADATD)。通过使用六个基线模型和六个数据集进行全面实验，我们证明E2USD能够在显著降低计算开销的情况下达到SOTA的准确性。我们的代码可在https://github.com/AI4CTS/E2Usd 找到。

    arXiv:2402.14041v1 Announce Type: cross  Abstract: We propose E2USD that enables efficient-yet-accurate unsupervised MTS state detection. E2USD exploits a Fast Fourier Transform-based Time Series Compressor (FFTCompress) and a Decomposed Dual-view Embedding Module (DDEM) that together encode input MTSs at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (FNCCLearning) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ADATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2USD is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at https://github.com/AI4CTS/E2Usd.
    
[^280]: SDXL-Lightning: 渐进式对抗性扩散蒸馏

    SDXL-Lightning: Progressive Adversarial Diffusion Distillation

    [https://arxiv.org/abs/2402.13929](https://arxiv.org/abs/2402.13929)

    提出了一种结合渐进和对抗性蒸馏的扩散蒸馏方法，在文本到图像生成任务中取得了新的最先进结果，并开源了相应模型。

    

    我们提出了一种扩散蒸馏方法，在基于SDXL的一步/几步1024像素文本到图像生成任务中实现了全新的最先进水平。我们的方法结合了渐进和对抗性蒸馏，实现了质量和模式覆盖之间的平衡。本文讨论了理论分析、判别器设计、模型公式和训练技巧。我们以LoRA和完整UNet权重的形式开源了我们的蒸馏SDXL-Lightning模型。

    arXiv:2402.13929v1 Announce Type: cross  Abstract: We propose a diffusion distillation method that achieves new state-of-the-art in one-step/few-step 1024px text-to-image generation based on SDXL. Our method combines progressive and adversarial distillation to achieve a balance between quality and mode coverage. In this paper, we discuss the theoretical analysis, discriminator design, model formulation, and training techniques. We open-source our distilled SDXL-Lightning models both as LoRA and full UNet weights.
    
[^281]: DSLR：多样性增强和结构学习用于基于重播的图持续学习

    DSLR: Diversity Enhancement and Structure Learning for Rehearsal-based Graph Continual Learning

    [https://arxiv.org/abs/2402.13711](https://arxiv.org/abs/2402.13711)

    DSLR提出了一种基于覆盖范围的多样性方法，以解决基于重播的图持续学习中回放节点过于集中导致过拟合和灾难性遗忘的问题。

    

    我们研究了基于重播方法中回放缓冲区对图持续学习（GCL）方法的影响。现有的基于重播的GCL方法为每个类别选择最具代表性的节点并将它们存储在重播缓冲区中，以供在训练后续任务时使用。然而，我们发现，仅考虑每个回放节点的类别代表性会使回放节点集中在每个类别的中心周围，可能存在过拟合于位于那些区域的节点的风险，从而加剧灾难性遗忘。此外，由于基于重播方法严重依赖于少数回放节点来保留从先前任务中获得的知识，涉及在模型训练中具有不相关邻居的回放节点可能对模型性能产生显着的负面影响。在本文中，我们提出了一种名为DSLR的GCL模型，具体来说，我们设计了一种基于覆盖范围的多样性（CD）

    arXiv:2402.13711v1 Announce Type: cross  Abstract: We investigate the replay buffer in rehearsal-based approaches for graph continual learning (GCL) methods. Existing rehearsal-based GCL methods select the most representative nodes for each class and store them in a replay buffer for later use in training subsequent tasks. However, we discovered that considering only the class representativeness of each replayed node makes the replayed nodes to be concentrated around the center of each class, incurring a potential risk of overfitting to nodes residing in those regions, which aggravates catastrophic forgetting. Moreover, as the rehearsal-based approach heavily relies on a few replayed nodes to retain knowledge obtained from previous tasks, involving the replayed nodes that have irrelevant neighbors in the model training may have a significant detrimental impact on model performance. In this paper, we propose a GCL model named DSLR, specifically, we devise a coverage-based diversity (CD)
    
[^282]: 基于大型语言模型的模态感知集成用于基于知识的视觉问答

    Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering

    [https://arxiv.org/abs/2402.12728](https://arxiv.org/abs/2402.12728)

    提出了一种模态感知的LLM集成方法（MAIL）用于针对KVQA，通过细致地利用多模态知识来处理图像理解和知识推理。

    

    知识驱动的视觉问答（KVQA）已被广泛研究，以利用外部知识如知识图谱（KG）来回答视觉问题。尽管已提出几种尝试利用大型语言模型（LLMs）作为隐含知识源，但由于LLMs可能生成幻觉，因此仍然具有挑战性。此外，多种知识来源，例如图像、知识图谱和LLMs，不能轻易对齐以应对复杂场景。为了解决这些问题，我们提出了一种针对KVQA的新颖的具有模态感知的LLM集成方法（MAIL）。它精心利用多模态知识进行图像理解和知识推理。具体而言，（i）我们提出了一种使用LLMs的两阶段提示策略，将图像密集地融入带有详细视觉特征的场景图中；（ii）我们通过将提到的实体与外部事实联系起来构建一个耦合的概念图；（iii）设计了一个定制的伪孪生图中介融合。

    arXiv:2402.12728v1 Announce Type: cross  Abstract: Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designe
    
[^283]: 复兴多变量时间序列预测：可学习分解与跨系列依赖关系和内部变化建模

    Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling

    [https://arxiv.org/abs/2402.12694](https://arxiv.org/abs/2402.12694)

    引入可学习分解策略和双注意力模块，同时捕捉跨系列依赖和内部变化，以应对复杂的多变量时间序列预测挑战。

    

    预测多变量时间序列是至关重要的，要求精确建模错综复杂模式，包括跨时间序列的依赖关系和内部变动。每个时间序列具有独特的趋势特征带来挑战，现有方法依赖基本的移动平均核可能难以处理现实数据中的非线性结构和复杂趋势。基于此，我们引入了一个可学习的分解策略，更合理地捕捉动态趋势信息。此外，我们提出了一个双注意力模块，专门用于同时捕捉跨系列依赖关系和内部变化，以实现更好的时间序列预测，其中通过通道自注意力和自回归自注意力实现。为了评估我们方法的有效性，我们在八个开源数据集上进行了实验，并将其与最先进的方法进行了比较。通过比较结果，我们的 Leddam...

    arXiv:2402.12694v1 Announce Type: new  Abstract: Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam
    
[^284]: 自主引导的稳健图结构细化

    Self-Guided Robust Graph Structure Refinement

    [https://arxiv.org/abs/2402.11837](https://arxiv.org/abs/2402.11837)

    本文提出了一个自主引导的GSR框架（SG-GSR），通过利用被攻击图中发现的干净子图，并提出了图增强和分组训练策略，以应对现有GSR方法在真实场景中受限的问题。

    

    近期研究发现，图神经网络容易受到对抗性攻击。为了抵御此类攻击，稳健图结构修正（GSR）方法旨在通过节点特征、图结构或外部信息来最小化对抗性边的影响。然而，我们发现现有的GSR方法受到狭窄假设的限制，比如假设干净的节点特征、适度的结构攻击以及可用的外部干净图，导致在真实场景中应用受限。本文提出了一个自主引导的GSR框架（SG-GSR），其利用给定被攻击图中发现的干净子图。此外，我们提出了一种新的图增强和分组训练策略来处理在干净子图提取中的两个技术挑战：1）结构信息的丢失，2）节点度分布的不平衡。大量实验证明

    arXiv:2402.11837v1 Announce Type: new  Abstract: Recent studies have revealed that GNNs are vulnerable to adversarial attacks. To defend against such attacks, robust graph structure refinement (GSR) methods aim at minimizing the effect of adversarial edges based on node features, graph structure, or external information. However, we have discovered that existing GSR methods are limited by narrowassumptions, such as assuming clean node features, moderate structural attacks, and the availability of external clean graphs, resulting in the restricted applicability in real-world scenarios. In this paper, we propose a self-guided GSR framework (SG-GSR), which utilizes a clean sub-graph found within the given attacked graph itself. Furthermore, we propose a novel graph augmentation and a group-training strategy to handle the two technical challenges in the clean sub-graph extraction: 1) loss of structural information, and 2) imbalanced node degree distribution. Extensive experiments demonstra
    
[^285]: 通过基于政策的自我判断来对齐大型语言模型

    Aligning Large Language Models by On-Policy Self-Judgment

    [https://arxiv.org/abs/2402.11253](https://arxiv.org/abs/2402.11253)

    本文提出了一个新颖的对齐框架SELF-JUDGE，通过增加式监督微调（JSFT）训练一个同时充当策略和评判器的单一模型，实现了参数高效的基于政策学习，无需额外的奖励模型。

    

    为了使大型语言模型与人类偏好保持一致，现有研究要么利用单独的奖励模型（RM）执行基于政策的学习，要么通过放弃基于政策的学习和对独立RM的需求简化训练过程。在本文中，我们提出了一个新颖的对齐框架SELF-JUDGE，它既是(1) 基于政策的学习，又是(2) 参数高效的，因为它不需要额外的RM来评估样本进行基于政策的学习。为此，我们提出了增强式监督微调（JSFT）来训练一个单一模型，作为策略和评判器。具体来说，我们将一对一判断任务视为指导式任务的特殊情况，从响应对中选择更好的响应。因此，得到的模型可以评判当前策略的即时响应偏好，从自身初始化。实验结果显示了SELF-JUDGE的有效性，优于基线模型。

    arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
    
[^286]: 加速半异步联邦学习

    Accelerating Semi-Asynchronous Federated Learning

    [https://arxiv.org/abs/2402.10991](https://arxiv.org/abs/2402.10991)

    提出了一种考虑贡献的异步联邦学习方法，动态调整接收到的更新的处理方式，以解决现实情况下同步上传数据可能出现的缓慢和不可靠问题。

    

    联邦学习（FL）是一种分布式机器学习范例，允许客户端在保护隐私的同时在其数据上训练模型。现有的FL算法，如Federated Averaging（FedAvg）及其变种，在许多情况下已经被证明收敛良好。然而，这些方法需要客户端以同步方式将其本地更新上传至服务器，这在现实情况下可能会变得缓慢和不可靠。为了解决这个问题，研究人员开发了异步FL方法，允许客户端继续使用陈旧的全局模型对其本地数据进行训练。然而，大多数这些方法仅仅聚合了所有接收到的更新，而没有考虑其相对贡献，这可能导致收敛速度变慢。在本文中，我们提出了一种考虑贡献的异步FL方法，考虑了接收到的更新的陈旧程度和统计异质性。我们的方法动态调整

    arXiv:2402.10991v1 Announce Type: cross  Abstract: Federated Learning (FL) is a distributed machine learning paradigm that allows clients to train models on their data while preserving their privacy. FL algorithms, such as Federated Averaging (FedAvg) and its variants, have been shown to converge well in many scenarios. However, these methods require clients to upload their local updates to the server in a synchronous manner, which can be slow and unreliable in realistic FL settings. To address this issue, researchers have developed asynchronous FL methods that allow clients to continue training on their local data using a stale global model. However, most of these methods simply aggregate all of the received updates without considering their relative contributions, which can slow down convergence. In this paper, we propose a contribution-aware asynchronous FL method that takes into account the staleness and statistical heterogeneity of the received updates. Our method dynamically adju
    
[^287]: 为什么Transformer对敏感函数困难?

    Why are Sensitive Functions Hard for Transformers?

    [https://arxiv.org/abs/2402.09963](https://arxiv.org/abs/2402.09963)

    本文证明了在Transformer架构下，损失函数的空间受到输入敏感性的限制，从而解释了Transformer对敏感函数的困难。这一理论统一了关于Transformer学习能力和偏见的广泛观察。

    

    经验研究发现，Transformer存在一系列的学习偏见和限制，如在学习计算简单形式语言（如PARITY）时的持久困难，以及对低阶函数的偏好。然而，现有的表达能力理论要么过度预测，要么低估了实际的学习能力。我们证明，在Transformer架构下，损失函数的空间受到输入敏感性的限制：输出对输入字符串的多个部分敏感的Transformer存在于参数空间中的孤立点，导致泛化中的低敏感性偏差。我们理论上和实证上证明了该理论统一了关于Transformer学习能力和偏见的广泛观察，如它们对低敏感性和低阶的泛化偏差，以及在长度泛化上的困难。

    arXiv:2402.09963v1 Announce Type: new  Abstract: Empirical studies have identified a range of learnability biases and limitations of transformers, such as a persistent difficulty in learning to compute simple formal languages such as PARITY, and a bias towards low-degree functions. However, theoretical understanding remains limited, with existing expressiveness theory either overpredicting or underpredicting realistic learning abilities. We prove that, under the transformer architecture, the loss landscape is constrained by the input-space sensitivity: Transformers whose output is sensitive to many parts of the input string inhabit isolated points in parameter space, leading to a low-sensitivity bias in generalization. We show theoretically and empirically that this theory unifies a broad array of empirical observations about the learning abilities and biases of transformers, such as their generalization bias towards low sensitivity and low degree, and difficulty in length generalizati
    
[^288]: MC-DBN：基于深度信念网络的模态补全模型

    MC-DBN: A Deep Belief Network-Based Model for Modality Completion

    [https://arxiv.org/abs/2402.09782](https://arxiv.org/abs/2402.09782)

    MC-DBN是一种基于深度信念网络的模态补全模型，利用完整数据的隐式特征来弥补附加不完整数据的差距，提高预测准确性。

    

    最近多模态人工智能（AI）的进展已经彻底改变了股市预测和心率监测领域。利用多样的数据源可以大大提高预测准确性。然而，额外的数据可能不总是与原始数据集相吻合。插值方法通常用于处理模态数据中的缺失值，但在稀疏信息情况下可能存在一些限制。为解决这一挑战，我们提出了一种模态补全的深度信念网络模型（MC-DBN）。该方法利用完整数据的隐式特征来弥补自身与附加不完整数据之间的差距。它确保增强的多模态数据与现实世界的动态特性密切相符，以提高模型的有效性。我们在两个来自股市预测和心率监测的数据集上对MC-DBN模型进行了评估。

    arXiv:2402.09782v1 Announce Type: cross  Abstract: Recent advancements in multi-modal artificial intelligence (AI) have revolutionized the fields of stock market forecasting and heart rate monitoring. Utilizing diverse data sources can substantially improve prediction accuracy. Nonetheless, additional data may not always align with the original dataset. Interpolation methods are commonly utilized for handling missing values in modal data, though they may exhibit limitations in the context of sparse information. Addressing this challenge, we propose a Modality Completion Deep Belief Network-Based Model (MC-DBN). This approach utilizes implicit features of complete data to compensate for gaps between itself and additional incomplete data. It ensures that the enhanced multi-modal data closely aligns with the dynamic nature of the real world to enhance the effectiveness of the model. We conduct evaluations of the MC-DBN model in two datasets from the stock market forecasting and heart rate
    
[^289]: 无需稀疏模型的稀疏且准确的解释

    Sparse and Faithful Explanations Without Sparse Models

    [https://arxiv.org/abs/2402.09702](https://arxiv.org/abs/2402.09702)

    引入了稀疏解释值(SEV)，用于衡量机器学习模型的决策稀疏性。即使模型不是稀疏的，许多机器学习模型在SEV的衡量下仍具有低决策稀疏性。

    

    即使模型不满足全局的稀疏性，决策仍然可以用少量的特征准确地描述。例如，对于某人而言，尽管没有信用历史，但申请大笔贷款可能会被拒绝，这就忽视了与其信用价值相关的任何证据。在本论文中，我们引入了稀疏解释值（SEV），这是一种衡量机器学习模型稀疏性的新方法。在以上贷款拒绝的例子中，SEV为1，因为只需要一个因素来解释为什么贷款被拒绝。SEV是对决策稀疏性的衡量，而不是对整体模型稀疏性的衡量，并且我们能够证明许多机器学习模型——即使它们不是稀疏的——实际上在SEV的衡量下具有低决策稀疏性。SEV使用超立方体上的移动进行定义，使得SEV能够在各种模型类别上一致地定义，其中移动限制反映了模型的性质。

    arXiv:2402.09702v1 Announce Type: new  Abstract: Even if a model is not globally sparse, it is possible for decisions made from that model to be accurately and faithfully described by a small number of features. For instance, an application for a large loan might be denied to someone because they have no credit history, which overwhelms any evidence towards their creditworthiness. In this work, we introduce the Sparse Explanation Value (SEV), a new way of measuring sparsity in machine learning models. In the loan denial example above, the SEV is 1 because only one factor is needed to explain why the loan was denied. SEV is a measure of decision sparsity rather than overall model sparsity, and we are able to show that many machine learning models -- even if they are not sparse -- actually have low decision sparsity, as measured by SEV. SEV is defined using movements over a hypercube, allowing SEV to be defined consistently over various model classes, with movement restrictions reflectin
    
[^290]: 通过机器学习在不断演化的知识图谱上预测高影响力的研究主题

    Forecasting high-impact research topics via machine learning on evolving knowledge graphs

    [https://arxiv.org/abs/2402.08640](https://arxiv.org/abs/2402.08640)

    通过机器学习预测未发布研究想法的影响力，我们使用一个由超过2100万篇科学论文构建的演化知识图谱，结合论文内容和历史引用的信息，高准确度预测未来的演化网络动态和新的研究方向的影响力。

    

    科学出版物的指数增长对人类研究者构成了严峻挑战。它迫使研究者将注意力集中在更狭窄的子领域上，使得发现其他领域的新颖且有影响力的研究想法和合作变得困难。虽然有办法预测科学论文未来的引用次数，但通常需要等到研究完成并且论文写成后才能进行评估，这样就错过了想法构思的早期阶段。在本文中，我们展示了如何预测从未被研究者发布的想法的影响力。为此，我们开发了一个大型的演化知识图谱，其中包含超过2100万篇科学论文。它结合了从论文内容中创建的语义网络和从历史引用中创建的影响网络。利用机器学习，我们可以高准确度地预测演化网络的动态情况，从而预测新的研究方向的影响力。我们预期这种能力将有助于研究者发现具有高影响力的研究主题。

    The exponential growth in scientific publications poses a severe challenge for human researchers. It forces attention to more narrow sub-fields, which makes it challenging to discover new impactful research ideas and collaborations outside one's own field. While there are ways to predict a scientific paper's future citation counts, they need the research to be finished and the paper written, usually assessing impact long after the idea was conceived. Here we show how to predict the impact of onsets of ideas that have never been published by researchers. For that, we developed a large evolving knowledge graph built from more than 21 million scientific papers. It combines a semantic network created from the content of the papers and an impact network created from the historic citations of papers. Using machine learning, we can predict the dynamic of the evolving network into the future with high accuracy, and thereby the impact of new research directions. We envision that the ability to 
    
[^291]: 无假设测试算法性能的限制

    The Limits of Assumption-free Tests for Algorithm Performance

    [https://arxiv.org/abs/2402.07388](https://arxiv.org/abs/2402.07388)

    这项研究探讨了使用有限数据量回答算法性能问题的基本限制，证明了黑盒测试方法无法准确回答算法在不同训练集上的整体性能和特定模型的性能问题。

    

    算法评价和比较是机器学习和统计学中基本的问题，一个算法在给定的建模任务中表现如何，哪个算法表现最佳？许多方法已经开发出来评估算法性能，通常基于交叉验证策略，将感兴趣的算法在不同的数据子集上重新训练，并评估其在留出数据点上的性能。尽管广泛使用这些程序，但对于这些方法的理论性质尚未完全理解。在这项工作中，我们探讨了在有限的数据量下回答这些问题的一些基本限制。特别地，我们区分了两个问题: 算法$A$在大小为$n$的训练集上学习问题有多好，以及在特定大小为$n$的训练数据集上运行$A$所产生的特定拟合模型有多好？我们的主要结果证明，对于任何将算法视为黑盒的测试方法，无法准确地回答这两个问题。

    Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algor
    
[^292]: 功能对齐回归：一种从数据中明确学习函数导数的方法

    Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data

    [https://arxiv.org/abs/2402.06104](https://arxiv.org/abs/2402.06104)

    该论文提出了一种名为FAR的方法，通过捕捉函数导数来更好、更高效地拟合底层真实函数。在合成数据集和八个真实世界任务中证明了该方法的有效性。

    

    回归是机器学习中的一个基本任务，在过去几十年中引起了广泛关注。传统的回归方法主要通过使用损失函数来将模型预测与每个个体数据样本的真实值对齐，然而，我们发现这种方法可能导致在不同样本之间关系的预测不够优化。近期的研究工作引入了标签相似性信息来改进回归方法，但在完全捕捉底层真实函数的复杂性方面仍存在明显的差距。在本文中，我们提出了FAR（功能对齐回归）作为一种更好、更高效的解决方案，通过捕捉函数导数来拟合底层真实函数。我们在两个合成数据集和六个领域的八个大规模真实世界任务中验证了该方法的有效性。

    Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
    
[^293]: 最后之舞：通过扩散模型和贝叶斯方法进行鲁棒后门攻击

    The last Dance : Robust backdoor attack via diffusion models and bayesian approach

    [https://arxiv.org/abs/2402.05967](https://arxiv.org/abs/2402.05967)

    本文介绍了一种通过扩散模型和贝叶斯方法进行鲁棒后门攻击的方法，具体应用于音频Transformer模型，并证明了攻击的可行性。

    

    扩散模型是最先进的深度学习生成模型，其通过逐步添加噪音和去噪的方式学习正向和反向扩散过程的原理进行训练。本文旨在欺骗基于音频的DNN模型，例如Hugging Face框架中的音频模型，特别是基于Transformer的人工智能模型，这些模型是强大的机器学习模型，节省时间，提供更高效的结果。我们证明了在Hugging Face推导出的音频Transformer上实现后门攻击（称为`BacKBayDiffMod`）的可行性。本文中开发的后门攻击基于毒化模型的训练数据，涉及后门扩散采样和贝叶斯方法分布的引入。

    Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
    
[^294]: 实时瓶颈和激波预测的中尺度交通预测

    Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction

    [https://arxiv.org/abs/2402.05663](https://arxiv.org/abs/2402.05663)

    该论文介绍了一种在实时中尺度交通预测中具有最先进效果的深度预测方法SA-LSTM，通过将自注意力与长短期记忆结合，实现了对多步预测的改进，并在短期和长期预测之间取得了平衡。

    

    准确的实时交通状态预测在交通控制研究中起着关键作用。特别是CIRCLES联合项目需要预测技术来减轻数据源延迟的影响。在MegaVanderTest实验取得成功之后，本文旨在克服当前系统限制，开发更适合的方法来改善下一轮实验的实时交通状态估计。在本文中，我们介绍了SA-LSTM，这是一种深度预测方法，将自注意力（SA）与长短期记忆（LSTM）在空间维度上结合，可以在实时中尺度交通预测中获得最先进的结果。我们将这种方法扩展到多步预测，使用n-step SA-LSTM，在短期和长期预测之间的平衡中优于传统的多步预测方法，同时实时运行。

    Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time.
    
[^295]: Minecraft-ify：用于游戏应用的Minecraft风格图像生成与文本引导的图像编辑

    Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application

    [https://arxiv.org/abs/2402.05448](https://arxiv.org/abs/2402.05448)

    本文提出了一种用于Minecraft游戏应用的图像生成和编辑系统"Minecraft-ify"，能够生成针对3D虚拟角色的面部聚焦图像，并支持使用文本进行图像编辑，提供了更自由和优化的用户体验。

    

    本文首先介绍了面向Minecraft视频游戏的角色纹理生成系统"Minecraft-ify"，该系统可以生成针对具有立方体流形的3D虚拟角色的面部聚焦图像以进行纹理映射。与现有项目或作品只生成纹理不同，提出的系统可以反转用户提供的真实图像，或从学习到的分布生成平均/随机外观。此外，它可以使用StyleGAN和StyleCLIP进行文本引导的操作。这些功能提供了更广泛的用户体验和更多的自由，是一种用户友好的AI工具。

    In this paper, we first present the character texture generation system \textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/
    
[^296]: SALAD-Bench: 一个针对大语言模型的层次化和全面性安全基准

    SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models

    [https://arxiv.org/abs/2402.05044](https://arxiv.org/abs/2402.05044)

    SALAD-Bench是一个针对大语言模型的全面安全基准，通过其大规模、丰富的分类和多功能性，以及对攻击和防御方法的评估，实现了对LLMs的有效管理和保护。

    

    在快速发展的大语言模型（LLM）领域中，确保强大的安全措施至关重要。为了满足这一关键需求，我们提出了一种特别设计用于评估LLMs、攻击和防御方法的安全基准，称为SALAD-Bench。SALAD-Bench通过其大规模、丰富多样的特性，以及跨三个层次的细致分类和多功能性，超越了传统基准。SALAD-Bench通过对标准查询和复杂查询（包括攻击、防御修改和多项选择）的精心设计，有效管理其固有的复杂性。为了确保无缝可靠的评估，我们引入了一种创新的评估器：基于LLM的MD-Judge，专注于攻击增强查询的问答对评估。以上组件将SALAD-Bench从标准的LLM安全评估扩展到了LLM攻击和防御方法评估，确保了联合目标的实用性。

    In the rapidly evolving landscape of Large Language Models (LLMs), ensuring robust safety measures is paramount. To meet this crucial need, we propose \emph{SALAD-Bench}, a safety benchmark specifically designed for evaluating LLMs, attack, and defense methods. Distinguished by its breadth, SALAD-Bench transcends conventional benchmarks through its large scale, rich diversity, intricate taxonomy spanning three levels, and versatile functionalities.SALAD-Bench is crafted with a meticulous array of questions, from standard queries to complex ones enriched with attack, defense modifications and multiple-choice. To effectively manage the inherent complexity, we introduce an innovative evaluators: the LLM-based MD-Judge for QA pairs with a particular focus on attack-enhanced queries, ensuring a seamless, and reliable evaluation. Above components extend SALAD-Bench from standard LLM safety evaluation to both LLM attack and defense methods evaluation, ensuring the joint-purpose utility. Our e
    
[^297]: 导航复杂性：通过扩展窗口匹配实现无损图谱精简

    Navigating Complexity: Toward Lossless Graph Condensation via Expanding Window Matching

    [https://arxiv.org/abs/2402.05011](https://arxiv.org/abs/2402.05011)

    本文通过连接先前被忽视的监督信号的方式，首次尝试实现无损图谱精简，以解决现有方法无法准确复制原始图谱的问题。

    

    图谱精简旨在通过合成紧凑的图谱来减少大规模图谱数据集的大小，同时不损失在其上训练的图神经网络（GNNs）的性能，这为减少训练GNNs的计算成本提供了启示。然而，现有方法往往无法准确复制某些数据集的原始图谱，从而未能实现无损精简的目标。为了理解这一现象，我们调查了潜在的原因，并揭示了先前最先进的轨迹匹配方法在优化精简图谱时提供了来自原始图谱的偏倚和受限的监督信号。这严重限制了精简图谱的规模和功效。在本文中，我们首次尝试通过连接先前被忽视的监督信号来实现无损图谱精简。

    Graph condensation aims to reduce the size of a large-scale graph dataset by synthesizing a compact counterpart without sacrificing the performance of Graph Neural Networks (GNNs) trained on it, which has shed light on reducing the computational cost for training GNNs. Nevertheless, existing methods often fall short of accurately replicating the original graph for certain datasets, thereby failing to achieve the objective of lossless condensation. To understand this phenomenon, we investigate the potential reasons and reveal that the previous state-of-the-art trajectory matching method provides biased and restricted supervision signals from the original graph when optimizing the condensed one. This significantly limits both the scale and efficacy of the condensed graph. In this paper, we make the first attempt toward \textit{lossless graph condensation} by bridging the previously neglected supervision signals. Specifically, we employ a curriculum learning strategy to train expert traje
    
[^298]: RL-VLM-F: 强化学习通过视觉语言基础模型反馈

    RL-VLM-F: Reinforcement Learning from Vision Language Foundation Model Feedback

    [https://arxiv.org/abs/2402.03681](https://arxiv.org/abs/2402.03681)

    RL-VLM-F是一种通过视觉语言基础模型反馈的强化学习方法，能够自动生成有效的奖励函数和策略，从而解决了传统强化学习中奖励设计的挑战。

    

    传统强化学习研究中的奖励设计一直是一个挑战，因为通常需要大量人力和反复试错的过程来设计有效的奖励函数。本文提出了一种自动生成奖励函数的方法，用于代理学习新任务，只使用任务目标的文本描述和代理的视觉观测，并利用视觉语言基础模型（VLMs）的反馈。我们的方法的关键是通过查询这些模型，基于任务目标的文本描述给出对代理的图像观测的偏好，并从偏好标签中学习奖励函数，而不是直接要求这些模型输出原始奖励分数，这可能存在噪音和不一致性。我们证明了RL-VLM-F在各种领域中成功地产生了有效的奖励和策略，包括经典控制以及刚性和灵活操纵方面。

    Reward engineering has long been a challenge in Reinforcement Learning (RL) research, as it often requires extensive human effort and iterative processes of trial-and-error to design effective reward functions. In this paper, we propose RL-VLM-F, a method that automatically generates reward functions for agents to learn new tasks, using only a text description of the task goal and the agent's visual observations, by leveraging feedbacks from vision language foundation models (VLMs). The key to our approach is to query these models to give preferences over pairs of the agent's image observations based on the text description of the task goal, and then learn a reward function from the preference labels, rather than directly prompting these models to output a raw reward score, which can be noisy and inconsistent. We demonstrate that RL-VLM-F successfully produces effective rewards and policies across various domains - including classic control, as well as manipulation of rigid, articulate
    
[^299]: 我们能去掉自适应梯度方法中的平方根吗？一个二阶角度的研究

    Can We Remove the Square-Root in Adaptive Gradient Methods? A Second-Order Perspective

    [https://arxiv.org/abs/2402.03496](https://arxiv.org/abs/2402.03496)

    移除自适应方法中的平方根可以在卷积结构上减小与SGD的泛化差距，同时保持在transformers上的性能。

    

    自适应梯度优化器如Adam(W)是许多深度学习结构（如transformers）的默认训练算法。它们的对角先验基于梯度外积，通过平方根加入到参数更新中。虽然这些方法通常被称为近似的二阶方法，但平方根表示了一个根本性的区别。在这项研究中，我们研究了在去掉平方根后自适应方法的行为如何变化，即加强它们的二阶动机。令人惊讶的是，我们发现这种去掉平方根的自适应方法能够在卷积结构上缩小与SGD的泛化差距，同时保持了在transformers上基于平方根的方法的性能。二阶角度对于开发具有非对角先验的自适应方法也具有实际好处。与像Shampoo这样基于平方根的对应方法不同，它们不需要数值不稳定的矩阵平方。

    Adaptive gradient optimizers like Adam(W) are the default training algorithms for many deep learning architectures, such as transformers. Their diagonal preconditioner is based on the gradient outer product which is incorporated into the parameter update via a square root. While these methods are often motivated as approximate second-order methods, the square root represents a fundamental difference. In this work, we investigate how the behavior of adaptive methods changes when we remove the root, i.e. strengthen their second-order motivation. Surprisingly, we find that such square-root-free adaptive methods close the generalization gap to SGD on convolutional architectures, while maintaining their root-based counterpart's performance on transformers. The second-order perspective also has practical benefits for the development of adaptive methods with non-diagonal preconditioner. In contrast to root-based counterparts like Shampoo, they do not require numerically unstable matrix square
    
[^300]: ToonAging: 艺术肖像风格转换下的人脸逆龄化

    ToonAging: Face Re-Aging upon Artistic Portrait Style Transfer

    [https://arxiv.org/abs/2402.02733](https://arxiv.org/abs/2402.02733)

    本研究提出了一种新颖的一阶段方法，结合肖像风格转换实现人脸逆龄化，解决了NPR图像上编辑年龄的问题，并在单个生成步骤中执行。该方法利用了现有的人脸逆龄化和风格转换网络，并且独特地融合了不同的潜在向量，从而保留了面部属性。

    

    人脸逆龄化是计算机视觉和图形学中的一个重要领域，在电影、广告和直播等逼真领域中具有重要应用。最近，将人脸逆龄化应用于非逼真图像，如漫画、插图和动画，在各种娱乐行业中成为一个新的需求。然而，缺乏一个能够无缝编辑NPR图像上显现年龄的网络意味着这些任务一直局限于一个简单的顺序方法，这往往会导致不愉快的伪影和由于域差异而丢失面部属性。在本文中，我们引入了一种新颖的单阶段人脸逆龄化方法，结合了肖像风格转换，在一个生成步骤中完成。我们利用现有的人脸逆龄化和风格转换网络，两者都在相同的PR领域进行训练。我们的方法独特地融合了不同的潜在向量，每个向量负责管理与衰老相关的属性。

    Face re-aging is a prominent field in computer vision and graphics, with significant applications in photorealistic domains such as movies, advertising, and live streaming. Recently, the need to apply face re-aging to non-photorealistic images, like comics, illustrations, and animations, has emerged as an extension in various entertainment sectors. However, the absence of a network capable of seamlessly editing the apparent age on NPR images means that these tasks have been confined to a naive approach, applying each task sequentially. This often results in unpleasant artifacts and a loss of facial attributes due to domain discrepancies. In this paper, we introduce a novel one-stage method for face re-aging combined with portrait style transfer, executed in a single generative step. We leverage existing face re-aging and style transfer networks, both trained within the same PR domain. Our method uniquely fuses distinct latent vectors, each responsible for managing aging-related attribu
    
[^301]: 一个真正联合的神经网络架构用于分割和解析

    A Truly Joint Neural Architecture for Segmentation and Parsing

    [https://arxiv.org/abs/2402.02564](https://arxiv.org/abs/2402.02564)

    本文通过引入一个联合神经网络架构，在形态丰富的语言中实现了同时进行形态分割和句法分析的任务。通过提供基于格子的表示法，保留了输入的所有形态模糊性，有效解决了以往基于神经网络的依存句法分析器的局限性。

    

    当代多语言依存句法分析器可以解析多种语言，但对于形态丰富的语言而言，其性能明显低于其他语言。主要挑战是由于输入标记的形态复杂性和模糊性较高，作为树中节点的语言单位事先是未知的。以往的基于神经网络的形态丰富语言的依存句法分析器遵循联合形态-句法假设，即形态分割和句法分析应该在解析过程中一并解决，而不是先进行分割再进行解析的流程。然而，目前的神经网络依存句法分析器采用严格的流水线方法。在本文中，我们引入了一个联合神经网络架构，将基于格子的表示法保留输入的所有形态模糊性，然后将其提供给一个基于弧的模型，该模型能够同时解决形态分割和句法分析任务。我们在希伯来语上进行了实验，该语言形态丰富且模糊性较高，结果表明...

    Contemporary multilingual dependency parsers can parse a diverse set of languages, but for Morphologically Rich Languages (MRLs), performance is attested to be lower than other languages. The key challenge is that, due to high morphological complexity and ambiguity of the space-delimited input tokens, the linguistic units that act as nodes in the tree are not known in advance. Pre-neural dependency parsers for MRLs subscribed to the joint morpho-syntactic hypothesis, stating that morphological segmentation and syntactic parsing should be solved jointly, rather than as a pipeline where segmentation precedes parsing. However, neural state-of-the-art parsers to date use a strict pipeline. In this paper we introduce a joint neural architecture where a lattice-based representation preserving all morphological ambiguity of the input is provided to an arc-factored model, which then solves the morphological segmentation and syntactic parsing tasks at once. Our experiments on Hebrew, a rich and
    
[^302]: LQER: 低秩量化误差重建用于LLMs

    LQER: Low-Rank Quantization Error Reconstruction for LLMs

    [https://arxiv.org/abs/2402.02446](https://arxiv.org/abs/2402.02446)

    LQER使用低秩逼近和激活引起的尺度矩阵，实现了对LLMs的近乎无损量化，无需知识蒸馏或梯度优化，并大幅减少硬件资源的使用。

    

    大型语言模型（LLMs）的训练后量化是具有挑战性的。在这项工作中，我们介绍了低秩量化误差减少（LQER）方法，该方法结合了量化和低秩逼近来恢复模型的能力。LQER利用激活引起的尺度矩阵将量化误差的奇异值分布推向期望的分布，从而实现了在各种LLMs和下游任务上近乎无损的W4A8量化，无需知识蒸馏、网格搜索或基于梯度的迭代优化。与现有方法不同，LQER的计算模式消除了从不规则内存位置收集高精度权重所需的专用Scatter和Gather过程。我们的W4A8 LLMs在六个热门下游任务上实现了近乎无损的性能，同时使用的硬件资源比领先的最新方法少1.36倍。一旦论文被接受，我们将开源我们的框架。

    Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.
    
[^303]: 学习通过原始-对偶策略梯度算法对无限时域平均回报受限MDP进行参数化通用策略

    Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm

    [https://arxiv.org/abs/2402.02042](https://arxiv.org/abs/2402.02042)

    该论文研究了无限时域平均回报受限MDPs的参数化通用策略，并提出了一种基于原始-对偶策略梯度算法，可在保证低遗憾的情况下管理约束条件，达到全局最优策略。算法的分析表明，其目标遗憾和约束违反均为 $\tilde{\mathcal{O}}({T}^{3/4})$。

    

    本文探索了无限时域平均回报受限马尔科夫决策过程（CMDP）的领域。据我们所知，这项工作是首次研究具有通用策略参数化的平均回报CMDP的遗憾和约束违规分析。为了解决这个挑战，我们提出了一种基于原始对偶的策略梯度算法，能够灵活地管理约束条件，并确保低遗憾保证以实现全局最优策略。特别地，我们证明了我们提出的算法在目标遗憾和约束违反上具有 $\tilde{\mathcal{O}}({T}^{3/4})$ 的界限。

    This paper explores the realm of infinite horizon average reward Constrained Markov Decision Processes (CMDP). To the best of our knowledge, this work is the first to delve into the regret and constraint violation analysis of average reward CMDPs with a general policy parametrization. To address this challenge, we propose a primal dual based policy gradient algorithm that adeptly manages the constraints while ensuring a low regret guarantee toward achieving a global optimal policy. In particular, we demonstrate that our proposed algorithm achieves $\tilde{\mathcal{O}}({T}^{3/4})$ objective regret and $\tilde{\mathcal{O}}({T}^{3/4})$ constraint violation bounds.
    
[^304]: 通过定向表示优化实现的安全提示驱动的大型语言模型(LLM)保护

    Prompt-Driven LLM Safeguarding via Directed Representation Optimization

    [https://arxiv.org/abs/2401.18018](https://arxiv.org/abs/2401.18018)

    通过研究模型表示的影响，我们发现安全提示并没有明显增强恶意和无害查询之间的区分，并提出了一种名为DRO的方法，用于自动优化安全提示。

    

    在大型语言模型(LLM)中，使用安全提示在模型输入之前是一种常见的保护实践，以使其不遵从包含恶意意图的查询。然而，安全提示的工作机制尚未完全理解，这妨碍了自动优化其以改善LLM安全性的潜力。针对这个问题，我们从模型表示的角度调查了安全提示的影响。我们发现在模型的表示空间中，有害和无害的查询可以在很大程度上区分开来，但安全提示并没有明显增强这一区分。相反，不同安全提示导致查询的表示朝着相似的方向移动，使得模型即使在查询无害时也更容易拒绝提供协助。受到这些发现的启发，我们提出了一种名为DRO（定向表示优化）的方法，用于自动安全提示优化。DRO将安全提示视为要优化的表示方向。

    Prepending model inputs with safety prompts is a common practice of safeguarding large language models (LLMs) from complying with queries that contain harmful intents. However, the working mechanisms of safety prompts have not yet been fully understood, which hinders the potential for automatically optimizing them for improved LLM safety. Motivated by this problem, we investigate the impact of safety prompts from the perspective of model representations. We find that in models' representation space, harmful and harmless queries can be largely distinguished, but this is not noticeably enhanced by safety prompts. Instead, the queries' representations are moved by different safety prompts in similar directions, where models become more prone to refusal (i.e., refusing to provide assistance) even when the queries are harmless. Inspired by these findings, we propose a method called DRO (Directed Representation Optimization) for automatic safety prompt optimization. DRO treats safety prompts
    
[^305]: 为核心集选择贡献的深度特征维度结构

    Contributing Dimension Structure of Deep Feature for Coreset Selection

    [https://arxiv.org/abs/2401.16193](https://arxiv.org/abs/2401.16193)

    现有核心集选择方法未能有效解决深度特征维度结构中维度之间差异对最终相似性的贡献，导致多样性样本选择的结果次优。

    

    核心集选择旨在为高效学习选择一组关键训练样本。随着训练数据集大小的激增，它在深度学习中日益受到关注。样本选择取决于两个主要方面：样本在提高性能方面的表示以及样本多样性在避免过拟合方面的作用。现有方法通常根据诸如L2范数之类的相似性度量来衡量数据的表示和多样性。它们通过特征、梯度或其他信息之间的相似性来引导分布匹配，有效地解决了表示问题。然而，有效多样性样本选择的结果却受困于次优性。这是因为相似性度量通常仅仅聚合维度相似性，而未意识到对于最终相似性贡献显著的维度之间的差异。因此，它们未能达到充分的效果。

    arXiv:2401.16193v2 Announce Type: replace  Abstract: Coreset selection seeks to choose a subset of crucial training samples for efficient learning. It has gained traction in deep learning, particularly with the surge in training dataset sizes. Sample selection hinges on two main aspects: a sample's representation in enhancing performance and the role of sample diversity in averting overfitting. Existing methods typically measure both the representation and diversity of data based on similarity metrics, such as L2-norm. They have capably tackled representation via distribution matching guided by the similarities of features, gradients, or other information between data. However, the results of effectively diverse sample selection are mired in sub-optimality. This is because the similarity metrics usually simply aggregate dimension similarities without acknowledging disparities among the dimensions that significantly contribute to the final similarity. As a result, they fall short of ade
    
[^306]: INCPrompt：面向任务的增量提示，无需重复练习的类别增量学习

    INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning

    [https://arxiv.org/abs/2401.11667](https://arxiv.org/abs/2401.11667)

    INCPrompt采用自适应关键学习者和面向任务的提示，结合通用和任务特定知识，有效缓解灾难性遗忘，表现优越，对持续学习性能具有显著影响。

    

    本文介绍了INCPrompt，一种创新的持续学习解决方案，有效地解决了灾难性遗忘问题。 INCPrompt的关键创新在于其使用自适应的关键学习者和面向任务的提示来捕获与任务相关的信息。 这种独特组合封装了跨任务的通用知识并编码了任务特定知识。 我们在多个持续学习基准上进行的全面评估表明，INCPrompt优于现有算法，显示出其在减轻灾难性遗忘的同时保持高性能的有效性。 这些结果突显了面向任务的增量提示对持续学习性能的重要影响。

    arXiv:2401.11667v2 Announce Type: replace  Abstract: This paper introduces INCPrompt, an innovative continual learning solution that effectively addresses catastrophic forgetting. INCPrompt's key innovation lies in its use of adaptive key-learner and task-aware prompts that capture task-relevant information. This unique combination encapsulates general knowledge across tasks and encodes task-specific knowledge. Our comprehensive evaluation across multiple continual learning benchmarks demonstrates INCPrompt's superiority over existing algorithms, showing its effectiveness in mitigating catastrophic forgetting while maintaining high performance. These results highlight the significant impact of task-aware incremental prompting on continual learning performance.
    
[^307]: 推断图神经网络的属性

    Inferring Properties of Graph Neural Networks

    [https://arxiv.org/abs/2401.03790](https://arxiv.org/abs/2401.03790)

    提出了GNNInfer，这是用于GNN的首个自动属性推断技术，通过识别和转换代表性影响结构，捕捉并推广GNN特定属性，最终通过建立模型提高推断属性的正确性。

    

    我们提出了GNNInfer，这是用于GNN的首个自动属性推断技术。为了解决GNN中可变输入结构的挑战，GNNInfer首先识别一组对GNN预测有显著贡献的代表性影响结构。利用这些结构，GNNInfer将每对影响结构和GNN转换为它们等效的FNN，然后利用现有的属性推断技术有效捕捉与这些影响结构特定的GNN属性。然后，GNNInfer将捕获的属性推广到包含这些影响结构的任何输入图中。最后，GNNInfer通过建立一个模型（决策树或线性回归）来提高推断属性的正确性，该模型估计给定完整输入图时GNN输出与推断属性的偏差。学习的模型有助于GNNInfer扩展推断的属性。

    arXiv:2401.03790v2 Announce Type: replace  Abstract: We propose GNNInfer, the first automatic property inference technique for GNNs. To tackle the challenge of varying input structures in GNNs, GNNInfer first identifies a set of representative influential structures that contribute significantly towards the prediction of a GNN. Using these structures, GNNInfer converts each pair of an influential structure and the GNN to their equivalent FNN and then leverages existing property inference techniques to effectively capture properties of the GNN that are specific to the influential structures. GNNINfer then generalizes the captured properties to any input graphs that contain the influential structures. Finally, GNNInfer improves the correctness of the inferred properties by building a model (either a decision tree or linear regression) that estimates the deviation of GNN output from the inferred properties given full input graphs. The learned model helps GNNInfer extend the inferred prope
    
[^308]: 使用孪生结构对GPT-3嵌入进行细化，用于技术帖子的重复检测

    Refining GPT-3 Embeddings with a Siamese Structure for Technical Post Duplicate Detection

    [https://arxiv.org/abs/2312.15068](https://arxiv.org/abs/2312.15068)

    本研究提出了使用孪生结构对GPT-3嵌入进行细化的方法，用于技术帖子的重复检测，解决了现有方法中存在的限制性问题。

    

    技术在线社区的一个目标是帮助开发者在一个地方找到正确答案。一个问题可以以不同的方式和措辞被提出，导致技术论坛上存在重复帖子。如何发现和链接重复帖子引起开发者社区和研究人员的关注。例如，Stack Overflow采用基于投票的机制来标记和关闭重复帖子。然而，及时处理这些不断出现的重复帖子仍然具有挑战性。因此，已经提出了各种方法来自动检测技术论坛帖子中的重复帖子。现有方法存在局限性，要么依赖于手工制作的相似性度量，无法充分捕捉帖子的语义，要么缺乏监督以提高性能。

    arXiv:2312.15068v2 Announce Type: replace-cross  Abstract: One goal of technical online communities is to help developers find the right answer in one place. A single question can be asked in different ways with different wordings, leading to the existence of duplicate posts on technical forums. The question of how to discover and link duplicate posts has garnered the attention of both developer communities and researchers. For example, Stack Overflow adopts a voting-based mechanism to mark and close duplicate posts. However, addressing these constantly emerging duplicate posts in a timely manner continues to pose challenges. Therefore, various approaches have been proposed to detect duplicate posts on technical forum posts automatically. The existing methods suffer from limitations either due to their reliance on handcrafted similarity metrics which can not sufficiently capture the semantics of posts, or their lack of supervision to improve the performance. Additionally, the efficienc
    
[^309]: 高维和高阶物理启发神经网络的 Hutchinson 迹估计

    Hutchinson Trace Estimation for High-Dimensional and High-Order Physics-Informed Neural Networks

    [https://arxiv.org/abs/2312.14499](https://arxiv.org/abs/2312.14499)

    介绍了 Hutchinson 迹估计（HTE），通过将整个 Hessian 矩阵的计算转换为 Hessian 矢量乘积（HVP），解决了 PINNs 处理高维和高阶 PDE 的挑战。

    

    arXiv:2312.14499v2 公告类型：替代交叉 摘要：物理启发神经网络（PINNs）已被证明在解决偏微分方程（PDEs）方面非常有效，特别是当一些数据可用时，通过无缝融合数据和物理学。然而，将PINNs扩展到高维甚至高阶PDE在自动微分在残差损失中的计算成本方面遇到了重大挑战。在这里，我们通过引入 Hutchinson 迹估计（HTE）来解决PINNs处理高维和高阶PDE的局限性。从科学计算中普遍存在的二阶高维PDE入手，HTE将整个Hessian矩阵的计算转换为Hessian矢量乘积（HVP）。这种方法通过 Taylor 模式自动微分减轻了计算瓶颈，并将内存消耗从Hessian矩阵减少到HVP。我们进一步展示了HTE收敛到或者

    arXiv:2312.14499v2 Announce Type: replace-cross  Abstract: Physics-Informed Neural Networks (PINNs) have proven effective in solving partial differential equations (PDEs), especially when some data are available by seamlessly blending data and physics. However, extending PINNs to high-dimensional and even high-order PDEs encounters significant challenges due to the computational cost associated with automatic differentiation in the residual loss. Herein, we address the limitations of PINNs in handling high-dimensional and high-order PDEs by introducing Hutchinson Trace Estimation (HTE). Starting with the second-order high-dimensional PDEs ubiquitous in scientific computing, HTE transforms the calculation of the entire Hessian matrix into a Hessian vector product (HVP). This approach alleviates the computational bottleneck via Taylor-mode automatic differentiation and significantly reduces memory consumption from the Hessian matrix to HVP. We further showcase HTE's convergence to the or
    
[^310]: 分层复杂度匹配学习产生了改进的大脑皮层V2区模型

    Layerwise complexity-matched learning yields an improved model of cortical area V2

    [https://arxiv.org/abs/2312.11436](https://arxiv.org/abs/2312.11436)

    通过分层复杂度匹配学习，我们开发了一种自下而上的自监督训练方法，最大化了特征相似性同时在不同位置的补丁上解除特征相关性。

    

    人类识别复杂视觉模式的能力是通过顺次区域在腹侧视觉皮层中执行的变换所形成的。最近的端到端训练的深度神经网络逼近了人类的能力，并且提供了迄今为止对层次结构的后期神经反应的最佳描述。然而，与传统的手工设计模型相比，或者与优化编码效率或预测的模型相比，这些网络对前期阶段提供了较差的描述。此外，用于端到端学习的梯度反向传播通常被认为在生物上是不切实际的。在这里，我们通过开发一种自下而上的自监督训练方法，独立地作用于连续层，从而克服了这两个限制。具体地，我们最大化了对局部变形自然图像补丁对之间的特征相似性，并在采样自其他位置的补丁时使特征去相关。

    arXiv:2312.11436v2 Announce Type: replace-cross  Abstract: Human ability to recognize complex visual patterns arises through transformations performed by successive areas in the ventral visual cortex. Deep neural networks trained end-to-end for object recognition approach human capabilities, and offer the best descriptions to date of neural responses in the late stages of the hierarchy. But these networks provide a poor account of the early stages, compared to traditional hand-engineered models, or models optimized for coding efficiency or prediction. Moreover, the gradient backpropagation used in end-to-end learning is generally considered to be biologically implausible. Here, we overcome both of these limitations by developing a bottom-up self-supervised training methodology that operates independently on successive layers. Specifically, we maximize feature similarity between pairs of locally-deformed natural image patches, while decorrelating features across patches sampled from oth
    
[^311]: 基于均值嵌入的分布式贝尔曼算子

    Distributional Bellman Operators over Mean Embeddings

    [https://arxiv.org/abs/2312.07358](https://arxiv.org/abs/2312.07358)

    提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架，推导出新算法并展示可与深度强化学习结合，提高表现。

    

    我们提出了一种基于学习回报分布的有限维均值嵌入的分布式强化学习算法框架。 我们基于这一框架推导出了几种新的动态规划和时序差分学习算法，提供了渐近收敛理论，并对这些算法在一系列表格任务上的实证表现进行了检验。此外，我们展示了这种方法可以与深度强化学习直接结合，得到一种新的深度强化学习代理，该代理在 Arcade Learning Environment 上优于基线分布式方法。

    arXiv:2312.07358v2 Announce Type: replace-cross  Abstract: We propose a novel algorithmic framework for distributional reinforcement learning, based on learning finite-dimensional mean embeddings of return distributions. We derive several new algorithms for dynamic programming and temporal-difference learning based on this framework, provide asymptotic convergence theory, and examine the empirical performance of the algorithms on a suite of tabular tasks. Further, we show that this approach can be straightforwardly combined with deep reinforcement learning, and obtain a new deep RL agent that improves over baseline distributional approaches on the Arcade Learning Environment.
    
[^312]: $t^3$-变分自动编码器：利用学生t分布和幂分歧学习重尾数据

    $t^3$-Variational Autoencoder: Learning Heavy-tailed Data with Student's t and Power Divergence

    [https://arxiv.org/abs/2312.01133](https://arxiv.org/abs/2312.01133)

    通过引入学生t分布和幂分歧，提出了$t^3$VAE变分自动编码器框架，以更好地处理重尾数据，并推导出新的优化目标。

    

    变分自动编码器（VAE）通常采用标准正态先验作为概率潜在编码器的正则化器。然而，高斯尾部往往衰减得太快，无法有效容纳编码点，无法保留数据中隐藏的关键结构。本文探讨了使用重尾模型来抵抗过度正则化的方法。借鉴信息几何的见解，我们提出了$t^3$VAE，一种修改后的VAE框架，它将学生t分布结合到先验、编码器和解码器中。这导致了一个幂形式的联合模型分布，我们认为这可以更好地拟合真实数据集。我们通过重新表达证据下界为两个统计流形之间KL散度的联合优化，将其替换为$\gamma$-幂分歧，这是幂族的一个自然替代方法。$t^3$VAE展现出卓越的低

    arXiv:2312.01133v2 Announce Type: replace-cross  Abstract: The variational autoencoder (VAE) typically employs a standard normal prior as a regularizer for the probabilistic latent encoder. However, the Gaussian tail often decays too quickly to effectively accommodate the encoded points, failing to preserve crucial structures hidden in the data. In this paper, we explore the use of heavy-tailed models to combat over-regularization. Drawing upon insights from information geometry, we propose $t^3$VAE, a modified VAE framework that incorporates Student's t-distributions for the prior, encoder, and decoder. This results in a joint model distribution of a power form which we argue can better fit real-world datasets. We derive a new objective by reformulating the evidence lower bound as joint optimization of KL divergence between two statistical manifolds and replacing with $\gamma$-power divergence, a natural alternative for power families. $t^3$VAE demonstrates superior generation of low-
    
[^313]: 一种基于物理约束神经普通微分方程方法与CFD求解器耦合的后验评估用于建模严谨的化学动力学

    A Posteriori Evaluation of a Physics-Constrained Neural Ordinary Differential Equations Approach Coupled with CFD Solver for Modeling Stiff Chemical Kinetics

    [https://arxiv.org/abs/2312.00038](https://arxiv.org/abs/2312.00038)

    扩展NeuralODE框架的研究通过在训练过程中直接将质量守恒约束纳入损失函数，有效地建模了严谨的化学动力学。

    

    解决详细化学计算的高计算成本对于预测计算流体动力学(CFD)模拟湍流反应流的挑战很大。这些模型通常需要解决一组耦合的严谨普通微分方程(ODE)。虽然深度学习技术已经被尝试用于开发更快的替代模型，但它们往往无法可靠地与CFD求解器集成。这种不稳定性是因为深度学习方法优化于训练错误，却未能确保与ODE求解器兼容，导致随时间累积错误。最近，基于神经ODE的技术提供了一种有效建模化学动力学的解决方案。在这项研究中，我们通过在训练过程中直接将质量守恒约束纳入损失函数，将NeuralODE框架扩展到严谨的化学动力学。这确保了总质量的守恒

    arXiv:2312.00038v3 Announce Type: replace-cross  Abstract: The high computational cost associated with solving for detailed chemistry poses a significant challenge for predictive computational fluid dynamics (CFD) simulations of turbulent reacting flows. These models often require solving a system of coupled stiff ordinary differential equations (ODEs). While deep learning techniques have been experimented with to develop faster surrogate models, they often fail to integrate reliably with CFD solvers. This instability arises because deep learning methods optimize for training error without ensuring compatibility with ODE solvers, leading to accumulation of errors over time. Recently, NeuralODE-based techniques have offered a promising solution by effectively modeling chemical kinetics. In this study, we extend the NeuralODE framework for stiff chemical kinetics by incorporating mass conservation constraints directly into the loss function during training. This ensures that the total ma
    
[^314]: 自适应实验中半参数高效推断

    Semiparametric Efficient Inference in Adaptive Experiments

    [https://arxiv.org/abs/2311.18274](https://arxiv.org/abs/2311.18274)

    自适应实验中提出了一种半参数高效推断方法，通过中心极限定理和置信序列实现了更紧凑的推断，具有更广泛的适用性。

    

    我们考虑了一个问题，即在一个顺序实验中，治疗效应的高效推断，其中主导将受试验对象分配给治疗或对照组的策略可以随时间变化。我们首先为自适应增强反向概率加权估计器提供了一个中心极限定理，该估计器在文献中的假设比以往更弱。该中心极限定理使得在固定样本量下进行高效推断成为可能。我们随后考虑了顺序推断设定，推导出既包含渐近性质又包含非渐近性质的置信序列，这些序列明显比以前的方法更紧凑。这些任意有效的方法使得能够在数据相关的停止时间（样本量）下进行推断。此外，我们使用了最近的离线策略估计文献中的倾向分数截断技术，以降低估计器的有限样本方差而不影响其效率。

    arXiv:2311.18274v3 Announce Type: replace-cross  Abstract: We consider the problem of efficient inference of the Average Treatment Effect in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting
    
[^315]: 可解释的图神经网络替代模型微调

    Interpretable Fine-Tuning for Graph Neural Network Surrogate Models

    [https://arxiv.org/abs/2311.07548](https://arxiv.org/abs/2311.07548)

    本论文引入了一种可解释的微调策略，通过应用于非结构网格化流体动力学建模的GNNs，增强了模型的预测能力，并通过识别可解释的物理空间区域及其对应的子图，帮助理解模型架构、优化目标和已知物理之间的关系。

    

    数据驱动的替代建模随着图神经网络（GNNs）的出现在最近几年内蓬勃发展，GNNs可以直接在基于网格的数据表示上运行。这项工作的目标是为GNN引入一种可解释的微调策略，应用于非结构网格化流体动力学建模。最终结果是一个增强的微调模型，它隔离了与预测任务密切相关的物理空间区域，相应于子图，同时保留了基线的预测能力。这些由微调的GNN识别出的结构在前向传递中是自适应生成的，并作为可解释的链接存在于基线模型架构、优化目标和已知问题特定物理之间。此外，通过正则化程序，微调的GNNs还可以在推断期间用于识别对应的图节点。

    arXiv:2311.07548v2 Announce Type: replace  Abstract: Data-driven surrogate modeling has surged in capability in recent years with the emergence of graph neural networks (GNNs), which can operate directly on mesh-based representations of data. The goal of this work is to introduce an interpretable fine-tuning strategy for GNNs, with application to unstructured mesh-based fluid dynamics modeling. The end result is an enhanced fine-tuned model that isolates regions in physical space, corresponding to sub-graphs, that are intrinsically linked to the forecasting task while retaining the predictive capability of the baseline. These structures, identified by the fine-tuned GNNs, are adaptively produced in the forward pass and serve as explainable links between the baseline model architecture, the optimization goal, and known problem-specific physics. Additionally, through a regularization procedure, the fine-tuned GNNs can also be used to identify, during inference, graph nodes that correspon
    
[^316]: Neuro-GPT: 面向EEG的基础模型

    Neuro-GPT: Towards A Foundation Model for EEG

    [https://arxiv.org/abs/2311.03764](https://arxiv.org/abs/2311.03764)

    Neuro-GPT是一个面向EEG数据的基础模型，通过自监督任务预训练和微调动作想象分类任务，显著改善分类性能，并展示了其泛化能力以解决EEG数据稀缺性和异质性挑战。

    

    为了处理脑机接口（BCI）任务中脑电图（EEG）数据的稀缺性和异质性，并利用大规模公开数据集的力量，我们提出了Neuro-GPT，这是一个由EEG编码器和GPT模型组成的基础模型。该基础模型在一个大规模数据集上进行自监督任务的预训练，学习如何重构被掩码的EEG片段。然后我们在动作想象分类任务上对模型进行微调，以验证其在低数据情境（9名受试者）中的性能。我们的实验证明，应用基础模型可以显著提高分类性能，相较于从头训练的模型，这为基础模型的泛化能力和它应对EEG数据稀缺性和异质性挑战的能力提供了证据。代码公开在github.com/wenhui0206/NeuroGPT。

    arXiv:2311.03764v4 Announce Type: replace  Abstract: To handle the scarcity and heterogeneity of electroencephalography (EEG) data for Brain-Computer Interface (BCI) tasks, and to harness the power of large publicly available data sets, we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model. The foundation model is pre-trained on a large-scale data set using a self-supervised task that learns how to reconstruct masked EEG segments. We then fine-tune the model on a Motor Imagery Classification task to validate its performance in a low-data regime (9 subjects). Our experiments demonstrate that applying a foundation model can significantly improve classification performance compared to a model trained from scratch, which provides evidence for the generalizability of the foundation model and its ability to address challenges of data scarcity and heterogeneity in EEG. The code is publicly available at github.com/wenhui0206/NeuroGPT.
    
[^317]: 关于风险感知代理理论：桥接演员-评论家和经济学

    On the Theory of Risk-Aware Agents: Bridging Actor-Critic and Economics

    [https://arxiv.org/abs/2310.19527](https://arxiv.org/abs/2310.19527)

    通过应用期望效用假设，本文揭示了风险中性和风险感知RL目标实际上可以通过使用指数效用函数的期望效用最大化来解释，提出了双演员-评论家（DAC）算法，为风险感知的RL算法贡献了框架。

    

    arXiv:2310.19527v2 公告类型：替换 摘要：风险感知强化学习（RL）算法如SAC和TD3在各种连续动作任务中的实证表现优于其风险中性对应物。然而，这些算法采用的悲观目标的理论基础尚未建立，这引发了关于它们实施的具体政策类别的问题。 在本研究中，我们应用了期望效用假设，这是经济学中的一个基本概念，以阐明风险中性和风险感知RL目标可以通过使用指数效用函数的期望效用最大化来解释。 这种方法揭示了风险感知政策有效地最大化了价值确定性等价物，使其与传统决策理论原则保持一致。此外，我们提出了双演员-评论家（Dual Actor-Critic，DAC）。 DAC是一种风险感知的无模型算法，具有两个不同的演员网络：一个用于时序差分的悲观演员。

    arXiv:2310.19527v2 Announce Type: replace  Abstract: Risk-aware Reinforcement Learning (RL) algorithms like SAC and TD3 were shown empirically to outperform their risk-neutral counterparts in a variety of continuous-action tasks. However, the theoretical basis for the pessimistic objectives these algorithms employ remains unestablished, raising questions about the specific class of policies they are implementing. In this work, we apply the expected utility hypothesis, a fundamental concept in economics, to illustrate that both risk-neutral and risk-aware RL goals can be interpreted through expected utility maximization using an exponential utility function. This approach reveals that risk-aware policies effectively maximize value certainty equivalent, aligning them with conventional decision theory principles. Furthermore, we propose Dual Actor-Critic (DAC). DAC is a risk-aware, model-free algorithm that features two distinct actor networks: a pessimistic actor for temporal-difference 
    
[^318]: DySurv：ICU中生存预测的动态深度学习模型

    DySurv: Dynamic Deep Learning Model for Survival Prediction in the ICU

    [https://arxiv.org/abs/2310.18681](https://arxiv.org/abs/2310.18681)

    DySurv是一种新型的动态深度学习模型，结合静态和时间序列测量，用于估计ICU患者的死亡风险，在多个基准测试中表现优异，并在实际世界的ICU数据上进行了评估。

    

    生存分析侧重于估计时间至事件分布，可帮助在医疗保健中进行动态风险预测。扩展经典的Cox模型，发展了深度学习技术，摆脱了比例风险的约束性假设。传统统计模型通常仅包含静态信息，在这项工作中，我们提出了一种名为DySurv的新型基于条件变分自动编码器的方法，它利用患者电子健康记录中的静态信息和时间序列测量的组合来动态估计死亡风险。DySurv在多个时间至事件基准测试中进行了测试，优于现有方法，包括深度学习方法，并且我们在来自MIMIC-IV和eICU的现实世界重症监护数据上进行了评估。 DySurv的预测能力持续稳定，生存估计在不同数据集上保持解耦。

    arXiv:2310.18681v2 Announce Type: replace  Abstract: Survival analysis focuses on estimating time-to-event distributions which can help in dynamic risk prediction in healthcare. Extending beyond the classical Cox model, deep learning techniques have been developed which moved away from the constraining assumptions of proportional hazards. Traditional statistical models often only include static information where, in this work, we propose a novel conditional variational autoencoder-based method called DySurv, which uses a combination of static and time-series measurements from patient electronic health records to estimate the risk of death dynamically. DySurv has been tested on several time-to-event benchmarks where it outperforms existing methods, including deep learning methods, and we evaluate it on real-world intensive care unit data from MIMIC-IV and eICU. The predictive capacity of DySurv is consistent and the survival estimates remain disentangled across different datasets suppor
    
[^319]: 对图神经网络公平性的对抗攻击

    Adversarial Attacks on Fairness of Graph Neural Networks

    [https://arxiv.org/abs/2310.13822](https://arxiv.org/abs/2310.13822)

    本文研究了对图神经网络公平性的对抗攻击问题，提出了G-FairAttack框架可以成功地攻击各种类型的公平性感知GNNs，并保持攻击的不可察觉性。

    

    具有公平性意识的图神经网络（GNNs）因能够减少在基于图的应用中对任何人口统计群体（例如女性）的预测偏见而引起了人们的关注。尽管这些方法极大地改善了GNNs的算法公平性，但公平性容易受到精心设计的对抗攻击的破坏。本文研究了对GNNs公平性的对抗攻击问题，并提出了G-FairAttack，这是一个攻击各种类型公平性感知GNNs的通用框架，对预测效用几乎没有察觉的影响。此外，我们提出了一种快速的计算技术来降低G-FairAttack的时间复杂度。实验研究表明，G-FairAttack成功地破坏了不同类型GNNs的公平性，同时保持了攻击的不可察觉性。我们在公平性攻击方面的研究揭示了公平性感知GNNs的潜在漏洞。

    arXiv:2310.13822v2 Announce Type: replace  Abstract: Fairness-aware graph neural networks (GNNs) have gained a surge of attention as they can reduce the bias of predictions on any demographic group (e.g., female) in graph-based applications. Although these methods greatly improve the algorithmic fairness of GNNs, the fairness can be easily corrupted by carefully designed adversarial attacks. In this paper, we investigate the problem of adversarial attacks on fairness of GNNs and propose G-FairAttack, a general framework for attacking various types of fairness-aware GNNs in terms of fairness with an unnoticeable effect on prediction utility. In addition, we propose a fast computation technique to reduce the time complexity of G-FairAttack. The experimental study demonstrates that G-FairAttack successfully corrupts the fairness of different types of GNNs while keeping the attack unnoticeable. Our study on fairness attacks sheds light on potential vulnerabilities in fairness-aware GNNs an
    
[^320]: Bongard-OpenWorld: 在真实世界中进行自由形式视觉概念的少样本推理

    Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World

    [https://arxiv.org/abs/2310.10207](https://arxiv.org/abs/2310.10207)

    Bongard-OpenWorld基准旨在评估机器视觉中对真实世界中的自由形式视觉概念进行少样本推理，并且提出了开放世界自由形式概念和真实世界图像两项新挑战。

    

    我们介绍了Bongard-OpenWorld，这是一个用于评估机器视觉中真实世界少样本推理的新基准。 它源自经典的Bongard问题（BPs）：给定两组图像（正和负），模型需要通过诱导视觉概念来确定查询图像所属的图像集，这些概念仅由正集中的图像所描述。 我们的基准继承了原始BPs的少样本概念归纳，同时增加了两层新挑战：1）开放世界的自由形式概念，因为Bongard-OpenWorld中的视觉概念是从开放词汇表中独特组合的术语，范围从对象类别到抽象视觉属性和常识事实知识； 2）真实世界的图像，而不是许多类似物使用的合成图表。在我们的探索中，Bongard-OpenWorld已经对当前的少样本推理算法提出了重大挑战。我们还远

    arXiv:2310.10207v2 Announce Type: replace  Abstract: We introduce Bongard-OpenWorld, a new benchmark for evaluating real-world few-shot reasoning for machine vision. It originates from the classical Bongard Problems (BPs): Given two sets of images (positive and negative), the model needs to identify the set that query images belong to by inducing the visual concepts, which is exclusively depicted by images from the positive set. Our benchmark inherits the few-shot concept induction of the original BPs while adding the two novel layers of challenge: 1) open-world free-form concepts, as the visual concepts in Bongard-OpenWorld are unique compositions of terms from an open vocabulary, ranging from object categories to abstract visual attributes and commonsense factual knowledge; 2) real-world images, as opposed to the synthetic diagrams used by many counterparts. In our exploration, Bongard-OpenWorld already imposes a significant challenge to current few-shot reasoning algorithms. We furt
    
[^321]: 基于边缘感知的在规模不平衡数据上训练的图自编码器用于解决旅行商问题

    An Edge-Aware Graph Autoencoder Trained on Scale-Imbalanced Data for Traveling Salesman Problems

    [https://arxiv.org/abs/2310.06543](https://arxiv.org/abs/2310.06543)

    提出了一种基于边缘感知的图自编码器模型，用于解决具有不同城市数量的旅行商问题，并通过学习不同规模样本来训练模型。

    

    近年来，对于组合优化的机器学习技术的研究急剧增加。已经证明，与传统的启发式方法和数学求解器相比，基于学习的方法在旅行商问题（TSP）方面在性能和计算效率方面表现更好。然而，大多数基于学习的TSP求解器主要设计用于固定规模的TSP实例，并且需要大量的训练样本才能达到最佳性能。为了填补这一空白，本作品提出了一种用于解决具有不同城市数量的TSP的数据驱动图表示学习方法。具体来说，我们将TSP形式化为一个链路预测任务，并提出了一种可以通过学习来自不同规模样本的TSP的边缘感知图自编码器（EdgeGAE）模型，该模型具有不平衡的分布。

    arXiv:2310.06543v2 Announce Type: replace  Abstract: In recent years, there has been a notable surge in research on machine learning techniques for combinatorial optimization. It has been shown that learning-based methods outperform traditional heuristics and mathematical solvers on the Traveling Salesman Problem (TSP) in terms of both performance and computational efficiency. However, most learning-based TSP solvers are primarily designed for fixed-scale TSP instances, and also require a large number of training samples to achieve optimal performance. To fill this gap, this work proposes a data-driven graph representation learning method for solving TSPs with various numbers of cities. Specifically, we formulate the TSP as a link prediction task and propose an edge-aware graph autoencoder (EdgeGAE) model that can solve TSPs by learning from various-scale samples with an imbalanced distribution. A residual gated encoder is trained to learn latent edge embeddings, followed by an edge-ce
    
[^322]: 通过联合协同训练保护敏感数据

    Protecting Sensitive Data through Federated Co-Training

    [https://arxiv.org/abs/2310.05696](https://arxiv.org/abs/2310.05696)

    提出了使用联合协同训练方法来保护敏感数据，通过在公共未标记数据集上共享硬标签代替模型参数，形成伪标签以结合私有数据训练本地模型，提高隐私保护效果并获得与联邦学习相媲美的模型质量。

    

    在许多应用中，敏感数据本质上是分布的，由于隐私问题可能无法汇总。联邦学习允许我们通过迭代地聚合本地模型的参数来协作训练模型，而无需合并数据。然而，可以通过共享模型参数推断出敏感数据。我们提出使用联合协同训练方法，在其中客户端分享公共未标记数据集上的硬标签，而不是模型参数。对共享标签的一致性形成了未标记数据集的伪标签，客户端将其与私有数据结合使用来训练本地模型。我们表明，共享硬标签大大提高了与共享模型参数相比的隐私保护。同时，联合协同训练实现了与联邦学习相媲美的模型质量。此外，它使我们能够使用像(梯度提升)决策树、规则集合等本地模型

    arXiv:2310.05696v2 Announce Type: replace  Abstract: In many applications, sensitive data is inherently distributed and may not be pooled due to privacy concerns. Federated learning allows us to collaboratively train a model without pooling the data by iteratively aggregating the parameters of local models. It is possible, though, to infer upon the sensitive data from the shared model parameters. We propose to use a federated co-training approach where clients share hard labels on a public unlabeled dataset instead of model parameters. A consensus on the shared labels forms a pseudo labeling for the unlabeled dataset that clients use in combination with their private data to train local models. We show that sharing hard labels substantially improves privacy over sharing model parameters. At the same time, federated co-training achieves a model quality comparable to federated learning. Moreover, it allows us to use local models such as (gradient boosted) decision trees, rule ensembles, 
    
[^323]: 语言模型代表空间和时间

    Language Models Represent Space and Time

    [https://arxiv.org/abs/2310.02207](https://arxiv.org/abs/2310.02207)

    现代大型语言模型学习到了丰富的时空表征，包括学习到了空间和时间的线性表征以及个体的“空间神经元”和“时间神经元”。

    

    大型语言模型（LLM）的能力引发了关于这些系统到底是仅仅学习了庞大的表面统计信息还是学到了更连贯、基于真实世界的表征的争论。我们通过分析Llama-2系列模型中学到的三个空间数据集（世界、美国、纽约的地点）和三个时间数据集（历史人物、艺术品、新闻头条）的学习表征找到了支持后者的证据。我们发现LLM在多个尺度上学习到了空间和时间的线性表征。这些表征对提示变化具有稳健性，并且在不同实体类型（例如城市和地标）之间是统一的。此外，我们还发现了可靠地编码空间和时间坐标的个体“空间神经元”和“时间神经元”。虽然还需要进一步的研究，但我们的结果表明现代LLM学习到了对真实世界的丰富时空表征。

    arXiv:2310.02207v3 Announce Type: replace-cross  Abstract: The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual "space neurons" and "time neurons" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real
    
[^324]: 光束枚举：自我条件化分子设计的概率可解释性

    Beam Enumeration: Probabilistic Explainability For Sample Efficient Self-conditioned Molecular Design

    [https://arxiv.org/abs/2309.13957](https://arxiv.org/abs/2309.13957)

    提出了光束枚举方法，从语言为基础的分子生成模型中详尽枚举最可能的子序列，提取分子亚结构，并通过强化学习实现自我条件生成，从而提升生成式设计的可解释性和样本效率。

    

    生成式分子设计已经从概念验证迈向真实世界的适用性，这一点可以从最近涌现的大量研究报告实验验证的论文中看出。解释性和样本效率方面的关键挑战为增强生成式设计提供了机会，可以直接优化昂贵高保真度预言并为领域专家提供可操作见解。在这里，我们提出了光束枚举，以详尽枚举基于语言的分子生成模型中最可能的子序列，并展示可以提取分子亚结构。当与强化学习相结合时，提取的亚结构变得有意义，提供了可解释性的来源，并通过自我条件生成提高了样本效率。光束枚举通常适用于任何基于语言的分子生成模型，显着改进了最近报告的增强记忆性能。

    arXiv:2309.13957v2 Announce Type: replace-cross  Abstract: Generative molecular design has moved from proof-of-concept to real-world applicability, as marked by the surge in very recent papers reporting experimental validation. Key challenges in explainability and sample efficiency present opportunities to enhance generative design to directly optimize expensive high-fidelity oracles and provide actionable insights to domain experts. Here, we propose Beam Enumeration to exhaustively enumerate the most probable sub-sequences from language-based molecular generative models and show that molecular substructures can be extracted. When coupled with reinforcement learning, extracted substructures become meaningful, providing a source of explainability and improving sample efficiency through self-conditioned generation. Beam Enumeration is generally applicable to any language-based molecular generative model and notably further improves the performance of the recently reported Augmented Memor
    
[^325]: 量子卷积神经网络中缓解过拟合的后训练方法

    A Post-Training Approach for Mitigating Overfitting in Quantum Convolutional Neural Networks

    [https://arxiv.org/abs/2309.01829](https://arxiv.org/abs/2309.01829)

    研究了用于减轻量子卷积神经网络过拟合的后训练方法，并发现将经典后训练方法神经元丢弃直接应用到量子设置中会导致成功概率显著下降，揭示了纠缠在QCNN中的关键作用和其对纠缠丢失的脆弱性。

    

    量子卷积神经网络（QCNN）作为量子计算机在NISQ时代的早期应用，一直被证明在多个任务中取得显著准确度，作为机器学习（ML）算法一直表现成功。然而，受其经典对应物的影响，QCNN容易出现过拟合问题。本文研究了用于减轻QCNN过拟合的后训练方法。我们发现，简单地将一种称为神经元丢弃的经典后训练方法适应到量子设置中，会导致一个显著且不希望的结果：QCNN的成功概率显著下降。我们认为，这一效果暴露出了纠缠在QCNN中的关键作用，以及QCNN对纠缠丢失的脆弱性。

    arXiv:2309.01829v2 Announce Type: replace-cross  Abstract: Quantum convolutional neural network (QCNN), an early application for quantum computers in the NISQ era, has been consistently proven successful as a machine learning (ML) algorithm for several tasks with significant accuracy. Derived from its classical counterpart, QCNN is prone to overfitting. Overfitting is a typical shortcoming of ML models that are trained too closely to the availed training dataset and perform relatively poorly on unseen datasets for a similar problem. In this work we study post-training approaches for mitigating overfitting in QCNNs. We find that a straightforward adaptation of a classical post-training method, known as neuron dropout, to the quantum setting leads to a significant and undesirable consequence: a substantial decrease in success probability of the QCNN. We argue that this effect exposes the crucial role of entanglement in QCNNs and the vulnerability of QCNNs to entanglement loss. Hence, we 
    
[^326]: 生成对抗网络的统一生成器损失函数

    A Unifying Generator Loss Function for Generative Adversarial Networks

    [https://arxiv.org/abs/2308.07233](https://arxiv.org/abs/2308.07233)

    引入了一个统一的生成器损失函数，称为$\mathcal{L}_\alpha$-GAN，通过最小化Jensen-$f_\alpha$-散度来优化生成器，可以恢复出文献中的多个GAN问题。

    

    引入了一个统一的$\alpha$参数化生成器损失函数，用于双目标生成对抗网络（GAN），该网络使用经典鉴别器损失函数，如原始GAN（VanillaGAN）系统中的损失函数。生成器损失函数基于对称类概率估计类型函数$\mathcal{L}_\alpha$，得到的GAN系统被称为$\mathcal{L}_\alpha$-GAN。在最佳鉴别器的情况下，表明生成器的优化问题包括最小化Jensen-$f_\alpha$-散度，这是Jensen-Shannon散度的自然泛化，其中$f_\alpha$是关于损失函数$\mathcal{L}_\alpha$的凸函数。还证明了这个$\mathcal{L}_\alpha$-GAN问题恢复了文献中一些GAN问题作为特殊情况，包括VanillaGAN、最小二乘GAN（LSGAN）、最小$k$阶GAN（L

    arXiv:2308.07233v2 Announce Type: replace  Abstract: A unifying $\alpha$-parametrized generator loss function is introduced for a dual-objective generative adversarial network (GAN), which uses a canonical (or classical) discriminator loss function such as the one in the original GAN (VanillaGAN) system. The generator loss function is based on a symmetric class probability estimation type function, $\mathcal{L}_\alpha$, and the resulting GAN system is termed $\mathcal{L}_\alpha$-GAN. Under an optimal discriminator, it is shown that the generator's optimization problem consists of minimizing a Jensen-$f_\alpha$-divergence, a natural generalization of the Jensen-Shannon divergence, where $f_\alpha$ is a convex function expressed in terms of the loss function $\mathcal{L}_\alpha$. It is also demonstrated that this $\mathcal{L}_\alpha$-GAN problem recovers as special cases a number of GAN problems in the literature, including VanillaGAN, Least Squares GAN (LSGAN), Least $k$th order GAN (L$
    
[^327]: MATNet: 多级融合变压器模型用于日前光伏发电预测

    MATNet: Multi-Level Fusion Transformer-Based Model for Day-Ahead PV Generation Forecasting

    [https://arxiv.org/abs/2306.10356](https://arxiv.org/abs/2306.10356)

    提出了MATNet，结合了人工智能范式与光伏发电的物理先验知识，通过多级联合融合方法进行日前光伏发电预测

    

    准确预测可再生能源发电对促进可再生能源整合到电力系统中至关重要。针对光伏单元，预测方法主要可分为基于物理和基于数据的策略两类，基于人工智能的模型提供了最先进的性能。然而，虽然这些基于人工智能的模型可以捕捉数据中的复杂模式和关系，但它们忽略了现象的潜在物理先验知识。因此，在本文中，我们提出了MATNet，一种新颖的基于自注意力变压器架构，用于多元多步日前光伏发电预测。它采用一种混合方法，将人工智能范式与基于物理的光伏发电的先验知识相结合。该模型通过多级联合融合方法输入历史光伏数据以及历史和预测天气数据。

    arXiv:2306.10356v2 Announce Type: replace-cross  Abstract: Accurate forecasting of renewable generation is crucial to facilitate the integration of RES into the power system. Focusing on PV units, forecasting methods can be divided into two main categories: physics-based and data-based strategies, with AI-based models providing state-of-the-art performance. However, while these AI-based models can capture complex patterns and relationships in the data, they ignore the underlying physical prior knowledge of the phenomenon. Therefore, in this paper we propose MATNet, a novel self-attention transformer-based architecture for multivariate multi-step day-ahead PV power generation forecasting. It consists of a hybrid approach that combines the AI paradigm with the prior physical knowledge of PV power generation of physics-based methods. The model is fed with historical PV data and historical and forecast weather data through a multi-level joint fusion approach. The effectiveness of the propo
    
[^328]: 具有实时预算约束的安全离线强化学习

    Safe Offline Reinforcement Learning with Real-Time Budget Constraints

    [https://arxiv.org/abs/2306.00603](https://arxiv.org/abs/2306.00603)

    提出了一种名为TREBI的新方法，在离线设置下解决强化学习中实时预算约束的问题，通过轨迹分布建模和扩散模型规划来提供性能保证。

    

    为促进强化学习（RL）在现实世界中的安全部署，近年来对安全RL的研究取得了显著进展。然而，文献中大多数现有工作仍专注于在线设置，训练过程中可能会发生对安全预算的风险违规。此外，在许多实际应用中，学得策略需要实时响应动态确定的安全预算（即约束阈值）。本文针对离线设置下的实时预算约束问题，并提出了基于轨迹的实时预算推断（TREBI）作为一个新颖解决方案，该方法从轨迹分布的角度对问题进行建模，并通过扩散模型规划来解决。从理论上讲，我们证明在离线设置下对情节奖励和成本的估计存在误差界限，从而提供了性能保证。

    arXiv:2306.00603v2 Announce Type: replace-cross  Abstract: Aiming at promoting the safe real-world deployment of Reinforcement Learning (RL), research on safe RL has made significant progress in recent years. However, most existing works in the literature still focus on the online setting where risky violations of the safety budget are likely to be incurred during training. Besides, in many real-world applications, the learned policy is required to respond to dynamically determined safety budgets (i.e., constraint threshold) in real time. In this paper, we target at the above real-time budget constraint problem under the offline setting, and propose Trajectory-based REal-time Budget Inference (TREBI) as a novel solution that models this problem from the perspective of trajectory distribution and solves it through diffusion model planning. Theoretically, we prove an error bound of the estimation on the episodic reward and cost under the offline setting and thus provide a performance gua
    
[^329]: 对异质性条件下GNN评估的批判性研究：我们真的在取得进展吗？

    A critical look at the evaluation of GNNs under heterophily: Are we really making progress?

    [https://arxiv.org/abs/2302.11640](https://arxiv.org/abs/2302.11640)

    挑战了异质图上特定模型评估的假设，揭示了标准数据集存在严重缺陷，移除重复节点对GNN性能有重大影响

    

    节点分类是一种经典的图机器学习任务，最近图神经网络（GNNs）在这方面取得了很好的成绩。然而，人们通常认为标准GNNs只对同质图表现良好，即边倾向于连接相同类别的节点的图。没有这种属性的图称为异质图，通常认为需要专门的方法才能在这种图上获得很强的性能。在这项工作中，我们挑战了这一假设。首先，我们展示了用于评估异质性特定模型的标准数据集具有严重缺陷，使得使用它们获得的结果不可靠。这些缺陷中最重要的是数据集Squirrel和Chameleon中存在大量重复节点，导致训练-测试数据泄漏。我们表明，在这些数据集上移除重复节点会严重影响GNN的性能。

    arXiv:2302.11640v2 Announce Type: replace  Abstract: Node classification is a classical graph machine learning task on which Graph Neural Networks (GNNs) have recently achieved strong results. However, it is often believed that standard GNNs only work well for homophilous graphs, i.e., graphs where edges tend to connect nodes of the same class. Graphs without this property are called heterophilous, and it is typically assumed that specialized methods are required to achieve strong performance on such graphs. In this work, we challenge this assumption. First, we show that the standard datasets used for evaluating heterophily-specific models have serious drawbacks, making results obtained by using them unreliable. The most significant of these drawbacks is the presence of a large number of duplicate nodes in the datasets Squirrel and Chameleon, which leads to train-test data leakage. We show that removing duplicate nodes strongly affects GNN performance on these datasets. Then, we propos
    
[^330]: 一种用于准确自动视网膜渗出物检测的全局和区域对比损失

    A Global and Patch-wise Contrastive Loss for Accurate Automated Exudate Detection

    [https://arxiv.org/abs/2302.11517](https://arxiv.org/abs/2302.11517)

    提出了一种全新的监督对比学习框架来优化硬渗出物分割，通过引入基于区域的密度对比方案和判别性边缘检查模块来处理硬渗出物的独特特征。

    

    糖尿病视网膜病变（DR）是导致全球盲目的主要原因。早期检测硬渗出物对于识别DR至关重要，有助于治疗糖尿病和预防视力损失。针对现有分割技术面临的硬渗出物独特特征，从不一致的形状到模糊的边界，我们提出了一个新颖的监督对比学习框架来优化硬渗出物分割。具体地，我们引入了一个基于区域的密度对比方案来区分不同病变浓度的区域，从而提高模型在分割小病变方面的能力。为了处理模糊的边界，我们开发了一个判别性边缘检查模块来动态分析边界周围的像素，并准确勾勒出渗出物。

    arXiv:2302.11517v2 Announce Type: replace-cross  Abstract: Diabetic retinopathy (DR) is a leading global cause of blindness. Early detection of hard exudates plays a crucial role in identifying DR, which aids in treating diabetes and preventing vision loss. However, the unique characteristics of hard exudates, ranging from their inconsistent shapes to indistinct boundaries, pose significant challenges to existing segmentation techniques. To address these issues, we present a novel supervised contrastive learning framework to optimize hard exudate segmentation. Specifically, we introduce a patch-wise density contrasting scheme to distinguish between areas with varying lesion concentrations, and therefore improve the model's proficiency in segmenting small lesions. To handle the ambiguous boundaries, we develop a discriminative edge inspection module to dynamically analyze the pixels that lie around the boundaries and accurately delineate the exudates. Upon evaluation using the IDRiD dat
    
[^331]: 高斯过程门控的分层专家混合模型

    Gaussian Process-Gated Hierarchical Mixtures of Experts

    [https://arxiv.org/abs/2302.04947](https://arxiv.org/abs/2302.04947)

    该论文提出了一种新颖的高斯过程门控的分层专家混合模型，通过使用GPs构建门控函数和专家，优于传统基于树的模型，同时在复杂性较低的情况下表现出良好性能，还提供了深层GPs和深度贝叶斯神经网络的可解释性。

    

    在这篇论文中，我们提出了一种新颖的高斯过程门控的分层专家混合模型（Gaussian Process-Gated Hierarchical Mixtures of Experts，GPHMEs）。与其他采用输入线性门控模型的专家混合模型不同，我们的模型采用了基于高斯过程（GPs）构建的门控函数。这些过程基于输入的非线性函数的随机特征。此外，我们模型中的专家也是用GPs构建的。GPHMEs的优化通过变分推断来实现。所提出的GPHMEs具有几个优点。它们优于在输入空间中对数据进行分区的基于树的HME基准，并且能够在减少复杂性的同时实现良好的性能。另一个优点是它们为深层GPs以及更一般的深度贝叶斯神经网络提供的可解释性。我们的GPHMEs在大规模数据集上展现了出色的性能，即使数据规模相当适中也是如此。

    arXiv:2302.04947v2 Announce Type: replace  Abstract: In this paper, we propose novel Gaussian process-gated hierarchical mixtures of experts (GPHMEs). Unlike other mixtures of experts with gating models linear in the input, our model employs gating functions built with Gaussian processes (GPs). These processes are based on random features that are non-linear functions of the inputs. Furthermore, the experts in our model are also constructed with GPs. The optimization of the GPHMEs is performed by variational inference. The proposed GPHMEs have several advantages. They outperform tree-based HME benchmarks that partition the data in the input space, and they achieve good performance with reduced complexity. Another advantage is the interpretability they provide for deep GPs, and more generally, for deep Bayesian neural networks. Our GPHMEs demonstrate excellent performance for large-scale data sets, even with quite modest sizes.
    
[^332]: 梯度塑造：增强反向工程中的后门攻击

    Gradient Shaping: Enhancing Backdoor Attack Against Reverse Engineering

    [https://arxiv.org/abs/2301.12318](https://arxiv.org/abs/2301.12318)

    本文分析了植入后门模型在触发输入周围的变化率，揭示了现有攻击倾向于在触发输入周围注入具有低变化率特征的后门，易被梯度触发器反转捕获。

    

    大多数现有方法检测植入后门机器学习（ML）模型采用触发器反转（也称为反向工程）和权重分析（也称为模型诊断）两种方法之一。本文首次尝试通过分析植入后门模型在触发输入周围的变化率来回答这个问题。研究表明，现有攻击往往在触发输入周围注入具有较低变化率特征的后门，这些后门易于被基于梯度的触发器反转所捕获。

    arXiv:2301.12318v2 Announce Type: replace-cross  Abstract: Most existing methods to detect backdoored machine learning (ML) models take one of the two approaches: trigger inversion (aka. reverse engineer) and weight analysis (aka. model diagnosis). In particular, the gradient-based trigger inversion is considered to be among the most effective backdoor detection techniques, as evidenced by the TrojAI competition, Trojan Detection Challenge and backdoorBench. However, little has been done to understand why this technique works so well and, more importantly, whether it raises the bar to the backdoor attack. In this paper, we report the first attempt to answer this question by analyzing the change rate of the backdoored model around its trigger-carrying inputs. Our study shows that existing attacks tend to inject the backdoor characterized by a low change rate around trigger-carrying inputs, which are easy to capture by gradient-based trigger inversion. In the meantime, we found that the 
    
[^333]: 具有定制隐私保护的社交感知聚类联邦学习

    Social-Aware Clustered Federated Learning with Customized Privacy Preservation

    [https://arxiv.org/abs/2212.13992](https://arxiv.org/abs/2212.13992)

    通过利用用户之间的社交关系，提出了SCFL，一种具有定制隐私保护的社交感知聚类联邦学习方案，实现了数据隐私和效率之间的平衡。

    

    联邦学习（FL）的一个关键特性是保护端用户的数据隐私。然而，在FL中仍然存在通过交换梯度可能导致的潜在隐私泄漏。因此，最近的研究通常探讨微分隐私（DP）方法，通过向计算结果添加噪声来解决隐私问题，并具有较低的开销，但这些方法会降低模型性能。本文通过利用用户之间的普遍社交连接，平衡了数据隐私和效率。具体来说，我们提出了一种新颖的社交感知聚类联邦学习方案SCFL，其中相互信任的个体可以自由组成一个社交集群，并在每个集群内聚合他们的原始模型更新（例如梯度），然后上传到云端进行全局聚合。通过在社交群体中混合模型更新，对手只能窃听社交层组合的结果，而无法窃听到个体隐私。

    arXiv:2212.13992v2 Announce Type: replace-cross  Abstract: A key feature of federated learning (FL) is to preserve the data privacy of end users. However, there still exist potential privacy leakage in exchanging gradients under FL. As a result, recent research often explores the differential privacy (DP) approaches to add noises to the computing results to address privacy concerns with low overheads, which however degrade the model performance. In this paper, we strike the balance of data privacy and efficiency by utilizing the pervasive social connections between users. Specifically, we propose SCFL, a novel Social-aware Clustered Federated Learning scheme, where mutually trusted individuals can freely form a social cluster and aggregate their raw model updates (e.g., gradients) inside each cluster before uploading to the cloud for global aggregation. By mixing model updates in a social group, adversaries can only eavesdrop the social-layer combined results, but not the privacy of in
    
[^334]: Primal Dual Alternating Proximal Gradient算法用于具有耦合线性约束的非光滑非凸极小极大问题

    Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints

    [https://arxiv.org/abs/2212.04672](https://arxiv.org/abs/2212.04672)

    提出了用于具有耦合线性约束的非光滑非凸极小极大问题的两种算法，分别具有迭代复杂度保证。

    

    非凸极小极大问题近年来在机器学习、信号处理和许多其他领域引起了广泛关注。本文提出了一种用于解决非光滑非凸（强）凹和非凸线性极小极大问题的原始对偶交替近端梯度（PDAPG）算法和原始对偶近端梯度（PDPG-L）算法，分别用于具有耦合线性约束的情况。这两种算法的迭代复杂度证明为 $\mathcal{O}\left( \varepsilon ^{-2} \right)$ （对应 $\mathcal{O}\left( \varepsilon ^{-4} \right)$）在非凸强凹 （对应非凸凹）情况下，以及 $\mathcal{O}\left( \varepsilon ^{-3} \right)$ 在非凸线性情况下，分别达到 $\varepsilon$-稳态点。据我们所知，它们是用于解决具有耦合线性约束的非凸极小极大问题的第一批具有迭代复杂度保证的算法。

    arXiv:2212.04672v3 Announce Type: replace-cross  Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm and a primal-dual proximal gradient (PDPG-L) algorithm for solving nonsmooth nonconvex-(strongly) concave and nonconvex-linear minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting and $\mathcal{O}\left( \varepsilon ^{-3} \right)$ under nonconvex-linear setting to reach an $\varepsilon$-stationary point, respectively. To our knowledge, they are the first two algorithms with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear const
    
[^335]: 在不需要监督的情况下发现语言模型中的潜在知识

    Discovering Latent Knowledge in Language Models Without Supervision

    [https://arxiv.org/abs/2212.03827](https://arxiv.org/abs/2212.03827)

    通过在语言模型的内部激活中直接发现潜在知识的方式，我们提出了一种纯粹无监督的方法，可以准确回答未标记模型激活的是非问题，并且在大型语言模型中恢复多样知识。

    

    训练语言模型的现有技术可能与真相不一致：如果我们用模仿学习训练模型，它们可能会重现人类的错误；如果我们训练它们生成人类评价高的文本，它们可能会输出人类评估者无法检测到的错误。我们提出通过在语言模型的内部激活中直接发现潜在知识的方式来规避这个问题，而且是纯粹无监督的方式。具体来说，我们引入了一种方法，能够准确回答只给定未标记模型激活的是非问题。该方法通过在激活空间中找到满足逻辑一致性属性的方向来工作，例如一个陈述及其否定具有相反的真值。我们展示，尽管没有使用监督和模型输出，我们的方法可以恢复大型语言模型中代表多样知识：在6个模型和10个问答数据集上，它表现优异。

    arXiv:2212.03827v2 Announce Type: replace-cross  Abstract: Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms 
    
[^336]: 使用条件生成模型预测量子系统的性质

    Predicting Properties of Quantum Systems with Conditional Generative Models

    [https://arxiv.org/abs/2211.16943](https://arxiv.org/abs/2211.16943)

    使用条件生成模型同时表示一系列状态，从测量中学习不同量子态的共享结构，可以预测基态的任意局部性质，无需为新的可观测量进行进一步训练

    

    机器学习最近作为一个强大的工具出现，用于预测量子多体系统的性质。对于许多间隙哈密顿量的基态，生成模型可以从单个量子态的测量中学习，精确重构出状态，从而预测局部可观测量。另外，分类和回归模型可以通过学习不同但相关状态的测量来预测局部可观测量。在这项工作中，我们结合了两种方法的优点，提出了使用条件生成模型来同时表示一系列状态，从测量中学习不同量子态的共享结构。训练好的模型使我们能够预测基态的任意局部性质，甚至对于未包含在训练数据中的状态，也无需为新的可观测量进行进一步训练。我们首先在二维随机海森堡模型上对我们的方法进行了数值验证。

    arXiv:2211.16943v2 Announce Type: replace-cross  Abstract: Machine learning has emerged recently as a powerful tool for predicting properties of quantum many-body systems. For many ground states of gapped Hamiltonians, generative models can learn from measurements of a single quantum state to reconstruct the state accurately enough to predict local observables. Alternatively, classification and regression models can predict local observables by learning from measurements on different but related states. In this work, we combine the benefits of both approaches and propose the use of conditional generative models to simultaneously represent a family of states, learning shared structures of different quantum states from measurements. The trained model enables us to predict arbitrary local properties of ground states, even for states not included in the training data, without necessitating further training for new observables. We first numerically validate our approach on 2D random Heisenb
    
[^337]: 利用算法公平性减轻黑盒属性推断攻击

    Leveraging Algorithmic Fairness to Mitigate Blackbox Attribute Inference Attacks

    [https://arxiv.org/abs/2211.10209](https://arxiv.org/abs/2211.10209)

    通过使用自适应阈值来考虑数据集中敏感属性类别不平衡，我们提出了一种实用且有效的属性推断攻击方法。

    

    机器学习（ML）模型已经被部署在高风险应用中，例如医疗保健和刑事司法。先前的研究表明，ML模型容易受到属性推断攻击的影响，攻击者利用一些背景知识训练ML攻击模型，通过利用可区分的模型预测来推断敏感属性。然而，一些先前的属性推断攻击对攻击者的背景知识（例如，敏感属性的边际分布）有很强的假设，并且不会带来比统计推断更多的隐私风险。此外，先前的攻击并未考虑来自真实应用程序的数据集中敏感属性的类别不平衡（例如，种族和性别）。在本文中，我们提出了一个考虑此不平衡的实用且有效的属性推断攻击，使用攻击模型预测的自适应阈值。我们对我们的方法进行了全面评估。

    arXiv:2211.10209v2 Announce Type: replace  Abstract: Machine learning (ML) models have been deployed for high-stakes applications, e.g., healthcare and criminal justice. Prior work has shown that ML models are vulnerable to attribute inference attacks where an adversary, with some background knowledge, trains an ML attack model to infer sensitive attributes by exploiting distinguishable model predictions. However, some prior attribute inference attacks have strong assumptions about adversary's background knowledge (e.g., marginal distribution of sensitive attribute) and pose no more privacy risk than statistical inference. Moreover, none of the prior attacks account for class imbalance of sensitive attribute in datasets coming from real-world applications (e.g., Race and Sex). In this paper, we propose an practical and effective attribute inference attack that accounts for this imbalance using an adaptive threshold over the attack model's predictions. We exhaustively evaluate our propo
    
[^338]: 一个生成形状组合框架，用于合成虚拟奇美拉人群

    A Generative Shape Compositional Framework to Synthesise Populations of Virtual Chimaeras

    [https://arxiv.org/abs/2210.01607](https://arxiv.org/abs/2210.01607)

    引入了一种能够合成完整复杂形状集合的生成模型，可用于构建虚拟奇美拉人群，有助于进行医疗器械的硅中试验证

    

    生成捕获足够变异性且保持合理的虚拟解剖人群对于进行医疗器械的硅中试验证是至关重要的。然而，并非每个个体都拥有所需的解剖形状。因此，在人群中常常存在缺失/部分重叠的解剖信息。我们引入了一个针对复杂解剖结构的生成形状模型，可以从未配对数据集中学习。所提出的生成模型能够合成完整的复杂形状集合，称为虚拟奇美拉，与自然人类奇美拉相对。我们将该框架应用于从整个心形组件数据库中构建虚拟奇美拉，其中每个奇美拉贡献心脏次级结构的样本。具体而言，我们提出了一个生成形状组合框架，包括两个组件 - 一个部分感知的生成形状

    arXiv:2210.01607v2 Announce Type: replace-cross  Abstract: Generating virtual populations of anatomy that capture sufficient variability while remaining plausible is essential for conducting in-silico trials of medical devices. However, not all anatomical shapes of interest are always available for each individual in a population. Hence, missing/partially-overlapping anatomical information is often available across individuals in a population. We introduce a generative shape model for complex anatomical structures, learnable from datasets of unpaired datasets. The proposed generative model can synthesise complete whole complex shape assemblies coined virtual chimaeras, as opposed to natural human chimaeras. We applied this framework to build virtual chimaeras from databases of whole-heart shape assemblies that each contribute samples for heart substructures. Specifically, we propose a generative shape compositional framework which comprises two components - a part-aware generative shap
    
[^339]: 多视图超复数学习用于乳腺癌筛查

    Multi-View Hypercomplex Learning for Breast Cancer Screening

    [https://arxiv.org/abs/2204.05798](https://arxiv.org/abs/2204.05798)

    本文提出了一种基于参数化超复数神经网络的多视图乳腺癌分类方法，能够模拟并利用乳房X光检查的不同视图之间的相关性，从而提高肿瘤识别效果。

    

    传统上，用于乳腺癌分类的深度学习方法执行单视图分析。然而，由于乳腺X-ray图像中包含的相关性，放射科医生同时分析组成乳房X光摄影检查的所有四个视图，这为识别肿瘤提供了关键信息。鉴于此，一些研究已经开始提出多视图方法。然而，在这样的现有架构中，乳房X光图像被独立的卷积分支处理为独立的图像，从而失去了它们之间的相关性。为了克服这些局限性，在本文中，我们提出了一种基于参数化超复数神经网络的多视图乳腺癌分类方法。由于超复数代数特性，我们的网络能够建模并利用组成乳房X光检查的不同视图之间的现有相关性，从而模拟阅片过程。

    arXiv:2204.05798v3 Announce Type: replace-cross  Abstract: Traditionally, deep learning methods for breast cancer classification perform a single-view analysis. However, radiologists simultaneously analyze all four views that compose a mammography exam, owing to the correlations contained in mammography views, which present crucial information for identifying tumors. In light of this, some studies have started to propose multi-view methods. Nevertheless, in such existing architectures, mammogram views are processed as independent images by separate convolutional branches, thus losing correlations among them. To overcome such limitations, in this paper, we propose a methodological approach for multi-view breast cancer classification based on parameterized hypercomplex neural networks. Thanks to hypercomplex algebra properties, our networks are able to model, and thus leverage, existing correlations between the different views that comprise a mammogram, thus mimicking the reading process
    
[^340]: 解决模型为基础的离线强化学习的样本复杂性问题

    Settling the Sample Complexity of Model-Based Offline Reinforcement Learning

    [https://arxiv.org/abs/2204.05275](https://arxiv.org/abs/2204.05275)

    该论文展示了基于模型的（或“插件”）方法在标签化马尔可夫决策过程（MDPs）中实现了无烧录成本的极小极优样本复杂性。

    

    本文关注离线强化学习（RL），它利用预先收集的数据进行学习，无需进一步探索。有效的离线RL应能适应分布转移和有限的数据覆盖。然而，先前的算法或分析要么受到次优样本复杂性的困扰，要么产生高昂的烧录成本以达到样本最优性，从而对样本匮乏应用中的高效离线RL构成障碍。

    arXiv:2204.05275v3 Announce Type: replace-cross  Abstract: This paper is concerned with offline reinforcement learning (RL), which learns using pre-collected data without further exploration. Effective offline RL would be able to accommodate distribution shift and limited data coverage. However, prior algorithms or analyses either suffer from suboptimal sample complexities or incur high burn-in cost to reach sample optimality, thus posing an impediment to efficient offline RL in sample-starved applications.   We demonstrate that the model-based (or "plug-in") approach achieves minimax-optimal sample complexity without burn-in cost for tabular Markov decision processes (MDPs). Concretely, consider a finite-horizon (resp. $\gamma$-discounted infinite-horizon) MDP with $S$ states and horizon $H$ (resp. effective horizon $\frac{1}{1-\gamma}$), and suppose the distribution shift of data is reflected by some single-policy clipped concentrability coefficient $C^{\star}_{\text{clipped}}$. We p
    
[^341]: 加强数字健康服务：基于机器学习的个性化运动目标设定方法

    Enhancing Digital Health Services: A Machine Learning Approach to Personalized Exercise Goal Setting

    [https://arxiv.org/abs/2204.00961](https://arxiv.org/abs/2204.00961)

    本研究通过开发机器学习算法，动态更新个性化运动目标，填补了现有方法对用户动态行为和健康状况变化的忽视。

    

    最近数字健康的利用率有所增加，这些服务提供广泛指导，通过设定每日运动目标来鼓励用户进行频繁运动，促进健康的生活方式。这些全面的指南是从考虑各种个性化行为因素发展而来的。然而，现有方法经常忽视用户的动态行为和其健康状况的变化。本研究旨在填补这一空白，通过开发一种机器学习算法，使用回顾性数据和现实行为轨迹动态更新自动建议的运动目标。我们设计了一种深度强化学习算法进行方法学研究，评估运动表现，考虑到体能疲劳效应。深度强化学习算法结合了深度学习技术来分析时间序列数据并推断用户的运动行为。

    arXiv:2204.00961v3 Announce Type: replace  Abstract: The utilization of digital health has increased recently, and these services provide extensive guidance to encourage users to exercise frequently by setting daily exercise goals to promote a healthy lifestyle. These comprehensive guides evolved from the consideration of various personalized behavioral factors. Nevertheless, existing approaches frequently neglect the users dynamic behavior and the changing in their health conditions. This study aims to fill this gap by developing a machine learning algorithm that dynamically updates auto-suggestion exercise goals using retrospective data and realistic behavior trajectory. We conducted a methodological study by designing a deep reinforcement learning algorithm to evaluate exercise performance, considering fitness-fatigue effects. The deep reinforcement learning algorithm combines deep learning techniques to analyse time series data and infer user exercise behavior. In addition, we use 
    
[^342]: CMGAN：基于Conformer的度量GAN用于语音增强

    CMGAN: Conformer-based Metric GAN for Speech Enhancement

    [https://arxiv.org/abs/2203.15149](https://arxiv.org/abs/2203.15149)

    本文提出了一种基于Conformer的度量生成对抗网络（CMGAN）用于时频域的语音增强，通过优化生成器以使得增强估计语音相对应的评估分数来进一步提高增强语音的质量。

    

    最近，卷积增强变压器（Conformer）在自动语音识别（ASR）和时域语音增强（SE）中取得了很好的表现，因为它可以捕捉语音信号中的局部和全局依赖关系。本文提出了一种基于Conformer的度量生成对抗网络（CMGAN）用于时频域的SE。在生成器中，我们利用两阶段的Conformer块通过对时间和频率依赖关系进行建模，聚合所有幅度和复数谱信息。在解码器阶段，幅度和复数谱的估计被解耦，然后一起合并以重构增强的语音。此外，引入了一个度量鉴别器，通过优化生成器以使得增强估计语音相对应的评估分数，进一步提高增强语音的质量。在Voice Bank+DEMAND数据集上进行了定量分析。

    arXiv:2203.15149v4 Announce Type: replace-cross  Abstract: Recently, convolution-augmented transformer (Conformer) has achieved promising performance in automatic speech recognition (ASR) and time-domain speech enhancement (SE), as it can capture both local and global dependencies in the speech signal. In this paper, we propose a conformer-based metric generative adversarial network (CMGAN) for SE in the time-frequency (TF) domain. In the generator, we utilize two-stage conformer blocks to aggregate all magnitude and complex spectrogram information by modeling both time and frequency dependencies. The estimation of magnitude and complex spectrogram is decoupled in the decoder stage and then jointly incorporated to reconstruct the enhanced speech. In addition, a metric discriminator is employed to further improve the quality of the enhanced estimated speech by optimizing the generator with respect to a corresponding evaluation score. Quantitative analysis on Voice Bank+DEMAND dataset in
    
[^343]: 伪微分神经算子：用于学习偏微分方程解算子的广义傅立叶神经算子

    Pseudo-Differential Neural Operator: Generalized Fourier Neural Operator for Learning Solution Operators of Partial Differential Equations

    [https://arxiv.org/abs/2201.11967](https://arxiv.org/abs/2201.11967)

    提出了伪微分积分算子（PDIO）来分析和推广傅立叶神经算子（FNO）中的傅立叶积分算子，实现了神经网络符号的平滑化，并证明了PDIO是有界线性算子，连续作用于Sobolev空间。

    

    学习两个函数空间之间的映射已经引起了相当多的研究关注。然而，学习偏微分方程（PDEs）的解算子在科学计算中仍然是一个挑战。最近，提出了傅立叶神经算子（FNO）来学习解算子，并且取得了出色的性能。在本研究中，我们提出了一种新颖的\textit{伪微分积分算子}（PDIO）来分析和推广FNO中的傅立叶积分算子。PDIO的灵感来自伪微分算子，这是一种由某个符号特征化的广义微分算子。我们使用神经网络来参数化这个符号，并展示基于神经网络的符号包含在平滑符号类中。随后，我们验证PDIO是有界线性算子，因此在Sobolev空间中是连续的。我们将PDIO与神经算子结合起来，发展一种\text

    arXiv:2201.11967v3 Announce Type: replace  Abstract: Learning the mapping between two function spaces has garnered considerable research attention. However, learning the solution operator of partial differential equations (PDEs) remains a challenge in scientific computing. Fourier neural operator (FNO) was recently proposed to learn solution operators, and it achieved an excellent performance. In this study, we propose a novel \textit{pseudo-differential integral operator} (PDIO) to analyze and generalize the Fourier integral operator in FNO. PDIO is inspired by a pseudo-differential operator, which is a generalized differential operator characterized by a certain symbol. We parameterize this symbol using a neural network and demonstrate that the neural network-based symbol is contained in a smooth symbol class. Subsequently, we verify that the PDIO is a bounded linear operator, and thus is continuous in the Sobolev space. We combine the PDIO with the neural operator to develop a \text
    
[^344]: 面向数据有限不平衡的组织的辅助学习

    Assisted Learning for Organizations with Limited Imbalanced Data

    [https://arxiv.org/abs/2109.09307](https://arxiv.org/abs/2109.09307)

    提出了一个辅助学习框架，帮助组织改善学习性能，通过购买外部服务提供商的辅助服务来提高模型性能，针对有限和不平衡的数据提出了有效的随机训练算法

    

    在大数据时代，许多大型组织正在将机器学习集成到其工作流程中，以促进数据分析。然而，由于他们可获得的数据有限且不平衡，他们训练的模型性能往往受到限制。在这项工作中，我们为协助组织改善学习性能开发了一个辅助学习框架。这些组织具有足够的计算资源，但受制于严格的数据共享和协作政策。他们的有限且不平衡的数据往往导致有偏推断和次优决策。在辅助学习中，组织学习者从外部服务提供商购买辅助服务，旨在在仅需几轮辅助的情况下提高其模型性能。我们为辅助深度学习和辅助强化学习开发了有效的随机训练算法。

    arXiv:2109.09307v4 Announce Type: replace  Abstract: In the era of big data, many big organizations are integrating machine learning into their work pipelines to facilitate data analysis. However, the performance of their trained models is often restricted by limited and imbalanced data available to them. In this work, we develop an assisted learning framework for assisting organizations to improve their learning performance. The organizations have sufficient computation resources but are subject to stringent data-sharing and collaboration policies. Their limited imbalanced data often cause biased inference and sub-optimal decision-making. In assisted learning, an organizational learner purchases assistance service from an external service provider and aims to enhance its model performance within only a few assistance rounds. We develop effective stochastic training algorithms for both assisted deep learning and assisted reinforcement learning. Different from existing distributed algor
    
[^345]: 多边形未调整的朗之万算法：为神经网络创建稳定高效的自适应算法

    Polygonal Unadjusted Langevin Algorithms: Creating stable and efficient adaptive algorithms for neural networks

    [https://arxiv.org/abs/2105.13937](https://arxiv.org/abs/2105.13937)

    提出了一种基于多边形未调整的朗之万算法的新类别算法，名为TH$\varepsilon$O POULA（或简称为TheoPouLa），通过稳定性、非渐进分析和实验表明其在神经网络优化中具有卓越性能。

    

    我们提出了一种新的基于朗之万算法的算法类别，克服了当前用于微调深度学习模型的流行自适应优化器的许多已知缺陷。其理论基础依赖于近期对于具有单调系数的随机微分方程（SDEs）的欧拉多边形逼近的发展。因此，它继承了温和算法的稳定性特性，同时解决了神经网络中的其他已知问题，例如梯度消失。特别地，我们对这个新类别算法的收敛特性进行了非渐进分析和全面的理论保证，我们将这个算法命名为TH$\varepsilon$O POULA（或简称为TheoPouLa）。最后，我们展示了使用不同类型的深度学习模型进行的几个实验，结果表明TheoPouLa相对于许多流行的自适应优化算法具有卓越性能。

    arXiv:2105.13937v3 Announce Type: replace  Abstract: We present a new class of Langevin based algorithms, which overcomes many of the known shortcomings of popular adaptive optimizers that are currently used for the fine tuning of deep learning models. Its underpinning theory relies on recent advances of Euler's polygonal approximations for stochastic differential equations (SDEs) with monotone coefficients. As a result, it inherits the stability properties of tamed algorithms, while it addresses other known issues, e.g. vanishing gradients in neural networks. In particular, we provide a nonasymptotic analysis and full theoretical guarantees for the convergence properties of an algorithm of this novel class, which we named TH$\varepsilon$O POULA (or, simply, TheoPouLa). Finally, several experiments are presented with different types of deep learning models, which show the superior performance of TheoPouLa over many popular adaptive optimization algorithms.
    
[^346]: 自适应有理激活以提升深度强化学习

    Adaptive Rational Activations to Boost Deep Reinforcement Learning

    [https://arxiv.org/abs/2102.09407](https://arxiv.org/abs/2102.09407)

    本研究提出了利用有理数作为适应性激活函数来改进深度强化学习，并展示了这种方法在Atari游戏中取得了一致的改进，特别是将简单的DQN提升为一个稳健的方法。

    

    生物学的最新见解显示，智能不仅源自神经元之间的连接，而且单个神经元承担的计算责任比以往预期的更多。这种观点在不断变化的强化学习环境中至关重要，然而当前方法仍然主要使用静态激活函数。在本工作中，我们阐述了为什么有理数适合作为可适应的激活函数，以及为什么将其包含到神经网络中至关重要。受Residual网络中循环性的启发，我们导出了有理单位在残差连接下封闭的条件，并制定了一个自然正则化的版本：循环有理数。我们证明，为流行算法配备（循环）有理数激活会显著提高Atari游戏的性能，尤其将简单的DQN转化为一种可靠的方法。

    arXiv:2102.09407v4 Announce Type: replace  Abstract: Latest insights from biology show that intelligence not only emerges from the connections between neurons but that individual neurons shoulder more computational responsibility than previously anticipated. This perspective should be critical in the context of constantly changing distinct reinforcement learning environments, yet current approaches still primarily employ static activation functions. In this work, we motivate why rationals are suitable for adaptable activation functions and why their inclusion into neural networks is crucial. Inspired by recurrence in residual networks, we derive a condition under which rational units are closed under residual connections and formulate a naturally regularised version: the recurrent-rational. We demonstrate that equipping popular algorithms with (recurrent-)rational activations leads to consistent improvements on Atari games, especially turning simple DQN into a solid approach, competiti
    
[^347]: 私人预测集

    Private Prediction Sets

    [https://arxiv.org/abs/2102.06202](https://arxiv.org/abs/2102.06202)

    该研究提出了一个基于符合性预测的框架，可以在保护个人隐私的同时返回可靠的不确定性量化的预测集。

    

    在涉及重要决策的现实环境中，部署机器学习系统通常需要可靠的不确定性量化和保护个人隐私。我们提出了一个框架，将这两个目标同时视为重要。我们的框架基于符合性预测，这种方法可以扩展预测模型，返回提供不确定性量化的预测集，这些集合可以证明以用户指定的概率（如90%）覆盖真实响应。当与经过私人训练的模型一起使用时，人们可能希望符合性预测会为生成的预测集提供隐私保证；不幸的是，情况并非如此。为了解决这一关键问题，我们开发了一种方法，该方法可以从任何预先训练的预测模型中输出差分私人预测集。我们的方法遵循分裂符合性预测的一般方法；我们使用保留数据

    arXiv:2102.06202v3 Announce Type: replace-cross  Abstract: In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately, this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data 
    
[^348]: FetusMap：3D超声胎儿姿势估计

    FetusMap: Fetal Pose Estimation in 3D Ultrasound

    [https://arxiv.org/abs/1910.04935](https://arxiv.org/abs/1910.04935)

    本文第一次提出了关于文献中胎儿3D姿势估计的工作，旨在提取整个胎儿的骨架并分配正确的躯干/肢体标签给不同的部分/关节。

    

    3D超声（US）技术激发了大量自动产前检查。然而，在3D超声中对整个胎儿的结构化描述的研究仍然很少。本文提出在US体积中估计胎儿的3D姿势，以促进其在全局和局部尺度上的定量分析。在3D US中存在很多挑战，包括体积尺寸大、图像质量差、解剖结构的对称性模糊和胎儿姿势的大变化，我们的贡献有三个方面。

    arXiv:1910.04935v2 Announce Type: replace-cross  Abstract: The 3D ultrasound (US) entrance inspires a multitude of automated prenatal examinations. However, studies about the structuralized description of the whole fetus in 3D US are still rare. In this paper, we propose to estimate the 3D pose of fetus in US volumes to facilitate its quantitative analyses in global and local scales. Given the great challenges in 3D US, including the high volume dimension, poor image quality, symmetric ambiguity in anatomical structures and large variations of fetal pose, our contribution is three-fold. (i) This is the first work about 3D pose estimation of fetus in the literature. We aim to extract the skeleton of whole fetus and assign different segments/joints with correct torso/limb labels. (ii) We propose a self-supervised learning (SSL) framework to finetune the deep network to form visually plausible pose predictions. Specifically, we leverage the landmark-based registration to effectively encod
    
[^349]: 具有热启动和主动终止功能的代理器用于在3D超声中定位平面

    Agent with Warm Start and Active Termination for Plane Localization in 3D Ultrasound

    [https://arxiv.org/abs/1910.04331](https://arxiv.org/abs/1910.04331)

    提出了一种具有热启动和主动终止功能的代理器，用于在3D超声中自动定位胎儿脑标准平面

    

    标准平面定位对超声（US）诊断至关重要。在产前超声中，使用2D探头手动获取数十个标准平面是耗时且依赖操作者的。相比之下，包含多个标准平面的3D US拥有更少的用户依赖性和更高的效率优势。然而，在US体积中手动定位平面由于巨大的搜索空间和胎儿姿势变化而具有挑战性。在本研究中，我们提出了一种新颖的强化学习（RL）框架，用于自动定位3D US中的胎儿脑标准平面。

    arXiv:1910.04331v2 Announce Type: replace-cross  Abstract: Standard plane localization is crucial for ultrasound (US) diagnosis. In prenatal US, dozens of standard planes are manually acquired with a 2D probe. It is time-consuming and operator-dependent. In comparison, 3D US containing multiple standard planes in one shot has the inherent advantages of less user-dependency and more efficiency. However, manual plane localization in US volume is challenging due to the huge search space and large fetal posture variation. In this study, we propose a novel reinforcement learning (RL) framework to automatically localize fetal brain standard planes in 3D US. Our contribution is two-fold. First, we equip the RL framework with a landmark-aware alignment module to provide warm start and strong spatial bounds for the agent actions, thus ensuring its effectiveness. Second, instead of passively and empirically terminating the agent inference, we propose a recurrent neural network based strategy for
    
[^350]: 三维经直肠超声深度关注特征在前列腺分割中的应用

    Deep Attentive Features for Prostate Segmentation in 3D Transrectal Ultrasound

    [https://arxiv.org/abs/1907.01743](https://arxiv.org/abs/1907.01743)

    本文提出了一种新型的3D深度神经网络，配备着关注模块，通过充分利用卷积神经网络不同层中编码的互补信息，实现了在经直肠超声图像中更好地前列腺分割，通过选择性地整合不同层级的特征来提高前列腺分割性能

    

    arXiv:1907.01743v2 公告类型: 替换-跨度  摘要: 在图像引导的前列腺干预和治疗计划中，自动前列腺分割在经直肠超声(TRUS)图像中至关重要。然而，由于 TRUS 中前列腺的边界缺失/模糊和不均匀的强度分布，以及前列腺形状的大量变异性，开发这样的自动解决方案仍然非常具有挑战性。本文发展了一种配备关注模块的新型3D深度神经网络，通过充分利用卷积神经网络 (CNN) 不同层中编码的互补信息，来更好地对 TRUS 中的前列腺进行分割。我们的关注模块利用关注机制，有选择地利用不同层集成的多级特征来完善每个单独层的特征，抑制 CNN 浅层中的非前列腺噪声，并将更多前列腺细节融入特征中。

    arXiv:1907.01743v2 Announce Type: replace-cross  Abstract: Automatic prostate segmentation in transrectal ultrasound (TRUS) images is of essential importance for image-guided prostate interventions and treatment planning. However, developing such automatic solutions remains very challenging due to the missing/ambiguous boundary and inhomogeneous intensity distribution of the prostate in TRUS, as well as the large variability in prostate shapes. This paper develops a novel 3D deep neural network equipped with attention modules for better prostate segmentation in TRUS by fully exploiting the complementary information encoded in different layers of the convolutional neural network (CNN). Our attention module utilizes the attention mechanism to selectively leverage the multilevel features integrated from different layers to refine the features at each individual layer, suppressing the non-prostate noise at shallow layers of the CNN and increasing more prostate details into features at deep
    
[^351]: 深度学习在多标签学习中的应用：一项全面调研

    Deep Learning for Multi-Label Learning: A Comprehensive Survey. (arXiv:2401.16549v1 [cs.LG])

    [http://arxiv.org/abs/2401.16549](http://arxiv.org/abs/2401.16549)

    深度学习在多标签学习中的综合调研，旨在审视深度学习在解决多标签分类中的挑战方面的最新进展。

    

    多标签学习是一个快速发展的研究领域，旨在从单个输入数据点中预测多个标签。在大数据时代，涉及多标签分类或排名的任务带来了重大而复杂的挑战，在各个领域引起了极大关注。多标签分类面临的困难包括处理高维数据、解决标签相关性和处理部分标签，传统方法在这方面表现不佳。近年来，人们越来越多地采用深度学习技术来更有效地应对多标签分类中的这些挑战。值得注意的是，针对深度学习在多标签学习中的综合研究还比较有限。因此，本调研旨在全面审视深度学习在多标签学习中的最新进展。

    Multi-label learning is a rapidly growing research area that aims to predict multiple labels from a single input data point. In the era of big data, tasks involving multi-label classification (MLC) or ranking present significant and intricate challenges, capturing considerable attention in diverse domains. Inherent difficulties in MLC include dealing with high-dimensional data, addressing label correlations, and handling partial labels, for which conventional methods prove ineffective. Recent years have witnessed a notable increase in adopting deep learning (DL) techniques to address these challenges more effectively in MLC. Notably, there is a burgeoning effort to harness the robust learning capabilities of DL for improved modelling of label dependencies and other challenges in MLC. However, it is noteworthy that comprehensive studies specifically dedicated to DL for multi-label learning are limited. Thus, this survey aims to thoroughly review recent progress in DL for multi-label lea
    
[^352]: 大模型时代中的数据增强研究综述

    A Survey on Data Augmentation in Large Model Era. (arXiv:2401.15422v1 [cs.LG])

    [http://arxiv.org/abs/2401.15422](http://arxiv.org/abs/2401.15422)

    这篇论文综述了大模型驱动的数据增强方法，包括图像增强、文本增强和配对数据增强。这些方法利用大模型的能力，有效提高了数据增强的效果，是解决大模型训练中数据质量不足的重要研究方向。

    

    大模型，包括大语言和扩散模型，在近似人类级智能方面显示出卓越的潜力，引起了学术界和工业界的极大关注。然而，训练这些大模型需要大量高质量的数据，并且随着这些模型的持续更新，现有的高质量数据储备可能很快用尽。这个挑战催生了大量关于数据增强方法的研究。利用大模型，这些数据增强技术超越了传统方法。本文综合考虑，提供了大模型驱动的数据增强方法的详尽回顾。我们首先将相关研究分为图像增强、文本增强和配对数据增强三个主要类别。然后，我们深入探讨了与大模型数据增强相关的各种数据后处理技术。

    Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data au
    
[^353]: 关于神经网络在通用多模态推理中的泛化能力的研究

    On the generalization capacity of neural networks during generic multimodal reasoning. (arXiv:2401.15030v1 [cs.LG])

    [http://arxiv.org/abs/2401.15030](http://arxiv.org/abs/2401.15030)

    本研究评估了不同神经网络架构在多模态泛化方面的能力，并发现具有多个注意力层或利用交叉注意机制的模型表现更好。

    

    Transformer的出现导致了大型语言模型（LLM）的发展, 这些模型似乎展示了类似人类的能力。为了评估这类模型和其他基本的神经网络架构在多模态领域的一般性，我们评估和比较了它们在多模态泛化方面的能力。我们引入了一个多模态问答基准来评估三种特定类型的超出分布（OOD）泛化性能：分心泛化（在分心存在的情况下泛化），系统的组合泛化（对新的任务排列的泛化）和有益的组合泛化（对更复杂的任务结构进行泛化）。我们发现，在不同的模型架构上（如RNN，Transformer，Perceivers等），具有多个注意力层或者利用输入领域之间的交叉注意机制的模型更好。我们的积极结果表明，对于多模态泛化，模型架构是重要因素。

    The advent of the Transformer has led to the development of large language models (LLM), which appear to demonstrate human-like capabilities. To assess the generality of this class of models and a variety of other base neural network architectures to multimodal domains, we evaluated and compared their capacity for multimodal generalization. We introduce a multimodal question-answer benchmark to evaluate three specific types of out-of-distribution (OOD) generalization performance: distractor generalization (generalization in the presence of distractors), systematic compositional generalization (generalization to new task permutations), and productive compositional generalization (generalization to more complex tasks structures). We found that across model architectures (e.g., RNNs, Transformers, Perceivers, etc.), models with multiple attention layers, or models that leveraged cross-attention mechanisms between input domains, fared better. Our positive results demonstrate that for multi
    
[^354]: FP6-LLM: 通过FP6中心算法-系统协同设计高效提供大型语言模型

    FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])

    [http://arxiv.org/abs/2401.14112](http://arxiv.org/abs/2401.14112)

    FP6-LLM提出了一种支持六位量化的GPU算法-系统协同设计方案，实现了在大型语言模型中推断成本和模型质量之间的平衡。

    

    六位量化（FP6）可以有效地减小大型语言模型（LLM）的大小，并在不同应用中保持模型质量的一致性。然而，现有系统不提供FP6量化的张量核心支持，并且在LLM推断过程中很难实现实际性能改进。由于（1）模型权重具有不规则位宽的不友好内存访问和（2）权重去量化的高运行时开销，支持在GPU上进行FP6量化是具有挑战性的。为了解决这些问题，我们提出了TC-FPx，这是第一个具有统一张量核心支持的浮点权重的完整GPU内核设计方案，适用于各种量化位宽。我们将TC-FPx内核集成到现有推断系统中，提供了新的端到端支持（称为FP6-LLM）用于量化LLM推断，从而实现了推断成本和模型质量之间更好的平衡。实验证明，FP6-LLM仅使用一部分存储空间就可以进行LLaMA-70b的推断。

    Six-bit quantization (FP6) can effectively reduce the size of large language models (LLMs) and preserve the model quality consistently across varied applications. However, existing systems do not provide Tensor Core support for FP6 quantization and struggle to achieve practical performance improvements during LLM inference. It is challenging to support FP6 quantization on GPUs due to (1) unfriendly memory access of model weights with irregular bit-width and (2) high runtime overhead of weight de-quantization. To address these problems, we propose TC-FPx, the first full-stack GPU kernel design scheme with unified Tensor Core support of float-point weights for various quantization bit-width. We integrate TC-FPx kernel into an existing inference system, providing new end-to-end support (called FP6-LLM) for quantized LLM inference, where better trade-offs between inference cost and model quality are achieved. Experiments show that FP6-LLM enables the inference of LLaMA-70b using only a sin
    
[^355]: 从推特到引用：揭示社交媒体影响者对人工智能研究可见性的影响

    Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])

    [http://arxiv.org/abs/2401.13782](http://arxiv.org/abs/2401.13782)

    本文研究了社交媒体影响者在提高机器学习研究的可见性方面的作用，发现被这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还探讨了被展示作者的地理、性别和机构多样性。

    

    随着人工智能和机器学习会议上被接受的论文数量达到数千篇，研究人员如何获取和阅读研究论文变得不清楚。本文研究了社交媒体影响者在增强机器学习研究可见性中的作用，特别是他们分享的论文引用次数。我们编制了一个包括8000多篇论文的全面数据集，涵盖了2018年12月至2023年10月的推特，以及基于出版年份、会议地点和摘要主题进行1：1匹配的对照组。我们的分析揭示了这些影响者认可的论文引用次数显著增加，中位数引用次数比对照组高2-3倍。此外，该研究还深入研究了被展示作者的地理、性别和机构多样性。这些发现突显了社交媒体在学术交流中的不断扩大的影响力，并强调了当今数字化时代不断发展的生态系统的重要性。

    As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
    
[^356]: 面向多智能体远程控制的基于语言到新兴通信的知识蒸馏

    Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control. (arXiv:2401.12624v1 [cs.AI])

    [http://arxiv.org/abs/2401.12624](http://arxiv.org/abs/2401.12624)

    这项工作通过将语言导向的语义通信与新兴通信相结合，通过知识蒸馏的方式，提出了一种面向多智能体远程控制的新框架，实现了更快的行程时间和更高的训练收敛速度。

    

    在这项工作中，我们比较了基于多智能体深度强化学习（MADRL）的新兴通信（EC）和由预训练的大型语言模型（LLM）使用人类语言的面向语言的语义通信（LSC）。在一个多智能体远程导航任务中，使用包含位置和通道地图的多模态输入数据，结果表明，EC在使用多模态数据时会产生高的训练成本和困难，而LSC由于LLM尺寸较大，会导致高的推理计算成本。为了解决它们各自的瓶颈，我们提出了一种通过知识蒸馏（KD）引导EC训练使用LSC的新颖框架：语言引导的EC（LEC）。模拟验证了LEC实现了更快的行程时间，避免了信道质量差的区域，并且在与EC相比能够加速MADRL训练收敛达到61.8%。

    In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.
    
[^357]: PartIR: 为机器学习组合SPMD分区策略

    PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v1 [cs.LG])

    [http://arxiv.org/abs/2401.11202](http://arxiv.org/abs/2401.11202)

    PartIR是一种用于机器学习的分区系统，具备表达力强和可预测性强的特点。它通过高级程序员发出的分区策略驱动，并采用增量重写方法，能够组合不同的分片策略，评估结果表明其可预测性、表达能力和达到峰值性能能力强。

    

    现代大规模神经网络（NN）的训练需要结合数据、模型或优化器分片的并行化策略。当策略变得复杂时，分区工具需要具备以下特点：1）表达力强，允许组合简单策略；2）可预测性强，可以通过分析估算性能。我们提出了PartIR，一种用于NN分区的设计。PartIR采用增量重写方法，与硬件和运行时无关。我们提供了一个简单而强大的API用于组合分片策略，并提供了一个模拟器进行验证。整个过程由高级程序员发出的分区策略驱动，既可以手动也可以自动。重要的是，这些策略与模型代码分开指定，易于更改。我们通过对几种不同模型的评估来展示PartIR的可预测性、表达能力和达到峰值性能的能力。

    Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..
    
[^358]: AutoChunk: 自动激活块用于内存高效的长序列推断

    AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence Inference. (arXiv:2401.10652v1 [cs.PF])

    [http://arxiv.org/abs/2401.10652](http://arxiv.org/abs/2401.10652)

    AutoChunk是一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。

    

    大型深度学习模型在各种应用中取得了令人瞩目的性能。然而，它们对内存的大量需求，包括参数内存和激活内存，已经成为实际应用中的重大挑战。现有方法主要处理参数内存，对激活内存的重要性却被忽视了。特别是对于长输入序列，随着序列长度的增加，激活内存预计会经历显著的指数增长。在这个方法中，我们提出了AutoChunk，一种自动和自适应的编译器系统，通过块策略有效地减少长序列推断的激活内存。所提出的系统通过多个阶段的优化生成块计划。在每个阶段，块搜索通过探索所有可能的块候选项，块选择通过识别最佳块进行。运行时，AutoChunk采用代码生成自动应用块策略。

    Large deep learning models have achieved impressive performance across a range of applications. However, their large memory requirements, including parameter memory and activation memory, have become a significant challenge for their practical serving. While existing methods mainly address parameter memory, the importance of activation memory has been overlooked. Especially for long input sequences, activation memory is expected to experience a significant exponential growth as the length of sequences increases. In this approach, we propose AutoChunk, an automatic and adaptive compiler system that efficiently reduces activation memory for long sequence inference by chunk strategies. The proposed system generates chunk plans by optimizing through multiple stages. In each stage, the chunk search pass explores all possible chunk candidates and the chunk selection pass identifies the optimal one. At runtime, AutoChunk employs code generation to automatically apply chunk strategies. The exp
    
[^359]: 使用机器学习库在结构化网格上解决具有界面捕获的离散多相流动方程

    Solving the Discretised Multiphase Flow Equations with Interface Capturing on Structured Grids Using Machine Learning Libraries. (arXiv:2401.06755v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2401.06755](http://arxiv.org/abs/2401.06755)

    本文使用AI4PDEs方法对具有界面捕获的离散多相流动方程进行求解，并引入了一种新的基于残差公式的压缩代数体积流方法。

    

    本文使用AI4PDEs方法（即用于偏微分方程的人工智能）解决了具有界面捕获的多相流动方程。AI4PDEs中的求解器使用机器学习库的工具来准确求解已经通过数值方法离散化的偏微分方程。卷积层可以用来将离散化表示为神经网络，其权重由数值方法确定，而不是通过训练确定。为了解决系统，通过一个U-Net架构的神经网络来实现多重网格求解器。非相溶的二相流动由三维不可压缩Navier-Stokes方程建模，其中包括表面张力和体积分数场的对流，该场描述了流体之间的界面。介绍了一种新的基于残差公式的压缩代数体积流方法，该方法使用Petrov-Galerkin进行准确性，并专为AI4PDEs设计。高阶有限差分方法被用于求解。

    This paper solves the multiphase flow equations with interface capturing using the AI4PDEs approach (Artificial Intelligence for Partial Differential Equations). The solver within AI4PDEs uses tools from machine learning (ML) libraries to solve (exactly) partial differential equations (PDEs) that have been discretised using numerical methods. Convolutional layers can be used to express the discretisations as a neural network, whose weights are determined by the numerical method, rather than by training. To solve the system, a multigrid solver is implemented through a neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier-Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov-Galerkin for accuracy and designed with AI4PDEs in mind. High-order fini
    
[^360]: 从轨迹和细胞命运信息中预测细胞身份

    Prediction of Cellular Identities from Trajectory and Cell Fate Information. (arXiv:2401.06182v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.06182](http://arxiv.org/abs/2401.06182)

    本研究提出了一种创新的方法，利用机器学习从早期C. elegans胚胎的图像序列中预测细胞身份。通过使用少量的细胞轨迹和细胞命运信息，我们的模型在有限数据条件下达到了超过90%的分类准确率。

    

    在图像序列中确定细胞身份是一项重要且具有挑战性的任务。传统的细胞识别方法是通过细胞追踪，这是复杂且耗时的。在本研究中，我们提出了一种创新的方法，使用机器学习来识别早期C. elegans胚胎内的细胞身份。我们使用随机森林、MLP和LSTM模型，对跨越胚胎发育的前4个小时的3D时间序列共聚焦数据集进行细胞分类准确性测试。通过利用个体细胞的少量时空特征，包括细胞轨迹和细胞命运信息，我们的模型在有限数据条件下实现了超过90%的准确率。我们还确定了最重要的特征贡献，并可以从生物学知识的角度解释这些特征。我们的研究展示了直接从简单的时空特征中预测4D图像序列中的细胞身份的成功。

    Determining cell identities in imaging sequences is an important yet challenging task. The conventional method for cell identification is via cell tracking, which is complex and can be time-consuming. In this study, we propose an innovative approach to cell identification during early C. elegans embryogenesis using machine learning. We employed random forest, MLP, and LSTM models, and tested cell classification accuracy on 3D time-lapse confocal datasets spanning the first 4 hours of embryogenesis. By leveraging a small number of spatial-temporal features of individual cells, including cell trajectory and cell fate information, our models achieve an accuracy of over 90%, even with limited data. We also determine the most important feature contributions and can interpret these features in the context of biological knowledge. Our research demonstrates the success of predicting cell identities in 4D imaging sequences directly from simple spatio-temporal features.
    
[^361]: 基于小波启发的多尺度图卷积循环网络用于交通预测

    Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting. (arXiv:2401.06040v1 [cs.LG])

    [http://arxiv.org/abs/2401.06040](http://arxiv.org/abs/2401.06040)

    本论文提出了一种基于小波启发的多尺度图卷积循环网络，用于交通预测。该方法将多尺度分析方法和深度学习方法相结合，对交通数据中的多尺度结构进行建模，并展现了较好的性能。

    

    交通预测是智能交通系统的基础。时空图神经网络在交通预测中展现出了最先进的性能。然而，这些方法并没有明确地对交通数据中的某些自然特征进行建模，比如包含不同粒度或尺度上的空间和时间变化的多尺度结构。为此，我们提出了一种基于小波启发的图卷积循环网络（WavGCRN），将多尺度分析（MSA）方法和深度学习（DL）方法相结合。在WavGCRN中，交通数据通过离散小波变换（DWT）被分解为时频分量，构建了多流输入结构；然后使用图卷积循环网络（GCRNs）作为编码器对每个流进行特征提取，提取不同尺度的时空特征；最后，可学习的逆DWT和GCRN被结合为解码器，融合信息。

    Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the inf
    
[^362]: VI-PANN: 利用转移学习和不确定性感知变分推理来提高音频模式识别中的泛化性能

    VI-PANN: Harnessing Transfer Learning and Uncertainty-Aware Variational Inference for Improved Generalization in Audio Pattern Recognition. (arXiv:2401.05531v1 [cs.LG])

    [http://arxiv.org/abs/2401.05531](http://arxiv.org/abs/2401.05531)

    本研究提出了VI-PANN，利用转移学习和不确定性感知变分推理方法，在音频模式识别中取得了改进的泛化性能。

    

    转移学习是一种越来越流行的深度学习模型训练方法，利用在多样、大规模数据集上训练基础模型获取的知识，应用于在可用领域或任务特定数据较少的下游任务中。现有文献中有许多转移学习技术和应用，但大部分研究使用的是确定性的深度学习模型，这些模型通常不经校准，也无法提供预测的认知（模型）不确定度。与确定性模型相比，贝叶斯深度学习模型往往能够很好地进行校准，提供预测的认知不确定度，并具有竞争性的预测性能。本研究提出了变分推理预训练音频神经网络（VI-PANNs）。VI-PANNs是基于流行的ResNet-54架构的变分推理变体，其在大规模音频事件检测数据集AudioSet上进行了预训练。

    Transfer learning (TL) is an increasingly popular approach to training deep learning (DL) models that leverages the knowledge gained by training a foundation model on diverse, large-scale datasets for use on downstream tasks where less domain- or task-specific data is available. The literature is rich with TL techniques and applications; however, the bulk of the research makes use of deterministic DL models which are often uncalibrated and lack the ability to communicate a measure of epistemic (model) uncertainty in prediction. Unlike their deterministic counterparts, Bayesian DL (BDL) models are often well-calibrated, provide access to epistemic uncertainty for a prediction, and are capable of achieving competitive predictive performance. In this study, we propose variational inference pre-trained audio neural networks (VI-PANNs). VI-PANNs are a variational inference variant of the popular ResNet-54 architecture which are pre-trained on AudioSet, a large-scale audio event detection da
    
[^363]: T-PRIME: 基于Transformer的边缘机器学习协议识别

    T-PRIME: Transformer-based Protocol Identification for Machine-learning at the Edge. (arXiv:2401.04837v1 [cs.LG])

    [http://arxiv.org/abs/2401.04837](http://arxiv.org/abs/2401.04837)

    T-PRIME是一个基于Transformer的边缘机器学习协议识别方法，通过注意机制学习传输帧的结构设计，克服了传统方法的局限性。实验证明其在深度学习硬件限制下的实时可行性，并证明了其优于传统方法和最先进的神经网络。

    

    频谱共享允许相同标准（例如802.11系列）或不同标准（例如LTE和DVB）的不同协议在重叠的频段中共存。随着这种范式的推广，无线系统必须在故意损坏前导码、极低信噪比和挑战性的信道条件下实时识别活动发射器和未经授权的波形。通过设计T-PRIME：一种基于Transformer的机器学习方法，我们克服了在这种情况下基于相关性的前导码匹配方法的局限性。T-PRIME通过其注意机制学习传输帧的结构设计，查看超出前导码的序列模式。本文提出了三个贡献：首先，对比Transformer模型并证明其优于传统方法和最先进的神经网络。其次，严格分析了T-PRIME在深度学习硬件限制下的实时可行性。

    Spectrum sharing allows different protocols of the same standard (e.g., 802.11 family) or different standards (e.g., LTE and DVB) to coexist in overlapping frequency bands. As this paradigm continues to spread, wireless systems must also evolve to identify active transmitters and unauthorized waveforms in real time under intentional distortion of preambles, extremely low signal-to-noise ratios and challenging channel conditions. We overcome limitations of correlation-based preamble matching methods in such conditions through the design of T-PRIME: a Transformer-based machine learning approach. T-PRIME learns the structural design of transmitted frames through its attention mechanism, looking at sequence patterns that go beyond the preamble alone. The paper makes three contributions: First, it compares Transformer models and demonstrates their superiority over traditional methods and state-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's real-time feasibility on Deep
    
[^364]: 将图神经网络与分数阶连续动力学相结合：鲁棒性研究

    Coupling Graph Neural Networks with Fractional Order Continuous Dynamics: A Robustness Study. (arXiv:2401.04331v1 [cs.LG])

    [http://arxiv.org/abs/2401.04331](http://arxiv.org/abs/2401.04331)

    本文详细研究了图神经分数阶微分方程模型的鲁棒性，通过实施分数阶微积分，模型在特征更新过程中考虑了长期记忆，对抗性条件下的性能仍未得到广泛探究。

    

    本文严格研究了图神经分数阶微分方程(FDE)模型的鲁棒性。该框架通过实施分数阶Caputo导数，超越了传统的图神经整数阶常微分方程(ODE)模型。利用分数阶微积分，我们的模型在特征更新过程中考虑了长期记忆，与传统图神经ODE模型中的无记忆马尔可夫更新不同。图神经FDE模型相对于图神经ODE模型在没有攻击或扰动的环境中已经被证明具有优势。尽管传统的图神经ODE模型在现有文献中已经被验证在存在对抗性攻击时具有一定的稳定性和弹性，但图神经FDE模型的鲁棒性，特别是在对抗性条件下的表现，仍未得到广泛探究。本文对图神经FDE模型的鲁棒性进行了详细评估。

    In this work, we rigorously investigate the robustness of graph neural fractional-order differential equation (FDE) models. This framework extends beyond traditional graph neural (integer-order) ordinary differential equation (ODE) models by implementing the time-fractional Caputo derivative. Utilizing fractional calculus allows our model to consider long-term memory during the feature updating process, diverging from the memoryless Markovian updates seen in traditional graph neural ODE models. The superiority of graph neural FDE models over graph neural ODE models has been established in environments free from attacks or perturbations. While traditional graph neural ODE models have been verified to possess a degree of stability and resilience in the presence of adversarial attacks in existing literature, the robustness of graph neural FDE models, especially under adversarial conditions, remains largely unexplored. This paper undertakes a detailed assessment of the robustness of graph 
    
[^365]: 用贝叶斯方法统一自监督聚类和能量模型

    A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])

    [http://arxiv.org/abs/2401.00873](http://arxiv.org/abs/2401.00873)

    该论文研究了用贝叶斯方法统一自监督聚类和能量模型，提出了一种标准化的推导方法，并设计了一个新的可靠地惩罚失败模式的下界。这个下界使得能够训练一个标准的骨架架构，而无需使用非对称元素。

    

    自监督学习是一种利用大量无标签数据的流行且强大的方法，文献中提出了各种训练目标。本研究对最先进的自监督学习目标进行贝叶斯分析，阐明了每个类别中潜在的概率图模型，并提出了一种从基本原理出发推导这些模型的标准方法。分析还表明了将自监督学习与基于似然的生成模型自然整合的方法。我们在基于聚类的自监督学习和能量模型领域中实现了这个概念，引入了一个新的下界，经证明能可靠地惩罚最重要的失败模式。此外，这个新提出的下界使得能够训练一个标准的骨干架构，而无需使用诸如停止梯度、动量编码器或专门的聚类等非对称元素。

    Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives, elucidating the underlying probabilistic graphical models in each class and presenting a standardized methodology for their derivation from first principles. The analysis also indicates a natural means of integrating self-supervised learning with likelihood-based generative models. We instantiate this concept within the realm of cluster-based self-supervised learning and energy models, introducing a novel lower bound which is proven to reliably penalize the most important failure modes. Furthermore, this newly proposed lower bound enables the training of a standard backbone architecture without the necessity for asymmetric elements such as stop gradients, momentum encoders, or specialized clusteri
    
[^366]: 通过倾斜指数层改善稳健性：基于通信理论的视角

    Improving Robustness via Tilted Exponential Layer: A Communication-Theoretic Perspective. (arXiv:2311.01047v1 [cs.LG])

    [http://arxiv.org/abs/2311.01047](http://arxiv.org/abs/2311.01047)

    本论文提出了一种基于通信理论的方法，通过神经竞争来增强神经网络层输出的信噪比，从而提高深度网络的稳健性。

    

    提升深度网络稳健性的最新技术大多依赖于合适的数据增强的经验风险最小化。本文提出了一种基于通信理论的互补方法，旨在通过学习和推理中的神经竞争来增强神经网络层输出的信噪比。除了最小化标准的端到端代价外，神经元通过最大化倾斜指数（TEXP）层的目标函数来竞争以稀疏地表示层输入。TEXP学习可以被解释为在数据噪声的高斯模型下通过最大似然估计来匹配滤波器。在TEXP层中，通过使用倾斜的softmax替代批量归一化来进行推理，可以解释为计算每个神经元代表的竞争信号假设的后验概率。通过简化模型提供洞察，我们通过在标准图像上的实验表明，

    State-of-the-art techniques for enhancing robustness of deep networks mostly rely on empirical risk minimization with suitable data augmentation. In this paper, we propose a complementary approach motivated by communication theory, aimed at enhancing the signal-to-noise ratio at the output of a neural network layer via neural competition during learning and inference. In addition to minimization of a standard end-to-end cost, neurons compete to sparsely represent layer inputs by maximization of a tilted exponential (TEXP) objective function for the layer. TEXP learning can be interpreted as maximum likelihood estimation of matched filters under a Gaussian model for data noise. Inference in a TEXP layer is accomplished by replacing batch norm by a tilted softmax, which can be interpreted as computation of posterior probabilities for the competing signaling hypotheses represented by each neuron. After providing insights via simplified models, we show, by experimentation on standard image
    
[^367]: Hodge-Compositional 边缘高斯过程

    Hodge-Compositional Edge Gaussian Processes. (arXiv:2310.19450v1 [stat.ML])

    [http://arxiv.org/abs/2310.19450](http://arxiv.org/abs/2310.19450)

    本论文提出了一种新的方法用于对边缘集合上的函数进行建模，该方法基于Hodge分解开发了适用于不同应用场景的无散度和无旋度的高斯过程，并通过组合它们来表示任意边缘函数。实验结果表明这种方法在流动数据推断中具有潜在的实际应用价值。

    

    我们提出了一种基于边缘集合的2-复形结构（类似于图形，其中边缘可形成三角面）的函数建模的有原则的高斯过程（GPs）。这种方法适用于学习网络上的流动类型数据，其中边缘流可以通过离散的散度和旋度来表征。借鉴Hodge分解，我们首先开发了适用于各种应用的无散度和无旋游的边缘GPs。然后将它们组合起来创建Hodge-组合边缘GPs，这些GPs足够表达任何边缘函数。这些GPs便于对边缘函数的不同Hodge分量进行直接和独立的学习，使我们能够在超参数优化过程中捕捉它们的相关性。为了突显它们的实际潜力，我们将它们应用于货币兑换、海洋流动和供水网络中的流动数据推断，并将其与替代模型进行比较。

    We propose principled Gaussian processes (GPs) for modeling functions defined over the edge set of a simplicial 2-complex, a structure similar to a graph in which edges may form triangular faces. This approach is intended for learning flow-type data on networks where edge flows can be characterized by the discrete divergence and curl. Drawing upon the Hodge decomposition, we first develop classes of divergence-free and curl-free edge GPs, suitable for various applications. We then combine them to create \emph{Hodge-compositional edge GPs} that are expressive enough to represent any edge function. These GPs facilitate direct and independent learning for the different Hodge components of edge functions, enabling us to capture their relevance during hyperparameter optimization. To highlight their practical potential, we apply them for flow data inference in currency exchange, ocean flows and water supply networks, comparing them to alternative models.
    
[^368]: 基于图注意力的深度强化学习用于解决带有负载相关成本的中国邮递员问题

    Graph Attention-based Deep Reinforcement Learning for solving the Chinese Postman Problem with Load-dependent costs. (arXiv:2310.15516v1 [cs.LG])

    [http://arxiv.org/abs/2310.15516](http://arxiv.org/abs/2310.15516)

    本论文提出了一个基于图注意力的深度强化学习方法来解决带有负载相关成本的中国邮递员问题。该方法将问题形式化为马尔可夫决策过程，引入了一个编码器和解码器的自回归模型来有效处理问题。

    

    最近，深度强化学习（DRL）模型在解决路径规划问题方面展现了良好的结果。然而，大多数DRL求解器通常是用来解决节点路径规划问题，例如旅行推销员问题（TSP）。与此同时，关于应用神经方法来解决弧路径规划问题，例如中国邮递员问题（CPP），的研究却十分有限，因为与TSP相比，它们的解空间通常更加不规则和复杂。为了填补这些空白，本文提出了一个新的DRL框架，来解决带有负载相关成本（CPP-LC）的CPP问题，这是一个具有负载约束的复杂弧路径规划问题。我们方法的创新点有两个。首先，我们将CPP-LC问题形式化为马尔可夫决策过程（MDP）顺序模型。随后，我们引入了一种基于DRL的自回归模型，即Arc-DRL模型，它由一个编码器和一个解码器组成，可以有效处理CPP-LC问题。这样的框架使得DRL模型能够高效地工作。

    Recently, Deep reinforcement learning (DRL) models have shown promising results in solving routing problems. However, most DRL solvers are commonly proposed to solve node routing problems, such as the Traveling Salesman Problem (TSP). Meanwhile, there has been limited research on applying neural methods to arc routing problems, such as the Chinese Postman Problem (CPP), since they often feature irregular and complex solution spaces compared to TSP. To fill these gaps, this paper proposes a novel DRL framework to address the CPP with load-dependent costs (CPP-LC) (Corberan et al., 2018), which is a complex arc routing problem with load constraints. The novelty of our method is two-fold. First, we formulate the CPP-LC as a Markov Decision Process (MDP) sequential model. Subsequently, we introduce an autoregressive model based on DRL, namely Arc-DRL, consisting of an encoder and decoder to address the CPP-LC challenge effectively. Such a framework allows the DRL model to work efficiently 
    
[^369]: 从插值到外推：算术Transformer的完整长度泛化

    From Interpolation to Extrapolation: Complete Length Generalization for Arithmetic Transformers. (arXiv:2310.11984v1 [cs.LG])

    [http://arxiv.org/abs/2310.11984](http://arxiv.org/abs/2310.11984)

    本文研究了Transformer模型在学习算术算法方面的能力，并通过注意力偏置以及Attention Bias Calibration（ABC）来实现对于长长度的泛化。

    

    自从提出以来，Transformer模型在各种任务中展现出了优秀的性能。然而，在算法任务中，长度泛化仍存在一些未解决的问题。在本文中，我们研究了Transformer模型在学习算术算法（如加法和乘法）方面的内在能力。通过实验证明和注意力分析，我们确定了实现最佳长度泛化的几个关键因素。我们展示了Transformer模型能够通过目标指向偏置来泛化到长长度。然后，我们引入了Attention Bias Calibration（ABC），这是一个校准阶段，使模型能够自动学习适当的注意力偏置，我们将其与相对位置编码的机制联系起来。我们证明使用ABC，Transformer模型可以在某些算术任务上实现前所未有的完美长度泛化。

    Since its introduction, the transformer model has demonstrated outstanding performance across various tasks. However, there are still unresolved issues regarding length generalization, particularly in algorithmic tasks. In this paper, we investigate the inherent capabilities of transformer models in learning arithmetic algorithms, such as addition and multiplication. Through experiments and attention analysis, we identify a number of crucial factors for achieving optimal length generalization. We show that transformer models are able to generalize to long lengths with the help of targeted attention biasing. We then introduce Attention Bias Calibration (ABC), a calibration stage that enables the model to automatically learn the proper attention biases, which we link to mechanisms in relative position encoding. We demonstrate that using ABC, the transformer model can achieve unprecedented perfect length generalization on certain arithmetic tasks.
    
[^370]: 使用自然梯度替代品优化分布

    Optimising Distributions with Natural Gradient Surrogates. (arXiv:2310.11837v1 [stat.ML])

    [http://arxiv.org/abs/2310.11837](http://arxiv.org/abs/2310.11837)

    本研究提出了一种新的技术，通过重新定义优化过程为针对易于计算自然梯度的替代分布的参数优化来解决计算自然梯度的挑战。该方法能够扩展可应用自然梯度的分布范围，速度快且易于实现。

    

    自然梯度方法已经被用于优化各种情况下的概率分布参数，通常能得到快速收敛的过程。然而，对于许多感兴趣的分布，计算自然梯度存在一些挑战。在这项工作中，我们提出了一种新的技术来解决这些问题，这涉及将优化重新定义为关于替代分布参数的优化，计算自然梯度很容易。我们给出了几个可以解释为应用这种技术的现有方法的例子，并提出了一种新的方法，可以将其应用于各种问题。我们的方法扩展了可以有效使用自然梯度的分布集合。此外，它快速、易于理解，可以使用标准的自动微分软件进行简单实现，并且不需要冗长的模型特定导数计算。我们在最大似然估计和变分推断上演示了我们的方法。

    Natural gradient methods have been used to optimise the parameters of probability distributions in a variety of settings, often resulting in fast-converging procedures. Unfortunately, for many distributions of interest, computing the natural gradient has a number of challenges. In this work we propose a novel technique for tackling such issues, which involves reframing the optimisation as one with respect to the parameters of a surrogate distribution, for which computing the natural gradient is easy. We give several examples of existing methods that can be interpreted as applying this technique, and propose a new method for applying it to a wide variety of problems. Our method expands the set of distributions that can be efficiently targeted with natural gradients. Furthermore, it is fast, easy to understand, simple to implement using standard autodiff software, and does not require lengthy model-specific derivations. We demonstrate our method on maximum likelihood estimation and varia
    
[^371]: 无限时域平均奖励强化学习的量子加速

    Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])

    [http://arxiv.org/abs/2310.11684](http://arxiv.org/abs/2310.11684)

    本研究探索了无限时域平均奖励强化学习中量子加速的潜力。我们提出了一种创新的量子框架，通过高效的量子均值估计技术，实现了指数级改进的遗憾保证。所提出的量子算法相较于经典算法，在遗憾界限上有显著改进。

    

    本文研究量子加速在解决无限时域Markov决策过程（MDPs）中提高平均奖励结果的潜力。我们引入了一种创新的量子框架，用于代理与未知MDP的互动，扩展了传统的交互范式。我们的方法涉及设计一种基于乐观主导的具有量子信号的表格强化学习算法，通过高效的量子均值估计技术获取代理获取的量子信号。通过深入的理论分析，我们证明了量子均值估计的优势能够在无限时域强化学习中导致遗憾保证的指数进展。具体地，所提出的量子算法实现了一个遗憾界为$\tilde{\mathcal{O}}(1)$的性能，这是相对于经典对应算法所展示的$\tilde{\mathcal{O}}(\sqrt{T})$界限的显著改进。

    This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
    
[^372]: BayesDiff: 通过贝叶斯推断估计扩散中的像素级不确定性

    BayesDiff: Estimating Pixel-wise Uncertainty in Diffusion via Bayesian Inference. (arXiv:2310.11142v1 [cs.CV])

    [http://arxiv.org/abs/2310.11142](http://arxiv.org/abs/2310.11142)

    BayesDiff提出了一种像素级不确定性估计方法，用于扩散模型生成结果。该方法通过贝叶斯推断和不确定性迭代原则来实现高效的估计，并可在图像生成任务中过滤低质量图像，增强成功生成结果，并纠正失败生成结果中的伪影。

    

    扩散模型在图像生成方面表现出色，但仍存在质量较低的生成结果，并且由于缺乏适当的样本度量，对其进行鉴别仍然具有挑战性。为了解决这个问题，我们提出了BayesDiff，一种基于贝叶斯推断的用于扩散模型生成结果的像素级不确定性估计器。具体而言，我们提出了一种新的不确定性迭代原则来描述扩散中的不确定性动态，并利用最后一层的拉普拉斯近似来实现高效的贝叶斯推断。估计的像素级不确定性不仅可以聚合成样本级度量，以过滤出质量较低的图像，而且还可以在文本转图像任务中增强成功的生成结果并纠正失败的生成结果中的伪影。大量实验证明了BayesDiff的有效性及其在实际应用中的潜力。

    Diffusion models have impressive image generation capability, but low-quality generations still exist, and their identification remains challenging due to the lack of a proper sample-wise metric. To address this, we propose BayesDiff, a pixel-wise uncertainty estimator for generations from diffusion models based on Bayesian inference. In particular, we derive a novel uncertainty iteration principle to characterize the uncertainty dynamics in diffusion, and leverage the last-layer Laplace approximation for efficient Bayesian inference. The estimated pixel-wise uncertainty can not only be aggregated into a sample-wise metric to filter out low-fidelity images but also aids in augmenting successful generations and rectifying artifacts in failed generations in text-to-image tasks. Extensive experiments demonstrate the efficacy of BayesDiff and its promise for practical applications.
    
[^373]: 响铃！概念去除方法在扩散模型中的可靠性如何？

    Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion Models?. (arXiv:2310.10012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.10012](http://arxiv.org/abs/2310.10012)

    本文研究了对于文本到图像合成的扩散模型中的潜在滥用问题的安全措施的有效性，并提出了一个用于评估的新颖概念检索算法。我们引入了一个名为Ring-A-Bell的模型无关的红队工具，可以事先准备整个评估过程，而无需先验知识。

    

    文本到图像(T2I)合成的扩散模型，如稳定的扩散(SD)，最近展示出了生成高质量内容的卓越能力。然而，这一进展引发了对潜在滥用的几个关注，特别是在创建受版权限制、禁止和受限内容，或者不适宜工作的(NSFW)图片方面。虽然已经采取了一些措施来缓解这些问题，例如在评估阶段实施安全过滤器或通过微调模型来消除不受欢迎的概念或风格，但这些安全措施在处理各种提示方面的有效性仍然很少被探索。在这项工作中，我们旨在通过提出一种用于评估的新颖概念检索算法来研究这些安全机制。我们引入了Ring-A-Bell，这是一个面向T2I扩散模型的模型无关的红队工具，整个评估可以在没有目标模型先验知识的情况下提前准备好。具体来说，Ring-A-Bell首先对激励进行分解，然后通过去除候选概念和计算特定概念的相关度来设计筛选机制。

    Diffusion models for text-to-image (T2I) synthesis, such as Stable Diffusion (SD), have recently demonstrated exceptional capabilities for generating high-quality content. However, this progress has raised several concerns of potential misuse, particularly in creating copyrighted, prohibited, and restricted content, or NSFW (not safe for work) images. While efforts have been made to mitigate such problems, either by implementing a safety filter at the evaluation stage or by fine-tuning models to eliminate undesirable concepts or styles, the effectiveness of these safety measures in dealing with a wide range of prompts remains largely unexplored. In this work, we aim to investigate these safety mechanisms by proposing one novel concept retrieval algorithm for evaluation. We introduce Ring-A-Bell, a model-agnostic red-teaming tool for T2I diffusion models, where the whole evaluation can be prepared in advance without prior knowledge of the target model. Specifically, Ring-A-Bell first pe
    
[^374]: 深度神经网络分类器中潜在二进制编码的出现

    Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])

    [http://arxiv.org/abs/2310.08224](http://arxiv.org/abs/2310.08224)

    这篇论文观察到在深度神经网络分类器的潜在空间中出现了二进制编码，这种编码通过引入线性倒数第二层和指数增长的损失函数产生，并且加速了收敛和提高了分类准确率。

    

    我们观察到深度神经网络分类器的潜在空间中出现了二进制编码。通过引入一个线性倒数第二层，并在训练过程中配备一个损失函数，该函数随着潜在空间中坐标$\vec{x}$的平方指数增长，诱导出了二进制编码。我们描述的现象是已知的一种被称为"神经崩溃"的特殊情况，它在训练的最后阶段出现，并导致潜在类均值崩溃为简单等角紧框架（ETF）的顶点。我们展示了二进制编码加速了收敛到简单等角紧框架的过程，并提高了分类准确率。

    We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
    
[^375]: 带有噪声标签的局部图聚类

    Local Graph Clustering with Noisy Labels. (arXiv:2310.08031v1 [cs.LG])

    [http://arxiv.org/abs/2310.08031](http://arxiv.org/abs/2310.08031)

    本论文研究了使用噪声节点标签作为额外节点信息的局部图聚类方法，并探究了将噪声标签纳入局部图聚类的好处。

    

    在机器学习问题中，对于带有额外节点信息（如文本、图像或标签）的图形的增加兴趣，促使了需要耗费大量资源处理整个图形的方法的流行。然而，对于从这样的数据中提取有用信息的快速局部方法（即不需要访问整个图形）的发展还很少。为此，我们提出了使用噪声节点标签作为额外节点信息的局部图聚类的研究。在这种设置下，节点根据所属簇的联属关系接收初始二进制标签：如果它们属于目标簇，则为1；否则为0。随后，这些标签的一部分会被翻转。我们研究了将噪声标签纳入局部图聚类的好处。通过构建带有这些标签的加权图形，我们研究了基于图扩散的局部聚类方法在原始图形和加权图形上的性能。从理论角度出发，

    The growing interest in machine learning problems over graphs with additional node information such as texts, images, or labels has popularized methods that require the costly operation of processing the entire graph. Yet, little effort has been made to the development of fast local methods (i.e. without accessing the entire graph) that extract useful information from such data. To that end, we propose a study of local graph clustering using noisy node labels as a proxy for additional node information. In this setting, nodes receive initial binary labels based on cluster affiliation: 1 if they belong to the target cluster and 0 otherwise. Subsequently, a fraction of these labels is flipped. We investigate the benefits of incorporating noisy labels for local graph clustering. By constructing a weighted graph with such labels, we study the performance of graph diffusion-based local clustering method on both the original and the weighted graphs. From a theoretical perspective, we consider
    
[^376]: 揭示单切平面误区：在机器人学习中应用黎曼几何的分析和澄清

    Unraveling the Single Tangent Space Fallacy: An Analysis and Clarification for Applying Riemannian Geometry in Robot Learning. (arXiv:2310.07902v1 [cs.RO])

    [http://arxiv.org/abs/2310.07902](http://arxiv.org/abs/2310.07902)

    "Unraveling the Single Tangent Space Fallacy"论文分析和澄清了在机器人学习中应用黎曼几何的误区，该误区是指将数据仅投影到单一切空间中的方法。

    

    在机器人领域，许多后续的机器人任务利用机器学习方法来处理、建模或合成数据。这些数据通常包含固体方向表示四元数的单位范数条件或刚度和可操纵性椭球的正定性等几何约束。有效处理这样的几何约束需要将微分几何工具纳入机器学习方法的制定中。在这个背景下，黎曼流形成为处理这种几何约束的强大数学框架。然而，最近在机器人学习中对其采用过程中存在的一个数学上的缺陷化简现象，被称为“单切平面误区”。这种方法仅涉及将感兴趣的数据投影到一个单一切空间（欧几里得空间）上，然后使用现成的学习算法

    In the realm of robotics, numerous downstream robotics tasks leverage machine learning methods for processing, modeling, or synthesizing data. Often, this data comprises variables that inherently carry geometric constraints, such as the unit-norm condition of quaternions representing rigid-body orientations or the positive definiteness of stiffness and manipulability ellipsoids. Handling such geometric constraints effectively requires the incorporation of tools from differential geometry into the formulation of machine learning methods. In this context, Riemannian manifolds emerge as a powerful mathematical framework to handle such geometric constraints. Nevertheless, their recent adoption in robot learning has been largely characterized by a mathematically-flawed simplification, hereinafter referred to as the ``single tangent space fallacy". This approach involves merely projecting the data of interest onto a single tangent (Euclidean) space, over which an off-the-shelf learning algor
    
[^377]: CrIBo: 通过跨图像对象级引导进行自监督学习

    CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping. (arXiv:2310.07855v1 [cs.CV])

    [http://arxiv.org/abs/2310.07855](http://arxiv.org/abs/2310.07855)

    CrIBo通过跨图像对象级引导进行自监督学习，可以提高密集视觉表示学习的性能，并在自然理解应用中表现出领先的性能。

    

    在利用最近邻检索进行自监督表示学习时，以对象为中心的图像已被证明是有益的。然而，当应用于以场景为中心的数据集时，这种方法面临限制，在图像中的多个对象仅在全局表示中被隐含地捕获。这种全局引导可能导致对象表示的不可取的纠缠。此外，即使是以对象为中心的数据集，也可以受益于更细粒度的引导方法。为了应对这些挑战，我们引入了一种新的适用于增强密集视觉表示学习的跨图像对象级引导方法。通过在整个训练过程中采用对象级最近邻引导，CrIBo成为一个明显强大和适当的候选方案，用于上下文学习，在测试时利用最近邻检索。CrIBo在后一个任务上显示出最先进的性能，同时在更标准的下游自然理解应用中具有很高的竞争力。

    Leveraging nearest neighbor retrieval for self-supervised representation learning has proven beneficial with object-centric images. However, this approach faces limitations when applied to scene-centric datasets, where multiple objects within an image are only implicitly captured in the global representation. Such global bootstrapping can lead to undesirable entanglement of object representations. Furthermore, even object-centric datasets stand to benefit from a finer-grained bootstrapping approach. In response to these challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method tailored to enhance dense visual representation learning. By employing object-level nearest neighbor bootstrapping throughout the training, CrIBo emerges as a notably strong and adequate candidate for in-context learning, leveraging nearest neighbor retrieval at test time. CrIBo shows state-of-the-art performance on the latter task while being highly competitive in more standard downstream se
    
[^378]: 通过信息论分布多样化实现联邦泛化能力

    Federated Generalization via Information-Theoretic Distribution Diversification. (arXiv:2310.07171v1 [cs.LG])

    [http://arxiv.org/abs/2310.07171](http://arxiv.org/abs/2310.07171)

    该论文研究了联邦学习中泛化能力的挑战，特别关注训练分布和测试分布的不匹配。提出了一种信息论的泛化方法来解决这个问题。

    

    联邦学习（FL）因其在无需直接数据共享的情况下进行协同模型训练的能力而日益突出。然而，客户端之间本地数据分布的巨大差异，通常被称为非独立同分布（non-IID）挑战，对FL的泛化能力构成了重大障碍。当并非所有客户端都参与训练过程时，情况变得更加复杂，这是由于不稳定的网络连接或有限的计算能力而常见。这可能极大地复杂化了对训练模型的泛化能力的评估。尽管最近的大量研究集中在涉及具有不同分布的参与客户端的未见数据的泛化差距问题上，但参与客户端的训练分布和非参与客户端的测试分布之间的差异却被大部分忽视了。为此，我们的论文揭示了一种基于信息论的泛化方法。

    Federated Learning (FL) has surged in prominence due to its capability of collaborative model training without direct data sharing. However, the vast disparity in local data distributions among clients, often termed the non-Independent Identically Distributed (non-IID) challenge, poses a significant hurdle to FL's generalization efficacy. The scenario becomes even more complex when not all clients participate in the training process, a common occurrence due to unstable network connections or limited computational capacities. This can greatly complicate the assessment of the trained models' generalization abilities. While a plethora of recent studies has centered on the generalization gap pertaining to unseen data from participating clients with diverse distributions, the divergence between the training distributions of participating clients and the testing distributions of non-participating ones has been largely overlooked. In response, our paper unveils an information-theoretic genera
    
[^379]: 通过同时学习面部标志检测、域分离和重建来提高面部动作单位检测的精度

    Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.05207](http://arxiv.org/abs/2310.05207)

    本文提出了一种新的面部动作单位（AU）检测框架，通过共享参数和引入多任务学习，在面部标志检测和AU域分离与重建之间实现了更好的性能。实验证明我们方法在野外AU检测方面优于现有方法。

    

    最近，如何将大量的在野非标记面部图像引入监督式面部动作单位（AU）检测框架中成为一个具有挑战性的问题。本文提出了一种新的AU检测框架，通过共享同构面部提取模块的参数，引入多任务学习，同时学习AU域分离和重建以及面部标志检测。另外，我们提出了一种基于对比学习的新特征对齐方案，通过简单的投影器和改进的对比损失添加了四个额外的中间监督器来促进特征重建的过程。在两个基准测试上的实验结果表明，我们在野外AU检测方面优于现有的方法。

    Recently how to introduce large amounts of unlabeled facial images in the wild into supervised Facial Action Unit (AU) detection frameworks has become a challenging problem. In this paper, we propose a new AU detection framework where multi-task learning is introduced to jointly learn AU domain separation and reconstruction and facial landmark detection by sharing the parameters of homostructural facial extraction modules. In addition, we propose a new feature alignment scheme based on contrastive learning by simple projectors and an improved contrastive loss, which adds four additional intermediate supervisors to promote the feature reconstruction process. Experimental results on two benchmarks demonstrate our superiority against the state-of-the-art methods for AU detection in the wild.
    
[^380]: 用于相对位置的函数插值改进了长上下文Transformer

    Functional Interpolation for Relative Positions Improves Long Context Transformers. (arXiv:2310.04418v1 [cs.LG])

    [http://arxiv.org/abs/2310.04418](http://arxiv.org/abs/2310.04418)

    这项研究提出了一种名为FIRE的函数相对位置编码与渐进插值方法，通过改进Transformer对更长上下文的泛化能力，并在零射击语言建模和长文本基准测试中进行了实证验证。

    

    在扩展这些模型的上下文长度时，防止Transformer在训练以外更长输入上性能下降一直是一个重要的挑战。虽然Transformer架构在可处理的输入序列长度上基本没有限制，但在训练过程中使用的位置编码的选择可能会限制这些模型在更长输入上的性能。我们提出了一种新颖的函数相对位置编码与渐进插值方法（FIRE），以改进Transformer对更长上下文的泛化能力。我们从理论上证明了这可以表示出一些流行的相对位置编码，如T5的RPE、Alibi和Kerple。接下来，我们在零射击语言建模和长文本基准测试上经验性地展示了FIRE模型在更长上下文中具有更好的泛化能力。

    Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.
    
[^381]: 非光滑弱凸有限和耦合组合优化

    Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])

    [http://arxiv.org/abs/2310.03234](http://arxiv.org/abs/2310.03234)

    本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。

    

    本文研究了一类新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)。由于其在机器学习和人工智能领域的广泛应用以及其解决基于经验风险最小化的随机算法的局限性，FCCO引起了越来越多的关注。然而，目前对于FCCO的研究假设内外函数都是光滑的，限制了其能够解决更多种类的问题的潜力。我们的研究从非光滑弱凸FCCO的角度进行了扩展，其中外函数是弱凸且非递减的，内函数是弱凸的。我们分析了一种单循环算法，并确定其在找到Moreau环的ε-稳定点的复杂度。

    This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
    
[^382]: 使用级联扩散模型进行热带气旋预测

    Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.01690](http://arxiv.org/abs/2310.01690)

    本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。

    

    随着气候变化，飓风变得更加强烈，基于人工智能模型的预测方法比基于数学模型的传统方法更加经济实惠和易于获取。本研究利用扩散模型通过整合卫星成像、遥感和大气数据，采用级联方法进行飓风轨迹和降水模式的预测，训练数据集包括来自六个主要盆地的51个飓风。实验证明，级联模型的最终预测在36小时内显示准确的预测结果，所有三项任务的结构相似性指数（SSIM）和峰值信噪比（PSNR）的值都超过了0.5和20dB。本研究还强调了扩散模型等人工智能方法在高性能需求（如飓风预测）方面的高效性和计算经济性，使其成为高度脆弱地区的理想选择。

    As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
    
[^383]: 将大型语言模型推至6G边缘：视野、挑战和机遇

    Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])

    [http://arxiv.org/abs/2309.16739](http://arxiv.org/abs/2309.16739)

    本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。

    

    大型语言模型(LLMs)展示了显著的能力，正在改变人工智能的发展并有可能塑造我们的未来。然而，由于LLMs的多模态特性，当前的基于云的部署面临着一些关键挑战：1) 响应时间长；2) 高带宽成本；以及3) 违反数据隐私。6G移动边缘计算(MEC)系统可能解决这些迫切问题。本文探讨了在6G边缘部署LLMs的潜力。我们首先介绍了由多模态LLMs提供支持的关键应用，包括机器人技术和医疗保健，以突出在终端用户附近部署LLMs的需求。然后，我们确定了在边缘部署LLMs时面临的关键挑战，并设想了适用于LLMs的6G MEC架构。此外，我们深入探讨了两个设计方面，即LLMs的边缘训练和边缘推理。在这两个方面，考虑到边缘的固有资源限制，我们讨论了各种前沿技术。

    Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
    
[^384]: 对比度连续多视角聚类与过滤结构融合

    Contrastive Continual Multi-view Clustering with Filtered Structural Fusion. (arXiv:2309.15135v1 [cs.LG])

    [http://arxiv.org/abs/2309.15135](http://arxiv.org/abs/2309.15135)

    提出了一种名为对比度连续多视角聚类与过滤结构融合（CCMVC-FSF）的新方法，用于解决多视角聚类在实时数据收集中的困难。该方法旨在防止先前知识遗忘和利用数据相关性指导新视图的聚类过程。

    

    多视角聚类适用于先前收集视图并提取一致和互补信息的应用，但忽略了数据视图按顺序收集的实时数据的情况。为了解决这个问题，我们提出了一种名为对比度连续多视角聚类与过滤结构融合（CCMVC-FSF）的新方法，以应对先前知识遗忘和新视图聚类的问题。

    Multi-view clustering thrives in applications where views are collected in advance by extracting consistent and complementary information among views. However, it overlooks scenarios where data views are collected sequentially, i.e., real-time data. Due to privacy issues or memory burden, previous views are not available with time in these situations. Some methods are proposed to handle it but are trapped in a stability-plasticity dilemma. In specific, these methods undergo a catastrophic forgetting of prior knowledge when a new view is attained. Such a catastrophic forgetting problem (CFP) would cause the consistent and complementary information hard to get and affect the clustering performance. To tackle this, we propose a novel method termed Contrastive Continual Multi-view Clustering with Filtered Structural Fusion (CCMVC-FSF). Precisely, considering that data correlations play a vital role in clustering and prior knowledge ought to guide the clustering process of a new view, we de
    
[^385]: 像素级平滑用于对抗相机运动扰动的可证明鲁棒性

    Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations. (arXiv:2309.13150v1 [cs.LG])

    [http://arxiv.org/abs/2309.13150](http://arxiv.org/abs/2309.13150)

    本文提出了一种用于对抗相机运动扰动的像素级平滑框架，通过在二维像素空间中使用平滑分布来提高鲁棒性认证的效率，并完全上界投影误差。

    

    最近几年，计算机视觉在自动驾驶和机器人领域取得了显著的进展。然而，深度学习的视觉感知模型在面对相机运动扰动时缺乏鲁棒性。目前用于评估鲁棒性的认证过程耗时且昂贵，因为需要在三维相机运动空间中进行蒙特卡洛采样得到大量图像投影。为了应对这些挑战，我们提出了一种新颖、高效且实用的框架，用于证明3D-2D投影变换对抗相机运动扰动的鲁棒性。我们的方法在二维像素空间而非三维物理空间中使用平滑分布，消除了昂贵的相机运动采样，并大大提高了鲁棒性认证的效率。通过像素级平滑分类器，我们能够使用一种均匀分区的技术完全上界投影误差。

    In recent years, computer vision has made remarkable advancements in autonomous driving and robotics. However, it has been observed that deep learning-based visual perception models lack robustness when faced with camera motion perturbations. The current certification process for assessing robustness is costly and time-consuming due to the extensive number of image projections required for Monte Carlo sampling in the 3D camera motion space. To address these challenges, we present a novel, efficient, and practical framework for certifying the robustness of 3D-2D projective transformations against camera motion perturbations. Our approach leverages a smoothing distribution over the 2D pixel space instead of in the 3D physical space, eliminating the need for costly camera motion sampling and significantly enhancing the efficiency of robustness certifications. With the pixel-wise smoothed classifier, we are able to fully upper bound the projection errors using a technique of uniform partit
    
[^386]: 优化模块化机器人组合：一种词典遗传算法的方法

    Optimizing Modular Robot Composition: A Lexicographic Genetic Algorithm Approach. (arXiv:2309.08399v1 [cs.RO])

    [http://arxiv.org/abs/2309.08399](http://arxiv.org/abs/2309.08399)

    本论文提出了一种将遗传算法与词典式评估相结合的方法来优化模块化机器人的组合，以克服以往方法中存在的设计空间不足和适应复杂任务的问题，并证明了这种方法在比以往范围更大的搜索空间中表现出更好的性能。

    

    工业机器人被设计为通用硬件，这限制了它们适应任务需求或环境变化的能力。而模块化机器人则提供了灵活性，可以轻松定制以适应不同的需求。机器人的形态，即机器人的形式和结构，对主要性能指标--采购成本、周期时间和能源效率有着重要影响。然而，针对特定任务的最佳模块组合仍然是一个尚未解决的问题，在开发任务定制的模块化机器人中面临重大障碍。以往的方法要么无法充分探索设计空间，要么无法适应复杂任务。我们提出了将遗传算法与词典式评估解决方案候选的组合，以克服这个问题，并在可能组合的数量上比先前的工作范围更大的搜索空间中进行导航。我们证明了我们的方法优于最先进的技术。

    Industrial robots are designed as general-purpose hardware, which limits their ability to adapt to changing task requirements or environments. Modular robots, on the other hand, offer flexibility and can be easily customized to suit diverse needs. The morphology, i.e., the form and structure of a robot, significantly impacts the primary performance metrics acquisition cost, cycle time, and energy efficiency. However, identifying an optimal module composition for a specific task remains an open problem, presenting a substantial hurdle in developing task-tailored modular robots. Previous approaches either lack adequate exploration of the design space or the possibility to adapt to complex tasks. We propose combining a genetic algorithm with a lexicographic evaluation of solution candidates to overcome this problem and navigate search spaces exceeding those in prior work by magnitudes in the number of possible compositions. We demonstrate that our approach outperforms a state-of-the-art b
    
[^387]: $G$-Mapper：学习Mapper构造中的覆盖

    $G$-Mapper: Learning a Cover in the Mapper Construction. (arXiv:2309.06634v1 [cs.LG])

    [http://arxiv.org/abs/2309.06634](http://arxiv.org/abs/2309.06634)

    本论文介绍了一种基于统计检验和聚类算法的优化Mapper图覆盖的方法，通过分割覆盖选择生成了保留数据集本质的Mapper图。

    

    Mapper算法是拓扑数据分析(TDA)中一种反映给定数据集结构的可视化技术。Mapper算法需要调整多个参数以生成一个"好看的"Mapper图。该论文关注于选择覆盖参数。我们提出了一种通过根据正态性的统计检验反复分割覆盖来优化Mapper图的算法。我们的算法基于$G$-means聚类，通过迭代地进行Anderson-Darling检验来寻找$k$-means中最佳的簇数。我们的分割过程利用高斯混合模型，根据给定数据的分布精心选择覆盖。对于合成和真实数据集的实验表明，我们的算法生成的覆盖使Mapper图保留了数据集的本质。

    The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
    
[^388]: SA-Solver：用于快速采样扩散模型的随机亚当求解器

    SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])

    [http://arxiv.org/abs/2309.05019](http://arxiv.org/abs/2309.05019)

    本文提出了一种改进的高效随机亚当方法SA-Solver，用于解扩散随机微分方程以生成高质量的数据，实验结果显示它在少步采样中相较于现有最先进的方法有改进或可比的性能，并达到了SOTA FID分数。

    

    扩散概率模型在生成任务中取得了相当大的成功。由于从扩散概率模型中进行采样相当于解扩散随机微分方程或常微分方程，这是一项耗时的工作，因此提出了许多基于改进的微分方程求解器的快速采样方法。这些技术中的大部分方法都考虑解扩散常微分方程，因为它具有更好的效率。然而，随机采样可以在生成多样化和高质量数据方面提供额外的优势。在这项工作中，我们从两个方面进行了对随机采样的综合分析：方差控制的扩散随机微分方程和线性多步扩散随机微分方程求解器。基于我们的分析，我们提出了SA-Solver，它是一种改进的高效随机亚当方法，用于解扩散随机微分方程以生成高质量的数据。我们的实验结果显示，SA-Solver实现了：1）在少步采样中与现有最先进的采样方法相比，有改进或可比性能；2）SOTA FID分数。

    Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
    
[^389]: ArtiGrasp：双手灵巧抓握和关节表达的物理合理合成

    ArtiGrasp: Physically Plausible Synthesis of Bi-Manual Dexterous Grasping and Articulation. (arXiv:2309.03891v1 [cs.RO])

    [http://arxiv.org/abs/2309.03891](http://arxiv.org/abs/2309.03891)

    ArtiGrasp是一种通过强化学习和物理模拟的方式，用一个统一的策略来合成双手灵巧抓握和关节表达的方法。

    

    我们提出了ArtiGrasp，一种新的方法来合成包括抓握和关节表达在内的双手手-物体交互。由于全局手腕运动和精确的手指控制对于物体的关节表达是必要的，这个任务具有挑战性。ArtiGrasp利用强化学习和物理模拟训练一个控制全局和局部手姿态的策略。我们的框架在一个共同的手姿态参考下统一了抓握和关节表达。此外，为了训练关节表达所需的精确手指控制，我们提出了一个逐渐增加难度的学习课程。它从单手操作静止物体开始，然后进行包括两只手和非静止物体的多智能体训练。为了评估我们的方法，我们引入了动态物体抓握和关节表达，这是一个将物体移到目标关节姿态的任务。这个任务需要抓握，关节表达和物体姿态的控制。

    We present ArtiGrasp, a novel method to synthesize bi-manual hand-object interactions that include grasping and articulation. This task is challenging due to the diversity of the global wrist motions and the precise finger control that are necessary to articulate objects. ArtiGrasp leverages reinforcement learning and physics simulations to train a policy that controls the global and local hand pose. Our framework unifies grasping and articulation within a single policy guided by a single hand pose reference. Moreover, to facilitate the training of the precise finger control required for articulation, we present a learning curriculum with increasing difficulty. It starts with single-hand manipulation of stationary objects and continues with multi-agent training including both hands and non-stationary objects. To evaluate our method, we introduce Dynamic Object Grasping and Articulation, a task that involves bringing an object into a target articulated pose. This task requires grasping,
    
[^390]: 基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统

    A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])

    [http://arxiv.org/abs/2309.03720](http://arxiv.org/abs/2309.03720)

    本文介绍了一个基于Hoeffding树和变点检测机制的连续学习场景下的天然气消费预测系统，通过数据流处理，实现了多步 ahead 的预测和持续学习能力。在复杂的实际应用场景中，通过评估预测模型的性能，证明了该方法的有效性。

    

    在规划天然气供应和消费以及优化获得天然气成本方面，考虑季节性和趋势性的天然气消费预测至关重要。本文介绍了一种新颖的多步 ahead 的天然气消费预测方法，并集成了变点检测，以实现模型选择和持续学习能力。通过数据流处理，评估了基于该方法的天然气消费预测模型在复杂的实际应用场景中的性能。我们采用Hoeffding树预测器作为预测模型，并使用剪裁的精确线性时间（PELT）算法进行变点检测。变点检测集成使得选择不同的模型成为可能。

    Forecasting natural gas consumption, considering seasonality and trends, is crucial in planning its supply and consumption and optimizing the cost of obtaining it, mainly by industrial entities. However, in times of threats to its supply, it is also a critical element that guarantees the supply of this raw material to meet individual consumers' needs, ensuring society's energy security. This article introduces a novel multistep ahead forecasting of natural gas consumption with change point detection integration for model collection selection with continual learning capabilities using data stream processing. The performance of the forecasting models based on the proposed approach is evaluated in a complex real-world use case of natural gas consumption forecasting. We employed Hoeffding tree predictors as forecasting models and the Pruned Exact Linear Time (PELT) algorithm for the change point detection procedure. The change point detection integration enables selecting a different model
    
[^391]: SortedNet，每个网络都有自己的位置：面向训练多对一神经网络的广义解决方案

    SortedNet, a Place for Every Network and Every Network in its Place: Towards a Generalized Solution for Training Many-in-One Neural Networks. (arXiv:2309.00255v1 [cs.LG])

    [http://arxiv.org/abs/2309.00255](http://arxiv.org/abs/2309.00255)

    SortedNet是一种广义解决方案，通过排序训练和概率方式，在深度神经网络的各个维度上实现高效动态推断。这种方法允许在模型推断过程中灵活适应计算负载，并且可以将子网络的数量扩展到数百个。

    

    随着深度学习模型的规模不断增大，如何在内存和计算约束下找到最优模型变得越来越重要。虽然神经网络的架构和组成部分通常允许以模块化的方式使用，但它们的训练过程并不意识到这种模块化。因此，传统的神经网络训练缺乏在推断过程中适应模型计算负载的灵活性。本文提出了SortedNet，这是一种广义且可扩展的解决方案，用于利用深度神经网络在各个维度上的内在模块化特性，实现高效的动态推断。我们的训练方法采用了一种嵌套结构的子模型和主模型共享参数的方式，并以排序和概率的方式训练它们。这种子网络的排序训练使我们能够在一轮训练中将子网络的数量扩展到数百个。我们利用一种新颖的更新方案在推断过程中动态调整子网络的计算负载。

    As the size of deep learning models continues to grow, finding optimal models under memory and computation constraints becomes increasingly more important. Although usually the architecture and constituent building blocks of neural networks allow them to be used in a modular way, their training process is not aware of this modularity. Consequently, conventional neural network training lacks the flexibility to adapt the computational load of the model during inference. This paper proposes SortedNet, a generalized and scalable solution to harness the inherent modularity of deep neural networks across various dimensions for efficient dynamic inference. Our training considers a nested architecture for the sub-models with shared parameters and trains them together with the main model in a sorted and probabilistic manner. This sorted training of sub-networks enables us to scale the number of sub-networks to hundreds using a single round of training. We utilize a novel updating scheme during 
    
[^392]: 通过独立分量拉普拉斯过程实现差分隐私的函数性摘要

    Differentially Private Functional Summaries via the Independent Component Laplace Process. (arXiv:2309.00125v1 [stat.ML])

    [http://arxiv.org/abs/2309.00125](http://arxiv.org/abs/2309.00125)

    本论文提出了一种新的差分隐私函数性摘要机制，通过使用独立分量拉普拉斯过程对无限维的函数性摘要进行扰动，放宽了对数据轨迹的假设，并相对于传统的有限维子空间嵌入方法保留了更高的效用。实验证明了该机制的可行性和有效性。

    

    在这项工作中，我们提出了一种称为独立分量拉普拉斯过程（ICLP）机制的差分隐私函数性摘要的新机制。通过将感兴趣的函数性摘要视为真正无限维对象，并使用ICLP噪声来扰动它们，该新机制放宽了关于数据轨迹的假设，并相对于文献中的经典有限维子空间嵌入方法保留了更高的效用。我们在多个函数空间中验证了所提出机制的可行性。我们考虑了几个统计估计问题，并通过轻微过平滑摘要来证明隐私成本不会主导统计误差，并且在渐近情况下可以忽略。对合成和真实数据集的数值实验证明了所提出机制的有效性。

    In this work, we propose a new mechanism for releasing differentially private functional summaries called the Independent Component Laplace Process, or ICLP, mechanism. By treating the functional summaries of interest as truly infinite-dimensional objects and perturbing them with the ICLP noise, this new mechanism relaxes assumptions on data trajectories and preserves higher utility compared to classical finite-dimensional subspace embedding approaches in the literature. We establish the feasibility of the proposed mechanism in multiple function spaces. Several statistical estimation problems are considered, and we demonstrate by slightly over-smoothing the summary, the privacy cost will not dominate the statistical error and is asymptotically negligible. Numerical experiments on synthetic and real datasets demonstrate the efficacy of the proposed mechanism.
    
[^393]: 人脸图像的神经隐式形变

    Neural Implicit Morphing of Face Images. (arXiv:2308.13888v1 [cs.CV])

    [http://arxiv.org/abs/2308.13888](http://arxiv.org/abs/2308.13888)

    本论文提出了一种利用神经网络实现人脸图像变形和混合的方法，通过利用网络的平滑性和灵活性，结合经典方法中的能量函数，实现了高效、准确和多样化的人脸变形效果。

    

    人脸变形是计算机图形学中的一个重要问题，具有众多艺术和取证应用。由于姿态、光照、性别和种族的变化，它一直以来都是一个具有挑战性的问题。通常，这个任务包括特征对齐的变形和无缝过渡的混合。我们提出利用基于坐标的神经网络来表示人脸图像的这种变形和混合。在训练过程中，我们利用这种网络的平滑性和灵活性，结合了经典方法中使用的能量函数，而无需进行离散化。此外，我们的方法是时间依赖的，允许对目标图像进行连续的变形和混合。在变形推理过程中，我们需要时间依赖变形的直接和逆变换。前者负责将目标图像变形为源图像，而后者则用于在相反方向进行变形。我们的神经变形网络具有高效、准确和多样化的性能。

    Face morphing is one of the seminal problems in computer graphics, with numerous artistic and forensic applications. It is notoriously challenging due to pose, lighting, gender, and ethnicity variations. Generally, this task consists of a warping for feature alignment and a blending for a seamless transition between the warped images.  We propose to leverage coordinate-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks, by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping, and blending of the target images.  During warping inference, we need both direct and inverse transformations of the time-dependent warping. The first is responsible for morphing the target image into the source image, while the inverse is used for morphing in the opposite direction. Our neural warping sto
    
[^394]: KinSPEAK: 通过半监督学习方法提高基尼亚兰达语语音识别的准确性

    KinSPEAK: Improving speech recognition for Kinyarwanda via semi-supervised learning methods. (arXiv:2308.11863v1 [eess.AS])

    [http://arxiv.org/abs/2308.11863](http://arxiv.org/abs/2308.11863)

    通过自监督预训练、课程进度微调和半监督学习利用无标签语音数据，该论文提出了一种改善基尼亚兰达语音识别的方法，实现了最先进的结果。

    

    尽管最近具备了大规模记录的基尼亚兰达语语音数据，但是实现基尼亚兰达语的强大语音识别仍然具有挑战性。本研究表明，使用自监督预训练，遵循简单的课程进度进行微调，以及使用半监督学习来利用大规模无标签语音数据，显著提高了基尼亚兰达语的语音识别性能。我们的方法仅关注使用公共领域数据。我们从公共网站收集了一个新的制作室级别的语音数据集，然后使用该数据集训练一个干净的基准模型。然后，使用该干净的基准模型对来自更多多样和嘈杂的公共数据集的样本进行排序，定义一个简单的课程训练进度。最后，我们将半监督学习应用于连续四代对大规模无标签数据进行标记和学习。我们的最终模型在新数据集上实现了3.2％的字错误率（WER），在Mozilla Common Voice基准测试中实现了15.9％的WER，达到了最先进的水平。

    Despite recent availability of large transcribed Kinyarwanda speech data, achieving robust speech recognition for Kinyarwanda is still challenging. In this work, we show that using self-supervised pre-training, following a simple curriculum schedule during fine-tuning and using semi-supervised learning to leverage large unlabelled speech data significantly improve speech recognition performance for Kinyarwanda. Our approach focuses on using public domain data only. A new studio-quality speech dataset is collected from a public website, then used to train a clean baseline model. The clean baseline model is then used to rank examples from a more diverse and noisy public dataset, defining a simple curriculum training schedule. Finally, we apply semi-supervised learning to label and learn from large unlabelled data in four successive generations. Our final model achieves 3.2% word error rate (WER) on the new dataset and 15.9% WER on Mozilla Common Voice benchmark, which is state-of-the-art
    
[^395]: MindMap：知识图谱激发大型语言模型的思维图思考方法

    MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])

    [http://arxiv.org/abs/2308.09729](http://arxiv.org/abs/2308.09729)

    本论文通过使用知识图谱来激发大型语言模型，解决了整合新知识、产生幻觉和决策过程不透明等问题，并通过生成思维导图展示了模型的推理路径，实验证明这种方法可以取得显著的实证增益。

    

    通常，大型语言模型存在无法整合新知识、产生幻觉和决策过程不透明等限制。本文探讨了如何利用知识图谱（KG）来激发大型语言模型，以解决整合最新知识和引发模型思维路径的问题。具体来说，我们构建了一个提示管道，使大型语言模型能够理解KG输入并利用隐含知识和检索到的外部知识进行推理。此外，我们研究了引发大型语言模型执行推理和生成答案的思维导图。研究发现，生成的思维导图基于知识的本体论，展示了大型语言模型的推理路径，从而为生产环境中的推理提供了探索和评估的可能性。对三个问答数据集的实验证明，MindMap提示方法带来了显著的实证增益。

    LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question & answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
    
[^396]: 非紧致统一逼近

    Noncompact uniform universal approximation. (arXiv:2308.03812v1 [cs.LG])

    [http://arxiv.org/abs/2308.03812](http://arxiv.org/abs/2308.03812)

    这篇论文将通用逼近定理推广到非紧致输入空间，并确定了在有界激活函数条件下可以通过神经网络一致逼近的函数类别，并提出了代数结构的意外结果。

    

    将通用逼近定理推广到在（非紧致）输入空间 \(\mathbb R^n\) 上的一致收敛。所有在无穷远处为零的连续函数都可以用具有一个隐藏层的神经网络进行一致逼近，对于所有具有渐近线性行为的连续激活函数 \(\varphi\neq0\)。当 \(\varphi\) 还被限制在有界时，我们准确确定了哪些函数可以通过神经网络进行一致逼近，得到了以下意想不到的结果。让 \(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\) 表示可以通过具有 \(l\) 个隐藏层和 \(n\) 个输入的神经网络进行一致逼近的函数的向量空间。对于所有的 \(n\) 和所有的 \(l\geq2\)，\(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\) 在逐点乘积下是一个代数。如果 \(\varphi\) 的左极限不等于其右极限（例如，当 \(\varphi\) 是sigmoid函数时），代数 \(\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}\)（\(l\geq2\)）会产生一些意外的结果。

    The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space $\mathbb R^n$. All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all continuous activation functions $\varphi\neq0$ with asymptotically linear behaviour at $\pm\infty$. When $\varphi$ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ denote the vector space of functions that are uniformly approximable by neural networks with $l$ hidden layers and $n$ inputs. For all $n$ and all $l\geq2$, $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ turns out to be an algebra under the pointwise product. If the left limit of $\varphi$ differs from its right limit (for instance, when $\varphi$ is sigmoidal) the algebra $\overline{\mathcal{N}_\varphi^l(\mathbb R^n)}$ ($l\geq2$) 
    
[^397]: 解决在线强化学习的样本复杂度问题

    Settling the Sample Complexity of Online Reinforcement Learning. (arXiv:2307.13586v1 [cs.LG])

    [http://arxiv.org/abs/2307.13586](http://arxiv.org/abs/2307.13586)

    本文解决了在线强化学习的样本复杂度问题，提出了一种基于模型的算法，它可以在有限时间不均匀马尔可夫决策问题中实现极小后悔的最优性。

    

    在线强化学习的一个核心问题是数据效率。虽然最近的一些工作在在线强化学习中实现了渐近最小的后悔，但这些结果的最优性仅在“大样本”情况下得到保证，为了使其算法运行最佳，需要付出巨大的预燃成本。如何在不产生任何预燃成本的情况下实现极小后悔的最优性一直是强化学习理论中的一个开放问题。我们解决了有限时间不均匀马尔可夫决策问题的这个问题。具体地，我们证明了一种修改版的单调值传播(MVP)算法，该算法是由\cite{zhang2020reinforcement}提出的一种基于模型的算法，使得后悔的量级为(模除对数因子)\begin{equation *} \min\biggr\{ \sqrt{SAH^3K}，\，HK \biggr\}，\end{equation *}其中$S$是状态数，$A$是动作数，$H$是规划时域，$K$是总的回合数。这个后悔的量级与极小化后悔量级是相匹配的。

    A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.  We settle this problem for the context of finite-horizon inhomogeneous Markov decision processes. Specifically, we prove that a modified version of Monotonic Value Propagation (MVP), a model-based algorithm proposed by \cite{zhang2020reinforcement}, achieves a regret on the order of (modulo log factors) \begin{equation*}  \min\big\{ \sqrt{SAH^3K}, \,HK \big\}, \end{equation*} where $S$ is the number of states, $A$ is the number of actions, $H$ is the planning horizon, and $K$ is the total number of episodes. This regret matches the minimax 
    
[^398]: 用物理信知的神经网络解决维度诅咒问题

    Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])

    [http://arxiv.org/abs/2307.12306](http://arxiv.org/abs/2307.12306)

    本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。

    

    维度诅咒(CoD)随着维度的增加，以指数级增长的计算成本来极度税费计算资源。这在解决高维偏微分方程(PDEs)中面临极大挑战，正如Richard Bellman在60年前首次指出的那样。尽管近年来在高维度上数值解决偏微分方程(PDEs)取得了一些成功，但这样的计算代价过高，而将一般非线性PDEs扩展到高维度从未实现过。本文提出了一种新方法，将物理信知的神经网络(PINNs)扩展到解决任意高维PDEs。该新方法称为随机维度梯度下降(SDGD)，将PDE的梯度分解为与不同维度对应的部分，并在训练PINNs的每次迭代中随机选择这些维度部分的子集进行采样。我们在理论上证明了所提出方法的收敛保证和其他期望属性。

    The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
    
[^399]: 一种新颖的多通道彩色图像去噪的截断范数正则化方法

    A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising. (arXiv:2307.07932v1 [eess.IV])

    [http://arxiv.org/abs/2307.07932](http://arxiv.org/abs/2307.07932)

    一种新颖的多通道彩色图像去噪方法，通过双加权截断核范数减去截断Frobenius范数最小化，利用非局部自相似性来提取相似的结构进行去噪，模型兼顾了跨通道差异和噪声的空间变化。

    

    鉴于低秩逼近方法具有高度的灵活性和显著的性能，在彩色图像去噪方面得到了广泛的研究。然而，这些方法大多忽略了跨通道差异或噪声的空间变化，这限制了它们在实际彩色图像去噪中的能力。为了克服这些缺点，本文提出了一种双加权截断核范数减去截断Frobenius范数最小化（DtNFM）的方法来去噪彩色图像。通过利用噪声图像的非局部自相似性，相似结构被收集并构造了一系列相似的块矩阵。对于每个分组，对其进行DtNFM模型来估计其去噪版本。通过连接所有去噪块矩阵，得到去噪图像。这种提出的DtNFM模型有两个优点。首先，它对跨通道差异和噪声的空间变化进行了建模和利用。这提供了足够的灵活性。

    Due to the high flexibility and remarkable performance, low-rank approximation methods has been widely studied for color image denoising. However, those methods mostly ignore either the cross-channel difference or the spatial variation of noise, which limits their capacity in real world color image denoising. To overcome those drawbacks, this paper is proposed to denoise color images with a double-weighted truncated nuclear norm minus truncated Frobenius norm minimization (DtNFM) method. Through exploiting the nonlocal self-similarity of the noisy image, the similar structures are gathered and a series of similar patch matrices are constructed. For each group, the DtNFM model is conducted for estimating its denoised version. The denoised image would be obtained by concatenating all the denoised patch matrices. The proposed DtNFM model has two merits. First, it models and utilizes both the cross-channel difference and the spatial variation of noise. This provides sufficient flexibility 
    
[^400]: One-Versus-Others Attention: 可扩展的多模态集成

    One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])

    [http://arxiv.org/abs/2307.05435](http://arxiv.org/abs/2307.05435)

    提出了一种可扩展的多模态集成方法，通过一对多（OvO）注意力机制解决了多模态学习中超过三个模态的注意力计算问题。

    

    随着多模态学习模型在问题回答和自动驾驶等各种任务上超越单模态方法，多模态学习模型变得日益重要。尽管多模态学习的重要性，现有的工作仅关注于自然语言处理应用，其中模态数通常少于四个（音频、视频、文本、图像）。然而，在其他领域，如医疗领域，数据输入可能包括X射线、PET扫描、MRI、遗传筛查、临床笔记等，这就需要高效而准确的信息融合。许多最先进的模型依赖于两两跨模态注意力，但对于超过三个模态的应用，这种方法不会很好地扩展。对于$n$个模态，计算注意力将导致$n \choose 2$的复杂度，可能需要大量的计算资源。为了解决这个问题，我们提出了一种新的领域中立的注意力机制，即一对多（OvO）注意力，该机制随着模态数量线性扩展。

    Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with
    
[^401]: 当拒绝学习对具有拒绝的回归问题最优时

    When No-Rejection Learning is Optimal for Regression with Rejection. (arXiv:2307.02932v1 [cs.LG])

    [http://arxiv.org/abs/2307.02932](http://arxiv.org/abs/2307.02932)

    本文研究了具有拒绝的回归问题，并调查了将其视为标准回归任务来学习预测器的无拒绝学习策略。

    

    拒绝学习是研究人类和人工智能在预测任务上相互作用的典型模型。该模型包括一个预测器和一个拒绝器。在样本到达时，拒绝器首先决定是否接受它；如果接受，预测器完成预测任务；如果被拒绝，则将预测推迟给人类。学习问题需要同时学习预测器和拒绝器。这改变了传统损失函数的结构，通常导致非凸性和一致性问题。对于带有拒绝的分类问题，一些研究开发了代理损失函数，同时具有可验证的一致性保证；与此同时，关于回归问题的研究较少。我们研究了带有拒绝的回归问题并研究了将其视为标准回归任务来学习预测器的无拒绝学习策略。

    Learning with rejection is a prototypical model for studying the interaction between humans and AI on prediction tasks. The model has two components, a predictor and a rejector. Upon the arrival of a sample, the rejector first decides whether to accept it; if accepted, the predictor fulfills the prediction task, and if rejected, the prediction will be deferred to humans. The learning problem requires learning a predictor and a rejector simultaneously. This changes the structure of the conventional loss function and often results in non-convexity and inconsistency issues. For the classification with rejection problem, several works develop surrogate losses for the jointly learning with provable consistency guarantees; in parallel, there has been less work for the regression counterpart. We study the regression with rejection (RwR) problem and investigate the no-rejection learning strategy which treats the RwR problem as a standard regression task to learn the predictor. We establish tha
    
[^402]: 用张量回归进行少样本个性化显著性预测，保留结构全局信息。

    Few-Shot Personalized Saliency Prediction Using Tensor Regression for Preserving Structural Global Information. (arXiv:2307.02799v1 [eess.IV])

    [http://arxiv.org/abs/2307.02799](http://arxiv.org/abs/2307.02799)

    本文提出了一种使用张量回归进行少样本个性化显著性预测的方法，以保留个性化显著性图的结构全局信息。

    

    本文提出了一种使用张量到矩阵回归进行少样本个性化显著性预测的方法，以保留个性化显著性图（PSM）的结构全局信息。与一般的显著性图相比，PSM具有巨大的潜力，因为它的映射指示了个体特定的视觉注意力，对于从凝视区域的异质性中获取个体视觉偏好非常有用。PSM的预测是为了获取未见图像的PSM，但由于个体凝视模式的复杂性，其预测仍然是一项具有挑战性的任务。为了从有限的眼动数据中识别个体凝视模式，先前的方法采用个体之间凝视趋势的相似性。然而，在先前的方法中，PSMs被向量化以适应预测模型，从而忽视了与图像对应的PSMs的结构全局信息。为了自动揭示PSMs之间的关系，我们聚焦于...

    This paper presents a few-shot personalized saliency prediction using tensor-to-matrix regression for preserving the structural global information of personalized saliency maps (PSMs). In contrast to a general saliency map, a PSM has been great potential since its map indicates the person-specific visual attention that is useful for obtaining individual visual preferences from heterogeneity of gazed areas. The PSM prediction is needed for acquiring the PSM for the unseen image, but its prediction is still a challenging task due to the complexity of individual gaze patterns. For recognizing individual gaze patterns from the limited amount of eye-tracking data, the previous methods adopt the similarity of gaze tendency between persons. However, in the previous methods, the PSMs are vectorized for the prediction model. In this way, the structural global information of the PSMs corresponding to the image is ignored. For automatically revealing the relationship between PSMs, we focus on the
    
[^403]: 使用基于图形平滑的Gibbs采样优化蛋白质适应性。

    Optimizing protein fitness using Gibbs sampling with Graph-based Smoothing. (arXiv:2307.00494v1 [q-bio.BM])

    [http://arxiv.org/abs/2307.00494](http://arxiv.org/abs/2307.00494)

    使用基于图形平滑的Gibbs采样方法（GGS）优化蛋白质适应性，消除了突变距离的限制，同时提高了搜索效率。该方法在发现高适应性蛋白质方面达到了最先进水平。

    

    能够设计出在给定任务上具有更高适应性的新型蛋白质对许多医学领域来说都是革命性的。然而，通过穷举搜索海量序列空间是不可行的。以前的方法将搜索限制在从参考序列的小突变半径范围内，但这样的启发式方法极大地限制了设计空间。我们的工作旨在消除突变距离的限制，同时实现高效的探索。我们提出了基于图形平滑的Gibbs采样（GGS），它通过迭代应用带有梯度的Gibbs来提出有利的突变，并使用基于图形平滑的方法去除导致假阳性的噪声梯度。我们的方法在训练集中发现了高适应性蛋白质，最多具有8个突变。我们通过研究GFP和AAV设计问题、消融试验和基准模型来阐明结果。

    The ability to design novel proteins with higher fitness on a given task would be revolutionary for many fields of medicine. However, brute-force search through the combinatorially large space of sequences is infeasible. Prior methods constrain search to a small mutational radius from a reference sequence, but such heuristics drastically limit the design space. Our work seeks to remove the restriction on mutational distance while enabling efficient exploration. We propose Gibbs sampling with Graph-based Smoothing (GGS) which iteratively applies Gibbs with gradients to propose advantageous mutations using graph-based smoothing to remove noisy gradients that lead to false positives. Our method is state-of-the-art in discovering high-fitness proteins with up to 8 mutations from the training set. We study the GFP and AAV design problems, ablations, and baselines to elucidate the results. Code: https://github.com/kirjner/GGS
    
[^404]: 运算学习中的维度诅咒

    The curse of dimensionality in operator learning. (arXiv:2306.15924v1 [cs.LG])

    [http://arxiv.org/abs/2306.15924](http://arxiv.org/abs/2306.15924)

    算子学习中存在维度诅咒，但对于由Hamilton-Jacobi方程定义的解算子可以克服维度诅咒。

    

    神经算子架构利用神经网络来近似映射函数空间之间的算子，可以用于通过模拟加速模型评估，或者从数据中发现模型。因此，这一方法在近年来受到越来越多的关注，引发了算子学习领域的快速发展。本文的第一项贡献是证明了对于一般的只由其 $C^r$ 或 Lipschitz 正则性特征化的算子类，算子学习遭受了维度诅咒，这里通过无穷维输入和输出函数空间的表征来精确定义维度诅咒。该结果适用于包括 PCA-Net、DeepONet 和 FNO 在内的多种现有神经算子。本文的第二项贡献是证明了对于由Hamilton-Jacobi方程定义的解算子，可以克服一般的维度诅咒；这是通过引入新的表示方法来实现的。

    Neural operator architectures employ neural networks to approximate operators mapping between Banach spaces of functions; they may be used to accelerate model evaluations via emulation, or to discover models from data. Consequently, the methodology has received increasing attention over recent years, giving rise to the rapidly growing field of operator learning. The first contribution of this paper is to prove that for general classes of operators which are characterized only by their $C^r$- or Lipschitz-regularity, operator learning suffers from a curse of dimensionality, defined precisely here in terms of representations of the infinite-dimensional input and output function spaces. The result is applicable to a wide variety of existing neural operators, including PCA-Net, DeepONet and the FNO. The second contribution of the paper is to prove that the general curse of dimensionality can be overcome for solution operators defined by the Hamilton-Jacobi equation; this is achieved by lev
    
[^405]: 具有局部可变测量尺度的随机变量的鲁棒统计比较

    Robust Statistical Comparison of Random Variables with Locally Varying Scale of Measurement. (arXiv:2306.12803v1 [stat.ML])

    [http://arxiv.org/abs/2306.12803](http://arxiv.org/abs/2306.12803)

    本研究提出了对于具有局部可变测量尺度的随机变量的广义随机优势（GSD）顺序，并通过线性优化和不精确概率模型提出了正则化的统计检验，解决了在这些非标准空间中如何正确利用全部信息来进行比较的问题。

    

    具有局部可变测量尺度的空间，在统计学和机器学习中是相当普遍的，比如说，具有不同缩放维度的多维结构。然而，如何正确地利用这些空间中编码的全部信息，仍然被认为是一个开放性问题。我们通过考虑一个基于随机变量期望的（集合）偏序关系来解决这个问题，这些随机变量映射到这些非标准空间中。当没有或完全的基数结构时，这个偏序关系包含随机优势和期望顺序作为极端情况。我们通过线性优化导出了一个适用于我们提出的广义随机优势（GSD）顺序的（正则化的）统计检验，并通过不精确概率模型使其更为鲁棒。我们的发现用多维贫困度量、金融和医学数据进行说明。

    Spaces with locally varying scale of measurement, like multidimensional structures with differently scaled dimensions, are pretty common in statistics and machine learning. Nevertheless, it is still understood as an open question how to exploit the entire information encoded in them properly. We address this problem by considering an order based on (sets of) expectations of random variables mapping into such non-standard spaces. This order contains stochastic dominance and expectation order as extreme cases when no, or respectively perfect, cardinal structure is given. We derive a (regularized) statistical test for our proposed generalized stochastic dominance (GSD) order, operationalize it by linear optimization, and robustify it by imprecise probability models. Our findings are illustrated with data from multidimensional poverty measurement, finance, and medicine.
    
[^406]: LabelBench：基于综合框架的标签高效学习基准评估

    LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])

    [http://arxiv.org/abs/2306.09910](http://arxiv.org/abs/2306.09910)

    本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。

    

    标记数据是现代机器学习应用程序的关键，但获取标记可能很昂贵。为了减缓这一成本，机器学习方法（如迁移学习、半监督学习和主动学习）旨在实现标签高效性：从相对较少的标记示例中实现高预测性能。虽然在实践中获得最佳的标签效率通常需要这些技术的组合，但现有的基准评估框架并没有捕捉到所有这些技术的协同组合。本文通过引入LabelBench解决了这个缺陷，这是一个新的计算效率高的综合性框架，用于联合评估多个标签高效学习技术。作为LabelBench的一个应用，我们引入了一种与半监督学习一起使用的最新主动学习方法的新基准，用于微调预训练的视觉转换器。我们的基准证明了比先前报告的更好的标签效率。

    Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
    
[^407]: DCTX-Conformer：针对低延迟统一流式和非流式Conformer的动态上下文传递

    DCTX-Conformer: Dynamic context carry-over for low latency unified streaming and non-streaming Conformer. (arXiv:2306.08175v1 [eess.AS])

    [http://arxiv.org/abs/2306.08175](http://arxiv.org/abs/2306.08175)

    提出了一种基于Conformer的新型动态上下文传递机制DCTX-Conformer，解决了流式识别性能差距的问题，相比于现有最优解，识别结果的误差率提高了25%，但对延迟影响可以忽略不计。

    

    基于Conformer的端到端模型现在已经变得普遍，被广泛用于流式和非流式自动语音识别（ASR）中。诸如双模式和动态分块训练等技术有助于统一流式和非流式系统。然而，在完整和有限的过去上下文的情况下，流式识别之间仍然存在着性能差距。为了解决这个问题，我们提出了一种新颖的动态上下文传递机制，将其与现有的最先进的统一ASR系统进行集成。我们的提议——动态上下文Conformer（DCTX-Conformer）利用了一个非重叠的上下文传递机制，同时考虑了一块的左上下文和一个或多个先前的上下文嵌入。由于额外的上下文嵌入，我们相对于目前最优解提升了25.0%的词错误率，而延迟影响可以忽略不计。

    Conformer-based end-to-end models have become ubiquitous these days and are commonly used in both streaming and non-streaming automatic speech recognition (ASR). Techniques like dual-mode and dynamic chunk training helped unify streaming and non-streaming systems. However, there remains a performance gap between streaming with a full and limited past context. To address this issue, we propose the integration of a novel dynamic contextual carry-over mechanism in a state-of-the-art (SOTA) unified ASR system. Our proposed dynamic context Conformer (DCTX-Conformer) utilizes a non-overlapping contextual carry-over mechanism that takes into account both the left context of a chunk and one or more preceding context embeddings. We outperform the SOTA by a relative 25.0% word error rate, with a negligible latency impact due to the additional context embeddings.
    
[^408]: Mol-Instructions: 一个大规模生物分子指令数据集，为大语言模型提供支持

    Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models. (arXiv:2306.08018v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.08018](http://arxiv.org/abs/2306.08018)

    Mol-Instructions是一个专门为生物分子领域设计的综合指令数据集，可以显著提高大语言模型在生物领域中的适应能力和认知敏锐度。

    

    大语言模型（LLM）以其卓越的任务处理能力和创新的输出，在许多领域推动了重大进展。然而，它们在生物分子研究等专业领域的熟练应用还受到限制。为了解决这个挑战，我们介绍了Mol-Instructions，这是一个经过精心策划、专门针对生物分子领域设计的综合指令数据集。Mol-Instructions由三个关键组成部分组成：分子导向指令、蛋白质导向指令和生物分子文本指令，每个部分都被策划用于增强LLM对生物分子特性和行为的理解和预测能力。通过对代表性LLM的广泛指令调整实验，我们强调了Mol-Instructions在增强大模型在生物分子研究复杂领域内的适应能力和认知敏锐度方面的潜力，从而促进生物分子领域的进一步发展。

    Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a meticulously curated, comprehensive instruction dataset expressly designed for the biomolecular realm. Mol-Instructions is composed of three pivotal components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions, each curated to enhance the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on the representative LLM, we underscore the potency of Mol-Instructions to enhance the adaptability and cognitive acuity of large models within the complex sphere of biomolecular studies, thereby promoting advancements in the biomol
    
[^409]: SGD中的投石机：训练损失中的尖峰及其通过特征学习对泛化的影响

    Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning. (arXiv:2306.04815v1 [cs.LG])

    [http://arxiv.org/abs/2306.04815](http://arxiv.org/abs/2306.04815)

    本文通过研究SGD训练损失中的尖峰现象，提出了“投石机”优化现象，通过增加与真实预测器的平均梯度外积对齐来促进特征学习，并证明较小的批量大小可提高泛化性能。

    

    本文首先解释了神经网络在使用随机梯度下降（SGD）进行训练时为什么经常出现训练损失尖峰的现象。我们提供了证据表明，SGD训练损失中的尖峰是“投石机”，这是一种优化现象，最初在[Lewkowycz等人，2020年]的大学习率GD中观察到。我们通过实验证明这些投石机出现在由正切内核的前几个特征向量所张成的低维子空间中，适用于GD和SGD。其次，我们提出了一个解释，即投石机如何通过增加与真实预测器的平均梯度外积（AGOP）对齐来促进特征学习，从而实现更好的泛化。此外，我们证明，在SGD中，更小的批量大小会导致更多的投石机出现，从而提高AGOP对齐和测试性能。

    In this paper, we first present an explanation regarding the common occurrence of spikes in the training loss when neural networks are trained with stochastic gradient descent (SGD). We provide evidence that the spikes in the training loss of SGD are "catapults", an optimization phenomenon originally observed in GD with large learning rates in [Lewkowycz et al. 2020]. We empirically show that these catapults occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel, for both GD and SGD. Second, we posit an explanation for how catapults lead to better generalization by demonstrating that catapults promote feature learning by increasing alignment with the Average Gradient Outer Product (AGOP) of the true predictor. Furthermore, we demonstrate that a smaller batch size in SGD induces a larger number of catapults, thereby improving AGOP alignment and test performance.
    
[^410]: 使用反事实预测集设计决策支持系统

    Designing Decision Support Systems Using Counterfactual Prediction Sets. (arXiv:2306.03928v1 [cs.LG])

    [http://arxiv.org/abs/2306.03928](http://arxiv.org/abs/2306.03928)

    本文提出了一种基于反事实预测集的决策支持系统设计方法，不同于传统的单一标签预测，它使用符合预测器构建预测集，并引导人类专家从中选择标签值。

    

    分类任务的决策支持系统通常被设计用于预测地面实况标签的值。然而，由于它们的预测并不完美，这些系统还需要让人类专家了解何时以及如何使用这些预测来更新自己的预测。不幸的是，这被证明是具有挑战性的。最近有人认为，另一种类型的决策支持系统可能会避开这个挑战。这些系统不是提供单个标签预测，而是使用符合预测器构建一组标签预测值，即预测集，并强制要求专家从预测集中预测一个标签值。然而，这些系统的设计和评估迄今仍依赖于样式化的专家模型，这引发了人们对它们的承诺的质疑。本文从在线学习的角度重新审视了这种系统的设计，并开发了一种不需要。

    Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not requi
    
[^411]: Transformers中多头注意力的记忆容量

    Memorization Capacity of Multi-Head Attention in Transformers. (arXiv:2306.02010v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02010](http://arxiv.org/abs/2306.02010)

    本文研究了多头注意力机制的记忆能力，发现在特定的假设下，注意力层可以记忆Ω(Hn)个示例序列，这对于理解transformers的记忆容量有重要意义。

    

    Transformers已成为语言和视觉任务的首选架构，但其理论属性，特别是记忆容量，仍然难以捉摸。本文研究了多头注意力机制的记忆能力，研究了它们能够记忆多少个示例序列，作为头数和序列长度的函数。在对视觉transformers的实验结果的启发下，我们引入了关于输入数据线性独立性的新假设，不同于通常使用的一般位置假设。在这些假设下，我们证明了具有H个头，维度d，上下文大小n < d的注意力层，具有Θ(Hd^2)个参数，可以记住Ω(Hn)个示例。我们的分析揭示了不同的注意力头如何处理不同的示例序列，受到softmax运算符的饱和特性的帮助。我们通过合成数据的实验验证了我们的发现。

    Transformers have become the go-to architecture for language and vision tasks, yet their theoretical properties, especially memorization capacity, remain elusive. This paper investigates the memorization abilities of multi-head attention mechanisms, examining how many example sequences they can memorize, as a function of the number of heads and sequence length. Motivated by experimental findings on vision transformers, we introduce novel assumptions about the linear independence of input data, distinct from the commonly used general-position assumption. Under these assumptions, we demonstrate that an attention layer with $H$ heads, dimension $d$, and context size $n < d$, featuring $\Theta(Hd^2)$ parameters, can memorize $\Omega(Hn)$ examples. Our analysis sheds light on how different attention heads handle various example sequences, aided by the softmax operator's saturation property. We validate our findings through experiments on synthetic data.
    
[^412]: 通过非负低秩半定规划实现最优K均值聚类

    Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])

    [http://arxiv.org/abs/2305.18436](http://arxiv.org/abs/2305.18436)

    本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。

    

    K均值聚类是一种广泛应用于大数据集中发现模式的机器学习方法。半定规划（SDP）松弛最近被提出用于解决K均值优化问题，具有很强的统计最优性保证。但实现SDP求解器的巨大成本使得这些保证无法应用于实际数据集。相比之下，非负矩阵分解（NMF）是一种简单的聚类算法，被机器学习从业者广泛使用，但缺乏坚实的统计基础或严格的保证。在本文中，我们描述了一种类似于NMF的算法，通过使用非凸Burer-Monteiro分解方法解决半定规划松弛的K均值公式的非负低秩限制。所得到的算法与最先进的NMF算法一样简单和可扩展，同时也享有与SDP相同的强大的统计最优性保证。在我们的实验中，我们证明了我们的算法优于现有的NMF算法，并在合成和实际数据集上表现与最先进的SDP求解器相当。

    $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
    
[^413]: 多目标遗传算法用于多视角特征选择

    Multi-Objective Genetic Algorithm for Multi-View Feature Selection. (arXiv:2305.18352v1 [cs.NE])

    [http://arxiv.org/abs/2305.18352](http://arxiv.org/abs/2305.18352)

    多视角数据提高了预测模型的准确性，但也使得高维数据增加，影响模型泛化能力。研究者提出了一种多视角多目标特征选择遗传算法（MMFS-GA），用于从多视角数据中选择最优的特征子集以提高模型精度和可解释性。

    

    多视角数据集提供了不同形式的数据，可以通过提供补充信息来增强预测模型。但是，使用多视角数据会导致高维数据的增加，这对可以导致泛化能力差的预测模型带来显著挑战。因此，从多视角数据集中选择相关特征不仅可以解决不良的泛化能力，还可以增强模型的可解释性。尽管传统特征选择方法取得了成功，但它们在利用跨模态的内在信息、缺乏泛化性和适用于特定分类任务方面存在局限性。我们提出了一种新的遗传算法策略，以克服传统特征选择方法在多视角数据上的这些局限性。我们提出的方法称为多视角多目标特征选择遗传算法（MMFS-GA）。这种方法同时选择最优的特征子集。

    Multi-view datasets offer diverse forms of data that can enhance prediction models by providing complementary information. However, the use of multi-view data leads to an increase in high-dimensional data, which poses significant challenges for the prediction models that can lead to poor generalization. Therefore, relevant feature selection from multi-view datasets is important as it not only addresses the poor generalization but also enhances the interpretability of the models. Despite the success of traditional feature selection methods, they have limitations in leveraging intrinsic information across modalities, lacking generalizability, and being tailored to specific classification tasks. We propose a novel genetic algorithm strategy to overcome these limitations of traditional feature selection methods for multi-view data. Our proposed approach, called the multi-view multi-objective feature selection genetic algorithm (MMFS-GA), simultaneously selects the optimal subset of feature
    
[^414]: 关于神经网络作为无限树状概率图模型的论文研究

    On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])

    [http://arxiv.org/abs/2305.17583](http://arxiv.org/abs/2305.17583)

    本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决深度神经网络(DNNs)缺乏PGMs的精确语义和明确定义的概率解释的问题。研究发现DNNs在前向传播时确实执行PGM推断的近似，这与现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似，潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    

    深度神经网络(DNNs)缺乏概率图模型(PGMs)的精确语义和明确定义的概率解释。本文提出了一种创新方法，通过构建与神经网络完全对应的无限树状PGMs来解决这个问题。我们的研究揭示了DNNs在前向传播期间确实执行PGM推断的近似，这与曾经的神经网络描述为核机器或无限大小的高斯过程的现有研究不同，它阐明了DNNs对PGMs中的精确推理的更直接近似。潜在的好处包括改进DNNs的教学和解释，以及能够合并PGMs和DNNs的算法。

    Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
    
[^415]: 从未知奖励的演示中学习安全约束

    Learning Safety Constraints from Demonstrations with Unknown Rewards. (arXiv:2305.16147v1 [cs.LG])

    [http://arxiv.org/abs/2305.16147](http://arxiv.org/abs/2305.16147)

    CoCoRL是一种从不知道奖励的已知安全演示中推断约束的方法，可以用于Constrained Markov Decision Process（CMDP），并且对于几乎最优演示能够无误差收敛于真实的安全集。

    

    本文提出了一种新方法，Convex Constraint Learning for Reinforcement Learning (CoCoRL)，用于从一组已知安全演示中推断Constrained Markov Decision Process (CMDP)的共享约束。与以前的方法只限于已知奖励或完全已知环境动态的演示相比，CoCoRL可以从具有不同未知奖励的演示中学习约束，而无需了解环境动态。CoCoRL基于演示构建了一个凸安全集，即使是潜在的次优演示也能保证安全。对于几乎最优演示，CoCoRL能够无误差收敛于真实的安全集。我们在表格环境和一个包含多个约束的连续驾驶仿真中评估CoCoRL。CoCoRL学习到的限制导致了安全的驾驶行为，并可以转移到不同的任务和环境。与之相反，基于学习已知回报的替代方法无法推广到具有不同回报的新环境，突显了CoCoRL在不知道回报函数的情况下学习约束的重要性。

    We propose Convex Constraint Learning for Reinforcement Learning (CoCoRL), a novel approach for inferring shared constraints in a Constrained Markov Decision Process (CMDP) from a set of safe demonstrations with possibly different reward functions. While previous work is limited to demonstrations with known rewards or fully known environment dynamics, CoCoRL can learn constraints from demonstrations with different unknown rewards without knowledge of the environment dynamics. CoCoRL constructs a convex safe set based on demonstrations, which provably guarantees safety even for potentially sub-optimal (but safe) demonstrations. For near-optimal demonstrations, CoCoRL converges to the true safe set with no policy regret. We evaluate CoCoRL in tabular environments and a continuous driving simulation with multiple constraints. CoCoRL learns constraints that lead to safe driving behavior and that can be transferred to different tasks and environments. In contrast, alternative methods based 
    
[^416]: 使用神经薛定谔桥实现非配对图像转换

    Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])

    [http://arxiv.org/abs/2305.15086](http://arxiv.org/abs/2305.15086)

    本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。

    

    扩散模型是一类生成模型，它通过模拟随机微分方程（SDE）从噪声生成数据。尽管扩散模型在最近取得了显著进展，但由于高斯先验假设，它们在非配对的图像转换任务中存在局限性。薛定谔桥是一种学习 SDE 以在两个任意分布之间转换的方法，被视为解决这个问题的一种有吸引力的解决方案。然而，迄今为止，薛定谔桥模型在高分辨率图像之间的非配对转换方面并不成功。在这项工作中，我们提出了非配对神经薛定谔桥（UNSB），它将薛定谔桥与对抗性训练和正则化相结合，以学习非配对数据之间的 SDE。我们证明了 UNSB 是可伸缩的，并且成功解决了各种非配对图像转换任务。

    Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
    
[^417]: 用基于深度学习原理的方法进行地质岩相生成

    A principled deep learning approach for geological facies generation. (arXiv:2305.13318v1 [physics.geo-ph])

    [http://arxiv.org/abs/2305.13318](http://arxiv.org/abs/2305.13318)

    本研究使用基于深度学习原理的生成对抗网络和深度变分推理应用于地质岩相生成，针对地下渠道进行了有条件模拟，并且比传统地质统计模型具有更高水平的准确性和物理逼真性。

    

    在各种地球科学应用中，模拟不可观测体积中的地质相是至关重要的。考虑到该问题的复杂性，深度生成学习是克服传统地质统计模型局限性（特别是缺乏物理逼真性）的一种有前途的方法。本研究旨在探索对生成对抗网络和深度变分推理进行应用，以便有条件地对地下渠道进行模拟。本文回顾了生成深度学习方法，特别是对抗性方法和旨在促进其训练的稳定化技术。本文提出的方法在以随机过程为基础的Flumy模型生成的二维和三维模拟上进行了测试。利用形态学指标比较我们提出的方法与以前的对抗生成网络迭代的结果。结果表明，通过利用最近的稳定技术，生成对抗网络可以成功地应用于地质岩相生成，并表现出比传统地质统计模型更高水平的准确性和物理逼真性。

    The simulation of geological facies in an unobservable volume is essential in various geoscience applications. Given the complexity of the problem, deep generative learning is a promising approach to overcome the limitations of traditional geostatistical simulation models, in particular their lack of physical realism. This research aims to investigate the application of generative adversarial networks and deep variational inference for conditionally simulating meandering channels in underground volumes. In this paper, we review the generative deep learning approaches, in particular the adversarial ones and the stabilization techniques that aim to facilitate their training. The proposed approach is tested on 2D and 3D simulations generated by the stochastic process-based model Flumy. Morphological metrics are utilized to compare our proposed method with earlier iterations of generative adversarial networks. The results indicate that by utilizing recent stabilization techniques, generati
    
[^418]: 深度时间图聚类

    Deep Temporal Graph Clustering. (arXiv:2305.10738v1 [cs.LG])

    [http://arxiv.org/abs/2305.10738](http://arxiv.org/abs/2305.10738)

    提出通用框架TGC 用于 deep temporal graph clustering, 解决了时间图只能作为静态图处理的难题，实现了对动态信息的聚类。实验证明了 TGC 的优越性。

    

    最近深度图聚类已经引起了很多关注，因为它可以增强模型在无监督场景下的表示学习能力。然而，适用于时间图的深度聚类方法 - 可以捕获关键的动态交互信息，并没有得到充分的探索。这意味着在许多面向聚类的现实场景中，时间图只能作为静态图来处理。这不仅导致了动态信息的丢失，也引发了巨大的计算消耗。为了解决这个问题，我们提出了一个名为TGC的通用框架，用于时间图深度聚类，它调整了深度聚类技术（聚类分配分布和邻接矩阵重构），以适应时间图基于交互序列的批处理模式。此外，我们还从几个方面讨论了时间图聚类与现有静态图聚类的差异。实验证明了TGC的卓越性能。

    Deep graph clustering has recently received significant attention due to its ability to enhance the representation learning capabilities of models in unsupervised scenarios. Nevertheless, deep clustering for temporal graphs, which could capture crucial dynamic interaction information, has not been fully explored. It means that in many clustering-oriented real-world scenarios, temporal graphs can only be processed as static graphs. This not only causes the loss of dynamic information but also triggers huge computational consumption. To solve the problem, we propose a general framework for deep Temporal Graph Clustering called TGC, which adjusts deep clustering techniques (clustering assignment distribution and adjacency matrix reconstruction) to suit the interaction sequence-based batch-processing pattern of temporal graphs. In addition, we discuss differences between temporal graph clustering and existing static graph clustering from several levels. To verify the superiority of the pro
    
[^419]: 一种用于智能按需公共交通预测公交车到达时间的新型神经网络方法

    A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit. (arXiv:2303.15495v1 [cs.LG])

    [http://arxiv.org/abs/2303.15495](http://arxiv.org/abs/2303.15495)

    本文介绍了一种基于神经网络的数据驱动方法，可以跨所有公交线路集体预测公交车到达每个交通点的时间，解决公交运输中公交车到达时间不准确和可靠的问题。

    

    在城市的主要公共交通系统中，公交运输存在着问题，其中包括对于乘客公交车到达时间的估计更加准确和可靠。这可能导致延误和减少乘客人数，尤其是在依靠公共交通的城市中更加严重。公交车到达时间与时间表不匹配是一个普遍的问题，导致固定时刻表的延迟。根据本文在纽约市公交数据上进行的研究，公交车到达时间和实际计划时间之间存在平均约八分钟或491秒的延迟。本研究提出了一种基于人工智能的数据驱动方法，用于估计每个交通点（站）公交车的到达时间。我们的方法基于全连接神经网络，可以在大都市区域中跨所有公交线路集体预测到达时间。我们的神经网络数据驱动方法为估算公交车到达时间提供了一种新的方式。

    Among the major public transportation systems in cities, bus transit has its problems, including more accuracy and reliability when estimating the bus arrival time for riders. This can lead to delays and decreased ridership, especially in cities where public transportation is heavily relied upon. A common issue is that the arrival times of buses do not match the schedules, resulting in latency for fixed schedules. According to the study in this paper on New York City bus data, there is an average delay of around eight minutes or 491 seconds mismatch between the bus arrivals and the actual scheduled time. This research paper presents a novel AI-based data-driven approach for estimating the arrival times of buses at each transit point (station). Our approach is based on a fully connected neural network and can predict the arrival time collectively across all bus lines in large metropolitan areas. Our neural-net data-driven approach provides a new way to estimate the arrival time of the b
    
[^420]: 带有Fisher线性判别分析的近似最优领域自适应

    Approximately optimal domain adaptation with Fisher's Linear Discriminant Analysis. (arXiv:2302.14186v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2302.14186](http://arxiv.org/abs/2302.14186)

    本文提出了一种基于Fisher线性判别的领域自适应模型，该模型是两个假设的凸组合，可以在不访问任何单个源任务的直接信息的情况下计算最优分类器，并在基于EEG和ECG的分类设置中展示了其有效性。

    

    我们提出了一类基于Fisher线性判别（FLD）的模型，用于领域自适应。该类模型是两个假设的凸组合：i）代表先前看到的源任务的平均假设和ii）在新的目标任务上训练的假设。对于特定的生成设置，我们在0-1损失下导出了两种模型的最优凸组合，提出了一种可计算的逼近，并研究了各种参数设置对最优假设、假设i）和假设ii）之间相对风险的影响。我们展示了所提出的最优分类器在基于EEG和ECG的分类设置中的有效性，并认为可以在不访问任何单个源任务的直接信息的情况下计算最优分类器。最后我们讨论了进一步的应用、限制和可能的未来方向。

    We propose a class of models based on Fisher's Linear Discriminant (FLD) in the context of domain adaptation. The class is the convex combination of two hypotheses: i) an average hypothesis representing previously seen source tasks and ii) a hypothesis trained on a new target task. For a particular generative setting we derive the optimal convex combination of the two models under 0-1 loss, propose a computable approximation, and study the effect of various parameter settings on the relative risks between the optimal hypothesis, hypothesis i), and hypothesis ii). We demonstrate the effectiveness of the proposed optimal classifier in the context of EEG- and ECG-based classification settings and argue that the optimal classifier can be computed without access to direct information from any of the individual source tasks. We conclude by discussing further applications, limitations, and possible future directions.
    
[^421]: 双价值网络在逆向合成规划中的应用

    Retrosynthetic Planning with Dual Value Networks. (arXiv:2301.13755v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.13755](http://arxiv.org/abs/2301.13755)

    PDVN是一种新的在线训练算法，它在逆向合成规划中利用双价值网络优化完整的路线，成功率和效率上均优于现有方法。

    

    逆向合成旨在从商业上可得的起始材料中找到合成目标分子的路线，是药物发现和材料设计中的关键任务。最近，基于机器学习的单步反应预测器与多步规划器的组合已经取得了令人鼓舞的结果。然而，单步预测器大多数情况下是离线训练的，只优化单步的准确性，而不考虑完整的路线。本文提出了一种新的在线训练算法PDVN，通过使用树形MDP来优化完整的路线，利用强化学习（RL）改善单步预测器。在PDVN中，我们构建了两个单独的价值网络，分别预测分子的可合成性和成本。为了保持单步预测器的准确性，我们设计了一个双分支网络结构。

    Retrosynthesis, which aims to find a route to synthesize a target molecule from commercially available starting materials, is a critical task in drug discovery and materials design. Recently, the combination of ML-based single-step reaction predictors with multi-step planners has led to promising results. However, the single-step predictors are mostly trained offline to optimize the single-step accuracy, without considering complete routes. Here, we leverage reinforcement learning (RL) to improve the single-step predictor, by using a tree-shaped MDP to optimize complete routes. Specifically, we propose a novel online training algorithm, called Planning with Dual Value Networks (PDVN), which alternates between the planning phase and updating phase. In PDVN, we construct two separate value networks to predict the synthesizability and cost of molecules, respectively. To maintain the single-step accuracy, we design a two-branch network structure for the single-step predictor. On the widely
    
[^422]: 论几何图神经网络表现力的研究

    On the Expressive Power of Geometric Graph Neural Networks. (arXiv:2301.09308v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09308](http://arxiv.org/abs/2301.09308)

    本文提出了几何版本的Weisfeiler-Leman测试(GWL)，可以区分几何图形，揭示了关键设计选择如何影响几何GNN的表现力

    

    通过 Weisfeiler-Leman (WL) 图同构测试，已经广泛研究了图神经网络 (GNNs) 的表现力。然而，标准的 GNNs 和 WL 框架不适用于嵌入欧几里得空间的几何图形，例如生物分子、材料和其他物理系统。在本文中，我们提出了 WL 测试的几何版本 (GWL)，以区分几何图形，同时尊重底层物理对称性：排列、旋转、反射和平移。我们使用 GWL 来表征具有不变或等变于物理对称性的几何 GNN 的表现力，以区分几何图形。GWL 揭示了关键设计选择如何影响几何 GNN 的表现力：(1) 不变层表现力有限，因为它们无法区分一跳相同的几何图形；(2) 等变层通过传播局部邻域之外的几何信息，区分更大类别的图形；(3)

    The expressive power of Graph Neural Networks (GNNs) has been studied extensively through the Weisfeiler-Leman (WL) graph isomorphism test. However, standard GNNs and the WL framework are inapplicable for geometric graphs embedded in Euclidean space, such as biomolecules, materials, and other physical systems. In this work, we propose a geometric version of the WL test (GWL) for discriminating geometric graphs while respecting the underlying physical symmetries: permutations, rotation, reflection, and translation. We use GWL to characterise the expressive power of geometric GNNs that are invariant or equivariant to physical symmetries in terms of distinguishing geometric graphs. GWL unpacks how key design choices influence geometric GNN expressivity: (1) Invariant layers have limited expressivity as they cannot distinguish one-hop identical geometric graphs; (2) Equivariant layers distinguish a larger class of graphs by propagating geometric information beyond local neighbourhoods; (3)
    
[^423]: FedTracker：为联邦学习模型提供所有权验证和可追溯性的保护机制

    FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model. (arXiv:2211.07160v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2211.07160](http://arxiv.org/abs/2211.07160)

    FedTracker是第一个为联邦学习模型提供所有权验证和追溯性的保护框架，采用双层保护方案，并利用持续学习原则提高保护性能。

    

    联邦学习（FL）是一种分布式机器学习范式，允许多个客户端协同训练一个全局模型，而无需共享他们的本地数据。然而，FL需要将模型暴露给各种参与者，这可能导致恶意客户端未经授权地分发或转售模型，从而损害FL团队的知识产权。为了阻止这种不当行为，建立一种验证模型所有权并追溯泄露者的机制至关重要。本文提出了FedTracker，这是第一个提供所有权验证和可追溯性的FL模型保护框架。FedTracker采用双层保护方案，包括全局水印机制和本地指纹机制。前者用于验证全局模型的所有权，而后者用于识别该模型来自哪个客户端。FedTracker利用持续学习（CL）原则来提高模型的保护性能。

    Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) princ
    
[^424]: 这是一个有限时间随机线性二次控制问题的策略梯度方法的收敛性研究

    Convergence of policy gradient methods for finite-horizon stochastic linear-quadratic control problems. (arXiv:2211.00617v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2211.00617](http://arxiv.org/abs/2211.00617)

    本文研究了有限时间随机线性二次控制问题中策略梯度方法的全局线性收敛性，并提出了基于Fisher几何和Bures-Wasserstein几何的几何感知梯度下降算法，该算法以线性速率全局收敛到最优策略。

    

    本文研究了有限时间连续时间探索性线性二次控制问题中策略梯度方法的全局线性收敛性。该设置包括具有非确定性成本和允许目标中添加额外的熵正则化项的随机线性二次控制问题。我们考虑了一个连续时间的高斯策略，其均值是状态变量的线性函数，方差与状态无关。与离散时间问题相反，策略中的成本函数不是严格凸函数，并且并非所有的下降方向都导致有界的迭代。我们提出了基于Fisher几何和Bures-Wasserstein几何的策略均值和协方差的几何感知梯度下降算法。策略迭代被证明满足先验界，并以线性速率全局收敛到最优策略。我们进一步提出了一种新的具有离散时间策略的PG方法。该算法利用了连续时间的分析结果，并实现了鲁棒的线性收敛。

    We study the global linear convergence of policy gradient (PG) methods for finite-horizon continuous-time exploratory linear-quadratic control (LQC) problems. The setting includes stochastic LQC problems with indefinite costs and allows additional entropy regularisers in the objective. We consider a continuous-time Gaussian policy whose mean is linear in the state variable and whose covariance is state-independent. Contrary to discrete-time problems, the cost is noncoercive in the policy and not all descent directions lead to bounded iterates. We propose geometry-aware gradient descents for the mean and covariance of the policy using the Fisher geometry and the Bures-Wasserstein geometry, respectively. The policy iterates are shown to satisfy an a-priori bound, and converge globally to the optimal policy with a linear rate. We further propose a novel PG method with discrete-time policies. The algorithm leverages the continuous-time analysis, and achieves a robust linear convergence acr
    
[^425]: AMPNet: 基于注意力的消息传递用于图神经网络

    AMPNet: Attention as Message Passing for Graph Neural Networks. (arXiv:2210.09475v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09475](http://arxiv.org/abs/2210.09475)

    AMPNet是一种用于图神经网络的基于注意力的消息传递层，能够对节点进行逐个特征编码，并通过跨节点注意力模型特征级别的交互。在实际生物系统上进行广泛基准测试表明，AMPNet在fMRI信号重建方面优于现有基准，并通过案例研究验证了其发现有意义的特征级别交互的能力。

    

    图神经网络（GNNs）已成为一种强大的用于图结构数据的表示学习框架。传统GNNs的一个关键限制是将每个节点表示为一个单一的特征向量，可能忽视了关于个体节点特征的复杂细节。我们提出了一种基于注意力的消息传递层AMPNet，用于GNNs，它对节点进行逐个特征编码，并在消息传递步骤中通过跨节点注意力模型特征级别的交互。我们通过在真实生物系统（如fMRI脑活动记录和空间基因组数据）上进行广泛的基准测试，证明了AMPNet的能力，它在fMRI信号重建方面相比现有基准提高了20％，在添加位置嵌入后又进一步提高了8％。最后，我们通过对生物系统的案例研究验证了AMPNet发现有意义的特征级别交互的能力。我们预计我们的架构将被广泛应用于图神经网络的研究中。

    Graph Neural Networks (GNNs) have emerged as a powerful representation learning framework for graph-structured data. A key limitation of conventional GNNs is their representation of each node with a singular feature vector, potentially overlooking intricate details about individual node features. Here, we propose an Attention-based Message-Passing layer for GNNs (AMPNet) that encodes individual features per node and models feature-level interactions through cross-node attention during message-passing steps. We demonstrate the abilities of AMPNet through extensive benchmarking on real-world biological systems such as fMRI brain activity recordings and spatial genomic data, improving over existing baselines by 20% on fMRI signal reconstruction, and further improving another 8% with positional embedding added. Finally, we validate the ability of AMPNet to uncover meaningful feature-level interactions through case studies on biological systems. We anticipate that our architecture will be h
    
[^426]: 边界感知不确定性可解释特征的探索

    Boundary-Aware Uncertainty for Feature Attribution Explainers. (arXiv:2210.02419v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02419](http://arxiv.org/abs/2210.02419)

    本文提出了一种名为高斯过程解释不确定性（GPEC）框架，它可以对复杂的黑盒分类器进行可靠的特征归因，该框架结合了决策边界感知不确定性和解释函数逼近不确定性，能够生成一个统一的不确定性估计。

    

    后续的解释方法已经成为理解高风险应用中黑盒分类器的关键工具。然而，高性能分类器通常是高度非线性的，并且在决策边界周围展现出复杂的行为，导致脆弱或误导性的局部解释。因此，有必要量化这种解释方法的不确定性，以了解何时可以信任解释。在本文中，我们提出了高斯过程解释不确定性（GPEC）框架，它生成了一个统一的不确定性估计，将决策边界感知不确定性与解释函数逼近不确定性相结合。我们介绍了一种新的基于测地线的核，它捕捉目标黑盒决策边界的复杂性。我们理论上证明所提出的核相似度随着决策边界的复杂性递增。该提出的框架非常灵活，可以与任何黑盒分类器和任何解释方法一起使用。我们在各种数据集上进行实验，并显示GPEC优于现有方法的不确定度估计，并导致更可靠的特征归因。

    Post-hoc explanation methods have become a critical tool for understanding black-box classifiers in high-stakes applications. However, high-performing classifiers are often highly nonlinear and can exhibit complex behavior around the decision boundary, leading to brittle or misleading local explanations. Therefore there is an impending need to quantify the uncertainty of such explanation methods in order to understand when explanations are trustworthy. In this work we propose the Gaussian Process Explanation unCertainty (GPEC) framework, which generates a unified uncertainty estimate combining decision boundary-aware uncertainty with explanation function approximation uncertainty. We introduce a novel geodesic-based kernel, which captures the complexity of the target black-box decision boundary. We show theoretically that the proposed kernel similarity increases with decision boundary complexity. The proposed framework is highly flexible; it can be used with any black-box classifier an
    
[^427]: 使用学习到的MDP同态映射的简单状态-动作抽象方法

    A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06356](http://arxiv.org/abs/2209.06356)

    本论文提出了一种在离散动作空间中构建同态映射的新方法，通过使用环境动力学的部分模型来推断相同状态的状态动作对，从而减小状态-动作空间的大小。

    

    动物能够在有限的经验中迅速推断出等价奖励和转移动力学的状态动作对集合。与此相反，现代强化学习系统必须通过反复试错来学习状态动作对集合的值等价性，这通常需要大量样本。已经提出了MDP同态映射的方法，将环境的观察MDP简化为抽象MDP，可以实现更高效的策略学习。因此，当可以事先构建适当的MDP同态映射时，可以取得令人印象深刻的样本效率改进，通常通过利用环境的对称性来实现。我们提出了一种新颖的方法来构建离散动作空间中的同态映射，该方法使用环境动力学的部分模型来推断哪些状态动作对导致相同的状态，从而减小状态-动作空间的大小。

    Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by
    
[^428]: 用于节点分类的图形数据集的特征化：同质性-异质性二分法及其延伸

    Characterizing Graph Datasets for Node Classification: Homophily-Heterophily Dichotomy and Beyond. (arXiv:2209.06177v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2209.06177](http://arxiv.org/abs/2209.06177)

    本论文研究了用于节点分类的图形数据集的特征化，发现目前常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。作者提出了一种新的同质性测量指标，称为调整同质性，该指标满足更多期望特性，并具有较少在图形机器学习文献中使用的特点。

    

    同质性是描述边连接相似节点倾向的图形属性；相反的概念为异质性。人们通常认为异质性图对于标准的消息传递图神经网络（GNN）是具有挑战性的，并且已经付出了许多努力来开发这种情况下的高效方法。然而，目前在文献中没有普遍被接受的同质性测量指标。在这项工作中，我们展示了常用的同质性测量方法存在严重缺陷，无法比较不同数据集中的同质性水平。为此，我们为正确的同质性测量指标形式化了期望的特性，并验证了哪些指标满足哪些特性。特别是，我们发现一种我们称之为调整同质性的指标满足比其他流行同质性测量方法更多的期望特性，而在图形机器学习文献中很少被使用。然后，我们超越了同质性-异质性二分法，提出了一种新的特征，使得...

    Homophily is a graph property describing the tendency of edges to connect similar nodes; the opposite is called heterophily. It is often believed that heterophilous graphs are challenging for standard message-passing graph neural networks (GNNs), and much effort has been put into developing efficient methods for this setting. However, there is no universally agreed-upon measure of homophily in the literature. In this work, we show that commonly used homophily measures have critical drawbacks preventing the comparison of homophily levels across different datasets. For this, we formalize desirable properties for a proper homophily measure and verify which measures satisfy which properties. In particular, we show that a measure that we call adjusted homophily satisfies more desirable properties than other popular homophily measures while being rarely used in graph machine learning literature. Then, we go beyond the homophily-heterophily dichotomy and propose a new characteristic that allo
    
[^429]: 多元长序列时间序列预测的通用记忆驱动变压器

    Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting. (arXiv:2207.07827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07827](http://arxiv.org/abs/2207.07827)

    本文提出了一种通用记忆驱动变压器，通过集成多个时间序列特征来驱动预测过程，逐步引入噪声以增强泛化能力，在多个数据集上实现了更优秀的预测性能。

    

    多元长序列时间序列预测(M-LSTF)是一个实际但具有挑战性的问题。与传统的时间序列预测任务不同，M-LSTF任务从两个方面更具挑战性：1) M-LSTF模型需要同时学习多个时间特征之间的时间序列模式；2)在滚动预测设置中，两个连续训练样本之间的相似度随着预测长度的增加而增加，这使得模型更易于过拟合。本文提出了一种通用记忆驱动变压器来解决M-LSTF问题。具体而言，我们首先提出了一个全局层面的记忆组件，通过集成多个时间序列特征来驱动预测过程。此外，我们采用渐进式的方式来训练我们的模型，以增强其泛化能力，逐步在训练样本中引入伯努利噪声。在多个领域的五个不同数据集上进行了大量实验。实验结果表明，我们提出的模型优于现有的方法，并在所有数据集上实现了更优异的预测性能。

    Multivariate long sequence time-series forecasting (M-LSTF) is a practical but challenging problem. Unlike traditional timer-series forecasting tasks, M-LSTF tasks are more challenging from two aspects: 1) M-LSTF models need to learn time-series patterns both within and between multiple time features; 2) Under the rolling forecasting setting, the similarity between two consecutive training samples increases with the increasing prediction length, which makes models more prone to overfitting. In this paper, we propose a generalizable memory-driven Transformer to target M-LSTF problems. Specifically, we first propose a global-level memory component to drive the forecasting procedure by integrating multiple time-series features. In addition, we adopt a progressive fashion to train our model to increase its generalizability, in which we gradually introduce Bernoulli noises to training samples. Extensive experiments have been performed on five different datasets across multiple fields. Exper
    

