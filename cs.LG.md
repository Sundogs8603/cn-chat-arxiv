# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts.](http://arxiv.org/abs/2303.17595) | 本研究指出传统的图片分类器学习过程忽视注释过程中的辅助信息，提出了使用注释副产品来训练模型的新方法，该方法可以减少虚假相关性并提高模型精度。 |
| [^2] | [Learning Human-to-Robot Handovers from Point Clouds.](http://arxiv.org/abs/2303.17592) | 该论文提出了一种基于点云的学习方法来控制人与机器人之间的递送过程，通过两阶段的教师-学生框架进行人机交互训练，在模拟基准、模拟到模拟转移和模拟到真实转移中均实现了显著的性能提升。 |
| [^3] | [Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2303.17591) | 不忘我是一种高效、低成本的解决文本到图像模型中删除指定的身份、对象或样式的方法。我们还介绍了记忆分数 (M-Score) 和概念基准 (ConceptBench) 来衡量模型生成通用概念的能力。在三个最先进的文本到图像模型上进行的广泛评估显示出了不忘我的有前途结果。 |
| [^4] | [Polarity is all you need to learn and transfer faster.](http://arxiv.org/abs/2303.17589) | 本文从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变，如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少，从而增加学习和转移的效率。 |
| [^5] | [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace.](http://arxiv.org/abs/2303.17580) | 用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。 |
| [^6] | [Online Learning and Disambiguations of Partial Concept Classes.](http://arxiv.org/abs/2303.17578) | 研究在线学习问题，证明可以构造一个部分概念类别，其是在线可学习的，但其任何扩展成为总概念类别却并不在线可学习。 |
| [^7] | [Elastic Weight Removal for Faithful and Abstractive Dialogue Generation.](http://arxiv.org/abs/2303.17574) | EWR方法通过费舍尔信息矩阵权衡语音生成模型中个体参数的重要性，提高对话回复的忠实性，取得了很好的效果。 |
| [^8] | [Using AI to Measure Parkinson's Disease Severity at Home.](http://arxiv.org/abs/2303.17573) | 该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。 |
| [^9] | [CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X.](http://arxiv.org/abs/2303.17568) | CodeGeeX是一个多语言模型，具有130亿参数，用于代码生成。经过广泛的实验证明，CodeGeeX在HumanEval-X上的代码生成和翻译任务中表现优异。此外，CodeGeeX可以将程序员的生产力提高22%。 |
| [^10] | [BloombergGPT: A Large Language Model for Finance.](http://arxiv.org/abs/2303.17564) | 本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。 |
| [^11] | [The AI Act proposal: a new right to technical interpretability?.](http://arxiv.org/abs/2303.17558) | 本文探讨了欧盟的AI法案是否足以表明在其法律框架中存在技术可解释性权利，并进一步探讨了是否需要将其纳入现行立法中。 |
| [^12] | [Recognition, recall, and retention of few-shot memories in large language models.](http://arxiv.org/abs/2303.17557) | 本文通过对大型语言模型进行实验，揭示了这种模型能够有效地记忆和泛化仅在训练中少次观察到的例子。 |
| [^13] | [Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness.](http://arxiv.org/abs/2303.17555) | 本文通过批判性回顾AI公平性文献中30篇交织性讨论，揭示研究人员普遍缺乏对交织性的整体理解，其一方面将其缩小为在群体子组上进行公平度量的优化，另一方面则在社会背景和权力结构的讨论方面存在欠缺。 |
| [^14] | [Whose Opinions Do Language Models Reflect?.](http://arxiv.org/abs/2303.17548) | 本文通过调查高质量的公共民意调查来创建一个新的数据集OpinionsQA，评估语言模型反映的观点与60个不同人口统计组的观点之间的一致性，发现当前语言模型反映的观点与美国人群组的观点存在巨大差异，甚至通过明确调整LM反映出的观点，仍然无法消除。 |
| [^15] | [PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models.](http://arxiv.org/abs/2303.17546) | 本论文提出了一种采用结构和外观配对扩散模型进行对象级图像编辑的方法，使用户能够精细控制图像中的不同对象属性，同时自动传播注入的外观到具有相似结构的对象。 |
| [^16] | [Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks.](http://arxiv.org/abs/2303.17523) | 本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。 |
| [^17] | [Pgx: Hardware-accelerated parallel game simulation for reinforcement learning.](http://arxiv.org/abs/2303.17503) | Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。 |
| [^18] | [Data-driven multiscale modeling of subgrid parameterizations in climate models.](http://arxiv.org/abs/2303.17496) | 该研究提出了一种基于数据驱动的气候模型多尺度子网格参数化建模方法，通过训练神经网络预测子网格强迫值来提高预测准确性。 |
| [^19] | [Language Models can Solve Computer Tasks.](http://arxiv.org/abs/2303.17491) | 本文研究表明，预训练的大型语言模型代理可以通过一个简单的提示方案使用自然语言执行计算机任务，该方法取得了很好的效果并在MiniWoB++基准测试中超越了监督学习和强化学习方法。 |
| [^20] | [Cost Sensitive GNN-based Imbalanced Learning for Mobile Social Network Fraud Detection.](http://arxiv.org/abs/2303.17486) | 本论文提出了代价敏感图神经网络（CSGNN），用于解决移动社交网络欺诈检测中的图像平衡问题，并在两个开源数据集上实现了显着的改进。 |
| [^21] | [Three-way causal attribute partial order structure analysis.](http://arxiv.org/abs/2303.17482) | 本文提出了一种名为三向因果属性偏序结构（3WCAPOS）的方法，将偏序正式结构分析（POFSA）演变为因果覆盖，从而增强模型的可解释性和分类性能，同时提出了因果因子（CF）概念评估属性和决策属性之间的因果相关性，并结合三向决策构建3WCAPOS，使得结构中节点的纯度更清晰，级别之间的变化更明显。 |
| [^22] | [On the Analysis of Computational Delays in Reinforcement Learning-based Rate Adaptation Algorithms.](http://arxiv.org/abs/2303.17477) | 本文分析了基于强化学习的自适应速率算法中计算延迟的问题，并提出了一种可减少计算延迟并提高算法响应能力的方法。 |
| [^23] | [Efficient distributed representations beyond negative sampling.](http://arxiv.org/abs/2303.17475) | 本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。 |
| [^24] | [Surrogate Neural Networks for Efficient Simulation-based Trajectory Planning Optimization.](http://arxiv.org/abs/2303.17468) | 本文提出了一种使用神经网络替代模型来优化高保真仿真的新方法，针对计算密集型和非线性的轨迹规划仿真，通过梯度下降优化找到最佳参考路径。 |
| [^25] | [Can I Trust My Simulation Model? Measuring the Quality of Business Process Simulation Models.](http://arxiv.org/abs/2303.17463) | 本文提出了一系列度量方法用于评估业务过程仿真模型的质量，以评估其复制过程观察行为的能力。 |
| [^26] | [Fast inference of latent space dynamics in huge relational event networks.](http://arxiv.org/abs/2303.17460) | 本研究提出了一种适用于巨型关系事件网络的基于可能性的算法，可以快速推理出潜在空间动态，并实现分层推断网络社区动态。 |
| [^27] | [NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images.](http://arxiv.org/abs/2303.17448) | 该论文提出了一种可解释的神经网络方法，结合Copula理论来解决异构遥感图像中的变化检测问题。 |
| [^28] | [Removing supervision in semantic segmentation with local-global matching and area balancing.](http://arxiv.org/abs/2303.17410) | 本文提出了一种基于局部-全局匹配和面积平衡的端到端模型，可以移除语义分割中的监督，并在弱监督和无监督方面达到最新水平。 |
| [^29] | [Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions.](http://arxiv.org/abs/2303.17396) | 本文讨论了离线RL代理的在线微调问题。通过研究数据多样性、算法选择和在线回放分布，我们提出了一个保守的策略优化过程，可以从离线预训练中实现稳定且样本有效的在线学习。 |
| [^30] | [Explainable Intrusion Detection Systems Using Competitive Learning Techniques.](http://arxiv.org/abs/2303.17387) | 本文提出一种基于竞争学习算法的白盒可解释入侵检测系统，与黑匣子方法相比，能够提供更为可解释的模型，且资源消耗较小。 |
| [^31] | [A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision.](http://arxiv.org/abs/2303.17376) | 本研究着重研究了计算机视觉中的多任务自回归解码器，通过广泛的系统试验分析了任务和数据混合、训练和正则化超参数、条件类型和特异性、模态组合等因素，发现通过冻结预训练编码器，采用小型解码器可接近于单任务基线。 |
| [^32] | [Invertible Convolution with Symmetric Paddings.](http://arxiv.org/abs/2303.17361) | 本文提出对称填充的可逆卷积方法，可通过DFT解析反演，可应用于多种填充模式；相关代码可在GitHub获取。 |
| [^33] | [DPP-based Client Selection for Federated Learning with Non-IID Data.](http://arxiv.org/abs/2303.17358) | 本文提出了一种名为FL-DP$^3$S的算法，它利用了数据分析和DPP采样技术，有效地多样化参与者的数据集，同时保护他们的数据隐私，从而解决了联邦学习中的通信瓶颈和数据异构性问题，并取得了比基准方案更好的效果。 |
| [^34] | [Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization.](http://arxiv.org/abs/2303.17354) | 该论文提出一种基于Transformer骨干网络的渐进式自监督学习方法，可用于图像异常检测和定位，其中第一阶段使用MAE模型进行正常图像的训练，第二阶段使用像素级数据增强技术来生成损坏的正常图像，最终通过像素重建误差矩阵和像素异常概率矩阵综合得到一个异常得分矩阵。 |
| [^35] | [GAT-COBO: Cost-Sensitive Graph Neural Network for Telecom Fraud Detection.](http://arxiv.org/abs/2303.17334) | 本文提出了一种针对电信欺诈检测而设计的图神经网络，名为GAT-COBO。该方法通过设计基于GAT的基本分类器和成本敏感学习器来解决图不平衡问题，实验结果证明其优于现有竞争方法。 |
| [^36] | [Sasaki Metric for Spline Models of Manifold-Valued Trajectories.](http://arxiv.org/abs/2303.17299) | 该论文提出了一个使用Sasaki度量实现比较离散轨迹的方法，从而在流形值测量中建立了一个计算高效、固有的黎曼分层模型，该方法还可用于群体趋势的平均轨迹估计，经过实验证明在飓风轨迹强度分类方面具有优越性。 |
| [^37] | [Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure.](http://arxiv.org/abs/2303.17276) | 本文研究了GPT-3、GPT-3.5和GPT-4模型在人类思维模式中的表现, 运用认识论理论提供了符号生成模型，通过实验证实的人类判断数据点以及ETR预测数据点的数量级对模型进行了检验。 |
| [^38] | [Multifactor Sequential Disentanglement via Structured Koopman Autoencoders.](http://arxiv.org/abs/2303.17264) | 提出了一种新的、简单易用的深度模型，利用结构化 Koopman 矩阵和解关联实现多因素去关联的任务，可以通过谱损失项得到去关联表示，在动态纹理和动作捕捉数据上展示了新的去关联能力。 |
| [^39] | [Demystifying Misconceptions in Social Bots Research.](http://arxiv.org/abs/2303.17251) | 这篇文章揭示了关于社交机器人研究的普遍误解，强调需要以严谨、公正和负责任的方式讨论虚假信息研究。 |
| [^40] | [Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios.](http://arxiv.org/abs/2303.17245) | 本文提出了一种理论上基础的深度MvC方法（MvCAN），旨在解决实际场景中嘈杂视图的问题，通过实现多视图一致性、互补性和噪声鲁棒性来减少嘈杂视图的副作用，并在实验证明该方法优于现有的MvC方法。 |
| [^41] | [Shapley Chains: Extending Shapley Values to Classifier Chains.](http://arxiv.org/abs/2303.17243) | Shapley Chains 将 Shapley 值扩展到分类器链上，通过考虑标签之间的相关性提供了更完整的特征贡献归属解释，尤其适用于多输出分类任务。 |
| [^42] | [Practical self-supervised continual learning with continual fine-tuning.](http://arxiv.org/abs/2303.17235) | 本文提出了一种实用的自监督连续学习方法，使用可用的标签来泛化自监督学习，并通过连续微调来减轻灾难性遗忘。 |
| [^43] | [The Graphical Nadaraya-Watson Estimator on Latent Position Models.](http://arxiv.org/abs/2303.17229) | 研究了潜在位置模型上的图形Nadaraya-Watson估计器的性质，对于更复杂的方法有理论指导意义。 |
| [^44] | [HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices.](http://arxiv.org/abs/2303.17218) | 本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。 |
| [^45] | [MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations.](http://arxiv.org/abs/2303.17156) | 本文提出了一种名为MAHALO的方法，可以统一离线强化学习和基于观测的模仿学习，帮助处理数据集质量不佳的情况下的顺序决策制定问题，并在实验中证明了其有效性。 |
| [^46] | [Mixed Autoencoder for Self-supervised Visual Representation Learning.](http://arxiv.org/abs/2303.17152) | 本文提出混合自编码器（MixedAE）用于自监督视觉表示学习，在MAE构架下通过同源识别等辅助预文本任务，解决了数据增强下相互信息增加导致性能下降的问题，并取得了遮蔽图像建模（MIM）增强中最先进的转移结果。 |
| [^47] | [Deep Generative Model and Its Applications in Efficient Wireless Network Management: A Tutorial and Case Study.](http://arxiv.org/abs/2303.17114) | 本文探讨了深度生成模型在提高无线网络管理效率方面的应用，提出了一种基于DGMs的管理框架，并进行了一项DGM模型的案例研究。 |
| [^48] | [Contextual Combinatorial Bandits with Probabilistically Triggered Arms.](http://arxiv.org/abs/2303.17110) | 本文研究了带有概率触发臂的情境组合赌博机，在不同条件下设计了C$^2$-UCB-T算法和VAC$^2$-UCB算法，并分别导出了对应的遗憾值上限，为相关应用提供了理论支持。 |
| [^49] | [Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models.](http://arxiv.org/abs/2303.17109) | 本文提出了一个从正半定随机微分方程中高效采样的方法，可以利用正半定-PSD模型在精度$\varepsilon$下生成iid样本。算法复杂度为$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$，其中$T$是时间步数，$\beta$是Fokker-Planck解的正则性。 |
| [^50] | [OpenMix: Exploring Outlier Samples for Misclassification Detection.](http://arxiv.org/abs/2303.17093) | 该论文介绍了一种名为OpenMix的新方法，通过学习拒绝异常样本生成的伪样本来提高深度神经分类器的可靠性，从而检测已知类别的分类错误和未知类别的OOD样本。 |
| [^51] | [Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling.](http://arxiv.org/abs/2303.17080) | 本文提出了一种名为“毒瘤招募”的数据攻击，通过选择性批量采样来混淆机器学习模型，即通过优化结构化训练批次的毒瘤数量来降低目标类的性能。 |
| [^52] | [Machine Learning for Partial Differential Equations.](http://arxiv.org/abs/2303.17078) | 本综述介绍了机器学习在偏微分方程研究中的应用，主要包括发现新的PDE、学习坐标系统和表示解算子，旨在使PDE更易于分析和改进传统的数值算法。 |
| [^53] | [DiffCollage: Parallel Generation of Large Content with Diffusion Models.](http://arxiv.org/abs/2303.17076) | DiffCollage是一种组合扩散模型，可以并行生成任意尺寸和形状的内容，应用于无限图像生成，全景图像生成和长时间文本引导运动生成等任务。 |
| [^54] | [DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents.](http://arxiv.org/abs/2303.17071) | 本文介绍了一种名为DERA的对话型解决代理，该代理利用LLM的对话能力提高了模型的补全能力。DERA框架化为两个代理类型之间的讨论，可以在医疗对话摘要和护理计划生成方面实现显著改进。 |
| [^55] | [Ideal Abstractions for Decision-Focused Learning.](http://arxiv.org/abs/2303.17062) | 本论文提出了一种通过自动配置输出空间以最小化与决策相关信息的损失来制定简化抽象的机器学习系统的方法。 |
| [^56] | [Audio-Visual Grouping Network for Sound Localization from Mixtures.](http://arxiv.org/abs/2303.17056) | 本文提出的 AVGN 网络利用音频-视觉样本和分组注意机制，发现类别感知的音频和视觉特征并对齐，以实现同时定位多个声源的目标。在两个基准测试上的结果表明，该方法在定位精度和对复杂环境的鲁棒性方面均优于现有方法。 |
| [^57] | [Training Neural Networks is NP-Hard in Fixed Dimension.](http://arxiv.org/abs/2303.17045) | 研究了训练具有ReLU和线性阈值激活函数的两层神经网络的固定维度下的NP难度。 回答了两个问题，证明了这两个问题在二维情况下是NP难的，此外在ReLU案例中证明了固定参数问题的参数化固定复杂度维数和ReLU数量的组合参数。 |
| [^58] | [Federated Stochastic Bandit Learning with Unobserved Context.](http://arxiv.org/abs/2303.17043) | 本文提出了一种联邦随机多臂上下文赌博算法以最大化累积奖励，针对未知上下文的情况通过执行特征向量转换解决问题。 |
| [^59] | [The secret of immersion: actor driven camera movement generation for auto-cinematography.](http://arxiv.org/abs/2303.17041) | 本文探讨了构成电影沉浸感的具体组成部分，并提出了一种基于演员驱动的摄像机移动生成系统，以实现情感和空间的沉浸感。 |
| [^60] | [EPG-MGCN: Ego-Planning Guided Multi-Graph Convolutional Network for Heterogeneous Agent Trajectory Prediction.](http://arxiv.org/abs/2303.17027) | EPG-MGCN是一种利用自我规划引导的多图卷积网络，通过建立社交交互模型，同时使用历史轨迹信息和自主车辆的未来规划来预测异构智能体的轨迹，以实现自动驾驶车辆的安全驾驶 |
| [^61] | [HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion.](http://arxiv.org/abs/2303.17015) | 本文提出了一种名为HyperDiffusion的新方法，用于无条件生成建模的隐式神经场。该方法在MLP权重上操作，生成了新的神经隐式场，其中包含由合成的MLP参数编码的信息。 |
| [^62] | [Specification-Guided Data Aggregation for Semantically Aware Imitation Learning.](http://arxiv.org/abs/2303.17010) | 本文提出了一种通过利用规范引导采样技术来聚合专业数据的方法，以语义感知的方式改进模仿学习模型，在自动驾驶领域实现了显着改进。 |
| [^63] | [A comparative evaluation of image-to-image translation methods for stain transfer in histopathology.](http://arxiv.org/abs/2303.17009) | 本文比较了12种图像到图像翻译方法在组织病理学染色转移方面的应用，证明了一些方法优于现有技术，可用于大规模染色组织病理学图像的合成。 |
| [^64] | [Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams.](http://arxiv.org/abs/2303.17003) | 本研究在巴西大学入学考试中评估了GPT-3.5和GPT-4模型，分析了不同提示策略，最终发现GPT-4与Chain-of-Thought提示结合表现最好，在2022年考试中准确率达到了87％。 |
| [^65] | [The G-invariant graph Laplacian.](http://arxiv.org/abs/2303.17001) | 本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。 |
| [^66] | [PopSparse: Accelerated block sparse matrix multiplication on IPU.](http://arxiv.org/abs/2303.16999) | 将大规模神经网络的计算成本降低到可接受的任务性能和加速改进是一个具有挑战的问题。本文介绍了一种利用IPU独特的硬件特性和数据中定义的任何块结构实现在Graphcore IPUs上快速稀疏操作的库——PopSparse。 |
| [^67] | [Does Sparsity Help in Learning Misspecified Linear Bandits?.](http://arxiv.org/abs/2303.16998) | 本文研究了稀疏性在解决不正确线性赌博机问题中的作用，证明了算法可以通过查询$ O(\varepsilon^{-s}d^s) $个操作来获得$O(\varepsilon)$-最优行动，其中$s$是稀疏性参数。 |
| [^68] | [Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+.](http://arxiv.org/abs/2303.16982) | 本文提出了一种使用Uni-Mol+的新方法来高精度预测量子化学属性，它能够从2D分子图自动生成3D构象，并通过迭代优化得到优化后的构象，为预测QC属性提供更加准确的基础。 |
| [^69] | [Sparse joint shift in multinomial classification.](http://arxiv.org/abs/2303.16971) | 该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。 |
| [^70] | [Fairness-Aware Data Valuation for Supervised Learning.](http://arxiv.org/abs/2303.16963) | 本篇论文提出了一个名为FADO的数据评估框架，旨在同时最大化性能和公平性，可以用于一系列机器学习任务中。该框架使用的基于熵的数据评估指标比现有指标计算效率更高，并可用于不公平性缓解预处理技术。 |
| [^71] | [FeDiSa: A Semi-asynchronous Federated Learning Framework for Power System Fault and Cyberattack Discrimination.](http://arxiv.org/abs/2303.16956) | FeDiSa是一种半异步联邦学习框架，用于电力系统故障和网络攻击判别，采用深度自编码器来实现协同训练，能够在保护隐私的前提下进行攻击检测模型的协同训练。 |
| [^72] | [A Generative Modeling Approach Using Quantum Gates.](http://arxiv.org/abs/2303.16955) | 本文提出了一种使用量子门生成新样本的生成建模方法，并在不同数据集上进行了实验证明其有效性。 |
| [^73] | [Leveraging joint sparsity in hierarchical Bayesian learning.](http://arxiv.org/abs/2303.16954) | 本文提出了一种分层贝叶斯学习方法，用于从多个测量向量中推断联合稀疏的参数向量，该方法使用共同的伽马分布超参数来强制联合稀疏性，并在实验中进行了验证。 |
| [^74] | [Meta-Learning Parameterized First-Order Optimizers using Differentiable Convex Optimization.](http://arxiv.org/abs/2303.16952) | 该研究提出了使用可微凸优化的元学习框架，将现有的一阶更新规则推广到更广的家族，证明在元学习者有足够类似任务的经验下，可以一步优化一系列线性最小二乘问题。 |
| [^75] | [De-coupling and De-positioning Dense Self-supervised Learning.](http://arxiv.org/abs/2303.16947) | 该论文介绍了一种解决密集自监督学习中耦合和位置偏差问题的方法，通过引入数据增强策略和解耦与去定位模块实现这一目标，实验证明该方法在目标分类、语义分割和目标检测等任务上具有更好的泛化性能。 |
| [^76] | [Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look Into Operation Importance.](http://arxiv.org/abs/2303.16938) | 本论文对当前广泛使用的NAS基准测试进行了经验研究，发现只需一小部分的操作即可生成接近最高性能的架构，同时这些基准测试存在缺点可能影响公平比较并提供不可靠结果。 |
| [^77] | [A New Deep Learning and XAI-Based Algorithm for Features Selection in Genomics.](http://arxiv.org/abs/2303.16914) | 本文提出了一种基于深度学习和可解释人工智能的新算法来选择基因组数据中最有信息的特征，为诊断、预后和精准医学提供支持，并在慢性淋巴细胞白血病数据集上取得了有效结果。 |
| [^78] | [Training Feedforward Neural Networks with Bayesian Hyper-Heuristics.](http://arxiv.org/abs/2303.16912) | 本研究提出了一种用于训练前馈神经网络的Population-based Bayesian hyper-heuristic算法，可以在训练过程的不同阶段自动寻找最佳的启发式算法来训练网络。 |
| [^79] | [CADM: Confusion Model-based Detection Method for Real-drift in Chunk Data Stream.](http://arxiv.org/abs/2303.16906) | 本文提出一种基于概念混淆的块数据流实时漂移检测新方法，利用真实和伪标签更新模型并使用余弦相似度测量漂移，成功降低了误报和漏报率。 |
| [^80] | [Machine learning-based spin structure detection.](http://arxiv.org/abs/2303.16905) | 该论文报道了使用机器学习算法检测磁性自旋结构的研究。作者提出了一个卷积神经网络，其可针对分割问题检测天线的位置和形状，具有更高的自动化效率，据此有望为内存和神经形态计算方案提供可行性支持。 |
| [^81] | [Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration.](http://arxiv.org/abs/2303.16904) | 本研究探讨了使用二维卷积神经网络技术对肺部CT扫描进行严重程度分类，为诊断肺部疾病如COVID-19和肺炎提供新的方法。 |
| [^82] | [MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks.](http://arxiv.org/abs/2303.16839) | 提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。 |
| [^83] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^84] | [Operator learning with PCA-Net: upper and lower complexity bounds.](http://arxiv.org/abs/2303.16317) | 本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。 |
| [^85] | [Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples.](http://arxiv.org/abs/2303.16270) | 提出了一种名叫one-shot VFL的实用VFL框架，可以同时解决通信瓶颈和有限重叠样本的问题。few-shot VFL则可以在进行一次或仅少量通信的情况下进一步提高准确性。 |
| [^86] | [Lifting uniform learners via distributional decomposition.](http://arxiv.org/abs/2303.16208) | 本文介绍了一种方法，可以将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布下有效的算法，而且对于单调分布，只需要用$\mathcal{D}$中的样本。算法的核心是通过一个算法将$\mathcal{D}$逼近成由子立方体混合而成的混合均匀分布。 |
| [^87] | [Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models.](http://arxiv.org/abs/2303.15999) | 本文开发了一种基于深度学习的半监督回归模型，可直接从图片中计算简绸纱线的线密度，以应用于古老油画，为艺术品保护和修复提供了可靠的线密度估计方法。 |
| [^88] | [Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks.](http://arxiv.org/abs/2303.15991) | 本文提出了面向资源受限的无线边缘网络的高效并行分裂学习（EPSL）框架，旨在加速模型训练。EPSL并行化客户端模型训练，通过聚合梯度降低了反向传播的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。同时，EPSL还设计了资源分配算法以优化计算和通信资源分配。 |
| [^89] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^90] | [Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment.](http://arxiv.org/abs/2303.15689) | 论文提出了一种新的深度不完整多视图聚类算法，采用了部分样本的交叉视图聚合机制和两个原型对齐策略，以解决不完整多视图聚类问题。 |
| [^91] | [Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis Across Six Depression Treatment Studies.](http://arxiv.org/abs/2303.15202) | 这项研究使用机器学习分析了六项抑郁症药物治疗研究的数据，并生成了结果导向的患者亚组，为患者的个性化治疗提供了指导。 |
| [^92] | [LONGNN: Spectral GNNs with Learnable Orthonormal Basis.](http://arxiv.org/abs/2303.13750) | 本研究提出了一种谱图神经网络LONGNN，它采用可学习正交标准基，并解决了固定多项式基和非归一化基础所带来的缺陷，经实验证明其在各种图数据集上具有优异的表现。 |
| [^93] | [Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States.](http://arxiv.org/abs/2303.13024) | 这篇论文提出了一种新的自监督聚类算法，能够在多元时间序列数据中确定并识别对于TBI等急性疾病治疗非常重要的生理状态。研究还利用临床数据验证并解释所识别的生理状态。 |
| [^94] | [Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning.](http://arxiv.org/abs/2303.09483) | 本文提出了一种辅助网络持续学习方法（ANCL），通过对流信息的控制，自然插值可塑性和稳定性之间的差异，有助于在神经网络中实现更好的稳定性-可塑性平衡。 |
| [^95] | [Making Vision Transformers Efficient from A Token Sparsification View.](http://arxiv.org/abs/2303.08685) | 本文提出了一种新的Semantic Token ViT (STViT)方法，实现了全局和本地视觉Transformer的高效性能，同时可用作下游任务的主干骨干。其通过聚类中心的语义令牌代表来代替图像令牌，实现较少的语义令牌即可达到同样的效果。 |
| [^96] | [Sensitive Region-based Metamorphic Testing Framework using Explainable AI.](http://arxiv.org/abs/2303.07580) | 提出了一种基于敏感区域的元测试框架，可以通过转换这些区域来有效地检测易出现错误分类的图像；敏感区域可以由可解释AI指定。 |
| [^97] | [Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT.](http://arxiv.org/abs/2303.02110) | 本研究旨在探讨基于深度学习的图像去噪方法在临床任务中的表现评估，发现使用保真度(FoMs)的评估不一定与任务为基础的评估一致，而基于信号检测理论(SDT)的评估方法提供了更客观、有意义的去噪效果评估方式，并证明虚拟临床试验（VCTs）是评估DL方法的实用工具。 |
| [^98] | [Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms.](http://arxiv.org/abs/2303.00515) | 本研究提出一种基于时空因果关系的新型水位预测模型，通过将因果结构形式化为多层网络和使用蒙版方法，提高了其可解释性，运用于汉江数据集的实际分析中表现优异。 |
| [^99] | [Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging.](http://arxiv.org/abs/2302.11510) | 本文提出了一种使用核心集进行选择性经验回放压缩的技术，可以提升终身学习的效率，应用于医学影像领域。 |
| [^100] | [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.](http://arxiv.org/abs/2302.09419) | 本文全面回顾了预训练基础模型的最新研究进展和发展历程，包括它们的架构、培训目标、预培训任务、微调策略和评估。同时，讨论了其局限性和未来研究方向。 |
| [^101] | [AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving.](http://arxiv.org/abs/2302.08646) | AutoFed 是一种支持异构感知联邦学习的框架，旨在充分利用自动驾驶车辆上的多模态传感数据，并以此实现稳健的自动驾驶。它通过伪标签和自编码器预训练的方法，在解决分布式AVs上具有异构数据的挑战方面表现良好。 |
| [^102] | [Video Probabilistic Diffusion Models in Projected Latent Space.](http://arxiv.org/abs/2302.07685) | 提出了投影概率扩散模型(PVDM)，在低维潜在空间中学习视频分布，从而可以在有限的资源下高效地训练高分辨率视频。 |
| [^103] | [Short-term Prediction and Filtering of Solar Power Using State-Space Gaussian Processes.](http://arxiv.org/abs/2302.00388) | 本研究提出一种利用状态空间高斯过程进行太阳能光伏能源生产短期预测和滤波的方法，通过结合现代变分推理技术，在处理大规模和非高斯分布数据时具有较高的可扩展性和准确性，有望为电站运营管理提供更加准确的决策依据。 |
| [^104] | [Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification.](http://arxiv.org/abs/2301.07609) | 该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。 |
| [^105] | [Efficient Online Learning with Memory via Frank-Wolfe Optimization: Algorithms with Bounded Dynamic Regret and Applications to Control.](http://arxiv.org/abs/2301.00497) | 本文介绍了基于Frank-Wolfe优化的高效内存在线学习算法，该算法以历史决策为基础，适应实时时变环境。该算法广泛应用于动态系统的在线控制，统计套利和时间序列预测等人工智能领域。 |
| [^106] | [Multi-Realism Image Compression with a Conditional Generator.](http://arxiv.org/abs/2212.13824) | 本文提出了一种多现实图像压缩方法，采用条件生成器控制失真和真实感的权衡，从而可以生成逼真的图像，缓解了以前方法合成细节不可控的问题。该方法在失真-真实感方面取得了业界领先的效果。 |
| [^107] | [Sliced Optimal Partial Transport.](http://arxiv.org/abs/2212.08049) | 本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。 |
| [^108] | [LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models.](http://arxiv.org/abs/2212.04088) | 本研究提出了一种新颖的方法LLM-Planner，利用大型语言模型为实体代理进行少样本规划，以实体代理目前所在的环境为基础，增强LLMs生成和更新计划，实验表明其在多任务和快速学习新任务的通用代理的开发中具有很好的表现。 |
| [^109] | [MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series.](http://arxiv.org/abs/2212.01141) | 本文提出了一种名为MHCCL的对比学习模型，可以从多元时间序列数据中学习语义丰富的表示，并利用层次聚类结构中的多粒度信息来滤除虚假负样本和补充正样本。 |
| [^110] | [Improving Pareto Front Learning via Multi-Sample Hypernetworks.](http://arxiv.org/abs/2212.01130) | 本文提出了一个新的PFL框架PHN-HVI，利用超网络生成一组多样的解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。 |
| [^111] | [CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning.](http://arxiv.org/abs/2211.13218) | CODA-Prompt是一种基于分解注意力提示，无需重复训练即可连续学习的方法，相比于其他提示方法具有更好的性能和效率。 |
| [^112] | [ConStruct-VL: Data-Free Continual Structured VL Concepts Learning.](http://arxiv.org/abs/2211.09790) | 该论文介绍了第一个持续的无数据结构化VL概念学习（ConStruct-VL）基准，旨在解决VL模型在结构化VL概念推理方面的瓶颈问题，并提出了一种数据-free的方法。 |
| [^113] | [Deep Temporal Modelling of Clinical Depression through Social Media Text.](http://arxiv.org/abs/2211.07717) | 本文通过使用抑郁症状检测分类器，从社交媒体文本提取临床相关特征，建立了一个模型用于检测用户的临床抑郁症，通过提供不同时间粒度的准确度度量来评估该模型。 |
| [^114] | [Learning dynamical systems: an example from open quantum system dynamics.](http://arxiv.org/abs/2211.06678) | 应用库普曼算子学习算法可以高效地学习量子系统的演化和可观测量，同时推断出底层动力学的对称性。 |
| [^115] | [Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization.](http://arxiv.org/abs/2211.01883) | 本文提出了一类基于动量的方差缩减和本地SGD技术的更快的联邦组合优化算法，用于解决非凸分布式组合问题，并使用自适应矩阵灵活地结合各种自适应学习率。 |
| [^116] | [MAgNET: A Graph U-Net Architecture for Mesh-Based Simulations.](http://arxiv.org/abs/2211.00713) | MAgNET是一个新颖的几何深度学习框架，基于MAg（多通道聚合）操作，采用图形U-Net架构处理任意结构（图形数据）的大维数据，能够高效地处理任意复杂的网格。 |
| [^117] | [Sparse Dynamical Features generation, application to Parkinson's Disease diagnosis.](http://arxiv.org/abs/2210.11624) | 本研究利用动态特征、频率和时间内容提取新的病情特征，在帕金森病的EEG信号诊断中获得了94％的准确性、96％的灵敏度和92％的特异度。 |
| [^118] | [Inference in conditioned dynamics through causality restoration.](http://arxiv.org/abs/2210.10179) | 本文提出了一种经由因果性恢复的方式来产生条件分布下的独立样本。 |
| [^119] | [Packed-Ensembles for Efficient Uncertainty Estimation.](http://arxiv.org/abs/2210.09184) | Packed-Ensembles是一种能够在标准神经网络内运行的轻量级结构化集合，它通过精心调节编码空间的维度来设计。该方法在不损失效果的情况下提高了训练和推理速度。 |
| [^120] | [Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection.](http://arxiv.org/abs/2210.00875) | 本文提出了一种针对数据集版权保护的无害和隐蔽的无目标后门水印方案，可以达到与最先进方案相当或更好的水印效果，并证明对模型性能无害且隐蔽。 |
| [^121] | [An efficient encoder-decoder architecture with top-down attention for speech separation.](http://arxiv.org/abs/2209.15200) | 本文提出了一种新的高效编码解码网络结构，名为TDANet，它通过模拟大脑的自上而下注意力来降低模型复杂度，并在语音分离任务中取得了具有竞争力的结果。 |
| [^122] | [Improving Small Molecule Generation using Mutual Information Machine.](http://arxiv.org/abs/2208.09016) | MolMIM是用于小分子药物发现的概率自编码器，其学习了一种信息丰富且聚类的潜在空间，并通过促进致密的潜在空间来采样有效的分子。通过与其他模型的比较，证明了MolMIM的更好的生成能力，并展示了其出色的分子优化性能。 |
| [^123] | [Conditional Antibody Design as 3D Equivariant Graph Translation.](http://arxiv.org/abs/2208.06073) | MEAN模型提出了一种新的抗体设计方法，通过E(3)-等变消息传递和注意机制更好地捕获不同组件之间的几何相关性，并且可以同时输出1D序列和3D结构。 |
| [^124] | [FixEval: Execution-based Evaluation of Program Fixes for Programming Problems.](http://arxiv.org/abs/2206.07796) | 本文介绍了 FixEval，这是一个由竞技编程问题的有缺陷代码提交和它们对应的修复组成的基准。FixEval 提供了大量的单元测试来评估模型生成的程序修复的正确性，并根据判决进行时间、内存限制和接受性的进一步信息评估。基于实验结果，我们发现基于执行的指标更准确，有助于识别生成的修复的弱点。 |
| [^125] | [Stochastic Zeroth Order Gradient and Hessian Estimators: Variance Reduction and Refined Bias Bounds.](http://arxiv.org/abs/2205.14737) | 研究了随机零阶梯度和Hessian估计，通过随机正交方向进行有限差分来减少方差。除了提供方差的约束，还提供改进的偏差边界。 |
| [^126] | [Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift.](http://arxiv.org/abs/2205.12186) | 该论文提出了一种基于全局原型的持续学习方法，在自监督信息的正则化下学习数据表示，以缓解负面表示漂移问题，并减少持续学习中的灾难性遗忘。 |
| [^127] | [Semi-Parametric Inducing Point Networks and Neural Processes.](http://arxiv.org/abs/2205.11718) | 提出一种半参数体系结构调用训练集的网络SPIN，并以之为基础构建出适用于大型上下文的诱导点神经过程，成功应用于元学习领域并提高了准确性表现。 |
| [^128] | [Clustered Graph Matching for Label Recovery and Graph Classification.](http://arxiv.org/abs/2205.03486) | 本论文提出一种利用顶点对齐的平均图，聚类平均图和混淆网络匹配的策略，比起传统的全局平均图策略，可以更有效地提高匹配性能和分类精度。 |
| [^129] | [Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation.](http://arxiv.org/abs/2205.00459) | 本文提出了基于脉冲表示的微分（DSR）方法，可以实现与人工神经网络（ANN）相竞争的高性能、低延迟训练的脉冲神经网络（SNN）。 |
| [^130] | [Information-theoretic limitations of data-based price discrimination.](http://arxiv.org/abs/2204.12723) | 本文研究基于数据的价格歧视，揭示了任何基于数据的定价策略在收入生成方面的信息论限制，提出了新的经验收益最大化（ERM）策略，并实现了最优收敛速率。 |
| [^131] | [Random Manifold Sampling and Joint Sparse Regularization for Multi-label Feature Selection.](http://arxiv.org/abs/2204.06445) | 本文提出了一种基于联合约束优化问题的 $\ell_{2,1}$ 和 $\ell_{F}$ 正则化方法来获得最相关的几个特征，并在流形正则化中实现了基于随机游走策略的高度稳健的邻域图。该方法在真实数据集上的比较实验中表现优异。 |
| [^132] | [AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation.](http://arxiv.org/abs/2203.09516) | 本文提出了一种自回归先验模型，可以用于解决三维任务中的形状完成、重建和生成问题。该模型可以在任意空间锚点位置的信息条件下进行形状完成，同时还可以用于单视图重建和基于语义的形状生成。 |
| [^133] | [Geometric Repair for Fair Classification at Any Decision Threshold.](http://arxiv.org/abs/2203.07490) | 本文研究了如何通过减少每个组分数分布之间的统计距离，在任何决策阈值下同时提高分类的公平性能，并提出了一种基于最佳运输的后处理算法。 |
| [^134] | [Provably Efficient Causal Model-Based Reinforcement Learning for Systematic Generalization.](http://arxiv.org/abs/2202.06545) | 本文提出了一种新的因果模型强化学习算法，利用环境的共享结构来最大化系统化泛化，在运动定律共享的环境系列中实现了优化的样本复杂度交换，并在基准问题上展示了有效性和挑战性。 |
| [^135] | [Approximation bounds for norm constrained neural networks with applications to regression and GANs.](http://arxiv.org/abs/2201.09418) | 本文研究了具范数约束的ReLU神经网络的逼近能力，并证明了对于平滑函数类，这些网络的逼近误差有上下界。此外，应用结果分析了回归和GAN分布估计问题的收敛性，最终证明了当GAN的判别器选择合适的具范数约束的神经网络时，可以实现学习概率分布的最优速率。 |
| [^136] | [MOEF: Modeling Occasion Evolution in Frequency Domain for Promotion-Aware Click-Through Rate Prediction.](http://arxiv.org/abs/2112.13747) | 本文提出了一种新的CTR模型MOEF，它通过在频域中建模场合演变来处理在线分布的不确定性，采用多个专家学习特征表示，取得了在真实世界的电子商务数据集上优于最先进的CTR模型的效果。 |
| [^137] | [CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization.](http://arxiv.org/abs/2110.10921) | 本文提出了基于类感知迹比优化的通道剪枝方法（CATRO），通过特征空间判别度量多通道的联合影响并合并保留通道的层次影响，有效降低计算负担并加速模型推理。 |
| [^138] | [Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers.](http://arxiv.org/abs/2107.13163) | 本文提出了统计上意义的近似的正式定义，研究了过度参数化的前馈神经网络和变换器的SM近似在布尔电路和图灵机中的应用，重点在于探索近似网络应该具有良好的统计可学性的概念，达到更有意义的近似效果。 |
| [^139] | [Neural network architectures using min-plus algebra for solving certain high dimensional optimal control problems and Hamilton-Jacobi PDEs.](http://arxiv.org/abs/2105.03336) | 本文提出了两种抽象的神经网络结构，用于计算某类高维最优控制问题的价值函数和最优控制。使用深度神经网络实现计算的几个数值结果也被展示了出来。同时，初步实现证明该神经网络结构相对于CPU具有良好的加速效果，为利用专门为神经网络设计的高效专用硬件解决高维最优控制问题和哈密顿-雅可比偏微分方程铺平了道路。 |
| [^140] | [Smart Choices and the Selection Monad.](http://arxiv.org/abs/2007.08926) | 该论文提出了基于选择和成本收益的系统描述方式，并从编程语言的角度研究了此方法。研究者定义了两种支持决策抽象的小语言，并给出了它们的操作语义和底层语义，并将底层语义增强为选择和概率单子。该研究通过应用于两个简单例子展示了此方法的实用性。 |
| [^141] | [DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning.](http://arxiv.org/abs/2003.06777) | 本文提出了一种新的few-shot图像分类方法，使用Earth Mover's Distance作为衡量图像结构距离的度量方法，并设计了交叉参考机制来生成EMD公式中元素的权重，实现了最优匹配流的生成，获得了在基准数据集上最先进的性能。 |
| [^142] | [Variational Wasserstein Barycenters for Geometric Clustering.](http://arxiv.org/abs/2002.10543) | 该论文提出了利用变分Wasserstein质心解决几何聚类问题的方法，特别是Monge WBs与K-means聚类和共同聚类相关，同时还提出了两个新问题——正则化K-means和Wasserstein质心压缩，并演示了VWBs在解决这些聚类相关问题的有效性。 |
| [^143] | [Relaxed Actor-Critic with Convergence Guarantees for Continuous-Time Optimal Control of Nonlinear Systems.](http://arxiv.org/abs/1909.05402) | 本论文提出了一种叫做RCTAC的算法，可以针对具有已知动态和无限地平线的非线性连续时间系统找到几乎最优策略，且无需控制系统的特定属性或初始策略的可接受性，同时在收敛速度和控制性能方面均优于现有算法。 |

# 详细

[^1]: 被忽视的免费午餐——使用注释副产品学习图像分类器

    Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts. (arXiv:2303.17595v1 [cs.CV])

    [http://arxiv.org/abs/2303.17595](http://arxiv.org/abs/2303.17595)

    本研究指出传统的图片分类器学习过程忽视注释过程中的辅助信息，提出了使用注释副产品来训练模型的新方法，该方法可以减少虚假相关性并提高模型精度。

    

    图像分类器的监督学习将人类知识通过图像和相应标签（X，Y）的对应关系转化为参数模型。本文认为这种简单且广泛使用的人类知识表示忽视了注释过程中丰富的辅助信息，例如在图像选择后留下的鼠标轨迹和点击的时间序列等。我们的洞见是，这些注释副产品Z提供了近似的人类关注信息，弱化了模型对前景线索的关注，减少了虚假相关性并防止了捷径学习。为了验证这一点，我们创建了ImageNet-AB和COCO-AB。它们是通过复制相应的原始注释任务来获得的ImageNet和COCO训练集，增加了样本级别的注释副产品。我们称使用注释副产品来训练模型的新方法为学习注释副产品（LUAB）。我们展示了一个简单的多任务损失，用于同时回归Z和Y已经可以提高模型精度。

    Supervised learning of image classifiers distills human knowledge into a parametric model through pairs of images and corresponding labels (X,Y). We argue that this simple and widely used representation of human knowledge neglects rich auxiliary information from the annotation procedure, such as the time-series of mouse traces and clicks left after image selection. Our insight is that such annotation byproducts Z provide approximate human attention that weakly guides the model to focus on the foreground cues, reducing spurious correlations and discouraging shortcut learning. To verify this, we create ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with sample-wise annotation byproducts, collected by replicating the respective original annotation tasks. We refer to the new paradigm of training models with annotation byproducts as learning using annotation byproducts (LUAB). We show that a simple multitask loss for regressing Z together with Y already improves 
    
[^2]: 基于点云的人机交互机器人递送控制方法学习

    Learning Human-to-Robot Handovers from Point Clouds. (arXiv:2303.17592v1 [cs.RO])

    [http://arxiv.org/abs/2303.17592](http://arxiv.org/abs/2303.17592)

    该论文提出了一种基于点云的学习方法来控制人与机器人之间的递送过程，通过两阶段的教师-学生框架进行人机交互训练，在模拟基准、模拟到模拟转移和模拟到真实转移中均实现了显著的性能提升。

    

    我们提出了第一个学习基于视觉的人机交互机器人递送控制方法的框架，这是一个关键的任务。虽然在具身化人工智能研究中，在模拟环境中训练机器人代理取得了显著进展，但由于模拟人类的困难而与人交互仍然具有挑战性。幸运的是，最近的研究已经开发出逼真的人机交互机器人递送的模拟环境。利用这一结果，我们介绍了一种方法，该方法通过两阶段的教师-学生框架进行人机交互训练，使用运动和抓握规划、强化学习和自监督。我们在模拟基准、模拟到模拟转移和模拟到真实转移中展示了显著的性能提升。

    We propose the first framework to learn control policies for vision-based human-to-robot handovers, a critical task for human-robot interaction. While research in Embodied AI has made significant progress in training robot agents in simulated environments, interacting with humans remains challenging due to the difficulties of simulating humans. Fortunately, recent research has developed realistic simulated environments for human-to-robot handovers. Leveraging this result, we introduce a method that is trained with a human-in-the-loop via a two-stage teacher-student framework that uses motion and grasp planning, reinforcement learning, and self-supervision. We show significant performance gains over baselines on a simulation benchmark, sim-to-sim transfer and sim-to-real transfer.
    
[^3]: “不忘我”：文本到图像扩散模型中的遗忘学习

    Forget-Me-Not: Learning to Forget in Text-to-Image Diffusion Models. (arXiv:2303.17591v1 [cs.CV])

    [http://arxiv.org/abs/2303.17591](http://arxiv.org/abs/2303.17591)

    不忘我是一种高效、低成本的解决文本到图像模型中删除指定的身份、对象或样式的方法。我们还介绍了记忆分数 (M-Score) 和概念基准 (ConceptBench) 来衡量模型生成通用概念的能力。在三个最先进的文本到图像模型上进行的广泛评估显示出了不忘我的有前途结果。

    

    深度学习模型的遗忘问题曾一度是学术界的主要关注点，但如今已成为产业界的普遍问题。文本到图像生成技术的重大进展引发了全球对隐私、版权和安全的讨论，因为这些模型学习了大量未授权的个人身份、内容、艺术创作和潜在的有害物质，随后用于生成和分发无控制的内容。为了解决这一挑战，我们提出了“不忘我”，这是一种高效、低成本的解决方案，旨在安全地从文本到图像模型中删除指定的身份、对象或样式，只需不到30秒的时间，而不会影响其生成其他内容的能力。在我们的方法之外，我们引入了“记忆分数 (M-Score)”和“概念基准 (ConceptBench)”来衡量模型生成通用概念的能力，分为三个主要类别：ID、对象和样式。使用M-Score和ConceptBench，我们对三种最先进的文本到图像模型的忘记我进行了广泛的评估，并展示了在记忆保留率、图像质量和推理速度等方面的有前途的结果。

    The unlearning problem of deep learning models, once primarily an academic concern, has become a prevalent issue in the industry. The significant advances in text-to-image generation techniques have prompted global discussions on privacy, copyright, and safety, as numerous unauthorized personal IDs, content, artistic creations, and potentially harmful materials have been learned by these models and later utilized to generate and distribute uncontrolled content. To address this challenge, we propose \textbf{Forget-Me-Not}, an efficient and low-cost solution designed to safely remove specified IDs, objects, or styles from a well-configured text-to-image model in as little as 30 seconds, without impairing its ability to generate other content. Alongside our method, we introduce the \textbf{Memorization Score (M-Score)} and \textbf{ConceptBench} to measure the models' capacity to generate general concepts, grouped into three primary categories: ID, object, and style. Using M-Score and Conc
    
[^4]: 极性是您学习和快速传递所需的全部

    Polarity is all you need to learn and transfer faster. (arXiv:2303.17589v1 [cs.LG])

    [http://arxiv.org/abs/2303.17589](http://arxiv.org/abs/2303.17589)

    本文从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变，如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少，从而增加学习和转移的效率。

    

    自然智能在动态世界中茁壮成长——它们可以很快地学习，有时仅需要少量样本。相比之下，人工智能通常需要大量的训练样本和计算能力才能学习。什么设计原则使得自然智能和人工智能之间存在如此明显的差异？在这里，我们从权重极性的角度提出了一个思路：发育过程会初始化有优势极性配置的自然智能，当自然智能成长和学习时，突触的大小发生变化，但极性基本保持不变。通过模拟和图像分类任务，我们证明了如果权重极性被适当地设置在先，那么网络学习所需的时间和数据将会减少。我们还明确说明了某些情况下，先验设置权重极性会对网络产生不利的影响。我们的工作从学习的统计和计算效率的角度阐述了权重极性的价值。

    Natural intelligences (NIs) thrive in a dynamic world - they learn quickly, sometimes with only a few samples. In contrast, Artificial intelligences (AIs) typically learn with prohibitive amount of training samples and computational power. What design principle difference between NI and AI could contribute to such a discrepancy? Here, we propose an angle from weight polarity: development processes initialize NIs with advantageous polarity configurations; as NIs grow and learn, synapse magnitudes update yet polarities are largely kept unchanged. We demonstrate with simulation and image classification tasks that if weight polarities are adequately set $\textit{a priori}$, then networks learn with less time and data. We also explicitly illustrate situations in which $\textit{a priori}$ setting the weight polarities is disadvantageous for networks. Our work illustrates the value of weight polarities from the perspective of statistical and computational efficiency during learning.
    
[^5]: HuggingGPT: 在HugingFace中使用ChatGPT及其伙伴解决AI任务

    HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])

    [http://arxiv.org/abs/2303.17580](http://arxiv.org/abs/2303.17580)

    用ChatGPT作为任务规划工具，利用大型语言模型（LLM）作为控制器来整合现有的AI模型，解决复杂的AI任务。

    

    解决不同领域和模态的复杂AI任务是通向人工智能的关键步骤。本文提出了一个系统，利用大型语言模型（LLMs）作为控制器来管理现有的AI模型以解决AI任务，语言成为通用接口来赋能它。具体来说，我们使用ChatGPT作为任务规划工具，根据HuggingFace中可用的模型功能描述来选择模型，在选定AI模型的情况下执行每个子任务，并总结响应。

    Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
    
[^6]: 在线学习与部分概念类别的消岐

    Online Learning and Disambiguations of Partial Concept Classes. (arXiv:2303.17578v1 [cs.LG])

    [http://arxiv.org/abs/2303.17578](http://arxiv.org/abs/2303.17578)

    研究在线学习问题，证明可以构造一个部分概念类别，其是在线可学习的，但其任何扩展成为总概念类别却并不在线可学习。

    

    近期，Alon、Hanneke、Holzman和Moran（FOCS '21）提出了一个统一框架，用于研究部分概念类别的可学习性。他们研究的其中一个核心问题是，部分概念类别的可学习性是否总是来自于将其“扩展”到总概念类别的可学习性。他们展示了这一点对于PAC学习不成立，但对于在线学习的更强概念他们将问题留给了读者。本文通过构造一个部分概念类别，证明其是在线可学习的，但其任何扩展成为总概念类别却并不在线可学习（即使是PAC可学习）。

    In a recent article, Alon, Hanneke, Holzman, and Moran (FOCS '21) introduced a unifying framework to study the learnability of classes of partial concepts. One of the central questions studied in their work is whether the learnability of a partial concept class is always inherited from the learnability of some ``extension'' of it to a total concept class.  They showed this is not the case for PAC learning but left the problem open for the stronger notion of online learnability.  We resolve this problem by constructing a class of partial concepts that is online learnable, but no extension of it to a class of total concepts is online learnable (or even PAC learnable).
    
[^7]: 适用于忠实和抽象化对话生成的弹性权重去除

    Elastic Weight Removal for Faithful and Abstractive Dialogue Generation. (arXiv:2303.17574v1 [cs.CL])

    [http://arxiv.org/abs/2303.17574](http://arxiv.org/abs/2303.17574)

    EWR方法通过费舍尔信息矩阵权衡语音生成模型中个体参数的重要性，提高对话回复的忠实性，取得了很好的效果。

    

    理想情况下，对话系统应该生成忠实于相关文档中包含的知识的回复。然而，许多模型生成了幻想的响应，其中包含与其相矛盾的信息或不可验证的信息。为了减轻这种不良行为，已经提出了在负面示例上微调“负面专家”，并从预训练模型的参数中减去它的参数。然而，直觉上，这并没有考虑到某些参数比其他参数更负责导致幻觉。因此，我们提出通过（近似）费舍尔信息矩阵来权衡它们的个体重要性，该矩阵衡量其估计的不确定性。我们将此方法称为弹性权重去除（EWR）。我们使用Flan-T5不同变体作为骨干语言模型评估我们的方法，并在多个信息寻求对话生成数据集上比较我们的方法与忠实性的最新技术。

    Ideally, dialogue systems should generate responses that are faithful to the knowledge contained in relevant documents. However, many models generate hallucinated responses instead that contradict it or contain unverifiable information. To mitigate such undesirable behaviour, it has been proposed to fine-tune a `negative expert' on negative examples and subtract its parameters from those of a pre-trained model. However, intuitively, this does not take into account that some parameters are more responsible than others in causing hallucinations. Thus, we propose to weigh their individual importance via (an approximation of) the Fisher Information matrix, which measures the uncertainty of their estimate. We call this method Elastic Weight Removal (EWR). We evaluate our method -- using different variants of Flan-T5 as a backbone language model -- on multiple datasets for information-seeking dialogue generation and compare our method with state-of-the-art techniques for faithfulness, such a
    
[^8]: 使用人工智能在家中测量帕金森病的严重程度

    Using AI to Measure Parkinson's Disease Severity at Home. (arXiv:2303.17573v1 [cs.LG])

    [http://arxiv.org/abs/2303.17573](http://arxiv.org/abs/2303.17573)

    该论文提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法，该方法可重复用于类似的运动任务，拥有较高的可靠性和准确性。

    

    我们提出了一种使用人工智能系统远程评估帕金森病患者运动表现的方法。参与者在网络摄像头前完成了运动任务（即点击手指），250名全球参与者的数据按照运动障碍协会统一帕金森病评分量表 (MDS-UPDRS) 的标准由三名专家神经学家进行了评估。神经学家的评估具有高度的可靠性，内部一致性系数（ICC）为0.88。我们开发了计算机算法来获得与MDS-UPDRS指南一致且与神经学家的评估高度相关的客观测量结果。我们的机器学习模型在这些指标的训练下表现优于一个MDS-UPDRS认证的评分者，平均绝对误差（MAE）为0.59，而评分者的MAE为0.79。然而，该模型的表现略逊于专家神经学家（0.53 MAE）。该方法可重复用于类似的运动任务，提供了可能性。

    We present an artificial intelligence system to remotely assess the motor performance of individuals with Parkinson's disease (PD). Participants performed a motor task (i.e., tapping fingers) in front of a webcam, and data from 250 global participants were rated by three expert neurologists following the Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS). The neurologists' ratings were highly reliable, with an intra-class correlation coefficient (ICC) of 0.88. We developed computer algorithms to obtain objective measurements that align with the MDS-UPDRS guideline and are strongly correlated with the neurologists' ratings. Our machine learning model trained on these measures outperformed an MDS-UPDRS certified rater, with a mean absolute error (MAE) of 0.59 compared to the rater's MAE of 0.79. However, the model performed slightly worse than the expert neurologists (0.53 MAE). The methodology can be replicated for similar motor tasks, providing the possibili
    
[^9]: CodeGeeX：多语言评估下的预训练代码生成模型

    CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X. (arXiv:2303.17568v1 [cs.LG])

    [http://arxiv.org/abs/2303.17568](http://arxiv.org/abs/2303.17568)

    CodeGeeX是一个多语言模型，具有130亿参数，用于代码生成。经过广泛的实验证明，CodeGeeX在HumanEval-X上的代码生成和翻译任务中表现优异。此外，CodeGeeX可以将程序员的生产力提高22%。

    

    大型预训练代码生成模型（如OpenAI Codex）可以生成正确语法和功能的代码，使程序员的编码更加高效，使我们对人工智能的追求更加贴近现实。本文介绍了CodeGeeX，一个具有130亿参数的多语言模型，用于代码生成。CodeGeeX在2022年6月时基于23种编程语言的8500亿令牌进行了预训练。我们的广泛实验表明，CodeGeeX在HumanEval-X上的代码生成和翻译任务中均优于规模相似的多语言代码模型。在HumanEval（仅限Python）的基础上，我们开发了HumanEval-X基准测试，通过手写C ++、Java、JavaScript和Go的解决方案来评估多语言模型。此外，我们在Visual Studio Code、JetBrains和Cloud Studio上构建了基于CodeGeeX的扩展，每周为数以万计的活跃用户生成47亿令牌。我们的用户研究表明，CodeGeeX可以将程序员的生产力提高22%。

    Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to inc
    
[^10]: BloombergGPT：金融领域的大型语言模型

    BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])

    [http://arxiv.org/abs/2303.17564](http://arxiv.org/abs/2303.17564)

    本文提出了BloombergGPT，一个500亿参数的金融领域的大型语言模型，其基于Bloomberg的广泛数据来源和通用数据集进行训练。通过混合数据集训练，该模型在金融任务上表现出色，并且不会牺牲在普通任务上的性能。

    

    自然语言处理在金融技术领域有着广泛而复杂的应用，从情感分析和命名实体识别到问答。大型语言模型（LLM）已被证明在各种任务上非常有效；然而，专为金融领域设计的LLM尚未在文献中报告。在本文中，我们提出了BloombergGPT，一个拥有500亿个参数的语言模型，它是基于广泛的金融数据进行训练的。我们构建了一种3630亿个标记的数据集，该数据集基于彭博社的广泛数据来源，可能是迄今最大的领域特定数据集，同时又增加了来自通用数据集的3450亿个标记。我们在标准LLM基准、开放式金融基准和一套最能准确反映我们预期用途的内部基准上验证了BloombergGPT。我们的混合数据集训练产生了一个在金融任务上明显优于现有模型的模型，同时不会牺牲普通任务的性能。

    The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
    
[^11]: 《AI法案提案：一项新的技术可解释性权利？》

    The AI Act proposal: a new right to technical interpretability?. (arXiv:2303.17558v1 [cs.CY])

    [http://arxiv.org/abs/2303.17558](http://arxiv.org/abs/2303.17558)

    本文探讨了欧盟的AI法案是否足以表明在其法律框架中存在技术可解释性权利，并进一步探讨了是否需要将其纳入现行立法中。

    

    AI的解释权问题涉及大量文献的讨论。在法律学者中，集中于《通用数据保护条例》中的第22条；在技术学者中，集中于能够帮助解释某个模型输出的技术（XAI）上。本文旨在调查《AI法案》中引入的新规定与《108公约》和《通用数据保护条例》相结合是否足以表明在欧盟的法律框架中存在技术可解释性权利，如果不是，则欧盟是否应将其包含在其现行立法中。

    The debate about the concept of the so called right to explanation in AI is the subject of a wealth of literature. It has focused, in the legal scholarship, on art. 22 GDPR and, in the technical scholarship, on techniques that help explain the output of a certain model (XAI). The purpose of this work is to investigate if the new provisions introduced by the proposal for a Regulation laying down harmonised rules on artificial intelligence (AI Act), in combination with Convention 108 plus and GDPR, are enough to indicate the existence of a right to technical explainability in the EU legal framework and, if not, whether the EU should include it in its current legislation. This is a preliminary work submitted to the online event organised by the Information Society Law Center and it will be later developed into a full paper.
    
[^12]: 大型语言模型的少样本记忆的识别、回忆和保持

    Recognition, recall, and retention of few-shot memories in large language models. (arXiv:2303.17557v1 [cs.CL])

    [http://arxiv.org/abs/2303.17557](http://arxiv.org/abs/2303.17557)

    本文通过对大型语言模型进行实验，揭示了这种模型能够有效地记忆和泛化仅在训练中少次观察到的例子。

    

    当代大型语言模型的训练在一个大多数训练样本仅在模型训练期间看到几次的模式下进行。本文通过对大型语言模型进行简单的识别、回忆和保持实验，探究模型对仅在训练期间少次观察到的样本的记忆及其在继续训练时持续的时间。实验结果表明，仅一个接触通常足以让模型在非常具有挑战性的识别实验中达到近乎完美的准确性。我们估计，在连续训练新样本的几个时期内，模型保持了已见样本的可识别特征。实验表明，大型语言模型具有记忆和泛化少样本例子的能力，这与它们在自然语言处理基准测试中的卓越表现一致。

    The training of modern large language models (LLMs) takes place in a regime where most training examples are seen only a few times by the model during the course of training. What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples? Here, we investigate these questions through simple recognition, recall, and retention experiments with LLMs. In recognition experiments, we ask if the model can distinguish the seen example from a novel example; in recall experiments, we ask if the model can correctly recall the seen example when cued by a part of it; and in retention experiments, we periodically probe the model's memory for the original examples as the model is trained continuously with new examples. We find that a single exposure is generally sufficient for a model to achieve near perfect accuracy even in very challenging recognition experiments. We estimate that the rec
    
[^13]: 对压迫矩阵的分解:揭示交织性在AI公平性中的作用的批判性回顾与再想象

    Factoring the Matrix of Domination: A Critical Review and Reimagination of Intersectionality in AI Fairness. (arXiv:2303.17555v1 [cs.CY])

    [http://arxiv.org/abs/2303.17555](http://arxiv.org/abs/2303.17555)

    本文通过批判性回顾AI公平性文献中30篇交织性讨论，揭示研究人员普遍缺乏对交织性的整体理解，其一方面将其缩小为在群体子组上进行公平度量的优化，另一方面则在社会背景和权力结构的讨论方面存在欠缺。

    

    交织性是一个关键框架，通过调查和实践，它使我们能够检查社会不平等如何通过结构和纪律领域持续存在。在AI公平的理念中，“公平性”是至关重要的，我们认为采用交织性作为分析框架对于有效地实现公平至关重要。通过对AI公平文献中30篇关于交织性的讨论进行批判性回顾，我们归纳和演绎出:1)交织性指导如何在AI公平范例中操作，2)揭示交织性的概念化和实现之间的差距。我们发现，研究人员普遍将交织性缩减为针对人口亚组的公平指标进行优化。他们也未能讨论它们的社会背景，当提到权力时，他们主要将其置于AI流程中。我们将进一步阐述并评估这些差距对于临床研究和实践的影响。

    Intersectionality is a critical framework that, through inquiry and praxis, allows us to examine how social inequalities persist through domains of structure and discipline. Given AI fairness' raison d'\^etre of ``fairness,'' we argue that adopting intersectionality as an analytical framework is pivotal to effectively operationalizing fairness. Through a critical review of how intersectionality is discussed in 30 papers from the AI fairness literature, we deductively and inductively: 1) map how intersectionality tenets operate within the AI fairness paradigm and 2) uncover gaps between the conceptualization and operationalization of intersectionality. We find that researchers overwhelmingly reduce intersectionality to optimizing for fairness metrics over demographic subgroups. They also fail to discuss their social context and when mentioning power, they mostly situate it only within the AI pipeline. We: 3) outline and assess the implications of these gaps for critical inquiry and prax
    
[^14]: 语言模型反映了谁的观点？

    Whose Opinions Do Language Models Reflect?. (arXiv:2303.17548v1 [cs.CL])

    [http://arxiv.org/abs/2303.17548](http://arxiv.org/abs/2303.17548)

    本文通过调查高质量的公共民意调查来创建一个新的数据集OpinionsQA，评估语言模型反映的观点与60个不同人口统计组的观点之间的一致性，发现当前语言模型反映的观点与美国人群组的观点存在巨大差异，甚至通过明确调整LM反映出的观点，仍然无法消除。

    

    语言模型（LM）在越来越多的开放环境中被使用，在针对主观查询的响应中反映的观点可能会对用户满意度产生深远影响，同时也可能塑造整个社会的观点。本文提出了一个定量框架，以调查LM反映的观点。我们利用高质量的公共民意调查和相关的人类反应来创建OpinionsQA，并对60个美国人口统计组的意见进行评估，并涉及从堕胎到自动化的各种话题。在各个话题上，我们发现当前LM反映的观点与美国人群组之间存在重大差异，这与民主党和共和党在气候变化问题上的分歧差不多。值得注意的是，即使明确将LM定向于特定的人口统计组，这种差异仍然存在。我们的分析不仅确认了先前对左倾倾向的观察结果，同时提出了这种差异的一个全新的理论解释。

    Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning
    
[^15]: PAIR-Diffusion: 采用结构和外观配对扩散模型进行对象级图像编辑

    PAIR-Diffusion: Object-Level Image Editing with Structure-and-Appearance Paired Diffusion Models. (arXiv:2303.17546v1 [cs.CV])

    [http://arxiv.org/abs/2303.17546](http://arxiv.org/abs/2303.17546)

    本论文提出了一种采用结构和外观配对扩散模型进行对象级图像编辑的方法，使用户能够精细控制图像中的不同对象属性，同时自动传播注入的外观到具有相似结构的对象。

    

    最近，使用扩散模型进行图像编辑发展迅速。以前的作品可以通过各种方式进行控制和编辑图像，某些作品使用高级条件（例如文本），而其他作品使用低级条件。然而，大多数作品缺乏对图像中不同对象的属性进行精细化控制，即对象级图像编辑。本文将图像视为由多个对象组成，每个对象由不同属性定义。我们发现结构和外观是最直观且最有用于编辑的属性。我们提出了结构和外观配对扩散模型（PAIR-Diffusion），该模型使用从图像中明确提取的结构和外观信息进行训练。所提出的模型使用户能够在对象和全局级别将参考图像的外观注入输入图像中。此外，PAIR-Diffusion自动将注入的外观传播到输入图像中具有类似结构的对象。

    Image editing using diffusion models has witnessed extremely fast-paced growth recently. There are various ways in which previous works enable controlling and editing images. Some works use high-level conditioning such as text, while others use low-level conditioning. Nevertheless, most of them lack fine-grained control over the properties of the different objects present in the image, i.e. object-level image editing. In this work, we consider an image as a composition of multiple objects, each defined by various properties. Out of these properties, we identify structure and appearance as the most intuitive to understand and useful for editing purposes. We propose Structure-and-Appearance Paired Diffusion model (PAIR-Diffusion), which is trained using structure and appearance information explicitly extracted from the images. The proposed model enables users to inject a reference image's appearance into the input image at both the object and global levels. Additionally, PAIR-Diffusion a
    
[^16]: 利用长短期记忆网络提高量子电路保真度

    Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks. (arXiv:2303.17523v1 [quant-ph])

    [http://arxiv.org/abs/2303.17523](http://arxiv.org/abs/2303.17523)

    本文提出使用长短期记忆网络解决量子计算中的保真度问题，利用时间序列预测方法预测量子电路的保真度。

    

    量子计算已进入噪声中间规模量子（NISQ）时代，目前我们拥有的量子处理器对辐射和温度等环境变量敏感，因此会产生嘈杂的输出。虽然已经有许多算法和应用程序用于NISQ处理器，但我们仍面临着解释其嘈杂结果的不确定性。具体来说，我们对所选择的量子态有多少信心？这种信心很重要，因为NISQ计算机将输出其量子位测量的概率分布，有时很难区分分布是否表示有意义的计算或只是随机噪声。本文提出了一种新方法来解决这个问题，将量子电路保真度预测框架为时间序列预测问题，因此可以利用长短期记忆（LSTM）神经网络的强大能力。一个完整的工作流程来构建训练电路

    Quantum computing has entered the Noisy Intermediate-Scale Quantum (NISQ) era. Currently, the quantum processors we have are sensitive to environmental variables like radiation and temperature, thus producing noisy outputs. Although many proposed algorithms and applications exist for NISQ processors, we still face uncertainties when interpreting their noisy results. Specifically, how much confidence do we have in the quantum states we are picking as the output? This confidence is important since a NISQ computer will output a probability distribution of its qubit measurements, and it is sometimes hard to distinguish whether the distribution represents meaningful computation or just random noise. This paper presents a novel approach to attack this problem by framing quantum circuit fidelity prediction as a Time Series Forecasting problem, therefore making it possible to utilize the power of Long Short-Term Memory (LSTM) neural networks. A complete workflow to build the training circuit d
    
[^17]: Pgx:强化学习硬件加速的并行游戏模拟器

    Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])

    [http://arxiv.org/abs/2303.17503](http://arxiv.org/abs/2303.17503)

    Pgx是一个用JAX编写的游戏模拟器集合，具有强化学习硬件加速能力，支持并行执行，速度比现有的强化学习库快10倍。 它实现了Backgammon，Shogi和Go等基准测试游戏。

    

    我们提出了Pgx，这是一个用JAX编写的棋盘游戏模拟器集合。由于JAX的自动向量化和即时编译功能，Pgx易于在GPU/TPU加速器上进行大规模并行执行。我们发现，在单个A100 GPU上的Pgx模拟比现有的强化学习库快10倍。Pgx实现了被认为是人工智能研究中至关重要的基准测试的游戏，如Backgammon，Shogi和Go。 Pgx可在https://github.com/sotetsuk/pgx获得。

    We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
    
[^18]: 基于数据驱动的气候模型子网格参数化多尺度建模

    Data-driven multiscale modeling of subgrid parameterizations in climate models. (arXiv:2303.17496v1 [physics.ao-ph])

    [http://arxiv.org/abs/2303.17496](http://arxiv.org/abs/2303.17496)

    该研究提出了一种基于数据驱动的气候模型多尺度子网格参数化建模方法，通过训练神经网络预测子网格强迫值来提高预测准确性。

    

    子网格参数化是目前气候模型中重要的组成部分，用于表示低于模型分辨率下发生的物理过程，从而产生准确的长期气候预测。已经尝试了多种方法来设计这些组件，包括深度学习方法。本研究中，我们评估了一种多尺度方法用于解决这个预测问题的概念验证。我们训练神经网络来预测测试模型上的子网格强迫值，并研究在精细-粗糙和粗糙-精细方向上使用额外信息可以获得的预测准确性提高。

    Subgrid parameterizations, which represent physical processes occurring below the resolution of current climate models, are an important component in producing accurate, long-term predictions for the climate. A variety of approaches have been tested to design these components, including deep learning methods. In this work, we evaluate a proof of concept illustrating a multiscale approach to this prediction problem. We train neural networks to predict subgrid forcing values on a testbed model and examine improvements in prediction accuracy that can be obtained by using additional information in both fine-to-coarse and coarse-to-fine directions.
    
[^19]: 语言模型能够解决计算机任务

    Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])

    [http://arxiv.org/abs/2303.17491](http://arxiv.org/abs/2303.17491)

    本文研究表明，预训练的大型语言模型代理可以通过一个简单的提示方案使用自然语言执行计算机任务，该方法取得了很好的效果并在MiniWoB++基准测试中超越了监督学习和强化学习方法。

    

    能够在计算机上执行通用任务的代理可以通过自动化重复任务和协助复杂问题的解决来提高效率和生产力。理想情况下，这些代理应该能够通过自然语言命令解决新的计算机任务。然而，先前解决这个问题的方法需要大量专家示范和任务特定的奖励函数，这两者对于新任务来说都不切实际。在这项工作中，我们展示了一个预先训练的大型语言模型（LLM）代理可以使用一个简单的提示方案（RCI），通过自然语言指导执行计算机任务，并在批评和改进输出的过程中取得很好的效果。RCI方法在自动化计算机任务方面明显优于现有的LLM方法，并在MiniWoB++基准测试中超越了监督学习（SL）和强化学习（RL）方法。RCI方法使用每个任务仅有的少数示范，与最新的SL+RL方法相竞争。

    Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
    
[^20]: 基于代价敏感图神经网络的移动社交网络欺诈检测

    Cost Sensitive GNN-based Imbalanced Learning for Mobile Social Network Fraud Detection. (arXiv:2303.17486v1 [cs.SI])

    [http://arxiv.org/abs/2303.17486](http://arxiv.org/abs/2303.17486)

    本论文提出了代价敏感图神经网络（CSGNN），用于解决移动社交网络欺诈检测中的图像平衡问题，并在两个开源数据集上实现了显着的改进。

    

    随着移动网络的快速发展，人们的社交联系得到了极大的便利。然而，移动社交网络欺诈的兴起给人们带来了巨大困扰，可能造成个人和社会财富的损失，并潜在地对经济造成重大损害。为了检测欺诈用户，广泛使用反映用户移动网络中社交行为的通话详单记录（CDR）数据。但是，上述数据中的不平衡问题可能严重阻碍基于图神经网络（GNN）的欺诈检测器的有效性，在先前的工作中几乎没有得到解决。本文将创造性地结合代价敏感学习和图神经网络，提出了一种新颖的代价敏感图神经网络（CSGNN）。我们在两个开源实际移动网络欺诈数据集上进行了广泛的实验。结果表明，CSGNN可以有效解决移动社交网络欺诈检测中的图像平衡问题，并在几个最先进的基线模型上实现了显着的改进。

    With the rapid development of mobile networks, the people's social contacts have been considerably facilitated. However, the rise of mobile social network fraud upon those networks, has caused a great deal of distress, in case of depleting personal and social wealth, then potentially doing significant economic harm. To detect fraudulent users, call detail record (CDR) data, which portrays the social behavior of users in mobile networks, has been widely utilized. But the imbalance problem in the aforementioned data, which could severely hinder the effectiveness of fraud detectors based on graph neural networks(GNN), has hardly been addressed in previous work. In this paper, we are going to present a novel Cost-Sensitive Graph Neural Network (CSGNN) by creatively combining cost-sensitive learning and graph neural networks. We conduct extensive experiments on two open-source realworld mobile network fraud datasets. The results show that CSGNN can effectively solve the graph imbalance prob
    
[^21]: 三向因果属性偏序结构分析

    Three-way causal attribute partial order structure analysis. (arXiv:2303.17482v1 [cs.AI])

    [http://arxiv.org/abs/2303.17482](http://arxiv.org/abs/2303.17482)

    本文提出了一种名为三向因果属性偏序结构（3WCAPOS）的方法，将偏序正式结构分析（POFSA）演变为因果覆盖，从而增强模型的可解释性和分类性能，同时提出了因果因子（CF）概念评估属性和决策属性之间的因果相关性，并结合三向决策构建3WCAPOS，使得结构中节点的纯度更清晰，级别之间的变化更明显。

    

    偏序正式结构分析（POFSA）作为一种新兴的认知学习模型，在知识处理领域得到了广泛的应用。本文提出了一种名为三向因果属性偏序结构（3WCAPOS）的方法，以从集合覆盖向因果覆盖演变，以增加模型的可解释性和分类性能。首先，提出了因果因子（CF）的概念，用于评估形式决策环境中属性和决策属性之间的因果相关性。然后，将CF与属性偏序结构相结合，定义了因果属性偏序结构的概念，使得集合覆盖演变成因果覆盖。最后，结合三向决策的思想，形成了3WCAPOS，使结构中节点的纯度更清晰，级别之间的变化更明显。除此之外，从分类能力出发进行了实验。

    As an emerging concept cognitive learning model, partial order formal structure analysis (POFSA) has been widely used in the field of knowledge processing. In this paper, we propose the method named three-way causal attribute partial order structure (3WCAPOS) to evolve the POFSA from set coverage to causal coverage in order to increase the interpretability and classification performance of the model. First, the concept of causal factor (CF) is proposed to evaluate the causal correlation between attributes and decision attributes in the formal decision context. Then, combining CF with attribute partial order structure, the concept of causal attribute partial order structure is defined and makes set coverage evolve into causal coverage. Finally, combined with the idea of three-way decision, 3WCAPOS is formed, which makes the purity of nodes in the structure clearer and the changes between levels more obviously. In addition, the experiments are carried out from the classification ability 
    
[^22]: 关于基于强化学习的自适应速率算法中计算延迟的分析。

    On the Analysis of Computational Delays in Reinforcement Learning-based Rate Adaptation Algorithms. (arXiv:2303.17477v1 [cs.NI])

    [http://arxiv.org/abs/2303.17477](http://arxiv.org/abs/2303.17477)

    本文分析了基于强化学习的自适应速率算法中计算延迟的问题，并提出了一种可减少计算延迟并提高算法响应能力的方法。

    

    多个研究工作已经应用强化学习算法来解决Wi-Fi网络中的速率自适应问题。无线电链路的动态特性要求算法对链接质量的变化做出反应。算法执行中的延迟可能会对其性能产生不利影响，并进一步降低网络性能。这一方面在现有研究中被忽视。本文提出了一种常见强化学习算法中计算延迟的分析方法，并提出了可应用于减少这些计算延迟并增加此类算法效率的方法。我们将提出的方法应用于一个基于强化学习的现有速率自适应算法。所得实验结果表明，算法执行时间减少了一个数量级，提高了其对链接质量变化的响应能力。

    Several research works have applied Reinforcement Learning (RL) algorithms to solve the Rate Adaptation (RA) problem in Wi-Fi networks. The dynamic nature of the radio link requires the algorithms to be responsive to changes in link quality. Delays in the execution of the algorithm may be detrimental to its performance, which in turn may decrease network performance. This aspect has been overlooked in the state of the art. In this paper, we present an analysis of common computational delays in RL-based RA algorithms, and propose a methodology that may be applied to reduce these computational delays and increase the efficiency of this type of algorithms. We apply the proposed methodology to an existing RL-based RA algorithm. The obtained experimental results indicate a reduction of one order of magnitude in the execution time of the algorithm, improving its responsiveness to link quality changes.
    
[^23]: 超越负采样的高效分布式表示方法

    Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])

    [http://arxiv.org/abs/2303.17475](http://arxiv.org/abs/2303.17475)

    本文介绍了一种高效的分布式表示（嵌入）学习方法，通过线性时间估计softmax归一化常数来实现学习过程，该方法优于负采样方法并在多项测试中验证了其有效性。

    

    本文介绍了一种高效的学习分布式表示（也称为嵌入）的方法。该方法通过最小化一个类似于Word2Vec算法中引入并在多个工作中采用的目标函数来实现。优化计算的瓶颈是softmax归一化常数的计算，这需要与样本大小呈二次比例的操作数。这种复杂度不适用于大型数据集，所以负采样是一个常见的解决方法，可以在与样本大小线性相关的时间内获得分布式表示。然而，负采样会改变损失函数，因此解决的是与最初提出的不同的优化问题。我们的贡献在于展示如何通过线性时间估计softmax归一化常数，从而设计了一种有效的优化策略来学习分布式表示。我们使用不同的数据集进行测试，并展示了我们的方法在嵌入质量和训练时间方面优于负采样。

    This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
    
[^24]: 用替代神经网络来进行高效的基于仿真轨迹规划优化

    Surrogate Neural Networks for Efficient Simulation-based Trajectory Planning Optimization. (arXiv:2303.17468v1 [math.OC])

    [http://arxiv.org/abs/2303.17468](http://arxiv.org/abs/2303.17468)

    本文提出了一种使用神经网络替代模型来优化高保真仿真的新方法，针对计算密集型和非线性的轨迹规划仿真，通过梯度下降优化找到最佳参考路径。

    

    本文提出了一种新方法，使用神经网络的替代模型来减少参考路径的仿真优化的计算时间。在没有系统的解析形式，只有输入输出数据可以用于创建仿真替代模型时，需要进行基于仿真的优化。像许多高保真仿真一样，这个轨迹规划仿真是非常非线性和计算密集型，使得迭代优化变得具有挑战性。通过梯度下降优化，我们的方法找到了降落高超声速飞行器的最佳参考路径。与之前文献中用于创建替代模型的大型数据集相比，我们的方法专门设计为最小化梯度下降优化器所需的仿真执行次数。我们证明了这种方法比手动调整输入参数的标准方法更有效。

    This paper presents a novel methodology that uses surrogate models in the form of neural networks to reduce the computation time of simulation-based optimization of a reference trajectory. Simulation-based optimization is necessary when there is no analytical form of the system accessible, only input-output data that can be used to create a surrogate model of the simulation. Like many high-fidelity simulations, this trajectory planning simulation is very nonlinear and computationally expensive, making it challenging to optimize iteratively. Through gradient descent optimization, our approach finds the optimal reference trajectory for landing a hypersonic vehicle. In contrast to the large datasets used to create the surrogate models in prior literature, our methodology is specifically designed to minimize the number of simulation executions required by the gradient descent optimizer. We demonstrated this methodology to be more efficient than the standard practice of hand-tuning the inpu
    
[^25]: 我能相信我的仿真模型吗？测量业务过程仿真模型的质量。

    Can I Trust My Simulation Model? Measuring the Quality of Business Process Simulation Models. (arXiv:2303.17463v1 [cs.SE])

    [http://arxiv.org/abs/2303.17463](http://arxiv.org/abs/2303.17463)

    本文提出了一系列度量方法用于评估业务过程仿真模型的质量，以评估其复制过程观察行为的能力。

    

    业务过程仿真(BPS)是一种分析不同场景下业务流程性能的方法。 BPS允许我们估计如果一个或多个资源不可用，过程的周期时间会是多少。 BPS的起点是一个注释有仿真参数的过程模型（BPS模型）。 BPS模型可以手动设计，基于从利益相关者和经验观察收集的信息，或者是根据执行数据自动发现的。在使用BPS模型时，无论其来源如何，一个关键问题是如何评估其质量。在本文中，我们提出了一系列评估BPS模型质量的度量方法，以评估其复制过程观察行为的能力。我们倡导一种方法，即不同度量方法针对不同的流程角度。我们评估提出的度量方法识别修改BPS模型的影响的能力以及揭示其相对能力。

    Business Process Simulation (BPS) is an approach to analyze the performance of business processes under different scenarios. For example, BPS allows us to estimate what would be the cycle time of a process if one or more resources became unavailable. The starting point of BPS is a process model annotated with simulation parameters (a BPS model). BPS models may be manually designed, based on information collected from stakeholders and empirical observations, or automatically discovered from execution data. Regardless of its origin, a key question when using a BPS model is how to assess its quality. In this paper, we propose a collection of measures to evaluate the quality of a BPS model w.r.t. its ability to replicate the observed behavior of the process. We advocate an approach whereby different measures tackle different process perspectives. We evaluate the ability of the proposed measures to discern the impact of modifications to a BPS model, and their ability to uncover the relative
    
[^26]: 巨型关系事件网络中潜在空间动态的快速推断

    Fast inference of latent space dynamics in huge relational event networks. (arXiv:2303.17460v1 [cs.SI])

    [http://arxiv.org/abs/2303.17460](http://arxiv.org/abs/2303.17460)

    本研究提出了一种适用于巨型关系事件网络的基于可能性的算法，可以快速推理出潜在空间动态，并实现分层推断网络社区动态。

    

    关系事件是社交互动的一种类型，有时被称为动态网络。它的动态通常取决于新兴的模式，即内生变量，或外部力量，即外生变量。然而，对于网络中的行为者，尤其是对于大型网络，全面的信息是罕见的。网络分析中的潜在空间方法是解释驱动网络配置的未测量协变量的流行方式。贝叶斯和EM类型的算法已被提出用于推断潜在空间，但是许多社交网络应用程序的规模以及过程（因此是潜在空间）的动态特性使得计算变得极其昂贵。在本项工作中，我们提出了一种基于可能性的算法，可以处理巨型关系事件网络。我们提出了一种嵌入到可解释的潜在空间中的推断网络社区动态的分层策略。节点动态是通过向后概率推断的贝叶斯统计模型实现的。

    Relational events are a type of social interactions, that sometimes are referred to as dynamic networks. Its dynamics typically depends on emerging patterns, so-called endogenous variables, or external forces, referred to as exogenous variables. Comprehensive information on the actors in the network, especially for huge networks, is rare, however. A latent space approach in network analysis has been a popular way to account for unmeasured covariates that are driving network configurations. Bayesian and EM-type algorithms have been proposed for inferring the latent space, but both the sheer size many social network applications as well as the dynamic nature of the process, and therefore the latent space, make computations prohibitively expensive. In this work we propose a likelihood-based algorithm that can deal with huge relational event networks. We propose a hierarchical strategy for inferring network community dynamics embedded into an interpretable latent space. Node dynamics are d
    
[^27]: NN-Copula-CD：一种基于Copula的可解释神经网络用于异构遥感图像变化检测

    NN-Copula-CD: A Copula-Guided Interpretable Neural Network for Change Detection in Heterogeneous Remote Sensing Images. (arXiv:2303.17448v1 [cs.CV])

    [http://arxiv.org/abs/2303.17448](http://arxiv.org/abs/2303.17448)

    该论文提出了一种可解释的神经网络方法，结合Copula理论来解决异构遥感图像中的变化检测问题。

    

    异构遥感图像中的变化检测是一个实际而具有挑战性的问题。过去十年来，深度神经网络(DNN)的发展让异构变化检测问题受益匪浅。然而，数据驱动的DNN始终像黑匣子一样，缺乏可解释性，这限制了DNN在大多数实际变化检测应用中的可靠性和可控性。为了解决这些问题，我们提出了一种基于Copula的可解释神经网络异构变化检测方法(NN-Copula-CD)。在NN-Copula-CD中，Copula的数学特征被设计为损失函数，用于监督一个简单的全连接神经网络学习变量之间的相关性。

    Change detection (CD) in heterogeneous remote sensing images is a practical and challenging issue for real-life emergencies. In the past decade, the heterogeneous CD problem has significantly benefited from the development of deep neural networks (DNN). However, the data-driven DNNs always perform like a black box where the lack of interpretability limits the trustworthiness and controllability of DNNs in most practical CD applications. As a strong knowledge-driven tool to measure correlation between random variables, Copula theory has been introduced into CD, yet it suffers from non-robust CD performance without manual prior selection for Copula functions. To address the above issues, we propose a knowledge-data-driven heterogeneous CD method (NN-Copula-CD) based on the Copula-guided interpretable neural network. In our NN-Copula-CD, the mathematical characteristics of Copula are designed as the losses to supervise a simple fully connected neural network to learn the correlation betwe
    
[^28]: 通过局部-全局匹配和面积平衡移除语义分割中的监督

    Removing supervision in semantic segmentation with local-global matching and area balancing. (arXiv:2303.17410v1 [cs.CV])

    [http://arxiv.org/abs/2303.17410](http://arxiv.org/abs/2303.17410)

    本文提出了一种基于局部-全局匹配和面积平衡的端到端模型，可以移除语义分割中的监督，并在弱监督和无监督方面达到最新水平。

    

    移除语义分割中的监督仍然很棘手。现有的方法可以处理常见的分类模式，但需要多阶段架构。我们设计了一种新的端到端模型，利用局部-全局补丁匹配预测语义分割的类别、良好的定位、对象的区域和形状。局部-全局匹配受到满足近似形状预测的面积约束的最优传输计划的驱动。我们的模型在只有图像级别标签的弱监督语义分割方面达到了最新水平，PascalVOC2012验证集上的mIoU为75％，在MS-COCO2014验证集上为46％。放弃图像级别标签并对聚类自监督学习的特征进行分组以产生伪多级标签，我们获得了一种用于语义分割的无监督模型。我们还在无监督语义分割方面达到了最新水平，PascalVOC2012验证集上的mIoU为43.6％，在MS-COCO2014验证集上为19.4％。代码可用。

    Removing supervision in semantic segmentation is still tricky. Current approaches can deal with common categorical patterns yet resort to multi-stage architectures. We design a novel end-to-end model leveraging local-global patch matching to predict categories, good localization, area and shape of objects for semantic segmentation. The local-global matching is, in turn, compelled by optimal transport plans fulfilling area constraints nearing a solution for exact shape prediction. Our model attains state-of-the-art in Weakly Supervised Semantic Segmentation, only image-level labels, with 75% mIoU on PascalVOC2012 val set and 46% on MS-COCO2014 val set. Dropping the image-level labels and clustering self-supervised learned features to yield pseudo-multi-level labels, we obtain an unsupervised model for semantic segmentation. We also attain state-of-the-art on Unsupervised Semantic Segmentation with 43.6% mIoU on PascalVOC2012 val set and 19.4% on MS-COCO2014 val set. The code is availabl
    
[^29]: 离线强化学习的微调：挑战、平衡和实际解决方案

    Finetuning from Offline Reinforcement Learning: Challenges, Trade-offs and Practical Solutions. (arXiv:2303.17396v1 [cs.LG])

    [http://arxiv.org/abs/2303.17396](http://arxiv.org/abs/2303.17396)

    本文讨论了离线RL代理的在线微调问题。通过研究数据多样性、算法选择和在线回放分布，我们提出了一个保守的策略优化过程，可以从离线预训练中实现稳定且样本有效的在线学习。

    

    离线强化学习（RL）允许在没有与环境交互的情况下通过离线数据集训练有能力的智能体。这些离线模型的在线微调可以进一步提高性能。但是，如何理想地微调从离线RL训练中获得的代理？虽然离线RL算法原则上可以用于微调，但在实践中，它们的在线性能提高缓慢。相反，我们发现可以使用标准的在线离策略算法进行更快的改进。然而，我们发现这种方法可能会遭受策略崩溃，即在初始在线学习过程中策略会严重退化。我们研究了策略崩溃的问题以及它与数据多样性、算法选择和在线回放分布之间的关系。基于这些洞见，我们提出了一个保守的策略优化过程，可以从离线预训练中实现稳定且样本有效的在线学习。

    Offline reinforcement learning (RL) allows for the training of competent agents from offline datasets without any interaction with the environment. Online finetuning of such offline models can further improve performance. But how should we ideally finetune agents obtained from offline RL training? While offline RL algorithms can in principle be used for finetuning, in practice, their online performance improves slowly. In contrast, we show that it is possible to use standard online off-policy algorithms for faster improvement. However, we find this approach may suffer from policy collapse, where the policy undergoes severe performance deterioration during initial online learning. We investigate the issue of policy collapse and how it relates to data diversity, algorithm choices and online replay distribution. Based on these insights, we propose a conservative policy optimization procedure that can achieve stable and sample-efficient online learning from offline pretraining.
    
[^30]: 使用竞争学习技术的可解释入侵检测系统

    Explainable Intrusion Detection Systems Using Competitive Learning Techniques. (arXiv:2303.17387v1 [cs.CR])

    [http://arxiv.org/abs/2303.17387](http://arxiv.org/abs/2303.17387)

    本文提出一种基于竞争学习算法的白盒可解释入侵检测系统，与黑匣子方法相比，能够提供更为可解释的模型，且资源消耗较小。

    

    当前，人工智能（AI）启用的入侵检测系统采用各种黑匣子方法。这些黑匣子方法通常使用基于错误学习（EBL）技术进行训练，并专注于创建准确的模型。这些模型具有高性能成本且不易解释。基于竞争学习（CL）的白盒可解释入侵检测系统（X-IDS）为这些问题提供了潜在解决方案。CL模型利用与EBL方法完全不同的学习范式。这种不同的学习过程使得CL算法族固有地可解释且资源消耗较小。在本文中，我们创建了一个基于DARPA对可解释系统的建议的X-IDS架构。在我们的架构中，我们利用像自组织映射（SOM）、增长式自组织映射（GSOM）和增长式分层自组织映射（GHSOM）等CL算法。生成的模型可以进行数据挖掘以创建可解释的规则。

    The current state of the art systems in Artificial Intelligence (AI) enabled intrusion detection use a variety of black box methods. These black box methods are generally trained using Error Based Learning (EBL) techniques with a focus on creating accurate models. These models have high performative costs and are not easily explainable. A white box Competitive Learning (CL) based eXplainable Intrusion Detection System (X-IDS) offers a potential solution to these problem. CL models utilize an entirely different learning paradigm than EBL approaches. This different learning process makes the CL family of algorithms innately explainable and less resource intensive. In this paper, we create an X-IDS architecture that is based on DARPA's recommendation for explainable systems. In our architecture we leverage CL algorithms like, Self Organizing Maps (SOM), Growing Self Organizing Maps (GSOM), and Growing Hierarchical Self Organizing Map (GHSOM). The resulting models can be data-mined to crea
    
[^31]: 计算机视觉多任务自回归解码器研究

    A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision. (arXiv:2303.17376v1 [cs.CV])

    [http://arxiv.org/abs/2303.17376](http://arxiv.org/abs/2303.17376)

    本研究着重研究了计算机视觉中的多任务自回归解码器，通过广泛的系统试验分析了任务和数据混合、训练和正则化超参数、条件类型和特异性、模态组合等因素，发现通过冻结预训练编码器，采用小型解码器可接近于单任务基线。

    

    最近出现了许多计算机视觉模型，能够执行多项任务，由图像编码器（通常是 ViT）和自回归解码器（通常是 Transformer）组成。然而，大部分工作仅呈现一个系统及其结果，对这些系统的设计决策和权衡方面留下了许多疑问。在这项工作中，我们旨在提供这些答案。我们仔细研究了自回归解码器在多模态计算机视觉多任务学习中的应用，包括分类、字幕、视觉问答和光学字符识别等。通过广泛的系统试验，我们研究了任务和数据混合、训练和正则化超参数、条件类型和特异性、模态组合等因素的影响。重要的是，我们将这些结果与经过充分调试的单任务基线进行了比较，以凸显多任务学习所带来的成本。一个关键的发现是，对于冻结预训练编码器上学习的小型解码器，性能接近于单任务基线。

    There has been a recent explosion of computer vision models which perform many tasks and are composed of an image encoder (usually a ViT) and an autoregressive decoder (usually a Transformer). However, most of this work simply presents one system and its results, leaving many questions regarding design decisions and trade-offs of such systems unanswered. In this work, we aim to provide such answers. We take a close look at autoregressive decoders for multi-task learning in multimodal computer vision, including classification, captioning, visual question answering, and optical character recognition. Through extensive systematic experiments, we study the effects of task and data mixture, training and regularization hyperparameters, conditioning type and specificity, modality combination, and more. Importantly, we compare these to well-tuned single-task baselines to highlight the cost incurred by multi-tasking. A key finding is that a small decoder learned on top of a frozen pretrained en
    
[^32]: 对称填充的可逆卷积

    Invertible Convolution with Symmetric Paddings. (arXiv:2303.17361v1 [cs.CV])

    [http://arxiv.org/abs/2303.17361](http://arxiv.org/abs/2303.17361)

    本文提出对称填充的可逆卷积方法，可通过DFT解析反演，可应用于多种填充模式；相关代码可在GitHub获取。

    

    本文展示了对称填充的卷积可以通过DFT得到解析反演。我们全面分析了几种不同的对称和反对称填充模式，并展示了多种可实现反演的情况。此实现可在\url{https://github.com/prclibo/iconv_dft}中获得。

    We show that symmetrically padded convolution can be analytically inverted via DFT. We comprehensively analyze several different symmetric and anti-symmetric padding modes and show that multiple cases exist where the inversion can be achieved. The implementation is available at \url{https://github.com/prclibo/iconv_dft}.
    
[^33]: 基于DPP的非IID数据联邦学习客户端选择方法

    DPP-based Client Selection for Federated Learning with Non-IID Data. (arXiv:2303.17358v1 [cs.LG])

    [http://arxiv.org/abs/2303.17358](http://arxiv.org/abs/2303.17358)

    本文提出了一种名为FL-DP$^3$S的算法，它利用了数据分析和DPP采样技术，有效地多样化参与者的数据集，同时保护他们的数据隐私，从而解决了联邦学习中的通信瓶颈和数据异构性问题，并取得了比基准方案更好的效果。

    

    本文提出了一种客户端选择（CS）方法来解决联邦学习（FL）中的通信瓶颈问题，同时也解决FL中数据异构性问题。我们利用数据分析和确定性点过程（DPP）采样技术开发了一种算法FL-DP$^3$S，它在每轮训练中有效地多样化参与者的数据集，同时保护他们的数据隐私。我们进行了大量实验来检验我们提出的方法的有效性，并证明我们的方案具有更快的收敛速度，以及比几个基准方案更小的通信开销。

    This paper proposes a client selection (CS) method to tackle the communication bottleneck of federated learning (FL) while concurrently coping with FL's data heterogeneity issue. Specifically, we first analyze the effect of CS in FL and show that FL training can be accelerated by adequately choosing participants to diversify the training dataset in each round of training. Based on this, we leverage data profiling and determinantal point process (DPP) sampling techniques to develop an algorithm termed Federated Learning with DPP-based Participant Selection (FL-DP$^3$S). This algorithm effectively diversifies the participants' datasets in each round of training while preserving their data privacy. We conduct extensive experiments to examine the efficacy of our proposed method. The results show that our scheme attains a faster convergence rate, as well as a smaller communication overhead than several baselines.
    
[^34]: 基于Transformer的渐进式自监督学习在异常检测和定位中的应用

    Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization. (arXiv:2303.17354v1 [cs.CV])

    [http://arxiv.org/abs/2303.17354](http://arxiv.org/abs/2303.17354)

    该论文提出一种基于Transformer骨干网络的渐进式自监督学习方法，可用于图像异常检测和定位，其中第一阶段使用MAE模型进行正常图像的训练，第二阶段使用像素级数据增强技术来生成损坏的正常图像，最终通过像素重建误差矩阵和像素异常概率矩阵综合得到一个异常得分矩阵。

    

    在机器学习领域，对于图像数据中的异常检测和定位的研究，尤其是在工业缺陷检测等实际应用中引起了重视。虽然现有的方法主要依赖于卷积神经网络（CNN）作为骨干网络，但我们提出了一种基于Transformer骨干网络的创新方法。我们的方法采用了两阶段的渐进式学习策略。在第一阶段，我们仅使用正常图像对Masked Autoencoder （MAE）模型进行训练，在第二阶段，我们实现了像素级数据增强技术以生成已损坏的正常图像及其相应的像素标签。这个过程使得模型可以学习如何修复损坏的区域和分类每个像素的状态。最终，该模型产生一个像素重建误差矩阵和一个像素异常概率矩阵，这两个矩阵综合成一个异常得分矩阵，有效地用于图像异常检测。

    In the machine learning domain, research on anomaly detection and localization within image data has garnered significant attention, particularly in practical applications such as industrial defect detection. While existing approaches predominantly rely on Convolutional Neural Networks (CNN) as their backbone network, we propose an innovative method based on the Transformer backbone network. Our approach employs a two-stage incremental learning strategy. In the first stage, we train a Masked Autoencoder (MAE) model exclusively on normal images. Subsequently, in the second stage, we implement pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process enables the model to learn how to repair corrupted regions and classify the state of each pixel. Ultimately, the model produces a pixel reconstruction error matrix and a pixel anomaly probability matrix, which are combined to create an anomaly scoring matrix that effective
    
[^35]: GAT-COBO：一种针对电信欺诈检测的成本敏感图神经网络

    GAT-COBO: Cost-Sensitive Graph Neural Network for Telecom Fraud Detection. (arXiv:2303.17334v1 [cs.LG])

    [http://arxiv.org/abs/2303.17334](http://arxiv.org/abs/2303.17334)

    本文提出了一种针对电信欺诈检测而设计的图神经网络，名为GAT-COBO。该方法通过设计基于GAT的基本分类器和成本敏感学习器来解决图不平衡问题，实验结果证明其优于现有竞争方法。

    

    随着移动通信技术（如5G）的快速发展，电信欺诈得到了显著增加，影响了个人财富和社会财富的损失。最近，图挖掘技术逐渐成为检测电信欺诈的主流解决方案。然而，由于帕累托原因引起的图不平衡问题给图数据挖掘带来了严重的挑战。这是一个新的和具有挑战性的问题，但之前很少有研究关注。本文提出了一种针对图不平衡问题的图注意力网络和成本敏感增强（GAT-COBO）方法。首先，我们设计了基于GAT的基本分类器来学习图中所有节点的嵌入。然后，将嵌入馈送到经过良好设计的成本敏感学习器中进行不平衡学习。接下来，根据分类错误成本更新权重，使模型更多地关注少数类。最终，我们将基于分类器和成本敏感学习器获得的节点嵌入相加，得出最终预测。实验结果表明，我们提出的GAT-COBO方法在与最先进的基准线相比时取得了令人满意的性能。

    Along with the rapid evolution of mobile communication technologies, such as 5G, there has been a drastically increase in telecom fraud, which significantly dissipates individual fortune and social wealth. In recent years, graph mining techniques are gradually becoming a mainstream solution for detecting telecom fraud. However, the graph imbalance problem, caused by the Pareto principle, brings severe challenges to graph data mining. This is a new and challenging problem, but little previous work has been noticed. In this paper, we propose a Graph ATtention network with COst-sensitive BOosting (GAT-COBO) for the graph imbalance problem. First, we design a GAT-based base classifier to learn the embeddings of all nodes in the graph. Then, we feed the embeddings into a well-designed cost-sensitive learner for imbalanced learning. Next, we update the weights according to the misclassification cost to make the model focus more on the minority class. Finally, we sum the node embeddings obtai
    
[^36]: 样条模型中测量流形值轨迹的Sasaki度量

    Sasaki Metric for Spline Models of Manifold-Valued Trajectories. (arXiv:2303.17299v1 [math.DG])

    [http://arxiv.org/abs/2303.17299](http://arxiv.org/abs/2303.17299)

    该论文提出了一个使用Sasaki度量实现比较离散轨迹的方法，从而在流形值测量中建立了一个计算高效、固有的黎曼分层模型，该方法还可用于群体趋势的平均轨迹估计，经过实验证明在飓风轨迹强度分类方面具有优越性。

    

    我们提出了一个通用的时空框架来分析流形值测量，它允许使用固有且计算高效的黎曼分层模型。特别地，利用回归，我们通过复合B\' ezier样条在黎曼流形上表达离散轨迹，提出了一种由Sasaki度量诱导的自然度量来比较轨迹，并将平均轨迹估计为群体趋势。我们在飓风轨迹上进行了定性和定量实验，将我们的框架与最先进的方法进行了比较。值得注意的是，我们的结果表明，基于样条的方法在轨迹强度分类方面具有优越性。

    We propose a generic spatiotemporal framework to analyze manifold-valued measurements, which allows for employing an intrinsic and computationally efficient Riemannian hierarchical model. Particularly, utilizing regression, we represent discrete trajectories in a Riemannian manifold by composite B\' ezier splines, propose a natural metric induced by the Sasaki metric to compare the trajectories, and estimate average trajectories as group-wise trends. We evaluate our framework in comparison to state-of-the-art methods within qualitative and quantitative experiments on hurricane tracks. Notably, our results demonstrate the superiority of spline-based approaches for an intensity classification of the tracks.
    
[^37]: 在GPT成功和失败的情况下，人类输入人类输出：论GPT朝向常识的趋同性

    Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure. (arXiv:2303.17276v1 [cs.AI])

    [http://arxiv.org/abs/2303.17276](http://arxiv.org/abs/2303.17276)

    本文研究了GPT-3、GPT-3.5和GPT-4模型在人类思维模式中的表现, 运用认识论理论提供了符号生成模型，通过实验证实的人类判断数据点以及ETR预测数据点的数量级对模型进行了检验。

    

    计算规模和微调的增加使得大语言模型（LLM）例如GPT的输出质量得到了显著提高。鉴于GPT-3和GPT-4都是使用大量由人类生成的文本进行训练的，我们可以问他们的输出在多大程度上反映了人类思维的模式，无论是正确还是错误的情况。认识论理论提供了关于人类成功和失败的符号生成模型，包括命题、量化、概率推理和决策。本文将ETR的最近一个书本的61个核心推理和判断问题提供给了GPT-3、GPT-3.5和GPT-4，这些问题包括经过实验证实的人类判断数据点和ETR预测的数据点，同时包含正确的推理模式和谬误和框架效应（ETR61基准测试）。 ETR61包括了Wason的卡牌任务、错觉推理、诱饵效应等经典案例。

    Increase in computational scale and fine-tuning has seen a dramatic improvement in the quality of outputs of large language models (LLMs) like GPT. Given that both GPT-3 and GPT-4 were trained on large quantities of human-generated text, we might ask to what extent their outputs reflect patterns of human thinking, both for correct and incorrect cases. The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking, across propositional, quantified, and probabilistic reasoning, as well as decision-making. We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a recent book-length presentation of ETR, consisting of experimentally verified data-points on human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory inferences, the decoy effect, and
    
[^38]: 通过结构化 Koopman 自编码器实现多因素顺序去关联

    Multifactor Sequential Disentanglement via Structured Koopman Autoencoders. (arXiv:2303.17264v1 [cs.LG])

    [http://arxiv.org/abs/2303.17264](http://arxiv.org/abs/2303.17264)

    提出了一种新的、简单易用的深度模型，利用结构化 Koopman 矩阵和解关联实现多因素去关联的任务，可以通过谱损失项得到去关联表示，在动态纹理和动作捕捉数据上展示了新的去关联能力。

    

    将复杂数据去关联成为其潜在变化因素是表示学习中的基本任务。现有的关于顺序去关联的工作大多提供两个因素表示，即将数据分离为时变和时不变因素。相反，我们考虑多因素去关联，其中生成多个（超出两个）语义去关联组件。我们的方法的关键是强大的归纳偏差，即我们假设潜在动态可以在潜在空间中线性表达。在这种假设下，利用最近引入的 Koopman 自编码器模型就变得自然。但是，在 Koopman 方法中并不能保证得到去关联表示，因此我们提出了一种新的谱损失项，可以得到结构化 Koopman 矩阵和解关联。总体上，我们提出了一个简单易用的新深度模型，完全无监督，支持多因素去关联。我们展示了我们方法在各种数据集上的新去关联能力，包括动态纹理和动作捕捉数据。

    Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disenta
    
[^39]: 揭开对社交机器人研究的误解

    Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])

    [http://arxiv.org/abs/2303.17251](http://arxiv.org/abs/2303.17251)

    这篇文章揭示了关于社交机器人研究的普遍误解，强调需要以严谨、公正和负责任的方式讨论虚假信息研究。

    

    社交机器人科学寻求解决网络虚假信息最受争议的形式之一的知识和解决方案。然而，社交机器人研究受到普遍的偏见、夸大的结果和误解的困扰，这些都为歧义、不切实际的期望和看似无法调和的发现打下了基础。克服这些问题对于确保可靠的解决方案和重申科学方法的有效性至关重要。在这篇文章中，我们修订了社交机器人研究中的一些最新结果，强调和纠正了事实错误以及方法论和概念问题。更重要的是，我们揭开了普遍的误解，解决了有关如何讨论社交机器人研究的基本问题。我们的分析揭示了以严谨、公正和负责任的方式讨论虚假信息研究的必要性。本文通过确定并驳斥社交机器人研究的支持者和反对者常用的谬误论证，支持这种努力。

    The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
    
[^40]: 研究和减轻多视角聚类中实际场景中嘈杂视图的副作用

    Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios. (arXiv:2303.17245v1 [cs.LG])

    [http://arxiv.org/abs/2303.17245](http://arxiv.org/abs/2303.17245)

    本文提出了一种理论上基础的深度MvC方法（MvCAN），旨在解决实际场景中嘈杂视图的问题，通过实现多视图一致性、互补性和噪声鲁棒性来减少嘈杂视图的副作用，并在实验证明该方法优于现有的MvC方法。

    

    多视角聚类（MvC）旨在探索多视图数据的类别结构，而无需标签监督。多视图比单视图提供更多信息，因此现有的MvC方法可以实现令人满意的性能。然而，在实际场景中，如果视图嘈杂，它们的性能可能会严重退化。在本文中，我们首先正式研究了嘈杂视图的缺点，随后提出了一个理论上基础的深度MvC方法（称为MvCAN）来解决这个问题。具体来说，我们提出了一个新颖的MvC目标，使得不共享参数和不一致的聚类预测可以跨越多个视图，以减少嘈杂视图的副作用。此外，设计了一种非参数迭代过程，以生成一个稳健的学习目标，以挖掘多个视图的有用信息。理论分析表明，MvCAN的工作是通过实现多视图一致性，互补性和噪声鲁棒性来实现的。最后，对公开基准数据集和新收集的实际数据集进行的实验证明，MvCAN在处理实际场景中的嘈杂视图方面优于现有的MvC方法。

    Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on publ
    
[^41]: Shapley Chains: 将 Shapley 值扩展到分类器链上

    Shapley Chains: Extending Shapley Values to Classifier Chains. (arXiv:2303.17243v1 [cs.LG])

    [http://arxiv.org/abs/2303.17243](http://arxiv.org/abs/2303.17243)

    Shapley Chains 将 Shapley 值扩展到分类器链上，通过考虑标签之间的相关性提供了更完整的特征贡献归属解释，尤其适用于多输出分类任务。

    

    尽管可解释机器学习模型受到越来越多的关注，但解释多个输出预测的方法还没有得到广泛解决。使用 Shapley 值将特征贡献归因于决策过程是解释局部个体和全局预测的最流行方法之一。然而，在多输出任务中，这些方法由于单独考虑每个输出而无法提供完整的特征解释。为了解决这个问题，我们提出了 Shapley Chains，通过在解释设计过程中包含标签相互依赖关系，使用分类器链将 Shapley 值分配为多输出分类中的特征重要性分数，从而区分这些特征分数的直接和间接影响。与现有方法相比，这种方法能够更完整地将特征贡献归属于多输出分类任务的预测中。

    In spite of increased attention on explainable machine learning models, explaining multi-output predictions has not yet been extensively addressed. Methods that use Shapley values to attribute feature contributions to the decision making are one of the most popular approaches to explain local individual and global predictions. By considering each output separately in multi-output tasks, these methods fail to provide complete feature explanations. We propose Shapley Chains to overcome this issue by including label interdependencies in the explanation design process. Shapley Chains assign Shapley values as feature importance scores in multi-output classification using classifier chains, by separating the direct and indirect influence of these feature scores. Compared to existing methods, this approach allows to attribute a more complete feature contribution to the predictions of multi-output classification tasks. We provide a mechanism to distribute the hidden contributions of the output
    
[^42]: 实用的自监督连续学习方法：连续微调

    Practical self-supervised continual learning with continual fine-tuning. (arXiv:2303.17235v1 [cs.LG])

    [http://arxiv.org/abs/2303.17235](http://arxiv.org/abs/2303.17235)

    本文提出了一种实用的自监督连续学习方法，使用可用的标签来泛化自监督学习，并通过连续微调来减轻灾难性遗忘。

    

    自监督学习在计算机视觉任务中表现出了出色的性能。而在连续学习情景中，模型仍然存在灾难性遗忘的问题。本文提出了一种能够使用任何步骤中的可用标签的方法，该方法能够在实际情况下泛化自监督连续学习，并且能够提供更多的灵活性。

    Self-supervised learning (SSL) has shown remarkable performance in computer vision tasks when trained offline. However, in a Continual Learning (CL) scenario where new data is introduced progressively, models still suffer from catastrophic forgetting. Retraining a model from scratch to adapt to newly generated data is time-consuming and inefficient. Previous approaches suggested re-purposing self-supervised objectives with knowledge distillation to mitigate forgetting across tasks, assuming that labels from all tasks are available during fine-tuning. In this paper, we generalize self-supervised continual learning in a practical setting where available labels can be leveraged in any step of the SSL process. With an increasing number of continual tasks, this offers more flexibility in the pre-training and fine-tuning phases. With Kaizen, we introduce a training architecture that is able to mitigate catastrophic forgetting for both the feature extractor and classifier with a carefully des
    
[^43]: 潜在位置模型上的图形Nadaraya-Watson估计器

    The Graphical Nadaraya-Watson Estimator on Latent Position Models. (arXiv:2303.17229v1 [stat.ML])

    [http://arxiv.org/abs/2303.17229](http://arxiv.org/abs/2303.17229)

    研究了潜在位置模型上的图形Nadaraya-Watson估计器的性质，对于更复杂的方法有理论指导意义。

    

    鉴于有标记节点的图形，我们对估计器的质量感兴趣，该估计器针对未标记节点预测其标记邻居的观测值的平均值。我们在这个背景下严格研究了浓度属性、方差界和风险界。虽然估计器本身非常简单，数据生成过程对于实际应用过于理想，但我们相信我们的小步骤将有助于更复杂方法（如图形神经网络）的理论理解。

    Given a graph with a subset of labeled nodes, we are interested in the quality of the averaging estimator which for an unlabeled node predicts the average of the observations of its labeled neighbours. We rigorously study concentration properties, variance bounds and risk bounds in this context. While the estimator itself is very simple and the data generating process is too idealistic for practical applications, we believe that our small steps will contribute towards the theoretical understanding of more sophisticated methods such as Graph Neural Networks.
    
[^44]: HARFLOW3D：一种面向FPGA设备的基于延迟的3D-CNN加速器工具链

    HARFLOW3D: A Latency-Oriented 3D-CNN Accelerator Toolflow for HAR on FPGA Devices. (arXiv:2303.17218v1 [cs.AR])

    [http://arxiv.org/abs/2303.17218](http://arxiv.org/abs/2303.17218)

    本研究提出了一种面向FPGA设备的基于延迟的3D-CNN加速器工具链HARFLOW3D，它以机器学习模型和FPGA的特性描述为输入，生成最小化计算延迟的设计。实验证明HARFLOW3D相比其他方案能够实现更低的延迟。

    

    3D卷积神经网络已被证明在人体动作识别任务中具有高效性和最先进的结果。本研究引入一种新的基于流式架构的工具链，将此类模型映射到FPGA上，考虑模型固有特性和目标FPGA设备的特征。HARFLOW3D工具链以ONNX格式的3D卷积神经网络和FPGA特性描述为输入，生成最小化计算延迟的设计。该工具链由多个部分组成，包括i) 3D CNN解析器，ii) 性能和资源模型，iii) 用于在生成的硬件上执行3D模型的调度算法，iv) 针对3D模型量身定制的资源感知优化引擎，v) 自动映射到可合成的FPGA代码。通过对各种3D CNN和FPGA系统配对进行多个实验，展示了工具链支持广泛模型和设备的能力。此外，与其他最先进的3D CNN加速器设计方法相比，该工具链实现了更低的延迟。

    For Human Action Recognition tasks (HAR), 3D Convolutional Neural Networks have proven to be highly effective, achieving state-of-the-art results. This study introduces a novel streaming architecture based toolflow for mapping such models onto FPGAs considering the model's inherent characteristics and the features of the targeted FPGA device. The HARFLOW3D toolflow takes as input a 3D CNN in ONNX format and a description of the FPGA characteristics, generating a design that minimizes the latency of the computation. The toolflow is comprised of a number of parts, including i) a 3D CNN parser, ii) a performance and resource model, iii) a scheduling algorithm for executing 3D models on the generated hardware, iv) a resource-aware optimization engine tailored for 3D models, v) an automated mapping to synthesizable code for FPGAs. The ability of the toolflow to support a broad range of models and devices is shown through a number of experiments on various 3D CNN and FPGA system pairs. Furth
    
[^45]: MAHALO: 统一离线强化学习和基于观测的模仿学习

    MAHALO: Unifying Offline Reinforcement Learning and Imitation Learning from Observations. (arXiv:2303.17156v1 [cs.LG])

    [http://arxiv.org/abs/2303.17156](http://arxiv.org/abs/2303.17156)

    本文提出了一种名为MAHALO的方法，可以统一离线强化学习和基于观测的模仿学习，帮助处理数据集质量不佳的情况下的顺序决策制定问题，并在实验中证明了其有效性。

    

    本文研究了一种名为离线观测学习（PLfO）的新的顺序决策制定范例。离线PLfO旨在使用质量不佳的数据集来学习策略：1）仅有一部分轨迹被标记为奖励，2）标记轨迹可能不包含动作，3）标记轨迹可能质量不高，4）总体数据可能不具备全面性。这些缺陷在真实的学习场景中很常见，因此离线PLfO包括许多现有的离线学习设置，包括离线模仿学习（IL），ILfO和强化学习（RL）。在本文中，我们提出了一种名为模态不可知对抗假设适应学习的离线PLfO的通用方法（MAHALO）。 MAHALO基于离线RL中的悲观主义概念，使用考虑由于数据集不足的不确定性的性能下界来优化策略。我们通过对有关观察和动作模态的有限假设进行对抗变换来实现这一想法。实验结果表明，MAHALO在各种离线PLfO任务上都很有效。

    We study a new paradigm for sequential decision making, called offline Policy Learning from Observation (PLfO). Offline PLfO aims to learn policies using datasets with substandard qualities: 1) only a subset of trajectories is labeled with rewards, 2) labeled trajectories may not contain actions, 3) labeled trajectories may not be of high quality, and 4) the overall data may not have full coverage. Such imperfection is common in real-world learning scenarios, so offline PLfO encompasses many existing offline learning setups, including offline imitation learning (IL), ILfO, and reinforcement learning (RL). In this work, we present a generic approach, called Modality-agnostic Adversarial Hypothesis Adaptation for Learning from Observations (MAHALO), for offline PLfO. Built upon the pessimism concept in offline RL, MAHALO optimizes the policy using a performance lower bound that accounts for uncertainty due to the dataset's insufficient converge. We implement this idea by adversarially tr
    
[^46]: 混合自编码器用于自监督视觉表示学习

    Mixed Autoencoder for Self-supervised Visual Representation Learning. (arXiv:2303.17152v1 [cs.CV])

    [http://arxiv.org/abs/2303.17152](http://arxiv.org/abs/2303.17152)

    本文提出混合自编码器（MixedAE）用于自监督视觉表示学习，在MAE构架下通过同源识别等辅助预文本任务，解决了数据增强下相互信息增加导致性能下降的问题，并取得了遮蔽图像建模（MIM）增强中最先进的转移结果。

    

    掩码自编码器（MAE）通过随机遮盖图像补丁和重建在各种视觉任务上展现出卓越的性能。然而，MAE的有效数据增强策略仍然是未解决的问题，不同于对比学习中的策略。本文研究了用于MAE的普遍混合增强。我们首先证明了朴素混合将由于相互信息的增加而降低模型性能。为了解决这个问题，我们提出了同源识别方法，一种辅助的预文本任务，不仅通过明确要求每个补丁识别同源补丁来缓解相互信息的增加，而且还可以执行面向对象的自监督预训练以获得更好的下游密集感知性能。通过大量的实验证明，我们的混合自编码器（MixedAE）在不同的遮蔽图像建模（MIM）增强中实现了最先进的转移结果。

    Masked Autoencoder (MAE) has demonstrated superior performance on various vision tasks via randomly masking image patches and reconstruction. However, effective data augmentation strategies for MAE still remain open questions, different from those in contrastive learning that serve as the most important part. This paper studies the prevailing mixing augmentation for MAE. We first demonstrate that naive mixing will in contrast degenerate model performance due to the increase of mutual information (MI). To address, we propose homologous recognition, an auxiliary pretext task, not only to alleviate the MI increasement by explicitly requiring each patch to recognize homologous patches, but also to perform object-aware self-supervised pre-training for better downstream dense perception performance. With extensive experiments, we demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the state-of-the-art transfer results among masked image modeling (MIM) augmentations on differen
    
[^47]: 深度生成模型及其在高效无线网络管理中的应用: 教程和案例研究

    Deep Generative Model and Its Applications in Efficient Wireless Network Management: A Tutorial and Case Study. (arXiv:2303.17114v1 [cs.NI])

    [http://arxiv.org/abs/2303.17114](http://arxiv.org/abs/2303.17114)

    本文探讨了深度生成模型在提高无线网络管理效率方面的应用，提出了一种基于DGMs的管理框架，并进行了一项DGM模型的案例研究。

    

    随着扩散模型和ChatGPT的惊人成功，深度生成模型(DGMs)正在经历2022年的爆炸式增长。不限于内容生成，由于其出色的复杂模式表示能力和生成出可信样本的能力，DGMs也被广泛应用于物联网、元宇宙和数字孪生等领域。在本文中，我们探讨了DGMs在重要任务中的应用，即提高无线网络管理的效率。具体来说，我们首先概述了生成AI，以及三种代表性的DGMs。然后，我们提出了一种基于DGMs增强的无线网络管理框架，在框架中详细说明了传统网络管理方法的问题，以及为什么DGMs能够高效地解决这些问题，并阐述了在管理无线网络中应用DGMs的逐步工作流程。此外，我们还使用最先进的DGM模型——扩散模型，在网络经济学上进行了一项案例研究，以生成有效的内容。

    With the phenomenal success of diffusion models and ChatGPT, deep generation models (DGMs) have been experiencing explosive growth from 2022. Not limited to content generation, DGMs are also widely adopted in Internet of Things, Metaverse, and digital twin, due to their outstanding ability to represent complex patterns and generate plausible samples. In this article, we explore the applications of DGMs in a crucial task, i.e., improving the efficiency of wireless network management. Specifically, we firstly overview the generative AI, as well as three representative DGMs. Then, a DGM-empowered framework for wireless network management is proposed, in which we elaborate the issues of the conventional network management approaches, why DGMs can address them efficiently, and the step-by-step workflow for applying DGMs in managing wireless networks. Moreover, we conduct a case study on network economics, using the state-of-the-art DGM model, i.e., diffusion model, to generate effective con
    
[^48]: 带有概率触发臂的情境组合赌博机

    Contextual Combinatorial Bandits with Probabilistically Triggered Arms. (arXiv:2303.17110v1 [cs.LG])

    [http://arxiv.org/abs/2303.17110](http://arxiv.org/abs/2303.17110)

    本文研究了带有概率触发臂的情境组合赌博机，在不同条件下设计了C$^2$-UCB-T算法和VAC$^2$-UCB算法，并分别导出了对应的遗憾值上限，为相关应用提供了理论支持。

    

    本研究探讨了在捕捉广泛应用范围的一系列平滑条件下的带有概率触发臂的情境组合赌博机(C$^2$MAB-T)，例如情境级联赌博机和情境最大化赌博机。在模拟触发概率(TPM)的条件下，我们设计了C$^2$-UCB-T算法，并提出了一种新的分析方法，实现了一个$\tilde{O}(d\sqrt{KT})$的遗憾值上限，消除了一个可能指数级增长的因子$O(1/p_{\min})$，其中$d$是情境的维数，$p_{\min}$是能被触发的任何臂的最小正概率，批大小$K$是每轮能被触发的臂的最大数量。在方差调制(VM)或触发概率和方差调制(TPVM)条件下，我们提出了一种新的方差自适应算法VAC$^2$-UCB，并导出了一个$\tilde{O}(d\sqrt{T})$的遗憾值上限，该上限与批大小$K$无关。作为一个有价值的副产品，我们发现我们的一个...

    We study contextual combinatorial bandits with probabilistically triggered arms (C$^2$MAB-T) under a variety of smoothness conditions that capture a wide range of applications, such as contextual cascading bandits and contextual influence maximization bandits. Under the triggering probability modulated (TPM) condition, we devise the C$^2$-UCB-T algorithm and propose a novel analysis that achieves an $\tilde{O}(d\sqrt{KT})$ regret bound, removing a potentially exponentially large factor $O(1/p_{\min})$, where $d$ is the dimension of contexts, $p_{\min}$ is the minimum positive probability that any arm can be triggered, and batch-size $K$ is the maximum number of arms that can be triggered per round. Under the variance modulated (VM) or triggering probability and variance modulated (TPVM) conditions, we propose a new variance-adaptive algorithm VAC$^2$-UCB and derive a regret bound $\tilde{O}(d\sqrt{T})$, which is independent of the batch-size $K$. As a valuable by-product, we find our a
    
[^49]: 正半定随机微分方程的高效采样

    Efficient Sampling of Stochastic Differential Equations with Positive Semi-Definite Models. (arXiv:2303.17109v1 [stat.ML])

    [http://arxiv.org/abs/2303.17109](http://arxiv.org/abs/2303.17109)

    本文提出了一个从正半定随机微分方程中高效采样的方法，可以利用正半定-PSD模型在精度$\varepsilon$下生成iid样本。算法复杂度为$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$，其中$T$是时间步数，$\beta$是Fokker-Planck解的正则性。

    

    本文解决了在已知漂移函数和扩散矩阵的情况下，从随机微分方程中高效采样的问题。所提出的方法利用了一个最近的概率模型（正半定-PSD模型）\citep{rudi2021psd}，从中可以获得精度为$\varepsilon$的独立同分布（iid）样本，其成本为$m^2 d \log(1/\varepsilon)$，其中$m$是模型的维度，$d$是空间的维度。所提出的方法包括：首先计算满足与SDE相关联的Fokker-Planck方程（或其分数变体）的PSD模型，误差为$\varepsilon$，然后从生成的PSD模型中采样。假设Fokker-Planck解具有一定的正则性（即$\beta$阶可微性以及其零点的一些几何条件），我们得到一个算法：（a）在准备阶段，获得具有L2距离$\varepsilon$的PSD模型作为真实概率密度函数的估计；（b）在采样阶段，以精度$\varepsilon$生成SDE解的iid样本。所得到的复杂度为$O(T d \log(1/\varepsilon) m^2 + d m^{\beta+1} \log(T)/\varepsilon^2)$，其中$T$是SDE的时间步数，$\beta$是Fokker-Planck解的正则性。

    This paper deals with the problem of efficient sampling from a stochastic differential equation, given the drift function and the diffusion matrix. The proposed approach leverages a recent model for probabilities \citep{rudi2021psd} (the positive semi-definite -- PSD model) from which it is possible to obtain independent and identically distributed (i.i.d.) samples at precision $\varepsilon$ with a cost that is $m^2 d \log(1/\varepsilon)$ where $m$ is the dimension of the model, $d$ the dimension of the space. The proposed approach consists in: first, computing the PSD model that satisfies the Fokker-Planck equation (or its fractional variant) associated with the SDE, up to error $\varepsilon$, and then sampling from the resulting PSD model. Assuming some regularity of the Fokker-Planck solution (i.e. $\beta$-times differentiability plus some geometric condition on its zeros) We obtain an algorithm that: (a) in the preparatory phase obtains a PSD model with L2 distance $\varepsilon$ fr
    
[^50]: OpenMix: 探索异常样本以检测分类错误

    OpenMix: Exploring Outlier Samples for Misclassification Detection. (arXiv:2303.17093v1 [cs.LG])

    [http://arxiv.org/abs/2303.17093](http://arxiv.org/abs/2303.17093)

    该论文介绍了一种名为OpenMix的新方法，通过学习拒绝异常样本生成的伪样本来提高深度神经分类器的可靠性，从而检测已知类别的分类错误和未知类别的OOD样本。

    

    在高风险应用中，可靠的深度神经分类器置信度估计是一个具有挑战性但基本要求。然而，现代深度神经网络通常对其错误预测过于自信。在这项工作中，我们利用易于获取的异常样本，即来自非目标类的未标记样本，帮助检测分类错误。特别地，我们发现出名的Outlier Exposure在检测未知类别的样本中非常强大，但在识别分类错误方面并没有提供任何帮助。基于这些观察，我们提出了一种名为OpenMix的新方法，通过学习拒绝通过异常转换生成的不确定伪样本来融合开放世界的知识。OpenMix在各种情境下显著提高了可靠性，建立了一个强大而统一的框架，用于检测已知类别的分类错误和未知类别的OOD样本。

    Reliable confidence estimation for deep neural classifiers is a challenging yet fundamental requirement in high-stakes applications. Unfortunately, modern deep neural networks are often overconfident for their erroneous predictions. In this work, we exploit the easily available outlier samples, i.e., unlabeled samples coming from non-target classes, for helping detect misclassification errors. Particularly, we find that the well-known Outlier Exposure, which is powerful in detecting out-of-distribution (OOD) samples from unknown classes, does not provide any gain in identifying misclassification errors. Based on these observations, we propose a novel method called OpenMix, which incorporates open-world knowledge by learning to reject uncertain pseudo-samples generated via outlier transformation. OpenMix significantly improves confidence reliability under various scenarios, establishing a strong and unified framework for detecting both misclassified samples from known classes and OOD sa
    
[^51]: “选择性批量采样中的毒瘤招募：一种混淆机器学习模型的数据攻击”

    Mole Recruitment: Poisoning of Image Classifiers via Selective Batch Sampling. (arXiv:2303.17080v1 [cs.LG])

    [http://arxiv.org/abs/2303.17080](http://arxiv.org/abs/2303.17080)

    本文提出了一种名为“毒瘤招募”的数据攻击，通过选择性批量采样来混淆机器学习模型，即通过优化结构化训练批次的毒瘤数量来降低目标类的性能。

    

    本文提出了一种数据毒瘤攻击，该攻击不需要对图像或标签进行任何操作就可以混淆机器学习模型。这是通过仅仅利用训练数据中最混淆的自然样本来实现的，这是一种新的有针对性的攻击形式，称为“毒瘤招募”。我们将毒瘤定义为一个类的训练样本，其与另一个类的样本最相似，并展示了通过优化毒瘤数量结构化训练批次，可以显著降低目标类的性能。我们在离线设置下展示了这种新型攻击的有效性，并在连续学习 (CL) 设置下展示了这种攻击的实际可行性。我们的分析揭示了最先进的模型容易受到“毒瘤招募”的影响，从而暴露了图像分类器以前未被发现的漏洞。

    In this work, we present a data poisoning attack that confounds machine learning models without any manipulation of the image or label. This is achieved by simply leveraging the most confounding natural samples found within the training data itself, in a new form of a targeted attack coined "Mole Recruitment." We define moles as the training samples of a class that appear most similar to samples of another class, and show that simply restructuring training batches with an optimal number of moles can lead to significant degradation in the performance of the targeted class. We show the efficacy of this novel attack in an offline setting across several standard image classification datasets, and demonstrate the real-world viability of this attack in a continual learning (CL) setting. Our analysis reveals that state-of-the-art models are susceptible to Mole Recruitment, thereby exposing a previously undetected vulnerability of image classifiers.
    
[^52]: 偏微分方程的机器学习。 （arXiv：2303.17078v1 [cs.LG]）

    Machine Learning for Partial Differential Equations. (arXiv:2303.17078v1 [cs.LG])

    [http://arxiv.org/abs/2303.17078](http://arxiv.org/abs/2303.17078)

    本综述介绍了机器学习在偏微分方程研究中的应用，主要包括发现新的PDE、学习坐标系统和表示解算子，旨在使PDE更易于分析和改进传统的数值算法。

    

    偏微分方程（PDE）是自然物理定律中最普遍和简洁的描述之一，可以捕捉到大量不同尺度物理现象的丰富多样性并进行紧凑的符号表示。本综述将检查几个机器学习正在推进的有前途的PDE研究领域，包括：1）发现新的用于复杂自然和工程系统的主导PDE和粗粒化近似，2）学习有效的坐标系统和降阶模型，以使PDE更易分析，以及3）表示解算子和改进传统的数值算法。在每个领域，我们总结了关键进展、持续挑战和进一步发展的机会。

    Partial differential equations (PDEs) are among the most universal and parsimonious descriptions of natural physical laws, capturing a rich variety of phenomenology and multi-scale physics in a compact and symbolic representation. This review will examine several promising avenues of PDE research that are being advanced by machine learning, including: 1) the discovery of new governing PDEs and coarse-grained approximations for complex natural and engineered systems, 2) learning effective coordinate systems and reduced-order models to make PDEs more amenable to analysis, and 3) representing solution operators and improving traditional numerical algorithms. In each of these fields, we summarize key advances, ongoing challenges, and opportunities for further development.
    
[^53]: DiffCollage: 基于扩散模型的大尺寸内容并行生成

    DiffCollage: Parallel Generation of Large Content with Diffusion Models. (arXiv:2303.17076v1 [cs.CV])

    [http://arxiv.org/abs/2303.17076](http://arxiv.org/abs/2303.17076)

    DiffCollage是一种组合扩散模型，可以并行生成任意尺寸和形状的内容，应用于无限图像生成，全景图像生成和长时间文本引导运动生成等任务。

    

    我们提出了DiffCollage，一种组合扩散模型，可以利用已训练在生成大内容片段上的扩散模型来生成大尺寸内容。我们的方法基于因子图表示，其中每个因子节点表示内容的一部分，变量节点表示它们的重叠。该表示允许我们聚合在各个节点上定义的扩散模型的中间输出，以并行生成任意尺寸和形状的内容，而无需使用自回归生成过程。我们将DiffCollage应用于各种任务，包括无限图像生成，全景图像生成和长时间文本引导运动生成。与强自回归基线的比较的广泛实验结果验证了我们方法的有效性。

    We present DiffCollage, a compositional diffusion model that can generate large content by leveraging diffusion models trained on generating pieces of the large content. Our approach is based on a factor graph representation where each factor node represents a portion of the content and a variable node represents their overlap. This representation allows us to aggregate intermediate outputs from diffusion models defined on individual nodes to generate content of arbitrary size and shape in parallel without resorting to an autoregressive generation procedure. We apply DiffCollage to various tasks, including infinite image generation, panorama image generation, and long-duration text-guided motion generation. Extensive experimental results with a comparison to strong autoregressive baselines verify the effectiveness of our approach.
    
[^54]: DERA: 使用对话型解决代理提高大型语言模型的补全能力

    DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. (arXiv:2303.17071v1 [cs.CL])

    [http://arxiv.org/abs/2303.17071](http://arxiv.org/abs/2303.17071)

    本文介绍了一种名为DERA的对话型解决代理，该代理利用LLM的对话能力提高了模型的补全能力。DERA框架化为两个代理类型之间的讨论，可以在医疗对话摘要和护理计划生成方面实现显著改进。

    

    大型语言模型已成为许多自然语言理解任务的有价值工具。在医疗等关乎安全性的应用中，这些模型的实用性取决于它们生成的输出是否具备事实准确和完整性。本文介绍了对话型解决代理（DERA）。DERA是一种由LLM（特别是GPT-4）的对话能力提供支持的模式，它为模型提供了一个简单且易于理解的论坛，用于沟通反馈并迭代改进输出。我们将对话框架化为两个代理类型之间的讨论：一个研究人员，负责处理信息并识别关键问题组件；以及一个决策者，具有将研究人员的信息集成并对最终输出做出判定的自主权。我们将DERA用于三个临床相关任务的测试。在医疗对话摘要和护理计划生成方面，DERA显示出比基础GPT-4性能显著提高的效果。

    Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output.  We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance 
    
[^55]: 决策导向学习中的理想抽象

    Ideal Abstractions for Decision-Focused Learning. (arXiv:2303.17062v1 [cs.LG])

    [http://arxiv.org/abs/2303.17062](http://arxiv.org/abs/2303.17062)

    本论文提出了一种通过自动配置输出空间以最小化与决策相关信息的损失来制定简化抽象的机器学习系统的方法。

    

    我们提出了一种通过识别和利用决策的效用结构来制定简化抽象的机器学习系统的方法。机器学习任务通常涉及高维输出空间（例如图像中每个像素或图中节点的预测），尽管对于下游的决策制定来说，粗略的输出空间通常已经足够了（例如图像中的区域而不是像素）。开发者常常手工制定输出空间的抽象，但存在着众多的抽象形式，而且选择模型输出空间的影响对其在下游决策制定方面的实用性尚不清楚。我们提出了一种方法，自动配置输出空间以最小化与决策相关信息的损失。采用几何角度，我们将算法的一步作为概率单纯形的投影，称之为fold，以最小化决策相关信息在H-熵意义下的总损失。关键是，L…

    We present a methodology for formulating simplifying abstractions in machine learning systems by identifying and harnessing the utility structure of decisions. Machine learning tasks commonly involve high-dimensional output spaces (e.g., predictions for every pixel in an image or node in a graph), even though a coarser output would often suffice for downstream decision-making (e.g., regions of an image instead of pixels). Developers often hand-engineer abstractions of the output space, but numerous abstractions are possible and it is unclear how the choice of output space for a model impacts its usefulness in downstream decision-making. We propose a method that configures the output space automatically in order to minimize the loss of decision-relevant information. Taking a geometric perspective, we formulate a step of the algorithm as a projection of the probability simplex, termed fold, that minimizes the total loss of decision-related information in the H-entropy sense. Crucially, l
    
[^56]: 音视频分组网络用于混合声音定位

    Audio-Visual Grouping Network for Sound Localization from Mixtures. (arXiv:2303.17056v1 [cs.CV])

    [http://arxiv.org/abs/2303.17056](http://arxiv.org/abs/2303.17056)

    本文提出的 AVGN 网络利用音频-视觉样本和分组注意机制，发现类别感知的音频和视觉特征并对齐，以实现同时定位多个声源的目标。在两个基准测试上的结果表明，该方法在定位精度和对复杂环境的鲁棒性方面均优于现有方法。

    

    声源定位是一项典型且具有挑战性的任务，可预测视频中声源的位置。以往的单声源方法主要利用音视频关联作为线索，以定位每张图片中的声源对象。由于原空间中有多个声源的混合属性，除了一项近期使用对比随机游走在图像和分离声音作为节点的图中处理多个声源的方法之外，很少有多声源方法同时进行本地化。尽管它们表现出有希望的性能，但它们只能处理固定数量的声源，并且不能为各个声源学习紧凑的类知识表示。为了缓解这个缺点，本文提出了一种新颖的音视频分组网络，即 AVGN，它可以直接从输入的音频混合和图像中学习每个声源的类别语义特征，以多声源同时定位。具体而言，我们的 AVGN 利用可学习的音视频样本和分组注意机制，发现类别感知音频和视觉特征，并进一步对其进行对齐以实现视频中的多声源定位。我们在两个基准测试上评估了我们的方法，实验结果表明，我们的方法在定位精度和对复杂环境的鲁棒性方面均优于现有方法。

    Sound source localization is a typical and challenging task that predicts the location of sound sources in a video. Previous single-source methods mainly used the audio-visual association as clues to localize sounding objects in each image. Due to the mixed property of multiple sound sources in the original space, there exist rare multi-source approaches to localizing multiple sources simultaneously, except for one recent work using a contrastive random walk in the graph with images and separated sound as nodes. Despite their promising performance, they can only handle a fixed number of sources, and they cannot learn compact class-aware representations for individual sources. To alleviate this shortcoming, in this paper, we propose a novel audio-visual grouping network, namely AVGN, that can directly learn category-wise semantic features for each source from the input audio mixture and image to localize multiple sources simultaneously. Specifically, our AVGN leverages learnable audio-v
    
[^57]: 训练神经网络在固定维度上是NP难的

    Training Neural Networks is NP-Hard in Fixed Dimension. (arXiv:2303.17045v1 [cs.CC])

    [http://arxiv.org/abs/2303.17045](http://arxiv.org/abs/2303.17045)

    研究了训练具有ReLU和线性阈值激活函数的两层神经网络的固定维度下的NP难度。 回答了两个问题，证明了这两个问题在二维情况下是NP难的，此外在ReLU案例中证明了固定参数问题的参数化固定复杂度维数和ReLU数量的组合参数。

    

    我们研究了在输入数据维度和隐藏神经元数量方面对两层神经网络进行参数化复杂性的研究，考虑ReLU和线性阈值激活函数。尽管这些问题的计算复杂性近年来已经被多次研究，但仍有几个问题尚未解决。我们回答了Arora et al. [ICLR '18]和Khalife和Basu [IPCO '22]的问题，显示两个问题在二维情况下都是NP难的，这排除了任何常数维度的多项式时间算法。我们还回答了Froese等人[JAIR '22]的问题，证明了具有零培训误差的四个ReLU(或两个线性阈值神经元)的W [1]-hardness。最后，在ReLU案例中，我们展示了参数化固定复杂度维数和ReLU数量的组合参数，如果网络被假定为计算凸映射，则可用于固定参数问题。我们的结果几乎完全解决了这些参数的复杂性状况。

    We study the parameterized complexity of training two-layer neural networks with respect to the dimension of the input data and the number of hidden neurons, considering ReLU and linear threshold activation functions. Albeit the computational complexity of these problems has been studied numerous times in recent years, several questions are still open. We answer questions by Arora et al. [ICLR '18] and Khalife and Basu [IPCO '22] showing that both problems are NP-hard for two dimensions, which excludes any polynomial-time algorithm for constant dimension. We also answer a question by Froese et al. [JAIR '22] proving W[1]-hardness for four ReLUs (or two linear threshold neurons) with zero training error. Finally, in the ReLU case, we show fixed-parameter tractability for the combined parameter number of dimensions and number of ReLUs if the network is assumed to compute a convex map. Our results settle the complexity status regarding these parameters almost completely.
    
[^58]: 无观测上下文的联邦随机赌博学习

    Federated Stochastic Bandit Learning with Unobserved Context. (arXiv:2303.17043v1 [cs.LG])

    [http://arxiv.org/abs/2303.17043](http://arxiv.org/abs/2303.17043)

    本文提出了一种联邦随机多臂上下文赌博算法以最大化累积奖励，针对未知上下文的情况通过执行特征向量转换解决问题。

    

    本文研究了具有未知上下文的联邦随机多臂上下文赌博问题，其中M个代理面临不同的赌博机并协作学习。通信模型由中央服务器组成，并且代理会定期与中央服务器共享其估计结果，以便选择最优动作以最小化总后悔。我们假设精确的上下文不可观察，代理仅观测上下文的分布。例如，当上下文本身是噪声测量或基于预测机制时，就会出现这种情况。我们的目标是开发一种分布式联邦算法，促进代理之间的协作学习，选择一系列最优动作以最大化累积奖励。通过执行特征向量转换，我们提出了一种基于消除的算法，并证明了线性参数化奖励函数的后悔界。最后，我们验证了算法的性能。

    We study the problem of federated stochastic multi-arm contextual bandits with unknown contexts, in which M agents are faced with different bandits and collaborate to learn. The communication model consists of a central server and the agents share their estimates with the central server periodically to learn to choose optimal actions in order to minimize the total regret. We assume that the exact contexts are not observable and the agents observe only a distribution of the contexts. Such a situation arises, for instance, when the context itself is a noisy measurement or based on a prediction mechanism. Our goal is to develop a distributed and federated algorithm that facilitates collaborative learning among the agents to select a sequence of optimal actions so as to maximize the cumulative reward. By performing a feature vector transformation, we propose an elimination-based algorithm and prove the regret bound for linearly parametrized reward functions. Finally, we validated the perfo
    
[^59]: 沉浸感秘诀：基于演员的自动生成电影摄影机移动的探索

    The secret of immersion: actor driven camera movement generation for auto-cinematography. (arXiv:2303.17041v1 [cs.MM])

    [http://arxiv.org/abs/2303.17041](http://arxiv.org/abs/2303.17041)

    本文探讨了构成电影沉浸感的具体组成部分，并提出了一种基于演员驱动的摄像机移动生成系统，以实现情感和空间的沉浸感。

    

    沉浸感在设计电影时扮演着至关重要的角色，然而，沉浸式拍摄的困难阻碍了设计师创造出令人满意的成果。在本文中，我们分析了构成电影沉浸感的具体组成部分，考虑了空间、情感和美学等方面，同时这些组成部分被结合到了一个高级评估机制中。在这样的沉浸机制的指导下，我们提出了一种基于GAN的摄像机控制系统，能够在3D虚拟环境中生成基于演员驱动的摄像机移动，以获得沉浸式电影序列。生成过程中提出的编码器-解码器架构将演员运动转换为以情感因素为条件的摄像机轨迹，确保了演员与摄像机的物理和心理同步以实现空间和情感的沉浸感。通过加入控制摄像机抖动以表达不同心理状态的正则化，情感沉浸感得到了进一步加强。

    Immersion plays a vital role when designing cinematic creations, yet the difficulty in immersive shooting prevents designers to create satisfactory outputs. In this work, we analyze the specific components that contribute to cinematographic immersion considering spatial, emotional, and aesthetic level, while these components are then combined into a high-level evaluation mechanism. Guided by such a immersion mechanism, we propose a GAN-based camera control system that is able to generate actor-driven camera movements in the 3D virtual environment to obtain immersive film sequences. The proposed encoder-decoder architecture in the generation flow transfers character motion into camera trajectory conditioned on an emotion factor. This ensures spatial and emotional immersion by performing actor-camera synchronization physically and psychologically. The emotional immersion is further strengthened by incorporating regularization that controls camera shakiness for expressing different mental
    
[^60]: EPG-MGCN: 基于自我规划引导的多图卷积网络用于异构智能体轨迹预测

    EPG-MGCN: Ego-Planning Guided Multi-Graph Convolutional Network for Heterogeneous Agent Trajectory Prediction. (arXiv:2303.17027v1 [cs.LG])

    [http://arxiv.org/abs/2303.17027](http://arxiv.org/abs/2303.17027)

    EPG-MGCN是一种利用自我规划引导的多图卷积网络，通过建立社交交互模型，同时使用历史轨迹信息和自主车辆的未来规划来预测异构智能体的轨迹，以实现自动驾驶车辆的安全驾驶

    

    自动驾驶汽车需要准确预测周围异构交通智能体（如车辆、行人、自行车等）的未来轨迹，以确保安全驾驶。本文提出了一种基于自我规划引导的多图卷积网络（EPG-MGCN），利用历史轨迹信息和自主车辆的未来规划信息来预测异构智能体的轨迹。EPG-MGCN首先通过采用四种图形拓扑结构（距离图、能见度图、规划图和类别图）来建立社交交互模型。然后，自主车辆的规划信息通过规划图和随后的规划引导预测模块进行编码，以减少轨迹的不确定性。

    To drive safely in complex traffic environments, autonomous vehicles need to make an accurate prediction of the future trajectories of nearby heterogeneous traffic agents (i.e., vehicles, pedestrians, bicyclists, etc). Due to the interactive nature, human drivers are accustomed to infer what the future situations will become if they are going to execute different maneuvers. To fully exploit the impacts of interactions, this paper proposes a ego-planning guided multi-graph convolutional network (EPG-MGCN) to predict the trajectories of heterogeneous agents using both historical trajectory information and ego vehicle's future planning information. The EPG-MGCN first models the social interactions by employing four graph topologies, i.e., distance graphs, visibility graphs, planning graphs and category graphs. Then, the planning information of the ego vehicle is encoded by both the planning graph and the subsequent planning-guided prediction module to reduce uncertainty in the trajectory 
    
[^61]: 超扩散：用权重空间扩散生成隐式神经场

    HyperDiffusion: Generating Implicit Neural Fields with Weight-Space Diffusion. (arXiv:2303.17015v1 [cs.CV])

    [http://arxiv.org/abs/2303.17015](http://arxiv.org/abs/2303.17015)

    本文提出了一种名为HyperDiffusion的新方法，用于无条件生成建模的隐式神经场。该方法在MLP权重上操作，生成了新的神经隐式场，其中包含由合成的MLP参数编码的信息。

    

    隐式神经场通常由多层感知器(MLP)编码，将坐标(例如xyz)映射到信号(例如符号距离)，已显示出极高的精度和紧凑性。但是，缺少规则和明确的网格结构也使得直接在隐式神经场上应用生成建模以合成新数据变得具有挑战性。为此，我们提出了超扩散(HyperDiffusion)，这是一种用于隐式神经场无条件生成建模的新方法。HyperDiffusion直接在MLP权重上操作，并通过合成MLP参数生成新的神经隐式场。具体来说，首先优化一系列MLP以忠实地表示各个数据样本。随后，在MLP权重空间中训练扩散过程以对神经隐式场的基础分布进行建模。HyperDiffusion使得扩散建模在隐式，紧凑且高保真的表示上成为可能。

    Implicit neural fields, typically encoded by a multilayer perceptron (MLP) that maps from coordinates (e.g., xyz) to signals (e.g., signed distances), have shown remarkable promise as a high-fidelity and compact representation. However, the lack of a regular and explicit grid structure also makes it challenging to apply generative modeling directly on implicit neural fields in order to synthesize new data. To this end, we propose HyperDiffusion, a novel approach for unconditional generative modeling of implicit neural fields. HyperDiffusion operates directly on MLP weights and generates new neural implicit fields encoded by synthesized MLP parameters. Specifically, a collection of MLPs is first optimized to faithfully represent individual data samples. Subsequently, a diffusion process is trained in this MLP weight space to model the underlying distribution of neural implicit fields. HyperDiffusion enables diffusion modeling over a implicit, compact, and yet high-fidelity representatio
    
[^62]: 语义感知模仿学习的规范引导数据聚合

    Specification-Guided Data Aggregation for Semantically Aware Imitation Learning. (arXiv:2303.17010v1 [cs.LG])

    [http://arxiv.org/abs/2303.17010](http://arxiv.org/abs/2303.17010)

    本文提出了一种通过利用规范引导采样技术来聚合专业数据的方法，以语义感知的方式改进模仿学习模型，在自动驾驶领域实现了显着改进。

    

    仿真和正式方法引导的环境采样的发展使得在许多安全关键场景中，如自动驾驶中，机器学习模型得到了严格的评估。这些环境采样技术的应用，以改进学习的模型本身为目的，尚未得到充分的利用。在这项工作中，我们介绍了一种新的方法，通过利用规范引导采样技术作为聚合新环境中专业数据的手段来以语义感知的方式改进模仿学习的模型。具体而言，我们创建一组正式规范，作为将可能环境空间划分为语义相似区域的手段，并确定该划分中我们所学习的模仿行为与专家之间最不同的元素。然后在这些确定的区域内聚合专家数据，从而更准确地模仿专家的行为语义。我们将我们的方法实例化到自动驾驶的背景下，显示出比传统的模仿学习技术有显着的改进。

    Advancements in simulation and formal methods-guided environment sampling have enabled the rigorous evaluation of machine learning models in a number of safety-critical scenarios, such as autonomous driving. Application of these environment sampling techniques towards improving the learned models themselves has yet to be fully exploited. In this work, we introduce a novel method for improving imitation-learned models in a semantically aware fashion by leveraging specification-guided sampling techniques as a means of aggregating expert data in new environments. Specifically, we create a set of formal specifications as a means of partitioning the space of possible environments into semantically similar regions, and identify elements of this partition where our learned imitation behaves most differently from the expert. We then aggregate expert data on environments in these identified regions, leading to more accurate imitation of the expert's behavior semantics. We instantiate our approa
    
[^63]: 组织病理学染色转移的图像到图像翻译方法的比较评估

    A comparative evaluation of image-to-image translation methods for stain transfer in histopathology. (arXiv:2303.17009v1 [eess.IV])

    [http://arxiv.org/abs/2303.17009](http://arxiv.org/abs/2303.17009)

    本文比较了12种图像到图像翻译方法在组织病理学染色转移方面的应用，证明了一些方法优于现有技术，可用于大规模染色组织病理学图像的合成。

    

    图像到图像翻译（I2I）方法允许生成具有不同风格但与原始图像共享内容的人工图像。近年来，随着基于生成对抗网络（GANs）的方法的进展，I2I方法实现了生成与自然图像无法区分的人工图像。最近，针对组织病理学中不同类型的染色，I2I方法也被用于生成模拟的染色组织的人工图像。我们将这个过程称为染色转移。由于I2I变种的数量不断增加，这使得选择最合适的I2I方法进行染色转移变得困难。在我们的工作中，我们比较了12种染色转移方法，其中3种基于传统方法，9种基于GAN图像处理方法。分析依赖于定量评估图像翻译质量的补充措施，以及对深度学习组织成像和医学诊断的适用性的评估，以及经验丰富的病理学家的定性评估。我们的实验表明，有几种I2I方法优于染色转移领域的现有技术，并可用于大规模染色组织病理学图像的合成。

    Image-to-image translation (I2I) methods allow the generation of artificial images that share the content of the original image but have a different style. With the advances in Generative Adversarial Networks (GANs)-based methods, I2I methods enabled the generation of artificial images that are indistinguishable from natural images. Recently, I2I methods were also employed in histopathology for generating artificial images of in silico stained tissues from a different type of staining. We refer to this process as stain transfer. The number of I2I variants is constantly increasing, which makes a well justified choice of the most suitable I2I methods for stain transfer challenging. In our work, we compare twelve stain transfer approaches, three of which are based on traditional and nine on GAN-based image processing methods. The analysis relies on complementary quantitative measures for the quality of image translation, the assessment of the suitability for deep learning-based tissue gra
    
[^64]: 在巴西大学入学考试中评估GPT-3.5和GPT-4模型

    Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams. (arXiv:2303.17003v1 [cs.CL])

    [http://arxiv.org/abs/2303.17003](http://arxiv.org/abs/2303.17003)

    本研究在巴西大学入学考试中评估了GPT-3.5和GPT-4模型，分析了不同提示策略，最终发现GPT-4与Chain-of-Thought提示结合表现最好，在2022年考试中准确率达到了87％。

    

    本研究旨在探索语言模型（LMs）在应对高风险的多项选择测试中的能力，这里以巴西大学广泛采用的多学科入学考试Exame Nacional do Ensino Médio（ENEM）为例。该考试对LMs提出了挑战，因为其问题可能涉及多个知识领域，需要理解来自不同领域的信息。例如，一个问题可能需要理解统计学和生物学才能解决。本研究分析了GPT-3.5和GPT-4模型对2009年至2017年考试以及2022年公开的考试问题的响应。此外，还测试了不同的提示策略，包括使用Chain-of-Thought（CoT）提示生成答案的解释。在2022年的考试中，表现最佳的模型是GPT-4并使用了CoT，在准确率方面达到了87％。

    The present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M\'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%,
    
[^65]: G不变图拉普拉斯算子

    The G-invariant graph Laplacian. (arXiv:2303.17001v1 [cs.LG])

    [http://arxiv.org/abs/2303.17001](http://arxiv.org/abs/2303.17001)

    本文提出了 G不变图拉普拉斯算子 用于处理数据集不仅在流形上，而且在一个连续群的作用下也是封闭的情形，相较于标准图拉普拉斯算子收敛速度更快。

    

    基于图拉普拉斯算子的算法已经被证明在降维、聚类和去噪等领域对流形数据非常有效。本文考虑的数据集不仅在流形上，而且在一个连续群的作用下也是封闭的。这类数据集的一个例子是沿着低维流形传播的体积，其中每个体积可以在三维空间中旋转。我们介绍了G不变图拉普拉斯算子，通过考虑数据集上的群的作用来广义化图拉普拉斯算子。我们显示了与标准图拉普拉斯算子类似，G不变图拉普拉斯算子收敛于数据流形上的Laplace-Beltrami算子，但收敛速度显著提高。此外，我们还表明G不变图拉普拉斯算子的特征函数具有群元素和某些矩阵的特征向量的张量积形式，可以使用F高效地计算。

    Graph Laplacian based algorithms for data lying on a manifold have been proven effective for tasks such as dimensionality reduction, clustering, and denoising. In this work, we consider data sets whose data point not only lie on a manifold, but are also closed under the action of a continuous group. An example of such data set is volumes that line on a low dimensional manifold, where each volume may be rotated in three-dimensional space. We introduce the G-invariant graph Laplacian that generalizes the graph Laplacian by accounting for the action of the group on the data set. We show that like the standard graph Laplacian, the G-invariant graph Laplacian converges to the Laplace-Beltrami operator on the data manifold, but with a significantly improved convergence rate. Furthermore, we show that the eigenfunctions of the G-invariant graph Laplacian admit the form of tensor products between the group elements and eigenvectors of certain matrices, which can be computed efficiently using F
    
[^66]: PopSparse：在IPU上加速的块稀疏矩阵乘法

    PopSparse: Accelerated block sparse matrix multiplication on IPU. (arXiv:2303.16999v1 [cs.LG])

    [http://arxiv.org/abs/2303.16999](http://arxiv.org/abs/2303.16999)

    将大规模神经网络的计算成本降低到可接受的任务性能和加速改进是一个具有挑战的问题。本文介绍了一种利用IPU独特的硬件特性和数据中定义的任何块结构实现在Graphcore IPUs上快速稀疏操作的库——PopSparse。

    

    在深度学习社区中，使用稀疏性降低大规模神经网络的计算成本引起了广泛关注。虽然在维持可接受的任务性能的同时减少FLOP和参数数量取得了许多成功，但实际上获得加速改进通常更加困难，特别是在通用加速器（GPA）上，如使用低精度数字格式的NVIDIA GPU。在本文中，我们介绍了PopSparse，一种利用IPU的独特硬件特性以及数据中定义的任何块结构实现在Graphcore IPUs上快速稀疏操作的库。我们针对两种不同类型的稀疏性：静态稀疏性，其中稀疏模式在编译时固定；动态稀疏性，其中每次运行模型时都可以改变。我们针对各种块大小、矩阵大小和密度在IPU上进行矩阵乘法的基准测试。结果表明：

    Reducing the computational cost of running large scale neural networks using sparsity has attracted great attention in the deep learning community. While much success has been achieved in reducing FLOP and parameter counts while maintaining acceptable task performance, achieving actual speed improvements has typically been much more difficult, particularly on general purpose accelerators (GPAs) such as NVIDIA GPUs using low precision number formats. In this work we introduce PopSparse, a library that enables fast sparse operations on Graphcore IPUs by leveraging both the unique hardware characteristics of IPUs as well as any block structure defined in the data. We target two different types of sparsity: static, where the sparsity pattern is fixed at compile-time; and dynamic, where it can change each time the model is run. We present benchmark results for matrix multiplication for both of these modes on IPU with a range of block sizes, matrix sizes and densities. Results indicate that 
    
[^67]: 稀疏性是否有助于学习不正确的线性赌博机？

    Does Sparsity Help in Learning Misspecified Linear Bandits?. (arXiv:2303.16998v1 [cs.LG])

    [http://arxiv.org/abs/2303.16998](http://arxiv.org/abs/2303.16998)

    本文研究了稀疏性在解决不正确线性赌博机问题中的作用，证明了算法可以通过查询$ O(\varepsilon^{-s}d^s) $个操作来获得$O(\varepsilon)$-最优行动，其中$s$是稀疏性参数。

    

    最近，学习线性不正确赌博机已经产生了对学习赌博机和强化学习（RL）的难度的有趣影响。具体而言，Du等人（2020）表明，即使学习者被赋予在$ \mathbb{R}^d$ 中近似赌博机或RL奖励的线性特征，且误差在$\varepsilon$的范围内，寻找一个$ O（\varepsilon）$ -最优行动需要至少拉出$ \Omega(\exp(d)) $的查询。此外，Lattimore等人（2020）展示了如何在$\operatorname{poly}(d/\varepsilon)$的查询中学习得到退化的$O(\varepsilon\sqrt{d})$ -最优解决方案。然而，关于实际参数的结构假设，如稀疏性，是否能打破$\varepsilon\sqrt{d}$的障碍仍不清楚。本文通过展示算法可以通过查询$ O(\varepsilon^{-s}d^s) $个操作来获得$O(\varepsilon)$-最优行动，其中$s$是稀疏性参数，以消除$ \exp(d)$-依赖性解决了这个问题。

    Recently, the study of linear misspecified bandits has generated intriguing implications of the hardness of learning in bandits and reinforcement learning (RL). In particular, Du et al. (2020) show that even if a learner is given linear features in $\mathbb{R}^d$ that approximate the rewards in a bandit or RL with a uniform error of $\varepsilon$, searching for an $O(\varepsilon)$-optimal action requires pulling at least $\Omega(\exp(d))$ queries. Furthermore, Lattimore et al. (2020) show that a degraded $O(\varepsilon\sqrt{d})$-optimal solution can be learned within $\operatorname{poly}(d/\varepsilon)$ queries. Yet it is unknown whether a structural assumption on the ground-truth parameter, such as sparsity, could break the $\varepsilon\sqrt{d}$ barrier. In this paper, we address this question by showing that algorithms can obtain $O(\varepsilon)$-optimal actions by querying $O(\varepsilon^{-s}d^s)$ actions, where $s$ is the sparsity parameter, removing the $\exp(d)$-dependence. We th
    
[^68]: 使用Uni-Mol+实现高精度的量子化学属性预测

    Highly Accurate Quantum Chemical Property Prediction with Uni-Mol+. (arXiv:2303.16982v1 [physics.chem-ph])

    [http://arxiv.org/abs/2303.16982](http://arxiv.org/abs/2303.16982)

    本文提出了一种使用Uni-Mol+的新方法来高精度预测量子化学属性，它能够从2D分子图自动生成3D构象，并通过迭代优化得到优化后的构象，为预测QC属性提供更加准确的基础。

    

    深度学习的最新发展在加速预测量子化学（QC）属性方面取得了显著进展，它消除了密度泛函理论等昂贵电子结构计算的需求。但是，以1D SMILES序列或2D分子图为基础的之前的方法未能实现高精度，因为QC属性主要取决于经电子结构方法优化的3D平衡构象。本文提出了一种名为Uni-Mol+的新方法来应对这一挑战。首先，给定一个2D分子图，Uni-Mol+从RDKit等廉价方法生成初始的3D构象。然后，初始构象被迭代地优化到其平衡构象，并进一步用于预测QC属性。所有这些步骤都是使用Transformer模型自动学习的。我们观察到，优化后的构象质量对QC属性预测至关重要。

    Recent developments in deep learning have made remarkable progress in speeding up the prediction of quantum chemical (QC) properties by removing the need for expensive electronic structure calculations like density functional theory. However, previous methods that relied on 1D SMILES sequences or 2D molecular graphs failed to achieve high accuracy as QC properties are primarily dependent on the 3D equilibrium conformations optimized by electronic structure methods. In this paper, we propose a novel approach called Uni-Mol+ to tackle this challenge. Firstly, given a 2D molecular graph, Uni-Mol+ generates an initial 3D conformation from inexpensive methods such as RDKit. Then, the initial conformation is iteratively optimized to its equilibrium conformation, and the optimized conformation is further used to predict the QC properties. All these steps are automatically learned using Transformer models. We observed the quality of the optimized conformation is crucial for QC property predict
    
[^69]: 多项式分类中的稀疏联合偏移

    Sparse joint shift in multinomial classification. (arXiv:2303.16971v1 [stat.ML])

    [http://arxiv.org/abs/2303.16971](http://arxiv.org/abs/2303.16971)

    该论文提出了一种稀疏联合偏移模型，用于解决整体数据集偏移问题，提供了传递SJS、修正类后验概率、SJS的可辨认性、SJS与协变量转移关系等新结果。

    

    稀疏联合偏移（SJS）是一种针对数据集整体偏移的可处理模型，可能会导致特征和标签的边际分布以及后验概率和类条件特征分布的变化。在没有标签观测的情况下，为目标数据集拟合SJS可能会产生标签的有效预测和类先验概率的估计。我们在特征集之间传递SJS方面提供了新的结果，提出了一个基于目标分布的类后验概率的条件修正公式，确定性SJS的可辨认性以及SJS和协变量转移之间的关系。此外，我们指出了用于估计SJS特征的算法中的不一致性，因为它们可能会妨碍寻找最优解。

    Sparse joint shift (SJS) was recently proposed as a tractable model for general dataset shift which may cause changes to the marginal distributions of features and labels as well as the posterior probabilities and the class-conditional feature distributions. Fitting SJS for a target dataset without label observations may produce valid predictions of labels and estimates of class prior probabilities. We present new results on the transmission of SJS from sets of features to larger sets of features, a conditional correction formula for the class posterior probabilities under the target distribution, identifiability of SJS, and the relationship between SJS and covariate shift. In addition, we point out inconsistencies in the algorithms which were proposed for estimating the characteristics of SJS, as they could hamper the search for optimal solutions.
    
[^70]: 训练数据的公平价值评估

    Fairness-Aware Data Valuation for Supervised Learning. (arXiv:2303.16963v1 [cs.LG])

    [http://arxiv.org/abs/2303.16963](http://arxiv.org/abs/2303.16963)

    本篇论文提出了一个名为FADO的数据评估框架，旨在同时最大化性能和公平性，可以用于一系列机器学习任务中。该框架使用的基于熵的数据评估指标比现有指标计算效率更高，并可用于不公平性缓解预处理技术。

    

    数据评估是研究训练实例对于给定预测任务价值的机器学习领域。虽然数据偏见是下游模型不公平的主要来源之一，但以往的数据评估工作并未考虑训练实例如何影响机器学习模型的性能和公平性。因此，我们提出了一种公平感知数据评估（FADO）框架，该框架可用于将公平性考虑纳入一系列与机器学习相关的任务中（例如数据预处理、探索性数据分析、主动学习）。我们提出了基于熵的数据评估指标，适用于解决我们的两个目标：最大化性能和公平性。该指标比现有指标更具计算效率。然后，我们展示了如何将FADO应用为不公平性缓解预处理技术的基础。我们的方法取得了有希望的结果——与基线相比，在性能损失小于1 p.p.的情况下，公平性提高了多达40 p.p.。

    Data valuation is a ML field that studies the value of training instances towards a given predictive task. Although data bias is one of the main sources of downstream model unfairness, previous work in data valuation does not consider how training instances may influence both performance and fairness of ML models. Thus, we propose Fairness-Aware Data vauatiOn (FADO), a data valuation framework that can be used to incorporate fairness concerns into a series of ML-related tasks (e.g., data pre-processing, exploratory data analysis, active learning). We propose an entropy-based data valuation metric suited to address our two-pronged goal of maximizing both performance and fairness, which is more computationally efficient than existing metrics. We then show how FADO can be applied as the basis for unfairness mitigation pre-processing techniques. Our methods achieve promising results -- up to a 40 p.p. improvement in fairness at a less than 1 p.p. loss in performance compared to a baseline 
    
[^71]: FeDiSa: 一种半异步联邦学习框架，用于电力系统故障和网络攻击判别

    FeDiSa: A Semi-asynchronous Federated Learning Framework for Power System Fault and Cyberattack Discrimination. (arXiv:2303.16956v1 [cs.CR])

    [http://arxiv.org/abs/2303.16956](http://arxiv.org/abs/2303.16956)

    FeDiSa是一种半异步联邦学习框架，用于电力系统故障和网络攻击判别，采用深度自编码器来实现协同训练，能够在保护隐私的前提下进行攻击检测模型的协同训练。

    

    随着智能电网领域的安全和隐私关切日益增加，近年来针对关键能源基础设施的入侵检测变得非常重要。为了解决隐私保护和分散的电力区域所面临的挑战，联邦学习（FL）已成为一种可行的隐私保护替代方案，它使得攻击检测模型的协同训练成为可能，而无需共享原始数据。为了解决传统同步FL所面临的技术挑战，本文提出了FeDiSa，这是一种针对电力系统故障和网络攻击判别的半异步联邦学习框架，考虑到通信延迟和低效的节点。具体而言，我们提出了一种由监控控制和数据采集子系统协同训练深度自编码器的方法，它们将本地模型更新上传到控制中心，然后执行半异步模型聚合。

    With growing security and privacy concerns in the Smart Grid domain, intrusion detection on critical energy infrastructure has become a high priority in recent years. To remedy the challenges of privacy preservation and decentralized power zones with strategic data owners, Federated Learning (FL) has contemporarily surfaced as a viable privacy-preserving alternative which enables collaborative training of attack detection models without requiring the sharing of raw data. To address some of the technical challenges associated with conventional synchronous FL, this paper proposes FeDiSa, a novel Semi-asynchronous Federated learning framework for power system faults and cyberattack Discrimination which takes into account communication latency and stragglers. Specifically, we propose a collaborative training of deep auto-encoder by Supervisory Control and Data Acquisition sub-systems which upload their local model updates to a control centre, which then perform a semi-asynchronous model ag
    
[^72]: 一种使用量子门的生成建模方法

    A Generative Modeling Approach Using Quantum Gates. (arXiv:2303.16955v1 [quant-ph])

    [http://arxiv.org/abs/2303.16955](http://arxiv.org/abs/2303.16955)

    本文提出了一种使用量子门生成新样本的生成建模方法，并在不同数据集上进行了实验证明其有效性。

    

    近些年来，量子计算作为解决复杂计算问题的有前途的技术，开始变得越来越流行。生成建模是一种技术，可以让我们学习并生成类似于原数据集的新数据样本。本文提出了一种使用量子门生成新样本的生成建模方法。我们从简要介绍量子计算和生成建模开始。接着，我们描述了我们的方法，这种方法涉及到将数据集编码成量子态，并使用量子门来操作这些状态以生成新样本。我们还提供了我们方法的数学细节，并通过在各种数据集上的实验结果来证明其有效性。

    In recent years, quantum computing has emerged as a promising technology for solving complex computational problems. Generative modeling is a technique that allows us to learn and generate new data samples similar to the original dataset. In this paper, we propose a generative modeling approach using quantum gates to generate new samples from a given dataset. We start with a brief introduction to quantum computing and generative modeling. Then, we describe our proposed approach, which involves encoding the dataset into quantum states and using quantum gates to manipulate these states to generate new samples. We also provide mathematical details of our approach and demonstrate its effectiveness through experimental results on various datasets.
    
[^73]: 利用联合稀疏性的分层贝叶斯学习方法

    Leveraging joint sparsity in hierarchical Bayesian learning. (arXiv:2303.16954v1 [stat.ML])

    [http://arxiv.org/abs/2303.16954](http://arxiv.org/abs/2303.16954)

    本文提出了一种分层贝叶斯学习方法，用于从多个测量向量中推断联合稀疏的参数向量，该方法使用共同的伽马分布超参数来强制联合稀疏性，并在实验中进行了验证。

    

    我们提出了一种分层贝叶斯学习方法，从多个测量向量中推断联合稀疏的参数向量。我们的模型为每个参数向量使用单独的条件高斯先验，并使用共同的伽马分布超参数来强制联合稀疏性。得到的联合稀疏性先验与现有的贝叶斯推断方法相结合，形成了一系列新算法。我们的数值实验，包括多线圈磁共振成像应用，证明了我们的新方法始终优于常用的分层贝叶斯方法。

    We present a hierarchical Bayesian learning approach to infer jointly sparse parameter vectors from multiple measurement vectors. Our model uses separate conditionally Gaussian priors for each parameter vector and common gamma-distributed hyper-parameters to enforce joint sparsity. The resulting joint-sparsity-promoting priors are combined with existing Bayesian inference methods to generate a new family of algorithms. Our numerical experiments, which include a multi-coil magnetic resonance imaging application, demonstrate that our new approach consistently outperforms commonly used hierarchical Bayesian methods.
    
[^74]: 使用可微凸优化元学习参数化的一阶优化器

    Meta-Learning Parameterized First-Order Optimizers using Differentiable Convex Optimization. (arXiv:2303.16952v1 [cs.LG])

    [http://arxiv.org/abs/2303.16952](http://arxiv.org/abs/2303.16952)

    该研究提出了使用可微凸优化的元学习框架，将现有的一阶更新规则推广到更广的家族，证明在元学习者有足够类似任务的经验下，可以一步优化一系列线性最小二乘问题。

    

    机器学习和控制中的传统优化方法主要依赖于一阶更新规则。针对某个任务选择合适方法和超参数常常需要试错或从业者直觉，这促进了元学习领域的发展。我们通过提出内循环优化步骤涉及可微凸优化(DCO)的元学习框架，推广了一个广泛的现有更新规则家族。我们通过展示此方法的理论吸引力，证明了在元学习者有足够的类似任务经验的情况下，它可以一步优化一系列线性最小二乘问题。在一系列说明性实验设置中，将DCO更新规则的各种实例与传统优化器进行了比较。

    Conventional optimization methods in machine learning and controls rely heavily on first-order update rules. Selecting the right method and hyperparameters for a particular task often involves trial-and-error or practitioner intuition, motivating the field of meta-learning. We generalize a broad family of preexisting update rules by proposing a meta-learning framework in which the inner loop optimization step involves solving a differentiable convex optimization (DCO). We illustrate the theoretical appeal of this approach by showing that it enables one-step optimization of a family of linear least squares problems, given that the meta-learner has sufficient exposure to similar tasks. Various instantiations of the DCO update rule are compared to conventional optimizers on a range of illustrative experimental settings.
    
[^75]: 解耦和去定位密集自监督学习

    De-coupling and De-positioning Dense Self-supervised Learning. (arXiv:2303.16947v1 [cs.CV])

    [http://arxiv.org/abs/2303.16947](http://arxiv.org/abs/2303.16947)

    该论文介绍了一种解决密集自监督学习中耦合和位置偏差问题的方法，通过引入数据增强策略和解耦与去定位模块实现这一目标，实验证明该方法在目标分类、语义分割和目标检测等任务上具有更好的泛化性能。

    

    密集自监督学习方法解决了处理具有多个物体的图像时使用图像级特征表示的限制。虽然利用分割映射和边界框提取的密集特征允许网络对每个物体执行自监督学习，但我们证明它们会受到耦合和位置偏差的影响。这是由于感受野随着层深度和零填充而增加所导致的。为解决这个问题，我们引入了三种数据增强策略，并在解耦模块和去定位模块中利用它们。我们的方法在COCO和一个新的具有挑战性的基准OpenImage-MINI上进行了实验，用于目标分类、语义分割和目标检测。我们广泛的实验证明了我们的方法相比SOT具有更好的泛化性能。

    Dense Self-Supervised Learning (SSL) methods address the limitations of using image-level feature representations when handling images with multiple objects. Although the dense features extracted by employing segmentation maps and bounding boxes allow networks to perform SSL for each object, we show that they suffer from coupling and positional bias, which arise from the receptive field increasing with layer depth and zero-padding. We address this by introducing three data augmentation strategies, and leveraging them in (i) a decoupling module that aims to robustify the network to variations in the object's surroundings, and (ii) a de-positioning module that encourages the network to discard positional object information. We demonstrate the benefits of our method on COCO and on a new challenging benchmark, OpenImage-MINI, for object classification, semantic segmentation, and object detection. Our extensive experiments evidence the better generalization of our method compared to the SOT
    
[^76]: 神经架构搜索基准测试是否设计良好？对操作重要性的深入研究

    Are Neural Architecture Search Benchmarks Well Designed? A Deeper Look Into Operation Importance. (arXiv:2303.16938v1 [cs.LG])

    [http://arxiv.org/abs/2303.16938](http://arxiv.org/abs/2303.16938)

    本论文对当前广泛使用的NAS基准测试进行了经验研究，发现只需一小部分的操作即可生成接近最高性能的架构，同时这些基准测试存在缺点可能影响公平比较并提供不可靠结果。

    

    神经架构搜索（NAS）基准测试显著提高了开发和比较NAS方法的能力，同时通过提供关于数千个训练过的神经网络的元信息，大幅减少了计算开销。然而，表格基准测试具有几个缺点，可能会阻碍公平比较并提供不可靠的结果。在这项工作中，我们对广泛使用的NAS-Bench-101、NAS-Bench-201和TransNAS-Bench-101基准测试进行了经验性分析，重点关注它们的通用性以及不同操作如何影响所生成架构的性能。我们发现，仅需要操作池的一部分即可生成接近最高性能范围的架构。此外，性能分布具有负偏斜。

    Neural Architecture Search (NAS) benchmarks significantly improved the capability of developing and comparing NAS methods while at the same time drastically reduced the computational overhead by providing meta-information about thousands of trained neural networks. However, tabular benchmarks have several drawbacks that can hinder fair comparisons and provide unreliable results. These usually focus on providing a small pool of operations in heavily constrained search spaces -- usually cell-based neural networks with pre-defined outer-skeletons. In this work, we conducted an empirical analysis of the widely used NAS-Bench-101, NAS-Bench-201 and TransNAS-Bench-101 benchmarks in terms of their generability and how different operations influence the performance of the generated architectures. We found that only a subset of the operation pool is required to generate architectures close to the upper-bound of the performance range. Also, the performance distribution is negatively skewed, havi
    
[^77]: 基于深度学习和XAI的基因组学特征选择新算法

    A New Deep Learning and XAI-Based Algorithm for Features Selection in Genomics. (arXiv:2303.16914v1 [q-bio.GN])

    [http://arxiv.org/abs/2303.16914](http://arxiv.org/abs/2303.16914)

    本文提出了一种基于深度学习和可解释人工智能的新算法来选择基因组数据中最有信息的特征，为诊断、预后和精准医学提供支持，并在慢性淋巴细胞白血病数据集上取得了有效结果。

    

    在功能基因组学领域，机器学习和深度学习对基因表达谱的分析越来越能够提供有意义的洞察力，特别是对于多种疾病的分析。本文提出了一种新的算法，利用自编码器的重构能力和一种专门定义的可解释人工智能基础分数，以选择作为诊断、预后和精准医学最具信息量的基因组数据特征。在慢性淋巴细胞白血病数据集上的应用结果表明，该算法的有效性，通过确定并提供一组具有意义的基因组数据特征，有助于进行进一步的医学调查。

    In the field of functional genomics, the analysis of gene expression profiles through Machine and Deep Learning is increasingly providing meaningful insight into a number of diseases. The paper proposes a novel algorithm to perform Feature Selection on genomic-scale data, which exploits the reconstruction capabilities of autoencoders and an ad-hoc defined Explainable Artificial Intelligence-based score in order to select the most informative genes for diagnosis, prognosis, and precision medicine. Results of the application on a Chronic Lymphocytic Leukemia dataset evidence the effectiveness of the algorithm, by identifying and suggesting a set of meaningful genes for further medical investigation.
    
[^78]: 使用贝叶斯超启发式算法训练前馈神经网络

    Training Feedforward Neural Networks with Bayesian Hyper-Heuristics. (arXiv:2303.16912v1 [cs.LG])

    [http://arxiv.org/abs/2303.16912](http://arxiv.org/abs/2303.16912)

    本研究提出了一种用于训练前馈神经网络的Population-based Bayesian hyper-heuristic算法，可以在训练过程的不同阶段自动寻找最佳的启发式算法来训练网络。

    

    训练前馈神经网络(FFNNs)的过程可以从自动化过程中受益，其中最佳的启发式算法通过高级基于概率的启发式算法自动寻找来训练网络。本研究引入了一种新型Population-based Bayesian hyper-heuristic (BHH)，用于训练前馈神经网络(FFNNs)。BHH的性能与十种流行的低级启发式方法进行了比较，每种方法都具有不同的查找行为。所选的启发式池包括经典的基于梯度的启发式方法以及元启发式( MHs)。实证过程对十四个具有不同性质的分类和回归问题的数据集进行了执行。研究表明，BHH能够很好地执行FFNNs的训练，并提供了一种自动化方法来在训练过程的各个阶段中寻找最佳的启发式算法。

    The process of training feedforward neural networks (FFNNs) can benefit from an automated process where the best heuristic to train the network is sought out automatically by means of a high-level probabilistic-based heuristic. This research introduces a novel population-based Bayesian hyper-heuristic (BHH) that is used to train feedforward neural networks (FFNNs). The performance of the BHH is compared to that of ten popular low-level heuristics, each with different search behaviours. The chosen heuristic pool consists of classic gradient-based heuristics as well as meta-heuristics (MHs). The empirical process is executed on fourteen datasets consisting of classification and regression problems with varying characteristics. The BHH is shown to be able to train FFNNs well and provide an automated method for finding the best heuristic to train the FFNNs at various stages of the training process.
    
[^79]: CADM：基于混淆模型的块数据流实时漂移检测方法

    CADM: Confusion Model-based Detection Method for Real-drift in Chunk Data Stream. (arXiv:2303.16906v1 [cs.LG])

    [http://arxiv.org/abs/2303.16906](http://arxiv.org/abs/2303.16906)

    本文提出一种基于概念混淆的块数据流实时漂移检测新方法，利用真实和伪标签更新模型并使用余弦相似度测量漂移，成功降低了误报和漏报率。

    

    概念漂移检测因其在健康监控和故障诊断等许多实际应用中的重要性而受到广泛关注。然而，传统的漂移检测方法在环境评估标准发生变化（即概念漂移）时往往表现不佳。本文提出了一种基于概念混淆的有限注释块数据流实时漂移检测新方法。当新的数据块到达时，我们使用真实标签和伪标签在预测和漂移检测后更新模型。在这种情况下，模型一旦发生漂移就会产生预测差异。我们使用余弦相似度来度量差异，并提出一种自适应阈值方法来找到异常值。实验证明，该方法在利用不同分类器时具有较低的误报率和漏报率。

    Concept drift detection has attracted considerable attention due to its importance in many real-world applications such as health monitoring and fault diagnosis. Conventionally, most advanced approaches will be of poor performance when the evaluation criteria of the environment has changed (i.e. concept drift), either can only detect and adapt to virtual drift. In this paper, we propose a new approach to detect real-drift in the chunk data stream with limited annotations based on concept confusion. When a new data chunk arrives, we use both real labels and pseudo labels to update the model after prediction and drift detection. In this context, the model will be confused and yields prediction difference once drift occurs. We then adopt cosine similarity to measure the difference. And an adaptive threshold method is proposed to find the abnormal value. Experiments show that our method has a low false alarm rate and false negative rate with the utilization of different classifiers.
    
[^80]: 基于机器学习的自旋结构检测

    Machine learning-based spin structure detection. (arXiv:2303.16905v1 [cs.LG])

    [http://arxiv.org/abs/2303.16905](http://arxiv.org/abs/2303.16905)

    该论文报道了使用机器学习算法检测磁性自旋结构的研究。作者提出了一个卷积神经网络，其可针对分割问题检测天线的位置和形状，具有更高的自动化效率，据此有望为内存和神经形态计算方案提供可行性支持。

    

    最重要的磁性自旋结构之一是拓扑稳定的天线状准粒子。其有趣的物理属性使其成为内存和高效神经形态计算方案的候选。对于器件操作，需要检测天线的位置、形状和尺寸，通常采用磁性成像。常用技术是磁光Kerr显微镜，该技术根据样品的材料组成、温度、材料生长过程等，测量会受到噪声、低对比度、强度梯度或其他光学伪像的影响。传统的图像分析软件需要手动处理，需要更自动化的解决方案。我们报道了一个卷积神经网络，专门用于分割问题，以检测我们测量中的天线位置和形状。利用选择的技术调整网络以优化预测，特别是检测到的团簇的数量。

    One of the most important magnetic spin structure is the topologically stabilised skyrmion quasi-particle. Its interesting physical properties make them candidates for memory and efficient neuromorphic computation schemes. For the device operation, detection of the position, shape, and size of skyrmions is required and magnetic imaging is typically employed. A frequently used technique is magneto-optical Kerr microscopy where depending on the samples material composition, temperature, material growing procedures, etc., the measurements suffer from noise, low-contrast, intensity gradients, or other optical artifacts. Conventional image analysis packages require manual treatment, and a more automatic solution is required. We report a convolutional neural network specifically designed for segmentation problems to detect the position and shape of skyrmions in our measurements. The network is tuned using selected techniques to optimize predictions and in particular the number of detected cl
    
[^81]: 通过二维卷积神经网络和肺部CT扫描对地玻璃影进行严重程度分类：一个三天的探索

    Severity classification of ground-glass opacity via 2-D convolutional neural network and lung CT scans: a 3-day exploration. (arXiv:2303.16904v1 [eess.IV])

    [http://arxiv.org/abs/2303.16904](http://arxiv.org/abs/2303.16904)

    本研究探讨了使用二维卷积神经网络技术对肺部CT扫描进行严重程度分类，为诊断肺部疾病如COVID-19和肺炎提供新的方法。

    

    地玻璃影是许多肺部疾病的标志，包括COVID19和肺炎患者。本研究使用新建的虚拟环境，在2023年3月17日创建，通过实验探讨了各种预训练的二维卷积神经网络，如稠密神经网络、ResNet和视觉变换器，以及微调的程度。根据经验实验，我们选择使用ADAM优化算法对所有CNN架构进行标准学习率0.001的微调，并在验证损失达到平原时进行早停。为每个训练的CNN制定严重程度分类规则，并在三天内进行了实验测试。

    Ground-glass opacity is a hallmark of numerous lung diseases, including patients with COVID19 and pneumonia. This brief note presents experimental results of a proof-of-concept framework that got implemented and tested over three days as driven by the third challenge entitled "COVID-19 Competition", hosted at the AI-Enabled Medical Image Analysis Workshop of the 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2023). Using a newly built virtual environment (created on March 17, 2023), we investigated various pre-trained two-dimensional convolutional neural networks (CNN) such as Dense Neural Network, Residual Neural Networks (ResNet), and Vision Transformers, as well as the extent of fine-tuning. Based on empirical experiments, we opted to fine-tune them using ADAM's optimization algorithm with a standard learning rate of 0.001 for all CNN architectures and apply early-stopping whenever the validation loss reached a plateau. For each trained CNN, th
    
[^82]: MaMMUT: 一种用于多模态任务联合学习的简单架构

    MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])

    [http://arxiv.org/abs/2303.16839](http://arxiv.org/abs/2303.16839)

    提出了一种名为MaMMUT的简单模型，可以通过两步方法容纳对比和生成学习，并在联合训练不同的视觉语言任务时表现出很高的效力。

    

    语言模型的发展已从编码-解码转向仅解码的设计。此外，普遍认为，最流行的两种多模态任务，生成任务和对比任务，往往互相冲突，难以在一个架构中容纳，并进一步需要用于下游任务的复杂调整。我们提出了一种新的培训范式，用于多模态任务的仅解码模型，这在联合学习这些不同的视觉语言任务方面非常有效。这是通过一个简单的模型MaMMUT实现的。它由单一的视觉编码器和一个文本解码器组成，并能够通过文本解码器上的新的两步方法容纳对比和生成学习。我们证明这些不同目标任务的联合训练是简单的，有效的，并最大化了模型的权重共享。此外，相同的架构使得对开放词汇对象检测的简单扩展成为可能。

    The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
    
[^83]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^84]: PCA-Net：操作学习的复杂性上下界

    Operator learning with PCA-Net: upper and lower complexity bounds. (arXiv:2303.16317v1 [cs.LG])

    [http://arxiv.org/abs/2303.16317](http://arxiv.org/abs/2303.16317)

    本文发展了PCA-Net的近似理论，得出了通用逼近结果，并识别出了使用PCA-Net进行高效操作学习的潜在障碍：输出分布的复杂性和算子空间的内在复杂性。

    

    神经算子在计算科学和工程中备受关注。PCA-Net是一种最近提出的神经算子架构，它将主成分分析(PCA)与神经网络相结合，以逼近潜在的算子。本文对这种方法进行了近似理论的发展，改进并显着扩展了此方向的以前的工作。在定性界限方面，本文得出了新颖的通用逼近结果，在对潜在算子和数据生成分布的最小假设的前提下。在定量限制方面，本文识别了使用PCA-Net进行高效操作学习的两个潜在障碍，通过导出下界进行了严格证明，第一个障碍与输出分布的复杂性有关，由PCA特征值的缓慢衰减来衡量；另一个障碍涉及无限维输入和输出空间之间的算子空间的内在复杂性。

    Neural operators are gaining attention in computational science and engineering. PCA-Net is a recently proposed neural operator architecture which combines principal component analysis (PCA) with neural networks to approximate an underlying operator. The present work develops approximation theory for this approach, improving and significantly extending previous work in this direction. In terms of qualitative bounds, this paper derives a novel universal approximation result, under minimal assumptions on the underlying operator and the data-generating distribution. In terms of quantitative bounds, two potential obstacles to efficient operator learning with PCA-Net are identified, and made rigorous through the derivation of lower complexity bounds; the first relates to the complexity of the output distribution, measured by a slow decay of the PCA eigenvalues. The other obstacle relates the inherent complexity of the space of operators between infinite-dimensional input and output spaces, 
    
[^85]: 在有限重叠样本情况下的通信高效的垂直联合学习方法

    Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples. (arXiv:2303.16270v1 [cs.LG])

    [http://arxiv.org/abs/2303.16270](http://arxiv.org/abs/2303.16270)

    提出了一种名叫one-shot VFL的实用VFL框架，可以同时解决通信瓶颈和有限重叠样本的问题。few-shot VFL则可以在进行一次或仅少量通信的情况下进一步提高准确性。

    

    垂直联合学习(VFL)是一种流行的协作学习方法，使客户端能够在不共享本地数据的情况下训练全局模型。VFL处理的是客户端数据具有不同的特征空间但共享一些重叠样本的情况。现有的VFL方法存在通信成本高和无法有效处理现实世界中常见的有限重叠样本问题。我们提出了一种名叫 \textbf{one-shot VFL} 的实用VFL框架，基于半监督学习，可以同时解决通信瓶颈和有限重叠样本的问题。我们还提出了 \textbf{few-shot VFL}，在只进行一次或仅少量通信的情况下，进一步提高准确性。在我们提出的框架中，客户端只需要与服务器进行一次或仅少量通信。我们在图像和表格数据集上评估了所提出的VFL框架。

    Federated learning is a popular collaborative learning approach that enables clients to train a global model without sharing their local data. Vertical federated learning (VFL) deals with scenarios in which the data on clients have different feature spaces but share some overlapping samples. Existing VFL approaches suffer from high communication costs and cannot deal efficiently with limited overlapping samples commonly seen in the real world. We propose a practical vertical federated learning (VFL) framework called \textbf{one-shot VFL} that can solve the communication bottleneck and the problem of limited overlapping samples simultaneously based on semi-supervised learning. We also propose \textbf{few-shot VFL} to improve the accuracy further with just one more communication round between the server and the clients. In our proposed framework, the clients only need to communicate with the server once or only a few times. We evaluate the proposed VFL framework on both image and tabular
    
[^86]: 利用分布分解提高均匀学习算法的性能

    Lifting uniform learners via distributional decomposition. (arXiv:2303.16208v1 [stat.ML])

    [http://arxiv.org/abs/2303.16208](http://arxiv.org/abs/2303.16208)

    本文介绍了一种方法，可以将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布下有效的算法，而且对于单调分布，只需要用$\mathcal{D}$中的样本。算法的核心是通过一个算法将$\mathcal{D}$逼近成由子立方体混合而成的混合均匀分布。

    

    我们展示了如何将任何在均匀分布下有效的PAC学习算法转换成一个在任意未知分布$\mathcal{D}$下有效的算法。我们的转换效率随$\mathcal{D}$的固有复杂性而变化，对于在$\{\pm 1\}^n$上的分布，其pmf由深度为$d$的决策树计算，则时间复杂度为$\mathrm{poly}(n, (md)^d)$，其中$m$是原始算法的样本复杂度。对于单调分布，我们的转换仅使用$\mathcal{D}$中的样本，而对于一般分布，我们使用子立方体条件样本。其中一个关键技术是一个算法，它在给出$\mathcal{D}$的访问权限的情况下，产生了一个最优决策树分解$\mathcal{D}$：一个逼近了$\mathcal{D}$的混合均匀分布的分离子立方体。通过这个分解，我们在每个子立方体上运行均匀分布学习器，并将结果合并起来。

    We show how any PAC learning algorithm that works under the uniform distribution can be transformed, in a blackbox fashion, into one that works under an arbitrary and unknown distribution $\mathcal{D}$. The efficiency of our transformation scales with the inherent complexity of $\mathcal{D}$, running in $\mathrm{poly}(n, (md)^d)$ time for distributions over $\{\pm 1\}^n$ whose pmfs are computed by depth-$d$ decision trees, where $m$ is the sample complexity of the original algorithm. For monotone distributions our transformation uses only samples from $\mathcal{D}$, and for general ones it uses subcube conditioning samples.  A key technical ingredient is an algorithm which, given the aforementioned access to $\mathcal{D}$, produces an optimal decision tree decomposition of $\mathcal{D}$: an approximation of $\mathcal{D}$ as a mixture of uniform distributions over disjoint subcubes. With this decomposition in hand, we run the uniform-distribution learner on each subcube and combine the 
    
[^87]: 一种基于半监督回归深度学习模型的简绸纱线数目计算方法以应用于古老油画

    Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models. (arXiv:2303.15999v1 [cs.CV])

    [http://arxiv.org/abs/2303.15999](http://arxiv.org/abs/2303.15999)

    本文开发了一种基于深度学习的半监督回归模型，可直接从图片中计算简绸纱线的线密度，以应用于古老油画，为艺术品保护和修复提供了可靠的线密度估计方法。

    

    本文作者开发了一种基于深度学习的回归方法，以执行对简绸纱线密度的估计，以进行画布分析。该新方法可以直接从图像中计算线密度，并在训练过程中使用了专家知识。该方法在艺术保存和修复领域具有潜在应用。

    In this work the authors develop regression approaches based on deep learning to perform thread density estimation for plain weave canvas analysis. Previous approaches were based on Fourier analysis, that are quite robust for some scenarios but fail in some other, in machine learning tools, that involve pre-labeling of the painting at hand, or the segmentation of thread crossing points, that provides good estimations in all scenarios with no need of pre-labeling. The segmentation approach is time-consuming as estimation of the densities is performed after locating the crossing points. In this novel proposal, we avoid this step by computing the density of threads directly from the image with a regression deep learning model. We also incorporate some improvements in the initial preprocessing of the input image with an impact on the final error. Several models are proposed and analyzed to retain the best one. Furthermore, we further reduce the density estimation error by introducing a sem
    
[^88]: 面向资源受限的无线边缘网络的高效并行分裂学习

    Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])

    [http://arxiv.org/abs/2303.15991](http://arxiv.org/abs/2303.15991)

    本文提出了面向资源受限的无线边缘网络的高效并行分裂学习（EPSL）框架，旨在加速模型训练。EPSL并行化客户端模型训练，通过聚合梯度降低了反向传播的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。同时，EPSL还设计了资源分配算法以优化计算和通信资源分配。

    

    随着神经网络越来越深，这阻碍了联合学习等隐私增强分布式学习方式（如联邦学习）在资源受限的设备上的民主化。为了解决这个挑战，本文倡导将边缘计算范式和并行分裂学习（PSL）相结合，允许多个客户端设备通过逐层模型分裂将大量的训练工作负载卸载到边缘服务器上。通过观察到现有的PSL方案会产生过多的训练延迟和大量的数据传输，我们提出了一种创新的PSL框架——高效并行分裂学习（EPSL），以加速模型训练。具体而言，EPSL将客户端模型训练并行化，并通过最后一层梯度聚合降低了反向传播（BP）的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。此外，通过考虑边缘设备的异构通道条件和计算能力，我们设计了资源分配算法以优化计算和通信资源分配。实验结果表明，EPSL通过将通信成本和训练时间分别降低76％和63％，优于最先进的PSL方法。

    The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil
    
[^89]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^90]: 带有交叉视图部分样本和原型对齐的深度不完整多视图聚类

    Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment. (arXiv:2303.15689v1 [cs.LG])

    [http://arxiv.org/abs/2303.15689](http://arxiv.org/abs/2303.15689)

    论文提出了一种新的深度不完整多视图聚类算法，采用了部分样本的交叉视图聚合机制和两个原型对齐策略，以解决不完整多视图聚类问题。

    

    现有的多视图聚类方法成功的前提是样本在多视图中保持完整性的假设。然而，在实际场景中，多视图样本由于数据损坏或传感器故障而部分地可用，导致不完整多视图聚类研究陷入困境。虽然已经有一些尝试来解决不完整多视图聚类问题，但存在以下缺点：（i）现有方法主要采用交叉视图对比学习，强制每个样本在多个视图中的表示是完全相同的，这可能会忽略视图差异和表示的灵活性；（ii）由于在多个视图中不存在未观察到的样本，所得到的聚类原型可能不对齐和偏斜，导致聚合不正确。为了解决以上问题，我们提出了一种跨视图部分样本和原型对齐网络(CPSPAN)，用于深度不完整多视图聚类。首先，与现有的基于对比的方法不同，我们采用部分样本的交叉视图聚合机制来捕捉视图之间的差异特征。其次，我们采用两个原型对齐策略来解决原型不对齐和聚合不正确的问题。

    The success of existing multi-view clustering relies on the assumption of sample integrity across multiple views. However, in real-world scenarios, samples of multi-view are partially available due to data corruption or sensor failure, which leads to incomplete multi-view clustering study (IMVC). Although several attempts have been proposed to address IMVC, they suffer from the following drawbacks: i) Existing methods mainly adopt cross-view contrastive learning forcing the representations of each sample across views to be exactly the same, which might ignore view discrepancy and flexibility in representations; ii) Due to the absence of non-observed samples across multiple views, the obtained prototypes of clusters might be unaligned and biased, leading to incorrect fusion. To address the above issues, we propose a Cross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep Incomplete Multi-view Clustering. Firstly, unlike existing contrastive-based methods, we adopt pa
    
[^91]: 旨在实现结果导向的患者亚组：六项抑郁症治疗研究的机器学习分析

    Towards Outcome-Driven Patient Subgroups: A Machine Learning Analysis Across Six Depression Treatment Studies. (arXiv:2303.15202v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15202](http://arxiv.org/abs/2303.15202)

    这项研究使用机器学习分析了六项抑郁症药物治疗研究的数据，并生成了结果导向的患者亚组，为患者的个性化治疗提供了指导。

    

    重度抑郁障碍(MDD) 是一种多样性疾病，大量的神经生物学基础可能与治疗反应的变异性有关。理解这种变异性的根源并预测结果一直是难以实现的。机器学习已经在预测MDD的治疗反应方面显示出潜力，但其中一项限制是机器学习模型的临床可解释性不足。我们使用差异原型神经网络(DPNN)分析了六个药物治疗抑郁症临床试验的数据(总数n = 5438)。DPNN可以派生出可用于生成患者亚组的病人原型，同时学习生成差异性治疗反应的概率。使用临床和人口统计数据训练了一个模型，用于分类缓解并输出五种一线单药疗法和三种联合治疗的个体缓解概率。模型使用留一研究法交叉验证、置换测试和特征重要性分析进行验证和临床解释性评估。DPNN模型准确预测了所有六个研究的缓解状况，通过鉴定与特定治疗反应更好相关的患者亚组来展示高的临床可解释性。这些研究结果有可能为MDD患者的个性化治疗建议提供信息。

    Major depressive disorder (MDD) is a heterogeneous condition; multiple underlying neurobiological substrates could be associated with treatment response variability. Understanding the sources of this variability and predicting outcomes has been elusive. Machine learning has shown promise in predicting treatment response in MDD, but one limitation has been the lack of clinical interpretability of machine learning models. We analyzed data from six clinical trials of pharmacological treatment for depression (total n = 5438) using the Differential Prototypes Neural Network (DPNN), a neural network model that derives patient prototypes which can be used to derive treatment-relevant patient clusters while learning to generate probabilities for differential treatment response. A model classifying remission and outputting individual remission probabilities for five first-line monotherapies and three combination treatments was trained using clinical and demographic data. Model validity and clin
    
[^92]: LONGNN: 具有可学习正交标准基的谱图神经网络

    LONGNN: Spectral GNNs with Learnable Orthonormal Basis. (arXiv:2303.13750v1 [cs.LG])

    [http://arxiv.org/abs/2303.13750](http://arxiv.org/abs/2303.13750)

    本研究提出了一种谱图神经网络LONGNN，它采用可学习正交标准基，并解决了固定多项式基和非归一化基础所带来的缺陷，经实验证明其在各种图数据集上具有优异的表现。

    

    近年来，大量的谱图神经网络（GNN）方法利用可学习系数的多项式基在许多节点级任务上实现了顶级性能。虽然已经探索了各种多项式基，但是每种方法都采用了固定的多项式基，可能不是给定图形的最佳选择。此外，我们确定了这些方法所谓的越界问题，并表明这在它们不太系统化的正则化策略和非归一化基础上有所根源。在本文中，我们首次尝试解决这两个问题。利用雅各比多项式，我们设计了一种新的具有可学习正交标准基的谱GNN，LON-GNN，并证明了正则化系数现在等效于正则化所学滤波函数的范数。我们在多样的图数据集上进行了广泛的实验，以评估LON-GNN的拟合和泛化能力，结果表明其优于几种最先进的方法。

    In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with Learnable OrthoNormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of LON-GNN, where the results imply its superiori
    
[^93]: 用于识别TBI生理状态的多元时间序列数据的自监督聚类

    Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States. (arXiv:2303.13024v1 [cs.LG])

    [http://arxiv.org/abs/2303.13024](http://arxiv.org/abs/2303.13024)

    这篇论文提出了一种新的自监督聚类算法，能够在多元时间序列数据中确定并识别对于TBI等急性疾病治疗非常重要的生理状态。研究还利用临床数据验证并解释所识别的生理状态。

    

    对于具有缺失值的多元时间序列数据确定临床相关的生理状态非常重要，这对于提供急性疾病（如颅脑损伤、呼吸衰竭和心力衰竭）的适当治疗至关重要。利用非时间序列聚类或数据插值和聚合技术可能导致有价值信息的丢失和偏见分析。在本研究中，我们应用了基于自监督的SLAC-Time算法，避免了插值或聚合，从而更有效地表示急性患者状态。通过使用SLAC-Time来聚类大型研究数据集中的数据，我们确定了三种不同的TBI生理状态及其具体特征。我们采用了各种聚类评估指标，并结合临床领域专家的意见来验证和解释所识别的生理状态。此外，我们发现了特定临床事件和生理状态之间的关系。

    Determining clinically relevant physiological states from multivariate time series data with missing values is essential for providing appropriate treatment for acute conditions such as Traumatic Brain Injury (TBI), respiratory failure, and heart failure. Utilizing non-temporal clustering or data imputation and aggregation techniques may lead to loss of valuable information and biased analyses. In our study, we apply the SLAC-Time algorithm, an innovative self-supervision-based approach that maintains data integrity by avoiding imputation or aggregation, offering a more useful representation of acute patient states. By using SLAC-Time to cluster data in a large research dataset, we identified three distinct TBI physiological states and their specific feature profiles. We employed various clustering evaluation metrics and incorporated input from a clinical domain expert to validate and interpret the identified physiological states. Further, we discovered how specific clinical events and
    
[^94]: 辅助网络在持续学习中实现更好的稳定性-可塑性平衡

    Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning. (arXiv:2303.09483v1 [cs.LG])

    [http://arxiv.org/abs/2303.09483](http://arxiv.org/abs/2303.09483)

    本文提出了一种辅助网络持续学习方法（ANCL），通过对流信息的控制，自然插值可塑性和稳定性之间的差异，有助于在神经网络中实现更好的稳定性-可塑性平衡。

    

    与人类顺序学习新任务的自然能力相比，神经网络被认为容易出现灾难性遗忘，即模型在被优化为新任务后，在旧任务上的表现急剧下降。为此，持续学习（CL）社区提出了几种解决方案，旨在使神经网络具有学习当前任务（可塑性）的能力，同时在以前的任务上实现高精度（稳定性）。尽管取得了显着的进展，但稳定性-可塑性平衡还远未得到解决，其基本机制尚不清楚。在这项工作中，我们提出了一种新方法——辅助网络持续学习（ANCL），它将一个额外的辅助网络应用于主要关注稳定性的持续学习模型中，从而促进模型的可塑性。更具体地说，所提出的框架通过控制主要网络和辅助网络之间信息的流动来自然地插值可塑性和稳定性之间的差异。多个数据集的实验结果表明，ANCL在可塑性和稳定性方面优于现有持续学习方法，实现了更好的平衡。

    In contrast to the natural capabilities of humans to learn new tasks in a sequential fashion, neural networks are known to suffer from catastrophic forgetting, where the model's performances on old tasks drop dramatically after being optimized for a new task. Since then, the continual learning (CL) community has proposed several solutions aiming to equip the neural network with the ability to learn the current task (plasticity) while still achieving high accuracy on the previous tasks (stability). Despite remarkable improvements, the plasticity-stability trade-off is still far from being solved and its underlying mechanism is poorly understood. In this work, we propose Auxiliary Network Continual Learning (ANCL), a novel method that applies an additional auxiliary network which promotes plasticity to the continually learned model which mainly focuses on stability. More concretely, the proposed framework materializes in a regularizer that naturally interpolates between plasticity and st
    
[^95]: 基于令牌稀疏化视角的视觉Transformer优化

    Making Vision Transformers Efficient from A Token Sparsification View. (arXiv:2303.08685v1 [cs.CV])

    [http://arxiv.org/abs/2303.08685](http://arxiv.org/abs/2303.08685)

    本文提出了一种新的Semantic Token ViT (STViT)方法，实现了全局和本地视觉Transformer的高效性能，同时可用作下游任务的主干骨干。其通过聚类中心的语义令牌代表来代替图像令牌，实现较少的语义令牌即可达到同样的效果。

    

    视觉Transformer (ViTs)的计算复杂度随着令牌数量呈二次增长，限制了其实际应用。为了实现高效的ViT，已有多种方法通过修剪冗余令牌来达到目的。然而，这些方法往往存在以下问题：(i) 显著的精度下降，(ii) 无法应用于本地视觉Transformer中，以及 (iii) 无法通用于下游任务的网络。本文提出了一种新颖的语义令牌ViT（STViT），用于实现全局和本地视觉Transformer的高效性能，并可作为下游任务的主干骨干进行修订。语义令牌代表聚类中心，其通过空间内的图像令牌汇集来初始化，并通过注意组件进行恢复，自适应地表示全局或本地的语义信息。由于其聚类性质，少量的语义令牌即可实现与众多图像令牌相同的效果，适用于全局和本地视觉Transformer。例如，对于DeiT-(Tiny, Small, Base)，仅需16个语义令牌即可达到相同效果。

    The quadratic computational complexity to the number of tokens limits the practical applications of Vision Transformers (ViTs). Several works propose to prune redundant tokens to achieve efficient ViTs. However, these methods generally suffer from (i) dramatic accuracy drops, (ii) application difficulty in the local vision transformer, and (iii) non-general-purpose networks for downstream tasks. In this work, we propose a novel Semantic Token ViT (STViT), for efficient global and local vision transformers, which can also be revised to serve as backbone for downstream tasks. The semantic tokens represent cluster centers, and they are initialized by pooling image tokens in space and recovered by attention, which can adaptively represent global or local semantic information. Due to the cluster properties, a few semantic tokens can attain the same effect as vast image tokens, for both global and local vision transformers. For instance, only 16 semantic tokens on DeiT-(Tiny,Small,Base) can 
    
[^96]: 基于敏感区域的可解释AI变态测试框架

    Sensitive Region-based Metamorphic Testing Framework using Explainable AI. (arXiv:2303.07580v1 [cs.LG])

    [http://arxiv.org/abs/2303.07580](http://arxiv.org/abs/2303.07580)

    提出了一种基于敏感区域的元测试框架，可以通过转换这些区域来有效地检测易出现错误分类的图像；敏感区域可以由可解释AI指定。

    

    深度学习是机器学习中最受欢迎的研究课题之一，基于深度学习的图像识别系统得到了快速发展。最近的研究使用变态测试来检测错误分类的图像。大多数研究讨论变态关系(MR)，但很少讨论应该转换哪些区域。我们的重点是存在敏感区域，即使进行小的转换也会容易改变预测结果，并提出一种变态测试框架，通过转换敏感区域有效地检测易出现错误分类的区域。我们的评估表明，敏感区域可以由可解释AI(XAI)指定，我们的框架有效地检测故障。

    Deep Learning (DL) is one of the most popular research topics in machine learning and DL-driven image recognition systems have developed rapidly. Recent research has used metamorphic testing (MT) to detect misclassified images. Most of them discuss metamorphic relations (MR), with little discussion on which regions should be transformed. We focus on the fact that there are sensitive regions where even a small transformation can easily change the prediction results and propose an MT framework that efficiently tests for regions prone to misclassification by transforming the sensitive regions. Our evaluation showed that the sensitive regions can be specified by Explainable AI (XAI) and our framework effectively detects faults.
    
[^97]: 深度学习去噪方法的客观任务评估的必要性：以心肌灌注SPECT为背景的研究

    Need for Objective Task-based Evaluation of Deep Learning-Based Denoising Methods: A Study in the Context of Myocardial Perfusion SPECT. (arXiv:2303.02110v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.02110](http://arxiv.org/abs/2303.02110)

    本研究旨在探讨基于深度学习的图像去噪方法在临床任务中的表现评估，发现使用保真度(FoMs)的评估不一定与任务为基础的评估一致，而基于信号检测理论(SDT)的评估方法提供了更客观、有意义的去噪效果评估方式，并证明虚拟临床试验（VCTs）是评估DL方法的实用工具。

    

    人工智能方法在核医学中引起了广泛的兴趣，其中特别关注使用基于深度学习（DL）的方法去噪低剂量、短采集时间或两者同时获取的图像。这些方法的客观评估对于临床应用至关重要。DL去噪核医学图像通常使用类似RMSE和SSIM这样的保真度（FoMs）进行评估。然而，这些图像是为临床任务而采集的，因此应该根据它们在这些任务中的表现来评估。本研究的目的是(1)调查使用这些FoMs的评估是否与客观的临床任务评估一致; (2)提供用于确定去噪对信号检测任务影响的理论分析; (3)展示虚拟临床试验（VCTs）用于评估DL方法的实用性。使用逼真的模拟器进行了一个VCT来评估DL去噪心肌灌注SPECT图像方法。采用客观的强制选择实验，使用信号检测理论（SDT）的基于任务的指标和FoMs评估了去噪效果。结果表明，使用FoMs评估去噪效果不一定与基于任务的评估相关。SDT指标提供了更客观和有意义的去噪效果评估方式。VCTs可为核医学中基于DL的去噪方法的评估提供有用的工具。

    Artificial intelligence-based methods have generated substantial interest in nuclear medicine. An area of significant interest has been using deep-learning (DL)-based approaches for denoising images acquired with lower doses, shorter acquisition times, or both. Objective evaluation of these approaches is essential for clinical application. DL-based approaches for denoising nuclear-medicine images have typically been evaluated using fidelity-based figures of merit (FoMs) such as RMSE and SSIM. However, these images are acquired for clinical tasks and thus should be evaluated based on their performance in these tasks. Our objectives were to (1) investigate whether evaluation with these FoMs is consistent with objective clinical-task-based evaluation; (2) provide a theoretical analysis for determining the impact of denoising on signal-detection tasks; (3) demonstrate the utility of virtual clinical trials (VCTs) to evaluate DL-based methods. A VCT to evaluate a DL-based method for denoisi
    
[^98]: 基于时空因果关系的可解释水位预测器

    Interpretable Water Level Forecaster with Spatiotemporal Causal Attention Mechanisms. (arXiv:2303.00515v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00515](http://arxiv.org/abs/2303.00515)

    本研究提出一种基于时空因果关系的新型水位预测模型，通过将因果结构形式化为多层网络和使用蒙版方法，提高了其可解释性，运用于汉江数据集的实际分析中表现优异。

    

    预测汉江水位对于交通控制和避免自然灾害至关重要，但涉及多种变量并相互复杂地联系着。本研究提出一种新型的转换器，利用变量先前知识基于因果关系，预测汉江济州桥的水位。我们的模型考虑到空间和时间因果关系，将因果结构形式化为多层网络并使用蒙版方法。凭借这种方法，我们可以根据先前的知识获得可解释性。在实际数据分析中，我们使用了2016年至2021年的汉江数据集，并将所提出的模型与深度学习模型进行了比较。

    Forecasting the water level of the Han river is important to control traffic and avoid natural disasters. There are many variables related to the Han river and they are intricately connected. In this work, we propose a novel transformer that exploits the causal relationship based on the prior knowledge among the variables and forecasts the water level at the Jamsu bridge in the Han river. Our proposed model considers both spatial and temporal causation by formalizing the causal structure as a multilayer network and using masking methods. Due to this approach, we can have interpretability that consistent with prior knowledge. In real data analysis, we use the Han river dataset from 2016 to 2021 and compare the proposed model with deep learning models.
    
[^99]: 使用核心集的选择性经验回放压缩用于医学影像领域中的终身深度强化学习

    Selective experience replay compression using coresets for lifelong deep reinforcement learning in medical imaging. (arXiv:2302.11510v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11510](http://arxiv.org/abs/2302.11510)

    本文提出了一种使用核心集进行选择性经验回放压缩的技术，可以提升终身学习的效率，应用于医学影像领域。

    

    选择性经验回放是将深度强化学习与终身学习结合的一种流行策略。选择性经验回放旨在重述以前任务中的选择性经验，以避免灾难性遗忘。此外，基于选择性经验回放的技术是模型不可知的，并允许在不同模型之间共享经验。然而，存储所有以前任务的经验会使得使用选择性经验回放的终身学习变得计算上非常昂贵和不切实际，特别是随着任务数量的增加。

    Selective experience replay is a popular strategy for integrating lifelong learning with deep reinforcement learning. Selective experience replay aims to recount selected experiences from previous tasks to avoid catastrophic forgetting. Furthermore, selective experience replay based techniques are model agnostic and allow experiences to be shared across different models. However, storing experiences from all previous tasks make lifelong learning using selective experience replay computationally very expensive and impractical as the number of tasks increase. To that end, we propose a reward distribution-preserving coreset compression technique for compressing experience replay buffers stored for selective experience replay.  We evaluated the coreset compression technique on the brain tumor segmentation (BRATS) dataset for the task of ventricle localization and on the whole-body MRI for localization of left knee cap, left kidney, right trochanter, left lung, and spleen. The coreset lifel
    
[^100]: 预训练基础模型综述：从BERT到ChatGPT的历程

    A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09419](http://arxiv.org/abs/2302.09419)

    本文全面回顾了预训练基础模型的最新研究进展和发展历程，包括它们的架构、培训目标、预培训任务、微调策略和评估。同时，讨论了其局限性和未来研究方向。

    

    预训练基础模型(PFMs)被认为是各种不同数据模态下游任务的基础。PFM(例如BERT、ChatGPT和GPT-4)在大规模数据上进行训练，为各种下游应用提供了合理的参数初始化。BERT从转换器中学习双向编码器表示，这些模型作为上下文语言模型在大型数据集上进行训练。类似地，生成式预训练变压器(GPT)方法采用转换器作为特征提取器，并采用自回归范式在大型数据集上进行训练。最近，ChatGPT在大语言模型中展现了令人兴奋的成功，它采用自回归式语言模型，可以进行零射击或少射击提示。PFM的卓越成就为各种AI领域带来了重大突破。许多研究提出了不同的方法，提高了对更新调查的需求。本研究全面回顾了PFMs的最新进展，包括它们的架构、培训目标、预培训任务、微调策略和评估。此外，我们还讨论了PFMs的局限性和未来潜在的研究方向。

    Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
    
[^101]: AutoFed：用于稳健自动驾驶的异构感知联邦多模态学习

    AutoFed: Heterogeneity-Aware Federated Multimodal Learning for Robust Autonomous Driving. (arXiv:2302.08646v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08646](http://arxiv.org/abs/2302.08646)

    AutoFed 是一种支持异构感知联邦学习的框架，旨在充分利用自动驾驶车辆上的多模态传感数据，并以此实现稳健的自动驾驶。它通过伪标签和自编码器预训练的方法，在解决分布式AVs上具有异构数据的挑战方面表现良好。

    

    自动驾驶中，基于车载传感器（如激光雷达、雷达和摄像头）的目标检测起着至关重要的作用，而这些传感器在模态上互为补充。尽管众感知技术可能潜在地利用这些传感器（数量巨大）来得出更全面的知识，但是，\textit{联邦学习}（FL）似乎是达到这个潜力的必要工具：它使得自动驾驶车辆（AVs）能够在不显式共享原始传感数据的情况下训练机器学习模型。然而，多模态传感器引入了分布式AVs（如标签数量偏差和不同形式）的各种数据异质性，给有效FL带来了重大挑战。为此，我们提出了AutoFed作为一种异构感知FL框架，充分利用AVs上的多模态传感数据，从而实现稳健的自动驾驶。具体而言，我们首先提出了一种新颖的模型，利用伪标签来避免错误地将未标记的对象视为背景。我们还提出了一种基于自编码器的预训练方法，用于学习多模态数据的通用特征表示。借助这些技术，AutoFed可以成功地聚合来自具有各种数据异质性的分布式AVs的多模态数据，并比传统FL和非FL方法实现更好的物体检测结果。

    Object detection with on-board sensors (e.g., lidar, radar, and camera) play a crucial role in autonomous driving (AD), and these sensors complement each other in modalities. While crowdsensing may potentially exploit these sensors (of huge quantity) to derive more comprehensive knowledge, \textit{federated learning} (FL) appears to be the necessary tool to reach this potential: it enables autonomous vehicles (AVs) to train machine learning models without explicitly sharing raw sensory data. However, the multimodal sensors introduce various data heterogeneity across distributed AVs (e.g., label quantity skews and varied modalities), posing critical challenges to effective FL. To this end, we present AutoFed as a heterogeneity-aware FL framework to fully exploit multimodal sensory data on AVs and thus enable robust AD. Specifically, we first propose a novel model leveraging pseudo-labeling to avoid mistakenly treating unlabeled objects as the background. We also propose an autoencoder-b
    
[^102]: 投影概率扩散模型中的视频生成

    Video Probabilistic Diffusion Models in Projected Latent Space. (arXiv:2302.07685v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.07685](http://arxiv.org/abs/2302.07685)

    提出了投影概率扩散模型(PVDM)，在低维潜在空间中学习视频分布，从而可以在有限的资源下高效地训练高分辨率视频。

    

    尽管深层生成模型取得了显著进展，但由于视频的高维性、复杂的时间动态和大的空间变化，合成高分辨率和时间连贯的视频仍然是一个挑战。最近，扩散模型在解决这个问题方面展现了其潜力，但它们的计算和内存效率却受到严重的限制。为了解决这个问题，我们提出了一种新的视频生成模型，称为投影概率扩散模型(PVDM)，它是一种概率扩散模型，它在低维潜在空间中学习视频分布，因此可以在有限的资源下高效地训练高分辨率视频。

    Despite the remarkable progress in deep generative models, synthesizing high-resolution and temporally coherent videos still remains a challenge due to their high-dimensionality and complex temporal dynamics along with large spatial variations. Recent works on diffusion models have shown their potential to solve this challenge, yet they suffer from severe computation- and memory-inefficiency that limit the scalability. To handle this issue, we propose a novel generative model for videos, coined projected latent video diffusion models (PVDM), a probabilistic diffusion model which learns a video distribution in a low-dimensional latent space and thus can be efficiently trained with high-resolution videos under limited resources. Specifically, PVDM is composed of two components: (a) an autoencoder that projects a given video as 2D-shaped latent vectors that factorize the complex cubic structure of video pixels and (b) a diffusion model architecture specialized for our new factorized laten
    
[^103]: 基于状态空间高斯过程的太阳能发电短期预测与滤波

    Short-term Prediction and Filtering of Solar Power Using State-Space Gaussian Processes. (arXiv:2302.00388v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00388](http://arxiv.org/abs/2302.00388)

    本研究提出一种利用状态空间高斯过程进行太阳能光伏能源生产短期预测和滤波的方法，通过结合现代变分推理技术，在处理大规模和非高斯分布数据时具有较高的可扩展性和准确性，有望为电站运营管理提供更加准确的决策依据。

    

    对于电站运营管理而言，太阳能光伏能源（PV）产量的短期预测非常重要。理想情况下，这些预测应该带有误差范围，使得下游决策可以考虑到不确定性。为了在这种情况下产生带有误差范围的预测，我们考虑使用高斯过程 (GPs) 来对英国太阳能光伏能源生产进行建模和预测。由于 PV 的数据量大且 PV 读数不服从高斯分布，所以标准 GP 回归模型难以实现。然而，借助最近高斯过程大规模推理的进展，特别是使用 GPs 的状态空间形式，以及现代变分推理技术，可以实现这一目标。因此，得到的模型不仅适用于大型数据集，而且还可以通过卡尔曼滤波处理连续数据流。

    Short-term forecasting of solar photovoltaic energy (PV) production is important for powerplant management. Ideally these forecasts are equipped with error bars, so that downstream decisions can account for uncertainty. To produce predictions with error bars in this setting, we consider Gaussian processes (GPs) for modelling and predicting solar photovoltaic energy production in the UK. A standard application of GP regression on the PV timeseries data is infeasible due to the large data size and non-Gaussianity of PV readings. However, this is made possible by leveraging recent advances in scalable GP inference, in particular, by using the state-space form of GPs, combined with modern variational inference techniques. The resulting model is not only scalable to large datasets but can also handle continuous data streams via Kalman filtering.
    
[^104]: 物理学知识作为不确定性量化模型的信息场理论

    Physics-informed Information Field Theory for Modeling Physical Systems with Uncertainty Quantification. (arXiv:2301.07609v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.07609](http://arxiv.org/abs/2301.07609)

    该论文扩展了信息场理论(IFT)到物理信息场理论(PIFT)，将描述场的物理定律的信息编码为函数先验。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    

    数据驱动的方法结合物理学知识是建模系统的强有力技术。此类模型的目标是通过将测量结果与已知物理定律相结合，高效地求解基本场。由于许多系统包含未知元素，如缺失参数、嘈杂数据或不完整的物理定律，因此这通常被视为一种不确定性量化问题。处理所有变量的常见技术通常取决于用于近似后验的数值方案，并且希望有一种不依赖于任何离散化的方法。信息场理论（IFT）提供了对不一定是高斯场的场进行统计学的工具。我们通过将描述场的物理定律的信息编码为函数先验来扩展IFT到物理信息场理论（PIFT）。从这个PIFT得出的后验与任何数值方案无关，并且可以捕捉多种模式。

    Data-driven approaches coupled with physical knowledge are powerful techniques to model systems. The goal of such models is to efficiently solve for the underlying field by combining measurements with known physical laws. As many systems contain unknown elements, such as missing parameters, noisy data, or incomplete physical laws, this is widely approached as an uncertainty quantification problem. The common techniques to handle all the variables typically depend on the numerical scheme used to approximate the posterior, and it is desirable to have a method which is independent of any such discretization. Information field theory (IFT) provides the tools necessary to perform statistics over fields that are not necessarily Gaussian. We extend IFT to physics-informed IFT (PIFT) by encoding the functional priors with information about the physical laws which describe the field. The posteriors derived from this PIFT remain independent of any numerical scheme and can capture multiple modes,
    
[^105]: 基于Frank-Wolfe优化的高效内存在线学习：具有有界动态遗憾的算法和控制应用

    Efficient Online Learning with Memory via Frank-Wolfe Optimization: Algorithms with Bounded Dynamic Regret and Applications to Control. (arXiv:2301.00497v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.00497](http://arxiv.org/abs/2301.00497)

    本文介绍了基于Frank-Wolfe优化的高效内存在线学习算法，该算法以历史决策为基础，适应实时时变环境。该算法广泛应用于动态系统的在线控制，统计套利和时间序列预测等人工智能领域。

    

    投影操作是在线学习中的典型计算瓶颈。本文在在线凸优化的记忆框架中实现了无投影的在线学习。具体地，OCO-M反映了决策历史如何影响当前结果，并允许在线学习损失函数依赖于当前和过去的决策。特别地，我们引入了第一个具有内存的无投影基学习算法，该算法使动态遗憾最小化，即最小化与任何时变决策序列的次优性。本算法以在线Frank-Wolfe（OFW）和Hedge算法为基础，旨在解决人工智能应用中的问题，例如需要在实时中适应时变环境，并考虑过去决策对现在的影响。这些应用包括：动态系统的在线控制，统计套利和时间序列预测。

    Projection operations are a typical computation bottleneck in online learning. In this paper, we enable projection-free online learning within the framework of Online Convex Optimization with Memory (OCO-M) -- OCO-M captures how the history of decisions affects the current outcome by allowing the online learning loss functions to depend on both current and past decisions. Particularly, we introduce the first projection-free meta-base learning algorithm with memory that minimizes dynamic regret, i.e., that minimizes the suboptimality against any sequence of time-varying decisions. We are motivated by artificial intelligence applications where autonomous agents need to adapt to time-varying environments in real-time, accounting for how past decisions affect the present. Examples of such applications are: online control of dynamical systems; statistical arbitrage; and time series prediction. The algorithm builds on the Online Frank-Wolfe (OFW) and Hedge algorithms. We demonstrate how our 
    
[^106]: 使用有条件生成器的多现实图像压缩

    Multi-Realism Image Compression with a Conditional Generator. (arXiv:2212.13824v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.13824](http://arxiv.org/abs/2212.13824)

    本文提出了一种多现实图像压缩方法，采用条件生成器控制失真和真实感的权衡，从而可以生成逼真的图像，缓解了以前方法合成细节不可控的问题。该方法在失真-真实感方面取得了业界领先的效果。

    

    通过优化速率-失真-真实感的权衡，生成式压缩方法可以在低比特率下生成详细、逼真的图像，而不是速率-失真优化模型生成的模糊重建图像。然而，以前的方法没有明确控制合成多少细节，这导致这些方法的一个普遍批评：用户可能担心会生成一个远离输入图像的误导性重建图像。在这项工作中，我们通过训练一个解码器来缓解这些问题，该解码器可以跨越两个状态并导航失真-真实感的权衡。从单个压缩表示中，接收器可以决定重构一个低均方误差接近输入的重构，具有高感知质量的逼真重构，或任何介于两者之间的东西。通过我们的方法，我们在失真-真实感方面设立了一个新的业界标杆，推动了可实现的失真-真实感配对的前沿，即我们的方法实现了b

    By optimizing the rate-distortion-realism trade-off, generative compression approaches produce detailed, realistic images, even at low bit rates, instead of the blurry reconstructions produced by rate-distortion optimized models. However, previous methods do not explicitly control how much detail is synthesized, which results in a common criticism of these methods: users might be worried that a misleading reconstruction far from the input image is generated. In this work, we alleviate these concerns by training a decoder that can bridge the two regimes and navigate the distortion-realism trade-off. From a single compressed representation, the receiver can decide to either reconstruct a low mean squared error reconstruction that is close to the input, a realistic reconstruction with high perceptual quality, or anything in between. With our method, we set a new state-of-the-art in distortion-realism, pushing the frontier of achievable distortion-realism pairs, i.e., our method achieves b
    
[^107]: 切片最优偏转运输

    Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08049](http://arxiv.org/abs/2212.08049)

    本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。

    

    最优传输（OT）已经在机器学习、数据科学和计算机视觉中变得极其流行。OT问题的核心假设是源和目标测度的总质量相等，这限制了它的应用。最优偏转运输（OPT）是最近提出的解决这个限制的方法。与OT问题类似，OPT的计算依赖于解决线性规划问题（通常在高维度中），这可能会变得计算上困难。在本文中，我们提出了一种计算一维非负测度之间OPT问题的有效算法。接下来，遵循切片OT距离的思想，我们利用切片定义了切片OPT距离。最后，我们展示了切片OPT-based方法在各种数值实验中的计算和精度优势。特别是，我们展示了我们提出的Sliced-OPT在噪声点云配准中的应用。

    Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
    
[^108]: LLM-Planner: 利用大型语言模型进行少样本实体代理规划

    LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.04088](http://arxiv.org/abs/2212.04088)

    本研究提出了一种新颖的方法LLM-Planner，利用大型语言模型为实体代理进行少样本规划，以实体代理目前所在的环境为基础，增强LLMs生成和更新计划，实验表明其在多任务和快速学习新任务的通用代理的开发中具有很好的表现。

    

    本研究关注利用大型语言模型（LLMs）作为规划器，让实体代理可以按照自然语言指令完成在视觉感知环境中的复杂任务。现有方法的高数据成本和低样本效率阻碍了多任务和快速学习新任务的通用代理的开发。本文提出了一种新颖的方法LLM-Planner，利用大型语言模型为实体代理进行少样本规划。我们进一步提出了一种简单但有效的方法，以实体代理目前所在的环境为基础，增强LLMs生成和更新计划。在ALFRED数据集上的实验表明，我们的方法可以取得非常有竞争力的少样本性能：尽管使用的配对训练数据不到0.5％，LLM-Planner的表现与使用完整训练数据训练的最新基线相当。现有方法几乎无法完成任何任务。

    This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
    
[^109]: MHCCL：用于多元时间序列的层次掩蔽聚类对比学习

    MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series. (arXiv:2212.01141v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01141](http://arxiv.org/abs/2212.01141)

    本文提出了一种名为MHCCL的对比学习模型，可以从多元时间序列数据中学习语义丰富的表示，并利用层次聚类结构中的多粒度信息来滤除虚假负样本和补充正样本。

    

    从原始无标签时间序列数据中学习语义丰富的表示对于分类和预测等下游任务至关重要。对比学习最近展示了在缺乏专家注释的情况下具有良好的表示学习能力。然而，现有的对比学习方法通常独立处理每个实例，导致共享相同语义的假负样本。为了解决这个问题，我们提出了MHCCL，一种层次掩蔽聚类对比学习模型，它利用由多个潜在分区组成的层次结构获得的语义信息来为多元时间序列建模。受到细粒度聚类保留更高纯度，而粗粒度聚类反映更高级别语义的观察的启发，我们提出了一种新颖的向下掩蔽策略，通过结合聚类层次结构中的多粒度信息，过滤掉虚假负面实例并补充正面实例。

    Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy
    
[^110]: 通过多样本超网络提高帕累托前沿学习

    Improving Pareto Front Learning via Multi-Sample Hypernetworks. (arXiv:2212.01130v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01130](http://arxiv.org/abs/2212.01130)

    本文提出了一个新的PFL框架PHN-HVI，利用超网络生成一组多样的解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。

    

    帕累托前沿学习(PFL)是一种有效的方法，用于获得从给定权衡向量到帕累托前沿解的映射函数，从而解决多目标优化(MOO)问题。然而，现有的PFL方法忽略了优化过程中解之间的关系，从而影响了获得的帕累托前沿的质量。为了解决这个问题，本文提出了一个新的PFL框架，即PHN-HVI，它使用超网络从多样的权衡偏好集生成多个解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。多个MOO机器学习数据集上的实验结果表明，相对于现有的PFL方法，PHN-HVI在帕累托前沿近似质量方面具有更好的性能。

    Pareto Front Learning (PFL) was recently introduced as an effective approach to obtain a mapping function from a given trade-off vector to a solution on the Pareto front, which solves the multi-objective optimization (MOO) problem. Due to the inherent trade-off between conflicting objectives, PFL offers a flexible approach in many scenarios in which the decision makers can not specify the preference of one Pareto solution over another, and must switch between them depending on the situation. However, existing PFL methods ignore the relationship between the solutions during the optimization process, which hinders the quality of the obtained front. To overcome this issue, we propose a novel PFL framework namely PHN-HVI, which employs a hypernetwork to generate multiple solutions from a set of diverse trade-off preferences and enhance the quality of the Pareto front by maximizing the Hypervolume indicator defined by these solutions. The experimental results on several MOO machine learning
    
[^111]: CODA-Prompt：基于分解注意力提示的无重训练连续学习方法

    CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning. (arXiv:2211.13218v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.13218](http://arxiv.org/abs/2211.13218)

    CODA-Prompt是一种基于分解注意力提示，无需重复训练即可连续学习的方法，相比于其他提示方法具有更好的性能和效率。

    

    计算机视觉模型在学习不断变化的训练数据中的新概念时容易产生所谓的灾难性遗忘现象。解决这个连续学习问题的典型方法需要对先前已经见过的数据进行大量的重复训练，这增加了内存成本并可能违反数据隐私。最近，大规模预训练视觉变换器模型的出现使提示方法成为一种替代数据重复训练的方法。这些方法依靠关键查询机制生成提示，并已被发现在已经建立的无重训练连续学习设置中高度抵抗灾难性遗忘。然而，这些方法的关键机制没有与任务序列一起进行端到端训练。我们的实验表明，这会导致它们的可塑性降低，从而牺牲新任务准确性，并无法从扩展的参数容量中受益。我们提出了一种新的连续学习方法CODA-Prompt，它使用分解注意力提示机制结合蒸馏损失来训练提示组件，从而实现与任务序列的端到端训练。我们在一系列视觉数据集上的实验表明，CODA-Prompt优于最近的提示方法，而不需要重复训练或额外的资源。

    Computer vision models suffer from a phenomenon known as catastrophic forgetting when learning novel concepts from continuously shifting training data. Typical solutions for this continual learning problem require extensive rehearsal of previously seen data, which increases memory costs and may violate data privacy. Recently, the emergence of large-scale pre-trained vision transformer models has enabled prompting approaches as an alternative to data-rehearsal. These approaches rely on a key-query mechanism to generate prompts and have been found to be highly resistant to catastrophic forgetting in the well-established rehearsal-free continual learning setting. However, the key mechanism of these methods is not trained end-to-end with the task sequence. Our experiments show that this leads to a reduction in their plasticity, hence sacrificing new task accuracy, and inability to benefit from expanded parameter capacity. We instead propose to learn a set of prompt components which are ass
    
[^112]: ConStruct-VL: 无需数据的持续结构化视觉语言概念学习

    ConStruct-VL: Data-Free Continual Structured VL Concepts Learning. (arXiv:2211.09790v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09790](http://arxiv.org/abs/2211.09790)

    该论文介绍了第一个持续的无数据结构化VL概念学习（ConStruct-VL）基准，旨在解决VL模型在结构化VL概念推理方面的瓶颈问题，并提出了一种数据-free的方法。

    

    最近，大规模预先训练的视觉语言基础模型在许多零样本下游任务中展示出了非凡的能力，能够通过仅包含短文本提示的定义来识别物体，并取得了竞争性的结果。然而，也已经表明，VL模型在结构化VL概念推理方面仍然很脆弱，例如识别物体属性、状态和物体间关系的能力。这导致推理错误，需要通过教授VL模型缺失的SVLC技能来进行更正；通常必须使用发现问题的私有数据来完成这一点，这自然而然地导致了一个无需任务ID的无数据持续VL学习设置。在这项工作中，我们介绍了第一个持续的无数据结构化VL概念学习（ConStruct-VL）基准，并表明它对许多现有的无数据CL策略都很具有挑战性。

    Recently, large-scale pre-trained Vision-and-Language (VL) foundation models have demonstrated remarkable capabilities in many zero-shot downstream tasks, achieving competitive results for recognizing objects defined by as little as short text prompts. However, it has also been shown that VL models are still brittle in Structured VL Concept (SVLC) reasoning, such as the ability to recognize object attributes, states, and inter-object relations. This leads to reasoning mistakes, which need to be corrected as they occur by teaching VL models the missing SVLC skills; often this must be done using private data where the issue was found, which naturally leads to a data-free continual (no task-id) VL learning setting. In this work, we introduce the first Continual Data-Free Structured VL Concepts Learning (ConStruct-VL) benchmark and show it is challenging for many existing data-free CL strategies. We, therefore, propose a data-free method comprised of a new approach of Adversarial Pseudo-Re
    
[^113]: 社交媒体文本深度时间建模在临床抑郁症中的应用

    Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.07717](http://arxiv.org/abs/2211.07717)

    本文通过使用抑郁症状检测分类器，从社交媒体文本提取临床相关特征，建立了一个模型用于检测用户的临床抑郁症，通过提供不同时间粒度的准确度度量来评估该模型。

    

    本文描述了一种基于用户时间轴社交媒体帖子的模型，用于检测用户的临床抑郁症。我们的模型使用了抑郁症状检测（DSD）分类器，该分类器基于最大数量的已经过临床医师注释的推文。我们随后使用我们的DSD模型来提取临床相关特征，例如抑郁症评分及其随后的时间模式，以及用户发布活动模式，例如量化他们的“无活动”或“沉默”。此外，为了评估这些提取特征的有效性，我们创建了三种数据集，包括来自两个现有的众所周知的用户级别抑郁症检测基准数据集的测试数据集。然后，我们提供了基于单个特征、基线特征和特征削减测试的准确度度量，在不同时间粒度的几个级别上。

    We describe the development of a model to detect user-level clinical depression based on a user's temporal social media posts. Our model uses a Depression Symptoms Detection (DSD) classifier, which is trained on the largest existing samples of clinician annotated tweets for clinical depression symptoms. We subsequently use our DSD model to extract clinically relevant features, e.g., depression scores and their consequent temporal patterns, as well as user posting activity patterns, e.g., quantifying their ``no activity'' or ``silence.'' Furthermore, to evaluate the efficacy of these extracted features, we create three kinds of datasets including a test dataset, from two existing well-known benchmark datasets for user-level depression detection. We then provide accuracy measures based on single features, baseline features and feature ablation tests, at several different levels of temporal granularity. The relevant data distributions and clinical depression detection related settings can
    
[^114]: 学习动态系统：来自开放量子系统动力学的举例

    Learning dynamical systems: an example from open quantum system dynamics. (arXiv:2211.06678v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.06678](http://arxiv.org/abs/2211.06678)

    应用库普曼算子学习算法可以高效地学习量子系统的演化和可观测量，同时推断出底层动力学的对称性。

    

    设计用于从数据中学习动态系统的机器学习算法可用于预测、控制和解释观察到的动态。在本文中，我们举例说明了其中一种算法，即库普曼算子学习，应用于开放量子系统动力学。我们将研究由退相干门耦合的小自旋链的动态，并展示了库普曼算子学习如何高效地学习密度矩阵的演化，以及与系统相关的每个物理可观察量的演化。最后，利用学习的库普曼算子的谱分解，我们展示了如何直接从数据推断出底层动力学遵循的对称性。

    Machine learning algorithms designed to learn dynamical systems from data can be used to forecast, control and interpret the observed dynamics. In this work we exemplify the use of one of such algorithms, namely Koopman operator learning, in the context of open quantum system dynamics. We will study the dynamics of a small spin chain coupled with dephasing gates and show how Koopman operator learning is an approach to efficiently learn not only the evolution of the density matrix, but also of every physical observable associated to the system. Finally, leveraging the spectral decomposition of the learned Koopman operator, we show how symmetries obeyed by the underlying dynamics can be inferred directly from data.
    
[^115]: 适应性动量的快速联邦学习方法用于分布式组合优化

    Faster Adaptive Momentum-Based Federated Methods for Distributed Composition Optimization. (arXiv:2211.01883v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01883](http://arxiv.org/abs/2211.01883)

    本文提出了一类基于动量的方差缩减和本地SGD技术的更快的联邦组合优化算法，用于解决非凸分布式组合问题，并使用自适应矩阵灵活地结合各种自适应学习率。

    

    联邦学习是机器学习中一种流行的分布式学习范式。组合优化是一种有效的分层学习模型，出现在许多机器学习应用中，如元学习和鲁棒学习。最近，虽然已经提出了一些联邦组合优化算法，但它们仍然存在高采样和通信复杂度的问题。因此，本文提出了一类更快的联邦组合优化算法（即MFCGD和AdaMFCGD），用于解决非凸分布式组合问题，其基于动量的方差缩减和本地SGD技术。特别地，我们的自适应算法（即AdaMFCGD）使用统一的自适应矩阵，灵活地结合各种自适应学习率。此外，我们对我们的算法在非独立同分布设置下进行了坚实的理论分析，并证明了我们的算法获得更低的采样和通信复杂度。

    Federated Learning is a popular distributed learning paradigm in machine learning. Meanwhile, composition optimization is an effective hierarchical learning model, which appears in many machine learning applications such as meta learning and robust learning. More recently, although a few federated composition optimization algorithms have been proposed, they still suffer from high sample and communication complexities. In the paper, thus, we propose a class of faster federated compositional optimization algorithms (i.e., MFCGD and AdaMFCGD) to solve the nonconvex distributed composition problems, which builds on the momentum-based variance reduced and local-SGD techniques. In particular, our adaptive algorithm (i.e., AdaMFCGD) uses a unified adaptive matrix to flexibly incorporate various adaptive learning rates. Moreover, we provide a solid theoretical analysis for our algorithms under non-i.i.d. setting, and prove our algorithms obtain a lower sample and communication complexities sim
    
[^116]: MAgNET: 面向基于网格模拟的图形U-Net架构

    MAgNET: A Graph U-Net Architecture for Mesh-Based Simulations. (arXiv:2211.00713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.00713](http://arxiv.org/abs/2211.00713)

    MAgNET是一个新颖的几何深度学习框架，基于MAg（多通道聚合）操作，采用图形U-Net架构处理任意结构（图形数据）的大维数据，能够高效地处理任意复杂的网格。

    

    在许多尖端应用中，高保真计算模型被证明速度太慢而不切实际，因此被更快的代理模型所替代。最近，深度学习技术在加速这种预测方面变得越来越重要。然而，当面对更大更复杂的问题时，它们往往表现不佳。因此，本文介绍了MAgNET：多通道聚合网络，这是一种新颖的几何深度学习框架，旨在处理任意结构（图形数据）的大维数据。MAgNET建立在MAg（多通道聚合）操作之上，它将卷积神经网络中多通道本地操作的概念推广到任意非网格输入。MAg层与所提出的新型图形池化/反池化操作交错，形成了一个图形U-Net架构，具有鲁棒性，可以高效地处理任意复杂的网格，并对大维图形结构数据进行监督学习。

    In many cutting-edge applications, high-fidelity computational models prove too slow to be practical and are thus replaced by much faster surrogate models. Recently, deep learning techniques have become increasingly important in accelerating such predictions. However, they tend to falter when faced with larger and more complex problems. Therefore, this work introduces MAgNET: Multi-channel Aggregation Network, a novel geometric deep learning framework designed to operate on large-dimensional data of arbitrary structure (graph data). MAgNET is built upon the MAg (Multichannel Aggregation) operation, which generalizes the concept of multi-channel local operations in convolutional neural networks to arbitrary non-grid inputs. The MAg layers are interleaved with the proposed novel graph pooling/unpooling operations to form a graph U-Net architecture that is robust and can handle arbitrary complex meshes, efficiently performing supervised learning on large-dimensional graph-structured data.
    
[^117]: 稀疏动态特征生成，应用于帕金森病诊断

    Sparse Dynamical Features generation, application to Parkinson's Disease diagnosis. (arXiv:2210.11624v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.11624](http://arxiv.org/abs/2210.11624)

    本研究利用动态特征、频率和时间内容提取新的病情特征，在帕金森病的EEG信号诊断中获得了94％的准确性、96％的灵敏度和92％的特异度。

    

    本研究聚焦于基于脑电图（EEG）信号的帕金森病（PD）诊断。我们提出了一种启发式大脑功能的新方法，利用EEG信号的动态特征、频率和时间内容提取新的病情特征。我们在一个公开数据集上进行了评估，该数据集包含50个被试中的25个PD患者的EEG信号，这些信号记录了一个3-oddball听觉任务。通过提取两个特征，并使用线性判别分析（LDA）分类器将其分开，我们可以使用单一通道以90％的准确率（$ p <0.03 $）将健康受试者与不健康受试者分开。通过聚合三个通道的信息并进行投票，我们获得了94％的准确性，96％的灵敏度和92％的特异度。评估是使用嵌套Leave-One-Out交叉验证过程进行的，从而避免了数据泄漏问题。

    In this study we focus on the diagnosis of Parkinson's Disease (PD) based on electroencephalogram (EEG) signals. We propose a new approach inspired by the functioning of the brain that uses the dynamics, frequency and temporal content of EEGs to extract new demarcating features of the disease. The method was evaluated on a publicly available dataset containing EEG signals recorded during a 3-oddball auditory task involving N = 50 subjects, of whom 25 suffer from PD. By extracting two features, and separating them with a straight line using a Linear Discriminant Analysis (LDA) classifier, we can separate the healthy from the unhealthy subjects with an accuracy of 90 % $(p < 0.03)$ using a single channel. By aggregating the information from three channels and making them vote, we obtain an accuracy of 94 %, a sensitivity of 96 % and a specificity of 92 %. The evaluation was carried out using a nested Leave-One-Out cross-validation procedure, thus preventing data leakage problems and givi
    
[^118]: 经由因果性恢复在条件动力学中进行推断

    Inference in conditioned dynamics through causality restoration. (arXiv:2210.10179v2 [physics.data-an] UPDATED)

    [http://arxiv.org/abs/2210.10179](http://arxiv.org/abs/2210.10179)

    本文提出了一种经由因果性恢复的方式来产生条件分布下的独立样本。

    

    从有条件的动力学中计算可观测量通常是计算上困难的，因为虽然从非条件的动力学中高效地获取独立样本通常是可行的，但通常必须丢弃大部分样本(以一种重要性抽样的形式)因为它们不满足所施加的条件。直接从有条件的分布中抽样是不易的，因为条件打破了动力学的因果特性，最终使抽样过程变得低效。一种标准的方法是通过Metropolis Monte-Carlo过程实现，但这个过程通常很慢，需要大量的Monte-Carlo步骤来获得少量的统计独立样本。我们提出了一种替代方法，用于从有条件的分布中产生独立的样本。该方法学习一个广义动力学模型的参数，该模型最优地描述了条件分布的变分。

    Computing observables from conditioned dynamics is typically computationally hard, because, although obtaining independent samples efficiently from the unconditioned dynamics is usually feasible, generally most of the samples must be discarded (in a form of importance sampling) because they do not satisfy the imposed conditions. Sampling directly from the conditioned distribution is non-trivial, as conditioning breaks the causal properties of the dynamics which ultimately renders the sampling procedure efficient. One standard way of achieving it is through a Metropolis Monte-Carlo procedure, but this procedure is normally slow and a very large number of Monte-Carlo steps is needed to obtain a small number of statistically independent samples. In this work, we propose an alternative method to produce independent samples from a conditioned distribution. The method learns the parameters of a generalized dynamical model that optimally describe the conditioned distribution in a variational 
    
[^119]: 紧凑集成用于高效的不确定性估计

    Packed-Ensembles for Efficient Uncertainty Estimation. (arXiv:2210.09184v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09184](http://arxiv.org/abs/2210.09184)

    Packed-Ensembles是一种能够在标准神经网络内运行的轻量级结构化集合，它通过精心调节编码空间的维度来设计。该方法在不损失效果的情况下提高了训练和推理速度。

    

    深度集成是实现关键指标（如准确性、校准、不确定性估计和超出分布检测）卓越性能的突出方法。但是，现实系统的硬件限制限制了更小的集合和较低容量的网络，严重损害了它们的性能和属性。我们引入了一种称为Packed-Ensembles（PE）的策略，通过精心调节其编码空间的维度来设计和训练轻量级结构化集合。我们利用组卷积将集合并行化为单个共享骨干，并进行前向传递以提高训练和推理速度。PE旨在在标准神经网络的内存限制内运行。

    Deep Ensembles (DE) are a prominent approach for achieving excellent performance on key metrics such as accuracy, calibration, uncertainty estimation, and out-of-distribution detection. However, hardware limitations of real-world systems constrain to smaller ensembles and lower-capacity networks, significantly deteriorating their performance and properties. We introduce Packed-Ensembles (PE), a strategy to design and train lightweight structured ensembles by carefully modulating the dimension of their encoding space. We leverage grouped convolutions to parallelize the ensemble into a single shared backbone and forward pass to improve training and inference speeds. PE is designed to operate within the memory limits of a standard neural network. Our extensive research indicates that PE accurately preserves the properties of DE, such as diversity, and performs equally well in terms of accuracy, calibration, out-of-distribution detection, and robustness to distribution shift. We make our c
    
[^120]: 无目标后门水印：朝着无害和隐蔽的数据集版权保护迈进

    Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.00875](http://arxiv.org/abs/2210.00875)

    本文提出了一种针对数据集版权保护的无害和隐蔽的无目标后门水印方案，可以达到与最先进方案相当或更好的水印效果，并证明对模型性能无害且隐蔽。

    

    深度神经网络已在实践中展现出了其优越性。可以说，深度神经网络的快速发展在很大程度上得益于高质量（开源）数据集，研究人员和开发人员可以在此基础上轻松地评估和改进他们的学习方法。由于数据收集通常是耗时甚至昂贵的，如何保护其版权具有重要意义并值得进一步探索。本文重新审视了数据集的所有权验证。我们发现，现有的验证方法由于有目标的后门水印的特性，会在受保护的数据集上训练的深度神经网络中引入新的安全风险。为了解决这个问题，在本文中，我们探讨了无目标后门水印方案，其中异常的模型行为不是确定性的。具体而言，我们介绍了两个分散度，并证明了它们的相关性，基于此我们在受污染标签和干净标签攻击设置下设计了无目标后门水印。实验结果表明，我们提出的方法在水印提取的准确性和模型性能上都能够达到甚至超过现有最先进的方案。此外，我们提出的方法还被证明对模型性能无害且隐蔽，不会引入任何可检测的扭曲或故障。

    Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean
    
[^121]: 一种具有自上而下注意力的高效编码解码结构用于语音分离。

    An efficient encoder-decoder architecture with top-down attention for speech separation. (arXiv:2209.15200v5 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2209.15200](http://arxiv.org/abs/2209.15200)

    本文提出了一种新的高效编码解码网络结构，名为TDANet，它通过模拟大脑的自上而下注意力来降低模型复杂度，并在语音分离任务中取得了具有竞争力的结果。

    

    深度神经网络在语音分离任务中已经显示出优秀的前景。然而，在保持低模型复杂度的同时获得良好结果在实际应用中仍然具有挑战性。在本文中，我们通过模拟大脑的自上而下注意力提供了一种生物启发的高效编码解码架构，称为TDANet，具有降低的模型复杂度，而不牺牲性能。TDANet中的自上而下注意力通过全局注意力(GA)模块和级联局部注意力(LA)层提取。GA模块将多尺度音频特征作为输入，提取全局注意力信号，然后通过直接自上而下连接来调制不同尺度的特征。LA层使用相邻层的特征作为输入，以提取局部注意力信号，该信号用于以自上而下的方式调制横向输入。在三个基准数据集上，TDANet始终获得了与之前最先进方法相当的分离性能，同时显着减少了参数数量和计算成本。

    Deep neural networks have shown excellent prospects in speech separation tasks. However, obtaining good results while keeping a low model complexity remains challenging in real-world applications. In this paper, we provide a bio-inspired efficient encoder-decoder architecture by mimicking the brain's top-down attention, called TDANet, with decreased model complexity without sacrificing performance. The top-down attention in TDANet is extracted by the global attention (GA) module and the cascaded local attention (LA) layers. The GA module takes multi-scale acoustic features as input to extract global attention signal, which then modulates features of different scales by direct top-down connections. The LA layers use features of adjacent layers as input to extract the local attention signal, which is used to modulate the lateral input in a top-down manner. On three benchmark datasets, TDANet consistently achieved competitive separation performance to previous state-of-the-art (SOTA) meth
    
[^122]: 使用互信息机器提高小分子生成

    Improving Small Molecule Generation using Mutual Information Machine. (arXiv:2208.09016v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09016](http://arxiv.org/abs/2208.09016)

    MolMIM是用于小分子药物发现的概率自编码器，其学习了一种信息丰富且聚类的潜在空间，并通过促进致密的潜在空间来采样有效的分子。通过与其他模型的比较，证明了MolMIM的更好的生成能力，并展示了其出色的分子优化性能。

    

    本文研究了控制小分子生成的任务，即在一定限制条件下（如与参考分子的相似度），寻找具有所需属性的新型分子。我们介绍了MolMIM，一种用于小分子药物发现的概率自编码器，它学习了一种信息丰富且聚类的潜在空间。 MolMIM是用互信息机器（MIM）学习训练的，并提供了变长SMILES字符串的固定长度表示。由于编码器-解码器模型可以学习具有无效样本的“空洞”的表示，因此我们在训练过程中提出了一种新颖的扩展，促进了致密的潜在空间，并允许模型从潜在代码的随机扰动中采样有效的分子。我们对MolMIM与几种可变大小和固定大小的编码器-解码器模型进行了全面比较，通过有效性、独特性和新颖性等指标证明了MolMIM的更好的生成能力。然后，我们利用CMA-ES（一种朴素演化算法）来优化分子性质的组合目标函数，并展示了MolMIM在分子优化方面的表现优于现有最先进的方法。

    We address the task of controlled generation of small molecules, which entails finding novel molecules with desired properties under certain constraints (e.g., similarity to a reference molecule). Here we introduce MolMIM, a probabilistic auto-encoder for small molecule drug discovery that learns an informative and clustered latent space. MolMIM is trained with Mutual Information Machine (MIM) learning, and provides a fixed length representation of variable length SMILES strings. Since encoder-decoder models can learn representations with ``holes'' of invalid samples, here we propose a novel extension to the training procedure which promotes a dense latent space, and allows the model to sample valid molecules from random perturbations of latent codes. We provide a thorough comparison of MolMIM to several variable-size and fixed-size encoder-decoder models, demonstrating MolMIM's superior generation as measured in terms of validity, uniqueness, and novelty. We then utilize CMA-ES, a nai
    
[^123]: 作为3D等变图翻译的条件性抗体设计

    Conditional Antibody Design as 3D Equivariant Graph Translation. (arXiv:2208.06073v6 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2208.06073](http://arxiv.org/abs/2208.06073)

    MEAN模型提出了一种新的抗体设计方法，通过E(3)-等变消息传递和注意机制更好地捕获不同组件之间的几何相关性，并且可以同时输出1D序列和3D结构。

    

    抗体设计对于治疗和生物研究具有价值。现有的基于深度学习的方法遇到了几个关键问题: 1) 对于互补决定区域(CDR)的生成存在不完整的上下文; 2)无法捕获输入结构的整个3D几何形状; 3)无法以自回归方式高效预测CDR序列。本文提出了多通道等变注意力网络(MEAN)，以共同设计CDR的1D序列和3D结构。明确地说，MEAN通过导入额外组件，包括目标抗原和抗体的轻链，将抗体设计公式化为条件图翻译问题。然后，MEAN采用E（3）-等变消息传递和提议的注意机制来更好地捕获不同组件之间的几何相关性。最后，它通过一个多轮逐渐完整的方案输出1D序列和3D结构，这样更加高效。

    Antibody design is valuable for therapeutic usage and biological research. Existing deep-learning-based methods encounter several key issues: 1) incomplete context for Complementarity-Determining Regions (CDRs) generation; 2) incapability of capturing the entire 3D geometry of the input structure; 3) inefficient prediction of the CDR sequences in an autoregressive manner. In this paper, we propose Multi-channel Equivariant Attention Network (MEAN) to co-design 1D sequences and 3D structures of CDRs. To be specific, MEAN formulates antibody design as a conditional graph translation problem by importing extra components including the target antigen and the light chain of the antibody. Then, MEAN resorts to E(3)-equivariant message passing along with a proposed attention mechanism to better capture the geometrical correlation between different components. Finally, it outputs both the 1D sequences and 3D structure via a multi-round progressive full-shot scheme, which enjoys more efficiency
    
[^124]: FixEval: 针对编程问题的程序修复的执行评估

    FixEval: Execution-based Evaluation of Program Fixes for Programming Problems. (arXiv:2206.07796v4 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2206.07796](http://arxiv.org/abs/2206.07796)

    本文介绍了 FixEval，这是一个由竞技编程问题的有缺陷代码提交和它们对应的修复组成的基准。FixEval 提供了大量的单元测试来评估模型生成的程序修复的正确性，并根据判决进行时间、内存限制和接受性的进一步信息评估。基于实验结果，我们发现基于执行的指标更准确，有助于识别生成的修复的弱点。

    

    现代软件的复杂性导致了检测和纠正软件 Bug 所需的时间和成本急剧增加。为此，研究人员探索了各种方法来自动生成有缺陷代码的修复程序。然而，由于给定任何 Bug 可能的修复的组合空间很大，因此很少有工具和数据集可用于有效评估生成的修复。为了解决这个问题，我们介绍了 FixEval，这是一个由竞技编程问题的有缺陷代码提交和它们对应的修复组成的基准。FixEval 提供了大量的单元测试来评估模型生成的程序修复的正确性，并根据判决进行时间、内存限制和接受性的进一步信息评估。我们将两个编程语言的 Transformer 语言模型作为基线，并使用基于匹配和执行的评估指标进行比较。我们的实验表明，基于匹配的指标可能会提供误导性的程序修复评估，而基于执行的指标更准确，有助于识别生成的修复的弱点。

    The complexity of modern software has led to a drastic increase in the time and cost associated with detecting and rectifying software bugs. In response, researchers have explored various methods to automatically generate fixes for buggy code. However, due to the large combinatorial space of possible fixes for any given bug, few tools and datasets are available to evaluate model-generated fixes effectively. To address this issue, we introduce FixEval, a benchmark comprising of buggy code submissions to competitive programming problems and their corresponding fixes. FixEval offers an extensive collection of unit tests to evaluate the correctness of model-generated program fixes and assess further information regarding time, memory constraints, and acceptance based on a verdict. We consider two Transformer language models pretrained on programming languages as our baseline and compare them using match-based and execution-based evaluation metrics. Our experiments show that match-based met
    
[^125]: 随机零阶梯度和Hessian估计：方差约简和精细偏差边界

    Stochastic Zeroth Order Gradient and Hessian Estimators: Variance Reduction and Refined Bias Bounds. (arXiv:2205.14737v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14737](http://arxiv.org/abs/2205.14737)

    研究了随机零阶梯度和Hessian估计，通过随机正交方向进行有限差分来减少方差。除了提供方差的约束，还提供改进的偏差边界。

    

    我们研究了实值函数在$\mathbb{R}^n$中的随机零阶梯度和Hessian估计。我们展示了，通过沿着随机正交方向进行有限差分，可以大大减少随机有限差分估计器的方差。特别地，我们为光滑函数设计了估计器，如果从Stiefel流形$\text{St}(n,k)$中采样$ \Theta \left( k \right) $随机方向，并且采用有限差分粒度$\delta$，则梯度估计器的方差受到 $ \mathcal{O} \left( \left( \frac{n}{k} - 1 \right) + \left( \frac{n^2}{k} - n \right) \delta^2 + \frac{ n^2 \delta^4 }{ k } \right) $的约束，Hessian估计器的方差受到 $\mathcal{O} \left( \left( \frac{n^2}{k^2} - 1 \right) + \left( \frac{n^4}{k^2} - n^2 \right) \delta^2 + \frac{n^4 \delta^4 }{k^2} \right) $的限制。当$k=n$时，方差变得非常小。此外，我们提供了改进的偏差边界。

    We study stochastic zeroth order gradient and Hessian estimators for real-valued functions in $\mathbb{R}^n$. We show that, via taking finite difference along random orthogonal directions, the variance of the stochastic finite difference estimators can be significantly reduced. In particular, we design estimators for smooth functions such that, if one uses $ \Theta \left( k \right) $ random directions sampled from the Stiefel's manifold $ \text{St} (n,k) $ and finite-difference granularity $\delta$, the variance of the gradient estimator is bounded by $ \mathcal{O} \left( \left( \frac{n}{k} - 1 \right) + \left( \frac{n^2}{k} - n \right) \delta^2 + \frac{ n^2 \delta^4 }{ k } \right) $, and the variance of the Hessian estimator is bounded by $\mathcal{O} \left( \left( \frac{n^2}{k^2} - 1 \right) + \left( \frac{n^4}{k^2} - n^2 \right) \delta^2 + \frac{n^4 \delta^4 }{k^2} \right) $. When $k = n$, the variances become negligibly small. In addition, we provide improved bias bounds for the es
    
[^126]: 基于全局原型的增强持续学习: 对抗负表示漂移

    Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift. (arXiv:2205.12186v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2205.12186](http://arxiv.org/abs/2205.12186)

    该论文提出了一种基于全局原型的持续学习方法，在自监督信息的正则化下学习数据表示，以缓解负面表示漂移问题，并减少持续学习中的灾难性遗忘。

    

    持续学习旨在学习一系列任务，其中数据分布从一个任务转移到另一个任务。在训练新任务数据时，旧任务的数据表示可能会漂移。一些负面的表示漂移可能会导致灾难性遗忘，因为会导致从本地学习的类别原型和数据表示在任务之间的相关性较差。为了缓解这种表示漂移，我们提出一种方法，通过全局原型指导学习，用自监督信息的正则化来学习数据表示。具体来说，对于NLP任务，我们将每个任务以屏蔽语言建模的方式进行公式化，并通过预训练的语言模型进行相邻注意机制学习任务。实验结果表明，我们提出的方法可以学习出具有较少表示漂移的相当一致的表示，并在不重新采样过去任务的数据的情况下显著减少持续学习中的灾难性遗忘。

    Continual learning (CL) aims to learn a sequence of tasks over time, with data distributions shifting from one task to another. When training on new task data, data representations from old tasks may drift. Some negative representation drift can result in catastrophic forgetting, by causing the locally learned class prototypes and data representations to correlate poorly across tasks. To mitigate such representation drift, we propose a method that finds global prototypes to guide the learning, and learns data representations with the regularization of the self-supervised information. Specifically, for NLP tasks, we formulate each task in a masked language modeling style, and learn the task via a neighbor attention mechanism over a pre-trained language model. Experimental results show that our proposed method can learn fairly consistent representations with less representation drift, and significantly reduce catastrophic forgetting in CL without resampling data from past tasks.
    
[^127]: 半参数诱导点网络和神经过程

    Semi-Parametric Inducing Point Networks and Neural Processes. (arXiv:2205.11718v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11718](http://arxiv.org/abs/2205.11718)

    提出一种半参数体系结构调用训练集的网络SPIN，并以之为基础构建出适用于大型上下文的诱导点神经过程，成功应用于元学习领域并提高了准确性表现。

    

    本文提出了半参数诱导点网络（SPIN），这是一种通用的体系结构，可以在推理时间以计算有效的方式查询训练集。半参数体系结构通常比参数模型更紧凑，但它们的计算复杂度通常是二次的。相反，SPIN通过数据点之间受诱导点方法启发的交叉注意机制实现了线性复杂度。查询大型训练集在元学习中尤其有用，因为它解锁了额外的训练信号，但常常超出现有模型的缩放限制。我们使用SPIN作为诱导点神经过程的基础，这是一种概率模型，支持在元学习中使用大型上下文，并在现有模型失败的情况下实现了高精度。在我们的实验中，SPIN减少了内存需求，在一系列元学习任务中提高了准确性，并在重要的实际问题，基因型 im...（根据原文长度截断）

    We introduce semi-parametric inducing point networks (SPIN), a general-purpose architecture that can query the training set at inference time in a compute-efficient manner. Semi-parametric architectures are typically more compact than parametric models, but their computational complexity is often quadratic. In contrast, SPIN attains linear complexity via a cross-attention mechanism between datapoints inspired by inducing point methods. Querying large training sets can be particularly useful in meta-learning, as it unlocks additional training signal, but often exceeds the scaling limits of existing models. We use SPIN as the basis of the Inducing Point Neural Process, a probabilistic model which supports large contexts in meta-learning and achieves high accuracy where existing models fail. In our experiments, SPIN reduces memory requirements, improves accuracy across a range of meta-learning tasks, and improves state-of-the-art performance on an important practical problem, genotype imp
    
[^128]: 聚类图匹配用于标签恢复和图分类

    Clustered Graph Matching for Label Recovery and Graph Classification. (arXiv:2205.03486v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2205.03486](http://arxiv.org/abs/2205.03486)

    本论文提出一种利用顶点对齐的平均图，聚类平均图和混淆网络匹配的策略，比起传统的全局平均图策略，可以更有效地提高匹配性能和分类精度。

    

    针对一组顶点对齐网络和一个额外的标签混淆网络，我们提出了一种利用顶点对齐集合中的信号来恢复混淆网络标签的方法。我们考虑将混淆网络与不同粒度下的顶点对齐集合中的平均网络进行匹配。我们证明并证实，在网络来自不同网络类的情况下，将网络聚类到类中，然后将新网络匹配到聚类平均值，可以比将其匹配到全局平均图产生更高的匹配性能。此外，通过最小化相对于每个聚类平均值的图匹配目标函数，这种方法同时对混淆图进行了分类和顶点标签恢复。这些理论研究通过一个有启示意义的真实数据实验匹配人类连接体来得到更多巩固。

    Given a collection of vertex-aligned networks and an additional label-shuffled network, we propose procedures for leveraging the signal in the vertex-aligned collection to recover the labels of the shuffled network. We consider matching the shuffled network to averages of the networks in the vertex-aligned collection at different levels of granularity. We demonstrate both in theory and practice that if the graphs come from different network classes, then clustering the networks into classes followed by matching the new graph to cluster-averages can yield higher fidelity matching performance than matching to the global average graph. Moreover, by minimizing the graph matching objective function with respect to each cluster average, this approach simultaneously classifies and recovers the vertex labels for the shuffled graph. These theoretical developments are further reinforced via an illuminating real data experiment matching human connectomes.
    
[^129]: 基于脉冲表示的微分训练高性能低延迟脉冲神经网络

    Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation. (arXiv:2205.00459v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2205.00459](http://arxiv.org/abs/2205.00459)

    本文提出了基于脉冲表示的微分（DSR）方法，可以实现与人工神经网络（ANN）相竞争的高性能、低延迟训练的脉冲神经网络（SNN）。

    

    脉冲神经网络（SNN）是一种非常有前途的能源高效的人工智能模型，尤其当它在神经形态硬件上实现时。然而，由于其不可微性，高效地训练SNN是一项挑战。目前大多数方法要么存在高延迟（即长的仿真时间步长）要么无法达到与人工神经网络（ANN）相当的高性能。本文提出了基于脉冲表示的微分（DSR）方法，可以实现与ANN相竞争的高性能且低延迟的训练。首先，我们使用（加权）发射率编码将脉冲列编码为脉冲表示。在脉冲表示的基础上，我们系统地推导出具有常见神经模型的脉冲动力学可以表示为一些次微分映射。从这个角度出发，我们提出的DSR方法通过这些映射的梯度训练SNN并避免了SNN训练中常见的不可微性问题。然后我们对噪声下表示误差进行了分析，并指出DSR方法对这些误差表现出强有力的容忍度。

    Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when represe
    
[^130]: 基于数据的价格歧视的信息论限制研究

    Information-theoretic limitations of data-based price discrimination. (arXiv:2204.12723v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2204.12723](http://arxiv.org/abs/2204.12723)

    本文研究基于数据的价格歧视，揭示了任何基于数据的定价策略在收入生成方面的信息论限制，提出了新的经验收益最大化（ERM）策略，并实现了最优收敛速率。

    

    本文研究了基于估值和外生变量数据随机样本的第三度价格歧视（3PD），其中外生变量是连续的，数据分布对卖方来说是未知的。本文的主要结果有两个方面。第一组结果是定价策略无关的，揭示了任何基于数据的定价策略在收入生成方面的信息论限制，分为3PD和均匀定价两种情况。第二组结果提出了$K$-markets经验收益最大化（ERM）策略，并显示$K$-markets ERM和均匀ERM策略实现了收入收敛到各自真实分布3PD和均匀定价最优解的最优收敛速率。我们的理论和数值结果表明，当样本量足够小的时候，均匀（即$1$-market）ERM策略产生的收入比$K$-markets ERM策略更高，反之亦然。

    This paper studies third-degree price discrimination (3PD) based on a random sample of valuation and covariate data, where the covariate is continuous, and the distribution of the data is unknown to the seller. The main results of this paper are twofold. The first set of results is pricing strategy independent and reveals the fundamental information-theoretic limitation of any data-based pricing strategy in revenue generation for two cases: 3PD and uniform pricing. The second set of results proposes the $K$-markets empirical revenue maximization (ERM) strategy and shows that the $K$-markets ERM and the uniform ERM strategies achieve the optimal rate of convergence in revenue to that generated by their respective true-distribution 3PD and uniform pricing optima. Our theoretical and numerical results suggest that the uniform (i.e., $1$-market) ERM strategy generates a larger revenue than the $K$-markets ERM strategy when the sample size is small enough, and vice versa.
    
[^131]: 随机流形采样和联合稀疏正则化的多标签特征选择

    Random Manifold Sampling and Joint Sparse Regularization for Multi-label Feature Selection. (arXiv:2204.06445v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2204.06445](http://arxiv.org/abs/2204.06445)

    本文提出了一种基于联合约束优化问题的 $\ell_{2,1}$ 和 $\ell_{F}$ 正则化方法来获得最相关的几个特征，并在流形正则化中实现了基于随机游走策略的高度稳健的邻域图。该方法在真实数据集上的比较实验中表现优异。

    

    多标签学习通常用于挖掘特征和标签之间的相关性，特征选择可以通过少量特征保留尽可能多的信息。 $\ell_{2,1}$ 正则化可以获得稀疏系数矩阵，但不能有效地解决多重共线性问题。本文提出的模型通过解决 $\ell_{2,1}$ 和 $\ell_{F}$ 正则化的联合约束优化问题来获取最相关的几个特征。在流形正则化中，我们根据联合信息矩阵实现了基于随机游走策略的高度稳健的邻域图。此外，我们还给出了解决该模型的算法并证明了其收敛性。在真实数据集上的比较实验表明，所提出的方法优于其他方法。

    Multi-label learning is usually used to mine the correlation between features and labels, and feature selection can retain as much information as possible through a small number of features. $\ell_{2,1}$ regularization method can get sparse coefficient matrix, but it can not solve multicollinearity problem effectively. The model proposed in this paper can obtain the most relevant few features by solving the joint constrained optimization problems of $\ell_{2,1}$ and $\ell_{F}$ regularization.In manifold regularization, we implement random walk strategy based on joint information matrix, and get a highly robust neighborhood graph.In addition, we given the algorithm for solving the model and proved its convergence.Comparative experiments on real-world data sets show that the proposed method outperforms other methods.
    
[^132]: AutoSDF: 用于三维完成、重建和生成的形状先验

    AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation. (arXiv:2203.09516v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.09516](http://arxiv.org/abs/2203.09516)

    本文提出了一种自回归先验模型，可以用于解决三维任务中的形状完成、重建和生成问题。该模型可以在任意空间锚点位置的信息条件下进行形状完成，同时还可以用于单视图重建和基于语义的形状生成。

    

    强大的先验条件可以使我们在信息不充分的情况下进行推理。在本文中，我们提出了一种三维形状的自回归先验，以解决多模态的三维任务，如形状完成、重建和生成。我们将三维形状的分布建模为非连续自回归分布，通过对三维形状的离散化、低维、象征性的网格状潜在表示进行建模。这使我们能够表示在任意一组空间锚定查询位置的信息条件下，三维形状的分布，从而在这些任意设置中进行形状完成（例如，仅给出后腿的视图即可生成完整的椅子）。我们还展示了学习的自回归先验可以用于条件任务，如单视图重建和基于语言的生成。这通过学习任务特定的朴素条件概率实现，这些条件概率可以由在最小成对数据上训练的轻量级模型进行近似。我们通过实验证明了该模型的有效性。

    Powerful priors allow us to perform inference with insufficient information. In this paper, we propose an autoregressive prior for 3D shapes to solve multimodal 3D tasks such as shape completion, reconstruction, and generation. We model the distribution over 3D shapes as a non-sequential autoregressive distribution over a discretized, low-dimensional, symbolic grid-like latent representation of 3D shapes. This enables us to represent distributions over 3D shapes conditioned on information from an arbitrary set of spatially anchored query locations and thus perform shape completion in such arbitrary settings (e.g., generating a complete chair given only a view of the back leg). We also show that the learned autoregressive prior can be leveraged for conditional tasks such as single-view reconstruction and language-based generation. This is achieved by learning task-specific naive conditionals which can be approximated by light-weight models trained on minimal paired data. We validate the
    
[^133]: 任何决策阈值下公平分类的几何修复

    Geometric Repair for Fair Classification at Any Decision Threshold. (arXiv:2203.07490v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.07490](http://arxiv.org/abs/2203.07490)

    本文研究了如何通过减少每个组分数分布之间的统计距离，在任何决策阈值下同时提高分类的公平性能，并提出了一种基于最佳运输的后处理算法。

    

    我们研究的问题是后处理监督机器学习回归器以在所有决策阈值下最大化公平的二分类问题。具体而言，我们表明通过减少每个组分数分布之间的统计距离，我们可以同时增加所有阈值上的公平性能，并且可以做到在不显着降低准确性的情况下实现。为此，我们介绍了一个形式化的分布平等度量，它捕获了不同保护组的分类分布相似程度。与先前只针对所有阈值的人口统计平等度研究的研究相比，我们的度量适用于大类公平性度量。我们的主要成果是提出一种基于最佳运输的后处理算法，可以证明它最大化了分布平等。我们在几个公平性基准测试上支持此结果。

    We study the problem of post-processing a supervised machine-learned regressor to maximize fair binary classification at all decision thresholds. Specifically, we show that by decreasing the statistical distance between each group's score distributions, we can increase fair performance across all thresholds at once, and that we can do so without a significant decrease in accuracy. To this end, we introduce a formal measure of distributional parity, which captures the degree of similarity in the distributions of classifications for different protected groups. In contrast to prior work, which has been limited to studies of demographic parity across all thresholds, our measure applies to a large class of fairness metrics. Our main result is to put forward a novel post-processing algorithm based on optimal transport, which provably maximizes distributional parity. We support this result with experiments on several fairness benchmarks.
    
[^134]: 系统化泛化的可证明有效因果模型强化学习

    Provably Efficient Causal Model-Based Reinforcement Learning for Systematic Generalization. (arXiv:2202.06545v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.06545](http://arxiv.org/abs/2202.06545)

    本文提出了一种新的因果模型强化学习算法，利用环境的共享结构来最大化系统化泛化，在运动定律共享的环境系列中实现了优化的样本复杂度交换，并在基准问题上展示了有效性和挑战性。

    

    在顺序决策环境中，智能体旨在实现对大规模、可能无限的环境的系统化泛化。这些环境被建模为离散的马尔可夫决策过程，其中状态和动作都用特征向量表示。环境的底层结构允许将转移动态分解为两个组成部分：一个是特定于环境的，另一个是共享的。例如，考虑一组共享运动法则的环境。在这种情况下，智能体可以从这些环境的子集中进行有限的无回报交互。然后，智能体必须能够仅依靠上述交互，近似地解决设置在任何环境中的任何规划任务。我们是否可以设计一个可证明有效的算法，实现这个雄心勃勃的系统化泛化目标呢？在本文中，我们部分地回答了这个问题。首先，我们可以对这类问题进行优化的样本复杂度交换。其次，我们提出了一种新的因果模型强化学习算法，称为通过因果推理实现系统化泛化（SGCI），该算法利用环境的共享结构来最大化系统化泛化。第三，我们证明了我们的算法在学习运动定律共享的环境系列时实现了优化的样本复杂度交换，并进一步展示了算法在一系列具有挑战性的基准问题上的有效性，这些问题对于最先进的强化学习算法而言具有挑战性。

    In the sequential decision making setting, an agent aims to achieve systematic generalization over a large, possibly infinite, set of environments. Such environments are modeled as discrete Markov decision processes with both states and actions represented through a feature vector. The underlying structure of the environments allows the transition dynamics to be factored into two components: one that is environment-specific and another that is shared. Consider a set of environments that share the laws of motion as an example. In this setting, the agent can take a finite amount of reward-free interactions from a subset of these environments. The agent then must be able to approximately solve any planning task defined over any environment in the original set, relying on the above interactions only. Can we design a provably efficient algorithm that achieves this ambitious goal of systematic generalization? In this paper, we give a partially positive answer to this question. First, we prov
    
[^135]: 具范数约束的神经网络的逼近误差界与应用研究

    Approximation bounds for norm constrained neural networks with applications to regression and GANs. (arXiv:2201.09418v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09418](http://arxiv.org/abs/2201.09418)

    本文研究了具范数约束的ReLU神经网络的逼近能力，并证明了对于平滑函数类，这些网络的逼近误差有上下界。此外，应用结果分析了回归和GAN分布估计问题的收敛性，最终证明了当GAN的判别器选择合适的具范数约束的神经网络时，可以实现学习概率分布的最优速率。

    

    本文研究带权重范数约束的ReLU神经网络的逼近能力，对于平滑的函数类，我们证明了这些网络的逼近误差上下界。通过神经网络的Rademacher复杂度导出下界证明，这可能具有独立的研究价值。我们应用这些逼近误差界限来分析使用具范数约束的神经网络进行回归和GAN分布估计的收敛性。特别的，我们得到了过参数神经网络的收敛速率。同时，我们还证明了当判别器选择合适的具范数约束的神经网络时，GAN可以实现学习概率分布的最优速率。

    This paper studies the approximation capacity of ReLU neural networks with norm constraint on the weights. We prove upper and lower bounds on the approximation error of these networks for smooth function classes. The lower bound is derived through the Rademacher complexity of neural networks, which may be of independent interest. We apply these approximation bounds to analyze the convergences of regression using norm constrained neural networks and distribution estimation by GANs. In particular, we obtain convergence rates for over-parameterized neural networks. It is also shown that GANs can achieve optimal rate of learning probability distributions, when the discriminator is a properly chosen norm constrained neural network.
    
[^136]: MOEF:建模频域中的场合演变，实现促销感知的点击率预测

    MOEF: Modeling Occasion Evolution in Frequency Domain for Promotion-Aware Click-Through Rate Prediction. (arXiv:2112.13747v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.13747](http://arxiv.org/abs/2112.13747)

    本文提出了一种新的CTR模型MOEF，它通过在频域中建模场合演变来处理在线分布的不确定性，采用多个专家学习特征表示，取得了在真实世界的电子商务数据集上优于最先进的CTR模型的效果。

    

    促销在电子商务中变得越来越重要和普遍，以吸引客户和促进销售，导致场合经常变化，从而驱动用户表现出不同的行为。在这种情况下，由于即将到来的场合分布的不确定性，大多数现有的点击率（CTR）模型无法在在线服务中良好地推广。本文提出了一种新颖的CTR模型，名为MOEF，用于在场合经常变化的情况下进行推荐。首先，我们设计了一个时间序列，其中包括从在线业务场景中生成的场合信号。由于场合信号在频域中更具有区别性，我们对时间窗口应用傅里叶变换，得到一系列频谱，然后通过场合演变层（OEL）进行处理。通过这种方式，可以学习高阶场合表示，以处理在线分布的不确定性。此外，我们采用多个专家来学习特征表示，表示为场合上下文编码（OCE）和模型感知注意力（MAA），以捕捉用户行为和项目特征的不同方面。在真实世界的电子商务数据集上进行的广泛实验证明，MOEF在离线评估和在线服务方面优于最先进的CTR模型。

    Promotions are becoming more important and prevalent in e-commerce to attract customers and boost sales, leading to frequent changes of occasions, which drives users to behave differently. In such situations, most existing Click-Through Rate (CTR) models can't generalize well to online serving due to distribution uncertainty of the upcoming occasion. In this paper, we propose a novel CTR model named MOEF for recommendations under frequent changes of occasions. Firstly, we design a time series that consists of occasion signals generated from the online business scenario. Since occasion signals are more discriminative in the frequency domain, we apply Fourier Transformation to sliding time windows upon the time series, obtaining a sequence of frequency spectrum which is then processed by Occasion Evolution Layer (OEL). In this way, a high-order occasion representation can be learned to handle the online distribution uncertainty. Moreover, we adopt multiple experts to learn feature repres
    
[^137]: CATRO：基于类感知的迹比优化的通道剪枝

    CATRO: Channel Pruning via Class-Aware Trace Ratio Optimization. (arXiv:2110.10921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.10921](http://arxiv.org/abs/2110.10921)

    本文提出了基于类感知迹比优化的通道剪枝方法（CATRO），通过特征空间判别度量多通道的联合影响并合并保留通道的层次影响，有效降低计算负担并加速模型推理。

    

    深度卷积神经网络在许多应用场景中存在高参数和计算冗余，必要时需要进行模型剪枝以获得轻量级和高效的网络。然而，大多数现有的剪枝方法是由经验启发式的，很少考虑通道的联合影响，导致性能不确定和次优。本文提出了一种新颖的基于类感知迹比优化的通道剪枝方法（CATRO），以减少计算负担并加速模型推理。利用少量样本的类别信息，CATRO通过特征空间判别度量多通道的联合影响，并合并保留通道的层次影响。通过将通道剪枝形式化为子模函数最大化问题，CATRO通过两阶段贪心迭代优化过程有效地解决了这个问题。更重要的是，我们提出了理论分析证明了我们所提出的CATRO方法的有效性和优越性。

    Deep convolutional neural networks are shown to be overkill with high parametric and computational redundancy in many application scenarios, and an increasing number of works have explored model pruning to obtain lightweight and efficient networks. However, most existing pruning approaches are driven by empirical heuristic and rarely consider the joint impact of channels, leading to unguaranteed and suboptimal performance. In this paper, we propose a novel channel pruning method via Class-Aware Trace Ratio Optimization (CATRO) to reduce the computational burden and accelerate the model inference. Utilizing class information from a few samples, CATRO measures the joint impact of multiple channels by feature space discriminations and consolidates the layer-wise impact of preserved channels. By formulating channel pruning as a submodular set function maximization problem, CATRO solves it efficiently via a two-stage greedy iterative optimization procedure. More importantly, we present theo
    
[^138]: 统计上意义的近似：一种在变换器中近似图灵机的案例研究

    Statistically Meaningful Approximation: a Case Study on Approximating Turing Machines with Transformers. (arXiv:2107.13163v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.13163](http://arxiv.org/abs/2107.13163)

    本文提出了统计上意义的近似的正式定义，研究了过度参数化的前馈神经网络和变换器的SM近似在布尔电路和图灵机中的应用，重点在于探索近似网络应该具有良好的统计可学性的概念，达到更有意义的近似效果。

    

    理论上研究神经网络结构的常用方法是分析它们可以近似的函数。然而，近似理论中的构造可能是不现实的，因此意义不太明确。为了解决这些问题，本文提出了统计上意义的（SM）近似的正式定义，要求近似网络具有良好的统计可学性。我们研究了两种函数类别的SM近似：布尔电路和图灵机。我们表明，过度参数化的前馈神经网络可以SM近似布尔电路，采样复杂度仅取决于电路大小，而不是网络大小。此外，我们还表明，变换器可以SM近似计算时间受$T$限制的图灵机，采样复杂度多项式地取决于字母大小、状态空间大小和$\log (T)$。我们还在...

    A common lens to theoretically study neural net architectures is to analyze the functions they can approximate. However, constructions from approximation theory may be unrealistic and therefore less meaningful. For example, a common unrealistic trick is to encode target function values using infinite precision. To address these issues, this work proposes a formal definition of statistically meaningful (SM) approximation which requires the approximating network to exhibit good statistical learnability. We study SM approximation for two function classes: boolean circuits and Turing machines. We show that overparameterized feedforward neural nets can SM approximate boolean circuits with sample complexity depending only polynomially on the circuit size, not the size of the network. In addition, we show that transformers can SM approximate Turing machines with computation time bounded by $T$ with sample complexity polynomial in the alphabet size, state space size, and $\log (T)$. We also in
    
[^139]: 使用min-plus代数的神经网络结构解决高维最优控制问题和哈密顿-雅可比偏微分方程

    Neural network architectures using min-plus algebra for solving certain high dimensional optimal control problems and Hamilton-Jacobi PDEs. (arXiv:2105.03336v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2105.03336](http://arxiv.org/abs/2105.03336)

    本文提出了两种抽象的神经网络结构，用于计算某类高维最优控制问题的价值函数和最优控制。使用深度神经网络实现计算的几个数值结果也被展示了出来。同时，初步实现证明该神经网络结构相对于CPU具有良好的加速效果，为利用专门为神经网络设计的高效专用硬件解决高维最优控制问题和哈密顿-雅可比偏微分方程铺平了道路。

    

    解决高维最优控制问题及其对应的哈密顿-雅可比偏微分方程是控制工程中重要而具有挑战性的问题。本文提出了两种抽象的神经网络结构，分别用于计算某类高维最优控制问题的价值函数和最优控制。我们对这两种抽象结构进行了数学分析。我们还展示了使用这些抽象结构的深度神经网络实现计算的几个数值结果。我们在FPGA上对我们提出的神经网络结构进行了初步实现，证明其相对于CPU具有良好的加速效果。这项工作为利用专门为神经网络设计的高效专用硬件解决高维最优控制问题和哈密顿-雅可比偏微分方程铺平了道路。

    Solving high dimensional optimal control problems and corresponding Hamilton-Jacobi PDEs are important but challenging problems in control engineering. In this paper, we propose two abstract neural network architectures which are respectively used to compute the value function and the optimal control for certain class of high dimensional optimal control problems. We provide the mathematical analysis for the two abstract architectures. We also show several numerical results computed using the deep neural network implementations of these abstract architectures. A preliminary implementation of our proposed neural network architecture on FPGAs shows promising speed up compared to CPUs. This work paves the way to leverage efficient dedicated hardware designed for neural networks to solve high dimensional optimal control problems and Hamilton-Jacobi PDEs.
    
[^140]: 智能选择和选择单子

    Smart Choices and the Selection Monad. (arXiv:2007.08926v8 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2007.08926](http://arxiv.org/abs/2007.08926)

    该论文提出了基于选择和成本收益的系统描述方式，并从编程语言的角度研究了此方法。研究者定义了两种支持决策抽象的小语言，并给出了它们的操作语义和底层语义，并将底层语义增强为选择和概率单子。该研究通过应用于两个简单例子展示了此方法的实用性。

    

    用选择和相应的成本和收益来描述系统，有望使算法设计人员和程序员从指定如何进行选择中解放出来；在实际实现中，可以通过优化技术和越来越多的机器学习方法来实现这些选择。我们从编程语言的角度研究了这种方法。我们定义了两种支持决策抽象的小语言：一个具有选择和收益，另一个则额外加入了概率。我们给出了操作语义和底层语义。针对第二种语言，我们考虑了三种底层语义，它们在可能的程序值和预期回报之间具有不同程度的相关性。操作语义将标准构造的通常语义与可能的执行策略空间的优化相结合。基于选择单子的组合底层语义，增加第二种语言中的概率单子。我们展示了选择单子可以被理解为一种特殊类型的 continuation 单子，而语言的操作语义和底层语义则通过单子翻译相关。最后，我们通过将其应用于两个简单的例子，展示了我们方法的有用性。

    Describing systems in terms of choices and their resulting costs and rewards offers the promise of freeing algorithm designers and programmers from specifying how those choices should be made; in implementations, the choices can be realized by optimization techniques and, increasingly, by machine-learning methods. We study this approach from a programming-language perspective. We define two small languages that support decision-making abstractions: one with choices and rewards, and the other additionally with probabilities. We give both operational and denotational semantics.  In the case of the second language we consider three denotational semantics, with varying degrees of correlation between possible program values and expected rewards. The operational semantics combine the usual semantics of standard constructs with optimization over spaces of possible execution strategies. The denotational semantics, which are compositional, rely on the selection monad, to handle choice, augmente
    
[^141]: DeepEMD: 不同iable Earth Mover's Distance 用于few-shot学习

    DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning. (arXiv:2003.06777v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2003.06777](http://arxiv.org/abs/2003.06777)

    本文提出了一种新的few-shot图像分类方法，使用Earth Mover's Distance作为衡量图像结构距离的度量方法，并设计了交叉参考机制来生成EMD公式中元素的权重，实现了最优匹配流的生成，获得了在基准数据集上最先进的性能。

    

    本文从图像区域之间最优匹配的角度出发，提出了一种few-shot图像分类方法。我们采用Earth Mover's Distance（EMD）作为衡量密集图像表示结构距离的度量方法，以决定图像的相似度。EMD为结构元素生成最佳匹配流，这些流具有最小的匹配代价，用于计算分类的图像距离。我们设计了一个交叉参考机制来生成EMD公式中元素的权重，它可以有效缓解杂乱背景和大量类内外观变化所引起的不良影响。我们提出学习一个结构化全连接层，直接使用EMD对密集图像表示进行分类，以实现k-shot分类。基于隐函数定理，在网络中插入EMD层进行端到端的训练。在基准数据集上进行的广泛实验结果表明，我们的方法在few-shot图像分类上实现了最先进的性能。

    In this work, we develop methods for few-shot image classification from a new perspective of optimal matching between image regions. We employ the Earth Mover's Distance (EMD) as a metric to compute a structural distance between dense image representations to determine image relevance. The EMD generates the optimal matching flows between structural elements that have the minimum matching cost, which is used to calculate the image distance for classification. To generate the important weights of elements in the EMD formulation, we design a cross-reference mechanism, which can effectively alleviate the adverse impact caused by the cluttered background and large intra-class appearance variations. To implement k-shot classification, we propose to learn a structured fully connected layer that can directly classify dense image representations with the EMD. Based on the implicit function theorem, the EMD can be inserted as a layer into the network for end-to-end training. Our extensive experi
    
[^142]: 变分Wasserstein质心用于几何聚类

    Variational Wasserstein Barycenters for Geometric Clustering. (arXiv:2002.10543v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.10543](http://arxiv.org/abs/2002.10543)

    该论文提出了利用变分Wasserstein质心解决几何聚类问题的方法，特别是Monge WBs与K-means聚类和共同聚类相关，同时还提出了两个新问题——正则化K-means和Wasserstein质心压缩，并演示了VWBs在解决这些聚类相关问题的有效性。

    

    我们提出通过解决具有变分原理的Monge映射来计算Wasserstein质心(WBs)。我们讨论了WBs的度量特性，并探索它们的联系，特别是Monge WBs与K-means聚类和共同聚类的联系。我们还讨论了Monge WBs在非平衡度量和球形域上的可行性。我们提出了两个新问题——正则化K-means和Wasserstein质心压缩。我们演示了使用VWBs解决这些聚类相关问题的方法。

    We propose to compute Wasserstein barycenters (WBs) by solving for Monge maps with variational principle. We discuss the metric properties of WBs and explore their connections, especially the connections of Monge WBs, to K-means clustering and co-clustering. We also discuss the feasibility of Monge WBs on unbalanced measures and spherical domains. We propose two new problems -regularized K-means and Wasserstein barycenter compression. We demonstrate the use of VWBs in solving these clustering-related problems.
    
[^143]: 针对非线性系统的连续时间最优控制的松弛演员-评论家算法及其收敛性保证

    Relaxed Actor-Critic with Convergence Guarantees for Continuous-Time Optimal Control of Nonlinear Systems. (arXiv:1909.05402v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/1909.05402](http://arxiv.org/abs/1909.05402)

    本论文提出了一种叫做RCTAC的算法，可以针对具有已知动态和无限地平线的非线性连续时间系统找到几乎最优策略，且无需控制系统的特定属性或初始策略的可接受性，同时在收敛速度和控制性能方面均优于现有算法。

    

    本文提出了一种名为松弛连续时间演员-评论家（RCTAC）算法的方法，用于发现具有已知动态和无限地平线的非线性连续时间（CT）系统的几乎最优策略，例如车辆的路径跟踪控制。 与现有的自适应动态规划算法相比，RCTAC具有几个优点。它不需要初始化策略的“可接受性”或者控制系统的输入仿射性质以实现收敛。相反，给定任何初始策略，RCTAC都可以收敛到一个适用的，随后是几乎最优的策略，用于具有饱和控制器的一般非线性系统。RCTAC包括两个阶段：热身阶段和广义策略迭代阶段。热身阶段通过最小化哈密顿量的平方来实现可接受性，而广义策略迭代阶段放宽了更新终止条件以实现更快的收敛。算法的收敛性和最优性已被提出并证明。对于路径跟踪控制和倒立摆控制问题的模拟结果表明，RCTAC在收敛速度和控制性能方面优于现有的自适应动态规划算法。

    This paper presents the Relaxed Continuous-Time Actor-critic (RCTAC) algorithm, a method for finding the nearly optimal policy for nonlinear continuous-time (CT) systems with known dynamics and infinite horizon, such as the path-tracking control of vehicles. RCTAC has several advantages over existing adaptive dynamic programming algorithms for CT systems. It does not require the ``admissibility" of the initialized policy or the input-affine nature of controlled systems for convergence. Instead, given any initial policy, RCTAC can converge to an admissible, and subsequently nearly optimal policy for a general nonlinear system with a saturated controller. RCTAC consists of two phases: a warm-up phase and a generalized policy iteration phase. The warm-up phase minimizes the square of the Hamiltonian to achieve admissibility, while the generalized policy iteration phase relaxes the update termination conditions for faster convergence. The convergence and optimality of the algorithm are pro
    

