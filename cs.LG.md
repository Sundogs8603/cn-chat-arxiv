# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset.](http://arxiv.org/abs/2305.00767) | 本文提出了RViDeformer原始视频去噪变换器及其配套数据集ReCRVD，其中利用高低ISO设置重新捕捉现有视频以构建噪声-清晰对，同时探索了非本地时空依赖关系的解决方案。 |
| [^2] | [Unsupervised anomaly detection algorithms on real-world data: how many do we need?.](http://arxiv.org/abs/2305.00735) | 对52个真实多元表格数据集上的32个无监督异常检测算法进行了评估，结果表明第kNN算法在本地数据集上优于其他算法，而EIF算法在全球数据集上表现最佳，建议实际使用这三款无监督异常检测算法的工具箱。 |
| [^3] | [What Do Self-Supervised Vision Transformers Learn?.](http://arxiv.org/abs/2305.00729) | 本文比较了对比学习和遮蔽图像建模在表示和 下游任务表现方面的差异。实验证明，自监督视觉变压器利用对比学习时能够捕捉更长程的全局模式并线性分离图像，但在自我关注力的同质性、可扩展性和密集预测性能方面存在一些问题。 |
| [^4] | [Predictions Based on Pixel Data: Insights from PDEs and Finite Differences.](http://arxiv.org/abs/2305.00723) | 本文介绍了基于像素数据的预测，通过对离散卷积和有限差分算子之间联系的利用，证明了逼近自偏微分方程空时离散出的序列可以使用相对较小的卷积(残差)网络进行。 |
| [^5] | [Full Scaling Automation for Sustainable Development of Green Data Centers.](http://arxiv.org/abs/2305.00706) | 提出了一种全面自动化扩展（FSA）机制来改善数据中心的能源利用效率，该机制利用深度表征学习来预测每个服务的未来负载并自动稳定相应的目标CPU使用率水平。 |
| [^6] | [On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring.](http://arxiv.org/abs/2305.00684) | 本文研究了多智能体决策制定的样本有效、均衡计算和局部监控问题，提出了复杂度上下界和算法，并发现多智能体情况下可能呈指数级难度。 |
| [^7] | [Robustified Learning for Online Optimization with Memory Costs.](http://arxiv.org/abs/2305.00677) | 本文提出了一种新颖的专家鲁棒学习（ERL）方法，在内存成本的在线优化中实现了良好的平均性能和鲁棒性。 |
| [^8] | [Dynamic Transfer Learning across Graphs.](http://arxiv.org/abs/2305.00664) | 该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。 |
| [^9] | [Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks.](http://arxiv.org/abs/2305.00663) | 本文提供了一个合理的理论来解释神经网络的高维空间，并且将激活函数的角色描述为放大函数，将低维线性空间映射为无限维超级空间。 |
| [^10] | [Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition.](http://arxiv.org/abs/2305.00654) | 本文提出了一种基于奇异值分解的自动表征学习模型，可以获得保留转换结构的表示形式并捕捉状态访问的相对频率。该方法不需要转移矩阵，可以利用深度网络，适用于部分可观察领域，并且在多任务设置中表现良好。 |
| [^11] | [Discover and Cure: Concept-aware Mitigation of Spurious Correlation.](http://arxiv.org/abs/2305.00650) | 本研究提出了一个可解释的方法框架(DISC)来抑制深度神经网络中的假相关，通过发现不稳定的概念并将其作为假属性干预训练数据来提高模型的泛化能力和可解释性。在目标识别任务中，DISC胜过了现有最先进的方法。 |
| [^12] | [Inferring the past: a combined CNN-LSTM deep learning framework to fuse satellites for historical inundation mapping.](http://arxiv.org/abs/2305.00640) | 本文提出了一种整合CNN-LSTM深度学习框架的方法，将Sentinel-1卫星数据和MODIS数据融合，以推算孟加拉国历史洪水的部分淹没区域。 |
| [^13] | [Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding.](http://arxiv.org/abs/2305.00633) | 本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。 |
| [^14] | [Diffusion Models for Time Series Applications: A Survey.](http://arxiv.org/abs/2305.00624) | 扩散模型作为基于深度学习的一类生成模型，在时间序列预测、插值和生成方面的应用得到了广泛的探讨和发展。该综述对于新研究人员提供了基础的入门资源，并鼓励未来对于该领域的进一步探索。 |
| [^15] | [A Simplified Framework for Contrastive Learning for Node Representations.](http://arxiv.org/abs/2305.00623) | 本文研究了对比学习和图神经网络相结合的节点嵌入方法，通过简单的列处理嵌入矩阵取代同行处理，在提高结果嵌入质量和训练时间的同时，提高了下游分类任务的性能。 |
| [^16] | [Proper Scoring Rules for Survival Analysis.](http://arxiv.org/abs/2305.00621) | 本文研究了适用于生存分析的四种评分规则的扩展，证明在概率分布估计离散化程度满足一定条件时是适当评分规则，并且比较结果显示对数得分和布莱尔得分的扩展最佳。 |
| [^17] | [Self-supervised Activity Representation Learning with Incremental Data: An Empirical Study.](http://arxiv.org/abs/2305.00619) | 本研究探究了在移动感知环境中使用自监督表示学习模型对时间序列数据进行分类的影响，我们提出并评估了一种流程，其中模型学习使用未标记的时间序列数据提取信息特征，然后使用模型提取的特征对标记数据进行分类。 |
| [^18] | [Differentiable Neural Networks with RePU Activation: with Applications to Score Estimation and Isotonic Regression.](http://arxiv.org/abs/2305.00608) | 该论文介绍了使用RePU激活函数的可微分神经网络，在近似$C^s$平滑函数及其导数的同时建立了下限误差界，并证明了其在降低维度灾难方面的能力，此外还提出了一种使用RePU网络的惩罚保序回归(PDIR)方法。 |
| [^19] | [Classification and Online Clustering of Zero-Day Malware.](http://arxiv.org/abs/2305.00605) | 本文研究了零日恶意软件的分类和在线聚类。实验使用 EMBER 数据集，对有流入的恶意软件样本进行了分类，得到了 95.33% 的平衡准确度。在剩下的数据中，使用自组织映射实现了纯度从 47.61% 到 77.68% 的聚类。 |
| [^20] | [ISAAC Newton: Input-based Approximate Curvature for Newton's Method.](http://arxiv.org/abs/2305.00604) | ISAAC Newton方法提出了一种使用选择的二阶信息调整梯度的方法，并且在选择批量大小小于神经元数量的情况下，计算开销消失，能够在小批量随机情况下有效训练。 |
| [^21] | [Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation.](http://arxiv.org/abs/2305.00603) | 本文提出了一种名为 Consolidator 的 mergeable adapter with grouped connections for visual adaptation，促进了视觉 transformer 的知识转移，实现了多个图像分类和对象检测任务的最新转移表现。 |
| [^22] | [StyleGenes: Discrete and Efficient Latent Distributions for GANs.](http://arxiv.org/abs/2305.00599) | 我们提出了一种离散的潜在分布来代替连续的先验分布，这种基于基因的潜在编码可以通过少量可学习参数表示大量唯一的潜在样本，并且提供了新的直观的潜在空间探索方法。 |
| [^23] | [Incremental procedural and sensorimotor learning in cognitive humanoid robots.](http://arxiv.org/abs/2305.00597) | 本文提出了一个基于CONAIM模型的认知代理，能够逐步学习程序，通过增加新功能来解决之前无法解决的任务。在模拟环境中，使用增强学习的单一程序学习机制对人形机器人进行了物体跟踪实验。 |
| [^24] | [Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection.](http://arxiv.org/abs/2305.00595) | 本文通过在三个深度学习库中实现两种最先进的方法，并进行评估，研究了深度学习库对在线自适应轻量级时间序列异常检测的影响。 |
| [^25] | [Reliable Gradient-free and Likelihood-free Prompt Tuning.](http://arxiv.org/abs/2305.00593) | 本文提供一种能够应对挑战性情景，即仅具备API访问权限的情况下，建模API进行优化的方法，并能够对推理不确定性进行量化。 |
| [^26] | [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model.](http://arxiv.org/abs/2305.00586) | 本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。 |
| [^27] | [Joint Learning of Policy with Unknown Temporal Constraints for Safe Reinforcement Learning.](http://arxiv.org/abs/2305.00576) | 基于逻辑约束强化学习算法和进化算法的信号时态逻辑规范合成框架，同时学习安全约束和最优强化学习策略，并有理论保证。 |
| [^28] | [Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL.](http://arxiv.org/abs/2305.00567) | 本文提出了一种新的数据驱动离线MORL设置和一个用于离线MORL的算法PEDA。PEDA通过在多个模型中选择最优模型来实现正交偏好。实验表明，PEDA在样本效率方面优于现有方法，同时还可扩展到具有更大动作空间的复杂环境中。 |
| [^29] | [Class-Balancing Diffusion Models.](http://arxiv.org/abs/2305.00562) | 这项工作探究了扩散模型在类别不平衡的数据上的表现，并提出了一种解决方案“类平衡扩散模型”通过使用分布调整正则化器进行训练。 |
| [^30] | [Collective Relational Inference for learning physics-consistent heterogeneous particle interactions.](http://arxiv.org/abs/2305.00557) | 本论文提出了一种新的概率方法用于学习异质性粒子相互作用的集体关系推断，与现有方法相比，该方法集体地推断不同边的相互作用类型，使用物理感应的图神经网络来学习具有物理一致性的成对相互作用，并在推断准确性和保持物理保真度方面一致优于现有方法。 |
| [^31] | [Reconstructing seen images from human brain activity via guided stochastic search.](http://arxiv.org/abs/2305.00556) | 本研究使用条件生成扩散模型，改进过去的视觉重建算法，通过对一小组图像的采样和编码模型的选择，实现了从人脑活动中高质量、保留语义内容的重建结果，并发现了视觉皮层不同区域的重建时间差异。 |
| [^32] | [MD-Manifold: A Medical-Distance-Based Representation Learning Approach for Medical Concept and Patient Representation.](http://arxiv.org/abs/2305.00553) | MD-Manifold采用医学距离的表示学习方法，引入新的数据增强方法、概念距离度量和患者-患者网络，有效地将医学领域知识和先前数据信息纳入考虑，提高医疗保健分析任务的性能表现。 |
| [^33] | [SoK: Pragmatic Assessment of Machine Learning for Network Intrusion Detection.](http://arxiv.org/abs/2305.00550) | 本文提出了“实用评估”的概念，使从业人员能够评估ML方法在网络入侵检测（NID）中的实际价值，并提出了一组指导方针用于设计实验，可以在考虑实际系统的实际限制的情况下，比较不同ML方法在NID中的有效性和效率。 |
| [^34] | [Calibration Error Estimation Using Fuzzy Binning.](http://arxiv.org/abs/2305.00543) | 本文提出了一种模糊校准误差度量（FCE），利用模糊分箱方法计算校准误差，从而缓解了概率偏斜的影响并提供了更紧密的估计值。与传统指标ECE相比，FCE在多类设置中表现更好，https://github.com/srdgFHE/FCE-paper。 |
| [^35] | [Interpretability of Machine Learning: Recent Advances and Future Prospects.](http://arxiv.org/abs/2305.00537) | 本文综述了最新的机器学习可解释性进展和未来的前景，并提供了相关的多媒体计算应用示例，包括跨模态表示学习、人脸识别和物体识别等。研究显示，机器学习的可解释性研究有着重要的研究方向和前景。 |
| [^36] | [Nearly Optimal Steiner Trees using Graph Neural Network Assisted Monte Carlo Tree Search.](http://arxiv.org/abs/2305.00535) | 本文提出了一种利用图神经网络和蒙特卡罗树搜索计算斯坦纳树的方法，它在各种类型的图上优于标准的2近似算法，常常找到最优解。 |
| [^37] | [ICQ: A Quantization Scheme for Best-Arm Identification Over Bit-Constrained Channels.](http://arxiv.org/abs/2305.00528) | 本文提出了一种名为ICQ的新颖量化方案，可用于现有的置信区间学习算法，如连续淘汰算法，并在位限制信道上实现最佳臂识别，具有与未量化算法相同的阶优化样本复杂度，且仅需要极少量的通信频率。 |
| [^38] | [StyleLipSync: Style-based Personalized Lip-sync Video Generation.](http://arxiv.org/abs/2305.00521) | 本文提出了一种基于风格的个性化唇形动画视频生成模型，可以准确地生成任意身份的唇形同步视频，且可用于增强未见面孔的特征。 |
| [^39] | [The ART of Transfer Learning: An Adaptive and Robust Pipeline.](http://arxiv.org/abs/2305.00520) | 本文提出了自适应稳健转移学习（ART）管道，使用通用机器学习算法实现转移学习，建立了非渐近学习理论，同时防止负面转移，并演示了它在回归、分类和稀疏学习上的良好性能。 |
| [^40] | [Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse.](http://arxiv.org/abs/2305.00510) | 本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。 |
| [^41] | [Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward.](http://arxiv.org/abs/2305.00508) | 本文提出了SEA算法，可在成就型环境中进行探索任务。SEA首先学习已知成就的表示和依赖关系图，然后通过构建控制器在线探索新成就。实验证明SEA能够准确地恢复成就结构并改善在一些复杂领域中的探索性能。 |
| [^42] | [Domain Agnostic Fourier Neural Operators.](http://arxiv.org/abs/2305.00478) | 介绍了一种新的神经算子架构 DAFNO，可以学习带有不规则几何和不断变化的域的代理。通过将平滑化的特征函数纳入 FNOs 的积分层架构中，并利用 FFT 来实现快速计算，以明确的方式将几何信息编码到架构中，DAFNO 相对于基线神经算子模型具有最先进的精度。 |
| [^43] | [Posterior Sampling for Deep Reinforcement Learning.](http://arxiv.org/abs/2305.00477) | 本文提出了用于深度强化学习的后验采样算法PSDRL，结合了高效的不确定性量化和特殊设计的持续规划算法，使其在提高样本效率的同时显著优于之前的尝试。 |
| [^44] | [Time series clustering based on prediction accuracy of global forecasting models.](http://arxiv.org/abs/2305.00473) | 本论文提出了一种基于预测准确性的时间序列聚类新方法，可以用于选择时间序列数据库中的聚类数，并且比传统方法更好。 |
| [^45] | [Efficient MILP Decomposition in Quantum Computing for ReLU Network Robustness.](http://arxiv.org/abs/2305.00472) | 本研究提出了一种用于ReLU网络鲁棒性的新型MILP量子分解方法，尤其针对解决量子位可用性、噪声和误差限制，可实现更高的成功率和更高的效率与精确度，对量子计算技术的发展有重要意义。 |
| [^46] | [Hypergraphs with Edge-Dependent Vertex Weights: Spectral Clustering based on the 1-Laplacian.](http://arxiv.org/abs/2305.00462) | 本文提出了一个超图聚类的框架，并使用依赖于边权的顶点权重来提高超图模型的表现力。根据理论分析，利用与1-Laplacian的第二小特征值相关的特征向量能够达到比传统方法更高的聚类精度。在实际数据集上的实验证明了这种方法的有效性。 |
| [^47] | [Predictability of Machine Learning Algorithms and Related Feature Extraction Techniques.](http://arxiv.org/abs/2305.00449) | 本论文通过矩阵因式分解设计了一个预测系统来预测特定数据集上特定模型的分类准确度，同时研究了随机森林、XGBoost和MLP等三种机器学习算法的性能预测，并得出了三个结论。 |
| [^48] | [Multi-Task Structural Learning using Local Task Similarity induced Neuron Creation and Removal.](http://arxiv.org/abs/2305.00441) | 本文提出了一种名为“多任务结构学习（MTSL）”的方法，可以同时学习多任务架构及其参数，其主要贡献在于将局部任务相似性纳入神经元创建和移除，从而提高了神经网络的泛化能力。 |
| [^49] | [META-SMGO-$\Delta$: similarity as a prior in black-box optimization.](http://arxiv.org/abs/2305.00438) | META-SMGO-$\Delta$ 通过将相似问题的先验知识应用到优化过程中来提高求解相似问题的效率。 |
| [^50] | [Transfer of knowledge among instruments in automatic music transcription.](http://arxiv.org/abs/2305.00426) | 本研究展示了如何利用软件合成器产生的合成音频数据来培训通用模型，为进一步的转移学习提供了良好的基础。研究结果表明，用合成数据进行培训可以成为预训练通用模型的良好基础，其中转录任务并不仅限于一个乐器。 |
| [^51] | [Exploring the Effectiveness of Large Language Models in Generating Unit Tests.](http://arxiv.org/abs/2305.00418) | 本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。 |
| [^52] | [Indexability of Finite State Restless Multi-Armed Bandit and Rollout Policy.](http://arxiv.org/abs/2305.00410) | 本文研究了有限状态不想静止多臂赌博机问题，提出了一种应用rollout策略的算法来解决问题，并且在单臂赌博机模型上展示了结构结果和可索引性。 |
| [^53] | [Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering.](http://arxiv.org/abs/2305.00393) | 本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。 |
| [^54] | [Importance Weighted Expectation-Maximization for Protein Sequence Design.](http://arxiv.org/abs/2305.00386) | 本文提出了一种名为IsEM-Pro的方法，用于根据给定适应性标准生成蛋白质序列。在推理期间，从其潜在空间采样可以增加多样性，指导了探索高适应性区域。实验表明，相比先前最佳方法，IsEM-Pro的平均适应性得分至少高出55％，并生成了更多样化和新颖的蛋白质序列。 |
| [^55] | [DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning.](http://arxiv.org/abs/2305.00380) | DualHSIC通过利用跨任务关系，提高了基于重新学习方法的连续学习性能。 |
| [^56] | [Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization.](http://arxiv.org/abs/2305.00374) | 本文提出了一种对抗性不变正则化方法（AIR）来强制对抗性对比学习（ACL）的学习表示呈现样式独立性，并用加权SIR和AIR实现ACL的鲁棒性增强。实验证实，该方法在各种对抗攻击和常见污染下均显著提高ACL的鲁棒性，并在多个基准测试中实现了最先进的性能。 |
| [^57] | [S2abEL: A Dataset for Entity Linking from Scientific Tables.](http://arxiv.org/abs/2305.00366) | 该论文提供了第一个专注于科学表格的 EL 数据集 S2abEL，用于实体链接任务。由于科学知识库的不完整性和语境影响，科学表格上的 EL 具有挑战性，该数据集专注于机器学习结果表中的 EL，包含手工标记的单元格类型、属性和实体链接，并引入了一种优于其他方法的神经基线方法。 |
| [^58] | [ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings.](http://arxiv.org/abs/2305.00365) | ReLBOT使用转移学习和深度RL技术来从现有的智能建筑中传递优化参数到新的建筑中，以减少强化学习代理引起的初始不适，有效降低了风险，并且实现了热身期时长6.2倍的提高和预测方差的132倍提高。 |
| [^59] | [Electricity Price Prediction for Energy Storage System Arbitrage: A Decision-focused Approach.](http://arxiv.org/abs/2305.00362) | 该论文提出了一种以决策为中心的电力价格预测方法，用于电力储能系统套利，通过遗憾度量预测价值下的实际决策和在实际价值下的最优决策之间的差异，学习提高预测和决策的准确性。 |
| [^60] | [POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models.](http://arxiv.org/abs/2305.00350) | 本文提出了一种基于提示的无监督微调框架，可以在未标记的目标数据上微调大型预训练模型以适应下游任务，实验结果表明该方法在图像分类、情感分析和自然语言推理等任务中表现更好。 |
| [^61] | [Representing Additive Gaussian Processes by Sparse Matrices.](http://arxiv.org/abs/2305.00324) | 本研究展示了对于加性Matérn高斯过程，通过稀疏矩阵和向量的公式可以有效地计算后验均值、后验方差、对数似然和梯度。 |
| [^62] | [Toward $L_\infty$-recovery of Nonlinear Functions: A Polynomial Sample Complexity Bound for Gaussian Random Fields.](http://arxiv.org/abs/2305.00322) | 本文利用随机性证明了从高斯随机场中绘制的随机基础事实函数的$L_\infty$-recovery可以使用多项式样本具有多项式样本复杂度边界。 |
| [^63] | [Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data.](http://arxiv.org/abs/2305.00320) | 本篇论文提出了一种名为MMSF的模型，能够保留模态特定的知识，提高受污染多模态图像的鲁棒性。同时，还采用了三种最先进的基于注意力的多模态融合模型，在V-I ReID中适应受污染多模态数据，动态平衡每种模态的重要性。 |
| [^64] | [Learning to Re-rank with Constrained Meta-Optimal Transport.](http://arxiv.org/abs/2305.00319) | 本文提出了一种快速、轻量级的约束元最优输运算法(CMOT)，用于预测公平随机重新排序策略，并使用一种新的采样算法VARN-SAM，比Birkhoff-von-Neumann分解(BvND)更有效。实验结果显示，CMOT实现了公平重新排序的最先进性能，同时比其他方法更快且更可扩展。 |
| [^65] | [The Ideal Continual Learner: An Agent That Never Forgets.](http://arxiv.org/abs/2305.00316) | 本文提出了一个新的持续学习框架——理想的持续学习者（ICL），通过构建避免灾难性遗忘，统一了多个持续学习方法，并为这些方法的优缺点提供了新的理论见解。 |
| [^66] | [Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning.](http://arxiv.org/abs/2305.00312) | 该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。 |
| [^67] | [A Coupled Flow Approach to Imitation Learning.](http://arxiv.org/abs/2305.00303) | 本文提出了一种新的模仿学习算法Coupled Flow Imitation Learning（CFIL），使用正则流模型的分布匹配来建模状态分布和状态行为分布。在基准任务中具有单个专家轨迹表现出最先进的性能。 |
| [^68] | [Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning.](http://arxiv.org/abs/2305.00286) | 该论文提出了一种基于自监督任务表示学习的元强化学习算法MoSS，可使人工智能代理能够适应未探索的任务分布及快速适应新任务。 |
| [^69] | [Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected.](http://arxiv.org/abs/2305.00278) | 这项工作对段落任意模型（SAM）的能力在玻璃相关的情况下进行了实证评估，发现SAM在镜面和透明物体中往往无法检测玻璃，这引起了在具有各种形式的玻璃的安全关键情况下部署SAM的关注。 |
| [^70] | [Ensemble Learning for CME Arrival Time Prediction.](http://arxiv.org/abs/2305.00258) | 该论文提出了一种名为CMETNet的集成学习方法，用于预测太阳喷发日冕物质抛射(CME)从太阳到地球的到达时间，以减少对人类系统的损害。 |
| [^71] | [Brain Tumor Segmentation from MRI Images using Deep Learning Techniques.](http://arxiv.org/abs/2305.00257) | 该论文使用深度学习模型对MRI成像数据进行处理，实现了对脑肿瘤的精确分割，为提高医学专家的工作效率提供了可行性。 |
| [^72] | [A Direct Sampling-Based Deep Learning Approach for Inverse Medium Scattering Problems.](http://arxiv.org/abs/2305.00250) | 提出了一种基于直接抽样的深度学习方法，利用U-Net神经网络学习指数函数和真实对比度之间的关系，实现了对逆中介质散射问题的高质量重建。 |
| [^73] | [Leveraging Unlabelled Data in Multiple-Instance Learning Problems for Improved Detection of Parkinsonian Tremor in Free-Living Conditions.](http://arxiv.org/abs/2305.00249) | 本文提出了一种利用无标签数据和少量标记数据相结合的方法进行多实例学习，以提高自由生活条件下帕金森病震颤检测的效果。 |
| [^74] | [Industry Classification Using a Novel Financial Time-Series Case Representation.](http://arxiv.org/abs/2305.00245) | 本论文提出一种新型金融时间序列案例表示法，可用于行业部门分类任务，通过对股票收益嵌入的表示，显着提高了模型性能。 |
| [^75] | [A Critical Analysis of the Limitation of Deep Learning based 3D Dental Mesh Segmentation Methods in Segmenting Partial Scans.](http://arxiv.org/abs/2305.00244) | 该论文发现目前的基于深度学习的牙齿分割算法对于局部扫描的检测效果较差，限制了其广泛应用。 |
| [^76] | [When Deep Learning Meets Polyhedral Theory: A Survey.](http://arxiv.org/abs/2305.00241) | 本文综述了深度学习与多面体理论的交叉领域。修正线性单元（ReLU）等函数使得一些神经网络结构能够通过多面体理论进行分析，应用线性和混合整数线性规划来实现网络修剪、鲁棒性分析和神经网络验证等任务。 |
| [^77] | [The FAIRy Tale of Genetic Algorithms.](http://arxiv.org/abs/2305.00238) | 本文扩展了FAIR数据原则以提高遗传算法（GA）的可重复性和可重用性，并提出了一个采用轻量级RDF格式的术语表以促进可重复性。 |
| [^78] | [Accelerated and Inexpensive Machine Learning for Manufacturing Processes with Incomplete Mechanistic Knowledge.](http://arxiv.org/abs/2305.00229) | 本文提出了一种基于迁移学习的加速和廉价的机器学习方法，通过在来自物理过程模型的计算成本低的数据上进行训练，然后在少量高成本实验数据上进行微调，能够在受机械振动和温度变化影响的制造过程中，提高零件质量预测的准确性并显著减少实验数据量。 |
| [^79] | [An Empirical Comparison of Optimizers for Quantum Machine Learning with SPSA-based Gradients.](http://arxiv.org/abs/2305.00224) | 本文对比了基于梯度和基于SPSA算法的量子机器学习优化器的性能，提出了一种新方法，该方法结合了来自SPSA的近似梯度和最先进的基于梯度的经典优化器，在简单回归中达到更好的收敛效果和更小的误差。 |
| [^80] | [PathRTM: Real-time prediction of KI-67 and tumor-infiltrated lymphocytes.](http://arxiv.org/abs/2305.00223) | 本文介绍了 PathRTM，一种用于自动化的KI-67增殖和肿瘤浸润淋巴细胞估计的深度神经网络检测器，在这篇论文中，作者展示了通过在PathRTM中添加自动生成的边界框标签可以显著提高估算精度。 |
| [^81] | [Physics-Guided Graph Neural Networks for Real-time AC/DC Power Flow Analysis.](http://arxiv.org/abs/2305.00216) | 本文提出了一种物理引导下的图神经网络，通过提升拓扑适应性并嵌入物理原理，实现了快速的交、直流电力流分析工具，并取得了比其他竞争对手更好的效果和计算效率。 |
| [^82] | [ShipHullGAN: A generic parametric modeller for ship hull design using deep convolutional generative model.](http://arxiv.org/abs/2305.00210) | ShipHullGAN是一个通用的船体参数化建模器，使用深度卷积生成对抗网络进行训练，可以生成和表征各种类型的船舶。通过新的形状提取和表示策略，将所有训练设计转换为相同分辨率的几何表示形式，并在生成器后添加空间填充层，以确保生成器可以覆盖所有设计类别。 |
| [^83] | [Data-Driven Subgroup Identification for Linear Regression.](http://arxiv.org/abs/2305.00195) | 本文提出了一个基于数据驱动方法的DDGroup，可以有效地识别具有特征与标签之间统一线性关系的子群，为医学研究提供了一种新的统计工具。 |
| [^84] | [An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System.](http://arxiv.org/abs/2305.00169) | 本文提出了一种基于证据推理算法和广义学习系统的实时多模态故障诊断方法，该方法在更新模型参数和计算效率方面具有优势，并且在基准数据集上取得了比现有方法更好的故障诊断性能。 |
| [^85] | [The Combination of Metal Oxides as Oxide Layers for RRAM and Artificial Intelligence.](http://arxiv.org/abs/2305.00166) | 本文综述了金属氧化物作为阻变存储器氧化层在RRAM和人工智能中的应用。通过AI技术的应用可优化RRAM器件性能，同时RRAM器件本身也可以作为硬件加速器和神经形态学计算的动力源来推动AI的发展。 |
| [^86] | [Taming graph kernels with random features.](http://arxiv.org/abs/2305.00156) | 本论文介绍了一种基于随机特征的图形随机特征（GRFs）机制，能够有效解决图内核算法时间复杂度是节点数的立方的问题，从而可将定义在图上的内核方法扩展到更大的网络中。 |
| [^87] | [Limits of Model Selection under Transfer Learning.](http://arxiv.org/abs/2305.00152) | 这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。 |
| [^88] | [Sequential Predictive Two-Sample and Independence Testing.](http://arxiv.org/abs/2305.00143) | 本文提出了一种基于预测的赌博策略来解决高维或结构化数据下非参数双样本和独立性检验问题。 |
| [^89] | [Leveraging Label Non-Uniformity for Node Classification in Graph Neural Networks.](http://arxiv.org/abs/2305.00139) | 本文针对标签不均匀的节点分类问题，提出了一种利用标签不均匀性进行图神经网络中的节点分类方法，在理论上分析了标签不均匀性在整个图中的变化，并提供了两种提高模型性能的方式，经实验证明可行。 |
| [^90] | [Joint Sensing, Communication, and AI: A Trifecta for Resilient THz User Experiences.](http://arxiv.org/abs/2305.00135) | 本文提出了一个THz用户体验鲁棒性的三位一体框架，包括联合感知、通信和人工智能。通过利用THz通道的稀疏性提取用户和环境的独特感知参数，结合多分辨率生成式AI来预测未来信息，然后提出了一个新颖的联合感知-通信协议来优化XR体验。 |
| [^91] | [Optimal Scheduling in IoT-Driven Smart Isolated Microgrids Based on Deep Reinforcement Learning.](http://arxiv.org/abs/2305.00127) | 本文使用深度强化学习解决物联网驱动智能孤网微电网中柴油发电机组的调度问题，通过学习历史数据生成实时决策以确保供需平衡，并减少操作成本。 |
| [^92] | [Improving CFD simulations by local machine-learned correction.](http://arxiv.org/abs/2305.00114) | 该研究引入一种使用机器学习模型逆向估算网格粗化导致信息丢失的方法，从而在CFD模拟中加入这些信息，提高低分辨率网格模拟的质量。 |
| [^93] | [Active Reinforcement Learning for Personalized Stress Monitoring in Everyday Settings.](http://arxiv.org/abs/2305.00111) | 本文提出了一种上下文感知的主动学习策略，以在部署时间为个性化微调模型的情况下，在日常环境中进行细粒度的压力检测。 |
| [^94] | [NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation.](http://arxiv.org/abs/2305.00097) | NNSplitter是一种主动保护深度神经网络模型知识产权的方案，通过将模型分为混淆模型和模型秘密两部分，采用可信执行环境和基于强化学习的控制器来最大化精度下降和减少混淆权重的数量。 |
| [^95] | [Improving Gradient Computation for Differentiable Physics Simulation with Contacts.](http://arxiv.org/abs/2305.00092) | 本文研究了接触情况下的可微分刚体模拟，发现现有的方法在接触法线方向不固定时会提供不准确的梯度，提出了一种通过连续的碰撞检测和利用碰撞时间来计算碰撞后的速度来改进梯度计算的方法TOI-Velocity。 |
| [^96] | [On the existence of solutions to adversarial training in multiclass classification.](http://arxiv.org/abs/2305.00075) | 本文研究了多类分类中敌对训练的鲁棒解存在性问题，证明了每个模型中存在 Borel 可测的鲁棒分类器，并与最优传输和总变差正则化建立了联系。在二元分类问题中，对不可知分类器的敌对训练问题存在 Borel 可测的解。 |
| [^97] | [Online Platt Scaling with Calibeating.](http://arxiv.org/abs/2305.00070) | 本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。 |
| [^98] | [Wearing face mask detection using deep learning through COVID-19 pandemic.](http://arxiv.org/abs/2305.00068) | 本研究探讨了在 COVID-19 疫情期间使用深度学习模型进行口罩佩戴检测的可行性。通过比较不同模型，选择了适用于实时和移动设备应用的最佳模型，并取得了高准确度。 |
| [^99] | [LAVA: Data Valuation without Pre-Specified Learning Algorithms.](http://arxiv.org/abs/2305.00054) | LAVA是一个学习算法无关的数据价值评估方法，它结合了学习算法的统计特性和训练数据的属性，通过迭代估计数据值来实现。LAVA比现有方法计算速度更快，精度更高，并且可以为不同的应用提供有意义的数据排名。 |
| [^100] | [Causal Reasoning and Large Language Models: Opening a New Frontier for Causality.](http://arxiv.org/abs/2305.00050) | 大型语言模型在因果推理任务中取得了新的最高准确率，但是其鲁棒性仍然存在难以预测的失败模式。 |
| [^101] | [Verification against in-situ observations for Data-Driven Weather Prediction.](http://arxiv.org/abs/2305.00048) | 数据驱动气象预测模型（DDWP）近年来发展迅速，但需要更加严格的真实观测验证来在操作预报中更安全地使用。 |
| [^102] | [Hedonic Prices and Quality Adjusted Price Indices Powered by AI.](http://arxiv.org/abs/2305.00044) | 本研究提出了一种基于深度神经网络和转换器的经验享乐模型，能够处理大量未结构化的产品数据，准确地估计产品的享乐价格和派生指数。 |
| [^103] | [Adversarial Representation Learning for Robust Privacy Preservation in Audio.](http://arxiv.org/abs/2305.00011) | 本研究提出了一种对抗性训练方法，用于学习音频的表征，从而有效地防止从音频记录的潜在特征中检测到语音活动，提出的方法能够使得包含语音的音频记录的潜在表征与不包含语音的音频记录的潜在表征无法被语音分类器区分出来。 |
| [^104] | [The R\'io Hortega University Hospital Glioblastoma dataset: a comprehensive collection of preoperative, early postoperative and recurrence MRI scans (RHUH-GBM).](http://arxiv.org/abs/2305.00005) | 这份数据集提供了包括MRI图像、容积评估、分子数据和生存细节在内的Glioblastoma患者相关数据，同时提供了专家纠正的肿瘤亚区划分，为发展术后和随访MRI扫描的算法提供了有价值的基准数据。 |
| [^105] | [Accurate ignition detection of solid fuel particles using machine learning.](http://arxiv.org/abs/2305.00004) | 本研究使用高速光学诊断技术和机器学习方法，对单颗粒点火过程进行精确确定。经过训练，使用FPN和ResNet网络均可显著提高检测的准确性和精度。本研究发现，FPN的分层特征在检测点火过程中更为有利。 |
| [^106] | [Neural Network Accelerated Process Design of Polycrystalline Microstructures.](http://arxiv.org/abs/2305.00003) | 通过神经网络加速工艺设计，减轻预测微结构演化的计算负担，并找到最佳加工路径。 |
| [^107] | [Galaxy Classification Using Transfer Learning and Ensemble of CNNs With Multiple Colour Spaces.](http://arxiv.org/abs/2305.00002) | 本研究使用多种颜色空间和CNN架构的集成方法，结合迁移学习技术，提出了一个自动分类星系的方法，并在Kaggle Galaxy Zoo数据集上实现了最先进的准确性。 |
| [^108] | [Feature Embedding Clustering using POCS-based Clustering Algorithm.](http://arxiv.org/abs/2305.00001) | 本文提出了一种新的聚类技术POCS-based clustering algorithm，将POCS收敛性质应用于聚类问题，相比于其他经典聚类方案具有更好的表现，可以用于解决特征嵌入聚类问题。 |
| [^109] | [The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share & Requests.](http://arxiv.org/abs/2304.14882) | ACM Multimedia 2023 计算语言学挑战赛涉及情感共享和请求检测，提供了基线特征提取和分类器方法。 |
| [^110] | [Segment Anything Model for Medical Images?.](http://arxiv.org/abs/2304.14660) | “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。 |
| [^111] | [Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark.](http://arxiv.org/abs/2304.14343) | 本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。 |
| [^112] | [TorchBench: Benchmarking PyTorch with High API Surface Coverage.](http://arxiv.org/abs/2304.14226) | TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。 |
| [^113] | [ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries.](http://arxiv.org/abs/2304.13620) | 本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。 |
| [^114] | [Structure Diagram Recognition in Financial Announcements.](http://arxiv.org/abs/2304.13240) | 本文提出了一种新的识别金融公告中结构图的方法，并通过两阶段方法生成了行业的第一个基准。实验结果显示，与现有方法相比，本文所提出的方法具有更高的有效性和效率。 |
| [^115] | [A Study on Improving Realism of Synthetic Data for Machine Learning.](http://arxiv.org/abs/2304.12463) | 本文研究并评估了一个合成到真实的生成模型，将合成渲染转换为更真实的风格以适用于通用数据集，并通过下游感知任务来量化和定性地评估其性能。 |
| [^116] | [Sparse Private LASSO Logistic Regression.](http://arxiv.org/abs/2304.12429) | 本文提出了一种稀疏逻辑回归的差分隐私方法，保持硬零，通过训练一个非私有的LASSO逻辑回归模型决定最后的模型选择中恰当的私有化非零系数的数量。 |
| [^117] | [Incorporating Experts' Judgment into Machine Learning Models.](http://arxiv.org/abs/2304.11870) | 本文提出了一种新的框架，利用生成对抗网络确定未标记数据点在训练数据中的代表程度，再根据这个程度将专家的判断融入机器学习模型，以减轻预测结果与专家判断之间的冲突。 |
| [^118] | [Autoregressive models for biomedical signal processing.](http://arxiv.org/abs/2304.11070) | 本文提出了一种新的自回归建模框架，通过超参数化损失函数来明确纳入数据不确定性，展示了该程序可以成功地去噪时间序列并成功重构系统参数。该范式可在神经科学的多种应用中使用。 |
| [^119] | [CKmeans and FCKmeans : Two Deterministic Initialization Procedures For Kmeans Algorithm Using Crowding Distance.](http://arxiv.org/abs/2304.09989) | 本论文提出了两种新型的确定性初始化过程（CKmeans和FCKmeans）以改进Kmeans聚类，并且实验证明这些过程在聚类准确度方面优于传统的Kmeans和Kmeans++。 |
| [^120] | [SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts.](http://arxiv.org/abs/2304.09548) | SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。 |
| [^121] | [A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation.](http://arxiv.org/abs/2304.07772) | 本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。 |
| [^122] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |
| [^123] | [Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search.](http://arxiv.org/abs/2304.05405) | 本文综述了最近在不同iable神经架构搜索中的研究进展，提出了一种新的基于挑战的分类法，对DARTS方法的贡献和影响进行了讨论，并探讨了未来的研究方向。 |
| [^124] | [Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization.](http://arxiv.org/abs/2303.17354) | 该论文提出一种基于Transformer骨干网络的渐进式自监督学习方法，可用于图像异常检测和定位，其中第一阶段使用MAE模型进行正常图像的训练，第二阶段使用像素级数据增强技术来生成损坏的正常图像，最终通过像素重建误差矩阵和像素异常概率矩阵综合得到一个异常得分矩阵。 |
| [^125] | [Ablating Concepts in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2303.13516) | 本论文提出了一种有效的方法，在不重新训练模型的情况下实现了预训练模型中的概念消融，可以消除文本到图像生成中的版权问题和样本记忆问题。 |
| [^126] | [Bayesian Optimization for Function Compositions with Applications to Dynamic Pricing.](http://arxiv.org/abs/2303.11954) | 本文提出了一种基于贝叶斯优化的函数组合方法，可以解决黑盒函数难以评估的问题，并在动态定价中得到应用。 |
| [^127] | [A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing.](http://arxiv.org/abs/2303.10446) | 该论文提出了一种用于音频信号处理的内容自适应可学习时频表示法，通过学习卷积滤波器与变换器架构来将小的波形块投影到小的潜在维度上。 |
| [^128] | [Could a Large Language Model be Conscious?.](http://arxiv.org/abs/2303.07103) | 本文分析了大型语言模型是否具有意识的可能性，目前的模型存在着意识的显著障碍，但未来十年随着障碍被克服，后继的大型语言模型可能会具有意识。 |
| [^129] | [Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings.](http://arxiv.org/abs/2303.05737) | 本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。 |
| [^130] | [KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input.](http://arxiv.org/abs/2303.05617) | 本文提出了一种基于关键点的RGB-D输入的六自由度抓取姿态合成方法，既可以从关键点检测中预测抓取姿态，也可以预测相对于相机的尺度，实验结果表明其优越性。 |
| [^131] | [Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation.](http://arxiv.org/abs/2303.04772) | 本文介绍了无限维度得分扩散模型在多个分辨率水平上的离散化方法，并使用多级扩散算法在多个分辨率上高效地学习。实证表明，该模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本，并可以生成不同分辨率的图像并处理矩形域。 |
| [^132] | [Extrapolative Controlled Sequence Generation via Iterative Refinement.](http://arxiv.org/abs/2303.04562) | 本文提出了一种名为ICE的新方法来解决外推控制序列生成问题，该方法使用迭代控制编辑技术，能够在自动设计领域，特别是药物研究领域取得较优的性能表现。 |
| [^133] | [Local Environment Poisoning Attacks on Federated Reinforcement Learning.](http://arxiv.org/abs/2303.02725) | 本文提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。 |
| [^134] | [R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents.](http://arxiv.org/abs/2303.00732) | R-U-SURE是一种考虑不确定性的代码建议方法，利用生成模型的随机样本作为代理生成提高效用的实现，并结合了决策论模型、最小贝叶斯风险解码、双重分解和决策图，能够在不需要完整代码的情况下生成结构化不确定性摘要。 |
| [^135] | [Targeted demand response for flexible energy communities using clustering techniques.](http://arxiv.org/abs/2303.00186) | 本研究探讨了使用机器学习算法中的聚类技术设计并执行需求响应（DR）计划的可行性，目的是改变分布式能源社区内供应者的消费行为，以最小化反向功率流和削减系统范围内的功峰需求。 |
| [^136] | [AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers.](http://arxiv.org/abs/2302.14705) | 本论文提出了一种稀疏感知加速器AccelTran，用于动态推理中的Transformer，通过运行时修剪激活并优化数据流实现了高性能和能量效率的平衡，相较于其他现有加速器，AccelTran在模型尺寸和内存带宽需求较低的情况下实现了平均1.9倍的加速比，同时保持了推理精度。 |
| [^137] | [A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT.](http://arxiv.org/abs/2302.09419) | 本文全面回顾了预训练基础模型的最新研究进展和发展历程，包括它们的架构、培训目标、预培训任务、微调策略和评估。同时，讨论了其局限性和未来研究方向。 |
| [^138] | [3D-aware Conditional Image Synthesis.](http://arxiv.org/abs/2302.08509) | 我们提出了一个三维感知的条件生成模型，通过给定的2D标签图合成相应的图像，并扩展了条件生成模型来实现明确的三维用户控制；我们还建立了一个交互式系统，允许用户随意编辑标签图并生成输出。 |
| [^139] | [Fast evaluation of spherical harmonics with sphericart.](http://arxiv.org/abs/2302.08381) | 该论文提出了一种sphericart库中实现的，高效而优雅的算法来评估实值球谐函数，其中具有现有方案的许多理想特性，并允许以数值稳定和计算效率高的方式计算笛卡尔导数。 |
| [^140] | [Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral Learning of Hidden Markov Models.](http://arxiv.org/abs/2302.07437) | 本文研究了隐马尔可夫模型谱学习中存在的问题，并提出了解决方案，包括提供了SHMM似然估计的误差渐近分布、提出投影SHMM算法可以减轻误差传播问题、并开发了SHMM和PSHMM的在线学习变体以适应潜在的非平稳性。研究结果表明PSHMM具有更好的性能表现。 |
| [^141] | [Bandit Social Learning: Exploration under Myopic Behavior.](http://arxiv.org/abs/2302.07425) | 该论文研究了自私行为下的劫匪社交学习问题，发现存在一种探索激励权衡，即武器探索和社交探索之间的权衡，受到代理的短视行为的限制会加剧这种权衡，并导致遗憾率与代理数量成线性关系。 |
| [^142] | [Direct Parameterization of Lipschitz-Bounded Deep Networks.](http://arxiv.org/abs/2301.11526) | 本文提出了一种直接参数化的深度神经网络，其具有拉普拉斯界限，通过标准梯度方法进行训练，避免了计算密集型的投影或障碍项。 |
| [^143] | [Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content.](http://arxiv.org/abs/2301.10871) | 本研究探讨了一种利用图形转换器网络和基于BERT的自然语言处理模型，结合注意力机制，预测社交媒体中仇恨言论的方法。该方法通过考虑帖子后续的讨论来成功检测出可能出现仇恨言论的情况。研究还探讨了所提出模型的有效性和不足，以及未来可以将其扩展到更全面的方向。 |
| [^144] | [Gossiped and Quantized Online Multi-Kernel Learning.](http://arxiv.org/abs/2301.09848) | 本文扩展了在线多核学习的研究，提出了一种八卦算法，证明了其在非完全连接的网络环境下可以达到亚线性后悔，具有较好的性能表现。 |
| [^145] | [Score-based Causal Representation Learning with Interventions.](http://arxiv.org/abs/2301.08230) | 本文研究了当潜在因果变量通过未知线性转换间接观察时的因果表征学习问题。其充分条件确保了干预效果可以从分数的变化中正确检测出来，并利用最小化分数函数变化的关键特性完美恢复有效变换。 |
| [^146] | [Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection.](http://arxiv.org/abs/2301.07015) | 本论文研究了机器人检测中现有数据集的局限性，并提出了简单的决策规则，进一步揭示了数据集收集和标记的限制对机器人检测准确性的影响。 |
| [^147] | [Neural Radiance Field Codebooks.](http://arxiv.org/abs/2301.04101) | 某项研究提出了一种新的方法 NRC，能够学习复杂场景、实现对象为中心的表示、并能够自动分割，在THOR中的对象导航中表现出良好的转移能力。 |
| [^148] | [Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer.](http://arxiv.org/abs/2301.01664) | 本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，提高基于句子Transformer的知识推理句子Transformer的性能，从而在KG中实现了多方面的可解释感知关系预测。 |
| [^149] | [Hungry Hungry Hippos: Towards Language Modeling with State Space Models.](http://arxiv.org/abs/2212.14052) | 本文针对SSMs在语言建模上表现不足以及硬件利用率低下的问题，提出了一种新的SSM层H3，并将其与建模关注机制相结合，通过硬件优化实现了语言建模基准的最新性能，突出SSMs在语言建模中的潜力。 |
| [^150] | [Knowledge-Guided Data-Centric AI in Healthcare: Progress, Shortcomings, and Future Directions.](http://arxiv.org/abs/2212.13591) | 简而言之，该论文讨论了如何使用数据中心的方法解决医学图像诊断中的“小数据”问题，介绍了数据增强、迁移学习、联邦学习和GAN的方法，并提出了使用知识引导的GAN将领域知识纳入训练数据生成过程中。 |
| [^151] | [Dataless Knowledge Fusion by Merging Weights of Language Models.](http://arxiv.org/abs/2212.09849) | 本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。 |
| [^152] | [Further analysis of multilevel Stein variational gradient descent with an application to the Bayesian inference of glacier ice models.](http://arxiv.org/abs/2212.03366) | 本文研究了多层斯坦变分梯度下降在大规模贝叶斯反问题中的应用，实现了数量级的加速。 |
| [^153] | [Edge Impulse: An MLOps Platform for Tiny Machine Learning.](http://arxiv.org/abs/2212.03332) | Edge Impulse是一个面向微型机器学习的MLOps平台，旨在为开发人员提供嵌入式和边缘ML系统的软硬件优化支持，解决TinyML的可移植性和优化问题。 |
| [^154] | [Improving Pareto Front Learning via Multi-Sample Hypernetworks.](http://arxiv.org/abs/2212.01130) | 本文提出了一个新的PFL框架PHN-HVI，利用超网络生成一组多样的解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。 |
| [^155] | [An Interpretable Hybrid Predictive Model of COVID-19 Cases using Autoregressive Model and LSTM.](http://arxiv.org/abs/2211.17014) | 本文提出了一种可解释的混合预测模型，该模型使用自回归模型和LSTM预测COVID-19病例，结合了两种模型的优势，通过数据自适应性决定模型块的相对贡献，在全面数值研究中展示了优异的性能。 |
| [^156] | [Game Theoretic Mixed Experts for Combinational Adversarial Machine Learning.](http://arxiv.org/abs/2211.14669) | 本文提出了一种博弈论框架，用于组合对抗攻击和防御，我们的框架提出了一种混合专家模型，专门针对特定的防御攻击进行防御，并以博弈论的方式进行合作和竞争，形成一个联合防御。 |
| [^157] | [Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models.](http://arxiv.org/abs/2211.11176) | 本研究提出了一种基于时间依赖图的通用图神经网络结构(GraphS4mer)，用于建立多元生物信号模型。该模型结合了结构化状态空间架构和动态演变的图结构学习层来解决长时序和复杂空间相关性，能有效地提高多元生物信号分类任务性能。 |
| [^158] | [SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering.](http://arxiv.org/abs/2210.15185) | SAM-RL使用不可导物理仿真和渲染，通过比较渲染图像和真实原始图像自动更新模型，并高效产生策略。感知感知的学习管道允许机器人选择信息丰富的视角监控任务过程。 用于完成机器人组装，工具操作和变形物体操作任务。 |
| [^159] | [Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation.](http://arxiv.org/abs/2210.13542) | 本文提出了一种通过 Bellman 固定点方程进行微分的方法，实现了值迭代网络及其变体的前后传递解耦，可实现在规划视程内稳定且灵活前向预算的扩展，展示了在不同规划任务上的出色表现。 |
| [^160] | [Baby Physical Safety Monitoring in Smart Home Using Action Recognition System.](http://arxiv.org/abs/2210.12527) | 本文提出了一种新的轻量级框架，将迁移学习技术与Conv2D LSTM层相结合，用于婴儿物理安全监测的行为识别。 |
| [^161] | [Multi-Parameter Performance Modeling via Tensor Completion.](http://arxiv.org/abs/2210.10184) | 本论文采用低秩张量分解来建模应用程序性能，通过对应用程序执行时间的逼近，实现对未观测区域的精确外推，并应用张量完成算法优化低秩正交-多项式（CP）分解，从而提高了预测准确性和存储效率。 |
| [^162] | [Attribute Inference Attacks in Online Multiplayer Video Games: a Case Study on Dota2.](http://arxiv.org/abs/2210.09028) | 本文是第一篇研究在线游戏属性推断攻击问题的论文，以 Dota2 为例，通过对玩家数据的挖掘，成功地利用机器学习推断出了玩家的真实生活属性。 |
| [^163] | [ToupleGDD: A Fine-Designed Solution of Influence Maximization by Deep Reinforcement Learning.](http://arxiv.org/abs/2210.07500) | 提出了一个新颖的端到端DRL框架ToupleGDD，用于解决影响力最大化问题，该框架将三个耦合的图神经网络用于网络嵌入，双重深度Q网络用于参数学习，通过统一的损失函数将它们集成起来，捕获网络中节点和边缘的更丰富和多样化的信息，实验结果表明ToupleGDD具有最先进的性能和泛化能力。 |
| [^164] | [MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers.](http://arxiv.org/abs/2210.06425) | MiniALBERT 是一种模型蒸馏技术，结合了跨层参数共享等策略，将完全参数化的语言模型知识转换成为紧凑递归学生模型。MiniALBERT 在基准 NLP 任务上的实验表明，它在性能上优于多个最先进的紧凑型语言模型，并且具有更少的参数数量。 |
| [^165] | [Occlusion-Aware Crowd Navigation Using People as Sensors.](http://arxiv.org/abs/2210.00552) | 利用深度强化学习和社交推断技术，该研究提出了一种解决拥挤环境下遮挡问题的方法，成功将模拟中的策略迁移到现实中的Turtlebot机器人上。 |
| [^166] | [Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the Monocular RGB-D input.](http://arxiv.org/abs/2209.08752) | 本文提出了一种基于关键点的单目RGB-D输入下的六自由度抓取点生成方法，该方法可以通过PnP算法恢复SE(3)姿态。机器人实验表明，我们的方法在抓取点提案的精度、多样性和时间成本方面优于其他方法并具有很强的应用潜力。 |
| [^167] | [Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks.](http://arxiv.org/abs/2209.06589) | 本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。 |
| [^168] | [Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiers.](http://arxiv.org/abs/2209.06408) | 本文提出了一种名为“Meta Pattern Concern Score”的新型评估指标，它基于概率预测的抽象表征和可调节的阈值，将人类价值观引入到多分类器中，可以用于根据人类价值观恰当地评估黑盒模型在现实中的应用效果，并且可以比较在具有不同人类价值观下使用不同分类器的同一数据集。 |
| [^169] | [Towards Improving Operation Economics: A Bilevel MIP-Based Closed-Loop Predict-and-Optimize Framework for Prescribing Unit Commitment.](http://arxiv.org/abs/2208.13065) | 本文提出了一个基于双层 MIP 的闭环预测优化框架，使用成本导向的预测器来改进电力系统的经济运行。该框架通过反馈循环迭代地改进预测器，实现了对机组组合的最佳操作。 |
| [^170] | [Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning.](http://arxiv.org/abs/2208.09894) | 本文研究了中心化剪裁在面对不同恶意代理时的脆弱性，提出了一种称为多引用点剪裁 (MRPC) 的算法来解决这个问题。MRPC 框架利用多个参考点有效地中和专门设计的 Byzantine attacks。实验结果表明，在各种类型的 Byzantine attacks 下，MRPC 显著优于最先进的 FL 方法。 |
| [^171] | [GSim: A Graph Neural Network based Relevance Measure for Heterogeneous Graphs.](http://arxiv.org/abs/2208.06144) | GSim是一种基于图神经网络的异构图关联度量方法，不需要预定义的元路径，能够捕捉异构图的隐含结构，已在多个数据集上得到验证。 |
| [^172] | [Conformal Risk Control.](http://arxiv.org/abs/2208.02814) | 该论文提出了一种符合保序的风险控制方法，可以控制任何单调损失函数的期望值，示例证明其在计算机视觉和自然语言处理领域具有控制误报率、图形距离和令牌级F1得分的能力。 |
| [^173] | [Wasserstein Graph Distance Based on $L_1$-Approximated Tree Edit Distance between Weisfeiler-Lehman Subtrees.](http://arxiv.org/abs/2207.04216) | 本文提出一种名为Wasserstein WL子树(WWLS)距离的新型图距离，通过利用WL子树作为节点邻域的结构信息，使用节点的WL子树之间的L1-近似树编辑距离(L1-TED)定义节点度量，解决了WL测试无法捕捉轻微结构差异的问题 |
| [^174] | [Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs.](http://arxiv.org/abs/2207.02295) | 本文在NVIDIA网卡中实现了强化学习数据中心拥塞控制，通过将RL-CC的复杂神经网络转化为决策树，实现了实时推理，并成功改善了网络拥塞下的尾部延迟和数据包丢失问题。 |
| [^175] | [Conditionally Elicitable Dynamic Risk Measures for Deep Reinforcement Learning.](http://arxiv.org/abs/2206.14666) | 本文提出一种可解决风险敏感型强化学习问题的新框架，使用动态谱风险度量进行优化，设计了一种可以逼近这些度量的深度神经网络算法，并开发了一种不需要额外嵌套转换的风险敏感演员-评论家算法。 |
| [^176] | [RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network.](http://arxiv.org/abs/2206.14098) | 本文提出了RevSilo，一个完全可逆的双向多尺度特征融合模块，它缓解了神经网络规模受限的问题。 |
| [^177] | [Offline RL for Natural Language Generation with Implicit Language Q Learning.](http://arxiv.org/abs/2206.11871) | 本研究提出了一种新颖的自然语言生成离线强化学习方法ILQL，通过在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，引导语言模型生成最大化用户指定的效用函数的语言输出，从而解决巨型语言模型完成用户指定任务时存在的不一致性问题。 |
| [^178] | [Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training.](http://arxiv.org/abs/2206.11215) | 本文提出了一个通用的认证理论，使得估计值和真实值之间的距离达到了约束，提出了一个基于语义关键点的姿态估计模型 C-3PO，并加入了可观察正确性和非退化性两个证书，该模型在合成和实际测量数据集方面取得了最先进的性能，同时提出了一个自我训练的方法以进一步提高模型性能。 |
| [^179] | [Learning Debiased Classifier with Biased Committee.](http://arxiv.org/abs/2206.10843) | 该论文探讨了一种新方法，使用偏差委员会来训练没有偏见属性标签的去偏分类器，解决神经网络在训练数据中的偏见问题。 |
| [^180] | [It's a super deal -- train recurrent network on noisy data and get smooth prediction free.](http://arxiv.org/abs/2206.04215) | 本研究探究了使用嘈杂数据训练递归神经网络进行时间序列预测的效果，并发现该方法能够实现免费平滑预测。在实现方法的基础上，我们还通过解释为何存在噪声压缩，探讨了递归网络在神经科学中的重要性。 |
| [^181] | [Integrating Symmetry into Differentiable Planning with Steerable Convolutions.](http://arxiv.org/abs/2206.03674) | 本文研究了如何在路径规划任务中使用对称性改善数据效率和泛化能力。将值迭代视为网格信号，并使用可控卷积来融合对称性。实验表明，我们的对称规划算法比非等变对应物在训练效率和泛化方面有很大提升。 |
| [^182] | [Improving adversarial robustness by putting more regularizations on less robust samples.](http://arxiv.org/abs/2206.03353) | 本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。 |
| [^183] | [Joint Energy Dispatch and Unit Commitment in Microgrids Based on Deep Reinforcement Learning.](http://arxiv.org/abs/2206.01663) | 本文采用深度强化学习算法HAFH-DDPG来学习隔离式微电网中的联合能量分配和机组开启决策问题，并提出了柴油发电机选择策略，以降低计算复杂度。 |
| [^184] | [Strategic Classification with Graph Neural Networks.](http://arxiv.org/abs/2205.15765) | 本文介绍了一种基于图神经网络的战略分类方法，利用社交关系来提高预测，并提出一种可微分的框架，用于学习基于图的分类器的战略鲁棒性，实验表明该方法在几个真实的网络数据集上十分实用。 |
| [^185] | [Lifelong Ensemble Learning based on Multiple Representations for Few-Shot Object Recognition.](http://arxiv.org/abs/2205.01982) | 本文提出了一种基于多重表示的终身集成学习方法，用于开放式场景下的少样本目标识别问题，适用于三维物体类别数量不固定、随时间增长的场景。模型针对各种类型物体进行处理，并进行了广泛实验。 |
| [^186] | [MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images.](http://arxiv.org/abs/2203.12362) | MONAI Label是一个开源的AI框架，支持快速标注医学图像。它提供了多种前端选项和两种主动学习策略，可以帮助研究人员加速分割算法的训练，进而提高模型性能。 |
| [^187] | [CMW-Net: Learning a Class-Aware Sample Weighting Mapping for Robust Deep Learning.](http://arxiv.org/abs/2202.05613) | 提出了一种从数据中自适应地学习显式的加权方案的元模型，用于缓解深度神经网络在具有数据偏见问题时的过度拟合问题。 |
| [^188] | [Complex-to-Real Sketches for Tensor Products with Applications to the Polynomial Kernel.](http://arxiv.org/abs/2202.02031) | 本论文提出一种Complex-to-Real草图方法，用于处理复合实张量乘积，取得了在多项式核上最先进的准确性和速度表现。 |
| [^189] | [A Machine Learning Framework for Distributed Functional Compression over Wireless Channels in IoT.](http://arxiv.org/abs/2201.09483) | 本论文开发了一种面向物联网的分布式功能压缩机器学习框架，采用了能够任意计算IoT所需函数压缩任务的Kolmogorov-Arnold表示定理，解决了基于云的方法在传输数据时给网络资源带来的压力问题。 |
| [^190] | [Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization.](http://arxiv.org/abs/2112.14368) | 本研究提出一种面向在线凸优化的动态遗憾算法，可以在一些简单的问题实例中进一步增强保证，具有几何直观性，实验表明其优于最先进的基线算法。 |
| [^191] | [A Sparse Expansion For Deep Gaussian Processes.](http://arxiv.org/abs/2112.05888) | 本文提出了一种基于高斯过程的稀疏展开方法，用于构建深度高斯过程模型。该方法可以提高计算效率，使得模型更加稀疏。 |
| [^192] | [Generate plane quad mesh with neural networks and tree search.](http://arxiv.org/abs/2111.07613) | 本论文提出了一种结合强化学习和树搜索的新方法，名为TreeMesh，用于生成高质量的平面四边形网格，可以比现有最先进的方法更快地生成。 |
| [^193] | [Robustness of Graph Neural Networks at Scale.](http://arxiv.org/abs/2110.14038) | 本文研究了规模下如何攻击和防御图神经网络（GNNs），提出了稀疏感知的一阶优化攻击和鲁棒性聚合函数Soft Median，有效提高了GNNs的可靠性和攻击力。 |
| [^194] | [Byzantine-robust Federated Learning through Collaborative Malicious Gradient Filtering.](http://arxiv.org/abs/2109.05872) | 本文提出了一种基于逐元素符号的方法，用于检测模型污染攻击，以解决联邦学习中拜占庭攻击的问题。 |
| [^195] | [SelfCF: A Simple Framework for Self-supervised Collaborative Filtering.](http://arxiv.org/abs/2107.03019) | SelfCF是一种自监督协同过滤框架，用于推荐场景，通过增强现有的深度学习协同过滤模型中输出的嵌入来简化算法以及避免昂贵的计算和潜在的负样本问题。 |
| [^196] | [Redundant representations help generalization in wide neural networks.](http://arxiv.org/abs/2106.03485) | 本文研究了各种卷积神经网络的最后一个隐藏层中的表示，发现如果最后一个隐藏表示足够宽，则其神经元倾向于分成携带相同信息的组，而冗余表示有助于宽神经网络的泛化。 |
| [^197] | [Diffusion Mechanism in Residual Neural Network: Theory and Applications.](http://arxiv.org/abs/2105.03155) | 本文提出了一种扩散残差网络，该网络在神经网络架构中内部引入了扩散机制，能够提高数据点之间的距离直径比，从而提高了类间点的可分性，减少了类内点之间的距离。 |
| [^198] | [Graph Representation Learning via Diversity-preserving Graph Refinement.](http://arxiv.org/abs/2103.07295) | 该论文提出了一种基于多样性保持的图结构细化的图表示学习方法，它利用已学习的节点表示来逐步改善图形结构质量。在多个下游任务上，包括节点分类、链接预测和图聚类，实验表明该方法优于现有的最先进方法。 |
| [^199] | [Doubly robust Thompson sampling for linear payoffs.](http://arxiv.org/abs/2102.01229) | 本文提出了一种新型多臂上下文赌博算法双重稳健汤普森抽样（DR Thompson Sampling），通过双重稳健估计器，解决了过去的上下文和奖励对选择依赖性导致的损失分解复杂等问题，得到了简化且改进的损失界限。 |
| [^200] | [Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data.](http://arxiv.org/abs/2012.02334) | 本文调查了10个最近提出的能量守恒神经网络模型，并比较了它们在4个物理系统中的表现，说明了利用这些模型设计基于能量的控制器的可能性。 |
| [^201] | [A supervised active learning method for identifying critical nodes in Wireless Sensor Network.](http://arxiv.org/abs/2004.08885) | 本论文提出了一种有监督主动学习方法，通过聚类和分类模块的迭代协作在较少的数据量下准确识别出对WSN特征影响更大的关键节点，克服了现有方法中针对非关键节点的识别偏差，并可应用于大规模WSN中。 |
| [^202] | [Best Principal Submatrix Selection for the Maximum Entropy Sampling Problem: Scalable Algorithms and Performance Guarantees.](http://arxiv.org/abs/2001.08537) | 本文提出了一个新的整数规划方法和连续松弛算法来解决最大熵采样问题，并提供了效率更高的确定性采样算法。同时，我们改进了已有的近似界限，并证明了局部搜索算法的第一个近似界限。 |
| [^203] | [IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters.](http://arxiv.org/abs/1903.12141) | 本文提出IMAE模型用于畸形训练数据的鲁棒深度学习，通过实践证实平均绝对误差（MAE）在处理示例时存在欠拟合问题，利用加权方差调整提高了拟合能力，同时保持了鲁棒性。 |

# 详细

[^1]: RViDeformer：具有更大基准数据集的高效原始视频去噪变换器

    RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset. (arXiv:2305.00767v1 [cs.CV])

    [http://arxiv.org/abs/2305.00767](http://arxiv.org/abs/2305.00767)

    本文提出了RViDeformer原始视频去噪变换器及其配套数据集ReCRVD，其中利用高低ISO设置重新捕捉现有视频以构建噪声-清晰对，同时探索了非本地时空依赖关系的解决方案。

    

    近年来，由于与成像过程的一致性和原始领域中成熟的噪声建模，原始视频去噪引起了越来越多的关注。然而，仍存在两个问题阻碍了去噪性能。首先，对于受控的原始视频去噪来说，没有具有真实运动的大型数据集，因为为真实动态场景捕捉噪声和清晰帧是困难的。为了解决这个问题，我们提出重新捕捉以高低ISO设置显示的现有高分辨率视频以构建噪声-清晰配对帧。这样，我们构建一个视频去噪数据集（命名为ReCRVD），其中包括120组噪声-清晰视频，其ISO值从1600到25600不等。其次，虽然非本地时空关注对于去噪有益，但它通常导致沉重的计算成本。为此，我们提出了一种高效的原始视频去噪变换器网络（RViDeformer），它探索了短距离和长距离相关性。具体而言，我们提出了一个空间变换器，用于处理本地视觉特征以减少空间冗余并加速计算。此外，为了解决长程噪声相关性，我们采用了一种新的时间变换器网络，同时模型化非本地时空依赖关系。

    In recent years, raw video denoising has garnered increased attention due to the consistency with the imaging process and well-studied noise modeling in the raw domain. However, two problems still hinder the denoising performance. Firstly, there is no large dataset with realistic motions for supervised raw video denoising, as capturing noisy and clean frames for real dynamic scenes is difficult. To address this, we propose recapturing existing high-resolution videos displayed on a 4K screen with high-low ISO settings to construct noisy-clean paired frames. In this way, we construct a video denoising dataset (named as ReCRVD) with 120 groups of noisy-clean videos, whose ISO values ranging from 1600 to 25600. Secondly, while non-local temporal-spatial attention is beneficial for denoising, it often leads to heavy computation costs. We propose an efficient raw video denoising transformer network (RViDeformer) that explores both short and long-distance correlations. Specifically, we propos
    
[^2]: 真实数据上的无监督异常检测算法：我们需要多少款算法？

    Unsupervised anomaly detection algorithms on real-world data: how many do we need?. (arXiv:2305.00735v1 [cs.LG])

    [http://arxiv.org/abs/2305.00735](http://arxiv.org/abs/2305.00735)

    对52个真实多元表格数据集上的32个无监督异常检测算法进行了评估，结果表明第kNN算法在本地数据集上优于其他算法，而EIF算法在全球数据集上表现最佳，建议实际使用这三款无监督异常检测算法的工具箱。

    

    本研究对52个真实多元表格数据集上的32个无监督异常检测算法进行了评估，是目前最大的无监督异常检测算法的比较研究。在这些数据集上，第kNN（到k个最近邻居的距离）算法的性能明显优于其他算法。通过可视化和聚类算法在所有数据集上的性能，我们发现了两个明显的簇：一个是“本地”数据集，另一个是“全局”数据集。在本地数据集中，kNN算法表现最佳，在全局数据集中，EIF算法表现最佳。综合考虑算法的计算复杂性，建议实际使用这三种无监督异常检测算法（分别是kNN、EIF和LOF算法）的工具箱。

    In this study we evaluate 32 unsupervised anomaly detection algorithms on 52 real-world multivariate tabular datasets, performing the largest comparison of unsupervised anomaly detection algorithms to date. On this collection of datasets, the $k$-thNN (distance to the $k$-nearest neighbor) algorithm significantly outperforms the most other algorithms. Visualizing and then clustering the relative performance of the considered algorithms on all datasets, we identify two clear clusters: one with ``local'' datasets, and another with ``global'' datasets. ``Local'' anomalies occupy a region with low density when compared to nearby samples, while ``global'' occupy an overall low density region in the feature space. On the local datasets the $k$NN ($k$-nearest neighbor) algorithm comes out on top. On the global datasets, the EIF (extended isolation forest) algorithm performs the best. Also taking into consideration the algorithms' computational complexity, a toolbox with these three unsupervis
    
[^3]: 自监督视觉变压器学习什么？

    What Do Self-Supervised Vision Transformers Learn?. (arXiv:2305.00729v1 [cs.CV])

    [http://arxiv.org/abs/2305.00729](http://arxiv.org/abs/2305.00729)

    本文比较了对比学习和遮蔽图像建模在表示和 下游任务表现方面的差异。实验证明，自监督视觉变压器利用对比学习时能够捕捉更长程的全局模式并线性分离图像，但在自我关注力的同质性、可扩展性和密集预测性能方面存在一些问题。

    

    本文对比了对比学习（CL）和遮蔽图像建模（MIM）在其表示和下游任务表现方面的差异，并阐述了自监督视觉变压器（ViTs）的性质。通过实验证明，CL训练自我关注力以捕捉比MIM更长程的全局模式，例如物体的形状，尤其是在ViT架构的后几层中。这使得ViTs能够在其表示空间中线性分离图像。但是，它也使得自我关注力对于所有查询标记和头部的同质性崩溃。这种自我关注力的同质性降低了表征的多样性，恶化了可扩展性和密集预测性能　。CL利用表示的低频信号，而MIM利用高频信号。由于低频和高频信息分别代表形状和质地，因此CL更加注重形状，而MIM则更加注重质地。

    We present a comparative study on how and why contrastive learning (CL) and masked image modeling (MIM) differ in their representations and in their performance of downstream tasks. In particular, we demonstrate that self-supervised Vision Transformers (ViTs) have the following properties: (1) CL trains self-attentions to capture longer-range global patterns than MIM, such as the shape of an object, especially in the later layers of the ViT architecture. This CL property helps ViTs linearly separate images in their representation spaces. However, it also makes the self-attentions collapse into homogeneity for all query tokens and heads. Such homogeneity of self-attention reduces the diversity of representations, worsening scalability and dense prediction performance. (2) CL utilizes the low-frequency signals of the representations, but MIM utilizes high-frequencies. Since low- and high-frequency information respectively represent shapes and textures, CL is more shape-oriented and MIM m
    
[^4]: 基于像素数据的预测: PDE和有限差分的深入探索

    Predictions Based on Pixel Data: Insights from PDEs and Finite Differences. (arXiv:2305.00723v1 [math.NA])

    [http://arxiv.org/abs/2305.00723](http://arxiv.org/abs/2305.00723)

    本文介绍了基于像素数据的预测，通过对离散卷积和有限差分算子之间联系的利用，证明了逼近自偏微分方程空时离散出的序列可以使用相对较小的卷积(残差)网络进行。

    

    神经网络是高维空间中许多逼近任务的最先进技术，这得到了大量实验证据的支持。然而，我们仍需要对它们可以逼近的内容以及以何种代价和精度逼近有一个坚实的理论理解。其中一个在涉及图像的逼近任务中有实际用途的网络体系结构是卷积(残差)网络。然而，由于这些网络中涉及的线性算子的局部性质，它们的分析比通用全连接神经网络更为复杂。本文重点介绍的是序列逼近任务，其中每个观察值由矩阵或高阶张量表示。我们证明，当逼近自偏微分方程空时离散出的序列时，可以使用相对较小的网络。我们通过利用离散卷积和有限差分算子之间的联系来构造这些结果。在整个过程中，我们设计了我们的网络。

    Neural networks are the state-of-the-art for many approximation tasks in high-dimensional spaces, as supported by an abundance of experimental evidence. However, we still need a solid theoretical understanding of what they can approximate and, more importantly, at what cost and accuracy. One network architecture of practical use, especially for approximation tasks involving images, is convolutional (residual) networks. However, due to the locality of the linear operators involved in these networks, their analysis is more complicated than for generic fully connected neural networks. This paper focuses on sequence approximation tasks, where a matrix or a higher-order tensor represents each observation. We show that when approximating sequences arising from space-time discretisations of PDEs we may use relatively small networks. We constructively derive these results by exploiting connections between discrete convolution and finite difference operators. Throughout, we design our network a
    
[^5]: 可持续发展绿色数据中心的全面自动化扩展机制

    Full Scaling Automation for Sustainable Development of Green Data Centers. (arXiv:2305.00706v1 [cs.DC])

    [http://arxiv.org/abs/2305.00706](http://arxiv.org/abs/2305.00706)

    提出了一种全面自动化扩展（FSA）机制来改善数据中心的能源利用效率，该机制利用深度表征学习来预测每个服务的未来负载并自动稳定相应的目标CPU使用率水平。

    

    云计算的快速崛起导致数据中心碳排放量惊人地增加，现在占全球温室气体排放的>3％，必须立即采取措施应对它们对全球气候日益增长的负担。这一努力的重点是提高资源利用率以节省电力消耗。我们提出的全面自动化扩展（FSA）机制是一种有效的方法，可以在大规模云计算集群中动态地适应不断变化的工作负载，使数据中心中的集群保持其所需的CPU利用率目标，从而改善能源效率。FSA利用深度表征学习的威力来准确预测每个服务的未来工作负载，并自动稳定相应的目标CPU使用率水平，不像之前的自动扩展方法，如Autopilot或FIRM，需要使用统计模型和专家知识来调整计算资源。

    The rapid rise in cloud computing has resulted in an alarming increase in data centers' carbon emissions, which now accounts for >3% of global greenhouse gas emissions, necessitating immediate steps to combat their mounting strain on the global climate. An important focus of this effort is to improve resource utilization in order to save electricity usage. Our proposed Full Scaling Automation (FSA) mechanism is an effective method of dynamically adapting resources to accommodate changing workloads in large-scale cloud computing clusters, enabling the clusters in data centers to maintain their desired CPU utilization target and thus improve energy efficiency. FSA harnesses the power of deep representation learning to accurately predict the future workload of each service and automatically stabilize the corresponding target CPU usage level, unlike the previous autoscaling methods, such as Autopilot or FIRM, that need to adjust computing resources with statistical models and expert knowle
    
[^6]: 关于多智能体决策制定的复杂性：从游戏学习到局部监控。

    On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring. (arXiv:2305.00684v1 [cs.LG])

    [http://arxiv.org/abs/2305.00684](http://arxiv.org/abs/2305.00684)

    本文研究了多智能体决策制定的样本有效、均衡计算和局部监控问题，提出了复杂度上下界和算法，并发现多智能体情况下可能呈指数级难度。

    

    多智能体强化学习（MARL）中的一个核心问题是理解结构条件和算法原则会导致哪些样本有效的学习保证，并且在我们从少数智能体转移到多数智能体时，这些考虑如何发生变化。本文在多智能体互动决策的一般框架下研究了这个问题，包括具有函数逼近的马尔可夫博弈和带有赌徒反馈的正则式博弈。我们关注均衡计算，其中集中式学习算法旨在通过控制与未知环境交互的多个智能体来计算均衡。我们的主要贡献是：1. 我们基于由Foster等人（2021）在单智能体情况下引入的复杂度度量方法—决策-估计系数，为多智能体决策制定了最佳样本复杂度的上界和下界。与单智能体情况下的最佳结果相比，我们表明多智能体情况下的问题在智能体数量方面可能呈指数级难度。2. 我们提出了一种新颖的算法，用于在具有函数逼近的大型马尔可夫博弈中进行高效的均衡计算，该算法基于乐观镜像下降法的原理。我们为我们的方法建立了样本复杂度界限，这些界限改进了先前在带有赌徒反馈的游戏中的工作。3. 我们考虑局部监控，这是一种反馈类型，其中决策制定者只观察智能体动作的摘要信息而不是全部信息。我们开发了我们算法的一个变体，该算法实现了此设置的收敛速度最优，与先前工作建立的下界相比。

    A central problem in the theory of multi-agent reinforcement learning (MARL) is to understand what structural conditions and algorithmic principles lead to sample-efficient learning guarantees, and how these considerations change as we move from few to many agents. We study this question in a general framework for interactive decision making with multiple agents, encompassing Markov games with function approximation and normal-form games with bandit feedback. We focus on equilibrium computation, in which a centralized learning algorithm aims to compute an equilibrium by controlling multiple agents that interact with an unknown environment. Our main contributions are:  - We provide upper and lower bounds on the optimal sample complexity for multi-agent decision making based on a multi-agent generalization of the Decision-Estimation Coefficient, a complexity measure introduced by Foster et al. (2021) in the single-agent counterpart to our setting. Compared to the best results for the sin
    
[^7]: 用于内存成本的在线优化的鲁棒学习

    Robustified Learning for Online Optimization with Memory Costs. (arXiv:2305.00677v1 [cs.LG])

    [http://arxiv.org/abs/2305.00677](http://arxiv.org/abs/2305.00677)

    本文提出了一种新颖的专家鲁棒学习（ERL）方法，在内存成本的在线优化中实现了良好的平均性能和鲁棒性。

    

    在线优化的内存成本有许多实际应用，其中顺序操作在不知道未来输入的情况下进行。然而，内存成本将随时间耦合的操作添加了重大挑战。传统上，通过各种专家设计的在线算法来解决这个问题，目标是实现有界的最坏情况竞争比，但结果的平均性能通常不令人满意。另一方面，新兴的基于机器学习（ML）的优化器可以提高平均性能，但缺乏最坏情况性能鲁棒性。在本文中，我们提出了一种新颖的专家鲁棒学习（ERL）方法，既实现了良好的平均性能又具有鲁棒性。更具体地，为了实现鲁棒性，ERL引入了一种新颖的投影算子，通过利用专家在线算法来使ML操作具有鲁棒性；为了实现平均性能，ERL使用基于递归结构的ML优化器进行训练，明确

    Online optimization with memory costs has many real-world applications, where sequential actions are made without knowing the future input. Nonetheless, the memory cost couples the actions over time, adding substantial challenges. Conventionally, this problem has been approached by various expert-designed online algorithms with the goal of achieving bounded worst-case competitive ratios, but the resulting average performance is often unsatisfactory. On the other hand, emerging machine learning (ML) based optimizers can improve the average performance, but suffer from the lack of worst-case performance robustness. In this paper, we propose a novel expert-robustified learning (ERL) approach, achieving {both} good average performance and robustness. More concretely, for robustness, ERL introduces a novel projection operator that robustifies ML actions by utilizing an expert online algorithm; for average performance, ERL trains the ML optimizer based on a recurrent architecture by explicit
    
[^8]: 跨图动态迁移学习

    Dynamic Transfer Learning across Graphs. (arXiv:2305.00664v1 [cs.LG])

    [http://arxiv.org/abs/2305.00664](http://arxiv.org/abs/2305.00664)

    该论文提出了一个新的问题：在动态图形环境下如何有效地进行跨图迁移学习，主要解决了领域演化对泛化性能的影响。

    

    在许多高风险领域中，跨图传输知识起着关键作用，包括运输网络、电子商务网络、神经科学和金融领域。我们提出了一个新问题：在动态设置下，考虑已观察到的具有标签的源图和标签稀疏的目标图，如何有效地表征不断变化的领域偏差，并优化目标域在下一个时间戳的泛化性能？为了回答这个问题，我们首次提出了跨图动态迁移学习设置下的一般化界限，这意味着泛化性能由领域演化控制。

    Transferring knowledge across graphs plays a pivotal role in many high-stake domains, ranging from transportation networks to e-commerce networks, from neuroscience to finance. To date, the vast majority of existing works assume both source and target domains are sampled from a universal and stationary distribution. However, many real-world systems are intrinsically dynamic, where the underlying domains are evolving over time. To bridge the gap, we propose to shift the problem to the dynamic setting and ask: given the label-rich source graphs and the label-scarce target graphs observed in previous T timestamps, how can we effectively characterize the evolving domain discrepancy and optimize the generalization performance of the target domain at the incoming T+1 timestamp? To answer the question, for the first time, we propose a generalization bound under the setting of dynamic transfer learning across graphs, which implies the generalization performance is dominated by domain evolution
    
[^9]: 激活函数不再激活：神经网络解释的合理理论

    Activation Functions Not To Active: A Plausible Theory on Interpreting Neural Networks. (arXiv:2305.00663v1 [cs.LG])

    [http://arxiv.org/abs/2305.00663](http://arxiv.org/abs/2305.00663)

    本文提供了一个合理的理论来解释神经网络的高维空间，并且将激活函数的角色描述为放大函数，将低维线性空间映射为无限维超级空间。

    

    研究人员普遍认为神经网络模拟一个高维空间，但却不能清晰地定义这个空间。那么这个空间是什么？它的维数是多少？是否具有有限的维数？本文开发了一个关于激活函数在神经网络中作用的可行理论，定义了一个高维（更精确地，是一个无限维）的空间。我们认为激活函数充当了一个放大函数的作用，将低维线性空间映射成了一个无限维的空间。

    Researchers commonly believe that neural networks model a high-dimensional space but cannot give a clear definition of this space. What is this space? What is its dimension? And does it has finite dimensions? In this paper, we develop a plausible theory on interpreting neural networks in terms of the role of activation functions in neural networks and define a high-dimensional (more precisely, an infinite-dimensional) space. We conjunction that the activation function acts as a magnifying function that maps the low-dimensional linear space into an infinite-dimensional space. Given a dataset with each example of $d$ features $f_1$, $f_2$, $\cdots$, $f_d$, we believe that NNs model a special space with infinite dimensions, each of which is a monomial $$\prod_{i_1, i_2, \cdots, i_d} f_1^{i_1} f_2^{i_2} \cdots f_d^{i_d}$$ for some non-negative integers ${i_1, i_2, \cdots, i_d} \in \mathbb{Z}_{0}^{+}=\{0,1,2,3,\ldots\} $. We term such an infinite-dimensional space $\textit{ Super Space (SS)
    
[^10]: 使用奇异值分解的深度强化学习中的表征学习和探索

    Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition. (arXiv:2305.00654v1 [cs.LG])

    [http://arxiv.org/abs/2305.00654](http://arxiv.org/abs/2305.00654)

    本文提出了一种基于奇异值分解的自动表征学习模型，可以获得保留转换结构的表示形式并捕捉状态访问的相对频率。该方法不需要转移矩阵，可以利用深度网络，适用于部分可观察领域，并且在多任务设置中表现良好。

    

    表现学习和探索是任何深度强化学习代理所面临的关键挑战。本文提供了一种基于奇异值分解的方法，可以用来获得保留域中潜在转换结构的表示形式。有趣的是，我们发现这些表示形式还捕捉了状态访问的相对频率，从而免费提供了伪计数的估计。为了将这种分解方法推广到大规模域，我们提供了一种不需要建立转移矩阵，可以利用深度网络，也允许小批量训练的算法。此外，我们从预测状态表示中吸取灵感，并扩展了我们的分解方法到部分可观察的环境。通过对部分可观察领域的多任务设置进行实验，我们展示了提出的方法不仅可以在DM-Lab-30环境中学习有用的表示形式。

    Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on DM-Lab-30 environments (that have inputs
    
[^11]: 发现并校正：概念感知的假相关抑制方法。

    Discover and Cure: Concept-aware Mitigation of Spurious Correlation. (arXiv:2305.00650v1 [cs.LG])

    [http://arxiv.org/abs/2305.00650](http://arxiv.org/abs/2305.00650)

    本研究提出了一个可解释的方法框架(DISC)来抑制深度神经网络中的假相关，通过发现不稳定的概念并将其作为假属性干预训练数据来提高模型的泛化能力和可解释性。在目标识别任务中，DISC胜过了现有最先进的方法。

    

    深度神经网络经常依赖于假相关来进行预测，这会导致无法超越训练环境的一般化。例如，将猫与床作为背景联系的模型，在没有床的其他环境中可能无法预测到猫的存在。抑制假相关对于构建可信赖的模型至关重要。然而，现有的方法缺乏透明度提供有关抑制过程的洞察。本研究提出了一个可解释的框架Discover and Cure (DISC)来解决这个问题。使用人类可解释的概念，DISC迭代地 1)发现不稳定的概念，将其作为假属性在不同环境下，然后 2)使用发现的概念干预训练数据以减少假相关。在系统实验中，DISC提供了比现有方法更优秀的泛化能力和可解释性。具体地，在目标识别任务中，它胜过了现有的最先进方法。

    Deep neural networks often rely on spurious correlations to make predictions, which hinders generalization beyond training environments. For instance, models that associate cats with bed backgrounds can fail to predict the existence of cats in other environments without beds. Mitigating spurious correlations is crucial in building trustworthy models. However, the existing works lack transparency to offer insights into the mitigation process. In this work, we propose an interpretable framework, Discover and Cure (DISC), to tackle the issue. With human-interpretable concepts, DISC iteratively 1) discovers unstable concepts across different environments as spurious attributes, then 2) intervenes on the training data using the discovered concepts to reduce spurious correlation. Across systematic experiments, DISC provides superior generalization ability and interpretability than the existing approaches. Specifically, it outperforms the state-of-the-art methods on an object recognition task
    
[^12]: 推算历史洪水: 整合CNN-LSTM深度学习框架，融合卫星数据进行历史淹没地图制作

    Inferring the past: a combined CNN-LSTM deep learning framework to fuse satellites for historical inundation mapping. (arXiv:2305.00640v1 [cs.CV])

    [http://arxiv.org/abs/2305.00640](http://arxiv.org/abs/2305.00640)

    本文提出了一种整合CNN-LSTM深度学习框架的方法，将Sentinel-1卫星数据和MODIS数据融合，以推算孟加拉国历史洪水的部分淹没区域。

    

    利用卫星数据绘制洪水地图对于管理和减轻洪水风险至关重要。卫星图像能够快速准确地分析大面积区域，为应急响应和灾害管理提供重要信息。从卫星图像中获取的历史洪水数据可以为长期规划、风险管理策略和与保险相关的决策提供信息。Sentinel-1卫星对于洪水监测非常有效，但对于较长的时间序列，可以将其他卫星如MODIS与深度学习模型相结合，准确识别和绘制过去的洪水事件。我们将开发一个整合了CNN-LSTM深度学习框架的模型来将Sentinel-1产生的部分被淹没区域与MODIS数据进行融合，以便在孟加拉国推算历史洪水。结果显示，我们的模型优于仅使用CNN的方法，并利用时空信息来预测部分淹没区域。该模型应用于历史MODIS数据。

    Mapping floods using satellite data is crucial for managing and mitigating flood risks. Satellite imagery enables rapid and accurate analysis of large areas, providing critical information for emergency response and disaster management. Historical flood data derived from satellite imagery can inform long-term planning, risk management strategies, and insurance-related decisions. The Sentinel-1 satellite is effective for flood detection, but for longer time series, other satellites such as MODIS can be used in combination with deep learning models to accurately identify and map past flood events. We here develop a combined CNN--LSTM deep learning framework to fuse Sentinel-1 derived fractional flooded area with MODIS data in order to infer historical floods over Bangladesh. The results show how our framework outperforms a CNN-only approach and takes advantage of not only space, but also time in order to predict the fractional inundated area. The model is applied to historical MODIS data
    
[^13]: 分解增强推理的自我评估引导解码

    Decomposition Enhances Reasoning via Self-Evaluation Guided Decoding. (arXiv:2305.00633v1 [cs.CL])

    [http://arxiv.org/abs/2305.00633](http://arxiv.org/abs/2305.00633)

    本论文提出了一种通过自我评估引导解码提高推理的方法，使用经过校准的自动标准探索推理搜索空间，使搜索能够产生更高质量的最终预测结果；使用自我评估引导的随机束搜索在产生推理链的质量和多样性之间平衡权衡，适应多数投票，并且可以准确判断逻辑错误，提高一致性和鲁棒性。

    

    我们提出了一种有效的提示方法，通过随机束搜索结合自我评估引导。我们的方法使用经过校准的自动标准探索推理搜索空间。这使得有效搜索能够产生更高质量的最终预测结果。使用自我评估引导的随机束搜索，我们在产生推理链的质量和多样性之间平衡权衡，从而能够适应多数投票，并在GSM8K、AQUA和StrategyQA基准测试中以少量示例准确性分别超越对应的Codex-backboned基线$6.34\%$、$9.56\%$和$5.46\%$。对我们的分解式推理分析发现，它可以指出逻辑错误并导致更高的一致性和鲁棒性。

    We propose an effective prompting approach that integrates self-evaluation guidance through stochastic beam search. Our approach explores the reasoning search space using a well-calibrated automatic criterion. This enables an efficient search to produce higher-quality final predictions. With the self-evaluation guided stochastic beam search, we also balance the quality--diversity trade-off in the generation of reasoning chains. This allows our approach to adapt well with majority voting and surpass the corresponding Codex-backboned baselines by $6.34\%$, $9.56\%$, and $5.46\%$ on the GSM8K, AQUA, and StrategyQA benchmarks, respectively, in few-shot accuracy. Analysis of our decompositional reasoning finds it pinpoints logic failures and leads to higher consistency and robustness.
    
[^14]: 时间序列应用中的扩散模型：综述

    Diffusion Models for Time Series Applications: A Survey. (arXiv:2305.00624v1 [cs.LG])

    [http://arxiv.org/abs/2305.00624](http://arxiv.org/abs/2305.00624)

    扩散模型作为基于深度学习的一类生成模型，在时间序列预测、插值和生成方面的应用得到了广泛的探讨和发展。该综述对于新研究人员提供了基础的入门资源，并鼓励未来对于该领域的进一步探索。

    

    扩散模型是基于深度学习的一类生成模型，在前沿机器学习研究中越来越受到关注。它们具有出色的样本生成性能，在图像、视频和文本合成方面得到广泛应用。近年来，扩散的概念已经扩展到时间序列应用，并开发出了许多强大的模型。考虑到这些模型缺乏系统性的综述和讨论，我们提供这篇综述作为新研究人员在这个领域的入门资源，并鼓励未来的研究。为了更好地理解，我们介绍了扩散模型的基础知识。除此之外，我们主要关注时间序列预测、插值和生成的基于扩散的方法，并在三个独立的部分中分别介绍它们。我们还比较了同一应用的不同方法。

    Diffusion models, a family of generative models based on deep learning, have become increasingly prominent in cutting-edge machine learning research. With a distinguished performance in generating samples that resemble the observed data, diffusion models are widely used in image, video, and text synthesis nowadays. In recent years, the concept of diffusion has been extended to time series applications, and many powerful models have been developed. Considering the deficiency of a methodical summary and discourse on these models, we provide this survey as an elementary resource for new researchers in this area and also an inspiration to motivate future research. For better understanding, we include an introduction about the basics of diffusion models. Except for this, we primarily focus on diffusion-based methods for time series forecasting, imputation, and generation, and present them respectively in three individual sections. We also compare different methods for the same application a
    
[^15]: 用于节点表示的对比学习简化框架

    A Simplified Framework for Contrastive Learning for Node Representations. (arXiv:2305.00623v1 [cs.LG])

    [http://arxiv.org/abs/2305.00623](http://arxiv.org/abs/2305.00623)

    本文研究了对比学习和图神经网络相结合的节点嵌入方法，通过简单的列处理嵌入矩阵取代同行处理，在提高结果嵌入质量和训练时间的同时，提高了下游分类任务的性能。

    

    最近对比学习已经被证明为一种提取丰富多样的数据表示的有力自监督学习框架。广义上讲，对比学习依赖于数据增强方案生成输入数据的两个版本，并通过最大化归一化温度缩放交叉熵损失（NT-Xent）来学习低维度表示以识别对应于同一原始实体的增强样本。在本文中，我们研究了将对比学习与图神经网络相结合在图中嵌入节点的潜力。具体而言，我们表明，通过简单的列处理嵌入矩阵，而不是同行处理，可以显著提高结果嵌入的质量和训练时间，而同行处理是大多数同行方法采用的多层感知器（MLPs）。这种修改可提高下游分类任务的性能。

    Contrastive learning has recently established itself as a powerful self-supervised learning framework for extracting rich and versatile data representations. Broadly speaking, contrastive learning relies on a data augmentation scheme to generate two versions of the input data and learns low-dimensional representations by maximizing a normalized temperature-scaled cross entropy loss (NT-Xent) to identify augmented samples corresponding to the same original entity. In this paper, we investigate the potential of deploying contrastive learning in combination with Graph Neural Networks for embedding nodes in a graph. Specifically, we show that the quality of the resulting embeddings and training time can be significantly improved by a simple column-wise postprocessing of the embedding matrix, instead of the row-wise postprocessing via multilayer perceptrons (MLPs) that is adopted by the majority of peer methods. This modification yields improvements in downstream classification tasks of up 
    
[^16]: 生存分析的适当评分规则研究

    Proper Scoring Rules for Survival Analysis. (arXiv:2305.00621v1 [stat.ME])

    [http://arxiv.org/abs/2305.00621](http://arxiv.org/abs/2305.00621)

    本文研究了适用于生存分析的四种评分规则的扩展，证明在概率分布估计离散化程度满足一定条件时是适当评分规则，并且比较结果显示对数得分和布莱尔得分的扩展最佳。

    

    生存分析是估计未来事件发生时间的概率分布的问题，可以看作是不确定性量化问题。尽管有关于严格适当评分规则的基本理论用于不确定性量化，但很少有人了解其在生存分析中的应用。本文研究了常用的四种严格适当评分规则在生存分析中的扩展，并证明这些扩展在概率分布估计的离散化程度满足一定条件时是适当评分规则。我们还使用真实数据集比较了这些扩展评分规则的估计性能，结果表明对数得分和布莱尔得分的扩展表现最佳。

    Survival analysis is the problem of estimating probability distributions for future event times, which can be seen as a problem in uncertainty quantification. Although there are fundamental theories on strictly proper scoring rules for uncertainty quantification, little is known about those for survival analysis. In this paper, we investigate extensions of four major strictly proper scoring rules for survival analysis and we prove that these extensions are proper under certain conditions, which arise from the discretization of the estimation of probability distributions. We also compare the estimation performances of these extended scoring rules by using real datasets, and the extensions of the logarithmic score and the Brier score performed the best.
    
[^17]: 自监督活动表示学习对增量数据的探究：一项实证研究

    Self-supervised Activity Representation Learning with Incremental Data: An Empirical Study. (arXiv:2305.00619v1 [cs.LG])

    [http://arxiv.org/abs/2305.00619](http://arxiv.org/abs/2305.00619)

    本研究探究了在移动感知环境中使用自监督表示学习模型对时间序列数据进行分类的影响，我们提出并评估了一种流程，其中模型学习使用未标记的时间序列数据提取信息特征，然后使用模型提取的特征对标记数据进行分类。

    

    在移动感知环境中，移动设备上的各种传感器持续生成大量的数据。分析这种不断增加的数据面临着许多挑战，包括获得标注数据的限制和环境的不断变化。自监督学习的最新进展已被利用作为预训练步骤，增强传统监督模型的性能，以解决标记数据集的缺失。本研究考察了使用自监督表示学习模型对时间序列分类任务的影响，其中数据是逐步可用的。我们提出并评估了一种流程，其中模型学习使用未标记的时间序列数据提取信息特征，然后使用模型提取的特征对标记数据进行分类。我们分析了在最终分类性能上，未标记数据的大小、分布和来源等因素的影响。

    In the context of mobile sensing environments, various sensors on mobile devices continually generate a vast amount of data. Analyzing this ever-increasing data presents several challenges, including limited access to annotated data and a constantly changing environment. Recent advancements in self-supervised learning have been utilized as a pre-training step to enhance the performance of conventional supervised models to address the absence of labelled datasets. This research examines the impact of using a self-supervised representation learning model for time series classification tasks in which data is incrementally available. We proposed and evaluated a workflow in which a model learns to extract informative features using a corpus of unlabeled time series data and then conducts classification on labelled data using features extracted by the model. We analyzed the effect of varying the size, distribution, and source of the unlabeled data on the final classification performance acro
    
[^18]: 使用RePU激活函数的可微分神经网络：在得分估计和保序回归中的应用。

    Differentiable Neural Networks with RePU Activation: with Applications to Score Estimation and Isotonic Regression. (arXiv:2305.00608v1 [stat.ML])

    [http://arxiv.org/abs/2305.00608](http://arxiv.org/abs/2305.00608)

    该论文介绍了使用RePU激活函数的可微分神经网络，在近似$C^s$平滑函数及其导数的同时建立了下限误差界，并证明了其在降低维度灾难方面的能力，此外还提出了一种使用RePU网络的惩罚保序回归(PDIR)方法。

    

    我们研究了由修正后的幂单元（RePU）函数激活的可微分神经网络的属性。我们展示了RePU神经网络的偏导数可以由混合激活RePU网络来表示，并推导了导数RePU网络函数类的复杂度的上界。在使用RePU激活的深度神经网络中，我们建立了同时近似$C^s$平滑函数及其导数的误差界。此外，当数据具有近似低维支持时，我们推导出改进的逼近误差界，证明了RePU网络减缓维度灾难的能力。为了说明我们的结果的实用性，我们考虑了深度得分匹配估计器(DSME)，并提出了一种使用RePU网络的惩罚保序回归(PDIR)。我们在假定目标函数属于$C^s$平滑函数类的情况下为DSME和PDIR建立非渐近超额风险界。

    We study the properties of differentiable neural networks activated by rectified power unit (RePU) functions. We show that the partial derivatives of RePU neural networks can be represented by RePUs mixed-activated networks and derive upper bounds for the complexity of the function class of derivatives of RePUs networks. We establish error bounds for simultaneously approximating $C^s$ smooth functions and their derivatives using RePU-activated deep neural networks. Furthermore, we derive improved approximation error bounds when data has an approximate low-dimensional support, demonstrating the ability of RePU networks to mitigate the curse of dimensionality. To illustrate the usefulness of our results, we consider a deep score matching estimator (DSME) and propose a penalized deep isotonic regression (PDIR) using RePU networks. We establish non-asymptotic excess risk bounds for DSME and PDIR under the assumption that the target functions belong to a class of $C^s$ smooth functions. We 
    
[^19]: 零日恶意软件的分类和在线聚类

    Classification and Online Clustering of Zero-Day Malware. (arXiv:2305.00605v1 [cs.CR])

    [http://arxiv.org/abs/2305.00605](http://arxiv.org/abs/2305.00605)

    本文研究了零日恶意软件的分类和在线聚类。实验使用 EMBER 数据集，对有流入的恶意软件样本进行了分类，得到了 95.33% 的平衡准确度。在剩下的数据中，使用自组织映射实现了纯度从 47.61% 到 77.68% 的聚类。

    

    不断产生大量新的恶意软件，我们需要将其与良性样本区分开来，并将其分类到恶意软件家族中。为此，我们需要研究现有恶意软件家族是如何发展，以及如何检查新出现的恶意软件家族。本文重点研究对入侵样本进行在线处理，将其分配给现有家族，或在新家族的情况下对其进行聚类。我们使用 EMBER 数据集中的七个流行恶意软件家族，其中四个在训练集中，另外三个在测试集中。通过多层感知器的分类得分，我们确定哪些样本将被分类，哪些将被聚类到新的恶意软件家族中。我们以平衡准确度为 95.33% 对 97.21% 的流数据进行了分类，然后使用自组织映射对剩余数据进行了聚类，实现了纯度从四个聚类的 47.61% 到十个聚类的 77.68%。

    A large amount of new malware is constantly being generated, which must not only be distinguished from benign samples, but also classified into malware families. For this purpose, investigating how existing malware families are developed and examining emerging families need to be explored. This paper focuses on the online processing of incoming malicious samples to assign them to existing families or, in the case of samples from new families, to cluster them. We experimented with seven prevalent malware families from the EMBER dataset, with four in the training set and three additional new families in the test set. Based on the classification score of the multilayer perceptron, we determined which samples would be classified and which would be clustered into new malware families. We classified 97.21% of streaming data with a balanced accuracy of 95.33%. Then, we clustered the remaining data using a self-organizing map, achieving a purity from 47.61% for four clusters to 77.68% for ten 
    
[^20]: ISAAC Newton：牛顿法的基于输入的近似曲率

    ISAAC Newton: Input-based Approximate Curvature for Newton's Method. (arXiv:2305.00604v1 [cs.LG])

    [http://arxiv.org/abs/2305.00604](http://arxiv.org/abs/2305.00604)

    ISAAC Newton方法提出了一种使用选择的二阶信息调整梯度的方法，并且在选择批量大小小于神经元数量的情况下，计算开销消失，能够在小批量随机情况下有效训练。

    

    我们提出了ISAAC（Input-baSed ApproximAte Curvature），该方法使用选择的二阶信息来调整梯度，并且在批量大小小于神经元数量的情况下具有渐近消失的计算开销。我们展示了在仅基于相应层的输入而不需要实质性计算开销的情况下，计算出一个良好的调节器是可能的。所提出的方法允许在小批量随机情况下有效训练，这使它与一阶以及二阶方法具有竞争力。

    We present ISAAC (Input-baSed ApproximAte Curvature), a novel method that conditions the gradient using selected second-order information and has an asymptotically vanishing computational overhead, assuming a batch size smaller than the number of neurons. We show that it is possible to compute a good conditioner based on only the input to a respective layer without a substantial computational overhead. The proposed method allows effective training even in small-batch stochastic regimes, which makes it competitive to first-order as well as second-order methods.
    
[^21]: Consolidator: 融合连接的可合并适配器，用于视觉领域的自适应

    Consolidator: Mergeable Adapter with Grouped Connections for Visual Adaptation. (arXiv:2305.00603v1 [cs.CV])

    [http://arxiv.org/abs/2305.00603](http://arxiv.org/abs/2305.00603)

    本文提出了一种名为 Consolidator 的 mergeable adapter with grouped connections for visual adaptation，促进了视觉 transformer 的知识转移，实现了多个图像分类和对象检测任务的最新转移表现。

    

    近年来，transformer 作为视觉特征提取器表现出超越传统卷积模型的强大能力。然而，视觉 transformer 的成功在很大程度上归功于其容纳大量参数的能力。这导致将大型模型适应下游任务面临新的挑战。为了解决这些问题，本论文提出了一种名为 "Consolidator" 的融合连接的可合并适配器，用于视觉领域的自适应，它促进了视觉 transformer 的知识转移，实现了多个图像分类和对象检测任务的最新转移表现。

    Recently, transformers have shown strong ability as visual feature extractors, surpassing traditional convolution-based models in various scenarios. However, the success of vision transformers largely owes to their capacity to accommodate numerous parameters. As a result, new challenges for adapting large models to downstream tasks arise. On the one hand, classic fine-tuning tunes all parameters in a huge model for every task and thus easily falls into overfitting, leading to inferior performance. On the other hand, on resource-limited devices, fine-tuning stores a full copy of parameters and thus is usually impracticable for the shortage of storage space. However, few works have focused on how to efficiently and effectively transfer knowledge in a vision transformer. Existing methods did not dive into the properties of visual features, leading to inferior performance. Moreover, some of them bring heavy inference cost though benefiting storage. To tackle these problems, we propose cons
    
[^22]: StyleGenes: GANs的离散高效潜在分布

    StyleGenes: Discrete and Efficient Latent Distributions for GANs. (arXiv:2305.00599v1 [cs.CV])

    [http://arxiv.org/abs/2305.00599](http://arxiv.org/abs/2305.00599)

    我们提出了一种离散的潜在分布来代替连续的先验分布，这种基于基因的潜在编码可以通过少量可学习参数表示大量唯一的潜在样本，并且提供了新的直观的潜在空间探索方法。

    

    我们提出了一种适用于生成对抗网络（GANs）的离散潜在分布。我们从一个可学习的有限潜在组中采样，而不是从连续的先验中绘制潜在向量。然而，直接参数化这种分布会导致内存的线性增加，以确保足够的样本多样性。我们通过从生物体中获得灵感来解决这个关键问题。我们将潜在空间分成一组基因，对于每个基因，我们训练一个小的基因变体库。因此，通过独立地对每个基因采样变体，然后将它们组合成最终的潜在向量，我们的方法可以用少量的可学习参数表示大量的唯一潜在样本。有趣的是，我们基于基因的潜在编码允许新的和直观的潜在空间探索方法，使得从我们的无条件模型中进行条件采样类似于创建角色的互动游戏。

    We propose a discrete latent distribution for Generative Adversarial Networks (GANs). Instead of drawing latent vectors from a continuous prior, we sample from a finite set of learnable latents. However, a direct parametrization of such a distribution leads to an intractable linear increase in memory in order to ensure sufficient sample diversity. We address this key issue by taking inspiration from the encoding of information in biological organisms. Instead of learning a separate latent vector for each sample, we split the latent space into a set of genes. For each gene, we train a small bank of gene variants. Thus, by independently sampling a variant for each gene and combining them into the final latent vector, our approach can represent a vast number of unique latent samples from a compact set of learnable parameters. Interestingly, our gene-inspired latent encoding allows for new and intuitive approaches to latent-space exploration, enabling conditional sampling from our uncondit
    
[^23]: 认知人形机器人的增量程序和感知动作学习

    Incremental procedural and sensorimotor learning in cognitive humanoid robots. (arXiv:2305.00597v1 [cs.RO])

    [http://arxiv.org/abs/2305.00597](http://arxiv.org/abs/2305.00597)

    本文提出了一个基于CONAIM模型的认知代理，能够逐步学习程序，通过增加新功能来解决之前无法解决的任务。在模拟环境中，使用增强学习的单一程序学习机制对人形机器人进行了物体跟踪实验。

    

    自动学习日益复杂动作和行为的能力是自主系统的长期目标。本研究受Jean Piaget感知动作三个子阶段的启发，提出了一种基于CONAIM（意识注意力集成模型）的认知代理，能够逐步学习程序。本文介绍了每个子阶段需要的认知功能以及如何添加新功能来解决代理先前无法解决的任务。在Cognitive Systems Toolkit（CST）模拟环境中，使用基于增强学习的单一程序学习机制对人形机器人进行实验，执行物体跟踪任务。

    The ability to automatically learn movements and behaviors of increasing complexity is a long-term goal in autonomous systems. Indeed, this is a very complex problem that involves understanding how knowledge is acquired and reused by humans as well as proposing mechanisms that allow artificial agents to reuse previous knowledge. Inspired by Jean Piaget's theory's first three sensorimotor substages, this work presents a cognitive agent based on CONAIM (Conscious Attention-Based Integrated Model) that can learn procedures incrementally. Throughout the paper, we show the cognitive functions required in each substage and how adding new functions helps address tasks previously unsolved by the agent. Experiments were conducted with a humanoid robot in a simulated environment modeled with the Cognitive Systems Toolkit (CST) performing an object tracking task. The system is modeled using a single procedural learning mechanism based on Reinforcement Learning. The increasing agent's cognitive co
    
[^24]: 深度学习库对在线自适应轻量级时间序列异常检测的影响

    Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection. (arXiv:2305.00595v1 [cs.LG])

    [http://arxiv.org/abs/2305.00595](http://arxiv.org/abs/2305.00595)

    本文通过在三个深度学习库中实现两种最先进的方法，并进行评估，研究了深度学习库对在线自适应轻量级时间序列异常检测的影响。

    

    在没有人为干预和领域知识的情况下提供在线自适应轻量级时间序列异常检测非常有价值。过去几年中引入了几种这样的异常检测方法，但它们都仅在一个深度学习库中实现。随着深度学习库的发展，不同的深度学习库如何影响这些异常检测方法尚不清楚，因为没有这样的评估。随机选择一个深度学习库来实现异常检测方法可能不能展现出该方法的真实性能，这可能会误导用户相信某种方法比另一种更好。因此，在本文中，我们通过在三个知名深度学习库中实现两种最先进的异常检测方法，并评估这些方法的结果，来研究深度学习库对在线自适应轻量级时间序列异常检测的影响。

    Providing online adaptive lightweight time series anomaly detection without human intervention and domain knowledge is highly valuable. Several such anomaly detection approaches have been introduced in the past years, but all of them were only implemented in one deep learning library. With the development of deep learning libraries, it is unclear how different deep learning libraries impact these anomaly detection approaches since there is no such evaluation available. Randomly choosing a deep learning library to implement an anomaly detection approach might not be able to show the true performance of the approach. It might also mislead users in believing one approach is better than another. Therefore, in this paper, we investigate the impact of deep learning libraries on online adaptive lightweight time series anomaly detection by implementing two state-of-the-art anomaly detection approaches in three well-known deep learning libraries and evaluating how these two approaches are indiv
    
[^25]: 可靠的无梯度和无似然对话式建模API优化方法

    Reliable Gradient-free and Likelihood-free Prompt Tuning. (arXiv:2305.00593v1 [cs.LG])

    [http://arxiv.org/abs/2305.00593](http://arxiv.org/abs/2305.00593)

    本文提供一种能够应对挑战性情景，即仅具备API访问权限的情况下，建模API进行优化的方法，并能够对推理不确定性进行量化。

    

    由于隐私或商业限制，大型预训练语言模型（PLMs）通常作为黑盒API提供。对这些模型进行下游任务的微调是具有挑战性的，因为既无法访问模型的内部表示，也无法通过它传播梯度。本文通过开发只有API访问权限的PLM的自适应技术来应对这些挑战。在最近的软提示调整工作的基础上，我们开发了在不需要计算梯度的情况下调整软提示的方法。此外，我们扩展了这些技术，不需要访问PLM除了输入嵌入之外的任何内部表示。我们的方法学习了提示的分布，而不是单一的提示，可以对推理不确定性进行量化，这是在仅具有API访问权限的情况下考虑提示不确定性的首次尝试。最后，通过广泛的实验，我们仔细检查了所提出的方法，并表明它们的性能与基于梯度和基于似然的方法相当，甚至更好。

    Due to privacy or commercial constraints, large pre-trained language models (PLMs) are often offered as black-box APIs. Fine-tuning such models to downstream tasks is challenging because one can neither access the model's internal representations nor propagate gradients through it. This paper addresses these challenges by developing techniques for adapting PLMs with only API access. Building on recent work on soft prompt tuning, we develop methods to tune the soft prompts without requiring gradient computation. Further, we develop extensions that in addition to not requiring gradients also do not need to access any internal representation of the PLM beyond the input embeddings. Moreover, instead of learning a single prompt, our methods learn a distribution over prompts allowing us to quantify predictive uncertainty. Ours is the first work to consider uncertainty in prompts when only having API access to the PLM. Finally, through extensive experiments, we carefully vet the proposed meth
    
[^26]: GPT-2是如何计算大于符号的？解释预训练语言模型中的数学能力

    How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])

    [http://arxiv.org/abs/2305.00586](http://arxiv.org/abs/2305.00586)

    本研究运用机械式可解释性技术探究了GPT-2 Small的数学能力，并确定了它的计算图中的一个小电路用于计算大于符号，该电路的多层感知器提高了结束年份大于开始年份的概率，并且该电路具有广泛的适用性。

    

    预训练语言模型在未被明确训练的任务上表现出惊人的能力，但它们如何实现这些功能却不为人所知。本文通过机械式可解释性技术探究预训练语言模型通常具有的基本数学能力。具体来说，我们以GPT-2 Small为例，研究其能否通过输入"战争持续时间是从1732年到17年"，预测出有效的两位数字的截止年份 (大于32年)。我们首先确定了一个电路，即GPT-2 Small计算图的一个小子集，用于计算这个任务的输出，然后我们解释了每个电路组件的作用，显示出GPT-2 Small的最终多层感知器提高了结束年份大于开始年份的概率。最后，我们证明了我们的电路适用于其他任务，在其他大于场景中发挥作用。

    Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years > 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
    
[^27]: 学习未知时间约束下的安全强化学习策略

    Joint Learning of Policy with Unknown Temporal Constraints for Safe Reinforcement Learning. (arXiv:2305.00576v1 [eess.SY])

    [http://arxiv.org/abs/2305.00576](http://arxiv.org/abs/2305.00576)

    基于逻辑约束强化学习算法和进化算法的信号时态逻辑规范合成框架，同时学习安全约束和最优强化学习策略，并有理论保证。

    

    在许多实际应用中，强化学习算法的安全约束往往是未知的或者没有明确定义的。我们提出了一个框架，在这样的环境中同时学习安全约束和最优强化学习策略，并且有理论保证。我们的方法将逻辑约束强化学习算法与进化算法结合起来，合成信号时态逻辑(STL)规范。该框架是由定理支持的，这些定理建立了我们的联合学习过程的收敛性，并提供了发现策略与真正最优策略之间的误差界限。我们在网格世界环境中展示了我们的框架，在演示我们理论在实践中的有效性的同时，成功地确定了可接受的安全约束和强化学习策略。

    In many real-world applications, safety constraints for reinforcement learning (RL) algorithms are either unknown or not explicitly defined. We propose a framework that concurrently learns safety constraints and optimal RL policies in such environments, supported by theoretical guarantees. Our approach merges a logically-constrained RL algorithm with an evolutionary algorithm to synthesize signal temporal logic (STL) specifications. The framework is underpinned by theorems that establish the convergence of our joint learning process and provide error bounds between the discovered policy and the true optimal policy. We showcased our framework in grid-world environments, successfully identifying both acceptable safety constraints and RL policies while demonstrating the effectiveness of our theorems in practice.
    
[^28]: 通过离线多目标强化学习扩展帕累托有效决策

    Scaling Pareto-Efficient Decision Making Via Offline Multi-Objective RL. (arXiv:2305.00567v1 [cs.LG])

    [http://arxiv.org/abs/2305.00567](http://arxiv.org/abs/2305.00567)

    本文提出了一种新的数据驱动离线MORL设置和一个用于离线MORL的算法PEDA。PEDA通过在多个模型中选择最优模型来实现正交偏好。实验表明，PEDA在样本效率方面优于现有方法，同时还可扩展到具有更大动作空间的复杂环境中。

    

    多目标强化学习（MORL）的目标是学习能同时优化多个竞争目标的策略。实际应用中，代理对目标的偏好可能不是先验已知的，因此我们需要在测试时能够适应任意偏好的策略。本文提出一种新的数据驱动离线MORL设置，我们希望只使用有限的离线演示数据集来学习一个偏好不敏感的策略代理。本文的两个主要贡献：第一，我们介绍了D4MORL，这是一组专门针对离线设置设计的MORL数据集。它包含180万个数据，是通过在6个MuJoCo环境中优化2-3个目标的参考策略的过程中获得的。第二，我们提出了帕累托有效决策代理（PEDA），它是一族离线MORL算法，通过构建和扩展决策转移（DT）方法来适用于新的MORL设置。 PEDS训练多个模型，每个模型都为不同的、随机选择的目标优化。在测试时，PEDA通过一种基于用户指定偏好的原则来选择模型。我们在六个MuJoCo环境下评估了PEDA的性能，并证明PEDA在样本效率方面优于最先进的MORL方法，并自然地扩展到更复杂的具有较大行动空间的环境中。

    The goal of multi-objective reinforcement learning (MORL) is to learn policies that simultaneously optimize multiple competing objectives. In practice, an agent's preferences over the objectives may not be known apriori, and hence, we require policies that can generalize to arbitrary preferences at test time. In this work, we propose a new data-driven setup for offline MORL, where we wish to learn a preference-agnostic policy agent using only a finite dataset of offline demonstrations of other agents and their preferences. The key contributions of this work are two-fold. First, we introduce D4MORL, (D)atasets for MORL that are specifically designed for offline settings. It contains 1.8 million annotated demonstrations obtained by rolling out reference policies that optimize for randomly sampled preferences on 6 MuJoCo environments with 2-3 objectives each. Second, we propose Pareto-Efficient Decision Agents (PEDA), a family of offline MORL algorithms that builds and extends Decision Tr
    
[^29]: 类平衡扩散模型

    Class-Balancing Diffusion Models. (arXiv:2305.00562v1 [cs.CV])

    [http://arxiv.org/abs/2305.00562](http://arxiv.org/abs/2305.00562)

    这项工作探究了扩散模型在类别不平衡的数据上的表现，并提出了一种解决方案“类平衡扩散模型”通过使用分布调整正则化器进行训练。

    

    最近的研究表明，基于扩散的模型在生成高质量视觉数据同时保持更好的多样性方面有优势。但这种观察结果只适用于策划好的数据分布，即数据样本经过精心处理以在标签方面均匀分布。在实践中，长尾数据分布似乎更为普遍，而扩散模型在这种类别不平衡的数据上的表现还未知。在这项工作中，我们首先研究了这个问题，观察到当扩散模型在类别不平衡的数据集上训练时，多样性和保真度显著降低。尤其是在尾部类别，生成的样本严重缺乏多样性，我们观察到严重的模式崩溃问题。为了解决这个问题，我们假设数据分布不是类平衡的，提出了一种名为“类平衡扩散模型”的解决方案，使用分布调整正则化器进行训练。

    Diffusion-based models have shown the merits of generating high-quality visual data while preserving better diversity in recent studies. However, such observation is only justified with curated data distribution, where the data samples are nicely pre-processed to be uniformly distributed in terms of their labels. In practice, a long-tailed data distribution appears more common and how diffusion models perform on such class-imbalanced data remains unknown. In this work, we first investigate this problem and observe significant degradation in both diversity and fidelity when the diffusion model is trained on datasets with class-imbalanced distributions. Especially in tail classes, the generations largely lose diversity and we observe severe mode-collapse issues. To tackle this problem, we set from the hypothesis that the data distribution is not class-balanced, and propose Class-Balancing Diffusion Models (CBDM) that are trained with a distribution adjustment regularizer as a solution. E
    
[^30]: 学习具有物理一致性的异质性粒子相互作用的集体关系推断

    Collective Relational Inference for learning physics-consistent heterogeneous particle interactions. (arXiv:2305.00557v1 [cs.LG])

    [http://arxiv.org/abs/2305.00557](http://arxiv.org/abs/2305.00557)

    本论文提出了一种新的概率方法用于学习异质性粒子相互作用的集体关系推断，与现有方法相比，该方法集体地推断不同边的相互作用类型，使用物理感应的图神经网络来学习具有物理一致性的成对相互作用，并在推断准确性和保持物理保真度方面一致优于现有方法。

    

    相互作用粒子系统在自然界和工程中无处不在。揭示粒子相互作用定律具有基本重要性，但由于底层配置复杂性而具有极大的挑战性。最近开发的机器学习方法在发现同质系统粒子轨迹中的成对相互作用方面显示出极大的潜力。然而，它们无法揭示异质系统中的相互作用，而这种系统在现实中普遍存在，其中多个相互作用类型同时存在，并且需要关系推断。在这里，我们提出了一种新的概率方法用于关系推断，与现有方法相比，具有两个独特的特征：首先，它集体地推断不同边的相互作用类型；其次，它使用物理感应的图神经网络来学习具有物理一致性的成对相互作用。我们在几个基准数据集上评估了所提出的方法，并证明其在推断的相互作用准确性和保持物理保真度方面一致优于现有方法。具体而言，我们的方法确定了具有重要物理意义的新型相互作用类型，揭示了统治系统的隐藏物理原理，并在提高物理性质的预测方面显示出极大的潜力。

    Interacting particle systems are ubiquitous in nature and engineering. Revealing particle interaction laws is of fundamental importance but also particularly challenging due to underlying configurational complexities. Recently developed machine learning methods show great potential in discovering pairwise interactions from particle trajectories in homogeneous systems. However, they fail to reveal interactions in heterogeneous systems that are prevalent in reality, where multiple interaction types coexist simultaneously and relational inference is required. Here, we propose a novel probabilistic method for relational inference, which possesses two distinctive characteristics compared to existing methods. First, it infers the interaction types of different edges collectively, and second, it uses a physics-induced graph neural network to learn physics-consistent pairwise interactions. We evaluate the proposed methodology across several benchmark datasets and demonstrate that it is consist
    
[^31]: 通过引导随机搜索从人脑活动中重建视觉图像

    Reconstructing seen images from human brain activity via guided stochastic search. (arXiv:2305.00556v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.00556](http://arxiv.org/abs/2305.00556)

    本研究使用条件生成扩散模型，改进过去的视觉重建算法，通过对一小组图像的采样和编码模型的选择，实现了从人脑活动中高质量、保留语义内容的重建结果，并发现了视觉皮层不同区域的重建时间差异。

    

    视觉重建算法是一种将脑活动映射到像素的解释工具。过去的重建算法采用大规模库的暴力搜索来选择候选图像，这些图像通过编码模型可以准确地预测脑活动。本研究使用条件生成扩散模型来扩展和改进这种基于搜索的策略。我们在大部分视觉皮层的体素中从人脑活动（7T fMRI）解码出语义描述符，然后使用扩散模型在此描述符的条件下对一小组图像进行采样。我们将每个样本通过编码模型，选择最能准确预测脑活动的图像，然后使用这些图像来种子另一个库。我们展示了这个过程通过在迭代中细化低级图像细节，同时保留语义内容而收敛到高质量的重建结果。有趣的是，收敛所需的时间在视觉皮层中有系统差异，表明了脑区的高层抽象概念需要更长的时间来集成和反映。

    Visual reconstruction algorithms are an interpretive tool that map brain activity to pixels. Past reconstruction algorithms employed brute-force search through a massive library to select candidate images that, when passed through an encoding model, accurately predict brain activity. Here, we use conditional generative diffusion models to extend and improve this search-based strategy. We decode a semantic descriptor from human brain activity (7T fMRI) in voxels across most of visual cortex, then use a diffusion model to sample a small library of images conditioned on this descriptor. We pass each sample through an encoding model, select the images that best predict brain activity, and then use these images to seed another library. We show that this process converges on high-quality reconstructions by refining low-level image details while preserving semantic content across iterations. Interestingly, the time-to-convergence differs systematically across visual cortex, suggesting a succi
    
[^32]: 基于医学距离的学习方法：医学概念和患者表示的一种方法

    MD-Manifold: A Medical-Distance-Based Representation Learning Approach for Medical Concept and Patient Representation. (arXiv:2305.00553v1 [cs.LG])

    [http://arxiv.org/abs/2305.00553](http://arxiv.org/abs/2305.00553)

    MD-Manifold采用医学距离的表示学习方法，引入新的数据增强方法、概念距离度量和患者-患者网络，有效地将医学领域知识和先前数据信息纳入考虑，提高医疗保健分析任务的性能表现。

    

    有效地表示医学概念和患者对于医疗保健分析应用非常重要。在医疗保健分析任务中，表示医学概念需要将医学领域知识和患者描述数据的先前信息纳入考虑。目前的方法，如特征工程和将医学概念映射到标准化术语中，存在捕获来自患者描述数据的动态模式的局限性。其他基于嵌入的方法在融合重要的医学领域知识方面存在困难，通常需要大量的训练数据，这对于大多数医疗保健系统来说可能不可行。我们提出的框架，MD-Manifold，引入了一种新的医学概念和患者表示方法。它包括一种新的数据增强方法、概念距离度量和患者-患者网络，以纳入关键的医学领域知识和先前的数据信息。然后，它采用流形学习方法生成良好的医学表示。在三个医疗保健数据集上的实验结果表明，MD-Manifold在改进疾病诊断和患者聚类等医疗保健分析任务方面具有有效性。

    Effectively representing medical concepts and patients is important for healthcare analytical applications. Representing medical concepts for healthcare analytical tasks requires incorporating medical domain knowledge and prior information from patient description data. Current methods, such as feature engineering and mapping medical concepts to standardized terminologies, have limitations in capturing the dynamic patterns from patient description data. Other embedding-based methods have difficulties in incorporating important medical domain knowledge and often require a large amount of training data, which may not be feasible for most healthcare systems. Our proposed framework, MD-Manifold, introduces a novel approach to medical concept and patient representation. It includes a new data augmentation approach, concept distance metric, and patient-patient network to incorporate crucial medical domain knowledge and prior data information. It then adapts manifold learning methods to gener
    
[^33]: SoK：用于网络入侵检测的机器学习实用评估

    SoK: Pragmatic Assessment of Machine Learning for Network Intrusion Detection. (arXiv:2305.00550v1 [cs.CR])

    [http://arxiv.org/abs/2305.00550](http://arxiv.org/abs/2305.00550)

    本文提出了“实用评估”的概念，使从业人员能够评估ML方法在网络入侵检测（NID）中的实际价值，并提出了一组指导方针用于设计实验，可以在考虑实际系统的实际限制的情况下，比较不同ML方法在NID中的有效性和效率。

    

    机器学习已经成为解决许多实际问题的有价值资产。但是，对于网络入侵检测（NID），科学上的机器学习进展仍然受到从业人员的怀疑。这种脱节是由于研究论文的内在局限性引起的，其中许多论文主要旨在展示新方法“胜过”先前的工作，往往忽视了在实际系统中部署所提出解决方案的实际影响。不幸的是，对于NID来说，ML的价值取决于许多因素，如硬件，这些因素常常被科学文献所忽略。本文旨在通过“改变”研究中采用的评估方法，减少从业者对NID的ML的怀疑。在阐明影响ML在NID中实际部署的“因素”之后，我们提出了“实用评估”的概念，使从业人员能够评估ML方法在NID中的实际价值。然后，我们展示了ML在NID的最新研究状态存在几个不匹配之处，其中提出的假设与现实世界的要求分歧。具体而言，本文提供了一份ML NID最新研究的批判性评估，说明研究中作出的假设与现实世界要求的差异。然后，我们提出了一组指导方针，用于设计实验，可以在考虑实际系统的实际限制的情况下，比较不同ML方法在NID中的有效性和效率。最后，我们通过对真实NID数据集进行实验来说明我们的指南的应用。

    Machine Learning (ML) has become a valuable asset to solve many real-world tasks. For Network Intrusion Detection (NID), however, scientific advances in ML are still seen with skepticism by practitioners. This disconnection is due to the intrinsically limited scope of research papers, many of which primarily aim to demonstrate new methods ``outperforming'' prior work -- oftentimes overlooking the practical implications for deploying the proposed solutions in real systems. Unfortunately, the value of ML for NID depends on a plethora of factors, such as hardware, that are often neglected in scientific literature.  This paper aims to reduce the practitioners' skepticism towards ML for NID by "changing" the evaluation methodology adopted in research. After elucidating which "factors" influence the operational deployment of ML in NID, we propose the notion of "pragmatic assessment", which enable practitioners to gauge the real value of ML methods for NID. Then, we show that the state-of-res
    
[^34]: 使用模糊分箱进行校准误差估计

    Calibration Error Estimation Using Fuzzy Binning. (arXiv:2305.00543v1 [cs.LG])

    [http://arxiv.org/abs/2305.00543](http://arxiv.org/abs/2305.00543)

    本文提出了一种模糊校准误差度量（FCE），利用模糊分箱方法计算校准误差，从而缓解了概率偏斜的影响并提供了更紧密的估计值。与传统指标ECE相比，FCE在多类设置中表现更好，https://github.com/srdgFHE/FCE-paper。

    

    基于神经网络的决策往往会过于自信，其原始结果的概率并不符合真实的决策概率。神经网络的校准是实现更可靠的深度学习框架的关键步骤。先前的校准误差度量主要利用清晰的分箱成员资格度量。这加剧了模型概率的偏斜，并描绘了校准误差的不完整图像。在本文中，我们提出了一种利用模糊分箱方法计算校准误差的模糊校准误差度量（FCE）。这种方法缓解了概率偏斜的影响，并在测量校准误差时提供了更紧密的估计值。我们比较了我们的指标与ECE在不同的数据群体和类别成员身份中的表现。我们的结果显示，FCE在校准误差估计方面表现更好，特别是在多类设置中，缓解了模型置信度分数偏斜对校准误差估计的影响。我们提供了我们的代码https://github.com/srdgFHE/FCE-paper，以便未来的可重复性和使用FCE进行校准误差估计。

    Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code a
    
[^35]: 机器学习的可解释性：最新进展和未来前景研究综述

    Interpretability of Machine Learning: Recent Advances and Future Prospects. (arXiv:2305.00537v1 [cs.MM])

    [http://arxiv.org/abs/2305.00537](http://arxiv.org/abs/2305.00537)

    本文综述了最新的机器学习可解释性进展和未来的前景，并提供了相关的多媒体计算应用示例，包括跨模态表示学习、人脸识别和物体识别等。研究显示，机器学习的可解释性研究有着重要的研究方向和前景。

    

    机器学习的广泛应用已经吸引了人们对各种多媒体内容（如文本、图像、音频和视频等）的研究和探索。特别是在深度神经网络等领域，现代机器学习的黑盒特性已成为面向多媒体研究和应用中表示学习的主要挑战。为了解决这个黑盒问题，近年来，机器学习的可解释性研究吸引了巨大的关注。本文对机器学习的可解释性最新进展和未来前景进行了综述，并提供了多个相关的多媒体计算应用示例，包括文本-图像跨模态表示学习、人脸识别和物体识别等。这个研究表明机器学习的可解释性研究有着重要的研究方向和前景。

    The proliferation of machine learning (ML) has drawn unprecedented interest in the study of various multimedia contents such as text, image, audio and video, among others. Consequently, understanding and learning ML-based representations have taken center stage in knowledge discovery in intelligent multimedia research and applications. Nevertheless, the black-box nature of contemporary ML, especially in deep neural networks (DNNs), has posed a primary challenge for ML-based representation learning. To address this black-box problem, the studies on interpretability of ML have attracted tremendous interests in recent years. This paper presents a survey on recent advances and future prospects on interpretability of ML, with several application examples pertinent to multimedia computing, including text-image cross-modal representation learning, face recognition, and the recognition of objects. It is evidently shown that the study of interpretability of ML promises an important research dir
    
[^36]: 利用图神经网络辅助蒙特卡罗树搜索获得近似最优的斯坦纳树

    Nearly Optimal Steiner Trees using Graph Neural Network Assisted Monte Carlo Tree Search. (arXiv:2305.00535v1 [cs.LG])

    [http://arxiv.org/abs/2305.00535](http://arxiv.org/abs/2305.00535)

    本文提出了一种利用图神经网络和蒙特卡罗树搜索计算斯坦纳树的方法，它在各种类型的图上优于标准的2近似算法，常常找到最优解。

    

    图神经网络在学习问题、组合问题和图问题中十分有效，例如子图同构问题和旅行商问题。本文提出了一种计算斯坦纳树的方法，该方法将图神经网络和蒙特卡罗树搜索相结合。首先我们训练一个图神经网络，该网络以部分解决方案为输入，输出一个新节点作为建议。然后在蒙特卡罗搜索中使用该神经网络来计算斯坦纳树。该方法在许多不同类型的图中一直表现优于标准的2近似算法，并且通常可以找到最优解。

    Graph neural networks are useful for learning problems, as well as for combinatorial and graph problems such as the Subgraph Isomorphism Problem and the Traveling Salesman Problem. We describe an approach for computing Steiner Trees by combining a graph neural network and Monte Carlo Tree Search. We first train a graph neural network that takes as input a partial solution and proposes a new node to be added as output. This neural network is then used in a Monte Carlo search to compute a Steiner tree. The proposed method consistently outperforms the standard 2-approximation algorithm on many different types of graphs and often finds the optimal solution.
    
[^37]: ICQ:一种适用于位限制信道的最佳臂识别量化方案

    ICQ: A Quantization Scheme for Best-Arm Identification Over Bit-Constrained Channels. (arXiv:2305.00528v1 [cs.LG])

    [http://arxiv.org/abs/2305.00528](http://arxiv.org/abs/2305.00528)

    本文提出了一种名为ICQ的新颖量化方案，可用于现有的置信区间学习算法，如连续淘汰算法，并在位限制信道上实现最佳臂识别，具有与未量化算法相同的阶优化样本复杂度，且仅需要极少量的通信频率。

    

    本文研究了多个代理的分布式变种多臂老虎机设置下的最佳臂识别问题，其中每个代理与老虎机的一臂相关联，生成遵循未知分布的随机回报。此外，每个代理可以通过位限制信道将观察到的回报与学习器通信。我们提出了一种新颖的量化方案，称为Inflating Confidence for Quantization（ICQ），可以应用于现有的基于置信区间的学习算法，例如连续淘汰算法。我们分析了应用于连续淘汰算法的ICQ的性能，并表明ICQ-SE的总体算法具有与（未量化的）SE算法相同的阶优化样本复杂度。此外，它只需要学习者和代理之间指数稀疏的通信频率，因此需要比现有量化方案更少的位数才能成功识别最佳臂。

    We study the problem of best-arm identification in a distributed variant of the multi-armed bandit setting, with a central learner and multiple agents. Each agent is associated with an arm of the bandit, generating stochastic rewards following an unknown distribution. Further, each agent can communicate the observed rewards with the learner over a bit-constrained channel. We propose a novel quantization scheme called Inflating Confidence for Quantization (ICQ) that can be applied to existing confidence-bound based learning algorithms such as Successive Elimination. We analyze the performance of ICQ applied to Successive Elimination and show that the overall algorithm, named ICQ-SE, has the order-optimal sample complexity as that of the (unquantized) SE algorithm. Moreover, it requires only an exponentially sparse frequency of communication between the learner and the agents, thus requiring considerably fewer bits than existing quantization schemes to successfully identify the best arm.
    
[^38]: StyleLipSync：基于风格的个性化唇形动画视频生成

    StyleLipSync: Style-based Personalized Lip-sync Video Generation. (arXiv:2305.00521v1 [cs.CV])

    [http://arxiv.org/abs/2305.00521](http://arxiv.org/abs/2305.00521)

    本文提出了一种基于风格的个性化唇形动画视频生成模型，可以准确地生成任意身份的唇形同步视频，且可用于增强未见面孔的特征。

    

    本文提出了StyleLipSync，这是一种基于风格的个性化唇形动画视频生成模型，它可以从任意音频生成无关身份的唇形同步视频。为了生成任意身份的视频，我们利用了预培训的StyleGAN的语义丰富潜空间中的表达性唇部先验知识，并通过线性变换设计视频一致性。与以往的唇形同步方法不同，我们引入了姿态感知遮罩，通过逐帧利用三维参数化网格预测器动态定位遮罩，提高了帧间自然度。此外，我们还提出了一种几乎不需要数据的唇形同步适应方法，通过引入同步正则化器来保留唇形同步泛化能力，同时增强人物特定的视觉信息。广泛的实验表明，我们的模型可以生成准确的唇形同步视频，甚至可以在零样本设置下增强未见面孔的特征。

    In this paper, we present StyleLipSync, a style-based personalized lip-sync video generative model that can generate identity-agnostic lip-synchronizing video from arbitrary audio. To generate a video of arbitrary identities, we leverage expressive lip prior from the semantically rich latent space of a pre-trained StyleGAN, where we can also design a video consistency with a linear transformation. In contrast to the previous lip-sync methods, we introduce pose-aware masking that dynamically locates the mask to improve the naturalness over frames by utilizing a 3D parametric mesh predictor frame by frame. Moreover, we propose a few-shot lip-sync adaptation method for an arbitrary person by introducing a sync regularizer that preserves lips-sync generalization while enhancing the person-specific visual information. Extensive experiments demonstrate that our model can generate accurate lip-sync videos even with the zero-shot setting and enhance characteristics of an unseen face using a fe
    
[^39]: 转移学习艺术：一种自适应和稳健的管道

    The ART of Transfer Learning: An Adaptive and Robust Pipeline. (arXiv:2305.00520v1 [stat.ML])

    [http://arxiv.org/abs/2305.00520](http://arxiv.org/abs/2305.00520)

    本文提出了自适应稳健转移学习（ART）管道，使用通用机器学习算法实现转移学习，建立了非渐近学习理论，同时防止负面转移，并演示了它在回归、分类和稀疏学习上的良好性能。

    

    转移学习是利用辅助数据资源来提高主要任务性能的重要工具。在本文中，我们提出自适应稳健转移学习（ART），一种使用通用机器学习算法进行转移学习的灵活管道。我们建立了ART的非渐近学习理论，为实现自适应转移并防止负面转移提供了可证明的理论保证。此外，我们介绍了一种ART集成聚合机制，用于在考虑多个候选算法时产生单个最终模型。通过回归、分类和稀疏学习的广泛实证研究，我们展示了ART的良好性能。我们进一步提出了一个涉及死亡率研究的真实数据分析。

    Transfer learning is an essential tool for improving the performance of primary tasks by leveraging information from auxiliary data resources. In this work, we propose Adaptive Robust Transfer Learning (ART), a flexible pipeline of performing transfer learning with generic machine learning algorithms. We establish the non-asymptotic learning theory of ART, providing a provable theoretical guarantee for achieving adaptive transfer while preventing negative transfer. Additionally, we introduce an ART-integrated-aggregating machine that produces a single final model when multiple candidate algorithms are considered. We demonstrate the promising performance of ART through extensive empirical studies on regression, classification, and sparse learning. We further present a real-data analysis for a mortality study.
    
[^40]: 通向自由计算架构: 关于深度学习生成元宇宙虚拟建筑的综合调研

    Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])

    [http://arxiv.org/abs/2305.00510](http://arxiv.org/abs/2305.00510)

    本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。

    

    利用深度学习的3D形状生成技术正在受到计算机视觉和建筑设计两方的越来越多的关注。本综合调查旨在调查和比较当前最新的基于深度生成模型（DGMs）的3D对象生成方法，包括生成对抗网络（GANs）、变分自动编码器（VAEs）、3D感知图像和扩散模型。我们调查了187篇文章(占2018-2022年间发表文章的80.7%)，以回顾在虚拟环境下建筑生成可能性的领域，限于建筑形式。我们提供了建筑研究、虚拟环境和相关技术方法的概述，接着回顾了离散体素生成、由2D图像生成的3D模型以及条件参数的最近趋势。我们强调了3D生成和参数化控制中尚未充分探讨的问题值得进一步研究。此外，我们推测包括生成多样性、新型输出和嵌入式构建等四个研究议程可能会成为未来研究的重点。

    3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
    
[^41]: 学习成就结构来在有稀疏奖励的领域进行结构化探索

    Learning Achievement Structure for Structured Exploration in Domains with Sparse Reward. (arXiv:2305.00508v1 [cs.LG])

    [http://arxiv.org/abs/2305.00508](http://arxiv.org/abs/2305.00508)

    本文提出了SEA算法，可在成就型环境中进行探索任务。SEA首先学习已知成就的表示和依赖关系图，然后通过构建控制器在线探索新成就。实验证明SEA能够准确地恢复成就结构并改善在一些复杂领域中的探索性能。

    

    本文提出了一种名为SEA的多阶段强化学习算法，旨在为基于成就的环境设计，即具有内部成就集的特定类型环境。SEA首先使用离线数据，使用确定性损失函数学习已知成就的表示，然后使用启发式算法恢复学习成就的依赖关系图，最后通过使用恢复的依赖关系图构建控制器，在线与环境交互学习掌握已知成就并探索新成就的策略。我们通过实验证明，SEA可以准确地恢复成就结构，并改善在像图像这样具有高维观察值的大型生成领域（如Crafter）中的探索能力。

    We propose Structured Exploration with Achievements (SEA), a multi-stage reinforcement learning algorithm designed for achievement-based environments, a particular type of environment with an internal achievement set. SEA first uses offline data to learn a representation of the known achievements with a determinant loss function, then recovers the dependency graph of the learned achievements with a heuristic algorithm, and finally interacts with the environment online to learn policies that master known achievements and explore new ones with a controller built with the recovered dependency graph. We empirically demonstrate that SEA can recover the achievement structure accurately and improve exploration in hard domains such as Crafter that are procedurally generated with high-dimensional observations like images.
    
[^42]: 域不可知傅里叶神经算子

    Domain Agnostic Fourier Neural Operators. (arXiv:2305.00478v1 [cs.LG])

    [http://arxiv.org/abs/2305.00478](http://arxiv.org/abs/2305.00478)

    介绍了一种新的神经算子架构 DAFNO，可以学习带有不规则几何和不断变化的域的代理。通过将平滑化的特征函数纳入 FNOs 的积分层架构中，并利用 FFT 来实现快速计算，以明确的方式将几何信息编码到架构中，DAFNO 相对于基线神经算子模型具有最先进的精度。

    

    傅里叶神经算子（FNOs）能够学习在函数空间之间高度非线性的映射，最近已成为学习复杂物理系统响应的热门工具。然而，为了实现良好的精度和效率，FNOs 依赖于快速傅里叶变换 (FFT)，该变换仅限于矩形域上的建模问题。为了消除这样的限制，允许 FFT 在不规则几何以及拓扑变化中使用，我们引入了域不可知傅里叶神经算子 (DAFNO)，一种用于学习带有不规则几何和不断变化的域的代理的新的神经算子架构。关键思想是将平滑化的特征函数纳入 FNOs 的积分层架构中，并利用 FFT 来实现快速计算，以便以明确的方式将几何信息编码到架构中。在我们的实证评估中，DAFNO 相对于基线神经算子模型具有最先进的精度。

    Fourier neural operators (FNOs) can learn highly nonlinear mappings between function spaces, and have recently become a popular tool for learning responses of complex physical systems. However, to achieve good accuracy and efficiency, FNOs rely on the Fast Fourier transform (FFT), which is restricted to modeling problems on rectangular domains. To lift such a restriction and permit FFT on irregular geometries as well as topology changes, we introduce domain agnostic Fourier neural operator (DAFNO), a novel neural operator architecture for learning surrogates with irregular geometries and evolving domains. The key idea is to incorporate a smoothed characteristic function in the integral layer architecture of FNOs, and leverage FFT to achieve rapid computations, in such a way that the geometric information is explicitly encoded in the architecture. In our empirical evaluation, DAFNO has achieved state-of-the-art accuracy as compared to baseline neural operator models on two benchmark dat
    
[^43]: 深度强化学习的后验采样

    Posterior Sampling for Deep Reinforcement Learning. (arXiv:2305.00477v1 [cs.LG])

    [http://arxiv.org/abs/2305.00477](http://arxiv.org/abs/2305.00477)

    本文提出了用于深度强化学习的后验采样算法PSDRL，结合了高效的不确定性量化和特殊设计的持续规划算法，使其在提高样本效率的同时显著优于之前的尝试。

    

    尽管深度强化学习算法取得了显著的成功，但样本效率仍然较低：它们需要大量的试错来找到好的策略。基于模型的算法通过构建可以用于规划的环境模型来提高样本效率。后验采样强化学习是这样一种基于模型的算法，在表格设置中由于其性能而引起了广泛的兴趣。本文介绍了用于深度强化学习的后验采样（PSDRL），这是第一个真正可扩展的后验采样强化学习的近似方法，保留了其基于模型的本质特征。PSDRL将潜在状态空间模型上的高效不确定性量化与基于值函数逼近的特殊设计的持续规划算法相结合。对Atari基准测试的广泛实验表明，PSDRL在提高样本效率的同时，显著优于以前的最先进尝试。

    Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being 
    
[^44]: 基于全局预测模型预测准确性的时间序列聚类方法

    Time series clustering based on prediction accuracy of global forecasting models. (arXiv:2305.00473v1 [stat.ML])

    [http://arxiv.org/abs/2305.00473](http://arxiv.org/abs/2305.00473)

    本论文提出了一种基于预测准确性的时间序列聚类新方法，可以用于选择时间序列数据库中的聚类数，并且比传统方法更好。

    

    本文提出了一种基于模型的时间序列聚类新方法。该方法依赖于两个迭代步骤：（i）通过考虑每个簇所涉及的时序，并由 pooling（集中）拟合 K 个全局预测模型；（ii）每个序列被分配到产生最佳预测的模型关联的组。与文献中大多数技术不同，该方法将预测准确性作为构建聚类分区的主要元素，其中包含共同最小化总体预测误差的组。因此，该方法导致了一个新的聚类范式，其中聚类解的质量是通过其预测能力来衡量的。此外，该过程还引起了在时间序列数据库中选择聚类数的有效机制，并且可以与任何回归模型类结合使用。广泛的模拟研究表明，该方法是要比传统方法更加好的选择。

    In this paper, a novel method to perform model-based clustering of time series is proposed. The procedure relies on two iterative steps: (i) K global forecasting models are fitted via pooling by considering the series pertaining to each cluster and (ii) each series is assigned to the group associated with the model producing the best forecasts according to a particular criterion. Unlike most techniques proposed in the literature, the method considers the predictive accuracy as the main element for constructing the clustering partition, which contains groups jointly minimizing the overall forecasting error. Thus, the approach leads to a new clustering paradigm where the quality of the clustering solution is measured in terms of its predictive capability. In addition, the procedure gives rise to an effective mechanism for selecting the number of clusters in a time series database and can be used in combination with any class of regression model. An extensive simulation study shows that o
    
[^45]: 量子计算中用于ReLU网络鲁棒性的高效MILP分解

    Efficient MILP Decomposition in Quantum Computing for ReLU Network Robustness. (arXiv:2305.00472v1 [quant-ph])

    [http://arxiv.org/abs/2305.00472](http://arxiv.org/abs/2305.00472)

    本研究提出了一种用于ReLU网络鲁棒性的新型MILP量子分解方法，尤其针对解决量子位可用性、噪声和误差限制，可实现更高的成功率和更高的效率与精确度，对量子计算技术的发展有重要意义。

    

    新兴的量子计算技术，如嘈杂中间规模量子（NISQ）设备，为解决数学优化问题提供了潜在的进展。然而，量子位的可用性、噪声和误差的限制对实际实施构成了挑战。在本研究中，我们研究了两种MILP分解方法，旨在减少原始问题大小并更高效地利用可用的NISQ设备。我们重点关注将原始问题分解成更小的子问题，然后使用组合的量子-经典硬件方法迭代地解决这些子问题。我们对Benders和Dantzig-Wolfe方法的MILP分解进行了详细分析。在我们的分析中，我们表明，在最坏情况下，解决Benders所需的量子比特数呈指数增长，而Dantzig-Wolfe的量子比特数保持不变。此外，我们利用Dantzig-Wolfe分解对证明ReLU网络对抗攻击的鲁棒性时的使用案例。我们的实验结果表明，所提出的量子MILP分解方法可以实现更高的成功率，并在效率和准确性方面优于经典优化方法。

    Emerging quantum computing technologies, such as Noisy Intermediate-Scale Quantum (NISQ) devices, offer potential advancements in solving mathematical optimization problems. However, limitations in qubit availability, noise, and errors pose challenges for practical implementation. In this study, we examine two decomposition methods for Mixed-Integer Linear Programming (MILP) designed to reduce the original problem size and utilize available NISQ devices more efficiently. We concentrate on breaking down the original problem into smaller subproblems, which are then solved iteratively using a combined quantum-classical hardware approach. We conduct a detailed analysis for the decomposition of MILP with Benders and Dantzig-Wolfe methods. In our analysis, we show that the number of qubits required to solve Benders is exponentially large in the worst-case, while remains constant for Dantzig-Wolfe. Additionally, we leverage Dantzig-Wolfe decomposition on the use-case of certifying the robustn
    
[^46]: 依赖于边权的超图：基于1-Laplacian的谱聚类

    Hypergraphs with Edge-Dependent Vertex Weights: Spectral Clustering based on the 1-Laplacian. (arXiv:2305.00462v1 [cs.LG])

    [http://arxiv.org/abs/2305.00462](http://arxiv.org/abs/2305.00462)

    本文提出了一个超图聚类的框架，并使用依赖于边权的顶点权重来提高超图模型的表现力。根据理论分析，利用与1-Laplacian的第二小特征值相关的特征向量能够达到比传统方法更高的聚类精度。在实际数据集上的实验证明了这种方法的有效性。

    

    本论文提出了一个灵活的框架，用于定义1-Laplacian超图，其中包括依赖于边的顶点权重。这些权重能够反映超边内顶点的不同重要性，从而使得超图模型比同质超图具有更高的表现力。本文利用与超图1-Laplacian第二小特征值相关的特征向量对顶点进行聚类。从基于规范化的Cheeger割的理论角度来看，这种方法的聚类精度应比传统拉普拉斯方法更高。实验证明，我们使用实际数据集证实了该谱聚类方法的有效性。此外，我们还展示了在我们框架内一个特殊情况下，相应的超图1-Laplacian等价于一个相关图的1-Laplacian，该图的特征向量能够更有效地计算，从而简化了计算。

    We propose a flexible framework for defining the 1-Laplacian of a hypergraph that incorporates edge-dependent vertex weights. These weights are able to reflect varying importance of vertices within a hyperedge, thus conferring the hypergraph model higher expressivity than homogeneous hypergraphs. We then utilize the eigenvector associated with the second smallest eigenvalue of the hypergraph 1-Laplacian to cluster the vertices. From a theoretical standpoint based on an adequately defined normalized Cheeger cut, this procedure is expected to achieve higher clustering accuracy than that based on the traditional Laplacian. Indeed, we confirm that this is the case using real-world datasets to demonstrate the effectiveness of the proposed spectral clustering approach. Moreover, we show that for a special case within our framework, the corresponding hypergraph 1-Laplacian is equivalent to the 1-Laplacian of a related graph, whose eigenvectors can be computed more efficiently, facilitating th
    
[^47]: 机器学习算法及特征提取技术的可预测性研究

    Predictability of Machine Learning Algorithms and Related Feature Extraction Techniques. (arXiv:2305.00449v1 [cs.LG])

    [http://arxiv.org/abs/2305.00449](http://arxiv.org/abs/2305.00449)

    本论文通过矩阵因式分解设计了一个预测系统来预测特定数据集上特定模型的分类准确度，同时研究了随机森林、XGBoost和MLP等三种机器学习算法的性能预测，并得出了三个结论。

    

    本论文基于矩阵因式分解设计了一个预测系统，可以预测特定数据集上特定模型的分类准确度。我们收集了50多个数据集，并对三个基本的机器学习算法——随机森林、XGBoost和多层感知器（MLP）的性能预测进行了全面的实证研究。具体来说，我们得出了以下结论：1.使用粗调模型可以预测优化模型的性能。2.使用特征提取技术可以预测MLP的性能。3. 使用隐式反馈可预测模型的性能。

    This thesis designs a prediction system based on matrix factorization to predict the classification accuracy of a specific model on a particular dataset. In this thesis, we conduct comprehensive empirical research on more than fifty datasets that we collected from the openml website. We study the performance prediction of three fundamental machine learning algorithms, namely, random forest, XGBoost, and MultiLayer Perceptron(MLP). In particular, we obtain the following results: 1. Predictability of fine-tuned models using coarse-tuned variants. 2. Predictability of MLP using feature extraction techniques. 3. Predict model performance using implicit feedback.
    
[^48]: 基于局部任务相似性启发的神经元创建和移除的多任务结构学习

    Multi-Task Structural Learning using Local Task Similarity induced Neuron Creation and Removal. (arXiv:2305.00441v1 [cs.LG])

    [http://arxiv.org/abs/2305.00441](http://arxiv.org/abs/2305.00441)

    本文提出了一种名为“多任务结构学习（MTSL）”的方法，可以同时学习多任务架构及其参数，其主要贡献在于将局部任务相似性纳入神经元创建和移除，从而提高了神经网络的泛化能力。

    

    多任务学习具有通过最大化任务间正向转移来提高泛化能力、减少任务干扰的潜力。然而，手动设计的网络架构在整个训练过程中保持静态，这限制了其发挥潜力。相比之下，大脑中的学习是通过结构性变化和突触强度变化相互作用实现的。因此，我们提出了“多任务结构学习（MTSL）”，它可以同时学习多任务架构及其参数。MTSL从每个任务的相同单任务网络开始，交替进行任务学习和结构学习。在任务学习阶段，每个网络专门处理相应的任务。在每个结构学习阶段中，从最早的层开始，局部相似的任务层首先将其知识传输到新创建的组层，然后再将其删除。然后MTSL在相应的组层中使用它们。

    Multi-task learning has the potential to improve generalization by maximizing positive transfer between tasks while reducing task interference. Fully achieving this potential is hindered by manually designed architectures that remain static throughout training. On the contrary, learning in the brain occurs through structural changes that are in tandem with changes in synaptic strength. Thus, we propose \textit{Multi-Task Structural Learning (MTSL)} that simultaneously learns the multi-task architecture and its parameters. MTSL begins with an identical single-task network for each task and alternates between a task-learning phase and a structural-learning phase. In the task learning phase, each network specializes in the corresponding task. In each of the structural learning phases, starting from the earliest layer, locally similar task layers first transfer their knowledge to a newly created group layer before being removed. MTSL then uses the group layer in place of the corresponding 
    
[^49]: META-SMGO-$\Delta$: 相似性作为黑箱优化先验知识的应用

    META-SMGO-$\Delta$: similarity as a prior in black-box optimization. (arXiv:2305.00438v1 [math.OC])

    [http://arxiv.org/abs/2305.00438](http://arxiv.org/abs/2305.00438)

    META-SMGO-$\Delta$ 通过将相似问题的先验知识应用到优化过程中来提高求解相似问题的效率。

    

    实际中全局优化问题常涉及相似问题重复求解。本文通过提供严格的相似性定义，将 META-learning 理论应用到 SMGO-$\Delta`$，一个近期提出的全局优化方法中，从类似过去经验中获取先验知识，以高效求解新的（相似）问题。通过基准数值实验，我们展示了基准算法 META-extension 的实际好处，同时提供其性能的理论界限。

    When solving global optimization problems in practice, one often ends up repeatedly solving problems that are similar to each others. By providing a rigorous definition of similarity, in this work we propose to incorporate the META-learning rationale into SMGO-$\Delta$, a global optimization approach recently proposed in the literature, to exploit priors obtained from similar past experience to efficiently solve new (similar) problems. Through a benchmark numerical example we show the practical benefits of our META-extension of the baseline algorithm, while providing theoretical bounds on its performance.
    
[^50]: 乐器自动转录中的知识转移

    Transfer of knowledge among instruments in automatic music transcription. (arXiv:2305.00426v1 [cs.SD])

    [http://arxiv.org/abs/2305.00426](http://arxiv.org/abs/2305.00426)

    本研究展示了如何利用软件合成器产生的合成音频数据来培训通用模型，为进一步的转移学习提供了良好的基础。研究结果表明，用合成数据进行培训可以成为预训练通用模型的良好基础，其中转录任务并不仅限于一个乐器。

    

    自动音乐转录（AMT）是音乐信息检索领域中最具挑战性的任务之一。它的过程是将音乐的音频录音转换为包含有关音符、和弦和节奏信息的符号表示。当前该领域的研究集中在开发基于变压器架构的新模型或使用半监督训练方法，这些方法取得了出色的结果，但训练这些模型的计算成本巨大。本研究展示了如何利用软件合成器产生的易于生成的合成音频数据来训练通用模型。它是进一步转移学习的良好基础，以快速适应其他乐器的转录模型。实现的结果证明，使用合成数据进行训练可以成为预训练通用模型的良好基础，其中转录任务并不集中于一个乐器。

    Automatic music transcription (AMT) is one of the most challenging tasks in the music information retrieval domain. It is the process of converting an audio recording of music into a symbolic representation containing information about the notes, chords, and rhythm. Current research in this domain focuses on developing new models based on transformer architecture or using methods to perform semi-supervised training, which gives outstanding results, but the computational cost of training such models is enormous.  This work shows how to employ easily generated synthesized audio data produced by software synthesizers to train a universal model. It is a good base for further transfer learning to quickly adapt transcription model for other instruments. Achieved results prove that using synthesized data for training may be a good base for pretraining general-purpose models, where the task of transcription is not focused on one instrument.
    
[^51]: 探究大型语言模型在生成单元测试方面的有效性

    Exploring the Effectiveness of Large Language Models in Generating Unit Tests. (arXiv:2305.00418v1 [cs.SE])

    [http://arxiv.org/abs/2305.00418](http://arxiv.org/abs/2305.00418)

    本文研究了三种生成模型在单元测试生成方面的效果，并发现在不经过微调的情况下，它们的覆盖率较低且存在测试味道问题。

    

    代码生成模型可以通过使用代码注释、现有代码或两者的组合来生成代码。本文调查了三个生成模型（CodeGen、Codex和GPT-3.5）在没有微调的情况下是否能够成功用于生成单元测试的效果。研究中使用了两个基准（HumanEval和Evosuite SF110）来调查环境生成对单元测试生成过程的影响。我们根据编译率、测试正确性、覆盖率和测试味道来评估模型。我们发现，Codex模型在HumanEval数据集上取得了超过80%的覆盖率，但在EvoSuite SF110基准中没有一个模型超过2%的覆盖率。生成的测试还存在测试味道问题，比如重复的断言和空测试。

    A code generation model generates code by taking a prompt from a code comment, existing code, or a combination of both. Although code generation models (e.g., GitHub Copilot) are increasingly being adopted in practice, it is unclear whether they can successfully be used for unit test generation without fine-tuning. To fill this gap, we investigated how well three generative models (CodeGen, Codex, and GPT-3.5) can generate test cases. We used two benchmarks (HumanEval and Evosuite SF110) to investigate the context generation's effect in the unit test generation process. We evaluated the models based on compilation rates, test correctness, coverage, and test smells. We found that the Codex model achieved above 80% coverage for the HumanEval dataset, but no model had more than 2% coverage for the EvoSuite SF110 benchmark. The generated tests also suffered from test smells, such as Duplicated Asserts and Empty Tests.
    
[^52]: 有限状态不想静止多臂赌博机和Rollout策略的可索引性

    Indexability of Finite State Restless Multi-Armed Bandit and Rollout Policy. (arXiv:2305.00410v1 [cs.LG])

    [http://arxiv.org/abs/2305.00410](http://arxiv.org/abs/2305.00410)

    本文研究了有限状态不想静止多臂赌博机问题，提出了一种应用rollout策略的算法来解决问题，并且在单臂赌博机模型上展示了结构结果和可索引性。

    

    本文研究了有限状态不想静止多臂赌博机问题。决策者可以在每个时间步骤中选择M个臂中的任意一个，臂的播放产生基于动作的状态相关奖励，当臂没有被播放时，它还提供基于状态和动作的奖励。决策者的目标是最大化无限时间长度的折扣奖励。传统的不想静止赌博机方法是使用 Whittle 索引策略。在这种策略中，每个时间步骤播放具有最高指数的M个臂。我们将不想静止赌博机问题分离成分析松弛约束不想静止赌博机问题。然后通过拉格朗日松弛问题，将不想静止赌博机问题分离成N个单臂不想静止赌博机问题。我们分析了单臂不想静止赌博机。为了研究 Whittle 索引策略，我们在单臂赌博机模型上展示了结构结果。我们定义了可索引性，并在特定情况下展示了可索引性。我们提出了一种应用rollout策略的算法来解决问题。

    We consider finite state restless multi-armed bandit problem. The decision maker can act on M bandits out of N bandits in each time step. The play of arm (active arm) yields state dependent rewards based on action and when the arm is not played, it also provides rewards based on the state and action. The objective of the decision maker is to maximize the infinite horizon discounted reward. The classical approach to restless bandits is Whittle index policy. In such policy, the M arms with highest indices are played at each time step. Here, one decouples the restless bandits problem by analyzing relaxed constrained restless bandits problem. Then by Lagrangian relaxation problem, one decouples restless bandits problem into N single-armed restless bandit problems. We analyze the single-armed restless bandit. In order to study the Whittle index policy, we show structural results on the single armed bandit model. We define indexability and show indexability in special cases. We propose an al
    
[^53]: 通过逆向神经渲染对动态场景进行物体中心体素化

    Object-Centric Voxelization of Dynamic Scenes via Inverse Neural Rendering. (arXiv:2305.00393v1 [cs.CV])

    [http://arxiv.org/abs/2305.00393](http://arxiv.org/abs/2305.00393)

    本文提出了一个逆向神经渲染框架DynaVol，可以在多实体动态场景中学习时间变化的体积表示，通过维护一个时间依赖的3D格点和联合学习格点级局部动态、物体级全局动态和组合神经辐射场来增强物体中心场景体素化的时空一致性。

    

    在无监督的3D场景中理解世界的组成动态非常具有挑战性。现有的方法要么未能有效利用时间线索，要么忽略了场景分解的多视角一致性。本文提出了DynaVol，一种逆向神经渲染框架，为多实体（如物体）的动态场景学习时间变化的体积表示提供了一个学习方法。它的主要贡献有两个。首先，它维护一个时间依赖的3D格点，动态而灵活地将空间位置绑定到不同的实体，从而在代表性水平上鼓励信息的分离。其次，我们的方法在端到端架构中联合学习格点级局部动态、物体级全局动态和组合神经辐射场，从而增强了物体中心场景体素化的时空一致性。我们提出了一个两阶段的DynaVol训练方案，并在合成和真实世界数据集上验证了它的有效性。

    Understanding the compositional dynamics of the world in unsupervised 3D scenarios is challenging. Existing approaches either fail to make effective use of time cues or ignore the multi-view consistency of scene decomposition. In this paper, we propose DynaVol, an inverse neural rendering framework that provides a pilot study for learning time-varying volumetric representations for dynamic scenes with multiple entities (like objects). It has two main contributions. First, it maintains a time-dependent 3D grid, which dynamically and flexibly binds the spatial locations to different entities, thus encouraging the separation of information at a representational level. Second, our approach jointly learns grid-level local dynamics, object-level global dynamics, and the compositional neural radiance fields in an end-to-end architecture, thereby enhancing the spatiotemporal consistency of object-centric scene voxelization. We present a two-stage training scheme for DynaVol and validate its ef
    
[^54]: 蛋白质序列设计的重要性加权期望最大化方法

    Importance Weighted Expectation-Maximization for Protein Sequence Design. (arXiv:2305.00386v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.00386](http://arxiv.org/abs/2305.00386)

    本文提出了一种名为IsEM-Pro的方法，用于根据给定适应性标准生成蛋白质序列。在推理期间，从其潜在空间采样可以增加多样性，指导了探索高适应性区域。实验表明，相比先前最佳方法，IsEM-Pro的平均适应性得分至少高出55％，并生成了更多样化和新颖的蛋白质序列。

    

    在生物和化学领域，设计具有所需生物功能的蛋白质序列非常重要。最近的机器学习方法使用代理序列-功能模型替代昂贵的湿实验验证。本文提出了一种名为IsEM-Pro的方法，用于根据给定的适应性标准生成蛋白质序列。它是一个潜在的生成模型，并受到另外一个学习的马尔可夫随机场结构特征的增强。研究者使用蒙特卡罗期望最大化方法（MCEM）来学习这个模型。在推理期间，从其潜在空间采样可以增加多样性，而其MRF特征则指导了探索高适应性区域。在八项蛋白质序列设计任务中的实验表明，我们的IsEM-Pro的平均适应性得分至少比先前最佳方法高55％，并且生成了更多样化和新颖的蛋白质序列。

    Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.
    
[^55]: 基于HSIC瓶颈和对齐的双重HSIC实现连续学习

    DualHSIC: HSIC-Bottleneck and Alignment for Continual Learning. (arXiv:2305.00380v1 [cs.LG])

    [http://arxiv.org/abs/2305.00380](http://arxiv.org/abs/2305.00380)

    DualHSIC通过利用跨任务关系，提高了基于重新学习方法的连续学习性能。

    

    基于重新学习的方法是连续学习的主要方法，通过保持小型固定大小的缓冲区来控制灾难性遗忘问题。尽管大多数基于重新学习的方法研究如何有效地利用缓冲区中过去数据中的知识，但很少注意到关键的任务特定和任务不变知识的跨任务关系。通过适当地利用跨任务关系，我们提出了一种名为DualHSIC的新的连续学习方法，以简单而有效的方式提高现有基于重新学习的方法的性能。DualHSIC由两个互补的组件组成，源自所谓的希尔伯特-施密特独立准则（HSIC）：HSIC-Bottleneck for Rehearsal (HBR)减少跨任务干扰，HSIC Alignment (HA)促进任务不变知识共享。广泛的实验表明，DualHSIC可以无缝地插入现有的基于重新学习的方法进行连续学习，并显著提高它们的性能。

    Rehearsal-based approaches are a mainstay of continual learning (CL). They mitigate the catastrophic forgetting problem by maintaining a small fixed-size buffer with a subset of data from past tasks. While most rehearsal-based approaches study how to effectively exploit the knowledge from the buffered past data, little attention is paid to the inter-task relationships with the critical task-specific and task-invariant knowledge. By appropriately leveraging inter-task relationships, we propose a novel CL method named DualHSIC to boost the performance of existing rehearsal-based methods in a simple yet effective way. DualHSIC consists of two complementary components that stem from the so-called Hilbert Schmidt independence criterion (HSIC): HSIC-Bottleneck for Rehearsal (HBR) lessens the inter-task interference and HSIC Alignment (HA) promotes task-invariant knowledge sharing. Extensive experiments show that DualHSIC can be seamlessly plugged into existing rehearsal-based methods for con
    
[^56]: 通过对抗性不变正则化增强对抗性对比学习

    Enhancing Adversarial Contrastive Learning via Adversarial Invariant Regularization. (arXiv:2305.00374v1 [cs.LG])

    [http://arxiv.org/abs/2305.00374](http://arxiv.org/abs/2305.00374)

    本文提出了一种对抗性不变正则化方法（AIR）来强制对抗性对比学习（ACL）的学习表示呈现样式独立性，并用加权SIR和AIR实现ACL的鲁棒性增强。实验证实，该方法在各种对抗攻击和常见污染下均显著提高ACL的鲁棒性，并在多个基准测试中实现了最先进的性能。

    

    对抗性对比学习(ACL)无需标签，将对抗性数据与标准对比学习(SCL)相结合，输出一个具有鲁棒性的表示，可泛化且抵抗对抗性攻击和常见污染。表示的样式独立属性已经被证明有助于提高鲁棒性的转移。标准不变正则化(SIR)已经被提出，使SCL通过学习的表示不受样式因素的影响。然而，如何通过ACL获得具有样式独立性质的鲁棒表示仍然不清楚。为此，我们利用因果推理技术，提出了一种对抗性不变正则化(AIR)，强制通过ACL学习到的鲁棒表示具有样式独立性。然后，我们使用不变正则化(IR)增强ACL，它是SIR和AIR的加权总和。理论上，我们证明AIR通过防止模型依赖样式因素来获得高对比分数，隐式地促进了ACL的鲁棒性。实验上，我们证明了我们提出的方法在各种对抗攻击和常见污染下显著提高了ACL的鲁棒性，并在多个基准测试中实现了最先进的性能。

    Adversarial contrastive learning (ACL), without requiring labels, incorporates adversarial data with standard contrastive learning (SCL) and outputs a robust representation which is generalizable and resistant to adversarial attacks and common corruptions. The style-independence property of representations has been validated to be beneficial in improving robustness transferability. Standard invariant regularization (SIR) has been proposed to make the learned representations via SCL to be independent of the style factors. However, how to equip robust representations learned via ACL with the style-independence property is still unclear so far. To this end, we leverage the technique of causal reasoning to propose an adversarial invariant regularization (AIR) that enforces robust representations learned via ACL to be style-independent. Then, we enhance ACL using invariant regularization (IR), which is a weighted sum of SIR and AIR. Theoretically, we show that AIR implicitly encourages the 
    
[^57]: S2abEL：一份用于科学表格实体链接的数据集

    S2abEL: A Dataset for Entity Linking from Scientific Tables. (arXiv:2305.00366v1 [cs.CL])

    [http://arxiv.org/abs/2305.00366](http://arxiv.org/abs/2305.00366)

    该论文提供了第一个专注于科学表格的 EL 数据集 S2abEL，用于实体链接任务。由于科学知识库的不完整性和语境影响，科学表格上的 EL 具有挑战性，该数据集专注于机器学习结果表中的 EL，包含手工标记的单元格类型、属性和实体链接，并引入了一种优于其他方法的神经基线方法。

    

    实体链接（EL）是将文本提及链接到知识库中相应条目的任务，这对于许多知识密集型的自然语言处理应用来说是至关重要的。当应用于科学论文中的表格时，EL是实现大规模科学知识库的一步，这可以实现先进的科学问答和分析。我们提供了第一个针对科学表格中的EL的数据集。科学表格的EL尤其具有挑战性，因为科学知识库可能非常不完整，并且通常需要理解论文中的文本以及表格的上下文来消除歧义。我们的数据集S2abEL专注于机器学习结果表中的EL，并包括来自PaperswithCode分类法的8,429个单元格的手工标记的单元格类型、来源属性和实体链接。我们引入了一种针对科学表格的神经基线方法，该方法包含许多知识库之外提及的实体，并显示它明显优于其他方法。

    Entity linking (EL) is the task of linking a textual mention to its corresponding entry in a knowledge base, and is critical for many knowledge-intensive NLP applications. When applied to tables in scientific papers, EL is a step toward large-scale scientific knowledge bases that could enable advanced scientific question answering and analytics. We present the first dataset for EL in scientific tables. EL for scientific tables is especially challenging because scientific knowledge bases can be very incomplete, and disambiguating table mentions typically requires understanding the papers's tet in addition to the table. Our dataset, S2abEL, focuses on EL in machine learning results tables and includes hand-labeled cell types, attributed sources, and entity links from the PaperswithCode taxonomy for 8,429 cells from 732 tables. We introduce a neural baseline method designed for EL on scientific tables containing many out-of-knowledge-base mentions, and show that it significantly outperfor
    
[^58]: ReLBOT：一种转移学习方法以最小化智能建筑中强化学习风险

    ReLBOT: A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Smart Buildings. (arXiv:2305.00365v1 [cs.LG])

    [http://arxiv.org/abs/2305.00365](http://arxiv.org/abs/2305.00365)

    ReLBOT使用转移学习和深度RL技术来从现有的智能建筑中传递优化参数到新的建筑中，以减少强化学习代理引起的初始不适，有效降低了风险，并且实现了热身期时长6.2倍的提高和预测方差的132倍提高。

    

    智能建筑旨在通过应用人工智能算法来优化能源消耗。当智能建筑投入使用时，没有历史数据可用于训练这些算法。在线强化学习（RL）算法显示出重要的前景，但它们的部署存在重大风险，因为当RL代理最初探索其行动空间时，它可能会给建筑居民带来重大不适。在本文中，我们提出了一种名为ReLBOT的新技术，它使用转移学习结合深度RL，从现有的优化智能建筑中传递知识到新投入使用的建筑中，以减少强化学习代理的热身期对建筑物的不利影响。我们证明取得了可观的成果，热身期的持续时间可提高6.2倍，并且预测方差可提高132倍。

    Smart buildings aim to optimize energy consumption by applying artificial intelligent algorithms. When a smart building is commissioned there is no historical data that could be used to train these algorithms. On-line Reinforcement Learning (RL) algorithms have shown significant promise, but their deployment carries a significant risk, because as the RL agent initially explores its action space it could cause significant discomfort to the building residents. In this paper we present ReLBOT, a new technique that uses transfer learning in conjunction with deep RL to transfer knowledge from an existing, optimized smart building, to the newly commissioning building, to reduce the adverse impact of the reinforcement learning agent's warm-up period. We demonstrate improvements of up to 6.2 times in the duration, and up to 132 times in prediction variance for the reinforcement learning agent's warm-up period.
    
[^59]: 电力储能系统套利的电力价格预测：一种以决策为中心的方法

    Electricity Price Prediction for Energy Storage System Arbitrage: A Decision-focused Approach. (arXiv:2305.00362v1 [cs.LG])

    [http://arxiv.org/abs/2305.00362](http://arxiv.org/abs/2305.00362)

    该论文提出了一种以决策为中心的电力价格预测方法，用于电力储能系统套利，通过遗憾度量预测价值下的实际决策和在实际价值下的最优决策之间的差异，学习提高预测和决策的准确性。

    

    电力价格预测在能量储存系统（ESS）管理中起着至关重要的作用。目前的预测模型主要关注减少预测误差，但忽略了它们对下游决策的影响。因此，本文提出了一种以决策为中心的电力价格预测方法，用于ESS套利，以弥合下游优化模型和预测模型之间的差距。该决策焦点方法旨在利用下游套利模型来训练预测模型。它通过遗憾度量预测价值下的实际决策和在实际价值下的最优决策之间的差异（即决策误差），将其转化为可处理的代理遗憾，并据此推导出预测模型的渐变。基于预测误差和决策误差，本文提出了混合损失和相应的随机梯度下降学习方法，以学习预测模型来提高预测和决策的准确性。

    Electricity price prediction plays a vital role in energy storage system (ESS) management. Current prediction models focus on reducing prediction errors but overlook their impact on downstream decision-making. So this paper proposes a decision-focused electricity price prediction approach for ESS arbitrage to bridge the gap from the downstream optimization model to the prediction model. The decision-focused approach aims at utilizing the downstream arbitrage model for training prediction models. It measures the difference between actual decisions under the predicted price and oracle decisions under the true price, i.e., decision error, by regret, transforms it into the tractable surrogate regret, and then derives the gradients to predicted price for training prediction models. Based on the prediction and decision errors, this paper proposes the hybrid loss and corresponding stochastic gradient descent learning method to learn prediction models for prediction and decision accuracy. The 
    
[^60]: POUF: 面向提示的无监督大型预训练模型微调

    POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models. (arXiv:2305.00350v1 [cs.LG])

    [http://arxiv.org/abs/2305.00350](http://arxiv.org/abs/2305.00350)

    本文提出了一种基于提示的无监督微调框架，可以在未标记的目标数据上微调大型预训练模型以适应下游任务，实验结果表明该方法在图像分类、情感分析和自然语言推理等任务中表现更好。

    

    通过提示，大规模预训练模型在近年来变得更加表现出色和强大。虽然这些大型模型具有零-shot 能力，但通常仍需要有标签的数据来适应下游任务。为了克服这个关键限制，我们提出了一种无监督微调框架，直接在未标记的目标数据上微调模型或提示。我们演示如何将该方法应用于语言增强的视觉和掩蔽语言模型，通过对齐从提示和目标数据中提取的离散分布来实现。为了验证我们方法的适用性，我们对图像分类、情感分析和自然语言推理任务进行了广泛的实验。在 13 个与图像相关的任务和 15 个与语言相关的任务中，该方法均比基线表现更好。

    Through prompting, large-scale pre-trained models have become more expressive and powerful, gaining significant attention in recent years. Though these big models have zero-shot capabilities, in general, labeled data are still required to adapt them to downstream tasks. To overcome this critical limitation, we propose an unsupervised fine-tuning framework to directly fine-tune the model or prompt on the unlabeled target data. We demonstrate how to apply our method to both language-augmented vision and masked-language models by aligning the discrete distributions extracted from the prompts and target data. To verify our approach's applicability, we conduct extensive experiments on image classification, sentiment analysis, and natural language inference tasks. Across 13 image-related tasks and 15 language-related ones, the proposed approach achieves consistent improvements over the baselines.
    
[^61]: 用稀疏矩阵表示加性高斯过程

    Representing Additive Gaussian Processes by Sparse Matrices. (arXiv:2305.00324v1 [stat.ML])

    [http://arxiv.org/abs/2305.00324](http://arxiv.org/abs/2305.00324)

    本研究展示了对于加性Matérn高斯过程，通过稀疏矩阵和向量的公式可以有效地计算后验均值、后验方差、对数似然和梯度。

    

    在广义相加模型中，加性Matérn高斯过程是最受欢迎的可扩展高维问题之一。由于它们的加性结构和随机微分方程表示，基于回归的算法可以将计算后验均值的时间复杂度从$O（n^3）$减少到$O（nlogn）$时间，其中$n$是数据大小。但是，将这些算法推广到有效计算后验方差和最大对数似然仍然是一个未解决的问题。在本研究中，我们展示了对于加性Matérn高斯过程，不仅后验均值，而且后验方差、对数似然和这三个函数的梯度可以用仅涉及稀疏矩阵和稀疏向量的公式表示。我们展示了如何使用这些稀疏公式来推广回归算法，以有效计算这三个函数的后验均值、后验方差、对数似然和梯度。

    Among generalized additive models, additive Mat\'ern Gaussian Processes (GPs) are one of the most popular for scalable high-dimensional problems. Thanks to their additive structure and stochastic differential equation representation, back-fitting-based algorithms can reduce the time complexity of computing the posterior mean from $O(n^3)$ to $O(n\log n)$ time where $n$ is the data size. However, generalizing these algorithms to efficiently compute the posterior variance and maximum log-likelihood remains an open problem. In this study, we demonstrate that for Additive Mat\'ern GPs, not only the posterior mean, but also the posterior variance, log-likelihood, and gradient of these three functions can be represented by formulas involving only sparse matrices and sparse vectors. We show how to use these sparse formulas to generalize back-fitting-based algorithms to efficiently compute the posterior mean, posterior variance, log-likelihood, and gradient of these three functions for additiv
    
[^62]: 非线性函数的$L_\infty$恢复：高斯随机场的多项式样本复杂度界限

    Toward $L_\infty$-recovery of Nonlinear Functions: A Polynomial Sample Complexity Bound for Gaussian Random Fields. (arXiv:2305.00322v1 [cs.LG])

    [http://arxiv.org/abs/2305.00322](http://arxiv.org/abs/2305.00322)

    本文利用随机性证明了从高斯随机场中绘制的随机基础事实函数的$L_\infty$-recovery可以使用多项式样本具有多项式样本复杂度边界。

    

    许多机器学习应用需要学习一个在整个输入域上具有小最坏情况误差的函数，即$L_\infty$误差，而大多数现有的理论工作只保证平均误差的恢复，例如$L_2$误差。即使对于看似简单的函数类，如常数范数无限宽度双层神经网络，从多项式样本进行$L_\infty$恢复也是不可能的。本文通过利用基础事实中的随机性，超越不可能性结果的一些初始步骤。我们证明了从高斯随机场中绘制的随机基础事实函数的多项式样本复杂度界限。我们的关键技术创新是证明了高斯随机场中的$k$阶球谐函数分量不能是尖锐的，即它们的$L_\infty$/$L_2$比率高概率上限制为$O(d \sqrt{\ln k})$。相反，对于一般函数，$k$阶球谐函数的最坏$L_\infty$/$L_2$比率是无界的。我们的工作为分析富函数类别的多项式样本的$L_\infty$恢复的样本复杂度开辟了新的方向。

    Many machine learning applications require learning a function with a small worst-case error over the entire input domain, that is, the $L_\infty$-error, whereas most existing theoretical works only guarantee recovery in average errors such as the $L_2$-error. $L_\infty$-recovery from polynomial samples is even impossible for seemingly simple function classes such as constant-norm infinite-width two-layer neural nets. This paper makes some initial steps beyond the impossibility results by leveraging the randomness in the ground-truth functions. We prove a polynomial sample complexity bound for random ground-truth functions drawn from Gaussian random fields. Our key technical novelty is to prove that the degree-$k$ spherical harmonics components of a function from Gaussian random field cannot be spiky in that their $L_\infty$/$L_2$ ratios are upperbounded by $O(d \sqrt{\ln k})$ with high probability. In contrast, the worst-case $L_\infty$/$L_2$ ratio for degree-$k$ spherical harmonics i
    
[^63]: 在受污染的多模态数据下，用于真实世界监控的视觉-红外人员再识别的融合

    Fusion for Visual-Infrared Person ReID in Real-World Surveillance Using Corrupted Multimodal Data. (arXiv:2305.00320v1 [cs.CV])

    [http://arxiv.org/abs/2305.00320](http://arxiv.org/abs/2305.00320)

    本篇论文提出了一种名为MMSF的模型，能够保留模态特定的知识，提高受污染多模态图像的鲁棒性。同时，还采用了三种最先进的基于注意力的多模态融合模型，在V-I ReID中适应受污染多模态数据，动态平衡每种模态的重要性。

    

    可见光-红外人员再识别(V-I ReID)旨在匹配由分布式RGB和IR摄像机捕获的个体图像。由于V和I模态之间的显著差异，特别是在真实世界的条件下，图像受到模糊、噪声和天气等因素的干扰，该任务变得具有挑战性。事实上，最先进的V-I ReID模型不能利用受污染的模态信息来维持高水平的准确性。在本文中，我们提出了一种高效的多模态V-I ReID模型——名为多模态中间流融合(MMSF)，用于提高受污染多模态图像的鲁棒性，并针对在V-I ReID中出现的受污染多模态数据，适应了三种最先进的基于注意力的多模态融合模型，可动态平衡每种模态的重要性。近期已提出评估协议以评估ReID模型在具有挑战性的真实世界场景下的鲁棒性。

    Visible-infrared person re-identification (V-I ReID) seeks to match images of individuals captured over a distributed network of RGB and IR cameras. The task is challenging due to the significant differences between V and I modalities, especially under real-world conditions, where images are corrupted by, e.g, blur, noise, and weather. Indeed, state-of-art V-I ReID models cannot leverage corrupted modality information to sustain a high level of accuracy. In this paper, we propose an efficient model for multimodal V-I ReID -- named Multimodal Middle Stream Fusion (MMSF) -- that preserves modality-specific knowledge for improved robustness to corrupted multimodal images. In addition, three state-of-art attention-based multimodal fusion models are adapted to address corrupted multimodal data in V-I ReID, allowing to dynamically balance each modality importance. Recently, evaluation protocols have been proposed to assess the robustness of ReID models under challenging real-world scenarios.
    
[^64]: 学习如何用约束元最优输运重新排序

    Learning to Re-rank with Constrained Meta-Optimal Transport. (arXiv:2305.00319v1 [cs.LG])

    [http://arxiv.org/abs/2305.00319](http://arxiv.org/abs/2305.00319)

    本文提出了一种快速、轻量级的约束元最优输运算法(CMOT)，用于预测公平随机重新排序策略，并使用一种新的采样算法VARN-SAM，比Birkhoff-von-Neumann分解(BvND)更有效。实验结果显示，CMOT实现了公平重新排序的最先进性能，同时比其他方法更快且更可扩展。

    

    在搜索系统中，许多重新排序策略依赖于随机排名策略，编码为满足期望中所需排名约束的双重随机 (DS) 矩阵，例如曝光公平性 (FOE)。这些策略通常是两阶段管道: \emph{i)}离线重新排序策略构建步骤和 \emph{ii)}在线排名步骤。建立重新排序策略需要反复解决约束优化问题，每个发布的查询解决一次。因此，为任何新/未见过的查询重新计算优化过程是必要的。关于抽样，Birkhoff-von-Neumann 分解 (BvND) 是从任何基于DS的策略中抽取排名的首选方法。然而，在线计算BvND过于昂贵。因此，BvND作为采样解决方案具有内存占用量，它可以随着 $N$ 查询和 $n$ 文档的增长而增长，其时间复杂度是 $\gO(N\, n^2)$。本文提出了一种新颖的、快速的、轻量级的方法来预测公平随机重新排序策略: 约束元最优输运 (CMOT)。CMOT是一种在线元学习算法，可以同时解决所有离线优化问题，并创建一个低成本元模型，可以准确地预测任何新查询的重新排序策略。此外，CMOT使用一种新的采样算法，称为变分排名采样 (VARN-SAM)，它满足相同的排名约束，同时更具内存效率和更快的计算在线。实验评估显示CMOT在公平重新排序方面达到了最先进的性能，同时比其他方法更快、更可扩展。

    Many re-ranking strategies in search systems rely on stochastic ranking policies, encoded as Doubly-Stochastic (DS) matrices, that satisfy desired ranking constraints in expectation, e.g., Fairness of Exposure (FOE). These strategies are generally two-stage pipelines: \emph{i)} an offline re-ranking policy construction step and \emph{ii)} an online sampling of rankings step. Building a re-ranking policy requires repeatedly solving a constrained optimization problem, one for each issued query. Thus, it is necessary to recompute the optimization procedure for any new/unseen query. Regarding sampling, the Birkhoff-von-Neumann decomposition (BvND) is the favored approach to draw rankings from any DS-based policy. However, the BvND is too costly to compute online. Hence, the BvND as a sampling solution is memory-consuming as it can grow as $\gO(N\, n^2)$ for $N$ queries and $n$ documents.  This paper offers a novel, fast, lightweight way to predict fair stochastic re-ranking policies: Const
    
[^65]: 理想的不断学习者: 不会遗忘的代理

    The Ideal Continual Learner: An Agent That Never Forgets. (arXiv:2305.00316v1 [cs.LG])

    [http://arxiv.org/abs/2305.00316](http://arxiv.org/abs/2305.00316)

    本文提出了一个新的持续学习框架——理想的持续学习者（ICL），通过构建避免灾难性遗忘，统一了多个持续学习方法，并为这些方法的优缺点提供了新的理论见解。

    

    持续学习的目标是找到一个模型，解决按顺序呈现给学习者的多个学习任务。在这种情况下，一个主要的挑战是，当学习新任务时，学习者可能会忘记如何解决先前的任务，这种现象称为灾难性遗忘。为了应对这一挑战，已经提出了许多实用的方法，包括基于记忆、基于正则化和基于扩展的方法。然而，对这些方法的严格理论理解仍然是困难的。本文旨在通过提出一种新的持续学习框架——理想的持续学习者（ICL），从理论与实践之间的鸿沟中跨越过去，ICL的构建保证避免灾难性遗忘，统一了多个成熟的持续学习方法，并为这些方法的优缺点提供了新的理论见解。我们还为ICL推导出了泛化界限，这使我们能够在理论上量化如何控制模型的复杂度来减轻灾难性遗忘。

    The goal of continual learning is to find a model that solves multiple learning tasks which are presented sequentially to the learner. A key challenge in this setting is that the learner may forget how to solve a previous task when learning a new task, a phenomenon known as catastrophic forgetting. To address this challenge, many practical methods have been proposed, including memory-based, regularization-based, and expansion-based methods. However, a rigorous theoretical understanding of these methods remains elusive. This paper aims to bridge this gap between theory and practice by proposing a new continual learning framework called Ideal Continual Learner (ICL), which is guaranteed to avoid catastrophic forgetting by construction. We show that ICL unifies multiple well-established continual learning methods and gives new theoretical insights into the strengths and weaknesses of these methods. We also derive generalization bounds for ICL which allow us to theoretically quantify how r
    
[^66]: 在有限制的多目标联邦学习中优化隐私、效用和效率

    Optimizing Privacy, Utility and Efficiency in Constrained Multi-Objective Federated Learning. (arXiv:2305.00312v1 [cs.LG])

    [http://arxiv.org/abs/2305.00312](http://arxiv.org/abs/2305.00312)

    该论文提供了一种在有限制的多目标联邦学习中优化隐私、效用和效率的方法，开发了两种改进的算法来解决隐私泄露、效用损失和训练成本等三个主要目标，并在两个真实世界的数据集上进行了实验验证，优于现有方法。

    

    传统上，联邦学习旨在优化单个目标，通常是效用。然而，为了使联邦学习系统值得信赖，它需要同时满足多个/多个目标，例如最大化模型性能、最小化隐私泄露和训练成本，并对恶意攻击具有鲁棒性。多目标优化（MOO）旨在同时优化多个相互冲突的目标，非常适合解决值得信赖的联合学习（TFL）的优化问题。在本文中，我们将MOO和TFL统一起来，通过制定约束的多目标联合学习（CMOFL）问题来解决此问题。在这种制定下，现有的MOO算法可以直接适用于TFL。不同于现有的CMOFL作品专注于效用、效率、公平性和鲁棒性，我们考虑优化隐私泄露以及效用损失和训练成本，这是TFL系统的三个主要目标之一。我们开发了两种改进的CMOFL算法，它们返回一组平衡良好的模型，满足隐私、效用和效率。基于两个真实世界的数据集的实验表明，我们的方法在隐私、效用和效率之间的权衡方面优于现有方法。

    Conventionally, federated learning aims to optimize a single objective, typically the utility. However, for a federated learning system to be trustworthy, it needs to simultaneously satisfy multiple/many objectives, such as maximizing model performance, minimizing privacy leakage and training cost, and being robust to malicious attacks. Multi-Objective Optimization (MOO) aiming to optimize multiple conflicting objectives at the same time is quite suitable for solving the optimization problem of Trustworthy Federated Learning (TFL). In this paper, we unify MOO and TFL by formulating the problem of constrained multi-objective federated learning (CMOFL). Under this formulation, existing MOO algorithms can be adapted to TFL straightforwardly. Different from existing CMOFL works focusing on utility, efficiency, fairness, and robustness, we consider optimizing privacy leakage along with utility loss and training cost, the three primary objectives of a TFL system. We develop two improved CMOF
    
[^67]: 一种耦合流方法用于模仿学习

    A Coupled Flow Approach to Imitation Learning. (arXiv:2305.00303v1 [cs.LG])

    [http://arxiv.org/abs/2305.00303](http://arxiv.org/abs/2305.00303)

    本文提出了一种新的模仿学习算法Coupled Flow Imitation Learning（CFIL），使用正则流模型的分布匹配来建模状态分布和状态行为分布。在基准任务中具有单个专家轨迹表现出最先进的性能。

    

    在强化学习和模仿学习中，策略引起的状态分布是一个非常重要的对象。它在策略梯度定理中起着至关重要的作用，并且与相关的状态行为分布一起被广泛引用。尽管状态分布非常重要，但它大多是间接地和理论上讨论，而不是明确地建模。原因是缺乏适当的密度估计工具。在这项工作中，我们研究了基于正则流模型的上述分布应用。特别是，我们使用通过Donsker-Varadhan表示的Kullback-Leibler（KL）散度的最优点耦合的一对流进行分布匹配的模仿学习。我们的算法Coupled Flow Imitation Learning（CFIL）在具有单个专家轨迹的基准任务上实现了最先进的性能，并且自然地扩展到各种形式的专家演示。

    In reinforcement learning and imitation learning, an object of central importance is the state distribution induced by the policy. It plays a crucial role in the policy gradient theorem, and references to it--along with the related state-action distribution--can be found all across the literature. Despite its importance, the state distribution is mostly discussed indirectly and theoretically, rather than being modeled explicitly. The reason being an absence of appropriate density estimation tools. In this work, we investigate applications of a normalizing flow-based model for the aforementioned distributions. In particular, we use a pair of flows coupled through the optimality point of the Donsker-Varadhan representation of the Kullback-Leibler (KL) divergence, for distribution matching based imitation learning. Our algorithm, Coupled Flow Imitation Learning (CFIL), achieves state-of-the-art performance on benchmark tasks with a single expert trajectory and extends naturally to a varie
    
[^68]: 基于自监督任务表示学习的元强化学习

    Meta-Reinforcement Learning Based on Self-Supervised Task Representation Learning. (arXiv:2305.00286v1 [cs.LG])

    [http://arxiv.org/abs/2305.00286](http://arxiv.org/abs/2305.00286)

    该论文提出了一种基于自监督任务表示学习的元强化学习算法MoSS，可使人工智能代理能够适应未探索的任务分布及快速适应新任务。

    

    元强化学习通过学习相关训练任务并最小化交互数据，使人工智能代理能够高效地适应新任务。然而，目前大部分相关研究仍局限于参数化和固定分布的狭窄任务集，并且在评估过程中不考虑任务分布的偏移，这限制了其应用。本文提出了一种基于自监督任务表示学习的上下文元强化学习算法MoSS，以应对这一挑战。该算法扩展了元强化学习到了先前未探索过的广泛非参数化任务分布，同时在非固定和偏移任务方面取得了最先进的结果。

    Meta-reinforcement learning enables artificial agents to learn from related training tasks and adapt to new tasks efficiently with minimal interaction data. However, most existing research is still limited to narrow task distributions that are parametric and stationary, and does not consider out-of-distribution tasks during the evaluation, thus, restricting its application. In this paper, we propose MoSS, a context-based Meta-reinforcement learning algorithm based on Self-Supervised task representation learning to address this challenge. We extend meta-RL to broad non-parametric task distributions which have never been explored before, and also achieve state-of-the-art results in non-stationary and out-of-distribution tasks. Specifically, MoSS consists of a task inference module and a policy module. We utilize the Gaussian mixture model for task representation to imitate the parametric and non-parametric task variations. Additionally, our online adaptation strategy enables the agent to
    
[^69]: 段落任意模型（SAM）遇到玻璃：镜面和透明物体不能被轻松检测

    Segment Anything Model (SAM) Meets Glass: Mirror and Transparent Objects Cannot Be Easily Detected. (arXiv:2305.00278v1 [cs.CV])

    [http://arxiv.org/abs/2305.00278](http://arxiv.org/abs/2305.00278)

    这项工作对段落任意模型（SAM）的能力在玻璃相关的情况下进行了实证评估，发现SAM在镜面和透明物体中往往无法检测玻璃，这引起了在具有各种形式的玻璃的安全关键情况下部署SAM的关注。

    

    Meta AI研究最近发布了SAM（Segment Anything Model），它是在超过10亿个掩模的大量分割数据集上训练的。作为计算机视觉领域的基础模型，SAM在通用物体分割方面的出色性能引起了人们的关注。尽管它在各种零-shot迁移任务中具有强大的能力，但它是否能够在具有挑战性的设置中检测到透明物体仍然未知。在这项工作中，我们对两种与玻璃相关的具有挑战性的情况进行了实证评估：镜面和透明物体。我们发现SAM经常无法检测到两种情况下的玻璃，这引起了在具有各种形式的玻璃的安全关键情况下部署SAM的关注。

    Meta AI Research has recently released SAM (Segment Anything Model) which is trained on a large segmentation dataset of over 1 billion masks. As a foundation model in the field of computer vision, SAM (Segment Anything Model) has gained attention for its impressive performance in generic object segmentation. Despite its strong capability in a wide range of zero-shot transfer tasks, it remains unknown whether SAM can detect things in challenging setups like transparent objects. In this work, we perform an empirical evaluation of two glass-related challenging scenarios: mirror and transparent objects. We found that SAM often fails to detect the glass in both scenarios, which raises concern for deploying the SAM in safety-critical situations that have various forms of glass.
    
[^70]: 集成学习用于CME到达时间预测

    Ensemble Learning for CME Arrival Time Prediction. (arXiv:2305.00258v1 [astro-ph.SR])

    [http://arxiv.org/abs/2305.00258](http://arxiv.org/abs/2305.00258)

    该论文提出了一种名为CMETNet的集成学习方法，用于预测太阳喷发日冕物质抛射(CME)从太阳到地球的到达时间，以减少对人类系统的损害。

    

    太阳不断向日冕层释放辐射和等离子体。偶尔，太阳会发射太阳喷发，如耀斑和日冕物质抛射(CME)。CME会带走大量物质和磁通量。直接朝向地球的CME可能会对人类系统造成严重后果。它可以摧毁电网/管道，卫星和通讯。因此，准确地监测和预测CME对于最小化对人类系统的损害是重要的。在这项研究中，我们提出了一种名为CMETNet的集成学习方法，用于预测CME从太阳到地球的到达时间。

    The Sun constantly releases radiation and plasma into the heliosphere. Sporadically, the Sun launches solar eruptions such as flares and coronal mass ejections (CMEs). CMEs carry away a huge amount of mass and magnetic flux with them. An Earth-directed CME can cause serious consequences to the human system. It can destroy power grids/pipelines, satellites, and communications. Therefore, accurately monitoring and predicting CMEs is important to minimize damages to the human system. In this study we propose an ensemble learning approach, named CMETNet, for predicting the arrival time of CMEs from the Sun to the Earth. We collect and integrate eruptive events from two solar cycles, #23 and #24, from 1996 to 2021 with a total of 363 geoeffective CMEs. The data used for making predictions include CME features, solar wind parameters and CME images obtained from the SOHO/LASCO C2 coronagraph. Our ensemble learning framework comprises regression algorithms for numerical data analysis and a con
    
[^71]: 利用深度学习技术从MRI图像中分割脑肿瘤

    Brain Tumor Segmentation from MRI Images using Deep Learning Techniques. (arXiv:2305.00257v1 [eess.IV])

    [http://arxiv.org/abs/2305.00257](http://arxiv.org/abs/2305.00257)

    该论文使用深度学习模型对MRI成像数据进行处理，实现了对脑肿瘤的精确分割，为提高医学专家的工作效率提供了可行性。

    

    无论是良性还是恶性的脑肿瘤，都可能危及生命，需要精细的努力来确定类型、起源和位置，更不用说治愈。医学专家的手动分割可能耗费时间，需要技术介入以提高准确性。为了进行医学图像分割，我们检查并确定了能够在脑肿瘤分割数据集中呈现一致结果的深度学习模型。在这项研究中，我们使用了一个公共MRI成像数据集，其中包含233名患者的3064个TI加权图像，涵盖脑膜瘤、胶质瘤和垂体瘤三种脑肿瘤变体。在对数据集文件进行转换和预处理后，我们采用了一些知名的图像分割深度学习模型进行实现和训练，如U-Net和Attention U-Net等不同的骨干结构，Deep Residual U-Net，ResUnet++和Recurrent Resi等。

    A brain tumor, whether benign or malignant, can potentially be life threatening and requires painstaking efforts in order to identify the type, origin and location, let alone cure one. Manual segmentation by medical specialists can be time-consuming, which calls out for the involvement of technology to hasten the process with high accuracy. For the purpose of medical image segmentation, we inspected and identified the capable deep learning model, which shows consistent results in the dataset used for brain tumor segmentation. In this study, a public MRI imaging dataset contains 3064 TI-weighted images from 233 patients with three variants of brain tumor, viz. meningioma, glioma, and pituitary tumor. The dataset files were converted and preprocessed before indulging into the methodology which employs implementation and training of some well-known image segmentation deep learning models like U-Net & Attention U-Net with various backbones, Deep Residual U-Net, ResUnet++ and Recurrent Resi
    
[^72]: 一种基于直接抽样的深度学习逆中介质散射问题方法

    A Direct Sampling-Based Deep Learning Approach for Inverse Medium Scattering Problems. (arXiv:2305.00250v1 [eess.SP])

    [http://arxiv.org/abs/2305.00250](http://arxiv.org/abs/2305.00250)

    提出了一种基于直接抽样的深度学习方法，利用U-Net神经网络学习指数函数和真实对比度之间的关系，实现了对逆中介质散射问题的高质量重建。

    

    本文侧重于解决逆中介质散射问题，其目标是基于散射数据恢复未知散射体。在受[23]中引入的高效直接抽样方法的启发下，我们提出了一种新颖的基于直接抽样的深度学习方法（DSM-DL），用于重建非均匀散射体。具体地，我们使用U-Net神经网络来学习指数函数和真实对比度之间的关系。我们提出的DSM-DL计算效率高，对噪声具有鲁棒性，易于实现，并能够自然地结合多个测量数据以实现高质量的重建。进行了一些代表性的测试，包括不同数量的入射波和不同的噪声水平，以评估所提出方法的性能。结果展示了将深度学习技术与DSM相结合在IMSP中具有良好的应用前景。

    In this work, we focus on the inverse medium scattering problem (IMSP), which aims to recover unknown scatterers based on measured scattered data. Motivated by the efficient direct sampling method (DSM) introduced in [23], we propose a novel direct sampling-based deep learning approach (DSM-DL)for reconstructing inhomogeneous scatterers. In particular, we use the U-Net neural network to learn the relation between the index functions and the true contrasts. Our proposed DSM-DL is computationally efficient, robust to noise, easy to implement, and able to naturally incorporate multiple measured data to achieve high-quality reconstructions. Some representative tests are carried out with varying numbers of incident waves and different noise levels to evaluate the performance of the proposed method. The results demonstrate the promising benefits of combining deep learning techniques with the DSM for IMSP.
    
[^73]: 利用无标签数据增强多实例学习诊断帕金森震颤的效果

    Leveraging Unlabelled Data in Multiple-Instance Learning Problems for Improved Detection of Parkinsonian Tremor in Free-Living Conditions. (arXiv:2305.00249v1 [cs.LG])

    [http://arxiv.org/abs/2305.00249](http://arxiv.org/abs/2305.00249)

    本文提出了一种利用无标签数据和少量标记数据相结合的方法进行多实例学习，以提高自由生活条件下帕金森病震颤检测的效果。

    

    近年来，数据驱动的方法用于远程检测帕金森病及其运动征象，旨在提供早期诊断的潜在临床效益。然而，在日常生活中进行数据收集是难以避免的，同时获取细粒度的标准并不容易。因此，该问题通常通过多实例学习来解决。然而，对于大规模的研究，获取所需的粗略标准并不容易，因为需要进行完整的神经学评估。相比之下，收集没有任何标准的数据要容易得多。然而，在多实例学习中利用无标签数据不是很直观，因为这个领域的研究还很少。本文提出了一种新的方法，通过利用无标签数据与少量标记数据相结合来进行多实例学习，从而诊断帕金森病震颤。在真实的帕金森数据上的实验结果证明了我们的方法在提高自由生活条件下的震颤检测方面的有效性。

    Data-driven approaches for remote detection of Parkinson's Disease and its motor symptoms have proliferated in recent years, owing to the potential clinical benefits of early diagnosis. The holy grail of such approaches is the free-living scenario, in which data are collected continuously and unobtrusively during every day life. However, obtaining fine-grained ground-truth and remaining unobtrusive is a contradiction and therefore, the problem is usually addressed via multiple-instance learning. Yet for large scale studies, obtaining even the necessary coarse ground-truth is not trivial, as a complete neurological evaluation is required. In contrast, large scale collection of data without any ground-truth is much easier. Nevertheless, utilizing unlabelled data in a multiple-instance setting is not straightforward, as the topic has received very little research attention. Here we try to fill this gap by introducing a new method for combining semi-supervised with multiple-instance learni
    
[^74]: 一种新型金融时间序列案例表示法在行业分类中的应用

    Industry Classification Using a Novel Financial Time-Series Case Representation. (arXiv:2305.00245v1 [cs.LG])

    [http://arxiv.org/abs/2305.00245](http://arxiv.org/abs/2305.00245)

    本论文提出一种新型金融时间序列案例表示法，可用于行业部门分类任务，通过对股票收益嵌入的表示，显着提高了模型性能。

    

    金融领域已被证明是各种机器学习问题的丰富源泉，包括预测、聚类和分类。研究人员可以访问大量的时间序列数据，即使有较小的性能改进也可以转化为显著的附加价值。在本文中，我们考虑使用基于案例的推理来解决这个领域中的一个重要任务，即使用历史股票收益时间序列数据进行行业部门分类。我们讨论了为什么时间序列数据对于传统的基于案例的推理方法可能具有一些重要的表示挑战，并提出了一种基于股票收益嵌入的新型表示，可以从原始股票收益数据中轻松计算。我们认为这种表示法非常适合于基于案例的推理，并使用一个大规模的公共数据集对我们的方法进行评估，用于行业部门分类任务，展示了实质性的性能提升。

    The financial domain has proven to be a fertile source of challenging machine learning problems across a variety of tasks including prediction, clustering, and classification. Researchers can access an abundance of time-series data and even modest performance improvements can be translated into significant additional value. In this work, we consider the use of case-based reasoning for an important task in this domain, by using historical stock returns time-series data for industry sector classification. We discuss why time-series data can present some significant representational challenges for conventional case-based reasoning approaches, and in response, we propose a novel representation based on stock returns embeddings, which can be readily calculated from raw stock returns data. We argue that this representation is well suited to case-based reasoning and evaluate our approach using a large-scale public dataset for the industry sector classification task, demonstrating substantial 
    
[^75]: 深度学习在三维牙齿网格分割方法中的局限性：对分割局部扫描的分析

    A Critical Analysis of the Limitation of Deep Learning based 3D Dental Mesh Segmentation Methods in Segmenting Partial Scans. (arXiv:2305.00244v1 [cs.CV])

    [http://arxiv.org/abs/2305.00244](http://arxiv.org/abs/2305.00244)

    该论文发现目前的基于深度学习的牙齿分割算法对于局部扫描的检测效果较差，限制了其广泛应用。

    

    牙齿分割是数字牙科学中不可或缺的一部分。许多基于深度学习的牙齿分割算法已被开发用于此任务。虽然在大多数情况下已实现了高精度，但大多数现有的牙齿分割技术对完整下颌模型做了一个隐含的限制性假设，并且报告基于完整下颌模型的准确性。在某些情况下，完整的下颌牙齿扫描并不需要或可能不可用。鉴于这个实际问题，了解目前可用的广泛使用的基于深度学习的牙齿分割技术的鲁棒性非常重要。为此，我们对部分口内扫描应用了可用的分割技术，并发现可用的深度学习技术极其不足。本文所呈现的分析和比较将帮助我们了解问题的严重性，并允许我们开发出健壮的牙齿分割技术。

    Tooth segmentation from intraoral scans is a crucial part of digital dentistry. Many Deep Learning based tooth segmentation algorithms have been developed for this task. In most of the cases, high accuracy has been achieved, although, most of the available tooth segmentation techniques make an implicit restrictive assumption of full jaw model and they report accuracy based on full jaw models. Medically, however, in certain cases, full jaw tooth scan is not required or may not be available. Given this practical issue, it is important to understand the robustness of currently available widely used Deep Learning based tooth segmentation techniques. For this purpose, we applied available segmentation techniques on partial intraoral scans and we discovered that the available deep Learning techniques under-perform drastically. The analysis and comparison presented in this work would help us in understanding the severity of the problem and allow us to develop robust tooth segmentation techniq
    
[^76]: 当深度学习遇见多面体理论：一项综述

    When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v1 [math.OC])

    [http://arxiv.org/abs/2305.00241](http://arxiv.org/abs/2305.00241)

    本文综述了深度学习与多面体理论的交叉领域。修正线性单元（ReLU）等函数使得一些神经网络结构能够通过多面体理论进行分析，应用线性和混合整数线性规划来实现网络修剪、鲁棒性分析和神经网络验证等任务。

    

    在过去的十年中，深度学习成为了预测建模的主要方法，得益于深度神经网络在计算机视觉和自然语言处理等任务中的显著准确性。与此同时，神经网络的结构回归到了基于分段常数和分段线性函数的简单表示，例如修正线性单元（ReLU），这种激活函数成为神经网络中最常用的类型。这使得某些类型的网络结构，如典型的全连接前馈神经网络，能够通过多面体理论进行分析，并应用线性规划（LP）和混合整数线性规划（MILP）等方法用于各种目的。本文综述了这个快速发展领域涌现的主要主题，为更详细地了解神经网络以及应用数学提供了新的视角。我们介绍了多面体理论的基础知识以及它与深度学习的关系，并回顾了该主题的最新进展，包括在网络修剪、鲁棒性分析和神经网络验证等任务中使用LP和MILP。最后，我们讨论了当前挑战和未来研究方向。

    In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\unicode{x2014}$such as the typical fully-connected feedforward neural network$\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to app
    
[^77]: 遗传算法的FAIR童话

    The FAIRy Tale of Genetic Algorithms. (arXiv:2305.00238v1 [cs.NE])

    [http://arxiv.org/abs/2305.00238](http://arxiv.org/abs/2305.00238)

    本文扩展了FAIR数据原则以提高遗传算法（GA）的可重复性和可重用性，并提出了一个采用轻量级RDF格式的术语表以促进可重复性。

    

    遗传算法（GA）是一种常用的元启发式进化算法，使用随机算子寻找最优解，在解决许多复杂的优化问题（如分类、优化和调度）方面证明了其有效性。但是，尽管其表现、流行和简单性，但对GA的可重复性和可重用性还没有受到足够的关注。在本文中，我们将可发现、可访问、可互用和可重用（FAIR）数据原则扩展到算法，以实现其可重复性和可重用性。我们选择GA作为示例演示所提出的原则的适用性。此外，我们还介绍了方法发展和GA变种的概述，这使得复制或甚至找到正确源变得具有挑战性。为了实现FAIR算法，我们提出了一个使用轻量级RDF格式的术语表（即$evo$），以促进可重复性。鉴于GA的随机性质，

    Genetic Algorithm (GA) is a popular meta-heuristic evolutionary algorithm that uses stochastic operators to find optimal solution and has proved its effectiveness in solving many complex optimization problems (such as classification, optimization, and scheduling). However, despite its performance, popularity and simplicity, not much attention has been paid towards reproducibility and reusability of GA. In this paper, we have extended Findable, Accessible, Interoperable and Reusable (FAIR) data principles to enable the reproducibility and reusability of algorithms. We have chosen GA as a usecase to the demonstrate the applicability of the proposed principles. Also we have presented an overview of methodological developments and variants of GA that makes it challenging to reproduce or even find the right source. Additionally, to enable FAIR algorithms, we propose a vocabulary (i.e. $evo$) using light weight RDF format, facilitating the reproducibility. Given the stochastic nature of GAs,
    
[^78]: 加速和廉价的机器学习在不完全机械知识的制造过程中的应用

    Accelerated and Inexpensive Machine Learning for Manufacturing Processes with Incomplete Mechanistic Knowledge. (arXiv:2305.00229v1 [cs.LG])

    [http://arxiv.org/abs/2305.00229](http://arxiv.org/abs/2305.00229)

    本文提出了一种基于迁移学习的加速和廉价的机器学习方法，通过在来自物理过程模型的计算成本低的数据上进行训练，然后在少量高成本实验数据上进行微调，能够在受机械振动和温度变化影响的制造过程中，提高零件质量预测的准确性并显著减少实验数据量。

    

    机器学习在制造过程中模拟参数效应越来越受到关注。但是，这种方法仅适用于已建立深度物理模型的过程，因为现有方法侧重于减少生成训练数据的实验和/或计算成本，而忽略了开发新过程的定性准确物理模型所具有的内在和显著代价。本文提出了一种基于迁移学习的方法来解决这个问题，其中一个机器学习模型在来自物理过程模型（源）的大量计算成本低的数据上进行训练，然后在少量高成本实验数据（目标）上进行微调。我们的方法的新颖之处在于将源模型的定性精度界限推向文献中认为高的水平，并且它是高模型开发成本的根源。我们通过预测制造过程中受机械振动和温度变化影响的添加制造（AM）钛制零件的质量来评估我们的方法。实验结果表明，我们的方法在准确性方面优于现有最先进方法，并显著减少了模型开发所需的实验数据量。

    Machine Learning (ML) is of increasing interest for modeling parametric effects in manufacturing processes. But this approach is limited to established processes for which a deep physics-based understanding has been developed over time, since state-of-the-art approaches focus on reducing the experimental and/or computational costs of generating the training data but ignore the inherent and significant cost of developing qualitatively accurate physics-based models for new processes . This paper proposes a transfer learning based approach to address this issue, in which a ML model is trained on a large amount of computationally inexpensive data from a physics-based process model (source) and then fine-tuned on a smaller amount of costly experimental data (target). The novelty lies in pushing the boundaries of the qualitative accuracy demanded of the source model, which is assumed to be high in the literature, and is the root of the high model development cost. Our approach is evaluated f
    
[^79]: 一种基于SPSA梯度的优化器在量子机器学习中的实证比较

    An Empirical Comparison of Optimizers for Quantum Machine Learning with SPSA-based Gradients. (arXiv:2305.00224v1 [quant-ph])

    [http://arxiv.org/abs/2305.00224](http://arxiv.org/abs/2305.00224)

    本文对比了基于梯度和基于SPSA算法的量子机器学习优化器的性能，提出了一种新方法，该方法结合了来自SPSA的近似梯度和最先进的基于梯度的经典优化器，在简单回归中达到更好的收敛效果和更小的误差。

    

    在过去几年中，VQA已经引起了量子计算社区的很多关注。它们的混合量子-经典特性与相对浅的量子电路使其成为展示NISQ设备能力的有前途平台。虽然经典机器学习社区专注于基于梯度的参数优化，但使用参数移动规则寻找接近精确梯度的VQC会引入大量采样开销。因此，无梯度优化器在量子机器学习圈中变得越来越受欢迎。最有前途的候选者之一是SPSA算法，由于其低计算成本和固有的噪声鲁棒性。我们介绍了一种新方法，该方法结合了来自SPSA的近似梯度和最先进的基于梯度的经典优化器。我们通过数值实验证明，这种方法在简单回归的收敛速度和绝对误差方面优于标准的SPSA和参数移动规则。

    VQA have attracted a lot of attention from the quantum computing community for the last few years. Their hybrid quantum-classical nature with relatively shallow quantum circuits makes them a promising platform for demonstrating the capabilities of NISQ devices. Although the classical machine learning community focuses on gradient-based parameter optimization, finding near-exact gradients for VQC with the parameter-shift rule introduces a large sampling overhead. Therefore, gradient-free optimizers have gained popularity in quantum machine learning circles. Among the most promising candidates is the SPSA algorithm, due to its low computational cost and inherent noise resilience. We introduce a novel approach that uses the approximated gradient from SPSA in combination with state-of-the-art gradient-based classical optimizers. We demonstrate numerically that this outperforms both standard SPSA and the parameter-shift rule in terms of convergence rate and absolute error in simple regressi
    
[^80]: PathRTM：实时预测KI-67和肿瘤浸润淋巴细胞的深度神经网络检测器

    PathRTM: Real-time prediction of KI-67 and tumor-infiltrated lymphocytes. (arXiv:2305.00223v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.00223](http://arxiv.org/abs/2305.00223)

    本文介绍了 PathRTM，一种用于自动化的KI-67增殖和肿瘤浸润淋巴细胞估计的深度神经网络检测器，在这篇论文中，作者展示了通过在PathRTM中添加自动生成的边界框标签可以显著提高估算精度。

    

    本文介绍了一种基于RTMDet的新型深度神经网络检测器PathRTM，用于自动化的KI-67增殖和肿瘤浸润淋巴细胞估计。KI-67增殖和肿瘤浸润淋巴细胞估计在癌症诊断和治疗中起着至关重要的作用。 PathRTM是PathoNet工作的扩展，该工作在每个细胞中使用单像素关键点，通过NuClick自动生成的边界框标签提供更高级的监督。我们证明，PathRTM能够显著提高KI-67增殖和肿瘤浸润淋巴细胞的估计精度。在我们的定制数据集上进行的实验表明，PathRTM在KI-67免疫阳性、免疫阴性和淋巴细胞检测方面取得了最先进的性能，平均精度（AP）达到41.3％。我们的结果表明，PathRTM是一种有前途的方法，能够准确估计KI-67增殖和肿瘤浸润淋巴细胞，提供注释效率和加速

    In this paper, we introduce PathRTM, a novel deep neural network detector based on RTMDet, for automated KI-67 proliferation and tumor-infiltrated lymphocyte estimation. KI-67 proliferation and tumor-infiltrated lymphocyte estimation play a crucial role in cancer diagnosis and treatment. PathRTM is an extension of the PathoNet work, which uses single pixel keypoints for within each cell. We demonstrate that PathRTM, with higher-level supervision in the form of bounding box labels generated automatically from the keypoints using NuClick, can significantly improve KI-67 proliferation and tumorinfiltrated lymphocyte estimation. Experiments on our custom dataset show that PathRTM achieves state-of-the-art performance in KI-67 immunopositive, immunonegative, and lymphocyte detection, with an average precision (AP) of 41.3%. Our results suggest that PathRTM is a promising approach for accurate KI-67 proliferation and tumor-infiltrated lymphocyte estimation, offering annotation efficiency, ac
    
[^81]: 物理引导下的图神经网络用于实时交直流电力流分析

    Physics-Guided Graph Neural Networks for Real-time AC/DC Power Flow Analysis. (arXiv:2305.00216v1 [eess.SY])

    [http://arxiv.org/abs/2305.00216](http://arxiv.org/abs/2305.00216)

    本文提出了一种物理引导下的图神经网络，通过提升拓扑适应性并嵌入物理原理，实现了快速的交、直流电力流分析工具，并取得了比其他竞争对手更好的效果和计算效率。

    

    直流和交流混合系统规模不断扩大，需要比以往更快的电力流分析工具。这篇论文提出了一种特定的物理引导下的图神经网络（PG-GNN），通过提升 PG-GNN 的拓扑适应性，实现了对交、直流电网的定制图形建模。为了避免从数据中复制不可靠的经验，利用对偶原理将交、直流物理嵌入 PG-GNN 中。接着，提出一种基于增广拉格朗日法的学习方案，以帮助 PG-GNN 以无监督无标签的方式更好地学习非凸模式。最后进行多个PG-GNN来掌握不同的直流控制模式。案例研究表明，与其他7个数据驱动的竞争对手相比，只有本论文提出的方法与基于模型的基准达到相同的性能，在计算效率上甚至超过其10倍。

    The increasing scale of alternating current and direct current (AC/DC) hybrid systems necessitates a faster power flow analysis tool than ever. This letter thus proposes a specific physics-guided graph neural network (PG-GNN). The tailored graph modelling of AC and DC grids is firstly advanced to enhance the topology adaptability of the PG-GNN. To eschew unreliable experience emulation from data, AC/DC physics are embedded in the PG-GNN using duality. Augmented Lagrangian method-based learning scheme is then presented to help the PG-GNN better learn nonconvex patterns in an unsupervised label-free manner. Multi-PG-GNN is finally conducted to master varied DC control modes. Case study shows that, relative to the other 7 data-driven rivals, only the proposed method matches the performance of the model-based benchmark, also beats it in computational efficiency beyond 10 times.
    
[^82]: ShipHullGAN: 使用深度卷积生成对抗网络构建的通用船体参数化建模器

    ShipHullGAN: A generic parametric modeller for ship hull design using deep convolutional generative model. (arXiv:2305.00210v1 [cs.LG])

    [http://arxiv.org/abs/2305.00210](http://arxiv.org/abs/2305.00210)

    ShipHullGAN是一个通用的船体参数化建模器，使用深度卷积生成对抗网络进行训练，可以生成和表征各种类型的船舶。通过新的形状提取和表示策略，将所有训练设计转换为相同分辨率的几何表示形式，并在生成器后添加空间填充层，以确保生成器可以覆盖所有设计类别。

    

    本文介绍了ShipHullGAN，这是一个使用深度卷积生成对抗网络（GANs）构建的通用船体参数化建模器，可用于船体的生成和表征。我们使用一个庞大的数据集（包括集装箱船、油轮、散货船、拖轮和供应船等多种船舶类型）对ShipHullGAN进行了训练，并开发了一种新的形状提取和表示策略，将所有训练设计转换为相同分辨率的共同几何表示形式。我们在生成器后添加了一个空间填充层，以确保所训练的生成器可以覆盖所有设计类别。

    In this work, we introduce ShipHullGAN, a generic parametric modeller built using deep convolutional generative adversarial networks (GANs) for the versatile representation and generation of ship hulls. At a high level, the new model intends to address the current conservatism in the parametric ship design paradigm, where parametric modellers can only handle a particular ship type. We trained ShipHullGAN on a large dataset of 52,591 \textit{physically validated} designs from a wide range of existing ship types, including container ships, tankers, bulk carriers, tugboats, and crew supply vessels. We developed a new shape extraction and representation strategy to convert all training designs into a common geometric representation of the same resolution, as typically GANs can only accept vectors of fixed dimension as input. A space-filling layer is placed right after the generator component to ensure that the trained generator can cover all design classes. During training, designs are pro
    
[^83]: 基于数据驱动的线性回归子群识别

    Data-Driven Subgroup Identification for Linear Regression. (arXiv:2305.00195v1 [cs.LG])

    [http://arxiv.org/abs/2305.00195](http://arxiv.org/abs/2305.00195)

    本文提出了一个基于数据驱动方法的DDGroup，可以有效地识别具有特征与标签之间统一线性关系的子群，为医学研究提供了一种新的统计工具。

    

    医学研究常常需要提取每个协变量与结果之间的关系，并使用统计置信度测量。为此，通常使用简单的参数模型（例如线性回归系数），但通常是在整个数据集上拟合。然而，协变量可能在整个人口中没有统一的影响，因此统一的简单模型可能会漏掉异质信号。在本文中，我们提出了基于数据驱动方法的DDGroup (data-driven group discovery)，以有效识别数据中具有特征与标签之间统一线性关系的子群。DDGroup输出的区域是可以解释的，而且在计算上易于实现，适用于使用。理论上我们证明，如果给定足够大的样本，DDGroup保证可以找到具有统一线性关系的子群。我们还在模拟数据集和真实的医学数据集上证明了DDGroup的有效性。

    Medical studies frequently require to extract the relationship between each covariate and the outcome with statistical confidence measures. To do this, simple parametric models are frequently used (e.g. coefficients of linear regression) but usually fitted on the whole dataset. However, it is common that the covariates may not have a uniform effect over the whole population and thus a unified simple model can miss the heterogeneous signal. For example, a linear model may be able to explain a subset of the data but fail on the rest due to the nonlinearity and heterogeneity in the data. In this paper, we propose DDGroup (data-driven group discovery), a data-driven method to effectively identify subgroups in the data with a uniform linear relationship between the features and the label. DDGroup outputs an interpretable region in which the linear model is expected to hold. It is simple to implement and computationally tractable for use. We show theoretically that, given a large enough samp
    
[^84]: 一种基于广义学习系统的证据实时多模态故障诊断方法

    An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on Broad Learning System. (arXiv:2305.00169v1 [cs.LG])

    [http://arxiv.org/abs/2305.00169](http://arxiv.org/abs/2305.00169)

    本文提出了一种基于证据推理算法和广义学习系统的实时多模态故障诊断方法，该方法在更新模型参数和计算效率方面具有优势，并且在基准数据集上取得了比现有方法更好的故障诊断性能。

    

    由于多种工况表现出的非高斯、多模态和中心漂移特征，故障诊断是工业界研究的重要领域。目前，数据驱动方法是该领域的主要研究方向，但它们在连续故障分类和故障分类器参数更新方面提出了挑战，尤其在多种操作模式和实时环境中。因此，实现工业系统的实时多模态故障诊断是一个迫切的问题。为了解决这个问题，本文提出了一种新的方法，利用证据推理（ER）算法来融合信息并合并来自不同基分类器的输出。这些基分类器使用广义学习系统（BLS）开发，以提高良好的故障诊断性能。此外，在这种方法中，采用伪标签学习方法来实时更新模型参数。为了证明所提出方法的有效性，我们在基准数据集上进行实验并与现有方法进行比较。结果表明，我们提出的方法在准确性和计算效率方面优于现有方法。

    Fault diagnosis is a crucial area of research in the industry due to diverse operating conditions that exhibit non-Gaussian, multi-mode, and center-drift characteristics. Currently, data-driven approaches are the main focus in the field, but they pose challenges for continuous fault classification and parameter updates of fault classifiers, particularly in multiple operating modes and real-time settings. Therefore, a pressing issue is to achieve real-time multi-mode fault diagnosis for industrial systems. To address this problem, this paper proposes a novel approach that utilizes an evidence reasoning (ER) algorithm to fuse information and merge outputs from different base classifiers. These base classifiers are developed using a broad learning system (BLS) to improve good fault diagnosis performance. Moreover, in this approach, the pseudo-label learning method is employed to update model parameters in real-time. To demonstrate the effectiveness of the proposed approach, we perform exp
    
[^85]: 金属氧化物作为阻变存储器氧化层在RRAM和人工智能中的应用

    The Combination of Metal Oxides as Oxide Layers for RRAM and Artificial Intelligence. (arXiv:2305.00166v1 [cs.ET])

    [http://arxiv.org/abs/2305.00166](http://arxiv.org/abs/2305.00166)

    本文综述了金属氧化物作为阻变存储器氧化层在RRAM和人工智能中的应用。通过AI技术的应用可优化RRAM器件性能，同时RRAM器件本身也可以作为硬件加速器和神经形态学计算的动力源来推动AI的发展。

    

    由于其高速、低功耗和优异的可扩展性，阻变存储器(RRAM)是下一代存储设备的有力候选者。金属氧化物通常用作RRAM器件中的氧化层，由于其高介电常数和稳定性。然而，为了进一步提高RRAM器件的性能，近年来的研究集中于集成人工智能(AI)。AI可用于优化RRAM器件的性能，同时RRAM也可以作为硬件加速器和神经形态学计算的动力源来推动AI。本综述提供了金属氧化物基RRAM和AI结合的概述，重点介绍了这两个方向的最新进展。我们讨论了利用AI来改善RRAM器件性能和利用RRAM来推动AI的应用，此外，我们还讨论了该领域的关键挑战并提供未来研究方向的见解。

    Resistive random-access memory (RRAM) is a promising candidate for next-generation memory devices due to its high speed, low power consumption, and excellent scalability. Metal oxides are commonly used as the oxide layer in RRAM devices due to their high dielectric constant and stability. However, to further improve the performance of RRAM devices, recent research has focused on integrating artificial intelligence (AI). AI can be used to optimize the performance of RRAM devices, while RRAM can also power AI as a hardware accelerator and in neuromorphic computing. This review paper provides an overview of the combination of metal oxides-based RRAM and AI, highlighting recent advances in these two directions. We discuss the use of AI to improve the performance of RRAM devices and the use of RRAM to power AI. Additionally, we address key challenges in the field and provide insights into future research directions
    
[^86]: 用随机特征驯服图内核

    Taming graph kernels with random features. (arXiv:2305.00156v1 [cs.LG])

    [http://arxiv.org/abs/2305.00156](http://arxiv.org/abs/2305.00156)

    本论文介绍了一种基于随机特征的图形随机特征（GRFs）机制，能够有效解决图内核算法时间复杂度是节点数的立方的问题，从而可将定义在图上的内核方法扩展到更大的网络中。

    

    本论文介绍了基于随机特征的图形随机特征（GRFs）机制。GRFs可以用于构建针对图节点定义的多个重要内核的无偏随机估计器，特别是正则化拉普拉斯内核。与非图形内核的常规RF一样，它们提供了手段来将定义在图上的内核方法扩展到更大的网络。重要的是，它们在应用于下游应用时，对于较小的图也提供了大幅的计算收益。因此，GRFs解决了图内核算法时间复杂度是节点数的立方的问题。我们提供了GRFs的详细理论分析和广泛的实证评估：从速度测试，通过弗罗贝尼乌斯相对误差分析到使用图内核进行kmeans图聚类。我们展示了GRFs计算中存在着一个令人尴尬简单的分布式算法，可以应用于需要分割的图。

    We introduce in this paper the mechanism of graph random features (GRFs). GRFs can be used to construct unbiased randomized estimators of several important kernels defined on graphs' nodes, in particular the regularized Laplacian kernel. As regular RFs for non-graph kernels, they provide means to scale up kernel methods defined on graphs to larger networks. Importantly, they give substantial computational gains also for smaller graphs, while applied in downstream applications. Consequently, GRFs address the notoriously difficult problem of cubic (in the number of the nodes of the graph) time complexity of graph kernels algorithms. We provide a detailed theoretical analysis of GRFs and an extensive empirical evaluation: from speed tests, through Frobenius relative error analysis to kmeans graph-clustering with graph kernels. We show that the computation of GRFs admits an embarrassingly simple distributed algorithm that can be applied if the graph under consideration needs to be split ac
    
[^87]: 转移学习下的模型选择限制

    Limits of Model Selection under Transfer Learning. (arXiv:2305.00152v1 [stat.ML])

    [http://arxiv.org/abs/2305.00152](http://arxiv.org/abs/2305.00152)

    这篇论文介绍了在转移学习下模型选择存在的限制，其转移距离会影响自适应速率，可能导致速率较慢。

    

    目前，关于转移学习或领域自适应的理论研究主要关注已知假设类或模型的情况；然而，在实践中，通常涉及一定程度的模型选择，这经常出现在超参数调整的总体范畴下：例如，我们可以考虑调整针对目标任务的正确神经网络架构的问题，同时利用来自相关源任务的数据。除了与模型选择有关的近似与估计误差的通常权衡之外，这个问题还带来了新的复杂度，即源分布与目标分布之间的转移距离，这个距离随着假设类的选择而发生变化。我们首次研究了这个问题，重点关注分类问题。特别的，分析揭示了一些引人注目的现象：自适应速率，即没有分布式信息时可达到的速率，可以任意慢于oracle速率，即在给定知识的情况下。

    Theoretical studies on transfer learning or domain adaptation have so far focused on situations with a known hypothesis class or model; however in practice, some amount of model selection is usually involved, often appearing under the umbrella term of hyperparameter-tuning: for example, one may think of the problem of tuning for the right neural network architecture towards a target task, while leveraging data from a related source task.  Now, in addition to the usual tradeoffs on approximation vs estimation errors involved in model selection, this problem brings in a new complexity term, namely, the transfer distance between source and target distributions, which is known to vary with the choice of hypothesis class.  We present a first study of this problem, focusing on classification; in particular, the analysis reveals some remarkable phenomena: adaptive rates, i.e., those achievable with no distributional information, can be arbitrarily slower than oracle rates, i.e., when given kn
    
[^88]: 顺序预测双样本和独立性检验

    Sequential Predictive Two-Sample and Independence Testing. (arXiv:2305.00143v1 [stat.ML])

    [http://arxiv.org/abs/2305.00143](http://arxiv.org/abs/2305.00143)

    本文提出了一种基于预测的赌博策略来解决高维或结构化数据下非参数双样本和独立性检验问题。

    

    我们研究了顺序非参数双样本和独立性检验的问题。顺序检验在线处理数据，允许使用观察到的数据来决定是否停止并拒绝原假设，或在保持类型I错误控制的同时收集更多数据。我们建立在(非参数)测试赌博原则之上，其中赌徒在未来观察中下注，他们的财富对证据反对原假设进行衡量。最近开发的基于核的赌博策略在简单分布上通常表现良好，但对于高维或结构化数据（如文本和图像）选择合适的核通常是棘手的。为解决这个问题，我们设计了基于预测的赌博策略，依赖于以下事实：如果一个顺序更新的预测器开始一致地确定(a)一个实例从哪个分布中绘制，或者(b)一个实例是从联合分布还是从边缘分布的乘积中绘制的，则分布是不同或相关的。我们的方法灵活，并对基础数据分布和维度不可知，同时保持一定的最优性保证。我们在模拟和实际数据上演示了我们的顺序测试框架的有效性。

    We study the problems of sequential nonparametric two-sample and independence testing. Sequential tests process data online and allow using observed data to decide whether to stop and reject the null hypothesis or to collect more data while maintaining type I error control. We build upon the principle of (nonparametric) testing by betting, where a gambler places bets on future observations and their wealth measures evidence against the null hypothesis. While recently developed kernel-based betting strategies often work well on simple distributions, selecting a suitable kernel for high-dimensional or structured data, such as text and images, is often nontrivial. To address this drawback, we design prediction-based betting strategies that rely on the following fact: if a sequentially updated predictor starts to consistently determine (a) which distribution an instance is drawn from, or (b) whether an instance is drawn from the joint distribution or the product of the marginal distributio
    
[^89]: 利用标签不均匀性进行图神经网络中的节点分类

    Leveraging Label Non-Uniformity for Node Classification in Graph Neural Networks. (arXiv:2305.00139v1 [cs.LG])

    [http://arxiv.org/abs/2305.00139](http://arxiv.org/abs/2305.00139)

    本文针对标签不均匀的节点分类问题，提出了一种利用标签不均匀性进行图神经网络中的节点分类方法，在理论上分析了标签不均匀性在整个图中的变化，并提供了两种提高模型性能的方式，经实验证明可行。

    

    在使用图神经网络（GNN）进行节点分类时，典型的模型会为每个节点生成不同类标签的对数。通过这些用softmax层输出最大对数的标签预测，我们演示了从数据集中可以推断出隐藏的图形结构信息。我们引入了标签不均匀性的关键概念，它是从对数的softmax分布和均匀分布之间的Wasserstein距离中导出的。我们证明了具有小标签不均匀性的节点更难分类正确。我们从理论上分析了标签不均匀性在整个图中的变化，这提供了提高模型性能的见解：增加高标签不均匀性的训练样本或删除边以减少具有小标签不均匀性的节点集的最大割大小。这些机制可以轻松地添加到基本的GNN模型中。实验结果表明，我们的方法提高了模型的性能。

    In node classification using graph neural networks (GNNs), a typical model generates logits for different class labels at each node. A softmax layer often outputs a label prediction based on the largest logit. We demonstrate that it is possible to infer hidden graph structural information from the dataset using these logits. We introduce the key notion of label non-uniformity, which is derived from the Wasserstein distance between the softmax distribution of the logits and the uniform distribution. We demonstrate that nodes with small label non-uniformity are harder to classify correctly. We theoretically analyze how the label non-uniformity varies across the graph, which provides insights into boosting the model performance: increasing training samples with high non-uniformity or dropping edges to reduce the maximal cut size of the node set of small non-uniformity. These mechanisms can be easily added to a base GNN model. Experimental results demonstrate that our approach improves the
    
[^90]: THz用户体验鲁棒性的三位一体：联合感知、通信和人工智能

    Joint Sensing, Communication, and AI: A Trifecta for Resilient THz User Experiences. (arXiv:2305.00135v1 [cs.NI])

    [http://arxiv.org/abs/2305.00135](http://arxiv.org/abs/2305.00135)

    本文提出了一个THz用户体验鲁棒性的三位一体框架，包括联合感知、通信和人工智能。通过利用THz通道的稀疏性提取用户和环境的独特感知参数，结合多分辨率生成式AI来预测未来信息，然后提出了一个新颖的联合感知-通信协议来优化XR体验。

    

    本文提出了一种新颖的联合感知、通信和人工智能框架，以优化太赫兹（THz）无线系统中的扩展现实（XR）体验。该框架包含三个主要组件，分别是：提出了一种张量分解框架，通过利用THz通道的稀疏性来提取XR用户及其环境的独特感知参数；其次，结合对抗性变换器，提出了一个多分辨率的生成式人工智能（AI）框架，用于预测缺失和未来的感知信息；最后，提出了一种新颖的联合感知-通信协议，其中感知和通信功能交错进行，并以分享和合作的方式运作。该协议针对具有鲁棒性的XR体验进行了优化，具有从感知和通信失败中恢复的能力。仿真结果证明了该框架相对于现有方法的优越性。

    In this paper a novel joint sensing, communication, and artificial intelligence (AI) framework is proposed so as to optimize extended reality (XR) experiences over terahertz (THz) wireless systems. The proposed framework consists of three main components. First, a tensor decomposition framework is proposed to extract unique sensing parameters for XR users and their environment by exploiting then THz channel sparsity. Essentially, THz band's quasi-opticality is exploited and the sensing parameters are extracted from the uplink communication signal, thereby allowing for the use of the same waveform, spectrum, and hardware for both communication and sensing functionalities. Then, the Cramer-Rao lower bound is derived to assess the accuracy of the estimated sensing parameters. Second, a non-autoregressive multi-resolution generative artificial intelligence (AI) framework integrated with an adversarial transformer is proposed to predict missing and future sensing information. The proposed f
    
[^91]: 基于深度强化学习的物联网驱动智能孤网微电网的最优调度

    Optimal Scheduling in IoT-Driven Smart Isolated Microgrids Based on Deep Reinforcement Learning. (arXiv:2305.00127v1 [cs.LG])

    [http://arxiv.org/abs/2305.00127](http://arxiv.org/abs/2305.00127)

    本文使用深度强化学习解决物联网驱动智能孤网微电网中柴油发电机组的调度问题，通过学习历史数据生成实时决策以确保供需平衡，并减少操作成本。

    

    本文研究了在物联网驱动的孤立微电网中，如何通过深度强化学习来进行柴油发电机组的调度问题。在可再生能源和负载需求不确定性下，充分利用可再生能源。通过学习并建立历史可再生资源和负载数据的最优策略模型，以便从先前小时内相应传感器接收到的过去可再生能源和负载数据的观测中生成实时决策。目标在于确保供需平衡的前提下减少操作成本。具体来说，我们构建了一个新的有限视界下马当前过程模型（POMDP），其中考虑了旋转备用。为了克服由于二进制发电机组开关决策和连续辐射（ED）决策的离散-连续混合作用空间的挑战，提出了一种混合动作有限视角RDPG（HAFH-RDPG）的DRL算法。

    In this paper, we investigate the scheduling issue of diesel generators (DGs) in an Internet of Things (IoT)-Driven isolated microgrid (MG) by deep reinforcement learning (DRL). The renewable energy is fully exploited under the uncertainty of renewable generation and load demand. The DRL agent learns an optimal policy from history renewable and load data of previous days, where the policy can generate real-time decisions based on observations of past renewable and load data of previous hours collected by connected sensors. The goal is to reduce operating cost on the premise of ensuring supply-demand balance. In specific, a novel finite-horizon partial observable Markov decision process (POMDP) model is conceived considering the spinning reserve. In order to overcome the challenge of discrete-continuous hybrid action space due to the binary DG switching decision and continuous energy dispatch (ED) decision, a DRL algorithm, namely the hybrid action finite-horizon RDPG (HAFH-RDPG), is pr
    
[^92]: 通过局部机器学习校正改善CFD模拟

    Improving CFD simulations by local machine-learned correction. (arXiv:2305.00114v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2305.00114](http://arxiv.org/abs/2305.00114)

    该研究引入一种使用机器学习模型逆向估算网格粗化导致信息丢失的方法，从而在CFD模拟中加入这些信息，提高低分辨率网格模拟的质量。

    

    由于解决更精细的尺度带来的高成本，高保真计算流体动力学(CFD)模拟在设计空间探索中可能会非常昂贵。这种计算成本/精度平衡是现代CFD模拟的主要挑战。本研究提出了一种方法，使用一个经过训练的机器学习模型，学习预测作为大尺度流特征函数的离散化误差，以逆向估算因网格粗化而失去信息的程度。然后在运行时将这些信息添加回低分辨率解中，从而增强了不足分辨率的粗网格模拟的质量。使用更粗糙的网格产生了速度上的非线性优势，而推断和纠正丢失信息的成本具有线性成本。我们展示了一个具有工程意义的问题的数值稳定性，即三维湍流通道流动。除了这个演示外，我们还展示了更多的结果。

    High-fidelity computational fluid dynamics (CFD) simulations for design space explorations can be exceedingly expensive due to the cost associated with resolving the finer scales. This computational cost/accuracy trade-off is a major challenge for modern CFD simulations. In the present study, we propose a method that uses a trained machine learning model that has learned to predict the discretization error as a function of largescale flow features to inversely estimate the degree of lost information due to mesh coarsening. This information is then added back to the low-resolution solution during runtime, thereby enhancing the quality of the under-resolved coarse mesh simulation. The use of a coarser mesh produces a non-linear benefit in speed while the cost of inferring and correcting for the lost information has a linear cost. We demonstrate the numerical stability of a problem of engineering interest, a 3D turbulent channel flow. In addition to this demonstration, we further show the
    
[^93]: 日常环境中个性化压力监测的主动强化学习

    Active Reinforcement Learning for Personalized Stress Monitoring in Everyday Settings. (arXiv:2305.00111v1 [cs.LG])

    [http://arxiv.org/abs/2305.00111](http://arxiv.org/abs/2305.00111)

    本文提出了一种上下文感知的主动学习策略，以在部署时间为个性化微调模型的情况下，在日常环境中进行细粒度的压力检测。

    

    大多数基于传感器的监测框架都假定需要处理大量标记数据集来训练准确的检测模型。但是，在部署时间需要个性化微调模型的情况下，需要通过与用户交互在线收集特定于个人的数据集。在这个阶段优化标签的收集是施加可容忍的用户负担的关键，同时最大化个人的改善。本文考虑基于可穿戴传感器的细粒度压力检测问题，提出了一种新颖的上下文感知的主动学习策略，可以共同最大化有意义的信号样本的请求和回应率。我们开发了一个多层传感器边缘云平台，以定期捕获生理信号并实时处理它们，以及收集标签并重新训练检测模型。我们收集了一个大的数据集。

    Most existing sensor-based monitoring frameworks presume that a large available labeled dataset is processed to train accurate detection models. However, in settings where personalization is necessary at deployment time to fine-tune the model, a person-specific dataset needs to be collected online by interacting with the users. Optimizing the collection of labels in such phase is instrumental to impose a tolerable burden on the users while maximizing personal improvement. In this paper, we consider a fine-grain stress detection problem based on wearable sensors targeting everyday settings, and propose a novel context-aware active learning strategy capable of jointly maximizing the meaningfulness of the signal samples we request the user to label and the response rate. We develop a multilayered sensor-edge-cloud platform to periodically capture physiological signals and process them in real-time, as well as to collect labels and retrain the detection model. We collect a large dataset an
    
[^94]: NNSplitter：基于自动权重混淆的DNN模型主动防御方案

    NNSplitter: An Active Defense Solution to DNN Model via Automated Weight Obfuscation. (arXiv:2305.00097v1 [cs.LG])

    [http://arxiv.org/abs/2305.00097](http://arxiv.org/abs/2305.00097)

    NNSplitter是一种主动保护深度神经网络模型知识产权的方案，通过将模型分为混淆模型和模型秘密两部分，采用可信执行环境和基于强化学习的控制器来最大化精度下降和减少混淆权重的数量。

    

    深度神经网络模型作为一种有价值的知识产权，已经通过数字水印等技术进行保护。然而，这种被动模型保护并不能完全防止模型滥用。本研究提出了一种主动模型知识产权保护方案，即NNSplitter，通过将模型分为两部分来主动保护模型：一个表现较差的混淆模型和由混淆权重的索引和原始值组成的模型秘密，只有授权用户才能访问。NNSplitter利用可信执行环境来保护秘密，并采用基于强化学习的控制器来减少混淆权重的数量，同时最大化精度下降。我们的实验表明，仅修改超过2800万个权重的313个（即0.001％），混淆VGG-11模型在Fashion-MNIST上的精度可以降低到10％。我们还证明NNSplitter具有隐蔽性和韧性。

    As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users. NNSplitter uses the trusted execution environment to secure the secrets and a reinforcement learning-based controller to reduce the number of obfuscated weights while maximizing accuracy drop. Our experiments show that by only modifying 313 out of over 28 million (i.e., 0.001%) weights, the accuracy of the obfuscated VGG-11 model on Fashion-MNIST can drop to 10%. We also demonstrate that NNSplitter is stealthy and resilient aga
    
[^95]: 通过接触改进可微物理模拟的梯度计算

    Improving Gradient Computation for Differentiable Physics Simulation with Contacts. (arXiv:2305.00092v1 [cs.LG])

    [http://arxiv.org/abs/2305.00092](http://arxiv.org/abs/2305.00092)

    本文研究了接触情况下的可微分刚体模拟，发现现有的方法在接触法线方向不固定时会提供不准确的梯度，提出了一种通过连续的碰撞检测和利用碰撞时间来计算碰撞后的速度来改进梯度计算的方法TOI-Velocity。

    

    可微分模拟使梯度能够反向传播到物理模拟中。通过基于梯度的优化，可以学习物理系统的动态和属性，或者将整个可微分模拟嵌入到深度学习模型中作为下游任务（如规划和控制）的一层。然而，目前的可微分模拟并不完善，可能会提供错误的梯度，从而在学习任务中降低其性能。在本文中，我们研究接触情况下的可微分刚体模拟。我们发现，当接触法线方向不固定时，现有的可微分模拟方法会提供不准确的梯度——这是当接触是两个移动物体之间发生时的一般情况。我们提出通过连续的碰撞检测和利用碰撞时间（TOI）来计算碰撞后的速度，从而改进梯度计算。我们在两个具有联系的刚体系统上演示了我们提出的方法TOI-Velocity。

    Differentiable simulation enables gradients to be back-propagated through physics simulations. In this way, one can learn the dynamics and properties of a physics system by gradient-based optimization or embed the whole differentiable simulation as a layer in a deep learning model for downstream tasks, such as planning and control. However, differentiable simulation at its current stage is not perfect and might provide wrong gradients that deteriorate its performance in learning tasks. In this paper, we study differentiable rigid-body simulation with contacts. We find that existing differentiable simulation methods provide inaccurate gradients when the contact normal direction is not fixed - a general situation when the contacts are between two moving objects. We propose to improve gradient computation by continuous collision detection and leverage the time-of-impact (TOI) to calculate the post-collision velocities. We demonstrate our proposed method, referred to as TOI-Velocity, on tw
    
[^96]: 多类分类中敌对训练解的存在性研究

    On the existence of solutions to adversarial training in multiclass classification. (arXiv:2305.00075v1 [cs.LG])

    [http://arxiv.org/abs/2305.00075](http://arxiv.org/abs/2305.00075)

    本文研究了多类分类中敌对训练的鲁棒解存在性问题，证明了每个模型中存在 Borel 可测的鲁棒分类器，并与最优传输和总变差正则化建立了联系。在二元分类问题中，对不可知分类器的敌对训练问题存在 Borel 可测的解。

    

    本文研究了敌对训练在多类分类问题中的三种模型，旨在构建对抗扰动下鲁棒的分类器。我们证明了每个模型中存在 Borel 可测的鲁棒分类器，并提供了敌对训练问题的统一视角，拓展了作者之前的最优传输联系，并在多类情况下敌对训练和总变差正则化之间建立了新的联系。作为我们结果的推论，我们证明了在二元分类设置中，对不可知分类器的敌对训练问题存在 Borel 可测的解，这一结果改进了关于敌对训练的文献，文献中仅已知只有在特征空间的扩大通用 $σ$-代数内存在鲁棒的分类器。

    We study three models of the problem of adversarial training in multiclass classification designed to construct robust classifiers against adversarial perturbations of data in the agnostic-classifier setting. We prove the existence of Borel measurable robust classifiers in each model and provide a unified perspective of the adversarial training problem, expanding the connections with optimal transport initiated by the authors in previous work and developing new connections between adversarial training in the multiclass setting and total variation regularization. As a corollary of our results, we prove the existence of Borel measurable solutions to the agnostic adversarial training problem in the binary classification setting, a result that improves results in the literature of adversarial training, where robust classifiers were only known to exist within the enlarged universal $\sigma$-algebra of the feature space.
    
[^97]: 在线Platt缩放及其校准方法

    Online Platt Scaling with Calibeating. (arXiv:2305.00070v1 [cs.LG])

    [http://arxiv.org/abs/2305.00070](http://arxiv.org/abs/2305.00070)

    本文提出了一种在线Platt缩放及其校准方法，其理论基础强大，可以处理分布漂移和对抗性结果序列，无需超参数调整，在一系列合成和真实数据集上表现出卓越的性能。

    

    我们提出了一种在线后校准方法，称为在线Platt缩放(OPS)，它将Platt缩放技术与在线逻辑回归相结合。我们展示了OPS如何在分布漂移的i.i.d.和非i.i.d.情况下平稳适应。此外，当最佳的Platt缩放模型本身被错误校准时，我们使用一种最近开发的称为calibeating的技术来增强OPS，使其更加鲁棒。理论上，我们得到的OPS+calibeating方法对于对抗性结果序列是保证校准的。在实验上，它在一系列合成和真实数据集上均表现出卓越的性能，无需超参数调整。最后，我们将所有OPS思想扩展到beta缩放方法。

    We present an online post-hoc calibration method, called Online Platt Scaling (OPS), which combines the Platt scaling technique with online logistic regression. We demonstrate that OPS smoothly adapts between i.i.d. and non-i.i.d. settings with distribution drift. Further, in scenarios where the best Platt scaling model is itself miscalibrated, we enhance OPS by incorporating a recently developed technique called calibeating to make it more robust. Theoretically, our resulting OPS+calibeating method is guaranteed to be calibrated for adversarial outcome sequences. Empirically, it is effective on a range of synthetic and real-world datasets, with and without distribution drifts, achieving superior performance without hyperparameter tuning. Finally, we extend all OPS ideas to the beta scaling method.
    
[^98]: 基于深度学习的口罩佩戴检测在COVID-19大流行期间的应用

    Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV])

    [http://arxiv.org/abs/2305.00068](http://arxiv.org/abs/2305.00068)

    本研究探讨了在 COVID-19 疫情期间使用深度学习模型进行口罩佩戴检测的可行性。通过比较不同模型，选择了适用于实时和移动设备应用的最佳模型，并取得了高准确度。

    

    在COVID-19疫情期间，佩戴口罩已被知晓为预防病毒传播的有效方法之一。深度学习模型的出色性能取代了人类在许多监控任务中的角色。监控口罩佩戴就是这样一项可以由深度学习模型完成的任务。 由于隔离的原因，面部口罩照片的数量有限，因此是这项任务的主要挑战。本文使用三种最先进的目标检测神经网络模型对口罩检测进行了研究，包括 Single Shot Detector（SSD）、两个版本的 You Only Look Once。根据不同模型的表现，选择最适合在现实世界和移动设备应用中使用的模型。实验结果表明，所提出的方法实现了用于实时和移动设备应用的高准确度。

    During the COVID-19 pandemic, wearing a face mask has been known to be an effective way to prevent the spread of COVID-19. In lots of monitoring tasks, humans have been replaced with computers thanks to the outstanding performance of the deep learning models. Monitoring the wearing of a face mask is another task that can be done by deep learning models with acceptable accuracy. The main challenge of this task is the limited amount of data because of the quarantine. In this paper, we did an investigation on the capability of three state-of-the-art object detection neural networks on face mask detection for real-time applications. As mentioned, here are three models used, Single Shot Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny, and YOLOv4-tiny-3l from which the best was selected. In the proposed method, according to the performance of different models, the best model that can be suitable for use in real-world and mobile device applications in comparison to
    
[^99]: LAVA: 无需预定学习算法的数据价值评估

    LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v1 [cs.LG])

    [http://arxiv.org/abs/2305.00054](http://arxiv.org/abs/2305.00054)

    LAVA是一个学习算法无关的数据价值评估方法，它结合了学习算法的统计特性和训练数据的属性，通过迭代估计数据值来实现。LAVA比现有方法计算速度更快，精度更高，并且可以为不同的应用提供有意义的数据排名。

    

    传统的数据价值评估问题是如何公平地分配学习算法的验证性能，致使计算得到的数据价值依赖于底层学习算法的许多设计选择。本文提出了一种新的框架LAVA，该框架结合了学习算法的统计特性和训练数据的属性，迭代估计数据值，使其无视下游的学习算法。我们展示了LAVA比现有方法计算速度更快，精度更高，并且它可以为不同的应用提供有意义的数据排名。

    Traditionally, data valuation is posed as a problem of equitably splitting the validation performance of a learning algorithm among the training data. As a result, the calculated data values depend on many design choices of the underlying learning algorithm. However, this dependence is undesirable for many use cases of data valuation, such as setting priorities over different data sources in a data acquisition process and informing pricing mechanisms in a data marketplace. In these scenarios, data needs to be valued before the actual analysis and the choice of the learning algorithm is still undetermined then. Another side-effect of the dependence is that to assess the value of individual points, one needs to re-run the learning algorithm with and without a point, which incurs a large computation burden.  This work leapfrogs over the current limits of data valuation methods by introducing a new framework that can value training data in a way that is oblivious to the downstream learning
    
[^100]: 因果推理与大型语言模型：开启因果研究的新篇章

    Causal Reasoning and Large Language Models: Opening a New Frontier for Causality. (arXiv:2305.00050v1 [cs.AI])

    [http://arxiv.org/abs/2305.00050](http://arxiv.org/abs/2305.00050)

    大型语言模型在因果推理任务中取得了新的最高准确率，但是其鲁棒性仍然存在难以预测的失败模式。

    

    大型语言模型的因果能力备受争议，并且对将其应用于医学、科学、法律和政策等具有社会影响力的领域具有重要意义。我们进一步探讨了LLMs及其因果推理的区别，以及潜在的建构和测量效度威胁。基于GPT-3.5和4的算法在多个因果基准测试上取得了新的最高准确率。与此同时，LLMs展示了难以预测的失败模式，我们提供了一些技术来解释它们的鲁棒性。

    The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.  Crucially, LLMs perform these causal tasks while relying on sources of knowledg
    
[^101]: 数据驱动气象预测的真实观测验证

    Verification against in-situ observations for Data-Driven Weather Prediction. (arXiv:2305.00048v1 [cs.LG])

    [http://arxiv.org/abs/2305.00048](http://arxiv.org/abs/2305.00048)

    数据驱动气象预测模型（DDWP）近年来发展迅速，但需要更加严格的真实观测验证来在操作预报中更安全地使用。

    

    近年来，数据驱动气象预测模型（DDWP）取得了快速发展，展示了高度接近数值天气预报（NWP）模型的能力。快速、准确、低成本的DDWP预报使其在操作预报中的使用成为一个具有吸引力的选择，但是，在真正的操作环境中对DDWPs进行严格的评估仍然需要努力。DDWP通常使用ERA5重新分析数据进行训练和评估，但是DDWP仅在模拟中进行过测试，即使模拟质量很高，也无法完全准确地代表真实世界。在操作预报中安全地使用DDWP需要更加彻底的“真实世界”验证，以及对当前DDWP的训练和评估方式进行仔细的研究。值得问一下，例如，用于训练的重新分析数据集对真实世界的模拟效果如何？这对于气候公正和气象数据的不均匀可用性非常重要。

    Data-driven weather prediction models (DDWPs) have made rapid strides in recent years, demonstrating an ability to approximate Numerical Weather Prediction (NWP) models to a high degree of accuracy. The fast, accurate, and low-cost DDWP forecasts make their use in operational forecasting an attractive proposition, however, there remains work to be done in rigorously evaluating DDWPs in a true operational setting. Typically trained and evaluated using ERA5 reanalysis data, DDWPs have been tested only in a simulation, which cannot represent the real world with complete accuracy even if it is of a very high quality. The safe use of DDWPs in operational forecasting requires more thorough "real-world" verification, as well as a careful examination of how DDWPs are currently trained and evaluated. It is worth asking, for instance, how well do the reanalysis datasets, used for training, simulate the real world? With an eye towards climate justice and the uneven availability of weather data: i
    
[^102]: 由人工智能驱动的享乐价格和质量调整价格指数

    Hedonic Prices and Quality Adjusted Price Indices Powered by AI. (arXiv:2305.00044v1 [econ.GN])

    [http://arxiv.org/abs/2305.00044](http://arxiv.org/abs/2305.00044)

    本研究提出了一种基于深度神经网络和转换器的经验享乐模型，能够处理大量未结构化的产品数据，准确地估计产品的享乐价格和派生指数。

    

    在当今的经济环境下，使用电子记录准确地实时测量价格指数的变化对于跟踪通胀和生产率至关重要。本文开发了经验享乐模型，能够处理大量未结构化的产品数据（文本、图像、价格和数量），并输出精确的享乐价格估计和派生指数。为实现这一目标，我们使用深度神经网络从文本描述和图像中生成抽象的产品属性或”特征“，然后使用这些属性来估算享乐价格函数。具体地，我们使用基于transformers的大型语言模型将有关产品的文本信息转换为数字特征，使用训练或微调过的产品描述信息，使用残差网络模型将产品图像转换为数字特征。为了产生估计的享乐价格函数，我们再次使用多任务神经网络，训练以在所有时间段同时预测产品的价格。

    Accurate, real-time measurements of price index changes using electronic records are essential for tracking inflation and productivity in today's economic environment. We develop empirical hedonic models that can process large amounts of unstructured product data (text, images, prices, quantities) and output accurate hedonic price estimates and derived indices. To accomplish this, we generate abstract product attributes, or ``features,'' from text descriptions and images using deep neural networks, and then use these attributes to estimate the hedonic price function. Specifically, we convert textual information about the product to numeric features using large language models based on transformers, trained or fine-tuned using product descriptions, and convert the product image to numeric features using a residual network model. To produce the estimated hedonic price function, we again use a multi-task neural network trained to predict a product's price in all time periods simultaneousl
    
[^103]: 对抗性表征学习在音频隐私保护中的应用

    Adversarial Representation Learning for Robust Privacy Preservation in Audio. (arXiv:2305.00011v1 [cs.SD])

    [http://arxiv.org/abs/2305.00011](http://arxiv.org/abs/2305.00011)

    本研究提出了一种对抗性训练方法，用于学习音频的表征，从而有效地防止从音频记录的潜在特征中检测到语音活动，提出的方法能够使得包含语音的音频记录的潜在表征与不包含语音的音频记录的潜在表征无法被语音分类器区分出来。

    

    声音事件检测系统广泛应用于各种应用程序，如监视和环境监测，其中数据被自动收集、处理并发送到云中进行声音识别。然而，这个过程可能无意中泄露用户或其周围环境的敏感信息，因此引起了隐私关注。在本研究中，我们提出了一种新颖的对抗训练方法，用于学习音频记录的表征，从而有效地防止从记录的潜在特征中检测到语音活动。所提出的方法训练模型生成包含语音的音频记录的不变潜在表征，这些表征不能由语音分类器从非语音记录中区分出来。我们的工作创新之处在于，优化算法中语音分类器的权重定期被训练在有监督方式下的分类器的权重所替换。这增加了语音分类器的判别能力。

    Sound event detection systems are widely used in various applications such as surveillance and environmental monitoring where data is automatically collected, processed, and sent to a cloud for sound recognition. However, this process may inadvertently reveal sensitive information about users or their surroundings, hence raising privacy concerns. In this study, we propose a novel adversarial training method for learning representations of audio recordings that effectively prevents the detection of speech activity from the latent features of the recordings. The proposed method trains a model to generate invariant latent representations of speech-containing audio recordings that cannot be distinguished from non-speech recordings by a speech classifier. The novelty of our work is in the optimization algorithm, where the speech classifier's weights are regularly replaced with the weights of classifiers trained in a supervised manner. This increases the discrimination power of the speech cl
    
[^104]: "Río Hortega University Hospital Glioblastoma Dataset: 一份包含术前、术后早期和复发MRI扫描的综合性数据集 (RHUH-GBM)"

    The R\'io Hortega University Hospital Glioblastoma dataset: a comprehensive collection of preoperative, early postoperative and recurrence MRI scans (RHUH-GBM). (arXiv:2305.00005v1 [q-bio.QM])

    [http://arxiv.org/abs/2305.00005](http://arxiv.org/abs/2305.00005)

    这份数据集提供了包括MRI图像、容积评估、分子数据和生存细节在内的Glioblastoma患者相关数据，同时提供了专家纠正的肿瘤亚区划分，为发展术后和随访MRI扫描的算法提供了有价值的基准数据。

    

    高度侵袭性的原发性脑肿瘤Glioblastoma与患者不良后果相关，MRI在诊断、特征描述和预测Glioblastoma进展方面扮演重要角色。然而，公共MRI资料库存在严重问题，包括术后和随访研究不足，以及专家肿瘤分割不足等。为了解决这些问题，我们提出了“Río Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)”，一个包含多参量MRI图像、容积评估、分子数据和对进行全面或近全增强肿瘤切除的Glioblastoma患者的生存细节的数据集。该数据集具有专家纠正的肿瘤亚区划分，为发展术后和随访MRI扫描的算法提供了有价值的基准数据。RHUH-GBM数据集的公开发布在Glioblastoma研究中做出了显著贡献，使科学界能够更好地理解此病并开展深入的研究。

    Glioblastoma, a highly aggressive primary brain tumor, is associated with poor patient outcomes. Although magnetic resonance imaging (MRI) plays a critical role in diagnosing, characterizing, and forecasting glioblastoma progression, public MRI repositories present significant drawbacks, including insufficient postoperative and follow-up studies as well as expert tumor segmentations. To address these issues, we present the "R\'io Hortega University Hospital Glioblastoma Dataset (RHUH-GBM)," a collection of multiparametric MRI images, volumetric assessments, molecular data, and survival details for glioblastoma patients who underwent total or near-total enhancing tumor resection. The dataset features expert-corrected segmentations of tumor subregions, offering valuable ground truth data for developing algorithms for postoperative and follow-up MRI scans. The public release of the RHUH-GBM dataset significantly contributes to glioblastoma research, enabling the scientific community to st
    
[^105]: 使用机器学习精确检测固体燃料颗粒的点火过程

    Accurate ignition detection of solid fuel particles using machine learning. (arXiv:2305.00004v1 [cs.LG])

    [http://arxiv.org/abs/2305.00004](http://arxiv.org/abs/2305.00004)

    本研究使用高速光学诊断技术和机器学习方法，对单颗粒点火过程进行精确确定。经过训练，使用FPN和ResNet网络均可显著提高检测的准确性和精度。本研究发现，FPN的分层特征在检测点火过程中更为有利。

    

    本文利用高速光学诊断技术与机器学习方法，重点研究了单颗粒点火的精确确定。通过同时进行10 kHz OH-LIF和DBI测量，可以可视化层流反应器中单个颗粒的点火过程。研究两种煤粒粒径（90-125μm和160-200μm）在常规空气和富氧条件下随氧气浓度增加的点火延迟时间。首先使用阈值方法评估点火延迟时间，结果与人眼观察得到的地面真实值存在明显偏差。然后，使用ResNet和FPN对地面真实值进行训练，并将其应用于预测点火时间。这两个网络都能够以显著更高的准确性和精度检测点火。此外，还考察了输入数据和网络深度对训练模型预测性能的影响。本研究表明，FPN的分层特征比ResNet的剩余连接更有利于精确检测点火过程。

    In the present work, accurate determination of single-particle ignition is focused on using high-speed optical diagnostics combined with machine learning approaches. Ignition of individual particles in a laminar flow reactor are visualized by simultaneous 10 kHz OH-LIF and DBI measurements. Two coal particle sizes of 90-125{\mu}m and 160-200{\mu}m are investigated in conventional air and oxy-fuel conditions with increasing oxygen concentrations. Ignition delay times are first evaluated with threshold methods, revealing obvious deviations compared to the ground truth detected by the human eye. Then, residual networks (ResNet) and feature pyramidal networks (FPN) are trained on the ground truth and applied to predict the ignition time.~Both networks are capable of detecting ignition with significantly higher accuracy and precision. Besides, influences of input data and depth of networks on the prediction performance of a trained model are examined.~The current study shows that the hierar
    
[^106]: 多晶微结构神经网络加速工艺设计

    Neural Network Accelerated Process Design of Polycrystalline Microstructures. (arXiv:2305.00003v1 [cs.CE])

    [http://arxiv.org/abs/2305.00003](http://arxiv.org/abs/2305.00003)

    通过神经网络加速工艺设计，减轻预测微结构演化的计算负担，并找到最佳加工路径。

    

    通过计算实验，找到维持理想材料结构的设计路径可以优化所需的材料性质。这需要采用多尺度方法来理解加工-（微观）结构-性能之间的相互作用，将宏观尺度（工艺参数）与中观（均质化的性质）和微观（晶体学纹理）尺度连接起来。由于问题的多尺度建模设置，可能的加工路径选择会随着决策树的加深而呈指数级增长，传统模拟器的速度达到关键计算阈值。为了减轻在给定加载条件下预测微结构演化的计算负担，我们开发了一种带物理约束的神经网络（NN）方法。该NN旨在学习每个基本过程下微观结构的演化。我们的方法在找到最佳加工路径方面是有效和鲁棒的。在本研究中，我们使用我们的方法成功地对铝微结构进行了优化。

    Computational experiments are exploited in finding a well-designed processing path to optimize material structures for desired properties. This requires understanding the interplay between the processing-(micro)structure-property linkages using a multi-scale approach that connects the macro-scale (process parameters) to meso (homogenized properties) and micro (crystallographic texture) scales. Due to the nature of the problem's multi-scale modeling setup, possible processing path choices could grow exponentially as the decision tree becomes deeper, and the traditional simulators' speed reaches a critical computational threshold. To lessen the computational burden for predicting microstructural evolution under given loading conditions, we develop a neural network (NN)-based method with physics-infused constraints. The NN aims to learn the evolution of microstructures under each elementary process. Our method is effective and robust in finding optimal processing paths. In this study, our
    
[^107]: 基于迁移学习和CNNs集成的多种颜色空间对银河分类

    Galaxy Classification Using Transfer Learning and Ensemble of CNNs With Multiple Colour Spaces. (arXiv:2305.00002v1 [astro-ph.IM])

    [http://arxiv.org/abs/2305.00002](http://arxiv.org/abs/2305.00002)

    本研究使用多种颜色空间和CNN架构的集成方法，结合迁移学习技术，提出了一个自动分类星系的方法，并在Kaggle Galaxy Zoo数据集上实现了最先进的准确性。

    

    大数据已成为天文学领域的常态，这使得它成为计算机科学研究的理想领域。天文学家通常根据星系形态来分类星系，这一做法可追溯至1936年的哈勃。在小型数据集上，可以通过个人或小团队进行分类，但现代望远镜带来的数据呈指数级增长，需要自动分类方法。该研究探讨了颜色空间变换对分类准确性的影响，并探究了CNN架构对该关系的影响。考虑到多种颜色空间（RGB、XYZ、LAB等）和CNN架构（VGG、ResNet、DenseNet、Xception等），利用预训练的模型和权重。然而，由于大多数预训练模型是在自然图像的背景下开发的，因此本研究还调查了使用迁移学习将这些模型适应于星系分类的效果。实验结果显示，利用迁移学习的多种颜色空间的CNNs集成比单个模型表现更好，并在Kaggle Galaxy Zoo数据集上实现了最先进的准确性。

    Big data has become the norm in astronomy, making it an ideal domain for computer science research. Astronomers typically classify galaxies based on their morphologies, a practice that dates back to Hubble (1936). With small datasets, classification could be performed by individuals or small teams, but the exponential growth of data from modern telescopes necessitates automated classification methods.  In December 2013, Winton Capital, Galaxy Zoo, and the Kaggle team created the Galaxy Challenge, which tasked participants with developing models to classify galaxies. The Kaggle Galaxy Zoo dataset has since been widely used by researchers. This study investigates the impact of colour space transformation on classification accuracy and explores the effect of CNN architecture on this relationship. Multiple colour spaces (RGB, XYZ, LAB, etc.) and CNN architectures (VGG, ResNet, DenseNet, Xception, etc.) are considered, utilizing pre-trained models and weights. However, as most pre-trained m
    
[^108]: 基于POCS聚类算法的特征嵌入聚类

    Feature Embedding Clustering using POCS-based Clustering Algorithm. (arXiv:2305.00001v1 [cs.LG])

    [http://arxiv.org/abs/2305.00001](http://arxiv.org/abs/2305.00001)

    本文提出了一种新的聚类技术POCS-based clustering algorithm，将POCS收敛性质应用于聚类问题，相比于其他经典聚类方案具有更好的表现，可以用于解决特征嵌入聚类问题。

    

    本文提出了一种使用基于POCS聚类算法（POCS代表投影到凸集）的新型聚类技术来解决特征嵌入聚类问题的应用。POCS聚类算法将POCS收敛性质应用于聚类问题，在聚类误差和执行速度方面表现出与其他经典聚类方案竞争性的性能。具体而言，POCS聚类算法将每个数据点视为一个凸集，并将并行投影操作从每个聚类原型到相应的数据成员，以最小化目标函数并更新原型。从5个名人人脸和MNIST数据集提取的合成嵌入数据集的实验结果表明，与K-Means和Fuzz等其他经典聚类方案相比，POCS聚类算法可以表现出良好的结果。

    An application of the POCS-based clustering algorithm (POCS stands for Projection Onto Convex Set), a novel clustering technique, for feature embedding clustering problems is proposed in this paper. The POCS-based clustering algorithm applies the POCS's convergence property to clustering problems and has shown competitive performance when compared with that of other classical clustering schemes in terms of clustering error and execution speed. Specifically, the POCS-based clustering algorithm treats each data point as a convex set and applies a parallel projection operation from every cluster prototype to corresponding data members in order to minimize the objective function and update the prototypes. The experimental results on the synthetic embedding datasets extracted from the 5 Celebrity Faces and MNIST datasets show that the POCS-based clustering algorithm can perform with favorable results when compared with those of other classical clustering schemes such as the K-Means and Fuzz
    
[^109]: ACM Multimedia 2023 计算语言学挑战赛：情感共享与请求

    The ACM Multimedia 2023 Computational Paralinguistics Challenge: Emotion Share & Requests. (arXiv:2304.14882v1 [cs.SD])

    [http://arxiv.org/abs/2304.14882](http://arxiv.org/abs/2304.14882)

    ACM Multimedia 2023 计算语言学挑战赛涉及情感共享和请求检测，提供了基线特征提取和分类器方法。

    

    ACM Multimedia 2023 计算语言学挑战赛首次在明确定义的条件下解决了两个不同的问题：在情感共享子挑战中需要对语音进行回归，而在请求子挑战中需要检测请求和抱怨。我们描述了子挑战、基线特征提取和分类器，其中包括通常的 ComPaRE 特征、auDeep 工具包和使用预训练 CNN 的深度特征提取以及 wav2vec2 模型的使用。

    The ACM Multimedia 2023 Computational Paralinguistics Challenge addresses two different problems for the first time in a research competition under well-defined conditions: In the Emotion Share Sub-Challenge, a regression on speech has to be made; and in the Requests Sub-Challenges, requests and complaints need to be detected. We describe the Sub-Challenges, baseline feature extraction, and classifiers based on the usual ComPaRE features, the auDeep toolkit, and deep feature extraction from pre-trained CNNs using the DeepSpectRum toolkit; in addition, wav2vec2 models are used.
    
[^110]: 医学图像的“Segment Anything Model”模型？

    Segment Anything Model for Medical Images?. (arXiv:2304.14660v1 [eess.IV])

    [http://arxiv.org/abs/2304.14660](http://arxiv.org/abs/2304.14660)

    “Segment Anything Model”（SAM）是适用于常规图像分割的基础模型，可以实现零样本图像分割，但在医学图像分割方面具有更高的挑战性。作者通过构建一个大型医学分割数据集来验证SAM在该领域的潜力。

    

    “Segment Anything Model”（SAM）是第一个适用于常规图像分割的基础模型。它设计了一种新颖的可推广分割任务，通过自动和手动两种模式实现了使用预训练模型进行零样本图像分割。SAM在各种自然图像分割任务中取得了显着的成果。然而，由于复杂的模态、细微的解剖结构、不确定的复杂对象边界和广泛的对象尺度，医学图像分割（MIS）更具挑战性。SAM在各种自然图像分割任务中取得了显着的成果。同时，零样本和高效的MIS可以很好地减少注释时间并促进医学图像分析的发展。因此，SAM似乎是一个潜在的工具，并且其在大型医学数据集上的表现应该进一步验证。我们收集和整理了52个开源数据集，并建立了一个具有16个模态和68个对象的大型医学分割数据集。

    The Segment Anything Model (SAM) is the first foundation model for general image segmentation. It designed a novel promotable segmentation task, ensuring zero-shot image segmentation using the pre-trained model via two main modes including automatic everything and manual prompt. SAM has achieved impressive results on various natural image segmentation tasks. However, medical image segmentation (MIS) is more challenging due to the complex modalities, fine anatomical structures, uncertain and complex object boundaries, and wide-range object scales. SAM has achieved impressive results on various natural image segmentation tasks. Meanwhile, zero-shot and efficient MIS can well reduce the annotation time and boost the development of medical image analysis. Hence, SAM seems to be a potential tool and its performance on large medical datasets should be further validated. We collected and sorted 52 open-source datasets, and build a large medical segmentation dataset with 16 modalities, 68 obje
    
[^111]: 实现高效和全面的城市时空预测：一个统一的库和性能基准

    Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])

    [http://arxiv.org/abs/2304.14343](http://arxiv.org/abs/2304.14343)

    本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。

    

    随着深度学习技术的不断推进和城市时空数据的积累，越来越多的深度学习模型被提出来解决城市时空预测问题。然而，现有领域存在许多限制，包括开放数据以各种格式存在，使用困难，极少数论文公开其代码和数据，以及开源模型经常使用不同的框架和平台，使得比较具有挑战性。迫切需要一个统一的框架来实施和评估这些方法。为解决这些问题，我们提供了一个城市时空预测的综合评估，并提出了一种称为原子文件的统一空间时间数据存储格式。我们还提出了一个名为LibCity的开源库，为研究人员提供了一个可靠的实验工具和一个方便的开发框架。在这个库中，我们已经重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。此外，我们还引入了一个城市时空预测模型性能基准，包括效率和有效性度量，以进行公平比较。在这个基准上的实验结果证明了我们提出的统一库和基准的有用性和有效性。

    As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
    
[^112]: TorchBench: 用高API表面覆盖率评估PyTorch性能的基准套件

    TorchBench: Benchmarking PyTorch with High API Surface Coverage. (arXiv:2304.14226v1 [cs.LG])

    [http://arxiv.org/abs/2304.14226](http://arxiv.org/abs/2304.14226)

    TorchBench是一款新型基准测试套件，可全面表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。

    

    深度学习是多个领域中的革命性技术。为了方便模型的开发和部署，提出了许多深度学习框架，其中PyTorch是最流行的解决方案之一。PyTorch软件栈的生态性能至关重要，可节省模型训练成本并减少模型推理的响应时间。本文提出了TorchBench，一款新型基准测试套件，用于研究PyTorch软件栈的性能。与现有基准测试套件不同，TorchBench包含了许多代表性模型，覆盖了大量PyTorch API表面。TorchBench能够全面地表征PyTorch软件栈的性能，指导模型、PyTorch框架和GPU库的性能优化。我们展示了TorchBench的两个实际用例。第一，我们对TorchBench进行性能剖析，以识别PyTorch的GPU性能效率问题。我们能够优化许多性能故障并向上游提交贡献。

    Deep learning (DL) has been a revolutionary technique in various domains. To facilitate the model development and deployment, many deep learning frameworks are proposed, among which PyTorch is one of the most popular solutions. The performance of ecosystem around PyTorch is critically important, which saves the costs of training models and reduces the response time of model inferences. In this paper, we propose TorchBench, a novel benchmark suite to study the performance of PyTorch software stack. Unlike existing benchmark suites, TorchBench encloses many representative models, covering a large PyTorch API surface. TorchBench is able to comprehensively characterize the performance of the PyTorch software stack, guiding the performance optimization across models, PyTorch framework, and GPU libraries. We show two practical use cases of TorchBench. (1) We profile TorchBench to identify GPU performance inefficiencies in PyTorch. We are able to optimize many performance bugs and upstream pa
    
[^113]: ChartSumm：长短摘要自动生成任务的全面基准数据集

    ChartSumm: A Comprehensive Benchmark for Automatic Chart Summarization of Long and Short Summaries. (arXiv:2304.13620v1 [cs.CL])

    [http://arxiv.org/abs/2304.13620](http://arxiv.org/abs/2304.13620)

    本文提出了ChartSumm数据集，用于长短摘要自动生成任务，包括84000多个图表及其元数据和描述。研究发现，现有的自动摘要模型虽然得分不错，但经常面临错觉、漏掉重要数据点以及不正确解释复杂趋势等问题。

    

    自动将图表转换为文本摘要是视障人士的有效工具，同时为用户提供表格数据的自然语言精确洞察力。大型、结构良好的数据集始终是数据驱动模型的关键部分。本文提出了ChartSumm：一个大规模基准数据集，包括共84363个图表及其元数据和描述，涵盖广泛的主题和图表类型，可生成长短摘要。强基线模型的广泛实验表明，尽管这些模型通过实现各种自动评估指标的得分来生成流畅且信息丰富的摘要，但它们经常遇到一些问题，例如产生错觉，漏掉重要的数据点，以及不正确地解释图表中的复杂趋势。我们还通过自动翻译工具探讨了将ChartSumm扩展到其他语言的潜力。这使得我们的数据集成为一个有挑战的任务。

    Automatic chart to text summarization is an effective tool for the visually impaired people along with providing precise insights of tabular data in natural language to the user. A large and well-structured dataset is always a key part for data driven models. In this paper, we propose ChartSumm: a large-scale benchmark dataset consisting of a total of 84,363 charts along with their metadata and descriptions covering a wide range of topics and chart types to generate short and long summaries. Extensive experiments with strong baseline models show that even though these models generate fluent and informative summaries by achieving decent scores in various automatic evaluation metrics, they often face issues like suffering from hallucination, missing out important data points, in addition to incorrect explanation of complex trends in the charts. We also investigated the potential of expanding ChartSumm to other languages using automated translation tools. These make our dataset a challeng
    
[^114]: 金融公告中结构图识别

    Structure Diagram Recognition in Financial Announcements. (arXiv:2304.13240v1 [cs.CV])

    [http://arxiv.org/abs/2304.13240](http://arxiv.org/abs/2304.13240)

    本文提出了一种新的识别金融公告中结构图的方法，并通过两阶段方法生成了行业的第一个基准。实验结果显示，与现有方法相比，本文所提出的方法具有更高的有效性和效率。

    

    在构建金融知识图谱并进一步提高各种金融应用的效率方面，准确从金融公告中的结构图中提取结构化数据具有很大的实际意义。本文提出了一种新的方法，能更好地检测和提取不同类型的连接线，包括不同方向和角度的直线、曲线和折线。我们开发了一个两阶段的方法，使用自动化工具合成并注释大量图表以训练初步识别模型，并自动标注真实结构图以进行少量手动更正，最终实验验证了我们提出的方法在基准数据集上的有效性和效率，并与现有方法相比取得了最先进的性能。

    Accurately extracting structured data from structure diagrams in financial announcements is of great practical importance for building financial knowledge graphs and further improving the efficiency of various financial applications. First, we proposed a new method for recognizing structure diagrams in financial announcements, which can better detect and extract different types of connecting lines, including straight lines, curves, and polylines of different orientations and angles. Second, we developed a two-stage method to efficiently generate the industry's first benchmark of structure diagrams from Chinese financial announcements, where a large number of diagrams were synthesized and annotated using an automated tool to train a preliminary recognition model with fairly good performance, and then a high-quality benchmark can be obtained by automatically annotating the real-world structure diagrams using the preliminary model and then making few manual corrections. Finally, we experi
    
[^115]: 提高机器学习合成数据的真实性研究

    A Study on Improving Realism of Synthetic Data for Machine Learning. (arXiv:2304.12463v1 [cs.CV])

    [http://arxiv.org/abs/2304.12463](http://arxiv.org/abs/2304.12463)

    本文研究并评估了一个合成到真实的生成模型，将合成渲染转换为更真实的风格以适用于通用数据集，并通过下游感知任务来量化和定性地评估其性能。

    

    利用生成对抗性学习的合成到真实数据转换已经取得了重大的成功，以改进合成数据。然而，目前仍有限的研究专注于深度评估和比较通用机器学习用途的对抗性训练的合成数据。本文旨在训练和评估将合成渲染转化为更真实风格的合成到真实生成模型，并利用未标记的真实世界数据进行普通目的数据集的条件操作，通过定量和定性的评估指标以及定义下游的感知任务进行广泛的性能评估和比较。

    Synthetic-to-real data translation using generative adversarial learning has achieved significant success to improve synthetic data. Yet, there are limited studies focusing on deep evaluation and comparison of adversarial training on general-purpose synthetic data for machine learning. This work aims to train and evaluate a synthetic-to-real generative model that transforms the synthetic renderings into more realistic styles on general-purpose datasets conditioned with unlabeled real-world data. Extensive performance evaluation and comparison have been conducted through qualitative and quantitative metrics, and a defined downstream perception task.
    
[^116]: 稀疏私有LASSO逻辑回归

    Sparse Private LASSO Logistic Regression. (arXiv:2304.12429v1 [cs.LG])

    [http://arxiv.org/abs/2304.12429](http://arxiv.org/abs/2304.12429)

    本文提出了一种稀疏逻辑回归的差分隐私方法，保持硬零，通过训练一个非私有的LASSO逻辑回归模型决定最后的模型选择中恰当的私有化非零系数的数量。

    

    LASSO正则化的逻辑回归因其内置特征选择功能而非常有用，允许从部署中删除系数并产生稀疏解。虽然已经开发了LASSO逻辑回归的差分隐私版本，但通常会产生密集解，从而降低了LASSO惩罚的内在实用性。在本文中，我们提出了一种用于稀疏逻辑回归的差分隐私方法，该方法保持硬零。我们的关键见解是首先训练一个非私有的LASSO逻辑回归模型来确定在最终模型选择中使用的恰当私有化非零系数的数量。为了证明我们方法的性能，我们对合成和真实数据集进行了实验。

    LASSO regularized logistic regression is particularly useful for its built-in feature selection, allowing coefficients to be removed from deployment and producing sparse solutions. Differentially private versions of LASSO logistic regression have been developed, but generally produce dense solutions, reducing the intrinsic utility of the LASSO penalty. In this paper, we present a differentially private method for sparse logistic regression that maintains hard zeros. Our key insight is to first train a non-private LASSO logistic regression model to determine an appropriate privatized number of non-zero coefficients to use in final model selection. To demonstrate our method's performance, we run experiments on synthetic and real-world datasets.
    
[^117]: 将专家判断融入机器学习模型

    Incorporating Experts' Judgment into Machine Learning Models. (arXiv:2304.11870v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.11870](http://arxiv.org/abs/2304.11870)

    本文提出了一种新的框架，利用生成对抗网络确定未标记数据点在训练数据中的代表程度，再根据这个程度将专家的判断融入机器学习模型，以减轻预测结果与专家判断之间的冲突。

    

    机器学习模型在许多应用中预测结果非常成功。然而，在某些情况下，领域专家可能对预期结果有判断，这可能与机器学习模型的预测相冲突。其中一个主要原因是训练数据可能并不完全代表人群。本文提出了一个新的框架，旨在利用专家的判断来减轻冲突。我们框架的基本思想是，首先利用生成对抗网络确定未标记数据点在训练数据中的代表程度。然后，根据这个程度，我们将专家的判断融入机器学习模型，以改正其预测结果，其中代表程度越高，我们就越少地纳入专家的判断，反之亦然。我们进行了多次数字实验。

    Machine learning (ML) models have been quite successful in predicting outcomes in many applications. However, in some cases, domain experts might have a judgment about the expected outcome that might conflict with the prediction of ML models. One main reason for this is that the training data might not be totally representative of the population. In this paper, we present a novel framework that aims at leveraging experts' judgment to mitigate the conflict. The underlying idea behind our framework is that we first determine, using a generative adversarial network, the degree of representation of an unlabeled data point in the training data. Then, based on such degree, we correct the \textcolor{black}{machine learning} model's prediction by incorporating the experts' judgment into it, where the higher that aforementioned degree of representation, the less the weight we put on the expert intuition that we add to our corrected output, and vice-versa. We perform multiple numerical experimen
    
[^118]: 生物医学信号处理中的自回归模型

    Autoregressive models for biomedical signal processing. (arXiv:2304.11070v1 [eess.SP])

    [http://arxiv.org/abs/2304.11070](http://arxiv.org/abs/2304.11070)

    本文提出了一种新的自回归建模框架，通过超参数化损失函数来明确纳入数据不确定性，展示了该程序可以成功地去噪时间序列并成功重构系统参数。该范式可在神经科学的多种应用中使用。

    

    自回归模型是分析时间序列的常用工具，在计算神经科学和生物医学工程等领域广泛应用。在这些领域，数据来自于脑活动的测量等，数据存在测量误差和基础系统模型不确定性。因此，使用自回归模型估算器的标准信号处理可能存在偏差。本文提出了一个自回归建模框架，通过超参数化损失函数明确地纳入这些不确定性。为了优化该损失函数，我们提出了一种交替状态和参数估计算法。我们的工作表明，该程序能够成功去噪时间序列并成功重构系统参数。这种新的范式可以在神经科学的多种应用中使用，例如脑机接口数据分析和对疾病如癫痫和帕金森病中的大脑动态的更好理解。

    Autoregressive models are ubiquitous tools for the analysis of time series in many domains such as computational neuroscience and biomedical engineering. In these domains, data is, for example, collected from measurements of brain activity. Crucially, this data is subject to measurement errors as well as uncertainties in the underlying system model. As a result, standard signal processing using autoregressive model estimators may be biased. We present a framework for autoregressive modelling that incorporates these uncertainties explicitly via an overparameterised loss function. To optimise this loss, we derive an algorithm that alternates between state and parameter estimation. Our work shows that the procedure is able to successfully denoise time series and successfully reconstruct system parameters. This new paradigm can be used in a multitude of applications in neuroscience such as brain-computer interface data analysis and better understanding of brain dynamics in diseases such as
    
[^119]: CKmeans和FCKmeans：使用拥挤距离的Kmeans算法的两种确定性初始化过程 （arXiv：2304.09989v1 [cs.LG]）

    CKmeans and FCKmeans : Two Deterministic Initialization Procedures For Kmeans Algorithm Using Crowding Distance. (arXiv:2304.09989v1 [cs.LG])

    [http://arxiv.org/abs/2304.09989](http://arxiv.org/abs/2304.09989)

    本论文提出了两种新型的确定性初始化过程（CKmeans和FCKmeans）以改进Kmeans聚类，并且实验证明这些过程在聚类准确度方面优于传统的Kmeans和Kmeans++。

    

    本文提出了两种基于修改后的拥挤距离的K-means聚类的新型确定性初始化过程，分别称为CKmeans和FCKmeans。这些过程利用更密集的点作为初始质心。在多个数据集上进行的实验研究表明，所提出的方法在聚类准确度方面优于Kmeans和Kmeans ++。CKmeans和FCKmeans的有效性归因于它们基于修改后的拥挤距离选择更好的初始质心的能力。总的来说，所提出的方法为改进K-means聚类提供了一种有希望的替代方案。

    This paper presents two novel deterministic initialization procedures for K-means clustering based on a modified crowding distance. The procedures, named CKmeans and FCKmeans, use more crowded points as initial centroids. Experimental studies on multiple datasets demonstrate that the proposed approach outperforms Kmeans and Kmeans++ in terms of clustering accuracy. The effectiveness of CKmeans and FCKmeans is attributed to their ability to select better initial centroids based on the modified crowding distance. Overall, the proposed approach provides a promising alternative for improving K-means clustering.
    
[^120]: SemEval 2023 任务6: LegalEval -- 理解法律文本

    SemEval 2023 Task 6: LegalEval -- Understanding Legal Texts. (arXiv:2304.09548v1 [cs.CL])

    [http://arxiv.org/abs/2304.09548](http://arxiv.org/abs/2304.09548)

    SemEval 2023举办了LegalEval共享任务，即理解法律文本，包括 自动结构化和语义连贯化的法律文件（Task-A），法律命名实体识别（Task-B）以及自动预测法律案件结果和提供预测解释（Task-C）。26个团队提交了系统论文并在所有子任务中优于基准线，但仍有改进空间。

    

    在人口众多的国家，待处理的法律案件呈指数增长。有必要开发基于自然语言处理的技术，对法律文件进行处理和自动理解。为了促进在法律自然语言处理领域的研究，我们在 SemEval 2023 上组织了共享任务 LegalEval - 理解法律文本。LegalEval 任务有三个子任务：Task-A（修辞角色标记）是自动将法律文件结构化为语义连贯的单元，Task-B（法律命名实体识别）处理在法律文件中识别相关实体，而 Task-C（法院判决预测与解释）探索了自动预测法律案件结果以及提供预测解释的可能性。共有26个团队（分布在全球的约100名参与者）提交了系统论文。在每个子任务中，所提出的系统都优于基准线；但是，仍然有很大的改进空间。本文介绍了 LegalEval 任务的组织和细节，并概述了参与系统及其性能。

    In populous countries, pending legal cases have been growing exponentially. There is a need for developing NLP-based techniques for processing and automatically understanding legal documents. To promote research in the area of Legal NLP we organized the shared task LegalEval - Understanding Legal Texts at SemEval 2023. LegalEval task has three sub-tasks: Task-A (Rhetorical Roles Labeling) is about automatically structuring legal documents into semantically coherent units, Task-B (Legal Named Entity Recognition) deals with identifying relevant entities in a legal document and Task-C (Court Judgement Prediction with Explanation) explores the possibility of automatically predicting the outcome of a legal case along with providing an explanation for the prediction. In total 26 teams (approx. 100 participants spread across the world) submitted systems paper. In each of the sub-tasks, the proposed systems outperformed the baselines; however, there is a lot of scope for improvement. This pape
    
[^121]: 自然语言到SPARQL查询生成的复制机制综合评估

    A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])

    [http://arxiv.org/abs/2304.07772](http://arxiv.org/abs/2304.07772)

    本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。

    

    近年来，神经机器翻译（NMT）领域在SPARQL查询生成方面有了显著的增长。最近，将复制机制与传统的编码器-解码器架构相结合，并使用预训练的编码器-解码器，创造了新的性能基准。本文展示了大量的实验，复制并扩展了最近的基于NMT的SPARQL生成研究，比较了预训练和非预训练模型、问题注释格式以及对于非预训练和预训练模型使用复制机制的影响。我们的结果表明，对于非预训练模型和预训练模型，添加复制机制或使用问题注释都可以提高性能，并为三个流行数据集设置了新的基准。

    In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
    
[^122]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    
[^123]: 不同iable神经架构搜索中神经网络设计的高效自动化:一项概述研究(arXiv: 2304.05405v1 [cs.LG])

    Efficient Automation of Neural Network Design: A Survey on Differentiable Neural Architecture Search. (arXiv:2304.05405v1 [cs.LG])

    [http://arxiv.org/abs/2304.05405](http://arxiv.org/abs/2304.05405)

    本文综述了最近在不同iable神经架构搜索中的研究进展，提出了一种新的基于挑战的分类法，对DARTS方法的贡献和影响进行了讨论，并探讨了未来的研究方向。

    

    在过去的几年中，不同iable神经架构搜索（DNAS）迅速成为自动发现深度神经网络结构的流行方法。 这种崛起主要归功于DARTS，这是第一个重要的DNAS方法之一。 与基于强化学习或进化算法的以前的作品相比，DNAS速度快了数个数量级，并且使用的计算资源更少。 在这篇全面的综述中，我们专门关注DNAS并审查了该领域的最新方法。 此外，我们提出了一种基于挑战的分类法来分类DNAS方法。 我们还讨论了过去几年对DNAS带来的贡献以及其对全球NAS领域的影响。 最后，我们通过提供一些未来研究方向的见解来做出结论。

    In the past few years, Differentiable Neural Architecture Search (DNAS) rapidly imposed itself as the trending approach to automate the discovery of deep neural network architectures. This rise is mainly due to the popularity of DARTS, one of the first major DNAS methods. In contrast with previous works based on Reinforcement Learning or Evolutionary Algorithms, DNAS is faster by several orders of magnitude and uses fewer computational resources. In this comprehensive survey, we focus specifically on DNAS and review recent approaches in this field. Furthermore, we propose a novel challenge-based taxonomy to classify DNAS methods. We also discuss the contributions brought to DNAS in the past few years and its impact on the global NAS field. Finally, we conclude by giving some insights into future research directions for the DNAS field.
    
[^124]: 基于Transformer的渐进式自监督学习在异常检测和定位中的应用

    Incremental Self-Supervised Learning Based on Transformer for Anomaly Detection and Localization. (arXiv:2303.17354v1 [cs.CV])

    [http://arxiv.org/abs/2303.17354](http://arxiv.org/abs/2303.17354)

    该论文提出一种基于Transformer骨干网络的渐进式自监督学习方法，可用于图像异常检测和定位，其中第一阶段使用MAE模型进行正常图像的训练，第二阶段使用像素级数据增强技术来生成损坏的正常图像，最终通过像素重建误差矩阵和像素异常概率矩阵综合得到一个异常得分矩阵。

    

    在机器学习领域，对于图像数据中的异常检测和定位的研究，尤其是在工业缺陷检测等实际应用中引起了重视。虽然现有的方法主要依赖于卷积神经网络（CNN）作为骨干网络，但我们提出了一种基于Transformer骨干网络的创新方法。我们的方法采用了两阶段的渐进式学习策略。在第一阶段，我们仅使用正常图像对Masked Autoencoder （MAE）模型进行训练，在第二阶段，我们实现了像素级数据增强技术以生成已损坏的正常图像及其相应的像素标签。这个过程使得模型可以学习如何修复损坏的区域和分类每个像素的状态。最终，该模型产生一个像素重建误差矩阵和一个像素异常概率矩阵，这两个矩阵综合成一个异常得分矩阵，有效地用于图像异常检测。

    In the machine learning domain, research on anomaly detection and localization within image data has garnered significant attention, particularly in practical applications such as industrial defect detection. While existing approaches predominantly rely on Convolutional Neural Networks (CNN) as their backbone network, we propose an innovative method based on the Transformer backbone network. Our approach employs a two-stage incremental learning strategy. In the first stage, we train a Masked Autoencoder (MAE) model exclusively on normal images. Subsequently, in the second stage, we implement pixel-level data augmentation techniques to generate corrupted normal images and their corresponding pixel labels. This process enables the model to learn how to repair corrupted regions and classify the state of each pixel. Ultimately, the model produces a pixel reconstruction error matrix and a pixel anomaly probability matrix, which are combined to create an anomaly scoring matrix that effective
    
[^125]: 文本到图像扩散模型中的概念消融

    Ablating Concepts in Text-to-Image Diffusion Models. (arXiv:2303.13516v1 [cs.CV])

    [http://arxiv.org/abs/2303.13516](http://arxiv.org/abs/2303.13516)

    本论文提出了一种有效的方法，在不重新训练模型的情况下实现了预训练模型中的概念消融，可以消除文本到图像生成中的版权问题和样本记忆问题。

    

    大规模的文本到图像扩散模型具有强大的组合能力，可以生成高保真度的图片。然而，这些模型通常需要在数量庞大的网络数据上进行训练，往往包含有版权材料、授权图像和个人照片。此外，这些模型已经被发现能够模仿不同艺术家的风格或记住准确的训练样本。如何在不重新训练模型的情况下去除这些版权概念或图像？为了达成这个目标，我们提出了一种有效的方法，在预训练模型中实现概念消融，即防止生成目标概念。我们的算法学习如何匹配一个锚定概念对应的图像分布和与目标风格、实例或文本提示相关的图像分布，以防止模型根据其文本条件生成目标概念。广泛的实验证明，我们的方法可以成功地防止消融概念的生成。

    Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated conce
    
[^126]: 基于贝叶斯优化的函数组合方法及其在动态定价中的应用

    Bayesian Optimization for Function Compositions with Applications to Dynamic Pricing. (arXiv:2303.11954v1 [cs.LG])

    [http://arxiv.org/abs/2303.11954](http://arxiv.org/abs/2303.11954)

    本文提出了一种基于贝叶斯优化的函数组合方法，可以解决黑盒函数难以评估的问题，并在动态定价中得到应用。

    

    贝叶斯优化（BO）被用来找到黑盒函数的全局最优解。本文提出了一种针对函数组合的实用BO方法，其中组合的形式已知，但各组成函数难以评估。通过为每个黑盒函数建立独立的高斯过程（GP）模型，我们提出EI和UCB基于BO算法，并证明它们能够胜过传统BO和目前先进算法。我们展示了所提出的方法在收益管理中动态定价时的一种新颖应用，尤其适用于昂贵的需求函数。

    Bayesian Optimization (BO) is used to find the global optima of black box functions. In this work, we propose a practical BO method of function compositions where the form of the composition is known but the constituent functions are expensive to evaluate. By assuming an independent Gaussian process (GP) model for each of the constituent black-box function, we propose EI and UCB based BO algorithms and demonstrate their ability to outperform vanilla BO and the current state-of-art algorithms. We demonstrate a novel application of the proposed methods to dynamic pricing in revenue management when the underlying demand function is expensive to evaluate.
    
[^127]: 一种用于音频信号处理的内容自适应可学习时频表示法

    A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])

    [http://arxiv.org/abs/2303.10446](http://arxiv.org/abs/2303.10446)

    该论文提出了一种用于音频信号处理的内容自适应可学习时频表示法，通过学习卷积滤波器与变换器架构来将小的波形块投影到小的潜在维度上。

    

    我们提出了一个可学习的内容自适应前端，用于音频信号处理。在深度学习的现代出现之前，我们使用固定表示的、不可学习的前端，如谱图或梅尔谱图，带/不带神经结构。随着卷积架构支持ASR和声学场景理解等各种应用，转向可学习前端，即从头开始学习和优化特定任务所需的基础函数和权重。在没有卷积块的变形器架构中，线性层将小的波形块投影到小的潜在维度上，然后将它们馈送到变形器架构中。在这项工作中，我们提出了一种计算内容自适应学习时频表示的方法。

    We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
    
[^128]: 大型语言模型可能会具有意识吗？

    Could a Large Language Model be Conscious?. (arXiv:2303.07103v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.07103](http://arxiv.org/abs/2303.07103)

    本文分析了大型语言模型是否具有意识的可能性，目前的模型存在着意识的显著障碍，但未来十年随着障碍被克服，后继的大型语言模型可能会具有意识。

    

    最近普遍讨论了大型语言模型是否具有感知或意识。我们是否应该认真考虑这个想法？本文将分析支持和反对这个想法的最有力的理由。根据意识科学中的主流假设，目前的模型存在着意识的显著障碍，例如缺乏循环处理、全局的工作空间和统一的智能机构等等。与此同时，这些障碍在未来十年左右都可能被克服。作者得出的结论是，虽然目前大型语言模型具有意识的可能性较小，但我们应该认真考虑后继的大型语言模型在不久的将来可能会具有意识。

    There has recently been widespread discussion of whether large language models might be sentient or conscious. Should we take this idea seriously? I will break down the strongest reasons for and against. Given mainstream assumptions in the science of consciousness, there are significant obstacles to consciousness in current models: for example, their lack of recurrent processing, a global workspace, and unified agency. At the same time, it is quite possible that these obstacles will be overcome in the next decade or so. I conclude that while it is somewhat unlikely that current large language models are conscious, we should take seriously the possibility that successors to large language models may be conscious in the not-too-distant future.
    
[^129]: 临床BERTScore：临床环境下自动语音识别性能的改进度量

    Clinical BERTScore: An Improved Measure of Automatic Speech Recognition Performance in Clinical Settings. (arXiv:2303.05737v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2303.05737](http://arxiv.org/abs/2303.05737)

    本文提出了一种临床BERTScore（CBERTScore）度量，它比其他度量更严厉地惩罚临床相关的错误，更接近于临床医生对医学句子的偏好。作者还收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    The paper proposes a Clinical BERTScore (CBERTScore) metric for ASR in medical contexts, which penalizes clinically-relevant mistakes more than other metrics and aligns more closely with clinician preferences. The authors also collect a benchmark of clinician preferences on medical sentences and release it for the community to further develop clinically-aware ASR metrics.

    医学环境中的自动语音识别（ASR）有潜力节省时间，降低成本，提高报告准确性并减少医生的疲劳。然而，由于避免医学相关的转录错误的重要性，医疗行业采用这种技术的速度较慢。在这项工作中，我们提出了临床BERTScore（CBERTScore），这是一种ASR度量，它比其他度量（WER、BLUE、METEOR等）更严厉地惩罚临床相关的错误。我们证明了这个度量更接近于临床医生对医学句子的偏好，有时差距很大。我们收集了13个临床医生对149个现实医学句子的偏好基准，称为临床转录偏好基准（CTP），证明CBERTScore更接近于临床医生的偏好，并将基准发布给社区以进一步开发具有临床意识的ASR度量。

    Automatic Speech Recognition (ASR) in medical contexts has the potential to save time, cut costs, increase report accuracy, and reduce physician burnout. However, the healthcare industry has been slower to adopt this technology, in part due to the importance of avoiding medically-relevant transcription mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR metric that penalizes clinically-relevant mistakes more than others. We demonstrate that this metric more closely aligns with clinician preferences on medical sentences as compared to other metrics (WER, BLUE, METEOR, etc), sometimes by wide margins. We collect a benchmark of 13 clinician preferences on 149 realistic medical sentences called the Clinician Transcript Preference benchmark (CTP), demonstrate that CBERTScore more closely matches what clinicians prefer, and release the benchmark for the community to further develop clinically-aware ASR metrics.
    
[^130]: KGNv2: 基于关键点的RGB-D输入六自由度抓取合成中的尺度和姿态分离

    KGNv2: Separating Scale and Pose Prediction for Keypoint-based 6-DoF Grasp Synthesis on RGB-D input. (arXiv:2303.05617v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.05617](http://arxiv.org/abs/2303.05617)

    本文提出了一种基于关键点的RGB-D输入的六自由度抓取姿态合成方法，既可以从关键点检测中预测抓取姿态，也可以预测相对于相机的尺度，实验结果表明其优越性。

    

    本文提出了一种6自由度抓取姿态合成方法，该方法基于关键点从2D/2.5D输入中进行。在前期研究中，基于关键点的抓取检测器已经证明了良好的结果，其中彩色图像提供的额外视觉信息弥补了嘈杂的深度感知。然而，它严重依赖于准确预测图像空间中的关键点位置。因此，我们设计了一种新的抓取生成网络，既可以从关键点检测中预测抓取姿态，也可以预测相对于相机的尺度。另外，我们还重新设计了关键点输出空间，以减轻关键点预测噪声对透视n点(PnP)算法的负面影响。实验结果表明，所提出的方法在性能上比基线表现出了显著的优越性，验证了我们方法的有效性。最后，尽管是在简单的合成对象上训练的，我们的方法也可以用于真实物体上的抓取。

    We propose a new 6-DoF grasp pose synthesis approach from 2D/2.5D input based on keypoints. Keypoint-based grasp detector from image input has demonstrated promising results in the previous study, where the additional visual information provided by color images compensates for the noisy depth perception. However, it relies heavily on accurately predicting the location of keypoints in the image space. In this paper, we devise a new grasp generation network that reduces the dependency on precise keypoint estimation. Given an RGB-D input, our network estimates both the grasp pose from keypoint detection as well as scale towards the camera. We further re-design the keypoint output space in order to mitigate the negative impact of keypoint prediction noise to Perspective-n-Point (PnP) algorithm. Experiments show that the proposed method outperforms the baseline by a large margin, validating the efficacy of our approach. Finally, despite trained on simple synthetic objects, our method demons
    
[^131]: 多级扩散：图像生成的无限维度基于得分的扩散模型

    Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04772](http://arxiv.org/abs/2303.04772)

    本文介绍了无限维度得分扩散模型在多个分辨率水平上的离散化方法，并使用多级扩散算法在多个分辨率上高效地学习。实证表明，该模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本，并可以生成不同分辨率的图像并处理矩形域。

    

    基于得分的扩散模型是近年来图像生成的最先进方法之一。现有的基于得分的扩散模型通常在有限维度设置中表述，其中图像被视为具有有限尺寸的张量。本文在无限维度设置中开发了基于得分的扩散模型，即我们将训练数据建模为支撑在矩形域上的函数。除了追求在更高分辨率下生成图像之外，我们的主要动机是创建一个良好定义的无限维度学习问题，以便可以在多个分辨率水平上一致地离散化它。我们希望获得能够横跨不同分辨率级别的扩散模型，并提高训练过程的效率。我们展示了如何克服当前基于得分的扩散模型在无限维度设置中存在的两个缺点。首先，我们修改了前向过程以确保在无限维度设置中潜在分布是良好定义的。其次，我们提出了一种多级扩散算法，使我们能够在多个分辨率上高效地学习。我们实证表明，我们的多级模型在相同或更高分辨率下产生比传统基于得分的扩散模型更高质量的样本。此外，我们的方法可以无缝地生成不同分辨率的图像并处理矩形域。

    Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
    
[^132]: 迭代修正的外推控制序列生成

    Extrapolative Controlled Sequence Generation via Iterative Refinement. (arXiv:2303.04562v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04562](http://arxiv.org/abs/2303.04562)

    本文提出了一种名为ICE的新方法来解决外推控制序列生成问题，该方法使用迭代控制编辑技术，能够在自动设计领域，特别是药物研究领域取得较优的性能表现。

    

    本研究探讨了外推控制生成问题，即生成属性值超出训练数据范围的序列。在自动设计领域，尤其是药物研究领域，这个任务至关重要，目标是设计出比现有序列更好（例如更稳定）的新型蛋白质。因此，按照定义，目标序列及其属性值超出训练分布，挑战现有直接生成目标序列方法。本研究提出了迭代控制外推（ICE）方法，通过迭代地对序列进行局部编辑来实现外推。我们使用合成的序列对对模型进行训练，演示微小的属性值改进。自然语言任务（情感分析）和两个蛋白质工程任务（ACE2稳定性和AAV适应性）的结果表明，ICE方法明显优于现有的最先进方法。

    We study the problem of extrapolative controlled generation, i.e., generating sequences with attribute values beyond the range seen in training. This task is of significant importance in automated design, especially drug discovery, where the goal is to design novel proteins that are \textit{better} (e.g., more stable) than existing sequences. Thus, by definition, the target sequences and their attribute values are out of the training distribution, posing challenges to existing methods that aim to directly generate the target sequence. Instead, in this work, we propose Iterative Controlled Extrapolation (ICE) which iteratively makes local edits to a sequence to enable extrapolation. We train the model on synthetically generated sequence pairs that demonstrate small improvement in the attribute value. Results on one natural language task (sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV fitness) show that ICE considerably outperforms state-of-the-art approach
    
[^133]: 联邦强化学习中的本地环境毒化攻击

    Local Environment Poisoning Attacks on Federated Reinforcement Learning. (arXiv:2303.02725v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02725](http://arxiv.org/abs/2303.02725)

    本文提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。

    

    联邦学习已成为解决传统强化学习任务的热门工具。多代理结构解决了传统强化学习中数据需求大的主要问题，而联邦机制保护了各个代理个体的数据隐私。然而，联邦机制也会暴露系统面临恶意代理的毒化攻击。本工作提出了第一个广义框架，将FRL的毒化攻击视为限制预算的优化问题，并设计了一个可以应用于基于策略的FRL的毒化攻击协议，并通过训练一对私人评价者和公共评价者将其扩展到使用演员-评论家作为本地RL算法的FRL。我们也讨论了从FL继承的一种传统防御策略以减轻这种风险。我们通过进行实验来验证我们的毒化攻击的有效性。

    Federated learning (FL) has become a popular tool for solving traditional Reinforcement Learning (RL) tasks. The multi-agent structure addresses the major concern of data-hungry in traditional RL, while the federated mechanism protects the data privacy of individual agents. However, the federated mechanism also exposes the system to poisoning by malicious agents that can mislead the trained policy. Despite the advantage brought by FL, the vulnerability of Federated Reinforcement Learning (FRL) has not been well-studied before. In this work, we propose the first general framework to characterize FRL poisoning as an optimization problem constrained by a limited budget and design a poisoning protocol that can be applied to policy-based FRL and extended to FRL with actor-critic as a local RL algorithm by training a pair of private and public critics. We also discuss a conventional defense strategy inherited from FL to mitigate this risk. We verify our poisoning effectiveness by conducting 
    
[^134]: R-U-SURE？通过最大化不同用户意图下的效用来实现考虑不确定性的代码建议

    R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents. (arXiv:2303.00732v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00732](http://arxiv.org/abs/2303.00732)

    R-U-SURE是一种考虑不确定性的代码建议方法，利用生成模型的随机样本作为代理生成提高效用的实现，并结合了决策论模型、最小贝叶斯风险解码、双重分解和决策图，能够在不需要完整代码的情况下生成结构化不确定性摘要。

    

    大型语言模型在预测结构化文本（如代码）方面表现出色，但是它们通常会在其输出中引入错误和虚假信息。当用于辅助软件开发人员时，这些模型可能会犯错，需要用户返回更正，或更糟糕的是，可能会引入用户可能会完全忽略的微妙的错误。我们提出了一种名为R-U-SURE的方法，它基于决策论模型，利用来自生成模型的随机样本作为最终用户未观察到的可能意图的代理，构建考虑不确定性的建议。我们的技术结合了最小贝叶斯风险解码、双重分解和决策图，以便仅通过对任意代码生成模型的样本访问和可选的AST解析器，高效地生成结构化不确定性摘要。我们在三个开发者辅助任务上展示了R-U-SURE，并显示它可以适用于不同的用户。

    Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely. We propose Randomized Utility-driven Synthesis of Uncertain REgions (R-U-SURE), an approach for building uncertainty-aware suggestions based on a decision-theoretic model of goal-conditioned utility, using random samples from a generative model as a proxy for the unobserved possible intents of the end user. Our technique combines minimum-Bayes-risk decoding, dual decomposition, and decision diagrams in order to efficiently produce structured uncertainty summaries, given only sample access to an arbitrary generative model of code and an optional AST parser. We demonstrate R-U-SURE on three developer-assistance tasks, and show that it can be applied different user i
    
[^135]: 基于聚类技术的灵活能源社区目标需求响应

    Targeted demand response for flexible energy communities using clustering techniques. (arXiv:2303.00186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00186](http://arxiv.org/abs/2303.00186)

    本研究探讨了使用机器学习算法中的聚类技术设计并执行需求响应（DR）计划的可行性，目的是改变分布式能源社区内供应者的消费行为，以最小化反向功率流和削减系统范围内的功峰需求。

    

    本研究探讨了使用聚类技术为商业和住宅社区的能量供应者设计和执行需求响应（DR）计划的可能性。该计划的目的是改变意大利分布式能源社区内的供应者的消费行为。这种聚合旨在：a）最小化在主要变电站处产生的反向功率流，该功率流在当地电网中的太阳能电池的发电量超过消耗时会发生; b）削减系统范围内的功峰需求，该需求通常发生在傍晚时分。在聚类阶段，我们采用了三种热门的电负荷聚类机器学习算法-即k-means，k-medoids和一种聚合层次聚类-alongside两种不同的距离度量-即欧几里得距离和受限动态时间扭曲（DTW）。我们使用多个验证度量来评估这些方法，包括一项新颖的指标-即峰值性能评分（PPS）

    The present study explores the use of clustering techniques for the design and implementation of a demand response (DR) program for commercial and residential prosumers. The goal of the program is to alter the consumption behavior of the prosumers pertaining to a distributed energy community in Italy. This aggregation aims to: a) minimize the reverse power flow at the primary substation, that occurs when generation from solar panels in the local grid exceeds consumption, and b) shave the system wide peak demand, that typically occurs during the hours of late afternoon. Regarding the clustering stage, three popular machine learning algorithms for electrical load clustering are employed -namely k-means, k-medoids and an agglomerative hierarchical clustering- alongside two different distance measures -namely euclidean and constrained dynamic time warping (DTW). We evaluate the methods using multiple validation metrics including a novel metric -namely peak performance score (PPS)- that we 
    
[^136]: AccelTran：一种稀疏感知加速器，用于动态推理中的Transformer

    AccelTran: A Sparsity-Aware Accelerator for Dynamic Inference with Transformers. (arXiv:2302.14705v2 [cs.AR] UPDATED)

    [http://arxiv.org/abs/2302.14705](http://arxiv.org/abs/2302.14705)

    本论文提出了一种稀疏感知加速器AccelTran，用于动态推理中的Transformer，通过运行时修剪激活并优化数据流实现了高性能和能量效率的平衡，相较于其他现有加速器，AccelTran在模型尺寸和内存带宽需求较低的情况下实现了平均1.9倍的加速比，同时保持了推理精度。

    

    基于Self-attention的Transformer模型在自然语言处理领域取得了巨大的成功。尽管它们高效，但由于它的二次计算复杂度和大的激活大小，加速Transformer是具有挑战性的。现有的Transformer加速器试图修剪其令牌以减少内存访问，但计算开销很高。此外，之前的工作直接操作参与注意力操作的大型矩阵，这限制了硬件利用率。为了应对这些挑战，本文提出了一种新的动态推理方案DynaTran，在运行时修剪激活，开销较低，大大减少了无效操作的数量，提高了Transformer推理的吞吐量。我们进一步建议使用沿着Transformer操作平铺矩阵的各种数据流来提高数据重用率，从而实现更高的能量效率。为了有效实现这些方法，我们提出了AccelTran，一种稀疏感知加速器，利用平铺感知硬件并行和一种新的修剪方案。与现有技术最先进的加速器相比，AccelTran在模型尺寸和内存带宽需求较低的情况下实现了平均加速比为1.9倍的竞争性推理精度，同时保持了大型和小型Transformer模型的竞争性推理精度。

    Self-attention-based transformer models have achieved tremendous success in the domain of natural language processing. Despite their efficacy, accelerating the transformer is challenging due to its quadratic computational complexity and large activation sizes. Existing transformer accelerators attempt to prune its tokens to reduce memory access, albeit with high compute overheads. Moreover, previous works directly operate on large matrices involved in the attention operation, which limits hardware utilization. In order to address these challenges, this work proposes a novel dynamic inference scheme, DynaTran, which prunes activations at runtime with low overhead, substantially reducing the number of ineffectual operations. This improves the throughput of transformer inference. We further propose tiling the matrices in transformer operations along with diverse dataflows to improve data reuse, thus enabling higher energy efficiency. To effectively implement these methods, we propose Acce
    
[^137]: 预训练基础模型综述：从BERT到ChatGPT的历程

    A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.09419](http://arxiv.org/abs/2302.09419)

    本文全面回顾了预训练基础模型的最新研究进展和发展历程，包括它们的架构、培训目标、预培训任务、微调策略和评估。同时，讨论了其局限性和未来研究方向。

    

    预训练基础模型(PFMs)被认为是各种不同数据模态下游任务的基础。PFM(例如BERT、ChatGPT和GPT-4)在大规模数据上进行训练，为各种下游应用提供了合理的参数初始化。BERT从转换器中学习双向编码器表示，这些模型作为上下文语言模型在大型数据集上进行训练。类似地，生成式预训练变压器(GPT)方法采用转换器作为特征提取器，并采用自回归范式在大型数据集上进行训练。最近，ChatGPT在大语言模型中展现了令人兴奋的成功，它采用自回归式语言模型，可以进行零射击或少射击提示。PFM的卓越成就为各种AI领域带来了重大突破。许多研究提出了不同的方法，提高了对更新调查的需求。本研究全面回顾了PFMs的最新进展，包括它们的架构、培训目标、预培训任务、微调策略和评估。此外，我们还讨论了PFMs的局限性和未来潜在的研究方向。

    Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
    
[^138]: 三维感知的条件图像合成

    3D-aware Conditional Image Synthesis. (arXiv:2302.08509v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.08509](http://arxiv.org/abs/2302.08509)

    我们提出了一个三维感知的条件生成模型，通过给定的2D标签图合成相应的图像，并扩展了条件生成模型来实现明确的三维用户控制；我们还建立了一个交互式系统，允许用户随意编辑标签图并生成输出。

    

    我们提出了pix2pix3D，这是一种三维感知的条件生成模型，用于可控的逼真图像合成。在给定2D标签图的情况下，如分割或边缘图，我们的模型学习从不同视角合成相应的图像。为了实现明确的三维用户控制，我们使用神经光辐射场扩展了条件生成模型。在给定普遍可用的单眼图像和标签映射对的情况下，我们的模型学习为每个三维点分配标签，除颜色和密度外，这使它能够同时呈现图像和像素对齐的标签映射。最后，我们构建了一个交互式系统，允许用户从任何视角编辑标签映射并相应生成输出。

    We propose pix2pix3D, a 3D-aware conditional generative model for controllable photorealistic image synthesis. Given a 2D label map, such as a segmentation or edge map, our model learns to synthesize a corresponding image from different viewpoints. To enable explicit 3D user control, we extend conditional generative models with neural radiance fields. Given widely-available monocular images and label map pairs, our model learns to assign a label to every 3D point in addition to color and density, which enables it to render the image and pixel-aligned label map simultaneously. Finally, we build an interactive system that allows users to edit the label map from any viewpoint and generate outputs accordingly.
    
[^139]: Sphericart中的球谐函数快速计算方法

    Fast evaluation of spherical harmonics with sphericart. (arXiv:2302.08381v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2302.08381](http://arxiv.org/abs/2302.08381)

    该论文提出了一种sphericart库中实现的，高效而优雅的算法来评估实值球谐函数，其中具有现有方案的许多理想特性，并允许以数值稳定和计算效率高的方式计算笛卡尔导数。

    

    球谐函数是在球面上展开函数的平滑、正交和对称适应性基础，它们在物理化学、地质学、大气科学、信号处理、计算机图形学等不同科学和技术领域中被常规使用。最近，它们已成为在几何机器学习中旋转等变模型的关键组成部分，包括应用于分子和材料的原子级建模。我们提出了一种优雅而高效的算法来评估实值球谐函数。我们的构造具有现有方案的许多理想特性，并允许以数值稳定和计算效率高的方式计算笛卡尔导数。为了方便使用，我们在sphericart中实现了这个算法，这是一个快速的C++库，还提供了C绑定、Python API和一个包括GPU核心的PyTorch实现。

    Spherical harmonics provide a smooth, orthogonal, and symmetry-adapted basis to expand functions on a sphere, and they are used routinely in physical and theoretical chemistry as well as in different fields of science and technology, from geology and atmospheric sciences to signal processing and computer graphics. More recently, they have become a key component of rotationally equivariant models in geometric machine learning, including applications to atomic-scale modeling of molecules and materials. We present an elegant and efficient algorithm for the evaluation of the real-valued spherical harmonics. Our construction features many of the desirable properties of existing schemes and allows to compute Cartesian derivatives in a numerically stable and computationally efficient manner. To facilitate usage, we implement this algorithm in sphericart, a fast C++ library which also provides C bindings, a Python API, and a PyTorch implementation that includes a GPU kernel.
    
[^140]: 缩小可用性差距：隐马尔可夫模型谱学习的理论与方法学进展

    Bridging the Usability Gap: Theoretical and Methodological Advances for Spectral Learning of Hidden Markov Models. (arXiv:2302.07437v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.07437](http://arxiv.org/abs/2302.07437)

    本文研究了隐马尔可夫模型谱学习中存在的问题，并提出了解决方案，包括提供了SHMM似然估计的误差渐近分布、提出投影SHMM算法可以减轻误差传播问题、并开发了SHMM和PSHMM的在线学习变体以适应潜在的非平稳性。研究结果表明PSHMM具有更好的性能表现。

    

    Baum-Welch（B-W）算法是推断隐马尔可夫模型(HMM)最广泛接受的方法。 然而，它很容易陷入局部最优，而且对于许多实时应用来说速度太慢。文献中提出了一种基于矩法（MOM）的HMM的谱学习（SHMM），旨在克服这些障碍。尽管有这样的承诺，但SHMM的渐近理论一直很难得到，而SHMM的长期性能可能会由于误差的无限传播而降低。在本文中，我们(1)提供了SHMM似然估计的近似误差的渐近分布，(2)提出了一种新算法称为投影SHMM（PSHMM），它可以减轻误差传播问题，(3)开发了SHMM和PSHMM的在线学习变体，以适应潜在的非平稳性。我们在模拟数据和来自真实世界应用的数据上比较了SHMM、PSHMM和B-W算法的性能。

    The Baum-Welch (B-W) algorithm is the most widely accepted method for inferring hidden Markov models (HMM). However, it is prone to getting stuck in local optima, and can be too slow for many real-time applications. Spectral learning of HMMs (SHMM), based on the method of moments (MOM) has been proposed in the literature to overcome these obstacles. Despite its promises, asymptotic theory for SHMM has been elusive, and the long-run performance of SHMM can degrade due to unchecked propagation of error. In this paper, we (1) provide an asymptotic distribution for the approximate error of the likelihood estimated by SHMM, (2) propose a novel algorithm called projected SHMM (PSHMM) that mitigates the problem of error propagation, and (3) develop online learning variants of both SHMM and PSHMM that accommodate potential nonstationarity. We compare the performance of SHMM with PSHMM and estimation through the B-W algorithm on both simulated data and data from real world applications, and fin
    
[^141]: 自私行为下的劫匪社交学习

    Bandit Social Learning: Exploration under Myopic Behavior. (arXiv:2302.07425v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.07425](http://arxiv.org/abs/2302.07425)

    该论文研究了自私行为下的劫匪社交学习问题，发现存在一种探索激励权衡，即武器探索和社交探索之间的权衡，受到代理的短视行为的限制会加剧这种权衡，并导致遗憾率与代理数量成线性关系。

    

    我们研究了一种社交学习动态，其中代理按照简单的多臂劫匪协议共同行动。代理以顺序方式到达，选择武器并接收相关奖励。每个代理观察先前代理的完整历史记录（武器和奖励），不存在私有信号。尽管代理共同面临开发和利用的探索折衷，但每个代理人都是一见钟情的，无需考虑探索。我们允许一系列与（参数化）置信区间一致的自私行为，包括“无偏”行为和各种行为偏差。虽然这些行为的极端版本对应于众所周知的劫匪算法，但我们证明了更温和的版本会导致明显的探索失败，因此遗憾率与代理数量成线性关系。我们通过分析“温和乐观”的代理提供匹配的遗憾上界。因此，我们建立了两种类型的探索激励之间的基本权衡：武器探索是固有于劫匪问题的，只受当前代理的行动影响，而社交探索是由先前代理行为驱动的，因此有利于未来代理。由于代理的短视行为限制了社交探索，这种权衡被加剧。

    We study social learning dynamics where the agents collectively follow a simple multi-armed bandit protocol. Agents arrive sequentially, choose arms and receive associated rewards. Each agent observes the full history (arms and rewards) of the previous agents, and there are no private signals. While collectively the agents face exploration-exploitation tradeoff, each agent acts myopically, without regards to exploration. Motivating scenarios concern reviews and ratings on online platforms.  We allow a wide range of myopic behaviors that are consistent with (parameterized) confidence intervals, including the "unbiased" behavior as well as various behaviorial biases. While extreme versions of these behaviors correspond to well-known bandit algorithms, we prove that more moderate versions lead to stark exploration failures, and consequently to regret rates that are linear in the number of agents. We provide matching upper bounds on regret by analyzing "moderately optimistic" agents.  As a
    
[^142]: 拉普拉斯有界深度神经网络的直接参数化

    Direct Parameterization of Lipschitz-Bounded Deep Networks. (arXiv:2301.11526v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11526](http://arxiv.org/abs/2301.11526)

    本文提出了一种直接参数化的深度神经网络，其具有拉普拉斯界限，通过标准梯度方法进行训练，避免了计算密集型的投影或障碍项。

    

    本文引入了一种新的深度神经网络参数化方式（全连接和卷积网络），具有有限灵敏度的拉普拉斯界限。与SDP方法不同的是，我们提供了一个"直接"参数化方式，并通过标准的梯度方法进行训练，而不需要任何计算密集型的投影或障碍项。

    This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed Lipschitz bounds, i.e. limited sensitivity to perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program (SDP), which does not scale to large models. In contrast to the SDP approach, we provide a ``direct'' parameterization, i.e. a smooth mapping from $\mathbb R^N$ onto the set of weights of Lipschitz-bounded networks. This enables training via standard gradient methods, without any computationally intensive projections or barrier terms. The new parameterization can equivalently be thought of as either a new layer type (the \textit{sandwich layer}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. Finally, the comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on
    
[^143]: 图形转换器方法针对变化动态内容识别仇恨言论的定性分析

    Qualitative Analysis of a Graph Transformer Approach to Addressing Hate Speech: Adapting to Dynamically Changing Content. (arXiv:2301.10871v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10871](http://arxiv.org/abs/2301.10871)

    本研究探讨了一种利用图形转换器网络和基于BERT的自然语言处理模型，结合注意力机制，预测社交媒体中仇恨言论的方法。该方法通过考虑帖子后续的讨论来成功检测出可能出现仇恨言论的情况。研究还探讨了所提出模型的有效性和不足，以及未来可以将其扩展到更全面的方向。

    

    本文探讨了一种利用图形转换器网络和基于BERT的自然语言处理模型，结合注意力机制，预测社交媒体中仇恨言论的方法。该方法通过考虑帖子后续的讨论来成功检测出可能出现仇恨言论的情况。本文对该方法进行了详细的定性分析，探讨了与竞争方法相比在哪些场景下有最优表现以及面临的挑战。同时，本文还探讨了当前社交媒体中存在的恶意图片等各种帖子类型，提出了扩展模型的思路。关键的洞见在于该方法注重对上下文概念的推理，因此该方法具备很大的前景。

    Our work advances an approach for predicting hate speech in social media, drawing out the critical need to consider the discussions that follow a post to successfully detect when hateful discourse may arise. Using graph transformer networks, coupled with modelling attention and BERT-level natural language processing, our approach can capture context and anticipate upcoming anti-social behaviour. In this paper, we offer a detailed qualitative analysis of this solution for hate speech detection in social networks, leading to insights into where the method has the most impressive outcomes in comparison with competitors and identifying scenarios where there are challenges to achieving ideal performance. Included is an exploration of the kinds of posts that permeate social media today, including the use of hateful images. This suggests avenues for extending our model to be more comprehensive. A key insight is that the focus on reasoning about the concept of context positions us well to be a
    
[^144]: 多核在线学习的八卦与量化

    Gossiped and Quantized Online Multi-Kernel Learning. (arXiv:2301.09848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.09848](http://arxiv.org/abs/2301.09848)

    本文扩展了在线多核学习的研究，提出了一种八卦算法，证明了其在非完全连接的网络环境下可以达到亚线性后悔，具有较好的性能表现。

    

    在很少有先验信息可用且无法进行集中学习的在线核学习中，过去的研究表明，只要网络中的每对节点都可以通信（即通信网络是完全图），分布式和在线多核学习就会提供亚线性后悔。此外，为了管理通信负载（通常是性能瓶颈），节点之间的通信可以量化。该文将这些结果扩展到非完全连接的图上，这在无线传感器网络中经常发生。为了解决这个挑战，我们提出了一种八卦算法，并提供了它达到亚线性后悔的证明。使用真实数据集的实验证实了我们的发现。

    In instances of online kernel learning where little prior information is available and centralized learning is unfeasible, past research has shown that distributed and online multi-kernel learning provides sub-linear regret as long as every pair of nodes in the network can communicate (i.e., the communications network is a complete graph). In addition, to manage the communication load, which is often a performance bottleneck, communications between nodes can be quantized. This letter expands on these results to non-fully connected graphs, which is often the case in wireless sensor networks. To address this challenge, we propose a gossip algorithm and provide a proof that it achieves sub-linear regret. Experiments with real datasets confirm our findings.
    
[^145]: 带干预的基于分数的因果表征学习

    Score-based Causal Representation Learning with Interventions. (arXiv:2301.08230v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2301.08230](http://arxiv.org/abs/2301.08230)

    本文研究了当潜在因果变量通过未知线性转换间接观察时的因果表征学习问题。其充分条件确保了干预效果可以从分数的变化中正确检测出来，并利用最小化分数函数变化的关键特性完美恢复有效变换。

    

    本文研究了当潜在因果变量通过未知线性转换间接观察时的因果表征学习问题。建立了DAG重构的充分条件，并表明潜在空间中的大类非线性模型满足这些条件，确保了干预效果可以从分数的变化中正确检测出来。利用最小化分数函数变化的关键特性，可以完美恢复有效变换。

    This paper studies the causal representation learning problem when the latent causal variables are observed indirectly through an unknown linear transformation. The objectives are: (i) recovering the unknown linear transformation (up to scaling) and (ii) determining the directed acyclic graph (DAG) underlying the latent variables. Sufficient conditions for DAG recovery are established, and it is shown that a large class of non-linear models in the latent space (e.g., causal mechanisms parameterized by two-layer neural networks) satisfy these conditions. These sufficient conditions ensure that the effect of an intervention can be detected correctly from changes in the score. Capitalizing on this property, recovering a valid transformation is facilitated by the following key property: any valid transformation renders latent variables' score function to necessarily have the minimal variations across different interventional environments. This property is leveraged for perfect recovery of 
    
[^146]: 简单的数据收集和标记方法限制了Twitter机器人检测基准数据集的实用性。

    Simplistic Collection and Labeling Practices Limit the Utility of Benchmark Datasets for Twitter Bot Detection. (arXiv:2301.07015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07015](http://arxiv.org/abs/2301.07015)

    本论文研究了机器人检测中现有数据集的局限性，并提出了简单的决策规则，进一步揭示了数据集收集和标记的限制对机器人检测准确性的影响。

    

    准确的机器人检测对于在线平台的安全和完整性至关重要。本论文提出了机器学习工具检测机器人的问题，特别关注现有数据集标注问题。通过研究现有数据集的局限性，本文提出了简单的决策规则，展示了数据集收集和标记的限制对机器人检测的影响。

    Accurate bot detection is necessary for the safety and integrity of online platforms. It is also crucial for research on the influence of bots in elections, the spread of misinformation, and financial market manipulation. Platforms deploy infrastructure to flag or remove automated accounts, but their tools and data are not publicly available. Thus, the public must rely on third-party bot detection. These tools employ machine learning and often achieve near perfect performance for classification on existing datasets, suggesting bot detection is accurate, reliable and fit for use in downstream applications. We provide evidence that this is not the case and show that high performance is attributable to limitations in dataset collection and labeling rather than sophistication of the tools. Specifically, we show that simple decision rules -- shallow decision trees trained on a small number of features -- achieve near-state-of-the-art performance on most available datasets and that bot detec
    
[^147]: 神经辐射场代码本。

    Neural Radiance Field Codebooks. (arXiv:2301.04101v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.04101](http://arxiv.org/abs/2301.04101)

    某项研究提出了一种新的方法 NRC，能够学习复杂场景、实现对象为中心的表示、并能够自动分割，在THOR中的对象导航中表现出良好的转移能力。

    

    世界的组合表示是实现高级场景理解和有效转移至下游任务的有希望的一步。为了实现这个目标，我们通过新颖的视图重构方法引入了神经辐射场代码本(NRC)，这是一种学习对象为中心的表示的可扩展方法。NRC学习使用对象编码字典通过体积渲染器解码以从新视图重建场景。这使得能够发现在场景中反复出现的视觉和几何模式，并且可转移到下游任务。我们展示了NRC表示在THOR中的对象导航中的良好转移能力，成功率比2D和3D表示学习方法高出3.1%。我们证明了我们的方法能够比之前的方法更好地执行更复杂的合成（THOR）和真实场景（NYU Depth）的无监督分割(29%)。

    Compositional representations of the world are a promising step towards enabling high-level scene understanding and efficient transfer to downstream tasks. Learning such representations for complex scenes and tasks remains an open challenge. Towards this goal, we introduce Neural Radiance Field Codebooks (NRC), a scalable method for learning object-centric representations through novel view reconstruction. NRC learns to reconstruct scenes from novel views using a dictionary of object codes which are decoded through a volumetric renderer. This enables the discovery of reoccurring visual and geometric patterns across scenes which are transferable to downstream tasks. We show that NRC representations transfer well to object navigation in THOR, outperforming 2D and 3D representation learning methods by 3.1% success rate. We demonstrate that our approach is able to perform unsupervised segmentation for more complex synthetic (THOR) and real scenes (NYU Depth) better than prior methods (29% 
    
[^148]: 基于句子Transformer进行多方面的可解释感知关系预测

    Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer. (arXiv:2301.01664v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.01664](http://arxiv.org/abs/2301.01664)

    本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，提高基于句子Transformer的知识推理句子Transformer的性能，从而在KG中实现了多方面的可解释感知关系预测。

    

    最近的知识图谱（KGs）研究表明，基于预训练语言模型的基于路径的方法在提供感知和可解释关系预测方面表现良好。本文提出了关系路径覆盖率和关系路径置信度的概念，以在模型训练之前过滤出不可靠的路径，以提高模型性能。此外，我们提出了“知识推理句子Transformer”（KRST）来预测KG中的感知关系。 KRST被设计为在KG中编码提取出的可靠路径，使我们能够正确地聚类路径并提供多方面的解释。我们在三个真实数据集上进行了广泛的实验。实验结果表明，与SOTA模型相比，KRST在大多数感知和感知测试用例（6个中的4个）以及12个few-shot测试用例中的11个上取得了最佳性能。

    Recent studies on knowledge graphs (KGs) show that path-based methods empowered by pre-trained language models perform well in the provision of inductive and explainable relation predictions. In this paper, we introduce the concepts of relation path coverage and relation path confidence to filter out unreliable paths prior to model training to elevate the model performance. Moreover, we propose Knowledge Reasoning Sentence Transformer (KRST) to predict inductive relations in KGs. KRST is designed to encode the extracted reliable paths in KGs, allowing us to properly cluster paths and provide multi-aspect explanations. We conduct extensive experiments on three real-world datasets. The experimental results show that compared to SOTA models, KRST achieves the best performance in most transductive and inductive test cases (4 of 6), and in 11 of 12 few-shot test cases.
    
[^149]: 饥饿的河马：基于状态空间模型的语言建模方法

    Hungry Hungry Hippos: Towards Language Modeling with State Space Models. (arXiv:2212.14052v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.14052](http://arxiv.org/abs/2212.14052)

    本文针对SSMs在语言建模上表现不足以及硬件利用率低下的问题，提出了一种新的SSM层H3，并将其与建模关注机制相结合，通过硬件优化实现了语言建模基准的最新性能，突出SSMs在语言建模中的潜力。

    

    状态空间模型（SSMs）在某些模态下表现出卓越的序列建模性能，但在语言建模方面表现不足。此外，尽管SSMs的序列长度近乎线性地扩展而不是二次方，但由于硬件利用率低下，它们仍然比变压器更慢。在本文中，我们取得了进展，理解了SSMs和建模关注机制之间的表现差距，并降低了SSMs和建模关注机制之间的硬件障碍。首先，我们使用合成的语言建模任务来理解SSMs和建模关注机制之间的差距。我们发现，现有的SSMs在两个方面存在困难：回忆先前的标记和跨序列比较标记。为了理解对语言建模的影响，我们提出了一个新的SSM层，H3，专门设计这些能力。H3在合成语言上与建模关注机制相匹配，并在OpenWebText上比变压器少了0.4 PPL。此外，一种混合模型，将H3和注意力以及硬件优化相结合，实现了语言建模基准测试的最新性能。我们的工作凸显了SSMs在语言建模方面的潜力，并为如何设计更好的SSMs提供了见解。

    State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 
    
[^150]: 知识引导下的数据中心人工智能在医疗保健中的进展、缺陷与未来方向

    Knowledge-Guided Data-Centric AI in Healthcare: Progress, Shortcomings, and Future Directions. (arXiv:2212.13591v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2212.13591](http://arxiv.org/abs/2212.13591)

    简而言之，该论文讨论了如何使用数据中心的方法解决医学图像诊断中的“小数据”问题，介绍了数据增强、迁移学习、联邦学习和GAN的方法，并提出了使用知识引导的GAN将领域知识纳入训练数据生成过程中。

    

    深度学习的成功很大程度上是由于大量包含特定概念或意义的训练数据的可用性。在医学领域，拥有覆盖特定疾病的多样化训练数据可以导致开发出能够准确预测该疾病的模型。然而，尽管有潜在的好处，由于缺乏高质量的注释数据，基于图像的诊断并没有取得显著进展。本文强调在数据表示方面采用数据中心的方法以提高数据质量的重要性，特别是在可用数据有限的情况下。为了解决这个“小数据”问题，我们讨论了四种生成和聚合训练数据的方法：数据增强、迁移学习、联邦学习和GAN（生成对抗网络）。我们还提出了使用知识引导的GAN来将领域知识纳入训练数据生成过程中。

    The success of deep learning is largely due to the availability of large amounts of training data that cover a wide range of examples of a particular concept or meaning. In the field of medicine, having a diverse set of training data on a particular disease can lead to the development of a model that is able to accurately predict the disease. However, despite the potential benefits, there have not been significant advances in image-based diagnosis due to a lack of high-quality annotated data. This article highlights the importance of using a data-centric approach to improve the quality of data representations, particularly in cases where the available data is limited. To address this "small-data" issue, we discuss four methods for generating and aggregating training data: data augmentation, transfer learning, federated learning, and GANs (generative adversarial networks). We also propose the use of knowledge-guided GANs to incorporate domain knowledge in the training data generation pr
    
[^151]: 通过合并语言模型的权重实现无数据知识融合

    Dataless Knowledge Fusion by Merging Weights of Language Models. (arXiv:2212.09849v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.09849](http://arxiv.org/abs/2212.09849)

    本文提出了一种无数据知识融合方法，可以合并在不同训练数据集上建立的单个模型，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。

    

    微调预训练语言模型已成为构建下游NLP模型的流行范式。通常情况下，经过微调的模型已经可用，但其训练数据不可用，由于数据隐私或知识产权问题。这就造成了跨模型融合知识以产生更好的单一模型的障碍。在本文中，我们研究了建立在不同训练数据集上的单个模型之间合并的问题，以得到一个在所有数据集领域上表现良好且可以推广到域外数据的单一模型。我们提出了一种无数据知识融合方法，该方法在参数空间中合并模型，由权重引导，以最小化合并模型和单个模型之间的预测差异。在一系列评估设置中，我们展示了该方法显著优于如Fisher加权平均或模型集成等基线。此外，我们发现我们的方法是一个有前途的多语言微调替代方案，因为它可以在不需要任何额外注释数据的情况下实现可比的性能。

    Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-of-domain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-
    
[^152]: 多层斯坦变分梯度下降的进一步分析及其在冰川冰模型贝叶斯推断中的应用

    Further analysis of multilevel Stein variational gradient descent with an application to the Bayesian inference of glacier ice models. (arXiv:2212.03366v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2212.03366](http://arxiv.org/abs/2212.03366)

    本文研究了多层斯坦变分梯度下降在大规模贝叶斯反问题中的应用，实现了数量级的加速。

    

    多层斯坦变分梯度下降是一种基于粒子的变分推断方法，利用具有不同成本和保真度的替代目标分布的层次结构来加速推断。本文的贡献有两个方面：第一，提出了一种之前成本复杂度分析的扩展，即使单层斯坦变分梯度下降的指数收敛速率依赖于迭代变化的参数也适用。第二，将多层斯坦变分梯度下降应用于推断Arolla冰川冰的离散基底滑动系数场的大规模贝叶斯反问题。数值实验表明，多层版本相对于单层版本实现了数量级的加速。

    Multilevel Stein variational gradient descent is a method for particle-based variational inference that leverages hierarchies of surrogate target distributions with varying costs and fidelity to computationally speed up inference. The contribution of this work is twofold. First, an extension of a previous cost complexity analysis is presented that applies even when the exponential convergence rate of single-level Stein variational gradient descent depends on iteration-varying parameters. Second, multilevel Stein variational gradient descent is applied to a large-scale Bayesian inverse problem of inferring discretized basal sliding coefficient fields of the Arolla glacier ice. The numerical experiments demonstrate that the multilevel version achieves orders of magnitude speedups compared to its single-level version.
    
[^153]: Edge Impulse: 面向微型机器学习的MLOps平台。

    Edge Impulse: An MLOps Platform for Tiny Machine Learning. (arXiv:2212.03332v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2212.03332](http://arxiv.org/abs/2212.03332)

    Edge Impulse是一个面向微型机器学习的MLOps平台，旨在为开发人员提供嵌入式和边缘ML系统的软硬件优化支持，解决TinyML的可移植性和优化问题。

    

    Edge Impulse是一个基于云的机器学习运营(MLOps)平台，用于开发可以部署到各种硬件目标的嵌入式和边缘ML(微型ML)系统。当前的TinyML工作流程存在着软件堆栈碎片化和异构部署硬件等问题，使得ML模型优化困难且不具备可移植性。我们提出了Edge Impulse，这是一个实用的MLOps平台，可实现大规模开发TinyML系统。Edge Impulse通过支持各种软件和硬件优化来解决这些挑战，并创建可扩展且可移植的软件堆栈，以适应各种嵌入式系统。 截至2022年10月，Edge Impulse托管了来自50,953名开发人员的118,185个项目。

    Edge Impulse is a cloud-based machine learning operations (MLOps) platform for developing embedded and edge ML (TinyML) systems that can be deployed to a wide range of hardware targets. Current TinyML workflows are plagued by fragmented software stacks and heterogeneous deployment hardware, making ML model optimizations difficult and unportable. We present Edge Impulse, a practical MLOps platform for developing TinyML systems at scale. Edge Impulse addresses these challenges and streamlines the TinyML design cycle by supporting various software and hardware optimizations to create an extensible and portable software stack for a multitude of embedded systems. As of Oct. 2022, Edge Impulse hosts 118,185 projects from 50,953 developers.
    
[^154]: 通过多样本超网络提高帕累托前沿学习

    Improving Pareto Front Learning via Multi-Sample Hypernetworks. (arXiv:2212.01130v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.01130](http://arxiv.org/abs/2212.01130)

    本文提出了一个新的PFL框架PHN-HVI，利用超网络生成一组多样的解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。

    

    帕累托前沿学习(PFL)是一种有效的方法，用于获得从给定权衡向量到帕累托前沿解的映射函数，从而解决多目标优化(MOO)问题。然而，现有的PFL方法忽略了优化过程中解之间的关系，从而影响了获得的帕累托前沿的质量。为了解决这个问题，本文提出了一个新的PFL框架，即PHN-HVI，它使用超网络从多样的权衡偏好集生成多个解，并通过最大化这些解定义的超体积指标来提高帕累托前沿的质量。多个MOO机器学习数据集上的实验结果表明，相对于现有的PFL方法，PHN-HVI在帕累托前沿近似质量方面具有更好的性能。

    Pareto Front Learning (PFL) was recently introduced as an effective approach to obtain a mapping function from a given trade-off vector to a solution on the Pareto front, which solves the multi-objective optimization (MOO) problem. Due to the inherent trade-off between conflicting objectives, PFL offers a flexible approach in many scenarios in which the decision makers can not specify the preference of one Pareto solution over another, and must switch between them depending on the situation. However, existing PFL methods ignore the relationship between the solutions during the optimization process, which hinders the quality of the obtained front. To overcome this issue, we propose a novel PFL framework namely PHN-HVI, which employs a hypernetwork to generate multiple solutions from a set of diverse trade-off preferences and enhance the quality of the Pareto front by maximizing the Hypervolume indicator defined by these solutions. The experimental results on several MOO machine learning
    
[^155]: 一种可解释的混合预测模型预测COVID-19病例数的自回归模型和LSTM

    An Interpretable Hybrid Predictive Model of COVID-19 Cases using Autoregressive Model and LSTM. (arXiv:2211.17014v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17014](http://arxiv.org/abs/2211.17014)

    本文提出了一种可解释的混合预测模型，该模型使用自回归模型和LSTM预测COVID-19病例，结合了两种模型的优势，通过数据自适应性决定模型块的相对贡献，在全面数值研究中展示了优异的性能。

    

    2019冠状病毒病（COVID-19）对全球健康和经济产生了深远的影响，因此构建准确且可解释的数据驱动预测模型以改善决策至关重要。本文提出了一种新颖的混合模型，该模型将自回归模型（AR）的可解释性和长短时记忆神经网络（LSTM）的预测能力相结合。该混合模型被正式建模为一个神经网络，其结构连接两个组成模型块，这两个块的相对贡献在训练过程中进行数据自适应性决定。通过对两个数据源的全面数值研究，我们展示了混合模型在其两个组成模型以及其他流行预测模型中的优异性能。

    The Coronavirus Disease 2019 (COVID-19) has a profound impact on global health and economy, making it crucial to build accurate and interpretable data-driven predictive models for COVID-19 cases to improve policy making. The extremely large scale of the pandemic and the intrinsically changing transmission characteristics pose great challenges for effective COVID-19 case prediction. To address this challenge, we propose a novel hybrid model in which the interpretability of the Autoregressive model (AR) and the predictive power of the long short-term memory neural networks (LSTM) join forces. The proposed hybrid model is formalized as a neural network with an architecture that connects two composing model blocks, of which the relative contribution is decided data-adaptively in the training procedure. We demonstrate the favorable performance of the hybrid model over its two component models as well as other popular predictive models through comprehensive numerical studies on two data sour
    
[^156]: 博弈论混合专家用于组合对抗机器学习

    Game Theoretic Mixed Experts for Combinational Adversarial Machine Learning. (arXiv:2211.14669v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14669](http://arxiv.org/abs/2211.14669)

    本文提出了一种博弈论框架，用于组合对抗攻击和防御，我们的框架提出了一种混合专家模型，专门针对特定的防御攻击进行防御，并以博弈论的方式进行合作和竞争，形成一个联合防御。

    

    最近在对抗机器学习方面的一些进展表明，那些被认为是强健的防御措施实际上还是容易受到针对其弱点进行定制化攻击的对抗攻击。这些防御措施包括随机变换的攻击（BaRT），有益人类的对抗训练（FAT），垃圾就是珍宝（TiT）以及由视觉转换器、大型转移模型和尖峰神经网络组成的组合模型。本文提出了一种博弈论框架，用于组合对抗攻击和防御，我们的框架提出了一种混合专家模型，每个专家专门针对特定的防御攻击进行防御。然后，这些专家会以博弈论的方式进行合作和竞争，形成一个联合防御。我们在各种数据集和攻击上展示了我们方法的有效性，并表明我们的模型优于现有的最先进的防御措施。

    Recent advances in adversarial machine learning have shown that defenses considered to be robust are actually susceptible to adversarial attacks which are specifically customized to target their weaknesses. These defenses include Barrage of Random Transforms (BaRT), Friendly Adversarial Training (FAT), Trash is Treasure (TiT) and ensemble models made up of Vision Transformers (ViTs), Big Transfer models and Spiking Neural Networks (SNNs). We first conduct a transferability analysis, to demonstrate the adversarial examples generated by customized attacks on one defense, are not often misclassified by another defense.  This finding leads to two important questions. First, how can the low transferability between defenses be utilized in a game theoretic framework to improve the robustness? Second, how can an adversary within this framework develop effective multi-model attacks? In this paper, we provide a game-theoretic framework for ensemble adversarial attacks and defenses. Our framework
    
[^157]: 用图神经网络和结构化状态空间模型建立多元生物信号模型

    Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models. (arXiv:2211.11176v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11176](http://arxiv.org/abs/2211.11176)

    本研究提出了一种基于时间依赖图的通用图神经网络结构(GraphS4mer)，用于建立多元生物信号模型。该模型结合了结构化状态空间架构和动态演变的图结构学习层来解决长时序和复杂空间相关性，能有效地提高多元生物信号分类任务性能。

    

    多元生物信号在许多医学领域中都很普遍，例如脑电图、多导睡眠图和心电图。由于（1）长时间范围内的时间依赖性和（2）电极之间复杂的空间相关性，建立多元生物信号的时空依赖关系模型是具有挑战性的。为了解决这些挑战，我们建议将多元生物信号表示为时间依赖图，并介绍了GraphS4mer，这是一种通用的图神经网络（GNN）结构，通过建立生物信号中的时空依赖关系来提高生物信号分类任务的性能。具体而言，（1）我们利用结构化状态空间架构，一种最先进的深度序列模型，来捕捉生物信号中长时间范围的时间依赖关系，并（2）我们建议在GraphS4mer中添加图结构学习层，以学习数据中动态演变的图结构。我们在三个不同的生物信号分类任务上评估我们的模型，并展示它优于几种基准模型，突显了它在建立具有复杂依赖关系的多元生物信号模型方面的有效性。

    Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show th
    
[^158]: 基于不可导物理仿真渲染的感知感知模型强化学习

    SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering. (arXiv:2210.15185v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.15185](http://arxiv.org/abs/2210.15185)

    SAM-RL使用不可导物理仿真和渲染，通过比较渲染图像和真实原始图像自动更新模型，并高效产生策略。感知感知的学习管道允许机器人选择信息丰富的视角监控任务过程。 用于完成机器人组装，工具操作和变形物体操作任务。

    

    模型为基础的强化学习（MBRL）具有比基于模型的强化学习更高的样本效率。如何从原始感官输入（如图像）自动有效地开发准确的模型，特别是针对复杂的环境和任务，是限制MBRL在现实世界中广泛应用的挑战性问题。本文提出了一种称为SAM-RL的感知感知模型强化学习系统。利用不可导物理仿真和渲染，SAM-RL通过比较渲染图像和真实原始图像自动更新模型并高效产生策略。通过感知感知学习管道，SAM-RL允许机器人选择一个信息丰富的视角来监控任务过程。我们将我们的框架应用于实际的三个操作任务：机器人装配，工具操纵和可变形物体操纵。我们证明了其有效性。

    Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectivene
    
[^159]: 通过隐式微分实现可变规模稳定的可微规划

    Scaling up and Stabilizing Differentiable Planning with Implicit Differentiation. (arXiv:2210.13542v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13542](http://arxiv.org/abs/2210.13542)

    本文提出了一种通过 Bellman 固定点方程进行微分的方法，实现了值迭代网络及其变体的前后传递解耦，可实现在规划视程内稳定且灵活前向预算的扩展，展示了在不同规划任务上的出色表现。

    

    可微规划承诺具有端到端的可微性和适应性。然而，一个问题阻止了它在更大规模的问题上的扩展：需要通过向前迭代层进行微分以计算梯度，这会将前向计算和反向传播耦合起来，并需要平衡前向规划器的性能和反向传递的计算成本。为了缓解这个问题，我们提出了通过 Bellman 固定点方程进行微分以将值迭代网络及其变体的前向和后向传递解耦的方法，这使得反向传播成本（在规划视程内）保持不变，同时前向预算更加灵活，有助于扩展到更大的问题上。我们研究了所提出的 VIN 隐式版本及其变体的收敛稳定性、可扩展性和效率，并在一系列规划任务上展示了它们的优越性：2D 导航、视觉导航以及构型空间和工作空间中的 2-DOF 操作。

    Differentiable planning promises end-to-end differentiability and adaptivity. However, an issue prevents it from scaling up to larger-scale problems: they need to differentiate through forward iteration layers to compute gradients, which couples forward computation and backpropagation, and needs to balance forward planner performance and computational cost of the backward pass. To alleviate this issue, we propose to differentiate through the Bellman fixed-point equation to decouple forward and backward passes for Value Iteration Network and its variants, which enables constant backward cost (in planning horizon) and flexible forward budget and helps scale up to large tasks. We study the convergence stability, scalability, and efficiency of the proposed implicit version of VIN and its variants and demonstrate their superiorities on a range of planning tasks: 2D navigation, visual navigation, and 2-DOF manipulation in configuration space and workspace.
    
[^160]: 智能家居中婴儿物理安全监测使用动作识别系统

    Baby Physical Safety Monitoring in Smart Home Using Action Recognition System. (arXiv:2210.12527v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.12527](http://arxiv.org/abs/2210.12527)

    本文提出了一种新的轻量级框架，将迁移学习技术与Conv2D LSTM层相结合，用于婴儿物理安全监测的行为识别。

    

    人类能够通过演绎推理直观地推断出两个状态之间发生的行为，这是因为大脑操作在双向通信模型上，这显著提高了基于与以往经验相关的特征的识别和预测的准确性。在过去的十年中，动作识别的深度学习模型有了显著的进展。然而，深度神经网络在特定的动作识别任务中面临着小型数据集的问题。与大多数动作识别任务一样，在空间-时间数据中准确描述活动的模糊性是一个缺点，可以通过策划适当的数据集来克服，包括对视频数据进行细致的注释和预处理，以分析各种识别任务。在本研究中，我们提出了一种新的轻量级框架，将迁移学习技术与Conv2D LSTM层相结合，从预训练的I3D模型中提取特征，用于婴儿物理安全监测的行为识别。

    Humans are able to intuitively deduce actions that took place between two states in observations via deductive reasoning. This is because the brain operates on a bidirectional communication model, which has radically improved the accuracy of recognition and prediction based on features connected to previous experiences. During the past decade, deep learning models for action recognition have significantly improved. However, deep neural networks struggle with these tasks on a smaller dataset for specific Action Recognition (AR) tasks. As with most action recognition tasks, the ambiguity of accurately describing activities in spatial-temporal data is a drawback that can be overcome by curating suitable datasets, including careful annotations and preprocessing of video data for analyzing various recognition tasks. In this study, we present a novel lightweight framework combining transfer learning techniques with a Conv2D LSTM layer to extract features from the pre-trained I3D model on the
    
[^161]: 基于张量完成的多参数性能建模

    Multi-Parameter Performance Modeling via Tensor Completion. (arXiv:2210.10184v2 [cs.PF] UPDATED)

    [http://arxiv.org/abs/2210.10184](http://arxiv.org/abs/2210.10184)

    本论文采用低秩张量分解来建模应用程序性能，通过对应用程序执行时间的逼近，实现对未观测区域的精确外推，并应用张量完成算法优化低秩正交-多项式（CP）分解，从而提高了预测准确性和存储效率。

    

    预测应用程序性能是性能调整、软硬件协同设计和作业调度等许多任务所依赖的模型之一。我们提出并评估了低秩张量分解来建模应用程序性能。我们使用规则网格对应用程序的输入和配置域进行离散化。在网格单元中映射的应用程序执行时间被平均并表示为张量元素。我们表明，低秩的正交-多项式（CP）张量分解通过对这些张量进行逼近是有效的。我们进一步表明，这种分解使得对一个应用程序参数空间中未观测区域的精确外推成为可能。然后，我们应用张量完成来优化给定一组稀疏的观察的运行时间的CP分解。我们考虑了六个应用的替代分段/网格模型和监督学习模型，并证明使用张量完成功能优化的CP分解具有更高的预测准确性和存储效率。

    Performance tuning, software/hardware co-design, and job scheduling are among the many tasks that rely on models to predict application performance. We propose and evaluate low rank tensor decomposition for modeling application performance. We discretize the input and configuration domain of an application using regular grids. Application execution times mapped within grid-cells are averaged and represented by tensor elements. We show that low-rank canonical-polyadic (CP) tensor decomposition is effective in approximating these tensors. We further show that this decomposition enables accurate extrapolation of unobserved regions of an application's parameter space. We then employ tensor completion to optimize a CP decomposition given a sparse set of observed runtimes. We consider alternative piecewise/grid-based models and supervised learning models for six applications and demonstrate that CP decomposition optimized using tensor completion offers higher prediction accuracy and memory-e
    
[^162]: 在线多人游戏中的属性推断攻击：以 Dota2 为例的案例研究

    Attribute Inference Attacks in Online Multiplayer Video Games: a Case Study on Dota2. (arXiv:2210.09028v5 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.09028](http://arxiv.org/abs/2210.09028)

    本文是第一篇研究在线游戏属性推断攻击问题的论文，以 Dota2 为例，通过对玩家数据的挖掘，成功地利用机器学习推断出了玩家的真实生活属性。

    

    您知道吗？超过 7000 万 Dota2 玩家的游戏数据可以公开访问，如果被恶意利用会带来什么问题？本文是第一篇研究这种问题的论文。鉴于视频游戏的普及，我们提出了 Dota2 环境下属性推断攻击（AIA）的第一个威胁模型。我们解释了攻击者如何（以及为什么）可以利用 Dota2 生态系统中丰富的公共数据来推断其玩家的私人信息。由于缺乏关于我们 AIA 效果的具体证据，我们通过对涵盖 26000 场比赛的约 500 名 Dota2 玩家进行广泛调查，验证了它们在现实中的影响力。在发现玩家的 Dota2 活动与其现实生活之间存在联系（$p$ < 0.01 和 $\rho$ > 0.3）后，我们进行了多种 AIA 的伦理实验，利用机器学习的能力推断出我们调查对象的现实生活属性。

    Did you know that over 70 million of Dota2 players have their in-game data freely accessible? What if such data is used in malicious ways? This paper is the first to investigate such a problem.  Motivated by the widespread popularity of video games, we propose the first threat model for Attribute Inference Attacks (AIA) in the Dota2 context. We explain how (and why) attackers can exploit the abundant public data in the Dota2 ecosystem to infer private information about its players. Due to lack of concrete evidence on the efficacy of our AIA, we empirically prove and assess their impact in reality. By conducting an extensive survey on $\sim$500 Dota2 players spanning over 26k matches, we verify whether a correlation exists between a player's Dota2 activity and their real-life. Then, after finding such a link ($p$ < 0.01 and $\rho$ > 0.3), we ethically perform diverse AIA. We leverage the capabilities of machine learning to infer real-life attributes of the respondents of our survey by u
    
[^163]: ToupleGDD：基于深度强化学习的影响力最大化问题的精细解决方案

    ToupleGDD: A Fine-Designed Solution of Influence Maximization by Deep Reinforcement Learning. (arXiv:2210.07500v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2210.07500](http://arxiv.org/abs/2210.07500)

    提出了一个新颖的端到端DRL框架ToupleGDD，用于解决影响力最大化问题，该框架将三个耦合的图神经网络用于网络嵌入，双重深度Q网络用于参数学习，通过统一的损失函数将它们集成起来，捕获网络中节点和边缘的更丰富和多样化的信息，实验结果表明ToupleGDD具有最先进的性能和泛化能力。

    

    影响力最大化（IM）问题始终是选择具有在网络中最大影响力的节点小子集的一个重要问题。然而，由于给定种子集合计算影响力扩散是#P-hard问题，现有的方法，包括启发式算法和近似算法，面临着理论保证，时间效率，泛化能力等方面的困难。同时，随着深度强化学习在人工智能和其他领域的最新成果，越来越多的工作已经关注于利用DRL解决组合优化问题。在此基础上，我们提出了一种新颖的端到端DRL框架ToupleGDD，用于解决IM问题，该框架将三个耦合的图神经网络用于网络嵌入，双重深度Q网络用于参数学习。与之前仅使用一个图神经网络或多个网络的线性组合来解决IM问题的DRL方法相比，ToupleGDD框架利用具有特定任务的多个图神经网络，并通过统一的损失函数将它们集成起来，从而能够捕获网络中节点和边缘的更丰富和多样化的信息。在真实的数据集上的实验结果表明，ToupleGDD框架具有最先进的性能和泛化能力，并且具有广泛应用的潜力。

    Aiming at selecting a small subset of nodes with maximum influence on networks, the Influence Maximization (IM) problem has been extensively studied. Since it is #P-hard to compute the influence spread given a seed set, the state-of-the-art methods, including heuristic and approximation algorithms, faced with great difficulties such as theoretical guarantee, time efficiency, generalization, etc. This makes it unable to adapt to large-scale networks and more complex applications. On the other side, with the latest achievements of Deep Reinforcement Learning (DRL) in artificial intelligence and other fields, lots of works have been focused on exploiting DRL to solve combinatorial optimization problems. Inspired by this, we propose a novel end-to-end DRL framework, ToupleGDD, to address the IM problem in this paper, which incorporates three coupled graph neural networks for network embedding and double deep Q-networks for parameters learning. Previous efforts to solve IM problem with DRL 
    
[^164]: 迷你ALBERT: 基于参数高效递归变换的模型蒸馏

    MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers. (arXiv:2210.06425v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.06425](http://arxiv.org/abs/2210.06425)

    MiniALBERT 是一种模型蒸馏技术，结合了跨层参数共享等策略，将完全参数化的语言模型知识转换成为紧凑递归学生模型。MiniALBERT 在基准 NLP 任务上的实验表明，它在性能上优于多个最先进的紧凑型语言模型，并且具有更少的参数数量。

    

    近年来，预训练语言模型 (LM) 由于在下游应用中的卓越表现，成为自然语言处理 (NLP) 中不可或缺的一部分。尽管这一辉煌的成就，LM 的可用性受限于计算和时间复杂度，以及它们日益增长的模型大小；这是被称为“过参数化”的问题。文献中提出了不同的策略来缓解这些问题，旨在创建有效的紧凑模型，将它们的性能与其膨胀的对应物几乎相匹配，而几乎不损失性能。在这个研究领域中最受欢迎的技术之一是模型蒸馏。而另一种强大但不常使用的技术是跨层参数共享。在这项工作中，我们将这两个策略结合起来，提出了 MiniALBERT，一种将完全参数化的 LM 的知识转换为紧凑递归学生的技术，同时我们研究了跨层参数共享和其他技术的效能，进一步提高了 MiniALBERT 的效率和性能。我们在基准 NLP 任务上的实验证明，MiniALBERT 优于多个最先进的紧凑型 LM，并保持更少的参数数量。

    Pre-trained Language Models (LMs) have become an integral part of Natural Language Processing (NLP) in recent years, due to their superior performance in downstream applications. In spite of this resounding success, the usability of LMs is constrained by computational and time complexity, along with their increasing size; an issue that has been referred to as `overparameterisation'. Different strategies have been proposed in the literature to alleviate these problems, with the aim to create effective compact models that nearly match the performance of their bloated counterparts with negligible performance losses. One of the most popular techniques in this area of research is model distillation. Another potent but underutilised technique is cross-layer parameter sharing. In this work, we combine these two strategies and present MiniALBERT, a technique for converting the knowledge of fully parameterised LMs (such as BERT) into a compact recursive student. In addition, we investigate the 
    
[^165]: 利用人作为传感器的遮挡感知人群导航

    Occlusion-Aware Crowd Navigation Using People as Sensors. (arXiv:2210.00552v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.00552](http://arxiv.org/abs/2210.00552)

    利用深度强化学习和社交推断技术，该研究提出了一种解决拥挤环境下遮挡问题的方法，成功将模拟中的策略迁移到现实中的Turtlebot机器人上。

    

    在拥挤空间中自主导航对于移动机器人来说是一个挑战，因为环境动态不稳定、仅能观察到局部。由于有限的传感器视野和遮挡的人类元素，遮挡在这种情境中普遍存在。先前的研究表明，可以利用观察到的人类互动行为来估计潜在的障碍物，尽管存在遮挡。我们提出将这种社交推断技术集成到规划流程中。我们使用一个特别设计的损失函数的变分自编码器来学习有意义遮挡推断的表示。该工作采用深度强化学习方法，将学习到的表征集成到遮挡感知规划中。在模拟中，我们的遮挡感知政策通过估计被遮挡的区域内的元素，实现了与完全可观察导航相当的碰撞避免性能。我们展示了从模拟到真实Turtlebot上的成功策略迁移。

    Autonomous navigation in crowded spaces poses a challenge for mobile robots due to the highly dynamic, partially observable environment. Occlusions are highly prevalent in such settings due to a limited sensor field of view and obstructing human agents. Previous work has shown that observed interactive behaviors of human agents can be used to estimate potential obstacles despite occlusions. We propose integrating such social inference techniques into the planning pipeline. We use a variational autoencoder with a specially designed loss function to learn representations that are meaningful for occlusion inference. This work adopts a deep reinforcement learning approach to incorporate the learned representation for occlusion-aware planning. In simulation, our occlusion-aware policy achieves comparable collision avoidance performance to fully observable navigation by estimating agents in occluded spaces. We demonstrate successful policy transfer from simulation to the real-world Turtlebot
    
[^166]: 基于关键点的单目RGB-D输入下的六自由度抓取点生成

    Keypoint-GraspNet: Keypoint-based 6-DoF Grasp Generation from the Monocular RGB-D input. (arXiv:2209.08752v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.08752](http://arxiv.org/abs/2209.08752)

    本文提出了一种基于关键点的单目RGB-D输入下的六自由度抓取点生成方法，该方法可以通过PnP算法恢复SE(3)姿态。机器人实验表明，我们的方法在抓取点提案的精度、多样性和时间成本方面优于其他方法并具有很强的应用潜力。

    

    目前已经在点云输入的六自由度抓取点学习中取得了很大的成功，但由于点集无序性而导致的计算成本仍然是一个问题。本文探索了从RGB-D输入中生成抓取点的方法。我们提出了Keypoint-GraspNet解决方案，该方案在图像空间中检测夹持器关键点的投影，然后使用PnP算法恢复SE(3)姿态。基于三维形体和抓取家族构建了一个合成数据集来检验我们的想法。基于度量的评估表明，我们的方法在抓取点提案的精度、多样性和时间成本方面优于基线。最后，机器人实验展示了高成功率，证明了这种方法在实际应用中的潜力。

    Great success has been achieved in the 6-DoF grasp learning from the point cloud input, yet the computational cost due to the point set orderlessness remains a concern. Alternatively, we explore the grasp generation from the RGB-D input in this paper. The proposed solution, Keypoint-GraspNet, detects the projection of the gripper keypoints in the image space and then recover the SE(3) poses with a PnP algorithm. A synthetic dataset based on the primitive shape and the grasp family is constructed to examine our idea. Metric-based evaluation reveals that our method outperforms the baselines in terms of the grasp proposal accuracy, diversity, and the time cost. Finally, robot experiments show high success rate, demonstrating the potential of the idea in the real-world applications.
    
[^167]: 多模块图神经网络的灵活表征促进更好的泛化能力

    Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06589](http://arxiv.org/abs/2209.06589)

    本研究探讨了图神经网络在推广到更大的图和从未见过的数据方面的局限，并提出了多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    

    图神经网络（GNN）已成为处理图结构数据的学习与推断的强大模型，但对于扩展到更大的图以及推广到从未见过的数据的基本限制的了解还不足。本文使用随机图生成器系统地研究了图的大小和结构属性如何影响GNN的预测性能，并提出多模块GNN框架，通过推广单个规范非线性变换来适应新图。结果表明，多模块GNN在合成和实际数据集上均显著提高了GNN的泛化能力，并在几项具有挑战性的任务上实现了最先进的性能。

    Graph neural networks (GNNs) have become compelling models designed to perform learning and inference on graph-structured data. However, little work has been done to understand the fundamental limitations of GNNs for scaling to larger graphs and generalizing to out-of-distribution (OOD) inputs. In this paper, we use a random graph generator to systematically investigate how the graph size and structural properties affect the predictive performance of GNNs. We present specific evidence that the average node degree is a key feature in determining whether GNNs can generalize to unseen graphs, and that the use of multiple node update functions can improve the generalization performance of GNNs when dealing with graphs of multimodal degree distributions. Accordingly, we propose a multi-module GNN framework that allows the network to adapt flexibly to new graphs by generalizing a single canonical nonlinear transformation over aggregated inputs. Our results show that the multi-module GNNs imp
    
[^168]: Meta Pattern Concern Score：一种基于人类价值观的多分类器评估新指标

    Meta Pattern Concern Score: A Novel Evaluation Measure with Human Values for Multi-classifiers. (arXiv:2209.06408v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06408](http://arxiv.org/abs/2209.06408)

    本文提出了一种名为“Meta Pattern Concern Score”的新型评估指标，它基于概率预测的抽象表征和可调节的阈值，将人类价值观引入到多分类器中，可以用于根据人类价值观恰当地评估黑盒模型在现实中的应用效果，并且可以比较在具有不同人类价值观下使用不同分类器的同一数据集。

    

    随着先进的分类器越来越多地应用于现实中的安全关键应用，如何根据特定的人类价值观恰当地评估黑盒模型在社区中仍然是一个问题。这些人类价值包括对不同严重程度的错误案例进行惩罚，并在总体性能减少特定危险案例的情况下做出妥协。在本文中，我们提出了一种名为“Meta Pattern Concern Score”的新型评估指标，它基于概率预测的抽象表征和可调节的阈值，将人类价值观引入到多分类器中。从技术上讲，我们从两种常见指标——混淆矩阵评估指标和损失值的优势和劣势中学习，使我们的指标在通用任务下同样有效，交叉熵损失在极限情况下成为我们指标的一种特殊情况。此外，我们的指标还可以用于比较在具有不同人类价值观下使用不同分类器的同一数据集。实验结果验证了我们的指标在各种数据集和模型中的有效性，同时也为未来的研究提供了参考。

    While advanced classifiers have been increasingly used in real-world safety-critical applications, how to properly evaluate the black-box models given specific human values remains a concern in the community. Such human values include punishing error cases of different severity in varying degrees and making compromises in general performance to reduce specific dangerous cases. In this paper, we propose a novel evaluation measure named Meta Pattern Concern Score based on the abstract representation of probabilistic prediction and the adjustable threshold for the concession in prediction confidence, to introduce the human values into multi-classifiers. Technically, we learn from the advantages and disadvantages of two kinds of common metrics, namely the confusion matrix-based evaluation measures and the loss values, so that our measure is effective as them even under general tasks, and the cross entropy loss becomes a special case of our measure in the limit. Besides, our measure can als
    
[^169]: 改善运营经济学：基于双层 MIP 的闭环预测优化框架来预测机组组合的操作计划

    Towards Improving Operation Economics: A Bilevel MIP-Based Closed-Loop Predict-and-Optimize Framework for Prescribing Unit Commitment. (arXiv:2208.13065v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.13065](http://arxiv.org/abs/2208.13065)

    本文提出了一个基于双层 MIP 的闭环预测优化框架，使用成本导向的预测器来改进电力系统的经济运行。该框架通过反馈循环迭代地改进预测器，实现了对机组组合的最佳操作。

    

    通常，系统操作员在开环预测优化过程中进行电力系统的经济运行：首先预测可再生能源(RES)的可用性和系统储备需求；根据这些预测，系统操作员解决诸如机组组合(UC)的优化模型，以确定相应的经济运行计划。然而，这种开环过程可能会实质性地损害操作经济性，因为它的预测器目光短浅地寻求改善即时的统计预测误差，而不是最终的操作成本。为此，本文提出了一个闭环预测优化框架，提供一种预测机组组合以改善操作经济性的方法。首先，利用双层混合整数规划模型针对最佳系统操作训练成本导向的预测器。上层基于其引起的操作成本来训练 RES 和储备预测器；下层则在给定预测的 RES 和储备的情况下，依据最佳操作原则求解 UC。这两个层级通过反馈环路进行交互性互动，直到收敛为止。在修改后的IEEE 24-bus系统上的数值实验表明，与三种最先进的 UC 基准线相比，所提出的框架具有高效性和有效性。

    Generally, system operators conduct the economic operation of power systems in an open-loop predict-then-optimize process: the renewable energy source (RES) availability and system reserve requirements are first predicted; given the predictions, system operators solve optimization models such as unit commitment (UC) to determine the economical operation plans accordingly. However, such an open-loop process could essentially compromise the operation economics because its predictors myopically seek to improve the immediate statistical prediction errors instead of the ultimate operation cost. To this end, this paper presents a closed-loop predict-and-optimize framework, offering a prescriptive UC to improve the operation economics. First, a bilevel mixed-integer programming model is leveraged to train cost-oriented predictors tailored for optimal system operations: the upper level trains the RES and reserve predictors based on their induced operation cost; the lower level, with given pred
    
[^170]: 拜占庭人也能从历史中学习：联邦学习中心化剪裁的衰落

    Byzantines can also Learn from History: Fall of Centered Clipping in Federated Learning. (arXiv:2208.09894v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.09894](http://arxiv.org/abs/2208.09894)

    本文研究了中心化剪裁在面对不同恶意代理时的脆弱性，提出了一种称为多引用点剪裁 (MRPC) 的算法来解决这个问题。MRPC 框架利用多个参考点有效地中和专门设计的 Byzantine attacks。实验结果表明，在各种类型的 Byzantine attacks 下，MRPC 显著优于最先进的 FL 方法。

    

    联邦学习 (FL) 框架由于在广泛的协作学习任务中的成功而越来越受欢迎，但也引起了某些安全问题。其中，拜占庭攻击的风险是特别关注的问题，这指的是恶意客户参与学习过程的可能性。因此，FL 中的一个关键目标是消除 Byzantine attacks 的潜在影响，确保最终模型是可信的。已经观察到，客户端的模型/更新之间的方差越大，隐藏 Byzantine attacks 的空间就越大。因此，通过使用动量，从而减少方差，可以削弱已知 Byzantine attacks 的力量。中心化剪裁 (CC) 框架进一步表明，上一次的动量项除了减少方差外，还可以作为一个参考点更好地消除 Byzantine attacks。在本文中，我们研究了在不同的恶意代理有不同目标时 CC 的脆弱性。我们提出了一种改进的剪裁算法称为多引用点剪裁 (MRPC)，以克服这种脆弱性。MRPC 框架有效地利用多个参考点来消除专门设计以绕过 CC 方法的 Byzantine attacks。实验结果表明，在各种类型的 Byzantine attacks 下，MRPC 显著优于最先进的 FL 方法。

    The increasing popularity of the federated learning (FL) framework due to its success in a wide range of collaborative learning tasks also induces certain security concerns. Among many vulnerabilities, the risk of Byzantine attacks is of particular concern, which refers to the possibility of malicious clients participating in the learning process. Hence, a crucial objective in FL is to neutralize the potential impact of Byzantine attacks, and to ensure that the final model is trustable. It has been observed that the higher the variance among the clients' models/updates, the more space there is for Byzantine attacks to be hidden. As a consequence, by utilizing momentum, and thus, reducing the variance, it is possible to weaken the strength of known Byzantine attacks. The centered clipping (CC) framework has further shown that, the momentum term from the previous iteration, besides reducing the variance, can be used as a reference point to neutralize Byzantine attacks better. In this wor
    
[^171]: GSim ：面向异构图的图神经网络关联度量

    GSim: A Graph Neural Network based Relevance Measure for Heterogeneous Graphs. (arXiv:2208.06144v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2208.06144](http://arxiv.org/abs/2208.06144)

    GSim是一种基于图神经网络的异构图关联度量方法，不需要预定义的元路径，能够捕捉异构图的隐含结构，已在多个数据集上得到验证。

    

    异构图在各个领域中广泛存在，包含具有多种类型的节点和边缘。关联度量是分析异构图的基本任务之一，目的是计算不同类型的两个对象之间的相关性，已经在许多应用程序中得到了广泛的应用，例如网络搜索、推荐和社区检测。大多数现有的关联度量方法专注于同质网络，但为异构图开发了一些方法，但它们经常需要预定义的元路径。定义有意义的元路径需要大量领域知识，这在基于架构丰富的异构图（如知识图谱）上极大地限制了它们的应用。最近，图神经网络已广泛应用于许多图挖掘任务中，但尚未用于衡量关联性。为解决上述问题，我们提出了GSim，一种基于图神经网络的异构图关联度量方法。GSim能够捕捉异构图的隐含结构，不需要预定义的元路径。我们在三个真实世界的数据集上评估了我们提出的方法，结果表明GSim显著优于几种最先进的方法。

    Heterogeneous graphs, which contain nodes and edges of multiple types, are prevalent in various domains, including bibliographic networks, social media, and knowledge graphs. As a fundamental task in analyzing heterogeneous graphs, relevance measure aims to calculate the relevance between two objects of different types, which has been used in many applications such as web search, recommendation, and community detection. Most of existing relevance measures focus on homogeneous networks where objects are of the same type, and a few measures are developed for heterogeneous graphs, but they often need the pre-defined meta-path. Defining meaningful meta-paths requires much domain knowledge, which largely limits their applications, especially on schema-rich heterogeneous graphs like knowledge graphs. Recently, the Graph Neural Network (GNN) has been widely applied in many graph mining tasks, but it has not been applied for measuring relevance yet. To address the aforementioned problems, we p
    
[^172]: 一种符合保序的风险控制方法

    Conformal Risk Control. (arXiv:2208.02814v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.02814](http://arxiv.org/abs/2208.02814)

    该论文提出了一种符合保序的风险控制方法，可以控制任何单调损失函数的期望值，示例证明其在计算机视觉和自然语言处理领域具有控制误报率、图形距离和令牌级F1得分的能力。

    

    我们将符合性预测推广至控制任何单调损失函数的期望值。该算法将分裂符合性预测及其覆盖保证进行了泛化。类似于符合性预测，符合保序的风险控制方法在$\mathcal{O}(1/n)$因子内保持紧密性。计算机视觉和自然语言处理领域的示例证明了我们算法在控制误报率、图形距离和令牌级F1得分方面的应用。

    We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.
    
[^173]: 基于 Weisfeiler-Lehman 子树 L1-近似树编辑距离的 Wasserstein 图距离

    Wasserstein Graph Distance Based on $L_1$-Approximated Tree Edit Distance between Weisfeiler-Lehman Subtrees. (arXiv:2207.04216v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.04216](http://arxiv.org/abs/2207.04216)

    本文提出一种名为Wasserstein WL子树(WWLS)距离的新型图距离，通过利用WL子树作为节点邻域的结构信息，使用节点的WL子树之间的L1-近似树编辑距离(L1-TED)定义节点度量，解决了WL测试无法捕捉轻微结构差异的问题

    

    Weisfeiler-Lehman (WL)测试是图机器学习中广泛使用的算法，包括图内核、图度量和图神经网络。然而，它仅关注图的一致性，无法检测轻微的结构差异。因此，这限制了它捕捉结构信息的能力，也限制了依赖WL测试的现有模型的性能。本文提出一种名为Wasserstein WL子树(WWLS)距离的新型图距离，以解决这个问题。我们的方法利用WL子树作为节点邻域的结构信息，并使用节点的WL子树之间的L1-近似树编辑距离(L1-TED)定义节点度量。随后，我们结合了Wasserstein距离和L1-TED来定义WWLS距离

    The Weisfeiler-Lehman (WL) test is a widely used algorithm in graph machine learning, including graph kernels, graph metrics, and graph neural networks. However, it focuses only on the consistency of the graph, which means that it is unable to detect slight structural differences. Consequently, this limits its ability to capture structural information, which also limits the performance of existing models that rely on the WL test. This limitation is particularly severe for traditional metrics defined by the WL test, which cannot precisely capture slight structural differences. In this paper, we propose a novel graph metric called the Wasserstein WL Subtree (WWLS) distance to address this problem. Our approach leverages the WL subtree as structural information for node neighborhoods and defines node metrics using the $L_1$-approximated tree edit distance ($L_1$-TED) between WL subtrees of nodes. Subsequently, we combine the Wasserstein distance and the $L_1$-TED to define the WWLS distan
    
[^174]: 在NVIDIA网卡中实现强化学习数据中心拥塞控制

    Implementing Reinforcement Learning Datacenter Congestion Control in NVIDIA NICs. (arXiv:2207.02295v4 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2207.02295](http://arxiv.org/abs/2207.02295)

    本文在NVIDIA网卡中实现了强化学习数据中心拥塞控制，通过将RL-CC的复杂神经网络转化为决策树，实现了实时推理，并成功改善了网络拥塞下的尾部延迟和数据包丢失问题。

    

    随着通信协议的发展，数据中心网络的利用率越来越高，拥塞更为频繁，导致延迟和丢包率增加。这种情况下，人工设计拥塞控制算法变得极其困难，需要开发人工智能方法来替代人力。但是，由于网络设备计算能力有限，目前不可能在网络设备上部署AI模型。本文提出了一个解决方案，基于最新的强化学习拥塞控制算法[arXiv:2207.02295]，构建了一个基于决策树的计算轻量级解决方案，将RL-CC的复杂神经网络转化为决策树，将其推理时间降低了500倍，使其在μ秒级决策时间要求内实现实时推理，且对质量影响不大。我们在一个实时集群中部署了转换后的策略，并与现代数据中心部署的流行拥塞控制算法进行了比较。在类似的流量条件下，我们的解决方案将尾部延迟率提高了x%，将数据包丢失率降低了y%。

    As communication protocols evolve, datacenter network utilization increases. As a result, congestion is more frequent, causing higher latency and packet loss. Combined with the increasing complexity of workloads, manual design of congestion control (CC) algorithms becomes extremely difficult. This calls for the development of AI approaches to replace the human effort. Unfortunately, it is currently not possible to deploy AI models on network devices due to their limited computational capabilities. Here, we offer a solution to this problem by building a computationally-light solution based on a recent reinforcement learning CC algorithm [arXiv:2207.02295]. We reduce the inference time of RL-CC by x500 by distilling its complex neural network into decision trees. This transformation enables real-time inference within the $\mu$-sec decision-time requirement, with a negligible effect on quality. We deploy the transformed policy on NVIDIA NICs in a live cluster. Compared to popular CC algor
    
[^175]: 条件可引出的深度强化学习动态风险度量

    Conditionally Elicitable Dynamic Risk Measures for Deep Reinforcement Learning. (arXiv:2206.14666v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14666](http://arxiv.org/abs/2206.14666)

    本文提出一种可解决风险敏感型强化学习问题的新框架，使用动态谱风险度量进行优化，设计了一种可以逼近这些度量的深度神经网络算法，并开发了一种不需要额外嵌套转换的风险敏感演员-评论家算法。

    

    我们提出了一种解决风险敏感型强化学习问题的新框架，其中代理通过优化时间一致的动态谱风险度量来实现。基于条件可引出性的概念，我们的方法构建了（严格一致的）评分函数，用作估计过程中的处罚项。我们的贡献有三个方面：（i）设计了一种利用深度神经网络估计一类动态谱风险度量的有效方法，（ii）证明了这些动态谱风险度量可以使用深度神经网络进行任意精度的逼近，以及（iii）开发了一种风险敏感的演员-评论家算法，该算法使用完整的剧集，不需要任何额外的嵌套转换。我们将概念上改进的强化学习算法与嵌套模拟方法进行了比较，并在两个设置中说明了其在统计套利和组合分配方面的性能，包括模拟和真实数据。

    We propose a novel framework to solve risk-sensitive reinforcement learning (RL) problems where the agent optimises time-consistent dynamic spectral risk measures. Based on the notion of conditional elicitability, our methodology constructs (strictly consistent) scoring functions that are used as penalizers in the estimation procedure. Our contribution is threefold: we (i) devise an efficient approach to estimate a class of dynamic spectral risk measures with deep neural networks, (ii) prove that these dynamic spectral risk measures may be approximated to any arbitrary accuracy using deep neural networks, and (iii) develop a risk-sensitive actor-critic algorithm that uses full episodes and does not require any additional nested transitions. We compare our conceptually improved reinforcement learning algorithm with the nested simulation approach and illustrate its performance in two settings: statistical arbitrage and portfolio allocation on both simulated and real data.
    
[^176]: RevBiFPN：完全可逆的双向特征金字塔网络

    RevBiFPN: The Fully Reversible Bidirectional Feature Pyramid Network. (arXiv:2206.14098v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.14098](http://arxiv.org/abs/2206.14098)

    本文提出了RevSilo，一个完全可逆的双向多尺度特征融合模块，它缓解了神经网络规模受限的问题。

    

    本文提出了RevSilo，这是第一个可逆的双向多尺度特征融合模块。与其他可逆方法一样，RevSilo通过重新计算来消除存储隐藏激活所需的内存；然而，现有方法不适用于多尺度特征融合，因此不能应用于大部分网络。双向多尺度特征融合促进了局部和全局的一致性，并已成为针对空间敏感任务的网络的设计原则。这些网络在使用高分辨率输入时，在各种计算机视觉任务中实现了最先进的结果。然而，训练这些网络需要保存大型的多分辨率激活所需的大量加速器内存。这些内存需求本质上限制了神经网络的规模，限制了由规模带来的改进。跨分辨率尺度运作的RevSilo缓解了这些问题。

    This work introduces RevSilo, the first reversible bidirectional multi-scale feature fusion module. Like other reversible methods, RevSilo eliminates the need to store hidden activations by recomputing them. However, existing reversible methods do not apply to multi-scale feature fusion and are, therefore, not applicable to a large class of networks. Bidirectional multi-scale feature fusion promotes local and global coherence and has become a de facto design principle for networks targeting spatially sensitive tasks, e.g., HRNet (Sun et al., 2019a) and EfficientDet (Tan et al., 2020). These networks achieve state-of-the-art results across various computer vision tasks when paired with high-resolution inputs. However, training them requires substantial accelerator memory for saving large, multi-resolution activations. These memory requirements inherently cap the size of neural networks, limiting improvements that come from scale. Operating across resolution scales, RevSilo alleviates th
    
[^177]: 基于隐式语言Q学习的自然语言生成离线强化学习方法

    Offline RL for Natural Language Generation with Implicit Language Q Learning. (arXiv:2206.11871v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.11871](http://arxiv.org/abs/2206.11871)

    本研究提出了一种新颖的自然语言生成离线强化学习方法ILQL，通过在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，引导语言模型生成最大化用户指定的效用函数的语言输出，从而解决巨型语言模型完成用户指定任务时存在的不一致性问题。

    

    巨型语言模型从文本语料库中提炼出广泛的知识。然而，当处理用户指定的任务时，它们可能会存在不一致性。这个问题可以通过在精心策划的数据集上进行监督学习或强化学习来解决。本文提出了一种新颖的离线强化学习方法，即隐式语言Q学习（ILQL），专门设计用于语言模型，它结合了RL算法的灵活效用最大化框架与监督学习利用先前收集的数据的能力，以及其简单性和稳定性。我们的方法在学习价值函数时采用价值保守性和隐式数据集支持约束的组合，然后将其用于引导语言模型生成，最大化用户指定的效用函数。除了经验性地验证了ILQL，我们还在自然语言生成中展示了离线RL能够有用的场景的详细经验分析。

    Large language models distill broad knowledge from text corpora. However, they can be inconsistent when it comes to completing user specified tasks. This issue can be addressed by finetuning such models via supervised learning on curated datasets, or via reinforcement learning. In this work, we propose a novel offline RL method, implicit language Q-learning (ILQL), designed for use on language models, that combines both the flexible utility maximization framework of RL algorithms with the ability of supervised learning to leverage previously collected data, as well as its simplicity and stability. Our method employs a combination of value conservatism alongside an implicit dataset support constraint in learning value functions, which are then used to guide language model generations towards maximizing user-specified utility functions. In addition to empirically validating ILQL, we present a detailed empirical analysis of situations where offline RL can be useful in natural language gen
    
[^178]: 可验证的三维物体姿态估计：基础、学习模型和自我训练

    Certifiable 3D Object Pose Estimation: Foundations, Learning Models, and Self-Training. (arXiv:2206.11215v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.11215](http://arxiv.org/abs/2206.11215)

    本文提出了一个通用的认证理论，使得估计值和真实值之间的距离达到了约束，提出了一个基于语义关键点的姿态估计模型 C-3PO，并加入了可观察正确性和非退化性两个证书，该模型在合成和实际测量数据集方面取得了最先进的性能，同时提出了一个自我训练的方法以进一步提高模型性能。

    

    本文考虑了一个可验证的物体姿态估计问题，即在给定一个物体的部分点云的情况下，不仅要估计物体的姿态，还要提供一个正确性证明。我们提出了一个关于端到端感知模型认证的通用理论，引入了一个被称为 $\zeta$-正确性的概念，它约束了估计值和真实值之间的距离。我们证明了 $\zeta$-正确性可以通过实现两个证书来评估：（i）一个可观察正确性的证书，该证书断言模型输出是否与输入数据和先验信息一致，（ii）一个非退化性证书，它断言输入数据是否足以计算出唯一的估计值。第二，我们将这个理论应用于设计一个新的基于学习的可证明姿态估计器。我们提出 C-3PO，这是一个基于语义关键点的姿态估计模型，增加了两个证书。所提出的模型在合成和实际测量数据集方面均取得了最先进的性能。我们还提出了一个自我训练的方法来进一步提高模型性能。据我们所知，我们的工作是关于可验证的三维物体姿态估计的第一个通用理论和方法。

    We consider a certifiable object pose estimation problem, where -- given a partial point cloud of an object -- the goal is to not only estimate the object pose, but also to provide a certificate of correctness for the resulting estimate. Our first contribution is a general theory of certification for end-to-end perception models. In particular, we introduce the notion of $\zeta$-correctness, which bounds the distance between an estimate and the ground truth. We show that $\zeta$-correctness can be assessed by implementing two certificates: (i) a certificate of observable correctness, that asserts if the model output is consistent with the input data and prior information, (ii) a certificate of non-degeneracy, that asserts whether the input data is sufficient to compute a unique estimate. Our second contribution is to apply this theory and design a new learning-based certifiable pose estimator. We propose C-3PO, a semantic-keypoint-based pose estimation model, augmented with the two cer
    
[^179]: 使用偏差委员会学习去偏分类器

    Learning Debiased Classifier with Biased Committee. (arXiv:2206.10843v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10843](http://arxiv.org/abs/2206.10843)

    该论文探讨了一种新方法，使用偏差委员会来训练没有偏见属性标签的去偏分类器，解决神经网络在训练数据中的偏见问题。

    

    神经网络很容易对训练数据中的类别和内在属性之间的假相关性产生偏见，从而破坏其泛化能力。该研究提出了一种新的方法来训练没有偏见属性标签的去偏分类器。关键思想是采用一个分类器委员会作为辅助模块，在训练主分类器时识别冲突数据（即没有假相关性的数据），并对它们分配较大的权重。委员会以自我启动集成的形式进行学习，其中大多数分类器存在偏差但具有多样化，并有意无法成功预测冲突数据的类别。委员会内部对预测难度的共识因此为识别和权重冲突数据提供了可靠线索。此外，该委员会还使用从主分类器转移的知识进行训练，从而逐渐消除其偏见。

    Neural networks are prone to be biased towards spurious correlations between classes and latent attributes exhibited in a major portion of training data, which ruins their generalization capability. We propose a new method for training debiased classifiers with no spurious attribute label. The key idea is to employ a committee of classifiers as an auxiliary module that identifies bias-conflicting data, i.e., data without spurious correlation, and assigns large weights to them when training the main classifier. The committee is learned as a bootstrapped ensemble so that a majority of its classifiers are biased as well as being diverse, and intentionally fail to predict classes of bias-conflicting data accordingly. The consensus within the committee on prediction difficulty thus provides a reliable cue for identifying and weighting bias-conflicting data. Moreover, the committee is also trained with knowledge transferred from the main classifier so that it gradually becomes debiased along
    
[^180]: 利用嘈杂数据进行递归网络训练，实现免费平滑预测

    It's a super deal -- train recurrent network on noisy data and get smooth prediction free. (arXiv:2206.04215v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.04215](http://arxiv.org/abs/2206.04215)

    本研究探究了使用嘈杂数据训练递归神经网络进行时间序列预测的效果，并发现该方法能够实现免费平滑预测。在实现方法的基础上，我们还通过解释为何存在噪声压缩，探讨了递归网络在神经科学中的重要性。

    

    最近的研究表明，基于嘈杂数据的预测递归神经网络可以生成平滑的预期轨迹。我们检验了训练数据集和输入序列中噪声成分对网络预测质量的影响。我们提出并讨论了预测过程中观察到的噪声压缩的解释。我们还讨论了递归网络在神经科学中对生物进化的重要性。

    Recent research demonstrate that prediction of time series by predictive recurrent neural networks based on the noisy input generates a smooth anticipated trajectory. We examine influence of the noise component in both the training data sets and the input sequences on network prediction quality. We propose and discuss an explanation of the observed noise compression in the predictive process. We also discuss importance of this property of recurrent networks in the neuroscience context for the evolution of living organisms.
    
[^181]: 将对称性融入可微规划中的可控卷积

    Integrating Symmetry into Differentiable Planning with Steerable Convolutions. (arXiv:2206.03674v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.03674](http://arxiv.org/abs/2206.03674)

    本文研究了如何在路径规划任务中使用对称性改善数据效率和泛化能力。将值迭代视为网格信号，并使用可控卷积来融合对称性。实验表明，我们的对称规划算法比非等变对应物在训练效率和泛化方面有很大提升。

    

    本文研究了在决策任务中出现对称性时，如何利用群对称性改善端到端可微规划算法的数据效率和泛化能力。受等变卷积网络的启发，我们将路径规划问题视为网格上的信号。我们展示了在这种情况下值迭代是一个线性等变算子，即可被（定向）卷积表示。这扩展了值迭代网络（VIN）在使用卷积网络进行路径规划时使用额外的旋转和反射对称性的方法。我们的实现基于VIN，并使用可控卷积网络来融合对称性。我们进行了四个任务的实验：2D导航，视觉导航，自由度（2DOFs）配置空间和工作空间操纵。与非等变对应物VIN和GPPN相比，我们的对称规划算法在训练效率和泛化方面有很大的提升。

    We study how group symmetry helps improve data efficiency and generalization for end-to-end differentiable planning algorithms when symmetry appears in decision-making tasks. Motivated by equivariant convolution networks, we treat the path planning problem as \textit{signals} over grids. We show that value iteration in this case is a linear equivariant operator, which is a (steerable) convolution. This extends Value Iteration Networks (VINs) on using convolutional networks for path planning with additional rotation and reflection symmetry. Our implementation is based on VINs and uses steerable convolution networks to incorporate symmetry. The experiments are performed on four tasks: 2D navigation, visual navigation, and 2 degrees of freedom (2DOFs) configuration space and workspace manipulation. Our symmetric planning algorithms improve training efficiency and generalization by large margins compared to non-equivariant counterparts, VIN and GPPN.
    
[^182]: 在不稳健样本上施加更多正则化以提高对抗性鲁棒性

    Improving adversarial robustness by putting more regularizations on less robust samples. (arXiv:2206.03353v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.03353](http://arxiv.org/abs/2206.03353)

    本文提出了一种新的对抗训练算法，通过在容易受到对抗攻击的数据上施加更多正则化以提高对抗性鲁棒性，得到了在准确性和鲁棒性方面均为最优的表现。

    

    对抗性训练是提高对抗攻击鲁棒性的一种方法，在人类视觉无法察觉的数据扰动下，使给定的深度神经网络产生误判。本文提出了一种新的对抗训练算法，它在理论上得到很好的证明，并且在实践中表现优于其他现有的算法。该算法的一个新的特点是：对于容易受到对抗攻击的数据，比其他现有的正则化算法更多地应用正则化。理论上，我们证明了我们的算法可以被理解为一个最小化经验风险的正则化算法，它来自一个新的鲁棒风险上界的动机。数值实验表明，我们提出的算法同时提高了泛化性能(在例子上的准确性)和鲁棒性(在对抗攻击上的准确性)，达到了最先进的性能水平。

    Adversarial training, which is to enhance robustness against adversarial attacks, has received much attention because it is easy to generate human-imperceptible perturbations of data to deceive a given deep neural network. In this paper, we propose a new adversarial training algorithm that is theoretically well motivated and empirically superior to other existing algorithms. A novel feature of the proposed algorithm is to apply more regularization to data vulnerable to adversarial attacks than other existing regularization algorithms do. Theoretically, we show that our algorithm can be understood as an algorithm of minimizing the regularized empirical risk motivated from a newly derived upper bound of the robust risk. Numerical experiments illustrate that our proposed algorithm improves the generalization (accuracy on examples) and robustness (accuracy on adversarial attacks) simultaneously to achieve the state-of-the-art performance.
    
[^183]: 基于深度强化学习的微电网联合能量分配和机组开启

    Joint Energy Dispatch and Unit Commitment in Microgrids Based on Deep Reinforcement Learning. (arXiv:2206.01663v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.01663](http://arxiv.org/abs/2206.01663)

    本文采用深度强化学习算法HAFH-DDPG来学习隔离式微电网中的联合能量分配和机组开启决策问题，并提出了柴油发电机选择策略，以降低计算复杂度。

    

    如今，应用可再生能源的微电网（MG）越来越广泛，这创造了对动态能源管理的强烈需求。本文采用深度强化学习（DRL）来学习隔离式MG中联合能量分配（ED）和机组开启（UC）决策的最优策略，以在确保供需平衡的前提下降低总电力成本。为了克服由于联合ED和UC而导致的离散-连续混合行动空间的挑战，我们提出了一种DRL算法，即混合行动有限地平线DDPG（HAFH-DDPG），它基于有限地平线动态规划（DP）框架，无缝地整合了两种经典DRL算法，即深度Q网络（DQN）和深度确定性策略梯度（DDPG）。此外，提出了一种柴油发电机（DG）选择策略，以支持简化行动空间，以降低此算法的计算复杂度。

    Nowadays, the application of microgrids (MG) with renewable energy is becoming more and more extensive, which creates a strong need for dynamic energy management. In this paper, deep reinforcement learning (DRL) is applied to learn an optimal policy for making joint energy dispatch (ED) and unit commitment (UC) decisions in an isolated MG, with the aim for reducing the total power generation cost on the premise of ensuring the supply-demand balance. In order to overcome the challenge of discrete-continuous hybrid action space due to joint ED and UC, we propose a DRL algorithm, i.e., the hybrid action finite-horizon DDPG (HAFH-DDPG), that seamlessly integrates two classical DRL algorithms, i.e., deep Q-network (DQN) and deep deterministic policy gradient (DDPG), based on a finite-horizon dynamic programming (DP) framework. Moreover, a diesel generator (DG) selection strategy is presented to support a simplified action space for reducing the computation complexity of this algorithm. Fina
    
[^184]: 基于图神经网络的战略分类

    Strategic Classification with Graph Neural Networks. (arXiv:2205.15765v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.15765](http://arxiv.org/abs/2205.15765)

    本文介绍了一种基于图神经网络的战略分类方法，利用社交关系来提高预测，并提出一种可微分的框架，用于学习基于图的分类器的战略鲁棒性，实验表明该方法在几个真实的网络数据集上十分实用。

    

    战略分类研究用户可以修改其特征以获得有利预测的学习设置。大多数现有的作品都着重于触发独立用户响应的简单分类器。本文研究了使用更复杂的模型进行学习的影响，这些模型打破了独立性假设。受到战略分类应用通常具有社交性质的思路的启发，我们专注于使用社交关系来提高预测的“图神经网络”。使用图进行学习引入了预测中的用户间相互依赖；我们的关键点是战略用户可以利用这些依赖来促进他们的目标。正如我们通过分析和模拟所展示的那样，这可能要么对系统不利，要么对系统有利。基于此，我们提出了一种可微分的框架，用于学习基于图的分类器的战略鲁棒性。在几个真实的网络数据集上的实验证明了我们方法的实用性。

    Strategic classification studies learning in settings where users can modify their features to obtain favorable predictions. Most current works focus on simple classifiers that trigger independent user responses. Here we examine the implications of learning with more elaborate models that break the independence assumption. Motivated by the idea that applications of strategic classification are often social in nature, we focus on \emph{graph neural networks}, which make use of social relations between users to improve predictions. Using a graph for learning introduces inter-user dependencies in prediction; our key point is that strategic users can exploit these to promote their goals. As we show through analysis and simulation, this can work either against the system -- or for it. Based on this, we propose a differentiable framework for strategically-robust learning of graph-based classifiers. Experiments on several real networked datasets demonstrate the utility of our approach.
    
[^185]: 基于多重表示的终身集成学习在少样本目标识别中的应用

    Lifelong Ensemble Learning based on Multiple Representations for Few-Shot Object Recognition. (arXiv:2205.01982v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.01982](http://arxiv.org/abs/2205.01982)

    本文提出了一种基于多重表示的终身集成学习方法，用于开放式场景下的少样本目标识别问题，适用于三维物体类别数量不固定、随时间增长的场景。模型针对各种类型物体进行处理，并进行了广泛实验。

    

    服务机器人越来越多地融入我们的日常生活，帮助我们处理各种任务。在这样的环境中，机器人经常面对新的物体，并需要以开放式方式学习它们。此外，这样的机器人必须能够识别各种物体类别。本文提出了一种基于多重表示的终身集成学习方法，以解决少样本目标识别问题。具体地，我们基于深度表示和手工制作的3D形状描述符形成集成方法。为了促进终身学习，每种方法都配备了一个存储和瞬间检索物体信息的记忆单元。所提出的模型适用于三维物体类别数量不固定、随时间增长的开放式学习场景。我们进行了广泛的实验，评估了所提出方法在离线及开放式场景下的性能。

    Service robots are integrating more and more into our daily lives to help us with various tasks. In such environments, robots frequently face new objects while working in the environment and need to learn them in an open-ended fashion. Furthermore, such robots must be able to recognize a wide range of object categories. In this paper, we present a lifelong ensemble learning approach based on multiple representations to address the few-shot object recognition problem. In particular, we form ensemble methods based on deep representations and handcrafted 3D shape descriptors. To facilitate lifelong learning, each approach is equipped with a memory unit for storing and retrieving object information instantly. The proposed model is suitable for open-ended learning scenarios where the number of 3D object categories is not fixed and can grow over time. We have performed extensive sets of experiments to assess the performance of the proposed approach in offline, and open-ended scenarios. For t
    
[^186]: MONAI Label: 一种用于AI辅助交互标注三维医学图像的框架

    MONAI Label: A framework for AI-assisted Interactive Labeling of 3D Medical Images. (arXiv:2203.12362v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2203.12362](http://arxiv.org/abs/2203.12362)

    MONAI Label是一个开源的AI框架，支持快速标注医学图像。它提供了多种前端选项和两种主动学习策略，可以帮助研究人员加速分割算法的训练，进而提高模型性能。

    

    缺乏标注数据集是训练新的任务特定监督机器学习模型的主要瓶颈，因为手动注释极其昂贵且耗时。为了解决这个问题，我们提出了MONAI Label，一个免费且开源的框架，旨在促进基于人工智能（AI）模型的应用程序开发，以缩短放射学数据集注释所需的时间。通过MONAI Label，研究人员可以开发专注于其专业领域的AI注释应用程序。它允许研究人员将他们的应用程序作为服务快速部署，并通过他们首选的用户界面提供给临床医生。目前，MONAI Label已经准备好支持本地安装（3D Slicer）和基于web的（OHIF）前端，并提供两种主动学习策略以促进和加速分割算法的训练。MONAI Label允许研究人员通过添加小量的手动注释来逐步改进他们基于AI的注释。总的来说，MONAI Label为AI辅助交互标注三维医学图像提供了高效且易于使用的解决方案。

    The lack of annotated datasets is a major bottleneck for training new task-specific supervised machine learning models, considering that manual annotation is extremely expensive and time-consuming. To address this problem, we present MONAI Label, a free and open-source framework that facilitates the development of applications based on artificial intelligence (AI) models that aim at reducing the time required to annotate radiology datasets. Through MONAI Label, researchers can develop AI annotation applications focusing on their domain of expertise. It allows researchers to readily deploy their apps as services, which can be made available to clinicians via their preferred user interface. Currently, MONAI Label readily supports locally installed (3D Slicer) and web-based (OHIF) frontends and offers two active learning strategies to facilitate and speed up the training of segmentation algorithms. MONAI Label allows researchers to make incremental improvements to their AI-based annotatio
    
[^187]: CMW-Net: 用于深度学习的类别感知样本加权映射的鲁棒性学习

    CMW-Net: Learning a Class-Aware Sample Weighting Mapping for Robust Deep Learning. (arXiv:2202.05613v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.05613](http://arxiv.org/abs/2202.05613)

    提出了一种从数据中自适应地学习显式的加权方案的元模型，用于缓解深度神经网络在具有数据偏见问题时的过度拟合问题。

    

    现代深度神经网络很容易过度拟合具有有偏标签或类别不平衡的训练数据。样本重新加权方法被广泛用于缓解这种数据偏见问题。然而，大多数现有方法需要手动预先指定加权方案以及它们的额外超参数，依赖于研究问题和训练数据的特性。为了解决这个问题，我们提出了一种元模型，能够从数据中自适应地学习显式的加权方案。具体来说，我们的方法将每个训练类别视为单独的学习任务，并旨在提取一个显式的加权函数，将样本损失和任务/类别特征作为输入，样本权重作为输出，从而期望对不同的样本类别施加自适应的变化加权方案。

    Modern deep neural networks can easily overfit to biased training data containing corrupted labels or class imbalance. Sample re-weighting methods are popularly used to alleviate this data bias issue. Most current methods, however, require to manually pre-specify the weighting schemes as well as their additional hyper-parameters relying on the characteristics of the investigated problem and training data. This makes them fairly hard to be generally applied in practical scenarios, due to their significant complexities and inter-class variations of data bias situations. To address this issue, we propose a meta-model capable of adaptively learning an explicit weighting scheme directly from data. Specifically, by seeing each training class as a separate learning task, our method aims to extract an explicit weighting function with sample loss and task/class feature as input, and sample weight as output, expecting to impose adaptively varying weighting schemes to different sample classes bas
    
[^188]: 复合实张量乘积的草图及其在多项式核上的应用

    Complex-to-Real Sketches for Tensor Products with Applications to the Polynomial Kernel. (arXiv:2202.02031v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.02031](http://arxiv.org/abs/2202.02031)

    本论文提出一种Complex-to-Real草图方法，用于处理复合实张量乘积，取得了在多项式核上最先进的准确性和速度表现。

    

    针对$p$个向量的张量积的随机草图遵循统计效率和计算加速之间的权衡。通常使用的方法避免显式计算高维张量积，导致在嵌入维度上具有亚最优的$\mathcal{O}(3^p)$依赖关系。我们提出了一个简单的Complex-to-Real (CtR)修改既有的草图，通过用复数替换实数随机投影，在嵌入维度上只需要较低的$\mathcal{O}(2^p)$因子。我们草图的输出是实值的，这使得它们的下游用途变得简单。特别地，我们将我们的草图应用于$p$倍自张量输入，这些输入对应于多项式内核的特征映射。我们展示了我们的方法与文献中其他随机逼近相比，在准确性和速度方面实现了最先进的性能。

    Randomized sketches of a tensor product of $p$ vectors follow a tradeoff between statistical efficiency and computational acceleration. Commonly used approaches avoid computing the high-dimensional tensor product explicitly, resulting in a suboptimal dependence of $\mathcal{O}(3^p)$ in the embedding dimension. We propose a simple Complex-to-Real (CtR) modification of well-known sketches that replaces real random projections by complex ones, incurring a lower $\mathcal{O}(2^p)$ factor in the embedding dimension. The output of our sketches is real-valued, which renders their downstream use straightforward. In particular, we apply our sketches to $p$-fold self-tensored inputs corresponding to the feature maps of the polynomial kernel. We show that our method achieves state-of-the-art performance in terms of accuracy and speed compared to other randomized approximations from the literature.
    
[^189]: 一种面向物联网的分布式功能压缩机器学习框架

    A Machine Learning Framework for Distributed Functional Compression over Wireless Channels in IoT. (arXiv:2201.09483v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.09483](http://arxiv.org/abs/2201.09483)

    本论文开发了一种面向物联网的分布式功能压缩机器学习框架，采用了能够任意计算IoT所需函数压缩任务的Kolmogorov-Arnold表示定理，解决了基于云的方法在传输数据时给网络资源带来的压力问题。

    

    物联网设备产生的海量数据和最新的机器学习技术将共同革新物理系统。在许多不同的领域中，从自动驾驶到增强现实，分布式物联网设备计算特定的目标函数，而这些目标函数并不像障碍物检测、物体识别等具有简单形式。传统的基于云的方法专注于将数据传输到中心位置进行训练或推理，这给网络资源带来了巨大的压力。为了解决这个问题，我们开发了目前为止我们所知道的第一个面向物联网的分布式功能压缩机器学习框架，可以在高斯多路访问信道（GMAC）和正交AWGN信道上进行。由于Kolmogorov-Arnold表示定理，我们的机器学习框架可以通过设计为IoT的所需函数压缩任务计算任意任意函数。重要的是原始感官数据永远不会传输到中心节点进行训练或推理。

    IoT devices generating enormous data and state-of-the-art machine learning techniques together will revolutionize cyber-physical systems. In many diverse fields, from autonomous driving to augmented reality, distributed IoT devices compute specific target functions without simple forms like obstacle detection, object recognition, etc. Traditional cloud-based methods that focus on transferring data to a central location either for training or inference place enormous strain on network resources. To address this, we develop, to the best of our knowledge, the first machine learning framework for distributed functional compression over both the Gaussian Multiple Access Channel (GMAC) and orthogonal AWGN channels. Due to the Kolmogorov-Arnold representation theorem, our machine learning framework can, by design, compute any arbitrary function for the desired functional compression task in IoT. Importantly the raw sensory data are never transferred to a central node for training or inference
    
[^190]: 非静态适应性：面向在线凸优化的问题相关的动态遗憾

    Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization. (arXiv:2112.14368v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.14368](http://arxiv.org/abs/2112.14368)

    本研究提出一种面向在线凸优化的动态遗憾算法，可以在一些简单的问题实例中进一步增强保证，具有几何直观性，实验表明其优于最先进的基线算法。

    

    本文研究了在非静态环境中的在线凸优化，并选择动态遗憾作为性能度量，定义为在线算法和任何可行比较器序列所累计的损失之间的差值。假设$T$是时间长度，$P_T$是实质上反映环境非静态性的路径长度，则最先进的动态遗憾是$\mathcal{O}(\sqrt{T(1+P_T)})$。尽管这个界限被证明对于凸函数是最小化的，但我们在本文中展示，在一些简单的问题实例中，特别是当在线函数是光滑的时候，可以进一步增强保证。具体地，我们介绍了新颖的在线算法，可以利用光滑性，并用损失函数的梯度变化、比较器序列的累计损失和这两个项的最小值代替动态遗憾中对$T$的依赖。这些量被证明具有几何直观性，并且与环境中的非静态现象密切相关。对合成和真实数据集的广泛实验表明，我们的算法优于最先进的基线算法。

    We investigate online convex optimization in non-stationary environments and choose the dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\mathcal{O}(\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantitie
    
[^191]: 一种用于深度高斯过程的稀疏展开方法

    A Sparse Expansion For Deep Gaussian Processes. (arXiv:2112.05888v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.05888](http://arxiv.org/abs/2112.05888)

    本文提出了一种基于高斯过程的稀疏展开方法，用于构建深度高斯过程模型。该方法可以提高计算效率，使得模型更加稀疏。

    

    本文针对具有复杂分布的随机过程，采用深度高斯过程（DGP）作为统计替代品。传统的DGP模型推断方法的计算复杂度很高，因为需要使用核矩阵进行大规模的训练和推断。本文提出了一种基于一系列高斯过程（TMGP）的准确推断和高效训练方案。我们构建了一个名为分层展开的TMGP诱导近似。接着，我们将多个TMGP的分层展开组合成一种称为深度TMGP（DTMGP）的模型。该模型具有以下特性：（1）每个激活函数的输出都是确定性的，而权重是从标准高斯分布中独立选择的；（2）在训练或预测中，只有polylog（M）（M个中的一部分）个激活函数具有非零输出，这使得模型变得更加稀疏。

    In this work, we use Deep Gaussian Processes (DGPs) as statistical surrogates for stochastic processes with complex distributions. Conventional inferential methods for DGP models can suffer from high computational complexity as they require large-scale operations with kernel matrices for training and inference. In this work, we propose an efficient scheme for accurate inference and efficient training based on a range of Gaussian Processes, called the Tensor Markov Gaussian Processes (TMGP). We construct an induced approximation of TMGP referred to as the hierarchical expansion. Next, we develop a deep TMGP (DTMGP) model as the composition of multiple hierarchical expansion of TMGPs. The proposed DTMGP model has the following properties: (1) the outputs of each activation function are deterministic while the weights are chosen independently from standard Gaussian distribution; (2) in training or prediction, only polylog(M) (out of M) activation functions have non-zero outputs, which sig
    
[^192]: 利用神经网络和树搜索生成平面四边形网格

    Generate plane quad mesh with neural networks and tree search. (arXiv:2111.07613v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.07613](http://arxiv.org/abs/2111.07613)

    本论文提出了一种结合强化学习和树搜索的新方法，名为TreeMesh，用于生成高质量的平面四边形网格，可以比现有最先进的方法更快地生成。

    

    在有限元方法（FEM）的历史上，网格生成的质量一直被认为是为工程师提供可靠仿真结果的重要因素。目前最健壮的元素提取方法是采用寻找优化目标函数的下一个元素的方法来加快提取速度，但这可能导致经过多次迭代后局部网格质量较差。本文提出了TreeMesh，该方法将这种方法与强化学习（也可能是监督学习）和一种新的蒙特卡罗树搜索（MCTS）相结合。该算法基于先前提出的方法，经过多次改进后，在相同的边界上性能优于以前的工作。此外，利用树搜索，我们的程序可以比现有最先进的方法更快地生成高质量的平面四边形网格。

    The quality of mesh generation has long been considered a vital aspect in providing engineers with reliable simulation results throughout the history of the Finite Element Method (FEM). The element extraction method, which is currently the most robust method, is used in business software. However, in order to speed up extraction, the approach is done by finding the next element that optimizes a target function, which can result in local mesh of bad quality after many time steps. We provide TreeMesh, a method that uses this method in conjunction with reinforcement learning (also possible with supervised learning) and a novel Monte-Carlo tree search (MCTS) (Coulom(2006), Kocsis and Szepesv\'ari(2006), Browne et~al.(2012)). The algorithm is based on a previously proposed approach (Pan et~al.(2021)). After making many improvements on DRL (algorithm, state-action-reward setting) and adding a MCTS, it outperforms the former work on the same boundary. Furthermore, using tree search, our progr
    
[^193]: 规模下图神经网络的鲁棒性

    Robustness of Graph Neural Networks at Scale. (arXiv:2110.14038v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.14038](http://arxiv.org/abs/2110.14038)

    本文研究了规模下如何攻击和防御图神经网络（GNNs），提出了稀疏感知的一阶优化攻击和鲁棒性聚合函数Soft Median，有效提高了GNNs的可靠性和攻击力。

    

    图神经网络（GNNs）由于其广泛的应用和受欢迎程度而变得越来越重要。然而，针对对抗攻击的现有研究只依赖于相对较小的图形。本文填补了这一空白，研究了如何在规模下攻击和防御GNNs。我们提出了两种稀疏感知的一阶优化攻击，尽管优化参数数量与节点数量二次关联，但仍保持高效的表示。我们发现公共代理损失不适合用于全局攻击GNNs，而我们的替代方案可以将攻击力翻倍。此外，为了提高GNNs的可靠性，我们设计了一个鲁棒性聚合函数，Soft Median，得到了在所有规模下的有效防御。我们以标准GNNs为基础，对比以前的研究，评估了我们的攻击和防御方法在100倍以上的图形上的效果。我们甚至通过将我们的技术扩展到可扩展的GNN，将规模进一步扩展了一个数量级。

    Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs' reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.
    
[^194]: 通过协同恶意梯度筛选实现拜占庭容错联邦学习

    Byzantine-robust Federated Learning through Collaborative Malicious Gradient Filtering. (arXiv:2109.05872v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.05872](http://arxiv.org/abs/2109.05872)

    本文提出了一种基于逐元素符号的方法，用于检测模型污染攻击，以解决联邦学习中拜占庭攻击的问题。

    

    联邦学习中基于梯度的训练容易受到故障/恶意客户端的攻击，这些被称为拜占庭客户端。现有工作为了解决这个问题，要么利用中央参数服务器的辅助数据来验证接收到的梯度(例如计算验证错误率)，要么利用基于统计学方法(例如中位数和Krum)来识别并从拜占庭客户端中删除恶意梯度。本文提出，在实际情况下，辅助数据可能并不总是可用的，因此我们专注于基于统计学方法的方法。但是，最近的研究表明，精心制作的模型污染攻击可以规避大多数基于中位数和距离的统计防御方法，从而使恶意梯度与诚实梯度难以区分。为了解决这个问题，我们发现梯度向量的逐元素符号可以提供有价值的洞察力，用于检测模型污染攻击。

    Gradient-based training in federated learning is known to be vulnerable to faulty/malicious clients, which are often modeled as Byzantine clients. To this end, previous work either makes use of auxiliary data at parameter server to verify the received gradients (e.g., by computing validation error rate) or leverages statistic-based methods (e.g. median and Krum) to identify and remove malicious gradients from Byzantine clients. In this paper, we remark that auxiliary data may not always be available in practice and focus on the statistic-based approach. However, recent work on model poisoning attacks has shown that well-crafted attacks can circumvent most of median- and distance-based statistical defense methods, making malicious gradients indistinguishable from honest ones. To tackle this challenge, we show that the element-wise sign of gradient vector can provide valuable insight in detecting model poisoning attacks. Based on our theoretical analysis of the \textit{Little is Enough} 
    
[^195]: SelfCF：一种简单的自监督协同过滤框架

    SelfCF: A Simple Framework for Self-supervised Collaborative Filtering. (arXiv:2107.03019v3 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2107.03019](http://arxiv.org/abs/2107.03019)

    SelfCF是一种自监督协同过滤框架，用于推荐场景，通过增强现有的深度学习协同过滤模型中输出的嵌入来简化算法以及避免昂贵的计算和潜在的负样本问题。

    

    协同过滤（CF）被广泛用于从观察到的交互中学习有用的用户和项目的潜在表示。现有的基于CF的方法通常采用负采样来区分不同的项目。在大型数据集上使用负采样进行训练计算成本很高。此外，必须根据定义的分布谨慎选择负项，以避免在训练数据集中选择观察到的正项。不可避免地，从训练数据集中采样的一些负项在测试集中可能是正项。我们提出了一种专门用于隐式反馈推荐场景的自监督协同过滤框架（SelfCF）。所提出的SelfCF框架简化了连体网络，并可轻松应用于现有的基于深度学习的CF模型，我们称其为骨干网络。SelfCF的主要思想是增强由骨干网络生成的输出嵌入。

    Collaborative filtering (CF) is widely used to learn informative latent representations of users and items from observed interactions. Existing CF-based methods commonly adopt negative sampling to discriminate different items. Training with negative sampling on large datasets is computationally expensive. Further, negative items should be carefully sampled under the defined distribution, in order to avoid selecting an observed positive item in the training dataset. Unavoidably, some negative items sampled from the training dataset could be positive in the test set. In this paper, we propose a self-supervised collaborative filtering framework (SelfCF), that is specially designed for recommender scenario with implicit feedback. The proposed SelfCF framework simplifies the Siamese networks and can be easily applied to existing deep-learning based CF models, which we refer to as backbone networks. The main idea of SelfCF is to augment the output embeddings generated by backbone networks, b
    
[^196]: 冗余表示对宽神经网络的泛化有帮助

    Redundant representations help generalization in wide neural networks. (arXiv:2106.03485v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.03485](http://arxiv.org/abs/2106.03485)

    本文研究了各种卷积神经网络的最后一个隐藏层中的表示，发现如果最后一个隐藏表示足够宽，则其神经元倾向于分成携带相同信息的组，而冗余表示有助于宽神经网络的泛化。

    

    深度神经网络（DNN）打破了经典的偏差-方差权衡：为DNN添加参数以插值其训练数据通常会改善其泛化性能。解释在深度网络中“良性过拟合”的机制仍然是一个未解决的挑战。在这里，我们研究了各种最先进的卷积神经网络的最后一个隐藏层表示，并发现如果最后一个隐藏表示足够宽，则其神经元倾向于分成携带相同信息的组，仅由统计独立噪声区分彼此。这种组的数量随层的宽度呈线性增加，但仅在宽度高于临界值时才会增加。我们展示了冗余神经元仅在训练过程达到插值且训练误差为零时出现。

    Deep neural networks (DNNs) defy the classical bias-variance trade-off: adding parameters to a DNN that interpolates its training data will typically improve its generalization performance. Explaining the mechanism behind this ``benign overfitting'' in deep networks remains an outstanding challenge. Here, we study the last hidden layer representations of various state-of-the-art convolutional neural networks and find that if the last hidden representation is wide enough, its neurons tend to split into groups that carry identical information, and differ from each other only by statistically independent noise. The number of such groups increases linearly with the width of the layer, but only if the width is above a critical value. We show that redundant neurons appear only when the training process reaches interpolation and the training error is zero.
    
[^197]: 残差神经网络中的扩散机制：理论与应用

    Diffusion Mechanism in Residual Neural Network: Theory and Applications. (arXiv:2105.03155v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2105.03155](http://arxiv.org/abs/2105.03155)

    本文提出了一种扩散残差网络，该网络在神经网络架构中内部引入了扩散机制，能够提高数据点之间的距离直径比，从而提高了类间点的可分性，减少了类内点之间的距离。

    

    扩散是许多物理过程中出现的基本内部机制，描述了不同对象之间的相互作用。在许多具有有限训练样本的学习任务中，扩散连接了标记和未标记的数据点，是实现高分类精度的关键组成部分。许多现有的深度学习方法在训练神经网络时直接施加融合损失。在本文中，受对流-扩散常微分方程的启发，我们提出了一种新颖的扩散残差网络（Diff-ResNet），内部将扩散引入神经网络的架构中。在假定具有结构化数据的情况下，证明了所提出的扩散块可以增加距离直径比，从而改善了类间点的可分性并减少了局部类内点之间的距离。此外，这种性质可以被残差网络轻松采用以构建可分离的超平面。

    Diffusion, a fundamental internal mechanism emerging in many physical processes, describes the interaction among different objects. In many learning tasks with limited training samples, the diffusion connects the labeled and unlabeled data points and is a critical component for achieving high classification accuracy. Many existing deep learning approaches directly impose the fusion loss when training neural networks. In this work, inspired by the convection-diffusion ordinary differential equations (ODEs), we propose a novel diffusion residual network (Diff-ResNet), internally introduces diffusion into the architectures of neural networks. Under the structured data assumption, it is proved that the proposed diffusion block can increase the distance-diameter ratio that improves the separability of inter-class points and reduces the distance among local intra-class points. Moreover, this property can be easily adopted by the residual networks for constructing the separable hyperplanes. E
    
[^198]: 基于多样性保持的图结构细化的图表示学习

    Graph Representation Learning via Diversity-preserving Graph Refinement. (arXiv:2103.07295v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.07295](http://arxiv.org/abs/2103.07295)

    该论文提出了一种基于多样性保持的图结构细化的图表示学习方法，它利用已学习的节点表示来逐步改善图形结构质量。在多个下游任务上，包括节点分类、链接预测和图聚类，实验表明该方法优于现有的最先进方法。

    

    对于真实的图数据，节点之间的复杂关系通常被表示为硬性二进制链接。显然，这是一种离散和简化的连续关系形式，严重限制了学习到的节点表示的可表达性。另一方面，嵌入空间中获得的节点表示可以反过来揭示节点之间的内在关系。为了更好地特征化节点关系并进一步促进节点表示的学习，一种直观的方法是使用嵌入的节点表示来细化最初给定的图结构。但是，全局细化所有节点之间的关系无法区分将不可避免地导致一些噪声边缘，这可能进一步混淆节点表示学习模型的训练。此外，大型图形上也存在可扩展性问题。为了解决这些问题，我们提出了一种局部结构感知的图形细化方法，利用已经学到的节点表示逐步改善图形结构质量。具体而言，我们首先通过对图中的随机游走模拟生成一个多样化的邻域结构集。然后，对于每个模拟邻域，我们在整个细化过程中保持邻域结构的多样性，同时细化邻域内的节点关系。在各种基准数据集上进行的评估实验结果表明，所提出的方法在多个下游任务上，包括节点分类、链接预测和图聚类，均优于现有最先进的方法。

    For real-world graph data, the complex relationship between nodes is often represented as a hard binary link. Obviously, it is a discrete and simplified form of continuous relationship between nodes, which seriously limits the expressibility of the learned node representation. On the other hand, the node representation obtained in the embedding space can in turn be used to reveal the intrinsic relationship between nodes. To better characterize the node relationships and further facilitate the learning of node representation, an intuitive way is to refine the originally given graph structure with the embedded node representations. However, such global refinement of the relationships among all nodes without distinction will inevitably lead to some noisy edges, which may further confuse the training of the node representation learning model. In addition, it also has scalability problems on large graphs. To address these issues, we propose a local structure aware graph refinement to progre
    
[^199]: 线性回报的双重稳健汤普森抽样算法

    Doubly robust Thompson sampling for linear payoffs. (arXiv:2102.01229v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2102.01229](http://arxiv.org/abs/2102.01229)

    本文提出了一种新型多臂上下文赌博算法双重稳健汤普森抽样（DR Thompson Sampling），通过双重稳健估计器，解决了过去的上下文和奖励对选择依赖性导致的损失分解复杂等问题，得到了简化且改进的损失界限。

    

    汤普森抽样在多臂上下文赌博问题中的应用面临一些挑战，如何根据过去的上下文和奖励对进行选择的依赖性使得后悔分析的复杂性增加。本文提出了一种名为双重稳健汤普森抽样（DR Thompson Sampling）的新型多臂上下文赌博算法，采用在缺失数据领域中使用的双重稳健估计器用于基于上下文的汤普森抽样（LinTS）。双重稳健汤普森抽样让损失分解更加简单，从而使得改进后的损失界限降到了 $\tilde{O}(\phi^{-2}\sqrt{T})$，其中 $\phi^2$ 是上下文协方差矩阵中的最小特征值。这是 \texttt{LinTS} 第一个不基于上下文维度 $d$，而是使用 $\phi^2$ 的损失界限。

    A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing. The dependence of the arm choice on the past context and reward pairs compounds the complexity of regret analysis. We propose a novel multi-armed contextual bandit algorithm called Doubly Robust (DR) Thompson Sampling employing the doubly-robust estimator used in missing data literature to Thompson Sampling with contexts (\texttt{LinTS}). Different from previous works relying on missing data techniques (\citet{dimakopoulou2019balanced}, \citet{kim2019doubly}), the proposed algorithm is designed to allow a novel additive regret decomposition leading to an improved regret bound with the order of $\tilde{O}(\phi^{-2}\sqrt{T})$, where $\phi^2$ is the minimum eigenvalue of the covariance matrix of contexts. This is the first regret bound of \texttt{LinTS} using $\phi^2$ without the dimension of the context, $d$. Applying the relationship be
    
[^200]: 基准能量守恒神经网络用于学习动力学数据的研究

    Benchmarking Energy-Conserving Neural Networks for Learning Dynamics from Data. (arXiv:2012.02334v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.02334](http://arxiv.org/abs/2012.02334)

    本文调查了10个最近提出的能量守恒神经网络模型，并比较了它们在4个物理系统中的表现，说明了利用这些模型设计基于能量的控制器的可能性。

    

    近年来，将物理知识作为归纳偏置引入深度学习框架，特别是在利用神经网络从观测到的时间序列数据中学习动力学方程方面引起了越来越多的关注。在本文中，我们调查了包括HNN、LNN、DeLaN、SymODEN、CHNN、CLNN及其变体在内的10个最近提出的能量守恒神经网络模型。我们提供了这些模型的理论简洁演绎，并解释了它们的相似之处和差异。我们比较了这些模型在4个物理系统中的表现。我们指出了利用这些能量守恒模型设计基于能量的控制器的可能性。

    The last few years have witnessed an increased interest in incorporating physics-informed inductive bias in deep learning frameworks. In particular, a growing volume of literature has been exploring ways to enforce energy conservation while using neural networks for learning dynamics from observed time-series data. In this work, we survey ten recently proposed energy-conserving neural network models, including HNN, LNN, DeLaN, SymODEN, CHNN, CLNN and their variants. We provide a compact derivation of the theory behind these models and explain their similarities and differences. Their performance are compared in 4 physical systems. We point out the possibility of leveraging some of these energy-conserving models to design energy-based controllers.
    
[^201]: 一种针对无线传感器网络中关键节点识别的有监督主动学习方法（arXiv:2004.08885v4 [cs.NI] UPDATED）

    A supervised active learning method for identifying critical nodes in Wireless Sensor Network. (arXiv:2004.08885v4 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2004.08885](http://arxiv.org/abs/2004.08885)

    本论文提出了一种有监督主动学习方法，通过聚类和分类模块的迭代协作在较少的数据量下准确识别出对WSN特征影响更大的关键节点，克服了现有方法中针对非关键节点的识别偏差，并可应用于大规模WSN中。

    

    无线传感器网络（WSN）的能源效率取决于其主要特征，包括跳数、用户位置、分配功率和中继。然而，识别对这些特征影响更大的节点需要大量的计算开销和能源消耗。本文提出了一种主动学习方法，以解决在WSN中识别关键节点的计算开销问题。所提出的方法可以克服识别非关键节点的偏差，需要更少的精调工作来适应WSN的动态特性。该方法通过聚类和分类模块的协作迭代地减少典型监督学习场景中所需的数据数量，并在存在非信息性示例，即非关键节点的情况下提高准确性。实验表明，与现有技术相比，所提出的方法具有更大的灵活性，可用于大规模WSN中。

    Energy Efficiency of a wireless sensor network (WSN) relies on its main characteristics, including hop-number, user's location, allocated power, and relay. Identifying nodes, which have more impact on these characteristics, is, however, subject to a substantial computational overhead and energy consumption. In this paper, we proposed an active learning approach to address the computational overhead of identifying critical nodes in a WSN. The proposed approach can overcome biasing in identifying non-critical nodes and needs much less effort in fine-tuning to adapt to the dynamic nature of WSN. This method benefits from the cooperation of clustering and classification modules to iteratively decrease the required number of data in a typical supervised learning scenario and to increase the accuracy in the presence of uninformative examples, i.e., non-critical nodes. Experiments show that the proposed method has more flexibility, compared to the state-of-the-art, to be employed in large sca
    
[^202]: 最大熵采样问题的最佳主子矩阵选择：可扩展算法和性能保证。

    Best Principal Submatrix Selection for the Maximum Entropy Sampling Problem: Scalable Algorithms and Performance Guarantees. (arXiv:2001.08537v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2001.08537](http://arxiv.org/abs/2001.08537)

    本文提出了一个新的整数规划方法和连续松弛算法来解决最大熵采样问题，并提供了效率更高的确定性采样算法。同时，我们改进了已有的近似界限，并证明了局部搜索算法的第一个近似界限。

    

    本文研究了一种经典的最大熵采样问题（MESP），它旨在从协方差矩阵中选择最具信息量的主子矩阵。通过研究其拉格朗日对偶和原始特征，我们提出了一种新的MESP整数规划方法，并证明其连续松弛可得近似最优解。我们进一步提供了一种高效的确定性采样算法，并说明其在近似界方面优于已有文献中的最佳界限。通过对奇异矩阵开发新的数学工具和分析所提出的凸整数规划的拉格朗日对偶，我们研究了广泛使用的局部搜索算法，并证明其首个近似界限。

    This paper studies a classic maximum entropy sampling problem (MESP), which aims to select the most informative principal submatrix of a prespecified size from a covariance matrix. MESP has been widely applied to many areas, including healthcare, power system, manufacturing and data science. By investigating its Lagrangian dual and primal characterization, we derive a novel convex integer program for MESP and show that its continuous relaxation yields a near-optimal solution. The results motivate us to study an efficient sampling algorithm and develop its approximation bound for MESP, which improves the best-known bound in literature. We then provide an efficient deterministic implementation of the sampling algorithm with the same approximation bound. By developing new mathematical tools for the singular matrices and analyzing the Lagrangian dual of the proposed convex integer program, we investigate the widely-used local search algorithm and prove its first-known approximation bound f
    
[^203]: IMAE用于噪声鲁棒学习：绝对值误差不平等对待示例，梯度大小的方差很重要。

    IMAE for Noise-Robust Learning: Mean Absolute Error Does Not Treat Examples Equally and Gradient Magnitude's Variance Matters. (arXiv:1903.12141v10 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1903.12141](http://arxiv.org/abs/1903.12141)

    本文提出IMAE模型用于畸形训练数据的鲁棒深度学习，通过实践证实平均绝对误差（MAE）在处理示例时存在欠拟合问题，利用加权方差调整提高了拟合能力，同时保持了鲁棒性。

    

    本文研究了从示例加权角度，即与对数的梯度大小来看待畸形训练数据的鲁棒深度学习。我们有两个关键发现：（1）平均绝对误差（MAE）不平等地处理示例。我们针对MAE进行了新的观察和深入分析，理论证明其鲁棒性。首先，我们揭示了其在实践中的欠拟合问题。其次，我们分析了MAE的鲁棒性是通过强调不确定示例而不是像前人研究中所声称的那样对待训练样本来实现的。（2）梯度大小的方差很重要。我们提出了一种有效而简单的解决方案，以增强MAE的拟合能力，同时保持其鲁棒性。在不改变MAE的整体加权方案（即哪些示例获得更高的权重）的情况下，我们仅通过非线性地改变其加权方差来实现这一点。

    In this work, we study robust deep learning against abnormal training data from the perspective of example weighting built in empirical loss functions, i.e., gradient magnitude with respect to logits, an angle that is not thoroughly studied so far. Consequently, we have two key findings: (1) Mean Absolute Error (MAE) Does Not Treat Examples Equally. We present new observations and insightful analysis about MAE, which is theoretically proved to be noise-robust. First, we reveal its underfitting problem in practice. Second, we analyse that MAE's noise-robustness is from emphasising on uncertain examples instead of treating training samples equally, as claimed in prior work. (2) The Variance of Gradient Magnitude Matters. We propose an effective and simple solution to enhance MAE's fitting ability while preserving its noise-robustness. Without changing MAE's overall weighting scheme, i.e., what examples get higher weights, we simply change its weighting variance non-linearly so that the i
    

