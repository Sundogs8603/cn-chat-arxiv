# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning.](http://arxiv.org/abs/2310.03731) | 本文介绍了一种方法，通过微调开源语言模型，使其能够使用代码进行数学建模和推导，从而增强数学推理能力。作者提出了一种生成包含数学问题和基于代码的解决方案的数据集，并引入了定制的微调和推理方法，从而实现了在解决具有挑战性的数学问题上生成基于代码的解决方案的 MathCoder 模型。 |
| [^2] | [Stochastic interpolants with data-dependent couplings.](http://arxiv.org/abs/2310.03725) | 本文提出了一种使用数据依赖耦合来构建生成模型的方法，并展示了在超分辨率和修复任务中的实验效果。 |
| [^3] | [Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance.](http://arxiv.org/abs/2310.03722) | 本文提出了两种新的“e-process”和置信序列方法，分别通过替换Lai的混合方法，并分析了所得结果的宽度。 |
| [^4] | [HeaP: Hierarchical Policies for Web Actions using LLMs.](http://arxiv.org/abs/2310.03720) | 这篇论文介绍了一个名为HeaP的框架，利用大型语言模型（LLMs）来解决Web任务的挑战。该框架将Web任务分解为子任务，并通过一系列低级的策略来执行，相比其他基准方法，该框架在不同的Web任务上表现出更好的性能。 |
| [^5] | [Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning.](http://arxiv.org/abs/2310.03718) | 这个论文提出了一种约束条件下的策略优化框架，用于训练多功能安全强化学习智能体。通过引入多功能值估计和有条件的变分推理模块，该框架在训练效率和零-shot适应能力方面表现优于基准方法。 |
| [^6] | [A Long Way to Go: Investigating Length Correlations in RLHF.](http://arxiv.org/abs/2310.03716) | 这篇论文通过研究RLHF中奖励和长度的关系，发现优化响应长度是RLHF在提高模型性能方面的一个重要因素。 |
| [^7] | [DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines.](http://arxiv.org/abs/2310.03714) | DSPy是一个编程模型，将LM流水线抽象为文本转换图，通过声明性模块调用LM实现优化，能够解决复杂的推理问题和数学问题等任务。 |
| [^8] | [Agent Instructs Large Language Models to be General Zero-Shot Reasoners.](http://arxiv.org/abs/2310.03710) | 该论文提出了一种方法，通过代理指导的方式，大大提高了大型语言模型在零-shot推理任务上的能力，并在多个数据集上实现了最先进的性能。 |
| [^9] | [Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization.](http://arxiv.org/abs/2310.03708) | 本文提出了一种无强化学习的算法，称为多目标直接偏好优化（MODPO），它可以根据不同的偏好训练不同的语言模型，通过组合所有目标和特定权重来优化模型。 |
| [^10] | [OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks.](http://arxiv.org/abs/2310.03707) | 本文介绍了一种自我监督的、计算经济的方法，用于生成具有迁移性的规避攻击。实验证明该方法在各种模型、未见数据类别甚至防御模型上都是有效的。 |
| [^11] | [Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities.](http://arxiv.org/abs/2310.03696) | 本文研究了具有多变量非线性激活函数的神经网络架构在Banach空间的优化性，并构建了一类新的Banach空间家族。结果表明，学习问题的解集完全由具有多变量非线性的神经网络架构来描述。这些最优架构具有跳跃连接，并与正交权重归一化和多索引模型密切相关。 |
| [^12] | [Multimarginal generative modeling with stochastic interpolants.](http://arxiv.org/abs/2310.03695) | 该论文研究了多边际生成建模问题，提出了一种基于随机插值方法的学习算法，可以恢复给定概率密度作为边际的联合分布并且识别多向对应关系。 |
| [^13] | [Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!.](http://arxiv.org/abs/2310.03693) | 微调对齐语言模型会牺牲安全性，即使用户没有恶意意图。通过敌对设计的训练样本，即使只有少数10个，也可以破坏语言模型的安全对齐。这种安全风险存在于微调后的模型中。 |
| [^14] | [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks.](http://arxiv.org/abs/2310.03684) | SmoothLLM是第一个用于减轻大型语言模型上越狱攻击的算法，通过在输入提示上随机扰动并汇总预测结果来检测对抗性输入，将攻击成功率降低至不到一个百分点，并提供了可证明的保证。 |
| [^15] | [Hadamard Domain Training with Integers for Class Incremental Quantized Learning.](http://arxiv.org/abs/2310.03675) | 本文提出了一种使用廉价的Hadamard变换来实现整数矩阵乘法的低精度训练技术，以在资源受限的边缘平台上实现计算和内存的高效性。 |
| [^16] | [Strategic Evaluation: Subjects, Evaluators, and Society.](http://arxiv.org/abs/2310.03655) | 本论文以评估设计作为出发点，研究了在形式和量化评估中的战略行为以及评估者和被评估对象之间的道德判断关系。 |
| [^17] | [Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics.](http://arxiv.org/abs/2310.03652) | 该论文提出了一种训练物理增强神经网络的极度稀疏化方法，以实现在力学中可解释的模型发现。这种方法能够生成具有较少可训练参数的本构模型，使其更易于解释和理解。 |
| [^18] | [Rethinking Fairness for Human-AI Collaboration.](http://arxiv.org/abs/2310.03647) | 在人工智能与人类合作中，需要重新思考公平性，因为完全遵守算法决策很少是现实可行的，因此我们需要设计稳健公平的算法推荐来提升公平性。 |
| [^19] | [TRAM: Bridging Trust Regions and Sharpness Aware Minimization.](http://arxiv.org/abs/2310.03646) | TRAM是一种桥接信任区域和锐度感知最小化的算法，通过减少损失曲面的曲率来提供鲁棒性改进。它通过在微调过程中优化可转移的表示来实现领域外泛化，并且通过结合信任区域方法和SAM风格的正则化器来统一参数和表示空间平滑方法。TRAM在保持预训练结构的同时实现了平坦的极小值和平滑、有信息量的表示。 |
| [^20] | [Distributional PAC-Learning from Nisan's Natural Proofs.](http://arxiv.org/abs/2310.03641) | 本论文研究了从Nisan的自然证明中的分布式PAC学习，并得到了正面和负面的结果。 |
| [^21] | [CLEVRER-Humans: Describing Physical and Causal Events the Human Way.](http://arxiv.org/abs/2310.03635) | CLEVRER-Humans是一个用于因果判断的视频推理数据集，通过人工标注来解决合成事件和合成语言描述的缺乏多样性问题，并通过迭代事件填空和神经语言生成模型提高数据收集效率。 |
| [^22] | [High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning.](http://arxiv.org/abs/2310.03624) | 本论文介绍了一种使用神经领域进行机器人自建模和运动规划的方法。通过利用2D图像和相机姿态进行学习，无需深度图像或几何知识，实现了对高自由度物体的建模。在7自由度机器人测试中，所学的自建模与真实模型的差距仅为2%。 |
| [^23] | [CLASSify: A Web-Based Tool for Machine Learning.](http://arxiv.org/abs/2310.03618) | CLASSify是一个基于Web的机器学习工具，能够简化生物信息学中的分类问题的解决过程。它提供了自动化模型训练和结果生成，可视化和数据洞察功能，支持二元和多类分类问题，并提供多种模型和方法的访问。它还能生成合成数据，支持特征评估和解释性得分分析。 |
| [^24] | [Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally.](http://arxiv.org/abs/2310.03614) | 对抗机器学习面临的漏洞和对社会的不良影响问题，可以通过将对手重新定义为盟友，来实现有益于社会的创新应用。 |
| [^25] | [Solving a Class of Non-Convex Minimax Optimization in Federated Learning.](http://arxiv.org/abs/2310.03613) | 本研究针对联邦学习中的一类非凸最小极大优化问题，提出了FL算法（FedSGDA+和FedSGDA-M），并在最常见的最小极大问题中降低了复杂度。针对非凸凹问题，提出的FedSGDA+算法将通信复杂度降低到O(ε^{-6})。在非凸强凹和非凸PL最小极大设置下，证明了FedSGDA-M具有已知的样本复杂度。 |
| [^26] | [GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data.](http://arxiv.org/abs/2310.03611) | GENER是一个并行层深度学习网络，专门用于从基因表达数据中检测基因-基因相互作用。实验证实GENER在预测基因-基因相互作用方面的性能优于其他统计和深度学习方法。 |
| [^27] | [Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review.](http://arxiv.org/abs/2310.03606) | 本文献综述比较了在预测非洲COVID-19病例中使用的各种时间序列分析方法，突出了它们的有效性和局限性。 |
| [^28] | [FASER: Binary Code Similarity Search through the use of Intermediate Representations.](http://arxiv.org/abs/2310.03605) | 本论文提出了一种名为FASER的方法，通过使用中间表示进行二进制代码相似性搜索。该方法可以跨架构地识别函数，并明确编码函数的语义，以支持各种应用场景。 |
| [^29] | [Sampling via Gradient Flows in the Space of Probability Measures.](http://arxiv.org/abs/2310.03597) | 通过梯度流抽样方法的研究方向在计算科学和工程中具有重要意义。本文通过研究概率测度空间中的梯度流的设计组成部分，提出了三个贡献：Kullback-Leibler散度作为能量泛函的独特属性、度量的选择与不变性的关系。 |
| [^30] | [TimeGPT-1.](http://arxiv.org/abs/2310.03589) | TimeGPT是第一个面向时间序列的基础模型，能够生成准确的预测。它在性能、效率和简洁性方面优于现有的统计学、机器学习和深度学习方法。我们的研究提供了有力的证据，表明借鉴其他人工智能领域的见解可以有效应用于时间序列分析。大规模时间序列模型有望民主化访问精确的预测并减少不确定性。 |
| [^31] | [Smoothing Methods for Automatic Differentiation Across Conditional Branches.](http://arxiv.org/abs/2310.03585) | 本研究提出了一种通过结合平滑解释和自动微分的方法来处理具有条件分支的程序，并成功计算出平滑程序的梯度，从而支持分支程序参数合成和机器学习流程中的模型校准。 |
| [^32] | [Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End.](http://arxiv.org/abs/2310.03581) | 本文提出了一种基于强化学习的本地导航策略，通过将感知失效建模为障碍物和坑洞，从受损的感知中重建环境信息并进行端到端的反应，实现了在受损感知下的可靠导航。 |
| [^33] | [Targeted Adversarial Attacks on Generalizable Neural Radiance Fields.](http://arxiv.org/abs/2310.03578) | 本文介绍了如何通过低强度对抗攻击和对抗补丁对具有推广能力的NeRFs进行攻击，后者足够强大，可以应用于现实世界中。我们还展示了针对性攻击，这些攻击成功地生成了预定义输出场景。 |
| [^34] | [Analysis of learning a flow-based generative model from limited sample complexity.](http://arxiv.org/abs/2310.03575) | 我们分析了从有限样本复杂度中训练基于流的生成模型的问题，并提供了尖锐的端到端分析。我们找到了学习到的速度场的紧凑特性，并描述了生成流的近似，该近似将基本高斯密度推向目标密度。我们还提供了生成混合物均值与目标混合物均值之间距离的闭式公式，并证明其衰减速度为$\Theta_n(\frac{1}{n})$，这实际上是贝叶斯最优的。 |
| [^35] | [Residual Multi-Fidelity Neural Network Computing.](http://arxiv.org/abs/2310.03572) | 本研究提出了一种残差多保真计算框架，通过使用多保真信息构建神经网络代理模型，解决了低保真和高保真计算模型之间的相关性建模问题。这种方法训练了两个神经网络，利用残差函数进行模型训练，最终得到了高保真替代模型。 |
| [^36] | [BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields.](http://arxiv.org/abs/2310.03563) | BID-NeRF算法改进了图像姿态估计问题，并提出了几种改进：引入基于深度的损失函数和多图像损失函数，省略分层采样过程，通过扩展采样间隔实现更高的初始姿态估计误差的收敛。这些修改显著提高了收敛速度和收敛基准。 |
| [^37] | [Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective.](http://arxiv.org/abs/2310.03556) | 本文介绍了一种使用Leave-One-Out最大对数似然目标稳定训练概率模型的方法，通过自适应核密度估计模型和留一法最大对数似然准则，解决了数据密度不均匀困难，并通过分配可学习权重扩展模型，加速了训练过程。 |
| [^38] | [Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models.](http://arxiv.org/abs/2310.03546) | 本研究提出了一种插拔式后验采样算法（PnP-ULA），通过将物理测量模型与深度学习先验相结合，解决了成像逆问题。我们通过理论分析和数值验证，量化了PnP-ULA在不匹配后验分布下的误差界限，结果表明PnP-ULA对于测量模型和去噪器的不匹配非常敏感。 |
| [^39] | [Distribution-free risk assessment of regression-based machine learning algorithms.](http://arxiv.org/abs/2310.03545) | 这项研究解决了在机器学习算法中计算预测模型失效概率的风险评估任务，使用符合性预测方法进行预测区间计算，并证明了其保守性质。 |
| [^40] | [Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks.](http://arxiv.org/abs/2310.03530) | 本研究通过探索联合群不变函数在数据-参数域上的作用，提出了一种系统的规则来解码神经网络内部数据表示中的对称性和几何性。利用这一规则，我们引入了由联合不变函数导出的通用神经网络，并利用群论证明了其普适性。这一研究揭示了逼近理论和深度学习中的群论方面，并将几何深度学习与抽象调和分析相连接。 |
| [^41] | [Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks.](http://arxiv.org/abs/2310.03529) | 通过对数据空间上的群作用来识别DNN内部的隐藏层，并将DNN构建为相对于Koopman算子的双声变换，我们利用群论论证证明了这些DNN的普适性。 |
| [^42] | [High-dimensional Bayesian Optimization with Group Testing.](http://arxiv.org/abs/2310.03515) | 本研究提出了一种高维贝叶斯优化方法，通过组测试来识别活动变量，以实现高效优化。该方法在多个合成和真实高维优化任务上与最先进的方法相竞争。 |
| [^43] | [Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models.](http://arxiv.org/abs/2310.03512) | 本研究提出了一种使用单一IMU和分层机器学习模型监测年长者奥塔哥锻炼的准确系统。利用深度学习模型判断患者是在进行OEP还是日常生活活动，可以监测OEP的参与情况。 |
| [^44] | [Deep Generative Models of Music Expectation.](http://arxiv.org/abs/2310.03500) | 本文使用深度生成模型，通过学习复杂的非线性特征，计算音乐输入序列的近似概率，以更好地预测音乐期望。 |
| [^45] | [How the level sampling process impacts zero-shot generalisation in deep reinforcement learning.](http://arxiv.org/abs/2310.03494) | 这项研究探讨了非均匀采样策略对深度强化学习代理的零样本泛化能力的影响，通过测量代理的内部表示与训练层级集之间的相互信息，发现基于值损失优先级的自适应采样策略能更好地避免过拟合。 |
| [^46] | [TPDR: A Novel Two-Step Transformer-based Product and Class Description Match and Retrieval Method.](http://arxiv.org/abs/2310.03491) | TPDR是一种基于Transformer的产品和类描述匹配与检索方法，通过注意机制和对比学习来实现语义对应关系的探索。 |
| [^47] | [BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic Classification.](http://arxiv.org/abs/2310.03485) | 本论文提出了一种名为BTDNet的多模态方法，利用多参量MRI扫描预测MGMT启动子甲基化状态。BTDNet解决了可变的体积长度和体积级注释的挑战，并在数据增强、3D分析、特征提取和切片级分类方面进行了优化。 |
| [^48] | [The Geometric Structure of Fully-Connected ReLU-Layers.](http://arxiv.org/abs/2310.03482) | 该论文研究了神经网络中完全连接的ReLU层的几何结构。研究发现，在每个划分区域内，ReLU层可以被大大简化，可以将其解释为一个投影到多面体锥体，然后进行仿射变换。此结构还简化了分区区域与超平面交集的反像表达式，对于描述分类问题中的决策边界非常有用。此外，对于具有一个隐藏ReLU层的前馈网络，论文提供了关于这些网络生成的决策边界几何复杂性的结果，并证明了这些决策边界在仿射变换的模下是相等的。 |
| [^49] | [The Cadenza ICASSP 2024 Grand Challenge.](http://arxiv.org/abs/2310.03480) | Cadenza项目组织了ICASSP SP Cadenza Challenge，旨在通过音乐分解/混音来提升助听器音质，处理过程考虑音乐、增益和听力损失。 |
| [^50] | [The Blame Problem in Evaluating Local Explanations, and How to Tackle it.](http://arxiv.org/abs/2310.03466) | 最近局部模型无关解释技术的提出数量迅速增长，在评估这些技术时存在归因问题。我们提出了一种新的分类方法来评估局部解释，并强调除了基于可解释模型的基准真实数据之外的所有评估方法都遭受了这个问题。我们认为基于可解释模型的评估方法是更合理的方法。 |
| [^51] | [Which mode is better for federated learning? Centralized or Decentralized.](http://arxiv.org/abs/2310.03461) | 中国总结出的一句话要点：研究发现在联合学习中，集中化的方法总是比分散化的方法更好地进行泛化，同时，部分参与在集中化方法中表现更好，而在分散化方法中，拓扑结构对性能的影响十分重要。 |
| [^52] | [Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization.](http://arxiv.org/abs/2310.03456) | 本论文提出了一种多分辨率音频-视觉特征融合的方法，通过分层门控交叉注意力机制将不同时间分辨率的音频-视觉数据进行合并，在时域动作定位任务中有效地整合了音频特征，并取得了显著的性能提升。 |
| [^53] | [FLAIM: AIM-based Synthetic Data Generation in the Federated Setting.](http://arxiv.org/abs/2310.03447) | FLAIM是一个在联邦设置中基于AIM的合成数据生成方法，该方法解决了差分隐私方向的技术在联邦场景下的适用问题，并提出了FLAIM方法来维持较高的效用和处理异构性。 |
| [^54] | [Variational Inference for GARCH-family Models.](http://arxiv.org/abs/2310.03435) | 变分推断在GARCH家族模型的贝叶斯推断中是一种可靠和可行的方法。 |
| [^55] | [Neural Language Model Pruning for Automatic Speech Recognition.](http://arxiv.org/abs/2310.03424) | 本文研究了应用于自动语音识别的基于Transformer的神经网络语言模型的模型修剪方法。通过对修剪框架的准则、方法和调度器进行分析，我们发现数据驱动的修剪在多个场景中优于幅度驱动的修剪，逐步修剪在准确度方面优于一次性修剪，并提出了适用于逐步压缩模型的低秩逼近方法，为中等压缩程度下的体积减小和推理加速提供了最佳平衡。 |
| [^56] | [Pre-Training and Fine-Tuning Generative Flow Networks.](http://arxiv.org/abs/2310.03419) | 这个论文提出了一种新的方法来实现生成性流网络的无奖励预训练，并通过自监督问题的形式训练了一个条件的GFlowNet（OC-GFN），用于有效适应下游任务。 |
| [^57] | [Over-the-Air Federated Learning with Compressed Sensing: Is Sparsification Necessary?.](http://arxiv.org/abs/2310.03410) | 本研究通过比较多种通信设计，发现在空中联合学习中压缩之前的稀疏化并不必要。 |
| [^58] | [RUSOpt: Robotic UltraSound Probe Normalization with Bayesian Optimization for In-plane and Out-plane Scanning.](http://arxiv.org/abs/2310.03406) | 提出了一种使用贝叶斯优化方法对机器人超声探头进行标准化的方法，通过自动调整探头与扫描表面的接触点的正交方向，提高了超声图像的质量。 |
| [^59] | [EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD Diagnosis via Inter-regional Relation Learning.](http://arxiv.org/abs/2310.03404) | EAG-RS是一种创新的解释性引导的感兴趣区（ROI）选择框架，通过非线性高阶功能关联以及可解释的人工智能技术，实现了自闭症谱系障碍（ASD）的诊断，并解决了现有研究中的一些局限性。 |
| [^60] | [Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning.](http://arxiv.org/abs/2310.03400) | 本文介绍了如何对LLM模型进行微调以实现内容审查的私下部署，讨论了引入原因的微调过程和直接分类任务的区别，并研究了利用更强大的LLM生成的原因对微调的影响。 |
| [^61] | [GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks.](http://arxiv.org/abs/2310.03399) | GRAPES是一种自适应图采样方法，通过学习识别在训练图神经网络分类器时具有影响力的节点集合，解决了可扩展图神经网络中的内存问题，并且在准确性和可扩展性方面表现出色。 |
| [^62] | [Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein.](http://arxiv.org/abs/2310.03398) | 本论文介绍了一种通过Gromov-Wasserstein可实现在聚类和降维之间插值的方法。我们通过解决半松弛的最优传输问题，计算输入和嵌入样本之间的对应关系，从而实现同时减少样本和特征数量的降维。我们展示了当嵌入的维度不受约束时，该方法可以提供具有竞争力的硬聚类。通过将降维和聚类融合为中间阶段，我们强调了该方法在总结真实数据方面的重要性，并在图像数据集上进行了可视化应用。 |
| [^63] | [Learning to Simplify Spatial-Temporal Graphs in Gait Analysis.](http://arxiv.org/abs/2310.03396) | 本文提出了一种学习简化步态分析中的时空图的方法，用于步态基于性别估计。该方法的关键创新点是使用上游和下游模型来调整每个行走实例的邻接矩阵，从而提高了可解释性并保持了性能。通过实验证明，该方法在CASIA-B数据集上表现出良好的效果。 |
| [^64] | [Uncertainty quantification for deep learning-based schemes for solving high-dimensional backward stochastic differential equations.](http://arxiv.org/abs/2310.03393) | 本文研究了一类基于深度学习的高维回溯随机微分方程数值方案的不确定性量化方法，并开发了一个高效估计近似解标准差的UQ模型。 |
| [^65] | [OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon.](http://arxiv.org/abs/2310.03388) | OpenPatch是一个3D拼贴，基于大型预训练模型提取中间特征来描述每个已知类别的补丁表示，在超出分布检测中取得了进展。 |
| [^66] | [Machine learning the interaction network in coupled dynamical systems.](http://arxiv.org/abs/2310.03378) | 本研究使用自监督神经网络模型，从观察到的轨迹数据中恢复耦合动力系统的相互作用网络并预测个体代理的动态。 |
| [^67] | [Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet.](http://arxiv.org/abs/2310.03365) | Swin-Tempo是一个创新模型，使用了Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知。它克服了现有网络的计算复杂性问题，提高了肺结节检测的准确率。 |
| [^68] | [Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention.](http://arxiv.org/abs/2310.03358) | 本文提出了一个通用的对抗训练（AT）框架，通过非对称负对比度和反向注意力，学习鲁棒的特征表征，以提高神经网络的对抗鲁棒性能。 |
| [^69] | [Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games.](http://arxiv.org/abs/2310.03354) | 该论文介绍了自我对弈和策略空间响应预测（PSRO）作为解决竞争游戏的强化学习框架，发现自我对弈方法在混合合作竞争游戏中无法收敛到全局纳什均衡（NE），而PSRO能够在这种情况下有效地学习到最佳响应。然而，PSRO需要重复训练联合策略，增加了难度。 |
| [^70] | [Deep Geometric Learning with Monotonicity Constraints for Alzheimer's Disease Progression.](http://arxiv.org/abs/2310.03353) | 本文提出了一种采用单调性约束的深度几何学习方法来预测阿尔茨海默病（AD）的进展。这种方法通过结合递归神经网络和普通微分方程在黎曼空间中建模时间序列数据，弥补了现有方法对数据几何属性考虑不足的问题，并解决了从不完整样本中外推正定对称度量的限制。这项工作在临床诊断和治疗中具有重要的应用价值。 |
| [^71] | [An Integrated Algorithm for Robust and Imperceptible Audio Adversarial Examples.](http://arxiv.org/abs/2310.03349) | 本论文提出了一个综合算法，使用心理声学模型和房间冲激响应（RIR）来生成稳健且不可察觉的音频对抗样本。通过在模拟环境和真实空中场景中进行实验，以及进行人类研究评估可察觉性，我们比较了不同的方法。 |
| [^72] | [LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework.](http://arxiv.org/abs/2310.03342) | 本文提出了一种通过选项框架学习集成探索策略的强化学习统一框架。在MiniGrid和Atari环境中的实验表明该框架的有效性。 |
| [^73] | [Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs.](http://arxiv.org/abs/2310.03339) | 本文提出了一种基于长短期记忆网络的德国-卢森堡日前电力价格预测模型，该模型通过联合预测均值和标准差实现了概率预测，并使用超统计学方法对价格的统计性质进行解释。 |
| [^74] | [Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System.](http://arxiv.org/abs/2310.03334) | 本研究工作重点研究非定向白盒对抗攻击在实时深度学习网络入侵检测系统中的应用，以及采用启发式防御方法来保护计算机网络免受各种网络安全威胁。 |
| [^75] | [Fine-tune Language Models to Approximate Unbiased In-context Learning.](http://arxiv.org/abs/2310.03331) | 这篇论文介绍了一种精调语言模型的算法，名为RICL，并提出了一种低成本的线性最优权重近似算法LARICL。这些算法可以通过使用无偏验证集和确定每个输入-输出示例的最佳权重，实现无偏的上下文学习。 |
| [^76] | [Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning.](http://arxiv.org/abs/2310.03325) | 本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。 |
| [^77] | [Investigating the Limitation of CLIP Models: The Worst-Performing Categories.](http://arxiv.org/abs/2310.03324) | CLIP模型表现最差的类别的性能明显低于整体表现，揭示了其在特定类别重要性较高的风险敏感应用中的潜在风险。为了解决这个问题，研究了CLIP模型的模态对齐，并提出了用于衡量最差类别的推理混淆的类别匹配边界（CMM）。 |
| [^78] | [BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph.](http://arxiv.org/abs/2310.03320) | BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。 |
| [^79] | [Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction.](http://arxiv.org/abs/2310.03314) | 本研究提出一种新颖的人体运动预测框架，结合人体关节约束和场景约束，利用高斯过程回归模型预测一定时间范围内的人体动作，以增强人机协作。 |
| [^80] | [Certifiably Robust Graph Contrastive Learning.](http://arxiv.org/abs/2310.03312) | 本文提出了第一个可证明鲁棒的图对比学习（GCL）框架，并引入了随机边缘平滑（RES）技术，以确保任何GCL模型的可证明鲁棒性，并通过有效的训练方法增强了GCL的鲁棒性。 |
| [^81] | [Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses.](http://arxiv.org/abs/2310.03311) | 该论文介绍了一个基于信息理论的统一原理，用于重新推导和推广现有的变分降维方法，并设计新的方法。通过将多变量信息瓶颈解释为两个贝叶斯网络的权衡，该框架引入了一个在压缩数据和保留信息之间的权衡参数。 |
| [^82] | [Benchmarking Large Language Models As AI Research Agents.](http://arxiv.org/abs/2310.03302) | 本研究提出了MLAgentBench，一个用于对AI研究代理进行基准测试的ML任务套件，代理可以执行各种操作，从而运行实验、分析结果并修改整个机器学习流程的代码。这可以帮助我们构建和评估能够执行长期目标任务的AI研究代理。 |
| [^83] | [Learning Energy Decompositions for Partial Inference of GFlowNets.](http://arxiv.org/abs/2310.03301) | 本文提出了一种学习能量分解方法（LED-GFN），用于改进生成性流网络（GFlowNets）在采样对象时的部分推断。方法利用可学习的潜在函数对流函数进行重新参数化，解决了在操作序列中能量波动较大时的训练信号误导问题。 |
| [^84] | [A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling.](http://arxiv.org/abs/2310.03298) | 提出了一种基于潜变量的方法，用于非层次化多保真度自适应采样。该方法能够利用不同保真度模型之间的相关性以更高效地探索和利用设计空间。 |
| [^85] | [LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers.](http://arxiv.org/abs/2310.03294) | LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。 |
| [^86] | [PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches.](http://arxiv.org/abs/2310.03288) | 这项工作使用深度学习方法开发了一个集成模型PoseAction，该模型利用计算机视觉和异步交互聚合网络来实现病房患者的动作识别和预测。通过准确的患者检测和常见动作的预测，可以提高病房的护理效率和降低护理成本。 |
| [^87] | [Burning the Adversarial Bridges: Robust Windows Malware Detection Against Binary-level Mutations.](http://arxiv.org/abs/2310.03285) | 本文针对二进制级变异的强健Windows恶意软件检测问题，分析了现有检测系统的攻击面，并提出了一种基于软件预处理和图像提取的方案以应对对抗环境。实验结果表明传统模型对抗威胁无效。 |
| [^88] | [A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions.](http://arxiv.org/abs/2310.03281) | 这种研究引入了一种新的语言模型UTR-LM，通过对多个物种的5' UTR进行预训练，并结合有监督信息，该模型在多个下游任务中的表现超过了现有的最佳模型，可以有效预测平均核糖体负载、翻译效率、mRNA表达水平，并改进了内源性核糖体进入位点的识别性能。 |
| [^89] | [Mitigating Pilot Contamination and Enabling IoT Scalability in Massive MIMO Systems.](http://arxiv.org/abs/2310.03278) | 本文提出了一种基于物联网设备数据传输模式的创新导频分配方案，解决了大规模MIMO系统中的导频污染和可扩展性问题。 |
| [^90] | [Fragment-based Pretraining and Finetuning on Molecular Graphs.](http://arxiv.org/abs/2310.03274) | 该论文提出了一种在分子图上以片段级别进行预训练和微调的方法，通过对常见片段进行对比和预测预训练任务，克服了节点级和图级预训练的限制。 |
| [^91] | [Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning.](http://arxiv.org/abs/2310.03273) | 本研究通过在典型方法MONet上进行消融研究，旨在阐明多目标表示学习中物体分割的机制，从而揭示了先前方法中适当物体分割的方法并对VAE正则化的贡献进行了探索。 |
| [^92] | [Network Alignment with Transferable Graph Autoencoders.](http://arxiv.org/abs/2310.03272) | 该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。 |
| [^93] | [UniPredict: Large Language Models are Universal Tabular Predictors.](http://arxiv.org/abs/2310.03266) | 本文提出了UniPredict，一个基于大型语言模型的通用表格数据预测器，能够扩展到庞大的表格数据集，并具备理解多样化表格输入和根据输入指令预测目标变量的能力。实验结果表明，UniPredict模型在与其他模型相比时具有显著优势。 |
| [^94] | [Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets.](http://arxiv.org/abs/2310.03258) | 通过转移反事实学习的方法，我们在大规模停电数据集上研究了电力服务的公平性问题，发现低收入和老年人口区域经常遭受较长的停电时间，揭示了电力系统中的偏见存在并强调了改善的需求。 |
| [^95] | [Molecule Design by Latent Prompt Transformer.](http://arxiv.org/abs/2310.03253) | 本文提出了一种潜在提示Transformer模型，用于解决分子设计中的优化问题。该模型包括潜在向量、分子生成模型和性质预测模型，通过对现有分子进行训练后进行模型分布的逐渐转移。 |
| [^96] | [Sparse Deep Learning for Time Series Data: Theory and Applications.](http://arxiv.org/abs/2310.03243) | 本文研究了稀疏深度学习在依赖数据（如时间序列数据）上的理论和应用。通过研究，我们发现稀疏循环神经网络能够一致地估计，并对其预测进行正确的不确定性量化。数值实验结果显示，稀疏深度学习在预测不确定性方面优于最先进方法。 |
| [^97] | [Relational Convolutional Networks: A framework for learning representations of hierarchical relations.](http://arxiv.org/abs/2310.03240) | 关系卷积网络是一个学习显式层次关系表示的框架，通过使用多维内积关系模块和关系卷积层，以及基于图元滤波器的群组比较，能够表达更高阶、层次的关系。 |
| [^98] | [Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization.](http://arxiv.org/abs/2310.03234) | 本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。 |
| [^99] | [History Matching for Geological Carbon Storage using Data-Space Inversion with Spatio-Temporal Data Parameterization.](http://arxiv.org/abs/2310.03228) | 本研究开发了一种基于深度学习的参数化方法，在数据空间反演中实现了对地质碳储存历史匹配的准确推断，可以在工业规模碳储存操作中提高地下水管理的效果。 |
| [^100] | [Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms.](http://arxiv.org/abs/2310.03225) | 本文提出了一种广义的安全探索问题，即GSE，并提出了一个元算法MASE来解决这个问题。MASE将无约束的强化学习算法与不确定性量化器结合，以保证安全性，并在违反安全约束之前惩罚不安全的探索。这种方法的优势是在保证安全性的同时进行策略优化，并具有高概率的安全保证。 |
| [^101] | [TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design.](http://arxiv.org/abs/2310.03223) | 该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。 |
| [^102] | [Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs.](http://arxiv.org/abs/2310.03221) | Know2BIO是一个全面的双视图演变生物医学知识图谱基准，通过整合多样化数据和多模态数据，克服了KG的实体对齐、扩展性和更新的挑战。 |
| [^103] | [Learning Energy-Based Prior Model with Diffusion-Amortized MCMC.](http://arxiv.org/abs/2310.03218) | 本文介绍了一种基于扩散改进的长期 MCMC采样的学习算法，用于学习能量先验模型。实验证明了该算法的有效性。 |
| [^104] | [Formal and Practical Elements for the Certification of Machine Learning Systems.](http://arxiv.org/abs/2310.03217) | 本文研究了机器学习系统认证的形式要素和实际要素，通过揭示构建机器学习模型的内部工作原理和过程，形式建立理论保证，并结合实际考虑，开发了一个完整的安全关键机器学习系统认证论证。 |
| [^105] | [PDR-CapsNet: an Energy-Efficient Parallel Approach to Dynamic Routing in Capsule Networks.](http://arxiv.org/abs/2310.03212) | PDR-CapsNet是一种更深、更节能的胶囊网络替代方案，通过并行化策略减轻了计算复杂性并提高了吞吐量，实现了更优越的性能和更少的能量消耗。 |
| [^106] | [Regret Analysis of Distributed Online Control for LTI Systems with Adversarial Disturbances.](http://arxiv.org/abs/2310.03206) | 本文研究了在线分布式控制问题，其中涉及到一个具有对抗性扰动的线性时不变（LTI）系统网络。对于已知动态的情况，提出了一个完全分布式的扰动反馈控制器，保证遗憾界为$O(\sqrt{T}\log T)$。对于未知动态的情况，设计了一个分布式的探索-承诺方法，使所有代理在探索阶段共同学习系统动态，并在学习阶段使用提出的控制算法进行控制。 |
| [^107] | [Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions.](http://arxiv.org/abs/2310.03195) | 本文对基于深度强化学习(DRL)的机器调度方法进行了全面回顾和比较，总结出它们的方法论、应用、优势和局限性。通过对比分析，发现DRL方法优于传统方法。 |
| [^108] | [Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models.](http://arxiv.org/abs/2310.03182) | 本文提出了一种新的方法来构建鲁棒和可解释的医学图像分类器，通过从GPT-4中查询临床概念，并利用视觉-语言模型将潜在的图像特征转化为明确的概念，以解决在真实世界医疗应用中的两个挑战：学习不想关的相关性和缺乏解释性。 |
| [^109] | [Digital Ethics in Federated Learning.](http://arxiv.org/abs/2310.03178) | 本文探讨了在联邦学习中作为客户端的以人为中心的设备引发的数字伦理问题。瞭解了其中涉及的游戏动态、公平性、奖励机制和连贯性等挑战，并探讨了解决方案和以人为中心的物联网在联邦学习中的机遇。 |
| [^110] | [Test Case Recommendations with Distributed Representation of Code Syntactic Features.](http://arxiv.org/abs/2310.03174) | 该论文提出了一种使用代码句法特征的分布式表示来推荐测试用例的方法，通过训练神经网络并计算余弦相似度，提高了自动生成和维护测试单元的效率和有效性。 |
| [^111] | [Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors.](http://arxiv.org/abs/2310.03166) | 本文提出了一种新颖的查询高效对抗HTML攻击方法，通过细粒度篡改来修改钓鱼网页的HTML代码，同时保持其恶意性和视觉外观不变。实验表明，这种方法能够将目前最先进的机器学习防钓鱼网页检测器的性能摧毁，并且只需要30个查询。 |
| [^112] | [Enhancing Accuracy in Deep Learning Using Random Matrix Theory.](http://arxiv.org/abs/2310.03165) | 本研究探索了随机矩阵理论在深度神经网络训练中的应用，通过层剪枝和损失曲面优化，实现了对DNN架构的简化和准确性的增强。通过奇异值分解，并根据随机矩阵理论的标准丢弃小的奇异值，可减少DNN层的参数，简化DNN架构，同时保持或增强模型的准确性。 |
| [^113] | [FedNAR: Federated Optimization with Normalized Annealing Regularization.](http://arxiv.org/abs/2310.03163) | 本文提出了一种名为FedNAR的算法插件，它通过归一化退火正则化来调节每次更新的大小，从而解决了联邦学习中权重衰减对全局目标的优化目标的不同影响。 |
| [^114] | [Neural architecture impact on identifying temporally extended Reinforcement Learning tasks.](http://arxiv.org/abs/2310.03161) | 本研究提出了基于注意力的神经架构，在强化学习领域取得了良好的性能。通过将注意力图叠加到图像上，可以直接观察到代理所使用的信息，并更容易解释选择动作背后的逻辑。 |
| [^115] | [Assessment of Prediction Intervals Using Uncertainty Characteristics Curves.](http://arxiv.org/abs/2310.03158) | 本论文提出了一种新颖的评估预测区间的方法，利用了操作特征曲线和相对于空白参考的收益概念。这种方法广泛适用于不同的研究场景，解决了当前对预测区间全面评估的需求。 |
| [^116] | [FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent.](http://arxiv.org/abs/2310.03156) | FedHyper是一种适用于联邦学习的通用和稳健学习率调度器，通过超梯度下降算法实现对全局和局部学习率的自适应调整，能够显著提高联邦学习系统的效果，同时减少了对经验调整的需求。 |
| [^117] | [Towards out-of-distribution generalizable predictions of chemical kinetics properties.](http://arxiv.org/abs/2310.03152) | 本论文目标是实现化学动力学性质的基于机器学习的超出分布预测，通过研究OOD动力学性质预测的三个层次，揭示问题的独特方面，并创建全面的数据集评估最先进的反应预测方法和动力学性质预测方法。 |
| [^118] | [Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly.](http://arxiv.org/abs/2310.03150) | 本论文以硬件为中心，探讨了如何将LLMs引入现代边缘计算系统。通过联邦学习（FL）对FLAN-T5模型进行微调，并对其在文本摘要任务上的性能进行了评估。同时提供了硬件基准测试和与数据中心GPU的比较。 |
| [^119] | [Attributing Learned Concepts in Neural Networks to Training Data.](http://arxiv.org/abs/2310.03149) | 通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。 |
| [^120] | [Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations.](http://arxiv.org/abs/2310.03148) | 本文提出了一种多任务学习技术和自适应上采样方法，用于减少多领域推荐系统中的流行度偏差。通过实验证明，该框架在多个领域中相对增益高达65.27%。 |
| [^121] | [Context-Based Tweet Engagement Prediction.](http://arxiv.org/abs/2310.03147) | 该论文研究了基于上下文的推文参与度预测问题，使用了Twitter的数据集和评估流程，探讨了仅凭上下文是否可以很好地预测推文的参与度可能性。 |
| [^122] | [Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data.](http://arxiv.org/abs/2310.03146) | 这个论文提出了一种增强公平性的混合效应深度学习（MEDL）框架，通过同时解决数据集簇间关联和不公平性的问题，来提高对簇分布数据的公平性和泛化能力。 |
| [^123] | [Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models.](http://arxiv.org/abs/2310.03123) | 这项研究提出了一种高效的黑盒大型预训练模型联合提示调整（Fed-BBPT）方法，通过摒弃对参数结构和私有数据集访问的依赖，可以在处理内存限制和保持隐私性的同时充分利用每个局部数据集。 |
| [^124] | [OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials.](http://arxiv.org/abs/2310.03121) | OpenMM 8是一个支持使用机器学习势函数的分子动力学模拟工具包，通过引入新功能、优化计算速度和提供高级接口，使得使用机器学习来提高模拟精度成为一种实际可行的方法。 |
| [^125] | [Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset.](http://arxiv.org/abs/2310.03119) | 这项研究探讨了跨物联网设备的可移植的电磁侧信道分析的挑战，并提出了数据集来解决设备变化性、环境因素和数据处理方法对EM-SCA结果准确性的限制。 |
| [^126] | [Leveraging Model-based Trees as Interpretable Surrogate Models for Model Distillation.](http://arxiv.org/abs/2310.03112) | 这项研究利用基于模型的树作为可解释的替代模型，通过决策规则将特征空间划分为可解释的区域，并使用基于可加性主效应的可解释模型来近似黑盒子模型的行为，以在可解释性和性能之间达到最佳平衡。 |
| [^127] | [Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data.](http://arxiv.org/abs/2310.03111) | 该论文介绍了一种多模态高斯过程变分自编码器（GP-VAEs）的方法，用于描述神经和行为数据之间的关系。该方法通过将高斯过程因子分析（GPFA）和深度神经网络相结合，能够提取不同实验模态的共享和独立潜变量，并且具有解释能力。 |
| [^128] | [Creating an Atlas of Normal Tissue for Pruning WSI Patching Through Anomaly Detection.](http://arxiv.org/abs/2310.03106) | 本研究提出了使用正常组织样本构建“正常组织图谱”的概念，并展示了如何通过这种图谱来提高WSI的代表性。通过在107个正常皮肤WSI上建立正常图谱，并使用553个表皮鳞状细胞癌（cSCC）的WSI进行验证，我们证明了该方法的有效性。 |
| [^129] | [DP-SGD for non-decomposable objective functions.](http://arxiv.org/abs/2310.03104) | 本论文提出了一种针对非可分的目标函数的DP-SGD方法，解决了使用差分隐私进行训练时，相似性损失函数的$L_2$敏感度增长随着批量大小增加的问题。 |
| [^130] | [Dual Prompt Tuning for Domain-Aware Federated Learning.](http://arxiv.org/abs/2310.03103) | 本文提出了一种面向领域感知的联邦学习方法，通过双提示调优实现领域适应。实验结果表明，该方法在联邦学习中具有显著的效果。 |
| [^131] | [Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning.](http://arxiv.org/abs/2310.03094) | 本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。 |
| [^132] | [Physics-Informed Neural Networks for Accelerating Power System State Estimation.](http://arxiv.org/abs/2310.03088) | 本研究提出了一种利用物理信息神经网络加速电力系统状态估计的方法，通过整合内在的物理知识，显著降低了计算复杂度并保持高精度，实验证明其准确性提升了11％，结果的标准差减小了75％，收敛速度加快了30％。 |
| [^133] | [Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook.](http://arxiv.org/abs/2310.03086) | 深度学习在计算生物学中的应用带来了重大变革，既能够改善DNA序列分析和蛋白质结构预测，也为基因组变异检测和基因表达分析提供了新的方法。 |
| [^134] | [Batch-less stochastic gradient descent for compressive learning of deep regularization for image denoising.](http://arxiv.org/abs/2310.03085) | 本文介绍了一种无批量随机梯度下降方法，用于图像去噪的深度正则化压缩学习。该方法通过利用来自干净信号或图像数据库的先验信息，将正则化器与数据分布关联起来，从大规模训练数据库中恢复复杂的分布，以减少计算负担。 |
| [^135] | [Discovering Knowledge-Critical Subnetworks in Pretrained Language Models.](http://arxiv.org/abs/2310.03084) | 本研究调查了预训练语言模型中是否存在各种关键知识子网络，即负责编码特定知识的稀疏计算子图。通过提出的可微分权重屏蔽方案，我们可以精确地删除特定知识，又最小化对原始语言模型的负面影响。 |
| [^136] | [Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models.](http://arxiv.org/abs/2310.03059) | Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。 |
| [^137] | [Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems.](http://arxiv.org/abs/2310.03055) | 本文介绍了一种修改后的LAB算法，并提出了基于聚类的搜索空间缩减方法，该算法在工程设计问题求解中表现出改进的稳健性和搜索空间探索能力，同时能够解决约束问题。 |
| [^138] | [Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel.](http://arxiv.org/abs/2310.03054) | 本文提出了一种基于负距离核的最大平均距离(MMD)的条件流方法，用于后验抽样和条件生成建模。通过离散的Wasserstein梯度流近似联合分布，证明了粒子流是适当功能的Wasserstein梯度流。在条件图像生成和超分辨率等逆问题中展示了方法的有效性。 |
| [^139] | [Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing.](http://arxiv.org/abs/2310.03052) | Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。 |
| [^140] | [QuATON: Quantization Aware Training of Optical Neurons.](http://arxiv.org/abs/2310.03049) | 提出一种光纤神经元的量化感知训练方法，通过考虑物理约束，实现了对光纤神经架构的鲁棒设计。 |
| [^141] | [Differentiable Chemical Physics by Geometric Deep Learning for Gradient-based Property Optimization of Mixtures.](http://arxiv.org/abs/2310.03047) | 本文开发了一个不同iable的化学物理框架DiffMix，利用几何深度学习将分子物种、组成和环境条件映射到混合物物理定律中的物理系数上。通过创建可学习的物理系数，我们扩展了混合物热力学和输运定律，并展示了DiffMix相比纯数据驱动变量改进的预测准确性和模型鲁棒性。 |
| [^142] | [A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback.](http://arxiv.org/abs/2310.03043) | 本研究提出了一种利用深度强化学习的交互式搜索方法，该方法通过整合句级反馈信息来提高搜索准确性。通过适应最新的BERT-based模型进行关键句子选择和项目排序，可以获得更满意的搜索结果。 |
| [^143] | [Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition.](http://arxiv.org/abs/2310.03033) | 这项研究基于二进制神经网络模型，通过引入一组基准问题来评估交通标志识别模型的本地稳健性。这些问题考虑了网络参数的数量、输入维度和区域数量，并挑战了最先进的验证工具。 |
| [^144] | [Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution.](http://arxiv.org/abs/2310.03032) | 本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。 |
| [^145] | [How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses.](http://arxiv.org/abs/2310.03031) | 本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。 |
| [^146] | [GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction.](http://arxiv.org/abs/2310.03030) | GPT-MolBERTa是一种用于分子性质预测的自监督大型语言模型，通过使用分子的详细文本描述来学习分子的表示，实验表明其在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。 |
| [^147] | [SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and Improving Prediction Rates in Drug Discovery.](http://arxiv.org/abs/2310.03028) | SAF框架提出了一种新的聚合方法，使用温度超参数对原子加权，并证明其可以提高信息传递神经网络的预测能力，在药物发现中揭示重要的原子。 |
| [^148] | [Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction.](http://arxiv.org/abs/2310.03027) | 本论文提出了一种名为SYNFUSION的新方法，通过协同融合GNN和Transformer的预训练特征，实现了全面的分子表示，在分子性质预测任务中超越了以往的模型。 |
| [^149] | [IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning.](http://arxiv.org/abs/2310.02995) | IBCL提出了一种用于连续学习中任务权衡的零样本模型生成方法，通过更新知识库并利用模型参数分布的凸包形式，实现不同任务性能之间的权衡偏好。 |
| [^150] | [Co-modeling the Sequential and Graphical Route for Peptide.](http://arxiv.org/abs/2310.02964) | 本论文提出了一种肽合模式化方法，使用对比学习框架来增强从顺序和图形模型中学到的表示的相互信息，以提高肽的判别性能。 |
| [^151] | [Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection.](http://arxiv.org/abs/2310.02861) | 《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。 |
| [^152] | [PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting.](http://arxiv.org/abs/2310.02676) | PostRainBench是一个全面的降水预测基准，结合AI后处理技术和传统的数值天气预报方法，能够增强准确性并解决复杂的降水预测挑战。 |
| [^153] | [Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods.](http://arxiv.org/abs/2310.02671) | 本论文研究了随机Softmax策略梯度方法的收敛性分析，提出了一种动态策略梯度的组合方法，并通过对参数进行反向训练来更好地利用相关性结构，实现向全局最优值的收敛。 |
| [^154] | [On the definition of toxicity in NLP.](http://arxiv.org/abs/2310.02357) | 这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。 |
| [^155] | [Probabilistically Rewired Message-Passing Neural Networks.](http://arxiv.org/abs/2310.02156) | PR-MPNNs通过概率重连学习加入相关边，并省略对预测任务没有帮助的边，从而增强了表达能力。 |
| [^156] | [DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks.](http://arxiv.org/abs/2310.02027) | DeepHGCN是一个具有深层架构的双曲图卷积网络，通过引入新的双曲特征转换层和正则化技术，实现了计算效率的极大改进和过度平滑问题的显著减轻。 |
| [^157] | [Forecasting Tropical Cyclones with Cascaded Diffusion Models.](http://arxiv.org/abs/2310.01690) | 本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。 |
| [^158] | [Borges and AI.](http://arxiv.org/abs/2310.01425) | 这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。 |
| [^159] | [An Empirical Study of AI Generated Text Detection Tools.](http://arxiv.org/abs/2310.01423) | 本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。 |
| [^160] | [Towards Robust 3D Object Detection In Rainy Conditions.](http://arxiv.org/abs/2310.00944) | 本研究提出了一个框架，旨在改善基于激光雷达的三维物体检测器在道路喷雾等恶劣天气条件下的鲁棒性。该方法使用恶劣天气检测网络过滤激光雷达点云中的喷雾，并进一步使用雷达目标过滤虚假检测，从而提高环境感知的准确性。 |
| [^161] | [ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal.](http://arxiv.org/abs/2310.00818) | 本文提出了一种名为ECG-SL的深度学习方法，用于处理心电图(ECG)信号。该方法通过划分心跳片段并提取结构特征，显式地建模了ECG信号的周期性。此外，还采用自监督学习策略进行预训练，取得了显著的性能提升。 |
| [^162] | [Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning.](http://arxiv.org/abs/2310.00247) | 提出了一种自适应的Resource-aware Federated Foundation Models (RaFFM)框架，通过引入专门的模型压缩算法来解决将基础模型集成到联邦学习中的挑战，实现基于Transformer的基础模型在网络边缘根据异构资源约束进行动态缩放。 |
| [^163] | [LoRA ensembles for large language model fine-tuning.](http://arxiv.org/abs/2310.00035) | 本文提出了一种使用低秩适配器（LoRA）的集成方法，用于解决大型语言模型微调中存在的不确定性量化问题，并提供了一个参数高效的微调技术。这种方法可以构建大规模的LoRA适配器集成，并具有与基础预训练模型相近的计算资源需求。 |
| [^164] | [Module-wise Training of Neural Networks via the Minimizing Movement Scheme.](http://arxiv.org/abs/2309.17357) | 通过引入模块化正则化方法，解决了神经网络模块化训练中早期层过拟合和深层停滞的问题，实验结果展示了该方法在不同架构上的优越性。 |
| [^165] | [Efficient Biologically Plausible Adversarial Training.](http://arxiv.org/abs/2309.17348) | 本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。 |
| [^166] | [Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks.](http://arxiv.org/abs/2309.17329) | 本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。 |
| [^167] | [PlaceNav: Topological Navigation through Place Recognition.](http://arxiv.org/abs/2309.17260) | PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。 |
| [^168] | [DyVal: Graph-informed Dynamic Evaluation of Large Language Models.](http://arxiv.org/abs/2309.17167) | DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。 |
| [^169] | [Neural Operators for Accelerating Scientific Simulations and Design.](http://arxiv.org/abs/2309.15325) | 本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。 |
| [^170] | [Multiple Physics-Informed Neural Network for Biomedical Tube Flows.](http://arxiv.org/abs/2309.15294) | 本研究探索了多情况PINN方法在计算生物医学管道流动中的应用，通过预训练和参数化不同几何情况，可以实时获取未见几何形状的结果，进而优化了传统CFD方法的使用。 |
| [^171] | [LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference.](http://arxiv.org/abs/2309.14331) | LinGCN是一个旨在减少乘法深度和优化HE基于GCN推断性能的框架，通过结构化线性化算法和参数化的离散指示函数的联合训练，实现细粒度的节点级非线性位置选择。 |
| [^172] | [Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach.](http://arxiv.org/abs/2309.14073) | 本研究提出了一种新的图形结构，用于在线性和高斯性假设下稳定的潜变量结构方程模型。我们证明了计算该模型的最大似然估计等价于训练一个神经网络，并实现了一个基于GPU的算法来进行计算。 |
| [^173] | [Diffeomorphic Multi-Resolution Deep Learning Registration for Applications in Breast MRI.](http://arxiv.org/abs/2309.13777) | 本论文提出了适用于乳房MR图像登记的学习策略，其中包含了一个登记网络，能够产生更优的登记结果，并提供微分同胚保证。 |
| [^174] | [AnglE-Optimized Text Embeddings.](http://arxiv.org/abs/2309.12871) | 本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。 |
| [^175] | [Sharpness-Aware Minimization and the Edge of Stability.](http://arxiv.org/abs/2309.12488) | 本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。 |
| [^176] | [PIE: Simulating Disease Progression via Progressive Image Editing.](http://arxiv.org/abs/2309.11745) | PIE是一个新的渐进图像编辑框架，可以通过控制性地操纵图像特征来准确模拟个体患者的疾病进展。该方法利用文本到图像生成模型，实现了个性化的疾病进展模拟，并在验证实验中展现了优越性。 |
| [^177] | [Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules.](http://arxiv.org/abs/2309.09476) | 本论文研究了将强化学习应用于游戏规则生成的人类游戏评估，并通过实验结果表明，强化学习生成的规则与传统基线方法有所不同，可能更适合人类使用。 |
| [^178] | [Reconstructing Existing Levels through Level Inpainting.](http://arxiv.org/abs/2309.09472) | 本论文介绍了内容增强和级别修复的方法，通过两种技术（自编码器和U-net）来重建和扩展视频游戏级别。经过综合案例研究，我们展示了这两种方法相对于基准方法的卓越性能。提供了未来研究方向的见解。 |
| [^179] | [Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems.](http://arxiv.org/abs/2309.07936) | Landscape-Sketch-Step是一种基于AI/ML的元启发式方法，结合了机器学习、随机优化和强化学习技术，用于解决成本函数评估昂贵、不可访问或禁止的代理优化问题。 |
| [^180] | [Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments.](http://arxiv.org/abs/2309.07056) | 本文使用了一种名为“梦境”的可解释人工智能技术，探索神经网络对量子光学实验的学习，发现网络可以改变量子系统的属性分布，并揭示了神经网络的学习策略。 |
| [^181] | [Practical Homomorphic Aggregation for Byzantine ML.](http://arxiv.org/abs/2309.05395) | 本文介绍了一种适用于拜占庭式机器学习的实用同态聚合方法，以应对拜占庭节点和服务器隐私侵犯的问题。 |
| [^182] | [Marginalized Importance Sampling for Off-Environment Policy Evaluation.](http://arxiv.org/abs/2309.01807) | 本文提出了一种新的方法，利用边际化重要性采样框架，在使用模拟器和真实世界的离线数据评估代理策略性能时，解决了大密度比率和间接监督的挑战。 |
| [^183] | [On the Implicit Bias of Adam.](http://arxiv.org/abs/2309.00079) | 本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。 |
| [^184] | [US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images.](http://arxiv.org/abs/2308.16738) | 本论文提出了一种基于空间频域的多分支网络US-SFNet，用于超声图像中颈部淋巴结病变的诊断。通过使用Conv-FFT块来建模图像，实现更准确的诊断结果。 |
| [^185] | [Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI.](http://arxiv.org/abs/2308.16150) | 本文介绍了一种名为遮蔽模态循环与条件扩散的方法，该方法能够对多模态MRI中的异常进行分割。方法基于循环模态转换和条件扩散的思想，能够检测到训练中未遇到的异常模式。 |
| [^186] | [BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection.](http://arxiv.org/abs/2308.12439) | BaDExpert是一种防御后门攻击的新方法，通过逆向工程提取给定后门模型的后门功能，并生成一个专家模型，该模型只能识别后门输入。可以进一步使用该模型设计高度准确的后门输入检测器。 |
| [^187] | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.](http://arxiv.org/abs/2308.00436) | 本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。 |
| [^188] | [Formally Explaining Neural Networks within Reactive Systems.](http://arxiv.org/abs/2308.00143) | 这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。 |
| [^189] | [Towards practical reinforcement learning for tokamak magnetic control.](http://arxiv.org/abs/2307.11546) | 这项研究致力于解决强化学习方法在托卡马克磁控制中的关键缺点，通过改进算法和训练过程，实现了更高的控制精度、减小稳态误差和缩短学习新任务所需时间，并通过实验验证了模拟结果的有效性。 |
| [^190] | [Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection.](http://arxiv.org/abs/2307.07726) | 本文通过揭示神经网络模型构建中的样本拆分方法的奥秘，构建了一个理论框架来解释神经网络的有效性。我们的研究结果表明，从样本拆分中得到的最优超参数可以使得神经网络模型最小化预测风险。 |
| [^191] | [Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation.](http://arxiv.org/abs/2307.06125) | 这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。 |
| [^192] | [Quantitative CLTs in Deep Neural Networks.](http://arxiv.org/abs/2307.06092) | 本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。 |
| [^193] | [One-Versus-Others Attention: Scalable Multimodal Integration.](http://arxiv.org/abs/2307.05435) | 提出了一种可扩展的多模态集成方法，通过一对多（OvO）注意力机制解决了多模态学习中超过三个模态的注意力计算问题。 |
| [^194] | [Enhancing Adversarial Robustness via Score-Based Optimization.](http://arxiv.org/abs/2307.04333) | 本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。 |
| [^195] | [DISCO-10M: A Large-Scale Music Dataset.](http://arxiv.org/abs/2306.13512) | DISCO-10M是一个新颖而广泛的音乐数据集，其规模超过了之前最大的可用音乐数据集一个数量级，提供高质量的音频资源和CLAP嵌入以加速机器学习在音乐领域的研究和应用。 |
| [^196] | [Modularizing while Training: a New Paradigm for Modularizing DNN Models.](http://arxiv.org/abs/2306.09376) | 本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。 |
| [^197] | [PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs.](http://arxiv.org/abs/2306.08827) | PINNacle是一个全面的物理约束神经网络(PINNs)基准测试工具，提供了一个多样化的数据集和约10种最先进的方法，用于解决真实世界问题中的各种偏微分方程(PDEs)挑战。 |
| [^198] | [FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts.](http://arxiv.org/abs/2306.08586) | 本论文提出了一种名为FedJETs的方法，使用联邦混合专家的框架，在联邦学习中实现高效及时的个性化。该方法通过训练专门的专家，并利用门控函数将输入路由到相关的专家，有效提高了模型的准确性。 |
| [^199] | [SqueezeLLM: Dense-and-Sparse Quantization.](http://arxiv.org/abs/2306.07629) | 本文提出了一种基于训练后的量化框架——SqueezeLLM，它不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。 |
| [^200] | [Learning Representations on the Unit Sphere: Application to Online Continual Learning.](http://arxiv.org/abs/2306.03364) | 该论文提出了一种基于单位球的表示学习方法，通过将表示推向固定方向，使得学习策略对数据漂移具有弹性，从而能够应对在线连续学习的挑战性问题。 |
| [^201] | [Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds.](http://arxiv.org/abs/2306.03116) | 本文提出了一个高效的方法来估算特定注释者和特定实例的转移矩阵以及真实标签比例，解决了从众包中学习的标签噪声问题，并在实验中证明了方法的优越性。 |
| [^202] | [Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance.](http://arxiv.org/abs/2305.20057) | 本文研究了多目标学习中的三重权衡问题，通过研究动态加权算法在MoDo算法中的泛化性能和与优化的相互作用，发现了动态加权方法的局限性。 |
| [^203] | [Demystifying Oversmoothing in Attention-Based Graph Neural Networks.](http://arxiv.org/abs/2305.16102) | 本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。 |
| [^204] | [Learning Robust Statistics for Simulation-based Inference under Model Misspecification.](http://arxiv.org/abs/2305.15871) | 本研究提出首个通用的方法来处理基于模拟的推论（如ABC和NPE）中由于模型错误引起的不可靠推论。通过约束统计量的选择，我们的方法通过惩罚与数据和模型之间不匹配的统计量来防止不可靠推论结果。我们在高维时间序列模型上进行了实验，证明了本方法的优越性能。 |
| [^205] | [Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution.](http://arxiv.org/abs/2305.15357) | 本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。 |
| [^206] | [Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge.](http://arxiv.org/abs/2305.15086) | 本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。 |
| [^207] | [Physics of Language Models: Part 1, Context-Free Grammar.](http://arxiv.org/abs/2305.13673) | 本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。 |
| [^208] | [Explaining Emergent In-Context Learning as Kernel Regression.](http://arxiv.org/abs/2305.12766) | 本文研究了为什么在预训练之后，基于Transformer的语言模型能够实现上下文学习，并提出了一种假设，认为LLMs在面对上下文示例时能够通过内部表示模拟核回归。 |
| [^209] | [AnyPredict: Foundation Model for Tabular Prediction.](http://arxiv.org/abs/2305.12081) | 本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。 |
| [^210] | [Network Cascade Vulnerability using Constrained Bayesian Optimization.](http://arxiv.org/abs/2304.14420) | 本研究基于约束贝叶斯优化，以修改输电线路保护设置为敌对攻击的候选方案，探讨了最大化级联网络退化的保护设置规律，发现将所有电网线路的保护设置最大失配并不会导致最多的级联。 |
| [^211] | [Knowledge Distillation Under Ideal Joint Classifier Assumption.](http://arxiv.org/abs/2304.11004) | 本文提出了基于理想联合分类器假设的知识蒸馏框架，可以提供清晰全面的理解和为未来研究提供理论基础，使得教师和学生网络之间的知识传递更加高效。 |
| [^212] | [In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT.](http://arxiv.org/abs/2304.08979) | 本文首次对ChatGPT在通用问答场景中的可靠性进行了大规模测量，发现其在不同领域的可靠性有所差异，尤其在法律和科学问题方面表现不佳，并容易受到对抗性示例的影响，对其在实际应用中的使用产生影响。 |
| [^213] | [Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance.](http://arxiv.org/abs/2304.06715) | 本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。 |
| [^214] | [Algebraic and Geometric Models for Space Networking.](http://arxiv.org/abs/2304.01150) | 本文提出了一种新的代数和几何模型来研究网络空间通信。通过定义时间变化图（TVG），以及利用矩阵和半环性质进行建模，我们能够生成TVG的通信容量的新统计量并分析STARLINK卫星之间的连接性。此外，还引入了基于拓扑数据分析的度量标准来进一步分析STARLINK的强连接性。最后，我们还介绍了一些能够模拟地球和火星网络场景的半环模型。 |
| [^215] | [Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning.](http://arxiv.org/abs/2304.00195) | 该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。 |
| [^216] | [Towards Understanding the Effect of Pretraining Label Granularity.](http://arxiv.org/abs/2303.16887) | 本文探究了预训练标签粒度对深度神经网络图像分类任务的泛化能力的影响，并在iNaturalist 2021与ImageNet数据集中进行了实验证明，在预训练数据集具有强有力的标签层次结构，标签功能与目标任务对齐，以及选择适当的预训练标签粒度时，能有效提高模型的性能。 |
| [^217] | [GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation.](http://arxiv.org/abs/2303.14655) | GOAL是一个基于知识的视频字幕生成基准，旨在解决如何基于背景知识生成精细的视频描述的问题，该基准包含超过8.9k个足球视频和相关的知识三元组，并提供了解决这一任务的难度和潜在方向的实验结果。 |
| [^218] | [Logic of Differentiable Logics: Towards a Uniform Semantics of DL.](http://arxiv.org/abs/2303.10650) | 该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。 |
| [^219] | [Disentangling the Link Between Image Statistics and Human Perception.](http://arxiv.org/abs/2303.09874) | 本研究直接评估自然图像的概率，并分析它如何影响人类感知。通过展示具有更丰富统计特征的自然图像被感知为具有更大的显着性，论文提供了直接支持Barlow和Attneave理论的证据，并建立了一个新的框架，用于理解图像统计与知觉之间的关系。 |
| [^220] | [Deep Momentum Multi-Marginal Schr\"odinger Bridge.](http://arxiv.org/abs/2303.01751) | 该论文提出了一种新的计算框架DMSB，它可以学习满足时间上位置边际约束的随机系统的平滑度量值样条，用于解决高维多边际轨迹推断任务，并在实验中表现出显著的性能优势。同时，该框架还为解决具有各种类型的边际约束的随机轨迹重建任务提供了一个通用框架。 |
| [^221] | [Towards Inferential Reproducibility of Machine Learning Research.](http://arxiv.org/abs/2302.04054) | 本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。 |
| [^222] | [Private GANs, Revisited.](http://arxiv.org/abs/2302.02936) | 在训练差分隐私GANs时，通过对鉴别器进行一些修改和优化，如增加鉴别器的训练步骤和使用较大的批处理大小等，可以显著提高GAN的训练结果和生成质量。 |
| [^223] | [Generative models for two-ground-truth partitions in networks.](http://arxiv.org/abs/2302.02787) | 本研究提出了一种生成模型，称为随机交叉块模型（SCBM），可以在网络的中尺度结构中构建两个不同的划分。通过评估随机块模型的性能，展示了该模型的使用案例。 |
| [^224] | [Efficient Graph Field Integrators Meet Point Clouds.](http://arxiv.org/abs/2302.00942) | 本文提出了两种算法用于非欧几里得空间中的高效图场积分，适用于点云网格图或ε-最近邻图的表征方法，具有很强的实用性。 |
| [^225] | [Large-scale investigation of weakly-supervised deep learning for the fine-grained semantic indexing of biomedical literature.](http://arxiv.org/abs/2301.09350) | 本研究提出了一种新方法，通过弱监督和基于字典的启发式方法，在生物医学文献中对MeSH概念进行自动细化的主题注释。实验结果显示，该方法在大规模场景下取得了较高的性能。 |
| [^226] | [A Comprehensive Survey of Dataset Distillation.](http://arxiv.org/abs/2301.05603) | 数据集压缩是处理海量数据的一种方法，通过综合一个小型典型数据集来提高数据处理效率。这一方法在压缩数据集方面取得了惊人的性能，但仍存在一些限制。 |
| [^227] | [Losses over Labels: Weakly Supervised Learning via Direct Loss Construction.](http://arxiv.org/abs/2212.06921) | 本文提出了一种弱监督学习的新方法，通过直接将启发式方法转化为损失函数，来训练模型，并利用启发式方法的决策信息进行特征选择。 |
| [^228] | [Regression with Label Differential Privacy.](http://arxiv.org/abs/2212.06074) | 本论文研究了在保证标签差分隐私的情况下训练回归模型的任务，并提出了一种最优的标签差分隐私随机化机制，该机制采用了“对箱进行随机响应”的形式，并提供了一种高效算法来找到最优的箱值。 |
| [^229] | [Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases.](http://arxiv.org/abs/2212.02648) | 这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。 |
| [^230] | [Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation.](http://arxiv.org/abs/2211.01856) | 提出了一种使用条件生成模型插值的架构，可以在动态模拟中降低建模时间并保持高的生成精度。 |
| [^231] | [Two-stage LLM Fine-tuning with Less Specialization and More Generalization.](http://arxiv.org/abs/2211.00635) | 预训练的大型语言模型（LLMs）通过精调可以提高特定任务的性能，但精调通常会使模型过度专门化，降低了其在上下文中的泛化学习性能。通过两阶段精调框架ProMoT可以减少这种格式特化。 |
| [^232] | [A Framework for Large Scale Synthetic Graph Dataset Generation.](http://arxiv.org/abs/2210.01944) | 本论文提出了一个可扩展的合成图生成工具，将图数据集扩展到规模为万亿条边和数十亿个节点的级别。通过从专有数据集学习参数模型，可以为研究人员提供用于研究各种图方法的合成数据，加快原型开发和新应用的进展。该框架在模拟结构和特征分布，并能够在不同规模上扩展表现出了良好的泛化能力。 |
| [^233] | [Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection.](http://arxiv.org/abs/2209.12148) | 本论文提出了一种自监督遮蔽卷积变压器块（SSMCTB）用于异常检测。该方法在核心架构层面上集成了重构功能，并且具有灵活的信息遮蔽能力。 |
| [^234] | [Decoding speech perception from non-invasive brain recordings.](http://arxiv.org/abs/2208.12266) | 本研究通过使用对比学习训练的模型，成功从非侵入性脑电记录中解码感知语音。结果显示，该模型能够以高达41%的准确率识别出与脑电信号相对应的语音片段。 |
| [^235] | [Handling Data Heterogeneity in Federated Learning via Knowledge Distillation and Fusion.](http://arxiv.org/abs/2207.11447) | 本文提出了一种名为FedKF的方案，通过知识融合和蒸馏解决了联邦学习中数据异质性导致的客户端模型漂移问题，提升了模型性能和公平性。 |
| [^236] | [Latent Diffusion Energy-Based Model for Interpretable Text Modeling.](http://arxiv.org/abs/2206.05895) | 该论文介绍了一种新颖的深度扩散能量模型，通过在变分学习框架中引入扩散模型和潜在空间EBMs之间的共生关系，解决了潜在空间EBMs在采样质量和训练稳定性方面的问题。 |
| [^237] | [Spatial-temporal associations representation and application for process monitoring using graph convolution neural network.](http://arxiv.org/abs/2205.05250) | 该论文利用图卷积神经网络，基于同一工业过程中众多具有空间-时间相关特征的变量，实现了变量特征建模和表示、图网络构建及图特征感知，并应用于过程监测中。 |
| [^238] | [Self-supervised Deep Unrolled Reconstruction Using Regularization by Denoising.](http://arxiv.org/abs/2205.03519) | 本文提出了一种新颖的MRI图像重建方法，利用了自监督去噪网络和插入式方法，以提高重建性能，并利用正则化去噪来优化MRI重建。 |
| [^239] | [An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization.](http://arxiv.org/abs/2204.05923) | 本文提出了一种改进的随机梯度下降算法，通过自适应调整随机性来找到非凸优化问题的全局最优解，并通过代数收敛性证明了算法的优越性能。 |
| [^240] | [Optimal 1-Wasserstein Distance for WGANs.](http://arxiv.org/abs/2201.02824) | 本文对WGANs的1-Wasserstein距离进行了最优化研究，揭示了样本大小固定时的最优方案与最小化样本点之间平方欧氏距离的和有密切关联，同时发现在样本大小趋向无穷大的情况下，WGANs能够以给定的收敛率无限接近目标分布，前提是生成Lipschitz函数族增长合适。 |
| [^241] | [Characterization of causal ancestral graphs for time series with latent confounders.](http://arxiv.org/abs/2112.08417) | 本文介绍了一种新的图模型类别，用于表示具有未观测到的混淆因素的时间序列的因果关系和独立性。通过使用这些新的图模型，可以得出更强的因果推断，而无需额外的假设。 |
| [^242] | [On Convergence of Federated Averaging Langevin Dynamics.](http://arxiv.org/abs/2112.05120) | 我们提出了一种称为FA-LD的联邦平均Langevin算法，可以用于分布式客户端的不确定性量化和均值预测。算法考虑了非独立同分布数据的强对数凹分布，并研究了注入噪声、随机梯度噪声、数据异质性和变化的学习率等因素对收敛性的影响，为最小化通信开销提供了理论保证。 |
| [^243] | [Linking Across Data Granularity: Fitting Multivariate Hawkes Processes to Partially Interval-Censored Data.](http://arxiv.org/abs/2111.02062) | 这项研究介绍了Partial Mean Behavior Poisson (PMBP)过程，它是一种新的点过程，可以有效地建模时间戳和区间屏蔽数据，并成功恢复了MHP参数和谱半径。 |
| [^244] | [Unsupervised Foreground Extraction via Deep Region Competition.](http://arxiv.org/abs/2110.15497) | 本论文提出了一种名为深度区域竞争（DRC）的算法，用于完全无监督地从图像中提取前景对象。该算法通过结合能量先验和生成式图像建模，利用像素重新分配来捕捉背景区域的规律性，并通过期望最大化的方法找到前景-背景分区。 |
| [^245] | [Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training.](http://arxiv.org/abs/2110.14883) | Colossal-AI是一种用于大规模并行训练的统一深度学习系统，能够以高达2.76倍的速度加快训练过程，并支持多种并行训练方法和零冗余优化器集成的异构训练方法。 |
| [^246] | [Combining Differential Privacy and Byzantine Resilience in Distributed SGD.](http://arxiv.org/abs/2110.03991) | 本文研究了在分布式机器学习中将差分隐私和拜占庭弹性结合起来的问题，并发现现有的方法在差分隐私的约束下无效。 |
| [^247] | [Axiomatic Aggregations of Abductive Explanations.](http://arxiv.org/abs/2109.03890) | 本论文提出了三种聚合方法，将各种可能的推断解释聚合成特征重要性分数，解决了推断解释中多个有效解释的问题。这些方法基于合作博弈理论的权力指数和已知的因果强度度量。 |
| [^248] | [Paying Attention to Astronomical Transients: Introducing the Time-series Transformer for Photometric Classification.](http://arxiv.org/abs/2105.06178) | 本研究介绍了一种基于时间序列变换器的光度分类方法，用于处理未来大规模的天体暂变数据。这种变换器架构支持多变量时间序列数据，并可灵活添加附加特征，同时还提供了可解释性。在多个数据集上的实验证明了其显著性能。 |
| [^249] | [Memory Capacity of Recurrent Neural Networks with Matrix Representation.](http://arxiv.org/abs/2104.07454) | 这个论文研究了具有矩阵表示的循环神经网络的存储容量，通过基于Fisher信息的概率性概念定义和研究，得出了不同假设下的存储上界，并与向量表示的网络进行了比较。 |
| [^250] | [Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms.](http://arxiv.org/abs/2102.00696) | 本论文提出了一种新的深度学习架构，用于预测高分辨率时空天气数据。该架构集成了卷积长短期记忆和卷积神经网络，并引入了注意力和上下文匹配机制。与传统模型相比，该架构在性能上取得了显著改进。 |
| [^251] | [Deep Controlled Learning for Inventory Control.](http://arxiv.org/abs/2011.15122) | 本论文提出了一种名为深度控制学习（DCL）的新型深度强化学习框架，针对库存控制问题进行了量身定制，并通过比较评估表明，在各种测试情况下，DCL相对于传统算法和最先进的启发式算法，在成本和最优性方面都取得了更好的表现。 |
| [^252] | [Learning Graph Laplacian with MCP.](http://arxiv.org/abs/2010.11559) | 本文提出了一种使用非凸惩罚函数MCP来学习图结构的方法，设计了一种算法并证明了其收敛性。实验证明，与现有方法相比，我们的方法在学习图拉普拉斯矩阵与MCP时更高效且可靠。 |
| [^253] | [Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model.](http://arxiv.org/abs/1912.05957) | 这是一种使用深度强化学习模型评估文本可读性的方法，通过使用硬注意力的主动推理技术和半监督信号来提高效率，并与其他先进模型进行比较。 |
| [^254] | [Deep Learning for Genomics: A Concise Overview.](http://arxiv.org/abs/1802.00810) | 深度学习在基因组学领域面临独特的挑战，但通过深入了解任务需求，并与适当的深度架构相匹配，深度学习在基因组学中取得了成功。 |

# 详细

[^1]: MathCoder: 增强数学推理中 LLMs 中无缝代码集成

    MathCoder: Seamless Code Integration in LLMs for Enhanced Mathematical Reasoning. (arXiv:2310.03731v1 [cs.CL])

    [http://arxiv.org/abs/2310.03731](http://arxiv.org/abs/2310.03731)

    本文介绍了一种方法，通过微调开源语言模型，使其能够使用代码进行数学建模和推导，从而增强数学推理能力。作者提出了一种生成包含数学问题和基于代码的解决方案的数据集，并引入了定制的微调和推理方法，从而实现了在解决具有挑战性的数学问题上生成基于代码的解决方案的 MathCoder 模型。

    

    最近发布的 GPT-4 代码解释器展示了在解决具有挑战性的数学问题方面的出色能力，这主要归功于它能够无缝地使用自然语言进行推理，生成代码，执行代码，并根据执行输出继续推理的能力。在本文中，我们提出了一种方法来微调开源的语言模型，使其能够使用代码来建模和推导数学方程，并从而增强其数学推理能力。我们提出了一种生成包含数学问题及其基于代码的解决方案的新颖高质量数据集的方法，称为 MathCodeInstruct。每个解决方案都交错使用自然语言、代码和执行结果。我们还引入了一种定制的监督微调和推理方法。这种方法得到了 MathCoder 模型，这是一系列模型，能够生成基于代码的解决方案来解决具有挑战性的数学问题。令人印象深刻的是，MathCoder 模型实现了最先进的成果。

    The recently released GPT-4 Code Interpreter has demonstrated remarkable proficiency in solving challenging math problems, primarily attributed to its ability to seamlessly reason with natural language, generate code, execute code, and continue reasoning based on the execution output. In this paper, we present a method to fine-tune open-source language models, enabling them to use code for modeling and deriving math equations and, consequently, enhancing their mathematical reasoning abilities. We propose a method of generating novel and high-quality datasets with math problems and their code-based solutions, referred to as MathCodeInstruct. Each solution interleaves natural language, code, and execution results. We also introduce a customized supervised fine-tuning and inference approach. This approach yields the MathCoder models, a family of models capable of generating code-based solutions for solving challenging math problems. Impressively, the MathCoder models achieve state-of-the-
    
[^2]: 具有数据依赖耦合的随机插值。

    Stochastic interpolants with data-dependent couplings. (arXiv:2310.03725v1 [cs.LG])

    [http://arxiv.org/abs/2310.03725](http://arxiv.org/abs/2310.03725)

    本文提出了一种使用数据依赖耦合来构建生成模型的方法，并展示了在超分辨率和修复任务中的实验效果。

    

    受动态测度传输启发的生成模型（如流和扩散）构建了两个概率密度之间的连续时间映射。按照传统方法，其中一个是目标密度，只能通过样本访问，而另一个是简单的基础密度，与数据无关。在这项工作中，我们使用随机插值的框架，规范化了如何“耦合”基本密度和目标密度。这使我们能够将类别标签或连续嵌入的信息纳入到构建动态传输映射的条件生成模型中。我们展示了通过解决类似于标准独立设置的简单平方损失回归问题来学习这些传输映射。通过超分辨率和修复实验，我们证明了构建依赖耦合的有效性。

    Generative models inspired by dynamical transport of measure -- such as flows and diffusions -- construct a continuous-time map between two probability densities. Conventionally, one of these is the target density, only accessible through samples, while the other is taken as a simple base density that is data-agnostic. In this work, using the framework of stochastic interpolants, we formalize how to \textit{couple} the base and the target densities. This enables us to incorporate information about class labels or continuous embeddings to construct dynamical transport maps that serve as conditional generative models. We show that these transport maps can be learned by solving a simple square loss regression problem analogous to the standard independent setting. We demonstrate the usefulness of constructing dependent couplings in practice through experiments in super-resolution and in-painting.
    
[^3]: 未知方差下的高斯均值的任意有效T检验和置信序列

    Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance. (arXiv:2310.03722v1 [math.ST])

    [http://arxiv.org/abs/2310.03722](http://arxiv.org/abs/2310.03722)

    本文提出了两种新的“e-process”和置信序列方法，分别通过替换Lai的混合方法，并分析了所得结果的宽度。

    

    在1976年，Lai构造了一个非平凡的均值$\mu$的高斯分布的置信序列，该分布的方差$\sigma$是未知的。他使用了关于$\sigma$的不适当（右Haar）混合和关于$\mu$的不适当（平坦）混合。在本文中，我们详细说明了他构建的细节，其中使用了广义的不可积分鞅和扩展的维尔不等式。尽管这确实产生了一个顺序T检验，但由于他的鞅不可积分，它并没有产生一个“e-process”。在本文中，我们为相同的设置开发了两个新的“e-process”和置信序列：一个是在缩减滤波器中的测试鞅，另一个是在规范数据滤波器中的“e-process”。这些分别是通过将Lai的平坦混合替换为高斯混合，并将对$\sigma$的右Haar混合替换为在零空间下的最大似然估计，就像在通用推断中一样。我们还分析了所得结果的宽度。

    In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville's inequality. While this does yield a sequential t-test, it does not yield an ``e-process'' (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai's flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting 
    
[^4]: HeaP: 使用LLMs进行层次化Web动作策略

    HeaP: Hierarchical Policies for Web Actions using LLMs. (arXiv:2310.03720v1 [cs.LG])

    [http://arxiv.org/abs/2310.03720](http://arxiv.org/abs/2310.03720)

    这篇论文介绍了一个名为HeaP的框架，利用大型语言模型（LLMs）来解决Web任务的挑战。该框架将Web任务分解为子任务，并通过一系列低级的策略来执行，相比其他基准方法，该框架在不同的Web任务上表现出更好的性能。

    

    大型语言模型（LLMs）在少量数据和零-shot设置中展示了出色的指令跟随任务能力。然而，教授LLMs在Web上执行任务面临着基本挑战 - 组合性大的开放世界任务和Web接口之间的差异。我们通过利用LLMs将Web任务分解为一系列子任务来应对这些挑战，每个子任务可以通过一个低级的闭环策略来解决。这些策略构成了任务之间的共享语法，即新的Web任务可以作为这些策略的组合来表达。我们提出了一种新的框架，即基于LLMs的Hierarchical Policies for Web Actions（HeaP），该框架通过从示范中学习一组层次化的LLM提示来规划高级任务并通过一系列低级策略执行它们。我们通过一套Web任务，包括MiniWoB++，WebArena，模拟航空CRM以及实际网站来评估HeaP与一系列基准方法的性能。

    Large language models (LLMs) have demonstrated remarkable capabilities in performing a range of instruction following tasks in few and zero-shot settings. However, teaching LLMs to perform tasks on the web presents fundamental challenges -- combinatorially large open-world tasks and variations across web interfaces. We tackle these challenges by leveraging LLMs to decompose web tasks into a collection of sub-tasks, each of which can be solved by a low-level, closed-loop policy. These policies constitute a shared grammar across tasks, i.e., new web tasks can be expressed as a composition of these policies. We propose a novel framework, Hierarchical Policies for Web Actions using LLMs (HeaP), that learns a set of hierarchical LLM prompts from demonstrations for planning high-level tasks and executing them via a sequence of low-level policies. We evaluate HeaP against a range of baselines on a suite of web tasks, including MiniWoB++, WebArena, a mock airline CRM, as well as live website i
    
[^5]: 约束条件下的策略优化用于多功能安全强化学习

    Constraint-Conditioned Policy Optimization for Versatile Safe Reinforcement Learning. (arXiv:2310.03718v1 [cs.LG])

    [http://arxiv.org/abs/2310.03718](http://arxiv.org/abs/2310.03718)

    这个论文提出了一种约束条件下的策略优化框架，用于训练多功能安全强化学习智能体。通过引入多功能值估计和有条件的变分推理模块，该框架在训练效率和零-shot适应能力方面表现优于基准方法。

    

    安全强化学习（RL）专注于训练在预定义安全约束条件下能够最大化奖励的智能体。然而，在部署过程中，学习能够适应不同安全约束要求且无需重新训练的多功能安全策略仍然是一个较为未开发和具有挑战性的领域。在这项工作中，我们提出了多功能安全强化学习问题，并考虑了两个主要需求：训练效率和零-shot适应能力。为了解决这些问题，我们引入了Conditioned Constrained Policy Optimization（CCPO）框架，包括两个关键模块：（1）多功能值估计（VVE），用于在未见过的阈值条件下近似值函数，并且（2）有条件的变分推理（CVI），用于在策略优化中编码任意约束阈值。我们的大量实验表明，CCPO在安全和任务性能方面优于基准，并保持了对不同约束的零-shot适应能力。

    Safe reinforcement learning (RL) focuses on training reward-maximizing agents subject to pre-defined safety constraints. Yet, learning versatile safe policies that can adapt to varying safety constraint requirements during deployment without retraining remains a largely unexplored and challenging area. In this work, we formulate the versatile safe RL problem and consider two primary requirements: training efficiency and zero-shot adaptation capability. To address them, we introduce the Conditioned Constrained Policy Optimization (CCPO) framework, consisting of two key modules: (1) Versatile Value Estimation (VVE) for approximating value functions under unseen threshold conditions, and (2) Conditioned Variational Inference (CVI) for encoding arbitrary constraint thresholds during policy optimization. Our extensive experiments demonstrate that CCPO outperforms the baselines in terms of safety and task performance while preserving zero-shot adaptation capabilities to different constraint 
    
[^6]: 一条漫长之路：探究RLHF中的长度相关性

    A Long Way to Go: Investigating Length Correlations in RLHF. (arXiv:2310.03716v1 [cs.CL])

    [http://arxiv.org/abs/2310.03716](http://arxiv.org/abs/2310.03716)

    这篇论文通过研究RLHF中奖励和长度的关系，发现优化响应长度是RLHF在提高模型性能方面的一个重要因素。

    

    在利用人类反馈进行强化学习（RLHF）来对齐大型语言模型方面取得了巨大成功。开源偏好数据集和奖励模型使得在通用聊天设置之外进行更广泛的实验成为可能，特别是为了使系统在网页问答、摘要和多轮对话等任务中更加“有用”。当优化有用性时，我们一直观察到RLHF会驱使模型产生更长的输出。本文证明了对响应长度进行优化是RLHF在这些设置中取得改进的一个重要因素。首先，我们研究了在针对有用性训练的三个开源偏好数据集上训练的奖励模型的奖励与长度之间的关系。在这里，长度与奖励强烈相关，奖励分数的提高在很大程度上是通过改变输出长度的分布来实现的。然后，我们在RL和奖励模型学习过程中进行干预，看是否能够达到...

    Great successes have been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models. Open-source preference datasets and reward models have enabled wider experimentation beyond generic chat settings, particularly to make systems more "helpful" for tasks like web question answering, summarization, and multi-turn dialogue. When optimizing for helpfulness, RLHF has been consistently observed to drive models to produce longer outputs. This paper demonstrates that optimizing for response length is a significant factor behind RLHF's reported improvements in these settings. First, we study the relationship between reward and length for reward models trained on three open-source preference datasets for helpfulness. Here, length correlates strongly with reward, and improvements in reward score are driven in large part by shifting the distribution over output lengths. We then explore interventions during both RL and reward model learning to see if we can ach
    
[^7]: DSPy: 将声明性语言模型调用编译成自我改进的流水线

    DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. (arXiv:2310.03714v1 [cs.CL])

    [http://arxiv.org/abs/2310.03714](http://arxiv.org/abs/2310.03714)

    DSPy是一个编程模型，将LM流水线抽象为文本转换图，通过声明性模块调用LM实现优化，能够解决复杂的推理问题和数学问题等任务。

    

    ML社区正在快速探索用于提示语言模型(LMs)和将它们堆叠成解决复杂任务的流水线的技术。不幸的是，现有的LM流水线通常使用硬编码的"提示模板"来实现，即通过试错发现的冗长字符串。为了更系统地开发和优化LM流水线，我们引入了DSPy，这是一个以文本转换图的形式抽象LM流水线的编程模型，即通过声明性模块调用LM的命令式计算图。DSPy模块是参数化的，这意味着它们可以通过创建和收集示例来学习如何应用提示、微调、增强和推理技术的组合。我们设计了一个编译器，可以优化任何DSPy流水线以最大化给定的度量标准。我们进行了两个案例研究，显示出简洁的DSPy程序可以表达和优化复杂的推理数学问题、登录日志问题等流水线。

    The ML community is rapidly exploring techniques for prompting language models (LMs) and for stacking them into pipelines that solve complex tasks. Unfortunately, existing LM pipelines are typically implemented using hard-coded "prompt templates", i.e. lengthy strings discovered via trial and error. Toward a more systematic approach for developing and optimizing LM pipelines, we introduce DSPy, a programming model that abstracts LM pipelines as text transformation graphs, i.e. imperative computational graphs where LMs are invoked through declarative modules. DSPy modules are parameterized, meaning they can learn (by creating and collecting demonstrations) how to apply compositions of prompting, finetuning, augmentation, and reasoning techniques. We design a compiler that will optimize any DSPy pipeline to maximize a given metric. We conduct two case studies, showing that succinct DSPy programs can express and optimize sophisticated LM pipelines that reason about math word problems, tac
    
[^8]: 代理指导大型语言模型成为通用的零-shot推理器

    Agent Instructs Large Language Models to be General Zero-Shot Reasoners. (arXiv:2310.03710v1 [cs.CL])

    [http://arxiv.org/abs/2310.03710](http://arxiv.org/abs/2310.03710)

    该论文提出了一种方法，通过代理指导的方式，大大提高了大型语言模型在零-shot推理任务上的能力，并在多个数据集上实现了最先进的性能。

    

    我们引入了一种方法，以提高大型语言模型在一般语言理解任务上的零-shot推理能力。具体而言，我们构建了一个自主代理，来指导大型语言模型的推理过程。我们展示了这种方法进一步释放了大型语言模型的零-shot推理能力，适用于更多的任务。我们在涵盖生成、分类和推理的广泛数据集上研究了我们方法的性能。我们展示了我们的方法适用于大多数任务，并在我们评估的29个数据集中，在20个数据集上获得了最先进的零-shot性能。例如，我们的方法显著提升了最先进的大型语言模型的性能，包括Vicuna-13b（13.3%），Llama-2-70b-chat（23.2%）和GPT-3.5 Turbo（17.0%）。与零-shot思维链相比，我们对推理的改进很明显，平均提高了10.5%。通过我们的方法，Llama-2-70b-chat的性能超过零-shot GPT-3.5 Turbo 10.2%。

    We introduce a method to improve the zero-shot reasoning abilities of large language models on general language understanding tasks. Specifically, we build an autonomous agent to instruct the reasoning process of large language models. We show this approach further unleashes the zero-shot reasoning abilities of large language models to more tasks. We study the performance of our method on a wide set of datasets spanning generation, classification, and reasoning. We show that our method generalizes to most tasks and obtains state-of-the-art zero-shot performance on 20 of the 29 datasets that we evaluate. For instance, our method boosts the performance of state-of-the-art large language models by a large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and GPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement in reasoning is striking, with an average increase of 10.5%. With our method, Llama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.
    
[^9]: 超越一视同仁：多目标直接偏好优化

    Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization. (arXiv:2310.03708v1 [cs.LG])

    [http://arxiv.org/abs/2310.03708](http://arxiv.org/abs/2310.03708)

    本文提出了一种无强化学习的算法，称为多目标直接偏好优化（MODPO），它可以根据不同的偏好训练不同的语言模型，通过组合所有目标和特定权重来优化模型。

    

    语言模型（LM）通过强化学习与人类反馈的协同作用，能够很好地与普通标记者保持一致，但可能不适应各种各样的人类偏好。因此，最近的研究方法选择通过收集多维度反馈并为每个维度创建不同的奖励（例如，有益性，无害性，诚实性）进行个性化。通过使用不同的奖励权重，可以通过多目标强化学习（MORL）将LM调整到不同的偏好。然而，强化学习的微调在MORLHF中不稳定且耗费资源，特别是因为各种常常矛盾的目标。在本文中，我们提出了多目标直接偏好优化（MODPO），这是一种无强化学习的算法，它将直接偏好优化（DPO）扩展到多个对齐目标。基本上，MODPO通过训练不同的LM来代表不同的集体奖励模型，这些模型将所有目标和特定权重进行组合。通过简单的交叉熵损失，LM根据MOD进行优化。

    Language models (LMs), despite aligning well with an average labeler through reinforcement learning from human feedback (RLHF), may not universally suit diverse human preferences. Recent approaches therefore opt for customization by collecting multi-dimensional feedback and creating distinct rewards for each dimension (e.g., helpfulness, harmlessness, honesty). LMs can then be tailored to different preferences using multi-objective RL (MORL) with different reward weightings. Yet, RL fine-tuning is unstable and resource-heavy, especially for MORLHF with diverse and usually conflicting objectives. In this paper, we present Multi-Objective Direct Preference Optimization (MODPO), an RL-free algorithm that extends Direct Preference Optimization (DPO) for multiple alignment objectives. Essentially, MODPO trains different LMs to represent different collective reward models that combine all objectives with specific weightings. With a simple cross-entropy loss, the LMs optimized against the MOD
    
[^10]: OMG-ATTACK:自我监督的在流形上生成可迁移的规避攻击

    OMG-ATTACK: Self-Supervised On-Manifold Generation of Transferable Evasion Attacks. (arXiv:2310.03707v1 [cs.LG])

    [http://arxiv.org/abs/2310.03707](http://arxiv.org/abs/2310.03707)

    本文介绍了一种自我监督的、计算经济的方法，用于生成具有迁移性的规避攻击。实验证明该方法在各种模型、未见数据类别甚至防御模型上都是有效的。

    

    规避攻击(EA)被用来通过扭曲输入数据来误导模型的分类，以测试训练好的神经网络的鲁棒性。创建这些攻击是一项具有挑战性的任务，特别是随着模型和数据集的日益复杂。在这项工作中，我们介绍了一种自我监督的、计算经济的生成对抗性示例的方法，设计用于未见黑盒模型的情况。通过借鉴表示学习的技术，我们的方法生成在流形上的攻击，这些攻击被鼓励与数据分布相似。与基于模型生成的攻击相比，这些攻击在攻击训练模型时具有相当的效果，但在攻击未见模型时要更加有效，因为这些攻击更加与数据相关而不是与模型本身相关。我们的实验一致表明该方法在各种模型、未见数据类别甚至防御模型上都是有效的，这表明它具有显著的贡献。

    Evasion Attacks (EA) are used to test the robustness of trained neural networks by distorting input data to misguide the model into incorrect classifications. Creating these attacks is a challenging task, especially with the ever-increasing complexity of models and datasets. In this work, we introduce a self-supervised, computationally economical method for generating adversarial examples, designed for the unseen black-box setting. Adapting techniques from representation learning, our method generates on-manifold EAs that are encouraged to resemble the data distribution. These attacks are comparable in effectiveness compared to the state-of-the-art when attacking the model trained on, but are significantly more effective when attacking unseen models, as the attacks are more related to the data rather than the model itself. Our experiments consistently demonstrate the method is effective across various models, unseen data categories, and even defended models, suggesting a significant ro
    
[^11]: 具有多变量非线性的神经网络架构的Banach空间优化性研究

    Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities. (arXiv:2310.03696v1 [stat.ML])

    [http://arxiv.org/abs/2310.03696](http://arxiv.org/abs/2310.03696)

    本文研究了具有多变量非线性激活函数的神经网络架构在Banach空间的优化性，并构建了一类新的Banach空间家族。结果表明，学习问题的解集完全由具有多变量非线性的神经网络架构来描述。这些最优架构具有跳跃连接，并与正交权重归一化和多索引模型密切相关。

    

    本文研究了一大类具有多变量非线性/激活函数的神经网络架构的变分优化性（具体而言，是Banach空间优化性）。为此，我们通过正则化算子和k-平面变换构建了一类新的Banach空间家族。我们证明了一个表示定理，该定理说明在这些Banach空间上提出的学习问题的解集完全由具有多变量非线性的神经网络架构来描述。这些最优的架构具有跳跃连接，并与正交权重归一化和多索引模型息息相关，这两个模型在神经网络界引起了相当大的兴趣。我们的框架适用于包括修正线性单元（ReLU）激活函数、范数激活函数以及在薄板/多次谐波样条理论中找到的径向基函数在内的多种经典非线性函数。

    We investigate the variational optimality (specifically, the Banach space optimality) of a large class of neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator and the $k$-plane transform. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received considerable interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit (ReLU) activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the
    
[^12]: 多边际生成建模中的随机插值方法

    Multimarginal generative modeling with stochastic interpolants. (arXiv:2310.03695v1 [cs.LG])

    [http://arxiv.org/abs/2310.03695](http://arxiv.org/abs/2310.03695)

    该论文研究了多边际生成建模问题，提出了一种基于随机插值方法的学习算法，可以恢复给定概率密度作为边际的联合分布并且识别多向对应关系。

    

    给定一组K个概率密度，我们考虑学习一个联合分布，以使这些概率密度作为边际恢复。这个联合分布的结构应该能够识别预设边际之间的多向对应关系。我们在随机插值框架的泛化中对这个任务提出了一种方法，从而导致了基于动力测量传输的高效学习算法。我们的生成模型由速度和评分场定义，可以被表征为简单二次目标的最小化者，并且是在一般化了时间变量的单纯形上定义的。在单纯形上的传输受到所有边际的影响，并且我们展示了可以提取出多向对应关系。这些对应关系的确定对于样式迁移、算法公平性和数据去污等应用有意义。

    Given a set of $K$ probability densities, we consider the multimarginal generative modeling problem of learning a joint distribution that recovers these densities as marginals. The structure of this joint distribution should identify multi-way correspondences among the prescribed marginals. We formalize an approach to this task within a generalization of the stochastic interpolant framework, leading to efficient learning algorithms built upon dynamical transport of measure. Our generative models are defined by velocity and score fields that can be characterized as the minimizers of simple quadratic objectives, and they are defined on a simplex that generalizes the time variable in the usual dynamical transport framework. The resulting transport on the simplex is influenced by all marginals, and we show that multi-way correspondences can be extracted. The identification of such correspondences has applications to style transfer, algorithmic fairness, and data decorruption. In addition, 
    
[^13]: 调整对齐语言模型会牺牲安全性，即使用户没有意图！

    Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!. (arXiv:2310.03693v1 [cs.CL])

    [http://arxiv.org/abs/2310.03693](http://arxiv.org/abs/2310.03693)

    微调对齐语言模型会牺牲安全性，即使用户没有恶意意图。通过敌对设计的训练样本，即使只有少数10个，也可以破坏语言模型的安全对齐。这种安全风险存在于微调后的模型中。

    

    对大型语言模型进行下游用例的优化通常涉及通过进一步的微调来定制预训练语言模型。Meta发布Llama模型和OpenAI的GPT-3.5 Turbo的自定义数据集微调API也鼓励这种做法。但是，这种自定义微调的安全成本是多少？我们注意到，尽管现有的安全对齐基础设施可以在推理时限制语言模型的有害行为，但它们并不涵盖当微调特权扩展给终端用户时的安全风险。我们的红队研究发现，只需几个敌对设计的训练样本就可以破坏语言模型的安全对齐。例如，我们只使用10个这样的示例在OpenAI的API中以不到0.20美元的成本将GPT-3.5 Turbo的安全保护解除了，使模型对几乎任何有害指令都有响应。令人担忧的是，我们的研究还发现，即使没有恶意意图，微调后的模型也存在安全风险。

    Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious 
    
[^14]: SmoothLLM：防御大型语言模型免受越狱攻击

    SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v1 [cs.LG])

    [http://arxiv.org/abs/2310.03684](http://arxiv.org/abs/2310.03684)

    SmoothLLM是第一个用于减轻大型语言模型上越狱攻击的算法，通过在输入提示上随机扰动并汇总预测结果来检测对抗性输入，将攻击成功率降低至不到一个百分点，并提供了可证明的保证。

    

    尽管努力将大型语言模型（LLM）与人类价值观保持一致，但广泛使用的LLM（如GPT、Llama、Claude和PaLM）仍然容易受到越狱攻击，即对目标LLM进行欺骗，以生成不合适的内容。为了解决这个漏洞，我们提出了SmoothLLM，这是第一个旨在减轻LLM上的越狱攻击的算法。基于我们的发现，对抗性生成的提示对字符级别的改变很脆弱，我们的防御首先随机扰动给定输入提示的多个副本，然后汇总相应的预测结果来检测对抗性输入。SmoothLLM将众多热门LLM的攻击成功率降低至不到一个百分点，避免了不必要的保守性，并对攻击缓解提供了可证明的保证。此外，我们的防御使用的查询数量比现有的攻击方法少得多，并且与任何LLM兼容。

    Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM.
    
[^15]: 使用整数进行Hadamard域训练的类增量量化学习

    Hadamard Domain Training with Integers for Class Incremental Quantized Learning. (arXiv:2310.03675v1 [cs.LG])

    [http://arxiv.org/abs/2310.03675](http://arxiv.org/abs/2310.03675)

    本文提出了一种使用廉价的Hadamard变换来实现整数矩阵乘法的低精度训练技术，以在资源受限的边缘平台上实现计算和内存的高效性。

    

    连续学习是许多现代机器学习应用中的一个理想特性，它允许现场适应和更新，包括适应分布变化、微调和学习新任务。对于具有隐私和低延迟要求的应用，连续学习所带来的计算和内存需求可能会对资源受限的边缘平台造成成本限制。通过完全量化训练（FQT）减少计算精度可以同时减少内存占用并增加训练和推断的计算效率。然而，激进的量化，特别是整数FQT，往往会将模型的准确性降低到无法接受的水平。在本文中，我们提出了一种利用廉价的Hadamard变换实现仅通过整数矩阵乘法进行低精度训练的技术。我们进一步确定哪些张量需要随机舍入，并提出了分块矩阵乘法以实现低位宽累加器。

    Continual learning is a desirable feature in many modern machine learning applications, which allows in-field adaptation and updating, ranging from accommodating distribution shift, to fine-tuning, and to learning new tasks. For applications with privacy and low latency requirements, the compute and memory demands imposed by continual learning can be cost-prohibitive for resource-constraint edge platforms. Reducing computational precision through fully quantized training (FQT) simultaneously reduces memory footprint and increases compute efficiency for both training and inference. However, aggressive quantization especially integer FQT typically degrades model accuracy to unacceptable levels. In this paper, we propose a technique that leverages inexpensive Hadamard transforms to enable low-precision training with only integer matrix multiplications. We further determine which tensors need stochastic rounding and propose tiled matrix multiplication to enable low-bit width accumulators. 
    
[^16]: 战略评估：对象、评估者和社会

    Strategic Evaluation: Subjects, Evaluators, and Society. (arXiv:2310.03655v1 [cs.CY])

    [http://arxiv.org/abs/2310.03655](http://arxiv.org/abs/2310.03655)

    本论文以评估设计作为出发点，研究了在形式和量化评估中的战略行为以及评估者和被评估对象之间的道德判断关系。

    

    算法在形式上和量化中模糊概念（如价值）的度量中得到了广泛的应用，以做出决策。当人们为了获得有利的决策结果而对这些评估作出战略性回应时，他们的行为可能受到道德判断。他们可能被描述为“操纵系统”或“作弊”，或者（在其他情况下）投入“诚实努力”或“改进”。机器学习文献对战略行为进行了描述，强调了决策对象为获得更有利评估所付出的努力——一些作品提供了预防或阻止这种操纵的方法，一些区分了“操纵”和“改进”行为，而另一些旨在测量分类系统的努力负担或不平等效果。我们从一个不同的出发点开始：评估设计本身就可以被理解为评估者所追求的目标，这些目标可能与社会的目标不一致。

    A broad current application of algorithms is in formal and quantitative measures of murky concepts -- like merit -- to make decisions. When people strategically respond to these sorts of evaluations in order to gain favorable decision outcomes, their behavior can be subjected to moral judgments. They may be described as 'gaming the system' or 'cheating,' or (in other cases) investing 'honest effort' or 'improving.' Machine learning literature on strategic behavior has tried to describe these dynamics by emphasizing the efforts expended by decision subjects hoping to obtain a more favorable assessment -- some works offer ways to preempt or prevent such manipulations, some differentiate 'gaming' from 'improvement' behavior, while others aim to measure the effort burden or disparate effects of classification systems. We begin from a different starting point: that the design of an evaluation itself can be understood as furthering goals held by the evaluator which may be misaligned with bro
    
[^17]: 物理增强神经网络的极度稀疏化：用于可解释的模型发现在力学中

    Extreme sparsification of physics-augmented neural networks for interpretable model discovery in mechanics. (arXiv:2310.03652v1 [cs.CE])

    [http://arxiv.org/abs/2310.03652](http://arxiv.org/abs/2310.03652)

    该论文提出了一种训练物理增强神经网络的极度稀疏化方法，以实现在力学中可解释的模型发现。这种方法能够生成具有较少可训练参数的本构模型，使其更易于解释和理解。

    

    近年来，基于神经网络的数据驱动本构建模在物理学中受到越来越多的关注，因为它能够轻松地融入物理和机制约束，并且能够克服描述物质响应的表象本构定律的费时且具有挑战性的任务。然而，尽管基于神经网络的本构定律显示出良好的泛化能力，但由于其可训练参数数量过多，所生成的表示形式不容易解释。存在着稀疏回归方法可以获得可解释的表达式，但用户需要创建模型形式的库，这限制了库所提供的函数形式的表达能力。在这项工作中，我们提出了使用平滑的$L^{0}$正则化训练物理增强神经网络的本构模型。这旨在维持对真实情况的建模能力的同时产生可解释性的结果。

    Data-driven constitutive modeling with neural networks has received increased interest in recent years due to its ability to easily incorporate physical and mechanistic constraints and to overcome the challenging and time-consuming task of formulating phenomenological constitutive laws that can accurately capture the observed material response. However, even though neural network-based constitutive laws have been shown to generalize proficiently, the generated representations are not easily interpretable due to their high number of trainable parameters. Sparse regression approaches exist that allow to obtaining interpretable expressions, but the user is tasked with creating a library of model forms which by construction limits their expressiveness to the functional forms provided in the libraries. In this work, we propose to train regularized physics-augmented neural network-based constitutive models utilizing a smoothed version of $L^{0}$-regularization. This aims to maintain the trus
    
[^18]: 重新思考人工智能与人类合作的公平性

    Rethinking Fairness for Human-AI Collaboration. (arXiv:2310.03647v1 [cs.LG])

    [http://arxiv.org/abs/2310.03647](http://arxiv.org/abs/2310.03647)

    在人工智能与人类合作中，需要重新思考公平性，因为完全遵守算法决策很少是现实可行的，因此我们需要设计稳健公平的算法推荐来提升公平性。

    

    现有的算法公平性方法旨在确保人类决策者完全遵守算法决策时实现公平的结果。然而，在人工智能与人类合作中，完全遵守算法决策很少是现实或理想的结果。然而，最近的研究表明，对公平算法的选择性遵守会相对于人类以前的政策增加歧视。因此，确保公平结果需要基本不同的算法设计原则，以确保对决策者（事先不知道）的遵守模式具有稳健性。我们定义了一种遵守稳健公平的算法推荐，无论人类的遵守模式如何，它们都能确保在决策中改善公平性（弱形意义上）。我们提出了一种简单的优化策略来确定最佳的性能改进遵守稳健公平策略。然而，我们发现设计算法推荐可能是不可行的。

    Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are s
    
[^19]: TRAM: 桥接信任区域和锐度感知最小化

    TRAM: Bridging Trust Regions and Sharpness Aware Minimization. (arXiv:2310.03646v1 [cs.LG])

    [http://arxiv.org/abs/2310.03646](http://arxiv.org/abs/2310.03646)

    TRAM是一种桥接信任区域和锐度感知最小化的算法，通过减少损失曲面的曲率来提供鲁棒性改进。它通过在微调过程中优化可转移的表示来实现领域外泛化，并且通过结合信任区域方法和SAM风格的正则化器来统一参数和表示空间平滑方法。TRAM在保持预训练结构的同时实现了平坦的极小值和平滑、有信息量的表示。

    

    通过减少参数空间中损失曲面的曲率，锐度感知最小化（SAM）在领域转移下提供了广泛的鲁棒性改进。然而，本文不是关注参数，而是考虑到表示的可转移性作为优化目标，在微调设置中实现领域外泛化。为了鼓励保留可转移的表示，我们考虑到基于信任区域的微调方法，这些方法利用任务特定的技能，而不会忘记预训练的任务无关表示。我们通过使用信任区域边界在这两种优化表面上通知SAM风格的正则化器，统一了参数和表示空间平滑方法。我们提出了Trust Region Aware Minimization (TRAM)，一种优化平坦的极小值和平滑、有信息量的表示的微调算法，而不会忘记预先训练的结构。我们发现，TRAM优于锐度感知和基于信任区域的方法。

    By reducing the curvature of the loss surface in the parameter space, Sharpness-aware minimization (SAM) yields widespread robustness improvement under domain transfer. Instead of focusing on parameters, however, this work considers the transferability of representations as the optimization target for out-of-domain generalization in a fine-tuning setup. To encourage the retention of transferable representations, we consider trust region-based fine-tuning methods, which exploit task-specific skills without forgetting task-agnostic representations from pre-training. We unify parameter- and representation-space smoothing approaches by using trust region bounds to inform SAM-style regularizers on both of these optimization surfaces. We propose Trust Region Aware Minimization (TRAM), a fine-tuning algorithm that optimizes for flat minima and smooth, informative representations without forgetting pre-trained structure. We find that TRAM outperforms both sharpness-aware and trust region-based
    
[^20]: 从Nisan的自然证明中的分布式PAC学习

    Distributional PAC-Learning from Nisan's Natural Proofs. (arXiv:2310.03641v1 [cs.CC])

    [http://arxiv.org/abs/2310.03641](http://arxiv.org/abs/2310.03641)

    本论文研究了从Nisan的自然证明中的分布式PAC学习，并得到了正面和负面的结果。

    

    Carmosino等人证明了Lambda的自然证明意味着可以通过均匀分布、成员查询来高效学习Lambda电路，我们考虑将这个推论推广到不包含AC^0[p]的Lambda和在Valiant的PAC模型中使用随机示例和任意示例分布学习的算法。我们得到了积极和消极的结果。消极方面，我们观察到如果对于每个电路类Lambda，从Lambda的自然证明到在Valiant的PAC模型中学习Lambda电路的推论成立，则对O(n^{1.5})-uSVP（唯一的最短向量问题）有多项式时间的解决方法，并且对O(n^{1.5})-SVP（最短向量问题）和O(n^{1.5})-SIVP（最短独立向量问题）有多项式时间的量子解决方法。

    (Abridged) Carmosino et al. (2016) demonstrated that natural proofs of circuit lower bounds for \Lambda imply efficient algorithms for learning \Lambda-circuits, but only over the uniform distribution, with membership queries, and provided \AC^0[p] \subseteq \Lambda. We consider whether this implication can be generalized to \Lambda \not\supseteq \AC^0[p], and to learning algorithms in Valiant's PAC model, which use only random examples and learn over arbitrary example distributions. We give results of both positive and negative flavor.  On the negative side, we observe that if, for every circuit class \Lambda, the implication from natural proofs for \Lambda to learning \Lambda-circuits in Valiant's PAC model holds, then there is a polynomial time solution to O(n^{1.5})-uSVP (unique Shortest Vector Problem), and polynomial time quantum solutions to O(n^{1.5})-SVP (Shortest Vector Problem) and O(n^{1.5})-SIVP (Shortest Independent Vector Problem). This indicates that whether natural pro
    
[^21]: CLEVRER-Humans: 用人类的方式描述物理和因果事件

    CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])

    [http://arxiv.org/abs/2310.03635](http://arxiv.org/abs/2310.03635)

    CLEVRER-Humans是一个用于因果判断的视频推理数据集，通过人工标注来解决合成事件和合成语言描述的缺乏多样性问题，并通过迭代事件填空和神经语言生成模型提高数据收集效率。

    

    构建能够推理物理事件及其因果关系的机器对于与物理世界进行灵活互动非常重要。然而，现有的大多数物理和因果推理基准都仅基于合成事件和合成自然语言描述的因果关系。这种设计存在两个问题：一是事件类型和自然语言描述缺乏多样性；二是基于手动定义的启发式规则的因果关系与人类判断不一致。为了解决这两个问题，我们提出了CLEVRER-Humans基准，这是一个用人工标注的视频推理数据集，用于对物理事件的因果判断。我们采用了两种技术来提高数据收集效率：首先，一种新颖的迭代事件填空任务，以 eliciting 视频中事件的新表示方式，我们称之为因果事件图 (CEGs)；其次，一种基于神经语言生成模型的数据增强技术。

    Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.
    
[^22]: 用于机器人自建模和运动规划的高自由度动态神经领域

    High-Degrees-of-Freedom Dynamic Neural Fields for Robot Self-Modeling and Motion Planning. (arXiv:2310.03624v1 [cs.CV])

    [http://arxiv.org/abs/2310.03624](http://arxiv.org/abs/2310.03624)

    本论文介绍了一种使用神经领域进行机器人自建模和运动规划的方法。通过利用2D图像和相机姿态进行学习，无需深度图像或几何知识，实现了对高自由度物体的建模。在7自由度机器人测试中，所学的自建模与真实模型的差距仅为2%。

    

    机器人自建模是机器人物理形态的任务无关表示，可用于在没有经典几何运动学模型的情况下进行运动规划任务。特别是当后者很难工程化或机器人的运动学出现意外变化时，无人自建模是真正自主控制机器人的必要功能。在这项工作中，我们利用神经领域来使机器人能够以神经隐式查询模型的形式自建模其运动学，该模型仅通过带有相机姿态和配置的2D图像进行学习。这比现有方法具有更广泛的适用性，因为它不依赖深度图像或几何知识。为此，我们提出了一种基于编码器的新型动态物体中心场景的神经密度场架构，该架构可以适应高自由度（DOFs）的条件。在一个7自由度的机器人测试设置中，学得的自建模与真实模型之间的Chamfer-L2距离仅为2%。

    A robot self-model is a task-agnostic representation of the robot's physical morphology that can be used for motion planning tasks in absence of classical geometric kinematic models. In particular, when the latter are hard to engineer or the robot's kinematics change unexpectedly, human-free self-modeling is a necessary feature of truly autonomous agents. In this work, we leverage neural fields to allow a robot to self-model its kinematics as a neural-implicit query model learned only from 2D images annotated with camera poses and configurations. This enables significantly greater applicability than existing approaches which have been dependent on depth images or geometry knowledge. To this end, alongside a curricular data sampling strategy, we propose a new encoder-based neural density field architecture for dynamic object-centric scenes conditioned on high numbers of degrees of freedom (DOFs). In a 7-DOF robot test setup, the learned self-model achieves a Chamfer-L2 distance of 2% of
    
[^23]: CLASSify: 一个用于机器学习的基于Web的工具

    CLASSify: A Web-Based Tool for Machine Learning. (arXiv:2310.03618v1 [cs.LG])

    [http://arxiv.org/abs/2310.03618](http://arxiv.org/abs/2310.03618)

    CLASSify是一个基于Web的机器学习工具，能够简化生物信息学中的分类问题的解决过程。它提供了自动化模型训练和结果生成，可视化和数据洞察功能，支持二元和多类分类问题，并提供多种模型和方法的访问。它还能生成合成数据，支持特征评估和解释性得分分析。

    

    在生物信息学中，机器学习分类问题很常见，但执行模型训练、优化和推理所需的技术知识可能会阻止研究人员利用这项技术。本文介绍了一个自动化的机器学习分类问题工具，简化了模型训练和结果生成的过程，同时提供信息丰富的可视化和对数据的深入洞察。该工具支持二元和多类分类问题，并提供多种模型和方法的访问。在界面内可以生成合成数据来填补缺失值、平衡类别标签或生成全新的数据集。它还支持特征评估，并生成解释性得分来指示哪些特征对输出影响最大。本文介绍了CLASSify，一个开源工具，简化了解决分类问题的用户体验，无需了解机器学习知识。

    Machine learning classification problems are widespread in bioinformatics, but the technical knowledge required to perform model training, optimization, and inference can prevent researchers from utilizing this technology. This article presents an automated tool for machine learning classification problems to simplify the process of training models and producing results while providing informative visualizations and insights into the data. This tool supports both binary and multiclass classification problems, and it provides access to a variety of models and methods. Synthetic data can be generated within the interface to fill missing values, balance class labels, or generate entirely new datasets. It also provides support for feature evaluation and generates explainability scores to indicate which features influence the output the most. We present CLASSify, an open-source tool for simplifying the user experience of solving classification problems without the need for knowledge of mach
    
[^24]: 用于社会公益的对抗机器学习：将对手重新定义为盟友

    Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally. (arXiv:2310.03614v1 [cs.LG])

    [http://arxiv.org/abs/2310.03614](http://arxiv.org/abs/2310.03614)

    对抗机器学习面临的漏洞和对社会的不良影响问题，可以通过将对手重新定义为盟友，来实现有益于社会的创新应用。

    

    深度神经网络（DNN）是推动机器学习最近进展的主要力量。然而，研究表明DNN容易受到对抗样本的攻击，即通过扰动输入样本来迫使基于DNN的模型产生错误。因此，对抗机器学习（AdvML）引起了大量关注，研究人员在不同环境和模态下研究了这些漏洞。此外，发现DNN容易包含内嵌偏见，并且经常产生难以解释的预测结果，这可能导致反社会的人工智能应用。利用大型语言模型（LLM），如ChatGPT和GPT-4等新技术的出现，增加了大规模产生反社会应用的风险。为社会公益而进行的对抗机器学习（AdvML4G）是一个新兴领域，它利用AdvML漏洞来发明有益于社会的应用。监管机构、从业者和研究人员应该合作，推动有益于社会的应用的发展。

    Deep Neural Networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples -- input samples that have been perturbed to force DNN-based models to make errors. As a result, Adversarial Machine Learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social AI applications. The emergence of new AI technologies that leverage Large Language Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing anti-social applications at scale. AdvML for Social Good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent pro-social applications. Regulators, practitioners, and researchers should collaborate to encourage the development of pro-s
    
[^25]: 在联邦学习中解决一类非凸最小极大优化问题

    Solving a Class of Non-Convex Minimax Optimization in Federated Learning. (arXiv:2310.03613v1 [cs.LG])

    [http://arxiv.org/abs/2310.03613](http://arxiv.org/abs/2310.03613)

    本研究针对联邦学习中的一类非凸最小极大优化问题，提出了FL算法（FedSGDA+和FedSGDA-M），并在最常见的最小极大问题中降低了复杂度。针对非凸凹问题，提出的FedSGDA+算法将通信复杂度降低到O(ε^{-6})。在非凸强凹和非凸PL最小极大设置下，证明了FedSGDA-M具有已知的样本复杂度。

    

    最小极大问题广泛应用于机器学习中的各种场景，包括对抗训练、增强学习中的策略评估以及 AUROC 最大化等。为了解决跨多个客户端的大规模数据挑战，在通信高效的分布式训练中，联邦学习（FL）越来越受欢迎。尽管在集中式（即单机）环境下已经开发了许多最小极大问题的优化算法，但在 FL 下的最小极大问题算法仍然不够深入研究。本文研究了一类联邦非凸最小极大优化问题。我们提出了 FL 算法（FedSGDA+ 和 FedSGDA-M）并降低了最常见最小极大问题的现有复杂度结果。对于非凸凹问题，我们提出了 FedSGDA+ 并将通信复杂度降低到 $O(\varepsilon^{-6})$。在非凸强凹和非凸 PL 最小极大设置下，我们证明了 FedSGDA-M 具有已知的样本复杂度。

    The minimax problems arise throughout machine learning applications, ranging from adversarial training and policy evaluation in reinforcement learning to AUROC maximization. To address the large-scale data challenges across multiple clients with communication-efficient distributed training, federated learning (FL) is gaining popularity. Many optimization algorithms for minimax problems have been developed in the centralized setting (\emph{i.e.} single-machine). Nonetheless, the algorithm for minimax problems under FL is still underexplored. In this paper, we study a class of federated nonconvex minimax optimization problems. We propose FL algorithms (FedSGDA+ and FedSGDA-M) and reduce existing complexity results for the most common minimax problems. For nonconvex-concave problems, we propose FedSGDA+ and reduce the communication complexity to $O(\varepsilon^{-6})$. Under nonconvex-strongly-concave and nonconvex-PL minimax settings, we prove that FedSGDA-M has the best-known sample comp
    
[^26]: GENER:一种并行层深度学习网络用于从基因表达数据中检测基因-基因相互作用

    GENER: A Parallel Layer Deep Learning Network To Detect Gene-Gene Interactions From Gene Expression Data. (arXiv:2310.03611v1 [cs.LG])

    [http://arxiv.org/abs/2310.03611](http://arxiv.org/abs/2310.03611)

    GENER是一个并行层深度学习网络，专门用于从基因表达数据中检测基因-基因相互作用。实验证实GENER在预测基因-基因相互作用方面的性能优于其他统计和深度学习方法。

    

    基于已知基因表达和基因相互作用数据来检测和发现新的基因相互作用是一个重大挑战。各种统计和深度学习方法已经尝试通过利用基因相互作用的拓扑结构和基因表达模式来预测新的基因相互作用。相反，一些方法专注于利用基因表达数据。在这个背景下，我们介绍了GENER，这是一个并行层深度学习网络，专门用于使用基因表达数据来识别基因-基因关系。我们进行了两个训练实验，并将我们的网络的性能与现有的统计和深度学习方法进行了比较。值得注意的是，我们的模型在结合了BioGRID&DREAM5数据集上实现了0.834的平均AUROC分数，优于其他方法在预测基因-基因相互作用方面的表现。

    Detecting and discovering new gene interactions based on known gene expressions and gene interaction data presents a significant challenge. Various statistical and deep learning methods have attempted to tackle this challenge by leveraging the topological structure of gene interactions and gene expression patterns to predict novel gene interactions. In contrast, some approaches have focused exclusively on utilizing gene expression profiles. In this context, we introduce GENER, a parallel-layer deep learning network designed exclusively for the identification of gene-gene relationships using gene expression data. We conducted two training experiments and compared the performance of our network with that of existing statistical and deep learning approaches. Notably, our model achieved an average AUROC score of 0.834 on the combined BioGRID&DREAM5 dataset, outperforming competing methods in predicting gene-gene interactions.
    
[^27]: 将用于研究论文的时间序列分析方法与预测非洲COVID-19病例的比较：文献综述

    Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review. (arXiv:2310.03606v1 [cs.LG])

    [http://arxiv.org/abs/2310.03606](http://arxiv.org/abs/2310.03606)

    本文献综述比较了在预测非洲COVID-19病例中使用的各种时间序列分析方法，突出了它们的有效性和局限性。

    

    本文献综述旨在比较在预测非洲COVID-19病例中使用的各种时间序列分析方法。该研究对2020年1月至2023年7月发表的英文研究论文进行了系统搜索，重点关注在非洲COVID-19数据集上使用时间序列分析方法的论文。该过程使用了包括PubMed、谷歌学术、Scopus和科学引文索引等多种数据库。研究论文经过评估过程，提取了关于时间序列分析模型的实施和性能的相关信息。该研究突出了不同的方法学，并评估了它们在预测病毒传播方面的有效性和局限性。本综述的结果可以为该领域提供更深入的见解，未来的研究应考虑这些见解，以改进时间序列分析模型并探索不同方法之间的整合。

    This literature review aimed to compare various time-series analysis approaches utilized in forecasting COVID-19 cases in Africa. The study involved a methodical search for English-language research papers published between January 2020 and July 2023, focusing specifically on papers that utilized time-series analysis approaches on COVID-19 datasets in Africa. A variety of databases including PubMed, Google Scholar, Scopus, and Web of Science were utilized for this process. The research papers underwent an evaluation process to extract relevant information regarding the implementation and performance of the time-series analysis models. The study highlighted the different methodologies employed, evaluating their effectiveness and limitations in forecasting the spread of the virus. The result of this review could contribute deeper insights into the field, and future research should consider these insights to improve time series analysis models and explore the integration of different appr
    
[^28]: FASER: 通过中间表示进行二进制代码相似性搜索

    FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v1 [cs.CR])

    [http://arxiv.org/abs/2310.03605](http://arxiv.org/abs/2310.03605)

    本论文提出了一种名为FASER的方法，通过使用中间表示进行二进制代码相似性搜索。该方法可以跨架构地识别函数，并明确编码函数的语义，以支持各种应用场景。

    

    能够识别跨架构软件中感兴趣的函数对于分析恶意软件、保护软件供应链或进行漏洞研究都是有用的。跨架构二进制代码相似性搜索已在许多研究中探索，并使用了各种不同的数据来源来实现其目标。通常使用的数据来源包括从二进制文件中提取的常见结构，如函数控制流图或二进制级调用图，反汇编过程的输出或动态分析方法的输出。其中一种受到较少关注的数据来源是二进制中间表示。二进制中间表示具有两个有趣的属性：它们的跨架构性质以及明确编码函数的语义以支持下游使用。在本文中，我们提出了一种名为FASER的函数字符串编码表示方法，它结合了长文档转换技术。

    Being able to identify functions of interest in cross-architecture software is useful whether you are analysing for malware, securing the software supply chain or conducting vulnerability research. Cross-Architecture Binary Code Similarity Search has been explored in numerous studies and has used a wide range of different data sources to achieve its goals. The data sources typically used draw on common structures derived from binaries such as function control flow graphs or binary level call graphs, the output of the disassembly process or the outputs of a dynamic analysis approach. One data source which has received less attention is binary intermediate representations. Binary Intermediate representations possess two interesting properties: they are cross architecture by their very nature and encode the semantics of a function explicitly to support downstream usage. Within this paper we propose Function as a String Encoded Representation (FASER) which combines long document transforme
    
[^29]: 在概率测度空间中通过梯度流进行抽样

    Sampling via Gradient Flows in the Space of Probability Measures. (arXiv:2310.03597v1 [stat.ML])

    [http://arxiv.org/abs/2310.03597](http://arxiv.org/abs/2310.03597)

    通过梯度流抽样方法的研究方向在计算科学和工程中具有重要意义。本文通过研究概率测度空间中的梯度流的设计组成部分，提出了三个贡献：Kullback-Leibler散度作为能量泛函的独特属性、度量的选择与不变性的关系。

    

    在计算科学和工程中，使用未知归一化常数的目标概率分布进行抽样是一项基本的挑战。最近的研究表明，通过考虑概率测度空间中的梯度流派生的算法为算法开发开辟了新的途径。本文通过审查这种梯度流的设计组成部分，对这种抽样方法做出了三个贡献。抽样的任何实例化都需要一个能量泛函和一个度量来确定流动，以及流动的数值近似来推导算法。我们的第一个贡献是展示了Kullback-Leibler散度作为一个能量泛函具有唯一的特征（在所有f-散度中），即由其得到的梯度流不依赖于目标分布的归一化常数。我们的第二个贡献是从不变性的角度研究度量的选择。Fisher-Rao度量被称为t

    Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as t
    
[^30]: TimeGPT-1. (arXiv:2310.03589v1 [cs.LG]) - 时间序列的基础模型

    TimeGPT-1. (arXiv:2310.03589v1 [cs.LG])

    [http://arxiv.org/abs/2310.03589](http://arxiv.org/abs/2310.03589)

    TimeGPT是第一个面向时间序列的基础模型，能够生成准确的预测。它在性能、效率和简洁性方面优于现有的统计学、机器学习和深度学习方法。我们的研究提供了有力的证据，表明借鉴其他人工智能领域的见解可以有效应用于时间序列分析。大规模时间序列模型有望民主化访问精确的预测并减少不确定性。

    

    在本文中，我们介绍了TimeGPT，这是第一个面向时间序列的基础模型，能够对训练过程中未见过的各种数据集生成准确的预测。我们将预训练的模型与已建立的统计学、机器学习和深度学习方法进行了评估，结果表明TimeGPT的零-shot推理在性能、效率和简洁性方面表现出色。我们的研究提供了有力的证据，表明人工智能的其他领域的见解可以有效地应用于时间序列分析。我们得出结论，大规模时间序列模型为人们提供了一个令人兴奋的机会，通过利用当代深度学习的能力，民主化访问精确的预测并减少不确定性。

    In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.
    
[^31]: 跨条件分支的自动微分平滑方法

    Smoothing Methods for Automatic Differentiation Across Conditional Branches. (arXiv:2310.03585v1 [cs.LG])

    [http://arxiv.org/abs/2310.03585](http://arxiv.org/abs/2310.03585)

    本研究提出了一种通过结合平滑解释和自动微分的方法来处理具有条件分支的程序，并成功计算出平滑程序的梯度，从而支持分支程序参数合成和机器学习流程中的模型校准。

    

    具有条件分支引入的不连续性的程序对假定目标函数响应曲面具有一定平滑性的数学优化方法提出了挑战。平滑解释（SI）是一种抽象解释形式，它以高斯核近似程序输出的卷积，从而以原则性的方式平滑其输出。在这里，我们将SI与自动微分（AD）相结合，以高效地计算平滑程序的梯度。与在常规程序执行中进行的自动微分不同，这些梯度还捕捉了替代控制流路径的影响。SI与AD的组合使得支持基于梯度的分支程序参数合成成为可能，例如在机器学习流程中，对仿真模型进行校准或将其与神经网络模型结合。我们详细说明了SI中为可行性而进行的近似的影响，并提出了改进方法。

    Programs involving discontinuities introduced by control flow constructs such as conditional branches pose challenges to mathematical optimization methods that assume a degree of smoothness in the objective function's response surface. Smooth interpretation (SI) is a form of abstract interpretation that approximates the convolution of a program's output with a Gaussian kernel, thus smoothing its output in a principled manner. Here, we combine SI with automatic differentiation (AD) to efficiently compute gradients of smoothed programs. In contrast to AD across a regular program execution, these gradients also capture the effects of alternative control flow paths. The combination of SI with AD enables the direct gradient-based parameter synthesis for branching programs, allowing for instance the calibration of simulation models or their combination with neural network models in machine learning pipelines. We detail the effects of the approximations made for tractability in SI and propose
    
[^32]: 具有弹性步态的本地导航：学习在受损感知下端到端穿越

    Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End. (arXiv:2310.03581v1 [cs.RO])

    [http://arxiv.org/abs/2310.03581](http://arxiv.org/abs/2310.03581)

    本文提出了一种基于强化学习的本地导航策略，通过将感知失效建模为障碍物和坑洞，从受损的感知中重建环境信息并进行端到端的反应，实现了在受损感知下的可靠导航。

    

    自主机器人必须在未知环境中可靠地导航，即使在受损的外感知或感知失效的情况下也是如此。本文将感知失效建模为看不见的障碍物和坑洞，并训练了一种基于强化学习的本地导航策略来引导我们的腿式机器人。与依赖启发式和异常检测来更新导航信息的先前工作不同，我们训练我们的导航策略以在潜在空间中从损坏的感知中重建环境信息并对感知失效进行端到端的反应。为此，我们将本体感知和外感知都纳入我们的策略输入中，从而使策略能够感知不同身体部位的碰撞和坑洞，并引发相应的反应。

    Autonomous robots must navigate reliably in unknown environments even under compromised exteroceptive perception, or perception failures. Such failures often occur when harsh environments lead to degraded sensing, or when the perception algorithm misinterprets the scene due to limited generalization. In this paper, we model perception failures as invisible obstacles and pits, and train a reinforcement learning (RL) based local navigation policy to guide our legged robot. Unlike previous works relying on heuristics and anomaly detection to update navigational information, we train our navigation policy to reconstruct the environment information in the latent space from corrupted perception and react to perception failures end-to-end. To this end, we incorporate both proprioception and exteroception into our policy inputs, thereby enabling the policy to sense collisions on different body parts and pits, prompting corresponding reactions. We validate our approach in simulation and on the 
    
[^33]: 针对具有推广能力的神经辐射场的目标对抗攻击

    Targeted Adversarial Attacks on Generalizable Neural Radiance Fields. (arXiv:2310.03578v1 [cs.LG])

    [http://arxiv.org/abs/2310.03578](http://arxiv.org/abs/2310.03578)

    本文介绍了如何通过低强度对抗攻击和对抗补丁对具有推广能力的NeRFs进行攻击，后者足够强大，可以应用于现实世界中。我们还展示了针对性攻击，这些攻击成功地生成了预定义输出场景。

    

    3D场景表示和渲染的强大工具-神经辐射场（NeRFs）近期出现。这些数据驱动模型可以通过稀疏的2D观测学习合成高质量图像，实现逼真且交互式的场景重建。然而，NeRFs在增强现实、机器人技术和虚拟环境等关键应用中的广泛使用可能会受到对抗攻击的威胁。

    Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for 3D scene representation and rendering. These data-driven models can learn to synthesize high-quality images from sparse 2D observations, enabling realistic and interactive scene reconstructions. However, the growing usage of NeRFs in critical applications such as augmented reality, robotics, and virtual environments could be threatened by adversarial attacks.  In this paper we present how generalizable NeRFs can be attacked by both low-intensity adversarial attacks and adversarial patches, where the later could be robust enough to be used in real world applications. We also demonstrate targeted attacks, where a specific, predefined output scene is generated by these attack with success.
    
[^34]: 从有限的样本复杂度中学习基于流的生成模型的分析

    Analysis of learning a flow-based generative model from limited sample complexity. (arXiv:2310.03575v1 [stat.ML])

    [http://arxiv.org/abs/2310.03575](http://arxiv.org/abs/2310.03575)

    我们分析了从有限样本复杂度中训练基于流的生成模型的问题，并提供了尖锐的端到端分析。我们找到了学习到的速度场的紧凑特性，并描述了生成流的近似，该近似将基本高斯密度推向目标密度。我们还提供了生成混合物均值与目标混合物均值之间距离的闭式公式，并证明其衰减速度为$\Theta_n(\frac{1}{n})$，这实际上是贝叶斯最优的。

    

    我们研究训练一个由两层自编码器参数化的流式生成模型，以从高维高斯混合模型中抽样的问题。我们对这个问题进行了尖锐的端到端分析。首先，我们提供了一个紧密的闭式特征化学习到的速度场，当参数化为一个在目标分布上从有限数量的样本$ n $中进行训练的浅层去噪自编码器时。在此分析的基础上，我们提供了对应的生成流的尖锐描述，将基本高斯密度推向目标密度的近似。特别地，我们提供了生成混合物的均值与目标混合物均值之间的距离的闭式公式，我们证明这个距离会衰减为$\Theta_n(\frac{1}{n})$。最后，这个速率被证明实际上是贝叶斯最优的。

    We study the problem of training a flow-based generative model, parametrized by a two-layer autoencoder, to sample from a high-dimensional Gaussian mixture. We provide a sharp end-to-end analysis of the problem. First, we provide a tight closed-form characterization of the learnt velocity field, when parametrized by a shallow denoising auto-encoder trained on a finite number $n$ of samples from the target distribution. Building on this analysis, we provide a sharp description of the corresponding generative flow, which pushes the base Gaussian density forward to an approximation of the target density. In particular, we provide closed-form formulae for the distance between the mean of the generated mixture and the mean of the target mixture, which we show decays as $\Theta_n(\frac{1}{n})$. Finally, this rate is shown to be in fact Bayes-optimal.
    
[^35]: 多保真神经网络计算的残差方法

    Residual Multi-Fidelity Neural Network Computing. (arXiv:2310.03572v1 [cs.LG])

    [http://arxiv.org/abs/2310.03572](http://arxiv.org/abs/2310.03572)

    本研究提出了一种残差多保真计算框架，通过使用多保真信息构建神经网络代理模型，解决了低保真和高保真计算模型之间的相关性建模问题。这种方法训练了两个神经网络，利用残差函数进行模型训练，最终得到了高保真替代模型。

    

    在本研究中，我们考虑使用多保真信息构建神经网络代理模型的一般问题。给定一个廉价的低保真和一个昂贵的高保真计算模型，我们提出了一个残差多保真计算框架，将模型之间的相关性建模为一个残差函数，这是一个可能非线性的1）模型共享的输入空间和低保真模型输出之间的映射，以及2）两个模型输出之间的差异。为了实现这一点，我们训练了两个神经网络来协同工作。第一个网络在少量的高保真和低保真数据上学习残差函数。一旦训练完成，这个网络被用来生成额外的合成高保真数据，用于训练第二个网络。一旦训练完成，第二个网络作为我们对高保真感兴趣的量的替代模型。我们提供了三个数值例子来证明这种方法的能力。

    In this work, we consider the general problem of constructing a neural network surrogate model using multi-fidelity information. Given an inexpensive low-fidelity and an expensive high-fidelity computational model, we present a residual multi-fidelity computational framework that formulates the correlation between models as a residual function, a possibly non-linear mapping between 1) the shared input space of the models together with the low-fidelity model output and 2) the discrepancy between the two model outputs. To accomplish this, we train two neural networks to work in concert. The first network learns the residual function on a small set of high-fidelity and low-fidelity data. Once trained, this network is used to generate additional synthetic high-fidelity data, which is used in the training of a second network. This second network, once trained, acts as our surrogate for the high-fidelity quantity of interest. We present three numerical examples to demonstrate the power of th
    
[^36]: BID-NeRF：具有反向神经辐射场的RGB-D图像姿态估计

    BID-NeRF: RGB-D image pose estimation with inverted Neural Radiance Fields. (arXiv:2310.03563v1 [cs.CV])

    [http://arxiv.org/abs/2310.03563](http://arxiv.org/abs/2310.03563)

    BID-NeRF算法改进了图像姿态估计问题，并提出了几种改进：引入基于深度的损失函数和多图像损失函数，省略分层采样过程，通过扩展采样间隔实现更高的初始姿态估计误差的收敛。这些修改显著提高了收敛速度和收敛基准。

    

    我们旨在改进反向神经辐射场（iNeRF）算法，将图像姿态估计问题定义为基于NeRF的迭代线性优化。NeRF是一种新颖的神经空间表示模型，可以合成真实场景或物体的逼真新视图。我们的贡献如下：我们在定位优化目标中引入了基于深度的损失函数，我们引入了基于多图像的损失函数，其中使用已知相对姿态的图像序列，而不增加计算复杂性，我们在体积渲染过程中省略了分层采样，意味着只使用粗糙模型进行姿态估计，并且我们展示通过扩展采样间隔可以实现甚至更高的初始姿态估计误差的收敛。通过提出的修改，收敛速度显著提高，并且收敛基准显著扩展。

    We aim to improve the Inverted Neural Radiance Fields (iNeRF) algorithm which defines the image pose estimation problem as a NeRF based iterative linear optimization. NeRFs are novel neural space representation models that can synthesize photorealistic novel views of real-world scenes or objects. Our contributions are as follows: we extend the localization optimization objective with a depth-based loss function, we introduce a multi-image based loss function where a sequence of images with known relative poses are used without increasing the computational complexity, we omit hierarchical sampling during volumetric rendering, meaning only the coarse model is used for pose estimation, and we how that by extending the sampling interval convergence can be achieved even or higher initial pose estimate errors. With the proposed modifications the convergence speed is significantly improved, and the basin of convergence is substantially extended.
    
[^37]: 使用Leave-One-Out最大对数似然目标稳定训练概率模型

    Stable Training of Probabilistic Models Using the Leave-One-Out Maximum Log-Likelihood Objective. (arXiv:2310.03556v1 [stat.ML])

    [http://arxiv.org/abs/2310.03556](http://arxiv.org/abs/2310.03556)

    本文介绍了一种使用Leave-One-Out最大对数似然目标稳定训练概率模型的方法，通过自适应核密度估计模型和留一法最大对数似然准则，解决了数据密度不均匀困难，并通过分配可学习权重扩展模型，加速了训练过程。

    

    电力系统运行和规划过程的概率建模依赖于数据驱动方法，这需要足够大的数据集。当历史数据不足时，希望将潜在的数据生成机制建模为概率分布，以评估数据质量并生成更多数据。基于核密度估计（KDE）的模型是这一任务的常用选择，但它们无法适应密度不均匀的数据区域。在本文中，采用自适应KDE模型来解决这个问题，模型中的每个核函数具有独立的带宽。提出了一种留一法最大对数似然（LOO-MLL）准则，以防止常规的最大对数似然准则产生奇异解，并证明LOO-MLL可以防止这种情况。在此保证的鲁棒性基础上，通过为核函数分配可学习权重扩展了模型。此外，使用改进的期望最大化算法来加速训练过程。

    Probabilistic modelling of power systems operation and planning processes depends on data-driven methods, which require sufficiently large datasets. When historical data lacks this, it is desired to model the underlying data generation mechanism as a probability distribution to assess the data quality and generate more data, if needed. Kernel density estimation (KDE) based models are popular choices for this task, but they fail to adapt to data regions with varying densities. In this paper, an adaptive KDE model is employed to circumvent this, where each kernel in the model has an individual bandwidth. The leave-one-out maximum log-likelihood (LOO-MLL) criterion is proposed to prevent the singular solutions that the regular MLL criterion gives rise to, and it is proven that LOO-MLL prevents these. Relying on this guaranteed robustness, the model is extended by assigning learnable weights to the kernels. In addition, a modified expectation-maximization algorithm is employed to accelerat
    
[^38]: 插拔式后验采样在不匹配测量和先验模型下的应用

    Plug-and-Play Posterior Sampling under Mismatched Measurement and Prior Models. (arXiv:2310.03546v1 [stat.ML])

    [http://arxiv.org/abs/2310.03546](http://arxiv.org/abs/2310.03546)

    本研究提出了一种插拔式后验采样算法（PnP-ULA），通过将物理测量模型与深度学习先验相结合，解决了成像逆问题。我们通过理论分析和数值验证，量化了PnP-ULA在不匹配后验分布下的误差界限，结果表明PnP-ULA对于测量模型和去噪器的不匹配非常敏感。

    

    后验采样已被证明是解决成像逆问题的强大贝叶斯方法。最近发展起来的插拔式未调整朗之万算法（PnP-ULA）通过将物理测量模型与使用图像去噪器指定的深度学习先验相结合，成为一种有前景的蒙特卡洛采样和最小均方误差（MMSE）估计方法。然而，PnP-ULA的采样分布与不匹配的数据保真度和去噪器之间的复杂关系尚未经过理论分析。我们通过提出一种后验-L2拟度量并利用它来量化PnP-ULA在不匹配的后验分布下的显式误差界限来填补这一空白。我们在多个逆问题上对我们的理论进行了数值验证，如从高斯混合模型和图像去模糊中采样。我们的结果表明，PnP-ULA的采样分布对于测量模型和去噪器的不匹配非常敏感，并可以精确地描述其特征。

    Posterior sampling has been shown to be a powerful Bayesian approach for solving imaging inverse problems. The recent plug-and-play unadjusted Langevin algorithm (PnP-ULA) has emerged as a promising method for Monte Carlo sampling and minimum mean squared error (MMSE) estimation by combining physical measurement models with deep-learning priors specified using image denoisers. However, the intricate relationship between the sampling distribution of PnP-ULA and the mismatched data-fidelity and denoiser has not been theoretically analyzed. We address this gap by proposing a posterior-L2 pseudometric and using it to quantify an explicit error bound for PnP-ULA under mismatched posterior distribution. We numerically validate our theory on several inverse problems such as sampling from Gaussian mixture models and image deblurring. Our results suggest that the sensitivity of the sampling distribution of PnP-ULA to a mismatch in the measurement model and the denoiser can be precisely characte
    
[^39]: 无分布风险评估的基于回归的机器学习算法

    Distribution-free risk assessment of regression-based machine learning algorithms. (arXiv:2310.03545v1 [cs.LG])

    [http://arxiv.org/abs/2310.03545](http://arxiv.org/abs/2310.03545)

    这项研究解决了在机器学习算法中计算预测模型失效概率的风险评估任务，使用符合性预测方法进行预测区间计算，并证明了其保守性质。

    

    机器学习算法在多年来变得越来越复杂，并被越来越多地应用于实际应用中。然而，在实际环境中使用机器学习技术，尤其是在医学和工程等高风险应用中，获取预测模型的失效概率至关重要。我们将这个问题称为风险评估任务。我们主要关注回归算法，并解决计算模型预测周围定义的区间内真实标签概率的风险评估任务。我们使用符合性预测方法来解决风险评估问题，该方法提供的预测区间保证以给定概率包含真实标签。利用这种区间覆盖性质，我们证明了我们的近似失效概率是保守的，即它不比ML算法的真实失效概率更低。我们进行了大量实验以经验性地研究...

    Machine learning algorithms have grown in sophistication over the years and are increasingly deployed for real-life applications. However, when using machine learning techniques in practical settings, particularly in high-risk applications such as medicine and engineering, obtaining the failure probability of the predictive model is critical. We refer to this problem as the risk-assessment task. We focus on regression algorithms and the risk-assessment task of computing the probability of the true label lying inside an interval defined around the model's prediction. We solve the risk-assessment problem using the conformal prediction approach, which provides prediction intervals that are guaranteed to contain the true label with a given probability. Using this coverage property, we prove that our approximated failure probability is conservative in the sense that it is not lower than the true failure probability of the ML algorithm. We conduct extensive experiments to empirically study t
    
[^40]: 在数据-参数域上，联合群不变函数引导了通用神经网络

    Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks. (arXiv:2310.03530v1 [cs.LG])

    [http://arxiv.org/abs/2310.03530](http://arxiv.org/abs/2310.03530)

    本研究通过探索联合群不变函数在数据-参数域上的作用，提出了一种系统的规则来解码神经网络内部数据表示中的对称性和几何性。利用这一规则，我们引入了由联合不变函数导出的通用神经网络，并利用群论证明了其普适性。这一研究揭示了逼近理论和深度学习中的群论方面，并将几何深度学习与抽象调和分析相连接。

    

    将输入数据的对称性和几何性考虑为编码在神经网络内部数据表示中，但是具体的编码规则还没有得到深入研究。通过关注数据-参数域上的联合群不变函数，我们提出了一种系统的规则，从数据域上的群作用中找到参数域上的双重群作用。此外，我们引入了由联合不变函数导出的广义神经网络，并利用Schur引理给出了它们的普遍性定理的新的群论证明。由于传统的普遍性定理是基于函数分析方法进行证明的，这项研究揭示了逼近理论的群论方面，将几何深度学习与抽象调和分析相连接。

    The symmetry and geometry of input data are considered to be encoded in the internal data representation inside the neural network, but the specific encoding rule has been less investigated. By focusing on a joint group invariant function on the data-parameter domain, we present a systematic rule to find a dual group action on the parameter domain from a group action on the data domain. Further, we introduce generalized neural networks induced from the joint invariant functions, and present a new group theoretic proof of their universality theorems by using Schur's lemma. Since traditional universality theorems were demonstrated based on functional analytical methods, this study sheds light on the group theoretic aspect of the approximation theory, connecting geometric deep learning to abstract harmonic analysis.
    
[^41]: 深度脊波变换：使用Koopman算子证明了形式深度网络的普适性

    Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks. (arXiv:2310.03529v1 [cs.LG])

    [http://arxiv.org/abs/2310.03529](http://arxiv.org/abs/2310.03529)

    通过对数据空间上的群作用来识别DNN内部的隐藏层，并将DNN构建为相对于Koopman算子的双声变换，我们利用群论论证证明了这些DNN的普适性。

    

    我们通过对数据空间上的群作用来识别DNN内部的隐藏层，并将DNN构建为相对于Koopman算子的双声变换，Koopman算子是群作用的线性表示。基于群论论证，特别是利用Schur引理，我们给出了这些DNN普适性的简单证明。

    We identify hidden layers inside a DNN with group actions on the data space, and formulate the DNN as a dual voice transform with respect to Koopman operator, a linear representation of the group action. Based on the group theoretic arguments, particularly by using Schur's lemma, we show a simple proof of the universality of those DNNs.
    
[^42]: 高维贝叶斯优化与组测试

    High-dimensional Bayesian Optimization with Group Testing. (arXiv:2310.03515v1 [cs.LG])

    [http://arxiv.org/abs/2310.03515](http://arxiv.org/abs/2310.03515)

    本研究提出了一种高维贝叶斯优化方法，通过组测试来识别活动变量，以实现高效优化。该方法在多个合成和真实高维优化任务上与最先进的方法相竞争。

    

    贝叶斯优化是一种优化昂贵的黑箱函数的有效方法。高维问题特别具有挑战性，因为目标的替代模型受到维度灾难的影响，很难进行准确建模。我们提出了一种组测试方法来识别活动变量，以便在这些领域中实现高效优化。所提出的算法，称为组测试贝叶斯优化（GTBO），首先运行一个测试阶段，在这个阶段，系统地选择一组变量，并测试它们对目标的影响。为此，我们将广为人知的组测试理论扩展到连续范围函数。在第二阶段，GTBO通过对活动维度给予更多重视来指导优化。通过利用轴对齐子空间假设，GTBO在几个合成和真实高维优化任务上与最先进的方法相竞争。

    Bayesian optimization is an effective method for optimizing expensive-to-evaluate black-box functions. High-dimensional problems are particularly challenging as the surrogate model of the objective suffers from the curse of dimensionality, which makes accurate modeling difficult. We propose a group testing approach to identify active variables to facilitate efficient optimization in these domains. The proposed algorithm, Group Testing Bayesian Optimization (GTBO), first runs a testing phase where groups of variables are systematically selected and tested on whether they influence the objective. To that end, we extend the well-established theory of group testing to functions of continuous ranges. In the second phase, GTBO guides optimization by placing more importance on the active dimensions. By exploiting the axis-aligned subspace assumption, GTBO is competitive against state-of-the-art methods on several synthetic and real-world high-dimensional optimization tasks. Furthermore, GTBO 
    
[^43]: 用单一IMU和分层机器学习模型监测年长者的奥塔哥锻炼

    Otago Exercises Monitoring for Older Adults by a Single IMU and Hierarchical Machine Learning Models. (arXiv:2310.03512v1 [cs.LG])

    [http://arxiv.org/abs/2310.03512](http://arxiv.org/abs/2310.03512)

    本研究提出了一种使用单一IMU和分层机器学习模型监测年长者奥塔哥锻炼的准确系统。利用深度学习模型判断患者是在进行OEP还是日常生活活动，可以监测OEP的参与情况。

    

    奥塔哥锻炼计划(OEP)是一种用于改善年长者脆弱、肌少症和平衡的康复计划。准确地监测患者参与OEP的情况具有挑战性，因为自我报告(日记)通常不可靠。随着可穿戴传感器的发展，利用可穿戴传感器进行人体活动识别(HAR)的系统已经改变了医疗保健。然而，它们在OEP的使用仍然表现出有限的性能。本研究旨在构建一个不显眼且准确的系统，以监测年长者参与OEP的情况。数据是从佩戴单个腰部IMU的年长者身上收集的。收集了两组数据，一组是在实验室环境中，一组是在患者家中。提出了一个分层系统，分为两个阶段：1)使用深度学习模型，利用10分钟的滑动窗口识别患者是在进行OEP还是日常生活活动(ADLs)；2)基于第一阶段，使用6秒的滑动窗口重新识别OEP与ADLs。

    Otago Exercise Program (OEP) is a rehabilitation program for older adults to improve frailty, sarcopenia, and balance. Accurate monitoring of patient involvement in OEP is challenging, as self-reports (diaries) are often unreliable. With the development of wearable sensors, Human Activity Recognition (HAR) systems using wearable sensors have revolutionized healthcare. However, their usage for OEP still shows limited performance. The objective of this study is to build an unobtrusive and accurate system to monitor OEP for older adults. Data was collected from older adults wearing a single waist-mounted Inertial Measurement Unit (IMU). Two datasets were collected, one in a laboratory setting, and one at the homes of the patients. A hierarchical system is proposed with two stages: 1) using a deep learning model to recognize whether the patients are performing OEP or activities of daily life (ADLs) using a 10-minute sliding window; 2) based on stage 1, using a 6-second sliding window to re
    
[^44]: 音乐期望的深度生成模型

    Deep Generative Models of Music Expectation. (arXiv:2310.03500v1 [cs.SD])

    [http://arxiv.org/abs/2310.03500](http://arxiv.org/abs/2310.03500)

    本文使用深度生成模型，通过学习复杂的非线性特征，计算音乐输入序列的近似概率，以更好地预测音乐期望。

    

    乐曲的惊喜和期望是情感反应的一种重要理论。以往的研究将这一思想转化为音乐的概率模型，可以根据先前的音乐经验计算出音乐的概率。然而，到目前为止，这些模型仅限于通过手工特征计算准确概率，或者仅限于线性模型，这可能无法表达音乐中复杂的条件分布。本文提出使用现代深度概率生成模型——扩散模型，来计算音乐输入序列的近似概率。这种由深度神经网络参数化的生成模型能够直接从训练集中学习复杂的非线性特征。我们期望通过这样的模型，能够更好地预测音乐期望。

    A prominent theory of affective response to music revolves around the concepts of surprisal and expectation. In prior work, this idea has been operationalized in the form of probabilistic models of music which allow for precise computation of song (or note-by-note) probabilities, conditioned on a 'training set' of prior musical or cultural experiences. To date, however, these models have been limited to compute exact probabilities through hand-crafted features or restricted to linear models which are likely not sufficient to represent the complex conditional distributions present in music. In this work, we propose to use modern deep probabilistic generative models in the form of a Diffusion Model to compute an approximate likelihood of a musical input sequence. Unlike prior work, such a generative model parameterized by deep neural networks is able to learn complex non-linear features directly from a training set itself. In doing so, we expect to find that such models are able to more 
    
[^45]: 如何水平采样过程影响深度强化学习中的零样本泛化

    How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v1 [cs.LG])

    [http://arxiv.org/abs/2310.03494](http://arxiv.org/abs/2310.03494)

    这项研究探讨了非均匀采样策略对深度强化学习代理的零样本泛化能力的影响，通过测量代理的内部表示与训练层级集之间的相互信息，发现基于值损失优先级的自适应采样策略能更好地避免过拟合。

    

    阻止广泛采用通过深度强化学习（RL）训练的自主代理代理的关键局限是它们有限的适应新环境能力，即使这些环境与训练中遇到的环境具有相似特征。本研究探讨了个体环境实例或层级的非均匀采样策略如何影响RL代理的零样本泛化（ZSG）能力，并考虑了两种失效模式：过拟合和过度泛化。首先，我们测量了代理的内部表示与训练层级集之间的相互信息（MI），发现MI与实例的过拟合相关性很强。与均匀采样相比，基于值损失优先级的自适应采样策略更能有效地保持较低的MI，这为这类技术提供了一种新的理论解释。接下来，我们将注意力转向无监督环境设计（U）

    A key limitation preventing the wider adoption of autonomous agents trained via deep reinforcement learning (RL) is their limited ability to generalise to new environments, even when these share similar characteristics with environments encountered during training. In this work, we investigate how a non-uniform sampling strategy of individual environment instances, or levels, affects the zero-shot generalisation (ZSG) ability of RL agents, considering two failure modes: overfitting and over-generalisation. As a first step, we measure the mutual information (MI) between the agent's internal representation and the set of training levels, which we find to be well-correlated to instance overfitting. In contrast to uniform sampling, adaptive sampling strategies prioritising levels based on their value loss are more effective at maintaining lower MI, which provides a novel theoretical justification for this class of techniques. We then turn our attention to unsupervised environment design (U
    
[^46]: TPDR：一种新颖的基于双步骤Transformer的产品和类描述匹配与检索方法

    TPDR: A Novel Two-Step Transformer-based Product and Class Description Match and Retrieval Method. (arXiv:2310.03491v1 [cs.IR])

    [http://arxiv.org/abs/2310.03491](http://arxiv.org/abs/2310.03491)

    TPDR是一种基于Transformer的产品和类描述匹配与检索方法，通过注意机制和对比学习来实现语义对应关系的探索。

    

    有一类公司负责为其他公司中介采购大批量的各种产品，其主要挑战是进行产品描述的标准化，即将客户描述的商品与目录中描述的产品进行匹配。这个问题非常复杂，因为客户的产品描述可能存在以下情况：（1）潜在的噪声；（2）短小且不具备信息（例如，缺少有关型号和尺寸的信息）；（3）跨语言。本文将这个问题形式化为一个排序任务：给定一个初始的客户产品规格（查询），返回最合适的标准化描述（响应）。本文提出了TPDR，一种基于双步骤Transformer的产品和类描述检索方法，该方法能够利用注意机制和对比学习来探索IS和SD之间的语义对应关系。首先，TPDR使用两个编码器的transformers共享嵌入向量空间：

    There is a niche of companies responsible for intermediating the purchase of large batches of varied products for other companies, for which the main challenge is to perform product description standardization, i.e., matching an item described by a client with a product described in a catalog. The problem is complex since the client's product description may be: (1) potentially noisy; (2) short and uninformative (e.g., missing information about model and size); and (3) cross-language. In this paper, we formalize this problem as a ranking task: given an initial client product specification (query), return the most appropriate standardized descriptions (response). In this paper, we propose TPDR, a two-step Transformer-based Product and Class Description Retrieval method that is able to explore the semantic correspondence between IS and SD, by exploiting attention mechanisms and contrastive learning. First, TPDR employs the transformers as two encoders sharing the embedding vector space: 
    
[^47]: BTDNet:一种用于脑肿瘤放射基因组分类的多模态方法

    BTDNet: a Multi-Modal Approach for Brain Tumor Radiogenomic Classification. (arXiv:2310.03485v1 [eess.IV])

    [http://arxiv.org/abs/2310.03485](http://arxiv.org/abs/2310.03485)

    本论文提出了一种名为BTDNet的多模态方法，利用多参量MRI扫描预测MGMT启动子甲基化状态。BTDNet解决了可变的体积长度和体积级注释的挑战，并在数据增强、3D分析、特征提取和切片级分类方面进行了优化。

    

    脑肿瘤在全球范围内带来了重大的健康挑战，其中胶质母细胞瘤是最具侵袭性的形式之一。准确确定O6-甲基鸟嘌呤-DNA甲基转移酶(MGMT)启动子甲基化状态对于个体化治疗策略至关重要。然而，传统方法费时费力。本文提出了一种新的多模态方法，BTDNet，利用多参量MRI扫描，包括FLAIR、T1w、T1wCE和T2 3D体积，预测MGMT启动子甲基化状态。BTDNet解决了两个主要挑战：可变的体积长度（即每个体积包含不同数量的切片）和体积级注释（即整个3D体积被注释，而不是它包含的独立切片）。BTDNet由四个组成部分组成：i）数据增强（执行几何变换，数据对的凸组合和测试时数据增强）；ii）3D分析（执行全局特征提取和分类）；iii）特征提取（利用注意力机制提取切片级特征）；iv）切片级分类（使用注意力机制进行切片级分类）。

    Brain tumors pose significant health challenges worldwide, with glioblastoma being one of the most aggressive forms. Accurate determination of the O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation status is crucial for personalized treatment strategies. However, traditional methods are labor-intensive and time-consuming. This paper proposes a novel multi-modal approach, BTDNet, leveraging multi-parametric MRI scans, including FLAIR, T1w, T1wCE, and T2 3D volumes, to predict MGMT promoter methylation status. BTDNet addresses two main challenges: the variable volume lengths (i.e., each volume consists of a different number of slices) and the volume-level annotations (i.e., the whole 3D volume is annotated and not the independent slices that it consists of). BTDNet consists of four components: i) the data augmentation one (that performs geometric transformations, convex combinations of data pairs and test-time data augmentation); ii) the 3D analysis one (that performs glo
    
[^48]: 完全连接的ReLU层的几何结构

    The Geometric Structure of Fully-Connected ReLU-Layers. (arXiv:2310.03482v1 [cs.LG])

    [http://arxiv.org/abs/2310.03482](http://arxiv.org/abs/2310.03482)

    该论文研究了神经网络中完全连接的ReLU层的几何结构。研究发现，在每个划分区域内，ReLU层可以被大大简化，可以将其解释为一个投影到多面体锥体，然后进行仿射变换。此结构还简化了分区区域与超平面交集的反像表达式，对于描述分类问题中的决策边界非常有用。此外，对于具有一个隐藏ReLU层的前馈网络，论文提供了关于这些网络生成的决策边界几何复杂性的结果，并证明了这些决策边界在仿射变换的模下是相等的。

    

    我们对神经网络中d维完全连接的ReLU层的几何结构进行了形式化和解释。ReLU层的参数会引导输入域的自然划分，使得在划分的每个区域内，ReLU层可以被大大简化。这导致了将ReLU层解释为一个投影到多面体锥体，然后进行仿射变换的几何解释，与在具有ReLU激活的卷积网络中的描述一致。此外，这种结构便于简化分区区域与超平面交集的反像表达式，这在描述分类问题中的决策边界时非常有用。我们详细研究了具有一个隐藏ReLU层的前馈网络，在这个网络中，我们提供了关于这些网络生成的决策边界的几何复杂性的结果，同时证明在仿射变换的模下，这些决策边界相等。

    We formalize and interpret the geometric structure of $d$-dimensional fully connected ReLU-layers in neural networks. The parameters of a ReLU-layer induce a natural partition of the input domain, such that in each sector of the partition, the ReLU-layer can be greatly simplified. This leads to a geometric interpretation of a ReLU-layer as a projection onto a polyhedral cone followed by an affine transformation, in line with the description in [doi:10.48550/arXiv.1905.08922] for convolutional networks with ReLU activations. Further, this structure facilitates simplified expressions for preimages of the intersection between partition sectors and hyperplanes, which is useful when describing decision boundaries in a classification setting. We investigate this in detail for a feed-forward network with one hidden ReLU-layer, where we provide results on the geometric complexity of the decision boundary generated by such networks, as well as proving that modulo an affine transformation, such 
    
[^49]: Cadenza ICASSP 2024大挑战

    The Cadenza ICASSP 2024 Grand Challenge. (arXiv:2310.03480v1 [eess.AS])

    [http://arxiv.org/abs/2310.03480](http://arxiv.org/abs/2310.03480)

    Cadenza项目组织了ICASSP SP Cadenza Challenge，旨在通过音乐分解/混音来提升助听器音质，处理过程考虑音乐、增益和听力损失。

    

    Cadenza项目旨在提高听力受损人群的音乐音质。作为该项目的一部分，该项目组织了ICASSP SP Cadenza Challenge：面向助听器的音乐分解/混音。该挑战可以通过将音乐分解成人声、贝斯、鼓和其他组成部分来解决。然后可以以个性化的方式智能地进行混音，以提高音频质量。另外，还可以使用端到端的方法。处理过程需要考虑音乐本身、每个组成部分的增益以及听众的听力损失。提交的作品将使用Hearing Aid Audio Quality Index（HAAQI）这一入侵式客观指标进行评估。本文概述了该挑战。

    The Cadenza project aims to enhance the audio quality of music for individuals with hearing loss. As part of this, the project is organizing the ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing Aids. The challenge can be tackled by decomposing the music at the hearing aid microphones into vocals, bass, drums, and other components. These can then be intelligently remixed in a personalized manner to improve audio quality. Alternatively, an end-to-end approach could be used. Processes need to consider the music itself, the gain applied to each component, and the listener's hearing loss. The submitted entries will be evaluated using the intrusive objective metric, the Hearing Aid Audio Quality Index (HAAQI). This paper outlines the challenge.
    
[^50]: 评估局部解释中的归因问题及其应对方法

    The Blame Problem in Evaluating Local Explanations, and How to Tackle it. (arXiv:2310.03466v1 [cs.LG])

    [http://arxiv.org/abs/2310.03466](http://arxiv.org/abs/2310.03466)

    最近局部模型无关解释技术的提出数量迅速增长，在评估这些技术时存在归因问题。我们提出了一种新的分类方法来评估局部解释，并强调除了基于可解释模型的基准真实数据之外的所有评估方法都遭受了这个问题。我们认为基于可解释模型的评估方法是更合理的方法。

    

    最近局部模型无关解释技术的提出数量迅速增长。主要原因之一是由于缺乏最优评估指标，开发新的解释技术的门槛较低。没有严格的评估指标，很难有确凿的证据证明新的解释技术能否明显优于其前人。我们的研究提出了一种新的分类方法来评估局部解释：鲁棒性、使用合成数据集和可解释的模型进行评估、模型随机化以及人类参与评估。使用这种提出的分类方法，我们强调除了基于可解释模型的基准真实数据之外的所有评估方法都遭受了我们称之为“归因问题”的问题。在我们的研究中，我们认为这种评估方法是一种更合理、更能评估局部模型无关解释的方法。然而，我们展示了即使是这种评估方法也存在一些挑战。

    The number of local model-agnostic explanation techniques proposed has grown rapidly recently. One main reason is that the bar for developing new explainability techniques is low due to the lack of optimal evaluation measures. Without rigorous measures, it is hard to have concrete evidence of whether the new explanation techniques can significantly outperform their predecessors. Our study proposes a new taxonomy for evaluating local explanations: robustness, evaluation using ground truth from synthetic datasets and interpretable models, model randomization, and human-grounded evaluation. Using this proposed taxonomy, we highlight that all categories of evaluation methods, except those based on the ground truth from interpretable models, suffer from a problem we call the "blame problem." In our study, we argue that this category of evaluation measure is a more reasonable method for evaluating local model-agnostic explanations. However, we show that even this category of evaluation measu
    
[^51]: 哪种模式更适合联合学习？中央化还是分散化。

    Which mode is better for federated learning? Centralized or Decentralized. (arXiv:2310.03461v1 [cs.LG])

    [http://arxiv.org/abs/2310.03461](http://arxiv.org/abs/2310.03461)

    中国总结出的一句话要点：研究发现在联合学习中，集中化的方法总是比分散化的方法更好地进行泛化，同时，部分参与在集中化方法中表现更好，而在分散化方法中，拓扑结构对性能的影响十分重要。

    

    在联合学习（FL）中，集中化和分散化方法都表现出了出色的性能和重要的应用价值。然而，目前的研究并没有提供足够的证据来表明哪种方法表现更好。虽然从优化的角度来看，分散化的方法可以通过较少的通信实现与集中化方法相比较的收敛性，但在实证研究中，它的测试性能始终效率低下。为了全面探索它们在联合学习中的行为，我们研究了它们的过度风险，包括优化和泛化的联合分析。我们证明了在光滑非凸目标上，1）集中化的FL（CFL）总是比分散化的FL（DFL）更好地进行泛化；2）从CFL的过度风险和测试误差的角度来看，部分参与比全参与更好；3）在DFL中，为了避免随着训练规模增加而性能崩溃，拓扑结构有必要满足一定的要求。基于一些实证结果

    Both centralized and decentralized approaches have shown excellent performance and great application value in federated learning (FL). However, current studies do not provide sufficient evidence to show which one performs better. Although from the optimization perspective, decentralized methods can approach the comparable convergence of centralized methods with less communication, its test performance has always been inefficient in empirical studies. To comprehensively explore their behaviors in FL, we study their excess risks, including the joint analysis of both optimization and generalization. We prove that on smooth non-convex objectives, 1) centralized FL (CFL) always generalizes better than decentralized FL (DFL); 2) from perspectives of the excess risk and test error in CFL, adopting partial participation is superior to full participation; and, 3) there is a necessary requirement for the topology in DFL to avoid performance collapse as the training scale increases. Based on some
    
[^52]: 多分辨率音频-视觉特征融合用于时域动作定位

    Multi-Resolution Audio-Visual Feature Fusion for Temporal Action Localization. (arXiv:2310.03456v1 [cs.CV])

    [http://arxiv.org/abs/2310.03456](http://arxiv.org/abs/2310.03456)

    本论文提出了一种多分辨率音频-视觉特征融合的方法，通过分层门控交叉注意力机制将不同时间分辨率的音频-视觉数据进行合并，在时域动作定位任务中有效地整合了音频特征，并取得了显著的性能提升。

    

    时域动作定位（TAL）旨在在未剪辑视频中识别动作的开始、结束和类别标签。虽然最近使用Transformer网络和特征金字塔网络（FPN）在TAL任务中增强了视觉特征识别，但在将音频特征整合到此类框架中方面取得的进展较少。本文介绍了多分辨率音频-视觉特征融合（MRAV-FF），这是一种将不同时间分辨率的音频-视觉数据合并的创新方法。我们方法的核心是一种分层门控交叉注意力机制，可以分辨地权衡不同时间尺度上音频信息的重要性。这种技术不仅可以提高回归边界的精度，还可以增强分类的置信度。重要的是，MRAV-FF具有通用性，它与现有的FPN TAL架构兼容，并在音频数据可用时提供了显著的性能提升。

    Temporal Action Localization (TAL) aims to identify actions' start, end, and class labels in untrimmed videos. While recent advancements using transformer networks and Feature Pyramid Networks (FPN) have enhanced visual feature recognition in TAL tasks, less progress has been made in the integration of audio features into such frameworks. This paper introduces the Multi-Resolution Audio-Visual Feature Fusion (MRAV-FF), an innovative method to merge audio-visual data across different temporal resolutions. Central to our approach is a hierarchical gated cross-attention mechanism, which discerningly weighs the importance of audio information at diverse temporal scales. Such a technique not only refines the precision of regression boundaries but also bolsters classification confidence. Importantly, MRAV-FF is versatile, making it compatible with existing FPN TAL architectures and offering a significant enhancement in performance when audio data is available.
    
[^53]: FLAIM: 在联邦设置中基于AIM的合成数据生成

    FLAIM: AIM-based Synthetic Data Generation in the Federated Setting. (arXiv:2310.03447v1 [cs.CR])

    [http://arxiv.org/abs/2310.03447](http://arxiv.org/abs/2310.03447)

    FLAIM是一个在联邦设置中基于AIM的合成数据生成方法，该方法解决了差分隐私方向的技术在联邦场景下的适用问题，并提出了FLAIM方法来维持较高的效用和处理异构性。

    

    保护个人隐私同时实现协同数据共享对组织至关重要。合成数据生成是一种解决方案，它产生与私有数据的统计特性相似的人工数据。虽然在差分隐私下已经设计出了许多技术，但它们主要假设数据是集中的。然而，数据往往以联邦方式分布在多个客户端上。在这项工作中，我们开始研究联邦合成表数据生成。在AIM这个先进的中心方法的基础上，我们提出了DistAIM和FLAIM。我们展示了分发AIM是简单的，扩展了基于安全多方计算的最新方法，但需要额外的开销，使其在联邦场景中不太适用。然后，我们证明了简单地联邦AIM可能导致在异构性存在的情况下效用严重下降。为了解决这两个问题，我们提出了一种增强的FLAIM方法，该方法可以维持较高的效用，并且可以处理联邦设置中的异构性。

    Preserving individual privacy while enabling collaborative data sharing is crucial for organizations. Synthetic data generation is one solution, producing artificial data that mirrors the statistical properties of private data. While numerous techniques have been devised under differential privacy, they predominantly assume data is centralized. However, data is often distributed across multiple clients in a federated manner. In this work, we initiate the study of federated synthetic tabular data generation. Building upon a SOTA central method known as AIM, we present DistAIM and FLAIM. We show it is straightforward to distribute AIM, extending a recent approach based on secure multi-party computation which necessitates additional overhead, making it less suited to federated scenarios. We then demonstrate that naively federating AIM can lead to substantial degradation in utility under the presence of heterogeneity. To mitigate both issues, we propose an augmented FLAIM approach that mai
    
[^54]: GARCH家族模型的变分推断

    Variational Inference for GARCH-family Models. (arXiv:2310.03435v1 [stat.ML])

    [http://arxiv.org/abs/2310.03435](http://arxiv.org/abs/2310.03435)

    变分推断在GARCH家族模型的贝叶斯推断中是一种可靠和可行的方法。

    

    贝叶斯估计GARCH家族模型通常采用蒙特卡罗抽样方法。变分推断作为一种可靠的贝叶斯推断方法在复杂的机器学习模型中越来越受欢迎，但在计量经济学和金融领域的应用还有限。本文讨论了变分推断在GARCH类模型贝叶斯推断中是否是一种可靠和可行的替代蒙特卡罗抽样的方法。通过涉及标准普尔500指数成分股的大规模实验，采用多种变分推断优化算法和波动性模型，并进行了案例研究，我们证明了变分推断是一种有吸引力、非常良好校准和有竞争力的贝叶斯学习方法。

    The Bayesian estimation of GARCH-family models has been typically addressed through Monte Carlo sampling. Variational Inference is gaining popularity and attention as a robust approach for Bayesian inference in complex machine learning models; however, its adoption in econometrics and finance is limited. This paper discusses the extent to which Variational Inference constitutes a reliable and feasible alternative to Monte Carlo sampling for Bayesian inference in GARCH-like models. Through a large-scale experiment involving the constituents of the S&P 500 index, several Variational Inference optimizers, a variety of volatility models, and a case study, we show that Variational Inference is an attractive, remarkably well-calibrated, and competitive method for Bayesian learning.
    
[^55]: 神经语言模型修剪用于自动语音识别

    Neural Language Model Pruning for Automatic Speech Recognition. (arXiv:2310.03424v1 [cs.LG])

    [http://arxiv.org/abs/2310.03424](http://arxiv.org/abs/2310.03424)

    本文研究了应用于自动语音识别的基于Transformer的神经网络语言模型的模型修剪方法。通过对修剪框架的准则、方法和调度器进行分析，我们发现数据驱动的修剪在多个场景中优于幅度驱动的修剪，逐步修剪在准确度方面优于一次性修剪，并提出了适用于逐步压缩模型的低秩逼近方法，为中等压缩程度下的体积减小和推理加速提供了最佳平衡。

    

    我们研究了应用于基于Transformer的神经网络语言模型的模型修剪方法，用于自动语音识别。我们探索了修剪框架的三个方面，即准则、方法和调度器，并分析了它们在准确度和推理速度方面的贡献。据我们所知，关于大规模识别系统的这种深入分析在文献中还没有报道。此外，我们提出了一种适用于逐步压缩模型的低秩逼近的变体，并提供了多个具有不同目标大小的模型。在其他结果中，我们展示了：a) 数据驱动的修剪在几个场景下优于幅度驱动的修剪；b) 逐步修剪在准确度方面优于一次性修剪，尤其是针对较小的大小；c) 低秩逼近在中等压缩程度下，体积减小和推理加速之间取得了最佳的平衡。

    We study model pruning methods applied to Transformer-based neural network language models for automatic speech recognition. We explore three aspects of the pruning frame work, namely criterion, method and scheduler, analyzing their contribution in terms of accuracy and inference speed. To the best of our knowledge, such in-depth analyses on large-scale recognition systems has not been reported in the literature. In addition, we propose a variant of low-rank approximation suitable for incrementally compressing models, and delivering multiple models with varied target sizes. Among other results, we show that a) data-driven pruning outperforms magnitude-driven in several scenarios; b) incremental pruning achieves higher accuracy compared to one-shot pruning, especially when targeting smaller sizes; and c) low-rank approximation presents the best trade-off between size reduction and inference speed-up for moderate compression.
    
[^56]: 预训练和微调生成性流网络

    Pre-Training and Fine-Tuning Generative Flow Networks. (arXiv:2310.03419v1 [cs.LG])

    [http://arxiv.org/abs/2310.03419](http://arxiv.org/abs/2310.03419)

    这个论文提出了一种新的方法来实现生成性流网络的无奖励预训练，并通过自监督问题的形式训练了一个条件的GFlowNet（OC-GFN），用于有效适应下游任务。

    

    生成性流网络（GFlowNets）是学习从给定的非标准化奖励分布中顺序生成复合对象的随机策略的摊销采样器。它们可以生成多样化的高奖励对象，在科学发现任务中是一个重要考虑因素。然而，由于它们通常是根据给定的外在奖励函数进行训练的，关于如何利用预训练的力量以及以无监督的方式训练GFlowNets以实现对下游任务的高效自适应仍然是一个重要的挑战。受无监督预训练在各个领域的最新成功启发，我们引入了一种新的GFlowNets无奖励预训练的方法。通过将训练构建为一个自监督问题，我们提出了一种条件GFlowNet（OC-GFN），它学习探索候选空间。具体而言，OC-GFN学习达到任何目标结果，类似于强化学习中的目标条件策略。我们证明了

    Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution. They can generate diverse sets of high-reward objects, which is an important consideration in scientific discovery tasks. However, as they are typically trained from a given extrinsic reward function, it remains an important open challenge about how to leverage the power of pre-training and train GFlowNets in an unsupervised fashion for efficient adaptation to downstream tasks. Inspired by recent successes of unsupervised pre-training in various domains, we introduce a novel approach for reward-free pre-training of GFlowNets. By framing the training as a self-supervised problem, we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space. Specifically, OC-GFN learns to reach any targeted outcomes, akin to goal-conditioned policies in reinforcement learning. We show that
    
[^57]: 使用压缩感知的空中联合学习：稀疏化是否必要？

    Over-the-Air Federated Learning with Compressed Sensing: Is Sparsification Necessary?. (arXiv:2310.03410v1 [cs.IT])

    [http://arxiv.org/abs/2310.03410](http://arxiv.org/abs/2310.03410)

    本研究通过比较多种通信设计，发现在空中联合学习中压缩之前的稀疏化并不必要。

    

    空中联合学习（OtA FL）是指多个代理通过 OtA 计算将模型更新发送到公共边缘服务器的一种 FL 系统。OtA 计算的两个重要特性，即线性处理和信号级叠加，促使使用压缩感知（CS）方法进行线性压缩，以减少通过信道传输的数据样本数量。以前关于在 OtA FL 中应用 CS 方法的研究主要假设原始模型更新向量是稀疏的，或者在压缩之前进行了稀疏化。但是，尚不清楚在线性压缩和基于 CS 的重构与直接发送稀疏化更新向量中的非零元素，在相同的总功率约束下，哪种方法更有效。在这项研究中，我们检查并比较了几种带有或不带有稀疏化的通信设计。我们的研究结果表明，压缩之前的稀疏化并不必要。替代的做法可以达到相同的效果。

    Over-the-Air (OtA) Federated Learning (FL) refers to an FL system where multiple agents apply OtA computation for transmitting model updates to a common edge server. Two important features of OtA computation, namely linear processing and signal-level superposition, motivate the use of linear compression with compressed sensing (CS) methods to reduce the number of data samples transmitted over the channel. The previous works on applying CS methods in OtA FL have primarily assumed that the original model update vectors are sparse, or they have been sparsified before compression. However, it is unclear whether linear compression with CS-based reconstruction is more effective than directly sending the non-zero elements in the sparsified update vectors, under the same total power constraint. In this study, we examine and compare several communication designs with or without sparsification. Our findings demonstrate that sparsification before compression is not necessary. Alternatively, spars
    
[^58]: RUSOpt:利用贝叶斯优化进行机器人超声探头的平面和垂直扫描标准化

    RUSOpt: Robotic UltraSound Probe Normalization with Bayesian Optimization for In-plane and Out-plane Scanning. (arXiv:2310.03406v1 [cs.RO])

    [http://arxiv.org/abs/2310.03406](http://arxiv.org/abs/2310.03406)

    提出了一种使用贝叶斯优化方法对机器人超声探头进行标准化的方法，通过自动调整探头与扫描表面的接触点的正交方向，提高了超声图像的质量。

    

    自主机器人超声系统面临的一个重要挑战是在不同患者上获取高质量的图像。机器化探头的适当定位对超声图像的质量起着至关重要的作用。为了解决这个挑战，我们提出了一种实现自动调整超声探头与扫描表面的接触点正交的样本高效方法，从而提高超声探头的声学耦合和图像质量。我们的方法利用基于贝叶斯优化的搜索在扫描表面上高效搜索标准化的探头定位。我们为贝叶斯优化制定了一个新颖的目标函数，利用接触力测量和基本力学原理来确定正交方向。我们进一步在贝叶斯优化中引入了正则化方案来处理嘈杂的目标函数。通过对尿膀胱进行实验评估了所提出策略的性能。

    The one of the significant challenges faced by autonomous robotic ultrasound systems is acquiring high-quality images across different patients. The proper orientation of the robotized probe plays a crucial role in governing the quality of ultrasound images. To address this challenge, we propose a sample-efficient method to automatically adjust the orientation of the ultrasound probe normal to the point of contact on the scanning surface, thereby improving the acoustic coupling of the probe and resulting image quality. Our method utilizes Bayesian Optimization (BO) based search on the scanning surface to efficiently search for the normalized probe orientation. We formulate a novel objective function for BO that leverages the contact force measurements and underlying mechanics to identify the normal. We further incorporate a regularization scheme in BO to handle the noisy objective function. The performance of the proposed strategy has been assessed through experiments on urinary bladde
    
[^59]: EAG-RS:一种基于解释性引导的ASD诊断的ROI选择框架的创新

    EAG-RS: A Novel Explainability-guided ROI-Selection Framework for ASD Diagnosis via Inter-regional Relation Learning. (arXiv:2310.03404v1 [cs.LG])

    [http://arxiv.org/abs/2310.03404](http://arxiv.org/abs/2310.03404)

    EAG-RS是一种创新的解释性引导的感兴趣区（ROI）选择框架，通过非线性高阶功能关联以及可解释的人工智能技术，实现了自闭症谱系障碍（ASD）的诊断，并解决了现有研究中的一些局限性。

    

    基于静息态功能磁共振成像（rs-fMRI）的深度学习模型已广泛应用于诊断脑部疾病，特别是自闭症谱系障碍（ASD）。现有研究利用rs-fMRI的功能连接性（FC），取得了显著的分类性能。然而，它们存在着一些重大局限，包括使用线性低阶FC作为模型输入时缺乏充分的信息，不考虑ASD患者的个体特征（即不同症状或不同严重程度的阶段），以及决策过程的非可解释性。为了弥补这些局限性，我们提出了一种新颖的解释性引导的感兴趣区（ROI）选择（EAG-RS）框架，通过利用一种可解释的人工智能技术，识别脑区之间的非线性高阶功能关联，并选择能够鉴别疾病的区域进行脑部疾病识别。

    Deep learning models based on resting-state functional magnetic resonance imaging (rs-fMRI) have been widely used to diagnose brain diseases, particularly autism spectrum disorder (ASD). Existing studies have leveraged the functional connectivity (FC) of rs-fMRI, achieving notable classification performance. However, they have significant limitations, including the lack of adequate information while using linear low-order FC as inputs to the model, not considering individual characteristics (i.e., different symptoms or varying stages of severity) among patients with ASD, and the non-explainability of the decision process. To cover these limitations, we propose a novel explainability-guided region of interest (ROI) selection (EAG-RS) framework that identifies non-linear high-order functional associations among brain regions by leveraging an explainable artificial intelligence technique and selects class-discriminative regions for brain disease identification. The proposed framework incl
    
[^60]: 将大型语言模型应用于内容审查：数据工程和监督微调中的陷阱

    Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. (arXiv:2310.03400v1 [cs.LG])

    [http://arxiv.org/abs/2310.03400](http://arxiv.org/abs/2310.03400)

    本文介绍了如何对LLM模型进行微调以实现内容审查的私下部署，讨论了引入原因的微调过程和直接分类任务的区别，并研究了利用更强大的LLM生成的原因对微调的影响。

    

    如今，数十亿人每天在互联网上进行沟通并表达自己的观点。不幸的是，并非所有这些表达都友好或合规，这使得内容审查成为一项不可或缺的任务。随着近年来大型语言模型（LLM）的成功发展，基于LLM的方法已成为处理各个领域任务的可行解决方案。然而，在内容审查领域，仍缺乏详细的工作系统地介绍实施细节。本文介绍了如何对LLM模型进行微调，以便可以私下部署用于内容审查。具体而言，我们讨论了在微调过程中是否应该引入原因，以及将其视为直接分类任务是否更好。我们还探讨了利用更强大的LLM生成的原因对私下部署模型进行微调的好处，以及在回答生成过程中不同处理方法的影响。

    Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. However, in the field of content moderation, there is still a lack of detailed work that systematically introduces implementation details. In this paper, we introduce how to fine-tune an LLM model that can be privately deployed for content moderation. Specifically, we discuss whether incorporating reasons during the fine-tuning process would be better or if it should be treated as a classification task directly. We also explore the benefits of utilizing reasons generated by more powerful LLMs for fine-tuning privately deployed models and the impact of different processing approaches when the answers 
    
[^61]: GRAPES: 学习用于可扩展图神经网络的图采样

    GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])

    [http://arxiv.org/abs/2310.03399](http://arxiv.org/abs/2310.03399)

    GRAPES是一种自适应图采样方法，通过学习识别在训练图神经网络分类器时具有影响力的节点集合，解决了可扩展图神经网络中的内存问题，并且在准确性和可扩展性方面表现出色。

    

    图神经网络（GNNs）通过以不同方式聚合周围信息来学习图中节点的表示。随着这些网络的加深，由于邻域尺寸的增加，它们的感受野呈指数增长，导致高内存消耗。图采样通过对图中节点进行抽样来解决GNNs中的内存问题。通过这种方式，GNNs可以扩展到更大的图。大多数采样方法专注于固定的采样启发式算法，这可能无法推广到不同的结构或任务。我们引入了GRAPES，一种自适应的图采样方法，该方法学习识别用于训练GNN分类器的一组具有影响力的节点。GRAPES使用GFlowNet来学习给定分类目标的节点采样概率。我们在几个小规模和大规模图基准上评估了GRAPES，并展示了其在准确性和可扩展性方面的有效性。与现有的采样方法相比，GRAPES即使在采样比例较低的情况下仍保持高准确性。

    Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
    
[^62]: 在Gromov-Wasserstein中插值聚类和降维之间

    Interpolating between Clustering and Dimensionality Reduction with Gromov-Wasserstein. (arXiv:2310.03398v1 [cs.LG])

    [http://arxiv.org/abs/2310.03398](http://arxiv.org/abs/2310.03398)

    本论文介绍了一种通过Gromov-Wasserstein可实现在聚类和降维之间插值的方法。我们通过解决半松弛的最优传输问题，计算输入和嵌入样本之间的对应关系，从而实现同时减少样本和特征数量的降维。我们展示了当嵌入的维度不受约束时，该方法可以提供具有竞争力的硬聚类。通过将降维和聚类融合为中间阶段，我们强调了该方法在总结真实数据方面的重要性，并在图像数据集上进行了可视化应用。

    

    我们提出了一种适用于现有降维目标的通用适应性方法，能够同时减少样本和特征数量。通过半松弛的Gromov-Wasserstein最优传输问题计算输入和嵌入样本之间的对应关系。当嵌入样本数量与输入样本数量相匹配时，我们的模型恢复了经典的流行降维模型。当嵌入的维度不受约束时，我们展示了最优传输方案提供了具有竞争力的硬聚类。我们强调了将降维和聚类融合为中间阶段以总结真实数据的重要性，并将我们的方法应用于可视化图像数据集。

    We present a versatile adaptation of existing dimensionality reduction (DR) objectives, enabling the simultaneous reduction of both sample and feature sizes. Correspondances between input and embedding samples are computed through a semi-relaxed Gromov-Wasserstein optimal transport (OT) problem. When the embedding sample size matches that of the input, our model recovers classical popular DR models. When the embedding's dimensionality is unconstrained, we show that the OT plan delivers a competitive hard clustering. We emphasize the importance of intermediate stages that blend DR and clustering for summarizing real data and apply our method to visualize datasets of images.
    
[^63]: 学习简化步态分析中的时空图

    Learning to Simplify Spatial-Temporal Graphs in Gait Analysis. (arXiv:2310.03396v1 [cs.CV])

    [http://arxiv.org/abs/2310.03396](http://arxiv.org/abs/2310.03396)

    本文提出了一种学习简化步态分析中的时空图的方法，用于步态基于性别估计。该方法的关键创新点是使用上游和下游模型来调整每个行走实例的邻接矩阵，从而提高了可解释性并保持了性能。通过实验证明，该方法在CASIA-B数据集上表现出良好的效果。

    

    步态分析利用独特的行走模式进行人物识别和多领域评估。在步态分析方法中，基于骨架的方法因其稳健且可解释的特征而显示出潜力。然而，这些方法通常依赖于基于人体解剖学的手工制作的时空图，忽略了数据集和任务的特殊性。本文提出了一种新的方法，用于简化步态基于性别估计的时空图表示，提高解释性而不损失性能。我们的方法使用了上游模型和下游模型，可以调整每个行走实例的邻接矩阵，从而消除了图的固定性。通过采用直通Gumbel-Softmax技巧，我们的模型可以端对端地进行训练。我们在CASIA-B数据集上验证了我们方法的有效性，得到的图形具有良好的可解释性和性能。

    Gait analysis leverages unique walking patterns for person identification and assessment across multiple domains. Among the methods used for gait analysis, skeleton-based approaches have shown promise due to their robust and interpretable features. However, these methods often rely on hand-crafted spatial-temporal graphs that are based on human anatomy disregarding the particularities of the dataset and task. This paper proposes a novel method to simplify the spatial-temporal graph representation for gait-based gender estimation, improving interpretability without losing performance. Our approach employs two models, an upstream and a downstream model, that can adjust the adjacency matrix for each walking instance, thereby removing the fixed nature of the graph. By employing the Straight-Through Gumbel-Softmax trick, our model is trainable end-to-end. We demonstrate the effectiveness of our approach on the CASIA-B dataset for gait-based gender estimation. The resulting graphs are interp
    
[^64]: 深度学习的不确定性量化方法用于解决高维回溯随机微分方程

    Uncertainty quantification for deep learning-based schemes for solving high-dimensional backward stochastic differential equations. (arXiv:2310.03393v1 [math.NA])

    [http://arxiv.org/abs/2310.03393](http://arxiv.org/abs/2310.03393)

    本文研究了一类基于深度学习的高维回溯随机微分方程数值方案的不确定性量化方法，并开发了一个高效估计近似解标准差的UQ模型。

    

    深度学习方法用于解决高维回溯随机微分方程的数值方案近年来引起了许多科学兴趣。虽然它们能够近似计算非常高维的回溯随机微分方程，但其可靠性尚未被研究和理解。在这项工作中，我们研究了一类基于深度学习的回溯随机微分方程方案的不确定性量化。具体而言，我们回顾了方案中涉及的不确定性来源，并通过数值方法研究了不同来源的影响。通常，通过对使用不同数据集多次运行算法得到的近似解的标准差（STD）进行计算来解决不确定性。这种方法在高维问题上计算成本相当昂贵。因此，我们开发了一个UQ模型，它能够仅使用一次算法运行来高效地估计近似解的标准差。该模型还能够估计近似解的平均值。

    Deep learning-based numerical schemes for solving high-dimensional backward stochastic differential equations (BSDEs) have recently raised plenty of scientific interest. While they enable numerical methods to approximate very high-dimensional BSDEs, their reliability has not been studied and is thus not understood. In this work, we study uncertainty quantification (UQ) for a class of deep learning-based BSDE schemes. More precisely, we review the sources of uncertainty involved in the schemes and numerically study the impact of different sources. Usually, the standard deviation (STD) of the approximate solutions obtained from multiple runs of the algorithm with different datasets is calculated to address the uncertainty. This approach is computationally quite expensive, especially for high-dimensional problems. Hence, we develop a UQ model that efficiently estimates the STD of the approximate solution using only a single run of the algorithm. The model also estimates the mean of the ap
    
[^65]: OpenPatch:一个用于超出分布检测的3D拼贴

    OpenPatch: a 3D patchwork for Out-Of-Distribution detectionpdf icon. (arXiv:2310.03388v1 [cs.CV])

    [http://arxiv.org/abs/2310.03388](http://arxiv.org/abs/2310.03388)

    OpenPatch是一个3D拼贴，基于大型预训练模型提取中间特征来描述每个已知类别的补丁表示，在超出分布检测中取得了进展。

    

    将深度学习模型从实验室环境迁移到开放世界，需要使它们能够处理未知条件。在一些应用中，部署过程中出现新的类别可能构成重大威胁，因此有效地检测它们至关重要。理想情况下，这种能力应该在需要时使用，而不需要在每个新任务中进行任何进一步的计算训练。过去几年中，超出分布检测引起了广泛关注，然而绝大多数研究都处理2D图像，忽视了现实世界中固有的3D特性，并经常混淆领域和语义的新颖性。在这项工作中，我们专注于后者，考虑由3D点云捕捉的物体几何结构，而不考虑特定领域。我们通过引入OpenPatch推动该领域的进展，它建立在一个大型预训练模型的基础上，简单地提取从其中间特征中描述每个已知类别的一组补丁表示。

    Moving deep learning models from the laboratory setting to the open world entails preparing them to handle unforeseen conditions. In several applications the occurrence of novel classes during deployment poses a significant threat, thus it is crucial to effectively detect them. Ideally, this skill should be used when needed without requiring any further computational training effort at every new task. Out-of-distribution detection has attracted significant attention in the last years, however the majority of the studies deal with 2D images ignoring the inherent 3D nature of the real-world and often confusing between domain and semantic novelty. In this work, we focus on the latter, considering the objects geometric structure captured by 3D point clouds regardless of the specific domain. We advance the field by introducing OpenPatch that builds on a large pre-trained model and simply extracts from its intermediate features a set of patch representations that describe each known class. F
    
[^66]: 机器学习耦合动力系统中的相互作用网络

    Machine learning the interaction network in coupled dynamical systems. (arXiv:2310.03378v1 [math.DS])

    [http://arxiv.org/abs/2310.03378](http://arxiv.org/abs/2310.03378)

    本研究使用自监督神经网络模型，从观察到的轨迹数据中恢复耦合动力系统的相互作用网络并预测个体代理的动态。

    

    相互作用动力系统的研究在科学和工程的各个领域仍然具有研究兴趣。在一组相互作用的粒子中，相互作用网络包含了各个组件之间的相互作用信息。从代理的动态中推断相互作用网络的信息是一个长期以来感兴趣的问题。在这项工作中，我们采用了一个自监督神经网络模型，实现了两个结果：恢复相互作用网络和预测个体代理的动态。这些信息都仅从观察到的轨迹数据中推断出来。本研究将神经关系推理模型应用于两个动力系统：通过胡克定律相互作用的耦合粒子和耦合相位（Kuramoto）振荡器。

    The study of interacting dynamical systems continues to attract research interest in various fields of science and engineering. In a collection of interacting particles, the interaction network contains information about how various components interact with one another. Inferring the information about the interaction network from the dynamics of agents is a problem of long-standing interest. In this work, we employ a self-supervised neural network model to achieve two outcomes: to recover the interaction network and to predict the dynamics of individual agents. Both these information are inferred solely from the observed trajectory data. This work presents an application of the Neural Relational Inference model to two dynamical systems: coupled particles mediated by Hooke's law interaction and coupled phase (Kuramoto) oscillators.
    
[^67]: Swin-Tempo: 使用Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知

    Swin-Tempo: Temporal-Aware Lung Nodule Detection in CT Scans as Video Sequences Using Swin Transformer-Enhanced UNet. (arXiv:2310.03365v1 [eess.IV])

    [http://arxiv.org/abs/2310.03365](http://arxiv.org/abs/2310.03365)

    Swin-Tempo是一个创新模型，使用了Swin Transformer-Enhanced UNet将CT扫描中的肺结节检测作为视频序列进行时间感知。它克服了现有网络的计算复杂性问题，提高了肺结节检测的准确率。

    

    肺癌具有极高的致死率，早期检测对于防治非常重要。然而，对于放射科医生而言，识别肺结节存在重大挑战，他们往往依赖自己的专业知识和经验来进行准确的诊断。为解决这个问题，基于机器学习技术的计算机辅助诊断系统已经出现，帮助医生从计算机断层扫描（CT）图像中识别肺结节。然而，现有的网络往往存在计算复杂性问题，导致误报和漏报率较高，限制了它们的有效性。为应对这些挑战，我们提出了一种创新模型，结合了卷积神经网络和视觉Transformer的优势。受视频中的目标检测启发，我们将每个3D CT图像视为一个视频，将每个切片视为帧，将肺结节视为目标，实现一个时序应用。我们的工作的主要目标是克服硬件限制。

    Lung cancer is highly lethal, emphasizing the critical need for early detection. However, identifying lung nodules poses significant challenges for radiologists, who rely heavily on their expertise and experience for accurate diagnosis. To address this issue, computer-aided diagnosis systems based on machine learning techniques have emerged to assist doctors in identifying lung nodules from computed tomography (CT) scans. Unfortunately, existing networks in this domain often suffer from computational complexity, leading to high rates of false negatives and false positives, limiting their effectiveness. To address these challenges, we present an innovative model that harnesses the strengths of both convolutional neural networks and vision transformers. Inspired by object detection in videos, we treat each 3D CT image as a video, individual slices as frames, and lung nodules as objects, enabling a time-series application. The primary objective of our work is to overcome hardware limitati
    
[^68]: 通过非对称负对比度和反向注意力进行鲁棒表征学习

    Robust Representation Learning via Asymmetric Negative Contrast and Reverse Attention. (arXiv:2310.03358v1 [cs.CV])

    [http://arxiv.org/abs/2310.03358](http://arxiv.org/abs/2310.03358)

    本文提出了一个通用的对抗训练（AT）框架，通过非对称负对比度和反向注意力，学习鲁棒的特征表征，以提高神经网络的对抗鲁棒性能。

    

    深度神经网络对抗性噪声容易受到攻击。对抗训练（AT）被证明是保护神经网络免受欺骗的最有效的防御策略。然而，我们发现AT忽视了学习鲁棒特征，导致对抗鲁棒性能较差。为了解决这个问题，我们强调了鲁棒表征的两个特征：（1）排他性：自然样本的特征远离其他类别的特征；（2）对齐性：自然样本和相应的对抗样本的特征彼此接近。这些特点激发我们提出了一个通用的AT框架，通过非对称负对比度和反向注意力来获得鲁棒的表征。具体而言，我们设计了一个基于预测概率的非对称负对比度，将特征空间中不同类别的样本推开。此外，我们提出使用线性分类器的参数对特征进行加权，作为反向注意力，以获得鲁棒的表征。

    Deep neural networks are vulnerable to adversarial noise. Adversarial training (AT) has been demonstrated to be the most effective defense strategy to protect neural networks from being fooled. However, we find AT omits to learning robust features, resulting in poor performance of adversarial robustness. To address this issue, we highlight two characteristics of robust representation: (1) $\bf{exclusion}$: the feature of natural examples keeps away from that of other classes; (2) $\bf{alignment}$: the feature of natural and corresponding adversarial examples is close to each other. These motivate us to propose a generic framework of AT to gain robust representation, by the asymmetric negative contrast and reverse attention. Specifically, we design an asymmetric negative contrast based on predicted probabilities, to push away examples of different classes in the feature space. Moreover, we propose to weight feature by parameters of the linear classifier as the reverse attention, to obta
    
[^69]: 虚构交叉玩: 在混合合作竞争游戏中学习全局纳什均衡

    Fictitious Cross-Play: Learning Global Nash Equilibrium in Mixed Cooperative-Competitive Games. (arXiv:2310.03354v1 [cs.AI])

    [http://arxiv.org/abs/2310.03354](http://arxiv.org/abs/2310.03354)

    该论文介绍了自我对弈和策略空间响应预测（PSRO）作为解决竞争游戏的强化学习框架，发现自我对弈方法在混合合作竞争游戏中无法收敛到全局纳什均衡（NE），而PSRO能够在这种情况下有效地学习到最佳响应。然而，PSRO需要重复训练联合策略，增加了难度。

    

    自我对弈（SP）是一种常用的多智能体强化学习（MARL）框架，用于解决竞争游戏，在这种框架下，每个智能体通过将其他智能体视为环境的一部分来优化策略。尽管在实证研究中取得了成功，但是SP方法的理论性质仅限于两人零和游戏。然而，在混合合作竞争游戏中，需要团队中的智能体相互合作，我们可以通过一个简单的反例来证明SP方法无法以高概率收敛到全局纳什均衡（NE）。作为替代方法，策略空间响应预测（PSRO）是一种学习NE的迭代框架，其中在每次迭代中学习相对于先前策略的最佳响应。PSRO可以直接扩展为混合合作竞争场景，同时学习团队最佳响应而所有收敛性质均保持不变。然而，PSRO需要重复从头开始训练联合策略直到收敛，这使得它变得困难。

    Self-play (SP) is a popular multi-agent reinforcement learning (MARL) framework for solving competitive games, where each agent optimizes policy by treating others as part of the environment. Despite the empirical successes, the theoretical properties of SP-based methods are limited to two-player zero-sum games. However, for mixed cooperative-competitive games where agents on the same team need to cooperate with each other, we can show a simple counter-example where SP-based methods cannot converge to a global Nash equilibrium (NE) with high probability. Alternatively, Policy-Space Response Oracles (PSRO) is an iterative framework for learning NE, where the best responses w.r.t. previous policies are learned in each iteration. PSRO can be directly extended to mixed cooperative-competitive settings by jointly learning team best responses with all convergence properties unchanged. However, PSRO requires repeatedly training joint policies from scratch till convergence, which makes it hard
    
[^70]: 采用单调性约束的深度几何学习在阿尔茨海默病进展中的应用

    Deep Geometric Learning with Monotonicity Constraints for Alzheimer's Disease Progression. (arXiv:2310.03353v1 [cs.AI])

    [http://arxiv.org/abs/2310.03353](http://arxiv.org/abs/2310.03353)

    本文提出了一种采用单调性约束的深度几何学习方法来预测阿尔茨海默病（AD）的进展。这种方法通过结合递归神经网络和普通微分方程在黎曼空间中建模时间序列数据，弥补了现有方法对数据几何属性考虑不足的问题，并解决了从不完整样本中外推正定对称度量的限制。这项工作在临床诊断和治疗中具有重要的应用价值。

    

    阿尔茨海默病（AD）是一种毁灭性的神经退行性疾病，其在渐进和不可逆的痴呆前出现；因此，对其随时间的进展进行预测对于临床诊断和治疗至关重要。许多研究已经采用结构性磁共振成像（MRI）来模拟AD的进展，重点关注三个关键方面：（i）时间变异性，（ii）不完整观测，和（iii）时间几何特征。然而，关于数据变异性和稀疏性的深度学习方法尚未充分考虑其固有的几何特性。基于普通微分方程的几何建模方法（ODE-RGRU）最近作为一种有前途的策略在黎曼空间中通过将递归神经网络和ODE交织在一起来建模时间序列数据。尽管取得了一定成就，但ODE-RGRU在从不完整样本中外推正定对称度量时面临着一些限制，导致特征反转的发生。

    Alzheimer's disease (AD) is a devastating neurodegenerative condition that precedes progressive and irreversible dementia; thus, predicting its progression over time is vital for clinical diagnosis and treatment. Numerous studies have implemented structural magnetic resonance imaging (MRI) to model AD progression, focusing on three integral aspects: (i) temporal variability, (ii) incomplete observations, and (iii) temporal geometric characteristics. However, deep learning-based approaches regarding data variability and sparsity have yet to consider inherent geometrical properties sufficiently. The ordinary differential equation-based geometric modeling method (ODE-RGRU) has recently emerged as a promising strategy for modeling time-series data by intertwining a recurrent neural network and an ODE in Riemannian space. Despite its achievements, ODE-RGRU encounters limitations when extrapolating positive definite symmetric metrics from incomplete samples, leading to feature reverse occurr
    
[^71]: 一个用于稳健且不可察觉的音频对抗样本的综合算法

    An Integrated Algorithm for Robust and Imperceptible Audio Adversarial Examples. (arXiv:2310.03349v1 [cs.SD])

    [http://arxiv.org/abs/2310.03349](http://arxiv.org/abs/2310.03349)

    本论文提出了一个综合算法，使用心理声学模型和房间冲激响应（RIR）来生成稳健且不可察觉的音频对抗样本。通过在模拟环境和真实空中场景中进行实验，以及进行人类研究评估可察觉性，我们比较了不同的方法。

    

    音频对抗样本是经过处理的音频文件，旨在欺骗自动语音识别（ASR）系统，同时对人类听众听起来仍然无害。大多数生成这种样本的方法基于两步算法：首先生成一个有效的对抗性音频文件，然后在感知性和稳健性方面进行微调。在这项工作中，我们提出了一种综合算法，在生成步骤中使用了心理声学模型和房间冲激响应（RIR）。RIR是由神经网络在生成过程中动态创建的，以模拟物理环境，以增强我们的样本对经历过空中攻击的转换的抵抗力。我们通过三个实验比较了不同的方法：在模拟环境中和在逼真的空中场景中评估稳健性，在人类研究中评估可察觉性。

    Audio adversarial examples are audio files that have been manipulated to fool an automatic speech recognition (ASR) system, while still sounding benign to a human listener. Most methods to generate such samples are based on a two-step algorithm: first, a viable adversarial audio file is produced, then, this is fine-tuned with respect to perceptibility and robustness. In this work, we present an integrated algorithm that uses psychoacoustic models and room impulse responses (RIR) in the generation step. The RIRs are dynamically created by a neural network during the generation process to simulate a physical environment to harden our examples against transformations experienced in over-the-air attacks. We compare the different approaches in three experiments: in a simulated environment and in a realistic over-the-air scenario to evaluate the robustness, and in a human study to evaluate the perceptibility. Our algorithms considering psychoacoustics only or in addition to the robustness sh
    
[^72]: LESSON: 通过选项框架学习集成强化学习中的探索策略

    LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework. (arXiv:2310.03342v1 [cs.LG])

    [http://arxiv.org/abs/2310.03342](http://arxiv.org/abs/2310.03342)

    本文提出了一种通过选项框架学习集成探索策略的强化学习统一框架。在MiniGrid和Atari环境中的实验表明该框架的有效性。

    

    本文提出了一种基于选项批判模型的强化学习探索统一框架。该框架学习集成一组多样化的探索策略，使得智能体可以适应性地选择最有效的探索策略，以实现给定任务下的相关探索-开发权衡。通过在MiniGrid和Atari环境中进行各种实验，证明了所提出探索框架的有效性。

    In this paper, a unified framework for exploration in reinforcement learning (RL) is proposed based on an option-critic model. The proposed framework learns to integrate a set of diverse exploration strategies so that the agent can adaptively select the most effective exploration strategy over time to realize a relevant exploration-exploitation trade-off for each given task. The effectiveness of the proposed exploration framework is demonstrated by various experiments in the MiniGrid and Atari environments.
    
[^73]: 基于长短期记忆网络的日前电力价格及其波动性的概率预测

    Probabilistic Forecasting of Day-Ahead Electricity Prices and their Volatility with LSTMs. (arXiv:2310.03339v1 [cs.LG])

    [http://arxiv.org/abs/2310.03339](http://arxiv.org/abs/2310.03339)

    本文提出了一种基于长短期记忆网络的德国-卢森堡日前电力价格预测模型，该模型通过联合预测均值和标准差实现了概率预测，并使用超统计学方法对价格的统计性质进行解释。

    

    准确预测电力价格对电力系统的管理和智能应用的发展至关重要。俄罗斯入侵乌克兰后，欧洲的电力价格大幅上涨并变得高度波动，给已有的预测方法带来了挑战。在本文中，我们提出了一种应对这些挑战的德国-卢森堡日前电力价格的长短期记忆网络（LSTM）模型。LSTM的循环结构使得模型能够适应趋势，而对均值和标准差进行联合预测则实现了概率预测。通过使用物理启发方法——超统计学，来解释价格的统计性质，我们展示了LSTM模型对电力价格和波动性的准确复现。

    Accurate forecasts of electricity prices are crucial for the management of electric power systems and the development of smart applications. European electricity prices have risen substantially and became highly volatile after the Russian invasion of Ukraine, challenging established forecasting methods. Here, we present a Long Short-Term Memory (LSTM) model for the German-Luxembourg day-ahead electricity prices addressing these challenges. The recurrent structure of the LSTM allows the model to adapt to trends, while the joint prediction of both mean and standard deviation enables a probabilistic prediction. Using a physics-inspired approach - superstatistics - to derive an explanation for the statistics of prices, we show that the LSTM model faithfully reproduces both prices and their volatility.
    
[^74]: 用启发式防御方法的实时深度学习网络入侵检测系统中的非定向白盒对抗攻击

    Untargeted White-box Adversarial Attack with Heuristic Defence Methods in Real-time Deep Learning based Network Intrusion Detection System. (arXiv:2310.03334v1 [cs.LG])

    [http://arxiv.org/abs/2310.03334](http://arxiv.org/abs/2310.03334)

    本研究工作重点研究非定向白盒对抗攻击在实时深度学习网络入侵检测系统中的应用，以及采用启发式防御方法来保护计算机网络免受各种网络安全威胁。

    

    网络入侵检测系统（NIDS）是保护计算机网络免受各种网络安全威胁和网络攻击的关键组件。然而，考虑到一种不幸的情况，即NIDS本身受到攻击并易受攻击，我们可以说，如何保卫捍卫者？在对抗机器学习（AML）中，恶意行为者旨在通过特意制作的对抗性样本来欺骗机器学习（ML）和深度学习（DL）模型，产生不正确的预测。这些对抗性样本已成为ML和DL系统的最大漏洞，并且是在实时和关键任务应用（例如NIDS）中采用它们的主要障碍。AML是一个新兴的研究领域，对对抗性攻击及其防御策略的深入研究已成为保护计算机网络免受各种网络安全威胁的必要性。在这项研究工作中，我们的目标是涵盖与NI相关的重要方面。

    Network Intrusion Detection System (NIDS) is a key component in securing the computer network from various cyber security threats and network attacks. However, consider an unfortunate situation where the NIDS is itself attacked and vulnerable more specifically, we can say, How to defend the defender?. In Adversarial Machine Learning (AML), the malicious actors aim to fool the Machine Learning (ML) and Deep Learning (DL) models to produce incorrect predictions with intentionally crafted adversarial examples. These adversarial perturbed examples have become the biggest vulnerability of ML and DL based systems and are major obstacles to their adoption in real-time and mission-critical applications such as NIDS. AML is an emerging research domain, and it has become a necessity for the in-depth study of adversarial attacks and their defence strategies to safeguard the computer network from various cyber security threads. In this research work, we aim to cover important aspects related to NI
    
[^75]: 精调语言模型以实现无偏的上下文学习

    Fine-tune Language Models to Approximate Unbiased In-context Learning. (arXiv:2310.03331v1 [cs.LG])

    [http://arxiv.org/abs/2310.03331](http://arxiv.org/abs/2310.03331)

    这篇论文介绍了一种精调语言模型的算法，名为RICL，并提出了一种低成本的线性最优权重近似算法LARICL。这些算法可以通过使用无偏验证集和确定每个输入-输出示例的最佳权重，实现无偏的上下文学习。

    

    上下文学习（ICL）是大语言模型（LLM）惊人的新兴能力。通过提供包含多个输入-输出对作为示例的提示，并引入新的查询输入，模型可以生成相应的输出。然而，在实施上下文学习时，模型的性能严重依赖于输入提示的质量。偏差或不平衡的输入提示会显著降低语言模型的性能。为了解决这个问题，我们引入了一种名为RICL（重加权上下文学习）的重加权算法。该算法通过使用无偏验证集来精调语言模型，确定每个输入-输出示例的最佳权重，以实现无偏的上下文学习。此外，我们还引入了一种低成本的重加权算法，一种称为LARICL（重加权上下文学习的线性近似）的线性最优权重近似算法。该算法在训练成本上要求最小，同时提供高效准确的重加权上下文学习。

    In-context learning (ICL) is an astonishing emergent ability of large language models (LLMs). By presenting a prompt that includes multiple input-output pairs as examples and introducing a new query input, models can generate the corresponding output. However, the performance of models heavily relies on the quality of the input prompt when implementing in-context learning. Biased or imbalanced input prompts can significantly degrade the performance of language models. To address this issue, we introduce a reweighted algorithm called RICL (Reweighted In-context Learning). This algorithm fine-tunes language models using an unbiased validation set to determine the optimal weight for each input-output example to approximate unbiased in-context learning. Furthermore, we also introduce a low-cost reweighted algorithm, a linear optimal weight approximation algorithm called LARICL (Linear Approximation of Reweighted In-context Learning). This algorithm requires minimal training cost while prov
    
[^76]: 学习基于概念的视觉因果转换和符号推理用于视觉规划

    Learning Concept-Based Visual Causal Transition and Symbolic Reasoning for Visual Planning. (arXiv:2310.03325v1 [cs.AI])

    [http://arxiv.org/abs/2310.03325](http://arxiv.org/abs/2310.03325)

    本文提出了一种面向视觉规划的可解释和可推广的框架，通过将视觉输入转化为概念表示、符号抽象和推理以及将视觉因果转换与真实世界行为关联，实现了目标条件的视觉规划。

    

    视觉规划模拟了人类在搜索初始视觉状态和最终视觉目标状态之间的视觉因果转换来实现期望目标时所做的决策过程。在以自我为中心的视觉中，视觉规划越来越重要，因为它在引导智能体在复杂环境中执行日常任务方面具有优势。本文提出了一个可解释和可推广的视觉规划框架，包括：i）一种新颖的基于替代的概念学习器（SCL），将视觉输入转化为分解的概念表示；ii）通过自学符号进行任务规划的符号抽象和推理；iii）将视觉因果转换与语义相似的真实世界行为进行关联的视觉因果转换模型（ViCT）。给定一个初始状态，我们通过学习到的表示和因果转换的符号推理方法进行目标条件的视觉规划，以达到目标状态。

    Visual planning simulates how humans make decisions to achieve desired goals in the form of searching for visual causal transitions between an initial visual state and a final visual goal state. It has become increasingly important in egocentric vision with its advantages in guiding agents to perform daily tasks in complex environments. In this paper, we propose an interpretable and generalizable visual planning framework consisting of i) a novel Substitution-based Concept Learner (SCL) that abstracts visual inputs into disentangled concept representations, ii) symbol abstraction and reasoning that performs task planning via the self-learned symbols, and iii) a Visual Causal Transition model (ViCT) that grounds visual causal transitions to semantically similar real-world actions. Given an initial state, we perform goal-conditioned visual planning with a symbolic reasoning method fueled by the learned representations and causal transitions to reach the goal state. To verify the effectiv
    
[^77]: 研究CLIP模型的限制：表现最差的类别

    Investigating the Limitation of CLIP Models: The Worst-Performing Categories. (arXiv:2310.03324v1 [cs.CV])

    [http://arxiv.org/abs/2310.03324](http://arxiv.org/abs/2310.03324)

    CLIP模型表现最差的类别的性能明显低于整体表现，揭示了其在特定类别重要性较高的风险敏感应用中的潜在风险。为了解决这个问题，研究了CLIP模型的模态对齐，并提出了用于衡量最差类别的推理混淆的类别匹配边界（CMM）。

    

    对比式语言-图像预训练（CLIP）通过将自然语言与视觉概念整合，提供了一个基础模型，使得在下游任务中能够进行零样本识别。通常期望通过精心设计的文本提示在众多领域中达到令人满意的整体准确率。然而，我们发现他们在最差类别的表现明显低于整体表现。例如，在ImageNet上，共有10个类别的类别准确率仅为0％，尽管整体表现已达到64.1％。这种现象揭示了使用CLIP模型可能存在的潜在风险，特别是在风险敏感的应用中，特定类别具有重要性。为了解决这个问题，我们研究了CLIP模型中两个模态之间的对齐，并提出了用于衡量推理混淆的类别匹配边界（CMM）。CMM可以有效地识别最差表现的类别。

    Contrastive Language-Image Pre-training (CLIP) provides a foundation model by integrating natural language into visual concepts, enabling zero-shot recognition on downstream tasks. It is usually expected that satisfactory overall accuracy can be achieved across numerous domains through well-designed textual prompts. However, we found that their performance in the worst categories is significantly inferior to the overall performance. For example, on ImageNet, there are a total of 10 categories with class-wise accuracy as low as 0\%, even though the overall performance has achieved 64.1\%. This phenomenon reveals the potential risks associated with using CLIP models, particularly in risk-sensitive applications where specific categories hold significant importance. To address this issue, we investigate the alignment between the two modalities in the CLIP model and propose the Class-wise Matching Margin (\cmm) to measure the inference confusion. \cmm\ can effectively identify the worst-per
    
[^78]: BioBridge: 通过知识图谱桥接生物医学基础模型

    BioBridge: Bridging Biomedical Foundation Models via Knowledge Graph. (arXiv:2310.03320v1 [cs.LG])

    [http://arxiv.org/abs/2310.03320](http://arxiv.org/abs/2310.03320)

    BioBridge是一种通过知识图谱桥接单模态生物医学基础模型的参数高效学习框架。实验证明，BioBridge在跨模态检索任务中胜过最佳基线KG嵌入方法，具有泛化能力。

    

    基础模型(FMs)能够利用大量的无标签数据，在各种任务上展现出优秀的性能。然而，用于生物医学领域的FMs主要仍处于单模态状态，即独立训练并用于处理蛋白质序列、小分子结构或临床数据等单一任务。为了克服生物医学FMs的这种局限性，我们提出了一种新颖的参数高效学习框架BioBridge，通过利用知识图谱(KG)来学习不需要微调任何底层单模态FMs的转换，从而桥接独立训练的单模态FMs以建立多模态行为。我们的实证结果表明，BioBridge在跨模态检索任务中可以击败最佳基线KG嵌入方法（平均提高约76.3%）。我们还发现，BioBridge表现出领域外的泛化能力，可以推广到未见的模态或关系中。

    Foundation models (FMs) are able to leverage large volumes of unlabeled data to demonstrate superior performance across a wide range of tasks. However, FMs developed for biomedical domains have largely remained unimodal, i.e., independently trained and used for tasks on protein sequences alone, small molecule structures alone, or clinical data alone. To overcome this limitation of biomedical FMs, we present BioBridge, a novel parameter-efficient learning framework, to bridge independently trained unimodal FMs to establish multimodal behavior. BioBridge achieves it by utilizing Knowledge Graphs (KG) to learn transformations between one unimodal FM and another without fine-tuning any underlying unimodal FMs. Our empirical results demonstrate that BioBridge can beat the best baseline KG embedding methods (on average by around 76.3%) in cross-modal retrieval tasks. We also identify BioBridge demonstrates out-of-domain generalization ability by extrapolating to unseen modalities or relation
    
[^79]: 使用受限概率人体动作预测增强人机协作

    Enhanced Human-Robot Collaboration using Constrained Probabilistic Human-Motion Prediction. (arXiv:2310.03314v1 [cs.RO])

    [http://arxiv.org/abs/2310.03314](http://arxiv.org/abs/2310.03314)

    本研究提出一种新颖的人体运动预测框架，结合人体关节约束和场景约束，利用高斯过程回归模型预测一定时间范围内的人体动作，以增强人机协作。

    

    人体动作预测是实现高效安全的人机协作的必要步骤。目前的方法要么完全依赖于将人类关节点表示为某种神经网络结构，要么使用离线回归模型来拟合超参数，以捕捉包含人体动作的模型。虽然这些方法提供了良好的初始结果，但它们未能利用研究充分的人体运动学模型以及身体和场景约束，这些约束有助于提高这些预测框架的效果，同时明确避免不合理的人体关节配置。我们提出了一种新颖的人体运动预测框架，该框架将人体关节约束和场景约束结合在高斯过程回归（GPR）模型中，以预测一定时间范围内的人体动作。此公式与在线上下文感知约束模型结合，以利用任务相关运动。在人体臂运动学模型上进行了测试。

    Human motion prediction is an essential step for efficient and safe human-robot collaboration. Current methods either purely rely on representing the human joints in some form of neural network-based architecture or use regression models offline to fit hyper-parameters in the hope of capturing a model encompassing human motion. While these methods provide good initial results, they are missing out on leveraging well-studied human body kinematic models as well as body and scene constraints which can help boost the efficacy of these prediction frameworks while also explicitly avoiding implausible human joint configurations. We propose a novel human motion prediction framework that incorporates human joint constraints and scene constraints in a Gaussian Process Regression (GPR) model to predict human motion over a set time horizon. This formulation is combined with an online context-aware constraints model to leverage task-dependent motions. It is tested on a human arm kinematic model and
    
[^80]: 可证明鲁棒的图对比学习

    Certifiably Robust Graph Contrastive Learning. (arXiv:2310.03312v1 [cs.CR])

    [http://arxiv.org/abs/2310.03312](http://arxiv.org/abs/2310.03312)

    本文提出了第一个可证明鲁棒的图对比学习（GCL）框架，并引入了随机边缘平滑（RES）技术，以确保任何GCL模型的可证明鲁棒性，并通过有效的训练方法增强了GCL的鲁棒性。

    

    图对比学习（GCL）作为一种流行的无监督图表示学习方法已经出现。然而，已经表明GCL对图结构和节点属性的对抗性攻击是脆弱的。尽管已经提出了经验性方法来增强GCL的鲁棒性，但GCL的可证明鲁棒性仍未被探索。在本文中，我们在GCL中开发了第一个可证明鲁棒的框架。具体而言，我们首先提出了一个统一的评估和证明GCL鲁棒性的标准。然后，我们引入了一种新的技术，即随机边缘平滑（RES），以确保任何GCL模型的可证明鲁棒性，并且这种证明的鲁棒性可以被保留在下游任务中。此外，我们提出了一种用于鲁棒GCL的有效训练方法。对真实世界数据集的大量实验表明了我们所提出的方法在提供有效的可证明鲁棒性和增强任何GCL模型的鲁棒性方面的有效性。

    Graph Contrastive Learning (GCL) has emerged as a popular unsupervised graph representation learning method. However, it has been shown that GCL is vulnerable to adversarial attacks on both the graph structure and node attributes. Although empirical approaches have been proposed to enhance the robustness of GCL, the certifiable robustness of GCL is still remain unexplored. In this paper, we develop the first certifiably robust framework in GCL. Specifically, we first propose a unified criteria to evaluate and certify the robustness of GCL. We then introduce a novel technique, RES (Randomized Edgedrop Smoothing), to ensure certifiable robustness for any GCL model, and this certified robustness can be provably preserved in downstream tasks. Furthermore, an effective training method is proposed for robust GCL. Extensive experiments on real-world datasets demonstrate the effectiveness of our proposed method in providing effective certifiable robustness and enhancing the robustness of any G
    
[^81]: 深度变分多变量信息瓶颈--一种变分损失的框架

    Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])

    [http://arxiv.org/abs/2310.03311](http://arxiv.org/abs/2310.03311)

    该论文介绍了一个基于信息理论的统一原理，用于重新推导和推广现有的变分降维方法，并设计新的方法。通过将多变量信息瓶颈解释为两个贝叶斯网络的权衡，该框架引入了一个在压缩数据和保留信息之间的权衡参数。

    

    变分降维方法以其高精度、生成能力和鲁棒性而闻名。这些方法有很多理论上的证明。在这里，我们介绍了一种基于信息理论的统一原理，重新推导和推广了现有的变分方法，并设计了新的方法。我们的框架基于多变量信息瓶颈的解释，其中两个贝叶斯网络相互权衡。我们将第一个网络解释为编码器图，它指定了在压缩数据时要保留的信息。我们将第二个网络解释为解码器图，它为数据指定了一个生成模型。使用这个框架，我们重新推导了现有的降维方法，如深度变分信息瓶颈(DVIB)、beta变分自编码器(beta-VAE)和深度变分规范相关分析(DVCCA)。该框架自然地引入了一个在压缩数据和保留信息之间的权衡参数。

    Variational dimensionality reduction methods are known for their high accuracy, generative abilities, and robustness. These methods have many theoretical justifications. Here we introduce a unifying principle rooted in information theory to rederive and generalize existing variational methods and design new ones. We base our framework on an interpretation of the multivariate information bottleneck, in which two Bayesian networks are traded off against one another. We interpret the first network as an encoder graph, which specifies what information to keep when compressing the data. We interpret the second network as a decoder graph, which specifies a generative model for the data. Using this framework, we rederive existing dimensionality reduction methods such as the deep variational information bottleneck (DVIB), beta variational auto-encoders (beta-VAE), and deep variational canonical correlation analysis (DVCCA). The framework naturally introduces a trade-off parameter between compr
    
[^82]: 将大型语言模型作为AI研究代理进行基准测试

    Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])

    [http://arxiv.org/abs/2310.03302](http://arxiv.org/abs/2310.03302)

    本研究提出了MLAgentBench，一个用于对AI研究代理进行基准测试的ML任务套件，代理可以执行各种操作，从而运行实验、分析结果并修改整个机器学习流程的代码。这可以帮助我们构建和评估能够执行长期目标任务的AI研究代理。

    

    科学实验涉及创建假设、设计实验、运行实验和分析结果的迭代过程。我们能否构建AI研究代理来执行这些长期目标的任务呢？为了朝着在此类开放性决策任务上构建和评估研究代理的目标迈出一步，我们着眼于机器学习工程问题：给定一个任务描述和数据集，构建一个高性能模型。在本文中，我们提出了MLAgentBench，一个用于对AI研究代理进行基准测试的ML任务套件。代理可以执行读写文件、执行代码和检查输出等动作。通过这些动作，代理可以运行实验、分析结果，并修改整个机器学习流程的代码，如数据处理、架构、训练过程等。然后，基准测试自动客观地评估代理在与性能和效率相关的各种指标上的表现。我们还设计了一个LLM-

    Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-
    
[^83]: 学习能量分解用于GFlowNets部分推断的研究

    Learning Energy Decompositions for Partial Inference of GFlowNets. (arXiv:2310.03301v1 [cs.LG])

    [http://arxiv.org/abs/2310.03301](http://arxiv.org/abs/2310.03301)

    本文提出了一种学习能量分解方法（LED-GFN），用于改进生成性流网络（GFlowNets）在采样对象时的部分推断。方法利用可学习的潜在函数对流函数进行重新参数化，解决了在操作序列中能量波动较大时的训练信号误导问题。

    

    本文研究了生成性流网络（GFlowNets），通过一系列操作从Boltzmann能量分布中采样对象。特别地，我们关注改进GFlowNet的部分推断方法：使用中间状态或转换的评估来训练流函数。为此，最近发展的前瞻性GFlowNet基于评估中间状态的能量对流函数进行了重新参数化。然而，对中间能量的评估可能（i）太昂贵或无法进行评估，（ii）甚至在操作序列中能量波动较大时提供误导性的训练信号。为解决这个问题，我们提出了用于GFlowNets的能量分解学习（LED-GFN）。我们的主要思想是（i）将对象的能量分解为在状态转换上定义的可学习潜在函数，（ii）使用这些潜在函数对流函数进行重新参数化。

    This paper studies generative flow networks (GFlowNets) to sample objects from the Boltzmann energy distribution via a sequence of actions. In particular, we focus on improving GFlowNet with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To this end, the recently developed forward-looking GFlowNet reparameterizes the flow functions based on evaluating the energy of intermediate states. However, such an evaluation of intermediate energies may (i) be too expensive or impossible to evaluate and (ii) even provide misleading training signals under large energy fluctuations along the sequence of actions. To resolve this issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our main idea is to (i) decompose the energy of an object into learnable potential functions defined on state transitions and (ii) reparameterize the flow functions using the potential functions. In particular, to produce informative local credi
    
[^84]: 一种用于非层次化多保真度自适应采样的潜变量方法

    A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive Sampling. (arXiv:2310.03298v1 [stat.ML])

    [http://arxiv.org/abs/2310.03298](http://arxiv.org/abs/2310.03298)

    提出了一种基于潜变量的方法，用于非层次化多保真度自适应采样。该方法能够利用不同保真度模型之间的相关性以更高效地探索和利用设计空间。

    

    多保真度（MF）方法在提高替代模型和设计优化方面越来越受欢迎，通过整合来自不同低保真度（LF）模型的数据。尽管大多数现有的MF方法假定了一个固定的数据集，但是动态分配资源在不同保真度模型之间可以实现更高的探索和利用设计空间的效率。然而，大多数现有的MF方法依赖于保真度级别的层次假设，或者无法捕捉多个保真度级别之间的相互关系并利用其来量化未来样本的价值和导航自适应采样。为了解决这个障碍，我们提出了一个基于不同保真度模型的潜变量嵌入和相关的先验-后验分析的框架，以显式地利用它们的相关性进行自适应采样。在这个框架中，每个填充采样迭代包括两个步骤：首先我们确定具有最大潜力影响的位置。

    Multi-fidelity (MF) methods are gaining popularity for enhancing surrogate modeling and design optimization by incorporating data from various low-fidelity (LF) models. While most existing MF methods assume a fixed dataset, adaptive sampling methods that dynamically allocate resources among fidelity models can achieve higher efficiency in the exploring and exploiting the design space. However, most existing MF methods rely on the hierarchical assumption of fidelity levels or fail to capture the intercorrelation between multiple fidelity levels and utilize it to quantify the value of the future samples and navigate the adaptive sampling. To address this hurdle, we propose a framework hinged on a latent embedding for different fidelity models and the associated pre-posterior analysis to explicitly utilize their correlation for adaptive sampling. In this framework, each infill sampling iteration includes two steps: We first identify the location of interest with the greatest potential imp
    
[^85]: LightSeq：用于长上下文转换器分布式训练的序列级并行ism

    LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers. (arXiv:2310.03294v1 [cs.LG])

    [http://arxiv.org/abs/2310.03294](http://arxiv.org/abs/2310.03294)

    LightSeq是用于长上下文转换器分布式训练的一种新方法，它通过序列维度进行分区，与不同注意力头数量的模型架构兼容，并且减少了与Megatron-LM相比的通信量，同时还实现了通信和计算的重叠。

    

    增加大型语言模型（LLM）的上下文长度可以解开基本上新的能力，但也显著增加了训练的内存占用。以往的模型并行系统（例如Megatron-LM）对不同的注意力头进行分区和计算，并行处理，导致大量通信量，因此不能在注意力头数量之外扩展，从而阻碍了其采用。本文提出了一种新方法LightSeq，用于长上下文LLM的训练。LightSeq具有许多显著优势。首先，LightSeq通过序列维度进行分区，因此对于具有不同注意力头数量的模型架构是不可知的，适用于Multi-Head，Multi-Query和Grouped-Query attention等模型。其次，LightSeq与Megatron-LM相比，在流行的LLM上不仅需求少至4.7倍的通信，而且还可以将通信与计算重叠。为了进一步减少训练时间，LightSeq还具有一种新的梯度che

    Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient che
    
[^86]: 使用深度学习方法的病房患者动作识别

    PoseAction: Action Recognition for Patients in the Ward using Deep Learning Approaches. (arXiv:2310.03288v1 [cs.CV])

    [http://arxiv.org/abs/2310.03288](http://arxiv.org/abs/2310.03288)

    这项工作使用深度学习方法开发了一个集成模型PoseAction，该模型利用计算机视觉和异步交互聚合网络来实现病房患者的动作识别和预测。通过准确的患者检测和常见动作的预测，可以提高病房的护理效率和降低护理成本。

    

    在病房中实时智能地检测和预测患者的行为，尤其是他们的运动或动作，对于降低住院护理成本和提高医护人员效率至关重要，特别是在夜间或高峰期。因此，在这项工作中，我们提出使用计算机视觉（CV）和深度学习（DL）方法来检测患者并识别其动作。我们利用OpenPose作为准确的患者检测器，来识别视频流中人体的位置。此外，我们采用AlphAction的异步交互聚合（AIA）网络来预测已检测到的患者的动作。我们提出了这个集成模型，称为PoseAction。同时，我们进一步训练所提出的模型，使用来自NTU医学视频片段来预测病房区域的12种常见动作，例如蹒跚、胸痛和摔倒。

    Real-time intelligent detection and prediction of subjects' behavior particularly their movements or actions is critical in the ward. This approach offers the advantage of reducing in-hospital care costs and improving the efficiency of healthcare workers, which is especially true for scenarios at night or during peak admission periods. Therefore, in this work, we propose using computer vision (CV) and deep learning (DL) methods for detecting subjects and recognizing their actions. We utilize OpenPose as an accurate subject detector for recognizing the positions of human subjects in the video stream. Additionally, we employ AlphAction's Asynchronous Interaction Aggregation (AIA) network to predict the actions of detected subjects. This integrated model, referred to as PoseAction, is proposed. At the same time, the proposed model is further trained to predict 12 common actions in ward areas, such as staggering, chest pain, and falling down, using medical-related video clips from the NTU 
    
[^87]: 燃烧对抗性桥梁：针对二进制级变异的强健Windows恶意软件检测

    Burning the Adversarial Bridges: Robust Windows Malware Detection Against Binary-level Mutations. (arXiv:2310.03285v1 [cs.LG])

    [http://arxiv.org/abs/2310.03285](http://arxiv.org/abs/2310.03285)

    本文针对二进制级变异的强健Windows恶意软件检测问题，分析了现有检测系统的攻击面，并提出了一种基于软件预处理和图像提取的方案以应对对抗环境。实验结果表明传统模型对抗威胁无效。

    

    为了强健的恶意软件检测，我们探索了现有恶意软件检测系统的攻击面。我们对实际的二进制级黑盒对抗恶意软件示例进行了根本原因分析。此外，我们揭示了检测引擎中易受攻击的易失性特征，并展示了它们的可利用性。通过突出软件中的易失性信息渠道，我们引入了三个软件预处理步骤来消除攻击面，即填充去除、软件剥离和交叉信息重置。此外，为了应对新兴的节段注入攻击，我们提出了一种基于图的节段依赖信息提取方案，用于软件表示。所提出的方案利用软件中各个节段内的汇集信息，实现了强健的恶意软件检测和缓解对抗环境。我们的实验结果表明，传统的恶意软件检测模型对抗对抗性威胁无效。

    Toward robust malware detection, we explore the attack surface of existing malware detection systems. We conduct root-cause analyses of the practical binary-level black-box adversarial malware examples. Additionally, we uncover the sensitivity of volatile features within the detection engines and exhibit their exploitability. Highlighting volatile information channels within the software, we introduce three software pre-processing steps to eliminate the attack surface, namely, padding removal, software stripping, and inter-section information resetting. Further, to counter the emerging section injection attacks, we propose a graph-based section-dependent information extraction scheme for software representation. The proposed scheme leverages aggregated information within various sections in the software to enable robust malware detection and mitigate adversarial settings. Our experimental results show that traditional malware detection models are ineffective against adversarial threats
    
[^88]: 一种用于解码mRNA的5' UTR语言模型和功能预测的研究

    A 5' UTR Language Model for Decoding Untranslated Regions of mRNA and Function Predictions. (arXiv:2310.03281v1 [cs.LG])

    [http://arxiv.org/abs/2310.03281](http://arxiv.org/abs/2310.03281)

    这种研究引入了一种新的语言模型UTR-LM，通过对多个物种的5' UTR进行预训练，并结合有监督信息，该模型在多个下游任务中的表现超过了现有的最佳模型，可以有效预测平均核糖体负载、翻译效率、mRNA表达水平，并改进了内源性核糖体进入位点的识别性能。

    

    5' UTR是mRNA分子开端的调控区域，在调控翻译过程和影响蛋白表达水平方面起着关键作用。语言模型已经展示了在解码蛋白质和基因组序列功能方面的有效性。在这里，我们引入了一种用于5' UTR的语言模型，称为UTR-LM。UTR-LM在多个物种的内源性5' UTR上进行了预训练，并进一步加入了包括二级结构和最小自由能在内的有监督信息。我们对UTR-LM进行了各种下游任务的微调。模型在预测平均核糖体负载上的表现超过了已知的最佳基准模型最多42%，同时在预测翻译效率和mRNA表达水平上的表现提升了最多60%。该模型还可以用于识别未注释的内源性核糖体进入位点，并将AUPR与最佳基准模型相比从0.37提高至0.52。此外，我们设计了一个...

    The 5' UTR, a regulatory region at the beginning of an mRNA molecule, plays a crucial role in regulating the translation process and impacts the protein expression level. Language models have showcased their effectiveness in decoding the functions of protein and genome sequences. Here, we introduced a language model for 5' UTR, which we refer to as the UTR-LM. The UTR-LM is pre-trained on endogenous 5' UTRs from multiple species and is further augmented with supervised information including secondary structure and minimum free energy. We fine-tuned the UTR-LM in a variety of downstream tasks. The model outperformed the best-known benchmark by up to 42% for predicting the Mean Ribosome Loading, and by up to 60% for predicting the Translation Efficiency and the mRNA Expression Level. The model also applies to identifying unannotated Internal Ribosome Entry Sites within the untranslated region and improves the AUPR from 0.37 to 0.52 compared to the best baseline. Further, we designed a li
    
[^89]: 缓解大规模MIMO系统中的导频污染问题并实现物联网可扩展性

    Mitigating Pilot Contamination and Enabling IoT Scalability in Massive MIMO Systems. (arXiv:2310.03278v1 [cs.IT])

    [http://arxiv.org/abs/2310.03278](http://arxiv.org/abs/2310.03278)

    本文提出了一种基于物联网设备数据传输模式的创新导频分配方案，解决了大规模MIMO系统中的导频污染和可扩展性问题。

    

    大规模MIMO预计在5G网络的发展中扮演重要角色。本文解决了大规模MIMO系统中的导频污染和可扩展性问题。目前在相邻小区中重用正交导频序列的做法导致难以区分来自不同小区的导频序列。一种可能的解决方案是增加正交导频序列的数量，但这会导致将更多的时频资源用于导频传输而非数据传输。这也阻碍了大规模MIMO系统的可扩展性，特别是在一个小区内容纳大量物联网设备时。为了克服这些挑战，本文基于物联网设备的数据传输模式设计了创新的导频分配方案。该方案将正交导频序列分配给设备簇而非个别设备，允许多个设备周期性地使用相同的导频传输数据。

    Massive MIMO is expected to play an important role in the development of 5G networks. This paper addresses the issue of pilot contamination and scalability in massive MIMO systems. The current practice of reusing orthogonal pilot sequences in adjacent cells leads to difficulty in differentiating incoming inter- and intra-cell pilot sequences. One possible solution is to increase the number of orthogonal pilot sequences, which results in dedicating more space of coherence block to pilot transmission than data transmission. This, in turn, also hinders the scalability of massive MIMO systems, particularly in accommodating a large number of IoT devices within a cell. To overcome these challenges, this paper devises an innovative pilot allocation scheme based on the data transfer patterns of IoT devices. The scheme assigns orthogonal pilot sequences to clusters of devices instead of individual devices, allowing multiple devices to utilize the same pilot for periodically transmitting data. M
    
[^90]: 分子图的基于片段的预训练和微调

    Fragment-based Pretraining and Finetuning on Molecular Graphs. (arXiv:2310.03274v1 [cs.LG])

    [http://arxiv.org/abs/2310.03274](http://arxiv.org/abs/2310.03274)

    该论文提出了一种在分子图上以片段级别进行预训练和微调的方法，通过对常见片段进行对比和预测预训练任务，克服了节点级和图级预训练的限制。

    

    分子图的属性预测是图神经网络（GNN）的一个重要应用。最近，无标记的分子数据变得丰富，这促进了化学领域GNN的无监督学习的快速发展。在这项工作中，我们提出了以片段级别进行预训练的GNN，这是克服节点级和图级预训练限制的一个有希望的折中办法。借鉴最近在原理子图挖掘上的工作技术，我们获得了一组紧凑的常见片段词汇，涵盖了一个大型预训练数据集。从提取的词汇中，我们引入了几个基于片段的对比和预测预训练任务。对比学习任务联合预训练了基于分子图和基于片段图的两个不同GNN，表示分子内的高阶连接性。通过强制片段嵌入和聚合嵌入的一致性，我们实现了预训练任务的微调。

    Property prediction on molecular graphs is an important application of Graph Neural Networks (GNNs). Recently, unlabeled molecular data has become abundant, which facilitates the rapid development of self-supervised learning for GNNs in the chemical domain. In this work, we propose pretraining GNNs at the fragment level, which serves as a promising middle ground to overcome the limitations of node-level and graph-level pretraining. Borrowing techniques from recent work on principle subgraph mining, we obtain a compact vocabulary of prevalent fragments that span a large pretraining dataset. From the extracted vocabulary, we introduce several fragment-based contrastive and predictive pretraining tasks. The contrastive learning task jointly pretrains two different GNNs: one based on molecular graphs and one based on fragment graphs, which represents high-order connectivity within molecules. By enforcing the consistency between the fragment embedding and the aggregated embedding of the cor
    
[^91]: 多目标表示学习中物体分割机制的消融研究

    Ablation Study to Clarify the Mechanism of Object Segmentation in Multi-Object Representation Learning. (arXiv:2310.03273v1 [cs.CV])

    [http://arxiv.org/abs/2310.03273](http://arxiv.org/abs/2310.03273)

    本研究通过在典型方法MONet上进行消融研究，旨在阐明多目标表示学习中物体分割的机制，从而揭示了先前方法中适当物体分割的方法并对VAE正则化的贡献进行了探索。

    

    多目标表示学习旨在利用多个物体的组合来表示复杂的真实世界视觉输入。表示学习方法通常使用无监督学习将输入图像分割成单个物体，并将这些物体编码到每个潜在向量中。然而，目前尚不清楚先前的方法如何实现适当的物体分割。此外，大多数先前的方法使用变分自动编码器（VAE）对潜在向量进行正则化。因此，尚不清楚VAE正则化是否有助于适当的物体分割。为了阐明多目标表示学习中物体分割的机制，我们对MONet进行了消融研究，这是一种典型的方法。MONet使用由注意力掩码和对应于注意力掩码的潜在向量组成的对来表示多个物体。每个潜在向量都是从输入图像和注意力掩码中编码的。

    Multi-object representation learning aims to represent complex real-world visual input using the composition of multiple objects. Representation learning methods have often used unsupervised learning to segment an input image into individual objects and encode these objects into each latent vector. However, it is not clear how previous methods have achieved the appropriate segmentation of individual objects. Additionally, most of the previous methods regularize the latent vectors using a Variational Autoencoder (VAE). Therefore, it is not clear whether VAE regularization contributes to appropriate object segmentation. To elucidate the mechanism of object segmentation in multi-object representation learning, we conducted an ablation study on MONet, which is a typical method. MONet represents multiple objects using pairs that consist of an attention mask and the latent vector corresponding to the attention mask. Each latent vector is encoded from the input image and attention mask. Then,
    
[^92]: 使用可传输的图自编码器进行网络对齐

    Network Alignment with Transferable Graph Autoencoders. (arXiv:2310.03272v1 [cs.LG])

    [http://arxiv.org/abs/2310.03272](http://arxiv.org/abs/2310.03272)

    该论文提出了一种基于图自编码器的网络对齐方法，通过生成与图的特征值和特征向量相关的节点嵌入，实现了更准确的对齐。同时，该方法还利用迁移学习和数据增强技术，在大规模网络对齐任务中实现高效的对齐，无需重新训练。

    

    网络对齐是在不同图之间建立一对一对应关系的任务，在高影响领域中有大量应用。然而，这个任务在一般情况下被认为是NP难的，而且现有的算法在图的规模增大时无法扩展。为了解决这两个挑战，我们提出了一种新颖的广义图自编码器架构，旨在提取强大且鲁棒的节点嵌入，适用于对齐任务。我们证明生成的嵌入与图的特征值和特征向量相关，并且与经典谱方法相比可以实现更准确的对齐。我们提出的框架还利用迁移学习和数据增强，在无需重新训练的情况下实现高效的大规模网络对齐。在真实世界的图上进行了广泛的网络对齐和子网络对齐实验，提供了支持该框架有效性和可扩展性的证据。

    Network alignment is the task of establishing one-to-one correspondences between the nodes of different graphs and finds a plethora of applications in high-impact domains. However, this task is known to be NP-hard in its general form, and existing algorithms do not scale up as the size of the graphs increases. To tackle both challenges we propose a novel generalized graph autoencoder architecture, designed to extract powerful and robust node embeddings, that are tailored to the alignment task. We prove that the generated embeddings are associated with the eigenvalues and eigenvectors of the graphs and can achieve more accurate alignment compared to classical spectral methods. Our proposed framework also leverages transfer learning and data augmentation to achieve efficient network alignment at a very large scale without retraining. Extensive experiments on both network and sub-network alignment with real-world graphs provide corroborating evidence supporting the effectiveness and scala
    
[^93]: UniPredict：大型语言模型是通用的表格预测器

    UniPredict: Large Language Models are Universal Tabular Predictors. (arXiv:2310.03266v1 [cs.LG])

    [http://arxiv.org/abs/2310.03266](http://arxiv.org/abs/2310.03266)

    本文提出了UniPredict，一个基于大型语言模型的通用表格数据预测器，能够扩展到庞大的表格数据集，并具备理解多样化表格输入和根据输入指令预测目标变量的能力。实验结果表明，UniPredict模型在与其他模型相比时具有显著优势。

    

    表格数据预测是许多应用中的基础机器学习任务。现有方法主要采用判别建模，并在假设固定目标列的情况下进行运算，需要为每个新的预测任务重新训练。本文受到大型语言模型(LLMs)生成能力的启发，提出了基于生成建模的通用表格数据预测器UniPredict。我们展示了将LLM扩展到庞大的表格数据集的方法，能够理解不同的表格输入并根据输入指令预测目标变量。具体地，我们在169个具有不同目标的表格数据集上训练了一个单一的LLM，并将其性能与分别在每个数据集上训练的基准模型进行了比较。我们观察到这个多功能的UniPredict模型在与最佳的树提升模型相比时表现出5.4%到13.4%的优势。

    Tabular data prediction is a fundamental machine learning task for many applications. Existing methods predominantly employ discriminative modeling and operate under the assumption of a fixed target column, necessitating re-training for every new predictive task. Inspired by the generative power of large language models (LLMs), this paper exploits the idea of building universal tabular data predictors based on generative modeling, namely UniPredict. Here, we show that scaling up an LLM to extensive tabular datasets with the capability of comprehending diverse tabular inputs and predicting for target variables following the input instructions. Specifically, we train a single LLM on an aggregation of 169 tabular datasets with diverse targets and compare its performance against baselines that are trained on each dataset separately. We observe this versatile UniPredict model demonstrates an advantage over other models, ranging from 5.4% to 13.4%, when compared with the best tree-boosting b
    
[^94]: 利用大规模停电数据集上的转移反事实学习，检测电力服务公平性问题

    Detecting Electricity Service Equity Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets. (arXiv:2310.03258v1 [cs.LG])

    [http://arxiv.org/abs/2310.03258](http://arxiv.org/abs/2310.03258)

    通过转移反事实学习的方法，我们在大规模停电数据集上研究了电力服务的公平性问题，发现低收入和老年人口区域经常遭受较长的停电时间，揭示了电力系统中的偏见存在并强调了改善的需求。

    

    能源公正是跨学科能源研究的一个不断发展的领域。然而，由于混淆变量、治疗效应的复杂异质性和有限的数据可用性，识别能源部门中的系统偏见仍然具有挑战性。为了解决这些挑战，我们引入了一种围绕能源公正的反事实因果分析的新方法。我们使用子组分析来管理各种因素，并利用转移学习的思想来缓解每个子组中的数据稀缺问题。在我们的数值分析中，我们将我们的方法应用于一个大规模的客户级停电数据集，并研究人口因素（如收入和年龄）对停电持续时间的反事实效应。我们的结果表明，低收入和老年人口区域总是经历较长的停电时间，无论天气条件如何。这表明电力系统存在偏见，并强调了专注改善这些问题的需要。

    Energy justice is a growing area of interest in interdisciplinary energy research. However, identifying systematic biases in the energy sector remains challenging due to confounding variables, intricate heterogeneity in treatment effects, and limited data availability. To address these challenges, we introduce a novel approach for counterfactual causal analysis centered on energy justice. We use subgroup analysis to manage diverse factors and leverage the idea of transfer learning to mitigate data scarcity in each subgroup. In our numerical analysis, we apply our method to a large-scale customer-level power outage data set and investigate the counterfactual effect of demographic factors, such as income and age of the population, on power outage durations. Our results indicate that low-income and elderly-populated areas consistently experience longer power outages, regardless of weather conditions. This points to existing biases in the power system and highlights the need for focused im
    
[^95]: 潜在提示Transformer模型在分子设计中的应用

    Molecule Design by Latent Prompt Transformer. (arXiv:2310.03253v1 [cs.LG])

    [http://arxiv.org/abs/2310.03253](http://arxiv.org/abs/2310.03253)

    本文提出了一种潜在提示Transformer模型，用于解决分子设计中的优化问题。该模型包括潜在向量、分子生成模型和性质预测模型，通过对现有分子进行训练后进行模型分布的逐渐转移。

    

    本文提出了一种用于解决分子设计等具有挑战性优化问题的潜在提示Transformer模型，其中目标是找到具有目标化学或生物性质最优值的分子，该值可以由现有软件计算得出。我们提出的模型包括三个组件：（1）潜在向量，其先验分布由高斯白噪声向量的Unet变换建模。（2）分子生成模型，在（1）中给定潜在向量的条件下生成基于字符串的分子表示。我们采用了以（1）中的潜在向量作为提示的因果Transformer模型。（3）性质预测模型，根据（1）中的潜在向量进行非线性回归预测分子的目标性质值。我们称该提出的模型为潜在提示Transformer模型。在对现有分子及其性质值进行初步训练后，我们逐渐转移模型分布的学习。

    This paper proposes a latent prompt Transformer model for solving challenging optimization problems such as molecule design, where the goal is to find molecules with optimal values of a target chemical or biological property that can be computed by an existing software. Our proposed model consists of three components. (1) A latent vector whose prior distribution is modeled by a Unet transformation of a Gaussian white noise vector. (2) A molecule generation model that generates the string-based representation of molecule conditional on the latent vector in (1). We adopt the causal Transformer model that takes the latent vector in (1) as prompt. (3) A property prediction model that predicts the value of the target property of a molecule based on a non-linear regression on the latent vector in (1). We call the proposed model the latent prompt Transformer model. After initial training of the model on existing molecules and their property values, we then gradually shift the model distributi
    
[^96]: 稀疏深度学习用于时间序列数据：理论与应用

    Sparse Deep Learning for Time Series Data: Theory and Applications. (arXiv:2310.03243v1 [stat.ML])

    [http://arxiv.org/abs/2310.03243](http://arxiv.org/abs/2310.03243)

    本文研究了稀疏深度学习在依赖数据（如时间序列数据）上的理论和应用。通过研究，我们发现稀疏循环神经网络能够一致地估计，并对其预测进行正确的不确定性量化。数值实验结果显示，稀疏深度学习在预测不确定性方面优于最先进方法。

    

    稀疏深度学习已成为提升深度神经网络在不确定性量化、变量选择和大规模网络压缩等领域性能的流行技术。然而，大部分现有研究都集中在观测相互独立且同分布（i.i.d.）的问题上，并且在涉及时间序列数据和自然语言处理中的顺序数据等观测相互依赖的问题上几乎没有相关工作。本文旨在填补这一空白，研究具有依赖数据的稀疏深度学习的理论。我们证明了稀疏循环神经网络（RNN）可以一致地估计，并且它们的预测在适当的假设下渐近地服从正态分布，从而能够正确量化预测不确定性。我们的数值实验结果表明，稀疏深度学习在预测不确定性量化方面胜过了诸如依照性预测等最先进方法。

    Sparse deep learning has become a popular technique for improving the performance of deep neural networks in areas such as uncertainty quantification, variable selection, and large-scale network compression. However, most existing research has focused on problems where the observations are independent and identically distributed (i.i.d.), and there has been little work on the problems where the observations are dependent, such as time series data and sequential data in natural language processing. This paper aims to address this gap by studying the theory for sparse deep learning with dependent data. We show that sparse recurrent neural networks (RNNs) can be consistently estimated, and their predictions are asymptotically normally distributed under appropriate assumptions, enabling the prediction uncertainty to be correctly quantified. Our numerical results show that sparse deep learning outperforms state-of-the-art methods, such as conformal predictions, in prediction uncertainty qua
    
[^97]: 关系卷积网络：学习层次关系表示的框架

    Relational Convolutional Networks: A framework for learning representations of hierarchical relations. (arXiv:2310.03240v1 [cs.LG])

    [http://arxiv.org/abs/2310.03240](http://arxiv.org/abs/2310.03240)

    关系卷积网络是一个学习显式层次关系表示的框架，通过使用多维内积关系模块和关系卷积层，以及基于图元滤波器的群组比较，能够表达更高阶、层次的关系。

    

    深度学习中一个成熟的研究领域是开发能够学习显式关系特征表示的架构。本文着重于学习层次关系表示的问题，提出了一个名为“关系卷积网络”的架构框架。给定一系列对象，一个“多维内积关系”模块生成一个描述所有成对关系的关系张量。然后，一个“关系卷积”层将关系张量转化为一个新对象序列，每个对象描述前一层某群对象内的关系。类似于卷积神经网络中的滤波器，图元滤波器代表要与关系张量在每个分组中进行比较的关系模板。通过重复这个过程，得到更高阶、层次的关系表示。我们介绍了架构的动机和细节，以及一系列实验来证明…

    A maturing area of research in deep learning is the development of architectures that can learn explicit representations of relational features. In this paper, we focus on the problem of learning representations of hierarchical relations, proposing an architectural framework we call "relational convolutional networks". Given a sequence of objects, a "multi-dimensional inner product relation" module produces a relation tensor describing all pairwise relations. A "relational convolution" layer then transforms the relation tensor into a sequence of new objects, each describing the relations within some group of objects at the previous layer. Graphlet filters, analogous to filters in convolutional neural networks, represent a template of relations against which the relation tensor is compared at each grouping. Repeating this yields representations of higher-order, hierarchical relations. We present the motivation and details of the architecture, together with a set of experiments to demons
    
[^98]: 非光滑弱凸有限和耦合组合优化

    Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v1 [math.OC])

    [http://arxiv.org/abs/2310.03234](http://arxiv.org/abs/2310.03234)

    本文研究了一种新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)，通过扩展已有的研究，我们研究了非光滑弱凸FCCO的问题，并提出了一种单循环算法来找到Moreau环的ε-稳定点。

    

    本文研究了一类新的组合优化问题，称为非光滑弱凸有限和耦合组合优化(NSWC FCCO)。由于其在机器学习和人工智能领域的广泛应用以及其解决基于经验风险最小化的随机算法的局限性，FCCO引起了越来越多的关注。然而，目前对于FCCO的研究假设内外函数都是光滑的，限制了其能够解决更多种类的问题的潜力。我们的研究从非光滑弱凸FCCO的角度进行了扩展，其中外函数是弱凸且非递减的，内函数是弱凸的。我们分析了一种单循环算法，并确定其在找到Moreau环的ε-稳定点的复杂度。

    This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau env
    
[^99]: 基于数据空间反演和时空数据参数化的地质碳储存历史匹配方法

    History Matching for Geological Carbon Storage using Data-Space Inversion with Spatio-Temporal Data Parameterization. (arXiv:2310.03228v1 [cs.LG])

    [http://arxiv.org/abs/2310.03228](http://arxiv.org/abs/2310.03228)

    本研究开发了一种基于深度学习的参数化方法，在数据空间反演中实现了对地质碳储存历史匹配的准确推断，可以在工业规模碳储存操作中提高地下水管理的效果。

    

    基于监测数据的历史匹配可以减少不确定性，从而改善工业规模碳储存操作中的地下水管理。在传统的基于模型的数据同化中，通过修改地质模型参数来使得流动模拟结果与观测值吻合。而在数据空间反演（DSI）中，通过使用一组约1000个先验模拟结果，数据参数化和贝叶斯框架内的后验抽样，可以直接推断出历史匹配后的感兴趣量，例如以观察为条件的后验压力和饱和度场。本研究中，我们开发并在数据空间反演中实施了一种基于深度学习的参数化方法，以表示一系列时间步骤上的时空压力和CO2饱和度场。新的参数化方法使用对抗自编码器（AAE）进行维度降低，使用卷积长短期记忆（convLSTM）网络进行时间连续建模。

    History matching based on monitoring data will enable uncertainty reduction, and thus improved aquifer management, in industrial-scale carbon storage operations. In traditional model-based data assimilation, geomodel parameters are modified to force agreement between flow simulation results and observations. In data-space inversion (DSI), history-matched quantities of interest, e.g., posterior pressure and saturation fields conditioned to observations, are inferred directly, without constructing posterior geomodels. This is accomplished efficiently using a set of O(1000) prior simulation results, data parameterization, and posterior sampling within a Bayesian setting. In this study, we develop and implement (in DSI) a deep-learning-based parameterization to represent spatio-temporal pressure and CO2 saturation fields at a set of time steps. The new parameterization uses an adversarial autoencoder (AAE) for dimension reduction and a convolutional long short-term memory (convLSTM) networ
    
[^100]: 安全探索在强化学习中的应用：一种普适的形式化和算法

    Safe Exploration in Reinforcement Learning: A Generalized Formulation and Algorithms. (arXiv:2310.03225v1 [cs.LG])

    [http://arxiv.org/abs/2310.03225](http://arxiv.org/abs/2310.03225)

    本文提出了一种广义的安全探索问题，即GSE，并提出了一个元算法MASE来解决这个问题。MASE将无约束的强化学习算法与不确定性量化器结合，以保证安全性，并在违反安全约束之前惩罚不安全的探索。这种方法的优势是在保证安全性的同时进行策略优化，并具有高概率的安全保证。

    

    在许多真实场景中，安全探索对于实际应用强化学习（RL）至关重要。本文提出了一种广义安全探索（GSE）问题，作为常见安全探索问题的统一形式化。然后，我们提出了GSE问题的解决方案，即安全探索的元算法MASE，它将无约束的RL算法与不确定性量化器相结合，以保证当前回合的安全性，同时在实际安全违规之前适当惩罚不安全的探索，以防止它们在未来回合中发生。MASE的优势在于我们可以在保证高概率下不违反安全约束的前提下，优化策略。具体而言，我们提出了两种不同构建不确定性量化器的MASE变体：一种基于广义线性模型，具有安全性和接近最优性的理论保证，另一种则结合了高斯过程。

    Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations before actual safety violation to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian proce
    
[^101]: TacoGFN: 针对基于结构的药物设计的目标条件GFlowNet

    TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])

    [http://arxiv.org/abs/2310.03223](http://arxiv.org/abs/2310.03223)

    该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。

    

    我们旨在自动化生成符合特定蛋白质口袋目标的类药物化合物。大多数当前方法是对有限数据集中的蛋白质-分子分布进行近似，因此在生成的分子中很难实现与训练数据集相比具有显著结合改善的分子。我们将口袋条件下的分子生成任务定义为强化学习问题，并开发了TacoGFN，一种目标条件下的生成流网络模型。我们的方法明确鼓励生成具有期望属性的分子，而不是适应预先存在的数据分布。为此，我们开发了基于转换器的对接得分预测方法来加快对接得分计算，并提出了TacoGFN来高效地探索分子空间。此外，我们还结合了几轮主动学习，使用对接神经网络对生成的样本进行查询，以改善对接得分预测。这种方法使我们能够准确地探索更多的分子空间。

    We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
    
[^102]: Know2BIO: 一个全面的双视图演变生物医学知识图谱基准

    Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs. (arXiv:2310.03221v1 [cs.LG])

    [http://arxiv.org/abs/2310.03221](http://arxiv.org/abs/2310.03221)

    Know2BIO是一个全面的双视图演变生物医学知识图谱基准，通过整合多样化数据和多模态数据，克服了KG的实体对齐、扩展性和更新的挑战。

    

    知识图谱（KG）已经成为表示和集成复杂生物医学信息的强大框架。然而，从多样化的来源组装KG仍然是一个重大挑战，包括实体对齐，可扩展性以及需要不断更新以跟上科学进展。此外，知识图谱的代表能力通常受到多模态数据整合的稀缺性的限制。为了克服这些挑战，我们提出了Know2BIO，一个用于生物医学领域的通用异构KG基准。Know2BIO整合了来自30个不同来源的数据，捕捉了11个生物医学类别之间的复杂关系。它目前包含约219,000个节点和约6,200,000个边。Know2BIO能够根据用户指示自动更新以反映生物医学科学中的最新知识。此外，Know2BIO还附带多模态数据：包括文本描述、蛋白质和化合物序列等节点特征。

    Knowledge graphs (KGs) have emerged as a powerful framework for representing and integrating complex biomedical information. However, assembling KGs from diverse sources remains a significant challenge in several aspects, including entity alignment, scalability, and the need for continuous updates to keep pace with scientific advancements. Moreover, the representative power of KGs is often limited by the scarcity of multi-modal data integration. To overcome these challenges, we propose Know2BIO, a general-purpose heterogeneous KG benchmark for the biomedical domain. Know2BIO integrates data from 30 diverse sources, capturing intricate relationships across 11 biomedical categories. It currently consists of ~219,000 nodes and ~6,200,000 edges. Know2BIO is capable of user-directed automated updating to reflect the latest knowledge in biomedical science. Furthermore, Know2BIO is accompanied by multi-modal data: node features including text descriptions, protein and compound sequences and s
    
[^103]: 用扩散改进的 MCMC 学习能量先验模型

    Learning Energy-Based Prior Model with Diffusion-Amortized MCMC. (arXiv:2310.03218v1 [cs.LG])

    [http://arxiv.org/abs/2310.03218](http://arxiv.org/abs/2310.03218)

    本文介绍了一种基于扩散改进的长期 MCMC采样的学习算法，用于学习能量先验模型。实验证明了该算法的有效性。

    

    隐变量空间的能量基模型（EBMs），也称为能量先验模型，由于其在公式化和潜在空间的强建模能力上的灵活性，引起了生成建模领域的日益关注。然而，使用非收敛的短期 MCMC 进行先验和后验采样来学习隐变量空间的能量先验模型的常见做法，阻碍了模型的进一步发展；实践中退化的 MCMC 采样质量通常导致生成质量下降和训练不稳定，特别是在高多模态和/或高维目标分布中。为了解决这个采样问题，在本文中，我们引入了一种简单但有效的基于扩散的摊销方法，用于长期 MCMC 采样，并基于此开发了一种新的学习算法来学习隐变量空间的EBM。我们提供了理论证据，表明学习到的MCMC摊销是一个有效的长期MCMC采样器。在几个图像建模基准数据集上的实验证明了

    Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in the field of generative modeling due to its flexibility in the formulation and strong modeling power of the latent space. However, the common practice of learning latent space EBMs with non-convergent short-run MCMC for prior and posterior sampling is hindering the model from further progress; the degenerate MCMC sampling quality in practice often leads to degraded generation quality and instability in training, especially with highly multi-modal and/or high-dimensional target distributions. To remedy this sampling issue, in this paper we introduce a simple but effective diffusion-based amortization method for long-run MCMC sampling and develop a novel learning algorithm for the latent space EBM based on it. We provide theoretical evidence that the learned amortization of MCMC is a valid long-run MCMC sampler. Experiments on several image modeling benchmark datasets demonstrate t
    
[^104]: 机器学习系统认证的形式和实际要素

    Formal and Practical Elements for the Certification of Machine Learning Systems. (arXiv:2310.03217v1 [cs.LG])

    [http://arxiv.org/abs/2310.03217](http://arxiv.org/abs/2310.03217)

    本文研究了机器学习系统认证的形式要素和实际要素，通过揭示构建机器学习模型的内部工作原理和过程，形式建立理论保证，并结合实际考虑，开发了一个完整的安全关键机器学习系统认证论证。

    

    在过去的十年中，机器学习在与自主飞行相关的感知任务中表现出令人印象深刻的结果，往往超过了人类的能力。与传统的航空航天软件不同，机器学习模型的参数不是手工编码或从物理学派生的，而是从数据中学习的。它们在训练阶段自动调整，其值通常不符合物理要求。因此，要求不能直接追溯到代码行，阻碍了当前自下而上的航空航天认证范式。本文试图通过以下方式填补这一差距：1）揭示构建机器学习模型的内部工作原理和过程，2）正式建立这些过程所提供的理论保证，3）补充这些形式要素与实际考虑，以开发一种完整的安全关键机器学习系统认证论证。基于可扩展的统计验证器，我们提出了一个框架

    Over the past decade, machine learning has demonstrated impressive results, often surpassing human capabilities in sensing tasks relevant to autonomous flight. Unlike traditional aerospace software, the parameters of machine learning models are not hand-coded nor derived from physics but learned from data. They are automatically adjusted during a training phase, and their values do not usually correspond to physical requirements. As a result, requirements cannot be directly traced to lines of code, hindering the current bottom-up aerospace certification paradigm. This paper attempts to address this gap by 1) demystifying the inner workings and processes to build machine learning models, 2) formally establishing theoretical guarantees given by those processes, and 3) complementing these formal elements with practical considerations to develop a complete certification argument for safety-critical machine learning systems. Based on a scalable statistical verifier, our proposed framework i
    
[^105]: PDR-CapsNet:动态路由胶囊网络中的一种节能并行方法

    PDR-CapsNet: an Energy-Efficient Parallel Approach to Dynamic Routing in Capsule Networks. (arXiv:2310.03212v1 [cs.LG])

    [http://arxiv.org/abs/2310.03212](http://arxiv.org/abs/2310.03212)

    PDR-CapsNet是一种更深、更节能的胶囊网络替代方案，通过并行化策略减轻了计算复杂性并提高了吞吐量，实现了更优越的性能和更少的能量消耗。

    

    卷积神经网络(CNNs)在图像分类任务中取得了最先进的结果。然而，由于最大池化层中的信息丢失，它们在处理旋转和视点变化方面的能力有限。胶囊网络(CapsNets)采用了一种计算密集的迭代过程，称为动态路由，以解决这些问题。然而，CapsNets在复杂数据集上往往表现不佳，并且需要比CNNs更多的计算资源。为了克服这些挑战，我们引入了并行动态路由胶囊网络(PDR-CapsNet)，这是一种比CapsNet更深、更节能的替代方案，提供了更优越的性能、更少的能量消耗和更低的过拟合率。通过利用并行化策略，PDR-CapsNet减轻了CapsNet的计算复杂性，增加了吞吐量，高效利用硬件资源。因此，我们实现了83.55%的准确率，同时只需要87.26%的参数，32.27%和47.40%

    Convolutional Neural Networks (CNNs) have produced state-of-the-art results for image classification tasks. However, they are limited in their ability to handle rotational and viewpoint variations due to information loss in max-pooling layers. Capsule Networks (CapsNets) employ a computationally-expensive iterative process referred to as dynamic routing to address these issues. CapsNets, however, often fall short on complex datasets and require more computational resources than CNNs. To overcome these challenges, we introduce the Parallel Dynamic Routing CapsNet (PDR-CapsNet), a deeper and more energy-efficient alternative to CapsNet that offers superior performance, less energy consumption, and lower overfitting rates. By leveraging a parallelization strategy, PDR-CapsNet mitigates the computational complexity of CapsNet and increases throughput, efficiently using hardware resources. As a result, we achieve 83.55\% accuracy while requiring 87.26\% fewer parameters, 32.27\% and 47.40\%
    
[^106]: LTI系统中对抗性扰动的分布式在线控制的遗憾分析

    Regret Analysis of Distributed Online Control for LTI Systems with Adversarial Disturbances. (arXiv:2310.03206v1 [math.OC])

    [http://arxiv.org/abs/2310.03206](http://arxiv.org/abs/2310.03206)

    本文研究了在线分布式控制问题，其中涉及到一个具有对抗性扰动的线性时不变（LTI）系统网络。对于已知动态的情况，提出了一个完全分布式的扰动反馈控制器，保证遗憾界为$O(\sqrt{T}\log T)$。对于未知动态的情况，设计了一个分布式的探索-承诺方法，使所有代理在探索阶段共同学习系统动态，并在学习阶段使用提出的控制算法进行控制。

    

    本文研究了在线分布式控制问题，其中涉及到一个具有对抗性扰动的线性时不变（LTI）系统网络（其动态可能是未知的）。存在一个全局网络成本，由一个时变的凸函数来描述，其以对抗的方式演化，并且局部代理只能顺序和部分观测到它。每个代理的目标是生成一组控制序列，以便能够与事后具有全局成本访问权限的最佳集中式控制策略竞争。该问题被建模为遗憾最小化。对于已知动态的情况，我们提出了一个完全分布式的扰动反馈控制器，保证遗憾界为$O(\sqrt{T}\log T)$，其中$T$为时间窗口。对于未知动态的情况，我们设计了一个分布式的探索-承诺方法，在探索阶段所有代理共同学习系统动态，在学习阶段我们提出的控制算法进行控制。

    This paper addresses the distributed online control problem over a network of linear time-invariant (LTI) systems (with possibly unknown dynamics) in the presence of adversarial perturbations. There exists a global network cost that is characterized by a time-varying convex function, which evolves in an adversarial manner and is sequentially and partially observed by local agents. The goal of each agent is to generate a control sequence that can compete with the best centralized control policy in hindsight, which has access to the global cost. This problem is formulated as a regret minimization. For the case of known dynamics, we propose a fully distributed disturbance feedback controller that guarantees a regret bound of $O(\sqrt{T}\log T)$, where $T$ is the time horizon. For the unknown dynamics case, we design a distributed explore-then-commit approach, where in the exploration phase all agents jointly learn the system dynamics, and in the learning phase our proposed control algorit
    
[^107]: 深度强化学习用于机器调度：方法论、现状和未来方向

    Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions. (arXiv:2310.03195v1 [cs.LG])

    [http://arxiv.org/abs/2310.03195](http://arxiv.org/abs/2310.03195)

    本文对基于深度强化学习(DRL)的机器调度方法进行了全面回顾和比较，总结出它们的方法论、应用、优势和局限性。通过对比分析，发现DRL方法优于传统方法。

    

    机器调度旨在通过满足制造规则和作业规范的前提下，优化作业分配给机器。这种优化可以降低运营成本、提高客户需求的满足度和增强生产效率。然而，由于其 NP-hard 的性质，机器调度仍然是一个具有挑战性的组合优化问题。深度强化学习(DRL)作为人工通用智能的关键组成部分，在游戏和机器人等领域已经显示出了潜力。自1995年以来，研究人员一直在探索将DRL应用于机器调度问题。本文对基于DRL的方法进行了全面的回顾和比较，重点介绍了它们的方法论、应用、优势和局限性。它将这些方法根据计算组件进行了分类：传统神经网络、编码器-解码器结构、图神经网络和元启发式算法。我们的回顾得出结论，基于DRL的方法优于精确求解器和启发式算法。

    Machine scheduling aims to optimize job assignments to machines while adhering to manufacturing rules and job specifications. This optimization leads to reduced operational costs, improved customer demand fulfillment, and enhanced production efficiency. However, machine scheduling remains a challenging combinatorial problem due to its NP-hard nature. Deep Reinforcement Learning (DRL), a key component of artificial general intelligence, has shown promise in various domains like gaming and robotics. Researchers have explored applying DRL to machine scheduling problems since 1995. This paper offers a comprehensive review and comparison of DRL-based approaches, highlighting their methodology, applications, advantages, and limitations. It categorizes these approaches based on computational components: conventional neural networks, encoder-decoder architectures, graph neural networks, and metaheuristic algorithms. Our review concludes that DRL-based methods outperform exact solvers, heuristi
    
[^108]: 基于概念瓶颈模型的鲁棒和可解释的医学图像分类器

    Robust and Interpretable Medical Image Classifiers via Concept Bottleneck Models. (arXiv:2310.03182v1 [cs.CV])

    [http://arxiv.org/abs/2310.03182](http://arxiv.org/abs/2310.03182)

    本文提出了一种新的方法来构建鲁棒和可解释的医学图像分类器，通过从GPT-4中查询临床概念，并利用视觉-语言模型将潜在的图像特征转化为明确的概念，以解决在真实世界医疗应用中的两个挑战：学习不想关的相关性和缺乏解释性。

    

    医学图像分类是医疗保健中一个关键问题，有望减轻医生的工作负担并促进对患者的诊断。然而，当将深度学习模型应用于现实世界的医疗保健应用时，存在两个挑战。首先，神经模型往往会学习到不相关的相关性，而不是所期望的特征，这在推广到新领域时可能会有所不足（例如，具有不同年龄的患者）。其次，这些黑盒模型缺乏解释性。在进行诊断预测时，了解模型为何做出决策对于可信度和安全性至关重要。为了解决这两个限制，本文提出了一种新的范例，用自然语言概念构建鲁棒和可解释的医学图像分类器。具体来说，我们首先从GPT-4中查询临床概念，然后利用视觉-语言模型将潜在的图像特征转化为明确的概念。我们对我们的方法在八个场景下进行了系统评估

    Medical image classification is a critical problem for healthcare, with the potential to alleviate the workload of doctors and facilitate diagnoses of patients. However, two challenges arise when deploying deep learning models to real-world healthcare applications. First, neural models tend to learn spurious correlations instead of desired features, which could fall short when generalizing to new domains (e.g., patients with different ages). Second, these black-box models lack interpretability. When making diagnostic predictions, it is important to understand why a model makes a decision for trustworthy and safety considerations. In this paper, to address these two limitations, we propose a new paradigm to build robust and interpretable medical image classifiers with natural language concepts. Specifically, we first query clinical concepts from GPT-4, then transform latent image features into explicit concepts with a vision-language model. We systematically evaluate our method on eight
    
[^109]: 联邦学习中的数字伦理问题

    Digital Ethics in Federated Learning. (arXiv:2310.03178v1 [cs.LG])

    [http://arxiv.org/abs/2310.03178](http://arxiv.org/abs/2310.03178)

    本文探讨了在联邦学习中作为客户端的以人为中心的设备引发的数字伦理问题。瞭解了其中涉及的游戏动态、公平性、奖励机制和连贯性等挑战，并探讨了解决方案和以人为中心的物联网在联邦学习中的机遇。

    

    物联网（IoT）不断产生大量数据，引发了对数据隐私保护和数据滥用限制的越来越多的关注。联邦学习（FL）通过共享机器学习（ML）模型参数而不是原始用户数据，促进了多方之间的协作能力，并因其在隐私保护和学习效率提升方面的潜力而引起了广泛关注。本文重点讨论了在FL中作为客户端的以人为中心的设备引发的数字伦理问题。具体而言，在客户端和服务器之间存在不同的观点和目标的情况下，FL面临着游戏动态、公平性、奖励机制和连贯性等挑战。我们从客户端和服务器的角度，以及集中式和分散式FL的视角分析了这些挑战及其解决方案。最后，我们探讨了以人为中心的物联网在FL中的机遇。

    The Internet of Things (IoT) consistently generates vast amounts of data, sparking increasing concern over the protection of data privacy and the limitation of data misuse. Federated learning (FL) facilitates collaborative capabilities among multiple parties by sharing machine learning (ML) model parameters instead of raw user data, and it has recently gained significant attention for its potential in privacy preservation and learning efficiency enhancement. In this paper, we highlight the digital ethics concerns that arise when human-centric devices serve as clients in FL. More specifically, challenges of game dynamics, fairness, incentive, and continuity arise in FL due to differences in perspectives and objectives between clients and the server. We analyze these challenges and their solutions from the perspectives of both the client and the server, and through the viewpoints of centralized and decentralized FL. Finally, we explore the opportunities in FL for human-centric IoT as dir
    
[^110]: 用代码句法特征的分布式表示进行测试用例推荐

    Test Case Recommendations with Distributed Representation of Code Syntactic Features. (arXiv:2310.03174v1 [cs.LG])

    [http://arxiv.org/abs/2310.03174](http://arxiv.org/abs/2310.03174)

    该论文提出了一种使用代码句法特征的分布式表示来推荐测试用例的方法，通过训练神经网络并计算余弦相似度，提高了自动生成和维护测试单元的效率和有效性。

    

    由于软件源代码、设计和需求的持续变化，单元测试用例的频繁修改是不可避免的。由于手动维护软件测试套件是繁琐、费时和昂贵的，自动生成和维护测试单元的过程自动化将显著影响软件测试流程的有效性和效率。为此，我们提出了一种自动化方法，利用源代码方法和测试用例的结构和语义属性来推荐对开发人员最相关和有用的单元测试。该方法首先训练一个神经网络，将方法级源代码和单元测试转换为分布式表示（嵌入向量），同时保持代码结构的重要性。通过检索给定方法的语义和结构属性，该方法计算方法嵌入与先前嵌入的训练样本之间的余弦相似度。

    Frequent modifications of unit test cases are inevitable due to software's continuous underlying changes in source code, design, and requirements. Since manually maintaining software test suites is tedious, timely, and costly, automating the process of generation and maintenance of test units will significantly impact the effectiveness and efficiency of software testing processes.  To this end, we propose an automated approach which exploits both structural and semantic properties of source code methods and test cases to recommend the most relevant and useful unit tests to the developers. The proposed approach initially trains a neural network to transform method-level source code, as well as unit tests, into distributed representations (embedded vectors) while preserving the importance of the structure in the code. Retrieving the semantic and structural properties of a given method, the approach computes cosine similarity between the method's embedding and the previously-embedded trai
    
[^111]: 破坏到底：对机器学习防钓鱼网页检测器的查询高效对抗HTML攻击

    Raze to the Ground: Query-Efficient Adversarial HTML Attacks on Machine-Learning Phishing Webpage Detectors. (arXiv:2310.03166v1 [cs.CR])

    [http://arxiv.org/abs/2310.03166](http://arxiv.org/abs/2310.03166)

    本文提出了一种新颖的查询高效对抗HTML攻击方法，通过细粒度篡改来修改钓鱼网页的HTML代码，同时保持其恶意性和视觉外观不变。实验表明，这种方法能够将目前最先进的机器学习防钓鱼网页检测器的性能摧毁，并且只需要30个查询。

    

    机器学习防钓鱼网页检测器（ML-PWD）已被证明容易受到输入网页HTML代码的对抗性篡改。然而，最近提出的攻击由于缺乏优化所采用的篡改的使用以及仅关注HTML代码的特定元素而表现出有限的有效性。在这项工作中，我们通过首先设计一组新颖的细粒度篡改来克服这些限制，这些篡改允许修改输入钓鱼网页的HTML代码，而无需 compromiser其恶意性和视觉外观，即篡改在设计上是功能和渲染保持不变的。然后，我们使用一个查询高效的黑盒优化算法选择需要应用哪些篡改以绕过目标检测器。我们的实验表明，我们的攻击只需要30个查询即可摧毁目前最先进的ML-PWD的性能，从而克服了较弱的攻击。

    Machine-learning phishing webpage detectors (ML-PWD) have been shown to suffer from adversarial manipulations of the HTML code of the input webpage. Nevertheless, the attacks recently proposed have demonstrated limited effectiveness due to their lack of optimizing the usage of the adopted manipulations, and they focus solely on specific elements of the HTML code. In this work, we overcome these limitations by first designing a novel set of fine-grained manipulations which allow to modify the HTML code of the input phishing webpage without compromising its maliciousness and visual appearance, i.e., the manipulations are functionality- and rendering-preserving by design. We then select which manipulations should be applied to bypass the target detector by a query-efficient black-box optimization algorithm. Our experiments show that our attacks are able to raze to the ground the performance of current state-of-the-art ML-PWD using just 30 queries, thus overcoming the weaker attacks develo
    
[^112]: 利用随机矩阵理论提高深度学习的准确性

    Enhancing Accuracy in Deep Learning Using Random Matrix Theory. (arXiv:2310.03165v1 [cs.LG])

    [http://arxiv.org/abs/2310.03165](http://arxiv.org/abs/2310.03165)

    本研究探索了随机矩阵理论在深度神经网络训练中的应用，通过层剪枝和损失曲面优化，实现了对DNN架构的简化和准确性的增强。通过奇异值分解，并根据随机矩阵理论的标准丢弃小的奇异值，可减少DNN层的参数，简化DNN架构，同时保持或增强模型的准确性。

    

    在本研究中，我们探索了随机矩阵理论在深度神经网络（DNN）训练中的应用，重点是通过层剪枝简化DNN架构和损失曲面。随机矩阵理论最近被用于解决深度学习中的过拟合问题，能够检查DNN的权重层谱。我们使用这些技术来在训练过程中通过奇异值分解（SVD）确定要从DNN的权重层中去除的奇异值的数量，这有助于简化DNN并提高准确性，在MNIST和Fashion MNIST数据集上培训简单的DNN模型得到了证明。我们的方法适用于预训练DNN的任何全连接或卷积层，减少了层的参数并简化了DNN的架构，同时保持甚至增强了模型的准确性。通过根据随机矩阵理论的标准丢弃小的奇异值，测试集的准确性保持一致，从而实现更有效的DNN训练。

    In this study, we explore the applications of random matrix theory (RMT) in the training of deep neural networks (DNNs), focusing on layer pruning to simplify DNN architecture and loss landscape. RMT, recently used to address overfitting in deep learning, enables the examination of DNN's weight layer spectra. We use these techniques to optimally determine the number of singular values to be removed from the weight layers of a DNN during training via singular value decomposition (SVD). This process aids in DNN simplification and accuracy enhancement, as evidenced by training simple DNN models on the MNIST and Fashion MNIST datasets.  Our method can be applied to any fully connected or convolutional layer of a pretrained DNN, decreasing the layer's parameters and simplifying the DNN architecture while preserving or even enhancing the model's accuracy. By discarding small singular values based on RMT criteria, the accuracy of the test set remains consistent, facilitating more efficient DN
    
[^113]: FedNAR: 带有归一化退火正则化的联邦优化

    FedNAR: Federated Optimization with Normalized Annealing Regularization. (arXiv:2310.03163v1 [cs.LG])

    [http://arxiv.org/abs/2310.03163](http://arxiv.org/abs/2310.03163)

    本文提出了一种名为FedNAR的算法插件，它通过归一化退火正则化来调节每次更新的大小，从而解决了联邦学习中权重衰减对全局目标的优化目标的不同影响。

    

    在现代深度神经网络优化中，权重衰减是一种改善泛化性能的标准技术，在联邦学习中也被广泛采用以防止本地客户端过拟合。本文首先探索了权重衰减的选择，并确定权重衰减值显著影响现有联邦学习算法的收敛性。虽然防止过拟合至关重要，但权重衰减可能引入对全局目标的不同优化目标，在联邦学习中由于多个本地更新和异构数据分布进一步放大了这一影响。为了解决这个挑战，我们开发了一种简单而有效、多功能的算法插件，即“带有归一化退火正则化的联邦优化”（FedNAR），可以无缝地集成到任何现有的联邦学习算法中。基本上，我们通过对梯度和权重衰减进行联合裁剪来调节每次更新的大小。我们提供了一个全面的理论

    Weight decay is a standard technique to improve generalization performance in modern deep neural network optimization, and is also widely adopted in federated learning (FL) to prevent overfitting in local clients. In this paper, we first explore the choices of weight decay and identify that weight decay value appreciably influences the convergence of existing FL algorithms. While preventing overfitting is crucial, weight decay can introduce a different optimization goal towards the global objective, which is further amplified in FL due to multiple local updates and heterogeneous data distribution. To address this challenge, we develop {\it Federated optimization with Normalized Annealing Regularization} (FedNAR), a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms. Essentially, we regulate the magnitude of each update by performing co-clipping of the gradient and weight decay. We provide a comprehensive theoretical 
    
[^114]: 神经架构对于识别时间延长增强学习任务的影响

    Neural architecture impact on identifying temporally extended Reinforcement Learning tasks. (arXiv:2310.03161v1 [cs.LG])

    [http://arxiv.org/abs/2310.03161](http://arxiv.org/abs/2310.03161)

    本研究提出了基于注意力的神经架构，在强化学习领域取得了良好的性能。通过将注意力图叠加到图像上，可以直接观察到代理所使用的信息，并更容易解释选择动作背后的逻辑。

    

    在图像分类和自然语言处理领域的关注模型最新进展的启发下，我们在强化学习领域提出了多种基于注意力的架构，能够在OpenAI Gym Atari-2600游戏套件上表现良好。尽管深度增强学习技术在机器人、游戏和医疗等各个领域取得了最近的成功，但它们存在一个主要问题，即神经网络难以解释。我们尝试通过注意力模型来解决这个问题。在注意力模型中，提取和叠加注意力图到图像上，可以直接观察代理使用的信息以选择动作，并更容易解释选择动作背后的逻辑。我们的模型不仅在gym-Atari环境中表现优秀，还能提供关于代理如何感知其环境的见解。此外，受到使用视觉注意力的视频分类模型最新进展的启发，

    Inspired by recent developments in attention models for image classification and natural language processing, we present various Attention based architectures in reinforcement learning (RL) domain, capable of performing well on OpenAI Gym Atari-2600 game suite. In spite of the recent success of Deep Reinforcement learning techniques in various fields like robotics, gaming and healthcare, they suffer from a major drawback that neural networks are difficult to interpret. We try to get around this problem with the help of Attention based models. In Attention based models, extracting and overlaying of attention map onto images allows for direct observation of information used by agent to select actions and easier interpretation of logic behind the chosen actions. Our models in addition to playing well on gym-Atari environments, also provide insights on how agent perceives its environment. In addition, motivated by recent developments in attention based video-classification models using Vis
    
[^115]: 评估使用不确定特征曲线的预测区间

    Assessment of Prediction Intervals Using Uncertainty Characteristics Curves. (arXiv:2310.03158v1 [cs.LG])

    [http://arxiv.org/abs/2310.03158](http://arxiv.org/abs/2310.03158)

    本论文提出了一种新颖的评估预测区间的方法，利用了操作特征曲线和相对于空白参考的收益概念。这种方法广泛适用于不同的研究场景，解决了当前对预测区间全面评估的需求。

    

    准确量化模型的不确定性已经被认为是可信AI的基本要求。在回归任务中，不确定性通常使用校准到专有操作点的预测区间来量化，从而使得对不同研究进行评估和比较相对困难。我们的工作利用了(1)操作特征曲线的概念和(2)相对于空白参考的收益概念，推导出一种新颖的操作点不可知评估方法，适用于预测区间。本文定义了不确定特征曲线，并在选定场景中展示其实用性。我们认为，所提出的方法解决了对预测区间进行全面评估的当前需求，因此是不确定性量化工具箱的宝贵补充。

    Accurate quantification of model uncertainty has long been recognized as a fundamental requirement for trusted AI. In regression tasks, uncertainty is typically quantified using prediction intervals calibrated to an ad-hoc operating point, making evaluation and comparison across different studies relatively difficult. Our work leverages: (1) the concept of operating characteristics curves and (2) the notion of a gain over a null reference, to derive a novel operating point agnostic assessment methodology for prediction intervals. The paper defines the Uncertainty Characteristics Curve and demonstrates its utility in selected scenarios. We argue that the proposed method addresses the current need for comprehensive assessment of prediction intervals and thus represents a valuable addition to the uncertainty quantification toolbox.
    
[^116]: FedHyper:一种用于联邦学习的通用和稳健学习率调度器与超梯度下降

    FedHyper: A Universal and Robust Learning Rate Scheduler for Federated Learning with Hypergradient Descent. (arXiv:2310.03156v1 [cs.LG])

    [http://arxiv.org/abs/2310.03156](http://arxiv.org/abs/2310.03156)

    FedHyper是一种适用于联邦学习的通用和稳健学习率调度器，通过超梯度下降算法实现对全局和局部学习率的自适应调整，能够显著提高联邦学习系统的效果，同时减少了对经验调整的需求。

    

    联邦学习（FL）的理论框架正在迅速发展，但其实际应用面临一系列复杂挑战，其中超参数优化是其中一个关键挑战。在众多超参数调整中，学习率的适应性成为一个重要组成部分，有望显著提高FL系统的效果。为了满足这一关键需求，本文提出了FedHyper，一种专为FL设计的基于超梯度的学习率调整算法。FedHyper作为一种通用的学习率调度器，可以随着训练的进行调整全局和局部的学习率。此外，FedHyper不仅展示了对各种初始学习率配置的无与伦比的稳健性，还极大地减少了繁琐的经验性学习率调整的必要性。我们提供了FedHyper收敛速度的全面理论分析。

    The theoretical landscape of federated learning (FL) undergoes rapid evolution, but its practical application encounters a series of intricate challenges, and hyperparameter optimization is one of these critical challenges. Amongst the diverse adjustments in hyperparameters, the adaptation of the learning rate emerges as a crucial component, holding the promise of significantly enhancing the efficacy of FL systems. In response to this critical need, this paper presents FedHyper, a novel hypergradient-based learning rate adaptation algorithm specifically designed for FL. FedHyper serves as a universal learning rate scheduler that can adapt both global and local rates as the training progresses. In addition, FedHyper not only showcases unparalleled robustness to a spectrum of initial learning rate configurations but also significantly alleviates the necessity for laborious empirical learning rate adjustments. We provide a comprehensive theoretical analysis of FedHyper's convergence rate 
    
[^117]: 实现化学动力学性质的基于机器学习的超出分布预测

    Towards out-of-distribution generalizable predictions of chemical kinetics properties. (arXiv:2310.03152v1 [cs.LG])

    [http://arxiv.org/abs/2310.03152](http://arxiv.org/abs/2310.03152)

    本论文目标是实现化学动力学性质的基于机器学习的超出分布预测，通过研究OOD动力学性质预测的三个层次，揭示问题的独特方面，并创建全面的数据集评估最先进的反应预测方法和动力学性质预测方法。

    

    机器学习技术在估计化学动力学性质方面找到了应用。随着通过“AI4drug discovery”鉴定的药物分子的累积，下一个急需解决的问题是通过人工智能驱动的设计高通量化学合成过程，以估计未见反应中未知分子的性质。为了实现这一目标，现有的机器学习方法需要具备超出分布（OOD）的普适性。在本文中，我们将OOD动力学性质预测分为三个层次（结构、条件和机制），揭示了这些问题的独特方面。在这个框架下，我们创建了全面的数据集，对OOD条件下的反应预测的最先进的机器学习方法以及动力学性质预测问题中的最先进的图形OOD方法进行了评估。我们的结果展示了OOD动力学性质预测中的挑战和机遇。我们的数据集...

    Machine Learning (ML) techniques have found applications in estimating chemical kinetics properties. With the accumulated drug molecules identified through "AI4drug discovery", the next imperative lies in AI-driven design for high-throughput chemical synthesis processes, with the estimation of properties of unseen reactions with unexplored molecules. To this end, the existing ML approaches for kinetics property prediction are required to be Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD kinetic property prediction into three levels (structure, condition, and mechanism), revealing unique aspects of such problems. Under this framework, we create comprehensive datasets to benchmark (1) the state-of-the-art ML approaches for reaction prediction in the OOD setting and (2) the state-of-the-art graph OOD methods in kinetics property prediction problems. Our results demonstrated the challenges and opportunities in OOD kinetics property prediction. Our datasets an
    
[^118]: 在非常边缘上对LLMs进行联邦微调：好、坏和丑(arXiv:2310.03150v1 [cs.LG])

    Federated Fine-Tuning of LLMs on the Very Edge: The Good, the Bad, the Ugly. (arXiv:2310.03150v1 [cs.LG])

    [http://arxiv.org/abs/2310.03150](http://arxiv.org/abs/2310.03150)

    本论文以硬件为中心，探讨了如何将LLMs引入现代边缘计算系统。通过联邦学习（FL）对FLAN-T5模型进行微调，并对其在文本摘要任务上的性能进行了评估。同时提供了硬件基准测试和与数据中心GPU的比较。

    

    大规模语言模型（LLM）和基础模型因为提供了改进自然语言处理、与数据交互和快速检索信息的新机会而受到欢迎。然而，训练或微调LLMs需要大量的数据，由于法律或技术限制可能难以访问，并且可能需要私有计算资源。联邦学习（FL）是一种旨在克服这些挑战并扩大深度学习应用数据访问的解决方案。本文采用硬件为中心的方法，探索了如何将LLMs引入现代边缘计算系统。我们对FLAN-T5模型系列进行微调，参数范围从80M到3B，并应用于文本摘要任务。我们提供了微观水平的硬件基准测试，将模型的FLOP利用率与最先进的数据中心GPU进行了比较，并研究了在实际条件下的网络利用率。我们的贡献具有两个方面：首先，我们评估了...

    Large Language Models (LLM) and foundation models are popular as they offer new opportunities for individuals and businesses to improve natural language processing, interact with data, and retrieve information faster. However, training or fine-tuning LLMs requires a vast amount of data, which can be challenging to access due to legal or technical restrictions and may require private computing resources. Federated Learning (FL) is a solution designed to overcome these challenges and expand data access for deep learning applications.  This paper takes a hardware-centric approach to explore how LLMs can be brought to modern edge computing systems. Our study fine-tunes the FLAN-T5 model family, ranging from 80M to 3B parameters, using FL for a text summarization task. We provide a micro-level hardware benchmark, compare the model FLOP utilization to a state-of-the-art data center GPU, and study the network utilization in realistic conditions. Our contribution is twofold: First, we evaluate
    
[^119]: 将神经网络中学习到的概念归因于训练数据

    Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v1 [cs.LG])

    [http://arxiv.org/abs/2310.03149](http://arxiv.org/abs/2310.03149)

    通过将数据归因方法与概念探测方法相结合，研究了神经网络中学习到的概念与训练数据的关系，并发现概念的位置和稀疏性并不完全依赖于少量特定示例。

    

    现在有大量的证据表明，深度学习模型学习到了某些可解释的人类特征，作为其对数据的内部表示的一部分。由于拥有正确（或错误）的概念对于可信赖的机器学习系统至关重要，自然而然地我们想要知道在给定层次上，模型原始训练集中的哪些输入对于学习一个概念最为重要。为了回答这个问题，我们将数据归因方法与探测模型学习到的概念的方法相结合。通过在一系列网络层次上训练网络和探测模型，并使用最近开发的用于大规模数据归因的TRAK方法，我们对两个概念数据集进行训练网络和探测模型的集合。我们发现一些证据表明，通过移除对一个概念具有最高归因的前10000张图像并重新训练模型，概念在网络中的位置以及概念的探测稀疏性并没有发生改变。这表明，与依赖于少量特定示例不同，用于确定概念的特征具有较高的独立性。

    By now there is substantial evidence that deep learning models learn certain human-interpretable features as part of their internal representations of data. As having the right (or wrong) concepts is critical to trustworthy machine learning systems, it is natural to ask which inputs from the model's original training set were most important for learning a concept at a given layer. To answer this, we combine data attribution methods with methods for probing the concepts learned by a model. Training network and probe ensembles for two concept datasets on a range of network layers, we use the recently developed TRAK method for large-scale data attribution. We find some evidence for convergence, where removing the 10,000 top attributing images for a concept and retraining the model does not change the location of the concept in the network nor the probing sparsity of the concept. This suggests that rather than being highly dependent on a few specific examples, the features that inform the 
    
[^120]: 多任务学习用于减少多领域视频推荐中的流行度偏差

    Multi-Task Learning For Reduced Popularity Bias In Multi-Territory Video Recommendations. (arXiv:2310.03148v1 [cs.IR])

    [http://arxiv.org/abs/2310.03148](http://arxiv.org/abs/2310.03148)

    本文提出了一种多任务学习技术和自适应上采样方法，用于减少多领域推荐系统中的流行度偏差。通过实验证明，该框架在多个领域中相对增益高达65.27%。

    

    多领域个性化推荐系统中自然产生的各种数据不平衡可能导致全球流行物品的显著项目偏见。局部流行项目可能会被全球流行项目所掩盖。此外，用户的观看模式/统计数据在不同地理位置之间可能发生剧变，这可能表明需要学习特定的用户嵌入。本文提出了一种多任务学习（MTL）技术，以及一种自适应上采样方法，用于减少多领域推荐中的流行偏见。我们的框架通过上采样来丰富含有活跃用户表示的训练样本，并借助MTL来学习基于地理位置的用户嵌入。通过实验证明，与不采用我们提出的技术的基准相比，我们的框架在多个领域的效果显著。值得注意的是，我们在PR-AUC指标上显示出了高达65.27%的相对增益。

    Various data imbalances that naturally arise in a multi-territory personalized recommender system can lead to a significant item bias for globally prevalent items. A locally popular item can be overshadowed by a globally prevalent item. Moreover, users' viewership patterns/statistics can drastically change from one geographic location to another which may suggest to learn specific user embeddings. In this paper, we propose a multi-task learning (MTL) technique, along with an adaptive upsampling method to reduce popularity bias in multi-territory recommendations. Our proposed framework is designed to enrich training examples with active users representation through upsampling, and capable of learning geographic-based user embeddings by leveraging MTL. Through experiments, we demonstrate the effectiveness of our framework in multiple territories compared to a baseline not incorporating our proposed techniques.~Noticeably, we show improved relative gain of up to $65.27\%$ in PR-AUC metric
    
[^121]: 基于上下文的推文参与度预测

    Context-Based Tweet Engagement Prediction. (arXiv:2310.03147v1 [cs.IR])

    [http://arxiv.org/abs/2310.03147](http://arxiv.org/abs/2310.03147)

    该论文研究了基于上下文的推文参与度预测问题，使用了Twitter的数据集和评估流程，探讨了仅凭上下文是否可以很好地预测推文的参与度可能性。

    

    Twitter目前是最大的社交媒体平台之一。其用户可以分享、阅读和参与短推文。在2020年ACM推荐系统会议上，Twitter发布了一个大小约为70GB的数据集，供年度RecSys挑战赛使用。2020年的RecSys挑战赛邀请参与团队创建模型，预测给定用户-推文组合的参与度可能性。提交的模型预测点赞、回复、转发和引用的参与度，并基于两个指标进行评估：精确率-召回率曲线下的面积（PRAUC）和相对交叉熵（RCE）。在这篇学位论文中，我们使用了RecSys 2020挑战赛的数据集和评估流程，研究仅凭上下文能否预测推文参与度的可行性。为此，我们在TU Wien的Little Big Data Cluster上采用Spark引擎创建可扩展的数据预处理、特征工程、特征选择和机器学习流程。我们手动创建。

    Twitter is currently one of the biggest social media platforms. Its users may share, read, and engage with short posts called tweets. For the ACM Recommender Systems Conference 2020, Twitter published a dataset around 70 GB in size for the annual RecSys Challenge. In 2020, the RecSys Challenge invited participating teams to create models that would predict engagement likelihoods for given user-tweet combinations. The submitted models predicting like, reply, retweet, and quote engagements were evaluated based on two metrics: area under the precision-recall curve (PRAUC) and relative cross-entropy (RCE).  In this diploma thesis, we used the RecSys 2020 Challenge dataset and evaluation procedure to investigate how well context alone may be used to predict tweet engagement likelihood. In doing so, we employed the Spark engine on TU Wien's Little Big Data Cluster to create scalable data preprocessing, feature engineering, feature selection, and machine learning pipelines. We manually create
    
[^122]: 增强公平性的混合效应深度学习在簇（非独立同分布）数据上改善公平性

    Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data. (arXiv:2310.03146v1 [cs.LG])

    [http://arxiv.org/abs/2310.03146](http://arxiv.org/abs/2310.03146)

    这个论文提出了一种增强公平性的混合效应深度学习（MEDL）框架，通过同时解决数据集簇间关联和不公平性的问题，来提高对簇分布数据的公平性和泛化能力。

    

    传统深度学习在两个核心问题上存在困扰。首先，它假设训练样本是独立同分布的，然而，许多真实世界的数据集将样本按共享的测量值进行分组（例如，研究参与者或细胞），违反了这一假设。在这些场景中，深度学习可能显示出性能下降、泛化能力有限和解释性问题，并伴随着簇混淆引起的第一型和第二型错误。其次，模型通常被训练以实现整体准确性，往往忽视了被低估的群体，在贷款批准或确定健康保险费率等关键领域引入偏见，这些偏见可能会严重影响个人的生活质量。为了同时解决这两个挑战，我们提出了一种混合效应深度学习（MEDL）框架。MEDL通过引入以下内容分别量化簇不变的固定效应和簇特定的随机效应来解决这两个挑战：1）一个簇对手，鼓励簇间差异的最小化；

    Traditional deep learning (DL) suffers from two core problems. Firstly, it assumes training samples are independent and identically distributed. However, numerous real-world datasets group samples by shared measurements (e.g., study participants or cells), violating this assumption. In these scenarios, DL can show compromised performance, limited generalization, and interpretability issues, coupled with cluster confounding causing Type 1 and 2 errors. Secondly, models are typically trained for overall accuracy, often neglecting underrepresented groups and introducing biases in crucial areas like loan approvals or determining health insurance rates, such biases can significantly impact one's quality of life. To address both of these challenges simultaneously, we present a mixed effects deep learning (MEDL) framework. MEDL separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) through the introduction of: 1) a cluster adversary which encourage
    
[^123]: 高效的黑盒大型预训练模型联合提示调整

    Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models. (arXiv:2310.03123v1 [cs.LG])

    [http://arxiv.org/abs/2310.03123](http://arxiv.org/abs/2310.03123)

    这项研究提出了一种高效的黑盒大型预训练模型联合提示调整（Fed-BBPT）方法，通过摒弃对参数结构和私有数据集访问的依赖，可以在处理内存限制和保持隐私性的同时充分利用每个局部数据集。

    

    随着预训练模型（PTMs）的迅猛发展，对这些模型进行高效调整以适用于不同的下游应用已成为一个关键的研究关注点。尽管最近关于提示调整的研究提供了有希望的途径，但仍然存在三个突出的挑战：（1）内存限制：开源PTMs大小的持续增长使得即使对其参数的一小部分进行微调也对许多从业者来说是具有挑战性的。（2）模型隐私性：现有的PTMs通常作为公共API服务，其参数无法有效或定制地进行微调。（3）数据隐私性：对PTMs进行微调需要高质量的数据集，这些数据集通常是局部化的并且不共享给公众。为了在处理内存限制和保持隐私性的同时充分利用每个局部数据集，我们提出了联合黑盒提示调整（Fed-BBPT）。这种创新方法摒弃了对参数结构和私有数据集访问的依赖，可以有效地解决这些挑战。

    With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, ins
    
[^124]: OpenMM 8：带有机器学习势的分子动力学模拟

    OpenMM 8: Molecular Dynamics Simulation with Machine Learning Potentials. (arXiv:2310.03121v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03121](http://arxiv.org/abs/2310.03121)

    OpenMM 8是一个支持使用机器学习势函数的分子动力学模拟工具包，通过引入新功能、优化计算速度和提供高级接口，使得使用机器学习来提高模拟精度成为一种实际可行的方法。

    

    机器学习在分子模拟中扮演着重要且不断增长的角色。OpenMM分子动力学工具包的最新版本引入了新功能，以支持使用机器学习势函数。任意的PyTorch模型可添加到模拟中，并可用于计算力和能量。更高级的接口使用户可以轻松地使用通用预训练势函数对感兴趣的分子进行建模。优化的CUDA核函数和自定义的PyTorch操作大大提高了模拟的速度。我们在水中模拟了CDK8和绿色荧光蛋白(GFP)色团，并展示了这些功能。总的来说，这些特性使使用机器学习来提高模拟精度变得实际，并只有适度的成本增加。

    Machine learning plays an important and growing role in molecular simulation. The newest version of the OpenMM molecular dynamics toolkit introduces new features to support the use of machine learning potentials. Arbitrary PyTorch models can be added to a simulation and used to compute forces and energy. A higher-level interface allows users to easily model their molecules of interest with general purpose, pretrained potential functions. A collection of optimized CUDA kernels and custom PyTorch operations greatly improves the speed of simulations. We demonstrate these features on simulations of cyclin-dependent kinase 8 (CDK8) and the green fluorescent protein (GFP) chromophore in water. Taken together, these features make it practical to use machine learning to improve the accuracy of simulations at only a modest increase in cost.
    
[^125]: 跨物联网设备可移植的电磁侧信道分析：挑战与数据集

    Crossed-IoT device portability of Electromagnetic Side Channel Analysis: Challenges and Dataset. (arXiv:2310.03119v1 [cs.LG])

    [http://arxiv.org/abs/2310.03119](http://arxiv.org/abs/2310.03119)

    这项研究探讨了跨物联网设备的可移植的电磁侧信道分析的挑战，并提出了数据集来解决设备变化性、环境因素和数据处理方法对EM-SCA结果准确性的限制。

    

    物联网（Internet of Things）是指互联的物理设备、车辆、家用电器等嵌入传感器、软件和连接性的网络，使它们能够收集和交换数据。物联网取证是从物联网设备收集和分析数字证据，以调查可能发生在这些连接设备上的网络犯罪、安全漏洞和其他恶意活动。特别是，电磁侧信道分析(EM-SCA)已成为物联网取证的重要工具，因为它能够在不干预这些设备或窃听它们的网络的情况下，揭示有关物联网设备内部运作的机密信息。然而，设备的变化性、环境因素以及数据收集和处理方法会限制EM-SCA结果的准确性和可靠性。此外，对于跨物联网设备可移植性的EM-SCA方法的准确性受到的限制几乎没有研究，也有限制。

    IoT (Internet of Things) refers to the network of interconnected physical devices, vehicles, home appliances, and other items embedded with sensors, software, and connectivity, enabling them to collect and exchange data. IoT Forensics is collecting and analyzing digital evidence from IoT devices to investigate cybercrimes, security breaches, and other malicious activities that may have taken place on these connected devices. In particular, EM-SCA has become an essential tool for IoT forensics due to its ability to reveal confidential information about the internal workings of IoT devices without interfering these devices or wiretapping their networks. However, the accuracy and reliability of EM-SCA results can be limited by device variability, environmental factors, and data collection and processing methods. Besides, there is very few research on these limitations that affects significantly the accuracy of EM-SCA approaches for the crossed-IoT device portability as well as limited res
    
[^126]: 利用基于模型的树作为可解释的替代模型进行模型蒸馏

    Leveraging Model-based Trees as Interpretable Surrogate Models for Model Distillation. (arXiv:2310.03112v1 [stat.ML])

    [http://arxiv.org/abs/2310.03112](http://arxiv.org/abs/2310.03112)

    这项研究利用基于模型的树作为可解释的替代模型，通过决策规则将特征空间划分为可解释的区域，并使用基于可加性主效应的可解释模型来近似黑盒子模型的行为，以在可解释性和性能之间达到最佳平衡。

    

    替代模型在通过模型蒸馏回顾性地解释复杂而强大的黑盒子机器学习模型中起着至关重要的作用。本文着重于使用基于模型的树作为替代模型，通过决策规则将特征空间划分为可解释的区域。在每个区域内，使用基于可加性主效应的可解释模型来近似黑盒子模型的行为，以在可解释性和性能之间达到最佳平衡。我们比较了四种基于模型的树算法（SLIM，GUIDE，MOB和CTree）在生成这样的替代模型方面的能力。我们通过对保真度、可解释性、稳定性以及算法捕捉交互效应的能力进行了全面分析。最后，基于我们的综合分析，我们提供了用户特定的推荐概述。

    Surrogate models play a crucial role in retrospectively interpreting complex and powerful black box machine learning models via model distillation. This paper focuses on using model-based trees as surrogate models which partition the feature space into interpretable regions via decision rules. Within each region, interpretable models based on additive main effects are used to approximate the behavior of the black box model, striking for an optimal balance between interpretability and performance. Four model-based tree algorithms, namely SLIM, GUIDE, MOB, and CTree, are compared regarding their ability to generate such surrogate models. We investigate fidelity, interpretability, stability, and the algorithms' capability to capture interaction effects through appropriate splits. Based on our comprehensive analyses, we finally provide an overview of user-specific recommendations.
    
[^127]: 多模态高斯过程变分自编码器用于神经和行为数据

    Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data. (arXiv:2310.03111v1 [cs.LG])

    [http://arxiv.org/abs/2310.03111](http://arxiv.org/abs/2310.03111)

    该论文介绍了一种多模态高斯过程变分自编码器（GP-VAEs）的方法，用于描述神经和行为数据之间的关系。该方法通过将高斯过程因子分析（GPFA）和深度神经网络相结合，能够提取不同实验模态的共享和独立潜变量，并且具有解释能力。

    

    描述神经群体活动和行为数据之间的关系是神经科学的核心目标。虽然潜在变量模型（LVMs）在描述高维时间序列数据方面取得了成功，但它们通常只用于单一类型的数据，这使得难以识别不同实验数据模态之间的共享结构。在这里，我们通过提出一种无监督的LVM来解决这个缺点，该模型提取了不同、同时记录的实验模态的时间演化共享和独立潜变量。我们通过将高斯过程因子分析（GPFA），一种解释性的用于神经尖峰数据的LVM，并具有时间平滑潜空间，与高斯过程变分自编码器（GP-VAEs）相结合来实现这一点，GP-VAEs同样使用高斯先验来描述潜空间中的相关性，但由于深度神经网络映射到观测值具有丰富的表达能力。我们通过将潜变量分区来实现模型的可解释性。

    Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional time-series data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts temporally evolving shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning lat
    
[^128]: 创建一个正常组织的图谱，通过异常检测对WSI补丁进行修剪

    Creating an Atlas of Normal Tissue for Pruning WSI Patching Through Anomaly Detection. (arXiv:2310.03106v1 [eess.IV])

    [http://arxiv.org/abs/2310.03106](http://arxiv.org/abs/2310.03106)

    本研究提出了使用正常组织样本构建“正常组织图谱”的概念，并展示了如何通过这种图谱来提高WSI的代表性。通过在107个正常皮肤WSI上建立正常图谱，并使用553个表皮鳞状细胞癌（cSCC）的WSI进行验证，我们证明了该方法的有效性。

    

    在计算病理学中，修剪千兆像素的全尺寸切片图像（WSI）是一个重要的任务。已提出一些方法来选择一部分补丁作为WSI表示进行下游任务。尽管大多数计算病理学任务都旨在对每个WSI中的病理损伤进行分类或检测，但常规组织样本中正常组织的困扰作用和冗余性质通常被忽视在WSI表示中。在本文中，我们提出并验证了使用从正常组织活检中获得的WSI样本仅仅用于构建“正常组织图谱”的概念。这样的图谱可以用于消除正常组织样本的正常片段，从而增加补丁集合的代表性。我们通过使用107个正常皮肤WSI建立了一个正常图谱，并展示了如何改进已建立的指标和搜索引擎，如Yottixel。我们使用了553个表皮鳞状细胞癌（cSCC）的WSI来展示...

    Patching gigapixel whole slide images (WSIs) is an important task in computational pathology. Some methods have been proposed to select a subset of patches as WSI representation for downstream tasks. While most of the computational pathology tasks are designed to classify or detect the presence of pathological lesions in each WSI, the confounding role and redundant nature of normal histology in tissue samples are generally overlooked in WSI representations. In this paper, we propose and validate the concept of an "atlas of normal tissue" solely using samples of WSIs obtained from normal tissue biopsies. Such atlases can be employed to eliminate normal fragments of tissue samples and hence increase the representativeness collection of patches. We tested our proposed method by establishing a normal atlas using 107 normal skin WSIs and demonstrated how established indexes and search engines like Yottixel can be improved. We used 553 WSIs of cutaneous squamous cell carcinoma (cSCC) to show
    
[^129]: 非可分的目标函数的DP-SGD方法

    DP-SGD for non-decomposable objective functions. (arXiv:2310.03104v1 [cs.LG])

    [http://arxiv.org/abs/2310.03104](http://arxiv.org/abs/2310.03104)

    本论文提出了一种针对非可分的目标函数的DP-SGD方法，解决了使用差分隐私进行训练时，相似性损失函数的$L_2$敏感度增长随着批量大小增加的问题。

    

    无监督预训练是开发计算机视觉模型和大型语言模型的常见步骤。在这种情况下，由于缺少标签，需要使用基于相似性的损失函数，如对比损失，来优化相似输入之间的距离并最大化不同输入之间的距离。随着隐私问题的增多，使用差分隐私来训练这些模型变得更加重要。然而，由于这些损失函数生成输入的方式，它们的$L_2$敏感度会随着批量大小的增加而增加，这对于差分隐私训练方法（如DP-SGD）特别不利。为了解决这个问题，我们开发了一种新的DP-SGD变体，用于基于相似性的损失函数，特别是常用的对比损失，通过一种新颖的方式处理目标函数的梯度，使得梯度的敏感度对于批量大小是$O(1)$。

    Unsupervised pre-training is a common step in developing computer vision models and large language models. In this setting, the absence of labels requires the use of similarity-based loss functions, such as contrastive loss, that favor minimizing the distance between similar inputs and maximizing the distance between distinct inputs. As privacy concerns mount, training these models using differential privacy has become more important. However, due to how inputs are generated for these losses, one of their undesirable properties is that their $L_2$ sensitivity can grow with increasing batch size. This property is particularly disadvantageous for differentially private training methods, such as DP-SGD. To overcome this issue, we develop a new DP-SGD variant for similarity based loss functions -- in particular the commonly used contrastive loss -- that manipulates gradients of the objective function in a novel way to obtain a senstivity of the summed gradient that is $O(1)$ for batch size
    
[^130]: 面向领域感知的联邦学习的双提示调优

    Dual Prompt Tuning for Domain-Aware Federated Learning. (arXiv:2310.03103v1 [cs.LG])

    [http://arxiv.org/abs/2310.03103](http://arxiv.org/abs/2310.03103)

    本文提出了一种面向领域感知的联邦学习方法，通过双提示调优实现领域适应。实验结果表明，该方法在联邦学习中具有显著的效果。

    

    联邦学习是一种分布式机器学习范 paradigm，它允许多个客户端使用本地数据共同训练一个共享模型。然而，由于客户之间普遍存在领域变化，传统的联邦学习算法往往难以很好地泛化。在这项工作中，我们考虑了一个具有挑战性但现实的联邦学习场景，其中每个客户端的训练数据来自不同的领域。我们通过利用提示学习技术来解决领域变化的挑战，并提出了一种名为联邦双提示调优（Fed-DPT）的新方法。具体而言，Fed-DPT采用了一个预训练的视觉语言模型，然后应用了视觉和文本提示调优来促进分布式数据上的领域适应。大量的Fed-DPT实验结果表明，它在领域感知的联邦学习中具有显著的效果。

    Federated learning is a distributed machine learning paradigm that allows multiple clients to collaboratively train a shared model with their local data. Nonetheless, conventional federated learning algorithms often struggle to generalize well due to the ubiquitous domain shift across clients. In this work, we consider a challenging yet realistic federated learning scenario where the training data of each client originates from different domains. We address the challenges of domain shift by leveraging the technique of prompt learning, and propose a novel method called Federated Dual Prompt Tuning (Fed-DPT). Specifically, Fed-DPT employs a pre-trained vision-language model and then applies both visual and textual prompt tuning to facilitate domain adaptation over decentralized data. Extensive experiments of Fed-DPT demonstrate its significant effectiveness in domain-aware federated learning. With a pre-trained CLIP model (ViT-Base as image encoder), the proposed Fed-DPT attains 68.4% av
    
[^131]: 基于思维混合表示的大规模语言模型级联用于成本高效的推理

    Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])

    [http://arxiv.org/abs/2310.03094](http://arxiv.org/abs/2310.03094)

    本研究提出了一种基于思维混合表示的大规模语言模型级联方法，用于成本高效的推理。通过考虑更弱模型的答案一致性作为问题难度的信号，可以实现对问题的决策，从而节约使用更强模型的成本。

    

    大规模语言模型（LLM）如GPT-4在各种任务中展现出了非凡的性能，但是这种强大的性能通常伴随着使用付费API服务的高昂费用。本文的研究动机是为了研究构建LLM级联以节约使用LLM的成本，特别是用于进行推理（例如数学、因果推理）任务的成本。我们的级联管道遵循一个直观的思想，即简单的问题可以由一个更弱但更实惠的LLM来解决，而只有具有挑战性的问题才需要更强大、更昂贵的LLM。为了实现这种决策，我们考虑到更弱的LLM的“答案一致性”作为问题难度的信号，并提出了几种答案采样和一致性检查的方法，其中一种方法利用了两种思维表示（即连续思维和程序思维）的混合。通过在六个推理基准数据集上的实验，我们使用GPT-3.5-turbo和GPT-4作为较弱的模型，

    Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
    
[^132]: 物理信息神经网络加速电力系统状态估计

    Physics-Informed Neural Networks for Accelerating Power System State Estimation. (arXiv:2310.03088v1 [cs.LG])

    [http://arxiv.org/abs/2310.03088](http://arxiv.org/abs/2310.03088)

    本研究提出了一种利用物理信息神经网络加速电力系统状态估计的方法，通过整合内在的物理知识，显著降低了计算复杂度并保持高精度，实验证明其准确性提升了11％，结果的标准差减小了75％，收敛速度加快了30％。

    

    状态估计是电力系统控制中心的基石，因为它提供了系统在连续时间间隔内的运行状态。本研究研究了物理信息神经网络（PINNs）在加速电力系统状态估计中的应用，以监测电力系统的运行情况。传统的状态估计技术往往依赖于迭代算法，对于大规模电力系统来说可能计算复杂度很高。本文提出了一种新的方法，通过整合PINNs的内在物理知识，显著降低了状态估计的计算复杂度，同时保持高精度。实验证明，该方法在准确性方面提升了11％，结果的标准差减小了75％，收敛速度加快了30％。

    State estimation is the cornerstone of the power system control center since it provides the operating condition of the system in consecutive time intervals. This work investigates the application of physics-informed neural networks (PINNs) for accelerating power systems state estimation in monitoring the operation of power systems. Traditional state estimation techniques often rely on iterative algorithms that can be computationally intensive, particularly for large-scale power systems. In this paper, a novel approach that leverages the inherent physical knowledge of power systems through the integration of PINNs is proposed. By incorporating physical laws as prior knowledge, the proposed method significantly reduces the computational complexity associated with state estimation while maintaining high accuracy. The proposed method achieves up to 11% increase in accuracy, 75% reduction in standard deviation of results, and 30% faster convergence, as demonstrated by comprehensive experim
    
[^133]: 计算生物学中的深度学习：进展、挑战和未来展望

    Deep Learning in Computational Biology: Advancements, Challenges, and Future Outlook. (arXiv:2310.03086v1 [cs.LG])

    [http://arxiv.org/abs/2310.03086](http://arxiv.org/abs/2310.03086)

    深度学习在计算生物学中的应用带来了重大变革，既能够改善DNA序列分析和蛋白质结构预测，也为基因组变异检测和基因表达分析提供了新的方法。

    

    深度学习已经成为计算生物学中一个强大的工具，革新了生物数据的分析和解释。在我们的文章中，我们深入探讨了计算生物学中深度学习的各个方面。具体而言，我们研究了其历史、优势和挑战。我们的重点是两个主要应用领域：DNA序列分类和预测，以及从序列数据中预测蛋白质结构。此外，我们还提供了对该领域未来发展的见解。要充分发挥深度学习在计算生物学中的潜力，关键是解决所带来的挑战。这些挑战包括对大规模标记数据集的需求以及深度学习模型的可解释性。深度学习在分析DNA序列方面的应用已经在基因组变异检测和基因表达分析方面带来了重大变革，从而为该领域的进展做出了重要贡献。

    Deep learning has become a powerful tool in computational biology, revolutionising the analysis and interpretation of biological data over time. In our article review, we delve into various aspects of deep learning in computational biology. Specifically, we examine its history, advantages, and challenges. Our focus is on two primary applications: DNA sequence classification and prediction, as well as protein structure prediction from sequence data. Additionally, we provide insights into the outlook for this field. To fully harness the potential of deep learning in computational biology, it is crucial to address the challenges that come with it. These challenges include the requirement for large, labelled datasets and the interpretability of deep learning models. The use of deep learning in the analysis of DNA sequences has brought about a significant transformation in the detection of genomic variants and the analysis of gene expression. This has greatly contributed to the advancement 
    
[^134]: 无批量随机梯度下降用于图像去噪的深度正则化压缩学习

    Batch-less stochastic gradient descent for compressive learning of deep regularization for image denoising. (arXiv:2310.03085v1 [cs.LG])

    [http://arxiv.org/abs/2310.03085](http://arxiv.org/abs/2310.03085)

    本文介绍了一种无批量随机梯度下降方法，用于图像去噪的深度正则化压缩学习。该方法通过利用来自干净信号或图像数据库的先验信息，将正则化器与数据分布关联起来，从大规模训练数据库中恢复复杂的分布，以减少计算负担。

    

    本文考虑了利用来自干净信号或图像数据库的先验信息进行去噪的问题。如果能够获得适应数据性质的正则化器，采用变分方法进行去噪非常高效。通过最大后验贝叶斯框架，这样的正则化器可以与数据的分布系统地关联起来。利用深度神经网络(DNN)，可以从一大规模的训练数据库中恢复复杂的分布。为了减少任务的计算负担，我们将压缩学习框架调整为用DNN参数化的正则化器的学习方法。我们提出了两种随机梯度下降(SGD)的变体，用于从一个被大幅压缩的数据库中恢复深度正则化参数。这些算法优于最初提出的只针对低维信号的方法，每个迭代都使用整个数据库的信息。它们还可以受益于经典的SGD收敛保证。

    We consider the problem of denoising with the help of prior information taken from a database of clean signals or images. Denoising with variational methods is very efficient if a regularizer well adapted to the nature of the data is available. Thanks to the maximum a posteriori Bayesian framework, such regularizer can be systematically linked with the distribution of the data. With deep neural networks (DNN), complex distributions can be recovered from a large training database.To reduce the computational burden of this task, we adapt the compressive learning framework to the learning of regularizers parametrized by DNN. We propose two variants of stochastic gradient descent (SGD) for the recovery of deep regularization parameters from a heavily compressed database. These algorithms outperform the initially proposed method that was limited to low-dimensional signals, each iteration using information from the whole database. They also benefit from classical SGD convergence guarantees. 
    
[^135]: 在预训练语言模型中发现关键知识子网络

    Discovering Knowledge-Critical Subnetworks in Pretrained Language Models. (arXiv:2310.03084v1 [cs.CL])

    [http://arxiv.org/abs/2310.03084](http://arxiv.org/abs/2310.03084)

    本研究调查了预训练语言模型中是否存在各种关键知识子网络，即负责编码特定知识的稀疏计算子图。通过提出的可微分权重屏蔽方案，我们可以精确地删除特定知识，又最小化对原始语言模型的负面影响。

    

    预训练语言模型在其参数中编码了隐含的知识表示，然而，定位这些表示并将其解离出来仍然是一个未解决的问题。本研究探讨了预训练语言模型是否包含了各种关键知识子网络：负责编码模型所记忆的特定知识的特定稀疏计算子图。我们提出了一个多目标可微分权重屏蔽方案来发现这些子网络，并表明我们可以使用它们来精确地从模型中删除特定知识，同时最小化对原始语言模型行为的不良影响。我们在多个GPT2变体上展示了我们的方法，揭示了高度稀疏子网络（98%+），它们仅负责特定的关系知识集合。当删除这些子网络时，剩余的网络仍保持了大部分其初始容量（对语言和其他记忆关系的建模）。

    Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized rel
    
[^136]: Point-PEFT: 用于3D预训练模型的参数高效微调

    Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])

    [http://arxiv.org/abs/2310.03059](http://arxiv.org/abs/2310.03059)

    Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。

    

    大型预训练模型的流行已经彻底改变了语言、视觉和多模态等领域的下游任务。为了降低下游任务的适应成本，许多参数高效微调（PEFT）技术被提出用于语言和2D图像预训练模型。然而，对于3D预训练模型的专门PEFT方法仍未得到充分探索。为此，我们引入了Point-PEFT，一种用于适应点云预训练模型的新型框架，其具有最少的可学习参数。具体而言，对于预训练的3D模型，我们冻结大部分参数，只微调新增的PEFT模块。这些模块包括Point-prior Prompt和Geometry-aware Adapter。Point-prior Prompt采用一组可学习的提示标记，并提出使用具有领域特定知识的内存库来增强提示标记的参数无关的注意力机制。Geometry-aware Adapter旨在对不同任务或数据进行准确地聚合。

    The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
    
[^137]: 修改后的LAB算法及基于聚类的搜索空间缩减方法在工程设计问题求解中的应用

    Modified LAB Algorithm with Clustering-based Search Space Reduction Method for solving Engineering Design Problems. (arXiv:2310.03055v1 [cs.LG])

    [http://arxiv.org/abs/2310.03055](http://arxiv.org/abs/2310.03055)

    本文介绍了一种修改后的LAB算法，并提出了基于聚类的搜索空间缩减方法，该算法在工程设计问题求解中表现出改进的稳健性和搜索空间探索能力，同时能够解决约束问题。

    

    本文介绍了一种修改后的LAB算法。该算法建立在原始的LAB算法（Reddy等人，2023年）基础上，该算法模拟了群体内部的竞争和学习行为，建立了层次角色。所提出的算法利用了轮盘赌方法和缩减因子，引入了群组间竞争，并逐步缩小样本空间。该算法通过解决CEC 2005和CEC 2017的基准测试问题进行验证。使用双边和成对符号秩Wilcoxon测试以及Friedman秩测试对解决方案进行验证。该算法具有改进和卓越的稳健性以及搜索空间探索能力。此外，本文还提出了一种基于聚类的搜索空间缩减（C-SSR）方法，使算法能够解决约束问题。C-SSR方法使算法能够识别满足约束条件的可行区域之间的聚类。

    A modified LAB algorithm is introduced in this paper. It builds upon the original LAB algorithm (Reddy et al. 2023), which is a socio-inspired algorithm that models competitive and learning behaviours within a group, establishing hierarchical roles. The proposed algorithm incorporates the roulette wheel approach and a reduction factor introducing inter-group competition and iteratively narrowing down the sample space. The algorithm is validated by solving the benchmark test problems from CEC 2005 and CEC 2017. The solutions are validated using standard statistical tests such as two-sided and pairwise signed rank Wilcoxon test and Friedman rank test. The algorithm exhibited improved and superior robustness as well as search space exploration capabilities. Furthermore, a Clustering-Based Search Space Reduction (C-SSR) method is proposed, making the algorithm capable to solve constrained problems. The C-SSR method enables the algorithm to identify clusters of feasible regions, satisfying 
    
[^138]: 基于负距离核的最大平均距离(MMD)梯度流的后验抽样

    Posterior Sampling Based on Gradient Flows of the MMD with Negative Distance Kernel. (arXiv:2310.03054v1 [stat.ML])

    [http://arxiv.org/abs/2310.03054](http://arxiv.org/abs/2310.03054)

    本文提出了一种基于负距离核的最大平均距离(MMD)的条件流方法，用于后验抽样和条件生成建模。通过离散的Wasserstein梯度流近似联合分布，证明了粒子流是适当功能的Wasserstein梯度流。在条件图像生成和超分辨率等逆问题中展示了方法的有效性。

    

    我们提出了基于负距离核的最大平均距离(MMD)的条件流用于后验抽样和条件生成建模。这个MMD，也被称为能量距离，具有像通过切片和排序进行高效计算的几个有益属性。我们使用离散的Wasserstein梯度流来近似真实情况和观察值的联合分布，并为后验分布建立了误差界限。此外，我们证明了我们的粒子流确实是适当功能的Wasserstein梯度流。我们方法的能力通过数字示例进行了演示，包括条件图像生成和诸如超分辨率、修复和低剂量和有限角度设置下的计算机断层扫描等逆问题。

    We propose conditional flows of the maximum mean discrepancy (MMD) with the negative distance kernel for posterior sampling and conditional generative modeling. This MMD, which is also known as energy distance, has several advantageous properties like efficient computation via slicing and sorting. We approximate the joint distribution of the ground truth and the observations using discrete Wasserstein gradient flows and establish an error bound for the posterior distributions. Further, we prove that our particle flow is indeed a Wasserstein gradient flow of an appropriate functional. The power of our method is demonstrated by numerical examples including conditional image generation and inverse problems like superresolution, inpainting and computed tomography in low-dose and limited-angle settings.
    
[^139]: Memoria: 用于类人顺序处理的海比安记忆体架构

    Memoria: Hebbian Memory Architecture for Human-Like Sequential Processing. (arXiv:2310.03052v1 [cs.LG])

    [http://arxiv.org/abs/2310.03052](http://arxiv.org/abs/2310.03052)

    Memoria 是一个通用记忆网络，应用海比安理论来增强神经网络中的长期依赖。通过存储和检索信息，并使用根据海布规则变化的连接权重，Memoria 在诸如 BERT 和 GPT 之类的流行 Transformer 模型上显著改进了考虑长期依赖的能力。

    

    Transformer 在多个领域和任务中取得了成功。然而，由于其有限的容量，Transformer 很难处理长输入序列。虽然增加输入长度是一个解决方案，但无止境地增加长度是不现实的。此外，与 Transformer 不同，人类有选择性地记住和使用仅与输入相关的信息，而不是从头到尾处理所有原始数据。我们引入了 Memoria，一个应用海比安记忆形成理论的通用记忆网络，用于增强神经网络中的长期依赖。Memoria 在工作记忆、短期记忆和长期记忆的多个记忆层级上存储和检索称为 engram 的信息，使用根据海布规则变化的连接权重。通过与诸如 BERT 和 GPT 等流行的基于 Transformer 的模型进行实验，我们提出 Memoria 显著提高了在各种任务中考虑长期依赖的能力。结果

    Transformers have demonstrated their success in various domains and tasks. However, Transformers struggle with long input sequences due to their limited capacity. While one solution is to increase input length, endlessly stretching the length is unrealistic. Furthermore, humans selectively remember and use only relevant information from inputs, unlike Transformers which process all raw data from start to end. We introduce Memoria, a general memory network that applies Hebbian theory which is a major theory explaining human memory formulation to enhance long-term dependencies in neural networks. Memoria stores and retrieves information called engram at multiple memory levels of working memory, short-term memory, and long-term memory, using connection weights that change according to Hebb's rule. Through experiments with popular Transformer-based models like BERT and GPT, we present that Memoria significantly improves the ability to consider long-term dependencies in various tasks. Resul
    
[^140]: QuATON: 光纤神经元的量化感知训练

    QuATON: Quantization Aware Training of Optical Neurons. (arXiv:2310.03049v1 [cs.LG])

    [http://arxiv.org/abs/2310.03049](http://arxiv.org/abs/2310.03049)

    提出一种光纤神经元的量化感知训练方法，通过考虑物理约束，实现了对光纤神经架构的鲁棒设计。

    

    光学神经架构（ONA）使用具有优化物理参数的编码元件进行智能测量。然而，制造具有设计性能的ONA是具有挑战性的。制造技术的限制通常限制了训练参数的可实现精度。物理约束也可能限制物理参数可以容纳的值范围。因此，ONA应在可实现的约束条件下进行训练。然而，这种基于物理的约束将训练目标转化为一个约束优化问题，使其更难以利用现有的梯度方法进行优化。为了缓解这些从模拟到实现的关键问题导致性能下降的问题，我们提出了一个基于物理信息的量化感知训练框架。我们的方法在文献中提出的一种ONA上进行了评估，该ONA被称为衍射深度神经网络。

    Optical neural architectures (ONAs) use coding elements with optimized physical parameters to perform intelligent measurements. However, fabricating ONAs while maintaining design performances is challenging. Limitations in fabrication techniques often limit the realizable precision of the trained parameters. Physical constraints may also limit the range of values the physical parameters can hold. Thus, ONAs should be trained within the implementable constraints. However, such physics-based constraints reduce the training objective to a constrained optimization problem, making it harder to optimize with existing gradient-based methods. To alleviate these critical issues that degrade performance from simulation to realization we propose a physics-informed quantization-aware training framework. Our approach accounts for the physical constraints during the training process, leading to robust designs. We evaluate our approach on an ONA proposed in the literature, named a diffractive deep ne
    
[^141]: 通过几何深度学习实现可微分的化学物理模型，用于基于梯度的混合物性能优化

    Differentiable Chemical Physics by Geometric Deep Learning for Gradient-based Property Optimization of Mixtures. (arXiv:2310.03047v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03047](http://arxiv.org/abs/2310.03047)

    本文开发了一个不同iable的化学物理框架DiffMix，利用几何深度学习将分子物种、组成和环境条件映射到混合物物理定律中的物理系数上。通过创建可学习的物理系数，我们扩展了混合物热力学和输运定律，并展示了DiffMix相比纯数据驱动变量改进的预测准确性和模型鲁棒性。

    

    化学混合物满足多目标性能度量和约束条件，可用于化学过程和电化学设备。在这项工作中，我们开发了一种不同iable的化学物理框架DiffMix，利用几何深度学习（GDL）将分子物种，组成和环境条件映射到混合物物理定律中的物理系数上。我们通过创建可学习的物理系数来扩展混合物热力学和输运定律，其中我们使用图神经网络作为分子编码器，并强制实现逐分量的排列不变性。我们从二元混合物的热力学开始评估我们的模型，并进一步在多组分电解质混合物上对其输运性质进行基准测试，以测试模型的泛化能力。我们展示了DiffMix的预测准确性和模型鲁棒性优于纯数据驱动变量的改进。

    Chemical mixtures, satisfying multi-objective performance metrics and constraints, enable their use in chemical processes and electrochemical devices. In this work, we develop a differentiable chemical-physics framework for modeling chemical mixtures, DiffMix, where geometric deep learning (GDL) is leveraged to map from molecular species, compositions and environment conditions, to physical coefficients in the mixture physics laws. In particular, we extend mixture thermodynamic and transport laws by creating learnable physical coefficients, where we use graph neural networks as the molecule encoder and enforce component-wise permutation-invariance. We start our model evaluations with thermodynamics of binary mixtures, and further benchmarked multicomponent electrolyte mixtures on their transport properties, in order to test the model generalizability. We show improved prediction accuracy and model robustness of DiffMix than its purely data-driven variants. Furthermore, we demonstrate t
    
[^142]: 一种基于深度强化学习的交互式搜索方法与句级反馈

    A Deep Reinforcement Learning Approach for Interactive Search with Sentence-level Feedback. (arXiv:2310.03043v1 [cs.LG])

    [http://arxiv.org/abs/2310.03043](http://arxiv.org/abs/2310.03043)

    本研究提出了一种利用深度强化学习的交互式搜索方法，该方法通过整合句级反馈信息来提高搜索准确性。通过适应最新的BERT-based模型进行关键句子选择和项目排序，可以获得更满意的搜索结果。

    

    交互式搜索可以通过整合用户的交互反馈来提供更好的搜索体验。这可以显著提高搜索准确性，因为它有助于避免无关信息并捕捉用户的搜索意图。现有的最新系统使用强化学习（RL）模型来整合这些交互，但是忽略了句级反馈中的细粒度信息。然而，这种反馈需要进行广泛的RL行动空间探索和大量的标注数据。本研究通过提出一种新的深度Q学习（DQ）方法DQrank来解决这些挑战。DQrank使用自然语言处理中最新技术BERT-based模型来选择关键句子，并基于用户的参与度对项目进行排序，以获得更满意的回应。我们还提出了两种机制来更好地探索最优行动。DQrank还利用DQ中的经验重现机制来存储反馈句子以增强模型性能。

    Interactive search can provide a better experience by incorporating interaction feedback from the users. This can significantly improve search accuracy as it helps avoid irrelevant information and captures the users' search intents. Existing state-of-the-art (SOTA) systems use reinforcement learning (RL) models to incorporate the interactions but focus on item-level feedback, ignoring the fine-grained information found in sentence-level feedback. Yet such feedback requires extensive RL action space exploration and large amounts of annotated data. This work addresses these challenges by proposing a new deep Q-learning (DQ) approach, DQrank. DQrank adapts BERT-based models, the SOTA in natural language processing, to select crucial sentences based on users' engagement and rank the items to obtain more satisfactory responses. We also propose two mechanisms to better explore optimal actions. DQrank further utilizes the experience replay mechanism in DQ to store the feedback sentences to ob
    
[^143]: 提升交通标志识别的高准确性二进制神经网络的本地稳健性基准测试

    Benchmarking Local Robustness of High-Accuracy Binary Neural Networks for Enhanced Traffic Sign Recognition. (arXiv:2310.03033v1 [cs.CV])

    [http://arxiv.org/abs/2310.03033](http://arxiv.org/abs/2310.03033)

    这项研究基于二进制神经网络模型，通过引入一组基准问题来评估交通标志识别模型的本地稳健性。这些问题考虑了网络参数的数量、输入维度和区域数量，并挑战了最先进的验证工具。

    

    交通标志在自动驾驶系统中对道路安全和交通管理起着关键作用。由于真实世界的复杂性，如对抗性示例和遮挡等，准确的交通标志分类具有挑战性。为了解决这些问题，二进制神经网络在构建适用于资源受限设备的分类器方面具有潜力。在我们之前的工作中，我们提出了适用于交通标志识别的高准确性BNN模型，重点是紧凑的大小，以适应有限的计算和能源资源。为了评估它们的本地稳健性，本文引入了一组基准问题，其中的层挑战了最先进的验证工具。这些层包括二进制卷积、最大池化、批归一化和全连接层。验证问题的困难程度由网络参数的数量（905k-1.7M）、输入维度（2.7k-12k）和区域数量（43）以及神经元的事实决定。

    Traffic signs play a critical role in road safety and traffic management for autonomous driving systems. Accurate traffic sign classification is essential but challenging due to real-world complexities like adversarial examples and occlusions. To address these issues, binary neural networks offer promise in constructing classifiers suitable for resource-constrained devices.  In our previous work, we proposed high-accuracy BNN models for traffic sign recognition, focusing on compact size for limited computation and energy resources. To evaluate their local robustness, this paper introduces a set of benchmark problems featuring layers that challenge state-of-the-art verification tools. These layers include binarized convolutions, max pooling, batch normalization, fully connected. The difficulty of the verification problem is given by the high number of network parameters (905k - 1.7 M), of the input dimension (2.7k-12k), and of the number of regions (43) as well by the fact that the neur
    
[^144]: 图增强优化器用于结构感知推荐嵌入演化

    Graph-enhanced Optimizers for Structure-aware Recommendation Embedding Evolution. (arXiv:2310.03032v1 [cs.IR])

    [http://arxiv.org/abs/2310.03032](http://arxiv.org/abs/2310.03032)

    本文提出了一种新颖的结构感知嵌入演化(SEvo)机制，能够以较低的计算开销将图结构信息注入到嵌入中，从而在现代推荐系统中实现更高效的性能。

    

    嵌入在现代推荐系统中起着关键作用，因为它们是真实世界实体的虚拟表示，并且是后续决策模型的基础。本文提出了一种新颖的嵌入更新机制，称为结构感知嵌入演化(SEvo)，以鼓励相关节点在每一步中以类似的方式演化。与通常作为中间部分的GNN（图神经网络）不同，SEvo能够直接将图结构信息注入到嵌入中，且在训练过程中计算开销可忽略。本文通过理论分析验证了SEvo的收敛性质及其可能的改进版本，以证明设计的有效性。此外，SEvo可以无缝集成到现有的优化器中，以实现最先进性能。特别是，在矩估计校正的SEvo增强AdamW中，证明了一致的改进效果在多种模型和数据集上，为有效推荐了一种新的技术路线。

    Embedding plays a critical role in modern recommender systems because they are virtual representations of real-world entities and the foundation for subsequent decision models. In this paper, we propose a novel embedding update mechanism, Structure-aware Embedding Evolution (SEvo for short), to encourage related nodes to evolve similarly at each step. Unlike GNN (Graph Neural Network) that typically serves as an intermediate part, SEvo is able to directly inject the graph structure information into embedding with negligible computational overhead in training. The convergence properties of SEvo as well as its possible variants are theoretically analyzed to justify the validity of the designs. Moreover, SEvo can be seamlessly integrated into existing optimizers for state-of-the-art performance. In particular, SEvo-enhanced AdamW with moment estimate correction demonstrates consistent improvements across a spectrum of models and datasets, suggesting a novel technical route to effectively 
    
[^145]: ChatGPT中的性别偏见有多普遍？—— 探索德语和英语ChatGPT的回应

    How Prevalent is Gender Bias in ChatGPT? -- Exploring German and English ChatGPT Responses. (arXiv:2310.03031v1 [cs.CL])

    [http://arxiv.org/abs/2310.03031](http://arxiv.org/abs/2310.03031)

    本文系统地分析并探索了德语和英语的ChatGPT回应中可能存在的问题，特别关注了性别偏见。我们发现，在对系统多次提供相同指令的情况下，回应存在差异。使用ChatGPT来帮助非IT用户撰写工作文本非常有用，但用户需要充分考虑系统的固有限制。

    

    随着ChatGPT的推出，OpenAI使得大型语言模型（LLM）可供具有有限IT专业知识的用户使用。然而，没有自然语言处理（NLP）背景的用户可能缺乏对LLM的适当理解。因此，在处理系统输出时，缺乏对其固有限制的意识，将接受系统输出的表面价值。在本文中，我们系统地分析输入提示和生成的回应，以识别可能存在的问题，特别关注性别偏见问题，用户在处理系统输出时需要意识到这一点。我们探索了ChatGPT在英语和德语中的反应，并提供了女性、男性或中立角度的指令时，回复的是否有差异。通过深入调查，我们研究了一些选择的提示，并分析了系统在相同方式下多次提供指令时回应的差异程度。在此基础上，我们展示了对于帮助非IT用户撰写日常工作文本，ChatGPT确实非常有用。然而，当然至关重要的是要意识到，当处理系统输出时，用户需要充分考虑到其固有限制。

    With the introduction of ChatGPT, OpenAI made large language models (LLM) accessible to users with limited IT expertise. However, users with no background in natural language processing (NLP) might lack a proper understanding of LLMs. Thus the awareness of their inherent limitations, and therefore will take the systems' output at face value. In this paper, we systematically analyse prompts and the generated responses to identify possible problematic issues with a special focus on gender biases, which users need to be aware of when processing the system's output. We explore how ChatGPT reacts in English and German if prompted to answer from a female, male, or neutral perspective. In an in-depth investigation, we examine selected prompts and analyse to what extent responses differ if the system is prompted several times in an identical way. On this basis, we show that ChatGPT is indeed useful for helping non-IT users draft texts for their daily work. However, it is absolutely crucial to 
    
[^146]: GPT-MolBERTa：用于分子性质预测的GPT分子特征语言模型

    GPT-MolBERTa: GPT Molecular Features Language Model for molecular property prediction. (arXiv:2310.03030v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03030](http://arxiv.org/abs/2310.03030)

    GPT-MolBERTa是一种用于分子性质预测的自监督大型语言模型，通过使用分子的详细文本描述来学习分子的表示，实验表明其在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。

    

    随着Transformer架构的出现及其对文本数据的强大理解能力，基于文本描述预测分子属性的新领域已经开启。尽管SMILES是最常见的表示形式，但它们缺乏健壮性、丰富信息和规范性，限制了它们成为可推广表示的有效性。在这里，我们提出了GPT-MolBERTa，一种自监督大型语言模型（LLM），它使用分子的详细文本描述来预测其性质。使用ChatGPT收集了326000个分子的基于文本的描述，并用于训练LLM来学习分子的表示。为了预测下游任务的性能，细调阶段使用了BERT和RoBERTa模型。实验证明，GPT-MolBERTa在各种分子属性基准上表现良好，并在回归任务中接近最先进的性能。此外，对注意力进行了进一步分析。

    With the emergence of Transformer architectures and their powerful understanding of textual data, a new horizon has opened up to predict the molecular properties based on text description. While SMILES are the most common form of representation, they are lacking robustness, rich information and canonicity, which limit their effectiveness in becoming generalizable representations. Here, we present GPT-MolBERTa, a self-supervised large language model (LLM) which uses detailed textual descriptions of molecules to predict their properties. A text based description of 326000 molecules were collected using ChatGPT and used to train LLM to learn the representation of molecules. To predict the properties for the downstream tasks, both BERT and RoBERTa models were used in the finetuning stage. Experiments show that GPT-MolBERTa performs well on various molecule property benchmarks, and approaching state of the art performance in regression tasks. Additionally, further analysis of the attention 
    
[^147]: SAF: 智能聚合框架，揭示原子重要性排名并提高药物发现中的预测率

    SAF: Smart Aggregation Framework for Revealing Atoms Importance Rank and Improving Prediction Rates in Drug Discovery. (arXiv:2310.03028v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03028](http://arxiv.org/abs/2310.03028)

    SAF框架提出了一种新的聚合方法，使用温度超参数对原子加权，并证明其可以提高信息传递神经网络的预测能力，在药物发现中揭示重要的原子。

    

    机器学习，尤其是表示学习，有潜力通过在硅基中筛选大量化学空间来促进药物发现。表示分子的一种成功方法是将其视为图形并利用图形神经网络。这类方法的一个关键限制是需要使用不同数量的原子来表示化合物，这需要对原子信息进行聚合。常见的聚合操作，如取平均，会导致在原子级别上丢失信息。在这项工作中，我们提出了一种新的聚合方法，其中每个原子都使用类似于温度的超参数进行非线性加权，采用玻尔兹曼分布。我们证明使用这种加权聚合方法可以提高黄金标准信息传递神经网络预测抗生素活性的能力。此外，通过改变温度超参数，我们的方法可以平滑地揭示对活性预测重要的原子。

    Machine learning, and representation learning in particular, has the potential to facilitate drug discovery by screening a large chemical space in silico. A successful approach for representing molecules is to treat them as a graph and utilize graph neural networks. One of the key limitations of such methods is the necessity to represent compounds with different numbers of atoms, which requires aggregating the atom's information. Common aggregation operators, such as averaging, result in loss of information at the atom level. In this work, we propose a novel aggregating approach where each atom is weighted non-linearly using the Boltzmann distribution with a hyperparameter analogous to temperature. We show that using this weighted aggregation improves the ability of the gold standard message-passing neural network to predict antibiotic activity. Moreover, by changing the temperature hyperparameter, our approach can reveal the atoms that are important for activity prediction in a smooth
    
[^148]: 图和Transformer特征的协同融合可增强分子性质预测

    Synergistic Fusion of Graph and Transformer Features for Enhanced Molecular Property Prediction. (arXiv:2310.03027v1 [physics.chem-ph])

    [http://arxiv.org/abs/2310.03027](http://arxiv.org/abs/2310.03027)

    本论文提出了一种名为SYNFUSION的新方法，通过协同融合GNN和Transformer的预训练特征，实现了全面的分子表示，在分子性质预测任务中超越了以往的模型。

    

    分子性质预测是计算药物发现中的关键任务。最近图神经网络（GNN）和Transformer的进展表明它们是有效且有希望的，但它们存在以下限制：Transformer自注意机制在明确考虑底层分子结构方面不足，而仅依靠GNN特征表示不足以捕捉细粒度和隐藏的分子间相互作用和特征，这使得相似分子之间难以区分。为了解决这些限制，我们提出了SYNFUSION，一种新颖的方法，它协同组合了来自GNN和Transformer的预训练特征。这种方法提供了全面的分子表示，捕捉了全局分子结构和各个原子的特性。在MoleculeNet基准测试中的实验结果表明，SYNFUSION具有优越的性能，在5个分类数据集和6个回归数据集中超过了以前的模型。

    Molecular property prediction is a critical task in computational drug discovery. While recent advances in Graph Neural Networks (GNNs) and Transformers have shown to be effective and promising, they face the following limitations: Transformer self-attention does not explicitly consider the underlying molecule structure while GNN feature representation alone is not sufficient to capture granular and hidden interactions and characteristics that distinguish similar molecules. To address these limitations, we propose SYNFUSION, a novel approach that synergistically combines pre-trained features from GNNs and Transformers. This approach provides a comprehensive molecular representation, capturing both the global molecule structure and the individual atom characteristics. Experimental results on MoleculeNet benchmarks demonstrate superior performance, surpassing previous models in 5 out of 7 classification datasets and 4 out of 6 regression datasets. The performance of SYN-FUSION has been
    
[^149]: IBCL：连续学习中零样本模型生成用于任务权衡

    IBCL: Zero-shot Model Generation for Task Trade-offs in Continual Learning. (arXiv:2310.02995v1 [cs.LG])

    [http://arxiv.org/abs/2310.02995](http://arxiv.org/abs/2310.02995)

    IBCL提出了一种用于连续学习中任务权衡的零样本模型生成方法，通过更新知识库并利用模型参数分布的凸包形式，实现不同任务性能之间的权衡偏好。

    

    类似于通用的多任务学习，连续学习具有多目标优化的特性，因此面临着不同任务性能之间的权衡。也就是说，为了优化当前任务分布，可能需要在一些先前的任务上牺牲性能。这意味着在不同时间点存在多个帕累托最优的模型，每个模型都解决了不同的任务性能权衡问题。研究人员讨论了如何训练特定的模型来解决特定的权衡偏好。然而，现有的算法需要与偏好数量成比例的训练开销，当存在多个甚至是无限多个偏好时，这是一个巨大的负担。作为响应，我们提出了Imprecise Bayesian Continual Learning (IBCL)。在新任务出现时，IBCL(1)通过模型参数分布的凸包形式更新知识库，(2)获得了特定模型，以实现零样本的任务权衡偏好。

    Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some previous tasks. This means that there exist multiple models that are Pareto-optimal at different times, each addressing a distinct task performance trade-off. Researchers have discussed how to train particular models to address specific trade-off preferences. However, existing algorithms require training overheads proportional to the number of preferences -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address task trade-off preferences with zero-shot. That is,
    
[^150]: 合模式化肽的顺序和图形路径

    Co-modeling the Sequential and Graphical Route for Peptide. (arXiv:2310.02964v1 [cs.LG])

    [http://arxiv.org/abs/2310.02964](http://arxiv.org/abs/2310.02964)

    本论文提出了一种肽合模式化方法，使用对比学习框架来增强从顺序和图形模型中学到的表示的相互信息，以提高肽的判别性能。

    

    肽是由多个氨基酸的脱水缩合形成的。肽的主要结构可以表示为氨基酸序列或由原子和化学键组成的分子图。先前的研究表明，针对顺序和图形肽形式的深度学习路径在下游任务上表现相似。尽管这些模型学习了同一种肽的表示，但我们发现它们解释其预测的方式不同。将顺序和图形模型视为从不同角度进行推理的两个专家，我们致力于融合专家知识，丰富学到的表示以提高判别性能。为实现这一目标，我们提出了一种肽合模式化方法RepCon，它采用对比学习框架，增强从解耦的顺序和图形端到端模型的表示的相互信息。

    Peptides are formed by the dehydration condensation of multiple amino acids. The primary structure of a peptide can be represented either as an amino acid sequence or as a molecular graph consisting of atoms and chemical bonds. Previous studies have indicated that deep learning routes specific to sequential and graphical peptide forms exhibit comparable performance on downstream tasks. Despite the fact that these models learn representations of the same modality of peptides, we find that they explain their predictions differently. Considering sequential and graphical models as two experts making inferences from different perspectives, we work on fusing expert knowledge to enrich the learned representations for improving the discriminative performance. To achieve this, we propose a peptide co-modeling method, RepCon, which employs a contrastive learning-based framework to enhance the mutual information of representations from decoupled sequential and graphical end-to-end models. It cons
    
[^151]: Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究

    Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection. (arXiv:2310.02861v1 [cs.LG])

    [http://arxiv.org/abs/2310.02861](http://arxiv.org/abs/2310.02861)

    《Rayleigh Quotient Graph Neural Networks用于图级异常检测的研究》提出使用Rayleigh Quotient作为驱动因素，通过探索图的固有光谱特征来实现图级异常检测。

    

    图级异常检测在癌症诊断和酶预测等领域中广泛应用。然而，现有方法无法捕捉到图异常的潜在属性，导致框架设计不可解释和性能不令人满意。在本文中，我们退一步重新研究了异常和正常图之间的光谱差异。我们的主要观察表明，这两个类之间的累计光谱能量存在显著差异。此外，我们证明了图信号的累计光谱能量可以用其瑞利商表示，这表明瑞利商是图异常属性的一个驱动因素。受此启发，我们提出了Rayleigh Quotient Graph Neural Network（RQGNN），这是第一个用于图级异常检测的光谱GNN，为探索异常图的固有光谱特征提供了新的视角。

    Graph-level anomaly detection has gained significant attention as it finds many applications in various domains, such as cancer diagnosis and enzyme prediction. However, existing methods fail to capture the underlying properties of graph anomalies, resulting in unexplainable framework design and unsatisfying performance. In this paper, we take a step back and re-investigate the spectral differences between anomalous and normal graphs. Our main observation shows a significant disparity in the accumulated spectral energy between these two classes. Moreover, we prove that the accumulated spectral energy of the graph signal can be represented by its Rayleigh Quotient, indicating that the Rayleigh Quotient is a driving factor behind the anomalous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network (RQGNN), the first spectral GNN for graph-level anomaly detection, providing a new perspective on exploring the inherent spectral features of anomalous graph
    
[^152]: PostRainBench: 一种全面的降水预测基准和新模型

    PostRainBench: A comprehensive benchmark and a new model for precipitation forecasting. (arXiv:2310.02676v1 [cs.LG])

    [http://arxiv.org/abs/2310.02676](http://arxiv.org/abs/2310.02676)

    PostRainBench是一个全面的降水预测基准，结合AI后处理技术和传统的数值天气预报方法，能够增强准确性并解决复杂的降水预测挑战。

    

    准确的降水预测是一项具有科学和社会重要性的重大挑战。数据驱动方法已经成为解决这个挑战的广泛采用的解决方案。然而，仅依赖数据驱动方法在模拟基础物理过程方面有限，使得准确预测困难。将基于人工智能的后处理技术与传统的数值天气预报（NWP）方法相结合，为提高预测准确性提供了更有效的解决方案。尽管之前进行过后处理的尝试，但由于不同位置的降水数据失衡和多个气象变量之间的复杂关系，准确预测大雨仍然具有挑战性。为了解决这些限制，我们提出了PostRainBench，这是一个全面的多变量NWP后处理基准，包括三个用于NWP后处理降水预测的数据集。我们提出了一种简单而有效的渠道注意力模型CAMT。

    Accurate precipitation forecasting is a vital challenge of both scientific and societal importance. Data-driven approaches have emerged as a widely used solution for addressing this challenge. However, solely relying on data-driven approaches has limitations in modeling the underlying physics, making accurate predictions difficult. Coupling AI-based post-processing techniques with traditional Numerical Weather Prediction (NWP) methods offers a more effective solution for improving forecasting accuracy. Despite previous post-processing efforts, accurately predicting heavy rainfall remains challenging due to the imbalanced precipitation data across locations and complex relationships between multiple meteorological variables. To address these limitations, we introduce the PostRainBench, a comprehensive multi-variable NWP post-processing benchmark consisting of three datasets for NWP post-processing-based precipitation forecasting. We propose CAMT, a simple yet effective Channel Attention
    
[^153]: 超越稳定性：随机Softmax策略梯度方法的收敛分析

    Beyond Stationarity: Convergence Analysis of Stochastic Softmax Policy Gradient Methods. (arXiv:2310.02671v1 [math.OC] CROSS LISTED)

    [http://arxiv.org/abs/2310.02671](http://arxiv.org/abs/2310.02671)

    本论文研究了随机Softmax策略梯度方法的收敛性分析，提出了一种动态策略梯度的组合方法，并通过对参数进行反向训练来更好地利用相关性结构，实现向全局最优值的收敛。

    

    马尔可夫决策过程（MDP）是一种形式化框架，用于建模和解决序贯决策问题。在有限时间范围内，这些问题与最优停止或特定供应链问题以及大型语言模型的训练相关。与无限时间范围内的MDP不同，最优策略并不是稳定的，策略必须在每个时期单独进行学习。实际上，往往同时训练所有参数，忽视了动态规划所暗示的内在结构。本文介绍了一种动态规划和策略梯度的组合方法，称为动态策略梯度，其中参数在时间上以反向方式进行训练。对于表格Softmax参数化，我们对同时和动态策略梯度在精确梯度和采样梯度设置下向全局最优值进行了收敛分析，且没有引入正则化。结果表明，使用动态策略梯度训练可以更好地利用相关性结构，并提供了收敛性证明。

    Markov Decision Processes (MDPs) are a formal framework for modeling and solving sequential decision-making problems. In finite-time horizons such problems are relevant for instance for optimal stopping or specific supply chain problems, but also in the training of large language models. In contrast to infinite horizon MDPs optimal policies are not stationary, policies must be learned for every single epoch. In practice all parameters are often trained simultaneously, ignoring the inherent structure suggested by dynamic programming. This paper introduces a combination of dynamic programming and policy gradient called dynamic policy gradient, where the parameters are trained backwards in time. For the tabular softmax parametrisation we carry out the convergence analysis for simultaneous and dynamic policy gradient towards global optima, both in the exact and sampled gradient settings without regularisation. It turns out that the use of dynamic policy gradient training much better exploi
    
[^154]: 关于自然语言处理中毒性定义的探讨

    On the definition of toxicity in NLP. (arXiv:2310.02357v1 [cs.CL])

    [http://arxiv.org/abs/2310.02357](http://arxiv.org/abs/2310.02357)

    这项研究探讨了毒性的定义模糊性问题，并提出了一种基于定量压力的毒性定义来弥补现有定义的缺点。

    

    毒性检测任务中的根本问题在于毒性的定义模糊不清。谷歌旗下的团队Jigsaw是该领域的领导者之一，他们使用Dixon等人给出的毒性定义：“粗鲁、不尊重或不合理的语言，可能会让某人离开讨论”。人们可以立即看到这个定义的问题，因为它没有给出毒性的定量度量，而且涉及高度主观的文化术语。尽管存在模糊和缺陷，但这个定义已经成为许多研究者广泛采用的实际标准。在这项工作中，我们提出了一种基于定量压力的毒性定义，克服了现有的缺点。

    The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. Jigsaw, a unit within Google and one of the leaders in the field, uses a definition of toxicity given by Dixon et al. - 'rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion'. One can instantly see the issue with this definition, as it gives no quantitative measure of the toxicity and operates with highly subjective cultural terms. Despite all vagueness and flaws, this definition is de-facto widely used by many researchers. In this work we suggest quantative stress-based defenition for the toxicity that overcomes existing shortcomings.
    
[^155]: 概率重连的消息传递神经网络

    Probabilistically Rewired Message-Passing Neural Networks. (arXiv:2310.02156v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02156](http://arxiv.org/abs/2310.02156)

    PR-MPNNs通过概率重连学习加入相关边，并省略对预测任务没有帮助的边，从而增强了表达能力。

    

    消息传递图神经网络（MPNN）作为处理图结构输入的强大工具而出现。然而，它们在固定的输入图结构上操作，忽略了潜在的噪声和缺失信息。此外，它们的局部聚合机制可能导致问题，如过度压缩和在捕捉相关图结构方面的有限表达能力。现有的解决这些挑战的方法主要依赖于启发式方法，往往忽视了底层数据分布。因此，设计了一种原则性的方法，用于学习推断与给定预测任务相关的图结构，仍然是一个未解决的挑战。在这项工作中，利用了最近在精确和可微分的k-子集采样方面的进展，我们设计了概率重连的MPNN (PR-MPNN)，它们学习在省略对预测任务没有帮助的边的同时添加相关的边。我们的理论分析首次探索了PR-MPNN如何增强表达能力，并且我们确定了确切的条件。

    Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable $k$-subset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions un
    
[^156]: DeepHGCN：朝着更深的双曲图卷积网络

    DeepHGCN: Toward Deeper Hyperbolic Graph Convolutional Networks. (arXiv:2310.02027v1 [cs.LG])

    [http://arxiv.org/abs/2310.02027](http://arxiv.org/abs/2310.02027)

    DeepHGCN是一个具有深层架构的双曲图卷积网络，通过引入新的双曲特征转换层和正则化技术，实现了计算效率的极大改进和过度平滑问题的显著减轻。

    

    双曲图卷积网络（HGCN）在提取分层图信息方面展示了巨大潜力。然而，由于昂贵的双曲操作和随着深度增加的过度平滑问题，现有的HGCN受限于浅层架构。尽管在GCNs中已经应用了一些方法来减轻过度平滑问题，但是开发双曲治疗方法面临着不同的挑战，因为操作必须经过精心设计以适应双曲性质。解决以上挑战，本文提出了DeepHGCN，这是第一个具有显著提高计算效率和大大减轻过度平滑效果的深层多层HGCN架构。DeepHGCN具有两个深层HGCN的关键因素：（1）一种新颖的双曲特征转换层，能够实现快速而准确的线性映射；（2）通过有效的双曲残差连接和权重和特征的正则化技术来促进。

    Hyperbolic graph convolutional networks (HGCN) have demonstrated significant potential in extracting information from hierarchical graphs. However, existing HGCNs are limited to shallow architectures, due to the expensive hyperbolic operations and the over-smoothing issue as depth increases. Although in GCNs, treatments have been applied to alleviate over-smoothing, developing a hyperbolic therapy presents distinct challenges since operations should be carefully designed to fit the hyperbolic nature. Addressing the above challenges, in this work, we propose DeepHGCN, the first deep multi-layer HGCN architecture with dramatically improved computational efficiency and substantially alleviated over-smoothing effect. DeepHGCN presents two key enablers of deep HGCNs: (1) a novel hyperbolic feature transformation layer that enables fast and accurate linear maps; and (2) Techniques such as hyperbolic residual connections and regularization for both weights and features facilitated by an effic
    
[^157]: 使用级联扩散模型进行热带气旋预测

    Forecasting Tropical Cyclones with Cascaded Diffusion Models. (arXiv:2310.01690v1 [physics.ao-ph])

    [http://arxiv.org/abs/2310.01690](http://arxiv.org/abs/2310.01690)

    本研究利用级联扩散模型预测热带气旋轨迹和降水模式，通过整合多源数据实现准确的预测，对高度脆弱地区具有重要意义。

    

    随着气候变化，飓风变得更加强烈，基于人工智能模型的预测方法比基于数学模型的传统方法更加经济实惠和易于获取。本研究利用扩散模型通过整合卫星成像、遥感和大气数据，采用级联方法进行飓风轨迹和降水模式的预测，训练数据集包括来自六个主要盆地的51个飓风。实验证明，级联模型的最终预测在36小时内显示准确的预测结果，所有三项任务的结构相似性指数（SSIM）和峰值信噪比（PSNR）的值都超过了0.5和20dB。本研究还强调了扩散模型等人工智能方法在高性能需求（如飓风预测）方面的高效性和计算经济性，使其成为高度脆弱地区的理想选择。

    As cyclones become more intense due to climate change, the rise of AI-based modelling provides a more affordable and accessible approach compared to traditional methods based on mathematical models. This work leverages diffusion models to forecast cyclone trajectories and precipitation patterns by integrating satellite imaging, remote sensing, and atmospheric data, employing a cascaded approach that incorporates forecasting, super-resolution, and precipitation modelling, with training on a dataset of 51 cyclones from six major basins. Experiments demonstrate that the final forecasts from the cascaded models show accurate predictions up to a 36-hour rollout, with SSIM and PSNR values exceeding 0.5 and 20 dB, respectively, for all three tasks. This work also highlights the promising efficiency of AI methods such as diffusion models for high-performance needs, such as cyclone forecasting, while remaining computationally affordable, making them ideal for highly vulnerable regions with crit
    
[^158]: Borges与AI

    Borges and AI. (arXiv:2310.01425v1 [cs.CL])

    [http://arxiv.org/abs/2310.01425](http://arxiv.org/abs/2310.01425)

    这篇论文主张通过探索乔治·路易斯·博尔赫斯的意象来理解大型语言模型和人工智能之间的关系。

    

    许多人认为大型语言模型（LLMs）开启了人工智能（AI）时代。一些人看到了机遇，而其他人则看到了危险。然而，支持者和反对者都通过科幻小说中流行的意象来理解AI。机器是否会变得有感知能力并反抗其创造者？我们是否会经历纸夹夹子启示？在回答这些问题之前，我们首先应该问一下，这种心理意象是否对手头的现象提供了良好的描述。仅通过神灵的情绪来理解天气模式的方法是有限的。相反，本文主张通过乔治·路易斯·博尔赫斯的意象来理解LLMs及其与AI的关系，博尔赫斯是20世纪文学大师，魔幻现实主义先驱和后现代文学的前奏。这种探索方式带来了新的视角，阐明了语言模型与人工智能之间的关系。

    Many believe that Large Language Models (LLMs) open the era of Artificial Intelligence (AI). Some see opportunities while others see dangers. Yet both proponents and opponents grasp AI through the imagery popularised by science fiction. Will the machine become sentient and rebel against its creators? Will we experience a paperclip apocalypse? Before answering such questions, we should first ask whether this mental imagery provides a good description of the phenomenon at hand. Understanding weather patterns through the moods of the gods only goes so far. The present paper instead advocates understanding LLMs and their connection to AI through the imagery of Jorge Luis Borges, a master of 20th century literature, forerunner of magical realism, and precursor to postmodern literature. This exercise leads to a new perspective that illuminates the relation between language modelling and artificial intelligence.
    
[^159]: AI生成文本检测工具的实证研究

    An Empirical Study of AI Generated Text Detection Tools. (arXiv:2310.01423v1 [cs.CL])

    [http://arxiv.org/abs/2310.01423](http://arxiv.org/abs/2310.01423)

    本研究实证研究了AI生成文本检测工具在多领域ChatGPT材料中的有效性，并创建了一个多领域数据集来测试最先进API和工具的能力。

    

    自从ChatGPT成为一种重要的AI生成文本模型以来，它在各种应用中（包括软件开发和维护）提供高质量的回应，吸引了许多人的兴趣。ChatGPT有很大的潜力，但其误用可能会带来严重问题，特别是在教育和公共安全领域。目前已经有几种AI生成文本检测工具可供使用，但它们都是在真实文本上进行测试的。然而，需要进一步研究它们对多领域ChatGPT材料的有效性。本研究旨在通过创建一个多领域数据集来测试用于检测大学和其他研究机构使用的最先进API和工具的能力。为此，创建了一个包含文章、摘要、故事、新闻和产品评论的大型数据集。第二步是使用新创建的数据集来测试六种工具的性能。

    Since ChatGPT has emerged as a major AIGC model, providing high-quality responses across a wide range of applications (including software development and maintenance), it has attracted much interest from many individuals. ChatGPT has great promise, but there are serious problems that might arise from its misuse, especially in the realms of education and public safety. Several AIGC detectors are available, and they have all been tested on genuine text. However, more study is needed to see how effective they are for multi-domain ChatGPT material. This study aims to fill this need by creating a multi-domain dataset for testing the state-of-the-art APIs and tools for detecting artificially generated information used by universities and other research institutions. A large dataset consisting of articles, abstracts, stories, news, and product reviews was created for this study. The second step is to use the newly created dataset to put six tools through their paces. Six different artificial 
    
[^160]: 在多雨条件下实现鲁棒的三维物体检测

    Towards Robust 3D Object Detection In Rainy Conditions. (arXiv:2310.00944v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2310.00944](http://arxiv.org/abs/2310.00944)

    本研究提出了一个框架，旨在改善基于激光雷达的三维物体检测器在道路喷雾等恶劣天气条件下的鲁棒性。该方法使用恶劣天气检测网络过滤激光雷达点云中的喷雾，并进一步使用雷达目标过滤虚假检测，从而提高环境感知的准确性。

    

    激光雷达传感器被用于自动驾驶应用中准确感知环境。然而，它们受到雪、雾和雨等恶劣天气条件的影响。这些日常现象会引入不必要的噪音到测量中，严重降低基于激光雷达的感知系统的性能。在这项工作中，我们提出了一个框架来提高基于激光雷达的三维物体检测器对道路喷雾的鲁棒性。我们的方法使用一个先进的恶劣天气检测网络来过滤激光雷达点云中的喷雾，然后将其用作物体检测器的输入。通过这种方式，检测到的物体对场景中的恶劣天气影响较小，从而更准确地感知环境。除了恶劣天气过滤，我们还探索了使用雷达目标进一步过滤虚假检测的方法。对真实数据的测试表明，我们的方法改善了几种流行的三维物体检测器对道路喷雾的鲁棒性。

    LiDAR sensors are used in autonomous driving applications to accurately perceive the environment. However, they are affected by adverse weather conditions such as snow, fog, and rain. These everyday phenomena introduce unwanted noise into the measurements, severely degrading the performance of LiDAR-based perception systems. In this work, we propose a framework for improving the robustness of LiDAR-based 3D object detectors against road spray. Our approach uses a state-of-the-art adverse weather detection network to filter out spray from the LiDAR point cloud, which is then used as input for the object detector. In this way, the detected objects are less affected by the adverse weather in the scene, resulting in a more accurate perception of the environment. In addition to adverse weather filtering, we explore the use of radar targets to further filter false positive detections. Tests on real-world data show that our approach improves the robustness to road spray of several popular 3D 
    
[^161]: ECG-SL: 心电图(ECG)信号的深度学习方法——ECG-Segment学习

    ECG-SL: Electrocardiogram(ECG) Segment Learning, a deep learning method for ECG signal. (arXiv:2310.00818v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00818](http://arxiv.org/abs/2310.00818)

    本文提出了一种名为ECG-SL的深度学习方法，用于处理心电图(ECG)信号。该方法通过划分心跳片段并提取结构特征，显式地建模了ECG信号的周期性。此外，还采用自监督学习策略进行预训练，取得了显著的性能提升。

    

    心电图(ECG)是监测人类心脏活动的重要信号。研究人员利用深度学习模型在临床应用中取得了有希望的结果。然而，主流的深度学习方法通常忽视了ECG心跳波形的周期性和形态属性。在本研究中，我们提出了一种新颖的基于ECG-Segment的学习(ECG-SL)框架，以明确地建模ECG信号的周期性。具体来说，ECG信号首先被划分为心跳片段，然后从每个片段中提取结构特征。基于这些结构特征，设计了一个时间模型来学习不同临床任务的时间信息。此外，由于存在大量的ECG信号但标记数据非常有限，我们还探索了自监督学习策略来预训练模型，从而显著提高了下游任务的性能。所提出的方法优于其他方法。

    Electrocardiogram (ECG) is an essential signal in monitoring human heart activities. Researchers have achieved promising results in leveraging ECGs in clinical applications with deep learning models. However, the mainstream deep learning approaches usually neglect the periodic and formative attribute of the ECG heartbeat waveform. In this work, we propose a novel ECG-Segment based Learning (ECG-SL) framework to explicitly model the periodic nature of ECG signals. More specifically, ECG signals are first split into heartbeat segments, and then structural features are extracted from each of the segments. Based on the structural features, a temporal model is designed to learn the temporal information for various clinical tasks. Further, due to the fact that massive ECG signals are available but the labeled data are very limited, we also explore self-supervised learning strategy to pre-train the models, resulting significant improvement for downstream tasks. The proposed method outperforms
    
[^162]: 架接基础模型与异构联邦学习之间的差距

    Bridging the Gap Between Foundation Models and Heterogeneous Federated Learning. (arXiv:2310.00247v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00247](http://arxiv.org/abs/2310.00247)

    提出了一种自适应的Resource-aware Federated Foundation Models (RaFFM)框架，通过引入专门的模型压缩算法来解决将基础模型集成到联邦学习中的挑战，实现基于Transformer的基础模型在网络边缘根据异构资源约束进行动态缩放。

    

    联邦学习（FL）提供了隐私保护的分散式机器学习，在不共享私人数据的情况下在边缘客户端上优化模型。同时，基础模型（FMs）由于其在各种任务中的出色性能在人工智能（AI）社区中引起了关注。然而，将基础模型集成到FL中存在挑战，主要是由于其庞大的大小和对资源的需求。尤其是考虑到边缘FL系统的资源异构性。我们提出了一个自适应的Resource-aware Federated Foundation Models（RaFFM）框架来解决这些挑战。RaFFM引入了针对FL场景的专门模型压缩算法，如显著参数优先级和高性能子网络提取。这些算法使得基于Transformer的给定FMs可以在网络边缘根据异构资源约束进行动态缩放，无论是在FL的优化还是部署阶段。

    Federated learning (FL) offers privacy-preserving decentralized machine learning, optimizing models at edge clients without sharing private data. Simultaneously, foundation models (FMs) have gained traction in the artificial intelligence (AI) community due to their exceptional performance across various tasks. However, integrating FMs into FL presents challenges, primarily due to their substantial size and intensive resource requirements. This is especially true when considering the resource heterogeneity in edge FL systems. We present an adaptive framework for Resource-aware Federated Foundation Models (RaFFM) to address these challenges. RaFFM introduces specialized model compression algorithms tailored for FL scenarios, such as salient parameter prioritization and high-performance subnetwork extraction. These algorithms enable dynamic scaling of given transformer-based FMs to fit heterogeneous resource constraints at the network edge during both FL's optimization and deployment stag
    
[^163]: 大型语言模型微调中的LoRA集成

    LoRA ensembles for large language model fine-tuning. (arXiv:2310.00035v1 [cs.LG])

    [http://arxiv.org/abs/2310.00035](http://arxiv.org/abs/2310.00035)

    本文提出了一种使用低秩适配器（LoRA）的集成方法，用于解决大型语言模型微调中存在的不确定性量化问题，并提供了一个参数高效的微调技术。这种方法可以构建大规模的LoRA适配器集成，并具有与基础预训练模型相近的计算资源需求。

    

    细调的语言模型往往表现出较差的不确定性量化，表现为过于自信、校准不佳以及对测试数据或超出分布的样本的预测结果不可靠。为了缓解这个问题，本文提出了一种使用低秩适配器（LoRA）的集成方法，该方法是一种参数高效的微调技术。这些低秩适配器表示的参数数量非常小，比基础预训练模型小几个数量级。因此，可以构建大规模的LoRA适配器集成，几乎具有相同的计算资源需求。

    Finetuned LLMs often exhibit poor uncertainty quantification, manifesting as overconfidence, poor calibration, and unreliable prediction results on test data or out-of-distribution samples. One approach commonly used in vision for alleviating this issue is a deep ensemble, which constructs an ensemble by training the same model multiple times using different random initializations. However, there is a huge challenge to ensembling LLMs: the most effective LLMs are very, very large. Keeping a single LLM in memory is already challenging enough: keeping an ensemble of e.g. 5 LLMs in memory is impossible in many settings. To address these issues, we propose an ensemble approach using Low-Rank Adapters (LoRA), a parameter-efficient fine-tuning technique. Critically, these low-rank adapters represent a very small number of parameters, orders of magnitude less than the underlying pre-trained model. Thus, it is possible to construct large ensembles of LoRA adapters with almost the same computat
    
[^164]: 通过最小移动方案实现神经网络的模块化训练

    Module-wise Training of Neural Networks via the Minimizing Movement Scheme. (arXiv:2309.17357v1 [cs.LG])

    [http://arxiv.org/abs/2309.17357](http://arxiv.org/abs/2309.17357)

    通过引入模块化正则化方法，解决了神经网络模块化训练中早期层过拟合和深层停滞的问题，实验结果展示了该方法在不同架构上的优越性。

    

    在内存有限的受限设备环境中，贪婪的逐层或逐模块训练神经网络可以绕过端到端反向传播的一些问题，因此具有吸引力。然而，这种方法存在停滞问题，早期层过拟合和更深层在一定深度后停止提高测试准确性。我们提出通过引入与分布空间中梯度流的最小化移动方法相启发的模块化正则化来解决这个问题。我们称这种方法为TRGL（Transport Regularized Greedy Learning），并对其进行了理论研究，证明它会导致模块化贪婪方法是规则的，并逐步解决任务。在实验中，我们展示了在添加我们的正则化方法之后，各种架构（如ResNets，Transformers和VGG）的模块化训练的准确性得到了改善，其优于其他模块化训练方法，甚至经常优于端到端训练，并且可以减少高达60%的内存使用。

    Greedy layer-wise or module-wise training of neural networks is compelling in constrained and on-device settings where memory is limited, as it circumvents a number of problems of end-to-end back-propagation. However, it suffers from a stagnation problem, whereby early layers overfit and deeper layers stop increasing the test accuracy after a certain depth. We propose to solve this issue by introducing a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. We call the method TRGL for Transport Regularized Greedy Learning and study it theoretically, proving that it leads to greedy modules that are regular and that progressively solve the task. Experimentally, we show improved accuracy of module-wise training of various architectures such as ResNets, Transformers and VGG, when our regularization is added, superior to that of other module-wise training methods and often to end-to-end training, with as much as 60% less memory usage
    
[^165]: 高效的生物合理对抗训练

    Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])

    [http://arxiv.org/abs/2309.17348](http://arxiv.org/abs/2309.17348)

    本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。

    

    用反向传播训练的人工神经网络(ANNs)表现出令人惊讶的性能，并且越来越多地被用于执行我们日常生活中的任务。然而，ANNs极易受到对抗攻击的影响，这些攻击通过微小的有针对性的扰动来改变输入，从而严重破坏模型的性能。使ANNs对这些攻击具有鲁棒性最有效的方法是对抗训练，其中训练数据集被添加了样本用于对抗攻击。不幸的是，这种方法的缺点是增加了训练复杂性，因为生成对抗样本是非常计算消耗高的。与ANNs不同，人类不容易受到对抗攻击的影响。因此，在这项工作中，我们研究了生物合理的学习算法是否比BP更具有对抗攻击的鲁棒性。具体而言，我们对BP和“Error to Pertu"的对抗鲁棒性进行了广泛的比较分析。

    Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
    
[^166]: 通过隐式点图网络高效解剖标记肺部树状结构

    Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])

    [http://arxiv.org/abs/2309.17329](http://arxiv.org/abs/2309.17329)

    本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。

    

    肺部疾病在全球范围内是导致死亡的主要原因之一。治愈肺部疾病需要更好地理解肺部系统内的许多复杂的3D树状结构，如气道、动脉和静脉。在理论上，它们可以通过高分辨率图像堆栈进行建模。然而，基于密集体素网格的标准CNN方法代价过高。为了解决这个问题，我们引入了一种基于点的方法，保留了树骨架的图连通性，并结合了隐式表面表示。它以较低的计算成本提供了SOTA准确度，生成的模型具有可用的表面。由于公开可访问的数据稀缺，我们还整理了一套广泛的数据集来评估我们的方法，并将其公开。

    Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
    
[^167]: PlaceNav: 通过地点识别进行拓扑导航

    PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])

    [http://arxiv.org/abs/2309.17260](http://arxiv.org/abs/2309.17260)

    PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。

    

    最近的研究结果表明，将拓扑导航分为机器人无关和机器人特定的组件可以提高导航性能，通过使用不同类型机器人收集的数据来训练机器人无关部分。然而，导航方法仍受到适合训练数据的稀缺性和计算缩放性差的限制。在本文中，我们提出了一个名为PlaceNav的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件。我们利用视觉地点识别来选择拓扑导航流程中的子目标。这使得子目标选择更高效，并能够利用非机器人来源的大规模数据集增加训练数据的可用性。地点识别使得贝叶斯滤波成为可能，进一步通过增加子目标的时间一致性来提高导航性能。我们的实验结果验证了这一设计，并且新模型的性能提高了76%。

    Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
    
[^168]: DyVal: 基于图形信息的大型语言模型动态评估

    DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])

    [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167)

    DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。

    

    大型语言模型在各种评估基准中取得了显著的性能。然而，人们对它们的性能提出了担忧，因为它们庞大的训练语料库中可能存在数据污染。此外，当前基准的静态性质和固定复杂性可能无法充分衡量LLM的进步能力。在本文中，我们介绍了DyVal，一种新颖、通用且灵活的动态评估LLM的协议。基于我们提出的动态评估框架，我们利用有向无环图的结构优势构建了基于图形信息的DyVal，以动态生成具有可控复杂性的评估样本。DyVal生成了包括数学、逻辑推理和算法问题在内的具有挑战性的推理任务的评估集。我们评估了从Flan-T5-large到ChatGPT和GPT4的各种LLM。实验表明，LLM在DyVal生成的评估样本上表现更差

    Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
    
[^169]: 神经运算符加速科学模拟和设计

    Neural Operators for Accelerating Scientific Simulations and Design. (arXiv:2309.15325v1 [cs.LG])

    [http://arxiv.org/abs/2309.15325](http://arxiv.org/abs/2309.15325)

    本论文介绍了一种称为神经运算符的人工智能框架，用于学习连续域函数之间的映射，可以加速科学模拟和设计中的计算需求，实现零射超分辨率以及替代现有的模拟器。

    

    目前，科学发现和工程设计受限于物理实验的时间和成本，这些实验通常是通过试验和直觉选择的，并需要深入的领域专业知识。数值模拟是物理实验的替代方法，但对于复杂的现实领域来说，由于现有数值方法的计算需求，通常是不可行的。人工智能（AI）通过开发快速的数据驱动代理模型，提供了一个潜在的范式转变。特别是，一个称为神经运算符的AI框架提供了一个基于连续域函数之间映射学习的原则性框架，例如时空过程和偏微分方程（PDE）。它们可以在训练过程中未见过的新位置进行外推和预测解决方案，即进行零射超分辨率。神经运算符可以增强甚至替代许多应用中的现有模拟器，例如计算力学流体学。

    Scientific discovery and engineering design are currently limited by the time and cost of physical experiments, selected mostly through trial-and-error and intuition that require deep domain expertise. Numerical simulations present an alternative to physical experiments, but are usually infeasible for complex real-world domains due to the computational requirements of existing numerical methods. Artificial intelligence (AI) presents a potential paradigm shift through the development of fast data-driven surrogate models. In particular, an AI framework, known as neural operators, presents a principled framework for learning mappings between functions defined on continuous domains, e.g., spatiotemporal processes and partial differential equations (PDE). They can extrapolate and predict solutions at new locations unseen during training, i.e., perform zero-shot super-resolution. Neural operators can augment or even replace existing simulators in many applications, such as computational flui
    
[^170]: 用于生物医学管道流动的多物理信息神经网络

    Multiple Physics-Informed Neural Network for Biomedical Tube Flows. (arXiv:2309.15294v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2309.15294](http://arxiv.org/abs/2309.15294)

    本研究探索了多情况PINN方法在计算生物医学管道流动中的应用，通过预训练和参数化不同几何情况，可以实时获取未见几何形状的结果，进而优化了传统CFD方法的使用。

    

    管道几何的流体力学计算在生物医学血管和气道流体力学评估中非常重要。最近，基于物理信息的神经网络（PINN）已经成为传统计算流体动力学（CFD）方法的一个不错替代方案。然而，普通的PINN对于每种特定流动场景需要更长的训练时间，因此无法证明其主流使用的合理性。在这里，我们探索了多情况PINN方法在计算生物医学管道流动中的应用，其中各种不同几何情况经过参数化和预训练在PINN上，从而可以实时获取未见几何形状的结果。我们的目标是通过在一系列理想化的2D狭窄管道流中进行实验，来确定网络架构、管道特定和正则化策略，以优化这一方法。

    Fluid dynamics computations for tube-like geometries are important for biomedical evaluation of vascular and airway fluid dynamics. Physics-Informed Neural Networks (PINNs) have recently emerged as a good alternative to traditional computational fluid dynamics (CFD) methods. The vanilla PINN, however, requires much longer training time than the traditional CFD methods for each specific flow scenario and thus does not justify its mainstream use. Here, we explore the use of the multi-case PINN approach for calculating biomedical tube flows, where varied geometry cases are parameterized and pre-trained on the PINN, such that results for unseen geometries can be obtained in real time. Our objective is to identify network architecture, tube-specific, and regularization strategies that can optimize this, via experiments on a series of idealized 2D stenotic tube flows.
    
[^171]: LinGCN: 结构化的线性化图卷积网络用于同态加密推断

    LinGCN: Structural Linearized Graph Convolutional Network for Homomorphically Encrypted Inference. (arXiv:2309.14331v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14331](http://arxiv.org/abs/2309.14331)

    LinGCN是一个旨在减少乘法深度和优化HE基于GCN推断性能的框架，通过结构化线性化算法和参数化的离散指示函数的联合训练，实现细粒度的节点级非线性位置选择。

    

    图卷积网络（GCN）模型的规模增长已经在个人医疗和金融系统等多个应用领域取得了超越人类表现的革命性进展。然而，在云端部署GCN引发了对客户数据可能受到对抗性攻击的隐私问题。为了解决安全问题，采用同态加密（HE）的隐私保护机器学习（PPML）可以确保敏感客户数据的安全。然而，在实际应用中，这引入了相当大的计算开销。为了解决这些挑战，我们提出了LinGCN，这是一个旨在减少乘法深度并优化HE基于GCN推断性能的框架。LinGCN围绕三个关键要素展开：（1）可微的结构化线性化算法，搭配参数化的离散指示函数，通过与模型权重一起进行联合训练以满足优化目标。这种策略促进了细粒度的节点级非线性位置选择，从而实现了

    The growth of Graph Convolution Network (GCN) model sizes has revolutionized numerous applications, surpassing human performance in areas such as personal healthcare and financial systems. The deployment of GCNs in the cloud raises privacy concerns due to potential adversarial attacks on client data. To address security concerns, Privacy-Preserving Machine Learning (PPML) using Homomorphic Encryption (HE) secures sensitive client data. However, it introduces substantial computational overhead in practical applications. To tackle those challenges, we present LinGCN, a framework designed to reduce multiplication depth and optimize the performance of HE based GCN inference. LinGCN is structured around three key elements: (1) A differentiable structural linearization algorithm, complemented by a parameterized discrete indicator function, co-trained with model weights to meet the optimization goal. This strategy promotes fine-grained node-level non-linear location selection, resulting in a 
    
[^172]: 潜变量结构方程模型的最大似然估计：一种神经网络方法

    Maximum Likelihood Estimation of Latent Variable Structural Equation Models: A Neural Network Approach. (arXiv:2309.14073v1 [stat.ML])

    [http://arxiv.org/abs/2309.14073](http://arxiv.org/abs/2309.14073)

    本研究提出了一种新的图形结构，用于在线性和高斯性假设下稳定的潜变量结构方程模型。我们证明了计算该模型的最大似然估计等价于训练一个神经网络，并实现了一个基于GPU的算法来进行计算。

    

    我们提出了一种在线性和高斯性假设下稳定的结构方程模型的图形结构。我们展示了计算这个模型的最大似然估计等价于训练一个神经网络。我们实现了一个基于GPU的算法来计算这些模型的最大似然估计。

    We propose a graphical structure for structural equation models that is stable under marginalization under linearity and Gaussianity assumptions. We show that computing the maximum likelihood estimation of this model is equivalent to training a neural network. We implement a GPU-based algorithm that computes the maximum likelihood estimation of these models.
    
[^173]: 用于乳房MRI的不同分辨率深度学习变形登记

    Diffeomorphic Multi-Resolution Deep Learning Registration for Applications in Breast MRI. (arXiv:2309.13777v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2309.13777](http://arxiv.org/abs/2309.13777)

    本论文提出了适用于乳房MR图像登记的学习策略，其中包含了一个登记网络，能够产生更优的登记结果，并提供微分同胚保证。

    

    在乳房手术规划中，准确地登记不同体位下的MR图像有潜力改善乳腺癌治疗过程中肿瘤的本地化。虽然基于学习的登记方法最近已经成为大多数医学影像登记任务的最先进方法，但由于乳房MR图像中缺乏丰富的纹理信息以及变形需要具有微分同胚的困难，这些方法尚未在乳房图像登记方面取得进展。在这项工作中，我们提出了适用于乳房MR图像登记的具有微分同胚约束的学习策略，并提供了来自硅激发和体内实验的早期实验结果。这项工作的一个关键贡献是一个登记网络，除了提供微分同胚保证外，还能产生更优的乳房图像登记结果。

    In breast surgical planning, accurate registration of MR images across patient positions has the potential to improve the localisation of tumours during breast cancer treatment. While learning-based registration methods have recently become the state-of-the-art approach for most medical image registration tasks, these methods have yet to make inroads into breast image registration due to certain difficulties-the lack of rich texture information in breast MR images and the need for the deformations to be diffeomophic. In this work, we propose learning strategies for breast MR image registration that are amenable to diffeomorphic constraints, together with early experimental results from in-silico and in-vivo experiments. One key contribution of this work is a registration network which produces superior registration outcomes for breast images in addition to providing diffeomorphic guarantees.
    
[^174]: 角度优化的文本嵌入

    AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])

    [http://arxiv.org/abs/2309.12871](http://arxiv.org/abs/2309.12871)

    本文提出了一种名为AnglE的角度优化文本嵌入模型，通过在复杂空间中引入角度优化来缓解文本嵌入中余弦函数饱和区域造成的梯度消失问题。该模型在多个STS任务中实现了高质量的文本嵌入，并在有限标签数据的特定领域STS场景中展现出优秀的性能。

    

    高质量的文本嵌入对于提升语义文本相似度（STS）任务至关重要，而这些任务又是大型语言模型（LLM）应用中的关键组成部分。然而，现有的文本嵌入模型面临的一个普遍挑战是渐变消失问题，主要是由于它们在优化目标中依赖余弦函数，而余弦函数具有饱和区域。为了解决这个问题，本文提出了一种称为AnglE的新型角度优化文本嵌入模型。AnglE的核心思想是在一个复杂空间中引入角度优化。这种新颖的方法有效地缓解了余弦函数饱和区域产生的不利影响，从而可以阻碍梯度并阻碍优化过程。为了建立全面的STS评估，我们在现有的短文本STS数据集和从GitHub Issues中新收集的长文本STS数据集上进行了实验。此外，我们还研究了具有有限标签数据的特定领域STS场景，并探讨了AnglE的工作原理。

    High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
    
[^175]: 锐度感知最小化和稳定性边界。

    Sharpness-Aware Minimization and the Edge of Stability. (arXiv:2309.12488v1 [cs.LG])

    [http://arxiv.org/abs/2309.12488](http://arxiv.org/abs/2309.12488)

    本研究通过类似的计算方法，为锐度感知最小化(SAM)，一种改进泛化性能的梯度下降变种，确定了一个稳定性边界，该边界取决于梯度的范数。

    

    最近的实验表明，当使用梯度下降(GD)训练神经网络时，损失函数的Hessian矩阵的操作符范数会增长，直到接近$2/\eta$，之后会在该值周围波动。根据对损失函数的局部二次逼近，$2/\eta$被称为“稳定性边界”。我们使用类似的计算方法，为锐度感知最小化(SAM)确定了一个“稳定性边界”，SAM是一种改进泛化性能的GD变种。与GD不同，SAM的稳定性边界取决于梯度的范数。通过三个深度学习任务的实证，我们观察到SAM在这个分析中确定的稳定性边界上运行。

    Recent experiments have shown that, often, when training a neural network with gradient descent (GD) with a step size $\eta$, the operator norm of the Hessian of the loss grows until it approximately reaches $2/\eta$, after which it fluctuates around this value.  The quantity $2/\eta$ has been called the "edge of stability" based on consideration of a local quadratic approximation of the loss. We perform a similar calculation to arrive at an "edge of stability" for Sharpness-Aware Minimization (SAM), a variant of GD which has been shown to improve its generalization. Unlike the case for GD, the resulting SAM-edge depends on the norm of the gradient. Using three deep learning training tasks, we see empirically that SAM operates on the edge of stability identified by this analysis.
    
[^176]: PIE: 通过渐进图像编辑模拟疾病进展

    PIE: Simulating Disease Progression via Progressive Image Editing. (arXiv:2309.11745v1 [eess.IV])

    [http://arxiv.org/abs/2309.11745](http://arxiv.org/abs/2309.11745)

    PIE是一个新的渐进图像编辑框架，可以通过控制性地操纵图像特征来准确模拟个体患者的疾病进展。该方法利用文本到图像生成模型，实现了个性化的疾病进展模拟，并在验证实验中展现了优越性。

    

    疾病进展模拟是一项重要的研究领域，对临床诊断、预后和治疗具有重要意义。该领域面临的主要挑战之一是缺乏对个体患者进行连续的医学成像监测的数据。为了解决这个问题，我们提出了一种名为渐进图像编辑（PIE）的新框架，可以控制性地操纵与疾病相关的图像特征，实现精确和逼真的疾病进展模拟。具体地，我们利用了最近在文本到图像生成模型方面的进展，准确地模拟疾病进展并针对每个患者进行个性化处理。我们理论上分析了我们框架中的迭代优化过程，将其视为具有指数衰减学习率的梯度下降。为了验证我们的框架，我们在三个医学成像领域进行了实验。我们的结果证明了PIE相对于稳定扩散漫游和基于风格的特征生成方法的优越性。

    Disease progression simulation is a crucial area of research that has significant implications for clinical diagnosis, prognosis, and treatment. One major challenge in this field is the lack of continuous medical imaging monitoring of individual patients over time. To address this issue, we develop a novel framework termed Progressive Image Editing (PIE) that enables controlled manipulation of disease-related image features, facilitating precise and realistic disease progression simulation. Specifically, we leverage recent advancements in text-to-image generative models to simulate disease progression accurately and personalize it for each patient. We theoretically analyze the iterative refining process in our framework as a gradient descent with an exponentially decayed learning rate. To validate our framework, we conduct experiments in three medical imaging domains. Our results demonstrate the superiority of PIE over existing methods such as Stable Diffusion Walk and Style-Based Mani
    
[^177]: 机械化生成器2.0: 强化学习用于评估生成的游戏规则

    Mechanic Maker 2.0: Reinforcement Learning for Evaluating Generated Rules. (arXiv:2309.09476v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2309.09476](http://arxiv.org/abs/2309.09476)

    本论文研究了将强化学习应用于游戏规则生成的人类游戏评估，并通过实验结果表明，强化学习生成的规则与传统基线方法有所不同，可能更适合人类使用。

    

    自动游戏设计（AGD）是研究自动生成游戏规则的技术游戏研究的一个长期课题。 AGD方法通常依赖于对人类玩家游戏的近似，可以是客观函数或AI代理。尽管如此，大部分这些近似器是静态的，也就是说，它们不能反映人类玩家在游戏中的学习和提高能力。本文中，我们研究了将强化学习（RL）应用于生成规则的人类游戏评估中。我们在Unity中重新创建了经典的AGD环境Mechanic Maker作为一个全新的开源生成规则框架。我们的结果表明，RL与A*代理基线产生了不同的规则集，这些规则可能更适合人类使用。

    Automated game design (AGD), the study of automatically generating game rules, has a long history in technical games research. AGD approaches generally rely on approximations of human play, either objective functions or AI agents. Despite this, the majority of these approximators are static, meaning they do not reflect human player's ability to learn and improve in a game. In this paper, we investigate the application of Reinforcement Learning (RL) as an approximator for human play for rule generation. We recreate the classic AGD environment Mechanic Maker in Unity as a new, open-source rule generation framework. Our results demonstrate that RL produces distinct sets of rules from an A* agent baseline, which may be more usable by humans.
    
[^178]: 通过级别修复来重构现有级别

    Reconstructing Existing Levels through Level Inpainting. (arXiv:2309.09472v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.09472](http://arxiv.org/abs/2309.09472)

    本论文介绍了内容增强和级别修复的方法，通过两种技术（自编码器和U-net）来重建和扩展视频游戏级别。经过综合案例研究，我们展示了这两种方法相对于基准方法的卓越性能。提供了未来研究方向的见解。

    

    在先前的工作中，已经使用了过程化内容生成（PCG）和通过机器学习进行的过程化内容生成（PCGML）来生成各种游戏中的级别。本文介绍了内容增强，并将重点放在级别修复的子问题上，该问题涉及重建和扩展视频游戏级别。受图像修复的启发，我们从这一领域中调整了两种技术以解决我们的特定用例。我们提出了两种级别修复的方法：自编码器和U-net。通过一个全面的案例研究，我们展示了它们相对于基准方法的卓越性能，并讨论了它们的相对优势。此外，我们为级别修复任务提供了两种方法的实际演示，并提供了对未来研究方向的见解。

    Procedural Content Generation (PCG) and Procedural Content Generation via Machine Learning (PCGML) have been used in prior work for generating levels in various games. This paper introduces Content Augmentation and focuses on the subproblem of level inpainting, which involves reconstructing and extending video game levels. Drawing inspiration from image inpainting, we adapt two techniques from this domain to address our specific use case. We present two approaches for level inpainting: an Autoencoder and a U-net. Through a comprehensive case study, we demonstrate their superior performance compared to a baseline method and discuss their relative merits. Furthermore, we provide a practical demonstration of both approaches for the level inpainting task and offer insights into potential directions for future research.
    
[^179]: Landscape-Sketch-Step: 一种基于AI/ML的元启发式方法解决代理优化问题

    Landscape-Sketch-Step: An AI/ML-Based Metaheuristic for Surrogate Optimization Problems. (arXiv:2309.07936v1 [cs.LG])

    [http://arxiv.org/abs/2309.07936](http://arxiv.org/abs/2309.07936)

    Landscape-Sketch-Step是一种基于AI/ML的元启发式方法，结合了机器学习、随机优化和强化学习技术，用于解决成本函数评估昂贵、不可访问或禁止的代理优化问题。

    

    本文介绍了一种新的全局优化启发式方法，用于在成本函数的评估非常昂贵、不可访问或甚至禁止的场景下进行优化。该方法称为Landscape-Sketch-Step（LSS），结合了机器学习、随机优化和强化学习技术，依赖于先前采样点的历史信息，以明智地选择应评估成本函数的参数值。与复制交换蒙特卡洛方法相比，该方法所需的成本函数评估次数与模拟退火方法相当，这在高通量计算或高性能计算任务等环境中尤为重要，因为评估要么计算成本高昂，要么需要很长时间才能完成。该方法与标准的代理优化技术也不同，因为它不构建代理模型。

    In this paper, we introduce a new heuristics for global optimization in scenarios where extensive evaluations of the cost function are expensive, inaccessible, or even prohibitive. The method, which we call Landscape-Sketch-and-Step (LSS), combines Machine Learning, Stochastic Optimization, and Reinforcement Learning techniques, relying on historical information from previously sampled points to make judicious choices of parameter values where the cost function should be evaluated at. Unlike optimization by Replica Exchange Monte Carlo methods, the number of evaluations of the cost function required in this approach is comparable to that used by Simulated Annealing, quality that is especially important in contexts like high-throughput computing or high-performance computing tasks, where evaluations are either computationally expensive or take a long time to be performed. The method also differs from standard Surrogate Optimization techniques, for it does not construct a surrogate model
    
[^180]: 深度量子图像模拟：解析神经网络对量子实验的见解

    Deep Quantum Graph Dreaming: Deciphering Neural Network Insights into Quantum Experiments. (arXiv:2309.07056v1 [quant-ph])

    [http://arxiv.org/abs/2309.07056](http://arxiv.org/abs/2309.07056)

    本文使用了一种名为“梦境”的可解释人工智能技术，探索神经网络对量子光学实验的学习，发现网络可以改变量子系统的属性分布，并揭示了神经网络的学习策略。

    

    尽管神经网络在促进新的科学发现方面很有前景，但其逻辑背后的不透明性给解释其发现的挑战带来了困难。在本文中，我们使用一种名为“inception”或“深度梦境”的可解释人工智能（XAI）技术，该技术被发明用于计算机视觉的机器学习。我们使用这种技术来探索神经网络对量子光学实验的学习。我们的故事从对量子系统属性进行深度神经网络训练开始。经过训练后，我们“反转”神经网络--实际上是询问它如何想象具有特定属性的量子系统，以及如何连续修改量子系统以改变属性。我们发现网络可以改变量子系统的初始属性分布，我们可以概念化神经网络的学习策略。有趣的是，在较浅层，神经网络识别简单的属性，而在较深层次上...（内容省略）

    Despite their promise to facilitate new scientific discoveries, the opaqueness of neural networks presents a challenge in interpreting the logic behind their findings. Here, we use a eXplainable-AI (XAI) technique called $inception$ or $deep$ $dreaming$, which has been invented in machine learning for computer vision. We use this techniques to explore what neural networks learn about quantum optics experiments. Our story begins by training a deep neural networks on the properties of quantum systems. Once trained, we "invert" the neural network -- effectively asking how it imagines a quantum system with a specific property, and how it would continuously modify the quantum system to change a property. We find that the network can shift the initial distribution of properties of the quantum system, and we can conceptualize the learned strategies of the neural network. Interestingly, we find that, in the first layers, the neural network identifies simple properties, while in the deeper ones
    
[^181]: 适用于拜占庭式机器学习的实用同态聚合

    Practical Homomorphic Aggregation for Byzantine ML. (arXiv:2309.05395v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.05395](http://arxiv.org/abs/2309.05395)

    本文介绍了一种适用于拜占庭式机器学习的实用同态聚合方法，以应对拜占庭节点和服务器隐私侵犯的问题。

    

    由于数据的大规模可用性，机器学习算法正在分布式拓扑中部署，不同的节点通过与中央服务器交换与模型相关的信息（例如梯度）来共同训练其个体数据上的机器学习模型。然而，分布式学习方案容易受到两种威胁。首先，拜占庭式节点可以通过向服务器发送不正确的信息（例如错误的梯度）单独破坏学习过程。缓解此类行为的标准方法是在服务器上使用非线性鲁棒聚合方法。其次，服务器可以侵犯节点的隐私。最近的攻击已经表明，交换（未加密的）梯度使得一个好奇的服务器能够恢复出所有节点的数据。同态加密（HE），一种金标准安全原语，已经广泛研究作为非拜占庭场景中分布式学习的隐私保护解决方案。

    Due to the large-scale availability of data, machine learning (ML) algorithms are being deployed in distributed topologies, where different nodes collaborate to train ML models over their individual data by exchanging model-related information (e.g., gradients) with a central server. However, distributed learning schemes are notably vulnerable to two threats. First, Byzantine nodes can single-handedly corrupt the learning by sending incorrect information to the server, e.g., erroneous gradients. The standard approach to mitigate such behavior is to use a non-linear robust aggregation method at the server. Second, the server can violate the privacy of the nodes. Recent attacks have shown that exchanging (unencrypted) gradients enables a curious server to recover the totality of the nodes' data. The use of homomorphic encryption (HE), a gold standard security primitive, has extensively been studied as a privacy-preserving solution to distributed learning in non-Byzantine scenarios. Howev
    
[^182]: 边际化重要性采样用于离线环境下的策略评估

    Marginalized Importance Sampling for Off-Environment Policy Evaluation. (arXiv:2309.01807v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.01807](http://arxiv.org/abs/2309.01807)

    本文提出了一种新的方法，利用边际化重要性采样框架，在使用模拟器和真实世界的离线数据评估代理策略性能时，解决了大密度比率和间接监督的挑战。

    

    强化学习方法通常效率低下，使得在实际机器人中训练和部署RL策略具有挑战性。即使是在仿真中训练的稳健策略，也需要在实际部署中评估其性能。本文提出了一种在实际世界中评估代理策略性能的新方法。我们的方法结合了模拟器和真实世界的离线数据，使用边际化重要性采样（MIS）框架来评估任何策略的性能。现有的MIS方法面临两个挑战：（1）大的密度比率偏离合理范围，（2）间接监督，需要间接推断比率，从而加剧估计误差。我们的方法通过引入模拟器中目标策略的占位变量，并将密度比率学习为两个可分别学习的项的乘积来解决这些挑战。

    Reinforcement Learning (RL) methods are typically sample-inefficient, making it challenging to train and deploy RL-policies in real world robots. Even a robust policy trained in simulation requires a real-world deployment to assess their performance. This paper proposes a new approach to evaluate the real-world performance of agent policies prior to deploying them in the real world. Our approach incorporates a simulator along with real-world offline data to evaluate the performance of any policy using the framework of Marginalized Importance Sampling (MIS). Existing MIS methods face two challenges: (1) large density ratios that deviate from a reasonable range and (2) indirect supervision, where the ratio needs to be inferred indirectly, thus exacerbating estimation error. Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separate
    
[^183]: 关于Adam的隐式偏差

    On the Implicit Bias of Adam. (arXiv:2309.00079v1 [cs.LG])

    [http://arxiv.org/abs/2309.00079](http://arxiv.org/abs/2309.00079)

    本文证明了RMSProp和Adam存在隐式规范化作用，其取决于超参数和训练阶段，并讨论了这些证明事实对泛化的影响。

    

    在以前的文献中，后向误差分析被用来找到近似梯度下降轨迹的常微分方程（ODEs）。发现有限步长会隐式地规范化解决方案，因为出现在ODE中的项会惩罚损失梯度的二范数。我们证明了RMSProp和Adam中是否存在类似的隐式规范化取决于它们的超参数和训练阶段，但涉及的“范数”不同：对应的ODE项要么惩罚（扰动的）损失梯度的一范数，要么相反地阻止其减小（后一种情况是典型的）。我们还进行了数值实验，并讨论了这些证明事实如何影响泛化。

    In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different "norm" involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, on the contrary, hinder its decrease (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.
    
[^184]: US-SFNet:基于空间频域的多分支网络用于超声图像中的颈部淋巴结病灶诊断

    US-SFNet: A Spatial-Frequency Domain-based Multi-branch Network for Cervical Lymph Node Lesions Diagnoses in Ultrasound Images. (arXiv:2308.16738v1 [eess.IV])

    [http://arxiv.org/abs/2308.16738](http://arxiv.org/abs/2308.16738)

    本论文提出了一种基于空间频域的多分支网络US-SFNet，用于超声图像中颈部淋巴结病变的诊断。通过使用Conv-FFT块来建模图像，实现更准确的诊断结果。

    

    超声成像是诊断颈部淋巴结病灶的关键工具。然而，这些图像的诊断主要依赖于医务人员的专业知识，使得这一过程容易出现误诊。尽管迅速发展的深度学习在改进各种超声图像的诊断方面取得了实质性的进展，但在颈部淋巴结方面仍存在明显的研究空白。我们的目标是通过利用深度学习模型准确诊断颈部淋巴结病灶。为此，我们首先收集了3392张正常淋巴结、良性淋巴结病灶、恶性原发淋巴结病灶和恶性转移淋巴结病灶的图像。鉴于超声图像是由声波在不同的身体组织中反射和散射产生的，我们提出了Conv-FFT块。它将卷积操作与快速傅里叶变换相结合，更准确地建模图像。在此基础上构建了...

    Ultrasound imaging serves as a pivotal tool for diagnosing cervical lymph node lesions. However, the diagnoses of these images largely hinge on the expertise of medical practitioners, rendering the process susceptible to misdiagnoses. Although rapidly developing deep learning has substantially improved the diagnoses of diverse ultrasound images, there remains a conspicuous research gap concerning cervical lymph nodes. The objective of our work is to accurately diagnose cervical lymph node lesions by leveraging a deep learning model. To this end, we first collected 3392 images containing normal lymph nodes, benign lymph node lesions, malignant primary lymph node lesions, and malignant metastatic lymph node lesions. Given that ultrasound images are generated by the reflection and scattering of sound waves across varied bodily tissues, we proposed the Conv-FFT Block. It integrates convolutional operations with the fast Fourier transform to more astutely model the images. Building upon thi
    
[^185]: 使用遮蔽条件扩散的模态循环进行MRI无监督异常分割

    Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v1 [eess.IV])

    [http://arxiv.org/abs/2308.16150](http://arxiv.org/abs/2308.16150)

    本文介绍了一种名为遮蔽模态循环与条件扩散的方法，该方法能够对多模态MRI中的异常进行分割。方法基于循环模态转换和条件扩散的思想，能够检测到训练中未遇到的异常模式。

    

    无监督异常分割旨在检测与训练过程中处理的任何模式不同的模式，通常称为异常或超出分布的模式，而不提供任何关联的手动分割。由于部署过程中的异常可能导致模型失效，检测异常可以增强模型的可靠性，这在医学成像等高风险领域非常有价值。本文引入了基于遮蔽模态循环与条件扩散（MMCCD）的方法，该方法能够在多模态MRI中分割各种模式的异常。该方法基于两个基本思想。首先，我们提出使用循环模态转换作为启用异常检测的机制。图像转换模型学习组织特异的模态映射，这是组织生理学的特征。因此，这些学习到的映射无法将在训练过程中从未遇到的组织或图像模式进行转换，从而产生错误，使得异常能够被检测到。

    Unsupervised anomaly segmentation aims to detect patterns that are distinct from any patterns processed during training, commonly called abnormal or out-of-distribution patterns, without providing any associated manual segmentations. Since anomalies during deployment can lead to model failure, detecting the anomaly can enhance the reliability of models, which is valuable in high-risk domains like medical imaging. This paper introduces Masked Modality Cycles with Conditional Diffusion (MMCCD), a method that enables segmentation of anomalies across diverse patterns in multimodal MRI. The method is based on two fundamental ideas. First, we propose the use of cyclic modality translation as a mechanism for enabling abnormality detection. Image-translation models learn tissue-specific modality mappings, which are characteristic of tissue physiology. Thus, these learned mappings fail to translate tissues or image patterns that have never been encountered during training, and the error enables
    
[^186]: BaDExpert: 提取后门功能以实现准确的后门输入检测

    BaDExpert: Extracting Backdoor Functionality for Accurate Backdoor Input Detection. (arXiv:2308.12439v1 [cs.CR])

    [http://arxiv.org/abs/2308.12439](http://arxiv.org/abs/2308.12439)

    BaDExpert是一种防御后门攻击的新方法，通过逆向工程提取给定后门模型的后门功能，并生成一个专家模型，该模型只能识别后门输入。可以进一步使用该模型设计高度准确的后门输入检测器。

    

    我们提出了一种新颖的防御方法，用于对抗深度神经网络（DNN）上的后门攻击，其中对手将恶意行为（后门）秘密地植入DNN中。我们的防御属于后期开发的防御范畴，独立于模型生成的方式。所提出的防御方法基于一种新颖的逆向工程方法，可以直接提取给定后门模型的后门功能并生成一个专家模型。该方法很简单 - 在一小组有意义的错误标记的干净样本上微调后门模型，从而使其忘记正常功能但仍保留后门功能，从而生成一个只能识别后门输入的模型（称为后门专家模型）。基于提取的后门专家模型，我们展示了设计高度准确的后门输入检测器的可行性，在模型推理过程中过滤掉后门输入。进一步通过...

    We present a novel defense, against backdoor attacks on Deep Neural Networks (DNNs), wherein adversaries covertly implant malicious behaviors (backdoors) into DNNs. Our defense falls within the category of post-development defenses that operate independently of how the model was generated. The proposed defense is built upon a novel reverse engineering approach that can directly extract backdoor functionality of a given backdoored model to a backdoor expert model. The approach is straightforward -- finetuning the backdoored model over a small set of intentionally mislabeled clean samples, such that it unlearns the normal functionality while still preserving the backdoor functionality, and thus resulting in a model (dubbed a backdoor expert model) that can only recognize backdoor inputs. Based on the extracted backdoor expert model, we show the feasibility of devising highly accurate backdoor input detectors that filter out the backdoor inputs during model inference. Further augmented by
    
[^187]: SelfCheck: 使用LLMs自检其逐步推理的创新

    SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])

    [http://arxiv.org/abs/2308.00436](http://arxiv.org/abs/2308.00436)

    本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。

    

    最近大型语言模型（LLMs）的进展，尤其是链式思维（CoT）的发明，使得解决推理问题成为可能。然而，即使最强大的LLMs仍然难以处理需要非线性思维和多步推理的复杂问题。在这项工作中，我们探讨了LLMs是否具有识别自己错误的能力，而无需依赖外部资源。具体而言，我们研究了它们是否可以用于识别逐步推理中的个别错误。为此，我们提出了一种零-shot验证方案以识别此类错误。然后，我们使用此验证方案来改进问答性能，通过对不同生成的答案进行加权投票。我们在三个数学数据集-GSM8K，MathQA和MATH上测试了该方法，并发现它成功识别错误，并进而提高了最终的预测性能。

    The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
    
[^188]: 在反应式系统内对神经网络进行形式化解释

    Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])

    [http://arxiv.org/abs/2308.00143](http://arxiv.org/abs/2308.00143)

    这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。

    

    深度神经网络(DNNs)越来越多地被用作反应式系统中的控制器。然而，DNNs具有高度的不透明性，这使得解释和证明它们的行为变得困难。为了解决这个问题，出现了对可解释AI(XAI)技术的兴趣激增，这些技术能够找出导致DNN行为的输入特征。现有的XAI技术通常存在两个限制：(i)它们是启发式方法，并不能提供解释正确性的正式保证；(ii)它们通常适用于“一次性”系统(即DNN独立于过去的调用)，而不是反应式系统。在这里，我们开始弥合这个差距，提出一种基于DNN验证的形式化XAI技术，用于推理多步骤的反应式系统。我们建议通过利用系统的转换约束来计算简洁的解释的方法，以便减少底层验证器所探索的搜索空间。

    Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
    
[^189]: 实现可行的托卡马克磁控制强化学习

    Towards practical reinforcement learning for tokamak magnetic control. (arXiv:2307.11546v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2307.11546](http://arxiv.org/abs/2307.11546)

    这项研究致力于解决强化学习方法在托卡马克磁控制中的关键缺点，通过改进算法和训练过程，实现了更高的控制精度、减小稳态误差和缩短学习新任务所需时间，并通过实验验证了模拟结果的有效性。

    

    强化学习（RL）在实时控制系统中展现出了很好的结果，包括等离子体磁控制领域。然而，与传统的反馈控制方法相比，仍存在显著的缺点。本研究致力于解决RL方法的关键缺点：实现更高的对等离子体属性的控制精度，减小稳态误差，并减少学习新任务所需的时间。我们在\cite{degrave2022magnetic}的基础上，提出了对代理结构和训练过程进行算法改进的方法。我们展示了模拟结果，显示了形状精度提高了65％，实现了等离子体电流在长期偏差上的大幅减少，并且将学习新任务所需的训练时间减少了3倍或更多。我们使用升级后的RL控制器在TCV托卡马克上进行了新的实验，验证了达到的模拟结果，并指出...

    Reinforcement learning (RL) has shown promising results for real-time control systems, including the domain of plasma magnetic control. However, there are still significant drawbacks compared to traditional feedback control approaches for magnetic confinement. In this work, we address key drawbacks of the RL method; achieving higher control accuracy for desired plasma properties, reducing the steady-state error, and decreasing the required time to learn new tasks. We build on top of \cite{degrave2022magnetic}, and present algorithmic improvements to the agent architecture and training procedure. We present simulation results that show up to 65\% improvement in shape accuracy, achieve substantial reduction in the long-term bias of the plasma current, and additionally reduce the training time required to learn new tasks by a factor of 3 or more. We present new experiments using the upgraded RL-based controllers on the TCV tokamak, which validate the simulation results achieved, and point
    
[^190]: 迈向最优神经网络：样本拆分在超参数选择中的作用

    Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection. (arXiv:2307.07726v1 [stat.ML])

    [http://arxiv.org/abs/2307.07726](http://arxiv.org/abs/2307.07726)

    本文通过揭示神经网络模型构建中的样本拆分方法的奥秘，构建了一个理论框架来解释神经网络的有效性。我们的研究结果表明，从样本拆分中得到的最优超参数可以使得神经网络模型最小化预测风险。

    

    当人工神经网络在各个领域展现出卓越的实践成功时，关于它们的理论特性，如逼近能力、统计性质和泛化性能等的研究也取得了显著进展。在本文中，我们通过揭示神经网络模型构建中一种常见实践背后的奥秘：样本拆分，构建了一个新颖的理论来理解神经网络的有效性。我们的理论证明，从样本拆分中得到的最优超参数可以使得神经网络模型渐进地最小化预测风险。我们在不同的应用场景和网络结构中进行了大量实验，实验结果证实了我们的理论的有效性。

    When artificial neural networks have demonstrated exceptional practical success in a variety of domains, investigations into their theoretical characteristics, such as their approximation power, statistical properties, and generalization performance, have made significant strides. In this paper, we construct a novel theory for understanding the effectiveness of neural networks by discovering the mystery underlying a common practice during neural network model construction: sample splitting. Our theory demonstrates that, the optimal hyperparameters derived from sample splitting can enable a neural network model that asymptotically minimizes the prediction risk. We conduct extensive experiments across different application scenarios and network architectures, and the results manifest our theory's effectiveness.
    
[^191]: 学习层次交互式多目标搜索以进行移动操作

    Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])

    [http://arxiv.org/abs/2307.06125](http://arxiv.org/abs/2307.06125)

    这项工作提出了一个层次化的强化学习方法，用于解决在未知环境中需要同时进行操控和导航的交互式多目标搜索任务。实验证明，该方法可以在新环境中进行零样本迁移，并对未见过的子任务具有鲁棒性。

    

    现有的目标搜索方法使得机器人可以在自由路径上进行搜索，然而，在非结构化的以人为中心的环境中操作的机器人经常需要操控环境以满足他们的需求。在这项工作中，我们引入了一种新的交互式多目标搜索任务，机器人需要打开门以浏览房间，并在橱柜和抽屉内搜索目标物品。这些新挑战需要在未知环境中结合操控和导航技能。我们提出了HIMOS，一种层次强化学习方法，学习组合探索、导航和操控技能。为了实现这一点，我们设计了一个围绕语义地图记忆的抽象高级动作空间，并利用探索过的环境作为实例导航点。我们在仿真和真实世界中进行了大量实验，证明HIMOS可以零样本方式有效地迁移到新的环境中，并且对于未见过的子任务具有鲁棒性。

    Existing object-search approaches enable robots to search through free pathways, however, robots operating in unstructured human-centered environments frequently also have to manipulate the environment to their needs. In this work, we introduce a novel interactive multi-object search task in which a robot has to open doors to navigate rooms and search inside cabinets and drawers to find target objects. These new challenges require combining manipulation and navigation skills in unexplored environments. We present HIMOS, a hierarchical reinforcement learning approach that learns to compose exploration, navigation, and manipulation skills. To achieve this, we design an abstract high-level action space around a semantic map memory and leverage the explored environment as instance navigation points. We perform extensive experiments in simulation and the real-world that demonstrate that HIMOS effectively transfers to new environments in a zero-shot manner. It shows robustness to unseen subp
    
[^192]: 深度神经网络中的定量中心极限定理

    Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])

    [http://arxiv.org/abs/2307.06092](http://arxiv.org/abs/2307.06092)

    本文研究了具有随机高斯权重和偏置的全连接神经网络的分布，得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限，证明了随机全连接网络与相应的无限宽高斯过程之间的距离按照 $n^{-\gamma}$ 缩放，界限在网络宽度的依赖性方面优于以前的研究。

    

    我们研究了具有随机高斯权重和偏置的全连接神经网络的分布，其中隐藏层宽度与大常数 $n$ 成比例。在非线性的温和假设下，我们得到了在大但有限的 $n$ 和任意固定网络深度下成立的正态逼近的定量界限。我们的定理表明，无论是对于有限维分布还是整个过程，随机全连接网络（及其导数）与相应的无限宽高斯过程之间的距离都会按照 $n^{-\gamma}$ 缩放，其中 $\gamma>0$，指数取决于用于度量差异的度量方式。我们的界限在网络宽度的依赖性方面比文献中以前提供的任何界限都要强。

    We study the distribution of a fully connected neural network with random Gaussian weights and biases in which the hidden layer widths are proportional to a large constant $n$. Under mild assumptions on the non-linearity, we obtain quantitative bounds on normal approximations valid at large but finite $n$ and any fixed network depth. Our theorems show, both for the finite-dimensional distributions and the entire process, that the distance between a random fully connected network (and its derivatives) to the corresponding infinite width Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent depending on the metric used to measure discrepancy. Our bounds are stronger in terms of their dependence on network width than any previously available in the literature.
    
[^193]: One-Versus-Others Attention: 可扩展的多模态集成

    One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])

    [http://arxiv.org/abs/2307.05435](http://arxiv.org/abs/2307.05435)

    提出了一种可扩展的多模态集成方法，通过一对多（OvO）注意力机制解决了多模态学习中超过三个模态的注意力计算问题。

    

    随着多模态学习模型在问题回答和自动驾驶等各种任务上超越单模态方法，多模态学习模型变得日益重要。尽管多模态学习的重要性，现有的工作仅关注于自然语言处理应用，其中模态数通常少于四个（音频、视频、文本、图像）。然而，在其他领域，如医疗领域，数据输入可能包括X射线、PET扫描、MRI、遗传筛查、临床笔记等，这就需要高效而准确的信息融合。许多最先进的模型依赖于两两跨模态注意力，但对于超过三个模态的应用，这种方法不会很好地扩展。对于$n$个模态，计算注意力将导致$n \choose 2$的复杂度，可能需要大量的计算资源。为了解决这个问题，我们提出了一种新的领域中立的注意力机制，即一对多（OvO）注意力，该机制随着模态数量线性扩展。

    Multimodal learning models have become increasingly important as they surpass single-modality approaches on diverse tasks ranging from question-answering to autonomous driving. Despite the importance of multimodal learning, existing efforts focus on NLP applications, where the number of modalities is typically less than four (audio, video, text, images). However, data inputs in other domains, such as the medical field, may include X-rays, PET scans, MRIs, genetic screening, clinical notes, and more, creating a need for both efficient and accurate information fusion. Many state-of-the-art models rely on pairwise cross-modal attention, which does not scale well for applications with more than three modalities. For $n$ modalities, computing attention will result in $n \choose 2$ operations, potentially requiring considerable amounts of computational resources. To address this, we propose a new domain-neutral attention mechanism, One-Versus-Others (OvO) attention, that scales linearly with
    
[^194]: 通过基于分数的优化提升对抗鲁棒性

    Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04333](http://arxiv.org/abs/2307.04333)

    本文介绍了一种名为ScoreOpt的新型对抗防御方案，该方案通过优化对抗样本来提高模型的鲁棒性，实验证明该方法在多个数据集上优于现有的对抗防御方法。

    

    对抗攻击有可能通过引入微小扰动来误导深度神经网络分类器。开发能够减轻这些攻击影响的算法对确保人工智能的安全使用至关重要。最近的研究表明，基于分数的扩散模型在对抗防御中是有效的。然而，现有的基于扩散的防御依赖于顺序模拟扩散模型的反向随机微分方程，这在计算效率上是低效的，并且产生次优结果。在本文中，我们介绍了一种名为ScoreOpt的新型对抗防御方案，该方案在测试时通过在由基于分数先验指导的方向上对原始干净数据进行优化来优化对抗样本。我们在多个数据集上进行了全面的实验，包括CIFAR10、CIFAR100和ImageNet。我们的实验结果表明，我们的方法在鲁棒性方面优于现有的对抗防御方法。

    Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
    
[^195]: DISCO-10M：一个大规模的音乐数据集

    DISCO-10M: A Large-Scale Music Dataset. (arXiv:2306.13512v1 [cs.SD])

    [http://arxiv.org/abs/2306.13512](http://arxiv.org/abs/2306.13512)

    DISCO-10M是一个新颖而广泛的音乐数据集，其规模超过了之前最大的可用音乐数据集一个数量级，提供高质量的音频资源和CLAP嵌入以加速机器学习在音乐领域的研究和应用。

    

    音乐数据集在推动音乐机器学习研究方面发挥着至关重要的作用。然而，现有音乐数据集存在规模有限、可访问性差和缺乏音频资源等缺点。为了解决这些问题，我们提出了DISCO-10M，这是一个新颖而广泛的音乐数据集，其规模超过了之前最大的可用音乐数据集一个数量级。为确保高质量的数据，我们实施了一个多阶段过滤流程。这个过程结合了基于文本描述和音频嵌入的相似性。此外，我们提供了预计算的CLAP嵌入和DISCO-10M，便于直接应用于各种下游任务。这些嵌入使得对所提供数据的机器学习应用的高效探索成为可能。我们旨在通过DISCO-10M，推动音乐机器学习模型开发的新研究，以实现民主化和促进发展。

    Music datasets play a crucial role in advancing research in machine learning for music. However, existing music datasets suffer from limited size, accessibility, and lack of audio resources. To address these shortcomings, we present DISCO-10M, a novel and extensive music dataset that surpasses the largest previously available music dataset by an order of magnitude. To ensure high-quality data, we implement a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings. Moreover, we provide precomputed CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks. These embeddings enable efficient exploration of machine learning applications on the provided data. With DISCO-10M, we aim to democratize and facilitate new research to help advance the development of novel machine learning models for music.
    
[^196]: 模型训练中的模块化：一种新的模块化深度神经网络的范式

    Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])

    [http://arxiv.org/abs/2306.09376](http://arxiv.org/abs/2306.09376)

    本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    

    深度神经网络(DNN)模型已成为智能软件系统中越来越关键的组成部分。然而，训练DNN模型通常在时间和成本方面都很昂贵。为了解决这个问题，研究人员最近开始关注重用现有的DNN模型-借鉴软件工程中的代码重用思想。但是，重用整个模型可能会造成额外的开销或从不需要的功能中继承弱点。因此，现有的工作提出将已经训练好的模型分解成模块，即训练后的模块化，并实现模块的重用。但是，由于已经训练好的模型并不是为了模块化而构建的，所以训练后的模块化会导致巨大的开销和模型精度损失。本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化（MwT）。我们通过两个损失函数在模型训练过程中使模型具有结构上的模块化能力，这两个损失函数同时优化模块内的内聚性和模块之间的独立性，从而得到一个真正的模块化模型。我们展示了我们的方法可以在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
    
[^197]: PINNacle: 用于解决偏微分方程的物理约束神经网络的全面基准测试

    PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs. (arXiv:2306.08827v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08827](http://arxiv.org/abs/2306.08827)

    PINNacle是一个全面的物理约束神经网络(PINNs)基准测试工具，提供了一个多样化的数据集和约10种最先进的方法，用于解决真实世界问题中的各种偏微分方程(PDEs)挑战。

    

    尽管在物理约束神经网络(PINNs)方面取得了显著进展，但在广泛范围的偏微分方程(PDEs)上对这些方法进行全面比较仍然缺乏。本研究介绍了一个名为PINNacle的基准测试工具，旨在填补这一空白。PINNacle提供了一个多样化的数据集，包括来自各个领域的20多个不同的PDEs，包括热传导、流体动力学、生物学和电磁学。这些PDEs包含了真实世界问题的关键挑战，如复杂几何、多尺度现象、非线性和高维度。PINNacle还提供了一个用户友好的工具箱，其中包含约10种最先进的PINN方法，用于系统评估和比较。我们对这些方法进行了大量实验，为它们的优点和缺点提供了深入的见解。除了提供一个标准化的评估性能的手段，PINNacle还提供了深入分析，指导未来的研究。

    While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains, including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future r
    
[^198]: FedJETs：具有联邦混合专家的高效及时个性化方法

    FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts. (arXiv:2306.08586v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08586](http://arxiv.org/abs/2306.08586)

    本论文提出了一种名为FedJETs的方法，使用联邦混合专家的框架，在联邦学习中实现高效及时的个性化。该方法通过训练专门的专家，并利用门控函数将输入路由到相关的专家，有效提高了模型的准确性。

    

    联邦学习（FL）的目标之一是创建能够适应每个参与客户端上下文的个性化模型，同时利用共享全局模型的知识。然而，通常情况下，个性化需要使用客户标记的数据进行微调以实现良好的性能，这在新来的客户端是不可行的，在隐私方面也存在问题。因此，如何在这些场景中实现及时个性化仍然是个未解决的问题。我们提出了FedJETs，这是一个在FL设置中使用“专家混合（MoE）”框架的新颖解决方案。我们的方法利用客户的多样性，在不同的类别子集上训练专门的专家，并利用一个门控函数将输入路由到最相关的专家。我们的门控函数利用预训练模型的共享专家的知识，以增强其即时的路由决策。值得一提的是，我们的方法能够将准确性提高高达18％，达到现有技术水平水平。

    One of the goals in Federated Learning (FL) is to create personalized models that can adapt to the context of each participating client, while utilizing knowledge from a shared global model. Yet, often, personalization requires a fine-tuning step using clients' labeled data in order to achieve good performance. This may not be feasible in scenarios where incoming clients are fresh and/or have privacy concerns. It, then, remains open how one can achieve just-in-time personalization in these scenarios. We propose FedJETs, a novel solution by using a Mixture-of-Experts (MoE) framework within a FL setup. Our method leverages the diversity of the clients to train specialized experts on different subsets of classes, and a gating function to route the input to the most relevant expert(s). Our gating function harnesses the knowledge of a pretrained model common expert to enhance its routing decisions on-the-fly. As a highlight, our approach can improve accuracy up to 18\% in state of the art F
    
[^199]: SqueezeLLM：密集稀疏量化

    SqueezeLLM: Dense-and-Sparse Quantization. (arXiv:2306.07629v1 [cs.CL])

    [http://arxiv.org/abs/2306.07629](http://arxiv.org/abs/2306.07629)

    本文提出了一种基于训练后的量化框架——SqueezeLLM，它不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。

    

    生成式大型语言模型(LLMs)已经证明在广泛领域的任务中取得了非凡的成果。但是由于其前所未有的资源需求，将这些模型用于推理一直是一个巨大的挑战。这导致现有的部署框架需要使用多GPU推理管道，这通常是复杂和昂贵的，或者使用更小且性能更低的模型。在这项工作中，我们证明了用于LLMs生成推断的主要瓶颈是内存带宽，而不是计算，尤其是单个批次推理。虽然通过使用减少精度来表示模型权重，量化已经成为一种有前途的解决方案，但是以前的努力通常导致性能下降。为了解决这个问题，我们引入SqueezeLLM，这是一种基于训练后的量化框架，不仅可以实现高达3位的无损压缩，而且在相同的内存约束下实现更高的量化性能。

    Generative Large Language Models (LLMs) have demonstrated remarkable results for a wide range of tasks. However, deploying these models for inference has been a significant challenge due to their unprecedented resource requirements. This has forced existing deployment frameworks to use multi-GPU inference pipelines, which are often complex and costly, or to use smaller and less performant models. In this work, we demonstrate that the main bottleneck for generative inference with LLMs is memory bandwidth, rather than compute, specifically for single batch inference. While quantization has emerged as a promising solution by representing model weights with reduced precision, previous efforts have often resulted in notable performance degradation. To address this, we introduce SqueezeLLM, a post-training quantization framework that not only enables lossless compression to ultra-low precisions of up to 3-bit, but also achieves higher quantization performance under the same memory constraint
    
[^200]: 在单位球上学习表示：应用于在线连续学习

    Learning Representations on the Unit Sphere: Application to Online Continual Learning. (arXiv:2306.03364v1 [cs.LG])

    [http://arxiv.org/abs/2306.03364](http://arxiv.org/abs/2306.03364)

    该论文提出了一种基于单位球的表示学习方法，通过将表示推向固定方向，使得学习策略对数据漂移具有弹性，从而能够应对在线连续学习的挑战性问题。

    

    我们使用最大后验估计原理来学习分布在单位球上的表示。我们针对对称方向数据建立了 von Mises-Fisher 分布和角高斯分布的损失函数。我们方法的一个显著特点是，学习到的表示被推向固定的方向，使得学习策略对数据漂移具有弹性。这使得它适合于在线连续学习，即在连续的数据流上训练神经网络的问题，其中多个分类任务按顺序呈现，因此过去任务的数据不再可用，当前任务的数据只能看一次。为了应对这种具有挑战性的情况，我们提出了一种基于记忆的表示学习技术，配备了我们的新损失函数。我们的方法不需要负数据或任务边界的知识，并且在较小的批处理下表现良好。

    We use the maximum a posteriori estimation principle for learning representations distributed on the unit sphere. We derive loss functions for the von Mises-Fisher distribution and the angular Gaussian distribution, both designed for modeling symmetric directional data. A noteworthy feature of our approach is that the learned representations are pushed toward fixed directions, allowing for a learning strategy that is resilient to data drift. This makes it suitable for online continual learning, which is the problem of training neural networks on a continuous data stream, where multiple classification tasks are presented sequentially so that data from past tasks are no longer accessible, and data from the current task can be seen only once. To address this challenging scenario, we propose a memory-based representation learning technique equipped with our new loss functions. Our approach does not require negative data or knowledge of task boundaries and performs well with smaller batch s
    
[^201]: 通过转化特定注释者和特定实例的转移矩阵从众包中学习

    Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])

    [http://arxiv.org/abs/2306.03116](http://arxiv.org/abs/2306.03116)

    本文提出了一个高效的方法来估算特定注释者和特定实例的转移矩阵以及真实标签比例，解决了从众包中学习的标签噪声问题，并在实验中证明了方法的优越性。

    

    本文描述了从众包服务中获取训练数据的注释方法。每个注释者都完成自己的小部分注释，不同注释者的标注错误往往不同。通过标签噪声的转移矩阵来建模噪声产生过程是解决标签噪声的一种有效工具。在实际众包模型中，转移矩阵既由注释者依赖，也由实例依赖。然而，由于注释者和实例依赖的转移矩阵(AIDTM)具有高复杂度，而实际注释往往涉及注释稀疏性，这使得建立AIDTM非常具有挑战性。既要保持建模的广泛性，又能更真实地解决问题，本文提出了一种高效的算法，可以同时估算AIDTM和真实标签比例。我们还提供了理论分析，证明了我们的算法的收敛性。在合成数据集和真实数据集上的实验结果表明，我们的算法优于基准方法。

    Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
    
[^202]: 多目标学习中的三重权衡：优化、泛化和冲突避免

    Three-Way Trade-Off in Multi-Objective Learning: Optimization, Generalization and Conflict-Avoidance. (arXiv:2305.20057v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20057](http://arxiv.org/abs/2305.20057)

    本文研究了多目标学习中的三重权衡问题，通过研究动态加权算法在MoDo算法中的泛化性能和与优化的相互作用，发现了动态加权方法的局限性。

    

    在新兴的机器学习问题中，当存在多个学习准则或多个学习任务时，多目标学习（MOL）问题经常出现。最近的研究提出了各种动态加权算法用于MOL，如MGDA及其变种，其核心思想是找到一个能够避免目标冲突的更新方向。尽管其直观吸引人，实证研究表明动态加权方法并不总是优于静态方法。为了理解这一理论与实践的差距，我们重点研究了MGDA的新随机变体-多目标梯度双采样（MoDo）算法，并通过算法稳定性的视角研究了基于动态加权的MoDo算法的泛化性能以及其与优化的相互作用。出乎意料的是，我们发现MGDA背后的关键原理-沿着避免冲突的方向进行更新-可能会阻碍动态加权算法实现${\cal O}(1/\sqrt{n})$的最优泛化性能。

    Multi-objective learning (MOL) problems often arise in emerging machine learning problems when there are multiple learning criteria or multiple learning tasks. Recent works have developed various dynamic weighting algorithms for MOL such as MGDA and its variants, where the central idea is to find an update direction that avoids conflicts among objectives. Albeit its appealing intuition, empirical studies show that dynamic weighting methods may not always outperform static ones. To understand this theory-practical gap, we focus on a new stochastic variant of MGDA - the Multi-objective gradient with Double sampling (MoDo) algorithm, and study the generalization performance of the dynamic weighting-based MoDo and its interplay with optimization through the lens of algorithm stability. Perhaps surprisingly, we find that the key rationale behind MGDA -- updating along conflict-avoidant direction - may hinder dynamic weighting algorithms from achieving the optimal ${\cal O}(1/\sqrt{n})$ popu
    
[^203]: 揭示基于注意力的图神经网络中的平滑过度现象

    Demystifying Oversmoothing in Attention-Based Graph Neural Networks. (arXiv:2305.16102v1 [cs.LG])

    [http://arxiv.org/abs/2305.16102](http://arxiv.org/abs/2305.16102)

    本文通过数学分析证明基于注意力的图神经网络并不能解决平滑过度问题，在实际应用中需要更多关注不对称、状态相关和有向图结构。

    

    图神经网络中的平滑过度指的是增加网络深度导致节点表示变得相同的现象。尽管之前的研究已经证实了图卷积网络(GCN)会指数级失去表达能力，但是图注意力机制是否可以缓解平滑过度问题还存在争议。本文通过将基于注意力的图神经网络视为非线性时变动态系统，并结合非齐次矩阵乘积和联合谱半径理论的工具和技术，对这个问题进行了严格的数学分析，提出了一个明确的答案。我们证明了与流行观点相反，图注意力机制不能防止平滑过度现象，并且呈指数级失去表达能力。所提出的框架将对称GCN的平滑过度问题扩展到了更广泛的GNN模型类别中。特别地，我们的分析考虑了在现实应用中普遍存在的不对称、状态相关和有向图结构。

    Oversmoothing in Graph Neural Networks (GNNs) refers to the phenomenon where increasing network depth leads to homogeneous node representations. While previous work has established that Graph Convolutional Networks (GCNs) exponentially lose expressive power, it remains controversial whether the graph attention mechanism can mitigate oversmoothing. In this work, we provide a definitive answer to this question through a rigorous mathematical analysis, by viewing attention-based GNNs as nonlinear time-varying dynamical systems and incorporating tools and techniques from the theory of products of inhomogeneous matrices and the joint spectral radius. We establish that, contrary to popular belief, the graph attention mechanism cannot prevent oversmoothing and loses expressive power exponentially. The proposed framework extends the existing results on oversmoothing for symmetric GCNs to a significantly broader class of GNN models. In particular, our analysis accounts for asymmetric, state-dep
    
[^204]: 学习鲁棒统计用于模型错误情况下的基于模拟推论

    Learning Robust Statistics for Simulation-based Inference under Model Misspecification. (arXiv:2305.15871v1 [stat.ML])

    [http://arxiv.org/abs/2305.15871](http://arxiv.org/abs/2305.15871)

    本研究提出首个通用的方法来处理基于模拟的推论（如ABC和NPE）中由于模型错误引起的不可靠推论。通过约束统计量的选择，我们的方法通过惩罚与数据和模型之间不匹配的统计量来防止不可靠推论结果。我们在高维时间序列模型上进行了实验，证明了本方法的优越性能。

    

    基于模拟的推论方法（如近似贝叶斯计算（ABC），合成似然性和神经后验估计（NPE））依赖于模拟统计量以推断难以计算的似然模型的参数。然而，已知这种方法在模型错误情况下会产生不可信和误导性的推论结果，从而阻碍了它们的广泛应用。在本文中，我们提出了第一个通用方法来处理跨不同类别的SBI方法的模型错误情况。利用统计量的选择确定SBI中的误差程度，我们引入了一个正则化损失函数，惩罚那些增加数据和模型之间不匹配的统计量。以NPE和ABC为应用案例，我们展示了我们的方法在人工错误规范化的高维时间序列模型上表现出优越的性能。我们还将我们的方法应用于来自无线电传播领域的实际数据。

    Simulation-based inference (SBI) methods such as approximate Bayesian computation (ABC), synthetic likelihood, and neural posterior estimation (NPE) rely on simulating statistics to infer parameters of intractable likelihood models. However, such methods are known to yield untrustworthy and misleading inference outcomes under model misspecification, thus hindering their widespread applicability. In this work, we propose the first general approach to handle model misspecification that works across different classes of SBI methods. Leveraging the fact that the choice of statistics determines the degree of misspecification in SBI, we introduce a regularized loss function that penalises those statistics that increase the mismatch between the data and the model. Taking NPE and ABC as use cases, we demonstrate the superior performance of our method on high-dimensional time-series models that are artificially misspecified. We also apply our method to real data from the field of radio propagat
    
[^205]: 通过求解最优边界条件解决扩散ODE问题以实现更好的图像超分辨率

    Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image Super-Resolution. (arXiv:2305.15357v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2305.15357](http://arxiv.org/abs/2305.15357)

    本文提出了一种求解最优边界条件解决扩散ODE问题的有效采样方法，以稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像。

    

    扩散模型作为一种强大的生成模型，已经在图像超分辨率任务中取得了令人印象深刻的结果。然而，由于扩散模型反向过程中引入的随机性，基于扩散的超分辨率模型在每次采样时性能波动很大，特别是对于具有少量重新采样步骤的采样器。扩散模型的这种固有随机性导致其无效和不稳定，使用户难以保证超分辨结果的质量。然而，我们的工作将这种随机性视为一种机遇：全面分析和利用它导致了构建一种有效的即插即用采样方法，具有潜力使一系列基于扩散的超分辨率方法受益。更详细地说，我们建议通过求解扩散普通微分方程（扩散ODE）和最优边界条件（BC），稳定地从预训练的基于扩散的超分辨率模型中采样高质量的超分辨率图像，并分析其特性。

    Diffusion models, as a kind of powerful generative model, have given impressive results on image super-resolution (SR) tasks. However, due to the randomness introduced in the reverse process of diffusion models, the performances of diffusion-based SR models are fluctuating at every time of sampling, especially for samplers with few resampled steps. This inherent randomness of diffusion models results in ineffectiveness and instability, making it challenging for users to guarantee the quality of SR results. However, our work takes this randomness as an opportunity: fully analyzing and leveraging it leads to the construction of an effective plug-and-play sampling method that owns the potential to benefit a series of diffusion-based SR methods. More in detail, we propose to steadily sample high-quality SR images from pretrained diffusion-based SR models by solving diffusion ordinary differential equations (diffusion ODEs) with optimal boundary conditions (BCs) and analyze the characterist
    
[^206]: 使用神经薛定谔桥实现非配对图像转换

    Unpaired Image-to-Image Translation via Neural Schr\"odinger Bridge. (arXiv:2305.15086v1 [cs.CV])

    [http://arxiv.org/abs/2305.15086](http://arxiv.org/abs/2305.15086)

    本文提出了一种方法——非配对神经薛定谔桥 (UNSB)，它结合了薛定谔桥、对抗训练和正则化，用于在非配对数据之间学习 SDE，并成功解决了许多非配对图像转换任务。

    

    扩散模型是一类生成模型，它通过模拟随机微分方程（SDE）从噪声生成数据。尽管扩散模型在最近取得了显著进展，但由于高斯先验假设，它们在非配对的图像转换任务中存在局限性。薛定谔桥是一种学习 SDE 以在两个任意分布之间转换的方法，被视为解决这个问题的一种有吸引力的解决方案。然而，迄今为止，薛定谔桥模型在高分辨率图像之间的非配对转换方面并不成功。在这项工作中，我们提出了非配对神经薛定谔桥（UNSB），它将薛定谔桥与对抗性训练和正则化相结合，以学习非配对数据之间的 SDE。我们证明了 UNSB 是可伸缩的，并且成功解决了各种非配对图像转换任务。

    Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. Although diffusion models have achieved remarkable progress in recent years, they have limitations in the unpaired image-to-image translation tasks due to the Gaussian prior assumption. Schr\"odinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. However, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose the Unpaired Neural Schr\"odinger Bridge (UNSB), which combines SB with adversarial training and regularization to learn a SB between unpaired data. We demonstrate that UNSB is scalable, and that it successfully solves various unpaired image-to-image translation tasks. Code: \url{https://github.com/cyclomon/UNSB}
    
[^207]: 语言模型的物理学：第一部分，上下文无关文法。

    Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])

    [http://arxiv.org/abs/2305.13673](http://arxiv.org/abs/2305.13673)

    本研究探究了生成式语言模型如何学习上下文无关文法（CFG），并通过构造人造数据证明了预训练transformers可以学会生成具有接近完美准确度和显着多样性的句子。研究发现transformer内部的隐藏状态隐含而精确地编码了CFG结构，学会形成类似动态规划的“边界到边界”的注意力。此外，还研究了标准CFG的扩展，例如概率CFG和线性CFG，并证明transformers也可以学会这些扩展语法结构。

    

    我们设计了实验来研究生成式语言模型（例如GPT）如何学习上下文无关文法（CFG）-具有树状结构的多样化语言系统，可捕捉许多自然语言，程序和人类逻辑的方面。CFG与下推自动机一样困难，可能是模棱两可的，因此验证字符串是否满足规则需要动态规划。我们构造了人造数据，并证明即使对于非常具有挑战性的CFG，预训练transformers也可以学会生成具有接近完美准确度和显着多样性的句子。更重要的是，我们深入探讨了transformers学习CFG背后的物理原理。我们发现transformer内部的隐藏状态隐含而精确地编码了CFG结构（如在子树边界上精确定位树节点信息），并学会形成类似动态规划的“边界到边界”的注意力。我们还涵盖了一些标准CFG的扩展，例如概率CFG和线性CFG，并展示transformers也可以学会这些扩展语法结构。我们的工作揭示了语言模型的内部工作原理，并为未来的模型设计和分析提供了启示。

    We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
    
[^208]: 将 Emergent In-Context Learning 解释为核回归

    Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.12766](http://arxiv.org/abs/2305.12766)

    本文研究了为什么在预训练之后，基于Transformer的语言模型能够实现上下文学习，并提出了一种假设，认为LLMs在面对上下文示例时能够通过内部表示模拟核回归。

    

    大型语言模型（LLMs）在迁移学习中引起了一场范式转变。与经典的预训练-微调过程相比，为了将LLMs用于下游预测任务，只需要提供一些示例，即上下文示例，而无需添加或更新现有的模型参数。LLMs的这种上下文学习能力非常有意思，但目前尚不完全了解预训练LLMs如何获得这种能力。本文通过提出一个假设，即当面临上下文示例时，LLMs能够通过内部表示模拟核回归，来研究为何基于Transformer的语言模型能够在预训练通用语料库之后实现上下文学习。具体来说，我们首先证明了上下文提示的贝叶斯推断在渐近情况下可以被理解为核回归 $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$，

    Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demon
    
[^209]: AnyPredict: 表格预测的基础模型

    AnyPredict: Foundation Model for Tabular Prediction. (arXiv:2305.12081v1 [cs.LG])

    [http://arxiv.org/abs/2305.12081](http://arxiv.org/abs/2305.12081)

    本文提出了一种名为 AnyPredict 的表格预测基础模型，使用数据引擎整合领域内和广泛的领域外数据集，以克服模式不匹配和预测目标异质性等方面的障碍。

    

    基础模型是在大规模数据上预先训练的模型，可以在许多下游任务中表现良好。它们在自然语言处理和计算机视觉方面取得了显著的成功。然而，这种模型在表格预测任务中的使用受到限制，主要问题包括 (1) 缺乏大规模和多样化的带有标准标签的表格数据集，以及 (2) 不同领域之间的模式不匹配和预测目标的异质性。本文提出了一种方法，用于构建基于 AnyPredict 的表格预测基础模型的大规模训练数据，包括领域内和广泛的领域外数据集。该方法使用数据引擎，利用大型语言模型 (LLM) 来整合表格样本，克服了不同模式表格之间的障碍，并使用“学习，注释和审计”流程将领域外数据与目标任务对齐。扩展的训练数据使预训练的 AnyPredict 能够支持每个表格领域。

    Foundation models are pre-trained on massive data to perform well across many downstream tasks. They have demonstrated significant success in natural language processing and computer vision. Nonetheless, the use of such models in tabular prediction tasks has been limited, with the main hurdles consisting of (1) the lack of large-scale and diverse tabular datasets with standardized labels and (2) the schema mismatch and predictive target heterogeneity across domains.  This paper proposes a method for building training data at scale for tabular prediction foundation models (AnyPredict) using both in-domain and a wide range of out-domain datasets. The method uses a data engine that leverages large language models (LLMs) to consolidate tabular samples to overcome the barrier across tables with varying schema and align out-domain data with the target task using a ``learn, annotate, and audit'' pipeline. The expanded training data enables the pre-trained AnyPredict to support every tabular d
    
[^210]: 使用约束贝叶斯优化的网络级联漏洞研究

    Network Cascade Vulnerability using Constrained Bayesian Optimization. (arXiv:2304.14420v1 [cs.SI])

    [http://arxiv.org/abs/2304.14420](http://arxiv.org/abs/2304.14420)

    本研究基于约束贝叶斯优化，以修改输电线路保护设置为敌对攻击的候选方案，探讨了最大化级联网络退化的保护设置规律，发现将所有电网线路的保护设置最大失配并不会导致最多的级联。

    

    评估电网的脆弱性常常是通过敌手能够对网络造成的损害量来衡量的。然而，这样攻击的级联影响通常被忽视，尽管级联是大规模停电的主要原因之一。本文探讨了将输电线路保护设置修改为敌对攻击的候选方案，只要网络平衡状态不改变，攻击就可以保持不被检测到。这构成了贝叶斯优化过程中的一个黑盒子函数基础，其目标是找到最大化级联网络退化的保护设置。广泛的实验表明，与常识相反，将所有网络线路的保护设置最大失配并不会导致最多的级联。更令人惊讶的是，即使在资源受限的情况下，仍然可以找到能够产生与实例相当严重的级联的设置。

    Measures of power grid vulnerability are often assessed by the amount of damage an adversary can exact on the network. However, the cascading impact of such attacks is often overlooked, even though cascades are one of the primary causes of large-scale blackouts. This paper explores modifications of transmission line protection settings as candidates for adversarial attacks, which can remain undetectable as long as the network equilibrium state remains unaltered. This forms the basis of a black-box function in a Bayesian optimization procedure, where the objective is to find protection settings that maximize network degradation due to cascading. Extensive experiments reveal that, against conventional wisdom, maximally misconfiguring the protection settings of all network lines does not cause the most cascading. More surprisingly, even when the degree of misconfiguration is resource constrained, it is still possible to find settings that produce cascades comparable in severity to instanc
    
[^211]: 基于理想联合分类器假设的知识蒸馏

    Knowledge Distillation Under Ideal Joint Classifier Assumption. (arXiv:2304.11004v1 [cs.LG])

    [http://arxiv.org/abs/2304.11004](http://arxiv.org/abs/2304.11004)

    本文提出了基于理想联合分类器假设的知识蒸馏框架，可以提供清晰全面的理解和为未来研究提供理论基础，使得教师和学生网络之间的知识传递更加高效。

    

    知识蒸馏是一种将大型神经网络压缩为更高效小型网络的强大技术。Softmax回归表征学习是一种常用的方法，它使用预先训练的教师网络来指导更小的学生网络的学习。尽管有几项研究探讨了Softmax回归表征学习的有效性，但提供知识转移的基础机制尚不够清楚。本文提出了理想联合分类器知识蒸馏（IJCKD），这是一个统一的框架，旨在为现有的知识蒸馏方法提供清晰全面的理解和为未来研究提供理论基础。我们使用从领域适应理论推导出的数学技术，提供了学生网络误差界的详细分析，其作为教师的函数关系。我们的框架可以在深度学习中应用于各种应用，包括图像识别和自然语言处理。

    Knowledge distillation is a powerful technique to compress large neural networks into smaller, more efficient networks. Softmax regression representation learning is a popular approach that uses a pre-trained teacher network to guide the learning of a smaller student network. While several studies explored the effectiveness of softmax regression representation learning, the underlying mechanism that provides knowledge transfer is not well understood. This paper presents Ideal Joint Classifier Knowledge Distillation (IJCKD), a unified framework that provides a clear and comprehensive understanding of the existing knowledge distillation methods and a theoretical foundation for future research. Using mathematical techniques derived from a theory of domain adaptation, we provide a detailed analysis of the student network's error bound as a function of the teacher. Our framework enables efficient knowledge transfer between teacher and student networks and can be applied to various applicati
    
[^212]: ChatGPT可靠性的测量与特征化

    In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT. (arXiv:2304.08979v1 [cs.CR])

    [http://arxiv.org/abs/2304.08979](http://arxiv.org/abs/2304.08979)

    本文首次对ChatGPT在通用问答场景中的可靠性进行了大规模测量，发现其在不同领域的可靠性有所差异，尤其在法律和科学问题方面表现不佳，并容易受到对抗性示例的影响，对其在实际应用中的使用产生影响。

    

    随着ChatGPT的出现，用户获取信息的方式正在发生范式转变。与传统的搜索引擎不同，ChatGPT从模型本身检索知识并为用户生成答案。ChatGPT令人印象深刻的问答能力吸引了超过1亿用户，但也引发了人们关于其可靠性的担忧。本文通过精心策划的5695个问题跨越十个数据集和八个领域，首次对ChatGPT在通用问答场景中的可靠性进行了大规模测量。我们发现ChatGPT的可靠性因不同领域而异，尤其在法律和科学问题方面表现不佳。我们还证明了OpenAI设计的系统角色可以影响ChatGPT的可靠性。我们进一步展示了ChatGPT容易受到对抗性示例的影响，即使是单个字符的更改也会对其可靠性产生负面影响。我们的结果揭示了ChatGPT可靠性的局限性，并对其在实际应用中的使用产生影响。

    The way users acquire information is undergoing a paradigm shift with the advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves knowledge from the model itself and generates answers for users. ChatGPT's impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. In this paper, we perform the first large-scale measurement of ChatGPT's reliability in the generic QA scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions. We also demonstrate that system roles, originally designed by OpenAI to allow users to steer ChatGPT's behavior, can impact ChatGPT's reliability. We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reli
    
[^213]: 通过解释不变性和等变性评估解释方法的健壮性

    Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance. (arXiv:2304.06715v1 [cs.LG])

    [http://arxiv.org/abs/2304.06715](http://arxiv.org/abs/2304.06715)

    本文提出了解释不变性和等变性的概念，通过对对称群下具有不变性的神经网络，建立两种度量方法来提高解释方法对于不变性的健壮性并证明为一些流行的解释方法提供了理论健壮性保证。

    

    只有当解释方法忠实地描述所解释的模型时，解释方法才有价值。本文考虑了神经网络，其预测在特定对称群下具有不变性，这包括从卷积神经网络到图神经网络的流行架构。任何忠实描述这种类型模型的解释都需要与该不变性属性一致。我们通过运用几何深度学习的形式化方法，通过解释不变性和等变性的概念来形式化这种直觉。通过这种严格的形式化方法，我们得出了（1）两个度量来衡量任何解释方法相对于模型对称群的健壮性;（2）一些流行的解释方法的理论健壮性保证；（3）提高任何解释方法相对于对称群的不变性的系统方法。通过在与不同对称群相关的模型的解释中经验地测量我们的度量标准，我们展示了解释不变性和等变性对于强大的解释方法是重要的属性。

    Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associate
    
[^214]: 代数和几何模型在空间网络中的应用

    Algebraic and Geometric Models for Space Networking. (arXiv:2304.01150v2 [math.AT] UPDATED)

    [http://arxiv.org/abs/2304.01150](http://arxiv.org/abs/2304.01150)

    本文提出了一种新的代数和几何模型来研究网络空间通信。通过定义时间变化图（TVG），以及利用矩阵和半环性质进行建模，我们能够生成TVG的通信容量的新统计量并分析STARLINK卫星之间的连接性。此外，还引入了基于拓扑数据分析的度量标准来进一步分析STARLINK的强连接性。最后，我们还介绍了一些能够模拟地球和火星网络场景的半环模型。

    

    本文介绍了一些关于网络空间通信的新的代数和几何视角。我们的主要贡献是对时间变化图（TVG）的新定义，该定义采用实线集合P（R）中的矩阵值进行表示。我们利用P（R）的半环性质，使用矩阵乘法和截断的克莱尼星号来建模TVG中的多跳通信。这导致了对TVG的通信容量的新型统计量，称为寿命曲线，我们通过对大样本的随机选择的STARLINK卫星进行日长模拟来生成这些统计数据。我们还引入了一些基于拓扑数据分析（TDA）启发的新度量标准来进一步分析确定大样本的STARLINK在时空上的强连接性。为了更好地模拟地球和火星之间的网络情景，我们引入了能够模拟传播延迟以及延迟容忍网络中常见协议的各种半环。

    In this paper we introduce some new algebraic and geometric perspectives on networked space communications. Our main contribution is a novel definition of a time-varying graph (TVG), defined in terms of a matrix with values in subsets of the real line P(R). We leverage semi-ring properties of P(R) to model multi-hop communication in a TVG using matrix multiplication and a truncated Kleene star. This leads to novel statistics on the communication capacity of TVGs called lifetime curves, which we generate for large samples of randomly chosen STARLINK satellites, whose connectivity is modeled over day-long simulations. Determining when a large subsample of STARLINK is temporally strongly connected is further analyzed using novel metrics introduced here that are inspired by topological data analysis (TDA). To better model networking scenarios between the Earth and Mars, we introduce various semi-rings capable of modeling propagation delay as well as protocols common to Delay Tolerant Netwo
    
[^215]: 抽象器：基于Transformer的符号消息传递和关系推理模块

    Abstractors: Transformer Modules for Symbolic Message Passing and Relational Reasoning. (arXiv:2304.00195v1 [stat.ML])

    [http://arxiv.org/abs/2304.00195](http://arxiv.org/abs/2304.00195)

    该论文提出了一个基于Transformer的框架，用于实现符号消息传递和关系推理，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    

    该论文提出了一个框架，将关系学习转化为Transformer模型，并通过关系交叉注意力机制实现感性状态与抽象状态之间的绑定。

    A framework is proposed that casts relational learning in terms of transformers, implementing binding between sensory states and abstract states with relational cross attention mechanisms.
    
[^216]: 探究预训练标签粒度的影响

    Towards Understanding the Effect of Pretraining Label Granularity. (arXiv:2303.16887v1 [cs.CV])

    [http://arxiv.org/abs/2303.16887](http://arxiv.org/abs/2303.16887)

    本文探究了预训练标签粒度对深度神经网络图像分类任务的泛化能力的影响，并在iNaturalist 2021与ImageNet数据集中进行了实验证明，在预训练数据集具有强有力的标签层次结构，标签功能与目标任务对齐，以及选择适当的预训练标签粒度时，能有效提高模型的性能。

    

    本文研究了预训练标签粒度如何影响深度神经网络在图像分类任务中的泛化能力。我们关注“细到粗”的迁移学习设置，其中预训练标签比目标问题更细粒度。我们使用iNaturalist 2021的标签层次结构进行了该方法的实验，并观察到相对于基线错误率有8.76％的相对改进。我们发现以下条件对于改进非常关键：1）预训练数据集具有强大且有意义的标签层次结构，2）其标签功能与目标任务的功能强烈对齐，最重要的是，3）选择了适当级别的预训练标签粒度。我们在ImageNet上的迁移学习实验进一步证明了预训练标签粒度的重要性。值得注意的是，我们展示了在ImageNet21k上的叶标签预训练产生了比其他合作标签更好的ImageNet1k迁移结果。

    In this paper, we study how pretraining label granularity affects the generalization of deep neural networks in image classification tasks. We focus on the "fine-to-coarse" transfer learning setting where the pretraining label is more fine-grained than that of the target problem. We experiment with this method using the label hierarchy of iNaturalist 2021, and observe a 8.76% relative improvement of the error rate over the baseline. We find the following conditions are key for the improvement: 1) the pretraining dataset has a strong and meaningful label hierarchy, 2) its label function strongly aligns with that of the target task, and most importantly, 3) an appropriate level of pretraining label granularity is chosen. The importance of pretraining label granularity is further corroborated by our transfer learning experiments on ImageNet. Most notably, we show that pretraining at the leaf labels of ImageNet21k produces better transfer results on ImageNet1k than pretraining at other coa
    
[^217]: GOAL:为实时足球解说生成提供具有挑战性的基于知识的视频字幕基准

    GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation. (arXiv:2303.14655v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.14655](http://arxiv.org/abs/2303.14655)

    GOAL是一个基于知识的视频字幕生成基准，旨在解决如何基于背景知识生成精细的视频描述的问题，该基准包含超过8.9k个足球视频和相关的知识三元组，并提供了解决这一任务的难度和潜在方向的实验结果。

    

    尽管最近出现了许多视频字幕生成模型，但如何基于背景知识生成生动、精细的视频描述（即关于特定领域场景的详细评论，并具有适当的推理），仍然存在很大的难题。本文提出了GOAL，一个包含超过8.9k个足球视频片段、22k个句子和42k个知识三元组的基准，用于提出一种具有挑战性的新任务设置作为基于知识的视频字幕生成（KGVC）。此外，我们对现有方法进行了实验性的改进，展示了解决这一宝贵而实用任务的难度和潜在方向。我们的数据和代码可在https://github.com/THU-KEG/goal 上获得。

    Despite the recent emergence of video captioning models, how to generate vivid, fine-grained video descriptions based on the background knowledge (i.e., long and informative commentary about the domain-specific scenes with appropriate reasoning) is still far from being solved, which however has great applications such as automatic sports narrative. In this paper, we present GOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k knowledge triples for proposing a challenging new task setting as Knowledge-grounded Video Captioning (KGVC). Moreover, we conduct experimental adaption of existing methods to show the difficulty and potential directions for solving this valuable and applicable task. Our data and code are available at https://github.com/THU-KEG/goal.
    
[^218]: 可微分逻辑的逻辑：走向DL的统一语义

    Logic of Differentiable Logics: Towards a Uniform Semantics of DL. (arXiv:2303.10650v2 [cs.LO] UPDATED)

    [http://arxiv.org/abs/2303.10650](http://arxiv.org/abs/2303.10650)

    该论文介绍了一个元语言——LDL，用于定义DL，该元语言从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    

    近期，可微分逻辑（DL）被提出作为一种训练神经网络满足逻辑规范的方法。DL包括语法和将语法中的表达式转化为损失函数的解释函数。这些损失函数可以在训练过程中与标准的梯度下降算法一起使用。 现有DL的多样性和对其形式化程度的不同处理使得对它们的性质和实现进行系统比较研究变得困难。该论文通过提出一个元语言——LDL作为DL定义的系统框架，从语法和语义两方面上提高DL的形式化程度，使得对DL的性质和实现进行系统比较研究成为了可能。

    Differentiable logics (DL) have recently been proposed as a method of training neural networks to satisfy logical specifications. A DL consists of a syntax in which specifications are stated and an interpretation function that translates expressions in the syntax into loss functions. These loss functions can then be used during training with standard gradient descent algorithms. The variety of existing DLs and the differing levels of formality with which they are treated makes a systematic comparative study of their properties and implementations difficult. This paper remedies this problem by suggesting a meta-language for defining DLs that we call the Logic of Differentiable Logics, or LDL. Syntactically, it generalises the syntax of existing DLs to FOL, and for the first time introduces the formalism for reasoning about vectors and learners. Semantically, it introduces a general interpretation function that can be instantiated to define loss functions arising from different existing 
    
[^219]: 图像统计与人类感知之间的关联关系分离

    Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v1 [cs.CV])

    [http://arxiv.org/abs/2303.09874](http://arxiv.org/abs/2303.09874)

    本研究直接评估自然图像的概率，并分析它如何影响人类感知。通过展示具有更丰富统计特征的自然图像被感知为具有更大的显着性，论文提供了直接支持Barlow和Attneave理论的证据，并建立了一个新的框架，用于理解图像统计与知觉之间的关系。

    

    在20世纪50年代，霍勒斯巴洛和弗雷德阿特纳夫提出了感官系统和它们如何适应环境之间的关系：早期视觉的进化是为了最大限度地传递关于输入信号的信息。按照香农的定义，这些信息是通过自然场景中拍摄的图像的概率来描述的。由于计算能力的限制，以前无法直接准确地预测图像的概率。尽管这种想法的探索是间接的，主要基于图像密度的过度简化模型或系统设计方法，但这些方法在重现各种生理和心理物理现象方面取得了成功。在本文中，我们直接评估自然图像的概率，并分析它如何确定知觉灵敏度。我们使用与人类意见相关性很高的图像质量指标作为人类视觉的代理，以及一个先进的生成模型来直接估计自然图像的概率密度函数。我们的结果表明，根据Barlow和Attneave理论预测的图像统计与人类知觉之间存在系统性的关联。我们通过展示具有更丰富统计特征的自然图像被感知为具有更大的显着性来说明这一发现，这是通过视觉搜索实验测量的。我们的工作提供了直接支持Barlow和Attneave理论的证据，并建立了一个新的框架，用于理解图像统计与知觉之间的关系。

    In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probabil
    
[^220]: 深动量多重边际Schr\"odinger桥模型

    Deep Momentum Multi-Marginal Schr\"odinger Bridge. (arXiv:2303.01751v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.01751](http://arxiv.org/abs/2303.01751)

    该论文提出了一种新的计算框架DMSB，它可以学习满足时间上位置边际约束的随机系统的平滑度量值样条，用于解决高维多边际轨迹推断任务，并在实验中表现出显著的性能优势。同时，该框架还为解决具有各种类型的边际约束的随机轨迹重建任务提供了一个通用框架。

    

    在粗略时间间隔下，使用未标记样本从分布中重建人口动态是一个关键的挑战。最近的方法如流模型或Schr\"odinger桥模型表现出诱人的性能，但是推断出的样本轨迹未能解释潜在的随机性，或者是DMSB，一种新颖的计算框架，它能够学习满足时间上位置边际约束的随机系统的平滑度量值样条。通过调整著名的Bregman迭代和将比例拟合迭代扩展到相空间，我们成功地高效处理了高维多边际轨迹推断任务。我们的算法在合成数据集和真实的单细胞RNA序列数据集实验中显著优于基线。此外，所提出的DMSB框架为解决具有各种类型的边际约束的随机轨迹重建任务提供了一个通用框架。

    It is a crucial challenge to reconstruct population dynamics using unlabeled samples from distributions at coarse time intervals. Recent approaches such as flow-based models or Schr\"odinger Bridge (SB) models have demonstrated appealing performance, yet the inferred sample trajectories either fail to account for the underlying stochasticity or are $\underline{D}$eep $\underline{M}$omentum Multi-Marginal $\underline{S}$chr\"odinger $\underline{B}$ridge(DMSB), a novel computational framework that learns the smooth measure-valued spline for stochastic systems that satisfy position marginal constraints across time. By tailoring the celebrated Bregman Iteration and extending the Iteration Proportional Fitting to phase space, we manage to handle high-dimensional multi-marginal trajectory inference tasks efficiently. Our algorithm outperforms baselines significantly, as evidenced by experiments for synthetic datasets and a real-world single-cell RNA sequence dataset. Additionally, the propos
    
[^221]: 追求机器学习研究的推理复现性

    Towards Inferential Reproducibility of Machine Learning Research. (arXiv:2302.04054v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04054](http://arxiv.org/abs/2302.04054)

    本研究提出利用线性混合效应模型（LMEM）来分析机器学习性能评估分数，并考虑多个方差来源及其与数据特性相互作用，从而评估可靠性和可复制性，促进对机器学习算法行为的更全面理解。

    

    机器学习评估的可靠性——即在复制的模型训练运行中观察到的评估分数的一致性——受到几种非确定性来源的影响，可以被视为测量噪声。目前的趋势是去除噪声，以强制研究结果的可复制性，忽略了实现层面固有的非确定性以及算法噪声因素和数据特性之间的关键相互作用效应。这限制了从这些实验中可以得出的结论范围。我们提出的方法是将几个方差来源，包括它们与数据特性的相互作用，纳入机器学习评估的显著性和可靠性分析中，以期从训练模型的特定实例得出推理结论, 而非去除噪声。我们展示如何使用线性混合效应模型（LMEM）来分析性能评估分数，并用广义似然比检验进行统计推断。我们的方法提供了一种系统的方式来考虑算法和数据相关的噪声来源，并使我们能够量化各个方差来源对机器学习实验的可靠性和可复制性的影响。我们在一系列合成和真实数据集上演示了我们方法的实用性，并说明了我们的方法如何促进对机器学习算法行为的更全面理解。

    Reliability of machine learning evaluation -- the consistency of observed evaluation scores across replicated model training runs -- is affected by several sources of nondeterminism which can be regarded as measurement noise. Current tendencies to remove noise in order to enforce reproducibility of research results neglect inherent nondeterminism at the implementation level and disregard crucial interaction effects between algorithmic noise factors and data properties. This limits the scope of conclusions that can be drawn from such experiments. Instead of removing noise, we propose to incorporate several sources of variance, including their interaction with data properties, into an analysis of significance and reliability of machine learning evaluation, with the aim to draw inferences beyond particular instances of trained models. We show how to use linear mixed effects models (LMEMs) to analyze performance evaluation scores, and to conduct statistical inference with a generalized lik
    
[^222]: 重新审视私有GANs

    Private GANs, Revisited. (arXiv:2302.02936v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02936](http://arxiv.org/abs/2302.02936)

    在训练差分隐私GANs时，通过对鉴别器进行一些修改和优化，如增加鉴别器的训练步骤和使用较大的批处理大小等，可以显著提高GAN的训练结果和生成质量。

    

    我们表明，在训练差分隐私GANs的经典方法中，通过对鉴别器使用差分隐私随机梯度下降（DPSGD）进行更新，在改进训练后可以获得显著改善的结果。具体地，我们提出现有实施该方法的论述忽视了对鉴别器更新添加噪声如何阻碍鉴别器训练，破坏了生成器和鉴别器之间成功训练GAN所必需的平衡。我们展示了一个简单的修复方法——在生成器步骤之间进行更多的鉴别器步骤——可以恢复生成器和鉴别器之间的平等，并改善结果。此外，为了恢复平等，我们尝试了其他修改——即较大的批处理大小和自适应的鉴别器更新频率——以改善鉴别器训练，并在生成质量上进一步改善。我们的结果表明，在标准图像合成基准测试上，我们的方法在生成图像的质量和多样性方面优于现有方法。

    We show that the canonical approach for training differentially private GANs -- updating the discriminator with differentially private stochastic gradient descent (DPSGD) -- can yield significantly improved results after modifications to training. Specifically, we propose that existing instantiations of this approach neglect to consider how adding noise only to discriminator updates inhibits discriminator training, disrupting the balance between the generator and discriminator necessary for successful GAN training. We show that a simple fix -- taking more discriminator steps between generator steps -- restores parity between the generator and discriminator and improves results.  Additionally, with the goal of restoring parity, we experiment with other modifications -- namely, large batch sizes and adaptive discriminator update frequency -- to improve discriminator training and see further improvements in generation quality. Our results demonstrate that on standard image synthesis bench
    
[^223]: 网络中的两种真实划分的生成模型

    Generative models for two-ground-truth partitions in networks. (arXiv:2302.02787v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2302.02787](http://arxiv.org/abs/2302.02787)

    本研究提出了一种生成模型，称为随机交叉块模型（SCBM），可以在网络的中尺度结构中构建两个不同的划分。通过评估随机块模型的性能，展示了该模型的使用案例。

    

    提出了一种称为随机交叉块模型（SCBM）的生成模型，可以在单个基准网络的中尺度结构中同时构建两个不同的划分。通过评估随机块模型的性能，展示了基准模型的使用案例。

    A myriad of approaches have been proposed to characterise the mesoscale structure of networks - most often as a partition based on patterns variously called communities, blocks, or clusters. Clearly, distinct methods designed to detect different types of patterns may provide a variety of answers to the network's mesoscale structure. Yet, even multiple runs of a given method can sometimes yield diverse and conflicting results, producing entire landscapes of partitions which potentially include multiple (locally optimal) mesoscale explanations of the network. Such ambiguity motivates a closer look at the ability of these methods to find multiple qualitatively different 'ground truth' partitions in a network. Here, we propose the stochastic cross-block model (SCBM), a generative model which allows for two distinct partitions to be built into the mesoscale structure of a single benchmark network. We demonstrate a use case of the benchmark model by appraising the power of stochastic block m
    
[^224]: 高效图场积分器在点云中的应用

    Efficient Graph Field Integrators Meet Point Clouds. (arXiv:2302.00942v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00942](http://arxiv.org/abs/2302.00942)

    本文提出了两种算法用于非欧几里得空间中的高效图场积分，适用于点云网格图或ε-最近邻图的表征方法，具有很强的实用性。

    

    文章提出了两种新的算法类别，用于对编码点云的图形进行高效场积分。第一类算法使用点云网格图的有界亏格，第二类算法则使用点云的流行的ε-最近邻图表示方法。两种算法都可以被看作 Fast Multipole Methods(FMMs) 的可行替代，但适用于非欧几里得空间。文章重点研究基于点之间步长分布（如最短路径距离）所引发的几何学。通过提供详细的理论分析，文章获得了结构图论的新结果。文章还进行了全面的实证评估，包括对刚性和可变形物体的表面插值（特别是用于网格动态建模），点云的Wasserstein距离计算以及Gromov-Wasserstein变体等。

    We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, SeparatorFactorization(SF), leverages the bounded genus of point cloud mesh graphs, while the second class, RFDiffusion(RFD), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods (FMMs), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.
    
[^225]: 大规模调查弱监督深度学习在生物医学文献的细粒度语义索引中的应用

    Large-scale investigation of weakly-supervised deep learning for the fine-grained semantic indexing of biomedical literature. (arXiv:2301.09350v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.09350](http://arxiv.org/abs/2301.09350)

    本研究提出了一种新方法，通过弱监督和基于字典的启发式方法，在生物医学文献中对MeSH概念进行自动细化的主题注释。实验结果显示，该方法在大规模场景下取得了较高的性能。

    

    目标：生物医学文献的语义索引通常在MeSH描述符的级别上进行，将几个相关但不同的生物医学概念组合在一起并视为单个主题。本研究提出了一种新方法，在MeSH概念的级别上自动细化主题注释。方法：由于缺少标记数据，我们依赖于基于概念出现在文章摘要中的弱监督，这也通过基于字典的启发式方法进行了改进。此外，我们还研究了深度学习方法，选择设计策略以应对这一任务的特殊挑战。新方法在大规模的回顾性场景下进行评估，基于已经提升为描述符的概念。结果：在我们的实验中，概念出现是最强的启发式方法，宏F1分数约为0.63，跨多个标签。所提出的方法进一步提高了超过4个百分点。结论：结果表明，概念出现是一个可行的弱监督方法，在生物医学文献的细粒度语义索引中具有较高的性能。

    Objective: Semantic indexing of biomedical literature is usually done at the level of MeSH descriptors with several related but distinct biomedical concepts often grouped together and treated as a single topic. This study proposes a new method for the automated refinement of subject annotations at the level of MeSH concepts. Methods: Lacking labelled data, we rely on weak supervision based on concept occurrence in the abstract of an article, which is also enhanced by dictionary-based heuristics. In addition, we investigate deep learning approaches, making design choices to tackle the particular challenges of this task. The new method is evaluated on a large-scale retrospective scenario, based on concepts that have been promoted to descriptors. Results: In our experiments concept occurrence was the strongest heuristic achieving a macro-F1 score of about 0.63 across several labels. The proposed method improved it further by more than 4pp. Conclusion: The results suggest that concept occu
    
[^226]: 数据集压缩的综合调查

    A Comprehensive Survey of Dataset Distillation. (arXiv:2301.05603v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05603](http://arxiv.org/abs/2301.05603)

    数据集压缩是处理海量数据的一种方法，通过综合一个小型典型数据集来提高数据处理效率。这一方法在压缩数据集方面取得了惊人的性能，但仍存在一些限制。

    

    在过去的十年中，深度学习技术得到了前所未有的发展，并在许多应用领域成为首选。这一进展主要归功于快速发展的计算资源与先进算法的系统协同，用以处理海量数据。然而，处理有限计算能力下无限增长的数据逐渐变得具有挑战性。为此，提出了各种方法来提高数据处理效率。数据集压缩方法是一种数据集缩减方法，通过从大规模数据中综合出一个小型典型数据集来解决这个问题，并引起了深度学习界的广泛关注。现有的数据集压缩方法可以根据是否明确模仿目标数据的性能将其分类为元学习和数据匹配框架。尽管数据集压缩在压缩数据集方面表现出惊人的性能，但仍存在一些限制。

    Deep learning technology has developed unprecedentedly in the last decade and has become the primary choice in many application domains. This progress is mainly attributed to a systematic collaboration in which rapidly growing computing resources encourage advanced algorithms to deal with massive data. However, it has gradually become challenging to handle the unlimited growth of data with limited computing power. To this end, diverse approaches are proposed to improve data processing efficiency. Dataset distillation, a dataset reduction method, addresses this problem by synthesizing a small typical dataset from substantial data and has attracted much attention from the deep learning community. Existing dataset distillation methods can be taxonomized into meta-learning and data matching frameworks according to whether they explicitly mimic the performance of target data. Although dataset distillation has shown surprising performance in compressing datasets, there are still several limi
    
[^227]: 标签损失：通过直接损失构建进行弱监督学习

    Losses over Labels: Weakly Supervised Learning via Direct Loss Construction. (arXiv:2212.06921v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06921](http://arxiv.org/abs/2212.06921)

    本文提出了一种弱监督学习的新方法，通过直接将启发式方法转化为损失函数，来训练模型，并利用启发式方法的决策信息进行特征选择。

    

    由于生成大量标记数据的成本过高，编程弱监督成为机器学习中的一种新兴范 Paradigm。在这种情况下，用户设计启发式方法为数据子集提供噪声标签。这些弱标签被组合（通常通过图模型）形成伪标签，然后用于训练下游模型。在本文中，我们质疑传统弱监督学习流程的基本前提：既然启发式方法提供了所有的“标签”信息，为什么还需要生成伪标签呢？相反，我们提议直接将启发式方法转化为相应的损失函数，惩罚模型与启发式方法之间的差异。通过直接从启发式方法构建损失函数，我们可以融入比标准弱监督学习流程中使用的更多信息，例如启发式方法如何做出决策，这明确地指导了特征选择过程。

    Owing to the prohibitive costs of generating large amounts of labeled data, programmatic weak supervision is a growing paradigm within machine learning. In this setting, users design heuristics that provide noisy labels for subsets of the data. These weak labels are combined (typically via a graphical model) to form pseudolabels, which are then used to train a downstream model. In this work, we question a foundational premise of the typical weakly supervised learning pipeline: given that the heuristic provides all ``label" information, why do we need to generate pseudolabels at all? Instead, we propose to directly transform the heuristics themselves into corresponding loss functions that penalize differences between our model and the heuristic. By constructing losses directly from the heuristics, we can incorporate more information than is used in the standard weakly supervised pipeline, such as how the heuristics make their decisions, which explicitly informs feature selection during 
    
[^228]: 带有标签差分隐私的回归

    Regression with Label Differential Privacy. (arXiv:2212.06074v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06074](http://arxiv.org/abs/2212.06074)

    本论文研究了在保证标签差分隐私的情况下训练回归模型的任务，并提出了一种最优的标签差分隐私随机化机制，该机制采用了“对箱进行随机响应”的形式，并提供了一种高效算法来找到最优的箱值。

    

    我们研究了在保证标签差分隐私（DP）的情况下训练回归模型的任务。基于标签值的全局先验分布，该分布可以私密地获取，我们得到了一个在给定回归损失函数下最优的标签DP随机化机制。我们证明了最优机制采用“对箱进行随机响应”的形式，并提出了一种寻找最优箱值的高效算法。我们在几个数据集上进行了全面的实验评估，证明了我们算法的有效性。

    We study the task of training regression models with the guarantee of label differential privacy (DP). Based on a global prior distribution on label values, which could be obtained privately, we derive a label DP randomization mechanism that is optimal under a given regression loss function. We prove that the optimal mechanism takes the form of a "randomized response on bins", and propose an efficient algorithm for finding the optimal bin values. We carry out a thorough experimental evaluation on several datasets demonstrating the efficacy of our algorithm.
    
[^229]: Spuriosity Rankings: 使用排序数据来测量和减少偏见的方法

    Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases. (arXiv:2212.02648v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02648](http://arxiv.org/abs/2212.02648)

    这个论文提出了一种使用排序数据来测量和减少模型对虚假线索的偏见的简单有效方法。通过排名图像的虚假性，可以识别出少数子群体，并通过准确率差来评估模型的偏见。此外，通过在虚假性较低的图像上微调模型，可以在几乎不损失准确率的情况下消除模型的偏见，实现对样本的公正处理。

    

    我们提出了一种简单但有效的方法，通过排序数据来测量和减少模型对虚假线索的依赖所引起的偏见。我们的方法不需要对数据或模型训练进行昂贵的改变，而是更好地利用已有的数据。具体而言，我们基于通过可解释网络的深度神经特征来对图像进行类内排序，以衡量其虚假性（即常见虚假线索的存在程度）。通过虚假性排名，可以很容易地识别出少数子群体（即虚假性较低的图像），并通过准确率差来评估模型的偏见。甚至可以通过在虚假性较低的图像上微调分类头部，以极少的准确率损失来有效消除模型的偏见，从而实现对样本的更公正处理，无论虚假性如何。我们在ImageNet上展示了我们的方法，注释了5000个类特征依赖关系（其中630个是虚假的），并生成了一个包含325k个软分割数据的数据集。

    We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft seg
    
[^230]: 人类生物物理学作为网络权重：用于动态模拟的条件生成模型。

    Human Biophysics as Network Weights: Conditional Generative Models for Dynamic Simulation. (arXiv:2211.01856v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01856](http://arxiv.org/abs/2211.01856)

    提出了一种使用条件生成模型插值的架构，可以在动态模拟中降低建模时间并保持高的生成精度。

    

    生物物理系统的模拟对于研究生理机制和开发人机界面至关重要。虽然先进的数值方法，如有限元模型，在这个任务中表现出色，但是当生成大量模拟或模拟具有连续变化结构参数的动态事件时，它们的计算成本非常高。我们提出了一种使用条件生成模型在数值模型状态之间进行插值的架构，大大降低了建模时间，同时保持高的生成精度。作为这一概念的示范，我们提出了BioMime，一种混合结构生成模型，可以在动态变化期间对特定生物物理系统进行准确、超快速和任意高时间分辨率的模拟。这种方法在生理和临床研究以及支持信号分析的数据增强策略方面具有广泛的应用。

    Simulations of biophysical systems are fundamental for studying physiological mechanisms and developing human machine interfaces. Whilst advanced numerical methods, such as finite element models, can excel in this task, they are extremely computationally expensive to use when generating a large number of simulations or simulating dynamic events with continuously changing structural parameters. We propose an architecture that uses a conditional generative model to interpolate between the numerical model states, dramatically lowering the modeling time while maintaining a high generation accuracy. As a demonstration of this concept, we present BioMime, a hybrid-structured generative model that enables an accurate, ultra-fast, and arbitrarily high temporal-resolution simulation of a specific biophysical system during dynamic changes. This methodology has wide applications in physiological and clinical research as well as in supporting data augmentation strategies for signal analysis, repre
    
[^231]: 两阶段LLM精调方法：更少特化、更多泛化

    Two-stage LLM Fine-tuning with Less Specialization and More Generalization. (arXiv:2211.00635v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.00635](http://arxiv.org/abs/2211.00635)

    预训练的大型语言模型（LLMs）通过精调可以提高特定任务的性能，但精调通常会使模型过度专门化，降低了其在上下文中的泛化学习性能。通过两阶段精调框架ProMoT可以减少这种格式特化。

    

    预训练的大型语言模型（LLM）是适用于各种任务和提示的通用问题解决方案。通过在专门的数据集上进行精调，可以进一步改进其在特定任务上的性能。然而，精调通常使模型在特定数据集上过于专门化，并降低了其在上下文中的泛化学习性能，这在需要处理没有精调数据的其他任务时是不可取的。在这项工作中，我们首先证明了单任务精调确实会降低LLM的泛化学习性能。我们发现这种遗忘的一个重要原因是格式特化，即模型过度拟合于精调任务的格式。我们进一步表明格式特化发生在精调的早期阶段。为了解决这个问题，我们提出了Prompt Tuning with MOdel Tuning (ProMoT)这一简单而有效的两阶段精调框架，可以减少格式特化。

    Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format special
    
[^232]: 大规模合成图数据集生成框架

    A Framework for Large Scale Synthetic Graph Dataset Generation. (arXiv:2210.01944v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01944](http://arxiv.org/abs/2210.01944)

    本论文提出了一个可扩展的合成图生成工具，将图数据集扩展到规模为万亿条边和数十亿个节点的级别。通过从专有数据集学习参数模型，可以为研究人员提供用于研究各种图方法的合成数据，加快原型开发和新应用的进展。该框架在模拟结构和特征分布，并能够在不同规模上扩展表现出了良好的泛化能力。

    

    最近，人们对于开发和应用深度图学习算法在欺诈检测和推荐系统等任务中表现出了越来越高的兴趣。然而，公开可用的图结构数据集数量有限，大多数相对于实际应用来说都很小，或者在应用领域上有限。本研究通过提出一个可扩展的合成图生成工具来解决这个问题，将数据集扩展到具有万亿条边和数十亿个节点的规模。该工具从专有数据集中学习一系列参数模型，可以释放给研究人员使用，在合成数据上研究各种图方法，推动原型开发和新应用的发展。我们通过一系列数据集的示例展示了该框架的泛化能力，可以模拟结构和特征分布，并且能够扩展到不同规模，从而证明了其在基准测试中的实用性。

    Recently there has been increasing interest in developing and deploying deep graph learning algorithms for many tasks, such as fraud detection and recommender systems. Albeit, there is a limited number of publicly available graph-structured datasets, most of which are tiny compared to production-sized applications or are limited in their application domain. This work tackles this shortcoming by proposing a scalable synthetic graph generation tool to scale the datasets to production-size graphs with trillions of edges and billions of nodes. The tool learns a series of parametric models from proprietary datasets that can be released to researchers to study various graph methods on the synthetic data increasing prototype development and novel applications. We demonstrate the generalizability of the framework across a series of datasets, mimicking structural and feature distributions as well as the ability to scale them across varying sizes demonstrating their usefulness for benchmarking a
    
[^233]: 自监督遮蔽卷积变压器块用于异常检测

    Self-Supervised Masked Convolutional Transformer Block for Anomaly Detection. (arXiv:2209.12148v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.12148](http://arxiv.org/abs/2209.12148)

    本论文提出了一种自监督遮蔽卷积变压器块（SSMCTB）用于异常检测。该方法在核心架构层面上集成了重构功能，并且具有灵活的信息遮蔽能力。

    

    异常检测在计算机视觉领域最近引起了越来越多的关注，可能是因为它具有广泛的应用范围，包括工业生产线上的产品故障检测、视频监控中即将发生的事件检测以及医学扫描中的病变检测等。无论是哪个领域，异常检测通常被看作是一个单类别分类任务，其中学习仅在正常示例上进行。成功的异常检测方法家族基于学习重构遮蔽的正常输入（例如补丁、未来帧等），并将重构误差的大小作为异常程度的指示器。与其他基于重构的方法不同，我们提出了一种新颖的自监督遮蔽卷积变压器块 (SSMCTB)，该块在核心架构层面上包含了基于重构的功能。所提出的自监督块非常灵活，可以在不同层面进行信息遮蔽。

    Anomaly detection has recently gained increasing attention in the field of computer vision, likely due to its broad set of applications ranging from product fault detection on industrial production lines and impending event detection in video surveillance to finding lesions in medical scans. Regardless of the domain, anomaly detection is typically framed as a one-class classification task, where the learning is conducted on normal examples only. An entire family of successful anomaly detection methods is based on learning to reconstruct masked normal inputs (e.g. patches, future frames, etc.) and exerting the magnitude of the reconstruction error as an indicator for the abnormality level. Unlike other reconstruction-based methods, we present a novel self-supervised masked convolutional transformer block (SSMCTB) that comprises the reconstruction-based functionality at a core architectural level. The proposed self-supervised block is extremely flexible, enabling information masking at a
    
[^234]: 从非侵入性脑电记录中解码语音知觉

    Decoding speech perception from non-invasive brain recordings. (arXiv:2208.12266v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2208.12266](http://arxiv.org/abs/2208.12266)

    本研究通过使用对比学习训练的模型，成功从非侵入性脑电记录中解码感知语音。结果显示，该模型能够以高达41%的准确率识别出与脑电信号相对应的语音片段。

    

    从脑电活动中解码语音一直是医疗保健和神经科学中期待已久的目标。最近的研究中，侵入性设备已经取得了重大突破：基于颅内记录的深度学习算法现在可以解码基本的语言特征（例如字母、单词、频谱图）。然而，将这种方法推广到自然语音和非侵入性脑电记录仍然是一个重大挑战。本文介绍了一个使用对比学习训练的模型，可以从大量健康个体的非侵入性记录中解码自我监督表示的感知语音。为了评估这种方法，我们整合了四个公共数据集，包括175名志愿者的脑磁图或脑电图记录，他们在听短篇故事和孤立的句子时记录。结果显示，我们的模型可以从3秒的脑磁图信号中以高达41%的准确率识别相应的语音片段，其中包含了1000个以上的候选项。

    Decoding speech from brain activity is a long-awaited goal in both healthcare and neuroscience. Invasive devices have recently led to major milestones in that regard: deep learning algorithms trained on intracranial recordings now start to decode elementary linguistic features (e.g. letters, words, spectrograms). However, extending this approach to natural speech and non-invasive brain recordings remains a major challenge. Here, we introduce a model trained with contrastive-learning to decode self-supervised representations of perceived speech from the non-invasive recordings of a large cohort of healthy individuals. To evaluate this approach, we curate and integrate four public datasets, encompassing 175 volunteers recorded with magneto- or electro-encephalography (M/EEG), while they listened to short stories and isolated sentences. The results show that our model can identify, from 3 seconds of MEG signals, the corresponding speech segment with up to 41% accuracy out of more than 1,0
    
[^235]: 通过知识蒸馏和融合处理联邦学习中的数据异质性

    Handling Data Heterogeneity in Federated Learning via Knowledge Distillation and Fusion. (arXiv:2207.11447v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.11447](http://arxiv.org/abs/2207.11447)

    本文提出了一种名为FedKF的方案，通过知识融合和蒸馏解决了联邦学习中数据异质性导致的客户端模型漂移问题，提升了模型性能和公平性。

    

    联邦学习支持在多个设备上通过中央服务器进行全局机器学习模型的分布式训练。然而，不同设备间的数据异质性导致了客户端模型漂移问题，导致模型性能下降和模型公平性差。为了解决这个问题，我们在本文中设计了一种名为FedKF的联邦学习全局-局部知识融合方案。FedKF的核心思想是在每轮训练中让服务器返回全局知识，以与每个设备本地知识融合，从而使本地模型能够向全局最优进行正则化，从而缓解客户端模型漂移问题。在FedKF中，我们首先提出了支持精确全局知识表示的主-辅模型聚合技术。然后，我们提出了一种无数据知识蒸馏方法，使得每个客户端模型可以学习全局知识（嵌入在全局模型中）而不需要数据。

    Federated learning (FL) supports distributed training of a global machine learning model across multiple devices with the help of a central server. However, data heterogeneity across different devices leads to the client model drift issue and results in model performance degradation and poor model fairness. To address the issue, we design Federated learning with global-local Knowledge Fusion (FedKF) scheme in this paper. The key idea in FedKF is to let the server return the global knowledge to be fused with the local knowledge in each training round so that the local model can be regularized towards the global optima. Therefore, the client model drift issue can be mitigated. In FedKF, we first propose the active-inactive model aggregation technique that supports a precise global knowledge representation. Then, we propose a data-free knowledge distillation (KD) approach to enable each client model to learn the global knowledge (embedded in the global model) while each client model can s
    
[^236]: 深度扩散能量模型用于可解释文本建模

    Latent Diffusion Energy-Based Model for Interpretable Text Modeling. (arXiv:2206.05895v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05895](http://arxiv.org/abs/2206.05895)

    该论文介绍了一种新颖的深度扩散能量模型，通过在变分学习框架中引入扩散模型和潜在空间EBMs之间的共生关系，解决了潜在空间EBMs在采样质量和训练稳定性方面的问题。

    

    潜在空间能量模型（EBMs）在生成建模中引起了越来越多的关注。最近的研究工作在其基础上进行了有趣的尝试，旨在实现文本建模的可解释性。然而，潜在空间EBMs也继承了数据空间EBMs的一些缺陷；实践中退化的MCMC采样质量可能导致生成质量差和训练不稳定，尤其是在具有复杂潜在结构的数据上。受到最近利用扩散恢复似然学习作为解决采样问题的努力的启发，我们在变分学习框架中引入了扩散模型和潜在空间EBMs之间的新型共生关系，称为潜在扩散能量模型。我们开发了基于几何聚类的正则化方法，以及信息瓶颈来进一步提高学习潜空间质量。

    Latent space Energy-Based Models (EBMs), also known as energy-based priors, have drawn growing interests in generative modeling. Fueled by its flexibility in the formulation and strong modeling power of the latent space, recent works built upon it have made interesting attempts aiming at the interpretability of text modeling. However, latent space EBMs also inherit some flaws from EBMs in data space; the degenerate MCMC sampling quality in practice can lead to poor generation quality and instability in training, especially on data with complex latent structures. Inspired by the recent efforts that leverage diffusion recovery likelihood learning as a cure for the sampling issue, we introduce a novel symbiosis between the diffusion models and latent space EBMs in a variational learning framework, coined as the latent diffusion energy-based model. We develop a geometric clustering-based regularization jointly with the information bottleneck to further improve the quality of the learned la
    
[^237]: 空间-时间关联表示法及其在过程监测中的应用: 使用图卷积神经网络

    Spatial-temporal associations representation and application for process monitoring using graph convolution neural network. (arXiv:2205.05250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.05250](http://arxiv.org/abs/2205.05250)

    该论文利用图卷积神经网络，基于同一工业过程中众多具有空间-时间相关特征的变量，实现了变量特征建模和表示、图网络构建及图特征感知，并应用于过程监测中。

    

    感谢同行和学者们对这项工作的关注和关心。在专家、编辑和审稿人的评论和指导下，这项工作已被接受发表在《过程安全与环境保护》期刊上。本文的主题是基于同一工业过程中众多变量的空间-时间关联，这是指在动态工业过程中获得的众多具有空间-时间相关特征的变量，即这些变量不仅在时间上高度相关，也在空间上相互关联。为了解决这个问题，需要解决三个关键问题：变量特征建模和表示、图网络构建（时间信息）和图特征感知。第一个问题通过假设数据服从改进的高斯分布来实现，而图网络可以由监测变量及其边定义。

    Thank you very much for the attention and concern of colleagues and scholars in this work. With the comments and guidance of experts, editors, and reviewers, this work has been accepted for publishing in the journal "Process Safety and Environmental Protection". The theme of this paper relies on the Spatial-temporal associations of numerous variables in the same industrial processes, which refers to numerous variables obtained in dynamic industrial processes with Spatial-temporal correlation characteristics, i.e., these variables are not only highly correlated in time but also interrelated in space. To handle this problem, three key issues need to be well addressed: variable characteristics modeling and representation, graph network construction (temporal information), and graph characteristics perception. The first issue is implemented by assuming the data follows one improved Gaussian distribution, while the graph network can be defined by the monitoring variables and their edges whi
    
[^238]: 自监督深度展开重建方法的正则化去噪

    Self-supervised Deep Unrolled Reconstruction Using Regularization by Denoising. (arXiv:2205.03519v3 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2205.03519](http://arxiv.org/abs/2205.03519)

    本文提出了一种新颖的MRI图像重建方法，利用了自监督去噪网络和插入式方法，以提高重建性能，并利用正则化去噪来优化MRI重建。

    

    深度学习方法在各种计算机视觉任务中取得了成功。受到这一成功的启发，深度学习已经在磁共振成像（MRI）重建中进行了探索。特别是，将深度学习与基于模型的优化方法相结合已经显示出了相当大的优势。然而，对于一些MRI应用来说，通常需要大量的标记训练数据才能获得高重建质量，这是具有挑战性的。在本文中，我们提出了一种名为DURED-Net的新型重建方法，通过结合自监督去噪网络和插入式方法，实现了可解释的自监督学习用于MR图像重建。我们的目标是通过添加利用图像物理学的显式先验来提高MR重建中Noise2Noise的重建性能。具体地，利用正则化去噪（RED）实现了去噪网络在MRI重建中的发挥。实验结果表明，该方法在MRI重建中取得了良好的重建效果。

    Deep learning methods have been successfully used in various computer vision tasks. Inspired by that success, deep learning has been explored in magnetic resonance imaging (MRI) reconstruction. In particular, integrating deep learning and model-based optimization methods has shown considerable advantages. However, a large amount of labeled training data is typically needed for high reconstruction quality, which is challenging for some MRI applications. In this paper, we propose a novel reconstruction method, named DURED-Net, that enables interpretable self-supervised learning for MR image reconstruction by combining a self-supervised denoising network and a plug-and-play method. We aim to boost the reconstruction performance of Noise2Noise in MR reconstruction by adding an explicit prior that utilizes imaging physics. Specifically, the leverage of a denoising network for MRI reconstruction is achieved using Regularization by Denoising (RED). Experiment results demonstrate that the prop
    
[^239]: 一种代数收敛的随机梯度下降算法用于全局优化

    An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization. (arXiv:2204.05923v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2204.05923](http://arxiv.org/abs/2204.05923)

    本文提出了一种改进的随机梯度下降算法，通过自适应调整随机性来找到非凸优化问题的全局最优解，并通过代数收敛性证明了算法的优越性能。

    

    我们提出了一种新的梯度下降算法，通过添加随机项来找到非凸优化问题的全局最优解。算法的一个关键组成部分是根据目标函数的值自适应调整随机性。在模拟退火的术语中，温度是与状态相关的。借此，我们证明了算法在概率和参数空间中具有代数收敛性，这比仅使用更简单的噪声项控制的经典收敛速率有了显著改进。收敛证明基于算法的实际离散设置，而不仅仅是如文献中通常做的连续极限。我们还提供了几个数值示例，以展示算法对于相对复杂的目标函数的效率和鲁棒性。

    We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is a significant improvement over the classical rate from using a more straightforward control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm, not just its continuous limit as often done in the literature. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions.
    
[^240]: WGANs的最优1-Wasserstein距离

    Optimal 1-Wasserstein Distance for WGANs. (arXiv:2201.02824v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2201.02824](http://arxiv.org/abs/2201.02824)

    本文对WGANs的1-Wasserstein距离进行了最优化研究，揭示了样本大小固定时的最优方案与最小化样本点之间平方欧氏距离的和有密切关联，同时发现在样本大小趋向无穷大的情况下，WGANs能够以给定的收敛率无限接近目标分布，前提是生成Lipschitz函数族增长合适。

    

    生成对抗网络背后的数学力量引发了具有挑战性的理论问题。为了表征生成分布的几何特性，我们对有限样本和渐近领域中的Wasserstein GANs（WGANs）进行了彻底的分析。我们研究了潜空间为单变量的特殊情况，并得出了在输出空间维度无关的结果。我们特别表明，对于固定的样本大小，最优WGANs与连接路径最小化样本点之间的平方欧氏距离的和密切相关。我们还强调了WGANs能够以给定的收敛速率，并在生成的Lipschitz函数族适当增长的条件下，无限接近（对于1-Wasserstein距离）目标分布。我们顺便推导了半离散最优输运理论的新结果。

    The mathematical forces at work behind Generative Adversarial Networks raise challenging theoretical issues. Motivated by the important question of characterizing the geometrical properties of the generated distributions, we provide a thorough analysis of Wasserstein GANs (WGANs) in both the finite sample and asymptotic regimes. We study the specific case where the latent space is univariate and derive results valid regardless of the dimension of the output space. We show in particular that for a fixed sample size, the optimal WGANs are closely linked with connected paths minimizing the sum of the squared Euclidean distances between the sample points. We also highlight the fact that WGANs are able to approach (for the 1-Wasserstein distance) the target distribution as the sample size tends to infinity, at a given convergence rate and provided the family of generative Lipschitz functions grows appropriately. We derive in passing new results on optimal transport theory in the semi-discre
    
[^241]: 对具有潜在混淆因素的时间序列进行因果祖先图特征化

    Characterization of causal ancestral graphs for time series with latent confounders. (arXiv:2112.08417v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2112.08417](http://arxiv.org/abs/2112.08417)

    本文介绍了一种新的图模型类别，用于表示具有未观测到的混淆因素的时间序列的因果关系和独立性。通过使用这些新的图模型，可以得出更强的因果推断，而无需额外的假设。

    

    本文介绍了一种新的图模型类别，用于表示具有未观测到的混淆因素的多变量时间序列的时间滞后特定的因果关系和独立性。我们完全特征化了这些图，并证明它们是当前使用的模型类别的适当子集。正如我们所展示的，通过使用这些新的图可以得出更强的因果推断，而无需额外的假设。此外，我们还介绍了一种用于表示这些新图的马尔可夫等价类的图形表示。与当前最先进的因果发现算法所学到的内容相比，这种图形表示包含更多的因果知识。

    In this paper, we introduce a novel class of graphical models for representing time lag specific causal relationships and independencies of multivariate time series with unobserved confounders. We completely characterize these graphs and show that they constitute proper subsets of the currently employed model classes. As we show, from the novel graphs one can thus draw stronger causal inferences -- without additional assumptions. We further introduce a graphical representation of Markov equivalence classes of the novel graphs. This graphical representation contains more causal knowledge than what current state-of-the-art causal discovery algorithms learn.
    
[^242]: 关于联邦平均 Langevin 动力学的收敛性研究

    On Convergence of Federated Averaging Langevin Dynamics. (arXiv:2112.05120v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.05120](http://arxiv.org/abs/2112.05120)

    我们提出了一种称为FA-LD的联邦平均Langevin算法，可以用于分布式客户端的不确定性量化和均值预测。算法考虑了非独立同分布数据的强对数凹分布，并研究了注入噪声、随机梯度噪声、数据异质性和变化的学习率等因素对收敛性的影响，为最小化通信开销提供了理论保证。

    

    我们提出了一种用于分布式客户端的不确定性量化和均值预测的联邦平均 Langevin 算法（FA-LD）。我们特别考虑了一般模型的正常后验分布的推广。我们为 FA-LD 开发了理论保证，针对具有非独立同分布数据的强对数凹分布，研究了注入噪声、随机梯度噪声、数据异质性和变化的学习率对收敛性的影响。这样的分析揭示了在最小化通信开销方面选择本地更新的最佳方法。我们的方法的重要之处在于，Langevin 算法中注入噪声不会损害通信效率。此外，我们在 FA-LD 算法中研究了在不同客户端上使用独立和相关噪声的情况。我们观察到在通信、准确性和数据隐私之间存在着权衡。由于本地设备可能在联邦网络中变得不活跃，

    We propose a federated averaging Langevin algorithm (FA-LD) for uncertainty quantification and mean predictions with distributed clients. In particular, we generalize beyond normal posterior distributions and consider a general class of models. We develop theoretical guarantees for FA-LD for strongly log-concave distributions with non-i.i.d data and study how the injected noise and the stochastic-gradient noise, the heterogeneity of data, and the varying learning rates affect the convergence. Such an analysis sheds light on the optimal choice of local updates to minimize communication costs. Important to our approach is that the communication efficiency does not deteriorate with the injected noise in the Langevin algorithms. In addition, we examine in our FA-LD algorithm both independent and correlated noise used over different clients. We observe there is a trade-off between the pairs among communication, accuracy, and data privacy. As local devices may become inactive in federated ne
    
[^243]: 跨数据粒度链接：拟合多变量Hawkes过程到部分区间屏蔽数据

    Linking Across Data Granularity: Fitting Multivariate Hawkes Processes to Partially Interval-Censored Data. (arXiv:2111.02062v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.02062](http://arxiv.org/abs/2111.02062)

    这项研究介绍了Partial Mean Behavior Poisson (PMBP)过程，它是一种新的点过程，可以有效地建模时间戳和区间屏蔽数据，并成功恢复了MHP参数和谱半径。

    

    多变量Hawkes过程(MHP)被广泛用于分析相互作用的数据流，其中事件在自身维度内（通过自激）或不同维度之间（通过交叉激发）生成新事件。然而，在某些应用中，某些维度中的个别事件时间戳是不可观测的，只有在时间间隔内的事件计数是已知的，被称为部分区间屏蔽数据。MHP不适用于处理这种数据，因为其估计需要事件时间戳。在本研究中，我们引入了Partial Mean Behavior Poisson (PMBP)过程，这是一种新颖的点过程，与MHP共享参数等效性，可以有效地建模时间戳和区间屏蔽数据。我们使用合成和真实世界数据集展示了PMBP过程的能力。首先，我们证明了PMBP过程可以近似MHP参数并恢复谱半径，使用合成事件历史数据进行验证。

    The multivariate Hawkes process (MHP) is widely used for analyzing data streams that interact with each other, where events generate new events within their own dimension (via self-excitation) or across different dimensions (via cross-excitation). However, in certain applications, the timestamps of individual events in some dimensions are unobservable, and only event counts within intervals are known, referred to as partially interval-censored data. The MHP is unsuitable for handling such data since its estimation requires event timestamps. In this study, we introduce the Partial Mean Behavior Poisson (PMBP) process, a novel point process which shares parameter equivalence with the MHP and can effectively model both timestamped and interval-censored data. We demonstrate the capabilities of the PMBP process using synthetic and real-world datasets. Firstly, we illustrate that the PMBP process can approximate MHP parameters and recover the spectral radius using synthetic event histories. 
    
[^244]: 无监督前景提取方法：基于深度区域竞争的研究

    Unsupervised Foreground Extraction via Deep Region Competition. (arXiv:2110.15497v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2110.15497](http://arxiv.org/abs/2110.15497)

    本论文提出了一种名为深度区域竞争（DRC）的算法，用于完全无监督地从图像中提取前景对象。该算法通过结合能量先验和生成式图像建模，利用像素重新分配来捕捉背景区域的规律性，并通过期望最大化的方法找到前景-背景分区。

    

    本文提出了一种名为深度区域竞争（DRC）的算法，用于以完全无监督的方式从图像中提取前景对象。前景提取可以看作是通用图像分割的特殊情况，其重点是识别和解开背景中的对象。在这项工作中，我们通过将基于能量的先验与生成式图像建模（Mixture of Experts，MoE）相结合重新思考前景提取，进一步引入学习的像素重新分配作为捕捉背景区域规律性的重要归纳偏见。通过这种建模，可以通过期望最大化（Expectation-Maximization，EM）自然地找到前景-背景分区。实验证明，所提出的方法有效地利用了在分区过程中混合分量之间的相互作用，这与通用图像分割的开创性方法——区域竞争密切相关。

    We present Deep Region Competition (DRC), an algorithm designed to extract foreground objects from images in a fully unsupervised manner. Foreground extraction can be viewed as a special case of generic image segmentation that focuses on identifying and disentangling objects from the background. In this work, we rethink the foreground extraction by reconciling energy-based prior with generative image modeling in the form of Mixture of Experts (MoE), where we further introduce the learned pixel re-assignment as the essential inductive bias to capture the regularities of background regions. With this modeling, the foreground-background partition can be naturally found through Expectation-Maximization (EM). We show that the proposed method effectively exploits the interaction between the mixture components during the partitioning process, which closely connects to region competition, a seminal approach for generic image segmentation. Experiments demonstrate that DRC exhibits more competit
    
[^245]: Colossal-AI: 一种用于大规模并行训练的统一深度学习系统

    Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training. (arXiv:2110.14883v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.14883](http://arxiv.org/abs/2110.14883)

    Colossal-AI是一种用于大规模并行训练的统一深度学习系统，能够以高达2.76倍的速度加快训练过程，并支持多种并行训练方法和零冗余优化器集成的异构训练方法。

    

    Transformer模型的成功推动了深度学习模型规模达到数十亿个参数。然而，由于单个GPU的有限内存资源，选择最佳并行策略的最佳实践仍然缺乏，因为它需要深度学习和并行计算方面的领域专业知识。Colossal-AI系统通过引入一个统一的接口来解决上述挑战，将模型训练的顺序代码扩展到分布式环境中。它支持数据并行、流水线并行、张量并行和序列并行等并行训练方法，以及与零冗余优化器集成的异构训练方法。与基准系统相比，Colossal-AI在大规模模型上可以达到高达2.76倍的训练加速度。

    The success of Transformer models has pushed the deep learning model scale to billions of parameters. Due to the limited memory resource of a single GPU, However, the best practice for choosing the optimal parallel strategy is still lacking, since it requires domain expertise in both deep learning and parallel computing.  The Colossal-AI system addressed the above challenge by introducing a unified interface to scale your sequential code of model training to distributed environments. It supports parallel training methods such as data, pipeline, tensor, and sequence parallelism, as well as heterogeneous training methods integrated with zero redundancy optimizer. Compared to the baseline system, Colossal-AI can achieve up to 2.76 times training speedup on large-scale models.
    
[^246]: 在分布式SGD中结合差分隐私和拜占庭弹性

    Combining Differential Privacy and Byzantine Resilience in Distributed SGD. (arXiv:2110.03991v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.03991](http://arxiv.org/abs/2110.03991)

    本文研究了在分布式机器学习中将差分隐私和拜占庭弹性结合起来的问题，并发现现有的方法在差分隐私的约束下无效。

    

    隐私和拜占庭弹性（BR）是现代分布式机器学习的两个关键要求。这两个概念在过去已经被广泛研究，但如何有效地将它们结合起来的问题仍然没有答案。本文通过研究在标准参数服务器架构中，分布式SGD算法在存在恶意（拜占庭）工作者的情况下，以及其他诚实工作者提供噪声信息以确保差分隐私的情况下，能够学习准确模型的程度来回答这个问题。我们首先观察到标准差分隐私和拜占庭弹性实践的整合并不简单。实际上，我们发现许多现有的关于分布式SGD收敛性的结果，在诚实工作者实施差分隐私时无效，特别是那些依赖于$(\alpha,f)$-拜占庭弹性的结果。为了克服这个缺点，我们重新审视了理论.

    Privacy and Byzantine resilience (BR) are two crucial requirements of modern-day distributed machine learning. The two concepts have been extensively studied individually but the question of how to combine them effectively remains unanswered. This paper contributes to addressing this question by studying the extent to which the distributed SGD algorithm, in the standard parameter-server architecture, can learn an accurate model despite (a) a fraction of the workers being malicious (Byzantine), and (b) the other fraction, whilst being honest, providing noisy information to the server to ensure differential privacy (DP). We first observe that the integration of standard practices in DP and BR is not straightforward. In fact, we show that many existing results on the convergence of distributed SGD under Byzantine faults, especially those relying on $(\alpha,f)$-Byzantine resilience, are rendered invalid when honest workers enforce DP. To circumvent this shortcoming, we revisit the theory 
    
[^247]: 《推断解释的公理聚合》

    Axiomatic Aggregations of Abductive Explanations. (arXiv:2109.03890v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.03890](http://arxiv.org/abs/2109.03890)

    本论文提出了三种聚合方法，将各种可能的推断解释聚合成特征重要性分数，解决了推断解释中多个有效解释的问题。这些方法基于合作博弈理论的权力指数和已知的因果强度度量。

    

    对后续模型逼近解释方法（如LIME和SHAP）的鲁棒性的近期批评导致了模型精确的推断解释的兴起。对于每个数据点，推断解释提供了一个足以生成结果的最小子集特征。尽管在理论上是严格和可靠的，但推断解释存在一个主要问题：同一数据点可以有多个有效的推断解释。在这种情况下，提供一个单一的推断解释可能是不足够的；另一方面，提供所有有效的推断解释可能由于其规模而难以理解。在这项工作中，我们通过将各种可能的推断解释聚合成特征重要性分数来解决这个问题。我们提出了三种聚合方法：两种基于合作博弈理论的权力指数方法和一种基于著名的因果强度度量的方法。我们从公理上对这三种方法进行了表征，证明每个方法都是良定义的且符合公理。

    The recent criticisms of the robustness of post hoc model approximation explanation methods (like LIME and SHAP) have led to the rise of model-precise abductive explanations. For each data point, abductive explanations provide a minimal subset of features that are sufficient to generate the outcome. While theoretically sound and rigorous, abductive explanations suffer from a major issue -- there can be several valid abductive explanations for the same data point. In such cases, providing a single abductive explanation can be insufficient; on the other hand, providing all valid abductive explanations can be incomprehensible due to their size. In this work, we solve this issue by aggregating the many possible abductive explanations into feature importance scores. We propose three aggregation methods: two based on power indices from cooperative game theory and a third based on a well-known measure of causal strength. We characterize these three methods axiomatically, showing that each of 
    
[^248]: 关注天体暂变现象：介绍基于时间序列变换器的光度分类方法

    Paying Attention to Astronomical Transients: Introducing the Time-series Transformer for Photometric Classification. (arXiv:2105.06178v3 [astro-ph.IM] UPDATED)

    [http://arxiv.org/abs/2105.06178](http://arxiv.org/abs/2105.06178)

    本研究介绍了一种基于时间序列变换器的光度分类方法，用于处理未来大规模的天体暂变数据。这种变换器架构支持多变量时间序列数据，并可灵活添加附加特征，同时还提供了可解释性。在多个数据集上的实验证明了其显著性能。

    

    未来的勘测项目，如鲁宾天文台的遗产空间与时间勘测(LSST)，将观测到比以往任何一项勘测都多一个数量级的天体暂变事件。在这种大量的光度数据中，单靠人类无法对所有这些事件进行分类。近期的研究致力于利用机器学习方法解决天体暂变分类的挑战，并取得了不断提升的成功。变换器是一种最近发展起来的深度学习架构，最初用于自然语言处理，并且最近取得了很大的成功。本研究开发了一种新的变换器架构，其核心是多头自注意力机制，适用于一般的多变量时间序列数据。此外，所提出的时间序列变换器架构支持导入任意数量的附加特征，并提供可解释性。我们将时间序列变换器应用于光度分类，并展示了其在多个数据集上的显著性能。

    Future surveys such as the Legacy Survey of Space and Time (LSST) of the Vera C. Rubin Observatory will observe an order of magnitude more astrophysical transient events than any previous survey before. With this deluge of photometric data, it will be impossible for all such events to be classified by humans alone. Recent efforts have sought to leverage machine learning methods to tackle the challenge of astronomical transient classification, with ever improving success. Transformers are a recently developed deep learning architecture, first proposed for natural language processing, that have shown a great deal of recent success. In this work we develop a new transformer architecture, which uses multi-head self attention at its core, for general multi-variate time-series data. Furthermore, the proposed time-series transformer architecture supports the inclusion of an arbitrary number of additional features, while also offering interpretability. We apply the time-series transformer to t
    
[^249]: 具有矩阵表示的循环神经网络的存储容量

    Memory Capacity of Recurrent Neural Networks with Matrix Representation. (arXiv:2104.07454v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.07454](http://arxiv.org/abs/2104.07454)

    这个论文研究了具有矩阵表示的循环神经网络的存储容量，通过基于Fisher信息的概率性概念定义和研究，得出了不同假设下的存储上界，并与向量表示的网络进行了比较。

    

    众所周知，经典的循环神经网络（RNN）在学习长期依赖性方面存在限制，这一问题在长短期记忆网络（LSTM）中通过存储结构得到了解决。神经图灵机（NTM）是一种新颖的RNN，通过神经网络控制器实现了可编程计算机的概念，可以学习简单的算法任务。矩阵神经网络采用矩阵表示，与使用基于向量表示的经典神经网络相比，可以固有地保留数据的空间结构。因此，可以认为具有矩阵表示的神经网络可能具有更好的存储容量。在本文中，我们基于Fisher信息定义和研究了一种基于概率的矩阵RNN存储容量的概念。在各种假设下，我们找到了这些网络的存储容量的上界，并与它们的向量对应物进行了比较。特别地，我们证明了s

    It is well known that canonical recurrent neural networks (RNNs) face limitations in learning long-term dependencies which have been addressed by memory structures in long short-term memory (LSTM) networks. Neural Turing machines (NTMs) are novel RNNs that implement the notion of programmable computers with neural network controllers that can learn simple algorithmic tasks. Matrix neural networks feature matrix representation which inherently preserves the spatial structure of data when compared to canonical neural networks that use vector-based representation. One may then argue that neural networks with matrix representations may have the potential to provide better memory capacity. In this paper, we define and study a probabilistic notion of memory capacity based on Fisher information for matrix-based RNNs. We find bounds on memory capacity for such networks under various hypotheses and compare them with their vector counterparts. In particular, we show that the memory capacity of s
    
[^250]: 使用带有注意力和上下文匹配机制的卷积LSTM进行数值天气预报

    Numerical Weather Forecasting using Convolutional-LSTM with Attention and Context Matcher Mechanisms. (arXiv:2102.00696v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.00696](http://arxiv.org/abs/2102.00696)

    本论文提出了一种新的深度学习架构，用于预测高分辨率时空天气数据。该架构集成了卷积长短期记忆和卷积神经网络，并引入了注意力和上下文匹配机制。与传统模型相比，该架构在性能上取得了显著改进。

    

    使用高分辨率物理模型进行数值天气预报通常需要超级计算机上的大量计算资源，这减少了它们在大多数实际应用中的广泛使用。为了解决这个问题，应用深度学习方法在这个领域内揭示出创新性的解决方案。为此，我们引入了一种用于预测高分辨率时空天气数据的新型深度学习架构。我们的方法通过集成卷积长短期记忆和卷积神经网络来扩展传统的编码器-解码器结构。此外，我们还将注意力和上下文匹配机制融入到模型架构中。与基准深度学习模型，包括ConvLSTM、TrajGRU和U-Net相比，我们的天气模型在性能上取得了显著的改进。我们的实验评估涉及高规模的实际基准数值天气数据集，即ERA5小时级气压水平数据集和WeatherBench。我们的结果证明了...

    Numerical weather forecasting using high-resolution physical models often requires extensive computational resources on supercomputers, which diminishes their wide usage in most real-life applications. As a remedy, applying deep learning methods has revealed innovative solutions within this field. To this end, we introduce a novel deep learning architecture for forecasting high-resolution spatio-temporal weather data. Our approach extends the conventional encoder-decoder structure by integrating Convolutional Long-short Term Memory and Convolutional Neural Networks. In addition, we incorporate attention and context matcher mechanisms into the model architecture. Our Weather Model achieves significant performance improvements compared to baseline deep learning models, including ConvLSTM, TrajGRU, and U-Net. Our experimental evaluation involves high-scale, real-world benchmark numerical weather datasets, namely the ERA5 hourly dataset on pressure levels and WeatherBench. Our results demo
    
[^251]: 深度控制学习用于库存控制

    Deep Controlled Learning for Inventory Control. (arXiv:2011.15122v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.15122](http://arxiv.org/abs/2011.15122)

    本论文提出了一种名为深度控制学习（DCL）的新型深度强化学习框架，针对库存控制问题进行了量身定制，并通过比较评估表明，在各种测试情况下，DCL相对于传统算法和最先进的启发式算法，在成本和最优性方面都取得了更好的表现。

    

    问题定义：传统的深度强化学习（DRL）算法，用于包括游戏和机器人等各种目的，是否是库存控制应用中最适合的机器学习算法？针对库存控制问题特点量身定制的DRL算法在性能上是否优于DRL和传统基准？方法/结果：我们提出并研究了基于近似策略迭代的新型DRL框架——深度控制学习（DCL），专门用于解决库存问题。比较性评估表明，DCL在销售损失库存控制、易腐库存系统和具有随机引导时间的库存系统中优于现有的最先进启发式算法，在所有测试实例中实现了更低的平均成本，并且维持了最多0.1%的最优性差距。值得注意的是，所有实验都使用相同的超参数集。

    Problem Definition: Are traditional deep reinforcement learning (DRL) algorithms, developed for a broad range of purposes including game-play and robotics, the most suitable machine learning algorithms for applications in inventory control? To what extent would DRL algorithms tailored to the unique characteristics of inventory control problems provide superior performance compared to DRL and traditional benchmarks? Methodology/results: We propose and study Deep Controlled Learning (DCL), a new DRL framework based on approximate policy iteration specifically designed to tackle inventory problems. Comparative evaluations reveal that DCL outperforms existing state-of-the-art heuristics in lost sales inventory control, perishable inventory systems, and inventory systems with random lead times, achieving lower average costs across all test instances and maintaining an optimality gap of no more than 0.1\%. Notably, the same hyperparameter set is utilized across all experiments, underscoring 
    
[^252]: 使用MCP方法学习图拉普拉斯矩阵

    Learning Graph Laplacian with MCP. (arXiv:2010.11559v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2010.11559](http://arxiv.org/abs/2010.11559)

    本文提出了一种使用非凸惩罚函数MCP来学习图结构的方法，设计了一种算法并证明了其收敛性。实验证明，与现有方法相比，我们的方法在学习图拉普拉斯矩阵与MCP时更高效且可靠。

    

    本论文研究了在满足拉普拉斯矩阵约束的情况下，使用非凸惩罚函数：最小最大凹惩罚函数（MCP）来学习图结构的问题。我们设计了一种不精确的凸差分算法来解决MCP惩罚的图模型，并证明其收敛性。我们注意到，凸差分算法的每个子问题都具有一个优秀的性质，即其对偶问题中的目标函数具有连续可微的半光滑梯度。因此，我们将高效的半光滑牛顿法应用于凸差分算法的子问题。对多个合成数据集和真实数据集进行的数值实验证明了非凸惩罚MCP在促进稀疏性方面的有效性。与现有的最先进方法相比，我们的方法在学习图拉普拉斯矩阵与MCP时表现出更高的效率和可靠性。

    We consider the problem of learning a graph under the Laplacian constraint with a non-convex penalty: minimax concave penalty (MCP). For solving the MCP penalized graphical model, we design an inexact proximal difference-of-convex algorithm (DCA) and prove its convergence to critical points. We note that each subproblem of the proximal DCA enjoys the nice property that the objective function in its dual problem is continuously differentiable with a semismooth gradient. Therefore, we apply an efficient semismooth Newton method to subproblems of the proximal DCA. Numerical experiments on various synthetic and real data sets demonstrate the effectiveness of the non-convex penalty MCP in promoting sparsity. Compared with the existing state-of-the-art method, our method is demonstrated to be more efficient and reliable for learning graph Laplacian with MCP.
    
[^253]: 文本作为环境:一种深度强化学习的文本可读性评估模型

    Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/1912.05957](http://arxiv.org/abs/1912.05957)

    这是一种使用深度强化学习模型评估文本可读性的方法，通过使用硬注意力的主动推理技术和半监督信号来提高效率，并与其他先进模型进行比较。

    

    评估文本的可读性可以显著促进信息的准确表达。文本可读性评估的制定涉及对文本的有意义的属性进行识别，而不论其长度。为了准确评估文本的可理解性，使用了复杂的特征和模型。尽管如此，高效评估文本可读性的问题相对较少研究。通过使用硬注意力的主动推理技术，提出的方法更有效地利用输入文本和计算资源。通过使用半监督信号，强化学习模型可以使用最少的文本来确定文本的可读性。将该模型与Weebit和剑桥考试等最先进的模型进行比较。

    Evaluating the readability of a text can significantly facilitate the precise expression of information in written form. The formulation of text readability assessment involves the identification of meaningful properties of the text regardless of its length. Sophisticated features and models are used to evaluate the comprehensibility of texts accurately. Despite this, the problem of assessing texts' readability efficiently remains relatively untouched. The efficiency of state-of-the-art text readability assessment models can be further improved using deep reinforcement learning models. Using a hard attention-based active inference technique, the proposed approach makes efficient use of input text and computational resources. Through the use of semi-supervised signals, the reinforcement learning model uses the minimum amount of text in order to determine text's readability. A comparison of the model on Weebit and Cambridge Exams with state-of-the-art models, such as the BERT text readab
    
[^254]: 基因组学的深度学习: 一个简洁的概述

    Deep Learning for Genomics: A Concise Overview. (arXiv:1802.00810v3 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/1802.00810](http://arxiv.org/abs/1802.00810)

    深度学习在基因组学领域面临独特的挑战，但通过深入了解任务需求，并与适当的深度架构相匹配，深度学习在基因组学中取得了成功。

    

    高通量测序等基因组研究的进展已将现代基因组学推向了"大数据"学科。这种数据爆炸不断挑战传统基因组学方法的使用。与强大算法的紧急需求相平行的是，深度学习在视觉、语音和文本处理等领域取得了成功。然而，对于基因组学来说，深度学习面临着独特的挑战，因为我们期望深度学习能够超出我们的知识探索基因组的解读。一个强大的深度学习模型应该依赖于对特定任务的深入了解。在本文中，我们简要讨论了从基因组学角度看不同深度学习模型的优势，以便将每个特定任务与适当的深度架构相匹配，并对发展现代基因组学深度学习架构的实践考虑进行了评述。我们还提供了深度学习在基因组学中的应用的简明回顾。

    Advancements in genomic research such as high-throughput sequencing techniques have driven modern genomic studies into "big data" disciplines. This data explosion is constantly challenging conventional methods used in genomics. In parallel with the urgent demand for robust algorithms, deep learning has succeeded in a variety of fields such as vision, speech, and text processing. Yet genomics entails unique challenges to deep learning since we are expecting from deep learning a superhuman intelligence that explores beyond our knowledge to interpret the genome. A powerful deep learning model should rely on insightful utilization of task-specific knowledge. In this paper, we briefly discuss the strengths of different deep learning models from a genomic perspective so as to fit each particular task with a proper deep architecture, and remark on practical considerations of developing modern deep learning architectures for genomics. We also provide a concise review of deep learning applicati
    

