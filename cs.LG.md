# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines](https://arxiv.org/abs/2404.00082) | 通过可学习延迟线实现可微分反馈延迟网络的参数优化，实现了对室内声学特性的数据驱动建模。 |
| [^2] | [Towards gaze-independent c-VEP BCI: A pilot study](https://arxiv.org/abs/2404.00031) | 这项试验性研究首次尝试朝向无需凝视的脑机接口拼写器，通过使用空间注意力而非眼球移动来解码视觉刺激，取得了很高的分类准确率。 |
| [^3] | [Towards LLM-RecSys Alignment with Textual ID Learning](https://arxiv.org/abs/2403.19021) | 通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。 |
| [^4] | [FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs](https://arxiv.org/abs/2403.15593) | FairerCLIP提出了一种在RKHSs中使用函数去除CLIP的零样本预测偏见的通用方法，使得预测更公平且更能抵抗虚假相关性。 |
| [^5] | [Towards auditory attention decoding with noise-tagging: A pilot study](https://arxiv.org/abs/2403.15523) | 这项试点研究首次尝试使用噪声标记刺激协议进行听觉注意力解码，取得了较高的性能表现。 |
| [^6] | [Learning to walk on new ground: Calibration-free decoding for c-VEP BCI](https://arxiv.org/abs/2403.15521) | 该研究介绍了一种基于事件相关电位的新型方法（UMM），与目前先进的c-VEP零训练方法（CCA）进行比较，证明了它们在无需校准的BCI系统中的有效性。 |
| [^7] | [A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection](https://arxiv.org/abs/2403.12864) | 本研究比较了不同深度学习架构在太空船数据异常检测中的有效性。 |
| [^8] | [Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes](https://arxiv.org/abs/2403.11678) | 通过3D感知的潜空间和跨场景共享信息，实现了NeRFs有效学习大量语义相似场景，并显著降低了训练时间和内存消耗 |
| [^9] | [PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor](https://arxiv.org/abs/2403.06668) | PeerAiD提出了一种新的对抗性蒸馏方法，通过让同行网络学习学生网络的对抗性示例，而不是自身的示例，来提升神经网络的鲁棒性。 |
| [^10] | [Biomedical Entity Linking as Multiple Choice Question Answering](https://arxiv.org/abs/2402.15189) | 提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。 |
| [^11] | [Spurious Correlations in Machine Learning: A Survey](https://arxiv.org/abs/2402.12715) | 机器学习系统对输入中偏见特征与标签之间的虚假相关性敏感，本文回顾了解决这一问题的最新方法，同时总结了数据集、基准和度量标准，并讨论了未来研究挑战。 |
| [^12] | [Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea](https://arxiv.org/abs/2402.08502) | 这项研究提出了一种可证明安全的强化学习方法，用于在开放海域上的船只中遵守交通规则，并引入了一种有效的验证方法来确定行为是否符合COLREGS规则。 |
| [^13] | [EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss](https://arxiv.org/abs/2402.05008) | 高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。 |
| [^14] | [SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems](https://arxiv.org/abs/2402.03145) | SafEDMD是一种基于EDMD的学习架构，通过稳定性和认证导向，生成可靠的数据驱动替代模型，并基于半定规划进行认证控制器设计。它在多个基准示例上展示了优于现有方法的优势。 |
| [^15] | [Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features](https://arxiv.org/abs/2402.02969) | 通过研究随机特征，我们发现注意力层具有较高的词敏感性，这对于理解transformers的成功以及自然语言处理任务中的上下文含义非常重要。 |
| [^16] | [Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion](https://arxiv.org/abs/2401.17583) | 本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。 |
| [^17] | [TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese.](http://arxiv.org/abs/2401.16640) | 这篇论文开发了用于低资源环境中的开放式基础模型，以巴西葡萄牙语为例，发布在GitHub和Hugging Face上供社区使用和进一步开发。 |
| [^18] | [How Can Large Language Models Understand Spatial-Temporal Data?.](http://arxiv.org/abs/2401.14192) | 本文提出了一种名为STG-LLM的创新方法，用于使大型语言模型能够理解时空数据并进行预测。该方法利用STG-Tokenizer将复杂的图形数据转化为简洁的标记，再通过STG-Adapter将标记化数据与LLM的理解能力进行连接。通过微调参数，STG-LLM能够有效地把握标记的语义，同时保留LLM的自然语言理解能力。通过广泛的实验验证了STG-LLM的优越性能。 |
| [^19] | [Exploration and Anti-Exploration with Distributional Random Network Distillation.](http://arxiv.org/abs/2401.09750) | 该论文提出了一种新的深度强化学习探索算法，称为分布式随机网络蒸馏（DRND）。该算法通过蒸馏随机网络的分布和隐式融入伪计数来改进奖励分配的精度，从而增强了探索过程。理论分析和实验结果均表明该方法的优越性。 |
| [^20] | [Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective.](http://arxiv.org/abs/2401.08667) | 该论文研究了利用物理信息神经网络(PINNs)实现数字孪生(DT)的潜力。提出了适用于无网格框架的自适应采样方法，并验证了PINNs在参数化的Navier-Stokes方程中的可扩展性和多保真度的优势。 |
| [^21] | [Are self-explanations from Large Language Models faithful?.](http://arxiv.org/abs/2401.07927) | 大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。 |
| [^22] | [Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks.](http://arxiv.org/abs/2401.07494) | 通过结合输入凸性和Lipschitz连续性的优势，我们开发了一种名为输入凸性Lipschitz循环神经网络的新型网络结构，在计算效率和对抗鲁棒性方面优于现有的循环单元，并适用于多种工程任务。 |
| [^23] | [GIST: Generated Inputs Sets Transferability in Deep Learning.](http://arxiv.org/abs/2311.00801) | 这篇论文介绍了一种在深度学习模型之间高效迁移测试集的新方法，通过选择具有用户感兴趣的属性的良好测试集，以达到改善可验证性和测试性的目的。 |
| [^24] | [Convergence of flow-based generative models via proximal gradient descent in Wasserstein space.](http://arxiv.org/abs/2310.17582) | 本文通过在Wasserstein空间中应用近端梯度下降，证明了基于流的生成模型的收敛性，并提供了生成数据分布的理论保证。 |
| [^25] | [Topological Data Analysis in smart manufacturing processes -- A survey on the state of the art.](http://arxiv.org/abs/2310.09319) | 本次调查总结了拓扑数据分析在智能制造和产业4.0背景下的最新进展，突出了其在工业生产和制造领域的关键优势和挑战，并讨论了未充分利用的TDA方法和已识别的应用类型，以推动更多的相关研究。 |
| [^26] | [Identifying the Risks of LM Agents with an LM-Emulated Sandbox.](http://arxiv.org/abs/2309.15817) | 通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。 |
| [^27] | [Large language models can accurately predict searcher preferences.](http://arxiv.org/abs/2309.10621) | 大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。 |
| [^28] | [Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs.](http://arxiv.org/abs/2309.05516) | 本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。 |
| [^29] | [Online Distributed Learning over Random Networks.](http://arxiv.org/abs/2309.00520) | 本文提出了在随机网络上进行在线分布式学习的DOT-ADMM算法，通过解决在线学习、异步计算、不可靠通信和不精确计算等挑战，在一大类凸学习问题上获得了线性收敛速度。 |
| [^30] | [AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System.](http://arxiv.org/abs/2307.04577) | AnyTeleop是一个通用的视觉导向的远程操作系统，支持多个不同的机械臂、手部、环境和摄像头配置，在实际实验和模拟中表现出色。 |
| [^31] | [Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values.](http://arxiv.org/abs/2306.06276) | 该论文提出应用对比学习方法，从有限的数据样本中学习基因表达数据的良好特征表示，并应用于Cox模型中，可以显著提高癌症预后的预测性能。 |
| [^32] | [C(NN)FD -- a deep learning framework for turbomachinery CFD analysis.](http://arxiv.org/abs/2306.05889) | 本文介绍了一种新型深度学习框架C(NN)FD，用于实时预测燃气轮机中轴向压缩机制造和组装变化对整体性能的影响。该框架可过滤掉CFD解决方案中的相关部分，因此具有可扩展性，且实时精度可与CFD基准相媲美。 |
| [^33] | [Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features.](http://arxiv.org/abs/2305.12100) | 本论文研究了ERM训练模型对抗强大黑盒攻击的安全问题，并通过两个指标量化模型安全性：单个样本的稳定性和查询与原始数据特征的对齐。在研究中，通过研究RF和NTK回归，证明随着泛化能力的提高，隐私保护可以得到加强。 |
| [^34] | [ACRoBat: Optimizing Auto-batching of Dynamic Deep Learning at Compile Time.](http://arxiv.org/abs/2305.10611) | ACRoBat是一种动态深度学习的自动批处理框架，在编译时进行混合静态+动态编译器优化和端到端张量代码生成，可将性能提高多达8.5倍。 |
| [^35] | [Node Feature Augmentation Vitaminizes Network Alignment.](http://arxiv.org/abs/2304.12751) | 本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。 |
| [^36] | [Uniform Pessimistic Risk and Optimal Portfolio.](http://arxiv.org/abs/2303.07158) | 本文提出了一种称为统一悲观风险的综合$\alpha$-风险版本和基于风险获得最优组合的计算算法，该方法可以用于估计韩国股票的悲观最优组合模型。 |
| [^37] | [ScionFL: Efficient and Robust Secure Quantized Aggregation.](http://arxiv.org/abs/2210.07376) | 本文介绍了ScionFL，这是第一个在联邦学习中高效运行在量化输入上并同时提供对恶意客户端强健性的安全聚合框架。 |
| [^38] | [Efficient Learning of Accurate Surrogates for Simulations of Complex Systems.](http://arxiv.org/abs/2207.12855) | 本论文提出了一种在线学习方法，由优化器驱动的采样方法，该方法可以提高替代模型的预测能力，并构建了一个比目前使用的替代模型更准确的替代模型，为复杂系统模拟提供了更加计算上高效的替代模型方法。 |

# 详细

[^1]: 基于可微分反馈延迟网络和可学习延迟线的数据驱动室内声学建模

    Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay Networks With Learnable Delay Lines

    [https://arxiv.org/abs/2404.00082](https://arxiv.org/abs/2404.00082)

    通过可学习延迟线实现可微分反馈延迟网络的参数优化，实现了对室内声学特性的数据驱动建模。

    

    在过去的几十年中，人们致力于设计人工混响算法，旨在模拟物理环境的室内声学。尽管取得了显著进展，但延迟网络模型的自动参数调整仍然是一个开放性挑战。我们提出了一种新方法，通过学习可微分反馈延迟网络（FDN）的参数，使其输出呈现出所测得的室内脉冲响应的感知特性。

    arXiv:2404.00082v1 Announce Type: cross  Abstract: Over the past few decades, extensive research has been devoted to the design of artificial reverberation algorithms aimed at emulating the room acoustics of physical environments. Despite significant advancements, automatic parameter tuning of delay-network models remains an open challenge. We introduce a novel method for finding the parameters of a Feedback Delay Network (FDN) such that its output renders the perceptual qualities of a measured room impulse response. The proposed approach involves the implementation of a differentiable FDN with trainable delay lines, which, for the first time, allows us to simultaneously learn each and every delay-network parameter via backpropagation. The iterative optimization process seeks to minimize a time-domain loss function incorporating differentiable terms accounting for energy decay and echo density. Through experimental validation, we show that the proposed method yields time-invariant freq
    
[^2]: 朝向无需凝视的c-VEP脑机接口：一项试验性研究

    Towards gaze-independent c-VEP BCI: A pilot study

    [https://arxiv.org/abs/2404.00031](https://arxiv.org/abs/2404.00031)

    这项试验性研究首次尝试朝向无需凝视的脑机接口拼写器，通过使用空间注意力而非眼球移动来解码视觉刺激，取得了很高的分类准确率。

    

    脑机接口（BCI）拼写器的一个局限性在于它们要求用户能够移动眼睛注视目标。对于无法自愿控制眼睛移动的用户（例如患有晚期肌萎缩侧索硬化症（ALS）的人群），这会造成问题。这项试验性研究首次迈出了朝向基于代码调制视觉诱发电位（c-VEP）的无需凝视的拼写器的第一步。参与者被呈现两个双侧位置的刺激，其中一个在闪烁，并被要求专注于这些刺激中的一个，可以通过直接看着刺激（明显条件）或使用空间注意力来完成，消除了眼球移动的需要（隐蔽条件）。被专注的刺激从脑电图（EEG）中解码，隐蔽和明显条件的分类准确率分别为88%和100%。这些基础性见解展示了这一技术的前景。

    arXiv:2404.00031v1 Announce Type: cross  Abstract: A limitation of brain-computer interface (BCI) spellers is that they require the user to be able to move the eyes to fixate on targets. This poses an issue for users who cannot voluntarily control their eye movements, for instance, people living with late-stage amyotrophic lateral sclerosis (ALS). This pilot study makes the first step towards a gaze-independent speller based on the code-modulated visual evoked potential (c-VEP). Participants were presented with two bi-laterally located stimuli, one of which was flashing, and were tasked to attend to one of these stimuli either by directly looking at the stimuli (overt condition) or by using spatial attention, eliminating the need for eye movement (covert condition). The attended stimuli were decoded from electroencephalography (EEG) and classification accuracies of 88% and 100% were obtained for the covert and overt conditions, respectively. These fundamental insights show the promisin
    
[^3]: 朝向LLM-RecSys对齐与文本ID学习的方向

    Towards LLM-RecSys Alignment with Textual ID Learning

    [https://arxiv.org/abs/2403.19021](https://arxiv.org/abs/2403.19021)

    通过提出IDGen，将每个推荐项目表示为独特、简洁、语义丰富的文本ID，从而使得基于大型语言模型的推荐更好地与自然语言生成对齐。

    

    基于大型语言模型(LLMs)的生成式推荐已经将传统的基于排名的推荐方式转变为文本生成范例。然而，与固有操作人类词汇的标准NLP任务相反，目前生成式推荐领域的研究在如何在文本生成范式中以简洁而有意义的ID表示有效编码推荐项目方面存在困难。为了更好地对齐LLMs与推荐需求，我们提出了IDGen，使用人类语言标记将每个项目表示为独特、简洁、语义丰富、与平台无关的文本ID。这通过在基于LLM的推荐系统旁训练文本ID生成器来实现，使个性化推荐能够无缝集成到自然语言生成中。值得注意的是，由于用户历史记录以自然语言表达并与原始数据集解耦，我们的方法提出了潜在的

    arXiv:2403.19021v1 Announce Type: cross  Abstract: Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potenti
    
[^4]: FairerCLIP: 在RKHSs中使用函数去除CLIP的零样本预测偏见

    FairerCLIP: Debiasing CLIP's Zero-Shot Predictions using Functions in RKHSs

    [https://arxiv.org/abs/2403.15593](https://arxiv.org/abs/2403.15593)

    FairerCLIP提出了一种在RKHSs中使用函数去除CLIP的零样本预测偏见的通用方法，使得预测更公平且更能抵抗虚假相关性。

    

    大型预训练的视觉-语言模型（如CLIP）提供了文本和图像的紧凑通用表示，已被证明在多个下游零样本预测任务中有效。然而，由于它们训练过程的性质，这些模型可能存在以下问题：1）传播或放大社会偏见和2）学习依赖虚假特征。本文提出FairerCLIP，一种通用方法，使CLIP的零样本预测更加公平且更能抵抗虚假相关性。我们在再生核希尔伯特空间（RKHSs）中联合去偏CLIP的图像和文本表示问题，这带来了多个好处：1）灵活性：与现有方法不同，现有方法要么专门用于学习有地面真相标签的情况，要么专门用于学习没有地面真相标签的情况，FairerCLIP能够适应两种学习情况。2）优化便利性：FairerCLIP对于迭代优化非常合适。

    arXiv:2403.15593v1 Announce Type: cross  Abstract: Large pre-trained vision-language models such as CLIP provide compact and general-purpose representations of text and images that are demonstrably effective across multiple downstream zero-shot prediction tasks. However, owing to the nature of their training process, these models have the potential to 1) propagate or amplify societal biases in the training data and 2) learn to rely on spurious features. This paper proposes FairerCLIP, a general approach for making zero-shot predictions of CLIP more fair and robust to spurious correlations. We formulate the problem of jointly debiasing CLIP's image and text representations in reproducing kernel Hilbert spaces (RKHSs), which affords multiple benefits: 1) Flexibility: Unlike existing approaches, which are specialized to either learn with or without ground-truth labels, FairerCLIP is adaptable to learning in both scenarios. 2) Ease of Optimization: FairerCLIP lends itself to an iterative o
    
[^5]: 采用噪声标记的听觉注意力解码研究：一项试点研究

    Towards auditory attention decoding with noise-tagging: A pilot study

    [https://arxiv.org/abs/2403.15523](https://arxiv.org/abs/2403.15523)

    这项试点研究首次尝试使用噪声标记刺激协议进行听觉注意力解码，取得了较高的性能表现。

    

    听觉注意力解码(AAD)旨在从大脑活动中提取被关注的说话者，提供了神经导向听觉设备和脑机接口等领域的应用前景。本试点研究首次尝试使用噪声标记刺激协议进行AAD，该协议引发了可靠的编码调制诱发电位，但在听觉模式下的探索还很有限。研究参与者依次呈现两个荷兰语言语音刺激，这些刺激被幅度调制为具有唯一二进制伪随机噪声码，有效地为其标记了附加可解码信息。我们比较了未调制音频与使用不同调制深度调制的音频的解码，以及传统AAD方法与标准解码噪声码方法的对比。我们的试点研究发现，与未调制音频相比，70至100%的调制深度的传统方法表现出更高的性能。

    arXiv:2403.15523v1 Announce Type: cross  Abstract: Auditory attention decoding (AAD) aims to extract from brain activity the attended speaker amidst candidate speakers, offering promising applications for neuro-steered hearing devices and brain-computer interfacing. This pilot study makes a first step towards AAD using the noise-tagging stimulus protocol, which evokes reliable code-modulated evoked potentials, but is minimally explored in the auditory modality. Participants were sequentially presented with two Dutch speech stimuli that were amplitude modulated with a unique binary pseudo-random noise-code, effectively tagging these with additional decodable information. We compared the decoding of unmodulated audio against audio modulated with various modulation depths, and a conventional AAD method against a standard method to decode noise-codes. Our pilot study revealed higher performances for the conventional method with 70 to 100 percent modulation depths compared to unmodulated au
    
[^6]: 学习在新领域行走：面向c-VEP BCI的无校准解码方法

    Learning to walk on new ground: Calibration-free decoding for c-VEP BCI

    [https://arxiv.org/abs/2403.15521](https://arxiv.org/abs/2403.15521)

    该研究介绍了一种基于事件相关电位的新型方法（UMM），与目前先进的c-VEP零训练方法（CCA）进行比较，证明了它们在无需校准的BCI系统中的有效性。

    

    本研究探讨了两种零训练方法，旨在增强脑机接口（BCI）的可用性，消除了校准会话的需求。我们介绍了一种根植于事件相关电位（ERP）领域的新颖方法，即无监督均值最大化（UMM），用于快速编码调制视觉诱发电位（c-VEP）刺激协议。我们将UMM与使用典型相关分析（CCA）的c-VEP零训练方法进行比较。比较包括对CCA和UMM的即时分类以及对先前分类试验进行累积学习的分类。我们的研究显示了两种方法在处理c-VEP数据集的复杂性方面的有效性，突出了它们之间的差异和独特优势。这项研究不仅为无校准BCI方法的实际实施提供了见解，还为进一步的探索和改进铺平了道路。

    arXiv:2403.15521v1 Announce Type: cross  Abstract: This study explores two zero-training methods aimed at enhancing the usability of brain-computer interfaces (BCIs) by eliminating the need for a calibration session. We introduce a novel method rooted in the event-related potential (ERP) domain, unsupervised mean maximization (UMM), to the fast code-modulated visual evoked potential (c-VEP) stimulus protocol. We compare UMM to the state-of-the-art c-VEP zero-training method that uses canonical correlation analysis (CCA). The comparison includes instantaneous classification and classification with cumulative learning from previously classified trials for both CCA and UMM. Our study shows the effectiveness of both methods in navigating the complexities of a c-VEP dataset, highlighting their differences and distinct strengths. This research not only provides insights into the practical implementation of calibration-free BCI methods but also paves the way for further exploration and refine
    
[^7]: 太空船异常检测的深度学习架构比较

    A Comparison of Deep Learning Architectures for Spacecraft Anomaly Detection

    [https://arxiv.org/abs/2403.12864](https://arxiv.org/abs/2403.12864)

    本研究比较了不同深度学习架构在太空船数据异常检测中的有效性。

    

    太空船操作具有极高的关键性，要求具有无可挑剔的可靠性和安全性。确保太空船的最佳性能需要及早检测和减轻异常情况，否则可能导致部件或任务失败。随着深度学习的出现，人们对利用这些复杂算法在空间操作中进行异常检测表现出了较大兴趣。本研究旨在比较各种深度学习架构在太空船数据异常检测中的有效性。正在研究的深度学习模型包括卷积神经网络（CNNs）、递归神经网络（RNNs）、长短期记忆（LSTM）网络以及基于Transformer的架构。这些模型中的每一个都是使用来自多个太空船任务的全面数据集进行训练和验证的，包括各种运行场景和异常类型。初步结果表明，虽然…

    arXiv:2403.12864v1 Announce Type: new  Abstract: Spacecraft operations are highly critical, demanding impeccable reliability and safety. Ensuring the optimal performance of a spacecraft requires the early detection and mitigation of anomalies, which could otherwise result in unit or mission failures. With the advent of deep learning, a surge of interest has been seen in leveraging these sophisticated algorithms for anomaly detection in space operations. This study aims to compare the efficacy of various deep learning architectures in detecting anomalies in spacecraft data. The deep learning models under investigation include Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, and Transformer-based architectures. Each of these models was trained and validated using a comprehensive dataset sourced from multiple spacecraft missions, encompassing diverse operational scenarios and anomaly types. Initial results indicate that while 
    
[^8]: 探索基于3D感知的潜空间，以有效学习多个场景

    Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous Scenes

    [https://arxiv.org/abs/2403.11678](https://arxiv.org/abs/2403.11678)

    通过3D感知的潜空间和跨场景共享信息，实现了NeRFs有效学习大量语义相似场景，并显著降低了训练时间和内存消耗

    

    我们提出了一种方法，通过结合两种技术来改善每个场景所需的训练时间和内存成本，从而实现将NeRFs无缝扩展到学习大量语义相似的场景。首先，我们学习了一个3D感知的潜空间，在其中训练三平面场景表示，从而降低了学习场景的分辨率。此外，我们提出了一种跨场景共享通用信息的方式，从而减少了学习特定场景的模型复杂性。当训练1000个场景时，我们的方法将每个场景的内存成本降低了44%，时间成本降低了86%。我们的项目页面位于https://3da-ae.github.io。

    arXiv:2403.11678v1 Announce Type: cross  Abstract: We present a method enabling the scaling of NeRFs to learn a large number of semantically-similar scenes. We combine two techniques to improve the required training time and memory cost per scene. First, we learn a 3D-aware latent space in which we train Tri-Plane scene representations, hence reducing the resolution at which scenes are learned. Moreover, we present a way to share common information across scenes, hence allowing for a reduction of model complexity to learn a particular scene. Our method reduces effective per-scene memory costs by 44% and per-scene time costs by 86% when training 1000 scenes. Our project page can be found at https://3da-ae.github.io .
    
[^9]: PeerAiD：改善专业同行导师的对抗性蒸馏

    PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor

    [https://arxiv.org/abs/2403.06668](https://arxiv.org/abs/2403.06668)

    PeerAiD提出了一种新的对抗性蒸馏方法，通过让同行网络学习学生网络的对抗性示例，而不是自身的示例，来提升神经网络的鲁棒性。

    

    神经网络的对抗鲁棒性在应用于安全关键领域时是一个重要问题。在这种情况下，对抗性蒸馏是一种有前途的选择，旨在提炼教师网络的鲁棒性，以改进小型学生网络的鲁棒性。我们提出了PeerAiD，通过让同行网络学习学生网络的对抗性示例，而不是针对自身的对抗性示例，来改进对抗性蒸馏。PeerAiD是一种对抗性蒸馏，同时训练同行网络和学生网络。

    arXiv:2403.06668v1 Announce Type: new  Abstract: Adversarial robustness of the neural network is a significant concern when it is applied to security-critical domains. In this situation, adversarial distillation is a promising option which aims to distill the robustness of the teacher network to improve the robustness of a small student network. Previous works pretrain the teacher network to make it robust to the adversarial examples aimed at itself. However, the adversarial examples are dependent on the parameters of the target network. The fixed teacher network inevitably degrades its robustness against the unseen transferred adversarial examples which targets the parameters of the student network in the adversarial distillation process. We propose PeerAiD to make a peer network learn the adversarial examples of the student network instead of adversarial examples aimed at itself. PeerAiD is an adversarial distillation that trains the peer network and the student network simultaneousl
    
[^10]: 将生物医学实体链接视为多项选择问答

    Biomedical Entity Linking as Multiple Choice Question Answering

    [https://arxiv.org/abs/2402.15189](https://arxiv.org/abs/2402.15189)

    提出了一种新颖的模型BioELQA，将生物医学实体链接看作是多项选择问答，通过使用快速检索器获得候选实体，实现了更好的实体链接效果。

    

    尽管预训练语言模型在生物医学实体链接（BioEL）方面取得了显著进展，但对于细粒度和长尾实体仍然存在挑战。为了解决这些挑战，我们提出了BioELQA，这是一种将生物医学实体链接视为多项选择问答的新颖模型。BioELQA首先利用快速检索器获得候选实体，将提及和候选实体共同呈现给生成器，然后输出与其选定实体相关的预测符号。这种公式使得不同候选实体之间的明确比较成为可能，从而捕捉了提及和实体之间以及实体之间的精细交互。为了改善长尾实体的泛化能力，我们检索相似的已标记训练实例作为线索，并将输入与检索实例连接到生成器。广泛的实验结果表明，BioELQA的表现优于统计结果。

    arXiv:2402.15189v1 Announce Type: cross  Abstract: Although biomedical entity linking (BioEL) has made significant progress with pre-trained language models, challenges still exist for fine-grained and long-tailed entities. To address these challenges, we present BioELQA, a novel model that treats Biomedical Entity Linking as Multiple Choice Question Answering. BioELQA first obtains candidate entities with a fast retriever, jointly presents the mention and candidate entities to a generator, and then outputs the predicted symbol associated with its chosen entity. This formulation enables explicit comparison of different candidate entities, thus capturing fine-grained interactions between mentions and entities, as well as among entities themselves. To improve generalization for long-tailed entities, we retrieve similar labeled training instances as clues and concatenate the input with retrieved instances for the generator. Extensive experimental results show that BioELQA outperforms stat
    
[^11]: 机器学习中的虚假相关性：一项调查

    Spurious Correlations in Machine Learning: A Survey

    [https://arxiv.org/abs/2402.12715](https://arxiv.org/abs/2402.12715)

    机器学习系统对输入中偏见特征与标签之间的虚假相关性敏感，本文回顾了解决这一问题的最新方法，同时总结了数据集、基准和度量标准，并讨论了未来研究挑战。

    

    众所周知，机器学习系统对输入中偏见特征（例如背景、纹理和次要对象）与相应标签之间的虚假相关性敏感。这些特征及其与标签的相关性被称为“虚假”，因为它们往往随着真实世界数据分布的变化而改变，这可能对模型的泛化能力和鲁棒性产生负面影响。在这项调查中，我们全面审查了这一问题，提供了一个关于解决机器学习模型中虚假相关性的当前最先进方法的分类法。此外，我们总结了现有的数据集、基准和度量标准，以帮助未来的研究。本文最后讨论了这一领域的最新进展和未来研究挑战，旨在为相关领域的研究人员提供宝贵的见解。

    arXiv:2402.12715v1 Announce Type: new  Abstract: Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as "spurious" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.
    
[^12]: 在开放海域上的安全强化学习中可证明的交通规则遵守

    Provable Traffic Rule Compliance in Safe Reinforcement Learning on the Open Sea

    [https://arxiv.org/abs/2402.08502](https://arxiv.org/abs/2402.08502)

    这项研究提出了一种可证明安全的强化学习方法，用于在开放海域上的船只中遵守交通规则，并引入了一种有效的验证方法来确定行为是否符合COLREGS规则。

    

    自主车辆必须遵守交通规则。这些规则通常使用时态逻辑进行形式化，导致使用基于优化的运动规划器解决这些约束变得困难。强化学习（RL）是一种有前途的方法，可以找到符合时态逻辑规范的运动规划。然而，纯强化学习算法基于随机探索，这在本质上是不安全的。为了解决这个问题，我们提出了一种可证明安全的RL方法，始终遵守交通规则。作为一个特定的应用领域，我们考虑在开放海域上的船只，这些船只必须遵守《海上避碰规则公约》（COLREGS）的规定。我们介绍了一种高效的验证方法，用于确定行为与使用时态逻辑形式化的COLREGS的符合性。我们的行为验证被集成到RL过程中，以便智能体只选择经过验证的行为。与只集成交通规则的智能体相比，

    Autonomous vehicles have to obey traffic rules. These rules are often formalized using temporal logic, resulting in constraints that are hard to solve using optimization-based motion planners. Reinforcement Learning (RL) is a promising method to find motion plans adhering to temporal logic specifications. However, vanilla RL algorithms are based on random exploration, which is inherently unsafe. To address this issue, we propose a provably safe RL approach that always complies with traffic rules. As a specific application area, we consider vessels on the open sea, which must adhere to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGS). We introduce an efficient verification approach that determines the compliance of actions with respect to the COLREGS formalized using temporal logic. Our action verification is integrated into the RL process so that the agent only selects verified actions. In contrast to agents that only integrate the traffic rule
    
[^13]: 高效ViT-SAM: 在不损失性能的情况下加速的段落任意模型

    EfficientViT-SAM: Accelerated Segment Anything Model Without Performance Loss

    [https://arxiv.org/abs/2402.05008](https://arxiv.org/abs/2402.05008)

    高效ViT-SAM是一种新的加速的段落任意模型，通过保留轻量级提示编码器和掩码解码器，并替换沉重的图像编码器，实现了48.9倍的加速而不牺牲性能。

    

    我们提出了一种新的加速段落任意模型——高效ViT-SAM。我们保留了SAM的轻量级提示编码器和掩码解码器，同时用高效ViT替换了沉重的图像编码器。在训练方面，我们首先从SAM-ViT-H图像编码器到高效ViT进行知识蒸馏。随后，我们对SA-1B数据集进行端到端训练。由于高效ViT的效率和容量，高效ViT-SAM在A100 GPU上相比SAM-ViT-H实现了48.9倍的TensorRT加速，而无需牺牲性能。我们的代码和预训练模型已在https://github.com/mit-han-lab/efficientvit发布。

    We present EfficientViT-SAM, a new family of accelerated segment anything models. We retain SAM's lightweight prompt encoder and mask decoder while replacing the heavy image encoder with EfficientViT. For the training, we begin with the knowledge distillation from the SAM-ViT-H image encoder to EfficientViT. Subsequently, we conduct end-to-end training on the SA-1B dataset. Benefiting from EfficientViT's efficiency and capacity, EfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over SAM-ViT-H without sacrificing performance. Our code and pre-trained models are released at https://github.com/mit-han-lab/efficientvit.
    
[^14]: SafEDMD：一种专为非线性动态系统数据驱动控制而设计的认证学习架构

    SafEDMD: A certified learning architecture tailored to data-driven control of nonlinear dynamical systems

    [https://arxiv.org/abs/2402.03145](https://arxiv.org/abs/2402.03145)

    SafEDMD是一种基于EDMD的学习架构，通过稳定性和认证导向，生成可靠的数据驱动替代模型，并基于半定规划进行认证控制器设计。它在多个基准示例上展示了优于现有方法的优势。

    

    Koopman算子作为机器学习动态控制系统的理论基础，其中算子通过扩展动态模态分解（EDMD）启发式近似。在本文中，我们提出了稳定性和认证导向的EDMD（SafEDMD）：一种新颖的基于EDMD的学习架构，它提供了严格的证书，从而以数据驱动的方式生成可靠的替代模型。为了确保SafEDMD的可靠性，我们推导出比例误差界限，这些界限在原点处消失，并且适用于控制任务，从而基于半定规划进行认证控制器设计。我们通过几个基准示例说明了所开发的机制，并强调其相对于现有方法的优势。

    The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose Stability- and certificate-oriented EDMD (SafEDMD): a novel EDMD-based learning architecture which comes along with rigorous certificates, resulting in a reliable surrogate model generated in a data-driven fashion. To ensure trustworthiness of SafEDMD, we derive proportional error bounds, which vanish at the origin and are tailored for control tasks, leading to certified controller design based on semi-definite programming. We illustrate the developed machinery by means of several benchmark examples and highlight the advantages over state-of-the-art methods.
    
[^15]: 关于注意力层的词敏感性的理解：通过随机特征的研究

    Towards Understanding the Word Sensitivity of Attention Layers: A Study via Random Features

    [https://arxiv.org/abs/2402.02969](https://arxiv.org/abs/2402.02969)

    通过研究随机特征，我们发现注意力层具有较高的词敏感性，这对于理解transformers的成功以及自然语言处理任务中的上下文含义非常重要。

    

    揭示transformers异常成功背后原因需要更好地理解为什么注意力层适用于自然语言处理任务。特别是，这些任务要求预测模型捕捉上下文含义，即使句子很长，这往往取决于一个或几个词。我们的工作在随机特征的典型设置中研究了这一关键属性，称为词敏感性（WS）。我们展示了注意力层具有较高的WS，即在嵌入空间中存在一个向量，能够大幅扰动随机注意力特征映射。这个论点关键地利用了注意力层中softmax的作用，突显了它相对于其他激活函数（如ReLU）的优势。相反，标准随机特征的WS是$1/\sqrt{n}$阶的，$n$是文本样本中的单词数，因此它随上下文的长度而衰减。然后，我们将这些关于词敏感性的结果转化为泛化界：由于...

    Unveiling the reasons behind the exceptional success of transformers requires a better understanding of why attention layers are suitable for NLP tasks. In particular, such tasks require predictive models to capture contextual meaning which often depends on one or few words, even if the sentence is long. Our work studies this key property, dubbed word sensitivity (WS), in the prototypical setting of random features. We show that attention layers enjoy high WS, namely, there exists a vector in the space of embeddings that largely perturbs the random attention features map. The argument critically exploits the role of the softmax in the attention layer, highlighting its benefit compared to other activations (e.g., ReLU). In contrast, the WS of standard random features is of order $1/\sqrt{n}$, $n$ being the number of words in the textual sample, and thus it decays with the length of the context. We then translate these results on the word sensitivity into generalization bounds: due to th
    
[^16]: 敏捷但安全：学习无碰撞高速四足机器人行走

    Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion

    [https://arxiv.org/abs/2401.17583](https://arxiv.org/abs/2401.17583)

    本文介绍了一种名为敏捷但安全（ABS）的学习控制框架，能够实现四足机器人的敏捷且无碰撞行走。该框架通过一个学习得到的控制论到达-避免值网络来实现策略切换，并通过协作运行的敏捷策略和恢复策略，使机器人能够高速且安全地导航。

    

    在杂乱环境中行走的四足机器人必须既敏捷以提高任务执行效率，又要确保安全，避免与障碍物或人碰撞。现有的研究要么开发保守的控制器（速度小于1.0 m/s）以确保安全，要么专注于敏捷性而未考虑潜在致命的碰撞。本文介绍了敏捷但安全（ABS）的学习控制框架，为四足机器人实现了敏捷且无碰撞的行走。ABS包括一个敏捷策略来在障碍物中执行灵活的动作技能，并且有一个恢复策略来避免失败，共同实现高速且无碰撞的导航。ABS中的策略切换由一个学习得到的控制论到达-避免值网络控制，该网络也指导恢复策略作为目标函数，从而在闭环中保护机器人。训练过程涉及敏捷策略、到达-避免值网络、恢复策略和外感知表征的学习。

    Legged robots navigating cluttered environments must be jointly agile for efficient task execution and safe to avoid collisions with obstacles or humans. Existing studies either develop conservative controllers (< 1.0 m/s) to ensure safety, or focus on agility without considering potentially fatal collisions. This paper introduces Agile But Safe (ABS), a learning-based control framework that enables agile and collision-free locomotion for quadrupedal robots. ABS involves an agile policy to execute agile motor skills amidst obstacles and a recovery policy to prevent failures, collaboratively achieving high-speed and collision-free navigation. The policy switch in ABS is governed by a learned control-theoretic reach-avoid value network, which also guides the recovery policy as an objective function, thereby safeguarding the robot in a closed loop. The training process involves the learning of the agile policy, the reach-avoid value network, the recovery policy, and an exteroception repre
    
[^17]: TeenyTinyLlama：基于巴西葡萄牙语训练的开源微型语言模型

    TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])

    [http://arxiv.org/abs/2401.16640](http://arxiv.org/abs/2401.16640)

    这篇论文开发了用于低资源环境中的开放式基础模型，以巴西葡萄牙语为例，发布在GitHub和Hugging Face上供社区使用和进一步开发。

    

    大型语言模型（LLMs）在自然语言处理方面取得了显著的进展，但在各种语言中的进展还不平衡。虽然大多数LLMs是在像英语这样的高资源语言中训练的，但多语言模型通常比单语言模型表现稍差。此外，它们的多语言基础有时会限制它们产生的副产品，如计算需求和许可制度。在本研究中，我们记录了为在低资源环境中使用而量身定制的开放式基础模型的开发过程、其局限性和优势。这就是TeenyTinyLlama：两个用于巴西葡萄牙语文本生成的紧凑型模型。我们在GitHub和Hugging Face上以宽松的Apache 2.0许可证发布它们，供社区使用和进一步开发。详见https://github.com/Nkluge-correa/TeenyTinyLlama

    Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
    
[^18]: 如何让大型语言模型理解时空数据？

    How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])

    [http://arxiv.org/abs/2401.14192](http://arxiv.org/abs/2401.14192)

    本文提出了一种名为STG-LLM的创新方法，用于使大型语言模型能够理解时空数据并进行预测。该方法利用STG-Tokenizer将复杂的图形数据转化为简洁的标记，再通过STG-Adapter将标记化数据与LLM的理解能力进行连接。通过微调参数，STG-LLM能够有效地把握标记的语义，同时保留LLM的自然语言理解能力。通过广泛的实验验证了STG-LLM的优越性能。

    

    尽管大型语言模型（LLM）在自然语言处理和计算机视觉等任务中占据主导地位，但利用它们的能力进行时空预测仍然具有挑战性。时序文本与复杂的时空数据之间的差异阻碍了该应用的实现。为了解决这个问题，本文提出了STG-LLM，一种创新的方法，为LLM赋予了时空预测的能力。我们通过以下方式解决数据不匹配的问题：1）STG-Tokenizer：这个时空图形标记器将复杂的图形数据转化为简洁的标记，捕捉了空间和时间关系；2）STG-Adapter：这个精简的适配器由线性编码和解码层组成，填补了标记化数据和LLM理解之间的差距。通过仅微调一小部分参数，它可以有效地把握STG-Tokenizer生成的标记的语义，同时保留LLM的原始自然语言理解能力。通过在多种数据集上进行广泛实验，我们验证了STG-LLM的优越性能。

    While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diver
    
[^19]: 使用分布式随机网络蒸馏的探索和反探索

    Exploration and Anti-Exploration with Distributional Random Network Distillation. (arXiv:2401.09750v1 [cs.LG])

    [http://arxiv.org/abs/2401.09750](http://arxiv.org/abs/2401.09750)

    该论文提出了一种新的深度强化学习探索算法，称为分布式随机网络蒸馏（DRND）。该算法通过蒸馏随机网络的分布和隐式融入伪计数来改进奖励分配的精度，从而增强了探索过程。理论分析和实验结果均表明该方法的优越性。

    

    在深度强化学习中，探索仍然是一个重要问题，对于一个智能体在未知环境中取得高回报至关重要。虽然目前的探索随机网络蒸馏（Random Network Distillation，RND）算法已在许多环境中证明有效，但它在奖励分配上往往需要更高的区分能力。本文突出了RND中的“奖励不一致”问题，并指出了其主要限制。为了解决这个问题，我们引入了分布式RND（DRND），它是RND的一个变体。DRND通过蒸馏随机网络的分布并隐式地融入伪计数来改进奖励分配的精度，从而增强了探索过程。我们的方法有效地缓解了不一致问题，而不会引入显著的计算开销。理论分析和实验结果均证明了我们方法的优越性。

    Exploration remains a critical issue in deep reinforcement learning for an agent to attain high returns in unknown environments. Although the prevailing exploration Random Network Distillation (RND) algorithm has been demonstrated to be effective in numerous environments, it often needs more discriminative power in bonus allocation. This paper highlights the ``bonus inconsistency'' issue within RND, pinpointing its primary limitation. To address this issue, we introduce the Distributional RND (DRND), a derivative of the RND. DRND enhances the exploration process by distilling a distribution of random networks and implicitly incorporating pseudo counts to improve the precision of bonus allocation. This refinement encourages agents to engage in more extensive exploration. Our method effectively mitigates the inconsistency issue without introducing significant computational overhead. Both theoretical analysis and experimental results demonstrate the superiority of our approach over the or
    
[^20]: 数据驱动的物理信息神经网络：数字孪生的观点

    Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective. (arXiv:2401.08667v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2401.08667](http://arxiv.org/abs/2401.08667)

    该论文研究了利用物理信息神经网络(PINNs)实现数字孪生(DT)的潜力。提出了适用于无网格框架的自适应采样方法，并验证了PINNs在参数化的Navier-Stokes方程中的可扩展性和多保真度的优势。

    

    本研究从不同角度探索了利用物理信息神经网络(PINNs)实现数字孪生(DT)的潜力。首先，研究了用于配点的各种自适应采样方法，以验证它们在无网格框架的PINNs中的有效性，该框架允许自动构建虚拟表示，无需手动生成网格。然后，检验了数据驱动的PINNs(DD-PINNs)框架的整体性能，该框架可以利用在DT场景中获得的数据集。其对参数化的Navier-Stokes方程的更一般物理性的可扩展性得到了验证，其中PINNs在雷诺数变化时无需重新训练。此外，由于实际上数据集经常可以在不同的保真度/稀疏度下收集，还提出并评估了多保真度的DD-PINNs。它们在外推任务中表现出了显著的预测性能，相对于单保真度方法提高了42％到62％。

    This study explores the potential of physics-informed neural networks (PINNs) for the realization of digital twins (DT) from various perspectives. First, various adaptive sampling approaches for collocation points are investigated to verify their effectiveness in the mesh-free framework of PINNs, which allows automated construction of virtual representation without manual mesh generation. Then, the overall performance of the data-driven PINNs (DD-PINNs) framework is examined, which can utilize the acquired datasets in DT scenarios. Its scalability to more general physics is validated within parametric Navier-Stokes equations, where PINNs do not need to be retrained as the Reynolds number varies. In addition, since datasets can be often collected from different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also proposed and evaluated. They show remarkable prediction performance even in the extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity approach.
    
[^21]: 大型语言模型的自我解释是否可靠?

    Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2401.07927](http://arxiv.org/abs/2401.07927)

    大型语言模型的自我解释是否可靠是一个重要的AI安全考虑因素，我们提出使用自洽性检测作为评估其可靠性和解释能力的方法。

    

    经过训练的大型语言模型在许多任务上表现出色，甚至能够提供其行为的解释。由于这些模型对公众是直接可访问的，因此存在这样的风险，即令人信服但错误的解释可能导致对大型语言模型的无支撑的自信。因此，解释能力和可靠性是AI安全的重要考虑因素。评估自我解释的可靠性和可解释性是一项具有挑战性的任务，因为这些模型对于人类来说过于复杂，无法注释什么是正确的解释。为了解决这个问题，我们提出使用自洽性检测作为可靠性的衡量指标。例如，如果一个大型语言模型说某组词对于做出预测很重要，那么在没有这些词的情况下，它应该无法做出相同的预测。虽然自洽性检测是一种常见的可靠性方法，但之前尚未应用于大型语言模型的自我解释中。我们将自洽性检测应用于...

    Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
    
[^22]: 输入凸性Lipschitz RNN: 一种用于工程任务的快速和鲁棒的方法

    Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering Tasks. (arXiv:2401.07494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.07494](http://arxiv.org/abs/2401.07494)

    通过结合输入凸性和Lipschitz连续性的优势，我们开发了一种名为输入凸性Lipschitz循环神经网络的新型网络结构，在计算效率和对抗鲁棒性方面优于现有的循环单元，并适用于多种工程任务。

    

    计算效率和对抗鲁棒性是真实世界工程应用中的关键因素。然而，传统的神经网络往往在同时或分别解决这两个问题方面存在不足。通过从自然物理系统和现有文献中获取的见解，已知输入凸性结构增强了计算效率，而Lipschitz约束结构增强了对抗鲁棒性。通过利用凸性和Lipschitz连续性的优点，我们开发了一种新的网络结构，称为输入凸性Lipschitz循环神经网络。该模型在计算效率和对抗鲁棒性方面表现优于现有的循环单元，适用于一系列工程任务，包括基准MNIST图像分类、新加坡LHT Holdings公司的实际太阳能光伏系统规划中的实时太阳辐射预测，以及化学反应器的实时模型预测控制优化等。

    Computational efficiency and adversarial robustness are critical factors in real-world engineering applications. Yet, conventional neural networks often fall short in addressing both simultaneously, or even separately. Drawing insights from natural physical systems and existing literature, it is known that an input convex architecture enhances computational efficiency, while a Lipschitz-constrained architecture bolsters adversarial robustness. By leveraging the strengths of convexity and Lipschitz continuity, we develop a novel network architecture, termed Input Convex Lipschitz Recurrent Neural Networks. This model outperforms existing recurrent units across a spectrum of engineering tasks in terms of computational efficiency and adversarial robustness. These tasks encompass a benchmark MNIST image classification, real-world solar irradiance prediction for Solar PV system planning at LHT Holdings in Singapore, and real-time Model Predictive Control optimization for a chemical reactor.
    
[^23]: GIST: 在深度学习中生成输入集合的可迁移性

    GIST: Generated Inputs Sets Transferability in Deep Learning. (arXiv:2311.00801v1 [cs.LG])

    [http://arxiv.org/abs/2311.00801](http://arxiv.org/abs/2311.00801)

    这篇论文介绍了一种在深度学习模型之间高效迁移测试集的新方法，通过选择具有用户感兴趣的属性的良好测试集，以达到改善可验证性和测试性的目的。

    

    随着对神经网络可验证性和可测试性的需求不断增加，越来越多的生成测试集的方法被开发出来。然而，这些技术中的每一种都倾向于强调特定的测试方面，并且可能非常耗时。缓解这个问题的一个简单解决方案是根据希望迁移的期望属性，在一些经过基准测试的模型和新测试模型之间转移测试集。本文介绍了GIST（生成输入集合的可迁移性），一种用于在深度学习模型之间高效迁移测试集的新方法。给定用户希望迁移的一个感兴趣的属性（例如，覆盖准则），GIST能够从基准提供的可用测试集中，从该属性的角度选择良好的测试集。我们通过两种模态和不同的测试集生成过程，在故障类型覆盖属性上对GIST进行经验评估，以证明该方法的可行性。

    As the demand for verifiability and testability of neural networks continues to rise, an increasing number of methods for generating test sets are being developed. However, each of these techniques tends to emphasize specific testing aspects and can be quite time-consuming. A straightforward solution to mitigate this issue is to transfer test sets between some benchmarked models and a new model under test, based on a desirable property one wishes to transfer. This paper introduces GIST (Generated Inputs Sets Transferability), a novel approach for the efficient transfer of test sets among Deep Learning models. Given a property of interest that a user wishes to transfer (e.g., coverage criterion), GIST enables the selection of good test sets from the point of view of this property among available ones from a benchmark. We empirically evaluate GIST on fault types coverage property with two modalities and different test set generation procedures to demonstrate the approach's feasibility. E
    
[^24]: 在Wasserstein空间中通过近端梯度下降实现基于流的生成模型的收敛性

    Convergence of flow-based generative models via proximal gradient descent in Wasserstein space. (arXiv:2310.17582v1 [stat.ML])

    [http://arxiv.org/abs/2310.17582](http://arxiv.org/abs/2310.17582)

    本文通过在Wasserstein空间中应用近端梯度下降，证明了基于流的生成模型的收敛性，并提供了生成数据分布的理论保证。

    

    基于流的生成模型在计算数据生成和似然函数方面具有一定的优势，并且最近在实证表现上显示出竞争力。与相关基于分数扩散模型的积累理论研究相比，对于在正向（数据到噪声）和反向（噪声到数据）方向上都是确定性的流模型的分析还很少。本文通过在归一化流网络中实施Jordan-Kinderleherer-Otto（JKO）方案的所谓JKO流模型，提供了通过渐进流模型生成数据分布的理论保证。利用Wasserstein空间中近端梯度下降（GD）的指数收敛性，我们证明了通过JKO流模型生成数据的Kullback-Leibler（KL）保证为$O(\varepsilon^2)$，其中使用$N \lesssim \log (1/\varepsilon)$个JKO步骤（流中的$N$个残差块），其中$\varepsilon$是每步一阶条件的误差。

    Flow-based generative models enjoy certain advantages in computing the data generation and the likelihood, and have recently shown competitive empirical performance. Compared to the accumulating theoretical studies on related score-based diffusion models, analysis of flow-based models, which are deterministic in both forward (data-to-noise) and reverse (noise-to-data) directions, remain sparse. In this paper, we provide a theoretical guarantee of generating data distribution by a progressive flow model, the so-called JKO flow model, which implements the Jordan-Kinderleherer-Otto (JKO) scheme in a normalizing flow network. Leveraging the exponential convergence of the proximal gradient descent (GD) in Wasserstein space, we prove the Kullback-Leibler (KL) guarantee of data generation by a JKO flow model to be $O(\varepsilon^2)$ when using $N \lesssim \log (1/\varepsilon)$ many JKO steps ($N$ Residual Blocks in the flow) where $\varepsilon $ is the error in the per-step first-order condit
    
[^25]: 智能制造过程中的拓扑数据分析--关于现状的一项调查

    Topological Data Analysis in smart manufacturing processes -- A survey on the state of the art. (arXiv:2310.09319v1 [cs.LG])

    [http://arxiv.org/abs/2310.09319](http://arxiv.org/abs/2310.09319)

    本次调查总结了拓扑数据分析在智能制造和产业4.0背景下的最新进展，突出了其在工业生产和制造领域的关键优势和挑战，并讨论了未充分利用的TDA方法和已识别的应用类型，以推动更多的相关研究。

    

    拓扑数据分析（TDA）是一种使用拓扑学技术对复杂的多维数据进行分析的数学方法，已经在医学、材料科学、生物学等多个领域被广泛而成功地应用。本调查总结了TDA在另一个应用领域中的最新进展：工业制造和产业4.0背景下的生产。我们对工业生产和制造领域中TDA应用进行了严谨可重复的文献搜索。通过对结果进行聚类和分析，基于其在制造过程中的应用领域和输入数据类型进行论述。我们突出了TDA在这一领域的关键优势及其工具，并描述了它的挑战以及未来的潜力。最后，我们讨论了在（特定领域的）工业中未充分利用的TDA方法和已识别的应用类型，旨在促进更多的研究在当前领域中的开展。

    Topological Data Analysis (TDA) is a mathematical method using techniques from topology for the analysis of complex, multi-dimensional data that has been widely and successfully applied in several fields such as medicine, material science, biology, and others. This survey summarizes the state of the art of TDA in yet another application area: industrial manufacturing and production in the context of Industry 4.0. We perform a rigorous and reproducible literature search of applications of TDA on the setting of industrial production and manufacturing. The resulting works are clustered and analyzed based on their application area within the manufacturing process and their input data type. We highlight the key benefits of TDA and their tools in this area and describe its challenges, as well as future potential. Finally, we discuss which TDA methods are underutilized in (the specific area of) industry and the identified types of application, with the goal of prompting more research in this 
    
[^26]: 使用LM模拟沙盒识别LM代理的风险

    Identifying the Risks of LM Agents with an LM-Emulated Sandbox. (arXiv:2309.15817v1 [cs.AI])

    [http://arxiv.org/abs/2309.15817](http://arxiv.org/abs/2309.15817)

    通过使用LM模拟工具执行和开发基于LM的自动安全评估器，该论文提出了一种解决测试LM代理的高成本和寻找高风险问题的方法。

    

    最近的语言模型（LM）代理和工具使用的技术进步，例如ChatGPT插件，使得代理具备了丰富的功能，但也放大了潜在的风险，如泄露私人数据或引发财务损失。识别这些风险是一项耗时的工作，需要实施工具，手动设置每个测试场景的环境，并找到风险案例。随着工具和代理变得越来越复杂，测试这些代理的高成本将使寻找高风险、长尾风险变得越来越困难。为了解决这些挑战，我们引入了ToolEmu：一个使用LM来模拟工具执行的框架，可以在不需要手动实例化的情况下对LM代理进行各种工具和场景的测试。除了模拟器，我们还开发了一个基于LM的自动安全评估器，用于检查代理的失败并量化相关风险。我们通过人工评估测试了工具模拟器和评估器，并发现了6个...

    Recent advances in Language Model (LM) agents and tool use, exemplified by applications like ChatGPT Plugins, enable a rich set of capabilities but also amplify potential risks - such as leaking private data or causing financial losses. Identifying these risks is labor-intensive, necessitating implementing the tools, manually setting up the environment for each test scenario, and finding risky cases. As tools and agents become more complex, the high cost of testing these agents will make it increasingly difficult to find high-stakes, long-tailed risks. To address these challenges, we introduce ToolEmu: a framework that uses an LM to emulate tool execution and enables the testing of LM agents against a diverse range of tools and scenarios, without manual instantiation. Alongside the emulator, we develop an LM-based automatic safety evaluator that examines agent failures and quantifies associated risks. We test both the tool emulator and evaluator through human evaluation and find that 6
    
[^27]: 大型语言模型能够准确预测搜索者的偏好

    Large language models can accurately predict searcher preferences. (arXiv:2309.10621v1 [cs.IR])

    [http://arxiv.org/abs/2309.10621](http://arxiv.org/abs/2309.10621)

    大型语言模型可以通过从真实用户那里获取高质量的第一方数据来准确预测搜索者的偏好。

    

    相关性标签是评估和优化搜索系统的关键。获取大量相关性标签通常需要第三方标注人员，但存在低质量数据的风险。本论文介绍了一种改进标签质量的替代方法，通过从真实用户那里获得仔细反馈来获取高质量的第一方数据。

    Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that 
    
[^28]: 通过有符号梯度下降优化LLMs量化中的权重舍入

    Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs. (arXiv:2309.05516v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.05516](http://arxiv.org/abs/2309.05516)

    本文提出一种名为SignRound的优化权重舍入的方法，通过使用有符号梯度进行轻量级分块调整，解决了大型语言模型(LLMs)的量化挑战。

    

    大型语言模型(LLMs)在执行语言相关任务方面表现出了非凡的能力。然而，由于其巨大的内存和存储需求，它们的部署面临着重大挑战。为了解决这个问题，仅针对权重的量化，特别是3位和4位仅针对权重的量化，已经成为最可行的解决方案之一。随着位数的减少，量化网格变得更加宽泛，从而强调了上下舍入的重要性。尽管先前的研究表明，在某些情况下，通过添加扰动细调上下舍入可以提高准确性，但我们的研究受制于这些扰动的精确且有限的边界，只有改变舍入值的阈值才具有重要性。因此，我们提出了一种简洁高效的优化权重舍入任务的方法。我们的方法名为SignRound，它涉及使用有符号梯度的轻量级分块调整。

    Large Language Models (LLMs) have proven their exceptional capabilities in performing language-related tasks. However, their deployment poses significant challenges due to their considerable memory and storage requirements. In response to this issue, weight-only quantization, particularly 3 and 4-bit weight-only quantization, has emerged as one of the most viable solutions. As the number of bits decreases, the quantization grid broadens, thus emphasizing the importance of up and down rounding. While previous studies have demonstrated that fine-tuning up and down rounding with the addition of perturbations can enhance accuracy in some scenarios, our study is driven by the precise and limited boundary of these perturbations, where only the threshold for altering the rounding value is of significance. Consequently, we propose a concise and highly effective approach for optimizing the weight rounding task. Our method, named SignRound, involves lightweight block-wise tuning using signed gra
    
[^29]: 在随机网络上的在线分布式学习

    Online Distributed Learning over Random Networks. (arXiv:2309.00520v1 [math.OC])

    [http://arxiv.org/abs/2309.00520](http://arxiv.org/abs/2309.00520)

    本文提出了在随机网络上进行在线分布式学习的DOT-ADMM算法，通过解决在线学习、异步计算、不可靠通信和不精确计算等挑战，在一大类凸学习问题上获得了线性收敛速度。

    

    最近，在各种场景中部署的多智能体系统使得在分布式环境下解决学习问题成为可能。在这种情况下，智能体的任务是收集本地数据，然后合作训练模型，而不直接共享数据。虽然分布式学习在保护智能体隐私方面具有优势，但在设计和分析适当的算法方面也存在一些挑战。本文特别关注以下由实际实施所驱动的挑战：（i）在线学习，其中本地数据随时间变化；（ii）异步智能体计算；（iii）不可靠和有限的通信；（iv）不精确的本地计算。为了应对这些挑战，我们介绍了分布式操作理论（DOT）版本的交替方向乘子法（ADMM），称之为DOT-ADMM算法。我们证明了它在大类凸学习问题上具有线性收敛速度。

    The recent deployment of multi-agent systems in a wide range of scenarios has enabled the solution of learning problems in a distributed fashion. In this context, agents are tasked with collecting local data and then cooperatively train a model, without directly sharing the data. While distributed learning offers the advantage of preserving agents' privacy, it also poses several challenges in terms of designing and analyzing suitable algorithms. This work focuses specifically on the following challenges motivated by practical implementation: (i) online learning, where the local data change over time; (ii) asynchronous agent computations; (iii) unreliable and limited communications; and (iv) inexact local computations. To tackle these challenges, we introduce the Distributed Operator Theoretical (DOT) version of the Alternating Direction Method of Multipliers (ADMM), which we call the DOT-ADMM Algorithm. We prove that it converges with a linear rate for a large class of convex learning 
    
[^30]: AnyTeleop: 通用视觉导向的灵巧机械臂手操作系统

    AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System. (arXiv:2307.04577v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.04577](http://arxiv.org/abs/2307.04577)

    AnyTeleop是一个通用的视觉导向的远程操作系统，支持多个不同的机械臂、手部、环境和摄像头配置，在实际实验和模拟中表现出色。

    

    基于视觉的远程操作为机器人提供了与环境进行物理交互的可能性，同时只需要低成本的摄像头传感器。然而，目前的基于视觉的远程操作系统是针对特定机器人模型和部署环境进行设计和工程化的，随着机器人模型的增加和操作环境的多样化，其扩展性较差。本文提出了AnyTeleop，一个统一和通用的远程操作系统，支持在单个系统中使用多个不同的机械臂、手部、现实环境和摄像头配置。尽管设计具有选择模拟器和真实硬件的灵活性，我们的系统仍然可以实现出色的性能。在实际实验中，AnyTeleop可以以更高的成功率击败为特定机器人硬件设计的以前系统，使用相同的机器人。在模拟中进行远程操作时，AnyTeleop可以实现更好的效果。

    Vision-based teleoperation offers the possibility to endow robots with human-level intelligence to physically interact with the environment, while only requiring low-cost camera sensors. However, current vision-based teleoperation systems are designed and engineered towards a particular robot model and deploy environment, which scales poorly as the pool of the robot models expands and the variety of the operating environment increases. In this paper, we propose AnyTeleop, a unified and general teleoperation system to support multiple different arms, hands, realities, and camera configurations within a single system. Although being designed to provide great flexibility to the choice of simulators and real hardware, our system can still achieve great performance. For real-world experiments, AnyTeleop can outperform a previous system that was designed for a specific robot hardware with a higher success rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to better 
    
[^31]: 利用对比学习预测癌症预后的基因表达值

    Contrastive Learning for Predicting Cancer Prognosis Using Gene Expression Values. (arXiv:2306.06276v1 [cs.LG])

    [http://arxiv.org/abs/2306.06276](http://arxiv.org/abs/2306.06276)

    该论文提出应用对比学习方法，从有限的数据样本中学习基因表达数据的良好特征表示，并应用于Cox模型中，可以显著提高癌症预后的预测性能。

    

    最近开发了多种人工神经网络（ANNs）作为Cox比例风险模型，以基于肿瘤转录组预测癌症预后。然而，它们并没有表现出比传统有规则化的Cox回归显着更好的性能。在有限数量的数据样本和高维特征空间存在的情况下，训练具有高预测能力的ANN是具有挑战性的。图像分类的最新进展表明，对比学习可以通过从有限数量的数据样本中学习良好的特征表示来促进进一步的学习任务。在本文中，我们将监督对比学习应用于肿瘤基因表达和临床数据，以在低维空间中学习特征表示。然后，我们使用这些学习到的特征来训练Cox模型，以预测癌症预后。使用来自The Cancer Genome Atlas（TCGA）的数据，我们证明了我们基于对比学习的Cox模型能够通过学习基因表达数据的良好特征表征，显著提高癌症预后预测的性能。

    Several artificial neural networks (ANNs) have recently been developed as the Cox proportional hazard model for predicting cancer prognosis based on tumor transcriptome. However, they have not demonstrated significantly better performance than the traditional Cox regression with regularization. Training an ANN with high prediction power is challenging in the presence of a limited number of data samples and a high-dimensional feature space. Recent advancements in image classification have shown that contrastive learning can facilitate further learning tasks by learning good feature representation from a limited number of data samples. In this paper, we applied supervised contrastive learning to tumor gene expression and clinical data to learn feature representations in a low-dimensional space. We then used these learned features to train the Cox model for predicting cancer prognosis. Using data from The Cancer Genome Atlas (TCGA), we demonstrated that our contrastive learning-based Cox 
    
[^32]: C(NN)FD - 一种用于涡轮机械CFD分析的深度学习框架

    C(NN)FD -- a deep learning framework for turbomachinery CFD analysis. (arXiv:2306.05889v1 [cs.LG])

    [http://arxiv.org/abs/2306.05889](http://arxiv.org/abs/2306.05889)

    本文介绍了一种新型深度学习框架C(NN)FD，用于实时预测燃气轮机中轴向压缩机制造和组装变化对整体性能的影响。该框架可过滤掉CFD解决方案中的相关部分，因此具有可扩展性，且实时精度可与CFD基准相媲美。

    

    深度学习方法已经在不同工业领域得到广泛应用。迄今为止，对于诸如CFD（计算流体力学）的物理模拟的应用仅限于工业相关性较小的简单测试案例。本文展示了一种新型深度学习框架的开发，用于实时预测燃气轮机中轴向压缩机制造和组装变化对整体性能的影响，重点关注于叶片间隙的变化。效率的散布可以显著增加$CO_2$排放量，因此具有重要的工业和环境意义。所提出的C(NN)FD架构实时精度可与CFD基准相媲美。预测流场并使用其计算相应的整体性能使该方法具有普适性，而仅过滤CFD解决方案的相关部分使该方法可扩展到实际应用中。

    Deep Learning methods have seen a wide range of successful applications across different industries. Up until now, applications to physical simulations such as CFD (Computational Fluid Dynamics), have been limited to simple test-cases of minor industrial relevance. This paper demonstrates the development of a novel deep learning framework for real-time predictions of the impact of manufacturing and build variations on the overall performance of axial compressors in gas turbines, with a focus on tip clearance variations. The associated scatter in efficiency can significantly increase the $CO_2$ emissions, thus being of great industrial and environmental relevance. The proposed \textit{C(NN)FD} architecture achieves in real-time accuracy comparable to the CFD benchmark. Predicting the flow field and using it to calculate the corresponding overall performance renders the methodology generalisable, while filtering only relevant parts of the CFD solution makes the methodology scalable to in
    
[^33]: 稳定性、泛化性和隐私保护：对于随机特征和NTK特征的精确分析

    Stability, Generalization and Privacy: Precise Analysis for Random and NTK Features. (arXiv:2305.12100v1 [stat.ML])

    [http://arxiv.org/abs/2305.12100](http://arxiv.org/abs/2305.12100)

    本论文研究了ERM训练模型对抗强大黑盒攻击的安全问题，并通过两个指标量化模型安全性：单个样本的稳定性和查询与原始数据特征的对齐。在研究中，通过研究RF和NTK回归，证明随着泛化能力的提高，隐私保护可以得到加强。

    

    深度学习模型容易受到恢复攻击，引起用户隐私保护的担忧。针对经验风险最小化（ERM）等常见算法通常不能直接实施安全保障的问题，本文研究了ERM训练模型对抗特定强大黑盒子攻击的安全问题。我们的分析通过两个看似不同但有联系的指标来量化模型安全性：一是相对于单个训练样本的模型稳定性，另一个是攻击查询和原始数据特征的特征对齐。虽然前者在学习理论中已经得到了很好的阐述，并与经典工作中的泛化误差相关，但在我们的研究中，第二种特性是新颖的。我们的关键技术结果为两种原型设置提供了特征对齐的精确刻画：随机特征（RF）和神经切向核（NTK）回归。这证明，随着泛化能力的提高，隐私保护能够得到加强，同时揭示了其他有趣的性质。

    Deep learning models can be vulnerable to recovery attacks, raising privacy concerns to users, and widespread algorithms such as empirical risk minimization (ERM) often do not directly enforce safety guarantees. In this paper, we study the safety of ERM-trained models against a family of powerful black-box attacks. Our analysis quantifies this safety via two separate terms: (i) the model stability with respect to individual training samples, and (ii) the feature alignment between the attacker query and the original data. While the first term is well established in learning theory and it is connected to the generalization error in classical work, the second one is, to the best of our knowledge, novel. Our key technical result provides a precise characterization of the feature alignment for the two prototypical settings of random features (RF) and neural tangent kernel (NTK) regression. This proves that privacy strengthens with an increase in the generalization capability, unveiling also
    
[^34]: ACRoBat：在编译时优化动态深度学习的自动批处理

    ACRoBat: Optimizing Auto-batching of Dynamic Deep Learning at Compile Time. (arXiv:2305.10611v1 [cs.LG])

    [http://arxiv.org/abs/2305.10611](http://arxiv.org/abs/2305.10611)

    ACRoBat是一种动态深度学习的自动批处理框架，在编译时进行混合静态+动态编译器优化和端到端张量代码生成，可将性能提高多达8.5倍。

    

    动态控制流是一种重要的技术，常用于设计表达力强和高效的深度学习计算，例如文本解析、机器翻译、提前退出深度模型等。然而，由此产生的控制流分叉使得批处理难以手动执行，这是一种重要的性能优化。本文提出了 ACRoBat 框架，通过执行混合静态+动态编译器优化和端到端张量代码生成，实现了动态深度学习计算的高效自动批处理。在 NVIDIA GeForce RTX 3070 GPU 上，ACRoBat 的性能比现有的自动批处理框架 DyNet 提高了多达 8.5 倍。

    Dynamic control flow is an important technique often used to design expressive and efficient deep learning computations for applications such as text parsing, machine translation, exiting early out of deep models and so on. However, the resulting control flow divergence makes batching, an important performance optimization, difficult to perform manually. In this paper, we present ACRoBat, a framework that enables efficient automatic batching for dynamic deep learning computations by performing hybrid static+dynamic compiler optimizations and end-to-end tensor code generation. ACRoBat performs up to 8.5X better than DyNet, a state-of-the-art framework for automatic batching, on an Nvidia GeForce RTX 3070 GPU.
    
[^35]: 节点特征增强改进网络对齐

    Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])

    [http://arxiv.org/abs/2304.12751](http://arxiv.org/abs/2304.12751)

    本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。

    

    网络对齐（NA）是通过给定网络的拓扑和/或特征信息来发现多个网络之间的节点对应关系的任务。虽然NA方法在各种场景下取得了显著的成功，但其有效性并不总是有额外信息，如先前的锚点链接和/或节点特征。为了解决这个实际的挑战，我们提出了Grad-Align+，这是一种新颖的NA方法，建立在最近一种最先进的NA方法Grad-Align之上，Grad-Align+仅逐步发现部分节点对，直到找到所有节点对。在设计Grad-Align+时，我们考虑如何通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法。为了实现这个目标，我们开发了由三个关键组成部分组成的Grad-Align+：基于中心性的节点特征增强（CNFA）、图切片生成和优化节点嵌入特征（ONIFE）。

    Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
    
[^36]: 统一悲观风险和最优组合

    Uniform Pessimistic Risk and Optimal Portfolio. (arXiv:2303.07158v1 [q-fin.PM])

    [http://arxiv.org/abs/2303.07158](http://arxiv.org/abs/2303.07158)

    本文提出了一种称为统一悲观风险的综合$\alpha$-风险版本和基于风险获得最优组合的计算算法，该方法可以用于估计韩国股票的悲观最优组合模型。

    This paper proposes a version of integrated $\alpha$-risk called the uniform pessimistic risk and a computational algorithm to obtain an optimal portfolio based on the risk. The proposed method can be used to estimate the pessimistic optimal portfolio models for Korean stocks.

    资产配置的最优性已经在风险度量的理论分析中得到广泛讨论。悲观主义是一种超越传统最优组合模型的最有吸引力的方法之一，$\alpha$-风险在推导出广泛的悲观最优组合中起着关键作用。然而，由悲观风险评估的最优组合的估计仍然具有挑战性，因为缺乏可用的估计模型和计算算法。在本研究中，我们提出了一种称为统一悲观风险的综合$\alpha$-风险版本和基于风险获得最优组合的计算算法。此外，我们从多个分位数回归、适当的评分规则和分布鲁棒优化三个不同的方法来研究所提出的风险的理论性质。同时，统一悲观风险被应用于估计韩国股票的悲观最优组合模型。

    The optimality of allocating assets has been widely discussed with the theoretical analysis of risk measures. Pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model, and the $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of an available estimation model and a computational algorithm. In this study, we propose a version of integrated $\alpha$-risk called the uniform pessimistic risk and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Also, the uniform pessimistic risk is applied to estimate the pessimistic optimal portfolio models for the Korean stock 
    
[^37]: ScionFL: 高效且强健的安全量化聚合

    ScionFL: Efficient and Robust Secure Quantized Aggregation. (arXiv:2210.07376v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2210.07376](http://arxiv.org/abs/2210.07376)

    本文介绍了ScionFL，这是第一个在联邦学习中高效运行在量化输入上并同时提供对恶意客户端强健性的安全聚合框架。

    

    安全聚合在联邦学习中被广泛应用，以减轻与中央聚合器看到所有参数更新有关的隐私问题。不幸的是，大多数现有的安全聚合方案忽略了两个关键的正交研究方向，旨在（i）显着减少客户端-服务器通信和（ii）减轻恶意客户端的影响。然而，这两个额外的属性对于实现跨设备的联邦学习与成千上万甚至百万（移动）参与者至关重要。在本文中，我们通过引入ScionFL，联合了两个研究方向，这是FL的第一个安全聚合框架，可以在量化输入上有效运行，同时提供对恶意客户端的强健性。我们的框架利用（新颖的）多方计算（MPC）技术，并支持多个线性（1比特）量化方案，包括利用随机哈达玛变换和卡申表示的方案。

    Secure aggregation is commonly used in federated learning (FL) to alleviate privacy concerns related to the central aggregator seeing all parameter updates in the clear. Unfortunately, most existing secure aggregation schemes ignore two critical orthogonal research directions that aim to (i) significantly reduce client-server communication and (ii) mitigate the impact of malicious clients. However, both of these additional properties are essential to facilitate cross-device FL with thousands or even millions of (mobile) participants. In this paper, we unite both research directions by introducing ScionFL, the first secure aggregation framework for FL that operates efficiently on quantized inputs and simultaneously provides robustness against malicious clients. Our framework leverages (novel) multi-party computation (MPC) techniques and supports multiple linear (1-bit) quantization schemes, including ones that utilize the randomized Hadamard transform and Kashin's representation. Our th
    
[^38]: 用于复杂系统模拟的有效学习准确替代模型的方法

    Efficient Learning of Accurate Surrogates for Simulations of Complex Systems. (arXiv:2207.12855v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12855](http://arxiv.org/abs/2207.12855)

    本论文提出了一种在线学习方法，由优化器驱动的采样方法，该方法可以提高替代模型的预测能力，并构建了一个比目前使用的替代模型更准确的替代模型，为复杂系统模拟提供了更加计算上高效的替代模型方法。

    

    机器学习方法越来越被用于构建计算上廉价的复杂物理模型的替代模型。当数据不稳定、稀疏或时间相关时，这些替代模型的预测能力会下降。为了找到一种可以提供任何潜在未来模型评估的有效预测的替代模型，我们介绍了一种由优化器驱动的在线学习方法。该方法有两个优点，首先它确保在训练数据中包含模型响应曲面上的所有拐点。其次，在任何新的模型评估之后，如果“得分”下降到下限，则会进行机器学习模型的“重新训练”（更新）。在基准函数上的测试表明，与传统的采样方法相比，由优化器引导的采样通常在局部极值附近的准确性方面表现优越，即使评分指标支持整体准确性。我们将该方法应用于核物质的模拟中，以证明它处理嘈杂、稀疏和时间相关的数据的能力。我们建立的替代模型比同样数量的训练数据构建的目前使用的替代模型更准确。我们的在线方法提供了一种计算上高效的替代模型，而传统的替代模型则需要大量的训练数据。

    Machine learning methods are increasingly used to build computationally inexpensive surrogates for complex physical models. The predictive capability of these surrogates suffers when data are noisy, sparse, or time-dependent. As we are interested in finding a surrogate that provides valid predictions of any potential future model evaluations, we introduce an online learning method empowered by optimizer-driven sampling. The method has two advantages over current approaches. First, it ensures that all turning points on the model response surface are included in the training data. Second, after any new model evaluations, surrogates are tested and "retrained" (updated) if the "score" drops below a validity threshold. Tests on benchmark functions reveal that optimizer-directed sampling generally outperforms traditional sampling methods in terms of accuracy around local extrema, even when the scoring metric favors overall accuracy. We apply our method to simulations of nuclear matter to dem
    

