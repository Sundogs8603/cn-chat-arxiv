# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning from Hypervectors: A Survey on Hypervector Encoding.](http://arxiv.org/abs/2308.00685) | 本调查研究聚焦于超维计算系统的输入和超维向量的生成，探讨了各种方法的限制、挑战和潜在好处。 |
| [^2] | [CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code.](http://arxiv.org/abs/2308.00683) | CodeBPE研究了用于源代码的大型语言模型预训练中不同子标记化选项的影响，找出了最有效和长度高效的子标记化方法，通过减少平均长度17%且不影响下游性能，可能提高质量0.5-2%。 |
| [^3] | [Discrete neural nets and polymorphic learning.](http://arxiv.org/abs/2308.00677) | 本文提出了一种基于离散模拟的神经网络，将Murski{i}的通用代数定理和Cybenko的通用逼近结果统一起来。同时，通过引入关系结构的多态性，我们提出了一种学习算法，用于经典学习任务。 |
| [^4] | [Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models.](http://arxiv.org/abs/2308.00675) | 这项工作提出了使用工具文档作为教导大型语言模型使用新工具的替代方法，并通过实证研究证明，仅使用工具文档的零-shot提示足以实现正确的工具使用。 |
| [^5] | [Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression.](http://arxiv.org/abs/2308.00672) | 本文研究了遗传编程中的主动学习方法，通过利用模型群体和不确定性度量来选择信息丰富的训练数据点，并发现微分熵是最好的不确定性度量。同时，通过将相关性作为多样性度量，可以更好地选择数据点进行训练。最后，通过帕累托优化方法综合考虑不确定性和多样性，实现了平衡地选择训练数据点的目的。 |
| [^6] | [Hessian-Aware Bayesian Optimization for Decision Making Systems.](http://arxiv.org/abs/2308.00629) | 本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。 |
| [^7] | [Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes.](http://arxiv.org/abs/2308.00628) | 这篇论文提出了一个室外多模态多视角多人类姿势数据库Human-M3，并介绍了一种基于多模态数据输入的算法来生成准确的人体姿势。这个数据库解决了现有数据集的不足，提供了更多的数据多样性。 |
| [^8] | [Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers.](http://arxiv.org/abs/2308.00607) | 本文探索了将本体和语义知识反映到图像分类器中的方法，提高了模型的可解释性和可信度。 |
| [^9] | [Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel.](http://arxiv.org/abs/2308.00583) | 使用量子核的支持向量回归方法被应用于半监督异常检测中，针对NISQ设备的限制，该方法具有理论保证、灵活性和兼容性。 |
| [^10] | [Predicting masked tokens in stochastic locations improves masked image modeling.](http://arxiv.org/abs/2308.00566) | 本论文提出了一种名为FlexPredict的随机模型，通过在模型中加入位置不确定性，以预测掩盖的标记位置，从而改善了掩盖图像建模的性能。 |
| [^11] | [Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms.](http://arxiv.org/abs/2308.00556) | 本文研究了测试时的对抗攻击对线性回归模型的影响，并确定了在保持预测性能的情况下可以达到的最佳鲁棒性水平。该研究揭示了鲁棒性和准确性之间的权衡，并在不损害准确性的情况下找到了鲁棒性的实现方法。 |
| [^12] | [Copula for Instance-wise Feature Selection and Ranking.](http://arxiv.org/abs/2308.00549) | 在实例级特征选择和排序中，我们提出了一种使用Copula的方法，能够更好地考虑特征之间的相关性，实验结果证明我们的方法能够捕捉有意义的相关性。 |
| [^13] | [Predicting Early Dropouts of an Active and Healthy Ageing App.](http://arxiv.org/abs/2308.00539) | 本文提出了一种机器学习方法，用于预测积极健康老龄化应用程序的早期退学。通过使用动态和静态特征构建分类模型以及采用过采样方法提高分类性能，我们获得了高质量的粘附度预测，并在科学挑战赛中获得了第一名。 |
| [^14] | [Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies.](http://arxiv.org/abs/2308.00537) | 提出了一种基于图嵌入动态特征的瞬态稳定性GEDF-SCL模型，利用监督对比学习结合GEDF来预测电力系统的瞬态稳定性，并考虑了拓扑信息。实验结果表明该模型具有良好的性能。 |
| [^15] | [Graph Contrastive Learning with Generative Adversarial Network.](http://arxiv.org/abs/2308.00535) | 该论文提出了一种利用生成对抗网络（GANs）进行图形对比学习的方法，通过学习视图的分布来提高图神经网络（GNN）的性能。 |
| [^16] | [A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction.](http://arxiv.org/abs/2308.00533) | 本文提出了一种新颖的时序多门专家混合模型（TMMOE），以同时预测车辆轨迹和驾驶意图。该模型利用共享层和专家层进行特征提取和信息识别，实现了对纵向位置、横向位置和驾驶意图之间关系的综合预测。 |
| [^17] | [Variational Label-Correlation Enhancement for Congestion Prediction.](http://arxiv.org/abs/2308.00529) | 本文提出了一种名为VAriational Label-Correlation Enhancement for Congestion Prediction（{\ours}）的方法，用于准确预测集成电路中的拥塞情况。这种方法利用了格子之间的空间标签相关性，能够更好地识别设计缺陷，加快电路设计过程。 |
| [^18] | [Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup.](http://arxiv.org/abs/2308.00522) | 本论文提出了一种名为FedLADA的联邦局部自适应修正优化器，通过引入局部修正技术和动量项，解决了联邦学习中粗糙收敛和客户端漂移加剧的问题，具有线性加速度。 |
| [^19] | [Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer.](http://arxiv.org/abs/2308.00507) | 本文提出了一种新的方法来改善胰腺癌的预后预测。该方法使用了可学习的神经距离来描述肿瘤与血管之间的关系，并通过融合局部和全局特征来改进多相CT影像中的肿瘤纹理特征提取。实验表明，该方法在预后预测中取得了较好的效果。 |
| [^20] | [Explainable Graph Spectral Clustering of Text Documents.](http://arxiv.org/abs/2308.00504) | 本文提出了一种可解释的文本文档的图谱聚类方法，通过展示组合拉普拉斯嵌入、K嵌入和词向量空间嵌入之间的等价性，构建了文本内容和聚类结果之间的桥梁。 |
| [^21] | [DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification.](http://arxiv.org/abs/2308.00475) | DINO-CXR是一种基于视觉变换器的自监督方法，可用于胸部X射线分类，并通过比较分析显示出在肺炎和COVID-19检测方面的有效性和优越性，同时需要较少的标注数据。 |
| [^22] | [Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?.](http://arxiv.org/abs/2308.00473) | 通过对最后一层进行重新训练，Deep Feature Reweighting（DFR）方法可以提高模型在存在虚假相关性的数据中的准确性，但其应用于实际医学数据时存在一定局限性。 |
| [^23] | [Mirror Natural Evolution Strategies.](http://arxiv.org/abs/2308.00469) | 这篇论文研究了基于零阶查询近似一阶和二阶信息的零阶优化算法，提出了一种新的重参数化目标函数并给出了对应的最小化算法，名为MiNES（镜像下降自然进化策略）。 |
| [^24] | [A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models.](http://arxiv.org/abs/2308.00452) | 该论文提出了MajorCert，通过找到在底层分类器中由同一个补丁区域在同一个样本上可以操纵的所有可能标签集合，并检查它们的大多数不变性来对样本进行补丁稳健性认证。 |
| [^25] | [SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.](http://arxiv.org/abs/2308.00436) | 本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。 |
| [^26] | [Tackling Hallucinations in Neural Chart Summarization.](http://arxiv.org/abs/2308.00399) | 本研究解决了神经图表摘要中的幻觉问题，通过自然语言推理预处理训练数据，显著减少了幻觉的产生，并通过缩短远距离依赖关系和添加与图表相关的信息来提高整体性能。 |
| [^27] | [A Survey of Time Series Anomaly Detection Methods in the AIOps Domain.](http://arxiv.org/abs/2308.00393) | 在AIOps领域中，通过调查时间序列异常检测方法，研究人员综述了互联网服务中大量监测KPI的重要性，并讨论了未来的发展方向。 |
| [^28] | [Counterfactual Graph Transformer for Traffic Flow Prediction.](http://arxiv.org/abs/2308.00391) | 本研究提出了一种针对交通流量预测的反事实图变压器模型（CGT），通过实例级解释器和扰动掩码生成器，可以获得具有解释性的预测模型和重要子图。 |
| [^29] | [Shape Completion with Prediction of Uncertain Regions.](http://arxiv.org/abs/2308.00377) | 该论文提出了两种方法来处理在给定模糊物体视图时可能存在的物体部分的不确定区域预测问题。研究表明这些方法可以作为任何预测空间占用的方法的直接扩展，通过后处理占用评分或直接预测不确定性指标来实现。这些方法与已知的概率形状完成方法进行了比较，并使用自动生成的深度图像数据集进行了验证。 |
| [^30] | [Learning Green's Function Efficiently Using Low-Rank Approximations.](http://arxiv.org/abs/2308.00350) | 该论文提出了一种使用低秩分解学习Green函数的方法，通过使用领域数据和蒙特卡洛样本进行分别学习和积分逼近，从而提高了计算效率并保持了准确性。 |
| [^31] | [Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness.](http://arxiv.org/abs/2308.00346) | 本文针对深度神经网络在实际应用中的鲁棒性问题，提出了一种通过动态集成选择技术在模型层面上改善模型的防御主动性的方法。通过在训练阶段引入Dirichlet分布作为子模型预测分布的先验，并在轻量级子模型下引入参数空间的多样性约束，构建备选的集成模型空间。在测试阶段，动态选取特定的子模型来提高模型的鲁棒性。 |
| [^32] | [Monitoring Algorithmic Fairness under Partial Observations.](http://arxiv.org/abs/2308.00341) | 本研究将算法公平性监控扩展到部分观察到的马尔可夫链模型的系统，并且可以监控包含对事件序列上数值函数的期望值的算术表达式的公平性属性。 |
| [^33] | [Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs.](http://arxiv.org/abs/2308.00327) | 针对混合整数规划问题，本研究提出了一种阈值感知学习的方法，通过优化变量分配率来提高可行解的质量。 |
| [^34] | [Pixel to policy: DQN Encoders for within & cross-game reinforcement learning.](http://arxiv.org/abs/2308.00318) | 本文研究了在强化学习中利用共享结构的DQN编码器的性能。通过迁移学习，该模型在不同任务和环境中学习可转移的策略，实现了更高效的学习和更好的性能。在多个游戏环境中，该模型的平均回合奖励为46.16，在20,000次回合内甚至超过了人类水平表现。 |
| [^35] | [Doubly Robust Instance-Reweighted Adversarial Training.](http://arxiv.org/abs/2308.00311) | 本文提出了一种新颖的双重稳健的实例重新加权对抗训练框架，通过分布鲁棒优化（DRO）技术获得重要性权重，并在最脆弱的示例上提高稳健性。 |
| [^36] | [GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients.](http://arxiv.org/abs/2308.00310) | GradOrth是一种简单而高效的方法，通过计算梯度在内部分布数据的低秩子空间中的投影范数来检测外部分布数据。这种方法展示了出色的性能。 |
| [^37] | [A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation.](http://arxiv.org/abs/2308.00287) | 这项研究关注于无监督领域自适应中的评估度量。通过将源准确率纳入度量中，并使用一种新的MLP分类器进行改进，该研究解决了原有度量的不足之处，并将其与数据增强相结合，改进了模型的质量评估。 |
| [^38] | [Predictive Modeling through Hyper-Bayesian Optimization.](http://arxiv.org/abs/2308.00285) | 本文提出了一种将模型选择和贝叶斯优化相结合的方法，以更快地达到函数的最优解。 |
| [^39] | [CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering.](http://arxiv.org/abs/2308.00284) | 本研究提出一种名为CLAMS的聚类模糊度测量方法，用于估计视觉聚类中的感知变异性。通过定性研究，我们确定了影响聚类的关键因素，并通过回归模块对聚类的模糊度进行估计。 |
| [^40] | [ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings.](http://arxiv.org/abs/2308.00282) | ZADU是一个Python库，通过提供扭曲度量方法可以高效评估降维嵌入的可靠性，并通过优化执行和分析各个数据点的贡献提供全面评估。 |
| [^41] | [Data Collaboration Analysis applied to Compound Datasets and the Introduction of Projection data to Non-IID settings.](http://arxiv.org/abs/2308.00280) | 该论文介绍了应用于化合物数据集的数据协作分析，以及引入投影数据到非独立同分布环境中。这种方法通过数据合作和使用辅助数据的方式提高了预测准确性。 |
| [^42] | [Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction.](http://arxiv.org/abs/2308.00279) | 本论文提出了一种基于噪声负样本自校正的鲁棒正负样本学习方法，通过首先学习简单的案例来减轻标签的不确定性，提高学习的鲁棒性。 |
| [^43] | [Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction.](http://arxiv.org/abs/2308.00278) | 本文提出了两种新的质量度量方法--标签可信度和标签连续性（Label-T&C）--改进了基于类别标签的降维评估的过程，不再假设类别在原始空间中形成良好的聚类，而是通过估计类别在原始空间和嵌入空间中形成聚类的程度和评估两者之间的差异来工作。 |
| [^44] | [Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions.](http://arxiv.org/abs/2308.00273) | 本文提出了一种通用神经网络架构来近似对称和分量组不变的函数，并将其与素描思想结合起来构建了一个用于近似点集之间Wasserstein距离的具体且高效的神经网络模型。 |
| [^45] | [Multi-Modality Multi-Loss Fusion Network.](http://arxiv.org/abs/2308.00264) | 多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。 |
| [^46] | [Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation.](http://arxiv.org/abs/2308.00263) | 该论文介绍了一种名为QAFeL的新算法，通过建立服务器和客户端之间的共享“隐藏”状态，采用量化方案解决了异步联邦学习中高通信成本的问题。该算法能够在保持高精度的同时显著减少客户端-服务器交互过程中传输的数据量。 |
| [^47] | [AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients.](http://arxiv.org/abs/2308.00258) | AQUILA是一个自适应量化梯度的通信高效联邦学习框架，解决了传输大规模模型时的通信开销和局部数据偏差导致的全局模型鲁棒性问题。 |
| [^48] | [Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique.](http://arxiv.org/abs/2308.00251) | 本文提出了一种在高维广义线性模型中进行最佳子集选择的快速且一致的算法，该算法通过拼接技术实现了高确定性的最佳子集选择，并在变量选择和系数估计方面优于现有方法。 |
| [^49] | [EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning.](http://arxiv.org/abs/2308.00246) | 本文提出了一种基于EEG的认知负荷分类的新解决方案，利用特征掩蔽自编码和情绪迁移学习来进行模型训练和分类。实验结果表明，这种方法取得了良好的结果。 |
| [^50] | [Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks.](http://arxiv.org/abs/2308.00231) | Capsa是一个统一框架，用于扩展具有风险感知的深度神经网络模型。它能够量化多种形式的风险，并将不同算法组合在一起以并行量化不同的风险指标。通过在复杂感知数据集上实现最先进的不确定性估计算法并进行基准测试，验证了capsa的有效性。capsa能够轻松组合aleatoric不确定性、epistemic不确定性和偏见估计能力 |
| [^51] | [Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias.](http://arxiv.org/abs/2308.00225) | 这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。 |
| [^52] | [Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits.](http://arxiv.org/abs/2308.00218) | 本研究提出了一种基于深度强化学习的电池调控分层V2G协调策略，旨在促进可再生能源利用和电网稳定。该策略可以实现多方利益，并且考虑了电网、电动车聚合器和用户的各种因素。 |
| [^53] | [Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF).](http://arxiv.org/abs/2308.00214) | 在影像导引的微创医疗过程中，我们提出了新的方法，利用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。 |
| [^54] | [SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks.](http://arxiv.org/abs/2308.00206) | 使用SkullGAN，一种生成对抗网络（GAN），生成合成的颅骨CT图像，可以减少对真实图像的依赖，加速将机器学习应用于医疗保健领域的整合。 |
| [^55] | [CBCL-PR: A Cognitively Inspired Model for Class-Incremental Learning in Robotics.](http://arxiv.org/abs/2308.00199) | 本论文提出了CBCL-PR框架，灵感来自海马体和新皮层的概念学习理论，用于解决机器人领域中少样本类增量学习的问题。该框架通过回放旧类别的数据来避免遗忘，在目标分类方面取得了最新的性能。同时，在机器人上的实验也证实了该框架在大量家庭物品的分类任务中有着优秀的表现。 |
| [^56] | [C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation.](http://arxiv.org/abs/2308.00193) | C-DARL是一种自监督的血管分割方法，通过对比学习和生成模块的使用，可以有效地学习多领域血管数据的分布，并生成更真实的血管表示。 |
| [^57] | [Universal Majorization-Minimization Algorithms.](http://arxiv.org/abs/2308.00190) | 这项研究提出了一种通用的主导-极小化算法，它通过自动推导主导器来解决任意问题，并且可以从任何起始点收敛，无需调整超参数。 |
| [^58] | [Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?.](http://arxiv.org/abs/2308.00189) | 生成模型作为复杂系统科学，它们能够完成任务的行为表现需要被解释和理解，以实现对其行为的控制和未来研究的指导。 |
| [^59] | [Attribution-Scores in Data Management and Explainable Machine Learning.](http://arxiv.org/abs/2308.00184) | 数据管理和可解释机器学习中的研究描述了使用实际因果关系来解释数据库查询答案和机器学习模型结果的责任评分的最新工作，以及与数据库修复和Shap-score计算相关的内容。 |
| [^60] | [General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset.](http://arxiv.org/abs/2308.00180) | 本文介绍了一种用于评估海底滑翔器在不可预测海洋环境中正常操作的异常检测算法，并通过实际滑翔器部署的大规模数据集进行验证。算法能够实时提供异常警报，使驾驶员能够控制滑翔器并避免进一步损害。 |
| [^61] | [Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity.](http://arxiv.org/abs/2308.00177) | 本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。 |
| [^62] | [A Flow Artist for High-Dimensional Cellular Data.](http://arxiv.org/abs/2308.00176) | 本研究提出了一个名为FlowArtist的神经网络，可以同时嵌入点和学习围绕点的向量场，从而更好地分离和可视化基于速度的结构。 |
| [^63] | [Federated Learning for Data and Model Heterogeneity in Medical Imaging.](http://arxiv.org/abs/2308.00155) | 本文提出了一种名为MDH-FL的方法，通过在联邦学习中同时利用数据和模型异质性，解决了数据异质性和模型异质性的问题，提高了全局模型的效率。 |
| [^64] | [DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification.](http://arxiv.org/abs/2308.00146) | DiffusAL是一种新颖的主动图学习方法，通过结合三个独立的评分函数，实现了在不同场景中的鲁棒性，并提高了节点分类的标签效率。 |
| [^65] | [Formally Explaining Neural Networks within Reactive Systems.](http://arxiv.org/abs/2308.00143) | 这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。 |
| [^66] | [Semi-Supervised Laplacian Learning on Stiefel Manifolds.](http://arxiv.org/abs/2308.00142) | 本论文将图上的半监督学习重新表述为非凸推广，通过流形对齐问题的解来找到好的分类器，并通过一种新的中心度度量来选择有信息量的样本。实验证明本方法在低标签率下具有更低的分类错误率。 |
| [^67] | [A Suite of Fairness Datasets for Tabular Classification.](http://arxiv.org/abs/2308.00133) | 这篇论文介绍了一套新的公平数据集，旨在解决表格分类中实验评估数据不足的问题，为未来公平感知机器学习研究提供更严谨的实验评估。 |
| [^68] | [Ensemble Learning with Residual Transformer for Brain Tumor Segmentation.](http://arxiv.org/abs/2308.00128) | 本文提出了一种将 Transformer 整合到 U-Net 中的新型网络架构，用于脑肿瘤分割。模型结合了注意力机制和像素级标签，通过残差连接和集成方法进一步提高了分割效果，在 BraTS 2021 数据集上取得了优于最先进方法的结果。 |
| [^69] | [DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms.](http://arxiv.org/abs/2308.00127) | DiviML是一种基于模块的启发式方法，用于在异构平台上将神经网络映射，通过自动分区和设备映射，实现了编译器级别的分布式神经网络编译，具有较好的可扩展性和质量评估能力。 |
| [^70] | [Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects.](http://arxiv.org/abs/2308.00091) | 该论文提出了一个卷积占据模型F-CON，用于在真实世界中进行复杂、未见过物体的密集堆放。通过与现有规划方法结合，F-CON可以有效解决高度遮挡、部分观察场景下的几何形状感知困难问题，并通过与其他最先进的形状补全方法的比较表现出更好的效果。 |
| [^71] | [New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions.](http://arxiv.org/abs/2308.00089) | 该论文介绍了一种新技术，用于证明测试分布单调性和对数凹性的下界。这种方法通过调整一对二项概率的概率构造一对矩匹配的分布族，实现了保持和违背定义不等式的分布，并得到了相关的新下界。 |
| [^72] | [Unsupervised machine learning shock capturing for High-Order CFD solvers.](http://arxiv.org/abs/2308.00086) | 我们提出了一种基于高斯混合模型（GMMs）的无监督机器学习震荡捕捉算法。这种算法具有显著的准确性和鲁棒性，适用于各种复杂几何结构和流动配置。 |
| [^73] | [A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks.](http://arxiv.org/abs/2308.00077) | 本论文研究了一种基于深度学习的网络入侵检测系统（NIDS）对抗对抗攻击的新型模型。通过实施强大的对抗攻击方法，如FGSM、JSMA、PGD和C&W，并采用对抗训练作为防御方法来增强NIDS模型的稳健性。 |
| [^74] | [Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events.](http://arxiv.org/abs/2308.00076) | 这篇论文介绍了一种新颖的技术和方法，旨在通过数据驱动的决策支持系统提升人群管理的规划和操作阶段。该方法利用创新的数据收集技术、数据集成和3D数字孪生技术，结合人工智能工具进行风险识别，并引入了蝴蝶结模型来评估和预测风险水平。 |
| [^75] | [Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model.](http://arxiv.org/abs/2308.00074) | 本文使用了可解释的人工智能（XAI）方法中的核SHAP方法来检测和解释网络异常，并且通过该方法优化了网络异常检测模型的准确性、召回率、精确度和F分数。 |
| [^76] | [Interpretable Stereotype Identification through Reasoning.](http://arxiv.org/abs/2308.00071) | 本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。 |
| [^77] | [FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models.](http://arxiv.org/abs/2308.00065) | FinPT是一种新颖的资金风险预测方法，通过在大型预训练基础模型上进行个人资料调整，填充金融表格数据并获得自然语言客户资料，从而提高预测准确性。 |
| [^78] | [T-Fusion Net: A Novel Deep Neural Network Augmented with Multiple Localizations based Spatial Attention Mechanisms for Covid-19 Detection.](http://arxiv.org/abs/2308.00053) | T-Fusion Net是一种新颖的深度神经网络，通过多个定位的空间注意机制提高了图像分类的性能。通过使用模糊最大融合的方法合并多个模型的输出，进一步提高了准确性。实验结果表明在Covid-19数据集上取得了良好的效果。 |
| [^79] | [Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges.](http://arxiv.org/abs/2308.00031) | 这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。 |
| [^80] | [Exploring how a Generative AI interprets music.](http://arxiv.org/abs/2308.00015) | 使用MusicVAE解释音乐时，大部分关于音高和节奏的信息被编码在前几个音乐神经元中，而旋律的概念则表现为较长的音乐序列中的独立神经元。 |
| [^81] | [Monaural Multi-Speaker Speech Separation Using Efficient Transformer Model.](http://arxiv.org/abs/2308.00010) | 该论文介绍了一种基于Transformer模型的单声道多人演讲分离方法，旨在减少计算复杂性，并在准确性和性能之间取得平衡。 |
| [^82] | [A 3D deep learning classifier and its explainability when assessing coronary artery disease.](http://arxiv.org/abs/2308.00009) | 本文提出了一种3D深度学习模型，用于直接分类冠心病患者和正常受试者，相较于2D模型提高了23.65%的性能，并通过Grad-GAM提供了可解释性。此外，通过与2D语义分割相结合，实现了更好的解释性和准确的异常定位。 |
| [^83] | [A data-centric deep learning approach to airway segmentation.](http://arxiv.org/abs/2308.00008) | 我们提出了一种基于数据的深度学习技术来进行气道分割，通过插值和图像分割提高数据质量，采用集成学习策略在多个尺度上聚合气道树，性能优于基线模型，适用于任何2D深度学习模型。 |
| [^84] | [An Overview Of Temporal Commonsense Reasoning and Acquisition.](http://arxiv.org/abs/2308.00002) | 本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。 |
| [^85] | [Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration.](http://arxiv.org/abs/2307.16499) | 本研究提出了一种新的方法，通过非刚性握姿注册来学习通用工具使用行为。该方法利用单个示范可扩展地学习新类别中的工具操作，并通过引导策略搜索解决复杂的工具使用任务，并在测试时推广到未见过的工具。 |
| [^86] | [L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space.](http://arxiv.org/abs/2307.16459) | L3DMC是一种使用混合曲率空间进行终身学习的蒸馏策略，通过建模和维护复杂几何结构来保留已经学到的知识。 |
| [^87] | [Subspace Distillation for Continual Learning.](http://arxiv.org/abs/2307.16419) | 该论文提出了一种新颖的知识蒸馏技术，通过近似数据流形，并用线性子空间建模结构，来在持续学习中减轻灾难性遗忘。实验证明，该方法优于其他方法，在多个具有挑战性的持续学习任务中表现出色。 |
| [^88] | [Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment.](http://arxiv.org/abs/2307.16210) | 在多模态实体对齐中，现有的方法忽视了视觉图像的不完整性和模糊性，本文通过分析表明模型在面对不完整性时容易出现过拟合和性能下降的问题。 |
| [^89] | [Backdoor Defense with Non-Adversarial Backdoor.](http://arxiv.org/abs/2307.15539) | 提出了一种非对抗性后门防御框架，通过在被污染样本中注入非对抗性后门，当触发时可以抑制攻击者对污染数据的后门攻击，同时保持对干净数据的影响有限。 |
| [^90] | [Noisy Interpolation Learning with Shallow Univariate ReLU Networks.](http://arxiv.org/abs/2307.15396) | 使用最小范数的两层ReLU网络进行噪声单变量回归的插值，对于$L_1$损失和$ p <2 $的$L_p$损失抑制过拟合，但对于$ p \geq 2 $的损失是灾难性的。 |
| [^91] | [Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization.](http://arxiv.org/abs/2307.13430) | 本论文提出了一种新颖的算法，可在分布式环境中实现线性加速，以解决组合极小化问题中内层函数的共识误差问题。 |
| [^92] | [The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking.](http://arxiv.org/abs/2307.13408) | 本研究利用开放银行作为例子，分析了大数据和强大的信息技术（如机器学习）所带来的公平性隐患。通过研究金融脆弱性的维度，我们展示了细致交易数据的能力，并提醒对其应当谨慎使用以避免潜在的歧视问题。 |
| [^93] | [MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning.](http://arxiv.org/abs/2307.13055) | 提出了一个模型无关配方MARIO，用于改善图对比学习的OOD泛化性能。MARIO引入了信息瓶颈原则和不变性原则，旨在获得具有分布偏移鲁棒性和不变性的图表示。 |
| [^94] | [Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models.](http://arxiv.org/abs/2307.11224) | Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。 |
| [^95] | [Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection.](http://arxiv.org/abs/2307.10869) | 云系统中的性能问题识别存在挑战，现有方法中的独立分析每个指标的异常无法解决问题，需要考虑指标之间的关联性。 |
| [^96] | [Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network.](http://arxiv.org/abs/2307.09866) | 该论文使用图神经网络和强化学习对城市基础设施相互依赖网络中的脆弱节点进行了准确建模和分析。 |
| [^97] | [End-to-End Neural Network Training for Hyperbox-Based Classification.](http://arxiv.org/abs/2307.09269) | 本论文提出了一种基于神经网络的端到端训练方法来处理基于超立方体的分类问题。与现有方法相比，该方法能够高效地训练超立方体模型，并大大减少训练时间，同时获得更优的分类结果。 |
| [^98] | [How is ChatGPT's behavior changing over time?.](http://arxiv.org/abs/2307.09009) | 本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。 |
| [^99] | [DISPEL: Domain Generalization via Domain-Specific Liberating.](http://arxiv.org/abs/2307.07181) | DISPEL是一种通过后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征的领域泛化方法。 |
| [^100] | [CrunchGPT: A chatGPT assisted framework for scientific machine learning.](http://arxiv.org/abs/2306.15551) | CrunchGPT是一个基于ChatGPT的科学机器学习辅助框架，通过简单的用户提示来协调整个科学机器学习的工作流程，实现无缝集成数据和物理知识，解决了SciML在预处理、问题建模、代码生成、后处理和分析等方面的耗时问题，拓展了其工业应用和数字孪生框架的适用性。 |
| [^101] | [ChiPFormer: Transferable Chip Placement via Offline Decision Transformer.](http://arxiv.org/abs/2306.14744) | ChiPFormer通过离线学习可转移的布局策略，显著提高了芯片布局的质量，并在减少布局时间的同时增强了对未知芯片电路的适应能力。 |
| [^102] | [CARL-G: Clustering-Accelerated Representation Learning on Graphs.](http://arxiv.org/abs/2306.06936) | CARL-G是一个基于聚类的图表征学习框架，通过使用受集群验证启发的损失函数，克服了负采样和复杂架构的问题。 |
| [^103] | [Provable convergence guarantees for black-box variational inference.](http://arxiv.org/abs/2306.03638) | 本文提出了一种基于密集高斯变分族的梯度估计器，在此基础上使用近端和投影随机梯度下降，提供了黑盒变分推断收敛于逼真推断问题的第一个严格保证。 |
| [^104] | [W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition.](http://arxiv.org/abs/2305.18624) | W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。 |
| [^105] | [Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models.](http://arxiv.org/abs/2305.17446) | 该论文通过发现预训练语言模型的内在任务特定子空间，提出了一种重新参数化和微调模型的新方法。研究发现在该子空间中，只需少量自由参数即可有效微调模型，并且某些维度对于引入任务特定知识至关重要。 |
| [^106] | [Reinforcement Learning With Reward Machines in Stochastic Games.](http://arxiv.org/abs/2305.17372) | 该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。 |
| [^107] | [Diable: Efficient Dialogue State Tracking as Operations on Tables.](http://arxiv.org/abs/2305.17020) | Diable是一个高效的对话状态跟踪系统，它通过在表格上进行操作来更新对话状态，相比现有方法时间效率提高了2.4倍，同时保持了竞争性的目标准确性。 |
| [^108] | [Learning and accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks.](http://arxiv.org/abs/2305.15920) | 本文使用GAN来学习一个原型晶格上的随机过程，并提出一种合适的多模型程序，可以显著提高精度。GAN似乎是处理复杂统计动力学问题的有前途的工具。 |
| [^109] | [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback.](http://arxiv.org/abs/2305.14387) | 该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。 |
| [^110] | [GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance.](http://arxiv.org/abs/2305.12073) | 本文对GELU激活函数进行了全面的数学分析和广泛的实验比较，证明了它在深度学习模型中具有优越的性能和适用性。 |
| [^111] | [Continual Multimodal Knowledge Graph Construction.](http://arxiv.org/abs/2305.08698) | 连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。 |
| [^112] | [Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts.](http://arxiv.org/abs/2305.05832) | 本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。 |
| [^113] | [FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity.](http://arxiv.org/abs/2305.05230) | 本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。 |
| [^114] | [Unsupervised Improvement of Audio-Text Cross-Modal Representations.](http://arxiv.org/abs/2305.01864) | 本研究探索了无监督的方法来改进跨模态音频-文本表征学习，通过使用领域特定的筛选和软标注对比性损失，成功提高了零-shot分类性能。 |
| [^115] | [(Local) Differential Privacy has NO Disparate Impact on Fairness.](http://arxiv.org/abs/2304.12845) | 本文研究了在 LDP 下收集多个敏感属性对公平性的影响并提出了考虑域大小的新的隐私预算分配方案，实验表明该方案在隐私、效用和公平性方面均优于最新的解决方案，LDP 带来了略微改善的公平性而不会明显影响性能。 |
| [^116] | [Hyper-Laplacian Regularized Concept Factorization in Low-rank Tensor Space for Multi-view Clustering.](http://arxiv.org/abs/2304.11435) | 本文提出了一种采用超拉普拉斯正则化概念因式分解的多视角聚类方法，在低秩张量空间中对每个视角进行聚类并提取非线性局部结构。 |
| [^117] | [Kernel interpolation generalizes poorly.](http://arxiv.org/abs/2303.15809) | 本文证明了核插值的泛化误差有一个下界，在较大范围的核函数中泛化能力较差，使得过拟合的宽神经网络泛化性能差。 |
| [^118] | [FedGH: Heterogeneous Federated Learning with Generalized Global Header.](http://arxiv.org/abs/2303.13137) | FedGH是一种异构联邦学习方法，可以使客户端持有具有不同结构的模型，通过训练共享的广义全局预测头来提高效率和性能。 |
| [^119] | [mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection.](http://arxiv.org/abs/2303.09901) | 本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。 |
| [^120] | [Trigger-Level Event Reconstruction for Neutrino Telescopes Using Sparse Submanifold Convolutional Neural Networks.](http://arxiv.org/abs/2303.08812) | SSCNN是一种用于解决中微子望远镜数据处理中稀疏、高维度和非规则几何形状等问题的网络，其事件重构性能优于传统和机器学习算法，运行速度大大提高，可用于改善中微子能量和方向的第一估计以播种更先进的重建。 |
| [^121] | [Convergence Rates for Non-Log-Concave Sampling and Log-Partition Estimation.](http://arxiv.org/abs/2303.03237) | 非对数凹势V的高维采样速率可以在一些条件下实现与凸函数相同的收敛速率。 |
| [^122] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | 本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。 |
| [^123] | [How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy.](http://arxiv.org/abs/2303.00654) | 这篇论文提供了关于如何将差分隐私应用于复杂机器学习模型的实用指南，填补了现有实践中的空白，为实现机器学习与差分隐私的结合提供了实际指导。 |
| [^124] | [Nystr\"om $M$-Hilbert-Schmidt Independence Criterion.](http://arxiv.org/abs/2302.09930) | 这项研究提出了Nystr\"om $M$-Hilbert-Schmidt独立准则，针对大规模应用的二次计算瓶颈问题进行了解决，并兼顾了多个随机变量的推广情况和理论保证。 |
| [^125] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^126] | [On student-teacher deviations in distillation: does it pay to disobey?.](http://arxiv.org/abs/2301.12923) | 通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。 |
| [^127] | [Abstracting Imperfect Information Away from Two-Player Zero-Sum Games.](http://arxiv.org/abs/2301.09159) | 通过正则化均衡，可以将两人零和博弈中的不完美信息抽象出来并作为完全信息问题处理。 |
| [^128] | [Machine Learning-Aided Efficient Decoding of Reed-Muller Subcodes.](http://arxiv.org/abs/2301.06251) | 本论文研究了具有灵活码率的Reed-Muller子码的高效解码问题，通过扩展递归投影聚合（RPA）译码算法，提出了subRPA和soft-subRPA算法，能够在维持较低复杂性的同时提高译码性能并实现可微分的解码算法。 |
| [^129] | [Lifelong Reinforcement Learning with Modulating Masks.](http://arxiv.org/abs/2212.11110) | 本文研究了终身强化学习中使用调整掩码的方法，通过将调整掩码应用于PPO和IMPALA代理，显著提高了在离散和连续强化学习任务中的性能。 |
| [^130] | [Deep Riemannian Networks for EEG Decoding.](http://arxiv.org/abs/2212.10426) | 本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。 |
| [^131] | [Reliable Measures of Spread in High Dimensional Latent Spaces.](http://arxiv.org/abs/2212.08172) | 本文定义了数据展开度的概念，并发现常用的度量方法不可靠，提出了八种备选的数据展开度量方法，并推荐使用一种基于主成分和一种基于熵的度量方法，这些方法可以可靠地比较不同模型的展开度。 |
| [^132] | [Learning useful representations for shifting tasks and distributions.](http://arxiv.org/abs/2212.07346) | 处理多个分布时，通过连接多个训练过程得到的表示比单个训练过程得到的表示更好。 |
| [^133] | [Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning.](http://arxiv.org/abs/2211.09273) | 本研究通过对抗机器学习的方法，提出了一种名为DARE-GP的解决方案，可以在不影响智能音箱效用的情况下，逃避与智能音箱相连的语音情感识别分类器，从而保护隐私。 |
| [^134] | [Data-driven low-dimensional dynamic model of Kolmogorov flow.](http://arxiv.org/abs/2210.16708) | 该论文提出了一种数据驱动的低维动力学模型方法，用于捕捉Kolmogorov流的流动动力学和属性。通过使用欠完备自编码器进行维度降低和潜在空间的离散时间模型开发，我们可以有效地建模具有挑战性的混沌和间歇行为。 |
| [^135] | [On the Generalized Likelihood Ratio Test and One-Class Classifiers.](http://arxiv.org/abs/2210.12494) | 本文考虑了一类分类器和广义似然比检验的问题，证明了多层感知器神经网络和支持向量机模型在收敛时会表现为广义似然比检验。同时，作者还展示了一类最小二乘SVM在收敛时也能达到广义似然比检验的效果。 |
| [^136] | [Learning Graphical Factor Models with Riemannian Optimization.](http://arxiv.org/abs/2210.11950) | 本文提出了一种灵活的算法框架，用于在协方差矩阵上具有低秩结构约束的图学习。通过使用Riemannian优化，利用正定矩阵和固定秩正半定矩阵的几何特性，解决了这类问题。 |
| [^137] | [3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows.](http://arxiv.org/abs/2210.11603) | 本文介绍了一种将文本到图像AI集成到3D设计工作流程中的方法，通过插件3DALL-E，设计师可以使用AI生成的图像灵感构建3D模型。研究结果显示，设计师对于3DALL-E在工作流程中有很大的潜力，可以生成参考图像、防止设计固化并激发设计的考虑。 |
| [^138] | [RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction.](http://arxiv.org/abs/2210.09309) | 本文扩展了RibSeg数据集到大规模基准测试RibSeg v2，加入了手动标注的肋骨标记和解剖中心线提取，共包含660个CT扫描（15,466个独立的肋骨），并提出了深度学习方法用于肋骨标记、基于骨架化方法用于中心线提取、一种稀疏点云表示CT扫描的方法，以及适用于该任务的评估指标。 |
| [^139] | [FAIR for AI: An interdisciplinary and international community building perspective.](http://arxiv.org/abs/2210.08973) | FAIR原则旨在实现学术数据的重用性，并在人工智能模型和数据领域得到应用。研究人员从不同背景和学科的角度讨论了他们在实践社区中定义和采用FAIR原则的经验和成果，以及追求和激励FAIR人工智能研究可能带来的结果。 |
| [^140] | [Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders.](http://arxiv.org/abs/2207.11749) | 本研究提出了一种基于自动编码器的解决方案，用于对未知数量的单通道水声信号进行源分离。通过固定输出通道数量和新的性能评估方法，避免了排列问题引起的维度灾难，并在实验证明与已知信号数量相似的分离性能。该算法具有竞争性能、可解释性和可扩展性，在该框架下达到了最先进的水平。 |
| [^141] | [Verifiable Goal Recognition for Autonomous Driving with Occlusions.](http://arxiv.org/abs/2206.14163) | OGRIT是一种用于自动驾驶的目标识别方法，能够处理遮挡导致的数据缺失，并且具有快速、准确、可解释和可验证的特点。 |
| [^142] | [Causal Discovery and Knowledge Injection for Contestable Neural Networks.](http://arxiv.org/abs/2205.09787) | 本研究提出了一种可以进行双向互动的方法，通过允许神经网络展示其所学因果图，并允许人类修改因果图后重新注入机器中，从而提供了一种调试神经网络的方式，实验结果显示该方法可以显著改善预测性能。 |
| [^143] | [A Framework and Benchmark for Deep Batch Active Learning for Regression.](http://arxiv.org/abs/2203.09410) | 本研究提出了一个深度批量主动学习回归的框架和基准测试，其中包括许多现有的贝叶斯和非贝叶斯方法。提出了一种替换常用最后一层特征的新方法，并结合一种新颖的聚类方法。在15个大型表格回归数据集上进行测试，该方法在基准测试中表现优异，适用于大型数据集且易于使用。 |
| [^144] | [AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators.](http://arxiv.org/abs/2203.03673) | AgraSSt是一种用于评估隐式图生成器质量的统计方法，通过构建基于核的差异度量，它能够确定学习到的图生成过程是否能生成类似给定输入图的图形，并提供有关图生成器训练过程的可解释性问题和可靠样本批次的信息。 |
| [^145] | [Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case.](http://arxiv.org/abs/2202.05069) | 本文提出了一种适用于线性回归情况的迁移学习算法，该算法能够将新数据与历史数据相结合，特别在新数据稀缺的情况下具有益处，并且在实验验证中表现出对负迁移学习的鲁棒性。 |
| [^146] | [Multi-Output Gaussian Process-Based Data Augmentation for Multi-Building and Multi-Floor Indoor Localization.](http://arxiv.org/abs/2202.01980) | 本论文研究了基于多输出高斯过程的三种接收信号强度指纹数据增强方法，用于解决室内定位中数据收集困难的问题。 |
| [^147] | [Identifying Pauli spin blockade using deep learning.](http://arxiv.org/abs/2202.00574) | 该论文介绍了一种使用深度学习算法自动识别Pauli自旋阻碍的方法，通过训练算法使用模拟数据和跨设备验证，克服了PSB数据的稀缺性，并在硅场效应晶体管器件上取得了96％的准确性。该方法具有鲁棒性，预计可在所有类型的量子点器件上使用。 |
| [^148] | [A Unified Benchmark for the Unknown Detection Capability of Deep Neural Networks.](http://arxiv.org/abs/2112.00337) | 该论文提出了一个统一评估标准，针对深度神经网络对未知样本的检测能力进行了全面考察。通过构建统一基准数据集，并比较现有方法的表现，发现Deep Ensemble方法具有较高的未知检测能力。 |
| [^149] | [Spectral learning of multivariate extremes.](http://arxiv.org/abs/2111.07799) | 我们提出了一种用于分析多元极值的谱聚类算法，并通过理论和数值实验展示了其在学习角度测度方面的性能。 |
| [^150] | [A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations.](http://arxiv.org/abs/2111.06077) | 本文综述介绍了超维计算和矢量符号化架构（HDC/VSA），这是一种利用高维分布式表示和代数属性的计算模型，融合了结构化符号表示和向量分布式表示的优势。该领域涉及多个学科，并介绍了多个相关模型。 |
| [^151] | [Integrated Conditional Estimation-Optimization.](http://arxiv.org/abs/2110.12351) | 该论文提出了一种综合条件估计-优化（ICEO）框架，可以在考虑优化问题结构的同时估计随机参数的条件分布，并提供了一些性能保证。 |
| [^152] | [The Complexity of Learning Approval-Based Multiwinner Voting Rules.](http://arxiv.org/abs/2110.00254) | 本研究关注基于批准的多赢者投票规则的学习问题，我们发现尽管存在指数级的可能结果，但通过多项式数量的样本可以高置信度和准确性地学习目标规则。 |
| [^153] | [Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights.](http://arxiv.org/abs/2109.04660) | 本文介绍了动态集体智能学习（DCIL）方法，利用精炼的梯度更新剪枝权重，通过形成双向转发路径来寻找高效稀疏模型。这种方法利用了剪枝和未剪枝权重的集体智能之间的学习协同作用。 |
| [^154] | [Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth.](http://arxiv.org/abs/2103.03404) | 这项研究提出了一种新的方法来理解自注意力网络，通过将其输出分解为较小的项，证明自注意力对“标记均衡性”的强归纳偏见，而跳跃连接和多层感知机可以阻止输出的退化。 |
| [^155] | [On the Universality of the Double Descent Peak in Ridgeless Regression.](http://arxiv.org/abs/2010.01851) | 我们证明了在无岭线性回归中存在一个双下降峰，无论输入分布的特征映射是确定性的还是随机的，都会导致期望均方泛化误差增加。并且我们的结果适用于广泛的输入分布类。 |
| [^156] | [An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting Using Deep Learning.](http://arxiv.org/abs/1905.02616) | 本研究提出了一种使用深度学习方法进行太阳辐照度预测的统一架构，能够在不同时间尺度上进行预测，并提出了一个框架来将这种方法扩展到每小时预测范围。 |
| [^157] | [Considerations When Learning Additive Explanations for Black-Box Models.](http://arxiv.org/abs/1801.08640) | 本文研究了非增加型模型的全局增加性解释方法，发现不同的解释方法以不同的方式刻画了黑盒模型预测函数中的非增加性成分。尽管精简的解释一般是最准确的增加性解释，但显式建模非增加性成分的树形解释往往更准确。机器学习从业者能够更好地利用增加性解释来完成各种任务。 |

# 详细

[^1]: 从超维向量中学习：关于超维编码的调查

    Learning from Hypervectors: A Survey on Hypervector Encoding. (arXiv:2308.00685v1 [cs.LG])

    [http://arxiv.org/abs/2308.00685](http://arxiv.org/abs/2308.00685)

    本调查研究聚焦于超维计算系统的输入和超维向量的生成，探讨了各种方法的限制、挑战和潜在好处。

    

    超维计算是一种模拟大脑结构，提供强大而高效的处理和学习模型的新兴计算范式。在超维计算中，数据被编码为长向量，称为超维向量，通常长度为1K到10K。文献提供了几种编码技术来生成正交或相关的超维向量，这取决于预期的应用。现有文献调查通常集中在超维计算系统的整体方面，包括系统输入、主要计算和最终输出。然而，本研究采取了更具体的方法。它聚焦于超维计算系统的输入和超维向量的生成，直接影响超维编码过程。本调查汇集了不同研究中关于超维向量生成的各种方法，并探讨了它们的限制、挑战和潜在的好处。通过对本调查的全面探索，读者将获得一些关于超维编码的重要见解。

    Hyperdimensional computing (HDC) is an emerging computing paradigm that imitates the brain's structure to offer a powerful and efficient processing and learning model. In HDC, the data are encoded with long vectors, called hypervectors, typically with a length of 1K to 10K. The literature provides several encoding techniques to generate orthogonal or correlated hypervectors, depending on the intended application. The existing surveys in the literature often focus on the overall aspects of HDC systems, including system inputs, primary computations, and final outputs. However, this study takes a more specific approach. It zeroes in on the HDC system input and the generation of hypervectors, directly influencing the hypervector encoding process. This survey brings together various methods for hypervector generation from different studies and explores the limitations, challenges, and potential benefits they entail. Through a comprehensive exploration of this survey, readers will acquire a 
    
[^2]: CodeBPE: 探索用于源代码的大型语言模型预训练的子标记化选项

    CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code. (arXiv:2308.00683v1 [cs.LG])

    [http://arxiv.org/abs/2308.00683](http://arxiv.org/abs/2308.00683)

    CodeBPE研究了用于源代码的大型语言模型预训练中不同子标记化选项的影响，找出了最有效和长度高效的子标记化方法，通过减少平均长度17%且不影响下游性能，可能提高质量0.5-2%。

    

    最近的研究广泛采用了针对源代码的大型语言模型预训练，提出了源代码特定的预训练目标，并研究了不同基于Transformer的语言模型架构在源代码中的适用性。本研究调查了这些模型的另一个重要方面，即不同子标记化选项的影响，并旨在确定最有效和长度高效的子标记化，考虑到代码的特殊性。我们提出了一种子标记化方法，平均长度减少了17%，且没有下游性能下降，并且表明精心选择的子标记化可能会提高质量0.5-2%，可能会略微增加长度。

    Recent works have widely adopted large language model pretraining for source code, suggested source code-specific pretraining objectives and investigated the applicability of various Transformer-based language model architectures for source code. This work investigates another important aspect of such models, namely the effect of different subtokenization options, and aims at identifying most effective and length-efficient subtokenizations, taking into account code specifics. We propose subtokenziation that reduces average length by 17% without downstream performance drop, and show that a carefully chosen subtokenization may improve quality by 0.5-2%, possibly with some length increase.
    
[^3]: 离散神经网络和多态学习

    Discrete neural nets and polymorphic learning. (arXiv:2308.00677v1 [cs.NE])

    [http://arxiv.org/abs/2308.00677](http://arxiv.org/abs/2308.00677)

    本文提出了一种基于离散模拟的神经网络，将Murski{i}的通用代数定理和Cybenko的通用逼近结果统一起来。同时，通过引入关系结构的多态性，我们提出了一种学习算法，用于经典学习任务。

    

    20世纪70年代Murski{i}提出的通用代数定理与20世纪80年代Cybenko等人关于神经网络的通用逼近结果有着惊人的相似性。本文考虑了经典神经网络的离散模拟，将这些结果放在一个统一的框架下。我们介绍了一种基于关系结构多态性的学习算法，并展示了如何将其用于经典学习任务。

    Theorems from universal algebra such as that of Murski\u{i} from the 1970s have a striking similarity to universal approximation results for neural nets along the lines of Cybenko's from the 1980s. We consider here a discrete analogue of the classical notion of a neural net which places these results in a unified setting. We introduce a learning algorithm based on polymorphisms of relational structures and show how to use it for a classical learning task.
    
[^4]: 工具文档使得大型语言模型能够进行零-shot工具使用

    Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models. (arXiv:2308.00675v1 [cs.CL])

    [http://arxiv.org/abs/2308.00675](http://arxiv.org/abs/2308.00675)

    这项工作提出了使用工具文档作为教导大型语言模型使用新工具的替代方法，并通过实证研究证明，仅使用工具文档的零-shot提示足以实现正确的工具使用。

    

    如今，通过提供一些工具使用的演示来教授大型语言模型（LLM）使用新工具。不幸的是，演示很难获得，并且如果选择了错误的演示，可能会导致不良的偏见使用。即使在罕见的情况下，演示是readily available的，也没有原则性的选择协议来确定提供多少个和哪些演示。随着任务变得更加复杂，选择搜索组合数的增长成为不可处理的。我们的工作提供了一种替代演示的方法：工具文档。我们主张使用工具文档来描述各个工具的使用，而不是演示。我们通过跨视觉和语言模态的6个任务上的三个主要实证发现支持我们的主张。首先，在现有的基准测试上，仅使用工具文档的零-shot提示足以引出正确的工具使用，达到了few-shot提示的性能水平。

    Today, large language models (LLMs) are taught to use new tools by providing a few demonstrations of the tool's usage. Unfortunately, demonstrations are hard to acquire, and can result in undesirable biased usage if the wrong demonstration is chosen. Even in the rare scenario that demonstrations are readily available, there is no principled selection protocol to determine how many and which ones to provide. As tasks grow more complex, the selection search grows combinatorially and invariably becomes intractable. Our work provides an alternative to demonstrations: tool documentation. We advocate the use of tool documentation, descriptions for the individual tool usage, over demonstrations. We substantiate our claim through three main empirical findings on 6 tasks across both vision and language modalities. First, on existing benchmarks, zero-shot prompts with only tool documentation are sufficient for eliciting proper tool usage, achieving performance on par with few-shot prompts. Secon
    
[^5]: 遗传编程中的主动学习：为符号回归指导高效数据收集的方法

    Active Learning in Genetic Programming: Guiding Efficient Data Collection for Symbolic Regression. (arXiv:2308.00672v1 [cs.NE])

    [http://arxiv.org/abs/2308.00672](http://arxiv.org/abs/2308.00672)

    本文研究了遗传编程中的主动学习方法，通过利用模型群体和不确定性度量来选择信息丰富的训练数据点，并发现微分熵是最好的不确定性度量。同时，通过将相关性作为多样性度量，可以更好地选择数据点进行训练。最后，通过帕累托优化方法综合考虑不确定性和多样性，实现了平衡地选择训练数据点的目的。

    

    本文研究了遗传编程中计算不确定性和多样性的各种方法。我们发现，通过使用模型合集结合不确定性度量，可以利用遗传编程中的模型群体选择信息丰富的训练数据点。我们探索了几种不确定性度量，并发现微分熵表现最好。我们还比较了两种数据多样性度量，并发现作为多样性度量的相关性比最小欧氏距离表现更好，尽管相关性具有一些缺点，不能在所有问题上使用。最后，我们使用帕累托优化方法将不确定性和多样性结合起来，以平衡地指导选择信息丰富且独特的数据点进行训练。

    This paper examines various methods of computing uncertainty and diversity for active learning in genetic programming. We found that the model population in genetic programming can be exploited to select informative training data points by using a model ensemble combined with an uncertainty metric. We explored several uncertainty metrics and found that differential entropy performed the best. We also compared two data diversity metrics and found that correlation as a diversity metric performs better than minimum Euclidean distance, although there are some drawbacks that prevent correlation from being used on all problems. Finally, we combined uncertainty and diversity using a Pareto optimization approach to allow both to be considered in a balanced way to guide the selection of informative and unique data points for training.
    
[^6]: Hessian-Aware Bayesian Optimization for Decision Making Systems - 感知海森贝叶斯优化在决策系统中的应用

    Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v1 [cs.LG])

    [http://arxiv.org/abs/2308.00629](http://arxiv.org/abs/2308.00629)

    本文介绍了一种感知海森贝叶斯优化算法，旨在解决决策系统优化中梯度反馈稀缺或无效的问题。通过引入紧凑的多层架构和角色概念，并利用感知海森贝叶斯优化方法对参数进行优化，作者实现了对复杂决策系统的高效优化。

    

    许多优化决策系统的方法依赖于梯度方法，需要从环境中获取有信息量的反馈。然而，当反馈稀缺或者无信息时，这些方法可能导致性能较差。贝叶斯优化等无导数方法可以减少对梯度反馈质量的依赖，但在复杂决策系统的高维环境中往往难以扩展。如果系统需要多个参与者之间的互动来实现共同目标，这个问题就加剧了。为了解决维度问题，我们提出了一种紧凑的多层架构，通过角色的概念来建模参与者之间的动态。此外，我们还引入了感知海森贝叶斯优化来高效地优化由大量参数参数化的多层架构。实验结果表明，我们的方法(HA-GP-UCB)在效果上是有效的。

    Many approaches for optimizing decision making systems rely on gradient based methods requiring informative feedback from the environment. However, in the case where such feedback is sparse or uninformative, such approaches may result in poor performance. Derivative-free approaches such as Bayesian Optimization mitigate the dependency on the quality of gradient feedback, but are known to scale poorly in the high-dimension setting of complex decision making systems. This problem is exacerbated if the system requires interactions between several actors cooperating to accomplish a shared goal. To address the dimensionality challenge, we propose a compact multi-layered architecture modeling the dynamics of actor interactions through the concept of role. Additionally, we introduce Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered architecture parameterized by a large number of parameters. Experimental results demonstrate that our method (HA-GP-UCB) works effectiv
    
[^7]: 人类-M3：一个用于室外场景中的3D人体姿势估计的多视角多模态数据集

    Human-M3: A Multi-view Multi-modal Dataset for 3D Human Pose Estimation in Outdoor Scenes. (arXiv:2308.00628v1 [cs.CV])

    [http://arxiv.org/abs/2308.00628](http://arxiv.org/abs/2308.00628)

    这篇论文提出了一个室外多模态多视角多人类姿势数据库Human-M3，并介绍了一种基于多模态数据输入的算法来生成准确的人体姿势。这个数据库解决了现有数据集的不足，提供了更多的数据多样性。

    

    最近，对于室外环境中的3D人体姿势估计越来越受到关注。然而，现有的室外场景3D人体姿势数据集缺乏多样性，因为它们主要只使用一种模态（RGB图像或点云），并且场景中通常只有一个人。数据集基础的有限范围严重阻碍了可用数据的变化性。在本文中，我们提出了Human-M3，这是一个室外多模态多视角多人类姿势数据库，其中包括室外场景的多视角RGB视频和相应的点云数据。为了获得准确的人体姿势，我们提出了一种基于多模态数据输入的算法来生成地面真值标注。这种方法利用了鲁棒的点云检测和跟踪，解决了之前室外场景中多个人的多视角RGB视频中可能存在的不准确人体定位和匹配模糊问题，生成了相关信息。

    3D human pose estimation in outdoor environments has garnered increasing attention recently. However, prevalent 3D human pose datasets pertaining to outdoor scenes lack diversity, as they predominantly utilize only one type of modality (RGB image or pointcloud), and often feature only one individual within each scene. This limited scope of dataset infrastructure considerably hinders the variability of available data. In this article, we propose Human-M3, an outdoor multi-modal multi-view multi-person human pose database which includes not only multi-view RGB videos of outdoor scenes but also corresponding pointclouds. In order to obtain accurate human poses, we propose an algorithm based on multi-modal data input to generate ground truth annotation. This benefits from robust pointcloud detection and tracking, which solves the problem of inaccurate human localization and matching ambiguity that may exist in previous multi-view RGB videos in outdoor multi-person scenes, and generates rel
    
[^8]: 超越One-Hot-Encoding: 注入语义驱动图像分类器

    Beyond One-Hot-Encoding: Injecting Semantics to Drive Image Classifiers. (arXiv:2308.00607v1 [cs.CV])

    [http://arxiv.org/abs/2308.00607](http://arxiv.org/abs/2308.00607)

    本文探索了将本体和语义知识反映到图像分类器中的方法，提高了模型的可解释性和可信度。

    

    图像中包含了与现实世界本体论相关的语义信息：狗的品种具有哺乳动物的相似性，食物的图片通常在家庭环境中描述，等等。然而，在对图像分类进行机器学习模型训练时，对象类之间的相对相似性常常与One-Hot-Encoding标签配对。根据这种逻辑，如果一个图像被标记为“勺子”，那么“茶勺”和“鲨鱼”在训练损失方面是同样错误的。为了克服这个限制，我们探索了整合反映本体和语义知识的额外目标的方法，提高了模型的可解释性和可信度。我们提出了一种通用方法，可以根据与分类标签相关的任何类型的语义信息导出额外的损失项。首先，我们展示了如何将我们的方法应用于本体和词嵌入，并讨论了由此得到的信息如何驱动受监督的学习过程。其次，

    Images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. However, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. According to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. To overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. We suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. First, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. Second
    
[^9]: 使用量子核的支持向量回归进行半监督异常检测

    Semisupervised Anomaly Detection using Support Vector Regression with Quantum Kernel. (arXiv:2308.00583v1 [quant-ph])

    [http://arxiv.org/abs/2308.00583](http://arxiv.org/abs/2308.00583)

    使用量子核的支持向量回归方法被应用于半监督异常检测中，针对NISQ设备的限制，该方法具有理论保证、灵活性和兼容性。

    

    异常检测（AD）涉及识别与其他数据有所不同的观测或事件。机器学习技术在自动化这个过程中显示出成功，通过检测大规模数据中隐藏的模式和偏差。量子计算在机器学习中的潜力已得到广泛认可，导致了大量研究工作以开发适用于量子机器学习（QML）算法。特别是对近期NISQ设备的QML算法的搜索正在全力进行。然而，NISQ设备由于其有限的量子比特相干时间、较少的量子比特数量和高误差率，存在额外的挑战。基于量子核估计的核方法已成为NISQ设备上进行QML的一种有前途的方法，具有理论保证、灵活性和与NISQ约束的兼容性。特别是利用量子核估计的支持向量机（SVM）在各种监督学习任务中显示出成功。

    Anomaly detection (AD) involves identifying observations or events that deviate in some way from the rest of the data. Machine learning techniques have shown success in automating this process by detecting hidden patterns and deviations in large-scale data. The potential of quantum computing for machine learning has been widely recognized, leading to extensive research efforts to develop suitable quantum machine learning (QML) algorithms. In particular, the search for QML algorithms for near-term NISQ devices is in full swing. However, NISQ devices pose additional challenges due to their limited qubit coherence times, low number of qubits, and high error rates. Kernel methods based on quantum kernel estimation have emerged as a promising approach to QML on NISQ devices, offering theoretical guarantees, versatility, and compatibility with NISQ constraints. Especially support vector machines (SVM) utilizing quantum kernel estimation have shown success in various supervised learning tasks
    
[^10]: 在随机位置预测掩盖的标记改善了掩盖图像建模

    Predicting masked tokens in stochastic locations improves masked image modeling. (arXiv:2308.00566v1 [cs.CV])

    [http://arxiv.org/abs/2308.00566](http://arxiv.org/abs/2308.00566)

    本论文提出了一种名为FlexPredict的随机模型，通过在模型中加入位置不确定性，以预测掩盖的标记位置，从而改善了掩盖图像建模的性能。

    

    自监督学习是深度学习中一种有前景的范式，通过构建需要学习有用表示的预训练任务，可以从无标签数据中进行学习。在自然语言处理中，主要的预训练任务是掩盖语言建模（MLM），而在计算机视觉中存在相应的掩盖图像建模（MIM）。然而，MIM具有挑战性，因为它需要在准确位置上预测语义内容。例如，给定一张不完整的狗的图片，我们可以猜测有一个尾巴，但我们无法确定它的确切位置。在这项工作中，我们提出了FlexPredict，这是一个考虑位置不确定性的随机模型，通过将模型条件化到随机掩盖的标记位置上，引导模型学习更加鲁棒对位置不确定性的特征。我们的方法改进了多个任务的下游性能，例如与MIM基准相比，FlexPredict在一系列任务上表现更好。

    Self-supervised learning is a promising paradigm in deep learning that enables learning from unlabeled data by constructing pretext tasks that require learning useful representations. In natural language processing, the dominant pretext task has been masked language modeling (MLM), while in computer vision there exists an equivalent called Masked Image Modeling (MIM). However, MIM is challenging because it requires predicting semantic content in accurate locations. E.g, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. In this work, we propose FlexPredict, a stochastic model that addresses this challenge by incorporating location uncertainty into the model. Specifically, we condition the model on stochastic masked token positions to guide the model toward learning features that are more robust to location uncertainties. Our approach improves downstream performance on a range of tasks, e.g, compared to MIM baselines, Fle
    
[^11]: 坚强的线性回归：相变和对一般范数的精确权衡

    Robust Linear Regression: Phase-Transitions and Precise Tradeoffs for General Norms. (arXiv:2308.00556v1 [stat.ML])

    [http://arxiv.org/abs/2308.00556](http://arxiv.org/abs/2308.00556)

    本文研究了测试时的对抗攻击对线性回归模型的影响，并确定了在保持预测性能的情况下可以达到的最佳鲁棒性水平。该研究揭示了鲁棒性和准确性之间的权衡，并在不损害准确性的情况下找到了鲁棒性的实现方法。

    

    本文研究了测试时对线性回归模型的对抗攻击对其影响，并确定了任何模型在保持给定水平的预测性能（准确度）的同时可以达到的最佳鲁棒性水平。通过定量估计，我们揭示了在不同领域中鲁棒性和准确性之间的基本权衡。我们获得了一个明确的描述，区分了在不损害标准准确性的情况下可以实现鲁棒性的情况与不可避免地需要权衡的情况。我们的研究结果在多种设置下通过简单的实验得到了经验证实。这项工作适用于特征协方差矩阵和任何性质的攻击范数，并超越了之前在该领域的工作。

    In this paper, we investigate the impact of test-time adversarial attacks on linear regression models and determine the optimal level of robustness that any model can reach while maintaining a given level of standard predictive performance (accuracy). Through quantitative estimates, we uncover fundamental tradeoffs between adversarial robustness and accuracy in different regimes. We obtain a precise characterization which distinguishes between regimes where robustness is achievable without hurting standard accuracy and regimes where a tradeoff might be unavoidable. Our findings are empirically confirmed with simple experiments that represent a variety of settings. This work applies to feature covariance matrices and attack norms of any nature, and extends beyond previous works in this area.
    
[^12]: Copula用于实例级特征选择和排序

    Copula for Instance-wise Feature Selection and Ranking. (arXiv:2308.00549v1 [cs.LG])

    [http://arxiv.org/abs/2308.00549](http://arxiv.org/abs/2308.00549)

    在实例级特征选择和排序中，我们提出了一种使用Copula的方法，能够更好地考虑特征之间的相关性，实验结果证明我们的方法能够捕捉有意义的相关性。

    

    在神经网络的背景下，实例级特征选择和排序方法可以为每个样本实现良好的特征选择。然而，现有的方法假设特征子集独立，对于考虑特征之间的依赖性存在缺陷。为了解决这个限制，我们提出将高斯copula，一种捕捉变量之间相关性的强大数学技术，无需进行额外的更改就可以将其纳入当前的特征选择框架中。通过在合成和真实数据集上的实验结果，在性能比较和可解释性方面，证明了我们的方法能够捕捉有意义的相关性。

    Instance-wise feature selection and ranking methods can achieve a good selection of task-friendly features for each sample in the context of neural networks. However, existing approaches that assume feature subsets to be independent are imperfect when considering the dependency between features. To address this limitation, we propose to incorporate the Gaussian copula, a powerful mathematical technique for capturing correlations between variables, into the current feature selection framework with no additional changes needed. Experimental results on both synthetic and real datasets, in terms of performance comparison and interpretability, demonstrate that our method is capable of capturing meaningful correlations.
    
[^13]: 预测一个积极健康老龄化应用程序的早期退学

    Predicting Early Dropouts of an Active and Healthy Ageing App. (arXiv:2308.00539v1 [cs.LG])

    [http://arxiv.org/abs/2308.00539](http://arxiv.org/abs/2308.00539)

    本文提出了一种机器学习方法，用于预测积极健康老龄化应用程序的早期退学。通过使用动态和静态特征构建分类模型以及采用过采样方法提高分类性能，我们获得了高质量的粘附度预测，并在科学挑战赛中获得了第一名。

    

    本文提出了一种机器学习方法，用于预测积极健康老龄化应用程序的早期退学。所提出的算法已提交给2022年IFMBE科学挑战赛，是IUPESM WC 2022的一部分。我们处理了给定的数据库并生成了七个数据集。我们使用预处理技术构建分类模型，使用动态和静态特征预测用户的粘附度。我们提交了11次官方运行，结果显示机器学习算法可以提供高质量的粘附度预测。根据结果，动态特征对模型的分类性能有积极影响。由于数据集的不平衡性质，我们采用了过采样方法，如SMOTE和ADASYN，以提高分类性能。过采样方法使结果获得了10%的显著改进。我们的方法在2022年IFMBE科学挑战赛中获得了第一名。

    In this work, we present a machine learning approach for predicting early dropouts of an active and healthy ageing app. The presented algorithms have been submitted to the IFMBE Scientific Challenge 2022, part of IUPESM WC 2022. We have processed the given database and generated seven datasets. We used pre-processing techniques to construct classification models that predict the adherence of users using dynamic and static features. We submitted 11 official runs and our results show that machine learning algorithms can provide high-quality adherence predictions. Based on the results, the dynamic features positively influence a model's classification performance. Due to the imbalanced nature of the dataset, we employed oversampling methods such as SMOTE and ADASYN to improve the classification performance. The oversampling approaches led to a remarkable improvement of 10\%. Our methods won first place in the IFMBE Scientific Challenge 2022.
    
[^14]: 动态特征下基于图嵌入的电力系统瞬态稳定性监测的监督对比学习

    Graph Embedding Dynamic Feature-based Supervised Contrastive Learning of Transient Stability for Changing Power Grid Topologies. (arXiv:2308.00537v1 [eess.SY])

    [http://arxiv.org/abs/2308.00537](http://arxiv.org/abs/2308.00537)

    提出了一种基于图嵌入动态特征的瞬态稳定性GEDF-SCL模型，利用监督对比学习结合GEDF来预测电力系统的瞬态稳定性，并考虑了拓扑信息。实验结果表明该模型具有良好的性能。

    

    在面对干扰时，准确的在线瞬态稳定性预测对于确保电力系统的稳定至关重要。传统的瞬态稳定性分析依赖于时间域仿真，不能快速适应电力网格拓扑的变化。为了将高维电力系统拓扑结构信息向量化为低维节点嵌入流数据，提出了基于图嵌入动态特征（GEDF）的瞬态稳定性GEDF-监督对比学习（GEDF-SCL）模型。该模型利用监督对比学习结合GEDF来预测瞬态稳定性，考虑了电力系统的拓扑信息。为了评估所提出的GEDF-SCL模型的性能，基于IEEE 39节点系统模型生成了拓扑结构不同的电力网格。通过在这些生成的电力系统拓扑上模拟N-1和N-$\bm{m}$-1故障，获取了瞬态运行数据。测试结果表明GEDF-SCL模型具有良好的瞬态稳定性预测性能。

    Accurate online transient stability prediction is critical for ensuring power system stability when facing disturbances. While traditional transient stablity analysis replies on the time domain simulations can not be quickly adapted to the power grid toplogy change. In order to vectorize high-dimensional power grid topological structure information into low-dimensional node-based graph embedding streaming data, graph embedding dynamic feature (GEDF) has been proposed. The transient stability GEDF-based supervised contrastive learning (GEDF-SCL) model uses supervised contrastive learning to predict transient stability with GEDFs, considering power grid topology information. To evaluate the performance of the proposed GEDF-SCL model, power grids of varying topologies were generated based on the IEEE 39-bus system model. Transient operational data was obtained by simulating N-1 and N-$\bm{m}$-1 contingencies on these generated power system topologies. Test result demonstrated that the GED
    
[^15]: 用生成对抗网络进行图形对比学习

    Graph Contrastive Learning with Generative Adversarial Network. (arXiv:2308.00535v1 [cs.LG])

    [http://arxiv.org/abs/2308.00535](http://arxiv.org/abs/2308.00535)

    该论文提出了一种利用生成对抗网络（GANs）进行图形对比学习的方法，通过学习视图的分布来提高图神经网络（GNN）的性能。

    

    图神经网络（GNN）通过监督式的端到端训练，在许多下游任务中展示了对利用节点表示的良好结果。为了解决现实应用中普遍存在的标签缺乏问题，通过最大化从原始图中生成的增强视图中节点之间的互信息，利用图对比学习（GCL）来训练带有有限或甚至没有标签的GNN。然而，现有文献中在视图生成中未考虑图的分布，导致忽视了大多数文献中未见边的情况，而实验证明这可以提高GCL的性能。因此，我们提出了将图生成对抗网络（GANs）用于学习GCL的视图分布，以便i）自动捕捉图的特征进行增强，并ii）联合训练图GAN模型和GCL模型。具体而言，我们提出了GACN，一种新颖的生成对抗网络模型，用于图形对比学习。

    Graph Neural Networks (GNNs) have demonstrated promising results on exploiting node representations for many downstream tasks through supervised end-to-end training. To deal with the widespread label scarcity issue in real-world applications, Graph Contrastive Learning (GCL) is leveraged to train GNNs with limited or even no labels by maximizing the mutual information between nodes in its augmented views generated from the original graph. However, the distribution of graphs remains unconsidered in view generation, resulting in the ignorance of unseen edges in most existing literature, which is empirically shown to be able to improve GCL's performance in our experiments. To this end, we propose to incorporate graph generative adversarial networks (GANs) to learn the distribution of views for GCL, in order to i) automatically capture the characteristic of graphs for augmentations, and ii) jointly train the graph GAN model and the GCL model. Specifically, we present GACN, a novel Generati
    
[^16]: 一种用于车辆轨迹和驾驶意图预测的新颖时序多门专家混合模型方法

    A Novel Temporal Multi-Gate Mixture-of-Experts Approach for Vehicle Trajectory and Driving Intention Prediction. (arXiv:2308.00533v1 [cs.LG])

    [http://arxiv.org/abs/2308.00533](http://arxiv.org/abs/2308.00533)

    本文提出了一种新颖的时序多门专家混合模型（TMMOE），以同时预测车辆轨迹和驾驶意图。该模型利用共享层和专家层进行特征提取和信息识别，实现了对纵向位置、横向位置和驾驶意图之间关系的综合预测。

    

    准确的车辆轨迹预测对于自动驾驶车辆和先进驾驶辅助系统至关重要。车辆轨迹预测包括纵向位置预测和横向位置预测这两个关键任务。驾驶意图与车辆运动之间存在着重要的相关性。现有的研究往往将这三个任务分开进行，没有考虑纵向位置、横向位置和驾驶意图之间的关系。本文提出了一种新颖的时序多门专家混合模型（TMMOE），以同时预测车辆轨迹和驾驶意图。该模型由共享层、专家层和全连接层组成。在模型中，共享层利用时序卷积网络（TCN）提取时序特征。然后，专家层根据三个任务的需要进行不同的信息识别。此外，全连接层用于预测车辆轨迹和驾驶意图之间的关系。

    Accurate Vehicle Trajectory Prediction is critical for automated vehicles and advanced driver assistance systems. Vehicle trajectory prediction consists of two essential tasks, i.e., longitudinal position prediction and lateral position prediction. There is a significant correlation between driving intentions and vehicle motion. In existing work, the three tasks are often conducted separately without considering the relationships between the longitudinal position, lateral position, and driving intention. In this paper, we propose a novel Temporal Multi-Gate Mixture-of-Experts (TMMOE) model for simultaneously predicting the vehicle trajectory and driving intention. The proposed model consists of three layers: a shared layer, an expert layer, and a fully connected layer. In the model, the shared layer utilizes Temporal Convolutional Networks (TCN) to extract temporal features. Then the expert layer is built to identify different information according to the three tasks. Moreover, the ful
    
[^17]: 变分标签相关增强方法用于拥塞预测

    Variational Label-Correlation Enhancement for Congestion Prediction. (arXiv:2308.00529v1 [cs.LG])

    [http://arxiv.org/abs/2308.00529](http://arxiv.org/abs/2308.00529)

    本文提出了一种名为VAriational Label-Correlation Enhancement for Congestion Prediction（{\ours}）的方法，用于准确预测集成电路中的拥塞情况。这种方法利用了格子之间的空间标签相关性，能够更好地识别设计缺陷，加快电路设计过程。

    

    大规模设计的物理设计过程是一项耗时的任务，通常需要几小时到几天才能完成，其中路由是最关键和复杂的步骤。随着集成电路（IC）的复杂性增加，对准确的路由质量预测的需求也增加。准确的拥塞预测有助于及早发现设计缺陷，从而加快电路设计并节约资源。尽管当前拥塞预测方法取得了一定的进展，但一个重要的方面却往往被忽视，即不同格子之间的空间标签相关性。空间标签相关性是电路设计的基本特征，一个格子的拥塞情况不仅与其自身有关，还受其相邻格子的影响。为了充分利用相邻格子之间的内在空间标签相关性，我们提出了一种新颖的方法，即VAriational Label-Correlation Enhancement for Congestion Prediction（{\ours}）。

    The physical design process of large-scale designs is a time-consuming task, often requiring hours to days to complete, with routing being the most critical and complex step. As the the complexity of Integrated Circuits (ICs) increases, there is an increased demand for accurate routing quality prediction. Accurate congestion prediction aids in identifying design flaws early on, thereby accelerating circuit design and conserving resources. Despite the advancements in current congestion prediction methodologies, an essential aspect that has been largely overlooked is the spatial label-correlation between different grids in congestion prediction. The spatial label-correlation is a fundamental characteristic of circuit design, where the congestion status of a grid is not isolated but inherently influenced by the conditions of its neighboring grids. In order to fully exploit the inherent spatial label-correlation between neighboring grids, we propose a novel approach, {\ours}, i.e., VAriati
    
[^18]: 高效的联邦学习：基于局部自适应修正优化器的线性加速

    Efficient Federated Learning via Local Adaptive Amended Optimizer with Linear Speedup. (arXiv:2308.00522v1 [cs.LG])

    [http://arxiv.org/abs/2308.00522](http://arxiv.org/abs/2308.00522)

    本论文提出了一种名为FedLADA的联邦局部自适应修正优化器，通过引入局部修正技术和动量项，解决了联邦学习中粗糙收敛和客户端漂移加剧的问题，具有线性加速度。

    

    自适应优化方法在分布式学习中取得了显著的成功，但将自适应优化器扩展到联邦学习中存在严重的效率问题，包括：（i）全局自适应优化器中由于梯度估计不准确导致的粗糙收敛；（ii）由于局部自适应优化器的局部过拟合导致的客户端漂移加剧。在本研究中，我们提出了一种新的基于动量的算法，利用全局梯度下降和局部自适应修正优化器来解决这些困难。具体而言，我们引入了一种局部修正技术到自适应优化器中，命名为联邦局部自适应修正优化器（FedLADA），它通过动量项估计上一轮通信中的全局平均偏差，并通过校正局部偏差来进一步提高经验训练速度并减轻异构过拟合问题。从理论上讲，我们建立了FedLADA的收敛速度，具有线性加速度

    Adaptive optimization has achieved notable success for distributed learning while extending adaptive optimizer to federated Learning (FL) suffers from severe inefficiency, including (i) rugged convergence due to inaccurate gradient estimation in global adaptive optimizer; (ii) client drifts exacerbated by local over-fitting with the local adaptive optimizer. In this work, we propose a novel momentum-based algorithm via utilizing the global gradient descent and locally adaptive amended optimizer to tackle these difficulties. Specifically, we incorporate a locally amended technique to the adaptive optimizer, named Federated Local ADaptive Amended optimizer (\textit{FedLADA}), which estimates the global average offset in the previous communication round and corrects the local offset through a momentum-like term to further improve the empirical training speed and mitigate the heterogeneous over-fitting. Theoretically, we establish the convergence rate of \textit{FedLADA} with a linear spee
    
[^19]: 使用多相CT结合神经距离和纹理感知变压器，提高胰腺癌预后预测的方法

    Improved Prognostic Prediction of Pancreatic Cancer Using Multi-Phase CT by Integrating Neural Distance and Texture-Aware Transformer. (arXiv:2308.00507v1 [eess.IV])

    [http://arxiv.org/abs/2308.00507](http://arxiv.org/abs/2308.00507)

    本文提出了一种新的方法来改善胰腺癌的预后预测。该方法使用了可学习的神经距离来描述肿瘤与血管之间的关系，并通过融合局部和全局特征来改进多相CT影像中的肿瘤纹理特征提取。实验表明，该方法在预后预测中取得了较好的效果。

    

    胰腺导管腺癌（PDAC）是一种高度致命的癌症，肿瘤-血管受累极大影响患者的可切除性和总体生存率。然而，当前的预后预测方法未能明确准确地调查肿瘤与附近重要血管之间的关系。本文提出了一种新颖的可学习的神经距离，描述了不同患者CT影像中肿瘤与血管之间的精确关系，并将其作为预后预测的主要特征。此外，不同于现有模型在动态对比增强CT成像上利用CNN或LSTM来利用肿瘤增强模式，我们通过融合局部和全局特征使用CNN和变压器模块，在多相对比增强CT中改进了动态与肿瘤相关的纹理特征提取，进一步增强了跨多相CT影像提取的特征。我们对所提出的方法进行了广泛评估和比较。

    Pancreatic ductal adenocarcinoma (PDAC) is a highly lethal cancer in which the tumor-vascular involvement greatly affects the resectability and, thus, overall survival of patients. However, current prognostic prediction methods fail to explicitly and accurately investigate relationships between the tumor and nearby important vessels. This paper proposes a novel learnable neural distance that describes the precise relationship between the tumor and vessels in CT images of different patients, adopting it as a major feature for prognosis prediction. Besides, different from existing models that used CNNs or LSTMs to exploit tumor enhancement patterns on dynamic contrast-enhanced CT imaging, we improved the extraction of dynamic tumor-related texture features in multi-phase contrast-enhanced CT by fusing local and global features using CNN and transformer modules, further enhancing the features extracted across multi-phase CT images. We extensively evaluated and compared the proposed method
    
[^20]: 可解释的文本文档的图谱聚类

    Explainable Graph Spectral Clustering of Text Documents. (arXiv:2308.00504v1 [cs.LG])

    [http://arxiv.org/abs/2308.00504](http://arxiv.org/abs/2308.00504)

    本文提出了一种可解释的文本文档的图谱聚类方法，通过展示组合拉普拉斯嵌入、K嵌入和词向量空间嵌入之间的等价性，构建了文本内容和聚类结果之间的桥梁。

    

    光谱聚类方法以其能够表示不同形状、密度等的聚类而闻名。然而，将这些算法应用于文本文档时，其结果很难向用户解释，特别是由于在光谱空间中的嵌入与文档内容没有明显的关系。因此，迫切需要研究解释聚类结果的方法。本文提出了对此目标的贡献。我们提出了解释基于组合拉普拉斯的图谱聚类结果的方法。该方法基于展示组合拉普拉斯嵌入、K嵌入（本文提出）和词向量空间嵌入的（近似）等价性。从而构建了文本内容和聚类结果之间的桥梁。我们为这种方法提供了理论背景。我们进行了实验研究，结果表明，在有利条件下，K嵌入很好地近似了拉普拉斯嵌入。

    Spectral clustering methods are known for their ability to represent clusters of diverse shapes, densities etc. However, results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Therefore there is an urgent need to elaborate methods for explaining the outcome of the clustering. This paper presents a contribution towards this goal. We present a proposal of explanation of results of combinatorial Laplacian based graph spectral clustering. It is based on showing (approximate) equivalence of combinatorial Laplacian embedding, $K$-embedding (proposed in this paper) and term vector space embedding. Hence a bridge is constructed between the textual contents and the clustering results. We provide theoretical background for this approach. We performed experimental study showing that $K$-embedding approximates well Laplacian embedding under favourable blo
    
[^21]: DINO-CXR: 基于视觉变换器的自监督方法用于胸部X射线分类

    DINO-CXR: A self supervised method based on vision transformer for chest X-ray classification. (arXiv:2308.00475v1 [eess.IV])

    [http://arxiv.org/abs/2308.00475](http://arxiv.org/abs/2308.00475)

    DINO-CXR是一种基于视觉变换器的自监督方法，可用于胸部X射线分类，并通过比较分析显示出在肺炎和COVID-19检测方面的有效性和优越性，同时需要较少的标注数据。

    

    在医学成像方法的发展中，标注有限的胸部X射线数据集是一个重要的瓶颈。自监督学习 (SSL) 可以通过在无标签数据上训练模型来缓解这个问题。此外，自监督预训练在自然图像的视觉识别方面取得了有希望的结果，但在医学图像分析方面并没有得到足够的关注。在这项工作中，我们提出了一种自监督方法 DINO-CXR，它是一种基于视觉变换器的自监督方法 DINO 在胸部X射线分类方面的新的改进。通过比较分析，我们展示了该方法在肺炎和COVID-19检测方面的有效性。通过定量分析还表明，该方法在准确率方面胜过现有的方法，并在AUC和F-1分数方面达到可比较的结果，同时需要较少的标注数据。

    The limited availability of labeled chest X-ray datasets is a significant bottleneck in the development of medical imaging methods. Self-supervised learning (SSL) can mitigate this problem by training models on unlabeled data. Furthermore, self-supervised pretraining has yielded promising results in visual recognition of natural images but has not been given much consideration in medical image analysis. In this work, we propose a self-supervised method, DINO-CXR, which is a novel adaptation of a self-supervised method, DINO, based on a vision transformer for chest X-ray classification. A comparative analysis is performed to show the effectiveness of the proposed method for both pneumonia and COVID-19 detection. Through a quantitative analysis, it is also shown that the proposed method outperforms state-of-the-art methods in terms of accuracy and achieves comparable results in terms of AUC and F-1 score while requiring significantly less labeled data.
    
[^22]: 最后一层的训练是否足以应对虚假相关性？

    Is Last Layer Re-Training Truly Sufficient for Robustness to Spurious Correlations?. (arXiv:2308.00473v1 [cs.LG])

    [http://arxiv.org/abs/2308.00473](http://arxiv.org/abs/2308.00473)

    通过对最后一层进行重新训练，Deep Feature Reweighting（DFR）方法可以提高模型在存在虚假相关性的数据中的准确性，但其应用于实际医学数据时存在一定局限性。

    

    以经验风险最小化（ERM）训练的模型已被知晓学会依赖虚假特征，即它们的预测基于与类别标签强相关但缺乏因果推理的非期望辅助特征。这种行为尤其在相关类别的样本组中，可能没有这些虚假特征或者相反类别的样本中存在这些虚假特征时，导致准确性的下降。最近提出的深度特征重加权（DFR）方法提高了这些最差样本组的准确性。基于ERM模型可以足够好地学习核心特征的主要论点，DFR只需对分类模型的最后一层进行小规模平衡数据集的重新训练。在本研究中，我们检验了DFR在医学领域真实数据中的适用性。此外，我们对最后一层重新训练有效性背后的推理进行了调查，结果表明尽管DFR具有提高最差样本组准确性的潜力，但其实现方式存在局限性。

    Models trained with empirical risk minimization (ERM) are known to learn to rely on spurious features, i.e., their prediction is based on undesired auxiliary features which are strongly correlated with class labels but lack causal reasoning. This behavior particularly degrades accuracy in groups of samples of the correlated class that are missing the spurious feature or samples of the opposite class but with the spurious feature present. The recently proposed Deep Feature Reweighting (DFR) method improves accuracy of these worst groups. Based on the main argument that ERM mods can learn core features sufficiently well, DFR only needs to retrain the last layer of the classification model with a small group-balanced data set. In this work, we examine the applicability of DFR to realistic data in the medical domain. Furthermore, we investigate the reasoning behind the effectiveness of last-layer retraining and show that even though DFR has the potential to improve the accuracy of the wors
    
[^23]: 镜像自然进化策略

    Mirror Natural Evolution Strategies. (arXiv:2308.00469v1 [cs.LG])

    [http://arxiv.org/abs/2308.00469](http://arxiv.org/abs/2308.00469)

    这篇论文研究了基于零阶查询近似一阶和二阶信息的零阶优化算法，提出了一种新的重参数化目标函数并给出了对应的最小化算法，名为MiNES（镜像下降自然进化策略）。

    

    零阶优化在机器学习应用中被广泛使用。然而，对于近似梯度使用零阶函数值差异的零阶优化算法的理论研究集中在随机方向上。近似梯度和Hessian信息的零阶查询算法的理论研究较少。本文主要研究利用零阶查询近似一阶和二阶信息的零阶优化理论。首先，我们提出了一个新的重参数化目标函数，其参数为$(\mu, \Sigma)$。这个重参数化目标函数在原目标函数的最小化器和Hessian的逆之处分别达到其最优值，但有小的扰动。因此，我们提出了一种新的算法来最小化我们提出的重参数化目标函数，我们称之为MiNES（镜像下降自然进化策略）。

    The zeroth-order optimization has been widely used in machine learning applications. However, the theoretical study of the zeroth-order optimization focus on the algorithms which approximate (first-order) gradients using (zeroth-order) function value difference at a random direction. The theory of algorithms which approximate the gradient and Hessian information by zeroth-order queries is much less studied. In this paper, we focus on the theory of zeroth-order optimization which utilizes both the first-order and second-order information approximated by the zeroth-order queries. We first propose a novel reparameterized objective function with parameters $(\mu, \Sigma)$. This reparameterized objective function achieves its optimum at the minimizer and the Hessian inverse of the original objective function respectively, but with small perturbations. Accordingly, we propose a new algorithm to minimize our proposed reparameterized objective, which we call \texttt{MiNES} (mirror descent natu
    
[^24]: 一个对深度学习模型的补丁稳健性认证的大多数不变方法

    A Majority Invariant Approach to Patch Robustness Certification for Deep Learning Models. (arXiv:2308.00452v1 [cs.LG])

    [http://arxiv.org/abs/2308.00452](http://arxiv.org/abs/2308.00452)

    该论文提出了MajorCert，通过找到在底层分类器中由同一个补丁区域在同一个样本上可以操纵的所有可能标签集合，并检查它们的大多数不变性来对样本进行补丁稳健性认证。

    

    补丁稳健性认证确保在给定样本上，没有补丁能够通过操纵深度学习模型来预测不同的标签。然而，现有的技术不能对无法在分类器或者补丁区域水平上达到严格标准的样本进行认证。本文提出了MajorCert。MajorCert首先找到在底层分类器中由同一个补丁区域在同一个样本上可以操纵的所有可能的标签集合，然后逐个枚举它们的组合，并最终检查所有这些组合的大多数不变性是否完整以对样本进行认证。

    Patch robustness certification ensures no patch within a given bound on a sample can manipulate a deep learning model to predict a different label. However, existing techniques cannot certify samples that cannot meet their strict bars at the classifier or patch region levels. This paper proposes MajorCert. MajorCert firstly finds all possible label sets manipulatable by the same patch region on the same sample across the underlying classifiers, then enumerates their combinations element-wise, and finally checks whether the majority invariant of all these combinations is intact to certify samples.
    
[^25]: SelfCheck: 使用LLMs自检其逐步推理的创新

    SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])

    [http://arxiv.org/abs/2308.00436](http://arxiv.org/abs/2308.00436)

    本论文研究了使用LLMs自检逐步推理的能力，提出了一种零-shot验证方案，成功识别错误并提高了问答性能。

    

    最近大型语言模型（LLMs）的进展，尤其是链式思维（CoT）的发明，使得解决推理问题成为可能。然而，即使最强大的LLMs仍然难以处理需要非线性思维和多步推理的复杂问题。在这项工作中，我们探讨了LLMs是否具有识别自己错误的能力，而无需依赖外部资源。具体而言，我们研究了它们是否可以用于识别逐步推理中的个别错误。为此，我们提出了一种零-shot验证方案以识别此类错误。然后，我们使用此验证方案来改进问答性能，通过对不同生成的答案进行加权投票。我们在三个数学数据集-GSM8K，MathQA和MATH上测试了该方法，并发现它成功识别错误，并进而提高了最终的预测性能。

    The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
    
[^26]: 处理神经图表摘要中的幻觉

    Tackling Hallucinations in Neural Chart Summarization. (arXiv:2308.00399v1 [cs.CL])

    [http://arxiv.org/abs/2308.00399](http://arxiv.org/abs/2308.00399)

    本研究解决了神经图表摘要中的幻觉问题，通过自然语言推理预处理训练数据，显著减少了幻觉的产生，并通过缩短远距离依赖关系和添加与图表相关的信息来提高整体性能。

    

    文本生成中的幻觉是指系统产生的文本未与输入关联。本研究解决了神经图表摘要中的幻觉问题。我们的分析表明，图表摘要训练数据集中的目标端经常包含额外的信息，导致幻觉的产生。我们提出了一种基于自然语言推理(NLI)的方法来预处理训练数据，并通过人工评估表明，我们的方法显著减少了幻觉的产生。我们还发现，缩短输入序列中的远距离依赖关系，并添加与图表相关的信息，如标题和图例，可以提高整体性能。

    Hallucinations in text generation occur when the system produces text that is not grounded in the input. In this work, we tackle the problem of hallucinations in neural chart summarization. Our analysis shows that the target side of chart summarization training datasets often contains additional information, leading to hallucinations. We propose a natural language inference (NLI) based method to preprocess the training data and show through human evaluation that our method significantly reduces hallucinations. We also found that shortening long-distance dependencies in the input sequence and adding chart-related information like title and legends improves the overall performance.
    
[^27]: 在AIOps领域中时间序列异常检测方法的调查

    A Survey of Time Series Anomaly Detection Methods in the AIOps Domain. (arXiv:2308.00393v1 [cs.LG])

    [http://arxiv.org/abs/2308.00393](http://arxiv.org/abs/2308.00393)

    在AIOps领域中，通过调查时间序列异常检测方法，研究人员综述了互联网服务中大量监测KPI的重要性，并讨论了未来的发展方向。

    

    基于互联网的服务取得了显著的成功，产生了大量的监测关键性能指标(KPI)，这些指标以单变量或多变量时间序列的形式呈现。监测和分析这些时间序列对于研究人员、服务运营商和值班工程师来检测表明服务故障或重大事件的异常值或异常情况至关重要。出现了许多先进的异常检测方法来解决可用性和性能问题。本综述全面概述了在使用人工智能技术自动化和优化操作流程的信息技术运营(AIOps)领域中的时间序列异常检测，此外，也探讨了基于最新进展的真实世界和下一代时间序列异常检测的未来方向。

    Internet-based services have seen remarkable success, generating vast amounts of monitored key performance indicators (KPIs) as univariate or multivariate time series. Monitoring and analyzing these time series are crucial for researchers, service operators, and on-call engineers to detect outliers or anomalies indicating service failures or significant events. Numerous advanced anomaly detection methods have emerged to address availability and performance issues. This review offers a comprehensive overview of time series anomaly detection in Artificial Intelligence for IT operations (AIOps), which uses AI capabilities to automate and optimize operational workflows. Additionally, it explores future directions for real-world and next-generation time-series anomaly detection based on recent advancements.
    
[^28]: 交通流量预测的反事实图变压器模型

    Counterfactual Graph Transformer for Traffic Flow Prediction. (arXiv:2308.00391v1 [cs.LG])

    [http://arxiv.org/abs/2308.00391](http://arxiv.org/abs/2308.00391)

    本研究提出了一种针对交通流量预测的反事实图变压器模型（CGT），通过实例级解释器和扰动掩码生成器，可以获得具有解释性的预测模型和重要子图。

    

    交通流量预测（TFP）是智能交通系统（ITS）的一个基本问题，它模拟了交通流量的潜在时空依赖关系，用于潜在拥堵预测。最近的基于图的模型采用了多种注意机制，取得了有希望的性能。然而，现有的交通流量预测方法往往会从数据集中继承偏差模式，并缺乏可解释性。为此，我们提出了一种专门为TFP设计的反事实图变压器（CGT）模型，该模型具有实例级解释器（例如，寻找重要子图）。我们设计了一个对输入传感器特征在时间维度上和图变压器模块上的图结构进行扰动的生成器，以获取空间和时间上的反事实解释。通过搜索输入数据特征和图结构上的最佳扰动掩码，我们可以获取简洁而主导的数据或图边链接，供后续使用。

    Traffic flow prediction (TFP) is a fundamental problem of the Intelligent Transportation System (ITS), as it models the latent spatial-temporal dependency of traffic flow for potential congestion prediction. Recent graph-based models with multiple kinds of attention mechanisms have achieved promising performance. However, existing methods for traffic flow prediction tend to inherit the bias pattern from the dataset and lack interpretability. To this end, we propose a Counterfactual Graph Transformer (CGT) model with an instance-level explainer (e.g., finding the important subgraphs) specifically designed for TFP. We design a perturbation mask generator over input sensor features at the time dimension and the graph structure on the graph transformer module to obtain spatial and temporal counterfactual explanations. By searching the optimal perturbation masks on the input data feature and graph structures, we can obtain the concise and dominant data or graph edge links for the subsequent
    
[^29]: 带有不确定区域预测的形状完成

    Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])

    [http://arxiv.org/abs/2308.00377](http://arxiv.org/abs/2308.00377)

    该论文提出了两种方法来处理在给定模糊物体视图时可能存在的物体部分的不确定区域预测问题。研究表明这些方法可以作为任何预测空间占用的方法的直接扩展，通过后处理占用评分或直接预测不确定性指标来实现。这些方法与已知的概率形状完成方法进行了比较，并使用自动生成的深度图像数据集进行了验证。

    

    形状完成，即从部分观测预测物体的完整几何形状，对于几个下游任务非常重要，尤其是机器人操作。当基于物体形状重建进行规划或实际抓取的预测时，指示严重几何不确定性是必不可少的。特别是在给定模糊的物体视图时，在整个物体部分存在 irreducible uncertainty 的扩展区域。为了处理这种重要情况，我们提出了两种新方法来预测这些不确定区域，这两种方法都可以作为预测局部空间占用的任何方法的直接扩展，一种是通过后处理占用评分，另一种是通过直接预测不确定性指标。我们将这些方法与两种已知的概率形状完成方法进行了比较。此外，我们还生成了一个基于ShapeNet的数据集，其中包含了真实渲染的物体视图深度图像及其带有地面真值标注。

    Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
    
[^30]: 使用低秩近似有效地学习Green函数

    Learning Green's Function Efficiently Using Low-Rank Approximations. (arXiv:2308.00350v1 [cs.LG])

    [http://arxiv.org/abs/2308.00350](http://arxiv.org/abs/2308.00350)

    该论文提出了一种使用低秩分解学习Green函数的方法，通过使用领域数据和蒙特卡洛样本进行分别学习和积分逼近，从而提高了计算效率并保持了准确性。

    

    使用深度学习模型学习Green函数可以解决不同类型的偏微分方程。使用深度学习学习Green函数的一个实际限制是重复的计算密集的蒙特卡洛积分逼近。我们提出一种通过低秩分解学习Green函数的方法，该方法通过使用领域数据进行评估和蒙特卡洛样本进行积分逼近，从而消除了冗余计算。通过实验证明，与MOD-Net相比，所提出的方法提高了计算时间，同时在准确性上与PINNs和MOD-Net相当。

    Learning the Green's function using deep learning models enables to solve different classes of partial differential equations. A practical limitation of using deep learning for the Green's function is the repeated computationally expensive Monte-Carlo integral approximations. We propose to learn the Green's function by low-rank decomposition, which results in a novel architecture to remove redundant computations by separate learning with domain data for evaluation and Monte-Carlo samples for integral approximation. Using experiments we show that the proposed method improves computational time compared to MOD-Net while achieving comparable accuracy compared to both PINNs and MOD-Net.
    
[^31]: 基于深度神经网络不确定性估计的动态集成选择方法用于对抗鲁棒性

    Dynamic ensemble selection based on Deep Neural Network Uncertainty Estimation for Adversarial Robustness. (arXiv:2308.00346v1 [cs.LG])

    [http://arxiv.org/abs/2308.00346](http://arxiv.org/abs/2308.00346)

    本文针对深度神经网络在实际应用中的鲁棒性问题，提出了一种通过动态集成选择技术在模型层面上改善模型的防御主动性的方法。通过在训练阶段引入Dirichlet分布作为子模型预测分布的先验，并在轻量级子模型下引入参数空间的多样性约束，构建备选的集成模型空间。在测试阶段，动态选取特定的子模型来提高模型的鲁棒性。

    

    深度神经网络在图像识别方面取得了显著的效果，但在实际应用中面临着大量数据不确定性的识别鲁棒性薄弱的问题。这种不确定性是由环境中不可避免的噪声以及可能的对抗性攻击引起的。动态方法可以有效地改善对抗性示例攻防竞赛中的防御主动性。与以前依赖于输入或决策的动态方法不同，本研究通过动态集成选择技术在模型层面上探索动态属性，以进一步保护模型免受白盒攻击并提高鲁棒性。具体而言，在训练阶段，将Dirichlet分布作为子模型预测分布的先验，并在轻量级子模型下引入参数空间的多样性约束，以构建备选集成模型空间。在测试阶段，特定的子模型被动态选取

    The deep neural network has attained significant efficiency in image recognition. However, it has vulnerable recognition robustness under extensive data uncertainty in practical applications. The uncertainty is attributed to the inevitable ambient noise and, more importantly, the possible adversarial attack. Dynamic methods can effectively improve the defense initiative in the arms race of attack and defense of adversarial examples. Different from the previous dynamic method depend on input or decision, this work explore the dynamic attributes in model level through dynamic ensemble selection technology to further protect the model from white-box attacks and improve the robustness. Specifically, in training phase the Dirichlet distribution is apply as prior of sub-models' predictive distribution, and the diversity constraint in parameter space is introduced under the lightweight sub-models to construct alternative ensembel model spaces. In test phase, the certain sub-models are dynamic
    
[^32]: 在部分观测条件下监控算法公平性

    Monitoring Algorithmic Fairness under Partial Observations. (arXiv:2308.00341v1 [cs.AI])

    [http://arxiv.org/abs/2308.00341](http://arxiv.org/abs/2308.00341)

    本研究将算法公平性监控扩展到部分观察到的马尔可夫链模型的系统，并且可以监控包含对事件序列上数值函数的期望值的算术表达式的公平性属性。

    

    随着人工智能和机器学习软件在决策中的应用越来越多地影响人类，它们在决策过程中保持公平和无偏的要求变得至关重要。为了补充设计时的偏差缓解措施，近期引入了一些运行时验证技术来监控部署系统的算法公平性。之前的监控技术假设对（未知的）被监控系统的状态具有完全可观测性。此外，它们只能监控被指定为不同事件的概率的算术表达式的公平性属性。在这项工作中，我们将公平性监控扩展到部分观察到的马尔可夫链（POMC）模型的系统，并且针对包含对事件序列上的数值函数的期望值的算术表达式的规范。我们仅做出的假设是基础POMC是非周期性的并且起始于稳定分布，且对于其混合时间有一个已知的上界。

    As AI and machine-learned software are used increasingly for making decisions that affect humans, it is imperative that they remain fair and unbiased in their decisions. To complement design-time bias mitigation measures, runtime verification techniques have been introduced recently to monitor the algorithmic fairness of deployed systems. Previous monitoring techniques assume full observability of the states of the (unknown) monitored system. Moreover, they can monitor only fairness properties that are specified as arithmetic expressions over the probabilities of different events. In this work, we extend fairness monitoring to systems modeled as partially observed Markov chains (POMC), and to specifications containing arithmetic expressions over the expected values of numerical functions on event sequences. The only assumptions we make are that the underlying POMC is aperiodic and starts in the stationary distribution, with a bound on its mixing time being known. These assumptions enab
    
[^33]: 针对混合整数规划生成可行解的阈值感知学习

    Threshold-aware Learning to Generate Feasible Solutions for Mixed Integer Programs. (arXiv:2308.00327v1 [math.OC])

    [http://arxiv.org/abs/2308.00327](http://arxiv.org/abs/2308.00327)

    针对混合整数规划问题，本研究提出了一种阈值感知学习的方法，通过优化变量分配率来提高可行解的质量。

    

    在有限时间内找到高质量的组合优化问题的可行解是具有挑战性的，因为其具有离散的特性。最近，越来越多的机器学习方法被用来解决组合优化问题。神经网络下潜 (Neural diving，ND) 是一种学习方法，用于生成混合整数规划问题中的部分离散变量赋值。然而，ND的一个主要缺点是机器学习和混合整数规划目标之间存在较大差异，即变量值分类准确度与原始解的界的差异。我们的研究发现，特定范围的变量分配率（覆盖率）会产生高质量的可行解，我们建议优化覆盖率来弥补学习和混合整数规划目标之间的差距。因此，我们提出了一种事后方法和一种基于学习的方法来优化覆盖率。我们方法的一个关键思想是共同学习限制覆盖率搜索空间。

    Finding a high-quality feasible solution to a combinatorial optimization (CO) problem in a limited time is challenging due to its discrete nature. Recently, there has been an increasing number of machine learning (ML) methods for addressing CO problems. Neural diving (ND) is one of the learning-based approaches to generating partial discrete variable assignments in Mixed Integer Programs (MIP), a framework for modeling CO problems. However, a major drawback of ND is a large discrepancy between the ML and MIP objectives, i.e., variable value classification accuracy over primal bound. Our study investigates that a specific range of variable assignment rates (coverage) yields high-quality feasible solutions, where we suggest optimizing the coverage bridges the gap between the learning and MIP objectives. Consequently, we introduce a post-hoc method and a learning-based approach for optimizing the coverage. A key idea of our approach is to jointly learn to restrict the coverage search spac
    
[^34]: 像素到策略：用于内部和跨游戏强化学习的DQN编码器

    Pixel to policy: DQN Encoders for within & cross-game reinforcement learning. (arXiv:2308.00318v1 [cs.LG])

    [http://arxiv.org/abs/2308.00318](http://arxiv.org/abs/2308.00318)

    本文研究了在强化学习中利用共享结构的DQN编码器的性能。通过迁移学习，该模型在不同任务和环境中学习可转移的策略，实现了更高效的学习和更好的性能。在多个游戏环境中，该模型的平均回合奖励为46.16，在20,000次回合内甚至超过了人类水平表现。

    

    强化学习可以应用于各种任务和环境。许多这些环境具有相似的共享结构，可以利用这些结构提高其他任务上的RL性能。迁移学习可以利用这种共享结构，通过学习可在不同任务和环境之间转移的策略，可以更有效地学习并在各种任务上改进性能。本研究探讨并比较了从零开始训练的RL模型和迁移学习不同方法的性能。此外，该研究还探讨了在多个游戏环境中训练的模型的性能，旨在开发通用的游戏代理以及使用DQN对预训练的编码器进行迁移学习，并在相同或不同游戏上进行训练。我们的DQN模型在平均回合奖励上达到了46.16，甚至超过了人类水平表现，仅仅使用了20,000次回合。

    Reinforcement Learning can be applied to various tasks, and environments. Many of these environments have a similar shared structure, which can be exploited to improve RL performance on other tasks. Transfer learning can be used to take advantage of this shared structure, by learning policies that are transferable across different tasks and environments and can lead to more efficient learning as well as improved performance on a wide range of tasks. This work explores as well as compares the performance between RL models being trained from the scratch and on different approaches of transfer learning. Additionally, the study explores the performance of a model trained on multiple game environments, with the goal of developing a universal game-playing agent as well as transfer learning a pre-trained encoder using DQN, and training it on the same game or a different game. Our DQN model achieves a mean episode reward of 46.16 which even beats the human-level performance with merely 20k epi
    
[^35]: 双重稳健的实例重新加权对抗训练

    Doubly Robust Instance-Reweighted Adversarial Training. (arXiv:2308.00311v1 [cs.LG])

    [http://arxiv.org/abs/2308.00311](http://arxiv.org/abs/2308.00311)

    本文提出了一种新颖的双重稳健的实例重新加权对抗训练框架，通过分布鲁棒优化（DRO）技术获得重要性权重，并在最脆弱的示例上提高稳健性。

    

    在有限的模型容量下，为对抗性数据分配重要性权重在训练对抗性稳健网络方面取得了巨大成功。然而，现有的实例重新加权对抗训练方法严重依赖于启发式算法和/或几何解释来确定这些重要性权重，使得这些算法缺乏严格的理论解释/保证。此外，最近的研究表明，对抗训练在训练分布中的稳健性表现非均匀，例如，某些类别的数据点比其他类别更容易受到对抗性攻击。为了解决这两个问题，本文提出了一种新颖的双重稳健的实例重新加权对抗训练框架，通过探索分布鲁棒优化（DRO）技术来获得重要性权重，并在最脆弱的示例上提高稳健性。

    Assigning importance weights to adversarial data has achieved great success in training adversarially robust networks under limited model capacity. However, existing instance-reweighted adversarial training (AT) methods heavily depend on heuristics and/or geometric interpretations to determine those importance weights, making these algorithms lack rigorous theoretical justification/guarantee. Moreover, recent research has shown that adversarial training suffers from a severe non-uniform robust performance across the training distribution, e.g., data points belonging to some classes can be much more vulnerable to adversarial attacks than others. To address both issues, in this paper, we propose a novel doubly-robust instance reweighted AT framework, which allows to obtain the importance weights via exploring distributionally robust optimization (DRO) techniques, and at the same time boosts the robustness on the most vulnerable examples. In particular, our importance weights are obtained
    
[^36]: GradOrth:一种简单而高效的通过梯度正交投影进行外部分布检测的方法

    GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients. (arXiv:2308.00310v1 [cs.CV])

    [http://arxiv.org/abs/2308.00310](http://arxiv.org/abs/2308.00310)

    GradOrth是一种简单而高效的方法，通过计算梯度在内部分布数据的低秩子空间中的投影范数来检测外部分布数据。这种方法展示了出色的性能。

    

    在现实世界的应用中，检测外部分布（OOD）数据对于确保机器学习模型的安全部署至关重要。然而，现有的OOD检测方法主要依赖于特征图或完整梯度空间信息来推导OOD分数，忽视了预训练网络中最重要的参数在内部分布（ID）数据中的作用。在本研究中，我们提出了一种名为GradOrth的新方法，以便基于一个有趣的观察结果，即用于识别OOD数据的重要特征位于内部分布（ID）数据的低秩子空间中。具体而言，我们通过计算在内部分布数据中被认为重要的子空间中的梯度投影范数来识别OOD数据。大的正交投影值（即小的投影值）表明样本为OOD，因为它捕捉到了ID数据的弱相关性。 这种简单而有效的方法表现出了出色的性能，展示了一个值得注意的创新。

    Detecting out-of-distribution (OOD) data is crucial for ensuring the safe deployment of machine learning models in real-world applications. However, existing OOD detection approaches primarily rely on the feature maps or the full gradient space information to derive OOD scores neglecting the role of most important parameters of the pre-trained network over in-distribution (ID) data. In this study, we propose a novel approach called GradOrth to facilitate OOD detection based on one intriguing observation that the important features to identify OOD data lie in the lower-rank subspace of in-distribution (ID) data. In particular, we identify OOD data by computing the norm of gradient projection on the subspaces considered important for the in-distribution data. A large orthogonal projection value (i.e. a small projection value) indicates the sample as OOD as it captures a weak correlation of the ID data. This simple yet effective method exhibits outstanding performance, showcasing a notabl
    
[^37]: 无监督评估度量用于实践和自动领域自适应的研究

    A Study of Unsupervised Evaluation Metrics for Practical and Automatic Domain Adaptation. (arXiv:2308.00287v1 [cs.CV])

    [http://arxiv.org/abs/2308.00287](http://arxiv.org/abs/2308.00287)

    这项研究关注于无监督领域自适应中的评估度量。通过将源准确率纳入度量中，并使用一种新的MLP分类器进行改进，该研究解决了原有度量的不足之处，并将其与数据增强相结合，改进了模型的质量评估。

    

    无监督领域自适应（UDA）方法可以在没有标签的情况下将模型转移到目标领域。然而，这些方法需要一个标记的目标验证集用于超参数调优和模型选择。本文旨在寻找一种能够评估转移模型质量的评估度量，而无需访问目标验证标签。我们首先基于模型预测的互信息的度量。通过实证分析，我们发现了这个度量的三个普遍问题：1）它没有考虑源结构；2）它容易受到攻击；3）它无法检测到由于源和目标特征过度对齐导致的负迁移。为了解决前两个问题，我们将源准确率纳入度量中，并使用一种在训练过程中保持独立的新的多层感知器（MLP）分类器，从而显著改进了结果。为了解决最后一个问题，我们将这个增强的度量与数据增强相结合，得到一个n

    Unsupervised domain adaptation (UDA) methods facilitate the transfer of models to target domains without labels. However, these methods necessitate a labeled target validation set for hyper-parameter tuning and model selection. In this paper, we aim to find an evaluation metric capable of assessing the quality of a transferred model without access to target validation labels. We begin with the metric based on mutual information of the model prediction. Through empirical analysis, we identify three prevalent issues with this metric: 1) It does not account for the source structure. 2) It can be easily attacked. 3) It fails to detect negative transfer caused by the over-alignment of source and target features. To address the first two issues, we incorporate source accuracy into the metric and employ a new MLP classifier that is held out during training, significantly improving the result. To tackle the final issue, we integrate this enhanced metric with data augmentation, resulting in a n
    
[^38]: 通过超贝叶斯优化进行预测模型

    Predictive Modeling through Hyper-Bayesian Optimization. (arXiv:2308.00285v1 [cs.LG])

    [http://arxiv.org/abs/2308.00285](http://arxiv.org/abs/2308.00285)

    本文提出了一种将模型选择和贝叶斯优化相结合的方法，以更快地达到函数的最优解。

    

    模型选择是基于模型的优化技术（如贝叶斯优化）最关键的问题之一。当前的方法通常将模型选择视为一个估计问题，需要定期更新来适应优化迭代中得到的观测结果。本文提出了一种新的方法来同时实现高效性。具体来说，我们提出了一种将模型选择和贝叶斯优化相结合的方法，以达到更快地达到函数的最优解。算法在模型空间和函数空间之间来回移动，其中推荐模型的好坏由一个评分函数来衡量并反馈，这个函数捕捉了模型在函数空间中对收敛的帮助程度。评分函数的推导方式使其抵消了贝叶斯优化在函数空间中的动态性质的影响，从而使模型选择问题保持稳定。这种来回迭代导致模型选择和贝叶斯优化都能快速收敛。

    Model selection is an integral problem of model based optimization techniques such as Bayesian optimization (BO). Current approaches often treat model selection as an estimation problem, to be periodically updated with observations coming from the optimization iterations. In this paper, we propose an alternative way to achieve both efficiently. Specifically, we propose a novel way of integrating model selection and BO for the single goal of reaching the function optima faster. The algorithm moves back and forth between BO in the model space and BO in the function space, where the goodness of the recommended model is captured by a score function and fed back, capturing how well the model helped convergence in the function space. The score function is derived in such a way that it neutralizes the effect of the moving nature of the BO in the function space, thus keeping the model selection problem stationary. This back and forth leads to quick convergence for both model selection and BO i
    
[^39]: CLAMS:一种用于估计视觉聚类中感知变异性的聚类模糊度测量方法

    CLAMS: A Cluster Ambiguity Measure for Estimating Perceptual Variability in Visual Clustering. (arXiv:2308.00284v1 [cs.HC])

    [http://arxiv.org/abs/2308.00284](http://arxiv.org/abs/2308.00284)

    本研究提出一种名为CLAMS的聚类模糊度测量方法，用于估计视觉聚类中的感知变异性。通过定性研究，我们确定了影响聚类的关键因素，并通过回归模块对聚类的模糊度进行估计。

    

    视觉聚类是散点图中常见的感知任务，支持各种分析任务（例如聚类识别）。然而，即使使用相同的散点图，由于个体间的差异和模糊的聚类边界，感知聚类的方式（即进行视觉聚类）可能会有所不同。尽管这种感知变异性对于基于视觉聚类的数据分析的可靠性提出了疑问，但我们缺乏一种系统评估这种变异性的有效方法。在这项研究中，我们研究了进行视觉聚类中的感知变异性，我们称之为聚类模糊度。为此，我们引入CLAMS，一种用于自动预测单色散点图中聚类模糊度的数据驱动的视觉质量测量方法。我们首先进行了定性研究，以确定影响聚类的视觉分离的关键因素（例如聚类间的接近度或大小差异）。基于研究结果，我们部署了一个回归模块来估计聚类的模糊度。

    Visual clustering is a common perceptual task in scatterplots that supports diverse analytics tasks (e.g., cluster identification). However, even with the same scatterplot, the ways of perceiving clusters (i.e., conducting visual clustering) can differ due to the differences among individuals and ambiguous cluster boundaries. Although such perceptual variability casts doubt on the reliability of data analysis based on visual clustering, we lack a systematic way to efficiently assess this variability. In this research, we study perceptual variability in conducting visual clustering, which we call Cluster Ambiguity. To this end, we introduce CLAMS, a data-driven visual quality measure for automatically predicting cluster ambiguity in monochrome scatterplots. We first conduct a qualitative study to identify key factors that affect the visual separation of clusters (e.g., proximity or size difference between clusters). Based on study findings, we deploy a regression module that estimates t
    
[^40]: ZADU: 评估降维嵌入可靠性的Python库

    ZADU: A Python Library for Evaluating the Reliability of Dimensionality Reduction Embeddings. (arXiv:2308.00282v1 [cs.LG])

    [http://arxiv.org/abs/2308.00282](http://arxiv.org/abs/2308.00282)

    ZADU是一个Python库，通过提供扭曲度量方法可以高效评估降维嵌入的可靠性，并通过优化执行和分析各个数据点的贡献提供全面评估。

    

    降维技术本质上会扭曲原始高维数据的结构，产生不完美的低维嵌入。为了评估降维嵌入的可靠性，提出了各种扭曲度量方法。然而，在实践中实现和执行扭曲度量一直是耗时且繁琐的。为了解决这个问题，本文介绍了一个Python库ZADU，提供了扭曲度量方法。ZADU不仅易于安装和执行，还通过三个关键特性实现了对降维嵌入的全面评估。首先，该库涵盖了各种扭曲度量方法。其次，它自动优化扭曲度量的执行，大大减少了执行多个度量所需的运行时间。最后，该库可显示个别数据点对整体扭曲的贡献，便于对降维嵌入进行详细分析。

    Dimensionality reduction (DR) techniques inherently distort the original structure of input high-dimensional data, producing imperfect low-dimensional embeddings. Diverse distortion measures have thus been proposed to evaluate the reliability of DR embeddings. However, implementing and executing distortion measures in practice has so far been time-consuming and tedious. To address this issue, we present ZADU, a Python library that provides distortion measures. ZADU is not only easy to install and execute but also enables comprehensive evaluation of DR embeddings through three key features. First, the library covers a wide range of distortion measures. Second, it automatically optimizes the execution of distortion measures, substantially reducing the running time required to execute multiple measures. Last, the library informs how individual points contribute to the overall distortions, facilitating the detailed analysis of DR embeddings. By simulating a real-world scenario of optimizin
    
[^41]: 数据协作分析在化合物数据集上的应用以及引入投影数据到非独立同分布的环境中

    Data Collaboration Analysis applied to Compound Datasets and the Introduction of Projection data to Non-IID settings. (arXiv:2308.00280v1 [cs.LG])

    [http://arxiv.org/abs/2308.00280](http://arxiv.org/abs/2308.00280)

    该论文介绍了应用于化合物数据集的数据协作分析，以及引入投影数据到非独立同分布环境中。这种方法通过数据合作和使用辅助数据的方式提高了预测准确性。

    

    鉴于把一种药物推向市场所需要的时间和费用，已经进行了许多研究，利用机器学习根据其结构预测化合物的性质。联邦学习已经应用于化合物数据集中，以提高其预测准确性，同时保护可能的专有信息。然而，在非独立同分布（non-IID）的情况下，联邦学习的准确性较低，即数据分区具有很大的标签偏差，并被认为不适用于化合物数据集，这往往具有较大的标签偏差。为解决这个限制，我们利用了一种分布式机器学习的替代方法，对来自开放源的化合物数据进行了数据协作分析（DC）。我们还提出了使用投影数据的数据协作分析方法（DCPd），该方法利用了辅助的PubChem数据。这提高了个体用户侧数据转换的质量。

    Given the time and expense associated with bringing a drug to market, numerous studies have been conducted to predict the properties of compounds based on their structure using machine learning. Federated learning has been applied to compound datasets to increase their prediction accuracy while safeguarding potentially proprietary information. However, federated learning is encumbered by low accuracy in not identically and independently distributed (non-IID) settings, i.e., data partitioning has a large label bias, and is considered unsuitable for compound datasets, which tend to have large label bias. To address this limitation, we utilized an alternative method of distributed machine learning to chemical compound data from open sources, called data collaboration analysis (DC). We also proposed data collaboration analysis using projection data (DCPd), which is an improved method that utilizes auxiliary PubChem data. This improves the quality of individual user-side data transformation
    
[^42]: 基于噪声负样本自校正的鲁棒正负样本学习

    Robust Positive-Unlabeled Learning via Noise Negative Sample Self-correction. (arXiv:2308.00279v1 [cs.LG])

    [http://arxiv.org/abs/2308.00279](http://arxiv.org/abs/2308.00279)

    本论文提出了一种基于噪声负样本自校正的鲁棒正负样本学习方法，通过首先学习简单的案例来减轻标签的不确定性，提高学习的鲁棒性。

    

    文献中将从正样本和无标签数据中学习称为正负样本（PU）学习，并在最近几年引起了很大关注。PU学习中一种常见的方法是使用临时阈值从无标签数据中采样一组伪负样本，以便可以应用传统的监督方法来包括正样本和负样本。由于无标签数据中标签的不确定性，将无标签的正样本错误分类为负样本的错误不可避免地出现，并且可能在训练过程中累积。这些错误经常导致性能下降和模型不稳定。为了减轻标签的不确定性影响，提高正负样本学习的鲁棒性，我们提出了一种新的鲁棒PU学习方法，其中训练策略受到人类学习的自然启示：首先学习简单的案例。相似的直觉已经在课程学习中被利用，只使用更简单的案例。

    Learning from positive and unlabeled data is known as positive-unlabeled (PU) learning in literature and has attracted much attention in recent years. One common approach in PU learning is to sample a set of pseudo-negatives from the unlabeled data using ad-hoc thresholds so that conventional supervised methods can be applied with both positive and negative samples. Owing to the label uncertainty among the unlabeled data, errors of misclassifying unlabeled positive samples as negative samples inevitably appear and may even accumulate during the training processes. Those errors often lead to performance degradation and model instability. To mitigate the impact of label uncertainty and improve the robustness of learning with positive and unlabeled data, we propose a new robust PU learning method with a training strategy motivated by the nature of human learning: easy cases should be learned first. Similar intuition has been utilized in curriculum learning to only use easier cases in the 
    
[^43]: 类别不等于聚类：改进基于标签的降维评估

    Classes are not Clusters: Improving Label-based Evaluation of Dimensionality Reduction. (arXiv:2308.00278v1 [cs.LG])

    [http://arxiv.org/abs/2308.00278](http://arxiv.org/abs/2308.00278)

    本文提出了两种新的质量度量方法--标签可信度和标签连续性（Label-T&C）--改进了基于类别标签的降维评估的过程，不再假设类别在原始空间中形成良好的聚类，而是通过估计类别在原始空间和嵌入空间中形成聚类的程度和评估两者之间的差异来工作。

    

    评估降维嵌入的可靠性的一种常见方法是量化标记类别在嵌入中如何形成紧凑且相互分离的聚类。这种方法基于一个假设，即类别在原始的高维空间中仍然是清晰的聚类。然而，在现实中，这个假设可能不成立；一个类别可能被分解成多个分离的聚类，多个类别可能合并成一个聚类。因此，我们不能总是保证使用类别标签进行评估的可信度。在本文中，我们引入了两种新的质量度量方法--标签可信度和标签连续性（Label-T&C）--改进了基于类别标签的降维评估的过程。Label-T&C不再假设类别在原始空间中形成良好的聚类，而是通过（1）估计类别在原始空间和嵌入空间中形成聚类的程度和（2）评估两者之间的差异来工作。

    A common way to evaluate the reliability of dimensionality reduction (DR) embeddings is to quantify how well labeled classes form compact, mutually separated clusters in the embeddings. This approach is based on the assumption that the classes stay as clear clusters in the original high-dimensional space. However, in reality, this assumption can be violated; a single class can be fragmented into multiple separated clusters, and multiple classes can be merged into a single cluster. We thus cannot always assure the credibility of the evaluation using class labels. In this paper, we introduce two novel quality measures -- Label-Trustworthiness and Label-Continuity (Label-T&C) -- advancing the process of DR evaluation based on class labels. Instead of assuming that classes are well-clustered in the original space, Label-T&C work by (1) estimating the extent to which classes form clusters in the original and embedded spaces and (2) evaluating the difference between the two. A quantitative e
    
[^44]: 通过一种对称和分量组不变功能的通用架构近似Wasserstein距离的神经网络

    Neural approximation of Wasserstein distance via a universal architecture for symmetric and factorwise group invariant functions. (arXiv:2308.00273v1 [cs.LG])

    [http://arxiv.org/abs/2308.00273](http://arxiv.org/abs/2308.00273)

    本文提出了一种通用神经网络架构来近似对称和分量组不变的函数，并将其与素描思想结合起来构建了一个用于近似点集之间Wasserstein距离的具体且高效的神经网络模型。

    

    学习复杂对象之间的距离函数，比如用于比较点集的Wasserstein距离，在机器学习应用中是一个常见的目标。然而，对于这种复杂对象（如点集和图形），函数往往需要对各种群操作（如排列或刚性变换）具有不变性。因此，这些复杂对象上的连续对称乘积函数（例如距离函数）也必须对这些群操作的乘积具有不变性。我们将这些函数称为对称和分量组不变函数（简称SFGI函数）。本文首先提出了一种用于近似SFGI函数的通用神经网络架构。本文的主要贡献是将这个通用神经网络与一个素描思想结合起来，开发出一种具体且高效的神经网络，可以近似点集之间的$p$-th Wasserstein距离。非常重要的是，所需的模型复杂度与点集的大小和维度无关。

    Learning distance functions between complex objects, such as the Wasserstein distance to compare point sets, is a common goal in machine learning applications. However, functions on such complex objects (e.g., point sets and graphs) are often required to be invariant to a wide variety of group actions e.g. permutation or rigid transformation. Therefore, continuous and symmetric product functions (such as distance functions) on such complex objects must also be invariant to the product of such group actions. We call these functions symmetric and factor-wise group invariant (or SFGI functions in short). In this paper, we first present a general neural network architecture for approximating SFGI functions. The main contribution of this paper combines this general neural network with a sketching idea to develop a specific and efficient neural network which can approximate the $p$-th Wasserstein distance between point sets. Very importantly, the required model complexity is independent of t
    
[^45]: 多模态多损失融合网络

    Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])

    [http://arxiv.org/abs/2308.00264](http://arxiv.org/abs/2308.00264)

    多模态多损失融合网络通过最佳选择和融合多个模态的特征，提高了情感检测的性能，并在多个数据集上实现了最先进的结果。这些研究结果表明了用于增强神经网络中情感检测的特征选择和融合方法的优化方向。

    

    在这项工作中，我们研究了跨多个模态选择和融合特征的最佳方法，并将其组合在神经网络中以改善情感检测。我们比较了不同的融合方法并且研究了多损失训练在多模态融合网络中的影响，从而确定了与子网性能相关的有用发现。我们最好的模型在三个数据集（CMU-MOSI、CMU-MOSEI和CH-SIMS）上实现了最先进的性能，并且在大多数指标上优于其他方法。我们发现，训练多模态特征可以改善单模态测试，并且基于数据集注释模式设计融合方法可以增强模型性能。这些结果表明了在神经网络中增强情感检测的优化特征选择和融合方法的发展方向。

    In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
    
[^46]: 使用双向量化通信和缓冲聚合的异步联邦学习

    Asynchronous Federated Learning with Bidirectional Quantized Communications and Buffered Aggregation. (arXiv:2308.00263v1 [cs.LG])

    [http://arxiv.org/abs/2308.00263](http://arxiv.org/abs/2308.00263)

    该论文介绍了一种名为QAFeL的新算法，通过建立服务器和客户端之间的共享“隐藏”状态，采用量化方案解决了异步联邦学习中高通信成本的问题。该算法能够在保持高精度的同时显著减少客户端-服务器交互过程中传输的数据量。

    

    异步联邦学习与缓冲聚合（FedBuff）是一种效率高、可伸缩性强的先进算法。然而，它存在高通信成本的问题，并未使用量化通信进行研究。为了解决这个问题，我们提出了一种新的算法（QAFeL），采用一种量化方案，在服务器和客户端之间建立共享的“隐藏”状态，避免了直接量化引起的误差传播。这种方法在保持高精度的同时，在客户端-服务器交互过程中显著减少了传输的数据量。我们提供了QAFeL的理论收敛保证，并通过标准基准实验验证了我们的分析结果。

    Asynchronous Federated Learning with Buffered Aggregation (FedBuff) is a state-of-the-art algorithm known for its efficiency and high scalability. However, it has a high communication cost, which has not been examined with quantized communications. To tackle this problem, we present a new algorithm (QAFeL), with a quantization scheme that establishes a shared "hidden" state between the server and clients to avoid the error propagation caused by direct quantization. This approach allows for high precision while significantly reducing the data transmitted during client-server interactions. We provide theoretical convergence guarantees for QAFeL and corroborate our analysis with experiments on a standard benchmark.
    
[^47]: AQUILA: 自适应量化懒汇聚梯度的通信高效联邦学习

    AQUILA: Communication Efficient Federated Learning with Adaptive Quantization of Lazily-Aggregated Gradients. (arXiv:2308.00258v1 [cs.LG])

    [http://arxiv.org/abs/2308.00258](http://arxiv.org/abs/2308.00258)

    AQUILA是一个自适应量化梯度的通信高效联邦学习框架，解决了传输大规模模型时的通信开销和局部数据偏差导致的全局模型鲁棒性问题。

    

    联邦学习的广泛应用受到高通信开销的挑战，主要来自大规模模型的传输。现有的自适应量化方法在每一轮训练中都假设设备参与均匀，在实践中不可行。此外，这些方法在选取量化级别时存在局限性，并经常忽视本地设备数据的偏差，从而影响全局模型的鲁棒性。为了解决这些问题，本文引入了一种名为AQUILA（自适应量化懒汇聚梯度）的新型自适应框架，以增强联邦学习的效率和鲁棒性。AQUILA整合了一种复杂的设备选择方法，优先考虑设备更新的质量和实用性。

    The widespread adoption of Federated Learning (FL), a privacy-preserving distributed learning methodology, has been impeded by the challenge of high communication overheads, typically arising from the transmission of large-scale models. Existing adaptive quantization methods, designed to mitigate these overheads, operate under the impractical assumption of uniform device participation in every training round. Additionally, these methods are limited in their adaptability due to the necessity of manual quantization level selection and often overlook biases inherent in local devices' data, thereby affecting the robustness of the global model. In response, this paper introduces AQUILA (adaptive quantization of lazily-aggregated gradients), a novel adaptive framework devised to effectively handle these issues, enhancing the efficiency and robustness of FL. AQUILA integrates a sophisticated device selection method that prioritizes the quality and usefulness of device updates. Utilizing the e
    
[^48]: 广义线性模型中的最佳子集选择：一种通过拼接技术的快速且一致的算法

    Best-Subset Selection in Generalized Linear Models: A Fast and Consistent Algorithm via Splicing Technique. (arXiv:2308.00251v1 [stat.ML])

    [http://arxiv.org/abs/2308.00251](http://arxiv.org/abs/2308.00251)

    本文提出了一种在高维广义线性模型中进行最佳子集选择的快速且一致的算法，该算法通过拼接技术实现了高确定性的最佳子集选择，并在变量选择和系数估计方面优于现有方法。

    

    在高维广义线性模型中，很重要的是确定一个能充分解释响应变化的稀疏模型。虽然最佳子集选择一直被认为是这类问题的终极目标，但要同时实现计算效率和统计保证却非常具有挑战性。本文目的在于利用快速算法，以高确定性选择最佳子集，解决这一难题。我们提出并演示了一种在正则条件下实现最佳子集恢复的算法。在一定条件下，我们的算法的计算复杂度与样本大小和维数的多项式级别相关。除了展示我们方法的统计特性，广泛的数值实验表明，它在变量选择和系数估计方面优于现有方法。运行时间分析显示，与流行的变量选择工具相比，我们的实现实现了近4倍的加速。

    In high-dimensional generalized linear models, it is crucial to identify a sparse model that adequately accounts for response variation. Although the best subset section has been widely regarded as the Holy Grail of problems of this type, achieving either computational efficiency or statistical guarantees is challenging. In this article, we intend to surmount this obstacle by utilizing a fast algorithm to select the best subset with high certainty. We proposed and illustrated an algorithm for best subset recovery in regularity conditions. Under mild conditions, the computational complexity of our algorithm scales polynomially with sample size and dimension. In addition to demonstrating the statistical properties of our method, extensive numerical experiments reveal that it outperforms existing methods for variable selection and coefficient estimation. The runtime analysis shows that our implementation achieves approximately a fourfold speedup compared to popular variable selection tool
    
[^49]: 基于特征掩蔽自编码和情绪迁移学习的基于EEG的认知负荷分类

    EEG-based Cognitive Load Classification using Feature Masked Autoencoding and Emotion Transfer Learning. (arXiv:2308.00246v1 [cs.LG])

    [http://arxiv.org/abs/2308.00246](http://arxiv.org/abs/2308.00246)

    本文提出了一种基于EEG的认知负荷分类的新解决方案，利用特征掩蔽自编码和情绪迁移学习来进行模型训练和分类。实验结果表明，这种方法取得了良好的结果。

    

    认知负荷是完成任务所需的心理努力量，在性能和决策结果中起着重要作用，因此在各种敏感领域中分类和分析认知负荷至关重要。本文提出了一种利用脑电图（EEG）分类认知负荷的新解决方案。我们的模型采用了一种变压器架构，利用情绪和认知负荷之间的迁移学习。我们使用自监督掩蔽自编码在情绪相关的EEG数据集上进行预训练，并使用冻结权重和微调进行迁移学习，进行下游的认知负荷分类。为了评估我们的方法，我们进行了一系列实验，利用了两个公开可用的基于EEG的情绪数据集，即SEED和SEED-IV，进行预训练，同时我们使用CL-Drive数据集进行下游的认知负荷分类。我们实验的结果表明，我们提出的方法取得了良好的结果。

    Cognitive load, the amount of mental effort required for task completion, plays an important role in performance and decision-making outcomes, making its classification and analysis essential in various sensitive domains. In this paper, we present a new solution for the classification of cognitive load using electroencephalogram (EEG). Our model uses a transformer architecture employing transfer learning between emotions and cognitive load. We pre-train our model using self-supervised masked autoencoding on emotion-related EEG datasets and use transfer learning with both frozen weights and fine-tuning to perform downstream cognitive load classification. To evaluate our method, we carry out a series of experiments utilizing two publicly available EEG-based emotion datasets, namely SEED and SEED-IV, for pre-training, while we use the CL-Drive dataset for downstream cognitive load classification. The results of our experiments show that our proposed approach achieves strong results and ou
    
[^50]: Capsa: 用于量化深度神经网络风险的统一框架

    Capsa: A Unified Framework for Quantifying Risk in Deep Neural Networks. (arXiv:2308.00231v1 [cs.LG])

    [http://arxiv.org/abs/2308.00231](http://arxiv.org/abs/2308.00231)

    Capsa是一个统一框架，用于扩展具有风险感知的深度神经网络模型。它能够量化多种形式的风险，并将不同算法组合在一起以并行量化不同的风险指标。通过在复杂感知数据集上实现最先进的不确定性估计算法并进行基准测试，验证了capsa的有效性。capsa能够轻松组合aleatoric不确定性、epistemic不确定性和偏见估计能力

    

    大规模深度神经网络在复杂问题上表现出色，但也常常出现突然、意外且灾难性的失败，尤其是在具有挑战性的情况下。现有的为神经网络提供风险感知的算法复杂而临时。具体而言，这些方法需要进行重大的工程改变，通常只针对特定设置进行开发，并且很难组合在一起。我们在这里提出了capsa，一个用于扩展具有风险感知的模型的框架。Capsa提供了一种量化多种形式风险的方法，并将不同的算法组合在一起以并行量化不同的风险指标。我们通过在capsa框架中实现最先进的不确定性估计算法并在复杂感知数据集上进行基准测试，验证了capsa的有效性。我们展示了capsa轻松组合aleatoric不确定性、epistemic不确定性和偏见估计的能力

    The modern pervasiveness of large-scale deep neural networks (NNs) is driven by their extraordinary performance on complex problems but is also plagued by their sudden, unexpected, and often catastrophic failures, particularly on challenging scenarios. Existing algorithms that provide risk-awareness to NNs are complex and ad-hoc. Specifically, these methods require significant engineering changes, are often developed only for particular settings, and are not easily composable. Here we present capsa, a framework for extending models with risk-awareness. Capsa provides a methodology for quantifying multiple forms of risk and composing different algorithms together to quantify different risk metrics in parallel. We validate capsa by implementing state-of-the-art uncertainty estimation algorithms within the capsa framework and benchmarking them on complex perception datasets. We demonstrate capsa's ability to easily compose aleatoric uncertainty, epistemic uncertainty, and bias estimation 
    
[^51]: 被指导的偏见：经过指导调优的语言模型呈现出新兴的认知偏见

    Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias. (arXiv:2308.00225v1 [cs.CL])

    [http://arxiv.org/abs/2308.00225](http://arxiv.org/abs/2308.00225)

    这项研究发现，经过指导调优的语言模型呈现出新兴的认知偏见，这对于理解和开发更可靠和无偏的语言模型至关重要。

    

    最近的研究表明，指导调优和从人类反馈中学习可以显著提高大语言模型（LMs）的能力。虽然这些调优方法可以使模型生成高质量的文本，但我们推测这些经过调优的模型可能会产生更多隐含的认知偏见。我们的研究提供了证据，表明这些经过调优的模型呈现出先前预训练模型中不存在或较不明显的偏见。我们对三种认知偏见进行了研究，包括矛盾效应、确定性效应和信念偏见，这些偏见已被证实对人类的决策和推理有影响。我们的研究结果突显了这些偏见在各种模型中的存在，特别是那些经过指导调优的模型，如Flan-T5、GPT3.5和GPT4。这项研究对于理解指导调优的LMs中的认知偏见是至关重要的，这有助于开发更可靠和无偏的语言模型。

    Recent studies show that instruction tuning and learning from human feedback improve the abilities of large language models (LMs) dramatically. While these tuning methods can make models generate high-quality text, we conjecture that more implicit cognitive biases may arise in these fine-tuned models. Our work provides evidence that these fine-tuned models exhibit biases that were absent or less pronounced in their pretrained predecessors. We examine the extent of this phenomenon in three cognitive biases - the decoy effect, the certainty effect, and the belief bias - all of which are known to influence human decision-making and reasoning. Our findings highlight the presence of these biases in various models, especially those that have undergone instruction tuning, such as Flan-T5, GPT3.5, and GPT4. This research constitutes a step toward comprehending cognitive biases in instruction-tuned LMs, which is crucial for the development of more reliable and unbiased language models.
    
[^52]: 基于深度强化学习的电池调控分层V2G协调策略以实现多方利益

    Deep Reinforcement Learning-Based Battery Conditioning Hierarchical V2G Coordination for Multi-Stakeholder Benefits. (arXiv:2308.00218v1 [eess.SY])

    [http://arxiv.org/abs/2308.00218](http://arxiv.org/abs/2308.00218)

    本研究提出了一种基于深度强化学习的电池调控分层V2G协调策略，旨在促进可再生能源利用和电网稳定。该策略可以实现多方利益，并且考虑了电网、电动车聚合器和用户的各种因素。

    

    随着电动车（EV）的普及和EV电子技术的发展，车辆对电网（V2G）技术和大规模调度策略已经出现，以促进可再生能源利用和电网稳定。本研究提出了基于深度强化学习（DRL）和权益证明算法的多方参与者分层V2G协调策略。此外，多方参与者包括电网、电动车聚合器（EVAs）和用户，并且所提出的策略可以实现多方利益。在电网方面，考虑到负荷波动和可再生能源消耗，而在EVA方面，考虑了能源限制和充电成本。用户方面考虑了电池SOX的三个关键调节参数，包括电荷状态、功率状态和健康状态。与四种典型基线相比，多方参与者分层协调策略可以增强可再生能源的利用。

    With the growing prevalence of electric vehicles (EVs) and advancements in EV electronics, vehicle-to-grid (V2G) techniques and large-scale scheduling strategies have emerged to promote renewable energy utilization and power grid stability. This study proposes a multi-stakeholder hierarchical V2G coordination based on deep reinforcement learning (DRL) and the Proof of Stake algorithm. Furthermore, the multi-stakeholders include the power grid, EV aggregators (EVAs), and users, and the proposed strategy can achieve multi-stakeholder benefits. On the grid side, load fluctuations and renewable energy consumption are considered, while on the EVA side, energy constraints and charging costs are considered. The three critical battery conditioning parameters of battery SOX are considered on the user side, including state of charge, state of power, and state of health. Compared with four typical baselines, the multi-stakeholder hierarchical coordination strategy can enhance renewable energy con
    
[^53]: 使用神经调整层析成像（NeTT）和掩蔽神经辐射场（mNeRF）的稳健单视锥形X射线姿态估计

    Robust Single-view Cone-beam X-ray Pose Estimation with Neural Tuned Tomography (NeTT) and Masked Neural Radiance Fields (mNeRF). (arXiv:2308.00214v1 [cs.CV])

    [http://arxiv.org/abs/2308.00214](http://arxiv.org/abs/2308.00214)

    在影像导引的微创医疗过程中，我们提出了新的方法，利用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。

    

    在影像导引的微创医疗过程中，许多任务可以看作是姿态估计问题，其中利用X射线投影来达到3D空间中的目标。近期在可微分渲染技术上的进展使得RGB相机视图合成和姿态估计的性能达到了最先进水平。在之前的工作基础上，我们引入了新的方法，用于使用X射线投影进行辐射透明物体的姿态估计，并且展示了优化视图合成在完成此任务中的关键作用。首先我们开发了一种算法（DiffDRR），能够在TensorFlow中高效计算数字重建放射图像（DRRs）并利用自动微分。结合经典的CBCT重建算法，我们通过梯度下降进行姿态估计，使用一个损失函数来量化从随机初始化姿态合成的DRR与目标处真实透视图像的相似性。

    Many tasks performed in image-guided, mini-invasive, medical procedures can be cast as pose estimation problems, where an X-ray projection is utilized to reach a target in 3D space. Recent advances in the differentiable rendering of optically reflective materials have enabled state-of-the-art performance in RGB camera view synthesis and pose estimation. Expanding on these prior works, we introduce new methods for pose estimation of radiolucent objects using X-ray projections, and we demonstrate the critical role of optimal view synthesis in performing this task. We first develop an algorithm (DiffDRR) that efficiently computes Digitally Reconstructed Radiographs (DRRs) and leverages automatic differentiation within TensorFlow. In conjunction with classic CBCT reconstruction algorithms, we perform pose estimation by gradient descent using a loss function that quantifies the similarity of the DRR synthesized from a randomly initialized pose and the true fluoroscopic image at the target p
    
[^54]: SkullGAN: 使用生成对抗网络生成合成的颅骨CT图像

    SkullGAN: Synthetic Skull CT Generation with Generative Adversarial Networks. (arXiv:2308.00206v1 [eess.IV])

    [http://arxiv.org/abs/2308.00206](http://arxiv.org/abs/2308.00206)

    使用SkullGAN，一种生成对抗网络（GAN），生成合成的颅骨CT图像，可以减少对真实图像的依赖，加速将机器学习应用于医疗保健领域的整合。

    

    深度学习在涉及人类颅骨的各种医疗应用中具有潜力，但需要大量经过策划的医学图像数据集。为了解决这个挑战，我们提出了SkullGAN，一种生成对抗网络（GAN），用于创建大规模的合成颅骨CT切片数据集，减少对真实图像的依赖并加速将机器学习应用于医疗保健领域的整合。在我们的方法中，对38个受试者进行了颅骨CT切片输入SkullGAN，这是一个包含超过2亿个参数的神经网络。生成的合成颅骨图像根据三个定量放射学特征进行评估：颅骨密度比（SDR）、平均厚度和平均强度。同时，使用t-分布随机邻域嵌入（t-SNE）进行进一步的分析，并将SkullGAN判别器作为分类器进行应用。结果表明，SkullGAN生成的图像与真实颅骨具有类似的关键定量放射学特征。进一步的确定性分析是通过进行了解决这个问题的办法。

    Deep learning offers potential for various healthcare applications involving the human skull but requires extensive datasets of curated medical images. To overcome this challenge, we propose SkullGAN, a generative adversarial network (GAN), to create large datasets of synthetic skull CT slices, reducing reliance on real images and accelerating the integration of machine learning into healthcare. In our method, CT slices of 38 subjects were fed to SkullGAN, a neural network comprising over 200 million parameters. The synthetic skull images generated were evaluated based on three quantitative radiological features: skull density ratio (SDR), mean thickness, and mean intensity. They were further analyzed using t-distributed stochastic neighbor embedding (t-SNE) and by applying the SkullGAN discriminator as a classifier. The results showed that SkullGAN-generated images demonstrated similar key quantitative radiological features to real skulls. Further definitive analysis was undertaken by
    
[^55]: CBCL-PR：一种灵感来自认知的机器人类增量学习模型

    CBCL-PR: A Cognitively Inspired Model for Class-Incremental Learning in Robotics. (arXiv:2308.00199v1 [cs.RO])

    [http://arxiv.org/abs/2308.00199](http://arxiv.org/abs/2308.00199)

    本论文提出了CBCL-PR框架，灵感来自海马体和新皮层的概念学习理论，用于解决机器人领域中少样本类增量学习的问题。该框架通过回放旧类别的数据来避免遗忘，在目标分类方面取得了最新的性能。同时，在机器人上的实验也证实了该框架在大量家庭物品的分类任务中有着优秀的表现。

    

    对于大多数实际应用，机器人需要在环境中有限的数据下进行不断地自适应和学习。本文考虑了少样本类增量学习（FSIL）的问题，即要求人工智能代理在只有少量数据样本的情况下进行增量学习，同时不会忘记之前学习的数据。为了解决这个问题，我们提出了一种新颖的框架，灵感来自海马体和新皮层的概念学习理论。我们的框架将对象类表示为一组簇，并将其存储在内存中。框架通过回放旧类别的簇生成的数据来避免在学习新类别时忘记。我们的方法在两个目标分类数据集上进行了评估，得到了类增量学习和FSIL方面的最新性能。我们还在机器人上评估了我们的框架在FSIL方面，结果表明机器人可以不断学习分类大量的家庭物品，且分类性能优秀。

    For most real-world applications, robots need to adapt and learn continually with limited data in their environments. In this paper, we consider the problem of Few-Shot class Incremental Learning (FSIL), in which an AI agent is required to learn incrementally from a few data samples without forgetting the data it has previously learned. To solve this problem, we present a novel framework inspired by theories of concept learning in the hippocampus and the neocortex. Our framework represents object classes in the form of sets of clusters and stores them in memory. The framework replays data generated by the clusters of the old classes, to avoid forgetting when learning new classes. Our approach is evaluated on two object classification datasets resulting in state-of-the-art (SOTA) performance for class-incremental learning and FSIL. We also evaluate our framework for FSIL on a robot demonstrating that the robot can continually learn to classify a large set of household objects with limit
    
[^56]: C-DARL: 无标签血管分割的对比扩散对抗表示学习

    C-DARL: Contrastive diffusion adversarial representation learning for label-free blood vessel segmentation. (arXiv:2308.00193v1 [eess.IV])

    [http://arxiv.org/abs/2308.00193](http://arxiv.org/abs/2308.00193)

    C-DARL是一种自监督的血管分割方法，通过对比学习和生成模块的使用，可以有效地学习多领域血管数据的分布，并生成更真实的血管表示。

    

    血管在医学成像中的分割是血管疾病诊断和介入治疗规划中的重要步骤之一，涉及图像医学和介入医学的广泛应用场景。然而，由于细微的分支和复杂的结构，血管掩膜的手工注释是具有挑战性和资源密集型的。为了克服这个问题，本文提出了一种自监督血管分割方法，称为对比扩散对抗表示学习（C-DARL）模型。我们的模型包括一个扩散模块和一个生成模块，通过从扩散潜变量生成合成血管图像来学习多领域血管数据的分布。此外，我们采用基于掩膜的对比损失进行对比学习，使模型可以学习更加真实的血管表示。为了验证C-DARL的有效性，我们使用多种血管数据集进行训练，包括冠状动脉造影图像、腹部数字化图像等。

    Blood vessel segmentation in medical imaging is one of the essential steps for vascular disease diagnosis and interventional planning in a broad spectrum of clinical scenarios in image-based medicine and interventional medicine. Unfortunately, manual annotation of the vessel masks is challenging and resource-intensive due to subtle branches and complex structures. To overcome this issue, this paper presents a self-supervised vessel segmentation method, dubbed the contrastive diffusion adversarial representation learning (C-DARL) model. Our model is composed of a diffusion module and a generation module that learns the distribution of multi-domain blood vessel data by generating synthetic vessel images from diffusion latent. Moreover, we employ contrastive learning through a mask-based contrastive loss so that the model can learn more realistic vessel representations. To validate the efficacy, C-DARL is trained using various vessel datasets, including coronary angiograms, abdominal digi
    
[^57]: 通用主导-极小化算法

    Universal Majorization-Minimization Algorithms. (arXiv:2308.00190v1 [math.OC])

    [http://arxiv.org/abs/2308.00190](http://arxiv.org/abs/2308.00190)

    这项研究提出了一种通用的主导-极小化算法，它通过自动推导主导器来解决任意问题，并且可以从任何起始点收敛，无需调整超参数。

    

    主导-极小化（MM）是一类优化方法，通过最小化一个局部紧上界，即主导器，来迭代地降低损失。传统上，主导器是手动推导的，因此MM只适用于少数经过深入研究的问题。我们提出一种优化器，通过使用最近的泰勒模式自动微分的广义化方法来自动推导主导器。这些通用的MM优化器可以应用于任意问题，并且从任何起始点收敛，无需调整超参数。

    Majorization-minimization (MM) is a family of optimization methods that iteratively reduce a loss by minimizing a locally-tight upper bound, called a majorizer. Traditionally, majorizers were derived by hand, and MM was only applicable to a small number of well-studied problems. We present optimizers that instead derive majorizers automatically, using a recent generalization of Taylor mode automatic differentiation. These universal MM optimizers can be applied to arbitrary problems and converge from any starting point, with no hyperparameter tuning.
    
[^58]: 生成模型作为复杂系统科学：如何理解大型语言模型的行为？

    Generative Models as a Complex Systems Science: How can we make sense of large language model behavior?. (arXiv:2308.00189v1 [cs.LG])

    [http://arxiv.org/abs/2308.00189](http://arxiv.org/abs/2308.00189)

    生成模型作为复杂系统科学，它们能够完成任务的行为表现需要被解释和理解，以实现对其行为的控制和未来研究的指导。

    

    从预训练模型中引导出期望的行为，同时避免不良行为，重新定义了自然语言处理并正在重新塑造我们与计算机的交互方式。曾经是一个科学工程学科，将构建模块堆叠在一起，现在可以说已经是一个复杂系统科学，其中寻求出现的行为以支持以前无法想象的用例。尽管有越来越多的基准测试来衡量任务性能，但我们缺乏解释语言模型展示这些任务完成的行为的解释。我们提出了一个系统性的努力，将语言模型的行为分解为解释跨任务性能的类别，以指导机械解释并帮助未来分析研究。

    Coaxing out desired behavior from pretrained models, while avoiding undesirable ones, has redefined NLP and is reshaping how we interact with computers. What was once a scientific engineering discipline-in which building blocks are stacked one on top of the other-is arguably already a complex systems science, in which emergent behaviors are sought out to support previously unimagined use cases.  Despite the ever increasing number of benchmarks that measure task performance, we lack explanations of what behaviors language models exhibit that allow them to complete these tasks in the first place. We argue for a systematic effort to decompose language model behavior into categories that explain cross-task performance, to guide mechanistic explanations and help future-proof analytic research.
    
[^59]: 数据管理和可解释机器学习中的归因分数

    Attribution-Scores in Data Management and Explainable Machine Learning. (arXiv:2308.00184v1 [cs.DB])

    [http://arxiv.org/abs/2308.00184](http://arxiv.org/abs/2308.00184)

    数据管理和可解释机器学习中的研究描述了使用实际因果关系来解释数据库查询答案和机器学习模型结果的责任评分的最新工作，以及与数据库修复和Shap-score计算相关的内容。

    

    我们描述了最近关于在数据库中使用实际因果关系来定义解释查询答案和机器学习分类模型结果的责任评分的研究。在数据库的情况下，我们展示并利用数据库修复与有用的连接。修复还用于给出数据库的一致性的量化度量。对于分类模型，责任评分得到了适当的扩展和说明。还分析和讨论了Shap-score的高效计算。重点放在作者和合作者的工作上。

    We describe recent research on the use of actual causality in the definition of responsibility scores as explanations for query answers in databases, and for outcomes from classification models in machine learning. In the case of databases, useful connections with database repairs are illustrated and exploited. Repairs are also used to give a quantitative measure of the consistency of a database. For classification models, the responsibility score is properly extended and illustrated. The efficient computation of Shap-score is also analyzed and discussed. The emphasis is placed on work done by the author and collaborators.
    
[^60]: 海底滑翔器的通用异常检测验证方法——大规模部署数据集的应用

    General Anomaly Detection of Underwater Gliders Validated by Large-scale Deployment Dataset. (arXiv:2308.00180v1 [cs.RO])

    [http://arxiv.org/abs/2308.00180](http://arxiv.org/abs/2308.00180)

    本文介绍了一种用于评估海底滑翔器在不可预测海洋环境中正常操作的异常检测算法，并通过实际滑翔器部署的大规模数据集进行验证。算法能够实时提供异常警报，使驾驶员能够控制滑翔器并避免进一步损害。

    

    本文利用一种异常检测算法评估在不可预测的海洋环境中海底滑翔器的正常操作。一旦检测到任何异常，可以向滑翔器驾驶员提供实时警报，使其能够接管滑翔器并防止进一步的损害。该检测算法应用于由Skidaway海洋研究所（SkIO）和南佛罗里达大学（USF）领导的实际滑翔器部署中收集的大量数据集。就泛化性而言，实验评估包括离线和在线检测模式。离线检测利用完整的回收后数据集，具有高分辨率的信息，对异常进行详细分析并与驾驶员日志进行比较。在线检测专注于从滑翔器传输的实时数据子集。虽然实时数据可能不包含与回收后数据一样丰富的信息，但在线检测是实时的。

    This paper employs an anomaly detection algorithm to assess the normal operation of underwater gliders in unpredictable ocean environments. Real-time alerts can be provided to glider pilots upon detecting any anomalies, enabling them to assume control of the glider and prevent further harm. The detection algorithm is applied to abundant data sets collected in real glider deployments led by the Skidaway Institute of Oceanography (SkIO) and the University of South Florida (USF). Regarding generality, the experimental evaluation is composed of both offline and online detection modes. The offline detection utilizes full post-recovery data sets, which carries high-resolution information, to present detailed analysis of the anomaly and compare it with pilot logs. The online detection focuses on the real-time subsets of data transmitted from the glider at the surfacing events. While the real-time data may not contain as much rich information as the post-recovery data, the online detection is 
    
[^61]: 预训练的深度模型在标签稀缺的Learning-To-Rank中胜过GBDTs

    Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])

    [http://arxiv.org/abs/2308.00177](http://arxiv.org/abs/2308.00177)

    本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。

    

    尽管深度学习模型在文本和图像领域是最先进的，但它们在表格形式的Learning-To-Rank问题上尚未一致地胜过梯度提升决策树(GBDTs)。近期在文本和图像任务上深度学习模型取得的性能提升主要依赖于无监督预训练，这种方法利用了比有标签数据多几个数量级的无标签数据。据我们所知，无监督预训练还未应用于Learning-To-Rank问题，而该问题通常产生大量无标签数据。本研究探究了无监督预训练是否能提高LTR性能，与GBDTs和其他非预训练模型相比。通过使用简单的设计选择(包括SimCLR-Rank，这是我们针对排名问题修改的SimCLR方法)，我们产生了预训练的深度学习模型，在有大量无标签数据且有限标签数据的情况下，显著优于GBDTs(和其他非预训练模型)。

    While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
    
[^62]: 一个用于高维细胞数据的流动艺术家

    A Flow Artist for High-Dimensional Cellular Data. (arXiv:2308.00176v1 [cs.LG])

    [http://arxiv.org/abs/2308.00176](http://arxiv.org/abs/2308.00176)

    本研究提出了一个名为FlowArtist的神经网络，可以同时嵌入点和学习围绕点的向量场，从而更好地分离和可视化基于速度的结构。

    

    本文考虑了将采样自相关流动或速度的底层流形的点云数据嵌入其中的问题。这样的数据在许多情况下出现，包括高通量生物学（如单细胞转录组学）中测量动态实体的静态快照。现有的嵌入技术要么不利用速度信息，要么独立地嵌入坐标和速度，即在现有点嵌入的基础上施加速度，或将点嵌入到一个预先指定的向量场中。在这里，我们提出了一种名为FlowArtist的神经网络，它在嵌入点的同时学习了一个围绕点的向量场。这种组合使得FlowArtist能够更好地分离和可视化基于速度的结构。我们的结果，在玩具数据集和单细胞RNA速度数据上，说明了利用坐标和速度信息同步进行嵌入和可视化高维数据的价值。

    We consider the problem of embedding point cloud data sampled from an underlying manifold with an associated flow or velocity. Such data arises in many contexts where static snapshots of dynamic entities are measured, including in high-throughput biology such as single-cell transcriptomics. Existing embedding techniques either do not utilize velocity information or embed the coordinates and velocities independently, i.e., they either impose velocities on top of an existing point embedding or embed points within a prescribed vector field. Here we present FlowArtist, a neural network that embeds points while jointly learning a vector field around the points. The combination allows FlowArtist to better separate and visualize velocity-informed structures. Our results, on toy datasets and single-cell RNA velocity data, illustrate the value of utilizing coordinate and velocity information in tandem for embedding and visualizing high-dimensional data.
    
[^63]: 医学影像中数据和模型异质性的联邦学习

    Federated Learning for Data and Model Heterogeneity in Medical Imaging. (arXiv:2308.00155v1 [cs.CV])

    [http://arxiv.org/abs/2308.00155](http://arxiv.org/abs/2308.00155)

    本文提出了一种名为MDH-FL的方法，通过在联邦学习中同时利用数据和模型异质性，解决了数据异质性和模型异质性的问题，提高了全局模型的效率。

    

    联邦学习是一种不同客户端参与协作学习但不共享彼此数据和中央服务器的进化型机器学习方法。在医院和工业等实际应用中，联邦学习解决了数据异质性和模型异质性作为协作训练不可避免的挑战。然而，现有方法未能有效解决联邦学习中的模型和数据异质性问题。本文同时利用数据和模型异质性，提出了一种名为MDH-FL的方法来解决这些问题，从而提高联邦学习中全局模型的效率。我们使用知识蒸馏和对称损失来降低异质性对模型性能的影响。

    Federated Learning (FL) is an evolving machine learning method in which multiple clients participate in collaborative learning without sharing their data with each other and the central server. In real-world applications such as hospitals and industries, FL counters the challenges of data heterogeneity and model heterogeneity as an inevitable part of the collaborative training. More specifically, different organizations, such as hospitals, have their own private data and customized models for local training. To the best of our knowledge, the existing methods do not effectively address both problems of model heterogeneity and data heterogeneity in FL. In this paper, we exploit the data and model heterogeneity simultaneously, and propose a method, MDH-FL (Exploiting Model and Data Heterogeneity in FL) to solve such problems to enhance the efficiency of the global model in FL. We use knowledge distillation and a symmetric loss to minimize the heterogeneity and its impact on the model perf
    
[^64]: DiffusAL: 将主动学习与图扩散相结合，实现节点分类的标签高效化

    DiffusAL: Coupling Active Learning with Graph Diffusion for Label-Efficient Node Classification. (arXiv:2308.00146v1 [cs.LG])

    [http://arxiv.org/abs/2308.00146](http://arxiv.org/abs/2308.00146)

    DiffusAL是一种新颖的主动图学习方法，通过结合三个独立的评分函数，实现了在不同场景中的鲁棒性，并提高了节点分类的标签效率。

    

    节点分类是属性图上的核心任务之一，但成功的图学习解决方案需要足够标记的数据。为了降低注释成本，主动图学习专注于选择最具质量的节点子集，以最大化标签效率。然而，决定对一个未标记的图使用哪种启发式方法来增加标签效率，始终是一个持久的挑战。现有的解决方案要么忽视了学到的模型和采样方法的对齐，要么只关注有限的选择方面。因此，它们有时比随机采样更差，或者只能与随机采样一样好。在这项工作中，我们引入了一种新颖的主动图学习方法，称为DiffusAL，在不同场景中表现出显著的鲁棒性。为了在不同图结构之间实现更好的可迁移性，我们以无参数的方式结合了三个独立的评分函数，以识别最具信息的节点样本以进行标记：i) 模型不确定性，ii) 多样性组件，以及iii)

    Node classification is one of the core tasks on attributed graphs, but successful graph learning solutions require sufficiently labeled data. To keep annotation costs low, active graph learning focuses on selecting the most qualitative subset of nodes that maximizes label efficiency. However, deciding which heuristic is best suited for an unlabeled graph to increase label efficiency is a persistent challenge. Existing solutions either neglect aligning the learned model and the sampling method or focus only on limited selection aspects. They are thus sometimes worse or only equally good as random sampling. In this work, we introduce a novel active graph learning approach called DiffusAL, showing significant robustness in diverse settings. Toward better transferability between different graph structures, we combine three independent scoring functions to identify the most informative node samples for labeling in a parameter-free way: i) Model Uncertainty, ii) Diversity Component, and iii)
    
[^65]: 在反应式系统内对神经网络进行形式化解释

    Formally Explaining Neural Networks within Reactive Systems. (arXiv:2308.00143v1 [cs.AI])

    [http://arxiv.org/abs/2308.00143](http://arxiv.org/abs/2308.00143)

    这项研究在反应式系统中提出了一种基于DNN验证的形式化XAI技术，可以解释DNN的行为，并且通过利用系统的转换约束来计算简洁的解释。

    

    深度神经网络(DNNs)越来越多地被用作反应式系统中的控制器。然而，DNNs具有高度的不透明性，这使得解释和证明它们的行为变得困难。为了解决这个问题，出现了对可解释AI(XAI)技术的兴趣激增，这些技术能够找出导致DNN行为的输入特征。现有的XAI技术通常存在两个限制：(i)它们是启发式方法，并不能提供解释正确性的正式保证；(ii)它们通常适用于“一次性”系统(即DNN独立于过去的调用)，而不是反应式系统。在这里，我们开始弥合这个差距，提出一种基于DNN验证的形式化XAI技术，用于推理多步骤的反应式系统。我们建议通过利用系统的转换约束来计算简洁的解释的方法，以便减少底层验证器所探索的搜索空间。

    Deep neural networks (DNNs) are increasingly being used as controllers in reactive systems. However, DNNs are highly opaque, which renders it difficult to explain and justify their actions. To mitigate this issue, there has been a surge of interest in explainable AI (XAI) techniques, capable of pinpointing the input features that caused the DNN to act as it did.  Existing XAI techniques typically face two limitations: (i) they are heuristic, and do not provide formal guarantees that the explanations are correct; and (ii) they often apply to ``one-shot'' systems (where the DNN is invoked independently of past invocations), as opposed to reactive systems.  Here, we begin bridging this gap, and propose a formal DNN-verification-based XAI technique for reasoning about multi-step, reactive systems. We suggest methods for efficiently calculating succinct explanations, by exploiting the system's transition constraints in order to curtail the search space explored by the underlying verifier. W
    
[^66]: 在Stiefel流形上的半监督Laplacian学习

    Semi-Supervised Laplacian Learning on Stiefel Manifolds. (arXiv:2308.00142v1 [cs.LG])

    [http://arxiv.org/abs/2308.00142](http://arxiv.org/abs/2308.00142)

    本论文将图上的半监督学习重新表述为非凸推广，通过流形对齐问题的解来找到好的分类器，并通过一种新的中心度度量来选择有信息量的样本。实验证明本方法在低标签率下具有更低的分类错误率。

    

    鉴于低标签率下经典Laplace学习算法退化的问题，我们提出将基于图的半监督学习重新表述为\emph{Trust-Region Subproblem} (TRS) 的非凸推广。这个改进是受到Laplacian特征向量在无限未标记数据极限下的可解性的启发。为了解决这个问题，我们首先证明了一阶条件暗示了流形对齐问题的解，而且经典的\emph{Orthogonal Procrustes} 问题的解可以被用于高效地找到适于进一步细化的好的分类器。接下来，我们解决了在低标签率下选择有监督样本的关键性。我们用图Laplacian的某个子矩阵的主特征向量推导出了一个新的中心度度量，来表征信息样本。我们证明了我们的框架与最近的最先进方法相比，能够实现更低的分类错误率。

    Motivated by the need to address the degeneracy of canonical Laplace learning algorithms in low label rates, we propose to reformulate graph-based semi-supervised learning as a nonconvex generalization of a \emph{Trust-Region Subproblem} (TRS). This reformulation is motivated by the well-posedness of Laplacian eigenvectors in the limit of infinite unlabeled data. To solve this problem, we first show that a first-order condition implies the solution of a manifold alignment problem and that solutions to the classical \emph{Orthogonal Procrustes} problem can be used to efficiently find good classifiers that are amenable to further refinement. Next, we address the criticality of selecting supervised samples at low-label rates. We characterize informative samples with a novel measure of centrality derived from the principal eigenvectors of a certain submatrix of the graph Laplacian. We demonstrate that our framework achieves lower classification error compared to recent state-of-the-art and
    
[^67]: 一套用于表格分类公平性的公平数据集

    A Suite of Fairness Datasets for Tabular Classification. (arXiv:2308.00133v1 [cs.LG])

    [http://arxiv.org/abs/2308.00133](http://arxiv.org/abs/2308.00133)

    这篇论文介绍了一套新的公平数据集，旨在解决表格分类中实验评估数据不足的问题，为未来公平感知机器学习研究提供更严谨的实验评估。

    

    已经有许多关于提高机器学习分类器在表格数据中公平性的算法的论文。不幸的是，大多数论文只使用了非常少的数据集进行实验评估。我们引入了一套函数，用于提取20个公正数据集并提供相关的公平元数据。希望这些数据集能够促进未来公平感知机器学习研究中更严谨的实验评估。

    There have been many papers with algorithms for improving fairness of machine-learning classifiers for tabular data. Unfortunately, most use only very few datasets for their experimental evaluation. We introduce a suite of functions for fetching 20 fairness datasets and providing associated fairness metadata. Hopefully, these will lead to more rigorous experimental evaluations in future fairness-aware machine learning research.
    
[^68]: 使用残差变压器进行脑肿瘤分割的集成学习

    Ensemble Learning with Residual Transformer for Brain Tumor Segmentation. (arXiv:2308.00128v1 [eess.IV])

    [http://arxiv.org/abs/2308.00128](http://arxiv.org/abs/2308.00128)

    本文提出了一种将 Transformer 整合到 U-Net 中的新型网络架构，用于脑肿瘤分割。模型结合了注意力机制和像素级标签，通过残差连接和集成方法进一步提高了分割效果，在 BraTS 2021 数据集上取得了优于最先进方法的结果。

    

    脑肿瘤分割是一个活跃的研究领域，由于高度复杂形状和纹理的肿瘤的划分困难以及常用的 U-Net 架构的失败。最近，在主流研究中，不同神经架构的组合成为一个重要研究方向，特别是 U-Net 和注意力机制结合的 Transformer。与之前的研究不同，本文提出了一种将 Transformer 整合到自适应 U-Net 中的新型网络架构，以合理的计算成本提取出 3D 体积信息。为了防止信息流失，我们进一步添加了残差连接并探索集成方法，因为评估的模型对不同案例和子区域有边缘。在 BraTS 2021 数据集上，我们的模型达到了87.6%的平均 Dice 分数，并超过了最先进的方法，展示了结合多个架构进行优化的潜力。

    Brain tumor segmentation is an active research area due to the difficulty in delineating highly complex shaped and textured tumors as well as the failure of the commonly used U-Net architectures. The combination of different neural architectures is among the mainstream research recently, particularly the combination of U-Net with Transformers because of their innate attention mechanism and pixel-wise labeling. Different from previous efforts, this paper proposes a novel network architecture that integrates Transformers into a self-adaptive U-Net to draw out 3D volumetric contexts with reasonable computational costs. We further add a residual connection to prevent degradation in information flow and explore ensemble methods, as the evaluated models have edges on different cases and sub-regions. On the BraTS 2021 dataset (3D), our model achieves 87.6% mean Dice score and outperforms the state-of-the-art methods, demonstrating the potential for combining multiple architectures to optimize
    
[^69]: DiviML: 一种用于在异构平台上映射神经网络的基于模块的启发式方法

    DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms. (arXiv:2308.00127v1 [cs.LG])

    [http://arxiv.org/abs/2308.00127](http://arxiv.org/abs/2308.00127)

    DiviML是一种基于模块的启发式方法，用于在异构平台上将神经网络映射，通过自动分区和设备映射，实现了编译器级别的分布式神经网络编译，具有较好的可扩展性和质量评估能力。

    

    数据中心越来越多地采用异构架构，并开始包括用于网络、视频处理和深度学习的专用硬件。为了利用现代数据中心的异构计算能力，我们开发了一种在编译器级别将深度神经网络(DNNs)分区映射到多个互联硬件设备的方法。我们提出了一种用于异构DNN编译的通用框架，提供自动分区和设备映射。我们的调度器集成了一个精确求解器，通过混合整数线性规划(MILP)的形式，并使用基于模块性的启发式方法实现可扩展性。此外，我们提出了一个理论下界公式来评估启发式解的质量。我们在一个由CPU和两个设备组成的异构系统上评估了我们的调度器，优化传统的DNNs和随机连接的神经网络，考虑到延迟和吞吐量的约束。

    Datacenters are increasingly becoming heterogeneous, and are starting to include specialized hardware for networking, video processing, and especially deep learning. To leverage the heterogeneous compute capability of modern datacenters, we develop an approach for compiler-level partitioning of deep neural networks (DNNs) onto multiple interconnected hardware devices. We present a general framework for heterogeneous DNN compilation, offering automatic partitioning and device mapping. Our scheduler integrates both an exact solver, through a mixed integer linear programming (MILP) formulation, and a modularity-based heuristic for scalability. Furthermore, we propose a theoretical lower bound formula for the optimal solution, which enables the assessment of the heuristic solutions' quality. We evaluate our scheduler in optimizing both conventional DNNs and randomly-wired neural networks, subject to latency and throughput constraints, on a heterogeneous system comprised of a CPU and two di
    
[^70]: 复杂、新颖物体的密集堆放的卷积占据模型

    Convolutional Occupancy Models for Dense Packing of Complex, Novel Objects. (arXiv:2308.00091v1 [cs.RO])

    [http://arxiv.org/abs/2308.00091](http://arxiv.org/abs/2308.00091)

    该论文提出了一个卷积占据模型F-CON，用于在真实世界中进行复杂、未见过物体的密集堆放。通过与现有规划方法结合，F-CON可以有效解决高度遮挡、部分观察场景下的几何形状感知困难问题，并通过与其他最先进的形状补全方法的比较表现出更好的效果。

    

    在许多仓储和物流应用中，密集堆放是一个重要的特性。这个领域的早期研究主要集中在仿真中的规划算法上，但现实世界中的堆放性能往往受到高度遮挡、部分观察场景中三维物体几何形状感知的困难的限制。在这项工作中，我们提出了一个完全卷积的形状补全模型F-CON，它可以轻松地与现成的规划方法结合，在现实世界中实现密集堆放。我们还发布了一个模拟数据集COB-3D-v2，可以用于训练真实世界机器人应用的形状补全模型，并使用该数据集证明F-CON优于其他最先进的形状补全方法。最后，我们将F-CON应用于真实世界的捡放系统中，并演示了在杂乱的场景中对复杂、未见过的物体进行密集堆放。在多种规划方法下，F-CON实现了比其他方法更好的密集堆放效果。

    Dense packing in pick-and-place systems is an important feature in many warehouse and logistics applications. Prior work in this space has largely focused on planning algorithms in simulation, but real-world packing performance is often bottlenecked by the difficulty of perceiving 3D object geometry in highly occluded, partially observed scenes. In this work, we present a fully-convolutional shape completion model, F-CON, which can be easily combined with off-the-shelf planning methods for dense packing in the real world. We also release a simulated dataset, COB-3D-v2, that can be used to train shape completion models for real-word robotics applications, and use it to demonstrate that F-CON outperforms other state-of-the-art shape completion methods. Finally, we equip a real-world pick-and-place system with F-CON, and demonstrate dense packing of complex, unseen objects in cluttered scenes. Across multiple planning methods, F-CON enables substantially better dense packing than other sh
    
[^71]: 测试分布的单调性和对数凹性的新下界

    New Lower Bounds for Testing Monotonicity and Log Concavity of Distributions. (arXiv:2308.00089v1 [cs.LG])

    [http://arxiv.org/abs/2308.00089](http://arxiv.org/abs/2308.00089)

    该论文介绍了一种新技术，用于证明测试分布单调性和对数凹性的下界。这种方法通过调整一对二项概率的概率构造一对矩匹配的分布族，实现了保持和违背定义不等式的分布，并得到了相关的新下界。

    

    我们开发了一种用于证明分布测试下界的新技术，该技术适用于涉及分布中二项概率的不等式定义的性质。利用这种技术，我们得到了关于离散立方体上的单调性测试的新下界以及关于对数凹性测试的紧密下界。我们的基本技术涉及通过调整一对二项概率的概率来构造一对矩匹配的分布族，使得一个族保持定义的不等式，而另一个族违背它们。

    We develop a new technique for proving distribution testing lower bounds for properties defined by inequalities involving the bin probabilities of the distribution in question. Using this technique we obtain new lower bounds for monotonicity testing over discrete cubes and tight lower bounds for log-concavity testing.  Our basic technique involves constructing a pair of moment-matching families of distributions by tweaking the probabilities of pairs of bins so that one family maintains the defining inequalities while the other violates them.
    
[^72]: 无监督机器学习震荡捕捉高阶CFD求解器

    Unsupervised machine learning shock capturing for High-Order CFD solvers. (arXiv:2308.00086v1 [cs.LG])

    [http://arxiv.org/abs/2308.00086](http://arxiv.org/abs/2308.00086)

    我们提出了一种基于高斯混合模型（GMMs）的无监督机器学习震荡捕捉算法。这种算法具有显著的准确性和鲁棒性，适用于各种复杂几何结构和流动配置。

    

    我们提出了一种基于高斯混合模型（GMMs）的新型无监督机器学习震荡捕捉算法。所提出的GMM传感器在检测震荡方面表现出了显著的准确性，并且在不需要参数调优的情况下在多样的测试案例上都表现出了较好的鲁棒性。我们将基于GMM的传感器与最先进的替代方法进行了比较。所有方法都集成到高阶可压性不连续Galerkin求解器中，人工黏性可以调节以捕捉震荡。超音速测试案例，包括高雷诺数，展示了传感器的性能，表明其效果与精调的最先进传感器相当。%节点DG方法允许在亚单元通量有差异的公式中进行潜在应用，超音速特征检测和网格细化。这种基于GMM的传感器适用于复杂几何结构和各种流动配置，其自适应性和无需大量训练数据集的功能使其具备广泛的应用潜力。

    We present a novel unsupervised machine learning shock capturing algorithm based on Gaussian Mixture Models (GMMs). The proposed GMM sensor demonstrates remarkable accuracy in detecting shocks and is robust across diverse test cases without the need for parameter tuning. We compare the GMM-based sensor with state-of-the-art alternatives. All methods are integrated into a high-order compressible discontinuous Galerkin solver where artificial viscosity can be modulated to capture shocks. Supersonic test cases, including high Reynolds numbers, showcase the sensor's performance, demonstrating the same effectiveness as fine-tuned state-of-the-art sensors. %The nodal DG aproach allows for potential applications in sub-cell flux-differencing formulations, supersonic feature detection, and mesh refinement. The adaptive nature and ability to function without extensive training datasets make this GMM-based sensor suitable for complex geometries and varied flow configurations. Our study reveals t
    
[^73]: 一种基于深度学习的新型网络入侵检测系统对抗对抗攻击的模型

    A Novel Deep Learning based Model to Defend Network Intrusion Detection System against Adversarial Attacks. (arXiv:2308.00077v1 [cs.CR])

    [http://arxiv.org/abs/2308.00077](http://arxiv.org/abs/2308.00077)

    本论文研究了一种基于深度学习的网络入侵检测系统（NIDS）对抗对抗攻击的新型模型。通过实施强大的对抗攻击方法，如FGSM、JSMA、PGD和C&W，并采用对抗训练作为防御方法来增强NIDS模型的稳健性。

    

    网络入侵检测系统（NIDS）是保护网络空间免受各种安全风险和未知网络攻击的重要工具。已经实施了许多基于机器学习（ML）和深度学习（DL）的NIDS解决方案。然而，所有这些解决方案都容易受到对抗性攻击的影响，恶意行为者通过向系统中注入对抗扰动样本来试图回避或欺骗模型。本研究的主要目标是研究强大的对抗攻击算法及其对DL-based NIDS的防御方法。快速梯度符号方法（FGSM），Jacobian显著性图攻击（JSMA），投影梯度下降（PGD）和Carlini＆Wagner（C＆W）是四种对NIDS实施的强大对抗攻击方法。作为防御方法，采用对抗训练来增强NIDS模型的稳健性。结果可以总结为三个阶段，即1）对抗攻击前，2）对抗攻击后，3）对抗攻击后。

    Network Intrusion Detection System (NIDS) is an essential tool in securing cyberspace from a variety of security risks and unknown cyberattacks. A number of solutions have been implemented for Machine Learning (ML), and Deep Learning (DL) based NIDS. However, all these solutions are vulnerable to adversarial attacks, in which the malicious actor tries to evade or fool the model by injecting adversarial perturbed examples into the system. The main aim of this research work is to study powerful adversarial attack algorithms and their defence method on DL-based NIDS. Fast Gradient Sign Method (FGSM), Jacobian Saliency Map Attack (JSMA), Projected Gradient Descent (PGD) and Carlini & Wagner (C&W) are four powerful adversarial attack methods implemented against the NIDS. As a defence method, Adversarial Training is used to increase the robustness of the NIDS model. The results are summarized in three phases, i.e., 1) before the adversarial attack, 2) after the adversarial attack, and 3) aft
    
[^74]: 人群安全管理系统：基于数据驱动的活动决策支持的规划和控制

    Crowd Safety Manager: Towards Data-Driven Active Decision Support for Planning and Control of Crowd Events. (arXiv:2308.00076v1 [cs.AI])

    [http://arxiv.org/abs/2308.00076](http://arxiv.org/abs/2308.00076)

    这篇论文介绍了一种新颖的技术和方法，旨在通过数据驱动的决策支持系统提升人群管理的规划和操作阶段。该方法利用创新的数据收集技术、数据集成和3D数字孪生技术，结合人工智能工具进行风险识别，并引入了蝴蝶结模型来评估和预测风险水平。

    

    本文提出了一种新颖的技术和方法，旨在增强规划和操作阶段的人群管理。该方法包括创新的数据收集技术、数据集成和可视化，使用3D数字孪生技术，并结合人工智能工具进行风险识别。本文介绍了“蝴蝶结”模型，这是一个综合性框架，旨在评估和预测风险水平。该模型结合了客观估计和预测，如交通流量运营和拥挤程度，以及各种恶化因素，如天气条件、情绪和游客的目的，以评估潜在事件风险。提出的框架应用于Scheveningen的人群安全管理项目，其中DigiTwin基于丰富的实时数据来源进行开发。一个值得注意的数据来源是Resono，提供访客数量和动向的见解，充分利用了一组

    This paper presents novel technology and methodology aimed at enhancing crowd management in both the planning and operational phases. The approach encompasses innovative data collection techniques, data integration, and visualization using a 3D Digital Twin, along with the incorporation of artificial intelligence (AI) tools for risk identification. The paper introduces the Bowtie model, a comprehensive framework designed to assess and predict risk levels. The model combines objective estimations and predictions, such as traffic flow operations and crowdedness levels, with various aggravating factors like weather conditions, sentiments, and the purpose of visitors, to evaluate the expected risk of incidents. The proposed framework is applied to the Crowd Safety Manager project in Scheveningen, where the DigiTwin is developed based on a wealth of real-time data sources. One noteworthy data source is Resono, offering insights into the number of visitors and their movements, leveraging a m
    
[^75]: 使用Kernel SHAP XAI方法优化网络异常检测模型

    Using Kernel SHAP XAI Method to optimize the Network Anomaly Detection Model. (arXiv:2308.00074v1 [cs.LG])

    [http://arxiv.org/abs/2308.00074](http://arxiv.org/abs/2308.00074)

    本文使用了可解释的人工智能（XAI）方法中的核SHAP方法来检测和解释网络异常，并且通过该方法优化了网络异常检测模型的准确性、召回率、精确度和F分数。

    

    异常检测及其解释在许多研究领域中非常重要，比如入侵检测、欺诈检测、网络流量和日志中的未知攻击检测。由于其无边界和缺乏监督性质，很难确定为什么一个实例是异常的，而另一个实例不是。这个问题的答案可能来自可解释的人工智能（XAI）这一新兴技术。XAI提供了解释和解释复杂模型（如深度学习）输出和工作的工具和技术。本文旨在通过XAI的核SHAP方法检测和解释网络异常。采用相同的方法来提高网络异常检测模型的准确性、召回率、精确度和F分数。使用最新的CICIDS2017数据集进行实验。创建了两个模型（Model_1和OPT_Model）进行比较。OPT_Model在无监督方式训练时的整体准确性和F分数为。

    Anomaly detection and its explanation is important in many research areas such as intrusion detection, fraud detection, unknown attack detection in network traffic and logs. It is challenging to identify the cause or explanation of why one instance is an anomaly? and the other is not due to its unbounded and lack of supervisory nature. The answer to this question is possible with the emerging technique of explainable artificial intelligence (XAI). XAI provides tools and techniques to interpret and explain the output and working of complex models such as Deep Learning (DL). This paper aims to detect and explain network anomalies with XAI, kernelSHAP method. The same approach is used to improve the network anomaly detection model in terms of accuracy, recall, precision and f score. The experiment is conduced with the latest CICIDS2017 dataset. Two models are created (Model_1 and OPT_Model) and compared. The overall accuracy and F score of OPT_Model (when trained in unsupervised way) are 
    
[^76]: 可解释的推理方法用于刻板印象识别

    Interpretable Stereotype Identification through Reasoning. (arXiv:2308.00071v1 [cs.CL])

    [http://arxiv.org/abs/2308.00071](http://arxiv.org/abs/2308.00071)

    本研究通过使用推理方法，在零射击刻板印象识别中取得了重要的进展，并发现推理的性能增益远远超过模型规模扩展的增益。推理不仅提高了准确性，还提高了决策的可解释性。

    

    鉴于语言模型训练使用了包含固有偏见的大量数据集，可能会不经意地持续系统性歧视，因此，审查和解决语言模型中的偏见变得至关重要，将公平性整合到它们的发展中，以确保这些模型具有公正和无偏的特性。在这项工作中，我们展示了基于Vicuna-13B-v1.3的零射击刻板印象识别中推理的重要性。尽管我们观察到从13B到33B的规模扩展会提高准确性，但我们表明推理的性能增益远远超过规模扩展的增益。我们的研究结果表明，推理可能是使LLMs在刻板印象等领域任务上超越规模定律的关键因素。此外，通过对选定的推理追踪进行定性分析，我们突出显示了推理不仅提高了准确性，还提高了决策的可解释性。

    Given that language models are trained on vast datasets that may contain inherent biases, there is a potential danger of inadvertently perpetuating systemic discrimination. Consequently, it becomes essential to examine and address biases in language models, integrating fairness into their development to ensure these models are equitable and free from bias. In this work, we demonstrate the importance of reasoning in zero-shot stereotype identification based on Vicuna-13B-v1.3. While we do observe improved accuracy by scaling from 13B to 33B, we show that the performance gain from reasoning significantly exceeds the gain from scaling up. Our findings suggest that reasoning could be a key factor that enables LLMs to trescend the scaling law on out-of-domain tasks such as stereotype identification. Additionally, through a qualitative analysis of select reasoning traces, we highlight how reasoning enhances not just accuracy but also the interpretability of the decision.
    
[^77]: FinPT:使用预训练基础模型进行资金风险预测中个人资料调整

    FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models. (arXiv:2308.00065v1 [q-fin.RM])

    [http://arxiv.org/abs/2308.00065](http://arxiv.org/abs/2308.00065)

    FinPT是一种新颖的资金风险预测方法，通过在大型预训练基础模型上进行个人资料调整，填充金融表格数据并获得自然语言客户资料，从而提高预测准确性。

    

    资金风险预测在金融领域中起着至关重要的作用。机器学习方法已被广泛应用于自动检测潜在风险，从而节省劳动成本。然而，近年来这一领域的发展滞后于以下两个事实：1）所使用的算法有些过时，特别是在生成AI和大型语言模型（LLMs）快速发展的背景下；2）缺乏统一且开源的金融基准已经阻碍了相关研究多年。为解决这些问题，我们提出了FinPT和FinBench：前者是一种新颖的资金风险预测方法，对大型预训练基础模型进行个人资料调整；后者是一套关于资金风险的高质量数据集，如违约、欺诈和流失。在FinPT中，我们将金融表格数据填充到预定义的指令模板中，通过提示LLMs获得自然语言客户资料，并进行精调。

    Financial risk prediction plays a crucial role in the financial sector. Machine learning methods have been widely applied for automatically detecting potential risks and thus saving the cost of labor. However, the development in this field is lagging behind in recent years by the following two facts: 1) the algorithms used are somewhat outdated, especially in the context of the fast advance of generative AI and large language models (LLMs); 2) the lack of a unified and open-sourced financial benchmark has impeded the related research for years. To tackle these issues, we propose FinPT and FinBench: the former is a novel approach for financial risk prediction that conduct Profile Tuning on large pretrained foundation models, and the latter is a set of high-quality datasets on financial risks such as default, fraud, and churn. In FinPT, we fill the financial tabular data into the pre-defined instruction template, obtain natural-language customer profiles by prompting LLMs, and fine-tune 
    
[^78]: T-Fusion Net：一种新颖的深度神经网络，基于多个定位的空间注意机制用于Covid-19检测

    T-Fusion Net: A Novel Deep Neural Network Augmented with Multiple Localizations based Spatial Attention Mechanisms for Covid-19 Detection. (arXiv:2308.00053v1 [eess.IV])

    [http://arxiv.org/abs/2308.00053](http://arxiv.org/abs/2308.00053)

    T-Fusion Net是一种新颖的深度神经网络，通过多个定位的空间注意机制提高了图像分类的性能。通过使用模糊最大融合的方法合并多个模型的输出，进一步提高了准确性。实验结果表明在Covid-19数据集上取得了良好的效果。

    

    最近几年，深度神经网络在图像分类任务中表现出更好的性能。然而，数据集的不断复杂化和对改进性能的需求需要探索创新技术。本研究提出了一种新的深度神经网络（称为T-Fusion Net），它增强了基于多个定位的空间注意机制。这种注意机制使得网络能够专注于相关的图像区域，提高其判别能力。进一步使用这种网络的均质模型来提高图像分类准确性。对于合并，提出的方法考虑了每个T-Fusion Net个体的多个实例。模型通过模糊最大融合来合并个体网络的输出。融合过程通过精心选择的参数进行优化，以在个体模型的贡献方面取得平衡。在基准Covid-19（SARS-CoV-2 CT扫描）数据集上的实验评估结果表明...（未完待续）

    In recent years, deep neural networks are yielding better performance in image classification tasks. However, the increasing complexity of datasets and the demand for improved performance necessitate the exploration of innovative techniques. The present work proposes a new deep neural network (called as, T-Fusion Net) that augments multiple localizations based spatial attention. This attention mechanism allows the network to focus on relevant image regions, improving its discriminative power. A homogeneous ensemble of the said network is further used to enhance image classification accuracy. For ensembling, the proposed approach considers multiple instances of individual T-Fusion Net. The model incorporates fuzzy max fusion to merge the outputs of individual nets. The fusion process is optimized through a carefully chosen parameter to strike a balance on the contributions of the individual models. Experimental evaluations on benchmark Covid-19 (SARS-CoV-2 CT scan) dataset demonstrate t
    
[^79]: 生成人工智能的强化学习：现状、机会和开放研究挑战

    Reinforcement Learning for Generative AI: State of the Art, Opportunities and Open Research Challenges. (arXiv:2308.00031v1 [cs.LG])

    [http://arxiv.org/abs/2308.00031](http://arxiv.org/abs/2308.00031)

    这篇论文调查了在生成人工智能中应用强化学习的现状、机会和开放研究问题。作者主要讨论了三种应用类型：无特定目标的生成方式、同时最大化目标函数的输出生成方式以及将无法通过目标函数捕捉的期望特征嵌入生成过程的方式。这个新兴领域中存在着丰富的机会和挑战。

    

    生成人工智能（AI）是近十年来计算机科学领域最令人兴奋的发展之一。与此同时，强化学习（RL）在各种机器学习任务中已经成为非常成功的范式。在本调查中，我们讨论了将RL应用于生成AI中的现状、机会和开放的研究问题。具体而言，我们将讨论三种应用类型，即作为一种无特定目标的生成方式，作为一种同时最大化目标函数的输出生成方式，以及作为一种将无法通过目标函数轻松捕捉的期望特征嵌入生成过程的方式。我们在调查结果中对这个迷人的新兴领域中的机会和挑战进行了深入的讨论。

    Generative Artificial Intelligence (AI) is one of the most exciting developments in Computer Science of the last decade. At the same time, Reinforcement Learning (RL) has emerged as a very successful paradigm for a variety of machine learning tasks. In this survey, we discuss the state of the art, opportunities and open research questions in applying RL to generative AI. In particular, we will discuss three types of applications, namely, RL as an alternative way for generation without specified objectives; as a way for generating outputs while concurrently maximizing an objective function; and, finally, as a way of embedding desired characteristics, which cannot be easily captured by means of an objective function, into the generative process. We conclude the survey with an in-depth discussion of the opportunities and challenges in this fascinating emerging area.
    
[^80]: 探索生成式人工智能如何解释音乐

    Exploring how a Generative AI interprets music. (arXiv:2308.00015v1 [cs.SD])

    [http://arxiv.org/abs/2308.00015](http://arxiv.org/abs/2308.00015)

    使用MusicVAE解释音乐时，大部分关于音高和节奏的信息被编码在前几个音乐神经元中，而旋律的概念则表现为较长的音乐序列中的独立神经元。

    

    我们使用Google的MusicVAE，一个具有512维潜在空间的变分自动编码器来表示几小节的音乐，并根据它们在描述音乐方面的相关性来组织潜在维度。我们发现，平均而言，大多数潜在神经元在输入真实音乐轨道时保持沉默：我们称之为“噪声”神经元。其余少数被激活的潜在神经元被称为“音乐”神经元。我们想知道哪些神经元携带着音乐信息以及它们编码的是哪种类型的音乐信息，即可以识别为音高、节奏或旋律的信息。我们发现，大部分关于音高和节奏的信息都被编码在前几个音乐神经元中：因此，神经网络构建了一些非线性编码许多用于描述音高和节奏的人为定义变量的变量。旋律的概念似乎只会在较长的音乐序列中出现在独立的神经元中。

    We use Google's MusicVAE, a Variational Auto-Encoder with a 512-dimensional latent space to represent a few bars of music, and organize the latent dimensions according to their relevance in describing music. We find that, on average, most latent neurons remain silent when fed real music tracks: we call these "noise" neurons. The remaining few dozens of latent neurons that do fire are called "music neurons". We ask which neurons carry the musical information and what kind of musical information they encode, namely something that can be identified as pitch, rhythm or melody. We find that most of the information about pitch and rhythm is encoded in the first few music neurons: the neural network has thus constructed a couple of variables that non-linearly encode many human-defined variables used to describe pitch and rhythm. The concept of melody only seems to show up in independent neurons for longer sequences of music.
    
[^81]: 单声道多人演讲分离使用高效Transformer模型

    Monaural Multi-Speaker Speech Separation Using Efficient Transformer Model. (arXiv:2308.00010v1 [cs.SD])

    [http://arxiv.org/abs/2308.00010](http://arxiv.org/abs/2308.00010)

    该论文介绍了一种基于Transformer模型的单声道多人演讲分离方法，旨在减少计算复杂性，并在准确性和性能之间取得平衡。

    

    多人聚会问题是一个难以分离或区分来自几个说话者的混合语音中的个别说话者的场景。在这个领域已经进行了几项研究，但模型的大小和复杂性正在与语音分离的准确性和鲁棒性进行权衡。"单声道多人演讲分离"提出了一种基于Transformer架构及其高效形式的演讲分离模型。该模型使用包含多样化说话者话语的LibriMix数据集进行训练。该模型可以从混合音频输入中分离出2个不同的说话者源。该模型的开发目标是减少语音分离模型的计算复杂性，并在与现有语音分离模型性能最小化权衡的同时，实现显著的改进。该项目预计将为语音分离领域的持续研究做出贡献，同时降低计算成本。

    Cocktail party problem is the scenario where it is difficult to separate or distinguish individual speaker from a mixed speech from several speakers. There have been several researches going on in this field but the size and complexity of the model is being traded off with the accuracy and robustness of speech separation. "Monaural multi-speaker speech separation" presents a speech-separation model based on the Transformer architecture and its efficient forms. The model has been trained with the LibriMix dataset containing diverse speakers' utterances. The model separates 2 distinct speaker sources from a mixed audio input. The developed model approaches the reduction in computational complexity of the speech separation model, with minimum tradeoff with the performance of prevalent speech separation model and it has shown significant movement towards that goal. This project foresees, a rise in contribution towards the ongoing research in the field of speech separation with computationa
    
[^82]: 一种用于评估冠心病的3D深度学习分类器及其可解释性

    A 3D deep learning classifier and its explainability when assessing coronary artery disease. (arXiv:2308.00009v1 [eess.IV])

    [http://arxiv.org/abs/2308.00009](http://arxiv.org/abs/2308.00009)

    本文提出了一种3D深度学习模型，用于直接分类冠心病患者和正常受试者，相较于2D模型提高了23.65%的性能，并通过Grad-GAM提供了可解释性。此外，通过与2D语义分割相结合，实现了更好的解释性和准确的异常定位。

    

    早期发现和诊断冠心病（CAD）可挽救生命并降低医疗成本。在本研究中，我们提出了一个3D Resnet-50深度学习模型，可以直接对计算机断层扫描冠状动脉造影图像上的正常受试者和冠心病患者进行分类。我们的方法比2D Resnet-50模型提高了23.65%。通过使用Grad-GAM提供了可解释性。此外，我们将3D冠心病分类与2D二类语义分割相结合，以提高解释性和准确的异常定位。

    Early detection and diagnosis of coronary artery disease (CAD) could save lives and reduce healthcare costs. In this study, we propose a 3D Resnet-50 deep learning model to directly classify normal subjects and CAD patients on computed tomography coronary angiography images. Our proposed method outperforms a 2D Resnet-50 model by 23.65%. Explainability is also provided by using a Grad-GAM. Furthermore, we link the 3D CAD classification to a 2D two-class semantic segmentation for improved explainability and accurate abnormality localisation.
    
[^83]: 基于数据的深度学习方法进行气道分割

    A data-centric deep learning approach to airway segmentation. (arXiv:2308.00008v1 [eess.IV])

    [http://arxiv.org/abs/2308.00008](http://arxiv.org/abs/2308.00008)

    我们提出了一种基于数据的深度学习技术来进行气道分割，通过插值和图像分割提高数据质量，采用集成学习策略在多个尺度上聚合气道树，性能优于基线模型，适用于任何2D深度学习模型。

    

    气道树异常的形态和分布可以用于诊断和疾病表征各种慢性呼吸状况。在这方面，气道分割在生成整个气道树轮廓以估计疾病范围和严重程度方面起着关键作用。在本研究中，我们提出了一种基于数据的深度学习技术来分割气道树。所提出的技术利用插值和图像分割来提高数据的有用性和质量。然后，我们实施了一个集成学习策略来聚合不同尺度下的分割的气道树。在分割性能方面（Dice相似系数），当使用组合损失时，我们的方法平均优于基线模型2.5%。此外，我们提出的技术使用GPU较少，灵活性高，可以部署在任何2D深度学习模型上。

    The morphology and distribution of airway tree abnormalities enables diagnosis and disease characterisation across a variety of chronic respiratory conditions. In this regard, airway segmentation plays a critical role in the production of the outline of the entire airway tree to enable estimation of disease extent and severity. In this study, we propose a data-centric deep learning technique to segment the airway tree. The proposed technique utilises interpolation and image split to improve data usefulness and quality. Then, an ensemble learning strategy is implemented to aggregate the segmented airway trees at different scales. In terms of segmentation performance (dice similarity coefficient), our method outperforms the baseline model by 2.5% on average when a combined loss is used. Further, our proposed technique has a low GPU usage and high flexibility enabling it to be deployed on any 2D deep learning model.
    
[^84]: 时间常识推理与获取概述

    An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])

    [http://arxiv.org/abs/2308.00002](http://arxiv.org/abs/2308.00002)

    本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。

    

    时间常识推理是指理解短语、动作和事件的典型时间背景并将其应用于需要这种知识的问题推理的能力。这种能力在时间自然语言处理任务中至关重要，可能应用于时间线摘要、时间问答和时间自然语言推断等方面。最近的研究表明，大型语言模型虽然善于生成语法正确的句子和解决分类任务，但在推理过程中往往会采取捷径，并陷入简单的语言陷阱。本文章概述了在时间常识推理领域的研究，特别关注通过各种增强方式提高语言模型的性能以及对越来越多数据集的评估。然而，这些增强模型在推理任务上仍然难以达到人类的水平。

    Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
    
[^85]: 学习具有非刚性握姿注册的通用工具使用

    Learning Generalizable Tool Use with Non-rigid Grasp-pose Registration. (arXiv:2307.16499v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2307.16499](http://arxiv.org/abs/2307.16499)

    本研究提出了一种新的方法，通过非刚性握姿注册来学习通用工具使用行为。该方法利用单个示范可扩展地学习新类别中的工具操作，并通过引导策略搜索解决复杂的工具使用任务，并在测试时推广到未见过的工具。

    

    工具使用是人类智能的一个标志性特征，由于复杂的接触和高维动作空间，它仍然是机器人领域中的一个挑战性问题。在这项工作中，我们提出了一种新的方法来实现工具使用行为的强化学习。我们的方法提供了一种可扩展的方式，只使用一个示范即可学习新类别中的工具操作。为此，我们提出了一种新的方法，将多指机器人手的抓取配置推广到新对象上。通过有利的初始化和形状奖励信号，我们使用此方法来引导策略搜索。学到的策略可以解决复杂的工具使用任务，并且在测试时可以推广到未见过的工具。训练过的策略的可视化和视频可在https://maltemosbach.github.io/generalizable_tool_use获得。

    Tool use, a hallmark feature of human intelligence, remains a challenging problem in robotics due the complex contacts and high-dimensional action space. In this work, we present a novel method to enable reinforcement learning of tool use behaviors. Our approach provides a scalable way to learn the operation of tools in a new category using only a single demonstration. To this end, we propose a new method for generalizing grasping configurations of multi-fingered robotic hands to novel objects. This is used to guide the policy search via favorable initializations and a shaped reward signal. The learned policies solve complex tool use tasks and generalize to unseen tools at test time. Visualizations and videos of the trained policies are available at https://maltemosbach.github.io/generalizable_tool_use.
    
[^86]: L3DMC: 使用混合曲率空间的蒸馏进行终身学习

    L3DMC: Lifelong Learning using Distillation via Mixed-Curvature Space. (arXiv:2307.16459v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.16459](http://arxiv.org/abs/2307.16459)

    L3DMC是一种使用混合曲率空间进行终身学习的蒸馏策略，通过建模和维护复杂几何结构来保留已经学到的知识。

    

    当终身学习模型在一系列任务上进行训练时，其性能会下降，因为在顺序学习新概念时嵌入空间的几何结构会发生变化。现有的终身学习方法大多在固定曲率（例如零曲率的欧几里德空间）上运行，这并不适合建模复杂的数据几何结构。此外，蒸馏策略直接应用于低维嵌入，通过使模型高度稳定来阻碍终身学习模型学习新概念。为了解决这个问题，我们提出了一种名为L3DMC的蒸馏策略，它在混合曲率空间上操作，通过建模和维护复杂的几何结构来保留已经学到的知识。我们建议使用正定的重构核希尔伯特空间（RKHS）将固定曲率空间（欧几里德和双曲）的投影低维嵌入到更高维度的空间中。

    The performance of a lifelong learning (L3) model degrades when it is trained on a series of tasks, as the geometrical formation of the embedding space changes while learning novel concepts sequentially. The majority of existing L3 approaches operate on a fixed-curvature (e.g., zero-curvature Euclidean) space that is not necessarily suitable for modeling the complex geometric structure of data. Furthermore, the distillation strategies apply constraints directly on low-dimensional embeddings, discouraging the L3 model from learning new concepts by making the model highly stable. To address the problem, we propose a distillation strategy named L3DMC that operates on mixed-curvature spaces to preserve the already-learned knowledge by modeling and maintaining complex geometrical structures. We propose to embed the projected low dimensional embedding of fixed-curvature spaces (Euclidean and hyperbolic) to higher-dimensional Reproducing Kernel Hilbert Space (RKHS) using a positive-definite k
    
[^87]: 持续学习的子空间蒸馏

    Subspace Distillation for Continual Learning. (arXiv:2307.16419v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.16419](http://arxiv.org/abs/2307.16419)

    该论文提出了一种新颖的知识蒸馏技术，通过近似数据流形，并用线性子空间建模结构，来在持续学习中减轻灾难性遗忘。实验证明，该方法优于其他方法，在多个具有挑战性的持续学习任务中表现出色。

    

    在持续学习中，一个最终的目标是保留在前面任务中学到的知识，同时学习新任务。为了减轻对先前知识的遗忘，我们提出了一种新颖的知识蒸馏技术，该技术考虑了神经网络潜在/输出空间的流形结构在学习新任务中。为了实现这一点，我们提出了一种近似数据流形的方法，从而通过线性子空间来建模结构并保持神经网络在学习新概念时的知识。我们证明了使用子空间建模具有一些有趣的特性，包括对噪声的鲁棒性，因此在持续学习中可以有效地减轻灾难性遗忘。我们还讨论并展示了我们的方法如何适应分类和分割问题。经验上，我们观察到我们的方法在几个具有挑战性的持续学习方法上表现优于其他方法。

    An ultimate objective in continual learning is to preserve knowledge learned in preceding tasks while learning new tasks. To mitigate forgetting prior knowledge, we propose a novel knowledge distillation technique that takes into the account the manifold structure of the latent/output space of a neural network in learning novel tasks. To achieve this, we propose to approximate the data manifold up-to its first order, hence benefiting from linear subspaces to model the structure and maintain the knowledge of a neural network while learning novel concepts. We demonstrate that the modeling with subspaces provides several intriguing properties, including robustness to noise and therefore effective for mitigating Catastrophic Forgetting in continual learning. We also discuss and show how our proposed method can be adopted to address both classification and segmentation problems. Empirically, we observe that our proposed method outperforms various continual learning methods on several challe
    
[^88]: 重新思考多模态实体对齐中的不确定性缺失和模糊的视觉模态

    Rethinking Uncertainly Missing and Ambiguous Visual Modality in Multi-Modal Entity Alignment. (arXiv:2307.16210v1 [cs.AI])

    [http://arxiv.org/abs/2307.16210](http://arxiv.org/abs/2307.16210)

    在多模态实体对齐中，现有的方法忽视了视觉图像的不完整性和模糊性，本文通过分析表明模型在面对不完整性时容易出现过拟合和性能下降的问题。

    

    作为实体对齐（EA）的重要扩展，多模态实体对齐（MMEA）旨在通过利用相关的视觉信息来识别跨不同知识图谱（KGs）之间的相同实体。然而，现有的MMEA方法主要集中在多模态实体特征的融合范式上，而忽视了缺失和内在模糊性的视觉图像所带来的挑战。本文对视觉模态不完整性进行了进一步分析，在我们提出的MMEA-UMVM数据集上对最新的MMEA模型进行了基准测试，该数据集包含涵盖双语和单语对齐KGs的类型，并采用标准（非迭代）和迭代训练范式来评估模型性能。我们的研究表明，在面对模态不完整性时，模型很容易过拟合模态噪声，并在高缺失模态的情况下出现性能振荡或下降。这证明了增加视觉不确定性的问题。

    As a crucial extension of entity alignment (EA), multi-modal entity alignment (MMEA) aims to identify identical entities across disparate knowledge graphs (KGs) by exploiting associated visual information. However, existing MMEA approaches primarily concentrate on the fusion paradigm of multi-modal entity features, while neglecting the challenges presented by the pervasive phenomenon of missing and intrinsic ambiguity of visual images. In this paper, we present a further analysis of visual modality incompleteness, benchmarking latest MMEA models on our proposed dataset MMEA-UMVM, where the types of alignment KGs covering bilingual and monolingual, with standard (non-iterative) and iterative training paradigms to evaluate the model performance. Our research indicates that, in the face of modality incompleteness, models succumb to overfitting the modality noise, and exhibit performance oscillations or declines at high rates of missing modality. This proves that the inclusion of additiona
    
[^89]: 非对抗性后门防御

    Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])

    [http://arxiv.org/abs/2307.15539](http://arxiv.org/abs/2307.15539)

    提出了一种非对抗性后门防御框架，通过在被污染样本中注入非对抗性后门，当触发时可以抑制攻击者对污染数据的后门攻击，同时保持对干净数据的影响有限。

    

    深度神经网络（DNNs）容易受到后门攻击的影响，这种攻击并不会影响网络对干净数据的性能，但一旦添加触发模式，就会操纵网络行为。现有的防御方法大大降低了攻击成功率，但它们在干净数据上的预测准确性仍然远远落后于干净模型。受后门攻击的隐蔽性和有效性的启发，我们提出了一个简单但非常有效的防御框架，该框架注入了针对被污染样本的非对抗性后门。按照后门攻击的一般步骤，我们检测一小组可疑样本，然后对它们应用毒化策略。一旦触发，非对抗性后门抑制了攻击者对污染数据的后门攻击，但对干净数据的影响有限。防御可以在数据预处理期间进行，而不需要对标准的端到端训练流程进行任何修改。

    Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on mul
    
[^90]: 基于浅层单变量ReLU网络的噪声插值学习

    Noisy Interpolation Learning with Shallow Univariate ReLU Networks. (arXiv:2307.15396v1 [cs.LG])

    [http://arxiv.org/abs/2307.15396](http://arxiv.org/abs/2307.15396)

    使用最小范数的两层ReLU网络进行噪声单变量回归的插值，对于$L_1$损失和$ p <2 $的$L_p$损失抑制过拟合，但对于$ p \geq 2 $的损失是灾难性的。

    

    我们研究了噪声单变量回归中使用最小范数（权重的$\ell_2$范数）的两层ReLU网络进行插值的渐近过拟合行为。我们发现对于$L_1$损失和$ p <2 $的任何$L_p$损失，过拟合现象会被抑制，但对于$ p \geq 2 $的损失是灾难性的。

    We study the asymptotic overfitting behavior of interpolation with minimum norm ($\ell_2$ of the weights) two-layer ReLU networks for noisy univariate regression. We show that overfitting is tempered for the $L_1$ loss, and any $L_p$ loss for $p<2$, but catastrophic for $p\geq 2$.
    
[^91]: 在分布式随机组合极小化优化中实现线性加速

    Achieving Linear Speedup in Decentralized Stochastic Compositional Minimax Optimization. (arXiv:2307.13430v1 [cs.LG])

    [http://arxiv.org/abs/2307.13430](http://arxiv.org/abs/2307.13430)

    本论文提出了一种新颖的算法，可在分布式环境中实现线性加速，以解决组合极小化问题中内层函数的共识误差问题。

    

    近年来，随机组合极小化问题引起了广泛关注，因为它涵盖了许多新兴的机器学习模型。与此同时，由于分布式数据的出现，需要在分散设置下优化这种问题。然而，损失函数中的组合结构给设计高效的分散式优化算法带来了独特的挑战。我们的研究表明，由于对内层函数的共识误差较大，标准的传递策略无法在分散式组合极小化问题中实现线性加速。为解决这个问题，我们开发了一种新颖的带有动量的分散式随机组合梯度下降算法，以减小内层函数的共识误差。因此，我们的理论结果表明，它可以实现与工作者数量成线性加速。我们相信这种新颖的算法能够在分布式环境中为组合极小化问题提供线性加速。

    The stochastic compositional minimax problem has attracted a surge of attention in recent years since it covers many emerging machine learning models. Meanwhile, due to the emergence of distributed data, optimizing this kind of problem under the decentralized setting becomes badly needed. However, the compositional structure in the loss function brings unique challenges to designing efficient decentralized optimization algorithms. In particular, our study shows that the standard gossip communication strategy cannot achieve linear speedup for decentralized compositional minimax problems due to the large consensus error about the inner-level function. To address this issue, we developed a novel decentralized stochastic compositional gradient descent ascent with momentum algorithm to reduce the consensus error in the inner-level function. As such, our theoretical results demonstrate that it is able to achieve linear speedup with respect to the number of workers. We believe this novel algo
    
[^92]: 大数据和信息技术对弱势群体的双刃剑：开放银行的一个警示故事

    The Double-Edged Sword of Big Data and Information Technology for the Disadvantaged: A Cautionary Tale from Open Banking. (arXiv:2307.13408v1 [cs.LG])

    [http://arxiv.org/abs/2307.13408](http://arxiv.org/abs/2307.13408)

    本研究利用开放银行作为例子，分析了大数据和强大的信息技术（如机器学习）所带来的公平性隐患。通过研究金融脆弱性的维度，我们展示了细致交易数据的能力，并提醒对其应当谨慎使用以避免潜在的歧视问题。

    

    本研究分析和展示了看似中立的数据和强大的技术（如机器学习）在开放银行等领域对公平性的隐藏影响。开放银行在金融服务领域引发了一场革命，为客户获取、管理、保留和风险评估打开了新的机会。然而，交易数据的细致程度可能会带来潜在的危害，未被注意的敏感和禁止性特征的代理可能导致间接歧视。在这个背景下，我们通过公平解释的视角，研究了金融脆弱性（FV）的维度，这是COVID-19和通胀上升带来的全球关注点。具体而言，我们试图了解导致FV的行为因素，以及它如何影响处于风险群体中的弱势群体。使用来自英国一家金融科技借贷公司的独特数据集，我们展示了细粒度交易数据的能力，同时也对其提出了警告。

    This research article analyses and demonstrates the hidden implications for fairness of seemingly neutral data coupled with powerful technology, such as machine learning (ML), using Open Banking as an example. Open Banking has ignited a revolution in financial services, opening new opportunities for customer acquisition, management, retention, and risk assessment. However, the granularity of transaction data holds potential for harm where unnoticed proxies for sensitive and prohibited characteristics may lead to indirect discrimination. Against this backdrop, we investigate the dimensions of financial vulnerability (FV), a global concern resulting from COVID-19 and rising inflation. Specifically, we look to understand the behavioral elements leading up to FV and its impact on at-risk, disadvantaged groups through the lens of fair interpretation. Using a unique dataset from a UK FinTech lender, we demonstrate the power of fine-grained transaction data while simultaneously cautioning its
    
[^93]: MARIO: 用于改善图对比学习的模型无关配方，提高OOD泛化性能

    MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v1 [cs.LG])

    [http://arxiv.org/abs/2307.13055](http://arxiv.org/abs/2307.13055)

    提出了一个模型无关配方MARIO，用于改善图对比学习的OOD泛化性能。MARIO引入了信息瓶颈原则和不变性原则，旨在获得具有分布偏移鲁棒性和不变性的图表示。

    

    在这项工作中，我们研究了图数据上无监督学习方法的域外泛化问题。这种情况特别具有挑战性，因为即使有标签，图神经网络(GNNs)也显示出对分布偏移的敏感性。为了解决这个挑战，我们提出了一种名为MARIO的模型无关配方，旨在开发具有分布偏移鲁棒性的图对比方法，克服现有框架的局限性：(i)信息瓶颈(IB)原则用于实现可泛化的表示，(ii)不变性原则采用对抗性数据增强来获得不变表示。据我们所知，这是第一项研究OOD泛化问题的工作

    In this work, we investigate the problem of out-of-distribution (OOD) generalization for unsupervised learning methods on graph data. This scenario is particularly challenging because graph neural networks (GNNs) have been shown to be sensitive to distributional shifts, even when labels are available. To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic \underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability of unsupervised graph contrastive learning methods, which we refer to as MARIO. MARIO introduces two principles aimed at developing distributional-shift-robust graph contrastive methods to overcome the limitations of existing frameworks: (i) Information Bottleneck (IB) principle for achieving generalizable representations and (ii) Invariant principle that incorporates adversarial data augmentation to obtain invariant representations. To the best of our knowledge, this is the first work that investigates the OOD generalization problem 
    
[^94]: Jina Embeddings:一种新颖的高性能句子嵌入模型

    Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])

    [http://arxiv.org/abs/2307.11224](http://arxiv.org/abs/2307.11224)

    Jina Embeddings是一组高性能的句子嵌入模型，能够捕捉文本的语义本质。该论文详细介绍了Jina Embeddings的开发过程，并通过性能评估验证了其优越性能。

    

    Jina Embeddings由一组高性能的句子嵌入模型组成，能够将各种文本输入转化为数值表示，从而捕捉文本的语义本质。虽然这些模型并非专门设计用于文本生成，但在密集检索和语义文本相似性等应用中表现出色。本文详细介绍了Jina Embeddings的开发过程，从创建高质量的成对和三元数据集开始。它强调了数据清理在数据集准备中的关键作用，并对模型训练过程进行了深入探讨，最后利用Massive Textual Embedding Benchmark（MTEB）进行了全面的性能评估。

    Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
    
[^95]: 云系统中的性能问题识别与关系-时间异常检测

    Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection. (arXiv:2307.10869v1 [cs.LG])

    [http://arxiv.org/abs/2307.10869](http://arxiv.org/abs/2307.10869)

    云系统中的性能问题识别存在挑战，现有方法中的独立分析每个指标的异常无法解决问题，需要考虑指标之间的关联性。

    

    性能问题在大规模云服务系统中普遍存在，可能导致巨额收入损失。为了确保可靠的性能，使用服务监控指标准确地识别和定位这些问题是至关重要的。鉴于现代云系统的复杂性和规模，这项任务可能会具有挑战性，并且可能需要超出个人能力的广泛专业知识和资源。一些现有的方法通过分析每个指标独立地检测异常来解决这个问题。然而，这可能会导致难以由工程师手动诊断的压倒性警报风暴。为了追求更好的性能，不仅应考虑指标的时间模式，还应考虑指标之间的关联性模式，这可以被形式化为多变量指标异常检测问题。然而，大多数研究在明确提取这两种类型的特征方面存在不足。此外，存在一些未标记的异常。

    Performance issues permeate large-scale cloud service systems, which can lead to huge revenue losses. To ensure reliable performance, it's essential to accurately identify and localize these issues using service monitoring metrics. Given the complexity and scale of modern cloud systems, this task can be challenging and may require extensive expertise and resources beyond the capacity of individual humans. Some existing methods tackle this problem by analyzing each metric independently to detect anomalies. However, this could incur overwhelming alert storms that are difficult for engineers to diagnose manually. To pursue better performance, not only the temporal patterns of metrics but also the correlation between metrics (i.e., relational patterns) should be considered, which can be formulated as a multivariate metrics anomaly detection problem. However, most of the studies fall short of extracting these two types of features explicitly. Moreover, there exist some unlabeled anomalies m
    
[^96]: 检测城市基础设施相互依赖网络中的脆弱节点

    Detecting Vulnerable Nodes in Urban Infrastructure Interdependent Network. (arXiv:2307.09866v1 [cs.LG])

    [http://arxiv.org/abs/2307.09866](http://arxiv.org/abs/2307.09866)

    该论文使用图神经网络和强化学习对城市基础设施相互依赖网络中的脆弱节点进行了准确建模和分析。

    

    理解和描述城市基础设施的脆弱性对我们具有重要价值，这些基础设施是城市正常运行所必需的工程设施，以网络的形式自然存在。潜在的应用包括保护脆弱设施和设计稳健的拓扑结构等。由于不同拓扑特性和基础设施脆弱性以及其复杂的演化机制之间的强关联，一些启发式分析和机器辅助分析在解决这种场景时存在局限性。在本文中，我们将相互依赖网络建模为异构图，并提出了一种基于图神经网络和强化学习的系统，可以在实际数据上进行训练，准确地描述城市系统的脆弱性。所提出的系统利用深度学习技术来理解和分析异构图，从而能够捕捉级联失败风险。

    Understanding and characterizing the vulnerability of urban infrastructures, which refers to the engineering facilities essential for the regular running of cities and that exist naturally in the form of networks, is of great value to us. Potential applications include protecting fragile facilities and designing robust topologies, etc. Due to the strong correlation between different topological characteristics and infrastructure vulnerability and their complicated evolution mechanisms, some heuristic and machine-assisted analysis fall short in addressing such a scenario. In this paper, we model the interdependent network as a heterogeneous graph and propose a system based on graph neural network with reinforcement learning, which can be trained on real-world data, to characterize the vulnerability of the city system accurately. The presented system leverages deep learning techniques to understand and analyze the heterogeneous graph, which enables us to capture the risk of cascade failu
    
[^97]: 基于超立方体的分类的端到端神经网络训练

    End-to-End Neural Network Training for Hyperbox-Based Classification. (arXiv:2307.09269v1 [cs.LG])

    [http://arxiv.org/abs/2307.09269](http://arxiv.org/abs/2307.09269)

    本论文提出了一种基于神经网络的端到端训练方法来处理基于超立方体的分类问题。与现有方法相比，该方法能够高效地训练超立方体模型，并大大减少训练时间，同时获得更优的分类结果。

    

    基于超立方体的分类被认为是一种有希望的技术，其中对数据的决策被表示为一系列正交的多维立方体（即超立方体），这些立方体通常具有可解释性和易读性。然而，现有方法已经不再能够有效地处理现今许多应用领域面临的不断增加的数据量。我们通过提出一种新颖的通过神经网络进行基于超立方体分类的完全可微分框架来填补这一差距。与以往的工作不同，我们的超立方体模型可以以端到端的方式高效训练，从而大大减少训练时间并获得更优的分类结果。

    Hyperbox-based classification has been seen as a promising technique in which decisions on the data are represented as a series of orthogonal, multidimensional boxes (i.e., hyperboxes) that are often interpretable and human-readable. However, existing methods are no longer capable of efficiently handling the increasing volume of data many application domains face nowadays. We address this gap by proposing a novel, fully differentiable framework for hyperbox-based classification via neural networks. In contrast to previous work, our hyperbox models can be efficiently trained in an end-to-end fashion, which leads to significantly reduced training times and superior classification results.
    
[^98]: ChatGPT的行为随时间变化如何？

    How is ChatGPT's behavior changing over time?. (arXiv:2307.09009v1 [cs.CL])

    [http://arxiv.org/abs/2307.09009](http://arxiv.org/abs/2307.09009)

    本论文评估了GPT-3.5和GPT-4模型在不同时间点上的性能和行为变化，发现它们的表现可以有很大的差异，包括在解决数学问题、回答敏感问题、生成代码和视觉推理等任务上。这些结果表明相同的语言模型服务的行为在相对短的时间内可以发生显著变化。

    

    GPT-3.5和GPT-4是两种广泛使用的大型语言模型（LLM）服务。然而，这些模型何时以及如何进行更新是不透明的。在这里，我们对GPT-3.5和GPT-4的2023年3月和2023年6月版本进行了评估，涉及四项不同的任务：1）解决数学问题，2）回答敏感/危险问题，3）生成代码和4）视觉推理。我们发现，GPT-3.5和GPT-4的性能和行为在时间上可以有很大的变化。例如，GPT-4（2023年3月）在识别质数方面表现非常出色（准确率为97.6%），但GPT-4（2023年6月）在相同的问题上表现非常差（准确率为2.4%）。有趣的是，GPT-3.5（2023年6月）在这个任务上比GPT-3.5（2023年3月）要好得多。GPT-4在6月份对回答敏感问题的意愿较3月份要低，而无论是GPT-4还是GPT-3.5在6月份的代码生成中都有更多的格式错误。总体而言，我们的发现表明相同LLM服务的行为在相对较短的时间内可以发生重大变化。

    GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relat
    
[^99]: DISPEL：通过领域特定解放进行域泛化

    DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])

    [http://arxiv.org/abs/2307.07181](http://arxiv.org/abs/2307.07181)

    DISPEL是一种通过后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征的领域泛化方法。

    

    领域泛化旨在通过仅在有限的源领域上进行训练，学习一个能够在未知测试领域上表现良好的泛化模型。然而，现有的领域泛化方法往往引入与预测无关的噪声或需要收集领域标签来解决。为了应对这些挑战，我们从不同的角度考虑了领域泛化问题，将底层特征组划分为领域共享和领域特定特征。然而，领域特定特征很难从输入数据中进行识别和区分。在这项工作中，我们提出了一种名为DISPEL（DomaIn-SPEcific Liberating）的后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征。具体而言，DISPEL利用一个掩蔽生成器为每个输入数据生成一个唯一的掩蔽来过滤领域特定特征。DISPEL框架非常灵活，可以应用于任何细粒度的掩蔽任务和领域泛化方法。

    Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fin
    
[^100]: CrunchGPT：基于ChatGPT的科学机器学习辅助框架

    CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])

    [http://arxiv.org/abs/2306.15551](http://arxiv.org/abs/2306.15551)

    CrunchGPT是一个基于ChatGPT的科学机器学习辅助框架，通过简单的用户提示来协调整个科学机器学习的工作流程，实现无缝集成数据和物理知识，解决了SciML在预处理、问题建模、代码生成、后处理和分析等方面的耗时问题，拓展了其工业应用和数字孪生框架的适用性。

    

    科学机器学习（SciML）近年来在计算科学和工程的许多不同领域取得了进展。其目标是在不需要复杂和计算密集的数据同化方案的情况下，无缝地将数据和物理知识集成起来。然而，预处理、问题建模、代码生成、后处理和分析仍然是耗时的，并且可能限制SciML在工业应用和数字孪生框架中的广泛适用性。在这里，我们将SciML的各个阶段整合到ChatGPT的伞下，形成CrunchGPT，它通过用户简单的提示来协调整个SciML的工作流程。具体而言，我们提供了两个示例，演示了CrunchGPT在气动学中优化机翼和在各种几何形状中获得流场的潜在用途，并强调了验证阶段。为了演示CrunchGPT的流程和

    Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still time consuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and
    
[^101]: ChiPFormer: 通过离线决策变换器实现可转移芯片布局

    ChiPFormer: Transferable Chip Placement via Offline Decision Transformer. (arXiv:2306.14744v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14744](http://arxiv.org/abs/2306.14744)

    ChiPFormer通过离线学习可转移的布局策略，显著提高了芯片布局的质量，并在减少布局时间的同时增强了对未知芯片电路的适应能力。

    

    在现代芯片设计中，布局是一个关键步骤，旨在确定芯片画布上电路模块的位置。最近的研究表明，强化学习可以提高芯片布局中的人类性能。然而，这种基于强化学习的方法在训练时间长且在未知的芯片电路中具有较低的转移能力。为了解决这些挑战，我们将芯片布局问题作为一个离线强化学习问题，并提出了ChiPFormer，它可以通过固定的离线数据学习到可转移的布局策略。ChiPFormer具有一些先前的研究所没有的优势。首先，ChiPFormer能够利用离线布局设计在多任务设置中更有效地学习可转移的策略。其次，ChiPFormer能够促进对未知芯片电路的有效微调，将布局运行时间从几小时缩短到几分钟。第三，对32个芯片电路的大量实验证明，ChiPFormer在减少布局时间的同时实现了显著提高的布局质量。

    Placement is a critical step in modern chip design, aiming to determine the positions of circuit modules on the chip canvas. Recent works have shown that reinforcement learning (RL) can improve human performance in chip placement. However, such an RL-based approach suffers from long training time and low transfer ability in unseen chip circuits. To resolve these challenges, we cast the chip placement as an offline RL formulation and present ChiPFormer that enables learning a transferable placement policy from fixed offline data. ChiPFormer has several advantages that prior arts do not have. First, ChiPFormer can exploit offline placement designs to learn transferable policies more efficiently in a multi-task setting. Second, ChiPFormer can promote effective finetuning for unseen chip circuits, reducing the placement runtime from hours to minutes. Third, extensive experiments on 32 chip circuits demonstrate that ChiPFormer achieves significantly better placement quality while reducing t
    
[^102]: CARL-G: 基于聚类加速的图表征学习

    CARL-G: Clustering-Accelerated Representation Learning on Graphs. (arXiv:2306.06936v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06936](http://arxiv.org/abs/2306.06936)

    CARL-G是一个基于聚类的图表征学习框架，通过使用受集群验证启发的损失函数，克服了负采样和复杂架构的问题。

    

    自我监督学习在图上取得了巨大的进展，并在各种下游任务中实现了出色的性能。然而，许多最先进的方法都存在一些障碍，阻碍了它们充分发挥潜力。例如，对比方法通常需要负采样，这在计算上往往是昂贵的。虽然非对比方法避免了这一昂贵的步骤，但大多数现有方法要么依赖过于复杂的架构，要么依赖于特定数据集的增强。在本文中，我们提出了一个问题：我们是否可以借鉴经典的无监督机器学习文献，以克服这些障碍？在我们的关键洞察的指导下，距离聚类的目标与对比学习的目标非常相似：都试图将相似项的表示拉在一起，并将不相似项分开。因此，我们提出了CARL-G - 一个新颖的基于聚类的图表征学习框架，它使用了一个受集群验证启发的损失函数。

    Self-supervised learning on graphs has made large strides in achieving great performance in various downstream tasks. However, many state-of-the-art methods suffer from a number of impediments, which prevent them from realizing their full potential. For instance, contrastive methods typically require negative sampling, which is often computationally costly. While non-contrastive methods avoid this expensive step, most existing methods either rely on overly complex architectures or dataset-specific augmentations. In this paper, we ask: Can we borrow from classical unsupervised machine learning literature in order to overcome those obstacles? Guided by our key insight that the goal of distance-based clustering closely resembles that of contrastive learning: both attempt to pull representations of similar items together and dissimilar items apart. As a result, we propose CARL-G - a novel clustering-based framework for graph representation learning that uses a loss inspired by Cluster Vali
    
[^103]: 黑盒变分推断的收敛性保证

    Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v1 [cs.LG])

    [http://arxiv.org/abs/2306.03638](http://arxiv.org/abs/2306.03638)

    本文提出了一种基于密集高斯变分族的梯度估计器，在此基础上使用近端和投影随机梯度下降，提供了黑盒变分推断收敛于逼真推断问题的第一个严格保证。

    

    尽管黑盒变分推断被广泛应用，但没有证明其随机优化成功的证明。我们提出这是现有随机优化证明中的理论差距，即具有异常噪声边界和复合非平滑目标的梯度估计器的挑战。对于密集的高斯变分族，我们观察到现有的基于再参数化的梯度估计器满足二次噪声界，并为使用该界限的近端和投影随机梯度下降提供新的收敛保证。这提供了第一个黑盒变分推断收敛于逼真推断问题的严格保证。

    While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.
    
[^104]: W-procer: 基于加权原型对比学习的医学少样本命名实体识别

    W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])

    [http://arxiv.org/abs/2305.18624](http://arxiv.org/abs/2305.18624)

    W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。

    

    对比学习已成为少样本命名实体识别（NER）的一种受欢迎的解决方案。传统配置力求减少具有相同标签的标记之间的距离，并增加具有不同标签的标记之间的距离。然而，在医学领域中存在大量被注释为“O”（即“OUTSIDE”）的实体，并且它们不希望被推离到当前对比学习方法标记为“O”以外的其他实体，这种设定效果不佳，可能会得出含有噪声原型标签的语义表示，尽管存在许多“O”标签实体与有标签实体相关。为解决这个挑战，我们提出了一种名为医学少样本命名实体识别中基于加权原型的对比学习方法（W-PROCER）。我们的方法主要围绕构建基于原型的对比损失和加权网络展开。这些组件在协助在医学领域中的迁移学习方面发挥了至关重要的作用。在实验中，我们将W-PROCER应用于一个公共的医学数据集，并展示了其相对于现有的最先进方法的优异表现。

    Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
    
[^105]: 微小子空间中发生微调: 探索预训练语言模型的内在任务特定子空间

    Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models. (arXiv:2305.17446v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17446](http://arxiv.org/abs/2305.17446)

    该论文通过发现预训练语言模型的内在任务特定子空间，提出了一种重新参数化和微调模型的新方法。研究发现在该子空间中，只需少量自由参数即可有效微调模型，并且某些维度对于引入任务特定知识至关重要。

    

    已知预训练语言模型（PLMs）过度参数化并具有显著的冗余，表明PLMs的自由度较小。本文从新的角度研究了重新参数化和微调PLMs的问题：发现内在的任务特定子空间。具体地，通过利用给定任务的微调过程的动态，学习了参数优化轨迹以揭示其内在的任务特定子空间。一个关键发现是，在子空间中，PLMs可以通过少量的自由参数进行有效的微调。此外，我们观察到在子空间的微调过程中出现了一些异常维度。禁用这些维度会严重降低模型性能。这表明这些维度对于引入任务特定知识到下游任务是至关重要的。

    Pre-trained language models (PLMs) are known to be overly parameterized and have significant redundancy, indicating a small degree of freedom of the PLMs. Motivated by the observation, in this paper, we study the problem of re-parameterizing and fine-tuning PLMs from a new perspective: Discovery of intrinsic task-specific subspace. Specifically, by exploiting the dynamics of the fine-tuning process for a given task, the parameter optimization trajectory is learned to uncover its intrinsic task-specific subspace. A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters. Beyond, we observe some outlier dimensions emerging during fine-tuning in the subspace. Disabling these dimensions degrades the model performance significantly. This suggests that these dimensions are crucial to induce task-specific knowledge to downstream tasks.
    
[^106]: 在随机博弈中使用奖励机制的强化学习

    Reinforcement Learning With Reward Machines in Stochastic Games. (arXiv:2305.17372v2 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2305.17372](http://arxiv.org/abs/2305.17372)

    该论文研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习，提出了一种基于奖励机制的算法，在纳什均衡下学习每个智能体的最佳应答策略，并证明了学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。

    

    我们研究了复杂任务中具有非马尔可夫回报函数的随机博弈的多智能体强化学习。我们利用奖励机制来整合高层次的复杂任务知识。我们开发了一种称为QRM-SG的算法，用于学习每个智能体在纳什均衡下的最佳应答策略。在QRM-SG中，我们在增广状态空间中定义了纳什均衡下的Q函数。增广状态空间整合了随机博弈的状态和奖励机制的状态。每个智能体学习了系统中所有智能体的Q函数。我们证明了在QRM-SG中学习的Q函数将会收敛于一个纳什均衡的Q函数，前提是在学习过程中每个时间步的阶段博弈具有全局最优点或者鞍点，并且智能体基于这一点进行最佳应答策略的Q函数更新。我们使用Lemke-Howson方法来得出给定当前Q函数时的最佳应答策略。

    We investigate multi-agent reinforcement learning for stochastic games with complex tasks, where the reward functions are non-Markovian. We utilize reward machines to incorporate high-level knowledge of complex tasks. We develop an algorithm called Q-learning with reward machines for stochastic games (QRM-SG), to learn the best-response strategy at Nash equilibrium for each agent. In QRM-SG, we define the Q-function at a Nash equilibrium in augmented state space. The augmented state space integrates the state of the stochastic game and the state of reward machines. Each agent learns the Q-functions of all agents in the system. We prove that Q-functions learned in QRM-SG converge to the Q-functions at a Nash equilibrium if the stage game at each time step during learning has a global optimum point or a saddle point, and the agents update Q-functions based on the best-response strategy at this point. We use the Lemke-Howson method to derive the best-response strategy given current Q-func
    
[^107]: Diable: 在表格上进行的高效对话状态跟踪

    Diable: Efficient Dialogue State Tracking as Operations on Tables. (arXiv:2305.17020v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.17020](http://arxiv.org/abs/2305.17020)

    Diable是一个高效的对话状态跟踪系统，它通过在表格上进行操作来更新对话状态，相比现有方法时间效率提高了2.4倍，同时保持了竞争性的目标准确性。

    

    目前的对话状态跟踪系统将完整的对话历史作为输入，将当前状态表示为包含所有槽的列表，并在每个对话回合中从头开始生成整个状态。这种方法效率低下，特别是当槽的数量很多且对话很长时。我们提出了Diable，一种新的任务形式化方法，简化了高效对话状态跟踪系统的设计和实现，并且可以轻松地嵌入大型语言模型。我们将对话状态表示为表格，并将对话状态跟踪形式化为表格操作任务。在每个回合中，系统通过基于对话上下文生成表格操作来更新先前的状态。在MultiWoz数据集上进行了大量实验，结果显示，Diable (i) 优于强大的高效对话状态跟踪基准，(ii) 时间效率比当前最先进的方法提高了2.4倍，同时保持竞争性的联合目标准确性，并且(iii) 对无噪声的输入具有鲁棒性。

    Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to no
    
[^108]: 基于多模型生成对抗网络的随机动力学学习和精确生成

    Learning and accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks. (arXiv:2305.15920v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2305.15920](http://arxiv.org/abs/2305.15920)

    本文使用GAN来学习一个原型晶格上的随机过程，并提出一种合适的多模型程序，可以显著提高精度。GAN似乎是处理复杂统计动力学问题的有前途的工具。

    

    生成对抗网络（GAN）已经在远离物理领域，如文本和图像生成方面展示出了巨大的潜力。本文使用GAN来学习一个原型晶格上的随机过程。通过合理地向原始数据添加噪声，我们成功地将生成器和鉴别器损失函数的值带到了它们的理想值附近。然而，像对抗性方法一样，震荡仍然存在。这会破坏模型选择和生成轨迹的质量。我们展示了，一种合适的多模型程序，在每一步随机选择生成器推进随机轨迹，可以显著提高精度。基于以上发现，GAN似乎是处理复杂统计动力学问题的有前途的工具。

    Generative Adversarial Networks (GANs) have shown immense potential in fields far from physics, such as in text and image generation. Here we use GANs to learn a prototypical stochastic process on a lattice. By suitably adding noise to the original data we succeed in bringing both the Generator and the Discriminator loss functions close to their ideal value. However, as typical for adversarial approaches, oscillations persist. This undermines model selection and the quality of the generated trajectory. We demonstrate that a suitable multi-model procedure where stochastic trajectories are advanced at each step upon randomly selecting a Generator leads to a remarkable increase in accuracy. Based on the reported findings GANs appears as a promising tool to tackle complex statistical dynamics.
    
[^109]: AlpacaFarm: 一种从人类反馈中学习的方法模拟框架

    AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v1 [cs.LG])

    [http://arxiv.org/abs/2305.14387](http://arxiv.org/abs/2305.14387)

    该论文提出了一种名为AlpacaFarm的低成本模拟器，该模拟器为从人类反馈中学习的研究和开发提供了一种解决方案，通过设计LLM提示来模拟人类反馈，提出自动评估并提供参考实现，克服了数据收集的高昂成本、缺乏可信的评估和缺乏参考方法实现的挑战。

    

    大型语言模型（LLMs）如ChatGPT因其良好的指令跟随能力而得到了广泛应用。开发这些LLMs需要使用人类反馈进行训练的复杂且尚不明确的工作流程。将此指令跟随过程复制和理解面临三大挑战： 数据收集的高昂成本，缺乏可信的评估和缺乏参考方法实现。我们通过AlpacaFarm解决了这些挑战，这是一个低成本的模拟器，可用于从反馈中学习的研究和开发。第一，我们设计了LLM提示来模拟人类反馈，其成本比众包工作者便宜45倍，并且与人类反馈具有高度一致性。第二，我们提出了一种自动评估方法，并将其与真实世界交互中获得的人类指令进行验证。第三，我们为几种从配对反馈中学习的方法（PPO，best-of-n，expert iteration等）提供了参考实现。

    Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well. Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these challenges with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM prompts to simulate human feedback that are 45x cheaper than crowdworkers and display high agreement with humans. Second, we propose an automatic evaluation and validate it against human instructions obtained on real-world interactions. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, and more) that learn from pairwise feedback
    
[^110]: 深度学习中的GELU激活函数：全面的数学分析和性能评估

    GELU Activation Function in Deep Learning: A Comprehensive Mathematical Analysis and Performance. (arXiv:2305.12073v1 [cs.LG])

    [http://arxiv.org/abs/2305.12073](http://arxiv.org/abs/2305.12073)

    本文对GELU激活函数进行了全面的数学分析和广泛的实验比较，证明了它在深度学习模型中具有优越的性能和适用性。

    

    在深度学习模型中，选择最合适的激活函数是影响其学习能力、稳定性和计算效率的关键因素。近年来，高斯误差线性单元（GELU）激活函数已经成为一种主流方法，在各种应用中超越了传统的激活函数，如修正线性单元（ReLU）。本文对GELU激活函数进行了严格的数学分析，详细探讨了其可微性、有界性、平稳性和光滑性等性质。此外，我们对GELU函数进行了广泛的实验比较，利用在CIFAR-10、CIFAR-100和STL-10数据集上训练的残差卷积网络作为实证测试基础。我们的结果证明了GELU相对于其他激活函数的卓越性能，确立了它在广泛的深度学习模型中的适用性。

    Selecting the most suitable activation function is a critical factor in the effectiveness of deep learning models, as it influences their learning capacity, stability, and computational efficiency. In recent years, the Gaussian Error Linear Unit (GELU) activation function has emerged as a dominant method, surpassing traditional functions such as the Rectified Linear Unit (ReLU) in various applications. This study presents a rigorous mathematical investigation of the GELU activation function, exploring its differentiability, boundedness, stationarity, and smoothness properties in detail. Additionally, we conduct an extensive experimental comparison of the GELU function against a broad range of alternative activation functions, utilizing a residual convolutional network trained on the CIFAR-10, CIFAR-100, and STL-10 datasets as the empirical testbed. Our results demonstrate the superior performance of GELU compared to other activation functions, establishing its suitability for a wide ra
    
[^111]: 连续多模态知识图谱构建

    Continual Multimodal Knowledge Graph Construction. (arXiv:2305.08698v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2305.08698](http://arxiv.org/abs/2305.08698)

    连续多模态知识图谱构建面临着灾难性遗忘的挑战，需要解决新增实体和关系以及多模态源数据变化的问题。

    

    多模态知识图谱构建（MKGC）涉及使用多种形式的数据，如文本和图像，创建实体和关系的结构化表示。然而，现有的MKGC模型在处理动态现实场景中新增实体和关系方面面临挑战。目前的连续知识图谱构建设置主要关注从文本数据中提取实体和关系，忽视了其他多模态源。因此，有必要探索连续MKGC的挑战，以解决灾难性遗忘现象，并确保保留从不同形式的数据中提取的过去知识。本研究致力于通过开发终身MKGC基准数据集来研究这个复杂的主题。基于经验发现，当多媒体数据训练时，一些典型的MKGC模型可能在连续设置中意外表现不佳，与那些仅利用文本资源的模型相比。我们以实验证据为基础，总结出以下论点：连续多模态知识图谱构建面临着数据源变化导致的灾难性遗忘问题。

    Multimodal Knowledge Graph Construction (MKGC) involves creating structured representations of entities and relations using multiple modalities, such as text and images. However, existing MKGC models face challenges in handling the addition of new entities and relations in dynamic real-world scenarios. The current continual setting for knowledge graph construction mainly focuses on entity and relation extraction from text data, overlooking other multimodal sources. Therefore, there arises the need to explore the challenge of continual MKGC to address the phenomenon of catastrophic forgetting and ensure the retention of past knowledge extracted from different forms of data. This research focuses on investigating this complex topic by developing lifelong MKGC benchmark datasets. Based on the empirical findings that several typical MKGC models, when trained on multimedia data, might unexpectedly underperform compared to those solely utilizing textual resources in a continual setting, we p
    
[^112]: 因果信息分离：为抗分布转移设计代理特征

    Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])

    [http://arxiv.org/abs/2305.05832](http://arxiv.org/abs/2305.05832)

    本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。

    

    统计预测模型通常是在与最终使用情况不同的概率分布中进行训练的。为了预测分布转移，有一种方法是利用因果机制在不同环境下保持不变的直觉来主动准备。本文针对一个具有挑战性的场景，其中目标的因果和反因果变量都是未被观察到的。利用信息论，我们为下游观测变量开发了特征选择和工程技术，这些变量充当代理。我们选择有助于建立稳定模型的代理，并使用辅助训练任务从代理中提取增强稳定性的信息。我们在合成数据和真实数据上展示了我们技术的有效性。

    Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
    
[^113]: FedNoRo: 针对类别不平衡和标签噪声异质性的噪声-鲁棒联邦学习

    FedNoRo: Towards Noise-Robust Federated Learning by Addressing Class Imbalance and Label Noise Heterogeneity. (arXiv:2305.05230v1 [cs.LG])

    [http://arxiv.org/abs/2305.05230](http://arxiv.org/abs/2305.05230)

    本文提出了一个名为 FedNoRo 的两阶段框架，用于解决类别不平衡和标签噪声异质性的联邦学习问题，并在 ICH 和 ISIC2019 数据集上取得了更好的表现。

    

    联邦噪声标签学习(FNLL)正在成为一种有前途的隐私保护的多源分散学习工具。现有研究基于全局数据类别平衡的假设，可能无法建模复杂的标签噪声，特别是在医学场景中。本文首先提出了一个更为真实的联邦标签噪声问题，其中全局数据是类别不平衡的，并且标签噪声是异质的，然后提出了一个名为 FedNoRo 的两阶段框架，用于噪声-鲁棒联邦学习。具体而言，在 FedNoRo 的第一阶段，采用每类损失指标之后跟随高斯混合模型进行嘈杂客户端识别。在第二阶段，同时采用知识蒸馏和距离感知聚合函数进行噪声-鲁棒联邦模型更新。对广泛使用的 ICH 和 ISIC2019 数据集的实验结果表明，FedNoRo 相对于最先进的 FNLL 方法在解决联邦学习中的类别不平衡和标签噪声异质性方面具有卓越的性能。

    Federated noisy label learning (FNLL) is emerging as a promising tool for privacy-preserving multi-source decentralized learning. Existing research, relying on the assumption of class-balanced global data, might be incapable to model complicated label noise, especially in medical scenarios. In this paper, we first formulate a new and more realistic federated label noise problem where global data is class-imbalanced and label noise is heterogeneous, and then propose a two-stage framework named FedNoRo for noise-robust federated learning. Specifically, in the first stage of FedNoRo, per-class loss indicators followed by Gaussian Mixture Model are deployed for noisy client identification. In the second stage, knowledge distillation and a distance-aware aggregation function are jointly adopted for noise-robust federated model updating. Experimental results on the widely-used ICH and ISIC2019 datasets demonstrate the superiority of FedNoRo against the state-of-the-art FNLL methods for addre
    
[^114]: 无监督改进音频-文本跨模态表征

    Unsupervised Improvement of Audio-Text Cross-Modal Representations. (arXiv:2305.01864v1 [cs.SD])

    [http://arxiv.org/abs/2305.01864](http://arxiv.org/abs/2305.01864)

    本研究探索了无监督的方法来改进跨模态音频-文本表征学习，通过使用领域特定的筛选和软标注对比性损失，成功提高了零-shot分类性能。

    

    最近通过使用语言模型来获得跨模态音频-文本表征取得了进展，克服了使用预定义标签的传统训练方法的局限性。这使得社区能够在零-shot分类等任务上取得进展，否则是不可能的。然而，学习这样的表征需要大量的人工注释的音频-文本对。本文研究了使用未配对文本和音频改进这些表征学习框架的无监督方法。我们探索了领域非特定和领域特定的筛选方法，创建我们用于进一步改进模型的音频-文本对。我们还表明，当与软标注对比性损失结合使用领域特定筛选时，我们能够在下游声音事件分类或声学场景分类任务的零-shot分类性能方面取得显着的改进。

    Recent advances in using language models to obtain cross-modal audio-text representations have overcome the limitations of conventional training approaches that use predefined labels. This has allowed the community to make progress in tasks like zero-shot classification, which would otherwise not be possible. However, learning such representations requires a large amount of human-annotated audio-text pairs. In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio. We explore domain-unspecific and domain-specific curation methods to create audio-text pairs that we use to further improve the model. We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks.
    
[^115]: （本地）差分隐私对公平性没有带来不平等影响

    (Local) Differential Privacy has NO Disparate Impact on Fairness. (arXiv:2304.12845v1 [cs.LG])

    [http://arxiv.org/abs/2304.12845](http://arxiv.org/abs/2304.12845)

    本文研究了在 LDP 下收集多个敏感属性对公平性的影响并提出了考虑域大小的新的隐私预算分配方案，实验表明该方案在隐私、效用和公平性方面均优于最新的解决方案，LDP 带来了略微改善的公平性而不会明显影响性能。

    

    近年来，本地差分隐私（LDP）作为强大隐私保护方法，在实际应用中得到了广泛的应用。通过 LDP，用户可以在将数据传输出去前在设备上对其进行扰动。然而，随着在各个行业中收集多个敏感信息的情况越来越普遍，仅收集单个敏感属性可能已经不足以保障用户的隐私。数据中的相关属性仍然可能导致对敏感属性的推断。本文通过实验证明了在 LDP 下收集多个敏感属性对公平性的影响。我们提出了一个新的隐私预算分配方案，考虑到敏感属性的不同域大小。在我们的实验中，这通常比现有最新解决方案的隐私-效用-公平性权衡方式更好。我们的结果表明，LDP 在学习问题中带来了略微改善的公平性，而不会明显影响性能。

    In recent years, Local Differential Privacy (LDP), a robust privacy-preserving methodology, has gained widespread adoption in real-world applications. With LDP, users can perturb their data on their devices before sending it out for analysis. However, as the collection of multiple sensitive information becomes more prevalent across various industries, collecting a single sensitive attribute under LDP may not be sufficient. Correlated attributes in the data may still lead to inferences about the sensitive attribute. This paper empirically studies the impact of collecting multiple sensitive attributes under LDP on fairness. We propose a novel privacy budget allocation scheme that considers the varying domain size of sensitive attributes. This generally led to a better privacy-utility-fairness trade-off in our experiments than the state-of-art solution. Our results show that LDP leads to slightly improved fairness in learning problems without significantly affecting the performance of the
    
[^116]: 低秩张量空间中的超拉普拉斯正则化概念因式分解多视角聚类方法

    Hyper-Laplacian Regularized Concept Factorization in Low-rank Tensor Space for Multi-view Clustering. (arXiv:2304.11435v1 [cs.LG])

    [http://arxiv.org/abs/2304.11435](http://arxiv.org/abs/2304.11435)

    本文提出了一种采用超拉普拉斯正则化概念因式分解的多视角聚类方法，在低秩张量空间中对每个视角进行聚类并提取非线性局部结构。

    

    基于张量的多视角子空间聚类方法已经取得了重要进展，能够评估高阶相关性并提高多视角数据的聚类分析。然而，现有研究大多数存在两个缺陷。首先，基于自表示的张量子空间学习通常引起高时间和空间复杂度，并且局限于感知嵌入空间中的非线性局部结构。其次，张量奇异值分解（t-SVD）模型将每个奇异值等分地重新分布，而无法考虑它们之间的不同重要性。为了解决这些问题，我们提出了一种低秩张量空间中的超拉普拉斯正则化概念因式分解（HLRCF）多视角聚类方法。具体地，我们将概念因式分解应用于探索每个视角的潜在聚类表示。进一步地，超图拉普拉斯正则化赋予了该模型提取非线性局部结构的能力。

    Tensor-oriented multi-view subspace clustering has achieved significant strides in assessing high-order correlations and improving clustering analysis of multi-view data. Nevertheless, most of existing investigations are typically hampered by the two flaws. First, self-representation based tensor subspace learning usually induces high time and space complexity, and is limited in perceiving nonlinear local structure in the embedding space. Second, the tensor singular value decomposition (t-SVD) model redistributes each singular value equally without considering the diverse importance among them. To well cope with the issues, we propose a hyper-Laplacian regularized concept factorization (HLRCF) in low-rank tensor space for multi-view clustering. Specifically, we adopt the concept factorization to explore the latent cluster-wise representation of each view. Further, the hypergraph Laplacian regularization endows the model with the capability of extracting the nonlinear local structures i
    
[^117]: 核插值的泛化能力较弱

    Kernel interpolation generalizes poorly. (arXiv:2303.15809v1 [cs.LG])

    [http://arxiv.org/abs/2303.15809](http://arxiv.org/abs/2303.15809)

    本文证明了核插值的泛化误差有一个下界，在较大范围的核函数中泛化能力较差，使得过拟合的宽神经网络泛化性能差。

    

    在核回归研究的复兴中，一个最有趣的问题可能是核插值是否具有较好的泛化能力，因为这可能有助于我们理解深度网络领域中的“良性过拟合现象”。在本文中，在温和的条件下，我们证明了对于任何$\varepsilon>0$，核插值的泛化误差都有一个下界$\Omega(n^{-\varepsilon})$。换句话说，核插值在较大范围的核函数中泛化能力较差。作为一个直接的推论，我们可以证明在球上定义的过拟合宽神经网络的泛化能力也较差。

    One of the most interesting problems in the recent renaissance of the studies in kernel regression might be whether the kernel interpolation can generalize well, since it may help us understand the `benign overfitting henomenon' reported in the literature on deep networks. In this paper, under mild conditions, we show that for any $\varepsilon>0$, the generalization error of kernel interpolation is lower bounded by $\Omega(n^{-\varepsilon})$. In other words, the kernel interpolation generalizes poorly for a large class of kernels. As a direct corollary, we can show that overfitted wide neural networks defined on sphere generalize poorly.
    
[^118]: FedGH:异构联邦学习与广义全局头

    FedGH: Heterogeneous Federated Learning with Generalized Global Header. (arXiv:2303.13137v1 [cs.LG])

    [http://arxiv.org/abs/2303.13137](http://arxiv.org/abs/2303.13137)

    FedGH是一种异构联邦学习方法，可以使客户端持有具有不同结构的模型，通过训练共享的广义全局预测头来提高效率和性能。

    

    联邦学习(Federated learning, FL)是一种新兴的机器学习范式，允许多个参与方在隐私保护的情况下协作训练共享模型。现有横向FL方法通常假定FL服务器和客户端持有相同的模型结构。然而，由于系统异构和个性化需求，使得允许客户端持有具有不同结构的模型已成为一个重要的方向。现有的模型异构FL方法通常需要公开可用的数据集，并产生高通信和/或计算成本，这限制了它们的性能。为解决这些限制，我们提出了联邦全局预测头(FedGH)方法。它是一种通信和计算效率高的模型异构FL框架，通过在FL服务器上对客户端模型提取的表示进行训练来训练共享的广义全局预测头。通过FedGH训练的广义全局预测头可以直接部署在客户端设备上，以实现高效的本地推理。我们在基准数据集和模型上的实验表明，FedGH在准确性、通信效率和模型个性化能力方面优于现有的模型异构FL方法。

    Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose the Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header lear
    
[^119]: SemEval-2023任务3上的mCPT：用于零样本和少样本框架检测的多语言标签感知对比预训练变压器

    mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])

    [http://arxiv.org/abs/2303.09901](http://arxiv.org/abs/2303.09901)

    本研究提出了mCPT模型用于多语言的、多标签的零样本或少样本的框架检测任务，并在西班牙语和其他8种语言中取得了良好的成绩。该方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。

    

    本文介绍了零样本的西班牙语框架检测任务的获胜系统，并在另外八种语言中取得了良好的成绩。框架检测任务的挑战在于在只有少量或零个样本的情况下识别一组14个框架，即多语言多标签的少样本和零样本设置。我们开发的解决方案采用了基于多语言变压器的预训练程序，使用标签感知对比损失函数。除了描述系统外，我们还进行了嵌入空间分析和消融研究，以展示我们的预训练程序如何支持框架检测以推进计算框架分析。

    This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
    
[^120]: 基于稀疏子流形卷积神经网络的中微子望远镜触发事件重构

    Trigger-Level Event Reconstruction for Neutrino Telescopes Using Sparse Submanifold Convolutional Neural Networks. (arXiv:2303.08812v1 [hep-ex])

    [http://arxiv.org/abs/2303.08812](http://arxiv.org/abs/2303.08812)

    SSCNN是一种用于解决中微子望远镜数据处理中稀疏、高维度和非规则几何形状等问题的网络，其事件重构性能优于传统和机器学习算法，运行速度大大提高，可用于改善中微子能量和方向的第一估计以播种更先进的重建。

    

    卷积神经网络（CNN）在科学数据分析中得到了广泛应用，包括在中微子望远镜中。然而，这些实验的数据对CNN提出了许多挑战，如非规则几何形状、稀疏性和高维度。因此，CNN在中微子望远镜数据上非常低效，并需要大量的预处理，导致信息损失。我们提出了稀疏子流形卷积（SSCNN）作为解决这些问题的方法，并显示SSCNN事件重构性能与传统和机器学习算法相比可比或更好。此外，我们的SSCNN在GPU上的运行速度约为传统CNN的16倍。由于这种加速，预计能够处理IceCube规模的中微子望远镜的触发级事件速率。这些网络可用于改善中微子能量和方向的第一估计以播种更先进的重建。

    Convolutional neural networks (CNNs) have seen extensive applications in scientific data analysis, including in neutrino telescopes. However, the data from these experiments present numerous challenges to CNNs, such as non-regular geometry, sparsity, and high dimensionality. Consequently, CNNs are highly inefficient on neutrino telescope data, and require significant pre-processing that results in information loss. We propose sparse submanifold convolutions (SSCNNs) as a solution to these issues and show that the SSCNN event reconstruction performance is comparable to or better than traditional and machine learning algorithms. Additionally, our SSCNN runs approximately 16 times faster than a traditional CNN on a GPU. As a result of this speedup, it is expected to be capable of handling the trigger-level event rate of IceCube-scale neutrino telescopes. These networks could be used to improve the first estimation of the neutrino energy and direction to seed more advanced reconstructions,
    
[^121]: 非对数凹采样和对数分区估计的收敛速率

    Convergence Rates for Non-Log-Concave Sampling and Log-Partition Estimation. (arXiv:2303.03237v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.03237](http://arxiv.org/abs/2303.03237)

    非对数凹势V的高维采样速率可以在一些条件下实现与凸函数相同的收敛速率。

    

    从吉布斯分布$p(x)\propto\exp(-V(x)/\epsilon)$中采样并计算其对数分区函数是统计学、机器学习和统计物理中的基本任务。然而，虽然有效的算法已知于凸势函数$V$，但非凸情况下的情况要困难得多，算法必然在最坏情况下受到维度灾难的困扰。最近，已经证明在适当的条件下，高维采样非对数凹势V的速率也可以达到同样快的速度。本文对这些结果进行了回顾，并强调了领域中的一些开放问题。

    Sampling from Gibbs distributions $p(x) \propto \exp(-V(x)/\varepsilon)$ and computing their log-partition function are fundamental tasks in statistics, machine learning, and statistical physics. However, while efficient algorithms are known for convex potentials $V$, the situation is much more difficult in the non-convex case, where algorithms necessarily suffer from the curse of dimensionality in the worst case. For optimization, which can be seen as a low-temperature limit of sampling, it is known that smooth functions $V$ allow faster convergence rates. Specifically, for $m$-times differentiable functions in $d$ dimensions, the optimal rate for algorithms with $n$ function evaluations is known to be $O(n^{-m/d})$, where the constant can potentially depend on $m, d$ and the function to be optimized. Hence, the curse of dimensionality can be alleviated for smooth functions at least in terms of the convergence rate. Recently, it has been shown that similarly fast rates can also be ach
    
[^122]: 以ELBOs的加权积分理解扩散目标

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    本文深入理解了扩散目标，并揭示了加权损失和ELBO目标之间的直接关系。

    

    文献中的扩散模型采用不同的目标进行优化，并且这些目标都是加权损失的特例，其中加权函数指定每个噪声级别的权重。均匀加权对应于最大似然的原则性近似ELBO的最大化。但是实际上，由于更好的样本质量，目前的扩散模型使用非均匀加权。本文揭示了加权损失（带有任何加权）和ELBO目标之间的直接关系。我们展示了加权损失可以被写成一种ELBOs的加权积分形式，其中每个噪声级别都有一个ELBO。如果权重函数是单调的，那么加权损失是一种基于似然的目标：它在简单的数据增强下（即高斯噪声扰动）下最大化ELBO。我们的主要贡献是更深入地理解了扩散目标，但我们还进行了一些比较单调和非单调权重的实验。

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^123]: 如何用差分隐私实现机器学习：机器学习与差分隐私实用指南

    How to DP-fy ML: A Practical Guide to Machine Learning with Differential Privacy. (arXiv:2303.00654v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00654](http://arxiv.org/abs/2303.00654)

    这篇论文提供了关于如何将差分隐私应用于复杂机器学习模型的实用指南，填补了现有实践中的空白，为实现机器学习与差分隐私的结合提供了实际指导。

    

    机器学习模型在现实世界应用广泛，并且是研究的重点。与此同时，社区开始意识到保护机器学习训练数据的隐私的重要性。差分隐私已经成为对数据匿名化做出正式陈述的黄金标准。然而，尽管在工业界已经有一些应用差分隐私的尝试，但将差分隐私应用于现实世界中的复杂机器学习模型仍然很少。差分隐私的应用受限于缺乏实际指导，不清楚需要什么样的隐私保证，并且在机器学习模型的隐私保护、效用和计算之间存在良好的平衡。调整和优化性能的技巧散布在论文中或者存在于从业者的头脑中。此外，文献似乎对于如何以及是否应用架构调整以及哪些组件在应用差分隐私时是“安全”的问题存在着相互矛盾的证据。本工作是一份自包含的指南，旨在填补这些空白并提供实际指导，帮助实现机器学习与差分隐私的结合。

    ML models are ubiquitous in real world applications and are a constant focus of research. At the same time, the community has started to realize the importance of protecting the privacy of ML training data.  Differential Privacy (DP) has become a gold standard for making formal statements about data anonymization. However, while some adoption of DP has happened in industry, attempts to apply DP to real world complex ML models are still few and far between. The adoption of DP is hindered by limited practical guidance of what DP protection entails, what privacy guarantees to aim for, and the difficulty of achieving good privacy-utility-computation trade-offs for ML models. Tricks for tuning and maximizing performance are scattered among papers or stored in the heads of practitioners. Furthermore, the literature seems to present conflicting evidence on how and whether to apply architectural adjustments and which components are "safe" to use with DP.  This work is a self-contained guide th
    
[^124]: Nystr\"om $M$-Hilbert-Schmidt独立准则

    Nystr\"om $M$-Hilbert-Schmidt Independence Criterion. (arXiv:2302.09930v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09930](http://arxiv.org/abs/2302.09930)

    这项研究提出了Nystr\"om $M$-Hilbert-Schmidt独立准则，针对大规模应用的二次计算瓶颈问题进行了解决，并兼顾了多个随机变量的推广情况和理论保证。

    

    核技术是数据科学中最受欢迎和强大的方法之一。核的广泛应用的关键特性包括：(i) 它们针对的领域数量多，(ii) 与核相关的函数类具有Hilbert结构，便于统计分析，以及(iii) 它们能够以不丢失信息的方式表示概率分布。这些特性导致了Hilbert-Schmidt独立准则(HSIC)的巨大成功，该准则能够在温和条件下捕捉随机变量的联合独立性，并允许具有二次计算复杂性的闭式估计器(相对于样本大小)。为了解决大规模应用中的二次计算瓶颈问题，已经提出了多个HSIC近似估计器，然而这些估计器限制于$M=2$个随机变量，不能自然地推广到$M \geq 2$的情况，并且缺乏理论保证。在这项工作中，我们提出了一个Nystr\"om $M$-Hilbert-Schmidt独立准则来解决这个问题。

    Kernel techniques are among the most popular and powerful approaches of data science. Among the key features that make kernels ubiquitous are (i) the number of domains they have been designed for, (ii) the Hilbert structure of the function class associated to kernels facilitating their statistical analysis, and (iii) their ability to represent probability distributions without loss of information. These properties give rise to the immense success of Hilbert-Schmidt independence criterion (HSIC) which is able to capture joint independence of random variables under mild conditions, and permits closed-form estimators with quadratic computational complexity (w.r.t. the sample size). In order to alleviate the quadratic computational bottleneck in large-scale applications, multiple HSIC approximations have been proposed, however these estimators are restricted to $M=2$ random variables, do not extend naturally to the $M\ge 2$ case, and lack theoretical guarantees. In this work, we propose an
    
[^125]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^126]: 关于知识蒸馏中的学生-教师偏差：违反规则是否有益？

    On student-teacher deviations in distillation: does it pay to disobey?. (arXiv:2301.12923v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12923](http://arxiv.org/abs/2301.12923)

    通过实验和理论分析，本论文发现在知识蒸馏中，学生网络对教师网络的概率偏离是系统性夸大的，同时也得到了更好的泛化能力。

    

    知识蒸馏（KD）被广泛用于通过训练学生模仿经过训练的“教师”网络的软概率来提高“学生”网络的测试准确性。然而，最近的研究表明，尽管被训练成适应教师的概率，学生不仅明显偏离这些概率，而且表现比教师更好。我们的研究旨在通过确定学生-教师偏差的确切性质，并论证它们与更好的泛化能力如何共存来解决这一看似矛盾的观察。首先，通过对图像和语言数据进行实验，我们确定这些偏差对应于学生系统性地夸大教师的自信水平。接下来，在一些简单的设置中，我们从理论和实证上建立了KD在收敛更快的过程中夸大了梯度下降的隐含偏差的证据。最后，

    Knowledge distillation (KD) has been widely-used to improve the test accuracy of a ``student'' network by training the student to mimic soft probabilities of a trained "teacher" network. Yet, it has been shown in recent work that, despite being trained to fit the teacher's probabilities, the student not only significantly deviates from these probabilities, but also performs even better than the teacher. Our work aims to reconcile this seemingly paradoxical observation by characterizing the precise nature of the student-teacher deviations, and by arguing how they can co-occur with better generalization. First, through experiments on image and language data, we identify that these deviations correspond to the student systematically exaggerating the confidence levels of the teacher. Next, we theoretically and empirically establish in some simple settings that KD also exaggerates the implicit bias of gradient descent in converging faster along the top eigendirections of the data. Finally, 
    
[^127]: 从两人零和博弈中抽象出不完美信息

    Abstracting Imperfect Information Away from Two-Player Zero-Sum Games. (arXiv:2301.09159v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2301.09159](http://arxiv.org/abs/2301.09159)

    通过正则化均衡，可以将两人零和博弈中的不完美信息抽象出来并作为完全信息问题处理。

    

    Nayyar等人在其开创性的工作中表明，通过在游戏过程中让玩家公开宣布其策略，不完美信息可以被从共同效益游戏中抽象出来。这个见解是支撑共同效益游戏合理的求解器和决策时间规划算法的基础。不幸的是，将同样的见解简单应用于两人零和博弈会失败，因为具有公开策略宣布的游戏的纳什均衡可能与原始游戏的纳什均衡不相对应。因此，现有的合理的决策时间规划算法需要复杂的额外机制，其具有不吸引人的特性。本文的主要贡献是展示某些正则化均衡不具有上述的不对应问题，因此，计算它们可以被视为完全信息问题。因为这些正则化均衡可以被无限接近纳什均衡，我们的结果为一种新的视角打开了大门。

    In their seminal work, Nayyar et al. (2013) showed that imperfect information can be abstracted away from common-payoff games by having players publicly announce their policies as they play. This insight underpins sound solvers and decision-time planning algorithms for common-payoff games. Unfortunately, a naive application of the same insight to two-player zero-sum games fails because Nash equilibria of the game with public policy announcements may not correspond to Nash equilibria of the original game. As a consequence, existing sound decision-time planning algorithms require complicated additional mechanisms that have unappealing properties. The main contribution of this work is showing that certain regularized equilibria do not possess the aforementioned non-correspondence problem -- thus, computing them can be treated as perfect-information problems. Because these regularized equilibria can be made arbitrarily close to Nash equilibria, our result opens the door to a new perspectiv
    
[^128]: 机器学习辅助下的Reed-Muller子码高效解码

    Machine Learning-Aided Efficient Decoding of Reed-Muller Subcodes. (arXiv:2301.06251v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2301.06251](http://arxiv.org/abs/2301.06251)

    本论文研究了具有灵活码率的Reed-Muller子码的高效解码问题，通过扩展递归投影聚合（RPA）译码算法，提出了subRPA和soft-subRPA算法，能够在维持较低复杂性的同时提高译码性能并实现可微分的解码算法。

    

    Reed-Muller（RM）码在一般的二进制输入无记忆对称信道上达到容量，并且据推测，在比例定律方面的性能与随机码相当。但是，这些结果是建立在对一般码参数使用最大似然译码器的情况下的。此外，RM码只能接受有限的码率集。对于有限长度的RM码，已经有诸如连续取消列表（SCL）译码器和最近引入的递归投影聚合（RPA）译码器等高效译码器可用。本文我们研究具有灵活码率的RM码子码。首先我们将RPA译码算法扩展到RM子码上。为了降低我们的译码算法（称为subRPA）的复杂性，我们研究了不同的投影剪枝方法。接下来，我们推导出基于软判断的版本，称为soft-subRPA，它不仅改进了subRPA的性能，还使得译码算法可微分。

    Reed-Muller (RM) codes achieve the capacity of general binary-input memoryless symmetric channels and are conjectured to have a comparable performance to that of random codes in terms of scaling laws. However, such results are established assuming maximum-likelihood decoders for general code parameters. Also, RM codes only admit limited sets of rates. Efficient decoders such as successive cancellation list (SCL) decoder and recently-introduced recursive projection-aggregation (RPA) decoders are available for RM codes at finite lengths. In this paper, we focus on subcodes of RM codes with flexible rates. We first extend the RPA decoding algorithm to RM subcodes. To lower the complexity of our decoding algorithm, referred to as subRPA, we investigate different approaches to prune the projections. Next, we derive the soft-decision based version of our algorithm, called soft-subRPA, that not only improves upon the performance of subRPA but also enables a differentiable decoding algorithm. 
    
[^129]: 使用调整掩码的终身强化学习

    Lifelong Reinforcement Learning with Modulating Masks. (arXiv:2212.11110v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.11110](http://arxiv.org/abs/2212.11110)

    本文研究了终身强化学习中使用调整掩码的方法，通过将调整掩码应用于PPO和IMPALA代理，显著提高了在离散和连续强化学习任务中的性能。

    

    终身学习旨在创建在其生命周期中持续和逐步学习的人工智能系统，类似生物学习。到目前为止，这方面的尝试遇到了问题，包括灾难性遗忘、任务之间的干扰，以及无法利用先前的知识。虽然已经有相当多的研究集中在学习涉及输入分布变化的多个监督分类任务上，但是终身强化学习必须处理状态和转换分布以及奖励函数的变化。最近针对分类问题开发的使用固定骨干网络的调整掩码对于处理如此大范围的任务变化特别适用。在本文中，我们将调整掩码应用于深层次的终身强化学习，具体包括PPO和IMPALA代理。在离散和连续强化学习任务中与终身强化学习基线进行了比较，结果显示出卓越的性能。我们进一步研究了先前任务的线性组合的使用。

    Lifelong learning aims to create AI systems that continuously and incrementally learn during a lifetime, similar to biological learning. Attempts so far have met problems, including catastrophic forgetting, interference among tasks, and the inability to exploit previous knowledge. While considerable research has focused on learning multiple supervised classification tasks that involve changes in the input distribution, lifelong reinforcement learning (LRL) must deal with variations in the state and transition distributions, and in the reward functions. Modulating masks with a fixed backbone network, recently developed for classification, are particularly suitable to deal with such a large spectrum of task variations. In this paper, we adapted modulating masks to work with deep LRL, specifically PPO and IMPALA agents. The comparison with LRL baselines in both discrete and continuous RL tasks shows superior performance. We further investigated the use of a linear combination of previousl
    
[^130]: EEG解码的深度黎曼网络

    Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10426](http://arxiv.org/abs/2212.10426)

    本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。

    

    当前在电脑脑电图（EEG）解码任务中，最先进的性能通常是由深度学习或基于黎曼几何的解码器实现的。最近，越来越多的人对深度黎曼网络（DRNs）产生了兴趣，可能结合了之前两类方法的优点。然而，还有一系列问题需要进一步洞察，以铺平DRNs在EEG中更广泛应用的道路。这些问题包括架构设计问题，如网络大小和端到端能力，以及模型训练问题。这些因素如何影响模型性能尚未被探索。此外，这些网络中的数据如何转换，以及是否与传统的EEG解码相关也不清楚。本研究旨在通过分析具有广泛超参数的DRNs来奠定这些主题领域的基础。使用两个公共EEG数据集测试了网络，并与最先进的基于黎曼几何的方法进行了比较。

    State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning or Riemannian-Geometry-based decoders. Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability as well as model training questions. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on two public EEG datasets and compared with sta
    
[^131]: 高维潜空间中可靠的展开度量方法

    Reliable Measures of Spread in High Dimensional Latent Spaces. (arXiv:2212.08172v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08172](http://arxiv.org/abs/2212.08172)

    本文定义了数据展开度的概念，并发现常用的度量方法不可靠，提出了八种备选的数据展开度量方法，并推荐使用一种基于主成分和一种基于熵的度量方法，这些方法可以可靠地比较不同模型的展开度。

    

    理解自然语言处理模型潜空间的几何特性可以通过操作这些特性来改善下游任务的性能。其中一个特性是模型潜空间中的数据展开度，即可用潜空间的充分利用程度。在这项工作中，我们定义了数据展开度，并证明了常用的数据展开度度量方法，平均余弦相似度和分区函数的最小/最大比例I（V），不能提供可靠的对比不同模型潜空间使用情况的指标。我们提出并研究了八种备选的数据展开度度量方法，其中除一种外，所有方法在应用于七种合成数据分布时都优于当前的度量方法。在我们提出的度量方法中，推荐使用一种基于主成分的度量方法和一种基于熵的度量方法，它们可以提供可靠的、相对的展开度量，并可用于比较不同大小和维度的模型。

    Understanding geometric properties of natural language processing models' latent spaces allows the manipulation of these properties for improved performance on downstream tasks. One such property is the amount of data spread in a model's latent space, or how fully the available latent space is being used. In this work, we define data spread and demonstrate that the commonly used measures of data spread, Average Cosine Similarity and a partition function min/max ratio I(V), do not provide reliable metrics to compare the use of latent space across models. We propose and examine eight alternative measures of data spread, all but one of which improve over these current metrics when applied to seven synthetic data distributions. Of our proposed measures, we recommend one principal component-based measure and one entropy-based measure that provide reliable, relative measures of spread and can be used to compare models of different sizes and dimensionalities.
    
[^132]: 学习针对不同任务和分布的有用表示

    Learning useful representations for shifting tasks and distributions. (arXiv:2212.07346v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07346](http://arxiv.org/abs/2212.07346)

    处理多个分布时，通过连接多个训练过程得到的表示比单个训练过程得到的表示更好。

    

    当我们处理多个分布时，以优化单个训练分布的预期成本为副作用学习表示的主导方法是否仍然是一个好方法？我们的论点是，在这种情况下，比单个优化过程获得的更丰富的表示更好。我们用简单的理论论证和利用一个表面上天真的集成技术进行的实验来支持这个论点：将从多个训练过程获得的表示连接起来，这些训练过程使用相同的数据、模型、算法和超参数，但使用不同的随机种子进行训练。这些独立训练的网络表现相似。然而，在涉及新分布的一些情况下，连接的表示比用单个训练运行训练的等效大小的网络表现出色。这证明了多个训练过程构建的表示实际上是不同的。

    Does the dominant approach to learn representations (as a side effect of optimizing an expected cost for a single training distribution) remain a good approach when we are dealing with multiple distributions? Our thesis is that such scenarios are better served by representations that are richer than those obtained with a single optimization episode. We support this thesis with simple theoretical arguments and with experiments utilizing an apparently na\"{\i}ve ensembling technique: concatenating the representations obtained from multiple training episodes using the same data, model, algorithm, and hyper-parameters, but different random seeds. These independently trained networks perform similarly. Yet, in a number of scenarios involving new distributions, the concatenated representation performs substantially better than an equivalently sized network trained with a single training run. This proves that the representations constructed by multiple training episodes are in fact different.
    
[^133]: 针对声学对抗性机器学习逃避的实时语音情感检测的隐私保护

    Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning. (arXiv:2211.09273v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.09273](http://arxiv.org/abs/2211.09273)

    本研究通过对抗机器学习的方法，提出了一种名为DARE-GP的解决方案，可以在不影响智能音箱效用的情况下，逃避与智能音箱相连的语音情感识别分类器，从而保护隐私。

    

    情感监视是一个新兴领域，涉及到广泛的隐私问题。这些问题在普遍存在的物联网设备和多传感器的支持下，变得更加严重，而这些设备可以支持这些监视用例。本文考虑了一个这样的应用案例：使用与智能音箱相连的语音情感识别（SER）分类器。本文展示了如何在不影响智能音箱的效用的情况下，逃避与智能音箱相连的黑盒SER分类器。通过对机器学习的对抗逃避的视角来考虑这个隐私问题。我们的解决方案名为“通过遗传规划击败声学情感识别的DARE-GP”，它使用遗传规划生成非侵入性的添加音频扰动（AAPs）。通过约束这些AAP的进化，可以保护转录准确性，同时降低SER分类器的性能。这些AAP的加性特性，以及为特定目标生成这些AAPs的方法，使DARE-GP成为一个有效的隐私保护方法。

    Emotional Surveillance is an emerging area with wide-reaching privacy concerns. These concerns are exacerbated by ubiquitous IoT devices with multiple sensors that can support these surveillance use cases. The work presented here considers one such use case: the use of a speech emotion recognition (SER) classifier tied to a smart speaker. This work demonstrates the ability to evade black-box SER classifiers tied to a smart speaker without compromising the utility of the smart speaker. This privacy concern is considered through the lens of adversarial evasion of machine learning. Our solution, Defeating Acoustic Recognition of Emotion via Genetic Programming (DARE-GP), uses genetic programming to generate non-invasive additive audio perturbations (AAPs). By constraining the evolution of these AAPs, transcription accuracy can be protected while simultaneously degrading SER classifier performance. The additive nature of these AAPs, along with an approach that generates these AAPs for a fi
    
[^134]: 数据驱动的Kolmogorov流低维动力学模型

    Data-driven low-dimensional dynamic model of Kolmogorov flow. (arXiv:2210.16708v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16708](http://arxiv.org/abs/2210.16708)

    该论文提出了一种数据驱动的低维动力学模型方法，用于捕捉Kolmogorov流的流动动力学和属性。通过使用欠完备自编码器进行维度降低和潜在空间的离散时间模型开发，我们可以有效地建模具有挑战性的混沌和间歇行为。

    

    减少计算成本以模拟流动动力学、并用于模型控制方法的降维模型是具有吸引力的。本研究提出了一种数据驱动的框架，用于有效捕捉流动的动力学和性质的最小维度模型。我们将其应用于在混沌和间歇行为中的Kolmogorov流，这在许多流程中很常见且难以建模。流动的轨迹接近于相对周期轨道(RPOs)，并夹杂着与包含RPOs的区域之间的偶发爆发事件相对应的远足。模型开发的第一步是使用一个欠完备的自编码器将完整状态数据映射到维度显著较低的潜在空间。然后，我们开发了潜在空间动力学的离散时间演化模型。通过分析模型在潜在空间维度上的性能，我们可以确定最佳的维度选择。

    Reduced order models (ROMs) that capture flow dynamics are of interest for decreasing computational costs for simulation as well as for model-based control approaches. This work presents a data-driven framework for minimal-dimensional models that effectively capture the dynamics and properties of the flow. We apply this to Kolmogorov flow in a regime consisting of chaotic and intermittent behavior, which is common in many flows processes and is challenging to model. The trajectory of the flow travels near relative periodic orbits (RPOs), interspersed with sporadic bursting events corresponding to excursions between the regions containing the RPOs. The first step in development of the models is use of an undercomplete autoencoder to map from the full state data down to a latent space of dramatically lower dimension. Then models of the discrete-time evolution of the dynamics in the latent space are developed. By analyzing the model performance as a function of latent space dimension we c
    
[^135]: 关于广义似然比检验和一类分类器

    On the Generalized Likelihood Ratio Test and One-Class Classifiers. (arXiv:2210.12494v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12494](http://arxiv.org/abs/2210.12494)

    本文考虑了一类分类器和广义似然比检验的问题，证明了多层感知器神经网络和支持向量机模型在收敛时会表现为广义似然比检验。同时，作者还展示了一类最小二乘SVM在收敛时也能达到广义似然比检验的效果。

    

    一类分类（OCC）是决定观察样本是否属于目标类的问题。我们考虑在包含目标类样本的数据集上学习一个表现为广义似然比检验（GLRT）的OCC模型的问题。当目标类的统计信息可用时，GLRT解决了相同的问题。GLRT是一个众所周知且在特定条件下可证明最佳的分类器。为此，我们考虑了多层感知器神经网络（NN）和支持向量机（SVM）模型。它们使用人工数据集训练为两类分类器，其中替代类使用在目标类数据集的定义域上均匀生成的随机样本。我们证明，在适当的假设下，模型在大数据集上收敛到了GLRT。此外，我们还展示了具有适当核函数的一类最小二乘SVM（OCLSSVM）在收敛时表现为GLRT。

    One-class classification (OCC) is the problem of deciding whether an observed sample belongs to a target class. We consider the problem of learning an OCC model that performs as the generalized likelihood ratio test (GLRT), given a dataset containing samples of the target class. The GLRT solves the same problem when the statistics of the target class are available. The GLRT is a well-known and provably optimal (under specific assumptions) classifier. To this end, we consider both the multilayer perceptron neural network (NN) and the support vector machine (SVM) models. They are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. We prove that, under suitable assumptions, the models converge (with a large dataset) to the GLRT. Moreover, we show that the one-class least squares SVM (OCLSSVM) with suitable kernels at convergence performs as the GLRT. Lastly, we
    
[^136]: 用Riemannian优化学习图形因子模型

    Learning Graphical Factor Models with Riemannian Optimization. (arXiv:2210.11950v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.11950](http://arxiv.org/abs/2210.11950)

    本文提出了一种灵活的算法框架，用于在协方差矩阵上具有低秩结构约束的图学习。通过使用Riemannian优化，利用正定矩阵和固定秩正半定矩阵的几何特性，解决了这类问题。

    

    图形模型和因子分析是多元统计学中成熟的工具。尽管这些模型都可以与协方差和精度矩阵的结构联系起来，但它们通常在图的学习过程中没有被共同利用。因此，本文通过提出一种在协方差矩阵上具有低秩结构约束的图学习的灵活算法框架来解决这个问题。该问题被表达为基于最大似然估计的罚函数方法，其中协方差矩阵可以选择性地被约束为低秩加对角线结构（低秩因子模型）。然后，我们利用正定矩阵和固定秩正半定矩阵的几何特性（这些特性非常适用于椭圆模型）来解决这类问题中的最优化问题。数值实验

    Graphical models and factor analysis are well-established tools in multivariate statistics. While these models can be both linked to structures exhibited by covariance and precision matrices, they are generally not jointly leveraged within graph learning processes. This paper therefore addresses this issue by proposing a flexible algorithmic framework for graph learning under low-rank structural constraints on the covariance matrix. The problem is expressed as penalized maximum likelihood estimation of an elliptical distribution (a generalization of Gaussian graphical models to possibly heavy-tailed distributions), where the covariance matrix is optionally constrained to be structured as low-rank plus diagonal (low-rank factor model). The resolution of this class of problems is then tackled with Riemannian optimization, where we leverage geometries of positive definite matrices and positive semi-definite matrices of fixed rank that are well suited to elliptical models. Numerical experi
    
[^137]: 3DALL-E: 将文本到图像AI集成到3D设计工作流程中

    3DALL-E: Integrating Text-to-Image AI in 3D Design Workflows. (arXiv:2210.11603v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2210.11603](http://arxiv.org/abs/2210.11603)

    本文介绍了一种将文本到图像AI集成到3D设计工作流程中的方法，通过插件3DALL-E，设计师可以使用AI生成的图像灵感构建3D模型。研究结果显示，设计师对于3DALL-E在工作流程中有很大的潜力，可以生成参考图像、防止设计固化并激发设计的考虑。

    

    文本到图像AI能够生成新颖的图像以供灵感，但其在3D设计工作流程中的应用以及设计师如何利用AI提供的灵感构建3D模型尚未被探索。为了研究这个问题，我们将DALL-E、GPT-3和CLIP集成到CAD软件中，创建了3DALL-E插件，用于生成3D设计的2D图像灵感。3DALL-E允许用户基于他们正在建模的内容构建文本和图像提示。在一项涉及13名设计师的研究中，我们发现设计师们认为3DALL-E在他们的工作流程中具有巨大的潜力，可以使用文本到图像AI生成参考图像，防止设计固化，并激发设计的考虑。我们详述了在3D建模任务中观察到的提示模式，并提供了参与者所观察到的提示复杂性的度量。根据我们的研究结果，我们讨论了如何将3DALL-E与现有的生成式设计工作流程相结合，并提出了提示文献目录作为一种人工智能-人类设计历史的形式。

    Text-to-image AI are capable of generating novel images for inspiration, but their applications for 3D design workflows and how designers can build 3D models using AI-provided inspiration have not yet been explored. To investigate this, we integrated DALL-E, GPT-3, and CLIP within a CAD software in 3DALL-E, a plugin that generates 2D image inspiration for 3D design. 3DALL-E allows users to construct text and image prompts based on what they are modeling. In a study with 13 designers, we found that designers saw great potential in 3DALL-E within their workflows and could use text-to-image AI to produce reference images, prevent design fixation, and inspire design considerations. We elaborate on prompting patterns observed across 3D modeling tasks and provide measures of prompt complexity observed across participants. From our findings, we discuss how 3DALL-E can merge with existing generative design workflows and propose prompt bibliographies as a form of human-AI design history.
    
[^138]: RibSeg v2：肋骨标记和解剖中心线提取的大规模基准测试

    RibSeg v2: A Large-scale Benchmark for Rib Labeling and Anatomical Centerline Extraction. (arXiv:2210.09309v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2210.09309](http://arxiv.org/abs/2210.09309)

    本文扩展了RibSeg数据集到大规模基准测试RibSeg v2，加入了手动标注的肋骨标记和解剖中心线提取，共包含660个CT扫描（15,466个独立的肋骨），并提出了深度学习方法用于肋骨标记、基于骨架化方法用于中心线提取、一种稀疏点云表示CT扫描的方法，以及适用于该任务的评估指标。

    

    自动化的肋骨标记和解剖中心线提取是各种临床应用的常见前提条件。以往的研究要么使用内部数据集，无法为社群所共享，要么只关注于肋骨分割而忽略了肋骨标记的临床意义。为了解决这些问题，本文将之前计算机断层扫描（CT）肋骨分割的RibSeg数据集扩展为综合性基准测试RibSeg v2，并加入了肋骨标记和解剖中心线提取的手动标注，共包含了660个CT扫描（15,466个独立的肋骨）。基于RibSeg v2数据集，我们开发了一种包含深度学习方法用于肋骨标记，以及基于骨架化方法用于中心线提取的流程。为了提高计算效率，我们还提出了一种稀疏点云表示CT扫描的方法，与标准的密集体素网格进行了比较。此外，我们还设计和分析评估指标，以解决每个任务的关键挑战。我们的数据集是，…

    Automatic rib labeling and anatomical centerline extraction are common prerequisites for various clinical applications. Prior studies either use in-house datasets that are inaccessible to communities, or focus on rib segmentation that neglects the clinical significance of rib labeling. To address these issues, we extend our prior dataset (RibSeg) on the binary rib segmentation task to a comprehensive benchmark, named RibSeg v2, with 660 CT scans (15,466 individual ribs in total) and annotations manually inspected by experts for rib labeling and anatomical centerline extraction. Based on the RibSeg v2, we develop a pipeline including deep learning-based methods for rib labeling, and a skeletonization-based method for centerline extraction. To improve computational efficiency, we propose a sparse point cloud representation of CT scans and compare it with standard dense voxel grids. Moreover, we design and analyze evaluation metrics to address the key challenges of each task. Our dataset,
    
[^139]: FAIR for AI:跨学科和国际社区建设的视角

    FAIR for AI: An interdisciplinary and international community building perspective. (arXiv:2210.08973v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2210.08973](http://arxiv.org/abs/2210.08973)

    FAIR原则旨在实现学术数据的重用性，并在人工智能模型和数据领域得到应用。研究人员从不同背景和学科的角度讨论了他们在实践社区中定义和采用FAIR原则的经验和成果，以及追求和激励FAIR人工智能研究可能带来的结果。

    

    2016年，提出了一组基本的可寻找、可访问、可互操作和可重用（FAIR）原则，作为适当的数据管理和管理的先决条件，旨在实现学术数据的可重用性。这些原则也适用于其他数字资产，并随着时间的推移，FAIR指导原则被重新解释或扩展，包括产生数据的软件、工具、算法和工作流程。如今，FAIR原则正在适应人工智能模型和数据的背景下。在这里，我们展示了来自不同国家、学科和背景的研究人员的观点、愿景和经验，他们领导着在他们的实践社区中定义和采用FAIR原则，并讨论追求和激励FAIR人工智能研究可能产生的结果。本报告的材料基于2022年6月7日在阿贡国家实验室举行的FAIR for AI研讨会。

    A foundational set of findable, accessible, interoperable, and reusable (FAIR) principles were proposed in 2016 as prerequisites for proper data management and stewardship, with the goal of enabling the reusability of scholarly data. The principles were also meant to apply to other digital assets, at a high level, and over time, the FAIR guiding principles have been re-interpreted or extended to include the software, tools, algorithms, and workflows that produce data. FAIR principles are now being adapted in the context of AI models and datasets. Here, we present the perspectives, vision, and experiences of researchers from different countries, disciplines, and backgrounds who are leading the definition and adoption of FAIR principles in their communities of practice, and discuss outcomes that may result from pursuing and incentivizing FAIR AI research. The material for this report builds on the FAIR for AI Workshop held at Argonne National Laboratory on June 7, 2022.
    
[^140]: 基于自动编码器的未知数量单通道水声信号源分离研究

    Source Separation of Unknown Numbers of Single-Channel Underwater Acoustic Signals Based on Autoencoders. (arXiv:2207.11749v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2207.11749](http://arxiv.org/abs/2207.11749)

    本研究提出了一种基于自动编码器的解决方案，用于对未知数量的单通道水声信号进行源分离。通过固定输出通道数量和新的性能评估方法，避免了排列问题引起的维度灾难，并在实验证明与已知信号数量相似的分离性能。该算法具有竞争性能、可解释性和可扩展性，在该框架下达到了最先进的水平。

    

    目前很少有研究关注未知数量信号的源分离问题，以及如何评估系统的性能尚不清楚。为了解决这两个问题，我们提出了一个具有固定输出通道数量的解决方案，避免了由于输出与目标对齐引起的排列问题导致的维度灾难。具体而言，我们提出了一个基于自动编码器的两步算法，并针对有静音通道的情况提出了一种新的性能评估方法。通过在模拟混合的辐射船噪声上进行的实验表明，所提出的解决方案可以达到与已知信号数量相似的分离性能。所提出的算法在已知信号数量的情况下取得了竞争性能，具有高度可解释性和可扩展性，并在该框架下达到了最先进的水平。

    Few existing studies focus on the source separation problem with unknown numbers of signals, and how to evaluate the performances of the systems is not yet clear. We propose a solution with a fixed number of output channels to address these two problems, enabling it to avoid the dimensional disaster caused by the permutation problem induced by the alignment of outputs to targets. Specifically, we propose a two-step algorithm based on autoencoders and a new performance evaluation method for situations with mute channels. Experiments conducted on simulated mixtures of radiated ship noise show that the proposed solution can achieve similar separation performance to that attained with a known number of signals. The proposed algorithm achieved competitive performance as two algorithms developed for known numbers of signals, which is highly explainable and extensible and get the state of the art under this framework.
    
[^141]: 具有遮挡的自动驾驶的可验证目标识别

    Verifiable Goal Recognition for Autonomous Driving with Occlusions. (arXiv:2206.14163v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.14163](http://arxiv.org/abs/2206.14163)

    OGRIT是一种用于自动驾驶的目标识别方法，能够处理遮挡导致的数据缺失，并且具有快速、准确、可解释和可验证的特点。

    

    目标识别（GR）涉及推断其他车辆的目标，例如特定的路口出口，这可以更准确地预测它们的未来行为。在自动驾驶中，车辆可能会遇到许多不同的情景，由于遮挡，环境可能是部分可观察的。我们提出了一种名为OGRIT（Goal Recognition with Interpretable Trees under Occlusion）的新型GR方法。OGRIT利用从车辆轨迹数据学习到的决策树来推断一组生成目标的概率。我们证明了OGRIT能够处理由于遮挡而导致的数据缺失，并且使用相同的学习决策树在多个场景中进行推理，同时具有计算速度快，准确性高，可解释性强和可验证性的特点。我们还发布了用于评估OGRIT的遮挡区域的inDO，rounDO和OpenDDO数据集。

    Goal recognition (GR) involves inferring the goals of other vehicles, such as a certain junction exit, which can enable more accurate prediction of their future behaviour. In autonomous driving, vehicles can encounter many different scenarios and the environment may be partially observable due to occlusions. We present a novel GR method named Goal Recognition with Interpretable Trees under Occlusion (OGRIT). OGRIT uses decision trees learned from vehicle trajectory data to infer the probabilities of a set of generated goals. We demonstrate that OGRIT can handle missing data due to occlusions and make inferences across multiple scenarios using the same learned decision trees, while being computationally fast, accurate, interpretable and verifiable. We also release the inDO, rounDO and OpenDDO datasets of occluded regions used to evaluate OGRIT.
    
[^142]: 可争议神经网络的因果发现与知识注入

    Causal Discovery and Knowledge Injection for Contestable Neural Networks. (arXiv:2205.09787v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.09787](http://arxiv.org/abs/2205.09787)

    本研究提出了一种可以进行双向互动的方法，通过允许神经网络展示其所学因果图，并允许人类修改因果图后重新注入机器中，从而提供了一种调试神经网络的方式，实验结果显示该方法可以显著改善预测性能。

    

    神经网络在解决机器学习任务方面表现出色，但它们是否学习到了相关的因果关系尚不清楚，而它们的黑箱特性使得模型构建者难以理解和调试。我们提出了一种新颖的方法来解决这些问题，通过允许神经网络驱动的机器展示其所学因果图，并允许人类修改因果图后重新注入机器中，实现双向互动。所学模型保证符合因果图并遵循专家知识，其中部分知识也可以事先给定。通过对模型行为进行可视化并实现知识注入，我们的方法允许从数据中发现因果结构并支撑预测的从业者进行调试。在真实和合成表格数据上的实验表明，我们的方法可以改进预测性能高达2.4倍。

    Neural networks have proven to be effective at solving machine learning tasks but it is unclear whether they learn any relevant causal relationships, while their black-box nature makes it difficult for modellers to understand and debug them. We propose a novel method overcoming these issues by allowing a two-way interaction whereby neural-network-empowered machines can expose the underpinning learnt causal graphs and humans can contest the machines by modifying the causal graphs before re-injecting them into the machines. The learnt models are guaranteed to conform to the graphs and adhere to expert knowledge, some of which can also be given up-front. By building a window into the model behaviour and enabling knowledge injection, our method allows practitioners to debug networks based on the causal structure discovered from the data and underpinning the predictions. Experiments with real and synthetic tabular data show that our method improves predictive performance up to 2.4x while pr
    
[^143]: 深度批量主动学习回归的框架和基准

    A Framework and Benchmark for Deep Batch Active Learning for Regression. (arXiv:2203.09410v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.09410](http://arxiv.org/abs/2203.09410)

    本研究提出了一个深度批量主动学习回归的框架和基准测试，其中包括许多现有的贝叶斯和非贝叶斯方法。提出了一种替换常用最后一层特征的新方法，并结合一种新颖的聚类方法。在15个大型表格回归数据集上进行测试，该方法在基准测试中表现优异，适用于大型数据集且易于使用。

    

    标注监督学习数据的获取成本较高。为了提高神经网络回归的样本效率，我们研究了自适应选择无标签数据批次进行标注的主动学习方法。我们提出了一个框架，用于构建这样的方法，基于(网络相关的)基础核、核变换和选择方法。我们的框架包括许多现有的基于高斯过程逼近神经网络的贝叶斯方法以及非贝叶斯方法。此外，我们建议用描绘有限宽度神经正切核替换常用的最后一层特征，并将它们与一种新颖的聚类方法相结合。为了评估不同的方法，我们介绍了一个由15个大型表格回归数据集组成的开放源代码的基准测试。我们提出的方法在基准测试中优于现有技术水平，适用于大型数据集，并且可以直接使用，无需调整网络架构或训练。

    The acquisition of labels for supervised learning can be expensive. In order to improve the sample-efficiency of neural network regression, we study active learning methods that adaptively select batches of unlabeled data for labeling. We present a framework for constructing such methods out of (network-dependent) base kernels, kernel transformations and selection methods. Our framework encompasses many existing Bayesian methods based on Gaussian Process approximations of neural networks as well as non-Bayesian methods. Additionally, we propose to replace the commonly used last-layer features with sketched finite-width Neural Tangent Kernels, and to combine them with a novel clustering method. To evaluate different methods, we introduce an open-source benchmark consisting of 15 large tabular regression data sets. Our proposed method outperforms the state-of-the-art on our benchmark, scales to large data sets, and works out-of-the-box without adjusting the network architecture or traini
    
[^144]: AgraSSt: 适用于解释性评估隐式图生成器的近似图斯坦统计方法

    AgraSSt: Approximate Graph Stein Statistics for Interpretable Assessment of Implicit Graph Generators. (arXiv:2203.03673v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.03673](http://arxiv.org/abs/2203.03673)

    AgraSSt是一种用于评估隐式图生成器质量的统计方法，通过构建基于核的差异度量，它能够确定学习到的图生成过程是否能生成类似给定输入图的图形，并提供有关图生成器训练过程的可解释性问题和可靠样本批次的信息。

    

    我们提出并分析了一种新的统计方法，称为AgraSSt，用于评估可能不存在显式形式的图生成器的质量。特别地，AgraSSt可用于确定学习到的图生成过程能否生成类似给定输入图的图形。受随机图的斯坦操纵符启发，AgraSSt的关键思想是基于从图生成器获得的操作符构建基于核的差异度量。AgraSSt可以为图生成器训练过程提供可解释的问题，并帮助识别可靠的样本批次用于下游任务。利用斯坦的方法，我们对广泛的随机图模型给出了理论保证。我们在已知图生成过程的合成输入图和最先进的（深度）图生成模型训练的真实世界输入图上提供了实证结果。

    We propose and analyse a novel statistical procedure, coined AgraSSt, to assess the quality of graph generators that may not be available in explicit form. In particular, AgraSSt can be used to determine whether a learnt graph generating process is capable of generating graphs that resemble a given input graph. Inspired by Stein operators for random graphs, the key idea of AgraSSt is the construction of a kernel discrepancy based on an operator obtained from the graph generator. AgraSSt can provide interpretable criticisms for a graph generator training procedure and help identify reliable sample batches for downstream tasks. Using Stein`s method we give theoretical guarantees for a broad class of random graph models. We provide empirical results on both synthetic input graphs with known graph generation procedures, and real-world input graphs that the state-of-the-art (deep) generative models for graphs are trained on.
    
[^145]: 不同输入维度数据集之间的迁移学习：线性回归情况下的算法和分析

    Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case. (arXiv:2202.05069v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.05069](http://arxiv.org/abs/2202.05069)

    本文提出了一种适用于线性回归情况的迁移学习算法，该算法能够将新数据与历史数据相结合，特别在新数据稀缺的情况下具有益处，并且在实验验证中表现出对负迁移学习的鲁棒性。

    

    随着新传感器和监测设备的发展，越来越多的数据源可以作为机器学习模型的输入。这些数据既可以帮助提高模型的准确性，但将这些新输入与历史数据相结合仍然是一个尚未详细研究的挑战。在本文中，我们提出了一种迁移学习算法，将新数据和历史数据结合起来，特别在新数据稀缺的情况下具有益处。我们将重点放在线性回归情况下，这使得我们能够对该方法的益处进行严格的理论研究。我们表明我们的方法对负迁移学习是具有鲁棒性的，并通过真实和模拟数据进行了实证验证。

    With the development of new sensors and monitoring devices, more sources of data become available to be used as inputs for machine learning models. These can on the one hand help to improve the accuracy of a model. On the other hand however, combining these new inputs with historical data remains a challenge that has not yet been studied in enough detail. In this work, we propose a transfer-learning algorithm that combines the new and the historical data, that is especially beneficial when the new data is scarce. We focus the approach on the linear regression case, which allows us to conduct a rigorous theoretical study on the benefits of the approach. We show that our approach is robust against negative transfer-learning, and we confirm this result empirically with real and simulated data.
    
[^146]: 多输出高斯过程数据增强用于多建筑和多楼层室内定位

    Multi-Output Gaussian Process-Based Data Augmentation for Multi-Building and Multi-Floor Indoor Localization. (arXiv:2202.01980v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2202.01980](http://arxiv.org/abs/2202.01980)

    本论文研究了基于多输出高斯过程的三种接收信号强度指纹数据增强方法，用于解决室内定位中数据收集困难的问题。

    

    基于接收信号强度指纹的定位成为室内定位的主流技术，因为它无需安装新的基础设施和修改现有设备，尤其是考虑到现代建筑中Wi-Fi设备的普及和无处不在的Wi-Fi接入。使用AI/ML技术如深度神经网络(DNNs)可以使定位指纹更精确、可靠，特别适用于大规模的多建筑和多楼层室内定位。然而，室内定位的DNNs应用依赖于大量经过预处理和精心标记的数据进行训练。考虑到在室内环境中的数据收集的困难，尤其是在当前COVID-19流行病情况下，我们研究了基于多输出高斯过程（MOGP）的三种不同的接收信号强度指纹数据增强方法，即通过单层楼、邻近楼层和单个建筑；不同于单输出高斯过程。

    Location fingerprinting based on RSSI becomes a mainstream indoor localization technique due to its advantage of not requiring the installation of new infrastructure and the modification of existing devices, especially given the prevalence of Wi-Fi-enabled devices and the ubiquitous Wi-Fi access in modern buildings. The use of AI/ML technologies like DNNs makes location fingerprinting more accurate and reliable, especially for large-scale multi-building and multi-floor indoor localization. The application of DNNs for indoor localization, however, depends on a large amount of preprocessed and deliberately-labeled data for their training. Considering the difficulty of the data collection in an indoor environment, especially under the current epidemic situation of COVID-19, we investigate three different methods of RSSI data augmentation based on Multi-Output Gaussian Process (MOGP), i.e., by a single floor, by neighboring floors, and by a single building; unlike Single-Output Gaussian Pr
    
[^147]: 使用深度学习识别Pauli自旋阻碍

    Identifying Pauli spin blockade using deep learning. (arXiv:2202.00574v3 [cond-mat.mes-hall] UPDATED)

    [http://arxiv.org/abs/2202.00574](http://arxiv.org/abs/2202.00574)

    该论文介绍了一种使用深度学习算法自动识别Pauli自旋阻碍的方法，通过训练算法使用模拟数据和跨设备验证，克服了PSB数据的稀缺性，并在硅场效应晶体管器件上取得了96％的准确性。该方法具有鲁棒性，预计可在所有类型的量子点器件上使用。

    

    Pauli自旋阻碍（PSB）可用于自旋量子位的初始化和读取，即使在高温下也能发挥作用，但很难识别。我们提出了一种利用电荷传输测量自动识别PSB的机器学习算法。通过训练算法使用模拟数据和跨设备验证，克服了PSB数据的稀缺性。我们在硅场效应晶体管器件上演示了我们的方法，并在不同的测试器件上报告了96％的准确性，这表明该方法对器件变异性具有鲁棒性。预计该方法可在所有类型的量子点器件上使用。

    Pauli spin blockade (PSB) can be employed as a great resource for spin qubit initialisation and readout even at elevated temperatures but it can be difficult to identify. We present a machine learning algorithm capable of automatically identifying PSB using charge transport measurements. The scarcity of PSB data is circumvented by training the algorithm with simulated data and by using cross-device validation. We demonstrate our approach on a silicon field-effect transistor device and report an accuracy of 96% on different test devices, giving evidence that the approach is robust to device variability. The approach is expected to be employable across all types of quantum dot devices.
    
[^148]: 深度神经网络的未知检测能力的统一评估标准

    A Unified Benchmark for the Unknown Detection Capability of Deep Neural Networks. (arXiv:2112.00337v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2112.00337](http://arxiv.org/abs/2112.00337)

    该论文提出了一个统一评估标准，针对深度神经网络对未知样本的检测能力进行了全面考察。通过构建统一基准数据集，并比较现有方法的表现，发现Deep Ensemble方法具有较高的未知检测能力。

    

    深度神经网络在各种任务上取得了出色的性能，但存在一个关键问题：对于完全未知的样本预测过于自信。许多研究已经提出成功过滤这些未知样本的方法，但只考虑了狭窄而具体的任务，如误分类检测、开集识别或分布外检测。在本研究中，我们认为这些任务应被视为根本相同的问题，因为理想的模型应具备对所有这些任务的检测能力。因此，我们引入了未知检测任务，将之前的单个任务进行整合，以对深度神经网络在广泛未知样本上的检测能力进行严格评估。为此，我们构建了不同规模的统一基准数据集，并对现有流行方法的未知检测能力进行了比较。

    Deep neural networks have achieved outstanding performance over various tasks, but they have a critical issue: over-confident predictions even for completely unknown samples. Many studies have been proposed to successfully filter out these unknown samples, but they only considered narrow and specific tasks, referred to as misclassification detection, open-set recognition, or out-of-distribution detection. In this work, we argue that these tasks should be treated as fundamentally an identical problem because an ideal model should possess detection capability for all those tasks. Therefore, we introduce the unknown detection task, an integration of previous individual tasks, for a rigorous examination of the detection capability of deep neural networks on a wide spectrum of unknown samples. To this end, unified benchmark datasets on different scales were constructed and the unknown detection capabilities of existing popular methods were subject to comparison. We found that Deep Ensemble 
    
[^149]: 多元极值的谱学习

    Spectral learning of multivariate extremes. (arXiv:2111.07799v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2111.07799](http://arxiv.org/abs/2111.07799)

    我们提出了一种用于分析多元极值的谱聚类算法，并通过理论和数值实验展示了其在学习角度测度方面的性能。

    

    我们提出了一种用于分析多元极值的谱聚类算法。具体而言，我们关注极值理论中由角度或谱测度表征的多元极值的渐近依赖性。我们的工作研究了谱聚类的理论性能，该聚类基于从极值样本中构建的随机k最近邻图，即对于半径超过一个较大阈值的随机向量的角度部分。具体而言，我们推导出线性因子模型产生的极值的渐近分布，并证明，在某些条件下，谱聚类可以一致地识别出在该模型中产生的极值的聚类。基于这个结果，我们提出了一种简单的一致性估计策略来学习角度测度。我们的理论结果与数值实验相结合，展示了我们方法在有限样本情况下的性能。

    We propose a spectral clustering algorithm for analyzing the dependence structure of multivariate extremes. More specifically, we focus on the asymptotic dependence of multivariate extremes characterized by the angular or spectral measure in extreme value theory. Our work studies the theoretical performance of spectral clustering based on a random $k$-nearest neighbor graph constructed from an extremal sample, i.e., the angular part of random vectors for which the radius exceeds a large threshold. In particular, we derive the asymptotic distribution of extremes arising from a linear factor model and prove that, under certain conditions, spectral clustering can consistently identify the clusters of extremes arising in this model. Leveraging this result we propose a simple consistent estimation strategy for learning the angular measure. Our theoretical findings are complemented with numerical experiments illustrating the finite sample performance of our methods.
    
[^150]: 《超维计算综述及矢量符号化架构》第一部分：模型和数据转换

    A Survey on Hyperdimensional Computing aka Vector Symbolic Architectures, Part I: Models and Data Transformations. (arXiv:2111.06077v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2111.06077](http://arxiv.org/abs/2111.06077)

    本文综述介绍了超维计算和矢量符号化架构（HDC/VSA），这是一种利用高维分布式表示和代数属性的计算模型，融合了结构化符号表示和向量分布式表示的优势。该领域涉及多个学科，并介绍了多个相关模型。

    

    这篇综述共分两部分，致力于介绍一个被称为超维计算和矢量符号化架构（HDC/VSA）的计算框架。HDC/VSA是一类使用高维分布式表示和依赖其关键操作的代数属性的计算模型，以融合结构化符号表示和向量分布式表示的优势。HDC/VSA家族中的显著模型包括张量积表示、全息减少表示、乘加置换、二元散射码和稀疏二元分布式表示，还有其他模型。HDC/VSA是一个高度跨学科的领域，涉及计算机科学、电子工程、人工智能、数学和认知科学。这一事实使得对该领域进行全面概述具有挑战性。

    This two-part comprehensive survey is devoted to a computing framework most commonly known under the names Hyperdimensional Computing and Vector Symbolic Architectures (HDC/VSA). Both names refer to a family of computational models that use high-dimensional distributed representations and rely on the algebraic properties of their key operations to incorporate the advantages of structured symbolic representations and vector distributed representations. Notable models in the HDC/VSA family are Tensor Product Representations, Holographic Reduced Representations, Multiply-Add-Permute, Binary Spatter Codes, and Sparse Binary Distributed Representations but there are other models too. HDC/VSA is a highly interdisciplinary field with connections to computer science, electrical engineering, artificial intelligence, mathematics, and cognitive science. This fact makes it challenging to create a thorough overview of the field. However, due to a surge of new researchers joining the field in recent
    
[^151]: 综合条件估计-优化

    Integrated Conditional Estimation-Optimization. (arXiv:2110.12351v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.12351](http://arxiv.org/abs/2110.12351)

    该论文提出了一种综合条件估计-优化（ICEO）框架，可以在考虑优化问题结构的同时估计随机参数的条件分布，并提供了一些性能保证。

    

    许多实际优化问题涉及具有概率分布的不确定参数，可以使用上下文特征信息进行估计。与先估计不确定参数的分布然后基于估计优化目标的标准方法相反，我们提出了一种综合条件估计-优化（ICEO）框架，该框架在考虑优化问题结构的同时估计随机参数的条件分布。我们直接建模随机参数的条件分布与上下文特征之间的关系，然后用与下游优化问题一致的目标估计概率模型。我们证明了我们的ICEO方法在适度规则条件下是渐进一致的，并进一步提供了一些推广界限形式的有限性能保证。计算上，使用

    Many real-world optimization problems involve uncertain parameters with probability distributions that can be estimated using contextual feature information. In contrast to the standard approach of first estimating the distribution of uncertain parameters and then optimizing the objective based on the estimation, we propose an integrated conditional estimation-optimization (ICEO) framework that estimates the underlying conditional distribution of the random parameter while considering the structure of the optimization problem. We directly model the relationship between the conditional distribution of the random parameter and the contextual features, and then estimate the probabilistic model with an objective that aligns with the downstream optimization problem. We show that our ICEO approach is asymptotically consistent under moderate regularity conditions and further provide finite performance guarantees in the form of generalization bounds. Computationally, performing estimation with
    
[^152]: 学习基于批准的多赢者投票规则的复杂性

    The Complexity of Learning Approval-Based Multiwinner Voting Rules. (arXiv:2110.00254v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2110.00254](http://arxiv.org/abs/2110.00254)

    本研究关注基于批准的多赢者投票规则的学习问题，我们发现尽管存在指数级的可能结果，但通过多项式数量的样本可以高置信度和准确性地学习目标规则。

    

    我们研究了多赢者投票的PAC可学习性，重点关注基于批准的委员会评分（ABCS）规则。这些规则适用于具有批准票的选民档案，每个选民批准一部分候选人。根据ABCS规则，每个$k$候选人委员会从每个选民那里收集得分，该得分取决于选民票数和其与委员会的交集的大小。然后，得分最高的委员会获胜。我们的目标是学习目标规则（即学习相应的评分函数），并利用少量采样档案中获胜的委员会的信息。尽管与单赢者选举相比存在指数级的结果，但我们表明样本复杂性仍然较低：多项式数量的样本足够提供足够的信息，以高置信度和准确性学习目标规则。不幸的是，即使对于学习任务来说也很简单，需要解决的任务也很简单，为了学习，我们在实验中证明了样本采样的效果。

    We study the {PAC} learnability of multiwinner voting, focusing on the class of approval-based committee scoring (ABCS) rules. These are voting rules applied on profiles with approval ballots, where each voter approves some of the candidates. According to ABCS rules, each committee of $k$ candidates collects from each voter a score, which depends on the size of the voter's ballot and on the size of its intersection with the committee. Then, committees of maximum score are the winning ones. Our goal is to learn a target rule (i.e., to learn the corresponding scoring function) using information about the winning committees of a small number of sampled profiles. Despite the existence of exponentially many outcomes compared to single-winner elections, we show that the sample complexity is still low: a polynomial number of samples carries enough information for learning the target rule with high confidence and accuracy. Unfortunately, even simple tasks that need to be solved for learning fr
    
[^153]: 动态集体智能学习：通过精炼的梯度找到高效稀疏模型以剪枝权重

    Dynamic Collective Intelligence Learning: Finding Efficient Sparse Model via Refined Gradients for Pruned Weights. (arXiv:2109.04660v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2109.04660](http://arxiv.org/abs/2109.04660)

    本文介绍了动态集体智能学习（DCIL）方法，利用精炼的梯度更新剪枝权重，通过形成双向转发路径来寻找高效稀疏模型。这种方法利用了剪枝和未剪枝权重的集体智能之间的学习协同作用。

    

    随着深度神经网络（DNN）的增长，DNN参数的数量大幅增加。这使得DNN模型难以部署在资源有限的嵌入式系统上。为了缓解这个问题，出现了动态剪枝方法，该方法利用直通估计（STE）来近似剪枝权重的梯度，在训练过程中寻找不同的稀疏模式。STE可以帮助剪枝权重在寻找动态稀疏模式的过程中复活。然而，使用这些粗糙的梯度会导致训练不稳定和性能下降，因为STE近似的梯度信号不可靠。在这项工作中，为了解决这个问题，我们引入了精炼的梯度来更新剪枝权重，通过从两组（剪枝和未剪枝）权重形成双向转发路径。我们提出了一种新颖的动态集体智能学习（DCIL），利用两组权重的集体智能之间的学习协同作用。

    With the growth of deep neural networks (DNN), the number of DNN parameters has drastically increased. This makes DNN models hard to be deployed on resource-limited embedded systems. To alleviate this problem, dynamic pruning methods have emerged, which try to find diverse sparsity patterns during training by utilizing Straight-Through-Estimator (STE) to approximate gradients of pruned weights. STE can help the pruned weights revive in the process of finding dynamic sparsity patterns. However, using these coarse gradients causes training instability and performance degradation owing to the unreliable gradient signal of the STE approximation. In this work, to tackle this issue, we introduce refined gradients to update the pruned weights by forming dual forwarding paths from two sets (pruned and unpruned) of weights. We propose a novel Dynamic Collective Intelligence Learning (DCIL) which makes use of the learning synergy between the collective intelligence of both weight sets. We verify
    
[^154]: 注意力不是答案：纯注意力在深度方面以双指数方式降低等级

    Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth. (arXiv:2103.03404v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.03404](http://arxiv.org/abs/2103.03404)

    这项研究提出了一种新的方法来理解自注意力网络，通过将其输出分解为较小的项，证明自注意力对“标记均衡性”的强归纳偏见，而跳跃连接和多层感知机可以阻止输出的退化。

    

    基于注意力的架构已经在机器学习中变得无处不在，然而我们对其有效性的原因的理解仍然有限。这项工作提出了一种新的理解自注意力网络的方法：我们展示了它们的输出可以分解为较小项的求和，每个项涉及在层间操作的一系列注意力头。利用这种分解，我们证明了自注意力对“标记均衡性”的强归纳偏见。具体地说，没有跳跃连接或多层感知机（MLP），输出会以双指数的方式收敛到一个秩为1的矩阵。另一方面，跳跃连接和MLP可以阻止输出的退化。我们的实验验证了所识别的收敛现象在不同变体的标准Transformer架构上的存在。

    Attention-based architectures have become ubiquitous in machine learning, yet our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms, each involving the operation of a sequence of attention heads across layers. Using this decomposition, we prove that self-attention possesses a strong inductive bias towards "token uniformity". Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the identified convergence phenomena on different variants of standard transformer architectures.
    
[^155]: 无岭回归中双下降峰的普遍性研究

    On the Universality of the Double Descent Peak in Ridgeless Regression. (arXiv:2010.01851v8 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2010.01851](http://arxiv.org/abs/2010.01851)

    我们证明了在无岭线性回归中存在一个双下降峰，无论输入分布的特征映射是确定性的还是随机的，都会导致期望均方泛化误差增加。并且我们的结果适用于广泛的输入分布类。

    

    我们证明了在无岭线性回归中由标签噪声引起的期望均方泛化误差的非渐近非分布相关下界。我们的下界将类似的已知结果推广到超参数化（插值）区域。与大多数前期工作不同，我们的分析适用于具有几乎必然完全秩特征矩阵的广泛输入分布类，这使我们能够覆盖各种确定性或随机特征映射类型。我们的下界是渐近尖锐的，并且意味着在存在标签噪声的情况下，无岭线性回归在任何这些特征映射的插值阈值周围表现不佳。我们详细分析了所施加的假设，并为解析（随机）特征映射提供了理论。利用这个理论，我们可以证明我们的假设对具有（勒贝格）密度的输入分布以及由分析激活函数给出的随机深度神经网络的特征映射成立。

    We prove a non-asymptotic distribution-independent lower bound for the expected mean squared generalization error caused by label noise in ridgeless linear regression. Our lower bound generalizes a similar known result to the overparameterized (interpolating) regime. In contrast to most previous works, our analysis applies to a broad class of input distributions with almost surely full-rank feature matrices, which allows us to cover various types of deterministic or random feature maps. Our lower bound is asymptotically sharp and implies that in the presence of label noise, ridgeless linear regression does not perform well around the interpolation threshold for any of these feature maps. We analyze the imposed assumptions in detail and provide a theory for analytic (random) feature maps. Using this theory, we can show that our assumptions are satisfied for input distributions with a (Lebesgue) density and feature maps given by random deep neural networks with analytic activation functi
    
[^156]: 使用深度学习的集成多时间尺度建模进行太阳辐照度预测

    An Integrated Multi-Time-Scale Modeling for Solar Irradiance Forecasting Using Deep Learning. (arXiv:1905.02616v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1905.02616](http://arxiv.org/abs/1905.02616)

    本研究提出了一种使用深度学习方法进行太阳辐照度预测的统一架构，能够在不同时间尺度上进行预测，并提出了一个框架来将这种方法扩展到每小时预测范围。

    

    针对短期太阳辐照度预测，传统的点预测方法由于太阳能功率的非平稳特性而变得不太有用。由于太阳能的变动性，需要更多的运行备用来确保电网的可靠运行。发电不确定性越大，运行备用需求越大，这将导致运行成本的增加。在这项研究工作中，我们提出了一种使用递归神经网络（RNN）和长短时记忆网络（LSTM）进行多时间尺度预测的统一架构，用于预测每天内的太阳辐照度。本文还提出了一个框架，将这种建模方法扩展到每小时预测范围，从而实现多时间范围的预测，能够预测每小时和每天的太阳辐照度。我们开发了一个端到端的流程来实施提出的架构。

    For short-term solar irradiance forecasting, the traditional point forecasting methods are rendered less useful due to the non-stationary characteristic of solar power. The amount of operating reserves required to maintain reliable operation of the electric grid rises due to the variability of solar energy. The higher the uncertainty in the generation, the greater the operating-reserve requirements, which translates to an increased cost of operation. In this research work, we propose a unified architecture for multi-time-scale predictions for intra-day solar irradiance forecasting using recurrent neural networks (RNN) and long-short-term memory networks (LSTMs). This paper also lays out a framework for extending this modeling approach to intra-hour forecasting horizons thus, making it a multi-time-horizon forecasting approach, capable of predicting intra-hour as well as intra-day solar irradiance. We develop an end-to-end pipeline to effectuate the proposed architecture. The performanc
    
[^157]: 在学习黑盒模型的增加性解释时需要考虑的问题

    Considerations When Learning Additive Explanations for Black-Box Models. (arXiv:1801.08640v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1801.08640](http://arxiv.org/abs/1801.08640)

    本文研究了非增加型模型的全局增加性解释方法，发现不同的解释方法以不同的方式刻画了黑盒模型预测函数中的非增加性成分。尽管精简的解释一般是最准确的增加性解释，但显式建模非增加性成分的树形解释往往更准确。机器学习从业者能够更好地利用增加性解释来完成各种任务。

    

    许多解释黑盒模型的方法，无论是局部还是全局的，都是增加型的。本文研究了非增加型模型的全局增加性解释，重点关注四种解释方法：局部依赖、适应全局环境的Shapley解释、精简的增加性解释和基于梯度的解释。我们展示了不同的解释方法以不同的方式刻画了黑盒模型预测函数中的非增加性成分。我们使用主效应和总效应的概念来锚定增加性解释，并定量评估增加性和非增加性解释。尽管精简的解释一般是最准确的增加性解释，但显式建模非增加性成分的树形解释往往更准确。尽管如此，我们的用户研究表明，机器学习从业者能够更好地利用增加性解释来完成各种任务。

    Many methods to explain black-box models, whether local or global, are additive. In this paper, we study global additive explanations for non-additive models, focusing on four explanation methods: partial dependence, Shapley explanations adapted to a global setting, distilled additive explanations, and gradient-based explanations. We show that different explanation methods characterize non-additive components in a black-box model's prediction function in different ways. We use the concepts of main and total effects to anchor additive explanations, and quantitatively evaluate additive and non-additive explanations. Even though distilled explanations are generally the most accurate additive explanations, non-additive explanations such as tree explanations that explicitly model non-additive components tend to be even more accurate. Despite this, our user study showed that machine learning practitioners were better able to leverage additive explanations for various tasks. These considerati
    

