# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Learning and Verification of Task Structure in Instructional Videos.](http://arxiv.org/abs/2303.13519) | 该论文提出了一种新的预训练视频模型——VideoTaskformer。通过从整体上学习表示来验证视频的正确执行，并预测下一步骤。同时，论文也提出了两个新的检测教学视频中错误的基准。 |
| [^2] | [Three ways to improve feature alignment for open vocabulary detection.](http://arxiv.org/abs/2303.13518) | 该论文提出了三种方法来改善开放式词汇检测中特征对齐的问题，包括增强文本嵌入、修改特征金字塔网络和检测头部、以及采用自学习方法。这些方法可以有效缓解模型在未见类上的性能问题。 |
| [^3] | [Ablating Concepts in Text-to-Image Diffusion Models.](http://arxiv.org/abs/2303.13516) | 本论文提出了一种有效的方法，在不重新训练模型的情况下实现了预训练模型中的概念消融，可以消除文本到图像生成中的版权问题和样本记忆问题。 |
| [^4] | [Persistent Nature: A Generative Model of Unbounded 3D Worlds.](http://arxiv.org/abs/2303.13515) | 本文介绍了一个生成无界3D世界的模型，探索了无条件合成无界自然场景的任务，使得在保持持久的3D世界模型的同时可以进行任意大的摄像机运动。其场景表示包括可扩展的平面场景布局网格以及全景天空穹顶，并仅从单视图互联网照片中学习生成世界模型。该模型可模拟在3D景观中长时间的飞行，同时支持一个持续的、摄像机独立的世界表示，具有高效的计算性能。 |
| [^5] | [Neural Preset for Color Style Transfer.](http://arxiv.org/abs/2303.13511) | 本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。 |
| [^6] | [The Quantization Model of Neural Scaling.](http://arxiv.org/abs/2303.13506) | 该论文提出了神经网络缩放的量化模型，通过将神经网络学习到的功能分为量子并对量子进行递减使用频率的顺序学习，可以解释损失缩放定律，并自动发现语言模型的多样能力。 |
| [^7] | [Chordal Averaging on Flag Manifolds and Its Applications.](http://arxiv.org/abs/2303.13501) | 本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。 |
| [^8] | [A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias.](http://arxiv.org/abs/2303.13500) | 研究对深度神经网络过度拟合数据中简单模式的倾向的易感性，提出了一种受控简单性正则化（CSR）方法来限制过度拟合，完成对模型的更深入研究，提高了模型的泛化能力和安全性。 |
| [^9] | [The effectiveness of MAE pre-pretraining for billion-scale pretraining.](http://arxiv.org/abs/2303.13496) | 本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。 |
| [^10] | [Boosting Reinforcement Learning and Planning with Demonstrations: A Survey.](http://arxiv.org/abs/2303.13489) | 强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。 |
| [^11] | [TactoFind: A Tactile Only System for Object Retrieval.](http://arxiv.org/abs/2303.13482) | TactoFind是一个纯触觉物品获取系统，它可以使用手指上的触控传感器，在没有任何视觉反馈的情况下定位、识别和抓取新的物体。 |
| [^12] | [Generalization with quantum geometry for learning unitaries.](http://arxiv.org/abs/2303.13462) | 本文研究了量子机器学习模型的泛化能力，使用数据的量子费舍尔信息度量来评估成功训练和泛化所需的电路参数和训练数据的数量，并展示通过去除对称性来提高泛化能力，同时发现超出分布泛化能力可以比使用相同分布更优。 |
| [^13] | [Optimization Dynamics of Equivariant and Augmented Neural Networks.](http://arxiv.org/abs/2303.13458) | 本论文研究了在对称数据上优化多层感知机的方法，比较了等变和增强两种策略的优缺点，证明了在自然假设下等变稳定点的集合和等变层的集合具有不变性，但增强模型的稳定点可能是不稳定的。 |
| [^14] | [Human Behavior in the Time of COVID-19: Learning from Big Data.](http://arxiv.org/abs/2303.13452) | 本研究综述了使用大数据技术研究COVID-19背景下人类行为的现有研究，分为使用大数据测量、模拟和干预人类行为三类研究。 |
| [^15] | [Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes.](http://arxiv.org/abs/2303.13450) | 该论文提出了一个全局-局部训练框架，使用对象代理合成 3D 场景，将每个对象表示为独立的 NeRF，并交替优化，从而实现对每个对象的完整表示的学习。 |
| [^16] | [GiveMeLabeledIssues: An Open Source Issue Recommendation System.](http://arxiv.org/abs/2303.13418) | GiveMeLabeledIssues是一个开源的任务推荐系统，利用API领域代理技能，从而可以帮助开发者更好地匹配任务，并减轻项目维护者的负担。在预测API领域时，该工具的精度达到了83.9%。 |
| [^17] | [Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense.](http://arxiv.org/abs/2303.13408) | 本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。 |
| [^18] | [Adaptive Endpointing with Deep Contextual Multi-armed Bandits.](http://arxiv.org/abs/2303.13407) | 在线学习的自适应端点检测方法，使用深度上下文多臂赌博机，避免使用昂贵的网格搜索，不需要真值标签，并成功减少了早期截止错误。 |
| [^19] | [Optimization and Optimizers for Adversarial Robustness.](http://arxiv.org/abs/2303.13401) | 本论文提出了一种新的算法框架，将通用的约束优化求解器与综合约束折叠结合起来，以提高深度学习模型的对抗性鲁棒性评估的可靠性和广泛性。 |
| [^20] | [Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis.](http://arxiv.org/abs/2303.13391) | Xplainer是一个透明且可解释的零样本诊断新框架，通过对存在的描述性观察进行分类来提高自动诊断集成到临床工作流程中的效率，同时避免需要大量注释数据的问题。 |
| [^21] | [Compositional Zero-Shot Domain Transfer with Text-to-Text Models.](http://arxiv.org/abs/2303.13386) | 提出了一种组合转移学习框架，用于专业领域中的零样本领域转移，使用未标记的领域自由文本进行领域和任务知识的共同学习，通过 NLGU 策略实现领域数据增强和标签预测，经实验证明在生物医学领域和放射学子领域具有优异的性能，胜过了现有的 SOTA。 |
| [^22] | [Adversarial Robustness of Learning-based Static Malware Classifiers.](http://arxiv.org/abs/2303.13372) | 本文提出了一种通用的对抗性补丁（UAP）攻击方法，只需附加一个相对较小的字节级补丁即可绕过MalConv分类器的检测，可以将恶意软件文件的检测率降低80％。同时，作者提出了窗口消除处理作为应对此种攻击的一种方法。 |
| [^23] | [Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review.](http://arxiv.org/abs/2303.13365) | 本文调查了自然语言处理和机器学习在需求规范化中的应用现状，发现启发式NLP方法是自动RF中最常用的NLP技术，机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。 |
| [^24] | [Reevaluating Data Partitioning for Emotion Detection in EmoWOZ.](http://arxiv.org/abs/2303.13364) | 本文重新评估了EmoWOZ数据集的数据划分，在此基础上提出了一种新的分层抽样方法用于处理高度不平衡和不均匀分布的情感标签。使用这个方法建立在EmoWoz上的模型表现更好，未来研究者应该采取这种划分以确保一致和准确的性能评估。 |
| [^25] | [FS-Real: Towards Real-World Cross-Device Federated Learning.](http://arxiv.org/abs/2303.13363) | 本文针对联邦学习中异构设备和规模的挑战，提出一种高效可扩展的原型系统，以支持跨设备联邦学习，从而弥补学术界和工业界间在联邦学习研究中的差距。 |
| [^26] | [Towards the Scalable Evaluation of Cooperativeness in Language Models.](http://arxiv.org/abs/2303.13360) | 本论文旨在对基于语言模型的合作性评估的可扩展性进行研究，通过生成特定博弈论结构场景并进行评估，不过目前生成质量较一般。 |
| [^27] | [Increasing Textual Context Size Boosts Medical Image-Text Matching.](http://arxiv.org/abs/2303.13340) | 利用滑动窗口技术增加了文本上下文的编码，提高了医疗图像-文本匹配的准确性，新模型ClipMD在两个数据集上都取得了最好的结果。 |
| [^28] | [Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI.](http://arxiv.org/abs/2303.13336) | 此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。 |
| [^29] | [Decentralized Adversarial Training over Graphs.](http://arxiv.org/abs/2303.13326) | 本文研究了在图上的去中心化对抗性训练，利用扩散学习的方法，开发了一种对抗性训练框架，增强了多个代理的鲁棒性以对抗攻击。 |
| [^30] | [Deep Generative Multi-Agent Imitation Model as a Computational Benchmark for Evaluating Human Performance in Complex Interactive Tasks: A Case Study in Football.](http://arxiv.org/abs/2303.13323) | 该论文使用基于生成模型的 AI 智能体作为计算基准，以评估人类在困难的涉及多个人类和情境因素的任务中的表现及发现有待改进的领域。该论文以足球表现分析为例，使用大型球员和球位置跟踪数据集训练生成模型，并成功进行足球比赛中的交互模仿。 |
| [^31] | [Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective.](http://arxiv.org/abs/2303.13299) | 本文提出针对后续特征归因方法所存在的不同解释的问题，引入PEAR损失项，从而提升模型的解释一致性，达到模型行为的可理解和可信任。 |
| [^32] | [Improving Generalization with Domain Convex Game.](http://arxiv.org/abs/2303.13297) | 本文提出了一种新的域泛化视角，将其重新解释为域之间的凸博弈，并通过鼓励每个多样化的域增强模型泛化和构建样本过滤器来提高域增强的有效性。 |
| [^33] | [Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees.](http://arxiv.org/abs/2303.13228) | 本文提出了一种算法来丰富神经网络训练数据集，从而减少最坏情况的违规，并提高其性能保证。 |
| [^34] | [Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs.](http://arxiv.org/abs/2303.13211) | 本文研究了DNN对清洁样本和毒化样本的频率敏感性差异，提出了一种简单而有效的FREAK基于频率的毒化样本检测算法，可有效防御频率后门攻击和一些空间攻击。 |
| [^35] | [Take 5: Interpretable Image Classification with a Handful of Features.](http://arxiv.org/abs/2303.13166) | 人们常常无法理解深度学习模型的决策过程，我们提出了一种Sparse Low-Dimensional Decision模型，它只使用一小部分可解释的特征进行决策，这使得该模型更容易被人理解，同时也具有与其他密集高维模型相似的准确性。 |
| [^36] | [Adiabatic replay for continual learning.](http://arxiv.org/abs/2303.13157) | 本研究提出了一种称为绝热重放的重放连续学习策略，它能够有选择性地重放与新数据相似的样本，从而提高学习效率。 |
| [^37] | [FedGH: Heterogeneous Federated Learning with Generalized Global Header.](http://arxiv.org/abs/2303.13137) | FedGH是一种异构联邦学习方法，可以使客户端持有具有不同结构的模型，通过训练共享的广义全局预测头来提高效率和性能。 |
| [^38] | [Laplacian Segmentation Networks: Improved Epistemic Uncertainty from Spatial Aleatoric Uncertainty.](http://arxiv.org/abs/2303.13123) | 所提出的拉普拉斯分割网络可同时捕获图像分割中的认知和随机不确定性，成功将高认知不确定性分配到OOF目标中。 |
| [^39] | [RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation Research.](http://arxiv.org/abs/2303.13117) | 本文提出了灵活的深度强化学习框架RLOR，能够用于各种运筹学问题。我们重新实现了车辆路径问题的自回归模型，并展示了这些模型可以从强化学习的最新进展中受益，同时也提高了训练速度。 |
| [^40] | [Adaptive Regularization for Class-Incremental Learning.](http://arxiv.org/abs/2303.13113) | 本文研究了适应性正则化在类增量学习中的应用，通过根据任务复杂度动态调整正则化强度，在学习新类别同时防止遗忘之前学习的类别。实验表明适应性正则化可以实现更加准确和不易遗忘的视觉增量学习。 |
| [^41] | [Keypoint-Guided Optimal Transport.](http://arxiv.org/abs/2303.13102) | 本文提出了一种关键点引导的最优输运模型，通过掩模约束和关键点的关系指导匹配，并可用Sinkhorn算法求解。 |
| [^42] | [The Probabilistic Stability of Stochastic Gradient Descent.](http://arxiv.org/abs/2303.13093) | 本文重新定义了随机梯度下降的稳定性，并使用概率稳定性来回答深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。 |
| [^43] | [Box-Level Active Detection.](http://arxiv.org/abs/2303.13089) | 该论文提出了一种基于边界框的主动学习目标检测方法，引入了一个控制每个周期基于边界框预算的框架 ComPAS，它可以优先考虑有信息量的目标，并避免冗余以进行公平比较和高效应用。 |
| [^44] | [MSAT: Biologically Inspired Multi-Stage Adaptive Threshold for Conversion of Spiking Neural Networks.](http://arxiv.org/abs/2303.13080) | 该论文提出了一种生物启发的多阶段自适应阈值方法用于转化人工神经网络为脉冲神经网络（SNN），该方法可以使得神经元更快地传递脉冲并传输更多信息，在精度和效率方面均优于现有的最新方法。 |
| [^45] | [Predicting the Initial Conditions of the Universe using Deep Learning.](http://arxiv.org/abs/2303.13056) | 本文首次证明了使用深度学习模型可以反向预测宇宙初始线性位移，该方法能够在减少计算量的同时准确恢复初始线性位移。 |
| [^46] | [Reimagining Application User Interface (UI) Design using Deep Learning Methods: Challenges and Opportunities.](http://arxiv.org/abs/2303.13055) | 深度学习方法在用户界面设计中的最新研究进展，将推动软件开发行业进步的高潜力领域。 |
| [^47] | [Towards Better Dynamic Graph Learning: New Architecture and Unified Library.](http://arxiv.org/abs/2303.13047) | 我们提出了一种基于Transformer的新型动态图学习架构DyGFormer，并引入了统一的库DyGLib，以促进可重复、可扩展和可信的动态图学习研究。DyGFormer通过邻居共现编码方案和分块技术实现更长期历史的高效推理，在13个不同领域的数据集上实现了最先进的性能。 |
| [^48] | [SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization.](http://arxiv.org/abs/2303.13035) | 研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型. |
| [^49] | [Preference-Aware Constrained Multi-Objective Bayesian Optimization.](http://arxiv.org/abs/2303.13034) | PAC-MOO是一个偏好感知的约束多目标贝叶斯优化方法，可以有效地解决在黑盒目标函数和从业者指定的目标偏好下，大部分输入空间是不可行的约束多目标优化问题。 |
| [^50] | [Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States.](http://arxiv.org/abs/2303.13024) | 这篇论文提出了一种新的自监督聚类算法，能够在多元时间序列数据中确定并识别对于TBI等急性疾病治疗非常重要的生理状态。研究还利用临床数据验证并解释所识别的生理状态。 |
| [^51] | [ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting.](http://arxiv.org/abs/2303.13022) | 本文介绍了一种名为ENVIDR的渲染和建模框架，在渲染具有挑战性的镜面反射表面方面表现出卓越性能，集成了神经渲染器和基于SDF的神经表面模型。 |
| [^52] | [Failure-tolerant Distributed Learning for Anomaly Detection in Wireless Networks.](http://arxiv.org/abs/2303.13015) | 本文提出了一种名为“Tol-FL”的新方法，通过结合扁平和星型拓扑结构的优势，增强了分布式网络中的变异检测性能和可靠性。 |
| [^53] | [Semantic Image Attack for Visual Model Diagnosis.](http://arxiv.org/abs/2303.13010) | 本文提出了一种新的基于对抗攻击的方法——语义图像攻击（SIA），可以提供语义对抗图像以便进行模型诊断、可解释性和鲁棒性。 |
| [^54] | [Controllable Inversion of Black-Box Face-Recognition Models via Diffusion.](http://arxiv.org/abs/2303.13006) | ID3PM方法通过扩散过程的随机性质来产生黑盒人脸识别模型的高度可控的反演，能够生成逼真且多样的输出，适用于数据增强、对抗性攻击和生成个性化的训练数据等多种应用。 |
| [^55] | [Adversarially Contrastive Estimation of Conditional Neural Processes.](http://arxiv.org/abs/2303.13004) | 本文提出了一种对抗性训练方案，通过噪声对比估计共同训练CNPs和EBM，并在各种数据集上改进了表现。 |
| [^56] | [Benchmarking the Reliability of Post-training Quantization: a Particular Focus on Worst-case Performance.](http://arxiv.org/abs/2303.13003) | 本论文探讨了后训练量化方法在极端情况下的可靠性问题，并在常用的PTQ方法上开展了系统评估。结果表明，大多数现有的PTQ方法在最劣情况下的性能表现不够可靠。因此需要开发更加强大的PTQ方法，以有效处理分布偏移和数据噪声，并改善最劣情况下的性能表现。 |
| [^57] | [Automated Federated Learning in Mobile Edge Networks -- Fast Adaptation and Convergence.](http://arxiv.org/abs/2303.12999) | 本文使用基于MAML的FL设计来最小化整体学习时间，优化FL超参数（例如采样数据大小和通信轮数）和资源分配（例如发送功率），同时考虑模型精度和能量消耗的限制。 |
| [^58] | [A Survey of Historical Learning: Learning Models with Learning History.](http://arxiv.org/abs/2303.12992) | 本文综述了“历史学习：带有学习历史的学习模型”这个主题，涵盖历史类型、功能部分和存储形式三个方面，是首个系统研究利用各种历史统计数据进行深度神经网络训练的综述论文。 |
| [^59] | [Continuous Indeterminate Probability Neural Network.](http://arxiv.org/abs/2303.12964) | 本文提出了CIPNN模型，该模型能够推导出连续潜在随机变量的解析解，同时提出了CIPAE自编码器，并通过可视化潜在随机变量的方法验证了模型的有效性。 |
| [^60] | [Forecast-Aware Model Driven LSTM.](http://arxiv.org/abs/2303.12963) | 本文旨在提出一种预测感知的模型驱动LSTM来解决空气质量预测中常见的偏差修正问题，此方法可以更好地处理极端空气质量事件。 |
| [^61] | [The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs.](http://arxiv.org/abs/2303.12961) | 本论文回顾了超过80个在非成像 EMR 数据上训练的基础模型，发现这些模型大多范围有限、训练集有限，且评估指标未对其对医疗系统贡献提供有意义见解。因此，本研究提出了一种更接近于医疗保健重要指标的医疗基础模型效益评估框架。 |
| [^62] | [Variantional autoencoder with decremental information bottleneck for disentanglement.](http://arxiv.org/abs/2303.12959) | 本论文提出了一种逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来平衡去纠缠和重构保真度，避免信息扩散问题。 |
| [^63] | [Reinforcement Learning with Exogenous States and Rewards.](http://arxiv.org/abs/2303.12957) | 该研究提出了一种强化学习的方法，通过将MDP分解为外生和内生两个部分，优化内生奖励，在状态空间的内生和外生状态空间没有事先给出的情况下，提出了正确的算法进行自动发现。 |
| [^64] | [TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional Cycle-Consistent Generative Adversarial Networks.](http://arxiv.org/abs/2303.12952) | TSI-GAN是一种无监督时间序列异常检测模型，它可以自动学习时间序列中的复杂模式，并具有良好的泛化性能。 |
| [^65] | [FTSO: Effective NAS via First Topology Second Operator.](http://arxiv.org/abs/2303.12948) | 本文提出了 FTSO 方法，将整个架构搜索分为两个子步骤：拓扑搜索和算子搜索。FTSO方法大大降低了 NAS 搜索的时间，同时提高了搜索到的架构准确性，并在ImageNet和CIFAR10数据集上达到了优秀的准确率。 |
| [^66] | [Deep Attention Recognition for Attack Identification in 5G UAV scenarios: Novel Architecture and End-to-End Evaluation.](http://arxiv.org/abs/2303.12947) | 该研究提出了在5G无人机场景中基于深度注意力识别的攻击识别解决方案。该方法使用SINR和RSSI参数识别直射视线（LoS）、非直射视线（NLoS）以及两种条件的概率性组合下的攻击。 |
| [^67] | [Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems.](http://arxiv.org/abs/2303.12928) | 本文从理论上将特定优化问题与多时间Hamilton-Jacobi PDEs联系起来，表明当解决这些问题时，同时解决了对应的多时间HJ PDEs和最优控制问题。利用这种联系，提出了一种新的算法，实现了深度神经网络泛化性能的改进。 |
| [^68] | [Revisiting the Fragility of Influence Functions.](http://arxiv.org/abs/2303.12922) | 本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。 |
| [^69] | [Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization.](http://arxiv.org/abs/2303.12921) | 本文研究了可复制算法与标准算法稳定性的联系，为一类统计问题提供了可复制算法的样本有效算法约化，同时表明这种等价关系必须在计算上崩溃。 |
| [^70] | [Self-distillation for surgical action recognition.](http://arxiv.org/abs/2303.12915) | 本文首次将自我蒸馏的概念引入到手术视频分析中，提出了一种异构集成方法，其使用Swine Transfomers作为骨干网络，并将自我蒸馏和多任务学习应用于模型设计中。在类别不平衡和潜在标签不明确的情况下，软标签通过自我蒸馏的方式获得是性能提升最大的因素。 |
| [^71] | [TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics.](http://arxiv.org/abs/2303.12914) | TRON是一种基于硅光子学的神经网络加速器，能够比同类Transformer加速器高14倍的吞吐量和8倍的能效。 |
| [^72] | [Cross-Layer Design for AI Acceleration with Non-Coherent Optical Computing.](http://arxiv.org/abs/2303.12910) | 本文介绍了如何使用交叉层设计来克服非相干光计算平台中存在的挑战，适应多种类型的AI工作负载，从而实现高速的AI加速。 |
| [^73] | [Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems.](http://arxiv.org/abs/2303.12891) | 本文对于用于网络安全入侵检测系统的三种特征降维方法进行了比较，结果表明使用蝙蝠算法的相关特征选择（CFS-BA）是最为高效的，仅用最佳随机森林信息增益（RF-IG）模型55%的时间构建，同时实现了99.99%的准确性。 |
| [^74] | [A dynamic risk score for early prediction of cardiogenic shock using machine learning.](http://arxiv.org/abs/2303.12888) | 该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。 |
| [^75] | [Robust Consensus in Ranking Data Analysis: Definitions, Properties and Computational Issues.](http://arxiv.org/abs/2303.12878) | 该论文介绍了排序数据分析中的鲁棒性共识问题以及该问题相关的统计方法，其中Consensus Ranking问题是重点，旨在通过中位数排名来总结排列的概率分布。 |
| [^76] | [Human Uncertainty in Concept-Based AI Systems.](http://arxiv.org/abs/2303.12872) | 本研究探讨了人类不确定性对概念驱动AI系统的影响，通过控制数据集的干扰因素，分析了现有模型的处理方法。 |
| [^77] | [NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions.](http://arxiv.org/abs/2303.12865) | 本文提出了一种基于神经辐射场（NeRF）和生成对抗网络（GAN）的新方法，通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，实现了基于姿态的2D GAN的高效3D感知生成。 |
| [^78] | [Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data.](http://arxiv.org/abs/2303.12861) | 本文提出了一种基于立方体的3D去噪扩散概率模型（DDPM）来重建CBCT并解决了存储整个正弦图的内存问题。通过将整个CBCT volume分成多个小立方体，该模型能够实现高效的计算并在视觉和定量评价方面优于现有方法。 |
| [^79] | [Anti-symmetric Barron functions and their approximation with sums of determinants.](http://arxiv.org/abs/2303.12856) | 用行列式和有效地逼近反对称巴龙函数，从而获得了阶乘级的复杂度提升和在从头算量子化学中的有效性。 |
| [^80] | [Three iterations of $(1-d)$-WL test distinguish non isometric clouds of $d$-dimensional points.](http://arxiv.org/abs/2303.12853) | 本文研究了WL测试在点云中的应用，结果发现三次迭代的$(d-1)$-WL测试可以区分$d$维欧几里得空间中的点云，且只需要一次迭代的$d$-WL测试就可以达到完整性。 |
| [^81] | [Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder.](http://arxiv.org/abs/2303.12848) | 该方法使用遮蔽自编码器进行对抗攻击检测和重构，不需要在测试时间更新模型权重，也不需要使用更多的对抗样本来增强训练集。 |
| [^82] | [The power and limitations of learning quantum dynamics incoherently.](http://arxiv.org/abs/2303.12834) | 本文证明在不相干框架下，通过浅层测量只能有效地学习低纠缠门。虽然该类框架为我们在不同物理平台之间转移量子过程提供了方法，但也存在一定的局限性。 |
| [^83] | [Co-Speech Gesture Synthesis using Discrete Gesture Token Learning.](http://arxiv.org/abs/2303.12822) | 该论文提出了一个两阶段的机制，使用离散的编码方式来解决合成共性语言手势中的不确定性问题，采用VAE和自回归变压器模型进行学习，能够生成多样化和逼真的共性语言手势。 |
| [^84] | [Towards A Visual Programming Tool to Create Deep Learning Models.](http://arxiv.org/abs/2303.12821) | DeepBlocks是一款可视化编程工具，允许DL开发人员设计、训练和评估模型，而无需依赖特定的编程语言。其通过构建典型模型结构实现其工作原理，结果表明开发人员可以视觉上设计复杂的DL架构。 |
| [^85] | [An Empirical Analysis of the Shift and Scale Parameters in BatchNorm.](http://arxiv.org/abs/2303.12818) | 本文通过实验比较重新参数化步骤与归一化步骤对BatchNorm成功的贡献，以研究BatchNorm中Shift和Scale参数的作用。 |
| [^86] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^87] | [Fixed points of arbitrarily deep 1-dimensional neural networks.](http://arxiv.org/abs/2303.12814) | 本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。 |
| [^88] | [A Comparison of Graph Neural Networks for Malware Classification.](http://arxiv.org/abs/2303.12812) | 本研究将恶意软件分类视为图分类问题，并基于本地度量剖面特征，使用多种图神经网络（GNN）架构进行了训练。研究表明，我们的最佳GNN模型优于以前的可比研究，且不会遭受过度拟合问题。 |
| [^89] | [SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication System.](http://arxiv.org/abs/2303.12811) | SignCRF是一个可扩展的无频道数据驱动射频认证平台，能够高精度地识别无线设备，不受移动性带来的动态信道影响。 |
| [^90] | [Granular-ball Optimization Algorithm.](http://arxiv.org/abs/2303.12807) | 粒球优化算法(GBO)是一种新的多粒度优化算法，可以通过引入粒球计算来提高全局搜索能力和收敛速度，实验结果表明，在这些方面它比现有的最先进的算法表现更优。 |
| [^91] | [Dermatologist-like explainable AI enhances trust and confidence in diagnosing melanoma.](http://arxiv.org/abs/2303.12806) | 研究开发了一种可解释人工智能系统，其能够对黑色素瘤和痣进行诊断，并能够给出易于解释的文本和区域解释，该系统能显著提高临床医生的准确性、信心和对其XAI支持的信任。 |
| [^92] | [Features matching using natural language processing.](http://arxiv.org/abs/2303.12804) | 本文提出了一种使用自然语言处理进行特征匹配的新混合模型，它可以减少匹配不同数据集所需的时间。 |
| [^93] | [Distributed Learning Meets 6G: A Communication and Computing Perspective.](http://arxiv.org/abs/2303.12802) | 本文探讨了基于分布式学习和FL策略的方法如何帮助实现6G网络下的严格KPI要求，同时强调了将DL方法应用于6G网络时所面临的挑战。 |
| [^94] | [A Data Augmentation Method and the Embedding Mechanism for Detection and Classification of Pulmonary Nodules on Small Samples.](http://arxiv.org/abs/2303.12801) | 该论文提出了一种新的数据增强方法和嵌入机制，可以提高深度学习模型的准确性和鲁棒性，从而实现对小样本肺结节的检测和分类。 |
| [^95] | [IoT Device Identification Based on Network Communication Analysis Using Deep Learning.](http://arxiv.org/abs/2303.12800) | 内部网络中允许连接的IoT设备和未知的IoT设备的识别变得越发重要。本研究提出了一种基于深度学习的自动识别方法，可以不需对网络通信进行复杂的特征处理。 |
| [^96] | [Time Series as Images: Vision Transformer for Irregularly Sampled Time Series.](http://arxiv.org/abs/2303.12799) | 本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。 |
| [^97] | [Interpersonal Distance Tracking with mmWave Radar and IMUs.](http://arxiv.org/abs/2303.12798) | 本文介绍了ImmTrack，它使用毫米波雷达和惯性测量单元数据来跟踪人际距离，并可以将惯性数据转移到雷达感测结果中。在更广泛的意义上，设备是第一个融合毫米波雷达和惯性测量单元数据用于同时用户跟踪和重新识别的系统。 |
| [^98] | [An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters.](http://arxiv.org/abs/2303.12797) | 本文提出一种基于进化的有向无环图的算法框架，用于自动生成高效且灵活的深度神经网络并优化相关的超参数。此框架可用于任何能够处理混合搜索空间的元启发式算法，并在时间序列预测数据集上表现出比已有模型更好的性能。 |
| [^99] | [An Analysis of Abstractive Text Summarization Using Pre-trained Models.](http://arxiv.org/abs/2303.12796) | 本文对使用预训练模型进行文本摘要的方法进行了评估，并在不同数据集上进行了比较，结果表明...... |
| [^100] | [Named Entity Recognition Based Automatic Generation of Research Highlights.](http://arxiv.org/abs/2303.12795) | 该研究使用命名实体识别技术自动生成研究亮点，探究其是否能提高生成亮点的质量。实验结果表明，增加命名实体信息可以提高生成亮点的性能。 |
| [^101] | [Lower Bound on the Bayesian Risk via Information Measure.](http://arxiv.org/abs/2303.12497) | 新提出一种方法计算贝叶斯风险下界，允许使用几乎任何信息度量，能提供与估计器无关的不可能结果。已应用于离散和连续参数问题，与最先进的技术进行了比较。 |
| [^102] | [Delay-Aware Hierarchical Federated Learning.](http://arxiv.org/abs/2303.12414) | 本论文提出了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率，并实现了一些政策以减少能量消耗和边缘到云端的通信。 |
| [^103] | [Random Inverse Problems Over Graphs: Decentralized Online Learning.](http://arxiv.org/abs/2303.11789) | 本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。 |
| [^104] | [CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning.](http://arxiv.org/abs/2303.10365) | CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。 |
| [^105] | [Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators.](http://arxiv.org/abs/2303.08431) | 本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。 |
| [^106] | [Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for Dynamic MRI.](http://arxiv.org/abs/2303.07150) | 本研究基于深度学习技术，通过学习采集轨迹来提高动态MRI的图像重建质量。 |
| [^107] | [Improved Regret Bounds for Online Kernel Selection under Bandit Feedback.](http://arxiv.org/abs/2303.05018) | 本文研究了在线内核选择的带有Bandit反馈的遗憾界限，提出了两种改进的界限。其中一种适用于光滑的损失函数，另一种则适用于Lipschitz损失函数，预期界限分别为$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$和$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$，并将其应用于具有时间约束的在线内核选择中。 |
| [^108] | [Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence.](http://arxiv.org/abs/2303.02829) | 本文介绍了归属分数和因果反事实在解释人工智能中的应用，重点关注因果关系领域中的逻辑推理和分数计算。 |
| [^109] | [Containing a spread through sequential learning: to exploit or to explore?.](http://arxiv.org/abs/2303.00141) | 本文提出了一种通过序贯学习来控制传播的测试和隔离策略，以最小化累积感染人数；可以通过贪心选择节点进行测试并具有性能保证，并设计了基于奖励的方法，在大型网络中具有更好的可计算性。 |
| [^110] | [Predicting the performance of hybrid ventilation in buildings using a multivariate attention-based biLSTM Encoder-Decoder neural network.](http://arxiv.org/abs/2302.04126) | 本文研究了一种基于多变量关注机制的双向LSTM编码器-解码器神经网络的能力，用于预测混合通风建筑内窗户开启或关闭时的室内空气温度。 |
| [^111] | [Real-Time Evaluation in Online Continual Learning: A New Hope.](http://arxiv.org/abs/2302.01047) | 该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。 |
| [^112] | [Improving Monte Carlo Evaluation with Offline Data.](http://arxiv.org/abs/2301.13734) | 本论文介绍了通过使用离线数据来提升蒙特卡罗评估方法，实现在保持相同估计准确度的前提下，减少在线样本数量的目的。通过使用一个定制的行为策略，可以比普通的 MC 估计器产生更小的方差。该行为策略可以从现有的离线数据中高效学习，我们的实验表明，相对于现有的最先进方法，我们的方法只需使用小部分在线样本就能实现相同的估计精度。 |
| [^113] | [Interpreting learning in biological neural networks as zero-order optimization method.](http://arxiv.org/abs/2301.11777) | 本文将大脑视为一种用于有监督学习的统计方法，将生物神经网络中的连接参数的本地更新规则与零阶优化方法相关联，并表明期望值实现了梯度下降的修改版。 |
| [^114] | [On the Convergence of No-Regret Learning Dynamics in Time-Varying Games.](http://arxiv.org/abs/2301.11241) | 本文研究了时变博弈中乐观梯度下降法（OGD）的收敛性，提出了明确的收敛界限，并建立了适用于时变总和多人博弈和元学习的新型双线性公式。 |
| [^115] | [Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization.](http://arxiv.org/abs/2301.07784) | 该论文提出了一种使用广义策略改进优先级来实现高效多目标学习的算法，从而通过主动学习策略，可以识别出每一时刻最有前途的偏好或目标，以更快地解决MORL问题，同时也可以识别出学习特定代理偏好的策略时最相关的历史经验。 |
| [^116] | [CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image.](http://arxiv.org/abs/2301.02232) | 本文介绍了一种可以将单张图像中物体的运动转移到未调整的3D模型中的神经网络方法，可以处理任意类别的对象，训练时只使用合成数据。 |
| [^117] | [Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples.](http://arxiv.org/abs/2301.01217) | 本文提出了一种更实用的标签不可知设置，以生成不可学习的样本，防止未经授权的机器学习模型训练。 |
| [^118] | [Self-Supervised Object Segmentation with a Cut-and-Pasting GAN.](http://arxiv.org/abs/2301.00366) | 提出了一种基于自监督的 Cut-and-Paste GAN，用于前景对象分割和组合图像生成，无需手动标注，通过学习全局数据表示和语义结构信息，实现了有意义的遮罩生成。 |
| [^119] | [HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for Highly Accurate Protein-Ligand Binding Affinity Prediction.](http://arxiv.org/abs/2212.12440) | 本文提出了一种新的深度学习架构HAC-Net，结合了三维卷积神经网络和两个图卷积网络的注意力机制，能够高精度地预测蛋白质与配体的结合亲和力，并在公认的基准测试集上获得最先进的结果。 |
| [^120] | [Learning Subgrid-scale Models with Neural Ordinary Differential Equations.](http://arxiv.org/abs/2212.09967) | 本文提出了利用神经常微分方程学习亚网格尺度模型的新方法，可以提高计算流体动力学求解器的准确性和效率。该方法具有NODE优点，可以参数化亚网格尺度、近似耦合算子，并提高低阶求解器的效率。 |
| [^121] | [Sliced Optimal Partial Transport.](http://arxiv.org/abs/2212.08049) | 本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。 |
| [^122] | [Spatially Selective Deep Non-linear Filters for Speaker Extraction.](http://arxiv.org/abs/2211.02420) | 本文提出基于空间选择的深度非线性滤波器，采用简单且有效的条件机制，可以在任意目标方向对其进行导向。该滤波器可以用于多人语音分离，实现非常准确的多说话者定位。 |
| [^123] | [A robust estimator of mutual information for deep learning interpretability.](http://arxiv.org/abs/2211.00024) | 该论文提出了一种鲁棒的互信息（MI）估计器，名为GMM-MI，它可以用于解释深度学习模型内部的工作机制，并在表示学习的情境下进行了验证。 |
| [^124] | [Symmetries, flat minima, and the conserved quantities of gradient flow.](http://arxiv.org/abs/2210.17216) | 该论文发现了一种通用框架，可以在参数空间中寻找连续对称性的方法，这种对称性可以雕刻出低损坏山谷。论文提出了一组新的非线性数据相关对称性，用于将训练好的模型变形，提高对某些对抗性攻击的鲁棒性并发现了梯度流的平坦极小值偏差问题，提出了一种改善梯度流寻找良好解的能力的方法。 |
| [^125] | [Multi-lingual Evaluation of Code Generation Models.](http://arxiv.org/abs/2210.14868) | 本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。 |
| [^126] | [Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets.](http://arxiv.org/abs/2210.14064) | 本文通过对超参数化线性RNN的权重假设，发现GD可以学习低维度状态空间，捕获长期动态。 |
| [^127] | [Convex and Nonconvex Sublinear Regression with Application to Data-driven Learning of Reach Sets.](http://arxiv.org/abs/2210.01919) | 本文提出了使用支撑函数对紧致集合进行学习的方法，并提出了两种算法进行子线性回归，分别为凸规划和非凸规划。本文在受控动态到达集的应用中进行了实验。 |
| [^128] | [Omnigrok: Grokking Beyond Algorithmic Data.](http://arxiv.org/abs/2210.01117) | 本文通过分析神经网络的损失景观，发现训练和测试损失之间的不匹配是grokking的原因，提出了“LU机制”，并成功诱导了算法数据集的grokking和消除了其grokking现象。它们的dramatic grokking依赖于表示学习。 |
| [^129] | [Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids.](http://arxiv.org/abs/2209.12693) | 本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。 |
| [^130] | [GP-net: Flexible Viewpoint Grasp Proposal.](http://arxiv.org/abs/2209.10404) | GP-net 可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取，在真实世界的实验中，它实现了 51.8% 的抓取成功率，相比之下，机器人抓取技术的最新方法成功率更低，需要定义工作空间。 |
| [^131] | [Normalizing Flows for Interventional Density Estimation.](http://arxiv.org/abs/2209.06203) | 本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。 |
| [^132] | [Diffusion Models in Vision: A Survey.](http://arxiv.org/abs/2209.04747) | 扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。 |
| [^133] | [Diffusion Models: A Comprehensive Survey of Methods and Applications.](http://arxiv.org/abs/2209.00796) | 本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。 |
| [^134] | [Convex mixed-integer optimization with Frank-Wolfe methods.](http://arxiv.org/abs/2208.11010) | 该论文提出了一种新的凸混合整数优化方法，它使用Frank-Wolfe算法在混合整数可行点的凸包上求解，以解决混合整数非线性优化问题。 |
| [^135] | [POCS-based Clustering Algorithm.](http://arxiv.org/abs/2208.08888) | 本文提出了一种基于POCS方法的聚类算法，该算法利用并行投影方法在特征空间中找到适当的聚类原型，实验结果表明在聚类误差和执行速度方面与传统聚类方法相比优势明显。 |
| [^136] | [Langevin Diffusion Variational Inference.](http://arxiv.org/abs/2208.07743) | 本论文提出了一种统一的分析方法以概括并改进现有的基于未调整Langevin转移的强大变分分布构建方法，同时提出了一种新方法，在基准测试中表现更好。 |
| [^137] | [A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases.](http://arxiv.org/abs/2208.05845) | 本研究通过提供包含47种不同属性注释的大规模数据集，并对三种最先进的Deepfake检测模型进行全面分析，旨在研究公共Deepfake数据集可能带来的AI偏差问题 |
| [^138] | [Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework.](http://arxiv.org/abs/2207.11143) | 本文研究了采用分散策略的MARL算法在梯度下降优化器下的次最优性，并提出了转化与蒸馏框架，该框架可以将多智能体MDP转化为单智能体MDP以实现分散执行。 |
| [^139] | [On the Importance and Applicability of Pre-Training for Federated Learning.](http://arxiv.org/abs/2206.11488) | 研究发现，在联邦学习中使用预训练可以改善性能，尤其是在非独立同分布客户数据的情况下。此外，使用合成数据或客户端数据进行分散式预训练也可以显著改善性能，并且不同的技术可以相互补充以进一步提高性能。 |
| [^140] | [gDDIM: Generalized denoising diffusion implicit models.](http://arxiv.org/abs/2206.05564) | 本研究将降噪扩散隐式模型（DDIM）扩展到一般扩散模型，创新性地提出了广义DDIM（gDDIM），并在模糊扩散模型（BDM）和临界阻尼朗之万扩散模型（CLD）中获得了超过20倍和15倍的加速效果。 |
| [^141] | [Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms.](http://arxiv.org/abs/2206.03792) | 本文研究了基于随机近似的采样算法，利用中心极限定理结构吸收扩散过程中的随机逼近误差并获得了改进的收敛保证。此外，对于SGLD和RBM，我们分别证明了不同的假设条件下较优的收敛率和参数范围。 |
| [^142] | [Physics-Embedded Neural Networks: Graph Neural PDE Solvers with Mixed Boundary Conditions.](http://arxiv.org/abs/2205.11912) | 本文介绍了一种名为物理嵌入神经网络的方法，该方法能够充分考虑边界条件，使用隐式方法预测长时间后的状态，并在各种形状上具有高度的泛化性能，为使用混合边界条件描述的PDE边界值问题提供了有前途的解决方案。 |
| [^143] | [Faith-Shap: The Faithful Shapley Interaction Index.](http://arxiv.org/abs/2203.00870) | 本文介绍了一种称为Faith-Shap的方法，用于为黑盒机器学习模型中的交互提供归因，它不需要放弃效率这一关键属性，并通过将Shapley值作为最忠实线性逼近系数的方法解决了唯一性问题。 |
| [^144] | [Distributed Random Reshuffling over Networks.](http://arxiv.org/abs/2112.15287) | 本文提出了一种分布式随机重洗（D-RR）算法，能够解决协作优化问题。在平滑强凸和平滑非凸目标函数的情况下，D-RR算法都能够实现很好的优化结果，并且优于分布式随机梯度下降算法。 |
| [^145] | [Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies in a Multi-agent Urban Driving Environment.](http://arxiv.org/abs/2112.11947) | 本论文提出了一个基准测试框架，用于评估和比较深度强化学习算法在单个和多个智能体自主驾驶环境中的性能。同时，提出了一种混合算法以提高深度强化学习策略在多代理驾驶环境中的鲁棒性。 |
| [^146] | [Focusing on Potential Named Entities During Active Label Acquisition.](http://arxiv.org/abs/2111.03837) | 本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。 |
| [^147] | [Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes.](http://arxiv.org/abs/2110.15332) | 本文提出了一种方法，可以在离线强化学习中，应用于从医疗保健或教育领域收集的观测数据。我们考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估，以解决观察到的行动可能受到未观察到的因素影响，导致估计值出现偏差的问题。 |
| [^148] | [A Coupled Design of Exploiting Record Similarity for Practical Vertical Federated Learning.](http://arxiv.org/abs/2106.06312) | 本文提出了一种新的耦合训练范式FedSim，将一对多链接集成到训练过程中，以解决现有VFL方法忽略“记录链接”过程的问题，并实现了更好的性能。 |
| [^149] | [The Low-Rank Simplicity Bias in Deep Networks.](http://arxiv.org/abs/2103.10427) | 本文研究了现代深度神经网络的泛化能力及可能的原因，发现深度网络具有归纳偏见，更倾向于寻找低有效秩嵌入的解决方案，并通过实证证明了该偏好在有限宽度线性和非线性模型上的实用性和鲁棒性。 |
| [^150] | [A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks.](http://arxiv.org/abs/2102.04518) | 本文提出了一种使用深度Q网络学习启发式函数，通过只进行一次前向传递计算相邻节点的转移成本和启发式值之和，并在不显式生成这些子节点的情况下指导搜索的Q*搜索算法，以大幅减少计算时间。在魔方问题上的实验表明，该方法能够高效地解决具有大动作空间的问题。 |
| [^151] | [Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments.](http://arxiv.org/abs/2012.10315) | 该论文提出了一种使用负对照、代理变量和工具变量的核方法，以识别治疗效果并学习非参数治疗效果。 作者证明了算法的一致性和收敛速度，并估计了香烟吸烟的剂量反应曲线。 |
| [^152] | [The Variational Method of Moments.](http://arxiv.org/abs/2012.09422) | 本文提出了一个非常通用的条件矩问题估计器类 - 变分矩方法，使得我们能够控制无限数量的矩，并提供了基于核方法和神经网络的多个VMM估计器的理论分析和证明。 |
| [^153] | [Joint Inference of Diffusion and Structure in Partially Observed Social Networks Using Coupled Matrix Factorization.](http://arxiv.org/abs/2010.01400) | 本文提出了一种概率生成模型DiffStru，通过学习耦合低维潜在因素，在部分观测社交网络中联合推断未观测到的扩散和结构网络。 |
| [^154] | [Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity.](http://arxiv.org/abs/2004.12908) | 本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。 |
| [^155] | [Almost Sure Convergence of Dropout Algorithms for Neural Networks.](http://arxiv.org/abs/2002.02247) | 本文提出了针对神经网络的dropout算法的收敛性及收敛速度的研究，给出了概率论证明，证明其权重将收敛于正常微分方程系统的投影唯一稳态点，同时给出了ε-定态点的通用样本复杂度限制。 |
| [^156] | [Student Engagement Detection Using Emotion Analysis, Eye Tracking and Head Movement with Machine Learning.](http://arxiv.org/abs/1909.12913) | 本文提出了一个利用情绪分析、眼动和头部运动进行学生参与度检测的机器学习系统。该系统能够使用笔记本电脑内置摄像头实时监测学生的专注度，通过结合眼睛和头部的运动信息以及面部表情来识别学生处于三种不同的参与度状态，并在典型的在线学习情境下进行了测试，结果表明其能够正确地识别学生的参与度水平。 |

# 详细

[^1]: 教学视频中任务结构的学习和验证

    Learning and Verification of Task Structure in Instructional Videos. (arXiv:2303.13519v1 [cs.CV])

    [http://arxiv.org/abs/2303.13519](http://arxiv.org/abs/2303.13519)

    该论文提出了一种新的预训练视频模型——VideoTaskformer。通过从整体上学习表示来验证视频的正确执行，并预测下一步骤。同时，论文也提出了两个新的检测教学视频中错误的基准。

    

    鉴于在线教学视频数量庞大，从视频中学习多步骤任务模型的多样性是一个诱人的目标。本文引入了一种新的预训练视频模型——VideoTaskformer，专注于表示教学视频的语义和结构。我们使用一种简单有效的目标来对VideoTaskformer进行预训练：从教学视频中随机屏蔽的步骤预测弱监督的文本标签（遮盖步骤建模）。与先前学习局部步骤表示的方法相比，我们的方法涉及全局学习，利用整个周围任务的视频作为上下文。从这些学习到的表示中，我们可以验证一个未见过的视频是否正确执行给定的任务，以及预测在给定步骤之后可能采取哪些步骤。我们引入了两个新的基准来检测教学视频中的错误，以验证是否存在异常步骤并检查步骤是否按正确的顺序执行。

    Given the enormous number of instructional videos available online, learning a diverse array of multi-step task models from videos is an appealing goal. We introduce a new pre-trained video model, VideoTaskformer, focused on representing the semantics and structure of instructional videos. We pre-train VideoTaskformer using a simple and effective objective: predicting weakly supervised textual labels for steps that are randomly masked out from an instructional video (masked step modeling). Compared to prior work which learns step representations locally, our approach involves learning them globally, leveraging video of the entire surrounding task as context. From these learned representations, we can verify if an unseen video correctly executes a given task, as well as forecast which steps are likely to be taken after a given step. We introduce two new benchmarks for detecting mistakes in instructional videos, to verify if there is an anomalous step and if steps are executed in the rig
    
[^2]: 改善开放式词汇检测中特征对齐的三种方法

    Three ways to improve feature alignment for open vocabulary detection. (arXiv:2303.13518v1 [cs.CV])

    [http://arxiv.org/abs/2303.13518](http://arxiv.org/abs/2303.13518)

    该论文提出了三种方法来改善开放式词汇检测中特征对齐的问题，包括增强文本嵌入、修改特征金字塔网络和检测头部、以及采用自学习方法。这些方法可以有效缓解模型在未见类上的性能问题。

    

    零样本开放式词汇检测的核心问题在于如何对齐视觉和文本特征，以使检测器在未见类上表现良好。之前的方法从头开始训练特征金字塔和检测头部，这破坏了预训练期间建立的视觉-文本特征对齐，并且难以防止语言模型忘记未见类。我们提出了三种方法来缓解这些问题。首先，使用简单的方案来增强文本嵌入，防止过度拟合到训练期间见到的少量类别，并同时节省内存和计算。其次，修改特征金字塔网络和检测头部，包括可训练门控快捷方式，这鼓励视觉-文本特征对齐，并确保在检测训练开始时实现特征对齐。最后，采用自学习方法利用更大的图像-文本对语料库，从而改善无人类注释类别的检测性能。

    The core problem in zero-shot open vocabulary detection is how to align visual and text features, so that the detector performs well on unseen classes. Previous approaches train the feature pyramid and detection head from scratch, which breaks the vision-text feature alignment established during pretraining, and struggles to prevent the language model from forgetting unseen classes.  We propose three methods to alleviate these issues. Firstly, a simple scheme is used to augment the text embeddings which prevents overfitting to a small number of classes seen during training, while simultaneously saving memory and computation. Secondly, the feature pyramid network and the detection head are modified to include trainable gated shortcuts, which encourages vision-text feature alignment and guarantees it at the start of detection training. Finally, a self-training approach is used to leverage a larger corpus of image-text pairs thus improving detection performance on classes with no human an
    
[^3]: 文本到图像扩散模型中的概念消融

    Ablating Concepts in Text-to-Image Diffusion Models. (arXiv:2303.13516v1 [cs.CV])

    [http://arxiv.org/abs/2303.13516](http://arxiv.org/abs/2303.13516)

    本论文提出了一种有效的方法，在不重新训练模型的情况下实现了预训练模型中的概念消融，可以消除文本到图像生成中的版权问题和样本记忆问题。

    

    大规模的文本到图像扩散模型具有强大的组合能力，可以生成高保真度的图片。然而，这些模型通常需要在数量庞大的网络数据上进行训练，往往包含有版权材料、授权图像和个人照片。此外，这些模型已经被发现能够模仿不同艺术家的风格或记住准确的训练样本。如何在不重新训练模型的情况下去除这些版权概念或图像？为了达成这个目标，我们提出了一种有效的方法，在预训练模型中实现概念消融，即防止生成目标概念。我们的算法学习如何匹配一个锚定概念对应的图像分布和与目标风格、实例或文本提示相关的图像分布，以防止模型根据其文本条件生成目标概念。广泛的实验证明，我们的方法可以成功地防止消融概念的生成。

    Large-scale text-to-image diffusion models can generate high-fidelity images with powerful compositional ability. However, these models are typically trained on an enormous amount of Internet data, often containing copyrighted material, licensed images, and personal photos. Furthermore, they have been found to replicate the style of various living artists or memorize exact training samples. How can we remove such copyrighted concepts or images without retraining the model from scratch? To achieve this goal, we propose an efficient method of ablating concepts in the pretrained model, i.e., preventing the generation of a target concept. Our algorithm learns to match the image distribution for a target style, instance, or text prompt we wish to ablate to the distribution corresponding to an anchor concept. This prevents the model from generating target concepts given its text condition. Extensive experiments show that our method can successfully prevent the generation of the ablated conce
    
[^4]: 持续的自然：一个生成无界3D世界的模型

    Persistent Nature: A Generative Model of Unbounded 3D Worlds. (arXiv:2303.13515v1 [cs.CV])

    [http://arxiv.org/abs/2303.13515](http://arxiv.org/abs/2303.13515)

    本文介绍了一个生成无界3D世界的模型，探索了无条件合成无界自然场景的任务，使得在保持持久的3D世界模型的同时可以进行任意大的摄像机运动。其场景表示包括可扩展的平面场景布局网格以及全景天空穹顶，并仅从单视图互联网照片中学习生成世界模型。该模型可模拟在3D景观中长时间的飞行，同时支持一个持续的、摄像机独立的世界表示，具有高效的计算性能。

    

    近年来的3D图像生成模型，尽管图像质量越来越逼真，但其操作的3D体积通常是固定的，且摄像机运动受限。我们探索了无条件合成无界自然场景的任务，使得在保持持久的3D世界模型的同时可以进行任意大的摄像机运动。我们的场景表示包括可扩展的平面场景布局网格，可以通过3D解码器和体积渲染从任意摄像机姿态进行渲染，以及全景天空穹顶。基于这种表示，我们仅从单视图互联网照片中学习生成世界模型。我们的方法可以模拟在3D景观中长时间的飞行，同时保持全局场景的一致性——例如，返回起点会产生相同的场景视图。我们的方法使得场景推断超越了当前3D生成模型的固定范围，同时支持一个持续的、摄像机独立的世界表示，而这种表示在计算效率上是非常高效的。

    Despite increasingly realistic image quality, recent 3D image generative models often operate on 3D volumes of fixed extent with limited camera motions. We investigate the task of unconditionally synthesizing unbounded nature scenes, enabling arbitrarily large camera motion while maintaining a persistent 3D world model. Our scene representation consists of an extendable, planar scene layout grid, which can be rendered from arbitrary camera poses via a 3D decoder and volume rendering, and a panoramic skydome. Based on this representation, we learn a generative world model solely from single-view internet photos. Our method enables simulating long flights through 3D landscapes, while maintaining global scene consistency--for instance, returning to the starting point yields the same view of the scene. Our approach enables scene extrapolation beyond the fixed bounds of current 3D generative models, while also supporting a persistent, camera-independent world representation that stands in c
    
[^5]: 神经预设：用于颜色风格转移的新技术

    Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])

    [http://arxiv.org/abs/2303.13511](http://arxiv.org/abs/2303.13511)

    本文提出了一种名为神经预设的技术，通过确定性神经颜色映射方法（DNCM）和两阶段流水线实现高质量的颜色风格转移，并且具有各种优势。

    

    本文提出了一种名为神经预设的技术，用于解决现有颜色风格转移方法的限制，包括可视化伪影、大量内存需求和缓慢的风格切换速度。本方法基于两个核心设计。首先，我们提出了一种确定性神经颜色映射方法（DNCM），通过自适应的颜色映射矩阵在每个像素上进行一致的操作，避免了伪影，并支持具有小内存占用的高分辨率输入。其次，我们通过将任务分为颜色归一化和风格化两个阶段来开发一个两阶段流水线，可以通过将颜色风格作为预设提取，并在归一化的输入图像上重复使用它们来实现有效的风格切换。由于存在成对数据集的问题，我们描述了如何通过自监督策略训练神经预设模型。通过全面的评估展示了神经预设相对于现有方法的各种优势。此外，我们展示了我们训练的模型可以自然地支持多个风格，并在定量和定性性能上实现了最新的表现。

    In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
    
[^6]: 神经网络缩放的量化模型

    The Quantization Model of Neural Scaling. (arXiv:2303.13506v1 [cs.LG])

    [http://arxiv.org/abs/2303.13506](http://arxiv.org/abs/2303.13506)

    该论文提出了神经网络缩放的量化模型，通过将神经网络学习到的功能分为量子并对量子进行递减使用频率的顺序学习，可以解释损失缩放定律，并自动发现语言模型的多样能力。

    

    我们提出了神经网络缩放定律的量化模型，解释了观察到的损失函数随着模型和数据规模的幂律下降以及随着规模的增加出现新能力的突然突破。我们从所谓的“量化假设”中推导出这个模型，其中学习到的神经网络功能被量化为离散块（“量子”）。我们在降序学习频率中学习量子，并表明当量子被以递减使用频率的顺序学习时，在使用频率中使用幂律可以解释观察到的损失缩放定律。我们在玩具数据集上验证了这个预测，然后研究了大型语言模型的缩放曲线如何分解。使用语言模型的内部，我们自动发现多样的模型能力（量子），并发现对应子问题的分布与我们理论预测的神经缩放指数产生了兼容性证据。

    We propose the $\textit{Quantization Model}$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the $\textit{Quantization Hypothesis}$, where learned network capabilities are quantized into discrete chunks ($\textit{quanta}$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.
    
[^7]: 旗型流形上的弦均值及其应用

    Chordal Averaging on Flag Manifolds and Its Applications. (arXiv:2303.13501v1 [cs.CV])

    [http://arxiv.org/abs/2303.13501](http://arxiv.org/abs/2303.13501)

    本文提出了一种可证明收敛的算法，用于在旗型流形上计算一组点的旗形均值和旗形中值，这种方法在计算机视觉问题中非常有用。

    

    本文提出了一种新的、可证明收敛的算法，用于在弦度量下计算旗型流形上一组点的旗形均值和旗形中值。旗型流形是一种数学空间，由嵌套的向量空间子空间序列组成，并且在维度上逐渐增加。旗型流形是已知的许多矩阵群的超集，包括Stiefel和Grassmanians，使其成为在各种计算机视觉问题中非常有用的通用对象。为了解决计算一阶旗帜统计数据的挑战，我们首先将问题转化为涉及辅助变量受Stiefel流形约束的问题。Stiefel流形是一组正交框架的空间，利用Stiefel流形优化的数值稳定性和效率，可以有效地计算旗形均值。通过一系列实验证明了我们的方法在Grassmann和旋转均值以及主成分问题中的有效性。

    This paper presents a new, provably-convergent algorithm for computing the flag-mean and flag-median of a set of points on a flag manifold under the chordal metric. The flag manifold is a mathematical space consisting of flags, which are sequences of nested subspaces of a vector space that increase in dimension. The flag manifold is a superset of a wide range of known matrix groups, including Stiefel and Grassmanians, making it a general object that is useful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we first transform the problem into one that involves auxiliary variables constrained to the Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, and leveraging the numerical stability and efficiency of Stiefel-manifold optimization enables us to compute the flag-mean effectively. Through a series of experiments, we show the competence of our method in Grassmann and rotation averaging, as well as princi
    
[^8]: 通过特征扭曲和简单性偏差来适应模型的更深入研究

    A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias. (arXiv:2303.13500v1 [cs.LG])

    [http://arxiv.org/abs/2303.13500](http://arxiv.org/abs/2303.13500)

    研究对深度神经网络过度拟合数据中简单模式的倾向的易感性，提出了一种受控简单性正则化（CSR）方法来限制过度拟合，完成对模型的更深入研究，提高了模型的泛化能力和安全性。

    

    预训练模型表达能力的提高增加了对启用安全有效的迁移学习的适应协议设计的兴趣。在传统的线性探测（LP）和微调（FT）策略之外，发现可以有效控制特征扭曲（即无法更新正交于分布内部的特征）的协议可以实现改进的越界泛化（OOD）。为了限制这种扭曲，提出了LP+FT协议，该协议首先学习线性探测，然后使用此初始化进行后续FT。但是，在本文中，我们发现当适应协议（LP、FT、LP+FT）也在多种安全目标（例如校准、鲁棒性等）上进行评估时，对特征扭曲的互补视角有助于解释协议行为。为此，我们研究了协议对简单性偏见（SB）的易感性，即深度神经网络过度拟合数据中简单模式的倾向，并提出了一种新的受控简单性正则化（CSR）方法，该方法鼓励对更复杂的特征进行泛化并限制过度拟合。我们的实验证明，CSR可以有效地补充旨在限制特征扭曲的协议，从而实现更强的OOD泛化和改进的安全性。

    Advances in the expressivity of pretrained models have increased interest in the design of adaptation protocols which enable safe and effective transfer learning. Going beyond conventional linear probing (LP) and fine tuning (FT) strategies, protocols that can effectively control feature distortion, i.e., the failure to update features orthogonal to the in-distribution, have been found to achieve improved out-of-distribution generalization (OOD). In order to limit this distortion, the LP+FT protocol, which first learns a linear probe and then uses this initialization for subsequent FT, was proposed. However, in this paper, we find when adaptation protocols (LP, FT, LP+FT) are also evaluated on a variety of safety objectives (e.g., calibration, robustness, etc.), a complementary perspective to feature distortion is helpful to explain protocol behavior. To this end, we study the susceptibility of protocols to simplicity bias (SB), i.e. the well-known propensity of deep neural networks to
    
[^9]: MAE预前置训练对于亿级预训练的有效性

    The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v1 [cs.CV])

    [http://arxiv.org/abs/2303.13496](http://arxiv.org/abs/2303.13496)

    本文在计算机视觉领域提出了一种自我监督的MAE技术预前置训练方法，该方法适用于亿级预训练规模，并可显著提高模型收敛性和下游转移性能。

    

    本文重新审视了计算机视觉中用于视觉识别任务的标准预训练-微调范式。通常情况下，最先进的基础模型使用数十亿张图像的大规模（弱）监督数据集进行预训练。我们引入了一个额外的预前置训练阶段，它使用了自我监督的MAE技术来初始化模型。虽然MAE技术仅被证明能够与模型大小相缩放，但我们发现它也可以随数据集大小缩放。因此，我们基于MAE的预前置训练可同时适用于训练基础模型的模型和数据规模。预前置训练在一系列模型规模（参数数百万到数十亿）和数据集大小（图像数百万到数十亿）上一致提高了模型收敛性和下游转移性能，且我们还测量了其在10个不同的视觉识别任务上的有效性，包括图像分类、视频识别和目标检测。

    This paper revisits the standard pretrain-then-finetune paradigm used in computer vision for visual recognition tasks. Typically, state-of-the-art foundation models are pretrained using large scale (weakly) supervised datasets with billions of images. We introduce an additional pre-pretraining stage that is simple and uses the self-supervised MAE technique to initialize the model. While MAE has only been shown to scale with the size of models, we find that it scales with the size of the training dataset as well. Thus, our MAE-based pre-pretraining scales with both model and data size making it applicable for training foundation models. Pre-pretraining consistently improves both the model convergence and the downstream transfer performance across a range of model scales (millions to billions of parameters), and dataset sizes (millions to billions of images). We measure the effectiveness of pre-pretraining on 10 different visual recognition tasks spanning image classification, video reco
    
[^10]: 使用示范加速强化学习与规划：一份综述

    Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])

    [http://arxiv.org/abs/2303.13489](http://arxiv.org/abs/2303.13489)

    强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。

    

    尽管强化学习最近取得了巨大的成功，但是这种试错式的学习方法在复杂环境下可能效率低下。与此相反，使用示范可以让智能体受益于专家的知识，而无需探索最佳行动。在本综述中，我们讨论了在顺序决策中使用示范的优点，以及学习为基础的决策制定范式（例如，强化学习和规划在学习的模型中如何应用示范），以及如何在各种情况下收集示范。此外，我们还举了一个实际的示范生成和利用管道的例子，并在最近提出的ManiSkill机器人学习基准中进行了说明。

    Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
    
[^11]: TactoFind：一种纯触觉物品获取系统

    TactoFind: A Tactile Only System for Object Retrieval. (arXiv:2303.13482v1 [cs.RO])

    [http://arxiv.org/abs/2303.13482](http://arxiv.org/abs/2303.13482)

    TactoFind是一个纯触觉物品获取系统，它可以使用手指上的触控传感器，在没有任何视觉反馈的情况下定位、识别和抓取新的物体。

    

    本文研究了在缺乏视觉感知、未知物体形状以及物体可以自由移动的场景下进行物品检索的问题。成功的解决方案需要定位自由物体、识别特定的物体实例并使用触觉反馈来抓取已识别的物体。与摄影机可观察整个场景的视觉不同，触觉传感器是局部的，并且仅观察与操纵器接触的场景部分。此外，通过触觉传感器收集信息需要在触摸表面施加力，这可能会扰乱场景本身。因此，触摸感知需要通过时间上的精细探索和信息集成来进行推理。我们提出了一个系统，可以利用手指触摸传感器上的稀疏触觉反馈，在没有任何视觉反馈的情况下定位、识别和抓取新的物体。

    We study the problem of object retrieval in scenarios where visual sensing is absent, object shapes are unknown beforehand and objects can move freely, like grabbing objects out of a drawer. Successful solutions require localizing free objects, identifying specific object instances, and then grasping the identified objects, only using touch feedback. Unlike vision, where cameras can observe the entire scene, touch sensors are local and only observe parts of the scene that are in contact with the manipulator. Moreover, information gathering via touch sensors necessitates applying forces on the touched surface which may disturb the scene itself. Reasoning with touch, therefore, requires careful exploration and integration of information over time -- a challenge we tackle. We present a system capable of using sparse tactile feedback from fingertip touch sensors on a dexterous hand to localize, identify and grasp novel objects without any visual feedback. Videos are available at https://ta
    
[^12]: 利用量子几何进行学习幺正变换的泛化

    Generalization with quantum geometry for learning unitaries. (arXiv:2303.13462v1 [quant-ph])

    [http://arxiv.org/abs/2303.13462](http://arxiv.org/abs/2303.13462)

    本文研究了量子机器学习模型的泛化能力，使用数据的量子费舍尔信息度量来评估成功训练和泛化所需的电路参数和训练数据的数量，并展示通过去除对称性来提高泛化能力，同时发现超出分布泛化能力可以比使用相同分布更优。

    

    泛化是量子机器学习模型从训练数据学习准确预测新数据的能力。在这里，我们引入数据的量子费舍尔信息度量(DQFIM)来确定模型何时能够泛化。对于幺正变换的可变学习，DQFIM量化了成功训练和泛化所需的电路参数和训练数据的数量。我们应用DQFIM来解释何时恒定数量的训练状态和多项式数量的参数足以实现泛化。此外，通过从训练数据中删除对称性，可以提高泛化能力。最后，我们显示，使用不同数据分布进行训练和测试的超出分布泛化能力可以比使用相同分布的能力更优。我们的研究为提高量子机器学习中的泛化能力开辟了新的方法。

    Generalization is the ability of quantum machine learning models to make accurate predictions on new data by learning from training data. Here, we introduce the data quantum Fisher information metric (DQFIM) to determine when a model can generalize. For variational learning of unitaries, the DQFIM quantifies the amount of circuit parameters and training data needed to successfully train and generalize. We apply the DQFIM to explain when a constant number of training states and polynomial number of parameters are sufficient for generalization. Further, we can improve generalization by removing symmetries from training data. Finally, we show that out-of-distribution generalization, where training and testing data are drawn from different data distributions, can be better than using the same distribution. Our work opens up new approaches to improve generalization in quantum machine learning.
    
[^13]: 等变增强神经网络的优化动态

    Optimization Dynamics of Equivariant and Augmented Neural Networks. (arXiv:2303.13458v1 [cs.LG])

    [http://arxiv.org/abs/2303.13458](http://arxiv.org/abs/2303.13458)

    本论文研究了在对称数据上优化多层感知机的方法，比较了等变和增强两种策略的优缺点，证明了在自然假设下等变稳定点的集合和等变层的集合具有不变性，但增强模型的稳定点可能是不稳定的。

    

    我们研究了在对称数据上优化多层感知机的方法。我们比较了限制架构等变和使用增强的策略。我们证明，在对损失和非线性性进行自然假设的情况下，等变稳定点的集合对于这两种策略是相同的，并且等变层的集合在增强模型的梯度流下是不变的。最后，我们表明，尽管等变模型的稳定点是稳定的，增强训练的稳定点可能是不稳定的。

    We investigate the optimization of multilayer perceptrons on symmetric data. We compare the strategy of constraining the architecture to be equivariant to that of using augmentation. We show that, under natural assumptions on the loss and non-linearities, the sets of equivariant stationary points are identical for the two strategies, and that the set of equivariant layers is invariant under the gradient flow for augmented models. Finally, we show that stationary points may be unstable for augmented training although they are stable for the equivariant models
    
[^14]: COVID-19背景下的人类行为：从大数据中学习 (arXiv:2303.13452v1 [cs.CY])

    Human Behavior in the Time of COVID-19: Learning from Big Data. (arXiv:2303.13452v1 [cs.CY])

    [http://arxiv.org/abs/2303.13452](http://arxiv.org/abs/2303.13452)

    本研究综述了使用大数据技术研究COVID-19背景下人类行为的现有研究，分为使用大数据测量、模拟和干预人类行为三类研究。

    

    自2020年3月世界卫生组织(WHO)将COVID-19定性为大流行病以来，截至2022年10月已有超过6亿例COVID-19确诊病例和超过600万例死亡。COVID-19大流行与人类行为之间的关系是复杂的。一方面，人类行为被发现可以塑造疾病的传播，另一方面，疫情影响并甚至改变了几乎每个方面的人类行为。为了提供对人类行为与COVID-19大流行之间复杂相互作用的全面理解，研究人员一直在采用自然语言处理、计算机视觉、音频信号处理、频繁模式挖掘和机器学习等大数据技术。在本研究中，我们总结了使用大数据技术研究COVID-19背景下人类行为的现有研究。特别是，我们将这些研究分为三类——使用大数据测量、模拟和干预人类行为的研究。

    Since the World Health Organization (WHO) characterized COVID-19 as a pandemic in March 2020, there have been over 600 million confirmed cases of COVID-19 and more than six million deaths as of October 2022. The relationship between the COVID-19 pandemic and human behavior is complicated. On one hand, human behavior is found to shape the spread of the disease. On the other hand, the pandemic has impacted and even changed human behavior in almost every aspect. To provide a holistic understanding of the complex interplay between human behavior and the COVID-19 pandemic, researchers have been employing big data techniques such as natural language processing, computer vision, audio signal processing, frequent pattern mining, and machine learning. In this study, we present an overview of the existing studies on using big data techniques to study human behavior in the time of the COVID-19 pandemic. In particular, we categorize these studies into three groups - using big data to measure, mode
    
[^15]: Set-the-Scene: 全局-局部训练用于生成可控的 NeRF 场景

    Set-the-Scene: Global-Local Training for Generating Controllable NeRF Scenes. (arXiv:2303.13450v1 [cs.CV])

    [http://arxiv.org/abs/2303.13450](http://arxiv.org/abs/2303.13450)

    该论文提出了一个全局-局部训练框架，使用对象代理合成 3D 场景，将每个对象表示为独立的 NeRF，并交替优化，从而实现对每个对象的完整表示的学习。

    

    近期在文本引导下的图像合成领域取得了重大突破，优化神经辐射场（NeRF）直接从文本中，最近的方法能够产生出色的结果。然而，这些方法在每个对象的放置或外观控制方面受到限制，因为它们代表整个场景。这在需要细化或操纵场景中的对象的情况下可能是一个重要问题。为了弥补这一不足，我们提出了一种新颖的全局-局部训练框架，使用对象代理合成 3D 场景。代理表示生成场景中对象的放置，并可选地定义其粗略几何形状。我们方法的关键在于将每个对象表示为独立的 NeRF。我们在优化每个 NeRF 自身和完整场景的组成部分之间交替进行。因此，可以学习到每个对象的完整表示，同时创建具有风格和照明匹配的和谐场景。

    Recent breakthroughs in text-guided image generation have led to remarkable progress in the field of 3D synthesis from text. By optimizing neural radiance fields (NeRF) directly from text, recent methods are able to produce remarkable results. Yet, these methods are limited in their control of each object's placement or appearance, as they represent the scene as a whole. This can be a major issue in scenarios that require refining or manipulating objects in the scene. To remedy this deficit, we propose a novel GlobalLocal training framework for synthesizing a 3D scene using object proxies. A proxy represents the object's placement in the generated scene and optionally defines its coarse geometry. The key to our approach is to represent each object as an independent NeRF. We alternate between optimizing each NeRF on its own and as part of the full scene. Thus, a complete representation of each object can be learned, while also creating a harmonious scene with style and lighting match. W
    
[^16]: GiveMeLabeledIssues：一个开放源代码的任务推荐系统

    GiveMeLabeledIssues: An Open Source Issue Recommendation System. (arXiv:2303.13418v1 [cs.SE])

    [http://arxiv.org/abs/2303.13418](http://arxiv.org/abs/2303.13418)

    GiveMeLabeledIssues是一个开源的任务推荐系统，利用API领域代理技能，从而可以帮助开发者更好地匹配任务，并减轻项目维护者的负担。在预测API领域时，该工具的精度达到了83.9%。

    

    开发者经常难以在开放源代码（OSS）项目的问题跟踪系统中找到合适的任务。正确的问题标记可以帮助选择任务，但当前的工具仅限于按照问题的类型（例如，错误、问题、好的第一个问题、功能等）对其进行分类。相反，本文介绍了一种工具（GiveMeLabeledIssues），它可以挖掘项目存储库并基于解决问题所需的技能对问题进行标记。我们利用解决方案涉及的API（例如，用户界面（UI）、测试、数据库（DB）等）领域作为所需技能的代理。GiveMeLabeledIssues有助于将开发者的技能与任务匹配，减轻项目维护者的负担。该工具在预测问题中涉及的API领域时获得了83.9％的精度。复制包含有关执行该工具和包含新项目的说明。演示视频可在https://www.youtube.com/watch?v=ic2quUue7i8找到。

    Developers often struggle to navigate an Open Source Software (OSS) project's issue-tracking system and find a suitable task. Proper issue labeling can aid task selection, but current tools are limited to classifying the issues according to their type (e.g., bug, question, good first issue, feature, etc.). In contrast, this paper presents a tool (GiveMeLabeledIssues) that mines project repositories and labels issues based on the skills required to solve them. We leverage the domain of the APIs involved in the solution (e.g., User Interface (UI), Test, Databases (DB), etc.) as a proxy for the required skills. GiveMeLabeledIssues facilitates matching developers' skills to tasks, reducing the burden on project maintainers. The tool obtained a precision of 83.9% when predicting the API domains involved in the issues. The replication package contains instructions on executing the tool and including new projects. A demo video is available at https://www.youtube.com/watch?v=ic2quUue7i8
    
[^17]: 语义转换混淆AI生成文本检测，而检索是一种有效的防御方法

    Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. (arXiv:2303.13408v1 [cs.CL])

    [http://arxiv.org/abs/2303.13408](http://arxiv.org/abs/2303.13408)

    本文探究了语义转换对于AI生成文本检测器的鲁棒性，通过训练的语义转换生成模型成功混淆了多个检测器。

    

    近期有多种方法被提出来用于识别恶意使用大型语言模型 (例如虚假内容创建或学术抄袭)中的AI生成文本，包括通过水印或统计异常点。本文探究这些文本检测算法对于AI生成文本的含义转换的鲁棒性。为了测试这些检测器的性能，我们首先训练了一个11B参数的语义转换生成模型(DIPPER)，该模型可以将段落进行语义转换，可选择利用周围文本(例如用户写的提示)作为上下文。DIPPER还使用标量旋钮来控制语义转换中词汇多样性和重新排列的程度。通过使用DIPPER来进行三种大型语言模型生成文本的语义转换，成功地混淆了多个文本检测器，包括水印检测、GPTZero、DetectGPT和OpenAI的文本分类器。例如，DIPPER将DetectGPT的检测准确率从70.3%降至4.6%（在恒定的1%误报率下）。

    To detect the deployment of large language models for malicious use cases (e.g., fake content creation or academic plagiarism), several approaches have recently been proposed for identifying AI-generated text via watermarks or statistical irregularities. How robust are these detection algorithms to paraphrases of AI-generated text? To stress test these detectors, we first train an 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, optionally leveraging surrounding text (e.g., user-written prompts) as context. DIPPER also uses scalar knobs to control the amount of lexical diversity and reordering in the paraphrases. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops the detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), withou
    
[^18]: 深度上下文多臂赌博机的自适应端点检测

    Adaptive Endpointing with Deep Contextual Multi-armed Bandits. (arXiv:2303.13407v1 [eess.AS])

    [http://arxiv.org/abs/2303.13407](http://arxiv.org/abs/2303.13407)

    在线学习的自适应端点检测方法，使用深度上下文多臂赌博机，避免使用昂贵的网格搜索，不需要真值标签，并成功减少了早期截止错误。

    

    目前的端点检测（EP）解决方案是在监督框架下进行学习的，这不允许模型获得反馈并在在线设置中改进。此外，通常使用昂贵的网格搜索来找到端点检测模型的最佳配置。本文旨在通过提出一种有效的方法，为给定语音级别的音频特征在在线设置中选择最佳的端点检测配置，同时避免超参数的网格搜索，从而为自适应端点检测提供解决方案。我们的方法不需要地面真值标签，并仅使用来自奖励信号的在线学习而不需要注释标签。具体地，我们提出了一种基于深度上下文多臂赌博机的方法，它结合了神经网络的表征能力和汤普森建模算法的行为探索行为。我们将我们的方法与几个基线进行比较，并证明我们的深度赌博模型也成功减少了早期截止误差。

    Current endpointing (EP) solutions learn in a supervised framework, which does not allow the model to incorporate feedback and improve in an online setting. Also, it is a common practice to utilize costly grid-search to find the best configuration for an endpointing model. In this paper, we aim to provide a solution for adaptive endpointing by proposing an efficient method for choosing an optimal endpointing configuration given utterance-level audio features in an online setting, while avoiding hyperparameter grid-search. Our method does not require ground truth labels, and only uses online learning from reward signals without requiring annotated labels. Specifically, we propose a deep contextual multi-armed bandit-based approach, which combines the representational power of neural networks with the action exploration behavior of Thompson modeling algorithms. We compare our approach to several baselines, and show that our deep bandit models also succeed in reducing early cutoff errors 
    
[^19]: 对抗鲁棒性的优化和优化器

    Optimization and Optimizers for Adversarial Robustness. (arXiv:2303.13401v1 [cs.LG])

    [http://arxiv.org/abs/2303.13401](http://arxiv.org/abs/2303.13401)

    本论文提出了一种新的算法框架，将通用的约束优化求解器与综合约束折叠结合起来，以提高深度学习模型的对抗性鲁棒性评估的可靠性和广泛性。

    

    深度学习模型的对抗性鲁棒性评估需要解决非平凡的约束优化问题。现有的数值算法主要依赖于投影梯度，并且主要处理由$\ell_1$，$\ell_2$和$\ell_\infty$距离建模的扰动。在本文中，我们介绍了一种新算法框架，它将通用的约束优化求解器PyGRANSO与Constraint Folding (PWCF)结合起来，可以增加更多可靠性和广泛性到最先进的RE软件包，例如AutoAttack。关于可靠性，PWCF提供带有稳定性测量和可行性测试的解决方案以评估解决方案的质量。对于广泛性，PWCF可以处理通常对现有投影梯度方法不可访问的扰动模型；主要要求是距离度量在几乎所有地方都可微分。利用PWCF的优势

    Empirical robustness evaluation (RE) of deep learning models against adversarial perturbations entails solving nontrivial constrained optimization problems. Existing numerical algorithms that are commonly used to solve them in practice predominantly rely on projected gradient, and mostly handle perturbations modeled by the $\ell_1$, $\ell_2$ and $\ell_\infty$ distances. In this paper, we introduce a novel algorithmic framework that blends a general-purpose constrained-optimization solver PyGRANSO with Constraint Folding (PWCF), which can add more reliability and generality to the state-of-the-art RE packages, e.g., AutoAttack. Regarding reliability, PWCF provides solutions with stationarity measures and feasibility tests to assess the solution quality. For generality, PWCF can handle perturbation models that are typically inaccessible to the existing projected gradient methods; the main requirement is the distance metric to be almost everywhere differentiable. Taking advantage of PWCF 
    
[^20]: Xplainer：从X射线观察到可解释的零样本诊断

    Xplainer: From X-Ray Observations to Explainable Zero-Shot Diagnosis. (arXiv:2303.13391v1 [cs.CV])

    [http://arxiv.org/abs/2303.13391](http://arxiv.org/abs/2303.13391)

    Xplainer是一个透明且可解释的零样本诊断新框架，通过对存在的描述性观察进行分类来提高自动诊断集成到临床工作流程中的效率，同时避免需要大量注释数据的问题。

    

    通过医学图像进行自动诊断预测，是支持临床决策的宝贵资源。然而，这样的系统通常需要在大量注释数据上进行训练，而医学领域的注释数据往往很少。零样本方法通过允许在不依赖标记数据的情况下灵活适应具有不同临床结果的新设置来解决这一挑战。此外，为了将自动诊断集成到临床工作流程中，方法应该是透明且可解释的，增加医疗专业人员的信任并促进正确性验证。在这项工作中，我们引入了Xplainer，这是一个在临床设置中进行可解释的零样本诊断的新框架。Xplainer将对比视觉语言模型的分类即描述方法适应于多标签医学诊断任务。具体而言，我们提示模型对存在的描述性观察进行分类，而不是直接预测诊断。

    Automated diagnosis prediction from medical images is a valuable resource to support clinical decision-making. However, such systems usually need to be trained on large amounts of annotated data, which often is scarce in the medical domain. Zero-shot methods address this challenge by allowing a flexible adaption to new settings with different clinical findings without relying on labeled data. Further, to integrate automated diagnosis in the clinical workflow, methods should be transparent and explainable, increasing medical professionals' trust and facilitating correctness verification. In this work, we introduce Xplainer, a novel framework for explainable zero-shot diagnosis in the clinical setting. Xplainer adapts the classification-by-description approach of contrastive vision-language models to the multi-label medical diagnosis task. Specifically, instead of directly predicting a diagnosis, we prompt the model to classify the existence of descriptive observations, which a radiologi
    
[^21]: 基于文本模型的组合零样本领域转移

    Compositional Zero-Shot Domain Transfer with Text-to-Text Models. (arXiv:2303.13386v1 [cs.CL])

    [http://arxiv.org/abs/2303.13386](http://arxiv.org/abs/2303.13386)

    提出了一种组合转移学习框架，用于专业领域中的零样本领域转移，使用未标记的领域自由文本进行领域和任务知识的共同学习，通过 NLGU 策略实现领域数据增强和标签预测，经实验证明在生物医学领域和放射学子领域具有优异的性能，胜过了现有的 SOTA。

    

    在专业领域中，标签稀缺是提高任务性能的瓶颈。我们提出了一种新的组合转移学习框架（DoT5 领域组合零样本 T5），用于零样本领域转移。在没有访问领域标签的情况下，DoT5以多任务的方式共同学习领域知识（从未标记的领域自由文本的 MLM 中学习）和任务知识（从更容易获取的通用领域数据的任务训练中学习）。为了提高任务训练的可转移性，我们设计了一种名为 NLGU 的策略：我们同时为领域标签到数据生成训练 NLG，从而实现用于自我微调的数据增强和用于标签预测的 NLU 训练。我们在生物医学领域和放射学的资源贫乏子领域上评估了 DoT5，重点关注 NLI、文本摘要和嵌入学习。通过多任务学习，DoT5证明了组合转移学习的有效性，尤其是在零样本转移方面胜过了现有的 SOTA。

    Label scarcity is a bottleneck for improving task performance in specialised domains. We propose a novel compositional transfer learning framework (DoT5 domain compositional zero-shot T5) for zero-shot domain transfer. Without access to in-domain labels, DoT5 jointly learns domain knowledge (from MLM of unlabelled in-domain free text) and task knowledge (from task training on more readily available general-domain data) in a multi-task manner. To improve the transferability of task training, we design a strategy named NLGU: we simultaneously train NLG for in-domain label-to-data generation which enables data augmentation for self-finetuning and NLU for label prediction. We evaluate DoT5 on the biomedical domain and the resource-lean subdomain of radiology, focusing on NLI, text summarisation and embedding learning. DoT5 demonstrates the effectiveness of compositional transfer learning through multi-task learning. In particular, DoT5 outperforms the current SOTA in zero-shot transfer b
    
[^22]: 基于学习的静态恶意软件分类器的对抗性鲁棒性

    Adversarial Robustness of Learning-based Static Malware Classifiers. (arXiv:2303.13372v1 [cs.CR])

    [http://arxiv.org/abs/2303.13372](http://arxiv.org/abs/2303.13372)

    本文提出了一种通用的对抗性补丁（UAP）攻击方法，只需附加一个相对较小的字节级补丁即可绕过MalConv分类器的检测，可以将恶意软件文件的检测率降低80％。同时，作者提出了窗口消除处理作为应对此种攻击的一种方法。

    

    恶意软件检测一直是恶意软件作者和反病毒系统之间持续的军备竞赛阶段。随着这场竞赛规模的不断增加，利用机器学习（ML）的解决方案得到了关注。然而，这种趋势使得直接对ML进行攻击对于对手而言成为一种有吸引力的前景。本文研究了这场军备竞赛的两个方面，即从恶意软件文件的原始字节中操作的基于卷积神经网络的流行分类器MalConv的角度。首先，我们表明MalConv易受到对抗性补丁攻击的影响:将一个字节级的补丁附加到恶意软件文件中，使其绕过检测的概率高达94.3％。此外，我们开发了一种通用的对抗性补丁（UAP）攻击，在任何包含该补丁的恶意软件文件的恒定时间内，可以将其检测率降低80％。即使相对于原始文件大小而言，这些补丁的大小也相对较小-在2％-8％之间。为了抵御这种攻击，我们进行了窗口消除处理，允许识别恶意代码的部分不受对抗性补丁攻击。

    Malware detection has long been a stage for an ongoing arms race between malware authors and anti-virus systems. Solutions that utilize machine learning (ML) gain traction as the scale of this arms race increases. This trend, however, makes performing attacks directly on ML an attractive prospect for adversaries. We study this arms race from both perspectives in the context of MalConv, a popular convolutional neural network-based malware classifier that operates on raw bytes of files. First, we show that MalConv is vulnerable to adversarial patch attacks: appending a byte-level patch to malware files bypasses detection 94.3% of the time. Moreover, we develop a universal adversarial patch (UAP) attack where a single patch can drop the detection rate in constant time of any malware file that contains it by 80%. These patches are effective even being relatively small with respect to the original file size -between 2%-8%. As a countermeasure, we then perform window ablation that allows u
    
[^23]: 自然语言处理和机器学习在需求规范化中的应用：一篇系统综述

    Requirement Formalisation using Natural Language Processing and Machine Learning: A Systematic Review. (arXiv:2303.13365v1 [cs.CL])

    [http://arxiv.org/abs/2303.13365](http://arxiv.org/abs/2303.13365)

    本文调查了自然语言处理和机器学习在需求规范化中的应用现状，发现启发式NLP方法是自动RF中最常用的NLP技术，机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。

    

    软件开发方法的改进吸引了开发人员在需求工程领域自动化需求规范化（RF）中应用自然语言处理（NLP）和机器学习（ML），报告了应用NLP和ML在减少自然语言编写的需求不确定性和不完整性方面的潜在优势。本文的目标是调查和分类现有的NLP和ML在RF上的工作，识别该领域的挑战并提供有前途的未来研究方向。为了实现这一目标，我们进行了系统文献综述，选取了来自常用库的257篇论文。通过定义包含和排除标准来过滤搜索结果，并选择了47项相关研究，时间跨度在2012年至2022年之间。我们发现启发式NLP方法是自动RF中最常用的NLP技术，主要应用于结构化数据，而机器学习方法则包括基于分类和聚类的技术。本文总结了现有的NLP和ML技术在RF领域中的较大进展，并指出了在应用中面临的挑战和未来研究的方向。

    Improvement of software development methodologies attracts developers to automatic Requirement Formalisation (RF) in the Requirement Engineering (RE) field. The potential advantages by applying Natural Language Processing (NLP) and Machine Learning (ML) in reducing the ambiguity and incompleteness of requirement written in natural languages is reported in different studies. The goal of this paper is to survey and classify existing work on NLP and ML for RF, identifying challenges in this domain and providing promising future research directions. To achieve this, we conducted a systematic literature review to outline the current state-of-the-art of NLP and ML techniques in RF by selecting 257 papers from common used libraries. The search result is filtered by defining inclusion and exclusion criteria and 47 relevant studies between 2012 and 2022 are selected. We found that heuristic NLP approaches are the most common NLP techniques used for automatic RF, primary operating on structured 
    
[^24]: 重新评估EmoWOZ中针对情感检测的数据划分

    Reevaluating Data Partitioning for Emotion Detection in EmoWOZ. (arXiv:2303.13364v1 [cs.CL])

    [http://arxiv.org/abs/2303.13364](http://arxiv.org/abs/2303.13364)

    本文重新评估了EmoWOZ数据集的数据划分，在此基础上提出了一种新的分层抽样方法用于处理高度不平衡和不均匀分布的情感标签。使用这个方法建立在EmoWoz上的模型表现更好，未来研究者应该采取这种划分以确保一致和准确的性能评估。

    

    本文聚焦于EmoWOZ数据集，该数据集是MultiWOZ数据集的扩展，提供了对话的情感标签。与原始的MultiWOZ数据集因其它目的被划分不同，EmoWOZ中情感标签高度不平衡，分布在不同划分中也不均匀，导致模型比较效果欠佳。为了解决这个问题、改善数据集的分布并减少数据集分布偏差，我们提出了一种基于情感标签的分层抽样方法，并引入了一种特殊技术来处理有多个情感标签的对话（序列）数据。使用我们提出的抽样方法，建立在EmoWoz上的模型可以表现更好，使它成为训练具有情感智能的对话代理更为可靠的资源。我们推荐未来的研究者使用这个新的数据集划分来确保一致和准确的性能评估。

    This paper focuses on the EmoWoz dataset, an extension of MultiWOZ that provides emotion labels for the dialogues. MultiWOZ was partitioned initially for another purpose, resulting in a distributional shift when considering the new purpose of emotion recognition. The emotion tags in EmoWoz are highly imbalanced and unevenly distributed across the partitions, which causes sub-optimal performance and poor comparison of models. We propose a stratified sampling scheme based on emotion tags to address this issue, improve the dataset's distribution, and reduce dataset shift. We also introduce a special technique to handle conversation (sequential) data with many emotional tags. Using our proposed sampling method, models built upon EmoWoz can perform better, making it a more reliable resource for training conversational agents with emotional intelligence. We recommend that future researchers use this new partitioning to ensure consistent and accurate performance evaluations.
    
[^25]: FS-Real：向真实跨设备联邦学习迈进

    FS-Real: Towards Real-World Cross-Device Federated Learning. (arXiv:2303.13363v1 [cs.LG])

    [http://arxiv.org/abs/2303.13363](http://arxiv.org/abs/2303.13363)

    本文针对联邦学习中异构设备和规模的挑战，提出一种高效可扩展的原型系统，以支持跨设备联邦学习，从而弥补学术界和工业界间在联邦学习研究中的差距。

    

    联邦学习旨在在分布式客户端的帮助下训练高质量的模型，而不上传他们的本地数据，这在学术界和工业界都引起了越来越多的关注。然而，现有作品大多使用不同的同构设备进行评估，这与真实场景中异构设备的多样性和变异性不符。此外，由于资源有限和软件堆栈复杂，使用异构设备进行研究和开发也是具有挑战性的。这些因素是联邦学习研究中重要但鲜有探索的，它们直接影响联邦与本地模型之间的训练动态和最终性能，使得联邦学习算法的有效性和可用性不明确。为了填补这一空白，在本文中，我们提出了一种高效且可扩展的原型系统，以支持异构设备上的跨设备联邦学习。

    Federated Learning (FL) aims to train high-quality models in collaboration with distributed clients while not uploading their local data, which attracts increasing attention in both academia and industry. However, there is still a considerable gap between the flourishing FL research and real-world scenarios, mainly caused by the characteristics of heterogeneous devices and its scales. Most existing works conduct evaluations with homogeneous devices, which are mismatched with the diversity and variability of heterogeneous devices in real-world scenarios. Moreover, it is challenging to conduct research and development at scale with heterogeneous devices due to limited resources and complex software stacks. These two key factors are important yet underexplored in FL research as they directly impact the FL training dynamics and final performance, making the effectiveness and usability of FL algorithms unclear. To bridge the gap, in this paper, we propose an efficient and scalable prototypi
    
[^26]: 基于语言模型的合作性评估的可扩展性研究

    Towards the Scalable Evaluation of Cooperativeness in Language Models. (arXiv:2303.13360v1 [cs.CL])

    [http://arxiv.org/abs/2303.13360](http://arxiv.org/abs/2303.13360)

    本论文旨在对基于语言模型的合作性评估的可扩展性进行研究，通过生成特定博弈论结构场景并进行评估，不过目前生成质量较一般。

    

    预训练的语言模型（PLMs）驱动的AI系统可能越来越多地用于辅助人类进行涉及其他代理人的高 stakes 交互，例如协商或冲突解决。符合合作的AI的目标，我们希望以亲社会的方式理解和塑造PLM的多代理行为。一个重要的第一步是对模型在各种合作问题上行为的评估。由于交互中期望的行为取决于精确的博弈结构，我们专注于使用众包工人和语言模型生成特定结构的场景。我们的工作如下。首先，我们讨论了生成特定博弈论结构场景的关键方法问题。其次，我们使用众包工人和语言模型来生成这些场景。我们发现两种情况下的生成质量往往是中等水平。此外，我们获得了以下结论：

    It is likely that AI systems driven by pre-trained language models (PLMs) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution. Consistent with the goals of Cooperative AI \citep{dafoe_open_2020}, we wish to understand and shape the multi-agent behaviors of PLMs in a pro-social manner. An important first step is the evaluation of model behaviour across diverse cooperation problems. Since desired behaviour in an interaction depends upon precise game-theoretic structure, we focus on generating scenarios with particular structures with both crowdworkers and a language model. Our work proceeds as follows. First, we discuss key methodological issues in the generation of scenarios corresponding to particular game-theoretic structures. Second, we employ both crowdworkers and a language model to generate such scenarios. We find that the quality of generations tends to be mediocre in both cases. We additionally get 
    
[^27]: 增加文本上下文大小提高医疗图像-文本匹配的准确性

    Increasing Textual Context Size Boosts Medical Image-Text Matching. (arXiv:2303.13340v1 [cs.LG])

    [http://arxiv.org/abs/2303.13340](http://arxiv.org/abs/2303.13340)

    利用滑动窗口技术增加了文本上下文的编码，提高了医疗图像-文本匹配的准确性，新模型ClipMD在两个数据集上都取得了最好的结果。

    

    这篇短技术报告展示了一种简单的技术，可以在医疗图像-文本匹配任务中获得最先进的结果。我们分析了使用OpenAI的CLIP进行图像-文本匹配的模型，并观察到CLIP在医疗领域中需要编码更长文本上下文的地方，其有限的文本输入大小会对下游性能产生负面影响。因此，我们使用简单的滑动窗口技术训练和发布了ClipMD，用于编码文本标题。ClipMD在两个医疗图像-文本数据集上进行了测试，并与其他图像-文本匹配模型进行了比较。结果表明，ClipMD在两个数据集上的表现都比其他模型要好得多。我们公开了我们的代码和预训练模型。

    This short technical report demonstrates a simple technique that yields state of the art results in medical image-text matching tasks. We analyze the use of OpenAI's CLIP, a general image-text matching model, and observe that CLIP's limited textual input size has negative impact on downstream performance in the medical domain where encoding longer textual contexts is often required. We thus train and release ClipMD, which is trained with a simple sliding window technique to encode textual captions. ClipMD was tested on two medical image-text datasets and compared with other image-text matching models. The results show that ClipMD outperforms other models on both datasets by a large margin. We make our code and pretrained model publicly available.
    
[^28]: 语音合成的音频扩散模型：基于生成AI的文本到语音和语音增强的概述

    Audio Diffusion Model for Speech Synthesis: A Survey on Text To Speech and Speech Enhancement in Generative AI. (arXiv:2303.13336v1 [cs.SD])

    [http://arxiv.org/abs/2303.13336](http://arxiv.org/abs/2303.13336)

    此文介绍了音频扩散模型，重点讨论了两个活跃任务：文本到语音和语音增强，并对实验结果进行了比较和讨论。

    

    生成AI在各个领域表现出了惊人的性能，其中语音合成是一个有趣的方向。随着扩散模型成为最流行的生成模型，许多工作已经尝试了两个活跃任务：文本到语音和语音增强。本文对音频扩散模型进行了概述，这是对现有调查的补充，这些调查要么缺乏基于扩散的语音合成的最新进展，要么强调在多个领域应用扩散模型的整体情况。具体而言，本文首先简要介绍了音频和扩散模型的背景。对于文本到语音任务，我们将方法分为三类，基于扩散模型采用的阶段：声学模型、声码器和端到端框架。此外，我们通过将某些信号从输入语音中删除或添加来将各种语音增强任务进行分类。本文还涵盖了实验结果的比较和讨论。

    Generative AI has demonstrated impressive performance in various fields, among which speech synthesis is an interesting direction. With the diffusion model as the most popular generative model, numerous works have attempted two active tasks: text to speech and speech enhancement. This work conducts a survey on audio diffusion model, which is complementary to existing surveys that either lack the recent progress of diffusion-based speech synthesis or highlight an overall picture of applying diffusion model in multiple fields. Specifically, this work first briefly introduces the background of audio and diffusion model. As for the text-to-speech task, we divide the methods into three categories based on the stage where diffusion model is adopted: acoustic model, vocoder and end-to-end framework. Moreover, we categorize various speech enhancement tasks by either certain signals are removed or added into the input speech. Comparisons of experimental results and discussions are also covered 
    
[^29]: 基于图的去中心化对抗性训练

    Decentralized Adversarial Training over Graphs. (arXiv:2303.13326v1 [cs.LG])

    [http://arxiv.org/abs/2303.13326](http://arxiv.org/abs/2303.13326)

    本文研究了在图上的去中心化对抗性训练，利用扩散学习的方法，开发了一种对抗性训练框架，增强了多个代理的鲁棒性以对抗攻击。

    

    近年来，机器学习模型对抗攻击的漏洞引起了广泛关注。大多数现有研究都集中在独立单一代理学习者的行为上。相比之下，本文研究了在图上的对抗性训练，其中各个单独的代理会受到空间中不同强度的扰动。预期通过链接代理和可能在图上实现的攻击模型的异质性，协调整个团队的强大协同作用可以帮助增强鲁棒性。本文使用扩散学习的极小-极大公式，为多代理系统开发了一种去中心化的对抗性训练框架。我们分析了该方案在凸和非凸环境下的收敛特性，并说明了增强的鲁棒性对抗攻击。

    The vulnerability of machine learning models to adversarial attacks has been attracting considerable attention in recent years. Most existing studies focus on the behavior of stand-alone single-agent learners. In comparison, this work studies adversarial training over graphs, where individual agents are subjected to perturbations of varied strength levels across space. It is expected that interactions by linked agents, and the heterogeneity of the attack models that are possible over the graph, can help enhance robustness in view of the coordination power of the group. Using a min-max formulation of diffusion learning, we develop a decentralized adversarial training framework for multi-agent systems. We analyze the convergence properties of the proposed scheme for both convex and non-convex environments, and illustrate the enhanced robustness to adversarial attacks.
    
[^30]: 复杂互动任务中的人类表现评估：一个基于深度生成多智能体模型的计算基准方法案例研究，以足球为例

    Deep Generative Multi-Agent Imitation Model as a Computational Benchmark for Evaluating Human Performance in Complex Interactive Tasks: A Case Study in Football. (arXiv:2303.13323v1 [stat.ML])

    [http://arxiv.org/abs/2303.13323](http://arxiv.org/abs/2303.13323)

    该论文使用基于生成模型的 AI 智能体作为计算基准，以评估人类在困难的涉及多个人类和情境因素的任务中的表现及发现有待改进的领域。该论文以足球表现分析为例，使用大型球员和球位置跟踪数据集训练生成模型，并成功进行足球比赛中的交互模仿。

    

    在许多应用中，如工程和体育中，评估人类的表现是常见的需求。评估人类在完成复杂的互动任务中的表现，最常见的方法是使用已被证明在该情境下有效的度量标准，或使用主观测量技术。然而，这可能是一个容易出错和不可靠的过程，因为静态度量标准无法捕捉到与这些任务相关的所有复杂情境，并且主观测量存在偏差。我们的研究的目标是创建基于数据驱动的 AI 智能体，作为计算基准来评估人类在解决涉及多个人类和情境因素的困难任务中的表现。我们在足球表现分析的背景下进行了演示。我们在大型球员和球位置跟踪数据集上训练了基于 Conditional Variational Recurrent Neural Network（VRNN）模型的生成模型。训练后的模型用于模仿两个团队在足球比赛中的交互。我们的结果表明，生成模型能够模拟比赛中逼真的球员移动和团队之间的交互。这个模型可以用作评估人类足球表现的基准，并确定需要提高的领域。

    Evaluating the performance of human is a common need across many applications, such as in engineering and sports. When evaluating human performance in completing complex and interactive tasks, the most common way is to use a metric having been proved efficient for that context, or to use subjective measurement techniques. However, this can be an error prone and unreliable process since static metrics cannot capture all the complex contexts associated with such tasks and biases exist in subjective measurement. The objective of our research is to create data-driven AI agents as computational benchmarks to evaluate human performance in solving difficult tasks involving multiple humans and contextual factors. We demonstrate this within the context of football performance analysis. We train a generative model based on Conditional Variational Recurrent Neural Network (VRNN) Model on a large player and ball tracking dataset. The trained model is used to imitate the interactions between two te
    
[^31]: 论如何解决不同解释方法所带来的问题：通过训练目标达成解释一致性

    Reckoning with the Disagreement Problem: Explanation Consensus as a Training Objective. (arXiv:2303.13299v1 [cs.LG])

    [http://arxiv.org/abs/2303.13299](http://arxiv.org/abs/2303.13299)

    本文提出针对后续特征归因方法所存在的不同解释的问题，引入PEAR损失项，从而提升模型的解释一致性，达到模型行为的可理解和可信任。

    

    随着深度神经网络逐渐在高风险领域中做出关键决策，监控和解释其行为成为必需。后续特征归因方法是一种常用的解释方法，可为输入中的每个特征分配得分，以衡量其对模型输出的影响。在实践中，这种方法的一个主要局限是不同的解释方法对于哪些特征更重要可能有不同的看法。本文提出了一种考虑不同解释方法的训练模型方法，我们引入Post hoc Explainer Agreement Regularization (PEAR)损失项以提升解释一致性。我们在三个数据集上观察到我们可以使用此损失项训练模型，以在未看见的数据上获得解释一致性的提升。

    As neural networks increasingly make critical decisions in high-stakes settings, monitoring and explaining their behavior in an understandable and trustworthy manner is a necessity. One commonly used type of explainer is post hoc feature attribution, a family of methods for giving each feature in an input a score corresponding to its influence on a model's output. A major limitation of this family of explainers in practice is that they can disagree on which features are more important than others. Our contribution in this paper is a method of training models with this disagreement problem in mind. We do this by introducing a Post hoc Explainer Agreement Regularization (PEAR) loss term alongside the standard term corresponding to accuracy, an additional term that measures the difference in feature attribution between a pair of explainers. We observe on three datasets that we can train a model with this loss term to improve explanation consensus on unseen data, and see improved consensus
    
[^32]: 基于域凸博弈的通用性改进研究

    Improving Generalization with Domain Convex Game. (arXiv:2303.13297v1 [cs.CV])

    [http://arxiv.org/abs/2303.13297](http://arxiv.org/abs/2303.13297)

    本文提出了一种新的域泛化视角，将其重新解释为域之间的凸博弈，并通过鼓励每个多样化的域增强模型泛化和构建样本过滤器来提高域增强的有效性。

    

    域泛化（DG）通过学习具有多个源域的模型来缓解深度神经网络的差泛化能力。 DG的经典解决方案是域增强，它的普遍信仰是通过多样化源域有助于超出分布范围的泛化。然而，这些主张仅基于直观理解，缺乏数学证明。我们的探索实验表明模型泛化和域多样性之间的相关性可能不是严格正相关的，这限制了域增强的有效性。因此，本研究旨在保证和进一步增强该领域的有效性。为此，我们提出了一个新的DG视角，将其重新解释为域之间的凸博弈。通过基于超模性的巧妙设计正则化项来鼓励每个多样化的域增强模型泛化，并构建样本过滤器来消除低质量样本，同时有效地提高域增强的有效性。

    Domain generalization (DG) tends to alleviate the poor generalization capability of deep neural networks by learning model with multiple source domains. A classical solution to DG is domain augmentation, the common belief of which is that diversifying source domains will be conducive to the out-of-distribution generalization. However, these claims are understood intuitively, rather than mathematically. Our explorations empirically reveal that the correlation between model generalization and the diversity of domains may be not strictly positive, which limits the effectiveness of domain augmentation. This work therefore aim to guarantee and further enhance the validity of this strand. To this end, we propose a new perspective on DG that recasts it as a convex game between domains. We first encourage each diversified domain to enhance model generalization by elaborately designing a regularization term based on supermodularity. Meanwhile, a sample filter is constructed to eliminate low-qua
    
[^33]: 丰富神经网络训练数据集以提高最坏情况性能保证

    Enriching Neural Network Training Dataset to Improve Worst-Case Performance Guarantees. (arXiv:2303.13228v1 [cs.LG])

    [http://arxiv.org/abs/2303.13228](http://arxiv.org/abs/2303.13228)

    本文提出了一种算法来丰富神经网络训练数据集，从而减少最坏情况的违规，并提高其性能保证。

    

    机器学习算法，特别是神经网络（NNs），是用于近似非线性关系（例如AC-OPF）的有价值的工具，并在部署时实现几个数量级的加速。通常在电力系统文献中，神经网络是通过在训练过程之前生成的固定数据集进行训练的。本文证明，在训练过程中调整神经网络训练数据集可以提高神经网络的性能，并大幅减少其最坏情况违规。本文提出了一个算法，用于识别和丰富关键数据点的训练数据集，以减少最坏情况违规，提供具有改进最坏情况性能保证的神经网络。我们在四个测试电力系统中演示了我们算法的性能，范围从39个总线到162个总线。

    Machine learning algorithms, especially Neural Networks (NNs), are a valuable tool used to approximate non-linear relationships, like the AC-Optimal Power Flow (AC-OPF), with considerable accuracy -- and achieving a speedup of several orders of magnitude when deployed for use. Often in power systems literature, the NNs are trained with a fixed dataset generated prior to the training process. In this paper, we show that adapting the NN training dataset during training can improve the NN performance and substantially reduce its worst-case violations. This paper proposes an algorithm that identifies and enriches the training dataset with critical datapoints that reduce the worst-case violations and deliver a neural network with improved worst-case performance guarantees. We demonstrate the performance of our algorithm in four test power systems, ranging from 39-buses to 162-buses.
    
[^34]: 不要慌张：一种基于频率的方法检测DNN中的后门毒化样本。

    Don't FREAK Out: A Frequency-Inspired Approach to Detecting Backdoor Poisoned Samples in DNNs. (arXiv:2303.13211v1 [cs.CR])

    [http://arxiv.org/abs/2303.13211](http://arxiv.org/abs/2303.13211)

    本文研究了DNN对清洁样本和毒化样本的频率敏感性差异，提出了一种简单而有效的FREAK基于频率的毒化样本检测算法，可有效防御频率后门攻击和一些空间攻击。

    

    本文研究了当DNN面对清洁样本和毒化样本时的频率敏感性。我们的分析显示，这两种类型样本的频率敏感性存在显著差异。在此基础上，我们提出了FREAK，一种简单而有效的基于频率的毒化样本检测算法。我们的实验结果不仅证明了FREAK对频率后门攻击的有效性，也对一些空间攻击具有防御能力。我们的工作只是利用这些洞见的第一步。我们相信我们的分析和提出的防御机制将为未来的后门防御研究和开发提供基础。

    In this paper we investigate the frequency sensitivity of Deep Neural Networks (DNNs) when presented with clean samples versus poisoned samples. Our analysis shows significant disparities in frequency sensitivity between these two types of samples. Building on these findings, we propose FREAK, a frequency-based poisoned sample detection algorithm that is simple yet effective. Our experimental results demonstrate the efficacy of FREAK not only against frequency backdoor attacks but also against some spatial attacks. Our work is just the first step in leveraging these insights. We believe that our analysis and proposed defense mechanism will provide a foundation for future research and development of backdoor defenses.
    
[^35]: 使用少量特征实现可解释的图像分类

    Take 5: Interpretable Image Classification with a Handful of Features. (arXiv:2303.13166v1 [cs.CV])

    [http://arxiv.org/abs/2303.13166](http://arxiv.org/abs/2303.13166)

    人们常常无法理解深度学习模型的决策过程，我们提出了一种Sparse Low-Dimensional Decision模型，它只使用一小部分可解释的特征进行决策，这使得该模型更容易被人理解，同时也具有与其他密集高维模型相似的准确性。

    

    深度神经网络使用数千个大多不可理解的特征来识别单个类别，这是任何人都无法理解的决策。我们提出了在深度神经网络中采用可解释的稀疏低维决策层，能够量化可解释性，具有可解释性，并在细颗粒度图像分类中进行了演示。我们认为，只有当特征是可解释的，并且只有极少量的特征用于单个决策时，人才能理解机器学习模型的决策。为此，最后一层必须是稀疏且维数较低的。我们将具有稀疏低维决策的模型称为Sparse Low-Dimensional Decision（SLDD）模型。我们展示了，相比密集高维决策层，SLDD模型在本地和全局上更容易解释，并能够保持竞争性准确性。此外，我们提出了一种可以提高模型特征多样性和准确性的损失函数。

    Deep Neural Networks use thousands of mostly incomprehensible features to identify a single class, a decision no human can follow. We propose an interpretable sparse and low dimensional final decision layer in a deep neural network with measurable aspects of interpretability and demonstrate it on fine-grained image classification. We argue that a human can only understand the decision of a machine learning model, if the features are interpretable and only very few of them are used for a single decision. For that matter, the final layer has to be sparse and, to make interpreting the features feasible, low dimensional. We call a model with a Sparse Low-Dimensional Decision SLDD-Model. We show that a SLDD-Model is easier to interpret locally and globally than a dense high-dimensional decision layer while being able to maintain competitive accuracy. Additionally, we propose a loss function that improves a model's feature diversity and accuracy. Our more interpretable SLDD-Model only uses 5
    
[^36]: 连续学习的绝热重放

    Adiabatic replay for continual learning. (arXiv:2303.13157v1 [cs.LG])

    [http://arxiv.org/abs/2303.13157](http://arxiv.org/abs/2303.13157)

    本研究提出了一种称为绝热重放的重放连续学习策略，它能够有选择性地重放与新数据相似的样本，从而提高学习效率。

    

    传统的基于重放的连续学习方法需要在每个新数据的学习阶段重放代表先前学习到的所有知识的样本，以避免灾难性遗忘。由于在连续学习问题中学到的知识量随时间增长，生成式重放会花费越来越多的时间来重新学习已知内容。在这个概念验证的研究中，我们提出了一种我们称之为绝热重放（AR）的重放连续学习策略，其效率来自于（合理的）假设每个新的学习阶段都是绝热的，即仅代表现有知识的小幅增加。每个新的学习阶段会触发一个选择性重放的采样过程，从现有知识库中选择相似于新数据的样本进行重放，而不是全部重放。完全重放不是必须的，因为AR通过GMMs表示数据分布，这些分布能够有选择性地更新它们的分布。

    Conventional replay-based approaches to continual learning (CL) require, for each learning phase with new data, the replay of samples representing all of the previously learned knowledge in order to avoid catastrophic forgetting. Since the amount of learned knowledge grows over time in CL problems, generative replay spends an increasing amount of time just re-learning what is already known. In this proof-of-concept study, we propose a replay-based CL strategy that we term adiabatic replay (AR), which derives its efficiency from the (reasonable) assumption that each new learning phase is adiabatic, i.e., represents only a small addition to existing knowledge. Each new learning phase triggers a sampling process that selectively replays, from the body of existing knowledge, just such samples that are similar to the new data, in contrast to replaying all of it. Complete replay is not required since AR represents the data distribution by GMMs, which are capable of selectively updating their
    
[^37]: FedGH:异构联邦学习与广义全局头

    FedGH: Heterogeneous Federated Learning with Generalized Global Header. (arXiv:2303.13137v1 [cs.LG])

    [http://arxiv.org/abs/2303.13137](http://arxiv.org/abs/2303.13137)

    FedGH是一种异构联邦学习方法，可以使客户端持有具有不同结构的模型，通过训练共享的广义全局预测头来提高效率和性能。

    

    联邦学习(Federated learning, FL)是一种新兴的机器学习范式，允许多个参与方在隐私保护的情况下协作训练共享模型。现有横向FL方法通常假定FL服务器和客户端持有相同的模型结构。然而，由于系统异构和个性化需求，使得允许客户端持有具有不同结构的模型已成为一个重要的方向。现有的模型异构FL方法通常需要公开可用的数据集，并产生高通信和/或计算成本，这限制了它们的性能。为解决这些限制，我们提出了联邦全局预测头(FedGH)方法。它是一种通信和计算效率高的模型异构FL框架，通过在FL服务器上对客户端模型提取的表示进行训练来训练共享的广义全局预测头。通过FedGH训练的广义全局预测头可以直接部署在客户端设备上，以实现高效的本地推理。我们在基准数据集和模型上的实验表明，FedGH在准确性、通信效率和模型个性化能力方面优于现有的模型异构FL方法。

    Federated learning (FL) is an emerging machine learning paradigm that allows multiple parties to train a shared model collaboratively in a privacy-preserving manner. Existing horizontal FL methods generally assume that the FL server and clients hold the same model structure. However, due to system heterogeneity and the need for personalization, enabling clients to hold models with diverse structures has become an important direction. Existing model-heterogeneous FL approaches often require publicly available datasets and incur high communication and/or computational costs, which limit their performances. To address these limitations, we propose the Federated Global prediction Header (FedGH) approach. It is a communication and computation-efficient model-heterogeneous FL framework which trains a shared generalized global prediction header with representations extracted by heterogeneous extractors for clients' models at the FL server. The trained generalized global prediction header lear
    
[^38]: 拉普拉斯分割网络: 从空间数据不确定性到改进的认知不确定性

    Laplacian Segmentation Networks: Improved Epistemic Uncertainty from Spatial Aleatoric Uncertainty. (arXiv:2303.13123v1 [cs.CV])

    [http://arxiv.org/abs/2303.13123](http://arxiv.org/abs/2303.13123)

    所提出的拉普拉斯分割网络可同时捕获图像分割中的认知和随机不确定性，成功将高认知不确定性分配到OOF目标中。

    

    媒体图像常常会出现非正常的情况，例如因为位置或扫描器的不同或图像损坏等原因而经常出现，这种媒体图像可能会对下游临床诊断或治疗产生影响。为了确保对这种错误分割的鲁棒性，我们提出了拉普拉斯分割网络（LSN），其能够共同建模图像分割中的认知（模型）不确定性和空间数据（随机）不确定性。我们使用具有空间相关性的logit分布捕获数据的不确定性。对于模型不确定性，我们提出了针对高维输出和具有跳过连接的大型神经网络的第一个拉普拉斯权重后验的逼近。从实证上，我们证明了建模空间像素相关性使得拉普拉斯分割网络能够将高认知不确定性成功分配到图像中的OOF目标。

    Out of distribution (OOD) medical images are frequently encountered, e.g. because of site- or scanner differences, or image corruption. OOD images come with a risk of incorrect image segmentation, potentially negatively affecting downstream diagnoses or treatment. To ensure robustness to such incorrect segmentations, we propose Laplacian Segmentation Networks (LSN) that jointly model epistemic (model) and aleatoric (data) uncertainty in image segmentation. We capture data uncertainty with a spatially correlated logit distribution. For model uncertainty, we propose the first Laplace approximation of the weight posterior that scales to large neural networks with skip connections that have high-dimensional outputs. Empirically, we demonstrate that modelling spatial pixel correlation allows the Laplacian Segmentation Network to successfully assign high epistemic uncertainty to out-of-distribution objects appearing within images.
    
[^39]: RLOR:一种灵活的深度强化学习框架，用于运筹学

    RLOR: A Flexible Framework of Deep Reinforcement Learning for Operation Research. (arXiv:2303.13117v1 [math.OC])

    [http://arxiv.org/abs/2303.13117](http://arxiv.org/abs/2303.13117)

    本文提出了灵活的深度强化学习框架RLOR，能够用于各种运筹学问题。我们重新实现了车辆路径问题的自回归模型，并展示了这些模型可以从强化学习的最新进展中受益，同时也提高了训练速度。

    

    强化学习已经应用于运筹学中，显示出在解决大型组合优化问题方面的潜力。然而，现有的研究侧重于针对某些问题开发神经网络架构，这些研究缺乏将强化学习的最新进展和自定义模型架构用于运筹学问题的灵活性。在本研究中，我们分析了车辆路径问题的端到端自回归模型，并展示了这些模型可以通过仔细重新实施模型架构来获益于强化学习的最新进展。特别地，我们重新实现了注意力模型，并在CleanRL中使用近端策略优化(PPO)进行训练，展示出至少8倍的训练时间加速。我们在此引入了RLOR，一种用于运筹学的灵活深度强化学习框架。我们相信灵活的框架对于开发各种运筹学问题的深度强化学习模型至关重要。

    Reinforcement learning has been applied in operation research and has shown promise in solving large combinatorial optimization problems. However, existing works focus on developing neural network architectures for certain problems. These works lack the flexibility to incorporate recent advances in reinforcement learning, as well as the flexibility of customizing model architectures for operation research problems. In this work, we analyze the end-to-end autoregressive models for vehicle routing problems and show that these models can benefit from the recent advances in reinforcement learning with a careful re-implementation of the model architecture. In particular, we re-implemented the Attention Model and trained it with Proximal Policy Optimization (PPO) in CleanRL, showing at least 8 times speed up in training time. We hereby introduce RLOR, a flexible framework for Deep Reinforcement Learning for Operation Research. We believe that a flexible framework is key to developing deep re
    
[^40]: 适应性正则化在类增量学习中的应用

    Adaptive Regularization for Class-Incremental Learning. (arXiv:2303.13113v1 [cs.LG])

    [http://arxiv.org/abs/2303.13113](http://arxiv.org/abs/2303.13113)

    本文研究了适应性正则化在类增量学习中的应用，通过根据任务复杂度动态调整正则化强度，在学习新类别同时防止遗忘之前学习的类别。实验表明适应性正则化可以实现更加准确和不易遗忘的视觉增量学习。

    

    类增量学习是指在维持先前学习的分类准确度的同时，更新具有新类别的深度分类器。在学习新类别的同时，通过正则化神经网络权重来防止遗忘之前学习的类别是常见的方法。然而，现有的正则化方法在整个增量学习过程中使用恒定的强度，可能无法反映所遇到的任务难度的变化。因此，本研究探讨了适应性正则化在类增量学习中的必要性，该方法根据手头任务的复杂度动态调整正则化强度。我们提出了一种基于贝叶斯优化的方法，自动确定每个学习任务的最佳正则化强度。通过两个数据集上的两种正则化方法的实验，结果表明适应性正则化对于实现更加准确和不易遗忘的视觉增量学习非常重要。

    Class-Incremental Learning updates a deep classifier with new categories while maintaining the previously observed class accuracy. Regularizing the neural network weights is a common method to prevent forgetting previously learned classes while learning novel ones. However, existing regularizers use a constant magnitude throughout the learning sessions, which may not reflect the varying levels of difficulty of the tasks encountered during incremental learning. This study investigates the necessity of adaptive regularization in Class-Incremental Learning, which dynamically adjusts the regularization strength according to the complexity of the task at hand. We propose a Bayesian Optimization-based approach to automatically determine the optimal regularization magnitude for each learning task. Our experiments on two datasets via two regularizers demonstrate the importance of adaptive regularization for achieving accurate and less forgetful visual incremental learning.
    
[^41]: 关键点引导的最优输运模型

    Keypoint-Guided Optimal Transport. (arXiv:2303.13102v1 [cs.CV])

    [http://arxiv.org/abs/2303.13102](http://arxiv.org/abs/2303.13102)

    本文提出了一种关键点引导的最优输运模型，通过掩模约束和关键点的关系指导匹配，并可用Sinkhorn算法求解。

    

    现有的最优输运方法主要通过输运成本/距离最小化来推导最优的输运计划/匹配，但这种方法在某些情况下可能导致错误的匹配。在许多应用程序中，在域之间注释一些匹配的关键点是合理的，甚至是不费力的。因此，研究如何利用注释的关键点来指导OT中的正确匹配具有价值。本文提出了一种新型的基于关键点引导的最优输运模型，通过关键点引导寻找最优匹配（即输运计划）。为了在OT中使用关键点，首先，我们提出了一种基于掩模的输运计划约束，用于保留关键点成对的匹配关系。其次，我们提出了保留每个数据点与关键点的关系以指导匹配的方法。所提出的KPG-RL模型可以通过Sinkhorn的算法求解，并且在分布在不同空间上时也适用。

    Existing Optimal Transport (OT) methods mainly derive the optimal transport plan/matching under the criterion of transport cost/distance minimization, which may cause incorrect matching in some cases. In many applications, annotating a few matched keypoints across domains is reasonable or even effortless in annotation burden. It is valuable to investigate how to leverage the annotated keypoints to guide the correct matching in OT. In this paper, we propose a novel KeyPoint-Guided model by ReLation preservation (KPG-RL) that searches for the optimal matching (i.e., transport plan) guided by the keypoints in OT. To impose the keypoints in OT, first, we propose a mask-based constraint of the transport plan that preserves the matching of keypoint pairs. Second, we propose to preserve the relation of each data point to the keypoints to guide the matching. The proposed KPG-RL model can be solved by Sinkhorn's algorithm and is applicable even when distributions are supported in different spac
    
[^42]: 随机梯度下降的概率稳定性

    The Probabilistic Stability of Stochastic Gradient Descent. (arXiv:2303.13093v1 [cs.LG])

    [http://arxiv.org/abs/2303.13093](http://arxiv.org/abs/2303.13093)

    本文重新定义了随机梯度下降的稳定性，并使用概率稳定性来回答深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。

    

    深度学习理论中的一个基本开放问题是如何定义和理解随机梯度下降(SGD)接近固定点的稳定性。传统文献依赖于参数统计矩，特别是参数方差的收敛来量化稳定性。本文重新定义了SGD的稳定性，并使用\textit{概率收敛}条件来定义SGD的\textit{概率稳定性}。提出的稳定性直接回答了深度学习理论中的一个根本问题：SGD如何从数量庞大的可能严重过拟合的解中选择有意义的神经网络解。为了实现这一点，我们表明只有在概率性稳定性的镜头下，SGD才表现出丰富而实际相关的学习阶段，如完全失去稳定性阶段、不正确学习阶段、收敛到低秩鞍点阶段和正确学习阶段。当应用于神经网络时，这些相图意味着具有实际意义的稳定和不稳定区域。

    A fundamental open problem in deep learning theory is how to define and understand the stability of stochastic gradient descent (SGD) close to a fixed point. Conventional literature relies on the convergence of statistical moments, esp., the variance, of the parameters to quantify the stability. We revisit the definition of stability for SGD and use the \textit{convergence in probability} condition to define the \textit{probabilistic stability} of SGD. The proposed stability directly answers a fundamental question in deep learning theory: how SGD selects a meaningful solution for a neural network from an enormous number of solutions that may overfit badly. To achieve this, we show that only under the lens of probabilistic stability does SGD exhibit rich and practically relevant phases of learning, such as the phases of the complete loss of stability, incorrect learning, convergence to low-rank saddles, and correct learning. When applied to a neural network, these phase diagrams imply t
    
[^43]: Box-Level  Active Detection：一种基于边界框的主动学习目标检测方法

    Box-Level Active Detection. (arXiv:2303.13089v1 [cs.CV])

    [http://arxiv.org/abs/2303.13089](http://arxiv.org/abs/2303.13089)

    该论文提出了一种基于边界框的主动学习目标检测方法，引入了一个控制每个周期基于边界框预算的框架 ComPAS，它可以优先考虑有信息量的目标，并避免冗余以进行公平比较和高效应用。

    

    主动学习在确定预算内的基础上选择信息样本进行注释，近年来已经被证明在目标检测中高效。然而，广泛使用的主动检测基准在图像级别上进行评估，这在人工工作量估计中是不现实的，并且对拥挤图像存在偏差。此外，现有方法仍然执行图像级别注释，但是对于相同图像中的所有目标平等评分会浪费预算并产生冗余标签。鉴于以上问题和限制，我们引入了一种基于边界框的主动检测框架，在每个周期内控制基于边界框的预算，优先考虑信息目标并避免冗余以进行公平比较和高效应用。在提出的基于边界框的方案下，我们设计了一种新的流程，即互补伪主动策略（ComPAS）。 它以互补方式利用人类注释和模型智能：高效的输入端委员会仅查询信息对象的标签。

    Active learning selects informative samples for annotation within budget, which has proven efficient recently on object detection. However, the widely used active detection benchmarks conduct image-level evaluation, which is unrealistic in human workload estimation and biased towards crowded images. Furthermore, existing methods still perform image-level annotation, but equally scoring all targets within the same image incurs waste of budget and redundant labels. Having revealed above problems and limitations, we introduce a box-level active detection framework that controls a box-based budget per cycle, prioritizes informative targets and avoids redundancy for fair comparison and efficient application.  Under the proposed box-level setting, we devise a novel pipeline, namely Complementary Pseudo Active Strategy (ComPAS). It exploits both human annotations and the model intelligence in a complementary fashion: an efficient input-end committee queries labels for informative objects only
    
[^44]: MSAT: 生物启发的多阶段自适应阈值用于脉冲神经网络的转换

    MSAT: Biologically Inspired Multi-Stage Adaptive Threshold for Conversion of Spiking Neural Networks. (arXiv:2303.13080v1 [cs.NE])

    [http://arxiv.org/abs/2303.13080](http://arxiv.org/abs/2303.13080)

    该论文提出了一种生物启发的多阶段自适应阈值方法用于转化人工神经网络为脉冲神经网络（SNN），该方法可以使得神经元更快地传递脉冲并传输更多信息，在精度和效率方面均优于现有的最新方法。

    

    由于脉冲稀疏性，脉冲神经网络（SNN）可以以低功耗进行推理。ANN-SNN转换是将经过良好训练的人工神经网络（ANNs）转换为深度SNN的有效方法。然而，现有方法通常使用常数阈值进行转换，这会阻止神经元快速传递脉冲到更深的层，并导致高时间延迟。为了解决这个问题，我们提出了一个多阶段自适应阈值（MSAT）的生物学模型机制。对于每个神经元，动态阈值随着发放历史和输入属性而变化，并且与平均膜电位正相关，与去极化率负相关。这种自适应膜电位和输入的方式可以及时调整阈值，更快地传递脉冲并传输更多信息。此外，我们引入了一种新颖的权重自适应算法来微调转换后的SNN，这显著提高了ANN-SNN转换的性能。实验表明，我们的方法在准确性和效率方面均优于现有的最新方法。

    Spiking Neural Networks (SNNs) can do inference with low power consumption due to their spike sparsity. ANN-SNN conversion is an efficient way to achieve deep SNNs by converting well-trained Artificial Neural Networks (ANNs). However, the existing methods commonly use constant threshold for conversion, which prevents neurons from rapidly delivering spikes to deeper layers and causes high time delay. In addition, the same response for different inputs may result in information loss during the information transmission. Inspired by the biological model mechanism, we propose a multi-stage adaptive threshold (MSAT). Specifically, for each neuron, the dynamic threshold varies with firing history and input properties and is positively correlated with the average membrane potential and negatively correlated with the rate of depolarization. The self-adaptation to membrane potential and input allows a timely adjustment of the threshold to fire spike faster and transmit more information. Moreover
    
[^45]: 使用深度学习预测宇宙的初始条件

    Predicting the Initial Conditions of the Universe using Deep Learning. (arXiv:2303.13056v1 [astro-ph.CO])

    [http://arxiv.org/abs/2303.13056](http://arxiv.org/abs/2303.13056)

    本文首次证明了使用深度学习模型可以反向预测宇宙初始线性位移，该方法能够在减少计算量的同时准确恢复初始线性位移。

    

    找到导致当前宇宙状态的初始条件是具有挑战性的，因为它涉及到搜索一个巨大的初始条件输入空间，并通过诸如N-体模拟等工具建模它们的演化，这是计算上昂贵的。深度学习已经成为一种替代建模工具，它可以学习N-体模拟的线性输入与红移为零时的最终非线性位移之间的映射，这可以显著加速向前的模拟。但是，这并不能减少初始条件的搜索空间。在本文中，我们首次证明了深度学习模型可以被用于反向映射。我们训练了一个基于V-Net的卷积神经网络，它可以在给定系统当前时间的非线性位移和宇宙学参数的情况下输出N-体系统的线性位移。我们证明了这个神经网络可以准确地恢复初始线性位移。

    Finding the initial conditions that led to the current state of the universe is challenging because it involves searching over a vast input space of initial conditions, along with modeling their evolution via tools such as N-body simulations which are computationally expensive. Deep learning has emerged as an alternate modeling tool that can learn the mapping between the linear input of an N-body simulation and the final nonlinear displacements at redshift zero, which can significantly accelerate the forward modeling. However, this does not help reduce the search space for initial conditions. In this paper, we demonstrate for the first time that a deep learning model can be trained for the reverse mapping. We train a V-Net based convolutional neural network, which outputs the linear displacement of an N-body system, given the current time nonlinear displacement and the cosmological parameters of the system. We demonstrate that this neural network accurately recovers the initial linear 
    
[^46]: 利用深度学习方法重新设计应用程序用户界面的挑战和机遇

    Reimagining Application User Interface (UI) Design using Deep Learning Methods: Challenges and Opportunities. (arXiv:2303.13055v1 [cs.HC])

    [http://arxiv.org/abs/2303.13055](http://arxiv.org/abs/2303.13055)

    深度学习方法在用户界面设计中的最新研究进展，将推动软件开发行业进步的高潜力领域。

    

    本文概述了深度学习方法在用户界面设计中的最新研究进展，涵盖了深度神经网络、卷积神经网络、循环神经网络、自编码器和生成对抗网络等众所周知的深度学习技术和广泛使用的数据集。我们重点介绍了该领域的重要问题和新兴研究前沿。我们认为，利用深度学习进行用户界面设计自动化任务可能是推动软件开发行业进步的高潜力领域。

    In this paper, we present a review of the recent work in deep learning methods for user interface design. The survey encompasses well known deep learning techniques (deep neural networks, convolutional neural networks, recurrent neural networks, autoencoders, and generative adversarial networks) and datasets widely used to design user interface applications. We highlight important problems and emerging research frontiers in this field. We believe that the use of deep learning for user interface design automation tasks could be one of the high potential fields for the advancement of the software development industry.
    
[^47]: 向更好的动态图学习迈进：新的架构和统一库

    Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])

    [http://arxiv.org/abs/2303.13047](http://arxiv.org/abs/2303.13047)

    我们提出了一种基于Transformer的新型动态图学习架构DyGFormer，并引入了统一的库DyGLib，以促进可重复、可扩展和可信的动态图学习研究。DyGFormer通过邻居共现编码方案和分块技术实现更长期历史的高效推理，在13个不同领域的数据集上实现了最先进的性能。

    

    我们提出了DyGFormer，这是一种基于Transformer的新型动态图学习架构，仅从节点历史的第一跳交互序列中学习。DyGFormer结合了两种不同的设计：一种邻居共现编码方案，探索源节点和目标节点基于它们的序列的相关性；一种分块技术，将每个序列分成多个块并将其馈送给Transformer，使模型能够有效而高效地受益于更长期的历史。我们还引入了DyGLib，这是一个统一的库，具有标准的训练管道、可扩展的编码接口和综合的评估协议，以促进可重复、可伸缩和可信的动态图学习研究。通过在来自各个领域的13个数据集上执行广泛的实验，进行推导/归纳动态链接预测和动态节点分类任务，我们观察到：DyGFormer在mo上实现了最先进的性能

    We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo
    
[^48]: SPeC：软提示校准在临床笔记摘要中降低性能变异的研究

    SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])

    [http://arxiv.org/abs/2303.13035](http://arxiv.org/abs/2303.13035)

    研究通过引入软提示嵌入，提出Soft Prompt-Based Calibration (SPeC)管道，来减轻输入变量对输出多样性的影响，降低性能变异. 此方法不仅比大语言模型(LLM)性能稳定，而且在临床笔记摘要任务上表现优于最先进的模型.

    

    电子健康记录（EHR）存储着包括病历、诊断、治疗和检测结果在内的大量患者信息。这些记录对于医疗保健专业人员做出明智的患者护理决策非常关键。摘要临床笔记可以帮助医疗保健专业人员更好地发现潜在健康风险，以及做出更好的决策。这一过程通过确保医疗保健专业人员可以访问最相关和最新的患者数据，有助于减少错误并提高患者的护理效果。最近的研究表明，将提示与大语言模型（LLM）相结合可以显著提高摘要任务的效率。然而，我们发现这种方法也会导致输出方差增加，即使提示意义相似，输出也会有明显的差异。为了解决这一挑战，我们引入了一个模型无关的软提示校准（SPeC）流程，该流程采用软提示嵌入来减轻输入变量对输出多样性的影响。我们的实验表明，SPeC不仅可以降低LLM的性能变异，而且在临床笔记摘要任务上优于现有的最先进模型。

    Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
    
[^49]: 偏好感知的约束多目标贝叶斯优化

    Preference-Aware Constrained Multi-Objective Bayesian Optimization. (arXiv:2303.13034v1 [cs.LG])

    [http://arxiv.org/abs/2303.13034](http://arxiv.org/abs/2303.13034)

    PAC-MOO是一个偏好感知的约束多目标贝叶斯优化方法，可以有效地解决在黑盒目标函数和从业者指定的目标偏好下，大部分输入空间是不可行的约束多目标优化问题。

    

    本文解决了在大部分输入空间是不可行（即违反约束条件）时，基于黑盒目标函数和从业者指定的目标偏好的约束多目标优化问题。这个问题在许多工程设计问题中都存在，包括模拟电路和电力系统设计。我们的总体目标是在可行的输入设计的小部分上近似最优Pareto集合。主要挑战包括设计空间的巨大大小、多个目标和大量的约束条件以及只能在进行昂贵的仿真后才能确认的可行的输入设计的小部分。我们提出了一种新颖而有效的偏好感知的约束多目标贝叶斯优化方法（PAC-MOO）来解决这些挑战。关键思想是学习输出目标和约束的代理模型，并根据从业者预测的偏好选择评估目标的候选输入。PAC-MOO根据预测的目标和约束的联合偏好迭代地选择下一个要模拟的输入设计，并使用新获得的数据更新代理模型。实验结果表明，与现有的最先进方法相比，所提出的方法具有有效性和效率。

    This paper addresses the problem of constrained multi-objective optimization over black-box objective functions with practitioner-specified preferences over the objectives when a large fraction of the input space is infeasible (i.e., violates constraints). This problem arises in many engineering design problems including analog circuits and electric power system design. Our overall goal is to approximate the optimal Pareto set over the small fraction of feasible input designs. The key challenges include the huge size of the design space, multiple objectives and large number of constraints, and the small fraction of feasible input designs which can be identified only after performing expensive simulations. We propose a novel and efficient preference-aware constrained multi-objective Bayesian optimization approach referred to as PAC-MOO to address these challenges. The key idea is to learn surrogate models for both output objectives and constraints, and select the candidate input for eva
    
[^50]: 用于识别TBI生理状态的多元时间序列数据的自监督聚类

    Self-Supervised Clustering of Multivariate Time-Series Data for Identifying TBI Physiological States. (arXiv:2303.13024v1 [cs.LG])

    [http://arxiv.org/abs/2303.13024](http://arxiv.org/abs/2303.13024)

    这篇论文提出了一种新的自监督聚类算法，能够在多元时间序列数据中确定并识别对于TBI等急性疾病治疗非常重要的生理状态。研究还利用临床数据验证并解释所识别的生理状态。

    

    对于具有缺失值的多元时间序列数据确定临床相关的生理状态非常重要，这对于提供急性疾病（如颅脑损伤、呼吸衰竭和心力衰竭）的适当治疗至关重要。利用非时间序列聚类或数据插值和聚合技术可能导致有价值信息的丢失和偏见分析。在本研究中，我们应用了基于自监督的SLAC-Time算法，避免了插值或聚合，从而更有效地表示急性患者状态。通过使用SLAC-Time来聚类大型研究数据集中的数据，我们确定了三种不同的TBI生理状态及其具体特征。我们采用了各种聚类评估指标，并结合临床领域专家的意见来验证和解释所识别的生理状态。此外，我们发现了特定临床事件和生理状态之间的关系。

    Determining clinically relevant physiological states from multivariate time series data with missing values is essential for providing appropriate treatment for acute conditions such as Traumatic Brain Injury (TBI), respiratory failure, and heart failure. Utilizing non-temporal clustering or data imputation and aggregation techniques may lead to loss of valuable information and biased analyses. In our study, we apply the SLAC-Time algorithm, an innovative self-supervision-based approach that maintains data integrity by avoiding imputation or aggregation, offering a more useful representation of acute patient states. By using SLAC-Time to cluster data in a large research dataset, we identified three distinct TBI physiological states and their specific feature profiles. We employed various clustering evaluation metrics and incorporated input from a clinical domain expert to validate and interpret the identified physiological states. Further, we discovered how specific clinical events and
    
[^51]: ENVIDR: 具有神经环境光照的隐式可微分渲染器

    ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting. (arXiv:2303.13022v1 [cs.CV])

    [http://arxiv.org/abs/2303.13022](http://arxiv.org/abs/2303.13022)

    本文介绍了一种名为ENVIDR的渲染和建模框架，在渲染具有挑战性的镜面反射表面方面表现出卓越性能，集成了神经渲染器和基于SDF的神经表面模型。

    

    近期神经渲染的进展显示出从多视图图像重建场景的巨大潜力，然而现有方法在准确表示具有光泽表面的物体方面仍然存在挑战。本文提出了 ENVIDR，一种用于高质量渲染和重构具有挑战性的镜面反射表面的渲染和建模框架。为了实现这一目标，我们首先提出了一种新颖的神经渲染器，并采用分解渲染组件来学习表面和环境光照的相互作用。该渲染器使用现有的物理渲染器进行训练，并与实际场景表示分离。接着，我们提出了一种基于 SDF 的神经表面模型，利用这个已学习的神经渲染器来表示一般的场景。我们的模型还通过行进表面反射的光线来合成由闪亮表面引起的间接光照。我们证明了我们的方法在处理具有光泽表面和镜面反射时，在具有挑战性的数据集上优于现有方法。

    Recent advances in neural rendering have shown great potential for reconstructing scenes from multiview images. However, accurately representing objects with glossy surfaces remains a challenge for existing methods. In this work, we introduce ENVIDR, a rendering and modeling framework for high-quality rendering and reconstruction of surfaces with challenging specular reflections. To achieve this, we first propose a novel neural renderer with decomposed rendering components to learn the interaction between surface and environment lighting. This renderer is trained using existing physically based renderers and is decoupled from actual scene representations. We then propose an SDF-based neural surface model that leverages this learned neural renderer to represent general scenes. Our model additionally synthesizes indirect illuminations caused by inter-reflections from shiny surfaces by marching surface-reflected rays. We demonstrate that our method outperforms state-of-art methods on chal
    
[^52]: 无线网络中容忍故障的分布式学习用于异常检测

    Failure-tolerant Distributed Learning for Anomaly Detection in Wireless Networks. (arXiv:2303.13015v1 [cs.LG])

    [http://arxiv.org/abs/2303.13015](http://arxiv.org/abs/2303.13015)

    本文提出了一种名为“Tol-FL”的新方法，通过结合扁平和星型拓扑结构的优势，增强了分布式网络中的变异检测性能和可靠性。

    

    大多数分布式技术的分析都是专注于它们的效率，而没有考虑它们的鲁棒性（或缺乏鲁棒性）。然而，这种考虑尤其重要，因为当设备或中央服务器出现故障时，这可能会瘫痪分布式系统。本文提出了一种通过结合扁平和星型拓扑结构来解决这些风险的新方法，将两者的性能和可靠性优势相结合。我们将这种方法称为“Tol-FL”，因为与联邦学习技术相比，它的故障容错能力提高了。我们的方法在客户端和服务器故障的各种逼真情况下，在异常检测AUROC方面优于先前方法高达8％，同时减少了设备故障风险。

    The analysis of distributed techniques is often focused upon their efficiency, without considering their robustness (or lack thereof). Such a consideration is particularly important when devices or central servers can fail, which can potentially cripple distributed systems. When such failures arise in wireless communications networks, important services that they use/provide (like anomaly detection) can be left inoperable and can result in a cascade of security problems. In this paper, we present a novel method to address these risks by combining both flat- and star-topologies, combining the performance and reliability benefits of both. We refer to this method as "Tol-FL", due to its increased failure-tolerance as compared to the technique of Federated Learning. Our approach both limits device failure risks while outperforming prior methods by up to 8% in terms of anomaly detection AUROC in a range of realistic settings that consider client as well as server failure, all while reducing
    
[^53]: 用于视觉模型诊断的语义图像攻击

    Semantic Image Attack for Visual Model Diagnosis. (arXiv:2303.13010v1 [cs.CV])

    [http://arxiv.org/abs/2303.13010](http://arxiv.org/abs/2303.13010)

    本文提出了一种新的基于对抗攻击的方法——语义图像攻击（SIA），可以提供语义对抗图像以便进行模型诊断、可解释性和鲁棒性。

    

    在实践中，对特定训练和测试数据集进行度量分析不能保证可靠或公平的机器学习模型。这部分原因是，获得平衡、多样和标记完美的数据集通常是昂贵、耗时和易出错的。本文提出了一种基于对抗攻击的方法——语义图像攻击（SIA），它提供了语义对抗图像，以便进行模型诊断、可解释性和鲁棒性。传统的对抗训练是一种增强机器学习模型对抗攻击的流行方法。然而，现有的对抗方法不结合两个方面，无法解释和分析模型的缺陷：语义可追溯性和感觉质量。SIA通过预定义的语义属性空间和图像空间上的迭代梯度上升结合了两个特征。我们证明了SIA的有效性。

    In practice, metric analysis on a specific train and test dataset does not guarantee reliable or fair ML models. This is partially due to the fact that obtaining a balanced, diverse, and perfectly labeled dataset is typically expensive, time-consuming, and error-prone. Rather than relying on a carefully designed test set to assess ML models' failures, fairness, or robustness, this paper proposes Semantic Image Attack (SIA), a method based on the adversarial attack that provides semantic adversarial images to allow model diagnosis, interpretability, and robustness. Traditional adversarial training is a popular methodology for robustifying ML models against attacks. However, existing adversarial methods do not combine the two aspects that enable the interpretation and analysis of the model's flaws: semantic traceability and perceptual quality. SIA combines the two features via iterative gradient ascent on a predefined semantic attribute space and the image space. We illustrate the validi
    
[^54]: 通过扩散可控地反向黑盒人脸识别模型状态

    Controllable Inversion of Black-Box Face-Recognition Models via Diffusion. (arXiv:2303.13006v1 [cs.CV])

    [http://arxiv.org/abs/2303.13006](http://arxiv.org/abs/2303.13006)

    ID3PM方法通过扩散过程的随机性质来产生黑盒人脸识别模型的高度可控的反演，能够生成逼真且多样的输出，适用于数据增强、对抗性攻击和生成个性化的训练数据等多种应用。

    

    人脸识别模型将人脸图像嵌入低维身份向量中，包含身份特征的抽象编码，这些特征允许区分个体。我们面临着在没有完全模型访问的情况下（即黑盒设置下）反向预训练人脸识别模型的潜在空间的挑战。已经有许多方法提出解决这个问题，但它们存在严重的缺陷，如缺乏现实输出、推理时间长以及对数据集和人脸识别模型的可访问性有强烈的要求。通过对黑盒反演问题的分析，我们展示了条件性扩散模型漏洞的自然涌现，并且即使没有身份特征的损失，我们也可以有效地从反向分布中进行采样。我们的方法名为身份去噪扩散概率模型（ID3PM），利用去噪扩散过程的随机性质来产生黑盒人脸识别模型的高度可控的反演。该方法能够生成逼真且多样的输出，使其适用于数据增强、对抗性攻击和生成个性化的训练数据等多种应用。

    Face recognition models embed a face image into a low-dimensional identity vector containing abstract encodings of identity-specific facial features that allow individuals to be distinguished from one another. We tackle the challenging task of inverting the latent space of pre-trained face recognition models without full model access (i.e. black-box setting). A variety of methods have been proposed in literature for this task, but they have serious shortcomings such as a lack of realistic outputs, long inference times, and strong requirements for the data set and accessibility of the face recognition model. Through an analysis of the black-box inversion problem, we show that the conditional diffusion model loss naturally emerges and that we can effectively sample from the inverse distribution even without an identity-specific loss. Our method, named identity denoising diffusion probabilistic model (ID3PM), leverages the stochastic nature of the denoising diffusion process to produce hi
    
[^55]: 对抗性对比条件神经过程估计

    Adversarially Contrastive Estimation of Conditional Neural Processes. (arXiv:2303.13004v1 [cs.LG])

    [http://arxiv.org/abs/2303.13004](http://arxiv.org/abs/2303.13004)

    本文提出了一种对抗性训练方案，通过噪声对比估计共同训练CNPs和EBM，并在各种数据集上改进了表现。

    

    条件神经过程(CNPs)通过准确的条件似然生成函数观测值，形成函数分布。然而，由于它们的预测分布被分解成一组无约束(通常为高斯分布)的输出，因此CNPs的表达能力对于高维度观测值是有限的。以前，可以使用潜变量或自回归似然来处理这个问题，但代价是难以训练和复杂度的平方级增加。我们提出了一种对抗性训练方案，使CNPs与常规的最大似然估计相结合。具体来说，我们使用噪声对比估计训练出一个能量基模型(EBM)，其要求EBM将真实观测值与CNP生成的样本区分出来。通过这种方式，CNP必须生成更接近基准答案的预测结果来欺骗EBM，而不仅仅是优化与固定形式似然有关的部分。从生成函数重构到下游回归任务，我们的方法在各种数据集上均表现优于强基线模型，这表明CNPs在建模复杂高维数据方面具有潜力。

    Conditional Neural Processes~(CNPs) formulate distributions over functions and generate function observations with exact conditional likelihoods. CNPs, however, have limited expressivity for high-dimensional observations, since their predictive distribution is factorized into a product of unconstrained (typically) Gaussian outputs. Previously, this could be handled using latent variables or autoregressive likelihood, but at the expense of intractable training and quadratically increased complexity. Instead, we propose calibrating CNPs with an adversarial training scheme besides regular maximum likelihood estimates. Specifically, we train an energy-based model (EBM) with noise contrastive estimation, which enforces EBM to identify true observations from the generations of CNP. In this way, CNP must generate predictions closer to the ground-truth to fool EBM, instead of merely optimizing with respect to the fixed-form likelihood. From generative function reconstruction to downstream regr
    
[^56]: 后训练量化的可靠性基准测试：特别关注最劣情况下的性能表现

    Benchmarking the Reliability of Post-training Quantization: a Particular Focus on Worst-case Performance. (arXiv:2303.13003v1 [cs.LG])

    [http://arxiv.org/abs/2303.13003](http://arxiv.org/abs/2303.13003)

    本论文探讨了后训练量化方法在极端情况下的可靠性问题，并在常用的PTQ方法上开展了系统评估。结果表明，大多数现有的PTQ方法在最劣情况下的性能表现不够可靠。因此需要开发更加强大的PTQ方法，以有效处理分布偏移和数据噪声，并改善最劣情况下的性能表现。

    

    后训练量化（PTQ）是一种流行的方法，用于压缩深度神经网络（DNNs），而不改变其原始结构或训练过程。尽管其有效性和便利性，但在存在某些极端情况（如分布偏移和数据噪声）下，PTQ方法的可靠性仍然很少被探索。本文首先在各种常用的PTQ方法上调查了这个问题。我们旨在回答与校准集分布变化、校准范式选择以及数据增强或采样策略对PTQ可靠性的影响相关的几个研究问题。在广泛的任务和常用的PTQ范例上进行了系统评估。结果显示，大多数现有的PTQ方法在最劣情况下的性能表现不够可靠，突出了需要更加强大的PTQ方法的需要。我们的发现为开发能够有效处理分布偏移和数据噪声，并改善PTQ方法最劣情况下性能的PTQ方法提供了一些见解。

    Post-training quantization (PTQ) is a popular method for compressing deep neural networks (DNNs) without modifying their original architecture or training procedures. Despite its effectiveness and convenience, the reliability of PTQ methods in the presence of some extrem cases such as distribution shift and data noise remains largely unexplored. This paper first investigates this problem on various commonly-used PTQ methods. We aim to answer several research questions related to the influence of calibration set distribution variations, calibration paradigm selection, and data augmentation or sampling strategies on PTQ reliability. A systematic evaluation process is conducted across a wide range of tasks and commonly-used PTQ paradigms. The results show that most existing PTQ methods are not reliable enough in term of the worst-case group performance, highlighting the need for more robust methods. Our findings provide insights for developing PTQ methods that can effectively handle distr
    
[^57]: 移动边缘网络中的自动联邦学习——快速适应和收敛

    Automated Federated Learning in Mobile Edge Networks -- Fast Adaptation and Convergence. (arXiv:2303.12999v1 [cs.LG])

    [http://arxiv.org/abs/2303.12999](http://arxiv.org/abs/2303.12999)

    本文使用基于MAML的FL设计来最小化整体学习时间，优化FL超参数（例如采样数据大小和通信轮数）和资源分配（例如发送功率），同时考虑模型精度和能量消耗的限制。

    

    联邦学习（FL）可以在移动边缘网络中用于分布式训练机器学习模型。最近，FL在模型无关元学习（MAML）框架中得到解释，这为FL带来了快速适应和收敛于异构数据集上的显着优势。然而，现有研究仅仅将MAML和FL结合起来，没有明确说明MAML给FL带来多少好处以及如何在移动边缘网络中最大化这种好处。

    Federated Learning (FL) can be used in mobile edge networks to train machine learning models in a distributed manner. Recently, FL has been interpreted within a Model-Agnostic Meta-Learning (MAML) framework, which brings FL significant advantages in fast adaptation and convergence over heterogeneous datasets. However, existing research simply combines MAML and FL without explicitly addressing how much benefit MAML brings to FL and how to maximize such benefit over mobile edge networks. In this paper, we quantify the benefit from two aspects: optimizing FL hyperparameters (i.e., sampled data size and the number of communication rounds) and resource allocation (i.e., transmit power) in mobile edge networks. Specifically, we formulate the MAML-based FL design as an overall learning time minimization problem, under the constraints of model accuracy and energy consumption. Facilitated by the convergence analysis of MAML-based FL, we decompose the formulated problem and then solve it using a
    
[^58]: 历史学习综述: 带有学习历史的学习模型

    A Survey of Historical Learning: Learning Models with Learning History. (arXiv:2303.12992v1 [cs.LG])

    [http://arxiv.org/abs/2303.12992](http://arxiv.org/abs/2303.12992)

    本文综述了“历史学习：带有学习历史的学习模型”这个主题，涵盖历史类型、功能部分和存储形式三个方面，是首个系统研究利用各种历史统计数据进行深度神经网络训练的综述论文。

    

    新的知识源于旧的知识。在训练历史记录中存储的各种类型的元素对于改善深度学习模型的学习非常有帮助。本文综述了“历史学习：带有学习历史的学习模型”这个主题，系统地研究从历史统计数据中进行深度神经网络训练的方法，涵盖历史类型、功能部分和存储形式三个方面。我们认为，这是首个系统研究利用各种历史统计数据进行深度神经网络训练的综述论文。文章还讨论了与此相关的话题，如循环/记忆网络、集成学习和强化学习。最后，我们还指出了这个主题的未来挑战，并鼓励学术界在设计算法时认真思考历史学习原则。

    New knowledge originates from the old. The various types of elements, deposited in the training history, are a large amount of wealth for improving learning deep models. In this survey, we comprehensively review and summarize the topic--``Historical Learning: Learning Models with Learning History'', which learns better neural models with the help of their learning history during its optimization, from three detailed aspects: Historical Type (what), Functional Part (where) and Storage Form (how). To our best knowledge, it is the first survey that systematically studies the methodologies which make use of various historical statistics when training deep neural networks. The discussions with related topics like recurrent/memory networks, ensemble learning, and reinforcement learning are demonstrated. We also expose future challenges of this topic and encourage the community to pay attention to the think of historical learning principles when designing algorithms. The paper list related to
    
[^59]: 连续不定概率神经网络

    Continuous Indeterminate Probability Neural Network. (arXiv:2303.12964v1 [cs.LG])

    [http://arxiv.org/abs/2303.12964](http://arxiv.org/abs/2303.12964)

    本文提出了CIPNN模型，该模型能够推导出连续潜在随机变量的解析解，同时提出了CIPAE自编码器，并通过可视化潜在随机变量的方法验证了模型的有效性。

    

    本文介绍了一种称为CIPNN（Continuous Indeterminate Probability Neural Network）的通用模型，该模型基于IPNN，用于离散潜在随机变量。目前，连续潜在变量的后验被认为是不可计算的，但是IPNN提出了新的理论，可以解决这个问题。本文的贡献有四个方面。首先，我们推导了连续潜在随机变量的后验计算的解析解，并提出了一个通用分类模型（CIPNN）。其次，我们提出了一种通用的自编码器——CIPAE（Continuous Indeterminate Probability Auto-Encoder），其中解码器部分不是神经网络，而是第一次使用全概率推理模型。第三，我们提出了一种新的可视化潜在随机变量的方法。我们使用N维潜在变量之一作为解码器来重建输入图像，即使是分类任务也能达到效果，这样，我们可以看到每个潜在变量代表什么。第四，我们通过MNIST和Fashion-MNIST数据集的实验证明了所提出模型的有效性。

    This paper introduces a general model called CIPNN - Continuous Indeterminate Probability Neural Network, and this model is based on IPNN, which is used for discrete latent random variables. Currently, posterior of continuous latent variables is regarded as intractable, with the new theory proposed by IPNN this problem can be solved. Our contributions are Four-fold. First, we derive the analytical solution of the posterior calculation of continuous latent random variables and propose a general classification model (CIPNN). Second, we propose a general auto-encoder called CIPAE - Continuous Indeterminate Probability Auto-Encoder, the decoder part is not a neural network and uses a fully probabilistic inference model for the first time. Third, we propose a new method to visualize the latent random variables, we use one of N dimensional latent variables as a decoder to reconstruct the input image, which can work even for classification tasks, in this way, we can see what each latent varia
    
[^60]: 预测感知的模型驱动LSTM

    Forecast-Aware Model Driven LSTM. (arXiv:2303.12963v1 [cs.LG])

    [http://arxiv.org/abs/2303.12963](http://arxiv.org/abs/2303.12963)

    本文旨在提出一种预测感知的模型驱动LSTM来解决空气质量预测中常见的偏差修正问题，此方法可以更好地处理极端空气质量事件。

    

    恶劣的空气质量会对人类健康产生严重影响。由于极端天气事件（如野火和热浪）增多，NOAA的空气质量预测受到了挑战。传统方法用于改正模型偏差时做了线性假设和基础分布假设，当出现极端空气质量事件时这些方法容易产生偏差修正过度或者不足。深度学习在非线性问题中广泛应用，通过学习通用性可以应对极端空气质量事件预测的需求。然而，当存在异常空气质量事件时，使用单个网络进行未来预测的标准深度网络方法不一定总是有效。

    Poor air quality can have a significant impact on human health. The National Oceanic and Atmospheric Administration (NOAA) air quality forecasting guidance is challenged by the increasing presence of extreme air quality events due to extreme weather events such as wild fires and heatwaves. These extreme air quality events further affect human health. Traditional methods used to correct model bias make assumptions about linearity and the underlying distribution. Extreme air quality events tend to occur without a strong signal leading up to the event and this behavior tends to cause existing methods to either under or over compensate for the bias. Deep learning holds promise for air quality forecasting in the presence of extreme air quality events due to its ability to generalize and learn nonlinear problems. However, in the presence of these anomalous air quality events, standard deep network approaches that use a single network for generalizing to future forecasts, may not always provi
    
[^61]: 临床基础模型的不稳定基础：针对 EMR 的大语言模型和基础模型的调查

    The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs. (arXiv:2303.12961v1 [cs.LG])

    [http://arxiv.org/abs/2303.12961](http://arxiv.org/abs/2303.12961)

    本论文回顾了超过80个在非成像 EMR 数据上训练的基础模型，发现这些模型大多范围有限、训练集有限，且评估指标未对其对医疗系统贡献提供有意义见解。因此，本研究提出了一种更接近于医疗保健重要指标的医疗基础模型效益评估框架。

    

    类似 ChatGPT 和 AlphaFold 的基础模型的成功引发了人们对于构建类似模型以改善 EMR（电子病历）以提高患者护理和医院运营的极大兴趣。然而，最近的炒作掩盖了我们对这些模型能力的关键缺失。我们回顾了超过80个在非成像 EMR 数据（即临床文本和/或结构化数据）上训练的基础模型，并创建了一个分类法来说明它们的体系结构、训练数据和潜在用例。我们发现大多数模型是在小型、范围有限的临床数据集（例如MIMIC-III）或广泛的公共生物医学语料库（例如PubMed）上进行训练的，并且在不提供对其对医疗系统有用处的有意义见解的任务上进行评估。基于这些发现，我们提出了一种更接近于医疗保健重要指标的医疗基础模型效益的改进评估框架。

    The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.
    
[^62]: 变分自编码器中逐步减少信息瓶颈的去纠缠方法

    Variantional autoencoder with decremental information bottleneck for disentanglement. (arXiv:2303.12959v1 [cs.LG])

    [http://arxiv.org/abs/2303.12959](http://arxiv.org/abs/2303.12959)

    本论文提出了一种逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来平衡去纠缠和重构保真度，避免信息扩散问题。

    

    变分自编码器中去纠缠学习的一个主要挑战是在权衡去纠缠和重构保真度之间的平衡。之前仅在一个潜在空间中进行的逐步方法无法同时优化这两个目标，因此在训练过程中扩展了信息瓶颈，以从去纠缠到重构进行优化。然而，大型瓶颈会失去去纠缠的约束，导致信息扩散问题。为了解决这个问题，我们提出了一种新颖的逐步减少信息瓶颈的变分自编码器方法，使用去纠缠不变变换来优化不同层的多个目标，称为DeVAE。通过逐渐减小不同潜在空间的信息瓶颈，DeVAE 平衡了去纠缠和重构保真度。由于具有多个潜在空间，DeVAE 允许同时优化多个目标，以在保持去纠缠约束的同时优化重构，避免信息扩散问题。

    One major challenge of disentanglement learning with variational autoencoders is the trade-off between disentanglement and reconstruction fidelity. Previous incremental methods with only on latent space cannot optimize these two targets simultaneously, so they expand the Information Bottleneck while training to {optimize from disentanglement to reconstruction. However, a large bottleneck will lose the constraint of disentanglement, causing the information diffusion problem. To tackle this issue, we present a novel decremental variational autoencoder with disentanglement-invariant transformations to optimize multiple objectives in different layers, termed DeVAE, for balancing disentanglement and reconstruction fidelity by decreasing the information bottleneck of diverse latent spaces gradually. Benefiting from the multiple latent spaces, DeVAE allows simultaneous optimization of multiple objectives to optimize reconstruction while keeping the constraint of disentanglement, avoiding info
    
[^63]: 具有外部状态和奖励的强化学习

    Reinforcement Learning with Exogenous States and Rewards. (arXiv:2303.12957v1 [cs.LG])

    [http://arxiv.org/abs/2303.12957](http://arxiv.org/abs/2303.12957)

    该研究提出了一种强化学习的方法，通过将MDP分解为外生和内生两个部分，优化内生奖励，在状态空间的内生和外生状态空间没有事先给出的情况下，提出了正确的算法进行自动发现。

    

    外部状态变量和奖励会通过向奖励信号注入不可控的变化而减慢强化学习的速度。本文对外部状态变量和奖励进行了正式化，并表明如果奖励函数加法分解成内生和外生两个部分，MDP可以分解为一个外生马尔可夫奖励过程（基于外部奖励）和一个内生马尔可夫决策过程（优化内生奖励）。内生MDP的任何最优策略也是原始MDP的最优策略，但由于内生奖励通常具有降低的方差，因此内生MDP更容易求解。我们研究了状态空间分解为内外生状态空间的情况，而这种状态空间分解并没有给出，而是必须发现。本文介绍并证明了在线性组合下发现内生和外生状态空间的算法的正确性。

    Exogenous state variables and rewards can slow reinforcement learning by injecting uncontrolled variation into the reward signal. This paper formalizes exogenous state variables and rewards and shows that if the reward function decomposes additively into endogenous and exogenous components, the MDP can be decomposed into an exogenous Markov Reward Process (based on the exogenous reward) and an endogenous Markov Decision Process (optimizing the endogenous reward). Any optimal policy for the endogenous MDP is also an optimal policy for the original MDP, but because the endogenous reward typically has reduced variance, the endogenous MDP is easier to solve. We study settings where the decomposition of the state space into exogenous and endogenous state spaces is not given but must be discovered. The paper introduces and proves correctness of algorithms for discovering the exogenous and endogenous subspaces of the state space when they are mixed through linear combination. These algorithms
    
[^64]: TSI-GAN: 使用卷积循环一致生成对抗网络的无监督时间序列异常检测

    TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional Cycle-Consistent Generative Adversarial Networks. (arXiv:2303.12952v1 [cs.LG])

    [http://arxiv.org/abs/2303.12952](http://arxiv.org/abs/2303.12952)

    TSI-GAN是一种无监督时间序列异常检测模型，它可以自动学习时间序列中的复杂模式，并具有良好的泛化性能。

    

    异常检测广泛应用于网络入侵检测、自动驾驶、医学诊断、信用卡欺诈等。然而，仍存在一些关键挑战，例如缺乏真实标签、存在复杂的时间模式和在不同数据集上进行泛化。本文提出了TSI-GAN，这是一种用于时间序列的无监督异常检测模型，可以自动学习复杂的时间模式并且具有很好的泛化性能，即不需要选择特定于数据集的参数、对底层数据做出统计假设或更改模型架构。

    Anomaly detection is widely used in network intrusion detection, autonomous driving, medical diagnosis, credit card frauds, etc. However, several key challenges remain open, such as lack of ground truth labels, presence of complex temporal patterns, and generalizing over different datasets. This paper proposes TSI-GAN, an unsupervised anomaly detection model for time-series that can learn complex temporal patterns automatically and generalize well, i.e., no need for choosing dataset-specific parameters, making statistical assumptions about underlying data, or changing model architectures. To achieve these goals, we convert each input time-series into a sequence of 2D images using two encoding techniques with the intent of capturing temporal patterns and various types of deviance. Moreover, we design a reconstructive GAN that uses convolutional layers in an encoder-decoder network and employs cycle-consistency loss during training to ensure that inverse mappings are accurate as well. In
    
[^65]: FTSO: 通过第一拓扑第二算子实现高效的 NAS

    FTSO: Effective NAS via First Topology Second Operator. (arXiv:2303.12948v1 [cs.CV])

    [http://arxiv.org/abs/2303.12948](http://arxiv.org/abs/2303.12948)

    本文提出了 FTSO 方法，将整个架构搜索分为两个子步骤：拓扑搜索和算子搜索。FTSO方法大大降低了 NAS 搜索的时间，同时提高了搜索到的架构准确性，并在ImageNet和CIFAR10数据集上达到了优秀的准确率。

    

    现有的一次性神经架构搜索（NAS）方法必须在巨大的超级网络上进行搜索，这导致了巨大的计算成本。为了降低这种成本，本文提出了一种方法，称为 FTSO，将整个架构搜索分为两个子步骤。具体而言，在第一步中，我们仅搜索拓扑，而在第二步中，我们搜索算子。 FTSO 不仅将 NAS 的搜索时间从几天缩短到 0.68 秒，而且显着提高了搜索到的架构的准确性。 在 ImageNet 上进行的广泛实验表明，在 18 秒内，FTSO 可以实现 76.4％ 的测试准确率，比 SOTA（PC-DARTS）高 1.5％。此外，在 CIFAR10 上进行搜索时，FTSO 可达到 97.77％的测试准确度，比 SOTA 高 0.27％，并且几乎可以节省 100％（99.8％）的搜索时间。

    Existing one-shot neural architecture search (NAS) methods have to conduct a search over a giant super-net, which leads to the huge computational cost. To reduce such cost, in this paper, we propose a method, called FTSO, to divide the whole architecture search into two sub-steps. Specifically, in the first step, we only search for the topology, and in the second step, we search for the operators. FTSO not only reduces NAS's search time from days to 0.68 seconds, but also significantly improves the found architecture's accuracy. Our extensive experiments on ImageNet show that within 18 seconds, FTSO can achieve a 76.4% testing accuracy, 1.5% higher than the SOTA, PC-DARTS. In addition, FTSO can reach a 97.77% testing accuracy, 0.27% higher than the SOTA, with nearly 100% (99.8%) search time saved, when searching on CIFAR10.
    
[^66]: 5G无人机场景中的深度注意力识别：新型架构和端到端评估

    Deep Attention Recognition for Attack Identification in 5G UAV scenarios: Novel Architecture and End-to-End Evaluation. (arXiv:2303.12947v1 [cs.CR])

    [http://arxiv.org/abs/2303.12947](http://arxiv.org/abs/2303.12947)

    该研究提出了在5G无人机场景中基于深度注意力识别的攻击识别解决方案。该方法使用SINR和RSSI参数识别直射视线（LoS）、非直射视线（NLoS）以及两种条件的概率性组合下的攻击。

    

    尽管5G框架具有强大的安全功能，攻击者仍将发现方法来破坏5G无人机（UAV）操作，并降低空地（A2G）链路中的UAV控制通信性能。在假设5G UAV通信基础设施永远不会完全安全的情况下，我们提出了深度注意力识别（DAtR）作为解决方案，它可以基于嵌入经认证的UAV中的小型深度网络来识别攻击。我们的方案使用两个可观测参数：信干噪比（SINR）和参考信号接收功率（RSSI），以识别具有直射视线（LoS）、非直射视线（NLoS）和两种条件的概率性组合的攻击。在测试的场景中，攻击者位于随机位置，其功率在每个模拟中都有所改变。此外，地面用户也被包含在网络中，以对攻击检测施加额外的复杂性。

    Despite the robust security features inherent in the 5G framework, attackers will still discover ways to disrupt 5G unmanned aerial vehicle (UAV) operations and decrease UAV control communication performance in Air-to-Ground (A2G) links. Operating under the assumption that the 5G UAV communications infrastructure will never be entirely secure, we propose Deep Attention Recognition (DAtR) as a solution to identify attacks based on a small deep network embedded in authenticated UAVs. Our proposed solution uses two observable parameters: the Signal-to-Interference-plus-Noise Ratio (SINR) and the Reference Signal Received Power (RSSI) to recognize attacks under Line-of-Sight (LoS), Non-Line-of-Sight (NLoS), and a probabilistic combination of the two conditions. In the tested scenarios, a number of attackers are located in random positions, while their power is varied in each simulation. Moreover, terrestrial users are included in the network to impose additional complexity on attack detect
    
[^67]: 利用多时间 Hamilton-Jacobi PDE 解决一些科学机器学习问题

    Leveraging Multi-time Hamilton-Jacobi PDEs for Certain Scientific Machine Learning Problems. (arXiv:2303.12928v1 [cs.LG])

    [http://arxiv.org/abs/2303.12928](http://arxiv.org/abs/2303.12928)

    本文从理论上将特定优化问题与多时间Hamilton-Jacobi PDEs联系起来，表明当解决这些问题时，同时解决了对应的多时间HJ PDEs和最优控制问题。利用这种联系，提出了一种新的算法，实现了深度神经网络泛化性能的改进。

    

    Hamilton-Jacobi 偏微分方程(HJ PDEs)与广泛领域，如最优控制、微分游戏和成像科学有着深刻的联系。通过将时间变量视为更高维的量，HJ PDEs 可以扩展到多时间的情况。本文在特定机器学习优化问题与多时间Hopf公式之间建立了一种新的理论联系，该公式对应于某些多时间 HJ PDEs 的解的表示。通过这种联系，我们通过展示当我们解决这些学习问题时，我们也解决了一个多时间 HJ PDE 和相应的最优控制问题，从而增加了某些机器学习应用程序的训练过程的可解释性。作为这种联系的第一个探索，我们发展了正则化线性回归问题与线性二次调节器 (LQR) 之间的关系。然后，我们利用我们的理论框架设计了一种新的深度神经网络训练算法，实现了改进的泛化性能。

    Hamilton-Jacobi partial differential equations (HJ PDEs) have deep connections with a wide range of fields, including optimal control, differential games, and imaging sciences. By considering the time variable to be a higher dimensional quantity, HJ PDEs can be extended to the multi-time case. In this paper, we establish a novel theoretical connection between specific optimization problems arising in machine learning and the multi-time Hopf formula, which corresponds to a representation of the solution to certain multi-time HJ PDEs. Through this connection, we increase the interpretability of the training process of certain machine learning applications by showing that when we solve these learning problems, we also solve a multi-time HJ PDE and, by extension, its corresponding optimal control problem. As a first exploration of this connection, we develop the relation between the regularized linear regression problem and the Linear Quadratic Regulator (LQR). We then leverage our theoret
    
[^68]: 重新审视影响函数的脆弱性

    Revisiting the Fragility of Influence Functions. (arXiv:2303.12922v1 [cs.LG])

    [http://arxiv.org/abs/2303.12922](http://arxiv.org/abs/2303.12922)

    本文研究了影响函数的脆弱性，并提出在非凸条件下使用深层模型和更复杂数据集来解决这一问题。

    

    最近几年有很多论文致力于解释深度学习模型的预测。然而，很少有方法被提出来验证这些解释的准确性或可信度。最近，影响函数被证明是一种评估深度神经网络在单个样本上的灵敏度的方法。但是，先前的研究表明影响函数易受噪声和数据分布不对称性影响，缺乏鲁棒性。本文旨在研究影响函数的脆弱性，通过探究影响函数背后的机理，从而为增强影响函数的鲁棒性提供新思路。

    In the last few years, many works have tried to explain the predictions of deep learning models. Few methods, however, have been proposed to verify the accuracy or faithfulness of these explanations. Recently, influence functions, which is a method that approximates the effect that leave-one-out training has on the loss function, has been shown to be fragile. The proposed reason for their fragility remains unclear. Although previous work suggests the use of regularization to increase robustness, this does not hold in all cases. In this work, we seek to investigate the experiments performed in the prior work in an effort to understand the underlying mechanisms of influence function fragility. First, we verify influence functions using procedures from the literature under conditions where the convexity assumptions of influence functions are met. Then, we relax these assumptions and study the effects of non-convexity by using deeper models and more complex datasets. Here, we analyze the k
    
[^69]: 稳定性稳定：可复制性、隐私和自适应推广之间的联系

    Stability is Stable: Connections between Replicability, Privacy, and Adaptive Generalization. (arXiv:2303.12921v1 [cs.LG])

    [http://arxiv.org/abs/2303.12921](http://arxiv.org/abs/2303.12921)

    本文研究了可复制算法与标准算法稳定性的联系，为一类统计问题提供了可复制算法的样本有效算法约化，同时表明这种等价关系必须在计算上崩溃。

    

    在Impagliazzo et al. [STOC '22]中引入了可复制算法的概念，用于描述在输入重新采样时稳定的随机算法。具体而言，当其随机性被固定且在从相同分布中绘制的新的i.i.d.样本上运行时，可复制算法会以很高的概率给出相同的输出。使用可复制算法进行数据分析可以通过确保分析结果在新数据集上进行分析时具有相同的结果来简化已发布结果的验证。在这项工作中，我们建立了可复制性与算法稳定性标准概念之间的新联系和分离。特别地，我们为一类广泛的统计问题给出了完美推广、近似差分隐私和可复制性之间的样本有效算法约化。相反，我们表明这种等价关系必须在计算上崩溃：存在具有可复制算法但不具有任何可用的差分隐私机制的问题。

    The notion of replicable algorithms was introduced in Impagliazzo et al. [STOC '22] to describe randomized algorithms that are stable under the resampling of their inputs. More precisely, a replicable algorithm gives the same output with high probability when its randomness is fixed and it is run on a new i.i.d. sample drawn from the same distribution. Using replicable algorithms for data analysis can facilitate the verification of published results by ensuring that the results of an analysis will be the same with high probability, even when that analysis is performed on a new data set.  In this work, we establish new connections and separations between replicability and standard notions of algorithmic stability. In particular, we give sample-efficient algorithmic reductions between perfect generalization, approximate differential privacy, and replicability for a broad class of statistical problems. Conversely, we show any such equivalence must break down computationally: there exist s
    
[^70]: 自我蒸馏用于手术动作识别

    Self-distillation for surgical action recognition. (arXiv:2303.12915v1 [cs.CV])

    [http://arxiv.org/abs/2303.12915](http://arxiv.org/abs/2303.12915)

    本文首次将自我蒸馏的概念引入到手术视频分析中，提出了一种异构集成方法，其使用Swine Transfomers作为骨干网络，并将自我蒸馏和多任务学习应用于模型设计中。在类别不平衡和潜在标签不明确的情况下，软标签通过自我蒸馏的方式获得是性能提升最大的因素。

    

    手术场景的理解是手术室中基于上下文的决策支持的关键前提。虽然基于深度学习的方法在各个领域已经达到甚至超过了人类的表现，但手术动作识别仍然是一个重大挑战。本文首次探讨了自我蒸馏的概念，作为应对手术视频分析中的类别不平衡和潜在标签歧义的方法。我们提出了一种异构集成方法，使用Swin Transfomers作为骨干网络，使用自我蒸馏和多任务学习的概念作为核心设计选择。通过交叉验证使用CholecT45挑战数据进行的削减研究表明，使用自我蒸馏获得的软标签是性能提升的最大因素。我们的方法在一个独立的测试集上进行的外部验证，通过提供我们推理模型的Docker容器来实现。

    Surgical scene understanding is a key prerequisite for contextaware decision support in the operating room. While deep learning-based approaches have already reached or even surpassed human performance in various fields, the task of surgical action recognition remains a major challenge. With this contribution, we are the first to investigate the concept of self-distillation as a means of addressing class imbalance and potential label ambiguity in surgical video analysis. Our proposed method is a heterogeneous ensemble of three models that use Swin Transfomers as backbone and the concepts of self-distillation and multi-task learning as core design choices. According to ablation studies performed with the CholecT45 challenge data via cross-validation, the biggest performance boost is achieved by the usage of soft labels obtained by self-distillation. External validation of our method on an independent test set was achieved by providing a Docker container of our inference model to the cha
    
[^71]: TRON：利用非相干硅光子学进行Transformer神经网络加速

    TRON: Transformer Neural Network Acceleration with Non-Coherent Silicon Photonics. (arXiv:2303.12914v1 [cs.LG])

    [http://arxiv.org/abs/2303.12914](http://arxiv.org/abs/2303.12914)

    TRON是一种基于硅光子学的神经网络加速器，能够比同类Transformer加速器高14倍的吞吐量和8倍的能效。

    

    Transformer神经网络目前被广泛应用于自然语言处理(NLP)和计算机视觉领域，但模型结构复杂，加快电子平台上的执行速度面临着挑战。本文提出了一种基于硅光子学的神经网络加速器TRON，可用于加速BERT和Vision Transformer等Transformer模型。实验结果表明，TRON的吞吐量至少比现有的Transformer加速器高14倍，能效至少高出8倍。

    Transformer neural networks are rapidly being integrated into state-of-the-art solutions for natural language processing (NLP) and computer vision. However, the complex structure of these models creates challenges for accelerating their execution on conventional electronic platforms. We propose the first silicon photonic hardware neural network accelerator called TRON for transformer-based models such as BERT, and Vision Transformers. Our analysis demonstrates that TRON exhibits at least 14x better throughput and 8x better energy efficiency, in comparison to state-of-the-art transformer accelerators.
    
[^72]: 基于非相干光计算的AI加速交叉层设计

    Cross-Layer Design for AI Acceleration with Non-Coherent Optical Computing. (arXiv:2303.12910v1 [cs.LG])

    [http://arxiv.org/abs/2303.12910](http://arxiv.org/abs/2303.12910)

    本文介绍了如何使用交叉层设计来克服非相干光计算平台中存在的挑战，适应多种类型的AI工作负载，从而实现高速的AI加速。

    

    新兴的人工智能应用，如ChatGPT、图形卷积网络和其他深度神经网络，需要大量的计算资源进行训练和推理。当代计算平台，如CPU、GPU和TPU正在努力满足这些AI应用的需求。非相干光计算代表了一种有前途的方法，可以实现AI工作负载的光速加速。在本文中，我们展示了交叉层设计如何克服非相干光计算平台中存在的挑战。我们描述了光学器件工程、调谐电路增强以及架构创新的方法，以适应各种AI工作负载。我们还讨论了硬件/软件协同设计的技术，可以智能地映射和适应AI软件，以提高其在非相干光计算平台上的性能。

    Emerging AI applications such as ChatGPT, graph convolutional networks, and other deep neural networks require massive computational resources for training and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs are struggling to keep up with the demands of these AI applications. Non-coherent optical computing represents a promising approach for light-speed acceleration of AI workloads. In this paper, we show how cross-layer design can overcome challenges in non-coherent optical computing platforms. We describe approaches for optical device engineering, tuning circuit enhancements, and architectural innovations to adapt optical computing to a variety of AI workloads. We also discuss techniques for hardware/software co-design that can intelligently map and adapt AI software to improve its performance on non-coherent optical computing platforms.
    
[^73]: 用于网络安全入侵检测系统的特征降维方法比较

    Feature Reduction Method Comparison Towards Explainability and Efficiency in Cybersecurity Intrusion Detection Systems. (arXiv:2303.12891v1 [cs.LG])

    [http://arxiv.org/abs/2303.12891](http://arxiv.org/abs/2303.12891)

    本文对于用于网络安全入侵检测系统的三种特征降维方法进行了比较，结果表明使用蝙蝠算法的相关特征选择（CFS-BA）是最为高效的，仅用最佳随机森林信息增益（RF-IG）模型55%的时间构建，同时实现了99.99%的准确性。

    

    在网络安全领域，入侵检测系统（IDS）根据收集到的计算机和网络数据检测和防止攻击。最近的研究中，IDS模型使用机器学习（ML）和深度学习（DL）方法构建，如随机森林（RF）和深度神经网络（DNN）。特征选择（FS）可用于构建更快，更可解释和更准确的模型。我们研究了三种不同的FS技术； 随机森林信息增益（RF-IG），使用蝙蝠算法的相关特征选择（CFS-BA）和使用阿基拉优化器的CFS（CFS-AO）。我们的结果显示，CFS-BA是最有效的FS方法，仅用最佳RF-IG模型55％的时间构建，同时实现了99.99％的准确性。这加强了先前对CFS-BA准确性的贡献，并在最终结果中建立了子集大小，CFS得分和RF-IG得分之间的关系。

    In the realm of cybersecurity, intrusion detection systems (IDS) detect and prevent attacks based on collected computer and network data. In recent research, IDS models have been constructed using machine learning (ML) and deep learning (DL) methods such as Random Forest (RF) and deep neural networks (DNN). Feature selection (FS) can be used to construct faster, more interpretable, and more accurate models. We look at three different FS techniques; RF information gain (RF-IG), correlation feature selection using the Bat Algorithm (CFS-BA), and CFS using the Aquila Optimizer (CFS-AO). Our results show CFS-BA to be the most efficient of the FS methods, building in 55% of the time of the best RF-IG model while achieving 99.99% of its accuracy. This reinforces prior contributions attesting to CFS-BA's accuracy while building upon the relationship between subset size, CFS score, and RF-IG score in final results.
    
[^74]: 使用机器学习的动态风险评分提前预测心源性休克

    A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])

    [http://arxiv.org/abs/2303.12888](http://arxiv.org/abs/2303.12888)

    该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。

    

    心肌梗死和心力衰竭是主要的心血管疾病，影响着美国数百万人的健康。发展心源性休克的患者中，发病率和死亡率最高。心源性休克的早期识别至关重要，及时实施治疗措施可以防止缺血、低血压以及由于心源性休克导致心输出量降低的有害循环。然而，由于心脏监护病房中海量数据的信息处理能力与缺乏有效的风险分层工具，对心源性休克的早期识别一直具有挑战性。我们基于深度学习开发了一个称为CShock的风险分层工具，用于预测入住心脏监护病房的急性失代偿性心力衰竭和/或心肌梗死患者的心源性休克发作。为了开发和验证CShock，我们使用由医师裁定的结果注释了心脏监护病房数据集。

    Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
    
[^75]: 排序数据分析中的鲁棒性共识：定义、属性和计算问题

    Robust Consensus in Ranking Data Analysis: Definitions, Properties and Computational Issues. (arXiv:2303.12878v1 [cs.LG])

    [http://arxiv.org/abs/2303.12878](http://arxiv.org/abs/2303.12878)

    该论文介绍了排序数据分析中的鲁棒性共识问题以及该问题相关的统计方法，其中Consensus Ranking问题是重点，旨在通过中位数排名来总结排列的概率分布。

    

    随着人工智能系统中鲁棒性问题日益突出，需要开发可靠的统计学习技术，即使在部分受损数据的情况下也能确保可靠性。偏好数据以 (完整) 排序的形式出现时也不例外，尤其是饱受此类数据支持或产生的技术(例如，搜索引擎，推荐系统)大规模部署之时，需要相应的概念和工具。然而，由于排列的集合 (即对称群 $\mathfrak{S}_n$) 没有向量空间结构，且排序数据分析中考虑的统计量的复杂性，使得在该领域中制定鲁棒性目标具有挑战性。在本文中，我们引入了鲁棒性概念以及专用的统计方法，针对排名数据分析中的旗舰问题：Consensus Ranking，旨在通过中位数排名来总结排列的概率分布。

    As the issue of robustness in AI systems becomes vital, statistical learning techniques that are reliable even in presence of partly contaminated data have to be developed. Preference data, in the form of (complete) rankings in the simplest situations, are no exception and the demand for appropriate concepts and tools is all the more pressing given that technologies fed by or producing this type of data (e.g. search engines, recommending systems) are now massively deployed. However, the lack of vector space structure for the set of rankings (i.e. the symmetric group $\mathfrak{S}_n$) and the complex nature of statistics considered in ranking data analysis make the formulation of robustness objectives in this domain challenging. In this paper, we introduce notions of robustness, together with dedicated statistical methods, for Consensus Ranking the flagship problem in ranking data analysis, aiming at summarizing a probability distribution on $\mathfrak{S}_n$ by a median ranking. Precise
    
[^76]: 概念驱动的AI系统中的人类不确定性

    Human Uncertainty in Concept-Based AI Systems. (arXiv:2303.12872v1 [cs.HC])

    [http://arxiv.org/abs/2303.12872](http://arxiv.org/abs/2303.12872)

    本研究探讨了人类不确定性对概念驱动AI系统的影响，通过控制数据集的干扰因素，分析了现有模型的处理方法。

    

    在安全关键领域中部署AI系统（如医疗AI系统与临床医生一起工作）时，将人类放入其中可能会减轻一些风险。然而，缓解人间误差和不确定因素在此类人工智能交互中引起的风险，是一个重要的且未被研究充分的问题。在本文中，我们研究了在概念驱动模型中人类不确定性的问题，这是一类在AI系统中启用概念干预功能的模型。该功能是指在与任务相关的人类可解释概念上，专家对其进行干预以获得人类反馈。之前的工作已经对此进行了研究，但通常假设人类是预言家，总是确定和正确的。然而，实际中人类的决策过程往往也会出现偶尔的错误和不确定性。我们通过两个新型数据集（UMNIST和CUB-S）探讨了现有的概念驱动模型如何处理来自人类的不确定干预。

    Placing a human in the loop may abate the risks of deploying AI systems in safety-critical settings (e.g., a clinician working with a medical AI system). However, mitigating risks arising from human error and uncertainty within such human-AI interactions is an important and understudied issue. In this work, we study human uncertainty in the context of concept-based models, a family of AI systems that enable human feedback via concept interventions where an expert intervenes on human-interpretable concepts relevant to the task. Prior work in this space often assumes that humans are oracles who are always certain and correct. Yet, real-world decision-making by humans is prone to occasional mistakes and uncertainty. We study how existing concept-based models deal with uncertain interventions from humans using two novel datasets: UMNIST, a visual dataset with controlled simulated uncertainty based on the MNIST dataset, and CUB-S, a relabeling of the popular CUB concept dataset with rich, d
    
[^77]: NeRF-GAN蒸馏：基于卷积的高效3D感知生成

    NeRF-GAN Distillation for Efficient 3D-Aware Generation with Convolutions. (arXiv:2303.12865v1 [cs.CV])

    [http://arxiv.org/abs/2303.12865](http://arxiv.org/abs/2303.12865)

    本文提出了一种基于神经辐射场（NeRF）和生成对抗网络（GAN）的新方法，通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，实现了基于姿态的2D GAN的高效3D感知生成。

    

    基于姿态的卷积生成模型在从单视图数据集进行高质量的3D一致图像生成方面存在困难，因为它们缺乏足够的3D先验知识。最近，将神经辐射场（NeRF）和生成对抗网络（GAN）等生成模型相结合，从单视图图像中生成3D感知图像，已经引起了广泛关注。NeRF-GAN利用了三维神经表示和体积渲染的强归纳偏差，但也带来了更高的计算复杂性。这项研究旨在通过从预训练的NeRF-GAN中提取3D知识进行蒸馏，重审基于姿态的二维GAN，在推理时间内实现高效的3D感知生成。我们提出了一种简单有效的方法，基于在基于姿态的卷积网络中重用预训练的NeRF-GAN的良好解耦潜在空间，直接生成与潜在的3D表达相对应的3D一致图片。在多个数据集上的实验表明，所提出的方法实现了更高效的3D感知生成。

    Pose-conditioned convolutional generative models struggle with high-quality 3D-consistent image generation from single-view datasets, due to their lack of sufficient 3D priors. Recently, the integration of Neural Radiance Fields (NeRFs) and generative models, such as Generative Adversarial Networks (GANs), has transformed 3D-aware generation from single-view images. NeRF-GANs exploit the strong inductive bias of 3D neural representations and volumetric rendering at the cost of higher computational complexity. This study aims at revisiting pose-conditioned 2D GANs for efficient 3D-aware generation at inference time by distilling 3D knowledge from pretrained NeRF-GANS. We propose a simple and effective method, based on re-using the well-disentangled latent space of a pre-trained NeRF-GAN in a pose-conditioned convolutional network to directly generate 3D-consistent images corresponding to the underlying 3D representations. Experiments on several datasets demonstrate that the proposed met
    
[^78]: 面向不完整数据的锥束CT重建的基于立方体的3D去噪扩散概率模型

    Cube-Based 3D Denoising Diffusion Probabilistic Model for Cone Beam Computed Tomography Reconstruction with Incomplete Data. (arXiv:2303.12861v1 [eess.IV])

    [http://arxiv.org/abs/2303.12861](http://arxiv.org/abs/2303.12861)

    本文提出了一种基于立方体的3D去噪扩散概率模型（DDPM）来重建CBCT并解决了存储整个正弦图的内存问题。通过将整个CBCT volume分成多个小立方体，该模型能够实现高效的计算并在视觉和定量评价方面优于现有方法。

    

    深度学习在计算机断层摄影（CT）重建中获得了广泛的研究，特别是在稀疏视图CT重建中。然而，将DL应用于稀疏视图锥束CT（CBCT）仍然具有挑战性。本文提出了一种基于立方体的3D去噪扩散概率模型（DDPM）来重建CBCT，并解决了存储整个正弦图的内存问题。我们的方法将整个CBCT volume分成多个小立方体，以实现高效的计算。实验结果表明，该方法在视觉和定量评价方面优于现有方法。

    Deep learning (DL) has been extensively researched in the field of computed tomography (CT) reconstruction with incomplete data, particularly in sparse-view CT reconstruction. However, applying DL to sparse-view cone beam CT (CBCT) remains challenging. Many models learn the mapping from sparse-view CT images to ground truth but struggle to achieve satisfactory performance in terms of global artifact removal. Incorporating sinogram data and utilizing dual-domain information can enhance anti-artifact performance, but this requires storing the entire sinogram in memory. This presents a memory issue for high-resolution CBCT sinograms, limiting further research and application. In this paper, we propose a cube-based 3D denoising diffusion probabilistic model (DDPM) for CBCT reconstruction using down-sampled data. A DDPM network, trained on cubes extracted from paired fully sampled sinograms and down-sampled sinograms, is employed to inpaint down-sampled sinograms. Our method divides the ent
    
[^79]: 反对称巴龙函数及其用行列式和表示的逼近

    Anti-symmetric Barron functions and their approximation with sums of determinants. (arXiv:2303.12856v1 [math.NA])

    [http://arxiv.org/abs/2303.12856](http://arxiv.org/abs/2303.12856)

    用行列式和有效地逼近反对称巴龙函数，从而获得了阶乘级的复杂度提升和在从头算量子化学中的有效性。

    

    在量子物理中，一个基本的问题是如何编码在粒子置换下完全反对称的函数。Barron空间由可被一层隐藏神经网络无限参数化的高维函数组成。通过明确地编码反对称结构，我们证明了属于Barron空间的反对称函数可以用行列式和有效逼近。这相对于在Barron空间的标准表示中的复杂度提供了阶乘级的改进，并为基于行列式的结构在从头算量子化学中有效的理论解释提供了依据。

    A fundamental problem in quantum physics is to encode functions that are completely anti-symmetric under permutations of identical particles. The Barron space consists of high-dimensional functions that can be parameterized by infinite neural networks with one hidden layer. By explicitly encoding the anti-symmetric structure, we prove that the anti-symmetric functions which belong to the Barron space can be efficiently approximated with sums of determinants. This yields a factorial improvement in complexity compared to the standard representation in the Barron space and provides a theoretical explanation for the effectiveness of determinant-based architectures in ab-initio quantum chemistry.
    
[^80]: 三次迭代的$(1-d)$-WL测试可以区分$d$维点云的非等距变换. (arXiv:2303.12853v1 [cs.LG])

    Three iterations of $(1-d)$-WL test distinguish non isometric clouds of $d$-dimensional points. (arXiv:2303.12853v1 [cs.LG])

    [http://arxiv.org/abs/2303.12853](http://arxiv.org/abs/2303.12853)

    本文研究了WL测试在点云中的应用，结果发现三次迭代的$(d-1)$-WL测试可以区分$d$维欧几里得空间中的点云，且只需要一次迭代的$d$-WL测试就可以达到完整性。

    

    Weisfeiler-Lehman (WL)测试是一个检查图同构的基本迭代算法。它被观察到是几种图神经网络体系结构设计的基础，这些网络的能力和性能可以用这个测试的表示能力来理解。受最近机器学习应用于涉及三维物体的数据集的发展启发，我们研究了当WL测试对完整的距离图表示的欧几里得点云是“完整的”时，它何时能够识别出任意一个任意点云.我们的主要结果是，$(d-1)$-维WL测试可以区分$d$维欧几里得空间中的点云，任何$d\ge 2$都可以，而且只需要进行三次测试。我们的结果对于$d=2,3$是紧的。我们还观察到$d$维WL测试只需要进行一次迭代就可以达到完整性。

    The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for checking isomorphism of graphs. It has also been observed that it underlies the design of several graph neural network architectures, whose capabilities and performance can be understood in terms of the expressive power of this test. Motivated by recent developments in machine learning applications to datasets involving three-dimensional objects, we study when the WL test is {\em complete} for clouds of euclidean points represented by complete distance graphs, i.e., when it can distinguish, up to isometry, any arbitrary such cloud.  Our main result states that the $(d-1)$-dimensional WL test is complete for point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that only three iterations of the test suffice. Our result is tight for $d = 2, 3$. We also observe that the $d$-dimensional WL test only requires one iteration to achieve completeness.
    
[^81]: 对抗攻击的测试时间防御：基于遮蔽自编码器的对抗样本检测和重构

    Test-time Defense against Adversarial Attacks: Detection and Reconstruction of Adversarial Examples via Masked Autoencoder. (arXiv:2303.12848v1 [cs.CV])

    [http://arxiv.org/abs/2303.12848](http://arxiv.org/abs/2303.12848)

    该方法使用遮蔽自编码器进行对抗攻击检测和重构，不需要在测试时间更新模型权重，也不需要使用更多的对抗样本来增强训练集。

    

    现有的对抗攻击防御方法可以分为训练时间和测试时间防御。训练时间防御需要大量的额外训练时间，通常无法推广到未见过的攻击。而测试时间防御需要访问（部分）模型权重以执行梯度下降，这对于冻结权重的模型可能不可行。为了解决这些挑战，我们提出了一种新的防御方法DRAM，它使用遮蔽自编码器（MAE）检测并重构多种类型的对抗攻击。我们演示了如何使用MAE损失构建KS测试来检测对抗攻击。此外，MAE损失可以用于修复未见攻击类型的对抗样本。因此，DRAM既不需要在测试时间更新模型权重，也不需要使用更多的对抗样本来增强训练集。在大规模的ImageN数据集上评估DRAM，实验结果表明其具有很高的鲁棒性和有效性。

    Existing defense methods against adversarial attacks can be categorized into training time and test time defenses. Training time defense, i.e., adversarial training, requires a significant amount of extra time for training and is often not able to be generalized to unseen attacks. On the other hand, test time defense by test time weight adaptation requires access to perform gradient descent on (part of) the model weights, which could be infeasible for models with frozen weights. To address these challenges, we propose DRAM, a novel defense method to Detect and Reconstruct multiple types of Adversarial attacks via Masked autoencoder (MAE). We demonstrate how to use MAE losses to build a KS-test to detect adversarial attacks. Moreover, the MAE losses can be used to repair adversarial samples from unseen attack types. In this sense, DRAM neither requires model weight updates in test time nor augments the training set with more adversarial samples. Evaluating DRAM on the large-scale ImageN
    
[^82]: 量子动力学的学习的能力与局限性

    The power and limitations of learning quantum dynamics incoherently. (arXiv:2303.12834v1 [quant-ph])

    [http://arxiv.org/abs/2303.12834](http://arxiv.org/abs/2303.12834)

    本文证明在不相干框架下，通过浅层测量只能有效地学习低纠缠门。虽然该类框架为我们在不同物理平台之间转移量子过程提供了方法，但也存在一定的局限性。

    

    量子过程学习是研究量子系统的重要工具之一。然而大部分研究都放在了自旋相干和器件自耦合的波动函数上，研究量子动力学在系统和目标不直接交互的情况下是否可以被学习并没有得到足够的关注。这类不相干的框架实际上非常吸引人，因为它们能够在不需要挑战性的混合纠缠方案中，为我们提供在不同物理平台之间转移量子过程的方法。在本文中，我们通过分析需要仿真的明确的相干学习策略的测量次数，提供了在不相干框架下学习幺正过程样本复杂度的界限。我们证明，如果允许任意测量，则任何有效表示的幺正矩阵都可以在不相干框架内被有效地学习；然而，如果仅限于浅层测量，则只能有效地学习低纠缠门。因此，我们的工作突出了学习量子动力学在不相干框架中的能力与局限性。

    Quantum process learning is emerging as an important tool to study quantum systems. While studied extensively in coherent frameworks, where the target and model system can share quantum information, less attention has been paid to whether the dynamics of quantum systems can be learned without the system and target directly interacting. Such incoherent frameworks are practically appealing since they open up methods of transpiling quantum processes between the different physical platforms without the need for technically challenging hybrid entanglement schemes. Here we provide bounds on the sample complexity of learning unitary processes incoherently by analyzing the number of measurements that are required to emulate well-established coherent learning strategies. We prove that if arbitrary measurements are allowed, then any efficiently representable unitary can be efficiently learned within the incoherent framework; however, when restricted to shallow-depth measurements only low-entangl
    
[^83]: 利用离散手势令牌学习的共性语言手势合成

    Co-Speech Gesture Synthesis using Discrete Gesture Token Learning. (arXiv:2303.12822v1 [cs.CV])

    [http://arxiv.org/abs/2303.12822](http://arxiv.org/abs/2303.12822)

    该论文提出了一个两阶段的机制，使用离散的编码方式来解决合成共性语言手势中的不确定性问题，采用VAE和自回归变压器模型进行学习，能够生成多样化和逼真的共性语言手势。

    

    制作逼真的共性语言手势是一个重要且尚未解决的问题，可以用于驱动人形机器人与人类用户进行交互和沟通。这种能力将改善人类用户对机器人的印象，并在教育、培训和医疗服务中找到应用。学习共性语言手势模型的一个挑战是，对于同一语音话语，可能存在多个合理的手势运动。确定性回归方法无法解决冲突样本，并可能产生过度平滑或抑制的运动。我们提出了一个两阶段模型，通过将手势片段建模为离散的潜在编码来解决这个不确定性问题，我们的方法利用RQ-VAE在第一阶段从训练数据中学习由手势令牌组成的离散码本，第二阶段使用两级自回归变压器模型学习残余码的先验分布，以及给出语音时手势令牌的条件分布。在大型数据集上的实验证明，所提出的方法可以生成多样化和逼真的共性语言手势。

    Synthesizing realistic co-speech gestures is an important and yet unsolved problem for creating believable motions that can drive a humanoid robot to interact and communicate with human users. Such capability will improve the impressions of the robots by human users and will find applications in education, training, and medical services. One challenge in learning the co-speech gesture model is that there may be multiple viable gesture motions for the same speech utterance. The deterministic regression methods can not resolve the conflicting samples and may produce over-smoothed or damped motions. We proposed a two-stage model to address this uncertainty issue in gesture synthesis by modeling the gesture segments as discrete latent codes. Our method utilizes RQ-VAE in the first stage to learn a discrete codebook consisting of gesture tokens from training data. In the second stage, a two-level autoregressive transformer model is used to learn the prior distribution of residual codes cond
    
[^84]: 一种用于创建深度学习模型的可视化编程工具

    Towards A Visual Programming Tool to Create Deep Learning Models. (arXiv:2303.12821v1 [cs.HC])

    [http://arxiv.org/abs/2303.12821](http://arxiv.org/abs/2303.12821)

    DeepBlocks是一款可视化编程工具，允许DL开发人员设计、训练和评估模型，而无需依赖特定的编程语言。其通过构建典型模型结构实现其工作原理，结果表明开发人员可以视觉上设计复杂的DL架构。

    

    深度学习（DL）开发人员来自不同的背景，例如医学、基因组学、金融和计算机科学。为了创建DL模型，他们必须学习和使用高级编程语言（例如Python），因此需要处理相关设置和解决编程错误。本文介绍了DeepBlocks，这是一款可视化编程工具，允许DL开发人员设计、训练和评估模型，而无需依赖特定的编程语言。DeepBlocks通过构建典型模型结构实现其工作原理：一系列可学习函数的顺序排列定义了模型的特定特征。我们通过对5个参与者的形式化访谈推导出了DeepBlocks的设计目标，并通过一个典型用例验证了该工具的第一个实现。结果是令人兴奋的，表明开发人员可以视觉上设计复杂的DL架构。

    Deep Learning (DL) developers come from different backgrounds, e.g., medicine, genomics, finance, and computer science. To create a DL model, they must learn and use high-level programming languages (e.g., Python), thus needing to handle related setups and solve programming errors. This paper presents DeepBlocks, a visual programming tool that allows DL developers to design, train, and evaluate models without relying on specific programming languages. DeepBlocks works by building on the typical model structure: a sequence of learnable functions whose arrangement defines the specific characteristics of the model. We derived DeepBlocks' design goals from a 5-participants formative interview, and we validated the first implementation of the tool through a typical use case. Results are promising and show that developers could visually design complex DL architectures.
    
[^85]: BatchNorm中Shift和Scale参数的实证分析

    An Empirical Analysis of the Shift and Scale Parameters in BatchNorm. (arXiv:2303.12818v1 [cs.LG])

    [http://arxiv.org/abs/2303.12818](http://arxiv.org/abs/2303.12818)

    本文通过实验比较重新参数化步骤与归一化步骤对BatchNorm成功的贡献，以研究BatchNorm中Shift和Scale参数的作用。

    

    Batch Normalization（BatchNorm）是一种改善深度神经网络训练的技术，特别是卷积神经网络（CNN）。虽然不清楚这种改进的原因，但已经经验证明BatchNorm可以增加性能，稳定性和准确性。BatchNorm包括归一化步骤以及可训练的Shift和Scale参数。在本文中，我们通过实证研究归一化步骤相对于移位和缩放的重新参数化对BatchNorm成功的贡献度。为了进行实验，我们在PyTorch中实现了两个新的优化器，分别为包含重新参数化步骤但不进行归一化（称为AffineLayer）和仅包含归一化步骤的版本（称为BatchNorm-minus）。我们将我们的AffineLayer和BatchNorm-minus的性能与标准BatchNorm进行比较，并将这些与不使用BatchNorm进行比较。

    Batch Normalization (BatchNorm) is a technique that improves the training of deep neural networks, especially Convolutional Neural Networks (CNN). It has been empirically demonstrated that BatchNorm increases performance, stability, and accuracy, although the reasons for such improvements are unclear. BatchNorm includes a normalization step as well as trainable shift and scale parameters. In this paper, we empirically examine the relative contribution to the success of BatchNorm of the normalization step, as compared to the re-parameterization via shifting and scaling. To conduct our experiments, we implement two new optimizers in PyTorch, namely, a version of BatchNorm that we refer to as AffineLayer, which includes the re-parameterization step without normalization, and a version with just the normalization step, that we call BatchNorm-minus. We compare the performance of our AffineLayer and BatchNorm-minus implementations to standard BatchNorm, and we also compare these to the case 
    
[^86]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^87]: 任意深度的一维神经网络的不动点

    Fixed points of arbitrarily deep 1-dimensional neural networks. (arXiv:2303.12814v1 [stat.ML])

    [http://arxiv.org/abs/2303.12814](http://arxiv.org/abs/2303.12814)

    本研究发现，具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点，为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    

    本文介绍了一个在$\mathbb{R}$上具有合成性且包含对数S型函数的新函数类。我们使用这个类来证明具有对数S型激活函数的任意深度的一维神经网络最多只有三个不动点。虽然这样的神经网络远离实际应用，但我们能够完全理解它们的不动点，并为深度神经网络的应用和理论之间构建了一个必要的桥梁。

    In this paper, we introduce a new class of functions on $\mathbb{R}$ that is closed under composition, and contains the logistic sigmoid function. We use this class to show that any 1-dimensional neural network of arbitrary depth with logistic sigmoid activation functions has at most three fixed points. While such neural networks are far from real world applications, we are able to completely understand their fixed points, providing a foundation to the much needed connection between application and theory of deep neural networks.
    
[^88]: 图神经网络在恶意软件分类中的比较

    A Comparison of Graph Neural Networks for Malware Classification. (arXiv:2303.12812v1 [cs.LG])

    [http://arxiv.org/abs/2303.12812](http://arxiv.org/abs/2303.12812)

    本研究将恶意软件分类视为图分类问题，并基于本地度量剖面特征，使用多种图神经网络（GNN）架构进行了训练。研究表明，我们的最佳GNN模型优于以前的可比研究，且不会遭受过度拟合问题。

    

    管理恶意软件威胁需要准确的检测和分类技术。传统的检测策略依赖于针对恶意软件的手动分析，提取相关特征的过程费时费力，需要专家知识。函数调用图由一组程序函数和它们的过程调用组成，提供了可用于分类恶意软件的丰富信息来源，无需传统技术中费力提取特征的步骤。本研究将恶意软件分类视为图分类问题，基于本地度量剖面特征，训练了多种图神经网络（GNN）架构来生成嵌入，然后进行分类。研究发现，我们最好的GNN模型优于以前的可比研究，包括知名的MalNet-Tiny Android恶意软件数据集。此外，我们的GNN模型不会遭受常见的过度拟合问题。

    Managing the threat posed by malware requires accurate detection and classification techniques. Traditional detection strategies, such as signature scanning, rely on manual analysis of malware to extract relevant features, which is labor intensive and requires expert knowledge. Function call graphs consist of a set of program functions and their inter-procedural calls, providing a rich source of information that can be leveraged to classify malware without the labor intensive feature extraction step of traditional techniques. In this research, we treat malware classification as a graph classification problem. Based on Local Degree Profile features, we train a wide range of Graph Neural Network (GNN) architectures to generate embeddings which we then classify. We find that our best GNN models outperform previous comparable research involving the well-known MalNet-Tiny Android malware dataset. In addition, our GNN models do not suffer from the overfitting issues that commonly afflict non
    
[^89]: SignCRF: 可扩展的无频道数据驱动射频认证系统

    SignCRF: Scalable Channel-agnostic Data-driven Radio Authentication System. (arXiv:2303.12811v1 [cs.CR])

    [http://arxiv.org/abs/2303.12811](http://arxiv.org/abs/2303.12811)

    SignCRF是一个可扩展的无频道数据驱动射频认证平台，能够高精度地识别无线设备，不受移动性带来的动态信道影响。

    

    无线电频率指纹通过深度学习(RFFDL)是一种数据驱动的物联网身份认证技术，利用与特定设备相关的独特硬件级制造缺陷来识别（指纹）基于传输波形引入的变化的设备。提出的SignCRF是一个可扩展的、无频道的、数据驱动的射频认证平台，在识别基于其独特的制造缺陷的无线设备方面具有无与伦比的精度，并且独立于由移动性引起的动态信道不规则性。SignCRF由三部分组成：(i)基线分类器，经过精细训练，能够高精度地扩展认证设备;(ii)环境翻译器，经过精心设计和训练，能够从RF信号中去除动态信道影响，同时保持收发机具体的信号；(iii)最大规则模块选择基线分类器和环境翻译器之间的最高精度认证技术。

    Radio Frequency Fingerprinting through Deep Learning (RFFDL) is a data-driven IoT authentication technique that leverages the unique hardware-level manufacturing imperfections associated with a particular device to recognize (fingerprint) the device based on variations introduced in the transmitted waveform. The proposed SignCRF is a scalable, channel-agnostic, data-driven radio authentication platform with unmatched precision in fingerprinting wireless devices based on their unique manufacturing impairments and independent of the dynamic channel irregularities caused by mobility. SignCRF consists of (i) a baseline classifier finely trained to authenticate devices with high accuracy and at scale; (ii) an environment translator carefully designed and trained to remove the dynamic channel impact from RF signals while maintaining the radio's specific signature; (iii) a Max-Rule module that selects the highest precision authentication technique between the baseline classifier and the envir
    
[^90]: 粒球优化算法

    Granular-ball Optimization Algorithm. (arXiv:2303.12807v1 [cs.LG])

    [http://arxiv.org/abs/2303.12807](http://arxiv.org/abs/2303.12807)

    粒球优化算法(GBO)是一种新的多粒度优化算法，可以通过引入粒球计算来提高全局搜索能力和收敛速度，实验结果表明，在这些方面它比现有的最先进的算法表现更优。

    

    现有的智能优化算法都是基于最小粒度即点的设计，导致全局搜索能力较弱且效率低下。为了解决这个问题，我们提出了一种新的多粒度优化算法，即粒球优化算法(GBO)，通过引入粒球计算来实现。GBO使用多个粒球来覆盖解空间，使用许多细小的细粒度粒球来描述重要部分，使用少量的大粗粒度粒球来描述不重要的部分，精细的多粒度数据描述能力提高了全局搜索能力和收敛速度。针对二十个基准函数的实验结果表明，与最流行的最先进的算法相比，GBO具有更好的性能和更快的速度，更接近最优解，没有超参数，设计更简单。

    The existing intelligent optimization algorithms are designed based on the finest granularity, i.e., a point. This leads to weak global search ability and inefficiency. To address this problem, we proposed a novel multi-granularity optimization algorithm, namely granular-ball optimization algorithm (GBO), by introducing granular-ball computing. GBO uses many granular-balls to cover the solution space. Quite a lot of small and fine-grained granular-balls are used to depict the important parts, and a little number of large and coarse-grained granular-balls are used to depict the inessential parts. Fine multi-granularity data description ability results in a higher global search capability and faster convergence speed. In comparison with the most popular and state-of-the-art algorithms, the experiments on twenty benchmark functions demonstrate its better performance. The faster speed, higher approximation ability of optimal solution, no hyper-parameters, and simpler design of GBO make it 
    
[^91]: 类皮肤科医生的可解释人工智能增强黑色素瘤的诊断信任和信心

    Dermatologist-like explainable AI enhances trust and confidence in diagnosing melanoma. (arXiv:2303.12806v1 [q-bio.QM])

    [http://arxiv.org/abs/2303.12806](http://arxiv.org/abs/2303.12806)

    研究开发了一种可解释人工智能系统，其能够对黑色素瘤和痣进行诊断，并能够给出易于解释的文本和区域解释，该系统能显著提高临床医生的准确性、信心和对其XAI支持的信任。

    

    尽管人工智能（AI）系统已被证明能提高黑色素瘤的初步诊断准确性，但这些系统在如何识别黑素瘤方面缺乏透明度，这给用户接受带来了严重障碍。可解释的人工智能（XAI）方法可以增加透明度，但大多数XAI方法无法产生明确定位的领域特定解释，使得解释难以解释。此外，XAI方法对皮肤科医生的影响尚未评估。我们扩展了两个现有分类器，开发了一种XAI系统，该系统在不同诊断的基础上产生了文本和区域解释，这些解释容易被皮肤科医生解释。为了评估此系统，我们进行了一个三部分的读者研究，以评估其对临床医生的诊断准确性、信心和对XAI支持的信任的影响。我们证明我们的XAI解释与医生的解释高度一致。

    Although artificial intelligence (AI) systems have been shown to improve the accuracy of initial melanoma diagnosis, the lack of transparency in how these systems identify melanoma poses severe obstacles to user acceptance. Explainable artificial intelligence (XAI) methods can help to increase transparency, but most XAI methods are unable to produce precisely located domain-specific explanations, making the explanations difficult to interpret. Moreover, the impact of XAI methods on dermatologists has not yet been evaluated. Extending on two existing classifiers, we developed an XAI system that produces text and region based explanations that are easily interpretable by dermatologists alongside its differential diagnoses of melanomas and nevi. To evaluate this system, we conducted a three-part reader study to assess its impact on clinicians' diagnostic accuracy, confidence, and trust in the XAI-support. We showed that our XAI's explanations were highly aligned with clinicians' explanati
    
[^92]: 使用自然语言处理进行特征匹配

    Features matching using natural language processing. (arXiv:2303.12804v1 [cs.DB])

    [http://arxiv.org/abs/2303.12804](http://arxiv.org/abs/2303.12804)

    本文提出了一种使用自然语言处理进行特征匹配的新混合模型，它可以减少匹配不同数据集所需的时间。

    

    特征匹配是匹配不同数据集的基本步骤。本文提出了一种新的混合模型，该模型由预训练的基于自然语言处理（NLP）的BERT模型与基于Jaccard相似度的统计模型并行使用，以测量两个不同数据集中特征列表之间的相似性。这减少了搜索相关性或手动匹配每个数据集中的特征所需的时间。

    The feature matching is a basic step in matching different datasets. This article proposes shows a new hybrid model of a pretrained Natural Language Processing (NLP) based model called BERT used in parallel with a statistical model based on Jaccard similarity to measure the similarity between list of features from two different datasets. This reduces the time required to search for correlations or manually match each feature from one dataset to another.
    
[^93]: 分布式学习 meets 6G：通信和计算视角

    Distributed Learning Meets 6G: A Communication and Computing Perspective. (arXiv:2303.12802v1 [cs.NI])

    [http://arxiv.org/abs/2303.12802](http://arxiv.org/abs/2303.12802)

    本文探讨了基于分布式学习和FL策略的方法如何帮助实现6G网络下的严格KPI要求，同时强调了将DL方法应用于6G网络时所面临的挑战。

    

    随着移动设备的计算能力和存储能力不断提高，并配合不断进化的电信网络范式，人们不断探索分布式学习（DL）框架，以实现下一代/6G蜂窝网络中所期望的严格基本性能指标（KPI）。在边缘计算的协作下，联邦学习（FL）已经成为杰出无线应用中的DL 架构选择。本文概述了DL框架，特别是基于FL策略，如何有助于实现6G愿景的一部分，并在通信和计算约束之间取得平衡。作为一个实际应用的案例，我们在FL框架内应用了多智能体强化学习（MARL）到动态频谱访问（DSA）问题，并呈现了初步的评估结果。同时本文还强调了将DL方法应用于6G网络时所面临的当前挑战。

    With the ever-improving computing capabilities and storage capacities of mobile devices in line with evolving telecommunication network paradigms, there has been an explosion of research interest towards exploring Distributed Learning (DL) frameworks to realize stringent key performance indicators (KPIs) that are expected in next-generation/6G cellular networks. In conjunction with Edge Computing, Federated Learning (FL) has emerged as the DL architecture of choice in prominent wireless applications. This article lays an outline of how DL in general and FL-based strategies specifically can contribute towards realizing a part of the 6G vision and strike a balance between communication and computing constraints. As a practical use case, we apply Multi-Agent Reinforcement Learning (MARL) within the FL framework to the Dynamic Spectrum Access (DSA) problem and present preliminary evaluation results. Top contemporary challenges in applying DL approaches to 6G networks are also highlighted.
    
[^94]: 一种针对小样本肺结节检测和分类的数据增强方法和嵌入机制 (arXiv:2303.12801v1 [eess.IV])

    A Data Augmentation Method and the Embedding Mechanism for Detection and Classification of Pulmonary Nodules on Small Samples. (arXiv:2303.12801v1 [eess.IV])

    [http://arxiv.org/abs/2303.12801](http://arxiv.org/abs/2303.12801)

    该论文提出了一种新的数据增强方法和嵌入机制，可以提高深度学习模型的准确性和鲁棒性，从而实现对小样本肺结节的检测和分类。

    

    CT扫描肺结节检测被用于肺癌的早期筛查。基于深度学习的计算机辅助诊断(CAD)可以识别CT图像中可疑的肺结节区域，从而提高CT诊断的准确性和效率。本文探讨了两种策略：基于生成模型的数据增强方法和基于嵌入机制的模型结构改进方法。在本研究中，提出了一种新的数据增强方法和一种嵌入机制，前者利用3D像素级统计算法生成肺结节，后者通过引入隐藏变量来更好地理解肺结节样本像素的含义。

    Detection of pulmonary nodules by CT is used for screening lung cancer in early stages.omputer aided diagnosis (CAD) based on deep-learning method can identify the suspected areas of pulmonary nodules in CT images, thus improving the accuracy and efficiency of CT diagnosis. The accuracy and robustness of deep learning models. Method:In this paper, we explore (1) the data augmentation method based on the generation model and (2) the model structure improvement method based on the embedding mechanism. Two strategies have been introduced in this study: a new data augmentation method and a embedding mechanism. In the augmentation method, a 3D pixel-level statistics algorithm is proposed to generate pulmonary nodule and by combing the faked pulmonary nodule and healthy lung, we generate new pulmonary nodule samples. The embedding mechanism are designed to better understand the meaning of pixels of the pulmonary nodule samples by introducing hidden variables. Result: The result of the 3DVNET
    
[^95]: 基于深度学习的IoT设备网络通信分析与识别技术

    IoT Device Identification Based on Network Communication Analysis Using Deep Learning. (arXiv:2303.12800v1 [cs.NI])

    [http://arxiv.org/abs/2303.12800](http://arxiv.org/abs/2303.12800)

    内部网络中允许连接的IoT设备和未知的IoT设备的识别变得越发重要。本研究提出了一种基于深度学习的自动识别方法，可以不需对网络通信进行复杂的特征处理。

    

    随着越来越多不安全的IoT设备的使用，对于攻击者来说，入侵组织的攻击方式越发多样。BYOD政策使得员工可以携带IoT设备进入组织并连接到组织的网络，同时也增加了组织网络受攻击的风险。为了应对这一威胁和保护网络，组织通常实施安全策略，只允许列入白名单的IoT设备连接到网络。为了监测这种策略的合规性，识别组织网络内允许连接的IoT设备与不在白名单中（未知的）IoT设备之间的差异已经变得非常关键。本研究将深度学习应用于网络通信中，实现了对网络内IoT设备的自动识别。与现有方法不同的是，所提出的方法不需要对网络通信进行复杂的特征工程处理。

    Attack vectors for adversaries have increased in organizations because of the growing use of less secure IoT devices. The risk of attacks on an organization's network has also increased due to the bring your own device (BYOD) policy which permits employees to bring IoT devices onto the premises and attach them to the organization's network. To tackle this threat and protect their networks, organizations generally implement security policies in which only white listed IoT devices are allowed on the organization's network. To monitor compliance with such policies, it has become essential to distinguish IoT devices permitted within an organization's network from non white listed (unknown) IoT devices. In this research, deep learning is applied to network communication for the automated identification of IoT devices permitted on the network. In contrast to existing methods, the proposed approach does not require complex feature engineering of the network communication, because the 'communi
    
[^96]: 时间序列视为图像：用视觉transformer处理不规则采样时间序列

    Time Series as Images: Vision Transformer for Irregularly Sampled Time Series. (arXiv:2303.12799v1 [cs.LG])

    [http://arxiv.org/abs/2303.12799](http://arxiv.org/abs/2303.12799)

    本文提出了一种新颖的方法，将不规则采样的时间序列转换为线图像，并适应强大的视觉transformer进行时间序列分类。该方法简化了算法设计，具有通用性，并展示了在多个医疗和人体活动数据集上明显优于最先进的专业算法的表现。

    

    在各个领域中，尤其是在医疗应用中，不规则抽样的时间序列越来越普遍。尽管已经提出了不同的高度定制化方法来解决不规则性问题，但如何有效地模拟它们的复杂动态和高稀疏性仍然是一个开放的问题。本文从全新的角度研究了这个问题：将不规则采样的时间序列转换为线图像，并调整强大的视觉transformer以执行与图像分类相同的时间序列分类。我们的方法在不假设先前知识的情况下大大简化了算法设计，并且可以被潜在地扩展为一个通用框架。尽管其简单性，我们展示了它在几个流行的医疗保健和人体活动数据集上明显优于最先进的专业算法。特别是在具有挑战性的离传感器设置中，即在测试期间屏蔽变量的子集中，性能比最佳基准提高了高达11％。

    Irregularly sampled time series are becoming increasingly prevalent in various domains, especially in medical applications. Although different highly-customized methods have been proposed to tackle irregularity, how to effectively model their complicated dynamics and high sparsity is still an open problem. This paper studies the problem from a whole new perspective: transforming irregularly sampled time series into line graph images and adapting powerful vision transformers to perform time series classification in the same way as image classification. Our approach largely simplifies algorithm designs without assuming prior knowledge and can be potentially extended as a general-purpose framework. Despite its simplicity, we show that it substantially outperforms state-of-the-art specialized algorithms on several popular healthcare and human activity datasets. Especially in the challenging leave-sensors-out setting where a subset of variables is masked during testing, the performance impr
    
[^97]: 用毫米波雷达和IMUs实现人际距离跟踪

    Interpersonal Distance Tracking with mmWave Radar and IMUs. (arXiv:2303.12798v1 [cs.NI])

    [http://arxiv.org/abs/2303.12798](http://arxiv.org/abs/2303.12798)

    本文介绍了ImmTrack，它使用毫米波雷达和惯性测量单元数据来跟踪人际距离，并可以将惯性数据转移到雷达感测结果中。在更广泛的意义上，设备是第一个融合毫米波雷达和惯性测量单元数据用于同时用户跟踪和重新识别的系统。

    

    跟踪人际距离对于实时社交距离管理和事后接触追踪以防止传染病的传播至关重要。这篇论文介绍了ImmTrack，该系统利用毫米波雷达和用户携带的智能手机或可穿戴设备的惯性测量数据来跟踪人际距离。通过将从雷达和惯性数据重建的移动轨迹进行匹配，惯性数据的伪身份可以在全局坐标系中转移到雷达感测结果中。重新识别的雷达感测移动轨迹然后用于跟踪人际距离。在更广泛的意义上，ImmTrack是第一个融合毫米波雷达和惯性测量单元数据用于同时用户跟踪和重新识别的系统。

    Tracking interpersonal distances is essential for real-time social distancing management and {\em ex-post} contact tracing to prevent spreads of contagious diseases. Bluetooth neighbor discovery has been employed for such purposes in combating COVID-19, but does not provide satisfactory spatiotemporal resolutions. This paper presents ImmTrack, a system that uses a millimeter wave radar and exploits the inertial measurement data from user-carried smartphones or wearables to track interpersonal distances. By matching the movement traces reconstructed from the radar and inertial data, the pseudo identities of the inertial data can be transferred to the radar sensing results in the global coordinate system. The re-identified, radar-sensed movement trajectories are then used to track interpersonal distances. In a broader sense, ImmTrack is the first system that fuses data from millimeter wave radar and inertial measurement units for simultaneous user tracking and re-identification. Evaluati
    
[^98]: 一种用于深度神经网络架构和超参数优化的算法框架

    An algorithmic framework for the optimization of deep neural networks architectures and hyperparameters. (arXiv:2303.12797v1 [cs.NE])

    [http://arxiv.org/abs/2303.12797](http://arxiv.org/abs/2303.12797)

    本文提出一种基于进化的有向无环图的算法框架，用于自动生成高效且灵活的深度神经网络并优化相关的超参数。此框架可用于任何能够处理混合搜索空间的元启发式算法，并在时间序列预测数据集上表现出比已有模型更好的性能。

    

    本文提出一种算法框架，用于自动生成高效的深度神经网络并优化相关的超参数。该框架基于进化的有向无环图(DAG)，定义了比文献中现有的搜索空间更为灵活的搜索空间，允许混合使用传统操作，如卷积、循环和密集层，以及较为新颖的操作，如自注意力机制。基于该搜索空间，我们提出了邻域搜索算子和演化搜索算子，以优化网络的架构和超参数。这些搜索算子可与任何能够处理混合搜索空间的元启发式算法一起使用。我们在时间序列预测数据集上使用进化算法测试了我们的算法框架。结果表明，我们的框架能够找到在许多数据集上性能优于基准模型的模型。

    In this paper, we propose an algorithmic framework to automatically generate efficient deep neural networks and optimize their associated hyperparameters. The framework is based on evolving directed acyclic graphs (DAGs), defining a more flexible search space than the existing ones in the literature. It allows mixtures of different classical operations: convolutions, recurrences and dense layers, but also more newfangled operations such as self-attention. Based on this search space we propose neighbourhood and evolution search operators to optimize both the architecture and hyper-parameters of our networks. These search operators can be used with any metaheuristic capable of handling mixed search spaces. We tested our algorithmic framework with an evolutionary algorithm on a time series prediction benchmark. The results demonstrate that our framework was able to find models outperforming the established baseline on numerous datasets.
    
[^99]: 使用预训练模型进行抽象文本摘要的分析

    An Analysis of Abstractive Text Summarization Using Pre-trained Models. (arXiv:2303.12796v1 [cs.CL])

    [http://arxiv.org/abs/2303.12796](http://arxiv.org/abs/2303.12796)

    本文对使用预训练模型进行文本摘要的方法进行了评估，并在不同数据集上进行了比较，结果表明......

    

    人们现在使用像谷歌、雅虎和必应这样的搜索引擎来查找互联网上的信息。由于数据爆炸，如果为用户提供相关的搜索结果摘要而不仅仅是网页链接将会很有帮助。文本摘要已成为帮助用户迅速掌握大量信息的关键方法。在本文中，对不同的预训练模型进行了在不同数据集上的评估。具体来说，我们使用了三个不同的预训练模型，分别是google/pegasus-cnn-dailymail、T5-base、facebook/bart-large-cnn。我们考虑了三个不同的数据集，分别是CNN-dailymail、SAMSum和BillSum，以从上述三个模型中获取输出。通过ROUGH和BLEU指标，在这些不同的数据集上比较了这些预训练模型，每个数据集有2000个示例。

    People nowadays use search engines like Google, Yahoo, and Bing to find information on the Internet. Due to explosion in data, it is helpful for users if they are provided relevant summaries of the search results rather than just links to webpages. Text summarization has become a vital approach to help consumers swiftly grasp vast amounts of information.In this paper, different pre-trained models for text summarization are evaluated on different datasets. Specifically, we have used three different pre-trained models, namely, google/pegasus-cnn-dailymail, T5-base, facebook/bart-large-cnn. We have considered three different datasets, namely, CNN-dailymail, SAMSum and BillSum to get the output from the above three models. The pre-trained models are compared over these different datasets, each of 2000 examples, through ROUGH and BLEU metrics.
    
[^100]: 基于命名实体识别的研究亮点自动生成技术研究

    Named Entity Recognition Based Automatic Generation of Research Highlights. (arXiv:2303.12795v1 [cs.CL])

    [http://arxiv.org/abs/2303.12795](http://arxiv.org/abs/2303.12795)

    该研究使用命名实体识别技术自动生成研究亮点，探究其是否能提高生成亮点的质量。实验结果表明，增加命名实体信息可以提高生成亮点的性能。

    

    传统科学论文摘要用于总结论文内容。近期，研究亮点作为摘要的补充，聚焦于论文的主要发现，但使用频率还不如摘要普遍。该研究旨在使用论文不同部分的输入，自动生成研究亮点。研究使用命名实体识别技术，探究它能否改进生成研究亮点的质量。研究使用两个深度学习模型：第一个是指针-生成器网络，第二个在第一个模型的基础上增加了覆盖机制。 然后将上述每个模型与命名实体识别特征相结合。该方法可用于为缺少亮点的论文生成亮点。实验结果显示，增加命名实体信息可以提高深度学习模型生成高质量研究亮点的性能。

    A scientific paper is traditionally prefaced by an abstract that summarizes the paper. Recently, research highlights that focus on the main findings of the paper have emerged as a complementary summary in addition to an abstract. However, highlights are not yet as common as abstracts, and are absent in many papers. In this paper, we aim to automatically generate research highlights using different sections of a research paper as input. We investigate whether the use of named entity recognition on the input improves the quality of the generated highlights. In particular, we have used two deep learning-based models: the first is a pointer-generator network, and the second augments the first model with coverage mechanism. We then augment each of the above models with named entity recognition features. The proposed method can be used to produce highlights for papers with missing highlights. Our experiments show that adding named entity information improves the performance of the deep learn
    
[^101]: 通过信息度量的下界贝叶斯风险

    Lower Bound on the Bayesian Risk via Information Measure. (arXiv:2303.12497v1 [cs.IT])

    [http://arxiv.org/abs/2303.12497](http://arxiv.org/abs/2303.12497)

    新提出一种方法计算贝叶斯风险下界，允许使用几乎任何信息度量，能提供与估计器无关的不可能结果。已应用于离散和连续参数问题，与最先进的技术进行了比较。

    

    本文关注参数估计，介绍了一种新的方法来计算贝叶斯风险下界。该方法允许使用几乎任何信息度量，包括Rényi的α，φ-分歧和Sibson的α-互信息。该 方法将分歧视为度量的函数，并利用度量空间和函数空间之间的对偶性。特别地，我们展示了通过马尔可夫不等式对其对偶进行上界限制，就可以用任何信息度量计算风险的下界。因此，由于分歧满足数据处理不等式，我们能够提供与估计器无关的不可能结果。然后将这些结果应用于涉及离散和连续参数的有趣问题，包括“捉迷藏”问题，并与最先进的技术进行比较。重要的观察是下界在样本数上的行为受到t的影响。

    This paper focuses on parameter estimation and introduces a new method for lower bounding the Bayesian risk. The method allows for the use of virtually \emph{any} information measure, including R\'enyi's $\alpha$, $\varphi$-Divergences, and Sibson's $\alpha$-Mutual Information. The approach considers divergences as functionals of measures and exploits the duality between spaces of measures and spaces of functions. In particular, we show that one can lower bound the risk with any information measure by upper bounding its dual via Markov's inequality. We are thus able to provide estimator-independent impossibility results thanks to the Data-Processing Inequalities that divergences satisfy. The results are then applied to settings of interest involving both discrete and continuous parameters, including the ``Hide-and-Seek'' problem, and compared to the state-of-the-art techniques. An important observation is that the behaviour of the lower bound in the number of samples is influenced by t
    
[^102]: 延迟感知的分层联邦学习

    Delay-Aware Hierarchical Federated Learning. (arXiv:2303.12414v1 [cs.LG])

    [http://arxiv.org/abs/2303.12414](http://arxiv.org/abs/2303.12414)

    本论文提出了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率，并实现了一些政策以减少能量消耗和边缘到云端的通信。

    

    联邦学习作为一种在分布式环境下训练模型的方法，已经越来越受到关注。本文引入了延迟感知的联邦学习(DFL)，通过解决边缘和云之间的通信延迟，提高了分布式机器学习模型训练的效率。DFL在每个全局聚合间隔期间对设备数据集执行多个随机梯度下降迭代，并通过边缘服务器在本地子网络中间断地聚合模型参数。云服务器通过局部-全局合并器将本地模型与全局部署模型同步。DFL的收敛行为在广义数据异质性度量下进行了理论研究。得出了一组条件，以实现O(1/k)的次线性收敛率。基于这些发现，开发了一个自适应控制算法来实现DFL，并实现了一些政策以减少能量消耗和边缘到云端的通信。

    Federated learning has gained popularity as a means of training models distributed across the wireless edge. The paper introduces delay-aware federated learning (DFL) to improve the efficiency of distributed machine learning (ML) model training by addressing communication delays between edge and cloud. DFL employs multiple stochastic gradient descent iterations on device datasets during each global aggregation interval and intermittently aggregates model parameters through edge servers in local subnetworks. The cloud server synchronizes the local models with the global deployed model computed via a local-global combiner at global synchronization. The convergence behavior of DFL is theoretically investigated under a generalized data heterogeneity metric. A set of conditions is obtained to achieve the sub-linear convergence rate of O(1/k). Based on these findings, an adaptive control algorithm is developed for DFL, implementing policies to mitigate energy consumption and edge-to-cloud co
    
[^103]: 图上随机逆问题：分布式在线学习

    Random Inverse Problems Over Graphs: Decentralized Online Learning. (arXiv:2303.11789v1 [cs.LG])

    [http://arxiv.org/abs/2303.11789](http://arxiv.org/abs/2303.11789)

    本文提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来，并发展了一种新的L2-渐近稳定性理论。该算法在网络图为连通且正向算子序列满足无限维度时空励磁条件的情况下，能够实现均方和几乎必然的强一致估计。

    

    我们建立了一个随机逆问题的框架，该问题具有实时的图上观测，并提出了一种基于在线数据流的分布式在线学习算法，将希尔伯特空间中的分布参数估计和再生核希尔伯特空间中的最小均方问题统一起来。我们将算法收敛性转化为带有L2有界鞅差分项的希尔伯特空间中随机时变差分方程的渐近稳定性，并发展了L2-渐近稳定性理论。结果表明，如果网络图是连通的，并且正向算子序列满足无限维度时空励磁条件，则所有节点的估计均为均方和几乎必然的强一致的。通过将RKHS中的分布式学习问题等效地转化为图上随机逆问题，我们提出了一种基于无中心节点的RKHS分布式在线学习算法。

    We establish a framework of random inverse problems with real-time observations over graphs, and present a decentralized online learning algorithm based on online data streams, which unifies the distributed parameter estimation in Hilbert space and the least mean square problem in reproducing kernel Hilbert space (RKHS-LMS). We transform the algorithm convergence into the asymptotic stability of randomly time-varying difference equations in Hilbert space with L2-bounded martingale difference terms and develop the L2 -asymptotic stability theory. It is shown that if the network graph is connected and the sequence of forward operators satisfies the infinitedimensional spatio-temporal persistence of excitation condition, then the estimates of all nodes are mean square and almost surely strongly consistent. By equivalently transferring the distributed learning problem in RKHS to the random inverse problem over graphs, we propose a decentralized online learning algorithm in RKHS based on no
    
[^104]: CroSel: 用于部分标签学习的自信伪标签的跨选择

    CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning. (arXiv:2303.10365v1 [cs.LG])

    [http://arxiv.org/abs/2303.10365](http://arxiv.org/abs/2303.10365)

    CroSel是一种处理伪标签噪声的新方法，通过利用历史预测信息和一致性正则化项来准确识别部分标签数据的真实标签。

    

    部分标签学习(PLL)是一个重要的弱监督学习问题，它允许每个训练示例有一个候选标签集，而不是一个单一的ground-truth标签。已经广泛探索了基于识别的方法来解决PLL中的标签歧义问题，这些方法将真实标签视为要识别的潜在变量。然而，准确和完整地识别真实标签仍然具有挑战性，这会在模型训练过程中导致伪标签中的噪声。本文提出了一种名为CroSel的新方法，该方法利用模型的历史预测信息来识别大多数训练示例的真实标签。首先，我们引入了一种交叉选择策略，使得两个深度模型可以相互选择部分标记数据的真实标签。此外，我们提出了一种新颖的一致性正则化项co-mix，以避免因虚假选择而引起的样本浪费和微小噪声。通过这种方式，CroSel能够挑选出大多数示例的真实标签。

    Partial-label learning (PLL) is an important weakly supervised learning problem, which allows each training example to have a candidate label set instead of a single ground-truth label. Identification-based methods have been widely explored to tackle label ambiguity issues in PLL, which regard the true label as a latent variable to be identified. However, identifying the true labels accurately and completely remains challenging, causing noise in pseudo labels during model training. In this paper, we propose a new method called CroSel, which leverages historical prediction information from models to identify true labels for most training examples. First, we introduce a cross selection strategy, which enables two deep models to select true labels of partially labeled data for each other. Besides, we propose a novel consistent regularization term called co-mix to avoid sample waste and tiny noise caused by false selection. In this way, CroSel can pick out the true labels of most examples 
    
[^105]: 政策梯度算法收敛于几乎线性二次型调节器的全局最优策略

    Policy Gradient Converges to the Globally Optimal Policy for Nearly Linear-Quadratic Regulators. (arXiv:2303.08431v1 [cs.LG])

    [http://arxiv.org/abs/2303.08431](http://arxiv.org/abs/2303.08431)

    本论文研究了强化学习方法在几乎线性二次型调节器系统中找到最优策略的问题，提出了一个策略梯度算法，可以以线性速率收敛于全局最优解。

    

    决策者只获得了非完整信息的非线性控制系统在各种应用中普遍存在。本研究探索了强化学习方法，以找到几乎线性二次型调节器系统中最优策略。我们考虑一个动态系统，结合线性和非线性组成部分，并由相同结构的策略进行管理。在假设非线性组成部分包含具有小型Lipschitz系数的内核的情况下，我们对成本函数的优化进行了表征。虽然成本函数通常是非凸的，但我们确立了全局最优解附近局部的强凸性和光滑性。此外，我们提出了一种初始化机制，以利用这些属性。在此基础上，我们设计了一个策略梯度算法，可以保证以线性速率收敛于全局最优解。

    Nonlinear control systems with partial information to the decision maker are prevalent in a variety of applications. As a step toward studying such nonlinear systems, this work explores reinforcement learning methods for finding the optimal policy in the nearly linear-quadratic regulator systems. In particular, we consider a dynamic system that combines linear and nonlinear components, and is governed by a policy with the same structure. Assuming that the nonlinear component comprises kernels with small Lipschitz coefficients, we characterize the optimization landscape of the cost function. Although the cost function is nonconvex in general, we establish the local strong convexity and smoothness in the vicinity of the global optimizer. Additionally, we propose an initialization mechanism to leverage these properties. Building on the developments, we design a policy gradient algorithm that is guaranteed to converge to the globally optimal policy with a linear rate.
    
[^106]: 多路径学习的动态MRI可行性的多采集轨迹

    Multi PILOT: Learned Feasible Multiple Acquisition Trajectories for Dynamic MRI. (arXiv:2303.07150v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2303.07150](http://arxiv.org/abs/2303.07150)

    本研究基于深度学习技术，通过学习采集轨迹来提高动态MRI的图像重建质量。

    

    动态磁共振成像(Dynamic Magnetic Resonance Imaging, MRI)因其在诊断领域中对人体内部器官和组织的动态成像具有强大的可靠性而成为领先的诊断工具。然而，使用MRI进行高时空分辨率成像需要相对较长的采集时间(因此成本增加)，这导致出现相关的运动伪影并降低了分辨率。压缩感知(Compressed Sensing, CS)技术成为减少MRI采集时间的常用工具，通过按某些采集轨迹在k-空间中进行图像子采样。一些研究特别关注于应用深度学习技术学习这些采集轨迹，以实现更好的图像重建，而不是使用预定义的采集轨迹。据我们所知，学习采集轨迹只在静态MRI的情况下探索过。在本研究中，我们考虑了采集轨迹在动态MRI情况下的学习。

    Dynamic Magnetic Resonance Imaging (MRI) is known to be a powerful and reliable technique for the dynamic imaging of internal organs and tissues, making it a leading diagnostic tool. A major difficulty in using MRI in this setting is the relatively long acquisition time (and, hence, increased cost) required for imaging in high spatio-temporal resolution, leading to the appearance of related motion artifacts and decrease in resolution. Compressed Sensing (CS) techniques have become a common tool to reduce MRI acquisition time by subsampling images in the k-space according to some acquisition trajectory. Several studies have particularly focused on applying deep learning techniques to learn these acquisition trajectories in order to attain better image reconstruction, rather than using some predefined set of trajectories. To the best of our knowledge, learning acquisition trajectories has been only explored in the context of static MRI. In this study, we consider acquisition trajectory l
    
[^107]: 在Bandit反馈下在线内核选择的改进遗憾界限

    Improved Regret Bounds for Online Kernel Selection under Bandit Feedback. (arXiv:2303.05018v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05018](http://arxiv.org/abs/2303.05018)

    本文研究了在线内核选择的带有Bandit反馈的遗憾界限，提出了两种改进的界限。其中一种适用于光滑的损失函数，另一种则适用于Lipschitz损失函数，预期界限分别为$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$和$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$，并将其应用于具有时间约束的在线内核选择中。

    

    本文研究了在线内核选择的带有Bandit反馈的遗憾界限，提出了两种改进的界限。对于光滑的损失函数，我们提出了一种算法，其预期界限为$O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$，其中$L_T(f^\ast_i)$是$\mathbb{H}_{i}=\{f\in\mathcal{H}_i:\Vert f\Vert_{\mathcal{H}_i}\leq U\}$中的最优假设的累积损失。对于Lipschitz损失函数，我们提出了一种算法，其预期界限为$O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$。本文还将这两种算法应用于具有时间约束的在线内核选择中，并证明了新的遗憾界限，从而改进了现有的贡献。

    In this paper, we improve the regret bound for online kernel selection under bandit feedback. Previous algorithm enjoys a $O((\Vert f\Vert^2_{\mathcal{H}_i}+1)K^{\frac{1}{3}}T^{\frac{2}{3}})$ expected bound for Lipschitz loss functions. We prove two types of regret bounds improving the previous bound. For smooth loss functions, we propose an algorithm with a $O(U^{\frac{2}{3}}K^{-\frac{1}{3}}(\sum^K_{i=1}L_T(f^\ast_i))^{\frac{2}{3}})$ expected bound where $L_T(f^\ast_i)$ is the cumulative losses of optimal hypothesis in $\mathbb{H}_{i}=\{f\in\mathcal{H}_i:\Vert f\Vert_{\mathcal{H}_i}\leq U\}$. The data-dependent bound keeps the previous worst-case bound and is smaller if most of candidate kernels match well with the data. For Lipschitz loss functions, we propose an algorithm with a $O(U\sqrt{KT}\ln^{\frac{2}{3}}{T})$ expected bound asymptotically improving the previous bound. We apply the two algorithms to online kernel selection with time constraint and prove new regret bounds matchin
    
[^108]: 作为解释的因果反事实和归属分数在人工智能中的应用

    Attribution-Scores and Causal Counterfactuals as Explanations in Artificial Intelligence. (arXiv:2303.02829v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2303.02829](http://arxiv.org/abs/2303.02829)

    本文介绍了归属分数和因果反事实在解释人工智能中的应用，重点关注因果关系领域中的逻辑推理和分数计算。

    

    在本文中，我们突出了解释对人工智能的普遍影响和解释人工智能的新发展的重要性，包括起源和不同方法之间的联系。我们用简单的术语描述了基于归属分数的数据管理和机器学习的解释，以及在因果关系领域中发现的反事实。我们阐述了在处理反事实时逻辑推理的重要性，以及它们用于计算分数的用途。

    In this expository article we highlight the relevance of explanations for artificial intelligence, in general, and for the newer developments in {\em explainable AI}, referring to origins and connections of and among different approaches. We describe in simple terms, explanations in data management and machine learning that are based on attribution-scores, and counterfactuals as found in the area of causality. We elaborate on the importance of logical reasoning when dealing with counterfactuals, and their use for score computation.
    
[^109]: 通过序贯学习来控制传播：利用还是探索？

    Containing a spread through sequential learning: to exploit or to explore?. (arXiv:2303.00141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00141](http://arxiv.org/abs/2303.00141)

    本文提出了一种通过序贯学习来控制传播的测试和隔离策略，以最小化累积感染人数；可以通过贪心选择节点进行测试并具有性能保证，并设计了基于奖励的方法，在大型网络中具有更好的可计算性。

    

    一种不良接触过程（如COVID-19）的传播可以通过检测和隔离被控制。然而，该过程的时间和空间演化以及通过隔离进行的控制使得检测与一般的主动搜索策略有着根本区别。本文通过一种主动学习的方法，设计了一种测试和隔离策略，以控制传播并在给定的测试预算下最小化累积感染人数。我们证明了通过贪心选择节点进行测试可以优化目标，并具有性能保证。此外，我们进一步设计了一种基于奖励的方法，有效地最小化累积感染人数的上界，并且在大型网络中具有更好的可计算性。然而，这些策略需要了解节点的感染概率，这些概率会动态变化，并且需要通过序列测试进行学习。我们开发了一种消息传递框架来进行感染概率的序贯学习，并展示了我们的策略对演化过程的动态性能具有良好的适应性。

    The spread of an undesirable contact process, such as an infectious disease (e.g. COVID-19), is contained through testing and isolation of infected nodes. The temporal and spatial evolution of the process (along with containment through isolation) render such detection as fundamentally different from active search detection strategies. In this work, through an active learning approach, we design testing and isolation strategies to contain the spread and minimize the cumulative infections under a given test budget. We prove that the objective can be optimized, with performance guarantees, by greedily selecting the nodes to test. We further design reward-based methodologies that effectively minimize an upper bound on the cumulative infections and are computationally more tractable in large networks. These policies, however, need knowledge about the nodes' infection probabilities which are dynamically changing and have to be learned by sequential testing. We develop a message-passing fram
    
[^110]: 一种基于多变量关注机制的双向LSTM编码器-解码器神经网络用于预测混合通风建筑的性能。

    Predicting the performance of hybrid ventilation in buildings using a multivariate attention-based biLSTM Encoder-Decoder neural network. (arXiv:2302.04126v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04126](http://arxiv.org/abs/2302.04126)

    本文研究了一种基于多变量关注机制的双向LSTM编码器-解码器神经网络的能力，用于预测混合通风建筑内窗户开启或关闭时的室内空气温度。

    

    混合通风是一种节能的解决方案，可以为大多数气候提供新鲜的空气，前提是它具有可靠的控制系统。为了使这样的系统得到最优的操作，需要高保真的控制定向模型。它应当能够基于操作条件（如窗户开启和HVAC运行计划）对室内空气温度进行近实时预测。本文研究了一种深度神经网络(DNN)，即一种基于多变量多头关注机制的长短时记忆(LSTM)编码器-解码器神经网络的能力，用于预测窗户开启或关闭时的室内空气温度。训练和测试数据来自于详细的多区域办公楼模型（EnergyPlus）。

    Hybrid ventilation is an energy-efficient solution to provide fresh air for most climates, given that it has a reliable control system. To operate such systems optimally, a high-fidelity control-oriented modesl is required. It should enable near-real time forecast of the indoor air temperature based on operational conditions such as window opening and HVAC operating schedules. However, physics-based control-oriented models (i.e., white-box models) are labour-intensive and computationally expensive. Alternatively, black-box models based on artificial neural networks can be trained to be good estimators for building dynamics. This paper investigates the capabilities of a deep neural network (DNN), which is a multivariate multi-head attention-based long short-term memory (LSTM) encoder-decoder neural network, to predict indoor air temperature when windows are opened or closed. Training and test data are generated from a detailed multi-zone office building model (EnergyPlus). Pseudo-random
    
[^111]: 在在线连续学习中进行实时评估：一个新希望

    Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01047](http://arxiv.org/abs/2302.01047)

    该研究针对现实环境中的连续学习提出了一种实时评估方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了实验。结果表明，简单的基线模型胜过了最先进的CL方法，证明现有方法在现实环境下的适用性存在问题。

    

    当前连续学习方法的评估通常假设在训练时间和计算方面没有限制。这对于任何实际世界的环境都是不现实的。因此我们提出了一种实时评估连续学习的方法，其中流不等待模型完成训练即揭示下一个数据进行预测。为了实现这一点，我们从计算成本的角度评估当前的CL方法，并在包含3900万个时间戳标记图像的大型数据集CLOC上进行了广泛的实验。结果表明，在这种评估下，一个简单的基线模型胜过了最先进的CL方法，这对现有方法在现实环境中的适用性提出了质疑。另外，我们还探讨了文献中常用的各种CL组件，包括记忆采样策略和正则化方法。我们发现，所有考虑的方法都无法与我们的简单基线模型竞争。

    Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
    
[^112]: 通过离线数据提升蒙特卡罗评估方法

    Improving Monte Carlo Evaluation with Offline Data. (arXiv:2301.13734v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13734](http://arxiv.org/abs/2301.13734)

    本论文介绍了通过使用离线数据来提升蒙特卡罗评估方法，实现在保持相同估计准确度的前提下，减少在线样本数量的目的。通过使用一个定制的行为策略，可以比普通的 MC 估计器产生更小的方差。该行为策略可以从现有的离线数据中高效学习，我们的实验表明，相对于现有的最先进方法，我们的方法只需使用小部分在线样本就能实现相同的估计精度。

    

    蒙特卡罗 (MC) 方法是估计策略表现最广泛使用的方法。给定一个感兴趣的策略，MC 方法通过重复运行该策略以收集样本并取出结果平均值来给出估计值。在此过程中收集的样本称为在线样本。为了获得准确的估计值，MC 方法需要消耗大量在线样本。当在线样本昂贵时，例如在线推荐和库存管理，我们希望在实现相同的估计准确度的同时减少在线样本数量。为此，我们使用离线 MC 方法，通过运行不同的策略（称为行为策略）评估感兴趣的策略。我们设计了一个定制的行为策略，使离线 MC 估计器的方差明显小于普通 MC 估计器。重要的是，该定制行为策略可以从现有的离线数据，即先前记录的数据中高效学习，这比在线样本要便宜得多。我们的实验表明，与现有的最先进方法相比，我们的方法只需使用小部分在线样本就能实现相同的估计精度。

    Monte Carlo (MC) methods are the most widely used methods to estimate the performance of a policy. Given an interested policy, MC methods give estimates by repeatedly running this policy to collect samples and taking the average of the outcomes. Samples collected during this process are called online samples. To get an accurate estimate, MC methods consume massive online samples. When online samples are expensive, e.g., online recommendations and inventory management, we want to reduce the number of online samples while achieving the same estimate accuracy. To this end, we use off-policy MC methods that evaluate the interested policy by running a different policy called behavior policy. We design a tailored behavior policy such that the variance of the off-policy MC estimator is provably smaller than the ordinary MC estimator. Importantly, this tailored behavior policy can be efficiently learned from existing offline data, i,e., previously logged data, which are much cheaper than onlin
    
[^113]: 将生物神经网络中的学习解释为零阶优化方法

    Interpreting learning in biological neural networks as zero-order optimization method. (arXiv:2301.11777v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11777](http://arxiv.org/abs/2301.11777)

    本文将大脑视为一种用于有监督学习的统计方法，将生物神经网络中的连接参数的本地更新规则与零阶优化方法相关联，并表明期望值实现了梯度下降的修改版。

    

    最近，针对人工神经网络（ANNs）的统计理解取得了重要进展。ANNs受大脑功能的启发，但在几个关键方面存在差异。特别地，连接参数的本地更新规则在生物神经网络（BNNs）中使得基于梯度下降的学习在生物学上不可信。在本文中，我们将大脑视为一种用于有监督学习的统计方法。主要贡献是将BNNs中的连接参数的本地更新规则与零阶优化方法相关联。研究表明，迭代的期望值实现了梯度下降的修改版。

    Recently, significant progress has been made regarding the statistical understanding of artificial neural networks (ANNs). ANNs are motivated by the functioning of the brain, but differ in several crucial aspects. In particular, the locality in the updating rule of the connection parameters in biological neural networks (BNNs) makes it biologically implausible that the learning of the brain is based on gradient descent. In this work, we look at the brain as a statistical method for supervised learning. The main contribution is to relate the local updating rule of the connection parameters in BNNs to a zero-order optimization method. It is shown that the expected values of the iterates implement a modification of gradient descent.
    
[^114]: 关于时变博弈中无悔学习动态的收敛性问题

    On the Convergence of No-Regret Learning Dynamics in Time-Varying Games. (arXiv:2301.11241v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11241](http://arxiv.org/abs/2301.11241)

    本文研究了时变博弈中乐观梯度下降法（OGD）的收敛性，提出了明确的收敛界限，并建立了适用于时变总和多人博弈和元学习的新型双线性公式。

    

    大多数关于博弈学习的文献都集中于底层重复博弈不发生变化的严格模式下。对于动态多智体游戏中无悔学习算法的收敛性问题，我们知之甚少。在本文中，我们研究了时变博弈中乐观梯度下降法（OGD）的收敛性。我们的框架针对自然变化度量的博弈序列的均衡间隙，为OGD提供了明确的收敛界限，从而涵盖了静态博弈的已知结果。此外，只要每场游戏都进行了多次，我们还通过强凸性-强凹性建立了改进的二阶变化界限。我们的结果还适用于时变的总和多人博弈，通过相关均衡的双线性公式，这对元学习以及获得针对变化依赖性后悔界限的精细需求具有新颖意义。

    Most of the literature on learning in games has focused on the restrictive setting where the underlying repeated game does not change over time. Much less is known about the convergence of no-regret learning algorithms in dynamic multiagent settings. In this paper, we characterize the convergence of optimistic gradient descent (OGD) in time-varying games. Our framework yields sharp convergence bounds for the equilibrium gap of OGD in zero-sum games parameterized on natural variation measures of the sequence of games, subsuming known results for static games. Furthermore, we establish improved second-order variation bounds under strong convexity-concavity, as long as each game is repeated multiple times. Our results also apply to time-varying general-sum multi-player games via a bilinear formulation of correlated equilibria, which has novel implications for meta-learning and for obtaining refined variation-dependent regret bounds, addressing questions left open in prior papers. Finally,
    
[^115]: 通过广义策略优化优先级实现高效多目标学习

    Sample-Efficient Multi-Objective Learning via Generalized Policy Improvement Prioritization. (arXiv:2301.07784v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.07784](http://arxiv.org/abs/2301.07784)

    该论文提出了一种使用广义策略改进优先级来实现高效多目标学习的算法，从而通过主动学习策略，可以识别出每一时刻最有前途的偏好或目标，以更快地解决MORL问题，同时也可以识别出学习特定代理偏好的策略时最相关的历史经验。

    

    多目标强化学习 (MORL) 算法解决了代理在可能冲突的奖励函数上有不同偏好的顺序决策问题。这些算法通常学习一组策略（每个策略都是为不同代理偏好而优化的），这组策略可以后来用于解决具有新偏好的问题。我们介绍了一种新算法，该算法使用广义策略改进 (GPI) 定义了原则上得出的组数，从而提高了固定样本数的学习效率。通过该算法，代理可以实现主动学习策略，并可以在每一时刻确定最有前途的偏好或目标，以更快地解决给定的MORL问题。同时，该算法也可以通过一种新的Dyna风格的MORL方法，识别出学习特定代理偏好的策略时最相关的以往经验。我们证明了我们的算法保证始终在有限的步数内收敛到最优解，或收敛到距离最优解 $\epsilon$-o。

    Multi-objective reinforcement learning (MORL) algorithms tackle sequential decision problems where agents may have different preferences over (possibly conflicting) reward functions. Such algorithms often learn a set of policies (each optimized for a particular agent preference) that can later be used to solve problems with novel preferences. We introduce a novel algorithm that uses Generalized Policy Improvement (GPI) to define principled, formally-derived prioritization schemes that improve sample-efficient learning. They implement active-learning strategies by which the agent can (i) identify the most promising preferences/objectives to train on at each moment, to more rapidly solve a given MORL problem; and (ii) identify which previous experiences are most relevant when learning a policy for a particular agent preference, via a novel Dyna-style MORL method. We prove our algorithm is guaranteed to always converge to an optimal solution in a finite number of steps, or an $\epsilon$-o
    
[^116]: 单张图像无类别3D关节转移的CA$^2$T-Net网络

    CA$^2$T-Net: Category-Agnostic 3D Articulation Transfer from Single Image. (arXiv:2301.02232v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.02232](http://arxiv.org/abs/2301.02232)

    本文介绍了一种可以将单张图像中物体的运动转移到未调整的3D模型中的神经网络方法，可以处理任意类别的对象，训练时只使用合成数据。

    

    本文介绍了一种神经网络方法，可以将单帧图像中关节物体的运动转移到未经调整的3D模型中。我们的网络学习预测物体的姿态、部分分割和相应的运动参数，以重现输入图像中显示的关节运动。网络由三个不同的分支组成，它们采用共享的联合图像形状嵌入，并进行端到端的训练。与以往的方法不同，我们的方法不依赖于对象的拓扑结构，并且可以处理来自任意类别的对象。我们的方法仅使用合成数据进行训练，可以自动地为网格添加动画，从真实图像中推断运动，并在测试时间将运动转移到功能上相似但几何上不同的3D模型。

    We present a neural network approach to transfer the motion from a single image of an articulated object to a rest-state (i.e., unarticulated) 3D model. Our network learns to predict the object's pose, part segmentation, and corresponding motion parameters to reproduce the articulation shown in the input image. The network is composed of three distinct branches that take a shared joint image-shape embedding and is trained end-to-end. Unlike previous methods, our approach is independent of the topology of the object and can work with objects from arbitrary categories. Our method, trained with only synthetic data, can be used to automatically animate a mesh, infer motion from real images, and transfer articulation to functionally similar but geometrically distinct 3D models at test time.
    
[^117]: 不可学习的聚类：面向标签不可知的不可学习样本

    Unlearnable Clusters: Towards Label-agnostic Unlearnable Examples. (arXiv:2301.01217v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2301.01217](http://arxiv.org/abs/2301.01217)

    本文提出了一种更实用的标签不可知设置，以生成不可学习的样本，防止未经授权的机器学习模型训练。

    This paper proposes a more practical label-agnostic setting to generate unlearnable examples, which can prevent unauthorized training of machine learning models.

    在互联网上，越来越多的人对开发不可学习的示例（UEs）来防止视觉隐私泄露感兴趣。UEs是添加了不可见但不可学习噪声的训练样本，已经发现可以防止未经授权的机器学习模型训练。UEs通常是通过一个双层优化框架和一个替代模型生成的，以从原始样本中去除（最小化）错误，然后应用于保护数据免受未知目标模型的攻击。然而，现有的UE生成方法都依赖于一个理想的假设，称为标签一致性，即假定黑客和保护者对于给定的样本持有相同的标签。在这项工作中，我们提出并推广了一个更实用的标签不可知设置，其中黑客可能会以与保护者完全不同的方式利用受保护的数据。例如，由保护者持有的m类不可学习数据集可能被黑客作为n类数据集利用。现有的UE生成方法在这种情况下失效。

    There is a growing interest in developing unlearnable examples (UEs) against visual privacy leaks on the Internet. UEs are training samples added with invisible but unlearnable noise, which have been found can prevent unauthorized training of machine learning models. UEs typically are generated via a bilevel optimization framework with a surrogate model to remove (minimize) errors from the original samples, and then applied to protect the data against unknown target models. However, existing UE generation methods all rely on an ideal assumption called label-consistency, where the hackers and protectors are assumed to hold the same label for a given sample. In this work, we propose and promote a more practical label-agnostic setting, where the hackers may exploit the protected data quite differently from the protectors. E.g., a m-class unlearnable dataset held by the protector may be exploited by the hacker as a n-class dataset. Existing UE generation methods are rendered ineffective in
    
[^118]: 自监督 Cut-and-Paste GAN 进行对象分割

    Self-Supervised Object Segmentation with a Cut-and-Pasting GAN. (arXiv:2301.00366v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00366](http://arxiv.org/abs/2301.00366)

    提出了一种基于自监督的 Cut-and-Paste GAN，用于前景对象分割和组合图像生成，无需手动标注，通过学习全局数据表示和语义结构信息，实现了有意义的遮罩生成。

    

    本文提出了一种基于自监督的 Cut-and-Paste GAN，用于进行前景对象分割并生成逼真的组合图像，无需手动标注。通过一个简单而有效的自监督方法，结合基于 U-Net 的鉴别器，完成了这一目标。该方法扩展了标准鉴别器的能力，不仅通过分类（真/假）学习全局数据表示，而且还通过使用自监督任务创建的伪标签学习语义和结构信息。该方法使生成器能够通过强制它从辨别器学习每像素的有意义的信息和全局图像反馈来创建有意义的遮罩。实验表明，我们提出的方法在标准基准数据集上显著优于现有最先进方法。

    This paper proposes a novel self-supervised based Cut-and-Paste GAN to perform foreground object segmentation and generate realistic composite images without manual annotations. We accomplish this goal by a simple yet effective self-supervised approach coupled with the U-Net based discriminator. The proposed method extends the ability of the standard discriminators to learn not only the global data representations via classification (real/fake) but also learn semantic and structural information through pseudo labels created using the self-supervised task. The proposed method empowers the generator to create meaningful masks by forcing it to learn informative per-pixel as well as global image feedback from the discriminator. Our experiments demonstrate that our proposed method significantly outperforms the state-of-the-art methods on the standard benchmark datasets.
    
[^119]: HAC-Net:一种基于注意力机制的混合卷积神经网络，用于高精度的蛋白质配体结合亲和力预测。

    HAC-Net: A Hybrid Attention-Based Convolutional Neural Network for Highly Accurate Protein-Ligand Binding Affinity Prediction. (arXiv:2212.12440v3 [q-bio.BM] UPDATED)

    [http://arxiv.org/abs/2212.12440](http://arxiv.org/abs/2212.12440)

    本文提出了一种新的深度学习架构HAC-Net，结合了三维卷积神经网络和两个图卷积网络的注意力机制，能够高精度地预测蛋白质与配体的结合亲和力，并在公认的基准测试集上获得最先进的结果。

    

    应用图像检测和图论中的深度学习概念极大地推进了蛋白-配体结合亲和力的预测，这是药物发现和蛋白工程领域的重要挑战。我们设计了一种新型深度学习架构HAC-Net，包括一个三维卷积神经网络和两个基于注意力聚合节点特征的图卷积网络。在蛋白质配体结合亲和力预测的领域公认的基准测试集PDBbind v.2016 core set上，HAC-Net获得了最先进的结果。我们还使用多个训练-测试划分全面评估模型的普适性，每个划分都最大化了训练集和测试集复合物的蛋白结构、蛋白序列或配体扩展连接指纹之间的差异。此外，我们使用相似性测度进行了十倍交叉验证。

    Applying deep learning concepts from image detection and graph theory has greatly advanced protein-ligand binding affinity prediction, a challenge with enormous ramifications for both drug discovery and protein engineering. We build upon these advances by designing a novel deep learning architecture consisting of a 3-dimensional convolutional neural network utilizing channel-wise attention and two graph convolutional networks utilizing attention-based aggregation of node features. HAC-Net (Hybrid Attention-Based Convolutional Neural Network) obtains state-of-the-art results on the PDBbind v.2016 core set, the most widely recognized benchmark in the field. We extensively assess the generalizability of our model using multiple train-test splits, each of which maximizes differences between either protein structures, protein sequences, or ligand extended-connectivity fingerprints of complexes in the training and test sets. Furthermore, we perform 10-fold cross-validation with a similarity 
    
[^120]: 利用神经常微分方程学习亚网格尺度模型

    Learning Subgrid-scale Models with Neural Ordinary Differential Equations. (arXiv:2212.09967v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2212.09967](http://arxiv.org/abs/2212.09967)

    本文提出了利用神经常微分方程学习亚网格尺度模型的新方法，可以提高计算流体动力学求解器的准确性和效率。该方法具有NODE优点，可以参数化亚网格尺度、近似耦合算子，并提高低阶求解器的效率。

    

    我们提出了一种新方法来学习偏微分方程(PDEs)的亚网格尺度模型。该方法基于神经常微分方程(NODE)来解决计算上的挑战，可以使用神经网络来学习从粗网格到细网格的映射，从而实现亚网格尺度参数化。我们提出了一种策略，利用NODE和部分知识来在连续级别上学习源动力学。该方法继承了NODE的优点，可以用于参数化亚网格尺度、近似耦合算子，并提高低阶求解器的效率。数值结果表明了我们方法的有效性。

    We propose a new approach to learning the subgrid-scale model when simulating partial differential equations (PDEs) solved by the method of lines and their representation in chaotic ordinary differential equations, based on neural ordinary differential equations (NODEs). Solving systems with fine temporal and spatial grid scales is an ongoing computational challenge, and closure models are generally difficult to tune. Machine learning approaches have increased the accuracy and efficiency of computational fluid dynamics solvers. In this approach neural networks are used to learn the coarse- to fine-grid map, which can be viewed as subgrid-scale parameterization. We propose a strategy that uses the NODE and partial knowledge to learn the source dynamics at a continuous level. Our method inherits the advantages of NODEs and can be used to parameterize subgrid scales, approximate coupling operators, and improve the efficiency of low-order solvers. Numerical results with the two-scale Loren
    
[^121]: 切片最优偏转运输

    Sliced Optimal Partial Transport. (arXiv:2212.08049v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.08049](http://arxiv.org/abs/2212.08049)

    本文提出了一种适用于一维非负测度之间最优偏转运输问题的高效算法，并通过切片的方式定义了切片最优偏转运输距离。

    

    最优传输（OT）已经在机器学习、数据科学和计算机视觉中变得极其流行。OT问题的核心假设是源和目标测度的总质量相等，这限制了它的应用。最优偏转运输（OPT）是最近提出的解决这个限制的方法。与OT问题类似，OPT的计算依赖于解决线性规划问题（通常在高维度中），这可能会变得计算上困难。在本文中，我们提出了一种计算一维非负测度之间OPT问题的有效算法。接下来，遵循切片OT距离的思想，我们利用切片定义了切片OPT距离。最后，我们展示了切片OPT-based方法在各种数值实验中的计算和精度优势。特别是，我们展示了我们提出的Sliced-OPT在噪声点云配准中的应用。

    Optimal transport (OT) has become exceedingly popular in machine learning, data science, and computer vision. The core assumption in the OT problem is the equal total amount of mass in source and target measures, which limits its application. Optimal Partial Transport (OPT) is a recently proposed solution to this limitation. Similar to the OT problem, the computation of OPT relies on solving a linear programming problem (often in high dimensions), which can become computationally prohibitive. In this paper, we propose an efficient algorithm for calculating the OPT problem between two non-negative measures in one dimension. Next, following the idea of sliced OT distances, we utilize slicing to define the sliced OPT distance. Finally, we demonstrate the computational and accuracy benefits of the sliced OPT-based method in various numerical experiments. In particular, we show an application of our proposed Sliced-OPT in noisy point cloud registration.
    
[^122]: 基于空间选择的深度非线性滤波器的语音提取

    Spatially Selective Deep Non-linear Filters for Speaker Extraction. (arXiv:2211.02420v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2211.02420](http://arxiv.org/abs/2211.02420)

    本文提出基于空间选择的深度非线性滤波器，采用简单且有效的条件机制，可以在任意目标方向对其进行导向。该滤波器可以用于多人语音分离，实现非常准确的多说话者定位。

    

    在多人同时说话的情况下，信号的空间特征是提取目标信号最显著的特征。在本文中，我们开发了一种深度联合空间谱非线性滤波器，可以在任意目标方向上进行导向。为此，我们提出了一种简单而有效的条件机制，该机制基于目标方向设置过滤器递归层的初始状态。我们表明，这种方案比基线方法更有效，并增加了滤波器的灵活性，并且不会降低性能成本。所得到的空间选择性非线性滤波器也可以用于任意数量说话者的语音分离，并且可以实现非常准确的多说话者定位，本文中已经有所展示。

    In a scenario with multiple persons talking simultaneously, the spatial characteristics of the signals are the most distinct feature for extracting the target signal. In this work, we develop a deep joint spatial-spectral non-linear filter that can be steered in an arbitrary target direction. For this we propose a simple and effective conditioning mechanism, which sets the initial state of the filter's recurrent layers based on the target direction. We show that this scheme is more effective than the baseline approach and increases the flexibility of the filter at no performance cost. The resulting spatially selective non-linear filters can also be used for speech separation of an arbitrary number of speakers and enable very accurate multi-speaker localization as we demonstrate in this paper.
    
[^123]: 一种深度学习可解释性的鲁棒互信息估计器

    A robust estimator of mutual information for deep learning interpretability. (arXiv:2211.00024v2 [physics.data-an] UPDATED)

    [http://arxiv.org/abs/2211.00024](http://arxiv.org/abs/2211.00024)

    该论文提出了一种鲁棒的互信息（MI）估计器，名为GMM-MI，它可以用于解释深度学习模型内部的工作机制，并在表示学习的情境下进行了验证。

    

    我们发展了互信息（MI）在信息论中广泛应用的度量方式，用于解释深度学习模型内部的工作机制。为了准确地从有限数量的样本中估计MI，我们提出了GMM-MI（读作“Jimmie”），这是一种基于高斯混合模型的算法，可用于离散和连续设置。GMM-MI在计算效率上是高效的，对于超参数的选择具有鲁棒性，并提供由于有限样本大小而引起的MI估计的不确定性。我们广泛地验证了GMM-MI在玩具数据上的表现，对比了已确定的互信息估计器的性能。然后，在表示学习的情境下，我们使用合成数据和描述高度非线性过程的物理数据集，训练深度学习模型，将高维数据编码为有意义的压缩（潜在）表示，并使用GMM-MI进行解释。

    We develop the use of mutual information (MI), a well-established metric in information theory, to interpret the inner workings of deep learning models. To accurately estimate MI from a finite number of samples, we present GMM-MI (pronounced $``$Jimmie$"$), an algorithm based on Gaussian mixture models that can be applied to both discrete and continuous settings. GMM-MI is computationally efficient, robust to the choice of hyperparameters and provides the uncertainty on the MI estimate due to the finite sample size. We extensively validate GMM-MI on toy data for which the ground truth MI is known, comparing its performance against established mutual information estimators. We then demonstrate the use of our MI estimator in the context of representation learning, working with synthetic data and physical datasets describing highly non-linear processes. We train deep learning models to encode high-dimensional data within a meaningful compressed (latent) representation, and use GMM-MI to q
    
[^124]: 对称性、平滑极小值和梯度流的守恒量

    Symmetries, flat minima, and the conserved quantities of gradient flow. (arXiv:2210.17216v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17216](http://arxiv.org/abs/2210.17216)

    该论文发现了一种通用框架，可以在参数空间中寻找连续对称性的方法，这种对称性可以雕刻出低损坏山谷。论文提出了一组新的非线性数据相关对称性，用于将训练好的模型变形，提高对某些对抗性攻击的鲁棒性并发现了梯度流的平坦极小值偏差问题，提出了一种改善梯度流寻找良好解的能力的方法。

    

    深度网络损失景观的经验研究表明，许多局部极小值通过低损耗山谷相连。然而，关于这些山谷的理论起源知之甚少。我们提出了一个在参数空间中寻找连续对称性的通用框架，该对称性雕刻了低损坏山谷。我们的框架利用了激活函数的等变性，并可应用于不同的层架构。为了将这个框架推广到非线性神经网络，我们引入了一组新的非线性数据相关对称性。这些对称性可以使训练好的模型变形，从而在新的样本上表现出相似的性能，这允许集成建立，提高对某些对抗性攻击的鲁棒性。然后，我们展示了与线性对称性相关的守恒量可用于定义沿着低损坏山谷的坐标系。这些守恒量有助于揭示使用常见初始化方法时，梯度流只探索了损失景观的一小部分，我们将其称为平坦极小值偏差。我们的框架提供了一种减轻这种偏差，改善梯度流寻找良好解的能力的方法。

    Empirical studies of the loss landscape of deep networks have revealed that many local minima are connected through low-loss valleys. Yet, little is known about the theoretical origin of such valleys. We present a general framework for finding continuous symmetries in the parameter space, which carve out low-loss valleys. Our framework uses equivariances of the activation functions and can be applied to different layer architectures. To generalize this framework to nonlinear neural networks, we introduce a novel set of nonlinear, data-dependent symmetries. These symmetries can transform a trained model such that it performs similarly on new samples, which allows ensemble building that improves robustness under certain adversarial attacks. We then show that conserved quantities associated with linear symmetries can be used to define coordinates along low-loss valleys. The conserved quantities help reveal that using common initialization methods, gradient flow only explores a small part 
    
[^125]: 代码生成模型的多语言评估

    Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14868](http://arxiv.org/abs/2210.14868)

    本研究提出了多种用于评估代码生成模型的新基准测试，能够以多语言方式评估模型性能，并探讨了多语言模型优于单语言模型、少量提示能教授模型新语言以及在单语言设置下拥有零-shot翻译能力等问题。

    

    我们提出了新的基准测试，用于评估代码生成模型：MBXP和Multilingual HumanEval，以及MathQA-X。这些数据集涵盖了10种以上的编程语言，并使用可扩展的转换框架将原始Python数据集中的提示和测试用例转译成目标语言中的相应数据。利用这些基准测试，我们能够以多语言方式评估代码生成模型的性能，并发现了语言模型在跨领域语言上的泛化能力、多语言模型在单语言模型上的优势、少量提示教授模型新语言的能力，以及在单语言设置下的零-shot翻译能力。此外，我们使用我们的代码生成模型进行大规模引导，以获取多种语言的合成规范解，这些解可用于其他与代码相关的评估，如代码插入、鲁棒性或摘要任务。总的来说，

    We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
    
[^126]: 用超参数化循环神经网络学习低维状态空间

    Learning Low Dimensional State Spaces with Overparameterized Recurrent Neural Nets. (arXiv:2210.14064v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14064](http://arxiv.org/abs/2210.14064)

    本文通过对超参数化线性RNN的权重假设，发现GD可以学习低维度状态空间，捕获长期动态。

    

    在深度学习中，过度参数化通常是指训练出来的神经网络具有多种表达能力，可以同时适合训练数据的多种方式，其中一些能够很好地推广，而另一些则不能。在循环神经网络（RNN）中，存在一层额外的过度参数化，意味着模型可能存在许多解，适用于训练中看到的序列长度，其中一些可以推广到较长序列，而另一些则不能。本文分析了当应用于超参数化线性RNN时，GD的外推属性。与最近提出的暗示GD偏向于短期记忆的论点相反，我们提供了证据表明GD可以学习低维状态空间，捕获长期记忆。具体而言，在对超完备线性RNN的权重做出某些假设的情况下，损失景观包含一个模式，在该模式下，参数数量超过了表示底层时间序列长期动态所需的维数。在这种情况下，GD会收敛于一个解，实现捕获长期动态所需的最小维度。

    Overparameterization in deep learning typically refers to settings where a trained neural network (NN) has representational capacity to fit the training data in many ways, some of which generalize well, while others do not. In the case of Recurrent Neural Networks (RNNs), there exists an additional layer of overparameterization, in the sense that a model may exhibit many solutions that generalize well for sequence lengths seen in training, some of which extrapolate to longer sequences, while others do not. Numerous works have studied the tendency of Gradient Descent (GD) to fit overparameterized NNs with solutions that generalize well. On the other hand, its tendency to fit overparameterized RNNs with solutions that extrapolate has been discovered only recently and is far less understood. In this paper, we analyze the extrapolation properties of GD when applied to overparameterized linear RNNs. In contrast to recent arguments suggesting an implicit bias towards short-term memory, we pr
    
[^127]: 具有凸和非凸子线性回归的研究及其在数据驱动的到达集学习中的应用。

    Convex and Nonconvex Sublinear Regression with Application to Data-driven Learning of Reach Sets. (arXiv:2210.01919v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.01919](http://arxiv.org/abs/2210.01919)

    本文提出了使用支撑函数对紧致集合进行学习的方法，并提出了两种算法进行子线性回归，分别为凸规划和非凸规划。本文在受控动态到达集的应用中进行了实验。

    

    本文考虑通过逐渐逼近一致函数（support function）的子线性回归方法，从有限数据中估计一个紧致集合。支撑函数在凸封闭运算的意义下能够唯一地刻画一个紧致集合，而且是子线性（凸以及一次正齐次的）。相反，任何子线性函数都是一个紧致集合的支撑函数。我们利用这一性质，将学习一个紧致集合的任务转化为学习它的支撑函数。为了进行子线性回归，我们提出了两种算法，一种是通过凸规划求解二次规划（QP）得到支撑函数，另一种是通过训练子线性神经网络实现非凸规划得到支撑函数。并通过数值实验展示了这些算法在轨迹数据中学习具有集合值输入不确定性的受控动态到达集的应用。

    We consider estimating a compact set from finite data by approximating the support function of that set via sublinear regression. Support functions uniquely characterize a compact set up to closure of convexification, and are sublinear (convex as well as positive homogeneous of degree one). Conversely, any sublinear function is the support function of a compact set. We leverage this property to transcribe the task of learning a compact set to that of learning its support function. We propose two algorithms to perform the sublinear regression, one via convex and another via nonconvex programming. The convex programming approach involves solving a quadratic program (QP). The nonconvex programming approach involves training a input sublinear neural network. We illustrate the proposed methods via numerical examples on learning the reach sets of controlled dynamics subject to set-valued input uncertainties from trajectory data.
    
[^128]: Omnigrok：理解超越算法数据的“Grokking”

    Omnigrok: Grokking Beyond Algorithmic Data. (arXiv:2210.01117v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01117](http://arxiv.org/abs/2210.01117)

    本文通过分析神经网络的损失景观，发现训练和测试损失之间的不匹配是grokking的原因，提出了“LU机制”，并成功诱导了算法数据集的grokking和消除了其grokking现象。它们的dramatic grokking依赖于表示学习。

    

    Grokking是一种不寻常的现象，指算法数据集在过拟合训练数据后长时间仍然能进行泛化，一直以来一直难以理解。本文旨在通过分析神经网络的损失景观来理解grokking，并确定训练和测试损失之间的不匹配是grokking的原因。我们将其称为“LU机制”，因为训练和测试损失（对模型权重规范）通常分别类似于“L”和“U”。这个简单的机制可以很好地解释grokking的许多方面：数据大小依赖性、权重衰减依赖性、表示的出现等。在直觉上给定的基础上，我们能够在涉及图像、语言和分子的任务中诱导grokking。反向来看，我们能够消除算法数据集的grokking。我们将算法数据集的dramatic grokking归因于表示学习。

    Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.
    
[^129]: 利用新数据在电力网通讯中的潜力

    Leveraging the Potential of Novel Data in Power Line Communication of Electricity Grids. (arXiv:2209.12693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.12693](http://arxiv.org/abs/2209.12693)

    本文提出了两个基于宽带电力线通信测量的首个数据集FiN-1和FiN-2，共收集了130亿个数据点。这些数据可以应用于资产管理、电网状态可视化和预测，以解决电力网在向可再生能源过渡和复杂负载配置的情况下面临的新挑战。

    

    电力网已成为日常生活中不可或缺的一部分，尽管在日常生活中往往不会注意到。只有当电力网不再可用时，我们通常才会特别意识到这种依赖。然而，重大变化，如向可再生能源（光伏、风力涡轮机等）的过渡以及复杂负载配置（电动汽车、家庭电池系统等）的能源消费者数量的增加，给电力网带来了新的挑战。为了解决这些挑战，我们提出了两个基于宽带电力线通信（PLC）基础设施测量的首个数据集。这两个数据集 FiN-1 和 FiN-2 在德国低压电网的一部分实际使用中收集，向大约440万人提供服务，并显示5100多个传感器收集的超过130亿个数据点。此外，我们提出了不同的用例，用于资产管理、电网状态可视化和预测。

    Electricity grids have become an essential part of daily life, even if they are often not noticed in everyday life. We usually only become particularly aware of this dependence by the time the electricity grid is no longer available. However, significant changes, such as the transition to renewable energy (photovoltaic, wind turbines, etc.) and an increasing number of energy consumers with complex load profiles (electric vehicles, home battery systems, etc.), pose new challenges for the electricity grid. To address these challenges, we propose two first-of-its-kind datasets based on measurements in a broadband powerline communications (PLC) infrastructure. Both datasets FiN-1 and FiN-2, were collected during real practical use in a part of the German low-voltage grid that supplies around 4.4 million people and show more than 13 billion datapoints collected by more than 5100 sensors. In addition, we present different use cases in asset management, grid state visualization, forecasting, 
    
[^130]: GP-net: 灵活的视角抓取提案

    GP-net: Flexible Viewpoint Grasp Proposal. (arXiv:2209.10404v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.10404](http://arxiv.org/abs/2209.10404)

    GP-net 可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取，在真实世界的实验中，它实现了 51.8% 的抓取成功率，相比之下，机器人抓取技术的最新方法成功率更低，需要定义工作空间。

    

    我们提出了一种名为 Grasp Proposal Network (GP-net) 的卷积神经网络模型，可以从灵活的视角，例如移动机械臂所体验的视角，生成六自由度的抓取。我们通过合成生成深度图像和标注抓取信息的数据集来训练 GP-net。在真实世界的实验中，我们使用 EGAD! 抓取基准测试对 GP-net 进行评估，并将其与两种常用算法——Volumetric Grasping Network (VGN) 和 Grasp Pose Detection package (GPD) 进行比较，在 PAL TIAGo 移动机器人上。与机器人抓取技术的最新方法相比，GP-net 可以用于从灵活的未知视角抓取对象，而无需定义工作空间，并且抓取成功率达到 51.8%，相比之下，VGN 为 51.1%，GPD 为 33.6%。我们提供了一个 ROS 包，以及我们的代码和预训练模型，网址为 https://aucoroboticsmu.github.io/GP-net/。

    We present the Grasp Proposal Network (GP-net), a Convolutional Neural Network model which can generate 6-DOF grasps from flexible viewpoints, e.g. as experienced by mobile manipulators. To train GP-net, we synthetically generate a dataset containing depth-images and ground-truth grasp information. In real-world experiments we use the EGAD! grasping benchmark to evaluate GP-net against two commonly used algorithms, the Volumetric Grasping Network (VGN) and the Grasp Pose Detection package (GPD), on a PAL TIAGo mobile manipulator. In contrast to the state-of-the-art methods in robotic grasping, GP-net can be used for grasping objects from flexible, unknown viewpoints without the need to define the workspace and achieves a grasp success of 51.8% compared to 51.1% for VGN and 33.6% for GPD. We provide a ROS package along with our code and pre-trained models at https://aucoroboticsmu.github.io/GP-net/.
    
[^131]: 针对干预密度估计的正则化流

    Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.06203](http://arxiv.org/abs/2209.06203)

    本研究提出了一种名为干预正则化流的全参数深度学习方法，用于从观测数据中估计干预后的潜在结果密度。该方法结合了nuisance flow和target flow，并开发了一个易于处理的优化目标，以实现有效和双重稳健的估计。实验证明，该方法优于现有最先进的方法。

    

    现有机器学习方法针对因果推断通常通过潜在结果的均值（例如平均处理效应）来计算数量。然而，这些数量并不能完全捕捉潜在结果分布的全部信息。本研究旨在从观测数据中估计干预后的潜在结果密度。为此，我们提出了一种新的全参数深度学习方法，称为干预正则化流。具体而言，我们组合了两种正则化流，即（i）用于估计干扰参数的nuisance flow和（ii）用于参数化估计潜在结果密度的target flow。我们进一步基于单步偏差校正开发了一个易于处理的优化目标，以有效和双重稳健的方式估计目标流参数。因此，我们的干预正则化流提供了一个正确归一化的密度估计器。在各种实验中，我们展示了我们的干预正则化流方法优于现有的用于从观测数据中估计潜在结果密度的最先进的方法。

    Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
    
[^132]: 视觉中的扩散模型：一项综述

    Diffusion Models in Vision: A Survey. (arXiv:2209.04747v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2209.04747](http://arxiv.org/abs/2209.04747)

    扩散模型是视觉中的新兴主题，其生成样本的质量和多样性受到广泛欣赏。本综述介绍了三种通用的扩散建模方法以及各种算法和架构方面的讨论，并总结比较了扩散模型与其他最先进的生成模型的性能。

    

    去噪扩散模型是计算机视觉领域中的新兴主题，展现了在生成建模领域中非凡的结果。扩散模型是一种基于深度学习的生成模型，由两个阶段组成，前向扩散和反向扩散。在前向扩散阶段，通过逐步添加高斯噪声逐渐扰动输入数据。在反向阶段，模型被任务为通过逐步学习逆转扩散过程，逐步恢复原始输入数据。尽管扩散模型的计算负担较大，即由于在采样过程中涉及的步骤数量较多导致的速度较慢，但其所生成样本的质量和多样性仍然受到广泛欣赏。在本篇文章中，我们提供了一个关于去噪扩散模型在视觉中应用的综合性评论，包括该领域的理论和实践贡献。首先，我们确定并介绍了三种通用的扩散建模方法：连续、离散和混合扩散模型。接着，我们讨论了扩散模型的各种算法和架构方面，如使用 Lévy 过程、不同形式的噪声、模型条件和正则化、多尺度架构和并行化技术。最后，我们总结并比较了去噪扩散模型与其他最先进的生成模型在多个数据集上的性能。

    Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e. low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modelin
    
[^133]: 扩散模型：方法和应用的综合调研

    Diffusion Models: A Comprehensive Survey of Methods and Applications. (arXiv:2209.00796v10 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.00796](http://arxiv.org/abs/2209.00796)

    本调研综合总结了扩散模型的方法和应用研究，包括高效采样、似然估计与特殊结构数据处理，介绍了扩散模型与其他生成模型相结合的潜力并回顾了其在不同领域的广泛应用，为进一步研究提出了可能的研究方向。

    

    扩散模型已成为一类具有记录性能的强大的深度生成模型，在许多应用中，包括图像合成、视频生成和分子设计方面表现出色。在这份调研中，我们概述了关于扩散模型的快速扩展研究，将研究分类为三个关键领域：高效采样、改进的似然估计和处理具有特殊结构的数据。我们还讨论了将扩散模型与其他生成模型相结合以实现增强结果的潜力。我们进一步回顾了扩散模型在计算机视觉、自然语言生成、时间数据建模以及其他科学学科的跨学科应用中的广泛应用。这个调研旨在提供一个具有背景的深入了解扩散模型的现状，确定关键的研究重点，指明可能的进一步研究领域。

    Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/Yan
    
[^134]: Frank-Wolfe方法的凸混合整数优化

    Convex mixed-integer optimization with Frank-Wolfe methods. (arXiv:2208.11010v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.11010](http://arxiv.org/abs/2208.11010)

    该论文提出了一种新的凸混合整数优化方法，它使用Frank-Wolfe算法在混合整数可行点的凸包上求解，以解决混合整数非线性优化问题。

    

    混合整数非线性优化涵盖了一类既有理论又有计算挑战的广泛问题。我们提出了一种基于分支定界算法的新型解决方案，其使用凸节点松弛的Frank-Wolfe算法来解决这些问题。这些松弛是在混合整数可行点的凸包上求解的，而不是通过调用混合整数线性求解器的连续松弛作为线性预言机来解决的。该方法在处理单个多面体约束的表示时计算可行解，利用混合整数线性求解器的全部能力，而不需要外部逼近方案，并且可以利用节点子问题的不精确解。

    Mixed-integer nonlinear optimization encompasses a broad class of problems that present both theoretical and computational challenges. We propose a new type of method to solve these problems based on a branch-and-bound algorithm with convex node relaxations. These relaxations are solved with a Frank-Wolfe algorithm over the convex hull of mixed-integer feasible points instead of the continuous relaxation via calls to a mixed-integer linear solver as the linear oracle. The proposed method computes feasible solutions while working on a single representation of the polyhedral constraints, leveraging the full extent of mixed-integer linear solvers without an outer approximation scheme and can exploit inexact solutions of node subproblems.
    
[^135]: 基于POCS的聚类算法

    POCS-based Clustering Algorithm. (arXiv:2208.08888v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.08888](http://arxiv.org/abs/2208.08888)

    本文提出了一种基于POCS方法的聚类算法，该算法利用并行投影方法在特征空间中找到适当的聚类原型，实验结果表明在聚类误差和执行速度方面与传统聚类方法相比优势明显。

    

    本文提出了一种基于凸集投影(POCS)方法的新型聚类技术，称为基于POCS的聚类算法。该算法利用POCS的并行投影方法在特征空间中找到适当的聚类原型。该算法将每个数据点视为一个凸集，并将聚类原型平行投影到成员数据点上，最小化数据聚类目标函数。在各种合成数据集上进行实验验证，结果表明，与其他传统聚类方法，包括模糊C-均值(FCM)和K-means聚类算法相比，提出的基于POCS的聚类算法在聚类误差和执行速度方面都具有竞争力和效率。

    A novel clustering technique based on the projection onto convex set (POCS) method, called POCS-based clustering algorithm, is proposed in this paper. The proposed POCS-based clustering algorithm exploits a parallel projection method of POCS to find appropriate cluster prototypes in the feature space. The algorithm considers each data point as a convex set and projects the cluster prototypes parallelly to the member data points. The projections are convexly combined to minimize the objective function for data clustering purpose. The performance of the proposed POCS-based clustering algorithm is verified through experiments on various synthetic datasets. The experimental results show that the proposed POCS-based clustering algorithm is competitive and efficient in terms of clustering error and execution speed when compared with other conventional clustering methods including Fuzzy C-Means (FCM) and K-means clustering algorithms.
    
[^136]: Langevin扩散变分推断

    Langevin Diffusion Variational Inference. (arXiv:2208.07743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.07743](http://arxiv.org/abs/2208.07743)

    本论文提出了一种统一的分析方法以概括并改进现有的基于未调整Langevin转移的强大变分分布构建方法，同时提出了一种新方法，在基准测试中表现更好。

    

    许多基于未调整Langevin转移的强大变分分布构建方法已经存在。但是，它们大多采用了各种不同的方法和技术，缺乏统一的分析和推导，从而使开发新方法和推理现有方法成为一项具有挑战性的任务。为了解决这个问题，我们提出了一种单一的分析方法，以统一和概括这些现有技术。主要思想是通过数值模拟欠阻尼Langevin扩散过程及其时间翻转对目标和变分进行增广。这种方法的好处有两个方面：它为许多现有方法提供了统一的公式，并简化了新方法的开发。事实上，利用我们的公式，我们提出了一种新方法，它结合了以前现有算法的优点；它使用欠阻尼Langevin转移和由得分网络参数化的强大的增广。我们的实证评估显示，我们提出的方法在一系列基准问题上优于现有方法。

    Many methods that build powerful variational distributions based on unadjusted Langevin transitions exist. Most of these were developed using a wide range of different approaches and techniques. Unfortunately, the lack of a unified analysis and derivation makes developing new methods and reasoning about existing ones a challenging task. We address this giving a single analysis that unifies and generalizes these existing techniques. The main idea is to augment the target and variational by numerically simulating the underdamped Langevin diffusion process and its time reversal. The benefits of this approach are twofold: it provides a unified formulation for many existing methods, and it simplifies the development of new ones. In fact, using our formulation we propose a new method that combines the strengths of previously existing algorithms; it uses underdamped Langevin transitions and powerful augmentations parameterized by a score network. Our empirical evaluation shows that our propos
    
[^137]: 基于大规模注释数据库的深度伪造检测 AI 偏差的全面分析

    A Comprehensive Analysis of AI Biases in DeepFake Detection With Massively Annotated Databases. (arXiv:2208.05845v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.05845](http://arxiv.org/abs/2208.05845)

    本研究通过提供包含47种不同属性注释的大规模数据集，并对三种最先进的Deepfake检测模型进行全面分析，旨在研究公共Deepfake数据集可能带来的AI偏差问题

    

    近年来，Deepfake 对图像和视频的篡改已经成为安全和社会的严重关注点。许多检测模型和数据集已经被提出，可靠地检测 Deepfake 数据。然而，人们越来越担心这些模型和训练数据集可能存在偏差，从而导致 Deepfake 检测器失效。本研究通过提供五个流行的 Deepfake 数据集中 47 种不同属性的大规模人口统计和非人口统计属性注释，并全面分析三种最先进的 Deepfake 检测模型对这些数据集的 AI 偏差问题，调查研究了超过 6500 万个标签的许多不同属性（包括人口统计学（年龄、性别、种族）和非人口统计学（头发、皮肤、配饰等）信息对检测性能的影响。结果表明，调查的数据库缺乏多样性，可能导致 AI 偏差。

    In recent years, image and video manipulations with Deepfake have become a severe concern for security and society. Many detection models and datasets have been proposed to detect Deepfake data reliably. However, there is an increased concern that these models and training databases might be biased and, thus, cause Deepfake detectors to fail. In this work, we investigate the bias issue caused by public Deepfake datasets by (a) providing large-scale demographic and non-demographic attribute annotations of 47 different attributes for five popular Deepfake datasets and (b) comprehensively analysing AI-bias of three state-of-the-art Deepfake detection backbone models on these datasets. The investigation analyses the influence of a large variety of distinctive attributes (from over 65M labels) on the detection performance, including demographic (age, gender, ethnicity) and non-demographic (hair, skin, accessories, etc.) information. The results indicate that investigated databases lack dive
    
[^138]: 《采用转化与蒸馏框架实现合作MARL全局最优性》

    Towards Global Optimality in Cooperative MARL with the Transformation And Distillation Framework. (arXiv:2207.11143v3 [cs.MA] UPDATED)

    [http://arxiv.org/abs/2207.11143](http://arxiv.org/abs/2207.11143)

    本文研究了采用分散策略的MARL算法在梯度下降优化器下的次最优性，并提出了转化与蒸馏框架，该框架可以将多智能体MDP转化为单智能体MDP以实现分散执行。

    

    在合作多智能体强化学习中，分散执行是一项核心需求。目前，大多数流行的MARL算法采用分散策略来实现分散执行，并使用梯度下降作为优化器。然而，在考虑到优化方法的情况下，这些算法几乎没有任何理论分析，我们发现当梯度下降被选为优化方法时，各种流行的分散策略MARL算法在玩具任务中都是次最优的。本文在理论上分析了两种常见的采用分散策略的算法——多智能体策略梯度方法和值分解方法，证明了它们在使用梯度下降时的次最优性。此外，我们提出了转化与蒸馏（TAD）框架，它将多智能体MDP重新制定为一种具有连续结构的特殊单智能体MDP，并通过蒸馏实现分散执行。

    Decentralized execution is one core demand in cooperative multi-agent reinforcement learning (MARL). Recently, most popular MARL algorithms have adopted decentralized policies to enable decentralized execution and use gradient descent as their optimizer. However, there is hardly any theoretical analysis of these algorithms taking the optimization method into consideration, and we find that various popular MARL algorithms with decentralized policies are suboptimal in toy tasks when gradient descent is chosen as their optimization method. In this paper, we theoretically analyze two common classes of algorithms with decentralized policies -- multi-agent policy gradient methods and value-decomposition methods to prove their suboptimality when gradient descent is used. In addition, we propose the Transformation And Distillation (TAD) framework, which reformulates a multi-agent MDP as a special single-agent MDP with a sequential structure and enables decentralized execution by distilling the
    
[^139]: 关于预训练在联邦学习中的重要性和适用性的研究

    On the Importance and Applicability of Pre-Training for Federated Learning. (arXiv:2206.11488v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.11488](http://arxiv.org/abs/2206.11488)

    研究发现，在联邦学习中使用预训练可以改善性能，尤其是在非独立同分布客户数据的情况下。此外，使用合成数据或客户端数据进行分散式预训练也可以显著改善性能，并且不同的技术可以相互补充以进一步提高性能。

    

    预训练在现代深度学习中被广泛用于提高模型的性能。然而，在联邦学习（FL）的文献中，神经网络大多数是使用随机权重初始化的。这引起了我们对进行系统研究探索FL预训练的兴趣。在多个视觉识别基准测试中，我们发现预训练不仅可以提高FL的性能，而且可以缩小它与中心化学习之间的准确度差距，特别是在非独立同分布客户数据的挑战性情况下。为了使我们的发现适用于没有直接获得预训练模型的情况，我们探索了使用合成数据或甚至使用客户端数据进行分散式预训练，并发现它们已经显著改善了FL的性能。有趣的是，我们探索的许多技术互补性很强，可以进一步提高性能，我们将这视为在实际应用中扩展深度FL的关键结果。

    Pre-training is prevalent in nowadays deep learning to improve the learned model's performance. However, in the literature on federated learning (FL), neural networks are mostly initialized with random weights. These attract our interest in conducting a systematic study to explore pre-training for FL. Across multiple visual recognition benchmarks, we found that pre-training can not only improve FL, but also close its accuracy gap to the counterpart centralized learning, especially in the challenging cases of non-IID clients' data. To make our findings applicable to situations where pre-trained models are not directly available, we explore pre-training with synthetic data or even with clients' data in a decentralized manner, and found that they can already improve FL notably. Interestingly, many of the techniques we explore are complementary to each other to further boost the performance, and we view this as a critical result toward scaling up deep FL for real-world applications. We con
    
[^140]: gDDIM: 广义降噪扩散隐式模型

    gDDIM: Generalized denoising diffusion implicit models. (arXiv:2206.05564v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.05564](http://arxiv.org/abs/2206.05564)

    本研究将降噪扩散隐式模型（DDIM）扩展到一般扩散模型，创新性地提出了广义DDIM（gDDIM），并在模糊扩散模型（BDM）和临界阻尼朗之万扩散模型（CLD）中获得了超过20倍和15倍的加速效果。

    

    我们的目标是将降噪扩散隐式模型(DDIM)拓展到除同向扩散外的一般扩散模型(DMs)。我们从数值方面研究DDIM的机制，发现可以通过在解相应随机微分方程时使用特定的分数近似来获得DDIM。我们提出了一种解释DDIM加速效果的方法，并解释了确定性采样方案比随机采样方案用于快速采样的优点。在此基础上，我们通过对分数网络进行一些微小而精巧的修改，将DDIM扩展到一般DMs，称为广义DDIM(gDDIM)。我们在两个非同向DMs（模糊扩散模型(BDM)和临界阻尼朗之万扩散模型(CLD)）中验证了gDDIM。我们观察到BDM中的加速效果超过20倍。在CLD中，通过引入一个控制峰度的扩散模型，我们在维持高质量的情况下使收敛速度提高了15倍。

    Our goal is to extend the denoising diffusion implicit model (DDIM) to general diffusion models~(DMs) besides isotropic diffusions. Instead of constructing a non-Markov noising process as in the original DDIM, we examine the mechanism of DDIM from a numerical perspective. We discover that the DDIM can be obtained by using some specific approximations of the score when solving the corresponding stochastic differential equation. We present an interpretation of the accelerating effects of DDIM that also explains the advantages of a deterministic sampling scheme over the stochastic one for fast sampling. Building on this insight, we extend DDIM to general DMs, coined generalized DDIM (gDDIM), with a small but delicate modification in parameterizing the score network. We validate gDDIM in two non-isotropic DMs: Blurring diffusion model (BDM) and Critically-damped Langevin diffusion model (CLD). We observe more than 20 times acceleration in BDM. In the CLD, a diffusion model by augmenting th
    
[^141]: 利用中心极限定理结构的随机梯度采样方法：改进的分析和更快的算法

    Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms. (arXiv:2206.03792v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/2206.03792](http://arxiv.org/abs/2206.03792)

    本文研究了基于随机近似的采样算法，利用中心极限定理结构吸收扩散过程中的随机逼近误差并获得了改进的收敛保证。此外，对于SGLD和RBM，我们分别证明了不同的假设条件下较优的收敛率和参数范围。

    

    本文研究了基于随机近似的采样算法，如随机梯度 langevin 动力学（SGLD）和随机批处理方法（RBM）用于相互作用粒子动力学（IPD）。我们观察到，由于中心极限定理（CLT），随机逼近引入的噪声几乎是高斯分布，而驱动布朗运动则是确切的高斯分布。我们利用这种结构来吸收扩散过程中的随机逼近误差，并获得了这些算法的改进收敛保证。对于 SGLD，我们证明了在不需要统一温暖启动的情况下KL散度的第一个稳定收敛率，假设目标密度满足一个对数 Sobolev 不等式。我们的结果意味着在显著较轻的假设条件下，相对于先前的工作，我们具有更优异的一阶 oracle 复杂性。我们还证明了 SGLD 的第一个保证，对于更弱的条件，如 H\''{o}lder 平滑性和 Poincare不等式，从而填补了现有技术和实际应用之间的差距。对于 RBM，我们在 IPD 的弱混合条件下获得了第一次收敛分析和最佳参数范围，这在统计物理和学习理论中具有几个含义。

    We consider stochastic approximations of sampling algorithms, such as Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM) for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by the stochastic approximation is nearly Gaussian due to the Central Limit Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness this structure to absorb the stochastic approximation error inside the diffusion process, and obtain improved convergence guarantees for these algorithms. For SGLD, we prove the first stable convergence rate in KL divergence without requiring uniform warm start, assuming the target density satisfies a Log-Sobolev Inequality. Our result implies superior first-order oracle complexity compared to prior works, under significantly milder assumptions. We also prove the first guarantees for SGLD under even weaker conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging the gap between the state-
    
[^142]: 物理嵌入神经网络：带混合边界条件的图神经PDE求解器

    Physics-Embedded Neural Networks: Graph Neural PDE Solvers with Mixed Boundary Conditions. (arXiv:2205.11912v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.11912](http://arxiv.org/abs/2205.11912)

    本文介绍了一种名为物理嵌入神经网络的方法，该方法能够充分考虑边界条件，使用隐式方法预测长时间后的状态，并在各种形状上具有高度的泛化性能，为使用混合边界条件描述的PDE边界值问题提供了有前途的解决方案。

    

    图神经网络（GNN）是一种有前途的方法，用于学习和预测边界值问题中描述的物理现象，例如带有边界条件的偏微分方程（PDE）。然而，现有的模型不足以充分考虑边界条件，从而无法可靠地预测此类问题。此外，由于GNN的局部连接性质，准确预测长时间后的状态，其中顶点之间的相互作用往往是全局的，是困难的。我们提出了一种称为物理嵌入神经网络的方法，考虑边界条件，并使用隐式方法预测长时间后的状态。它基于E（n）-等变GNN构建，因此在各种形状上具有高度的泛化性能。我们证明，我们的模型学习了复杂形状中的流动现象，并在速度-精度权衡方面优于经过良好优化的经典求解器和最先进的机器学习模型。因此，我们的模型为使用混合边界条件描述的PDE边界值问题提供了有前途的解决方案。

    Graph neural network (GNN) is a promising approach to learning and predicting physical phenomena described in boundary value problems, such as partial differential equations (PDEs) with boundary conditions. However, existing models inadequately treat boundary conditions essential for the reliable prediction of such problems. In addition, because of the locally connected nature of GNNs, it is difficult to accurately predict the state after a long time, where interaction between vertices tends to be global. We present our approach termed physics-embedded neural networks that considers boundary conditions and predicts the state after a long time using an implicit method. It is built based on an E(n)-equivariant GNN, resulting in high generalization performance on various shapes. We demonstrate that our model learns flow phenomena in complex shapes and outperforms a well-optimized classical solver and a state-of-the-art machine learning model in speed-accuracy trade-off. Therefore, our mod
    
[^143]: Faith-Shap：忠实的Shapley交互指数

    Faith-Shap: The Faithful Shapley Interaction Index. (arXiv:2203.00870v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.00870](http://arxiv.org/abs/2203.00870)

    本文介绍了一种称为Faith-Shap的方法，用于为黑盒机器学习模型中的交互提供归因，它不需要放弃效率这一关键属性，并通过将Shapley值作为最忠实线性逼近系数的方法解决了唯一性问题。

    

    Shapley值最初是为了在联盟博弈中为个体玩家分配归因而设计的，现在已成为解释性机器学习中常用的方法，可为黑盒机器学习模型的输入特征提供归因。 Shapley值的关键吸引力在于它们独特地满足一组非常自然的公理属性。然而，将Shapley值扩展到分配相互作用而非个体玩家的归因，即交互指数，是非平凡的：由于原始Shapley值扩展到交互上的自然公理集不再指定唯一的交互指数，因此许多提案引入了更少“自然”的公理，同时放弃效率这一关键公理，以获得唯一的交互指数。在这项工作中，我们不引入其他相互冲突的公理，而是采取Shapley值作为最忠实线性逼近的系数的观点。

    Shapley values, which were originally designed to assign attributions to individual players in coalition games, have become a commonly used approach in explainable machine learning to provide attributions to input features for black-box machine learning models. A key attraction of Shapley values is that they uniquely satisfy a very natural set of axiomatic properties. However, extending the Shapley value to assigning attributions to interactions rather than individual players, an interaction index, is non-trivial: as the natural set of axioms for the original Shapley values, extended to the context of interactions, no longer specify a unique interaction index. Many proposals thus introduce additional less ''natural'' axioms, while sacrificing the key axiom of efficiency, in order to obtain unique interaction indices. In this work, rather than introduce additional conflicting axioms, we adopt the viewpoint of Shapley values as coefficients of the most faithful linear approximation to th
    
[^144]: 分布式随机重洗算法在网络中的应用

    Distributed Random Reshuffling over Networks. (arXiv:2112.15287v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2112.15287](http://arxiv.org/abs/2112.15287)

    本文提出了一种分布式随机重洗（D-RR）算法，能够解决协作优化问题。在平滑强凸和平滑非凸目标函数的情况下，D-RR算法都能够实现很好的优化结果，并且优于分布式随机梯度下降算法。

    

    本文研究了一类分布式优化问题，即在连通网络上，N个代理以协作的方式最小化本地代价函数的平均值。为了解决这个问题，我们提出了一种分布式随机重洗（D-RR）算法用来在每个代理中调用随机重洗（RR）更新。我们证明了D-RR继承了RR对于平滑强凸和平滑非凸目标函数的优越特性。特别是，对于平滑强凸目标函数，D-RR在迭代到全局最小值的平方距离方面达到了$\mathcal O(1/T^2)$收敛率（其中$T$表示迭代次数）。当目标函数被假定为平滑非凸时，我们证明了D-RR以$\mathcal O(1/T^{2/3})$的速率将梯度的平方范数驱动到0。这些收敛结果与集中式的RR（上到常数因子）相匹配并且优于分布式随机梯度下降算法。

    In this paper, we consider distributed optimization problems where $n$ agents, each possessing a local cost function, collaboratively minimize the average of the local cost functions over a connected network. To solve the problem, we propose a distributed random reshuffling (D-RR) algorithm that invokes the random reshuffling (RR) update in each agent. We show that D-RR inherits favorable characteristics of RR for both smooth strongly convex and smooth nonconvex objective functions. In particular, for smooth strongly convex objective functions, D-RR achieves $\mathcal{O}(1/T^2)$ rate of convergence (where $T$ counts epoch number) in terms of the squared distance between the iterate and the global minimizer. When the objective function is assumed to be smooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$ at a rate of $\mathcal{O}(1/T^{2/3})$. These convergence results match those of centralized RR (up to constant factors) and outperform the distributed stochast
    
[^145]: 在多智能体城市驾驶环境中评估深度强化学习自主策略的鲁棒性

    Evaluating the Robustness of Deep Reinforcement Learning for Autonomous Policies in a Multi-agent Urban Driving Environment. (arXiv:2112.11947v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2112.11947](http://arxiv.org/abs/2112.11947)

    本论文提出了一个基准测试框架，用于评估和比较深度强化学习算法在单个和多个智能体自主驾驶环境中的性能。同时，提出了一种混合算法以提高深度强化学习策略在多代理驾驶环境中的鲁棒性。

    

    深度强化学习被广泛用于在模拟驾驶环境中训练自主车策略。由于各种强化学习算法的大量可用性，并缺乏对它们在不同驾驶场景下的系统比较，我们不确定哪些算法更有效地用于单车和多车驾驶环境中自主汽车软件的训练。为评估深度强化学习在基于视觉的自主驾驶中的性能，我们提供了一个开放且可重复使用的基准测试框架，用于系统评估和比较分析单个和多个强化学习算法用于自动驾驶。利用该框架，我们进行了离散和连续动作空间深度强化学习算法的比较研究。同时，我们还提出了一种混合算法，将离散和连续动作空间相结合，以提高深度强化学习策略在多代理驾驶环境中的鲁棒性。

    Deep reinforcement learning is actively used for training autonomous car policies in a simulated driving environment. Due to the large availability of various reinforcement learning algorithms and the lack of their systematic comparison across different driving scenarios, we are unsure of which ones are more effective for training autonomous car software in single-agent as well as multi-agent driving environments. A benchmarking framework for the comparison of deep reinforcement learning in a vision-based autonomous driving will open up the possibilities for training better autonomous car driving policies. To address these challenges, we provide an open and reusable benchmarking framework for systematic evaluation and comparative analysis of deep reinforcement learning algorithms for autonomous driving in a single- and multi-agent environment. Using the framework, we perform a comparative study of discrete and continuous action space deep reinforcement learning algorithms. We also prop
    
[^146]: 集中关注潜在命名实体的主动标注获取

    Focusing on Potential Named Entities During Active Label Acquisition. (arXiv:2111.03837v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2111.03837](http://arxiv.org/abs/2111.03837)

    本文提出了几个AL句子查询评估函数，关注潜在正面标记，并使用更好的数据驱动的正常化方法，以最小化NER注释成本。

    

    命名实体识别(NER)旨在识别结构化文本中命名实体的提及并将其分类到预定义的命名实体类别中。虽然基于深度学习的预训练语言模型有助于在NER中实现良好的预测性能，但许多特定领域的NER应用仍需要大量标记数据。主动学习(AL)是解决标签获取问题的通用框架，已用于NER任务，以最小化注释成本而不牺牲模型性能。然而，标记的严重不均匀类分布引入了设计有效的NER主动学习查询方法的挑战。我们提出了几个AL句子查询评估函数，更多关注潜在的正面标记，并使用基于句子和标记成本评估策略来评估这些提议的函数。我们还提出了更好的数据驱动的正常化方法，以惩罚过长或过短的句子。

    Named entity recognition (NER) aims to identify mentions of named entities in an unstructured text and classify them into predefined named entity classes. While deep learning-based pre-trained language models help to achieve good predictive performances in NER, many domain-specific NER applications still call for a substantial amount of labeled data. Active learning (AL), a general framework for the label acquisition problem, has been used for NER tasks to minimize the annotation cost without sacrificing model performance. However, the heavily imbalanced class distribution of tokens introduces challenges in designing effective AL querying methods for NER. We propose several AL sentence query evaluation functions that pay more attention to potential positive tokens, and evaluate these proposed functions with both sentence-based and token-based cost evaluation strategies. We also propose a better data-driven normalization approach to penalize sentences that are too long or too short. Our
    
[^147]: 近端强化学习：部分观测马尔可夫决策过程中高效的离线策略评估

    Proximal Reinforcement Learning: Efficient Off-Policy Evaluation in Partially Observed Markov Decision Processes. (arXiv:2110.15332v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.15332](http://arxiv.org/abs/2110.15332)

    本文提出了一种方法，可以在离线强化学习中，应用于从医疗保健或教育领域收集的观测数据。我们考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估，以解决观察到的行动可能受到未观察到的因素影响，导致估计值出现偏差的问题。

    

    在从医疗保健或教育领域收集的观察数据应用离线强化学习时，一个普遍的关注点是，观察到的行动可能受到未观察到的因素的影响，引起混淆并导致在假设完美马尔可夫决策过程模型的情况下得出的估计值出现偏差。本文考虑在部分观测马尔可夫决策过程（POMDP）中进行离线策略评估。具体来说，我们考虑在给定只由不同且未知的策略生成的具有部分状态观测的轨迹的情况下估计POMDP中给定目标策略的值，该策略可能依赖于未观察到的状态。我们解决了两个问题：什么条件允许我们从观察到的数据中识别目标策略值，并且在识别的情况下，如何最好地估计它。为了回答这些问题，我们将近端因果推断框架扩展到我们的POMDP设置中，提供了许多场景，其中通过所谓的桥接函数的存在实现了识别。

    In applications of offline reinforcement learning to observational data, such as in healthcare or education, a general concern is that observed actions might be affected by unobserved factors, inducing confounding and biasing estimates derived under the assumption of a perfect Markov decision process (MDP) model. Here we tackle this by considering off-policy evaluation in a partially observed MDP (POMDP). Specifically, we consider estimating the value of a given target policy in a POMDP given trajectories with only partial state observations generated by a different and unknown policy that may depend on the unobserved state. We tackle two questions: what conditions allow us to identify the target policy value from the observed data and, given identification, how to best estimate it. To answer these, we extend the framework of proximal causal inference to our POMDP setting, providing a variety of settings where identification is made possible by the existence of so-called bridge functio
    
[^148]: 一种利用记录相似性进行实际垂直联邦学习的耦合设计

    A Coupled Design of Exploiting Record Similarity for Practical Vertical Federated Learning. (arXiv:2106.06312v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.06312](http://arxiv.org/abs/2106.06312)

    本文提出了一种新的耦合训练范式FedSim，将一对多链接集成到训练过程中，以解决现有VFL方法忽略“记录链接”过程的问题，并实现了更好的性能。

    

    联邦学习是一种学习范式，可以在不公开原始数据的情况下实现跨不同方参与方的协作学习。垂直联邦学习（VFL）是其中一种应用广泛的具体实现，其中参与方共享相同的样本，但只持有部分特征。然而，大多数现有的VFL研究都忽略了"记录链接"过程。它们设计的算法要么假设来自不同方的数据可以被准确地链接，要么仅将每个记录链接到其最相似的相邻记录。这些方法可能无法捕捉其他不太相似记录的关键特征。此外，这种不合理的链接不能通过训练进行纠正，因为现有的方法在训练过程中没有提供关于链接的反馈。在本文中，我们设计了一种新的耦合训练范式FedSim，将一对多链接集成到训练过程中。除了为具有模糊标识符的许多实际应用程序提供VFL外，FedSim还实现更好的每个

    Federated learning is a learning paradigm to enable collaborative learning across different parties without revealing raw data. Notably, vertical federated learning (VFL), where parties share the same set of samples but only hold partial features, has a wide range of real-world applications. However, most existing studies in VFL disregard the "record linkage" process. They design algorithms either assuming the data from different parties can be exactly linked or simply linking each record with its most similar neighboring record. These approaches may fail to capture the key features from other less similar records. Moreover, such improper linkage cannot be corrected by training since existing approaches provide no feedback on linkage during training. In this paper, we design a novel coupled training paradigm, FedSim, that integrates one-to-many linkage into the training process. Besides enabling VFL in many real-world applications with fuzzy identifiers, FedSim also achieves better per
    
[^149]: 深度网络中的低秩简单性偏好

    The Low-Rank Simplicity Bias in Deep Networks. (arXiv:2103.10427v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.10427](http://arxiv.org/abs/2103.10427)

    本文研究了现代深度神经网络的泛化能力及可能的原因，发现深度网络具有归纳偏见，更倾向于寻找低有效秩嵌入的解决方案，并通过实证证明了该偏好在有限宽度线性和非线性模型上的实用性和鲁棒性。

    

    现代深度神经网络通常表现出惊人的泛化能力，本文探究和拓展了更深层网络具有归纳偏见，以寻找低有效秩嵌入解决方案的假设，通过实证证明了该偏好在有限宽度线性和非线性模型上的实用性并且具有鲁棒性。进一步展示了通过对神经网络的线性过参数化来实现深度非线性模型。

    Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear 
    
[^150]: 不扩展的A*搜索：用深度Q网络学习启发式函数

    A* Search Without Expansions: Learning Heuristic Functions with Deep Q-Networks. (arXiv:2102.04518v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2102.04518](http://arxiv.org/abs/2102.04518)

    本文提出了一种使用深度Q网络学习启发式函数，通过只进行一次前向传递计算相邻节点的转移成本和启发式值之和，并在不显式生成这些子节点的情况下指导搜索的Q*搜索算法，以大幅减少计算时间。在魔方问题上的实验表明，该方法能够高效地解决具有大动作空间的问题。

    

    高效地使用 A* 搜索解决具有大动作空间的问题对于人工智能社区几十年来一直非常重要。这是因为 A* 搜索的计算和存储需求随着动作空间的大小呈线性增长。当 A* 搜索使用计算代价高昂的函数逼近器（如深度神经网络）学习启发式函数时，这种负担变得更加明显。为了解决这个问题，我们引入了 Q* 搜索，一种使用深度 Q 网络引导搜索的搜索算法，以利用一个事实，即在不显式生成这些子节点的情况下，一个节点的子节点的转移成本和启发式值之和可以通过单次前向传递计算。这显着降低了计算时间，并且每次迭代只需要生成一个节点。我们使用 Q* 搜索来解决魔方问题，并将其们表示为一个包含 1872 个元动作的大动作空间。

    Efficiently solving problems with large action spaces using A* search has been of importance to the artificial intelligence community for decades. This is because the computation and memory requirements of A* search grow linearly with the size of the action space. This burden becomes even more apparent when A* search uses a heuristic function learned by computationally expensive function approximators, such as deep neural networks. To address this problem, we introduce Q* search, a search algorithm that uses deep Q-networks to guide search in order to take advantage of the fact that the sum of the transition costs and heuristic values of the children of a node can be computed with a single forward pass through a deep Q-network without explicitly generating those children. This significantly reduces computation time and requires only one node to be generated per iteration. We use Q* search to solve the Rubik's cube when formulated with a large action space that includes 1872 meta-action
    
[^151]: 无法观测到的混淆变量的核方法：负对照、代理变量和工具变量

    Kernel Methods for Unobserved Confounding: Negative Controls, Proxies, and Instruments. (arXiv:2012.10315v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.10315](http://arxiv.org/abs/2012.10315)

    该论文提出了一种使用负对照、代理变量和工具变量的核方法，以识别治疗效果并学习非参数治疗效果。 作者证明了算法的一致性和收敛速度，并估计了香烟吸烟的剂量反应曲线。

    

    负对照是一种在存在未测量混淆的情况下学习治疗与结果之间因果关系的策略。如果有两个辅助变量可用：一个负对照治疗（对实际结果没有影响）和一个负对照结果（不受实际治疗影响），则仍然可以识别治疗效果。 这些辅助变量也可以视为传统控制变量集的代理变量，并且它们类似于工具变量。我提出了一族基于核岭回归的算法，在负对照下学习非参数治疗效果。示例包括剂量反应曲线、具有分布偏移的剂量反应曲线和异质性治疗效果。 数据可以是离散、连续和低维、高维或无限维。我证明了均匀一致性并提供了有限样本收敛率。 我估计了香烟吸烟剂量反应曲线。

    Negative control is a strategy for learning the causal relationship between treatment and outcome in the presence of unmeasured confounding. The treatment effect can nonetheless be identified if two auxiliary variables are available: a negative control treatment (which has no effect on the actual outcome), and a negative control outcome (which is not affected by the actual treatment). These auxiliary variables can also be viewed as proxies for a traditional set of control variables, and they bear resemblance to instrumental variables. I propose a family of algorithms based on kernel ridge regression for learning nonparametric treatment effects with negative controls. Examples include dose response curves, dose response curves with distribution shift, and heterogeneous treatment effects. Data may be discrete or continuous, and low, high, or infinite dimensional. I prove uniform consistency and provide finite sample rates of convergence. I estimate the dose response curve of cigarette sm
    
[^152]: 条件矩问题的变分矩方法

    The Variational Method of Moments. (arXiv:2012.09422v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.09422](http://arxiv.org/abs/2012.09422)

    本文提出了一个非常通用的条件矩问题估计器类 - 变分矩方法，使得我们能够控制无限数量的矩，并提供了基于核方法和神经网络的多个VMM估计器的理论分析和证明。

    

    条件矩问题是描述结构性因果参数的有力形式化工具。一个常见的方法是将问题转化为有限组的边际矩条件，并应用最优加权广义矩法（OWGMM）。本文提出了一个称之为变分矩方法（VMM）的非常通用的条件矩问题估计器类，并自然地使我们能够控制无限数量的矩。作者对多个VMM估计器进行了详细的理论分析，包括基于核方法和神经网络的估计器，并提供了这些方法在一定条件下一致估计真实因果参数的证明。

    The conditional moment problem is a powerful formulation for describing structural causal parameters in terms of observables, a prominent example being instrumental variable regression. A standard approach reduces the problem to a finite set of marginal moment conditions and applies the optimally weighted generalized method of moments (OWGMM), but this requires we know a finite set of identifying moments, can still be inefficient even if identifying, or can be theoretically efficient but practically unwieldy if we use a growing sieve of moment conditions. Motivated by a variational minimax reformulation of OWGMM, we define a very general class of estimators for the conditional moment problem, which we term the variational method of moments (VMM) and which naturally enables controlling infinitely-many moments. We provide a detailed theoretical analysis of multiple VMM estimators, including ones based on kernel methods and neural nets, and provide conditions under which these are consist
    
[^153]: 利用耦合矩阵分解在部分观测社交网络中联合推断扩散和结构

    Joint Inference of Diffusion and Structure in Partially Observed Social Networks Using Coupled Matrix Factorization. (arXiv:2010.01400v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2010.01400](http://arxiv.org/abs/2010.01400)

    本文提出了一种概率生成模型DiffStru，通过学习耦合低维潜在因素，在部分观测社交网络中联合推断未观测到的扩散和结构网络。

    

    在大规模网络中，完整数据往往难以获取，因此缺失数据的问题是分析和建模真实世界社交网络的一个关键和不可避免的问题。本文从部分观测数据中学习模型，推断未观测到的扩散和结构网络。我们提出了一种概率生成模型DiffStru，通过学习耦合低维潜在因素，利用节点链接和级联过程之间的相互关系。除了推断未见数据外，潜在因素（如社区检测）还可以帮助网络分类问题。

    Access to complete data in large-scale networks is often infeasible. Therefore, the problem of missing data is a crucial and unavoidable issue in the analysis and modeling of real-world social networks. However, most of the research on different aspects of social networks does not consider this limitation. One effective way to solve this problem is to recover the missing data as a pre-processing step. In this paper, a model is learned from partially observed data to infer unobserved diffusion and structure networks. To jointly discover omitted diffusion activities and hidden network structures, we develop a probabilistic generative model called "DiffStru." The interrelations among links of nodes and cascade processes are utilized in the proposed method via learning coupled with low-dimensional latent factors. Besides inferring unseen data, latent factors such as community detection may also aid in network classification problems. We tested different missing data scenarios on simulated 
    
[^154]: 代表性集成在准线性复杂度下实现协同生命周期学习

    Representation Ensembling for Synergistic Lifelong Learning with Quasilinear Complexity. (arXiv:2004.12908v16 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2004.12908](http://arxiv.org/abs/2004.12908)

    本文提出了一种名为RELL的方法，利用知识蒸馏和知识保持正则化方法，以协同集成在不同任务上独立学习的表示，在准线性复杂度下实现了前向和后向传递。实验结果表明，在各种基准数据集上，RELL的表现优于现有的最先进方法，尤其是在存在灾难性遗忘的情况下，能够显着改善反向传递。

    

    在终身学习中，数据不仅可以用于改进当前任务的性能，还可以用于之前和尚未遇到的任务。传统的机器学习则从空白状态开始，仅针对单个任务使用数据。虽然传统迁移学习算法可以提高未来任务的性能，但在学习新任务后对旧任务的性能下降（称为遗忘）。近期针对连续或终身学习的许多方法都试图在给定新任务的情况下保持对旧任务的性能。但是，仅努力避免忘记将目标定得过低。终身学习的目标不仅应该是提高未来任务（前向传递）的性能，而且还应该是用任何新数据提高过去任务（反向传递）的性能。我们的关键见解是，我们可以协同集成分别在不同任务上独立学习的表示，以实现准线性复杂度下的前向和后向传递。本文提出了一种新方法，称为“终身学习中的表示集成（RELL）”，它集成了知识蒸馏和知识保持正则化方法，以利用不同表示中包含的互补信息。我们的实验表明，RELL在各种基准数据集上都优于现有最先进方法，尤其是在存在灾难性遗忘的情况下实现了显着更好的反向传递。

    In lifelong learning, data are used to improve performance not only on the current task, but also on previously encountered, and as yet unencountered tasks. In contrast, classical machine learning, which we define as, starts from a blank slate, or tabula rasa and uses data only for the single task at hand. While typical transfer learning algorithms can improve performance on future tasks, their performance on prior tasks degrades upon learning new tasks (called forgetting). Many recent approaches for continual or lifelong learning have attempted to maintain performance on old tasks given new tasks. But striving to avoid forgetting sets the goal unnecessarily low. The goal of lifelong learning should be not only to improve performance on future tasks (forward transfer) but also on past tasks (backward transfer) with any new data. Our key insight is that we can synergistically ensemble representations -- that were learned independently on disparate tasks -- to enable both forward and bac
    
[^155]: 神经网络的Dropout算法的几乎必然收敛性

    Almost Sure Convergence of Dropout Algorithms for Neural Networks. (arXiv:2002.02247v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2002.02247](http://arxiv.org/abs/2002.02247)

    本文提出了针对神经网络的dropout算法的收敛性及收敛速度的研究，给出了概率论证明，证明其权重将收敛于正常微分方程系统的投影唯一稳态点，同时给出了ε-定态点的通用样本复杂度限制。

    

    本文研究了受Dropout（Hinton等，2012）启发的神经网络随机训练算法的收敛性和收敛速度。为了避免训练期间的过度拟合，实践中的dropout算法实际上是通过在随机梯度下降（SGD）的每次迭代期间将神经网络的权重矩阵逐元素与独立绘制该函数的{0,1} -值矩阵相乘。本文提出了一个概率理论证明，针对具有可微、多项式有界激活函数的全连通神经网络，如果我们在使用dropout算法时将权重投影到紧致集上，则NN的权重将收敛于正常微分方程（ODEs）的投影系统的唯一定常点。在此通用收敛性保证之后，我们继续研究dropout的收敛速度。首先，我们获得了找到ε-定态点的通用样本复杂度界限。

    We investigate the convergence and convergence rate of stochastic training algorithms for Neural Networks (NNs) that have been inspired by Dropout (Hinton et al., 2012). With the goal of avoiding overfitting during training of NNs, dropout algorithms consist in practice of multiplying the weight matrices of a NN componentwise by independently drawn random matrices with $\{0, 1 \}$-valued entries during each iteration of Stochastic Gradient Descent (SGD). This paper presents a probability theoretical proof that for fully-connected NNs with differentiable, polynomially bounded activation functions, if we project the weights onto a compact set when using a dropout algorithm, then the weights of the NN converge to a unique stationary point of a projected system of Ordinary Differential Equations (ODEs). After this general convergence guarantee, we go on to investigate the convergence rate of dropout. Firstly, we obtain generic sample complexity bounds for finding $\epsilon$-stationary poin
    
[^156]: 利用情绪分析、眼动和头部运动进行学生参与度检测的机器学习系统

    Student Engagement Detection Using Emotion Analysis, Eye Tracking and Head Movement with Machine Learning. (arXiv:1909.12913v5 [cs.CV] UPDATED)

    [http://arxiv.org/abs/1909.12913](http://arxiv.org/abs/1909.12913)

    本文提出了一个利用情绪分析、眼动和头部运动进行学生参与度检测的机器学习系统。该系统能够使用笔记本电脑内置摄像头实时监测学生的专注度，通过结合眼睛和头部的运动信息以及面部表情来识别学生处于三种不同的参与度状态，并在典型的在线学习情境下进行了测试，结果表明其能够正确地识别学生的参与度水平。

    

    随着远程学习和在线学习的不断增多，建立一个能够确定学生参与度的系统对于教师、研究人员和政策制定者来说至关重要。本文提出了一个系统来检测学生参与度。它仅使用笔记本电脑内置的典型网络摄像头提供的信息，并被设计为实时工作。我们结合了眼睛和头部的运动信息以及面部表情来产生带有三种参与度分类的专注度指数：“非常参与度”，“名义上参与度”和“没有参与度”。该系统在典型的在线学习情形下进行了测试，结果显示它能够正确识别学生处于“非常参与度”，“名义上参与度”和“没有参与度”的时期。此外，结果还表明，成绩最好的学生也具有更高的专注度。

    With the increase of distance learning, in general, and e-learning, in particular, having a system capable of determining the engagement of students is of primordial importance, and one of the biggest challenges, both for teachers, researchers and policy makers. Here, we present a system to detect the engagement level of the students. It uses only information provided by the typical built-in web-camera present in a laptop computer, and was designed to work in real time. We combine information about the movements of the eyes and head, and facial emotions to produce a concentration index with three classes of engagement: "very engaged", "nominally engaged" and "not engaged at all". The system was tested in a typical e-learning scenario, and the results show that it correctly identifies each period of time where students were "very engaged", "nominally engaged" and "not engaged at all". Additionally, the results also show that the students with best scores also have higher concentration i
    

