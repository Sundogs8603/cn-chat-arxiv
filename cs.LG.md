# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Generalization Properties of Diffusion Models.](http://arxiv.org/abs/2311.01797) | 本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。 |
| [^2] | [High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise.](http://arxiv.org/abs/2310.18784) | 本研究探讨了一类非线性随机梯度下降方法的高概率收敛边界。对于具有Lipschitz连续梯度的强凸损失函数，即使噪声是重尾的，结果证明了对失败概率的对数依赖。这些结果适用于剪切、归一化和量化等任何具有有界输出的非线性函数。 |
| [^3] | [Data-Centric Financial Large Language Models.](http://arxiv.org/abs/2310.17784) | 本文介绍了一种数据中心化的方法，通过预处理和预理解数据来改善大型语言模型（LLMs）在金融任务中的性能。实验证明，采用该方法的金融LLMs在金融分析和解释任务上达到了最先进的水平。 |
| [^4] | [Balancing central and marginal rejection when combining independent significance tests.](http://arxiv.org/abs/2310.16600) | 该论文提出了一种在组合独立显著性检验时平衡中心和边缘拒绝的方法，并提出了一种用于测量两者平衡的组合函数。 |
| [^5] | [Improving Diffusion Models for ECG Imputation with an Augmented Template Prior.](http://arxiv.org/abs/2310.15742) | 该论文提出了一种模板引导去噪扩散概率模型PulseDiff，通过使用信息先验来改进心电图(ECG)的插值和预测准确性。此模型考虑了主体间变化和心跳关系，从而提高了性能。 |
| [^6] | [On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers.](http://arxiv.org/abs/2310.14421) | 本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。 |
| [^7] | [DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation.](http://arxiv.org/abs/2310.12570) | DA-TransUNet 提出了一种新的深度医学图像分割框架，集成了Transformer和双重注意力块。通过注意机制和多方面特征提取，提升医学图像分割结果。 |
| [^8] | [Generative Intrinsic Optimization: Intrisic Control with Model Learning.](http://arxiv.org/abs/2310.08100) | 这项工作提出了一种生成内在优化的方法，通过结合模型学习和内在控制，实现了对不同形式结果的综合处理。这种方法保证了收敛到最优策略，有助于提高样本效率并考虑环境不确定性。 |
| [^9] | [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains.](http://arxiv.org/abs/2310.07838) | 本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。 |
| [^10] | [GenTKG: Generative Forecasting on Temporal Knowledge Graph.](http://arxiv.org/abs/2310.07793) | 研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。 |
| [^11] | [A Corrected Expected Improvement Acquisition Function Under Noisy Observations.](http://arxiv.org/abs/2310.05166) | 这个论文提出了一个修正的期望改善采集函数，在贝叶斯优化中解决了对于有噪声观测的情况下忽略候选解不确定性的问题。 |
| [^12] | [Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas.](http://arxiv.org/abs/2309.14610) | 本研究基于无监督图深度学习模型提出了集成城市洪水风险评级模型，能够捕捉区域之间的空间依赖关系和洪水危险与城市要素之间的复杂相互作用，揭示了城市地区的突发洪水风险概况 |
| [^13] | [Quantifying Credit Portfolio sensitivity to asset correlations with interpretable generative neural networks.](http://arxiv.org/abs/2309.08652) | 本研究提出了一种新方法，利用生成的金融相关矩阵量化信贷组合对资产相关性的敏感性。通过使用可解释的变分自动编码器（VAE）的潜在空间表示，揭示了影响投资组合多元化的关键因素。 |
| [^14] | [Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties.](http://arxiv.org/abs/2309.00305) | 本研究通过应用机器学习技术，开发并研究了两个不同领域的数据集，用于高效预测材料科学中的微结构属性。 |
| [^15] | [Homological Convolutional Neural Networks.](http://arxiv.org/abs/2308.13816) | 提出了一种新的同调卷积神经网络架构，通过拓扑约束网络表示来利用表格数据的结构组织，从稀疏的表格数据中获取空间信息，具有良好的有效性和泛化能力。 |
| [^16] | [Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic.](http://arxiv.org/abs/2308.07336) | 本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。 |
| [^17] | [Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping.](http://arxiv.org/abs/2307.15807) | 这项研究通过使用物联网设备和机器学习方法，系统地探索了在工业机械中进行异常检测的挑战和机遇。该研究强调了机器学习算法在自动化检测工业机械异常中的重要性。 |
| [^18] | [FinGPT: Democratizing Internet-scale Data for Financial Large Language Models.](http://arxiv.org/abs/2307.10485) | FinGPT是一个开源的数据为中心的框架，旨在将互联网规模的金融数据民主化为金融大语言模型。它提供了自动收集和整理实时金融数据的功能，解决了金融文本数据稀缺的问题。 |
| [^19] | [No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models.](http://arxiv.org/abs/2307.06440) | 本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。 |
| [^20] | [HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution.](http://arxiv.org/abs/2306.15794) | HyenaDNA是一种基于隐式卷积的基因组序列建模方法，可以在单核苷酸分辨率下对长范围相互作用进行建模。 |
| [^21] | [Review of compressed embedding layers and their applications for recommender systems.](http://arxiv.org/abs/2306.13724) | 论文综述了可训练的、压缩的嵌入层在压缩大型神经网络推荐系统中的应用，并提供了相关实验结果。 |
| [^22] | [Towards a robust and reliable deep learning approach for detection of compact binary mergers in gravitational wave data.](http://arxiv.org/abs/2306.11797) | 本篇论文通过开发一个能在保持数据纯度的前提下实现稳健性和可靠性的深度学习模型，提高了搜索速度、参数空间覆盖和搜索灵敏度，而该模型在LIGO数据上测试的表现表明其具有出色的性能。 |
| [^23] | [Phase Transitions of Civil Unrest across Countries and Time.](http://arxiv.org/abs/2306.08698) | 跨国和时间的社会动荡相变研究探索了集体社会动荡是否可以被描述为一系列具有可测量和可识别特征的循环相变，并证明了宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，以及普遍机制可能支撑着社会动荡的某些方面。 |
| [^24] | [Unbiased Learning of Deep Generative Models with Structured Discrete Representations.](http://arxiv.org/abs/2306.08230) | 该论文提出了一种名为结构化变分自编码器的深度生成模型，它通过图像模型的结构和可解释性以及深度学习的适用于高维数据的灵活似然，结合两种框架的优势。同时，该论文还提出了一种学习SVAE的新算法，与此同时，推导出了一种计算自然梯度的方法，这些优化创新使得SVAE首次能与最先进的时间序列模型进行比较。 |
| [^25] | [Single-Model Attribution via Final-Layer Inversion.](http://arxiv.org/abs/2306.06210) | 本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。 |
| [^26] | [SGLD-Based Information Criteria and the Over-Parameterized Regime.](http://arxiv.org/abs/2306.05583) | 本研究提出针对超参数化学习算法的SGLD信息准则，通过KL信息和KL散度罚项来追踪双下降现象。 |
| [^27] | [Diffused Redundancy in Pre-trained Representations.](http://arxiv.org/abs/2306.00183) | 本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。 |
| [^28] | [Research And Implementation Of Drug Target Interaction Confidence Measurement Method Based On Causal Intervention.](http://arxiv.org/abs/2306.00041) | 本研究提出了一种基于因果干预的置信度测量方法来提高药物靶点相互作用预测模型的准确性，着重解决了知识图谱嵌入模型的不足问题。 |
| [^29] | [Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems.](http://arxiv.org/abs/2304.06193) | 本文提出了一种针对非线性部分观测动态系统的学习控制策略参数化，能够自动满足闭环系统的稳定性和用户可调整的鲁棒性条件，在两个强化学习任务的模拟中表现良好且有强鲁棒性。 |
| [^30] | [DWA: Differential Wavelet Amplifier for Image Super-Resolution.](http://arxiv.org/abs/2304.01994) | 本文介绍了一种基于小波的图像超分辨率模块DWA，通过利用两个卷积滤波器的差异改进小波SR模型，在小波域中提高相关特征提取并抑制噪声。在现有的SR模型中集成DWA，如DWSR和MWCNN，可以显示出其有效性。 |
| [^31] | [Generalized partitioned local depth.](http://arxiv.org/abs/2303.10167) | 本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。 |
| [^32] | [PAC-Bayesian Generalization Bounds for Adversarial Generative Models.](http://arxiv.org/abs/2302.08942) | 将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。 |
| [^33] | [A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity.](http://arxiv.org/abs/2302.06015) | 本文提供了第一份对于浅层ViT进行训练的理论分析，证明了使用SGD训练会产生稀疏的注意力图，目前的样本复杂度与标记相关令牌的分数倒数、标记级别的令牌噪声水平和初始模型错误呈正相关关系。 |
| [^34] | [Self-Consistent Velocity Matching of Probability Flows.](http://arxiv.org/abs/2301.13737) | 我们提出了一种无网格的框架，用于求解守恒性偏微分方程，包括时间相关的Fokker-Planck方程和Wasserstein梯度流。通过自洽的速度匹配方法和迭代形式，我们的方法绕过了计算障碍，并在高维情况下表现出优越性能。 |
| [^35] | [On the Lipschitz Constant of Deep Networks and Double Descent.](http://arxiv.org/abs/2301.12309) | 本文通过实验研究发现，深度网络的利普希茨常数趋势与测试误差密切相关，通过建立参数空间和输入空间梯度之间的联系，确定了损失函数曲率和距离初始化参数的距离对于深度网络的优化和模型函数复杂度限制是关键因素，该研究对隐式正则化和网络的有效模型复杂度提供了新的见解。 |
| [^36] | [Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification.](http://arxiv.org/abs/2301.09702) | 本文提出了一个Synthesis Model Bank（SMB）来处理无监督领域自适应人物再识别中的光照变化。SMB包括卷积神经网络（CNN）和马氏距离矩阵，通过使用不同光照条件的合成数据进行训练，增强了光照变化的鲁棒性。 |
| [^37] | [Improved Kernel Alignment Regret Bound for Online Kernel Learning.](http://arxiv.org/abs/2212.12989) | 本文提出了一种新的算法，在线核学习中的新算法遗憾界和计算复杂度优于以前的结果。该算法的遗憾界和计算复杂度取决于核矩阵特征值的衰减率。 |
| [^38] | [HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization.](http://arxiv.org/abs/2211.08253) | 本文提出了一种新的领域泛化方法HMOE，它不需要领域标签，更具可解释性，使用超网络生成专家权重，能够在低维向量空间中探索专家的相似性，实验结果表明HMOE可以划分混合数据并取得更好的效果。 |
| [^39] | [Neural Moving Horizon Estimation for Robust Flight Control.](http://arxiv.org/abs/2206.10397) | 本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。 |

# 详细

[^1]: 关于扩散模型的泛化属性

    On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])

    [http://arxiv.org/abs/2311.01797](http://arxiv.org/abs/2311.01797)

    本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。

    

    扩散模型是一类生成模型，用于建立一个随机传输映射，将经验观测到的但未知的目标分布与已知的先验分布联系起来。尽管在实际应用中取得了显著的成功，但对其泛化能力的理论理解仍未充分发展。本文对扩散模型的泛化属性进行了全面的理论研究。我们建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，表明在样本大小$n$和模型容量$m$上都存在多项式小的泛化误差($O(n^{-2/5}+m^{-4/5})$)，在停止训练时可以避免维度诅咒（即数据维度不呈指数级增长）。此外，我们将定量分析扩展到了一个数据依赖的情景，其中目标分布被描绘为一系列的概率密度函数。

    Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progr
    
[^2]: 高概率收敛边界下的非线性随机梯度下降在重尾噪声下的研究

    High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. (arXiv:2310.18784v1 [cs.LG])

    [http://arxiv.org/abs/2310.18784](http://arxiv.org/abs/2310.18784)

    本研究探讨了一类非线性随机梯度下降方法的高概率收敛边界。对于具有Lipschitz连续梯度的强凸损失函数，即使噪声是重尾的，结果证明了对失败概率的对数依赖。这些结果适用于剪切、归一化和量化等任何具有有界输出的非线性函数。

    

    最近几个研究工作研究了随机梯度下降（SGD）及其剪切变体的高概率收敛。与普通的SGD相比，剪切SGD在实际中更加稳定，并且在理论上有对数依赖于失败概率的额外好处。然而，其他实际非线性SGD变体（如符号SGD、量化SGD和归一化SGD）的收敛性理解要少得多，这些方法实现了改进的通信效率或加速收敛。在本工作中，我们研究了一类广义非线性SGD方法的高概率收敛边界。对于具有Lipschitz连续梯度的强凸损失函数，即使噪声是重尾的，我们证明了对失败概率的对数依赖。与剪切SGD的结果相比，我们的结果更为一般，适用于具有有界输出的任何非线性函数，如剪切、归一化和量化。

    Several recent works have studied the convergence \textit{in high probability} of stochastic gradient descent (SGD) and its clipped variant. Compared to vanilla SGD, clipped SGD is practically more stable and has the additional theoretical benefit of logarithmic dependence on the failure probability. However, the convergence of other practical nonlinear variants of SGD, e.g., sign SGD, quantized SGD and normalized SGD, that achieve improved communication efficiency or accelerated convergence is much less understood. In this work, we study the convergence bounds \textit{in high probability} of a broad class of nonlinear SGD methods. For strongly convex loss functions with Lipschitz continuous gradients, we prove a logarithmic dependence on the failure probability, even when the noise is heavy-tailed. Strictly more general than the results for clipped SGD, our results hold for any nonlinearity with bounded (component-wise or joint) outputs, such as clipping, normalization, and quantizati
    
[^3]: 数据中心化的金融大型语言模型

    Data-Centric Financial Large Language Models. (arXiv:2310.17784v1 [cs.CL])

    [http://arxiv.org/abs/2310.17784](http://arxiv.org/abs/2310.17784)

    本文介绍了一种数据中心化的方法，通过预处理和预理解数据来改善大型语言模型（LLMs）在金融任务中的性能。实验证明，采用该方法的金融LLMs在金融分析和解释任务上达到了最先进的水平。

    

    大型语言模型（LLMs）在自然语言任务中表现出良好的潜力，但直接应用于复杂领域如金融时却遇到困难。LLMs难以推理和整合所有相关信息。我们提出了一种数据中心化的方法，使LLMs能够更好地处理金融任务。我们的关键观点是，不是一次性给LLM负载过多信息，而是更有效地对数据进行预处理和预理解。我们使用多任务基于提示的微调来创建金融LLM（FLLM），以实现数据预处理和预理解。然而，每个任务的标记数据有限。为了克服手动注释的成本，我们采用了自动生成训练数据的增强推理（AAR）来修改FLLM自身输出的伪标签。实验证明，我们的数据中心化FLLM与AAR相比，显著优于为原始文本设计的基线金融LLMs，在金融分析和解释任务上达到了最先进的水平。

    Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We 
    
[^4]: 在组合独立显著性检验时平衡中心和边缘拒绝

    Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v1 [stat.ME])

    [http://arxiv.org/abs/2310.16600](http://arxiv.org/abs/2310.16600)

    该论文提出了一种在组合独立显著性检验时平衡中心和边缘拒绝的方法，并提出了一种用于测量两者平衡的组合函数。

    

    当原始数据不可用时，评估一组p值的显著性的常见方法是将它们与汇集函数进行组合。这些汇集的p值将p值样本转化为一个表现类似于单变量p值的单一数值。为了明确讨论这些函数，引入了一系列交叉假设，以传达p值中非零证据的强度和普遍性，然后讨论了常规汇集公式。在特定交叉假设的UMP汇集p值中观察到的模式推动了对于中心和边缘拒绝水平在α处的定义和讨论。证明了中心拒绝总是大于等于边缘拒绝，从而提出了一种用于测量两者在汇集的p值中平衡的商。基于χ²_κ分位数变换的组合函数被提出以控制这个商，并且被证明是有效的。

    A common approach to evaluating the significance of a collection of $p$-values combines them with a pooling function, in particular when the original data are not available. These pooled $p$-values convert a sample of $p$-values into a single number which behaves like a univariate $p$-value. To clarify discussion of these functions, a telescoping series of alternative hypotheses are introduced that communicate the strength and prevalence of non-null evidence in the $p$-values before general pooling formulae are discussed. A pattern noticed in the UMP pooled $p$-value for a particular alternative motivates the definition and discussion of central and marginal rejection levels at $\alpha$. It is proven that central rejection is always greater than or equal to marginal rejection, motivating a quotient to measure the balance between the two for pooled $p$-values. A combining function based on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this quotient and shown to be
    
[^5]: 用增强的模板先验改进心电图(ECG)插值的扩散模型

    Improving Diffusion Models for ECG Imputation with an Augmented Template Prior. (arXiv:2310.15742v1 [cs.LG])

    [http://arxiv.org/abs/2310.15742](http://arxiv.org/abs/2310.15742)

    该论文提出了一种模板引导去噪扩散概率模型PulseDiff，通过使用信息先验来改进心电图(ECG)的插值和预测准确性。此模型考虑了主体间变化和心跳关系，从而提高了性能。

    

    作为常规临床护理的一部分，如心电图(ECG)等脉冲信号被广泛收集。然而，使用移动健康系统收集的信号存在噪音和质量差的录音，导致缺失值的存在，这是一个重要问题，降低了信号的质量，并影响了自动化的下游任务。最近的研究探索使用概率时间序列模型对ECG进行缺失值插补。然而，与确定性模型相比，它们的性能仍然有限，因为训练目标中没有明确考虑主体间的变化和心跳关系。在这项工作中，为了改善概率模型的ECG插值和预测准确性，我们提出了一个条件信息先验的模板引导去噪扩散概率模型，PulseDiff。具体而言，1)我们首先从观测中提取一个主体级别的脉冲模板作为先验

    Pulsative signals such as the electrocardiogram (ECG) are extensively collected as part of routine clinical care. However, noisy and poor-quality recordings, leading to missing values, are a major issue for signals collected using mobile health systems, decreasing the signal quality and affecting the automated downstream tasks. Recent studies have explored imputation of missing values for ECG with probabilistic time-series models. Nevertheless, in comparison with the deterministic models, their performance is still limited, as the variations across subjects and heart-beat relationships are not explicitly considered in the training objective. In this work, to improve the ECG imputation and forecasting accuracy with probabilistic models, we present an template-guided denoising diffusion probabilistic model, PulseDiff, which is conditioned an informative prior for a range of health conditions. Specifically, 1) we first extract a subject-level pulsative template from the observation as an 
    
[^6]: 对AI分类器的对抗鲁棒性度量的存在性，唯一性和可扩展性研究

    On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])

    [http://arxiv.org/abs/2310.14421](http://arxiv.org/abs/2310.14421)

    本文研究了针对AI分类器的对抗鲁棒性度量的存在性、唯一性和可扩展性，提出了可以验证的数学条件，并在合成基准测试和生物医学应用中进行了实际计算和解释。

    

    本文提出并证明了针对（局部）唯一可逆分类器、广义线性模型（GLM）和熵AI（EAI）具有最小对抗路径（MAP）和最小对抗距离（MAD）的存在性、唯一性和明确的分析计算的简单可验证的数学条件。在常见的合成基准测试数据集上，针对神经网络、提升随机森林、GLM和EAI等各类AI工具进行MAP和MAD的实际计算、比较和解释，包括双卷状螺旋线及其扩展以及两个生物医学数据问题（用于健康保险理赔预测和心脏病发作致死率分类）。在生物医学应用中，展示了MAP如何在预定义的可访问控制变量子集中提供唯一的最小患者特定风险缓解干预措施。

    Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
    
[^7]: DA-TransUNet: 将Spatial和Channel Dual Attention与Transformer U-Net集成，用于医学图像分割

    DA-TransUNet: Integrating Spatial and Channel Dual Attention with Transformer U-Net for Medical Image Segmentation. (arXiv:2310.12570v1 [eess.IV])

    [http://arxiv.org/abs/2310.12570](http://arxiv.org/abs/2310.12570)

    DA-TransUNet 提出了一种新的深度医学图像分割框架，集成了Transformer和双重注意力块。通过注意机制和多方面特征提取，提升医学图像分割结果。

    

    由于强大的深度表示学习，自动化医学图像分割取得了很大的进展。Transformer的影响导致了对其变体的研究，并大规模替代传统的CNN模块。然而，这种趋势经常忽视了Transformer的固有特征提取能力以及通过微小调整对模型和Transformer模块的潜在改进。本研究提出了一种新颖的深度医学图像分割框架，称为DA-TransUNet，旨在将Transformer和双重注意块引入传统U形架构的编码器和解码器中。与之前基于Transformer的解决方案不同，我们的DA-TransUNet利用了Transformer的注意机制和DA-Block的多方面特征提取，可以有效地结合全局、局部和多尺度特征以增强医学图像分割。同时，实验结果表明在之前的Transformer U-Net的基础上添加了一个双重注意块。

    Great progress has been made in automatic medical image segmentation due to powerful deep representation learning. The influence of transformer has led to research into its variants, and large-scale replacement of traditional CNN modules. However, such trend often overlooks the intrinsic feature extraction capabilities of the transformer and potential refinements to both the model and the transformer module through minor adjustments. This study proposes a novel deep medical image segmentation framework, called DA-TransUNet, aiming to introduce the Transformer and dual attention block into the encoder and decoder of the traditional U-shaped architecture. Unlike prior transformer-based solutions, our DA-TransUNet utilizes attention mechanism of transformer and multifaceted feature extraction of DA-Block, which can efficiently combine global, local, and multi-scale features to enhance medical image segmentation. Meanwhile, experimental results show that a dual attention block is added bef
    
[^8]: 生成内在优化：具有模型学习的内在控制

    Generative Intrinsic Optimization: Intrisic Control with Model Learning. (arXiv:2310.08100v1 [cs.LG])

    [http://arxiv.org/abs/2310.08100](http://arxiv.org/abs/2310.08100)

    这项工作提出了一种生成内在优化的方法，通过结合模型学习和内在控制，实现了对不同形式结果的综合处理。这种方法保证了收敛到最优策略，有助于提高样本效率并考虑环境不确定性。

    

    未来序列代表在环境中执行动作后的结果。当基于信息论概念的相互信息驱动时，它寻求最具信息量的结果。显式结果可能因状态、回报或轨迹而异，用于不同目的，如学分分配或模仿学习。然而，将内在动机与奖励最大化结合的固有性质往往被忽视。在这项工作中，我们提出了一种变分方法，共同学习估计相互信息和动力学模型的必要数量，为合并不同形式的感兴趣结果提供了一个通用框架。结合到策略迭代方案中，我们的方法保证收敛到最优策略。虽然我们主要关注理论分析，但我们的方法打开了利用带有模型学习的内在控制以提高样本效率并纳入环境不确定性的可能性。

    Future sequence represents the outcome after executing the action into the environment. When driven by the information-theoretic concept of mutual information, it seeks maximally informative consequences. Explicit outcomes may vary across state, return, or trajectory serving different purposes such as credit assignment or imitation learning. However, the inherent nature of incorporating intrinsic motivation with reward maximization is often neglected. In this work, we propose a variational approach to jointly learn the necessary quantity for estimating the mutual information and the dynamics model, providing a general framework for incorporating different forms of outcomes of interest. Integrated into a policy iteration scheme, our approach guarantees convergence to the optimal policy. While we mainly focus on theoretical analysis, our approach opens the possibilities of leveraging intrinsic control with model learning to enhance sample efficiency and incorporate uncertainty of the env
    
[^9]: 探索有限领域知识传递的基本限制

    Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])

    [http://arxiv.org/abs/2310.07838](http://arxiv.org/abs/2310.07838)

    本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。

    

    我们对通过从教师到概率化学生分类器的n个样本进行知识传递的统计效率进行了表征，其中输入空间S和标签A为有限域。我们发现，在三个渐进级别上的特权信息可以加快传递的速度。在第一级别上，只有具有困难标签的样本是已知的，最大似然估计器能够达到最小化速率sqrt(|S||A|/n)。第二级别上，除了已知的困难标签样本外，还有采样标签的教师概率可用，这将收敛速度的下界提高到|S||A|/n。然而，在第二个数据采集协议下，最小化交叉熵损失的朴素适应会导致渐近偏差的学生。我们克服了这个限制，并通过使用一种新颖的经验变体的平方误差逻辑损失来实现了基本限制。第三级别进一步赋予学生软标签。

    We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
    
[^10]: GenTKG: 基于生成模型的时间知识图谱预测

    GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])

    [http://arxiv.org/abs/2310.07793](http://arxiv.org/abs/2310.07793)

    研究提出了一种名为GenTKG的生成模型，用于在时间知识图谱上进行预测。该模型通过结合基于时间逻辑规则的检索策略和轻量级的参数效率指导，克服了复杂的时间图数据结构和庞大的数据量所带来的挑战。

    

    大规模语言模型(LLM)的快速发展引发了对时间知识图谱(tKG)领域的兴趣，其中传统的基于嵌入和规则的模型占主导地位。目前仍然存在一个问题，即预训练的LLM是否能够理解结构化的时间关系数据，并取代它们成为时间关系预测的基础模型。因此，我们将时间知识预测引入生成模式。然而，在复杂的时间图数据结构和LLM可以处理的序列自然表达之间存在巨大的鸿沟，在tKG的庞大数据量和微调LLM的巨大计算成本之间也存在挑战。为了解决这些挑战，我们提出了一种新颖的检索增强生成框架，称为GenTKG，它在tKG上执行生成式预测，结合了基于时间逻辑规则的检索策略和轻量级的参数效率指导。通过大量实验证明了GenTKG的有效性。

    The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
    
[^11]: 一个在有噪声观测下修正的期望改善采集函数

    A Corrected Expected Improvement Acquisition Function Under Noisy Observations. (arXiv:2310.05166v1 [cs.LG])

    [http://arxiv.org/abs/2310.05166](http://arxiv.org/abs/2310.05166)

    这个论文提出了一个修正的期望改善采集函数，在贝叶斯优化中解决了对于有噪声观测的情况下忽略候选解不确定性的问题。

    

    序列最大化期望改善(EI)是贝叶斯优化中最常用的策略之一，因其简单性和处理噪声观测的能力而广泛应用。特别是，在噪声环境中，改善函数通常使用最佳后验均值作为最佳候选解。然而，在许多解析的EI类型方法中，常常忽略与候选解相关的不确定性：在无噪声的情况下导出了一个闭合形式的采集函数，然后应用于有噪声观测的情况。为了解决这个限制，我们提出了一种修正EI的方法，将高斯过程(GP)模型提供的协方差信息纳入其闭合形式表达式中。这个采集函数与经典的无噪声结果相吻合，我们认为它应该取代贝叶斯优化软件包、教程和教材中的那个公式。这个改进的采集函数为有噪声和无噪声的解提供了良好的适用性。

    Sequential maximization of expected improvement (EI) is one of the most widely used policies in Bayesian optimization because of its simplicity and ability to handle noisy observations. In particular, the improvement function often uses the best posterior mean as the best incumbent in noisy settings. However, the uncertainty associated with the incumbent solution is often neglected in many analytic EI-type methods: a closed-form acquisition function is derived in the noise-free setting, but then applied to the setting with noisy observations. To address this limitation, we propose a modification of EI that corrects its closed-form expression by incorporating the covariance information provided by the Gaussian Process (GP) model. This acquisition function specializes to the classical noise-free result, and we argue should replace that formula in Bayesian optimization software packages, tutorials, and textbooks. This enhanced acquisition provides good generality for noisy and noiseless s
    
[^12]: 无监督的图深度学习揭示了城市地区突发洪水风险概况

    Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v1 [cs.LG])

    [http://arxiv.org/abs/2309.14610](http://arxiv.org/abs/2309.14610)

    本研究基于无监督图深度学习模型提出了集成城市洪水风险评级模型，能够捕捉区域之间的空间依赖关系和洪水危险与城市要素之间的复杂相互作用，揭示了城市地区的突发洪水风险概况

    

    城市洪水风险源于与洪水危险、洪水暴露以及社会和物理脆弱性相关的多个要素之间的复杂和非线性相互作用，以及复杂的空间洪水依赖关系。然而，现有的用于表征城市洪水风险的方法主要是基于洪水平原地图，侧重于有限数量的要素，主要是危险和暴露要素，没有考虑要素之间的相互作用或空间区域之间的依赖关系。为了填补这一空白，本研究提出了一种基于新颖的无监督图深度学习模型（称为FloodRisk-Net）的集成城市洪水风险评级模型。FloodRisk-Net能够捕捉区域之间的空间依赖关系以及洪水危险和城市要素之间的复杂和非线性相互作用，从而确定突发洪水风险。利用美国多个都市统计区（MSAs）的数据，该模型将它们的洪水风险特征化为

    Urban flood risk emerges from complex and nonlinear interactions among multiple features related to flood hazard, flood exposure, and social and physical vulnerabilities, along with the complex spatial flood dependence relationships. Existing approaches for characterizing urban flood risk, however, are primarily based on flood plain maps, focusing on a limited number of features, primarily hazard and exposure features, without consideration of feature interactions or the dependence relationships among spatial areas. To address this gap, this study presents an integrated urban flood-risk rating model based on a novel unsupervised graph deep learning model (called FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among areas and complex and nonlinear interactions among flood hazards and urban features for specifying emergent flood risk. Using data from multiple metropolitan statistical areas (MSAs) in the United States, the model characterizes their flood risk into
    
[^13]: 用可解释的生成神经网络量化信贷组合对资产相关性的敏感性

    Quantifying Credit Portfolio sensitivity to asset correlations with interpretable generative neural networks. (arXiv:2309.08652v1 [q-fin.RM])

    [http://arxiv.org/abs/2309.08652](http://arxiv.org/abs/2309.08652)

    本研究提出了一种新方法，利用生成的金融相关矩阵量化信贷组合对资产相关性的敏感性。通过使用可解释的变分自动编码器（VAE）的潜在空间表示，揭示了影响投资组合多元化的关键因素。

    

    在本研究中，我们提出了一种新的方法，利用深度学习模型生成的合成金融相关矩阵，量化信贷组合价值风险（VaR）对资产相关性的敏感性。之前的工作中，使用生成对抗网络（GANs）演示了生成能捕捉到资产收益的经验相关矩阵中观察到的基本特征的可信相关矩阵。我们使用变分自动编码器（VAE）而不是GANs，以实现更可解释的潜在空间表示。通过我们的分析，我们揭示了VAE潜在空间可以成为捕捉影响投资组合多元化的关键因素的有用工具，特别是与信贷组合敏感性和资产相关性变化相关的因素。

    In this research, we propose a novel approach for the quantification of credit portfolio Value-at-Risk (VaR) sensitivity to asset correlations with the use of synthetic financial correlation matrices generated with deep learning models. In previous work Generative Adversarial Networks (GANs) were employed to demonstrate the generation of plausible correlation matrices, that capture the essential characteristics observed in empirical correlation matrices estimated on asset returns. Instead of GANs, we employ Variational Autoencoders (VAE) to achieve a more interpretable latent space representation. Through our analysis, we reveal that the VAE latent space can be a useful tool to capture the crucial factors impacting portfolio diversification, particularly in relation to credit portfolio sensitivity to asset correlations changes.
    
[^14]: 高效的材料科学模拟代理模型：基于机器学习的微结构属性预测

    Efficient Surrogate Models for Materials Science Simulations: Machine Learning-based Prediction of Microstructure Properties. (arXiv:2309.00305v1 [cs.LG])

    [http://arxiv.org/abs/2309.00305](http://arxiv.org/abs/2309.00305)

    本研究通过应用机器学习技术，开发并研究了两个不同领域的数据集，用于高效预测材料科学中的微结构属性。

    

    在化学、生物学、气象学、物理学、工程学和材料科学等许多科学领域中，确定、理解和预测所谓的结构-性质关系是一项重要任务。结构指的是物质、材料或物质的空间分布，而性质是一种结果特性，通常以非平凡的方式取决于结构的空间细节。传统上，这些任务通常采用正向模拟模型进行。最近，在这些科学领域中，已经应用了几种机器学习算法来增强和加速模拟模型，或作为代理模型。在本研究中，我们开发和研究了基于两个不同材料科学领域的数据集的六种机器学习技术的应用：用于预测磁性域形成的二维灰度模型的数据，以及表示双相微结构演变的数据。

    Determining, understanding, and predicting the so-called structure-property relation is an important task in many scientific disciplines, such as chemistry, biology, meteorology, physics, engineering, and materials science. Structure refers to the spatial distribution of, e.g., substances, material, or matter in general, while property is a resulting characteristic that usually depends in a non-trivial way on spatial details of the structure. Traditionally, forward simulations models have been used for such tasks. Recently, several machine learning algorithms have been applied in these scientific fields to enhance and accelerate simulation models or as surrogate models. In this work, we develop and investigate the applications of six machine learning techniques based on two different datasets from the domain of materials science: data from a two-dimensional Ising model for predicting the formation of magnetic domains and data representing the evolution of dual-phase microstructures fro
    
[^15]: 同调卷积神经网络

    Homological Convolutional Neural Networks. (arXiv:2308.13816v1 [cs.LG])

    [http://arxiv.org/abs/2308.13816](http://arxiv.org/abs/2308.13816)

    提出了一种新的同调卷积神经网络架构，通过拓扑约束网络表示来利用表格数据的结构组织，从稀疏的表格数据中获取空间信息，具有良好的有效性和泛化能力。

    

    深度学习方法已经在分类和回归任务上展示出了卓越的性能，但是在表格数据上，经典机器学习方法往往比越来越复杂的深度学习架构更具计算效率且同样有效。这是因为表格数据中特征间的相关性比图像或自然语言的空间或语义关系要弱，而且需要在没有任何先验信息的情况下对依赖关系进行建模。在本文中，我们提出了一种新的深度学习架构，通过拓扑约束网络表示来利用数据的结构组织，从稀疏的表格数据中获取空间信息。所得模型利用了卷积的能力，并围绕网络拓扑中的有限概念来保证（i）数据一致性、（ii）表示有效性、和（iii）容量。我们在多个任务和数据集上进行了实验证明了该模型的有效性和泛化能力。

    Deep learning methods have demonstrated outstanding performances on classification and regression tasks on homogeneous data types (e.g., image, audio, and text data). However, tabular data still poses a challenge with classic machine learning approaches being often computationally cheaper and equally effective than increasingly complex deep learning architectures. The challenge arises from the fact that, in tabular data, the correlation among features is weaker than the one from spatial or semantic relationships in images or natural languages, and the dependency structures need to be modeled without any prior information. In this work, we propose a novel deep learning architecture that exploits the data structural organization through topologically constrained network representations to gain spatial information from sparse tabular data. The resulting model leverages the power of convolutions and is centered on a limited number of concepts from network topology to guarantee (i) a data-c
    
[^16]: 从合成语料库和形式逻辑学习演绎推理

    Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])

    [http://arxiv.org/abs/2308.07336](http://arxiv.org/abs/2308.07336)

    本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。

    

    我们研究了一种从合成语料库中学习演绎推理能力的语言模型（LMs）方法。之前的研究使用了具体的演绎规则来生成演绎示例，但这些规则受限或者是任意的。这可能限制了所获得演绎推理能力的泛化能力。我们重新思考并采用基于形式逻辑理论的一组良好基础的演绎规则，当这些规则以多步方式组合时，可以推导出任何其他演绎规则。我们通过实验证明，在提出的语料库上训练的LMs，即$\textbf{FLD}$（$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction），获得了更具泛化性的演绎推理能力。此外，我们确定了演绎推理语料库可以增强LMs的推理能力的方面，以及不同方面无法增强的方面。最后，基于这些结果，我们讨论了将演绎语料库或其他方法应用于每个方面的未来方向。

    We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
    
[^17]: 使用物联网设备和机器学习在工业机械中进行异常检测：一种系统映射

    Anomaly Detection in Industrial Machinery using IoT Devices and Machine Learning: a Systematic Mapping. (arXiv:2307.15807v1 [cs.LG])

    [http://arxiv.org/abs/2307.15807](http://arxiv.org/abs/2307.15807)

    这项研究通过使用物联网设备和机器学习方法，系统地探索了在工业机械中进行异常检测的挑战和机遇。该研究强调了机器学习算法在自动化检测工业机械异常中的重要性。

    

    在智能工业中，异常检测对于预防设备故障、减少停机时间和提高安全性至关重要。物联网（IoT）使得从工业机械中收集大量数据成为可能，为异常检测提供了丰富的信息来源。然而，物联网生态系统生成的数据量和复杂性使得人工手动检测异常变得困难。机器学习算法可以通过分析生成的数据来自动化工业机械中的异常检测。此外，每种技术基于数据的性质和相应的系统都有特定的优势和弱点。然而，目前关于异常检测的研究主要集中在解决网络和网络安全相关问题，对于工业部门的关注有限。此外，这些研究还未涵盖使用机器学习在工业机械中进行异常检测所涉及的挑战。

    Anomaly detection is critical in the smart industry for preventing equipment failure, reducing downtime, and improving safety. Internet of Things (IoT) has enabled the collection of large volumes of data from industrial machinery, providing a rich source of information for Anomaly Detection. However, the volume and complexity of data generated by the Internet of Things ecosystems make it difficult for humans to detect anomalies manually. Machine learning (ML) algorithms can automate anomaly detection in industrial machinery by analyzing generated data. Besides, each technique has specific strengths and weaknesses based on the data nature and its corresponding systems. However, the current systematic mapping studies on Anomaly Detection primarily focus on addressing network and cybersecurity-related problems, with limited attention given to the industrial sector. Additionally, these studies do not cover the challenges involved in using ML for Anomaly Detection in industrial machinery wi
    
[^18]: FinGPT: 将互联网规模的金融数据民主化为金融大语言模型

    FinGPT: Democratizing Internet-scale Data for Financial Large Language Models. (arXiv:2307.10485v1 [cs.CL])

    [http://arxiv.org/abs/2307.10485](http://arxiv.org/abs/2307.10485)

    FinGPT是一个开源的数据为中心的框架，旨在将互联网规模的金融数据民主化为金融大语言模型。它提供了自动收集和整理实时金融数据的功能，解决了金融文本数据稀缺的问题。

    

    大型语言模型（LLM）在理解和生成类似人类文本方面展示了卓越的能力，这可能会彻底改变金融行业。然而，现有的LLM在金融领域往往表现不佳，主要原因是一般文本数据与金融文本数据之间的差异。不幸的是，现有的金融文本数据集数量有限（大小较小），而第一个金融LLM（FinLLM）BloombergGPT是封闭的（只发布了训练日志）。鉴于此，我们的目标是通过Internet规模的金融数据将LLM民主化，由于数据来源多样、信噪比低和时间有效性高，这是一个开放性的挑战。为了解决这些挑战，我们引入了一个开源和数据为中心的框架“金融生成预训练变压器（FinGPT）”，它可以自动收集和整理来自互联网上超过34个不同来源的实时金融数据。

    Large language models (LLMs) have demonstrated remarkable proficiency in understanding and generating human-like texts, which may potentially revolutionize the finance industry. However, existing LLMs often fall short in the financial field, which is mainly attributed to the disparities between general text data and financial text data. Unfortunately, there is only a limited number of financial text datasets available (quite small size), and BloombergGPT, the first financial LLM (FinLLM), is close-sourced (only the training logs were released). In light of this, we aim to democratize Internet-scale financial data for LLMs, which is an open challenge due to diverse data sources, low signal-to-noise ratio, and high time-validity. To address the challenges, we introduce an open-sourced and data-centric framework, \textit{Financial Generative Pre-trained Transformer (FinGPT)}, that automates the collection and curation of real-time financial data from >34 diverse sources on the Internet, p
    
[^19]: 没有训练就没有收益：重新审视基于Transformer的语言模型的高效训练算法

    No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])

    [http://arxiv.org/abs/2307.06440](http://arxiv.org/abs/2307.06440)

    本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。

    

    近年来，训练Transformer-based语言模型所需的计算量急剧增加。这一趋势促使研究者们开展了针对高效训练算法的研究，旨在比标准训练更快地改善训练、验证和下游性能。在这项工作中，我们重新审视了三类这样的算法：动态架构（层叠、层丢弃）、批量选择（选择性反向传播、RHO损失）和高效优化器（Lion、Sophia）。当使用这些方法在固定计算预算下对BERT和T5进行预训练时，我们发现它们的训练、验证和下游收益相对于一个具有完全衰减学习率的基线而言会消失。我们定义了一个评估协议，可以通过将所有计算时间映射到一个称为参考系统时间的参考机器上，在任意机器上进行计算。我们讨论了我们提出的协议的局限性，并发布了我们的代码，以鼓励对高效训练的严格研究。

    The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
    
[^20]: HyenaDNA：单核苷酸分辨率下的长范围基因组序列建模

    HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution. (arXiv:2306.15794v1 [cs.LG])

    [http://arxiv.org/abs/2306.15794](http://arxiv.org/abs/2306.15794)

    HyenaDNA是一种基于隐式卷积的基因组序列建模方法，可以在单核苷酸分辨率下对长范围相互作用进行建模。

    

    基因组（DNA）序列编码了大量关于基因调控和蛋白质合成的信息。类似自然语言模型，研究人员提出了基因组的基础模型，从非标记的基因组数据中学习可推广的特征，然后进行下游任务的微调，如识别调控元件。由于注意力的二次扩展，先前基于Transformer的基因组模型仅使用512到4k个标记作为上下文（<0.001%的人类基因组），严重限制了DNA的长范围相互作用建模。此外，这些方法依赖于标记器来聚合有意义的DNA单元，丢失了单核苷酸分辨率，其中微小的遗传变异可以通过单核苷酸多态性（SNP）完全改变蛋白质功能。最近，基于隐式卷积的大型语言模型Hyena显示出能够与注意力质量相匹配，同时允许更长的上下文长度和更低的时间复杂性。利用Hyenas n

    Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyenas n
    
[^21]: 可训练的压缩嵌入层及其在推荐系统上的应用综述

    Review of compressed embedding layers and their applications for recommender systems. (arXiv:2306.13724v1 [cs.LG])

    [http://arxiv.org/abs/2306.13724](http://arxiv.org/abs/2306.13724)

    论文综述了可训练的、压缩的嵌入层在压缩大型神经网络推荐系统中的应用，并提供了相关实验结果。

    

    我们回顾了可训练的、压缩的嵌入层的文献，并讨论了它们在压缩巨型神经推荐系统方面的适用性。我们还报告了使用我们的压缩嵌入层所测得的结果。

    We review the literature on trainable, compressed embedding layers and discuss their applicability for compressing gigantic neural recommender systems. We also report the results we measured with our compressed embedding layers.
    
[^22]: 针对引力波数据中的紧凑二进制合并事件检测，实现深度学习方法的稳健可靠

    Towards a robust and reliable deep learning approach for detection of compact binary mergers in gravitational wave data. (arXiv:2306.11797v1 [gr-qc])

    [http://arxiv.org/abs/2306.11797](http://arxiv.org/abs/2306.11797)

    本篇论文通过开发一个能在保持数据纯度的前提下实现稳健性和可靠性的深度学习模型，提高了搜索速度、参数空间覆盖和搜索灵敏度，而该模型在LIGO数据上测试的表现表明其具有出色的性能。

    

    深度学习（DL）方法通过学习泛化的信号和噪声模型以及在GPU上快速推断，为增强引力波（GW）搜索的速度、参数空间覆盖和搜索灵敏度提供了巨大的希望，但是DL模型的不透明性严重损害了其可靠性。在本研究中，我们逐步开发一个DL模型，并努力改进其稳健性和可靠性。首先，我们解决了保持训练数据纯度的问题，通过导出一种更好地反映数据中“啁啾”信号特征视觉强度的新度量标准。利用通过变分自编码器（VAE）获得的减少、平滑的表示，我们构建了一个分类器来搜索紧凑二进制合并（CBC）信号。我们在真实的LIGO数据上的测试表明了该模型的出色性能。然而，在通过对抗性攻击来探究模型的稳健性时，其简单故障模式则成为了一个痛点。

    The ability of deep learning (DL) approaches to learn generalised signal and noise models, coupled with their fast inference on GPUs, holds great promise for enhancing gravitational-wave (GW) searches in terms of speed, parameter space coverage, and search sensitivity. However, the opaque nature of DL models severely harms their reliability. In this work, we meticulously develop a DL model stage-wise and work towards improving its robustness and reliability. First, we address the problems in maintaining the purity of training data by deriving a new metric that better reflects the visual strength of the "chirp" signal features in the data. Using a reduced, smooth representation obtained through a variational auto-encoder (VAE), we build a classifier to search for compact binary coalescence (CBC) signals. Our tests on real LIGO data show an impressive performance of the model. However, upon probing the robustness of the model through adversarial attacks, its simple failure modes were ide
    
[^23]: 跨国和时间的社会动荡相变

    Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v2 [physics.soc-ph] UPDATED)

    [http://arxiv.org/abs/2306.08698](http://arxiv.org/abs/2306.08698)

    跨国和时间的社会动荡相变研究探索了集体社会动荡是否可以被描述为一系列具有可测量和可识别特征的循环相变，并证明了宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，以及普遍机制可能支撑着社会动荡的某些方面。

    

    相变是复杂系统中突发转变的特征，尽管在物理和自然科学中已经进行了大量研究，但在社会系统中对这一现象的实证研究相对较少。本研究的目标是探索集体社会动荡的动力学是否可以被合理地描述为一系列循环相变，其中每个阶段具有可测量和可识别的潜在特征。我们引入了一个宏观水平的社会动荡统计模型，并使用包括1946年至2017年在内的170个国家的全面数据集来评估其可行性。我们的研究结果表明，这个宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，并且普遍的机制可能潜在地支撑着社会动荡的某些方面。我们还引入了一个新的量表来衡量一个国家的社会动荡程度。

    Phase transitions, characterized by abrupt shifts between macroscopic patterns of organization, are ubiquitous in complex systems. Despite considerable research in the physical and natural sciences, the empirical study of this phenomenon in societal systems is relatively underdeveloped. The goal of this study is to explore whether the dynamics of collective civil unrest can be plausibly characterized as a sequence of recurrent phase shifts, with each phase having measurable and identifiable latent characteristics. We introduce a macro-level statistical model of civil unrest and evaluate its plausibility using a comprehensive dataset of civil unrest events in 170 countries from 1946 to 2017. Our findings demonstrate that the macro-level phase model effectively captures the characteristics of civil unrest data from diverse countries globally and that universal mechanisms may underlie certain aspects of the dynamics of civil unrest. We also introduce a new scale to quantify a country's lo
    
[^24]: 结构化离散表示的深度生成模型的无偏学习

    Unbiased Learning of Deep Generative Models with Structured Discrete Representations. (arXiv:2306.08230v1 [cs.LG])

    [http://arxiv.org/abs/2306.08230](http://arxiv.org/abs/2306.08230)

    该论文提出了一种名为结构化变分自编码器的深度生成模型，它通过图像模型的结构和可解释性以及深度学习的适用于高维数据的灵活似然，结合两种框架的优势。同时，该论文还提出了一种学习SVAE的新算法，与此同时，推导出了一种计算自然梯度的方法，这些优化创新使得SVAE首次能与最先进的时间序列模型进行比较。

    

    通过将图形模型与深度学习架构组合，我们学习具有两种框架优势的生成模型。 结构化变分自编码器（SVAE）从图形模型继承结构和可解释性，从深度学习中继承了适用于高维数据的灵活似然，但是会带来相当大的优化挑战。 我们提出了学习SVAE的新算法，并且首次证明了SVAE在含有缺失数据且包含离散潜变量时处理多模态不确定性的能力。我们的内存高效隐式微分方案使得SVAE可以通过梯度下降来学习，并且证明了鲁棒性。为了更快地学习准确的图形模型参数，我们推导了一种计算自然梯度的方法，而不需要手动进行导出，从而避免了先前工作中发现的偏差。这些优化创新使得首次能够将SVAE与最先进的时间序列模型进行比较。

    By composing graphical models with deep learning architectures, we learn generative models with the strengths of both frameworks. The structured variational autoencoder (SVAE) inherits structure and interpretability from graphical models, and flexible likelihoods for high-dimensional data from deep learning, but poses substantial optimization challenges. We propose novel algorithms for learning SVAEs, and are the first to demonstrate the SVAE's ability to handle multimodal uncertainty when data is missing by incorporating discrete latent variables. Our memory-efficient implicit differentiation scheme makes the SVAE tractable to learn via gradient descent, while demonstrating robustness to incomplete optimization. To more rapidly learn accurate graphical model parameters, we derive a method for computing natural gradients without manual derivations, which avoids biases found in prior work. These optimization innovations enable the first comparisons of the SVAE to state-of-the-art time s
    
[^25]: 通过最终层反演进行单模型归因

    Single-Model Attribution via Final-Layer Inversion. (arXiv:2306.06210v1 [cs.CV])

    [http://arxiv.org/abs/2306.06210](http://arxiv.org/abs/2306.06210)

    本文提出了一种利用最终层反演和异常检测的开放式单模型归因方法，解决了以往方法要么局限于封闭式环境、要么需要对生成模型进行不必要的改变的问题。实验结果表明该方法优于现有方法。

    

    最近关于生成模型方面的开创性发展引起了人们对于实用单模型归因的兴趣。这些方法可以预测一个样本是由特定的生成器生成的还是不是，例如，为了证明知识产权盗窃行为。然而，以前的方法要么局限于封闭式环境，要么需要对生成模型进行不必要的改变。本文提出了FLIPAD，一种基于最终层反演和异常检测的开放式单模型归因方法，以解决这些问题。我们展示利用的最终层反演可以简化为一个凸的 Lasso 优化问题，从而使我们的方法在理论上可靠且计算效率高。理论结果还得到了实验研究的支持，证明本文方法的有效性，优于现有方法。

    Recent groundbreaking developments on generative modeling have sparked interest in practical single-model attribution. Such methods predict whether a sample was generated by a specific generator or not, for instance, to prove intellectual property theft. However, previous works are either limited to the closed-world setting or require undesirable changes of the generative model. We address these shortcomings by proposing FLIPAD, a new approach for single-model attribution in the open-world setting based on final-layer inversion and anomaly detection. We show that the utilized final-layer inversion can be reduced to a convex lasso optimization problem, making our approach theoretically sound and computationally efficient. The theoretical findings are accompanied by an experimental study demonstrating the effectiveness of our approach, outperforming the existing methods.
    
[^26]: 基于SGLD的信息准则与超参数化模型研究

    SGLD-Based Information Criteria and the Over-Parameterized Regime. (arXiv:2306.05583v1 [cs.LG])

    [http://arxiv.org/abs/2306.05583](http://arxiv.org/abs/2306.05583)

    本研究提出针对超参数化学习算法的SGLD信息准则，通过KL信息和KL散度罚项来追踪双下降现象。

    

    “双丘陵”是指过度参数化学习算法在插值阈值之外的意外测试损失下降，这不是由于标准渐进方法的局限性，导致经典形式的信息准则无法预测。我们使用信息风险最小化框架更新这些分析，并为通过随机梯度Langevin动力学（SGLD）学习的模型提供了Akaike信息准则（AIC）和贝叶斯信息准则（BIC）。值得注意的是，SGLD的AIC和BIC罚项对应特定的信息度量，即对称的KL信息和KL散度。我们通过表征大量参数模型的SGLD-BIC扩展了此信息理论分析，其中参数数$p$和样本数$n$趋于无穷大，$p/n$固定。我们的实验表明，改进的SGLD-BIC可以跟踪双下降曲线。

    Double-descent refers to the unexpected drop in test loss of a learning algorithm beyond an interpolating threshold with over-parameterization, which is not predicted by information criteria in their classical forms due to the limitations in the standard asymptotic approach. We update these analyses using the information risk minimization framework and provide Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) for models learned by stochastic gradient Langevin dynamics (SGLD). Notably, the AIC and BIC penalty terms for SGLD correspond to specific information measures, i.e., symmetrized KL information and KL divergence. We extend this information-theoretic analysis to over-parameterized models by characterizing the SGLD-based BIC for the random feature model in the regime where the number of parameters $p$ and the number of samples $n$ tend to infinity, with $p/n$ fixed. Our experiments demonstrate that the refined SGLD-based BIC can track the double-descent cur
    
[^27]: 预训练表示中的扩散冗余

    Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])

    [http://arxiv.org/abs/2306.00183](http://arxiv.org/abs/2306.00183)

    本文发现预训练表示中存在一定程度的扩散冗余，即随机选择一部分神经元与全部神经元相似度高且性能相似，可用于降低实际部署中的计算和内存成本，同时保持相当的性能水平。

    

    在大型数据集上预训练神经网络获得的表示已被越来越多地成功应用于各种下游任务中。在本文中，我们更加深入地研究了这种预训练表示中的特征是如何被编码的。我们发现，在给定层中学到的表示展现出一定程度的扩散冗余，即对于超过一个阈值大小的任何随机子集神经元，都与完整层具有很高的相似度，并且在各种下游任务中能够表现出与整个层相似的性能。我们在各种不同的神经架构（包括CNN和Transformer）上进行了实验，使用了ImageNet1k和ImageNet21k进行预训练，并评估了各种下游任务，如图像分类、目标检测和自然语言处理。我们的实验结果表明，可以利用预训练表示中的冗余来降低在实际部署中使用这些模型的计算和内存成本，同时仍然保持相当的性能水平。

    Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
    
[^28]: 基于因果干预的药物靶点相互作用可信度测量方法的研究与实现

    Research And Implementation Of Drug Target Interaction Confidence Measurement Method Based On Causal Intervention. (arXiv:2306.00041v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.00041](http://arxiv.org/abs/2306.00041)

    本研究提出了一种基于因果干预的置信度测量方法来提高药物靶点相互作用预测模型的准确性，着重解决了知识图谱嵌入模型的不足问题。

    

    药物与靶点相互作用（DTI）的识别和发现是药物研究和开发中的重要步骤，可以帮助科学家发现新药并加速开发过程。近年来，知识图谱和相关的知识图谱嵌入（KGE）模型在药物发现领域迅速发展并表现出良好性能。在药物靶点识别任务中，模型的真实性和准确性不足会导致误判率增加和药物开发效率低下。为解决以上问题，本研究聚焦于以知识映射为核心技术的药物靶点链接预测问题，并采用基于因果干预的置信度测量方法来测量三元组得分，从而提高药物靶点相互作用预测模型的准确性。通过在不同的KGE模型上与传统的Softmax和Sigmod置信度测量方法进行比较，结果表明...

    The identification and discovery of drug-target Interaction (DTI) is an important step in the field of Drug research and development, which can help scientists discover new drugs and accelerate the development process. KnowledgeGraph and the related knowledge graph Embedding (KGE) model develop rapidly and show good performance in the field of drug discovery in recent years. In the task of drug target identification, the lack of authenticity and accuracy of the model will lead to the increase of misjudgment rate and the low efficiency of drug development. To solve the above problems, this study focused on the problem of drug target link prediction with knowledge mapping as the core technology, and adopted the confidence measurement method based on causal intervention to measure the triplet score, so as to improve the accuracy of drug target interaction prediction model. By comparing with the traditional Softmax and Sigmod confidence measurement methods on different KGE models, the resu
    
[^29]: 针对部分观测非线性系统的全收敛和Lipschitz闭环学习策略参数化

    Learning Over All Contracting and Lipschitz Closed-Loops for Partially-Observed Nonlinear Systems. (arXiv:2304.06193v1 [eess.SY])

    [http://arxiv.org/abs/2304.06193](http://arxiv.org/abs/2304.06193)

    本文提出了一种针对非线性部分观测动态系统的学习控制策略参数化，能够自动满足闭环系统的稳定性和用户可调整的鲁棒性条件，在两个强化学习任务的模拟中表现良好且有强鲁棒性。

    

    本文针对非线性部分观测动态系统提出了一种基于 Youla 参数化和最近提出的 REN 模型的学习控制策略参数化。我们证明，该策略参数化自动满足了闭环系统的稳定性（收敛）和用户可调整的鲁棒性（Lipschitz）条件。这意味着它可以用于安全的基于学习的控制，不需要额外的约束或投影来强制稳定性或鲁棒性。我们在两个强化学习任务的模拟中测试了新的策略类：1）磁悬浮，2）倒置旋转臂摆。我们发现 Youla-REN 在确保稳定性和展示对敌对扰动提高的鲁棒性的同时，表现类似于现有的学习控制和最优控制方法。

    This paper presents a policy parameterization for learning-based control on nonlinear, partially-observed dynamical systems. The parameterization is based on a nonlinear version of the Youla parameterization and the recently proposed Recurrent Equilibrium Network (REN) class of models. We prove that the resulting Youla-REN parameterization automatically satisfies stability (contraction) and user-tunable robustness (Lipschitz) conditions on the closed-loop system. This means it can be used for safe learning-based control with no additional constraints or projections required to enforce stability or robustness. We test the new policy class in simulation on two reinforcement learning tasks: 1) magnetic suspension, and 2) inverting a rotary-arm pendulum. We find that the Youla-REN performs similarly to existing learning-based and optimal control methods while also ensuring stability and exhibiting improved robustness to adversarial disturbances.
    
[^30]: DWA：差分小波放大器用于图像超分辨率

    DWA: Differential Wavelet Amplifier for Image Super-Resolution. (arXiv:2304.01994v1 [cs.CV])

    [http://arxiv.org/abs/2304.01994](http://arxiv.org/abs/2304.01994)

    本文介绍了一种基于小波的图像超分辨率模块DWA，通过利用两个卷积滤波器的差异改进小波SR模型，在小波域中提高相关特征提取并抑制噪声。在现有的SR模型中集成DWA，如DWSR和MWCNN，可以显示出其有效性。

    

    本文介绍了一种差分小波放大器(DWA)，这是一种基于小波的图像超分辨率(SR)模块。DWA为最近收到较少关注的混合离散小波变换(DWT)方法注入活力。DWT能够有效地为SR提供图像表示，并将其输入的空间面积减少4倍，从而减小了模型总大小和计算成本，并且成为可持续ML的一种有吸引力的方法。我们提出的DWA模型通过利用两个卷积滤波器之间的差异来改进小波SR模型，在小波域中提高相关特征提取，强调局部对比度并抑制输入信号中的常见噪声。将其集成到现有的SR模型中，如DWSR和MWCNN，可以显示出其有效性，并在SR任务中实现了明显的提高。此外，DWA使DWSR和MWCNN可以直接应用于输入图像空间，因为它省略了DWT表示的通道方式。

    This work introduces Differential Wavelet Amplifier (DWA), a drop-in module for wavelet-based image Super-Resolution (SR). DWA invigorates an approach recently receiving less attention, namely Discrete Wavelet Transformation (DWT). DWT enables an efficient image representation for SR and reduces the spatial area of its input by a factor of 4, the overall model size, and computation cost, framing it as an attractive approach for sustainable ML. Our proposed DWA model improves wavelet-based SR models by leveraging the difference between two convolutional filters to refine relevant feature extraction in the wavelet domain, emphasizing local contrasts and suppressing common noise in the input signals. We show its effectiveness by integrating it into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear improvement in classical SR tasks. Moreover, DWA enables a direct application of DWSR and MWCNN to input image space, reducing the DWT representation channel-wise since it omits 
    
[^31]: 广义划分局部深度

    Generalized partitioned local depth. (arXiv:2303.10167v1 [stat.ML])

    [http://arxiv.org/abs/2303.10167](http://arxiv.org/abs/2303.10167)

    本文提出了一个广义的凝聚概念，构建在分区局部深度的技术基础上，扩展了早期结果并应用于具有不确定性的数据的社区发现中。

    

    本文提供了一个最近由Berenhaut、Moore和Melvin [Proccedings of the National Academy of Sciences, 119 (4) (2022)]提出的凝聚概念的概括。所提出的表述基于分区局部深度的技术并提炼了两个关键概率概念：局部相关性和支持分割。早期结果在新的背景下得到扩展，并包括在具有不确定性的数据中揭示社区的应用示例。

    In this paper we provide a generalization of the concept of cohesion as introduced recently by Berenhaut, Moore and Melvin [Proceedings of the National Academy of Sciences, 119 (4) (2022)]. The formulation presented builds on the technique of partitioned local depth by distilling two key probabilistic concepts: local relevance and support division. Earlier results are extended within the new context, and examples of applications to revealing communities in data with uncertainty are included.
    
[^32]: 面向对抗生成模型的PAC-Bayesian泛化界

    PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08942](http://arxiv.org/abs/2302.08942)

    将PAC-Bayesian理论扩展到生成模型，为基于Wasserstein距离和总变差距离的模型提供了泛化界，为Wasserstein GAN和Energy-Based GAN提供了新的训练目标，并在合成数据集上展示出非虚空泛化界。

    

    我们将PAC-Bayesian理论扩展到生成模型，并为基于Wasserstein距离和总变差距离的模型开发了泛化界。我们第一个关于Wasserstein距离的结果假设实例空间是有界的，而我们的第二个结果利用了降维的优势。我们的结果自然适用于Wasserstein GAN和Energy-Based GAN，而我们的界限为这两种GAN提供了新的训练目标。尽管我们的工作主要是理论性的，但我们进行了数值实验，展示了Wasserstein GAN在合成数据集上的非虚空泛化界。

    We extend PAC-Bayesian theory to generative models and develop generalization bounds for models based on the Wasserstein distance and the total variation distance. Our first result on the Wasserstein distance assumes the instance space is bounded, while our second result takes advantage of dimensionality reduction. Our results naturally apply to Wasserstein GANs and Energy-Based GANs, and our bounds provide new training objectives for these two. Although our work is mainly theoretical, we perform numerical experiments showing non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
    
[^33]: 浅层视觉Transformer的理论理解：学习、泛化和样本复杂性的分析

    A Theoretical Understanding of Shallow Vision Transformers: Learning, Generalization, and Sample Complexity. (arXiv:2302.06015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06015](http://arxiv.org/abs/2302.06015)

    本文提供了第一份对于浅层ViT进行训练的理论分析，证明了使用SGD训练会产生稀疏的注意力图，目前的样本复杂度与标记相关令牌的分数倒数、标记级别的令牌噪声水平和初始模型错误呈正相关关系。

    

    近年来，具有自我注意机制的视觉Transformer（ViTs）在许多视觉任务中取得了巨大的实证成功。然而，由于层间的非凸交互，理论上的学习和泛化分析大多是难以理解的。本文提供了对于一项分类任务，使用一个自我注意层和两层感知机的浅层ViT进行训练的第一篇理论分析，建立了对于数据模型的描述，该模型可以同时表征标记相关和标记不相关的令牌。我们界定了达到零泛化误差的样本复杂性。我们的样本复杂性限制与标记相关令牌的部分倒数、标记级别的令牌噪声水平和初始模型误差呈正相关。我们还证明了使用随机梯度下降SGD（stochastic gradient descent）进行训练过程会导致稀疏的注意力图，这是对于注意力成功的一种形式证明。此外，本文指出，适当的令牌确定是确保实现最优性能的关键。

    Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token spa
    
[^34]: 自洽的概率流速度匹配方法

    Self-Consistent Velocity Matching of Probability Flows. (arXiv:2301.13737v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13737](http://arxiv.org/abs/2301.13737)

    我们提出了一种无网格的框架，用于求解守恒性偏微分方程，包括时间相关的Fokker-Planck方程和Wasserstein梯度流。通过自洽的速度匹配方法和迭代形式，我们的方法绕过了计算障碍，并在高维情况下表现出优越性能。

    

    我们提出了一种无网格的可扩展框架，用于求解一类包括时间相关的Fokker-Planck方程和Wasserstein梯度流在内的守恒性偏微分方程。主要观察是PDE解的时间变化速度场需要是自洽的：它必须满足一个包含相同速度场的概率流的不动点方程。我们使用迭代形式和有偏梯度估计器的方法，绕过了具有强大实证性能的重大计算障碍，而不是直接最小化不动点方程的残差。与现有方法相比，我们的方法不受时间或空间离散化的限制，涵盖了更广泛的PDEs，并且可以在高维情况下进行扩展。实验表明，当解析解可用时，我们的方法可以准确恢复解析解，并在高维情况下取得优越的性能。

    We present a discretization-free scalable framework for solving a large class of mass-conserving partial differential equations (PDEs), including the time-dependent Fokker-Planck equation and the Wasserstein gradient flow. The main observation is that the time-varying velocity field of the PDE solution needs to be self-consistent: it must satisfy a fixed-point equation involving the probability flow characterized by the same velocity field. Instead of directly minimizing the residual of the fixed-point equation with neural parameterization, we use an iterative formulation with a biased gradient estimator that bypasses significant computational obstacles with strong empirical performance. Compared to existing approaches, our method does not suffer from temporal or spatial discretization, covers a wider range of PDEs, and scales to high dimensions. Experimentally, our method recovers analytical solutions accurately when they are available and achieves superior performance in high dimensi
    
[^35]: 关于深度网络和双重下降的利普希茨常数

    On the Lipschitz Constant of Deep Networks and Double Descent. (arXiv:2301.12309v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12309](http://arxiv.org/abs/2301.12309)

    本文通过实验研究发现，深度网络的利普希茨常数趋势与测试误差密切相关，通过建立参数空间和输入空间梯度之间的联系，确定了损失函数曲率和距离初始化参数的距离对于深度网络的优化和模型函数复杂度限制是关键因素，该研究对隐式正则化和网络的有效模型复杂度提供了新的见解。

    

    目前关于深度网络泛化误差的界限都是基于输入变量的平滑或有界依赖性，没有研究探究实践中控制这些因素的机制。本文对经历双重衰减的深度网络的实验利普希茨常数进行了广泛的实验研究，并强调了非单调的趋势，与测试误差密切相关。通过建立随机梯度下降的参数空间和输入空间梯度之间的联系，我们分离出两个重要因素，即损失函数曲率和距离初始化参数的距离，分别控制关键点周围的优化动态，并限制模型函数的复杂度，即使在训练数据之外。我们的研究揭示了超参数化的隐式正则化和实践中网络的有效模型复杂度的新见解。

    Existing bounds on the generalization error of deep networks assume some form of smooth or bounded dependence on the input variable, falling short of investigating the mechanisms controlling such factors in practice. In this work, we present an extensive experimental study of the empirical Lipschitz constant of deep networks undergoing double descent, and highlight non-monotonic trends strongly correlating with the test error. Building a connection between parameter-space and input-space gradients for SGD around a critical point, we isolate two important factors -- namely loss landscape curvature and distance of parameters from initialization -- respectively controlling optimization dynamics around a critical point and bounding model function complexity, even beyond the training data. Our study presents novels insights on implicit regularization via overparameterization, and effective model complexity for networks trained in practice.
    
[^36]: 使用图像合成进行光照变化校正的无监督领域自适应人物再识别

    Illumination Variation Correction Using Image Synthesis For Unsupervised Domain Adaptive Person Re-Identification. (arXiv:2301.09702v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2301.09702](http://arxiv.org/abs/2301.09702)

    本文提出了一个Synthesis Model Bank（SMB）来处理无监督领域自适应人物再识别中的光照变化。SMB包括卷积神经网络（CNN）和马氏距离矩阵，通过使用不同光照条件的合成数据进行训练，增强了光照变化的鲁棒性。

    

    无监督领域自适应（UDA）人物再识别旨在从源领域的标记图像中学习身份信息，并将其应用于目标领域的未标记图像。许多无监督再识别方法的一个主要问题是它们在大的领域变化（如光照、视角和遮挡）方面表现不佳。本文提出了一个合成模型库（SMB）来处理无监督人物再识别中的光照变化。所提出的SMB包括用于特征提取的多个卷积神经网络（CNN）和用于距离度量的马氏距离矩阵。它们使用具有不同光照条件的合成数据进行训练，这使得SMB对光照变化具有鲁棒性。为了更好地量化光照强度并提高合成图像的质量，我们引入了一个基于GAN的新型三维虚拟人类数据集进行图像合成。通过实验，我们证明了所提出方法的有效性。

    Unsupervised domain adaptive (UDA) person re-identification (re-ID) aims to learn identity information from labeled images in source domains and apply it to unlabeled images in a target domain. One major issue with many unsupervised re-identification methods is that they do not perform well relative to large domain variations such as illumination, viewpoint, and occlusions. In this paper, we propose a Synthesis Model Bank (SMB) to deal with illumination variation in unsupervised person re-ID. The proposed SMB consists of several convolutional neural networks (CNN) for feature extraction and Mahalanobis matrices for distance metrics. They are trained using synthetic data with different illumination conditions such that their synergistic effect makes the SMB robust against illumination variation. To better quantify the illumination intensity and improve the quality of synthetic images, we introduce a new 3D virtual-human dataset for GAN-based image synthesis. From our experiments, the pr
    
[^37]: 在线核学习中的改进核对齐遗憾界

    Improved Kernel Alignment Regret Bound for Online Kernel Learning. (arXiv:2212.12989v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.12989](http://arxiv.org/abs/2212.12989)

    本文提出了一种新的算法，在线核学习中的新算法遗憾界和计算复杂度优于以前的结果。该算法的遗憾界和计算复杂度取决于核矩阵特征值的衰减率。

    

    本文针对Hinge损失函数下的在线核学习，改进了核对齐遗憾界。我们提出了一种算法，其遗憾界和计算复杂度优于以前的结果。如果核矩阵的特征值呈指数衰减，则我们的算法在计算复杂度为$O(\ln^2{T})$，遗憾界为$O(\sqrt{\mathcal{A}_T})$。否则，我们的算法在计算复杂度为$O(\sqrt{\mathcal{A}_TT})$，遗憾界为$O((\mathcal{A}_TT)^{\frac{1}{4}})$。我们将算法扩展到批量学习，并获得了$O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$的余量风险界，取得了改进。

    In this paper, we improve the kernel alignment regret bound for online kernel learning in the regime of the Hinge loss function. Previous algorithm achieves a regret of $O((\mathcal{A}_TT\ln{T})^{\frac{1}{4}})$ at a computational complexity (space and per-round time) of $O(\sqrt{\mathcal{A}_TT\ln{T}})$, where $\mathcal{A}_T$ is called \textit{kernel alignment}. We propose an algorithm whose regret bound and computational complexity are better than previous results. Our results depend on the decay rate of eigenvalues of the kernel matrix. If the eigenvalues of the kernel matrix decay exponentially, then our algorithm enjoys a regret of $O(\sqrt{\mathcal{A}_T})$ at a computational complexity of $O(\ln^2{T})$. Otherwise, our algorithm enjoys a regret of $O((\mathcal{A}_TT)^{\frac{1}{4}})$ at a computational complexity of $O(\sqrt{\mathcal{A}_TT})$. We extend our algorithm to batch learning and obtain a $O(\frac{1}{T}\sqrt{\mathbb{E}[\mathcal{A}_T]})$ excess risk bound which improves the p
    
[^38]: HMOE: 基于超网络的专家混合模型用于领域泛化

    HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08253](http://arxiv.org/abs/2211.08253)

    本文提出了一种新的领域泛化方法HMOE，它不需要领域标签，更具可解释性，使用超网络生成专家权重，能够在低维向量空间中探索专家的相似性，实验结果表明HMOE可以划分混合数据并取得更好的效果。

    This paper proposes a novel domain generalization method called HMOE, which does not rely on domain labels and is more interpretable. HMOE uses hypernetworks to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. Experimental results show that HMOE can divide mixed data and achieve better performance.

    由于领域转移，机器学习系统通常无法很好地推广到与训练数据不同的领域，这就是领域泛化（DG）的目的。尽管已经开发了各种各样的DG方法，但大多数缺乏可解释性，并且需要在许多实际场景中不可用的领域标签。本文提出了一种新的DG方法，称为HMOE：基于超网络的专家混合模型（MoE），它不依赖于领域标签，并且更具可解释性。MoE在识别数据中的异质模式方面证明了其有效性。对于DG问题，异质性正是由于领域转移而产生的。HMOE使用超网络将向量作为输入来生成专家权重，这使得专家可以共享有用的元知识，并能够在低维向量空间中探索专家的相似性。我们在公平和统一的基准测试-DomainBed下将HMOE与其他DG算法进行比较。我们的广泛实验表明，HMOE可以划分混合数据并取得更好的效果。

    Due to domain shift, machine learning systems typically fail to generalize well to domains different from those of training data, which is what domain generalization (DG) aims to address. Although various DG methods have been developed, most of them lack interpretability and require domain labels that are not available in many real-world scenarios. This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture of Experts (MoE), which does not rely on domain labels and is more interpretable. MoE proves effective in identifying heterogeneous patterns in data. For the DG problem, heterogeneity arises exactly from domain shift. HMOE uses hypernetworks taking vectors as input to generate experts' weights, which allows experts to share useful meta-knowledge and enables exploring experts' similarities in a low-dimensional vector space. We compare HMOE with other DG algorithms under a fair and unified benchmark-DomainBed. Our extensive experiments show that HMOE can divide mixe
    
[^39]: 鲁棒飞行控制的神经移动视界估计

    Neural Moving Horizon Estimation for Robust Flight Control. (arXiv:2206.10397v10 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2206.10397](http://arxiv.org/abs/2206.10397)

    本文提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整神经网络建模的关键参数，并适应不同的飞行场景。通过推导出与加权矩阵相关的解析梯度，我们实现了将该估计器作为可学习层嵌入神经网络进行高效学习。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    

    估计和应对干扰对于四旋翼飞行控制至关重要。现有的估计器通常需要对特定飞行场景进行大量调整，或者经过广泛的地面真实干扰数据训练，才能实现令人满意的性能。在本文中，我们提出了一种神经移动视界估计器（NeuroMHE），它可以自动调整由神经网络建模的关键参数，并适应不同的飞行场景。我们通过推导与加权矩阵相关的MHE估计的解析梯度，实现了将MHE作为可学习层嵌入神经网络以实现高效学习的无缝融合。有趣的是，我们证明可以使用递归形式的卡尔曼滤波器高效地计算出梯度。此外，我们还开发了一种基于模型的策略梯度算法，以从四旋翼轨迹跟踪误差中直接训练NeuroMHE，而不需要地面真实干扰数据。

    Estimating and reacting to disturbances is crucial for robust flight control of quadrotors. Existing estimators typically require significant tuning for a specific flight scenario or training with extensive ground-truth disturbance data to achieve satisfactory performance. In this paper, we propose a neural moving horizon estimator (NeuroMHE) that can automatically tune the key parameters modeled by a neural network and adapt to different flight scenarios. We achieve this by deriving the analytical gradients of the MHE estimates with respect to the weighting matrices, which enables a seamless embedding of the MHE as a learnable layer into neural networks for highly effective learning. Interestingly, we show that the gradients can be computed efficiently using a Kalman filter in a recursive form. Moreover, we develop a model-based policy gradient algorithm to train NeuroMHE directly from the quadrotor trajectory tracking error without needing the ground-truth disturbance data. The effec
    

