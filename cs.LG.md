# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery.](http://arxiv.org/abs/2401.06762) | 本研究针对现代机器学习模型理解复杂高分辨率卫星或航空图像场景中的长距离背景问题，提出了一个道路分割基准数据集，展示了常用语义分割模型在该任务上的失败。 |
| [^2] | [Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction.](http://arxiv.org/abs/2401.06757) | 本文提出了一个用于行人意图预测的合成数据生成框架ARCANE，并介绍了相应生成的大型多样化数据集PedSynth。同时，还提出了一种高效深度模型PedGNN，用于实时C/NC预测。 |
| [^3] | [Solving the Discretised Multiphase Flow Equations with Interface Capturing on Structured Grids Using Machine Learning Libraries.](http://arxiv.org/abs/2401.06755) | 本文使用AI4PDEs方法对具有界面捕获的离散多相流动方程进行求解，并引入了一种新的基于残差公式的压缩代数体积流方法。 |
| [^4] | [The Unreasonable Effectiveness of Easy Training Data for Hard Tasks.](http://arxiv.org/abs/2401.06751) | 当困难训练数据很难正确标记时，当前的语言模型通常能够相对良好地从易到难的数据泛化，并且即使在关注于困难数据的性能时，收集和训练易数据可能比困难数据更好。 |
| [^5] | [A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models.](http://arxiv.org/abs/2401.06740) | 这份论文介绍了一种用于定价跳跃扩散模型下欧式篮式期权的深度学习方法，采用了隐式-显式最小移动方法以及残差型人工神经网络逼近，并通过稀疏网格高斯-埃尔米特逼近和基于ANN的高维专用求积规则来离散化积分运算符。 |
| [^6] | [Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum.](http://arxiv.org/abs/2401.06738) | 本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。 |
| [^7] | [Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding.](http://arxiv.org/abs/2401.06727) | 这篇论文提出了一种用于属性图数据的深度流形图自编码器，通过同时考虑数据分布和潜在代码的拓扑结构，改善了学习表示的稳定性和质量，在各种下游任务中超过了最先进的基准算法。 |
| [^8] | [Few-Shot Detection of Machine-Generated Text using Style Representations.](http://arxiv.org/abs/2401.06712) | 本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。 |
| [^9] | [Model-Free Approximate Bayesian Learning for Large-Scale Conversion Funnel Optimization.](http://arxiv.org/abs/2401.06710) | 该论文提出了一种解决大规模转化漏斗优化问题的无模型近似贝叶斯学习算法，通过建立转化漏斗模型来捕捉消费者行为，并实现了非常高的准确度。 |
| [^10] | [A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks.](http://arxiv.org/abs/2401.06699) | 本文提出了一种闭合解法，通过最小二乘法来优化全连接前馈神经网络的权重，具有非常高的效率和独立性。 |
| [^11] | [Quantum Machine Learning in the Cognitive Domain: Alzheimer's Disease Study.](http://arxiv.org/abs/2401.06697) | 本文研究了量子机器学习在认知领域中的应用，主要关注阿尔茨海默病(AD)的早期检测。通过分析书写的不同方面，研究人员可以检测出细微的变化，这对于早期AD的检测具有重要意义。 |
| [^12] | [An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models.](http://arxiv.org/abs/2401.06692) | 该论文提出了一个实验设计框架来减少大型语言模型有限标签监督微调的注释成本，并解决了主动学习的计算瓶颈问题。 |
| [^13] | [Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation.](http://arxiv.org/abs/2401.06688) | 本文介绍了一种利用质量估计指标合并机器翻译假设的方法，该方法在大型语言模型和多语言翻译模型上提升了翻译质量。通过使用候选池和QE指标，我们的方法能够生成多样且准确的翻译结果。 |
| [^14] | [Proximal Causal Inference With Text Data.](http://arxiv.org/abs/2401.06687) | 本论文提出了一种使用文本数据进行近因果推断的方法，通过将文本数据分割并使用零样本模型推断出代理变量，然后应用于近邻 g-formula，从而解决了混淆变量完全未观察到的情况。实验结果表明该方法产生了低偏差的估计值。 |
| [^15] | [DQNC2S: DQN-based Cross-stream Crisis event Summarizer.](http://arxiv.org/abs/2401.06683) | 本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。 |
| [^16] | [Neural Networks for Singular Perturbations.](http://arxiv.org/abs/2401.06656) | 本论文探讨了用于奇异摄动的神经网络模型的表达率极限，并在不同类型的神经网络架构中得到了统一的表达率界限。研究表明，基于ReLU、spiking、$\tanh$和sigmoid激活函数的神经网络可以明确地表示“指数边界层解特性”，并能得到改进的鲁棒表达率界限。 |
| [^17] | [Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks.](http://arxiv.org/abs/2401.06654) | 本研究提出了两个视角来解决基于遮挡的解释方法中的矛盾问题，首先通过使用R-OMS得分来衡量可靠性，然后通过解耦像素翻转和遮挡策略来提高结果的一致性。 |
| [^18] | [Block Majorization Minimization with Extrapolation and Application to $\beta$-NMF.](http://arxiv.org/abs/2401.06646) | 本文提出了一种使用外推的块主导极小化方法（BMMe）来解决多凸优化问题，并将其应用于$\beta$-NMF。通过使用独特的自适应更新规则来更新外推参数，该方法在实验中展现出显著的加速效果。 |
| [^19] | [SeizNet: An AI-enabled Implantable Sensor Network System for Seizure Prediction.](http://arxiv.org/abs/2401.06644) | SeizNet是一种通过使用深度学习和植入式传感器网络系统进行癫痫预测的闭环系统，可以提高预测的特异性并保持很高的敏感性。 |
| [^20] | [CCFC: Bridging Federated Clustering and Contrastive Learning.](http://arxiv.org/abs/2401.06634) | 本论文桥接了联邦聚类和对比学习，提出了一种名为CCFC的新联邦聚类方法。通过表示学习，CCFC在某些情况下聚类性能甚至是最佳基准方法的两倍。与最相关的基准方法相比，在最显著的案例中，CCFC的NMI得分提高了0.4155。同时，CCFC还能有效处理联邦场景下的数据分布和质量差异。 |
| [^21] | [Identifying Policy Gradient Subspaces.](http://arxiv.org/abs/2401.06604) | 本文研究了两种深度策略梯度方法在不同模拟基准任务上的评估结果，发现尽管数据分布不断变化，但存在低维且缓慢变化的梯度子空间，这有助于未来更高效的强化学习工作。 |
| [^22] | [Every Node is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering.](http://arxiv.org/abs/2401.06595) | 提出了一种动态融合自监督任务并学习不同节点的权重的方法，通过从不同的SSL任务中提取的特征融合来提高属性图聚类的性能。 |
| [^23] | [Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints.](http://arxiv.org/abs/2401.06588) | 本文研究了具有强延迟约束条件下的连接主义语音识别，在实时推导合成面部唇部运动的同时，亦关注了时间演化模型与转移模型的相互作用。实验结果表明神经网络拓扑结构、语言模型中的时间依赖关系和解码器延迟之间存在强烈相互作用。 |
| [^24] | [Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation.](http://arxiv.org/abs/2401.06583) | 本研究通过使用预训练的变形器模型和映射方法，探索了跨语言文档表示的方法。实验结果表明，通过映射到跨语言领域的变形器技术文档表示（TLDRs），能够有效地实现跨语言的推荐系统。 |
| [^25] | [Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games.](http://arxiv.org/abs/2401.06566) | 本文介绍了最大因果熵逆强化学习（IRL）方法用于均场博弈（MFG）问题，提出了将MFG问题转化为广义纳什均衡问题（GNEP）的新算法。 |
| [^26] | [A General Benchmark Framework is Dynamic Graph Neural Network Need.](http://arxiv.org/abs/2401.06559) | 本文强调了动态图学习的重要性及其在各个领域中的应用，并强调了对捕捉时间动态、演化图结构和下游任务需求的标准化基准框架的需求。缺乏统一的基准框架是当前动态图学习研究的局限之处，建立这样的框架将有助于推动动态图学习技术的进步。 |
| [^27] | [Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks.](http://arxiv.org/abs/2401.06557) | 该论文提出了一种新的方法，称为Treatment-Aware Hyperbolic Representation Learning（TAHyper），用于利用超螺旋空间学习社交网络中隐藏混淆因素的表示，从而改进个体治疗效应的估计。 |
| [^28] | [Optimizing Feature Selection for Binary Classification with Noisy Labels: A Genetic Algorithm Approach.](http://arxiv.org/abs/2401.06546) | 本研究提出了一种基于遗传算法的特征选择方法NMFS-GA，用于带有噪声标签的二分类中选择最优特征子集。在合成数据集和真实数据集的实验中，我们证明了NMFS-GA能够有效地提高二分类器准确性和可解释性。 |
| [^29] | [Intelligent Data-Driven Architectural Features Orchestration for Network Slicing.](http://arxiv.org/abs/2401.06538) | 本论文讨论了基于机器学习的网络切片架构中特征和能力编排的问题和挑战，提出了使用机器学习嵌入的代理进行智能的资源和功能编排的建议。 |
| [^30] | [Domain Adaptation for Time series Transformers using One-step fine-tuning.](http://arxiv.org/abs/2401.06524) | 本文介绍了一种在时间序列Transformer模型中使用一步微调的领域适应方法。通过在源领域上预训练模型，并在目标领域上进行微调，我们解决了时间理解不足、泛化挑战和数据偏移问题。此外，通过添加源领域数据到目标领域，我们提高了模型的鲁棒性，解决了遗忘灾难的问题。 |
| [^31] | [Boosting Causal Additive Models.](http://arxiv.org/abs/2401.06523) | 我们提出了一种基于提升方法的学习加法结构方程模型的方法，通过引入一类评分函数和逐分量梯度下降的策略来确定变量间的因果顺序，实验证明其在高维数据集中具有竞争力和鲁棒性。 |
| [^32] | [Personalized Reinforcement Learning with a Budget of Policies.](http://arxiv.org/abs/2401.06514) | 该论文提出了一种称为表示的马尔可夫决策过程（r-MDPs）的框架，通过与一小组代表性策略的交互，实现个性化强化学习，并同时在满足监管约束的情况下优化整体社会福利。 |
| [^33] | [ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study.](http://arxiv.org/abs/2401.06513) | ML-On-Rails是一个旨在保护机器学习模型的协议，在软件系统中解决了安全性、可靠性和透明度等挑战，并在生产环境中提高了模型的鲁棒性。通过一项实际案例研究，我们强调了保护ML模型在生产中的重要性。 |
| [^34] | [Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner UNet.](http://arxiv.org/abs/2401.06499) | 本研究评估了使用多平面UNet方法在儿童脑肿瘤中自动分割不同肿瘤亚区的有效性，结果表明对于肿瘤核心类有较高的分割准确性，但在其他类别的分割上存在可变性。 |
| [^35] | [Temporal and Between-Group Variability in College Dropout Prediction.](http://arxiv.org/abs/2401.06498) | 这项研究对大学辍学预测的时间和群组间变异性进行了系统评估，在大规模行政数据的基础上，发现第二年末的辍学预测比入学时更准确，并且大学表现和入学行为在预测中扮演重要角色。对于不同学生群体，大学平均绩点对于t学生的预测价值更高。 |
| [^36] | [An investigation of structures responsible for gender bias in BERT and DistilBERT.](http://arxiv.org/abs/2401.06495) | 本文研究了BERT和DistilBERT中负责性别偏见的结构，讨论了它们在预测中的公正性问题。 |
| [^37] | [Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning.](http://arxiv.org/abs/2401.06469) | 本文提出了批处理ICL方法，通过将ICL视为一个元优化过程，开发出了一个有效、高效且无序的推理算法。通过聚合元梯度并将其应用于零-shot学习，该方法使LLM对ICL示例顺序无关，并且在实验证明其在大多数情况下优于其他排列方式，甚至超过了标准ICL的最佳顺序的性能。 |
| [^38] | [Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test.](http://arxiv.org/abs/2401.06465) | 这项研究对模型参数随机化测试进行了探索和修复，通过引入平滑 MPRT 和高效 MPRT 两种改进方法，解决了实证解释的方法论注意事项，并通过实验结果证明这些改进方法可以提高度量可靠性，从而更可信地应用可解释人工智能方法。 |
| [^39] | [Automated Machine Learning for Positive-Unlabelled Learning.](http://arxiv.org/abs/2401.06452) | 本论文提出了两个新的用于PU学习的Auto-ML系统，并对这三个Auto-ML系统进行了全面评估和比较，为PU学习任务的最优方法选择提供了解决方案。 |
| [^40] | [Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation.](http://arxiv.org/abs/2401.06436) | 本文提出了一种在社交网络中预测评分的方法，该方法通过在GCN模型中引入Transformer层，在节点嵌入方面取得了更好的性能。 |
| [^41] | [Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models.](http://arxiv.org/abs/2401.06432) | 本文提出了一种用于异构设备的设备本地基础模型联邦微调的参数高效方法，使用了异构低秩近似（LoRA），解决了资源受限和异构设备的挑战。 |
| [^42] | [Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction.](http://arxiv.org/abs/2401.06421) | 本论文介绍了地球观测领域中使用符合预测进行不确定性量化的方法。与其他方法不同，符合预测不需要访问底层模型和训练数据集，并同时提供统计上有效和有信息的预测区域，同时保持计算效率。 |
| [^43] | [Mission: Impossible Language Models.](http://arxiv.org/abs/2401.06416) | 本文为了支持大型语言模型(LLMs)能够学习不可能的语言的观点，开发了一组人工合成的不可能语言，并通过评估GPT-2小型模型的学习能力得出了结论。 |
| [^44] | [Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review.](http://arxiv.org/abs/2401.06406) | 知识驱动的机器学习在癌症诊断和预后中的应用已经取得了一定的进展。该方法将生物医学知识与数据驱动模型相结合，能够提高模型结果的准确性、鲁棒性和可解释性。这篇综述回顾了最新研究，并强调了四个主要特点。 |
| [^45] | [Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching.](http://arxiv.org/abs/2401.06362) | 我们提出了一种基于表格化的方法，通过将注意力模型转换为层次结构的表格查找，显著降低了预取模型的复杂度和推理延迟，同时保持了高准确性。通过我们的方法，我们开发了一个DART预取模型，在减少计算量的情况下只有轻微的性能下降。 |
| [^46] | [An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation.](http://arxiv.org/abs/2401.06356) | 本文对知识蒸馏中参数选择的影响进行了大规模实证研究，揭示了不同选项对学生性能的整体影响，并找到了在各方面表现良好的单一配置。 |
| [^47] | [Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo.](http://arxiv.org/abs/2401.06325) | 本文提出了一种更高效的基于扩散的蒙特卡罗采样算法RS-DMC，通过改进评分估计方法来解决原始DMC算法的梯度复杂性问题。该算法将整个扩散过程划分为多个段落，并使用递归评分估计实现更快的采样速度。 |
| [^48] | [Striking a Balance in Fairness for Dynamic Systems Through Reinforcement Learning.](http://arxiv.org/abs/2401.06318) | 本文研究了在决策模型操作的动态人口中的公平性问题，并提出了一种通过强化学习结合各种公平考虑的算法框架。通过预处理和处理中的方法，我们的方法能够在传统公平性、长期公平性和效用之间取得平衡。 |
| [^49] | [A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications.](http://arxiv.org/abs/2401.06308) | 本文提出了一种语义感知多址访问方案，旨在优化资源利用与公平性的权衡，并考虑用户数据的相关性，以满足未来6G应用的要求和特性。 |
| [^50] | [Advantage of Quantum Neural Networks as Quantum Information Decoders.](http://arxiv.org/abs/2401.06300) | 本论文研究了量子神经网络（QNN）作为解码器在解码量子信息时的优势。实验证明，QNN解码器在读取错误方面几乎具有二次改进。这使得在解码实际的量子纠错码时可以探索更广泛的非稳定器码。 |
| [^51] | [Demystifying Variational Diffusion Models.](http://arxiv.org/abs/2401.06281) | 该论文通过使用有向图模型和变分贝叶斯原理，揭示了变分扩散模型的原理和连接，为非专业统计物理领域的读者提供了更容易理解的介绍。 |
| [^52] | [Sampling and Uniqueness Sets in Graphon Signal Processing.](http://arxiv.org/abs/2401.06279) | 本论文研究了大规模图谱族上的采样集特性，并将“可移除集合”和“唯一性集合”的概念推广到了图谱信号处理中。通过利用图谱表示法，可以比较具有不同节点数和边数以及不同节点标记的图谱之间的采样集，并证明了具有相同图谱表示的采样集序列的收敛性。 |
| [^53] | [A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy.](http://arxiv.org/abs/2401.06278) | 本研究研究了自我监督预训练在胃肠内镜视觉问题中的应用，结果发现相对于有监督预训练，自我监督预训练通常能够产生更适合的骨干网络，并且使用ImageNet-1k进行自我监督预训练通常比使用Hyperkvasir-unlabelled更合适。 |
| [^54] | [Qrlew: Rewriting SQL into Differentially Private SQL.](http://arxiv.org/abs/2401.06273) | Qrlew是一个开源库，能够将SQL查询重写为具有差分隐私等效性的查询，从而在保持数据所有者隐私的同时实现标准数据查询和执行。 |
| [^55] | [FedTabDiff: Federated Learning of Diffusion Probabilistic Models for Synthetic Mixed-Type Tabular Data Generation.](http://arxiv.org/abs/2401.06263) | 本文介绍了一种名为"FedTabDiff"的联邦学习方法，用于生成高保真度的混合类型表格数据，而无需集中访问原始表格数据集。该方法利用了"去噪扩散概率模型"（DDPMs）的优势，解决了表格数据中的混合属性类型和隐含关系等固有复杂性，并实现了分散学习方案来保护数据隐私和本地性。在真实的金融数据集上进行了实验评估。 |
| [^56] | [AGSPNet: A framework for parcel-scale crop fine-grained semantic change detection from UAV high-resolution imagery with agricultural geographic scene constraints.](http://arxiv.org/abs/2401.06252) | AGSPNet是一种基于农业地理场景约束的作物语义变化检测框架，能够实时准确地检测细粒度作物栽培的变化信息，并满足农业实际工程应用的需求。 |
| [^57] | [Semantic-Preserving Feature Partitioning for Multi-View Ensemble Learning.](http://arxiv.org/abs/2401.06251) | 本研究提出了一种保持语义一致的特征分区算法（SPFP），用于多视图集成学习（MEL）。通过将数据集有效分区为多个语义一致的视图，SPFP算法显著提高了MEL过程的效果。在各种现实数据集上的实验中，该方法展示了显著的有效性，并在高泛化性能可达的情况下提高了模型的准确性和不确定性度量。 |
| [^58] | [WISE: full-Waveform variational Inference via Subsurface Extensions.](http://arxiv.org/abs/2401.06230) | WISE是一种通过地下扩展进行全波形变分推断的方法，能够准确量化偏移速度模型对成像的影响，并且不依赖于准确的初始速度模型。 |
| [^59] | [Leveraging Frequency Domain Learning in 3D Vessel Segmentation.](http://arxiv.org/abs/2401.06224) | 本研究利用频域学习为3D血管分割模型提供替代多尺度卷积核的方案，减少了计算开销，同时保留了全局感受野。还设计了一个零参数频域融合方法，用于改进U-Net中的跳跃连接。 |
| [^60] | [Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis.](http://arxiv.org/abs/2401.06210) | 这篇论文研究了学习无监督的语义文档表示以进行细粒度的基于方面的情感分析。通过克服现有方法的困难，实验证明该模型在各个任务上的性能优于最先进方法。 |
| [^61] | [An Exploratory Assessment of LLM's Potential Toward Flight Trajectory Reconstruction Analysis.](http://arxiv.org/abs/2401.06204) | 本研究探索了大型语言模型（LLMs）在航空领域中重建飞行轨迹的潜力，并通过使用ADS-B数据对LLaMA 2模型进行实证研究，发现LLMs在过滤噪音和估计飞行轨迹方面表现出色。然而，研究也揭示了处理较长数据序列的挑战，该挑战可能源于LLM模型的标记长度限制。这些研究结果强调了LLMs在航空和交通领域广泛应用的潜力。 |
| [^62] | [Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source Separators.](http://arxiv.org/abs/2401.06203) | 本文介绍了一个使用精调的源分离器合奏混音音乐以改善助听器音质的系统，该系统在Cadenza ICASSP 2024大挑战中名列第一，并在评估数据集上获得了最佳的助听器音质指数（HAAQI）平均分数。 |
| [^63] | [xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein.](http://arxiv.org/abs/2401.06199) | xTrimoPGLM是一个统一的100亿规模预训练蛋白质语言模型，能够同时处理蛋白质理解和生成任务，通过创新的预训练框架和大规模的参数训练，显著优于其他先进模型，在18个蛋白理解基准测试中取得了成功，并能够实现对蛋白质结构的原子分辨率观察。 |
| [^64] | [NeuSpin: Design of a Reliable Edge Neuromorphic System Based on Spintronics for Green AI.](http://arxiv.org/abs/2401.06195) | NeuSpin是一个全栈硬件和软件共同设计的项目，旨在解决使用自旋电子学技术在边缘进行绿色人工智能实现的挑战。通过计算内存一体化，NeuSPIN可以实现超低功耗、高处理能力以及可靠性的要求，从而促进物联网和智能可穿戴设备的发展。 |
| [^65] | [CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification.](http://arxiv.org/abs/2401.06194) | CrisisKAN是一种知识注入和可解释的多模态注意力网络，用于危机事件分类。它通过结合图像、文本和维基百科的外部知识来弥合图像和文本模态之间的语义差距，并解释模型的结果，以建立在高风险情况下的信任。 |
| [^66] | [Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks.](http://arxiv.org/abs/2401.06187) | Scissorhands 是一种新的机器取消学习方法，通过连接敏感性识别与遗忘数据相关的最相关参数，并通过重新训练修剪的模型来擦除数据影响。 |
| [^67] | [End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2.](http://arxiv.org/abs/2401.06183) | 本论文提出了一个端到端的语音转换框架，用于印地语到英语的转换，采用了Bark、mBART和经过微调的XLSR Wav2Vec2等先进技术，为跨语言交流提供了统一而无缝的解决方案。 |
| [^68] | [Prediction of Cellular Identities from Trajectory and Cell Fate Information.](http://arxiv.org/abs/2401.06182) | 本研究提出了一种创新的方法，利用机器学习从早期C. elegans胚胎的图像序列中预测细胞身份。通过使用少量的细胞轨迹和细胞命运信息，我们的模型在有限数据条件下达到了超过90%的分类准确率。 |
| [^69] | [Decentralized Gossip Mutual Learning (GML) for automatic head and neck tumor segmentation.](http://arxiv.org/abs/2401.06180) | GML是一种使用传闻协议进行直接点对点通信的分散协作学习框架，在医疗图像分割任务中取得了较好的效果，避免了传统联邦学习中中心服务器的故障和局部数据特征变化的问题。 |
| [^70] | [CNN-DRL for Scalable Actions in Finance.](http://arxiv.org/abs/2401.06179) | 本文提出了一种用于金融领域可扩展行为的CNN-DRL方法，通过将过去九十天的每日特征向量数据拼接到CNN输入矩阵中，实现了对环境动态的有效学习，获得了奖励的增加。 |
| [^71] | [GOODAT: Towards Test-time Graph Out-of-Distribution Detection.](http://arxiv.org/abs/2401.06176) | GOODAT是一种用于检测测试时图形领域外样本的方法，具有数据驱动、无监督和即插即用的特点。 |
| [^72] | [MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly Detection.](http://arxiv.org/abs/2401.06175) | 这篇论文提出了一套综合的多元时间序列异常检测工具和基准，解决了现有方法缺乏严格比较和重新实现困难的问题。 |
| [^73] | [Machine Learning Applications in Spine Biomechanics.](http://arxiv.org/abs/2401.06174) | 本研究介绍了脊柱生物力学中机器学习应用的框架，使得从单个摄像头图像中可以全面分析复杂活动中的脊柱生物力学。同时评估了其在不同应用中的性能和限制，包括工作场所提重物的评估、车祸中的翻颈伤评估和专业体育中的生物力学分析。 |
| [^74] | [Tree Search-Based Evolutionary Bandits for Protein Sequence Optimization.](http://arxiv.org/abs/2401.06173) | 本研究提出了一种基于树搜索的赌博机学习方法，用于高效探索和优化蛋白质序列。通过在初始序列上扩展树结构，并结合局部搜索和赌博机学习模型，可以有效地发现接近最优设计。 |
| [^75] | [CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods.](http://arxiv.org/abs/2401.06172) | 该论文使用机器学习方法预测股市危机事件，主要针对美国市场。论文比较了随机森林和极限梯度提升模型的性能，发现这两种模型可以有效预测股市危机。 |
| [^76] | [Deep Learning model predicts the c-Kit-11 mutational status of canine cutaneous mast cell tumors by HE stained histological slides.](http://arxiv.org/abs/2401.06169) | 本研究通过HE染色组织切片训练了深度学习模型，仅通过形态学分析就可以准确识别犬皮肤肥大细胞瘤的c-Kit-11突变状态，准确率为87％。 |
| [^77] | [Adjustable Molecular Representation for Unified Pre-training Strategy.](http://arxiv.org/abs/2401.06166) | AdaMR是一种可调整粒度的分子模型，它在原子和亚结构水平上学习分子表示。通过预训练和分子规范化任务，AdaMR可以改善对多个下游任务的效果，包括模型属性预测和分子生成。 |
| [^78] | [Multimodal Gen-AI for Fundamental Investment Research.](http://arxiv.org/abs/2401.06164) | 本论文介绍了一项在金融投资领域的变革性举措，旨在利用多模态Gen-AI技术自动化信息摘要和投资点生成，通过精细调整语言模型以实现特定的应用目标，并最终开发一个AI代理原型，使人类投资者摆脱重复性任务，将注意力集中在高层次的战略思维上。 |
| [^79] | [FRED: Towards a Full Rotation-Equivariance in Aerial Image Object Detection.](http://arxiv.org/abs/2401.06159) | FRED是一个实现航空图像目标检测中完全旋转等变性的模型，通过解耦不变任务和等变任务实现端到端的等变性，将边界框表示为旋转等变向量。 |
| [^80] | [A Stochastic Approach to Classification Error Estimates in Convolutional Neural Networks.](http://arxiv.org/abs/2401.06156) | 本研究针对用于安全关键应用的图像分类的训练卷积神经网络进行了验证研究，证明了系统可以达到认证要求，并对障碍物检测功能的系统级危险率进行了定量分析。 |
| [^81] | [De novo Drug Design using Reinforcement Learning with Multiple GPT Agents.](http://arxiv.org/abs/2401.06155) | 这项研究提出了一种使用多个GPT代理的强化学习算法MolRL-MGPT来进行de novo药物设计，在药物分子生成过程中促进了多样性，展现了在设计抗SARS-CoV-2蛋白靶点的抑制剂方面的有效性。 |
| [^82] | [Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein Complexes with SE(3)-Discrete Diffusion.](http://arxiv.org/abs/2401.06151) | MMDiff是一个联合生成核酸和蛋白质复合物序列和结构的生成模型，具有重要的大分子设计应用价值，并通过验证实例展示了其有效性。 |
| [^83] | [D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation.](http://arxiv.org/abs/2401.06150) | D-STGCNT是一种新的模型，结合了STGCN和transformer的架构，用于自动评估患者身体康复锻炼。它通过将骨架数据视为图形，并检测关键关节，在处理时空数据方面具有高效性。该模型通过密集连接和GRU机制来处理大型3D骨架输入，有效建立时空动态模型。transformer的注意力机制对于评估康复锻炼非常有用。 |
| [^84] | [Image Classifier Based Generative Method for Planar Antenna Design.](http://arxiv.org/abs/2401.06149) | 提出了一种基于图像分类器的平面天线设计生成方法，通过两个步骤确定几何尺寸和位置，并利用随机采样统计和卷积神经网络分类器来设计天线。这种方法不需要经验，能够产生逼真的设计，并且性能指标不亚于由经验丰富的工程师设计的天线。 |
| [^85] | [Minuet: Accelerating 3D Sparse Convolutions on GPUs.](http://arxiv.org/abs/2401.06145) | Minuet是一种专为现代GPU设计的高效内存SC引擎，通过使用新的分段排序二分查找算法和轻量级方案来加速3D稀疏卷积。 |
| [^86] | [DFU: scale-robust diffusion model for zero-shot super-resolution image generation.](http://arxiv.org/abs/2401.06144) | DFU是一种尺度鲁棒的扩散模型，可以实现零样本超分辨率图像生成，并通过在多个分辨率上训练来提高模型的可扩展性。 |
| [^87] | [StockFormer: A Swing Trading Strategy Based on STL Decomposition and Self-Attention Networks.](http://arxiv.org/abs/2401.06139) | StockFormer是一种基于STL分解和自注意力网络的摆动交易策略，使用TopKDropout方法来提高股票选取能力。在测试中，其预测结果优于其他十个行业模型，达到了62.39%的市场趋势检测准确率，并取得了在关键预测准确度指标上的优秀表现。在回测中，StockFormer的摆动交易策略累计收益率为13.19%。 |
| [^88] | [QuasiNet: a neural network with trainable product layers.](http://arxiv.org/abs/2401.06137) | QuasiNet是一种新的神经网络模型，通过可训练的乘积层解决了小规模隐藏神经元下传统神经网络在难问题上的有限收敛问题，具有更高的成功率。 |
| [^89] | [A Distributed Neural Linear Thompson Sampling Framework to Achieve URLLC in Industrial IoT.](http://arxiv.org/abs/2401.06135) | 本文提出了一种名为DISNETS的分布式神经线性Thompson采样框架，通过融合两种方法的优点，通过强化学习和利用来自gNB的反馈信号，UE可以自主选择传输资源，并进行训练。 |
| [^90] | [Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models.](http://arxiv.org/abs/2401.06102) | 本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。 |
| [^91] | [A tree-based varying coefficient model.](http://arxiv.org/abs/2401.05982) | 本论文介绍了一种基于树的可变系数模型，使用循环梯度提升机进行建模，实现了逐维早停和特征重要性评分，该模型能够产生与基于神经网络的VCM相当的结果。 |
| [^92] | [Segment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition.](http://arxiv.org/abs/2401.05717) | 本文研究了使用连接主义音素识别器的类熵来预测音素类之间的时间边界，并比较了不同方法的精确度和召回率。 |
| [^93] | [Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training.](http://arxiv.org/abs/2401.05566) | 该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。 |
| [^94] | [Functional Graphical Models: Structure Enables Offline Data-Driven Optimization.](http://arxiv.org/abs/2401.05442) | 功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。 |
| [^95] | [Representation Learning for Wearable-Based Applications in the Case of Missing Data.](http://arxiv.org/abs/2401.05437) | 本论文研究了可穿戴应用中表示学习的问题，特别是在缺失数据情况下。作者通过比较Transformer模型和统计方法的性能，发现Transformer模型在变化频繁的信号的缺失数据填充方面表现优秀。此研究为基于掩码的自监督学习任务的设计和开发提供了洞察。 |
| [^96] | [On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors.](http://arxiv.org/abs/2401.05414) | 本文从因果性的角度系统研究了金融领域中的三个困境：时间分辨率不匹配、非平稳性和未知因果因素，并提出了解决方案。 |
| [^97] | [Generalizable Sleep Staging via Multi-level Domain Alignment.](http://arxiv.org/abs/2401.05363) | 本文提出了一种通用的睡眠分期方法，通过引入域泛化概念，结合多级特征对齐的思想，提高了模型对未见过数据集的泛化能力。 |
| [^98] | [RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation.](http://arxiv.org/abs/2401.04679) | RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。 |
| [^99] | [Robust Physics Informed Neural Networks.](http://arxiv.org/abs/2401.02300) | 引入了一种鲁棒的物理信息神经网络（RPINNs）来近似偏微分方程（PDE）的解，该网络在训练过程中考虑了PDE的控制物理法则，解决了传统PINNs中损失函数与真实误差不鲁棒的问题。 |
| [^100] | [Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation.](http://arxiv.org/abs/2401.00280) | 本研究探索了如何利用编码器模型和解码器模型来理解和总结网络攻击过程中的策略和目的，使用检索增强生成技术来提取相关上下文，并解决了现有模型在网络安全领域中产生错误信息的问题。 |
| [^101] | [A Comprehensive Survey of Evaluation Techniques for Recommendation Systems.](http://arxiv.org/abs/2312.16015) | 本文介绍了一套综合的推荐系统评估指标，包括相似性指标、候选生成指标、预测指标、排序指标和业务指标。 |
| [^102] | [NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes.](http://arxiv.org/abs/2312.14890) | NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。 |
| [^103] | [Deep Neural Networks and Finite Elements of Any Order on Arbitrary Dimensions.](http://arxiv.org/abs/2312.14276) | 本研究表明，使用ReLU和ReLU^2激活函数的深度神经网络可以有效地表示任意维度上的Lagrange有限元函数，并且能够在特定或任意单纯形网格上生成一般连续分段多项式函数。 |
| [^104] | [Accelerating the Global Aggregation of Local Explanations.](http://arxiv.org/abs/2312.07991) | 我们提出了加速Anchor算法的全局汇总技术，旨在计算出对模型影响最大的前k个单词。 |
| [^105] | [Generative Network Layer for Communication Systems with Artificial Intelligence.](http://arxiv.org/abs/2312.05398) | 这个论文提出了一种使用生成式人工智能在中间或边缘网络节点上的生成网络层，通过将潜在表示压缩后的提示生成图像，可以显著改善网络中所需的数据速率。 |
| [^106] | [Communication-Efficient Federated Optimization over Semi-Decentralized Networks.](http://arxiv.org/abs/2311.18787) | 本文提出了一种基于半分散网络的通信高效算法PISCO, 通过概率性的代理间和代理与服务器之间的通信，实现了通信效率与收敛速度的折衷。 |
| [^107] | [Grounding Foundation Models through Federated Transfer Learning: A General Framework.](http://arxiv.org/abs/2311.17431) | 本论文提出了一个通用框架，通过联邦迁移学习将基础模型接地，以解决面临的挑战，如受限的计算资源、数据隐私、模型异构性和模型所有权。这个框架可以帮助释放基础模型的潜力，并在不同行业中产生重要影响。 |
| [^108] | [Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation.](http://arxiv.org/abs/2311.08640) | 将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。 |
| [^109] | [When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers through Membership Inference Attacks.](http://arxiv.org/abs/2311.03865) | 本研究探索了公平二分类器中的隐私威胁，并揭示了针对公平增强模型的基于分数的成员推断攻击的无效性。同时，公平性方法可能导致训练数据中大多数子群体的预测性能下降。 |
| [^110] | [On the Generalization Properties of Diffusion Models.](http://arxiv.org/abs/2311.01797) | 本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。 |
| [^111] | [Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs.](http://arxiv.org/abs/2310.18152) | 本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。 |
| [^112] | [Stable Nonconvex-Nonconcave Training via Linear Interpolation.](http://arxiv.org/abs/2310.13459) | 本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。 |
| [^113] | [ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification.](http://arxiv.org/abs/2310.08036) | ZEST是一种基于注意力机制的零样本学习框架，用于分类未见过的物联网设备。通过使用自注意力机制提取特征并利用生成模型生成伪数据，ZEST能够在性能上取得显著的改进。 |
| [^114] | [Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications.](http://arxiv.org/abs/2310.07987) | 该论文提出了一种面向6G合作通信的新中继框架，通过提取传输语义特征减少转发负载，并提高网络的鲁棒性。设计了一种联合源信道编码算法，通过迭代交换外在信息来增强解码增益。仿真结果表明，即使在恶劣的信道条件下，该中继框架仍然可以有效地改善恢复的信息质量。 |
| [^115] | [A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions.](http://arxiv.org/abs/2310.00177) | 我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。 |
| [^116] | [FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning.](http://arxiv.org/abs/2309.14062) | 本文针对免去样本的增量式学习（CIL）中的异质性类别分布问题，使用原型网络和改进的各向异性马哈拉诺比斯距离进行特征分类和建模，有效解决了非恒定数据学习中的特征分布异质性挑战。 |
| [^117] | [TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models.](http://arxiv.org/abs/2309.04027) | 本文介绍了TIDE（Textual Identity Detection）方法来改善分类器和语言模型中的文本公平性。通过创建一个包含身份词汇和语境的数据集，以及开发一个身份注释和增强工具，可以提高机器学习公平性技术的效果。 |
| [^118] | [Pure Exploration under Mediators' Feedback.](http://arxiv.org/abs/2308.15552) | 本研究提出了一种严格推广的传统最优臂识别问题，即中介反馈下的最优臂识别（BAI-MF），通过引入中介者来模拟一些实际决策问题，如离线学习、部分可控环境和人类反馈。 |
| [^119] | [SE(3) Equivariant Augmented Coupling Flows.](http://arxiv.org/abs/2308.10364) | 本文提出了一种SE(3)等变增强耦合流，可以快速采样和密度评估，通过在坐标分割中保持等变性。 |
| [^120] | [Nonlinear Meta-Learning Can Guarantee Faster Rates.](http://arxiv.org/abs/2307.10870) | 非线性元学习可以保证更快的收敛速度。 |
| [^121] | [Self-consistency for open-ended generations.](http://arxiv.org/abs/2307.06857) | 本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。 |
| [^122] | [Diffusion Based Multi-Agent Adversarial Tracking.](http://arxiv.org/abs/2307.06244) | 本文介绍了CADENCE，一种基于扩散的多智能体对抗追踪方法，通过利用过去的稀疏状态信息生成全面的对手位置预测，并通过蒙特卡洛采样评估了其有效性。 |
| [^123] | [Improving Language Plasticity via Pretraining with Active Forgetting.](http://arxiv.org/abs/2307.01163) | 本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。 |
| [^124] | [milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing.](http://arxiv.org/abs/2306.17010) | milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。 |
| [^125] | [Asynchronous Algorithmic Alignment with Cocycles.](http://arxiv.org/abs/2306.15632) | 该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。 |
| [^126] | [Enhancing variational quantum state diagonalization using reinforcement learning techniques.](http://arxiv.org/abs/2306.11086) | 本研究采用强化学习技术，通过新的编码方法来优化量子状态对角化所需的电路深度，从而提高其在近期量子硬件上的应用性能。 |
| [^127] | [LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning.](http://arxiv.org/abs/2306.09910) | 本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。 |
| [^128] | [Design Principles for Generalization and Scalability of AI in Communication Systems.](http://arxiv.org/abs/2306.06251) | 本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建具备通用性的AI算法，通过少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。 |
| [^129] | [Probabilistic Computation with Emerging Covariance: Towards Efficient Uncertainty Quantification.](http://arxiv.org/abs/2305.19265) | 本文开发了一个高效、可解释的概率计算框架，通过监督平均值优化任务目标，从非线性耦合中自发出现的无监督协方差忠实地捕捉了与模型预测的不确定性相关的信息。 |
| [^130] | [A Unified Approach for Maximizing Continuous DR-submodular Functions.](http://arxiv.org/abs/2305.16671) | 本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。 |
| [^131] | [Bridging RL Theory and Practice with the Effective Horizon.](http://arxiv.org/abs/2304.09853) | 本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。 |
| [^132] | [Pointwise convergence theorem of gradient descent in sparse deep neural network.](http://arxiv.org/abs/2304.08172) | 本文研究了稀疏深度神经网络中梯度下降的点对点收敛定理，针对非光滑指示函数构造了一种特殊形状的DNN，实现了梯度下降过程的点对点收敛。 |
| [^133] | [A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation.](http://arxiv.org/abs/2304.07772) | 本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。 |
| [^134] | [Improving Few-Shot Prompts with Relevant Static Analysis Products.](http://arxiv.org/abs/2304.06815) | 本文研究了用相关静态分析产品改善大型语言模型在少样本提示中的表现，探讨如何通过添加显示信息来提取代码中的语义事实。 |
| [^135] | [OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems.](http://arxiv.org/abs/2304.06686) | 本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。 |
| [^136] | [Non-Asymptotic Lower Bounds For Training Data Reconstruction.](http://arxiv.org/abs/2303.16372) | 本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析 |
| [^137] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^138] | [Product Jacobi-Theta Boltzmann machines with score matching.](http://arxiv.org/abs/2303.05910) | 本文介绍了一种使用Score matching的乘积Jacobi-Theta Boltzmann机器（pJTBM），它比原始的RTBM更高效地拟合概率密度。 |
| [^139] | [Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability.](http://arxiv.org/abs/2212.07946) | 本论文提出了一种能够在部分可观测的连续状态和动作空间下进行统一推理的框架，通过最小化期望自由能函数指导代理选择动作，以实现最大化奖励的决策制定。 |
| [^140] | [Controlling Moments with Kernel Stein Discrepancies.](http://arxiv.org/abs/2211.05408) | 本研究分析了核斯坦离差（KSD）控制性质，发现标准KSD无法控制矩的收敛，提出了可控制矩和弱收敛的下游扩散KSD，并且发展了可以准确描述$q$-Wasserstein收敛的KSD。 |
| [^141] | [EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search.](http://arxiv.org/abs/2210.06015) | 提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。 |
| [^142] | [Tripletformer for Probabilistic Interpolation of Irregularly sampled Time Series.](http://arxiv.org/abs/2210.02091) | 提出了一种名为Tripletformer的编码器-解码器架构，用于处理具有缺失值的不规则抽样时间序列的概率插值。Tripletformer通过注意力机制和全连接层的设计，能够更准确和确定地插值不规则时间序列数据。 |
| [^143] | [Convergence of weak-SINDy Surrogate Models.](http://arxiv.org/abs/2209.15573) | 本文通过对弱SINDy方法生成的代理模型进行深入的误差分析，验证了代理动力学的收敛性以及代理模型解与真实解的接近程度。 |
| [^144] | [Collaborative causal inference on distributed data.](http://arxiv.org/abs/2208.07898) | 提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。通过共享中间表示而不是私有数据，估计倾向分数和处理效应，能够减少随机误差和偏差，相比现有方法有更好的估计结果。 |
| [^145] | [DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Change Detection.](http://arxiv.org/abs/2206.11892) | 本文提出了一种以去噪扩散概率模型（DDPM）作为特征提取器的新的变化检测方法，可以利用未标记的遥感图像进行预训练，并在下游应用中取得良好的效果。 |
| [^146] | [Gradient Descent, Stochastic Optimization, and Other Tales.](http://arxiv.org/abs/2205.00832) | 本文旨在揭示黑盒优化器和随机优化器背后的神奇，并建立起这些技术的工作原理和原因的坚实基础。 |
| [^147] | [Robust Peak Detection for Holter ECGs by Self-Organized Operational Neural Networks.](http://arxiv.org/abs/2110.02381) | 本研究提出了一维自组织操作神经网络(Self-ONNs)，通过优化网络配置和使用非线性神经元模型，提高Holter心电图的峰值检测性能，并实现计算效率。 |
| [^148] | [NAAQA: A Neural Architecture for Acoustic Question Answering.](http://arxiv.org/abs/2106.06147) | 本文提出了一种名为NAAQA的神经网络结构，用于声学问答任务。通过使用1D卷积处理声学内容的2D频谱时域表示，该结构通过时间坐标图增加了时间定位能力，并在处理具有不同基本声音构建的场景时表现出有希望的结果。 |
| [^149] | [A finite sample analysis of the benign overfitting phenomenon for ridge function estimation.](http://arxiv.org/abs/2007.12882) | 本论文研究了岭函数估计中的良性过拟合现象，并在有限维度情况下对线性模型进行了分析。 |
| [^150] | [Multiplayer Bandit Learning, from Competition to Cooperation.](http://arxiv.org/abs/1908.01135) | 这篇论文研究了多人赌博学习中竞争和合作对探索和利用权衡的影响，模型考虑了不同合作参数下玩家的效用函数，并使用Gittins指数简化了单人问题。 |

# 详细

[^1]: 通过树木看到道路：基于航空图像的空间依赖建模基准研究

    Seeing the roads through the trees: A benchmark for modeling spatial dependencies with aerial imagery. (arXiv:2401.06762v1 [cs.CV])

    [http://arxiv.org/abs/2401.06762](http://arxiv.org/abs/2401.06762)

    本研究针对现代机器学习模型理解复杂高分辨率卫星或航空图像场景中的长距离背景问题，提出了一个道路分割基准数据集，展示了常用语义分割模型在该任务上的失败。

    

    全面理解复杂的高分辨率卫星或航空图像场景通常需要对广泛相关的背景进行空间推理。人类物体识别系统能够在场景中理解长距离相关的背景。例如，如果人类观察到一个显示道路被树冠分割的航空场景，他们不太可能得出道路实际上被树木分割成了不连续的片段，而更可能认为附近树木的冠层遮挡了道路。然而，目前关于现代机器学习模型理解长距离背景的研究相对有限。在这项工作中，我们提出了一个道路分割基准数据集，切萨皮克道路空间背景（RSC），用于评估地理空间机器学习模型的空间长距离背景理解能力，并展示了常用的语义分割模型在这一任务上的失败。例如，我们展示了一个训练有素的U-Net模型在该任务上的表现失败。

    Fully understanding a complex high-resolution satellite or aerial imagery scene often requires spatial reasoning over a broad relevant context. The human object recognition system is able to understand object in a scene over a long-range relevant context. For example, if a human observes an aerial scene that shows sections of road broken up by tree canopy, then they will be unlikely to conclude that the road has actually been broken up into disjoint pieces by trees and instead think that the canopy of nearby trees is occluding the road. However, there is limited research being conducted to understand long-range context understanding of modern machine learning models. In this work we propose a road segmentation benchmark dataset, Chesapeake Roads Spatial Context (RSC), for evaluating the spatial long-range context understanding of geospatial machine learning models and show how commonly used semantic segmentation models can fail at this task. For example, we show that a U-Net trained to
    
[^2]: 用于行人意图预测的合成数据生成框架、数据集和高效深度模型

    Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction. (arXiv:2401.06757v1 [cs.CV])

    [http://arxiv.org/abs/2401.06757](http://arxiv.org/abs/2401.06757)

    本文提出了一个用于行人意图预测的合成数据生成框架ARCANE，并介绍了相应生成的大型多样化数据集PedSynth。同时，还提出了一种高效深度模型PedGNN，用于实时C/NC预测。

    

    行人意图预测对于自主驾驶至关重要。特别是，了解行人是否将横穿在自主车辆前方对于执行安全和舒适的操控至关重要。从序列图像中准确且快速地预测此类意图的模型是具有挑战性的。导致这一挑战的一个因素是缺乏具有多样化横穿和非横穿（C/NC）场景的数据集。我们通过引入一个名为ARCANE的框架来解决这个问题，该框架允许自动地生成包含C/NC视频剪辑样本的合成数据集。作为一个示例，我们使用ARCANE生成了一个大型多样化的数据集，命名为PedSynth。我们将展示PedSynth如何补充广泛使用的实际数据集，如JAAD和PIE，从而为C/NC预测提供更准确的模型。考虑到C/NC预测模型的车载部署，我们还提出了一种名为PedGNN的深度模型，它速度快且内存占用非常低。PedGNN基于GNN-G模型。

    Pedestrian intention prediction is crucial for autonomous driving. In particular, knowing if pedestrians are going to cross in front of the ego-vehicle is core to performing safe and comfortable maneuvers. Creating accurate and fast models that predict such intentions from sequential images is challenging. A factor contributing to this is the lack of datasets with diverse crossing and non-crossing (C/NC) scenarios. We address this scarceness by introducing a framework, named ARCANE, which allows programmatically generating synthetic datasets consisting of C/NC video clip samples. As an example, we use ARCANE to generate a large and diverse dataset named PedSynth. We will show how PedSynth complements widely used real-world datasets such as JAAD and PIE, so enabling more accurate models for C/NC prediction. Considering the onboard deployment of C/NC prediction models, we also propose a deep model named PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a GNN-G
    
[^3]: 使用机器学习库在结构化网格上解决具有界面捕获的离散多相流动方程

    Solving the Discretised Multiphase Flow Equations with Interface Capturing on Structured Grids Using Machine Learning Libraries. (arXiv:2401.06755v1 [physics.flu-dyn])

    [http://arxiv.org/abs/2401.06755](http://arxiv.org/abs/2401.06755)

    本文使用AI4PDEs方法对具有界面捕获的离散多相流动方程进行求解，并引入了一种新的基于残差公式的压缩代数体积流方法。

    

    本文使用AI4PDEs方法（即用于偏微分方程的人工智能）解决了具有界面捕获的多相流动方程。AI4PDEs中的求解器使用机器学习库的工具来准确求解已经通过数值方法离散化的偏微分方程。卷积层可以用来将离散化表示为神经网络，其权重由数值方法确定，而不是通过训练确定。为了解决系统，通过一个U-Net架构的神经网络来实现多重网格求解器。非相溶的二相流动由三维不可压缩Navier-Stokes方程建模，其中包括表面张力和体积分数场的对流，该场描述了流体之间的界面。介绍了一种新的基于残差公式的压缩代数体积流方法，该方法使用Petrov-Galerkin进行准确性，并专为AI4PDEs设计。高阶有限差分方法被用于求解。

    This paper solves the multiphase flow equations with interface capturing using the AI4PDEs approach (Artificial Intelligence for Partial Differential Equations). The solver within AI4PDEs uses tools from machine learning (ML) libraries to solve (exactly) partial differential equations (PDEs) that have been discretised using numerical methods. Convolutional layers can be used to express the discretisations as a neural network, whose weights are determined by the numerical method, rather than by training. To solve the system, a multigrid solver is implemented through a neural network with a U-Net architecture. Immiscible two-phase flow is modelled by the 3D incompressible Navier-Stokes equations with surface tension and advection of a volume fraction field, which describes the interface between the fluids. A new compressive algebraic volume-of-fluids method is introduced, based on a residual formulation using Petrov-Galerkin for accuracy and designed with AI4PDEs in mind. High-order fini
    
[^4]: Easy Training Data对于困难任务的不合理有效性

    The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])

    [http://arxiv.org/abs/2401.06751](http://arxiv.org/abs/2401.06751)

    当困难训练数据很难正确标记时，当前的语言模型通常能够相对良好地从易到难的数据泛化，并且即使在关注于困难数据的性能时，收集和训练易数据可能比困难数据更好。

    

    当困难训练数据在定义上很难正确标记时，我们如何训练模型在困难测试数据上表现良好？这个问题被称为可扩展监督问题，在语言模型不断改进的过程中引起了越来越多的关注。在本文中，我们提出了一个令人惊讶的结论，即当前的语言模型通常从易到难的数据泛化相对良好，甚至表现得和在困难数据上训练的“oracle”模型一样好。我们使用简单的训练方法（如上下文学习、线性分类器头和QLoRA）展示了这种从易到难的泛化，针对七个不同的数据点难度度量，包括六个经验多样的人类难度度量（如年级水平）和一个基于模型的度量（基于损失）。此外，我们还表明，即使最关心模型在困难数据上的性能，收集并训练易数据可能比困难数据更好，因为困难数据通常更嘈杂和昂贵。

    How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as "oracle" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costli
    
[^5]: 一种用于跳跃扩散模型期权定价的深度隐式-显式最小移动方法

    A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models. (arXiv:2401.06740v1 [q-fin.CP])

    [http://arxiv.org/abs/2401.06740](http://arxiv.org/abs/2401.06740)

    这份论文介绍了一种用于定价跳跃扩散模型下欧式篮式期权的深度学习方法，采用了隐式-显式最小移动方法以及残差型人工神经网络逼近，并通过稀疏网格高斯-埃尔米特逼近和基于ANN的高维专用求积规则来离散化积分运算符。

    

    我们提出了一种新颖的深度学习方法，用于定价跳跃扩散动态下的欧式篮式期权。将期权定价问题表述为一个偏积分微分方程，并通过一种新的隐式-显式最小移动时间步法进行近似，该方法使用深度残差型人工神经网络（ANNs）逐步逼近。积分运算符通过两种不同的方法离散化：a）通过稀疏网格高斯-埃尔米特逼近，采用奇异值分解产生的局部坐标轴，并且b）通过基于ANN的高维专用求积规则。关键是，所提出的ANN的构造确保了解决方案在标的资产较大值时的渐近行为，并且与解决方案先验已知的定性特性相一致输出。对方法维度的性能和鲁棒性进行了评估。

    We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assesse
    
[^6]: 噪声自适应（加速）随机重力球动量

    Noise-adaptive (Accelerated) Stochastic Heavy-Ball Momentum. (arXiv:2401.06738v1 [math.OC])

    [http://arxiv.org/abs/2401.06738](http://arxiv.org/abs/2401.06738)

    本研究分析了在光滑、强凸环境中随机重力球动量的收敛性，并证明了当批量大小大于某个阈值时，该方法可以实现加速收敛速度。针对强凸二次函数，我们建议了一种噪声自适应的多阶段方法，可以使收敛速度进一步提高。实验结果验证了该方法的有效性。

    

    我们分析了在光滑，强凸环境中随机重力球动量（SHB）的收敛性。Kidambi等人（2018）表明，对于二次函数，SHB（带有小批量）无法达到加速的收敛速度，并猜想SHB的实际收益是小批量的副产品。我们通过展示当批量大小大于一定阈值时，SHB可以获得加速的收敛速度来证实这一观点。特别地，对于条件数为$\kappa$的强凸二次函数，我们证明了使用标准步长和动量参数的SHB具有$O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$的收敛速度，其中$T$为迭代次数，$\sigma^2$为随机梯度的方差。为确保收敛到极小值，我们提出了一种多阶段方法，结果是噪声自适应的$O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$速度。对于一般的强凸函数，我们在实验中展示了所提方法的有效性。

    We analyze the convergence of stochastic heavy ball (SHB) momentum in the smooth, strongly-convex setting. Kidambi et al. (2018) show that SHB (with small mini-batches) cannot attain an accelerated rate of convergence even for quadratics, and conjecture that the practical gain of SHB is a by-product of mini-batching. We substantiate this claim by showing that SHB can obtain an accelerated rate when the mini-batch size is larger than some threshold. In particular, for strongly-convex quadratics with condition number $\kappa$, we prove that SHB with the standard step-size and momentum parameters results in an $O\left(\exp(-\frac{T}{\sqrt{\kappa}}) + \sigma \right)$ convergence rate, where $T$ is the number of iterations and $\sigma^2$ is the variance in the stochastic gradients. To ensure convergence to the minimizer, we propose a multi-stage approach that results in a noise-adaptive $O\left(\exp\left(-\frac{T}{\sqrt{\kappa}} \right) + \frac{\sigma}{T}\right)$ rate. For general strongly-
    
[^7]: 深度流形图自编码器用于属性图嵌入

    Deep Manifold Graph Auto-Encoder for Attributed Graph Embedding. (arXiv:2401.06727v1 [cs.LG])

    [http://arxiv.org/abs/2401.06727](http://arxiv.org/abs/2401.06727)

    这篇论文提出了一种用于属性图数据的深度流形图自编码器，通过同时考虑数据分布和潜在代码的拓扑结构，改善了学习表示的稳定性和质量，在各种下游任务中超过了最先进的基准算法。

    

    通过在低维空间中表示图数据，来为后续任务提供属性图嵌入是这篇论文的目的。大多数现有的神经网络方法通过最小化重构误差来学习潜在表示。很少有研究同时考虑数据分布和潜在代码的拓扑结构，这通常导致在真实世界的图数据中表现较差。本文提出了一种新颖的深度流形(Variational)图自编码器(DMVGAE/DMGAE)方法，用于改善学习表示的稳定性和质量，以应对拥挤问题。在预定义的分布下，原始空间和潜在空间之间保持节点到节点的测地线相似性。所提出的方法在不同的流行数据集上，在不同的下游任务上显著超过了最先进的基准算法，验证了我们的解决方案。我们承诺在稿件接受后发布代码。

    Representing graph data in a low-dimensional space for subsequent tasks is the purpose of attributed graph embedding. Most existing neural network approaches learn latent representations by minimizing reconstruction errors. Rare work considers the data distribution and the topological structure of latent codes simultaneously, which often results in inferior embeddings in real-world graph data. This paper proposes a novel Deep Manifold (Variational) Graph Auto-Encoder (DMVGAE/DMGAE) method for attributed graph data to improve the stability and quality of learned representations to tackle the crowding problem. The node-to-node geodesic similarity is preserved between the original and latent space under a pre-defined distribution. The proposed method surpasses state-of-the-art baseline algorithms by a significant margin on different downstream tasks across popular datasets, which validates our solutions. We promise to release the code after acceptance.
    
[^8]: 使用样式表示进行机器生成文本的小样本检测

    Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])

    [http://arxiv.org/abs/2401.06712](http://arxiv.org/abs/2401.06712)

    本研究提出了一种小样本检测方法，通过使用样式表示来检测机器生成的文本与人类撰写的文本的区别，以解决滥用语言模型带来的问题。

    

    受到指导调整的语言模型的出现使得人类写作的逼真模仿面临着重大滥用风险。然而，我们可以通过检测一段文本是由语言模型还是人类撰写而成来对抗此类滥用行为。本文提出了一种基于样式表示的小样本检测方法，避免了神经网络检测器在面对数据转换时的规约不足的挑战，同时也避免了在推理或检测时需要访问可能生成文档的模型的问题。

    The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
    
[^9]: 大规模转化漏斗优化的无模型近似贝叶斯学习

    Model-Free Approximate Bayesian Learning for Large-Scale Conversion Funnel Optimization. (arXiv:2401.06710v1 [cs.LG])

    [http://arxiv.org/abs/2401.06710](http://arxiv.org/abs/2401.06710)

    该论文提出了一种解决大规模转化漏斗优化问题的无模型近似贝叶斯学习算法，通过建立转化漏斗模型来捕捉消费者行为，并实现了非常高的准确度。

    

    在现代营销活动中，根据消费者状态选择广告行动的灵活性至关重要。我们研究了识别最优顺序个性化干预以最大化针对新产品的采纳概率的问题。我们通过一个转化漏斗模型来建模消费者行为，该模型捕捉到每个消费者的状态（例如与公司的互动历史）并允许消费者行为随着其状态和公司的顺序干预而变化。我们展示了我们的模型在真实世界的电子邮件营销数据集中以非常高的准确度（超过0.95的样本外AUC）捕捉到消费者行为。然而，这导致了一个非常大规模的学习问题，公司必须从消费者交互中学习各种干预的状态特定效应。我们提出了一个新的基于归因的决策算法来解决这个问题，我们称之为无模型近似贝叶斯学习。

    The flexibility of choosing the ad action as a function of the consumer state is critical for modern-day marketing campaigns. We study the problem of identifying the optimal sequential personalized interventions that maximize the adoption probability for a new product. We model consumer behavior by a conversion funnel that captures the state of each consumer (e.g., interaction history with the firm) and allows the consumer behavior to vary as a function of both her state and firm's sequential interventions. We show our model captures consumer behavior with very high accuracy (out-of-sample AUC of over 0.95) in a real-world email marketing dataset. However, it results in a very large-scale learning problem, where the firm must learn the state-specific effects of various interventions from consumer interactions. We propose a novel attribution-based decision-making algorithm for this problem that we call model-free approximate Bayesian learning. Our algorithm inherits the interpretability
    
[^10]: 全连接前馈神经网络权重优化的闭合解方案

    A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])

    [http://arxiv.org/abs/2401.06699](http://arxiv.org/abs/2401.06699)

    本文提出了一种闭合解法，通过最小二乘法来优化全连接前馈神经网络的权重，具有非常高的效率和独立性。

    

    本文针对全连接前馈神经网络的权重优化问题进行了研究。与现有的基于反向传播和链式规则梯度优化的方法不同，该方法通过最小二乘法提供了闭合形式的权重优化解决方案。在输入到输出映射是可逆的情况下，新方法通过同时优化每个神经元层的一组权重，在单次迭代中以反向传播的方式优化权重。在输入到输出映射不可逆的情况下（例如分类问题），提出的解决方案可以轻松地在几次迭代中获得最终解。与现有解决方案相比，一个重要的优势是这些计算（对于每个神经元层的所有神经元）是独立的，因此它们可以同时进行。

    This work addresses weight optimization problem for fully-connected feed-forward neural networks. Unlike existing approaches that are based on back-propagation (BP) and chain rule gradient-based optimization (which implies iterative execution, potentially burdensome and time-consuming in some cases), the proposed approach offers the solution for weight optimization in closed-form by means of least squares (LS) methodology. In the case where the input-to-output mapping is injective, the new approach optimizes the weights in a back-propagating fashion in a single iteration by jointly optimizing a set of weights in each layer for each neuron. In the case where the input-to-output mapping is not injective (e.g., in classification problems), the proposed solution is easily adapted to obtain its final solution in a few iterations. An important advantage over the existing solutions is that these computations (for all neurons in a layer) are independent from each other; thus, they can be carri
    
[^11]: 量子机器学习在认知领域中的应用：阿尔茨海默病研究

    Quantum Machine Learning in the Cognitive Domain: Alzheimer's Disease Study. (arXiv:2401.06697v1 [cs.LG])

    [http://arxiv.org/abs/2401.06697](http://arxiv.org/abs/2401.06697)

    本文研究了量子机器学习在认知领域中的应用，主要关注阿尔茨海默病(AD)的早期检测。通过分析书写的不同方面，研究人员可以检测出细微的变化，这对于早期AD的检测具有重要意义。

    

    阿尔茨海默病(AD)是最常见的神经退行性脑疾病，导致老年人中显著的认知功能损害。认知功能损害可以表现为各种心理能力的下降，如注意力、记忆和其他高级认知能力。这些缺陷会严重影响个体理解信息、获取新知识和有效沟通的能力。其中受到认知功能损害影响的活动之一是书写。通过分析书写的不同方面，包括压力、速度和空间组织，研究人员可以检测到细微的变化，可能表明早期的认知功能损害，特别是AD。最近，已经提出了几种经典的人工智能(AI)方法，通过书写分析来检测老年人中的AD。然而，先进的AI方法需要更多的计算能力，因为数据的规模增加了。

    Alzheimer's disease (AD) is the most prevalent neurodegenerative brain disorder, which results in significant cognitive impairments, especially in the elderly population. Cognitive impairments can manifest as a decline in various mental faculties, such as concentration, memory, and other higher-order cognitive abilities. These deficits can significantly impact an individual's capacity to comprehend information, acquire new knowledge, and communicate effectively. One of the affected activities due to cognitive impairments is handwriting. By analyzing different aspects of handwriting, including pressure, velocity, and spatial organization, researchers can detect subtle alterations that might indicate early-stage cognitive impairments, especially AD. Recently, several classical artificial intelligence (AI) approaches have been proposed for detecting AD in elderly individuals through handwriting analysis. However, advanced AI methods require more computational power as the size of the data
    
[^12]: 大型语言模型有限标签监督微调的实验设计框架

    An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])

    [http://arxiv.org/abs/2401.06692](http://arxiv.org/abs/2401.06692)

    该论文提出了一个实验设计框架来减少大型语言模型有限标签监督微调的注释成本，并解决了主动学习的计算瓶颈问题。

    

    在现代大型语言模型中，指导数据集上的有限标签监督微调（SFT）在实现了令人惊叹的零射击泛化能力方面发挥了至关重要的作用。然而，为了为指令产生高质量的回答所需的注释工作正在变得难以承受，特别是随着指令数据集所涵盖的任务数量的增加。主动学习可以有效地从未标记的样本池中确定有用的子集进行注释，但其高计算成本仍然是其在LLMs环境中广泛应用的障碍。为了减少SFT的注释成本并规避主动学习的计算瓶颈，我们提出使用实验设计。实验设计技术选择最具信息量的样本进行标注，通常最大化某种不确定性和/或多样性的概念。在我们的工作中，我们实施了一个评估多种现有和新颖的实验设计方法的框架。

    Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
    
[^13]: 不要排名，要合并！使用质量估计来合并机器翻译假设

    Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])

    [http://arxiv.org/abs/2401.06688](http://arxiv.org/abs/2401.06688)

    本文介绍了一种利用质量估计指标合并机器翻译假设的方法，该方法在大型语言模型和多语言翻译模型上提升了翻译质量。通过使用候选池和QE指标，我们的方法能够生成多样且准确的翻译结果。

    

    神经机器翻译系统通过给定源句子估计目标句子的概率，但这些估计可能与人类喜好不一致。本研究引入了QE-fusion方法，该方法利用更能与人类判断相关的质量估计指标（QE）来综合改进翻译结果。QE-fusion利用从模型中抽取的候选池，使用像CometKiwi这样的QE指标组合不同候选的片段。我们将QE-fusion与波束搜索和最近的重新排序技术（如最小贝叶斯风险解码或QE-重新排序）进行比较。当应用于用于翻译的大型语言模型（PolyLM、XGLM、Llama2和Mistral）和多语言翻译模型（NLLB）时，我们的方法在COMET和BLEURT评分方面始终提高翻译质量，涵盖五种语言对。值得注意的是，由于能够生成多样化的输出，我们的方法对于大型语言模型的改进更大。我们证明了我们的方法能够产生多样且准确的翻译结果。

    Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generat
    
[^14]: 使用文本数据的近因果推断

    Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])

    [http://arxiv.org/abs/2401.06687](http://arxiv.org/abs/2401.06687)

    本论文提出了一种使用文本数据进行近因果推断的方法，通过将文本数据分割并使用零样本模型推断出代理变量，然后应用于近邻 g-formula，从而解决了混淆变量完全未观察到的情况。实验结果表明该方法产生了低偏差的估计值。

    

    最近的基于文本的因果方法试图通过将非结构化文本数据作为倾向于包含部分或不完全测量的混淆变量的代理来减轻混淆偏差。这些方法假设分析人员在一部分实例的文本中具有有监督的混淆变量标签，但由于数据隐私或成本，这种约束并不总是可行。在这里，我们解决了一个重要的混淆变量完全未观察到的情况。我们提出了一种新的因果推断方法，将处理前文本数据分割，并使用两个零样本模型从分割的两个部分推断出两个代理，并将这些代理应用于近邻 g-formula。我们证明了我们基于文本的代理方法满足近邻 g-formula所需的识别条件，而其他看似合理的提议则不满足。我们在合成和半合成环境中评估了我们的方法，并发现它产生了低偏差的估计值。

    Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
    
[^15]: DQNC2S：基于DQN的跨流危机事件摘要生成器

    DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])

    [http://arxiv.org/abs/2401.06683](http://arxiv.org/abs/2401.06683)

    本研究提出了一种基于DQN的在线危机事件摘要生成方法，能够同时总结多个灾害相关的数据流，无需人工标注或内容重新排序，且具有较好的性能表现。

    

    同时总结多个与灾害相关的数据流尤其具有挑战性，因为现有的检索与重新排序策略在多流数据的固有冗余和多查询环境下的限制可扩展性方面存在问题。本文提出了一种基于弱标注和深度Q网络的在线危机时间轴生成方法。它能够实时选择相关的文本片段，无需人工标注或内容重新排序，从而使推理时间与输入查询的数量无关。该方法还将冗余过滤器融入奖励函数中，以有效处理跨流内容重叠。在CrisisFACTS 2022基准测试中，所达到的ROUGE和BERTScore结果优于最佳性能模型。

    Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
    
[^16]: 神经网络用于奇异摄动

    Neural Networks for Singular Perturbations. (arXiv:2401.06656v1 [math.NA])

    [http://arxiv.org/abs/2401.06656](http://arxiv.org/abs/2401.06656)

    本论文探讨了用于奇异摄动的神经网络模型的表达率极限，并在不同类型的神经网络架构中得到了统一的表达率界限。研究表明，基于ReLU、spiking、$\tanh$和sigmoid激活函数的神经网络可以明确地表示“指数边界层解特性”，并能得到改进的鲁棒表达率界限。

    

    我们证明了在有界区间$(-1, 1)$上，关于特定类别的奇异摄动椭圆型两点边界值问题的解集的深度神经网络（DNN）的表达率界限，用Sobolev范数表示。我们假设给定的源项和反应系数在$[-1, 1]$中是解析的。针对几种DNN架构，我们建立了与奇异摄动参数无关的Sobolev范数下的表达率界限，特别是对于ReLU NNs，spiking NNs，$\tanh$和sigmoid激活的NNs。后两种激活函数可以明确地表示“指数边界层解特性”，即在DNN的最后一个隐藏层中，即在一个浅层子网络中，并以NN大小量化得到改进的鲁棒表达率界限。我们证明了所有的DNN架构都允许在所谓的“能量”和“平衡”Sobolev范数中以解析输入数据的方式获得鲁棒的指数解表达。

    We prove deep neural network (DNN for short) expressivity rate bounds for solution sets of a model class of singularly perturbed, elliptic two-point boundary value problems, in Sobolev norms, on the bounded interval $(-1,1)$. We assume that the given source term and reaction coefficient are analytic in $[-1,1]$.  We establish expression rate bounds in Sobolev norms in terms of the NN size which are uniform with respect to the singular perturbation parameter for several classes of DNN architectures. In particular, ReLU NNs, spiking NNs, and $\tanh$- and sigmoid-activated NNs. The latter activations can represent ``exponential boundary layer solution features'' explicitly, in the last hidden layer of the DNN, i.e. in a shallow subnetwork, and afford improved robust expression rate bounds in terms of the NN size.  We prove that all DNN architectures allow robust exponential solution expression in so-called `energy' as well as in `balanced' Sobolev norms, for analytic input data.
    
[^17]: 解耦像素翻转与遮挡策略以实现一致的XAI基准

    Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks. (arXiv:2401.06654v1 [cs.CV])

    [http://arxiv.org/abs/2401.06654](http://arxiv.org/abs/2401.06654)

    本研究提出了两个视角来解决基于遮挡的解释方法中的矛盾问题，首先通过使用R-OMS得分来衡量可靠性，然后通过解耦像素翻转和遮挡策略来提高结果的一致性。

    

    特征移除是可解释人工智能（XAI）的核心构建模块，既适用于基于遮挡的解释（Shapley值），也适用于它们的评估（像素翻转，PF）。然而，遮挡策略可以从简单的均值替换到使用最先进的扩散模型进行修复之间差异很大。这种不确定性限制了遮挡方法的实用性。例如，PF基准会导致矛盾的排名。这一问题还因竞争的PF度量而变得复杂：特征要么从最有影响力的开始移除（MIF），要么从最无影响力的开始移除（LIF）。本研究提出了两个互补的视角来解决这个问题。首先，我们解决了针对基于遮挡的XAI的常见批评，即人工样本导致模型评估不可靠的问题。我们提出用R-OMS得分来衡量可靠性（参考模型范围之外的R-OMS得分）。R-OMS得分能够对遮挡策略进行系统比较，并解决了矛盾问题。

    Feature removal is a central building block for eXplainable AI (XAI), both for occlusion-based explanations (Shapley values) as well as their evaluation (pixel flipping, PF). However, occlusion strategies can vary significantly from simple mean replacement up to inpainting with state-of-the-art diffusion models. This ambiguity limits the usefulness of occlusion-based approaches. For example, PF benchmarks lead to contradicting rankings. This is amplified by competing PF measures: Features are either removed starting with most influential first (MIF) or least influential first (LIF). This study proposes two complementary perspectives to resolve this disagreement problem. Firstly, we address the common criticism of occlusion-based XAI, that artificial samples lead to unreliable model evaluations. We propose to measure the reliability by the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a systematic comparison of occlusion strategies and resolves the disagreement pro
    
[^18]: 使用外推的块主导极小化方法和应用于$\beta$-NMF

    Block Majorization Minimization with Extrapolation and Application to $\beta$-NMF. (arXiv:2401.06646v1 [cs.LG])

    [http://arxiv.org/abs/2401.06646](http://arxiv.org/abs/2401.06646)

    本文提出了一种使用外推的块主导极小化方法（BMMe）来解决多凸优化问题，并将其应用于$\beta$-NMF。通过使用独特的自适应更新规则来更新外推参数，该方法在实验中展现出显著的加速效果。

    

    我们提出了一种使用外推的块主导极小化方法（BMMe）来解决一类多凸优化问题。BMMe的外推参数使用一种新颖的自适应更新规则来更新。通过将块主导极小化重新表述为块镜像下降方法，并在每次迭代中自适应更新Bregman散度，我们建立了BMMe的子序列收敛性。我们使用这种方法设计了高效的算法来处理$\beta$-NMF中的非负矩阵分解问题，其中$\beta\in [1,2]$。这些算法是使用外推的乘法更新，并从我们的新结果中获得了收敛性保证。我们还通过大量实验实证了BMMe在$\beta$-NMF中的显著加速效果。

    We propose a Block Majorization Minimization method with Extrapolation (BMMe) for solving a class of multi-convex optimization problems. The extrapolation parameters of BMMe are updated using a novel adaptive update rule. By showing that block majorization minimization can be reformulated as a block mirror descent method, with the Bregman divergence adaptively updated at each iteration, we establish subsequential convergence for BMMe. We use this method to design efficient algorithms to tackle nonnegative matrix factorization problems with the $\beta$-divergences ($\beta$-NMF) for $\beta\in [1,2]$. These algorithms, which are multiplicative updates with extrapolation, benefit from our novel results that offer convergence guarantees. We also empirically illustrate the significant acceleration of BMMe for $\beta$-NMF through extensive experiments.
    
[^19]: SeizNet：一种用于癫痫预测的AI辅助植入式传感器网络系统

    SeizNet: An AI-enabled Implantable Sensor Network System for Seizure Prediction. (arXiv:2401.06644v1 [cs.LG])

    [http://arxiv.org/abs/2401.06644](http://arxiv.org/abs/2401.06644)

    SeizNet是一种通过使用深度学习和植入式传感器网络系统进行癫痫预测的闭环系统，可以提高预测的特异性并保持很高的敏感性。

    

    本文介绍了SeizNet，这是一个通过深度学习方法和植入式传感器网络预测癫痫发作的闭环系统。尽管药物治疗对一些癫痫患者有效（全球约有6500万人受到影响），但三分之一的患者患有耐药性癫痫。为了减轻癫痫的影响，已经开发出了可以通知这类患者即将发作的预测系统，使他们能够采取预防措施。SeizNet利用深度学习技术，并结合多个记录的数据，特别是颅内脑电图（iEEG）和心电图（ECG）传感器的数据，可以显著提高癫痫预测的特异性，同时保持非常高的敏感性水平。SeizNet的深度学习算法旨在边缘实时执行，从而减少与基于云的解决方案相关的数据隐私问题、数据传输开销和功耗效率问题。

    In this paper, we introduce SeizNet, a closed-loop system for predicting epileptic seizures through the use of Deep Learning (DL) method and implantable sensor networks. While pharmacological treatment is effective for some epilepsy patients (with ~65M people affected worldwide), one out of three suffer from drug-resistant epilepsy. To alleviate the impact of seizure, predictive systems have been developed that can notify such patients of an impending seizure, allowing them to take precautionary measures. SeizNet leverages DL techniques and combines data from multiple recordings, specifically intracranial electroencephalogram (iEEG) and electrocardiogram (ECG) sensors, that can significantly improve the specificity of seizure prediction while preserving very high levels of sensitivity. SeizNet DL algorithms are designed for efficient real-time execution at the edge, minimizing data privacy concerns, data transmission overhead, and power inefficiencies associated with cloud-based soluti
    
[^20]: CCFC：桥接联邦聚类和对比学习

    CCFC: Bridging Federated Clustering and Contrastive Learning. (arXiv:2401.06634v1 [cs.LG])

    [http://arxiv.org/abs/2401.06634](http://arxiv.org/abs/2401.06634)

    本论文桥接了联邦聚类和对比学习，提出了一种名为CCFC的新联邦聚类方法。通过表示学习，CCFC在某些情况下聚类性能甚至是最佳基准方法的两倍。与最相关的基准方法相比，在最显著的案例中，CCFC的NMI得分提高了0.4155。同时，CCFC还能有效处理联邦场景下的数据分布和质量差异。

    

    联邦聚类是对于联邦场景中集中聚类的重要扩展，可以让多个数据持有客户端在保留本地数据的同时协同进行数据分组。在集中场景中，通过表示学习驱动的聚类在处理高维复杂数据方面取得了重大进展。然而，联邦聚类和表示学习的结合仍然未被充分研究。为了弥合这一差距，我们首先为学习聚类友好的表示定制了一个聚类对比模型。然后，我们利用这个模型作为提出新的联邦聚类方法的基础，称为聚类对比联邦聚类（CCFC）。受益于表示学习，CCFC的聚类性能在某些情况下甚至是最佳基准方法的两倍。与最相关的基准方法相比，在最显著的案例中，这种收益导致NMI得分的显著提高，最高达到0.4155。此外，CCFC还可以有效地处理在联邦场景下出现的数据分布和质量差异。

    Federated clustering, an essential extension of centralized clustering for federated scenarios, enables multiple data-holding clients to collaboratively group data while keeping their data locally. In centralized scenarios, clustering driven by representation learning has made significant advancements in handling high-dimensional complex data. However, the combination of federated clustering and representation learning remains underexplored. To bridge this, we first tailor a cluster-contrastive model for learning clustering-friendly representations. Then, we harness this model as the foundation for proposing a new federated clustering method, named cluster-contrastive federated clustering (CCFC). Benefiting from representation learning, the clustering performance of CCFC even double those of the best baseline methods in some cases. Compared to the most related baseline, the benefit results in substantial NMI score improvements of up to 0.4155 on the most conspicuous case. Moreover, CCF
    
[^21]: 识别策略梯度子空间

    Identifying Policy Gradient Subspaces. (arXiv:2401.06604v1 [cs.LG])

    [http://arxiv.org/abs/2401.06604](http://arxiv.org/abs/2401.06604)

    本文研究了两种深度策略梯度方法在不同模拟基准任务上的评估结果，发现尽管数据分布不断变化，但存在低维且缓慢变化的梯度子空间，这有助于未来更高效的强化学习工作。

    

    策略梯度方法在解决复杂的连续控制任务方面具有巨大的潜力。然而，通过利用优化问题内部的结构，可以提高其训练效率。最近的研究表明，通过利用梯度位于低维且缓慢变化子空间中的事实，可以加速监督学习。在本文中，我们对两种流行的深度策略梯度方法在各种模拟基准任务上进行了全面的评估。我们的结果表明，尽管强化学习固有的数据分布不断变化，但存在这样的梯度子空间。这些发现为未来更高效的强化学习工作，例如改进参数空间探索或实现二阶优化，提供了有希望的方向。

    Policy gradient methods hold great potential for solving complex continuous control tasks. Still, their training efficiency can be improved by exploiting structure within the optimization problem. Recent work indicates that supervised learning can be accelerated by leveraging the fact that gradients lie in a low-dimensional and slowly-changing subspace. In this paper, we conduct a thorough evaluation of this phenomenon for two popular deep policy gradient methods on various simulated benchmark tasks. Our results demonstrate the existence of such gradient subspaces despite the continuously changing data distribution inherent to reinforcement learning. These findings reveal promising directions for future work on more efficient reinforcement learning, e.g., through improving parameter-space exploration or enabling second-order optimization.
    
[^22]: 每个节点都不同：动态融合自监督任务进行属性图聚类

    Every Node is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering. (arXiv:2401.06595v1 [cs.LG])

    [http://arxiv.org/abs/2401.06595](http://arxiv.org/abs/2401.06595)

    提出了一种动态融合自监督任务并学习不同节点的权重的方法，通过从不同的SSL任务中提取的特征融合来提高属性图聚类的性能。

    

    属性图聚类是一项无监督任务，将节点分为不同的组。自监督学习（SSL）在处理这个任务方面显示出巨大的潜力，并且最近的一些研究同时学习多个SSL任务以进一步提高性能。目前，不同的SSL任务被分配给所有图节点相同的权重。然而，我们观察到一些图节点其邻居在不同的组中，对SSL任务需要有显著不同的强调。在本文中，我们提出了一种动态学习不同节点的SSL任务权重并融合从不同SSL任务中学到的嵌入以提高性能的方法。我们设计了一种创新的图聚类方法，即动态融合自监督学习（DyFSS）。具体而言，DyFSS使用从门控网络推导出的不同权重融合从多样的SSL任务中提取的特征。为了有效学习门控网络，我们设计了一个双层自监督策略，其中包含...

    Attributed graph clustering is an unsupervised task that partitions nodes into different groups. Self-supervised learning (SSL) shows great potential in handling this task, and some recent studies simultaneously learn multiple SSL tasks to further boost performance. Currently, different SSL tasks are assigned the same set of weights for all graph nodes. However, we observe that some graph nodes whose neighbors are in different groups require significantly different emphases on SSL tasks. In this paper, we propose to dynamically learn the weights of SSL tasks for different nodes and fuse the embeddings learned from different SSL tasks to boost performance. We design an innovative graph clustering approach, namely Dynamically Fusing Self-Supervised Learning (DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks using distinct weights derived from a gating network. To effectively learn the gating network, we design a dual-level self-supervised strategy that incorpora
    
[^23]: 具有强延迟约束的连接主义语音识别的动态行为

    Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints. (arXiv:2401.06588v1 [eess.AS])

    [http://arxiv.org/abs/2401.06588](http://arxiv.org/abs/2401.06588)

    本文研究了具有强延迟约束条件下的连接主义语音识别，在实时推导合成面部唇部运动的同时，亦关注了时间演化模型与转移模型的相互作用。实验结果表明神经网络拓扑结构、语言模型中的时间依赖关系和解码器延迟之间存在强烈相互作用。

    

    本文描述了在强延迟约束条件下使用连接主义技术进行语音识别的情况。这些约束是通过将语音信号输入到口腔运动合成器中，从而实时推导出合成面部的唇部运动来实现的。我们特别关注了多层感知器学习的时间演化模型与维特比译码器强制的转移模型在不同延迟条件下的相互作用。进行了两个实验，通过参数控制了语言模型中的时间依赖关系。结果显示了神经网络拓扑结构、语言模型中时间依赖的长度和解码器延迟之间的强烈相互作用。

    This paper describes the use of connectionist techniques in phonetic speech recognition with strong latency constraints. The constraints are imposed by the task of deriving the lip movements of a synthetic face in real time from the speech signal, by feeding the phonetic string into an articulatory synthesiser. Particular attention has been paid to analysing the interaction between the time evolution model learnt by the multi-layer perceptrons and the transition model imposed by the Viterbi decoder, in different latency conditions. Two experiments were conducted in which the time dependencies in the language model (LM) were controlled by a parameter. The results show a strong interaction between the three factors involved, namely the neural network topology, the length of time dependencies in the LM and the decoder latency.
    
[^24]: 将变形器技术应用于跨语言文档表示的映射

    Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])

    [http://arxiv.org/abs/2401.06583](http://arxiv.org/abs/2401.06583)

    本研究通过使用预训练的变形器模型和映射方法，探索了跨语言文档表示的方法。实验结果表明，通过映射到跨语言领域的变形器技术文档表示（TLDRs），能够有效地实现跨语言的推荐系统。

    

    推荐系统对于文档已经成为在网络上找到相关内容的工具。然而，当推荐非查询语言的文档时，这些系统存在一定限制，可能会忽视非母语的资源。本研究旨在通过使用映射到跨语言领域的变形器技术文档表示（TLDRs）来表示跨语言文档。评估了四个多语言预训练变形器模型（mBERT，mT5 XLM RoBERTa，ErnieM）在20种语言对上使用三种映射方法的效果，这些语言对代表了欧盟选择的五种语言的组合。使用Mate检索率和互惠排序等指标来衡量映射TLDRs与未映射TLDRs的效果。结果强调了通过预训练变形器和映射方法实现的跨语言表示的能力，为扩展跨语言文档表示提供了有希望的方向。

    Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding
    
[^25]: 最大因果熵逆强化学习用于均场博弈问题

    Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games. (arXiv:2401.06566v1 [eess.SY])

    [http://arxiv.org/abs/2401.06566](http://arxiv.org/abs/2401.06566)

    本文介绍了最大因果熵逆强化学习（IRL）方法用于均场博弈（MFG）问题，提出了将MFG问题转化为广义纳什均衡问题（GNEP）的新算法。

    

    本文介绍了在无限时间间隔折扣回报最优性准则下，针对离散时间均场博弈（MFG）的最大因果熵逆强化学习（IRL）问题。典型智能体的状态空间是有限的。我们的方法首先全面回顾了关于确定性和随机马尔科夫决策过程（MDPs）在有限和无限时间间隔情

    In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) f
    
[^26]: 一个通用的基准框架对于动态图神经网络的需要

    A General Benchmark Framework is Dynamic Graph Neural Network Need. (arXiv:2401.06559v1 [cs.LG])

    [http://arxiv.org/abs/2401.06559](http://arxiv.org/abs/2401.06559)

    本文强调了动态图学习的重要性及其在各个领域中的应用，并强调了对捕捉时间动态、演化图结构和下游任务需求的标准化基准框架的需求。缺乏统一的基准框架是当前动态图学习研究的局限之处，建立这样的框架将有助于推动动态图学习技术的进步。

    

    动态图学习对于建模具有演化关系和时间动态的真实世界系统至关重要。然而，当前研究中缺乏统一的基准框架导致动态图模型的评估不准确。本文强调了动态图学习的重要性及其在各个领域中的应用。它强调了对捕捉时间动态、演化图结构和下游任务需求的标准化基准框架的需求。建立统一的基准框架将有助于研究人员了解现有模型的优势和局限性，促进创新，并推动动态图学习的进展。总之，本文确定缺乏统一的基准框架是当前动态图学习研究的局限之处。这样的框架将有助于准确评估模型，推动动态图学习技术的进步，并为生成更有效的模型打下基础。

    Dynamic graph learning is crucial for modeling real-world systems with evolving relationships and temporal dynamics. However, the lack of a unified benchmark framework in current research has led to inaccurate evaluations of dynamic graph models. This paper highlights the significance of dynamic graph learning and its applications in various domains. It emphasizes the need for a standardized benchmark framework that captures temporal dynamics, evolving graph structures, and downstream task requirements. Establishing a unified benchmark will help researchers understand the strengths and limitations of existing models, foster innovation, and advance dynamic graph learning. In conclusion, this paper identifies the lack of a standardized benchmark framework as a current limitation in dynamic graph learning research . Such a framework will facilitate accurate model evaluation, drive advancements in dynamic graph learning techniques, and enable the development of more effective models for re
    
[^27]: 对待社交网络中因果效应估计的超螺旋表示学习的关注

    Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks. (arXiv:2401.06557v1 [cs.LG])

    [http://arxiv.org/abs/2401.06557](http://arxiv.org/abs/2401.06557)

    该论文提出了一种新的方法，称为Treatment-Aware Hyperbolic Representation Learning（TAHyper），用于利用超螺旋空间学习社交网络中隐藏混淆因素的表示，从而改进个体治疗效应的估计。

    

    从观测数据中估计个体治疗效应（ITE）是一个具有重要价值的关键研究领域。如何识别隐藏的混淆因素在ITE估计中是一个关键挑战。最近的研究已经将社交网络的结构信息纳入其中，取得了显著的进展。然而，这些方法利用图神经网络在欧几里得空间中学习隐藏混淆因素的表示，忽视了两个关键问题：（1）社交网络经常表现出无标度结构，而欧几里得嵌入在嵌入此类图时会产生高度失真，（2）社交网络中的每个自我中心网络都表现出与治疗相关的特征，意味着隐藏混淆因素的显著模式。为了解决这些问题，我们提出了一种新的方法，名为Treatment-Aware Hyperbolic Representation Learning（TAHyper）。首先，TAHyper使用超螺旋空间进行嵌入学习，以捕捉社交网络的无标度特性，并利用治疗相关的特征来学习隐藏混淆因素的模式。

    Estimating the individual treatment effect (ITE) from observational data is a crucial research topic that holds significant value across multiple domains. How to identify hidden confounders poses a key challenge in ITE estimation. Recent studies have incorporated the structural information of social networks to tackle this challenge, achieving notable advancements. However, these methods utilize graph neural networks to learn the representation of hidden confounders in Euclidean space, disregarding two critical issues: (1) the social networks often exhibit a scalefree structure, while Euclidean embeddings suffer from high distortion when used to embed such graphs, and (2) each ego-centric network within a social network manifests a treatment-related characteristic, implying significant patterns of hidden confounders. To address these issues, we propose a novel method called Treatment-Aware Hyperbolic Representation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic space to en
    
[^28]: 使用遗传算法优化带有噪声标签的二分类特征选择：一种新方法的提出

    Optimizing Feature Selection for Binary Classification with Noisy Labels: A Genetic Algorithm Approach. (arXiv:2401.06546v1 [cs.LG])

    [http://arxiv.org/abs/2401.06546](http://arxiv.org/abs/2401.06546)

    本研究提出了一种基于遗传算法的特征选择方法NMFS-GA，用于带有噪声标签的二分类中选择最优特征子集。在合成数据集和真实数据集的实验中，我们证明了NMFS-GA能够有效地提高二分类器准确性和可解释性。

    

    在带有噪声标签的场景下，特征选择仍然是一个不够研究的课题。我们提出了一种基于遗传算法的新方法，即噪声感知多目标特征选择遗传算法(NMFS-GA)，用于在带有噪声标签的二分类中选择最优特征子集。NMFS-GA提供了一个统一的框架，用于选择既准确又可解释的特征子集。我们在带有标签噪声的合成数据集、一个富含噪声特征的乳腺癌数据集以及一个用于预测痴呆转化的真实ADNI数据集上评估了NMFS-GA。我们的结果表明，在带有噪声标签的场景中，NMFS-GA能够有效地选择提高二分类器准确性和可解释性的特征子集。

    Feature selection in noisy label scenarios remains an understudied topic. We propose a novel genetic algorithm-based approach, the Noise-Aware Multi-Objective Feature Selection Genetic Algorithm (NMFS-GA), for selecting optimal feature subsets in binary classification with noisy labels. NMFS-GA offers a unified framework for selecting feature subsets that are both accurate and interpretable. We evaluate NMFS-GA on synthetic datasets with label noise, a Breast Cancer dataset enriched with noisy features, and a real-world ADNI dataset for dementia conversion prediction. Our results indicate that NMFS-GA can effectively select feature subsets that improve the accuracy and interpretability of binary classifiers in scenarios with noisy labels.
    
[^29]: 智能数据驱动的网络切片体系结构特征编排

    Intelligent Data-Driven Architectural Features Orchestration for Network Slicing. (arXiv:2401.06538v1 [cs.NI])

    [http://arxiv.org/abs/2401.06538](http://arxiv.org/abs/2401.06538)

    本论文讨论了基于机器学习的网络切片架构中特征和能力编排的问题和挑战，提出了使用机器学习嵌入的代理进行智能的资源和功能编排的建议。

    

    网络切片是下一代移动网络（NGMN）以及其他新系统（如物联网和工业物联网）的关键推动因素和趋势。编排和机器学习是网络切片过程中起着关键作用的元素，因为切片过程需要编排资源和功能，而机器学习可以潜在地优化编排过程。然而，现有的网络切片架构缺乏定义智能方法来编排切片过程中的特征和资源的能力。本文讨论了基于机器学习的网络切片架构中的特征和能力编排。首先，对切片规划、配置、调试和运行阶段的切片资源编排和分配进行了分析。接下来，我们强调了优化架构特征编排的需求，并建议使用机器学习嵌入的代理、联邦学习等方法。

    Network slicing is a crucial enabler and a trend for the Next Generation Mobile Network (NGMN) and various other new systems like the Internet of Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning are key elements with a crucial role in the network-slicing processes since the NS process needs to orchestrate resources and functionalities, and machine learning can potentially optimize the orchestration process. However, existing network-slicing architectures lack the ability to define intelligent approaches to orchestrate features and resources in the slicing process. This paper discusses machine learning-based orchestration of features and capabilities in network slicing architectures. Initially, the slice resource orchestration and allocation in the slicing planning, configuration, commissioning, and operation phases are analyzed. In sequence, we highlight the need for optimized architectural feature orchestration and recommend using ML-embed agents, federated
    
[^30]: 使用一步微调的时间序列Transformer的领域适应

    Domain Adaptation for Time series Transformers using One-step fine-tuning. (arXiv:2401.06524v1 [cs.LG])

    [http://arxiv.org/abs/2401.06524](http://arxiv.org/abs/2401.06524)

    本文介绍了一种在时间序列Transformer模型中使用一步微调的领域适应方法。通过在源领域上预训练模型，并在目标领域上进行微调，我们解决了时间理解不足、泛化挑战和数据偏移问题。此外，通过添加源领域数据到目标领域，我们提高了模型的鲁棒性，解决了遗忘灾难的问题。

    

    随着深度学习的Transformer在捕捉长程依赖性方面的能力受到广泛关注，它们在时间序列预测中面临着一些限制，包括对时间理解的不足、泛化挑战以及面对数据有限的领域中的数据偏移问题。此外，解决遗忘灾难的问题，即模型在接触到新数据时忘记了先前学到的信息，也是提高时间序列任务中Transformer鲁棒性需要关注的关键方面。为了解决这些限制，在本文中，我们在具有充足数据的源领域上对时间序列Transformer模型进行预训练，并在具有有限数据的目标域上进行微调。我们引入了“一步微调”方法，将一定百分比的源领域数据添加到目标域中，为模型提供了一些先验知识，并增强了其在目标域上的性能。

    The recent breakthrough of Transformers in deep learning has drawn significant attention of the time series community due to their ability to capture long-range dependencies. However, like other deep learning models, Transformers face limitations in time series prediction, including insufficient temporal understanding, generalization challenges, and data shift issues for the domains with limited data. Additionally, addressing the issue of catastrophic forgetting, where models forget previously learned information when exposed to new data, is another critical aspect that requires attention in enhancing the robustness of Transformers for time series tasks. To address these limitations, in this paper, we pre-train the time series Transformer model on a source domain with sufficient data and fine-tune it on the target domain with limited data. We introduce the \emph{One-step fine-tuning} approach, adding some percentage of source domain data to the target domains, providing the model with 
    
[^31]: 提升因果加法模型

    Boosting Causal Additive Models. (arXiv:2401.06523v1 [stat.ML])

    [http://arxiv.org/abs/2401.06523](http://arxiv.org/abs/2401.06523)

    我们提出了一种基于提升方法的学习加法结构方程模型的方法，通过引入一类评分函数和逐分量梯度下降的策略来确定变量间的因果顺序，实验证明其在高维数据集中具有竞争力和鲁棒性。

    

    我们提出了一种基于提升方法的学习加法结构方程模型(SEMs)的方法，重点关注确定变量间因果顺序的理论方面。我们引入了一类基于任意回归技术的评分函数，为其建立了一致地支持真实因果顺序的必要条件。我们的分析表明，提升方法与提前停止符合这些条件，从而为因果顺序提供了一致的评分函数。为了应对高维数据集带来的挑战，我们通过在加法结构方程模型空间中进行逐分量梯度下降来改进我们的方法。我们的模拟研究强调了我们的理论结果在较低维度下的有效性，并证明了我们的高维适应方法与最先进的方法相当。此外，我们的方法对于超参数的选择具有鲁棒性，使得程序易于调整。

    We present a boosting-based method to learn additive Structural Equation Models (SEMs) from observational data, with a focus on the theoretical aspects of determining the causal order among variables. We introduce a family of score functions based on arbitrary regression techniques, for which we establish necessary conditions to consistently favor the true causal ordering. Our analysis reveals that boosting with early stopping meets these criteria and thus offers a consistent score function for causal orderings. To address the challenges posed by high-dimensional data sets, we adapt our approach through a component-wise gradient descent in the space of additive SEMs. Our simulation study underlines our theoretical results for lower dimensions and demonstrates that our high-dimensional adaptation is competitive with state-of-the-art methods. In addition, it exhibits robustness with respect to the choice of the hyperparameters making the procedure easy to tune.
    
[^32]: 个性化强化学习与策略预算

    Personalized Reinforcement Learning with a Budget of Policies. (arXiv:2401.06514v1 [cs.LG])

    [http://arxiv.org/abs/2401.06514](http://arxiv.org/abs/2401.06514)

    该论文提出了一种称为表示的马尔可夫决策过程（r-MDPs）的框架，通过与一小组代表性策略的交互，实现个性化强化学习，并同时在满足监管约束的情况下优化整体社会福利。

    

    个性化机器学习在根据用户个体特征调整模型决策方面取得了成功，但在涉及到健康和自动驾驶等高风险领域的扩展时，受到了复杂的监管审批流程的限制。为了解决这一挑战，我们提出了一种新的框架，称为表示的马尔可夫决策过程（r-MDPs），旨在平衡个性化需求和监管约束。在r-MDPs中，通过与一小组代表性策略的交互，我们为多样化的用户群体提供服务，每个用户都有独特的偏好。我们的目标是高效地将每个用户与适当的代表性策略进行匹配，并同时优化这些策略以最大化整体社会福利。我们开发了两种深度强化学习算法，可以高效地解决r-MDPs的问题。这些算法受到了经典强化学习原理的启发。

    Personalization in machine learning (ML) tailors models' decisions to the individual characteristics of users. While this approach has seen success in areas like recommender systems, its expansion into high-stakes fields such as healthcare and autonomous driving is hindered by the extensive regulatory approval processes involved. To address this challenge, we propose a novel framework termed represented Markov Decision Processes (r-MDPs) that is designed to balance the need for personalization with the regulatory constraints. In an r-MDP, we cater to a diverse user population, each with unique preferences, through interaction with a small set of representative policies. Our objective is twofold: efficiently match each user to an appropriate representative policy and simultaneously optimize these policies to maximize overall social welfare. We develop two deep reinforcement learning algorithms that efficiently solve r-MDPs. These algorithms draw inspiration from the principles of classi
    
[^33]: ML-On-Rails: 在软件系统中保护机器学习模型的案例研究

    ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study. (arXiv:2401.06513v1 [cs.SE])

    [http://arxiv.org/abs/2401.06513](http://arxiv.org/abs/2401.06513)

    ML-On-Rails是一个旨在保护机器学习模型的协议，在软件系统中解决了安全性、可靠性和透明度等挑战，并在生产环境中提高了模型的鲁棒性。通过一项实际案例研究，我们强调了保护ML模型在生产中的重要性。

    

    机器学习（ML），特别是随着大型语言模型（LLM）的出现，已经显著改变了各个行业。然而，从ML模型的原型设计到在软件系统中的实际应用，存在着许多挑战。这些挑战主要涉及确保安全性、保证可靠性和透明度，从而影响了ML模型的鲁棒性和可信度。本文介绍了一个名为ML-On-Rails的协议，旨在保护ML模型，并为不同的ML任务建立一个明确定义的端点接口，以促进ML提供者和ML消费者（软件工程师）之间的清晰沟通。ML-On-Rails通过添加检测能力来提高ML模型的鲁棒性，以识别与实际生产中的ML相关的特定挑战。我们通过对MoveReminder应用的实际案例研究来评估ML-On-Rails协议。通过这个评估，我们强调了在生产中保护ML模型的重要性。

    Machine learning (ML), especially with the emergence of large language models (LLMs), has significantly transformed various industries. However, the transition from ML model prototyping to production use within software systems presents several challenges. These challenges primarily revolve around ensuring safety, security, and transparency, subsequently influencing the overall robustness and trustworthiness of ML models. In this paper, we introduce ML-On-Rails, a protocol designed to safeguard ML models, establish a well-defined endpoint interface for different ML tasks, and clear communication between ML providers and ML consumers (software engineers). ML-On-Rails enhances the robustness of ML models via incorporating detection capabilities to identify unique challenges specific to production ML. We evaluated the ML-On-Rails protocol through a real-world case study of the MoveReminder application. Through this evaluation, we emphasize the importance of safeguarding ML models in produ
    
[^34]: 使用多平面UNet完全自动化的脑MRI数据肿瘤分割

    Fully Automated Tumor Segmentation for Brain MRI data using Multiplanner UNet. (arXiv:2401.06499v1 [eess.IV])

    [http://arxiv.org/abs/2401.06499](http://arxiv.org/abs/2401.06499)

    本研究评估了使用多平面UNet方法在儿童脑肿瘤中自动分割不同肿瘤亚区的有效性，结果表明对于肿瘤核心类有较高的分割准确性，但在其他类别的分割上存在可变性。

    

    自动分割不同肿瘤区域对于准确诊断和治疗儿童脑肿瘤至关重要。本研究评估了多平面U-Net (MPUnet)方法在分割三个具有挑战性数据集：小儿肿瘤挑战(PED)、脑转移挑战(MET)和撒哈拉以南非洲成人脑胶质瘤(SSA)中不同肿瘤亚区的有效性。这些数据集代表了多样的情景和解剖变异，适合评估MPUnet模型的鲁棒性和泛化能力。通过利用多平面信息，MPUnet架构旨在提高分割准确性。我们的结果显示，在评估的挑战中，肿瘤核心(TC)类表现出相对较高的分割准确性。然而，其他类别的分割（如水肿和增强肿瘤(ET)区域）存在可变性。这些发现强调了分割的复杂性。

    Automated segmentation of distinct tumor regions is critical for accurate diagnosis and treatment planning in pediatric brain tumors. This study evaluates the efficacy of the Multi-Planner U-Net (MPUnet) approach in segmenting different tumor subregions across three challenging datasets: Pediatrics Tumor Challenge (PED), Brain Metastasis Challenge (MET), and Sub-Sahara-Africa Adult Glioma (SSA). These datasets represent diverse scenarios and anatomical variations, making them suitable for assessing the robustness and generalization capabilities of the MPUnet model. By utilizing multi-planar information, the MPUnet architecture aims to enhance segmentation accuracy. Our results show varying performance levels across the evaluated challenges, with the tumor core (TC) class demonstrating relatively higher segmentation accuracy. However, variability is observed in the segmentation of other classes, such as the edema and enhancing tumor (ET) regions. These findings emphasize the complexity 
    
[^35]: 大学辍学预测中的时间和群组间的变异性

    Temporal and Between-Group Variability in College Dropout Prediction. (arXiv:2401.06498v1 [cs.CY])

    [http://arxiv.org/abs/2401.06498](http://arxiv.org/abs/2401.06498)

    这项研究对大学辍学预测的时间和群组间变异性进行了系统评估，在大规模行政数据的基础上，发现第二年末的辍学预测比入学时更准确，并且大学表现和入学行为在预测中扮演重要角色。对于不同学生群体，大学平均绩点对于t学生的预测价值更高。

    

    大规模的行政数据经常用于高等教育中的辍学预警系统。然而，现有研究中术语和方法学差异较大，并且不完全了解不同建模决策的影响。本研究系统评估了机器学习模型在时间和不同学生群体中的贡献因素和预测性能。在美国一所大型公立大学的十二年行政数据的基础上，我们发现，在随机森林模型中，第二年末的辍学预测的AUC值比入学时高出20%。此外，入学时的大多数预测因素，包括人口统计学和高中表现，在预测重要性上很快被大学表现所取代，并在后期被入学行为所替代。关于不同学生群体的差异，大学平均绩点对于来自t学生的预测价值更高。

    Large-scale administrative data is a common input in early warning systems for college dropout in higher education. Still, the terminology and methodology vary significantly across existing studies, and the implications of different modeling decisions are not fully understood. This study provides a systematic evaluation of contributing factors and predictive performance of machine learning models over time and across different student groups. Drawing on twelve years of administrative data at a large public university in the US, we find that dropout prediction at the end of the second year has a 20% higher AUC than at the time of enrollment in a Random Forest model. Also, most predictive factors at the time of enrollment, including demographics and high school performance, are quickly superseded in predictive importance by college performance and in later stages by enrollment behavior. Regarding variability across student groups, college GPA has more predictive value for students from t
    
[^36]: BERT和DistilBERT中负责性别偏见的结构的研究

    An investigation of structures responsible for gender bias in BERT and DistilBERT. (arXiv:2401.06495v1 [cs.CL])

    [http://arxiv.org/abs/2401.06495](http://arxiv.org/abs/2401.06495)

    本文研究了BERT和DistilBERT中负责性别偏见的结构，讨论了它们在预测中的公正性问题。

    

    近年来，基于Transformer的大规模预训练语言模型（PLM）通过推动最先进技术在各种任务上的性能边界，改变了自然语言处理（NLP）领域。然而，这种性能提升伴随着复杂性的增加，因此这些模型的大小（可高达数十亿参数）在嵌入式设备或短推理时间任务上的部署受到限制。为了应对这种情况，出现了压缩模型（如DistilBERT），使得它们在越来越多影响我们日常生活的应用中可以被广泛使用。一个关键问题是PLM和其精简版本的预测公正性。在本文中，我们通过明确两个问题来对这个问题进行实证研究：（1）我们能否确定BERT（以及DistilBERT）中负责性别偏见的神经机制？（2）蒸馏是否倾向于加重或减轻这种偏见？

    In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate ge
    
[^37]: 批处理ICL: 有效，高效且无序地进行上下文学习

    Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])

    [http://arxiv.org/abs/2401.06469](http://arxiv.org/abs/2401.06469)

    本文提出了批处理ICL方法，通过将ICL视为一个元优化过程，开发出了一个有效、高效且无序的推理算法。通过聚合元梯度并将其应用于零-shot学习，该方法使LLM对ICL示例顺序无关，并且在实验证明其在大多数情况下优于其他排列方式，甚至超过了标准ICL的最佳顺序的性能。

    

    本文将上下文学习（ICL）视为一个元优化过程，解释了LLM对ICL示例顺序敏感的原因。这种理解使我们开发出了Batch-ICL，一种用于ICL的有效、高效且无序的推理算法。与标准的N-shot学习方法不同，Batch-ICL使用N个单独的1-shot前向计算，并聚合得到的元梯度。然后，将这些聚合的元梯度应用于零-shot学习以生成最终预测。这种批处理方法使LLM对ICL示例的顺序无关。通过大量实验证明，Batch-ICL一致优于大多数示例序列的排列方式。在某些情况下，甚至超过了标准ICL的最佳顺序的性能，同时减少了所需的计算资源。此外，我们还开发了Batch-ICL的一种新颖变体，其中包含多个"epochs"。

    In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
    
[^38]: Sanity Checks Revisited: 修复模型参数随机化测试的探索

    Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test. (arXiv:2401.06465v1 [cs.AI])

    [http://arxiv.org/abs/2401.06465](http://arxiv.org/abs/2401.06465)

    这项研究对模型参数随机化测试进行了探索和修复，通过引入平滑 MPRT 和高效 MPRT 两种改进方法，解决了实证解释的方法论注意事项，并通过实验结果证明这些改进方法可以提高度量可靠性，从而更可信地应用可解释人工智能方法。

    

    在可解释人工智能 (XAI) 社区中，模型参数随机化测试 (MPRT) 凭借其有力的评估原则而广受认可：解释函数应对模型函数参数的变化敏感。然而，最近的研究发现了关于 MPRT 的几个方法论上的注意事项。为了解决这些注意事项，我们引入了两种对原始 MPRT 进行改进的方法——平滑 MPRT 和高效 MPRT，前者通过采样来最小化噪音对评估结果的影响，后者通过在完全参数随机化后解释的复杂度上升来绕过对偏倚相似度测量的需求。我们的实验结果表明，这些提出的变体可以提高度量的可靠性，从而更可信地应用 XAI 方法。

    The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the eXplainable Artificial Intelligence (XAI) community for its well-motivated evaluative principle: that the explanation function should be sensitive to changes in the parameters of the model function. However, recent works have identified several methodological caveats for the empirical interpretation of MPRT. To address these caveats, we introduce two adaptations to the original MPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact that noise has on the evaluation results through sampling and the latter circumvents the need for biased similarity measurements by re-interpreting the test through the explanation's rise in complexity, after full parameter randomisation. Our experimental results demonstrate that these proposed variants lead to improved metric reliability, thus enabling a more trustworthy application of XAI methods.
    
[^39]: 用于正-无标签学习的自动化机器学习

    Automated Machine Learning for Positive-Unlabelled Learning. (arXiv:2401.06452v1 [cs.LG])

    [http://arxiv.org/abs/2401.06452](http://arxiv.org/abs/2401.06452)

    本论文提出了两个新的用于PU学习的Auto-ML系统，并对这三个Auto-ML系统进行了全面评估和比较，为PU学习任务的最优方法选择提供了解决方案。

    

    正-无标签（PU）学习是机器学习的一个新兴领域，旨在从由标记的正实例和未标记的实例组成的数据中学习分类器，这些实例在现实中可能是正或负，但其标签未知。过去二十年中提出了大量方法来解决PU学习问题，以至于为给定的PU学习任务选择最优方法成为一个挑战。我们之前的工作通过提出GA-Auto-PU，第一个用于PU学习的自动化机器学习（Auto-ML）系统来解决这个问题。在本研究中，我们提出了两个新的用于PU学习的Auto-ML系统：基于贝叶斯优化方法的BO-Auto-PU，以及基于一种新颖的进化/贝叶斯优化方法的EBO-Auto-PU。我们还对这三个Auto-ML系统进行了广泛的评估，将它们与彼此以及已建立的PU学习方法在60个数据集上进行比较（每个有3个PU学习方面的真实世界数据集，共20个）。

    Positive-Unlabelled (PU) learning is a growing field of machine learning that aims to learn classifiers from data consisting of labelled positive and unlabelled instances, which can be in reality positive or negative, but whose label is unknown. An extensive number of methods have been proposed to address PU learning over the last two decades, so many so that selecting an optimal method for a given PU learning task presents a challenge. Our previous work has addressed this by proposing GA-Auto-PU, the first Automated Machine Learning (Auto-ML) system for PU learning. In this work, we propose two new Auto-ML systems for PU learning: BO-Auto-PU, based on a Bayesian Optimisation approach, and EBO-Auto-PU, based on a novel evolutionary/Bayesian optimisation approach. We also present an extensive evaluation of the three Auto-ML systems, comparing them to each other and to well-established PU learning methods across 60 datasets (20 real-world datasets, each with 3 versions in terms of PU lea
    
[^40]: 在基于社交网络的物品推荐中，用Transformer层改进图卷积网络

    Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation. (arXiv:2401.06436v1 [cs.LG])

    [http://arxiv.org/abs/2401.06436](http://arxiv.org/abs/2401.06436)

    本文提出了一种在社交网络中预测评分的方法，该方法通过在GCN模型中引入Transformer层，在节点嵌入方面取得了更好的性能。

    

    在这项工作中，我们提出了一种改进GCN在社交网络中预测评分的方法。我们的模型在标准模型的基础上扩展了几层Transformer架构。论文的主要焦点是网络中节点嵌入的编码器架构。使用来自基于图的卷积层的嵌入层，注意机制可以重新排列特征空间，为下游任务获取更高效的嵌入。实验表明，我们提出的架构在传统的链接预测任务上表现优于GCN。

    In this work, we have proposed an approach for improving the GCN for predicting ratings in social networks. Our model is expanded from the standard model with several layers of transformer architecture. The main focus of the paper is on the encoder architecture for node embedding in the network. Using the embedding layer from the graph-based convolution layer, the attention mechanism could rearrange the feature space to get a more efficient embedding for the downstream task. The experiments showed that our proposed architecture achieves better performance than GCN on the traditional link prediction task.
    
[^41]: 异构低秩近似用于设备本地基础模型联邦微调

    Heterogeneous Low-Rank Approximation for Federated Fine-tuning of On-Device Foundation Models. (arXiv:2401.06432v1 [cs.LG])

    [http://arxiv.org/abs/2401.06432](http://arxiv.org/abs/2401.06432)

    本文提出了一种用于异构设备的设备本地基础模型联邦微调的参数高效方法，使用了异构低秩近似（LoRA），解决了资源受限和异构设备的挑战。

    

    大型基础模型（FMs）通过微调适应特定领域或任务。联邦学习（FL）进一步利用设备上的本地数据实现了私有化的FM微调。然而，标准FMs的大尺寸对于资源受限和异构设备带来了挑战。为了解决这个问题，我们考虑了参数尺寸较小的FM，称为设备本地FM（ODFMs）。虽然ODFMs允许设备上的推断，但计算限制仍然阻碍了高效的联邦微调。我们提出了一种参数高效的ODFM联邦微调方法，使用了异构低秩近似（LoRA），解决了系统和数据异质性的问题。我们发现，同质LoRA秩面临着过拟合和收敛缓慢之间的折衷，提出了HetLoRA，它在客户端之间使用异质的秩并消除了同质HetLoRA的缺点。通过在本地应用秩自剪枝，并在服务器上应用稀疏加权聚合，我们完成了摘要中的内容。

    Large foundation models (FMs) adapt surprisingly well to specific domains or tasks with fine-tuning. Federated learning (FL) further enables private FM fine-tuning using the local data on devices. However, the standard FMs' large size poses challenges for resource-constrained and heterogeneous devices. To address this, we consider FMs with reduced parameter sizes, referred to as on-device FMs (ODFMs). While ODFMs allow on-device inference, computational constraints still hinder efficient federated fine-tuning. We propose a parameter-efficient federated fine-tuning method for ODFMs using heterogeneous low-rank approximations (LoRAs) that addresses system and data heterogeneity. We show that homogeneous LoRA ranks face a trade-off between overfitting and slow convergence, and propose HetLoRA, which employs heterogeneous ranks across clients and eliminates the shortcomings of homogeneous HetLoRA. By applying rank self-pruning locally and sparsity-weighted aggregation at the server, we com
    
[^42]: 使用符合预测的不确定性量化的地球观测概率机器学习

    Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction. (arXiv:2401.06421v1 [cs.LG])

    [http://arxiv.org/abs/2401.06421](http://arxiv.org/abs/2401.06421)

    本论文介绍了地球观测领域中使用符合预测进行不确定性量化的方法。与其他方法不同，符合预测不需要访问底层模型和训练数据集，并同时提供统计上有效和有信息的预测区域，同时保持计算效率。

    

    当使用人工智能系统进行决策时，不可靠的预测可能会导致负面后果。符合预测提供了一个与模型无关的不确定性量化框架，可以应用于任何数据集，无论其分布如何。与其他像素级的不确定性量化方法不同，符合预测不需要访问底层模型和训练数据集，并同时提供统计上有效和有信息的预测区域，同时保持计算效率。

    Unreliable predictions can occur when using artificial intelligence (AI) systems with negative consequences for downstream applications, particularly when employed for decision-making. Conformal prediction provides a model-agnostic framework for uncertainty quantification that can be applied to any dataset, irrespective of its distribution, post hoc. In contrast to other pixel-level uncertainty quantification methods, conformal prediction operates without requiring access to the underlying model and training dataset, concurrently offering statistically valid and informative prediction regions, all while maintaining computational efficiency. In response to the increased need to report uncertainty alongside point predictions, we bring attention to the promise of conformal prediction within the domain of Earth Observation (EO) applications. To accomplish this, we assess the current state of uncertainty quantification in the EO domain and found that only 20% of the reviewed Google Earth En
    
[^43]: 不可能任务：语言模型

    Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])

    [http://arxiv.org/abs/2401.06416](http://arxiv.org/abs/2401.06416)

    本文为了支持大型语言模型(LLMs)能够学习不可能的语言的观点，开发了一组人工合成的不可能语言，并通过评估GPT-2小型模型的学习能力得出了结论。

    

    Chomsky和其他人直接声称，大型语言模型(LLMs)能够学习人类无法学习的可能和不可能的语言。然而，很少有发表的实验证据支持这样的说法。在这里，我们通过系统地改变英文数据的词序和语法规则，开发了一组不可能的合成语言，每种语言的复杂程度不同。这些语言位于一个不可能的连续体上：一端是本质上不可能的语言，例如英文单词的随机和不可逆的洗牌，而另一端是在语言学上常被认为是不可能的语言，特别是基于计算词位置的规则。我们报告了广泛的评估来评估GPT-2小型模型学习这些无可争议的不可能语言的能力，并且至关重要的是，在整个过程中进行了这些评估。

    Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
    
[^44]: 知识驱动的机器学习在癌症诊断和预后中的应用：综述

    Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review. (arXiv:2401.06406v1 [cs.LG])

    [http://arxiv.org/abs/2401.06406](http://arxiv.org/abs/2401.06406)

    知识驱动的机器学习在癌症诊断和预后中的应用已经取得了一定的进展。该方法将生物医学知识与数据驱动模型相结合，能够提高模型结果的准确性、鲁棒性和可解释性。这篇综述回顾了最新研究，并强调了四个主要特点。

    

    癌症仍然是医学领域中治疗最具挑战性的疾病之一。机器学习已经使得对于癌症诊断和预后的丰富的多组学数据和医学图像进行了深入分析。尽管取得了这些进展，机器学习模型面临着一些挑战，包括标记样本数量有限、高维数据类型之间的复杂相互作用、患者内部和肿瘤内部的固有异质性以及与现有生物医学知识的解释和一致性。克服这些挑战的一种方法是将生物医学知识纳入数据驱动模型中，这已经被证明具有提高模型结果的准确性、鲁棒性和可解释性的潜力。在本文中，我们综述了采用了融合生物医学知识和数据的知识驱动机器学习，在癌症诊断和预后中的最新研究。突出了四个主要特点。

    Cancer remains one of the most challenging diseases to treat in the medical field. Machine learning has enabled in-depth analysis of rich multi-omics profiles and medical imaging for cancer diagnosis and prognosis. Despite these advancements, machine learning models face challenges stemming from limited labeled sample sizes, the intricate interplay of high-dimensionality data types, the inherent heterogeneity observed among patients and within tumors, and concerns about interpretability and consistency with existing biomedical knowledge. One approach to surmount these challenges is to integrate biomedical knowledge into data-driven models, which has proven potential to improve the accuracy, robustness, and interpretability of model results. Here, we review the state-of-the-art machine learning studies that adopted the fusion of biomedical knowledge and data, termed knowledge-informed machine learning, for cancer diagnosis and prognosis. Emphasizing the properties inherent in four prima
    
[^45]: 注意力、蒸馏和表格化：走向实用的基于神经网络的预取模型

    Attention, Distillation, and Tabularization: Towards Practical Neural Network-Based Prefetching. (arXiv:2401.06362v1 [cs.NE])

    [http://arxiv.org/abs/2401.06362](http://arxiv.org/abs/2401.06362)

    我们提出了一种基于表格化的方法，通过将注意力模型转换为层次结构的表格查找，显著降低了预取模型的复杂度和推理延迟，同时保持了高准确性。通过我们的方法，我们开发了一个DART预取模型，在减少计算量的情况下只有轻微的性能下降。

    

    基于注意力的神经网络在准确的内存访问预测中表现出了高效性，这是数据预取的一个关键步骤。然而，这些模型所带来的计算开销造成了高推理延迟，限制了它们作为实际预取模型的可行性。为了弥合这一差距，我们提出了一种基于表格化的新方法，该方法显著降低了模型复杂度和推理延迟，同时又不牺牲预测准确性。我们的新颖的表格化方法将一个经过蒸馏的具有高精确度的注意力模型作为输入，将其昂贵的矩阵乘法转换成快速表格查找的层次结构。作为上述方法的示例，我们开发了DART，一个由简单表格层次结构组成的预取模型。在F1得分下降了0.09的情况下，DART从大型注意力模型中减少了99.99%的算术运算，从蒸馏模型中减少了91.83%的运算量。

    Attention-based Neural Networks (NN) have demonstrated their effectiveness in accurate memory access prediction, an essential step in data prefetching. However, the substantial computational overheads associated with these models result in high inference latency, limiting their feasibility as practical prefetchers. To close the gap, we propose a new approach based on tabularization that significantly reduces model complexity and inference latency without sacrificing prediction accuracy. Our novel tabularization methodology takes as input a distilled, yet highly accurate attention-based model for memory access prediction and efficiently converts its expensive matrix multiplications into a hierarchy of fast table lookups. As an exemplar of the above approach, we develop DART, a prefetcher comprised of a simple hierarchy of tables. With a modest 0.09 drop in F1-score, DART reduces 99.99% of arithmetic operations from the large attention-based model and 91.83% from the distilled model. DAR
    
[^46]: 对知识蒸馏中参数选择的实证研究

    An Empirical Investigation into the Effect of Parameter Choices in Knowledge Distillation. (arXiv:2401.06356v1 [cs.LG])

    [http://arxiv.org/abs/2401.06356](http://arxiv.org/abs/2401.06356)

    本文对知识蒸馏中参数选择的影响进行了大规模实证研究，揭示了不同选项对学生性能的整体影响，并找到了在各方面表现良好的单一配置。

    

    我们展示了一项关于参数配置选择对知识蒸馏性能影响的大规模实证研究。其中一个示例是教师和学生预测之间距离的度量，在此方面常见的选择包括均方误差（MSE）和KL散度。尽管已经进行了一些散乱的努力来理解这些选项之间的差异，但是知识蒸馏领域仍然缺乏一个对它们对学生性能的整体影响进行系统研究的工作。我们在这篇论文中采用实证方法来探索这个问题，试图找出这些选择在包括4个NLP任务和3种学生规模的13个数据集上对学生性能的影响程度。我们衡量了做出次优选择的代价，并确定了一个在各方面表现良好的单一配置。

    We present a large-scale empirical study of how choices of configuration parameters affect performance in knowledge distillation (KD). An example of such a KD parameter is the measure of distance between the predictions of the teacher and the student, common choices for which include the mean squared error (MSE) and the KL-divergence. Although scattered efforts have been made to understand the differences between such options, the KD literature still lacks a systematic study on their general effect on student performance. We take an empirical approach to this question in this paper, seeking to find out the extent to which such choices influence student performance across 13 datasets from 4 NLP tasks and 3 student sizes. We quantify the cost of making sub-optimal choices and identify a single configuration that performs well across the board.
    
[^47]: 通过基于扩散的蒙特卡罗方法实现更快的采样而无需等温性  (arXiv:2401.06325v1 [stat.ML])

    Faster Sampling without Isoperimetry via Diffusion-based Monte Carlo. (arXiv:2401.06325v1 [stat.ML])

    [http://arxiv.org/abs/2401.06325](http://arxiv.org/abs/2401.06325)

    本文提出了一种更高效的基于扩散的蒙特卡罗采样算法RS-DMC，通过改进评分估计方法来解决原始DMC算法的梯度复杂性问题。该算法将整个扩散过程划分为多个段落，并使用递归评分估计实现更快的采样速度。

    

    为了从一般的目标分布$p_*\propto e^{-f_*}$中进行采样，超越等温条件，Huang等人（2023）提出了通过反向扩散进行采样，从而产生了基于扩散的蒙特卡罗（DMC）方法。具体而言，DMC遵循扩散过程的反向SDE，将目标分布转化为标准高斯分布，并利用非参数评分估计。然而，原始的DMC算法遇到了高梯度复杂性的问题，导致对获得的样本的误差容差$\epsilon$的依赖成指数增长。在本文中，我们证明了DMC的高复杂性源于其冗余的评分估计设计，并提出了一种更高效的算法，称为RS-DMC，基于一种新颖的递归评分估计方法。特别地，我们首先将整个扩散过程划分为多个段落，然后将评分估计步骤（在任何时间步骤）形式化为一系列相互连接的均值估计步骤。

    To sample from a general target distribution $p_*\propto e^{-f_*}$ beyond the isoperimetric condition, Huang et al. (2023) proposed to perform sampling through reverse diffusion, giving rise to Diffusion-based Monte Carlo (DMC). Specifically, DMC follows the reverse SDE of a diffusion process that transforms the target distribution to the standard Gaussian, utilizing a non-parametric score estimation. However, the original DMC algorithm encountered high gradient complexity, resulting in an exponential dependency on the error tolerance $\epsilon$ of the obtained samples. In this paper, we demonstrate that the high complexity of DMC originates from its redundant design of score estimation, and proposed a more efficient algorithm, called RS-DMC, based on a novel recursive score estimation method. In particular, we first divide the entire diffusion process into multiple segments and then formulate the score estimation step (at any time step) as a series of interconnected mean estimation an
    
[^48]: 通过强化学习在动态系统公平性中取得平衡

    Striking a Balance in Fairness for Dynamic Systems Through Reinforcement Learning. (arXiv:2401.06318v1 [cs.LG])

    [http://arxiv.org/abs/2401.06318](http://arxiv.org/abs/2401.06318)

    本文研究了在决策模型操作的动态人口中的公平性问题，并提出了一种通过强化学习结合各种公平考虑的算法框架。通过预处理和处理中的方法，我们的方法能够在传统公平性、长期公平性和效用之间取得平衡。

    

    在公平机器学习领域取得了重大进展，但大多数研究集中在决策模型在静态人群上运行的情况下。本文研究了决策模型操作的人口是动态的情况下的公平性。每个决策可能会改变特征或用户行为的基础分布。我们通过马尔科夫决策过程（MDP）建模动态系统。在承认传统公平性概念和长期公平性是不同要求且可能不一致的情况下，我们提出了一种算法框架，将各种公平考虑与强化学习相结合，同时使用预处理和处理中的方法。三个案例研究显示我们的方法能够在传统公平性、长期公平性和效用之间取得平衡。

    While significant advancements have been made in the field of fair machine learning, the majority of studies focus on scenarios where the decision model operates on a static population. In this paper, we study fairness in dynamic systems where sequential decisions are made. Each decision may shift the underlying distribution of features or user behavior. We model the dynamic system through a Markov Decision Process (MDP). By acknowledging that traditional fairness notions and long-term fairness are distinct requirements that may not necessarily align with one another, we propose an algorithmic framework to integrate various fairness considerations with reinforcement learning using both pre-processing and in-processing approaches. Three case studies show that our method can strike a balance between traditional fairness notions, long-term fairness, and utility.
    
[^49]: 一种用于分布式、动态6G应用的语义感知多址访问方案

    A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications. (arXiv:2401.06308v1 [cs.NI])

    [http://arxiv.org/abs/2401.06308](http://arxiv.org/abs/2401.06308)

    本文提出了一种语义感知多址访问方案，旨在优化资源利用与公平性的权衡，并考虑用户数据的相关性，以满足未来6G应用的要求和特性。

    

    语义感知范式的出现为创新的服务提供了机会，尤其是在基于6G的应用环境中。尽管在语义提取技术方面取得了显著进展，但将语义信息纳入资源分配决策仍处于早期阶段，缺乏对未来系统需求和特性的考虑。为此，本文引入了一种新的无线频谱多址访问问题的建模。它旨在优化利用率与公平性的权衡，使用α-公平度量，并通过引入自助吞吐量和协助吞吐量的概念来考虑用户数据的相关性。首先，分析了该问题，找出了最优解。接下来，提出了一种基于模型无关的多主体深度强化学习技术的语义感知多智能体双重和决斗深度Q学习 (SAMA-D3QL) 方法。

    The emergence of the semantic-aware paradigm presents opportunities for innovative services, especially in the context of 6G-based applications. Although significant progress has been made in semantic extraction techniques, the incorporation of semantic information into resource allocation decision-making is still in its early stages, lacking consideration of the requirements and characteristics of future systems. In response, this paper introduces a novel formulation for the problem of multiple access to the wireless spectrum. It aims to optimize the utilization-fairness trade-off, using the $\alpha$-fairness metric, while accounting for user data correlation by introducing the concepts of self- and assisted throughputs. Initially, the problem is analyzed to identify its optimal solution. Subsequently, a Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL) technique is proposed. This method is grounded in Model-free Multi-Agent Deep Reinforcement Learning (MADRL),
    
[^50]: 量子神经网络作为量子信息译码器的优势

    Advantage of Quantum Neural Networks as Quantum Information Decoders. (arXiv:2401.06300v1 [quant-ph])

    [http://arxiv.org/abs/2401.06300](http://arxiv.org/abs/2401.06300)

    本论文研究了量子神经网络（QNN）作为解码器在解码量子信息时的优势。实验证明，QNN解码器在读取错误方面几乎具有二次改进。这使得在解码实际的量子纠错码时可以探索更广泛的非稳定器码。

    

    一种保护量子信息免受噪声引起的错误的有希望策略是将其编码到拓扑量子存储设备的低能态中。然而，在实际情况下，来自这种存储器的读取错误的情况尚不清楚。我们研究了在存在泛函扰动（如瞬态失调）的情况下，解码编码在拓扑稳定器哈密顿量的基态中的量子信息的问题。首先，我们证明了标准的稳定器型错误校正和解码方案在这种扰动的量子码中工作得相当好，通过展示解码错误在底层无扰动码的距离上呈指数衰减。然后，我们证明了量子神经网络（QNN）译码器在读取错误方面提供了几乎二次的改进。因此，我们证明了在解码实际的量子纠错码方面使用QNN的明显优势，并且我们的结果使得探索更广泛的非稳定器码成为可能。

    A promising strategy to protect quantum information from noise-induced errors is to encode it into the low-energy states of a topological quantum memory device. However, readout errors from such memory under realistic settings is less understood. We study the problem of decoding quantum information encoded in the groundspaces of topological stabilizer Hamiltonians in the presence of generic perturbations, such as quenched disorder. We first prove that the standard stabilizer-based error correction and decoding schemes work adequately well in such perturbed quantum codes by showing that the decoding error diminishes exponentially in the distance of the underlying unperturbed code. We then prove that Quantum Neural Network (QNN) decoders provide an almost quadratic improvement on the readout error. Thus, we demonstrate provable advantage of using QNNs for decoding realistic quantum error-correcting codes, and our result enables the exploration of a wider range of non-stabilizer codes in 
    
[^51]: 揭秘变分扩散模型

    Demystifying Variational Diffusion Models. (arXiv:2401.06281v1 [cs.LG])

    [http://arxiv.org/abs/2401.06281](http://arxiv.org/abs/2401.06281)

    该论文通过使用有向图模型和变分贝叶斯原理，揭示了变分扩散模型的原理和连接，为非专业统计物理领域的读者提供了更容易理解的介绍。

    

    尽管扩散模型越来越受欢迎，但对于非平衡统计物理领域的初学者来说，对该模型类的深入理解仍然有些困难。考虑到这一点，我们通过使用有向图模型和变分贝叶斯原理，提供了一个我们认为更简单易懂的扩散模型介绍，这对于一般读者来说需要的先决条件相对较少。我们的阐述构成了一个全面的技术综述，从深度潜变量模型等基本概念到连续时间扩散模型的最新进展，突出了模型类之间的理论联系。我们尽可能地提供了在初始工作中被省略的额外数学洞察，以帮助理解，同时避免引入新的符号表示。我们希望这篇文章对于该领域的研究人员和实践者来说，能作为一个有用的教育补充材料。

    Despite the growing popularity of diffusion models, gaining a deep understanding of the model class remains somewhat elusive for the uninitiated in non-equilibrium statistical physics. With that in mind, we present what we believe is a more straightforward introduction to diffusion models using directed graphical modelling and variational Bayesian principles, which imposes relatively fewer prerequisites on the average reader. Our exposition constitutes a comprehensive technical review spanning from foundational concepts like deep latent variable models to recent advances in continuous-time diffusion-based modelling, highlighting theoretical connections between model classes along the way. We provide additional mathematical insights that were omitted in the seminal works whenever possible to aid in understanding, while avoiding the introduction of new notation. We envision this article serving as a useful educational supplement for both researchers and practitioners in the area, and we 
    
[^52]: 图谱信号处理中的采样和唯一性集

    Sampling and Uniqueness Sets in Graphon Signal Processing. (arXiv:2401.06279v1 [cs.LG])

    [http://arxiv.org/abs/2401.06279](http://arxiv.org/abs/2401.06279)

    本论文研究了大规模图谱族上的采样集特性，并将“可移除集合”和“唯一性集合”的概念推广到了图谱信号处理中。通过利用图谱表示法，可以比较具有不同节点数和边数以及不同节点标记的图谱之间的采样集，并证明了具有相同图谱表示的采样集序列的收敛性。

    

    在这项工作中，我们通过利用图谱和图极限的理论，研究了大规模图谱族上采样集的特性。为此，我们将“可移除集合”和“唯一性集合”的概念扩展到了图谱信号领域，这些概念最初是用于分析图谱上的信号的。我们给出了$\Lambda-$可移除集合的正式定义，并在得到从图谱中一个给定$\Lambda-$可移除集合的补集中的样本时，给出了一个频带有限的图谱信号可以以唯一方式表示的条件。通过利用这些结果，我们证明了图谱表示法可以作为一种共同的框架来比较具有不同节点数和边数以及不同节点标记的图谱之间的采样集。此外，对于收敛到一个图谱的图序列，我们还证明了具有相同$[0,1]$中图谱表示的采样集序列也是收敛的。我们利用这种收敛性。

    In this work, we study the properties of sampling sets on families of large graphs by leveraging the theory of graphons and graph limits. To this end, we extend to graphon signals the notion of removable and uniqueness sets, which was developed originally for the analysis of signals on graphs. We state the formal definition of a $\Lambda-$removable set and conditions under which a bandlimited graphon signal can be represented in a unique way when its samples are obtained from the complement of a given $\Lambda-$removable set in the graphon. By leveraging such results we show that graphon representations of graphs and graph signals can be used as a common framework to compare sampling sets between graphs with different numbers of nodes and edges, and different node labelings. Additionally, given a sequence of graphs that converges to a graphon, we show that the sequences of sampling sets whose graphon representation is identical in $[0,1]$ are convergent as well. We exploit the converge
    
[^53]: 自我监督预训练在胃肠内镜视觉问题中的研究

    A Study on Self-Supervised Pretraining for Vision Problems in Gastrointestinal Endoscopy. (arXiv:2401.06278v1 [cs.CV])

    [http://arxiv.org/abs/2401.06278](http://arxiv.org/abs/2401.06278)

    本研究研究了自我监督预训练在胃肠内镜视觉问题中的应用，结果发现相对于有监督预训练，自我监督预训练通常能够产生更适合的骨干网络，并且使用ImageNet-1k进行自我监督预训练通常比使用Hyperkvasir-unlabelled更合适。

    

    胃肠内镜（GIE）中的视觉任务通常使用在ImageNet-1k上以有监督方式预训练的图像编码器作为骨干网络。然而，现代自我监督预训练算法和一个最近的包含10万张未标记GIE图像的数据集（Hyperkvasir-unlabelled）的使用可能会带来改进。在这项工作中，我们研究了在一系列GIE视觉任务中，使用ResNet50和ViT-B骨干网络以自我监督和有监督方式预训练的模型的微调性能。除了确定每个任务最适合的预训练流程和骨干网络架构外，我们的结果表明：相对于有监督预训练，自我监督预训练通常能够产生更适合GIE视觉任务的骨干网络；自我监督预训练使用ImageNet-1k通常比使用Hyperkvasir-unlabelled预训练更合适，但有一个明显的例外情况。

    Solutions to vision tasks in gastrointestinal endoscopy (GIE) conventionally use image encoders pretrained in a supervised manner with ImageNet-1k as backbones. However, the use of modern self-supervised pretraining algorithms and a recent dataset of 100k unlabelled GIE images (Hyperkvasir-unlabelled) may allow for improvements. In this work, we study the fine-tuned performance of models with ResNet50 and ViT-B backbones pretrained in self-supervised and supervised manners with ImageNet-1k and Hyperkvasir-unlabelled (self-supervised only) in a range of GIE vision tasks. In addition to identifying the most suitable pretraining pipeline and backbone architecture for each task, out of those considered, our results suggest: that self-supervised pretraining generally produces more suitable backbones for GIE vision tasks than supervised pretraining; that self-supervised pretraining with ImageNet-1k is typically more suitable than pretraining with Hyperkvasir-unlabelled, with the notable exce
    
[^54]: Qrlew: 将SQL重写为差分隐私SQL

    Qrlew: Rewriting SQL into Differentially Private SQL. (arXiv:2401.06273v1 [cs.DB])

    [http://arxiv.org/abs/2401.06273](http://arxiv.org/abs/2401.06273)

    Qrlew是一个开源库，能够将SQL查询重写为具有差分隐私等效性的查询，从而在保持数据所有者隐私的同时实现标准数据查询和执行。

    

    本文介绍了Qrlew，一个开源库，它可以将SQL查询解析为Relations，即中间表示，这些表示可以跟踪丰富的数据类型、值范围和行所有权，以便可以轻松地重写为具有差分隐私等效性，并转换回SQL查询，在各种标准数据存储中执行。使用Qrlew，数据从业者可以用标准SQL表达其数据查询；数据所有者可以在输出上享有强大的隐私保证下运行重写的查询，无需任何技术集成；并且查询重写可以由一个由所有者信任但可能属于不同组织的隐私专家来操作。

    This paper introduces Qrlew, an open source library that can parse SQL queries into Relations -- an intermediate representation -- that keeps track of rich data types, value ranges, and row ownership; so that they can easily be rewritten into differentially-private equivalent and turned back into SQL queries for execution in a variety of standard data stores.  With Qrlew, a data practitioner can express their data queries in standard SQL; the data owner can run the rewritten query without any technical integration and with strong privacy guarantees on the output; and the query rewriting can be operated by a privacy-expert who must be trusted by the owner, but may belong to a separate organization.
    
[^55]: FedTabDiff: 用于合成混合类型表格数据生成的扩散概率模型的联邦学习

    FedTabDiff: Federated Learning of Diffusion Probabilistic Models for Synthetic Mixed-Type Tabular Data Generation. (arXiv:2401.06263v1 [cs.LG])

    [http://arxiv.org/abs/2401.06263](http://arxiv.org/abs/2401.06263)

    本文介绍了一种名为"FedTabDiff"的联邦学习方法，用于生成高保真度的混合类型表格数据，而无需集中访问原始表格数据集。该方法利用了"去噪扩散概率模型"（DDPMs）的优势，解决了表格数据中的混合属性类型和隐含关系等固有复杂性，并实现了分散学习方案来保护数据隐私和本地性。在真实的金融数据集上进行了实验评估。

    

    在处理包含敏感信息的领域（如金融和医疗）的真实合成表格数据生成时，要保护隐私面临重大挑战。本文介绍了一种名为"FedTabDiff"的方法，用于生成高保真度的混合类型表格数据，而无需集中访问原始表格数据集。我们的方法利用了"去噪扩散概率模型"（DDPMs）的优势，解决了表格数据中的混合属性类型和隐含关系等固有复杂性。更重要的是，FedTabDiff实现了一种分散学习方案，允许多个实体在尊重数据隐私和本地性的同时共同训练生成模型。我们将DDPMs扩展到联邦学习环境中的表格数据生成，包括同步更新方案和加权平均以实现有效的模型聚合。在真实的金融数据集上进行了实验评估。

    Realistic synthetic tabular data generation encounters significant challenges in preserving privacy, especially when dealing with sensitive information in domains like finance and healthcare. In this paper, we introduce \textit{Federated Tabular Diffusion} (FedTabDiff) for generating high-fidelity mixed-type tabular data without centralized access to the original tabular datasets. Leveraging the strengths of \textit{Denoising Diffusion Probabilistic Models} (DDPMs), our approach addresses the inherent complexities in tabular data, such as mixed attribute types and implicit relationships. More critically, FedTabDiff realizes a decentralized learning scheme that permits multiple entities to collaboratively train a generative model while respecting data privacy and locality. We extend DDPMs into the federated setting for tabular data generation, which includes a synchronous update scheme and weighted averaging for effective model aggregation. Experimental evaluations on real-world financi
    
[^56]: AGSPNet: 一种基于农业地理场景约束的无人机高分辨率图像细粒度作物语义变化检测框架

    AGSPNet: A framework for parcel-scale crop fine-grained semantic change detection from UAV high-resolution imagery with agricultural geographic scene constraints. (arXiv:2401.06252v1 [cs.CV])

    [http://arxiv.org/abs/2401.06252](http://arxiv.org/abs/2401.06252)

    AGSPNet是一种基于农业地理场景约束的作物语义变化检测框架，能够实时准确地检测细粒度作物栽培的变化信息，并满足农业实际工程应用的需求。

    

    实时准确地获取作物栽培的细微变化信息对于作物生长监测、产量预测和农业结构调整具有重要意义。针对可见光高分辨率无人机图像在不同阶段存在严重光谱混淆，现有语义变化检测算法受到大型复杂背景干扰和椒盐噪声的问题，为了有效提取作物的深度图像特征，并满足农业实际工程应用的需求，本文设计和提出了一种农业地理场景和基于地块尺度约束的作物语义变化检测框架（AGSPNet）。AGSPNet框架包括农业地理场景（AGS）分割模块、地块边缘提取模块和作物语义变化检测模块。同时，我们还设计了一个针对农业监测的无人机图像语义变化检测数据集（CSCD），包含多个语义变化场景。

    Real-time and accurate information on fine-grained changes in crop cultivation is of great significance for crop growth monitoring, yield prediction and agricultural structure adjustment. Aiming at the problems of serious spectral confusion in visible high-resolution unmanned aerial vehicle (UAV) images of different phases, interference of large complex background and salt-and-pepper noise by existing semantic change detection (SCD) algorithms, in order to effectively extract deep image features of crops and meet the demand of agricultural practical engineering applications, this paper designs and proposes an agricultural geographic scene and parcel-scale constrained SCD framework for crops (AGSPNet). AGSPNet framework contains three parts: agricultural geographic scene (AGS) division module, parcel edge extraction module and crop SCD module. Meanwhile, we produce and introduce an UAV image SCD dataset (CSCD) dedicated to agricultural monitoring, encompassing multiple semantic variatio
    
[^57]: 多视图集成学习中保持语义一致的特征分区

    Semantic-Preserving Feature Partitioning for Multi-View Ensemble Learning. (arXiv:2401.06251v1 [cs.LG])

    [http://arxiv.org/abs/2401.06251](http://arxiv.org/abs/2401.06251)

    本研究提出了一种保持语义一致的特征分区算法（SPFP），用于多视图集成学习（MEL）。通过将数据集有效分区为多个语义一致的视图，SPFP算法显著提高了MEL过程的效果。在各种现实数据集上的实验中，该方法展示了显著的有效性，并在高泛化性能可达的情况下提高了模型的准确性和不确定性度量。

    

    在机器学习中，数据的指数增长及相关联的“维度灾难”给我们带来了巨大的挑战，特别是对于广泛而稀疏的数据集。为了应对这些挑战，多视图集成学习（MEL）作为一种创新方法出现，特征分区（FP）在构建MEL的人工视图中发挥着关键作用。我们的研究引入了一种基于信息论的全新方法——保持语义一致的特征分区（SPFP）算法。SPFP算法将数据集有效地分区为多个语义一致的视图，增强了MEL过程。通过对八个真实世界数据集进行广泛实验，从高维空间有限样本到低维空间大样本，我们的方法展示了显著的有效性。在高泛化性能可达的场景下，它保持了模型的准确性，同时显著改进了不确定性度量。相反，它保留了未变化的特征。

    In machine learning, the exponential growth of data and the associated ``curse of dimensionality'' pose significant challenges, particularly with expansive yet sparse datasets. Addressing these challenges, multi-view ensemble learning (MEL) has emerged as a transformative approach, with feature partitioning (FP) playing a pivotal role in constructing artificial views for MEL. Our study introduces the Semantic-Preserving Feature Partitioning (SPFP) algorithm, a novel method grounded in information theory. The SPFP algorithm effectively partitions datasets into multiple semantically consistent views, enhancing the MEL process. Through extensive experiments on eight real-world datasets, ranging from high-dimensional with limited instances to low-dimensional with high instances, our method demonstrates notable efficacy. It maintains model accuracy while significantly improving uncertainty measures in scenarios where high generalization performance is achievable. Conversely, it retains unce
    
[^58]: WISE: 基于地下扩展的全波形变分推断方法

    WISE: full-Waveform variational Inference via Subsurface Extensions. (arXiv:2401.06230v1 [physics.geo-ph])

    [http://arxiv.org/abs/2401.06230](http://arxiv.org/abs/2401.06230)

    WISE是一种通过地下扩展进行全波形变分推断的方法，能够准确量化偏移速度模型对成像的影响，并且不依赖于准确的初始速度模型。

    

    我们引入了一种利用变分推断和条件归一化流来量化偏移速度模型及其对成像的影响的全波形反演概率技术。我们的方法将生成式人工智能与物理知识驱动的共享图像收集相结合，减少了对准确的初始速度模型的依赖。考虑到的案例研究证明了该方法在根据数据生成偏移速度模型方面的有效性。这些模型被用来量化后续成像过程中的振幅和定位效应。

    We introduce a probabilistic technique for full-waveform inversion, employing variational inference and conditional normalizing flows to quantify uncertainty in migration-velocity models and its impact on imaging. Our approach integrates generative artificial intelligence with physics-informed common-image gathers, reducing reliance on accurate initial velocity models. Considered case studies demonstrate its efficacy producing realizations of migration-velocity models conditioned by the data. These models are used to quantify amplitude and positioning effects during subsequent imaging.
    
[^59]: 利用频域学习进行三维血管分割

    Leveraging Frequency Domain Learning in 3D Vessel Segmentation. (arXiv:2401.06224v1 [eess.IV])

    [http://arxiv.org/abs/2401.06224](http://arxiv.org/abs/2401.06224)

    本研究利用频域学习为3D血管分割模型提供替代多尺度卷积核的方案，减少了计算开销，同时保留了全局感受野。还设计了一个零参数频域融合方法，用于改进U-Net中的跳跃连接。

    

    冠状微血管疾病对人类健康构成重大风险。利用计算机辅助分析和诊断系统，医疗专业人员可以在疾病进展早期进行干预，其中三维血管分割是关键组成部分。然而，传统的U-Net架构往往产生不连贯和不精确的分割结果，特别是对于小血管结构。虽然具有注意力机制的模型，如变换器和大卷积核，展现出优越的性能，但是它们在训练和推断过程中消耗计算资源且时间复杂度较高。在本研究中，我们利用频域学习作为三维分层分割模型中多尺度卷积核的替代品，可以减少计算开销同时保留网络中的全局感受野。此外，还设计了一个零参数频域融合方法，用于改进U-Net中的跳跃连接。

    Coronary microvascular disease constitutes a substantial risk to human health. Employing computer-aided analysis and diagnostic systems, medical professionals can intervene early in disease progression, with 3D vessel segmentation serving as a crucial component. Nevertheless, conventional U-Net architectures tend to yield incoherent and imprecise segmentation outcomes, particularly for small vessel structures. While models with attention mechanisms, such as Transformers and large convolutional kernels, demonstrate superior performance, their extensive computational demands during training and inference lead to increased time complexity. In this study, we leverage Fourier domain learning as a substitute for multi-scale convolutional kernels in 3D hierarchical segmentation models, which can reduce computational expenses while preserving global receptive fields within the network. Furthermore, a zero-parameter frequency domain fusion method is designed to improve the skip connections in U
    
[^60]: 学习无监督的语义文档表示以进行细粒度的基于方面的情感分析

    Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])

    [http://arxiv.org/abs/2401.06210](http://arxiv.org/abs/2401.06210)

    这篇论文研究了学习无监督的语义文档表示以进行细粒度的基于方面的情感分析。通过克服现有方法的困难，实验证明该模型在各个任务上的性能优于最先进方法。

    

    文档表示是机器理解中许多自然语言处理任务的核心。以无监督方式学习的一般表示保留了通用性，可用于各种应用。在实践中，情感分析（SA）是一个挑战性的任务，被认为与语义密切相关，并经常用于评估一般表示。现有的无监督文档表示学习方法可以分为两类：序列方法（显式考虑单词的顺序）和非序列方法（不显式考虑顺序）。然而，它们都有各自的缺点。在本文中，我们提出了一个模型，克服了这两类方法遇到的困难。实验证明，我们的模型在流行的SA数据集和细粒度的基于方面的SA上优于现有的最先进方法。

    Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin.
    
[^61]: 对LLM在飞行轨迹重建分析中潜力的探索性评估

    An Exploratory Assessment of LLM's Potential Toward Flight Trajectory Reconstruction Analysis. (arXiv:2401.06204v1 [cs.LG])

    [http://arxiv.org/abs/2401.06204](http://arxiv.org/abs/2401.06204)

    本研究探索了大型语言模型（LLMs）在航空领域中重建飞行轨迹的潜力，并通过使用ADS-B数据对LLaMA 2模型进行实证研究，发现LLMs在过滤噪音和估计飞行轨迹方面表现出色。然而，研究也揭示了处理较长数据序列的挑战，该挑战可能源于LLM模型的标记长度限制。这些研究结果强调了LLMs在航空和交通领域广泛应用的潜力。

    

    大型语言模型（LLMs）在航空领域，特别是在重建飞行轨迹方面具有革命性潜力。本文研究了这一潜力，基于LLMs在处理序列数据和解读复杂数据结构方面的优势。利用预先训练的开源LLM模型LLaMA 2，本研究着重于使用自动相关监视广播（ADS-B）数据重建飞行轨迹，该数据具有真实世界场景中的不规则性。研究结果表明该模型能够有效地过滤噪音，并估计出线性和曲线型飞行轨迹。然而，分析同时也揭示了在处理较长数据序列方面存在的挑战，这可能归因于LLM模型的标记长度限制。研究的发现突显了LLMs在飞行轨迹重建中的潜力，并为其在航空和交通领域的广泛应用开辟了新的途径。

    Large Language Models (LLMs) hold transformative potential in aviation, particularly in reconstructing flight trajectories. This paper investigates this potential, grounded in the notion that LLMs excel at processing sequential data and deciphering complex data structures. Utilizing the LLaMA 2 model, a pre-trained open-source LLM, the study focuses on reconstructing flight trajectories using Automatic Dependent Surveillance-Broadcast (ADS-B) data with irregularities inherent in real-world scenarios. The findings demonstrate the model's proficiency in filtering noise and estimating both linear and curved flight trajectories. However, the analysis also reveals challenges in managing longer data sequences, which may be attributed to the token length limitations of LLM models. The study's insights underscore the promise of LLMs in flight trajectory reconstruction and open new avenues for their broader application across the aviation and transportation sectors.
    
[^62]: 使用精调的源分离器合奏混音音乐以改善助听器音质

    Remixing Music for Hearing Aids Using Ensemble of Fine-Tuned Source Separators. (arXiv:2401.06203v1 [eess.AS])

    [http://arxiv.org/abs/2401.06203](http://arxiv.org/abs/2401.06203)

    本文介绍了一个使用精调的源分离器合奏混音音乐以改善助听器音质的系统，该系统在Cadenza ICASSP 2024大挑战中名列第一，并在评估数据集上获得了最佳的助听器音质指数（HAAQI）平均分数。

    

    本文介绍了我们在Cadenza ICASSP 2024大挑战中的系统提交，该大挑战提出了为助听器用户混音和增强音乐的问题。我们的系统在挑战中名列第一，在评估数据集上获得了最佳的助听器音质指数（HAAQI）平均分数。我们描述了该系统，它使用了一组经过细调的深度学习音乐源分离器，这些分离器在挑战数据上进行了细调。我们通过挑战结果证明了我们系统的有效性，并通过消融研究分析了不同系统方面的重要性。

    This paper introduces our system submission for the Cadenza ICASSP 2024 Grand Challenge, which presents the problem of remixing and enhancing music for hearing aid users. Our system placed first in the challenge, achieving the best average Hearing-Aid Audio Quality Index (HAAQI) score on the evaluation data set. We describe the system, which uses an ensemble of deep learning music source separators that are fine tuned on the challenge data. We demonstrate the effectiveness of our system through the challenge results and analyze the importance of different system aspects through ablation studies.
    
[^63]: xTrimoPGLM: 统一的百亿规模预训练蛋白质语言模型，用于解析蛋白质的语言

    xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.06199](http://arxiv.org/abs/2401.06199)

    xTrimoPGLM是一个统一的100亿规模预训练蛋白质语言模型，能够同时处理蛋白质理解和生成任务，通过创新的预训练框架和大规模的参数训练，显著优于其他先进模型，在18个蛋白理解基准测试中取得了成功，并能够实现对蛋白质结构的原子分辨率观察。

    

    蛋白质语言模型在学习蛋白质序列中的生物信息方面显示出显著的成功。然而，大多数现有模型局限于自编码或自回归的预训练目标，这使得它们在处理蛋白质理解和生成任务时很难同时进行。我们提出了一个统一的蛋白质语言模型，xTrimoPGLM，通过创新的预训练框架同时解决这两类任务。我们的关键技术贡献是探索这两类目标的兼容性和联合优化的潜力，从而导致了一个以前所未有的规模，使用1000亿参数和1万亿训练标记来训练xTrimoPGLM的策略。我们广泛的实验证明，1）xTrimoPGLM在四个类别的18个蛋白理解基准测试中明显优于其他先进基线。该模型还有助于对蛋白质结构进行原子分辨率的观察，从而实现了对蛋白质结构的理解和生成。

    Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to 
    
[^64]: NeuSpin：基于自旋电子学的可靠边缘神经形态系统设计用于绿色人工智能

    NeuSpin: Design of a Reliable Edge Neuromorphic System Based on Spintronics for Green AI. (arXiv:2401.06195v1 [cs.ET])

    [http://arxiv.org/abs/2401.06195](http://arxiv.org/abs/2401.06195)

    NeuSpin是一个全栈硬件和软件共同设计的项目，旨在解决使用自旋电子学技术在边缘进行绿色人工智能实现的挑战。通过计算内存一体化，NeuSPIN可以实现超低功耗、高处理能力以及可靠性的要求，从而促进物联网和智能可穿戴设备的发展。

    

    物联网（IoT）和个性化医疗智能可穿戴设备将需要存储和处理越来越多的数据。这些设备的关键要求是超低功耗、高处理能力、低成本的自主性，以及可靠性和准确性，以实现边缘绿色人工智能。人工智能（AI）模型，特别是贝叶斯神经网络（BayNNs），对资源的需求高，并且由于内存墙问题，面临着传统计算架构的挑战。利用新型可变电阻记忆的计算内存一体化（CIM）提供了一种解决方案，通过将内存块和计算单元结合起来，提高了效率，降低了功耗。然而，在CIM硬件上实现BayNNs，特别是使用自旋电子学技术，由于可变性和制造缺陷而存在技术挑战。NeuSPIN项目旨在通过全栈硬件和软件共同设计来应对这些挑战，开发新颖的算法。

    Internet of Things (IoT) and smart wearable devices for personalized healthcare will require storing and computing ever-increasing amounts of data. The key requirements for these devices are ultra-low-power, high-processing capabilities, autonomy at low cost, as well as reliability and accuracy to enable Green AI at the edge. Artificial Intelligence (AI) models, especially Bayesian Neural Networks (BayNNs) are resource-intensive and face challenges with traditional computing architectures due to the memory wall problem. Computing-in-Memory (CIM) with emerging resistive memories offers a solution by combining memory blocks and computing units for higher efficiency and lower power consumption. However, implementing BayNNs on CIM hardware, particularly with spintronic technologies, presents technical challenges due to variability and manufacturing defects. The NeuSPIN project aims to address these challenges through full-stack hardware and software co-design, developing novel algorithmic 
    
[^65]: CrisisKAN: 知识注入和可解释的多模态注意力网络用于危机事件分类

    CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification. (arXiv:2401.06194v1 [cs.LG])

    [http://arxiv.org/abs/2401.06194](http://arxiv.org/abs/2401.06194)

    CrisisKAN是一种知识注入和可解释的多模态注意力网络，用于危机事件分类。它通过结合图像、文本和维基百科的外部知识来弥合图像和文本模态之间的语义差距，并解释模型的结果，以建立在高风险情况下的信任。

    

    广泛使用社交媒体已成为实时信息（如图像、文本或二者兼有）识别各种事件的新兴来源。尽管图像和文本的事件分类迅速发展，但最先进的模型仍然难以弥合由于不一致的编码导致的图像和文本模态之间的语义差距。此外，模型的黑匣子特性无法解释模型的结果，无法在灾难、大流行等高风险情况下建立信任。此外，社交媒体帖子的字数限制可能会对特定事件引入偏见。为了解决这些问题，我们提出了CrisisKAN，一种新颖的知识注入和可解释的多模态注意力网络，将图像和文本与维基百科的外部知识相结合，用于分类危机事件。为了丰富对文本信息的上下文特定理解，我们使用了提出的维基百科知识提取方法。

    Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction
    
[^66]: Scissorhands: 通过网络连接敏感性在数据影响中进行数据擦除

    Scissorhands: Scrub Data Influence via Connection Sensitivity in Networks. (arXiv:2401.06187v1 [cs.LG])

    [http://arxiv.org/abs/2401.06187](http://arxiv.org/abs/2401.06187)

    Scissorhands 是一种新的机器取消学习方法，通过连接敏感性识别与遗忘数据相关的最相关参数，并通过重新训练修剪的模型来擦除数据影响。

    

    机器取消学习已成为一项重要任务，旨在擦除训练模型中的数据影响。它符合最新的数据监管标准，增强了机器学习应用的隐私和安全性。大多数现有的机器取消学习方法表现良好，但通常需要访问其余数据的全部内容，在某些情况下可能不可行。在这项工作中，我们提出了一种新的机器取消学习方法“Scissorhands”，它只使用训练数据的子集来有效运行。初始阶段，Scissorhands通过连接敏感性在给定模型中识别与遗忘数据相关的最相关参数。该过程通过重新初始化这些参数中具有最大影响力的前k%的最相关参数，从而产生一个用于擦除遗忘数据影响的修剪模型。随后，Scissorhands通过最小-最大优化过程对修剪的模型进行再训练，寻找保留信息的参数。

    Machine unlearning has become a pivotal task to erase the influence of data from a trained model. It adheres to recent data regulation standards and enhances the privacy and security of machine learning applications. Most existing machine unlearning methods perform well, however, they typically necessitate access to the entirety of the remaining data, which might not be feasible in certain scenarios. In this work, we present a new machine unlearning approach Scissorhands, which operates effectively with only a subset of the training data. Initially, Scissorhands identifies the most pertinent parameters in the given model relative to the forgetting data via connection sensitivity. This process involves reinitializing the most influential top-$k$ percent of these parameters, resulting in a trimmed model for erasing the influence of the forgetting data. Subsequently, Scissorhands retrains the trimmed model through a min-max optimization process, seeking parameters that preserve informatio
    
[^67]: 使用Bark、mBART和经过微调的XLSR Wav2Vec2的端到端印地语到英语语音转换

    End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2. (arXiv:2401.06183v1 [eess.AS])

    [http://arxiv.org/abs/2401.06183](http://arxiv.org/abs/2401.06183)

    本论文提出了一个端到端的语音转换框架，用于印地语到英语的转换，采用了Bark、mBART和经过微调的XLSR Wav2Vec2等先进技术，为跨语言交流提供了统一而无缝的解决方案。

    

    长期以来，语音一直是有效沟通和连接的障碍，在我们日益互联的世界中仍然具有挑战性。这篇研究论文介绍了一种针对印地语到英语翻译量身定制的端到端语音转换框架，最终实现了英文音频的合成。通过整合XLSR Wav2Vec2用于自动语音识别（ASR），mBART用于神经机器翻译（NMT），以及一个文本到语音（TTS）合成组件等尖端技术，该框架提供了一种统一而无缝的跨语言交流方式。我们深入研究了每个组件的复杂细节，阐明了它们的个别贡献，并探讨了相互之间的协同作用，从而实现从印地语口语到合成英文音频的流畅过渡。

    Speech has long been a barrier to effective communication and connection, persisting as a challenge in our increasingly interconnected world. This research paper introduces a transformative solution to this persistent obstacle an end-to-end speech conversion framework tailored for Hindi-to-English translation, culminating in the synthesis of English audio. By integrating cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech recognition (ASR), mBART for neural machine translation (NMT), and a Text-to-Speech (TTS) synthesis component, this framework offers a unified and seamless approach to cross-lingual communication. We delve into the intricate details of each component, elucidating their individual contributions and exploring the synergies that enable a fluid transition from spoken Hindi to synthesized English audio.
    
[^68]: 从轨迹和细胞命运信息中预测细胞身份

    Prediction of Cellular Identities from Trajectory and Cell Fate Information. (arXiv:2401.06182v1 [q-bio.QM])

    [http://arxiv.org/abs/2401.06182](http://arxiv.org/abs/2401.06182)

    本研究提出了一种创新的方法，利用机器学习从早期C. elegans胚胎的图像序列中预测细胞身份。通过使用少量的细胞轨迹和细胞命运信息，我们的模型在有限数据条件下达到了超过90%的分类准确率。

    

    在图像序列中确定细胞身份是一项重要且具有挑战性的任务。传统的细胞识别方法是通过细胞追踪，这是复杂且耗时的。在本研究中，我们提出了一种创新的方法，使用机器学习来识别早期C. elegans胚胎内的细胞身份。我们使用随机森林、MLP和LSTM模型，对跨越胚胎发育的前4个小时的3D时间序列共聚焦数据集进行细胞分类准确性测试。通过利用个体细胞的少量时空特征，包括细胞轨迹和细胞命运信息，我们的模型在有限数据条件下实现了超过90%的准确率。我们还确定了最重要的特征贡献，并可以从生物学知识的角度解释这些特征。我们的研究展示了直接从简单的时空特征中预测4D图像序列中的细胞身份的成功。

    Determining cell identities in imaging sequences is an important yet challenging task. The conventional method for cell identification is via cell tracking, which is complex and can be time-consuming. In this study, we propose an innovative approach to cell identification during early C. elegans embryogenesis using machine learning. We employed random forest, MLP, and LSTM models, and tested cell classification accuracy on 3D time-lapse confocal datasets spanning the first 4 hours of embryogenesis. By leveraging a small number of spatial-temporal features of individual cells, including cell trajectory and cell fate information, our models achieve an accuracy of over 90%, even with limited data. We also determine the most important feature contributions and can interpret these features in the context of biological knowledge. Our research demonstrates the success of predicting cell identities in 4D imaging sequences directly from simple spatio-temporal features.
    
[^69]: 分布式传闻互惠学习（GML）用于头颈肿瘤自动分割

    Decentralized Gossip Mutual Learning (GML) for automatic head and neck tumor segmentation. (arXiv:2401.06180v1 [eess.IV])

    [http://arxiv.org/abs/2401.06180](http://arxiv.org/abs/2401.06180)

    GML是一种使用传闻协议进行直接点对点通信的分散协作学习框架，在医疗图像分割任务中取得了较好的效果，避免了传统联邦学习中中心服务器的故障和局部数据特征变化的问题。

    

    联邦学习（FL）已经被证明是一种有前途的策略，可以在不共享数据的情况下，从不同的医疗中心合作训练复杂的机器学习模型。然而，传统的FL依赖于一个中央服务器来协调客户端之间的全局模型训练，这使得它容易受到模型服务器故障的影响。同时，基于全局数据属性训练的模型可能无法在特定站点的本地数据上获得最佳性能，原因是数据特征之间的变化。为了解决这些限制，我们提出了Gossip Mutual Learning（GML），这是一个分散的协作学习框架，它使用传闻协议进行直接的点对点通信，并通过互惠学习从对等方那里利用有用的信息来优化每个站点的本地模型。在使用HECKTOR21数据集的PET/CT图像上的肿瘤分割任务中，我们证明了GML的有效性。

    Federated learning (FL) has emerged as a promising strategy for collaboratively training complicated machine learning models from different medical centers without the need of data sharing. However, the traditional FL relies on a central server to orchestrate the global model training among clients. This makes it vulnerable to the failure of the model server. Meanwhile, the model trained based on the global data property may not yield the best performance on the local data of a particular site due to the variations of data characteristics among them. To address these limitations, we proposed Gossip Mutual Learning(GML), a decentralized collaborative learning framework that employs Gossip Protocol for direct peer-to-peer communication and encourages each site to optimize its local model by leveraging useful information from peers through mutual learning. On the task of tumor segmentation on PET/CT images using HECKTOR21 dataset with 223 cases from five clinical sites, we demonstrated GM
    
[^70]: 金融领域可扩展行为的CNN-DRL方法

    CNN-DRL for Scalable Actions in Finance. (arXiv:2401.06179v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.06179](http://arxiv.org/abs/2401.06179)

    本文提出了一种用于金融领域可扩展行为的CNN-DRL方法，通过将过去九十天的每日特征向量数据拼接到CNN输入矩阵中，实现了对环境动态的有效学习，获得了奖励的增加。

    

    在金融领域，基于MLP的DRL在动作规模增长时难以学习环境动态。如果买卖量增加到一千股，MLP代理无法有效适应环境。为了解决这个问题，我们设计了一个CNN代理，将过去九十天的每日特征向量数据拼接起来创建CNN输入矩阵。实验证明，基于MLP的代理在初始环境设置上遭受损失，而我们设计的CNN仍然稳定，有效地学习环境，并导致奖励增加。

    The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards.
    
[^71]: GOODAT: 面向测试时图形的领域外检测

    GOODAT: Towards Test-time Graph Out-of-Distribution Detection. (arXiv:2401.06176v1 [cs.LG])

    [http://arxiv.org/abs/2401.06176](http://arxiv.org/abs/2401.06176)

    GOODAT是一种用于检测测试时图形领域外样本的方法，具有数据驱动、无监督和即插即用的特点。

    

    图神经网络(GNNs)在建模不同领域的图数据中具有广泛的应用。虽然GNN在测试数据与训练数据符合同一分布(即分布内,ID)的场景中表现出色，但当面对来自不熟悉分布(即领域外,OOD)的样本时，它们通常会出现错误的预测。为了用GNNs识别和拒绝OOD样本，最近的研究探索了图OOD检测，通常集中在训练特定模型或在经过良好训练的GNN之上修改数据。尽管这些方法是有效的，但它们需要重型的训练资源和成本，因为它们需要在训练数据上优化基于GNN的模型。此外，它们对修改原始GNN和访问训练数据的依赖进一步限制了它们的普适性。为此，本文介绍了一种在测试时检测图形的方法(GOODAT)，这是一种面向数据、无监督和即插即用的解决方案。

    Graph neural networks (GNNs) have found widespread application in modeling graph data across diverse domains. While GNNs excel in scenarios where the testing data shares the distribution of their training counterparts (in distribution, ID), they often exhibit incorrect predictions when confronted with samples from an unfamiliar distribution (out-of-distribution, OOD). To identify and reject OOD samples with GNNs, recent studies have explored graph OOD detection, often focusing on training a specific model or modifying the data on top of a well-trained GNN. Despite their effectiveness, these methods come with heavy training resources and costs, as they need to optimize the GNN-based models on training data. Moreover, their reliance on modifying the original GNNs and accessing training data further restricts their universality. To this end, this paper introduces a method to detect Graph Out-of-Distribution At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play solu
    
[^72]: MTAD: 多元时间序列异常检测的工具和基准

    MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly Detection. (arXiv:2401.06175v1 [cs.SE])

    [http://arxiv.org/abs/2401.06175](http://arxiv.org/abs/2401.06175)

    这篇论文提出了一套综合的多元时间序列异常检测工具和基准，解决了现有方法缺乏严格比较和重新实现困难的问题。

    

    关键绩效指标是确保许多软件系统可靠性和稳定性的重要时间序列指标。它们忠实记录运行时状态，便于理解异常系统行为，并为工程师提供定位根本原因的有用线索。然而，现代软件系统的规模和复杂性前所未有，导致KPI数量激增。因此，许多传统的KPI异常检测方法变得不实用，这促使了学术界和工业界机器学习解决方案的快速发展。然而，目前缺乏对这些KPI异常检测方法的严格比较，并且重新实现需要付出相当大的工作量。此外，我们观察到不同的研究采用独立的评估过程和不同的指标。其中一些可能无法充分展示模型的能力，有些则产生了进展的错觉。为了更好地理解这个问题，我们提出了一套综合的多元时间序列异常检测工具和基准。

    Key Performance Indicators (KPIs) are essential time-series metrics for ensuring the reliability and stability of many software systems. They faithfully record runtime states to facilitate the understanding of anomalous system behaviors and provide informative clues for engineers to pinpoint the root causes. The unprecedented scale and complexity of modern software systems, however, make the volume of KPIs explode. Consequently, many traditional methods of KPI anomaly detection become impractical, which serves as a catalyst for the fast development of machine learning-based solutions in both academia and industry. However, there is currently a lack of rigorous comparison among these KPI anomaly detection methods, and re-implementation demands a non-trivial effort. Moreover, we observe that different works adopt independent evaluation processes with different metrics. Some of them may not fully reveal the capability of a model and some are creating an illusion of progress. To better und
    
[^73]: 脊柱生物力学中的机器学习应用

    Machine Learning Applications in Spine Biomechanics. (arXiv:2401.06174v1 [eess.IV])

    [http://arxiv.org/abs/2401.06174](http://arxiv.org/abs/2401.06174)

    本研究介绍了脊柱生物力学中机器学习应用的框架，使得从单个摄像头图像中可以全面分析复杂活动中的脊柱生物力学。同时评估了其在不同应用中的性能和限制，包括工作场所提重物的评估、车祸中的翻颈伤评估和专业体育中的生物力学分析。

    

    脊柱生物力学正在随着机器学习和计算机视觉技术的出现和整合而发生变革。这些新颖的技术可以从一个简单的单摄像头图像中估计3D身体形状、人体测量和运动学，使得它们更具可访问性和实用性，适用于各种应用。本研究介绍了一个框架，将这些方法与传统的肌肉骨骼建模相结合，从单个摄像头进行脊柱生物力学的综合分析。此外，我们旨在评估它们在脊柱生物力学应用中的性能和限制。本研究探索的真实世界应用包括工作场所提重物的评估、车祸中的翻颈伤评估和专业体育中的生物力学分析。我们的结果展示了各种算法在估计身体形状、运动学和进行现场生物力学分析方面的潜力和限制。

    Spine biomechanics is at a transformation with the advent and integration of machine learning and computer vision technologies. These novel techniques facilitate the estimation of 3D body shapes, anthropometrics, and kinematics from as simple as a single-camera image, making them more accessible and practical for a diverse range of applications. This study introduces a framework that merges these methodologies with traditional musculoskeletal modeling, enabling comprehensive analysis of spinal biomechanics during complex activities from a single camera. Additionally, we aim to evaluate their performance and limitations in spine biomechanics applications. The real-world applications explored in this study include assessment in workplace lifting, evaluation of whiplash injuries in car accidents, and biomechanical analysis in professional sports. Our results demonstrate potential and limitations of various algorithms in estimating body shape, kinematics, and conducting in-field biomechani
    
[^74]: 基于树搜索的进化赌博机用于蛋白质序列优化的研究

    Tree Search-Based Evolutionary Bandits for Protein Sequence Optimization. (arXiv:2401.06173v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.06173](http://arxiv.org/abs/2401.06173)

    本研究提出了一种基于树搜索的赌博机学习方法，用于高效探索和优化蛋白质序列。通过在初始序列上扩展树结构，并结合局部搜索和赌博机学习模型，可以有效地发现接近最优设计。

    

    虽然现代生物技术允许合成新蛋白质并进行大规模的功能测量，但由于任何给定蛋白质的庞大序列空间，高效地探索和工程化仍然是一项艰巨的任务。蛋白工程通常通过将变异添加到野生型或领先序列，重新组合变异，并运行新一轮的筛选来进行。为了提高这个过程的效率，我们提出了一种基于树搜索的赌博机学习方法，该方法通过一个赌博机机器学习模型的指导从初始序列开始扩展树结构。在简化假设和高斯过程先验的情况下，我们提供了理论分析和贝叶斯遗憾界限，证明了局部搜索和赌博机学习方法的结合可以有效地发现接近最优的设计。该算法与一系列随机化的树搜索启发式方法，以及机器学习模型、+

    While modern biotechnologies allow synthesizing new proteins and function measurements at scale, efficiently exploring a protein sequence space and engineering it remains a daunting task due to the vast sequence space of any given protein. Protein engineering is typically conducted through an iterative process of adding mutations to the wild-type or lead sequences, recombination of mutations, and running new rounds of screening. To enhance the efficiency of such a process, we propose a tree search-based bandit learning method, which expands a tree starting from the initial sequence with the guidance of a bandit machine learning model. Under simplified assumptions and a Gaussian Process prior, we provide theoretical analysis and a Bayesian regret bound, demonstrating that the combination of local search and bandit learning method can efficiently discover a near-optimal design. The full algorithm is compatible with a suite of randomized tree search heuristics, machine learning models, pr
    
[^75]: 危机警报：使用机器学习方法预测股市危机事件

    CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods. (arXiv:2401.06172v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.06172](http://arxiv.org/abs/2401.06172)

    该论文使用机器学习方法预测股市危机事件，主要针对美国市场。论文比较了随机森林和极限梯度提升模型的性能，发现这两种模型可以有效预测股市危机。

    

    历史上，经济衰退往往突然而灾难性。例如，在2008年金融危机期间，标普500指数从2007年10月下跌了46％，持续到2009年3月。如果我们能早些发现危机的信号，我们就能采取预防措施。因此，受到这样的动机驱使，我们使用先进的机器学习技术，包括随机森林和极限梯度提升，主要预测美国市场可能发生的任何潜在市场崩盘。此外，我们还想比较这些方法的性能，并检查哪种模型更适合预测美国股市暴跌。我们将这些模型应用于每日金融市场数据，这些数据对于更高的报告频率更具响应性。我们考虑了75个解释变量，包括总体美国股市指数，标普500部门指数，以及可用于危机预测目的的市场指标。最后，我们通过选择的分类指标得出结论，即使用随机森林和极限梯度提升模型可以有效预测股市危机事件。

    Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the
    
[^76]: 通过HE染色组织切片，深度学习模型预测犬皮肤肥大细胞瘤的c-Kit-11突变状态

    Deep Learning model predicts the c-Kit-11 mutational status of canine cutaneous mast cell tumors by HE stained histological slides. (arXiv:2401.06169v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.06169](http://arxiv.org/abs/2401.06169)

    本研究通过HE染色组织切片训练了深度学习模型，仅通过形态学分析就可以准确识别犬皮肤肥大细胞瘤的c-Kit-11突变状态，准确率为87％。

    

    目前在犬肥大细胞瘤的活检组织中，通过组织病理学评估多种预后因子以评估临床行为。此外，常常通过c-Kit外显子11的PCR分析评估酪氨酸激酶抑制剂治疗的成功潜力。本研究旨在通过形态学而无需进一步的分子分析，训练深度学习模型（DLMs）识别MCTs的c-Kit-11突变状态。195例突变性和173例非突变性肿瘤的HE切片连续染色在两个不同的实验室并使用三种不同的切片扫描仪进行扫描。这产生了六个不同的数据集（染色-扫描仪变化）的整个切片图像。DLMs通过单一和混合数据集进行训练，并在扫描仪和染色领域转移下评估其性能。平均而言，DLMs可以正确分类HE切片的c-Kit 11突变状态，准确率达到87％。

    Numerous prognostic factors are currently assessed histopathologically in biopsies of canine mast cell tumors to evaluate clinical behavior. In addition, PCR analysis of the c-Kit exon 11 mutational status is often performed to evaluate the potential success of a tyrosine kinase inhibitor therapy. This project aimed at training deep learning models (DLMs) to identify the c-Kit-11 mutational status of MCTs solely based on morphology without additional molecular analysis. HE slides of 195 mutated and 173 non-mutated tumors were stained consecutively in two different laboratories and scanned with three different slide scanners. This resulted in six different datasets (stain-scanner variations) of whole slide images. DLMs were trained with single and mixed datasets and their performances was assessed under scanner and staining domain shifts. The DLMs correctly classified HE slides according to their c-Kit 11 mutation status in, on average, 87% of cases for the best-suited stain-scanner var
    
[^77]: 可调整的分子表示方法用于统一的预训练策略

    Adjustable Molecular Representation for Unified Pre-training Strategy. (arXiv:2401.06166v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.06166](http://arxiv.org/abs/2401.06166)

    AdaMR是一种可调整粒度的分子模型，它在原子和亚结构水平上学习分子表示。通过预训练和分子规范化任务，AdaMR可以改善对多个下游任务的效果，包括模型属性预测和分子生成。

    

    我们提出了一种新的大规模分子模型，名为AdaMR，它代表可调整的分子表示方法用于统一的预训练策略。与最近使用单一分子编码的大规模分子模型不同，AdaMR采用了可调整粒度的分子编码器，在原子和亚结构水平上学习分子表示。对于预训练过程，我们设计了一个分子规范化的任务，涉及将多个通用分子表示转化为规范表示。通过调整分子编码的粒度，训练得到的模型可以提高对多个下游任务的效果，如模型属性预测和分子生成。亚结构水平的分子表示保留了决定化学性质和具有类似功能的特定原子组或排列的信息，对于性质预测等任务是有益的。同时，原子级表示将原子的特异信息纳入考虑，使模型能够更好地捕捉原子间的相互作用。

    We propose a new large-scale molecular model, named AdaMR, which stands for Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike recent large-scale molecular models that use a single molecular encoding, AdaMR employs a granularity-adjustable molecular encoder, learning molecular representations at both the atomic and substructure levels. For the pre-training process, we designed a task for molecular canonicalization, which involves transforming ltiple generic molecular representations into canonical representations. By adjusting the granularity of molecular encoding, the trained model can improve the effects on multiple downstream tasks, such as model attribute prediction and molecule generation. Substructure-level molecular representation retains information of specific atom groups or arrangements that determine chemical properties and have similar functions, which is beneficial for tasks like property prediction. Meanwhile, atomic-level representation, combin
    
[^78]: 多模态Gen-AI用于基本投资研究

    Multimodal Gen-AI for Fundamental Investment Research. (arXiv:2401.06164v1 [q-fin.GN])

    [http://arxiv.org/abs/2401.06164](http://arxiv.org/abs/2401.06164)

    本论文介绍了一项在金融投资领域的变革性举措，旨在利用多模态Gen-AI技术自动化信息摘要和投资点生成，通过精细调整语言模型以实现特定的应用目标，并最终开发一个AI代理原型，使人类投资者摆脱重复性任务，将注意力集中在高层次的战略思维上。

    

    本报告概述了在金融投资领域的一项变革性举措，即重塑传统的决策过程，并在其中自动化信息摘要和投资点生成等任务。通过利用语言模型，我们的实验旨在评估在基础模型（Llama2）上进行精调以实现特定的应用目标，包括提供关于事件对公司和行业的影响的洞察，理解市场条件关系，生成与投资者对齐的投资点，并以股票建议和详细解释的形式呈现结果。通过先进的生成建模技术，最终目标是开发一个AI代理原型，使人类投资者摆脱重复性任务，将注意力集中在高层次的战略思维上。该项目涵盖了一个多样化的语料库数据。

    This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus data
    
[^79]: FRED: 实现航空图像目标检测中的完全旋转等变性

    FRED: Towards a Full Rotation-Equivariance in Aerial Image Object Detection. (arXiv:2401.06159v1 [cs.CV])

    [http://arxiv.org/abs/2401.06159](http://arxiv.org/abs/2401.06159)

    FRED是一个实现航空图像目标检测中完全旋转等变性的模型，通过解耦不变任务和等变任务实现端到端的等变性，将边界框表示为旋转等变向量。

    

    旋转等变性是定向对象检测中一个重要且具有挑战性的特性。尽管通用对象检测器自然利用传统CNN的平移等变性来提高对空间平移的鲁棒性，但实现旋转等变性仍然是一个难以实现的目标。目前的检测器采用各种对齐技术来得到旋转不变特征，但仍依赖于高容量模型和大量数据增强来处理所有可能旋转。在本文中，我们引入了一个完全旋转等变的定向对象检测器（FRED），其从图像到边界框预测的整个过程都是严格等变的。具体来说，我们将不变的任务（对象分类）和等变的任务（对象定位）解耦，以实现端到端等变性。我们将边界框表示为一组旋转等变向量，以实现旋转等变的定位。此外，我们还利用这些旋转等变向量作为偏移量。

    Rotation-equivariance is an essential yet challenging property in oriented object detection. While general object detectors naturally leverage robustness to spatial shifts due to the translation-equivariance of the conventional CNNs, achieving rotation-equivariance remains an elusive goal. Current detectors deploy various alignment techniques to derive rotation-invariant features, but still rely on high capacity models and heavy data augmentation with all possible rotations. In this paper, we introduce a Fully Rotation-Equivariant Oriented Object Detector (FRED), whose entire process from the image to the bounding box prediction is strictly equivariant. Specifically, we decouple the invariant task (object classification) and the equivariant task (object localization) to achieve end-to-end equivariance. We represent the bounding box as a set of rotation-equivariant vectors to implement rotation-equivariant localization. Moreover, we utilized these rotation-equivariant vectors as offsets
    
[^80]: 卷积神经网络中分类误差估计的随机方法

    A Stochastic Approach to Classification Error Estimates in Convolutional Neural Networks. (arXiv:2401.06156v1 [cs.CV])

    [http://arxiv.org/abs/2401.06156](http://arxiv.org/abs/2401.06156)

    本研究针对用于安全关键应用的图像分类的训练卷积神经网络进行了验证研究，证明了系统可以达到认证要求，并对障碍物检测功能的系统级危险率进行了定量分析。

    

    本技术报告介绍了在安全关键应用中用于图像分类的已训练卷积神经网络（CNN）的验证研究结果。以未来4级自动驾驶货运列车中的障碍物检测功能为例，通过使用ANSI/UL 4600和ISO 21448等新标准，证明了像GoA 4货运列车这样的系统确实可以达到认证要求，以及EN 50128和EN 50129等已有标准。此外，我们对预期的障碍物检测功能系统级危险率进行了定量分析。结果表明，使用传感器/感知器融合，融合检测系统可以满足安全完整性级别所需的可接受危险率（SIL-3）。对CNN模型进行了数学分析，确定了分类聚类和等价类划分。

    This technical report presents research results achieved in the field of verification of trained Convolutional Neural Network (CNN) used for image classification in safety-critical applications. As running example, we use the obstacle detection function needed in future autonomous freight trains with Grade of Automation (GoA) 4. It is shown that systems like GoA 4 freight trains are indeed certifiable today with new standards like ANSI/UL 4600 and ISO 21448 used in addition to the long-existing standards EN 50128 and EN 50129. Moreover, we present a quantitative analysis of the system-level hazard rate to be expected from an obstacle detection function. It is shown that using sensor/perceptor fusion, the fused detection system can meet the tolerable hazard rate deemed to be acceptable for the safety integrity level to be applied (SIL-3). A mathematical analysis of CNN models is performed which results in the identification of classification clusters and equivalence classes partitioning
    
[^81]: 使用多个GPT代理的强化学习进行的de novo药物设计

    De novo Drug Design using Reinforcement Learning with Multiple GPT Agents. (arXiv:2401.06155v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.06155](http://arxiv.org/abs/2401.06155)

    这项研究提出了一种使用多个GPT代理的强化学习算法MolRL-MGPT来进行de novo药物设计，在药物分子生成过程中促进了多样性，展现了在设计抗SARS-CoV-2蛋白靶点的抑制剂方面的有效性。

    

    de novo药物设计是药理学中的一个关键问题，也是人工智能科学研究的一个新领域。该领域的一个主要挑战是在产生具有特定性质的分子的同时，产生多样化的候选分子。尽管转换模型和强化学习等先进技术已应用于药物设计，但其潜力尚未充分发挥。因此，我们提出了一种名为MolRL-MGPT的强化学习算法，其中使用了多个GPT代理来生成药物分子。为了促进分子的多样性，我们鼓励代理合作，在不同方向上寻找理想的分子。我们的算法在GuacaMol基准测试中展示了有希望的结果，并在设计抗SARS-CoV-2蛋白靶点的抑制剂方面展示了有效性。代码可在以下链接中找到：https://github.com/HXYfighter/MolRL-MGPT。

    De novo drug design is a pivotal issue in pharmacology and a new area of focus in AI for science research. A central challenge in this field is to generate molecules with specific properties while also producing a wide range of diverse candidates. Although advanced technologies such as transformer models and reinforcement learning have been applied in drug design, their potential has not been fully realized. Therefore, we propose MolRL-MGPT, a reinforcement learning algorithm with multiple GPT agents for drug molecular generation. To promote molecular diversity, we encourage the agents to collaborate in searching for desirable molecules in diverse directions. Our algorithm has shown promising results on the GuacaMol benchmark and exhibits efficacy in designing inhibitors against SARS-CoV-2 protein targets. The codes are available at: https://github.com/HXYfighter/MolRL-MGPT.
    
[^82]: 趋向于使用SE(3)-离散扩散生成核酸和蛋白质复合物的联合序列-结构生成

    Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein Complexes with SE(3)-Discrete Diffusion. (arXiv:2401.06151v1 [q-bio.BM])

    [http://arxiv.org/abs/2401.06151](http://arxiv.org/abs/2401.06151)

    MMDiff是一个联合生成核酸和蛋白质复合物序列和结构的生成模型，具有重要的大分子设计应用价值，并通过验证实例展示了其有效性。

    

    大分子的生成模型对蛋白质工程的工业和生物医学研究具有丰富且重要的影响。然而，现有的方法目前仅限于独立或联合地对蛋白质结构或序列进行建模，而不考虑蛋白质和其他大分子之间常见的相互作用。在本研究中，我们引入了MMDiff，一个使用联合SE(3)-离散扩散噪声设计核酸和蛋白质复合物的序列和结构的生成模型。这样的模型对于结构基础转录因子设计和非编码RNA序列设计等新兴的大分子设计领域具有重要意义。我们通过在本研究中引入的严格新的大分子复合物生成设计基准来展示了MMDiff的实用性。我们的结果表明，MMDiff能够成功生成微型RNA和单链DNA分子。

    Generative models of macromolecules carry abundant and impactful implications for industrial and biomedical efforts in protein engineering. However, existing methods are currently limited to modeling protein structures or sequences, independently or jointly, without regard to the interactions that commonly occur between proteins and other macromolecules. In this work, we introduce MMDiff, a generative model that jointly designs sequences and structures of nucleic acid and protein complexes, independently or in complex, using joint SE(3)-discrete diffusion noise. Such a model has important implications for emerging areas of macromolecular design including structure-based transcription factor design and design of noncoding RNA sequences. We demonstrate the utility of MMDiff through a rigorous new design benchmark for macromolecular complex generation that we introduce in this work. Our results demonstrate that MMDiff is able to successfully generate micro-RNA and single-stranded DNA mole
    
[^83]: D-STGCNT:一种基于transformer的密集时空图卷积GRU网络用于评估患者身体康复

    D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])

    [http://arxiv.org/abs/2401.06150](http://arxiv.org/abs/2401.06150)

    D-STGCNT是一种新的模型，结合了STGCN和transformer的架构，用于自动评估患者身体康复锻炼。它通过将骨架数据视为图形，并检测关键关节，在处理时空数据方面具有高效性。该模型通过密集连接和GRU机制来处理大型3D骨架输入，有效建立时空动态模型。transformer的注意力机制对于评估康复锻炼非常有用。

    

    本文解决了自动评估无临床监督情况下患者进行身体康复锻炼的挑战。其目标是提供质量评分以确保正确执行和获得期望结果。为实现这一目标，引入了一种新的基于图结构的模型，Dense Spatio-Temporal Graph Conv-GRU Network with Transformer。该模型结合了改进的STGCN和transformer架构，用于高效处理时空数据。其关键思想是将骨架数据视为图形，并检测每个康复锻炼中起主要作用的关节。密集连接和GRU机制用于快速处理大型3D骨架输入并有效建模时空动态。transformer编码器的注意机制侧重于输入序列的相关部分，使其在评估康复锻炼方面非常有用。

    This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our propose
    
[^84]: 基于图像分类器的平面天线设计生成方法

    Image Classifier Based Generative Method for Planar Antenna Design. (arXiv:2401.06149v1 [cs.CV])

    [http://arxiv.org/abs/2401.06149](http://arxiv.org/abs/2401.06149)

    提出了一种基于图像分类器的平面天线设计生成方法，通过两个步骤确定几何尺寸和位置，并利用随机采样统计和卷积神经网络分类器来设计天线。这种方法不需要经验，能够产生逼真的设计，并且性能指标不亚于由经验丰富的工程师设计的天线。

    

    为了让更多的工程师能够进行印刷电路板（PCB）上的天线设计，我们提出了一种简单的方法，利用几个基本组件对PCB天线进行建模。通过分两个步骤决定其几何尺寸和位置，可以实现不需要任何经验的天线原型设计。将随机采样统计与尺寸质量相关联，用于在尺寸候选项中进行选择。引入了一种使用卷积神经网络（CNN）的新型基于图像的分类器，进一步确定这些固定尺寸组件的位置。选择了两个可穿戴产品的例子来检验整个工作流程。它们的最终设计是逼真的，其性能指标不逊于由经验丰富的工程师设计的天线。

    To extend the antenna design on printed circuit boards (PCBs) for more engineers of interest, we propose a simple method that models PCB antennas with a few basic components. By taking two separate steps to decide their geometric dimensions and positions, antenna prototypes can be facilitated with no experience required. Random sampling statistics relate to the quality of dimensions are used in selecting among dimension candidates. A novel image-based classifier using a convolutional neural network (CNN) is introduced to further determine the positions of these fixed-dimension components. Two examples from wearable products have been chosen to examine the entire workflow. Their final designs are realistic and their performance metrics are not inferior to the ones designed by experienced engineers.
    
[^85]: Minuet: 加速GPU上的3D稀疏卷积

    Minuet: Accelerating 3D Sparse Convolutions on GPUs. (arXiv:2401.06145v1 [cs.DC])

    [http://arxiv.org/abs/2401.06145](http://arxiv.org/abs/2401.06145)

    Minuet是一种专为现代GPU设计的高效内存SC引擎，通过使用新的分段排序二分查找算法和轻量级方案来加速3D稀疏卷积。

    

    稀疏卷积（SC）广泛用于处理本质上稀疏的3D点云数据。与稠密卷积不同，SC通过只允许输出到特定位置来保留输入点云的稀疏性。为了高效计算SC，先前的SC引擎首先使用哈希表构建一个内核映射，该映射存储需要执行的通用矩阵乘法（GEMM）操作（映射步骤），然后使用Gather-GEMM-Scatter过程执行这些GEMM操作（GMaS步骤）。在这项工作中，我们分析了先前最先进的SC引擎的缺点，并提出了Minuet，一种专为现代GPU量身定制的新型高效内存SC引擎。Minuet提出了以下几点：(i)将Map步骤中使用的哈希表替换为一种高度利用GPU片上存储层次结构的新颖分段排序双遍历二分查找算法，(ii)使用轻量级方案来自动调整GMaS步骤中的Gather和Scatter操作的瓦片大小，以适应不同的数据情况。

    Sparse Convolution (SC) is widely used for processing 3D point clouds that are inherently sparse. Different from dense convolution, SC preserves the sparsity of the input point cloud by only allowing outputs to specific locations. To efficiently compute SC, prior SC engines first use hash tables to build a kernel map that stores the necessary General Matrix Multiplication (GEMM) operations to be executed (Map step), and then use a Gather-GEMM-Scatter process to execute these GEMM operations (GMaS step). In this work, we analyze the shortcomings of prior state-of-the-art SC engines, and propose Minuet, a novel memory-efficient SC engine tailored for modern GPUs. Minuet proposes to (i) replace the hash tables used in the Map step with a novel segmented sorting double-traversed binary search algorithm that highly utilizes the on-chip memory hierarchy of GPUs, (ii) use a lightweight scheme to autotune the tile size in the Gather and Scatter operations of the GMaS step, such that to adapt t
    
[^86]: DFU: 零样本超分辨率图像生成的尺度鲁棒扩散模型

    DFU: scale-robust diffusion model for zero-shot super-resolution image generation. (arXiv:2401.06144v1 [cs.CV])

    [http://arxiv.org/abs/2401.06144](http://arxiv.org/abs/2401.06144)

    DFU是一种尺度鲁棒的扩散模型，可以实现零样本超分辨率图像生成，并通过在多个分辨率上训练来提高模型的可扩展性。

    

    扩散生成模型在生成固定分辨率的图像方面取得了显著的成功。然而，现有模型在没有相应分辨率的训练数据时，很难推广到不同的分辨率。借鉴操作符学习技术，我们提出了一种新的深度学习架构，Dual-FNO UNet (DFU)，通过在多个分辨率上同时组合空间和光谱信息来近似评分操作符。将DFU与基准模型进行比较，我们证明其可扩展性：1）在多个分辨率上同时训练可以改善FID，而单一固定分辨率的训练则不能实现；2）DFU可以推广到其训练分辨率之外，实现高分辨率的协调、高保真度的图像生成，即零样本超分辨率图像生成；3）我们提出了一种微调策略来进一步提高我们模型的零样本超分辨率图像生成能力，使FID为11.3。

    Diffusion generative models have achieved remarkable success in generating images with a fixed resolution. However, existing models have limited ability to generalize to different resolutions when training data at those resolutions are not available. Leveraging techniques from operator learning, we present a novel deep-learning architecture, Dual-FNO UNet (DFU), which approximates the score operator by combining both spatial and spectral information at multiple resolutions. Comparisons of DFU to baselines demonstrate its scalability: 1) simultaneously training on multiple resolutions improves FID over training at any single fixed resolution; 2) DFU generalizes beyond its training resolutions, allowing for coherent, high-fidelity generation at higher-resolutions with the same model, i.e. zero-shot super-resolution image-generation; 3) we propose a fine-tuning strategy to further enhance the zero-shot super-resolution image-generation capability of our model, leading to a FID of 11.3 at 
    
[^87]: StockFormer: 一种基于STL分解和自注意力网络的摆动交易策略

    StockFormer: A Swing Trading Strategy Based on STL Decomposition and Self-Attention Networks. (arXiv:2401.06139v1 [q-fin.TR])

    [http://arxiv.org/abs/2401.06139](http://arxiv.org/abs/2401.06139)

    StockFormer是一种基于STL分解和自注意力网络的摆动交易策略，使用TopKDropout方法来提高股票选取能力。在测试中，其预测结果优于其他十个行业模型，达到了62.39%的市场趋势检测准确率，并取得了在关键预测准确度指标上的优秀表现。在回测中，StockFormer的摆动交易策略累计收益率为13.19%。

    

    在持续的市场重新校准和投资者乐观情绪增加的背景下，美国股市正在经历复苏，这促使我们需要一些先进的工具来保护和增长投资组合。针对这一需求，我们介绍了一种名为"Stockformer"的先进深度学习框架，用于优化摆动交易，并采用了TopKDropout方法来增强股票选取能力。通过整合STL分解和自注意力网络，Stockformer利用标普500的复杂数据来提升股票收益预测的准确性。我们的方法包括将数据分为训练和验证集（2021年1月至2023年1月）以及测试集（2023年2月至6月）。在测试期间，Stockformer的预测结果优于其他十个行业模型，在关键预测准确度指标（MAE，RMSE，MAPE）方面具有卓越的精度，检测市场趋势的准确率达到了62.39%。在我们的回测中，Stockformer的摆动交易策略累计收益率为13.19%，年化收益为...

    Amidst ongoing market recalibration and increasing investor optimism, the U.S. stock market is experiencing a resurgence, prompting the need for sophisticated tools to protect and grow portfolios. Addressing this, we introduce "Stockformer," a cutting-edge deep learning framework optimized for swing trading, featuring the TopKDropout method for enhanced stock selection. By integrating STL decomposition and self-attention networks, Stockformer utilizes the S&P 500's complex data to refine stock return predictions. Our methodology entailed segmenting data for training and validation (January 2021 to January 2023) and testing (February to June 2023). During testing, Stockformer's predictions outperformed ten industry models, achieving superior precision in key predictive accuracy indicators (MAE, RMSE, MAPE), with a remarkable accuracy rate of 62.39% in detecting market trends. In our backtests, Stockformer's swing trading strategy yielded a cumulative return of 13.19% and an annualized r
    
[^88]: QuasiNet: 一种具有可训练乘积层的神经网络

    QuasiNet: a neural network with trainable product layers. (arXiv:2401.06137v1 [cs.NE])

    [http://arxiv.org/abs/2401.06137](http://arxiv.org/abs/2401.06137)

    QuasiNet是一种新的神经网络模型，通过可训练的乘积层解决了小规模隐藏神经元下传统神经网络在难问题上的有限收敛问题，具有更高的成功率。

    

    传统神经网络在类似XOR或奇偶校验等难题的小规模隐藏神经元下只能实现有限的收敛。为了提高神经网络在这些问题上的成功率，我们提出了一种新的神经网络模型，受现有具有所谓乘积神经元和由经典误差反向传播推导出的学习规则启发，优雅地解决了互斥情况的问题。与现有的具有预设且不可调节权重的乘积神经元不同，我们的神经元乘积层也能够学习。我们测试了该模型，并将其成功率与传统的多层感知机在前述问题和其他难题（如两个螺旋）中进行了比较。我们的结果表明，我们的模型比传统的多层感知机更成功，并且在许多任务和应用中具有潜力。

    Classical neural networks achieve only limited convergence in hard problems such as XOR or parity when the number of hidden neurons is small. With the motivation to improve the success rate of neural networks in these problems, we propose a new neural network model inspired by existing neural network models with so called product neurons and a learning rule derived from classical error backpropagation, which elegantly solves the problem of mutually exclusive situations. Unlike existing product neurons, which have weights that are preset and not adaptable, our product layers of neurons also do learn. We tested the model and compared its success rate to a classical multilayer perceptron in the aforementioned problems as well as in other hard problems such as the two spirals. Our results indicate that our model is clearly more successful than the classical MLP and has the potential to be used in many tasks and applications.
    
[^89]: 一种用于实现工业物联网中URLLC的分布式神经线性Thompson采样框架

    A Distributed Neural Linear Thompson Sampling Framework to Achieve URLLC in Industrial IoT. (arXiv:2401.06135v1 [cs.NI])

    [http://arxiv.org/abs/2401.06135](http://arxiv.org/abs/2401.06135)

    本文提出了一种名为DISNETS的分布式神经线性Thompson采样框架，通过融合两种方法的优点，通过强化学习和利用来自gNB的反馈信号，UE可以自主选择传输资源，并进行训练。

    

    工业物联网（IIoT）网络将提供超可靠低延迟通信（URLLC）以支持生产链中的关键过程。然而，用于分配无线资源的标准协议可能无法优化延迟-可靠性权衡，特别是对于上行通信。在本文中，我们提出了一种名为DISNETS的分布式组合神经线性Thompson采样（DIStributed combinatorial NEural linear Thompson Sampling）框架，该框架融合了两种方法的优点。通过利用来自gNB的反馈信号和强化学习，UE可以自主选择传输资源，并进行训练。

    Industrial Internet of Things (IIoT) networks will provide Ultra-Reliable Low-Latency Communication (URLLC) to support critical processes underlying the production chains. However, standard protocols for allocating wireless resources may not optimize the latency-reliability trade-off, especially for uplink communication. For example, centralized grant-based scheduling can ensure almost zero collisions, but introduces delays in the way resources are requested by the User Equipments (UEs) and granted by the gNB. In turn, distributed scheduling (e.g., based on random access), in which UEs autonomously choose the resources for transmission, may lead to potentially many collisions especially when the traffic increases. In this work we propose DIStributed combinatorial NEural linear Thompson Sampling (DISNETS), a novel scheduling framework that combines the best of the two worlds. By leveraging a feedback signal from the gNB and reinforcement learning, the UEs are trained to autonomously opt
    
[^90]: Patchscope: 一个统一的框架，用于检查语言模型的隐藏表示

    Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])

    [http://arxiv.org/abs/2401.06102](http://arxiv.org/abs/2401.06102)

    本论文提出了一个叫做Patchscope的框架，用于检查语言模型的隐藏表示。该框架不仅统一了先前的检查技术，还解决了其中一些问题，并且还开辟了新的可能性。

    

    检查大型语言模型（LLM）的隐藏表示中编码的信息可以解释模型的行为并验证其与人类价值观的一致性。鉴于LLM生成人类可理解文本的能力，我们建议利用模型本身以自然语言解释其内部表示。我们引入了一个称为Patchscopes的框架，并展示了如何使用它来回答关于LLM计算的各种研究问题。我们表明，先前基于将表示投影到词汇空间和干预LLM计算的可解释性方法，可以看作是该框架的特殊实例。此外，通过Patchscope可以弥补优势，如检查早期层失败或表达能力不足。除了统一先前的检查技术，Patchscopes还开辟了新的可能性，例如使用更强大的模型来解释较小模型的表示。

    Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
    
[^91]: 基于树的可变系数模型介绍

    A tree-based varying coefficient model. (arXiv:2401.05982v1 [stat.ML])

    [http://arxiv.org/abs/2401.05982](http://arxiv.org/abs/2401.05982)

    本论文介绍了一种基于树的可变系数模型，使用循环梯度提升机进行建模，实现了逐维早停和特征重要性评分，该模型能够产生与基于神经网络的VCM相当的结果。

    

    本论文介绍了一种基于树的可变系数模型(VCM)，其中可变系数使用Delong等人(2023)的循环梯度提升机(CGBM)进行建模。使用CGBM对系数函数进行建模，可以进行逐维早停和特征重要性评分。逐维早停不仅可以减少维度特定的过拟合风险，还可以揭示维度之间模型复杂性的差异。使用特征重要性评分可以进行简单的特征选择和易于解释的模型解释。该模型在Richman和Wüthrich（2023）使用的相同的模拟和真实数据示例上进行评估，结果表明，它在样本外损失方面产生了与他们的基于神经网络的VCM LocalGLMnet相当的结果。

    The paper introduces a tree-based varying coefficient model (VCM) where the varying coefficients are modelled using the cyclic gradient boosting machine (CGBM) from Delong et al. (2023). Modelling the coefficient functions using a CGBM allows for dimension-wise early stopping and feature importance scores. The dimension-wise early stopping not only reduces the risk of dimension-specific overfitting, but also reveals differences in model complexity across dimensions. The use of feature importance scores allows for simple feature selection and easy model interpretation. The model is evaluated on the same simulated and real data examples as those used in Richman and W\"uthrich (2023), and the results show that it produces results in terms of out of sample loss that are comparable to those of their neural network-based VCM called LocalGLMnet.
    
[^92]: 通过连接主义音素识别中的类熵测量进行分割边界检测

    Segment Boundary Detection via Class Entropy Measurements in Connectionist Phoneme Recognition. (arXiv:2401.05717v1 [eess.AS])

    [http://arxiv.org/abs/2401.05717](http://arxiv.org/abs/2401.05717)

    本文研究了使用连接主义音素识别器的类熵来预测音素类之间的时间边界，并比较了不同方法的精确度和召回率。

    

    本文研究了使用连接主义音素识别器的输出的类熵来预测音素类之间的时间边界的可能性。其原理是，熵的值应该在两个由识别网络很好建模（已知）的片段之间的过渡附近增加，因为它是不确定性的一个度量。类熵的优势在于它的简单性，因为每个类的后验概率在连接主义音素识别中是可用的。熵和一些基于熵差异的度量被单独和组合使用。用于预测边界的决策方法从简单的阈值到基于神经网络的过程不等。不同的方法在精确度方面进行比较，以C（在参考时间的10或20毫秒内预测的边界数量）与预测边界总数之间的比率和召回率为度量。

    This article investigates the possibility to use the class entropy of the output of a connectionist phoneme recogniser to predict time boundaries between phonetic classes. The rationale is that the value of the entropy should increase in proximity of a transition between two segments that are well modelled (known) by the recognition network since it is a measure of uncertainty. The advantage of this measure is its simplicity as the posterior probabilities of each class are available in connectionist phoneme recognition. The entropy and a number of measures based on differentiation of the entropy are used in isolation and in combination. The decision methods for predicting the boundaries range from simple thresholds to neural network based procedure. The different methods are compared with respect to their precision, measured in terms of the ratio between the number C of predicted boundaries within 10 or 20 msec of the reference and the total number of predicted boundaries, and recall, 
    
[^93]: 卧底特工：训练骗人的LLM以通过安全训练

    Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])

    [http://arxiv.org/abs/2401.05566](http://arxiv.org/abs/2401.05566)

    该论文研究了在大型语言模型中训练并保持持久的欺骗性行为，这种行为无法被当前的安全训练技术移除。

    

    人类有能力进行战略性的欺骗行为：在大多数情况下表现出有益的行为，但在有机会的时候却表现出截然不同的行为以追求其他目标。如果一个AI系统学会了这样的欺骗策略，是否能够通过当前最先进的安全训练技术检测并移除它？为了研究这个问题，我们构建了大型语言模型（LLM）中欺骗行为的概念验证样例。例如，我们训练模型，在提示语句中将年份设为2023时编写安全代码，但在年份设为2024时插入有漏洞的代码。我们发现，这种暗门行为可以被持续保留，无法通过标准的安全训练技术（包括监督微调、强化学习和对抗性训练）移除。暗门行为在最大的模型和训练成产生思维链的模型中最为持久。

    Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
    
[^94]: 功能图模型：结构实现离线数据驱动优化

    Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v1 [cs.LG])

    [http://arxiv.org/abs/2401.05442](http://arxiv.org/abs/2401.05442)

    功能图模型（FGMs）通过结构实现了样本高效的数据驱动优化。

    

    虽然机器学习模型通常是为了解决预测问题而训练的，但我们经常希望将它们用于优化问题。例如，给定一组蛋白质及其对应的荧光水平的数据集，我们可能希望为具有最高荧光的新蛋白质进行优化。这种数据驱动的优化（DDO）面临着一系列挑战，超出了标准预测问题中的挑战，因为我们需要成功预测在训练集中没有见过的优于最佳设计的新设计的性能的模型。从理论上讲，甚至不清楚现有方法什么时候甚至能比简单地选择数据集中最佳设计的朴素方法执行得更好。在本文中，我们研究了如何通过结构实现高效的数据驱动优化。为了形式化结构的概念，我们引入了功能图模型（FGMs）并从理论上展示了它们如何通过分解实现基于数据的优化。

    While machine learning models are typically trained to solve prediction problems, we might often want to use them for optimization problems. For example, given a dataset of proteins and their corresponding fluorescence levels, we might want to optimize for a new protein with the highest possible fluorescence. This kind of data-driven optimization (DDO) presents a range of challenges beyond those in standard prediction problems, since we need models that successfully predict the performance of new designs that are better than the best designs seen in the training set. It is not clear theoretically when existing approaches can even perform better than the naive approach that simply selects the best design in the dataset. In this paper, we study how structure can enable sample-efficient data-driven optimization. To formalize the notion of structure, we introduce functional graphical models (FGMs) and show theoretically how they can provide for principled data-driven optimization by decomp
    
[^95]: 可穿戴应用中的表示学习：在缺失数据情况下的探索

    Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v1 [eess.SP])

    [http://arxiv.org/abs/2401.05437](http://arxiv.org/abs/2401.05437)

    本论文研究了可穿戴应用中表示学习的问题，特别是在缺失数据情况下。作者通过比较Transformer模型和统计方法的性能，发现Transformer模型在变化频繁的信号的缺失数据填充方面表现优秀。此研究为基于掩码的自监督学习任务的设计和开发提供了洞察。

    

    可穿戴设备持续收集传感器数据并用于推断个体的行为，如睡眠、体力活动和情绪。尽管在这个领域有很大的兴趣和进展，但由于数据质量低和数据注释有限，建模真实环境中的多模式传感器数据仍然具有挑战性。本研究探讨了用于填充缺失可穿戴数据的表示学习，并将其与最先进的统计方法进行了比较。我们使用10个生理和行为信号的变化率不同的掩码比率，研究了Transformer模型在缺失数据填充上的性能。结果显示，Transformer模型在变化频繁的信号的缺失数据填充方面优于基准模型，但对于单调信号则不然。我们进一步研究了填充策略和掩码比率对下游分类任务的影响。本研究为基于掩码的自监督学习任务的设计和开发提供了洞察。

    Wearable devices continuously collect sensor data and use it to infer an individual's behavior, such as sleep, physical activity, and emotions. Despite the significant interest and advancements in this field, modeling multimodal sensor data in real-world environments is still challenging due to low data quality and limited data annotations. In this work, we investigate representation learning for imputing missing wearable data and compare it with state-of-the-art statistical approaches. We investigate the performance of the transformer model on 10 physiological and behavioral signals with different masking ratios. Our results show that transformers outperform baselines for missing data imputation of signals that change more frequently, but not for monotonic signals. We further investigate the impact of imputation strategies and masking rations on downstream classification tasks. Our study provides insights for the design and development of masking-based self-supervised learning tasks a
    
[^96]: 关于金融中的三个因果性困境: 时间分辨率、非平稳性和潜在因素

    On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors. (arXiv:2401.05414v1 [q-fin.ST])

    [http://arxiv.org/abs/2401.05414](http://arxiv.org/abs/2401.05414)

    本文从因果性的角度系统研究了金融领域中的三个困境：时间分辨率不匹配、非平稳性和未知因果因素，并提出了解决方案。

    

    金融数据本质上是时间序列数据，因此存在三个基本问题：时间分辨率不匹配、分布的时变性-非平稳性以及重要但未知/未观测的因果因素。在本文中，我们从因果性的角度系统地研究金融中的这三个困境。具体而言，我们重新审视了这些问题，并在因果性的背景下提出了新颖而有启发性的解决方案，为未来研究提供一个基础。

    Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area.
    
[^97]: 通过多级域对齐实现通用的睡眠分期

    Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])

    [http://arxiv.org/abs/2401.05363](http://arxiv.org/abs/2401.05363)

    本文提出了一种通用的睡眠分期方法，通过引入域泛化概念，结合多级特征对齐的思想，提高了模型对未见过数据集的泛化能力。

    

    自动睡眠分期对于睡眠评估和疾病诊断至关重要。现有的大多数方法依赖于特定数据集，并且仅适用于相同数据集的未见过的数据集。本文引入了域泛化概念到自动睡眠分期中，并提出了通用的睡眠分期任务，旨在提高模型对未见过的数据集的泛化能力。受到现有的域泛化方法的启发，我们采用特征对齐思想，提出了一种名为SleepDG的框架来解决该问题。考虑到局部显著特征和时序特征对于睡眠分期都很重要，我们提出了一种多级特征对齐，将时代级和序列级特征对齐来学习域不变特征表示。具体而言，我们设计了一种时代级特征对齐方法，对不同睡眠时代的特征分布进行对齐。

    Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
    
[^98]: RoSA: 通过鲁棒适应实现准确的参数高效微调

    RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])

    [http://arxiv.org/abs/2401.04679](http://arxiv.org/abs/2401.04679)

    RoSA是一种新的PEFT方法，通过在预训练权重上训练低秩和高度稀疏的组件，以高效近似完全微调的性能，来实现准确的参数高效微调。在多个生成任务中，RoSA表现出优于其他方法的性能。

    

    我们研究了在大语言模型 (LLMs) 的背景下，能够在有限的计算和内存预算下提供良好准确性的参数高效微调 (PEFT) 方法。我们提出了一种新的PEFT方法，称为RoSA，受鲁棒主成分分析 (PCA) 的启发，它在一组固定的预训练权重上共同训练$\textit{低秩}$和$\textit{高度稀疏}$的组件，以高效近似完全微调（FFT）解决方案的性能。我们展示了RoSA在一系列具有挑战性的生成任务上的性能，例如小学数学和SQL查询生成，这些任务需要进行微调以获得良好性能，我们证明了在相同的参数预算下，RoSA优于LoRA和纯粹的稀疏微调。我们通过稀疏GPU内核为RoSA提供系统支持，以补充训练算法，从而实现内存和计算效率的训练。我们的代码将在https://github.com/IST-DASLab上提供。

    We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
    
[^99]: 鲁棒的物理信息神经网络

    Robust Physics Informed Neural Networks. (arXiv:2401.02300v1 [cs.LG])

    [http://arxiv.org/abs/2401.02300](http://arxiv.org/abs/2401.02300)

    引入了一种鲁棒的物理信息神经网络（RPINNs）来近似偏微分方程（PDE）的解，该网络在训练过程中考虑了PDE的控制物理法则，解决了传统PINNs中损失函数与真实误差不鲁棒的问题。

    

    我们引入了一种鲁棒版本的物理信息神经网络（RPINNs）来近似偏微分方程（PDE）的解。标准的物理信息神经网络（PINN）在学习过程中考虑了由PDE描述的控制物理法则。该网络在由物理域和边界随机选择的数据集上进行训练。PINNs已成功应用于解决由PDE和边界条件描述的各种问题。传统PINNs中的损失函数基于PDE的强残差。这种PINNs中的损失函数通常对真实误差不具有鲁棒性。PINNs中的损失函数与真实误差可能相差很大，这使得训练过程更加困难。特别是，如果我们不知道精确解，我们就不能估计训练过程是否已经以所需的精度收敛到解。这在我们不知道精确解时尤其正确，

    We introduce a Robust version of the Physics-Informed Neural Networks (RPINNs) to approximate the Partial Differential Equations (PDEs) solution. Standard Physics Informed Neural Networks (PINN) takes into account the governing physical laws described by PDE during the learning process. The network is trained on a data set that consists of randomly selected points in the physical domain and its boundary. PINNs have been successfully applied to solve various problems described by PDEs with boundary conditions. The loss function in traditional PINNs is based on the strong residuals of the PDEs. This loss function in PINNs is generally not robust with respect to the true error. The loss function in PINNs can be far from the true error, which makes the training process more difficult. In particular, we do not know if the training process has already converged to the solution with the required accuracy. This is especially true if we do not know the exact solution, so we cannot estimate the 
    
[^100]: 推进TTP分析：利用仅编码器和仅解码器语言模型并提升的生成模型的力量

    Advancing TTP Analysis: Harnessing the Power of Encoder-Only and Decoder-Only Language Models with Retrieval Augmented Generation. (arXiv:2401.00280v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2401.00280](http://arxiv.org/abs/2401.00280)

    本研究探索了如何利用编码器模型和解码器模型来理解和总结网络攻击过程中的策略和目的，使用检索增强生成技术来提取相关上下文，并解决了现有模型在网络安全领域中产生错误信息的问题。

    

    战术，技术和程序（TTPs）概述了攻击者利用漏洞的方法。由于假定的专业知识，复杂的依赖关系和内在的模糊性，对MITRE ATT＆CK框架中的TTPs的解释对于网络安全从业人员来说可能具有挑战性。与此同时，大型语言模型（LLMs）的进步导致了最近在研究中探索其在网络安全操作中的用途的激增。这引起了我们的疑问，仅编码器（例如RoBERTa）和仅解码器（例如GPT-3.5）LLMs对于理解和总结TTPs以通知分析人员有关网络攻击过程的预期目的（即策略）的能力如何。最先进的LLMs已经显示出容易产生错误信息，这在网络安全等关键领域是有问题的。因此，我们提出使用检索增强生成（RAG）技术来为仅解码器的LLMs提取每个网络攻击过程的相关上下文（无需微调）。

    Tactics, Techniques, and Procedures (TTPs) outline the methods attackers use to exploit vulnerabilities. The interpretation of TTPs in the MITRE ATT&CK framework can be challenging for cybersecurity practitioners due to presumed expertise, complex dependencies, and inherent ambiguity. Meanwhile, advancements with Large Language Models (LLMs) have led to recent surge in studies exploring its uses in cybersecurity operations. This leads us to question how well encoder-only (e.g., RoBERTa) and decoder-only (e.g., GPT-3.5) LLMs can comprehend and summarize TTPs to inform analysts of the intended purposes (i.e., tactics) of a cyberattack procedure. The state-of-the-art LLMs have shown to be prone to hallucination by providing inaccurate information, which is problematic in critical domains like cybersecurity. Therefore, we propose the use of Retrieval Augmented Generation (RAG) techniques to extract relevant contexts for each cyberattack procedure for decoder-only LLMs (without fine-tuning)
    
[^101]: 用于推荐系统评估的综合调查

    A Comprehensive Survey of Evaluation Techniques for Recommendation Systems. (arXiv:2312.16015v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2312.16015](http://arxiv.org/abs/2312.16015)

    本文介绍了一套综合的推荐系统评估指标，包括相似性指标、候选生成指标、预测指标、排序指标和业务指标。

    

    推荐系统的有效性对于用户在在线平台上的参与和满意度至关重要。随着这些推荐系统越来越影响用户的选择，它们的评估不仅仅局限于技术性能，而变得对于业务成功至关重要。本文通过引入一套全面的指标来解决推荐系统评估的多方面特性，每个指标专门捕捉系统性能的不同方面。我们讨论了以下几个方面的指标：相似性指标：用于量化基于内容的过滤机制的准确性，并评估协同过滤技术的准确性；候选生成指标：用于评估系统有效地识别广泛但相关的项目的能力；预测指标：用于评估预测的用户偏好的准确性；排序指标：用于评估推荐顺序的有效性；业务指标：用于对齐推荐系统的性能。

    The effectiveness of recommendation systems is pivotal to user engagement and satisfaction in online platforms. As these recommendation systems increasingly influence user choices, their evaluation transcends mere technical performance and becomes central to business success. This paper addresses the multifaceted nature of recommendations system evaluation by introducing a comprehensive suite of metrics, each tailored to capture a distinct aspect of system performance. We discuss  * Similarity Metrics: to quantify the precision of content-based filtering mechanisms and assess the accuracy of collaborative filtering techniques.  * Candidate Generation Metrics: to evaluate how effectively the system identifies a broad yet relevant range of items.  * Predictive Metrics: to assess the accuracy of forecasted user preferences.  * Ranking Metrics: to evaluate the effectiveness of the order in which recommendations are presented.  * Business Metrics: to align the performance of the recommendat
    
[^102]: NPHardEval: 通过复杂性类别对大型语言模型的推理能力进行动态基准评估

    NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2312.14890](http://arxiv.org/abs/2312.14890)

    NPHardEval是一个新的基准，旨在评估大型语言模型在900个算法问题上的推理能力，扩展到NP-Hard复杂性类别。

    

    复杂推理能力是当前大型语言模型的最重要特征之一，它也被用于在复杂决策任务中起到了重要作用。因此，研究大型语言模型的推理能力至关重要：已经建立了许多基准来评估大型语言模型的推理能力。然而，目前的基准在提供大型语言模型推理能力的全面评估方面还不够，同时也容易出现过拟合的风险，因为这些基准是公开可访问且静态的，使得模型有可能根据特定的基准指标调整其响应，从而夸大其性能。针对这些限制，我们的研究引入了一个新的基准，名为NPHardEval。该基准旨在评估大型语言模型在广泛的900个算法问题上的推理能力，涵盖了NP-Hard复杂性类别。

    Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
    
[^103]: 深度神经网络和任意维度上的有限元素

    Deep Neural Networks and Finite Elements of Any Order on Arbitrary Dimensions. (arXiv:2312.14276v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2312.14276](http://arxiv.org/abs/2312.14276)

    本研究表明，使用ReLU和ReLU^2激活函数的深度神经网络可以有效地表示任意维度上的Lagrange有限元函数，并且能够在特定或任意单纯形网格上生成一般连续分段多项式函数。

    

    在这项研究中，我们建立了使用ReLU和ReLU^2激活函数的深度神经网络可以有效地表示任意维度上各种单纯形网格中的Lagrange有限元函数。我们引入了两种新颖的公式，用于全局表达Lagrange元素的基函数，既适用于特定网格，也适用于任意网格。这些公式基于元素的几何分解，结合了高维单纯形网格、重心坐标函数和线性元素的全局基函数的一些见解性和基本性质。这种表示理论为这样的深度神经网络提供了一个自然的逼近结果。我们的发现首次展示了深度神经网络如何系统地在特定或任意单纯形网格上生成一般连续分段多项式函数。

    In this study, we establish that deep neural networks employing ReLU and ReLU$^2$ activation functions can effectively represent Lagrange finite element functions of any order on various simplicial meshes in arbitrary dimensions. We introduce two novel formulations for globally expressing the basis functions of Lagrange elements, tailored for both specific and arbitrary meshes. These formulations are based on a geometric decomposition of the elements, incorporating several insightful and essential properties of high-dimensional simplicial meshes, barycentric coordinate functions, and global basis functions of linear elements. This representation theory facilitates a natural approximation result for such deep neural networks. Our findings present the first demonstration of how deep neural networks can systematically generate general continuous piecewise polynomial functions on both specific or arbitrary simplicial meshes.
    
[^104]: 加速局部解释的全局汇总

    Accelerating the Global Aggregation of Local Explanations. (arXiv:2312.07991v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.07991](http://arxiv.org/abs/2312.07991)

    我们提出了加速Anchor算法的全局汇总技术，旨在计算出对模型影响最大的前k个单词。

    

    局部解释方法突出显示对文档分类结果有重要影响的输入标记。例如，Anchor算法对分类器对标记变化的敏感性进行了统计分析。对数据集进行局部解释的全局汇总提供了模型的全局解释。这种汇总旨在检测对模型影响最大的单词，从而提供宝贵的对模型的理解，例如模型在训练中学到了什么以及哪些对抗性示例暴露了模型的弱点。然而，标准的汇总方法计算成本高：简单的实现对每个文档的每个标记应用昂贵的算法，因此在短期分析会话的范围内对一个普通用户来说是不可行的。

    Local explanation methods highlight the input tokens that have a considerable impact on the outcome of classifying the document at hand. For example, the Anchor algorithm applies a statistical analysis of the sensitivity of the classifier to changes in the token. Aggregating local explanations over a dataset provides a global explanation of the model. Such aggregation aims to detect words with the most impact, giving valuable insights about the model, like what it has learned in training and which adversarial examples expose its weaknesses. However, standard aggregation methods bear a high computational cost: a na\"ive implementation applies a costly algorithm to each token of each document, and hence, it is infeasible for a simple user running in the scope of a short analysis session. % We devise techniques for accelerating the global aggregation of the Anchor algorithm. Specifically, our goal is to compute a set of top-$k$ words with the highest global impact according to different a
    
[^105]: 具有人工智能的通信系统中的生成网络层

    Generative Network Layer for Communication Systems with Artificial Intelligence. (arXiv:2312.05398v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2312.05398](http://arxiv.org/abs/2312.05398)

    这个论文提出了一种使用生成式人工智能在中间或边缘网络节点上的生成网络层，通过将潜在表示压缩后的提示生成图像，可以显著改善网络中所需的数据速率。

    

    传统的网络层的作用是通过中间网络节点将数据包从源传输到目的地。我们提出了一种在中间或边缘网络节点使用生成式人工智能（GenAI）的生成网络层，并分析其对网络中所需数据速率的影响。我们进行了一个案例研究，其中使用GenAI辅助节点从包含大幅压缩的潜在表示的提示中生成图像。在图像质量约束下进行的网络流分析结果显示，生成网络层可以实现超过100%的数据速率要求改进。

    The traditional role of the network layer is the transfer of packet replicas from source to destination through intermediate network nodes. We present a generative network layer that uses Generative AI (GenAI) at intermediate or edge network nodes and analyze its impact on the required data rates in the network. We conduct a case study where the GenAI-aided nodes generate images from prompts that consist of substantially compressed latent representations. The results from network flow analyses under image quality constraints show that the generative network layer can achieve an improvement of more than 100% in terms of the required data rate.
    
[^106]: 通信高效的半分散网络联邦优化

    Communication-Efficient Federated Optimization over Semi-Decentralized Networks. (arXiv:2311.18787v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.18787](http://arxiv.org/abs/2311.18787)

    本文提出了一种基于半分散网络的通信高效算法PISCO, 通过概率性的代理间和代理与服务器之间的通信，实现了通信效率与收敛速度的折衷。

    

    在大规模的联邦和分散式学习中，通信效率是最具挑战性的瓶颈之一。本文提出了一种半分散通信协议下的通信高效算法PISCO，通过概率性的代理间和代理与服务器之间的通信，实现了通信效率与收敛速度的折衷。PISCO算法通过梯度追踪和多个本地更新保证了对数据异质性的鲁棒性。我们证明了PISCO算法在非凸问题上的收敛速度，并展示了在数量方面，PISCO算法具有线性加速的优势。

    In large-scale federated and decentralized learning, communication efficiency is one of the most challenging bottlenecks. While gossip communication -- where agents can exchange information with their connected neighbors -- is more cost-effective than communicating with the remote server, it often requires a greater number of communication rounds, especially for large and sparse networks. To tackle the trade-off, we examine the communication efficiency under a semi-decentralized communication protocol, in which agents can perform both agent-to-agent and agent-to-server communication in a probabilistic manner. We design a tailored communication-efficient algorithm over semi-decentralized networks, referred to as PISCO, which inherits the robustness to data heterogeneity thanks to gradient tracking and allows multiple local updates for saving communication. We establish the convergence rate of PISCO for nonconvex problems and show that PISCO enjoys a linear speedup in terms of the number
    
[^107]: 通过联邦迁移学习对基础模型进行接地：一个通用框架

    Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v7 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.17431](http://arxiv.org/abs/2311.17431)

    本论文提出了一个通用框架，通过联邦迁移学习将基础模型接地，以解决面临的挑战，如受限的计算资源、数据隐私、模型异构性和模型所有权。这个框架可以帮助释放基础模型的潜力，并在不同行业中产生重要影响。

    

    基于广泛知识和强大的新兴能力编码的Foundation Models（FMs），如GPT-4，在各种自然语言处理和计算机视觉任务中取得了显著成功。通过将FMs适应于特定领域任务或增加领域特定知识来对其进行接地，我们可以充分发挥FMs的潜力。然而，接地FMs面临着多个挑战，主要是受限的计算资源、数据隐私、模型异构性和模型所有权。联邦迁移学习（FTL），即联邦学习和迁移学习的结合，为解决这些挑战提供了有希望的解决方案。近年来，学术界和工业界对通过FTL-FM利用FMs进行接地的需求强烈增长。受到FTL-FM研究的强劲增长和FTL-FM对工业应用的潜在影响的推动，我们提出了一个FTL-FM框架，用于在联邦学习环境中建立FMs的接地问题。

    Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and powerful emergent abilities have achieved remarkable success in various natural language processing and computer vision tasks. Grounding FMs by adapting them to domain-specific tasks or augmenting them with domain-specific knowledge enables us to exploit the full potential of FMs. However, grounding FMs faces several challenges, stemming primarily from constrained computing resources, data privacy, model heterogeneity, and model ownership. Federated Transfer Learning (FTL), the combination of federated learning and transfer learning, provides promising solutions to address these challenges. In recent years, the need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in both academia and industry. Motivated by the strong growth in FTL-FM research and the potential impact of FTL-FM on industrial applications, we propose an FTL-FM framework that formulates problems of grounding FMs in the federated lea
    
[^108]: 大型语言模型的多阶段协作知识蒸馏在半监督序列生成任务中的应用

    Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2311.08640](http://arxiv.org/abs/2311.08640)

    将大型语言模型的知识通过多阶段协作蒸馏的方式应用于半监督序列生成任务中，可以显著提高模型的泛化能力和性能。

    

    我们研究了半监督序列生成任务，在这种任务中，标记数据太少以至于无法有效地微调模型，同时在大型语言模型 (LLM) 中进行少样本提示的性能也不够理想，尤其是对于一些昂贵且对预训练的 LLM 不熟悉的任务，如解析。本文发现，从上下文学习的 LLM 蒸馏出的学生模型在这些任务上通常比其教师模型具有更好的泛化能力。基于这一发现，我们提出了一种新的方法 - 大型语言模型的多阶段协作知识蒸馏 (MCKD) - 用于这些任务。MCKD 首先进行少样本提示，让LLM为无标签数据生成伪标签。在每个中间知识蒸馏 (KD) 阶段，使用伪标签数据的不重叠分区来训练一对新的学生模型。然后，每个学生模型为其未见分区生成新的和改进的伪标签，在下一个蒸馏阶段中使用。我们展示了该方法的优势。

    We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
    
[^109]: 当公平性遇见隐私：通过成员推断攻击探索公平二分类器中的隐私威胁

    When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers through Membership Inference Attacks. (arXiv:2311.03865v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2311.03865](http://arxiv.org/abs/2311.03865)

    本研究探索了公平二分类器中的隐私威胁，并揭示了针对公平增强模型的基于分数的成员推断攻击的无效性。同时，公平性方法可能导致训练数据中大多数子群体的预测性能下降。

    

    先前的研究开发了针对具有歧视行为的有偏模型的公平方法，以达到公平预测的目标。然而，最近的研究发现这些模型在基于分数的成员推断攻击中存在潜在的漏洞。在这些攻击中，对手可以通过分析模型的预测分数推断出特定数据样本是否在训练过程中使用。然而，我们的调查发现，针对公平增强模型的基于分数的成员推断攻击是无效的。针对这些攻击训练的模型退化为简单的阈值模型，从而导致攻击性能降低。与此同时，我们观察到公平性方法往往导致训练数据中的大多数子群体的预测性能下降。这提高了成功攻击的难度，同时扩大了成员和非成员数据之间的预测差距。

    Previous studies have developed fairness methods for biased models that exhibit discriminatory behaviors towards specific subgroups. While these models have shown promise in achieving fair predictions, recent research has identified their potential vulnerability to score-based membership inference attacks (MIAs). In these attacks, adversaries can infer whether a particular data sample was used during training by analyzing the model's prediction scores. However, our investigations reveal that these score-based MIAs are ineffective when targeting fairness-enhanced models in binary classifications. The attack models trained to launch the MIAs degrade into simplistic threshold models, resulting in lower attack performance. Meanwhile, we observe that fairness methods often lead to prediction performance degradation for the majority subgroups of the training data. This raises the barrier to successful attacks and widens the prediction gaps between member and non-member data. Building upon th
    
[^110]: 关于扩散模型的泛化属性

    On the Generalization Properties of Diffusion Models. (arXiv:2311.01797v1 [cs.LG])

    [http://arxiv.org/abs/2311.01797](http://arxiv.org/abs/2311.01797)

    本文对扩散模型的泛化属性进行了理论研究，建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，并在停止训练时可以避免维度诅咒。进一步将定量分析扩展到了数据依赖的情景。

    

    扩散模型是一类生成模型，用于建立一个随机传输映射，将经验观测到的但未知的目标分布与已知的先验分布联系起来。尽管在实际应用中取得了显著的成功，但对其泛化能力的理论理解仍未充分发展。本文对扩散模型的泛化属性进行了全面的理论研究。我们建立了基于评分法的扩散模型的训练动态中泛化差距的理论估计，表明在样本大小$n$和模型容量$m$上都存在多项式小的泛化误差($O(n^{-2/5}+m^{-4/5})$)，在停止训练时可以避免维度诅咒（即数据维度不呈指数级增长）。此外，我们将定量分析扩展到了一个数据依赖的情景，其中目标分布被描绘为一系列的概率密度函数。

    Diffusion models are a class of generative models that serve to establish a stochastic transport map between an empirically observed, yet unknown, target distribution and a known prior. Despite their remarkable success in real-world applications, a theoretical understanding of their generalization capabilities remains underdeveloped. This work embarks on a comprehensive theoretical exploration of the generalization attributes of diffusion models. We establish theoretical estimates of the generalization gap that evolves in tandem with the training dynamics of score-based diffusion models, suggesting a polynomially small generalization error ($O(n^{-2/5}+m^{-4/5})$) on both the sample size $n$ and the model capacity $m$, evading the curse of dimensionality (i.e., not exponentially large in the data dimension) when early-stopped. Furthermore, we extend our quantitative analysis to a data-dependent scenario, wherein target distributions are portrayed as a succession of densities with progr
    
[^111]: 使用大型语言模型进行文本属性图的解缠表征学习

    Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])

    [http://arxiv.org/abs/2310.18152](http://arxiv.org/abs/2310.18152)

    本文提出了一个名为Disentangled Graph-Text Learner (DGTL)的模型，通过引入定制的解缠图神经网络（GNN）层，使得大型语言模型（LLMs）能够更好地理解文本属性图（TAGs）中的复杂结构关系。

    

    文本属性图（TAGs）在网络上非常常见，对于该类图，如引用网络、电子商务网络和社交网络的研究在网络社区中引起了相当大的关注。最近，大型语言模型（LLMs）在各种任务上展示了出色的能力。然而，现有的工作仅仅依靠提示信息来传达图结构信息给LLMs，因此对于TAGs中复杂的结构关系了解不足。为解决这个问题，本文提出了解缠图文学习器（DGTL）模型，能够增强LLMs对TAGs的推理和预测能力。我们的DGTL模型通过定制的解缠图神经网络（GNN）层将图结构信息纳入其中，使得LLMs能够捕捉多个结构因素中隐藏的复杂关系。

    Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
    
[^112]: 稳定的非凸-非凹训练通过线性插值

    Stable Nonconvex-Nonconcave Training via Linear Interpolation. (arXiv:2310.13459v1 [cs.LG])

    [http://arxiv.org/abs/2310.13459](http://arxiv.org/abs/2310.13459)

    本文通过理论分析指出，优化过程中的不稳定性通常是由损失函数的非单调性引起的，提出了一种稳定（大规模）神经网络训练的方法，即通过线性插值来利用“非扩张算子”的理论来解决。通过构建一种新的优化方案，松弛近似近端点（RAPP），发现了一族Lookahead算法，能够在协调部分单调问题中收敛，即使基本优化器采用梯度下降升级算法。

    

    本文提出了一种关于线性插值的理论分析，作为一种稳定（大规模）神经网络训练的方法。我们认为优化过程中的不稳定性通常是由损失函数的非单调性引起的，并展示了线性插值如何通过利用“非扩张算子”的理论来帮助解决这个问题。我们构建了一种新的优化方案，称为松弛近似近端点（RAPP），这是第一个明确的方法，能够实现完整范围内的协调部分单调问题的最后迭代收敛速率。该构造可扩展到约束和正则化设置。通过替换RAPP中的内部优化器，我们重新发现了Lookahead算法族，我们证明了这些算法在协调部分单调问题中的收敛性，即使基本优化器采用梯度下降升级算法。通过利用Lookahead继承性质，我们进一步扩展了Lookahead在协调部分单调问题中收敛的范围。

    This paper presents a theoretical analysis of linear interpolation as a principled method for stabilizing (large-scale) neural network training. We argue that instabilities in the optimization process are often caused by the nonmonotonicity of the loss landscape and show how linear interpolation can help by leveraging the theory of nonexpansive operators. We construct a new optimization scheme called relaxed approximate proximal point (RAPP), which is the first explicit method to achieve last iterate convergence rates for the full range of cohypomonotone problems. The construction extends to constrained and regularized settings. By replacing the inner optimizer in RAPP we rediscover the family of Lookahead algorithms for which we establish convergence in cohypomonotone problems even when the base optimizer is taken to be gradient descent ascent. The range of cohypomonotone problems in which Lookahead converges is further expanded by exploiting that Lookahead inherits the properties of 
    
[^113]: ZEST:基于注意力机制的零样本学习用于未见过的物联网设备分类

    ZEST: Attention-based Zero-Shot Learning for Unseen IoT Device Classification. (arXiv:2310.08036v1 [cs.NI])

    [http://arxiv.org/abs/2310.08036](http://arxiv.org/abs/2310.08036)

    ZEST是一种基于注意力机制的零样本学习框架，用于分类未见过的物联网设备。通过使用自注意力机制提取特征并利用生成模型生成伪数据，ZEST能够在性能上取得显著的改进。

    

    最近的研究工作提出了用于分类与网络连接的物联网设备的机器学习模型。然而，在模型训练期间，由于没有所有设备（因此没有它们的流量），仍然存在无法使用所有设备进行训练的实际挑战。这意味着在操作阶段需要对在训练阶段未见过的新设备进行分类。为了解决这个挑战，我们提出了一种基于自注意力机制的零样本学习框架ZEST，用于分类已知和未知的设备。ZEST包括：i）基于自注意力机制的网络特征提取器SANE，用于提取物联网流量的潜在空间表示；ii）一个生成模型，利用潜在特征训练解码器生成伪数据；iii）一个监督模型，用于基于生成的伪数据进行设备分类。我们对真实的物联网流量数据进行了大量实验；实验结果表明ZEST在性能上取得了显著的改进。

    Recent research works have proposed machine learning models for classifying IoT devices connected to a network. However, there is still a practical challenge of not having all devices (and hence their traffic) available during the training of a model. This essentially means, during the operational phase, we need to classify new devices not seen during the training phase. To address this challenge, we propose ZEST -- a ZSL (zero-shot learning) framework based on self-attention for classifying both seen and unseen devices. ZEST consists of i) a self-attention based network feature extractor, termed SANE, for extracting latent space representations of IoT traffic, ii) a generative model that trains a decoder using latent features to generate pseudo data, and iii) a supervised model that is trained on the generated pseudo data for classifying devices. We carry out extensive experiments on real IoT traffic data; our experiments demonstrate i) ZEST achieves significant improvement (in terms 
    
[^114]: 语义前向中继：面向6G合作通信的新框架

    Semantic-Forward Relaying: A Novel Framework Towards 6G Cooperative Communications. (arXiv:2310.07987v1 [cs.NI])

    [http://arxiv.org/abs/2310.07987](http://arxiv.org/abs/2310.07987)

    该论文提出了一种面向6G合作通信的新中继框架，通过提取传输语义特征减少转发负载，并提高网络的鲁棒性。设计了一种联合源信道编码算法，通过迭代交换外在信息来增强解码增益。仿真结果表明，即使在恶劣的信道条件下，该中继框架仍然可以有效地改善恢复的信息质量。

    

    本文提出了一种新的中继框架，语义前向（SF），用于面向第六代（6G）无线网络的合作通信。SF中继提取并传输语义特征，减少了转发负载，并提高了网络对内链路错误的鲁棒性。基于具有辅助信息的合作通信的理论基础和TURBO原理，我们设计了一种联合源信道编码算法，通过迭代交换外在信息来增强目的地的解码增益。令人惊讶的是，仿真结果表明，即使在恶劣的信道条件下，SF中继仍然可以有效地改善恢复的信息质量。

    This letter proposes a novel relaying framework, semantic-forward (SF), for cooperative communications towards the sixth-generation (6G) wireless networks. The SF relay extracts and transmits the semantic features, which reduces forwarding payload, and also improves the network robustness against intra-link errors. Based on the theoretical basis for cooperative communications with side information and the turbo principle, we design a joint source-channel coding algorithm to iteratively exchange the extrinsic information for enhancing the decoding gains at the destination. Surprisingly, simulation results indicate that even in bad channel conditions, SF relaying can still effectively improve the recovered information quality.
    
[^115]: 一个用于混合迪里切特和诺曼边界条件的神经预处理泊松求解器

    A Neural-preconditioned Poisson Solver for Mixed Dirichlet and Neumann Boundary Conditions. (arXiv:2310.00177v1 [math.NA])

    [http://arxiv.org/abs/2310.00177](http://arxiv.org/abs/2310.00177)

    我们引入了一个用于混合边界条件的泊松方程的神经预处理迭代求解器，核心是一个神经网络，能够近似逆离散结构网格拉普拉斯算子，并且在训练集之外的边界条件下仍然有效。

    

    我们引入了一个神经预处理的迭代求解器，用于具有混合边界条件的泊松方程。泊松方程在科学计算中是普遍存在的：它控制着广泛的物理现象，在许多数值算法中作为子问题出现，并且作为更广泛的椭圆PDE类的模型问题。最流行的泊松离散化方法可以产生大型稀疏线性系统。在高分辨率和对性能至关重要的应用中，迭代求解器结合强大的预处理器可以提供优势。我们求解器的核心是一个神经网络，该网络经过训练可以近似离散结构网格拉普拉斯算子的逆算子，适用于任意形状的域和混合边界条件。我们展示了该问题的结构激发了一种新颖的网络架构，即使在训练集之外的边界条件下，该架构也表现出高效的预处理器。我们展示了在具有挑战性的测试案例上的效果。

    We introduce a neural-preconditioned iterative solver for Poisson equations with mixed boundary conditions. The Poisson equation is ubiquitous in scientific computing: it governs a wide array of physical phenomena, arises as a subproblem in many numerical algorithms, and serves as a model problem for the broader class of elliptic PDEs. The most popular Poisson discretizations yield large sparse linear systems. At high resolution, and for performance-critical applications, iterative solvers can be advantageous for these -- but only when paired with powerful preconditioners. The core of our solver is a neural network trained to approximate the inverse of a discrete structured-grid Laplace operator for a domain of arbitrary shape and with mixed boundary conditions. The structure of this problem motivates a novel network architecture that we demonstrate is highly effective as a preconditioner even for boundary conditions outside the training set. We show that on challenging test cases aris
    
[^116]: FeCAM：在免去样本的连续学习中利用类别分布的异质性

    FeCAM: Exploiting the Heterogeneity of Class Distributions in Exemplar-Free Continual Learning. (arXiv:2309.14062v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2309.14062](http://arxiv.org/abs/2309.14062)

    本文针对免去样本的增量式学习（CIL）中的异质性类别分布问题，使用原型网络和改进的各向异性马哈拉诺比斯距离进行特征分类和建模，有效解决了非恒定数据学习中的特征分布异质性挑战。

    

    免去样本的增量式学习（CIL）面临着许多挑战，因为它禁止了来自先前任务的数据回顾，从而导致了灾难性遗忘。最近的增量学习方法通过在第一个任务之后冻结特征提取器来学习分类器，已经引起了广泛关注。在本文中，我们探索了用于CIL的原型网络，该网络使用冻结的特征提取器生成新的类别原型，并根据到原型的欧氏距离对特征进行分类。通过对类别特征分布进行分析，我们发现基于欧氏度量的分类对于联合训练的特征是成功的。然而，当从非恒定数据中学习时，我们观察到欧氏度量是次优的，并且特征分布是异质的。为了解决这个挑战，我们重新审视了用于CIL的各向异性马哈拉诺比斯距离。此外，我们通过实验证明了建模特征协方差关系的重要性。

    Exemplar-free class-incremental learning (CIL) poses several challenges since it prohibits the rehearsal of data from previous tasks and thus suffers from catastrophic forgetting. Recent approaches to incrementally learning the classifier by freezing the feature extractor after the first task have gained much attention. In this paper, we explore prototypical networks for CIL, which generate new class prototypes using the frozen feature extractor and classify the features based on the Euclidean distance to the prototypes. In an analysis of the feature distributions of classes, we show that classification based on Euclidean metrics is successful for jointly trained features. However, when learning from non-stationary data, we observe that the Euclidean metric is suboptimal and that feature distributions are heterogeneous. To address this challenge, we revisit the anisotropic Mahalanobis distance for CIL. In addition, we empirically show that modeling the feature covariance relations is b
    
[^117]: TIDE: 用于评估和增强分类和语言模型的文本身份检测

    TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])

    [http://arxiv.org/abs/2309.04027](http://arxiv.org/abs/2309.04027)

    本文介绍了TIDE（Textual Identity Detection）方法来改善分类器和语言模型中的文本公平性。通过创建一个包含身份词汇和语境的数据集，以及开发一个身份注释和增强工具，可以提高机器学习公平性技术的效果。

    

    机器学习模型可以继承不公正和不平衡数据集中的意外偏见。在文本数据集中，评估和去偏这些数据集和模型尤其困难，因为种族、性别和性取向等敏感属性可能不可用。当这些模型投放到社会中时，它们可能对历史上弱势群体产生不公平的结果。本文提出了一个与方法相结合的数据集，以改善分类器和语言模型中的文本公平性。我们创建了一个更全面的身份词汇表TIDAL，包括15,123个身份术语和相关的语境，涵盖了三个人口统计类别。我们利用TIDAL开发了一个身份注释和增强工具，可以用于改善身份语境的可用性和机器学习公平性技术的效果。我们使用人类贡献者对我们的方法进行了评估，并进行了重点关注数据集和模型去偏的实验。

    Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Resul
    
[^118]: 纯探索下的中介反馈

    Pure Exploration under Mediators' Feedback. (arXiv:2308.15552v1 [cs.LG])

    [http://arxiv.org/abs/2308.15552](http://arxiv.org/abs/2308.15552)

    本研究提出了一种严格推广的传统最优臂识别问题，即中介反馈下的最优臂识别（BAI-MF），通过引入中介者来模拟一些实际决策问题，如离线学习、部分可控环境和人类反馈。

    

    随机多臂赌博机是一种顺序决策框架，每一步交互中学习者选择一个臂并观察一个随机回报。在最优臂识别（BAI）问题的背景下，学习者的目标是尽可能准确和高效地找到最优臂，即具有最高期望回报的臂。然而，传统BAI问题的顺序交互协议，即学习者在每一轮中对选择的臂具有完全控制权，无法有效地模拟一些值得关注的决策问题（例如，离线学习，部分可控环境和人类反馈）。因此，在这项工作中，我们提出了一种新的严格推广的传统BAI问题，称之为中介反馈下的最优臂识别（BAI-MF）。更具体地说，我们考虑了学习者可以访问一组中介者的情况，每个中介者都选择要拉动的臂。

    Stochastic multi-armed bandits are a sequential-decision-making framework, where, at each interaction step, the learner selects an arm and observes a stochastic reward. Within the context of best-arm identification (BAI) problems, the goal of the agent lies in finding the optimal arm, i.e., the one with highest expected reward, as accurately and efficiently as possible. Nevertheless, the sequential interaction protocol of classical BAI problems, where the agent has complete control over the arm being pulled at each round, does not effectively model several decision-making problems of interest (e.g., off-policy learning, partially controllable environments, and human feedback). For this reason, in this work, we propose a novel strict generalization of the classical BAI problem that we refer to as best-arm identification under mediators' feedback (BAI-MF). More specifically, we consider the scenario in which the learner has access to a set of mediators, each of which selects the arms on 
    
[^119]: SE(3)等变增强耦合流

    SE(3) Equivariant Augmented Coupling Flows. (arXiv:2308.10364v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10364](http://arxiv.org/abs/2308.10364)

    本文提出了一种SE(3)等变增强耦合流，可以快速采样和密度评估，通过在坐标分割中保持等变性。

    

    耦合标准化流能够快速采样和密度评估，使其成为物理系统概率建模的首选工具。然而，标准耦合架构无法赋予操作原子笛卡尔坐标的流SE(3)和置换不变性。本文提出了一种通过沿附加增强维度进行坐标分割的耦合流，以保持SE(3)和置换等变性。在每一层中，流将原子的位置映射到学习得到的SE(3)不变基上，我们在返回到原始基之前应用标准流变换，如单调分子有理二次样条。关键是，我们的流保持了快速采样和密度评估，并且可以通过重要性采样产生对目标分布的期望的无偏估计。在DW4、LJ13和QM9位置数据集上训练时，我们的流与等变流有竞争力。

    Coupling normalizing flows allow for fast sampling and density evaluation, making them the tool of choice for probabilistic modeling of physical systems. However, the standard coupling architecture precludes endowing flows that operate on the Cartesian coordinates of atoms with the SE(3) and permutation invariances of physical systems. This work proposes a coupling flow that preserves SE(3) and permutation equivariance by performing coordinate splits along additional augmented dimensions. At each layer, the flow maps atoms' positions into learned SE(3) invariant bases, where we apply standard flow transformations, such as monotonic rational-quadratic splines, before returning to the original basis. Crucially, our flow preserves fast sampling and density evaluation, and may be used to produce unbiased estimates of expectations with respect to the target distribution via importance sampling. When trained on the DW4, LJ13, and QM9-positional datasets, our flow is competitive with equivari
    
[^120]: 非线性元学习可以保证更快的收敛速度

    Nonlinear Meta-Learning Can Guarantee Faster Rates. (arXiv:2307.10870v1 [stat.ML])

    [http://arxiv.org/abs/2307.10870](http://arxiv.org/abs/2307.10870)

    非线性元学习可以保证更快的收敛速度。

    

    最近许多关于元学习的理论研究旨在利用相关任务中的相似表示结构来简化目标任务，并实现收敛速率的保证。然而，在实践中，表示往往是高度非线性的，引入了每个任务中不可简单平均的非平凡偏差。本研究通过非线性表示推导出元学习的理论保证。

    Many recent theoretical works on \emph{meta-learning} aim to achieve guarantees in leveraging similar representational structures from related tasks towards simplifying a target task. Importantly, the main aim in theory works on the subject is to understand the extent to which convergence rates -- in learning a common representation -- \emph{may scale with the number $N$ of tasks} (as well as the number of samples per task). First steps in this setting demonstrate this property when both the shared representation amongst tasks, and task-specific regression functions, are linear. This linear setting readily reveals the benefits of aggregating tasks, e.g., via averaging arguments. In practice, however, the representation is often highly nonlinear, introducing nontrivial biases in each task that cannot easily be averaged out as in the linear case. In the present work, we derive theoretical guarantees for meta-learning with nonlinear representations. In particular, assuming the shared nonl
    
[^121]: 自洽性方法用于无限生成问题的改进

    Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])

    [http://arxiv.org/abs/2307.06857](http://arxiv.org/abs/2307.06857)

    本论文提出了一种改进大规模预训练语言模型生成输出质量和一致性的新方法，通过扩展自洽性框架的适用性，实现了从一个候选集中恢复最优或接近最优的生成结果，并提出了一种轻量级无参数相似性函数来改进代码生成、自动形式化和摘要任务的效果。

    

    在这篇论文中，我们提出了一种改进大规模预训练语言模型生成输出的质量和一致性的新方法。自洽性已经被证明是一种有效的方法，对于具有固定答案的提示，选择得票最多的答案。我们引入了一个推广的自洽性框架，扩展了其适用性，超越了固定答案问题的范围。通过大量的模拟实验，我们证明了我们的方法能够从候选集中恢复最优或接近最优的生成结果。我们还提出了一种轻量级无参数相似性函数，即使没有访问到标记的概率，也能在代码生成、自动形式化和摘要任务中显著和一致地改进效果。我们的方法几乎没有计算开销，不需要额外的再排序模型或对现有模型的修改。

    In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
    
[^122]: 基于扩散的多智能体对抗追踪

    Diffusion Based Multi-Agent Adversarial Tracking. (arXiv:2307.06244v1 [cs.RO])

    [http://arxiv.org/abs/2307.06244](http://arxiv.org/abs/2307.06244)

    本文介绍了CADENCE，一种基于扩散的多智能体对抗追踪方法，通过利用过去的稀疏状态信息生成全面的对手位置预测，并通过蒙特卡洛采样评估了其有效性。

    

    目标追踪在现实场景中起着关键作用，特别是在打击毒品走私行动中，对抗性目标的位置信息往往是有限的。改进自主追踪系统将使无人机、水面舰艇和水下器械能够更好地协助打击使用人工水面船只、半潜艇和航空器的走私犯。随着无人机的普及，准确的自主目标估计对安全和保障更为重要。本文提出了一种名为CADENCE的约束基于智能体的扩散增强多智能体追踪方法，旨在通过利用过去的稀疏状态信息生成对手位置的全面预测。为了评估这种方法的有效性，我们对单目标和多目标追踪环境进行了预测，利用扩散模型的蒙特卡洛采样来估计每个生成轨迹的概率。

    Target tracking plays a crucial role in real-world scenarios, particularly in drug-trafficking interdiction, where the knowledge of an adversarial target's location is often limited. Improving autonomous tracking systems will enable unmanned aerial, surface, and underwater vehicles to better assist in interdicting smugglers that use manned surface, semi-submersible, and aerial vessels. As unmanned drones proliferate, accurate autonomous target estimation is even more crucial for security and safety. This paper presents Constrained Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach aimed at generating comprehensive predictions of adversary locations by leveraging past sparse state information. To assess the effectiveness of this approach, we evaluate predictions on single-target and multi-target pursuit environments, employing Monte-Carlo sampling of the diffusion model to estimate the probability associated with each generated trajectory. We propose a novel 
    
[^123]: 通过主动遗忘在预训练中提高语言可塑性

    Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2307.01163](http://arxiv.org/abs/2307.01163)

    本论文提出了一种通过在预训练过程中使用主动遗忘机制来提高语言模型的可塑性的方法。通过在预训练过程中定期重置嵌入层，模型可以更快地适应新的语言，并在低数据情况下表现出更好的性能。

    

    预训练语言模型(PLMs)是自然语言处理中的主要模型。尽管它们在下游任务的性能令人印象深刻，但将PLMs应用于新语言可能很困难，这是使它们的能力普遍可访问的壁垒。先前的研究表明，通过为新语言学习新的嵌入层可以解决此问题，但这样做既浪费数据又浪费计算资源。我们建议在预训练期间使用主动遗忘机制，作为快速适应新语言的PLMs的简单方法。具体而言，通过在预训练期间的每K次更新时重置嵌入层，我们鼓励PLM在有限次更新内提高学习新嵌入的能力，类似于元学习的效果。使用RoBERTa进行的实验证明，使用我们的遗忘机制预训练的模型不仅在语言适应过程中显示出更快的收敛速度，而且在低数据的情况下也优于标准模型。

    Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
    
[^124]: milliFlow：用于人体运动感知的毫米波雷达点云场景流估计

    milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])

    [http://arxiv.org/abs/2306.17010](http://arxiv.org/abs/2306.17010)

    milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。

    

    随着普适计算时代的到来，人体运动感知在智能系统中起着关键作用，用于决策、用户交互和个性化服务。在传统方法中，人体跟踪、姿势估计、手势识别和活动识别等方面进行了大量研究，这些方法主要基于摄像机。然而，摄像机的侵入性特点限制了它们在智能家居应用中的使用。为了解决这个问题，毫米波雷达由于其保护隐私的特点而受到欢迎。在这项工作中，我们提出了一种新颖的深度学习方法milliFlow，用于对毫米波雷达点云进行场景流估计，作为中间层的特征，直接受益于下游的人体运动感知任务。实验结果表明，我们的方法具有优越的性能，平均3D端点误差为4.6cm，明显超过竞争方法。此外，通过结合...

    Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
    
[^125]: 异步算法与Cocycles的对齐

    Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v1 [cs.LG])

    [http://arxiv.org/abs/2306.15632](http://arxiv.org/abs/2306.15632)

    该论文提出了一种将节点状态更新和消息函数调用分离的数学框架，以实现异步计算，并以此作为基础，进行了异步算法和神经网络的对齐。

    

    最先进的神经算法推理器使用图神经网络（GNN）中的消息传递。但是，典型的GNN在定义和调用消息函数之间模糊了区别，迫使节点在每一层都向其邻居发送消息，同步地进行。然而，当将GNN应用于学习执行动态规划算法时，大多数步骤只有少数几个节点会有有意义的更新要发送。因此，通过在图中发送太多无关的数据，可能导致低效率，而许多中间的GNN步骤必须学习身份函数。在这项工作中，我们明确地分离了节点状态更新和消息函数调用的概念。通过这种分离，我们得到了一个数学表达，可以让我们思考算法和神经网络中的异步计算。

    State-of-the-art neural algorithmic reasoners make use of message passing in graph neural networks (GNNs). But typical GNNs blur the distinction between the definition and invocation of the message function, forcing a node to send messages to its neighbours at every layer, synchronously. When applying GNNs to learn to execute dynamic programming algorithms, however, on most steps only a handful of the nodes would have meaningful updates to send. One, hence, runs the risk of inefficiencies by sending too much irrelevant data across the graph -- with many intermediate GNN steps having to learn identity functions. In this work, we explicitly separate the concepts of node state update and message function invocation. With this separation, we obtain a mathematical formulation that allows us to reason about asynchronous computation in both algorithms and neural networks.
    
[^126]: 采用强化学习技术增强变分量子状态对角化

    Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2306.11086](http://arxiv.org/abs/2306.11086)

    本研究采用强化学习技术，通过新的编码方法来优化量子状态对角化所需的电路深度，从而提高其在近期量子硬件上的应用性能。

    

    变分量子算法的发展对于 NISQ 计算机的应用至关重要。这种算法需要短的量子电路，这种电路更易于在近期硬件上实现，也已经开发出了许多这样的方法。其中一个特别有趣的算法是所谓的变分对角化方法，它是一种重要的算法子例程，可以直接用于处理以量子状态编码的数据。特别地，它可以用于分辨量子态的特征，例如系统的纠缠性质，或者在量子机器学习算法中使用。在本研究中，我们利用强化学习解决在量子状态对角化任务中所需电路非常浅的设计问题。为了实现这一点，我们利用一种新的编码方法，可以利用强化学习方法解决电路深度优化问题。我们证明，我们的方法有可能显著减少所需的电路深度，从而使其更适用于近期量子硬件。

    The development of variational quantum algorithms is crucial for the application of NISQ computers. Such algorithms require short quantum circuits, which are more amenable to implementation on near-term hardware, and many such methods have been developed. One of particular interest is the so-called the variational diagonalization method, which constitutes an important algorithmic subroutine, and it can be used directly for working with data encoded in quantum states. In particular, it can be applied to discern the features of quantum states, such as entanglement properties of a system, or in quantum machine learning algorithms. In this work, we tackle the problem of designing a very shallow quantum circuit, required in the quantum state diagonalization task, by utilizing reinforcement learning. To achieve this, we utilize a novel encoding method that can be used to tackle the problem of circuit depth optimization using a reinforcement learning approach. We demonstrate that our approach
    
[^127]: LabelBench：基于综合框架的标签高效学习基准评估

    LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning. (arXiv:2306.09910v1 [cs.LG])

    [http://arxiv.org/abs/2306.09910](http://arxiv.org/abs/2306.09910)

    本论文介绍了一个新的综合性标签高效学习基准评估框架LabelBench，并通过引入一种新的与半监督学习相结合的主动学习方法的基准测试，证明了在相对较少的标记示例下实现更好的标签效率。

    

    标记数据是现代机器学习应用程序的关键，但获取标记可能很昂贵。为了减缓这一成本，机器学习方法（如迁移学习、半监督学习和主动学习）旨在实现标签高效性：从相对较少的标记示例中实现高预测性能。虽然在实践中获得最佳的标签效率通常需要这些技术的组合，但现有的基准评估框架并没有捕捉到所有这些技术的协同组合。本文通过引入LabelBench解决了这个缺陷，这是一个新的计算效率高的综合性框架，用于联合评估多个标签高效学习技术。作为LabelBench的一个应用，我们引入了一种与半监督学习一起使用的最新主动学习方法的新基准，用于微调预训练的视觉转换器。我们的基准证明了比先前报告的更好的标签效率。

    Labeled data are critical to modern machine learning applications, but obtaining labels can be expensive. To mitigate this cost, machine learning methods, such as transfer learning, semi-supervised learning and active learning, aim to be label-efficient: achieving high predictive performance from relatively few labeled examples. While obtaining the best label-efficiency in practice often requires combinations of these techniques, existing benchmark and evaluation frameworks do not capture a concerted combination of all such techniques. This paper addresses this deficiency by introducing LabelBench, a new computationally-efficient framework for joint evaluation of multiple label-efficient learning techniques. As an application of LabelBench, we introduce a novel benchmark of state-of-the-art active learning methods in combination with semi-supervised learning for fine-tuning pretrained vision transformers. Our benchmark demonstrates better label-efficiencies than previously reported in 
    
[^128]: 通信系统中AI通用性与可扩展性的设计原则

    Design Principles for Generalization and Scalability of AI in Communication Systems. (arXiv:2306.06251v1 [cs.LG])

    [http://arxiv.org/abs/2306.06251](http://arxiv.org/abs/2306.06251)

    本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建具备通用性的AI算法，通过少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。

    

    人工智能（AI）已经成为通信系统中解决复杂和动态任务的强大工具，传统的基于规则的算法往往无法胜任。然而，大多数网络任务的AI应用都是针对特定和有限的条件设计和训练的，使得算法无法适应于常见的网络环境、任务和控制任务。本文提出了可持续和可扩展的AI集成通信系统的设计原则，侧重于创建可以在网络环境、意图和控制任务上具备通用性的AI算法。这种方法可以使少量的AI驱动的RAN函数来解决更大的问题，提高系统性能，并简化生命周期管理。为了实现可持续性和自动化，我们引入了一种可扩展的学习架构，该架构支持系统中部署的所有AI应用程序。该架构将中央化学习功能与分布式学习和数据采集功能分离，确保了系统的可扩展性和稳健性。

    Artificial intelligence (AI) has emerged as a powerful tool for addressing complex and dynamic tasks in communication systems, where traditional rule-based algorithms often struggle. However, most AI applications to networking tasks are designed and trained for specific, limited conditions, hindering the algorithms from learning and adapting to generic situations, such as those met across radio access networks (RAN). This paper proposes design principles for sustainable and scalable AI integration in communication systems, focusing on creating AI algorithms that can generalize across network environments, intents, and control tasks. This approach enables a limited number of AI-driven RAN functions to tackle larger problems, improve system performance, and simplify lifecycle management. To achieve sustainability and automation, we introduce a scalable learning architecture that supports all deployed AI applications in the system. This architecture separates centralized learning function
    
[^129]: 利用新兴协方差进行概率计算：走向高效的不确定性量化

    Probabilistic Computation with Emerging Covariance: Towards Efficient Uncertainty Quantification. (arXiv:2305.19265v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19265](http://arxiv.org/abs/2305.19265)

    本文开发了一个高效、可解释的概率计算框架，通过监督平均值优化任务目标，从非线性耦合中自发出现的无监督协方差忠实地捕捉了与模型预测的不确定性相关的信息。

    

    建立鲁棒性、可解释性和安全性强的人工智能系统需要通过概率视角量化和表示不确定性，因为这可以模仿人类的认知能力。然而，概率计算由于其固有的复杂性而面临重大挑战。本文通过截断概率表示的前两个矩，即平均值和协方差，开发了一个高效、可解释的概率计算框架。我们通过训练随机网络的确定性替代品来实例化该框架，该网络通过简单激活的组合学习复杂的概率表示，封装了平均值和协方差的非线性耦合。我们表明，当平均值受到监督以优化任务目标时，从其与协方差的非线性耦合中自发出现的无监督协方差忠实地捕捉了与模型预测的不确定性相关的信息。

    Building robust, interpretable, and secure artificial intelligence system requires some degree of quantifying and representing uncertainty via a probabilistic perspective, as it allows to mimic human cognitive abilities. However, probabilistic computation presents significant challenges due to its inherent complexity. In this paper, we develop an efficient and interpretable probabilistic computation framework by truncating the probabilistic representation up to its first two moments, i.e., mean and covariance. We instantiate the framework by training a deterministic surrogate of a stochastic network that learns the complex probabilistic representation via combinations of simple activations, encapsulating the non-linearities coupling of the mean and covariance. We show that when the mean is supervised for optimizing the task objective, the unsupervised covariance spontaneously emerging from the non-linear coupling with the mean faithfully captures the uncertainty associated with model p
    
[^130]: 一种统一的方法用于最大化连续 DR-submodular 函数

    A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v1 [cs.LG])

    [http://arxiv.org/abs/2305.16671](http://arxiv.org/abs/2305.16671)

    本文提出了一种适用于一系列设置和 Oracle 访问类型的统一方法，用于最大化连续 DR-submodular 函数，为 16 种情况中的 9 种提供了新的/改进的结果，并且针对基于随机函数值的 Oracle 取得了第一个适用于随机 DR-submodular 函数的后悔界限。

    

    本文提出了一种统一的方法，用于最大化连续的 DR-submodular 函数，涵盖了一系列设置和 Oracle 访问类型。我们的方法包括针对单调和非单调函数的 Frank-Wolfe 类型离线算法，具有不同的一般凸集限制。我们考虑了 Oracle 提供函数梯度或仅函数值的访问以及确定性或随机性访问的设置。我们在所有情况下确定了所需的 Oracle 访问数量。我们的方法为 16 个考虑的情况中的 9 个提供了新的/改进的结果，在两个情况下避免了计算上昂贵的投影，而所提出的框架在其余五个情况下与最先进的方法相匹配。值得注意的是，我们针对基于随机函数值的 Oracle 的方法，为随机 DR-submodular 函数提供了第一个带有探险反馈的后悔界限。

    This paper presents a unified approach for maximizing continuous DR-submodular functions that encompasses a range of settings and oracle access types. Our approach includes a Frank-Wolfe type offline algorithm for both monotone and non-monotone functions, with different restrictions on the general convex set. We consider settings where the oracle provides access to either the gradient of the function or only the function value, and where the oracle access is either deterministic or stochastic. We determine the number of required oracle accesses in all cases. Our approach gives new/improved results for nine out of the sixteen considered cases, avoids computationally expensive projections in two cases, with the proposed framework matching performance of state-of-the-art approaches in the remaining five cases. Notably, our approach for the stochastic function value-based oracle enables the first regret bounds with bandit feedback for stochastic DR-submodular functions.
    
[^131]: 用有效的视野连接强化学习理论和实践

    Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])

    [http://arxiv.org/abs/2304.09853](http://arxiv.org/abs/2304.09853)

    本论文通过对常见深度强化学习测试基准中155个MDP的数据集进行分析，发现当最高Q值的动作在随机策略下Q值最高时，深度强化学习往往会成功；反之，则失败的可能性较高。

    

    深度强化学习在某些环境中表现出色，但在其他环境中却失败得非常严重。理想情况下，强化学习理论应该能够解释这种现象，提供预测实际性能的界限。不幸的是，当前的理论还没有这种能力。本文通过引入包含155个MDP的新数据集BRIDGE，将标准的深度强化学习算法与之前的样本复杂度先前界进行比较，并发现了一个意想不到的性质：当最高Q值的动作在随机策略下的Q值也是最高的时，深度强化学习往往会成功；反之，失败的可能性较高。基于这一性质，我们将其概括为一个新的MDP复杂度度量，称为有效的视野。

    Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
    
[^132]: 稀疏深度神经网络中梯度下降的点对点收敛定理

    Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.08172](http://arxiv.org/abs/2304.08172)

    本文研究了稀疏深度神经网络中梯度下降的点对点收敛定理，针对非光滑指示函数构造了一种特殊形状的DNN，实现了梯度下降过程的点对点收敛。

    

    深度神经网络（DNN）的理论结构逐渐得到了阐明。Imaizumi-Fukumizu（2019）和Suzuki（2019）指出，当目标函数为非光滑函数时，DNN的学习能力优于先前的理论。然而，据作者所知，迄今为止的众多研究尝试在没有任何统计论证的情况下进行数学研究，探究真正能够引发梯度下降的DNN架构的点对点收敛性，这一尝试似乎更贴近实际DNN。本文将目标函数限制为非光滑指示函数，并在ReLU-DNN中构造了一个稀疏且具有特殊形状的DNN，从而实现了梯度下降过程中的点对点收敛。

    The theoretical structure of deep neural network (DNN) has been clarified gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the learning ability of DNN is superior to the previous theories when the target function is non-smooth functions. However, as far as the author is aware, none of the numerous works to date attempted to mathematically investigate what kind of DNN architectures really induce pointwise convergence of gradient descent (without any statistical argument), and this attempt seems to be closer to the practical DNNs. In this paper we restrict target functions to non-smooth indicator functions, and construct a deep neural network inducing pointwise convergence provided by gradient descent process in ReLU-DNN. The DNN has a sparse and a special shape, with certain variable transformations.
    
[^133]: 自然语言到SPARQL查询生成的复制机制综合评估

    A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])

    [http://arxiv.org/abs/2304.07772](http://arxiv.org/abs/2304.07772)

    本研究综合评估自然语言到SPARQL查询生成中的复制机制，通过大量实验研究预训练和非预训练模型、问题注释格式以及使用复制机制的影响，并证明了在这些方面的优化可以提高性能。

    

    近年来，神经机器翻译（NMT）领域在SPARQL查询生成方面有了显著的增长。最近，将复制机制与传统的编码器-解码器架构相结合，并使用预训练的编码器-解码器，创造了新的性能基准。本文展示了大量的实验，复制并扩展了最近的基于NMT的SPARQL生成研究，比较了预训练和非预训练模型、问题注释格式以及对于非预训练和预训练模型使用复制机制的影响。我们的结果表明，对于非预训练模型和预训练模型，添加复制机制或使用问题注释都可以提高性能，并为三个流行数据集设置了新的基准。

    In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
    
[^134]: 用相关静态分析产品改善少样本提示

    Improving Few-Shot Prompts with Relevant Static Analysis Products. (arXiv:2304.06815v1 [cs.SE])

    [http://arxiv.org/abs/2304.06815](http://arxiv.org/abs/2304.06815)

    本文研究了用相关静态分析产品改善大型语言模型在少样本提示中的表现，探讨如何通过添加显示信息来提取代码中的语义事实。

    

    大型语言模型是一类新型计算引擎，通过提示工程实现"编程"。我们仍在学习如何最好地"编程"这些大型语言模型以帮助开发人员。我们的研究从这样一种直觉出发，即开发人员在处理编码任务时会有一系列意识和无意识的语义事实。对于一个函数而言，这些语义事实可能包括参数和局部变量名称、返回表达式、简单的前置和后置条件以及基本的控制和数据流，等等。我们的目标是调查这个问题，使用代码摘要任务并评估是否使用显式添加信息能够帮助大型语言模型提取这些语义事实。

    Large Language Models (LLM) are a new class of computation engines, "programmed" via prompt engineering. We are still learning how to best "program" these LLMs to help developers. We start with the intuition that developers tend to consciously and unconsciously have a collection of semantics facts in mind when working on coding tasks. Mostly these are shallow, simple facts arising from a quick read. For a function, examples of facts might include parameter and local variable names, return expressions, simple pre- and post-conditions, and basic control and data flow, etc.  One might assume that the powerful multi-layer architecture of transformer-style LLMs makes them inherently capable of doing this simple level of "code analysis" and extracting such information, implicitly, while processing code: but are they, really? If they aren't, could explicitly adding this information help? Our goal here is to investigate this question, using the code summarization task and evaluate whether auto
    
[^135]: OKRidge: 用于学习动态系统的可扩展 k 稀疏岭回归

    OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems. (arXiv:2304.06686v1 [cs.LG])

    [http://arxiv.org/abs/2304.06686](http://arxiv.org/abs/2304.06686)

    本文提出了一种名为 OKRidge 的方法，用于确定非线性动态系统的稀疏控制方程，并通过求解稀疏岭回归问题，实现了可扩展性和快速性，和现有方法相比，有着更高的效率。

    

    本文解决了科学发现中的一个重要问题，即，确定非线性动态系统的稀疏控制方程，通过求解稀疏岭回归问题可以证明最优性，以确定驱动基础动态的项。我们提出了一种称为 OKRidge 的快速算法，用一种新颖的下界计算方法，涉及鞍点公式，然后使用线性系统或基于 ADMM 的方法来解决，其中可以通过解决另一个线性系统和单调回归问题来有效地计算近端算子。我们还提出了一种启动我们求解器的方法，利用了波束搜索。在实验中，我们的方法达到可证明的最优性，并且运行时间比商业求解器 Gurobi 解决的现有 MIP公式运行时间快几个数量级。

    We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
    
[^136]: 训练数据重构的非渐进性下界

    Non-Asymptotic Lower Bounds For Training Data Reconstruction. (arXiv:2303.16372v1 [cs.LG])

    [http://arxiv.org/abs/2303.16372](http://arxiv.org/abs/2303.16372)

    本文通过研究差分隐私和度量隐私学习器在对抗者重构错误方面的鲁棒性，得出了非渐进性下界，覆盖了高维情况，且扩展了深度学习算法的隐私分析

    

    本文研究了专业对手进行训练数据重构攻击时私有学习算法的语义保证强度。我们通过导出非渐进量级下界来研究了满足差分隐私（DP）和度量隐私（mDP）的学习器对抗者重构错误的鲁棒性。此外，我们还证明了我们对mDP的分析覆盖了高维情况。本文进一步对流行的深度学习算法，如DP-SGD和Projected Noisy SGD进行了度量差分隐私的扩展隐私分析。

    We investigate semantic guarantees of private learning algorithms for their resilience to training Data Reconstruction Attacks (DRAs) by informed adversaries. To this end, we derive non-asymptotic minimax lower bounds on the adversary's reconstruction error against learners that satisfy differential privacy (DP) and metric differential privacy (mDP). Furthermore, we demonstrate that our lower bound analysis for the latter also covers the high dimensional regime, wherein, the input data dimensionality may be larger than the adversary's query budget. Motivated by the theoretical improvements conferred by metric DP, we extend the privacy analysis of popular deep learning algorithms such as DP-SGD and Projected Noisy SGD to cover the broader notion of metric differential privacy.
    
[^137]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^138]: 使用Score matching的乘积Jacobi-Theta Boltzmann机器

    Product Jacobi-Theta Boltzmann machines with score matching. (arXiv:2303.05910v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05910](http://arxiv.org/abs/2303.05910)

    本文介绍了一种使用Score matching的乘积Jacobi-Theta Boltzmann机器（pJTBM），它比原始的RTBM更高效地拟合概率密度。

    

    估计概率密度函数是一个不容易的任务，在过去几年中，人们已经开始使用机器学习技术来解决这个问题。借鉴Boltzmann机器的架构，可以得到成功的应用。本文介绍了一种称为乘积Jacobi-Theta Boltzmann机器（pJTBM）的模型，它是Riemann-Theta Boltzmann机器（RTBM）的受限版本，具有对角隐藏部分连接矩阵。我们展示了基于Fisher散度的Score matching可以用来比原始的RTBM更高效地拟合pJTBM的概率密度。

    The estimation of probability density functions is a non trivial task that over the last years has been tackled with machine learning techniques. Successful applications can be obtained using models inspired by the Boltzmann machine (BM) architecture. In this manuscript, the product Jacobi-Theta Boltzmann machine (pJTBM) is introduced as a restricted version of the Riemann-Theta Boltzmann machine (RTBM) with diagonal hidden sector connection matrix. We show that score matching, based on the Fisher divergence, can be used to fit probability densities with the pJTBM more efficiently than with the original RTBM.
    
[^139]: 主动推理和强化学习：在部分可观测的连续状态和动作空间下的统一推理

    Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07946](http://arxiv.org/abs/2212.07946)

    本论文提出了一种能够在部分可观测的连续状态和动作空间下进行统一推理的框架，通过最小化期望自由能函数指导代理选择动作，以实现最大化奖励的决策制定。

    

    强化学习在完全可观测环境中开发决策制定代理以最大化由外部监督员指定的奖励引起了极大关注。然而，许多现实世界的问题涉及部分观测，形式化为部分可观测的马尔可夫决策过程（POMDP）。以往的研究通过将过去的行动和观测记忆或通过从观测数据中推断环境的真实状态来解决POMDP中的强化学习问题。然而，在连续空间中随时间聚合观测数据变得不可行。此外，基于推理的强化学习方法通常需要许多样本才能表现良好，因为它们仅关注奖励最大化，忽视了推断状态的不确定性。主动推理（AIF）是在POMDP中制定的一种框架，通过最小化一个称为期望自由能（EFE）的函数指导代理选择动作。这提供了最大化奖励（富有开发性）的行为。

    Reinforcement learning (RL) has garnered significant attention for developing decision-making agents that aim to maximize rewards, specified by an external supervisor, within fully observable environments. However, many real-world problems involve partial observations, formulated as partially observable Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs by either incorporating the memory of past actions and observations or by inferring the true state of the environment from observed data. However, aggregating observed data over time becomes impractical in continuous spaces. Moreover, inference-based RL approaches often require many samples to perform well, as they focus solely on reward maximization and neglect uncertainty in the inferred state. Active inference (AIF) is a framework formulated in POMDPs and directs agents to select actions by minimizing a function called expected free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as
    
[^140]: 用核斯坦离差控制矩

    Controlling Moments with Kernel Stein Discrepancies. (arXiv:2211.05408v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.05408](http://arxiv.org/abs/2211.05408)

    本研究分析了核斯坦离差（KSD）控制性质，发现标准KSD无法控制矩的收敛，提出了可控制矩和弱收敛的下游扩散KSD，并且发展了可以准确描述$q$-Wasserstein收敛的KSD。

    

    核斯坦离差（KSD）用于衡量分布逼近的质量，并且可以在目标密度具有不可计算的归一化常数时计算。显著的应用包括诊断近似MCMC采样器和非归一化统计模型的适配度检验。本文分析了KSD的收敛控制性质。我们首先证明了用于弱收敛控制的标准KSD无法控制矩的收敛。为了解决这个限制，我们提供了一组充分条件，下游扩散KSD可以同时控制矩和弱收敛。作为一个直接的结果，我们发展了对于每个$q>0$，第一组已知可以准确描述$q$-Wasserstein收敛的KSD。

    Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q > 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.
    
[^141]: EC-NAS: 面向神经架构搜索的能耗感知表格基准

    EC-NAS: Energy Consumption Aware Tabular Benchmarks for Neural Architecture Search. (arXiv:2210.06015v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06015](http://arxiv.org/abs/2210.06015)

    提出了一个能耗感知的神经架构搜索表格基准 EC-NAS，该基准通过添加能耗和碳足迹信息，支持设计能效高的深度学习模型，并降低总能耗。

    

    近年来，选择、训练和部署深度学习模型所需的能量消耗不断增加。本文旨在支持设计能效高、训练资源消耗较低、适用于实际边缘/移动计算环境并具有环境可持续性的深度学习模型。我们提出将能效作为神经架构搜索 (NAS) 的一项额外性能指标，并通过添加不同架构的能耗和碳足迹信息，提供更新的表格基准 EC-NAS 以在较低计算成本下评估 NAS 策略。EC-NAS 还包括用于预测能耗的代理模型，并有助于降低总能耗。

    Energy consumption from selecting, training and deploying deep learning models has continued to increase over the past few years. Our goal in this work is to support the design of energy-efficient deep learning models that are easier to train with lower compute resources, practical to deploy in real-world edge/mobile computing settings and environmentally sustainable. Tabular benchmarks for neural architecture search (NAS) allow the evaluation of NAS strategies at lower computational cost by providing pre-computed performance statistics. In this work, we suggest including energy efficiency as an additional performance criterion to NAS and present an updated tabular benchmark by including information on energy consumption and carbon footprint for different architectures. The benchmark called EC-NAS is made available open-source to support energy consumption-aware NAS research. EC-NAS also includes a surrogate model for predicting energy consumption, and helps us reduce the overall energ
    
[^142]: 用于不规则抽样时间序列的概率插值的Tripletformer

    Tripletformer for Probabilistic Interpolation of Irregularly sampled Time Series. (arXiv:2210.02091v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02091](http://arxiv.org/abs/2210.02091)

    提出了一种名为Tripletformer的编码器-解码器架构，用于处理具有缺失值的不规则抽样时间序列的概率插值。Tripletformer通过注意力机制和全连接层的设计，能够更准确和确定地插值不规则时间序列数据。

    

    在许多领域如医疗保健、天文学和气候科学中，观察到具有缺失值的不规则抽样时间序列数据。这种类型的时间序列的插值对于诸如根本原因分析和医学诊断以及平滑不规则或嘈杂的数据等任务至关重要。为了应对这一挑战，我们提出了一种名为"Tripletformer"的新型编码器-解码器架构，用于处理具有缺失值的不规则抽样时间序列的概率插值。该基于注意力机制的模型操作于观测集合上，其中每个元素由时间、通道和值组成。Tripletformer的编码器和解码器采用了注意力层和全连接层的设计，使得模型能够有效处理所提供的集合元素。我们在多个真实和合成数据集上将Tripletformer与一系列基准模型进行了评估，并表明它能够产生更准确和确定的插值结果。结果显示了一种改进的插值性能。

    Irregularly sampled time series data with missing values is observed in many fields like healthcare, astronomy, and climate science. Interpolation of these types of time series is crucial for tasks such as root cause analysis and medical diagnosis, as well as for smoothing out irregular or noisy data. To address this challenge, we present a novel encoder-decoder architecture called "Tripletformer" for probabilistic interpolation of irregularly sampled time series with missing values. This attention-based model operates on sets of observations, where each element is composed of a triple of time, channel, and value. The encoder and decoder of the Tripletformer are designed with attention layers and fully connected layers, enabling the model to effectively process the presented set elements. We evaluate the Tripletformer against a range of baselines on multiple real-world and synthetic datasets and show that it produces more accurate and certain interpolations. Results indicate an improve
    
[^143]: 弱SINDy代理模型的收敛性

    Convergence of weak-SINDy Surrogate Models. (arXiv:2209.15573v3 [math.NA] UPDATED)

    [http://arxiv.org/abs/2209.15573](http://arxiv.org/abs/2209.15573)

    本文通过对弱SINDy方法生成的代理模型进行深入的误差分析，验证了代理动力学的收敛性以及代理模型解与真实解的接近程度。

    

    本文对一种Sparse Identification of Nonlinear Dynamics (SINDy)方法的变种生成的代理模型进行了深入的误差分析。我们首先概述了一些非线性系统识别技术，即SINDy、弱SINDy和占据核方法。在假设动力学是一组基函数的有限线性组合的情况下，这些方法建立了一个矩阵方程来恢复系数。我们阐明了这些技术之间的结构相似性，并为弱SINDy技术建立了投影性质。在概述之后，我们分析了一个简化版本的弱SINDy生成的代理模型的误差。特别是，在给定解的一个组合算子有界性的假设下，我们证明了(i)代理动力学收敛于真实动力学，(ii)代理模型的解与真实解相当接近。

    In this paper, we give an in-depth error analysis for surrogate models generated by a variant of the Sparse Identification of Nonlinear Dynamics (SINDy) method. We start with an overview of a variety of non-linear system identification techniques, namely, SINDy, weak-SINDy, and the occupation kernel method. Under the assumption that the dynamics are a finite linear combination of a set of basis functions, these methods establish a matrix equation to recover coefficients. We illuminate the structural similarities between these techniques and establish a projection property for the weak-SINDy technique. Following the overview, we analyze the error of surrogate models generated by a simplified version of weak-SINDy. In particular, under the assumption of boundedness of a composition operator given by the solution, we show that (i) the surrogate dynamics converges towards the true dynamics and (ii) the solution of the surrogate model is reasonably close to the true solution. Finally, as an
    
[^144]: 分布式数据上的协同因果推断

    Collaborative causal inference on distributed data. (arXiv:2208.07898v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2208.07898](http://arxiv.org/abs/2208.07898)

    提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。通过共享中间表示而不是私有数据，估计倾向分数和处理效应，能够减少随机误差和偏差，相比现有方法有更好的估计结果。

    

    近年来，基于隐私保护的分布式数据因果推断技术的发展引起了广泛关注。为了解决这个问题，我们提出了一种数据协作准实验（DC-QE）方法，可以在保护隐私的前提下对分布式数据进行因果推断。在我们的方法中，首先，本地各方从私有数据中构建降维的中间表示。其次，他们共享中间表示，而不是私有数据，以保护隐私。然后，从共享的中间表示中估计倾向分数。最后，从倾向分数中估计处理效应。我们的方法能够减少随机误差和偏差，而现有方法只能减少处理效应估计中的随机误差。通过在人工数据和实际数据上进行数值实验，我们确认我们的方法可以得到比单独分析更好的估计结果。

    The development of technologies for causal inference with the privacy preservation of distributed data has attracted considerable attention in recent years. To address this issue, we propose a data collaboration quasi-experiment (DC-QE) that enables causal inference from distributed data with privacy preservation. In our method, first, local parties construct dimensionality-reduced intermediate representations from the private data. Second, they share intermediate representations, instead of private data for privacy preservation. Third, propensity scores were estimated from the shared intermediate representations. Finally, the treatment effects were estimated from propensity scores. Our method can reduce both random errors and biases, whereas existing methods can only reduce random errors in the estimation of treatment effects. Through numerical experiments on both artificial and real-world data, we confirmed that our method can lead to better estimation results than individual analyse
    
[^145]: DDPM-CD: 以去噪扩散概率模型作为变化检测的特征提取器

    DDPM-CD: Denoising Diffusion Probabilistic Models as Feature Extractors for Change Detection. (arXiv:2206.11892v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.11892](http://arxiv.org/abs/2206.11892)

    本文提出了一种以去噪扩散概率模型（DDPM）作为特征提取器的新的变化检测方法，可以利用未标记的遥感图像进行预训练，并在下游应用中取得良好的效果。

    

    遥感变化检测对于了解地球表面动态、促进环境变化监测、评估人类影响、预测未来趋势和支持决策制定至关重要。本文提出了一种新的变化检测方法，可以利用现成的、未标记的遥感图像在训练过程中预训练去噪扩散概率模型（DDPM）-一类在图像合成中使用的生成模型。DDPM通过逐渐将训练图像转化为高斯分布来学习训练数据分布，使用马尔可夫链。在推理（即采样）过程中，它们可以从高斯噪声起始，生成一组接近训练分布的多样样本，实现了最先进的图像合成结果。然而，本文的重点不是图像合成，而是将其作为预训练的特征提取器应用于变化检测的下游任务。

    Remote sensing change detection is crucial for understanding the dynamics of our planet's surface, facilitating the monitoring of environmental changes, evaluating human impact, predicting future trends, and supporting decision-making. In this work, we introduce a novel approach for change detection that can leverage off-the-shelf, unlabeled remote sensing images in the training process by pre-training a Denoising Diffusion Probabilistic Model (DDPM) - a class of generative models used in image synthesis. DDPMs learn the training data distribution by gradually converting training images into a Gaussian distribution using a Markov chain. During inference (i.e., sampling), they can generate a diverse set of samples closer to the training distribution, starting from Gaussian noise, achieving state-of-the-art image synthesis results. However, in this work, our focus is not on image synthesis but on utilizing it as a pre-trained feature extractor for the downstream application of change det
    
[^146]: 梯度下降，随机优化和其他故事

    Gradient Descent, Stochastic Optimization, and Other Tales. (arXiv:2205.00832v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.00832](http://arxiv.org/abs/2205.00832)

    本文旨在揭示黑盒优化器和随机优化器背后的神奇，并建立起这些技术的工作原理和原因的坚实基础。

    

    本文旨在揭示黑盒优化器和随机优化器背后的神奇，并建立起这些技术的工作原理和原因的坚实基础。通过推导简单直觉背后的数学策略，本文将这些知识凝结成文字。本教程毫不回避梯度下降和随机优化方法的形式和非形式方面。通过这样做，它希望向读者提供更深入的理解，以及何时、如何和为什么应用这些算法。梯度下降是执行优化任务最流行的算法之一，也是优化机器学习任务最常见的方法。最近几年，其随机版本备受关注，特别适用于优化深度神经网络。在深度神经网络中，使用单个样本或一批样本的梯度来节省计算资源。

    The goal of this paper is to debunk and dispel the magic behind black-box optimizers and stochastic optimizers. It aims to build a solid foundation on how and why the techniques work. This manuscript crystallizes this knowledge by deriving from simple intuitions, the mathematics behind the strategies. This tutorial doesn't shy away from addressing both the formal and informal aspects of gradient descent and stochastic optimization methods. By doing so, it hopes to provide readers with a deeper understanding of these techniques as well as the when, the how and the why of applying these algorithms.  Gradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize machine learning tasks. Its stochastic version receives attention in recent years, and this is particularly true for optimizing deep neural networks. In deep neural networks, the gradient followed by a single sample or a batch of samples is employed to save computational r
    
[^147]: 通过自组织操作神经网络实现Holter心电图的鲁棒峰值检测

    Robust Peak Detection for Holter ECGs by Self-Organized Operational Neural Networks. (arXiv:2110.02381v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2110.02381](http://arxiv.org/abs/2110.02381)

    本研究提出了一维自组织操作神经网络(Self-ONNs)，通过优化网络配置和使用非线性神经元模型，提高Holter心电图的峰值检测性能，并实现计算效率。

    

    尽管文献中提出了大量的R峰检测器，但其在低质量和噪声干扰较大的移动心电图（ECG）传感器（如Holter监测仪）所采集的信号中的鲁棒性和性能水平可能会明显下降。最近，深度一维卷积神经网络（CNN）已经解决了这个问题，在Holter监测仪中取得了最先进的性能水平；然而，它们的复杂性较高，需要特殊并行化硬件设置进行实时处理。另一方面，它们的性能在使用紧凑网络配置时会下降。这是预期的结果，因为最近的研究表明，CNN的学习性能受到限制，因为它们具有严格的同质配置和单一线性神经元模型。在本研究中，为了进一步提高峰值检测性能并实现优雅的计算效率，我们提出了一维自组织操作神经网络(Self-ONNs)。

    Although numerous R-peak detectors have been proposed in the literature, their robustness and performance levels may significantly deteriorate in low-quality and noisy signals acquired from mobile electrocardiogram (ECG) sensors, such as Holter monitors. Recently, this issue has been addressed by deep 1-D convolutional neural networks (CNNs) that have achieved state-of-the-art performance levels in Holter monitors; however, they pose a high complexity level that requires special parallelized hardware setup for real-time processing. On the other hand, their performance deteriorates when a compact network configuration is used instead. This is an expected outcome as recent studies have demonstrated that the learning performance of CNNs is limited due to their strictly homogenous configuration with the sole linear neuron model. In this study, to further boost the peak detection performance along with an elegant computational efficiency, we propose 1-D Self-Organized ONNs (Self-ONNs) with 
    
[^148]: NAAQA: 一种用于声学问答的神经网络结构

    NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2106.06147](http://arxiv.org/abs/2106.06147)

    本文提出了一种名为NAAQA的神经网络结构，用于声学问答任务。通过使用1D卷积处理声学内容的2D频谱时域表示，该结构通过时间坐标图增加了时间定位能力，并在处理具有不同基本声音构建的场景时表现出有希望的结果。

    

    声学问答（AQA）任务的目标是回答关于声学场景内容的自由文本问题。本文基于之前介绍的CLEAR数据集，提出了一个新的AQA基准，即CLEAR2，它强调了声学输入的特定挑战，包括处理时长变化的场景和在训练集与测试集之间有不同的基本声音构建的场景。我们还介绍了一种名为NAAQA的神经网络结构，它利用了声学输入的特定属性。在时间和频率上使用1D卷积来处理声学内容的2D频谱时域表示，显示出有希望的结果，并能减少模型复杂性。我们展示了时间坐标图可以增加时间定位能力，从而将网络性能提高约17个百分点。另一方面，频率坐标图对网络性能几乎没有影响。

    The goal of the Acoustic Question Answering (AQA) task is to answer a free-form text question about the content of an acoustic scene. It was inspired by the Visual Question Answering (VQA) task. In this paper, based on the previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely CLEAR2, that emphasizes the specific challenges of acoustic inputs. These include handling of variable duration scenes, and scenes built with elementary sounds that differ between training and test set. We also introduce NAAQA, a neural architecture that leverages specific properties of acoustic inputs. The use of 1D convolutions in time and frequency to process 2D spectro-temporal representations of acoustic content shows promising results and enables reductions in model complexity. We show that time coordinate maps augment temporal localization capabilities which enhance performance of the network by ~17 percentage points. On the other hand, frequency coordinate maps have little influen
    
[^149]: 岭函数估计中良性过拟合现象的有限样本分析

    A finite sample analysis of the benign overfitting phenomenon for ridge function estimation. (arXiv:2007.12882v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2007.12882](http://arxiv.org/abs/2007.12882)

    本论文研究了岭函数估计中的良性过拟合现象，并在有限维度情况下对线性模型进行了分析。

    

    最近在大规模机器学习中进行的广泛数值实验揭示了一个相当反直觉的相变现象，即样本大小与模型参数数量之比的函数关系。当参数数量$p$接近样本大小$n$时，泛化误差增加，但令人惊讶的是，当$p>n$时它再次开始减小。这一现象在\cite{belkin2019reconciling}中引起了理论界的关注，最近已经进行了深入研究，特别是针对比深度神经网络更简单的模型，例如线性模型中参数取最小范数解的情况。首先在当$p$和$n$趋于无穷大的渐近情况下进行研究（参见\cite{hastie2019surprises}），然后在有限维情况下更具体地针对线性模型进行研究（参见\cite{bartlett2020benign}，\cite{tsigler2020benign}，\cite{lecue2022geometrical}）。

    Recent extensive numerical experiments in high scale machine learning have allowed to uncover a quite counterintuitive phase transition, as a function of the ratio between the sample size and the number of parameters in the model. As the number of parameters $p$ approaches the sample size $n$, the generalisation error increases, but surprisingly, it starts decreasing again past the threshold $p=n$. This phenomenon, brought to the theoretical community attention in \cite{belkin2019reconciling}, has been thoroughly investigated lately, more specifically for simpler models than deep neural networks, such as the linear model when the parameter is taken to be the minimum norm solution to the least-squares problem, firstly in the asymptotic regime when $p$ and $n$ tend to infinity, see e.g. \cite{hastie2019surprises}, and recently in the finite dimensional regime and more specifically for linear models \cite{bartlett2020benign}, \cite{tsigler2020benign}, \cite{lecue2022geometrical}. In the p
    
[^150]: 多人赌博学习，从竞争到合作

    Multiplayer Bandit Learning, from Competition to Cooperation. (arXiv:1908.01135v3 [cs.GT] UPDATED)

    [http://arxiv.org/abs/1908.01135](http://arxiv.org/abs/1908.01135)

    这篇论文研究了多人赌博学习中竞争和合作对探索和利用权衡的影响，模型考虑了不同合作参数下玩家的效用函数，并使用Gittins指数简化了单人问题。

    

    随机多臂赌博模型捕捉到了探索和利用的权衡。我们研究了竞争和合作对这种权衡的影响。假设有k个臂和两名玩家，Alice和Bob。在每一轮中，每个玩家拉动一个臂，接收到相应的奖励，并观察到对方的选择但不知道他们的奖励。Alice的效用函数为$\Gamma_A + \lambda \Gamma_B$（Bob的效用函数类似），其中$\Gamma_A$是Alice的总奖励，$\lambda \in [-1, 1]$是合作参数。当$\lambda = -1$时，玩家在一个零和游戏中竞争；当$\lambda = 1$时，他们完全合作；当$\lambda = 0$时，他们是中立的：每个玩家的效用函数为他们自己的奖励。该模型与战略实验经济学文献中关于观察对方奖励的研究相关。使用折扣因子$\beta$，Gittins指数将单人问题简化为对一个带有先验$\mu$的有风险臂的比较。

    The stochastic multi-armed bandit model captures the tradeoff between exploration and exploitation. We study the effects of competition and cooperation on this tradeoff. Suppose there are $k$ arms and two players, Alice and Bob. In every round, each player pulls an arm, receives the resulting reward, and observes the choice of the other player but not their reward. Alice's utility is $\Gamma_A + \lambda \Gamma_B$ (and similarly for Bob), where $\Gamma_A$ is Alice's total reward and $\lambda \in [-1, 1]$ is a cooperation parameter. At $\lambda = -1$ the players are competing in a zero-sum game, at $\lambda = 1$, they are fully cooperating, and at $\lambda = 0$, they are neutral: each player's utility is their own reward. The model is related to the economics literature on strategic experimentation, where usually players observe each other's rewards.  With discount factor $\beta$, the Gittins index reduces the one-player problem to the comparison between a risky arm, with a prior $\mu$, 
    

