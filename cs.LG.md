# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SHARCS: Shared Concept Space for Explainable Multimodal Learning.](http://arxiv.org/abs/2307.00316) | SHARCS是一种解释性多模态学习方法，它通过学习和映射可解释的概念到一个统一的概念空间，实现了解释性的任务预测和改进性能。 |
| [^2] | [Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD.](http://arxiv.org/abs/2307.00310) | 本文开发了一种新的DP-SGD分析方法，可以在训练过程中对许多数据点的隐私泄漏进行更准确的评估。 |
| [^3] | [Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey.](http://arxiv.org/abs/2307.00309) | 这篇综述论文总结了在点云分类中针对对抗攻击和防御技术的当前进展，介绍了对抗攻击原理和对抗样本生成方法，以及防御策略分类和未来的研究方向。 |
| [^4] | [SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation.](http://arxiv.org/abs/2307.00306) | 该论文介绍了一种叫做SyMFM6D的面向对称多视角融合的6D物体姿态估计方法。该方法通过多方向融合网络有效地融合多个视角的RGB-D帧，并通过预测关键点和实例语义分割来计算6D姿态。通过新的训练过程和目标函数，该方法能够解决对称物体的歧义问题，并在单视角和多视角姿态估计方面取得了显著的性能提升。 |
| [^5] | [Applied Bayesian Structural Health Monitoring: inclinometer data anomaly detection and forecasting.](http://arxiv.org/abs/2307.00305) | 本文介绍了将贝叶斯技术应用于测斜仪数据的异常检测和预测，并且展示了如何通过量化和评估不确定性来最小化成本和风险。 |
| [^6] | [Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems.](http://arxiv.org/abs/2307.00296) | 该论文研究了在非光滑最优控制问题中加速原始-对偶方法的两种方法：增大步长和算子学习。研究表明，增大步长的加速原始-对偶方法可以在保证收敛性的同时简单有效地提高计算速度；而算子学习则通过构建神经网络代理模型来加速求解涉及的偏微分方程。 |
| [^7] | [AutoST: Training-free Neural Architecture Search for Spiking Transformers.](http://arxiv.org/abs/2307.00293) | AutoST是一种无需训练的神经架构搜索方法，用于识别高性能和能效的脉冲变压器架构。 |
| [^8] | [All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning.](http://arxiv.org/abs/2307.00290) | 本论文介绍了一种称为全能SAM的流程，通过在整个AI开发工作流程中使用SAM，并且在推理阶段无需手动提示，实现了从弱注释到基于提示的像素级细胞核分割的目标。 |
| [^9] | [CMA-ES for Post Hoc Ensembling in AutoML: A Great Success and Salvageable Failure.](http://arxiv.org/abs/2307.00286) | 本论文研究使用CMA-ES来替代贪婪集成选择方法，在AutoML中的表现。比较结果表明对于ROC AUC指标，CMA-ES出现了严重过拟合问题。 |
| [^10] | [Assembled-OpenML: Creating Efficient Benchmarks for Ensembles in AutoML with OpenML.](http://arxiv.org/abs/2307.00285) | Assembled-OpenML是一个使用OpenML构建集成算法元数据集的工具，通过使用预测结果来降低计算成本，实现了自动机器学习中集成算法技术的比较。 |
| [^11] | [SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency.](http://arxiv.org/abs/2307.00280) | SysNoise是一种在深度学习的训练-部署周期中经常发生的噪音，该论文通过实验证明了SysNoise对不同任务的模型稳健性会带来一定影响，并提出了常见的缓解方法。 |
| [^12] | [Common Knowledge Learning for Generating Transferable Adversarial Examples.](http://arxiv.org/abs/2307.00274) | 本文提出了一种常识学习框架，通过学习更好的网络权重生成具有更好可迁移性的对抗样本，解决了输出不一致性问题。该框架通过构建多教师模型并提取知识，减少模型特定的特征，获得更好的输出分布。 |
| [^13] | [Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning.](http://arxiv.org/abs/2307.00268) | 本文介绍了一种利用差分隐私噪音的本地中毒攻击方法（PeLPA）以绕过异常检测系统，并针对合作多智能体强化学习（CMARL）中的私有知识共享过程中的中毒威胁。研究结果表明，在不同环境下，PeLPA攻击能够显著增加平均步数。 |
| [^14] | [An ML approach to resolution of singularities.](http://arxiv.org/abs/2307.00252) | 该论文介绍了一种新的机器学习方法，使用强化学习代理来解决奇点问题中的最优解。实验证明在多项式相加的总数方面，该方法超过了当前最先进的选择启发式算法，展示了近期研究的潜力。 |
| [^15] | [Safe Screening for Unbalanced Optimal Transport.](http://arxiv.org/abs/2307.00247) | 本文介绍了一种利用安全筛选技术来加速不平衡最优传输问题优化过程的框架，通过识别和消除稀疏解中的零元素，显著提高了筛选效率。 |
| [^16] | [On a Relation Between the Rate-Distortion Function and Optimal Transport.](http://arxiv.org/abs/2307.00246) | 这项研究发现了速率-失真函数和最优传输理论之间的联系，并将它们在解决标量量化问题方面统一起来。 |
| [^17] | [Unified Transfer Learning Models for High-Dimensional Linear Regression.](http://arxiv.org/abs/2307.00238) | UTrans是一种统一转移学习模型，它能检测可转移变量和源数据，并具有较低的估计和预测误差，同时保持可解释性。 |
| [^18] | [Hierarchical Federated Learning Incentivization for Gas Usage Estimation.](http://arxiv.org/abs/2307.00233) | 提出了一个基于层次化联邦学习的气体使用估计激励机制，旨在解决燃气公司和供暖站在联邦学习训练过程中的积极参与问题，并通过奖励参与者的贡献来支持水平和垂直的联邦学习。 |
| [^19] | [Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study.](http://arxiv.org/abs/2307.00231) | 本研究调查了在高光谱图像分类中应用前向前向算法（FFA），该算法通过计算局部优势函数来减轻对计算资源和架构扩展的依赖。 |
| [^20] | [InferTurbo: A Scalable System for Boosting Full-graph Inference of Graph Neural Network over Huge Graphs.](http://arxiv.org/abs/2307.00228) | InferTurbo是一种可扩展系统，用于处理工业场景中大规模图上的图神经网络推断任务。它采用类似GAS模式的计算范式和数据流描述方法，并通过迭代方式进行计算。 |
| [^21] | [Causal Structure Learning by Using Intersection of Markov Blankets.](http://arxiv.org/abs/2307.00227) | 本文提出了一种新颖的因果结构学习算法，该算法利用马尔可夫毯交集，并结合了贝叶斯网络和结构因果模型的特性。此外，还提出了EEMBI-PC，它是EEMBI的扩展版本，将PC算法的最后一步集成到EEMBI中。 |
| [^22] | [S-Omninet: Structured Data Enhanced Universal Multimodal Learning Architecture.](http://arxiv.org/abs/2307.00226) | S-Omninet是一种结构化数据增强的通用多模态学习架构，通过引入交叉缓存注意力、整合视觉输入的块嵌入和支持结构化数据，能够同时处理多个模态和任务，有效地学习来自各个维度的结构化数据和非结构化数据。 |
| [^23] | [Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals.](http://arxiv.org/abs/2307.00222) | 本论文重新思考和重新设计图神经网络，通过在连续图扩散函数空间中引入变分分析的归纳偏置，解决了GNN在捕捉长程依赖和全局模式上的局限性，并通过引入总变差（TV）来解决过度平滑问题，从而构建了具有数学保证的离散深度模型。 |
| [^24] | [A Constructive Approach to Function Realization by Neural Stochastic Differential Equations.](http://arxiv.org/abs/2307.00215) | 本文采用了一种构造性方法，通过限制系统动力学来刻画可以实现的函数类，从而避免了高复杂度的控制。实现方法包括神经随机微分方程、确定性动力系统和输出映射的级联连接。这些结果有助于提高函数逼近算法的实际可行性。 |
| [^25] | [More for Less: Compact Convolutional Transformers Enable Robust Medical Image Classification with Limited Data.](http://arxiv.org/abs/2307.00213) | 本研究调查了紧凑卷积转换器（CCT）在有限数据条件下进行稳健的医学图像分类的有效性，通过将转换器与卷积层结合，CCT表现出了在规模适中的数据集上高准确性。 |
| [^26] | [An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application.](http://arxiv.org/abs/2307.00185) | 本文提出了一种可解释的增量随机权重神经网络的构造算法，通过几何信息约束和节点池策略解决了难以解释隐藏参数与残差误差之间关系的问题。这种算法在大规模数据建模任务中表现出了良好的性能。 |
| [^27] | [Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks.](http://arxiv.org/abs/2307.00175) | 本文讨论了大型语言模型是否具有信念以及如何衡量它们的问题，并通过实证结果和对最新论证的分析，指出现在仍然没有针对大型语言模型的谎言探测器。 |
| [^28] | [The Integer Linear Programming Inference Cookbook.](http://arxiv.org/abs/2307.00171) | 本论文介绍了一个整数线性规划推理手册，用于将推理问题转化为整数线性规划实例。通过一系列技巧的演示，帮助读者理解如何应用这些方法。论文最后提供了两个示例以说明这些技巧的使用。 |
| [^29] | [VoxWatch: An open-set speaker recognition benchmark on VoxCeleb.](http://arxiv.org/abs/2307.00169) | 本研究提出了一个针对VoxCeleb的开放式演讲者识别基准，并探讨了处理观察名单大小对检测性能影响的技术。 |
| [^30] | [U-Calibration: Forecasting for an Unknown Agent.](http://arxiv.org/abs/2307.00168) | 研究者们提出了一种针对未知代理的预测问题的新度量指标U-校准，该指标能够保证所有代理具有次线性的悔恨值。 |
| [^31] | [What do self-supervised speech models know about words?.](http://arxiv.org/abs/2307.00162) | 通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。 |
| [^32] | [FFPDG: Fast, Fair and Private Data Generation.](http://arxiv.org/abs/2307.00161) | 提出了一种快速、公平、灵活和私密的数据生成方法，可以在真实应用场景下表现良好。 |
| [^33] | [The Effect of Balancing Methods on Model Behavior in Imbalanced Classification Problems.](http://arxiv.org/abs/2307.00157) | 平衡方法对不平衡分类问题中模型行为产生显著影响。这些发现强调了平衡分析在模型训练中的重要性。 |
| [^34] | [Stitched ViTs are Flexible Vision Backbones.](http://arxiv.org/abs/2307.00154) | 本研究通过拼接预训练模型族群，提出了SN-Netv2，它是一个灵活的视觉骨干网络框架，可以在运行时实现多样性的性能和效率权衡。 |
| [^35] | [Hierarchical Neural Coding for Controllable CAD Model Generation.](http://arxiv.org/abs/2307.00149) | 本文提出了一种可控的CAD模型生成方法，通过分层神经编码和代码树来表示设计概念和控制生成过程，并在传统任务和条件生成任务上展示了优越性能。 |
| [^36] | [Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows.](http://arxiv.org/abs/2307.00144) | 本文通过定义和研究梯度流中的守恒定律，以及在模型的雅可比矩阵生成的李代数上进行有限维代数运算，揭示了超参数调节的模型保留了一些优化初始化的特性，这可能解释了训练模型具有良好泛化特性的原因。 |
| [^37] | [BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting.](http://arxiv.org/abs/2307.00142) | 本文提出了BuildingsBench，这是一个包含900K座建筑物的大规模数据集，旨在解决短期负荷预测中数据集不足的问题。通过该数据集，我们进行了两个任务的基准评估，并发现经过合成预训练的模型具有良好的泛化能力。 |
| [^38] | [Risk-sensitive Actor-free Policy via Convex Optimization.](http://arxiv.org/abs/2307.00141) | 本文提出了一种通过凸优化实现风险敏感的无演员策略，利用条件风险值作为目标函数，并使用凸神经网络模型来识别全局最优动作，实验结果表明该方法在保持有效的风险控制方面很有效。 |
| [^39] | [Generalization Limits of Graph Neural Networks in Identity Effects Learning.](http://arxiv.org/abs/2307.00134) | 本研究在学习身份效应的背景下，分析了图神经网络在泛化属性和基本限制方面的新性质，以及在两个字母的单词案例中的具体应用。 |
| [^40] | [Machine learning for advancing low-temperature plasma modeling and simulation.](http://arxiv.org/abs/2307.00131) | 机器学习在低温等离子体建模和模拟方面的应用受到了广泛关注，它能够提供新的方法和途径，促进等离子体科学和技术的发展。 |
| [^41] | [Accelerating Inexact HyperGradient Descent for Bilevel Optimization.](http://arxiv.org/abs/2307.00126) | 提出一种加速非精确超梯度下降的方法用于双层优化，可以在较低的复杂度下找到一阶和二阶稳定点，成为双层优化和凸-凹极小极大优化问题中的最新最佳状态。 |
| [^42] | [RObotic MAnipulation Network (ROMAN) -- Hybrid Hierarchical Learning for Solving Complex Sequential Tasks.](http://arxiv.org/abs/2307.00125) | RObotic MAnipulation Network（ROMAN）通过混合层次学习框架解决复杂的连续操作任务，实现了任务多样性和鲁棒的失败恢复。 |
| [^43] | [How Do Human Users Teach a Continual Learning Robot in Repeated Interactions?.](http://arxiv.org/abs/2307.00123) | 本文采用以人类为中心的方法研究持续学习，通过对40名参与者与持续学习机器人的长期交互进行定性和定量分析，发现人类的教学风格存在显著变化。 |
| [^44] | [Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control.](http://arxiv.org/abs/2307.00117) | 本文介绍了一种半监督语言接口控制机器人的方法，通过联合图像和目标信息的策略以及少量的语言数据实现了在真实世界中的稳健性能。 |
| [^45] | [Ticket-BERT: Labeling Incident Management Tickets with Language Models.](http://arxiv.org/abs/2307.00108) | Ticket-BERT是一个使用语言模型为事件管理票据进行标注的方法，在解决复杂的票据数据和时间敏感性问题方面具有优势。 |
| [^46] | [Distance Functions and Normalization Under Stream Scenarios.](http://arxiv.org/abs/2307.00106) | 论文研究了在流场景中的数据规范化问题，比较了八种距离函数的准确度，并发现在没有预先了解数据流信息的情况下，使用原始数据流和Canberra距离的组合效果较好。 |
| [^47] | [Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems.](http://arxiv.org/abs/2307.00104) | 本论文通过对视频序列中烟雾模式的时间分析，提出了一种用于实时检测被遮挡的野火火焰的方法，通过预测火灾位置来帮助无人机对抗森林火灾。这种方法具有独特的检测被遮挡的火灾的能力。 |
| [^48] | [Redeeming Data Science by Decision Modelling.](http://arxiv.org/abs/2307.00088) | 本文提出了决策建模的概念，用于赎回数据科学实践并解决其基础失去的问题。决策建模将传统的机器学习模型与显式价值模型结合起来，通过六个原则核心构成，提高了决策质量。 |
| [^49] | [Inter-case Predictive Process Monitoring: A candidate for Quantum Machine Learning?.](http://arxiv.org/abs/2307.00080) | 该论文研究了跨案例预测过程监测在预测准确性上的影响，并包括了量子机器学习模型，对于处理高维特征具有优势。 |
| [^50] | [Dataset balancing can hurt model performance.](http://arxiv.org/abs/2307.00079) | 数据集平衡方法在提高常见类别性能的同时，会损害稀有类别的性能，并且其效果依赖于评估集。 |
| [^51] | [Transformers in Healthcare: A Survey.](http://arxiv.org/abs/2307.00067) | Transformer神经网络架构在医疗保健中的应用具有巨大潜力，可以用于医学影像分析、临床诊断、报告生成、数据重建和药物/蛋白质合成。 |
| [^52] | [Improving the Transferability of Time Series Forecasting with Decomposition Adaptation.](http://arxiv.org/abs/2307.00066) | 通过分解自适应，我们提出了一种新的转让架构SeDAN，通过对齐来自不同领域数据集的可迁移知识来改善时间序列预测的性能。 |
| [^53] | [Towards Brain Inspired Design for Addressing the Shortcomings of ANNs.](http://arxiv.org/abs/2307.00039) | 本研究通过对比人工神经网络和小脑中基于错误的神经元组织方式，发现在人工神经网络中引入个性化错误视图的神经元群体可以提高学习效率、减少不平衡数据和捷径策略的影响，从而提高泛化能力。 |
| [^54] | [Machine learning for potion development at Hogwarts.](http://arxiv.org/abs/2307.00036) | 机器学习方法在霍格沃茨魔法学校的魔药发展中表现出潜在的创新和贡献。 |
| [^55] | [Parameter Identification for Partial Differential Equations with Spatiotemporal Varying Coefficients.](http://arxiv.org/abs/2307.00035) | 本论文提出了一个新的框架，用于解决具有空时变系数的偏微分方程中的参数识别问题。通过约束自适应物理信息神经网络和有限混合模型方法，我们可以准确确定复杂多状态系统的未知参数。 |
| [^56] | [Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making.](http://arxiv.org/abs/2307.00033) | 本研究应用数据工程方法解决微生物组数据中的类别不平衡和高维度问题，针对囊性纤维化婴儿的数据集实施了四种机器学习分类器，并提高了医学决策的准确性。 |
| [^57] | [Uncertainty Informed Optimal Resource Allocation with Gaussian Process based Bayesian Inference.](http://arxiv.org/abs/2307.00032) | 本研究提出了一种基于高斯过程贝叶斯推理的数据驱动方法，用于将医疗资源（疫苗）不确定性导向地分配给异质人群以管理流行病传播。研究解决了参数估计和整合、非线性ODE约束和参数不确定性等问题。 |
| [^58] | [Seeing in Words: Learning to Classify through Language Bottlenecks.](http://arxiv.org/abs/2307.00028) | 本文提出了一种通过语言瓶颈学习分类的方法，利用文本表示特征的视觉模型能够有效分类ImageNet图像，可以增加神经网络的可解释性。 |
| [^59] | [EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech.](http://arxiv.org/abs/2307.00024) | 本研究提出了一种名为EmoSpeech的模型，通过对FastSpeech2进行一系列修改，实现了情感语音的合成。根据评估结果，EmoSpeech在生成的语音中具有更高的MOS得分和情感识别准确度。模型引入了条件机制，可以有效处理文本中情感不均衡分布的问题，生成具有更高MOS和情感表达的音频。 |
| [^60] | [Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions.](http://arxiv.org/abs/2307.00014) | 本文综述了惯性导航领域中当前的深度学习方法，包括对不同车辆操作领域的研究、滤波参数学习的改进以及惯性传感器的校准和去噪方法。翻译过的论文标题: 惯性导航与深度学习：当前趋势与未来方向的综述 |
| [^61] | [Black-Box Prediction of Flaky Test Fix Categories Using Language Models.](http://arxiv.org/abs/2307.00012) | 本文提出了一个使用语言模型的框架，可以自动生成易出错测试的标记数据集，并通过分析测试代码来预测测试的修复类别。实验结果表明UniXcoder优于CodeBERT。 |
| [^62] | [Automated Assignment and Classification of Software Issues.](http://arxiv.org/abs/2307.00009) | 本论文提出了一种自动分配和分类软件问题的方法。通过使用经过精心策划的语言特征和不同的机器学习方法，将问题分配给最相关的团队成员，并将其分类为不同的类别，以提高工作效率和准确性。 |
| [^63] | [PV Fleet Modeling via Smooth Periodic Gaussian Copula.](http://arxiv.org/abs/2307.00004) | 本文提出了一种通过平滑周期高斯Copula模型联合建模光伏系统群发电的方法，该方法可以捕捉数据的昼夜变化、系统间的依赖关系和随时间的依赖关系。通过该模型可以进行合成数据生成、缺失数据填补、异常检测和预测。 |
| [^64] | [Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions.](http://arxiv.org/abs/2306.17624) | Sphere2Vec是一种多尺度位置编码器，用于在球面上编码点坐标时保持球面距离，解决了大规模真实世界GPS坐标数据集中的距离度量问题。 |
| [^65] | [Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version).](http://arxiv.org/abs/2306.17323) | 该论文通过状态空间缩减和输入分割提出了一个扩展DNN分析的模型检验框架，解决了模型检验的可扩展性问题。 |
| [^66] | [Improving Online Continual Learning Performance and Stability with Temporal Ensembles.](http://arxiv.org/abs/2306.16817) | 该研究通过模型集成方法改进了在线连续学习的性能和稳定性，通过综合利用来自不同训练任务的模型，显著提高了在线连续学习的表现。 |
| [^67] | [Separable Physics-Informed Neural Networks.](http://arxiv.org/abs/2306.15969) | 这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。 |
| [^68] | [GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation.](http://arxiv.org/abs/2306.15868) | 提出了一种基于对比学习的带有梯度引导采样策略（GraSS）用于遥感图像语义分割任务，解决了正样本混淆和特征适应偏差的问题。 |
| [^69] | [SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design.](http://arxiv.org/abs/2306.15656) | SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。 |
| [^70] | [DataCI: A Platform for Data-Centric AI on Streaming Data.](http://arxiv.org/abs/2306.15538) | DataCI是一个开源平台，专为流数据中的数据中心人工智能而设计，提供丰富的API和版本控制功能，具有易于使用和有效性，可以改变流数据背景下的数据中心人工智能实践。 |
| [^71] | [Learning non-Markovian Decision-Making from State-only Sequences.](http://arxiv.org/abs/2306.15156) | 本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。 |
| [^72] | [Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction.](http://arxiv.org/abs/2306.14891) | 该论文提出了一种模糊条件扩散的方法，可以利用隐式扩散先验并以可控强度应用于图像修复。同时，结合扩散引导的注意力图，可以对面部图像进行解释性和自主的修复。 |
| [^73] | [PMaF: Deep Declarative Layers for Principal Matrix Features.](http://arxiv.org/abs/2306.14759) | 本文介绍了PMaF框架，使用声明性的深度层来学习主要矩阵特征，通过迭代优化解决问题并应用双层优化框架进行反向传播，从而提高效率。实验证明了该框架优于现有的基线模型。 |
| [^74] | [Near Optimal Heteroscedastic Regression with Symbiotic Learning.](http://arxiv.org/abs/2306.14288) | 本研究提出了一种基于共生学习的异方差回归的近似最优算法，可以在统计学、计量经济学、时间序列分析等领域，以及在不同来源数据质量不一的机器学习中应用。 |
| [^75] | [A First Order Meta Stackelberg Method for Robust Federated Learning.](http://arxiv.org/abs/2306.13800) | 本研究提出了一种鲁棒的联邦学习防御方法，使用元Stackelberg学习算法解决贝叶斯Stackelberg马尔科夫博弈，实现自适应防御，与现有技术相匹配并在实验中表现出色。 |
| [^76] | [On Addressing the Limitations of Graph Neural Networks.](http://arxiv.org/abs/2306.12640) | 本文讨论了图神经网络的两个挑战：过度平滑和异质性，提出了解决方案并展望了未来的研究方向。 |
| [^77] | [State-wise Constrained Policy Optimization.](http://arxiv.org/abs/2306.12594) | 本文提出了一种新的通用策略搜索算法，State-wise Constrained Policy Optimization (SCPO)，可用于处理状态限制约束下的强化学习，具有良好的期望状态约束保证和最坏安全违反的有界性。 |
| [^78] | [A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design.](http://arxiv.org/abs/2306.11768) | 本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。 |
| [^79] | [G-NM: A Group of Numerical Time Series Prediction Models.](http://arxiv.org/abs/2306.11667) | G-NM是一组集合了传统和现代模型的数字时间序列预测模型，旨在提高对复杂自然现象中的模式和趋势的预测能力。 |
| [^80] | [Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network.](http://arxiv.org/abs/2306.10946) | 本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。 |
| [^81] | [Evolving Strategies for Competitive Multi-Agent Search.](http://arxiv.org/abs/2306.10640) | 本文研究了基于竞争多智能体搜索的演化策略，通过实验验证了进化计算可以用于发现在不同竞争环境中的有效搜索策略。 |
| [^82] | [Federated Few-shot Learning.](http://arxiv.org/abs/2306.10234) | 本研究提出了一种名为“联邦少样本学习”的新问题，旨在解决联邦学习在少样本数据上的性能问题。我们提出了一个简单而有效的框架，使用特征提取和任务适应模块以及注意力机制来提高模型对于少样本客户端的泛化能力，在各种数据集上取得了最先进的联邦少样本学习性能。 |
| [^83] | [Online Heavy-tailed Change-point detection.](http://arxiv.org/abs/2306.09548) | 本文提出了一种在线变点检测算法，可以应对重尾分布且保证有限的假阳性率。 |
| [^84] | [Tree Variational Autoencoders.](http://arxiv.org/abs/2306.08984) | 树形变分自编码器(TreeVAE)是一种新的生成式层次聚类模型，通过学习灵活的树状潜变量后验分布，层次划分数据样本并揭示隐藏结构。该模型利用树的生成式架构进行轻量级条件推理，同时通过专门的叶子解码器提高生成性能。在各种数据集上，TreeVAE发现了潜在簇并找到了有意义的层次关系。与顺序对应物相比，TreeVAE提供了更具竞争力的对数似然下界。 |
| [^85] | [Differentiating Metropolis-Hastings to Optimize Intractable Densities.](http://arxiv.org/abs/2306.07961) | 本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。 |
| [^86] | [Taxonomy-Structured Domain Adaptation.](http://arxiv.org/abs/2306.07874) | 本文提出了分层结构领域自适应方法，通过分层结构领域的形式化描述来缓解不同领域之间的分布偏移，该方法在合成和实际数据集上实现了最先进的性能。 |
| [^87] | [Automated 3D Pre-Training for Molecular Property Prediction.](http://arxiv.org/abs/2306.07812) | 通过在3D分子图上进行预训练，该论文提出了一种更有效地预测分子属性的方法，该方法可以在不需要分子几何结构的情况下进行微调。 |
| [^88] | [V-LoL: A Diagnostic Dataset for Visual Logical Learning.](http://arxiv.org/abs/2306.07743) | V-LoL是一个结合视觉和逻辑挑战的诊断数据集，其中包括了V-LoL-Trains，该数据集首次将复杂的视觉场景和灵活的逻辑推理任务结合起来，为研究广泛的视觉逻辑学习挑战提供了平台。 |
| [^89] | [DRCFS: Doubly Robust Causal Feature Selection.](http://arxiv.org/abs/2306.07024) | DRCFS是一个双重稳健因果特征选择方法，可以在非线性和高维环境中识别因果特征，优于现有方法。 |
| [^90] | [Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization.](http://arxiv.org/abs/2306.06674) | 该论文提出了一种自监督等式嵌入深度Lagrange对偶算法，用于解决不带标签的逼近限制优化问题。此方法通过在神经网络中嵌入等式约束来确保可行解，并使用原始-对偶方法进行训练，同时DeepLDE取得了最好的优化结果。 |
| [^91] | [Layer-level activation mechanism.](http://arxiv.org/abs/2306.04940) | 去噪声更好，表现更好的分层级别激活机制 |
| [^92] | [On the Reliability of Watermarks for Large Language Models.](http://arxiv.org/abs/2306.04634) | 本文研究了大型语言模型水印在混合其他文本来源时的可靠性，并提供了在实际应用中的建议。 |
| [^93] | [ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis.](http://arxiv.org/abs/2306.04527) | ContriMix是一种无需标识和手工调优的领域泛化技术，在显微镜图像中通过分离和学习生成合成图像的方式，解决了领域泛化的问题。 |
| [^94] | [Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching.](http://arxiv.org/abs/2306.04376) | 本文提出一种名为DFM框架的方法，用于量化标签偏移，并证明了其性能上限和鲁棒性。使用基于核的DFM版本可以提高效率、可扩展性和鲁棒性。 |
| [^95] | [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How.](http://arxiv.org/abs/2306.03828) | 本文提出一种方法快速选择最佳的预训练模型和微调超参数，通过生成大规模元数据集并元学习多保真度性能预测器，并在学习新数据集时使用该预测器进行超参数优化，可以快速实现此目标。 |
| [^96] | [LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion.](http://arxiv.org/abs/2306.02561) | 本论文提出了LLM-Blender，它是一个集成框架，旨在利用不同的开源大型语言模型的优秀特性，实现始终如一的卓越性能。PairRanker和GenFuser是该框架的两个模块，PairRanker使用成对比较方法来区分候选输出，并且GenFuser旨在合并排名最高的候选者，以生成改进的输出。 |
| [^97] | [Transfer learning for atomistic simulations using GNNs and kernel mean embeddings.](http://arxiv.org/abs/2306.01589) | 本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。 |
| [^98] | [Train Offline, Test Online: A Real Robot Learning Benchmark.](http://arxiv.org/abs/2306.00942) | 这是一个提供离线训练和在线测试的机器人学习基准系统，通过共享机器人硬件和开源数据集，解决了机器人学习研究中的挑战，并为未来的研究提供了方便和直接的比较方法。 |
| [^99] | [Reconstructing Graph Diffusion History from a Single Snapshot.](http://arxiv.org/abs/2306.00488) | 本文研究了从单个快照中重建图扩散历史的问题，揭示了现有方法的局限性，并提出了一种新的方法。 |
| [^100] | [Representer Point Selection for Explaining Regularized High-dimensional Models.](http://arxiv.org/abs/2305.20002) | 我们提出了一种解释高维模型预测的新方法，通过代表点选择来解释每个训练样本的重要性权重。我们的方法可以适用于各种正则化模型，并在协同过滤领域中有具体应用。 |
| [^101] | [W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition.](http://arxiv.org/abs/2305.18624) | W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。 |
| [^102] | [Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer.](http://arxiv.org/abs/2305.16380) | 本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。 |
| [^103] | [Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization.](http://arxiv.org/abs/2305.15889) | 本文提出了一种Feature Heterogeneity Distance(FHD)度量标准来衡量领域的异质性，并引入了一个新的实验模式Contrastive Convergence for Domain Generalization (CCDG) 来寻找最佳的监督信号来提高泛化。实验表明，我们的方法比其他最先进的方法更加有效和优越。 |
| [^104] | [GUARD: A Safe Reinforcement Learning Benchmark.](http://arxiv.org/abs/2305.13681) | GUARD是一个广义统一安全强化学习开发基准测试平台，是目前广泛遍布且包含各种RL代理、任务和安全约束规范的一站式基准测试，能够全面涵盖最先进的安全RL算法，并具有高度的可自定义性。 |
| [^105] | [Restore Anything Pipeline: Segment Anything Meets Image Restoration.](http://arxiv.org/abs/2305.13093) | 本文提出了一种新颖的交互式和基于对象级别的图像恢复方法，即Restore Anything Pipeline (RAP)，该方法通过将图像分割技术与可控的图像恢复模型相结合，为多个图像恢复任务创建了一个用户友好的流程。 |
| [^106] | [Towards Object Re-Identification from Point Clouds for 3D MOT.](http://arxiv.org/abs/2305.10210) | 该论文研究面向三维MOT中的点云再识别问题，提出了一种轻量级匹配头用于点云ReID的网络，通过实验结果表明，随着传感器分辨率的提高和观测点密度的增加，点云ReID的表现逐渐接近于图像ReID。 |
| [^107] | [Neural Wave Functions for Superfluids.](http://arxiv.org/abs/2305.06989) | 本论文利用费米神经网络波函数方法研究了均匀费米气体超流，提出一种针对FermiNet模型的改进方法，获得了极其准确的结果。 |
| [^108] | [How to Index Item IDs for Recommendation Foundation Models.](http://arxiv.org/abs/2305.06569) | 本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。 |
| [^109] | [Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging.](http://arxiv.org/abs/2305.06174) | 本文分析了工业、倡导组织和气候倡导组织在社交媒体上如何影响气候变化的叙事，并提出了一个最小化监督模型组合方法，用于识别Facebook上气候广告的立场。 |
| [^110] | [On the Relation between Sharpness-Aware Minimization and Adversarial Robustness.](http://arxiv.org/abs/2305.05392) | SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗性能。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。SAM单独使用可以在不牺牲清晰度精度的情况下提高对抗鲁棒性。 |
| [^111] | [Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems.](http://arxiv.org/abs/2305.03774) | 提出一种新的物理知识的、边界条件感知的、局部化学习方法，将E2C和E2CO模型扩展到对流扩散反应系统中，可以以极高的准确性预测系统未来状态，同时大幅减少训练时间。 |
| [^112] | [BrainNPT: Pre-training of Transformer networks for brain network classification.](http://arxiv.org/abs/2305.01666) | 本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。 |
| [^113] | [Wearing face mask detection using deep learning through COVID-19 pandemic.](http://arxiv.org/abs/2305.00068) | 本研究探讨了在 COVID-19 疫情期间使用深度学习模型进行口罩佩戴检测的可行性。通过比较不同模型，选择了适用于实时和移动设备应用的最佳模型，并取得了高准确度。 |
| [^114] | [Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark.](http://arxiv.org/abs/2304.14343) | 本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。 |
| [^115] | [On Manifold Learning in Plato's Cave: Remarks on Manifold Learning and Physical Phenomena.](http://arxiv.org/abs/2304.14248) | 本文通过一个警示故事阐释了分析数据时，测量几何和底层现象几何差异带来的问题，以及这种差异在某些情况下如何导致对一个修正过的问题给出错误答案。这些问题适用于降维和无监督学习领域。 |
| [^116] | [UNADON: Transformer-based model to predict genome-wide chromosome spatial position.](http://arxiv.org/abs/2304.13230) | UNADON是一种基于Transformer的深度学习模型，可以预测全基因组的染色体空间位置。通过使用序列特征和表观遗传信号，UNADON在训练单个细胞系时能够高度准确地预测染色质空间定位到核体，并揭示了潜在影响染色质区隔的序列和表观遗传因素。 |
| [^117] | [Latent Traversals in Generative Models as Potential Flows.](http://arxiv.org/abs/2304.12944) | 该论文使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行，以实现解缠，并通过分类器进行约束。 |
| [^118] | [Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack.](http://arxiv.org/abs/2304.11436) | 本文揭示了即便使用FedMD的安全机制，仍存在被精心设计的恶意攻击利用的风险，如Paired-Logits反演攻击，会导致隐私数据曝光。 |
| [^119] | [The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning.](http://arxiv.org/abs/2304.09914) | 本文使用机器学习的算法分析了来自15个不同国家的220个政治领袖的YouTube视频，总结了政治领袖面部情感表达的差异。 |
| [^120] | [Attention Mixtures for Time-Aware Sequential Recommendation.](http://arxiv.org/abs/2304.08158) | MOJITO是一种改进的Transformer顺序推荐系统，利用注意力混合建模用户偏好和时间背景的复杂依赖关系，从而准确预测下一个推荐物品。在多个真实数据集中，MOJITO表现优于现有的Transformer模型。 |
| [^121] | [Towards Better Evaluation of GNN Expressiveness with BREC Dataset.](http://arxiv.org/abs/2304.07702) | 本论文介绍了一个新的Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)数据集，并使用BREC评估了几种现有的GNN模型的表达力，表明一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。 |
| [^122] | [Variations of Squeeze and Excitation networks.](http://arxiv.org/abs/2304.06502) | 本文提出了Squeeze and Excitation网络的变体来改进重要特征的学习过程，从而提高神经网络的性能。实验表明这些变体在残差网络上效果良好。 |
| [^123] | [Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition.](http://arxiv.org/abs/2304.01117) | 本文分析了2022年基因与进化计算会议举办的竞赛，评估了符号回归的新方法在面对现实数据时的表现，并提供了可解释性评估的实际方法。 |
| [^124] | [TOFA: Transfer-Once-for-All.](http://arxiv.org/abs/2303.15485) | TOFA使用权重共享来进行神经架构搜索，以优化超网以适应各种设备的各种部署情况。与现有方法不同，TOFA在小数据集上进行训练，计算训练成本与部署方案数量无关。TOFA使用统一的半监督训练方法来解决小数据集带来的挑战。 |
| [^125] | [Verifying Properties of Tsetlin Machines.](http://arxiv.org/abs/2303.14464) | 该论文介绍了一种对Tsetlin Machine进行正式验证性质的方法，并展示了对抗性鲁棒性、等价性和相似性方面的结果。此外，还将新的模型相似性概念应用于TsMs。 |
| [^126] | [Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing.](http://arxiv.org/abs/2303.14244) | 本论文研究了过参数化低秩矩阵感知问题，证明了通过因子化方法训练的过参数化模型可以收敛，并且隐式平衡和正则化可以促进泛化。 |
| [^127] | [Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks.](http://arxiv.org/abs/2303.11338) | 本论文提出了一个开源生物信号领域泛化评估基准，并引入一种专门解决生物信号中领域泛化问题的神经网络架构DGNet-Bio。通过实验证明，DGNet-Bio在ECG和EEG分类领域泛化上优于现有方法。 |
| [^128] | [ExoplANNET: A deep learning algorithm to detect and identify planetary signals in radial velocity data.](http://arxiv.org/abs/2303.09335) | 本文介绍了一种神经网络算法ExoplANNET，旨在解决径向速度法检测系外行星的挑战，在存在与星体相关的噪声的情况下进行行星信号的检测和分类，经过合成数据和真实数据的测试，取得了有前途的结果。 |
| [^129] | [Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint.](http://arxiv.org/abs/2303.04356) | 本文改进了软Actor-Critic（SAC）算法的实现，通过引入可学习的状态相关的松弛变量来适当处理不等式约束，实现了最大化策略熵。这对于增强机器人控制器的鲁棒性非常有用。 |
| [^130] | [Physics-Informed Deep Learning For Traffic State Estimation: A Survey and the Outlook.](http://arxiv.org/abs/2303.02063) | 本文调查了将物理知识和深度学习相结合的PIDL在交通状态估计中的应用。在设计PIDL计算图的过程中，重点考虑了如何将物理知识编码到深度神经网络中，并展示了不同架构设计在交通状态估计中的差异和效果。 |
| [^131] | [A Closer Look at the Intervention Procedure of Concept Bottleneck Models.](http://arxiv.org/abs/2302.14260) | 本文研究了概念瓶颈模型介入程序的提高介入效果的方法，通过选择介入概念以及深入分析发现，在实际情况下，明智的介入策略可以将任务误差降低十倍以上，而且这个差异可相当明显。 |
| [^132] | [From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning.](http://arxiv.org/abs/2302.12559) | 本论文研究了将差分隐私机器学习算法视为有噪声的不动点迭代，从而从这一框架中推导出隐私和效用结果。通过利用这个新的视角，我们恢复了流行的私有梯度下降方法并提供了一种有原则的方法来设计和分析新的私有优化算法。我们通过通用框架推导出了用于集中式、联合和完全去中心化学习的新颖私有ADMM算法，并通过迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果提供效用保证。 |
| [^133] | [Graph Neural Networks with Learnable and Optimal Polynomial Bases.](http://arxiv.org/abs/2302.12432) | 本文提出了两种具有可学习和最优多项式基函数的谱图神经网络模型，通过学习多项式基函数和计算最优基函数，解决了多项式滤波器在模型有效性方面的问题。 |
| [^134] | [Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs.](http://arxiv.org/abs/2302.11700) | 该论文研究了经济学中两种机制的可学习性：菜单抽奖和两部分票价。他们提出了第一个针对这两种机制的在线学习算法。 |
| [^135] | [Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks.](http://arxiv.org/abs/2302.11628) | 本文提出了一种名为特征分区聚合的认证防御方法，用于对抗$\ell_0$逃避、后门和污染攻击。与现有防御方法相比，FPA速度更快，提供更大的鲁棒性保证，且能够免费提供额外的鲁棒性维度。 |
| [^136] | [Likelihood Annealing: Fast Calibrated Uncertainty for Regression.](http://arxiv.org/abs/2302.11012) | 该论文提出了一种名为似然退火的快速校准回归任务不确定性估计方法，能够改进深度回归模型的收敛性并产生校准的不确定性估计。 |
| [^137] | [Red Teaming Deep Neural Networks with Feature Synthesis Tools.](http://arxiv.org/abs/2302.10894) | 本文提出了一个用于评估可解释性工具的基准，通过训练模型以对特定触发器产生特定输出的方式，可以解决传统可解释性方法无法分析未知特征行为的问题。 |
| [^138] | [FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection.](http://arxiv.org/abs/2302.10681) | 本文提出了一种基于显著性指导的神经特征压缩与浅层变分瓶颈注入的新的资源意识压缩模型的框架，实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。 |
| [^139] | [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat.](http://arxiv.org/abs/2302.10289) | 本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。 |
| [^140] | [Neural Algorithmic Reasoning with Causal Regularisation.](http://arxiv.org/abs/2302.10258) | 提出了一种具有因果正则化的神经算法推理方法，通过观察到对于某些中间计算来说存在许多不同的输入，可以开发数据增强程序，生成能够使目标算法有完全相同下一轨迹步骤的输入，从而提高在分布外测试数据上的性能。 |
| [^141] | [Topological Feature Selection: A Graph-Based Filter Feature Selection Approach.](http://arxiv.org/abs/2302.09543) | 本文提出了一种基于图论的特征选择方法，利用拓扑学和依赖关系，具有高度灵活性和解释性，在16个基准数据集上显示出优于或匹配于当前最先进技术的表现。 |
| [^142] | [Extensible Motion-based Identification of XR Users using Non-Specific Motion Data.](http://arxiv.org/abs/2302.07517) | 提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。 |
| [^143] | [Derandomized Novelty Detection with FDR Control via Conformal E-values.](http://arxiv.org/abs/2302.07294) | 通过使用统一E-值来量化统计显著性，我们提出了一种去随机化的新颖性检测方法，该方法可以稳定地聚合相同数据的多次分析的证据，同时控制虚假发现率。 |
| [^144] | [Task-Specific Skill Localization in Fine-tuned Language Models.](http://arxiv.org/abs/2302.06600) | 本文提出了针对Fine-tuned语言模型中任务特定技能定位的问题，并提出了一种解决方案，通过优化可以识别出贡献模型性能的非常小的参数子集，使得将Fine-tuned的值嫁接到这个子集上可以获得几乎和Fine-tuned模型一样好的性能。 |
| [^145] | [Less is More: Selective Layer Finetuning with SubTuning.](http://arxiv.org/abs/2302.06354) | 本研究提出了一种选择性层微调与子微调的方法，通过仅对精心选择的层进行微调，而将其余权重保持在预训练值上。该方法在准确性上能够与全模型微调相媲美，并在训练数据稀缺时表现更好。这一简单而有效的方法适用于多任务学习，并能够在推理过程中实现任务间的资源共享。 |
| [^146] | [Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data.](http://arxiv.org/abs/2302.06279) | 本文研究使用神经形态数据和多样化的刺激在脉冲神经网络中的后门攻击问题。 |
| [^147] | [A High-dimensional Convergence Theorem for U-statistics with Applications to Kernel-based Testing.](http://arxiv.org/abs/2302.05686) | 本论文证明了一个适用于核心测试的高维U统计的收敛定理，并发现U统计的极限分布会经历从非退化高斯极限到退化极限的相变。这一现象对于高维情况下的非退化U统计具有较大方差和不对称分布的非高斯极限具有重要意义。此外，我们提出的界限适用于任何有限数量和维度的样本，与底层函数的特征值无关，并且在某些假设下与维度无关。我们还将我们的理论应用到两个常用的基于核函数的分布测试方法，MMD和KSD，来研究它们的高维性能。我们的结果能够准确预测测试功率如何与维度和带宽的关系。 |
| [^148] | [State-wise Safe Reinforcement Learning: A Survey.](http://arxiv.org/abs/2302.03122) | 本文综合回顾了强化学习中解决基于状态约束的方法，讨论了它们在安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面的联系、差异和权衡，并讨论了未来发展方向。 |
| [^149] | [Enhancing Exploration in Latent Space Bayesian Optimization.](http://arxiv.org/abs/2302.02399) | 本文提出了一种新的方法来提高潜空间贝叶斯优化（LSBO）的探索能力。方法包括潜在一致性感知获取函数（LCA-AF）和增加一致性点的潜空间生成方法（LCA-VAE），将它们结合起来形成了LCA-LSBO。实验证明LCA-LSBO在图像生成和全新的化学设计任务中表现出改进的性能。 |
| [^150] | [Diversity Induced Environment Design via Self-Play.](http://arxiv.org/abs/2302.02119) | 本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。 |
| [^151] | [Learning to Optimize for Reinforcement Learning.](http://arxiv.org/abs/2302.01470) | 学习优化器在监督学习中取得了显著的成功，但在强化学习中面临梯度范围变化大、梯度分布非独立且不同、高方差偏差等问题。本文提出了梯度处理、管道训练和一种新颖的优化器结构来解决这些问题。 |
| [^152] | [Sample Efficient Deep Reinforcement Learning via Local Planning.](http://arxiv.org/abs/2301.12579) | 提出了一种名为UFLP的算法框架，通过重置环境到高不确定性状态来提高深度强化学习的样本效率，实验证明这个简单的过程可以显著改善采样成本，并在困难的探索任务上取得超人类的表现。 |
| [^153] | [Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection.](http://arxiv.org/abs/2301.10908) | 本文提出了一种用于提取和检测图像中后门模式的简单方法，称为认知精炼（CD）。通过优化输入掩码，我们可以提取一个小模式，该模式可以导致模型产生相同的输出。使用CD和提取的模式，我们发现了后门攻击的一个有趣现象：后门样本的模式都非常小。所以可以利用学到的掩码从污染的训练数据集中检测和删除后门样本。 |
| [^154] | [Increasing Fairness via Combination with Learning Guarantees.](http://arxiv.org/abs/2301.10813) | 该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。 |
| [^155] | [Finding Lookalike Customers for E-Commerce Marketing.](http://arxiv.org/abs/2301.03147) | 本文介绍了一个以客户为中心的营销活动中寻找相似客户的可扩展和高效系统。该系统能处理亿级客户，并使用深度学习嵌入模型和近似最近邻搜索方法来寻找感兴趣的相似客户。通过构建可解释且有意义的客户相似度度量，该模型能够处理各种业务兴趣。 |
| [^156] | [CC-FedAvg: Computationally Customized Federated Averaging.](http://arxiv.org/abs/2212.13679) | 本论文提出了一个称为CC-FedAvg的计算定制的联邦平均算法，可让参与者根据其计算预算决定在每轮中是否执行传统的本地训练或模型估算。实验结果表明，CC-FedAvg能够显著提高模型性能并降低通信成本。 |
| [^157] | [When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories.](http://arxiv.org/abs/2212.10511) | 本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。 |
| [^158] | [Machine Learning and Polymer Self-Consistent Field Theory in Two Spatial Dimensions.](http://arxiv.org/abs/2212.10478) | 本文介绍了一种利用机器学习和聚合物自洽场理论模拟数据的计算框架，用于加速研究嵌段共聚物参数空间。创新包括使用CNN处理离散化的局部平均单体密度场、使用GAN预测局部平均单体密度场，以及实现空间平移和旋转不变性。该框架取得了成功。 |
| [^159] | [Data Poisoning Attack Aiming the Vulnerability of Continual Learning.](http://arxiv.org/abs/2211.15875) | 该论文针对基于正则化的持续学习方法的脆弱性，通过提出一种简单的任务特定数据污染攻击来展示其脆弱性。该攻击导致特定任务的性能下降，并且实验证明了这种脆弱性的重要性。 |
| [^160] | [Shapley Curves: A Smoothing Perspective.](http://arxiv.org/abs/2211.13289) | 本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。 |
| [^161] | [Backdoor Cleansing with Unlabeled Data.](http://arxiv.org/abs/2211.12044) | 本文提出了一种无标签数据的后门清除方法，通过逐层权重重新初始化和知识蒸馏来有效清除可疑网络的后门行为，并在基准数据集上取得较好效果。 |
| [^162] | [Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models.](http://arxiv.org/abs/2211.11736) | 本文提出了一种通过视觉-语言模型的指令增强方法，利用预训练模型将互联网规模的知识导入现有机器人数据集，实现机器人技能的获取。 |
| [^163] | [Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task.](http://arxiv.org/abs/2211.05039) | 该论文提出了一个具有挑战性的决策任务，主动获取多模态时间数据。通过权衡获取成本和预测性能，学习代理程序来主动选择获取的输入模态。该方法能够解决具有实际相关推理技能的合成情景，并在真实数据集上成功学习到成本反应式的获取行为，但无法学习到自适应的获取策略，突显了任务的困难性。 |
| [^164] | [Quantum-probabilistic Hamiltonian learning for generative modelling & anomaly detection.](http://arxiv.org/abs/2211.03803) | 本研究探讨了学习和利用量子力学系统的哈密顿和其变分热状态估计用于数据分析技术的可能性。我们通过将方法应用于大型强子对撞机数据的生成建模和异常检测中，展示了这些方法在量子多体系统中的应用潜力。 |
| [^165] | [Unlearning Graph Classifiers with Limited Data Resources.](http://arxiv.org/abs/2211.03216) | 本文研究了有限数据资源下的图分类器遗忘方法，提出了一种基于图散射变换的非线性近似图遗忘方法，并进行了计算复杂性的理论分析。 |
| [^166] | [Analysis of a Deep Learning Model for 12-Lead ECG Classification Reveals Learned Features Similar to Diagnostic Criteria.](http://arxiv.org/abs/2211.01738) | 本研究通过应用归因方法对12导联心电图分类的预训练深度神经网络进行分析，揭示了与诊断标准类似的学习特征。相关性分数的分析显示，随着分类概率的增加，平均值也增加，并且在接近零时产生误分类。 |
| [^167] | [CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data.](http://arxiv.org/abs/2210.17283) | CausalBench是一个用于评估基于单细胞干扰实验的真实世界干预数据上的网络推断方法的基准测试套件。通过系统评估，发现当前的方法可扩展性差，限制了性能。使用干预信息的方法并没有超越那些不使用干预信息的方法。 |
| [^168] | [Machine Unlearning of Federated Clusters.](http://arxiv.org/abs/2210.16424) | 本论文介绍了联邦聚类的机器未学习问题，并提出了一个定制的安全联邦聚类框架的高效未学习机制。同时，为了保护客户数据隐私，作者还开发了安全压缩多集合聚合框架。所提出的方法性能优良，适用于解决稀疏安全联邦学习问题和其他一般问题。 |
| [^169] | [Causal Explanation for Reinforcement Learning: Quantifying State and Temporal Importance.](http://arxiv.org/abs/2210.13507) | 本文开发了一种因果解释机制，能够量化状态对行动的因果重要性和随时间变化的重要性，并通过一系列的模拟研究证明了该机制在强化学习策略解释方面的优势。 |
| [^170] | [RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations.](http://arxiv.org/abs/2210.10737) | 通过随机稀疏计算，本研究提出了一种加速图神经网络训练的方法，解决了稀疏图操作难以加速和不规则数据格式导致的效率问题。 |
| [^171] | [CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling.](http://arxiv.org/abs/2210.07661) | 本文提出了一种全面的注意力基准测试（CAB），用于评估在建模长序列时的高效注意力方法。CAB包括了细粒度的注意力分类体系，涵盖了非因果自注意力、因果自注意力、非因果交叉注意力和因果交叉注意力四种注意力模式，并采集了七个真实世界任务进行评估。 |
| [^172] | [Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge.](http://arxiv.org/abs/2210.05723) | 本文研究了用于汇聚信息的各种汇聚运算符在嵌入编码中的应用。我们发现所有考虑的汇聚运算符都可以满足认识汇聚原则，但这仅在嵌入具有足够高维度并满足特定约束的情况下成立。这些约束对嵌入在实践中的使用具有重要影响。 |
| [^173] | [Neural Extended Kalman Filters for Learning and Predicting Dynamics of Structural Systems.](http://arxiv.org/abs/2210.04165) | 本论文提出了一种称为神经扩展卡尔曼滤波器（Neural EKF）的可学习卡尔曼滤波方法，用于学习复杂物理系统的潜在演化动力学。这种方法可以通过端到端训练来学习过程动力学和传感观测的建模，提高结构响应预测的准确性。 |
| [^174] | [The Vendi Score: A Diversity Evaluation Metric for Machine Learning.](http://arxiv.org/abs/2210.02410) | 本文提出了一种用于机器学习的多样性评估指标Vendi分数，它能够灵活地衡量不同形式的多样性，而且不需要参考数据集，适用于任何生成模型和数据集。 |
| [^175] | [Learning Signal Temporal Logic through Neural Network for Interpretable Classification.](http://arxiv.org/abs/2210.01910) | 本文提出了一个可解释的神经符号化框架，用于时间序列行为的分类。通过使用信号时态逻辑（STL）约束神经网络计算图的搜索，并设计新的时间函数和稀疏softmax函数，能够高效地学习一个紧凑的STL公式，实现时间序列数据的分类。通过与最先进的基线方法进行比较，在驾驶场景和海军监视案例研究中展示了该方法的计算效率、紧凑性和可解释性。 |
| [^176] | [Vertical Semi-Federated Learning for Efficient Online Advertising.](http://arxiv.org/abs/2209.15635) | 垂直半联合学习为在线广告领域提供了高效的解决方案，通过学习一个联合感知的局部模型以应对传统垂直联合学习的限制。 |
| [^177] | [FAIR-FATE: Fair Federated Learning with Momentum.](http://arxiv.org/abs/2209.13678) | FAIR-FATE是一种公平联邦学习算法，通过公平感知的聚合方法实现组公平性并保持高效用性。 |
| [^178] | [Learning When to Advise Human Decision Makers.](http://arxiv.org/abs/2209.13578) | 本文提出了一种新颖的人工智能系统设计，其中算法与人类用户以双向互动的方式交互，仅在对用户的决策有益时提供建议。实验证明，这种方法能够改善人类的决策能力，并在促进人类学习、保留人类决策的补充优势方面具有额外的优势。 |
| [^179] | [Convolutional Neural Network (CNN) to reduce construction loss in JPEG compression caused by Discrete Fourier Transform (DFT).](http://arxiv.org/abs/2209.03475) | 该论文介绍了使用卷积神经网络（CNN）来减少JPEG压缩中由离散傅里叶变换（DFT）引起的构造性损失。通过在特征提取中使用卷积，可以得到更少冗余的特征图和更小的数据集，从而提高图像压缩的质量。 |
| [^180] | [Online Bidding Algorithms for Return-on-Spend Constrained Advertisers.](http://arxiv.org/abs/2208.13713) | 本研究提出了一个用于满足投放成本回报率限制的广告商的在线竞价算法，通过简便的在线算法实现了接近最优的遗憾值，并且总结了与先前工作的集成性能。 |
| [^181] | [Community Detection in the Hypergraph SBM: Optimal Recovery Given the Similarity Matrix.](http://arxiv.org/abs/2208.12227) | 本文研究了超图随机块模型中的社区发现问题，通过分析相似性矩阵和设计谱算法，实现了在不同密度条件下的精确恢复和高效计算。 |
| [^182] | [Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness.](http://arxiv.org/abs/2208.06648) | 本文研究了填补选择对不同群体的重建误差和下游预测的算法公平性属性的影响。 |
| [^183] | [Feature-Based Time-Series Analysis in R using the theft Package.](http://arxiv.org/abs/2208.06146) | 本研究介绍了在R中使用theft包进行基于特征的时间序列分析的方法，并指出了当前存在的问题包括缺乏统一的访问点以及用户需要掌握多种编程语言来获得所有特征集。 |
| [^184] | [Infant movement classification through pressure distribution analysis -- added value for research and clinical implementation.](http://arxiv.org/abs/2208.00884) | 本文提出了一种创新型的非侵入式方法，使用压力传感器对婴儿的一般运动进行分类，旨在实现早期神经肌肉障碍（如脑瘫）的客观检测。研究结果表明，使用压力数据可以区分婴儿的典型运动模式，即“坐立不安期”和“坐立不安前期”。 |
| [^185] | [DRSOM: A Dimension Reduced Second-Order Method.](http://arxiv.org/abs/2208.00208) | 本文提出了一种降维的二阶优化方法（DRSOM），通过在少数方向上利用曲率信息来保持二阶方法的收敛性，并将计算开销控制在与一阶方法相当的程度。理论上证明了该方法在满足近似黑塞矩阵假设的条件下具有局部二次收敛和全局收敛速度为 $O(\epsilon^{-3/2})$。通过各种计算实验展示了该方法的适用性和性能。 |
| [^186] | [MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior.](http://arxiv.org/abs/2207.10553) | MABe22是一个多物种多任务基准，用于评估学习行为表示的质量。它采集了来自各种生物学实验的数据，测试了多种自监督学习方法，并发现人类行动数据集上的方法不能完全适用于动物数据集。 |
| [^187] | [Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery.](http://arxiv.org/abs/2206.10540) | 本文重新审视并重新创建了符号回归数据集，以评估符号回归在科学发现中的潜力和能力，并提出使用归一化编辑距离进行度量。 |
| [^188] | [A unified stochastic approximation framework for learning in games.](http://arxiv.org/abs/2206.03922) | 我们提出了一个统一的随机逼近框架，用于分析学习博弈的长期行为。该框架整合了多种常见的学习算法，并得到了多个新的收敛结果，包括对纳什均衡的识别和吸引行动配置集合的概率高收敛。 |
| [^189] | [Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective.](http://arxiv.org/abs/2206.03851) | 本文从分布转移视角出发，研究了从偏向反馈中学习无偏算法进行推荐的问题。通过建立无偏推荐与分布转移的关系，对现有无偏学习方法进行了理论解释并提出了两个泛化界限。 |
| [^190] | [Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms.](http://arxiv.org/abs/2206.03792) | 本文研究了基于随机近似的采样算法，利用中心极限定理结构吸收扩散过程中的随机逼近误差并获得了改进的收敛保证。此外，对于SGLD和RBM，我们分别证明了不同的假设条件下较优的收敛率和参数范围。 |
| [^191] | [NIPQ: Noise proxy-based Integrated Pseudo-Quantization.](http://arxiv.org/abs/2206.00820) | NIPQ提出了一种基于噪声代理的集成伪量化方法，通过融合截断理念，能够统一支持激活和权重的伪量化。NIPQ避免了直通估计器(STE)在量化感知训练中的不稳定性，并在实验中展现了优于现有量化算法的性能。 |
| [^192] | [How Biased are Your Features?: Computing Fairness Influence Functions with Global Sensitivity Analysis.](http://arxiv.org/abs/2206.00667) | 本论文介绍了公正影响函数（FIF），通过全局敏感性分析的方法量化了不同特征对分类器偏见的影响，从而解决了公平性问题中的核心关注点。 |
| [^193] | [Deep Direct Discriminative Decoders for High-dimensional Time-series Data Analysis.](http://arxiv.org/abs/2205.10947) | 这篇论文提出了一种用于高维时间序列数据分析的新方法，即深度直接判别解码器（D4）。D4通过引入深度神经网络的表达能力和可扩展性，有效地估计了高维观测信号下的潜在状态过程，并在多个数据集上展示了比传统方法更好的性能。 |
| [^194] | [Transformers in Time-series Analysis: A Tutorial.](http://arxiv.org/abs/2205.01138) | 这篇教程介绍了Transformer架构在时间序列分析中的应用，包括核心组件和多项增强方法，并提供了训练Transformer的最佳实践和技巧。 |
| [^195] | [Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations.](http://arxiv.org/abs/2204.02937) | 本文研究表明，简单的最后一层重新训练足以提高神经网络分类器对虚假相关性的鲁棒性，可以在虚假相关性基准测试中与最先进的方法相媲美或胜过，但其复杂度和计算开销较低。此外，对于在ImageNet训练的大型模型进行最后一层重新训练，仅几分钟的训练时间就可以显著降低对背景和纹理信息的依赖，提高对协变量转变的鲁棒性。 |
| [^196] | [Learning Hidden Markov Models When the Locations of Missing Observations are Unknown.](http://arxiv.org/abs/2203.06527) | 本文研究了当缺失观测的位置未知时学习隐马尔可夫模型的问题，并提供了不需要先验信息的重建算法。 |
| [^197] | [A Proximal Algorithm for Sampling.](http://arxiv.org/abs/2202.13975) | 本论文提出了一种用于处理缺乏平滑性的势能的采样问题的近端算法，在凸和非凸情况下均可适用。该算法的关键创新点在于基于拒绝采样的交替采样框架的实际实现，比现有方法更高效。 |
| [^198] | [Motif Graph Neural Network.](http://arxiv.org/abs/2112.14900) | Motif图神经网络（MGNN）是一种新颖的框架，用于更好地捕捉高阶图结构，并解决传统图神经网络在区分低阶和高阶图结构上的限制。 |
| [^199] | [A Black-box NLP Classifier Attacker.](http://arxiv.org/abs/2112.11660) | 本文提出了一个黑盒NLP分类器攻击模型，通过基于自注意机制的词选择和贪婪搜索算法进行词替换，解决了影响NLP领域传统图像攻击方法不适用的问题。 |
| [^200] | [Second-Order Mirror Descent: Convergence in Games Beyond Averaging and Discounting.](http://arxiv.org/abs/2111.09982) | 本文提出了二阶镜像下降（MD2）动力学，在博弈中收敛到非严格变分稳定状态（VSS），不需使用常见技巧，同时具有无遗憾和指数收敛速率。该方法还可以导出许多新的连续时间原始空间动力学，并且通过随机逼近技术对观测有噪声情况下的离散时间MD2提供了收敛保证。 |
| [^201] | [Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm.](http://arxiv.org/abs/2111.03892) | 本论文提出了TND-NAS框架，利用可微架构搜索框架和多目标NAS的兼容性，通过策略梯度算法实现高效多目标神经架构搜索。实验结果表明其优于现有方法。 |
| [^202] | [Deviance Matrix Factorization.](http://arxiv.org/abs/2110.05674) | 该论文研究了一种适用于偏差数据损失的通用矩阵分解方法，并且通过应用广义线性模型理论提供了支持，该方法具有灵活的算法和处理结构性零元素的能力。作者通过模拟研究和案例研究证明了该方法的鲁棒性和应用广泛性。 |
| [^203] | [Networked Time Series Prediction with Incomplete Data.](http://arxiv.org/abs/2110.02271) | 本文研究了具有不完整数据的网络时间序列（NETS）预测问题。提出了NETS-ImpGAN深度学习框架，可以训练不完整数据，并引入了图时序注意力网络来捕捉时间序列之间的相关性和时间相关性。 |
| [^204] | [Arterial blood pressure waveform in liver transplant surgery possesses variability of morphology reflecting recipients' acuity and predicting short term outcomes.](http://arxiv.org/abs/2109.10258) | 该研究使用动力扩散映射算法研究了肝移植手术中动脉血压波形的形态可变性与接受者病情以及短期结果之间的相关性。 |
| [^205] | [Truth Discovery in Sequence Labels from Crowds.](http://arxiv.org/abs/2109.04470) | 本研究提出了一种基于优化的方法（$AggSLC$），从众包数据中推断顺序标签的真实值，以解决顺序标签汇总任务中的复杂依赖关系问题。 |
| [^206] | [Learning an Explicit Hyperparameter Prediction Function Conditioned on Tasks.](http://arxiv.org/abs/2107.02378) | 本研究探索了一种基于任务的学习方法，学习一个显式超参数预测函数以适应不同的查询任务。 |
| [^207] | [Contextual Inverse Optimization: Offline and Online Learning.](http://arxiv.org/abs/2106.14015) | 这项研究研究了具有反馈信息的离线和在线情境优化问题，通过观察最佳动作并最小化后悔来优化决策制定。 |
| [^208] | [Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation.](http://arxiv.org/abs/2106.03907) | 本论文提出了一种深度代理因果学习（PCL）方法，用于在存在混淆因素的情况下估计治疗对结果的因果效应。通过构建治疗和代理之间的模型，并利用该模型在给定代理的情况下学习治疗对结果的影响，PCL可以保证恢复真实的因果效应。作者还提出了一种名为深度特征代理变量方法（DFPV）的新方法，用于处理高维和非线性复杂关系的情况，并表明DFPV在合成基准测试中的性能优于最先进的PCL方法。 |
| [^209] | [Shaped Policy Search for Evolutionary Strategies using Waypoints.](http://arxiv.org/abs/2105.14639) | 本文提出了一种使用航点的进化策略形状化策略搜索方法，旨在改进强化学习中黑盒方法的探索能力。该方法通过学习代理的动力学，结合优化过程加速训练，并在Carla驾驶和UR5机械臂模拟器上进行了实验证明其普适性。 |
| [^210] | [Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism.](http://arxiv.org/abs/2103.12021) | 本论文提出了一种新的联机强化学习框架，通过平滑插值的方式将模仿学习和纯联机强化学习统一起来。框架围绕着一种衡量行为策略与专家策略偏离程度的弱版本集中系数展开。通过该框架，研究者进一步研究了算法设计的问题：能否开发出实现最小极大最优性的算法？ |
| [^211] | [Generalized iterated-sums signatures.](http://arxiv.org/abs/2012.04597) | 本文研究了广义的迭代和签名的代数性质，并介绍了三个类似于机器学习应用的非线性变换。 |
| [^212] | [Transfer Learning in Deep Reinforcement Learning: A Survey.](http://arxiv.org/abs/2009.07888) | 这篇综述调查了深度强化学习领域中的迁移学习方法的最新进展，并提供了一个对这些方法进行分类的框架。分析了它们的目标、方法学、兼容的强化学习背景以及实际应用，并探讨了迁移学习与其他相关主题之间的联系。 |
| [^213] | [Learning to Switch Among Agents in a Team via 2-Layer Markov Decision Processes.](http://arxiv.org/abs/2002.04258) | 本文研究了在团队中学习切换代理控制的问题，并开发了一种在线学习算法，通过学习代理的策略和环境的转移概率，在不同自动化水平下使现有的强化学习代理能够工作。该算法的总遗憾与最佳切换策略相比是次线性的，当多个代理团队在相似环境中运行时，该算法从维护环境的共享置信界中获益匪浅。 |
| [^214] | [Meta Adaptation using Importance Weighted Demonstrations.](http://arxiv.org/abs/1911.10322) | 本文提出了一种使用重要权重示范的元适应性学习算法，通过对特定任务的先前知识进行分配重要权重，实现了在任何相关任务上的泛化。实验证明，该方法能够使机器人在多样化环境任务中进行训练，并通过少量示范适应未知环境。 |

# 详细

[^1]: SHARCS: 解释性多模态学习的共享概念空间

    SHARCS: Shared Concept Space for Explainable Multimodal Learning. (arXiv:2307.00316v1 [cs.LG])

    [http://arxiv.org/abs/2307.00316](http://arxiv.org/abs/2307.00316)

    SHARCS是一种解释性多模态学习方法，它通过学习和映射可解释的概念到一个统一的概念空间，实现了解释性的任务预测和改进性能。

    

    多模态学习是解决复杂现实世界问题的重要范式，在这种问题中，单个数据模态通常无法准确解决给定的建模任务。虽然各种深度学习方法已经成功地解决了这些挑战，但它们的推理过程通常是不透明的，限制了原则性的可解释跨模态分析和领域专家干预的能力。在本文中，我们介绍了SHARCS（SHARed Concept Space）——一种用于解释性多模态学习的新概念方法。SHARCS从不同的异构模态中学习和映射可解释的概念到一个统一的概念流形，从而产生了语义相似的跨模态概念的直观投影。我们证明了这种方法可以导致固有可解释的任务预测，同时也提高了下游的预测性能。此外，我们还展示了SHARCS可以运行并明显优于其他方法。

    Multimodal learning is an essential paradigm for addressing complex real-world problems, where individual data modalities are typically insufficient to accurately solve a given modelling task. While various deep learning approaches have successfully addressed these challenges, their reasoning process is often opaque; limiting the capabilities for a principled explainable cross-modal analysis and any domain-expert intervention. In this paper, we introduce SHARCS (SHARed Concept Space) -- a novel concept-based approach for explainable multimodal learning. SHARCS learns and maps interpretable concepts from different heterogeneous modalities into a single unified concept-manifold, which leads to an intuitive projection of semantically similar cross-modal concepts. We demonstrate that such an approach can lead to inherently explainable task predictions while also improving downstream predictive performance. Moreover, we show that SHARCS can operate and significantly outperform other approac
    
[^2]: 梯度相似：敏感度经常被过高估计在DP-SGD中

    Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])

    [http://arxiv.org/abs/2307.00310](http://arxiv.org/abs/2307.00310)

    本文开发了一种新的DP-SGD分析方法，可以在训练过程中对许多数据点的隐私泄漏进行更准确的评估。

    

    差分隐私随机梯度下降（DP-SGD）是私有深度学习的标准算法。虽然已知其隐私分析在最坏情况下是紧密的，但是一些实证结果表明，在常见的基准数据集上训练时，所得到的模型对许多数据点的隐私泄漏显著减少。在本文中，我们为DP-SGD开发了一种新的分析方法，捕捉到在数据集中具有相似邻居的点享受更好隐私性的直觉。形式上来说，这是通过修改从训练数据集计算得到的模型更新的每步隐私性分析来实现的。我们进一步开发了一个新的组合定理，以有效地利用这个新的每步分析来推理整个训练过程。总而言之，我们的评估结果表明，这种新颖的DP-SGD分析使我们能够正式地显示DP-SGD对许多数据点的隐私泄漏显著减少。

    Differentially private stochastic gradient descent (DP-SGD) is the canonical algorithm for private deep learning. While it is known that its privacy analysis is tight in the worst-case, several empirical results suggest that when training on common benchmark datasets, the models obtained leak significantly less privacy for many datapoints. In this paper, we develop a new analysis for DP-SGD that captures the intuition that points with similar neighbors in the dataset enjoy better privacy than outliers. Formally, this is done by modifying the per-step privacy analysis of DP-SGD to introduce a dependence on the distribution of model updates computed from a training dataset. We further develop a new composition theorem to effectively use this new per-step analysis to reason about an entire training run. Put all together, our evaluation shows that this novel DP-SGD analysis allows us to now formally show that DP-SGD leaks significantly less privacy for many datapoints. In particular, we ob
    
[^3]: 对3D点云分类的对抗攻击和防御方法综述

    Adversarial Attacks and Defenses on 3D Point Cloud Classification: A Survey. (arXiv:2307.00309v1 [cs.CV])

    [http://arxiv.org/abs/2307.00309](http://arxiv.org/abs/2307.00309)

    这篇综述论文总结了在点云分类中针对对抗攻击和防御技术的当前进展，介绍了对抗攻击原理和对抗样本生成方法，以及防御策略分类和未来的研究方向。

    

    深度学习在2D视觉领域已经成功解决了各种任务，成为主流的人工智能技术。最近，对3D点云的深度学习在解决该领域的各种任务上越来越受欢迎。尽管取得了显著的成就，深度学习算法却容易受到对抗性攻击的影响。这些攻击在人眼中是无法察觉的，但却能轻易欺骗深度神经网络在测试和部署阶段。为了促进未来的研究，本综述总结了关于点云分类中对抗攻击和防御技术的当前进展。本文首先介绍了对抗攻击的原理和特点，总结并分析了近年来的对抗样本生成方法。此外，它将防御策略分为输入转换，数据优化和深度模型修改三类。最后，提出了该领域中几个具有挑战性的问题和未来的研究方向。

    Deep learning has successfully solved a wide range of tasks in 2D vision as a dominant AI technique. Recently, deep learning on 3D point clouds is becoming increasingly popular for addressing various tasks in this field. Despite remarkable achievements, deep learning algorithms are vulnerable to adversarial attacks. These attacks are imperceptible to the human eye but can easily fool deep neural networks in the testing and deployment stage. To encourage future research, this survey summarizes the current progress on adversarial attack and defense techniques on point cloud classification. This paper first introduces the principles and characteristics of adversarial attacks and summarizes and analyzes the adversarial example generation methods in recent years. Besides, it classifies defense strategies as input transformation, data optimization, and deep model modification. Finally, it presents several challenging issues and future research directions in this domain.
    
[^4]: SyMFM6D：面向对称多方位融合的多视角6D物体姿态估计

    SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation. (arXiv:2307.00306v1 [cs.CV])

    [http://arxiv.org/abs/2307.00306](http://arxiv.org/abs/2307.00306)

    该论文介绍了一种叫做SyMFM6D的面向对称多视角融合的6D物体姿态估计方法。该方法通过多方向融合网络有效地融合多个视角的RGB-D帧，并通过预测关键点和实例语义分割来计算6D姿态。通过新的训练过程和目标函数，该方法能够解决对称物体的歧义问题，并在单视角和多视角姿态估计方面取得了显著的性能提升。

    

    检测物体并估计其6D姿态对于自动化系统与环境安全互动至关重要。然而，大多数6D姿态估计器仅依赖于单个摄像头帧，并且受到由于物体对称性而引起的遮挡和模糊的影响。我们通过提出一种新颖的面向对称多视角6D姿态估计器SyMFM6D来解决这个问题。我们的方法通过深度多方向融合网络有效地融合多个角度的RGB-D帧，并同时预测场景中所有物体的预定义关键点。基于关键点和实例语义分割，我们通过最小二乘拟合高效计算6D姿态。为了解决对称物体的歧义问题，我们提出了一种新的训练过程用于对称感知的关键点检测，包括一种新的目标函数。我们的SyMFM6D网络在单视角和多视角6D姿态估计方面显著优于现有技术。此外，我们还展示了该方法对姿态估计的影响。

    Detecting objects and estimating their 6D poses is essential for automated systems to interact safely with the environment. Most 6D pose estimators, however, rely on a single camera frame and suffer from occlusions and ambiguities due to object symmetries. We overcome this issue by presenting a novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach efficiently fuses the RGB-D frames from multiple perspectives in a deep multi-directional fusion network and predicts predefined keypoints for all objects in the scene simultaneously. Based on the keypoints and an instance semantic segmentation, we efficiently compute the 6D poses by least-squares fitting. To address the ambiguity issues for symmetric objects, we propose a novel training procedure for symmetry-aware keypoint detection including a new objective function. Our SyMFM6D network significantly outperforms the state-of-the-art in both single-view and multi-view 6D pose estimation. We furthermore show the effe
    
[^5]: 应用贝叶斯结构健康监测：测斜仪数据异常检测和预测

    Applied Bayesian Structural Health Monitoring: inclinometer data anomaly detection and forecasting. (arXiv:2307.00305v1 [cs.LG])

    [http://arxiv.org/abs/2307.00305](http://arxiv.org/abs/2307.00305)

    本文介绍了将贝叶斯技术应用于测斜仪数据的异常检测和预测，并且展示了如何通过量化和评估不确定性来最小化成本和风险。

    

    测斜仪探头是一种可以用来测量土方坡体变形的设备。本文展示了将贝叶斯技术应用于实际测斜仪数据的新颖方法，可以提供异常检测和预测功能。具体来说，本文详细介绍了对整个英国铁路网络中收集的测斜仪数据进行分析的情况。监测数据处理过程中，从业人员通常有两个目标，一是识别任何异常或危险的运动，二是通过预测来预测潜在未来的不良情况。在本文中，我们应用了不确定性量化（UQ）技术，通过实施贝叶斯方法，对测斜仪数据进行异常检测和预测。通过量化和评估适当的不确定性，可以最小化成本和风险。这个框架可以促进增强的决策和风险分析。我们展示了测斜仪数据可以被描述为

    Inclinometer probes are devices that can be used to measure deformations within earthwork slopes. This paper demonstrates a novel application of Bayesian techniques to real-world inclinometer data, providing both anomaly detection and forecasting. Specifically, this paper details an analysis of data collected from inclinometer data across the entire UK rail network.  Practitioners have effectively two goals when processing monitoring data. The first is to identify any anomalous or dangerous movements, and the second is to predict potential future adverse scenarios by forecasting. In this paper we apply Uncertainty Quantification (UQ) techniques by implementing a Bayesian approach to anomaly detection and forecasting for inclinometer data. Subsequently, both costs and risks may be minimised by quantifying and evaluating the appropriate uncertainties. This framework may then act as an enabler for enhanced decision making and risk analysis.  We show that inclinometer data can be described
    
[^6]: 扩大步长和算子学习的加速原始-对偶方法在非光滑最优控制问题中的应用

    Accelerated primal-dual methods with enlarged step sizes and operator learning for nonsmooth optimal control problems. (arXiv:2307.00296v1 [math.OC])

    [http://arxiv.org/abs/2307.00296](http://arxiv.org/abs/2307.00296)

    该论文研究了在非光滑最优控制问题中加速原始-对偶方法的两种方法：增大步长和算子学习。研究表明，增大步长的加速原始-对偶方法可以在保证收敛性的同时简单有效地提高计算速度；而算子学习则通过构建神经网络代理模型来加速求解涉及的偏微分方程。

    

    我们考虑一类具有偏微分方程（PDE）约束的非光滑最优控制问题，由于其非光滑目标函数和离散化后的高维和病态系统，这类问题非常具有挑战性。我们重点研究了原始-对偶方法的应用，该方法可以分别处理不同类型的变量，因此每次迭代的主要计算只需要解决两个PDE。我们的目标是通过使用较大的步长或算子学习技术来加速原始-对偶方法。对于具有较大步长的加速原始-对偶方法，其收敛性仍然可以得到严格证明，同时它以一种简单且普遍的方式数值上加速了原始-对偶方法。对于算子学习加速，我们构建了深度神经网络代理模型来表示涉及的PDE。一旦学习到一个神经算子，解决一个PDE只需要进行神经网络的前向传播。

    We consider a general class of nonsmooth optimal control problems with partial differential equation (PDE) constraints, which are very challenging due to its nonsmooth objective functionals and the resulting high-dimensional and ill-conditioned systems after discretization. We focus on the application of a primal-dual method, with which different types of variables can be treated individually and thus its main computation at each iteration only requires solving two PDEs. Our target is to accelerate the primal-dual method with either larger step sizes or operator learning techniques. For the accelerated primal-dual method with larger step sizes, its convergence can be still proved rigorously while it numerically accelerates the original primal-dual method in a simple and universal way. For the operator learning acceleration, we construct deep neural network surrogate models for the involved PDEs. Once a neural operator is learned, solving a PDE requires only a forward pass of the neural
    
[^7]: AutoST：无需训练的脉冲变压器神经架构搜索

    AutoST: Training-free Neural Architecture Search for Spiking Transformers. (arXiv:2307.00293v1 [cs.NE])

    [http://arxiv.org/abs/2307.00293](http://arxiv.org/abs/2307.00293)

    AutoST是一种无需训练的神经架构搜索方法，用于识别高性能和能效的脉冲变压器架构。

    

    脉冲变压器因同时具备脉冲神经网络（SNN）的能效和变压器的高容量而备受关注。然而，现有的脉冲变压器架构基于人工神经网络（ANN）的推导，存在明显的架构差距，导致性能不及其ANN对应物。传统方法通常依赖手动过程或神经架构搜索（NAS）方法来寻找最优架构，但这些方法要么耗时，要么在内存占用和计算时间方面代价高昂。为解决这些限制，我们提出了AutoST，一种用于脉冲变压器的无需训练的NAS方法，可以快速识别高性能和能效的脉冲变压器架构。与现有的无需训练NAS方法不同，我们提出利用浮点运算（FLOPs）作为一种指导可行性的度量，

    Spiking Transformers have gained considerable attention because they achieve both the energy efficiency of Spiking Neural Networks (SNNs) and the high capacity of Transformers. However, the existing Spiking Transformer architectures, derived from ANNs, exhibit a notable architectural gap, resulting in suboptimal performance compared to their ANN counterparts. Traditional approaches to discovering optimal architectures primarily rely on either manual procedures, which are time-consuming, or Neural Architecture Search (NAS) methods, which are usually expensive in terms of memory footprints and computation time. To address these limitations, we introduce AutoST, a training-free NAS method for Spiking Transformers, to rapidly identify high-performance and energy-efficient Spiking Transformer architectures. Unlike existing training-free NAS methods, which struggle with the non-differentiability and high sparsity inherent in SNNs, we propose to utilize Floating-Point Operations (FLOPs) as a 
    
[^8]: 全能SAM：从弱注释到基于提示的微观细胞核分割

    All-in-SAM: from Weak Annotation to Pixel-wise Nuclei Segmentation with Prompt-based Finetuning. (arXiv:2307.00290v1 [cs.CV])

    [http://arxiv.org/abs/2307.00290](http://arxiv.org/abs/2307.00290)

    本论文介绍了一种称为全能SAM的流程，通过在整个AI开发工作流程中使用SAM，并且在推理阶段无需手动提示，实现了从弱注释到基于提示的像素级细胞核分割的目标。

    

    目前，Segment Anything Model (SAM)是一种使用提示的通用零样本分割模型。然而，该流程在推理阶段仍然需要手动提示，对于生物医学图像分割仍然资源密集。本文介绍了一种称为全能SAM的流程，它在整个AI开发工作流程中使用了SAM，并且在推理阶段无需手动提示。具体而言，SAM首先利用弱提示（例如点、边界框）生成像素级注释，然后使用像素级注释对SAM分割模型进行微调。实验结果表明了两个关键发现：1）所提出的pi

    The Segment Anything Model (SAM) is a recently proposed prompt-based segmentation model in a generic zero-shot segmentation approach. With the zero-shot segmentation capacity, SAM achieved impressive flexibility and precision on various segmentation tasks. However, the current pipeline requires manual prompts during the inference stage, which is still resource intensive for biomedical image segmentation. In this paper, instead of using prompts during the inference stage, we introduce a pipeline that utilizes the SAM, called all-in-SAM, through the entire AI development workflow (from annotation generation to model finetuning) without requiring manual prompts during the inference stage. Specifically, SAM is first employed to generate pixel-level annotations from weak prompts (e.g., points, bounding box). Then, the pixel-level annotations are used to finetune the SAM segmentation model rather than training from scratch. Our experimental results reveal two key findings: 1) the proposed pi
    
[^9]: CMA-ES用于后续集成在AutoML中：一个巨大的成功和可挽救的失败。

    CMA-ES for Post Hoc Ensembling in AutoML: A Great Success and Salvageable Failure. (arXiv:2307.00286v1 [cs.LG])

    [http://arxiv.org/abs/2307.00286](http://arxiv.org/abs/2307.00286)

    本论文研究使用CMA-ES来替代贪婪集成选择方法，在AutoML中的表现。比较结果表明对于ROC AUC指标，CMA-ES出现了严重过拟合问题。

    

    许多最先进的自动机器学习（AutoML）系统在模型选择后使用Caruana等人（2004年）的贪婪集成选择（GES）来集成找到的模型。从而提高预测性能并且像 Auto-Sklearn 1 般提示，其他方法，如堆叠或无梯度数值优化，容易过拟合。Auto-Sklearn 1 中的过拟合比其他AutoML系统更有可能，因为它仅使用质量较低的验证数据进行后续集成。因此，我们有动力分析Auto-Sklearn 1 的观点是否适用于具有质量更高的验证数据的系统。因此，我们将协方差矩阵适应进化策略（CMA-ES），一个最先进的无梯度数值优化方法，与AutoGluon的AutoML基准测试中的GES在71个分类数据集上的性能进行了比较。我们发现，Auto-Sklearn的观点取决于选择的度量指标。对于ROC AUC指标，CMA-ES出现了严重的过拟合。

    Many state-of-the-art automated machine learning (AutoML) systems use greedy ensemble selection (GES) by Caruana et al. (2004) to ensemble models found during model selection post hoc. Thereby, boosting predictive performance and likely following Auto-Sklearn 1's insight that alternatives, like stacking or gradient-free numerical optimization, overfit. Overfitting in Auto-Sklearn 1 is much more likely than in other AutoML systems because it uses only low-quality validation data for post hoc ensembling. Therefore, we were motivated to analyze whether Auto-Sklearn 1's insight holds true for systems with higher-quality validation data. Consequently, we compared the performance of covariance matrix adaptation evolution strategy (CMA-ES), state-of-the-art gradient-free numerical optimization, to GES on the 71 classification datasets from the AutoML benchmark for AutoGluon. We found that Auto-Sklearn's insight depends on the chosen metric. For the metric ROC AUC, CMA-ES overfits drastically 
    
[^10]: Assembled-OpenML：使用OpenML创建自动机器学习中高效的集成算法基准

    Assembled-OpenML: Creating Efficient Benchmarks for Ensembles in AutoML with OpenML. (arXiv:2307.00285v1 [cs.LG])

    [http://arxiv.org/abs/2307.00285](http://arxiv.org/abs/2307.00285)

    Assembled-OpenML是一个使用OpenML构建集成算法元数据集的工具，通过使用预测结果来降低计算成本，实现了自动机器学习中集成算法技术的比较。

    

    自动机器学习（AutoML）框架经常使用集成算法。开发人员需要比较不同的集成算法技术，以选择适合AutoML框架的技术。到目前为止，集成算法技术的比较通常是计算密集型的，因为需要训练和评估许多基础模型一次或多次。因此，我们提出了Assembled-OpenML。Assembled-OpenML是一个基于Python的工具，使用OpenML构建集成算法的元数据集。元数据集称为Metatask，包含OpenML任务的数据、任务的数据集以及任务的模型评估预测数据。通过使用存储在元数据集中的预测结果，我们可以使集成算法技术的比较计算成本更低，而无需训练和评估基础模型。为了介绍Assembled-OpenML，我们描述了我们工具的第一个版本，并展示了使用Assembled-OpenML比较一组集成算法的示例。

    Automated Machine Learning (AutoML) frameworks regularly use ensembles. Developers need to compare different ensemble techniques to select appropriate techniques for an AutoML framework from the many potential techniques. So far, the comparison of ensemble techniques is often computationally expensive, because many base models must be trained and evaluated one or multiple times. Therefore, we present Assembled-OpenML. Assembled-OpenML is a Python tool, which builds meta-datasets for ensembles using OpenML. A meta-dataset, called Metatask, consists of the data of an OpenML task, the task's dataset, and prediction data from model evaluations for the task. We can make the comparison of ensemble techniques computationally cheaper by using the predictions stored in a metatask instead of training and evaluating base models. To introduce Assembled-OpenML, we describe the first version of our tool. Moreover, we present an example of using Assembled-OpenML to compare a set of ensemble technique
    
[^11]: SysNoise: 探索和评估训练-部署系统的不一致性

    SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency. (arXiv:2307.00280v1 [cs.LG])

    [http://arxiv.org/abs/2307.00280](http://arxiv.org/abs/2307.00280)

    SysNoise是一种在深度学习的训练-部署周期中经常发生的噪音，该论文通过实验证明了SysNoise对不同任务的模型稳健性会带来一定影响，并提出了常见的缓解方法。

    

    大量研究表明，深度学习模型容易受到敌对和自然噪音的影响，然而对于由不同系统实现引起的噪音对模型的稳健性知之甚少。本文首次引入SysNoise，一种在深度学习的训练-部署周期中经常发生但往往被忽视的噪音。具体而言，SysNoise发生在源训练系统在部署时切换到不同的目标系统时，各种微小系统不匹配累加起来会产生显著差异。我们首先对SysNoise进行了识别和分类，分为基于推理阶段的三个类别；然后建立了一个综合性评估标准，以定量评估SysNoise对20多种模型的影响，包括图像分类、目标检测、实例分割和自然语言处理任务。我们广泛的实验揭示了SysNoise对不同任务的模型稳健性会带来一定的影响，并提出了常见的缓解方法。

    Extensive studies have shown that deep learning models are vulnerable to adversarial and natural noises, yet little is known about model robustness on noises caused by different system implementations. In this paper, we for the first time introduce SysNoise, a frequently occurred but often overlooked noise in the deep learning training-deployment cycle. In particular, SysNoise happens when the source training system switches to a disparate target system in deployments, where various tiny system mismatch adds up to a non-negligible difference. We first identify and classify SysNoise into three categories based on the inference stage; we then build a holistic benchmark to quantitatively measure the impact of SysNoise on 20+ models, comprehending image classification, object detection, instance segmentation and natural language processing tasks. Our extensive experiments revealed that SysNoise could bring certain impacts on model robustness across different tasks and common mitigations li
    
[^12]: 生成可迁移对抗样本的常识学习

    Common Knowledge Learning for Generating Transferable Adversarial Examples. (arXiv:2307.00274v1 [cs.LG])

    [http://arxiv.org/abs/2307.00274](http://arxiv.org/abs/2307.00274)

    本文提出了一种常识学习框架，通过学习更好的网络权重生成具有更好可迁移性的对抗样本，解决了输出不一致性问题。该框架通过构建多教师模型并提取知识，减少模型特定的特征，获得更好的输出分布。

    

    本文关注一种重要的黑盒攻击类型，即基于迁移的对抗攻击，在这种攻击中，对手通过一个替代（原始）模型生成对抗样本，并利用它们来攻击一个未知的目标模型，而不知道其信息。现有方法在源模型和目标模型来自不同类型的DNN架构（例如ResNet-18和Swin Transformer）时往往给出了不令人满意的对抗可迁移性。本文观察到上述现象是由输出不一致性问题引起的。为了在固定的网络架构下缓解这个问题并有效利用现有的DNN模型，我们提出了一种常识学习（CKL）框架，通过学习更好的网络权重生成具有更好可迁移性的对抗样本。具体来说，为了减少模型特定的特征并获得更好的输出分布，我们构建了一个多教师框架，从不同教师模型中提取知识。

    This paper focuses on an important type of black-box attacks, i.e., transfer-based adversarial attacks, where the adversary generates adversarial examples by a substitute (source) model and utilize them to attack an unseen target model, without knowing its information. Existing methods tend to give unsatisfactory adversarial transferability when the source and target models are from different types of DNN architectures (e.g. ResNet-18 and Swin Transformer). In this paper, we observe that the above phenomenon is induced by the output inconsistency problem. To alleviate this problem while effectively utilizing the existing DNN models, we propose a common knowledge learning (CKL) framework to learn better network weights to generate adversarial examples with better transferability, under fixed network architectures. Specifically, to reduce the model-specific features and obtain better output distributions, we construct a multi-teacher framework, where the knowledge is distilled from diffe
    
[^13]: 明里暗中：利用差分隐私噪音的抵御恶意攻击的本地中毒攻击方法在多智能体强化学习中的应用

    Hiding in Plain Sight: Differential Privacy Noise Exploitation for Evasion-resilient Localized Poisoning Attacks in Multiagent Reinforcement Learning. (arXiv:2307.00268v1 [cs.LG])

    [http://arxiv.org/abs/2307.00268](http://arxiv.org/abs/2307.00268)

    本文介绍了一种利用差分隐私噪音的本地中毒攻击方法（PeLPA）以绕过异常检测系统，并针对合作多智能体强化学习（CMARL）中的私有知识共享过程中的中毒威胁。研究结果表明，在不同环境下，PeLPA攻击能够显著增加平均步数。

    

    最近，差分隐私（DP）被引入到合作多智能体强化学习（CMARL）中，以保护智能体在知识共享过程中免受对手的推断攻击。然而，我们认为由DP机制引入的噪音可能会在CMARL中的私有知识共享过程中意外地产生一种新的中毒威胁，这在文献中尚未得到研究。为了解决这个问题，我们提出了一种自适应的、利用隐私的、抵御逃避攻击的本地中毒攻击方法（PeLPA），利用了DP噪音的特性，绕过异常检测系统，并阻碍CMARL模型的最优收敛。我们在不同的环境中对我们提出的PeLPA攻击进行了严格的评估，包括非对抗和多对抗环境。我们的研究结果表明，在中等规模环境中，攻击者比例为20%和40%的PeLPA攻击可能导致平均步数的增加。

    Lately, differential privacy (DP) has been introduced in cooperative multiagent reinforcement learning (CMARL) to safeguard the agents' privacy against adversarial inference during knowledge sharing. Nevertheless, we argue that the noise introduced by DP mechanisms may inadvertently give rise to a novel poisoning threat, specifically in the context of private knowledge sharing during CMARL, which remains unexplored in the literature. To address this shortcoming, we present an adaptive, privacy-exploiting, and evasion-resilient localized poisoning attack (PeLPA) that capitalizes on the inherent DP-noise to circumvent anomaly detection systems and hinder the optimal convergence of the CMARL model. We rigorously evaluate our proposed PeLPA attack in diverse environments, encompassing both non-adversarial and multiple-adversarial contexts. Our findings reveal that, in a medium-scale environment, the PeLPA attack with attacker ratios of 20% and 40% can lead to an increase in average steps t
    
[^14]: 一种解决奇点问题的机器学习方法

    An ML approach to resolution of singularities. (arXiv:2307.00252v1 [cs.LG])

    [http://arxiv.org/abs/2307.00252](http://arxiv.org/abs/2307.00252)

    该论文介绍了一种新的机器学习方法，使用强化学习代理来解决奇点问题中的最优解。实验证明在多项式相加的总数方面，该方法超过了当前最先进的选择启发式算法，展示了近期研究的潜力。

    

    多项式方程组的解集通常包含不光滑、奇异的点。解决奇点是几何中的基本过程，我们将奇点替换为光滑点，同时保持解集的剩余部分不变。解决奇点并不是唯一的：通常的方法是反复进行被称为“blowing-up”的基本操作，解决的复杂性高度依赖于某些选择。这个过程可以转化成不同版本的两人博弈，即所谓的Hironaka游戏，而第一位玩家的获胜策略提供了解决奇点问题的解。本文介绍了一种新的Hironaka游戏方法，使用强化学习代理来寻找奇点的最优解。在某些领域中，训练模型在多项式相加的总数方面优于最先进的选择启发式算法，这证明了最近发展的潜力。

    The solution set of a system of polynomial equations typically contains ill-behaved, singular points. Resolution is a fundamental process in geometry in which we replace singular points with smooth points, while keeping the rest of the solution set unchanged. Resolutions are not unique: the usual way to describe them involves repeatedly performing a fundamental operation known as "blowing-up", and the complexity of the resolution highly depends on certain choices. The process can be translated into various versions of a 2-player game, the so-called Hironaka game, and a winning strategy for the first player provides a solution to the resolution problem. In this paper we introduce a new approach to the Hironaka game that uses reinforcement learning agents to find optimal resolutions of singularities. In certain domains, the trained model outperforms state-of-the-art selection heuristics in total number of polynomial additions performed, which provides a proof-of-concept that recent devel
    
[^15]: 安全筛选不平衡最优传输的方法

    Safe Screening for Unbalanced Optimal Transport. (arXiv:2307.00247v1 [math.OC])

    [http://arxiv.org/abs/2307.00247](http://arxiv.org/abs/2307.00247)

    本文介绍了一种利用安全筛选技术来加速不平衡最优传输问题优化过程的框架，通过识别和消除稀疏解中的零元素，显著提高了筛选效率。

    

    本文介绍了一种利用安全筛选技术来加速优化过程的不平衡最优传输问题的框架，通过主动识别和消除稀疏解中的零元素。我们通过分析解的上界、考虑对偶问题的局部强凸性，证明了将安全筛选应用于具有$\ell_2$-惩罚和KL-惩罚的不平衡最优传输问题的可行性。考虑到不平衡最优传输相对于一般索引矩阵上的Lasso问题的特定结构特征，我们特别提出了一种新的近似投影、椭圆安全区域构造和两个超平面松弛方法。这些改进显著提高了不平衡最优传输问题的筛选效率，而不改变算法的复杂性。

    This paper introduces a framework that utilizes the Safe Screening technique to accelerate the optimization process of the Unbalanced Optimal Transport (UOT) problem by proactively identifying and eliminating zero elements in the sparse solutions. We demonstrate the feasibility of applying Safe Screening to the UOT problem with $\ell_2$-penalty and KL-penalty by conducting an analysis of the solution's bounds and considering the local strong convexity of the dual problem. Considering the specific structural characteristics of the UOT in comparison to general Lasso problems on the index matrix, we specifically propose a novel approximate projection, an elliptical safe region construction, and a two-hyperplane relaxation method. These enhancements significantly improve the screening efficiency for the UOT's without altering the algorithm's complexity.
    
[^16]: 关于速率-失真函数和最优传输之间的关系

    On a Relation Between the Rate-Distortion Function and Optimal Transport. (arXiv:2307.00246v1 [cs.IT])

    [http://arxiv.org/abs/2307.00246](http://arxiv.org/abs/2307.00246)

    这项研究发现了速率-失真函数和最优传输理论之间的联系，并将它们在解决标量量化问题方面统一起来。

    

    我们讨论了速率-失真函数和最优传输（OT）理论之间的关系，尽管它们乍一看似乎不相关。特别地，我们展示了通过极值熵最优传输距离定义的函数等价于速率-失真函数。我们通过数值验证了这个结果，以及将蒙日问题和坎托洛维奇问题与最优标量量化相连的先前结果。因此，我们通过使用各自的最优传输求解器，以一种替代的方式统一了解决标量量化和速率-失真函数。

    We discuss a relationship between rate-distortion and optimal transport (OT) theory, even though they seem to be unrelated at first glance. In particular, we show that a function defined via an extremal entropic OT distance is equivalent to the rate-distortion function. We numerically verify this result as well as previous results that connect the Monge and Kantorovich problems to optimal scalar quantization. Thus, we unify solving scalar quantization and rate-distortion functions in an alternative fashion by using their respective optimal transport solvers.
    
[^17]: 高维线性回归的统一转移学习模型

    Unified Transfer Learning Models for High-Dimensional Linear Regression. (arXiv:2307.00238v1 [stat.ML])

    [http://arxiv.org/abs/2307.00238](http://arxiv.org/abs/2307.00238)

    UTrans是一种统一转移学习模型，它能检测可转移变量和源数据，并具有较低的估计和预测误差，同时保持可解释性。

    

    在现代数据分析中，当目标数据稀缺而源数据充足，或者源数据和目标数据的分布不同的情况下，转移学习在发挥重要作用。本文提出了一种可解释的统一转移学习模型，称为UTrans，该模型能够检测可转移变量和源数据。具体来说，我们建立了估计误差界限，并证明我们的界限低于仅有目标数据的界限。此外，我们基于假设检验提出了一种源数据检测算法，用于排除不可转移的数据。我们在多个实验中评估和比较了UTrans与现有算法。结果显示，UTrans在保持可解释性的同时，比现有方法具有更低的估计和预测误差。最后，我们将其应用于美国代际流动数据，并将我们提出的算法与经典的机器学习算法进行比较。

    Transfer learning plays a key role in modern data analysis when: (1) the target data are scarce but the source data are sufficient; (2) the distributions of the source and target data are heterogeneous. This paper develops an interpretable unified transfer learning model, termed as UTrans, which can detect both transferable variables and source data. More specifically, we establish the estimation error bounds and prove that our bounds are lower than those with target data only. Besides, we propose a source detection algorithm based on hypothesis testing to exclude the nontransferable data. We evaluate and compare UTrans to the existing algorithms in multiple experiments. It is shown that UTrans attains much lower estimation and prediction errors than the existing methods, while preserving interpretability. We finally apply it to the US intergenerational mobility data and compare our proposed algorithms to the classical machine learning algorithms.
    
[^18]: 基于层次化联邦学习的气体使用估计激励机制

    Hierarchical Federated Learning Incentivization for Gas Usage Estimation. (arXiv:2307.00233v1 [cs.LG])

    [http://arxiv.org/abs/2307.00233](http://arxiv.org/abs/2307.00233)

    提出了一个基于层次化联邦学习的气体使用估计激励机制，旨在解决燃气公司和供暖站在联邦学习训练过程中的积极参与问题，并通过奖励参与者的贡献来支持水平和垂直的联邦学习。

    

    准确估计气体使用量对于天然气配送网络的高效运行和节约运营成本至关重要。传统方法依赖于集中式数据处理，存在隐私风险。联邦学习通过使每个参与者（如燃气公司和供暖站）能够进行本地数据处理，为解决这个问题提供了解决方案。然而，本地训练和通信开销可能会使燃气公司和供暖站对联邦学习训练过程不太积极参与。为了解决这个挑战，我们提出了一个基于层次化联邦学习的气体使用估计激励机制（HI-GAS），它在天然气和绿色能源行业的领先企业之一——ENN集团中进行了实验验证。它旨在支持燃气公司之间的水平联邦学习，以及在层次化联邦学习生态系统中每个燃气公司和供暖站之间的垂直联邦学习，并根据他们对联邦学习的贡献对参与者进行奖励。

    Accurately estimating gas usage is essential for the efficient functioning of gas distribution networks and saving operational costs. Traditional methods rely on centralized data processing, which poses privacy risks. Federated learning (FL) offers a solution to this problem by enabling local data processing on each participant, such as gas companies and heating stations. However, local training and communication overhead may discourage gas companies and heating stations from actively participating in the FL training process. To address this challenge, we propose a Hierarchical FL Incentive Mechanism for Gas Usage Estimation (HI-GAS), which has been testbedded in the ENN Group, one of the leading players in the natural gas and green energy industry. It is designed to support horizontal FL among gas companies, and vertical FL among each gas company and heating station within a hierarchical FL ecosystem, rewarding participants based on their contributions to FL. In addition, a hierarchic
    
[^19]: 前向前向算法用于高光谱图像分类：初步研究

    Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study. (arXiv:2307.00231v1 [cs.CV])

    [http://arxiv.org/abs/2307.00231](http://arxiv.org/abs/2307.00231)

    本研究调查了在高光谱图像分类中应用前向前向算法（FFA），该算法通过计算局部优势函数来减轻对计算资源和架构扩展的依赖。

    

    反向传播算法长期以来一直是神经网络中优化权重和偏差的事实标准，尤其在先进的深度学习模型中。它在自然语言处理、计算机视觉和遥感等领域的广泛应用已经在各种任务的自动化方面引发了革命。反向传播的流行源于它在分类、检测和分割等任务中能够实现卓越的性能。然而，反向传播并非没有限制，包括对初始条件的敏感性、梯度消失、过拟合和计算复杂性。最近引入的一种前向前向算法（FFA）通过计算局部优势函数来优化网络参数，从而减轻了对大量计算资源和不断扩展架构的依赖。本研究调查了FFA在高光谱图像分类中的应用。

    The back-propagation algorithm has long been the de-facto standard in optimizing weights and biases in neural networks, particularly in cutting-edge deep learning models. Its widespread adoption in fields like natural language processing, computer vision, and remote sensing has revolutionized automation in various tasks. The popularity of back-propagation stems from its ability to achieve outstanding performance in tasks such as classification, detection, and segmentation. Nevertheless, back-propagation is not without its limitations, encompassing sensitivity to initial conditions, vanishing gradients, overfitting, and computational complexity. The recent introduction of a forward-forward algorithm (FFA), which computes local goodness functions to optimize network parameters, alleviates the dependence on substantial computational resources and the constant need for architectural scaling. This study investigates the application of FFA for hyperspectral image classification. Experimental
    
[^20]: InferTurbo: 一种用于处理大规模图上图神经网络全图推断的可扩展系统

    InferTurbo: A Scalable System for Boosting Full-graph Inference of Graph Neural Network over Huge Graphs. (arXiv:2307.00228v1 [cs.LG])

    [http://arxiv.org/abs/2307.00228](http://arxiv.org/abs/2307.00228)

    InferTurbo是一种可扩展系统，用于处理工业场景中大规模图上的图神经网络推断任务。它采用类似GAS模式的计算范式和数据流描述方法，并通过迭代方式进行计算。

    

    图神经网络（GNN）的推断任务在工业场景中面临着挑战，特别是在巨大的图上，存在可扩展性、不一致性和冗余计算等问题。为了解决这些挑战，我们提出了一种名为InferTurbo的可扩展系统，用于提升工业场景中的GNN推断任务。受“像顶点一样思考”的哲学启发，我们提出了一种类似GAS（Gather-Apply-Scatter）模式的计算范式和数据流描述方法。通过迭代方式进行GNN的计算，顶点通过入边收集消息，通过传递相关的GNN层和这些消息来更新状态信息，并通过出边将更新后的信息发送给其他顶点。按照这种模式，InferTurbo可以使用替代后端（如批处理）构建。

    GNN inference is a non-trivial task, especially in industrial scenarios with giant graphs, given three main challenges, i.e., scalability tailored for full-graph inference on huge graphs, inconsistency caused by stochastic acceleration strategies (e.g., sampling), and the serious redundant computation issue. To address the above challenges, we propose a scalable system named InferTurbo to boost the GNN inference tasks in industrial scenarios. Inspired by the philosophy of ``think-like-a-vertex", a GAS-like (Gather-Apply-Scatter) schema is proposed to describe the computation paradigm and data flow of GNN inference. The computation of GNNs is expressed in an iteration manner, in which a vertex would gather messages via in-edges and update its state information by forwarding an associated layer of GNNs with those messages and then send the updated information to other vertexes via out-edges. Following the schema, the proposed InferTurbo can be built with alternative backends (e.g., batch
    
[^21]: 利用马尔可夫毯交集进行因果结构学习

    Causal Structure Learning by Using Intersection of Markov Blankets. (arXiv:2307.00227v1 [stat.ML])

    [http://arxiv.org/abs/2307.00227](http://arxiv.org/abs/2307.00227)

    本文提出了一种新颖的因果结构学习算法，该算法利用马尔可夫毯交集，并结合了贝叶斯网络和结构因果模型的特性。此外，还提出了EEMBI-PC，它是EEMBI的扩展版本，将PC算法的最后一步集成到EEMBI中。

    

    在本文中，我们介绍了一种新颖的因果结构学习算法，称为内源和外源马尔可夫毯交集（EEMBI），它结合了贝叶斯网络和结构因果模型（SCM）的特性。此外，我们提出了EEMBI的扩展版本，即EEMBI-PC，它将PC算法的最后一步集成到EEMBI中。

    In this paper, we introduce a novel causal structure learning algorithm called Endogenous and Exogenous Markov Blankets Intersection (EEMBI), which combines the properties of Bayesian networks and Structural Causal Models (SCM). Furthermore, we propose an extended version of EEMBI, namely EEMBI-PC, which integrates the last step of the PC algorithm into EEMBI.
    
[^22]: S-Omninet: 结构化数据增强的通用多模态学习架构

    S-Omninet: Structured Data Enhanced Universal Multimodal Learning Architecture. (arXiv:2307.00226v1 [cs.CV])

    [http://arxiv.org/abs/2307.00226](http://arxiv.org/abs/2307.00226)

    S-Omninet是一种结构化数据增强的通用多模态学习架构，通过引入交叉缓存注意力、整合视觉输入的块嵌入和支持结构化数据，能够同时处理多个模态和任务，有效地学习来自各个维度的结构化数据和非结构化数据。

    

    多模态多任务学习近年来越来越受到关注。单模态模型在各个领域的多个任务上取得了惊人的进展。通过整合来自多个模态的数据，多模态学习提供了进一步改进的机会。许多方法被提出来处理特定类型的多模态数据，如视觉和语言数据。其中一些方法设计用于同时处理多个模态和任务。在这项工作中，我们通过引入交叉缓存注意力、整合视觉输入的块嵌入和支持结构化数据来扩展和改进Omninet，一种能够同时处理多个模态和任务的架构。提出的结构化数据增强的Omninet (S-Omninet) 是一种通用模型，通过交叉缓存注意力能够有效地从各个维度的结构化数据和非结构化数据中学习。

    Multimodal multitask learning has attracted an increasing interest in recent years. Singlemodal models have been advancing rapidly and have achieved astonishing results on various tasks across multiple domains. Multimodal learning offers opportunities for further improvements by integrating data from multiple modalities. Many methods are proposed to learn on a specific type of multimodal data, such as vision and language data. A few of them are designed to handle several modalities and tasks at a time. In this work, we extend and improve Omninet, an architecture that is capable of handling multiple modalities and tasks at a time, by introducing cross-cache attention, integrating patch embeddings for vision inputs, and supporting structured data. The proposed Structured-data-enhanced Omninet (S-Omninet) is a universal model that is capable of learning from structured data of various dimensions effectively with unstructured data through cross-cache attention, which enables interactions a
    
[^23]: 在连续图扩散函数空间中重新思考和重新设计图神经网络

    Re-Think and Re-Design Graph Neural Networks in Spaces of Continuous Graph Diffusion Functionals. (arXiv:2307.00222v1 [cs.LG])

    [http://arxiv.org/abs/2307.00222](http://arxiv.org/abs/2307.00222)

    本论文重新思考和重新设计图神经网络，通过在连续图扩散函数空间中引入变分分析的归纳偏置，解决了GNN在捕捉长程依赖和全局模式上的局限性，并通过引入总变差（TV）来解决过度平滑问题，从而构建了具有数学保证的离散深度模型。

    

    图神经网络（GNN）被广泛应用于社交网络和生物系统等领域。然而，GNN的局部性假设将信息交换限制在相邻节点之间，制约了其捕捉图中长程依赖和全局模式的能力。为了解决这个问题，我们提出了一种基于变分分析的新归纳偏置，受到了Brachistochrone问题的启发。我们的框架建立了离散GNN模型和连续扩散函数之间的映射关系。这使得能够在连续域设计应用特定的目标函数，并构建具有数学保证的离散深度模型。为了解决GNN中的过度平滑问题，我们分析了现有的逐层图嵌入模型，并发现它们等价于图梯度的l2-范数积分泛函，导致过度平滑。类似于图像去噪中的边保持滤波器，我们引入了总变差（TV）来解决这个问题。

    Graph neural networks (GNNs) are widely used in domains like social networks and biological systems. However, the locality assumption of GNNs, which limits information exchange to neighboring nodes, hampers their ability to capture long-range dependencies and global patterns in graphs. To address this, we propose a new inductive bias based on variational analysis, drawing inspiration from the Brachistochrone problem. Our framework establishes a mapping between discrete GNN models and continuous diffusion functionals. This enables the design of application-specific objective functions in the continuous domain and the construction of discrete deep models with mathematical guarantees. To tackle over-smoothing in GNNs, we analyze the existing layer-by-layer graph embedding models and identify that they are equivalent to l2-norm integral functionals of graph gradients, which cause over-smoothing. Similar to edge-preserving filters in image denoising, we introduce total variation (TV) to ali
    
[^24]: 一种基于神经随机微分方程的函数实现的构造性方法

    A Constructive Approach to Function Realization by Neural Stochastic Differential Equations. (arXiv:2307.00215v1 [math.OC])

    [http://arxiv.org/abs/2307.00215](http://arxiv.org/abs/2307.00215)

    本文采用了一种构造性方法，通过限制系统动力学来刻画可以实现的函数类，从而避免了高复杂度的控制。实现方法包括神经随机微分方程、确定性动力系统和输出映射的级联连接。这些结果有助于提高函数逼近算法的实际可行性。

    

    通过神经动力系统进行函数逼近的问题通常采用自上而下的方法：用给定结构的复杂模型可以将任何连续函数逼近到任意精度。然而，这可能导致在应用中不实际的高复杂度控制。本文采用相反的构造性方法：我们对系统动力学施加各种结构限制，从而刻画了可以通过这种系统实现的函数类。系统实现为神经随机微分方程（Neural SDE）、确定性动力系统和一个输出映射的级联连接。采用概率和几何（李论）方法来刻画这些系统实现的函数类。

    The problem of function approximation by neural dynamical systems has typically been approached in a top-down manner: Any continuous function can be approximated to an arbitrary accuracy by a sufficiently complex model with a given architecture. This can lead to high-complexity controls which are impractical in applications. In this paper, we take the opposite, constructive approach: We impose various structural restrictions on system dynamics and consequently characterize the class of functions that can be realized by such a system. The systems are implemented as a cascade interconnection of a neural stochastic differential equation (Neural SDE), a deterministic dynamical system, and a readout map. Both probabilistic and geometric (Lie-theoretic) methods are used to characterize the classes of functions realized by such systems.
    
[^25]: 更少获取更多：紧凑卷积转换器在有限数据条件下实现稳健的医学图像分类

    More for Less: Compact Convolutional Transformers Enable Robust Medical Image Classification with Limited Data. (arXiv:2307.00213v1 [cs.CV])

    [http://arxiv.org/abs/2307.00213](http://arxiv.org/abs/2307.00213)

    本研究调查了紧凑卷积转换器（CCT）在有限数据条件下进行稳健的医学图像分类的有效性，通过将转换器与卷积层结合，CCT表现出了在规模适中的数据集上高准确性。

    

    转换器是一种非常强大的工具，可以在各个领域中应用于不同的任务，从文本生成到图像描述。然而，转换器通常需要大量的训练数据，在生物医学领域中，获取高质量标记数据常常具有挑战性且昂贵。本研究调查了紧凑卷积转换器（CCT）在有限数据条件下进行稳健的医学图像分类的有效性，解决了传统Vision Transformers面临的一个关键问题，即它们对大型数据集的要求。作为转换器和卷积层的混合体，CCT在规模适中的数据集上展现了很高的准确性。我们使用了一个基准数据集，其中包含了八种不同细胞类型的外周血细胞图像，每种细胞类型有约2000个低分辨率（28x28x3像素）样本。尽管数据集的大小比通常与Vision Transformers使用的数据集要小，我们达到了可观的92.49%的分类准确度和mi指标。

    Transformers are very powerful tools for a variety of tasks across domains, from text generation to image captioning. However, transformers require substantial amounts of training data, which is often a challenge in biomedical settings, where high quality labeled data can be challenging or expensive to obtain. This study investigates the efficacy of Compact Convolutional Transformers (CCT) for robust medical image classification with limited data, addressing a key issue faced by conventional Vision Transformers - their requirement for large datasets. A hybrid of transformers and convolutional layers, CCTs demonstrate high accuracy on modestly sized datasets. We employed a benchmark dataset of peripheral blood cell images of eight distinct cell types, each represented by approximately 2,000 low-resolution (28x28x3 pixel) samples. Despite the dataset size being smaller than those typically used with Vision Transformers, we achieved a commendable classification accuracy of 92.49% and a mi
    
[^26]: 一种可解释的增量随机权重神经网络及其应用的构造算法

    An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])

    [http://arxiv.org/abs/2307.00185](http://arxiv.org/abs/2307.00185)

    本文提出了一种可解释的增量随机权重神经网络的构造算法，通过几何信息约束和节点池策略解决了难以解释隐藏参数与残差误差之间关系的问题。这种算法在大规模数据建模任务中表现出了良好的性能。

    

    增量随机权重神经网络(IRWNNs)由于易于实现和快速学习而受到关注。然而，IRWNNs的一个显著缺点是难以解释隐藏参数（节点）与残差误差（模型性能）之间的关系。为了解决这个问题，本文提出了一个具有几何信息约束的可解释的构造算法(ICA)。首先，基于隐藏参数与残差误差之间的几何关系，提出了一个可解释的几何信息约束来随机分配隐藏参数。同时，采用节点池策略获取更有利于收敛的隐藏参数。此外，证明了ICA的通用逼近性质。最后，提出了ICA的轻量级版本用于大规模数据建模任务。在六个基准数据集上的实验结果表明了该算法的有效性。

    Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
    
[^27]: 语言模型仍然没有谎言探测器：探究经验和概念上的障碍

    Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks. (arXiv:2307.00175v1 [cs.CL])

    [http://arxiv.org/abs/2307.00175](http://arxiv.org/abs/2307.00175)

    本文讨论了大型语言模型是否具有信念以及如何衡量它们的问题，并通过实证结果和对最新论证的分析，指出现在仍然没有针对大型语言模型的谎言探测器。

    

    本文讨论了大型语言模型（LLM）是否具有信念以及如何衡量它们的问题。首先，我们评估了Azaria和Mitchell（2023）以及Burns等人（2022）提出的两种现有方法，结果表明这些方法在基本方面无法推广。随后我们认为，即使LLM具有信念，这些方法也不太可能在概念上成功。因此，现在仍然没有针对LLM的谎言探测器。在描述了我们的实证结果后，我们退后一步，思考在首次中我们是否应该期待LLM具有类似信念的东西。我们考虑了一些旨在证明LLM不能有信念的最新论证，展示了这些论证是误导性的。我们对围绕LLM中信念的问题的问题提出了更有成效的框架，并强调了该问题的经验性质。最后，我们提出了一些未来工作的具体路径建议。

    We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.
    
[^28]: 整数线性规划推理手册

    The Integer Linear Programming Inference Cookbook. (arXiv:2307.00171v1 [cs.AI])

    [http://arxiv.org/abs/2307.00171](http://arxiv.org/abs/2307.00171)

    本论文介绍了一个整数线性规划推理手册，用于将推理问题转化为整数线性规划实例。通过一系列技巧的演示，帮助读者理解如何应用这些方法。论文最后提供了两个示例以说明这些技巧的使用。

    

    多年来，整数线性规划已被用于模拟自然语言处理问题中的推理。本调查旨在指导读者将新的推理问题框架化为整数线性规划的实例，并以一系列的技巧进行组织。最后，我们将通过两个实例来说明这些技巧的使用。

    Over the years, integer linear programs have been employed to model inference in many natural language processing problems. This survey is meant to guide the reader through the process of framing a new inference problem as an instance of an integer linear program and is structured as a collection of recipes. At the end, we will see two worked examples to illustrate the use of these recipes.
    
[^29]: VoxWatch: VoxCeleb上的开放式演讲者识别基准。 (arXiv:2307.00169v1 [eess.AS])

    VoxWatch: An open-set speaker recognition benchmark on VoxCeleb. (arXiv:2307.00169v1 [eess.AS])

    [http://arxiv.org/abs/2307.00169](http://arxiv.org/abs/2307.00169)

    本研究提出了一个针对VoxCeleb的开放式演讲者识别基准，并探讨了处理观察名单大小对检测性能影响的技术。

    

    尽管开放式演讲者识别（OSI）在欺诈预防等广泛实际应用中具有广泛的应用，但与演讲者验证（SV）相比，演讲者识别社区对其关注较少。 OSI涉及确定测试语音样本是否属于一组预先注册的个体（内部集）的演讲者，或者是否来自一个外部集演讲者。除了与语音变异相关的典型挑战外，OSI还容易出现“误报问题”；随着内部集演讲者人口（也称为观察名单）的增加，外部集分数变大，导致误报率增加。这对金融机构和边境安全等应用尤为具有挑战性，其中观察名单的大小通常为几千个演讲者。因此，系统地量化误报问题并开发减轻观察名单大小对检测性能影响的技术是重要的。以前的研究

    Despite its broad practical applications such as in fraud prevention, open-set speaker identification (OSI) has received less attention in the speaker recognition community compared to speaker verification (SV). OSI deals with determining if a test speech sample belongs to a speaker from a set of pre-enrolled individuals (in-set) or if it is from an out-of-set speaker. In addition to the typical challenges associated with speech variability, OSI is prone to the "false-alarm problem"; as the size of the in-set speaker population (a.k.a watchlist) grows, the out-of-set scores become larger, leading to increased false alarm rates. This is in particular challenging for applications in financial institutions and border security where the watchlist size is typically of the order of several thousand speakers. Therefore, it is important to systematically quantify the false-alarm problem, and develop techniques that alleviate the impact of watchlist size on detection performance. Prior studies 
    
[^30]: U-Calibration: 针对未知代理的预测

    U-Calibration: Forecasting for an Unknown Agent. (arXiv:2307.00168v1 [cs.LG])

    [http://arxiv.org/abs/2307.00168](http://arxiv.org/abs/2307.00168)

    研究者们提出了一种针对未知代理的预测问题的新度量指标U-校准，该指标能够保证所有代理具有次线性的悔恨值。

    

    我们考虑评估针对由理性代理消费预测并根据预测采取行动的二元事件的预测问题，但是对于预测者来说理性代理的效用是未知的。我们表明，优化单一评分规则（例如Brier得分）的预测无法保证对于所有可能的代理都具有低悔恨值。相反，良好校准的预测保证所有代理都具有次线性的悔恨值。然而，此处校准并不是必要条件（对于校准不良的预测，仍可能为所有可能的代理提供良好的悔恨值保证），而校准的预测程序与针对单一评分规则的预测程序相比具有更差的收敛速度。受此启发，我们提出了一种新的用于评估预测的度量指标，称为U-校准，即在任何有界评分规则下评估时，预测序列的最大悔恨值。我们表明，次线性的U-校准误差是必要的。

    We consider the problem of evaluating forecasts of binary events whose predictions are consumed by rational agents who take an action in response to a prediction, but whose utility is unknown to the forecaster. We show that optimizing forecasts for a single scoring rule (e.g., the Brier score) cannot guarantee low regret for all possible agents. In contrast, forecasts that are well-calibrated guarantee that all agents incur sublinear regret. However, calibration is not a necessary criterion here (it is possible for miscalibrated forecasts to provide good regret guarantees for all possible agents), and calibrated forecasting procedures have provably worse convergence rates than forecasting procedures targeting a single scoring rule.  Motivated by this, we present a new metric for evaluating forecasts that we call U-calibration, equal to the maximal regret of the sequence of forecasts when evaluated under any bounded scoring rule. We show that sublinear U-calibration error is a necessary
    
[^31]: 自我监督的语音模型对单词的了解程度是什么？

    What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])

    [http://arxiv.org/abs/2307.00162](http://arxiv.org/abs/2307.00162)

    通过对自我监督的语音模型进行分析，发现这些模型在不同层中编码了不同的语言信息，也学习了类似音素的子词单元。与单词相关的信息主要在中间的模型层中，同时一些低级信息在更高的层中也得以保留。

    

    在过去几年中，许多自我监督的语音模型（S3Ms）被引入，为各种语音任务提供了性能和数据效率的改进。有证据表明，不同的S3Ms在不同的层中编码语言信息，而且一些S3Ms似乎学习了类似于音素的子词单元。然而，这些模型捕捉更大的语言单元（如单词）的程度以及单词相关信息的编码位置仍然不清楚。在这项研究中，我们对来自三个S3Ms的不同层的单词片段表示进行了多种分析：wav2vec2、HuBERT和WavLM。我们利用规范相关分析（CCA），一种轻量级的分析工具，来衡量这些表示与单词级语言属性之间的相似性。我们发现最大的单词级语言内容往往出现在中间的模型层，而一些低级信息（如发音）也在更高的层中保留。

    Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
    
[^32]: FFPDG: 快速、公平和私密的数据生成

    FFPDG: Fast, Fair and Private Data Generation. (arXiv:2307.00161v1 [cs.LG])

    [http://arxiv.org/abs/2307.00161](http://arxiv.org/abs/2307.00161)

    提出了一种快速、公平、灵活和私密的数据生成方法，可以在真实应用场景下表现良好。

    

    生成建模经常被用于合成数据生成。公平性和隐私权是合成数据面临的两个大问题。尽管最近的基于GAN的方法在保护隐私方面表现良好，但生成的数据可能更具偏见。与此同时，这些方法需要高计算资源。在这项工作中，我们设计了一种快速、公平、灵活和私密的数据生成方法。我们理论上和实践上证明了我们方法的有效性。我们展示了通过该方法生成的数据训练的模型在真实应用场景下（推断阶段）表现良好。

    Generative modeling has been used frequently in synthetic data generation. Fairness and privacy are two big concerns for synthetic data. Although Recent GAN [\cite{goodfellow2014generative}] based methods show good results in preserving privacy, the generated data may be more biased. At the same time, these methods require high computation resources. In this work, we design a fast, fair, flexible and private data generation method. We show the effectiveness of our method theoretically and empirically. We show that models trained on data generated by the proposed method can perform well (in inference stage) on real application scenarios.
    
[^33]: 平衡方法对不平衡分类问题中模型行为的影响

    The Effect of Balancing Methods on Model Behavior in Imbalanced Classification Problems. (arXiv:2307.00157v1 [cs.LG])

    [http://arxiv.org/abs/2307.00157](http://arxiv.org/abs/2307.00157)

    平衡方法对不平衡分类问题中模型行为产生显著影响。这些发现强调了平衡分析在模型训练中的重要性。

    

    不平衡数据对分类问题构成了重要的挑战，因为模型性能受到对少数类别学习不足的影响。平衡方法通常被用来解决这个问题。然而，这些技术可能会导致过拟合或者信息丢失等问题。本研究探讨了平衡方法更具挑战性的方面——它们对模型行为的影响。为了捕捉这些变化，本研究使用了可解释人工智能工具来比较在平衡前后训练的模型。除了变量重要性方法外，本研究还使用了部分依赖轮廓和累积局部影响技术。进行了真实和模拟数据集的测试，并开发了一个开源Python包edgaro来方便进行这种分析。所得到的结果显示，由于平衡方法的影响，模型行为发生了显著变化，可能会使模型对平衡分布产生偏见。这些发现证实了平衡分析对模型行为有重要影响。

    Imbalanced data poses a significant challenge in classification as model performance is affected by insufficient learning from minority classes. Balancing methods are often used to address this problem. However, such techniques can lead to problems such as overfitting or loss of information. This study addresses a more challenging aspect of balancing methods - their impact on model behavior. To capture these changes, Explainable Artificial Intelligence tools are used to compare models trained on datasets before and after balancing. In addition to the variable importance method, this study uses the partial dependence profile and accumulated local effects techniques. Real and simulated datasets are tested, and an open-source Python package edgaro is developed to facilitate this analysis. The results obtained show significant changes in model behavior due to balancing methods, which can lead to biased models toward a balanced distribution. These findings confirm that balancing analysis sh
    
[^34]: Stitched ViTs是灵活的视觉骨干网络

    Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])

    [http://arxiv.org/abs/2307.00154](http://arxiv.org/abs/2307.00154)

    本研究通过拼接预训练模型族群，提出了SN-Netv2，它是一个灵活的视觉骨干网络框架，可以在运行时实现多样性的性能和效率权衡。

    

    大型预训练的普通视觉Transformer（ViTs）已成为许多下游任务的主力。然而，利用现成的ViTs的现有工作在训练和部署方面效率低下，因为采用不同大小的ViTs需要单独训练，并受到固定的性能-效率权衡的限制。在本文中，我们受到可拼接神经网络的启发，这是一个通过拼接预训练模型族群来快速生成涵盖丰富子网络的单一模型的新框架，支持在运行时的多样性性能-效率权衡。在此基础上，我们引入了SN-Netv2，这是一个系统改进的模型拼接框架，用于促进下游任务的适应。具体而言，我们首先提出了一个双向拼接方案来扩大拼接空间。然后，我们设计了一个考虑空间中底层FLOPs分布的资源受限采样策略，以改善采样质量。最后，我们对SN-Netv2进行了细微调整来进一步提高性能和效率。

    Large pretrained plain vision Transformers (ViTs) have been the workhorse for many downstream tasks. However, existing works utilizing off-the-shelf ViTs are inefficient in terms of training and deployment, because adopting ViTs with individual sizes requires separate training and is restricted by fixed performance-efficiency trade-offs. In this paper, we are inspired by stitchable neural networks, which is a new framework that cheaply produces a single model that covers rich subnetworks by stitching pretrained model families, supporting diverse performance-efficiency trade-offs at runtime. Building upon this foundation, we introduce SN-Netv2, a systematically improved model stitching framework to facilitate downstream task adaptation. Specifically, we first propose a Two-way stitching scheme to enlarge the stitching space. We then design a resource-constrained sampling strategy that takes into account the underlying FLOPs distributions in the space for improved sampling. Finally, we o
    
[^35]: 可控的CAD模型生成的分层神经编码

    Hierarchical Neural Coding for Controllable CAD Model Generation. (arXiv:2307.00149v1 [cs.CV])

    [http://arxiv.org/abs/2307.00149](http://arxiv.org/abs/2307.00149)

    本文提出了一种可控的CAD模型生成方法，通过分层神经编码和代码树来表示设计概念和控制生成过程，并在传统任务和条件生成任务上展示了优越性能。

    

    本文提出了一种新颖的计算机辅助设计（CAD）生成模型，它通过将CAD模型的高级设计概念表示为三层神经编码的层次树，从全局部件排列到局部曲线几何，同时通过使用代码树指定目标设计来控制CAD模型的生成或完成。具体地说，一种新颖的带有“掩码跳跃连接”的矢量量化VAE变体将设计变化提取为三个层次的神经代码簿。两阶段级联自回归变压器学习从不完整的CAD模型生成代码树，然后根据预期的设计完成CAD模型。大量实验证明了在传统任务（如随机生成）上的卓越性能，同时在条件生成任务上实现了新颖的交互能力。代码可在https://github.com/samxuxiang/hnc-cad找到。

    This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a three-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target design using a code tree. Concretely, a novel variant of a vector quantized VAE with "masked skip connection" extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the intended design. Extensive experiments demonstrate superior performance on conventional tasks such as random generation while enabling novel interaction capabilities on conditional generation tasks. The code is available at https://github.com/samxuxiang/hnc-cad.
    
[^36]: 遵守法律并遵循流程：梯度流的守恒定律

    Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows. (arXiv:2307.00144v1 [cs.LG])

    [http://arxiv.org/abs/2307.00144](http://arxiv.org/abs/2307.00144)

    本文通过定义和研究梯度流中的守恒定律，以及在模型的雅可比矩阵生成的李代数上进行有限维代数运算，揭示了超参数调节的模型保留了一些优化初始化的特性，这可能解释了训练模型具有良好泛化特性的原因。

    

    理解梯度下降动力学的几何特性是解密非常大的机器学习模型最近成功的关键因素。一个引人注目的观察是，超参数调节的模型保留了一些优化初始化的特性。这种“隐式偏差”被认为是训练模型具有有利特性并能解释其良好泛化特性的原因。本文的目的有三个。首先，我们严格介绍了“守恒定律”的定义和基本性质，这些守恒定律是在给定模型（例如具有给定架构的ReLU网络）的梯度流中独立保持的最大量。不论使用任何训练数据和任何损失函数。然后，我们解释如何通过对模型的雅可比矩阵生成的李代数进行有限维代数运算，找到这些数量的确切数量。最后，我们提供了在SageMath中实现的算法。

    Understanding the geometric properties of gradient descent dynamics is a key ingredient in deciphering the recent success of very large machine learning models. A striking observation is that trained over-parameterized models retain some properties of the optimization initialization. This "implicit bias" is believed to be responsible for some favorable properties of the trained models and could explain their good generalization properties. The purpose of this article is threefold. First, we rigorously expose the definition and basic properties of "conservation laws", which are maximal sets of independent quantities conserved during gradient flows of a given model (e.g. of a ReLU network with a given architecture) with any training data and any loss. Then we explain how to find the exact number of these quantities by performing finite-dimensional algebraic manipulations on the Lie algebra generated by the Jacobian of the model. Finally, we provide algorithms (implemented in SageMath) to
    
[^37]: BuildingsBench：一个包含900K座建筑物的大规模数据集和短期负荷预测基准

    BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting. (arXiv:2307.00142v1 [cs.LG])

    [http://arxiv.org/abs/2307.00142](http://arxiv.org/abs/2307.00142)

    本文提出了BuildingsBench，这是一个包含900K座建筑物的大规模数据集，旨在解决短期负荷预测中数据集不足的问题。通过该数据集，我们进行了两个任务的基准评估，并发现经过合成预训练的模型具有良好的泛化能力。

    

    针对短期负荷预测(STLF)中缺乏开放、大规模、高建筑多样性数据集的问题，本文提出了BuildingsBench，包括1)包含900K个模拟建筑的大规模数据集Buildings-900K，以模拟美国的建筑库存，以及2)拥有来自7个开放数据集的超过1900个真实住宅和商业建筑物的评估平台。BuildingsBench为两个未被充分探索的任务提供了基准：零-shot STLF，其中预训练模型在未见过的建筑上进行评估而无需微调；以及迁移学习，其中预训练模型在目标建筑上进行微调。本次基准分析的主要发现是，经过合成预训练的模型意外地具有良好的泛化能力。

    Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-finetune paradigm for STLF. To help address this, we present BuildingsBench, which consists of 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock, and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly w
    
[^38]: 通过凸优化实现风险敏感的无演员策略

    Risk-sensitive Actor-free Policy via Convex Optimization. (arXiv:2307.00141v1 [cs.LG])

    [http://arxiv.org/abs/2307.00141](http://arxiv.org/abs/2307.00141)

    本文提出了一种通过凸优化实现风险敏感的无演员策略，利用条件风险值作为目标函数，并使用凸神经网络模型来识别全局最优动作，实验结果表明该方法在保持有效的风险控制方面很有效。

    

    传统的强化学习方法在优化代理时没有考虑安全性，可能导致意想不到的后果。本文提出了一种基于条件风险值的风险敏感的最优无演员策略。风险敏感的目标函数使用了一个输入凸神经网络来建模，以确保对动作的凸性，并通过简单的梯度跟随方法来识别全局最优动作。实验结果表明我们的方法在保持有效的风险控制方面的有效性。

    Traditional reinforcement learning methods optimize agents without considering safety, potentially resulting in unintended consequences. In this paper, we propose an optimal actor-free policy that optimizes a risk-sensitive criterion based on the conditional value at risk. The risk-sensitive objective function is modeled using an input-convex neural network ensuring convexity with respect to the actions and enabling the identification of globally optimal actions through simple gradient-following methods. Experimental results demonstrate the efficacy of our approach in maintaining effective risk control.
    
[^39]: 图神经网络在身份效应学习中的泛化限制

    Generalization Limits of Graph Neural Networks in Identity Effects Learning. (arXiv:2307.00134v1 [cs.LG])

    [http://arxiv.org/abs/2307.00134](http://arxiv.org/abs/2307.00134)

    本研究在学习身份效应的背景下，分析了图神经网络在泛化属性和基本限制方面的新性质，以及在两个字母的单词案例中的具体应用。

    

    图神经网络在各种图领域的数据驱动学习中已经成为一个强有力的工具。它们通常基于消息传递机制，并且由于其与Weisfeiler-Lehman(WL)图同构测试紧密相连的直观表述而越来越受到欢迎，从表达能力上讲，它们已被证明与WL测试等价。在本研究中，我们在学习所谓的身份效应（即确定一个对象是否由两个相同的组件组成）的背景下，建立了GNN在泛化属性和基本限制方面的新性质。我们的研究是出于理解GNN在执行简单认知任务时的能力的需求，可能在计算语言学和化学领域具有潜在应用。我们分析了两个案例研究：（i）两个字母的单词，我们展示了通过随机梯度下降训练的GNN在利用正交时无法对未见字母进行泛化的情况。

    Graph Neural Networks (GNNs) have emerged as a powerful tool for data-driven learning on various graph domains. They are usually based on a message-passing mechanism and have gained increasing popularity for their intuitive formulation, which is closely linked to the Weisfeiler-Lehman (WL) test for graph isomorphism to which they have been proven equivalent in terms of expressive power. In this work, we establish new generalization properties and fundamental limits of GNNs in the context of learning so-called identity effects, i.e., the task of determining whether an object is composed of two identical components or not. Our study is motivated by the need to understand the capabilities of GNNs when performing simple cognitive tasks, with potential applications in computational linguistics and chemistry. We analyze two case studies: (i) two-letters words, for which we show that GNNs trained via stochastic gradient descent are unable to generalize to unseen letters when utilizing orthogo
    
[^40]: 机器学习在推动低温等离子体建模和模拟方面的应用

    Machine learning for advancing low-temperature plasma modeling and simulation. (arXiv:2307.00131v1 [physics.plasm-ph])

    [http://arxiv.org/abs/2307.00131](http://arxiv.org/abs/2307.00131)

    机器学习在低温等离子体建模和模拟方面的应用受到了广泛关注，它能够提供新的方法和途径，促进等离子体科学和技术的发展。

    

    机器学习在许多科学领域都产生了巨大影响，近年来，在低温等离子体建模和模拟领域也引起了极大的关注。尽管它的应用应该被谨慎评估，但机器学习和数据驱动建模的最新进展在等离子体建模和模拟的许多方面都受益匪浅。在这项综述中，我们主要追求两个目标：(a)我们主要关注低温等离子体建模和模拟的方法的现状进行综述。通过将我们的综述划分为等离子体物理学、等离子体化学、等离子体表面相互作用和等离子体过程控制四个方面，我们旨在广泛讨论文献中的相关实例。(b)我们提供了对等离子体科学和技术潜在进展的展望。我们特别阐述了可能通过从其他科学学科中的适应推动的进展。我们认为，已知的未解决问题和新的研究领域将使低温等离子体建模和模拟取得重大进展。

    Machine learning has had an enormous impact in many scientific disciplines. Also in the field of low-temperature plasma modeling and simulation it has attracted significant interest within the past years. Whereas its application should be carefully assessed in general, many aspects of plasma modeling and simulation have benefited substantially from recent developments within the field of machine learning and data-driven modeling. In this survey, we approach two main objectives: (a) We review the state-of-the-art focusing on approaches to low-temperature plasma modeling and simulation. By dividing our survey into plasma physics, plasma chemistry, plasma-surface interactions, and plasma process control, we aim to extensively discuss relevant examples from literature. (b) We provide a perspective of potential advances to plasma science and technology. We specifically elaborate on advances possibly enabled by adaptation from other scientific disciplines. We argue that not only the known un
    
[^41]: 加速非精确超梯度下降用于双层优化

    Accelerating Inexact HyperGradient Descent for Bilevel Optimization. (arXiv:2307.00126v1 [math.OC])

    [http://arxiv.org/abs/2307.00126](http://arxiv.org/abs/2307.00126)

    提出一种加速非精确超梯度下降的方法用于双层优化，可以在较低的复杂度下找到一阶和二阶稳定点，成为双层优化和凸-凹极小极大优化问题中的最新最佳状态。

    

    我们提出了一种解决一般非凸-凸双层优化问题的方法。我们的方法——"重新启动的加速超梯度下降" (RAHGD) 方法——可以找到一个 $\epsilon$-一阶稳定点，其 oracle 复杂度为 $\tilde{\mathcal{O}}(\kappa^{3.25}\epsilon^{-1.75})$，其中 $\kappa$ 是下层目标的条件数，$\epsilon$ 是期望精度。我们还提出了 RAHGD 的扰动变体，用于在相同的 oracle 复杂度下找到一个 $\big(\epsilon,\mathcal{O}(\kappa^{2.5}\sqrt{\epsilon}\,)\big)$-二阶稳定点。我们的结果在双层优化中实现了已知最好的理论保证，并且改进了现有凸-凹极小极大优化问题中找到二阶稳定点的上界复杂度，为最新的基准设置了一个新的最佳状态。我们进行了实证研究。

    We present a method for solving general nonconvex-strongly-convex bilevel optimization problems. Our method -- the \emph{Restarted Accelerated HyperGradient Descent} (\texttt{RAHGD}) method -- finds an $\epsilon$-first-order stationary point of the objective with $\tilde{\mathcal{O}}(\kappa^{3.25}\epsilon^{-1.75})$ oracle complexity, where $\kappa$ is the condition number of the lower-level objective and $\epsilon$ is the desired accuracy. We also propose a perturbed variant of \texttt{RAHGD} for finding an $\big(\epsilon,\mathcal{O}(\kappa^{2.5}\sqrt{\epsilon}\,)\big)$-second-order stationary point within the same order of oracle complexity. Our results achieve the best-known theoretical guarantees for finding stationary points in bilevel optimization and also improve upon the existing upper complexity bound for finding second-order stationary points in nonconvex-strongly-concave minimax optimization problems, setting a new state-of-the-art benchmark. Empirical studies are conducted t
    
[^42]: RObotic MAnipulation Network（ROMAN）--混合层次学习解决复杂的连续任务

    RObotic MAnipulation Network (ROMAN) -- Hybrid Hierarchical Learning for Solving Complex Sequential Tasks. (arXiv:2307.00125v1 [cs.RO])

    [http://arxiv.org/abs/2307.00125](http://arxiv.org/abs/2307.00125)

    RObotic MAnipulation Network（ROMAN）通过混合层次学习框架解决复杂的连续操作任务，实现了任务多样性和鲁棒的失败恢复。

    

    在实体人工智能中，解决长序列任务面临着重大挑战。使机器人系统能够执行多样化的连续任务，并具备广泛的操作技能是一个活跃的研究领域。在本文中，我们提出了一种混合层次学习框架，即ROBOTIC Manipulation Network（ROMAN），以解决机器人操作中的多个复杂任务的长时间任务。ROMAN通过集成行为克隆、模仿学习和强化学习来实现任务的多样性以及鲁棒的失败恢复。它包括一个中央操作网络，协调一组不同的神经网络，每个网络专注于不同的可重组子任务，生成它们在复杂的长时间操作任务中的正确连续动作。实验结果表明，通过协调和激活这些专门的操作专家，ROMAN生成了正确的顺序激活。

    Solving long sequential tasks poses a significant challenge in embodied artificial intelligence. Enabling a robotic system to perform diverse sequential tasks with a broad range of manipulation skills is an active area of research. In this work, we present a Hybrid Hierarchical Learning framework, the Robotic Manipulation Network (ROMAN), to address the challenge of solving multiple complex tasks over long time horizons in robotic manipulation. ROMAN achieves task versatility and robust failure recovery by integrating behavioural cloning, imitation learning, and reinforcement learning. It consists of a central manipulation network that coordinates an ensemble of various neural networks, each specialising in distinct re-combinable sub-tasks to generate their correct in-sequence actions for solving complex long-horizon manipulation tasks. Experimental results show that by orchestrating and activating these specialised manipulation experts, ROMAN generates correct sequential activations f
    
[^43]: 人类用户如何通过重复交互教授持续学习机器人？

    How Do Human Users Teach a Continual Learning Robot in Repeated Interactions?. (arXiv:2307.00123v1 [cs.RO])

    [http://arxiv.org/abs/2307.00123](http://arxiv.org/abs/2307.00123)

    本文采用以人类为中心的方法研究持续学习，通过对40名参与者与持续学习机器人的长期交互进行定性和定量分析，发现人类的教学风格存在显著变化。

    

    持续学习已经成为近年来机器学习和人机交互领域的重要研究方向，旨在让机器人能够在与人类的长期交互中不断学习其环境。然而，大多数持续学习研究都是以机器人为中心，旨在开发能够快速学习静态数据集中新信息的持续学习算法。本文采用以人类为中心的方法来研究持续学习，以了解人类如何在长期时间内教授持续学习机器人，以及他们的教学风格是否存在变化。我们进行了一项与40名参与者的面对面研究，他们与一个持续学习机器人进行了200次交互。本次研究采用了两种不同的持续学习模型，部署在一个Fetch移动操作机器人上。对研究中收集的数据进行了详尽的定性和定量分析，结果显示存在显著的变化。

    Continual learning (CL) has emerged as an important avenue of research in recent years, at the intersection of Machine Learning (ML) and Human-Robot Interaction (HRI), to allow robots to continually learn in their environments over long-term interactions with humans. Most research in continual learning, however, has been robot-centered to develop continual learning algorithms that can quickly learn new information on static datasets. In this paper, we take a human-centered approach to continual learning, to understand how humans teach continual learning robots over the long term and if there are variations in their teaching styles. We conducted an in-person study with 40 participants that interacted with a continual learning robot in 200 sessions. In this between-participant study, we used two different CL models deployed on a Fetch mobile manipulator robot. An extensive qualitative and quantitative analysis of the data collected in the study shows that there is significant variation a
    
[^44]: 指导目标表征：一种半监督语言接口控制机器人的方法

    Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control. (arXiv:2307.00117v1 [cs.RO])

    [http://arxiv.org/abs/2307.00117](http://arxiv.org/abs/2307.00117)

    本文介绍了一种半监督语言接口控制机器人的方法，通过联合图像和目标信息的策略以及少量的语言数据实现了在真实世界中的稳健性能。

    

    我们的目标是使机器人能够按照自然语言指令行动，例如“将毛巾放在微波炉旁边”。但是获取大量带有语言指令标签的标注数据非常困难。相比之下，获取对图像目标作出响应的策略要容易得多，因为任何自主尝试或演示都可以在事后用最终状态作为目标进行标记。在这项工作中，我们提出了一种方法，只利用少量的语言数据，利用联合图像和目标信息的策略来处理语言接口。以前的工作在使用视觉-语言模型或联合训练语言-目标-条件策略方面取得了一些进展，但迄今为止，这两种方法都没有有效地扩展到实际机器人任务中，而不需要大量的人工注释。我们的方法通过从标记数据中学习一种将语言与目标图像对齐的嵌入，实现了在真实世界中的稳健性能。

    Our goal is for robots to follow natural language instructions like "put the towel next to the microwave." But getting large amounts of labeled data, i.e. data that contains demonstrations of tasks labeled with the language instruction, is prohibitive. In contrast, obtaining policies that respond to image goals is much easier, because any autonomous trial or demonstration can be labeled in hindsight with its final state as the goal. In this work, we contribute a method that taps into joint image- and goal- conditioned policies with language using only a small amount of language data. Prior work has made progress on this using vision-language models or by jointly training language-goal-conditioned policies, but so far neither method has scaled effectively to real-world robot tasks without significant human annotation. Our method achieves robust performance in the real world by learning an embedding from the labeled data that aligns language not to the goal image, but rather to the desir
    
[^45]: Ticket-BERT:使用语言模型为事件管理票据进行标注

    Ticket-BERT: Labeling Incident Management Tickets with Language Models. (arXiv:2307.00108v1 [cs.CL])

    [http://arxiv.org/abs/2307.00108](http://arxiv.org/abs/2307.00108)

    Ticket-BERT是一个使用语言模型为事件管理票据进行标注的方法，在解决复杂的票据数据和时间敏感性问题方面具有优势。

    

    对于解决优先级事件票据的一个重要方面是高效地使用精细分类来标注这些票据。然而，票据数据通常很复杂，给现代机器学习方法带来了几个独特的挑战：（1）票据既可以由预定义算法的机器生成，也可以由具有不同协议的具有领域专业知识的工程师更新和创建，（2）票据频繁进行修订，通过修改全部或部分票据描述来更新票据状态，（3）票据标注是时间敏感的，需要根据软件和硬件改进的快速生命周期进行知识更新和新的标签。为了解决这些问题，我们介绍了Ticket-BERT，它使用我们提出的票据数据集训练了一个简单但健壮的语言模型来为票据进行标注。实验证明，Ticket-BERT在Azure认知服务上优于基线和最先进的文本分类器。我们进一步将Ticket-BERT封装到一个积极的le...

    An essential aspect of prioritizing incident tickets for resolution is efficiently labeling tickets with fine-grained categories. However, ticket data is often complex and poses several unique challenges for modern machine learning methods: (1) tickets are created and updated either by machines with pre-defined algorithms or by engineers with domain expertise that share different protocols, (2) tickets receive frequent revisions that update ticket status by modifying all or parts of ticket descriptions, and (3) ticket labeling is time-sensitive and requires knowledge updates and new labels per the rapid software and hardware improvement lifecycle. To handle these issues, we introduce Ticket- BERT which trains a simple yet robust language model for labeling tickets using our proposed ticket datasets. Experiments demonstrate the superiority of Ticket-BERT over baselines and state-of-the-art text classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT with an active le
    
[^46]: 距离函数在流场景下的规范化研究

    Distance Functions and Normalization Under Stream Scenarios. (arXiv:2307.00106v1 [cs.LG])

    [http://arxiv.org/abs/2307.00106](http://arxiv.org/abs/2307.00106)

    论文研究了在流场景中的数据规范化问题，比较了八种距离函数的准确度，并发现在没有预先了解数据流信息的情况下，使用原始数据流和Canberra距离的组合效果较好。

    

    数据规范化是建模分类系统时的重要任务。在处理数据流时，由于可能无法预先了解特征的属性（如最小/最大值），且这些属性可能会随时间变化，因此数据规范化变得尤为具有挑战性。我们比较了八种著名的距离函数在没有规范化的数据流中生成的准确度，以及考虑到接收到的第一批数据的统计信息和考虑到上一批数据的规范化。我们认为将全流程视为规范化的实验协议是不现实的，并会导致偏倚和糟糕的结果。我们的结果表明，当事先不了解数据流的信息时，使用原始数据流而不应用规范化，以及使用Canberra距离，可能是一个好的组合。

    Data normalization is an essential task when modeling a classification system. When dealing with data streams, data normalization becomes especially challenging since we may not know in advance the properties of the features, such as their minimum/maximum values, and these properties may change over time. We compare the accuracies generated by eight well-known distance functions in data streams without normalization, normalized considering the statistics of the first batch of data received, and considering the previous batch received. We argue that experimental protocols for streams that consider the full stream as normalized are unrealistic and can lead to biased and poor results. Our results indicate that using the original data stream without applying normalization, and the Canberra distance, can be a good combination when no information about the data stream is known beforehand.
    
[^47]: 通过对无人机捕捉的烟雾模式的时间分析来检测被遮挡的野火火焰

    Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems. (arXiv:2307.00104v1 [cs.CV])

    [http://arxiv.org/abs/2307.00104](http://arxiv.org/abs/2307.00104)

    本论文通过对视频序列中烟雾模式的时间分析，提出了一种用于实时检测被遮挡的野火火焰的方法，通过预测火灾位置来帮助无人机对抗森林火灾。这种方法具有独特的检测被遮挡的火灾的能力。

    

    本研究论文解决了使用仅配备RGB相机的无人机实时检测被树木、烟雾、云雾和其他自然屏障遮挡的野火（当火焰被遮挡时）的挑战。我们提出了一种新的方法，利用视频序列中烟雾模式的语义分割进行时间分析。我们的方法利用了基于深度卷积神经网络的编码器-解码器架构，采用预训练的CNN编码器和3D卷积进行解码，并使用特征的顺序叠加来利用时间变化。预测的火灾位置可以帮助无人机有效地对抗森林火灾，并准确定位火焰位置进行阻燃化学物质投放。我们将我们的方法应用于从FLAME2数据集衍生的精选数据集，该数据集包括RGB视频和IR视频以确定地面真实情况。我们提出的方法具有检测被遮挡的火灾的独特属性，并实现了一种Dice系数。

    This research paper addresses the challenge of detecting obscured wildfires (when the fire flames are covered by trees, smoke, clouds, and other natural barriers) in real-time using drones equipped only with RGB cameras. We propose a novel methodology that employs semantic segmentation based on the temporal analysis of smoke patterns in video sequences. Our approach utilizes an encoder-decoder architecture based on deep convolutional neural network architecture with a pre-trained CNN encoder and 3D convolutions for decoding while using sequential stacking of features to exploit temporal variations. The predicted fire locations can assist drones in effectively combating forest fires and pinpoint fire retardant chemical drop on exact flame locations. We applied our method to a curated dataset derived from the FLAME2 dataset that includes RGB video along with IR video to determine the ground truth. Our proposed method has a unique property of detecting obscured fire and achieves a Dice sc
    
[^48]: 通过决策建模，赎回数据科学

    Redeeming Data Science by Decision Modelling. (arXiv:2307.00088v1 [cs.LG])

    [http://arxiv.org/abs/2307.00088](http://arxiv.org/abs/2307.00088)

    本文提出了决策建模的概念，用于赎回数据科学实践并解决其基础失去的问题。决策建模将传统的机器学习模型与显式价值模型结合起来，通过六个原则核心构成，提高了决策质量。

    

    随着数据科学应用的爆炸性增长，该领域已经摆脱了其基础。本文认为，需要在对人工智能中的贝叶斯方法熟悉的领域进行新的应用研究，以借鉴人工智能技术来建立数据科学实践，并将其称为“决策建模”。本文简要回顾了建模过程，将其视为构建因果图模型，然后从流行的商业文献的角度讨论该过程的六个原则，构成了“决策质量”框架。我们认为任何成功的应用机器学习建模工作都必须包括这六个原则。我们解释了决策建模如何将传统的机器学习模型与显式价值模型结合起来。为了给出具体的例子，我们展示了如何将模型的ROC曲线与效用模型整合起来。

    With the explosion of applications of Data Science, the field is has come loose from its foundations. This article argues for a new program of applied research in areas familiar to researchers in Bayesian methods in AI that are needed to ground the practice of Data Science by borrowing from AI techniques for model formulation that we term ``Decision Modelling.'' This article briefly reviews the formulation process as building a causal graphical model, then discusses the process in terms of six principles that comprise \emph{Decision Quality}, a framework from the popular business literature. We claim that any successful applied ML modelling effort must include these six principles.  We explain how Decision Modelling combines a conventional machine learning model with an explicit value model. To give a specific example we show how this is done by integrating a model's ROC curve with a utility model.
    
[^49]: 跨案例预测过程监测：量子机器学习的候选者？

    Inter-case Predictive Process Monitoring: A candidate for Quantum Machine Learning?. (arXiv:2307.00080v1 [cs.LG])

    [http://arxiv.org/abs/2307.00080](http://arxiv.org/abs/2307.00080)

    该论文研究了跨案例预测过程监测在预测准确性上的影响，并包括了量子机器学习模型，对于处理高维特征具有优势。

    

    无论领域如何，预测正在运行的过程实例的未来行为都是决策者关注的问题，特别是当多个实例相互作用时。受机器学习研究的最新进展的推动，已经提出了几种方法来自动预测过程的下一个活动、结果或剩余时间。然而，构建具有高预测能力的模型既需要从事件日志数据中提取有意义的特征的内在知识，又需要捕捉数据中的复杂模式的模型。这项工作以跨案例预测过程监测（PPM）的最新进展为基础，全面评估了跨案例特征对预测准确性的影响。此外，它还包括量子机器学习模型，这些模型被预期在特征维度不断扩展时比经典模型具有优势。对来自BPI挑战赛的真实训练数据的评估表明，跨案例特征对预测准确性有着显著影响。

    Regardless of the domain, forecasting the future behaviour of a running process instance is a question of interest for decision makers, especially when multiple instances interact. Fostered by the recent advances in machine learning research, several methods have been proposed to predict the next activity, outcome or remaining time of a process automatically. Still, building a model with high predictive power requires both - intrinsic knowledge of how to extract meaningful features from the event log data and a model that captures complex patterns in data. This work builds upon the recent progress in inter-case Predictive Process Monitoring (PPM) and comprehensively benchmarks the impact of inter-case features on prediction accuracy. Moreover, it includes quantum machine learning models, which are expected to provide an advantage over classical models with a scaling amount of feature dimensions. The evaluation on real-world training data from the BPI challenge shows that the inter-case
    
[^50]: 数据集平衡会损害模型性能

    Dataset balancing can hurt model performance. (arXiv:2307.00079v1 [cs.LG])

    [http://arxiv.org/abs/2307.00079](http://arxiv.org/abs/2307.00079)

    数据集平衡方法在提高常见类别性能的同时，会损害稀有类别的性能，并且其效果依赖于评估集。

    

    从每个类别示例分布不均匀的训练数据进行机器学习可能会导致模型在常见类别上性能优秀，但对于稀有类别的性能不佳。AudioSet拥有527个声音事件类别，其先验分布范围非常广泛。通常通过对每个类别指标的简单平均来评估AudioSet的分类性能，这意味着稀有类别的性能与常见类别的性能同等重要。一些最近的论文利用数据集平衡技术来提高在AudioSet上的性能。然而，我们发现，尽管平衡在公开的AudioSet评估数据上改善了性能，但同时也损害了在相同条件下收集的未发布评估集的性能。通过调整平衡程度，我们证明了其好处是脆弱的，并且取决于评估集。我们也没有找到显示平衡相对于常见类别会改善稀有类别性能的证据。

    Machine learning from training data with a skewed distribution of examples per class can lead to models that favor performance on common classes at the expense of performance on rare ones. AudioSet has a very wide range of priors over its 527 sound event classes. Classification performance on AudioSet is usually evaluated by a simple average over per-class metrics, meaning that performance on rare classes is equal in importance to the performance on common ones. Several recent papers have used dataset balancing techniques to improve performance on AudioSet. We find, however, that while balancing improves performance on the public AudioSet evaluation data it simultaneously hurts performance on an unpublished evaluation set collected under the same conditions. By varying the degree of balancing, we show that its benefits are fragile and depend on the evaluation set. We also do not find evidence indicating that balancing improves rare class performance relative to common classes. We there
    
[^51]: 医疗保健中的Transformer：一项调查

    Transformers in Healthcare: A Survey. (arXiv:2307.00067v1 [cs.AI])

    [http://arxiv.org/abs/2307.00067](http://arxiv.org/abs/2307.00067)

    Transformer神经网络架构在医疗保健中的应用具有巨大潜力，可以用于医学影像分析、临床诊断、报告生成、数据重建和药物/蛋白质合成。

    

    随着人工智能在医疗保健等社会各个方面的日益渗透，Transformer神经网络架构的采用正在快速改变许多应用。Transformer是一种深度学习架构，最初是为了解决通用自然语言处理（NLP）任务而开发的，并随后在包括医疗保健在内的许多领域得到了应用。在本调查论文中，我们概述了该架构如何被应用于分析各种形式的数据，包括医学影像、结构化和非结构化的电子健康记录（EHR）、社交媒体、生理信号和生物分子序列。这些模型可以帮助临床诊断、报告生成、数据重建和药物/蛋白质合成。我们使用首选报告事项进行系统综述和meta分析（PRISMA）指南来确定相关研究。我们还讨论了使用Transformer的优点和局限性。

    With Artificial Intelligence (AI) increasingly permeating various aspects of society, including healthcare, the adoption of the Transformers neural network architecture is rapidly changing many applications. Transformer is a type of deep learning architecture initially developed to solve general-purpose Natural Language Processing (NLP) tasks and has subsequently been adapted in many fields, including healthcare. In this survey paper, we provide an overview of how this architecture has been adopted to analyze various forms of data, including medical imaging, structured and unstructured Electronic Health Records (EHR), social media, physiological signals, and biomolecular sequences. Those models could help in clinical diagnosis, report generation, data reconstruction, and drug/protein synthesis. We identified relevant studies using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We also discuss the benefits and limitations of using transformer
    
[^52]: 提高时间序列预测的可迁移性：通过分解自适应

    Improving the Transferability of Time Series Forecasting with Decomposition Adaptation. (arXiv:2307.00066v1 [cs.LG])

    [http://arxiv.org/abs/2307.00066](http://arxiv.org/abs/2307.00066)

    通过分解自适应，我们提出了一种新的转让架构SeDAN，通过对齐来自不同领域数据集的可迁移知识来改善时间序列预测的性能。

    

    基于深度学习的神经预测模型通过有效的模式挖掘和特征表示取得了显著进展。有效学习的前提是收集足够的数据。然而，在时间序列预测中很难获得足够的数据，这限制了神经预测模型的性能。为了缓解数据稀缺的限制，我们设计了一种新的转让架构Sequence Decomposition Adaptation Network（SeDAN），通过将来自跨领域数据集的可迁移知识进行对齐，以提高目标领域的预测性能。重新思考时间序列数据中特征的可迁移性，我们提出了隐式对比分解方法，将原始特征分解为包括季节性和趋势性特征在内的组成部分，这些特征更容易迁移。然后，我们为不同领域中的分解特征设计了相应的自适应方法。具体而言，对于季节性特征，我们进行联合分布适应。

    Due to effective pattern mining and feature representation, neural forecasting models based on deep learning have achieved great progress. The premise of effective learning is to collect sufficient data. However, in time series forecasting, it is difficult to obtain enough data, which limits the performance of neural forecasting models. To alleviate the data scarcity limitation, we design Sequence Decomposition Adaptation Network (SeDAN) which is a novel transfer architecture to improve forecasting performance on the target domain by aligning transferable knowledge from cross-domain datasets. Rethinking the transferability of features in time series data, we propose Implicit Contrastive Decomposition to decompose the original features into components including seasonal and trend features, which are easier to transfer. Then we design the corresponding adaptation methods for decomposed features in different domains. Specifically, for seasonal features, we perform joint distribution adapt
    
[^53]: 以脑启发设计解决人工神经网络的不足之处

    Towards Brain Inspired Design for Addressing the Shortcomings of ANNs. (arXiv:2307.00039v1 [cs.NE])

    [http://arxiv.org/abs/2307.00039](http://arxiv.org/abs/2307.00039)

    本研究通过对比人工神经网络和小脑中基于错误的神经元组织方式，发现在人工神经网络中引入个性化错误视图的神经元群体可以提高学习效率、减少不平衡数据和捷径策略的影响，从而提高泛化能力。

    

    随着对大脑功能机制的理解提高，从神经科学中获得的洞察力对于AI算法的发展值得进一步考虑。本文通过将一种基于树状结构的人工神经网络架构与最近的神经科学研究[27]进行对比，认为小脑中基于错误的神经元组织方式可能是行为和学习的几个理想特征的原因。我们随后分析了模型在不同情景下的学习行为和特点，以评估类似机制在人工神经网络中的潜在好处。我们的实证结果表明，在个性化错误视图的神经元群体可以有效地在类别不平衡和有限数据情况下进行学习，并减少受意外捷径策略的影响，从而提高泛化能力。这项工作突出了将学习方式从神经科学中转化为人工神经网络中的潜力。

    As our understanding of the mechanisms of brain function is enhanced, the value of insights gained from neuroscience to the development of AI algorithms deserves further consideration. Here, we draw parallels with an existing tree-based ANN architecture and a recent neuroscience study[27] arguing that the error-based organization of neurons in the cerebellum that share a preference for a personalized view of the entire error space, may account for several desirable features of behavior and learning. We then analyze the learning behavior and characteristics of the model under varying scenarios to gauge the potential benefits of a similar mechanism in ANN. Our empirical results suggest that having separate populations of neurons with personalized error views can enable efficient learning under class imbalance and limited data, and reduce the susceptibility to unintended shortcut strategies, leading to improved generalization. This work highlights the potential of translating the learning
    
[^54]: 霍格沃茨魔法学校中的魔药发展的机器学习方法

    Machine learning for potion development at Hogwarts. (arXiv:2307.00036v1 [cs.LG])

    [http://arxiv.org/abs/2307.00036](http://arxiv.org/abs/2307.00036)

    机器学习方法在霍格沃茨魔法学校的魔药发展中表现出潜在的创新和贡献。

    

    目标：确定机器学习方法是否能够为霍格沃茨魔法学校的研究和教学生成有用的魔药配方。设计：使用深度神经网络将生成的配方分类到标准药物分类系统中。数据来源：从哈利波特维基中提取的霍格沃茨课程的72个魔药配方。结果：大多数生成的配方属于兴奋剂和皮肤药品的类别。每个类别的预测配方数量与训练配方数量相对应。预测的概率通常超过90%，但有些配方被分类到2个或更多具有相似概率的类别中，这增加了对预测效果的预期的复杂性。结论：基于机器学习的方法能够为霍格沃茨魔法学校的教学和研究生成潜在有用的魔药配方。这与非魔法世界中的类似努力相一致。

    Objective: To determine whether machine learning methods can generate useful potion recipes for research and teaching at Hogwarts School of Witchcraft and Wizardry. Design: Using deep neural networks to classify generated recipes into a standard drug classification system. Setting: Hogwarts School of Witchcraft and Wizardry. Data sources: 72 potion recipes from the Hogwarts curriculum, extracted from the Harry Potter Wiki. Results: Most generated recipes fall into the categories of psychoanaleptics and dermatologicals. The number of recipes predicted for each category reflected the number of training recipes. Predicted probabilities were often above 90% but some recipes were classified into 2 or more categories with similar probabilities which complicates anticipating the predicted effects. Conclusions: Machine learning powered methods are able to generate potentially useful potion recipes for teaching and research at Hogwarts. This corresponds to similar efforts in the non-magical wor
    
[^55]: 参数识别及其在具有空时变系数的偏微分方程中的应用

    Parameter Identification for Partial Differential Equations with Spatiotemporal Varying Coefficients. (arXiv:2307.00035v1 [cs.LG])

    [http://arxiv.org/abs/2307.00035](http://arxiv.org/abs/2307.00035)

    本论文提出了一个新的框架，用于解决具有空时变系数的偏微分方程中的参数识别问题。通过约束自适应物理信息神经网络和有限混合模型方法，我们可以准确确定复杂多状态系统的未知参数。

    

    为了理解具有多个状态的复杂系统，揭示系统输出的这些状态的标识是必不可少的。然而，描述这些系统的数学模型通常呈现非线性，因此从观测到的空时数据中解决参数反问题变得具有挑战性。在从这些系统获得的观测数据的基础上，我们提出了一个新的框架，用于研究由空时变参数的偏微分方程所支配的多状态系统的参数识别问题。我们的框架包括两个主要组成部分：一个约束的自适应物理信息神经网络，作为我们的参数识别方法的一个子网络，以及一种有限混合模型方法来检测可能参数变化的区域。通过我们的方案，我们可以准确确定复杂多状态系统的未知变化参数，从而实现了参数识别。

    To comprehend complex systems with multiple states, it is imperative to reveal the identity of these states by system outputs. Nevertheless, the mathematical models describing these systems often exhibit nonlinearity so that render the resolution of the parameter inverse problem from the observed spatiotemporal data a challenging endeavor. Starting from the observed data obtained from such systems, we propose a novel framework that facilitates the investigation of parameter identification for multi-state systems governed by spatiotemporal varying parametric partial differential equations. Our framework consists of two integral components: a constrained self-adaptive physics-informed neural network, encompassing a sub-network, as our methodology for parameter identification, and a finite mixture model approach to detect regions of probable parameter variations. Through our scheme, we can precisely ascertain the unknown varying parameters of the complex multi-state system, thereby accomp
    
[^56]: 应用数据工程方法解决微生物组数据在医学决策中的挑战

    Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making. (arXiv:2307.00033v1 [q-bio.QM])

    [http://arxiv.org/abs/2307.00033](http://arxiv.org/abs/2307.00033)

    本研究应用数据工程方法解决微生物组数据中的类别不平衡和高维度问题，针对囊性纤维化婴儿的数据集实施了四种机器学习分类器，并提高了医学决策的准确性。

    

    通过它们与多个器官的相互作用，人体肠道微生物群落已被证明对机体的众多生理功能起着贡献，并且还与一系列病理条件有关。过去几十年的大量研究工作已经提供了有关肠道微生物群落的相对分类分布的有价值的信息，可以实现个体化医学。然而，微生物组数据存在类别不平衡和高维度问题，这些问题必须加以解决。在本研究中，我们实施了数据工程算法来解决与微生物组数据相关的上述问题。我们在以囊性纤维化婴儿的先前发表的数据集上实现了四种标准机器学习分类器（逻辑回归 (LR)、支持向量机 (SVM)、随机森林 (RF) 和极限梯度提升 (XGB) 决策树），并比较了不同分类器的性能。本研究结果表明，数据工程方法能有效解决微生物组数据中的类别不平衡和高维度问题，从而提高医学决策的准确性。

    The human gut microbiota is known to contribute to numerous physiological functions of the body through their interplay with multiple organs and also implicated in a myriad of pathological conditions. Prolific research work in the past few decades have yielded valuable information regarding the relative taxonomic distribution of the gut microbiota that could enable personalized medicine. Unfortunately, the microbiome data suffers from class imbalance and high dimensionality issues that must be addressed. In this study, we have implemented data engineering algorithms to address the above-mentioned issues inherent to microbiome data. Four standard machine learning classifiers (logistic regression (LR), support vector machines (SVM), random forests (RF), and extreme gradient boosting (XGB) decision trees) were implemented on a previously published dataset of infants with cystic fibrosis exhibiting normal vs abnormal growth patterns. The issue of class imbalance and high dimensionality of 
    
[^57]: 基于高斯过程贝叶斯推理的不确定性导向最优资源分配

    Uncertainty Informed Optimal Resource Allocation with Gaussian Process based Bayesian Inference. (arXiv:2307.00032v1 [math.OC])

    [http://arxiv.org/abs/2307.00032](http://arxiv.org/abs/2307.00032)

    本研究提出了一种基于高斯过程贝叶斯推理的数据驱动方法，用于将医疗资源（疫苗）不确定性导向地分配给异质人群以管理流行病传播。研究解决了参数估计和整合、非线性ODE约束和参数不确定性等问题。

    

    我们关注于将医疗资源（疫苗）不确定性导向地分配给异质人群以管理流行病传播的问题。我们解决了两个相关问题：（1）对于一个流行病传播的常微分方程（ODE）模型，我们如何在资源分配决策中估计和整合参数不确定性？（2）对于一个通用的随机优化问题（资源分配）如何计算处理非线性ODE约束和参数不确定性？据我们所知，当前的文献无法完全解决这些问题。在这里，我们提出了一种数据驱动的方法，在新的随机优化问题表达式中准确而可行地表示参数不确定性。我们首先使用高斯过程进行贝叶斯推理来估计ODE模型参数的分布，从而生成可行的情景集。然后，我们开发了一个并行化的解算算法，考虑到情景和参数不确定性。

    We focus on the problem of uncertainty informed allocation of medical resources (vaccines) to heterogeneous populations for managing epidemic spread. We tackle two related questions: (1) For a compartmental ordinary differential equation (ODE) model of epidemic spread, how can we estimate and integrate parameter uncertainty into resource allocation decisions? (2) How can we computationally handle both nonlinear ODE constraints and parameter uncertainties for a generic stochastic optimization problem for resource allocation? To the best of our knowledge current literature does not fully resolve these questions. Here, we develop a data-driven approach to represent parameter uncertainty accurately and tractably in a novel stochastic optimization problem formulation. We first generate a tractable scenario set by estimating the distribution on ODE model parameters using Bayesian inference with Gaussian processes. Next, we develop a parallelized solution algorithm that accounts for scenario-
    
[^58]: 通过语言瓶颈学习分类的“看见文字”论文

    Seeing in Words: Learning to Classify through Language Bottlenecks. (arXiv:2307.00028v1 [cs.CV])

    [http://arxiv.org/abs/2307.00028](http://arxiv.org/abs/2307.00028)

    本文提出了一种通过语言瓶颈学习分类的方法，利用文本表示特征的视觉模型能够有效分类ImageNet图像，可以增加神经网络的可解释性。

    

    尽管计算机视觉的神经网络在基准测试中取得了高准确性，但它们提取的特征往往是无法解释的。相比之下，人类可以用简洁直观的描述来解释他们的预测。为了将可解释性引入神经网络，我们训练了一个将特征表示为文本的视觉模型。我们展示了这样的模型在对ImageNet图像进行分类时的有效性，并讨论了我们在训练过程中遇到的挑战。

    Neural networks for computer vision extract uninterpretable features despite achieving high accuracy on benchmarks. In contrast, humans can explain their predictions using succinct and intuitive descriptions. To incorporate explainability into neural networks, we train a vision model whose feature representations are text. We show that such a model can effectively classify ImageNet images, and we discuss the challenges we encountered when training it.
    
[^59]: EmoSpeech: 将FastSpeech2引导至情感文本到语音的研究

    EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech. (arXiv:2307.00024v1 [eess.AS])

    [http://arxiv.org/abs/2307.00024](http://arxiv.org/abs/2307.00024)

    本研究提出了一种名为EmoSpeech的模型，通过对FastSpeech2进行一系列修改，实现了情感语音的合成。根据评估结果，EmoSpeech在生成的语音中具有更高的MOS得分和情感识别准确度。模型引入了条件机制，可以有效处理文本中情感不均衡分布的问题，生成具有更高MOS和情感表达的音频。

    

    当前最先进的语音合成模型致力于实现与人类声音尽可能接近的效果。因此，情感建模是文本到语音（TTS）研究中的重要一环。在我们的工作中，我们选择了FastSpeech2作为起点，并提出了一系列修改来合成情感语音。根据自动和人工评估，我们的模型EmoSpeech在生成的语音中无论是MOS得分还是情感识别准确度都超过了现有模型。我们为FastSpeech2架构中每个扩展提供了详细的消融研究，形成了EmoSpeech。文本中情感的不均衡分布对于更好的合成语音和音调感知至关重要。我们的模型包含了一种条件机制，通过允许情感以不同的强度水平对每个音素进行贡献，有效处理了这个问题。人工评估表明，我们提出的修改生成了具有更高MOS和情感表达的音频。

    State-of-the-art speech synthesis models try to get as close as possible to the human voice. Hence, modelling emotions is an essential part of Text-To-Speech (TTS) research. In our work, we selected FastSpeech2 as the starting point and proposed a series of modifications for synthesizing emotional speech. According to automatic and human evaluation, our model, EmoSpeech, surpasses existing models regarding both MOS score and emotion recognition accuracy in generated speech. We provided a detailed ablation study for every extension to FastSpeech2 architecture that forms EmoSpeech. The uneven distribution of emotions in the text is crucial for better, synthesized speech and intonation perception. Our model includes a conditioning mechanism that effectively handles this issue by allowing emotions to contribute to each phone with varying intensity levels. The human assessment indicates that proposed modifications generate audio with higher MOS and emotional expressiveness.
    
[^60]: 惯性导航与深度学习：当前趋势与未来方向的综述

    Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions. (arXiv:2307.00014v1 [cs.RO])

    [http://arxiv.org/abs/2307.00014](http://arxiv.org/abs/2307.00014)

    本文综述了惯性导航领域中当前的深度学习方法，包括对不同车辆操作领域的研究、滤波参数学习的改进以及惯性传感器的校准和去噪方法。翻译过的论文标题: 惯性导航与深度学习：当前趋势与未来方向的综述

    

    惯性传感在许多应用和平台中被使用，从智能手机等日常设备到自动驾驶车辆等复杂设备。近年来，机器学习和深度学习技术在惯性传感领域取得了显著发展。这是由于高效的计算硬件的发展和公开可用的传感器数据的可获得性。这些数据驱动的方法被用于强化基于模型的导航和传感器融合算法。本文提供了对这些深度学习方法的深入综述。我们分别考察了每个车辆操作领域，包括陆地、空中和海洋。每个领域分为纯惯性进展和基于滤波参数学习的改进。此外，我们还回顾了用于校准和去噪惯性传感器的深度学习方法。在整篇论文中，我们讨论了这些趋势和未来方向。我们还提供了常用的统计数据。

    Inertial sensing is used in many applications and platforms, ranging from day-to-day devices such as smartphones to very complex ones such as autonomous vehicles. In recent years, the development of machine learning and deep learning techniques has increased significantly in the field of inertial sensing. This is due to the development of efficient computing hardware and the accessibility of publicly available sensor data. These data-driven approaches are used to empower model-based navigation and sensor fusion algorithms. This paper provides an in-depth review of those deep learning methods. We examine separately, each vehicle operation domain including land, air, and sea. Each domain is divided into pure inertial advances and improvements based on filter parameters learning. In addition, we review deep learning approaches for calibrating and denoising inertial sensors. Throughout the paper, we discuss these trends and future directions. We also provide statistics on the commonly used
    
[^61]: 使用语言模型的黑盒预测易出错测试修复类别

    Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])

    [http://arxiv.org/abs/2307.00012](http://arxiv.org/abs/2307.00012)

    本文提出了一个使用语言模型的框架，可以自动生成易出错测试的标记数据集，并通过分析测试代码来预测测试的修复类别。实验结果表明UniXcoder优于CodeBERT。

    

    易出错测试会在相同软件版本的测试下非确定性地通过或失败，引起混乱并浪费开发者时间。尽管机器学习模型已经被用于预测易出错性及其根本原因，但在提供修复支持方面仍有较少工作。为了填补这一空白，我们提出了一个框架，通过仅分析测试代码自动生成13个修复类别的标记数据集，并训练模型来预测易出错测试的修复类别。虽然在当前阶段准确预测修复本身是不现实的，但这些类别提供了关于需要检查的测试代码部分的精确指导。我们的方法基于语言模型，即CodeBERT和UniXcoder，其输出经过前馈神经网络（FNN）或基于孪生网络的Few Shot Learning（FSL）进行了微调。我们的实验结果表明，UniXcoder在正确预测大多数修复类别方面表现优于CodeBERT。

    Flaky tests are problematic because they non-deterministically pass or fail for the same software version under test, causing confusion and wasting developer time. While machine learning models have been used to predict flakiness and its root causes, there is less work on providing support to fix the problem. To address this gap, we propose a framework that automatically generates labeled datasets for 13 fix categories and train models to predict the fix category of a flaky test by analyzing the test code only. Though it is unrealistic at this stage to accurately predict the fix itself, the categories provide precise guidance about what part of the test code to look at. Our approach is based on language models, namely CodeBERT and UniXcoder, whose output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese Network-based Few Shot Learning (FSL). Our experimental results show that UniXcoder outperforms CodeBERT, in correctly predicting most of the categories of fixes a dev
    
[^62]: 自动分配和分类软件问题

    Automated Assignment and Classification of Software Issues. (arXiv:2307.00009v1 [cs.CL])

    [http://arxiv.org/abs/2307.00009](http://arxiv.org/abs/2307.00009)

    本论文提出了一种自动分配和分类软件问题的方法。通过使用经过精心策划的语言特征和不同的机器学习方法，将问题分配给最相关的团队成员，并将其分类为不同的类别，以提高工作效率和准确性。

    

    软件问题包含修复、改进或创建新线程的工作单元，在开发过程中促进团队成员之间的沟通。将问题分配给最相关的团队成员并确定问题的类别是一项繁琐且具有挑战性的任务。错误的分类会导致项目延迟和重新工作，给团队成员带来麻烦。本文提出了一组经过精心策划的用于浅层机器学习方法的语言特征，并将浅层方法和集成方法与深度语言模型的性能进行了比较。与现有技术不同的是，我们将问题分配给四种角色（设计师、开发人员、测试人员和领导者），而不是特定的个人或团队，以促进我们解决方案的普遍性。我们还考虑开发人员的经验水平，以反映我们解决方案的工业实践。我们采用分类方法将问题分类为不同的类别，包括错误、新功能、改进等。

    Software issues contain units of work to fix, improve or create new threads during the development and facilitate communication among the team members. Assigning an issue to the most relevant team member and determining a category of an issue is a tedious and challenging task. Wrong classifications cause delays and rework in the project and trouble among the team members. This thesis proposes a set of carefully curated linguistic features for shallow machine learning methods and compares the performance of shallow and ensemble methods with deep language models. Unlike the state-of-the-art, we assign issues to four roles (designer, developer, tester, and leader) rather than to specific individuals or teams to contribute to the generality of our solution. We also consider the level of experience of the developers to reflect the industrial practices in our solution formulation. We employ a classification approach to categorize issues into distinct classes, namely bug, new feature, improve
    
[^63]: 通过平滑周期高斯Copula模型建模光伏电站群

    PV Fleet Modeling via Smooth Periodic Gaussian Copula. (arXiv:2307.00004v1 [stat.AP])

    [http://arxiv.org/abs/2307.00004](http://arxiv.org/abs/2307.00004)

    本文提出了一种通过平滑周期高斯Copula模型联合建模光伏系统群发电的方法，该方法可以捕捉数据的昼夜变化、系统间的依赖关系和随时间的依赖关系。通过该模型可以进行合成数据生成、缺失数据填补、异常检测和预测。

    

    我们提出了一种联合建模光伏（PV）系统群的发电方法。我们提出了一种白盒方法，它可以找到一个函数，可以将矢量时间序列数据映射为独立同分布的标准正态变量。基于一种新颖的方法来拟合平滑周期Copula变换到数据，该方法可以捕捉到数据的许多方面，如功率输出分布的昼夜变化、不同PV系统之间的依赖关系以及随时间的依赖关系。它由可解释的步骤组成，并且可扩展到许多系统。通过系统和时间的光伏电站群联合概率模型可以用来生成合成数据、填补缺失数据、进行异常检测和进行预测。本文中，我们解释了该方法并展示了这些应用。

    We present a method for jointly modeling power generation from a fleet of photovoltaic (PV) systems. We propose a white-box method that finds a function that invertibly maps vector time-series data to independent and identically distributed standard normal variables. The proposed method, based on a novel approach for fitting a smooth, periodic copula transform to data, captures many aspects of the data such as diurnal variation in the distribution of power output, dependencies among different PV systems, and dependencies across time. It consists of interpretable steps and is scalable to many systems. The resulting joint probability model of PV fleet output across systems and time can be used to generate synthetic data, impute missing data, perform anomaly detection, and make forecasts. In this paper, we explain the method and demonstrate these applications.
    
[^64]: Sphere2Vec：一种适用于大规模地理空间预测的球面上通用位置表示学习方法

    Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v1 [cs.CV])

    [http://arxiv.org/abs/2306.17624](http://arxiv.org/abs/2306.17624)

    Sphere2Vec是一种多尺度位置编码器，用于在球面上编码点坐标时保持球面距离，解决了大规模真实世界GPS坐标数据集中的距离度量问题。

    

    在机器学习中，为空间中的点生成适合学习的表示是一个基本且长期存在的问题。最近，提出了多尺度编码方案（如Space2Vec和NeRF），可以直接将二维/三维欧几里得空间中的任意点编码为高维向量，并成功应用于各种地理空间预测和生成任务。然而，目前所有的二维和三维位置编码器都是设计用来模拟欧几里得空间中的点距离。因此，在应用于需要在球面上进行距离度量学习的大规模真实世界GPS坐标数据集时，这两种类型的模型都会出现问题，原因是地图投影失真问题（2D）和球面到欧几里得距离近似误差（3D）。为了解决这些问题，我们提出了一种称为Sphere2Vec的多尺度位置编码器，可以在球面上编码点坐标时保持球面距离。我们在球面上的位置编码的距离保持编码的统一视角上进行了探索。

    Generating learning-friendly representations for points in space is a fundamental and long-standing problem in ML. Recently, multi-scale encoding schemes (such as Space2Vec and NeRF) were proposed to directly encode any point in 2D/3D Euclidean space as a high-dimensional vector, and has been successfully applied to various geospatial prediction and generative tasks. However, all current 2D and 3D location encoders are designed to model point distances in Euclidean space. So when applied to large-scale real-world GPS coordinate datasets, which require distance metric learning on the spherical surface, both types of models can fail due to the map projection distortion problem (2D) and the spherical-to-Euclidean distance approximation error (3D). To solve these problems, we propose a multi-scale location encoder called Sphere2Vec which can preserve spherical distances when encoding point coordinates on a spherical surface. We developed a unified view of distance-reserving encoding on sph
    
[^65]: 通过状态空间缩减和输入分割来扩展DNN分析的模型检验

    Scaling Model Checking for DNN Analysis via State-Space Reduction and Input Segmentation (Extended Version). (arXiv:2306.17323v1 [cs.LG])

    [http://arxiv.org/abs/2306.17323](http://arxiv.org/abs/2306.17323)

    该论文通过状态空间缩减和输入分割提出了一个扩展DNN分析的模型检验框架，解决了模型检验的可扩展性问题。

    

    鉴于神经网络（NN）在真实世界应用中表现出的学习能力和性能，基于NN的机器学习系统的使用持续增长。然而，文献中的各种案例研究和经验发现表明，微小的NN输入变化可能导致错误和不可取的NN行为。这引起了对其形式分析的广泛兴趣，旨在提供关于给定NN行为的保证。现有的框架使用可满足性求解和线性规划为训练的NN提供了稳健性和/或安全性保证。我们提出了FANNet，这是第一个基于模型检验的框架，用于分析更广泛范围的NN属性。然而，与模型检验相关的状态空间爆炸导致了可扩展性问题，使得FANNet只适用于小型NN。本工作开发了状态空间缩减和输入分割方法，以提高可扩展性和计时效率。

    Owing to their remarkable learning capabilities and performance in real-world applications, the use of machine learning systems based on Neural Networks (NNs) has been continuously increasing. However, various case studies and empirical findings in the literature suggest that slight variations to NN inputs can lead to erroneous and undesirable NN behavior. This has led to considerable interest in their formal analysis, aiming to provide guarantees regarding a given NN's behavior. Existing frameworks provide robustness and/or safety guarantees for the trained NNs, using satisfiability solving and linear programming. We proposed FANNet, the first model checking-based framework for analyzing a broader range of NN properties. However, the state-space explosion associated with model checking entails a scalability problem, making the FANNet applicable only to small NNs. This work develops state-space reduction and input segmentation approaches, to improve the scalability and timing efficienc
    
[^66]: 使用时间集成改进在线连续学习性能和稳定性

    Improving Online Continual Learning Performance and Stability with Temporal Ensembles. (arXiv:2306.16817v1 [cs.LG])

    [http://arxiv.org/abs/2306.16817](http://arxiv.org/abs/2306.16817)

    该研究通过模型集成方法改进了在线连续学习的性能和稳定性，通过综合利用来自不同训练任务的模型，显著提高了在线连续学习的表现。

    

    当神经网络在大型数据集上进行大量迭代训练时，它们非常有效。然而，当它们在非平稳的数据流和在线方式下进行训练时，其性能会下降：(1)在线设置限制了数据的可用性，(2)由于数据的非平稳性导致灾难性遗忘。此外，几篇最近的文章表明连续学习中使用的重放方法在模型持续评估时存在稳定性差距。在本文中，我们研究了模型集成作为改进在线连续学习性能和稳定性的一种方法。我们观察到，简单地集成来自各种训练任务的模型显著提高了在线连续学习的性能。基于这一观察，并从半监督学习中获取灵感，我们提出了一种改进的连续学习框架，该框架综合利用了显式和隐式知识。

    Neural networks are very effective when trained on large datasets for a large number of iterations. However, when they are trained on non-stationary streams of data and in an online fashion, their performance is reduced (1) by the online setup, which limits the availability of data, (2) due to catastrophic forgetting because of the non-stationary nature of the data. Furthermore, several recent works (Caccia et al., 2022; Lange et al., 2023) arXiv:2205.1345(2) showed that replay methods used in continual learning suffer from the stability gap, encountered when evaluating the model continually (rather than only on task boundaries). In this article, we study the effect of model ensembling as a way to improve performance and stability in online continual learning. We notice that naively ensembling models coming from a variety of training tasks increases the performance in online continual learning considerably. Starting from this observation, and drawing inspirations from semi-supervised l
    
[^67]: 可分离的物理信息神经网络

    Separable Physics-Informed Neural Networks. (arXiv:2306.15969v1 [cs.LG])

    [http://arxiv.org/abs/2306.15969](http://arxiv.org/abs/2306.15969)

    这项研究提出了一种可分离的物理信息神经网络（SPINN），通过逐个处理轴来显著减少了多维 PDE 中的网络传播数量，并使用正向模式自动微分降低了计算成本，使得可以在单个普通 GPU 上使用大量的配点。

    

    物理信息神经网络(PINNs)最近已经成为有希望的基于数据的PDE求解器，在各种PDE上显示出令人鼓舞的结果。然而，训练PINNs来解决多维PDE和逼近高度复杂解函数存在根本限制。在这些具有挑战性的PDE上所需的训练点数量(配点)大大增加，但由于昂贵的计算成本和庞大的内存开销，其受到严重限制。为了解决这个问题，我们提出了一种用于PINNs的网络架构和训练算法。所提出的方法，可分离的PINN (SPINN)，在多维PDE中按轴逐个处理，从而显著减少了网络传播的数量，不同于传统PINNs中的逐点处理。我们还提出使用正向模式自动微分来降低计算PDE残差的计算成本，从而在单个普通GPU上可以使用大量的配点(>10^7)。

    Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (>10^7) on a single commodity GPU. The
    
[^68]: GraSS:带有梯度引导采样策略的对比学习用于遥感图像语义分割

    GraSS: Contrastive Learning with Gradient Guided Sampling Strategy for Remote Sensing Image Semantic Segmentation. (arXiv:2306.15868v1 [cs.LG])

    [http://arxiv.org/abs/2306.15868](http://arxiv.org/abs/2306.15868)

    提出了一种基于对比学习的带有梯度引导采样策略（GraSS）用于遥感图像语义分割任务，解决了正样本混淆和特征适应偏差的问题。

    

    自监督对比学习（SSCL）在遥感图像（RSI）理解方面取得了重大的里程碑。其核心在于设计一种无监督实例区分预训练任务，从大量无标签图像中提取有利于下游任务的图像特征。然而，现有的基于实例区分的SSCL在应用于RSI语义分割任务时存在两个限制：1）正样本混淆问题；2）特征适应偏差。在需要像素级或目标级特征的语义分割任务中，它引入了特征适应偏差。在本研究中，我们观察到鉴别信息可以通过无监督对比损失的梯度映射到RSI的特定区域，这些特定区域往往包含特殊的地面对象。基于此，我们提出了一种带有梯度引导采样策略的对比学习（GraSS）用于RSI语义分割。

    Self-supervised contrastive learning (SSCL) has achieved significant milestones in remote sensing image (RSI) understanding. Its essence lies in designing an unsupervised instance discrimination pretext task to extract image features from a large number of unlabeled images that are beneficial for downstream tasks. However, existing instance discrimination based SSCL suffer from two limitations when applied to the RSI semantic segmentation task: 1) Positive sample confounding issue; 2) Feature adaptation bias. It introduces a feature adaptation bias when applied to semantic segmentation tasks that require pixel-level or object-level features. In this study, We observed that the discrimination information can be mapped to specific regions in RSI through the gradient of unsupervised contrastive loss, these specific regions tend to contain singular ground objects. Based on this, we propose contrastive learning with Gradient guided Sampling Strategy (GraSS) for RSI semantic segmentation. Gr
    
[^69]: SparseOptimizer: 通过Moreau-Yosida正则化来降低语言模型的稀疏性，并通过编译器共同设计来加速

    SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])

    [http://arxiv.org/abs/2306.15656](http://arxiv.org/abs/2306.15656)

    SparseOptimizer是一种深度学习优化器，通过Moreau-Yosida正则化在大型语言模型中引入稀疏性。它采用嵌入的收缩操作符，无需对代码进行修改即可适应各种大型语言模型，并在各种基准数据集上实现与密集型模型相当的性能，同时减少参数数量。

    

    本文介绍了SparseOptimizer，一种新颖的深度学习优化器，通过Moreau-Yosida正则化在大型语言模型（如BERT，ALBERT和GPT）中自然地引入稀疏性。SparseOptimizer设计的关键是嵌入的收缩操作符，它在优化过程中直接引入稀疏性。这个操作符通过坚实的理论框架支持，并包含了一个分析解，从而增强了优化器的鲁棒性和效果。重要的是，SparseOptimizer的即插即用功能消除了对代码修改的需求，使其成为适用于各种大型语言模型的通用适应工具。在GLUE、RACE、SQuAD1和SQuAD2等基准数据集上的实证评估表明，通过SparseOptimizer稀疏化后的SparseBERT和SparseALBERT在性能上与密集型的BERT和ALBERT相当，同时显著减少了参数数量。

    This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
    
[^70]: DataCI: 一个用于流数据中数据中心人工智能的平台

    DataCI: A Platform for Data-Centric AI on Streaming Data. (arXiv:2306.15538v1 [cs.DC])

    [http://arxiv.org/abs/2306.15538](http://arxiv.org/abs/2306.15538)

    DataCI是一个开源平台，专为流数据中的数据中心人工智能而设计，提供丰富的API和版本控制功能，具有易于使用和有效性，可以改变流数据背景下的数据中心人工智能实践。

    

    我们介绍了DataCI，这是一个专门用于动态流数据场景中的数据中心人工智能的综合开源平台。DataCI提供了基础设施，具有丰富的API，用于无缝流数据集管理、数据中心流程的开发和评估，并且提供了精心设计的版本控制功能，以跟踪流程的衍生。另外，DataCI还提供了直观的图形界面，提供更好的交互体验。初步研究和演示证明了DataCI易于使用和有效性，凸显了它在流数据背景下改变数据中心人工智能实践的潜力。

    We introduce DataCI, a comprehensive open-source platform designed specifically for data-centric AI in dynamic streaming data settings. DataCI provides 1) an infrastructure with rich APIs for seamless streaming dataset management, data-centric pipeline development and evaluation on streaming scenarios, 2) an carefully designed versioning control function to track the pipeline lineage, and 3) an intuitive graphical interface for a better interactive user experience. Preliminary studies and demonstrations attest to the easy-to-use and effectiveness of DataCI, highlighting its potential to revolutionize the practice of data-centric AI in streaming data contexts.
    
[^71]: 从仅状态序列学习非马尔科夫决策

    Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])

    [http://arxiv.org/abs/2306.15156](http://arxiv.org/abs/2306.15156)

    本文提出了一种从仅状态序列学习非马尔科夫决策的方法，通过深度生成建模和最大似然估计实现基于模型的模仿。学习的模型能够实现“推理式决策”，并在路径规划任务中展示了有效性。

    

    传统的模仿学习假设能够获得展示者的动作，但是在自然环境中这些动作通常无法观测。此外，在这些环境中的序列决策行为可能偏离标准马尔科夫决策过程（MDP）的假设。为了解决这些挑战，我们探索了非马尔科夫决策过程（nMDP）中仅状态序列的深度生成建模，其中策略是潜在状态转移生成器的能量先验。我们开发了最大似然估计来实现基于模型的模仿，其中包括对先验进行短期MCMC采样和对后验进行重要性采样。学习的模型实现了“推理式决策”，即无模型策略执行等价于先验采样，基于模型的规划则是从策略初始化的后验采样。我们在一个具有非马尔科夫特征的原型路径规划任务中证明了所提方法的有效性。

    Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
    
[^72]: 模糊条件扩散和扩散投影注意力在面部图像修复中的应用

    Fuzzy-Conditioned Diffusion and Diffusion Projection Attention Applied to Facial Image Correction. (arXiv:2306.14891v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.14891](http://arxiv.org/abs/2306.14891)

    该论文提出了一种模糊条件扩散的方法，可以利用隐式扩散先验并以可控强度应用于图像修复。同时，结合扩散引导的注意力图，可以对面部图像进行解释性和自主的修复。

    

    图像扩散最近在图像合成和隐式成为图像先验方面显示出了卓越的性能。这种先验已经用于条件化来解决修补问题，但只支持基于二进制用户的条件化。我们提出了一种模糊条件扩散，可以利用可控强度的隐式扩散先验。我们的模糊条件可以逐像素应用，使得可以以不同程度修改不同图像组成部分。此外，我们还提出了一种应用于面部图像修复的方法，将我们的模糊条件扩散与扩散引导的注意力图相结合。我们的注意力图可以估计异常的程度，通过在扩散空间上进行投影获得。我们展示了我们的方法也导致了解释性和自主的面部图像修复。

    Image diffusion has recently shown remarkable performance in image synthesis and implicitly as an image prior. Such a prior has been used with conditioning to solve the inpainting problem, but only supporting binary user-based conditioning. We derive a fuzzy-conditioned diffusion, where implicit diffusion priors can be exploited with controllable strength. Our fuzzy conditioning can be applied pixel-wise, enabling the modification of different image components to varying degrees. Additionally, we propose an application to facial image correction, where we combine our fuzzy-conditioned diffusion with diffusion-derived attention maps. Our map estimates the degree of anomaly, and we obtain it by projecting on the diffusion space. We show how our approach also leads to interpretable and autonomous facial image correction.
    
[^73]: PMaF:用于主要矩阵特征的深度声明性层

    PMaF: Deep Declarative Layers for Principal Matrix Features. (arXiv:2306.14759v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14759](http://arxiv.org/abs/2306.14759)

    本文介绍了PMaF框架，使用声明性的深度层来学习主要矩阵特征，通过迭代优化解决问题并应用双层优化框架进行反向传播，从而提高效率。实验证明了该框架优于现有的基线模型。

    

    本文探讨了两种不同iable的深度声明性层，即球上的最小二乘法(LESS)和隐式特征值分解(IED)，用于学习主要矩阵特征(PMaF)。这可以用一个低维向量表示包含来自高维矩阵的主要信息的数据特征。我们首先通过前向传递的迭代优化来解决问题，然后在一个双层优化框架下反向传播解决方案进行隐式梯度。具体来说，我们研究了自适应下降步骤和反向线搜索方法以及切线空间中的下降衰减，以提高LESS的前向通过程的效率。与此同时，在LESS和IED的反向传递中使用了优化的数据结构，大大降低了计算复杂度。在经验上，通过比较解决方案的最优性和计算要求，我们证明了我们的层优于现成的基线模型。

    We explore two differentiable deep declarative layers, namely least squares on sphere (LESS) and implicit eigen decomposition (IED), for learning the principal matrix features (PMaF). This can be used to represent data features with a low-dimension vector containing dominant information from a high-dimension matrix. We first solve the problems with iterative optimization in the forward pass and then backpropagate the solution for implicit gradients under a bi-level optimization framework. Particularly, adaptive descent steps with the backtracking line search method and descent decay in the tangent space are studied to improve the forward pass efficiency of LESS. Meanwhile, exploited data structures are used to greatly reduce the computational complexity in the backward pass of LESS and IED. Empirically, we demonstrate the superiority of our layers over the off-the-shelf baselines by comparing the solution optimality and computational requirements.
    
[^74]: 基于共生学习的异方差回归的近似最优算法研究

    Near Optimal Heteroscedastic Regression with Symbiotic Learning. (arXiv:2306.14288v1 [stat.ML])

    [http://arxiv.org/abs/2306.14288](http://arxiv.org/abs/2306.14288)

    本研究提出了一种基于共生学习的异方差回归的近似最优算法，可以在统计学、计量经济学、时间序列分析等领域，以及在不同来源数据质量不一的机器学习中应用。

    

    本研究针对经典的异方差线性回归问题展开讨论。假设我们有n个样本 $(\mathbf{x}_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$，其中 $y_i = \langle \mathbf{w}^{*}, \mathbf{x}_i \rangle + \epsilon_i \cdot \langle \mathbf{f}^{*}, \mathbf{x}_i \rangle$， $\mathbf{x}_i \sim N(0,\mathbf{I})$，$\epsilon_i \sim N(0,1)$，我们的目标是估计 $\mathbf{w}^{*}$。在统计学、计量经济学、时间序列分析等领域，异方差模型具有广泛的应用，同时，在机器学习中如果数据来源不同，而不同来源的数据质量也不一，则异方差模型也显得特别相关。本研究表明，我们可以估计出$\mathbf{w}^{*}$的平方范数，误差为$\tilde{O}\left(\|\mathbf{f}^{*}\|^2 \cdot \left(\frac{1}{n} + \left(\frac{d}{n}\right)^2\right)\right)$，并证明了一个匹配的下限（上界存在对数因子）。本研究的结果显著改进了异方差回归问题的近似最优算法。

    We consider the classical problem of heteroscedastic linear regression, where we are given $n$ samples $(\mathbf{x}_i, y_i) \in \mathbb{R}^d \times \mathbb{R}$ obtained from $y_i = \langle \mathbf{w}^{*}, \mathbf{x}_i \rangle + \epsilon_i \cdot \langle \mathbf{f}^{*}, \mathbf{x}_i \rangle$, where $\mathbf{x}_i \sim N(0,\mathbf{I})$, $\epsilon_i \sim N(0,1)$, and our task is to estimate $\mathbf{w}^{*}$. In addition to the classical applications of heteroscedastic models in fields such as statistics, econometrics, time series analysis etc., it is also particularly relevant in machine learning when data is collected from multiple sources of varying but apriori unknown quality, e.g., large model training. Our work shows that we can estimate $\mathbf{w}^{*}$ in squared norm up to an error of $\tilde{O}\left(\|\mathbf{f}^{*}\|^2 \cdot \left(\frac{1}{n} + \left(\frac{d}{n}\right)^2\right)\right)$ and prove a matching lower bound (up to logarithmic factors). Our result substantially improves 
    
[^75]: 一种鲁棒联邦学习的一阶Meta Stackelberg方法

    A First Order Meta Stackelberg Method for Robust Federated Learning. (arXiv:2306.13800v1 [cs.LG])

    [http://arxiv.org/abs/2306.13800](http://arxiv.org/abs/2306.13800)

    本研究提出了一种鲁棒的联邦学习防御方法，使用元Stackelberg学习算法解决贝叶斯Stackelberg马尔科夫博弈，实现自适应防御，与现有技术相匹配并在实验中表现出色。

    

    先前的研究表明，联邦学习系统面临着各种安全风险。尽管提出了多种防御策略，但它们往往是非自适应的，只针对某些类型的攻击，从而无法抵御不可预测或自适应的威胁。本研究将对抗性联邦学习建模为贝叶斯Stackelberg马尔科夫博弈(BSMG)以捕捉防御者对各种攻击类型的不完全信息。我们提出了元Stackelberg学习(meta-SL)，这是一种可证明有效的元学习算法，用于解决BSMG中的均衡策略，从而实现自适应的FL防御。我们证明，meta-SL在$O(\varepsilon^{-2})$梯度迭代中收敛于一阶$\varepsilon$-均衡点，每次迭代需要$O(\varepsilon^{-4})$个样本，与现有技术相匹配。经验证据表明，我们的元Stackelberg框架在强大的模型污染和后门攻击方面表现出色。

    Previous research has shown that federated learning (FL) systems are exposed to an array of security risks. Despite the proposal of several defensive strategies, they tend to be non-adaptive and specific to certain types of attacks, rendering them ineffective against unpredictable or adaptive threats. This work models adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) to capture the defender's incomplete information of various attack types. We propose meta-Stackelberg learning (meta-SL), a provably efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG, leading to an adaptable FL defense. We demonstrate that meta-SL converges to the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$ gradient iterations, with $O(\varepsilon^{-4})$ samples needed per iteration, matching the state of the art. Empirical evidence indicates that our meta-Stackelberg framework performs exceptionally well against potent model poisoning and backdo
    
[^76]: 解决图神经网络的局限性

    On Addressing the Limitations of Graph Neural Networks. (arXiv:2306.12640v1 [cs.LG])

    [http://arxiv.org/abs/2306.12640](http://arxiv.org/abs/2306.12640)

    本文讨论了图神经网络的两个挑战：过度平滑和异质性，提出了解决方案并展望了未来的研究方向。

    

    本文总结了关于图卷积网络（GCNs）的两个问题：过度平滑和异质性挑战，并概述了未来要探索的方向。

    This report gives a summary of two problems about graph convolutional networks (GCNs): over-smoothing and heterophily challenges, and outlines future directions to explore.
    
[^77]: 约束下的策略优化：State-wise Constrained Policy Optimization

    State-wise Constrained Policy Optimization. (arXiv:2306.12594v1 [cs.LG])

    [http://arxiv.org/abs/2306.12594](http://arxiv.org/abs/2306.12594)

    本文提出了一种新的通用策略搜索算法，State-wise Constrained Policy Optimization (SCPO)，可用于处理状态限制约束下的强化学习，具有良好的期望状态约束保证和最坏安全违反的有界性。

    

    强化学习算法在模拟环境中已经取得了巨大的成功，但是在实际问题中应用仍然面临着重大挑战，其中安全性是一个主要问题。特别地，对于许多具有挑战性的任务，例如自动驾驶和机器人操作，强制执行状态限制是十分必要的。然而，现有的约束马尔可夫决策过程（CMDP）框架下的安全强化学习算法并没有考虑状态约束。为填补这一空白，我们提出了State-wise Constrained Policy Optimization（SCPO），这是第一个旨在处理状态限制的通用策略搜索算法。SCPO能够在期望上保证状态约束的满足。特别地，我们引入了最大马尔可夫决策过程框架，并证明了在SCPO下最坏的安全违反是有界的。我们在大量训练神经网络策略时展示了该方法的有效性。

    Reinforcement Learning (RL) algorithms have shown tremendous success in simulation environments, but their application to real-world problems faces significant challenges, with safety being a major concern. In particular, enforcing state-wise constraints is essential for many challenging tasks such as autonomous driving and robot manipulation. However, existing safe RL algorithms under the framework of Constrained Markov Decision Process (CMDP) do not consider state-wise constraints. To address this gap, we propose State-wise Constrained Policy Optimization (SCPO), the first general-purpose policy search algorithm for state-wise constrained reinforcement learning. SCPO provides guarantees for state-wise constraint satisfaction in expectation. In particular, we introduce the framework of Maximum Markov Decision Process, and prove that the worst-case safety violation is bounded under SCPO. We demonstrate the effectiveness of our approach on training neural network policies for extensive 
    
[^78]: 基于几何深度学习的结构药物设计系统综述

    A Systematic Survey in Geometric Deep Learning for Structure-based Drug Design. (arXiv:2306.11768v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.11768](http://arxiv.org/abs/2306.11768)

    本文在系统回顾几何深度学习在结构药物设计中的最新进展，分别讨论了不同任务并按不同的几何深度学习方法进行组织。该领域的前景看好，但仍存在挑战。

    

    结构药物设计利用蛋白质的三维几何结构来识别潜在的药物候选物，在药物发现中变得越来越重要。然而，基于物理化学建模和专家领域知识的传统方法费时费力。近年来，几何深度学习的发展，可以处理和整合三维几何数据，加上类似AlphaFold的工具提供准确的蛋白质三维结构预测，极大地推动了结构药物设计的进展。在本文中，我们系统地回顾了几何深度学习在结构药物设计中的最新进展。我们从结构药物设计中的主流任务、常用的3D蛋白质表示和预测/生成模型入手，然后详细介绍每个任务的回顾（例如结合位点预测、结合构象生成、\emph{de novo} 分子设计等），并按不同的几何深度学习方法进行组织。最后，我们总结了该领域未来研究的挑战和前景。

    Structure-based drug design (SBDD), which utilizes the three-dimensional geometry of proteins to identify potential drug candidates, is becoming increasingly vital in drug discovery. However, traditional methods based on physiochemical modeling and experts' domain knowledge are time-consuming and laborious. The recent advancements in geometric deep learning, which integrates and processes 3D geometric data, coupled with the availability of accurate protein 3D structure predictions from tools like AlphaFold, have significantly propelled progress in structure-based drug design. In this paper, we systematically review the recent progress of geometric deep learning for structure-based drug design. We start with a brief discussion of the mainstream tasks in structure-based drug design, commonly used 3D protein representations and representative predictive/generative models. Then we delve into detailed reviews for each task (binding site prediction, binding pose generation, \emph{de novo} mo
    
[^79]: G-NM：一组数字时间序列预测模型

    G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11667](http://arxiv.org/abs/2306.11667)

    G-NM是一组集合了传统和现代模型的数字时间序列预测模型，旨在提高对复杂自然现象中的模式和趋势的预测能力。

    

    本研究聚焦于开发和实施一个综合的数字时间序列预测模型集合，统称为数字时间序列预测模型组（G-NM）。该集合包括传统模型如自回归综合移动平均（ARIMA）、Holt-Winters方法和支持向量回归（SVR），以及现代神经网络模型，如循环神经网络（RNN）和长短期记忆（LSTM）。G-NM明确构建以增强我们对复杂自然现象中固有模式和趋势的预测能力。通过利用与这些事件相关的时间序列数据，G-NM便于对此类现象在延长时间段内进行预测。本研究的主要目标是推进我们对此类事件的理解，并大幅提高预测准确性。G-NM包括线性和非线性依赖关系，以及季节性趋势。

    In this study, we focus on the development and implementation of a comprehensive ensemble of numerical time series forecasting models, collectively referred to as the Group of Numerical Time Series Prediction Model (G-NM). This inclusive set comprises traditional models such as Autoregressive Integrated Moving Average (ARIMA), Holt-Winters' method, and Support Vector Regression (SVR), in addition to modern neural network models including Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is explicitly constructed to augment our predictive capabilities related to patterns and trends inherent in complex natural phenomena. By utilizing time series data relevant to these events, G-NM facilitates the prediction of such phenomena over extended periods. The primary objective of this research is to both advance our understanding of such occurrences and to significantly enhance the accuracy of our forecasts. G-NM encapsulates both linear and non-linear dependencies, seasonal
    
[^80]: 基于注意力知识图卷积网络的旅游景点推荐

    Tourist Attractions Recommendation based on Attention Knowledge Graph Convolution Network. (arXiv:2306.10946v1 [cs.IR] CROSS LISTED)

    [http://arxiv.org/abs/2306.10946](http://arxiv.org/abs/2306.10946)

    本文提出了一种基于注意力知识图卷积网络的旅游景点推荐模型，通过自动语义发掘目标景点的相邻实体，根据旅客的喜好选择，预测类似景点的概率，实验中取得良好效果。

    

    基于知识图谱的推荐算法在相对成熟阶段，但在特定领域的推荐仍存在问题。例如在旅游领域，选择适合的旅游景点属性流程作为推荐基础较为复杂。本文提出改进的注意力知识图卷积网络模型(Att-KGCN)，自动语义地发掘目标景点的相邻实体，利用注意力层将相对相似的位置进行聚合，并通过推理旅客喜好选择，预测类似景点的概率作为推荐系统。实验中，采用索科特拉岛-也门的旅游数据，证明了注意力知识图卷积网络在旅游领域的景点推荐效果良好。

    The recommendation algorithm based on knowledge graphs is at a relatively mature stage. However, there are still some problems in the recommendation of specific areas. For example, in the tourism field, selecting suitable tourist attraction attributes process is complicated as the recommendation basis for tourist attractions. In this paper, we propose the improved Attention Knowledge Graph Convolution Network model, named (Att-KGCN), which automatically discovers the neighboring entities of the target scenic spot semantically. The attention layer aggregates relatively similar locations and represents them with an adjacent vector. Then, according to the tourist's preferred choices, the model predicts the probability of similar spots as a recommendation system. A knowledge graph dataset of tourist attractions used based on tourism data on Socotra Island-Yemen. Through experiments, it is verified that the Attention Knowledge Graph Convolution Network has a good effect on the recommendatio
    
[^81]: 基于竞争多智能体搜索的演化策略

    Evolving Strategies for Competitive Multi-Agent Search. (arXiv:2306.10640v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2306.10640](http://arxiv.org/abs/2306.10640)

    本文研究了基于竞争多智能体搜索的演化策略，通过实验验证了进化计算可以用于发现在不同竞争环境中的有效搜索策略。

    

    虽然进化计算适用于工程领域的自动发现，但它也可用于揭示人类和组织如何更有效地执行任务。通过以组织中的创新搜索问题为例，本文首先将人类创造性问题解决形式化为竞争多智能体搜索（CMAS）。CMAS不同于现有的单智能体和团队搜索问题，因为智能体通过了解其他智能体的搜索情况以及这些搜索导致的搜索景观的动态变化来进行交互。主要假设是，进化计算可以用于发现CMAS的有效策略；该假设在一系列关于NK模型的实验中得到验证，即部分相关且可调整崎岖的适应度景观。不同的专门策略针对每个不同的竞争环境进行了演化，同时还演化出了表现良好的通用策略。

    While evolutionary computation is well suited for automatic discovery in engineering, it can also be used to gain insight into how humans and organizations could perform more effectively. Using a real-world problem of innovation search in organizations as the motivating example, this article first formalizes human creative problem solving as competitive multi-agent search (CMAS). CMAS is different from existing single-agent and team search problems in that the agents interact through knowledge of other agents' searches and through the dynamic changes in the search landscape that result from these searches. The main hypothesis is that evolutionary computation can be used to discover effective strategies for CMAS; this hypothesis is verified in a series of experiments on the NK model, i.e.\ partially correlated and tunably rugged fitness landscapes. Different specialized strategies are evolved for each different competitive environment, and also general strategies that perform well acros
    
[^82]: 联邦少样本学习

    Federated Few-shot Learning. (arXiv:2306.10234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10234](http://arxiv.org/abs/2306.10234)

    本研究提出了一种名为“联邦少样本学习”的新问题，旨在解决联邦学习在少样本数据上的性能问题。我们提出了一个简单而有效的框架，使用特征提取和任务适应模块以及注意力机制来提高模型对于少样本客户端的泛化能力，在各种数据集上取得了最先进的联邦少样本学习性能。

    

    联邦学习使得多个客户端可以在不交换本地数据的情况下协作学习一个机器学习模型。然而，现有的方法通常假设每个客户端都有足够的数据用于训练，而事实上某些客户端可能只有有限数量的样本，即少样本数据。这种情况下，现有的联邦学习方法可能在这些客户端上遇到显著的性能下降。本文提出了一个处理这一问题的框架，命名为联邦少样本学习。我们设计了特征提取模块和任务适应模块来提高模型对于少样本客户端的泛化能力，并利用注意力机制来动态地调整每个客户端在训练过程中的重要性。实验结果表明，我们的方法在各种数据集上均取得了最先进的联邦少样本学习性能。

    Federated Learning (FL) enables multiple clients to collaboratively learn a machine learning model without exchanging their own local data. In this way, the server can exploit the computational power of all clients and train the model on a larger set of data samples among all clients. Although such a mechanism is proven to be effective in various fields, existing works generally assume that each client preserves sufficient data for training. In practice, however, certain clients may only contain a limited number of samples (i.e., few-shot samples). For example, the available photo data taken by a specific user with a new mobile device is relatively rare. In this scenario, existing FL efforts typically encounter a significant performance drop on these clients. Therefore, it is urgent to develop a few-shot model that can generalize to clients with limited data under the FL scenario. In this paper, we refer to this novel problem as federated few-shot learning. Nevertheless, the problem re
    
[^83]: 在线重尾变点检测

    Online Heavy-tailed Change-point detection. (arXiv:2306.09548v1 [stat.ML])

    [http://arxiv.org/abs/2306.09548](http://arxiv.org/abs/2306.09548)

    本文提出了一种在线变点检测算法，可以应对重尾分布且保证有限的假阳性率。

    

    我们研究了在线变点检测 (OCPD) 的算法，其中样本可能是重尾分布，一个接一个地呈现，并且必须尽早检测到底层均值的变化。我们提出了一种基于裁剪随机梯度下降 (SGD) 的算法，即使我们仅假定数据生成过程的第二阶矩有界，该算法也能正常工作。我们派生了在所有具有有界第二矩的分布族中最坏情况下的有限样本假阳性率 (FPR) 的保证。因此，我们的方法是第一个保证有限样本 FPR 的 OCPD 算法，即使数据是高维的，底层分布是重尾的。我们论文的技术贡献是展示了裁剪 SGD 可以估计随机向量的均值并同时在所有置信度值上提供置信度界限。我们将这个稳健的估计与并集边界论证相结合，构建一个有限的顺序变点算法。

    We study algorithms for online change-point detection (OCPD), where samples that are potentially heavy-tailed, are presented one at a time and a change in the underlying mean must be detected as early as possible. We present an algorithm based on clipped Stochastic Gradient Descent (SGD), that works even if we only assume that the second moment of the data generating process is bounded. We derive guarantees on worst-case, finite-sample false-positive rate (FPR) over the family of all distributions with bounded second moment. Thus, our method is the first OCPD algorithm that guarantees finite-sample FPR, even if the data is high dimensional and the underlying distributions are heavy-tailed. The technical contribution of our paper is to show that clipped-SGD can estimate the mean of a random vector and simultaneously provide confidence bounds at all confidence values. We combine this robust estimate with a union bound argument and construct a sequential change-point algorithm with finite
    
[^84]: 树形变分自编码器

    Tree Variational Autoencoders. (arXiv:2306.08984v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08984](http://arxiv.org/abs/2306.08984)

    树形变分自编码器(TreeVAE)是一种新的生成式层次聚类模型，通过学习灵活的树状潜变量后验分布，层次划分数据样本并揭示隐藏结构。该模型利用树的生成式架构进行轻量级条件推理，同时通过专门的叶子解码器提高生成性能。在各种数据集上，TreeVAE发现了潜在簇并找到了有意义的层次关系。与顺序对应物相比，TreeVAE提供了更具竞争力的对数似然下界。

    

    我们提出了一种新的生成式层次聚类模型，该模型学习了一个灵活的基于树的潜变量的后验分布。提出的树形变分自编码器(TreeVAE)根据数据的固有特征对样本进行层次划分，揭示了数据中的隐藏结构。它根据潜变量之间的依赖关系调整其结构以发现最优的编码树。所提出的基于树的生成式架构允许轻量级的条件推理，并通过利用专门的叶子解码器提高生成性能。我们展示了TreeVAE在各种数据集上，包括真实世界的图像数据中发现的潜在簇，并找到了不同组之间的有意义的层次关系。我们通过实验证明，与顺序对应物相比，TreeVAE提供了更具竞争力的对数似然下界。最后，由于其生成性质，TreeVAE能够从已学习的分布中生成新样本。

    We propose a new generative hierarchical clustering model that learns a flexible tree-based posterior distribution over latent variables. The proposed Tree Variational Autoencoder (TreeVAE) hierarchically divides samples according to their intrinsic characteristics, shedding light on hidden structure in the data. It adapts its architecture to discover the optimal tree for encoding dependencies between latent variables. The proposed tree-based generative architecture permits lightweight conditional inference and improves generative performance by utilizing specialized leaf decoders. We show that TreeVAE uncovers underlying clusters in the data and finds meaningful hierarchical relations between the different groups on a variety of datasets, including real-world imaging data. We present empirically that TreeVAE provides a more competitive log-likelihood lower bound than the sequential counterparts. Finally, due to its generative nature, TreeVAE is able to generate new samples from the di
    
[^85]: 通过不偏微分对抗复杂密度生成，基于互联马尔科夫链不偏微分优化 MH 采样方法

    Differentiating Metropolis-Hastings to Optimize Intractable Densities. (arXiv:2306.07961v1 [stat.ML])

    [http://arxiv.org/abs/2306.07961](http://arxiv.org/abs/2306.07961)

    本文通过基于互联马尔科夫链的不偏微分，开发出一种无偏、低方差和自动的方法对复杂密度进行生成，从而实现对 MH 采样器的优化。

    

    在概率模型推理中，目标密度函数通常变得难以计算，需要使用 Monte Carlo 计算。本文开发了一种不偏微分 Metropolis-Hastings 采样器的方法，使我们可以通过概率推理来进行微分。通过将随机微分的最新进展与 Markov 链耦合方法相结合，可以实现无偏，低方差和自动的程序。这使我们能够将基于梯度的优化应用于由于繁琐的目标密度导致期望的情况下。我们通过在高斯混合模型中找到一个模棱两可的观察和在 Ising 模型中最大化比热来演示了我们的方法。

    When performing inference on probabilistic models, target densities often become intractable, necessitating the use of Monte Carlo samplers. We develop a methodology for unbiased differentiation of the Metropolis-Hastings sampler, allowing us to differentiate through probabilistic inference. By fusing recent advances in stochastic differentiation with Markov chain coupling schemes, the procedure can be made unbiased, low-variance, and automatic. This allows us to apply gradient-based optimization to objectives expressed as expectations over intractable target densities. We demonstrate our approach by finding an ambiguous observation in a Gaussian mixture model and by maximizing the specific heat in an Ising model.
    
[^86]: 分层结构领域自适应

    Taxonomy-Structured Domain Adaptation. (arXiv:2306.07874v1 [cs.LG])

    [http://arxiv.org/abs/2306.07874](http://arxiv.org/abs/2306.07874)

    本文提出了分层结构领域自适应方法，通过分层结构领域的形式化描述来缓解不同领域之间的分布偏移，该方法在合成和实际数据集上实现了最先进的性能。

    

    领域自适应旨在缓解不同领域之间的分布偏移。然而，传统的方法大多限于分类领域，这严重简化了真实世界中微妙的领域关系。在本文中，我们通过分层结构领域进行推广，将领域形式化为具有嵌套的层次相似结构，例如动物物种和产品目录。我们建立在经典对抗框架之上，并引入了一种新颖的分类器，该分类器与对抗性鉴别器竞争以保留层次结构信息。当给定非信息领域分类（例如，所有叶节点都链接到根节点的扁平分类）时，平衡点恢复经典的对抗领域适应解决方案，同时在其他分类中产生非平凡的结果。在合成和实际数据集上实证结果表明，我们的方法实现了最先进的性能，并成功进行了自适应。代码可在https://gith获得。

    Domain adaptation aims to mitigate distribution shifts among different domains. However, traditional formulations are mostly limited to categorical domains, greatly simplifying nuanced domain relationships in the real world. In this work, we tackle a generalization with taxonomy-structured domains, which formalizes domains with nested, hierarchical similarity structures such as animal species and product catalogs. We build on the classic adversarial framework and introduce a novel taxonomist, which competes with the adversarial discriminator to preserve the taxonomy information. The equilibrium recovers the classic adversarial domain adaptation's solution if given a non-informative domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root node) while yielding non-trivial results with other taxonomies. Empirically, our method achieves state-of-the-art performance on both synthetic and real-world datasets with successful adaptation. Code is available at https://gith
    
[^87]: 分子属性预测的自动化三维预训练

    Automated 3D Pre-Training for Molecular Property Prediction. (arXiv:2306.07812v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.07812](http://arxiv.org/abs/2306.07812)

    通过在3D分子图上进行预训练，该论文提出了一种更有效地预测分子属性的方法，该方法可以在不需要分子几何结构的情况下进行微调。

    

    分子属性预测是药物研发和材料科学中的重要问题。由于分子的几何结构对于分子属性预测的必要性已经被证明，因此将3D信息与各种图形学习方法相结合以提高预测性能。然而，由于高计算成本，在许多现实应用中获得分子的几何结构是不可行的。在这项工作中，我们提出了一种新的3D预训练框架（称为3D PGT），它在3D分子图上预训练模型，然后在没有3D结构的分子图上进行微调。基于化学键长，化学键角和二面角这三个基本几何描述符对应于完整的分子3D构形，我们首先开发了一个基于这三个属性的多任务生成预训练框架。接下来，为了自动融合这三项生成任务，我们设计了一种使用“总能量”来搜索的替代指标。

    Molecular property prediction is an important problem in drug discovery and materials science. As geometric structures have been demonstrated necessary for molecular property prediction, 3D information has been combined with various graph learning methods to boost prediction performance. However, obtaining the geometric structure of molecules is not feasible in many real-world applications due to the high computational cost. In this work, we propose a novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D molecular graphs, and then fine-tunes it on molecular graphs without 3D structures. Based on fact that bond length, bond angle, and dihedral angle are three basic geometric descriptors corresponding to a complete molecular 3D conformer, we first develop a multi-task generative pre-train framework based on these three attributes. Next, to automatically fuse these three generative tasks, we design a surrogate metric using the \textit{total energy} to search for 
    
[^88]: V-LoL: 一种用于视觉逻辑学习的诊断数据集

    V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v1 [cs.AI])

    [http://arxiv.org/abs/2306.07743](http://arxiv.org/abs/2306.07743)

    V-LoL是一个结合视觉和逻辑挑战的诊断数据集，其中包括了V-LoL-Trains，该数据集首次将复杂的视觉场景和灵活的逻辑推理任务结合起来，为研究广泛的视觉逻辑学习挑战提供了平台。

    

    尽管近期在视觉AI领域有了许多成功的进展，但仍存在不同的缺点；包括缺少精确的逻辑推理、抽象的概括能力以及理解复杂和嘈杂的场景等。不幸的是，现有的基准测试数据集并不能捕捉到这些方面中的多数。深度学习数据集关注视觉复杂数据但只有简单的视觉推理任务，归纳逻辑数据集包括复杂的逻辑学习任务，但是缺乏视觉的组成部分。为了解决这个问题，我们提出了视觉逻辑学习数据集V-LoL，它无缝地结合了视觉和逻辑的挑战。值得注意的是，我们首次推出了V-LoL的第一个实例，名为V-LoL-Trains，它是符号AI中一个经典基准测试的视觉呈现，即Michalski火车问题。通过在一个通用框架内结合复杂的视觉场景和灵活的逻辑推理任务，V-LoL-Trains为研究广泛的视觉逻辑学习挑战提供了平台。

    Despite the successes of recent developments in visual AI, different shortcomings still exist; from missing exact logical reasoning, to abstract generalization abilities, to understanding complex and noisy scenes. Unfortunately, existing benchmarks, were not designed to capture more than a few of these aspects. Whereas deep learning datasets focus on visually complex data but simple visual reasoning tasks, inductive logic datasets involve complex logical learning tasks, however, lack the visual component. To address this, we propose the visual logical learning dataset, V-LoL, that seamlessly combines visual and logical challenges. Notably, we introduce the first instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic benchmark in symbolic AI, the Michalski train problem. By incorporating intricate visual scenes and flexible logical reasoning tasks within a versatile framework, V-LoL-Trains provides a platform for investigating a wide range of visual logical learning ch
    
[^89]: DRCFS：双重稳健因果特征选择

    DRCFS: Doubly Robust Causal Feature Selection. (arXiv:2306.07024v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07024](http://arxiv.org/abs/2306.07024)

    DRCFS是一个双重稳健因果特征选择方法，可以在非线性和高维环境中识别因果特征，优于现有方法。

    

    在许多科学领域中，了解对于特定目标变量非常相关的复杂系统特征是非常重要的。现有的方法通常局限于线性设置，有时缺乏保证，并且在大多数情况下无法扩展到需要处理的问题，特别是图像问题。我们提出了DRCFS，一种双重稳健特征选择方法，可以在非线性和高维设置中识别因果特征。我们提供了理论保证，阐明了我们的假设必要条件，并在广泛的模拟和半合成数据集上进行了大量实验。DRCFS在选择具有鲁棒性特征的技巧非常非线性和高维问题上明显优于现有的最先进方法。

    Knowing the features of a complex system that are highly relevant to a particular target variable is of fundamental interest in many areas of science. Existing approaches are often limited to linear settings, sometimes lack guarantees, and in most cases, do not scale to the problem at hand, in particular to images. We propose DRCFS, a doubly robust feature selection method for identifying the causal features even in nonlinear and high dimensional settings. We provide theoretical guarantees, illustrate necessary conditions for our assumptions, and perform extensive experiments across a wide range of simulated and semi-synthetic datasets. DRCFS significantly outperforms existing state-of-the-art methods, selecting robust features even in challenging highly non-linear and high-dimensional problems.
    
[^90]: 自监督等式嵌入深度Lagrange对偶算法优化逼近限制优化问题

    Self-supervised Equality Embedded Deep Lagrange Dual for Approximate Constrained Optimization. (arXiv:2306.06674v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2306.06674](http://arxiv.org/abs/2306.06674)

    该论文提出了一种自监督等式嵌入深度Lagrange对偶算法，用于解决不带标签的逼近限制优化问题。此方法通过在神经网络中嵌入等式约束来确保可行解，并使用原始-对偶方法进行训练，同时DeepLDE取得了最好的优化结果。

    

    在限制优化问题中，传统求解方法通常计算量较大，特别是在规模较大、时间敏感的问题上更是如此。因此，使用神经网络作为快速最优解逼近器引起了人们的越来越大兴趣，但是将约束条件与神经网络结合起来是具有挑战性的。为此，我们提出了一种称为DeepLDE的深度Lagrange对偶算法，该框架学习在不使用标签的情况下寻找最优解，通过将等式约束嵌入神经网络来确保可行解，并使用原始-对偶方法对不等式约束进行训练。此外，我们证明了DeepLDE的收敛性，并表明仅靠原始-对偶学习方法无法确保等式约束，需要等式嵌入的帮助。在凸、非凸和交流最优潮流（AC-OPF）问题的模拟结果中，我们展示了DeepLDE的最优性能而且始终保证可行解。

    Conventional solvers are often computationally expensive for constrained optimization, particularly in large-scale and time-critical problems. While this leads to a growing interest in using neural networks (NNs) as fast optimal solution approximators, incorporating the constraints with NNs is challenging. In this regard, we propose deep Lagrange dual with equality embedding (DeepLDE), a framework that learns to find an optimal solution without using labels. To ensure feasible solutions, we embed equality constraints into the NNs and train the NNs using the primal-dual method to impose inequality constraints. Furthermore, we prove the convergence of DeepLDE and show that the primal-dual learning method alone cannot ensure equality constraints without the help of equality embedding. Simulation results on convex, non-convex, and AC optimal power flow (AC-OPF) problems show that the proposed DeepLDE achieves the smallest optimality gap among all the NN-based approaches while always ensuri
    
[^91]: 分层级别激活机制

    Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])

    [http://arxiv.org/abs/2306.04940](http://arxiv.org/abs/2306.04940)

    去噪声更好，表现更好的分层级别激活机制

    

    本文提出了一种新颖的激活机制，旨在建立分层级别激活功能（LayerAct）。这些功能旨在通过减少输入偏移所导致的激活输出的分层级波动来降低传统元素级激活功能的噪音鲁棒性。此外，LayerAct功能实现了类似于零的平均激活输出，而不限制激活输出空间。我们进行了分析和实验，证明LayerAct功能在噪声鲁棒性方面优于元素级激活功能，并且经验证明这些功能的平均激活结果类似于零。在三个基准图像分类任务的实验结果表明，在处理嘈杂的图像数据集时，LayerAct功能比元素级激活功能表现更好，而在大多数情况下，清洁数据集的表现也是优越的。

    In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
    
[^92]: 论大型语言模型水印的可靠性

    On the Reliability of Watermarks for Large Language Models. (arXiv:2306.04634v1 [cs.LG])

    [http://arxiv.org/abs/2306.04634](http://arxiv.org/abs/2306.04634)

    本文研究了大型语言模型水印在混合其他文本来源时的可靠性，并提供了在实际应用中的建议。

    

    大型语言模型(LLMs)已经开始应用于日常使用，并有能力在未来的十年内产生大量的文本。机器生成的文本可能会取代互联网上的人类写作文本，并有可能被用于恶意目的，如钓鱼攻击和社交媒体机器人。水印是一种简单有效的策略，通过使LLM生成的文本可检测和可记录，来降低这些伤害。然而，一个关键问题仍然存在：在现实中混合了其他的文本来源，被人类写作者或其他语言模型改写，被用于社交和技术领域的各种应用时，水印在实际设置中的可靠性如何？在本文中，我们探讨了不同的检测方案，量化了它们检测水印的能力，并确定在每个情况下需要观察多少机器生成的文本才能可靠地检测水印。我们特别强调了当水印与其他文本来源混合时水印的可靠性，并提供了未来使用LLM生成的文本水印的建议。

    Large language models (LLMs) are now deployed to everyday use and positioned to produce large quantities of text in the coming decade. Machine-generated text may displace human-written text on the internet and has the potential to be used for malicious purposes, such as spearphishing attacks and social media bots. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet, a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text might be mixed with other text sources, paraphrased by human writers or other language models, and used for applications in a broad number of domains, both social and technical. In this paper, we explore different detection schemes, quantify their power at detecting watermarks, and determine how much machine-generated text needs to be observed in each scenario to reliably detect the watermark. We especially highlight o
    
[^93]: ContriMix：显微镜图像分析中基于无监督内容属性分离的领域泛化方法

    ContriMix: Unsupervised disentanglement of content and attribute for domain generalization in microscopy image analysis. (arXiv:2306.04527v1 [eess.IV])

    [http://arxiv.org/abs/2306.04527](http://arxiv.org/abs/2306.04527)

    ContriMix是一种无需标识和手工调优的领域泛化技术，在显微镜图像中通过分离和学习生成合成图像的方式，解决了领域泛化的问题。

    

    针对组织学和荧光成像等显微镜图像中的领域泛化问题，我们提出了 ContriMix，它采用无监督学习方式分离出显微镜图像中的生物学内容和技术变异，并学习生成合成图像，避免了需要手工 fine-tuning 的问题。我们在组织学和荧光成像实验中验证了 ContriMix 的有效性，取得了基于领域泛化的最新成果。

    Domain generalization is critical for real-world applications of machine learning models to microscopy images, including histopathology and fluorescence imaging. Artifacts in histopathology arise through a complex combination of factors relating to tissue collection and laboratory processing, as well as factors intrinsic to patient samples. In fluorescence imaging, these artifacts stem from variations across experimental batches. The complexity and subtlety of these artifacts make the enumeration of data domains intractable. Therefore, augmentation-based methods of domain generalization that require domain identifiers and manual fine-tuning are inadequate in this setting. To overcome this challenge, we introduce ContriMix, a domain generalization technique that learns to generate synthetic images by disentangling and permuting the biological content ("content") and technical variations ("attributes") in microscopy images. ContriMix does not rely on domain identifiers or handcrafted aug
    
[^94]: 基于分布特征匹配的标签偏移量量化及其鲁棒性保证

    Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v1 [stat.ML])

    [http://arxiv.org/abs/2306.04376](http://arxiv.org/abs/2306.04376)

    本文提出一种名为DFM框架的方法，用于量化标签偏移，并证明了其性能上限和鲁棒性。使用基于核的DFM版本可以提高效率、可扩展性和鲁棒性。

    

    量化学习处理在标签偏移下估计目标标签分布的任务。本文首先提出了一个统一的框架，分布特征匹配（DFM），将先前文献中引入的各种估计器恢复为特定实例。我们推导了DFM程序的一般性能界，改进了先前在特定情况下推导的界限的若干关键方面。然后，我们将这一分析扩展到研究DFM程序在未精确假设标签偏移量的情况下的鲁棒性，特别是在目标受到未知分布污染的情况下。这些理论发现在模拟和实际数据集上得到了详细的数字研究确认。我们还使用随机傅里叶特征原理介绍了一种高效，可扩展且具有鲁棒性的基于核的DFM版本。

    Quantification learning deals with the task of estimating the target label distribution under label shift. In this paper, we first present a unifying framework, distribution feature matching (DFM), that recovers as particular instances various estimators introduced in previous literature. We derive a general performance bound for DFM procedures, improving in several key aspects upon previous bounds derived in particular cases. We then extend this analysis to study robustness of DFM procedures in the misspecified setting under departure from the exact label shift hypothesis, in particular in the case of contamination of the target by an unknown distribution. These theoretical findings are confirmed by a detailed numerical study on simulated and real-world datasets. We also introduce an efficient, scalable and robust version of kernel-based DFM using the Random Fourier Feature principle.
    
[^95]: Quick-Tune：快速学习应该使用哪个预训练模型以及如何微调它

    Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How. (arXiv:2306.03828v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.03828](http://arxiv.org/abs/2306.03828)

    本文提出一种方法快速选择最佳的预训练模型和微调超参数，通过生成大规模元数据集并元学习多保真度性能预测器，并在学习新数据集时使用该预测器进行超参数优化，可以快速实现此目标。

    

    随着预训练模型数量的不断增加，机器学习从业者不断面临一个问题：应该使用哪个预训练模型以及该如何微调它以适应新的数据集。本文提出了一种方法，联合搜索最佳的预训练模型和微调超参数。我们的方法通过评估超过20k个超参数配置在87个数据集上微调24个预训练图像分类模型来生成大规模元数据集，并在其学习曲线上元学习多保真度性能预测器，以用于快速超参数优化。我们的实证研究表明，我们的方法能够快速选择一个准确的预训练模型并找到它的最佳超参数。

    With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a multi-fidelity performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.
    
[^96]: LLM-Blender: 利用成对排名和生成融合集成大型语言模型

    LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02561](http://arxiv.org/abs/2306.02561)

    本论文提出了LLM-Blender，它是一个集成框架，旨在利用不同的开源大型语言模型的优秀特性，实现始终如一的卓越性能。PairRanker和GenFuser是该框架的两个模块，PairRanker使用成对比较方法来区分候选输出，并且GenFuser旨在合并排名最高的候选者，以生成改进的输出。

    

    本论文提出了LLM-Blender，一个集成框架，旨在通过利用多个开源大型语言模型（LLMs）的不同优势来达到始终如一的卓越性能。我们的框架由两个模块组成：PairRanker和GenFuser，以应对不同示例的最优LLMs可以显着变化的观察。PairRanker使用专门的成对比较方法来区分候选输出之间的微小差异。它联合编码输入文本和一对候选者，使用交叉注意编码器来确定优越者。我们的结果表明，PairRanker与ChatGPT的排名相关性最高。然后，GenFuser旨在合并排名最高的候选者，通过利用它们的优势和减少它们的弱点来生成改进的输出。为了促进大规模评估，我们介绍了一个基准数据集MixInstruct，它是多个指令数据集的混合，具有oracle p。

    We present LLM-Blender, an ensembling framework designed to attain consistently superior performance by leveraging the diverse strengths of multiple open-source large language models (LLMs). Our framework consists of two modules: PairRanker and GenFuser, addressing the observation that optimal LLMs for different examples can significantly vary. PairRanker employs a specialized pairwise comparison method to distinguish subtle differences between candidate outputs. It jointly encodes the input text and a pair of candidates, using cross-attention encoders to determine the superior one. Our results demonstrate that PairRanker exhibits the highest correlation with ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates, generating an improved output by capitalizing on their strengths and mitigating their weaknesses. To facilitate large-scale evaluation, we introduce a benchmark dataset, MixInstruct, which is a mixture of multiple instruction datasets featuring oracle p
    
[^97]: 基于GNN和核均值嵌入的原子模拟传递学习

    Transfer learning for atomistic simulations using GNNs and kernel mean embeddings. (arXiv:2306.01589v1 [cs.LG])

    [http://arxiv.org/abs/2306.01589](http://arxiv.org/abs/2306.01589)

    本论文提出了一种传递学习算法，利用图神经网络和核均值嵌入在原子模拟中学习了势能表面。该方法在现实数据集上表现良好，展现出较好的可概括性和可转移性能。

    

    使用机器学习方法学习的原子相互作用势在原子模拟中得到了成功的应用。然而，深度学习管道需要大量数据，而生成参考计算是计算上要求很高的。为了克服这一困难，我们提出了一种传递学习算法，利用了图神经网络（GNNs）在描述化学环境方面的能力，以及核均值嵌入。我们从预先在OC20数据集上进行过训练的GNN中提取特征映射，并使用它来从催化过程的系统特定数据集中学习势能表面。我们的方法进一步通过灵活的核函数来增强，该核函数包括化学物种信息，从而提高了性能和可解释性。我们在一系列逐渐复杂的现实数据集上测试了我们的方法，展示了出色的概括能力和可转移性能，改进了依赖GNNs或岭回归方法的方法。

    Interatomic potentials learned using machine learning methods have been successfully applied to atomistic simulations. However, deep learning pipelines are notoriously data-hungry, while generating reference calculations is computationally demanding. To overcome this difficulty, we propose a transfer learning algorithm that leverages the ability of graph neural networks (GNNs) in describing chemical environments, together with kernel mean embeddings. We extract a feature map from GNNs pre-trained on the OC20 dataset and use it to learn the potential energy surface from system-specific datasets of catalytic processes. Our method is further enhanced by a flexible kernel function that incorporates chemical species information, resulting in improved performance and interpretability. We test our approach on a series of realistic datasets of increasing complexity, showing excellent generalization and transferability performance, and improving on methods that rely on GNNs or ridge regression 
    
[^98]: 离线训练，在线测试：一个真实的机器人学习基准

    Train Offline, Test Online: A Real Robot Learning Benchmark. (arXiv:2306.00942v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2306.00942](http://arxiv.org/abs/2306.00942)

    这是一个提供离线训练和在线测试的机器人学习基准系统，通过共享机器人硬件和开源数据集，解决了机器人学习研究中的挑战，并为未来的研究提供了方便和直接的比较方法。

    

    机器人学习研究受到三个挑战的限制：机器人昂贵（很少有实验室可以参与），每个人使用不同的机器人（研究结果在实验室之间不具有普遍性），我们缺乏互联网规模的机器人数据。通过一个新的基准系统：离线训练、在线测试（TOTO），我们解决了这些挑战。TOTO为远程用户提供共享机器人硬件以评估常见任务上的方法，并提供这些任务的开源数据集进行离线训练。它的操作任务套件需要在看不见的对象、位置和光照条件下进行挑战性的泛化。我们通过TOTO在五个机构远程贡献的，比较了五个预训练视觉表示和四个离线策略学习基线的初步结果。然而，TOTO真正的潜力在于未来：我们发布了这个基准系统，任何用户都可以提交进一步的研究成果，轻松地直接与多种方法进行比较，无需获得硬件或收集数据。

    Three challenges limit the progress of robot learning research: robots are expensive (few labs can participate), everyone uses different robots (findings do not generalize across labs), and we lack internet-scale robotics data. We take on these challenges via a new benchmark: Train Offline, Test Online (TOTO). TOTO provides remote users with access to shared robotic hardware for evaluating methods on common tasks and an open-source dataset of these tasks for offline training. Its manipulation task suite requires challenging generalization to unseen objects, positions, and lighting. We present initial results on TOTO comparing five pretrained visual representations and four offline policy learning baselines, remotely contributed by five institutions. The real promise of TOTO, however, lies in the future: we release the benchmark for additional submissions from any user, enabling easy, direct comparison to several methods without the need to obtain hardware or collect data.
    
[^99]: 从单个快照中重建图扩散历史

    Reconstructing Graph Diffusion History from a Single Snapshot. (arXiv:2306.00488v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.00488](http://arxiv.org/abs/2306.00488)

    本文研究了从单个快照中重建图扩散历史的问题，揭示了现有方法的局限性，并提出了一种新的方法。

    

    图扩散在许多重要应用中普遍存在。在这些应用中，完整的扩散历史在确定动态模式、反思预防措施和预测干预效果方面起着至关重要的作用。尽管它们很重要，但完整的扩散历史很少可用，并且由于病态、爆炸性的搜索空间和训练数据的缺乏而具有极高的挑战性。迄今为止，很少有方法用于扩散历史重建。它们仅基于最大似然估计（MLE）公式，需要知道真实的扩散参数。在本文中，我们研究了一个更难的问题，即从单个快照（DASH）中重建扩散历史，我们试图仅从最终快照重建历史，而不知道真实的扩散参数。我们从理论分析开始，揭示了MLE公式的基本限制。

    Diffusion on graphs is ubiquitous with numerous high-impact applications. In these applications, complete diffusion histories play an essential role in terms of identifying dynamical patterns, reflecting on precaution actions, and forecasting intervention effects. Despite their importance, complete diffusion histories are rarely available and are highly challenging to reconstruct due to ill-posedness, explosive search space, and scarcity of training data. To date, few methods exist for diffusion history reconstruction. They are exclusively based on the maximum likelihood estimation (MLE) formulation and require to know true diffusion parameters. In this paper, we study an even harder problem, namely reconstructing Diffusion history from A single SnapsHot} (DASH), where we seek to reconstruct the history from only the final snapshot without knowing true diffusion parameters. We start with theoretical analyses that reveal a fundamental limitation of the MLE formulation. We prove: (a) est
    
[^100]: 解释正则化的高维模型的代表点选择

    Representer Point Selection for Explaining Regularized High-dimensional Models. (arXiv:2305.20002v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.20002](http://arxiv.org/abs/2305.20002)

    我们提出了一种解释高维模型预测的新方法，通过代表点选择来解释每个训练样本的重要性权重。我们的方法可以适用于各种正则化模型，并在协同过滤领域中有具体应用。

    

    我们介绍了一种新颖的基于样本的解释方法，称为高维代表点，可以用于解释正则化的高维模型对每个训练样本的重要性权重。我们提出了一个适用于一般正则化高维模型的新型代表定理，该定理将模型的预测分解为每个训练样本的贡献：正（负）值对应于正（负）影响训练样本对模型预测的影响。我们推导了$\ell_1$正则化稀疏模型和核范数正则化低秩模型的经典实例的结果。作为一个案例研究，我们进一步研究了在协同过滤的背景下低秩模型的应用，其中我们为特定的流行模型类实例化了高维代表点。最后，我们在三个实际的二进制分类问题上研究了我们提出的方法的实证性表现。

    We introduce a novel class of sample-based explanations we term high-dimensional representers, that can be used to explain the predictions of a regularized high-dimensional model in terms of importance weights for each of the training samples. Our workhorse is a novel representer theorem for general regularized high-dimensional models, which decomposes the model prediction in terms of contributions from each of the training samples: with positive (negative) values corresponding to positive (negative) impact training samples to the model's prediction. We derive consequences for the canonical instances of $\ell_1$ regularized sparse models, and nuclear norm regularized low-rank models. As a case study, we further investigate the application of low-rank models in the context of collaborative filtering, where we instantiate high-dimensional representers for specific popular classes of models. Finally, we study the empirical performance of our proposed methods on three real-world binary cla
    
[^101]: W-procer: 基于加权原型对比学习的医学少样本命名实体识别

    W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v1 [cs.CL])

    [http://arxiv.org/abs/2305.18624](http://arxiv.org/abs/2305.18624)

    W-procer是一种基于加权原型对比学习的医学少样本命名实体识别方法，在构建基于原型的对比损失和加权网络方面具有创新性，优于现有的最先进方法。

    

    对比学习已成为少样本命名实体识别（NER）的一种受欢迎的解决方案。传统配置力求减少具有相同标签的标记之间的距离，并增加具有不同标签的标记之间的距离。然而，在医学领域中存在大量被注释为“O”（即“OUTSIDE”）的实体，并且它们不希望被推离到当前对比学习方法标记为“O”以外的其他实体，这种设定效果不佳，可能会得出含有噪声原型标签的语义表示，尽管存在许多“O”标签实体与有标签实体相关。为解决这个挑战，我们提出了一种名为医学少样本命名实体识别中基于加权原型的对比学习方法（W-PROCER）。我们的方法主要围绕构建基于原型的对比损失和加权网络展开。这些组件在协助在医学领域中的迁移学习方面发挥了至关重要的作用。在实验中，我们将W-PROCER应用于一个公共的医学数据集，并展示了其相对于现有的最先进方法的优异表现。

    Contrastive learning has become a popular solution for few-shot Name Entity Recognization (NER). The conventional configuration strives to reduce the distance between tokens with the same labels and increase the distance between tokens with different labels. The effect of this setup may, however, in the medical domain, there are a lot of entities annotated as OUTSIDE (O), and they are undesirably pushed apart to other entities that are not labeled as OUTSIDE (O) by the current contrastive learning method end up with a noisy prototype for the semantic representation of the label, though there are many OUTSIDE (O) labeled entities are relevant to the labeled entities. To address this challenge, we propose a novel method named Weighted Prototypical Contrastive Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our approach primarily revolves around constructing the prototype-based contractive loss and weighting network. These components play a crucial role in assisting t
    
[^102]: 扫描与拍照：理解1层Transformer中的训练动态和标记组成

    Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])

    [http://arxiv.org/abs/2305.16380](http://arxiv.org/abs/2305.16380)

    本文分析了1层Transformer在下一个标记预测任务中的SGD训练动态，证明了自我关注层充当了“区分性扫描算法”，从而逐步关注到相关标记并排除不相关的标记，总结相关信息在编码表示中。同时研究了标记频率、上下文和初始化自我关注层等对Transformer性能的影响。

    

    Transformer架构在多个研究领域表现出了惊人的性能，并成为许多神经网络模型的基础。然而，我们对其如何工作的理解仍然有限。特别是，通过简单的预测性损失，表示如何从梯度训练动态中出现仍然是一个谜。在本文中，针对具有一个自我关注层和一个解码器层的1层Transformer，我们以数学严谨的方式分析其在下一个标记预测任务中的SGD训练动态。我们打开了自我关注层组合输入标记的动态过程的黑盒子，并揭示了底层归纳偏差的本质。具体而言，在没有位置编码、长输入序列和解码器层学习速度快于自我关注层的假设下，我们证明了自我关注层充当了“区分性扫描算法”：从均匀注意力开始，它逐渐关注到相关标记，排除不相关的标记，直到所有相关信息被扫描并总结在编码表示中。我们的分析还显示了标记频率和上下文如何影响注意权重，以及自我关注层初始化如何影响收敛速度。

    Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
    
[^103]: 针对领域泛化的异质性量化和对比探索

    Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization. (arXiv:2305.15889v1 [cs.LG])

    [http://arxiv.org/abs/2305.15889](http://arxiv.org/abs/2305.15889)

    本文提出了一种Feature Heterogeneity Distance(FHD)度量标准来衡量领域的异质性，并引入了一个新的实验模式Contrastive Convergence for Domain Generalization (CCDG) 来寻找最佳的监督信号来提高泛化。实验表明，我们的方法比其他最先进的方法更加有效和优越。

    

    领域泛化(DG)是现实世界应用中普遍存在的问题，通过利用几个源域训练出对未见过目标域进行有效泛化的模型。由于领域标签-即每个数据点来自哪个域自然存在，因此大多数DG算法将它们作为一种监督信息来提高泛化性能。然而，由于领域之间缺乏异质性，即领域之间的差异，原始的域标签可能不是最佳的监督信号。本文提出了一种新的度量标准Feature Heterogeneity Distance(FHD)来衡量领域的异质性，并引入一个新的实验模式CCDG，用于寻找最佳的监督信号来提高泛化。大量实验和消融研究表明，我们所提出的FHD度量标准和CCDG模式比其他最先进的方法更加有效和优越。

    Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under 
    
[^104]: GUARD: 一个安全强化学习基准测试平台

    GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v1 [cs.LG])

    [http://arxiv.org/abs/2305.13681](http://arxiv.org/abs/2305.13681)

    GUARD是一个广义统一安全强化学习开发基准测试平台，是目前广泛遍布且包含各种RL代理、任务和安全约束规范的一站式基准测试，能够全面涵盖最先进的安全RL算法，并具有高度的可自定义性。

    

    由于试错的性质，将RL算法应用于安全关键的现实应用（例如自动驾驶、人机交互、机器人操作等）通常是具有挑战性的，因为这些错误是不可容忍的。最近，安全RL（即约束RL）已经在文献中迅速出现，其中代理在满足约束条件的同时，探索环境。由于算法和任务的多样性，比较现有的安全RL算法仍然很困难。为了填补这一空白，我们介绍了GUARD，一个广义统一安全强化学习开发基准测试平台。与现有基准相比，GUARD具有几个优点。首先，GUARD是一个广义基准测试平台，具有各种RL代理、任务和安全约束规范。其次，GUARD全面涵盖了最先进的安全RL算法，并具有自包含的实现。第三，GUARD在任务和算法方面具有高度的可自定义性。我们提供了状态下现有方法在GUARD上的基准测试结果。

    Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state
    
[^105]: Restore Anything Pipeline: Segment Anything Meets Image Restoration.

    Restore Anything Pipeline: Segment Anything Meets Image Restoration. (arXiv:2305.13093v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2305.13093](http://arxiv.org/abs/2305.13093)

    本文提出了一种新颖的交互式和基于对象级别的图像恢复方法，即Restore Anything Pipeline (RAP)，该方法通过将图像分割技术与可控的图像恢复模型相结合，为多个图像恢复任务创建了一个用户友好的流程。

    

    近期的图像恢复方法使用深度学习取得了显著的进展。然而，现有方法往往将整个图像视为一个单一实体，无法考虑到图像中表现出个体纹理属性的不同对象。现有方法通常生成单一结果，可能无法满足不同用户的偏好。本文介绍了一种新颖的交互式和基于对象级别的图像恢复方法，即Restore Anything Pipeline (RAP)，该方法采用可控模型来生成用户可以选择的不同结果。RAP将最近的Segment Anything Model (SAM)的图像分割技术与可控的图像恢复模型相结合，为多个图像恢复任务创建了一个用户友好的流程。我们通过将RAP应用于图像去模糊、图像去噪和JPEG伪影去除这三个常见的图像恢复任务来展示其多功能性。实验结果表明，RAP能够在不同图像恢复任务上表现出色。

    Recent image restoration methods have produced significant advancements using deep learning. However, existing methods tend to treat the whole image as a single entity, failing to account for the distinct objects in the image that exhibit individual texture properties. Existing methods also typically generate a single result, which may not suit the preferences of different users. In this paper, we introduce the Restore Anything Pipeline (RAP), a novel interactive and per-object level image restoration approach that incorporates a controllable model to generate different results that users may choose from. RAP incorporates image segmentation through the recent Segment Anything Model (SAM) into a controllable image restoration model to create a user-friendly pipeline for several image restoration tasks. We demonstrate the versatility of RAP by applying it to three common image restoration tasks: image deblurring, image denoising, and JPEG artifact removal. Our experiments show that RAP p
    
[^106]: 面向三维MOT中的点云目标再识别

    Towards Object Re-Identification from Point Clouds for 3D MOT. (arXiv:2305.10210v1 [cs.CV])

    [http://arxiv.org/abs/2305.10210](http://arxiv.org/abs/2305.10210)

    该论文研究面向三维MOT中的点云再识别问题，提出了一种轻量级匹配头用于点云ReID的网络，通过实验结果表明，随着传感器分辨率的提高和观测点密度的增加，点云ReID的表现逐渐接近于图像ReID。

    

    本研究旨在通过学习从剪裁的点云观测中匹配对象对（例如使用其预测的三维边界框）来解决三维多目标跟踪（MOT）上的对象再识别（ReID）问题。 我们不关心三维MOT的SOTA性能，而是追求回答以下问题：在实际的跟踪检测环境中，与图片中的ReID相比，来自点云的对象ReID的表现如何？ 为了实现这样的研究，我们提出了一个可以连接到任何集合或序列处理骨干（例如PointNet或ViT）的轻量级匹配头，为两种模态创造可比较的对象ReID网络家族。在孪生样式下运行，我们提出的点云ReID网络可以在实时（10 hz）中进行数千个成对比较。我们的研究结果表明，其表现随着更高的传感器分辨率而提高，并在观测足够密集时接近图像ReID的表现。

    In this work, we study the problem of object re-identification (ReID) in a 3D multi-object tracking (MOT) context, by learning to match pairs of objects from cropped (e.g., using their predicted 3D bounding boxes) point cloud observations. We are not concerned with SOTA performance for 3D MOT, however. Instead, we seek to answer the following question: In a realistic tracking by-detection context, how does object ReID from point clouds perform relative to ReID from images? To enable such a study, we propose a lightweight matching head that can be concatenated to any set or sequence processing backbone (e.g., PointNet or ViT), creating a family of comparable object ReID networks for both modalities. Run in siamese style, our proposed point-cloud ReID networks can make thousands of pairwise comparisons in real-time (10 hz). Our findings demonstrate that their performance increases with higher sensor resolution and approaches that of image ReID when observations are sufficiently dense. Ad
    
[^107]: 超流体的神经波函数研究

    Neural Wave Functions for Superfluids. (arXiv:2305.06989v1 [cond-mat.quant-gas])

    [http://arxiv.org/abs/2305.06989](http://arxiv.org/abs/2305.06989)

    本论文利用费米神经网络波函数方法研究了均匀费米气体超流，提出一种针对FermiNet模型的改进方法，获得了极其准确的结果。

    

    理解超流性仍然是凝聚态物理的一个主要目标。在这里，我们利用最近开发的费米神经网络（FermiNet）波函数Ansatz进行变分蒙特卡洛计算来解决这一挑战。我们研究了一个具有强烈短程双体相互作用的系统-- 均匀费米气体，该系统已知存在超流基态，但难以定量描述。我们展示了在研究均匀费米气体时FermiNet Ansatz的关键局限性，并提出了一种简单的修改，其表现显著优于原始FermiNet，可以给出高度准确的结果。我们数学证明了新的Ansatz是原始FermiNet体系结构的严格概括，尽管使用的参数更少。我们的方法与FermiNet共享几个优势:使用神经网络消除了底层基组的需求;网络的灵活性在变分量子Monte Carlo中产生了极其准确的结果。

    Understanding superfluidity remains a major goal of condensed matter physics. Here we tackle this challenge utilizing the recently developed Fermionic neural network (FermiNet) wave function Ansatz for variational Monte Carlo calculations. We study the unitary Fermi gas, a system with strong, short-range, two-body interactions known to possess a superfluid ground state but difficult to describe quantitively. We demonstrate key limitations of the FermiNet Ansatz in studying the unitary Fermi gas and propose a simple modification that outperforms the original FermiNet significantly, giving highly accurate results. We prove mathematically that the new Ansatz is a strict generalization of the original FermiNet architecture, despite the use of fewer parameters. Our approach shares several advantanges with the FermiNet: the use of a neural network removes the need for an underlying basis set; and the flexiblity of the network yields extremely accurate results within a variational quantum Mon
    
[^108]: 如何为推荐基础模型索引项目ID

    How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])

    [http://arxiv.org/abs/2305.06569](http://arxiv.org/abs/2305.06569)

    本研究对推荐基础模型的项目索引问题进行了系统检查，提出了一种新的上下文感知索引方法，该方法在项目推荐准确性和文本生成质量方面具有优势。

    

    推荐基础模型将推荐任务转换为自然语言任务，利用大型语言模型（LLM）进行推荐。它通过直接生成建议的项目而不是计算传统推荐模型中每个候选项目的排名得分，简化了推荐管道，避免了多段过滤的问题。为了避免在决定要推荐哪些项目时生成过长的文本，为推荐基础模型创建LLM兼容的项目ID是必要的。本研究系统地研究了推荐基础模型的项目索引问题，以P5为代表的主干模型，并使用各种索引方法复制其结果。我们首先讨论了几种微不足道的项目索引方法（如独立索引、标题索引和随机索引）的问题，并表明它们不适用于推荐基础模型，然后提出了一种新的索引方法，称为上下文感知索引。我们表明，这种索引方法在项目推荐准确性和文本生成质量方面优于其他索引方法。

    Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
    
[^109]: 使用贝叶斯模型平均分析社交媒体上的气候宣传活动

    Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])

    [http://arxiv.org/abs/2305.06174](http://arxiv.org/abs/2305.06174)

    本文分析了工业、倡导组织和气候倡导组织在社交媒体上如何影响气候变化的叙事，并提出了一个最小化监督模型组合方法，用于识别Facebook上气候广告的立场。

    

    气候变化是我们时代的核心问题，我们正处于一个关键时刻。各种利益集团、社会运动组织和个人在社交媒体上开展针对这个问题的集体行动。此外，社交媒体上的问题倡导活动往往是针对当前社会关注的问题，特别是能源行业面临的问题。本文的目标是分析工业、倡导组织和气候倡导组织如何利用社交媒体影响气候变化的叙事。在这项工作中，我们提出了一个最小化监督模型组合方法，并结合消息主题来识别Facebook上气候广告的立场。最后，我们发布了我们的立场数据集、模型和与气候宣传活动相关的主题，供未来的舆情挖掘和自动检测气候变化立场的研究使用。

    Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [56] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.
    
[^110]: 关于锐度感知优化与对抗鲁棒性之间的关系

    On the Relation between Sharpness-Aware Minimization and Adversarial Robustness. (arXiv:2305.05392v1 [cs.LG])

    [http://arxiv.org/abs/2305.05392](http://arxiv.org/abs/2305.05392)

    SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗性能。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。SAM单独使用可以在不牺牲清晰度精度的情况下提高对抗鲁棒性。

    

    我们在对抗鲁棒性的背景下提出了对锐度感知优化（SAM）的新理解。本文指出，SAM和对抗性训练（AT）都可以视为特定的特征扰动，其改善了对抗鲁棒性。然而，SAM和AT在扰动强度方面是不同的，从而带来了不同的精度和鲁棒性权衡。在一个简化模型中，我们提供了这些声明的理论证据和严格的数学证明。此外，我们进行了实验证明，仅利用SAM可以实现比标准训练更好的对抗鲁棒性，这是意外的好处。由于对抗训练可能会导致清晰度精度的降低，我们展示了仅使用SAM可以在不牺牲清晰度精度的情况下提高鲁棒性。源代码可在https://github.com/weizeming/SAM_AT获取。

    We propose a novel understanding of Sharpness-Aware Minimization (SAM) in the context of adversarial robustness. In this paper, we point out that both SAM and adversarial training (AT) can be viewed as specific feature perturbations, which improve adversarial robustness. However, we note that SAM and AT are distinct in terms of perturbation strength, leading to different accuracy and robustness trade-offs. We provide theoretical evidence for these claims in a simplified model with rigorous mathematical proofs. Furthermore, we conduct experiment to demonstrate that only utilizing SAM can achieve superior adversarial robustness compared to standard training, which is an unexpected benefit. As adversarial training can suffer from a decrease in clean accuracy, we show that using SAM alone can improve robustness without sacrificing clean accuracy. Code is available at https://github.com/weizeming/SAM_AT.
    
[^111]: 物理知识的局部化学习用于对流扩散反应系统

    Physics-Informed Localized Learning for Advection-Diffusion-Reaction Systems. (arXiv:2305.03774v1 [cs.LG])

    [http://arxiv.org/abs/2305.03774](http://arxiv.org/abs/2305.03774)

    提出一种新的物理知识的、边界条件感知的、局部化学习方法，将E2C和E2CO模型扩展到对流扩散反应系统中，可以以极高的准确性预测系统未来状态，同时大幅减少训练时间。

    

    全球对新能源解决方案的追求，如地热和碳捕集和封存计划，对当前先进的地下流体模拟器产生了新的需求。要求能够在短时间内同时模拟大量储层状态，为应用机器学习技术进行代理建模开辟了机会。我们提出了一种新的局部学习方法，该方法具有物理知识和边界条件意识，将嵌入式控制（E2C）和嵌入式控制和观察（E2CO）模型扩展到学习对流扩散反应系统中全局状态变量的局部表示。我们展示了我们的模型在储层模拟数据上训练，能够以只使用少量可用的信息，高度准确地预测系统的未来状态，同时与原始的E2C和E2C

    The global push for new energy solutions, such as Geothermal, and Carbon Capture and Sequestration initiatives has thrust new demands upon the current state-of the-art subsurface fluid simulators. The requirement to be able to simulate a large order of reservoir states simultaneously in a short period of time has opened the door of opportunity for the application of machine learning techniques for surrogate modelling. We propose a novel physics-informed and boundary conditions-aware Localized Learning method which extends the Embed-to-Control (E2C) and Embed-to-Control and Observed (E2CO) models to learn local representations of global state variables in an Advection-Diffusion Reaction system. We show that our model trained on reservoir simulation data is able to predict future states of the system, given a set of controls, to a great deal of accuracy with only a fraction of the available information, while also reducing training times significantly compared to the original E2C and E2C
    
[^112]: BrainNPT：用于脑网络分类的Transformer网络的预训练

    BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v1 [q-bio.NC])

    [http://arxiv.org/abs/2305.01666](http://arxiv.org/abs/2305.01666)

    本文提出了一种名为BrainNPT的基于Transformer的神经网络，用于脑功能网络分类，并提出了两种预训练策略，利用未标记的脑网络数据来学习结构。

    

    近年来，深度学习方法在脑成像分析方面的进展迅速，但往往受到有限标记数据的限制。在未标记数据上预训练的模型已在许多领域中展示了有前景的特征学习改进，包括自然语言处理和计算机视觉。然而，在脑网络分析中，这种技术尚未得到充分探索。在本文中，我们以Transformer网络为基础的预训练方法为重点，利用现有的未标记数据进行脑功能网络分类。首先，我们提出了一种基于Transformer的神经网络，名为BrainNPT，用于脑功能网络分类。所提出的方法利用<cls>标记作为分类嵌入向量，以便于Transformer模型有效地捕获脑网络的表示。其次，我们提出了两种预训练策略的预训练架构，用于BrainNPT模型，以利用未标记的脑网络数据来学习结构。

    Deep learning methods have advanced quickly in brain imaging analysis over the past few years, but they are usually restricted by the limited labeled data. Pre-trained model on unlabeled data has presented promising improvement in feature learning in many domains, including natural language processing and computer vision. However, this technique is under-explored in brain network analysis. In this paper, we focused on pre-training methods with Transformer networks to leverage existing unlabeled data for brain functional network classification. First, we proposed a Transformer-based neural network, named as BrainNPT, for brain functional network classification. The proposed method leveraged <cls> token as a classification embedding vector for the Transformer model to effectively capture the representation of brain network. Second, We proposed a pre-training architecture with two pre-training strategies for BrainNPT model to leverage unlabeled brain network data to learn the structure in
    
[^113]: 基于深度学习的口罩佩戴检测在COVID-19大流行期间的应用

    Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV])

    [http://arxiv.org/abs/2305.00068](http://arxiv.org/abs/2305.00068)

    本研究探讨了在 COVID-19 疫情期间使用深度学习模型进行口罩佩戴检测的可行性。通过比较不同模型，选择了适用于实时和移动设备应用的最佳模型，并取得了高准确度。

    

    在COVID-19疫情期间，佩戴口罩已被知晓为预防病毒传播的有效方法之一。深度学习模型的出色性能取代了人类在许多监控任务中的角色。监控口罩佩戴就是这样一项可以由深度学习模型完成的任务。 由于隔离的原因，面部口罩照片的数量有限，因此是这项任务的主要挑战。本文使用三种最先进的目标检测神经网络模型对口罩检测进行了研究，包括 Single Shot Detector（SSD）、两个版本的 You Only Look Once。根据不同模型的表现，选择最适合在现实世界和移动设备应用中使用的模型。实验结果表明，所提出的方法实现了用于实时和移动设备应用的高准确度。

    During the COVID-19 pandemic, wearing a face mask has been known to be an effective way to prevent the spread of COVID-19. In lots of monitoring tasks, humans have been replaced with computers thanks to the outstanding performance of the deep learning models. Monitoring the wearing of a face mask is another task that can be done by deep learning models with acceptable accuracy. The main challenge of this task is the limited amount of data because of the quarantine. In this paper, we did an investigation on the capability of three state-of-the-art object detection neural networks on face mask detection for real-time applications. As mentioned, here are three models used, Single Shot Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny, and YOLOv4-tiny-3l from which the best was selected. In the proposed method, according to the performance of different models, the best model that can be suitable for use in real-world and mobile device applications in comparison to
    
[^114]: 实现高效和全面的城市时空预测：一个统一的库和性能基准

    Towards Efficient and Comprehensive Urban Spatial-Temporal Prediction: A Unified Library and Performance Benchmark. (arXiv:2304.14343v1 [cs.LG])

    [http://arxiv.org/abs/2304.14343](http://arxiv.org/abs/2304.14343)

    本研究提出了一种称为原子文件的统一空间时间数据存储格式，开发了一个名为LibCity的开源库，重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。同时还提出了城市时空预测模型的性能基准，为这一领域提供了一个可靠的评估工具。

    

    随着深度学习技术的不断推进和城市时空数据的积累，越来越多的深度学习模型被提出来解决城市时空预测问题。然而，现有领域存在许多限制，包括开放数据以各种格式存在，使用困难，极少数论文公开其代码和数据，以及开源模型经常使用不同的框架和平台，使得比较具有挑战性。迫切需要一个统一的框架来实施和评估这些方法。为解决这些问题，我们提供了一个城市时空预测的综合评估，并提出了一种称为原子文件的统一空间时间数据存储格式。我们还提出了一个名为LibCity的开源库，为研究人员提供了一个可靠的实验工具和一个方便的开发框架。在这个库中，我们已经重新构建了65个空间时间预测模型，并收集了55个空间时间数据集。此外，我们还引入了一个城市时空预测模型性能基准，包括效率和有效性度量，以进行公平比较。在这个基准上的实验结果证明了我们提出的统一库和基准的有用性和有效性。

    As deep learning technology advances and more urban spatial-temporal data accumulates, an increasing number of deep learning models are being proposed to solve urban spatial-temporal prediction problems. However, there are limitations in the existing field, including open-source data being in various formats and difficult to use, few papers making their code and data openly available, and open-source models often using different frameworks and platforms, making comparisons challenging. A standardized framework is urgently needed to implement and evaluate these methods. To address these issues, we provide a comprehensive review of urban spatial-temporal prediction and propose a unified storage format for spatial-temporal data called atomic files. We also propose LibCity, an open-source library that offers researchers a credible experimental tool and a convenient development framework. In this library, we have reproduced 65 spatial-temporal prediction models and collected 55 spatial-temp
    
[^115]: 关于洛克斯洞穴的流形学习：关于流形学习和物理现象的评论（arXiv:2304.14248v1 [stat.ML]）

    On Manifold Learning in Plato's Cave: Remarks on Manifold Learning and Physical Phenomena. (arXiv:2304.14248v1 [stat.ML])

    [http://arxiv.org/abs/2304.14248](http://arxiv.org/abs/2304.14248)

    本文通过一个警示故事阐释了分析数据时，测量几何和底层现象几何差异带来的问题，以及这种差异在某些情况下如何导致对一个修正过的问题给出错误答案。这些问题适用于降维和无监督学习领域。

    

    许多机器学习技术尝试通过测量不需要对物理现象或测量设备进行显式建模的低维流形结构来推断潜在物理现象的低维流形结构，这篇论文提出了关于测量几何和底层现象几何之间差异的警示故事。在普通情况下，这篇论文所展示的度量形变在数学上是直接而不可避免的，并且它只是数个类似效应中的一个。虽然这并不总是出现问题，但我们提供了一个标准且无害数据处理过程的例子，其中这种影响导致对一个看似简单的问题给出了错误的答案。尽管我们关注流形学习，但这些问题广泛适用于降维和无监督学习领域。

    Many techniques in machine learning attempt explicitly or implicitly to infer a low-dimensional manifold structure of an underlying physical phenomenon from measurements without an explicit model of the phenomenon or the measurement apparatus. This paper presents a cautionary tale regarding the discrepancy between the geometry of measurements and the geometry of the underlying phenomenon in a benign setting. The deformation in the metric illustrated in this paper is mathematically straightforward and unavoidable in the general case, and it is only one of several similar effects. While this is not always problematic, we provide an example of an arguably standard and harmless data processing procedure where this effect leads to an incorrect answer to a seemingly simple question. Although we focus on manifold learning, these issues apply broadly to dimensionality reduction and unsupervised learning.
    
[^116]: UNADON：用于预测全基因组染色体空间位置的基于Transformer的模型

    UNADON: Transformer-based model to predict genome-wide chromosome spatial position. (arXiv:2304.13230v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.13230](http://arxiv.org/abs/2304.13230)

    UNADON是一种基于Transformer的深度学习模型，可以预测全基因组的染色体空间位置。通过使用序列特征和表观遗传信号，UNADON在训练单个细胞系时能够高度准确地预测染色质空间定位到核体，并揭示了潜在影响染色质区隔的序列和表观遗传因素。

    

    染色体相对于功能性核体的空间定位与基因组功能（如转录）密切相关。然而，影响全基因组范围内染色质空间定位的序列模式和表观遗传特征尚未被很好地理解。在这里，我们开发了一种新的基于Transformer的深度学习模型UNADON，它使用序列特征和表观遗传信号，预测了通过TSA-seq测量的特定类型核体的全基因组细胞学距离。在四种细胞系（K562，H1，HFFc6，HCT116）中对UNADON的评估表明，在单个细胞系训练时，高度准确地预测了染色质空间定位到核体。UNADON在未见过的细胞类型中表现良好。重要的是，我们揭示了影响大尺度染色质区隔到核体的潜在序列和表观遗传因素。综上，UNADON为了解基因组中的序列特征和染色质空间定位的原理提供了新的见解，并为研究核的功能组织提供了有价值的工具。

    The spatial positioning of chromosomes relative to functional nuclear bodies is intertwined with genome functions such as transcription. However, the sequence patterns and epigenomic features that collectively influence chromatin spatial positioning in a genome-wide manner are not well understood. Here, we develop a new transformer-based deep learning model called UNADON, which predicts the genome-wide cytological distance to a specific type of nuclear body, as measured by TSA-seq, using both sequence features and epigenomic signals. Evaluations of UNADON in four cell lines (K562, H1, HFFc6, HCT116) show high accuracy in predicting chromatin spatial positioning to nuclear bodies when trained on a single cell line. UNADON also performed well in an unseen cell type. Importantly, we reveal potential sequence and epigenomic factors that affect large-scale chromatin compartmentalization to nuclear bodies. Together, UNADON provides new insights into the principles between sequence features a
    
[^117]: 潜在生成模型中的潜在遍历作为潜在流的潜在路线

    Latent Traversals in Generative Models as Potential Flows. (arXiv:2304.12944v1 [cs.LG])

    [http://arxiv.org/abs/2304.12944](http://arxiv.org/abs/2304.12944)

    该论文使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行，以实现解缠，并通过分类器进行约束。

    

    尽管深度生成模型的最近进展已经取得了显著进展，但它们的潜在空间的基本结构仍然很不好理解，因此执行语义上有意义的潜在遍历的任务仍然是一个开放的研究挑战。大多数之前的工作旨在通过线性建模潜在结构，并找到相应的线性方向，从而产生“解缠”的代数。在这项工作中，我们提议改为使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行。这些潜在景观受到物理学、最优运输和神经科学的启发，被作为物理上现实的偏微分方程来学习，从而允许它们在空间和时间上具有灵活性。为了实现解缠，同时学习了多个势能，并通过分类器进行约束，使其具有明显差异且在语义上自我一致。

    Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consisten
    
[^118]: 通过Paired-Logits反演攻击恢复图像的FedMD

    Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v1 [cs.CR])

    [http://arxiv.org/abs/2304.11436](http://arxiv.org/abs/2304.11436)

    本文揭示了即便使用FedMD的安全机制，仍存在被精心设计的恶意攻击利用的风险，如Paired-Logits反演攻击，会导致隐私数据曝光。

    

    联邦学习与模型蒸馏（FedMD）是一种新兴的协作学习范式，其中仅传输公共数据集的输出logits作为蒸馏知识，而不是传递易受梯度反演攻击的私有模型参数，这是联邦学习中已知的隐私风险。本文发现，即使共享公共数据集的输出 logit比直接共享梯度更安全，仍存在因精心设计的恶意攻击导致的数据曝光风险。我们的研究表明，恶意服务器可以训练一个反演神经网络来利用服务器和客户端模型之间的置信度差，针对FedMD及其变种进行PLI（配对logits反演）攻击。在多个人脸识别数据集上进行的实验证明，在类似于FedMD的方案中，仅使用公共数据集的配对服务器-客户端logits，恶意服务器能够重构私有图像。

    Federated Learning with Model Distillation (FedMD) is a nascent collaborative learning paradigm, where only output logits of public datasets are transmitted as distilled knowledge, instead of passing on private model parameters that are susceptible to gradient inversion attacks, a known privacy risk in federated learning. In this paper, we found that even though sharing output logits of public datasets is safer than directly sharing gradients, there still exists a substantial risk of data exposure caused by carefully designed malicious attacks. Our study shows that a malicious server can inject a PLI (Paired-Logits Inversion) attack against FedMD and its variants by training an inversion neural network that exploits the confidence gap between the server and client models. Experiments on multiple facial recognition datasets validate that under FedMD-like schemes, by using paired server-client logits of public datasets only, the malicious server is able to reconstruct private images on a
    
[^119]: 柿子政治的面孔：使用机器学习比较政治领袖面部情感表达的差异

    The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])

    [http://arxiv.org/abs/2304.09914](http://arxiv.org/abs/2304.09914)

    本文使用机器学习的算法分析了来自15个不同国家的220个政治领袖的YouTube视频，总结了政治领袖面部情感表达的差异。

    

    网络媒体已经彻底改变了政治信息在全球范围内的传播和消费方式，这种转变促使政治人物采取新的策略来捕捉和保持选民的注意力。这些策略往往依赖于情感说服和吸引。随着虚拟空间中视觉内容越来越普遍，很多政治沟通也被标志着唤起情感的视频内容和图像。本文提供了一种新的分析方法。我们将基于现有训练好的卷积神经网络架构提供的Python库fer，应用一种基于深度学习的计算机视觉算法，对描绘来自15个不同国家的政治领袖的220个YouTube视频样本进行分析。该算法返回情绪分数，每一帧都代表6种情绪状态（愤怒，厌恶，恐惧，快乐，悲伤和惊讶）和一个中性表情。

    Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
    
[^120]: 时间感知顺序推荐中的注意力混合

    Attention Mixtures for Time-Aware Sequential Recommendation. (arXiv:2304.08158v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2304.08158](http://arxiv.org/abs/2304.08158)

    MOJITO是一种改进的Transformer顺序推荐系统，利用注意力混合建模用户偏好和时间背景的复杂依赖关系，从而准确预测下一个推荐物品。在多个真实数据集中，MOJITO表现优于现有的Transformer模型。

    

    Transformer模型在顺序推荐中表现出强大的能力。然而，现有的架构经常忽视用户偏好和时间背景之间的复杂依赖关系。在本篇短文中，我们介绍了MOJITO，一种改进的Transformer顺序推荐系统，它解决了这个局限性。MOJITO利用基于注意力的时间背景和物品嵌入表示的高斯混合进行顺序建模。这种方法可以准确地预测下一个应该向用户推荐哪些物品，这取决于过去的行为和时间背景。我们通过在多个真实世界数据集上进行实证实验，证明了我们方法的相关性，优于现有的Transformer顺序推荐模型。

    Transformers emerged as powerful methods for sequential recommendation. However, existing architectures often overlook the complex dependencies between user preferences and the temporal context. In this short paper, we introduce MOJITO, an improved Transformer sequential recommender system that addresses this limitation. MOJITO leverages Gaussian mixtures of attention-based temporal context and item embedding representations for sequential modeling. Such an approach permits to accurately predict which items should be recommended next to users depending on past actions and the temporal context. We demonstrate the relevance of our approach, by empirically outperforming existing Transformers for sequential recommendation on several real-world datasets.
    
[^121]: 用BREC数据集更好地评估GNN表达力

    Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v1 [cs.LG])

    [http://arxiv.org/abs/2304.07702](http://arxiv.org/abs/2304.07702)

    本论文介绍了一个新的Benchmark for Evaluating the Robustness of GNNs to Topological Changes (BREC)数据集，并使用BREC评估了几种现有的GNN模型的表达力，表明一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。

    

    关于图神经网络（GNN）的理论表达力的研究得到了快速发展，并提出了许多增强表达力的方法。然而，除了严格遵循k维Weisfeiler-Lehman（k-WL）测试层次结构的少数方法外，大多数方法都没有统一的表达力度量。它们的理论分析通常限于区分某些非同构图族，导致在定量比较表达力方面存在困难。与理论分析相反，衡量表达能力的另一种方法是在包含1-WL不可区分图的特定数据集上评估模型性能。然而，以前专门设计用于此目的的数据集面临着难度（任何超越1-WL的模型准确率几乎达到100％）、粒度（模型倾向于要么完全正确，要么接近随机猜测）和规模（每个数据集中仅有少量本质不同的图）的问题。为了解决这些受限制的评估问题，我们提出了一个新的GNN鲁棒性评估基准（BREC），该基准包含许多结构多样的图，并允许对模型表达力进行更精细的评估。我们使用BREC评估了几种现有的GNN模型的表达力，并展示了一些模型在以前的基准测试中表现良好，但在BREC上遇到了困难，突显了需要更好的GNN表现力评估的必要性。

    Research on the theoretical expressiveness of Graph Neural Networks (GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy), granularity (models tend to be either 100% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limi
    
[^122]: Squeeze and Excitation网络的变体

    Variations of Squeeze and Excitation networks. (arXiv:2304.06502v1 [cs.CV])

    [http://arxiv.org/abs/2304.06502](http://arxiv.org/abs/2304.06502)

    本文提出了Squeeze and Excitation网络的变体来改进重要特征的学习过程，从而提高神经网络的性能。实验表明这些变体在残差网络上效果良好。

    

    卷积神经网络学习空间特征，并在内核中紧密相连。SE模块打破了神经网络传递整体结果至下一层的传统路线。相反，SE仅传递包含其挤压和激励模块的重要特征进行学习。本文提出了SE模块的变体，改进了挤压和激励的过程，并提高了性能。所提出的挤压或激励层使得层权重的转换变得更加平滑。这些变化还保留了SE模块的特点。实验结果在残差网络上进行，并进行了表格化的展示。

    Convolutional neural networks learns spatial features and are heavily interlinked within kernels. The SE module have broken the traditional route of neural networks passing the entire result to next layer. Instead SE only passes important features to be learned with its squeeze and excitation (SE) module. We propose variations of the SE module which improvises the process of squeeze and excitation and enhances the performance. The proposed squeezing or exciting the layer makes it possible for having a smooth transition of layer weights. These proposed variations also retain the characteristics of SE module. The experimented results are carried out on residual networks and the results are tabulated.
    
[^123]: 数据科学中的可解释符号回归：2022年竞赛分析

    Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition. (arXiv:2304.01117v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01117](http://arxiv.org/abs/2304.01117)

    本文分析了2022年基因与进化计算会议举办的竞赛，评估了符号回归的新方法在面对现实数据时的表现，并提供了可解释性评估的实际方法。

    

    符号回归是寻找能够准确描述研究现象的解析表达式的方法。这种方法的主要优势是返回可解释的模型，能够给用户提供深刻的见解。历史上，符号回归的大多数算法都基于进化算法。然而，最近出现了大量新的提案，这些提案使用了列举算法、混合线性整数规划、神经网络和贝叶斯优化等方法。为了评估这些新方法在面对现实世界数据中经常遇到的一组常见挑战时的表现如何，我们在2022年遗传与进化计算会议上举办了一次竞赛，其中包含不同的合成和真实世界数据集，参赛者对这些数据集是盲测试的。对于真实世界的部分，我们使用了领域专家来评估候选模型的可解释性。我们对结果进行了深入分析。

    Symbolic regression searches for analytic expressions that accurately describe studied phenomena. The main attraction of this approach is that it returns an interpretable model that can be insightful to users. Historically, the majority of algorithms for symbolic regression have been based on evolutionary algorithms. However, there has been a recent surge of new proposals that instead utilize approaches such as enumeration algorithms, mixed linear integer programming, neural networks, and Bayesian optimization. In order to assess how well these new approaches behave on a set of common challenges often faced in real-world data, we hosted a competition at the 2022 Genetic and Evolutionary Computation Conference consisting of different synthetic and real-world datasets which were blind to entrants. For the real-world track, we assessed interpretability in a realistic way by using a domain expert to judge the trustworthiness of candidate models.We present an in-depth analysis of the result
    
[^124]: TOFA：一次转移全能的神经架构搜索

    TOFA: Transfer-Once-for-All. (arXiv:2303.15485v1 [cs.LG])

    [http://arxiv.org/abs/2303.15485](http://arxiv.org/abs/2303.15485)

    TOFA使用权重共享来进行神经架构搜索，以优化超网以适应各种设备的各种部署情况。与现有方法不同，TOFA在小数据集上进行训练，计算训练成本与部署方案数量无关。TOFA使用统一的半监督训练方法来解决小数据集带来的挑战。

    

    权重共享神经架构搜索旨在为不同资源约束的许多设备优化可配置的神经网络模型（超网）以满足各种部署场景。现有方法使用进化搜索从在非常大的数据集上训练的超网中提取多个模型，然后对感兴趣的通常很小的真实数据集上提取的模型进行微调。因此，训练的计算成本随着不同模型部署方案的数量而线性增长。因此，我们提出了Transfer-Once-For-All（TOFA），用于在小数据集上进行超网风格的训练，在任意数量的边缘部署方案上具有恒定的计算训练成本。给定任务，TOFA获得定制的神经网络，优化任意数量的边缘部署方案的拓扑和权重。为了克服小数据带来的挑战，TOFA利用统一的半监督训练损失同时训练超网内的所有子网。

    Weight-sharing neural architecture search aims to optimize a configurable neural network model (supernet) for a variety of deployment scenarios across many devices with different resource constraints. Existing approaches use evolutionary search to extract a number of models from a supernet trained on a very large data set, and then fine-tune the extracted models on the typically small, real-world data set of interest. The computational cost of training thus grows linearly with the number of different model deployment scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style training on small data sets with constant computational training cost over any number of edge deployment scenarios. Given a task, TOFA obtains custom neural networks, both the topology and the weights, optimized for any number of edge deployment scenarios. To overcome the challenges arising from small data, TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets w
    
[^125]: Tsetlin机器的性质验证

    Verifying Properties of Tsetlin Machines. (arXiv:2303.14464v1 [cs.LG])

    [http://arxiv.org/abs/2303.14464](http://arxiv.org/abs/2303.14464)

    该论文介绍了一种对Tsetlin Machine进行正式验证性质的方法，并展示了对抗性鲁棒性、等价性和相似性方面的结果。此外，还将新的模型相似性概念应用于TsMs。

    

    Tsetlin机器（TsMs）是一种有前途且易于解释的机器学习方法，可应用于各种分类任务。我们将TsMs精确编码为命题逻辑并使用SAT求解器正式验证了TsMs的性质。特别地，在这项工作中，我们引入了一种机器学习模型相似性的概念，并将我们的概念应用于检查TsMs的相似性。我们还考虑了文献中的鲁棒性和等价性概念，并为TsMs重新调整了它们。然后，我们展示了我们编码的正确性，并为TsMs的性质——对抗鲁棒性，等价性和相似性提供结果。在我们的实验中，我们分别应用MNIST和IMDB数据集进行图像和情感分类。我们将使用TsMs检查对抗性鲁棒性的结果与文献中在MNIST上使用二进制神经网络获得的结果进行讨论。

    Tsetlin Machines (TsMs) are a promising and interpretable machine learning method which can be applied for various classification tasks. We present an exact encoding of TsMs into propositional logic and formally verify properties of TsMs using a SAT solver. In particular, we introduce in this work a notion of similarity of machine learning models and apply our notion to check for similarity of TsMs. We also consider notions of robustness and equivalence from the literature and adapt them for TsMs. Then, we show the correctness of our encoding and provide results for the properties: adversarial robustness, equivalence, and similarity of TsMs. In our experiments, we employ the MNIST and IMDB datasets for (respectively) image and sentiment classification. We discuss the results for verifying robustness obtained with TsMs with those in the literature obtained with Binarized Neural Networks on MNIST.
    
[^126]: 隐式平衡和正则化：过参数化非对称矩阵感知中的泛化和收敛保证

    Implicit Balancing and Regularization: Generalization and Convergence Guarantees for Overparameterized Asymmetric Matrix Sensing. (arXiv:2303.14244v1 [cs.LG])

    [http://arxiv.org/abs/2303.14244](http://arxiv.org/abs/2303.14244)

    本论文研究了过参数化低秩矩阵感知问题，证明了通过因子化方法训练的过参数化模型可以收敛，并且隐式平衡和正则化可以促进泛化。

    

    最近，对于训练过参数化学习模型的基于梯度的方法的收敛和泛化属性有了重要进展。然而，其中许多方面，包括小随机初始化的角色以及模型的各种参数在梯度更新中如何耦合以促进良好的泛化，仍然是很神秘的。最近一系列的论文已经开始研究非凸对称半正定（PSD）矩阵感知问题的形式，在这个问题中需要从几个线性测量中重建一个低秩PSD矩阵。这种底层的对称性/PSD性对于现有的这个问题的收敛和泛化保证是至关重要的。在本文中，我们研究了一个一般的过参数化的低秩矩阵感知问题，其中希望从少量的线性测量中重建一个非对称矩形低秩矩阵。我们证明了通过因子化来训练的过参数化模型在这个问题上可以收敛，而隐式平衡和正则化可以促进泛化。

    Recently, there has been significant progress in understanding the convergence and generalization properties of gradient-based methods for training overparameterized learning models. However, many aspects including the role of small random initialization and how the various parameters of the model are coupled during gradient-based updates to facilitate good generalization remain largely mysterious. A series of recent papers have begun to study this role for non-convex formulations of symmetric Positive Semi-Definite (PSD) matrix sensing problems which involve reconstructing a low-rank PSD matrix from a few linear measurements. The underlying symmetry/PSDness is crucial to existing convergence and generalization guarantees for this problem. In this paper, we study a general overparameterized low-rank matrix sensing problem where one wishes to reconstruct an asymmetric rectangular low-rank matrix from a few linear measurements. We prove that an overparameterized model trained via factori
    
[^127]: 面向心电图和脑电图分类的领域泛化：算法和基准

    Towards Domain Generalization for ECG and EEG Classification: Algorithms and Benchmarks. (arXiv:2303.11338v1 [eess.SP])

    [http://arxiv.org/abs/2303.11338](http://arxiv.org/abs/2303.11338)

    本论文提出了一个开源生物信号领域泛化评估基准，并引入一种专门解决生物信号中领域泛化问题的神经网络架构DGNet-Bio。通过实验证明，DGNet-Bio在ECG和EEG分类领域泛化上优于现有方法。

    

    尽管机器和深度学习系统在许多领域取得了巨大的成功，但它们尚未能够在医疗保健的关键任务中牢固地确立自己。其中一个主要原因在于，当模型面对之前未见过的分布之外的样本时，它们的性能会显著下降。这就是领域泛化（DG）问题。我们的目标是提出一个评估DG算法的基准，并引入一种新颖的架构来解决生物信号分类中的DG问题。在本文中，我们描述了生物信号的领域泛化问题，重点关注心电图（ECG）和脑电图（EEG），并提出并实现了一个开源生物信号领域泛化评估基准。此外，我们将计算机视觉领域的最先进DG算法改进为1D生物信号分类问题，并评估它们的有效性。最后，我们还介绍了一种新颖的神经网络架构，称为DGNet-Bio，专门设计用于解决生物信号中的DG问题。我们的实验表明，DGNet-Bio在新提出的ECG和EEG分类领域泛化基准上优于现有方法。

    Despite their immense success in numerous fields, machine and deep learning systems have not have not yet been able to firmly establish themselves in mission-critical applications in healthcare. One of the main reasons lies in the fact that when models are presented with previously unseen, Out-of-Distribution samples, their performance deteriorates significantly. This is known as the Domain Generalization (DG) problem. Our objective in this work is to propose a benchmark for evaluating DG algorithms, in addition to introducing a novel architecture for tackling DG in biosignal classification. In this paper, we describe the Domain Generalization problem for biosignals, focusing on electrocardiograms (ECG) and electroencephalograms (EEG) and propose and implement an open-source biosignal DG evaluation benchmark. Furthermore, we adapt state-of-the-art DG algorithms from computer vision to the problem of 1D biosignal classification and evaluate their effectiveness. Finally, we also introduc
    
[^128]: ExoplANNET: 一种用于检测和确认径向速度数据中行星信号的深度学习算法

    ExoplANNET: A deep learning algorithm to detect and identify planetary signals in radial velocity data. (arXiv:2303.09335v1 [astro-ph.EP])

    [http://arxiv.org/abs/2303.09335](http://arxiv.org/abs/2303.09335)

    本文介绍了一种神经网络算法ExoplANNET，旨在解决径向速度法检测系外行星的挑战，在存在与星体相关的噪声的情况下进行行星信号的检测和分类，经过合成数据和真实数据的测试，取得了有前途的结果。

    

    利用径向速度法检测系外行星的方法在于探测由未知下部恒星伴星引起的恒星速度变化。仪器误差、不规则时间采样以及源自于恒星内在变异的不同噪声源可能会阻碍数据的解释，甚至导致虚假探测结果。近期，在系外行星领域出现了使用机器学习算法的研究，其中一些结果甚至超过了传统技术的结果。本研究旨在探索神经网络在径向速度法中的应用范围，特别是在存在与星体相关的噪声的情况下进行系外行星探测。提出了一种神经网络算法，以代替径向速度法检测到的信号的重要性计算并将其分类为行星还是非行星来源。该算法使用已知有行星系统的合成数据进行训练，然后在未知有行星系统的合成和真实数据上进行测试。ExoplANNET算法展现出有前途的结果，并表明深度学习算法可以在系外行星的鉴定和表征中发挥重要作用。

    The detection of exoplanets with the radial velocity method consists in detecting variations of the stellar velocity caused by an unseen sub-stellar companion. Instrumental errors, irregular time sampling, and different noise sources originating in the intrinsic variability of the star can hinder the interpretation of the data, and even lead to spurious detections. In recent times, work began to emerge in the field of extrasolar planets that use Machine Learning algorithms, some with results that exceed those obtained with the traditional techniques in the field. We seek to explore the scope of the neural networks in the radial velocity method, in particular for exoplanet detection in the presence of correlated noise of stellar origin. In this work, a neural network is proposed to replace the computation of the significance of the signal detected with the radial velocity method and to classify it as of planetary origin or not. The algorithm is trained using synthetic data of systems wi
    
[^129]: 具有真正满足不等式约束的软Actor-Critic算法

    Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint. (arXiv:2303.04356v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04356](http://arxiv.org/abs/2303.04356)

    本文改进了软Actor-Critic（SAC）算法的实现，通过引入可学习的状态相关的松弛变量来适当处理不等式约束，实现了最大化策略熵。这对于增强机器人控制器的鲁棒性非常有用。

    

    在强化学习中，软Actor-Critic（SAC）被认为是下一代机器人控制方案之一。其最大化策略熵的能力可以使机器人控制器对噪声和扰动具有鲁棒性，这对于实际的机器人应用非常有用。然而，在当前的实现中，最大化策略熵的优先级是自动调节的，其规则可以解释为等式约束，将策略熵绑定到指定的下界。因此，当前的SAC不再最大化策略熵，与我们的期望相反。为了解决SAC中的这个问题，本文改进了其实现，引入了一个可学习的状态相关的松弛变量，以适当处理不等式约束，通过将其重新制定为相应的等式约束来最大化策略熵。引入的松弛变量通过考虑满足不等式约束的双重目标的切换型损失函数进行优化。

    Soft actor-critic (SAC) in reinforcement learning is expected to be one of the next-generation robot control schemes. Its ability to maximize policy entropy would make a robotic controller robust to noise and perturbation, which is useful for real-world robot applications. However, the priority of maximizing the policy entropy is automatically tuned in the current implementation, the rule of which can be interpreted as one for equality constraint, binding the policy entropy into its specified lower bound. The current SAC is therefore no longer maximize the policy entropy, contrary to our expectation. To resolve this issue in SAC, this paper improves its implementation with a learnable state-dependent slack variable for appropriately handling the inequality constraint to maximize the policy entropy by reformulating it as the corresponding equality constraint. The introduced slack variable is optimized by a switching-type loss function that takes into account the dual objectives of satis
    
[^130]: 基于物理知识的深度学习在交通状态估计中的应用: 一项调查和展望

    Physics-Informed Deep Learning For Traffic State Estimation: A Survey and the Outlook. (arXiv:2303.02063v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.02063](http://arxiv.org/abs/2303.02063)

    本文调查了将物理知识和深度学习相结合的PIDL在交通状态估计中的应用。在设计PIDL计算图的过程中，重点考虑了如何将物理知识编码到深度神经网络中，并展示了不同架构设计在交通状态估计中的差异和效果。

    

    基于物理知识的深度学习（Physics-Informed Deep Learning, PIDL）通过将物理模型和深度神经网络(DNN)相结合，具有较强的预测能力和训练效率。本文的主要挑战是设计一个结合物理知识和DNN的计算图，即如何将物理知识编码到DNN中，以及如何表示物理知识和数据组成部分。针对交通状态估计（Traffic State Estimation, TSE）这个交通工程中的核心问题，本文提供了多种PIDL计算图的架构设计，并展示了在观测数据、问题类型和目标不同的情况下，如何使用相同的真实数据集对这些变体进行比较。

    For its robust predictive power (compared to pure physics-based models) and sample-efficient training (compared to pure deep learning models), physics-informed deep learning (PIDL), a paradigm hybridizing physics-based models and deep neural networks (DNN), has been booming in science and engineering fields. One key challenge of applying PIDL to various domains and problems lies in the design of a computational graph that integrates physics and DNNs. In other words, how physics are encoded into DNNs and how the physics and data components are represented. In this paper, we provide a variety of architecture designs of PIDL computational graphs and how these structures are customized to traffic state estimation (TSE), a central problem in transportation engineering. When observation data, problem type, and goal vary, we demonstrate potential architectures of PIDL computational graphs and compare these variants using the same real-world dataset.
    
[^131]: 对概念瓶颈模型介入程序的更深入研究

    A Closer Look at the Intervention Procedure of Concept Bottleneck Models. (arXiv:2302.14260v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14260](http://arxiv.org/abs/2302.14260)

    本文研究了概念瓶颈模型介入程序的提高介入效果的方法，通过选择介入概念以及深入分析发现，在实际情况下，明智的介入策略可以将任务误差降低十倍以上，而且这个差异可相当明显。

    

    概念瓶颈模型(CBMs)是一类可解释的神经网络模型，基于其高级概念预测给定输入的目标响应。与标准的端到端模型不同，CBMs使领域专家能够在测试时对预测的概念进行干预并纠正任何错误，以便在最终进行更准确的任务预测。尽管这种可干预性提供了一种强大的控制途径，但介入程序的许多方面仍然相当未知。在本研究中，我们开发了各种方式来选择介入概念以提高介入效果，并进行了各种深入的分析，以了解在不同情况下它们的演变方式。具体而言，我们发现在实际情况下，经过明智介入的策略可以将任务误差降低十倍以上，而与当前基线相比，在相同数量的干预次数下，这个差异可以相当明显。

    Concept bottleneck models (CBMs) are a class of interpretable neural network models that predict the target response of a given input based on its high-level concepts. Unlike the standard end-to-end models, CBMs enable domain experts to intervene on the predicted concepts and rectify any mistakes at test time, so that more accurate task predictions can be made at the end. While such intervenability provides a powerful avenue of control, many aspects of the intervention procedure remain rather unexplored. In this work, we develop various ways of selecting intervening concepts to improve the intervention effectiveness and conduct an array of in-depth analyses as to how they evolve under different circumstances. Specifically, we find that an informed intervention strategy can reduce the task error more than ten times compared to the current baseline under the same amount of intervention counts in realistic settings, and yet, this can vary quite significantly when taking into account diffe
    
[^132]: 从有噪声的不动点迭代到私有的ADMM算法，用于集中式和联合学习

    From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning. (arXiv:2302.12559v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12559](http://arxiv.org/abs/2302.12559)

    本论文研究了将差分隐私机器学习算法视为有噪声的不动点迭代，从而从这一框架中推导出隐私和效用结果。通过利用这个新的视角，我们恢复了流行的私有梯度下降方法并提供了一种有原则的方法来设计和分析新的私有优化算法。我们通过通用框架推导出了用于集中式、联合和完全去中心化学习的新颖私有ADMM算法，并通过迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果提供效用保证。

    

    我们研究差分隐私（DP）机器学习算法作为噪声固定点迭代的实例，以便从这个经过充分研究的框架中得出隐私和效用结果。我们展示了这一新的视角恢复了流行的私有梯度下降方法，如DP-SGD，并为设计和分析新的私有优化算法提供了一种有原则的方式。专注于广泛使用的交替方向乘子法（ADMM）方法，我们利用我们的通用框架推导出用于集中式、联合和完全去中心化学习的新颖私有ADMM算法。对于这三个算法，我们利用迭代和子采样的隐私放大建立了强隐私保证。最后，我们利用最近的有噪声固定点迭代的线性收敛结果进行统一分析，提供效用保证。

    We study differentially private (DP) machine learning algorithms as instances of noisy fixed-point iterations, in order to derive privacy and utility results from this well-studied framework. We show that this new perspective recovers popular private gradient-based methods like DP-SGD and provides a principled way to design and analyze new private optimization algorithms in a flexible manner. Focusing on the widely-used Alternating Directions Method of Multipliers (ADMM) method, we use our general framework to derive novel private ADMM algorithms for centralized, federated and fully decentralized learning. For these three algorithms, we establish strong privacy guarantees leveraging privacy amplification by iteration and by subsampling. Finally, we provide utility guarantees using a unified analysis that exploits a recent linear convergence result for noisy fixed-point iterations.
    
[^133]: 具有可学习和最优多项式基函数的图神经网络

    Graph Neural Networks with Learnable and Optimal Polynomial Bases. (arXiv:2302.12432v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12432](http://arxiv.org/abs/2302.12432)

    本文提出了两种具有可学习和最优多项式基函数的谱图神经网络模型，通过学习多项式基函数和计算最优基函数，解决了多项式滤波器在模型有效性方面的问题。

    

    多项式滤波器是一种图神经网络，通常使用预定的多项式基函数，并从训练数据中学习系数。然而，模型的有效性很大程度上取决于多项式基函数的性质。因此，我们提出了两种谱图神经网络模型，它们能够肯定回答上述问题。首先，受到Favard定理的启发，我们提出了FavardGNN模型，该模型从所有可能的正交基函数空间中学习多项式基函数。其次，我们研究了Wang和Zhang（2022年）提出的所谓无法解决的最优多项式基函数的定义，并提出了一个简单模型OptBasisGNN，可计算给定图结构和图信号的最优基函数。进行了大量实验验证。

    Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features?  In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from Wang & Zhang (2022) and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conduct
    
[^134]: 学习最大化菜单抽奖和两部分票价的论文

    Learning Revenue Maximizing Menus of Lotteries and Two-Part Tariffs. (arXiv:2302.11700v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2302.11700](http://arxiv.org/abs/2302.11700)

    该论文研究了经济学中两种机制的可学习性：菜单抽奖和两部分票价。他们提出了第一个针对这两种机制的在线学习算法。

    

    我们通过研究在学习理论和计算经济学交叉领域中近年来蓬勃发展的一系列工作，推进了经济学中两类机制的可学习性研究，分别是菜单抽奖和两部分票价。前者是一类旨在销售多个物品的随机机制，已知能够实现超出确定性机制的收益，而后者则是针对销售单个物品多个单位（副本）的设计，适用于现实世界中的场景，如汽车或自行车共享服务等。我们关注如何从买家估值数据中学习出高收益的这类机制，涵盖多种分布设置，既有直接获得买家估值样本的情况，也有更具挑战性、研究较少的在线设置，其中买家一个接一个到来，并且对他们的估值没有分布假设。我们的主要贡献是提出了第一个针对菜单抽奖和两部分票价的在线学习算法。

    We advance a recently flourishing line of work at the intersection of learning theory and computational economics by studying the learnability of two classes of mechanisms prominent in economics, namely menus of lotteries and two-part tariffs. The former is a family of randomized mechanisms designed for selling multiple items, known to achieve revenue beyond deterministic mechanisms, while the latter is designed for selling multiple units (copies) of a single item with applications in real-world scenarios such as car or bike-sharing services. We focus on learning high-revenue mechanisms of this form from buyer valuation data in both distributional settings, where we have access to buyers' valuation samples up-front, and the more challenging and less-studied online settings, where buyers arrive one-at-a-time and no distributional assumption is made about their values.  Our main contribution is proposing the first online learning algorithms for menus of lotteries and two-part tariffs wit
    
[^135]: 特征分区聚合：一种快速的对$\ell_0$攻击的认证防御方法

    Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks. (arXiv:2302.11628v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11628](http://arxiv.org/abs/2302.11628)

    本文提出了一种名为特征分区聚合的认证防御方法，用于对抗$\ell_0$逃避、后门和污染攻击。与现有防御方法相比，FPA速度更快，提供更大的鲁棒性保证，且能够免费提供额外的鲁棒性维度。

    

    稀疏的或$\ell_0$对抗攻击会任意扰动未知的特征子集。$\ell_0$鲁棒性分析特别适用于异构（表格）数据，其中特征具有不同的类型或尺度。目前最先进的$\ell_0$认证防御基于随机平滑，并仅适用于逃避攻击。本文提出了特征分区聚合（FPA）--一种针对$\ell_0$逃避、后门和污染攻击的认证防御。FPA通过集成生成更强的鲁棒性保证，其子模型是在不相交的特征集上训练的。与最先进的$\ell_0$防御相比，FPA速度提高了多达3000倍，并提供了更大的中位数鲁棒性保证（例如，对于CIFAR10的中位数证书为13像素，MNIST的中位数证书为12像素，Weather的中位数证书为4个特征，Ames的中位数证书为3个特征），这意味着FPA能够免费提供额外的鲁棒性维度。

    Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset of the features. $\ell_0$ robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to 3,000${\times}$ faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.
    
[^136]: 可用于回归的快速校准不确定性的似然退火方法

    Likelihood Annealing: Fast Calibrated Uncertainty for Regression. (arXiv:2302.11012v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11012](http://arxiv.org/abs/2302.11012)

    该论文提出了一种名为似然退火的快速校准回归任务不确定性估计方法，能够改进深度回归模型的收敛性并产生校准的不确定性估计。

    

    近年来，深度学习的发展表明，不确定性估计在医学影像、自然语言处理和自主系统等应用中变得越来越重要。然而，准确量化不确定性仍然是一个具有挑战性的问题，特别是在输出空间连续的回归任务中。允许回归问题进行不确定性估计的深度学习方法通常收敛速度较慢，并产生不良校准的不确定性估计，不能有效用于量化。最近提出的事后校准技术很少适用于回归问题，并且常常给已经较慢的模型训练阶段增加了额外开销。本文提出了一种用于回归任务的快速校准不确定性估计方法，称为似然退火，它能够持续改进深度回归模型的收敛性，并在没有任何事后校准阶段的情况下产生校准的不确定性。

    Recent advances in deep learning have shown that uncertainty estimation is becoming increasingly important in applications such as medical imaging, natural language processing, and autonomous systems. However, accurately quantifying uncertainty remains a challenging problem, especially in regression tasks where the output space is continuous. Deep learning approaches that allow uncertainty estimation for regression problems often converge slowly and yield poorly calibrated uncertainty estimates that can not be effectively used for quantification. Recently proposed post hoc calibration techniques are seldom applicable to regression problems and often add overhead to an already slow model training phase. This work presents a fast calibrated uncertainty estimation method for regression tasks called Likelihood Annealing, that consistently improves the convergence of deep regression models and yields calibrated uncertainty without any post hoc calibration phase. Unlike previous methods for 
    
[^137]: 使用特征合成工具对深度神经网络进行红队演练

    Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10894](http://arxiv.org/abs/2302.10894)

    本文提出了一个用于评估可解释性工具的基准，通过训练模型以对特定触发器产生特定输出的方式，可以解决传统可解释性方法无法分析未知特征行为的问题。

    

    可解释的人工智能工具通常旨在理解模型在超出分布范围（OOD）的情况下的行为。尽管这个研究领域受到了关注，但在这些工具中很少有能够发现模型中的新颖、以前未知的错误的案例。我们认为，这部分原因在于许多可解释性方法的共同特点：它们使用特定的数据集分析和解释模型的行为。虽然这很有用，但这些工具只能分析用户可以事先采样或识别的特征所引发的行为。为了解决这个问题，一个不断增加的研究领域涉及使用不依赖于数据集的特征合成方法来解释模型。本文的主要贡献是提出了一个评估可解释性工具的基准。我们的关键观点是，我们可以训练模型以对特定触发器（例如，插入图像的特定补丁）产生特定输出（即标签），然后评估可解释性工具的有效性。

    Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset.  In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate inte
    
[^138]: FrankenSplit:基于显著性指导的神经特征压缩与浅层变分瓶颈注入

    FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.10681](http://arxiv.org/abs/2302.10681)

    本文提出了一种基于显著性指导的神经特征压缩与浅层变分瓶颈注入的新的资源意识压缩模型的框架，实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。

    

    移动AI加速器的崛起使得对延迟敏感的应用可以在客户端上执行轻量级深度神经网络（DNN）。然而，需要强大模型的关键应用程序需要将请求下放，而高维数据将争夺有限的带宽。本文提出了一种新的资源意识压缩模型的框架并在反映边缘设备和服务器之间不对称资源分配的环境中进行了广泛评估。我们的方法在不降低准确性的情况下实现了比最先进的SC方法低60％的比特率，并且比现有的编解码标准的下放快16倍。

    The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
    
[^139]: 将黑匣子分解为可解释模型的混合物：路线规划，解释，重复。

    Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10289](http://arxiv.org/abs/2302.10289)

    本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。

    

    机器学习模型设计要么从解释性模型开始，要么从黑盒开始并事后解释。黑盒模型灵活但难以解释，而解释性模型本质上是可解释的。然而，解释性模型需要广泛的机器学习知识，并且往往比它们的黑盒变体不够灵活和表现不佳。本文旨在模糊黑盒的事后解释和构建可解释模型之间的界限。我们从黑盒开始，迭代地Carve出一种混合解释模型（MoIE）和一个残余网络。每个可解释模型专门处理一个样本子集，并使用一阶逻辑(FOL)对其进行解释，从黑盒中提供基本推理概念。我们通过灵活的残差路由其余的样本。我们在残转网络上重复该方法，直到所有可解释模型解释所需比例的数据。我们进行了大量实验，结果表明我们的路线规划，解释和重复方法在各种数据集上优于目前几种黑匣子模型解释方法，并产生高度可解释的模型。

    ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
    
[^140]: 具有因果正则化的神经算法推理

    Neural Algorithmic Reasoning with Causal Regularisation. (arXiv:2302.10258v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10258](http://arxiv.org/abs/2302.10258)

    提出了一种具有因果正则化的神经算法推理方法，通过观察到对于某些中间计算来说存在许多不同的输入，可以开发数据增强程序，生成能够使目标算法有完全相同下一轨迹步骤的输入，从而提高在分布外测试数据上的性能。

    

    最近关于神经算法推理的研究探究了神经网络的推理能力，有效地证明了它们可以学习在训练分布中未见过的数据上执行经典算法。然而，现有神经推理器在分布外（OOD）的测试数据上性能显著下降，因为输入的规模更大。在这项工作中，我们做出了重要观察：对于算法来说，有很多不同的输入会表现出完全相同的中间计算。这种洞察力使我们能够开发数据增强的程序，根据算法的中间轨迹生成能够使目标算法有完全相同下一轨迹步骤的输入。我们通过使用由我们的观察导出并在因果图中形式化的自监督目标来确保这些输入下的下一步预测的不变性。我们证明了由此产生的方法，我们称之为Hint-ReLIC，改善了OOD测试数据上的性能。

    Recent work on neural algorithmic reasoning has investigated the reasoning capabilities of neural networks, effectively demonstrating they can learn to execute classical algorithms on unseen data coming from the train distribution. However, the performance of existing neural reasoners significantly degrades on out-of-distribution (OOD) test data, where inputs have larger sizes. In this work, we make an important observation: there are many different inputs for which an algorithm will perform certain intermediate computations identically. This insight allows us to develop data augmentation procedures that, given an algorithm's intermediate trajectory, produce inputs for which the target algorithm would have exactly the same next trajectory step. We ensure invariance in the next-step prediction across such inputs, by employing a self-supervised objective derived by our observation, formalised in a causal graph. We prove that the resulting method, which we call Hint-ReLIC, improves the OO
    
[^141]: 基于拓扑学的特征选择方法：一种基于图论的过滤特征选择方法

    Topological Feature Selection: A Graph-Based Filter Feature Selection Approach. (arXiv:2302.09543v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09543](http://arxiv.org/abs/2302.09543)

    本文提出了一种基于图论的特征选择方法，利用拓扑学和依赖关系，具有高度灵活性和解释性，在16个基准数据集上显示出优于或匹配于当前最先进技术的表现。

    

    本文介绍了一种新型的无监督的基于图论过滤方法进行特征选择的技术，利用了拓扑约束网络表示的威力。我们使用一系列弦图（三角最大过滤图）模拟特征之间的相互依赖关系，并通过研究特征在网络内的相对位置来最大化特征相关性的可能性。这种方法相对于其他方法有三个特点：（i）高度可调，易于适应输入数据的性质；（ii）完全可解释，同时保持了显著的简单性；（iii）计算成本比其替代方案更加便宜。我们在来自不同应用领域的16个基准数据集上测试了我们的算法，结果表明在异构评估条件下，它优于或与当前最先进技术相匹配。

    In this paper, we introduce a novel unsupervised, graph-based filter feature selection technique which exploits the power of topologically constrained network representations. We model dependency structures among features using a family of chordal graphs (the Triangulated Maximally Filtered Graph), and we maximise the likelihood of features' relevance by studying their relative position inside the network. Such an approach presents three aspects that are particularly satisfactory compared to its alternatives: (i) it is highly tunable and easily adaptable to the nature of input data; (ii) it is fully explainable, maintaining, at the same time, a remarkable level of simplicity; (iii) it is computationally cheaper compared to its alternatives. We test our algorithm on 16 benchmark datasets from different applicative domains showing that it outperforms or matches the current state-of-the-art under heterogeneous evaluation conditions.
    
[^142]: 使用非特定运动数据的可扩展XR用户基于运动的识别

    Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07517](http://arxiv.org/abs/2302.07517)

    提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。

    

    本文提出了一种基于嵌入式和深度度量学习结合的方法，将距离和分类两种方法的优势相结合，用于通过用户的运动来识别扩展现实用户。我们在“半衰期：Alyx”VR游戏的用户数据集上进行了模型训练，并使用现有的基线分类模型作为对比。研究结果表明，基于嵌入式的方法可以通过只使用几分钟的注册数据，识别新用户的非特定运动，可以在几秒钟内注册新用户，而重新训练基线方法需要花费将近一天的时间，当只有很少的注册数据可用时，比基线方法更可靠，可以用于识别使用不同VR设备记录的新用户数据集。综上所述，我们的解决方案为易于扩展的XR用户识别系统奠定基础，可应用于广泛场景。

    In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
    
[^143]: 使用统一的E-值通过FDR控制去随机化的新颖性检测

    Derandomized Novelty Detection with FDR Control via Conformal E-values. (arXiv:2302.07294v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.07294](http://arxiv.org/abs/2302.07294)

    通过使用统一E-值来量化统计显著性，我们提出了一种去随机化的新颖性检测方法，该方法可以稳定地聚合相同数据的多次分析的证据，同时控制虚假发现率。

    

    统一推理提供了一种通用的无分布方法，用于严格校准任何机器学习算法的新颖性检测输出。虽然这种方法有很多优点，但它也有随机性的限制，即在分析相同数据时可能会导致不同的结果，这可能会妨碍对任何发现的解释。我们提出通过利用适当的统一E-值而不是p-值来量化统计显著性，使统一推理更加稳定。这种解决方案允许有效地汇总对相同数据多次分析的证据，同时可靠地控制虚假发现率。此外，我们还展示了与标准统一推理相比，所提方法可以减少随机性而不会损失太多功率，部分原因是基于从相同数据中精心提取的附加辅助信息来加权统一E-值的创新方法。通过合成数据的模拟实验表明...

    Conformal inference provides a general distribution-free method to rigorously calibrate the output of any machine learning algorithm for novelty detection. While this approach has many strengths, it has the limitation of being randomized, in the sense that it may lead to different results when analyzing twice the same data, and this can hinder the interpretation of any findings. We propose to make conformal inferences more stable by leveraging suitable conformal e-values instead of p-values to quantify statistical significance. This solution allows the evidence gathered from multiple analyses of the same data to be aggregated effectively while provably controlling the false discovery rate. Further, we show that the proposed method can reduce randomness without much loss of power compared to standard conformal inference, partly thanks to an innovative way of weighting conformal e-values based on additional side information carefully extracted from the same data. Simulations with synthet
    
[^144]: 针对任务的技能定位在Fine-tuned语言模型中的应用

    Task-Specific Skill Localization in Fine-tuned Language Models. (arXiv:2302.06600v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.06600](http://arxiv.org/abs/2302.06600)

    本文提出了针对Fine-tuned语言模型中任务特定技能定位的问题，并提出了一种解决方案，通过优化可以识别出贡献模型性能的非常小的参数子集，使得将Fine-tuned的值嫁接到这个子集上可以获得几乎和Fine-tuned模型一样好的性能。

    

    预训练的语言模型可以通过Fine-tuned来解决各种NLP任务，包括少样本情况下的任务。因此，Fine-tuning使得模型能够快速掌握任务特定的“技能”，但是关于这些新学到的技能在庞大模型中的位置的研究还很有限。本文引入了技能定位的概念，并提出了一种解决方案。给定下游任务和在该任务上进行Fine-tuned的模型，利用简单的优化方法可以识别出负责模型性能的非常小的参数子集（占模型参数的约0.01%），这个子集对模型性能的贡献占比超过95%。换句话说，仅将Fine-tuned的值嫁接到预训练模型的这个小子集上，就可以获得几乎和Fine-tuned模型一样好的性能。虽然与最近关于参数高效Fine-tuning的工作相似，但这里的两个新颖之处是：（i）不需要在子集上进行进一步的重新训练（不像“lottery tickets”那样）。（ii）相对于传统的Fine-tuned模型，可以看到显著的改进。

    Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters ($\sim0.01$% of model parameters) responsible for ($>95$%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanil
    
[^145]: 少即是多：选择性层微调与子微调

    Less is More: Selective Layer Finetuning with SubTuning. (arXiv:2302.06354v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06354](http://arxiv.org/abs/2302.06354)

    本研究提出了一种选择性层微调与子微调的方法，通过仅对精心选择的层进行微调，而将其余权重保持在预训练值上。该方法在准确性上能够与全模型微调相媲美，并在训练数据稀缺时表现更好。这一简单而有效的方法适用于多任务学习，并能够在推理过程中实现任务间的资源共享。

    

    对预训练模型进行微调已成为在新任务上训练神经网络的标准方法，导致了快速收敛和改善的性能。在这项工作中，我们研究了一种替代的微调方法，即不对网络的所有权重进行微调，而是只训练一组精心选择的层，使其余的权重保持在其初始（预训练）值上。我们证明了\emph{子微调}（SubTuning）经常能够达到与对模型进行全微调相当的准确性，并且在训练数据稀缺时甚至超过了全微调的性能。因此，SubTuning允许以最小的计算成本部署新任务，同时享受整个模型微调的好处。这为多任务学习提供了一种简单有效的方法，在推理时不同任务之间不干扰，而在大部分资源上共享。我们展示了SubTuning在多个任务上的效率。

    Finetuning a pretrained model has become a standard approach for training neural networks on novel tasks, resulting in fast convergence and improved performance. In this work, we study an alternative finetuning method, where instead of finetuning all the weights of the network, we only train a carefully chosen subset of layers, keeping the rest of the weights frozen at their initial (pretrained) values. We demonstrate that \emph{subset finetuning} (or SubTuning) often achieves accuracy comparable to full finetuning of the model, and even surpasses the performance of full finetuning when training data is scarce. Therefore, SubTuning allows deploying new tasks at minimal computational cost, while enjoying the benefits of finetuning the entire model. This yields a simple and effective method for multi-task learning, where different tasks do not interfere with one another, and yet share most of the resources at inference time. We demonstrate the efficiency of SubTuning across multiple task
    
[^146]: Sneaky Spikes: 用神经形态数据在脉冲神经网络中揭示隐蔽的后门攻击

    Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data. (arXiv:2302.06279v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.06279](http://arxiv.org/abs/2302.06279)

    本文研究使用神经形态数据和多样化的刺激在脉冲神经网络中的后门攻击问题。

    

    深度神经网络（DNN）在各种任务中显示出了卓越的性能，包括图像和语音识别。然而，最大化DNN的效果需要通过训练对众多超参数和网络参数进行精细优化。此外，高性能的DNN涉及许多参数，在训练过程中消耗大量能源。为了克服这些挑战，研究人员转向了脉冲神经网络（SNN），其提供了增强的能源效率和生物学可行的数据处理能力，使其非常适合感知数据任务，特别是在神经形态数据方面。尽管存在优势，SNN与DNN一样，容易受到各种威胁的影响，包括对抗性示例和后门攻击。然而，关于SNN在理解和对抗这些攻击方面，仍需要进一步探索。本文深入研究了使用神经形态数据和多样化的刺激的SNN中的后门攻击。

    Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.  This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse trig
    
[^147]: 一种适用于核心测试的U统计的高维收敛定理

    A High-dimensional Convergence Theorem for U-statistics with Applications to Kernel-based Testing. (arXiv:2302.05686v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2302.05686](http://arxiv.org/abs/2302.05686)

    本论文证明了一个适用于核心测试的高维U统计的收敛定理，并发现U统计的极限分布会经历从非退化高斯极限到退化极限的相变。这一现象对于高维情况下的非退化U统计具有较大方差和不对称分布的非高斯极限具有重要意义。此外，我们提出的界限适用于任何有限数量和维度的样本，与底层函数的特征值无关，并且在某些假设下与维度无关。我们还将我们的理论应用到两个常用的基于核函数的分布测试方法，MMD和KSD，来研究它们的高维性能。我们的结果能够准确预测测试功率如何与维度和带宽的关系。

    

    我们证明了一个U统计的二次收敛定理，其中数据维度$d$可以随样本大小$n$的变化而变化。我们发现，一个U统计的极限分布会经历从非退化高斯极限到退化极限的相变，不论其退化性如何，只取决于一个矩比率。一个令人惊讶的结果是，在高维情况下，一个非退化的U统计可能具有一个具有较大方差和不对称分布的非高斯极限。我们的界限对任何有限的$n$和$d$都是有效的，与底层函数的个别特征值无关，并且在一个适度的假设下与维度无关。作为应用，我们将我们的理论应用到两个流行的基于核心的分布测试，MMD和KSD上，这些测试在高维性能的研究一直是有挑战性的。在一个简单的经验设置中，我们的结果正确地预测了在固定阈值下测试功率如何随着$d$和带宽的缩放。

    We prove a convergence theorem for U-statistics of degree two, where the data dimension $d$ is allowed to scale with sample size $n$. We find that the limiting distribution of a U-statistic undergoes a phase transition from the non-degenerate Gaussian limit to the degenerate limit, regardless of its degeneracy and depending only on a moment ratio. A surprising consequence is that a non-degenerate U-statistic in high dimensions can have a non-Gaussian limit with a larger variance and asymmetric distribution. Our bounds are valid for any finite $n$ and $d$, independent of individual eigenvalues of the underlying function, and dimension-independent under a mild assumption. As an application, we apply our theory to two popular kernel-based distribution tests, MMD and KSD, whose high-dimensional performance has been challenging to study. In a simple empirical setting, our results correctly predict how the test power at a fixed threshold scales with $d$ and the bandwidth.
    
[^148]: 基于状态的安全强化学习：综述

    State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03122](http://arxiv.org/abs/2302.03122)

    本文综合回顾了强化学习中解决基于状态约束的方法，讨论了它们在安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面的联系、差异和权衡，并讨论了未来发展方向。

    

    尽管强化学习在仿真环境中取得了巨大的成功，但将其应用于实际场景仍然面临许多挑战。其中一个主要关注点是安全性，也就是约束满足。状态约束是实际应用中最常见且最具挑战性的约束之一，这对于许多挑战性任务，如自动驾驶、机器人操作等而言是必要和关键的。本文综述了现有的解决基于状态的约束的强化学习方法，并在状态约束马尔可夫决策过程的框架下，从安全保障和可扩展性、安全性和奖励表现、收敛后和训练过程中的安全性等方面，讨论了现有方法的联系、差异和权衡。我们还总结了当前方法的局限性并讨论了未来发展方向。

    Despite the tremendous success of Reinforcement Learning (RL) algorithms in simulation environments, applying RL to real-world applications still faces many challenges. A major concern is safety, in another word, constraint satisfaction. State-wise constraints are one of the most common constraints in real-world applications and one of the most challenging constraints in Safe RL. Enforcing state-wise constraints is necessary and essential to many challenging tasks such as autonomous driving, robot manipulation. This paper provides a comprehensive review of existing approaches that address state-wise constraints in RL. Under the framework of State-wise Constrained Markov Decision Process (SCMDP), we will discuss the connections, differences, and trade-offs of existing approaches in terms of (i) safety guarantee and scalability, (ii) safety and reward performance, and (iii) safety after convergence and during training. We also summarize limitations of current methods and discuss potentia
    
[^149]: 增强潜空间贝叶斯优化中的探索能力

    Enhancing Exploration in Latent Space Bayesian Optimization. (arXiv:2302.02399v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02399](http://arxiv.org/abs/2302.02399)

    本文提出了一种新的方法来提高潜空间贝叶斯优化（LSBO）的探索能力。方法包括潜在一致性感知获取函数（LCA-AF）和增加一致性点的潜空间生成方法（LCA-VAE），将它们结合起来形成了LCA-LSBO。实验证明LCA-LSBO在图像生成和全新的化学设计任务中表现出改进的性能。

    

    潜空间贝叶斯优化（LSBO）将生成模型（通常是变分自编码器）与贝叶斯优化相结合，以生成感兴趣的全新对象。然而，由于贝叶斯优化和变分自编码器之间的目标不匹配，导致了LSBO面临挑战和推广能力的减弱。本文提出了增强LSBO效率并克服这一挑战的新思路。首先，我们引入了潜在的一致性和不一致性的概念，这是LSBO中一个关键的问题，起源于BO-VAE之间的不匹配。为了解决这个问题，我们提出了潜在的一致意识获取函数（LCA-AF），利用LSBO中的一致性区域。此外，我们提出了LCA-VAE，一种新的VAE方法，它生成具有增加的一致性点的潜空间，提高了BO的推广能力。结合LCA-VAE和LCA-AF，我们发展了LCA-LSBO。实验评估证实了LCA-LSBO在图像生成和全新的化学设计任务中的改进性能。

    Latent Space Bayesian Optimization (LSBO) combines generative models, typically Variational Autoencoders (VAE), with Bayesian Optimization (BO) to generate de novo objects of interest. However, LSBO faces challenges due to the mismatch between the objectives of BO and VAE, resulting in poor extrapolation capabilities. In this paper, we propose novel contributions to enhance LSBO efficiency and overcome this challenge. We first introduce the concept of latent consistency/inconsistency as a crucial problem in LSBO, arising from the BO-VAE mismatch. To address this, we propose the Latent Consistent Aware-Acquisition Function (LCA-AF) that leverages consistent regions in LSBO. Additionally, we present LCA-VAE, a novel VAE method that generates a latent space with increased consistent points, improving BO's extrapolation capabilities. Combining LCA-VAE and LCA-AF, we develop LCA-LSBO. Experimental evaluations validate the improved performance of LCA-LSBO in image generation and de-novo chem
    
[^150]: 通过自我对战实现多样化诱导的环境设计

    Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.02119](http://arxiv.org/abs/2302.02119)

    本文提出了一种利用自我对战技术的任务不可知方法，来识别环境中的观察/隐藏状态，并将多样性引入非监督环境设计框架中，从而提高了环境设计的效率和有效性。

    

    最近关于环境分布设计的研究已经展示出训练有效的通用能力代理的前景。它的成功部分在于一种自适应课程学习的形式，该形式通过生成代理能力的前沿环境实例（或级别）。然而，这种环境设计框架经常在具有挑战性的设计空间中发现有效级别方面存在困难，并需要与环境进行高成本交互。本文的目的是在非监督环境设计（UED）框架中引入多样性。具体来说，我们提出了一种任务不可知的方法来识别对给定级别具有代表性的观察/隐藏状态。然后利用这种方法的结果来表征两个级别之间的多样性，正如我们所展示的，这对于有效性能至关重要。此外，为了提高采样效率，我们加入了自我对战技术，使得环境生成器能够自动生成环境。

    Recent work on designing an appropriate distribution of environments has shown promise for training effective generally capable agents. Its success is partly because of a form of adaptive curriculum learning that generates environment instances (or levels) at the frontier of the agent's capabilities. However, such an environment design framework often struggles to find effective levels in challenging design spaces and requires costly interactions with the environment. In this paper, we aim to introduce diversity in the Unsupervised Environment Design (UED) framework. Specifically, we propose a task-agnostic method to identify observed/hidden states that are representative of a given level. The outcome of this method is then utilized to characterize the diversity between two levels, which as we show can be crucial to effective performance. In addition, to improve sampling efficiency, we incorporate the self-play technique that allows the environment generator to automatically generate e
    
[^151]: 学习优化增强学习

    Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01470](http://arxiv.org/abs/2302.01470)

    学习优化器在监督学习中取得了显著的成功，但在强化学习中面临梯度范围变化大、梯度分布非独立且不同、高方差偏差等问题。本文提出了梯度处理、管道训练和一种新颖的优化器结构来解决这些问题。

    

    近年来，通过利用更多的数据、计算和不同的任务，学习优化器在监督学习中取得了显著的成功，超过了传统手动设计的优化器。然而，强化学习与监督学习本质上不同，这些学习优化器在简单的强化学习任务中效果不佳。我们调查了这一现象，发现了三个问题。首先，强化学习代理的梯度在对数上变化范围很大，而在绝对值上范围较小，这使得神经网络难以获得准确的参数更新。其次，代理梯度分布非独立且不同，导致元训练效率低下。最后，由于代理与环境之间的高度随机交互，代理梯度存在较高的偏差和方差，增加了强化学习优化器的学习难度。我们提出了梯度处理、管道训练和一种新颖的优化器结构。

    In recent years, by leveraging more data, computation, and diverse tasks, learned optimizers have achieved remarkable success in supervised learning, outperforming classical hand-designed optimizers. Reinforcement learning (RL) is essentially different from supervised learning and in practice these learned optimizers do not work well even in simple RL tasks. We investigate this phenomenon and identity three issues. First, the gradients of an RL agent vary across a wide range in logarithms while their absolute values are in a small range, making neural networks hard to obtain accurate parameter updates. Second, the agent-gradient distribution is non-independent and identically distributed, leading to inefficient meta-training. Finally, due to highly stochastic agent-environment interactions, the agent-gradients have high bias and variance, which increase the difficulty of learning an optimizer for RL. We propose gradient processing, pipeline training, and a novel optimizer structure wit
    
[^152]: 通过本地规划实现样本有效的深度强化学习

    Sample Efficient Deep Reinforcement Learning via Local Planning. (arXiv:2301.12579v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12579](http://arxiv.org/abs/2301.12579)

    提出了一种名为UFLP的算法框架，通过重置环境到高不确定性状态来提高深度强化学习的样本效率，实验证明这个简单的过程可以显著改善采样成本，并在困难的探索任务上取得超人类的表现。

    

    本文的重点是在模拟器上进行样本有效的深度强化学习。模拟器的一个有用特性是可以将环境重置到先前观察到的状态。我们提出了一种名为“不确定性优先本地规划”（UFLP）的算法框架，利用了这个特性。具体而言，在每个数据收集迭代中，我们的元算法以一定的概率将环境重置为具有高不确定性的观察状态，而不是根据初始状态分布进行采样。然后，代理-环境交互就像在标准在线强化学习设置中一样进行。我们证明了这个简单的过程可以显著改善几个基线强化学习算法在困难的探索任务上的采样成本。值得注意的是，利用我们的框架，我们可以使用简单（分布式）双重DQN在臭名昭著的难度很高的Atari游戏“蒙特祖玛之复仇”上实现超人类的表现。我们的工作可以看作是一种有效的方法。

    The focus of this work is sample-efficient deep reinforcement learning (RL) with a simulator. One useful property of simulators is that it is typically easy to reset the environment to a previously observed state. We propose an algorithmic framework, named uncertainty-first local planning (UFLP), that takes advantage of this property. Concretely, in each data collection iteration, with some probability, our meta-algorithm resets the environment to an observed state which has high uncertainty, instead of sampling according to the initial-state distribution. The agent-environment interaction then proceeds as in the standard online RL setting. We demonstrate that this simple procedure can dramatically improve the sample cost of several baseline RL algorithms on difficult exploration tasks. Notably, with our framework, we can achieve super-human performance on the notoriously hard Atari game, Montezuma's Revenge, with a simple (distributional) double DQN. Our work can be seen as an efficie
    
[^153]: 在图像中提取认知后门模式的方法: 一种用于后门样本检测的 SOTA 方法

    Distilling Cognitive Backdoor Patterns within an Image: A SOTA Method for Backdoor Sample Detection. (arXiv:2301.10908v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10908](http://arxiv.org/abs/2301.10908)

    本文提出了一种用于提取和检测图像中后门模式的简单方法，称为认知精炼（CD）。通过优化输入掩码，我们可以提取一个小模式，该模式可以导致模型产生相同的输出。使用CD和提取的模式，我们发现了后门攻击的一个有趣现象：后门样本的模式都非常小。所以可以利用学到的掩码从污染的训练数据集中检测和删除后门样本。

    

    本文提出了一种简单的方法来提取和检测图像中的后门模式: 认知精炼 (CD)。该方法通过优化输入掩码来提取输入图像中的小模式，该模式可以导致模型产生相同的输出。通过使用CD和提取的模式，我们揭示了后门攻击的一个有趣现象：尽管不同攻击使用不同形式和大小的触发模式，但后门样本的模式都惊人地小。因此，可以利用学到的掩码从被污染的训练数据集中检测和删除后门样本。我们进行了大量实验证明CD可以稳健地检测各种高级后门样本。

    This paper proposes a simple method to distill and detect backdoor patterns within an image: \emph{Cognitive Distillation} (CD). The idea is to extract the "minimal essence" from an input image responsible for the model's prediction. CD optimizes an input mask to extract a small pattern from the input image that can lead to the same model output (i.e., logits or deep features). The extracted pattern can help understand the cognitive mechanism of a model on clean vs. backdoor images and is thus called a \emph{Cognitive Pattern} (CP). Using CD and the distilled CPs, we uncover an interesting phenomenon of backdoor attacks: despite the various forms and sizes of trigger patterns used by different attacks, the CPs of backdoor samples are all surprisingly and suspiciously small. One thus can leverage the learned mask to detect and remove backdoor examples from poisoned training datasets. We conduct extensive experiments to show that CD can robustly detect a wide range of advanced backdoor a
    
[^154]: 通过学习保证提高公平性

    Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10813](http://arxiv.org/abs/2301.10813)

    该论文提出了一种公平质量度量方法，名为判别风险，旨在反映个体和群体公平性。此外，研究者还讨论了公平性是否可以在理论上得到保证。

    

    随着机器学习系统在越来越多的现实场景中得到广泛应用，对于隐藏在机器学习模型中的潜在歧视的担忧正在增加。许多技术已经被开发出来以增强公平性，包括常用的群体公平性度量和几种结合集成学习的公平感知方法。然而，现有的公平度量只能关注其中之一，即群体公平性或个体公平性，它们之间的硬性兼容性暗示了即使其中之一得到满足，仍可能存在偏见。此外，现有的提升公平性的机制通常只提供经验结果来证明其有效性，但很少有论文讨论公平性是否可以在理论上得到保证。为了解决这些问题，本文提出了一种公平质量度量方法——判别风险，以反映个体和群体公平性两个方面。此外，我们还研究了p...

    The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
    
[^155]: 寻找电子商务营销的相似客户

    Finding Lookalike Customers for E-Commerce Marketing. (arXiv:2301.03147v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03147](http://arxiv.org/abs/2301.03147)

    本文介绍了一个以客户为中心的营销活动中寻找相似客户的可扩展和高效系统。该系统能处理亿级客户，并使用深度学习嵌入模型和近似最近邻搜索方法来寻找感兴趣的相似客户。通过构建可解释且有意义的客户相似度度量，该模型能够处理各种业务兴趣。

    

    以客户为中心的营销活动为沃尔玛的电子商务网站流量贡献了很大的一部分。随着客户数据规模的增大，扩大营销受众以触达更多客户对电子商务公司的业务增长和为客户带来更多价值变得更为关键。在本文中，我们提出了一个可扩展且高效的系统来扩大营销活动的目标受众，该系统可以处理亿级客户。我们使用基于深度学习的嵌入模型来表示客户，使用一种近似最近邻搜索方法快速找到感兴趣的相似客户。该模型能够通过构建可解释且有意义的客户相似度度量来处理各种业务兴趣。我们进行了大量实验来展示我们的系统和客户嵌入模型的出色性能。

    Customer-centric marketing campaigns generate a large portion of e-commerce website traffic for Walmart. As the scale of customer data grows larger, expanding the marketing audience to reach more customers is becoming more critical for e-commerce companies to drive business growth and bring more value to customers. In this paper, we present a scalable and efficient system to expand targeted audience of marketing campaigns, which can handle hundreds of millions of customers. We use a deep learning based embedding model to represent customers and an approximate nearest neighbor search method to quickly find lookalike customers of interest. The model can deal with various business interests by constructing interpretable and meaningful customer similarity metrics. We conduct extensive experiments to demonstrate the great performance of our system and customer embedding model.
    
[^156]: CC-FedAvg：计算定制的联邦平均算法

    CC-FedAvg: Computationally Customized Federated Averaging. (arXiv:2212.13679v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.13679](http://arxiv.org/abs/2212.13679)

    本论文提出了一个称为CC-FedAvg的计算定制的联邦平均算法，可让参与者根据其计算预算决定在每轮中是否执行传统的本地训练或模型估算。实验结果表明，CC-FedAvg能够显著提高模型性能并降低通信成本。

    

    联邦学习是一种新兴的模型训练方式，通过分布在众多物联网设备上的数据进行模型训练。它在本质上假设参与者的计算能力相同，但实际上，由于不同的能源预算或并行执行的任务不同，参与者计算资源存在着差异。缺乏计算预算的参与者必须适当规划其受限计算资源的使用，否则他们将无法完成整个训练过程，导致模型性能下降。为了解决这个问题，我们提出了一种估算本地模型而无需计算密集迭代的策略。基于此，我们提出了计算定制的联邦平均算法(CC-FedAvg)，允许参与者根据其当前的计算预算，在每个轮次中决定是执行传统的本地训练还是模型估算。理论分析和实验结果均表明，与传统的联邦平均算法相比，CC-FedAvg能显著提高模型性能并降低通信成本。

    Federated learning (FL) is an emerging paradigm to train model with distributed data from numerous Internet of Things (IoT) devices. It inherently assumes a uniform capacity among participants. However, due to different conditions such as differing energy budgets or executing parallel unrelated tasks, participants have diverse computational resources in practice. Participants with insufficient computation budgets must plan for the use of restricted computational resources appropriately, otherwise they would be unable to complete the entire training procedure, resulting in model performance decline. To address the this issue, we propose a strategy for estimating local models without computationally intensive iterations. Based on it, we propose Computationally Customized Federated Averaging (CC-FedAvg), which allows participants to determine whether to perform traditional local training or model estimation in each round based on their current computational budgets. Both theoretical analy
    
[^157]: 何时不信任语言模型：探索参数和非参数记忆的有效性和限制。

    When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories. (arXiv:2212.10511v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2212.10511](http://arxiv.org/abs/2212.10511)

    本文通过对10个模型和4种增强方法的实验，发现语言模型在记忆不太流行的实际知识方面存在困难，而检索增强的语言模型表现较好，提出了一种检索增强语言模型的简单有效方法。

    

    尽管大型语言模型在各种任务上表现出色，但仍然难以处理需要丰富世界知识的任务，这暗示了仅依靠其参数来编码丰富的世界知识的局限性。本文旨在通过对10个模型和4种增强方法在PopQA上进行大规模知识探测实验，以了解语言模型在记忆事实知识方面的优点和局限性。我们发现，语言模型难以记忆不太流行的实际知识，并且在长尾中，扩展规模无法明显改善记忆实际知识。然后，我们展示了检索增强的语言模型在很大程度上胜过级别大得多的语言模型，而未经协助的语言模型在涉及高流行实体的问题上仍然具有竞争力。基于这些发现，我们设计了一种简单而有效的强大和高效的检索增强语言模型方法，该方法仅在需要时检索非参数记忆。

    Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs' strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on PopQA, our new open-domain QA dataset with 14k questions. We find that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the long tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those findings, we devise a simple, yet effective, method for powerful and efficient retrieval-augmented LMs, which retrieves non-parametric memories only whe
    
[^158]: 机器学习和聚合物自洽场理论在二维空间中的应用

    Machine Learning and Polymer Self-Consistent Field Theory in Two Spatial Dimensions. (arXiv:2212.10478v2 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2212.10478](http://arxiv.org/abs/2212.10478)

    本文介绍了一种利用机器学习和聚合物自洽场理论模拟数据的计算框架，用于加速研究嵌段共聚物参数空间。创新包括使用CNN处理离散化的局部平均单体密度场、使用GAN预测局部平均单体密度场，以及实现空间平移和旋转不变性。该框架取得了成功。

    

    本文提出了一种利用深度学习从自洽场理论模拟中获取数据，加速研究嵌段共聚物参数空间的计算框架。这是对[1]中引入的框架的实质性二维扩展。文章提出了几个创新和改进: (1) 使用Sobolev空间训练的卷积神经网络(CNN)来处理离散化的局部平均单体密度场的指数维度增加，并对预测的场论密集哈密顿量实施强力的空间平移和旋转不变性。 (2) 引入了生成对抗网络(GAN)来高效准确地预测鞍点、局部平均单体密度场，而无需使用训练集进行梯度下降方法。这种GAN方法节省了存储和计算成本。 (3) 所提出的机器学习框架取得了成功。

    A computational framework that leverages data from self-consistent field theory simulations with deep learning to accelerate the exploration of parameter space for block copolymers is presented. This is a substantial two-dimensional extension of the framework introduced in [1]. Several innovations and improvements are proposed. (1) A Sobolev space-trained, convolutional neural network (CNN) is employed to handle the exponential dimension increase of the discretized, local average monomer density fields and to strongly enforce both spatial translation and rotation invariance of the predicted, field-theoretic intensive Hamiltonian. (2) A generative adversarial network (GAN) is introduced to efficiently and accurately predict saddle point, local average monomer density fields without resorting to gradient descent methods that employ the training set. This GAN approach yields important savings of both memory and computational cost. (3) The proposed machine learning framework is successfull
    
[^159]: 针对持续学习脆弱性的数据污染攻击

    Data Poisoning Attack Aiming the Vulnerability of Continual Learning. (arXiv:2211.15875v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15875](http://arxiv.org/abs/2211.15875)

    该论文针对基于正则化的持续学习方法的脆弱性，通过提出一种简单的任务特定数据污染攻击来展示其脆弱性。该攻击导致特定任务的性能下降，并且实验证明了这种脆弱性的重要性。

    

    通常，基于正则化的持续学习模型限制对先前任务数据的访问，以模拟与内存和隐私有关的真实世界约束。然而，这在这些模型中引入了一个问题，即无法跟踪每个任务的性能。实质上，当前的持续学习方法容易受到对先前任务的攻击。我们通过提出一种简单的任务特定数据污染攻击来展示基于正则化的持续学习方法的脆弱性，该攻击可以在新任务的学习过程中使用。所提攻击生成的训练数据导致攻击者针对的特定任务性能下降。我们在两个具有代表性的基于正则化的持续学习方法（弹性权重整合（EWC）和突触智能（SI））上进行了攻击实验，这两种方法是使用MNIST数据集的变体进行训练的。实验结果证明了本文提出的脆弱性，并展示了其重要性。

    Generally, regularization-based continual learning models limit access to the previous task data to imitate the real-world constraints related to memory and privacy. However, this introduces a problem in these models by not being able to track the performance on each task. In essence, current continual learning methods are susceptible to attacks on previous tasks. We demonstrate the vulnerability of regularization-based continual learning methods by presenting a simple task-specific data poisoning attack that can be used in the learning process of a new task. Training data generated by the proposed attack causes performance degradation on a specific task targeted by the attacker. We experiment with the attack on the two representative regularization-based continual learning methods, Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI), trained with variants of MNIST dataset. The experiment results justify the vulnerability proposed in this paper and demonstrate the importa
    
[^160]: Shapley曲线：一种平滑视角

    Shapley Curves: A Smoothing Perspective. (arXiv:2211.13289v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.13289](http://arxiv.org/abs/2211.13289)

    本文以平滑的角度引入了Shapley曲线作为局部变量重要性的度量，提出了两种估计策略，并在特征的独立和依赖情况下得到了一致性和渐近正态性，为估计的Shapley曲线构建了置信区间并进行了推断，通过实验证实了渐近结果。应用中分析了哪些属性驱动车辆价格。

    

    源自合作博弈理论，Shapley值已成为应用机器学习中最广泛使用的变量重要性度量之一。然而，对Shapley值的统计理解仍然有限。本文以非参数(或平滑)的角度，引入Shapley曲线作为局部变量重要性的度量。我们提出了两种估计策略，并在特征独立和依赖的情况下都得出了一致性和渐近正态性。这样，我们可以构建置信区间并对估计的Shapley曲线进行推断。我们提出了一种新颖的野蛮引导程序版本，专门调整以获得Shapley曲线的良好有限样本覆盖。渐近结果在大量实验证实了。在实证应用中，我们分析了哪些属性驱动了车辆的价格。

    Originating from cooperative game theory, Shapley values have become one of the most widely used measures for variable importance in applied Machine Learning. However, the statistical understanding of Shapley values is still limited. In this paper, we take a nonparametric (or smoothing) perspective by introducing Shapley curves as a local measure of variable importance. We propose two estimation strategies and derive the consistency and asymptotic normality both under independence and dependence among the features. This allows us to construct confidence intervals and conduct inference on the estimated Shapley curves. We propose a novel version of the wild bootstrap procedure, specifically adjusted to give good finite sample coverage of the Shapley curves. The asymptotic results are validated in extensive experiments. In an empirical application, we analyze which attributes drive the prices of vehicles.
    
[^161]: 无标签数据的后门清除

    Backdoor Cleansing with Unlabeled Data. (arXiv:2211.12044v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12044](http://arxiv.org/abs/2211.12044)

    本文提出了一种无标签数据的后门清除方法，通过逐层权重重新初始化和知识蒸馏来有效清除可疑网络的后门行为，并在基准数据集上取得较好效果。

    

    随着深度神经网络的计算需求增加，公司和组织已经开始外部化训练过程。但是，外部训练的深度神经网络可能会面临后门攻击。因此，关键在于防御这种攻击，即后处理一个可疑模型，使其的后门行为得到缓解，同时其对于干净输入的正常预测能力仍然保持不受影响。为了消除异常的后门行为，现有方法主要依赖于额外的标记干净样本。然而，这样的要求可能是不切实际的，因为训练数据通常对最终用户不可用。本文研究了绕过这种障碍的可能性，并提出了一种新的防御方法，不需要训练标签。通过精心设计的逐层权重重新初始化和知识蒸馏，我们的方法可以有效地清除可疑网络的后门行为，同时对其正常行为的影响微乎其微。我们在基准数据集上评估了我们的方法，并显示它胜过现有的无标签防御方法。

    Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network with negligible compromise in its normal beh
    
[^162]: 通过视觉-语言模型的指令增强实现机器人技能获取

    Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. (arXiv:2211.11736v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.11736](http://arxiv.org/abs/2211.11736)

    本文提出了一种通过视觉-语言模型的指令增强方法，利用预训练模型将互联网规模的知识导入现有机器人数据集，实现机器人技能的获取。

    

    最近几年，在学习遵循自然语言指令的机器人操作策略方面取得了很大进展。这些方法通常从机器人-语言数据语料库中学习，该数据要么是为特定任务而收集的，要么是在事后由人工昂贵地重新标注的，带有丰富的语言描述。最近，大规模预训练的视觉-语言模型（VLMs）如CLIP或ViLD已应用于机器人学习表示和场景描述。这些预训练模型能否作为机器人数据的自动标注工具，将互联网规模的知识导入现有数据集，使其对于未在其地面真实注释中反映的任务也能发挥作用？为了实现这一目标，我们引入了数据驱动的指令增强（DIAL）来用于基于语言的控制：我们利用CLIP的语义理解，利用半监督的语言标签将知识传递到大规模无标签演示数据集上。

    In recent years, much progress has been made in learning robotic manipulation policies that follow natural language instructions. Such methods typically learn from corpora of robot-language data that was either collected with specific tasks in mind or expensively re-labelled by humans with rich language descriptions in hindsight. Recently, large-scale pretrained vision-language models (VLMs) like CLIP or ViLD have been applied to robotics for learning representations and scene descriptors. Can these pretrained models serve as automatic labelers for robot data, effectively importing Internet-scale knowledge into existing datasets to make them useful even for tasks that are not reflected in their ground truth annotations? To accomplish this, we introduce Data-driven Instruction Augmentation for Language-conditioned control (DIAL): we utilize semi-supervised language labels leveraging the semantic understanding of CLIP to propagate knowledge onto large datasets of unlabelled demonstration
    
[^163]: 多模态时间数据的主动获取：一个具有挑战性的决策任务

    Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task. (arXiv:2211.05039v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.05039](http://arxiv.org/abs/2211.05039)

    该论文提出了一个具有挑战性的决策任务，主动获取多模态时间数据。通过权衡获取成本和预测性能，学习代理程序来主动选择获取的输入模态。该方法能够解决具有实际相关推理技能的合成情景，并在真实数据集上成功学习到成本反应式的获取行为，但无法学习到自适应的获取策略，突显了任务的困难性。

    

    我们介绍了一个具有挑战性的决策任务，我们称之为多模态时间数据的主动获取（A2MT）。在许多实际场景中，输入特征在测试时不容易获得，必须以较大代价获取。通过A2MT，我们的目标是学习代理程序，使其能够主动选择要获取的输入模态，权衡获取成本与预测性能。A2MT扩展了之前的任务，称为主动特征获取，以便进行关于高维输入的时间决策。我们提出了一种基于Perceiver IO架构的方法来实现A2MT。我们的代理程序能够解决一个需要实际相关的跨模态推理技能的新颖合成情景。在两个大规模的真实数据集Kinetics-700和AudioSet上，我们的代理程序成功地学习了成本反应式的获取行为。然而，消融实验表明它们无法学习到自适应的获取策略，突显了该任务的困难性。

    We introduce a challenging decision-making task that we call active acquisition for multimodal temporal data (A2MT). In many real-world scenarios, input features are not readily available at test time and must instead be acquired at significant cost. With A2MT, we aim to learn agents that actively select which modalities of an input to acquire, trading off acquisition cost and predictive performance. A2MT extends a previous task called active feature acquisition to temporal decision making about high-dimensional inputs. We propose a method based on the Perceiver IO architecture to address A2MT in practice. Our agents are able to solve a novel synthetic scenario requiring practically relevant cross-modal reasoning skills. On two large-scale, real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn cost-reactive acquisition behavior. However, an ablation reveals they are unable to learn adaptive acquisition strategies, emphasizing the difficulty of the task even for 
    
[^164]: 量子概率哈密顿学习用于生成模型和异常检测

    Quantum-probabilistic Hamiltonian learning for generative modelling & anomaly detection. (arXiv:2211.03803v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.03803](http://arxiv.org/abs/2211.03803)

    本研究探讨了学习和利用量子力学系统的哈密顿和其变分热状态估计用于数据分析技术的可能性。我们通过将方法应用于大型强子对撞机数据的生成建模和异常检测中，展示了这些方法在量子多体系统中的应用潜力。

    

    一个孤立的量子力学系统的哈密顿决定了其动力学和物理行为。本研究调查了学习和利用系统的哈密顿和其变分热状态估计用于数据分析技术的可能性。为此，我们采用基于量子哈密顿的模型方法对模拟大型强子对撞机数据进行生成建模，并证明了这样的数据可以表示为混合态。在进一步的步骤中，我们利用学到的哈密顿进行异常检测，展示了不同样本类型在被视为量子多体系统时可以形成不同的动力学行为。我们利用这些特征来量化样本类型之间的差异。我们的研究结果表明，设计用于场论计算的方法可以在机器学习应用中利用，从而将理论方法应用于数据分析技术中。

    The Hamiltonian of an isolated quantum mechanical system determines its dynamics and physical behaviour. This study investigates the possibility of learning and utilising a system's Hamiltonian and its variational thermal state estimation for data analysis techniques. For this purpose, we employ the method of Quantum Hamiltonian-Based Models for the generative modelling of simulated Large Hadron Collider data and demonstrate the representability of such data as a mixed state. In a further step, we use the learned Hamiltonian for anomaly detection, showing that different sample types can form distinct dynamical behaviours once treated as a quantum many-body system. We exploit these characteristics to quantify the difference between sample types. Our findings show that the methodologies designed for field theory computations can be utilised in machine learning applications to employ theoretical approaches in data analysis techniques.
    
[^165]: 有限数据资源下的图分类器遗忘方法

    Unlearning Graph Classifiers with Limited Data Resources. (arXiv:2211.03216v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03216](http://arxiv.org/abs/2211.03216)

    本文研究了有限数据资源下的图分类器遗忘方法，提出了一种基于图散射变换的非线性近似图遗忘方法，并进行了计算复杂性的理论分析。

    

    随着用户隐私需求的增加，对机器学习模型进行受控数据删除（机器遗忘）成为数据敏感的网络应用（如社交网络和推荐系统）中的重要特性。然而，目前还不清楚如何在图神经网络（GNNs）中进行高效的机器遗忘，尤其是在训练样本数量较少的情况下，此时遗忘可能严重损害模型性能。为解决这个问题，我们首次研究了遗忘图散射变换（GST），这是一个高效的数学框架，对特征或图拓扑扰动具有可证明的稳定性，并且在图分类性能方面与GNNs相当。我们的主要贡献是基于GSTs的第一个已知的非线性近似图遗忘方法。第二个贡献是对所提出的遗忘方法的计算复杂性进行理论分析。

    As the demand for user privacy grows, controlled data removal (machine unlearning) is becoming an important feature of machine learning models for data-sensitive Web applications such as social networks and recommender systems. Nevertheless, at this point it is still largely unknown how to perform efficient machine unlearning of graph neural networks (GNNs); this is especially the case when the number of training samples is small, in which case unlearning can seriously compromise the performance of the model. To address this issue, we initiate the study of unlearning the Graph Scattering Transform (GST), a mathematical framework that is efficient, provably stable under feature or graph topology perturbations, and offers graph classification performance comparable to that of GNNs. Our main contribution is the first known nonlinear approximate graph unlearning method based on GSTs. Our second contribution is a theoretical analysis of the computational complexity of the proposed unlearnin
    
[^166]: 对于12导联心电图分类的深度学习模型分析揭示了类似于诊断标准的学习特征

    Analysis of a Deep Learning Model for 12-Lead ECG Classification Reveals Learned Features Similar to Diagnostic Criteria. (arXiv:2211.01738v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2211.01738](http://arxiv.org/abs/2211.01738)

    本研究通过应用归因方法对12导联心电图分类的预训练深度神经网络进行分析，揭示了与诊断标准类似的学习特征。相关性分数的分析显示，随着分类概率的增加，平均值也增加，并且在接近零时产生误分类。

    

    尽管深度神经网络具有出色的性能，但它们在临床实践中仍未得到采用，部分原因是它们缺乏可解释性。本研究将归因方法应用于预训练的12导联心电图分类深度神经网络（DNN），以打开这个“黑盒子”，理解模型预测和学习特征之间的关系。我们对来自公共数据集的数据进行分类，并且归因方法为每个分类信号样本分配“相关性分数”。这样可以分析网络在训练过程中学到了什么，我们提出了以下定量方法：a）类别的平均相关性分数，b）导联的平均相关性分数，c）平均节拍的平均相关性分数。对房颤（AF）和左束支传导阻滞（LBBB）的相关性分数进行分析与健康对照组相比，结果表明a）随着分类概率的增加，平均值增加，并且接近零时对应于误分类，b）cor

    Despite their remarkable performance, deep neural networks remain unadopted in clinical practice, which is considered to be partially due to their lack in explainability. In this work, we apply attribution methods to a pre-trained deep neural network (DNN) for 12-lead electrocardiography classification to open this "black box" and understand the relationship between model prediction and learned features. We classify data from a public data set and the attribution methods assign a "relevance score" to each sample of the classified signals. This allows analyzing what the network learned during training, for which we propose quantitative methods: average relevance scores over a) classes, b) leads, and c) average beats. The analyses of relevance scores for atrial fibrillation (AF) and left bundle branch block (LBBB) compared to healthy controls show that their mean values a) increase with higher classification probability and correspond to false classifications when around zero, and b) cor
    
[^167]: CausalBench：基于单细胞干扰数据的网络推断大规模基准测试

    CausalBench: A Large-scale Benchmark for Network Inference from Single-cell Perturbation Data. (arXiv:2210.17283v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.17283](http://arxiv.org/abs/2210.17283)

    CausalBench是一个用于评估基于单细胞干扰实验的真实世界干预数据上的网络推断方法的基准测试套件。通过系统评估，发现当前的方法可扩展性差，限制了性能。使用干预信息的方法并没有超越那些不使用干预信息的方法。

    

    因果推断是多个科学学科的重要方面，并经常应用于医学等高影响应用中。然而，在真实世界环境中评估因果推断方法的性能具有挑战性，因为需要在干预和对照条件下观察。在合成数据集上进行的传统评估不能反映真实世界系统中的性能。为了解决这个问题，我们引入了CausalBench，这是一个基准测试套件，用于评估基于大规模单细胞干扰实验的真实世界干预数据上的网络推断方法。CausalBench包括生物学动机的性能度量，包括新的基于分布的干预度量。使用我们的CausalBench套件对最先进的因果推断方法进行系统评估，突出了当前方法的可扩展性差，从而限制了性能。此外，使用干预信息的方法并没有超越那些不使用干预信息的方法

    Causal inference is a vital aspect of multiple scientific disciplines and is routinely applied to high-impact applications such as medicine. However, evaluating the performance of causal inference methods in real-world environments is challenging due to the need for observations under both interventional and control conditions. Traditional evaluations conducted on synthetic datasets do not reflect the performance in real-world systems. To address this, we introduce CausalBench, a benchmark suite for evaluating network inference methods on real-world interventional data from large-scale single-cell perturbation experiments. CausalBench incorporates biologically-motivated performance metrics, including new distribution-based interventional metrics. A systematic evaluation of state-of-the-art causal inference methods using our CausalBench suite highlights how poor scalability of current methods limits performance. Moreover, methods that use interventional information do not outperform tho
    
[^168]: 联邦聚类的机器未学习

    Machine Unlearning of Federated Clusters. (arXiv:2210.16424v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.16424](http://arxiv.org/abs/2210.16424)

    本论文介绍了联邦聚类的机器未学习问题，并提出了一个定制的安全联邦聚类框架的高效未学习机制。同时，为了保护客户数据隐私，作者还开发了安全压缩多集合聚合框架。所提出的方法性能优良，适用于解决稀疏安全联邦学习问题和其他一般问题。

    

    联邦聚类 (FC) 是一个出现在许多实际应用中的无监督学习问题，包括个性化推荐和医疗系统。随着最近确保“被遗忘权”的法律的采纳，机器未学习问题对于FC方法变得非常重要。我们首次介绍了FC的机器未学习问题，并提出了一个定制的安全FC框架的高效未学习机制。我们的FC框架利用了特殊的初始化过程，我们证明对于未学习非常适用。为了保护客户数据隐私，我们开发了安全压缩多集合聚合 (SCMA) 框架，解决了聚类中遇到的稀疏安全联邦学习 (FL) 问题以及更一般的问题。为了同时促进低通信复杂性和秘密共享协议，我们将Reed-Solomon编码与特殊评估点集成到我们的SCMA流水线中。

    Federated clustering (FC) is an unsupervised learning problem that arises in a number of practical applications, including personalized recommender and healthcare systems. With the adoption of recent laws ensuring the "right to be forgotten", the problem of machine unlearning for FC methods has become of significant importance. We introduce, for the first time, the problem of machine unlearning for FC, and propose an efficient unlearning mechanism for a customized secure FC framework. Our FC framework utilizes special initialization procedures that we show are well-suited for unlearning. To protect client data privacy, we develop the secure compressed multiset aggregation (SCMA) framework that addresses sparse secure federated learning (FL) problems encountered during clustering as well as more general problems. To simultaneously facilitate low communication complexity and secret sharing protocols, we integrate Reed-Solomon encoding with special evaluation points into our SCMA pipeline
    
[^169]: 强化学习的因果解释：量化状态和时间重要性

    Causal Explanation for Reinforcement Learning: Quantifying State and Temporal Importance. (arXiv:2210.13507v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.13507](http://arxiv.org/abs/2210.13507)

    本文开发了一种因果解释机制，能够量化状态对行动的因果重要性和随时间变化的重要性，并通过一系列的模拟研究证明了该机制在强化学习策略解释方面的优势。

    

    解释性在机器学习中起着越来越重要的作用。此外，人们通过因果镜头来看待世界，因此更倾向于因果解释而非关联解释。因此，在本文中，我们开发了一种因果解释机制，以量化状态对行动的因果重要性和随时间变化的重要性。我们还通过一系列的模拟研究，包括农田灌溉、21点、避碰和月球着陆器等，展示了我们机制在强化学习策略解释方面相对于最先进的关联方法的优势。

    Explainability plays an increasingly important role in machine learning. Furthermore, humans view the world through a causal lens and thus prefer causal explanations over associational ones. Therefore, in this paper, we develop a causal explanation mechanism that quantifies the causal importance of states on actions and such importance over time. We also demonstrate the advantages of our mechanism over state-of-the-art associational methods in terms of RL policy explanation through a series of simulation studies, including crop irrigation, Blackjack, collision avoidance, and lunar lander.
    
[^170]: RSC: 通过随机稀疏计算加速图神经网络训练

    RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations. (arXiv:2210.10737v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10737](http://arxiv.org/abs/2210.10737)

    通过随机稀疏计算，本研究提出了一种加速图神经网络训练的方法，解决了稀疏图操作难以加速和不规则数据格式导致的效率问题。

    

    图神经网络（GNN）的训练非常耗时，因为硬件难以加速稀疏图操作。先前的研究通过基于采样的逼近来降低时间复杂度，但牺牲了计算精度。在这个思路的基础上，以往的工作成功地加速了基于稠密矩阵的操作（如卷积和线性操作），准确度下降可以忽略不计。然而，与稠密矩阵不同，稀疏矩阵以不规则的数据格式存储，每行/列可能有不同数量的非零元素。因此，与稠密矩阵相比，逼近稀疏操作存在两个独特的挑战：（1）我们无法直接控制逼近稀疏操作的效率，因为计算仅在非零元素上执行；（2）基于子采样的稀疏矩阵处理效率更低，因为存在不规则数据格式。为了解决这些问题，我们的关键思想是控制准确度和效率的权衡。

    The training of graph neural networks (GNNs) is extremely time consuming because sparse graph-based operations are hard to be accelerated by hardware. Prior art explores trading off the computational precision to reduce the time complexity via sampling-based approximation. Based on the idea, previous works successfully accelerate the dense matrix based operations (e.g., convolution and linear) with negligible accuracy drop. However, unlike dense matrices, sparse matrices are stored in the irregular data format such that each row/column may have different number of non-zero entries. Thus, compared to the dense counterpart, approximating sparse operations has two unique challenges (1) we cannot directly control the efficiency of approximated sparse operation since the computation is only executed on non-zero entries; (2) sub-sampling sparse matrices is much more inefficient due to the irregular data format. To address the issues, our key idea is to control the accuracy-efficiency trade o
    
[^171]: CAB: 长序列建模的全面注意力基准测试

    CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling. (arXiv:2210.07661v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.07661](http://arxiv.org/abs/2210.07661)

    本文提出了一种全面的注意力基准测试（CAB），用于评估在建模长序列时的高效注意力方法。CAB包括了细粒度的注意力分类体系，涵盖了非因果自注意力、因果自注意力、非因果交叉注意力和因果交叉注意力四种注意力模式，并采集了七个真实世界任务进行评估。

    

    Transformer在语言、图像和语音处理方面取得了显著的成就。最近，人们提出了各种高效的注意力架构，以提高Transformer在建模长序列时的效率，同时大幅保留其表现力。一个广泛使用的用于测试这些高效方法在长序列建模能力上的基准是长距离竞技场（LRA）。然而，LRA只关注标准的双向（或非因果）自注意力，完全忽略了交叉注意力和单向（或因果）注意力，而这对于下游应用同样重要。在本文中，我们提出了一种全面的注意力基准测试（CAB），采用了细粒度的注意力分类体系，包括非因果自注意力、因果自注意力、非因果交叉注意力和因果交叉注意力四种可区分的注意力模式。CAB收集了来自不同研究领域的七个真实世界任务，以评估在四种注意力模式下的高效注意力。

    Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tas
    
[^172]: 嵌入作为认识状态：关于用于获得知识的汇聚运算符的限制

    Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge. (arXiv:2210.05723v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.05723](http://arxiv.org/abs/2210.05723)

    本文研究了用于汇聚信息的各种汇聚运算符在嵌入编码中的应用。我们发现所有考虑的汇聚运算符都可以满足认识汇聚原则，但这仅在嵌入具有足够高维度并满足特定约束的情况下成立。这些约束对嵌入在实践中的使用具有重要影响。

    

    各种神经网络架构依靠汇聚运算符来聚合来自不同来源的信息。在这种情况下，通常默认向量编码为认识状态，即向量捕捉到已获取有关某些感兴趣属性的证据，并且汇聚这些向量会得到一个结合这些证据的向量。我们研究了一系列标准汇聚运算符，在什么条件下它们与这个被称为认识汇聚原则的想法兼容。我们发现所有考虑的汇聚运算符都可以满足认识汇聚原则，但这仅在嵌入具有足够高维度的情况下成立，对于大多数汇聚运算符，嵌入需要满足特定约束（例如具有非负坐标）。此外，我们还展示了这些约束对嵌入在实践中的使用具有重要影响。特别是，我们发现当认识汇聚原则不能被满足时，使用一个无约束的稀疏解更好。

    Various neural network architectures rely on pooling operators to aggregate information coming from different sources. It is often implicitly assumed in such contexts that vectors encode epistemic states, i.e. that vectors capture the evidence that has been obtained about some properties of interest, and that pooling these vectors yields a vector that combines this evidence. We study, for a number of standard pooling operators, under what conditions they are compatible with this idea, which we call the epistemic pooling principle. While we find that all the considered pooling operators can satisfy the epistemic pooling principle, this only holds when embeddings are sufficiently high-dimensional and, for most pooling operators, when the embeddings satisfy particular constraints (e.g. having non-negative coordinates). We furthermore show that these constraints have important implications on how the embeddings can be used in practice. In particular, we find that when the epistemic pooling
    
[^173]: 神经扩展卡尔曼滤波器用于学习和预测结构系统的动力学

    Neural Extended Kalman Filters for Learning and Predicting Dynamics of Structural Systems. (arXiv:2210.04165v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.04165](http://arxiv.org/abs/2210.04165)

    本论文提出了一种称为神经扩展卡尔曼滤波器（Neural EKF）的可学习卡尔曼滤波方法，用于学习复杂物理系统的潜在演化动力学。这种方法可以通过端到端训练来学习过程动力学和传感观测的建模，提高结构响应预测的准确性。

    

    准确的结构响应预测是结构健康监测和控制应用的主要驱动力。这往往需要所提出的模型充分捕捉复杂结构系统的基本动力学。在这项工作中，我们利用可学习的扩展卡尔曼滤波器（称为神经扩展卡尔曼滤波器）来学习复杂物理系统的潜在演化动力学。神经扩展卡尔曼滤波器是传统卡尔曼滤波器的广义版本，其中过程动力学和传感观测的建模可以通过神经网络来参数化，因此可以通过端到端训练来学习。该方法在变分推理框架下实现，卡尔曼滤波器通过感知测量进行推理。通常，传统的变分推理模型的参数是独立于潜在动力学模型的神经网络参数化的。这种特点使得推理和重构的准确性相对较弱。

    Accurate structural response prediction forms a main driver for structural health monitoring and control applications. This often requires the proposed model to adequately capture the underlying dynamics of complex structural systems. In this work, we utilize a learnable Extended Kalman Filter (EKF), named the Neural Extended Kalman Filter (Neural EKF) throughout this paper, for learning the latent evolution dynamics of complex physical systems. The Neural EKF is a generalized version of the conventional EKF, where the modeling of process dynamics and sensory observations can be parameterized by neural networks, therefore learned by end-to-end training. The method is implemented under the variational inference framework with the EKF conducting inference from sensing measurements. Typically, conventional variational inference models are parameterized by neural networks independent of the latent dynamics models. This characteristic makes the inference and reconstruction accuracy weakly b
    
[^174]: The Vendi分数: 一种用于机器学习的多样性评估指标

    The Vendi Score: A Diversity Evaluation Metric for Machine Learning. (arXiv:2210.02410v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02410](http://arxiv.org/abs/2210.02410)

    本文提出了一种用于机器学习的多样性评估指标Vendi分数，它能够灵活地衡量不同形式的多样性，而且不需要参考数据集，适用于任何生成模型和数据集。

    

    多样性是机器学习（ML）中许多领域的重要标准，包括生成建模和数据集策划。然而，现有的衡量多样性的指标往往是针对特定领域的，并且灵活性有限。本文通过提出Vendi分数来解决多样性评估问题，该指标将生态学和量子统计力学的思想与ML相结合并进行扩展。Vendi分数定义为相似性矩阵的特征值的香农熵的指数函数。这个矩阵是由用户定义的相似性函数应用于要评估多样性的样本而诱导出的。通过使用相似性函数作为输入，Vendi分数使用户能够指定任何所需的多样性形式。与ML中的许多现有指标不同，Vendi分数不需要参考数据集或样本或标签的分布，因此它通用且适用于任何生成模型、解码算法和来自任何领域的数据集。

    Diversity is an important criterion for many areas of machine learning (ML), including generative modeling and dataset curation. However, existing metrics for measuring diversity are often domain-specific and limited in flexibility. In this paper, we address the diversity evaluation problem by proposing the Vendi Score, which connects and extends ideas from ecology and quantum statistical mechanics to ML. The Vendi Score is defined as the exponential of the Shannon entropy of the eigenvalues of a similarity matrix. This matrix is induced by a user-defined similarity function applied to the sample to be evaluated for diversity. In taking a similarity function as input, the Vendi Score enables its user to specify any desired form of diversity. Importantly, unlike many existing metrics in ML, the Vendi Score does not require a reference dataset or distribution over samples or labels, it is therefore general and applicable to any generative model, decoding algorithm, and dataset from any d
    
[^175]: 使用神经网络学习信号时态逻辑进行可解释分类

    Learning Signal Temporal Logic through Neural Network for Interpretable Classification. (arXiv:2210.01910v2 [cs.FL] UPDATED)

    [http://arxiv.org/abs/2210.01910](http://arxiv.org/abs/2210.01910)

    本文提出了一个可解释的神经符号化框架，用于时间序列行为的分类。通过使用信号时态逻辑（STL）约束神经网络计算图的搜索，并设计新的时间函数和稀疏softmax函数，能够高效地学习一个紧凑的STL公式，实现时间序列数据的分类。通过与最先进的基线方法进行比较，在驾驶场景和海军监视案例研究中展示了该方法的计算效率、紧凑性和可解释性。

    

    使用神经网络的机器学习技术已经在时间序列数据分类方面取得了良好的成果。然而，它们产生的模型很难进行验证和解释。本文提出了一个可解释的神经符号化框架，用于时间序列行为的分类。我们特别使用一种表达能力强的形式语言，即信号时态逻辑（Signal Temporal Logic，STL），来约束神经网络计算图的搜索。我们设计了一种新的时间函数和稀疏softmax函数，以提高神经-STL框架的准确性和精度。因此，我们可以通过现成的基于梯度的工具高效地学习一个紧凑的STL公式，用于时间序列数据的分类。通过驾驶场景和海军监视案例研究与最先进的基线方法进行比较，我们展示了所提出方法的计算效率、紧凑性和可解释性。

    Machine learning techniques using neural networks have achieved promising success for time-series data classification. However, the models that they produce are challenging to verify and interpret. In this paper, we propose an explainable neural-symbolic framework for the classification of time-series behaviors. In particular, we use an expressive formal language, namely Signal Temporal Logic (STL), to constrain the search of the computation graph for a neural network. We design a novel time function and sparse softmax function to improve the soundness and precision of the neural-STL framework. As a result, we can efficiently learn a compact STL formula for the classification of time-series data through off-the-shelf gradient-based tools. We demonstrate the computational efficiency, compactness, and interpretability of the proposed method through driving scenarios and naval surveillance case studies, compared with state-of-the-art baselines.
    
[^176]: 垂直半联合学习用于高效在线广告

    Vertical Semi-Federated Learning for Efficient Online Advertising. (arXiv:2209.15635v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15635](http://arxiv.org/abs/2209.15635)

    垂直半联合学习为在线广告领域提供了高效的解决方案，通过学习一个联合感知的局部模型以应对传统垂直联合学习的限制。

    

    传统的垂直联合学习架构存在两个主要问题：1）适用范围受限于重叠样本；2）实时联合服务的系统挑战较高，这限制了其在广告系统中的应用。为解决这些问题，我们提出了一种新的学习设置——半垂直联合学习(Semi-VFL)，以应对这些挑战。半垂直联合学习旨在实现垂直联合学习的实际工业应用方式，通过学习一个联合感知的局部模型，该模型表现优于单方模型，同时保持了局部服务的便利性。为此，我们提出了精心设计的联合特权学习框架(JPL)，来解决被动方特征缺失和适应整个样本空间这两个问题。具体而言，我们构建了一个推理高效的适用于整个样本空间的单方学生模型，同时保持了联合特征扩展的优势。新的表示蒸馏

    The traditional vertical federated learning schema suffers from two main issues: 1) restricted applicable scope to overlapped samples and 2) high system challenge of real-time federated serving, which limits its application to advertising systems. To this end, we advocate a new learning setting Semi-VFL (Vertical Semi-Federated Learning) to tackle these challenge. Semi-VFL is proposed to achieve a practical industry application fashion for VFL, by learning a federation-aware local model which performs better than single-party models and meanwhile maintain the convenience of local-serving. For this purpose, we propose the carefully designed Joint Privileged Learning framework (JPL) to i) alleviate the absence of the passive party's feature and ii) adapt to the whole sample space. Specifically, we build an inference-efficient single-party student model applicable to the whole sample space and meanwhile maintain the advantage of the federated feature extension. New representation distilla
    
[^177]: FAIR-FATE: 具有动量的公平联邦学习

    FAIR-FATE: Fair Federated Learning with Momentum. (arXiv:2209.13678v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.13678](http://arxiv.org/abs/2209.13678)

    FAIR-FATE是一种公平联邦学习算法，通过公平感知的聚合方法实现组公平性并保持高效用性。

    

    虽然公平感知的机器学习算法受到了越来越多的关注，但重点一直是集中式机器学习，对分散式方法的研究还不够。联邦学习是一种分散式的机器学习形式，其中客户端训练本地模型，服务器汇总它们以获得共享的全局模型。客户端之间的数据异质性是联邦学习的一个常见特征，这可能导致或加剧由敏感属性（如种族或性别）定义的特权组的歧视。在这项工作中，我们提出了FAIR-FATE：一种新颖的公平联邦学习算法，旨在通过一种公平感知的聚合方法计算全局模型，从而实现组公平性同时保持高效用性。为实现这一目标，全局模型更新采用一个动量项来估计公平模型更新，帮助克服非公平梯度的震荡。

    While fairness-aware machine learning algorithms have been receiving increasing attention, the focus has been on centralized machine learning, leaving decentralized methods underexplored. Federated Learning is a decentralized form of machine learning where clients train local models with a server aggregating them to obtain a shared global model. Data heterogeneity amongst clients is a common characteristic of Federated Learning, which may induce or exacerbate discrimination of unprivileged groups defined by sensitive attributes such as race or gender. In this work we propose FAIR-FATE: a novel FAIR FederATEd Learning algorithm that aims to achieve group fairness while maintaining high utility via a fairness-aware aggregation method that computes the global model by taking into account the fairness of the clients. To achieve that, the global model update is computed by estimating a fair model update using a Momentum term that helps to overcome the oscillations of non-fair gradients. To 
    
[^178]: 学习什么时候为人类决策者提供建议

    Learning When to Advise Human Decision Makers. (arXiv:2209.13578v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.13578](http://arxiv.org/abs/2209.13578)

    本文提出了一种新颖的人工智能系统设计，其中算法与人类用户以双向互动的方式交互，仅在对用户的决策有益时提供建议。实验证明，这种方法能够改善人类的决策能力，并在促进人类学习、保留人类决策的补充优势方面具有额外的优势。

    

    人工智能系统越来越多地被用于提供建议，以促进在医疗、刑事司法和金融等各个领域的人类决策。我们提出一个新颖的人工智能系统设计，其中算法与人类用户以双向互动的方式交互，旨在仅在对用户的决策有益时提供建议。一项大规模实验的结果表明，与固定的非交互式建议方法相比，我们的建议方法能够在需要的时候提供建议，并显著改善人类的决策能力。这种方法在促进人类学习、保留人类决策的补充优势方面还具有额外的优势。

    Artificial intelligence (AI) systems are increasingly used for providing advice to facilitate human decision making in a wide range of domains, such as healthcare, criminal justice, and finance. Motivated by limitations of the current practice where algorithmic advice is provided to human users as a constant element in the decision-making pipeline, in this paper we raise the question of when should algorithms provide advice? We propose a novel design of AI systems in which the algorithm interacts with the human user in a two-sided manner and aims to provide advice only when it is likely to be beneficial for the user in making their decision. The results of a large-scale experiment show that our advising approach manages to provide advice at times of need and to significantly improve human decision making compared to fixed, non-interactive, advising approaches. This approach has additional advantages in facilitating human learning, preserving complementary strengths of human decision ma
    
[^179]: 使用卷积神经网络（CNN）减少JPEG压缩中由离散傅里叶变换（DFT）引起的构造性损失

    Convolutional Neural Network (CNN) to reduce construction loss in JPEG compression caused by Discrete Fourier Transform (DFT). (arXiv:2209.03475v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.03475](http://arxiv.org/abs/2209.03475)

    该论文介绍了使用卷积神经网络（CNN）来减少JPEG压缩中由离散傅里叶变换（DFT）引起的构造性损失。通过在特征提取中使用卷积，可以得到更少冗余的特征图和更小的数据集，从而提高图像压缩的质量。

    

    近几十年来，数字图像处理取得了巨大的发展。因此，提出了许多数据压缩策略，目标是最小化表示图像所需的信息量。其中，JPEG压缩是最流行的方法之一，在多媒体和数字应用中得到了广泛应用。DFT的周期性特性使得很难满足图像相对边界的周期条件，从而产生严重的伪影，降低了图像的感知视觉质量。另一方面，深度学习最近在语音识别、图像降噪和自然语言处理等应用方面取得了卓越的结果。卷积神经网络（CNN）比大多数其他类型的深度神经网络更受关注。在特征提取中使用卷积可以得到更少冗余的特征图和更小的数据集，这对图像压缩至关重要。

    In recent decades, digital image processing has gained enormous popularity. Consequently, a number of data compression strategies have been put forth, with the goal of minimizing the amount of information required to represent images. Among them, JPEG compression is one of the most popular methods that has been widely applied in multimedia and digital applications. The periodic nature of DFT makes it impossible to meet the periodic condition of an image's opposing edges without producing severe artifacts, which lowers the image's perceptual visual quality. On the other hand, deep learning has recently achieved outstanding results for applications like speech recognition, image reduction, and natural language processing. Convolutional Neural Networks (CNN) have received more attention than most other types of deep neural networks. The use of convolution in feature extraction results in a less redundant feature map and a smaller dataset, both of which are crucial for image compression. I
    
[^180]: 面向收益限制广告商的在线竞价算法

    Online Bidding Algorithms for Return-on-Spend Constrained Advertisers. (arXiv:2208.13713v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.13713](http://arxiv.org/abs/2208.13713)

    本研究提出了一个用于满足投放成本回报率限制的广告商的在线竞价算法，通过简便的在线算法实现了接近最优的遗憾值，并且总结了与先前工作的集成性能。

    

    在线广告业近年来成长为一个竞争激烈且复杂的数十亿美元行业，广告商在大规模和高频率下进行广告位竞价。这导致需求增加了对于高效的"自动竞价"算法，以确定最大化广告商目标的投标价格，同时满足特定的限制条件。本研究探讨了单个最大化价值广告商面临的越来越流行的限制条件之一：投放成本回报率（RoS）。我们以相对于知道所有查询的最优算法的遗憾值为衡量标准来量化效率。我们提出了一个简单的在线算法，当查询序列是来自某个分布的独立同分布样本时，该算法在期望值上实现了接近最优的遗憾值，同时始终遵守指定的RoS约束。我们还将我们的结果与Balseiro、Lu和Mirrokni [BLM20]的先前工作相结合，以在尊重约束的同时实现接近最优的遗憾值。

    Online advertising has recently grown into a highly competitive and complex multi-billion-dollar industry, with advertisers bidding for ad slots at large scales and high frequencies. This has resulted in a growing need for efficient "auto-bidding" algorithms that determine the bids for incoming queries to maximize advertisers' targets subject to their specified constraints. This work explores efficient online algorithms for a single value-maximizing advertiser under an increasingly popular constraint: Return-on-Spend (RoS). We quantify efficiency in terms of regret relative to the optimal algorithm, which knows all queries a priori.  We contribute a simple online algorithm that achieves near-optimal regret in expectation while always respecting the specified RoS constraint when the input sequence of queries are i.i.d. samples from some distribution. We also integrate our results with the previous work of Balseiro, Lu, and Mirrokni [BLM20] to achieve near-optimal regret while respecting
    
[^181]: 超图SBM中的社区发现：给定相似性矩阵的最优恢复

    Community Detection in the Hypergraph SBM: Optimal Recovery Given the Similarity Matrix. (arXiv:2208.12227v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2208.12227](http://arxiv.org/abs/2208.12227)

    本文研究了超图随机块模型中的社区发现问题，通过分析相似性矩阵和设计谱算法，实现了在不同密度条件下的精确恢复和高效计算。

    

    社区发现是网络科学中的一个基础问题。本文考虑了超图随机块模型（HSBM）中的社区发现问题，重点是确切的社区恢复。我们研究了基于相似性矩阵W的多项式时间算法的性能，其中W_{ij}表示同时包含i和j的超边的数量。在这个信息模型下，Kim，Bandeira和Goemans确定了在对数度数区间内的精确恢复的信息论阈值，他们提出了一个半定规划松弛，认为这是最优的猜想。在本文中，我们证实了这个猜想。我们还设计了一个简单且高效的谱算法，几乎具有线性运行时间，并且证明它达到了信息论阈值。此外，这个谱算法还能在更密集的情况下成功，并且比之前的方法更高效，因此被认为是一种有效的方法。

    Community detection is a fundamental problem in network science. In this paper, we consider community detection in hypergraphs drawn from the $hypergraph$ $stochastic$ $block$ $model$ (HSBM), with a focus on exact community recovery. We study the performance of polynomial-time algorithms which operate on the $similarity$ $matrix$ $W$, where $W_{ij}$ reports the number of hyperedges containing both $i$ and $j$. Under this information model, Kim, Bandeira, and Goemans determined the information-theoretic threshold for exact recovery in the logarithmic degree regime, and proposed a semidefinite programming relaxation which they conjectured to be optimal. In this paper, we confirm this conjecture. We also design a simple and highly efficient spectral algorithm with nearly linear runtime and show that it achieves the information-theoretic threshold. Moreover, the spectral algorithm also succeeds in denser regimes and is considerably more efficient than previous approaches, establishing it a
    
[^182]: 在临床存在下的填补策略：对算法公平性的影响

    Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness. (arXiv:2208.06648v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06648](http://arxiv.org/abs/2208.06648)

    本文研究了填补选择对不同群体的重建误差和下游预测的算法公平性属性的影响。

    

    机器学习可能会强化数据中的偏见，而我们在这个工作中提出，数据中缺失的内容也会产生偏见。在医疗领域，偏见已经在医疗历史上留下了深深的烙印，导致边缘化群体受到不平等的护理。缺失数据中的模式通常反映了这些群体的差异，但是特定群体缺失的算法公平性影响还不太清楚。尽管其潜在影响巨大，但填补往往被忽视为一个预处理步骤，而关注点放在了重建误差的减少和整体性能上，忽略了填补如何对不同群体产生影响。我们的工作研究了填补选择对不同群体的重建误差和下游预测的算法公平性属性的影响。

    Machine learning risks reinforcing biases present in data, and, as we argue in this work, in what is absent from data. In healthcare, biases have marked medical history, leading to unequal care affecting marginalised groups. Patterns in missing data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is often an overlooked preprocessing step, with attention placed on the reduction of reconstruction error and overall performance, ignoring how imputation can affect groups differently. Our work studies how imputation choices affect reconstruction errors across groups and algorithmic fairness properties of downstream predictions.
    
[^183]: 在R中使用theft包进行基于特征的时间序列分析

    Feature-Based Time-Series Analysis in R using the theft Package. (arXiv:2208.06146v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.06146](http://arxiv.org/abs/2208.06146)

    本研究介绍了在R中使用theft包进行基于特征的时间序列分析的方法，并指出了当前存在的问题包括缺乏统一的访问点以及用户需要掌握多种编程语言来获得所有特征集。

    

    时间序列在各个科学领域中被测量和分析。一种量化时间序列结构的方法是通过计算一组摘要统计量或"特征"，然后用特征向量的属性来表示时间序列。结果得到的特征空间是可解释和信息丰富的，使得传统的统计学习方法，包括聚类、回归和分类，可以应用于时间序列数据集。存在许多开源软件包在多种编程语言中计算时间序列特征集，包括catch22（22个特征：Matlab、R、Python、Julia）、feasts（42个特征：R）、tsfeatures（63个特征：R）、Kats（40个特征：Python）、tsfresh（779个特征：Python）和TSFEL（390个特征：Python）。然而，存在几个问题：（i）目前尚无这些软件包的单一访问点；（ii）要访问所有特征集，用户必须精通多种语言；（iii）th

    Time series are measured and analyzed across the sciences. One method of quantifying the structure of time series is by calculating a set of summary statistics or `features', and then representing a time series in terms of its properties as a feature vector. The resulting feature space is interpretable and informative, and enables conventional statistical learning approaches, including clustering, regression, and classification, to be applied to time-series datasets. Many open-source software packages for computing sets of time-series features exist across multiple programming languages, including catch22 (22 features: Matlab, R, Python, Julia), feasts (42 features: R), tsfeatures (63 features: R), Kats (40 features: Python), tsfresh (779 features: Python), and TSFEL (390 features: Python). However, there are several issues: (i) a singular access point to these packages is not currently available; (ii) to access all feature sets, users must be fluent in multiple languages; and (iii) th
    
[^184]: 基于压力分布分析的婴儿运动分类——研究和临床应用的附加价值

    Infant movement classification through pressure distribution analysis -- added value for research and clinical implementation. (arXiv:2208.00884v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2208.00884](http://arxiv.org/abs/2208.00884)

    本文提出了一种创新型的非侵入式方法，使用压力传感器对婴儿的一般运动进行分类，旨在实现早期神经肌肉障碍（如脑瘫）的客观检测。研究结果表明，使用压力数据可以区分婴儿的典型运动模式，即“坐立不安期”和“坐立不安前期”。

    

    本研究旨在通过使用压力传感设备来对婴儿的一般运动进行分类，从而实现早期的神经肌肉障碍（如脑瘫）的客观检测。本文测试了使用压力数据来区分“坐立不安期”（即坐立不安运动）与“坐立不安前期”（即扭动运动）的典型运动模式的可行性。在此过程中，我们记录了每个婴儿在出生后 4-16 周的间隔期内连续七个实验室会话的多模态传感器数据，包括来自一个 32x32 网格压力传感垫及其 1024 个传感器的压力数据。为了验证概念，从两个目标年龄段中，每个持续 5 秒的 1776 个压力数据片段被用于运动分类。每个片段都是根据相应的同步视频数据由人工评估员进行预注释的，标记为坐立不安存在或不存在。

    Aiming at objective early detection of neuromotor disorders such as cerebral palsy, we proposed an innovative non-intrusive approach using a pressure sensing device to classify infant general movements (GMs). Here, we tested the feasibility of using pressure data to differentiate typical GM patterns of the ''fidgety period'' (i.e., fidgety movements) vs. the ''pre-fidgety period'' (i.e., writhing movements). Participants (N = 45) were sampled from a typically-developing infant cohort. Multi-modal sensor data, including pressure data from a 32x32-grid pressure sensing mat with 1024 sensors, were prospectively recorded for each infant in seven succeeding laboratory sessions in biweekly intervals from 4-16 weeks of post-term age. For proof-of-concept, 1776 pressure data snippets, each 5s long, from the two targeted age periods were taken for movement classification. Each snippet was pre-annotated based on corresponding synchronised video data by human assessors as either fidgety present (
    
[^185]: DRSOM: 一种降维的二阶优化方法

    DRSOM: A Dimension Reduced Second-Order Method. (arXiv:2208.00208v3 [math.OC] UPDATED)

    [http://arxiv.org/abs/2208.00208](http://arxiv.org/abs/2208.00208)

    本文提出了一种降维的二阶优化方法（DRSOM），通过在少数方向上利用曲率信息来保持二阶方法的收敛性，并将计算开销控制在与一阶方法相当的程度。理论上证明了该方法在满足近似黑塞矩阵假设的条件下具有局部二次收敛和全局收敛速度为 $O(\epsilon^{-3/2})$。通过各种计算实验展示了该方法的适用性和性能。

    

    本文提出了一种用于凸和非凸（无约束）优化的降维二阶方法（DRSOM）。在类似信赖域的框架下，我们的方法在只使用少数方向的曲率信息的同时保持了二阶方法的收敛性。因此，我们的方法的计算开销仍然与一阶方法（如梯度下降法）相当。在理论上，我们证明了该方法具有局部二次收敛性和全局收敛速度为 $O(\epsilon^{-3/2})$，以满足一阶和二阶条件，如果子空间满足常用的近似黑塞矩阵的假设。我们进一步展示了，如果在算法的最后阶段定期使用类似Krylov方法的纠正步骤，那么可以消除该假设。通过各种计算实验展示了DRSOM的适用性和性能，包括$L_2 - L_p$最小化、CUTEst问题和传感器网络。

    In this paper, we propose a Dimension-Reduced Second-Order Method (DRSOM) for convex and nonconvex (unconstrained) optimization. Under a trust-region-like framework, our method preserves the convergence of the second-order method while using only curvature information in a few directions. Consequently, the computational overhead of our method remains comparable to the first-order such as the gradient descent method. Theoretically, we show that the method has a local quadratic convergence and a global convergence rate of $O(\epsilon^{-3/2})$ to satisfy the first-order and second-order conditions if the subspace satisfies a commonly adopted approximated Hessian assumption. We further show that this assumption can be removed if we perform a corrector step using a Krylov-like method periodically at the end stage of the algorithm. The applicability and performance of DRSOM are exhibited by various computational experiments, including $L_2 - L_p$ minimization, CUTEst problems, and sensor net
    
[^186]: MABe22：一种用于学习行为表示的多物种多任务基准

    MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior. (arXiv:2207.10553v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.10553](http://arxiv.org/abs/2207.10553)

    MABe22是一个多物种多任务基准，用于评估学习行为表示的质量。它采集了来自各种生物学实验的数据，测试了多种自监督学习方法，并发现人类行动数据集上的方法不能完全适用于动物数据集。

    

    我们引入了MABe22，一个大规模的多智能体视频和轨迹基准，用于评估学习行为表示的质量。该数据集采集自各种生物学实验，包括三个相互作用的小鼠三元组（470万帧的视频+姿态跟踪数据，1000万帧的仅姿态数据），共生甲虫-蚂蚁相互作用（1000万帧的视频数据）和一群互动的苍蝇（440万帧的姿态跟踪数据）。除了这些数据，我们还引入了一系列真实生活中的下游分析任务，以评估学习表示的质量，通过评估它们在保留关于实验条件（例如品系、时间、光遗传刺激）和动物行为信息方面的能力。我们测试了多种最先进的自监督视频和轨迹表示学习方法，以展示我们基准的用途，并揭示了使用人类行动数据集开发的方法不能完全适用于动物数据集。

    We introduce MABe22, a large-scale, multi-agent video and trajectory benchmark to assess the quality of learned behavior representations. This dataset is collected from a variety of biology experiments, and includes triplets of interacting mice (4.7 million frames video+pose tracking data, 10 million frames pose only), symbiotic beetle-ant interactions (10 million frames video data), and groups of interacting flies (4.4 million frames of pose tracking data). Accompanying these data, we introduce a panel of real-life downstream analysis tasks to assess the quality of learned representations by evaluating how well they preserve information about the experimental conditions (e.g. strain, time of day, optogenetic stimulation) and animal behavior. We test multiple state-of-the-art self-supervised video and trajectory representation learning methods to demonstrate the use of our benchmark, revealing that methods developed using human action datasets do not fully translate to animal datasets.
    
[^187]: 重新思考科学发现中符号回归数据集和基准

    Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10540](http://arxiv.org/abs/2206.10540)

    本文重新审视并重新创建了符号回归数据集，以评估符号回归在科学发现中的潜力和能力，并提出使用归一化编辑距离进行度量。

    

    本文重新审视符号回归（SR）的数据集和评估标准，特别关注其在科学发现中的潜力。针对现有数据集中基于费曼物理讲义的一组公式，我们重新创建了120个数据集，讨论符号回归在科学发现中的性能（SRSD）。对于这120个SRSD数据集，我们仔细审查了公式及其变量的属性，设计了合理的实值范围来采样值，以便我们的新SRSD数据集可用于评估SRSD的潜力，如SR方法是否能从这样的数据集中（重新）发现物理定律。我们还创建了另外120个包含虚拟变量的数据集，以检验SR方法是否能够仅选择必要变量。另外，我们提出使用预测方程与真实方程树之间的归一化编辑距离（NED）来解决现有SR度量存在的一个关键问题，即二元度量问题。

    This paper revisits datasets and evaluation criteria for Symbolic Regression (SR), specifically focused on its potential for scientific discovery. Focused on a set of formulas used in the existing datasets based on Feynman Lectures on Physics, we recreate 120 datasets to discuss the performance of symbolic regression for scientific discovery (SRSD). For each of the 120 SRSD datasets, we carefully review the properties of the formula and its variables to design reasonably realistic sampling ranges of values so that our new SRSD datasets can be used for evaluating the potential of SRSD such as whether or not an SR method can (re)discover physical laws from such datasets. We also create another 120 datasets that contain dummy variables to examine whether SR methods can choose necessary variables only. Besides, we propose to use normalized edit distances (NED) between a predicted equation and the true equation trees for addressing a critical issue that existing SR metrics are either binary
    
[^188]: 统一的随机逼近框架用于学习博弈

    A unified stochastic approximation framework for learning in games. (arXiv:2206.03922v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2206.03922](http://arxiv.org/abs/2206.03922)

    我们提出了一个统一的随机逼近框架，用于分析学习博弈的长期行为。该框架整合了多种常见的学习算法，并得到了多个新的收敛结果，包括对纳什均衡的识别和吸引行动配置集合的概率高收敛。

    

    我们提出了一个灵活的随机逼近框架，用于分析学习博弈的长期行为（包括连续和有限博弈）。所提出的分析模板涵盖了包括基于梯度的方法、用于有限博弈学习的指数/乘法权重算法、上限和遗漏变体等在内的多种常见学习算法。除了提供对这些算法的整合视角，我们的框架还能够得到多个新的收敛结果，包括渐近和有限时间内的结果，在连续和有限博弈中均适用。具体来说，我们提供了一系列可用于识别纳什均衡类别和高概率吸引行动配置集合的标准，我们还引入了一种连贯性的概念，即游戏理论属性，包括严格和尖锐均衡，并且导致在有限时间内收敛。重要的是，我们的分析适用于基于oracle的方法和遗漏方法。

    We develop a flexible stochastic approximation framework for analyzing the long-run behavior of learning in games (both continuous and finite). The proposed analysis template incorporates a wide array of popular learning algorithms, including gradient-based methods, the exponential/multiplicative weights algorithm for learning in finite games, optimistic and bandit variants of the above, etc. In addition to providing an integrated view of these algorithms, our framework further allows us to obtain several new convergence results, both asymptotic and in finite time, in both continuous and finite games. Specifically, we provide a range of criteria for identifying classes of Nash equilibria and sets of action profiles that are attracting with high probability, and we also introduce the notion of coherence, a game-theoretic property that includes strict and sharp equilibria, and which leads to convergence in finite time. Importantly, our analysis applies to both oracle-based and bandit, pa
    
[^189]: 在无偏推荐中重新考虑学习目标：分布转移视角下的研究

    Reconsidering Learning Objectives in Unbiased Recommendation: A Distribution Shift Perspective. (arXiv:2206.03851v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2206.03851](http://arxiv.org/abs/2206.03851)

    本文从分布转移视角出发，研究了从偏向反馈中学习无偏算法进行推荐的问题。通过建立无偏推荐与分布转移的关系，对现有无偏学习方法进行了理论解释并提出了两个泛化界限。

    

    本文研究了从偏向反馈中学习无偏算法进行推荐的问题，我们从一个新颖的分布转移视角来解决这个问题。最近在无偏推荐领域的研究中，通过各种技术如重新加权、多任务学习和元学习，取得了最新的成果。尽管它们在实证上取得了成功，但大部分缺乏理论保证，导致了理论和最新算法之间的显著差距。本文提出了对现有无偏学习目标为何适用于无偏推荐的理论理解。我们建立了无偏推荐与分布转移之间的密切关系，显示了现有的无偏学习目标隐含地将有偏的训练分布与无偏的测试分布对齐。基于这个关系，我们针对现有的无偏学习方法发展了两个泛化界限并分析了它们的学习行为。

    This work studies the problem of learning unbiased algorithms from biased feedback for recommendation. We address this problem from a novel distribution shift perspective. Recent works in unbiased recommendation have advanced the state-of-the-art with various techniques such as re-weighting, multi-task learning, and meta-learning. Despite their empirical successes, most of them lack theoretical guarantees, forming non-negligible gaps between theories and recent algorithms. In this paper, we propose a theoretical understanding of why existing unbiased learning objectives work for unbiased recommendation. We establish a close connection between unbiased recommendation and distribution shift, which shows that existing unbiased learning objectives implicitly align biased training and unbiased test distributions. Built upon this connection, we develop two generalization bounds for existing unbiased learning methods and analyze their learning behavior. Besides, as a result of the distributio
    
[^190]: 利用中心极限定理结构的随机梯度采样方法：改进的分析和更快的算法

    Utilising the CLT Structure in Stochastic Gradient based Sampling : Improved Analysis and Faster Algorithms. (arXiv:2206.03792v3 [math.PR] UPDATED)

    [http://arxiv.org/abs/2206.03792](http://arxiv.org/abs/2206.03792)

    本文研究了基于随机近似的采样算法，利用中心极限定理结构吸收扩散过程中的随机逼近误差并获得了改进的收敛保证。此外，对于SGLD和RBM，我们分别证明了不同的假设条件下较优的收敛率和参数范围。

    

    本文研究了基于随机近似的采样算法，如随机梯度 langevin 动力学（SGLD）和随机批处理方法（RBM）用于相互作用粒子动力学（IPD）。我们观察到，由于中心极限定理（CLT），随机逼近引入的噪声几乎是高斯分布，而驱动布朗运动则是确切的高斯分布。我们利用这种结构来吸收扩散过程中的随机逼近误差，并获得了这些算法的改进收敛保证。对于 SGLD，我们证明了在不需要统一温暖启动的情况下KL散度的第一个稳定收敛率，假设目标密度满足一个对数 Sobolev 不等式。我们的结果意味着在显著较轻的假设条件下，相对于先前的工作，我们具有更优异的一阶 oracle 复杂性。我们还证明了 SGLD 的第一个保证，对于更弱的条件，如 H\''{o}lder 平滑性和 Poincare不等式，从而填补了现有技术和实际应用之间的差距。对于 RBM，我们在 IPD 的弱混合条件下获得了第一次收敛分析和最佳参数范围，这在统计物理和学习理论中具有几个含义。

    We consider stochastic approximations of sampling algorithms, such as Stochastic Gradient Langevin Dynamics (SGLD) and the Random Batch Method (RBM) for Interacting Particle Dynamcs (IPD). We observe that the noise introduced by the stochastic approximation is nearly Gaussian due to the Central Limit Theorem (CLT) while the driving Brownian motion is exactly Gaussian. We harness this structure to absorb the stochastic approximation error inside the diffusion process, and obtain improved convergence guarantees for these algorithms. For SGLD, we prove the first stable convergence rate in KL divergence without requiring uniform warm start, assuming the target density satisfies a Log-Sobolev Inequality. Our result implies superior first-order oracle complexity compared to prior works, under significantly milder assumptions. We also prove the first guarantees for SGLD under even weaker conditions such as H\"{o}lder smoothness and Poincare Inequality, thus bridging the gap between the state-
    
[^191]: NIPQ: 基于噪声代理的集成伪量化

    NIPQ: Noise proxy-based Integrated Pseudo-Quantization. (arXiv:2206.00820v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00820](http://arxiv.org/abs/2206.00820)

    NIPQ提出了一种基于噪声代理的集成伪量化方法，通过融合截断理念，能够统一支持激活和权重的伪量化。NIPQ避免了直通估计器(STE)在量化感知训练中的不稳定性，并在实验中展现了优于现有量化算法的性能。

    

    直通估计器（STE）在量化感知训练（QAT）相关研究中备受青睐，它通过近似使梯度流过非可微函数。然而，在QAT中，STE导致不稳定的收敛，降低了低精度的质量。最近，伪量化训练被提出作为使用伪量化噪声而不是STE更新可学习参数的替代方法。在这项研究中，我们提出了一种新颖的基于噪声代理的集成伪量化（NIPQ）方法，通过将截断理念融入伪量化框架，为激活和权重两方面提供了统一的伪量化支持。NIPQ通过梯度下降更新所有的量化参数（如位宽和截断边界）以及网络参数，避免了STE不稳定性。根据我们的广泛实验，NIPQ在各种视图下表现优于现有的量化算法。

    Straight-through estimator (STE), which enables the gradient flow over the non-differentiable function via approximation, has been favored in studies related to quantization-aware training (QAT). However, STE incurs unstable convergence during QAT, resulting in notable quality degradation in low precision. Recently, pseudoquantization training has been proposed as an alternative approach to updating the learnable parameters using the pseudo-quantization noise instead of STE. In this study, we propose a novel noise proxy-based integrated pseudoquantization (NIPQ) that enables unified support of pseudoquantization for both activation and weight by integrating the idea of truncation on the pseudo-quantization framework. NIPQ updates all of the quantization parameters (e.g., bit-width and truncation boundary) as well as the network parameters via gradient descent without STE instability. According to our extensive experiments, NIPQ outperforms existing quantization algorithms in various vi
    
[^192]: 特征有多偏见？：通过全局敏感性分析计算公正影响函数

    How Biased are Your Features?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00667](http://arxiv.org/abs/2206.00667)

    本论文介绍了公正影响函数（FIF），通过全局敏感性分析的方法量化了不同特征对分类器偏见的影响，从而解决了公平性问题中的核心关注点。

    

    在机器学习中的公平性问题因其在高风险决策任务中的广泛应用而受到广泛关注。未受监管的机器学习分类器可能对数据中的某些人口群体表现出偏见，因此量化和减轻分类器偏见是公平性问题的核心关注点。本文旨在量化数据集中不同特征对分类器偏见的影响。为了做到这一点，我们引入了公正影响函数（FIF）。该函数将偏见分解为其在个体特征和多个特征的交集中的组成部分。关键思想是将现有的群体公平性度量表示为分类器预测的条件方差的差异，并根据全局敏感性分析的分解进行方差估计。为了估计FIFs，我们提出了一个名为FairXplainer的算法，该算法应用分类器预测的方差分解。

    Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier's prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm FairXplainer that applies variance decomposition of classifier's prediction following l
    
[^193]: 高维时间序列数据分析的深度直接判别解码器

    Deep Direct Discriminative Decoders for High-dimensional Time-series Data Analysis. (arXiv:2205.10947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.10947](http://arxiv.org/abs/2205.10947)

    这篇论文提出了一种用于高维时间序列数据分析的新方法，即深度直接判别解码器（D4）。D4通过引入深度神经网络的表达能力和可扩展性，有效地估计了高维观测信号下的潜在状态过程，并在多个数据集上展示了比传统方法更好的性能。

    

    状态空间模型（SSMs）被广泛应用于时间序列数据分析中。SSMs依赖于对状态和观测过程的明确定义。当观测数据的维度增加或观测数据分布偏离正态分布时，描述这些过程并不总是容易的，这成为建模的挑战。在这里，我们提出了一种用于高维观测过程的新的SSM表达形式。我们将这个解决方案称为深度直接判别解码器（D4）。D4将深度神经网络的表达能力和可扩展性引入到SSM表达形式中，使我们能够构建一个新的解决方案，通过高维观测信号高效地估计潜在的状态过程。我们在模拟和真实数据（如Lorenz吸引子、Langevin动力学、随机行走动力学和大鼠海马鞭状神经数据）上演示了D4解决方案，并展示了它比传统的SSMs和RNNs更好的性能。D4可以应用于

    The state-space models (SSMs) are widely utilized in the analysis of time-series data. SSMs rely on an explicit definition of the state and observation processes. Characterizing these processes is not always easy and becomes a modeling challenge when the dimension of observed data grows or the observed data distribution deviates from the normal distribution. Here, we propose a new formulation of SSM for high-dimensional observation processes. We call this solution the deep direct discriminative decoder (D4). The D4 brings deep neural networks' expressiveness and scalability to the SSM formulation letting us build a novel solution that efficiently estimates the underlying state processes through high-dimensional observation signal. We demonstrate the D4 solutions in simulated and real data such as Lorenz attractors, Langevin dynamics, random walk dynamics, and rat hippocampus spiking neural data and show that the D4 performs better than traditional SSMs and RNNs. The D4 can be applied t
    
[^194]: 时间序列分析中的Transformer：教程

    Transformers in Time-series Analysis: A Tutorial. (arXiv:2205.01138v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.01138](http://arxiv.org/abs/2205.01138)

    这篇教程介绍了Transformer架构在时间序列分析中的应用，包括核心组件和多项增强方法，并提供了训练Transformer的最佳实践和技巧。

    

    Transformer架构在自然语言处理和计算机视觉等领域有广泛应用。最近，Transformer被应用于时间序列分析的各个方面。本教程概述了Transformer架构及其应用，并从最近的研究论文中提供了一些示例。我们详细解释了Transformer的核心组件，包括自注意力机制、位置编码、多头注意力和编码器/解码器。还强调了对初始Transformer架构的多项增强方法来处理时间序列任务。本教程还提供了在时间序列分析中有效训练Transformer的最佳实践和技术。

    Transformer architecture has widespread applications, particularly in Natural Language Processing and computer vision. Recently Transformers have been employed in various aspects of time-series analysis. This tutorial provides an overview of the Transformer architecture, its applications, and a collection of examples from recent research papers in time-series analysis. We delve into an explanation of the core components of the Transformer, including the self-attention mechanism, positional encoding, multi-head, and encoder/decoder. Several enhancements to the initial, Transformer architecture are highlighted to tackle time-series tasks. The tutorial also provides best practices and techniques to overcome the challenge of effectively training Transformers for time-series analysis.
    
[^195]: 最后一层重新训练足以提高对虚假相关性的鲁棒性

    Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations. (arXiv:2204.02937v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.02937](http://arxiv.org/abs/2204.02937)

    本文研究表明，简单的最后一层重新训练足以提高神经网络分类器对虚假相关性的鲁棒性，可以在虚假相关性基准测试中与最先进的方法相媲美或胜过，但其复杂度和计算开销较低。此外，对于在ImageNet训练的大型模型进行最后一层重新训练，仅几分钟的训练时间就可以显著降低对背景和纹理信息的依赖，提高对协变量转变的鲁棒性。

    

    神经网络分类器可以主要依靠简单的虚假特征（如背景）进行预测。然而，即使在这些情况下，我们展示了它们仍然经常学习与数据所需属性相关的核心特征，与最近的研究结果相反。受到这一启示的启发，我们证明了简单的最后一层重新训练可以在虚假相关性基准测试中与甚至胜过最先进的方法，但其复杂度和计算开销显著较低。此外，我们还展示了对于在ImageNet训练的大型模型上进行最后一层重新训练，仅经过几分钟的单GPU训练，也可以显著降低对背景和纹理信息的依赖，提高对协变量转变的鲁棒性。

    Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.
    
[^196]: 当缺失观测的位置未知时学习隐马尔可夫模型

    Learning Hidden Markov Models When the Locations of Missing Observations are Unknown. (arXiv:2203.06527v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.06527](http://arxiv.org/abs/2203.06527)

    本文研究了当缺失观测的位置未知时学习隐马尔可夫模型的问题，并提供了不需要先验信息的重建算法。

    

    隐马尔可夫模型（HMM）是用于序列数据分析的最常用的统计模型之一。HMM具有处理缺失数据的能力，这也是它具有通用性的关键之一。然而，标准的HMM学习算法基于缺失观测在观测序列中的位置已知的假设。在自然科学中，这种假设常常不成立，因此通常使用特殊变体的HMM，称为Silent-state HMMs（SHMMs）。尽管这些算法被广泛使用，但它们严重依赖于潜在链的特定结构假设，比如非循环性，这限制了这些方法的适用性。而且，即使在非循环情况下，已经证明这些方法可能导致重建效果差。本文研究了从具有未知缺失观测位置数据中学习HMM的一般问题。我们提供了不需要任何先验信息的重建算法。

    The Hidden Markov Model (HMM) is one of the most widely used statistical models for sequential data analysis. One of the key reasons for this versatility is the ability of HMM to deal with missing data. However, standard HMM learning algorithms rely crucially on the assumption that the positions of the missing observations \emph{within the observation sequence} are known. In the natural sciences, where this assumption is often violated, special variants of HMM, commonly known as Silent-state HMMs (SHMMs), are used. Despite their widespread use, these algorithms strongly rely on specific structural assumptions of the underlying chain, such as acyclicity, thus limiting the applicability of these methods. Moreover, even in the acyclic case, it has been shown that these methods can lead to poor reconstruction. In this paper we consider the general problem of learning an HMM from data with unknown missing observation locations. We provide reconstruction algorithms that do not require any as
    
[^197]: 一种用于采样的近端算法

    A Proximal Algorithm for Sampling. (arXiv:2202.13975v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.13975](http://arxiv.org/abs/2202.13975)

    本论文提出了一种用于处理缺乏平滑性的势能的采样问题的近端算法，在凸和非凸情况下均可适用。该算法的关键创新点在于基于拒绝采样的交替采样框架的实际实现，比现有方法更高效。

    

    我们研究了与缺乏平滑性的势能相关的采样问题。这些势能可以是凸的或非凸的。不同于标准的平滑设置，这些势能只被认为是弱平滑或非平滑的，或者是多个这样的函数的求和。我们开发了一种采样算法，该算法类似于用于这种具有挑战性的采样任务的优化问题的近端算法。我们的算法基于一种称为交替采样框架（ASF）的Gibbs采样的特殊情况。这项工作的关键贡献是基于拒绝采样的ASF的实际实现，适用于非凸和不一定平滑的凸势能。在本工作考虑的几乎所有采样案例中，我们的近端采样算法的复杂性都优于所有现有方法。

    We study sampling problems associated with potentials that lack smoothness. The potentials can be either convex or non-convex. Departing from the standard smooth setting, the potentials are only assumed to be weakly smooth or non-smooth, or the summation of multiple such functions. We develop a sampling algorithm that resembles proximal algorithms in optimization for this challenging sampling task. Our algorithm is based on a special case of Gibbs sampling known as the alternating sampling framework (ASF). The key contribution of this work is a practical realization of the ASF based on rejection sampling for both non-convex and convex potentials that are not necessarily smooth. In almost all the cases of sampling considered in this work, our proximal sampling algorithm achieves better complexity than all existing methods.
    
[^198]: Motif图神经网络

    Motif Graph Neural Network. (arXiv:2112.14900v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.14900](http://arxiv.org/abs/2112.14900)

    Motif图神经网络（MGNN）是一种新颖的框架，用于更好地捕捉高阶图结构，并解决传统图神经网络在区分低阶和高阶图结构上的限制。

    

    图可以模拟实体之间的复杂相互作用，在许多重要的应用中自然发生。这些应用通常可以归纳为标准的图学习任务，其中一个关键步骤是学习低维图表示。目前，在图嵌入方法中，图神经网络（GNNs）是最受欢迎的模型。然而，传统的邻域聚合下的GNNs在区分高阶图结构和低阶图结构方面，具有有限的判别能力。为了捕捉高阶结构，研究人员转而使用图案，发展了基于图案的GNNs。然而，现有的基于图案的GNNs在高阶结构上仍然存在较低的判别能力。为了克服上述限制，我们提出了Motif图神经网络（MGNN），这是一个新颖的框架，以更好地捕捉高阶结构，基于我们提出的图案冗余性最小化算子和单射图案互补。

    Graphs can model complicated interactions between entities, which naturally emerge in many important applications. These applications can often be cast into standard graph learning tasks, in which a crucial step is to learn low-dimensional graph representations. Graph neural networks (GNNs) are currently the most popular model in graph embedding approaches. However, standard GNNs in the neighborhood aggregation paradigm suffer from limited discriminative power in distinguishing \emph{high-order} graph structures as opposed to \emph{low-order} structures. To capture high-order structures, researchers have resorted to motifs and developed motif-based GNNs. However, existing motif-based GNNs still often suffer from less discriminative power on high-order structures. To overcome the above limitations, we propose Motif Graph Neural Network (MGNN), a novel framework to better capture high-order structures, hinging on our proposed motif redundancy minimization operator and injective motif com
    
[^199]: 一个黑盒NLP分类器攻击器

    A Black-box NLP Classifier Attacker. (arXiv:2112.11660v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.11660](http://arxiv.org/abs/2112.11660)

    本文提出了一个黑盒NLP分类器攻击模型，通过基于自注意机制的词选择和贪婪搜索算法进行词替换，解决了影响NLP领域传统图像攻击方法不适用的问题。

    

    深度神经网络在解决各种现实世界任务中具有广泛的应用，并在计算机视觉、图像分类和自然语言处理等领域取得了令人满意的结果。与此同时，神经网络的安全性和鲁棒性已经变得非常重要，因为各种研究已经显示了神经网络的脆弱性。以自然语言处理任务为例，神经网络可能被一个与原始文本高度相似的、经过仔细修改的文本所迷惑。根据之前的研究，大部分研究都集中在图像领域；与图像对抗攻击不同，文本以离散序列表示，传统的图像攻击方法在NLP领域不适用。本文提出了一个基于自注意机制的词级NLP情感分类器攻击模型，其中包括基于词选择的自注意机制和贪婪搜索算法进行词替换。我们进行了实验验证...

    Deep neural networks have a wide range of applications in solving various real-world tasks and have achieved satisfactory results, in domains such as computer vision, image classification, and natural language processing. Meanwhile, the security and robustness of neural networks have become imperative, as diverse researches have shown the vulnerable aspects of neural networks. Case in point, in Natural language processing tasks, the neural network may be fooled by an attentively modified text, which has a high similarity to the original one. As per previous research, most of the studies are focused on the image domain; Different from image adversarial attacks, the text is represented in a discrete sequence, traditional image attack methods are not applicable in the NLP field. In this paper, we propose a word-level NLP sentiment classifier attack model, which includes a self-attention mechanism-based word selection method and a greedy search algorithm for word substitution. We experimen
    
[^200]: 二阶镜像下降：超越平均和折扣的收敛性

    Second-Order Mirror Descent: Convergence in Games Beyond Averaging and Discounting. (arXiv:2111.09982v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2111.09982](http://arxiv.org/abs/2111.09982)

    本文提出了二阶镜像下降（MD2）动力学，在博弈中收敛到非严格变分稳定状态（VSS），不需使用常见技巧，同时具有无遗憾和指数收敛速率。该方法还可以导出许多新的连续时间原始空间动力学，并且通过随机逼近技术对观测有噪声情况下的离散时间MD2提供了收敛保证。

    

    本文提出了连续时间博弈理论镜像下降（MD）动态的二阶扩展，称为MD2，其证明了在不使用常见的辅助技术如时间平均或折扣的情况下，可以收敛到仅仅（但不一定是严格的）变分稳定状态（VSS）。我们展示了MD2在稍作修改后可以享受无遗憾和指数收敛速率，朝着强VSS的方向。MD2还可以用于导出许多新颖的连续时间原始空间动力学。然后我们使用随机逼近技术提供了离散时间MD2在观测有噪声的情况下收敛到内部仅仅VSS的收敛保证。为了说明我们的结果，我们提供了选定的模拟实验。

    In this paper, we propose a second-order extension of the continuous-time game-theoretic mirror descent (MD) dynamics, referred to as MD2, which provably converges to mere (but not necessarily strict) variationally stable states (VSS) without using common auxiliary techniques such as time-averaging or discounting. We show that MD2 enjoys no-regret as well as an exponential rate of convergence towards strong VSS upon a slight modification. MD2 can also be used to derive many novel continuous-time primal-space dynamics. We then use stochastic approximation techniques to provide a convergence guarantee of discrete-time MD2 with noisy observations towards interior mere VSS. Selected simulations are provided to illustrate our results.
    
[^201]: 通过策略梯度算法的高效多目标神经架构搜索框架

    Efficient Multi-objective Neural Architecture Search Framework via Policy Gradient Algorithm. (arXiv:2111.03892v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.03892](http://arxiv.org/abs/2111.03892)

    本论文提出了TND-NAS框架，利用可微架构搜索框架和多目标NAS的兼容性，通过策略梯度算法实现高效多目标神经架构搜索。实验结果表明其优于现有方法。

    

    可微架构搜索已成为神经架构搜索领域的主流研究课题，相对于早期的EA-based和RL-based方法，其高效率备受青睐。然而，这些方法已不再能够自然地应对不可微参数，如能源和资源受限效率等。针对多目标NAS领域的研究旨在解决这个问题，但由于对每个候选架构进行唯一的优化，因此需要大量的计算资源。基于此，我们提出了TND-NAS，它具有不可微参数的兼容性和不同iable NAS框架中的高效性。实验结果表明，在几个基准数据集上，我们提出的TND-NAS在搜索效率和解决方案质量方面优于现有方法。

    Differentiable architecture search has gradually become the mainstream research topic in the field of Neural Architecture Search (NAS) for its high efficiency compared with the early NAS (EA-based, RL-based) methods. Recent differentiable NAS also aims at further improving the search performance and reducing the GPU-memory consumption. However, these methods are no longer naturally capable of tackling the non-differentiable objectives, e.g., energy, resource-constrained efficiency, and other metrics, let alone the multi-objective search demands. Researches in the multi-objective NAS field target this but requires vast computational resources cause of the sole optimization of each candidate architecture. In light of this discrepancy, we propose the TND-NAS, which is with the merits of the high efficiency in differentiable NAS framework and the compatibility among non-differentiable metrics in Multi-objective NAS. Under the differentiable NAS framework, with the continuous relaxation of 
    
[^202]: 偏差矩阵分解

    Deviance Matrix Factorization. (arXiv:2110.05674v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.05674](http://arxiv.org/abs/2110.05674)

    该论文研究了一种适用于偏差数据损失的通用矩阵分解方法，并且通过应用广义线性模型理论提供了支持，该方法具有灵活的算法和处理结构性零元素的能力。作者通过模拟研究和案例研究证明了该方法的鲁棒性和应用广泛性。

    

    我们研究了一种用于偏差数据损失的通用矩阵分解方法，将普遍存在的奇异值分解扩展到平方误差损失之外。虽然之前已经有类似的方法，但我们的方法利用了广义线性模型（GLM）的经典统计方法，并提供了一个灵活的算法，可以通过条目权重来处理结构性零元素。此外，通过调整GLM理论的结果，我们通过以下方式支持这些分解：（i）在GLM设置下显示强一致性，（ii）通过广义Hosmer-Lemeshow检验检验所选择指数族分布的适应性，以及（iii）通过最大特征值间隔法确定分解的秩。为了进一步支持我们的发现，我们进行了模拟研究，评估对分解假设的鲁棒性，并使用图像人脸识别、自然语言处理、网络分析和生物医学等基准数据集进行了广泛的案例研究。

    We investigate a general matrix factorization for deviance-based data losses, extending the ubiquitous singular value decomposition beyond squared error loss. While similar approaches have been explored before, our method leverages classical statistical methodology from generalized linear models (GLMs) and provides an efficient algorithm that is flexible enough to allow for structural zeros via entry weights. Moreover, by adapting results from GLM theory, we provide support for these decompositions by (i) showing strong consistency under the GLM setup, (ii) checking the adequacy of a chosen exponential family via a generalized Hosmer-Lemeshow test, and (iii) determining the rank of the decomposition via a maximum eigenvalue gap method. To further support our findings, we conduct simulation studies to assess robustness to decomposition assumptions and extensive case studies using benchmark datasets from image face recognition, natural language processing, network analysis, and biomedica
    
[^203]: 具有不完整数据的网络时间序列预测

    Networked Time Series Prediction with Incomplete Data. (arXiv:2110.02271v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.02271](http://arxiv.org/abs/2110.02271)

    本文研究了具有不完整数据的网络时间序列（NETS）预测问题。提出了NETS-ImpGAN深度学习框架，可以训练不完整数据，并引入了图时序注意力网络来捕捉时间序列之间的相关性和时间相关性。

    

    网络时间序列（NETS）是给定图上的一组时间序列，每个节点对应一个时间序列。它在智能交通、环境监测和移动网络管理等领域都有广泛应用。在这些应用中，一个重要的任务是基于历史值和底层图来预测NETS的未来值。大多数现有的方法需要完整的数据进行训练。然而，在现实世界的情况下，由于传感器故障、不完全感知覆盖等原因，数据缺失是常见的。本文研究了具有不完整数据的NETS预测问题。我们提出了NETS-ImpGAN，一种可以在历史和未来的缺失值上训练的新型深度学习框架。此外，我们提出了新颖的图时序注意力网络，通过引入注意力机制来捕捉时间序列之间的相关性和时间相关性。我们在三个真实数据集上进行了大量实验。

    A networked time series (NETS) is a family of time series on a given graph, one for each node. It has found a wide range of applications from intelligent transportation, environment monitoring to mobile network management. An important task in such applications is to predict the future values of a NETS based on its historical values and the underlying graph. Most existing methods require complete data for training. However, in real-world scenarios, it is not uncommon to have missing data due to sensor malfunction, incomplete sensing coverage, etc. In this paper, we study the problem of NETS prediction with incomplete data. We propose NETS-ImpGAN, a novel deep learning framework that can be trained on incomplete data with missing values in both history and future. Furthermore, we propose novel Graph Temporal Attention Networks by incorporating the attention mechanism to capture both inter-time series correlations and temporal correlations. We conduct extensive experiments on three real-
    
[^204]: 肝移植手术中的动脉血压波形具有反映接受者病情和预测短期结果的形态可变性

    Arterial blood pressure waveform in liver transplant surgery possesses variability of morphology reflecting recipients' acuity and predicting short term outcomes. (arXiv:2109.10258v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2109.10258](http://arxiv.org/abs/2109.10258)

    该研究使用动力扩散映射算法研究了肝移植手术中动脉血压波形的形态可变性与接受者病情以及短期结果之间的相关性。

    

    背景：我们研究了动脉血压（ABP）波形形态的心拍间波动背后的临床信息。我们提出了动力扩散映射算法（DDMap），用于量化形态的可变性。潜在的生理学机制可能涉及各种生理机制之间的复杂相互作用，以调节心血管系统。由于肝移植手术包含不同的阶段，我们研究了其在不同手术步骤中的临床行为。方法：我们的研究使用基于无监督流形学习的DDmap算法，得到了形态的心拍间可变性的定量指标。我们检查了ABP形态可变性与疾病急性度（由终末期肝病模型评分（MELD）评分、术后实验室数据和4种早期移植失败（EAF）评分指示）之间的相关性。结果：在85名入组患者中，ABP形态的可变性与疾病急性度、术后实验室数据以及4种早期移植失败（EAF）评分之间存在相关性。

    Background: We investigated clinical information underneath the beat-to-beat fluctuation of the arterial blood pressure (ABP) waveform morphology. We proposed the Dynamical Diffusion Map algorithm (DDMap) to quantify the variability of morphology. The underlying physiology could be the compensatory mechanisms involving complex interactions between various physiological mechanisms to regulate the cardiovascular system. As a liver transplant surgery contains distinct periods, we investigated its clinical behavior in different surgical steps. Methods: Our study used DDmap algorithm, based on unsupervised manifold learning, to obtain a quantitative index for the beat-to-beat variability of morphology. We examined the correlation between the variability of ABP morphology and disease acuity as indicated by Model for End-Stage Liver Disease (MELD) scores, the postoperative laboratory data, and 4 early allograft failure (EAF) scores. Results: Among the 85 enrolled patients, the variability of 
    
[^205]: 从众包数据中发现顺序标签中的真实性

    Truth Discovery in Sequence Labels from Crowds. (arXiv:2109.04470v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2109.04470](http://arxiv.org/abs/2109.04470)

    本研究提出了一种基于优化的方法（$AggSLC$），从众包数据中推断顺序标签的真实值，以解决顺序标签汇总任务中的复杂依赖关系问题。

    

    标注质量和数量对于自然语言处理中的顺序标签学习任务的性能有积极影响。雇佣领域专家对语料库进行标注在时间和金钱上非常昂贵。为了解决这个问题，已经开始使用众包平台（如Amazon Mechanical Turk）来辅助标注。然而，通过这种方式收集的标注容易受到众包工人专业知识不足而产生的人为错误的影响。现有的标注汇总方法假设标注是独立的，因此在处理具有复杂依赖关系的顺序标签汇总任务时面临挑战。为了解决这些挑战，我们提出了一种基于优化的方法，利用工作者提供的标注来推断顺序标签的真实值。我们提出的顺序标签汇总方法（$AggSLC$）同时考虑了顺序标签任务的特征、工作者可信度和优势。

    Annotation quality and quantity positively affect the learning performance of sequence labeling, a vital task in Natural Language Processing. Hiring domain experts to annotate a corpus is very costly in terms of money and time. Crowdsourcing platforms, such as Amazon Mechanical Turk (AMT), have been deployed to assist in this purpose. However, the annotations collected this way are prone to human errors due to the lack of expertise of the crowd workers. Existing literature in annotation aggregation assumes that annotations are independent and thus faces challenges when handling the sequential label aggregation tasks with complex dependencies. To conquer the challenges, we propose an optimization-based method that infers the ground truth labels using annotations provided by workers for sequential labeling tasks. The proposed Aggregation method for Sequential Labels from Crowds ($AggSLC$) jointly considers the characteristics of sequential labeling tasks, workers' reliabilities, and adva
    
[^206]: 基于任务的显式超参数预测模型的学习

    Learning an Explicit Hyperparameter Prediction Function Conditioned on Tasks. (arXiv:2107.02378v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.02378](http://arxiv.org/abs/2107.02378)

    本研究探索了一种基于任务的学习方法，学习一个显式超参数预测函数以适应不同的查询任务。

    

    元学习近年来引起了机器学习界的广泛关注。与传统的机器学习旨在学习内在的预测规则以预测新的查询数据标签不同，元学习旨在从观察到的任务中学习机器学习的学习方法，以便通过利用元学习的学习方法对新的查询任务进行泛化。在本研究中，我们将这样的学习方法解释为学习一个显式超参数预测函数，该函数由所有训练任务共享。具体而言，这个函数被表示为一个参数化函数，称为元学习器，将训练/测试任务映射到其合适的超参数设置，提取自称为元学习机的预先指定的函数集。这种设置保证了元学习的学习方法能够灵活地适应不同的查询任务，而不是像许多现有的元学习方法一样，只获得固定的超参数，适应性较差。

    Meta learning has attracted much attention recently in machine learning community. Contrary to conventional machine learning aiming to learn inherent prediction rules to predict labels for new query data, meta learning aims to learn the learning methodology for machine learning from observed tasks, so as to generalize to new query tasks by leveraging the meta-learned learning methodology. In this study, we interpret such learning methodology as learning an explicit hyper-parameter prediction function shared by all training tasks. Specifically, this function is represented as a parameterized function called meta-learner, mapping from a training/test task to its suitable hyper-parameter setting, extracted from a pre-specified function set called meta learning machine. Such setting guarantees that the meta-learned learning methodology is able to flexibly fit diverse query tasks, instead of only obtaining fixed hyper-parameters by many current meta learning methods, with less adaptability 
    
[^207]: 上下文逆优化：离线和在线学习

    Contextual Inverse Optimization: Offline and Online Learning. (arXiv:2106.14015v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.14015](http://arxiv.org/abs/2106.14015)

    这项研究研究了具有反馈信息的离线和在线情境优化问题，通过观察最佳动作并最小化后悔来优化决策制定。

    

    我们研究了具有反馈信息的离线和在线情境优化问题，其中我们观察到的不是损失，而是一个具有完全了解目标函数的预测神经网络将会采取的最佳动作。我们的目标是最小化后悔，后悔定义为我们的损失与全知预测神经网络产生的损失之间的差异。在离线情境中，决策者可以获得过去时期的信息并需要做出一个决策，而在在线情境中，决策者根据每个时期的新一组可行动作和情境函数来动态优化决策。对于离线情境，我们将最优极小极大策略特征化，确定了可以作为数据产生的信息的基础几何形状的函数表现。在在线情境中，我们利用这种几何特征来优化累积后悔。我们开发了算法来找到累积后悔的最小化策略。

    We study the problems of offline and online contextual optimization with feedback information, where instead of observing the loss, we observe, after-the-fact, the optimal action an oracle with full knowledge of the objective function would have taken. We aim to minimize regret, which is defined as the difference between our losses and the ones incurred by an all-knowing oracle. In the offline setting, the decision-maker has information available from past periods and needs to make one decision, while in the online setting, the decision-maker optimizes decisions dynamically over time based a new set of feasible actions and contextual functions in each period. For the offline setting, we characterize the optimal minimax policy, establishing the performance that can be achieved as a function of the underlying geometry of the information induced by the data. In the online setting, we leverage this geometric characterization to optimize the cumulative regret. We develop an algorithm that y
    
[^208]: 深度代理因果学习及其在混淆赌博策略评估中的应用

    Deep Proxy Causal Learning and its Application to Confounded Bandit Policy Evaluation. (arXiv:2106.03907v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.03907](http://arxiv.org/abs/2106.03907)

    本论文提出了一种深度代理因果学习（PCL）方法，用于在存在混淆因素的情况下估计治疗对结果的因果效应。通过构建治疗和代理之间的模型，并利用该模型在给定代理的情况下学习治疗对结果的影响，PCL可以保证恢复真实的因果效应。作者还提出了一种名为深度特征代理变量方法（DFPV）的新方法，用于处理高维和非线性复杂关系的情况，并表明DFPV在合成基准测试中的性能优于最先进的PCL方法。

    

    代理因果学习（PCL）是一种在存在未观察到的混淆因素时，利用代理（结构化侧面信息）估计治疗对结果的因果效应的方法。这是通过两阶段回归实现的：在第一阶段，我们建模治疗和代理之间的关系；在第二阶段，我们利用这个模型来学习在给定代理提供的上下文下，治疗对结果的影响。PCL在可识别条件下保证恢复真实的因果效应。我们提出了一种新的PCL方法，深度特征代理变量方法（DFPV），以解决代理、治疗和结果为高维且具有非线性复杂关系的情况，如深度神经网络特征表示。我们表明DFPV在具有挑战性的合成基准测试中优于最近的最先进的PCL方法，包括涉及高维图像数据的设置。此外，我们还展示了PCL的应用...

    Proxy causal learning (PCL) is a method for estimating the causal effect of treatments on outcomes in the presence of unobserved confounding, using proxies (structured side information) for the confounder. This is achieved via two-stage regression: in the first stage, we model relations among the treatment and proxies; in the second stage, we use this model to learn the effect of treatment on the outcome, given the context provided by the proxies. PCL guarantees recovery of the true causal effect, subject to identifiability conditions. We propose a novel method for PCL, the deep feature proxy variable method (DFPV), to address the case where the proxies, treatments, and outcomes are high-dimensional and have nonlinear complex relationships, as represented by deep neural network features. We show that DFPV outperforms recent state-of-the-art PCL methods on challenging synthetic benchmarks, including settings involving high dimensional image data. Furthermore, we show that PCL can be app
    
[^209]: 使用航点的进化策略形状化策略搜索

    Shaped Policy Search for Evolutionary Strategies using Waypoints. (arXiv:2105.14639v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2105.14639](http://arxiv.org/abs/2105.14639)

    本文提出了一种使用航点的进化策略形状化策略搜索方法，旨在改进强化学习中黑盒方法的探索能力。该方法通过学习代理的动力学，结合优化过程加速训练，并在Carla驾驶和UR5机械臂模拟器上进行了实验证明其普适性。

    

    本文致力于改进黑盒方法，特别是进化策略在应用于具有中间航点/子目标的强化学习问题时的探索能力。由于进化策略具有高度可并行化的特点，我们将不仅提取标量累积奖励，而是利用在推出/评估过程中获得的轨迹中的状态-动作对来学习代理的动力学。然后，将学到的动力学用于优化过程以加速训练。最后，我们通过在Carla驾驶和UR5机械臂模拟器上进行的实验结果展示了我们提出的方法的普适性。

    In this paper, we try to improve exploration in Blackbox methods, particularly Evolution strategies (ES), when applied to Reinforcement Learning (RL) problems where intermediate waypoints/subgoals are available. Since Evolutionary strategies are highly parallelizable, instead of extracting just a scalar cumulative reward, we use the state-action pairs from the trajectories obtained during rollouts/evaluations, to learn the dynamics of the agent. The learnt dynamics are then used in the optimization procedure to speed-up training. Lastly, we show how our proposed approach is universally applicable by presenting results from experiments conducted on Carla driving and UR5 robotic arm simulators.
    
[^210]: 联机强化学习与模仿学习的桥梁：一个悲观的故事

    Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. (arXiv:2103.12021v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.12021](http://arxiv.org/abs/2103.12021)

    本论文提出了一种新的联机强化学习框架，通过平滑插值的方式将模仿学习和纯联机强化学习统一起来。框架围绕着一种衡量行为策略与专家策略偏离程度的弱版本集中系数展开。通过该框架，研究者进一步研究了算法设计的问题：能否开发出实现最小极大最优性的算法？

    

    联机（或批次）强化学习算法旨在从固定的数据集中学习最优策略，而无需主动收集数据。根据离线数据集的组成，主要使用两种方法：适用于专家数据集的模仿学习和通常需要均匀覆盖数据集的纯联机强化学习。从实践的角度来看，数据集通常偏离这两个极端，并且通常事先不知道确切的数据组成。为了填补这一差距，我们提出了一个新的联机强化学习框架，它在数据组成的两个极端之间平滑插值，从而统一了模仿学习和纯联机强化学习。新的框架围绕一个弱版本的集中系数展开，该系数衡量了行为策略与专家策略之间的偏离程度。在这个新的框架下，我们进一步研究了算法设计的问题：能否开发出一种实现最小极大最优性的算法？

    Offline (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a fixed dataset without active data collection. Based on the composition of the offline dataset, two main categories of methods are used: imitation learning which is suitable for expert datasets and vanilla offline RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown a priori. To bridge this gap, we present a new offline RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla offline RL. The new framework is centered around a weak version of the concentrability coefficient that measures the deviation from the behavior policy to the expert policy alone.  Under this new framework, we further investigate the question on algorithm design: can one develop an algorithm that achieves a minimax optimal r
    
[^211]: 广义的迭代和签名

    Generalized iterated-sums signatures. (arXiv:2012.04597v3 [math.RA] UPDATED)

    [http://arxiv.org/abs/2012.04597](http://arxiv.org/abs/2012.04597)

    本文研究了广义的迭代和签名的代数性质，并介绍了三个类似于机器学习应用的非线性变换。

    

    我们探讨了迭代和签名的广义版本的代数性质，受到F. Király和H. Oberhauser之前的工作启发。特别地，我们展示了通过考虑后者上的变形准混洗乘积，如何恢复与张量代数上的相关线性映射的特征性质。我们介绍了三个非线性变换在迭代和签名上，类似于机器学习应用，并展示了它们的一些性质。

    We explore the algebraic properties of a generalized version of the iterated-sums signature, inspired by previous work of F.~Kir\'aly and H.~Oberhauser. In particular, we show how to recover the character property of the associated linear map over the tensor algebra by considering a deformed quasi-shuffle product of words on the latter. We introduce three non-linear transformations on iterated-sums signatures, close in spirit to Machine Learning applications, and show some of their properties.
    
[^212]: 深度强化学习中的迁移学习综述

    Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2009.07888](http://arxiv.org/abs/2009.07888)

    这篇综述调查了深度强化学习领域中的迁移学习方法的最新进展，并提供了一个对这些方法进行分类的框架。分析了它们的目标、方法学、兼容的强化学习背景以及实际应用，并探讨了迁移学习与其他相关主题之间的联系。

    

    强化学习是解决序列决策问题的学习范式。近年来，随着深度神经网络的快速发展，强化学习取得了显著的进展。除了在机器人和游戏等诸多领域中具有良好前景的强化学习，迁移学习作为一种解决强化学习面临的各种挑战的方法已经出现，通过从外部专业知识中转移知识，以提高学习过程的效率和效果。在这项综述中，我们系统地调查了深度强化学习领域中的迁移学习方法的最新进展。具体而言，我们提供了一个对最先进的迁移学习方法进行分类的框架，在此框架下分析了它们的目标、方法学、兼容的强化学习背景以及实际应用。我们还探讨了迁移学习与其他相关主题之间的联系。

    Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics 
    
[^213]: 通过2层马尔可夫决策过程学习在团队中切换代理的方法

    Learning to Switch Among Agents in a Team via 2-Layer Markov Decision Processes. (arXiv:2002.04258v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.04258](http://arxiv.org/abs/2002.04258)

    本文研究了在团队中学习切换代理控制的问题，并开发了一种在线学习算法，通过学习代理的策略和环境的转移概率，在不同自动化水平下使现有的强化学习代理能够工作。该算法的总遗憾与最佳切换策略相比是次线性的，当多个代理团队在相似环境中运行时，该算法从维护环境的共享置信界中获益匪浅。

    

    强化学习代理在通常以完全自主的方式工作的假设下开发和评估 - 它们将采取所有行动。本文的目标是开发算法，通过学习在代理之间切换控制，使现有的强化学习代理能够在不同的自动化水平下工作。为此，我们首先正式定义了通过2层马尔可夫决策过程在团队中学习切换控制的问题。然后，我们使用代理的策略和环境的转移概率的上置信界开发了一种在线学习算法，以找到一系列切换策略。我们的算法相对于最佳切换策略的总遗憾在学习步骤的数量上是次线性的，并且每当多个代理团队在相似的环境中运行时，我们的算法从维护环境的共享置信界中获得很大的好处。

    Reinforcement learning agents have been mostly developed and evaluated under the assumption that they will operate in a fully autonomous manner -- they will take all actions. In this work, our goal is to develop algorithms that, by learning to switch control between agents, allow existing reinforcement learning agents to operate under different automation levels. To this end, we first formally define the problem of learning to switch control among agents in a team via a 2-layer Markov decision process. Then, we develop an online learning algorithm that uses upper confidence bounds on the agents' policies and the environment's transition probabilities to find a sequence of switching policies. The total regret of our algorithm with respect to the optimal switching policy is sublinear in the number of learning steps and, whenever multiple teams of agents operate in a similar environment, our algorithm greatly benefits from maintaining shared confidence bounds for the environments' transit
    
[^214]: 使用重要权重示范的元适应性学习

    Meta Adaptation using Importance Weighted Demonstrations. (arXiv:1911.10322v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1911.10322](http://arxiv.org/abs/1911.10322)

    本文提出了一种使用重要权重示范的元适应性学习算法，通过对特定任务的先前知识进行分配重要权重，实现了在任何相关任务上的泛化。实验证明，该方法能够使机器人在多样化环境任务中进行训练，并通过少量示范适应未知环境。

    

    由于其高样本效率，模仿学习变得极为流行。然而，在实际应用场景中，由于大多数任务的轨迹分布不断变化，仅仅基于连续聚合的数据来进行模型拟合是徒劳的。在某些情况下，分布发生如此大的变化，以至于智能体很难推断出新任务。我们提出了一种新颖的算法，通过对一组特定任务的先前知识进行分配重要权重，从而在任何相关任务上进行泛化。我们展示了一些实验，在这些实验中，机器人从多样化的环境任务中训练，并能够通过少量示范进行学习，从而适应未知环境。我们还开发了一个原型机器人系统，在视觉导航任务上测试我们的方法，并获得了能够验证这些假设的实验证据。

    Imitation learning has gained immense popularity because of its high sample-efficiency. However, in real-world scenarios, where the trajectory distribution of most of the tasks dynamically shifts, model fitting on continuously aggregated data alone would be futile. In some cases, the distribution shifts, so much, that it is difficult for an agent to infer the new task. We propose a novel algorithm to generalize on any related task by leveraging prior knowledge on a set of specific tasks, which involves assigning importance weights to each past demonstration. We show experiments where the robot is trained from a diversity of environmental tasks and is also able to adapt to an unseen environment, using few-shot learning. We also developed a prototype robot system to test our approach on the task of visual navigation, and experimental results obtained were able to confirm these suppositions.
    

