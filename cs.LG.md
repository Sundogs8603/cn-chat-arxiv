# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [FlashDecoding++: Faster Large Language Model Inference on GPUs.](http://arxiv.org/abs/2311.01282) | FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。 |
| [^2] | [Score Normalization for a Faster Diffusion Exponential Integrator Sampler.](http://arxiv.org/abs/2311.00157) | 该论文提出了一种得分归一化方法，用于改进扩散指数积分器采样器的生成质量和减小积分误差。 |
| [^3] | [SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning.](http://arxiv.org/abs/2310.19805) | 这篇论文提出了一种称为SERA的奖励增强框架，用于改善离线到在线强化学习中的探索能力。它通过设计内在奖励来鼓励agent进行探索，并实现更好的在线微调效果。 |
| [^4] | [An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits.](http://arxiv.org/abs/2310.19025) | 该论文提出了一种用于对抗性上下文赌博问题的面向Oracle高效的放松方法，通过调用离线优化Oracle来降低遗憾界限，并且在界限方面取得了显著的改进，达到了先前最佳界限，并与原始界限相匹配。 |
| [^5] | [Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles.](http://arxiv.org/abs/2310.15952) | 本文引入了一种新颖的三阶段方法，通过变换器和条件扩散模型来改善医学图像分类模型对实际应用中常见成像变异性的鲁棒性。 |
| [^6] | [Conversational Financial Information Retrieval Model (ConFIRM).](http://arxiv.org/abs/2310.13001) | ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。 |
| [^7] | [PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning.](http://arxiv.org/abs/2310.09183) | 本研究提出了一种名为PRIOR的个性化先验方案，用于解决个性化联邦学习中由于全局模型忽视特定信息而导致的不完整信息问题。该方案使用带Bregman散度的PFL框架将个性化先验与本地目标函数解耦，以提高适应性和性能。 |
| [^8] | [Stabilizing Estimates of Shapley Values with Control Variates.](http://arxiv.org/abs/2310.07672) | 使用控制变量的方法稳定Shapley值的估计，减少了模型解释的不确定性，适用于任何机器学习模型。 |
| [^9] | [An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l.](http://arxiv.org/abs/2310.07325) | 在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。 |
| [^10] | [Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory.](http://arxiv.org/abs/2310.04935) | 这项工作利用PAC-Bayesian理论为变分自动编码器提供了统计保证，包括对后验分布、重构损失和输入与生成分布之间距离的上界。 |
| [^11] | [Attention-based Multi-task Learning for Base Editor Outcome Prediction.](http://arxiv.org/abs/2310.02919) | 基于注意力的多任务学习模型可以加速基因编辑设计过程，并提高编辑结果预测的准确性。 |
| [^12] | [Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization.](http://arxiv.org/abs/2310.02519) | 提出了一种参数化凸支配（PCM）方法，用于在摊销优化中逼近目标函数。该方法具有通用逼近性能，并可以通过单个凸优化获得全局最小值。 |
| [^13] | [Bag of Tricks for Fully Test-Time Adaptation.](http://arxiv.org/abs/2310.02416) | 本研究提出了一种充分测试时间适应（TTA）的技巧，包括小批量归一化、流平衡、可靠样本选择和网络置信度校准。通过对这些技术的详细分析，我们揭示了它们在准确性、计算能力和模型复杂性之间的权衡，并取得了新的最先进的结果。 |
| [^14] | [CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images.](http://arxiv.org/abs/2310.00233) | causalimages R 包提供了使用图像进行因果推断的新工具，并能够整合卫星和生物医学图像等新数据源，通过分解治疗效果的异质性和控制混淆变量来进行分析。 |
| [^15] | [Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer.](http://arxiv.org/abs/2309.17340) | Outage-Watch使用极端事件正则化的方法提早预测云服务中断，通过捕获质量指标的恶化情况，改善灵活性并提高尾部分布的学习能力。 |
| [^16] | [Neuro-Inspired Hierarchical Multimodal Learning.](http://arxiv.org/abs/2309.15877) | 这项研究提出了一种神经启发的分层多模态学习方法，利用信息瓶颈理论构建了一种有效且紧凑的信息流，实现了对真实世界的全面和准确的感知。 |
| [^17] | [Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors.](http://arxiv.org/abs/2309.13135) | 该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。 |
| [^18] | [State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding.](http://arxiv.org/abs/2309.12482) | 本论文致力于开发一种基于概念的解释方法，旨在提高非AI专家对AI决策的理解。通过定义顺序决策设置中的“概念”以及探索基于概念的解释对RL agent学习效果和最终用户对agent决策理解的双重好处，我们提出了一个统一的框架。 |
| [^19] | [Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems.](http://arxiv.org/abs/2308.10697) | 本文研究了随机动力系统中的动态模式分解(DMD)方法。通过在Koopman框架中引入方差来解决挑战，包括虚假模式和本征谱。结果表明，需要超越预期来有效处理随机系统。 |
| [^20] | [Adaptive Preferential Attached kNN Graph With Distribution-Awareness.](http://arxiv.org/abs/2308.02442) | 本文提出了一种名为paNNG的算法，它结合了自适应kNN和基于分布的图构建。通过包含分布信息，paNNG能够有效提升模糊样本的性能，并实现更好的准确性和泛化能力。 |
| [^21] | [Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity.](http://arxiv.org/abs/2308.00177) | 本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。 |
| [^22] | [Efficient Semi-Supervised Federated Learning for Heterogeneous Participants.](http://arxiv.org/abs/2307.15870) | 本论文提出了一种高效的半监督异构参与者联邦学习系统，通过引入聚类正则化来改进模型在数据非独立同分布情况下的性能，并对模型收敛性进行了理论和实验研究。 |
| [^23] | [Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams).](http://arxiv.org/abs/2307.02509) | 本文提出了一种计算框架，用于将传统的自编码神经网络扩展到合并树的Wasserstein度量空间。算法在准确性和可解释性方面表现出 |
| [^24] | [Solving Kernel Ridge Regression with Gradient-Based Optimization Methods.](http://arxiv.org/abs/2306.16838) | 本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。 |
| [^25] | [StrainNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks.](http://arxiv.org/abs/2306.12818) | 该论文介绍了一种使用SE(3)-等变图神经网络预测晶体结构弹性性能的方法，可以高效地产生准确的弹性模量，并且能够预测应变能密度和相关的弹性常数，具有一定的可拓展性。 |
| [^26] | [Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks.](http://arxiv.org/abs/2306.05674) | 本论文基于神经切向核理论，提出了一种高效的方法以减少超参数化神经网络中的过程不确定性，并只需要使用一种辅助网络就可以消除这种不确定性。 |
| [^27] | [Transformers learn to implement preconditioned gradient descent for in-context learning.](http://arxiv.org/abs/2306.00297) | 本论文通过研究线性transformers在训练过程中的全局最小值，证明了对于一个注意力层，transformers能够实现一次预处理梯度下降，而对于一个$k$个注意力层的transformer，它可以实现多次预处理梯度下降。 |
| [^28] | [Dynamic Sparsity Is Channel-Level Sparsity Learner.](http://arxiv.org/abs/2305.19454) | 本文提出了一种名为Channel-aware dynamic sparse (Chase)的方法，使用端到端训练实现了GPU友好的通道级别稀疏，不需要任何特殊操作，并且可以直接在通用硬件上加速，显著减小模型大小，同时保持性能。 |
| [^29] | [Adapting Fairness Interventions to Missing Values.](http://arxiv.org/abs/2305.19429) | 本文研究了如何在缺失值的情况下实现公平的分类。传统方法会加剧歧视。本文证明从插补数据训练分类器会恶化组公平性和平均准确性。作者提出可扩展和适应性的算法，可以与其他公平干预算法结合使用，以处理所有可能的缺失模式。 |
| [^30] | [Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers.](http://arxiv.org/abs/2305.17328) | 《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。 |
| [^31] | [Flexible Grammar-Based Constrained Decoding for Language Models.](http://arxiv.org/abs/2305.13971) | 本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。 |
| [^32] | [Deep quantum neural networks form Gaussian processes.](http://arxiv.org/abs/2305.09957) | 本文证明了基于Haar随机酉或正交深量子神经网络的某些模型的输出会收敛于高斯过程。然而，这种高斯过程不能用于通过贝叶斯统计学来有效预测QNN的输出。 |
| [^33] | [Asymmetrically-powered Neural Image Compression with Shallow Decoders.](http://arxiv.org/abs/2304.06244) | 采用浅层或线性解码转换来缩小解码复杂度，同时利用通常不对称的计算预算、更强的编码网络和迭代编码来保持压缩性能，实现了速率失真性能与传统方法相竞争，解码复杂度降低80%以上。 |
| [^34] | [Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks.](http://arxiv.org/abs/2304.05727) | 本文提出了一种新方法，Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化，从而大大减少了对隐藏Clever Hans策略的依赖，并实现了更高的性能。 |
| [^35] | [PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling.](http://arxiv.org/abs/2304.04307) | PriorCVAE 提出了一种处理高斯过程先验 MCMC 参数推断的贝叶斯深度生成建模新方法，可通过将 VAE 建模条件化于随机过程超参数处理超参数推断与学习先验之间的信息流断裂问题。 |
| [^36] | [Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models.](http://arxiv.org/abs/2304.02754) | 本研究使用两种经典认知心理学技术来估算人类和GPT-3等大型语言模型的词汇语义结构，结果表明人类的概念结构稳健鲁棒，而大型语言模型的行为估算结构更多取决于具体任务。 |
| [^37] | [Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model.](http://arxiv.org/abs/2304.02169) | 此论文提出了一种名为HALO的方法，它是一个层级自回归模型，可以生成高保真、细粒度电子健康记录数据，而这些数据可以用于训练准确的ML模型，且无需涉及隐私问题。 |
| [^38] | [Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility.](http://arxiv.org/abs/2303.08994) | 物理信息神经网络被应用于电力系统动态模拟，可以比传统方法快10至1000倍，同时准确性和数值稳定性也足够满足需求。同时，他们也提出了一个基于梯度的方法来规范化NN训练。 |
| [^39] | [LExecutor: Learning-Guided Execution.](http://arxiv.org/abs/2302.02343) | LExecutor是一个学习引导的执行方法，通过让神经模型预测缺失值，并将其注入到执行中，可以以自由约束的方式执行任意代码片段。该方法在Python代码和从Stack Overflow提取的代码片段上表现良好。 |
| [^40] | [NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation.](http://arxiv.org/abs/2211.04370) | NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。 |
| [^41] | [Primal Estimated Subgradient Solver for SVM for Imbalanced Classification.](http://arxiv.org/abs/2206.09311) | 本研究旨在实现对不平衡数据集的分类，并评估成本敏感的PEGASOS SVM的性能，同时将核函数纳入SVM中扩展Ding的工作。 |
| [^42] | [Independent and Decentralized Learning in Markov Potential Games.](http://arxiv.org/abs/2205.14590) | 独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。 |
| [^43] | [On the (In)security of Peer-to-Peer Decentralized Machine Learning.](http://arxiv.org/abs/2205.08443) | 本文对去中心化学习进行了首次深入隐私分析，引入了一系列新颖的攻击方法，并证明去中心化学习并未提供比联邦学习更好的安全优势，反而增加了攻击面。而且，隐私保护配置需要全连接网络，失去了实际优势，完全打败了去中心化方法的目标。 |
| [^44] | [SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models.](http://arxiv.org/abs/2108.13672) | 使用无注意力的序列模型SANSformer在电子健康记录中进行自我监督预测，充分挖掘了Transformer在EHR应用中的优势。主要应用于预测未来的医疗资源利用，特别适用于处理不同患者子组，如罕见疾病患者。 |
| [^45] | [Learning Runtime Decisions for Adaptive Real-Time Perception.](http://arxiv.org/abs/2106.05665) | Chanakya是一种学习近似执行框架，通过在实时感知中平衡精度和延迟的训练，自动学习权衡的决策，并在多个感知任务中取得了最先进的性能。 |

# 详细

[^1]: FlashDecoding++: 在GPU上加速大规模语言模型推理的更快算法

    FlashDecoding++: Faster Large Language Model Inference on GPUs. (arXiv:2311.01282v1 [cs.LG])

    [http://arxiv.org/abs/2311.01282](http://arxiv.org/abs/2311.01282)

    FlashDecoding++是一种快速的LLM推理引擎，通过解决同步部分softmax更新、未充分利用扁平GEMM计算和静态数据流导致的性能损失等挑战，实现了大规模语言模型推理的加速。

    

    随着大规模语言模型在各个领域的重要性日益增加，加速语言模型推理仍然存在一些挑战未解决：(1) 同步部分softmax更新。softmax操作需要同步更新每个部分softmax结果，导致LLM中注意力计算的开销增加约20%。(2) 未充分利用扁平GEMM计算。在LLM推理中执行GEMM的矩阵形状是扁平的，导致在先前的设计中填充零后计算未充分利用，性能损失超过50%。(3) 静态数据流导致的性能损失。LLM中的内核性能取决于不同的输入数据特征、硬件配置等。单一和静态的数据流可能导致LLM推理中不同形状的GEMM的性能损失达到50.25%。我们提出了FlashDecoding++，一种快速支持主流LLM和硬件后端的LLM推理引擎。为了解决上述挑战，FlashDecoding++实现了以下目标：

    As the Large Language Model (LLM) becomes increasingly important in various domains. However, the following challenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax update. The softmax operation requires a synchronized update operation among each partial softmax result, leading to ~20% overheads for the attention computation in LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat, leading to under-utilized computation and >50% performance loss after padding zeros in previous designs. (3) Performance loss due to static dataflow. Kernel performance in LLM depends on varied input data features, hardware configurations, etc. A single and static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM inference.  We present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
    
[^2]: 一个更快的扩散指数积分器采样器的得分归一化

    Score Normalization for a Faster Diffusion Exponential Integrator Sampler. (arXiv:2311.00157v1 [cs.LG])

    [http://arxiv.org/abs/2311.00157](http://arxiv.org/abs/2311.00157)

    该论文提出了一种得分归一化方法，用于改进扩散指数积分器采样器的生成质量和减小积分误差。

    

    最近，张等人提出了一种用于从扩散模型中快速生成样本的扩散指数积分器采样器（DEIS）。它利用概率流常微分方程（ODE）的半线性特性来大大减小积分误差，并在低函数评估次数（NFEs）时提高生成质量。这种方法的关键是得分函数重参数化，它通过减少在每个积分步骤中使用固定得分函数估计而引起的积分误差。原始作者使用了用于噪声预测训练的模型默认的参数化方法，即将得分乘以条件正向噪声分布的标准差。然而，我们发现，尽管这种得分参数化的绝对平均值在大部分反向采样过程中接近常数，但在采样结束时它会迅速变化。为了简单修复这个问题，我们建议对得分进行重新参数化（在...

    Recently, zhang et al have proposed the Diffusion Exponential Integrator Sampler (DEIS) for fast generation of samples from Diffusion Models. It leverages the semi-linear nature of the probability flow ordinary differential equation (ODE) in order to greatly reduce integration error and improve generation quality at low numbers of function evaluations (NFEs). Key to this approach is the score function reparameterisation, which reduces the integration error incurred from using a fixed score function estimate over each integration step. The original authors use the default parameterisation used by models trained for noise prediction -- multiply the score by the standard deviation of the conditional forward noising distribution. We find that although the mean absolute value of this score parameterisation is close to constant for a large portion of the reverse sampling process, it changes rapidly at the end of sampling. As a simple fix, we propose to instead reparameterise the score (at in
    
[^3]: SERA：离线到在线强化学习中的样本高效奖励增强

    SERA:Sample Efficient Reward Augmentation in offline-to-online Reinforcement Learning. (arXiv:2310.19805v1 [cs.LG])

    [http://arxiv.org/abs/2310.19805](http://arxiv.org/abs/2310.19805)

    这篇论文提出了一种称为SERA的奖励增强框架，用于改善离线到在线强化学习中的探索能力。它通过设计内在奖励来鼓励agent进行探索，并实现更好的在线微调效果。

    

    离线强化学习的一个潜在应用是使用现有的静态数据集来初始化预训练策略，然后进行后续在线微调。然而，直接对离线预训练策略进行微调往往会导致次优性能。主要原因是离线保守方法降低了agent的探索能力，从而影响了在线微调的性能。为了增强在线微调过程中的探索能力，从而提高整体的在线微调性能，我们引入了一种称为样本高效奖励增强（SERA）的通用奖励增强框架。SERA旨在通过设计鼓励agent进行探索的内在奖励来改善在线微调的性能。具体来说，它隐式地实现了状态边缘匹配（SMM）并惩罚超出分布范围的状态行动，从而鼓励agent覆盖目标状态密度，并实现更好的在线微调结果。

    A prospective application of offline reinforcement learning (RL) involves initializing a pre-trained policy using existing static datasets for subsequent online fine-tuning. However, direct fine-tuning of the offline pre-trained policy often results in sub-optimal performance. A primary reason is that offline conservative methods diminish the agent's capability of exploration, thereby impacting online fine-tuning performance. To enhance exploration during online fine-tuning and thus enhance the overall online fine-tuning performance, we introduce a generalized reward augmentation framework called Sample Efficient Reward Augmentation (SERA). SERA aims to improve the performance of online fine-tuning by designing intrinsic rewards that encourage the agent to explore. Specifically, it implicitly implements State Marginal Matching (SMM) and penalizes out-of-distribution (OOD) state actions, thus encouraging agents to cover the target state density, and achieving better online fine-tuning r
    
[^4]: 《一种改进的面向Oracle高效的对抗性上下文赌博问题的放松方法》

    An Improved Relaxation for Oracle-Efficient Adversarial Contextual Bandits. (arXiv:2310.19025v1 [cs.LG])

    [http://arxiv.org/abs/2310.19025](http://arxiv.org/abs/2310.19025)

    该论文提出了一种用于对抗性上下文赌博问题的面向Oracle高效的放松方法，通过调用离线优化Oracle来降低遗憾界限，并且在界限方面取得了显著的改进，达到了先前最佳界限，并与原始界限相匹配。

    

    我们提出了一种面向Oracle高效的放松方法，用于处理对抗性上下文赌博问题，其中上下文是从已知分布中顺序独立抽取的，而成本序列则由在线对手选择。我们的算法具有一个$O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$的遗憾界限，并且每轮最多调用$O(K)$次离线优化Oracle，其中$K$表示动作的数量，$T$表示轮数，$\Pi$表示策略集。这是第一个改进Syrgkanis等人在NeurIPS 2016中获得的$O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$界限的结果，并且也是与Langford和Zhang在NeurIPS 2007中为随机情况提出的原始界限相匹配的结果。

    We present an oracle-efficient relaxation for the adversarial contextual bandits problem, where the contexts are sequentially drawn i.i.d from a known distribution and the cost sequence is chosen by an online adversary. Our algorithm has a regret bound of $O(T^{\frac{2}{3}}(K\log(|\Pi|))^{\frac{1}{3}})$ and makes at most $O(K)$ calls per round to an offline optimization oracle, where $K$ denotes the number of actions, $T$ denotes the number of rounds and $\Pi$ denotes the set of policies. This is the first result to improve the prior best bound of $O((TK)^{\frac{2}{3}}(\log(|\Pi|))^{\frac{1}{3}})$ as obtained by Syrgkanis et al. at NeurIPS 2016, and the first to match the original bound of Langford and Zhang at NeurIPS 2007 which was obtained for the stochastic case.
    
[^5]: 通过潜在引导扩散和嵌套集成改进医学图像分类的鲁棒性和可靠性

    Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15952](http://arxiv.org/abs/2310.15952)

    本文引入了一种新颖的三阶段方法，通过变换器和条件扩散模型来改善医学图像分类模型对实际应用中常见成像变异性的鲁棒性。

    

    尽管深度学习模型在各种医学图像分析任务中取得了显著的成功，但在真实临床环境中部署这些模型需要它们对所获取的图像的变异性具有鲁棒性。许多方法会对训练数据应用预定义的转换，以增强测试时的鲁棒性，但这些转换可能无法确保模型对患者图像中的多样性变异性具有鲁棒性。在本文中，我们提出了一种基于变换器和条件扩散模型的新型三阶段方法，旨在提高模型对实践中常见的成像变异性的鲁棒性，而无需预先确定的数据增强策略。为了实现这一目标，多个图像编码器首先学习分层特征表示来构建辨别潜在空间。接下来，一个由潜在代码引导的逆扩散过程作用于有信息先验，并提出预测候选。

    While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
    
[^6]: 会话式金融信息检索模型（ConFIRM）

    Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])

    [http://arxiv.org/abs/2310.13001](http://arxiv.org/abs/2310.13001)

    ConFIRM是一种会话式金融信息检索模型，通过合成金融领域特定问答对和评估参数微调方法，实现了超过90%的准确性，为金融对话系统提供了数据高效的解决方案。

    

    随着大型语言模型（LLM）的指数级增长，利用它们在金融等专门领域的新兴特性具有探索的价值。然而，金融等受监管领域具有独特的约束条件，需要具备针对该领域的优化框架。我们提出了ConFIRM，一种基于LLM的会话式金融信息检索模型，用于查询意图分类和知识库标记。ConFIRM包括两个模块：1）一种合成金融领域特定问答对的方法，以及2）评估参数高效的微调方法来进行查询分类任务。我们生成了一个包含4000多个样本的数据集，并在单独的测试集上评估了准确性。ConFIRM实现了超过90%的准确性，这对于符合监管要求至关重要。ConFIRM提供了一种数据高效的解决方案，用于提取金融对话系统的精确查询意图。

    With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
    
[^7]: PRIOR: 个性化先验用于重新激活联邦学习中被忽视的信息

    PRIOR: Personalized Prior for Reactivating the Information Overlooked in Federated Learning. (arXiv:2310.09183v1 [cs.LG])

    [http://arxiv.org/abs/2310.09183](http://arxiv.org/abs/2310.09183)

    本研究提出了一种名为PRIOR的个性化先验方案，用于解决个性化联邦学习中由于全局模型忽视特定信息而导致的不完整信息问题。该方案使用带Bregman散度的PFL框架将个性化先验与本地目标函数解耦，以提高适应性和性能。

    

    传统的联邦学习（FL）通过在保护隐私的前提下训练机器学习模型，而异质数据特性降低了局部模型的性能。个性化联邦学习（PFL）通过从全局模型中合成个性化模型来解决这个问题，在本地数据上进行训练。然而，这样的全局模型可能忽视了客户端被采样的特定信息。本文提出了一种新的方案，通过将个性化先验知识注入到每个客户端的全局模型中，试图减轻PFL中引入的不完整信息问题。我们提出的方法的核心是一种框架，即带Bregman散度（pFedBreD）的PFL，通过在个性化场景中使用Bregman散度正则化的本地目标函数，使个性化先验与之解耦，具有更强的适应性。我们还放松了镜像下降（RMD），以显式地提取先验知识，提供可选择的策略。

    Classical federated learning (FL) enables training machine learning models without sharing data for privacy preservation, but heterogeneous data characteristic degrades the performance of the localized model. Personalized FL (PFL) addresses this by synthesizing personalized models from a global model via training on local data. Such a global model may overlook the specific information that the clients have been sampled. In this paper, we propose a novel scheme to inject personalized prior knowledge into the global model in each client, which attempts to mitigate the introduced incomplete information problem in PFL. At the heart of our proposed approach is a framework, the PFL with Bregman Divergence (pFedBreD), decoupling the personalized prior from the local objective function regularized by Bregman divergence for greater adaptability in personalized scenarios. We also relax the mirror descent (RMD) to extract the prior explicitly to provide optional strategies. Additionally, our pFed
    
[^8]: 用控制变量稳定Shapley值的估计

    Stabilizing Estimates of Shapley Values with Control Variates. (arXiv:2310.07672v1 [stat.ML])

    [http://arxiv.org/abs/2310.07672](http://arxiv.org/abs/2310.07672)

    使用控制变量的方法稳定Shapley值的估计，减少了模型解释的不确定性，适用于任何机器学习模型。

    

    Shapley值是解释黑盒机器学习模型预测最流行的工具之一。然而，它们的计算成本很高，因此采用抽样近似来减少不确定性。为了稳定这些模型解释，我们提出了一种基于控制变量的蒙特卡洛技术的方法，称为ControlSHAP。我们的方法适用于任何机器学习模型，并且几乎不需要额外的计算或建模工作。在多个高维数据集上，我们发现它可以显著减少Shapley估计的蒙特卡洛变异性。

    Shapley values are among the most popular tools for explaining predictions of blackbox machine learning models. However, their high computational cost motivates the use of sampling approximations, inducing a considerable degree of uncertainty. To stabilize these model explanations, we propose ControlSHAP, an approach based on the Monte Carlo technique of control variates. Our methodology is applicable to any machine learning model and requires virtually no extra computation or modeling effort. On several high-dimensional datasets, we find it can produce dramatic reductions in the Monte Carlo variability of Shapley estimates.
    
[^9]: 直接逻辑属性的对抗性样本：gelu-4l中的内存管理

    An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])

    [http://arxiv.org/abs/2310.07325](http://arxiv.org/abs/2310.07325)

    在gelu-4l中，我们提供了证据表明内存管理对于transformer模型至关重要，并说明了Direct Logit Attribution技术的不准确之处。

    

    我们提供了一个4层transformer中内存管理的具体证据。具体来说，我们发现在前向传播过程中，模型组件一致地移除前面组件的输出，这是一种清理行为。我们的研究结果表明，解释性技术Direct Logit Attribution提供了误导性结果。我们展示了明确的例子，证明这种技术是不准确的，因为它没有考虑到清理行为。

    We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
    
[^10]: 使用PAC-Bayesian理论给变分自动编码器提供统计保证

    Statistical Guarantees for Variational Autoencoders using PAC-Bayesian Theory. (arXiv:2310.04935v1 [cs.LG])

    [http://arxiv.org/abs/2310.04935](http://arxiv.org/abs/2310.04935)

    这项工作利用PAC-Bayesian理论为变分自动编码器提供了统计保证，包括对后验分布、重构损失和输入与生成分布之间距离的上界。

    

    自从它们的问世以来，变分自动编码器（VAEs）在机器学习中变得非常重要。尽管它们被广泛使用，关于它们的理论性质仍存在许多问题。本文利用PAC-Bayesian理论为VAEs提供统计保证。首先，我们推导出了基于独立样本的后验分布的首个PAC-Bayesian界限。然后，利用这一结果为VAE的重构损失提供了泛化保证，同时提供了输入分布与VAE生成模型定义的分布之间距离的上界。更重要的是，我们提供了输入分布与VAE生成模型定义的分布之间Wasserstein距离的上界。

    Since their inception, Variational Autoencoders (VAEs) have become central in machine learning. Despite their widespread use, numerous questions regarding their theoretical properties remain open. Using PAC-Bayesian theory, this work develops statistical guarantees for VAEs. First, we derive the first PAC-Bayesian bound for posterior distributions conditioned on individual samples from the data-generating distribution. Then, we utilize this result to develop generalization guarantees for the VAE's reconstruction loss, as well as upper bounds on the distance between the input and the regenerated distributions. More importantly, we provide upper bounds on the Wasserstein distance between the input distribution and the distribution defined by the VAE's generative model.
    
[^11]: 基于注意力的多任务学习用于碱基编辑结果预测

    Attention-based Multi-task Learning for Base Editor Outcome Prediction. (arXiv:2310.02919v1 [cs.LG])

    [http://arxiv.org/abs/2310.02919](http://arxiv.org/abs/2310.02919)

    基于注意力的多任务学习模型可以加速基因编辑设计过程，并提高编辑结果预测的准确性。

    

    人类遗传疾病通常由点突变引起，这凸显了对精确基因组编辑技术的关键需求。其中，碱基编辑以其能够在单个核苷酸水平上进行定向改变而脱颖而出。然而，其临床应用受到编辑效率低和非预期突变的限制，需要在实验室进行大量的试错实验。为了加速这个过程，我们提出了一个基于注意力的两阶段机器学习模型，该模型学习预测给定基因组目标序列的所有可能编辑结果的可能性。我们进一步提出了一个多任务学习模式，同时学习多种碱基编辑器（即变体）。我们模型的预测结果在多个数据集和碱基编辑器变体上始终表现出与实际实验结果的强相关性。这些结果进一步验证了模型改进和加速基因编辑设计过程能力。

    Human genetic diseases often arise from point mutations, emphasizing the critical need for precise genome editing techniques. Among these, base editing stands out as it allows targeted alterations at the single nucleotide level. However, its clinical application is hindered by low editing efficiency and unintended mutations, necessitating extensive trial-and-error experimentation in the laboratory. To speed up this process, we present an attention-based two-stage machine learning model that learns to predict the likelihood of all possible editing outcomes for a given genomic target sequence. We further propose a multi-task learning schema to jointly learn multiple base editors (i.e. variants) at once. Our model's predictions consistently demonstrated a strong correlation with the actual experimental results on multiple datasets and base editor variants. These results provide further validation for the models' capacity to enhance and accelerate the process of refining base editing desig
    
[^12]: 参参数化凸支配法用于摊销优化中目标函数的逼近

    Parameterized Convex Minorant for Objective Function Approximation in Amortized Optimization. (arXiv:2310.02519v1 [cs.LG])

    [http://arxiv.org/abs/2310.02519](http://arxiv.org/abs/2310.02519)

    提出了一种参数化凸支配（PCM）方法，用于在摊销优化中逼近目标函数。该方法具有通用逼近性能，并可以通过单个凸优化获得全局最小值。

    

    提出了一种参数化凸支配（PCM）方法，用于在摊销优化中逼近目标函数。在提出的方法中，目标函数逼近器由PCM和非负间隙函数之和表示，其中目标函数逼近器在优化变量上由PCM凸函数下界约束。所提出的目标函数逼近器是连续函数的通用逼近器，PCM的全局最小值达到目标函数逼近器的全局最小值。因此，可以通过单个凸优化获取目标函数逼近器的全局最小值。作为所提方法的实现，提出了扩展的参数化对数和指数网络，利用参数化的对数和指数网络作为PCM。对于非参数化凸目标函数逼近和基于学习的非线性模型预测控制进行了数值模拟。

    Parameterized convex minorant (PCM) method is proposed for the approximation of the objective function in amortized optimization. In the proposed method, the objective function approximator is expressed by the sum of a PCM and a nonnegative gap function, where the objective function approximator is bounded from below by the PCM convex in the optimization variable. The proposed objective function approximator is a universal approximator for continuous functions, and the global minimizer of the PCM attains the global minimum of the objective function approximator. Therefore, the global minimizer of the objective function approximator can be obtained by a single convex optimization. As a realization of the proposed method, extended parameterized log-sum-exp network is proposed by utilizing a parameterized log-sum-exp network as the PCM. Numerical simulation is performed for non-parameterized-convex objective function approximation and for learning-based nonlinear model predictive control 
    
[^13]: 充分测试时间适应的技巧

    Bag of Tricks for Fully Test-Time Adaptation. (arXiv:2310.02416v1 [cs.LG])

    [http://arxiv.org/abs/2310.02416](http://arxiv.org/abs/2310.02416)

    本研究提出了一种充分测试时间适应（TTA）的技巧，包括小批量归一化、流平衡、可靠样本选择和网络置信度校准。通过对这些技术的详细分析，我们揭示了它们在准确性、计算能力和模型复杂性之间的权衡，并取得了新的最先进的结果。

    

    充分测试时间适应（TTA）旨在适应数据漂移，最近引起了广泛关注。已经提出了许多技巧和技术，以确保在任意未标记数据流上进行稳健学习。然而，评估每个个体技术的真正影响并进行公平比较仍然是一个重大挑战。为了帮助巩固社区知识，我们提出了对所选正交TTA技术的分类，包括小批量归一化、流平衡、可靠样本选择和网络置信度校准。我们详细分析了每种方法在不同感兴趣的场景下的影响。通过我们的分析，我们揭示了这些技术在准确性、所需计算能力和模型复杂性之间产生的权衡。我们还发现了结合技术时产生的协同效应，并能够建立新的最先进结果。

    Fully Test-Time Adaptation (TTA), which aims at adapting models to data drifts, has recently attracted wide interest. Numerous tricks and techniques have been proposed to ensure robust learning on arbitrary streams of unlabeled data. However, assessing the true impact of each individual technique and obtaining a fair comparison still constitutes a significant challenge. To help consolidate the community's knowledge, we present a categorization of selected orthogonal TTA techniques, including small batch normalization, stream rebalancing, reliable sample selection, and network confidence calibration. We meticulously dissect the effect of each approach on different scenarios of interest. Through our analysis, we shed light on trade-offs induced by those techniques between accuracy, the computational power required, and model complexity. We also uncover the synergy that arises when combining techniques and are able to establish new state-of-the-art results.
    
[^14]: CausalImages: 一个用于地球观测、生物医学和社会科学图像的因果推断的 R 包

    CausalImages: An R Package for Causal Inference with Earth Observation, Bio-medical, and Social Science Images. (arXiv:2310.00233v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.00233](http://arxiv.org/abs/2310.00233)

    causalimages R 包提供了使用图像进行因果推断的新工具，并能够整合卫星和生物医学图像等新数据源，通过分解治疗效果的异质性和控制混淆变量来进行分析。

    

    causalimages R 包能够通过图像和图像序列数据进行因果推断，为将卫星和生物医学图像等新数据源整合到因果关系研究中提供新工具。一组函数可用于基于图像的因果推断分析。例如，一种关键函数利用可解释的贝叶斯框架通过图像来分解治疗效果的异质性。这样可以确定哪种类型的图像或图像序列对干预最敏感。第二个建模函数允许研究人员使用图像来控制混淆变量。该包还允许研究人员生成作为图像或视频内容向量摘要的嵌入。最后，还提供了基础结构函数，例如用于将大规模图像和图像序列数据写入序列化字节字符串以进行更快速的图像分析的工具。因此，causalimages 在 R 中开启了因果推断的新能力，让研究

    The causalimages R package enables causal inference with image and image sequence data, providing new tools for integrating novel data sources like satellite and bio-medical imagery into the study of cause and effect. One set of functions enables image-based causal inference analyses. For example, one key function decomposes treatment effect heterogeneity by images using an interpretable Bayesian framework. This allows for determining which types of images or image sequences are most responsive to interventions. A second modeling function allows researchers to control for confounding using images. The package also allows investigators to produce embeddings that serve as vector summaries of the image or video content. Finally, infrastructural functions are also provided, such as tools for writing large-scale image and image sequence data as sequentialized byte strings for more rapid image analysis. causalimages therefore opens new capabilities for causal inference in R, letting research
    
[^15]: Outage-Watch: 使用极端事件正则化提早预测服务中断

    Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer. (arXiv:2309.17340v1 [cs.DC])

    [http://arxiv.org/abs/2309.17340](http://arxiv.org/abs/2309.17340)

    Outage-Watch使用极端事件正则化的方法提早预测云服务中断，通过捕获质量指标的恶化情况，改善灵活性并提高尾部分布的学习能力。

    

    云服务无处不在，关键云服务的故障是不可避免的事实。为了保留客户并防止收入损失，提供高可靠性保证对于这些服务来说是非常重要的。预测服务中断是一种减轻严重程度和恢复时间的方法。由于这些事件的罕见性，预测关键性故障是困难的。此外，关键故障在可观测数据方面定义模糊。我们提出的方法Outage-Watch将关键的服务中断定义为一组指标所捕获的服务质量（QoS）的恶化情况。Outage-Watch通过使用当前系统状态来预测QoS指标是否会超过阈值并引发极端事件，提前检测此类中断。使用高斯混合模型来模拟QoS指标的分布以提高灵活性，而极端事件正则化有助于提高尾部分布的学习能力。

    Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is 
    
[^16]: 神经启发的分层多模态学习

    Neuro-Inspired Hierarchical Multimodal Learning. (arXiv:2309.15877v1 [cs.LG])

    [http://arxiv.org/abs/2309.15877](http://arxiv.org/abs/2309.15877)

    这项研究提出了一种神经启发的分层多模态学习方法，利用信息瓶颈理论构建了一种有效且紧凑的信息流，实现了对真实世界的全面和准确的感知。

    

    整合和处理来自多种信息源或模态对于获得对真实世界的全面和准确的感知至关重要。受到神经科学的启发，我们开发了信息论分层感知(ITHP)模型，该模型利用了信息瓶颈的概念。与大多数旨在将所有模态纳入输入的传统融合模型不同，我们的模型将主要模态指定为输入，而其余模态则作为信息路径中的检测器。我们提出的感知模型的重点是通过在潜在状态和输入模态状态之间最小化相互信息并在潜在状态和其余模态之间最大化相互信息的平衡，构建一种有效且紧凑的信息流。这种方法导致了保留相关信息并最小化冗余的紧凑潜在状态表示，从而实现更好的感知。

    Integrating and processing information from various sources or modalities are critical for obtaining a comprehensive and accurate perception of the real world. Drawing inspiration from neuroscience, we develop the Information-Theoretic Hierarchical Perception (ITHP) model, which utilizes the concept of information bottleneck. Distinct from most traditional fusion models that aim to incorporate all modalities as input, our model designates the prime modality as input, while the remaining modalities act as detectors in the information pathway. Our proposed perception model focuses on constructing an effective and compact information flow by achieving a balance between the minimization of mutual information between the latent state and the input modal state, and the maximization of mutual information between the latent states and the remaining modal states. This approach leads to compact latent state representations that retain relevant information while minimizing redundancy, thereby sub
    
[^17]: 使用深度学习和药动学先验预测治疗反应

    Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])

    [http://arxiv.org/abs/2309.13135](http://arxiv.org/abs/2309.13135)

    该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。

    

    对医疗时间序列的预测对于早期检测不良结果和患者监测至关重要。然而，由于数据嘈杂和间歇性，实际中预测可能很困难。这些挑战通常通过外部因素诱导的变化点（如药物使用）而加剧。我们提出了一种新颖的编码器，以向深度学习模型提供药物的药动学效应信息，从而实现对受治疗影响的时间序列的准确预测。我们展示了我们方法在使用逼真模拟和真实世界数据预测血糖的任务中的有效性。我们的药动学编码器使深度学习模型在模拟数据上超过基准约11％，在真实世界数据上超过8％。所提出的方法可以在临床实践中具有多种有益应用，例如发出关于意外治疗反应的早期警告，或帮助表征特定于患者的治疗效果。

    Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
    
[^18]: State2Explanation:基于概念的解释：有利于Agent学习和用户理解

    State2Explanation: Concept-Based Explanations to Benefit Agent Learning and User Understanding. (arXiv:2309.12482v1 [cs.LG])

    [http://arxiv.org/abs/2309.12482](http://arxiv.org/abs/2309.12482)

    本论文致力于开发一种基于概念的解释方法，旨在提高非AI专家对AI决策的理解。通过定义顺序决策设置中的“概念”以及探索基于概念的解释对RL agent学习效果和最终用户对agent决策理解的双重好处，我们提出了一个统一的框架。

    

    随着非AI专家使用更复杂的AI系统来完成日常任务，人们越来越努力开发能够为非AI专家理解的AI决策提供解释的方法。为了实现这个目标，利用高级概念并生成基于概念的解释已经成为一种流行的方法。大多数基于概念的解释都是为分类技术而开发的，我们认为目前关于顺序决策的方法还存在一定限制。在这项工作中，我们首先提出了在顺序决策设置中定义“概念”的愿望。受到“Protege效应”的启发，该效应说明解释知识通常会增强个体的自主学习能力，我们探索了基于概念的解释对RL agent的学习效果和最终用户对agent决策理解的双重好处。为此，我们提出了一个统一的框架，St

    With more complex AI systems used by non-AI experts to complete daily tasks, there is an increasing effort to develop methods that produce explanations of AI decision making understandable by non-AI experts. Towards this effort, leveraging higher-level concepts and producing concept-based explanations have become a popular method. Most concept-based explanations have been developed for classification techniques, and we posit that the few existing methods for sequential decision making are limited in scope. In this work, we first contribute a desiderata for defining "concepts" in sequential decision making settings. Additionally, inspired by the Protege Effect which states explaining knowledge often reinforces one's self-learning, we explore the utility of concept-based explanations providing a dual benefit to the RL agent by improving agent learning rate, and to the end-user by improving end-user understanding of agent decision making. To this end, we contribute a unified framework, St
    
[^19]: 超越预期：随机动力系统的剩余动态模式分解和方差

    Beyond expectations: Residual Dynamic Mode Decomposition and Variance for Stochastic Dynamical Systems. (arXiv:2308.10697v2 [math.DS] UPDATED)

    [http://arxiv.org/abs/2308.10697](http://arxiv.org/abs/2308.10697)

    本文研究了随机动力系统中的动态模式分解(DMD)方法。通过在Koopman框架中引入方差来解决挑战，包括虚假模式和本征谱。结果表明，需要超越预期来有效处理随机系统。

    

    Koopman算符将非线性动力系统线性化，使得其谱信息至关重要。已经开发了许多算法来近似这些谱特性，其中动态模式分解(DMD)是基于投影方法的典型代表。尽管Koopman算符本身是线性的，但它在无穷维的可观测空间中起作用，这带来了挑战，包括虚假模式，本征谱和Koopman模式分解的验证。虽然最近的研究已经解决了确定性系统的这些挑战，但对于随机系统的已验证的DMD方法仍存在明显的差距，其中Koopman算符测量可观测量的期望值。我们表明，需要超越预期来解决这些问题。通过将方差纳入Koopman框架，我们解决了这些挑战。通过额外的类似DMD的矩阵，我们近似一个残差平方和的和以及方差。

    Koopman operators linearize nonlinear dynamical systems, making their spectral information of crucial interest. Numerous algorithms have been developed to approximate these spectral properties, and Dynamic Mode Decomposition (DMD) stands out as the poster child of projection-based methods. Although the Koopman operator itself is linear, the fact that it acts in an infinite-dimensional space of observables poses challenges. These include spurious modes, essential spectra, and the verification of Koopman mode decompositions. While recent work has addressed these challenges for deterministic systems, there remains a notable gap in verified DMD methods for stochastic systems, where the Koopman operator measures the expectation of observables. We show that it is necessary to go beyond expectations to address these issues. By incorporating variance into the Koopman framework, we address these challenges. Through an additional DMD-type matrix, we approximate the sum of a squared residual and 
    
[^20]: 使用分布感知的自适应优先级附加kNN图

    Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])

    [http://arxiv.org/abs/2308.02442](http://arxiv.org/abs/2308.02442)

    本文提出了一种名为paNNG的算法，它结合了自适应kNN和基于分布的图构建。通过包含分布信息，paNNG能够有效提升模糊样本的性能，并实现更好的准确性和泛化能力。

    

    基于图的kNN算法因其简单性和有效性在机器学习任务中广受欢迎。然而，传统的kNN图对于k值的固定依赖可能会影响其性能，特别是在涉及复杂数据分布的情况下。此外，与其他分类模型类似，决策边界上存在的模糊样本常常是一个挑战，因为它们更容易被错误分类。为了解决这些问题，我们提出了优先级附加k-最近邻图（paNNG），它将自适应的kNN与基于分布的图构建相结合。通过结合分布信息，paNNG可以显著提高模糊样本的性能，通过“拉”它们回到原始类别，从而实现改进的整体准确性和泛化能力。通过在多样化的基准数据集上进行严格评估，paNNG的性能超越了现有算法，展示了它的优越性。

    Graph-based kNN algorithms have garnered widespread popularity for machine learning tasks, due to their simplicity and effectiveness. However, the conventional kNN graph's reliance on a fixed value of k can hinder its performance, especially in scenarios involving complex data distributions. Moreover, like other classification models, the presence of ambiguous samples along decision boundaries often presents a challenge, as they are more prone to incorrect classification. To address these issues, we propose the Preferential Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with distribution-based graph construction. By incorporating distribution information, paNNG can significantly improve performance for ambiguous samples by "pulling" them towards their original classes and hence enable enhanced overall accuracy and generalization capability. Through rigorous evaluations on diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms, showcasing its 
    
[^21]: 预训练的深度模型在标签稀缺的Learning-To-Rank中胜过GBDTs

    Pretrained deep models outperform GBDTs in Learning-To-Rank under label scarcity. (arXiv:2308.00177v1 [cs.LG])

    [http://arxiv.org/abs/2308.00177](http://arxiv.org/abs/2308.00177)

    本研究研究了在标签稀缺的Learning-To-Rank问题中，无监督预训练的深度模型是否能胜过GBDTs和其他非预训练模型。实验结果表明，通过使用SimCLR-Rank方法进行无监督预训练，我们的深度学习模型在大量无标签数据和有限标签数据的情况下取得了显著优势。

    

    尽管深度学习模型在文本和图像领域是最先进的，但它们在表格形式的Learning-To-Rank问题上尚未一致地胜过梯度提升决策树(GBDTs)。近期在文本和图像任务上深度学习模型取得的性能提升主要依赖于无监督预训练，这种方法利用了比有标签数据多几个数量级的无标签数据。据我们所知，无监督预训练还未应用于Learning-To-Rank问题，而该问题通常产生大量无标签数据。本研究探究了无监督预训练是否能提高LTR性能，与GBDTs和其他非预训练模型相比。通过使用简单的设计选择(包括SimCLR-Rank，这是我们针对排名问题修改的SimCLR方法)，我们产生了预训练的深度学习模型，在有大量无标签数据且有限标签数据的情况下，显著优于GBDTs(和其他非预训练模型)。

    While deep learning (DL) models are state-of-the-art in text and image domains, they have not yet consistently outperformed Gradient Boosted Decision Trees (GBDTs) on tabular Learning-To-Rank (LTR) problems. Most of the recent performance gains attained by DL models in text and image tasks have used unsupervised pretraining, which exploits orders of magnitude more unlabeled data than labeled data. To the best of our knowledge, unsupervised pretraining has not been applied to the LTR problem, which often produces vast amounts of unlabeled data. In this work, we study whether unsupervised pretraining can improve LTR performance over GBDTs and other non-pretrained models. Using simple design choices--including SimCLR-Rank, our ranking-specific modification of SimCLR (an unsupervised pretraining method for images)--we produce pretrained deep learning models that soundly outperform GBDTs (and other non-pretrained models) in the case where labeled data is vastly outnumbered by unlabeled data
    
[^22]: 高效的半监督异构参与者联邦学习

    Efficient Semi-Supervised Federated Learning for Heterogeneous Participants. (arXiv:2307.15870v1 [cs.LG])

    [http://arxiv.org/abs/2307.15870](http://arxiv.org/abs/2307.15870)

    本论文提出了一种高效的半监督异构参与者联邦学习系统，通过引入聚类正则化来改进模型在数据非独立同分布情况下的性能，并对模型收敛性进行了理论和实验研究。

    

    联邦学习（FL）允许多个客户端在私有数据上协同训练机器学习模型，但在资源有限的环境中训练和部署大型模型用于广泛应用是具有挑战性的。幸运的是，分离式联邦学习（SFL）通过减轻客户端的计算和通信负担提供了优秀的解决方案。SFL通常假设客户端具有标记的数据进行本地训练，然而在实践中并非总是如此。以前的研究采用半监督技术来利用FL中的无标记数据，但数据的非独立同分布性提出了确保训练效率的另一个挑战。在这里，我们提出了一种新颖的系统Pseudo-Clustering Semi-SFL，用于在标记数据位于服务器上的情境下训练模型。通过引入聚类正则化，可以提高数据非独立同分布情况下的模型性能。此外，我们对模型收敛性进行了理论和实验研究，发现了...

    Federated Learning (FL) has emerged to allow multiple clients to collaboratively train machine learning models on their private data. However, training and deploying large models for broader applications is challenging in resource-constrained environments. Fortunately, Split Federated Learning (SFL) offers an excellent solution by alleviating the computation and communication burden on the clients SFL often assumes labeled data for local training on clients, however, it is not the case in practice.Prior works have adopted semi-supervised techniques for leveraging unlabeled data in FL, but data non-IIDness poses another challenge to ensure training efficiency. Herein, we propose Pseudo-Clustering Semi-SFL, a novel system for training models in scenarios where labeled data reside on the server. By introducing Clustering Regularization, model performance under data non-IIDness can be improved. Besides, our theoretical and experimental investigations into model convergence reveal that the 
    
[^23]: Wasserstein自编码合并树（和持续图）

    Wasserstein Auto-Encoders of Merge Trees (and Persistence Diagrams). (arXiv:2307.02509v1 [cs.LG])

    [http://arxiv.org/abs/2307.02509](http://arxiv.org/abs/2307.02509)

    本文提出了一种计算框架，用于将传统的自编码神经网络扩展到合并树的Wasserstein度量空间。算法在准确性和可解释性方面表现出

    

    本文提出了一种计算框架，用于合并树的Wasserstein自编码(MT-WAE)，这是将传统的自编码神经网络架构扩展到合并树的Wasserstein度量空间的一种新颖方法。与操作向量化数据的传统自编码器不同，我们的公式在网络的每一层明确地操作合并树的关联度量空间，从而获得更高的准确性和可解释性。我们的新颖神经网络方法可以解释为前期线性尝试[65]的非线性推广。它也很容易扩展到持续图。对公共连锁反应的广泛实验表明我们算法的效率，MT-WAE的计算平均时间仅为几分钟。我们将我们的贡献的实用性通过两个应用来展示，这些应用是基于以前关于合并树编码的工作[65]进行调整得到的。首先，我们将MT-WAE应用于数据压缩，并可靠地压缩合并树。

    This paper presents a computational framework for the Wasserstein auto-encoding of merge trees (MT-WAE), a novel extension of the classical auto-encoder neural network architecture to the Wasserstein metric space of merge trees. In contrast to traditional auto-encoders which operate on vectorized data, our formulation explicitly manipulates merge trees on their associated metric space at each layer of the network, resulting in superior accuracy and interpretability. Our novel neural network approach can be interpreted as a non-linear generalization of previous linear attempts [65] at merge tree encoding. It also trivially extends to persistence diagrams. Extensive experiments on public ensembles demonstrate the efficiency of our algorithms, with MT-WAE computations in the orders of minutes on average. We show the utility of our contributions in two applications adapted from previous work on merge tree encoding [65]. First, we apply MT-WAE to data reduction and reliably compress merge t
    
[^24]: 用基于梯度的优化方法解决核岭回归问题

    Solving Kernel Ridge Regression with Gradient-Based Optimization Methods. (arXiv:2306.16838v1 [stat.ML])

    [http://arxiv.org/abs/2306.16838](http://arxiv.org/abs/2306.16838)

    本研究提出了一种新的方法来解决核岭回归问题，通过等价的目标函数形式和基于梯度的优化方法，我们不仅可以使用其他惩罚方法，还能够从梯度下降的角度研究核岭回归。通过提前停止的正则化，我们推导出了一个闭合解，即核梯度流（KGF），并证明了KGF和KRR之间的差异。我们还将KRR泛化，使用$\ell_1$和$\ell_\infty$惩罚方法，并发现使用这些方法得到的解与前向分步回归和符号梯度下降结合提前停止得到的解非常相似。因此，我们减少了计算复杂度重的近端梯度下降算法的需求。

    

    核岭回归（KRR）是线性岭回归的非线性推广。在这里，我们引入了KRR目标函数的等价形式，为使用其他惩罚方法和从梯度下降的角度研究核岭回归打开了可能。通过连续时间的视角，我们推导出了一个闭合解——核梯度流（KGF），通过提前停止的正则化，让我们能够在KGF和KRR之间理论上界定差异。我们用$\ell_1$和$\ell_\infty$惩罚方法将KRR泛化，并利用类似KGF和KRR之间的相似性，使用这些惩罚方法得到的解与使用前向分步回归（也称为坐标下降）和符号梯度下降结合提前停止得到的解非常相似。因此，减少了计算复杂度重的近端梯度下降算法的需求。

    Kernel ridge regression, KRR, is a non-linear generalization of linear ridge regression. Here, we introduce an equivalent formulation of the objective function of KRR, opening up both for using other penalties than the ridge penalty and for studying kernel ridge regression from the perspective of gradient descent. Using a continuous-time perspective, we derive a closed-form solution, kernel gradient flow, KGF, with regularization through early stopping, which allows us to theoretically bound the differences between KGF and KRR. We generalize KRR by replacing the ridge penalty with the $\ell_1$ and $\ell_\infty$ penalties and utilize the fact that analogously to the similarities between KGF and KRR, the solutions obtained when using these penalties are very similar to those obtained from forward stagewise regression (also known as coordinate descent) and sign gradient descent in combination with early stopping. Thus the need for computationally heavy proximal gradient descent algorithms
    
[^25]: StrainNet: 使用SE(3)-等变图神经网络预测晶体结构弹性性能

    StrainNet: Predicting crystal structure elastic properties using SE(3)-equivariant graph neural networks. (arXiv:2306.12818v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2306.12818](http://arxiv.org/abs/2306.12818)

    该论文介绍了一种使用SE(3)-等变图神经网络预测晶体结构弹性性能的方法，可以高效地产生准确的弹性模量，并且能够预测应变能密度和相关的弹性常数，具有一定的可拓展性。

    

    准确预测晶态固体的弹性性质对于计算材料科学至关重要。然而，传统的从头算方法在计算上非常耗时，特别是对于研究具有大量原子的复杂材料时如此。我们引入了一种新颖的数据驱动方法，使用SE(3)-等变图神经网络(GNN)高效预测晶体结构的弹性性质。该方法产生了与最近的数据驱动研究相当准确的重要标量弹性模量。重要的是，我们的对称感知GNN模型还能够预测应变能密度(SED)和相关弹性常数，这些是受晶体学群影响显著的基本张量量。该模型一致区分SED张量的独立元素，符合晶体结构的对称性。最后，我们的深度学习模型具有一定的可拓展性，可用于不同类型的晶体。

    Accurately predicting the elastic properties of crystalline solids is vital for computational materials science. However, traditional atomistic scale ab initio approaches are computationally intensive, especially for studying complex materials with a large number of atoms in a unit cell. We introduce a novel data-driven approach to efficiently predict the elastic properties of crystal structures using SE(3)-equivariant graph neural networks (GNNs). This approach yields important scalar elastic moduli with the accuracy comparable to recent data-driven studies. Importantly, our symmetry-aware GNNs model also enables the prediction of the strain energy density (SED) and the associated elastic constants, the fundamental tensorial quantities that are significantly influenced by a material's crystallographic group. The model consistently distinguishes independent elements of SED tensors, in accordance with the symmetry of the crystal structures. Finally, our deep learning model possesses mea
    
[^26]: 面向超参数化神经网络的高效不确定性量化和减少方法

    Efficient Uncertainty Quantification and Reduction for Over-Parameterized Neural Networks. (arXiv:2306.05674v1 [stat.ML])

    [http://arxiv.org/abs/2306.05674](http://arxiv.org/abs/2306.05674)

    本论文基于神经切向核理论，提出了一种高效的方法以减少超参数化神经网络中的过程不确定性，并只需要使用一种辅助网络就可以消除这种不确定性。

    

    不确定性量化（UQ）对于机器学习模型的可靠性评估和改进至关重要。在深度学习中，不确定性不仅来自数据，还来自训练过程中注入的大量噪声和偏差。这些噪声和偏差妨碍了统计保证的实现，并且由于需要重复的网络重新训练，对UQ提出了计算挑战。基于最近的神经切向核理论，我们创建了具有统计保证的方案，以通过非常低的计算量量化和减少超参数化神经网络的过程不确定性。特别地，我们的方法基于我们称为过程噪声校正（PNC）预测器，通过只使用一种适当标记数据集上训练的辅助网络来消除过程不确定性，而不是使用深层集成中的许多重新训练的网络。此外，通过将我们的PNC预测器与所提出的先验模型结合起来，我们可以显著减少所需的网络正则化。

    Uncertainty quantification (UQ) is important for reliability assessment and enhancement of machine learning models. In deep learning, uncertainties arise not only from data, but also from the training procedure that often injects substantial noises and biases. These hinder the attainment of statistical guarantees and, moreover, impose computational challenges on UQ due to the need for repeated network retraining. Building upon the recent neural tangent kernel theory, we create statistically guaranteed schemes to principally \emph{quantify}, and \emph{remove}, the procedural uncertainty of over-parameterized neural networks with very low computation effort. In particular, our approach, based on what we call a procedural-noise-correcting (PNC) predictor, removes the procedural uncertainty by using only \emph{one} auxiliary network that is trained on a suitably labeled data set, instead of many retrained networks employed in deep ensembles. Moreover, by combining our PNC predictor with su
    
[^27]: Transformers学习实现预处理梯度下降用于上下文学习

    Transformers learn to implement preconditioned gradient descent for in-context learning. (arXiv:2306.00297v1 [cs.LG])

    [http://arxiv.org/abs/2306.00297](http://arxiv.org/abs/2306.00297)

    本论文通过研究线性transformers在训练过程中的全局最小值，证明了对于一个注意力层，transformers能够实现一次预处理梯度下降，而对于一个$k$个注意力层的transformer，它可以实现多次预处理梯度下降。

    

    受到transformers在上下文学习方面的显著能力的驱动，一些研究表明transformers可以实现像梯度下降这样的算法。通过精心的权重构造，这些研究表明多层transformers具有足够的表达能力来模拟梯度下降迭代。超越表达能力的问题，我们问：transformers能否通过在随机问题实例上训练来学习实现这样的算法？据我们所知，通过对线性回归的随机实例进行训练，我们第一次在这个问题上取得了理论进展，通过对线性transformers的损失函数进行分析。对于一个注意力层，我们证明了训练目标的全局最小值实现了一次预处理梯度下降。值得注意的是，预处理矩阵不仅适应输入分布，而且还适应于数据不充分引起的方差。对于一个具有$k$个注意力层的transformer，我们证明了一定条件下它可以实现多次预处理梯度下降。

    Motivated by the striking ability of transformers for in-context learning, several works demonstrate that transformers can implement algorithms like gradient descent. By a careful construction of weights, these works show that multiple layers of transformers are expressive enough to simulate gradient descent iterations. Going beyond the question of expressivity, we ask: Can transformers learn to implement such algorithms by training over random problem instances? To our knowledge, we make the first theoretical progress toward this question via analysis of the loss landscape for linear transformers trained over random instances of linear regression. For a single attention layer, we prove the global minimum of the training objective implements a single iteration of preconditioned gradient descent. Notably, the preconditioning matrix not only adapts to the input distribution but also to the variance induced by data inadequacy. For a transformer with $k$ attention layers, we prove certain 
    
[^28]: 动态稀疏是通道级稀疏的学习者

    Dynamic Sparsity Is Channel-Level Sparsity Learner. (arXiv:2305.19454v1 [cs.LG])

    [http://arxiv.org/abs/2305.19454](http://arxiv.org/abs/2305.19454)

    本文提出了一种名为Channel-aware dynamic sparse (Chase)的方法，使用端到端训练实现了GPU友好的通道级别稀疏，不需要任何特殊操作，并且可以直接在通用硬件上加速，显著减小模型大小，同时保持性能。

    

    稀疏训练由于在整个训练过程和推理中具有诱人的节省能力而受到机器学习的广泛关注。动态稀疏训练(DST)作为一种领先的稀疏训练方法，可以从零开始训练深度神经网络，以达到与密集对应物性能相匹配的高稀疏性能。然而，大多数DST之前的研究都表明它们的有效性是在高度不规则的稀疏模式下的非结构化稀疏性上，这在常见硬件上得到了有限的支持。这种限制阻碍了DST在实践中的使用。在本文中，我们提出了一种名为通道感知动态稀疏（Chase）的方法，它将非结构化动态稀疏的性能转换为适合GPU友好的通道级别稀疏，在一个端到端的训练过程中实现，而不需要任何特殊的操作。所得到的小型稀疏网络可以直接通过通用硬件加速，而无需使用专用的稀疏硬件加速器。我们在各种数据集上的实验结果表明，Chase可以在深度神经网络上实现高通道级稀疏性，同时保持其性能，并显着减小模型的大小。

    Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using a
    
[^29]: 缺失值下的公平性干预措施的适应性研究

    Adapting Fairness Interventions to Missing Values. (arXiv:2305.19429v1 [cs.LG])

    [http://arxiv.org/abs/2305.19429](http://arxiv.org/abs/2305.19429)

    本文研究了如何在缺失值的情况下实现公平的分类。传统方法会加剧歧视。本文证明从插补数据训练分类器会恶化组公平性和平均准确性。作者提出可扩展和适应性的算法，可以与其他公平干预算法结合使用，以处理所有可能的缺失模式。

    

    真实世界中数据的缺失值对算法公平性提出了显著而独特的挑战。不同的族群可能不会同等地受到缺失数据的影响，而处理缺失值的标准程序，即先对数据进行插补，然后使用插补的数据进行分类，这个过程被称为“插补再分类”，会加剧歧视。本文分析了缺失值如何影响算法公平性。我们首先证明了从插补数据训练分类器会显著恶化可以实现的组公平性和平均准确性的值。这是因为插补数据会导致数据缺失模式的丢失，数据缺失模式通常会传达有关预测标签的信息。我们提出了可扩展和适应性的算法，用于处理缺失值的公平分类。这些算法可以与任何现有的公平干预算法结合使用，以处理所有可能的缺失模式，并保留信息。

    Missing values in real-world data pose a significant and unique challenge to algorithmic fairness. Different demographic groups may be unequally affected by missing data, and the standard procedure for handling missing values where first data is imputed, then the imputed data is used for classification -- a procedure referred to as "impute-then-classify" -- can exacerbate discrimination. In this paper, we analyze how missing values affect algorithmic fairness. We first prove that training a classifier from imputed data can significantly worsen the achievable values of group fairness and average accuracy. This is because imputing data results in the loss of the missing pattern of the data, which often conveys information about the predictive label. We present scalable and adaptive algorithms for fair classification with missing values. These algorithms can be combined with any preexisting fairness-intervention algorithm to handle all possible missing patterns while preserving informatio
    
[^30]: 《Zero-TPrune: 基于预训练Transformers关注图的零射击令牌剪枝方法》

    Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])

    [http://arxiv.org/abs/2305.17328](http://arxiv.org/abs/2305.17328)

    《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。

    

    最近在边缘设备上部署Transformer模型变得越来越具有挑战性，原因是模型的体积呈指数级增长，而推理成本则随输入序列中令牌数量的平方提高。令牌剪枝是解决这一挑战的新兴解决方法之一，由于其易于在各种Transformer支持的模型上部署。然而，大多数令牌剪枝方法需要在剪枝后或期间进行计算密集型的微调过程，在许多情况下这是不可取的。最近的一些研究探讨了没有微调的即插即用的预训练Transformer的剪枝方法。但是，它们只考虑了令牌的重要性。在这项工作中，我们提出了Zero-TPrune，这是一种零射击方法，它既考虑令牌的重要性又考虑相似性来执行令牌剪枝。Zero-TPrune利用预训练Transformer模型的注意图来为令牌生成一个重要性排名并移除信息较少的令牌。注意矩阵可用于推断即插即用的模型。

    Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
    
[^31]: 基于语法约束的语言模型灵活解码技术

    Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])

    [http://arxiv.org/abs/2305.13971](http://arxiv.org/abs/2305.13971)

    本文提出了一种使用形式语法约束丰富解码步骤的方法，有效生成符合特定语法的复杂输出结构，同时允许任何上下文无关语法集成。实验证明该方法在四个信息提取任务上实现了最先进的性能表现。

    

    LLM在许多任务中展现出了惊人的少量样本表现，但在生成信息提取所需的复杂输出结构时仍存在困难。这个限制源于LLM在没有微调的情况下倾向于生成自由文本而不是遵循特定语法的精确结构。在本文中，我们提出在解码步骤中使用形式语法约束来丰富模型。在搜索过程中，只有符合语法产生规则的有效令牌能被考虑到。这样就强制只产生有效的序列。我们的框架非常通用和灵活，允许任何上下文无关语法(CFG)集成到我们的自定义约束beam搜索实现中。我们展示了许多NLP任务的输出可以被表示为形式语言，使它们适合在我们的框架中直接使用。对于输出空间取决于输入的任务，我们提出了基于输入的CFG，根据特定于输入的特征更新产生规则。实验证明了我们的方法在生成复杂输出结构方面的有效性，并在四个信息提取任务上实现了最先进的性能。

    LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
    
[^32]: 深度量子神经网络对应高斯过程

    Deep quantum neural networks form Gaussian processes. (arXiv:2305.09957v1 [quant-ph])

    [http://arxiv.org/abs/2305.09957](http://arxiv.org/abs/2305.09957)

    本文证明了基于Haar随机酉或正交深量子神经网络的某些模型的输出会收敛于高斯过程。然而，这种高斯过程不能用于通过贝叶斯统计学来有效预测QNN的输出。

    

    众所周知，从独立同分布的先验条件开始初始化的人工神经网络在隐藏层神经元数目足够大的极限下收敛到高斯过程。本文证明了量子神经网络（QNNs）也存在类似的结果。特别地，我们证明了基于Haar随机酉或正交深QNNs的某些模型的输出在希尔伯特空间维度$d$足够大时会收敛于高斯过程。由于输入状态、测量的可观测量以及酉矩阵的元素不独立等因素的作用，本文对这一结果的推导比经典情形更加微妙。我们分析的一个重要后果是，这个结果得到的高斯过程不能通过贝叶斯统计学来有效地预测QNN的输出。此外，我们的定理表明，Haar随机QNNs中的测量现象比以前认为的要更严重，我们证明了演员的集中现象。

    It is well known that artificial neural networks initialized from independent and identically distributed priors converge to Gaussian processes in the limit of large number of neurons per hidden layer. In this work we prove an analogous result for Quantum Neural Networks (QNNs). Namely, we show that the outputs of certain models based on Haar random unitary or orthogonal deep QNNs converge to Gaussian processes in the limit of large Hilbert space dimension $d$. The derivation of this result is more nuanced than in the classical case due the role played by the input states, the measurement observable, and the fact that the entries of unitary matrices are not independent. An important consequence of our analysis is that the ensuing Gaussian processes cannot be used to efficiently predict the outputs of the QNN via Bayesian statistics. Furthermore, our theorems imply that the concentration of measure phenomenon in Haar random QNNs is much worse than previously thought, as we prove that ex
    
[^33]: 具有浅层解码器的不对称神经图像压缩

    Asymmetrically-powered Neural Image Compression with Shallow Decoders. (arXiv:2304.06244v1 [eess.IV])

    [http://arxiv.org/abs/2304.06244](http://arxiv.org/abs/2304.06244)

    采用浅层或线性解码转换来缩小解码复杂度，同时利用通常不对称的计算预算、更强的编码网络和迭代编码来保持压缩性能，实现了速率失真性能与传统方法相竞争，解码复杂度降低80%以上。

    

    近年来，神经图像压缩方法表现越来越强。但与传统编解码器相比，它们的计算复杂度高出数个数量级，这成为实际部署的障碍。本文通过采用浅层甚至线性解码转换来缩小解码复杂度的差距迈出了一步。为了弥补由此导致的压缩性能下降，作者利用编码和解码之间通常不对称的计算预算，采用更强大的编码网络和迭代编码。理论上证明了这个想法，实验结果在神经图像压缩的速率失真和解码复杂度的权衡方面开启了新的前沿。具体而言，我们实现了与Minnen等人（2018）的平均比例超先验体系结构相竞争的速率失真性能，同时减小了总体解码复杂度80％，或者超过。

    Neural image compression methods have seen increasingly strong performance in recent years. However, they suffer orders of magnitude higher computational complexity compared to traditional codecs, which stands in the way of real-world deployment. This paper takes a step forward in closing this gap in decoding complexity by adopting shallow or even linear decoding transforms. To compensate for the resulting drop in compression performance, we exploit the often asymmetrical computation budget between encoding and decoding, by adopting more powerful encoder networks and iterative encoding. We theoretically formalize the intuition behind, and our experimental results establish a new frontier in the trade-off between rate-distortion and decoding complexity for neural image compression. Specifically, we achieve rate-distortion performance competitive with the established mean-scale hyperprior architecture of Minnen et al. (2018), while reducing the overall decoding complexity by 80 %, or ove
    
[^34]: 在深度神经网络中预防性修剪Clever Hans策略

    Preemptively Pruning Clever-Hans Strategies in Deep Neural Networks. (arXiv:2304.05727v1 [cs.LG])

    [http://arxiv.org/abs/2304.05727](http://arxiv.org/abs/2304.05727)

    本文提出了一种新方法，Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化，从而大大减少了对隐藏Clever Hans策略的依赖，并实现了更高的性能。

    

    可解释的AI已成为验证机器学习模型的流行工具。解释模型的决策策略与用户的领域知识之间的不匹配（例如Clever Hans效应）也被认为是改进错误模型的起点。然而，当用户和解释达成一致时，要怎么做就不那么清楚了。本文通过展示用户接受解释并不保证ML模型的良好功能，特别是一些隐藏的Clever Hans效应可能仍然未被发现，证明了这一点。我们通过贡献一个新方法Explanation-Guided Exposure Minimization (EGEM)，该方法预防性地修剪了ML模型中未受到积极解释反馈的变化。自然画像数据的实验表明，我们的方法导致模型大大减少了对隐藏的Clever Hans策略的依赖，并因此实现了更高的性能。

    Explainable AI has become a popular tool for validating machine learning models. Mismatches between the explained model's decision strategy and the user's domain knowledge (e.g. Clever Hans effects) have also been recognized as a starting point for improving faulty models. However, it is less clear what to do when the user and the explanation agree. In this paper, we demonstrate that acceptance of explanations by the user is not a guarantee for a ML model to function well, in particular, some Clever Hans effects may remain undetected. Such hidden flaws of the model can nevertheless be mitigated, and we demonstrate this by contributing a new method, Explanation-Guided Exposure Minimization (EGEM), that premptively prunes variations in the ML model that have not been the subject of positive explanation feedback. Experiments on natural image data demonstrate that our approach leads to models that strongly reduce their reliance on hidden Clever Hans strategies, and consequently achieve hig
    
[^35]: PriorCVAE: 基于贝叶斯深度生成建模的可扩展 MCMC 参数推断

    PriorCVAE: scalable MCMC parameter inference with Bayesian deep generative modelling. (arXiv:2304.04307v1 [stat.ML])

    [http://arxiv.org/abs/2304.04307](http://arxiv.org/abs/2304.04307)

    PriorCVAE 提出了一种处理高斯过程先验 MCMC 参数推断的贝叶斯深度生成建模新方法，可通过将 VAE 建模条件化于随机过程超参数处理超参数推断与学习先验之间的信息流断裂问题。

    

    在应用场景中，推理速度和模型灵活性至关重要，贝叶斯推断在具有随机过程先验的模型中（如高斯过程）被广泛应用。最近的研究表明，使用变分自动编码器（VAE）等深度生成模型可以编码由 GP 先验或其有限实现引起的计算瓶颈，并且所学生成器可以代替 MCMC 推断中的原始先验。虽然此方法实现了快速而高效的推理，但它丢失了关于随机过程超参数的信息，导致超参数推断不可能和学到的先验模糊不清。我们建议解决上述问题，通过将 VAE 建模条件化于随机过程超参数，以便超参数与 GP 实现一起进行编码。

    In applied fields where the speed of inference and model flexibility are crucial, the use of Bayesian inference for models with a stochastic process as their prior, e.g. Gaussian processes (GPs) is ubiquitous. Recent literature has demonstrated that the computational bottleneck caused by GP priors or their finite realizations can be encoded using deep generative models such as variational autoencoders (VAEs), and the learned generators can then be used instead of the original priors during Markov chain Monte Carlo (MCMC) inference in a drop-in manner. While this approach enables fast and highly efficient inference, it loses information about the stochastic process hyperparameters, and, as a consequence, makes inference over hyperparameters impossible and the learned priors indistinct. We propose to resolve the aforementioned issue and disentangle the learned priors by conditioning the VAE on stochastic process hyperparameters. This way, the hyperparameters are encoded alongside GP real
    
[^36]: 人类和大型语言模型中的概念结构表现的差异性

    Behavioral estimates of conceptual structure are robust across tasks in humans but not large language models. (arXiv:2304.02754v1 [cs.AI])

    [http://arxiv.org/abs/2304.02754](http://arxiv.org/abs/2304.02754)

    本研究使用两种经典认知心理学技术来估算人类和GPT-3等大型语言模型的词汇语义结构，结果表明人类的概念结构稳健鲁棒，而大型语言模型的行为估算结构更多取决于具体任务。

    

    多年以来，神经网络语言模型一直被用作研究心理和脑部概念表征的工具。然而，在当代语言人工智能中，我们可以使用与人类参与者几乎相同的方法来探讨概念表征的潜在结构。本研究使用两种经典的认知心理学技术来估算和比较人类和一个著名的大型语言模型（GPT-3的DaVinci变体）的词汇语义结构。研究表明，人类的概念结构强大且鲁棒，不受文化、语言和估算方法的差异影响；大型语言模型中的行为估算结果相对稳定，但具体取决于任务本身。这些结果表明，虽然人类参与者的行为估算结果可靠，但在使用大型语言模型进行人类认知处理相关推断时，需要谨慎。

    Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language AIs, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses two common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known AI, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from AI behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task 
    
[^37]: 基于层级自回归语言模型合成极高维长期电子健康记录

    Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])

    [http://arxiv.org/abs/2304.02169](http://arxiv.org/abs/2304.02169)

    此论文提出了一种名为HALO的方法，它是一个层级自回归模型，可以生成高保真、细粒度电子健康记录数据，而这些数据可以用于训练准确的ML模型，且无需涉及隐私问题。

    

    合成的电子健康记录(EHRs)能够在机器学习(ML)和统计分析中作为真实EHRs的替代品，既真实又保护隐私。然而，由于高维数据的内在复杂性，以其原始高度维形式生成高保真、细粒度电子健康记录(EHR)数据对现有方法构成了挑战。本文提出了一种名为“Hierarchical Autoregressive Language mOdel (HALO)”的方法，用于生成纵向高维EHR数据，该方法保留了真实EHR的统计特性，可以用于训练准确的ML模型而不涉及隐私问题。我们的HALO方法被设计为一个层级自回归模型，生成一组针对医学代码、临床就诊和病人记录的概率密度函数，可以在其原始未聚合形式下生成真实的EHR数据，无需进行变量选择或聚合。此外，我们的模型还产生大量的随机样本，以提供复杂度较低但仍有意义的EHR数据。

    Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
    
[^38]: 物理信息神经网络用于时域模拟: 精度，计算成本和灵活性

    Physics-Informed Neural Networks for Time-Domain Simulations: Accuracy, Computational Cost, and Flexibility. (arXiv:2303.08994v1 [eess.SY])

    [http://arxiv.org/abs/2303.08994](http://arxiv.org/abs/2303.08994)

    物理信息神经网络被应用于电力系统动态模拟，可以比传统方法快10至1000倍，同时准确性和数值稳定性也足够满足需求。同时，他们也提出了一个基于梯度的方法来规范化NN训练。

    

    电力系统动态模拟是一个计算上昂贵的任务。考虑到发电和需求模式不确定性的增加，需要不断评估数千个场景以确保电力系统的安全性。近年来，物理信息神经网络（PINNs）已被提出作为加速非线性动力系统计算的有前途的解决方案。本文研究了这些方法在电力系统动力学中的适用性，重点关注对负载扰动的动态响应。通过将 PINNs 预测与传统求解器的解决方案进行比较，我们发现 PINNs 可以比传统求解器快 10 到 1000 倍。同时，我们发现它们即使对于大时间步长也足够准确和数值稳定。为了促进更深入的理解，本文还通过在损失函数中引入基于梯度的项，提出了神经网络（NN）训练的新规范化方法。

    The simulation of power system dynamics poses a computationally expensive task. Considering the growing uncertainty of generation and demand patterns, thousands of scenarios need to be continuously assessed to ensure the safety of power systems. Physics-Informed Neural Networks (PINNs) have recently emerged as a promising solution for drastically accelerating computations of non-linear dynamical systems. This work investigates the applicability of these methods for power system dynamics, focusing on the dynamic response to load disturbances. Comparing the prediction of PINNs to the solution of conventional solvers, we find that PINNs can be 10 to 1000 times faster than conventional solvers. At the same time, we find them to be sufficiently accurate and numerically stable even for large time steps. To facilitate a deeper understanding, this paper also presents a new regularisation of Neural Network (NN) training by introducing a gradient-based term in the loss function. The resulting NN
    
[^39]: LExecutor: 学习引导的执行

    LExecutor: Learning-Guided Execution. (arXiv:2302.02343v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2302.02343](http://arxiv.org/abs/2302.02343)

    LExecutor是一个学习引导的执行方法，通过让神经模型预测缺失值，并将其注入到执行中，可以以自由约束的方式执行任意代码片段。该方法在Python代码和从Stack Overflow提取的代码片段上表现良好。

    

    执行代码对于各种程序分析任务是必不可少的，例如通过异常检测错误或获取执行跟踪以进行进一步的动态分析。然而，在实践中执行任意代码片段通常很困难，例如由于缺少变量定义、缺少用户输入和缺少第三方依赖。本文介绍了LExecutor，一种学习引导的方法，用于以自由约束的方式执行任意代码片段。关键思想是让神经模型预测否则会导致程序停滞的缺失值，并将这些值注入到执行中。例如，LExecutor为未定义的变量注入可能的值，并为缺失的函数调用返回预测可能的返回值。我们在来自流行开源项目的Python代码和从Stack Overflow中提取的代码片段上评估了该方法。神经模型以79.5%到98.2%的准确率预测真实值。

    Executing code is essential for various program analysis tasks, e.g., to detect bugs that manifest through exceptions or to obtain execution traces for further dynamic analysis. However, executing an arbitrary piece of code is often difficult in practice, e.g., because of missing variable definitions, missing user inputs, and missing third-party dependencies. This paper presents LExecutor, a learning-guided approach for executing arbitrary code snippets in an underconstrained way. The key idea is to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution. For example, LExecutor injects likely values for otherwise undefined variables and likely return values of calls to otherwise missing functions. We evaluate the approach on Python code from popular open-source projects and on code snippets extracted from Stack Overflow. The neural model predicts realistic values with an accuracy between 79.5% and 98.2
    
[^40]: NESTER：一种自适应的神经符号化方法进行治疗效果评估

    NESTER: An Adaptive Neurosymbolic Method for Treatment Effect Estimation. (arXiv:2211.04370v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2211.04370](http://arxiv.org/abs/2211.04370)

    NESTER是一种自适应的神经符号化方法进行治疗效果评估，将治疗效果估计的所有要求集成到一个框架中，该方法比现有最先进的方法在多个基准数据集上性能更好。

    

    从观测数据中进行治疗效果评估是因果推断中的一个核心问题。基于潜在结果框架的方法通过利用因果推断中的归纳偏置和启发式方法来解决这个问题。每种现有的技术都通过设计神经网络架构和正则化器来解决治疗效果评估的特定方面，例如控制倾向得分、强制随机化等。在本文中，我们提出了一种自适应方法，称为神经符号治疗效果估计器（NESTER），它是一种治疗效果评估的通用方法。NESTER将治疗效果估计的所有要求集成到一个框架中。为此，我们设计了一个基于文献中使用的归纳偏置的治疗效果估计的领域特定语言（DSL）。我们还在理论上研究了NESTER在治疗效果估计任务中的能力。我们全面的实证结果表明，与现有的最先进方法相比，NESTER在多个基准数据集上的效果更好。

    Treatment effect estimation from observational data is a central problem in causal inference. Methods based on potential outcomes framework solve this problem by exploiting inductive biases and heuristics from causal inference. Each existing technique addresses a specific aspect of treatment effect estimation, such as controlling propensity score, enforcing randomization, etc., by designing neural network architectures and regularizers. In this paper, we propose an adaptive method called Neurosymbolic Treatment Effect Estimator (NESTER), a generalized method for treatment effect estimation. NESTER brings together all the desiderata for treatment effect estimation into one framework. For this purpose, we design a Domain Specific Language (DSL) for the treatment effect estimation based on inductive biases used in literature. We also theoretically study NESTER's capability for the treatment effect estimation task. Our comprehensive empirical results show that NESTER performs better on ben
    
[^41]: 基于原始估计亚梯度求解器的不平衡分类SVM

    Primal Estimated Subgradient Solver for SVM for Imbalanced Classification. (arXiv:2206.09311v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.09311](http://arxiv.org/abs/2206.09311)

    本研究旨在实现对不平衡数据集的分类，并评估成本敏感的PEGASOS SVM的性能，同时将核函数纳入SVM中扩展Ding的工作。

    

    本研究旨在通过实验证明我们的成本敏感PEGASOS SVM在主多次要比从8.6：1到130：1的不平衡数据集上具有良好的性能，并确定包括截距（偏见）、正则化和参数是否会影响我们选择的数据集上的性能。虽然许多人采用SMOTE方法，但我们旨在采用一种计算量较小的方法。通过检查学习曲线来评估性能，这些曲线可以诊断我们是过度拟合还是欠拟合，或者我们选择了过度代表性或欠代表性的训练/测试数据。我们还将在验证曲线中查看超参数的背景与测试和训练误差之间的关系。我们将基准化我们的PEGASOS成本敏感SVM与Ding的LINEAR SVM DECIDL方法的结果。他在一个数据集中获得了0.5的ROC-AUC。我们的工作将通过将核函数纳入SVM来扩展Ding的工作。我们将使用Python而不是MATLAB，因为Python具有更有效地存储我的数据集的字典。

    We aim to demonstrate in experiments that our cost sensitive PEGASOS SVM achieves good performance on imbalanced data sets with a Majority to Minority Ratio ranging from 8.6:1 to 130:1 and to ascertain whether the including intercept (bias), regularization and parameters affects performance on our selection of datasets. Although many resort to SMOTE methods, we aim for a less computationally intensive method. We evaluate the performance by examining the learning curves. These curves diagnose whether we overfit or underfit or we choose over representative or under representative training/test data. We will also see the background of the hyperparameters versus the test and train error in validation curves. We benchmark our PEGASOS Cost-Sensitive SVM's results of Ding's LINEAR SVM DECIDL method. He obtained an ROC-AUC of .5 in one dataset. Our work will extend the work of Ding by incorporating kernels into SVM. We will use Python rather than MATLAB as python has dictionaries for storing m
    
[^42]: 马尔科夫潜在博弈中的独立和去中心化学习

    Independent and Decentralized Learning in Markov Potential Games. (arXiv:2205.14590v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14590](http://arxiv.org/abs/2205.14590)

    独立的去中心化学习在马尔科夫潜在博弈中有效，通过更新Q函数可以引导策略收敛到稳定的纳什平衡点。

    

    我们提出了一种多智能体强化学习机制，并分析了它在无限时间折扣马尔科夫潜在博弈中的收敛性。我们专注于独立和去中心化的设置，在这种设置下，玩家不了解游戏模型，也不能进行协调。在每个阶段，玩家通过异步方式更新他们的打扰Q函数的估计值，该函数根据实现的一阶段奖励评估他们的总体条件付款。然后，玩家通过将基于估计Q函数的平滑最优一阶段偏差策略纳入其策略中来独立地更新其策略。学习动态的关键特征是Q函数估计是以比策略更快的时间尺度进行更新的。我们证明了我们的学习动态引导的策略在概率1的情况下收敛到马尔科夫潜在博弈的稳定纳什平衡。我们的结果凸显了简单学习动态在达到马尔可夫潜在博弈的稳定纳什平衡方面的功效，即使是在独立和去中心化代理环境中。

    We propose a multi-agent reinforcement learning dynamics, and analyze its convergence in infinite-horizon discounted Markov potential games. We focus on the independent and decentralized setting, where players do not have knowledge of the game model and cannot coordinate. In each stage, players update their estimate of a perturbed Q-function that evaluates their total contingent payoff based on the realized one-stage reward in an asynchronous manner. Then, players independently update their policies by incorporating a smoothed optimal one-stage deviation strategy based on the estimated Q-function. A key feature of the learning dynamics is that the Q-function estimates are updated at a faster timescale than the policies. We prove that the policies induced by our learning dynamics converge to a stationary Nash equilibrium in Markov potential games with probability 1. Our results highlight the efficacy of simple learning dynamics in reaching a stationary Nash equilibrium even in environme
    
[^43]: 关于点对点去中心化机器学习的（不）安全性研究

    On the (In)security of Peer-to-Peer Decentralized Machine Learning. (arXiv:2205.08443v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2205.08443](http://arxiv.org/abs/2205.08443)

    本文对去中心化学习进行了首次深入隐私分析，引入了一系列新颖的攻击方法，并证明去中心化学习并未提供比联邦学习更好的安全优势，反而增加了攻击面。而且，隐私保护配置需要全连接网络，失去了实际优势，完全打败了去中心化方法的目标。

    

    本文对去中心化学习框架进行了首次深入隐私分析，该框架是一种协作机器学习框架，旨在解决联邦学习的主要限制。我们引入了一系列新颖的攻击方法，包括被动和主动的去中心化敌手，我们证明了与去中心化学习提出者所声称的相反，去中心化学习并没有提供任何关于联邦学习的安全优势。相反，它增加了攻击面，使得系统中的任何用户都能够进行隐私攻击，例如梯度逆推，甚至获得对诚实用户的本地模型的完全控制。我们还表明，鉴于当前防护技术，去中心化学习的隐私保护配置需要全连接网络，失去了与联邦设置的任何实际优势，因此完全打败了去中心化方法的目标。

    In this work, we carry out the first, in-depth, privacy analysis of Decentralized Learning -- a collaborative machine learning framework aimed at addressing the main limitations of federated learning. We introduce a suite of novel attacks for both passive and active decentralized adversaries. We demonstrate that, contrary to what is claimed by decentralized learning proposers, decentralized learning does not offer any security advantage over federated learning. Rather, it increases the attack surface enabling any user in the system to perform privacy attacks such as gradient inversion, and even gain full control over honest users' local model. We also show that, given the state of the art in protections, privacy-preserving configurations of decentralized learning require fully connected networks, losing any practical advantage over the federated setup and therefore completely defeating the objective of the decentralized approach.
    
[^44]: SANSformers: 无注意力模型在电子健康记录中的自我监督预测

    SANSformers: Self-Supervised Forecasting in Electronic Health Records with Attention-Free Models. (arXiv:2108.13672v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2108.13672](http://arxiv.org/abs/2108.13672)

    使用无注意力的序列模型SANSformer在电子健康记录中进行自我监督预测，充分挖掘了Transformer在EHR应用中的优势。主要应用于预测未来的医疗资源利用，特别适用于处理不同患者子组，如罕见疾病患者。

    

    Transformer神经网络在电子健康记录（EHR）中的应用面临挑战，由于EHR数据具有独特的多维顺序结构，所以与简单的线性模型相比往往表现不佳。因此，Transformer的优势，如高效的迁移学习和改进的可扩展性，在EHR应用中没有得到充分利用。为了克服这些挑战，我们引入了SANSformer，这是一种新颖的无注意力序列模型，专门设计以适应EHR数据的独特特征。我们的主要应用领域是预测未来的医疗资源利用，这是有效分配医疗资源的关键任务。当处理不同的患者子组时，这个任务变得特别困难。这些被唯一的健康轨迹所特征化的子组，往往规模较小，如罕见疾病患者，需要特殊的建模方法。为了解决这个问题，我们采用了一种自我监督的方法，将多个子组的预测任务转化为一个整体的预测任务。

    The application of Transformer neural networks to Electronic Health Records (EHR) is challenging due to the distinct, multidimensional sequential structure of EHR data, often leading to underperformance when compared to simpler linear models. Thus, the advantages of Transformers, such as efficient transfer learning and improved scalability are not fully exploited in EHR applications. To overcome these challenges, we introduce SANSformer, a novel attention-free sequential model designed specifically with inductive biases to cater for the unique characteristics of EHR data.  Our main application area is predicting future healthcare utilization, a crucial task for effectively allocating healthcare resources. This task becomes particularly difficult when dealing with divergent patient subgroups. These subgroups, characterized by unique health trajectories and often small in size, such as patients with rare diseases, require specialized modeling approaches. To address this, we adopt a self-
    
[^45]: 自适应实时感知的运行时决策学习

    Learning Runtime Decisions for Adaptive Real-Time Perception. (arXiv:2106.05665v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2106.05665](http://arxiv.org/abs/2106.05665)

    Chanakya是一种学习近似执行框架，通过在实时感知中平衡精度和延迟的训练，自动学习权衡的决策，并在多个感知任务中取得了最先进的性能。

    

    实时感知需要计划的资源利用。计算规划在实时感知中受精度和延迟两个因素控制。存在可运行时决策（例如选择输入分辨率）会引起权衡，影响给定硬件上的性能，来自固有（内容，例如场景混乱）和外在（系统，例如资料争用）特征。我们提出了 Chanakya，一个学习近似执行框架，它自然地从流式感知范例中衍生出来，以自动学习这些权衡所引起的决策。Chanakya 是通过新颖的奖励训练平衡精度和延迟来隐式地进行训练，而不是近似这两个目标。Chanakya 同时考虑内在和外在背景，以及来自多个视角的预测深度，以学习从图像序列到动态可调整的感知算法流水线的映射。所提出的方法学习了实时感知中的高效决策，并在多个感知任务上取得了最先进的性能。

    Real-time perception requires planned resource utilization. Computational planning in real-time perception is governed by two considerations -- accuracy and latency. There exist run-time decisions (e.g. choice of input resolution) that induce tradeoffs affecting performance on a given hardware, arising from intrinsic (content, e.g. scene clutter) and extrinsic (system, e.g. resource contention) characteristics.  Earlier runtime execution frameworks employed rule-based decision algorithms and operated with a fixed algorithm latency budget to balance these concerns, which is sub-optimal and inflexible. We propose Chanakya, a learned approximate execution framework that naturally derives from the streaming perception paradigm, to automatically learn decisions induced by these tradeoffs instead. Chanakya is trained via novel rewards balancing accuracy and latency implicitly, without approximating either objectives. Chanakya simultaneously considers intrinsic and extrinsic context, and pred
    

