# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion.](http://arxiv.org/abs/2305.06356) | 具有4D动态场景表示的HumanRF能够从多视角视频输入中捕捉全身外貌，以高压缩率捕捉精细细节并支持高分辨率。ActorsHQ提供了12MP的镜头，为长序列获得时间上连贯的人物重建。 |
| [^2] | [Supervised learning with probabilistic morphisms and kernel mean embeddings.](http://arxiv.org/abs/2305.06348) | 本文提出了监督学习中正确损失函数的概念，其通过概率测度的条件正则概率测度解决线性算子方程的问题得到定义，适用于可测空间的输入空间和标签空间。 |
| [^3] | [CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators.](http://arxiv.org/abs/2305.06347) | CosmoPower-JAX使用可微的宇宙模拟器进行高维贝叶斯推断，其采用JAX的特性以及GPU技术加速参数估计，可以有效地探索高维参数空间并在短时间内获得准确的参数和后验分布。 |
| [^4] | [Frequency-Supported Neural Networks for Nonlinear Dynamical System Identification.](http://arxiv.org/abs/2305.06344) | 本文提出了一种新的神经网络结构——频率支持神经网络，它通过加入频率信息适应于非线性系统辨识任务，并在多个任务中表现出优越性。 |
| [^5] | [Optimizing Drug Design by Merging Generative AI With Active Learning Frameworks.](http://arxiv.org/abs/2305.06334) | 该论文提出了一种基于主动学习的生成AI工作流程，可以克服当前生成AI方法的局限性，去设计出具有高预测亲和力的可行化学分子。 |
| [^6] | [Similarity of Neural Network Models: A Survey of Functional and Representational Measures.](http://arxiv.org/abs/2305.06329) | 本文综述了神经网络模型相似度的两个观点：表示性相似和功能相似，提供了这两个家族的详细描述，并总结和讨论了其属性和关系，并提出了实践建议。 |
| [^7] | [Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception.](http://arxiv.org/abs/2305.06324) | 本文提出了集成多模态感知（IMP）方法，将多模态输入集成到单个编码器中，采用交替梯度下降法（AGD）和混合专家（MoE）相结合的方法实现高效的模型和任务扩展，取得了在多个基准测试中具有竞争力的性能表现。 |
| [^8] | [NervePool: A Simplicial Pooling Layer.](http://arxiv.org/abs/2305.06315) | 单纯复形池化层NervePool在池化图结构数据时，基于顶点分区生成单纯复形的分层表示，可以灵活地建模更高阶的关系，同时缩小高维单纯形，实现降采样，减少计算成本和减少过拟合。 |
| [^9] | [Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks.](http://arxiv.org/abs/2305.06314) | 本文提出一种称为Scan2LoD3的新方法，通过改进外墙层次的语义三维分割来精确重建具有语义信息的LoD3建筑模型。 |
| [^10] | [Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning.](http://arxiv.org/abs/2305.06295) | 本文采用深度强化学习算法，基于电子病历来学习获得正确诊断所需的观察序列的最优顺序。因为诊断指南的缺陷，尤其对罕见病或患有多种病的患者，DRL算法具有重要现实意义。 |
| [^11] | [Joint Metrics Matter: A Better Standard for Trajectory Forecasting.](http://arxiv.org/abs/2305.06292) | 这项研究展示了边际度量标准不足以完全捕捉多个互动代理的联合表现，提出使用联合度量标准进行轨迹预测方法评估的重要性。 |
| [^12] | [Learning Video-Conditioned Policies for Unseen Manipulation Tasks.](http://arxiv.org/abs/2305.06289) | 该论文提出了一种名为 ViP 的新方法，将人类的任务演示视频映射到机器人操作技能中，实现了人机交互中指定任务的目标。 |
| [^13] | [Vertical Federated Learning over Cloud-RAN: Convergence Analysis and System Optimization.](http://arxiv.org/abs/2305.06279) | 提出了一种基于云无线接入网的垂直联邦学习系统，通过利用空中计算和协作模型聚合来实现快速而精确的模型聚合。为了解决由空中计算和前程传输容量限制引起的误差问题，对该系统进行了收敛分析和系统优化。 |
| [^14] | [FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation.](http://arxiv.org/abs/2305.06272) | 本文提出了一个名为FedPDD的隐私保护双重蒸馏框架，用于跨平台联邦推荐。该框架包括教师蒸馏和学生蒸馏两个阶段，在不传输模型信息的情况下，通过有效地转移知识和使用一种新的蒸馏损失函数来构建全局模型，实现了最先进的性能。 |
| [^15] | [Deep Reinforcement Learning Based Resource Allocation for Cloud Native Wireless Network.](http://arxiv.org/abs/2305.06249) | 本文研究了一种基于云原生无线架构的资源分配方法，针对网络切片和多接入边缘计算等场景，利用深度强化学习技术提出了两个无模型算法，能够动态训练分配策略，有效提高了网络效率。 |
| [^16] | [Rethinking the Value of Labels for Instance-Dependent Label Noise Learning.](http://arxiv.org/abs/2305.06247) | 本文提出了一种利用深度生成模型和因果表征学习处理实例相关标签噪声问题的新算法，能很好地识别高层次的内容和风格潜在因素，并在合成和真实数据集上验证了其有效性。 |
| [^17] | [Explainable Knowledge Distillation for On-device Chest X-Ray Classification.](http://arxiv.org/abs/2305.06244) | 该论文提出了一种利用可解释的人工智能（XAI）技术和知识蒸馏（KD）策略，创建用于设备内多标签CXr图像分类的紧凑深度学习模型，在三个基准CXr数据集上表现出更好的性能。 |
| [^18] | [Penalized deep neural networks estimator with general loss functions under weak dependence.](http://arxiv.org/abs/2305.06230) | 本文提出了一种稀疏惩罚深度神经网络预测方法，适用于学习弱相关性过程，并在特定情况下使用$\theta_\infty$系数。文中还提供了相应的神谕不等式和收敛速度，对于目标函数足够光滑的情况，超额风险的收敛速度接近于$\mathcal{O}(n^{-1/3})$。其中提供了模拟结果和应用于预测Vitória颗粒物的案例。 |
| [^19] | [Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources.](http://arxiv.org/abs/2305.06217) | 补丁学习是一种新的范式，通过整合来自不同数据源的信息，解决了数据隐私、异构数据来源和无法充分利用多个数据模态的挑战。它可以同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。 |
| [^20] | [Sequence-Agnostic Multi-Object Navigation.](http://arxiv.org/abs/2305.06178) | 该论文提出了一种无序多物体导航算法，该算法利用深度强化学习框架，通过适当的奖励规范，奖励单个和多个目标对象类别的进展，实现了对多个对象类别的准确定位，适用于动态变化的实际应用。 |
| [^21] | [Fine-tuning Language Models with Generative Adversarial Feedback.](http://arxiv.org/abs/2305.06176) | 本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。 |
| [^22] | [Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging.](http://arxiv.org/abs/2305.06174) | 本文分析了工业、倡导组织和气候倡导组织在社交媒体上如何影响气候变化的叙事，并提出了一个最小化监督模型组合方法，用于识别Facebook上气候广告的立场。 |
| [^23] | [K-SpecPart: A Supervised Spectral Framework for Multi-Way Hypergraph Partitioning Solution Improvement.](http://arxiv.org/abs/2305.06167) | 本论文提出了一个名为K-SpecPart的超图划分算法，通过解决广义特征值问题，捕捉平衡分区目标和全局超图结构，在多元划分中提高算法质量。 |
| [^24] | [EdgeNet : Encoder-decoder generative Network for Auction Design in E-commerce Online Advertising.](http://arxiv.org/abs/2305.06158) | EdgeNet是一种编码器-解码器生成网络，可用于在线电商广告拍卖中的数据驱动竞价设计。与传统模型相比，EdgeNet 可以更好地捕捉广告间的相互影响，并利用丰富的上下文信息，提高广告竞拍的效率。 |
| [^25] | [Leveraging Synthetic Targets for Machine Translation.](http://arxiv.org/abs/2305.06155) | 本文提供了一种利用预训练模型生成合成目标从而提高机器翻译性能的方法，并发现其在不同测试基准下的表现优于使用真实数据训练，这一方法在有限资源的情况下尤其有用。 |
| [^26] | [A semi-automatic method for document classification in the shipping industry.](http://arxiv.org/abs/2305.06148) | 运输行业使用OCR技术对文件进行分类以提高准确性和效率。该研究提出半自动化的方法，基于关键词频率构建一个稳健的文档分类系统，其在收集的85个违约案例和555个非违约案例上实现了93.31%的高分类准确度。 |
| [^27] | [Feature Expansion for Graph Neural Networks.](http://arxiv.org/abs/2305.06142) | 本文通过分析图神经网络中的特征空间，提出了特征子空间展开和结构主成分两种方法来扩展特征空间，从而获得更好的结果。 |
| [^28] | [CrudeBERT: Applying Economic Theory towards fine-tuning Transformer-based Sentiment Analysis Models to the Crude Oil Market.](http://arxiv.org/abs/2305.06140) | 本研究提出了一种识别及分类影响原油市场供需的新闻事件的方法，并据此设计了CrudeBERT情感分析模型，该模型优于其他专有和开源方案，为原油期货市场相关标题提供情感分类的改进表现。 |
| [^29] | [A Neural Emulator for Uncertainty Estimation of Fire Propagation.](http://arxiv.org/abs/2305.06139) | 本论文提出了一种新的神经网络模型，直接估计给定输入参数不确定性的火灾传播概率，省去了繁重的模拟集合，能够更有效地对火灾传播的不确定性进行分析。 |
| [^30] | [A proof of convergence of inverse reinforcement learning for multi-objective optimization.](http://arxiv.org/abs/2305.06137) | 本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。 |
| [^31] | [FedDWA: Personalized Federated Learning with Online Weight Adjustment.](http://arxiv.org/abs/2305.06124) | 本文提出了一种个性化联邦学习算法，名为FedDWA，采用动态权重调整来保护数据隐私并以更少的通信开销捕捉客户之间的相似性，能够训练高精度和高效的个性化模型。 |
| [^32] | [Few-shot Action Recognition via Intra- and Inter-Video Information Maximization.](http://arxiv.org/abs/2305.06114) | 本论文提出了一种称为VIM的框架，采用自适应空间-时间视频采样器和时空动作增强器，用于小样本动作识别，充分利用视频内部和视频间信息以提高识别精度。 |
| [^33] | [XMI-ICU: Explainable Machine Learning Model for Pseudo-Dynamic Prediction of Mortality in the ICU for Heart Attack Patients.](http://arxiv.org/abs/2305.06109) | 本研究提出了一种新的伪动态机器学习框架，用于心脏病患者在 ICU 中的死亡率预测，并提供了诊断可解释性和临床风险分析。该方法在预测死亡率时提供了高精度的预测结果和时间分辨率可解释性结果。 |
| [^34] | [Few-shot Link Prediction on N-ary Facts.](http://arxiv.org/abs/2305.06104) | 本文提出了一个新任务——少样本N-元事实链接预测，并提出了一个名为FLEN的模型来实现。FLEN由三个模块组成，可以从有限的标记实例中预测N-元事实中的缺失实体。 |
| [^35] | [Towards Better Graph Representation Learning with Parameterized Decomposition & Filtering.](http://arxiv.org/abs/2305.06102) | 本研究提出一个新的通用框架，采用参数化分解和滤波，统一了现有的GNN模型，提高了GNN的灵活性，缓解了现有模型的平滑和放大问题，并开发了简单但有效的模型，在各种图形学习任务中实现了显著的改进和计算效率。 |
| [^36] | [XTab: Cross-table Pretraining for Tabular Transformers.](http://arxiv.org/abs/2305.06090) | XTab 是一个用于不同领域的跨表格预训练的框架，它通过使用独立的特征工程师和联邦学习解决表格之间不一致的列类型和数量的挑战。在测试中， XTab 可以提高多个表格变换器的泛化能力、学习速度和性能。 |
| [^37] | [A Glimpse in ChatGPT Capabilities and its impact for AI research.](http://arxiv.org/abs/2305.06087) | ChatGPT展示了LLMs的强大能力，但这些模型的训练和运行需要巨大的计算资源和高昂的成本，预计会给AI研究带来重大影响。 |
| [^38] | [Best Arm Identification in Bandits with Limited Precision Sampling.](http://arxiv.org/abs/2305.06082) | 本文研究了在有限精度下采样的多臂赌博机问题，提出了一种修改的跟踪算法来处理最优分配的非唯一性，并证明了其渐进最优性。 |
| [^39] | [Towards Effective Visual Representations for Partial-Label Learning.](http://arxiv.org/abs/2305.06080) | 本文提出了一个简单的框架PaPi，用于解决部分标签学习（PLL）问题，该框架通过由线性分类器指导的原型分类器优化来作为正样本，从而提高表示学习的性能，促进了标签消歧。 |
| [^40] | [iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling Perspectives?.](http://arxiv.org/abs/2305.06074) | 本文讨论了建模注释者不一致性的两种方法：分布式软标记方法和建模个体注释者或其组的观点。本文通过将两种方法结合起来的方式来模拟注释者不一致性。尽管多任务方法之前表现良好，但我们发现其在包含不同注释者观点的数据集上表现不佳。研究结果表明，该方法可以更细致地理解注释者不一致性，为更准确的观点建模提供可能。 |
| [^41] | [Enhancing Quantum Support Vector Machines through Variational Kernel Training.](http://arxiv.org/abs/2305.06063) | 本文研究了量子支持向量机，并提出了一种新方法：量子变分核支持向量机（QVK-SVM），能够在准确性、损失和混淆矩阵指标方面优于现有模型。它应该成为未来QML研究中的可靠工具。 |
| [^42] | [Visual Tuning.](http://arxiv.org/abs/2305.06061) | 本文综述了视觉调整的发展与现状，将近期的视觉调整技术分为五类，包括提示调整、适配器调整、参数翻译、紧凑调整和模块调整，并提出了未来研究方向。 |
| [^43] | [Compressing neural network by tensor network with exponentially fewer variational parameters.](http://arxiv.org/abs/2305.06058) | 本文提出了一种通用的压缩方案，将神经网络的可变参数编码为多层张量网络，明显减少了可变参数的数量，并在多个神经网络和数据集上表现出了卓越的压缩性能，以VGG-16的测试精度提高为例。 |
| [^44] | [A Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems.](http://arxiv.org/abs/2305.06055) | 本文提出了一个基于时间依赖的分类系统，用以帮助更好地了解自动化决策系统中的反馈循环，其中反馈循环可能会导致系统中的偏见得到加剧。 |
| [^45] | [Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods.](http://arxiv.org/abs/2305.06044) | 本文比较了不同的缺失数据处理方法对相关图的影响，建议使用直接参数估计法(DPER)来绘制相关图 |
| [^46] | [Blockwise Principal Component Analysis for monotone missing data imputation and dimensionality reduction.](http://arxiv.org/abs/2305.06042) | 基于块的主成分分析处理单调缺失数据的插值与降维框架，可以显著地减少插补时间，适用于大数据集。 |
| [^47] | [Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments.](http://arxiv.org/abs/2305.06026) | 本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。 |
| [^48] | [Global Convergence of Deep Galerkin and PINNs Methods for Solving Partial Differential Equations.](http://arxiv.org/abs/2305.06000) | 该论文证明了 Deep Galerkin Method（DGM）算法在全局收敛时，训练得到的神经网络会收敛于一个无限维的线性常微分方程的解，从而逼近求解高维偏微分方程。在一些情况下，PINNs 方法会更稳定。 |
| [^49] | [Structural Hawkes Processes for Learning Causal Structure from Discrete-Time Event Sequences.](http://arxiv.org/abs/2305.05986) | 本文提出了一种叫做结构Hawkes过程的方法，通过利用瞬时效应学习离散时间事件序列中事件类型之间的因果结构。 |
| [^50] | [Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models.](http://arxiv.org/abs/2305.05982) | 本文需要解决医疗对话摘要的问题，通过将任务划分为几个小的对话任务依次构建，使用GPT-3大型语言模型，动态构建提示任务以及确定医学实体及其确认状态。 |
| [^51] | [Graph Neural Networks and 3-Dimensional Topology.](http://arxiv.org/abs/2305.05966) | 本研究利用图神经网络解决了一类由管道图描述的三维流形分类问题，并训练了一个高准确性的GNN模型，同时介绍了GNN在增强学习中的应用。 |
| [^52] | [Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference.](http://arxiv.org/abs/2305.05933) | Spectrum Breathing是一种保护空中联合学习免受干扰的实际方法，通过将随机梯度剪枝和扩频级联起来，以压制干扰而无需扩展带宽。代价是增加的学习延迟。 |
| [^53] | [Deep Learning for Predicting Progression of Patellofemoral Osteoarthritis Based on Lateral Knee Radiographs, Demographic Data and Symptomatic Assessments.](http://arxiv.org/abs/2305.05927) | 本研究利用深度学习和注意力机制预测膝关节骨关节炎进展，发现成像数据在预测中起到重要作用。 |
| [^54] | [Fast Distributed Inference Serving for Large Language Models.](http://arxiv.org/abs/2305.05920) | FastServe是一种针对大型语言模型的分布式推理服务系统，利用抢占式调度和跳过-连接多级反馈队列，最小化模型推断的作业完成时间(JCT)。 |
| [^55] | [A Hybrid of Generative and Discriminative Models Based on the Gaussian-coupled Softmax Layer.](http://arxiv.org/abs/2305.05912) | 本文提出了一种基于神经网络的生成模型和判别模型混合的方法，该方法通过高斯耦合softmax层允许分类器估计类后验分布和类条件数据分布，具有半监督学习和置信度校准的应用价值。 |
| [^56] | [Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers.](http://arxiv.org/abs/2305.05909) | 该论文提出了一种基于演化生成辅助敌对攻击来提高训练策略鲁棒性的方法，用于解决合作多智能体强化学习中在不同环境中测试时策略扰动的问题，并在实验中表现优于最先进的CMARL方法。 |
| [^57] | [DPMLBench: Holistic Evaluation of Differentially Private Machine Learning.](http://arxiv.org/abs/2305.05900) | 本文提出了DPMLBench框架，通过在图像分类任务上综合衡量加强DP-SGD的DPML算法的实用性和防御能力，填补了比较DPML算法改进表现的空白，提高了DPML算法的性能。 |
| [^58] | [CUTS+: High-dimensional Causal Discovery from Irregular Time-series.](http://arxiv.org/abs/2305.05890) | CUTS+是一种基于Granger因果和图神经网络(MPGNN)的因果发现算法，通过引入粗到细发现（C2FD）技术提高可扩展性。实验结果表明，CUTS+在非规则高维数据上的因果发现性能大幅度提高。 |
| [^59] | [Deep Partial Multi-Label Learning with Graph Disambiguation.](http://arxiv.org/abs/2305.05882) | 本文提出了一种新的基于图形消歧的深度部分多标签模型（PLAIN），它可以从候选标签中恢复标签置信度，并利用标签依赖性，提高多标签学习的性能。 |
| [^60] | [Achieving Diversity in Counterfactual Explanations: a Review and Discussion.](http://arxiv.org/abs/2305.05840) | 本篇论文综述了因果关系解释中多样性的概念及其定义，提出了生成多种因果关系例子以解释一个预测结果的方法，探讨了这种方法的优点和局限性。 |
| [^61] | [Reference-based OCT Angiogram Super-resolution with Learnable Texture Generation.](http://arxiv.org/abs/2305.05835) | 本研究提出了一种基于参考的超分辨率图像生成框架，使用可训练的纹理生成器产生纹理从而保持OCTA图像的分辨率，解决了扫描区域增大导致分辨率下降的问题。 |
| [^62] | [Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts.](http://arxiv.org/abs/2305.05832) | 本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。 |
| [^63] | [Convergence of a Normal Map-based Prox-SGD Method under the KL Inequality.](http://arxiv.org/abs/2305.05828) | 本文提出了一种新的随机正态映射算法用于非凸复合型优化问题，并证明其收敛性质。该方法扩展了基本Proximal随机梯度法的更有限的收敛保证。 |
| [^64] | [Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation.](http://arxiv.org/abs/2305.05827) | 本文研究了基于对比学习和领域自适应的方法来减轻贷款筛选中的表征偏差，提高了贷款筛选所使用机器学习算法的性能，特别对于低社会经济背景借款人的表现有很大的改善。 |
| [^65] | [Best-Effort Adaptation.](http://arxiv.org/abs/2305.05816) | 研究了最佳努力适应性问题，提出了一种新的基于差异的理论分析方法以及用于标准域适应性问题的改进学习算法，表现出很好的实验效果。 |
| [^66] | [Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review.](http://arxiv.org/abs/2305.05813) | 本文综述了过去十年中遥感影像变化检测方面的重大进展，介绍了问题定义、数据集、评估指标和变压器等初步知识，深度学习作为一种强大的特征提取工具被广泛运用于解决检测遥感图像中的变化的复杂挑战。 |
| [^67] | [Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization.](http://arxiv.org/abs/2305.05812) | 本文提出了一种基于深度强化学习的方案，以解决核电站燃料优化问题，能够提高核电站的性能和安全。 |
| [^68] | [On the Information Capacity of Nearest Neighbor Representations.](http://arxiv.org/abs/2305.05808) | 本文研究了一种联想计算模型，其中计算与记忆不可分，通过从输入向量到最近邻锚点的收敛完成计算。文章关注于在这个模型中表示布尔函数的信息容量。 |
| [^69] | [Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues.](http://arxiv.org/abs/2305.05807) | 本文研究了数据集中的分布变化对深度学习模型的影响，并提出了一个综合协议来分析多样性变化和相关性变化。使用皮肤癌分析分类问题的实例，发现模型不仅会学习和传播相关性变化，而且可能会使用错误的特征。 |
| [^70] | [Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation.](http://arxiv.org/abs/2305.05803) | 本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。 |
| [^71] | [Seeing double with a multifunctional reservoir computer.](http://arxiv.org/abs/2305.05799) | 本文研究了储备计算机的多功能性能力，探讨了不同吸引子之间的关系如何影响其实现多任务。结果表明，要实现多功能性，需要RC内部网络c的谱半径合适选择的临界依赖性。 |
| [^72] | [Testing for Overfitting.](http://arxiv.org/abs/2305.05792) | 本文提出了一种能够使用训练数据进行评估模型性能的假设检验方法，可以准确地定义和检测过拟合。 |
| [^73] | [Enhancing Gappy Speech Audio Signals with Generative Adversarial Networks.](http://arxiv.org/abs/2305.05780) | 本文利用机器学习和生成对抗网络提出了一种方法，通过将语音转换为Mel频谱图并使用图像修复技术进行间隔恢复，实现了接近实时的音频间隔重建，填充后的质量与原始数据相近，可以有效提高语音信号的质量。 |
| [^74] | [Learning to Parallelize with OpenMP by Augmented Heterogeneous AST Representation.](http://arxiv.org/abs/2305.05779) | 该论文提出了一种名为Graph2Par的基于图形的深度学习方法，利用叫做Augmented-AST的抽象语法树表示法来检测并行化代码区域。实验结果显示，该方法在检测可并行化循环方面优于最先进技术。 |
| [^75] | [Multi-Object Self-Supervised Depth Denoising.](http://arxiv.org/abs/2305.05778) | 本文提出了一种自监督的多目标深度去噪流程，使用来自高质量传感器的深度图进行监督，去噪低质量深度摄像机的深度图。 |
| [^76] | [DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text.](http://arxiv.org/abs/2305.05773) | 本文提出了一种基于深度学习的文本水印技术DeepTextMark，可用于检测大语言模型生成的文本。该技术实现了盲目性、鲁棒性、隐蔽性和可靠性，并在水印检测精度和抵抗攻击方面优于现有方法。 |
| [^77] | [DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models.](http://arxiv.org/abs/2305.05768) | DifFIQA是一个新的人脸图像质量评估方法，使用去噪扩散概率模型（DDPM）来进行面部图像扰动，并通过量化这些扰动对应的图像嵌入的影响来进行质量评估，以提供可靠的样本质量预测。 |
| [^78] | [Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization.](http://arxiv.org/abs/2305.05760) | 本研究提出了一种新方法，可基于周期时间设置超参数，使得PPO和SAC在广泛的循环时间范围内进行学习，同时实现了接近耗时的在线超参数调整获得的性能。 |
| [^79] | [Ranking & Reweighting Improves Group Distributional Robustness.](http://arxiv.org/abs/2305.05759) | 本文提出了一种利用折扣累积增益（DCG）排序并加权处理训练数据以提高模型对低代表性组的鲁棒性的方法，实验证明其优于先前方法。 |
| [^80] | [When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution.](http://arxiv.org/abs/2305.05754) | 论文解决了在协作建筑任务中如何解决模棱两可的情况，从而提出了何时寻求澄清以及应该询问什么澄清问题这两个关键问题的解答方法。 |
| [^81] | [A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks.](http://arxiv.org/abs/2305.05750) | 本文为深度神经网络硬件可靠性进行了系统文献综述，总结了现有的可靠性评估方法以及数据集和基准的评估，揭示了这一领域的研究空缺。 |
| [^82] | [DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors.](http://arxiv.org/abs/2305.05738) | DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。 |
| [^83] | [Duke Spleen Data Set: A Publicly Available Spleen MRI and CT dataset for Training Segmentation.](http://arxiv.org/abs/2305.05732) | 杜克脾脏数据集（DSDS）是一种公开的脾脏MRI和CT数据集，用于训练深度学习模型进行自动脾脏分割。该数据集包含109个来自患有慢性肝病和门静脉高压的患者的图像，并包括多种脾脏形状和大小的混合特征。 |
| [^84] | [Enhancing Clinical Predictive Modeling through Model Complexity-Driven Class Proportion Tuning for Class Imbalanced Data: An Empirical Study on Opioid Overdose Prediction.](http://arxiv.org/abs/2305.05722) | 该研究提出了一种针对类别不平衡问题的理论框架，通过将最佳类别比例与模型复杂度联系起来，每种模型都能够得到正确的类别比例，实验结果表明此方法在阿片类药物过量预测问题上能够显著提高模型性能。 |
| [^85] | [Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files.](http://arxiv.org/abs/2305.05708) | 使用语言模型可以直接生成三维的新颖有效的分子、材料和蛋白质结合位点的结构，而不需转换成线性字符串表示形式。 |
| [^86] | [DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects.](http://arxiv.org/abs/2305.05706) | 该论文提出了一个名为 DexArt 的基准测试，其旨在通过使用物理模拟器，评估机器人手在未见过的关节物体上进行灵巧操作的泛化能力。 |
| [^87] | [Effects of data time lag in a decision-making system using machine learning for pork price prediction.](http://arxiv.org/abs/2305.05677) | 本文研究了数据采集延迟对机器学习预测模型在猪肉价格预测中的影响，并提出了解决方案，通过利用同日获取的数据，可以有效缓解数据时间滞后的影响。 |
| [^88] | [UAdam: Unified Adam-Type Algorithmic Framework for Non-Convex Stochastic Optimization.](http://arxiv.org/abs/2305.05675) | 本论文提出了统一的Adam型算法框架UAdam，该框架是包括Adam及其变体在内的特例，并且在非凸随机设置下具有收敛性，可以以$\mathcal{O}(1/T)$的速率收敛到静止点的邻域。通过选择适当的超参数，香草Adam也可以收敛。 |
| [^89] | [Enhancing Road Safety through Accurate Detection of Hazardous Driving Behaviors with Graph Convolutional Recurrent Networks.](http://arxiv.org/abs/2305.05670) | 本文提出了一种基于图卷积循环网络的可靠DBD系统，利用公共传感器提高了驾驶行为检测模型的准确性和实用性。 |
| [^90] | [Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA) Specimens.](http://arxiv.org/abs/2305.05668) | 本研究介绍了一种基于神经符号人工智能的算法，能够高精度预测增材制造聚乳酸（PLA）样品的冲击强度。 |
| [^91] | [Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare.](http://arxiv.org/abs/2305.05640) | 本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。 |
| [^92] | [TASTY: A Transformer based Approach to Space and Time complexitY.](http://arxiv.org/abs/2305.05379) | 本文旨在通过创建一个跨多种语言的代码片段标记数据集，以填补从代码中分类时间和空间复杂性的空白，并提出了使用基于代码的多模型来实现这一目标。 |
| [^93] | [Web Content Filtering through knowledge distillation of Large Language Models.](http://arxiv.org/abs/2305.05027) | 本文提出了一种基于大语言模型知识蒸馏的 URL 分类方法，可用于网络内容过滤，其学生模型在参数数量减少 175 倍的情况下，精度提升了 9%，超过了当前最先进方法。 |
| [^94] | [Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification.](http://arxiv.org/abs/2305.04228) | 本研究提出了使用异构有向超图表示AST，并使用异构有向超图神经网络处理图形进行代码分类，超过了现有方法。 |
| [^95] | [Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning.](http://arxiv.org/abs/2305.03870) | 本研究讨论了实际领域中离线训练或增长批量训练的限制，提出了一种教师向学习者进行知识转移的方法，使得数据数量和多样性得到提高。 |
| [^96] | [Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection.](http://arxiv.org/abs/2305.00595) | 本文通过在三个深度学习库中实现两种最先进的方法，并进行评估，研究了深度学习库对在线自适应轻量级时间序列异常检测的影响。 |
| [^97] | [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation.](http://arxiv.org/abs/2304.12566) | 本文提出了一种新的测试时间自适应（TTA）方法AdaNPC，通过利用非参数分类器进行建模从而避免了离线目标数据或在推理时使用额外的复杂优化过程。AdaNPC从存储器中回顾最相似的 K 个样本进行投票预测，逐渐改变存储器中的样本分布以提高测试域性能。 |
| [^98] | [Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications.](http://arxiv.org/abs/2304.12330) | 本文提出了一种基于并行bootstrap的on-policy深度强化学习方法，通过部分轨迹缓冲区和返回bootstrapping步骤来实现灵活使用并行环境，同时保持更新的on-policy性，该方法在连续流控制问题上有很好的应用前景。 |
| [^99] | [Medical Image Deidentification, Cleaning and Compression Using Pylogik.](http://arxiv.org/abs/2304.12322) | 提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。 |
| [^100] | [Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs.](http://arxiv.org/abs/2304.12233) | 本文提出了一种基于扩散的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。与现有具有 3D 几何结构机器学习模型相比，TSdiff 的准确性和效率都更高。TSDiff 能够找到比参考数据库更优化的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。 |
| [^101] | [Privacy-Preserving CNN Training with Transfer Learning.](http://arxiv.org/abs/2304.03807) | 本文提出了一种使用迁移学习实现同态加密技术下隐私保护的CNN训练的方案，通过转换思想和更快的梯度变体，取得了最先进的性能。 |
| [^102] | [Module-based regularization improves Gaussian graphical models when observing noisy data.](http://arxiv.org/abs/2303.16796) | 建议将推断网络的模块化结构整合到正则化强度的交叉验证中，以改善高斯图模型在观察含噪数据时的表现。 |
| [^103] | [Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization.](http://arxiv.org/abs/2303.10758) | 本论文证明了在平稳随机凸优化中，GD和SGD的泛化下界可以降低，并且长时间的训练可能导致更差的泛化能力，这与其他研究成果不同。 |
| [^104] | [Speech Modeling with a Hierarchical Transformer Dynamical VAE.](http://arxiv.org/abs/2303.09404) | HiT-DVAE是一种层次Transformer动态变分自编码器，能够优于其他DVAEs在语音频谱建模方面。它具有两个潜变量水平和简单的训练过程，并且具有很高的潜力在低级别语音处理方面。 |
| [^105] | [Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records.](http://arxiv.org/abs/2303.08290) | 本文发现，CNN在健康记录文本编码方面的多功能性和隐含层次结构可以提高其性能，提出了一种基于CNN的编码器来处理不同类型的EHR特征，并在临床任务中展示了其有效性。 |
| [^106] | [Generalised Scale-Space Properties for Probabilistic Diffusion Models.](http://arxiv.org/abs/2303.07900) | 本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。 |
| [^107] | [Fast Attention Requires Bounded Entries.](http://arxiv.org/abs/2302.13214) | 本文研究了内积关注计算的快速算法问题，提出了两个结果，证明了在B = O(sqrt(log n))时存在一种n ^（1 + O（1））时间算法。 |
| [^108] | [Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search.](http://arxiv.org/abs/2302.11223) | 本文提出了一种基于蒙特卡罗树搜索的深度生成式符号回归方法，结合了神经方法和遗传编程算法的优点，运用上下文感知的神经突变模型，并在知名数据集中展示出最先进的性能。 |
| [^109] | [AttentionMixer: An Accurate and Interpretable Framework for Process Monitoring.](http://arxiv.org/abs/2302.10426) | AttentionMixer是一个旨在为能量变换厂建立准确且可解释的辐射监测框架的数据驱动方法，其技术创新点为空间和时间自适应消息传递块和注意力和可视化技术。 |
| [^110] | [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat.](http://arxiv.org/abs/2302.10289) | 本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。 |
| [^111] | [Approximately Bayes-Optimal Pseudo Label Selection.](http://arxiv.org/abs/2302.08883) | 本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。 |
| [^112] | [SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning.](http://arxiv.org/abs/2301.11520) | 本文提出了一种称为SNeRL的语义感知神经辐射场，它通过学习3D-aware的隐式表示来进行强化学习，并在基于像素的以及最新的3D感知表示方法中表现出更好的性能。 |
| [^113] | [What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News.](http://arxiv.org/abs/2301.08146) | 该论文介绍了一种自动化的本地新闻检测和基于内容的本地新闻推荐方法，通过弱监督框架和自动化数据处理，与传统方法相比具有更高的准确性和覆盖率。 |
| [^114] | [Deep learning enhanced noise spectroscopy of a spin qubit environment.](http://arxiv.org/abs/2301.05079) | 本文研究使用深度学习模型提高噪声谱学准确性，实验表明此方法可以比标准技术更加准确，并且需要更少的序列。 |
| [^115] | [Synthetic data generation method for data-free knowledge distillation in regression neural networks.](http://arxiv.org/abs/2301.04338) | 本文提出了一个针对回归任务的无数据知识蒸馏方法，利用生成对抗网络生成合成数据，通过原师生网络的标签训练学生网络。研究了不同的生成方法，提出了新的生成策略，可以直接优化学生网络的性能。 |
| [^116] | [TarViS: A Unified Approach for Target-based Video Segmentation.](http://arxiv.org/abs/2301.02657) | 提出了一种名为TarViS的新颖、统一的网络架构，可用于任何需要在视频中分段任意定义的“目标”集的任务。可以联合训练不同任务的数据集集合，并且可以在推断期间在任务之间进行热切换。在四个不同的任务中进行了应用，均优于现有最先进方法。 |
| [^117] | [Hybrid Multi-agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems.](http://arxiv.org/abs/2212.07313) | 本文介绍了一种基于深度强化学习的混合多智能体自主出行系统，采用多智能体Soft Actor-Critic和加权二分图匹配，能够解决自主移动出行系统中的主动请求分配和拒绝决策问题，并在性能、稳定性和计算方面优于现有基准。 |
| [^118] | [VISEM-Tracking, a human spermatozoa tracking dataset.](http://arxiv.org/abs/2212.02842) | 本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。 |
| [^119] | [A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization.](http://arxiv.org/abs/2212.02387) | 本文提出了一种称为去中心化递归梯度上升法（DREAM）的简单高效算法，用于解决去中心化非凸极小化问题，并实现了寻找原函数的 $\epsilon$-稳定点的最佳理论保证。 |
| [^120] | [Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph.](http://arxiv.org/abs/2212.01923) | 基于查询的多模态路径融合算法可以有效地对知识库进行补全，提高了性能，并且使用了基于查询的技术来提高系统的效率。 |
| [^121] | [$2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations.](http://arxiv.org/abs/2211.01703) | 本文研究了带承诺和噪声观测的$2\times 2$零和博弈，发现平衡点总是存在的；领导者的动作观测结果对于追随者来说要么是有益的，要么是无关紧要的；该博弈的收益在均衡点上被上界限制为纯策略下的SE的收益，下界为混合策略下的纳什均衡的收益。 |
| [^122] | [Modelling black-box audio effects with time-varying feature modulation.](http://arxiv.org/abs/2211.00497) | 提出了利用时变特征调制来模拟黑盒音频效应，可以更好地捕获长时间尺度上的依赖关系，适用于fuzz和压缩等音频效应。 |
| [^123] | [Approximation of nearly-periodic symplectic maps via structure-preserving neural networks.](http://arxiv.org/abs/2210.05087) | 本文提出了一种结构保护神经网络方法，可以逼近几乎周期辛映射，并引出离散时间绝热不变量和稳定性。 |
| [^124] | [Learnware: Small Models Do Big.](http://arxiv.org/abs/2210.03647) | Learnware范式旨在帮助用户充分利用小型模型，实现超越原始目的的事情，以解决当前机器学习技术面临的数据量大、训练难度高、数据安全等问题。 |
| [^125] | [Self-supervised Learning for Clustering of Wireless Spectrum Activity.](http://arxiv.org/abs/2210.02899) | 本研究使用无人监督学习技术探索无线电频谱活动，比较了两种不同的无人监督学习模型和一种混合模型，实现了精准的频谱活动聚类。 |
| [^126] | [Training neural network ensembles via trajectory sampling.](http://arxiv.org/abs/2209.11116) | 本研究使用随机系统中稀有轨迹的技术定义和训练NNE，通过对轨迹进行偏置来训练NNE，相较于更传统的梯度方法具有潜在优势。 |
| [^127] | [Non-iterative generation of an optimal mesh for a blade passage using deep reinforcement learning.](http://arxiv.org/abs/2209.05280) | 本文介绍了一种使用深度强化学习来非迭代生成任意叶片通道最优网格的方法，实现了自动化和减少人为干预，大大提高了计算效率。 |
| [^128] | [FreeREA: Training-Free Evolution-based Architecture Search.](http://arxiv.org/abs/2207.05135) | 本研究提出了一种无需训练步骤的进化式架构搜索算法FreeREA，可直接在目标硬件上优化搜索，且能够在性能最大化的同时，极大地减小内存占用。实验结果证明其比手动设计更高效。 |
| [^129] | [On the Generalization of Spiking Neural Networks via Minimum Description Length and Structural Stability.](http://arxiv.org/abs/2207.04876) | 本研究通过利用最小描述长度原则和结构稳定性为脉冲神经网络提供了一个明确的泛化界限，并指定了最大稳定分歧解数的下限和上限。 |
| [^130] | [Extending regionalization algorithms to explore spatial process heterogeneity.](http://arxiv.org/abs/2206.09429) | 提出两种新的空间区域划分算法，应用于合成和真实数据集，并取得了较好的成果。其中，两阶段K模型算法表现最佳，具有较好的模型拟合、区域重构和系数估计能力。 |
| [^131] | [Multimodal Learning with Transformers: A Survey.](http://arxiv.org/abs/2206.06488) | 本文综述了基于Transformer的多模态学习技术，包括Vanilla Transformer、Vision Transformer和多模态Transformer，在多模态预训练和特定任务中的应用，以及共享的常见挑战和设计。并探讨了开放的问题和潜在研究方向。 |
| [^132] | [Quantum Policy Iteration via Amplitude Estimation and Grover Search -- Towards Quantum Advantage for Reinforcement Learning.](http://arxiv.org/abs/2206.04741) | 该论文提出了一种量子强化学习方法，并证明在代理和环境的无误差高效量子实现的基础上，该方法可以在样本复杂度方面比基于经典蒙特卡罗的方法有更好的性能提升。 |
| [^133] | [Progressive Purification for Instance-Dependent Partial Label Learning.](http://arxiv.org/abs/2206.00830) | 本研究提出了一种叫做POP的方法来解决实例相关的局部标签学习问题，POP在每个时代更新学习模型并逐步净化每个候选标签集，能以特定的快速速度扩大模型可靠性的范围。 |
| [^134] | [Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost.](http://arxiv.org/abs/2205.14198) | 本文描述了如何使用低成本使任何分层聚类公平平衡，扩展了先前的工作，并提出了成本公平权衡的概念。 |
| [^135] | [From Modern CNNs to Vision Transformers: Assessing the Performance, Robustness, and Classification Strategies of Deep Learning Models in Histopathology.](http://arxiv.org/abs/2204.05044) | 本文评估了现代CNN和视觉Transformer模型在组织病理学中的性能、鲁棒性和分类策略。在乳腺癌、胃癌和结直肠癌全切片图像等五个数据集上进行了广泛测试，结果表明ViT模型在分类准确性方面优于其他CNN，而Swin Transformer模型在对抗染色变化的鲁棒性方面表现最佳。 |
| [^136] | [Efficiently Escaping Saddle Points in Bilevel Optimization.](http://arxiv.org/abs/2202.03684) | 本文研究了双层优化中的鞍点逃逸方法，提出了两种算法，一种是用于确定性问题的扰动近似隐式微分算法，一种是用于随机问题的不精确非曲率源自噪声算法，并且提供了扰动多步梯度下降算法的非渐近分析。 |
| [^137] | [Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant.](http://arxiv.org/abs/2201.10838) | 本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现隐私保护的逻辑回归训练，并成功提升了收敛速度，实现了同态逻辑回归训练仅需较少的迭代次数。 |
| [^138] | [GAP-Gen: Guided Automatic Python Code Generation.](http://arxiv.org/abs/2201.08810) | 本文介绍了一种基于 Python 语法约束和语义约束的引导自动生成 Python 代码的方法 GAP-Gen，通过微调 T5 和 CodeT5 这两种语言模型，在自动生成 Python 代码任务上保持了高生成性能。 |
| [^139] | [More is Less: Inducing Sparsity via Overparameterization.](http://arxiv.org/abs/2112.11027) | 通过过度参数化的神经网络模型，在稀疏恢复（压缩感知）中，采用过度参数的平方损失函数，推出了一个迭代算法解决问题，展示了神经网络对稀疏问题的良好适应性和推广能力，以及竞争性的解决方案。 |
| [^140] | [Modern Non-Linear Function-on-Function Regression.](http://arxiv.org/abs/2107.14151) | 本研究提出一种利用神经网络分析功能数据的新型非线性函数回归模型，通过连续隐藏层实现对功能响应建模，并提供了两种模型拟合策略（FDNN和FBNN），并通过正则化技术得到更加简明的结果。 |
| [^141] | [Non-linear Functional Modeling using Neural Networks.](http://arxiv.org/abs/2104.09371) | 本文提出了一种基于神经网络的、适用于函数数据的新型非线性模型。我们提出了两种变体，旨在显式利用函数数据中固有的结构，并通过全面的模拟研究和实际数据示例证明了该方法的有效性。 |
| [^142] | [Automated Mapping of Vulnerability Advisories onto their Fix Commits in Open Source Repositories.](http://arxiv.org/abs/2103.13375) | 本文提出了一种结合启发式和机器学习（特别是自然语言处理）的方法，将漏洞公告与其修复提交映射到开源代码库中，以解决缺乏全面准确的漏洞数据来源的问题。 |
| [^143] | [Improved Image Wasserstein Attacks and Defenses.](http://arxiv.org/abs/2004.12478) | 本文提出了一种基于Wasserstein距离限制的更好的威胁模型，可以更准确地限制图像扰动。作者指出并纠正了先前Wasserstein威胁模型定义中的缺陷，并展示了更强的攻击和防御。然而，当前的Wasserstein-鲁棒模型在防御现实世界中出现的扰动方面可能受到限制。 |

# 详细

[^1]: HumanRF：用于运动中人的高保真神经辐射场

    HumanRF: High-Fidelity Neural Radiance Fields for Humans in Motion. (arXiv:2305.06356v1 [cs.CV])

    [http://arxiv.org/abs/2305.06356](http://arxiv.org/abs/2305.06356)

    具有4D动态场景表示的HumanRF能够从多视角视频输入中捕捉全身外貌，以高压缩率捕捉精细细节并支持高分辨率。ActorsHQ提供了12MP的镜头，为长序列获得时间上连贯的人物重建。

    

    在各种应用程序中，如电影制作、电脑游戏或视频会议中，高保真地表现人类表现是一个重要的构建块。为了接近生产级的质量，我们介绍了HumanRF，这是一个4D动态神经场景表示，从多视角视频输入中捕捉运动中的全身外貌，并使其可以在新的、看不见的视角下播放。我们的新型表示作为一个动态视频编码，通过将时空分解为一个时间矩阵向量分解，以高压缩率捕捉精细细节。这使我们可以为长序列获得时间上连贯的人物重建，并在具有挑战性的动作情况下表示高分辨率的细节。虽然大多数研究集中在合成4MP或更低分辨率，但我们解决了在12MP上操作的挑战。为此，我们介绍了ActorsHQ，这是一个新的多视角数据集，为160个摄像机提供了12MP的镜头。

    Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 
    
[^2]: 带概率态射和核平均嵌入的监督学习

    Supervised learning with probabilistic morphisms and kernel mean embeddings. (arXiv:2305.06348v1 [math.ST])

    [http://arxiv.org/abs/2305.06348](http://arxiv.org/abs/2305.06348)

    本文提出了监督学习中正确损失函数的概念，其通过概率测度的条件正则概率测度解决线性算子方程的问题得到定义，适用于可测空间的输入空间和标签空间。

    

    本文提出了一个监督学习的生成模型中正确损失函数的概念，适用于可测空间的输入空间X和标签空间Y。 生成模型中的正确损失函数必须正确地度量可能预测器的假设空间H中的元素与监管运算符之间的差异，而监管运算符可能不属于H。 为了定义正确的损失函数，本文提出了一个关于概率测度μ在投影ΠX：X×Y→X相对于概率测度μ𝑋×𝑌的条件正则概率测度μY| X的特殊性质的表征方法，作为线性算子方程的解决方案。 如果Y是一个具有Borel σ-代数 BY的可分的可度量化拓扑空间，则提出了关于概率测度μ相对于投影ΠX的条件正则概率测度μY| X的另一种特殊性质的表征方法。

    In this paper I propose a concept of a correct loss function in a generative model of supervised learning for an input space $\mathcal{X}$ and a label space $\mathcal{Y}$, which are measurable spaces. A correct loss function in a generative model of supervised learning must correctly measure the discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible predictors and the supervisor operator, which may not belong to $\mathcal{H}$. To define correct loss functions, I propose a characterization of a regular conditional probability measure $\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X} \times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}: \mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator equation. If $\mathcal{Y}$ is a separable metrizable topological space with the Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose another characterization of a regular conditional probability measure
    
[^3]: CosmoPower-JAX:利用可微的宇宙模拟器进行高维贝叶斯推断

    CosmoPower-JAX: high-dimensional Bayesian inference with differentiable cosmological emulators. (arXiv:2305.06347v1 [astro-ph.CO])

    [http://arxiv.org/abs/2305.06347](http://arxiv.org/abs/2305.06347)

    CosmoPower-JAX使用可微的宇宙模拟器进行高维贝叶斯推断，其采用JAX的特性以及GPU技术加速参数估计，可以有效地探索高维参数空间并在短时间内获得准确的参数和后验分布。

    

    我们介绍了CosmoPower-JAX，这是CosmoPower框架的一个基于JAX的实现，通过构建宇宙功率谱的神经仿真器来加速宇宙学推断。我们展示了如何使用JAX的自动微分、批量评估和即时编译特性，并在图形处理器（GPU）上运行推断流水线，通过先进的基于梯度的采样技术将参数估计加速数倍。这些可以用于高效地探索高维参数空间，例如用于下一代宇宙学调查分析的空间。我们展示了CosmoPower-JAX在两个模拟的第四阶段配置中的准确性和计算效率。我们首先考虑了一个进行37个模型参数的宇宙剪切分析的单个调查。我们使用CosmoPower-JAX和哈密顿蒙特卡罗取样器派生的等高线进行了验证，结果与未使用仿真似然的嵌套取样器派生的等高线相符。接下来，我们考虑了宇宙剪切和星系聚类的联合分析，将参数空间增加到167个维度。我们展示了我们的方法在计算速度和准确性方面优于现有的实现，使我们能够在几分钟而不是几天内产生后验和参数限制。

    We present CosmoPower-JAX, a JAX-based implementation of the CosmoPower framework, which accelerates cosmological inference by building neural emulators of cosmological power spectra. We show how, using the automatic differentiation, batch evaluation and just-in-time compilation features of JAX, and running the inference pipeline on graphics processing units (GPUs), parameter estimation can be accelerated by orders of magnitude with advanced gradient-based sampling techniques. These can be used to efficiently explore high-dimensional parameter spaces, such as those needed for the analysis of next-generation cosmological surveys. We showcase the accuracy and computational efficiency of CosmoPower-JAX on two simulated Stage IV configurations. We first consider a single survey performing a cosmic shear analysis totalling 37 model parameters. We validate the contours derived with CosmoPower-JAX and a Hamiltonian Monte Carlo sampler against those derived with a nested sampler and without em
    
[^4]: 面向非线性动态系统辨识的频率支持神经网络

    Frequency-Supported Neural Networks for Nonlinear Dynamical System Identification. (arXiv:2305.06344v1 [cs.LG])

    [http://arxiv.org/abs/2305.06344](http://arxiv.org/abs/2305.06344)

    本文提出了一种新的神经网络结构——频率支持神经网络，它通过加入频率信息适应于非线性系统辨识任务，并在多个任务中表现出优越性。

    

    神经网络是一种非常通用的模型，能够学习多个变量之间的各种关系。其中一种关系在实践中非常有趣，即非线性系统的输入输出关系，具有多种应用。研究能够估计这种关系的模型是一门广泛的学科，具有许多理论和实际结果。神经网络非常通用，但存在多种特殊情况，包括卷积神经网络和循环神经网络，分别用于特定的应用，即图像和序列处理。我们提出了一个假设，即通过将频率信息纳入到通用网络结构中来调整其结构，应该可以得到一种专门适用于非线性系统辨识的网络。此外，我们还表明，从理论上讲，可以添加这种频率信息而不会损失通用性。我们称这种新结构为频率支持神经网络（FSNN），并在多个非线性动态系统辨识任务中展示了其优越性。

    Neural networks are a very general type of model capable of learning various relationships between multiple variables. One example of such relationships, particularly interesting in practice, is the input-output relation of nonlinear systems, which has a multitude of applications. Studying models capable of estimating such relation is a broad discipline with numerous theoretical and practical results. Neural networks are very general, but multiple special cases exist, including convolutional neural networks and recurrent neural networks, which are adjusted for specific applications, which are image and sequence processing respectively. We formulate a hypothesis that adjusting general network structure by incorporating frequency information into it should result in a network specifically well suited to nonlinear system identification. Moreover, we show that it is possible to add this frequency information without the loss of generality from a theoretical perspective. We call this new st
    
[^5]: 将生成AI与主动学习框架相结合来优化药物设计

    Optimizing Drug Design by Merging Generative AI With Active Learning Frameworks. (arXiv:2305.06334v1 [q-bio.BM])

    [http://arxiv.org/abs/2305.06334](http://arxiv.org/abs/2305.06334)

    该论文提出了一种基于主动学习的生成AI工作流程，可以克服当前生成AI方法的局限性，去设计出具有高预测亲和力的可行化学分子。

    

    传统的药物研发项目正在通过机器学习方法进行转型。其中，生成AI方法因其设计新分子和增强现有分子特性的能力而受到关注。然而，当前的生成AI方法存在局限性，例如对目标的亲和力低，ADME/PK特性未知或缺乏合成可追溯性等。为了提高生成AI方法的适用范围，我们开发了基于变分自编码器和主动学习步骤的工作流程。设计的生成AI工作流程从分子指标，包括药物相似性、可合成性、相似性和对接得分中迭代学习。此外，我们还在最后的选择步骤中包括了一组基于先进的分子建模模拟的层次化标准。我们在两个模型系统CDK2和KRAS上测试了我们的生成AI工作流程。在两种情况下，我们的模型生成了具有高预测亲和力的可行化学分子。

    Traditional drug discovery programs are being transformed by the advent of machine learning methods. Among these, Generative AI methods (GM) have gained attention due to their ability to design new molecules and enhance specific properties of existing ones. However, current GM methods have limitations, such as low affinity towards the target, unknown ADME/PK properties, or the lack of synthetic tractability. To improve the applicability domain of GM methods, we have developed a workflow based on a variational autoencoder coupled with active learning steps. The designed GM workflow iteratively learns from molecular metrics, including drug likeliness, synthesizability, similarity, and docking scores. In addition, we also included a hierarchical set of criteria based on advanced molecular modeling simulations during a final selection step. We tested our GM workflow on two model systems, CDK2 and KRAS. In both cases, our model generated chemically viable molecules with a high predicted aff
    
[^6]: 神经网络模型的相似度：功能和表示性测量的综述

    Similarity of Neural Network Models: A Survey of Functional and Representational Measures. (arXiv:2305.06329v1 [cs.LG])

    [http://arxiv.org/abs/2305.06329](http://arxiv.org/abs/2305.06329)

    本文综述了神经网络模型相似度的两个观点：表示性相似和功能相似，提供了这两个家族的详细描述，并总结和讨论了其属性和关系，并提出了实践建议。

    

    衡量神经网络的相似性已成为一个非常重要且备受研究关注的问题，以了解和利用神经网络的差异。虽然有几种观点可以描述神经网络的相似性，但是本文特别关注两个互补的观点，即(i) 表示性相似，考虑中间神经层的激活差异，和(ii) 功能相似，考虑模型输出的差异。在本文中，我们全面概述了这两个神经网络模型相似性测量的家族。除了提供现有测量的详细描述外，我们还总结和讨论了这些测量的属性和关系，并指出了开放的研究问题。此外，我们提供了实用建议，可以指导研究人员和实践者利用这些测量。我们希望本文为我们的社区参与更多有用的工作奠定基础。

    Measuring similarity of neural networks has become an issue of great importance and research interest to understand and utilize differences of neural networks. While there are several perspectives on how neural networks can be similar, we specifically focus on two complementing perspectives, i.e., (i) representational similarity, which considers how activations of intermediate neural layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In this survey, we provide a comprehensive overview of these two families of similarity measures for neural network models. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties and relationships of these measures, and point to open research problems. Further, we provide practical recommendations that can guide researchers as well as practitioners in applying the measures. We hope our work lays a foundation for our community to engage in more s
    
[^7]: AGD和MoE用于集成多模态感知

    Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])

    [http://arxiv.org/abs/2305.06324](http://arxiv.org/abs/2305.06324)

    本文提出了集成多模态感知（IMP）方法，将多模态输入集成到单个编码器中，采用交替梯度下降法（AGD）和混合专家（MoE）相结合的方法实现高效的模型和任务扩展，取得了在多个基准测试中具有竞争力的性能表现。

    

    本文提出了一种简单可扩展的多模态多任务训练和建模方法——集成多模态感知（IMP）。IMP将图像、视频、文本和音频等多模态输入集成到单个Transformer编码器中，并具有最小的模态特定组件。IMP使用了一种新颖的设计，将交替梯度下降法（AGD）和混合专家（MoE）相结合，以实现高效的模型和任务扩展。通过广泛的实证研究，我们揭示了以下关键见解：1）在多样化的异构模态、损失函数和任务上交替执行梯度下降更新，并同时改变输入分辨率，可以有效提高多模态理解。2）在单一的模态不可知编码器上使用MoE进行模型稀疏化可以显著提高性能，胜过使用模态特定编码器或额外融合层的稠密模型，并大大缓解模态之间的冲突。IMP在三个多模态基准测试中取得了具有竞争力的性能，胜过了大部分已发表的方法。

    We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model \& task scaling. We conduct extensive empirical studies about IMP and reveal the following key insights: 1) performing gradient descent updates by alternating on diverse heterogeneous modalities, loss functions, and tasks, while also varying input resolutions, efficiently improves multimodal understanding. 2) model sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigating the conflicts between modalities. IMP achieves competitive p
    
[^8]: NervePool: 一个单纯复形池化层

    NervePool: A Simplicial Pooling Layer. (arXiv:2305.06315v1 [cs.CG])

    [http://arxiv.org/abs/2305.06315](http://arxiv.org/abs/2305.06315)

    单纯复形池化层NervePool在池化图结构数据时，基于顶点分区生成单纯复形的分层表示，可以灵活地建模更高阶的关系，同时缩小高维单纯形，实现降采样，减少计算成本和减少过拟合。

    

    对于图结构数据的深度学习问题，池化层对于降采样、减少计算成本和减少过拟合都很重要。我们提出了一个池化层，NervePool，适用于单纯复形结构的数据，这种结构是图的推广，包括比顶点和边更高维度的单纯形；这种结构可以更灵活地建模更高阶的关系。所提出的单纯复合缩小方案基于顶点的分区构建，这使得我们可以生成单纯复形的分层表示，以一种学习的方式折叠信息。NervePool建立在学习的顶点群集分配的基础上，并以一种确定性的方式扩展到高维单纯形的缩小。虽然在实践中，池化操作是通过一系列矩阵运算来计算的，但是其拓扑动机是一个基于单纯形星星的并集和神经复合体的集合构造。

    For deep learning problems on graph-structured data, pooling layers are important for down sampling, reducing computational cost, and to minimize overfitting. We define a pooling layer, NervePool, for data structured as simplicial complexes, which are generalizations of graphs that include higher-dimensional simplices beyond vertices and edges; this structure allows for greater flexibility in modeling higher-order relationships. The proposed simplicial coarsening scheme is built upon partitions of vertices, which allow us to generate hierarchical representations of simplicial complexes, collapsing information in a learned fashion. NervePool builds on the learned vertex cluster assignments and extends to coarsening of higher dimensional simplices in a deterministic fashion. While in practice, the pooling operations are computed via a series of matrix operations, the topological motivation is a set-theoretic construction based on unions of stars of simplices and the nerve complex
    
[^9]: Scan2LoD3: 使用射线投射和贝叶斯网络重建具有语义信息的LoD3三维建筑模型

    Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks. (arXiv:2305.06314v1 [cs.CV])

    [http://arxiv.org/abs/2305.06314](http://arxiv.org/abs/2305.06314)

    本文提出一种称为Scan2LoD3的新方法，通过改进外墙层次的语义三维分割来精确重建具有语义信息的LoD3建筑模型。

    

    在精细程度为LoD3的级别上重建带有语义信息的三维建筑模型一直是一个长期存在的挑战。与基于网格的模型不同，这些模型需要具有完全的几何形状和外墙层次的物体语义信息。这种要求严格的语义三维重建的主要挑战在于可靠地在三维输入数据的外墙层次进行语义分割。我们提出了一种称为Scan2LoD3的新方法，通过改进外墙层次的语义三维分割来精确重建具有语义信息的LoD3建筑模型。为此，我们利用激光物理学和三维建筑模型的先验知识来概率地识别模型冲突。这些概率物理冲突提出了模型开口的位置：它们的最终语义和形状是通过融合冲突、三维点云和二维图像的多模态概率图的贝叶斯网络推断出来的。为了满足严格的LoD3要求，我们利用估计的形状在三维建筑先验中切割出开口，并从库中适配语义三维对象。

    Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge. Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\c{c}ade level. The principal challenge of such demanding semantic 3D reconstruction is reliable fa\c{c}ade-level semantic segmentation of 3D input data. We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\c{c}ade-level semantic 3D segmentation. To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts. These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a libra
    
[^10]: 使用深度强化学习从电子病历中提取诊断路径

    Extracting Diagnosis Pathways from Electronic Health Records Using Deep Reinforcement Learning. (arXiv:2305.06295v1 [cs.LG])

    [http://arxiv.org/abs/2305.06295](http://arxiv.org/abs/2305.06295)

    本文采用深度强化学习算法，基于电子病历来学习获得正确诊断所需的观察序列的最优顺序。因为诊断指南的缺陷，尤其对罕见病或患有多种病的患者，DRL算法具有重要现实意义。

    

    临床诊断指南旨在说明可能导致诊断的步骤。指南能够理性地规范化临床决策，但由于它们的建立是为了覆盖大多数人群，因此在指导罕见病或患有多种病的患者获得正确诊断方面，存在缺陷。本文受指南启发，将诊断任务形式化为序列决策问题，并研究了使用电子病历(EHRs)训练的深度强化学习算法来学习获得正确诊断所需的观察序列的最优顺序。由于DRL算法的多样性和对上下文的敏感性，我们考虑了几种方法和设置，并将它们与彼此和经典分类器进行了比较。我们在一个合成但逼真的数据集上进行了实验。

    Clinical diagnosis guidelines aim at specifying the steps that may lead to a diagnosis. Guidelines enable rationalizing and normalizing clinical decisions but suffer drawbacks as they are built to cover the majority of the population and may fail in guiding to the right diagnosis for patients with uncommon conditions or multiple pathologies. Moreover, their updates are long and expensive, making them unsuitable to emerging practices. Inspired by guidelines, we formulate the task of diagnosis as a sequential decision-making problem and study the use of Deep Reinforcement Learning (DRL) algorithms trained on Electronic Health Records (EHRs) to learn the optimal sequence of observations to perform in order to obtain a correct diagnosis. Because of the variety of DRL algorithms and of their sensitivity to the context, we considered several approaches and settings that we compared to each other, and to classical classifiers. We experimented on a synthetic but realistic dataset to differenti
    
[^11]: 联合度量重要性：轨迹预测的更好标准

    Joint Metrics Matter: A Better Standard for Trajectory Forecasting. (arXiv:2305.06292v1 [cs.RO])

    [http://arxiv.org/abs/2305.06292](http://arxiv.org/abs/2305.06292)

    这项研究展示了边际度量标准不足以完全捕捉多个互动代理的联合表现，提出使用联合度量标准进行轨迹预测方法评估的重要性。

    

    多模态轨迹预测方法通常使用单个代理度量标准（边际度量标准），例如最小平均位移误差（ADE）和最终位移误差（FDE）进行评估。然而，这些度量标准无法捕捉多个互动代理的联合表现。仅关注边际度量标准可能会导致不自然的预测，如碰撞轨迹或明显一起行走的人的发散轨迹。因此，针对边际度量标准优化的方法会导致过于乐观的性能估计，这对轨迹预测研究的进展有害。为了应对边际度量标准的局限性，我们首次全面评估了最先进的轨迹预测方法与多代理度量标准（联合度量标准）之间的关系：JADE、JFDE和碰撞率。我们通过量化证据和定性示例展示了联合度量标准的重要性，而不是边际度量标准。

    Multi-modal trajectory forecasting methods commonly evaluate using single-agent metrics (marginal metrics), such as minimum Average Displacement Error (ADE) and Final Displacement Error (FDE), which fail to capture joint performance of multiple interacting agents. Only focusing on marginal metrics can lead to unnatural predictions, such as colliding trajectories or diverging trajectories for people who are clearly walking together as a group. Consequently, methods optimized for marginal metrics lead to overly-optimistic estimations of performance, which is detrimental to progress in trajectory forecasting research. In response to the limitations of marginal metrics, we present the first comprehensive evaluation of state-of-the-art (SOTA) trajectory forecasting methods with respect to multi-agent metrics (joint metrics): JADE, JFDE, and collision rate. We demonstrate the importance of joint metrics as opposed to marginal metrics with quantitative evidence and qualitative examples drawn 
    
[^12]: 学习基于视频的无人任务策略

    Learning Video-Conditioned Policies for Unseen Manipulation Tasks. (arXiv:2305.06289v1 [cs.RO])

    [http://arxiv.org/abs/2305.06289](http://arxiv.org/abs/2305.06289)

    该论文提出了一种名为 ViP 的新方法，将人类的任务演示视频映射到机器人操作技能中，实现了人机交互中指定任务的目标。

    

    实现非专业用户指定机器人命令是构建通用智能体能够解决各种任务的关键。通过人示范目标任务的视频来指定目标机器人目标是一种方便的方式，我们提出了基于视频的策略学习（ViP），将已经看不见的任务的人示范映射到机器人操作技能。

    The ability to specify robot commands by a non-expert user is critical for building generalist agents capable of solving a large variety of tasks. One convenient way to specify the intended robot goal is by a video of a person demonstrating the target task. While prior work typically aims to imitate human demonstrations performed in robot environments, here we focus on a more realistic and challenging setup with demonstrations recorded in natural and diverse human environments. We propose Video-conditioned Policy learning (ViP), a data-driven approach that maps human demonstrations of previously unseen tasks to robot manipulation skills. To this end, we learn our policy to generate appropriate actions given current scene observations and a video of the target task. To encourage generalization to new tasks, we avoid particular tasks during training and learn our policy from unlabelled robot trajectories and corresponding robot videos. Both robot and human videos in our framework are rep
    
[^13]: 云无线接入网中的垂直联邦学习：收敛分析与系统优化

    Vertical Federated Learning over Cloud-RAN: Convergence Analysis and System Optimization. (arXiv:2305.06279v1 [cs.IT])

    [http://arxiv.org/abs/2305.06279](http://arxiv.org/abs/2305.06279)

    提出了一种基于云无线接入网的垂直联邦学习系统，通过利用空中计算和协作模型聚合来实现快速而精确的模型聚合。为了解决由空中计算和前程传输容量限制引起的误差问题，对该系统进行了收敛分析和系统优化。

    

    垂直联邦学习是一种协作机器学习框架，它使得设备可以从特征分区数据集中学习全局模型，而无需共享本地原始数据。本文提出了一种基于云无线接入网的垂直联邦学习系统，旨在通过利用空中计算和通过协作模型聚合解决地理分布式边缘服务器之间的通信延迟问题，实现快速而准确的模型聚合。然而，空中计算引起的模型聚合误差以及由于限制的前程传输容量引起的量化误差会降低垂直联邦学习的学习性能。为了解决这些问题，我们对收敛分析和系统优化问题进行了研究。

    Vertical federated learning (FL) is a collaborative machine learning framework that enables devices to learn a global model from the feature-partition datasets without sharing local raw data. However, as the number of the local intermediate outputs is proportional to the training samples, it is critical to develop communication-efficient techniques for wireless vertical FL to support high-dimensional model aggregation with full device participation. In this paper, we propose a novel cloud radio access network (Cloud-RAN) based vertical FL system to enable fast and accurate model aggregation by leveraging over-the-air computation (AirComp) and alleviating communication straggler issue with cooperative model aggregation among geographically distributed edge servers. However, the model aggregation error caused by AirComp and quantization errors caused by the limited fronthaul capacity degrade the learning performance for vertical FL. To address these issues, we characterize the convergenc
    
[^14]: FedPDD：用于跨平台联邦推荐的隐私保护双重蒸馏框架

    FedPDD: A Privacy-preserving Double Distillation Framework for Cross-silo Federated Recommendation. (arXiv:2305.06272v1 [cs.IR])

    [http://arxiv.org/abs/2305.06272](http://arxiv.org/abs/2305.06272)

    本文提出了一个名为FedPDD的隐私保护双重蒸馏框架，用于跨平台联邦推荐。该框架包括教师蒸馏和学生蒸馏两个阶段，在不传输模型信息的情况下，通过有效地转移知识和使用一种新的蒸馏损失函数来构建全局模型，实现了最先进的性能。

    

    跨平台推荐旨在通过从不同平台收集异构特征来提高推荐准确性。然而，越来越严格的隐私保护法规限制了这种平台间的跨界协作，因此不能聚合数据用于训练。联邦学习（FL）是推荐场景中处理数据孤岛问题的实用解决方案。现有的跨平台FL方法通过利用重叠用户的数据协同构建全局模型，传输模型信息。然而，在现实中，重叠用户的数量往往非常小，从而大大限制了此类方法的性能。此外，训练期间传输模型信息需要高通信成本，可能会造成严重的隐私泄露。本文提出了一种新的隐私保护双重蒸馏框架 FedPDD 用于跨平台联邦推荐，该框架通过有效地转移知识来保护隐私。FedPDD 包括两个阶段：教师蒸馏和学生蒸馏。在教师蒸馏阶段，每个平台在自己的数据上训练本地模型，并将来自这些模型的知识蒸馏到一个小的、带有噪声的教师模型中。然后，在学生蒸馏阶段，每个平台通过一种新的蒸馏损失函数，同时从教师模型和本地数据中学习，训练自己的学生模型。FedPDD 在两个真实世界的跨平台联邦推荐数据集上实现了最先进的性能，同时保护隐私。

    Cross-platform recommendation aims to improve recommendation accuracy by gathering heterogeneous features from different platforms. However, such cross-silo collaborations between platforms are restricted by increasingly stringent privacy protection regulations, thus data cannot be aggregated for training. Federated learning (FL) is a practical solution to deal with the data silo problem in recommendation scenarios. Existing cross-silo FL methods transmit model information to collaboratively build a global model by leveraging the data of overlapped users. However, in reality, the number of overlapped users is often very small, thus largely limiting the performance of such approaches. Moreover, transmitting model information during training requires high communication costs and may cause serious privacy leakage. In this paper, we propose a novel privacy-preserving double distillation framework named FedPDD for cross-silo federated recommendation, which efficiently transfers knowledge wh
    
[^15]: 基于深度强化学习的云原生无线网络资源分配研究

    Deep Reinforcement Learning Based Resource Allocation for Cloud Native Wireless Network. (arXiv:2305.06249v1 [cs.NI])

    [http://arxiv.org/abs/2305.06249](http://arxiv.org/abs/2305.06249)

    本文研究了一种基于云原生无线架构的资源分配方法，针对网络切片和多接入边缘计算等场景，利用深度强化学习技术提出了两个无模型算法，能够动态训练分配策略，有效提高了网络效率。

    

    云原生技术革命性地改变了5G及6G通信网络，提供了前所未有的运营自动化、灵活性和适应性。然而，云原生服务和应用的广泛实现给动态云计算环境下的资源分配提出了新的挑战。为了解决这个问题，我们研究了一种使用基于容器的虚拟化实现灵活服务部署的云原生无线架构，并研究了两个代表性应用场景：网络切片和多接入边缘计算。为了优化这些场景下的资源分配，我们利用深度强化学习技术，引入了两个无模型算法，能够监控网络状态并动态训练分配策略。我们使用Free5gc开发的测试平台验证了算法的有效性。我们的实验结果表明，在提高网络效率方面，云原生技术和深度强化学习具有巨大的潜力。

    Cloud native technology has revolutionized 5G beyond and 6G communication networks, offering unprecedented levels of operational automation, flexibility, and adaptability. However, the vast array of cloud native services and applications presents a new challenge in resource allocation for dynamic cloud computing environments. To tackle this challenge, we investigate a cloud native wireless architecture that employs container-based virtualization to enable flexible service deployment. We then study two representative use cases: network slicing and Multi-Access Edge Computing. To optimize resource allocation in these scenarios, we leverage deep reinforcement learning techniques and introduce two model-free algorithms capable of monitoring the network state and dynamically training allocation policies. We validate the effectiveness of our algorithms in a testbed developed using Free5gc. Our findings demonstrate significant improvements in network efficiency, underscoring the potential of 
    
[^16]: 关于实例相关标签噪声学习价值的反思

    Rethinking the Value of Labels for Instance-Dependent Label Noise Learning. (arXiv:2305.06247v1 [cs.LG])

    [http://arxiv.org/abs/2305.06247](http://arxiv.org/abs/2305.06247)

    本文提出了一种利用深度生成模型和因果表征学习处理实例相关标签噪声问题的新算法，能很好地识别高层次的内容和风格潜在因素，并在合成和真实数据集上验证了其有效性。

    

    在大规模数据集中，标签噪声普遍存在，大大降低了深度学习算法的性能。由于实例相关噪声转移矩阵的不可识别性，大部分现有算法通过假设噪声标签生成过程与实例特征无关来解决该问题。然而，在真实世界的应用中，噪声标签通常取决于真实标签和特征。本文提出了一种新的深度生成模型来处理实例相关标签噪声问题，避免了明确建模噪声转移矩阵。我们的算法利用因果表征学习，同时从数据中识别高层次的内容和风格潜在因素。通过利用结构因果模型中的噪声标签监督信息，我们在广泛的合成和真实世界的实例相关标签噪声数据集上进行了实证评估，证明了所提出算法的显著性能。

    Label noise widely exists in large-scale datasets and significantly degenerates the performances of deep learning algorithms. Due to the non-identifiability of the instance-dependent noise transition matrix, most existing algorithms address the problem by assuming the noisy label generation process to be independent of the instance features. Unfortunately, noisy labels in real-world applications often depend on both the true label and the features. In this work, we tackle instance-dependent label noise with a novel deep generative model that avoids explicitly modeling the noise transition matrix. Our algorithm leverages casual representation learning and simultaneously identifies the high-level content and style latent factors from the data. By exploiting the supervision information of noisy labels with structural causal models, our empirical evaluations on a wide range of synthetic and real-world instance-dependent label noise datasets demonstrate that the proposed algorithm significa
    
[^17]: 可解释的知识蒸馏用于设备内胸部X光分类

    Explainable Knowledge Distillation for On-device Chest X-Ray Classification. (arXiv:2305.06244v1 [cs.CV])

    [http://arxiv.org/abs/2305.06244](http://arxiv.org/abs/2305.06244)

    该论文提出了一种利用可解释的人工智能（XAI）技术和知识蒸馏（KD）策略，创建用于设备内多标签CXr图像分类的紧凑深度学习模型，在三个基准CXr数据集上表现出更好的性能。

    

    利用复杂的深度学习方法进行自动多标签胸部X射线（CXR）图像分类已经在临床诊断中取得了显著进展。然而，大多数深度模型具有高计算需求，这使得它们不太适合低计算需求的小型设备。为了解决这个问题，我们提出了一种知识蒸馏（KD）策略，用于创建用于实时多标签CXR图像分类的紧凑深度学习模型。我们研究了不同的CNN和Transforms作为teacher来向较小的student蒸馏知识。然后，我们采用可解释的人工智能（XAI）来提供模型决策的可视化解释，从而改善KD。我们在三个基准CXR数据集上的结果表明，我们的KD策略提供了紧凑学生模型的改进性能，因此是许多有限硬件平台的可行选择。例如，当以DenseNet161作为teacher模型时，我们的方法将模型大小从115 MB减小到5 MB，评估时间加速了4倍。

    Automated multi-label chest X-rays (CXR) image classification has achieved substantial progress in clinical diagnosis via utilizing sophisticated deep learning approaches. However, most deep models have high computational demands, which makes them less feasible for compact devices with low computational requirements. To overcome this problem, we propose a knowledge distillation (KD) strategy to create the compact deep learning model for the real-time multi-label CXR image classification. We study different alternatives of CNNs and Transforms as the teacher to distill the knowledge to a smaller student. Then, we employed explainable artificial intelligence (XAI) to provide the visual explanation for the model decision improved by the KD. Our results on three benchmark CXR datasets show that our KD strategy provides the improved performance on the compact student model, thus being the feasible choice for many limited hardware platforms. For instance, when using DenseNet161 as the teacher
    
[^18]: 带有一般损失函数的惩罚深度神经网络估计器在弱相关性下的应用

    Penalized deep neural networks estimator with general loss functions under weak dependence. (arXiv:2305.06230v1 [stat.ML])

    [http://arxiv.org/abs/2305.06230](http://arxiv.org/abs/2305.06230)

    本文提出了一种稀疏惩罚深度神经网络预测方法，适用于学习弱相关性过程，并在特定情况下使用$\theta_\infty$系数。文中还提供了相应的神谕不等式和收敛速度，对于目标函数足够光滑的情况，超额风险的收敛速度接近于$\mathcal{O}(n^{-1/3})$。其中提供了模拟结果和应用于预测Vitória颗粒物的案例。

    

    本文使用广泛的损失函数，对于学习弱相关性过程进行了稀疏惩罚深度神经网络预测。我们处理包括回归估计、分类、时间序列预测在内的一般框架，考虑了$\psi$弱相关结构，并针对有界观测的特定情况，也使用了$\theta_\infty$系数。在这种$\theta_\infty$弱相依的情况下，提供了一种在深度神经网络预测器类中提供非渐近泛化界限。对于学习$\psi$和$\theta_\infty$弱相关的过程，确定了稀疏惩罚深度神经网络估计器超额风险的神谕不等式。当目标函数足够光滑时，这些超额风险的收敛速度接近于$\mathcal{O}(n^{-1/3})$。提供了一些模拟结果，并应用于Vitória颗粒物的预测。

    This paper carries out sparse-penalized deep neural networks predictors for learning weakly dependent processes, with a broad class of loss functions. We deal with a general framework that includes, regression estimation, classification, times series prediction, $\cdots$ The $\psi$-weak dependence structure is considered, and for the specific case of bounded observations, $\theta_\infty$-coefficients are also used. In this case of $\theta_\infty$-weakly dependent, a non asymptotic generalization bound within the class of deep neural networks predictors is provided. For learning both $\psi$ and $\theta_\infty$-weakly dependent processes, oracle inequalities for the excess risk of the sparse-penalized deep neural networks estimators are established. When the target function is sufficiently smooth, the convergence rate of these excess risk is close to $\mathcal{O}(n^{-1/3})$. Some simulation results are provided, and application to the forecast of the particulate matter in the Vit\'{o}ria
    
[^19]: 补丁学习：实现跨不同生物医学数据源的综合分析的范式。

    Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])

    [http://arxiv.org/abs/2305.06217](http://arxiv.org/abs/2305.06217)

    补丁学习是一种新的范式，通过整合来自不同数据源的信息，解决了数据隐私、异构数据来源和无法充分利用多个数据模态的挑战。它可以同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。

    

    在医疗保健领域中，机器学习（ML）提供了许多增强患者护理、人口健康和医疗保健提供者工作流程的机会。然而，由于数据隐私、异构数据来源和无法充分利用多个数据模态的挑战，现实中的临床和成本效益仍然有限。在这篇观点论文中，我们介绍了“补丁学习”（PL），这是一种新的范式，通过集成来自不同数据来源（例如，临床免费文本、医学图像、组学）和分布在不同安全站点上的不同数据模态的不同数据集的信息来解决这些限制。PL允许同时利用互补的数据来源，同时保护数据隐私，从而实现开发更全面和具有普适性的ML模型。我们介绍了补丁学习的概念以及其在医疗保健领域的当前实现，探讨了解决各种问题的潜在机会和适用数据来源。

    Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce "patchwork learning" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various
    
[^20]: 无序多物体导航

    Sequence-Agnostic Multi-Object Navigation. (arXiv:2305.06178v1 [cs.RO])

    [http://arxiv.org/abs/2305.06178](http://arxiv.org/abs/2305.06178)

    该论文提出了一种无序多物体导航算法，该算法利用深度强化学习框架，通过适当的奖励规范，奖励单个和多个目标对象类别的进展，实现了对多个对象类别的准确定位，适用于动态变化的实际应用。

    

    多物体导航 (MultiON) 任务要求机器人定位多种物体类别的实例。这是家庭或工厂辅助机器人的基本任务。已有的 MultiON 方法将此视为对象导航 (ON) 的直接扩展，即本文所述的 ON 任务需要提前指定探索对象类别的顺序。这在动态变化的实际应用中具有很大的局限性。本文描述了一种基于演员-评论家体系结构和适当的奖励规范的深度强化学习框架，用于无序 MultiON。我们的框架利用过去的经验，并试图奖励单个和多个目标对象类别的进展。我们在 AI Habitat 3D 模拟环境中使用 Gibson 基准数据集中的照片逼真场景进行实验，结果表明我们的方法性能更好。

    The Multi-Object Navigation (MultiON) task requires a robot to localize an instance (each) of multiple object classes. It is a fundamental task for an assistive robot in a home or a factory. Existing methods for MultiON have viewed this as a direct extension of Object Navigation (ON), the task of localising an instance of one object class, and are pre-sequenced, i.e., the sequence in which the object classes are to be explored is provided in advance. This is a strong limitation in practical applications characterized by dynamic changes. This paper describes a deep reinforcement learning framework for sequence-agnostic MultiON based on an actor-critic architecture and a suitable reward specification. Our framework leverages past experiences and seeks to reward progress toward individual as well as multiple target object classes. We use photo-realistic scenes from the Gibson benchmark dataset in the AI Habitat 3D simulation environment to experimentally show that our method performs bett
    
[^21]: 通过生成对抗反馈对语言模型进行微调

    Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])

    [http://arxiv.org/abs/2305.06176](http://arxiv.org/abs/2305.06176)

    本研究探讨了一种新的方法，使用生成对抗反馈的强化学习(RLGAF)对大型语言模型进行微调，以取代仅受人类反馈的强化学习(RLHF)，从而消除评估者的专业限制并提高性能。

    

    通过人类反馈的强化学习已经显著提高了大型语言模型(LLMs)的性能，使其输出与人类期望的价值观保持一致。然而，RLHF受到人类评估者的专业知识和生产力限制。在本研究中，我们研究了一种替代方法: 使用生成对抗反馈的强化学习(RLGAF)代替RLHF。我们的初步发现表明，RLGAF可以帮助对齐LLM的输出，同时不会受到RLHF固有的限制，为进一步自动化AI对齐的研究提供了有希望的途径。

    Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
    
[^22]: 使用贝叶斯模型平均分析社交媒体上的气候宣传活动

    Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])

    [http://arxiv.org/abs/2305.06174](http://arxiv.org/abs/2305.06174)

    本文分析了工业、倡导组织和气候倡导组织在社交媒体上如何影响气候变化的叙事，并提出了一个最小化监督模型组合方法，用于识别Facebook上气候广告的立场。

    

    气候变化是我们时代的核心问题，我们正处于一个关键时刻。各种利益集团、社会运动组织和个人在社交媒体上开展针对这个问题的集体行动。此外，社交媒体上的问题倡导活动往往是针对当前社会关注的问题，特别是能源行业面临的问题。本文的目标是分析工业、倡导组织和气候倡导组织如何利用社交媒体影响气候变化的叙事。在这项工作中，我们提出了一个最小化监督模型组合方法，并结合消息主题来识别Facebook上气候广告的立场。最后，我们发布了我们的立场数据集、模型和与气候宣传活动相关的主题，供未来的舆情挖掘和自动检测气候变化立场的研究使用。

    Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [56] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.
    
[^23]: K-SpecPart: 一种用于多元超图划分解决方案改进的监督谱框架

    K-SpecPart: A Supervised Spectral Framework for Multi-Way Hypergraph Partitioning Solution Improvement. (arXiv:2305.06167v1 [cs.LG])

    [http://arxiv.org/abs/2305.06167](http://arxiv.org/abs/2305.06167)

    本论文提出了一个名为K-SpecPart的超图划分算法，通过解决广义特征值问题，捕捉平衡分区目标和全局超图结构，在多元划分中提高算法质量。

    

    现有的超图划分器采用多层次策略，构建多个更粗糙的超图来进行切割尺寸的优化，但存在一些局限性：（一）粗化过程依赖于局部邻域结构，忽略了全局超图结构；（二）优化启发式算法存在陷入局部最小值的风险。本文提出了一种新的监督谱框架——K-SpecPart，通过解决广义特征值问题解决了这些限制，捕捉了平衡分区目标和全局超图结构，并利用高质量的多层次划分方案作为提示。在多元划分的情况下，K-SpecPart从多元提示划分方案中获得多个双向划分方案。将这些方案整合到广义特征值问题中以计算特征向量，从而创建大维度嵌入。线性判别分析（LDA）用于将其转换为低维嵌入。

    State-of-the-art hypergraph partitioners follow the multilevel paradigm, constructing multiple levels of coarser hypergraphs to drive cutsize refinement. These partitioners face limitations: (i) coarsening processes depend on local neighborhood structure, ignoring global hypergraph structure; (ii) refinement heuristics risk entrapment in local minima. We introduce K-SpecPart, a supervised spectral framework addressing these limitations by solving a generalized eigenvalue problem, capturing balanced partitioning objectives and global hypergraph structure in a low-dimensional vertex embedding while leveraging high-quality multilevel partitioning solutions as hints. In multi-way partitioning, K-SpecPart derives multiple bipartitioning solutions from a multi-way hint partitioning solution. It integrates these solutions into the generalized eigenvalue problem to compute eigenvectors, creating a large-dimensional embedding. Linear Discriminant Analysis (LDA) is used to transform this into a 
    
[^24]: EdgeNet：电子商务在线广告竞价设计的编码器-解码器生成网络

    EdgeNet : Encoder-decoder generative Network for Auction Design in E-commerce Online Advertising. (arXiv:2305.06158v1 [cs.IR])

    [http://arxiv.org/abs/2305.06158](http://arxiv.org/abs/2305.06158)

    EdgeNet是一种编码器-解码器生成网络，可用于在线电商广告拍卖中的数据驱动竞价设计。与传统模型相比，EdgeNet 可以更好地捕捉广告间的相互影响，并利用丰富的上下文信息，提高广告竞拍的效率。

    

    我们提出了一种新的编码器-解码器生成网络EdgeNet，引入了一种新颖的编码器-解码器框架，用于在线电子商务广告中数据驱动的拍卖设计。我们打破了广义次高价（GSP）的神经拍卖范式，提高了数据利用效率，同时确保了拍卖机制的经济特征。具体而言，EdgeNet引入了基于transformer的编码器来更好地捕捉不同广告间的相互作用。与基于GSP的神经拍卖模型相比，我们设计了一个自回归解码器，以更好地利用在线广告竞拍中的丰富上下文信息。EdgeNet在概念上简单易懂，并易于扩展到现有的端到端神经拍卖框架中。我们在广泛的电子商务广告竞拍中验证了EdgeNet的有效性，展示了其在提高用户体验和平台收入方面的潜力。

    We present a new encoder-decoder generative network dubbed EdgeNet, which introduces a novel encoder-decoder framework for data-driven auction design in online e-commerce advertising. We break the neural auction paradigm of Generalized-Second-Price(GSP), and improve the utilization efficiency of data while ensuring the economic characteristics of the auction mechanism. Specifically, EdgeNet introduces a transformer-based encoder to better capture the mutual influence among different candidate advertisements. In contrast to GSP based neural auction model, we design an autoregressive decoder to better utilize the rich context information in online advertising auctions. EdgeNet is conceptually simple and easy to extend to the existing end-to-end neural auction framework. We validate the efficiency of EdgeNet on a wide range of e-commercial advertising auction, demonstrating its potential in improving user experience and platform revenue.
    
[^25]: 利用合成目标进行机器翻译

    Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])

    [http://arxiv.org/abs/2305.06155](http://arxiv.org/abs/2305.06155)

    本文提供了一种利用预训练模型生成合成目标从而提高机器翻译性能的方法，并发现其在不同测试基准下的表现优于使用真实数据训练，这一方法在有限资源的情况下尤其有用。

    

    在本文中，我们提供了一种在有限资源情况下训练机器翻译模型的方法，该方法通过利用通过大型预训练模型生成的合成目标数据。我们表明，在双语、多语言和语音翻译设置的不同基准测试中，使用合成目标训练模型的性能优于使用实际的正确数据训练模型。这种性能差距随着可用资源的限制（数据集大小和模型参数数量）的增加而变得更大。我们还对性能提升是否与优化的便利性或预测的更确定性相关进行了初步分析，以及这种范例是否能够提高在不同测试领域的超出分布性能。

    In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.
    
[^26]: 一种运输行业文档分类的半自动化方法

    A semi-automatic method for document classification in the shipping industry. (arXiv:2305.06148v1 [cs.CL])

    [http://arxiv.org/abs/2305.06148](http://arxiv.org/abs/2305.06148)

    运输行业使用OCR技术对文件进行分类以提高准确性和效率。该研究提出半自动化的方法，基于关键词频率构建一个稳健的文档分类系统，其在收集的85个违约案例和555个非违约案例上实现了93.31%的高分类准确度。

    

    在运输行业中，文档分类对于确保必要的文件被正确识别并处理以通过海关清关流程起着至关重要的作用。OCR技术被用于自动化文档分类过程，其中包括识别商业发票、装箱清单、进出口报关单、提单、海运提单、证书、航空或铁路货运提单、到达通知书、原产地证明、进口商安全申报和信用证等重要文件。通过使用OCR技术，运输行业可以提高文档分类和海关清关流程的准确性和效率。本研究旨在基于关键词频率构建一个稳健的文档分类系统。研究通过分析IN-D公司提供的违约诉讼文档得出，该文档收集自新加坡政府司法网站。数据库中包含85个违约案例和555个非违约案例，提出的方法实现了93.31%的高分类准确度。

    In the shipping industry, document classification plays a crucial role in ensuring that the necessary documents are properly identified and processed for customs clearance. OCR technology is being used to automate the process of document classification, which involves identifying important documents such as Commercial Invoices, Packing Lists, Export/Import Customs Declarations, Bills of Lading, Sea Waybills, Certificates, Air or Rail Waybills, Arrival Notices, Certificate of Origin, Importer Security Filings, and Letters of Credit. By using OCR technology, the shipping industry can improve accuracy and efficiency in document classification and streamline the customs clearance process. The aim of this study is to build a robust document classification system based on keyword frequencies. The research is carried out by analyzing Contract-Breach law documents available with IN-D. The documents were collected by scraping the Singapore Government Judiciary website. The database developed ha
    
[^27]: 图神经网络的特征扩展

    Feature Expansion for Graph Neural Networks. (arXiv:2305.06142v1 [cs.LG])

    [http://arxiv.org/abs/2305.06142](http://arxiv.org/abs/2305.06142)

    本文通过分析图神经网络中的特征空间，提出了特征子空间展开和结构主成分两种方法来扩展特征空间，从而获得更好的结果。

    

    图神经网络旨在学习图结构数据的表示，并展现出令人瞩目的性能，尤其在节点分类方面。然而，支配表示学习的特征空间在图神经网络中尚未被系统地研究。本文提出通过分析空间模型和谱模型的特征空间来填补这一空白。我们将图神经网络分解为确定的特征空间和可训练的权重，从而通过矩阵空间分析明确地研究特征空间。特别地，我们在理论上发现，由于重复聚合，特征空间倾向于线性相关。基于这些发现，我们提出1）特征子空间展开和2）结构主成分来扩展特征空间。广泛的实验验证了我们的方法，取得了更好的结果。

    Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the 
    
[^28]: CrudeBERT：应用经济学理论改进基于Transformer的情感分析模型在原油市场中的应用

    CrudeBERT: Applying Economic Theory towards fine-tuning Transformer-based Sentiment Analysis Models to the Crude Oil Market. (arXiv:2305.06140v1 [cs.IR])

    [http://arxiv.org/abs/2305.06140](http://arxiv.org/abs/2305.06140)

    本研究提出了一种识别及分类影响原油市场供需的新闻事件的方法，并据此设计了CrudeBERT情感分析模型，该模型优于其他专有和开源方案，为原油期货市场相关标题提供情感分类的改进表现。

    

    基于新闻媒体情感来预测市场走向在数据分析中已有悠久传统。随着自然语言处理的进步，出现了可进行上下文感知情感分类的Transformer架构。然而，当前为金融市场制定的方法，如FinBERT，无法区分影响特定资产价值驱动因素。本文通过在大量相关新闻头条中识别和分类影响原油市场供需的事件，介绍了一种解决此问题的方法。我们随后提出了CrudeBERT，这是一个依据这些事件来上下文化并优化FinBERT的情感分析模型，从而提供与原油期货市场相关的标题的情感分类的改进表现。一项广泛的评估展示了CrudeBERT在原油领域胜过专有和开源解决方案。

    Predicting market movements based on the sentiment of news media has a long tradition in data analysis. With advances in natural language processing, transformer architectures have emerged that enable contextually aware sentiment classification. Nevertheless, current methods built for the general financial market such as FinBERT cannot distinguish asset-specific value-driving factors. This paper addresses this shortcoming by presenting a method that identifies and classifies events that impact supply and demand in the crude oil markets within a large corpus of relevant news headlines. We then introduce CrudeBERT, a new sentiment analysis model that draws upon these events to contextualize and fine-tune FinBERT, thereby yielding improved sentiment classifications for headlines related to the crude oil futures market. An extensive evaluation demonstrates that CrudeBERT outperforms proprietary and open-source solutions in the domain of crude oil.
    
[^29]: 火灾传播不确定性估计的神经仿真器

    A Neural Emulator for Uncertainty Estimation of Fire Propagation. (arXiv:2305.06139v1 [cs.LG])

    [http://arxiv.org/abs/2305.06139](http://arxiv.org/abs/2305.06139)

    本论文提出了一种新的神经网络模型，直接估计给定输入参数不确定性的火灾传播概率，省去了繁重的模拟集合，能够更有效地对火灾传播的不确定性进行分析。

    

    森林火灾的传播是一个高度随机的过程，环境条件的微小变化（例如风速和方向）会导致观察到的行为发生巨大变化。传统方法通过模拟集合生成概率图来量化火线传播的不确定性。但使用集合通常计算成本高昂，这会限制不确定性分析的范围。为解决这个问题，我们探索了一种基于时空神经网络建模方法，直接估计给定输入参数不确定性的火灾传播概率。通过在模型训练过程中有意试图扰动输入的天气预报来表示不确定性。计算负载集中于模型训练过程中，这允许在部署期间探索更大的概率空间。经验评估表明，所提出的模型实现了与传统 S*t模拟所产生的相当火灾边界。

    Wildfire propagation is a highly stochastic process where small changes in environmental conditions (such as wind speed and direction) can lead to large changes in observed behaviour. A traditional approach to quantify uncertainty in fire-front progression is to generate probability maps via ensembles of simulations. However, use of ensembles is typically computationally expensive, which can limit the scope of uncertainty analysis. To address this, we explore the use of a spatio-temporal neural-based modelling approach to directly estimate the likelihood of fire propagation given uncertainty in input parameters. The uncertainty is represented by deliberately perturbing the input weather forecast during model training. The computational load is concentrated in the model training process, which allows larger probability spaces to be explored during deployment. Empirical evaluations indicate that the proposed model achieves comparable fire boundaries to those produced by the traditional S
    
[^30]: 多目标优化的逆强化学习的收敛性证明研究

    A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])

    [http://arxiv.org/abs/2305.06137](http://arxiv.org/abs/2305.06137)

    本论文证明了多目标优化的逆强化学习方法在理论层面上的收敛性，包括Wasserstein逆强化学习和常规逆强化学习方法。

    

    本文通过将等效于多目标优化的WIRL问题的逆问题与投影次梯度法相结合，证明了Wasserstein逆强化学习（WIRL）在多目标优化中的收敛性。此外，我们还证明了逆强化学习（最大熵逆强化学习，导引成本学习）在多目标优化中的收敛性。

    We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
    
[^31]: FedDWA: 个性化联邦学习与动态权重调整

    FedDWA: Personalized Federated Learning with Online Weight Adjustment. (arXiv:2305.06124v1 [cs.LG])

    [http://arxiv.org/abs/2305.06124](http://arxiv.org/abs/2305.06124)

    本文提出了一种个性化联邦学习算法，名为FedDWA，采用动态权重调整来保护数据隐私并以更少的通信开销捕捉客户之间的相似性，能够训练高精度和高效的个性化模型。

    

    与传统的联邦学习不同，个性化联邦学习（PFL）能够根据每个客户端的独特需求来训练定制化模型。主流方法是采用一种加权聚合方法来生成个性化模型，其中权重是由不同客户端之间的损失值或模型参数确定的。然而，这种方法要求客户端下载其他模型，不仅增加了通信流量，而且可能侵犯数据隐私。我们在本文中提出了一种新的PFL算法，称为FedDWA（带动态权重调整的联邦学习），来解决上述问题，该算法利用参数服务器（PS）根据从客户端收集的模型计算个性化聚合权重。这样，FedDWA可以以更少的通信开销捕捉客户之间的相似性。我们将PFL问题制定为一种优化问题，并通过引入动态权重调整机制设计了一种新算法。FedDWA能够学习高精度和高效的个性化模型，同时保护数据隐私。我们在综合合成和真实数据集上进行了广泛的实验，证明了FedDWA的有效性。

    Different from conventional federated learning, personalized federated learning (PFL) is able to train a customized model for each individual client according to its unique requirement. The mainstream approach is to adopt a kind of weighted aggregation method to generate personalized models, in which weights are determined by the loss value or model parameters among different clients. However, such kinds of methods require clients to download others' models. It not only sheer increases communication traffic but also potentially infringes data privacy. In this paper, we propose a new PFL algorithm called \emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address the above problem, which leverages the parameter server (PS) to compute personalized aggregation weights based on collected models from clients. In this way, FedDWA can capture similarities between clients with much less communication overhead. More specifically, we formulate the PFL problem as an optimization 
    
[^32]: 基于视频内部和视频间信息最大化的小样本动作识别

    Few-shot Action Recognition via Intra- and Inter-Video Information Maximization. (arXiv:2305.06114v1 [cs.CV])

    [http://arxiv.org/abs/2305.06114](http://arxiv.org/abs/2305.06114)

    本论文提出了一种称为VIM的框架，采用自适应空间-时间视频采样器和时空动作增强器，用于小样本动作识别，充分利用视频内部和视频间信息以提高识别精度。

    

    当前的小样本动作识别涉及两个主要的信息源用于分类: (1) 来自视频片段内部的视频内部信息，由单个视频剪辑中的帧内容确定，以及 (2) 通过视频之间的关系(例如，特征相似性)测量的视频间信息。然而，现有方法未充分利用这两个信息源。关于视频内部信息，当前的输入视频采样操作可能会遗漏关键的动作信息，降低视频数据的利用效率。对于视频间信息，视频之间的动作不对齐使得计算精确关系变得困难。此外，如何共同考虑视频内外信息在小样本动作识别中仍未被充分探索。为此，我们提出了一个新的框架，称为视频信息最大化 (VIM)，用于小样本视频动作识别。VIM配备了一个自适应的时空视频采样器和一个时空动作增强器。

    Current few-shot action recognition involves two primary sources of information for classification:(1) intra-video information, determined by frame content within a single video clip, and (2) inter-video information, measured by relationships (e.g., feature similarity) among videos. However, existing methods inadequately exploit these two information sources. In terms of intra-video information, current sampling operations for input videos may omit critical action information, reducing the utilization efficiency of video data. For the inter-video information, the action misalignment among videos makes it challenging to calculate precise relationships. Moreover, how to jointly consider both inter- and intra-video information remains under-explored for few-shot action recognition. To this end, we propose a novel framework, Video Information Maximization (VIM), for few-shot video action recognition. VIM is equipped with an adaptive spatial-temporal video sampler and a spatiotemporal actio
    
[^33]: XMI-ICU: 可解释的机器学习模型，用于心脏病患者在 ICU 的 伪动态死亡率预测

    XMI-ICU: Explainable Machine Learning Model for Pseudo-Dynamic Prediction of Mortality in the ICU for Heart Attack Patients. (arXiv:2305.06109v1 [cs.LG])

    [http://arxiv.org/abs/2305.06109](http://arxiv.org/abs/2305.06109)

    本研究提出了一种新的伪动态机器学习框架，用于心脏病患者在 ICU 中的死亡率预测，并提供了诊断可解释性和临床风险分析。该方法在预测死亡率时提供了高精度的预测结果和时间分辨率可解释性结果。

    

    心脏病是美国和全球死亡率的主要贡献因素之一。进入重症监护室(ICU)的诊断为心肌梗死或 MI 的患者有更高的死亡风险。本研究使用从 eICU 和 MIMIC-IV 数据库提取的两个回顾性队列，开发了一种新的伪动态机器学习框架，用于在 ICU 中进行死亡率预测，并提供诊断可解释性和临床风险分析。该方法提供了高精度的预测结果，在事件发生前 24 小时为 ICU 患者提供时间分辨率可解释性结果。该框架的性能依赖于极端梯度提升(Extreme Gradient Boosting)，在来自 eICU 的校验测试集上进行了评估，并使用时间分辨率 Shapley 值的最重要特征通过外部验证在 MIMIC-IV 队列上获得了 AUC 为 91.0，分别预测了 6 小时死亡率。我们证明了本研究框架的成功。

    Heart attack remain one of the greatest contributors to mortality in the United States and globally. Patients admitted to the intensive care unit (ICU) with diagnosed heart attack (myocardial infarction or MI) are at higher risk of death. In this study, we use two retrospective cohorts extracted from the eICU and MIMIC-IV databases, to develop a novel pseudo-dynamic machine learning framework for mortality prediction in the ICU with interpretability and clinical risk analysis. The method provides accurate prediction for ICU patients up to 24 hours before the event and provide time-resolved interpretability results. The performance of the framework relying on extreme gradient boosting was evaluated on a held-out test set from eICU, and externally validated on the MIMIC-IV cohort using the most important features identified by time-resolved Shapley values achieving AUCs of 91.0 (balanced accuracy of 82.3) for 6-hour prediction of mortality respectively. We show that our framework success
    
[^34]: N-元事实的少样本链接预测

    Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])

    [http://arxiv.org/abs/2305.06104](http://arxiv.org/abs/2305.06104)

    本文提出了一个新任务——少样本N-元事实链接预测，并提出了一个名为FLEN的模型来实现。FLEN由三个模块组成，可以从有限的标记实例中预测N-元事实中的缺失实体。

    

    N-元事实由主要三元组（头实体、关系、尾实体）和任意数量的辅助属性值对组成，这在现实世界的知识图谱中很常见。对于N-元事实的链接预测是预测其中一个元素的缺失，填补缺失元素有助于丰富知识图谱并促进许多下游应用程序。以往的研究通常需要大量高质量的数据来理解N-元事实中的元素，但这些研究忽视了少样本关系，在现实世界的场景中却很常见。因此，本文引入一个新任务——少样本N-元事实链接预测，旨在使用有限的标记实例来预测N-元事实中的缺失实体。我们也提出了一个针对N-元事实的少样本链接预测模型FLEN，它由三个模块组成：关系学习模块、支持特定调整模块和查询推理模块。

    N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
    
[^35]: 采用参数分解和滤波技术实现更好的图表征学习

    Towards Better Graph Representation Learning with Parameterized Decomposition & Filtering. (arXiv:2305.06102v1 [cs.LG])

    [http://arxiv.org/abs/2305.06102](http://arxiv.org/abs/2305.06102)

    本研究提出一个新的通用框架，采用参数化分解和滤波，统一了现有的GNN模型，提高了GNN的灵活性，缓解了现有模型的平滑和放大问题，并开发了简单但有效的模型，在各种图形学习任务中实现了显著的改进和计算效率。

    

    提出一种有效而灵活的矩阵来表示图形是一个基本的挑战，其已从多个角度进行了探索，例如，使用图傅里叶变换中的滤波。在本文中，我们从参数化分解和滤波的角度开发了一个新的通用框架，统一了许多现有的GNN模型，并展示了如何提高GNN的灵活性，同时减轻现有模型的平滑和放大问题。本质上，我们表明，具有可学习多项式滤波器的谱图卷积是这种表述的约束变体，放弃这些约束使我们的模型能够同时表达所需的分解和滤波。基于这个广义框架，我们开发的模型在实现上简单，但在各种图形学习任务中实现了显著的改进和计算效率。 代码可在 https://github.com/qslim/PDF 获得。

    Proposing an effective and flexible matrix to represent a graph is a fundamental challenge that has been explored from multiple perspectives, e.g., filtering in Graph Fourier Transforms. In this work, we develop a novel and general framework which unifies many existing GNN models from the view of parameterized decomposition and filtering, and show how it helps to enhance the flexibility of GNNs while alleviating the smoothness and amplification issues of existing models. Essentially, we show that the extensively studied spectral graph convolutions with learnable polynomial filters are constrained variants of this formulation, and releasing these constraints enables our model to express the desired decomposition and filtering simultaneously. Based on this generalized framework, we develop models that are simple in implementation but achieve significant improvements and computational efficiency on a variety of graph learning tasks. Code is available at https://github.com/qslim/PDF.
    
[^36]: XTab: 跨表格表格变换器的交叉表预训练

    XTab: Cross-table Pretraining for Tabular Transformers. (arXiv:2305.06090v1 [cs.LG])

    [http://arxiv.org/abs/2305.06090](http://arxiv.org/abs/2305.06090)

    XTab 是一个用于不同领域的跨表格预训练的框架，它通过使用独立的特征工程师和联邦学习解决表格之间不一致的列类型和数量的挑战。在测试中， XTab 可以提高多个表格变换器的泛化能力、学习速度和性能。

    

    自我监督学习在计算机视觉和自然语言处理中的成功促使了对表格数据的预训练方法的开发。然而，大多数现有的表格自我监督学习模型未能利用多个数据表之间的信息，并且无法推广到新表中。本文介绍了XTab，这是一种用于在来自各个领域的数据集上进行交叉表预训练的框架。我们通过利用独立的特征工程师和使用联邦学习来预先训练共享组件，解决了表格之间不一致的列类型和数量的挑战。

    The success of self-supervised learning in computer vision and natural language processing has motivated pretraining methods on tabular data. However, most existing tabular self-supervised learning models fail to leverage information across multiple data tables and cannot generalize to new tables. In this work, we introduce XTab, a framework for cross-table pretraining of tabular transformers on datasets from various domains. We address the challenge of inconsistent column types and quantities among tables by utilizing independent featurizers and using federated learning to pretrain the shared component. Tested on 84 tabular prediction tasks from the OpenML-AutoML Benchmark (AMLB), we show that (1) XTab consistently boosts the generalizability, learning speed, and performance of multiple tabular transformers, (2) by pretraining FT-Transformer via XTab, we achieve superior performance than other state-of-the-art tabular deep learning models on various tasks such as regression, binary, a
    
[^37]: ChatGPT能力展示及其对AI研究的影响

    A Glimpse in ChatGPT Capabilities and its impact for AI research. (arXiv:2305.06087v1 [cs.AI])

    [http://arxiv.org/abs/2305.06087](http://arxiv.org/abs/2305.06087)

    ChatGPT展示了LLMs的强大能力，但这些模型的训练和运行需要巨大的计算资源和高昂的成本，预计会给AI研究带来重大影响。

    

    大型语言模型（LLMs）最近在人工智能（AI）研究领域成为热门话题。像Google、亚马逊、Facebook、特斯拉和苹果（GAFA）这样的公司正在大力发展这些模型。这些模型会使用海量的数据进行训练，可用于各种任务，包括语言翻译、文章生成和问答系统。然而，训练和运行这些模型所需的计算资源非常巨大，而硬件和电力的成本可能限制了那些没有GAFA资金和资源的研究实验室。本文将研究LLMs对AI研究的影响，重点关注GPT3.5/ChatGPT3.4，并给出使用这些模型进行研究的一些示例。

    Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development. These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering. However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA. In this paper, we will examine the impact of LLMs on AI research. The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing. We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and
    
[^38]: 有限精度采样下的赌博机最优臂识别问题研究

    Best Arm Identification in Bandits with Limited Precision Sampling. (arXiv:2305.06082v1 [cs.LG])

    [http://arxiv.org/abs/2305.06082](http://arxiv.org/abs/2305.06082)

    本文研究了在有限精度下采样的多臂赌博机问题，提出了一种修改的跟踪算法来处理最优分配的非唯一性，并证明了其渐进最优性。

    

    本文研究了一种多臂赌博机问题的变体，在该问题中，学习者在选择臂时有限的精度。学习者只能通过特定的探索组合（即所谓的箱子）采样臂。具体而言，在每个采样时刻，学习者选择一个箱子，然后根据箱子特定的概率分布拉动臂，揭示被拉动的臂及其瞬时收益，其目标是通过最小化期望停止时间来找到最佳臂，而误差概率受到上限约束。

    We study best arm identification in a variant of the multi-armed bandit problem where the learner has limited precision in arm selection. The learner can only sample arms via certain exploration bundles, which we refer to as boxes. In particular, at each sampling epoch, the learner selects a box, which in turn causes an arm to get pulled as per a box-specific probability distribution. The pulled arm and its instantaneous reward are revealed to the learner, whose goal is to find the best arm by minimising the expected stopping time, subject to an upper bound on the error probability. We present an asymptotic lower bound on the expected stopping time, which holds as the error probability vanishes. We show that the optimal allocation suggested by the lower bound is, in general, non-unique and therefore challenging to track. We propose a modified tracking-based algorithm to handle non-unique optimal allocations, and demonstrate that it is asymptotically optimal. We also present non-asympto
    
[^39]: 面向部分标签学习的有效视觉表示方法研究

    Towards Effective Visual Representations for Partial-Label Learning. (arXiv:2305.06080v1 [cs.CV])

    [http://arxiv.org/abs/2305.06080](http://arxiv.org/abs/2305.06080)

    本文提出了一个简单的框架PaPi，用于解决部分标签学习（PLL）问题，该框架通过由线性分类器指导的原型分类器优化来作为正样本，从而提高表示学习的性能，促进了标签消歧。

    

    针对部分标签学习（PLL）的情况，每个训练实例仅可获得一组包含未知真实标签的模棱两可的候选标签，对于视觉任务，对比学习最近提高了PLL的性能，归因于对相同/不同实体类别进行对比学习得到的表示学习。本文提出了一个称为PaPi的简单框架，重新思考最先进的对比学习PLL方法PiCO[24]，它通过由线性分类器指导的原型分类器优化来作为正样本和伪标签预测负样本，从而提高表示学习的性能，促进了标签消歧。

    Under partial-label learning (PLL) where, for each training instance, only a set of ambiguous candidate labels containing the unknown true label is accessible, contrastive learning has recently boosted the performance of PLL on vision tasks, attributed to representations learned by contrasting the same/different classes of entities. Without access to true labels, positive points are predicted using pseudo-labels that are inherently noisy, and negative points often require large batches or momentum encoders, resulting in unreliable similarity information and a high computational overhead. In this paper, we rethink a state-of-the-art contrastive PLL method PiCO[24], inspiring the design of a simple framework termed PaPi (Partial-label learning with a guided Prototypical classifier), which demonstrates significant scope for improvement in representation learning, thus contributing to label disambiguation. PaPi guides the optimization of a prototypical classifier by a linear classifier wit
    
[^40]: SemEval-2023任务11中的iLab Le-Wi-Di：模拟不一致性还是模拟观点？

    iLab at SemEval-2023 Task 11 Le-Wi-Di: Modelling Disagreement or Modelling Perspectives?. (arXiv:2305.06074v1 [cs.CL])

    [http://arxiv.org/abs/2305.06074](http://arxiv.org/abs/2305.06074)

    本文讨论了建模注释者不一致性的两种方法：分布式软标记方法和建模个体注释者或其组的观点。本文通过将两种方法结合起来的方式来模拟注释者不一致性。尽管多任务方法之前表现良好，但我们发现其在包含不同注释者观点的数据集上表现不佳。研究结果表明，该方法可以更细致地理解注释者不一致性，为更准确的观点建模提供可能。

    

    对于建模注释者不一致性，有两种竞争的方法：分布式软标记方法（旨在捕捉不一致程度）或建模个体注释者或其组的观点。我们采用了一个先前在建模观点方面表现成功的多任务架构，对SEMEVAL任务11的性能进行评估。我们将两种方法结合起来，即预测个别注释者的观点作为预测注释者不一致性的过渡步骤。尽管此方法之前表现良好，但我们发现在包含不同注释者观点的数据集上，多任务方法的表现不佳，这表明这种方法在建模观点时可能并不总是适用的。此外，我们的结果表明，虽然强烈的观点主义方法可能不会根据分布式方法使用的评估指标取得最先进的性能，但我们的方法允许更细致的理解注释者不一致性，并可以导致更准确的观点建模。

    There are two competing approaches for modelling annotator disagreement: distributional soft-labelling approaches (which aim to capture the level of disagreement) or modelling perspectives of individual annotators or groups thereof. We adapt a multi-task architecture -- which has previously shown success in modelling perspectives -- to evaluate its performance on the SEMEVAL Task 11. We do so by combining both approaches, i.e. predicting individual annotator perspectives as an interim step towards predicting annotator disagreement. Despite its previous success, we found that a multi-task approach performed poorly on datasets which contained distinct annotator opinions, suggesting that this approach may not always be suitable when modelling perspectives. Furthermore, our results explain that while strongly perspectivist approaches might not achieve state-of-the-art performance according to evaluation metrics used by distributional approaches, our approach allows for a more nuanced under
    
[^41]: 通过误差函数核训练增强量子支持向量机

    Enhancing Quantum Support Vector Machines through Variational Kernel Training. (arXiv:2305.06063v1 [quant-ph])

    [http://arxiv.org/abs/2305.06063](http://arxiv.org/abs/2305.06063)

    本文研究了量子支持向量机，并提出了一种新方法：量子变分核支持向量机（QVK-SVM），能够在准确性、损失和混淆矩阵指标方面优于现有模型。它应该成为未来QML研究中的可靠工具。

    

    最近，量子机器学习（QML）取得了巨大的进展，其中量子支持向量机（QSVM）成为了一种有前途的模型。本文重点关注两种现有的QSVM方法：量子核支持向量机（QK-SVM）和量子变分支持向量机（QV-SVM）。虽然两种方法均取得了令人印象深刻的结果，但我们提出了一种新的方法，即量子变分核支持向量机（QVK-SVM），以增强准确性，融合了QK-SVM和QV-SVM的优点。我们在鸢尾花数据集上进行了广泛的实验，发现QVK-SVM在准确性、损失和混淆矩阵指标方面均优于现有模型。我们的结果表明，QVK-SVM作为一种可靠的、具有变革性的QML应用工具具有巨大的潜力。因此，我们建议在未来的QML研究中采用它。

    Quantum machine learning (QML) has witnessed immense progress recently, with quantum support vector machines (QSVMs) emerging as a promising model. This paper focuses on the two existing QSVM methods: quantum kernel SVM (QK-SVM) and quantum variational SVM (QV-SVM). While both have yielded impressive results, we present a novel approach that synergizes the strengths of QK-SVM and QV-SVM to enhance accuracy. Our proposed model, quantum variational kernel SVM (QVK-SVM), leverages the quantum kernel and quantum variational algorithm. We conducted extensive experiments on the Iris dataset and observed that QVK-SVM outperforms both existing models in terms of accuracy, loss, and confusion matrix indicators. Our results demonstrate that QVK-SVM holds tremendous potential as a reliable and transformative tool for QML applications. Hence, we recommend its adoption in future QML research endeavors.
    
[^42]: 视觉调整

    Visual Tuning. (arXiv:2305.06061v1 [cs.CV])

    [http://arxiv.org/abs/2305.06061](http://arxiv.org/abs/2305.06061)

    本文综述了视觉调整的发展与现状，将近期的视觉调整技术分为五类，包括提示调整、适配器调整、参数翻译、紧凑调整和模块调整，并提出了未来研究方向。

    

    对视觉模型进行微调已被广泛证明在许多下游视觉任务中具有有前途的表现。随着预训练视觉基础模型的惊人发展，视觉调整跳出了标准的模式操作，即微调整个预训练模型或仅微调完全连接层。相反，近期的进展可以通过更新更少的参数实现比全面微调整个预训练参数更优异的表现，使边缘设备和下游应用程序可以重复使用部署在云端的日益庞大的基础模型。为了帮助研究人员全面了解视觉调整的全貌和未来方向，本综述描绘了大量的近期研究作品，提供了现有工作和模型系统和全面的概述。具体而言，它提供了视觉调整的详细背景，并将最近的视觉调整技术分为五组：提示调整、适配器调整、参数翻译、紧凑调整和模块调整。本文还强调了当前视觉调整技术的限制和挑战，并提出了几个未来研究方向。

    Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter 
    
[^43]: 使用指数级别的少量变分参数的张量网络压缩神经网络

    Compressing neural network by tensor network with exponentially fewer variational parameters. (arXiv:2305.06058v1 [cs.LG])

    [http://arxiv.org/abs/2305.06058](http://arxiv.org/abs/2305.06058)

    本文提出了一种通用的压缩方案，将神经网络的可变参数编码为多层张量网络，明显减少了可变参数的数量，并在多个神经网络和数据集上表现出了卓越的压缩性能，以VGG-16的测试精度提高为例。

    

    为了解决神经网络（NN）所包含的巨大可变的参数问题，本文提出了一种将这些参数 encoding 为多层张量网络（TN）的压缩方案。这种方案演示了出色的压缩性能，超过了以浅层张量网络为基础的现有最先进方法。例如，VGG-16中的3个卷积层的大约1000万参数被压缩到具有仅632个参数的TN中，而在CIFAR-10上的测试准确性令人惊喜地提高了81.14％。

    Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just $632$ parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from $81.14
    
[^44]: 反馈循环分类及其与自动化决策系统中偏见的关系

    A Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems. (arXiv:2305.06055v1 [cs.CY])

    [http://arxiv.org/abs/2305.06055](http://arxiv.org/abs/2305.06055)

    本文提出了一个基于时间依赖的分类系统，用以帮助更好地了解自动化决策系统中的反馈循环，其中反馈循环可能会导致系统中的偏见得到加剧。

    

    基于预测的决策系统在各个领域中变得越来越普遍。先前的研究已经证明，这类系统容易受到失控的反馈循环的影响，例如警察不考虑实际犯罪率，一再地前往同一个社区，这加剧了现有的偏见。在实践中，自动化决策对系统本身产生了动态反馈效应，这可能会持续很长时间，使得短视的设计选择难以控制系统的演变。尽管研究人员开始提出长期解决方案以防止负面结果（如对某些群体的偏见），但这些干预大部分取决于临时建模假设，目前缺乏对基于机器学习的决策系统中反馈动态的严格理论理解。在本文中，我们使用动力系统理论的语言，这是应用数学的一个分支，用于分析自然和人工的系统和它们如何随时间变化。我们提出了一个基于时间依赖的分类系统，以便更好地了解自动化决策的反馈循环类别。

    Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself that can perpetuate over time, making it difficult for short-sighted design choices to control the system's evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the
    
[^45]: 缺失值下的相关性可视化：填充法和直接参数估计法的比较

    Correlation visualization under missing values: a comparison between imputation and direct parameter estimation methods. (arXiv:2305.06044v1 [cs.LG])

    [http://arxiv.org/abs/2305.06044](http://arxiv.org/abs/2305.06044)

    本文比较了不同的缺失数据处理方法对相关图的影响，建议使用直接参数估计法(DPER)来绘制相关图

    

    相关矩阵可视化对于理解数据集中变量之间的关系至关重要，但是缺失数据会对相关系数的估计产生显著挑战。本文比较了不同的缺失数据处理方法对相关图的影响，重点关注两种常见的缺失模式：随机和单调。我们旨在为研究人员和实践者提供实用的策略和建议，以创建和分析相关图。我们的实验结果表明，虽然填充法通常用于缺失数据，但使用填充的数据来生成相关矩阵图可能会导致对特征之间关系的误导性推断。我们建议基于其在实验中的表现，使用DPER，一种直接参数估计方法，绘制相关矩阵图。

    Correlation matrix visualization is essential for understanding the relationships between variables in a dataset, but missing data can pose a significant challenge in estimating correlation coefficients. In this paper, we compare the effects of various missing data methods on the correlation plot, focusing on two common missing patterns: random and monotone. We aim to provide practical strategies and recommendations for researchers and practitioners in creating and analyzing the correlation plot. Our experimental results suggest that while imputation is commonly used for missing data, using imputed data for plotting the correlation matrix may lead to a significantly misleading inference of the relation between the features. We recommend using DPER, a direct parameter estimation approach, for plotting the correlation matrix based on its performance in the experiments.
    
[^46]: 基于块的主成分分析处理单调缺失数据的插值与降维

    Blockwise Principal Component Analysis for monotone missing data imputation and dimensionality reduction. (arXiv:2305.06042v1 [cs.LG])

    [http://arxiv.org/abs/2305.06042](http://arxiv.org/abs/2305.06042)

    基于块的主成分分析处理单调缺失数据的插值与降维框架，可以显著地减少插补时间，适用于大数据集。

    

    单调缺失数据是数据分析中常见的问题，插值和降维的组合可能会面临计算复杂度高的问题，特别是对于不断增长的数据集。为了解决这个问题，我们提出了一种叫BPI的框架，它利用了块内主成分分析的方法处理单调缺失数据的插值以及降维问题。该框架可以与各种插补技术一起使用，并且与插补后的降维相比，其可以显著地减少插补时间。通过实验验证，该框架具有较高效率并且有效。此外，我们的实验结果还表明，将MICE直接应用于缺失数据可能不是最佳方法。

    Monotone missing data is a common problem in data analysis. However, imputation combined with dimensionality reduction can be computationally expensive, especially with the increasing size of datasets. To address this issue, we propose a Blockwise principal component analysis Imputation (BPI) framework for dimensionality reduction and imputation of monotone missing data. The framework conducts Principal Component Analysis (PCA) on the observed part of each monotone block of the data and then imputes on merging the obtained principal components using a chosen imputation technique. BPI can work with various imputation techniques and can significantly reduce imputation time compared to conducting dimensionality reduction after imputation. This makes it a practical and efficient approach for large datasets with monotone missing data. Our experiments validate the improvement in speed. In addition, our experiments also show that while applying MICE imputation directly on missing data may not
    
[^47]: 搜索UGLE真相：无监督GNN学习环境的调查

    Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])

    [http://arxiv.org/abs/2305.06026](http://arxiv.org/abs/2305.06026)

    本文提出了一种GNN学习环境下的社区检测算法比较框架，包括数据集和评估指标，以解决目前文献中对于基于GNN的社区检测缺乏公平且严谨评估的问题。

    

    图神经网络 (GNN) 是任何机器学习任务中的一个重要工具，因为它们能够学习图结构上的函数，这是一种强大和表达性强的数据表示。社区检测是一种无监督任务，越来越多地使用GNN进行。利用节点特征的多维度与图的连接性对图中的节点进行聚类，对从社交网络到基因组学的真实世界任务有许多应用。不幸的是，目前文献中缺乏公平且严谨评估基于GNN的社区检测的充分基准环境，从而可能阻碍这一新兴领域的进展。我们观察到这种情况下的特定困难是模糊的超参数调整环境与性能和评估数据集的冲突指标。在这项工作中，我们提出和评估了框架，用于在GNN学习环境中进行一致的社区检测算法比较。我们提供了一个基准数据集，并提出了评估指标，反映了检测到的社区的内在质量以及聚类的准确性。

    Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
    
[^48]: 求解偏微分方程的 Deep Galerkin 与 PINNs 方法的全局收敛性。

    Global Convergence of Deep Galerkin and PINNs Methods for Solving Partial Differential Equations. (arXiv:2305.06000v1 [math.NA])

    [http://arxiv.org/abs/2305.06000](http://arxiv.org/abs/2305.06000)

    该论文证明了 Deep Galerkin Method（DGM）算法在全局收敛时，训练得到的神经网络会收敛于一个无限维的线性常微分方程的解，从而逼近求解高维偏微分方程。在一些情况下，PINNs 方法会更稳定。

    

    在高维偏微分方程的数值求解中，由于维度诅咒，传统方法如有限差分无法求解。近期，一系列基于深度学习的方式被开发出来，通过神经网络来逼近求解结果。在这篇论文中，我们证明了目前解决偏微分方程的深度学习算法 Deep Galerkin Method (DGM) 的全局收敛性。DGM 通过随机梯度下降训练一个神经网络逼近求解结果。当单层网络中隐层单元的数量趋向于无穷大时（即"宽度网络极限"），训练后的神经网络逐渐收敛于一个无限维的线性常微分方程的解。极限逼近器的 PDE 残差在训练时间趋近于无穷大时趋近于零。在合理的假设下，我们将 Deep Galerkin Method 的结果与另一种常用方法 PINNs 的结果进行比较， PINNs 在一些情况下可能会更稳定。

    Numerically solving high-dimensional partial differential equations (PDEs) is a major challenge. Conventional methods, such as finite difference methods, are unable to solve high-dimensional PDEs due to the curse-of-dimensionality. A variety of deep learning methods have been recently developed to try and solve high-dimensional PDEs by approximating the solution using a neural network. In this paper, we prove global convergence for one of the commonly-used deep learning algorithms for solving PDEs, the Deep Galerkin Method (DGM). DGM trains a neural network approximator to solve the PDE using stochastic gradient descent. We prove that, as the number of hidden units in the single-layer network goes to infinity (i.e., in the ``wide network limit"), the trained neural network converges to the solution of an infinite-dimensional linear ordinary differential equation (ODE). The PDE residual of the limiting approximator converges to zero as the training time $\rightarrow \infty$. Under mild 
    
[^49]: 从离散化时间事件序列中学习因果结构的结构Hawkes过程

    Structural Hawkes Processes for Learning Causal Structure from Discrete-Time Event Sequences. (arXiv:2305.05986v1 [cs.LG])

    [http://arxiv.org/abs/2305.05986](http://arxiv.org/abs/2305.05986)

    本文提出了一种叫做结构Hawkes过程的方法，通过利用瞬时效应学习离散时间事件序列中事件类型之间的因果结构。

    

    从离散时间事件序列中学习事件类型之间的因果结构是一个重要但具有挑战性的任务。现有的方法，如基于多元Hawkes过程的方法，大多都归结为学习所谓的Granger因果关系，它假定因果事件在效应事件之前严格发生。这种假设在低分辨率离散时间事件序列中往往是不可行的；而典型的离散Hawkes过程主要受到瞬时效应引起的可识别性问题的困扰，即由于低分辨率数据而同时发生的因果关系不会被Granger因果性所捕捉。在本文中，我们提出了一种结构Hawkes过程（SHPs），利用瞬时效应来学习离散事件序列中事件类型之间的因果结构。所提出的方法具有最小化-最大化似然函数的特点。

    Learning causal structure among event types from discrete-time event sequences is a particularly important but challenging task. Existing methods, such as the multivariate Hawkes processes based methods, mostly boil down to learning the so-called Granger causality which assumes that the cause event happens strictly prior to its effect event. Such an assumption is often untenable beyond applications, especially when dealing with discrete-time event sequences in low-resolution; and typical discrete Hawkes processes mainly suffer from identifiability issues raised by the instantaneous effect, i.e., the causal relationship that occurred simultaneously due to the low-resolution data will not be captured by Granger causality. In this work, we propose Structure Hawkes Processes (SHPs) that leverage the instantaneous effect for learning the causal structure among events type in discrete-time event sequence. The proposed method is featured with the minorization-maximization of the likelihood fu
    
[^50]: 使用大型语言模型的多阶段方法生成医学精确的患者-医生对话摘要

    Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models. (arXiv:2305.05982v1 [cs.CL])

    [http://arxiv.org/abs/2305.05982](http://arxiv.org/abs/2305.05982)

    本文需要解决医疗对话摘要的问题，通过将任务划分为几个小的对话任务依次构建，使用GPT-3大型语言模型，动态构建提示任务以及确定医学实体及其确认状态。

    

    医疗提供者对患者访问的摘要具有关键作用，包括临床决策、协调医疗团队和患者的参考资料。有效的摘要需要流畅，并准确捕捉对话中的所有医学相关信息，尽管患者语言的复杂性。即使在访问摘要中出现轻微的不准确 (例如，在发热时总结为“患者没有发烧”) 也会对患者的护理结果产生不利影响。本文将医学会话摘要问题划分为数个小型对话理解任务，其会依次构建。首先，我们确定对话中的医学实体及其确认状态，作为构建模块，通过在关联患者信息的情况下进行动态构建少量提示任务。使用GPT-3作为我们的支撑。

    A medical provider's summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing "patient does not have a fever" when a fever is present) can be detrimental to the outcome of care for the patient.  This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our 
    
[^51]: 图神经网络和三维拓扑

    Graph Neural Networks and 3-Dimensional Topology. (arXiv:2305.05966v1 [math.GT])

    [http://arxiv.org/abs/2305.05966](http://arxiv.org/abs/2305.05966)

    本研究利用图神经网络解决了一类由管道图描述的三维流形分类问题，并训练了一个高准确性的GNN模型，同时介绍了GNN在增强学习中的应用。

    

    本论文在某些简单情境中测试了将几何深度学习应用到低维拓扑问题的有效性。具体来说，我们考虑由管道图描述的三维流形类别，并使用图神经网络（GNN）解决判断一对图是否提供同胚三维流形问题。我们使用监督学习训练GNN，以高准确性提供这种问题的答案。此外，如果答案为肯定，我们考虑通过GNN进行增强学习，找到将一对图相关联的Neumann移动序列。这个情境可以作为解决一对Kirby图是否提供同构三维或四维流形问题的玩具模型理解。

    We test the efficiency of applying Geometric Deep Learning to the problems in low-dimensional topology in a certain simple setting. Specifically, we consider the class of 3-manifolds described by plumbing graphs and use Graph Neural Networks (GNN) for the problem of deciding whether a pair of graphs give homeomorphic 3-manifolds. We use supervised learning to train a GNN that provides the answer to such a question with high accuracy. Moreover, we consider reinforcement learning by a GNN to find a sequence of Neumann moves that relates the pair of graphs if the answer is positive. The setting can be understood as a toy model of the problem of deciding whether a pair of Kirby diagrams give diffeomorphic 3- or 4-manifolds.
    
[^52]: Spectrum Breathing：保护空中联合学习免受干扰

    Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference. (arXiv:2305.05933v1 [cs.LG])

    [http://arxiv.org/abs/2305.05933](http://arxiv.org/abs/2305.05933)

    Spectrum Breathing是一种保护空中联合学习免受干扰的实际方法，通过将随机梯度剪枝和扩频级联起来，以压制干扰而无需扩展带宽。代价是增加的学习延迟。

    

    联合学习是一种从分布式移动数据中蒸馏人工智能的广泛应用范例。但联合学习在移动网络中的部署可能会受到邻近单元或干扰源的干扰而受损。现有的干扰抑制技术需要多单元合作或至少需要昂贵的干扰通道状态信息。另一方面，将干扰视为噪声进行功率控制可能并不有效，由于预算限制，也由于这种机制可能会触发干扰源的反制措施。作为保护空中联合学习免受干扰的实际方法，我们提出了Spectrum Breathing，它将随机梯度剪枝和扩频级联起来，以压制干扰而无需扩展带宽。代价是通过利用剪枝导致学习速度优雅降低而增加的学习延迟。我们将两个操作同步，以保证它们的级别是相互对应的。

    Federated Learning (FL) is a widely embraced paradigm for distilling artificial intelligence from distributed mobile data. However, the deployment of FL in mobile networks can be compromised by exposure to interference from neighboring cells or jammers. Existing interference mitigation techniques require multi-cell cooperation or at least interference channel state information, which is expensive in practice. On the other hand, power control that treats interference as noise may not be effective due to limited power budgets, and also that this mechanism can trigger countermeasures by interference sources. As a practical approach for protecting FL against interference, we propose Spectrum Breathing, which cascades stochastic-gradient pruning and spread spectrum to suppress interference without bandwidth expansion. The cost is higher learning latency by exploiting the graceful degradation of learning speed due to pruning. We synchronize the two operations such that their levels are contr
    
[^53]: 基于深度学习和注意力机制的膝关节X线照片预测髌股关节骨关节炎进展的研究

    Deep Learning for Predicting Progression of Patellofemoral Osteoarthritis Based on Lateral Knee Radiographs, Demographic Data and Symptomatic Assessments. (arXiv:2305.05927v1 [eess.IV])

    [http://arxiv.org/abs/2305.05927](http://arxiv.org/abs/2305.05927)

    本研究利用深度学习和注意力机制预测膝关节骨关节炎进展，发现成像数据在预测中起到重要作用。

    

    本研究提出了一种利用深度学习和注意力机制预测髌股关节骨关节炎（PFOA）在七年内放射性进展的新框架。该研究包括来自MOST研究基线的主体（1832个主体，3276个膝盖），并使用自动化的标志检测工具（BoneFinder）在膝关节X线的侧面识别PF关节感兴趣区域。在5倍交叉验证设置中，基于成像数据开发了一种端到端DL方法来预测PFOA进展。开发了一组基于已知风险因素的基线，并使用梯度提升机（GBM）进行分析。风险因素包括年龄，性别，BMI和WOMAC评分，以及胫骨-股骨关节的放射性骨关节炎分期（KL分数）。最后，我们使用成像和临床数据训练了一个集成模型。在个体模型中，我们的深度卷积神经网络注意模型的性能获得了最佳综合性能，AUC-ROC为0.83。我们的研究证明了利用DL和注意力机制预测PFOA进展的潜力，并强调了除了临床数据外还需要纳入成像数据以进行准确预测的重要性。

    In this study, we propose a novel framework that utilizes deep learning (DL) and attention mechanisms to predict the radiographic progression of patellofemoral osteoarthritis (PFOA) over a period of seven years. This study included subjects (1832 subjects, 3276 knees) from the baseline of the MOST study. PF joint regions-of-interest were identified using an automated landmark detection tool (BoneFinder) on lateral knee X-rays. An end-to-end DL method was developed for predicting PFOA progression based on imaging data in a 5-fold cross-validation setting. A set of baselines based on known risk factors were developed and analyzed using gradient boosting machine (GBM). Risk factors included age, sex, BMI and WOMAC score, and the radiographic osteoarthritis stage of the tibiofemoral joint (KL score). Finally, we trained an ensemble model using both imaging and clinical data. Among the individual models, the performance of our deep convolutional neural network attention model achieved the b
    
[^54]: 大型语言模型快速分布式推断服务

    Fast Distributed Inference Serving for Large Language Models. (arXiv:2305.05920v1 [cs.LG])

    [http://arxiv.org/abs/2305.05920](http://arxiv.org/abs/2305.05920)

    FastServe是一种针对大型语言模型的分布式推理服务系统，利用抢占式调度和跳过-连接多级反馈队列，最小化模型推断的作业完成时间(JCT)。

    

    大型语言模型(LLM)推动了以ChatGPT为代表的新一代互动AI应用程序的发展。这些应用程序的交互性要求模型推断的低作业完成时间(JCT)。现有的LLM服务系统使用的是运行到完成的处理方式，存在头部阻塞和长JCT的问题。我们提出了FastServe，一种针对LLMs的分布式推理服务系统。FastServe利用LLM推理的自回归模式，以每个输出标记的粒度实现抢占式，使用新颖的跳过-连接多级反馈队列调度器最小化JCT。基于LLM推理的新半信息不可知设置，调度程序利用输入长度信息来为每个到达作业分配适当的初始队列来连接。高于所连接队列的优先级队列被跳过以减少降级。我们设计了一种高效的GPU内存管理机制，以提前清除不再使用的GPU缓存，并对常用模型进行缓存。

    Large language models (LLMs) power a new generation of interactive AI applications exemplified by ChatGPT. The interactive nature of these applications demand low job completion time (JCT) for model inference. Existing LLM serving systems use run-to-completion processing for inference jobs, which suffers from head-of-line blocking and long JCT. We present FastServe, a distributed inference serving system for LLMs. FastServe exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token. FastServe uses preemptive scheduling to minimize JCT with a novel skip-join Multi-Level Feedback Queue scheduler. Based on the new semi information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join. The higher priority queues than the joined queue are skipped to reduce demotions. We design an efficient GPU memory management mechanism that proactivel
    
[^55]: 基于高斯耦合softmax层的生成模型和判别模型的混合方法

    A Hybrid of Generative and Discriminative Models Based on the Gaussian-coupled Softmax Layer. (arXiv:2305.05912v1 [cs.LG])

    [http://arxiv.org/abs/2305.05912](http://arxiv.org/abs/2305.05912)

    本文提出了一种基于神经网络的生成模型和判别模型混合的方法，该方法通过高斯耦合softmax层允许分类器估计类后验分布和类条件数据分布，具有半监督学习和置信度校准的应用价值。

    

    生成模型通过无监督数据和可校准的置信度在分类任务中具有优势，而判别模型则在模型结构和学习算法的简单性以及在性能方面超越其生成模型方面具有优势。本文提出了一种在单个神经网络中训练判别模型和生成模型混合的方法，该方法展现了两种模型的特点。关键的思想是高斯耦合softmax层，该层是一个全连接层，具有softmax激活函数和高斯分布耦合。该层可以嵌入到基于神经网络的分类器中，并允许分类器估计类后验分布和类条件数据分布。我们证明了所提出的混合模型可以应用于半监督学习和置信度校准。

    Generative models have advantageous characteristics for classification tasks such as the availability of unsupervised data and calibrated confidence, whereas discriminative models have advantages in terms of the simplicity of their model structures and learning algorithms and their ability to outperform their generative counterparts. In this paper, we propose a method to train a hybrid of discriminative and generative models in a single neural network (NN), which exhibits the characteristics of both models. The key idea is the Gaussian-coupled softmax layer, which is a fully connected layer with a softmax activation function coupled with Gaussian distributions. This layer can be embedded into an NN-based classifier and allows the classifier to estimate both the class posterior distribution and the class-conditional data distribution. We demonstrate that the proposed hybrid model can be applied to semi-supervised learning and confidence calibration.
    
[^56]: 基于演化辅助敌对攻击的鲁棒多智能体协调

    Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers. (arXiv:2305.05909v1 [cs.MA])

    [http://arxiv.org/abs/2305.05909](http://arxiv.org/abs/2305.05909)

    该论文提出了一种基于演化生成辅助敌对攻击来提高训练策略鲁棒性的方法，用于解决合作多智能体强化学习中在不同环境中测试时策略扰动的问题，并在实验中表现优于最先进的CMARL方法。

    

    合作多智能体强化学习(CMARL)在许多实际应用中表现出良好的前景。以往的工作主要集中在通过解决针对MARL的挑战（如非稳态、信用分配、可扩展性）来提高协调能力，但在不同的环境中测试时忽略了策略扰动问题。这个问题在问题定义或算法设计方面还没有得到考虑。为了解决这个问题，我们首先将问题建模为一个有限策略对抗Dec-POMDP问题(LPA-Dec-POMDP)，其中一些来自团队的协调员可能会意外而不可预测地遭遇到有限数量的恶意行为攻击，但常规的协调员仍然会为既定目标而努力。然后，我们提出了通过演化生成辅助敌对攻击来实现鲁棒多智能体协调(ROMANCE)的方法，这样训练的策略可以在训练过程中遭遇多样化和强大的辅助敌对攻击，从而在测试场景中实现高鲁棒性。我们在两个具有挑战性的基准测试中的实验表明，ROMANCE优于最先进的CMARL方法。

    Cooperative multi-agent reinforcement learning (CMARL) has shown to be promising for many real-world applications. Previous works mainly focus on improving coordination ability via solving MARL-specific challenges (e.g., non-stationarity, credit assignment, scalability), but ignore the policy perturbation issue when testing in a different environment. This issue hasn't been considered in problem formulation or efficient algorithm design. To address this issue, we firstly model the problem as a limited policy adversary Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might accidentally and unpredictably encounter a limited number of malicious action attacks, but the regular coordinators still strive for the intended goal. Then, we propose Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to encounter diversified and strong auxiliary adversarial attacks during training, thus achieving h
    
[^57]: DPMLBench：差分隐私机器学习的整体评估

    DPMLBench: Holistic Evaluation of Differentially Private Machine Learning. (arXiv:2305.05900v1 [cs.LG])

    [http://arxiv.org/abs/2305.05900](http://arxiv.org/abs/2305.05900)

    本文提出了DPMLBench框架，通过在图像分类任务上综合衡量加强DP-SGD的DPML算法的实用性和防御能力，填补了比较DPML算法改进表现的空白，提高了DPML算法的性能。

    

    差分隐私（DP）作为一种严格的数学定义，量化了隐私泄露，已成为隐私保护的一个广为接受的标准。结合强大的机器学习技术，差分隐私机器学习（DPML）变得越来越重要。然而，作为最经典的DPML算法之一，DP-SGD会造成显著的效用损失，这阻碍了DPML在实践中的部署。为了缓解这个问题，许多研究最近提出了基于DP-SGD的改进算法，但是这些研究是孤立的，无法全面衡量算法中提出的改进的表现。更重要的是，还缺乏全面研究来比较这些DPML算法的改进在效用、防御能力和泛化能力方面的表现。本文通过在图像分类任务上对改进的DPML算法进行综合测量，对实用性和防御能力进行评估，填补了这一空白。我们提出了一个具有建设性和全面性的框架DPMLBench来评估DPML算法，并将其应用于度量所提出的算法在不同的隐私预算、数据集和模型下的实用性和防御能力表现。实验结果表明我们提出的DPMLBench框架优于现有框架，同时还显示出现有最先进的DPML算法的显著改进。

    Differential privacy (DP), as a rigorous mathematical definition quantifying privacy leakage, has become a well-accepted standard for privacy protection. Combined with powerful machine learning techniques, differentially private machine learning (DPML) is increasingly important. As the most classic DPML algorithm, DP-SGD incurs a significant loss of utility, which hinders DPML's deployment in practice. Many studies have recently proposed improved algorithms based on DP-SGD to mitigate utility loss. However, these studies are isolated and cannot comprehensively measure the performance of improvements proposed in algorithms. More importantly, there is a lack of comprehensive research to compare improvements in these DPML algorithms across utility, defensive capabilities, and generalizability.  We fill this gap by performing a holistic measurement of improved DPML algorithms on utility and defense capability against membership inference attacks (MIAs) on image classification tasks. We fir
    
[^58]: CUTS+：基于非规则时间序列的高维因果发现

    CUTS+: High-dimensional Causal Discovery from Irregular Time-series. (arXiv:2305.05890v1 [cs.LG])

    [http://arxiv.org/abs/2305.05890](http://arxiv.org/abs/2305.05890)

    CUTS+是一种基于Granger因果和图神经网络(MPGNN)的因果发现算法，通过引入粗到细发现（C2FD）技术提高可扩展性。实验结果表明，CUTS+在非规则高维数据上的因果发现性能大幅度提高。

    

    时间序列中的因果关系发现是机器学习领域中一个根本性的问题，可以在复杂情境下进行因果推理和决策制定。最近，研究人员成功地通过将神经网络与 Granger 因果相结合来发现因果关系，但是当遇到高维数据时，由于高度冗余的网络设计和庞大的因果图，它们的性能会严重下降。此外，观察值中的缺失条目进一步阻碍了因果结构学习。为了克服这些限制，我们提出 CUTS+，它建立在基于 Granger 因果的因果发现方法 CUTS 上，并通过引入一种称为粗到细发现（C2FD）的技术和利用基于消息传递的图神经网络（MPGNN）提高了可扩展性。与之前的方法在模拟、准实际和真实数据集上相比，我们展示了 CUTS+ 在不同类型的非规则高维数据上大大提高了因果发现的性能。

    Causal discovery in time-series is a fundamental problem in the machine learning community, enabling causal reasoning and decision-making in complex scenarios. Recently, researchers successfully discover causality by combining neural networks with Granger causality, but their performances degrade largely when encountering high-dimensional data because of the highly redundant network design and huge causal graphs. Moreover, the missing entries in the observations further hamper the causal structural learning. To overcome these limitations, We propose CUTS+, which is built on the Granger-causality-based causal discovery method CUTS and raises the scalability by introducing a technique called Coarse-to-fine-discovery (C2FD) and leveraging a message-passing-based graph neural network (MPGNN). Compared to previous methods on simulated, quasi-real, and real datasets, we show that CUTS+ largely improves the causal discovery performance on high-dimensional data with different types of irregula
    
[^59]: 带有图形消歧的深度部分多标签学习

    Deep Partial Multi-Label Learning with Graph Disambiguation. (arXiv:2305.05882v1 [cs.LG])

    [http://arxiv.org/abs/2305.05882](http://arxiv.org/abs/2305.05882)

    本文提出了一种新的基于图形消歧的深度部分多标签模型（PLAIN），它可以从候选标签中恢复标签置信度，并利用标签依赖性，提高多标签学习的性能。

    

    在部分多标签学习（PML）中，每个数据示例都配备有候选标签集，其中包括多个地面真实标签和其他假阳性标签。最近，基于图形的方法在处理PML问题方面表现良好，能够从候选标签中估计准确的置信度分数。然而，我们观察到现有的基于图形的PML方法通常采用线性多标签分类器，因此无法达到卓越的性能。在这项工作中，我们尝试消除几个障碍，将它们扩展为深度模型，并提出了一种新的带有图形- 消歧的深度部分多标签模型（PLAIN）。具体而言，我们引入了实例级和标签级相似性以恢复标签置信度，并利用标签依赖性。在每个训练时期，标签在实例和标签图上传播以产生相对准确的伪标签；然后，我们训练深度模型以适应数值标签。

    In partial multi-label learning (PML), each data example is equipped with a candidate label set, which consists of multiple ground-truth labels and other false-positive labels. Recently, graph-based methods, which demonstrate a good ability to estimate accurate confidence scores from candidate labels, have been prevalent to deal with PML problems. However, we observe that existing graph-based PML methods typically adopt linear multi-label classifiers and thus fail to achieve superior performance. In this work, we attempt to remove several obstacles for extending them to deep models and propose a novel deep Partial multi-Label model with grAph-disambIguatioN (PLAIN). Specifically, we introduce the instance-level and label-level similarities to recover label confidences as well as exploit label dependencies. At each training epoch, labels are propagated on the instance and label graphs to produce relatively accurate pseudo-labels; then, we train the deep model to fit the numerical labels
    
[^60]: 实现因果关系解释的多样性：一项综述和讨论

    Achieving Diversity in Counterfactual Explanations: a Review and Discussion. (arXiv:2305.05840v1 [cs.AI])

    [http://arxiv.org/abs/2305.05840](http://arxiv.org/abs/2305.05840)

    本篇论文综述了因果关系解释中多样性的概念及其定义，提出了生成多种因果关系例子以解释一个预测结果的方法，探讨了这种方法的优点和局限性。

    

    在可解释的人工智能领域中，因果关系例子通过指出更改实例以更改其预测的修改来解释受过训练的决策模型的预测结果。这些因果关系例子通常定义为解决一个优化问题的解决方案，其中代价函数结合了几个衡量满足用户需求的良好解释的标准。可以考虑多种这样适当的性质，因为用户需求通常是未知的，而且不同用户之间也有所不同；它们的选择和规范化是困难的。为了解决这个问题，一些方法提出生成一组不同的因果关系例子来解释一个预测结果，而不是单个的。本文提出了对这个多样性概念的许多、有时相互矛盾的定义的综述。它讨论了它们的基本原则、假设、优势和局限性。此外，它介绍了一些关于这个特定研究主题的最新工作和未来发展方向。

    In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypothe
    
[^61]: 基于参考的光学相干断层扫描血管造影超分辨率图像，带有可学习的纹理生成

    Reference-based OCT Angiogram Super-resolution with Learnable Texture Generation. (arXiv:2305.05835v1 [eess.IV])

    [http://arxiv.org/abs/2305.05835](http://arxiv.org/abs/2305.05835)

    本研究提出了一种基于参考的超分辨率图像生成框架，使用可训练的纹理生成器产生纹理从而保持OCTA图像的分辨率，解决了扫描区域增大导致分辨率下降的问题。

    

    光学相干断层扫描血管造影（OCTA）是一种可视化视网膜微血管的新型成像技术，并已广泛应用于临床。高分辨率OCTA对于定量和定性地准确识别不同视网膜疾病的潜在生物标志物非常重要。然而，OCTA的一个显著问题是在固定采集时间的情况下增加视野时，分辨率不可避免地降低。为解决这个问题，我们提出了一种新颖的基于参考的超分辨率（RefSR）框架，以保持OCTA图像的分辨率同时增加扫描区域。具体而言，使用正常的RefSR流程中的纹理来训练一个可学习的纹理生成器（LTG），该生成器被设计用于根据输入生成纹理。

    Optical coherence tomography angiography (OCTA) is a new imaging modality to visualize retinal microvasculature and has been readily adopted in clinics. High-resolution OCT angiograms are important to qualitatively and quantitatively identify potential biomarkers for different retinal diseases accurately. However, one significant problem of OCTA is the inevitable decrease in resolution when increasing the field-of-view given a fixed acquisition time. To address this issue, we propose a novel reference-based super-resolution (RefSR) framework to preserve the resolution of the OCT angiograms while increasing the scanning area. Specifically, textures from the normal RefSR pipeline are used to train a learnable texture generator (LTG), which is designed to generate textures according to the input. The key difference between the proposed method and traditional RefSR models is that the textures used during inference are generated by the LTG instead of being searched from a single reference i
    
[^62]: 因果信息分离：为抗分布转移设计代理特征

    Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])

    [http://arxiv.org/abs/2305.05832](http://arxiv.org/abs/2305.05832)

    本文提出了利用因果机制在不同环境下保持不变的直觉来主动准备的代理特征选择和工程技术，用以应对统计预测模型在分布转移情况下的稳定性问题。

    

    统计预测模型通常是在与最终使用情况不同的概率分布中进行训练的。为了预测分布转移，有一种方法是利用因果机制在不同环境下保持不变的直觉来主动准备。本文针对一个具有挑战性的场景，其中目标的因果和反因果变量都是未被观察到的。利用信息论，我们为下游观测变量开发了特征选择和工程技术，这些变量充当代理。我们选择有助于建立稳定模型的代理，并使用辅助训练任务从代理中提取增强稳定性的信息。我们在合成数据和真实数据上展示了我们技术的有效性。

    Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
    
[^63]: 基于正态映射的Prox-SGD方法在KL不等式下的收敛性

    Convergence of a Normal Map-based Prox-SGD Method under the KL Inequality. (arXiv:2305.05828v1 [math.OC])

    [http://arxiv.org/abs/2305.05828](http://arxiv.org/abs/2305.05828)

    本文提出了一种新的随机正态映射算法用于非凸复合型优化问题，并证明其收敛性质。该方法扩展了基本Proximal随机梯度法的更有限的收敛保证。

    

    本文提出了一种新颖的随机正态映射算法（$\mathsf{norM}\text{-}\mathsf{SGD}$）用于非凸复合型优化问题，并讨论了其收敛性质。使用基于时间窗口的策略，首先分析了$\mathsf{norM}\text{-}\mathsf{SGD}$的全局收敛行为，并证明了所生成的迭代序列$\{\boldsymbol{x}^k\}_k$的每个累积点几乎确定地和期望上都对应于一个稳定点。所得结果在标准假设下成立，并扩展了基本Proximal随机梯度法的更有限的收敛保证。此外，基于著名的Kurdyka-{\L}ojasiewicz（KL）分析框架，我们为迭代序列$\{\boldsymbol{x}^k\}_k$提供了新的逐点收敛结果，并得出了取决于基础KL指数$\boldsymbol{\theta}$和步长动态$\{\alpha_k\}_k$的收敛速率。

    In this paper, we present a novel stochastic normal map-based algorithm ($\mathsf{norM}\text{-}\mathsf{SGD}$) for nonconvex composite-type optimization problems and discuss its convergence properties. Using a time window-based strategy, we first analyze the global convergence behavior of $\mathsf{norM}\text{-}\mathsf{SGD}$ and it is shown that every accumulation point of the generated sequence of iterates $\{\boldsymbol{x}^k\}_k$ corresponds to a stationary point almost surely and in an expectation sense. The obtained results hold under standard assumptions and extend the more limited convergence guarantees of the basic proximal stochastic gradient method. In addition, based on the well-known Kurdyka-{\L}ojasiewicz (KL) analysis framework, we provide novel point-wise convergence results for the iterates $\{\boldsymbol{x}^k\}_k$ and derive convergence rates that depend on the underlying KL exponent $\boldsymbol{\theta}$ and the step size dynamics $\{\alpha_k\}_k$. Specifically, for the 
    
[^64]: 包容性FinTech贷款：基于对比学习和领域自适应的方法

    Inclusive FinTech Lending via Contrastive Learning and Domain Adaptation. (arXiv:2305.05827v1 [cs.LG])

    [http://arxiv.org/abs/2305.05827](http://arxiv.org/abs/2305.05827)

    本文研究了基于对比学习和领域自适应的方法来减轻贷款筛选中的表征偏差，提高了贷款筛选所使用机器学习算法的性能，特别对于低社会经济背景借款人的表现有很大的改善。

    

    FinTech贷款在促进金融普惠方面发挥了重要作用，它缩短了处理时间和成本，提高了用户体验，并使得那些可能在传统银行贷款中无法获得信用认证的人们可以获得贷款。然而，人们对贷款筛选中潜在的算法决策偏见持有担忧。机器学习算法评估信用质量时可能会受到训练数据中的表征偏差的影响，因为我们只能访问已获批准贷款申请的违约结果标签，对于此类申请，借款人的社会经济特征优于被拒绝的贷款申请。因此，基于标记数据训练的模型在历史上被批准的人群中表现良好，但在低社会经济背景的借款人中不具有良好的泛化能力。本文研究了一个真实的FinTech贷款平台贷款筛选中的表征偏差问题，提出了一种基于对比学习和领域自适应的新方法来减少表征偏差，提高贷款筛选所使用机器学习算法的性能。我们的方法包括使用来自已批准和被拒绝的贷款申请的标记数据混合训练对比模型。然后，我们将学习到的表征适应于具有较低社会经济特征的贷款申请人的目标领域。我们的实验结果证明了我们的方法在减轻表征偏差，提高低社会经济背景借款人的贷款筛选准确性方面的有效性。

    FinTech lending (e.g., micro-lending) has played a significant role in facilitating financial inclusion. It has reduced processing times and costs, enhanced the user experience, and made it possible for people to obtain loans who may not have qualified for credit from traditional lenders. However, there are concerns about the potentially biased algorithmic decision-making during loan screening. Machine learning algorithms used to evaluate credit quality can be influenced by representation bias in the training data, as we only have access to the default outcome labels of approved loan applications, for which the borrowers' socioeconomic characteristics are better than those of rejected ones. In this case, the model trained on the labeled data performs well on the historically approved population, but does not generalize well to borrowers of low socioeconomic background. In this paper, we investigate the problem of representation bias in loan screening for a real-world FinTech lending pl
    
[^65]: 最佳努力适应性

    Best-Effort Adaptation. (arXiv:2305.05816v1 [cs.LG])

    [http://arxiv.org/abs/2305.05816](http://arxiv.org/abs/2305.05816)

    研究了最佳努力适应性问题，提出了一种新的基于差异的理论分析方法以及用于标准域适应性问题的改进学习算法，表现出很好的实验效果。

    

    我们研究了一个由多个应用和考虑因素激发出的最佳努力适应性问题，其中包括确定一个精确的预测器以用于目标域，虽然只有适量的已标记样本可用，但利用来自另一个拥有大量已标记样本的域的信息。我们提出了一种新的和通用的基于差异的理论分析样本重新加权方法，包括在权重上均匀保持的界限。我们展示了这些边界如何指导我们详细讨论的学习算法的设计。我们进一步展示了我们的学习保证和算法为标准域适应性问题提供了改进的解决方案，其中目标域只有少量标记数据或没有标记数据可用。最后，我们报告了一系列实验的结果，展示了我们的最佳努力适应性和域适应算法的有效性，以及与几个基线的比较。

    We study a problem of best-effort adaptation motivated by several applications and considerations, which consists of determining an accurate predictor for a target domain, for which a moderate amount of labeled samples are available, while leveraging information from another domain for which substantially more labeled samples are at one's disposal. We present a new and general discrepancy-based theoretical analysis of sample reweighting methods, including bounds holding uniformly over the weights. We show how these bounds can guide the design of learning algorithms that we discuss in detail. We further show that our learning guarantees and algorithms provide improved solutions for standard domain adaptation problems, for which few labeled data or none are available from the target domain. We finally report the results of a series of experiments demonstrating the effectiveness of our best-effort adaptation and domain adaptation algorithms, as well as comparisons with several baselines. 
    
[^66]: 遥感变化检测方法在上个十年的综述

    Change Detection Methods for Remote Sensing in the Last Decade: A Comprehensive Review. (arXiv:2305.05813v1 [cs.CV])

    [http://arxiv.org/abs/2305.05813](http://arxiv.org/abs/2305.05813)

    本文综述了过去十年中遥感影像变化检测方面的重大进展，介绍了问题定义、数据集、评估指标和变压器等初步知识，深度学习作为一种强大的特征提取工具被广泛运用于解决检测遥感图像中的变化的复杂挑战。

    

    变化检测是遥感中一项基本而广泛应用的任务，旨在检测和分析同一地理区域随时间发生的变化，具有广泛的城市发展、农业调查和土地覆盖监测应用。由于各种因素，包括图像质量变化、噪声、注册误差、光照变化、复杂景观和空间异质性，检测遥感图像中的变化是一个复杂的挑战。近年来，深度学习作为一种强大的特征提取工具应运而生，并且在解决这些问题方面具有很大的灵活性，因此被广泛用于许多图像处理任务。本文综述了过去十年中遥感影像变化检测方面的重大进展。我们首先介绍了变化检测任务的一些初步知识，如问题定义、数据集、评估指标和变压器基础知识。

    Change detection is an essential and widely utilized task in remote sensing that aims to detect and analyze changes occurring in the same geographical area over time, which has broad applications in urban development, agricultural surveys, and land cover monitoring. Detecting changes in remote sensing images is a complex challenge due to various factors, including variations in image quality, noise, registration errors, illumination changes, complex landscapes, and spatial heterogeneity. In recent years, deep learning has emerged as a powerful tool for feature extraction and addressing these challenges. Its versatility has resulted in its widespread adoption for numerous image-processing tasks. This paper presents a comprehensive survey of significant advancements in change detection for remote sensing images over the past decade. We first introduce some preliminary knowledge for the change detection task, such as problem definition, datasets, evaluation metrics, and transformer basics
    
[^67]: 核电站燃料优化的强化学习算法评估

    Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization. (arXiv:2305.05812v1 [cs.LG])

    [http://arxiv.org/abs/2305.05812](http://arxiv.org/abs/2305.05812)

    本文提出了一种基于深度强化学习的方案，以解决核电站燃料优化问题，能够提高核电站的性能和安全。

    

    自商业核能产业诞生以来，人们一直在研究核燃料装载模式的优化问题。它具有多个目标和约束条件，候选模式数量非常高，因此无法明确解决。不同的核能公用事业和供应商使用随机优化方法来执行燃料循环重装设计，但手动设计的解决方案仍然是主流。为了改进现有状态的燃料循环重装模式，我们旨在创建一种尽可能可扩展的方法，符合设计师的性能和安全目标。为了帮助完成此任务，我们利用了深度强化学习（RL），特别是近端策略优化。最近，RL在游戏中的成功给它带来了强劲的推动。本文阐述了这种方法的基础，并提议研究影响RL算法性能的几个超参数的行为。

    The nuclear fuel loading pattern optimization problem has been studied since the dawn of the commercial nuclear energy industry. It is characterized by multiple objectives and constraints, with a very high number of candidate patterns, which makes it impossible to solve explicitly. Stochastic optimization methodologies are used by different nuclear utilities and vendors to perform fuel cycle reload design. Nevertheless, hand-designed solutions continue to be the prevalent method in the industry. To improve the state-of-the-art core reload patterns, we aim to create a method as scalable as possible, that agrees with the designer's goal of performance and safety. To help in this task Deep Reinforcement Learning (RL), in particular, Proximal Policy Optimization is leveraged. RL has recently experienced a strong impetus from its successes applied to games. This paper lays out the foundation of this method and proposes to study the behavior of several hyper-parameters that influence the RL 
    
[^68]: 关于最近邻表示的信息容量研究

    On the Information Capacity of Nearest Neighbor Representations. (arXiv:2305.05808v1 [cs.CC])

    [http://arxiv.org/abs/2305.05808](http://arxiv.org/abs/2305.05808)

    本文研究了一种联想计算模型，其中计算与记忆不可分，通过从输入向量到最近邻锚点的收敛完成计算。文章关注于在这个模型中表示布尔函数的信息容量。

    

    传统的计算机和人类大脑在计算和存储方面有着不同的架构。受到大脑的启发，本文提出了一种联想计算模型，其中内存由一组在$\mathbb{R}^n$空间中的向量（称为“锚点”）定义，计算是通过从输入向量到最近邻锚点的收敛完成的，输出是与一个锚点相关联的标签。具体而言，本文研究了布尔函数在联想计算模型中的表示，其中输入为二进制向量，相应的输出为最近邻锚点的一个标签（$0$或$1$）。在这个模型中，布尔函数的信息容量与两个数量相关联：(i) 锚点的数量（称为“最近邻复杂度”）和(ii)

    The $\textit{von Neumann Computer Architecture}$ has a distinction between computation and memory. In contrast, the brain has an integrated architecture where computation and memory are indistinguishable. Motivated by the architecture of the brain, we propose a model of $\textit{associative computation}$ where memory is defined by a set of vectors in $\mathbb{R}^n$ (that we call $\textit{anchors}$), computation is performed by convergence from an input vector to a nearest neighbor anchor, and the output is a label associated with an anchor. Specifically, in this paper, we study the representation of Boolean functions in the associative computation model, where the inputs are binary vectors and the corresponding outputs are the labels ($0$ or $1$) of the nearest neighbor anchors. The information capacity of a Boolean function in this model is associated with two quantities: $\textit{(i)}$ the number of anchors (called $\textit{Nearest Neighbor (NN) Complexity}$) and $\textit{(ii)}$ the 
    
[^69]: 即使很小的相关性和多样性变化也会导致数据集偏差问题

    Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v1 [cs.CV])

    [http://arxiv.org/abs/2305.05807](http://arxiv.org/abs/2305.05807)

    本文研究了数据集中的分布变化对深度学习模型的影响，并提出了一个综合协议来分析多样性变化和相关性变化。使用皮肤癌分析分类问题的实例，发现模型不仅会学习和传播相关性变化，而且可能会使用错误的特征。

    

    分布变化在实际数据集中很常见，会影响深度学习模型的性能和可靠性。本文研究了两种类型的分布变化：多样性变化和相关性变化。我们提出了一个综合协议，使用同时存在这两种变化的数据集来分析它们。最后，我们将我们的方法应用于一个真实的皮肤癌分析分类问题中，使用了超出数据集和专门的偏差注释。我们的协议揭示了三个发现：1）模型即使进行了低偏差训练也会学习并传播相关性变化，这可能会累积和结合难以解释的弱偏差的风险；2）模型在高、低偏差情况下可以学习到稳健的特征，但是如果测试样本有错误的特征它们可能会使用这些特征。

    Distribution shifts are common in real-world datasets and can affect the performance and reliability of deep learning models. In this paper, we study two types of distribution shifts: diversity shifts, which occur when test samples exhibit patterns unseen during training, and correlation shifts, which occur when test data present a different correlation between seen invariant and spurious features. We propose an integrated protocol to analyze both types of shifts using datasets where they co-exist in a controllable manner. Finally, we apply our approach to a real-world classification problem of skin cancer analysis, using out-of-distribution datasets and specialized bias annotations. Our protocol reveals three findings: 1) Models learn and propagate correlation shifts even with low-bias training; this poses a risk of accumulating and combining unaccountable weak biases; 2) Models learn robust features in highand low-bias scenarios but use spurious ones if test samples have them; this
    
[^70]: 基于Segment Anything Model (SAM)增强伪标签的弱监督语义分割

    Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])

    [http://arxiv.org/abs/2305.05803](http://arxiv.org/abs/2305.05803)

    本论文提出了一种通过利用Segment Anything Model (SAM)增强Class Activation Maps (CAM)生成高质量伪标签的方法来解决弱监督语义分割中CAM的局部激活和虚假激活的限制问题。

    

    仅使用图像级别的监督的弱监督语义分割(WSSS)由于其与像素级注释相比的低成本而越来越受到关注。大多数现有方法依赖于类激活图(CAM)生成像素级的伪标签进行监督训练。然而，众所周知，CAM经常遭受局部激活的限制-只激活最具区分性的部分，而不是整个对象区域和虚假的激活-不必要地激活物体周围的背景。本研究引入了一种简单而有效的方法来解决这些限制，即利用最近发布的Segment Anything Model (SAM)增强CAM生成高质量的伪标签。SAM是一个分割基础模型，展示了将图像分割成段落的强零-shot能力，但缺乏这些区域的语义标签。为了解决这个问题，我们采用特定类别的伪标签作为信号来选择m。

    Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
    
[^71]: 多功能储备计算机的双重视觉效应

    Seeing double with a multifunctional reservoir computer. (arXiv:2305.05799v1 [math.DS])

    [http://arxiv.org/abs/2305.05799](http://arxiv.org/abs/2305.05799)

    本文研究了储备计算机的多功能性能力，探讨了不同吸引子之间的关系如何影响其实现多任务。结果表明，要实现多功能性，需要RC内部网络c的谱半径合适选择的临界依赖性。

    

    多功能生物神经网络利用多重稳定性以执行多种任务而不改变任何网络属性。使人工神经网络（ANN）获得某些多稳定性以执行多个任务，其中每个任务与网络状态空间中的特定吸引子相关联，从机器学习的角度自然具有许多优点。因为与多稳定性有关，本文探讨了不同吸引子之间的关系如何影响储备计算机（RC）的多功能性能力，储备计算机是一种以ANN形式呈现的动态系统。我们构建了“双重视觉效应”问题来系统地研究当两个吸引子之间存在重叠时RC如何重构吸引子的共存。随着重叠量的增加，我们发现要实现多功能性，需要RC内部网络c的谱半径合适选择的临界依赖性。

    Multifunctional biological neural networks exploit multistability in order to perform multiple tasks without changing any network properties. Enabling artificial neural networks (ANNs) to obtain certain multistabilities in order to perform several tasks, where each task is related to a particular attractor in the network's state space, naturally has many benefits from a machine learning perspective. Given the association to multistability, in this paper we explore how the relationship between different attractors influences the ability of a reservoir computer (RC), which is a dynamical system in the form of an ANN, to achieve multifunctionality. We construct the `seeing double' problem to systematically study how a RC reconstructs a coexistence of attractors when there is an overlap between them. As the amount of overlap increases, we discover that for multifunctionality to occur, there is a critical dependence on a suitable choice of the spectral radius for the RC's internal network c
    
[^72]: 过拟合的测试

    Testing for Overfitting. (arXiv:2305.05792v1 [stat.ML])

    [http://arxiv.org/abs/2305.05792](http://arxiv.org/abs/2305.05792)

    本文提出了一种能够使用训练数据进行评估模型性能的假设检验方法，可以准确地定义和检测过拟合。

    

    在机器学习中，高复杂度的模型常见过拟合现象，即模型能够很好地代表数据，但无法推广到基础数据生成过程。解决过拟合的典型方法是在留置集上计算经验风险，一旦风险开始增加，就停止（或标记何时停止）。虽然这种方法输出了良好泛化的模型，但其实现原理主要是启发式的。本文讨论过拟合问题，解释了为什么使用训练数据进行评估时，标准渐近和浓度结果不成立。我们随后提出并阐述了一个假设检验，通过该检验可以对使用训练数据评估模型性能，并量化地定义和检测过拟合。我们依靠确保经验均值应该高概率地近似其真实均值的浓度界限，以得出他们应该相互接近的结论。

    High complexity models are notorious in machine learning for overfitting, a phenomenon in which models well represent data but fail to generalize an underlying data generating process. A typical procedure for circumventing overfitting computes empirical risk on a holdout set and halts once (or flags that/when) it begins to increase. Such practice often helps in outputting a well-generalizing model, but justification for why it works is primarily heuristic.  We discuss the overfitting problem and explain why standard asymptotic and concentration results do not hold for evaluation with training data. We then proceed to introduce and argue for a hypothesis test by means of which both model performance may be evaluated using training data, and overfitting quantitatively defined and detected. We rely on said concentration bounds which guarantee that empirical means should, with high probability, approximate their true mean to conclude that they should approximate each other. We stipulate co
    
[^73]: 利用生成对抗网络增强有噪音的语音信号的间隔

    Enhancing Gappy Speech Audio Signals with Generative Adversarial Networks. (arXiv:2305.05780v1 [cs.SD])

    [http://arxiv.org/abs/2305.05780](http://arxiv.org/abs/2305.05780)

    本文利用机器学习和生成对抗网络提出了一种方法，通过将语音转换为Mel频谱图并使用图像修复技术进行间隔恢复，实现了接近实时的音频间隔重建，填充后的质量与原始数据相近，可以有效提高语音信号的质量。

    

    语音中的噪声、漏报和片段丢失是常见问题，尤其是在语音识别等应用中影响很大。本文利用机器学习方法增强320ms内的语音信号中的间隔。通过将音频转换为Mel-频谱图并使用图像修复技术进行间隔恢复。最终将完整的Mel-频谱图转换回音频，并使用Parallel-WaveGAN声码器集成到音频流中。研究结果表明，使用具有GPU的GAN系统实现接近实时的音频间隔重建。与预期相符，音频中间隔越小，填充后的质量就越好。在240ms的间隔上，最佳的模型的平均意见分数（MOS）为3.737，这足以被人类感知为好的音频质量。

    Gaps, dropouts and short clips of corrupted audio are a common problem and particularly annoying when they occur in speech. This paper uses machine learning to regenerate gaps of up to 320ms in an audio speech signal. Audio regeneration is translated into image regeneration by transforming audio into a Mel-spectrogram and using image in-painting to regenerate the gaps. The full Mel-spectrogram is then transferred back to audio using the Parallel-WaveGAN vocoder and integrated into the audio stream. Using a sample of 1300 spoken audio clips of between 1 and 10 seconds taken from the publicly-available LJSpeech dataset our results show regeneration of audio gaps in close to real time using GANs with a GPU equipped system. As expected, the smaller the gap in the audio, the better the quality of the filled gaps. On a gap of 240ms the average mean opinion score (MOS) for the best performing models was 3.737, on a scale of 1 (worst) to 5 (best) which is sufficient for a human to perceive as 
    
[^74]: 通过增强异构AST表示法学习使用OpenMP并行化

    Learning to Parallelize with OpenMP by Augmented Heterogeneous AST Representation. (arXiv:2305.05779v1 [cs.LG])

    [http://arxiv.org/abs/2305.05779](http://arxiv.org/abs/2305.05779)

    该论文提出了一种名为Graph2Par的基于图形的深度学习方法，利用叫做Augmented-AST的抽象语法树表示法来检测并行化代码区域。实验结果显示，该方法在检测可并行化循环方面优于最先进技术。

    

    检测可并行化的代码区域是一项具有挑战性的任务，即使对于经验丰富的开发人员也是如此。为了解决这些挑战，我们提出了一种新颖的基于图形的学习方法，称为Graph2Par，它利用了一种名为Augmented-AST的异构增强的抽象语法树表示来表示代码。该方法主要关注使用OpenMP进行循环级并行化。我们还创建了一个包含18598个可并行化和13972个不可并行化循环的OMP\_Serial数据集，将代码分析任务转换为二进制分类问题。基准数据集上的实验结果表明，我们的方法优于检测可并行化循环的最先进技术，取得了高达88.74%的F1得分和81.94%的精度。

    Detecting parallelizable code regions is a challenging task, even for experienced developers. Numerous recent studies have explored the use of machine learning for code analysis and program synthesis, including parallelization, in light of the success of machine learning in natural language processing. However, applying machine learning techniques to parallelism detection presents several challenges, such as the lack of an adequate dataset for training, an effective code representation with rich information, and a suitable machine learning model to learn the latent features of code for diverse analyses. To address these challenges, we propose a novel graph-based learning approach called Graph2Par that utilizes a heterogeneous augmented abstract syntax tree (Augmented-AST) representation for code. The proposed approach primarily focused on loop-level parallelization with OpenMP. Moreover, we create an OMP\_Serial dataset with 18598 parallelizable and 13972 non-parallelizable loops to tr
    
[^75]: 多目标自监督深度去噪

    Multi-Object Self-Supervised Depth Denoising. (arXiv:2305.05778v1 [cs.LG])

    [http://arxiv.org/abs/2305.05778](http://arxiv.org/abs/2305.05778)

    本文提出了一种自监督的多目标深度去噪流程，使用来自高质量传感器的深度图进行监督，去噪低质量深度摄像机的深度图。

    

    深度摄像机经常用于机器人操作中的视觉伺服，但是小型紧凑的深度摄像机的质量通常不足以进行深度重建，这对于机器人工作空间的精确跟踪和感知是必须的。在Shabanov等人（2021）的基础上，本文提出了一种自监督的多目标深度去噪流程，使用来自高质量传感器的深度图作为近似于实际监督信号，对来自低质量传感器的深度图进行去噪。我们展示了一种计算效率高的方法，在空间中对两个帧对进行对齐并检索基于帧的多目标掩码，以获取一个干净的带标签数据集，用于训练去噪神经网络。我们提出的工作的实现可以在https://github.com/alr-internship/self-supervised-depth-denoising找到。

    Depth cameras are frequently used in robotic manipulation, e.g. for visual servoing. The quality of small and compact depth cameras is though often not sufficient for depth reconstruction, which is required for precise tracking in and perception of the robot's working space. Based on the work of Shabanov et al. (2021), in this work, we present a self-supervised multi-object depth denoising pipeline, that uses depth maps of higher-quality sensors as close-to-ground-truth supervisory signals to denoise depth maps coming from a lower-quality sensor. We display a computationally efficient way to align sets of two frame pairs in space and retrieve a frame-based multi-object mask, in order to receive a clean labeled dataset to train a denoising neural network on. The implementation of our presented work can be found at https://github.com/alr-internship/self-supervised-depth-denoising.
    
[^76]: DeepTextMark：基于深度学习的文本水印技术用于检测大语言模型生成的文本

    DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text. (arXiv:2305.05773v1 [cs.MM])

    [http://arxiv.org/abs/2305.05773](http://arxiv.org/abs/2305.05773)

    本文提出了一种基于深度学习的文本水印技术DeepTextMark，可用于检测大语言模型生成的文本。该技术实现了盲目性、鲁棒性、隐蔽性和可靠性，并在水印检测精度和抵抗攻击方面优于现有方法。

    

    随着大型语言模型（LLM）的迅速发展，文本生成器的能力得到了提升。为了防止潜在的滥用，检测文本是否由LLM生成变得越来越重要。一些相关的工作试图使用将输入文本分类为人类编写的或LLM生成的二元分类器来解决这个问题。然而，这些分类器已被证明是不可靠的。由于分类结果可能对具有影响力的决策产生影响，文本源的检测需要具有高质量。为此，本文提出了DeepTextMark，一种基于深度学习的文本水印方法，用于文本源检测。DeepTextMark通过应用Word2Vec和句子编码进行水印插入，并使用基于Transformer的分类器进行水印检测，同时实现了盲目性、鲁棒性、隐蔽性和可靠性。正如本文所进一步讨论的那样，这些特性对于通用文本源检测是不可或缺的，并且DeepTextMark在水印检测精度和抵抗攻击方面优于现有方法。

    The capabilities of text generators have grown with the rapid development of Large Language Models (LLM). To prevent potential misuse, the ability to detect whether texts are produced by LLM has become increasingly important. Several related works have attempted to solve this problem using binary classifiers that categorize input text as human-written or LLM-generated. However, these classifiers have been shown to be unreliable. As impactful decisions could be made based on the result of the classification, the text source detection needs to be high-quality. To this end, this paper presents DeepTextMark, a deep learning-based text watermarking method for text source detection. Applying Word2Vec and Sentence Encoding for watermark insertion and a transformer-based classifier for watermark detection, DeepTextMark achieves blindness, robustness, imperceptibility, and reliability simultaneously. As discussed further in the paper, these traits are indispensable for generic text source detec
    
[^77]: DifFIQA: 使用去噪扩散概率模型进行人脸图像质量评估

    DifFIQA: Face Image Quality Assessment Using Denoising Diffusion Probabilistic Models. (arXiv:2305.05768v1 [cs.CV])

    [http://arxiv.org/abs/2305.05768](http://arxiv.org/abs/2305.05768)

    DifFIQA是一个新的人脸图像质量评估方法，使用去噪扩散概率模型（DDPM）来进行面部图像扰动，并通过量化这些扰动对应的图像嵌入的影响来进行质量评估，以提供可靠的样本质量预测。

    

    现代人脸识别（FR）模型在受控情境下表现出色，但在部署到未受控制的（现实世界的）环境中时，往往由于关于捕获面部数据质量的不确定性而导致性能下降。人脸图像质量评估（FIQA）技术旨在通过提供样本质量预测来缓解这些性能降级，这些预测可用于拒绝低质量样本并减少误匹配错误。然而，尽管不断取得改进，但确保在具有多样化特征的面部图像中可靠地估计质量仍然具有挑战性。在本文中，我们提出了一种名为DifFIQA的新型FIQA方法，它依赖于去噪扩散概率模型（DDPM）并确保高度竞争的结果。该方法的主要思想是利用DDPM的前向和后向过程来扰动面部图像，并量化这些扰动对应的图像嵌入的影响以进行质量评估。

    Modern face recognition (FR) models excel in constrained scenarios, but often suffer from decreased performance when deployed in unconstrained (real-world) environments due to uncertainties surrounding the quality of the captured facial data. Face image quality assessment (FIQA) techniques aim to mitigate these performance degradations by providing FR models with sample-quality predictions that can be used to reject low-quality samples and reduce false match errors. However, despite steady improvements, ensuring reliable quality estimates across facial images with diverse characteristics remains challenging. In this paper, we present a powerful new FIQA approach, named DifFIQA, which relies on denoising diffusion probabilistic models (DDPM) and ensures highly competitive results. The main idea behind the approach is to utilize the forward and backward processes of DDPMs to perturb facial images and quantify the impact of these perturbations on the corresponding image embeddings for qua
    
[^78]: 降低现实策略优化中循环时间调整的代价

    Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization. (arXiv:2305.05760v1 [cs.LG])

    [http://arxiv.org/abs/2305.05760](http://arxiv.org/abs/2305.05760)

    本研究提出了一种新方法，可基于周期时间设置超参数，使得PPO和SAC在广泛的循环时间范围内进行学习，同时实现了接近耗时的在线超参数调整获得的性能。

    

    连续时间强化学习任务通常使用固定周期时间的离散步骤进行操作。实践中需要为给定任务选择操作周期时间，一个重要问题是学习算法的超参数是否需要为每个周期时间重新调整，这对于现实世界的机器人来说是不可行的。在本文中，我们研究了两种策略梯度算法--PPO和SAC--在不同的周期时间下使用基线超参数值的情况。通过使用基线超参数在一个基准任务中展示这两种算法表现良好的情况，我们发现当选择不同于任务默认值的周期时间时，使用基线超参数的PPO无法学习。此外，当超参数用于每个周期时间时，基于基线的PPO和SAC表现均明显劣于它们的调整值。我们提出了一种基于周期时间设置这些超参数的新方法。在我们的仿真机器人运动任务实验中，我们的方法使得PPO和SAC在极其广泛的周期时间范围内进行学习，同时实现了接近于耗时的在线超参数调整获得的性能。

    Continuous-time reinforcement learning tasks commonly use discrete steps of fixed cycle times for actions. As practitioners need to choose the action-cycle time for a given task, a significant concern is whether the hyper-parameters of the learning algorithm need to be re-tuned for each choice of the cycle time, which is prohibitive for real-world robotics. In this work, we investigate the widely-used baseline hyper-parameter values of two policy gradient algorithms -- PPO and SAC -- across different cycle times. Using a benchmark task where the baseline hyper-parameters of both algorithms were shown to work well, we reveal that when a cycle time different than the task default is chosen, PPO with baseline hyper-parameters fails to learn. Moreover, both PPO and SAC with their baseline hyper-parameters perform substantially worse than their tuned values for each cycle time. We propose novel approaches for setting these hyper-parameters based on the cycle time. In our experiments on simu
    
[^79]: 排名和重新加权提高了组的分布鲁棒性

    Ranking & Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])

    [http://arxiv.org/abs/2305.05759](http://arxiv.org/abs/2305.05759)

    本文提出了一种利用折扣累积增益（DCG）排序并加权处理训练数据以提高模型对低代表性组的鲁棒性的方法，实验证明其优于先前方法。

    

    最近的研究表明，通过经验风险最小化（ERM）进行标准训练可能会产生在平均精度上表现出色但在低代表性组上准确性较低的模型，这是由于表征中虚假特征的普遍存在所致。解决这个组鲁棒性问题的主要方法是在训练数据上最小化最坏的组误差（类似于极小值策略），希望它会在测试数据上有良好的泛化性能。然而，这种方法往往是次优的，尤其是当测试数据集中包含以前未见过的组时。本文受信息检索和Learning-to-Rank文献的启发，首先提出使用折扣累积增益（DCG）作为模型质量度量标准，以促进更好的超参数调整和模型选择。作为一种基于排序的度量标准，DCG加权多个性能较差的组（而不仅仅是考虑性能最差的组）。作为自然的下一步，我们基于这些结果提出了一种新的组重新加权方法，在训练期间鼓励模型集中于低代表性的组。在几个基准数据集上的实验证明，我们提出的方法优于先前的最先进方法，并显著提高了对组不平衡的鲁棒性。

    Recent work has shown that standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on underrepresented groups due to the prevalence of spurious features. A predominant approach to tackle this group robustness problem minimizes the worst group error (akin to a minimax strategy) on the training data, hoping it will generalize well on the testing data. However, this is often suboptimal, especially when the out-of-distribution (OOD) test data contains previously unseen groups. Inspired by ideas from the information retrieval and learning-to-rank literature, this paper first proposes to use Discounted Cumulative Gain (DCG) as a metric of model quality for facilitating better hyperparameter tuning and model selection. Being a ranking-based metric, DCG weights multiple poorly-performing groups (instead of considering just the group with the worst performance). As a natural next step, we build on our results to propose a
    
[^80]: 通过世界状态和文本指令提问的时间和问题：IGLU NLP挑战解决方案

    When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution. (arXiv:2305.05754v1 [cs.CL])

    [http://arxiv.org/abs/2305.05754](http://arxiv.org/abs/2305.05754)

    论文解决了在协作建筑任务中如何解决模棱两可的情况，从而提出了何时寻求澄清以及应该询问什么澄清问题这两个关键问题的解答方法。

    

    在协作任务中，有效的交流对于实现共同目标至关重要。其中一个任务是协作建筑，在这个任务中，建筑者必须相互通信，在诸如Minecraft之类的模拟环境中构建所需的结构。我们旨在开发一个智能建筑代理，根据用户对话建造结构。然而，在协作建筑中，建筑者可能会遇到难以解读的情况，因为信息和指令有限，导致模棱两可。在NeurIPS 2022竞赛的NLP任务中，我们回答了两个关键的研究问题，旨在填补这一空白：代理何时应该寻求澄清，应该询问什么澄清问题？我们通过两个子任务朝着这个目标迈进，一个是分类任务，一个是排序任务。对于分类任务，目标是根据当前的世界状态和对话历史确定代理是否应该寻求澄清。对于排序任务，目标是提供一份可能的澄清问题的排名列表。我们的方法结合了基于规则的启发式方法和机器学习模型，并在IGLU NLP挑战数据集上取得了竞争性的性能。

    In collaborative tasks, effective communication is crucial for achieving joint goals. One such task is collaborative building where builders must communicate with each other to construct desired structures in a simulated environment such as Minecraft. We aim to develop an intelligent builder agent to build structures based on user input through dialogue. However, in collaborative building, builders may encounter situations that are difficult to interpret based on the available information and instructions, leading to ambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key research questions, with the goal of filling this gap: when should the agent ask for clarification, and what clarification questions should it ask? We move towards this target with two sub-tasks, a classification task and a ranking task. For the classification task, the goal is to determine whether the agent should ask for clarification based on the current world state and dialogue history. For the ran
    
[^81]: 深度神经网络硬件可靠性评估方法的系统文献综述

    A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks. (arXiv:2305.05750v1 [cs.LG])

    [http://arxiv.org/abs/2305.05750](http://arxiv.org/abs/2305.05750)

    本文为深度神经网络硬件可靠性进行了系统文献综述，总结了现有的可靠性评估方法以及数据集和基准的评估，揭示了这一领域的研究空缺。

    

    人工智能（AI）和机器学习（ML）特别是由于其学习解决复杂问题的能力而被应用于各种应用程序中。过去十年中，机器学习的快速进展提出了由大量神经元和层组成的深度神经网络（DNN）。深度神经网络硬件加速器（DHA）用于将DNNs部署到目标应用程序中，同时在硬件故障/错误会导致灾难性后果的安全关键应用程序中也受益于DHA。因此，DNN的可靠性是研究的重要主题。近年来，已经发表了多项研究，以评估DNNs的可靠性。在这方面，已经提出了各种各样的可靠性评估方法，适用于各种平台和应用程序。因此，有必要总结现有技术以确定研究DNN的可靠性的研究中存在的差距。在这项工作中，我们进行了系统文献综述（SLR），以确定现有的DNN硬件可靠性评估方法。我们还回顾了现有的可用于DNN可靠性评估的数据集和基准。本研究的结果将帮助研究人员了解当前深度神经网络硬件可靠性评估方法的最新技术，同时也揭示了这一领域的遗漏空缺。

    Artificial Intelligence (AI) and, in particular, Machine Learning (ML) have emerged to be utilized in various applications due to their capability to learn how to solve complex problems. Over the last decade, rapid advances in ML have presented Deep Neural Networks (DNNs) consisting of a large number of neurons and layers. DNN Hardware Accelerators (DHAs) are leveraged to deploy DNNs in the target applications. Safety-critical applications, where hardware faults/errors would result in catastrophic consequences, also benefit from DHAs. Therefore, the reliability of DNNs is an essential subject of research. In recent years, several studies have been published accordingly to assess the reliability of DNNs. In this regard, various reliability assessment methods have been proposed on a variety of platforms and applications. Hence, there is a need to summarize the state of the art to identify the gaps in the study of the reliability of DNNs. In this work, we conduct a Systematic Literature R
    
[^82]: DOCTOR：基于可穿戴医疗传感器的多疾病检测持续学习框架

    DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on Wearable Medical Sensors. (arXiv:2305.05738v1 [cs.LG])

    [http://arxiv.org/abs/2305.05738](http://arxiv.org/abs/2305.05738)

    DOCTOR是一种基于可穿戴医疗传感器的多疾病检测持续学习框架，采用了多头深度神经网络和Exemplar-replay风格的CL算法。它可以不断地学习新任务，并在内存使用、电池消耗和检测复杂度方面优于传统的ML驱动疾病检测方法。

    

    现代机器学习（ML）和边缘设备中的可穿戴医疗传感器（WMS）的进步使得智能医疗的ML驱动疾病检测成为可能。传统的ML驱动疾病检测方法依赖于为每种疾病和相应的WMS数据定制个别模型。然而，这种方法缺乏对分布变化和新任务分类的适应性。同时，为了检测每个新疾病，需要从头开始重新构建和训练模型。针对这些挑战，我们提出了基于WMS的多疾病检测持续学习框架DOCTOR。它采用了多头深度神经网络（DNN）和一种Exemplar-replay风格的CL算法。CL算法使得框架能够不断地学习新任务，其中涉及不同的数据分布、分类类别和疾病检测任务。DOCTOR在使用来自实际WMS的公共数据集进行四种常见疾病检测方面取得了最先进的性能。同时，在内存使用、电池消耗和检测复杂度方面，DOCTOR也优于基线方法。

    Modern advances in machine learning (ML) and wearable medical sensors (WMSs) in edge devices have enabled ML-driven disease detection for smart healthcare. Conventional ML-driven disease detection methods rely on customizing individual models for each disease and its corresponding WMS data. However, such methods lack adaptability to distribution shifts and new task classification classes. Also, they need to be rearchitected and retrained from scratch for each new disease. Moreover, installing multiple ML models in an edge device consumes excessive memory, drains the battery faster, and complicates the detection process. To address these challenges, we propose DOCTOR, a multi-disease detection continual learning (CL) framework based on WMSs. It employs a multi-headed deep neural network (DNN) and an exemplar-replay-style CL algorithm. The CL algorithm enables the framework to continually learn new missions where different data distributions, classification classes, and disease detection
    
[^83]: 杜克脾脏数据集：用于训练分割算法的公开的MRI和CT脾脏数据集

    Duke Spleen Data Set: A Publicly Available Spleen MRI and CT dataset for Training Segmentation. (arXiv:2305.05732v1 [eess.IV])

    [http://arxiv.org/abs/2305.05732](http://arxiv.org/abs/2305.05732)

    杜克脾脏数据集（DSDS）是一种公开的脾脏MRI和CT数据集，用于训练深度学习模型进行自动脾脏分割。该数据集包含109个来自患有慢性肝病和门静脉高压的患者的图像，并包括多种脾脏形状和大小的混合特征。

    

    脾脏体积的测量主要与患有慢性肝病和门静脉高压的患者有关，因为他们的脾脏形状和大小通常异常。然而，手动分割脾脏以获得其体积是一个耗时的过程。深度学习算法已被证明是自动化脾脏分割的有效工具，但训练这些算法需要一个适当的数据集。据我们所知，现有的几个公开的脾脏分割数据集缺乏混淆特征，如腹水和腹壁静脉曲张。为解决这个问题，开发了杜克脾脏数据集（DSDS），其中包括来自慢性肝病和门静脉高压患者的109个CT和MRI体积。该数据集包括各种图像类型、供应商、平面和对比度，以及由于基础疾病状态导致的不同的脾脏形状和大小。DSDS旨在促进创建能考虑疾病脾脏复杂性的强大脾脏分割模型。

    Spleen volumetry is primarily associated with patients suffering from chronic liver disease and portal hypertension, as they often have spleens with abnormal shapes and sizes. However, manually segmenting the spleen to obtain its volume is a time-consuming process. Deep learning algorithms have proven to be effective in automating spleen segmentation, but a suitable dataset is necessary for training such algorithms. To our knowledge, the few publicly available datasets for spleen segmentation lack confounding features such as ascites and abdominal varices. To address this issue, the Duke Spleen Data Set (DSDS) has been developed, which includes 109 CT and MRI volumes from patients with chronic liver disease and portal hypertension. The dataset includes a diverse range of image types, vendors, planes, and contrasts, as well as varying spleen shapes and sizes due to underlying disease states. The DSDS aims to facilitate the creation of robust spleen segmentation models that can take into
    
[^84]: 基于模型复杂度的类别比例调整增强临床预测模型：以阿片类药物过量预测为例的实证研究

    Enhancing Clinical Predictive Modeling through Model Complexity-Driven Class Proportion Tuning for Class Imbalanced Data: An Empirical Study on Opioid Overdose Prediction. (arXiv:2305.05722v1 [cs.LG])

    [http://arxiv.org/abs/2305.05722](http://arxiv.org/abs/2305.05722)

    该研究提出了一种针对类别不平衡问题的理论框架，通过将最佳类别比例与模型复杂度联系起来，每种模型都能够得到正确的类别比例，实验结果表明此方法在阿片类药物过量预测问题上能够显著提高模型性能。

    

    类别不平衡问题在医疗领域广泛存在，严重破坏了临床预测模型的性能。大多数缓解问题的方法通过重新平衡类别比例来实现，它们主要假设重新平衡的比例应该是原始数据的函数，与所使用的模型无关。本文对这一流行假设提出了挑战，提出了将最佳类别比例与模型复杂度联系起来的理论框架，从而针对每种模型调整类别比例。我们在阿片类药物过量预测问题上的实验突出了调整类别比例的性能提升。严格的回归分析也证实了所提出的理论框架的优点，以及控制模型复杂度和最佳类别比例之间的超参数之间的统计显着相关性。

    Class imbalance problems widely exist in the medical field and heavily deteriorates performance of clinical predictive models. Most techniques to alleviate the problem rebalance class proportions and they predominantly assume the rebalanced proportions should be a function of the original data and oblivious to the model one uses. This work challenges this prevailing assumption and proposes that links the optimal class proportions to the model complexity, thereby tuning the class proportions per model. Our experiments on the opioid overdose prediction problem highlight the performance gain of tuning class proportions. Rigorous regression analysis also confirms the advantages of the theoretical framework proposed and the statistically significant correlation between the hyperparameters controlling the model complexity and the optimal class proportions.
    
[^85]: 语言模型可以直接以XYZ、CIF和PDB文件的形式生成分子、材料和蛋白质结合位点的三维结构

    Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files. (arXiv:2305.05708v1 [cs.LG])

    [http://arxiv.org/abs/2305.05708](http://arxiv.org/abs/2305.05708)

    使用语言模型可以直接生成三维的新颖有效的分子、材料和蛋白质结合位点的结构，而不需转换成线性字符串表示形式。

    

    语言模型是分子设计的强大工具。当前主流的思路是将分子图形解析成线性字符串表示，从而易于训练。这种方法非常成功，但它仅限于可以完全通过图形表示的化学结构，如有机分子，而材料和生物分子结构如蛋白质结合位点需要更完整的表示方式，其中包括它们的原子在空间中的相对定位。在本研究中，我们展示了语言模型如何通过使用下一个令牌预测进行训练，不需要任何架构修改，就能够从各种基于化学结构的非常不同的分布中生成三维的新颖且有效的结构。特别是，我们证明了直接训练于从化学文件格式如XYZ文件、晶体学信息文件（CIF）或蛋白质数据库文件（PDB）派生的序列的语言模型可以直接生成三维结构。

    Language models are powerful tools for molecular design. Currently, the dominant paradigm is to parse molecular graphs into linear string representations that can easily be trained on. This approach has been very successful, however, it is limited to chemical structures that can be completely represented by a graph -- like organic molecules -- while materials and biomolecular structures like protein binding sites require a more complete representation that includes the relative positioning of their atoms in space. In this work, we show how language models, without any architecture modifications, trained using next-token prediction -- can generate novel and valid structures in three dimensions from various substantially different distributions of chemical structures. In particular, we demonstrate that language models trained directly on sequences derived directly from chemical file formats like XYZ files, Crystallographic Information files (CIFs), or Protein Data Bank files (PDBs) can d
    
[^86]: DexArt：基于关节物体的通用灵巧操作基准测试

    DexArt: Benchmarking Generalizable Dexterous Manipulation with Articulated Objects. (arXiv:2305.05706v1 [cs.RO])

    [http://arxiv.org/abs/2305.05706](http://arxiv.org/abs/2305.05706)

    该论文提出了一个名为 DexArt 的基准测试，其旨在通过使用物理模拟器，评估机器人手在未见过的关节物体上进行灵巧操作的泛化能力。

    

    为了使机器人能够通用，我们需要让机器人像人类一样操作日常使用的关节物品。目前，机器人操纵大量依赖于使用平行夹具，这使得机器人局限于一些物体的操作。而使用多指机器人手将更好地近似人类行为，并使机器人在多样化的关节物体上进行操作。为此，我们提出了一个新的基准测试称为 DexArt，它涉及使用物理模拟器进行关节物体的灵巧操作。在我们的基准测试中，我们定义了多个复杂的操作任务，机器人手需要在每个任务中操作多样化的关节物体。我们的主要重点是评估在未见过的关节物体上学习到的策略的泛化能力。考虑到手和物体的自由度很高，这非常具有挑战性。我们使用强化学习和三维表示学习来实现泛化。通过实验，我们证明了我们的方法在未见过的关节物体上具有很好的泛化性。

    To enable general-purpose robots, we will require the robot to operate daily articulated objects as humans do. Current robot manipulation has heavily relied on using a parallel gripper, which restricts the robot to a limited set of objects. On the other hand, operating with a multi-finger robot hand will allow better approximation to human behavior and enable the robot to operate on diverse articulated objects. To this end, we propose a new benchmark called DexArt, which involves Dexterous manipulation with Articulated objects in a physical simulator. In our benchmark, we define multiple complex manipulation tasks, and the robot hand will need to manipulate diverse articulated objects within each task. Our main focus is to evaluate the generalizability of the learned policy on unseen articulated objects. This is very challenging given the high degrees of freedom of both hands and objects. We use Reinforcement Learning with 3D representation learning to achieve generalization. Through e
    
[^87]: 应用机器学习的决策系统中，数据时间滞后对猪肉价格预测的影响研究

    Effects of data time lag in a decision-making system using machine learning for pork price prediction. (arXiv:2305.05677v1 [cs.LG])

    [http://arxiv.org/abs/2305.05677](http://arxiv.org/abs/2305.05677)

    本文研究了数据采集延迟对机器学习预测模型在猪肉价格预测中的影响，并提出了解决方案，通过利用同日获取的数据，可以有效缓解数据时间滞后的影响。

    

    西班牙是世界上第三大猪肉生产国，许多农场依赖于这个市场。然而，当前的定价体系是不公平的，因为一些人比其他人有更好的市场信息。在这种情况下，历史价格是易于获取和经济实惠的数据来源，可以帮助所有代理商更好地了解市场。然而，数据采集延迟可能会影响他们的定价决策。在本文中，我们研究了多个预测算法在价格预测系统中的数据采集延迟对其影响。我们描述了最佳提案的集成到决策支持系统原型中，并在实际情况下进行了测试。具体而言，我们使用西班牙最重要的地区猪肉市场的公共数据，由农业部发布，延迟两周，和订阅方式获取的同一日的同一市场数据。结果表明，最佳预测算法和两周延迟的数据相比，订阅数据可以减少错误差异，这证明了集成多个算法可以有效缓解数据时间滞后的影响。

    Spain is the third-largest producer of pork meat in the world, and many farms in several regions depend on the evolution of this market. However, the current pricing system is unfair, as some actors have better market information than others. In this context, historical pricing is an easy-to-find and affordable data source that can help all agents to be better informed. However, the time lag in data acquisition can affect their pricing decisions. In this paper, we study the effect that data acquisition delay has on a price prediction system using multiple prediction algorithms. We describe the integration of the best proposal into a decision support system prototype and test it in a real-case scenario. Specifically, we use public data from the most important regional pork meat markets in Spain published by the Ministry of Agriculture with a two-week delay and subscription-based data of the same markets obtained on the same day. The results show that the error difference between the bes
    
[^88]: UAdam：非凸随机优化的统一Adam型算法框架

    UAdam: Unified Adam-Type Algorithmic Framework for Non-Convex Stochastic Optimization. (arXiv:2305.05675v1 [cs.LG])

    [http://arxiv.org/abs/2305.05675](http://arxiv.org/abs/2305.05675)

    本论文提出了统一的Adam型算法框架UAdam，该框架是包括Adam及其变体在内的特例，并且在非凸随机设置下具有收敛性，可以以$\mathcal{O}(1/T)$的速率收敛到静止点的邻域。通过选择适当的超参数，香草Adam也可以收敛。

    

    Adam型算法在深度学习中已成为优化的首选，然而，尽管成功，其收敛性仍不够理解。为此，我们引入了一个统一Adam型算法框架（称为UAdam）。它配备了第二阶矩的一般形式，可以包括Adam及其变体作为特例，如NAdam、AMSGrad、AdaBound、AdaFom和Adan。在非凸随机设置下，我们对UAdam进行了严格的收敛性分析，表明UAdam以$\mathcal{O}(1/T)$的速率收敛到静止点的邻域。此外，随着$\beta$的增加，邻域的大小减小。重要的是，我们的分析仅要求第一阶动量因子足够接近1，对于第二阶动量因子没有任何限制。理论结果还表明，通过选择适当的超参数，香草Adam可以收敛，这为实践者选择Adam族算法的超参数提供了指导。

    Adam-type algorithms have become a preferred choice for optimisation in the deep learning setting, however, despite success, their convergence is still not well understood. To this end, we introduce a unified framework for Adam-type algorithms (called UAdam). This is equipped with a general form of the second-order moment, which makes it possible to include Adam and its variants as special cases, such as NAdam, AMSGrad, AdaBound, AdaFom, and Adan. This is supported by a rigorous convergence analysis of UAdam in the non-convex stochastic setting, showing that UAdam converges to the neighborhood of stationary points with the rate of $\mathcal{O}(1/T)$. Furthermore, the size of neighborhood decreases as $\beta$ increases. Importantly, our analysis only requires the first-order momentum factor to be close enough to 1, without any restrictions on the second-order momentum factor. Theoretical results also show that vanilla Adam can converge by selecting appropriate hyperparameters, which pro
    
[^89]: 基于图卷积循环网络的危险驾驶行为精准检测提高道路安全性

    Enhancing Road Safety through Accurate Detection of Hazardous Driving Behaviors with Graph Convolutional Recurrent Networks. (arXiv:2305.05670v1 [cs.LG])

    [http://arxiv.org/abs/2305.05670](http://arxiv.org/abs/2305.05670)

    本文提出了一种基于图卷积循环网络的可靠DBD系统，利用公共传感器提高了驾驶行为检测模型的准确性和实用性。

    

    车祸仍然是全球重大的公共安全问题，其中的大多数归因于驾驶员错误，包括不足的驾驶知识、不合规定以及不良驾驶习惯。为了提高道路安全性，多项驾驶行为检测（DBD）系统已经被提出，以识别安全和不安全的驾驶行为。这些研究中的许多利用了从控制器区域网络（CAN）总线获取的传感器数据来构建模型。然而，使用公共可用传感器已知会降低检测模型的准确性，而将供应商特定的传感器纳入数据集中则可以增加准确性。为了解决现有方法的局限性，我们提出了一个基于图卷积长短期记忆网络（GConvLSTM）的可靠DBD系统，利用公共传感器提高了DBD模型的精度和实用性。此外，我们还加入了非公共传感器来评估模型的有效性。

    Car accidents remain a significant public safety issue worldwide, with the majority of them attributed to driver errors stemming from inadequate driving knowledge, non-compliance with regulations, and poor driving habits. To improve road safety, Driving Behavior Detection (DBD) systems have been proposed in several studies to identify safe and unsafe driving behavior. Many of these studies have utilized sensor data obtained from the Controller Area Network (CAN) bus to construct their models. However, the use of publicly available sensors is known to reduce the accuracy of detection models, while incorporating vendor-specific sensors into the dataset increases accuracy. To address the limitations of existing approaches, we present a reliable DBD system based on Graph Convolutional Long Short-Term Memory Networks (GConvLSTM) that enhances the precision and practicality of DBD models using public sensors. Additionally, we incorporate non-public sensors to evaluate the model's effectivene
    
[^90]: 基于神经符号人工智能算法的预测增材制造聚乳酸（PLA）样品冲击强度

    Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA) Specimens. (arXiv:2305.05668v1 [cs.LG])

    [http://arxiv.org/abs/2305.05668](http://arxiv.org/abs/2305.05668)

    本研究介绍了一种基于神经符号人工智能的算法，能够高精度预测增材制造聚乳酸（PLA）样品的冲击强度。

    

    本研究介绍了神经符号人工智能（NSAI）在预测增材制造聚乳酸（PLA）组件冲击强度方面的应用，是NSAI在增材制造领域首次应用。NSAI模型融合了神经网络和符号人工智能的优势，相比传统的机器学习技术能够提供更精准和准确的预测。采集了实验数据并进行了合成增强，使模型更加精确。神经符号模型使用包括输入、两个隐藏层、和一个输出层的神经网络架构来开发，接着是一个代表符号组件的决策树回归器。通过对比评估训练集和验证集的均方误差（MSE）和R2值，模型的表现被与简单人工神经网络（ANN）模型进行了基准测试。结果显示，神经符号模型优于简单的ANN模型，在两个数据集上的MSE更低，R2值更高。总的来说，所提出的基于NSAI算法的方法在预测增材制造PLA组件的冲击强度方面表现出极高的准确性。

    In this study, we introduce application of Neurosymbolic Artificial Intelligence (NSAI) for predicting the impact strength of additive manufactured polylactic acid (PLA) components, representing the first-ever use of NSAI in the domain of additive manufacturing. The NSAI model amalgamates the advantages of neural networks and symbolic AI, offering a more robust and accurate prediction than traditional machine learning techniques. Experimental data was collected and synthetically augmented to 1000 data points, enhancing the model's precision. The Neurosymbolic model was developed using a neural network architecture comprising input, two hidden layers, and an output layer, followed by a decision tree regressor representing the symbolic component. The model's performance was benchmarked against a Simple Artificial Neural Network (ANN) model by assessing mean squared error (MSE) and R-squared (R2) values for both training and validation datasets. The results reveal that the Neurosymbolic m
    
[^91]: 面向个人或实体的知识图谱表示学习：在医疗保健领域应用的研究

    Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])

    [http://arxiv.org/abs/2305.05640](http://arxiv.org/abs/2305.05640)

    本研究提出了一种在医疗保健领域构建面向实体的知识图谱的端到端表示学习方法 HEER，通过将领域特定的约束和特征纳入到图嵌入算法中，有效地改善了下游预测任务。

    

    知识图谱是一种按本体或模式组织信息的流行方式，已经在从搜索到推荐的各种场景中得到了应用。尽管在知识图谱方面有了一些进展，但知识表示仍然是跨行业的一个非常棘手的任务，特别是在生物医学和医疗保健领域，由于实体之间的复杂相互关系、异质性、缺乏标准化和数据稀疏性等因素，这一任务尤其具有挑战性。本文提出了一种面向医疗保健领域构建面向实体的知识图谱的端到端表示学习方法，重点是捕捉生物医学领域的独特特征。所提出的框架名为HEER（Healthcare Entity-Entity Representation learning），将领域特定的约束和特征纳入到图嵌入算法中。对多个基准数据集的结果表明，与最先进的方法相比，HEER在改善下游预测任务方面具有有效性。

    Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
    
[^92]: TASTY：一种基于Transformer的时空复杂度分析方法

    TASTY: A Transformer based Approach to Space and Time complexitY. (arXiv:2305.05379v1 [cs.SE])

    [http://arxiv.org/abs/2305.05379](http://arxiv.org/abs/2305.05379)

    本文旨在通过创建一个跨多种语言的代码片段标记数据集，以填补从代码中分类时间和空间复杂性的空白，并提出了使用基于代码的多模型来实现这一目标。

    

    基于代码的语言模型在软件工程领域中表现出非常有前途的结果，如代码的完善、代码的补全和生成。然而，由于缺乏数据集，从代码中分类时间和空间复杂性的任务还未得到广泛探索，先前的努力仅限于Java。在这个项目中，我们旨在通过创建一个跨多种语言的代码片段标记数据集来填补这些空白（目前是Python和C ++数据集，不久将发布C，C＃和JavaScript数据集）。我们发现现有的时间复杂性计算库和工具仅适用于少数用例。缺乏明确定义的基于规则的系统促使运用最近提出的基于代码的多模型。我们展示了死代码消除和增加LM的最大序列长度的有效性。除了时间复杂性外，我们还建议使用LM来寻找空间复杂性。

    Code based Language Models (LMs) have shown very promising results in the field of software engineering with applications such as code refinement, code completion and generation. However, the task of time and space complexity classification from code has not been extensively explored due to a lack of datasets, with prior endeavors being limited to Java. In this project, we aim to address these gaps by creating a labelled dataset of code snippets spanning multiple languages (Python and C++ datasets currently, with C, C#, and JavaScript datasets being released shortly). We find that existing time complexity calculation libraries and tools only apply to a limited number of use-cases. The lack of a well-defined rule based system motivates the application of several recently proposed code-based LMs. We demonstrate the effectiveness of dead code elimination and increasing the maximum sequence length of LMs. In addition to time complexity, we propose to use LMs to find space complexities from
    
[^93]: 基于大语言模型知识蒸馏的网络内容过滤方法

    Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])

    [http://arxiv.org/abs/2305.05027](http://arxiv.org/abs/2305.05027)

    本文提出了一种基于大语言模型知识蒸馏的 URL 分类方法，可用于网络内容过滤，其学生模型在参数数量减少 175 倍的情况下，精度提升了 9%，超过了当前最先进方法。

    

    本文提出了一种基于大语言模型的 URL 分类方法，旨在实现网络内容过滤的主要目标：保障组织免受法律和伦理风险，限制访问高风险或可疑网站，以及促进安全的专业工作环境。我们的方法利用大语言模型生成准确的分类，并利用已有的知识蒸馏技术创建更小、更专业的学生模型，以用于网络内容过滤。在将通过大型安全供应商收集的客户遥测数据的 30 个不同内容类别的网站进行分类的任务中，我们的学生模型通过蒸馏结果实现了 9% 的分类精度提升，超过了当前最先进方法。我们的学生模型在参数数量上与原始的大语言模型相比减少了 175 倍，从而达到了与老师模型相匹配的性能，可以用于大规模的在线扫描。

    We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9\% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large vo
    
[^94]: 基于抽象语法树的异构有向超图神经网络用于代码分类

    Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2305.04228](http://arxiv.org/abs/2305.04228)

    本研究提出了使用异构有向超图表示AST，并使用异构有向超图神经网络处理图形进行代码分类，超过了现有方法。

    

    代码分类是程序理解和自动编码中的一个难题。由于程序的模糊语法和复杂语义，大多数现有研究使用基于抽象语法树（AST）和图神经网络（GNN）的技术创建代码表示用于代码分类。这些技术利用代码的结构和语义信息，但只考虑节点之间的成对关系，忽略了AST中节点之间已经存在的高阶相关性，可能导致代码结构信息的丢失。本研究提出使用异构有向超图（HDHG）表示AST，并使用异构有向超图神经网络（HDHGN）处理图形。HDHG保留了节点之间的高阶相关性，并更全面地编码了AST的语义和结构信息。HDHGN通过聚合不同节点的特征并使用不同的函数对其进行处理来对AST进行建模。在四个数据集上的实验表明，HDHG和HDHGN在代码分类任务中超越了现有方法。

    Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
    
[^95]: 在增长批量强化学习中教师向学习者的知识转移

    Knowledge Transfer from Teachers to Learners in Growing-Batch Reinforcement Learning. (arXiv:2305.03870v1 [cs.LG])

    [http://arxiv.org/abs/2305.03870](http://arxiv.org/abs/2305.03870)

    本研究讨论了实际领域中离线训练或增长批量训练的限制，提出了一种教师向学习者进行知识转移的方法，使得数据数量和多样性得到提高。

    

    序贯决策制定的标准方法利用了智能体不断与其环境交互和改善其控制策略的能力。然而，由于安全、伦理和实用性的限制，这种试错实验方法在许多实际领域（如医疗保健和机器人技术）中往往不可行。在这种情况下，这些领域中的控制策略通常是通过以先前记录的数据为基础进行离线训练或逐步扩展进行训练的。在这个设置中，固定的策略被部署到环境中，并用于收集一整个批次的新数据，然后与过去的批次汇总并用于更新策略。可以多次重复这个改进周期。虽然在实际领域中有限数量的这样的周期是可行的，但产生的数据数量和多样性远远低于标准的不断交互方法。但是，在这些领域中进行数据收集通常是与人类专家协作的。

    Standard approaches to sequential decision-making exploit an agent's ability to continually interact with its environment and improve its control policy. However, due to safety, ethical, and practicality constraints, this type of trial-and-error experimentation is often infeasible in many real-world domains such as healthcare and robotics. Instead, control policies in these domains are typically trained offline from previously logged data or in a growing-batch manner. In this setting a fixed policy is deployed to the environment and used to gather an entire batch of new data before being aggregated with past batches and used to update the policy. This improvement cycle can then be repeated multiple times. While a limited number of such cycles is feasible in real-world domains, the quantity and diversity of the resulting data are much lower than in the standard continually-interacting approach. However, data collection in these domains is often performed in conjunction with human expert
    
[^96]: 深度学习库对在线自适应轻量级时间序列异常检测的影响

    Impact of Deep Learning Libraries on Online Adaptive Lightweight Time Series Anomaly Detection. (arXiv:2305.00595v1 [cs.LG])

    [http://arxiv.org/abs/2305.00595](http://arxiv.org/abs/2305.00595)

    本文通过在三个深度学习库中实现两种最先进的方法，并进行评估，研究了深度学习库对在线自适应轻量级时间序列异常检测的影响。

    

    在没有人为干预和领域知识的情况下提供在线自适应轻量级时间序列异常检测非常有价值。过去几年中引入了几种这样的异常检测方法，但它们都仅在一个深度学习库中实现。随着深度学习库的发展，不同的深度学习库如何影响这些异常检测方法尚不清楚，因为没有这样的评估。随机选择一个深度学习库来实现异常检测方法可能不能展现出该方法的真实性能，这可能会误导用户相信某种方法比另一种更好。因此，在本文中，我们通过在三个知名深度学习库中实现两种最先进的异常检测方法，并评估这些方法的结果，来研究深度学习库对在线自适应轻量级时间序列异常检测的影响。

    Providing online adaptive lightweight time series anomaly detection without human intervention and domain knowledge is highly valuable. Several such anomaly detection approaches have been introduced in the past years, but all of them were only implemented in one deep learning library. With the development of deep learning libraries, it is unclear how different deep learning libraries impact these anomaly detection approaches since there is no such evaluation available. Randomly choosing a deep learning library to implement an anomaly detection approach might not be able to show the true performance of the approach. It might also mislead users in believing one approach is better than another. Therefore, in this paper, we investigate the impact of deep learning libraries on online adaptive lightweight time series anomaly detection by implementing two state-of-the-art anomaly detection approaches in three well-known deep learning libraries and evaluating how these two approaches are indiv
    
[^97]: AdaNPC：探索非参数分类器进行测试时间适应性

    AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation. (arXiv:2304.12566v1 [cs.LG])

    [http://arxiv.org/abs/2304.12566](http://arxiv.org/abs/2304.12566)

    本文提出了一种新的测试时间自适应（TTA）方法AdaNPC，通过利用非参数分类器进行建模从而避免了离线目标数据或在推理时使用额外的复杂优化过程。AdaNPC从存储器中回顾最相似的 K 个样本进行投票预测，逐渐改变存储器中的样本分布以提高测试域性能。

    

    许多最近的机器学习任务都集中在开发能够推广到未见过分布的模型上。域通用性（DG）已成为各个领域中的关键课题之一。几篇文献表明，如果不利用目标域的信息，域通用性可能会变得极其困难。为了解决这个问题，提出了测试时间自适应（TTA）方法。现有的TTA方法需要离线目标数据或在推理阶段使用额外的复杂优化过程。本文采用非参数分类器进行测试时间自适应（AdaNPC）。具体地，在训练过程中构建一个包含特征和标签对的存储器。在推理时，给定一个测试实例，AdaNPC首先从存储器中回顾K个最相似的样本进行投票预测，然后将测试特征和预测标签添加到存储器中。通过这种方式，存储器中的样本分布可以逐渐从训练分布向测试分布变化，从而提高测试域的性能。我们在各种基准数据集上进行了大量实验，结果表明我们的方法优于几种最先进的TTA方法，并取得了与完全监督方法相当的性能。

    Many recent machine learning tasks focus to develop models that can generalize to unseen distributions. Domain generalization (DG) has become one of the key topics in various fields. Several literatures show that DG can be arbitrarily hard without exploiting target domain information. To address this issue, test-time adaptive (TTA) methods are proposed. Existing TTA methods require offline target data or extra sophisticated optimization procedures during the inference stage. In this work, we adopt Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC). In particular, we construct a memory that contains the feature and label pairs from training domains. During inference, given a test instance, AdaNPC first recalls K closed samples from the memory to vote for the prediction, and then the test feature and predicted label are added to the memory. In this way, the sample distribution in the memory can be gradually changed from the training distribution towards the test distr
    
[^98]: 基于并行bootstrap的连续流控制应用的on-policy深度强化学习

    Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])

    [http://arxiv.org/abs/2304.12330](http://arxiv.org/abs/2304.12330)

    本文提出了一种基于并行bootstrap的on-policy深度强化学习方法，通过部分轨迹缓冲区和返回bootstrapping步骤来实现灵活使用并行环境，同时保持更新的on-policy性，该方法在连续流控制问题上有很好的应用前景。

    

    深度强化学习与数值流控问题的耦合近期引起了相当大的关注，取得了突破性的成果并为该领域开辟了新的前景。然而，由于流体动力学求解器的计算成本通常很高，在学习过程中使用并行环境是实现有效控制的必要手段。尽管如此，大多数基于流控的深度强化学习文献仍依赖于on-policy算法，而这种算法的高并行转移收集可能会破坏理论假设并导致次优的控制模型。为了克服这个问题，我们提出了一个基于部分轨迹缓冲区的并行模式，通过一个返回bootstrapping步骤，允许灵活地使用并行环境，同时保持更新的on-policy性。该方法在文献中一个耗费大量计算的连续流控制问题上进行了说明。

    The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.
    
[^99]: 使用Pylogik进行医学影像去标识化和清洗压缩

    Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])

    [http://arxiv.org/abs/2304.12322](http://arxiv.org/abs/2304.12322)

    提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。

    

    应用大数据和机器学习在医疗记录信息方面须注意，必须清洗和去标识化数据。当受保护的健康信息嵌入在影像元数据中时，促进多中心合作中数据共享和协调变得尤其困难。我们提出了一个新的Python框架下的库，称为PyLogik，帮助解决超声图像特别具有挑战性的数据清洗问题，因为这些图像直接包含很多PHI。PyLogik通过一系列的文本检测/提取、过滤、阈值化、形态学和轮廓比较处理图像体积。这种方法去标识化图像，减小文件大小，并为深度学习和数据共享应用准备好了图像数据。为了评估PyLogik在兴趣区域（ROI）的识别有效性，随机抽取了50张心脏超声图像（超声心动图）进行处理。

    Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
    
[^100]: 基于扩散的生成式人工智能在探索二维分子图的过渡态上的应用

    Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v2 [physics.chem-ph] UPDATED)

    [http://arxiv.org/abs/2304.12233](http://arxiv.org/abs/2304.12233)

    本文提出了一种基于扩散的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。与现有具有 3D 几何结构机器学习模型相比，TSdiff 的准确性和效率都更高。TSDiff 能够找到比参考数据库更优化的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。

    

    探究过渡态几何结构对于阐明化学反应机理和模拟反应动力学至关重要。最近，机器学习模型在预测过渡态几何结构方面表现出了出色的性能。然而，它们需要反应物和产物的 3D 形态和方向作为输入，这需要大量的努力和计算成本。在这里，我们提出了一种基于随机扩散方法的生成式方法(TSDiff)，用于从二维分子图中预测过渡态几何结构。TSDiff在准确性和效率方面优于现有的具有 3D 几何结构的机器学习模型。此外，它可以从训练中了解到各种反应的过渡态几何分布，从而能够采样各种过渡态构象。因此，TSDiff 能够找到比参考数据库中更为有利的反应途径，在反应势垒更低的情况下找到更为优化的反应路径。这些结果表明，TSDiff 在加速化学反应和反应途径的发现方面具有潜在的价值。

    The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows pr
    
[^101]: 使用迁移学习实现隐私保护的CNN训练

    Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])

    [http://arxiv.org/abs/2304.03807](http://arxiv.org/abs/2304.03807)

    本文提出了一种使用迁移学习实现同态加密技术下隐私保护的CNN训练的方案，通过转换思想和更快的梯度变体，取得了最先进的性能。

    

    隐私保护的神经网络推理已经得到很好的研究，同时保持同态CNN训练仍然是一项挑战性的任务。在本文中，我们提出了一种实用的解决方案来实现基于同态加密技术的隐私保护CNN训练。据我们所知，这是第一次成功突破这个难题，以前没有任何工作达到这个目标。采用了几种技术：（1）通过迁移学习，可以将隐私保护的CNN训练简化为同态神经网络训练，甚至是多类逻辑回归（MLR）训练；（2）通过更快的梯度变体$\texttt{Quadratic Gradient}$，应用于MLR的增强梯度方法，在收敛速度方面具有最先进的性能；（3）我们采用数学中的变换思想，将加密域中的近似Softmax函数转换成已经研究过的逼近方法，从而得到更好的结果。

    Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
    
[^102]: 基于模块化正则化可改善观察含噪数据的高斯图模型

    Module-based regularization improves Gaussian graphical models when observing noisy data. (arXiv:2303.16796v1 [physics.data-an])

    [http://arxiv.org/abs/2303.16796](http://arxiv.org/abs/2303.16796)

    建议将推断网络的模块化结构整合到正则化强度的交叉验证中，以改善高斯图模型在观察含噪数据时的表现。

    

    研究人员经常使用高斯图模型表示多变量相关数据中的关系，这些模型需要正则化来稀疏模型。我们建议在正则化强度的交叉验证中，将推断网络的模块化结构整合起来以平衡欠拟合和过拟合。使用合成和真实数据，我们发现与使用高斯对数似然进行交叉验证的标准方法（图形套索法）相比，这种方法可以更好地恢复和推断含噪声数据中的模块化结构。

    Researchers often represent relations in multi-variate correlational data using Gaussian graphical models, which require regularization to sparsify the models. Acknowledging that they often study the modular structure of the inferred network, we suggest integrating it in the cross-validation of the regularization strength to balance under- and overfitting. Using synthetic and real data, we show that this approach allows us to better recover and infer modular structure in noisy data compared with the graphical lasso, a standard approach using the Gaussian log-likelihood when cross-validating the regularization strength.
    
[^103]: 平稳随机凸优化中GD和SGD的泛化下界降低

    Lower Generalization Bounds for GD and SGD in Smooth Stochastic Convex Optimization. (arXiv:2303.10758v1 [cs.LG])

    [http://arxiv.org/abs/2303.10758](http://arxiv.org/abs/2303.10758)

    本论文证明了在平稳随机凸优化中，GD和SGD的泛化下界可以降低，并且长时间的训练可能导致更差的泛化能力，这与其他研究成果不同。

    

    最近，学习理论界在刻画一般凸损失梯度方法的泛化误差方面取得了进展。本文侧重于讨论在泛化光滑随机凸优化（SCO）问题中训练时间如何影响泛化能力。我们首先为一般的不可实现SCO问题提供了严格的下界。此外，现有的上界结果表明，假设损失可实现（即最优解同时最小化所有数据点）可以提高样本复杂度。但是，当训练时间长且缺乏下界时，这种改进会受到损害。我们对此进行了研究，提供了对于梯度下降（GD）和随机梯度下降（SGD）在两种可实现情况下的过量风险下界：1）实现需$T = O(n)$，和（2）实现需$T = \Omega(n)$，其中$T$表示训练迭代次数，$n$为训练数据集的大小。这些下界的证明使用了来自优化的现代工具，包括对偶理论和镜像下降。我们的结果表明，在可实现的SCO中，更长的训练时间可能会导致更差的泛化，这与文献中的先前发现形成鲜明对比。我们还提供了支持我们结果的数值实验。

    Recent progress was made in characterizing the generalization error of gradient methods for general convex loss by the learning theory community. In this work, we focus on how training longer might affect generalization in smooth stochastic convex optimization (SCO) problems. We first provide tight lower bounds for general non-realizable SCO problems. Furthermore, existing upper bound results suggest that sample complexity can be improved by assuming the loss is realizable, i.e. an optimal solution simultaneously minimizes all the data points. However, this improvement is compromised when training time is long and lower bounds are lacking. Our paper examines this observation by providing excess risk lower bounds for gradient descent (GD) and stochastic gradient descent (SGD) in two realizable settings: 1) realizable with $T = O(n)$, and (2) realizable with $T = \Omega(n)$, where $T$ denotes the number of training iterations and $n$ is the size of the training dataset. These bounds are 
    
[^104]: 一种层次Transformer动态变分自编码器的语音建模

    Speech Modeling with a Hierarchical Transformer Dynamical VAE. (arXiv:2303.09404v1 [eess.AS])

    [http://arxiv.org/abs/2303.09404](http://arxiv.org/abs/2303.09404)

    HiT-DVAE是一种层次Transformer动态变分自编码器，能够优于其他DVAEs在语音频谱建模方面。它具有两个潜变量水平和简单的训练过程，并且具有很高的潜力在低级别语音处理方面。

    

    动态变分自编码器（DVAEs）是一类潜变量深度生成模型，扩展了VAE以对观察到的数据序列和相应的潜向量序列进行建模。在文献中的几乎所有DVAEs中，每个序列内部和两个序列之间的时间依赖性都由循环神经网络来建模。在本文中，我们提出使用Hierarchical Transformer DVAE（HiT-DVAE）模拟语音信号，它是一种具有两个潜变量水平（序列级和帧级）的DVAE，并且其中时间依赖性是使用Transformer架构实现的。我们展示了HiT-DVAE在语音频谱建模方面优于其他若干DVAEs，同时还能够实现更简单的训练过程，显示出其在下游低级别语音处理任务（如语音增强）中具有很高的潜力。

    The dynamical variational autoencoders (DVAEs) are a family of latent-variable deep generative models that extends the VAE to model a sequence of observed data and a corresponding sequence of latent vectors. In almost all the DVAEs of the literature, the temporal dependencies within each sequence and across the two sequences are modeled with recurrent neural networks. In this paper, we propose to model speech signals with the Hierarchical Transformer DVAE (HiT-DVAE), which is a DVAE with two levels of latent variable (sequence-wise and frame-wise) and in which the temporal dependencies are implemented with the Transformer architecture. We show that HiT-DVAE outperforms several other DVAEs for speech spectrogram modeling, while enabling a simpler training procedure, revealing its high potential for downstream low-level speech processing tasks such as speech enhancement.
    
[^105]: 重新发现CNN在原始电子健康记录文本编码中的多功能性

    Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v1 [cs.LG])

    [http://arxiv.org/abs/2303.08290](http://arxiv.org/abs/2303.08290)

    本文发现，CNN在健康记录文本编码方面的多功能性和隐含层次结构可以提高其性能，提出了一种基于CNN的编码器来处理不同类型的EHR特征，并在临床任务中展示了其有效性。

    

    充分利用电子健康记录（EHR）中丰富的信息正逐渐成为医学领域的重要话题。最近的工作提出了一个有前途的框架，该框架可以在不考虑其格式和医学编码标准的情况下嵌入原始EHR数据的整个特征。然而，该框架仅侧重于对EHR进行最小的预处理，未考虑如何学习高效的EHR表示，包括计算和内存使用等方面。在本文中，我们寻找一种多功能的编码器，不仅将大量数据缩小到可管理的大小，还能很好地保留患者的核心信息，以执行各种临床任务。我们发现，具有分层结构的卷积神经网络（CNN）在各种任务（如重建，预测和生成）中经常优于最先进的模型，即使参数较少且训练时间较短。此外，利用EHR数据的固有层次结构可以提高CNN的性能。在这些发现的基础上，我们提出了一种基于CNN的编码器，可以处理不同类型的EHR特征，并证明了所提出的模型在几种临床任务中的有效性。

    Making the most use of abundant information in electronic health records (EHR) is rapidly becoming an important topic in the medical domain. Recent work presented a promising framework that embeds entire features in raw EHR data regardless of its form and medical code standards. The framework, however, only focuses on encoding EHR with minimal preprocessing and fails to consider how to learn efficient EHR representation in terms of computation and memory usage. In this paper, we search for a versatile encoder not only reducing the large data into a manageable size but also well preserving the core information of patients to perform diverse clinical tasks. We found that hierarchically structured Convolutional Neural Network (CNN) often outperforms the state-of-the-art model on diverse tasks such as reconstruction, prediction, and generation, even with fewer parameters and less training time. Moreover, it turns out that making use of the inherent hierarchy of EHR data can boost the perfo
    
[^106]: 概率扩散模型的广义尺度空间特性

    Generalised Scale-Space Properties for Probabilistic Diffusion Models. (arXiv:2303.07900v1 [eess.IV])

    [http://arxiv.org/abs/2303.07900](http://arxiv.org/abs/2303.07900)

    本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。

    

    概率扩散模型在深度学习社区中越来越受欢迎。它们生成从学习图像分布的令人信服的样本，具有广泛的实际应用。这些方法最初是受漂移-扩散过程的启发，但在近期的实践导向的出版物中，这些起源得到了较少的关注。本文从尺度空间研究的角度研究概率扩散模型，并展示它们在演化概率分布上满足广义尺度空间特性。此外，我们讨论了深度学习和基于模型的世界中漂移-扩散物理核心概念解释之间的相似性和差异。为此，我们考察了概率扩散与渗透滤波器之间的关系。

    Probabilistic diffusion models enjoy increasing popularity in the deep learning community. They generate convincing samples from a learned distribution of input images with a wide field of practical applications. Originally, these approaches were motivated from drift-diffusion processes, but these origins find less attention in recent, practice-oriented publications.  We investigate probabilistic diffusion models from the viewpoint of scale-space research and show that they fulfil generalised scale-space properties on evolving probability distributions. Moreover, we discuss similarities and differences between interpretations of the physical core concept of drift-diffusion in the deep learning and model-based world. To this end, we examine relations of probabilistic diffusion to osmosis filters.
    
[^107]: 快速注意力需要有界条目

    Fast Attention Requires Bounded Entries. (arXiv:2302.13214v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13214](http://arxiv.org/abs/2302.13214)

    本文研究了内积关注计算的快速算法问题，提出了两个结果，证明了在B = O(sqrt(log n))时存在一种n ^（1 + O（1））时间算法。

    

    在现代机器学习中，内积关注计算是训练大型语言模型（如Transformer，GPT-1，BERT，GPT-2，GPT-3和ChatGPT）的基本任务。形式上，在这个问题中，输入三个矩阵$Q，K，V \in [-B，B]^{n \times d}$，目标是构造矩阵$\mathrm{Att}(Q,K,V) := \mathrm{diag}(A {\bf 1}_n)^{-1} A V \in \mathbb{R}^{n \times d}$，其中 $A = \exp(QK^\top/d)$ 是“注意力矩阵”，$\exp$是分量应用。本文研究是否可以通过隐式利用矩阵 $A$ 来实现更快的算法。我们提出了两个结果，证明了在 $B = \Theta(\sqrt{\log n})$处存在一个尖锐的转换。$\bullet$ 如果 $d = O(\log n)$，$B = o(\sqrt{\log n})$，则存在一个$n^{1+o(1)}$时间算法来近似$\mathbb{R}^{n \times d}$中的 $\mathrm{Att}(Q,K,V)$。

    In modern machine learning, inner product attention computation is a fundamental task for training large language models such as Transformer, GPT-1, BERT, GPT-2, GPT-3 and ChatGPT. Formally, in this problem, one is given as input three matrices $Q, K, V \in [-B,B]^{n \times d}$, and the goal is to construct the matrix $\mathrm{Att}(Q,K,V) := \mathrm{diag}(A {\bf 1}_n)^{-1} A V \in \mathbb{R}^{n \times d}$, where $A = \exp(QK^\top/d)$ is the `attention matrix', and $\exp$ is applied entry-wise. Straightforward methods for this problem explicitly compute the $n \times n$ attention matrix $A$, and hence require time $\Omega(n^2)$ even when $d = n^{o(1)}$ is small.  In this paper, we investigate whether faster algorithms are possible by implicitly making use of the matrix $A$. We present two results, showing that there is a sharp transition at $B = \Theta(\sqrt{\log n})$.  $\bullet$ If $d = O(\log n)$ and $B = o(\sqrt{\log n})$, there is an $n^{1+o(1)}$ time algorithm to approximate $\math
    
[^108]: 基于蒙特卡罗树搜索的深度生成式符号回归

    Deep Generative Symbolic Regression with Monte-Carlo-Tree-Search. (arXiv:2302.11223v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11223](http://arxiv.org/abs/2302.11223)

    本文提出了一种基于蒙特卡罗树搜索的深度生成式符号回归方法，结合了神经方法和遗传编程算法的优点，运用上下文感知的神经突变模型，并在知名数据集中展示出最先进的性能。

    

    符号回归（SR）是从数值数据中学习符号表达式的问题。最近，在模拟生成的合成数据集上训练的深度神经模型表现出与更经典的遗传编程（GP）算法相媲美的性能。与其GP对应物不同，这些神经方法是针对给定上下文的数据集生成表达式的。本文介绍了一种新方法，它结合了两种方法的优点，基于使用上下文感知的神经突变模型的蒙特卡罗树搜索过程，该模型最初预先训练以学习有前途的突变，并且通过在线方式从成功的经验中进一步完善。该方法展示了在知名的数据集上最先进的性能。

    Symbolic regression (SR) is the problem of learning a symbolic expression from numerical data. Recently, deep neural models trained on procedurally-generated synthetic datasets showed competitive performance compared to more classical Genetic Programming (GP) algorithms. Unlike their GP counterparts, these neural approaches are trained to generate expressions from datasets given as context. This allows them to produce accurate expressions in a single forward pass at test time. However, they usually do not benefit from search abilities, which result in low performance compared to GP on out-of-distribution datasets. In this paper, we propose a novel method which provides the best of both worlds, based on a Monte-Carlo Tree Search procedure using a context-aware neural mutation model, which is initially pre-trained to learn promising mutations, and further refined from successful experiences in an online fashion. The approach demonstrates state-of-the-art performance on the well-known \te
    
[^109]: AttentionMixer：一个准确且可解释的过程监测框架

    AttentionMixer: An Accurate and Interpretable Framework for Process Monitoring. (arXiv:2302.10426v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2302.10426](http://arxiv.org/abs/2302.10426)

    AttentionMixer是一个旨在为能量变换厂建立准确且可解释的辐射监测框架的数据驱动方法，其技术创新点为空间和时间自适应消息传递块和注意力和可视化技术。

    

    在极端工作条件下运行的高效能转换工厂的安全性高度依赖于准确且可解释的自动监测系统。然而，目前可用的数据驱动监测系统在高准确性或可解释性方面往往无法满足要求，从而阻碍了它们在实践中的应用。为了克服这一限制，提出了一种基于广义消息传递框架的数据驱动方法——AttentionMixer，旨在为能量变换厂建立一个准确且可解释的辐射监测框架。为了提高模型的准确性，第一项技术贡献包括开发空间和时间自适应消息传递块，分别用于捕获空间和时间相关性；这两个块通过混合算子级联。为了增强模型可解释性，第二项技术贡献涉及实现注意力和可视化技术，使得可以对模型的预测做出解释。

    An accurate and explainable automatic monitoring system is critical for the safety of high efficiency energy conversion plants that operate under extreme working condition. Nonetheless, currently available data-driven monitoring systems often fall short in meeting the requirements for either high-accuracy or interpretability, which hinders their application in practice. To overcome this limitation, a data-driven approach, AttentionMixer, is proposed under a generalized message passing framework, with the goal of establishing an accurate and interpretable radiation monitoring framework for energy conversion plants. To improve the model accuracy, the first technical contribution involves the development of spatial and temporal adaptive message passing blocks, which enable the capture of spatial and temporal correlations, respectively; the two blocks are cascaded through a mixing operator. To enhance the model interpretability, the second technical contribution involves the implementation
    
[^110]: 将黑匣子分解为可解释模型的混合物：路线规划，解释，重复。

    Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2302.10289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10289](http://arxiv.org/abs/2302.10289)

    本文提出了一种从黑盒模型中构建可解释模型的方法。该方法将黑盒模型分成可解释模型的混合物和残差网络，并使用一阶逻辑对可解释模型进行基本推理。此方法在多个数据集上表现优异且产生高度可解释的模型。

    

    机器学习模型设计要么从解释性模型开始，要么从黑盒开始并事后解释。黑盒模型灵活但难以解释，而解释性模型本质上是可解释的。然而，解释性模型需要广泛的机器学习知识，并且往往比它们的黑盒变体不够灵活和表现不佳。本文旨在模糊黑盒的事后解释和构建可解释模型之间的界限。我们从黑盒开始，迭代地Carve出一种混合解释模型（MoIE）和一个残余网络。每个可解释模型专门处理一个样本子集，并使用一阶逻辑(FOL)对其进行解释，从黑盒中提供基本推理概念。我们通过灵活的残差路由其余的样本。我们在残转网络上重复该方法，直到所有可解释模型解释所需比例的数据。我们进行了大量实验，结果表明我们的路线规划，解释和重复方法在各种数据集上优于目前几种黑匣子模型解释方法，并产生高度可解释的模型。

    ML model design either starts with an interpretable model or a Blackbox and explains it post hoc. Blackbox models are flexible but difficult to explain, while interpretable models are inherently explainable. Yet, interpretable models require extensive ML knowledge and tend to be less flexible and underperforming than their Blackbox variants. This paper aims to blur the distinction between a post hoc explanation of a Blackbox and constructing interpretable models. Beginning with a Blackbox, we iteratively carve out a mixture of interpretable experts (MoIE) and a residual network. Each interpretable model specializes in a subset of samples and explains them using First Order Logic (FOL), providing basic reasoning on concepts from the Blackbox. We route the remaining samples through a flexible residual. We repeat the method on the residual network until all the interpretable models explain the desired proportion of data. Our extensive experiments show that our route, interpret, and repeat
    
[^111]: 近乎贝叶斯最优的伪标签选择

    Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08883](http://arxiv.org/abs/2302.08883)

    本文介绍了BPLS，一种用于PLS的贝叶斯框架，通过解析逼近选择标签实例的标准，以避免由过度自信但错误预测的实例选择而导致的确认偏差问题。

    

    自训练的半监督学习严重依赖于伪标签选择（PLS）。选择通常取决于初始模型拟合标记数据的程度。过早的过拟合可能通过选择具有过度自信但错误的预测的实例（通常称为确认偏差）而传播到最终模型。本文介绍了BPLS，这是一种用于PLS的贝叶斯框架，旨在减轻这个问题。其核心是选择标签实例的标准：伪样本的后验预测的分析近似。我们通过证明伪样本的后验预测的贝叶斯最优性获得了这种选择标准。我们进一步通过解析逼近克服计算难题。它与边际似然的关系使我们能够提出基于拉普拉斯方法和高斯积分的逼近。我们针对参数广义线性和非参数广义加性模型对BPLS进行了实证评估。

    Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
    
[^112]: SNeRL: 语义感知的神经辐射场用于强化学习

    SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning. (arXiv:2301.11520v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11520](http://arxiv.org/abs/2301.11520)

    本文提出了一种称为SNeRL的语义感知神经辐射场，它通过学习3D-aware的隐式表示来进行强化学习，并在基于像素的以及最新的3D感知表示方法中表现出更好的性能。

    

    传统的强化学习表示方法很难有效地融合人类直观的3D环境理解，因此经常表现出次优性能。本文提出了一种称为SNeRL的语义感知神经辐射场，它通过联合优化卷积编码器和语义感知神经辐射场（NeRF）来从多视角图像中学习3D感知神经隐式表示。我们在NeRF中引入了3D语义和蒸馏特征场，并与RGB辐射场并行用于强化学习中的语义和对象中心表示学习。SNeRL在无模型和有模型强化学习中不仅优于以往的基于像素的表示方法，还优于最近的3D感知表示方法。

    As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.
    
[^113]: 你所在社区发生了什么？一种弱监督方法用于发现本地新闻。

    What's happening in your neighborhood? A Weakly Supervised Approach to Detect Local News. (arXiv:2301.08146v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2301.08146](http://arxiv.org/abs/2301.08146)

    该论文介绍了一种自动化的本地新闻检测和基于内容的本地新闻推荐方法，通过弱监督框架和自动化数据处理，与传统方法相比具有更高的准确性和覆盖率。

    

    本地新闻是影响特定地理区域（如城市、县和州）用户的新闻子集。检测本地新闻是准确地推荐本地新闻的关键步骤。基于最新的自然语言处理技术，我们开发了一种集成化的流程，实现了自动化本地新闻检测和基于内容的本地新闻推荐。本文着重介绍了管道的第一步骤：（1）结合领域知识和自动数据处理的弱监督框架，（2）可扩展到多语言设置。与斯坦福CoreNLP NER模型相比，我们的流程在经过真实世界和人工标记数据的评估时具有更高的精度和召回率。

    Local news articles are a subset of news that impact users in a geographical area, such as a city, county, or state. Detecting local news (Step 1) and subsequently deciding its geographical location as well as radius of impact (Step 2) are two important steps towards accurate local news recommendation. Naive rule-based methods, such as detecting city names from the news title, tend to give erroneous results due to lack of understanding of the news content. Empowered by the latest development in natural language processing, we develop an integrated pipeline that enables automatic local news detection and content-based local news recommendations. In this paper, we focus on Step 1 of the pipeline, which highlights: (1) a weakly supervised framework incorporated with domain knowledge and auto data processing, and (2) scalability to multi-lingual settings. Compared with Stanford CoreNLP NER model, our pipeline has higher precision and recall evaluated on a real-world and human-labeled datas
    
[^114]: 深度学习增强自旋量子比特环境的噪声谱学

    Deep learning enhanced noise spectroscopy of a spin qubit environment. (arXiv:2301.05079v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2301.05079](http://arxiv.org/abs/2301.05079)

    本文研究使用深度学习模型提高噪声谱学准确性，实验表明此方法可以比标准技术更加准确，并且需要更少的序列。

    

    量子系统与环境之间的不良相互作用通常导致超态在时间上的相干衰减。精确了解环境引起的噪声的频谱内容对于保护量子比特相干性并优化其在量子器件应用中的使用至关重要。我们实验证明，使用神经网络可以极大地增加噪声谱学的准确性，通过重构碳杂质组合围绕钻石氮-空位（NV）中心所特征化的功率谱密度。神经网络训练于NV中心的自旋相干函数之上，NV中心受不同的Carr-Purcell序列控制，通常用于动力学解偶（DD）。结果表明，深度学习模型可以比标准DD噪声谱学技术更加准确，并且同时需要更少的DD序列。

    The undesired interaction of a quantum system with its environment generally leads to a coherence decay of superposition states in time. A precise knowledge of the spectral content of the noise induced by the environment is crucial to protect qubit coherence and optimize its employment in quantum device applications. We experimentally show that the use of neural networks can highly increase the accuracy of noise spectroscopy, by reconstructing the power spectral density that characterizes an ensemble of carbon impurities around a nitrogen-vacancy (NV) center in diamond. Neural networks are trained over spin coherence functions of the NV center subjected to different Carr-Purcell sequences, typically used for dynamical decoupling (DD). As a result, we determine that deep learning models can be more accurate than standard DD noise-spectroscopy techniques, by requiring at the same time a much smaller number of DD sequences.
    
[^115]: 一种用于回归神经网络中数据无关知识蒸馏的合成数据生成方法

    Synthetic data generation method for data-free knowledge distillation in regression neural networks. (arXiv:2301.04338v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.04338](http://arxiv.org/abs/2301.04338)

    本文提出了一个针对回归任务的无数据知识蒸馏方法，利用生成对抗网络生成合成数据，通过原师生网络的标签训练学生网络。研究了不同的生成方法，提出了新的生成策略，可以直接优化学生网络的性能。

    

    知识蒸馏是一种将大小不同的神经网络压缩至效果相近的技术。现有的知识蒸馏方法主要适用于分类任务，并需要使用用于训练原模型的数据。为了解决回归任务下无原始数据的知识蒸馏问题，先前的研究提出了一种数据无关知识蒸馏方法，其中使用生成对抗网络生成合成数据，并用原模型预测的标签来训练学生模型。在本研究中，我们研究了各种合成数据生成方法的行为，并提出了一种新的合成数据生成策略，该策略直接优化了学生模型的性能。

    Knowledge distillation is the technique of compressing a larger neural network, known as the teacher, into a smaller neural network, known as the student, while still trying to maintain the performance of the larger neural network as much as possible. Existing methods of knowledge distillation are mostly applicable for classification tasks. Many of them also require access to the data used to train the teacher model. To address the problem of knowledge distillation for regression tasks under the absence of original training data, previous work has proposed a data-free knowledge distillation method where synthetic data are generated using a generator model trained adversarially against the student model. These synthetic data and their labels predicted by the teacher model are then used to train the student model. In this study, we investigate the behavior of various synthetic data generation methods and propose a new synthetic data generation strategy that directly optimizes for a large
    
[^116]: TarViS：一种面向目标的视频分割的统一方法

    TarViS: A Unified Approach for Target-based Video Segmentation. (arXiv:2301.02657v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.02657](http://arxiv.org/abs/2301.02657)

    提出了一种名为TarViS的新颖、统一的网络架构，可用于任何需要在视频中分段任意定义的“目标”集的任务。可以联合训练不同任务的数据集集合，并且可以在推断期间在任务之间进行热切换。在四个不同的任务中进行了应用，均优于现有最先进方法。

    

    视频分割领域目前被划分为不同的任务，涵盖了多个基准。尽管最先进技术正在快速发展，但是当前方法主要是针对特定任务的，这些方法不能概念上推广到其他任务。受到具有多任务能力的最近方法的启发，我们提出了TarViS：一种新颖的统一网络架构，可应用于需要在视频中分段任意定义的“目标”集的任何任务。我们的方法在任务定义这些目标的方式方面具有灵活性，因为它将后者建模为抽象的“查询”，然后用于预测像素精确的目标掩码。单个TarViS模型可以联合训练跨越不同任务的数据集集合，并且可以在推断期间在任务之间进行热切换，无需任何特定于任务的重新训练。为了证明其有效性，我们将TarViS应用于四个不同的任务，即视频实例分割(VIS)、视频全景分割(VPS)、视频语义分割和视频目标分割(VOS)。实验结果表明，TarViS在所有任务中均优于现有最先进方法。

    The general domain of video segmentation is currently fragmented into different tasks spanning multiple benchmarks. Despite rapid progress in the state-of-the-art, current methods are overwhelmingly task-specific and cannot conceptually generalize to other tasks. Inspired by recent approaches with multi-task capability, we propose TarViS: a novel, unified network architecture that can be applied to any task that requires segmenting a set of arbitrarily defined 'targets' in video. Our approach is flexible with respect to how tasks define these targets, since it models the latter as abstract 'queries' which are then used to predict pixel-precise target masks. A single TarViS model can be trained jointly on a collection of datasets spanning different tasks, and can hot-swap between tasks during inference without any task-specific retraining. To demonstrate its effectiveness, we apply TarViS to four different tasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS
    
[^117]: 基于深度强化学习的混合多智能体自主出行系统

    Hybrid Multi-agent Deep Reinforcement Learning for Autonomous Mobility on Demand Systems. (arXiv:2212.07313v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07313](http://arxiv.org/abs/2212.07313)

    本文介绍了一种基于深度强化学习的混合多智能体自主出行系统，采用多智能体Soft Actor-Critic和加权二分图匹配，能够解决自主移动出行系统中的主动请求分配和拒绝决策问题，并在性能、稳定性和计算方面优于现有基准。

    

    本文考虑了面向自主移动出行系统中的利润最大化运营商的主动请求分配和拒绝决策的顺序决策问题。我们将这个问题形式化为马尔可夫决策过程，并提出了多智能体Soft Actor-Critic和加权二分图匹配的新颖组合，以获得一种预期的控制策略。因此，我们将运营商的行动空间分解开来，但仍然获得了全局协调的决策。基于真实出租车数据的实验表明，我们的方法在性能、稳定性和计算方面优于现有的基准。

    We consider the sequential decision-making problem of making proactive request assignment and rejection decisions for a profit-maximizing operator of an autonomous mobility on demand system. We formalize this problem as a Markov decision process and propose a novel combination of multi-agent Soft Actor-Critic and weighted bipartite matching to obtain an anticipative control policy. Thereby, we factorize the operator's otherwise intractable action space, but still obtain a globally coordinated decision. Experiments based on real-world taxi data show that our method outperforms state of the art benchmarks with respect to performance, stability, and computational tractability.
    
[^118]: VISEM-Tracking，一份人类精子跟踪数据集

    VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02842](http://arxiv.org/abs/2212.02842)

    本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。

    

    精子运动的手动评估需要显微镜观察，由于所观察的精子在视野中的快速移动，这是具有挑战性的。为了获得正确的结果，手动评估需要进行广泛的培训。因此，在诊所中，计算机辅助精子分析（CASA）变得越来越常用。尽管如此，需要更多数据来训练监督式机器学习方法，以提高在评估精子运动和运动学方面的精度和可靠性。在这方面，我们提供了一个名为VISEM-Tracking的数据集，其中包含20个30秒的视频记录（包括29,196帧）的湿性精子制备物，具备手动注释的包围框坐标和由该领域的专家分析的一组精子特征。除了已注释的数据，我们还提供了未标记的视频剪辑，以便通过自监督或无监督学习等方法轻松访问和分析数据。作为本文的一部分，我们提出了基线精子检测性能。

    A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
    
[^119]: 一种简单高效的去中心化非凸极小化问题随机算法

    A Simple and Efficient Stochastic Algorithm for Decentralized Nonconvex-Strongly-Concave Minimax Optimization. (arXiv:2212.02387v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.02387](http://arxiv.org/abs/2212.02387)

    本文提出了一种称为去中心化递归梯度上升法（DREAM）的简单高效算法，用于解决去中心化非凸极小化问题，并实现了寻找原函数的 $\epsilon$-稳定点的最佳理论保证。

    

    本文研究了去中心化非凸极小化问题的随机优化。我们提出了一种简单高效的算法，称为去中心化递归梯度上升法（\texttt{DREAM}），它实现了寻找原函数的$\epsilon$-稳定点的最佳已知理论保证。在在线设置下，所提出的方法需要$\mathcal{O}(\kappa^3\epsilon^{-3})$随机一阶预言机（SFO）调用以及$\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$通信轮次来找到$\epsilon$-稳定点，其中$\kappa$是条件数，$\lambda_2(W)$是八卦矩阵$W$的次大特征值。对于完全由$N$个分量函数组成的离线设置，所提出的方法需要$\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO 调用和与在线设置相同的通信复杂度。

    This paper studies the stochastic optimization for decentralized nonconvex-strongly-concave minimax problem. We propose a simple and efficient algorithm, called Decentralized Recursive-gradient descEnt Ascent Method (\texttt{DREAM}), which achieves the best-known theoretical guarantee for finding the $\epsilon$-stationary point of the primal function. For the online setting, the proposed method requires $\mathcal{O}(\kappa^3\epsilon^{-3})$ stochastic first-order oracle (SFO) calls and $\mathcal{O}\big(\kappa^2\epsilon^{-2}/\sqrt{1-\lambda_2(W)}\,\big)$ communication rounds to find an $\epsilon$-stationary point, where $\kappa$ is the condition number and $\lambda_2(W)$ is the second-largest eigenvalue of the gossip matrix~$W$. For the offline setting with totally $N$ component functions, the proposed method requires $\mathcal{O}\big(\kappa^2 \sqrt{N} \epsilon^{-2}\big)$ SFO calls and the same communication complexity as the online setting.
    
[^120]: 基于查询的多模态路径融合知识库补全

    Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph. (arXiv:2212.01923v2 [cs.DB] UPDATED)

    [http://arxiv.org/abs/2212.01923](http://arxiv.org/abs/2212.01923)

    基于查询的多模态路径融合算法可以有效地对知识库进行补全，提高了性能，并且使用了基于查询的技术来提高系统的效率。

    

    在过去的几年中，大型知识库已经被构建来存储大量的知识。然而，这些知识库高度不完整，例如Freebase中有70%的人没有出生地点。为了解决这个问题，我们提出了一个使用结构化和非结构化信息的多模态融合的、基于查询驱动的知识库补全系统。为了有效地融合来自Web的非结构化信息和知识库中的结构化信息以实现良好的性能，我们的系统基于问答和规则推理构建了多模态知识图。我们提出了一个多模态路径融合算法，在多模态知识图中基于不同的路径对候选答案进行排名，取得了比问答、规则推理和基线融合算法更好的性能。为了提高系统效率，我们使用了基于查询的技术来减少系统的运行时间，为用户查询提供快速响应。

    Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Ex
    
[^121]: 带承诺和噪声观测的$2\times 2$零和博弈研究

    $2 \times 2$ Zero-Sum Games with Commitments and Noisy Observations. (arXiv:2211.01703v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2211.01703](http://arxiv.org/abs/2211.01703)

    本文研究了带承诺和噪声观测的$2\times 2$零和博弈，发现平衡点总是存在的；领导者的动作观测结果对于追随者来说要么是有益的，要么是无关紧要的；该博弈的收益在均衡点上被上界限制为纯策略下的SE的收益，下界为混合策略下的纳什均衡的收益。

    

    本文研究了在以下假设下的$2\times 2$零和博弈：$(1)$其中一位玩家（领导者）承诺通过采样给定的概率分布（策略）来选择他的动作;$(2)$领导者宣布他的动作，这个动作通过二进制信道被对手（追随者）观察到;$(3)$追随者基于领导者的策略和领导者动作的噪声观测来选择她的策略。在这些条件下，平衡点被证明总是存在的。有趣的是，即使受到噪声的影响，观察领导者的行动对追随者来说实质上要么是有益的，要么是无关紧要的。具体而言，在这个博弈的均衡点上，收益被上界限制为纯策略下SE的收益；并且下界为纳什均衡的收益，这等价于混合策略下的SE。最后，我们提供了必要和充分的条件来观察均衡点的收益。

    In this paper, $2\times2$ zero-sum games are studied under the following assumptions: $(1)$ One of the players (the leader) commits to choose its actions by sampling a given probability measure (strategy); $(2)$ The leader announces its action, which is observed by its opponent (the follower) through a binary channel; and $(3)$ the follower chooses its strategy based on the knowledge of the leader's strategy and the noisy observation of the leader's action. Under these conditions, the equilibrium is shown to always exist. Interestingly, even subject to noise, observing the actions of the leader is shown to be either beneficial or immaterial for the follower. More specifically, the payoff at the equilibrium of this game is upper bounded by the payoff at the Stackelberg equilibrium (SE) in pure strategies; and lower bounded by the payoff at the Nash equilibrium, which is equivalent to the SE in mixed strategies.Finally, necessary and sufficient conditions for observing the payoff at equi
    
[^122]: 利用时变特征调制模拟黑盒音频效应

    Modelling black-box audio effects with time-varying feature modulation. (arXiv:2211.00497v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2211.00497](http://arxiv.org/abs/2211.00497)

    提出了利用时变特征调制来模拟黑盒音频效应，可以更好地捕获长时间尺度上的依赖关系，适用于fuzz和压缩等音频效应。

    

    深度学习方法用于黑盒建模音效已显示出很大的潜力。但是，现有大部分工作集中在具有相对较短时间尺度行为的非线性效应，例如吉他放大器和失真。虽然递归和卷积结构在理论上可以扩展到长时间尺度来捕获行为，但我们发现简单地扩展现有结构的宽度、深度或膨胀因子不能令其在模拟fuzz和动态范围压缩等音频效应时表现得十分令人满意。为了解决这个问题，我们提出在现有的时间卷积骨干中整合时变特征的线性调制的方法，使中间激活可以进行可学习的自适应。我们展示了我们的方法更准确捕获了一系列fuzz和压缩实现的长距离依赖关系，包括时间和频率领域的指标。我们提供了声音示例。

    Deep learning approaches for black-box modelling of audio effects have shown promise, however, the majority of existing work focuses on nonlinear effects with behaviour on relatively short time-scales, such as guitar amplifiers and distortion. While recurrent and convolutional architectures can theoretically be extended to capture behaviour at longer time scales, we show that simply scaling the width, depth, or dilation factor of existing architectures does not result in satisfactory performance when modelling audio effects such as fuzz and dynamic range compression. To address this, we propose the integration of time-varying feature-wise linear modulation into existing temporal convolutional backbones, an approach that enables learnable adaptation of the intermediate activations. We demonstrate that our approach more accurately captures long-range dependencies for a range of fuzz and compressor implementations across both time and frequency domain metrics. We provide sound examples, s
    
[^123]: 通过结构保护神经网络逼近几乎周期辛映射

    Approximation of nearly-periodic symplectic maps via structure-preserving neural networks. (arXiv:2210.05087v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.05087](http://arxiv.org/abs/2210.05087)

    本文提出了一种结构保护神经网络方法，可以逼近几乎周期辛映射，并引出离散时间绝热不变量和稳定性。

    

    当参数 $\varepsilon$ 趋近于 0 时，具有参数 $\varepsilon$ 的连续时间动力学系统满足所有轨迹均为周期性，且角频率不会消失。几乎周期映射是几乎周期系统的离散时间模拟，定义为参数依赖的微分同胚，当极限旋转非共振时，它们允许形式上的所有阶 $U(1)$ 对称性。对于精确辛预辩空间上的哈密顿几乎周期映射，形式上的 $U(1)$ 对称性引出了一个离散时间绝热不变量。本文构建了一种新颖的结构保护神经网络，以逼近几乎周期辛映射。我们称之为辛陀螺神经元的神经网络结构保证了所得到的模拟映射是几乎周期和辛、并引出了离散时间绝热不变量和长时间稳定性。这一结构保护的新型神经网络方法为几乎周期辛映射的数值研究提供了有力工具。

    A continuous-time dynamical system with parameter $\varepsilon$ is nearly-periodic if all its trajectories are periodic with nowhere-vanishing angular frequency as $\varepsilon$ approaches 0. Nearly-periodic maps are discrete-time analogues of nearly-periodic systems, defined as parameter-dependent diffeomorphisms that limit to rotations along a circle action, and they admit formal $U(1)$ symmetries to all orders when the limiting rotation is non-resonant. For Hamiltonian nearly-periodic maps on exact presymplectic manifolds, the formal $U(1)$ symmetry gives rise to a discrete-time adiabatic invariant. In this paper, we construct a novel structure-preserving neural network to approximate nearly-periodic symplectic maps. This neural network architecture, which we call symplectic gyroceptron, ensures that the resulting surrogate map is nearly-periodic and symplectic, and that it gives rise to a discrete-time adiabatic invariant and a long-time stability. This new structure-preserving neu
    
[^124]: Learnware: 小模型实现大作为

    Learnware: Small Models Do Big. (arXiv:2210.03647v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.03647](http://arxiv.org/abs/2210.03647)

    Learnware范式旨在帮助用户充分利用小型模型，实现超越原始目的的事情，以解决当前机器学习技术面临的数据量大、训练难度高、数据安全等问题。

    

    当前机器学习技术存在训练数据量大、训练技能高、连续学习难、遗忘风险大、数据隐私和专有信息泄露等问题，而过去的大模型范式虽然在自然语言处理和计算机视觉应用中取得了惊人的结果，但并未解决这些问题，反而成为严重的碳排放源。该文概述了Learnware范式，让用户不需要从头构建机器学习模型，希望利用小型模型可以实现超越原始目的的事情，其中关键是规范，可以使训练的模型得到充分鉴别。

    There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identi
    
[^125]: 无人监督学习用于无线电频谱活动聚类

    Self-supervised Learning for Clustering of Wireless Spectrum Activity. (arXiv:2210.02899v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2210.02899](http://arxiv.org/abs/2210.02899)

    本研究使用无人监督学习技术探索无线电频谱活动，比较了两种不同的无人监督学习模型和一种混合模型，实现了精准的频谱活动聚类。

    

    近年来，通过机器学习技术处理无线电频谱数据以解决认知无线电网络相关问题，如异常检测、调制分类、技术分类和设备指纹等领域，取得了很大进展。大多数解决方案都是基于受控制的、带标签的数据，并使用监督式学习方法进行处理。然而，在现实世界环境下测量的频谱数据高度不确定，其标记是一项费时且昂贵的过程，需要领域专业知识，因此成为在该领域使用监督式学习方法的主要缺点之一。本文研究了在现实世界未标记数据中利用无人监督学习（SSL）来探索频谱活动的使用。具体来说，我们比较了两种 SSL 模型的性能，一种基于 DeepCluster 参考体系结构，另一种适用于频谱活动识别和聚类，以及一种新型的混合 SSL 方法，结合了两种模型。我们的实验评估表明，这三种方法都可以有效地对频谱活动数据进行聚类，混合方法的性能最佳。

    In recent years, much work has been done on processing of wireless spectrum data involving machine learning techniques in domain-related problems for cognitive radio networks, such as anomaly detection, modulation classification, technology classification and device fingerprinting. Most of the solutions are based on labeled data, created in a controlled manner and processed with supervised learning approaches. However, spectrum data measured in real-world environment is highly nondeterministic, making its labeling a laborious and expensive process, requiring domain expertise, thus being one of the main drawbacks of using supervised learning approaches in this domain. In this paper, we investigate the use of self-supervised learning (SSL) for exploring spectrum activities in a real-world unlabeled data. In particular, we compare the performance of two SSL models, one based on a reference DeepCluster architecture and one adapted for spectrum activity identification and clustering, and a 
    
[^126]: 通过轨迹抽样训练神经网络集成

    Training neural network ensembles via trajectory sampling. (arXiv:2209.11116v2 [cond-mat.stat-mech] UPDATED)

    [http://arxiv.org/abs/2209.11116](http://arxiv.org/abs/2209.11116)

    本研究使用随机系统中稀有轨迹的技术定义和训练NNE，通过对轨迹进行偏置来训练NNE，相较于更传统的梯度方法具有潜在优势。

    

    在机器学习中，神经网络集成(NNEs)近年来再次引起关注。它通过从多个较小的模型中获得预测结果，而不是从单个更大的模型中获得结果。本文展示了如何使用随机系统中稀有轨迹的技术来定义和训练NNE。我们根据模型参数在简单且时间离散的扩散动力学下的轨迹来定义NNE，并通过偏置这些轨迹来训练NNE以达到小的时间积分误差，这由适当的计数场作为超参数来控制。我们在一系列简单的监督学习任务中展示了这种技术的可行性。我们讨论了我们的轨迹抽样方法与更传统的梯度方法相比的潜在优势。

    In machine learning, there is renewed interest in neural network ensembles (NNEs), whereby predictions are obtained as an aggregate from a diverse set of smaller models, rather than from a single larger model. Here, we show how to define and train a NNE using techniques from the study of rare trajectories in stochastic systems. We define an NNE in terms of the trajectory of the model parameters under a simple, and discrete in time, diffusive dynamics, and train the NNE by biasing these trajectories towards a small time-integrated loss, as controlled by appropriate counting fields which act as hyperparameters. We demonstrate the viability of this technique on a range of simple supervised learning tasks. We discuss potential advantages of our trajectory sampling approach compared with more conventional gradient based methods.
    
[^127]: 使用深度强化学习非迭代生成叶片通道最优网格

    Non-iterative generation of an optimal mesh for a blade passage using deep reinforcement learning. (arXiv:2209.05280v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05280](http://arxiv.org/abs/2209.05280)

    本文介绍了一种使用深度强化学习来非迭代生成任意叶片通道最优网格的方法，实现了自动化和减少人为干预，大大提高了计算效率。

    

    本文提出了一种使用深度强化学习(DRL)来非迭代生成任意叶片通道最优网格的方法。尽管使用经验法或优化算法自动化网格生成，但对于新几何结构仍需要反复调整网格参数。本方法采用基于DRL的多条件优化技术，以叶片几何形状为函数定义最优网格参数，实现自动化、减少人为干预和计算效率最小化。通过训练一个椭圆网格生成器对网格参数进行优化，该生成器为任意叶片几何形状生成结构化网格。在DRL过程的每个回合中，通过更新网格参数，训练生成器生成随机选择的叶片通道的最优网格，同时评估网格质量。

    A method using deep reinforcement learning (DRL) to non-iteratively generate an optimal mesh for an arbitrary blade passage is developed. Despite automation in mesh generation using either an empirical approach or an optimization algorithm, repeated tuning of meshing parameters is still required for a new geometry. The method developed herein employs a DRL-based multi-condition optimization technique to define optimal meshing parameters as a function of the blade geometry, attaining automation, minimization of human intervention, and computational efficiency. The meshing parameters are optimized by training an elliptic mesh generator which generates a structured mesh for a blade passage with an arbitrary blade geometry. During each episode of the DRL process, the mesh generator is trained to produce an optimal mesh for a randomly selected blade passage by updating the meshing parameters until the mesh quality, as measured by the ratio of determinants of the Jacobian matrices and the sk
    
[^128]: FreeREA: 无需训练的进化式架构搜索算法

    FreeREA: Training-Free Evolution-based Architecture Search. (arXiv:2207.05135v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2207.05135](http://arxiv.org/abs/2207.05135)

    本研究提出了一种无需训练步骤的进化式架构搜索算法FreeREA，可直接在目标硬件上优化搜索，且能够在性能最大化的同时，极大地减小内存占用。实验结果证明其比手动设计更高效。

    

    在过去的十年中，机器学习领域的大部分研究都是为了改进现有模型，以增加神经网络在各种不同任务的解决方案中的性能。然而，这样的进步往往以增加模型内存和计算需求的代价为代价。这对于研究成果在实际环境中的部署具有相当大的限制性，其中成本、能源消耗和框架的复杂性发挥着至关重要的作用。本文提出了一种新颖的进化式架构搜索算法FreeREA，通过该算法可以快速识别性能最大化且内存占用最小化的神经网络，而无需训练步骤，并在目标硬件上直接优化架构搜索，无需代理指标。实验结果表明了FreeREA的有效性，它能够在需要少达487倍计算资源的情况下，胜过最先进的手动设计架构。

    In the last decade, most research in Machine Learning contributed to the improvement of existing models, with the aim of increasing the performance of neural networks for the solution of a variety of different tasks. However, such advancements often come at the cost of an increase of model memory and computational requirements. This represents a significant limitation for the deployability of research output in realistic settings, where the cost, the energy consumption, and the complexity of the framework play a crucial role. To solve this issue, the designer should search for models that maximise the performance while limiting its footprint. Typical approaches to reach this goal rely either on manual procedures, which cannot guarantee the optimality of the final design, or upon Neural Architecture Search algorithms to automatise the process, at the expenses of extremely high computational time. This paper provides a solution for the fast identification of a neural network that maximis
    
[^129]: 基于最小描述长度和结构稳定性的脉冲神经网络的泛化研究

    On the Generalization of Spiking Neural Networks via Minimum Description Length and Structural Stability. (arXiv:2207.04876v2 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2207.04876](http://arxiv.org/abs/2207.04876)

    本研究通过利用最小描述长度原则和结构稳定性为脉冲神经网络提供了一个明确的泛化界限，并指定了最大稳定分歧解数的下限和上限。

    

    过去几十年中，由于其对于建模时间相关数据的潜力，脉冲神经网络引起了越来越多的关注。许多经验算法和技术已经被开发出来。然而，从理论上讲，训练后的脉冲神经网络在未见数据上的表现仍然是未知的。本研究通过利用最小描述长度原则，为脉冲神经网络提供一个明确的泛化界限。此外，我们通过结构稳定性实施了SNN的描述长度，并指定了最大稳定分歧解数的下限和上限，将在SNN中确定结构稳定性的挑战转化为一个具有定量特性的数学问题。

    The past decades have witnessed an increasing interest in spiking neural networks due to their great potential of modeling time-dependent data. Many empirical algorithms and techniques have been developed. However, theoretically, it remains unknown whether and to what extent a trained spiking neural network performs well on unseen data. This work takes one step in this direction by exploiting the minimum description length principle and thus, presents an explicit generalization bound for spiking neural networks. Further, we implement the description length of SNNs through structural stability and specify the lower and upper bounds of the maximum number of stable bifurcation solutions, which convert the challenge of qualifying structural stability in SNNs into a mathematical problem with quantitative properties.
    
[^130]: 将区域化算法扩展到探索空间进程异质性

    Extending regionalization algorithms to explore spatial process heterogeneity. (arXiv:2206.09429v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2206.09429](http://arxiv.org/abs/2206.09429)

    提出两种新的空间区域划分算法，应用于合成和真实数据集，并取得了较好的成果。其中，两阶段K模型算法表现最佳，具有较好的模型拟合、区域重构和系数估计能力。

    

    在空间回归模型中，空间异质性可以通过连续或离散规范进行考虑。后者涉及到确定具有同质变量之间的均质关系的连续空间区域（空间区域）。尽管在空间分析领域中提出并研究了各种区域化算法，但优化空间区域的方法基本未被探索。本文提出了两种新的空间区域划分算法，即两阶段K模型和区域K模型。我们还将经典的自动分区程序扩展到空间回归背景下。这些算法被应用于一系列合成数据集和两个真实数据集。结果表明，所有三种算法在模型拟合、区域重构和系数估计方面均达到了超越或可比较的性能，而两阶段K模型算法在模型拟合、区域重构和系数估计方面明显优于现有方法。

    In spatial regression models, spatial heterogeneity may be considered with either continuous or discrete specifications. The latter is related to delineation of spatially connected regions with homogeneous relationships between variables (spatial regimes). Although various regionalization algorithms have been proposed and studied in the field of spatial analytics, methods to optimize spatial regimes have been largely unexplored. In this paper, we propose two new algorithms for spatial regime delineation, two-stage K-Models and Regional-K-Models. We also extend the classic Automatic Zoning Procedure to spatial regression context. The proposed algorithms are applied to a series of synthetic datasets and two real-world datasets. Results indicate that all three algorithms achieve superior or comparable performance to existing approaches, while the two-stage K-Models algorithm largely outperforms existing approaches on model fitting, region reconstruction, and coefficient estimation. Our wo
    
[^131]: 基于Transformer的多模态学习综述

    Multimodal Learning with Transformers: A Survey. (arXiv:2206.06488v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.06488](http://arxiv.org/abs/2206.06488)

    本文综述了基于Transformer的多模态学习技术，包括Vanilla Transformer、Vision Transformer和多模态Transformer，在多模态预训练和特定任务中的应用，以及共享的常见挑战和设计。并探讨了开放的问题和潜在研究方向。

    

    Transformer是一种有前途的神经网络学习器，在各种机器学习任务中取得了巨大成功。由于多模态应用和大数据的普及，基于Transformer的多模态学习成为人工智能研究的热门话题。本文介绍了一项针对多模态数据的Transformer技术全面综述。该综述的主要内容包括：（1）多模态学习与Transformer生态系统及多模态大数据时代的背景，（2）从几何拓扑学的角度对Vanilla Transformer、Vision Transformer和多模态Transformer进行理论综述，（3）通过两种重要范例，即多模态预训练和特定多模态任务，综述多模态Transformer应用，（4）总结多模态Transformer模型和应用所共享的常见挑战和设计，（5）讨论社区的开放性问题和潜在研究方向。

    Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community
    
[^132]: 量子策略迭代：基于振幅估计和Grover搜索——走向量子强化学习的优势

    Quantum Policy Iteration via Amplitude Estimation and Grover Search -- Towards Quantum Advantage for Reinforcement Learning. (arXiv:2206.04741v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2206.04741](http://arxiv.org/abs/2206.04741)

    该论文提出了一种量子强化学习方法，并证明在代理和环境的无误差高效量子实现的基础上，该方法可以在样本复杂度方面比基于经典蒙特卡罗的方法有更好的性能提升。

    

    我们提出了一种全新的量子强化学习方法，并进行了详细的实现和模拟。我们的工作证明量子算法可以用于解决强化学习问题，并表明在代理和环境的无误差高效量子实现的基础上，量子方法在样本复杂度方面可以证明比基于经典蒙特卡罗的方法有更好的性能提升。我们的方法详细地展示了如何将振幅估计和Grover搜索组合成策略评估和改进方案。首先，我们开发了量子策略评估（QPE），其比一个类似的经典蒙特卡罗估计高效平方，基于量子力学实现有限马尔可夫决策过程（MDP）。在QPE的基础上，我们推导出一种量子策略迭代，它使用Grover搜索重复改进初始策略，直到达到最佳策略。最后，我们提供了一种实现方案。

    We present a full implementation and simulation of a novel quantum reinforcement learning method. Our work is a detailed and formal proof of concept for how quantum algorithms can be used to solve reinforcement learning problems and shows that, given access to error-free, efficient quantum realizations of the agent and environment, quantum methods can yield provable improvements over classical Monte-Carlo based methods in terms of sample complexity. Our approach shows in detail how to combine amplitude estimation and Grover search into a policy evaluation and improvement scheme. We first develop quantum policy evaluation (QPE) which is quadratically more efficient compared to an analogous classical Monte Carlo estimation and is based on a quantum mechanical realization of a finite Markov decision process (MDP). Building on QPE, we derive a quantum policy iteration that repeatedly improves an initial policy using Grover search until the optimum is reached. Finally, we present an impleme
    
[^133]: 实例相关的局部标签学习的渐进式纯化方法

    Progressive Purification for Instance-Dependent Partial Label Learning. (arXiv:2206.00830v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.00830](http://arxiv.org/abs/2206.00830)

    本研究提出了一种叫做POP的方法来解决实例相关的局部标签学习问题，POP在每个时代更新学习模型并逐步净化每个候选标签集，能以特定的快速速度扩大模型可靠性的范围。

    

    局部标签学习（PLL）旨在从每个注释有候选标签集的示例中训练多类分类器，其中一个固定但未知的候选标签是正确的。近几年来，候选标签的无实例独立生成过程已被广泛研究，基于此在PLL方面取得了许多理论进展。然而，实际上候选标签总是与实例相关的，没有理论保证在实例相关的PLL示例上训练的模型能够收敛到理想值。因此，本文提出了一种名为POP的理论上有根据和实际有效的方法，即逐步纯化实例相关的局部标签学习。具体而言，POP在每个时代更新学习模型并逐步净化每个候选标签集。理论上，我们证明了POP在合适的快速速度下扩大模型可靠性的范围，并最终逼近

    Partial label learning (PLL) aims to train multiclass classifiers from the examples each annotated with a set of candidate labels where a fixed but unknown candidate label is correct. In the last few years, the instance-independent generation process of candidate labels has been extensively studied, on the basis of which many theoretical advances have been made in PLL. Nevertheless, the candidate labels are always instance-dependent in practice and there is no theoretical guarantee that the model trained on the instance-dependent PLL examples can converge to an ideal one. In this paper, a theoretically grounded and practically effective approach named POP, i.e. PrOgressive Purification for instance-dependent partial label learning, is proposed. Specifically, POP updates the learning model and purifies each candidate label set progressively in every epoch. Theoretically, we prove that POP enlarges the region appropriately fast where the model is reliable, and eventually approximates the
    
[^134]: 通用化降维：低成本使任何分层聚类公平平衡

    Generalized Reductions: Making any Hierarchical Clustering Fair and Balanced with Low Cost. (arXiv:2205.14198v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14198](http://arxiv.org/abs/2205.14198)

    本文描述了如何使用低成本使任何分层聚类公平平衡，扩展了先前的工作，并提出了成本公平权衡的概念。

    

    聚类是现代统计分析流程的基本构建模块。近年来，公平聚类受到机器学习界的广泛关注。我们是第一批在Ahmadian等人在2020年的NeurIPS结果之后研究分层聚类公平性的人。我们使用Dasgupta的成本函数进行评估，这可能是分层聚类评估中最常见的理论指标之一。我们的工作将先前的$ O(n^{5/6}poly \ log（n))$成本公平近似大大改进为接近多对数$ O(n^\delta poly \ log（n))$的成本公平近似，其中任何常数$\delta \ in（0,1）$。这个结果建立了成本公平权衡，并扩展到比先前的工作更广泛的公平约束。我们还展示了如何改变现有的分层聚类以保证在层次结构的任何级别上的公平性和簇平衡。

    Clustering is a fundamental building block of modern statistical analysis pipelines. Fair clustering has seen much attention from the machine learning community in recent years. We are some of the first to study fairness in the context of hierarchical clustering, after the results of Ahmadian et al. from NeurIPS in 2020. We evaluate our results using Dasgupta's cost function, perhaps one of the most prevalent theoretical metrics for hierarchical clustering evaluation. Our work vastly improves the previous $O(n^{5/6}poly\log(n))$ fair approximation for cost to a near polylogarithmic $O(n^\delta poly\log(n))$ fair approximation for any constant $\delta\in(0,1)$. This result establishes a cost-fairness tradeoff and extends to broader fairness constraints than the previous work. We also show how to alter existing hierarchical clusterings to guarantee fairness and cluster balance across any level in the hierarchy.
    
[^135]: 从现代CNN到视觉Transformer：评估深度学习模型在组织病理学中的性能、鲁棒性和分类策略

    From Modern CNNs to Vision Transformers: Assessing the Performance, Robustness, and Classification Strategies of Deep Learning Models in Histopathology. (arXiv:2204.05044v2 [eess.IV] CROSS LISTED)

    [http://arxiv.org/abs/2204.05044](http://arxiv.org/abs/2204.05044)

    本文评估了现代CNN和视觉Transformer模型在组织病理学中的性能、鲁棒性和分类策略。在乳腺癌、胃癌和结直肠癌全切片图像等五个数据集上进行了广泛测试，结果表明ViT模型在分类准确性方面优于其他CNN，而Swin Transformer模型在对抗染色变化的鲁棒性方面表现最佳。

    

    机器学习正在改变组织病理学领域，但该领域缺乏全面评估最新模型的方法，不仅要考虑简单的分类准确性，还要考虑其他质量要求。为此，我们开发了一种新的方法，对一系列分类模型进行了广泛评估，包括最新的视觉Transformer和卷积神经网络，如ConvNeXt、ResNet（BiT）、Inception、ViT和Swin Transformer，并在有监督或无监督预训练的情况下进行了测试。我们对包含乳腺癌、胃癌和结直肠癌全切片图像的五个广泛使用的组织病理学数据集进行了全面测试，并开发了一种新方法，使用图像转换模型来评估癌症分类模型对染色变化的鲁棒性。此外，我们扩展了现有的可解释性方法，系统地揭示了它们学到的特征。我们的评估表明，ViT模型在分类准确性方面优于其他CNN，而Swin Transformer模型在对抗染色变化的鲁棒性方面表现最佳。此外，我们证明了预训练可以提高大多数模型的分类性能和鲁棒性。最后，我们的分析揭示了这些模型学到的特征，包括空间频率信息和肿瘤特征。

    While machine learning is currently transforming the field of histopathology, the domain lacks a comprehensive evaluation of state-of-the-art models based on essential but complementary quality requirements beyond a mere classification accuracy. In order to fill this gap, we developed a new methodology to extensively evaluate a wide range of classification models, including recent vision transformers, and convolutional neural networks such as: ConvNeXt, ResNet (BiT), Inception, ViT and Swin transformer, with and without supervised or self-supervised pretraining. We thoroughly tested the models on five widely used histopathology datasets containing whole slide images of breast, gastric, and colorectal cancer and developed a novel approach using an image-to-image translation model to assess the robustness of a cancer classification model against stain variations. Further, we extended existing interpretability methods to previously unstudied models and systematically reveal insights of th
    
[^136]: 基于近似隐式微分的双层优化中的鞍点逃逸方法

    Efficiently Escaping Saddle Points in Bilevel Optimization. (arXiv:2202.03684v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.03684](http://arxiv.org/abs/2202.03684)

    本文研究了双层优化中的鞍点逃逸方法，提出了两种算法，一种是用于确定性问题的扰动近似隐式微分算法，一种是用于随机问题的不精确非曲率源自噪声算法，并且提供了扰动多步梯度下降算法的非渐近分析。

    

    双层优化是机器学习和优化中的基本问题之一。最近的理论发展集中于寻找非凸强凸情况下的一阶稳定点。本文分析了一些算法，这些算法可以在非凸强凸双层优化中逃离鞍点。具体而言，我们展示了一个带有热启动策略的扰动近似隐式微分（AID）可以在$\tilde{O}(\epsilon^{-2})$迭代中高概率地找到$\epsilon$-近似局部最小值的双层优化。此外，我们提出了一种不精确非曲率源自噪声算法（iNEON），这是一种纯一阶算法，可以逃离鞍点并找到随机双层优化的局部最小值。作为一个副产品，我们提供了扰动多步梯度下降（GDmax）算法的第一个非渐近分析，该算法收敛到最小化最大问题的局部极小值点。

    Bilevel optimization is one of the fundamental problems in machine learning and optimization. Recent theoretical developments in bilevel optimization focus on finding the first-order stationary points for nonconvex-strongly-convex cases. In this paper, we analyze algorithms that can escape saddle points in nonconvex-strongly-convex bilevel optimization. Specifically, we show that the perturbed approximate implicit differentiation (AID) with a warm start strategy finds $\epsilon$-approximate local minimum of bilevel optimization in $\tilde{O}(\epsilon^{-2})$ iterations with high probability. Moreover, we propose an inexact NEgative-curvature-Originated-from-Noise Algorithm (iNEON), a pure first-order algorithm that can escape saddle point and find local minimum of stochastic bilevel optimization. As a by-product, we provide the first nonasymptotic analysis of perturbed multi-step gradient descent ascent (GDmax) algorithm that converges to local minimax point for minimax problems.
    
[^137]: 隐私保护的逻辑回归训练及其更快的梯度变种

    Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant. (arXiv:2201.10838v3 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2201.10838](http://arxiv.org/abs/2201.10838)

    本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现隐私保护的逻辑回归训练，并成功提升了收敛速度，实现了同态逻辑回归训练仅需较少的迭代次数。

    

    多年来，加密数据上的逻辑回归训练一直是一个有吸引力的安全解决方案。本文提出了一种更快的梯度变种——二次梯度，用于在同态加密领域实现逻辑回归训练，其核心可以看作是简化的固定Hessian的扩展。我们分别向Nesterov的加速梯度方法（NAG）和自适应梯度算法（Adagrad）增强了该梯度变种，并在多个数据集上评估了增强算法。实验结果表明，增强方法在收敛速度上比朴素的一阶梯度方法具有最先进的性能。然后，我们采用增强的NAG方法来实现同态逻辑回归训练，并在仅3次迭代中获得了可比较的结果。

    Logistic regression training over encrypted data has been an attractive idea to security concerns for years. In this paper, we propose a faster gradient variant called $\texttt{quadratic gradient}$ to implement logistic regression training in a homomorphic encryption domain, the core of which can be seen as an extension of the simplified fixed Hessian.  We enhance Nesterov's accelerated gradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with this gradient variant and evaluate the enhanced algorithms on several datasets. Experimental results show that the enhanced methods have a state-of-the-art performance in convergence speed compared to the naive first-order gradient methods. We then adopt the enhanced NAG method to implement homomorphic logistic regression training and obtain a comparable result by only $3$ iterations.
    
[^138]: GAP-Gen: 引导自动生成 Python 代码的方法

    GAP-Gen: Guided Automatic Python Code Generation. (arXiv:2201.08810v2 [cs.PL] UPDATED)

    [http://arxiv.org/abs/2201.08810](http://arxiv.org/abs/2201.08810)

    本文介绍了一种基于 Python 语法约束和语义约束的引导自动生成 Python 代码的方法 GAP-Gen，通过微调 T5 和 CodeT5 这两种语言模型，在自动生成 Python 代码任务上保持了高生成性能。

    

    自然语言描述自动生成代码在软件开发过程中非常有助于提高效率。本文提出了一种基于 Python 语法约束和语义约束的引导自动生成 Python 代码的方法 GAP-Gen。我们首先介绍了 Syntax-Flow，这是一种简化的抽象语法树（AST）形式的 Python 语法约束，并且在代码中保留了关键的语法信息，从而减小了抽象语法树的规模和复杂性。除了 Syntax-Flow，我们还介绍了 Variable-Flow，它能够在整个代码中一致地抽象变量和函数名。我们的工作重点不是在预训练上，而是在修改微调过程，从而减少计算需求，但在自动生成 Python 代码任务上保持高生成性能。 GAP-Gen 使用 T5 和 CodeT5 这两种基于 transformer 的语言模型，通过 CodeSearchNet 数据集进行微调。

    Automatic code generation from natural language descriptions can be highly beneficial during the process of software development. In this work, we propose GAP-Gen, a Guided Automatic Python Code Generation method based on Python syntactic constraints and semantic constraints. We first introduce Python syntactic constraints in the form of Syntax-Flow, which is a simplified version of Abstract Syntax Tree (AST) reducing the size and high complexity of Abstract Syntax Tree but maintaining crucial syntactic information of Python code. In addition to Syntax-Flow, we introduce Variable-Flow which abstracts variable and function names consistently through out the code. In our work, rather than pretraining, we focus on modifying the finetuning process which reduces computational requirements but retains high generation performance on automatic Python code generation task. GAP-Gen fine-tunes the transformer based language models T5 and CodeT5 using the Code-to-Docstring datasets CodeSearchNet, 
    
[^139]: 通过过度参数来引导稀疏性: More is Less

    More is Less: Inducing Sparsity via Overparameterization. (arXiv:2112.11027v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2112.11027](http://arxiv.org/abs/2112.11027)

    通过过度参数化的神经网络模型，在稀疏恢复（压缩感知）中，采用过度参数的平方损失函数，推出了一个迭代算法解决问题，展示了神经网络对稀疏问题的良好适应性和推广能力，以及竞争性的解决方案。

    

    在深度学习中，常常会过度参数化神经网络，即使用比训练样本更多的参数。令人惊讶的是，通过（随机）梯度下降训练神经网络会导致良好的泛化模型，而传统的统计学则会认为会过拟合。为了理解这种隐式偏差现象，我们研究了稀疏恢复（压缩感知）的特殊情况，它本身就很有趣。更具体地，为了从欠定线性测量中重构向量，我们引入了相应的过度参数化平方损失函数，其中要重构的向量被深度分解成多个向量。我们表明，如果存在一个精确的解，则过度参数化的丢失函数的香草梯度流会收敛到最小$\ell_1$范数的精确解的良好逼近。后者被广泛认为是能促进稀疏解的。作为副产品，我们的结果显著提高了稀疏模式之间的转移学习，并提出了使用神经网络进行稀疏恢复的新的竞争性方法。

    In deep learning it is common to overparameterize neural networks, that is, to use more parameters than training samples. Quite surprisingly training the neural network via (stochastic) gradient descent leads to models that generalize very well, while classical statistics would suggest overfitting. In order to gain understanding of this implicit bias phenomenon we study the special case of sparse recovery (compressed sensing) which is of interest on its own. More precisely, in order to reconstruct a vector from underdetermined linear measurements, we introduce a corresponding overparameterized square loss functional, where the vector to be reconstructed is deeply factorized into several vectors. We show that, if there exists an exact solution, vanilla gradient flow for the overparameterized loss functional converges to a good approximation of the solution of minimal $\ell_1$-norm. The latter is well-known to promote sparse solutions. As a by-product, our results significantly improve t
    
[^140]: 现代非线性函数回归模型：使用神经网络分析功能数据

    Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME] CROSS LISTED)

    [http://arxiv.org/abs/2107.14151](http://arxiv.org/abs/2107.14151)

    本研究提出一种利用神经网络分析功能数据的新型非线性函数回归模型，通过连续隐藏层实现对功能响应建模，并提供了两种模型拟合策略（FDNN和FBNN），并通过正则化技术得到更加简明的结果。

    

    本论文引入了一种新的非线性函数回归模型类，使用神经网络分析功能数据。我们提出了一个框架，使用由连续神经元组成的隐藏层，称为连续隐藏层，用于功能响应建模，并提供了两种模型拟合策略：功能直接神经网络（FDNN）和功能基础神经网络（FBNN）。这两种方法都是专门设计来利用功能数据固有的结构，并捕捉功能预测变量和功能响应变量之间存在的复杂关系。我们通过求解函数梯度并实施正则化技术进行模型拟合，得到更简明的结果。我们通过广泛的模拟研究和实际数据示例展示了我们提出的方法在处理复杂功能模型方面的强大灵活性。

    We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, Functional Direct Neural Network (FDNN) and Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.
    
[^141]: 基于神经网络的非线性函数建模

    Non-linear Functional Modeling using Neural Networks. (arXiv:2104.09371v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.09371](http://arxiv.org/abs/2104.09371)

    本文提出了一种基于神经网络的、适用于函数数据的新型非线性模型。我们提出了两种变体，旨在显式利用函数数据中固有的结构，并通过全面的模拟研究和实际数据示例证明了该方法的有效性。

    

    本文介绍了一种基于神经网络的新型非线性函数数据模型。深度学习在非线性建模方面非常成功，但在函数数据设置方面却很少有研究。我们提出了两种变体：一种是具有连续隐藏层的函数神经网络，称为函数直接神经网络（FDNN），另一种则利用基扩展和连续隐藏层，称为基函数神经网络（FBNN）。两种变体都是设计用来显式利用函数数据中固有的结构。为了拟合这些模型，我们导出了一种基于函数梯度的优化算法。我们通过全面的模拟研究和实际数据示例展示了所提出方法在处理复杂函数模型方面的有效性。

    We introduce a new class of non-linear models for functional data based on neural networks. Deep learning has been very successful in non-linear modeling, but there has been little work done in the functional data setting. We propose two variations of our framework: a functional neural network with continuous hidden layers, called the Functional Direct Neural Network (FDNN), and a second version that utilizes basis expansions and continuous hidden layers, called the Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data. To fit these models we derive a functional gradient based optimization algorithm. The effectiveness of the proposed methods in handling complex functional models is demonstrated by comprehensive simulation studies and real data examples.
    
[^142]: 自动将漏洞公告映射到开源代码库中的修复提交

    Automated Mapping of Vulnerability Advisories onto their Fix Commits in Open Source Repositories. (arXiv:2103.13375v2 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2103.13375](http://arxiv.org/abs/2103.13375)

    本文提出了一种结合启发式和机器学习（特别是自然语言处理）的方法，将漏洞公告与其修复提交映射到开源代码库中，以解决缺乏全面准确的漏洞数据来源的问题。

    

    缺乏全面准确的漏洞数据来源是研究和理解软件漏洞（及其修正）的重大障碍。本文提出了一种方法，该方法结合了从实际经验中产生的启发式和机器学习（ML），特别是自然语言处理（NLP），以解决此问题。方法包含三个阶段。首先，从公告中提取包含漏洞关键信息的记录（用自然语言表达）。其次，使用启发式方法，通过过滤已知与任务不相关的提交来从受影响项目的源代码库中获取一组候选修复提交。最后，对于每个这样的候选提交，该方法构建一个数值特征向量，反映提交的特征与手头的漏洞匹配相关。然后利用特征向量进行预测。

    The lack of comprehensive sources of accurate vulnerability data represents a critical obstacle to studying and understanding software vulnerabilities (and their corrections). In this paper, we present an approach that combines heuristics stemming from practical experience and machine-learning (ML) specifically, natural language processing (NLP) - to address this problem. Our method consists of three phases. First, an advisory record containing key information about a vulnerability is extracted from an advisory (expressed in natural language). Second, using heuristics, a subset of candidate fix commits is obtained from the source code repository of the affected project by filtering out commits that are known to be irrelevant for the task at hand. Finally, for each such candidate commit, our method builds a numerical feature vector reflecting the characteristics of the commit that are relevant to predicting its match with the advisory at hand. The feature vectors are then exploited fo
    
[^143]: 提高图像Wasserstein攻击和防御的效果

    Improved Image Wasserstein Attacks and Defenses. (arXiv:2004.12478v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2004.12478](http://arxiv.org/abs/2004.12478)

    本文提出了一种基于Wasserstein距离限制的更好的威胁模型，可以更准确地限制图像扰动。作者指出并纠正了先前Wasserstein威胁模型定义中的缺陷，并展示了更强的攻击和防御。然而，当前的Wasserstein-鲁棒模型在防御现实世界中出现的扰动方面可能受到限制。

    

    近期文献中已经广泛研究了对于$\ell_p$限制下受到图像扰动的鲁棒性。但是，现实中的扰动很少表现出$\ell_p$威胁模型所假定的像素独立性。最近提出了一个基于Wasserstein距离限制的威胁模型，它限制扰动为像素质量移动。我们指出并纠正了先前Wasserstein威胁模型定义中的缺陷，并在我们更好地定义的框架下探索了更强的攻击和防御。最后，我们讨论了当前Wasserstein-鲁棒模型在防御现实世界中出现的扰动方面的无能为力。我们的代码和训练模型可在https://github.com/edwardjhu/improved_wasserstein找到。

    Robustness against image perturbations bounded by a $\ell_p$ ball have been well-studied in recent literature. Perturbations in the real-world, however, rarely exhibit the pixel independence that $\ell_p$ threat models assume. A recently proposed Wasserstein distance-bounded threat model is a promising alternative that limits the perturbation to pixel mass movements. We point out and rectify flaws in previous definition of the Wasserstein threat model and explore stronger attacks and defenses under our better-defined framework. Lastly, we discuss the inability of current Wasserstein-robust models in defending against perturbations seen in the real world. Our code and trained models are available at https://github.com/edwardjhu/improved_wasserstein .
    

