# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [An Interpretable Systematic Review of Machine Learning Models for Predictive Maintenance of Aircraft Engine.](http://arxiv.org/abs/2309.13310) | 本文对机器学习模型预测飞机发动机维修情况的可解释性进行了综述，并通过使用不同的方法和分析，取得了较高的预测准确率。 |
| [^2] | [CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity.](http://arxiv.org/abs/2309.13307) | 提出了一种名为CORE的新技术，可以在分布式机器学习中通过投影和重构信息来减少通信复杂度，从而实现更高效的训练和扩展性。 |
| [^3] | [C$^2$VAE: Gaussian Copula-based VAE Differing Disentangled from Coupled Representations with Contrastive Posterior.](http://arxiv.org/abs/2309.13303) | 这篇论文提出了一种C$^2$VAE模型，通过联合学习非耦合且相关的隐藏因素，并通过自监督分类器消除耦合表示，以增强非耦合表示学习。该模型在不依赖先验知识和强建模假设的情况下，使用总相关驱动分解后验来学习因子化的非耦合表示，并利用神经高斯Copula模型提取隐藏特征之间的依赖关系来获得耦合表示。 |
| [^4] | [Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework.](http://arxiv.org/abs/2309.13278) | 本研究提出了一个对于强化学习非同策略评估问题的统一误差量化框架，并解决了分布偏移的挑战。通过在一个单一的区间内共同量化两个估计误差源，该框架揭示了之前隐藏的误差权衡，从而提高了置信区间的准确性。 |
| [^5] | [Order-preserving Consistency Regularization for Domain Adaptation and Generalization.](http://arxiv.org/abs/2309.13258) | 提出了一种适用于跨领域任务的有序一致性正则化（OCR），通过保持预测的有序性，使模型对于特定领域的属性具有鲁棒性，并在多个跨领域任务上取得了明显的优势。 |
| [^6] | [Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks.](http://arxiv.org/abs/2309.13256) | 本研究针对预训练语言模型作为少样本学习者的安全风险进行了初步研究，发现其极易受到反向攻击。为了应对这个问题，我们提出了一种名为MDP的轻量级、可插拔且有效的防御方法，通过利用被污染样本和清洁样本之间的掩码敏感性差距来识别污染样本。实证评估结果表明，MDP在攻击效果和检测逃避性之间形成了进退两难。 |
| [^7] | [Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training.](http://arxiv.org/abs/2309.13254) | 这篇论文介绍了Zen，一种用于分布式DNN训练中近似最优稀疏张量同步的方法。通过分析流行的DNN模型中稀疏张量的特性，并系统地探索设计空间，找到了最佳的通信方案。通过减少通信流量和提高训练效率，Zen有效地提升了分布式训练的性能。 |
| [^8] | [Can I Trust the Explanations? Investigating Explainable Machine Learning Methods for Monotonic Models.](http://arxiv.org/abs/2309.13246) | 本研究研究了可解释机器学习方法在单调模型中的应用，发现了解释的可靠性和模型的单调性之间的关系。 |
| [^9] | [Importance of negative sampling in weak label learning.](http://arxiv.org/abs/2309.13227) | 弱标签学习中选择负实例的策略尚未得到广泛研究。本文研究了几种衡量负实例有用性的采样策略，并在实验证明这些策略可以提高弱标签分类性能和降低计算成本。 |
| [^10] | [Pick Planning Strategies for Large-Scale Package Manipulation.](http://arxiv.org/abs/2309.13224) | 本文介绍了亚马逊机器人公司的Robot Induction（Robin）舰队中的大规模包裹操纵，通过使用拣选成功预测器以及训练的拣选质量估计方法，在真实生产系统中进行自动化仓储操作。 |
| [^11] | [Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks.](http://arxiv.org/abs/2309.13223) | 本文提出了一种基于因果推理的框架，用于构建AI本地化的无线网络，以应对现有“AI for wireless”范式的短板。该框架通过解决AI模型的黑匣子特性、曲线拟合特性、对大量训练数据的依赖以及大型神经网络的能量效率低下等问题，克服了数据驱动型、训练密集型AI的局限性。 |
| [^12] | [Assessing the Impact of Personality on Affective States from Video Game Communication.](http://arxiv.org/abs/2309.13214) | 本研究探讨了个性对团队合作的虚拟现实游戏玩家的情感表达方式的影响。通过分析两周内11名玩家的聊天记录，我们发现了个性变量与情感表达之间的合理相关性，例如较低的自我能力与增加的困惑之间的关系，以及个人烦恼与内在和外在形象问题的增多之间的关系。 |
| [^13] | [The LHCb ultra-fast simulation option, Lamarr: design and validation.](http://arxiv.org/abs/2309.13213) | LHCb超快速模拟选项Lamarr是一个基于Gaudi的框架，旨在为LHCb探测器的模拟提供最快速的解决方案，通过参数化探测器响应和重建算法来满足未来大规模模拟数据样本的需求。 |
| [^14] | [Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications.](http://arxiv.org/abs/2309.13207) | Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty. |
| [^15] | [A Practical Survey on Zero-shot Prompt Design for In-context Learning.](http://arxiv.org/abs/2309.13205) | 本文综述了针对上下文学习的零样本提示设计技术，并探讨了不同类型提示对大型语言模型性能的影响。研究重点讨论了人工设计、优化算法和评价方法等多种提示设计方法，以优化模型在不同任务上的性能。同时，本文强调了考虑多种指标和缺乏单一最佳提示等评估挑战。该研究揭示了提示设计在充分发挥大型语言模型潜力方面的关键作用。 |
| [^16] | [Federated Short-Term Load Forecasting with Personalization Layers for Heterogeneous Clients.](http://arxiv.org/abs/2309.13194) | 本文介绍了一种基于个性化层的联邦短期负载预测方法，该方法通过在联邦学习框架中训练特定层时仅使用客户的数据来缓解异质性数据带来的训练模型质量下降问题。通过实验证明，采用个性化层训练的模型具有优越的性能，在保证隐私的同时实现了良好的预测效果。 |
| [^17] | [Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation.](http://arxiv.org/abs/2309.13192) | 本文提出了GreenTrainer，一种新的LLM细调技术，通过自适应评估不同张量的反向传播成本和对细调模型准确性的贡献，以实现绿色AI。 |
| [^18] | [Spatial-frequency channels, shape bias, and adversarial robustness.](http://arxiv.org/abs/2309.13190) | 本研究通过临界带屏蔽实验揭示了人类物体识别和神经网络物体识别在空间频率通道上的差异，发现人类在自然图像中识别物体所使用的通道与字母和光栅识别相同，而神经网络具有不同的频率通道和鲁棒性。 |
| [^19] | [Masked Discriminators for Content-Consistent Unpaired Image-to-Image Translation.](http://arxiv.org/abs/2309.13188) | 本论文提出了一种基于遮盖的判别器方法，用于减少无配对图像转换中的内容不一致性。通过在两个域的全局判别器上使用基于内容的遮罩，可以显著降低不一致性。同时，引入了局部判别器和相似性采样策略，以减少由于遮盖过程引起的伪影。 |
| [^20] | [Visualizing Topological Importance: A Class-Driven Approach.](http://arxiv.org/abs/2309.13185) | 本文提出了一种可视化拓扑重要性的方法，能够揭示与类别标签相关的每个数据集中重要的拓扑结构。 |
| [^21] | [Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning.](http://arxiv.org/abs/2309.13181) | 本研究通过引入学习挑战诊断器(LCD)来分析视频游戏对深度强化学习的计算需求，并在Procgen基准测试中发现新的挑战分类。结果表明，LCD的预测可靠且能指导算法的发展。 |
| [^22] | [Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation.](http://arxiv.org/abs/2309.13179) | 通过机器学习支持的多物理仿真加速多目标优化，提出了一种框架来逼近和加速复杂的多物理仿真，并在实验中展示了其有效性。 |
| [^23] | [Flow Factorized Representation Learning.](http://arxiv.org/abs/2309.13167) | 本文提出了一种称为流动因式表示学习的新观点，并展示了比现有框架更高效更有用的结构化表示的学习方法。 |
| [^24] | [Invisible Watermarking for Audio Generation Diffusion Models.](http://arxiv.org/abs/2309.13166) | 该论文提出了一种用于音频扩散模型的隐形数字水印技术，在保证正常音频生成的同时，还能为模型验证提供保护层，用于鉴别模型所有权和维护其完整性。 |
| [^25] | [GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior.](http://arxiv.org/abs/2309.13160) | 本文提出了一种基于高斯混合后验的VAE方法，重新定义了ELBO，引入正则化项和PatchGAN鉴别器，能够生成逼真的人脸。 |
| [^26] | [Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations.](http://arxiv.org/abs/2309.13150) | 本文提出了一种用于对抗相机运动扰动的像素级平滑框架，通过在二维像素空间中使用平滑分布来提高鲁棒性认证的效率，并完全上界投影误差。 |
| [^27] | [Trading-off Mutual Information on Feature Aggregation for Face Recognition.](http://arxiv.org/abs/2309.13137) | 本文提出了一种技术，用于聚合ArcFace和AdaFace两个最先进的人脸识别模型的输出，通过利用变换器注意机制来增强整体人脸识别系统的判别能力。同时，改进了自注意机制以有效捕捉局部和全局依赖关系。 |
| [^28] | [Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors.](http://arxiv.org/abs/2309.13135) | 该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。 |
| [^29] | [AntiBARTy Diffusion for Property Guided Antibody Design.](http://arxiv.org/abs/2309.13129) | 该论文提出了一种基于机器学习的方法AntiBARTy来导向抗体设计，通过训练抗体特异性语言模型，并利用其潜在空间来训练性质条件扩散模型，实现了生成具有改进溶解性的新型抗体的能力。 |
| [^30] | [Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins.](http://arxiv.org/abs/2309.13108) | 使用基于张量网络的电路编译方法AMLET，该方法可以解决量子电路加载经典数据的“输入问题”。作者在金融、图像、流体和蛋白质领域进行了广泛的数值实验，并展示了该方法的有效性。 |
| [^31] | [OpportunityFinder: A Framework for Automated Causal Inference.](http://arxiv.org/abs/2309.13103) | OpportunityFinder是一个无需编码的因果推断框架，可以帮助非专业用户进行各种面板数据的因果推断研究，节省科学家和经济学家的带宽，并提供统计和严格的敏感性和稳健性分析。 |
| [^32] | [Importance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR.](http://arxiv.org/abs/2309.13102) | 本论文通过使用联邦学习训练端到端ASR模型，研究了优化器对平滑性的重要性，并总结了适用的算法和最佳实践。 |
| [^33] | [Topological Data Mapping of Online Hate Speech, Misinformation, and General Mental Health: A Large Language Model Based Study.](http://arxiv.org/abs/2309.13098) | 该研究使用大型语言模型分析了社交媒体上的在线仇恨言论、虚假信息和心理健康之间的关系。 |
| [^34] | [Multiple Independent DE Optimizations to Tackle Uncertainty and Variability in Demand in Inventory Management.](http://arxiv.org/abs/2309.13095) | 本研究通过比较多种算法，发现差分进化（DE）算法在优化库存管理中表现优异，能有效降低库存成本，特别适用于不确定需求模式的情况下。 |
| [^35] | [Prototype-Enhanced Hypergraph Learning for Heterogeneous Information Networks.](http://arxiv.org/abs/2309.13092) | 该论文提出了一种增强型原型超图学习方法，用于异构信息网络中的节点分类。该方法使用超图捕捉高阶关系并提取语义信息，不依赖于元路径。通过利用原型的强大功能，该方法提高了超图学习的鲁棒性，并向我们揭示了潜在网络结构的可解释性洞察力。 |
| [^36] | [Learning algorithms for identification of whisky using portable Raman spectroscopy.](http://arxiv.org/abs/2309.13087) | 本研究使用机器学习算法和便携式拉曼光谱仪，通过样品的光谱信息进行威士忌的快速鉴别和浓度分析，实现了高精准度的品牌识别和成分检测。 |
| [^37] | [Towards Lexical Analysis of Dog Vocalizations via Online Videos.](http://arxiv.org/abs/2309.13086) | 本研究通过在线视频的数据驱动研究，探索了狗叫声的语义，发现了支持以前启发式研究的证据，并提出了关于狗叫声的新的观点和发现。 |
| [^38] | [Does My Dog ''Speak'' Like Me? The Acoustic Correlation between Pet Dogs and Their Human Owners.](http://arxiv.org/abs/2309.13085) | 这项研究初步调查了宠物狗叫声与其主人语言环境之间的声学相关性，并发现了两种语言环境下狗叫声的显著声学差异，并找到了一些可能与主人语言模式相关的狗叫声特征。 |
| [^39] | [SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels.](http://arxiv.org/abs/2309.13080) | 这个论文提出了一个名为SPICED的新闻相似性检测数据集，包括七个主题，并提供了四种不同的方法来生成新闻。 |
| [^40] | [LPML: LLM-Prompting Markup Language for Mathematical Reasoning.](http://arxiv.org/abs/2309.13078) | 本论文提出了LPML，一种用于数学推理的LLM提示标记语言。通过将Chain-of-Thought方法和Python REPL与该标记语言结合，我们能够控制LLM生成文本中的错误，并增强其推理能力。我们的方法能够实现利用Python计算纠正错误和解决挑战性数学问题，而只需要零样本提示。 |
| [^41] | [A Differentiable Framework for End-to-End Learning of Hybrid Structured Compression.](http://arxiv.org/abs/2309.13077) | 提出了一个可微分的端到端混合结构压缩学习框架，该框架能够在单一的分析公式中融合滤波器选择、秩选择和预算约束，并通过梯度优化实现端到端学习。实验证明了该框架的有效性，超过了现有的结构化压缩方法。 |
| [^42] | [SCREWS: A Modular Framework for Reasoning with Revisions.](http://arxiv.org/abs/2309.13075) | SCREWS是一个模块化框架，用于推理修订。它能够统一先前的方法并提供新的策略来识别改进的推理链。在多样的推理任务上，使用最先进的LLMs（ChatGPT和GPT-4）评估SCREWS的性能，并发现了有用的新的推理策略。 |
| [^43] | [Weakly Supervised Reasoning by Neuro-Symbolic Approaches.](http://arxiv.org/abs/2309.13072) | 本文介绍了一种基于神经符号方法的弱监督推理框架，该框架将符号主义和连接主义结合起来，成功应用于各种自然语言处理任务，并通过设计具有符号潜在结构的神经系统，并应用强化学习或松弛方法来进行推理。 |
| [^44] | [Tree-Based Reconstructive Partitioning: A Novel Low-Data Level Generation Approach.](http://arxiv.org/abs/2309.13071) | 基于树的重建分区（TRP）是一种新颖的PCGML方法，能够在游戏开发的早期阶段引入，无需人类专业知识或大量训练数据。 |
| [^45] | [Machine Learning Technique Based Fake News Detection.](http://arxiv.org/abs/2309.13069) | 本论文通过使用机器学习技术训练了一个模型，可以有效地检测假新闻。在实验中，我们发现朴素贝叶斯分类器的准确率为56%，是最佳模型。 |
| [^46] | [UNICON: A unified framework for behavior-based consumer segmentation in e-commerce.](http://arxiv.org/abs/2309.13068) | UNICON是一个统一的深度学习消费者细分框架，利用丰富的消费者行为数据进行个性化，实现了通过扩大预定义的目标种子细分来获取类似目标的个性化结果，并通过揭示具有相似性倾向的非明显消费者细分来获取数据驱动的个性化结果。 |
| [^47] | [Causal Discovery and Counterfactual Explanations for Personalized Student Learning.](http://arxiv.org/abs/2309.13066) | 该论文使用因果发现技术来识别学生表现的因果预测因素，并应用反事实分析来提供个性化建议。 |
| [^48] | [InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning.](http://arxiv.org/abs/2309.13064) | InvestLM是一个通过对金融领域指导数据集进行调优的大型语言模型，具有强大的理解金融文本的能力，并在投资相关问题上提供有帮助的回答。金融专家评价其与最先进的商业模型可媲美，并在金融NLP基准问题上展现了强大的泛化能力。 |
| [^49] | [Implementing Learning Principles with a Personal AI Tutor: A Case Study.](http://arxiv.org/abs/2309.13060) | 本研究通过将AI导师与学习计划相结合，实施了个性化、检索练习和间隔重复等学习原理，研究结果显示，积极使用AI导师参与学习的学生获得了显著更高的成绩。 |
| [^50] | [Students Success Modeling: Most Important Factors.](http://arxiv.org/abs/2309.13052) | 本研究提出了一个深度学习模型，用于预测有风险的学生，并根据学生的课程进展不同阶段进行调整。实验结果显示，在最早的阶段就可以相当准确地区分将要毕业的学生与有风险的学生，并且后者的预测准确性会迅速提高，但在后者这个类别内部的区分度会相对较低。 |
| [^51] | [Decoding the Alphabet Soup of Degrees in the United States Postsecondary Education System Through Hybrid Method: Database and Text Mining.](http://arxiv.org/abs/2309.13050) | 本文提出了一个混合模型，通过数据库和文本挖掘方法，解码了美国高等教育系统中学位的不确定表达，并通过对学生追踪报告进行解释和分类，实现了对学位级别的准确预测。这种分类有助于研究学生成功和流动的模式。 |
| [^52] | [NeuroCADR: Drug Repurposing to Reveal Novel Anti-Epileptic Drug Candidates Through an Integrated Computational Approach.](http://arxiv.org/abs/2309.13047) | 本文介绍了一种名为NeuroCADR的算法，通过综合计算方法进行药物重用以揭示新的抗癫痫药物候选人。该方法利用数据库的信息，确定靶蛋白和药物分子的相互作用，通过k最近邻算法、随机森林分类和决策树等方法提高了准确性。 |
| [^53] | [Privacy Preserving Machine Learning for Behavioral Authentication Systems.](http://arxiv.org/abs/2309.13046) | 本文研究了针对行为认证系统的隐私保护机器学习。我们使用随机投影技术来确保神经网络模型中的数据隐私，以防止隐私攻击。这种方法可以消除个人资料数据库的需求，并能有效验证用户的身份。 |
| [^54] | [Expressive variational quantum circuits provide inherent privacy in federated learning.](http://arxiv.org/abs/2309.13002) | 表达性变分量子电路模型在联邦学习中提供固有隐私保护，同时通过使用过度参数化保证模型可训练性。通过解决高次多元切比雪夫多项式方程的复杂性，实现对梯度反转攻击的困难性。 |
| [^55] | [ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals.](http://arxiv.org/abs/2309.12312) | ForceSight是一个使用文本引导的移动操作系统，通过深度神经网络预测视觉力导向目标。在实验中，该系统展示了在未见环境中进行精确抓取、抽屉打开和物体交接等任务的能力，并取得了较高的成功率。 |
| [^56] | [Learning to Drive Anywhere.](http://arxiv.org/abs/2309.12295) | 本文提出了一种能够学习适应不同地理位置和驾驶行为的模型，该模型通过引入基于地理位置的通道注意机制，在数据驱动的方式下高效地学习并灵活地建模不同地区之间的相似性和差异性。 |
| [^57] | [The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A".](http://arxiv.org/abs/2309.12288) | LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。 |
| [^58] | [Latent Diffusion Models for Structural Component Design.](http://arxiv.org/abs/2309.11601) | 本文提出了一个潜在扩散模型的框架，用于生成符合加载条件的结构组件设计。与其他生成方法相比，该方法允许对现有设计进行编辑，并且具有近优性。实验结果证明了生成设计的结构性能和潜在候选设计的可变性。 |
| [^59] | [Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition.](http://arxiv.org/abs/2309.11327) | 本研究通过收集和标注数据以及探索切换方法，提出了一种有效的突尼斯方言自动语音识别解决方案，并且通过人工评估来消除拼写不合适的干扰。 |
| [^60] | [AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning.](http://arxiv.org/abs/2309.10980) | 本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。 |
| [^61] | [Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds.](http://arxiv.org/abs/2309.10918) | 该论文研究了定义在紧致Riemannian流形上的内在Matern高斯过程和外在过程之间的收缩速率，并发现它们的速率在适当匹配平滑参数的情况下是相等的。 |
| [^62] | [Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers.](http://arxiv.org/abs/2309.10639) | 本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。 |
| [^63] | [A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents.](http://arxiv.org/abs/2309.10563) | 本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。 |
| [^64] | [FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning.](http://arxiv.org/abs/2309.10283) | FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。 |
| [^65] | [Recovering from Privacy-Preserving Masking with Large Language Models.](http://arxiv.org/abs/2309.08628) | 本文利用大型语言模型（LLM）探索了替换标识信息的方法，并在下游语言建模任务上进行了评估。实验结果表明，使用混淆语料库训练的模型能够达到可比较的性能。 |
| [^66] | [Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof.](http://arxiv.org/abs/2309.08339) | 本文分析了ADAM在非凸设置中具有恒定步长的收敛性，给出了步长达到几乎肯定渐近收敛的充分条件，并提供了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。 |
| [^67] | [Structure-Preserving Transformers for Sequences of SPD Matrices.](http://arxiv.org/abs/2309.07579) | 本文介绍了一种保持序列的对称正定矩阵的黎曼几何特性的结构保持变压器机制，并将其应用于自动睡眠分期，取得了高水平的阶段性能。 |
| [^68] | [Efficient Finite Initialization for Tensorized Neural Networks.](http://arxiv.org/abs/2309.06577) | 这种方法提出了一种高效有限初始化张量化神经网络层的方法，避免了参数爆炸问题，并通过使用弗罗贝尼乌斯范数的迭代部分形式来计算范数，使其具有有限范围。应用于不同层的实验表明其性能良好。 |
| [^69] | [Language Models as Black-Box Optimizers for Vision-Language Models.](http://arxiv.org/abs/2309.05950) | 本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。 |
| [^70] | [Training of Spiking Neural Network joint Curriculum Learning Strategy.](http://arxiv.org/abs/2309.04737) | 该论文提出了一种将课程学习引入脉冲神经网络的训练模型，使其更类似于人类学习过程，并提高了其生物解释性。 |
| [^71] | [Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis.](http://arxiv.org/abs/2309.03033) | 本研究利用深度学习方法通过基因表达分析实现对患者多囊肾病（PKD）的准确和早期检测。 |
| [^72] | [A Lightweight and Transferable Design for Robust LEGO Manipulation.](http://arxiv.org/abs/2309.02354) | 本文介绍了一种轻量化且可传递的设计，用于解决机器人乐高操纵中的复杂性和精确性要求。通过硬件软件协同设计和进化策略优化，实现了高效可靠的乐高操纵，并展示了该设计的普适性和可传递性。 |
| [^73] | [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models.](http://arxiv.org/abs/2309.01219) | 本文调查了大型语言模型中幻觉的检测、解释和缓解的最新研究，提出了幻觉现象和评估基准的分类，并讨论了未来研究的潜在方向。 |
| [^74] | [Tropical Geometric Tools for Machine Learning: the TML package.](http://arxiv.org/abs/2309.01082) | TML软件包是第一个包含一套全面工具和方法的R软件包，用于处理与热带凸性相关的基本计算和可视化，以及使用热带度量进行监督和无监督学习模型的统计推断。 |
| [^75] | [Algebraic, Topological, and Mereological Foundations of Existential Granules.](http://arxiv.org/abs/2308.16157) | 本研究从代数、拓扑和细部学的角度创造了新的存在性颗粒概念，并刻画了其特征。这些颗粒首先确定自己，然后与环境互动，并且适用于多种颗粒计算理论框架。研究结果对算法开发、分类问题应用和方法推广的数学基础具有重要意义。 |
| [^76] | [Text Style Transfer Evaluation Using Large Language Models.](http://arxiv.org/abs/2308.13577) | 大型语言模型（LLMs）有潜力成为人工评估和其他自动化评价指标的可行替代方案。 |
| [^77] | [Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors.](http://arxiv.org/abs/2308.13546) | 本研究通过利用超扫描技术，引入功能性图对比学习方法探究基于刻板印象的压力引发的情绪传染。研究结果揭示了情绪传染与认知功能之间的复杂相互作用。 |
| [^78] | [Multi-event Video-Text Retrieval.](http://arxiv.org/abs/2308.11551) | 本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。 |
| [^79] | [How Much Temporal Long-Term Context is Needed for Action Segmentation?.](http://arxiv.org/abs/2308.11358) | 本文提出了一种基于transformer的模型，利用稀疏注意力捕捉视频的完整上下文，以回答时间行动分割需要多少长期时间上下文。通过与当前最先进的方法进行比较，在三个时间行动分割数据集上取得了良好的性能。 |
| [^80] | [Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection.](http://arxiv.org/abs/2308.11119) | 本文提出了一种利用CLIP作为数据源的零样本异常检测方法，通过随机词语数据增强的方式改善了训练效率，并应用prompt-guided分类进行图像的检测。该方法克服了以往需为每个对象类别训练模型的低效问题，有潜力应用于工业领域。 |
| [^81] | [A Comprehensive Empirical Evaluation on Online Continual Learning.](http://arxiv.org/abs/2308.10328) | 这项综合实证评估了解决在线持续学习问题的各种方法，发现大多数方法存在稳定性和欠拟合问题，但所学习的表示与独立同分布的训练相当。 |
| [^82] | [GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning.](http://arxiv.org/abs/2308.10279) | 这里是中文总结出的一句话要点：GPFL是一种新的个性化联邦学习方法，它能够同时学习全局和个性化的特征信息，在效果、可扩展性、公平性、稳定性和隐私性方面优于其他方法，并减轻了过拟合现象，提升了准确度。 |
| [^83] | [Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction.](http://arxiv.org/abs/2308.05103) | 本研究提出了一种零射波自监督学习的多次扫描图像重建方法，用于改进扩散MRI，通过深度学习和虚拟线圈的应用，能够克服多次扫描中的相位变化问题，并提高图像重建质量。 |
| [^84] | [Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations.](http://arxiv.org/abs/2308.03882) | 本文提出了一种利用未见状态增强的策略，在离线强化学习中通过基于价值的扰动和过滤，实现了对离线数据之外的状态的利用和泛化。 |
| [^85] | [FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks.](http://arxiv.org/abs/2307.14751) | FLARE是第一个用于验证疑似深度强化学习策略是否是另一个策略的非法副本的指纹机制，通过使用通用对抗性掩码作为指纹，并测量动作一致性值来验证被盗策略的真实所有权。 |
| [^86] | [Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior.](http://arxiv.org/abs/2307.14619) | 本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。 |
| [^87] | [Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space.](http://arxiv.org/abs/2307.12032) | 本论文引入创新的增强迁移学习模型，以及通过在霍夫空间中使用新的SR损失函数改善飞行轨迹线检测的方法。这些方法能够准确地检测飞行轨迹并且对数据需求较少，为基于机器学习的航空研究中的飞行轨迹检测提供了新的解决方案。 |
| [^88] | [MASR: Multi-label Aware Speech Representation.](http://arxiv.org/abs/2307.10982) | MASR是一种多标签感知语音表示学习框架，可以利用多个外部知识源增强元数据信息的利用，并在多个下游任务上展示了显著的性能提升。 |
| [^89] | [Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection.](http://arxiv.org/abs/2307.06422) | 本论文提出了一种差分私有的解耦图卷积方法，用于多粒度拓扑保护。引入了图差分隐私框架，可以确保模型参数和预测的私密性。 |
| [^90] | [Onion Universe Algorithm: Applications in Weakly Supervised Learning.](http://arxiv.org/abs/2307.04870) | 洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。 |
| [^91] | [False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers.](http://arxiv.org/abs/2307.04358) | 本研究利用可解释的人工智能方法分析了基于深度学习的DGA分类器的推理过程，并揭示了其中的偏见。通过消除这些偏见，我们设计了一个无偏见且上下文感知的检测系统，保持了最先进分类器的检测率，同时提出了一个视觉分析系统，以增加对检测方法的信任和透明度。 |
| [^92] | [Learning to Communicate using Contrastive Learning.](http://arxiv.org/abs/2307.01403) | 本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。 |
| [^93] | [MoVie: Visual Model-Based Policy Adaptation for View Generalization.](http://arxiv.org/abs/2307.00972) | 本论文提出了一种名为MoVie的方法，通过基于视觉模型的策略自适应实现了视图泛化。该方法在不需要显式奖励信号和训练过程修改的情况下，在多个实际场景中表现出卓越的性能，相对改进达到了33%至152%。 |
| [^94] | [ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram.](http://arxiv.org/abs/2306.15681) | ECG-QA是第一个专为心电图分析设计的问答数据集，包括涵盖广泛临床相关ECG主题的问题模板和多样化的ECG解读问题。这一资源将为未来的医疗保健问答研究提供有价值的见解。 |
| [^95] | [Simulating counterfactuals.](http://arxiv.org/abs/2306.15328) | 该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。 |
| [^96] | [Diverse Community Data for Benchmarking Data Privacy Algorithms.](http://arxiv.org/abs/2306.13216) | 多样社区数据摘要旨在为隐私保护机器学习研究提供真实、多样和复杂的基准数据，以解决合成数据的偏差和隐私问题。 |
| [^97] | [Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes.](http://arxiv.org/abs/2306.12045) | 本研究提出 TeCoS-LVM 模型，使用脉冲神经元以模拟自然视觉刺激的神经响应。该模型能够自适应地探索和利用刺激序列中的时间依赖关系，避免丢失脉冲列中的信息。 |
| [^98] | [DropCompute: simple and more robust distributed synchronous training via compute variance reduction.](http://arxiv.org/abs/2306.10598) | 该论文提出了一种称为DropCompute的简单且稳定的分布式同步训练方法，通过减少工作节点间的计算变异性来提高训练的稳定性。 |
| [^99] | [Learning-Augmented Decentralized Online Convex Optimization in Networks.](http://arxiv.org/abs/2306.10158) | 本论文研究了网络中的分布式在线凸优化问题，并提出了一种新的学习增强的分布式在线优化算法（LADO），该算法使个体智能体能够基于本地信息选择行动。与现有的算法不同，LADO在分布式设置中实现了强鲁棒性保证，并通过考虑鲁棒性要求来训练机器学习策略。此外，我们还证明了LADO算法的平均成本界限，揭示了平均性能和最坏情况鲁棒性之间的折衷。 |
| [^100] | [Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization.](http://arxiv.org/abs/2306.09803) | 本文介绍了一个模块化框架和基准，用于组合和混合变量贝叶斯优化，并提供多样的合成和真实世界基准测试。通过此框架，作者展示了4种常见的MCBO技术。 |
| [^101] | [Modularizing while Training: a New Paradigm for Modularizing DNN Models.](http://arxiv.org/abs/2306.09376) | 本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。 |
| [^102] | [Re-Benchmarking Pool-Based Active Learning for Binary Classification.](http://arxiv.org/abs/2306.08954) | 本论文通过重新基准化实验证明了不确定性采样策略仍然是大多数数据集上有效和首选的选择，并揭示了模型兼容性问题的重要性。 |
| [^103] | [How to estimate carbon footprint when training deep learning models? A guide and review.](http://arxiv.org/abs/2306.08323) | 这篇论文提出了一份详尽的指南和综述，介绍了如何估计训练深度学习模型的碳足迹，并比较了多种在线和软件工具的能源消耗估计结果。研究为AI从业人员在选择合适的工具和基础设施方面提供了建议。 |
| [^104] | [Efficient Quantization-aware Training with Adaptive Coreset Selection.](http://arxiv.org/abs/2306.07215) | 本研究提出了一种用于改善量化感知训练的训练效率的方法，通过核心集选择和两个重要性指标来选择训练数据的子集。 |
| [^105] | [PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning.](http://arxiv.org/abs/2306.06394) | PEAR是一种基于原始操作的自适应重标记方法，用于Boosting层次强化学习。它通过对专家演示进行自适应重标记来生成高效的子目标监督，并通过联合优化强化学习和模仿学习来训练分层代理。实验结果显示，PEAR能够在具有挑战性的机器人环境中取得良好的性能。 |
| [^106] | [Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey.](http://arxiv.org/abs/2306.06123) | 本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。 |
| [^107] | [RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain.](http://arxiv.org/abs/2306.04054) | RescueSpeech是一个用于搜救领域语音识别的德语语音数据集，但目前最先进的方法仍无法令人满意。 |
| [^108] | [Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization.](http://arxiv.org/abs/2306.02879) | 本文通过研究神经元激活状态，提出了神经元激活覆盖度（NAC）作为衡量神经元行为的指标。利用NAC可以有效区分域内和离域输入，简化离域检测问题，并且NAC与模型的泛化能力间存在正相关关系。 |
| [^109] | [End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes.](http://arxiv.org/abs/2305.15930) | 本文提出了第一个可泛化到学习获取函数的神经过程端到端框架，使用强化学习解决了缺乏标签获取数据以及利用代理模型或获取函数的传统Meta-BO方法训练过程中的挑战。 |
| [^110] | [The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning.](http://arxiv.org/abs/2305.15703) | 通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。 |
| [^111] | [Black-Box Variational Inference Converges.](http://arxiv.org/abs/2305.15349) | 通过对黑盒变分推断（BBVI）的分析，发现一些常见的算法设计选择可能会导致次优收敛速率，但使用带有近端随机梯度下降的BBVI可以实现最强收敛率保证。 |
| [^112] | [Beyond Individual Input for Deep Anomaly Detection on Tabular Data.](http://arxiv.org/abs/2305.15121) | 本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。 |
| [^113] | [Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion.](http://arxiv.org/abs/2305.14375) | 本文提出了一种新的基于图学习的节点排序方法（MGL2Rank），充分利用了道路网络的丰富特征，并且在实验中表现出比现有方法更高的精度和效率。 |
| [^114] | [QFA2SR: Query-Free Adversarial Transfer Attacks to Speaker Recognition Systems.](http://arxiv.org/abs/2305.14097) | 本论文提出了QFA2SR，一种无查询的黑盒攻击方法，用于对抗说话人识别系统。该方法利用对抗声音的可传递性，通过定制损失函数、SRS集成和时频腐蚀等新方法，实现了有效且不可察觉的攻击。 |
| [^115] | [Few-Shot Continual Learning for Conditional Generative Adversarial Networks.](http://arxiv.org/abs/2305.11400) | 本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。 |
| [^116] | [On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2305.11283) | 本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。 |
| [^117] | [Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence.](http://arxiv.org/abs/2305.10468) | 该论文提出了一个更为强大的人工神经网络模型，该模型中同一隐藏层中的隐藏神经元相互连接，可以学习复杂模式并加速收敛速度。 |
| [^118] | [ZeroFlow: Fast Zero Label Scene Flow via Distillation.](http://arxiv.org/abs/2305.10424) | ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。 |
| [^119] | [Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review.](http://arxiv.org/abs/2305.06739) | 该综述针对MRI中的运动问题，综述了基于深度学习对运动校正的方法，发现了不同应用之间的差异和共同点，在未来的方向上提出了建议。 |
| [^120] | [Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy.](http://arxiv.org/abs/2305.06360) | 本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。 |
| [^121] | [Recommender Systems with Generative Retrieval.](http://arxiv.org/abs/2305.05065) | 本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。 |
| [^122] | [DMF-TONN: Direct Mesh-free Topology Optimization using Neural Networks.](http://arxiv.org/abs/2305.04107) | 本文提出了一种直接无网格法进行拓扑优化，集成了神经网络并用于求解遵从度和体积分数约束违规的优化问题，无需传统的有限元分析和网格化，能够无缝地集成到后处理软件中。 |
| [^123] | [Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs.](http://arxiv.org/abs/2305.03935) | 本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。 |
| [^124] | [How to address monotonicity for model risk management?.](http://arxiv.org/abs/2305.00799) | 本文提出了使用神经加性模型的单调的树丛来实现单调性和透明性的结合，以确立透明机器学习模型的问责和公平性。通过实证示例，证明该方法透明、有问责性和公平，尤其对于避免单调性问题具有优势。 |
| [^125] | [Benchmarking Low-Shot Robustness to Natural Distribution Shifts.](http://arxiv.org/abs/2304.11263) | 本文通过对不同少样本数据集、架构、预训练初始化和稳健性干预的自然分布漂移的稳健性进行了首次深入研究，发现没有单一的选择模型比其他模型更稳健，现有的干预措施也可能无法提高某些数据集的稳健性。 |
| [^126] | [Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams.](http://arxiv.org/abs/2304.10740) | 本文研究了基于多模态的深度学习融合技术在信用评级预测中的应用，通过比较不同融合策略和深度学习模型的组合，证明了一个基于CNN的多模态模型通过两种融合策略优于其他多模态技术，同时在比较简单和复杂的模型中发现，更复杂的模型并不一定表现更好。 |
| [^127] | [A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions.](http://arxiv.org/abs/2304.06787) | 本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。 |
| [^128] | [Controllable Textual Inversion for Personalized Text-to-Image Generation.](http://arxiv.org/abs/2304.05265) | 本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。 |
| [^129] | [On algorithmically boosting fixed-point computations.](http://arxiv.org/abs/2304.04665) | 本论文提出了一种称为"算法增强"的通用算法抽象，针对纳什均衡问题进行了研究，并展示了该方法可以以指数速度加速线性映射的收敛，同时还可以将非收敛的迭代算法转换为收敛的算法。 |
| [^130] | [Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots.](http://arxiv.org/abs/2304.03593) | 本论文提出了一种基于碰撞概率的无图Crowd Navigation方法，使用深度强化学习(DRL)来感知人群的危险程度，确保机器人在通过拥挤环境时的安全，同时提高模型的可扩展性。 |
| [^131] | [Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models.](http://arxiv.org/abs/2303.16047) | 提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。 |
| [^132] | [Towards Better Dynamic Graph Learning: New Architecture and Unified Library.](http://arxiv.org/abs/2303.13047) | 我们提出了一种基于Transformer的新型动态图学习架构DyGFormer，并引入了统一的库DyGLib，以促进可重复、可扩展和可信的动态图学习研究。DyGFormer通过邻居共现编码方案和分块技术实现更长期历史的高效推理，在13个不同领域的数据集上实现了最先进的性能。 |
| [^133] | [Secure Aggregation in Federated Learning is not Private: Leaking User Data at Large Scale through Model Modification.](http://arxiv.org/abs/2303.12233) | 联邦学习虽然能够消除数据共享，但共享的梯度可能会包含私密信息，并且攻击者可以通过恶意修改架构和参数或使用优化从共享的梯度中近似用户数据，导致用户数据泄露。 |
| [^134] | [Lamarr: LHCb ultra-fast simulation based on machine learning models deployed within Gauss.](http://arxiv.org/abs/2303.11428) | LHCb实验中的90%计算资源用于生产模拟数据样本，而Lamarr是一个基于机器学习模型的系统，通过对LHCb实验的探测器响应和重建算法进行参数化，加快了模拟产出。 |
| [^135] | [Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations.](http://arxiv.org/abs/2303.10523) | 本文提出了一种无监督的方法，通过对CNN进行转换，从而更好地解释中间层的表示，提取了一个可解释性欠完备基础，并证明该方法在各种网络结构和训练数据集上都很有效。 |
| [^136] | [Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies.](http://arxiv.org/abs/2303.07551) | 本文提出通过在权重空间中合并训练于不同 MuJoCo 运动问题上的 Decision Transformer 的子集，形成多任务模型。通过共享一些辅助任务的训练以及共同使用预训练初始化，能够获得更好的结果。这个方向的研究有助于使代理的过程民主化和分发。 |
| [^137] | [CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction.](http://arxiv.org/abs/2303.06945) | 本论文提出了一种基于共进化增强的全局关注神经网络，它是一种用于蛋白质相互作用位点预测的深度学习模型，结合了共进化特征和全局关注机制以更好地捕捉氨基酸残基之间的关系并考虑到所有残基的贡献。 |
| [^138] | [Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients.](http://arxiv.org/abs/2303.05341) | 通过引入罚函数，我们提出了一种创新的深度部分线性Cox模型，用于在肺癌患者的CT扫描中分析死亡风险。该模型能有效地整合已知和新兴的风险因素，解决了参数维度超出样本大小和非参数建模中维度灾难的问题。 |
| [^139] | [GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning.](http://arxiv.org/abs/2303.05193) | 本文提出了一种名为GOATS的方法，使用目标采样自适应课程强化学习技术，通过插值位置目标和数量目标的分布创建学习过程中的课程来解决机器人舀取任务中的位置目标和水量目标问题，取得了比基线更好的表现。 |
| [^140] | [Travel Demand Forecasting: A Fair AI Approach.](http://arxiv.org/abs/2303.01692) | 本研究提出了一种新的方法来开发具有公平意识的、高度准确的旅行需求预测模型，该方法可以同时提高AI模型对于多个受保护属性的公平性。 |
| [^141] | [Decision-Oriented Learning with Differentiable Submodular Maximization for Vehicle Routing Problem.](http://arxiv.org/abs/2303.01543) | 本文研究了面向车辆路径问题的决策导向学习方法，该方法利用可微子模最大化学习函数，将上下文观察映射到子模函数参数。该研究强调了在考虑下游任务优化的情况下，传统的独立解决预测阶段的学习问题可能导致与最终目标不一致的结果。 |
| [^142] | [Targeted demand response for flexible energy communities using clustering techniques.](http://arxiv.org/abs/2303.00186) | 本研究探讨了使用机器学习算法中的聚类技术设计并执行需求响应（DR）计划的可行性，目的是改变分布式能源社区内供应者的消费行为，以最小化反向功率流和削减系统范围内的功峰需求。 |
| [^143] | [Permutation Equivariant Neural Functionals.](http://arxiv.org/abs/2302.14040) | 本文介绍了置换等变神经功能网络的设计，通过对权重进行置换对称性编码，实现对其他网络权重或梯度进行处理，为学习优化、处理隐式神经表示等应用提供了架构原则。 |
| [^144] | [DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification.](http://arxiv.org/abs/2302.13057) | DeepBrainPrint是一个用于脑MRI再识别的对比框架，使用半自监督对比深度学习方法，通过创建有效的脑指纹进行实时图像检索，并引入新的图像转换方法以提高检索的稳健性和考虑患者的年龄和疾病进展。 |
| [^145] | [A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers.](http://arxiv.org/abs/2302.12168) | 本文通过比较评估深度学习模型在提前一天负荷预测中的准确性，重点研究了葡萄牙国家净聚合STLF，并分析了多层感知机（MLP）、长短期记忆网络（LSTM）、神经基础扩展系数分析（N-BEATS）、时间卷积网络（TCN）和时间融合变压器（TFT）等多个模型的影响因素。 |
| [^146] | [One Fits All:Power General Time Series Analysis by Pretrained LM.](http://arxiv.org/abs/2302.11939) | 本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。 |
| [^147] | [Graph Adversarial Immunization for Certifiable Robustness.](http://arxiv.org/abs/2302.08051) | 本文提出了图对抗免疫方法，通过疫苗接种一部分图结构来提高图的鲁棒性，避免对抗性攻击。通过边和节点两种免疫方式，可以有效地提高图的抵抗能力。 |
| [^148] | [Global Convergence Rate of Deep Equilibrium Models with General Activations.](http://arxiv.org/abs/2302.05797) | 该论文研究了具有一般激活函数的深度平衡模型（DEQ）的全局收敛速度，证明了梯度下降以线性收敛速度收敛到全局最优解，并解决了限制平衡点Gram矩阵最小特征值的挑战。 |
| [^149] | [Predicting the cardinality and maximum degree of a reduced Gr\"obner basis.](http://arxiv.org/abs/2302.05364) | 该论文利用神经网络回归模型预测了简化Gr\"obner基的基数和最大总度数，结果表明神经网络具有更好的性能统计，相比于朴素猜测或多元回归模型。 |
| [^150] | [A Graph-Based Modeling Framework for Tracing Hydrological Pollutant Transport in Surface Waters.](http://arxiv.org/abs/2302.04991) | 本研究提出了一种名为 HydroGraphs 的基于图的建模框架，用于分析水文污染物的传输和命运。该框架可以根据开源数据构建，具有简化的水文系统表示，并且可以使用常见的图分析和数据可视化技术进行分析和可视化，能够帮助准确定位污染源和脆弱区域。 |
| [^151] | [Disentanglement of Latent Representations via Causal Interventions.](http://arxiv.org/abs/2302.00869) | 该论文提出了一种基于因果干预的解缠方法，通过将量化向量视为因果变量，并在因果图中进行干预，生成影响图像中唯一变异因素的原子过渡。 |
| [^152] | [Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2302.00521) | 这项工作填补了离线多智能体增强学习（MARL）领域的一个空白，提供了Off-the-Grid MARL（OG-MARL）数据集和基准，帮助社区衡量进展。 |
| [^153] | [Laplacian-based Semi-Supervised Learning in Multilayer Hypergraphs by Coordinate Descent.](http://arxiv.org/abs/2301.12184) | 本文研究了多层超图中的拉普拉斯半监督学习问题，并采用坐标下降方法解决该问题。实验证明了使用适当选择规则的坐标下降方法的潜力。 |
| [^154] | [Salesforce CausalAI Library: A Fast and Scalable Framework for Causal Analysis of Time Series and Tabular Data.](http://arxiv.org/abs/2301.10859) | Salesforce CausalAI库是一个快速可扩展的框架，用于进行时间序列和表格数据的因果分析。它支持离散、连续和异质类型的数据，提供了处理线性和非线性因果关系的算法，并包括一个用于生成具有指定结构方程模型的合成数据的数据生成器。用户可以通过用户界面进行因果分析，无需编程。 |
| [^155] | [Asynchronous Deep Double Duelling Q-Learning for Trading-Signal Execution in Limit Order Book Markets.](http://arxiv.org/abs/2301.08688) | 该论文利用深度强化学习训练了一个代理，将高频交易信号转化为交易策略，并使用异步双对决Q学习进行交易信号执行。该方法独立研究了适应性交易的性能以及与具体的预测算法无关的影响。 |
| [^156] | [A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding.](http://arxiv.org/abs/2301.03403) | 本文提供了关于自动文本摘要系统的综述，包括方法、数据、评估和编码。作者通过引用的方式回顾了相关文献，并介绍了不同的摘要生成方法。此外，还对可用于评估和数据训练的数据集进行了综述，并使用CNN语料库数据集对方法进行了实证探索。 |
| [^157] | [StitchNet: Composing Neural Networks from Pre-Trained Fragments.](http://arxiv.org/abs/2301.01947) | StitchNet提出了一种新的神经网络创建方式，它通过组合预训练神经网络的片段来创建高性能的网络，无需传统训练的大量计算资源和数据要求。通过居中核对齐（CKA），可以有效指导片段的选择，以满足特定准确性需求和计算资源限制。此外，StitchNet还可以实现即时个性化模型创建和推断。 |
| [^158] | [Goal-Guided Transformer-Enabled Reinforcement Learning for Efficient Autonomous Navigation.](http://arxiv.org/abs/2301.00362) | 该论文提出了一种利用目标引导变压器增强学习的方法，通过将目标信息与场景表示耦合，实现高效自主导航。通过使用专家先验进行预训练，提高了数据效率。 |
| [^159] | [MolCPT: Molecule Continuous Prompt Tuning to Generalize Molecular Representation Learning.](http://arxiv.org/abs/2212.10614) | 该论文介绍了一种新的训练范式"MolCPT"，用于改善图神经网络在分子表示学习中的泛化能力。该方法利用任务相关的模式子图来进行预训练和微调，以提高对广泛的分子空间的推广能力。 |
| [^160] | [Phases, Modalities, Temporal and Spatial Locality: Domain Specific ML Prefetcher for Accelerating Graph Analytics.](http://arxiv.org/abs/2212.05250) | MPGraph是一种针对图分析加速的领域特定机器学习预取器，引入了相位转变的软检测、相位特定多模态模型和链式时空预取等优化策略，通过CSTP实现了12.52-21.23%的IPC改进，并在性能上超越了其他预取器。 |
| [^161] | [Hedging Complexity in Generalization via a Parametric Distributionally Robust Optimization Framework.](http://arxiv.org/abs/2212.01518) | 通过使用参数分布鲁棒优化框架，我们提出了一种降低高维复杂问题泛化误差的简单方法，并且在各种设置下取得了显著改进的效果。 |
| [^162] | [PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization.](http://arxiv.org/abs/2212.00979) | 本文提出了一种基于比例幅度谱训练增强的方法 PASTA，可有效提高合成数据到真实数据的泛化性能，在多个 Syn-to-Real 任务上均具有优越性能。 |
| [^163] | [SnCQA: A hardware-efficient equivariant quantum convolutional circuit architecture.](http://arxiv.org/abs/2211.12711) | SnCQA是一种硬件高效的等变量子卷积电路架构，通过利用排列对称性和空间晶格对称性，适用于解决存在排列对称性的机器学习问题，具有更高的可扩展性、准确性和噪声韧性。 |
| [^164] | [Private Ad Modeling with DP-SGD.](http://arxiv.org/abs/2211.11896) | 本研究将差分隐私随机梯度下降（DP-SGD）应用于广告建模任务，证明了该方法可以在处理高类别不平衡和稀疏梯度更新的广告数据中提供隐私和效用。 |
| [^165] | [REPAIR: REnormalizing Permuted Activations for Interpolation Repair.](http://arxiv.org/abs/2211.08403) | 作者发现仅使用神经元对齐方法不能有效解决线性插值中激活方差坍缩的问题，因此提出了REPAIR方法来修复插值的归一化置换激活。实验证明，在各种架构中将REPAIR与神经元对齐方法结合使用可以大幅降低障碍。 |
| [^166] | [A Generalist Framework for Panoptic Segmentation of Images and Videos.](http://arxiv.org/abs/2210.06366) | 这个论文提出了一个通用框架，用于图像和视频的全景分割。他们将全景分割问题定义为离散数据生成问题，并提出了一个简单的扩散模型来建模全景掩码。他们的方法能够在流式设置中建模视频，并自动学习跟踪对象实例，并在实验中展现出与最先进的专家方法竞争的能力。 |
| [^167] | [Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials.](http://arxiv.org/abs/2210.05178) | 本文提出了一种基于离线RL的机器人预训练框架(PTR)，可以将现有的机器人数据集上的预训练与少量任务特定数据相结合，从而有效地学习新任务，无需表示学习或vision-based pretraining。 |
| [^168] | [Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data.](http://arxiv.org/abs/2209.15505) | 本研究提出了一种名为动量追踪的方法，其收敛速度与数据异质性无关，在分布式深度学习中具有应对异质数据的能力。 |
| [^169] | [Enumeration of max-pooling responses with generalized permutohedra.](http://arxiv.org/abs/2209.14978) | 广义排列胞内枚举了最大池化响应的组合学，通过计算Minkowski和的顶点数量，得到了最大池化层的线性区域数量，并得到了关于窗口大小和步幅的一维最大池化层顶点和面的生成函数和封闭公式，以及一个特殊情况下二维最大池化的顶点数量。 |
| [^170] | [Deep Variational Free Energy Approach to Dense Hydrogen.](http://arxiv.org/abs/2209.06095) | 该论文提出了一种基于深度生成模型的变分自由能方法，用于研究密集氢的状态方程。通过优化两个神经网络，研究者达到了与先前的耦合电子-离子蒙特卡洛计算相当的变分自由能。在行星条件下，预测的密集氢状态方程比其他计算方法更致密。这种方法还可以直接获得密集氢的熵和自由能，为行星模拟和高压物理研究提供了新的机会。 |
| [^171] | [Fraud Dataset Benchmark and Applications.](http://arxiv.org/abs/2208.14417) | 欺诈数据集基准（FDB）是一个针对欺诈检测的公开可用数据集的汇编，涵盖了各种欺诈相关任务，为解决欺诈检测中的独特挑战提供了标准化的数据集和基准。 (arXiv:2208.14417v3 [cs.LG] UPDATED) |
| [^172] | [DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization.](http://arxiv.org/abs/2208.09708) | DenseShift网络是一种准确和高效的低位幂乘法量化方法，通过改进Shift网络的精度和引入非量化浮点激活来提高性能。 |
| [^173] | [Reduced Implication-bias Logic Loss for Neuro-Symbolic Learning.](http://arxiv.org/abs/2208.06838) | 本文提出了一种减少蕴含偏差逻辑损失（RILL）的方法，用于解决在神经符号学习中由于从模糊逻辑算子中派生的损失函数带来的偏差问题。实证研究表明，RILL相比有偏差的逻辑损失函数在知识库不完整和标记数据不足时具有显著的改进和更强的稳健性。 |
| [^174] | [EgPDE-Net: Building Continuous Neural Networks for Time Series Prediction with Exogenous Variables.](http://arxiv.org/abs/2208.01913) | 本文提出了一个连续神经网络模型EgPDE-Net，用于包含外生变量的时间序列预测，通过考虑外生变量之间的关系和其对目标序列的影响，学习多变量时间序列中的未知偏微分方程系统。 |
| [^175] | [Adversarial Camouflage for Node Injection Attack on Graphs.](http://arxiv.org/abs/2208.01819) | 本文提出了一种名为CANA的对抗伪装框架，可以在节点注入攻击中使被注入节点看起来正常，并提高在实际场景下防御/检测方法的攻击性能。 |
| [^176] | [Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization.](http://arxiv.org/abs/2207.00479) | 这项研究提出了一种异步分散的贝叶斯优化方法，可以实现大规模超参数优化，并在Polairs超级计算机上展示了模型准确性的改进。 |
| [^177] | [Explainable and High-Performance Hate and Offensive Speech Detection.](http://arxiv.org/abs/2206.12983) | 这项研究构建了一个可解释且高性能的模型，基于XGBoost算法，用于检测社交媒体平台上的仇恨和冒犯性言论。该模型在不平衡的Twitter数据上显示出更好的性能，并且在降采样后的数据中也表现出优越性能。 |
| [^178] | [Scalable Distributed Algorithms for Size-Constrained Submodular Maximization in the MapReduce and Adaptive Complexity Models.](http://arxiv.org/abs/2206.09563) | 本论文研究了在MapReduce和适应性复杂度模型中用于大小约束子模型最大化问题的可扩展分布式算法。通过使用几种次线性自适应算法，我们证明了这些算法满足在MR环境中所需的一致性特性，并且提出了世界首个具有恒定MR轮次的线性时间分布式算法。 |
| [^179] | [Federated Learning via Inexact ADMM.](http://arxiv.org/abs/2204.10607) | 本文提出了一种非精确ADMM算法，用于解决联邦学习中的高效优化问题。该算法既具有计算和通信效率，能够应对滞后效应，又在较弱条件下收敛，并且在数值性能方面表现出色。 |
| [^180] | [Sparse Federated Learning with Hierarchical Personalized Models.](http://arxiv.org/abs/2203.13517) | 本研究提出了一种称为sFedHP的带有层次化个性化模型的稀疏联邦学习算法，通过使用基于Moreau包络的分层近似映射和连续可微的近似L1范数作为稀疏约束，显著提高了面对多样数据的全局模型性能。 |
| [^181] | [Graph Reinforcement Learning for Radio Resource Allocation.](http://arxiv.org/abs/2203.03906) | 该论文介绍了一种利用图强化学习方法进行无线资源分配的方法，通过利用拓扑信息和排列特性，降低了深度强化学习的训练复杂性，并通过优化预测功率分配问题来验证方法的有效性。 |
| [^182] | [Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation.](http://arxiv.org/abs/2202.04176) | 本研究提出了一种通过主题建模和相对密度估计来进行犯罪热点建模的方法。实验证明该方法可以捕捉到被调度员忽视的地理热点趋势，这些热点趋势往往与整体事件密度的增加相混淆。 |
| [^183] | [Efficient Direct-Connect Topologies for Collective Communications.](http://arxiv.org/abs/2202.03356) | 本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。 |
| [^184] | [Leveraging Queue Length and Attention Mechanisms for Enhanced Traffic Signal Control Optimization.](http://arxiv.org/abs/2201.00006) | 这篇论文提出了一种利用队列长度和注意力机制的交通信号控制优化方法。作者提出了Max Queue-Length (M-QL)和AttentionLight两种新方法，实验结果表明M-QL方法优于现有的强化学习方法，并且AttentionLight方法适用于各种交通场景并具有更好的性能表现。 |
| [^185] | [Faster Rates for Compressed Federated Learning with Client-Variance Reduction.](http://arxiv.org/abs/2112.13097) | 本文提出了压缩和客户端方差减少方法COFIG和FRECON，以解决分布式和联邦学习应用中的通信瓶颈和客户端方差问题。在非凸设置下，COFIG具有$O(\frac{{(1+\omega)^{3/2}\sqrt{N}}}{{S\epsilon^2}}+\frac{{(1+\omega)N^{2/3}}}{{S\epsilon^2}})$的通信轮数上限，在凸设置下，COFIG收敛。 |
| [^186] | [On the Fairness of Machine-Assisted Human Decisions.](http://arxiv.org/abs/2110.15310) | 本研究通过形式模型和实验室实验考察了机器预测的性质如何影响人类最终决策。实验发现，包含有偏见的人类决策者可能逆转算法结构与决策质量之间的关系，并且排除受保护群体信息可能无法减少差异甚至可能增加差异。 |
| [^187] | [Tangent Space and Dimension Estimation with the Wasserstein Distance.](http://arxiv.org/abs/2110.06357) | 本文使用局部主成分分析算法，通过数学严格的边界估计了维度和切空间的采样点要求，并同步考虑了噪声非均匀数据分布和变化噪声。所有边界中的常数都有明确描述。 |
| [^188] | [BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter.](http://arxiv.org/abs/2105.01331) | 本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。 |
| [^189] | [Algorithmic Solution for Systems of Linear Equations, in $\mathcal{O}(mn)$ time.](http://arxiv.org/abs/2104.12570) | 该论文提出了一种以$\mathcal{O}(mn)$时间复杂度求解线性方程组的新算法，具有快速、向量化、低内存分配需求等特点，并在非方阵线性方程组中表现出较高的准确性与效率。算法收敛性得到理论证明，同时还使用这一算法解决了特征选择问题。 |
| [^190] | [Attention-Based Multimodal Image Matching.](http://arxiv.org/abs/2103.11247) | 提出了一种基于注意力的方法用于多模态图像块匹配，通过Transformer编码器聚合多尺度图像特征，实现了任务特定的外观不变性。在多个基准测试中取得了最新的最高精度，证明了方法的通用性。 |
| [^191] | [Enabling Binary Neural Network Training on the Edge.](http://arxiv.org/abs/2102.04270) | 本文研究了在边缘设备上实现二值神经网络训练的方法，并展示了通过量化反向传播操作实现了显著的内存占用减少，同时几乎没有降低准确率。 |
| [^192] | [Deep Controlled Learning for Inventory Control.](http://arxiv.org/abs/2011.15122) | 本论文提出了一种名为深度控制学习（DCL）的新型深度强化学习框架，针对库存控制问题进行了量身定制，并通过比较评估表明，在各种测试情况下，DCL相对于传统算法和最先进的启发式算法，在成本和最优性方面都取得了更好的表现。 |
| [^193] | [Decision-making for Autonomous Vehicles on Highway: Deep Reinforcement Learning with Continuous Action Horizon.](http://arxiv.org/abs/2008.11852) | 本文提出了一种基于深度强化学习的决策策略，用于自主车辆在高速公路上的连续视角决策问题。该方法通过引入近端策略优化的算法，实现了高学习效率和优秀的控制性能。该策略从多个角度进行评估，并展示了在类似驾驶场景中的在线应用潜力。 |
| [^194] | [Brainstorming Generative Adversarial Networks (BGANs): Towards Multi-Agent Generative Models with Distributed Private Datasets.](http://arxiv.org/abs/2002.00306) | 提出了一种新颖的脑力风暴生成对抗网络（BGAN）架构，实现多个代理在完全分布式的方式下生成类似真实数据的样本，解决了多个代理共享有限且分布式数据集的问题。 |

# 详细

[^1]: 机器学习模型对飞机发动机预测维修的可解释性系统综述

    An Interpretable Systematic Review of Machine Learning Models for Predictive Maintenance of Aircraft Engine. (arXiv:2309.13310v1 [cs.LG])

    [http://arxiv.org/abs/2309.13310](http://arxiv.org/abs/2309.13310)

    本文对机器学习模型预测飞机发动机维修情况的可解释性进行了综述，并通过使用不同的方法和分析，取得了较高的预测准确率。

    

    本文提出了一种可解释性的综述，对多种机器学习和深度学习模型进行了研究，以预测飞机发动机的维修情况，以避免任何灾难的发生。该策略的优点之一是可以使用较小的数据集。在本研究中，利用传感器数据使用LSTM、Bi-LSTM、RNN、Bi-RNN、GRU、随机森林、KNN、朴素贝叶斯和梯度提升等方法进行飞机发动机故障预测。我们解释了如何使用深度学习和机器学习来生成预测模型，仅使用一个数据源的简单情况。我们使用lime来解释模型，以帮助我们理解为什么机器学习模型表现不如深度学习模型。对多个测试数据进行了广泛的模型行为分析，以理解模型黑盒情况。GRU、Bi-LSTM和LSTM分别实现了97.8%、97.14%和96.42%的较高准确率。

    This paper presents an interpretable review of various machine learning and deep learning models to predict the maintenance of aircraft engine to avoid any kind of disaster. One of the advantages of the strategy is that it can work with modest datasets. In this study, sensor data is utilized to predict aircraft engine failure within a predetermined number of cycles using LSTM, Bi-LSTM, RNN, Bi-RNN GRU, Random Forest, KNN, Naive Bayes, and Gradient Boosting. We explain how deep learning and machine learning can be used to generate predictions in predictive maintenance using a straightforward scenario with just one data source. We applied lime to the models to help us understand why machine learning models did not perform well than deep learning models. An extensive analysis of the model's behavior is presented for several test data to understand the black box scenario of the models. A lucrative accuracy of 97.8%, 97.14%, and 96.42% are achieved by GRU, Bi-LSTM, and LSTM respectively whi
    
[^2]: CORE: 可证明低通信复杂度的分布式优化中的通用随机重构

    CORE: Common Random Reconstruction for Distributed Optimization with Provable Low Communication Complexity. (arXiv:2309.13307v1 [cs.LG])

    [http://arxiv.org/abs/2309.13307](http://arxiv.org/abs/2309.13307)

    提出了一种名为CORE的新技术，可以在分布式机器学习中通过投影和重构信息来减少通信复杂度，从而实现更高效的训练和扩展性。

    

    随着分布式机器学习成为大规模机器学习任务的重要技术，通信复杂度已成为加速训练和扩展机器数量的主要瓶颈。本文提出了一种名为CORE的新技术，它可以用于在没有其他严格条件的情况下压缩机器之间传输的信息，以减少通信复杂度。特别地，我们的技术将向量值信息通过共同随机向量投影到低维空间，并在通信后使用相同的随机噪声对信息进行重构。我们将CORE应用于两个分布式任务，分别是线性模型上的凸优化和通用非凸优化，并设计了新的分布式算法，实现了可证明的更低通信复杂度。例如，我们证明了对于线性模型，基于CORE的算法可以将梯度向量编码为O(1)位。

    With distributed machine learning being a prominent technique for large-scale machine learning tasks, communication complexity has become a major bottleneck for speeding up training and scaling up machine numbers. In this paper, we propose a new technique named Common randOm REconstruction(CORE), which can be used to compress the information transmitted between machines in order to reduce communication complexity without other strict conditions. Especially, our technique CORE projects the vector-valued information to a low-dimensional one through common random vectors and reconstructs the information with the same random noises after communication. We apply CORE to two distributed tasks, respectively convex optimization on linear models and generic non-convex optimization, and design new distributed algorithms, which achieve provably lower communication complexities. For example, we show for linear models CORE-based algorithm can encode the gradient vector to $\mathcal{O}(1)$-bits (aga
    
[^3]: C$^2$VAE：基于高斯Copula的VAE与对比后验的非耦合与非耦合表示有差异的联合学习

    C$^2$VAE: Gaussian Copula-based VAE Differing Disentangled from Coupled Representations with Contrastive Posterior. (arXiv:2309.13303v1 [cs.LG])

    [http://arxiv.org/abs/2309.13303](http://arxiv.org/abs/2309.13303)

    这篇论文提出了一种C$^2$VAE模型，通过联合学习非耦合且相关的隐藏因素，并通过自监督分类器消除耦合表示，以增强非耦合表示学习。该模型在不依赖先验知识和强建模假设的情况下，使用总相关驱动分解后验来学习因子化的非耦合表示，并利用神经高斯Copula模型提取隐藏特征之间的依赖关系来获得耦合表示。

    

    我们提出了一个自监督变分自动编码器（VAE），以联合学习非耦合且相关的隐藏因素，然后通过自监督分类器增强非耦合表示学习，以对比方式消除耦合表示。为此，引入了一种无需依赖先验知识和在神经架构中涉及后验的强建模假设的对比Copula VAE（C$^2$VAE）。C$^2$VAE使用总相关（TC）驱动分解来因子化后验（ELBO），以学习因子化的非耦合表示，并通过神经高斯Copula提取隐藏特征之间的依赖关系以获得耦合表示。然后，自监督对比分类器区分非耦合表示和耦合表示，其中对比损失用于正则化该对比分类。

    We present a self-supervised variational autoencoder (VAE) to jointly learn disentangled and dependent hidden factors and then enhance disentangled representation learning by a self-supervised classifier to eliminate coupled representations in a contrastive manner. To this end, a Contrastive Copula VAE (C$^2$VAE) is introduced without relying on prior knowledge about data in the probabilistic principle and involving strong modeling assumptions on the posterior in the neural architecture. C$^2$VAE simultaneously factorizes the posterior (evidence lower bound, ELBO) with total correlation (TC)-driven decomposition for learning factorized disentangled representations and extracts the dependencies between hidden features by a neural Gaussian copula for copula coupled representations. Then, a self-supervised contrastive classifier differentiates the disentangled representations from the coupled representations, where a contrastive loss regularizes this contrastive classification together wi
    
[^4]: 分布偏移感知的强化学习非同策略区间估计方法：一个统一的误差量化框架

    Distributional Shift-Aware Off-Policy Interval Estimation: A Unified Error Quantification Framework. (arXiv:2309.13278v1 [stat.ML])

    [http://arxiv.org/abs/2309.13278](http://arxiv.org/abs/2309.13278)

    本研究提出了一个对于强化学习非同策略评估问题的统一误差量化框架，并解决了分布偏移的挑战。通过在一个单一的区间内共同量化两个估计误差源，该框架揭示了之前隐藏的误差权衡，从而提高了置信区间的准确性。

    

    本文研究了在无限时间马尔可夫决策过程中的高置信度非同策略评估问题，其目标是仅利用从未知行为策略预先收集的离线数据为目标策略的值建立一个置信区间（CI）。该任务面临两个主要挑战：在CI估计中提供全面且严格的误差量化，并解决由目标策略产生的分布偏移问题，该分布与离线数据生成过程之间存在差异。受到创新的统一误差分析的启发，我们在一个单一的区间内共同量化两个估计误差来源：在建模边际化重要性权重时的规范不准确误差和抽样导致的统计不确定性。这一统一的框架揭示了误差之间以前隐藏的权衡，从而削弱了CI的紧密性。通过依靠精心设计的判别函数，提出了一种新的解决方案来克服分布偏移问题。

    We study high-confidence off-policy evaluation in the context of infinite-horizon Markov decision processes, where the objective is to establish a confidence interval (CI) for the target policy value using only offline data pre-collected from unknown behavior policies. This task faces two primary challenges: providing a comprehensive and rigorous error quantification in CI estimation, and addressing the distributional shift that results from discrepancies between the distribution induced by the target policy and the offline data-generating process. Motivated by an innovative unified error analysis, we jointly quantify the two sources of estimation errors: the misspecification error on modeling marginalized importance weights and the statistical uncertainty due to sampling, within a single interval. This unified framework reveals a previously hidden tradeoff between the errors, which undermines the tightness of the CI. Relying on a carefully designed discriminator function, the proposed
    
[^5]: 跨领域适应和泛化的有序一致性正则化

    Order-preserving Consistency Regularization for Domain Adaptation and Generalization. (arXiv:2309.13258v1 [cs.CV])

    [http://arxiv.org/abs/2309.13258](http://arxiv.org/abs/2309.13258)

    提出了一种适用于跨领域任务的有序一致性正则化（OCR），通过保持预测的有序性，使模型对于特定领域的属性具有鲁棒性，并在多个跨领域任务上取得了明显的优势。

    

    如果深度学习模型对特定领域的属性（如光线、背景、相机角度等）过于敏感，那么在跨领域挑战中，深度学习模型会失败。为了解决这个问题，通常采用数据增强和一致性正则化的方法，使模型对特定领域的属性不那么敏感。一致性正则化强制模型对同一图像的两个视角输出相同的表示或预测。然而，这些约束对于分类概率来说要么过于严格，要么不具有有序性。在这项工作中，我们提出了适用于跨领域任务的有序一致性正则化（OCR）。预测的有序性使模型对于任务无关的变换具有鲁棒性。结果，模型对特定领域的属性不那么敏感。综合实验表明，我们的方法在五个不同的跨领域任务上具有明显的优势。

    Deep learning models fail on cross-domain challenges if the model is oversensitive to domain-specific attributes, e.g., lightning, background, camera angle, etc. To alleviate this problem, data augmentation coupled with consistency regularization are commonly adopted to make the model less sensitive to domain-specific attributes. Consistency regularization enforces the model to output the same representation or prediction for two views of one image. These constraints, however, are either too strict or not order-preserving for the classification probabilities. In this work, we propose the Order-preserving Consistency Regularization (OCR) for cross-domain tasks. The order-preserving property for the prediction makes the model robust to task-irrelevant transformations. As a result, the model becomes less sensitive to the domain-specific attributes. The comprehensive experiments show that our method achieves clear advantages on five different cross-domain tasks.
    
[^6]: 针对预训练语言模型的少样本学习者的反向攻击防御

    Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks. (arXiv:2309.13256v1 [cs.LG])

    [http://arxiv.org/abs/2309.13256](http://arxiv.org/abs/2309.13256)

    本研究针对预训练语言模型作为少样本学习者的安全风险进行了初步研究，发现其极易受到反向攻击。为了应对这个问题，我们提出了一种名为MDP的轻量级、可插拔且有效的防御方法，通过利用被污染样本和清洁样本之间的掩码敏感性差距来识别污染样本。实证评估结果表明，MDP在攻击效果和检测逃避性之间形成了进退两难。

    

    预训练语言模型（PLM）作为少样本学习者展示出了卓越的性能。然而，在这种设置下，它们的安全风险尚未得到广泛探究。在这项工作中，我们进行了一项初步研究，表明少样本学习者的PLMs极易受到反向攻击，而现有的防御措施由于少样本情境的独特挑战而不足。为了应对这些挑战，我们提出了MDP，一种新颖、轻量级、可插拔且有效的预训练语言模型少样本学习者的防御方法。具体而言，MDP利用了被污染样本和清洁样本之间的掩码敏感性差距：参考有限的少样本数据作为分布锚点，它比较不同掩码下给定样本的表示，并识别出具有显著变化的被污染样本。我们通过分析表明，MDP对于攻击者在攻击效果和检测逃避性之间产生了有趣的进退两难。实证评估使用be

    Pre-trained language models (PLMs) have demonstrated remarkable performance as few-shot learners. However, their security risks under such settings are largely unexplored. In this work, we conduct a pilot study showing that PLMs as few-shot learners are highly vulnerable to backdoor attacks while existing defenses are inadequate due to the unique challenges of few-shot scenarios. To address such challenges, we advocate MDP, a novel lightweight, pluggable, and effective defense for PLMs as few-shot learners. Specifically, MDP leverages the gap between the masking-sensitivity of poisoned and clean samples: with reference to the limited few-shot data as distributional anchors, it compares the representations of given samples under varying masking and identifies poisoned samples as ones with significant variations. We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness. The empirical evaluation using be
    
[^7]: Zen：用于分布式DNN训练的近似最优稀疏张量同步

    Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training. (arXiv:2309.13254v1 [cs.LG])

    [http://arxiv.org/abs/2309.13254](http://arxiv.org/abs/2309.13254)

    这篇论文介绍了Zen，一种用于分布式DNN训练中近似最优稀疏张量同步的方法。通过分析流行的DNN模型中稀疏张量的特性，并系统地探索设计空间，找到了最佳的通信方案。通过减少通信流量和提高训练效率，Zen有效地提升了分布式训练的性能。

    

    分布式训练是使用多个GPU扩展深度神经网络(DNN)训练的事实标准。分布式训练的性能瓶颈在于渐变同步的通信。最近，实践者观察到渐变张量中存在稀疏性，表明可以减少通信的流量并提高端到端的训练效率。然而，完全发挥稀疏性的最佳通信方案仍然缺失。本文旨在解决这一问题。我们首先分析了流行DNN模型中稀疏张量的特性，以了解稀疏性的基本原理。然后，我们系统地探索了稀疏张量通信方案的设计空间并找到了最优解。

    Distributed training is the de facto standard to scale up the training of Deep Neural Networks (DNNs) with multiple GPUs. The performance bottleneck of distributed training lies in communications for gradient synchronization. Recently, practitioners have observed sparsity in gradient tensors, suggesting the potential to reduce the traffic volume in communication and improve end-to-end training efficiency. Yet, the optimal communication scheme to fully leverage sparsity is still missing. This paper aims to address this gap. We first analyze the characteristics of sparse tensors in popular DNN models to understand the fundamentals of sparsity. We then systematically explore the design space of communication schemes for sparse tensors and find the optimal one. % We then find the optimal scheme based on the characteristics by systematically exploring the design space. We also develop a gradient synchronization system called Zen that approximately realizes it for sparse tensors. We demonstr
    
[^8]: 我可以相信解释吗？研究可解释机器学习方法在单调模型中的应用

    Can I Trust the Explanations? Investigating Explainable Machine Learning Methods for Monotonic Models. (arXiv:2309.13246v1 [cs.LG])

    [http://arxiv.org/abs/2309.13246](http://arxiv.org/abs/2309.13246)

    本研究研究了可解释机器学习方法在单调模型中的应用，发现了解释的可靠性和模型的单调性之间的关系。

    

    近年来，可解释机器学习方法取得了很大成功。尽管成功，但大多数可解释机器学习方法都是应用于黑盒模型而没有任何领域知识。通过结合领域知识，以科学为基础的机器学习模型展现出更好的泛化和解释性。但是，如果我们将可解释的机器学习方法应用于基于科学知识的机器学习模型，我们能获得一致的科学解释吗？这个问题在展示三种不同类型的单调模型的背景下得到了回答。为了展示单调性，我们提出了三个公理。相应地，这项研究表明，当仅涉及个体单调性时，基准Shapley值提供了良好的解释；然而，当涉及强大的成对单调性时，集成梯度方法在平均上提供了合理的解释。

    In recent years, explainable machine learning methods have been very successful. Despite their success, most explainable machine learning methods are applied to black-box models without any domain knowledge. By incorporating domain knowledge, science-informed machine learning models have demonstrated better generalization and interpretation. But do we obtain consistent scientific explanations if we apply explainable machine learning methods to science-informed machine learning models? This question is addressed in the context of monotonic models that exhibit three different types of monotonicity. To demonstrate monotonicity, we propose three axioms. Accordingly, this study shows that when only individual monotonicity is involved, the baseline Shapley value provides good explanations; however, when strong pairwise monotonicity is involved, the Integrated gradients method provides reasonable explanations on average.
    
[^9]: 弱标签学习中负采样的重要性

    Importance of negative sampling in weak label learning. (arXiv:2309.13227v1 [cs.LG])

    [http://arxiv.org/abs/2309.13227](http://arxiv.org/abs/2309.13227)

    弱标签学习中选择负实例的策略尚未得到广泛研究。本文研究了几种衡量负实例有用性的采样策略，并在实验证明这些策略可以提高弱标签分类性能和降低计算成本。

    

    弱标签学习是一项具有挑战性的任务，它需要从包含正实例和负实例的数据“包”中学习，但只知道包的标签。负实例的数量通常比正实例多，因此选择最具信息价值的负实例对性能至关重要。这个问题的负实例选择策略在弱标签学习中尚未被广泛研究。本文研究了几种采样策略，可以衡量负实例在弱标签学习中的有用性，并相应地选择它们。我们在CIFAR-10和AudioSet数据集上测试了我们的方法，并表明它在弱标签分类性能上有所提升，并减少了与随机采样方法相比的计算成本。我们的工作揭示了负实例并非全都无关紧要，明智地选择负实例可以有益于弱标签学习。

    Weak-label learning is a challenging task that requires learning from data "bags" containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.
    
[^10]: 大规模包裹操纵的拣选计划策略

    Pick Planning Strategies for Large-Scale Package Manipulation. (arXiv:2309.13224v1 [cs.RO])

    [http://arxiv.org/abs/2309.13224](http://arxiv.org/abs/2309.13224)

    本文介绍了亚马逊机器人公司的Robot Induction（Robin）舰队中的大规模包裹操纵，通过使用拣选成功预测器以及训练的拣选质量估计方法，在真实生产系统中进行自动化仓储操作。

    

    自动化仓储操作可以降低物流成本，最终降低消费者的价格，加快交货速度，并增强对市场波动的适应力。本文展示了亚马逊机器人公司的机器人引导（Robin）舰队中的大规模包裹操纵，用于每天拣选和单独处理600万个包裹，并且目前已经处理了20亿个包裹。它描述了随着时间推移开发的各种启发式方法及其后继方法，后继方法利用了在真实生产数据上训练的拣选成功预测器。据作者所知，这项工作是在真实生产系统中首次大规模部署学习的拣选质量估计方法。

    Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to market fluctuations.  This extended abstract showcases a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which is used for picking and singulating up to 6 million packages per day and so far has manipulated over 2 billion packages. It describes the various heuristic methods developed over time and their successor, which utilizes a pick success predictor trained on real production data.  To the best of the authors' knowledge, this work is the first large-scale deployment of learned pick quality estimation methods in a real production system.
    
[^11]: 因果推理：为下一代AI本地化无线网络开辟革命性道路

    Causal Reasoning: Charting a Revolutionary Course for Next-Generation AI-Native Wireless Networks. (arXiv:2309.13223v1 [cs.IT])

    [http://arxiv.org/abs/2309.13223](http://arxiv.org/abs/2309.13223)

    本文提出了一种基于因果推理的框架，用于构建AI本地化的无线网络，以应对现有“AI for wireless”范式的短板。该框架通过解决AI模型的黑匣子特性、曲线拟合特性、对大量训练数据的依赖以及大型神经网络的能量效率低下等问题，克服了数据驱动型、训练密集型AI的局限性。

    

    尽管基本前提是下一代无线网络（例如6G）将是人工智能（AI）本地化的，但到目前为止，大多数现有的工作仍然要么是定性的，要么是对现有“AI用于无线”范式的增量扩展。实际上，创建AI本地化的无线网络面临着重要的技术挑战，因为数据驱动型、训练密集型的AI的局限性。这些限制包括AI模型的黑匣子特性、它们的曲线拟合特性（这可能限制它们的推理和适应能力）、它们对大量训练数据的依赖以及大型神经网络的能量效率低下等。作为对这些限制的回应，本文提出了一个全面的、具有前瞻性的愿景，通过引入一个基于因果推理的新框架来解决这些缺点。该框架基于因果发现、因果表示学习和因果推断。

    Despite the basic premise that next-generation wireless networks (e.g., 6G) will be artificial intelligence (AI)-native, to date, most existing efforts remain either qualitative or incremental extensions to existing ``AI for wireless'' paradigms. Indeed, creating AI-native wireless networks faces significant technical challenges due to the limitations of data-driven, training-intensive AI. These limitations include the black-box nature of the AI models, their curve-fitting nature, which can limit their ability to reason and adapt, their reliance on large amounts of training data, and the energy inefficiency of large neural networks. In response to these limitations, this article presents a comprehensive, forward-looking vision that addresses these shortcomings by introducing a novel framework for building AI-native wireless networks; grounded in the emerging field of causal reasoning. Causal reasoning, founded on causal discovery, causal representation learning, and causal inference, c
    
[^12]: 评估个性对于视频游戏交流中情感状态的影响

    Assessing the Impact of Personality on Affective States from Video Game Communication. (arXiv:2309.13214v1 [cs.AI])

    [http://arxiv.org/abs/2309.13214](http://arxiv.org/abs/2309.13214)

    本研究探讨了个性对团队合作的虚拟现实游戏玩家的情感表达方式的影响。通过分析两周内11名玩家的聊天记录，我们发现了个性变量与情感表达之间的合理相关性，例如较低的自我能力与增加的困惑之间的关系，以及个人烦恼与内在和外在形象问题的增多之间的关系。

    

    个性的个体差异决定了我们的喜好、特征和价值观，这同样适用于我们表达自己的方式。在当前技术和社会的进步和转变中，基于文本的沟通变得普遍，并且通常甚至超过了自然的语音交流，带来了不同的挑战和机遇。在这项探索性工作中，我们研究了个性对基于团队合作的虚拟现实游戏玩家情感表达方式的影响。我们在两周内收集了十一个玩家的聊天记录，根据他们的情感状态进行标记，并评估了它们与五个人格领域和方面之间的关联。在应用多元线性回归之后，我们发现了一系列合理的相关性，即（组合）个性变量与表达的情感之间的关系--例如，较低的自我能力（C1）可以预测增加的困惑，个人烦恼可以预测通过内在和外在形象问题的增多。

    Individual differences in personality determine our preferences, traits and values, which should similarly hold for the way we express ourselves. With current advancements and transformations of technology and society, text-based communication has become ordinary and often even surpasses natural voice conversations -- with distinct challenges and opportunities. In this exploratory work, we investigate the impact of personality on the tendency how players of a team-based collaborative alternate reality game express themselves affectively. We collected chat logs from eleven players over two weeks, labeled them according to their affective state, and assessed the connection between them and the five-factor personality domains and facets. After applying multi-linear regression, we found a series of reasonable correlations between (combinations of) personality variables and expressed affect -- as increased confusion could be predicted by lower self-competence (C1), personal annoyance by vul
    
[^13]: LHCb超快速模拟选项Lamarr：设计与验证

    The LHCb ultra-fast simulation option, Lamarr: design and validation. (arXiv:2309.13213v1 [hep-ex])

    [http://arxiv.org/abs/2309.13213](http://arxiv.org/abs/2309.13213)

    LHCb超快速模拟选项Lamarr是一个基于Gaudi的框架，旨在为LHCb探测器的模拟提供最快速的解决方案，通过参数化探测器响应和重建算法来满足未来大规模模拟数据样本的需求。

    

    在LHCb中，详细的探测器模拟是CPU资源的主要消耗者，在CERN的大型强子对撞机第二阶段运行期间，已经使用了总计超过90％的计算预算。随着LHCb探测器在LHC的第三阶段运行期间收集数据，需要更大规模的模拟数据样本，并且即使存在快速模拟选项，这些需求也将远远超过实验的承诺资源。为了满足分析需求以解释信号与背景以及测量效率的未来需求，必须发展技术和技巧来产生模拟样本。在这种背景下，我们提出了Lamarr，这是一个基于Gaudi的框架，旨在提供最快速的LHCb探测器模拟解决方案。Lamarr由一系列模块组成，这些模块对LHCb实验的探测器响应和重建算法进行参数化。大部分参数化都采用了深度生成模型和梯度提升决策树进行训练。

    Detailed detector simulation is the major consumer of CPU resources at LHCb, having used more than 90% of the total computing budget during Run 2 of the Large Hadron Collider at CERN. As data is collected by the upgraded LHCb detector during Run 3 of the LHC, larger requests for simulated data samples are necessary, and will far exceed the pledged resources of the experiment, even with existing fast simulation options. An evolution of technologies and techniques to produce simulated samples is mandatory to meet the upcoming needs of analysis to interpret signal versus background and measure efficiencies. In this context, we propose Lamarr, a Gaudi-based framework designed to offer the fastest solution for the simulation of the LHCb detector. Lamarr consists of a pipeline of modules parameterizing both the detector response and the reconstruction algorithms of the LHCb experiment. Most of the parameterizations are made of Deep Generative Models and Gradient Boosted Decision Trees traine
    
[^14]: Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])

    Evidential Deep Learning: Enhancing Predictive Uncertainty Estimation for Earth System Science Applications. (arXiv:2309.13207v1 [cs.LG])

    [http://arxiv.org/abs/2309.13207](http://arxiv.org/abs/2309.13207)

    Evidential deep learning extends parametric deep learning to higher-order distributions, enabling the estimation of both aleatoric and epistemic uncertainty with one model. This study shows that evidential deep learning models achieve predictive accuracy comparable to standard methods while robustly quantifying both sources of uncertainty.

    

    预测不确定性的可靠量化对于了解天气和气候结果的驱动因素至关重要。集合提供预测不确定性估计，并且可以进行物理分解，但物理和机器学习集合都需要计算量很大。参数化深度学习可以通过预测概率分布的参数来估计不确定性，但不考虑认识不确定性。证据深度学习是一种将参数化深度学习扩展到高阶分布的技术，可以通过一个模型同时考虑两种不确定性：随机误差和认识误差。本研究比较了从证据神经网络和集合中得到的不确定性。通过对冬季降水类型的分类和地表层通量的回归应用，我们展示了证据深度学习模型达到了与标准方法相媲美的预测准确性，同时可靠地量化这两种来源的不确定性。

    Robust quantification of predictive uncertainty is critical for understanding factors that drive weather and climate outcomes. Ensembles provide predictive uncertainty estimates and can be decomposed physically, but both physics and machine learning ensembles are computationally expensive. Parametric deep learning can estimate uncertainty with one model by predicting the parameters of a probability distribution but do not account for epistemic uncertainty.. Evidential deep learning, a technique that extends parametric deep learning to higher-order distributions, can account for both aleatoric and epistemic uncertainty with one model. This study compares the uncertainty derived from evidential neural networks to those obtained from ensembles. Through applications of classification of winter precipitation type and regression of surface layer fluxes, we show evidential deep learning models attaining predictive accuracy rivaling standard methods, while robustly quantifying both sources of 
    
[^15]: 针对上下文学习的零样本提示设计的实际调查

    A Practical Survey on Zero-shot Prompt Design for In-context Learning. (arXiv:2309.13205v1 [cs.CL])

    [http://arxiv.org/abs/2309.13205](http://arxiv.org/abs/2309.13205)

    本文综述了针对上下文学习的零样本提示设计技术，并探讨了不同类型提示对大型语言模型性能的影响。研究重点讨论了人工设计、优化算法和评价方法等多种提示设计方法，以优化模型在不同任务上的性能。同时，本文强调了考虑多种指标和缺乏单一最佳提示等评估挑战。该研究揭示了提示设计在充分发挥大型语言模型潜力方面的关键作用。

    

    大型语言模型（LLM）的显著进展在自然语言处理（NLP）任务中带来了显著的改进。本文对上下文学习技术进行了综合回顾，重点关注不同类型的提示，包括离散、连续、少样本和零样本，并探讨它们对LLM性能的影响。我们探索了各种提示设计方法，如人工设计、优化算法和评价方法，以优化LLM在各种任务中的性能。我们的回顾涵盖了提示工程领域的关键研究，讨论了其方法论和对该领域的贡献。我们还深入探讨了在评估提示性能方面面临的挑战，包括缺乏单一的"最佳"提示和考虑多个指标的重要性。总之，本文强调了提示设计在发挥LLM的全部潜力中的关键作用，并提供了关于人工设计、优化算法和评价方法结合的见解。

    The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single "best" prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, opt
    
[^16]: 基于个性化层的异质客户联邦短期负载预测

    Federated Short-Term Load Forecasting with Personalization Layers for Heterogeneous Clients. (arXiv:2309.13194v1 [cs.LG])

    [http://arxiv.org/abs/2309.13194](http://arxiv.org/abs/2309.13194)

    本文介绍了一种基于个性化层的联邦短期负载预测方法，该方法通过在联邦学习框架中训练特定层时仅使用客户的数据来缓解异质性数据带来的训练模型质量下降问题。通过实验证明，采用个性化层训练的模型具有优越的性能，在保证隐私的同时实现了良好的预测效果。

    

    智能电表的出现为训练短期负载预测(STLF)模型提供了广泛的能源消耗数据收集。为了回应隐私关切，提出了联邦学习(FL)作为一种保护隐私的训练方法，然而随着客户数据变得异质化，训练模型的质量会降低。本文通过使用个性化层来缓解这个缺点，在FL框架中训练STLF模型的特定层仅由客户自己的数据进行训练。为此，我们提出了一种个性化联邦学习算法(PL-FL)，使FL能够处理个性化层。PL-FL算法是通过使用Argonne隐私保护联邦学习软件包实现的。我们使用包含多个商业建筑异质能源消耗数据的NREL ComStock数据集测试了模型的预测性能。通过PL-FL训练的模型表现出了优越的性能，证明个性化层使得个性化联邦学习成为可能。

    The advent of smart meters has enabled pervasive collection of energy consumption data for training short-term load forecasting (STLF) models. In response to privacy concerns, federated learning (FL) has been proposed as a privacy-preserving approach for training, but the quality of trained models degrades as client data becomes heterogeneous. In this paper we alleviate this drawback using personalization layers, wherein certain layers of an STLF model in an FL framework are trained exclusively on the clients' own data. To that end, we propose a personalized FL algorithm (PL-FL) enabling FL to handle personalization layers. The PL-FL algorithm is implemented by using the Argonne Privacy-Preserving Federated Learning package. We test the forecast performance of models trained on the NREL ComStock dataset, which contains heterogeneous energy consumption data of multiple commercial buildings. Superior performance of models trained with PL-FL demonstrates that personalization layers enable
    
[^17]: 通过自适应反向传播实现大型语言模型的绿色AI细调

    Towards Green AI in Fine-tuning Large Language Models via Adaptive Backpropagation. (arXiv:2309.13192v1 [cs.LG])

    [http://arxiv.org/abs/2309.13192](http://arxiv.org/abs/2309.13192)

    本文提出了GreenTrainer，一种新的LLM细调技术，通过自适应评估不同张量的反向传播成本和对细调模型准确性的贡献，以实现绿色AI。

    

    细调是将预训练的大型语言模型（LLMs）适应到下游应用中最有效的方法。随着LLM驱动的AI应用的快速增长以及开源LLM的民主化，非专业人员也可以进行细调，但是全球范围内对LLM的大规模细调可能导致能源消耗和碳足迹显著增加，从而对环境产生重大影响。实现绿色AI以减少细调的FLOPs直接相关，但是现有的高效LLM细调技术只能实现有限的FLOPs降低，因为它们忽视了细调中的反向传播成本。为了解决这个限制，本文提出了GreenTrainer，一种新的LLM细调技术，通过自适应评估不同张量的反向传播成本和对细调模型准确性的贡献，通过选择最有效的张量来最小化细调成本。

    Fine-tuning is the most effective way of adapting pre-trained large language models (LLMs) to downstream applications. With the fast growth of LLM-enabled AI applications and democratization of open-souced LLMs, fine-tuning has become possible for non-expert individuals, but intensively performed LLM fine-tuning worldwide could result in significantly high energy consumption and carbon footprint, which may bring large environmental impact. Mitigating such environmental impact towards Green AI directly correlates to reducing the FLOPs of fine-tuning, but existing techniques on efficient LLM fine-tuning can only achieve limited reduction of such FLOPs, due to their ignorance of the backpropagation cost in fine-tuning. To address this limitation, in this paper we present GreenTrainer, a new LLM fine-tuning technique that adaptively evaluates different tensors' backpropagation costs and contributions to the fine-tuned model accuracy, to minimize the fine-tuning cost by selecting the most a
    
[^18]: 空间频率通道、形状偏倚和对抗鲁棒性

    Spatial-frequency channels, shape bias, and adversarial robustness. (arXiv:2309.13190v1 [cs.LG])

    [http://arxiv.org/abs/2309.13190](http://arxiv.org/abs/2309.13190)

    本研究通过临界带屏蔽实验揭示了人类物体识别和神经网络物体识别在空间频率通道上的差异，发现人类在自然图像中识别物体所使用的通道与字母和光栅识别相同，而神经网络具有不同的频率通道和鲁棒性。

    

    人类和神经网络在识别物体时使用了哪些空间频率信息？在神经科学中，临界带屏蔽是一种常用的工具，可以揭示用于物体识别的频率选择性滤波器。临界带屏蔽测量了识别性能对不同空间频率添加噪声的敏感度。现有的临界带屏蔽研究表明，人类通过空间频率滤波器（或“通道”）识别周期性模式（光栅）和字母，该滤波器的频带宽度为一个八度（频率翻倍）。在这里，我们将临界带屏蔽引入为网络与人类对比的任务，并在窄带噪声存在下测试了14名人类和76个神经网络在16路ImageNet分类上的表现。我们发现人类使用与字母和光栅相同的一个八度宽的通道来识别自然图像中的物体，使其成为人类物体识别的一个典型特征。另一方面，神经网络使用了不同的频率通道，在不同的空间频率中具有不同的鲁棒性。

    What spatial frequency information do humans and neural networks use to recognize objects? In neuroscience, critical band masking is an established tool that can reveal the frequency-selective filters used for object recognition. Critical band masking measures the sensitivity of recognition performance to noise added at each spatial frequency. Existing critical band masking studies show that humans recognize periodic patterns (gratings) and letters by means of a spatial-frequency filter (or "channel'') that has a frequency bandwidth of one octave (doubling of frequency). Here, we introduce critical band masking as a task for network-human comparison and test 14 humans and 76 neural networks on 16-way ImageNet categorization in the presence of narrowband noise. We find that humans recognize objects in natural images using the same one-octave-wide channel that they use for letters and gratings, making it a canonical feature of human object recognition. On the other hand, the neural netwo
    
[^19]: 基于遮盖的判别器用于内容一致性不配对的图像转换

    Masked Discriminators for Content-Consistent Unpaired Image-to-Image Translation. (arXiv:2309.13188v1 [cs.CV])

    [http://arxiv.org/abs/2309.13188](http://arxiv.org/abs/2309.13188)

    本论文提出了一种基于遮盖的判别器方法，用于减少无配对图像转换中的内容不一致性。通过在两个域的全局判别器上使用基于内容的遮罩，可以显著降低不一致性。同时，引入了局部判别器和相似性采样策略，以减少由于遮盖过程引起的伪影。

    

    无配对的图像转换的一个共同目标是在模仿目标域的风格的同时保持源图像和转换后图像之间的内容一致性。由于两个域的数据集之间存在偏差，许多方法在转换过程中会产生不一致性。大多数方法用于缓解这些不一致性的方法没有对判别器进行限制，导致训练设置更加无法确定。此外，这些方法都不适用于更大的裁剪尺寸。在这项工作中，我们展示了通过使用基于内容的遮罩对两个域的全局判别器的输入进行遮盖可以显著减少内容不一致性。然而，这种策略会导致可以追溯到遮罩过程的伪影。为了减少这些伪影，我们引入了一种在使用相似性采样策略选择的小裁剪对上操作的局部判别器。此外，我们还将这种采样策略应用于全局采样。

    A common goal of unpaired image-to-image translation is to preserve content consistency between source images and translated images while mimicking the style of the target domain. Due to biases between the datasets of both domains, many methods suffer from inconsistencies caused by the translation process. Most approaches introduced to mitigate these inconsistencies do not constrain the discriminator, leading to an even more ill-posed training setup. Moreover, none of these approaches is designed for larger crop sizes. In this work, we show that masking the inputs of a global discriminator for both domains with a content-based mask is sufficient to reduce content inconsistencies significantly. However, this strategy leads to artifacts that can be traced back to the masking process. To reduce these artifacts, we introduce a local discriminator that operates on pairs of small crops selected with a similarity sampling strategy. Furthermore, we apply this sampling strategy to sample global
    
[^20]: 可视化拓扑重要性：一种面向类别的方法

    Visualizing Topological Importance: A Class-Driven Approach. (arXiv:2309.13185v1 [cs.LG])

    [http://arxiv.org/abs/2309.13185](http://arxiv.org/abs/2309.13185)

    本文提出了一种可视化拓扑重要性的方法，能够揭示与类别标签相关的每个数据集中重要的拓扑结构。

    

    本文提出了一种可视化定义数据类别的拓扑特征重要性的方法。拓扑特征能够抽象复杂数据的基本结构，在可视化和分析过程中是不可或缺的组成部分。然而，并非所有的拓扑特征都具有相等的重要性。迄今为止，对于特征重要性的默认定义往往是固定和假设的。本文展示了如何利用可解释的深度学习方法来适应拓扑分类，并提供了第一种方法来揭示与类别标签相关的每个数据集中重要的拓扑结构。具体而言，该方法使用一个学习的度量分类器，并以持续图的点的密度估计作为输入。这个度量学习如何重新加权密度以使分类准确率高。通过提取这个权重，得到了持续图上的一个重要性场。

    This paper presents the first approach to visualize the importance of topological features that define classes of data. Topological features, with their ability to abstract the fundamental structure of complex data, are an integral component of visualization and analysis pipelines. Although not all topological features present in data are of equal importance. To date, the default definition of feature importance is often assumed and fixed. This work shows how proven explainable deep learning approaches can be adapted for use in topological classification. In doing so, it provides the first technique that illuminates what topological structures are important in each dataset in regards to their class label. In particular, the approach uses a learned metric classifier with a density estimator of the points of a persistence diagram as input. This metric learns how to reweigh this density such that classification accuracy is high. By extracting this weight, an importance field on persistent
    
[^21]: 分析和利用视频游戏对深度强化学习的计算需求

    Diagnosing and exploiting the computational demands of videos games for deep reinforcement learning. (arXiv:2309.13181v1 [cs.LG])

    [http://arxiv.org/abs/2309.13181](http://arxiv.org/abs/2309.13181)

    本研究通过引入学习挑战诊断器(LCD)来分析视频游戏对深度强化学习的计算需求，并在Procgen基准测试中发现新的挑战分类。结果表明，LCD的预测可靠且能指导算法的发展。

    

    人类通过与环境互动并感知行动结果来学习。深度强化学习算法在视频游戏中能够实现与人类相媲美甚至更好的表现，这在人工智能领域是一个里程碑。然而，目前还不清楚深度强化学习模型成功的原因是视觉表示学习的进步，还是强化学习算法发现更好策略的有效性，或者两者兼具。为了解决这个问题，我们引入了学习挑战诊断器（LCD），这是一种能够单独测量任务中感知和强化学习需求的工具。我们使用LCD在Procgen基准测试中发现了一种新的挑战分类，并证明这些预测既高度可靠，又能指导算法的发展。更广泛地讲，LCD揭示了在像P这样的整个视频游戏基准测试中优化深度强化学习算法时可能出现的多种失败情况。

    Humans learn by interacting with their environments and perceiving the outcomes of their actions. A landmark in artificial intelligence has been the development of deep reinforcement learning (dRL) algorithms capable of doing the same in video games, on par with or better than humans. However, it remains unclear whether the successes of dRL models reflect advances in visual representation learning, the effectiveness of reinforcement learning algorithms at discovering better policies, or both. To address this question, we introduce the Learning Challenge Diagnosticator (LCD), a tool that separately measures the perceptual and reinforcement learning demands of a task. We use LCD to discover a novel taxonomy of challenges in the Procgen benchmark, and demonstrate that these predictions are both highly reliable and can instruct algorithmic development. More broadly, the LCD reveals multiple failure cases that can occur when optimizing dRL algorithms over entire video game benchmarks like P
    
[^22]: 通过机器学习支持的多物理仿真增强多目标优化

    Enhancing Multi-Objective Optimization through Machine Learning-Supported Multiphysics Simulation. (arXiv:2309.13179v1 [cs.LG])

    [http://arxiv.org/abs/2309.13179](http://arxiv.org/abs/2309.13179)

    通过机器学习支持的多物理仿真加速多目标优化，提出了一种框架来逼近和加速复杂的多物理仿真，并在实验中展示了其有效性。

    

    多物理仿真涉及多个耦合物理现象，很快变得计算复杂。这给寻找满足多个目标的最优配置带来了挑战，因为优化算法通常需要多次查询仿真。本文提出了一种方法论框架，通过训练、自优化和自组织代理模型来逼近和加速多物理仿真。我们生成了两个真实世界的表格数据集，并公开提供，展示了代理模型可以在相对少量的数据上准确逼近底层仿真。我们结合四种机器学习和深度学习算法以及两种优化算法和综合评估策略进行了大量实验。最后，通过验证生成的帕累托最优集，评估了我们的结合训练和优化流程的性能。

    Multiphysics simulations that involve multiple coupled physical phenomena quickly become computationally expensive. This imposes challenges for practitioners aiming to find optimal configurations for these problems satisfying multiple objectives, as optimization algorithms often require querying the simulation many times. This paper presents a methodological framework for training, self-optimizing, and self-organizing surrogate models to approximate and speed up Multiphysics simulations. We generate two real-world tabular datasets, which we make publicly available, and show that surrogate models can be trained on relatively small amounts of data to approximate the underlying simulations accurately. We conduct extensive experiments combining four machine learning and deep learning algorithms with two optimization algorithms and a comprehensive evaluation strategy. Finally, we evaluate the performance of our combined training and optimization pipeline by verifying the generated Pareto-op
    
[^23]: 流动因式表示学习

    Flow Factorized Representation Learning. (arXiv:2309.13167v1 [cs.LG])

    [http://arxiv.org/abs/2309.13167](http://arxiv.org/abs/2309.13167)

    本文提出了一种称为流动因式表示学习的新观点，并展示了比现有框架更高效更有用的结构化表示的学习方法。

    

    表示学习研究的一个重要目标是实现与真实因素变化有关的有用因式化表示。区分嵌入和等变表示学习领域从不同的角度接近了这个理想；然而，迄今为止，大多数方法都被证明要么规定不明确，要么灵活性不足，不能有效地将所有感兴趣的因素在学习的潜在空间中分离开来。在这项工作中，我们提出了一种称为流动因式表示学习的结构化表示学习的替代观点，并展示了该方法学习到的表示比现有框架更高效、更有用的结构。具体而言，我们引入了一个生成模型，该模型指定了一组不同输入变换的潜在概率路径。每个潜在流是由一个学习的势函数的梯度场生成的。

    A prominent goal of representation learning research is to achieve representations which are factorized in a useful manner with respect to the ground truth factors of variation. The fields of disentangled and equivariant representation learning have approached this ideal from a range of complimentary perspectives; however, to date, most approaches have proven to either be ill-specified or insufficiently flexible to effectively separate all realistic factors of interest in a learned latent space. In this work, we propose an alternative viewpoint on such structured representation learning which we call Flow Factorized Representation Learning, and demonstrate it to learn both more efficient and more usefully structured representations than existing frameworks. Specifically, we introduce a generative model which specifies a distinct set of latent probability paths that define different input transformations. Each latent flow is generated by the gradient field of a learned potential followi
    
[^24]: 用于音频生成扩散模型的隐形数字水印

    Invisible Watermarking for Audio Generation Diffusion Models. (arXiv:2309.13166v1 [cs.SD])

    [http://arxiv.org/abs/2309.13166](http://arxiv.org/abs/2309.13166)

    该论文提出了一种用于音频扩散模型的隐形数字水印技术，在保证正常音频生成的同时，还能为模型验证提供保护层，用于鉴别模型所有权和维护其完整性。

    

    在图像领域中，由于其数据生成和转换的能力，扩散模型在各种任务中取得了最先进的性能，因此在图像和音频领域都备受重视。在迅速发展的音频机器学习领域，保护模型的完整性和确立数据的版权至关重要。本文提出了一种首次应用于训练在mel频谱图上的音频扩散模型的水印技术，这对上述挑战提供了一种新颖的方法。我们的模型不仅在正常音频生成方面表现出色，而且还引入了一个不可见的水印触发机制来进行模型验证。这个水印触发器作为一种保护层，能够识别模型的所有者并确保其完整性。通过大量实验证明，不可见的水印触发器在防止未经授权的修改的同时还能保持高效的合法音频传输。

    Diffusion models have gained prominence in the image domain for their capabilities in data generation and transformation, achieving state-of-the-art performance in various tasks in both image and audio domains. In the rapidly evolving field of audio-based machine learning, safeguarding model integrity and establishing data copyright are of paramount importance. This paper presents the first watermarking technique applied to audio diffusion models trained on mel-spectrograms. This offers a novel approach to the aforementioned challenges. Our model excels not only in benign audio generation, but also incorporates an invisible watermarking trigger mechanism for model verification. This watermark trigger serves as a protective layer, enabling the identification of model ownership and ensuring its integrity. Through extensive experiments, we demonstrate that invisible watermark triggers can effectively protect against unauthorized modifications while maintaining high utility in benign audio
    
[^25]: GAMIX-VAE: 一种基于高斯混合后验的VAE

    GAMIX-VAE: A VAE with Gaussian Mixture Based Posterior. (arXiv:2309.13160v1 [cs.LG])

    [http://arxiv.org/abs/2309.13160](http://arxiv.org/abs/2309.13160)

    本文提出了一种基于高斯混合后验的VAE方法，重新定义了ELBO，引入正则化项和PatchGAN鉴别器，能够生成逼真的人脸。

    

    变分自动编码器（VAEs）已成为机器学习中生成建模和表示学习的基石。本文探讨了VAEs的一个细微方面，重点是解释KL Divergence，这是Evidence Lower Bound（ELBO）中的关键组成部分，它控制了重构准确性和正则化之间的权衡。虽然KL Divergence让潜变量分布与先验分布对齐，给整个潜空间加上结构约束，但却不限制各个变量分布。所提出的方法重新定义了带有高斯混合的后验概率的ELBO，引入了正则化项以防止方差崩溃，并使用PatchGAN鉴别器来增强纹理逼真度。实现细节涉及Encoder和Decoder的ResNetV2架构。实验证明了生成逼真的人脸的能力，为提供了一个有希望的解决方案。

    Variational Autoencoders (VAEs) have become a cornerstone in generative modeling and representation learning within machine learning. This paper explores a nuanced aspect of VAEs, focusing on interpreting the Kullback Leibler (KL) Divergence, a critical component within the Evidence Lower Bound (ELBO) that governs the trade-off between reconstruction accuracy and regularization. While the KL Divergence enforces alignment between latent variable distributions and a prior imposing a structure on the overall latent space but leaves individual variable distributions unconstrained. The proposed method redefines the ELBO with a mixture of Gaussians for the posterior probability, introduces a regularization term to prevent variance collapse, and employs a PatchGAN discriminator to enhance texture realism. Implementation details involve ResNetV2 architectures for both the Encoder and Decoder. The experiments demonstrate the ability to generate realistic faces, offering a promising solution for
    
[^26]: 像素级平滑用于对抗相机运动扰动的可证明鲁棒性

    Pixel-wise Smoothing for Certified Robustness against Camera Motion Perturbations. (arXiv:2309.13150v1 [cs.LG])

    [http://arxiv.org/abs/2309.13150](http://arxiv.org/abs/2309.13150)

    本文提出了一种用于对抗相机运动扰动的像素级平滑框架，通过在二维像素空间中使用平滑分布来提高鲁棒性认证的效率，并完全上界投影误差。

    

    最近几年，计算机视觉在自动驾驶和机器人领域取得了显著的进展。然而，深度学习的视觉感知模型在面对相机运动扰动时缺乏鲁棒性。目前用于评估鲁棒性的认证过程耗时且昂贵，因为需要在三维相机运动空间中进行蒙特卡洛采样得到大量图像投影。为了应对这些挑战，我们提出了一种新颖、高效且实用的框架，用于证明3D-2D投影变换对抗相机运动扰动的鲁棒性。我们的方法在二维像素空间而非三维物理空间中使用平滑分布，消除了昂贵的相机运动采样，并大大提高了鲁棒性认证的效率。通过像素级平滑分类器，我们能够使用一种均匀分区的技术完全上界投影误差。

    In recent years, computer vision has made remarkable advancements in autonomous driving and robotics. However, it has been observed that deep learning-based visual perception models lack robustness when faced with camera motion perturbations. The current certification process for assessing robustness is costly and time-consuming due to the extensive number of image projections required for Monte Carlo sampling in the 3D camera motion space. To address these challenges, we present a novel, efficient, and practical framework for certifying the robustness of 3D-2D projective transformations against camera motion perturbations. Our approach leverages a smoothing distribution over the 2D pixel space instead of in the 3D physical space, eliminating the need for costly camera motion sampling and significantly enhancing the efficiency of robustness certifications. With the pixel-wise smoothed classifier, we are able to fully upper bound the projection errors using a technique of uniform partit
    
[^27]: 牺牲特征聚合中的互信息用于人脸识别

    Trading-off Mutual Information on Feature Aggregation for Face Recognition. (arXiv:2309.13137v1 [cs.CV])

    [http://arxiv.org/abs/2309.13137](http://arxiv.org/abs/2309.13137)

    本文提出了一种技术，用于聚合ArcFace和AdaFace两个最先进的人脸识别模型的输出，通过利用变换器注意机制来增强整体人脸识别系统的判别能力。同时，改进了自注意机制以有效捕捉局部和全局依赖关系。

    

    尽管人脸识别技术取得了很大进展，但这些方法的准确度还不够。为了提高识别性能，本文提出了一种技术，可以聚合两个最先进的人脸识别模型ArcFace和AdaFace的输出。在我们的方法中，我们利用变换器注意机制来利用两个特征图的不同部分之间的关系。通过这样做，我们的目标是增强整个人脸识别系统的判别能力。特征聚合中的一个挑战是有效建模局部和全局依赖关系。传统的变换器以捕捉长程依赖性而闻名，但却往往难以准确建模局部依赖关系。为了解决这个问题，我们改进了自注意机制，以有效捕捉局部和全局依赖关系。这使得我们的模型能够利用对应位置中的重叠接受域。

    Despite the advances in the field of Face Recognition (FR), the precision of these methods is not yet sufficient. To improve the FR performance, this paper proposes a technique to aggregate the outputs of two state-of-the-art (SOTA) deep FR models, namely ArcFace and AdaFace. In our approach, we leverage the transformer attention mechanism to exploit the relationship between different parts of two feature maps. By doing so, we aim to enhance the overall discriminative power of the FR system. One of the challenges in feature aggregation is the effective modeling of both local and global dependencies. Conventional transformers are known for their ability to capture long-range dependencies, but they often struggle with modeling local dependencies accurately. To address this limitation, we augment the self-attention mechanism to capture both local and global dependencies effectively. This allows our model to take advantage of the overlapping receptive fields present in corresponding locati
    
[^28]: 使用深度学习和药动学先验预测治疗反应

    Forecasting Response to Treatment with Deep Learning and Pharmacokinetic Priors. (arXiv:2309.13135v1 [cs.LG])

    [http://arxiv.org/abs/2309.13135](http://arxiv.org/abs/2309.13135)

    该研究提出了一种使用深度学习和药动学先验预测治疗反应的方法。研究者通过一个新颖的编码器提供药物的药动学信息，从而实现对时间序列的精确预测。实验结果显示，在逼真模拟和真实世界数据上，该方法比基准模型的预测准确性提高了约11%和8%。这种方法在临床实践中具有多种有益应用，如发出早期警告和定量特定患者的治疗效果。

    

    对医疗时间序列的预测对于早期检测不良结果和患者监测至关重要。然而，由于数据嘈杂和间歇性，实际中预测可能很困难。这些挑战通常通过外部因素诱导的变化点（如药物使用）而加剧。我们提出了一种新颖的编码器，以向深度学习模型提供药物的药动学效应信息，从而实现对受治疗影响的时间序列的准确预测。我们展示了我们方法在使用逼真模拟和真实世界数据预测血糖的任务中的有效性。我们的药动学编码器使深度学习模型在模拟数据上超过基准约11％，在真实世界数据上超过8％。所提出的方法可以在临床实践中具有多种有益应用，例如发出关于意外治疗反应的早期警告，或帮助表征特定于患者的治疗效果。

    Forecasting healthcare time series is crucial for early detection of adverse outcomes and for patient monitoring. Forecasting, however, can be difficult in practice due to noisy and intermittent data. The challenges are often exacerbated by change points induced via extrinsic factors, such as the administration of medication. We propose a novel encoder that informs deep learning models of the pharmacokinetic effects of drugs to allow for accurate forecasting of time series affected by treatment. We showcase the effectiveness of our approach in a task to forecast blood glucose using both realistically simulated and real-world data. Our pharmacokinetic encoder helps deep learning models surpass baselines by approximately 11% on simulated data and 8% on real-world data. The proposed approach can have multiple beneficial applications in clinical practice, such as issuing early warnings about unexpected treatment responses, or helping to characterize patient-specific treatment effects in te
    
[^29]: 反BARTy扩散用于基于性质导向的抗体设计

    AntiBARTy Diffusion for Property Guided Antibody Design. (arXiv:2309.13129v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.13129](http://arxiv.org/abs/2309.13129)

    该论文提出了一种基于机器学习的方法AntiBARTy来导向抗体设计，通过训练抗体特异性语言模型，并利用其潜在空间来训练性质条件扩散模型，实现了生成具有改进溶解性的新型抗体的能力。

    

    在过去的十年里，由于其高特异性和低不良反应风险，抗体在治疗中的重要性逐渐增加，与其他药物模式相比。尽管传统的抗体发现主要是湿实验驱动的，但基于机器学习的生成建模快速发展使得基于计算模拟的方法越来越可行。为此，我们训练了一个抗体特异性语言模型AntiBARTy，基于BART (双向和自回归变换器)，并使用其潜在空间训练了一个性质条件扩散模型，以指导IgG新生抗体的设计。作为一个测试案例，我们展示了我们能够有效地生成具有改进的无病毒溶解性的新型抗体，同时保持抗体的有效性和控制序列多样性。

    Over the past decade, antibodies have steadily grown in therapeutic importance thanks to their high specificity and low risk of adverse effects compared to other drug modalities. While traditional antibody discovery is primarily wet lab driven, the rapid improvement of ML-based generative modeling has made in-silico approaches an increasingly viable route for discovery and engineering. To this end, we train an antibody-specific language model, AntiBARTy, based on BART (Bidirectional and Auto-Regressive Transformer) and use its latent space to train a property-conditional diffusion model for guided IgG de novo design. As a test case, we show that we can effectively generate novel antibodies with improved in-silico solubility while maintaining antibody validity and controlling sequence diversity.
    
[^30]: 数据加载通常具有短深度：基于张量网络的金融、图像、流体和蛋白质量子电路

    Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins. (arXiv:2309.13108v1 [quant-ph])

    [http://arxiv.org/abs/2309.13108](http://arxiv.org/abs/2309.13108)

    使用基于张量网络的电路编译方法AMLET，该方法可以解决量子电路加载经典数据的“输入问题”。作者在金融、图像、流体和蛋白质领域进行了广泛的数值实验，并展示了该方法的有效性。

    

    尽管在开发用于研究经典数据集的量子算法方面取得了显著进展，但简单加载经典数据的成本是实现量子优势的障碍。当使用振幅编码时，加载任意经典向量需要与比特数成指数关系的电路深度。在这里，我们通过两个贡献来解决这个“输入问题”。首先，我们引入了一种基于张量网络（TN）理论的电路编译方法。我们的方法——AMLET（自动多层加载器利用TNs）——通过精心构建特定的TN拓扑结构，并可以根据需要调整电路深度。其次，我们对来自金融、图像、流体力学和蛋白质四个不同领域的真实经典数据进行了数值实验。据我们所知，这是迄今为止关于将经典数据加载到量子计算机中的最广泛的数值分析。与这一领域最近的其他工作一致，所需的

    Though there has been substantial progress in developing quantum algorithms to study classical datasets, the cost of simply loading classical data is an obstacle to quantum advantage. When the amplitude encoding is used, loading an arbitrary classical vector requires up to exponential circuit depths with respect to the number of qubits. Here, we address this ``input problem'' with two contributions. First, we introduce a circuit compilation method based on tensor network (TN) theory. Our method -- AMLET (Automatic Multi-layer Loader Exploiting TNs) -- proceeds via careful construction of a specific TN topology and can be tailored to arbitrary circuit depths. Second, we perform numerical experiments on real-world classical data from four distinct areas: finance, images, fluid mechanics, and proteins. To the best of our knowledge, this is the broadest numerical analysis to date of loading classical data into a quantum computer. Consistent with other recent work in this area, the required
    
[^31]: OpportunityFinder：一个自动因果推断的框架

    OpportunityFinder: A Framework for Automated Causal Inference. (arXiv:2309.13103v1 [cs.LG])

    [http://arxiv.org/abs/2309.13103](http://arxiv.org/abs/2309.13103)

    OpportunityFinder是一个无需编码的因果推断框架，可以帮助非专业用户进行各种面板数据的因果推断研究，节省科学家和经济学家的带宽，并提供统计和严格的敏感性和稳健性分析。

    

    我们介绍了OpportunityFinder，一个无需编码的框架，用于为非专业用户进行各种面板数据的因果推断研究。在当前状态下，OpportunityFinder只需要用户提供原始观察数据和配置文件。然后触发一个流水线来检查/处理数据，选择适合的算法来执行因果研究。它返回处理的结果，包括治疗对预期结果的因果影响，以及敏感性和稳健性的结果。因果推断被广泛研究和用于估计个人与产品和特征相互作用的下游影响。通常这些因果研究是由科学家和/或经济学家定期进行的。业务利益相关者通常受限于科学家或经济学家进行因果研究的带宽。我们提供OpportunityFinder作为常见的因果研究的解决方案，具有四个关键特点：（1）易于使用，适用于业务分析师和决策者。(2)由于无编程需求，有效节省了科学家和经济学家的带宽。(3)它是一个端到端的系统，它可以负责数据处理、推断配置、推断评估等所有事情。(4)它提供了关于推断质量的统计和严格的敏感性和稳健性分析。

    We introduce OpportunityFinder, a code-less framework for performing a variety of causal inference studies with panel data for non-expert users. In its current state, OpportunityFinder only requires users to provide raw observational data and a configuration file. A pipeline is then triggered that inspects/processes data, chooses the suitable algorithm(s) to execute the causal study. It returns the causal impact of the treatment on the configured outcome, together with sensitivity and robustness results. Causal inference is widely studied and used to estimate the downstream impact of individual's interactions with products and features. It is common that these causal studies are performed by scientists and/or economists periodically. Business stakeholders are often bottle-necked on scientist or economist bandwidth to conduct causal studies. We offer OpportunityFinder as a solution for commonly performed causal studies with four key features: (1) easy to use for both Business Analysts a
    
[^32]: FL4ASR中优化器引起的平滑性的重要性：理解端到端ASR的联邦学习

    Importance of Smoothness Induced by Optimizers in FL4ASR: Towards Understanding Federated Learning for End-to-End ASR. (arXiv:2309.13102v1 [eess.AS])

    [http://arxiv.org/abs/2309.13102](http://arxiv.org/abs/2309.13102)

    本论文通过使用联邦学习训练端到端ASR模型，研究了优化器对平滑性的重要性，并总结了适用的算法和最佳实践。

    

    本文首先使用联邦学习（FL）训练端到端自动语音识别（ASR）模型，并研究了在最小化FL模型与中心化模型之间的单词错误率性能差距方面，可以起到关键作用的基本考虑因素。具体而言，我们研究了自适应优化器的影响，通过改变连接主义时序分类（CTC）权重来研究损失特征，通过种子起始值对模型进行初始化，通过从中心化训练的经验中延续建模设置到FL中，例如预层或后层归一化，以及针对异质数据分布的ASR的FL专用超参数，如本地epoch数量、客户端采样大小和学习率调度器。我们阐明了一些优化器通过引导平滑性而比其他优化器更好的工作原理。我们还总结了算法的适用性、趋势，并提出了最佳实践。

    In this paper, we start by training End-to-End Automatic Speech Recognition (ASR) models using Federated Learning (FL) and examining the fundamental considerations that can be pivotal in minimizing the performance gap in terms of word error rate between models trained using FL versus their centralized counterpart. Specifically, we study the effect of (i) adaptive optimizers, (ii) loss characteristics via altering Connectionist Temporal Classification (CTC) weight, (iii) model initialization through seed start, (iv) carrying over modeling setup from experiences in centralized training to FL, e.g., pre-layer or post-layer normalization, and (v) FL-specific hyperparameters, such as number of local epochs, client sampling size, and learning rate scheduler, specifically for ASR under heterogeneous data distribution. We shed light on how some optimizers work better than others via inducing smoothness. We also summarize the applicability of algorithms, trends, and propose best practices from 
    
[^33]: 在线仇恨言论、虚假信息和心理健康的拓扑数据映射：一项基于大型语言模型的研究

    Topological Data Mapping of Online Hate Speech, Misinformation, and General Mental Health: A Large Language Model Based Study. (arXiv:2309.13098v1 [cs.LG])

    [http://arxiv.org/abs/2309.13098](http://arxiv.org/abs/2309.13098)

    该研究使用大型语言模型分析了社交媒体上的在线仇恨言论、虚假信息和心理健康之间的关系。

    

    社交媒体的出现引发了人们对其传播仇恨言论和虚假信息的潜力的担忧，除了导致偏见和歧视，还被怀疑在增加美国社会暴力和犯罪方面起到一定作用。尽管文献已经表明在线发布仇恨言论和虚假信息与发帖者的某些人格特征之间存在关联，但在线仇恨言论/虚假信息与发帖者整体心理健康的关系和相关性仍然不明确。其中一个困难在于缺乏适当的数据分析工具，能够足够分析海量的社交媒体帖子以揭示潜在的隐藏联系。近年来，机器学习和大型语言模型（如ChatGPT）的进展使得这种分析成为可能。本研究收集了精心选择的社交媒体平台上的数千条帖子。

    The advent of social media has led to an increased concern over its potential to propagate hate speech and misinformation, which, in addition to contributing to prejudice and discrimination, has been suspected of playing a role in increasing social violence and crimes in the United States. While literature has shown the existence of an association between posting hate speech and misinformation online and certain personality traits of posters, the general relationship and relevance of online hate speech/misinformation in the context of overall psychological wellbeing of posters remain elusive. One difficulty lies in the lack of adequate data analytics tools capable of adequately analyzing the massive amount of social media posts to uncover the underlying hidden links. Recent progresses in machine learning and large language models such as ChatGPT have made such an analysis possible. In this study, we collected thousands of posts from carefully selected communities on the social media si
    
[^34]: 通过多个独立的差分进化优化方法解决库存管理中的需求不确定性和变异性

    Multiple Independent DE Optimizations to Tackle Uncertainty and Variability in Demand in Inventory Management. (arXiv:2309.13095v1 [cs.NE])

    [http://arxiv.org/abs/2309.13095](http://arxiv.org/abs/2309.13095)

    本研究通过比较多种算法，发现差分进化（DE）算法在优化库存管理中表现优异，能有效降低库存成本，特别适用于不确定需求模式的情况下。

    

    为了确定元启发式差分进化优化策略在随机需求情况下对库存管理的有效性，本实证研究进行了深入调查。主要目标是在不确定需求模式的情况下确定最有效的减少库存成本策略。库存成本是指企业持有和管理库存所产生的费用。该方法将连续审查库存管理政策与蒙特卡罗模拟（MCS）相结合。为了找到最优解，本研究专注于元启发式方法，并比较了多个算法。结果表明，差分进化（DE）算法在优化库存管理方面优于其竞争对手。为了调整参数，本研究采用了拉丁超立方采样（LHS）统计方法。为了确定最终解决方案，本研究采用了一种方法，将多个独立优化结果综合起来。

    To determine the effectiveness of metaheuristic Differential Evolution optimization strategy for inventory management (IM) in the context of stochastic demand, this empirical study undertakes a thorough investigation. The primary objective is to discern the most effective strategy for minimizing inventory costs within the context of uncertain demand patterns. Inventory costs refer to the expenses associated with holding and managing inventory within a business. The approach combines a continuous review of IM policies with a Monte Carlo Simulation (MCS). To find the optimal solution, the study focuses on meta-heuristic approaches and compares multiple algorithms. The outcomes reveal that the Differential Evolution (DE) algorithm outperforms its counterparts in optimizing IM. To fine-tune the parameters, the study employs the Latin Hypercube Sampling (LHS) statistical method. To determine the final solution, a method is employed in this study which combines the outcomes of multiple indep
    
[^35]: 增强型原型超图学习在异构信息网络中的应用

    Prototype-Enhanced Hypergraph Learning for Heterogeneous Information Networks. (arXiv:2309.13092v1 [cs.LG])

    [http://arxiv.org/abs/2309.13092](http://arxiv.org/abs/2309.13092)

    该论文提出了一种增强型原型超图学习方法，用于异构信息网络中的节点分类。该方法使用超图捕捉高阶关系并提取语义信息，不依赖于元路径。通过利用原型的强大功能，该方法提高了超图学习的鲁棒性，并向我们揭示了潜在网络结构的可解释性洞察力。

    

    多媒体数据中的关系的多样性和复杂性导致了异构信息网络（HINs）。从这样的网络中捕捉语义需要能够利用HINs的全部丰富性的方法。现有的建模HINs的方法使用的是最初设计用于图神经网络和HINs分解分析的技术，比如使用手动预定义的元路径。本文介绍了一种新颖的、增强原型超图学习方法，用于HINs中的节点分类。我们的方法使用超图而不是图，捕捉节点之间的高阶关系，并提取语义信息，而无需依赖于元路径。我们的方法利用原型的强大功能来提高超图学习过程的鲁棒性，并为揭示潜在的网络结构提供了可解释性的洞察力。对三个真实的HINs进行了大量实验证明了我们的方法的有效性。

    The variety and complexity of relations in multimedia data lead to Heterogeneous Information Networks (HINs). Capturing the semantics from such networks requires approaches capable of utilizing the full richness of the HINs. Existing methods for modeling HINs employ techniques originally designed for graph neural networks, and HINs decomposition analysis, like using manually predefined metapaths. In this paper, we introduce a novel prototype-enhanced hypergraph learning approach for node classification in HINs. Using hypergraphs instead of graphs, our method captures higher-order relationships among nodes and extracts semantic information without relying on metapaths. Our method leverages the power of prototypes to improve the robustness of the hypergraph learning process and creates the potential to provide human-interpretable insights into the underlying network structure. Extensive experiments on three real-world HINs demonstrate the effectiveness of our method.
    
[^36]: 使用便携式拉曼光谱仪进行威士忌鉴别的学习算法

    Learning algorithms for identification of whisky using portable Raman spectroscopy. (arXiv:2309.13087v1 [cs.LG])

    [http://arxiv.org/abs/2309.13087](http://arxiv.org/abs/2309.13087)

    本研究使用机器学习算法和便携式拉曼光谱仪，通过样品的光谱信息进行威士忌的快速鉴别和浓度分析，实现了高精准度的品牌识别和成分检测。

    

    可靠的高价值产品鉴别，如威士忌，是一个越来越重要的领域，因为品牌替换（即欺诈产品）和质量控制对于该行业至关重要。我们研究了一系列机器学习算法，并将其直接与便携式拉曼光谱仪连接，以识别和表征商业威士忌样品中的乙醇/甲醇浓度。我们证明机器学习模型可以在二十八个商业样品中实现超过99％的品牌识别准确率。为了展示这种方法的灵活性，我们使用相同的样品和算法来量化乙醇浓度，并测量掺入威士忌样品中的甲醇水平。然后，我们将我们的机器学习技术与透瓶法结合起来，进行光谱分析和鉴别，而无需将样品从原始容器中倒出，展示了实际潜力。

    Reliable identification of high-value products such as whisky is an increasingly important area, as issues such as brand substitution (i.e. fraudulent products) and quality control are critical to the industry. We have examined a range of machine learning algorithms and interfaced them directly with a portable Raman spectroscopy device to both identify and characterize the ethanol/methanol concentrations of commercial whisky samples. We demonstrate that machine learning models can achieve over 99% accuracy in brand identification across twenty-eight commercial samples. To demonstrate the flexibility of this approach we utilised the same samples and algorithms to quantify ethanol concentrations, as well as measuring methanol levels in spiked whisky samples. Our machine learning techniques are then combined with a through-the-bottle method to perform spectral analysis and identification without requiring the sample to be decanted from the original container, showing the practical potenti
    
[^37]: 通过在线视频对狗叫声进行词汇分析的研究

    Towards Lexical Analysis of Dog Vocalizations via Online Videos. (arXiv:2309.13086v1 [cs.SD])

    [http://arxiv.org/abs/2309.13086](http://arxiv.org/abs/2309.13086)

    本研究通过在线视频的数据驱动研究，探索了狗叫声的语义，发现了支持以前启发式研究的证据，并提出了关于狗叫声的新的观点和发现。

    

    解析动物语言的语义一直是一个重大挑战。本研究通过将不同声音类型与一致的语义相关联，对狗叫声的语义进行了数据驱动的研究。我们首先提出了一个新的Shiba Inu声音数据集，同时还收集了来自YouTube的上下文信息，如位置和活动，通过一套完善的流程。该框架也适用于其他动物物种。通过研究狗叫声与相应的位置和活动之间的条件概率分析，我们发现了支持以前启发式研究关于不同狗叫声的语义意义的证据。例如，咆哮可以表示互动。此外，我们的研究还得出了新的观点，即现有的词汇类型可以细分为更精细的子类型，对于Shiba Inu来说，最小的语义单元是与词汇相关的。例如，呜咽声可以细分为两种类型，求关注和不适。

    Deciphering the semantics of animal language has been a grand challenge. This study presents a data-driven investigation into the semantics of dog vocalizations via correlating different sound types with consistent semantics. We first present a new dataset of Shiba Inu sounds, along with contextual information such as location and activity, collected from YouTube with a well-constructed pipeline. The framework is also applicable to other animal species. Based on the analysis of conditioned probability between dog vocalizations and corresponding location and activity, we discover supporting evidence for previous heuristic research on the semantic meaning of various dog sounds. For instance, growls can signify interactions. Furthermore, our study yields new insights that existing word types can be subdivided into finer-grained subtypes and minimal semantic unit for Shiba Inu is word-related. For example, whimper can be subdivided into two types, attention-seeking and discomfort.
    
[^38]: 我的狗和我有相同的“语言”吗？宠物狗和主人之间的声学相关性

    Does My Dog ''Speak'' Like Me? The Acoustic Correlation between Pet Dogs and Their Human Owners. (arXiv:2309.13085v1 [cs.SD])

    [http://arxiv.org/abs/2309.13085](http://arxiv.org/abs/2309.13085)

    这项研究初步调查了宠物狗叫声与其主人语言环境之间的声学相关性，并发现了两种语言环境下狗叫声的显著声学差异，并找到了一些可能与主人语言模式相关的狗叫声特征。

    

    宿主语言对宠物的叫声产生影响是一个有趣但很少被探索的问题。本文首次对家狗叫声和它们人类宿主的语言环境之间可能存在的相关性进行了初步调查。我们首先提供了一个新的Shiba Inu狗叫声数据集，其中包括7500个干净的声音片段，还包括这些叫声的上下文信息和它们主人精心设计的数据处理流程中的语音片段。上下文信息包括录音时的场景类别、狗的位置和活动。通过分类任务和显著因素分析，我们发现了来自两种语言环境的狗叫声中的显著声学差异。我们进一步确定了一些可能与宠物主人的语言模式相关的狗叫声的声学特征。

    How hosts language influence their pets' vocalization is an interesting yet underexplored problem. This paper presents a preliminary investigation into the possible correlation between domestic dog vocal expressions and their human host's language environment. We first present a new dataset of Shiba Inu dog vocals from YouTube, which provides 7500 clean sound clips, including their contextual information of these vocals and their owner's speech clips with a carefully-designed data processing pipeline. The contextual information includes the scene category in which the vocal was recorded, the dog's location and activity. With a classification task and prominent factor analysis, we discover significant acoustic differences in the dog vocals from the two language environments. We further identify some acoustic features from dog vocalizations that are potentially correlated to their host language patterns.
    
[^39]: SPICED: 具有多个主题和复杂程度的新闻相似性检测数据集

    SPICED: News Similarity Detection Dataset with Multiple Topics and Complexity Levels. (arXiv:2309.13080v1 [cs.CL])

    [http://arxiv.org/abs/2309.13080](http://arxiv.org/abs/2309.13080)

    这个论文提出了一个名为SPICED的新闻相似性检测数据集，包括七个主题，并提供了四种不同的方法来生成新闻。

    

    如今，使用智能系统来检测新闻文章中的冗余信息已经变得非常普遍，以增强用户体验，尤其是随着新闻媒体的蓬勃发展。然而，新闻的异质性可能导致这些系统中的虚假发现：简单的启发式算法，比如一对新闻是否都涉及政治问题，可以提供强大但具有误导性的下游性能。将新闻相似性数据集分割成主题可以通过强制模型学习如何在更狭窄的领域中区分显著特征来改进这些模型的训练。然而，这需要存在目前缺乏的专题特定数据集。在本文中，我们提出了一个新的相似新闻数据集SPICED，其中包括七个主题：犯罪与法律、文化与娱乐、灾难与事故、经济与商业、政治与冲突、科学与技术以及体育。此外，我们提供了四种不同的方法来生成新闻。

    Nowadays, the use of intelligent systems to detect redundant information in news articles has become especially prevalent with the proliferation of news media outlets in order to enhance user experience. However, the heterogeneous nature of news can lead to spurious findings in these systems: Simple heuristics such as whether a pair of news are both about politics can provide strong but deceptive downstream performance. Segmenting news similarity datasets into topics improves the training of these models by forcing them to learn how to distinguish salient characteristics under more narrow domains. However, this requires the existence of topic-specific datasets, which are currently lacking. In this article, we propose a new dataset of similar news, SPICED, which includes seven topics: Crime & Law, Culture & Entertainment, Disasters & Accidents, Economy & Business, Politics & Conflicts, Science & Technology, and Sports. Futhermore, we present four distinct approaches for generating news 
    
[^40]: LPML: 数学推理的LLM提示标记语言

    LPML: LLM-Prompting Markup Language for Mathematical Reasoning. (arXiv:2309.13078v1 [cs.AI])

    [http://arxiv.org/abs/2309.13078](http://arxiv.org/abs/2309.13078)

    本论文提出了LPML，一种用于数学推理的LLM提示标记语言。通过将Chain-of-Thought方法和Python REPL与该标记语言结合，我们能够控制LLM生成文本中的错误，并增强其推理能力。我们的方法能够实现利用Python计算纠正错误和解决挑战性数学问题，而只需要零样本提示。

    

    在利用大型语言模型（LLMs）进行数学推理时，解决LLMs生成文本中的推理和计算错误是一个关键挑战。在本文中，我们提出了一种新的框架，将Chain-of-Thought（CoT）方法与外部工具（Python REPL）相结合。我们发现，通过提示LLMs生成类似XML标记语言的结构化文本，我们可以无缝地集成CoT和外部工具，并控制LLMs的不良行为。通过我们的方法，LLMs可以利用Python计算来纠正CoT中的错误。我们将我们的方法应用于ChatGPT（GPT-3.5）来解决具有挑战性的数学问题，并证明通过标记语言将CoT和Python REPL结合起来可以增强LLMs的推理能力。我们的方法使LLMs能够使用零样本提示编写标记语言，并进行高级数学推理。

    In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.
    
[^41]: 一种可微分的端到端混合结构压缩学习框架

    A Differentiable Framework for End-to-End Learning of Hybrid Structured Compression. (arXiv:2309.13077v1 [cs.LG])

    [http://arxiv.org/abs/2309.13077](http://arxiv.org/abs/2309.13077)

    提出了一个可微分的端到端混合结构压缩学习框架，该框架能够在单一的分析公式中融合滤波器选择、秩选择和预算约束，并通过梯度优化实现端到端学习。实验证明了该框架的有效性，超过了现有的结构化压缩方法。

    

    滤波器剪枝和低秩分解是结构化压缩的两个基本技术。虽然最近的研究尝试了整合这两种技术优势的混合方法，但性能提升一直很有限。本研究提出了一个称为Differentiable Framework (DF)的框架，它能够将滤波器选择、秩选择和预算约束融合成一个单一的分析公式。在该框架下，我们引入了用于滤波器选择的DML-S，将调度集成到现有的掩码学习技术中。此外，我们还提出了用于秩选择的DTL-S，利用奇异值阈值运算符。DF框架结合DML-S和DTL-S提供了一种混合结构压缩方法，在梯度优化的过程中实现了端到端学习。实验证明了DF的有效性，超过了现有的结构化压缩方法。我们的工作为建立一个强大而通用的研究方向奠定了基础。

    Filter pruning and low-rank decomposition are two of the foundational techniques for structured compression. Although recent efforts have explored hybrid approaches aiming to integrate the advantages of both techniques, their performance gains have been modest at best. In this study, we develop a \textit{Differentiable Framework~(DF)} that can express filter selection, rank selection, and budget constraint into a single analytical formulation. Within the framework, we introduce DML-S for filter selection, integrating scheduling into existing mask learning techniques. Additionally, we present DTL-S for rank selection, utilizing a singular value thresholding operator. The framework with DML-S and DTL-S offers a hybrid structured compression methodology that facilitates end-to-end learning through gradient-base optimization. Experimental results demonstrate the efficacy of DF, surpassing state-of-the-art structured compression methods. Our work establishes a robust and versatile avenue fo
    
[^42]: SCREWS: 一种用于推理修订的模块化框架

    SCREWS: A Modular Framework for Reasoning with Revisions. (arXiv:2309.13075v1 [cs.AI])

    [http://arxiv.org/abs/2309.13075](http://arxiv.org/abs/2309.13075)

    SCREWS是一个模块化框架，用于推理修订。它能够统一先前的方法并提供新的策略来识别改进的推理链。在多样的推理任务上，使用最先进的LLMs（ChatGPT和GPT-4）评估SCREWS的性能，并发现了有用的新的推理策略。

    

    大型语言模型 (LLMs) 可以通过根据反馈不断改进和修订其输出来提高在各种任务上的准确性。我们观察到这些修订可能会引入错误，如果是这样的话，最好回滚到先前的结果。此外，修订通常是同质的：它们使用与产生初始答案的相同推理方法，这可能无法纠正错误。为了在这个领域中进行探索，我们提出了 SCREWS，一种用于推理修订的模块化框架。它由三个主要模块组成: 采样、条件重新采样和选择，每个模块都包含可以根据任务手动选择的子模块。我们展示了 SCREWS 不仅将几个先前的方法统一到一个共同的框架中，还揭示了几种用于识别改进的推理链的新策略。我们使用最先进的LLMs （ChatGPT 和 GPT-4）在多样的推理任务上评估我们的框架，并揭示了有用的新的推理策略。

    Large language models (LLMs) can improve their accuracy on various tasks through iteratively refining and revising their output based on feedback. We observe that these revisions can introduce errors, in which case it is better to roll back to a previous result. Further, revisions are typically homogeneous: they use the same reasoning method that produced the initial answer, which may not correct errors. To enable exploration in this space, we present SCREWS, a modular framework for reasoning with revisions. It is comprised of three main modules: Sampling, Conditional Resampling, and Selection, each consisting of sub-modules that can be hand-selected per task. We show that SCREWS not only unifies several previous approaches under a common framework, but also reveals several novel strategies for identifying improved reasoning chains. We evaluate our framework with state-of-the-art LLMs (ChatGPT and GPT-4) on a diverse set of reasoning tasks and uncover useful new reasoning strategies fo
    
[^43]: 基于神经符号方法的弱监督推理

    Weakly Supervised Reasoning by Neuro-Symbolic Approaches. (arXiv:2309.13072v1 [cs.CL])

    [http://arxiv.org/abs/2309.13072](http://arxiv.org/abs/2309.13072)

    本文介绍了一种基于神经符号方法的弱监督推理框架，该框架将符号主义和连接主义结合起来，成功应用于各种自然语言处理任务，并通过设计具有符号潜在结构的神经系统，并应用强化学习或松弛方法来进行推理。

    

    深度学习极大地提高了各种自然语言处理（NLP）任务的性能。然而，大多数深度学习模型是黑盒机器，缺乏明确的解释。在本章中，我们将介绍我们在NLP方面的神经符号方法的最新进展，该方法结合了不同的人工智能学派，即符号主义和连接主义。一般而言，我们会设计一个带有符号潜在结构的神经系统，用于NLP任务，并应用强化学习或其松弛方法来进行下游任务中的弱监督推理。我们的框架已成功应用于各种任务，包括表格查询推理、句法结构推理、信息抽取推理和规则推理。对于每个应用，我们将介绍背景、我们的方法和实验结果。

    Deep learning has largely improved the performance of various natural language processing (NLP) tasks. However, most deep learning models are black-box machinery, and lack explicit interpretation. In this chapter, we will introduce our recent progress on neuro-symbolic approaches to NLP, which combines different schools of AI, namely, symbolism and connectionism. Generally, we will design a neural system with symbolic latent structures for an NLP task, and apply reinforcement learning or its relaxation to perform weakly supervised reasoning in the downstream task. Our framework has been successfully applied to various tasks, including table query reasoning, syntactic structure reasoning, information extraction reasoning, and rule reasoning. For each application, we will introduce the background, our approach, and experimental results.
    
[^44]: 基于树的重建分区：一种新颖的低数据级生成方法

    Tree-Based Reconstructive Partitioning: A Novel Low-Data Level Generation Approach. (arXiv:2309.13071v1 [cs.AI])

    [http://arxiv.org/abs/2309.13071](http://arxiv.org/abs/2309.13071)

    基于树的重建分区（TRP）是一种新颖的PCGML方法，能够在游戏开发的早期阶段引入，无需人类专业知识或大量训练数据。

    

    程序化内容生成（PCG）是一种算法生成内容的方法，通常应用于游戏。已经有一些基于机器学习的PCG方法出现在已发表的游戏中。然而，在游戏开发的早期阶段应用这些方法可能会很困难。PCG需要在规则或函数中表示设计师对质量的概念的专业知识，而基于机器学习的PCG通常需要大量的训练数据，这在开发初期可能无法获取。本文介绍了一种名为基于树的重建分区（TRP）的新颖PCGML方法，旨在解决这个问题。我们在两个领域的实验结果表明，TRP生成的关卡更具可玩性和连贯性，并且这种方法在使用较少训练数据的情况下更具泛化能力。我们认为TRP是一种有前途的新方法，可以使PCGML在游戏开发的早期阶段引入，而不需要人类专业知识或大量训练数据。

    Procedural Content Generation (PCG) is the algorithmic generation of content, often applied to games. PCG and PCG via Machine Learning (PCGML) have appeared in published games. However, it can prove difficult to apply these approaches in the early stages of an in-development game. PCG requires expertise in representing designer notions of quality in rules or functions, and PCGML typically requires significant training data, which may not be available early in development. In this paper, we introduce Tree-based Reconstructive Partitioning (TRP), a novel PCGML approach aimed to address this problem. Our results, across two domains, demonstrate that TRP produces levels that are more playable and coherent, and that the approach is more generalizable with less training data. We consider TRP to be a promising new approach that can afford the introduction of PCGML into the early stages of game development without requiring human expertise or significant training data.
    
[^45]: 基于机器学习技术的假新闻检测

    Machine Learning Technique Based Fake News Detection. (arXiv:2309.13069v1 [cs.CL])

    [http://arxiv.org/abs/2309.13069](http://arxiv.org/abs/2309.13069)

    本论文通过使用机器学习技术训练了一个模型，可以有效地检测假新闻。在实验中，我们发现朴素贝叶斯分类器的准确率为56%，是最佳模型。

    

    假新闻引起了公众和学术界的关注。这种虚假信息有能力影响公众的看法，给恶意团体影响公共事件（如选举）的机会。任何人都可以分享关于任何人或任何事情的虚假新闻或事实，以谋取个人利益或给某人带来麻烦。此外，信息因分享的地区而异。因此，在本文中，我们使用我们收集的1876条新闻数据训练了一个模型，通过使用自然语言处理方法对数据进行预处理，从而获得干净和过滤的文本。我们的研究使用了3个流行的机器学习算法（随机梯度下降、朴素贝叶斯、逻辑回归）和2个深度学习算法（长短期记忆、权重丢弃LSTM或AWD-LSTM）。经过我们的研究，我们发现了准确率为56%，F1-macro分数为的最佳朴素贝叶斯分类器。

    False news has received attention from both the general public and the scholarly world. Such false information has the ability to affect public perception, giving nefarious groups the chance to influence the results of public events like elections. Anyone can share fake news or facts about anyone or anything for their personal gain or to cause someone trouble. Also, information varies depending on the part of the world it is shared on. Thus, in this paper, we have trained a model to classify fake and true news by utilizing the 1876 news data from our collected dataset. We have preprocessed the data to get clean and filtered texts by following the Natural Language Processing approaches. Our research conducts 3 popular Machine Learning (Stochastic gradient descent, Na\"ive Bayes, Logistic Regression,) and 2 Deep Learning (Long-Short Term Memory, ASGD Weight-Dropped LSTM, or AWD-LSTM) algorithms. After we have found our best Naive Bayes classifier with 56% accuracy and an F1-macro score o
    
[^46]: UNICON:一种在电子商务中基于行为的消费者细分的统一框架

    UNICON: A unified framework for behavior-based consumer segmentation in e-commerce. (arXiv:2309.13068v1 [cs.IR])

    [http://arxiv.org/abs/2309.13068](http://arxiv.org/abs/2309.13068)

    UNICON是一个统一的深度学习消费者细分框架，利用丰富的消费者行为数据进行个性化，实现了通过扩大预定义的目标种子细分来获取类似目标的个性化结果，并通过揭示具有相似性倾向的非明显消费者细分来获取数据驱动的个性化结果。

    

    数据驱动的个性化是时尚电子商务的关键实践，提高了企业为消费者提供更相关内容的方式。而超级个性化为每个消费者提供高度个定制的体验，但需要大量的个人数据来创建个性化的用户旅程。为了减轻这一问题，基于群体的个性化提供了基于更广泛的共同偏好构建的中度个性化，并且仍能对结果进行个性化。我们引入了UNICON，这是一个统一的深度学习消费者细分框架，利用丰富的消费者行为数据来学习长期的潜在表示，并利用它们提取两种关键类型的细分，以满足不同的个性化使用案例：类似目标，通过与行为相似的消费者扩大预定义的目标种子细分，并且数据驱动，揭示具有相似性倾向的非明显消费者细分。通过详细的实验证明，

    Data-driven personalization is a key practice in fashion e-commerce, improving the way businesses serve their consumers needs with more relevant content. While hyper-personalization offers highly targeted experiences to each consumer, it requires a significant amount of private data to create an individualized journey. To alleviate this, group-based personalization provides a moderate level of personalization built on broader common preferences of a consumer segment, while still being able to personalize the results. We introduce UNICON, a unified deep learning consumer segmentation framework that leverages rich consumer behavior data to learn long-term latent representations and utilizes them to extract two pivotal types of segmentation catering various personalization use-cases: lookalike, expanding a predefined target seed segment with consumers of similar behavior, and data-driven, revealing non-obvious consumer segments with similar affinities. We demonstrate through extensive exp
    
[^47]: 个性化学生学习中的因果发现和反事实解释

    Causal Discovery and Counterfactual Explanations for Personalized Student Learning. (arXiv:2309.13066v1 [cs.CY])

    [http://arxiv.org/abs/2309.13066](http://arxiv.org/abs/2309.13066)

    该论文使用因果发现技术来识别学生表现的因果预测因素，并应用反事实分析来提供个性化建议。

    

    该论文关注于识别学生表现的原因，以提供个性化的改进通过率的建议。我们提出了超越预测模型的需求，改为识别因果关系。我们提出使用因果发现技术来实现这一目标。该研究的主要贡献包括使用因果发现来识别学生表现的因果预测因素，并应用反事实分析来提供个性化的建议。该论文描述了将因果发现方法（特别是PC算法）应用于真实学生表现数据的情况。它解决了样本大小限制等挑战，并强调了领域知识在因果发现中的作用。结果揭示了识别出的因果关系，例如早期考试成绩和数学能力对最终学生表现的影响。该研究的局限性包括对准确因果发现的领域专业知识的依赖。

    The paper focuses on identifying the causes of student performance to provide personalized recommendations for improving pass rates. We introduce the need to move beyond predictive models and instead identify causal relationships. We propose using causal discovery techniques to achieve this. The study's main contributions include using causal discovery to identify causal predictors of student performance and applying counterfactual analysis to provide personalized recommendations. The paper describes the application of causal discovery methods, specifically the PC algorithm, to real-life student performance data. It addresses challenges such as sample size limitations and emphasizes the role of domain knowledge in causal discovery. The results reveal the identified causal relationships, such as the influence of earlier test grades and mathematical ability on final student performance. Limitations of this study include the reliance on domain expertise for accurate causal discovery, and 
    
[^48]: InvestLM：使用金融领域指导调优的大型语言模型

    InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning. (arXiv:2309.13064v1 [q-fin.GN])

    [http://arxiv.org/abs/2309.13064](http://arxiv.org/abs/2309.13064)

    InvestLM是一个通过对金融领域指导数据集进行调优的大型语言模型，具有强大的理解金融文本的能力，并在投资相关问题上提供有帮助的回答。金融专家评价其与最先进的商业模型可媲美，并在金融NLP基准问题上展现了强大的泛化能力。

    

    我们介绍了一种新的金融领域大型语言模型InvestLM，该模型通过精心策划的与金融投资相关的指导数据集对LLaMA-65B进行调优。受到“少即是多”的启发，我们手动策划了一个既小又多样的指导数据集，涵盖了从特许金融分析师（CFA）考试问题到SEC文件和Stackexchange量化金融讨论的广泛金融相关主题。InvestLM表现出良好的理解金融文本的能力，并对投资相关问题提供有帮助的回答。包括对冲基金经理和研究分析师在内的金融专家将InvestLM的回答评价为与最先进的商业模型（GPT-3.5、GPT-4和Claude-2）可媲美。对一组金融NLP基准问题进行零样本评估表明了其强大的泛化能力。从研究角度来看，本研究表明可以使用高质量的领域特定LLM进行调优。

    We present a new financial domain large language model, InvestLM, tuned on LLaMA-65B (Touvron et al., 2023), using a carefully curated instruction dataset related to financial investment. Inspired by less-is-more-for-alignment (Zhou et al., 2023), we manually curate a small yet diverse instruction dataset, covering a wide range of financial related topics, from Chartered Financial Analyst (CFA) exam questions to SEC filings to Stackexchange quantitative finance discussions. InvestLM shows strong capabilities in understanding financial text and provides helpful responses to investment related questions. Financial experts, including hedge fund managers and research analysts, rate InvestLM's response as comparable to those of state-of-the-art commercial models (GPT-3.5, GPT-4 and Claude-2). Zero-shot evaluation on a set of financial NLP benchmarks demonstrates strong generalizability. From a research perspective, this work suggests that a high-quality domain specific LLM can be tuned usin
    
[^49]: 用个人AI导师实施学习原理：一个案例研究

    Implementing Learning Principles with a Personal AI Tutor: A Case Study. (arXiv:2309.13060v1 [cs.CY])

    [http://arxiv.org/abs/2309.13060](http://arxiv.org/abs/2309.13060)

    本研究通过将AI导师与学习计划相结合，实施了个性化、检索练习和间隔重复等学习原理，研究结果显示，积极使用AI导师参与学习的学生获得了显著更高的成绩。

    

    基于个性化、检索练习和间隔重复等原则的有效学习策略往往难以在实践中实施。在这里，我们探索了将AI导师与学习计划相结合，根据学习科学进行补充。在UniDistance Suisse进行了一个学期长的研究，将一个AI导师应用提供给修读神经科学课程的心理学学生（N=51）。通过使用GPT-3从现有课程材料自动生成微学习问题，AI导师开发了每个学生对关键概念的理解的动态神经网络模型。这使得可以根据每个学生个体水平和能力个性化实施分布式检索练习。结果表明，积极使用AI导师参与学习的学生获得了显著更高的成绩。此外，积极参与导致平均提高了最多15个百分点，相比于平行课程。

    Effective learning strategies based on principles like personalization, retrieval practice, and spaced repetition are often challenging to implement due to practical constraints. Here we explore the integration of AI tutors to complement learning programs in accordance with learning sciences. A semester-long study was conducted at UniDistance Suisse, where an AI tutor app was provided to psychology students taking a neuroscience course (N=51). After automatically generating microlearning questions from existing course materials using GPT-3, the AI tutor developed a dynamic neural-network model of each student's grasp of key concepts. This enabled the implementation of distributed retrieval practice, personalized to each student's individual level and abilities. The results indicate that students who actively engaged with the AI tutor achieved significantly higher grades. Moreover, active engagement led to an average improvement of up to 15 percentile points compared to a parallel cours
    
[^50]: 学生成功建模：最重要的因素

    Students Success Modeling: Most Important Factors. (arXiv:2309.13052v1 [cs.CY])

    [http://arxiv.org/abs/2309.13052](http://arxiv.org/abs/2309.13052)

    本研究提出了一个深度学习模型，用于预测有风险的学生，并根据学生的课程进展不同阶段进行调整。实验结果显示，在最早的阶段就可以相当准确地区分将要毕业的学生与有风险的学生，并且后者的预测准确性会迅速提高，但在后者这个类别内部的区分度会相对较低。

    

    对于高等教育机构来说，保留率的重要性促使数据分析师提出了各种方法来预测有风险的学生。在激励下，本研究提出了一个深度学习模型，该模型使用了60,822名学生的121个不同类别的特征，这些特征通过记录进行了提取或工程化处理。该模型旨在识别有可能毕业的学生，有可能转校的学生以及有可能辍学中途离开高等学府的学生。本研究还尝试根据学生的课程进展不同阶段进行预测。为此，引入了LSTM层来考虑时间因素。我们的实验表明，在最早的阶段就可以相当准确地区分将要毕业的学生与有风险的学生，并且后者的预测准确性会迅速提高，但在后者这个类别内部的区分度会相对较低。

    The importance of retention rate for higher education institutions has encouraged data analysts to present various methods to predict at-risk students. The present study, motivated by the same encouragement, proposes a deep learning model trained with 121 features of diverse categories extracted or engineered out of the records of 60,822 postsecondary students. The model undertakes to identify students likely to graduate, the ones likely to transfer to a different school, and the ones likely to drop out and leave their higher education unfinished. This study undertakes to adjust its predictive methods for different stages of curricular progress of students. The temporal aspects introduced for this purpose are accounted for by incorporating layers of LSTM in the model. Our experiments demonstrate that distinguishing between to-be-graduate and at-risk students is reasonably achievable in the earliest stages, and then it rapidly improves, but the resolution within the latter category (dro
    
[^51]: 通过混合方法：数据库和文本挖掘，解码美国高等教育体系中的学位的字母组合。

    Decoding the Alphabet Soup of Degrees in the United States Postsecondary Education System Through Hybrid Method: Database and Text Mining. (arXiv:2309.13050v1 [cs.IR])

    [http://arxiv.org/abs/2309.13050](http://arxiv.org/abs/2309.13050)

    本文提出了一个混合模型，通过数据库和文本挖掘方法，解码了美国高等教育系统中学位的不确定表达，并通过对学生追踪报告进行解释和分类，实现了对学位级别的准确预测。这种分类有助于研究学生成功和流动的模式。

    

    本文提出了一个模型，用于预测在国家学生清算中心（NSC）的学生追踪报告中含糊不清地表达的高等教育学位（例如学士、硕士等）的级别。该模型是两个模块的混合体。第一个模块通过参考我们编制的近950个美国高等教育机构学位标题缩写的综合数据库，解释NSC报告中嵌入的相关缩写元素。第二个模块是CNN-BiLSTM模型的特征分类和文本挖掘的组合，前面有数个繁重的预处理步骤。本文提出的模型是通过四个不同分辨率的多标签数据集进行训练的，并在最复杂的数据集上返回了97.83％的准确率。这种对学位级别的彻底分类将为研究学生成功和流动的模式提供见解。

    This paper proposes a model to predict the levels (e.g., Bachelor, Master, etc.) of postsecondary degree awards that have been ambiguously expressed in the student tracking reports of the National Student Clearinghouse (NSC). The model will be the hybrid of two modules. The first module interprets the relevant abbreviatory elements embedded in NSC reports by referring to a comprehensive database that we have made of nearly 950 abbreviations for degree titles used by American postsecondary educators. The second module is a combination of feature classification and text mining modeled with CNN-BiLSTM, which is preceded by several steps of heavy pre-processing. The model proposed in this paper was trained with four multi-label datasets of different grades of resolution and returned 97.83\% accuracy with the most sophisticated dataset. Such a thorough classification of degree levels will provide insights into the modeling patterns of student success and mobility. To date, such a classifica
    
[^52]: NeuroCADR:通过综合计算方法进行药物重用以揭示新的抗癫痫药物候选人

    NeuroCADR: Drug Repurposing to Reveal Novel Anti-Epileptic Drug Candidates Through an Integrated Computational Approach. (arXiv:2309.13047v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.13047](http://arxiv.org/abs/2309.13047)

    本文介绍了一种名为NeuroCADR的算法，通过综合计算方法进行药物重用以揭示新的抗癫痫药物候选人。该方法利用数据库的信息，确定靶蛋白和药物分子的相互作用，通过k最近邻算法、随机森林分类和决策树等方法提高了准确性。

    

    药物重用是一种新兴的药物发现方法，涉及将现有药物重新用于新的目的。与传统的药物开发的全新过程相比，重用药物更快速、更廉价、更少失败。最近，药物重用在计算机模拟中进行，利用药物和化学信息的数据库确定靶蛋白和药物分子之间的相互作用，从而识别潜在的药物候选人。提出了一种名为NeuroCADR的算法，它是通过多重途径的方法进行药物重用的一种新颖系统，包括k最近邻算法（KNN）、随机森林分类和决策树。数据来自几个数据库，包括疾病、症状、基因和相关药物分子之间的相互作用，然后编译成二进制表示的数据集。该方法显示出较高的准确性，表现优于近似方法。

    Drug repurposing is an emerging approach for drug discovery involving the reassignment of existing drugs for novel purposes. An alternative to the traditional de novo process of drug development, repurposed drugs are faster, cheaper, and less failure prone than drugs developed from traditional methods. Recently, drug repurposing has been performed in silico, in which databases of drugs and chemical information are used to determine interactions between target proteins and drug molecules to identify potential drug candidates. A proposed algorithm is NeuroCADR, a novel system for drug repurposing via a multi-pronged approach consisting of k-nearest neighbor algorithms (KNN), random forest classification, and decision trees. Data was sourced from several databases consisting of interactions between diseases, symptoms, genes, and affiliated drug molecules, which were then compiled into datasets expressed in binary. The proposed method displayed a high level of accuracy, outperforming nearl
    
[^53]: 针对行为认证系统的隐私保护机器学习

    Privacy Preserving Machine Learning for Behavioral Authentication Systems. (arXiv:2309.13046v1 [cs.IR])

    [http://arxiv.org/abs/2309.13046](http://arxiv.org/abs/2309.13046)

    本文研究了针对行为认证系统的隐私保护机器学习。我们使用随机投影技术来确保神经网络模型中的数据隐私，以防止隐私攻击。这种方法可以消除个人资料数据库的需求，并能有效验证用户的身份。

    

    行为认证系统使用用户的行为特征来验证其身份。通过在用户个人资料上训练神经网络分类器，可以构建一个行为认证验证算法。训练好的神经网络模型对呈现的验证数据进行分类，如果分类结果与声明的身份匹配，则接受该声明。这种基于分类的方法消除了维护个人资料数据库的需求。然而，类似于其他神经网络结构，行为认证系统的神经网络分类器容易受到隐私攻击。为了保护神经网络中使用的训练和测试数据的隐私，广泛使用各种不同的技术。本文主要关注一种非加密的方法，我们使用随机投影来确保神经网络模型中的数据隐私。随机投影是一种基于随机矩阵的距离保持转换。在与验证者共享个人资料之前，用户将通过随机投影对其个人资料进行转换，并保持其隐私性。

    A behavioral authentication (BA) system uses the behavioral characteristics of users to verify their identity claims. A BA verification algorithm can be constructed by training a neural network (NN) classifier on users' profiles. The trained NN model classifies the presented verification data, and if the classification matches the claimed identity, the verification algorithm accepts the claim. This classification-based approach removes the need to maintain a profile database. However, similar to other NN architectures, the NN classifier of the BA system is vulnerable to privacy attacks. To protect the privacy of training and test data used in an NN different techniques are widely used. In this paper, our focus is on a non-crypto-based approach, and we used random projection (RP) to ensure data privacy in an NN model. RP is a distance-preserving transformation based on a random matrix. Before sharing the profiles with the verifier, users will transform their profiles by RP and keep thei
    
[^54]: 表达性变分量子电路在联邦学习中提供固有隐私

    Expressive variational quantum circuits provide inherent privacy in federated learning. (arXiv:2309.13002v1 [quant-ph])

    [http://arxiv.org/abs/2309.13002](http://arxiv.org/abs/2309.13002)

    表达性变分量子电路模型在联邦学习中提供固有隐私保护，同时通过使用过度参数化保证模型可训练性。通过解决高次多元切比雪夫多项式方程的复杂性，实现对梯度反转攻击的困难性。

    

    联邦学习已经成为一种可行的分布式解决方案，可以在不与中央聚合器共享数据的情况下训练机器学习模型。然而，已经显示出标准的基于神经网络的联邦学习模型容易受到与服务器共享的梯度的数据泄露攻击。在这项工作中，我们介绍了使用表达性编码映射和过度参数化ans\"tze构建的变分量子电路模型的联邦学习。我们证明了表达性映射导致对梯度反转攻击具有固有的隐私保护，而过度参数化确保了模型的可训练性。我们的隐私框架集中在通过量子电路梯度生成的高次多元切比雪夫多项式的解决复杂性上。我们提出了令人信服的论点，强调在精确和近似情况下解决这些方程的固有困难性。此外，我们深入探讨了基于机器学习的攻击策略。

    Federated learning has emerged as a viable distributed solution to train machine learning models without the actual need to share data with the central aggregator. However, standard neural network-based federated learning models have been shown to be susceptible to data leakage from the gradients shared with the server. In this work, we introduce federated learning with variational quantum circuit model built using expressive encoding maps coupled with overparameterized ans\"atze. We show that expressive maps lead to inherent privacy against gradient inversion attacks, while overparameterization ensures model trainability. Our privacy framework centers on the complexity of solving the system of high-degree multivariate Chebyshev polynomials generated by the gradients of quantum circuit. We present compelling arguments highlighting the inherent difficulty in solving these equations, both in exact and approximate scenarios. Additionally, we delve into machine learning-based attack strate
    
[^55]: ForceSight: 使用文本引导的视觉力导向移动操作

    ForceSight: Text-Guided Mobile Manipulation with Visual-Force Goals. (arXiv:2309.12312v1 [cs.RO])

    [http://arxiv.org/abs/2309.12312](http://arxiv.org/abs/2309.12312)

    ForceSight是一个使用文本引导的移动操作系统，通过深度神经网络预测视觉力导向目标。在实验中，该系统展示了在未见环境中进行精确抓取、抽屉打开和物体交接等任务的能力，并取得了较高的成功率。

    

    我们提出了一个名为ForceSight的系统，它使用深度神经网络通过文本引导来预测视觉力导向的目标。给定一张RGBD图片和一个文本提示，ForceSight可以确定相机坐标系下的目标末端执行器位姿（运动目标）和相关的力量（力量目标）。这两个组成部分共同形成了一个视觉力导向目标。之前的研究已经表明，输出人可解释的运动目标的深度模型可以实现真实机器人的巧妙操作。力量在操作中至关重要，但在这些系统中通常被限制在较低层次的执行中。当应用于带有手臂和眼睛的移动操作装置的ForceSight时，在与训练数据差异显著的未见环境中，能够以81%的成功率完成诸如精确抓取、抽屉打开和物体交接等任务。在另一项独立实验中，ForceSight仅使用视觉伺服，不考虑力量信息，但依然显示出较高的操作成功率。

    We present ForceSight, a system for text-guided mobile manipulation that predicts visual-force goals using a deep neural network. Given a single RGBD image combined with a text prompt, ForceSight determines a target end-effector pose in the camera frame (kinematic goal) and the associated forces (force goal). Together, these two components form a visual-force goal. Prior work has demonstrated that deep models outputting human-interpretable kinematic goals can enable dexterous manipulation by real robots. Forces are critical to manipulation, yet have typically been relegated to lower-level execution in these systems. When deployed on a mobile manipulator equipped with an eye-in-hand RGBD camera, ForceSight performed tasks such as precision grasps, drawer opening, and object handovers with an 81% success rate in unseen environments with object instances that differed significantly from the training data. In a separate experiment, relying exclusively on visual servoing and ignoring force 
    
[^56]: 学习驾驶到任何地方

    Learning to Drive Anywhere. (arXiv:2309.12295v1 [cs.CV])

    [http://arxiv.org/abs/2309.12295](http://arxiv.org/abs/2309.12295)

    本文提出了一种能够学习适应不同地理位置和驾驶行为的模型，该模型通过引入基于地理位置的通道注意机制，在数据驱动的方式下高效地学习并灵活地建模不同地区之间的相似性和差异性。

    

    人类驾驶员可以无缝地适应不同地理位置的驾驶决策，包括不同的道路条件和交通规则，例如左驾驶和右驾驶。然而，现有的自动驾驶模型只能在限定的操作领域内部署，不能考虑不同地理位置之间的驾驶行为差异和模型的可扩展性。本文提出了AnyD，一种单一的具有地理感知的条件性模仿学习（CIL）模型，能够高效地从具有动态环境、交通和社会特征的异构和全球分布的数据中进行学习。我们的关键见解是引入一个高容量的基于地理位置的通道注意机制，可以在数据驱动的方式下有效地适应本地细微差异并灵活地建模不同地区之间的相似性。通过优化对比性模仿目标，我们提出的方法可以高效地适应固有的不平衡数据分布和地理位置差异。

    Human drivers can seamlessly adapt their driving decisions across geographical locations with diverse conditions and rules of the road, e.g., left vs. right-hand traffic. In contrast, existing models for autonomous driving have been thus far only deployed within restricted operational domains, i.e., without accounting for varying driving behaviors across locations or model scalability. In this work, we propose AnyD, a single geographically-aware conditional imitation learning (CIL) model that can efficiently learn from heterogeneous and globally distributed data with dynamic environmental, traffic, and social characteristics. Our key insight is to introduce a high-capacity geo-location-based channel attention mechanism that effectively adapts to local nuances while also flexibly modeling similarities among regions in a data-driven manner. By optimizing a contrastive imitation objective, our proposed approach can efficiently scale across inherently imbalanced data distributions and loca
    
[^57]: 翻转诅咒: 在大型语言模型中训练的"A是B"无法学习"B是A"

    The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])

    [http://arxiv.org/abs/2309.12288](http://arxiv.org/abs/2309.12288)

    LLMs模型在训练中只能学习到"A是B"的结构，无法自动推广到"B是A"。这表明模型在逻辑推断上存在基本失败和训练集中模式的推广问题。

    

    我们揭示了自回归大型语言模型（LLM）在泛化上的令人惊讶的失败。如果一个模型是基于"A是B"形式的句子进行训练，它不会自动推广到相反的方向"B是A"。这就是翻转诅咒。例如，如果一个模型是基于"Olaf Scholz是德国第九任总理"进行训练的，它不会自动能够回答问题"谁是德国第九任总理？"。此外，正确答案（"Olaf Scholz"）的可能性不会比随机名字更高。因此，模型在逻辑推断上存在基本失败，并且不会推广到它们训练集中的普遍模式（即如果出现"A是B"，则"B是A"更可能出现）。我们通过在虚构的陈述（如"Uriah Hawthorne是'Abyssal Melodies'的作曲家"）上对GPT-3和Llama-1进行微调，并展示它们无法正确回答"谁创作了'Abyssal Melodies'?"来提供翻转诅咒的证据。

    We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
    
[^58]: 结构组件设计的潜在扩散模型

    Latent Diffusion Models for Structural Component Design. (arXiv:2309.11601v1 [cs.LG])

    [http://arxiv.org/abs/2309.11601](http://arxiv.org/abs/2309.11601)

    本文提出了一个潜在扩散模型的框架，用于生成符合加载条件的结构组件设计。与其他生成方法相比，该方法允许对现有设计进行编辑，并且具有近优性。实验结果证明了生成设计的结构性能和潜在候选设计的可变性。

    

    最近生成模型，尤其是扩散模型的进展，已经在生成模型领域带来了革命性变化，实现了符合用户需求的高质量图像生成。本文提出了一个框架，用于生成结构组件的设计。具体而言，我们采用潜在扩散模型生成满足一组具体加载条件的组件潜在设计。相比于其他生成方法如生成对抗网络（GAN），我们的方法的一个明显优势是可以编辑现有设计。我们使用使用SIMP算法得到的结构拓扑优化几何数据集训练模型，因此我们的框架生成的设计具有固有的近优性。我们的工作提供了支持生成设计结构性能和潜在候选设计的可变性的定量结果。此外，我们提供了框架的可扩展性证据。

    Recent advances in generative modeling, namely Diffusion models, have revolutionized generative modeling, enabling high-quality image generation tailored to user needs. This paper proposes a framework for the generative design of structural components. Specifically, we employ a Latent Diffusion model to generate potential designs of a component that can satisfy a set of problem-specific loading conditions. One of the distinct advantages our approach offers over other generative approaches, such as generative adversarial networks (GANs), is that it permits the editing of existing designs. We train our model using a dataset of geometries obtained from structural topology optimization utilizing the SIMP algorithm. Consequently, our framework generates inherently near-optimal designs. Our work presents quantitative results that support the structural performance of the generated designs and the variability in potential candidate designs. Furthermore, we provide evidence of the scalability 
    
[^59]: 利用数据收集和无监督学习进行切换突尼斯阿拉伯语的自动语音识别

    Leveraging Data Collection and Unsupervised Learning for Code-switched Tunisian Arabic Automatic Speech Recognition. (arXiv:2309.11327v1 [eess.AS])

    [http://arxiv.org/abs/2309.11327](http://arxiv.org/abs/2309.11327)

    本研究通过收集和标注数据以及探索切换方法，提出了一种有效的突尼斯方言自动语音识别解决方案，并且通过人工评估来消除拼写不合适的干扰。

    

    开发能够有效识别方言的自动语音识别（ASR）解决方案需要创新的方法，不仅要解决数据稀缺问题，还要处理语言多样性的复杂性。本文针对突尼斯方言，解决了上述ASR挑战。首先，收集了文本和音频数据，并且在某些情况下进行了标注。其次，我们探索自我监督、半监督和少样本切换方法，以在不同突尼斯测试集上推动最先进的技术；涵盖不同的声学、语言和韵律条件。最后，鉴于常规拼写的缺失，我们对转录进行人工评估，以避免测试参考中的拼写不合适所带来的噪声。我们的模型可以转录突尼斯阿拉伯语、英语和法语混合语言的音频样本，并公开发布了所有训练和测试所使用的数据，供公众使用和进一步改进。

    Crafting an effective Automatic Speech Recognition (ASR) solution for dialects demands innovative approaches that not only address the data scarcity issue but also navigate the intricacies of linguistic diversity. In this paper, we address the aforementioned ASR challenge, focusing on the Tunisian dialect. First, textual and audio data is collected and in some cases annotated. Second, we explore self-supervision, semi-supervision and few-shot code-switching approaches to push the state-of-the-art on different Tunisian test sets; covering different acoustic, linguistic and prosodic conditions. Finally, and given the absence of conventional spelling, we produce a human evaluation of our transcripts to avoid the noise coming from spelling inadequacies in our testing references. Our models, allowing to transcribe audio samples in a linguistic mix involving Tunisian Arabic, English and French, and all the data used during training and testing are released for public use and further improvem
    
[^60]: 基于多智能体深度强化学习的AI驱动患者监测

    AI-Driven Patient Monitoring with Multi-Agent Deep Reinforcement Learning. (arXiv:2309.10980v1 [cs.LG])

    [http://arxiv.org/abs/2309.10980](http://arxiv.org/abs/2309.10980)

    本研究提出了一种基于多智能体深度强化学习的AI驱动患者监测框架，通过部署多个学习智能体，针对不同的生理特征进行监测，并根据紧急程度预警医疗紧急团队。

    

    有效的患者监测对及时干预和改善医疗结果至关重要。传统的监测系统往往难以处理复杂、动态的环境和波动的生命体征，导致延迟发现危急情况。为了应对这一挑战，我们提出了一种新颖的基于多智能体深度强化学习（DRL）的AI驱动患者监测框架。我们的方法部署了多个学习智能体，每个智能体专门负责监测特定的生理特征，如心率、呼吸和体温。这些智能体与通用的医疗监测环境进行交互，学习患者的行为模式，并根据估计的紧急程度做出通知相应医疗紧急团队（MET）的决策。在本研究中，我们使用来自两个数据集（PPG-DaLiA和WESAD）的真实生理和运动数据评估了提出的多智能体DRL框架的性能。

    Effective patient monitoring is vital for timely interventions and improved healthcare outcomes. Traditional monitoring systems often struggle to handle complex, dynamic environments with fluctuating vital signs, leading to delays in identifying critical conditions. To address this challenge, we propose a novel AI-driven patient monitoring framework using multi-agent deep reinforcement learning (DRL). Our approach deploys multiple learning agents, each dedicated to monitoring a specific physiological feature, such as heart rate, respiration, and temperature. These agents interact with a generic healthcare monitoring environment, learn the patients' behavior patterns, and make informed decisions to alert the corresponding Medical Emergency Teams (METs) based on the level of emergency estimated. In this study, we evaluate the performance of the proposed multi-agent DRL framework using real-world physiological and motion data from two datasets: PPG-DaLiA and WESAD. We compare the results 
    
[^61]: Riemannian流形上Matern高斯过程的后验收缩速率

    Posterior Contraction Rates for Mat\'ern Gaussian Processes on Riemannian Manifolds. (arXiv:2309.10918v1 [stat.ML])

    [http://arxiv.org/abs/2309.10918](http://arxiv.org/abs/2309.10918)

    该论文研究了定义在紧致Riemannian流形上的内在Matern高斯过程和外在过程之间的收缩速率，并发现它们的速率在适当匹配平滑参数的情况下是相等的。

    

    高斯过程在许多依赖于不确定性量化的机器学习应用中被使用。最近，已经开发了在几何设置下处理这些模型的计算工具，例如，当输入位于Riemannian流形上时。这引出了一个问题：这些内在模型在理论上是否可以证明相比于将所有相关量嵌入到$\mathbb{R}^d$并使用普通欧几里德高斯过程的限制，可以带来更好的性能？为了研究这个问题，我们证明了定义在紧致Riemannian流形上的内在Matern高斯过程的最优收缩速率。我们还通过流形和环境Sobolev空间之间的迹和扩展定理证明了外在过程的类似速率：令人惊讶的是，所得到的速率与内在过程的速率相符，前提是它们的平滑参数适当匹配。我们在一些实证数据上进行了对这些速率的演示。

    Gaussian processes are used in many machine learning applications that rely on uncertainty quantification. Recently, computational tools for working with these models in geometric settings, such as when inputs lie on a Riemannian manifold, have been developed. This raises the question: can these intrinsic models be shown theoretically to lead to better performance, compared to simply embedding all relevant quantities into $\mathbb{R}^d$ and using the restriction of an ordinary Euclidean Gaussian process? To study this, we prove optimal contraction rates for intrinsic Mat\'ern Gaussian processes defined on compact Riemannian manifolds. We also prove analogous rates for extrinsic processes using trace and extension theorems between manifold and ambient Sobolev spaces: somewhat surprisingly, the rates obtained turn out to coincide with those of the intrinsic processes, provided that their smoothness parameters are matched appropriately. We illustrate these rates empirically on a number of
    
[^62]: 深度学习网络的几何结构和全局${\mathcal L}^2$最小化器的构建

    Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v1 [cs.LG])

    [http://arxiv.org/abs/2309.10639](http://arxiv.org/abs/2309.10639)

    本文提供了深度学习网络结构的几何解释，并且构建了全局最小化器族，该族能够全局最小化成本函数。此外，还确定了各种退化局部最小值。

    

    本文提供了对深度学习（DL）网络结构的几何解释，该网络具有$L$个隐藏层，斜坡激活函数，${\mathcal L}^2$ Schatten类（或Hilbert-Schmidt）成本函数，以及相等维度$Q\geq1$的输入和输出空间${\mathbb R}^Q$。隐藏层也定义在${\mathbb R}^{Q}$的空间上。我们利用我们最新的关于浅层神经网络的结果，在$L\geq Q$的情况下构造了一个明确的最小化器族，该族能够全局最小化成本函数，并且我们证明这个族是退化的。在这里提到的上下文中，DL网络的隐藏层通过对训练输入的递归截断映射的应用来“整理”训练输入，以最小化噪声与信号的比率。此外，我们确定了$2^Q-1$个不同的退化局部最小值。

    In this paper, we provide a geometric interpretation of the structure of Deep Learning (DL) networks, characterized by $L$ hidden layers, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, and input and output spaces ${\mathbb R}^Q$ with equal dimension $Q\geq1$. The hidden layers are defined on spaces ${\mathbb R}^{Q}$, as well. We apply our recent results on shallow neural networks to construct an explicit family of minimizers for the global minimum of the cost function in the case $L\geq Q$, which we show to be degenerate. In the context presented here, the hidden layers of the DL network "curate" the training inputs by recursive application of a truncation map that minimizes the noise to signal ratio of the training inputs. Moreover, we determine a set of $2^Q-1$ distinct degenerate local minima of the cost function.
    
[^63]: 一个用于分类和解释大型非结构化法律文件的分层神经框架

    A Hierarchical Neural Framework for Classification and its Explanation in Large Unstructured Legal Documents. (arXiv:2309.10563v1 [cs.IR])

    [http://arxiv.org/abs/2309.10563](http://arxiv.org/abs/2309.10563)

    本论文提出了一个名为MESc的分层神经框架，用于分类和解释大型非结构化法律文件。通过将文件分成多个部分并使用大型语言模型的嵌入和无监督聚类，该框架能够实现从长文档中预测判决并提取解释。

    

    自动法律判决预测及其解释常常面临长达数万字的案例文件和非统一结构的问题。在没有结构标注的文件上预测判决并提取解释变得更具挑战性。本论文将这一问题定义为“稀缺标注法律文件”，并通过一种称为MESc（基于多阶段编码器的带聚类的监督）的深度学习分类框架来探索缺乏结构信息和长文档的特点。具体来说，我们将文档分成多个部分，从自定义微调的大型语言模型的最后四个层中提取它们的嵌入，并试图通过无监督聚类来近似它们的结构。然后，我们利用另一组Transformer编码器层学习部分之间的表示。我们探索了多十亿参数的大型语言模型在这种情况下的适应性。

    Automatic legal judgment prediction and its explanation suffer from the problem of long case documents exceeding tens of thousands of words, in general, and having a non-uniform structure. Predicting judgments from such documents and extracting their explanation becomes a challenging task, more so on documents with no structural annotation. We define this problem as "scarce annotated legal documents" and explore their lack of structural information and their long lengths with a deep learning-based classification framework which we call MESc; "Multi-stage Encoder-based Supervised with-clustering"; for judgment prediction. Specifically, we divide a document into parts to extract their embeddings from the last four layers of a custom fine-tuned Large Language Model, and try to approximate their structure through unsupervised clustering. Which we use in another set of transformer encoder layers to learn the inter-chunk representations. We explore the adaptability of LLMs with multi-billion
    
[^64]: FRAMU: 基于注意力的联邦强化学习机器遗忘

    FRAMU: Attention-based Machine Unlearning using Federated Reinforcement Learning. (arXiv:2309.10283v1 [cs.LG])

    [http://arxiv.org/abs/2309.10283](http://arxiv.org/abs/2309.10283)

    FRAMU是一种基于注意力和联邦强化学习的机器遗忘框架，通过自适应学习机制、隐私保护技术和优化策略，处理各种数据源并保持准确性和隐私，适应波动的数据环境，支持持续模型演进。

    

    机器遗忘是一个新兴领域，通过允许从机器学习过程中删除私有或无关数据，解决数据隐私问题。使用过时的、私有的和无关的数据会引发与隐私和模型效率相关的挑战。这些问题不仅影响模型在机器学习和遗忘中的准确性和计算效率，还会对数据隐私造成威胁。为了解决这些挑战，我们引入了一种新颖的框架，即基于注意力的联邦强化学习机器遗忘（FRAMU）。该框架融合了自适应学习机制、隐私保护技术和优化策略，是处理各种数据源（单模态或多模态）同时保持准确性和隐私性的综合解决方案。FRAMU的优势在于其适应波动的数据环境、遗忘过时、私有或无关数据的能力，以及支持模型持续演进的支持。

    Machine Unlearning is an emerging field that addresses data privacy issues by enabling the removal of private or irrelevant data from the Machine Learning process. Challenges related to privacy and model efficiency arise from the use of outdated, private, and irrelevant data. These issues compromise both the accuracy and the computational efficiency of models in both Machine Learning and Unlearning. To mitigate these challenges, we introduce a novel framework, Attention-based Machine Unlearning using Federated Reinforcement Learning (FRAMU). This framework incorporates adaptive learning mechanisms, privacy preservation techniques, and optimization strategies, making it a well-rounded solution for handling various data sources, either single-modality or multi-modality, while maintaining accuracy and privacy. FRAMU's strength lies in its adaptability to fluctuating data landscapes, its ability to unlearn outdated, private, or irrelevant data, and its support for continual model evolution
    
[^65]: 通过大型语言模型进行隐私保护掩码的恢复

    Recovering from Privacy-Preserving Masking with Large Language Models. (arXiv:2309.08628v1 [cs.CL])

    [http://arxiv.org/abs/2309.08628](http://arxiv.org/abs/2309.08628)

    本文利用大型语言模型（LLM）探索了替换标识信息的方法，并在下游语言建模任务上进行了评估。实验结果表明，使用混淆语料库训练的模型能够达到可比较的性能。

    

    模型适应对于处理代理训练数据和实际用户数据之间的差异非常重要。为了有效地进行适应，用户的文本数据通常存储在服务器或本地设备上，下游的自然语言处理模型可以使用这些领域内的数据进行直接训练。然而，这可能会引起隐私和安全问题，因为存在向对手泄露用户信息的额外风险。最近，人们开始探索使用通用标记替换文本中的标识信息。在这项工作中，我们利用大型语言模型（LLM）来建议替换掩码标记的方法，并在下游语言建模任务上评估其效果。具体而言，我们提出了多种基于预训练和微调的LLM方法，并在不同数据集上进行实证研究以比较这些方法。实验结果表明，在混淆语料库上训练的模型能够达到可比较的性能。

    Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve compar
    
[^66]: ADAM在非凸设置中具有恒定步长的收敛性：一个简单的证明

    Convergence of ADAM with Constant Step Size in Non-Convex Settings: A Simple Proof. (arXiv:2309.08339v1 [cs.LG])

    [http://arxiv.org/abs/2309.08339](http://arxiv.org/abs/2309.08339)

    本文分析了ADAM在非凸设置中具有恒定步长的收敛性，给出了步长达到几乎肯定渐近收敛的充分条件，并提供了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。

    

    在神经网络训练中，RMSProp和ADAM仍然是广泛使用的优化算法。它们的性能关键之一在于选择适当的步长，这会显著影响它们的有效性。值得注意的是，这些算法的性能可以因选择的步长而变化很大。此外，关于它们的理论收敛性问题仍然是一个感兴趣的话题。在本文中，我们在非凸设置中对ADAM的恒定步长版本进行了理论分析。我们证明了步长达到几乎肯定渐近收敛到零的充分条件，而只需最小的假设。我们还给出了确定性ADAM在处理平滑非凸函数时达到近似临界性所需的运行时间界限。

    In neural network training, RMSProp and ADAM remain widely favoured optimization algorithms. One of the keys to their performance lies in selecting the correct step size, which can significantly influence their effectiveness. It is worth noting that these algorithms performance can vary considerably, depending on the chosen step sizes. Additionally, questions about their theoretical convergence properties continue to be a subject of interest. In this paper, we theoretically analyze a constant stepsize version of ADAM in the non-convex setting. We show sufficient conditions for the stepsize to achieve almost sure asymptotic convergence of the gradients to zero with minimal assumptions. We also provide runtime bounds for deterministic ADAM to reach approximate criticality when working with smooth, non-convex functions.
    
[^67]: 保持结构的变压器用于序列的SPD矩阵

    Structure-Preserving Transformers for Sequences of SPD Matrices. (arXiv:2309.07579v1 [cs.LG])

    [http://arxiv.org/abs/2309.07579](http://arxiv.org/abs/2309.07579)

    本文介绍了一种保持序列的对称正定矩阵的黎曼几何特性的结构保持变压器机制，并将其应用于自动睡眠分期，取得了高水平的阶段性能。

    

    近年来，基于变压器的自注意力机制已成功应用于各种上下文相关的数据类型的分析，从文本到图像等，包括非欧几里得几何的数据。本文提出了一种这样的机制，用于分类序列的对称正定矩阵，并在整个分析过程中保持它们的黎曼几何特性。我们将我们的方法应用于来自标准数据集中的脑电图协方差矩阵序列的自动睡眠分期，取得了高水平的阶段性能。

    In recent years, Transformer-based auto-attention mechanisms have been successfully applied to the analysis of a variety of context-reliant data types, from texts to images and beyond, including data from non-Euclidean geometries. In this paper, we present such a mechanism, designed to classify sequences of Symmetric Positive Definite matrices while preserving their Riemannian geometry throughout the analysis. We apply our method to automatic sleep staging on timeseries of EEG-derived covariance matrices from a standard dataset, obtaining high levels of stage-wise performance.
    
[^68]: 高效有限初始化张量化神经网络的方法

    Efficient Finite Initialization for Tensorized Neural Networks. (arXiv:2309.06577v1 [cs.LG])

    [http://arxiv.org/abs/2309.06577](http://arxiv.org/abs/2309.06577)

    这种方法提出了一种高效有限初始化张量化神经网络层的方法，避免了参数爆炸问题，并通过使用弗罗贝尼乌斯范数的迭代部分形式来计算范数，使其具有有限范围。应用于不同层的实验表明其性能良好。

    

    我们提出了一种新的方法，用于初始化张量化神经网络的层，以避免参数爆炸。该方法适用于具有大量节点的层，其中所有或大多数节点与输入或输出有连接。该方法的核心是使用该层的弗罗贝尼乌斯范数的迭代部分形式，使其具有有限的范围。这个范数的计算是高效的，对于大多数情况都可以完全或部分计算。我们将这个方法应用于不同的层，并检查其性能。我们创建了一个Python函数，在i3BQuantum存储库的Jupyter Notebook中可以运行它：https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb

    We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
    
[^69]: 语言模型作为视觉-语言模型的黑盒优化器

    Language Models as Black-Box Optimizers for Vision-Language Models. (arXiv:2309.05950v1 [cs.CL])

    [http://arxiv.org/abs/2309.05950](http://arxiv.org/abs/2309.05950)

    本论文介绍了一种新的视觉-语言模型 (VLMs) 微调方法，通过自然语言提示来避免访问模型参数，采用聊天式的语言模型作为黑盒优化器，在少样本图像分类任务中达到效果。

    

    预训练在大规模网络数据集上的视觉-语言模型 (VLMs) 展示了在各种视觉和多模态任务中的显著能力。目前，VLMs 的微调方法主要在白盒环境中操作，需要访问模型参数进行反向传播。然而，许多 VLMs 依赖于专有数据且不开源，限制了使用白盒方法进行微调。鉴于像 ChatGPT 这样的受欢迎私有大型语言模型 (LLMs) 仍然提供基于语言的用户界面，我们旨在通过自然语言提示开发一种新的 VLMs 微调方法，从而避免访问模型参数、特征嵌入或输出 logits 的需要。在这种设置下，我们提出使用基于聊天的 LLMs 作为黑盒优化器，以在使用 CLIP 进行少样本图像分类的示例任务中寻找最佳文本提示。具体而言，我们采用自动"爬山"程序，它能收敛到有效的提示上。

    Vision-language models (VLMs) pre-trained on web-scale datasets have demonstrated remarkable capabilities across a variety of vision and multimodal tasks. Currently, fine-tuning methods for VLMs mainly operate in a white-box setting, requiring access to model parameters for backpropagation. However, many VLMs rely on proprietary data and are not open-source, which restricts the use of white-box approaches for fine-tuning. Given that popular private large language models (LLMs) like ChatGPT still offer a language-based user interface, we aim to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits. In this setup, we propose employing chat-based LLMs as black-box optimizers to search for the best text prompt on the illustrative task of few-shot image classification using CLIP. Specifically, we adopt an automatic "hill-climbing" procedure that converges on an effective prom
    
[^70]: Spiking Neural Network联合课程学习策略的训练

    Training of Spiking Neural Network joint Curriculum Learning Strategy. (arXiv:2309.04737v1 [cs.LG])

    [http://arxiv.org/abs/2309.04737](http://arxiv.org/abs/2309.04737)

    该论文提出了一种将课程学习引入脉冲神经网络的训练模型，使其更类似于人类学习过程，并提高了其生物解释性。

    

    从简单到复杂，逐渐引入难度的概念是人类学习的自然过程。脉冲神经网络（SNNs）旨在模拟人类信息处理的方式，但目前的SNNs模型将所有样本视为平等，这与人类学习的原则不符，并忽视了SNNs的生物合理性。为了解决这个问题，我们提出了一个将课程学习（CL）引入SNNs的CL-SNN模型，使SNNs更像人类学习，并提供更高的生物解释性。CL是一种训练策略，提倡在逐渐引入更具挑战性的数据之前向模型展示更容易的数据，模拟了人类学习的过程。我们使用具有信心感知的损失来衡量和处理不同难度水平的样本。通过学习不同样本的置信度，模型自动减少了难样本对参数优化的贡献。我们在实验中进行了研究

    Starting with small and simple concepts, and gradually introducing complex and difficult concepts is the natural process of human learning. Spiking Neural Networks (SNNs) aim to mimic the way humans process information, but current SNNs models treat all samples equally, which does not align with the principles of human learning and overlooks the biological plausibility of SNNs. To address this, we propose a CL-SNN model that introduces Curriculum Learning(CL) into SNNs, making SNNs learn more like humans and providing higher biological interpretability. CL is a training strategy that advocates presenting easier data to models before gradually introducing more challenging data, mimicking the human learning process. We use a confidence-aware loss to measure and process the samples with different difficulty levels. By learning the confidence of different samples, the model reduces the contribution of difficult samples to parameter optimization automatically. We conducted experiments on st
    
[^71]: 深度学习在多囊肾病中的应用: 通过基因表达分析实现对患者的准确和早期检测

    Deep Learning for Polycystic Kidney Disease: Utilizing Neural Networks for Accurate and Early Detection through Gene Expression Analysis. (arXiv:2309.03033v1 [cs.LG])

    [http://arxiv.org/abs/2309.03033](http://arxiv.org/abs/2309.03033)

    本研究利用深度学习方法通过基因表达分析实现对患者多囊肾病（PKD）的准确和早期检测。

    

    多囊肾病（PKD）可能导致患者肾脏中囊肿的形成，进而导致致命的并发症，因此早期检测PKD对于有效管理该病情至关重要。然而，诊断中涉及的各种患者特定因素使其对临床医生来说是一个复杂的难题。因此，在本研究中，我们旨在利用基于深度学习的方法来进行早期疾病检测。通过分析患者的基因表达，设计的神经网络可以对患者可能的PKD进行准确且可靠的预测。

    With Polycystic Kidney Disease (PKD) potentially leading to fatal complications in patients due to the formation of cysts in the kidneys, early detection of PKD is crucial for effective management of the condition. However, the various patient-specific factors that play a role in the diagnosis make it an intricate puzzle for clinicians to solve. Therefore, in this study, we aim to utilize a deep learning-based approach for early disease detection. The devised neural network can achieve accurate and robust predictions for possible PKD in patients by analyzing patient gene expressions.
    
[^72]: 一种轻量化且可传递的设计用于稳健的乐高操纵

    A Lightweight and Transferable Design for Robust LEGO Manipulation. (arXiv:2309.02354v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2309.02354](http://arxiv.org/abs/2309.02354)

    本文介绍了一种轻量化且可传递的设计，用于解决机器人乐高操纵中的复杂性和精确性要求。通过硬件软件协同设计和进化策略优化，实现了高效可靠的乐高操纵，并展示了该设计的普适性和可传递性。

    

    乐高是一个用于原型化像素化对象的众所周知的平台。然而，机器人乐高原型化（即操纵乐高积木）由于紧密的连接和精确性要求而具有挑战性。本文研究了安全高效的机器人乐高操纵。具体而言，本文通过硬件软件协同设计减少了操纵的复杂性。设计了一个末端工具（EOAT），它减少了问题维度，使大型工业机器人能够轻松操纵乐高积木。此外，本文使用进化策略安全地优化机器人运动，用于乐高操纵。实验表明，EOAT在操纵乐高积木方面表现可靠，而学习框架可以有效且安全地将操纵性能提高到100%的成功率。所设计的协同设计已经部署到多台机器人（FANUC LR-mate 200id/7L和Yaskawa GP4）上，以展示其普适性和可传递性。最后，我们展示了本研究的创新和贡献。

    LEGO is a well-known platform for prototyping pixelized objects. However, robotic LEGO prototyping (i.e. manipulating LEGO bricks) is challenging due to the tight connections and accuracy requirement. This paper investigates safe and efficient robotic LEGO manipulation. In particular, this paper reduces the complexity of the manipulation by hardware-software co-design. An end-of-arm tool (EOAT) is designed, which reduces the problem dimension and allows large industrial robots to easily manipulate LEGO bricks. In addition, this paper uses evolution strategy to safely optimize the robot motion for LEGO manipulation. Experiments demonstrate that the EOAT performs reliably in manipulating LEGO bricks and the learning framework can effectively and safely improve the manipulation performance to a 100% success rate. The co-design is deployed to multiple robots (i.e. FANUC LR-mate 200id/7L and Yaskawa GP4) to demonstrate its generalizability and transferability. In the end, we show that the p
    
[^73]: AI海洋中的妖怪之歌：大型语言模型中的幻觉调查

    Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. (arXiv:2309.01219v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.01219](http://arxiv.org/abs/2309.01219)

    本文调查了大型语言模型中幻觉的检测、解释和缓解的最新研究，提出了幻觉现象和评估基准的分类，并讨论了未来研究的潜在方向。

    

    尽管大型语言模型（LLMs）在各种下游任务中展示出了卓越的能力，但人们对其产生幻觉的倾向表示担忧：LLMs有时会生成与用户输入不符、与先前生成的内容相矛盾或与已建立的世界知识不符的内容。这种现象对LLMs在现实场景中的可靠性构成了重大挑战。本文对关于幻觉检测、解释和缓解的最新研究进行了调查，重点探讨了LLMs所面临的独特挑战。我们提出了LLM幻觉现象和评估基准的分类，分析了现有的旨在缓解LLM幻觉的方法，并讨论了未来研究的潜在方向。

    While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
    
[^74]: 用于机器学习的热带几何工具：TML软件包

    Tropical Geometric Tools for Machine Learning: the TML package. (arXiv:2309.01082v1 [stat.ML])

    [http://arxiv.org/abs/2309.01082](http://arxiv.org/abs/2309.01082)

    TML软件包是第一个包含一套全面工具和方法的R软件包，用于处理与热带凸性相关的基本计算和可视化，以及使用热带度量进行监督和无监督学习模型的统计推断。

    

    在过去的十年中，热带几何学的发展提供了许多直接应用于统计学习问题的工具。TML软件包是第一个包含一套全面的工具和方法的R软件包，用于处理与热带凸性相关的基本计算、热带凸集的可视化，以及使用热带度量和热带投影环上的max-plus代数进行监督和无监督学习模型。主要的，TML软件包使用Hit and Run Markov chain Monte Carlo采样器与热带度量作为统计推断的主要工具。除了基本计算和热带HAR采样器的各种应用之外，我们还关注TML软件包中包含的几种监督和无监督方法，包括热带主成分分析、热带逻辑回归和热带核密度估计。

    In the last decade, developments in tropical geometry have provided a number of uses directly applicable to problems in statistical learning. The TML package is the first R package which contains a comprehensive set of tools and methods used for basic computations related to tropical convexity, visualization of tropically convex sets, as well as supervised and unsupervised learning models using the tropical metric under the max-plus algebra over the tropical projective torus. Primarily, the TML package employs a Hit and Run Markov chain Monte Carlo sampler in conjunction with the tropical metric as its main tool for statistical inference. In addition to basic computation and various applications of the tropical HAR sampler, we also focus on several supervised and unsupervised methods incorporated in the TML package including tropical principal component analysis, tropical logistic regression and tropical kernel density estimation.
    
[^75]: 存在性颗粒的代数、拓扑和细部学基础

    Algebraic, Topological, and Mereological Foundations of Existential Granules. (arXiv:2308.16157v1 [cs.LO])

    [http://arxiv.org/abs/2308.16157](http://arxiv.org/abs/2308.16157)

    本研究从代数、拓扑和细部学的角度创造了新的存在性颗粒概念，并刻画了其特征。这些颗粒首先确定自己，然后与环境互动，并且适用于多种颗粒计算理论框架。研究结果对算法开发、分类问题应用和方法推广的数学基础具有重要意义。

    

    在这项研究中，发明了确定自己的存在性颗粒的新概念，并从代数、拓扑和细部学的角度对其进行了刻画。存在性颗粒是那些最初确定自己，并随后与其环境进行交互的颗粒。这个概念的示例，比如颗粒球，在之前其他人的作品中虽然定义不完备、算法建立不充分、理论化不足，但已经在粗糙集和软计算的应用中使用。研究表明它们适合于颗粒计算的多个理论框架（公理化、适应性等）。这种刻画旨在用于算法开发、分类问题的应用以及可能的方法推广的数学基础。此外，还提出了许多开放问题并提供了方向。

    In this research, new concepts of existential granules that determine themselves are invented, and are characterized from algebraic, topological, and mereological perspectives. Existential granules are those that determine themselves initially, and interact with their environment subsequently. Examples of the concept, such as those of granular balls, though inadequately defined, algorithmically established, and insufficiently theorized in earlier works by others, are already used in applications of rough sets and soft computing. It is shown that they fit into multiple theoretical frameworks (axiomatic, adaptive, and others) of granular computing. The characterization is intended for algorithm development, application to classification problems and possible mathematical foundations of generalizations of the approach. Additionally, many open problems are posed and directions provided.
    
[^76]: 使用大型语言模型进行文本风格转换评估

    Text Style Transfer Evaluation Using Large Language Models. (arXiv:2308.13577v1 [cs.CL])

    [http://arxiv.org/abs/2308.13577](http://arxiv.org/abs/2308.13577)

    大型语言模型（LLMs）有潜力成为人工评估和其他自动化评价指标的可行替代方案。

    

    文本风格转换（TST）的评估具有挑战性，因为生成文本的质量表现在多个方面，每个方面都很难单独衡量：风格转换准确性、内容保留和整体流畅性。人工评估是TST评估的黄金标准，然而，它费时费力，并且结果难以重复。许多自动化指标被用于评估这些方面的性能，作为人工评估的替代品。然而，许多自动化指标与人工评估之间的相关性仍然不清楚，对它们作为可靠基准的效果产生了怀疑。最近大型语言模型（LLMs）的进展已经证明了它们不仅能够匹配，而且在各种未见任务中还能超过平均人类表现。这表明LLMs有潜力成为人工评估和其他自动化指标的可行替代方案。我们评估了...

    Text Style Transfer (TST) is challenging to evaluate because the quality of the generated text manifests itself in multiple aspects, each of which is hard to measure individually: style transfer accuracy, content preservation, and overall fluency of the text. Human evaluation is the gold standard in TST evaluation; however, it is expensive, and the results are difficult to reproduce. Numerous automated metrics are employed to assess performance in these aspects, serving as substitutes for human evaluation. However, the correlation between many of these automated metrics and human evaluations remains unclear, raising doubts about their effectiveness as reliable benchmarks. Recent advancements in Large Language Models (LLMs) have demonstrated their ability to not only match but also surpass the average human performance across a wide range of unseen tasks. This suggests that LLMs have the potential to serve as a viable alternative to human evaluation and other automated metrics. We asses
    
[^77]: Hyperscanning EEG的功能性图对比学习揭示了基于刻板印象的压力引发的情绪传染

    Functional Graph Contrastive Learning of Hyperscanning EEG Reveals Emotional Contagion Evoked by Stereotype-Based Stressors. (arXiv:2308.13546v1 [eess.SP])

    [http://arxiv.org/abs/2308.13546](http://arxiv.org/abs/2308.13546)

    本研究通过利用超扫描技术，引入功能性图对比学习方法探究基于刻板印象的压力引发的情绪传染。研究结果揭示了情绪传染与认知功能之间的复杂相互作用。

    

    本研究深入探讨情绪传染的细微差异及其对双人互动中表现的影响。具体而言，研究聚焦于女性对的合作解决问题任务中基于刻板印象的压力背景。通过对情绪传染的研究，旨在揭示其潜在机制和影响。利用基于EEG的超扫描技术，本研究引入了一种名为功能性图对比学习（fGCL）的创新方法，提取主体不变的神经活动模式表示。这些表示进一步应用动态图分类（DGC）模型进行分析，旨在剖析情绪传染的过程。通过对脑部同步和连接性的研究，揭示了情绪传染与认知功能之间的复杂相互作用。结果强调情绪传染在塑造轨迹中的重要作用。

    This study delves into the intricacies of emotional contagion and its impact on performance within dyadic interactions. Specifically, it focuses on the context of stereotype-based stress (SBS) during collaborative problem-solving tasks among female pairs. Through an exploration of emotional contagion, the research seeks to unveil its underlying mechanisms and effects. Leveraging EEG-based hyperscanning technology, the study introduces an innovative approach known as functional Graph Contrastive Learning (fGCL), which extracts subject-invariant representations of neural activity patterns. These representations are further subjected to analysis using the Dynamic Graph Classification (DGC) model, aimed at dissecting the process of emotional contagion. By scrutinizing brain synchronization and connectivity, the study reveals the intricate interplay between emotional contagion and cognitive functioning. The results underscore the substantial role of emotional contagion in shaping the trajec
    
[^78]: 多事件视频文本检索

    Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])

    [http://arxiv.org/abs/2308.11551](http://arxiv.org/abs/2308.11551)

    本研究引入了多事件视频文本检索（MeVTR）任务，解决了传统视频文本检索任务中的一种特殊场景，即每个视频包含多个不同事件的情况。

    

    视频文本检索（VTR）是互联网上海量视频文本数据时代中一项关键的多模态任务。使用双流视觉-语言模型架构学习视频文本对的联合表示成为VTR任务中一种突出的方法。然而，这些模型在假设视频文本对应是双射的情况下运行，并忽视了更实际的情况，即视频内容通常涵盖多个事件，而用户查询或网页元数据等文本往往是具体的，并对应单个事件。这造成了之前的训练目标与实际应用之间的差距，在推理过程中可能导致早期模型的性能下降。本研究引入了多事件视频文本检索（MeVTR）任务，针对每个视频包含多个不同事件的场景，作为传统视频文本检索任务的一个利基场景。

    Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
    
[^79]: 行动分割需要多少长期时间上下文？

    How Much Temporal Long-Term Context is Needed for Action Segmentation?. (arXiv:2308.11358v1 [cs.CV])

    [http://arxiv.org/abs/2308.11358](http://arxiv.org/abs/2308.11358)

    本文提出了一种基于transformer的模型，利用稀疏注意力捕捉视频的完整上下文，以回答时间行动分割需要多少长期时间上下文。通过与当前最先进的方法进行比较，在三个时间行动分割数据集上取得了良好的性能。

    

    在视频中建模长期上下文对于许多细粒度任务包括时间行动分割至关重要。一个有趣的问题是，为了达到最佳性能，需要多少长期时间上下文仍然是一个未解之谜。虽然transformers可以对视频的长期上下文进行建模，但对于长视频，这在计算上是不可行的。因此，最近关于时间行动分割的研究结合了使用局部时间窗口计算出的自注意力的时间卷积网络。虽然这些方法显示出良好的结果，但它们的性能受到无法捕捉视频的完整上下文的限制。在这项工作中，我们通过引入基于transformer的模型并利用稀疏注意力来捕捉视频的完整上下文，试图回答需要多少长期时间上下文才能进行时间行动分割。我们将我们的模型与目前的三个数据集上的时间行动分割的最新技术水平进行比较，这三个数据集包括50Salads，Brea...

    Modeling long-term context in videos is crucial for many fine-grained tasks including temporal action segmentation. An interesting question that is still open is how much long-term temporal context is needed for optimal performance. While transformers can model the long-term context of a video, this becomes computationally prohibitive for long videos. Recent works on temporal action segmentation thus combine temporal convolutional networks with self-attentions that are computed only for a local temporal window. While these approaches show good results, their performance is limited by their inability to capture the full context of a video. In this work, we try to answer how much long-term temporal context is required for temporal action segmentation by introducing a transformer-based model that leverages sparse attention to capture the full context of a video. We compare our model with the current state of the art on three datasets for temporal action segmentation, namely 50Salads, Brea
    
[^80]: 使用CLIP进行随机词语数据增强的零样本异常检测方法

    Random Word Data Augmentation with CLIP for Zero-Shot Anomaly Detection. (arXiv:2308.11119v1 [cs.CV])

    [http://arxiv.org/abs/2308.11119](http://arxiv.org/abs/2308.11119)

    本文提出了一种利用CLIP作为数据源的零样本异常检测方法，通过随机词语数据增强的方式改善了训练效率，并应用prompt-guided分类进行图像的检测。该方法克服了以往需为每个对象类别训练模型的低效问题，有潜力应用于工业领域。

    

    本文提出了一种新颖的方法，利用视觉-语言模型CLIP作为数据源进行零样本异常检测。由于潜在的工业应用，人们已经付出了大量的努力来开发异常检测器。考虑到获取各种异常样本以用于训练的困难，大多数现有方法仅用正常样本训练模型，并在推理过程中从正常样本的分布中测量其差异，这需要为每个对象类别训练一个模型。为了解决这种低效的训练需求，设计了一种基于CLIP的异常检测器，它以滑动窗口的方式对图像的每个部分应用prompt-guided分类。然而，该方法仍然受到以已知对象类别仔细组合提示的劳动力的影响。为了解决以上问题，我们提出利用CLIP作为训练的数据源。我们的方法使用CLI中的文本编码器生成文本嵌入。

    This paper presents a novel method that leverages a visual-language model, CLIP, as a data source for zero-shot anomaly detection. Tremendous efforts have been put towards developing anomaly detectors due to their potential industrial applications. Considering the difficulty in acquiring various anomalous samples for training, most existing methods train models with only normal samples and measure discrepancies from the distribution of normal samples during inference, which requires training a model for each object category. The problem of this inefficient training requirement has been tackled by designing a CLIP-based anomaly detector that applies prompt-guided classification to each part of an image in a sliding window manner. However, the method still suffers from the labor of careful prompt ensembling with known object categories. To overcome the issues above, we propose leveraging CLIP as a data source for training. Our method generates text embeddings with the text encoder in CLI
    
[^81]: 在线持续学习的综合实证评估

    A Comprehensive Empirical Evaluation on Online Continual Learning. (arXiv:2308.10328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10328](http://arxiv.org/abs/2308.10328)

    这项综合实证评估了解决在线持续学习问题的各种方法，发现大多数方法存在稳定性和欠拟合问题，但所学习的表示与独立同分布的训练相当。

    

    在线持续学习旨在通过在数据流上直接学习，处理时间变化的分布，并仅存储一小部分数据，以更接近实时学习体验。在这个实证评估中，我们评估了文献中解决在线持续学习问题的各种方法。具体而言，我们在图像分类的类增量设置下，从数据流中逐步学习新的类别。我们在Split-CIFAR100和Split-TinyImagenet基准上比较了这些方法，并测量它们的平均准确性、遗忘率、稳定性和表示质量，以评估算法在整个训练过程中的多个方面。我们发现大多数方法都存在稳定性和欠拟合问题。然而，在相同的计算预算下，所学习的表示与独立同分布的训练相当。没有明确的优胜者从重新评估后的结果中出现。

    Online continual learning aims to get closer to a live learning experience by learning directly on a stream of data with temporally shifting distribution and by storing a minimum amount of data from that stream. In this empirical evaluation, we evaluate various methods from the literature that tackle online continual learning. More specifically, we focus on the class-incremental setting in the context of image classification, where the learner must learn new classes incrementally from a stream of data. We compare these methods on the Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their average accuracy, forgetting, stability, and quality of the representations, to evaluate various aspects of the algorithm at the end but also during the whole training period. We find that most methods suffer from stability and underfitting issues. However, the learned representations are comparable to i.i.d. training under the same computational budget. No clear winner emerges from the re
    
[^82]: GPFL: 同时学习全局和个性化特征信息以实现个性化联邦学习

    GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning. (arXiv:2308.10279v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.10279](http://arxiv.org/abs/2308.10279)

    这里是中文总结出的一句话要点：GPFL是一种新的个性化联邦学习方法，它能够同时学习全局和个性化的特征信息，在效果、可扩展性、公平性、稳定性和隐私性方面优于其他方法，并减轻了过拟合现象，提升了准确度。

    

    联邦学习（FL）因其保护隐私和协作学习的能力而受到欢迎。最近，个性化FL（pFL）因其能够解决统计异质性并在FL中实现个性化而受到关注。然而，从特征提取的角度来看，大多数现有的pFL方法只关注在本地训练过程中提取全局或个性化的特征信息，这无法满足pFL的协作学习和个性化目标。为了解决这个问题，我们提出了一种新的pFL方法，名为GPFL，用于在每个客户端上同时学习全局和个性化的特征信息。我们在三种统计异质的设置下对六个数据集进行了大量实验，并展示了GPFL在效果、可扩展性、公平性、稳定性和隐私性方面优于十种最先进方法的优越性。此外，GPFL减轻了过拟合现象，并在准确度上超过了基准线最高8.99%。

    Federated Learning (FL) is popular for its privacy-preserving and collaborative learning capabilities. Recently, personalized FL (pFL) has received attention for its ability to address statistical heterogeneity and achieve personalization in FL. However, from the perspective of feature extraction, most existing pFL methods only focus on extracting global or personalized feature information during local training, which fails to meet the collaborative learning and personalization goals of pFL. To address this, we propose a new pFL method, named GPFL, to simultaneously learn global and personalized feature information on each client. We conduct extensive experiments on six datasets in three statistically heterogeneous settings and show the superiority of GPFL over ten state-of-the-art methods regarding effectiveness, scalability, fairness, stability, and privacy. Besides, GPFL mitigates overfitting and outperforms the baselines by up to 8.99% in accuracy.
    
[^83]: 利用零射波自监督学习重建改进的多次扫描扩散加权MRI

    Improved Multi-Shot Diffusion-Weighted MRI with Zero-Shot Self-Supervised Learning Reconstruction. (arXiv:2308.05103v1 [eess.IV])

    [http://arxiv.org/abs/2308.05103](http://arxiv.org/abs/2308.05103)

    本研究提出了一种零射波自监督学习的多次扫描图像重建方法，用于改进扩散MRI，通过深度学习和虚拟线圈的应用，能够克服多次扫描中的相位变化问题，并提高图像重建质量。

    

    扩散MRI通常使用回波平面成像（EPI）进行，因为其采集时间快。然而，扩散加权图像的分辨率常常受到磁场不均匀性相关伪影以及T2和T2*弛豫效应引起的模糊影响。为了解决这些局限，常常采用多次扫描EPI（msEPI）与并行成像技术相结合。然而，由于多个扫描之间的相位变化，重建msEPI可能具有挑战性。在本研究中，我们引入了一种新颖的msEPI重建方法，称为zero-MIRID（零射波自监督学习多次扫描图像重建改进扩散MRI）。该方法通过结合基于深度学习的图像正则化技术来联合重建msEPI数据。该网络在k空间和图像空间中都使用CNN去噪器，并利用虚拟线圈来增强图像重建条件。通过采用自监督学习技术和分割

    Diffusion MRI is commonly performed using echo-planar imaging (EPI) due to its rapid acquisition time. However, the resolution of diffusion-weighted images is often limited by magnetic field inhomogeneity-related artifacts and blurring induced by T2- and T2*-relaxation effects. To address these limitations, multi-shot EPI (msEPI) combined with parallel imaging techniques is frequently employed. Nevertheless, reconstructing msEPI can be challenging due to phase variation between multiple shots. In this study, we introduce a novel msEPI reconstruction approach called zero-MIRID (zero-shot self-supervised learning of Multi-shot Image Reconstruction for Improved Diffusion MRI). This method jointly reconstructs msEPI data by incorporating deep learning-based image regularization techniques. The network incorporates CNN denoisers in both k- and image-spaces, while leveraging virtual coils to enhance image reconstruction conditioning. By employing a self-supervised learning technique and divi
    
[^84]: 通过未见过的状态增强利用广义化在离线强化学习中

    Exploiting Generalization in Offline Reinforcement Learning via Unseen State Augmentations. (arXiv:2308.03882v1 [cs.LG])

    [http://arxiv.org/abs/2308.03882](http://arxiv.org/abs/2308.03882)

    本文提出了一种利用未见状态增强的策略，在离线强化学习中通过基于价值的扰动和过滤，实现了对离线数据之外的状态的利用和泛化。

    

    离线强化学习方法通过对未见过的状态和动作进行保守价值评估来平衡探索和利用。无模型方法会对所有未见过的动作进行惩罚，而有模型方法可以进一步通过模型展开对未见过的状态进行利用。然而，由于两个因素，这些方法在找到离线数据之外的未见过的状态时存在困难：(a)由于级联模型误差，模型的展开范围非常短，(b)模型展开仅以离线数据中观察到的状态为起点。我们放宽了第二个假设，并提出了一种新颖的未见过状态增强策略，以允许学得的模型和价值估计在未见状态中泛化。我们的策略通过对观察到的状态进行基于价值的扰动来找到未见过的状态，然后通过过滤具有过高的启发性不确定性估计（高误差）或过低的（过于相似）

    Offline reinforcement learning (RL) methods strike a balance between exploration and exploitation by conservative value estimation -- penalizing values of unseen states and actions. Model-free methods penalize values at all unseen actions, while model-based methods are able to further exploit unseen states via model rollouts. However, such methods are handicapped in their ability to find unseen states far away from the available offline data due to two factors -- (a) very short rollout horizons in models due to cascading model errors, and (b) model rollouts originating solely from states observed in offline data. We relax the second assumption and present a novel unseen state augmentation strategy to allow exploitation of unseen states where the learned model and value estimates generalize. Our strategy finds unseen states by value-informed perturbations of seen states followed by filtering out states with epistemic uncertainty estimates too high (high error) or too low (too similar to
    
[^85]: FLARE: 使用通用对抗性掩码对指纹深度强化学习智能体进行识别

    FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks. (arXiv:2307.14751v1 [cs.LG])

    [http://arxiv.org/abs/2307.14751](http://arxiv.org/abs/2307.14751)

    FLARE是第一个用于验证疑似深度强化学习策略是否是另一个策略的非法副本的指纹机制，通过使用通用对抗性掩码作为指纹，并测量动作一致性值来验证被盗策略的真实所有权。

    

    我们提出了FLARE，这是第一个用于验证疑似深度强化学习(DRL)策略是否是另一个（受害）策略的非法副本的指纹机制。我们首先展示了通过找到不可传递的、通用的对抗性掩码，即扰动，可以生成成功地从受害策略传递到其修改版本但不能传递到独立训练的策略的对抗性样本。FLARE利用这些掩码作为指纹，通过对通过掩码扰动的状态上的动作一致性值进行测量来验证被盗的DRL策略的真实所有权。我们的实证评估表明，FLARE是有效的（对于被盗副本具有100%的动作一致性），并且不会错误地指控独立策略（无误报）。FLARE还对模型修改攻击具有鲁棒性，并且不容易被更明智的对手规避而对智能体性能产生负面影响。我们还表明，并非所有的通用对抗性掩码都是适用的。

    We propose FLARE, the first fingerprinting mechanism to verify whether a suspected Deep Reinforcement Learning (DRL) policy is an illegitimate copy of another (victim) policy. We first show that it is possible to find non-transferable, universal adversarial masks, i.e., perturbations, to generate adversarial examples that can successfully transfer from a victim policy to its modified versions but not to independently trained policies. FLARE employs these masks as fingerprints to verify the true ownership of stolen DRL policies by measuring an action agreement value over states perturbed via such masks. Our empirical evaluations show that FLARE is effective (100% action agreement on stolen copies) and does not falsely accuse independent policies (no false positives). FLARE is also robust to model modification attacks and cannot be easily evaded by more informed adversaries without negatively impacting agent performance. We also show that not all universal adversarial masks are suitable 
    
[^86]: 模仿复杂轨迹：桥接低层稳定性与高层行为

    Imitating Complex Trajectories: Bridging Low-Level Stability and High-Level Behavior. (arXiv:2307.14619v1 [cs.LG])

    [http://arxiv.org/abs/2307.14619](http://arxiv.org/abs/2307.14619)

    本文提出了一个理论框架，研究了在非线性动态系统中模仿复杂专家演示的行为。通过稳定模仿策略并确保准确估计演示者分布，可以使模仿者与演示者的轨迹分布相近。

    

    我们提出了一个理论框架来研究在非线性动态系统中模仿随机、非马尔可夫、潜在多模态（即“复杂”）专家演示的行为。我们的框架使用低层控制器（无论是学习的还是隐含的）来稳定围绕专家演示的模仿策略。我们证明，在（a）合适的低层稳定性保证和（b）学习策略的随机连续性属性（我们称之为“总变差连续性”）（TVC）的情况下，一个精确估计演示者状态分布上的行动的模仿者会与演示者对整个轨迹的分布相近。然后，我们证明可以通过将流行的数据增强规则与一种新颖的算法技巧相结合（即在执行时添加增强噪声）来确保TVC并且最小程度上降低精度。我们将我们的保证实例化为由扩散模型参数化的策略，并证明如果学习者准确地估计了演示者的分布，则最终完成这种实例化。

    We propose a theoretical framework for studying the imitation of stochastic, non-Markovian, potentially multi-modal (i.e. "complex" ) expert demonstrations in nonlinear dynamical systems. Our framework invokes low-level controllers either learned or implicit in position-command control - to stabilize imitation policies around expert demonstrations. We show that with (a) a suitable low-level stability guarantee and (b) a stochastic continuity property of the learned policy we call "total variation continuity" (TVC), an imitator that accurately estimates actions on the demonstrator's state distribution closely matches the demonstrator's distribution over entire trajectories. We then show that TVC can be ensured with minimal degradation of accuracy by combining a popular data-augmentation regimen with a novel algorithmic trick: adding augmentation noise at execution time. We instantiate our guarantees for policies parameterized by diffusion models and prove that if the learner accuratel
    
[^87]: 通过在霍夫空间中采用新的SR损失函数进行增强的迁移学习的飞行轨迹划分

    Flight Contrail Segmentation via Augmented Transfer Learning with Novel SR Loss Function in Hough Space. (arXiv:2307.12032v1 [cs.CV])

    [http://arxiv.org/abs/2307.12032](http://arxiv.org/abs/2307.12032)

    本论文引入创新的增强迁移学习模型，以及通过在霍夫空间中使用新的SR损失函数改善飞行轨迹线检测的方法。这些方法能够准确地检测飞行轨迹并且对数据需求较少，为基于机器学习的航空研究中的飞行轨迹检测提供了新的解决方案。

    

    空中交通对环境产生了重大挑战，特别是飞行轨迹造成的对气候变化的贡献，由于其潜在的全球变暖影响。从卫星图像中检测飞行轨迹一直以来都是一个长期存在的难题。传统的计算机视觉技术在不同的图像条件下存在局限性，而使用典型的卷积神经网络的机器学习方法受到手工标注的飞行轨迹数据集和专门的学习过程的限制。在本文中，我们引入了一种创新的模型，基于增强的迁移学习，能够准确地检测飞行轨迹，并且数据需求较少。我们还提出了一种新的损失函数，SR Loss，通过将图像空间转换为霍夫空间来改善飞行轨迹线的检测。我们的研究为基于机器学习的航空研究中的飞行轨迹检测打开了新的途径，提供了解决大型手工标注数据集缺乏问题，并显著增强了飞行轨迹检测能力。

    Air transport poses significant environmental challenges, particularly the contribution of flight contrails to climate change due to their potential global warming impact. Detecting contrails from satellite images has been a long-standing challenge. Traditional computer vision techniques have limitations under varying image conditions, and machine learning approaches using typical convolutional neural networks are hindered by the scarcity of hand-labeled contrail datasets and contrail-tailored learning processes. In this paper, we introduce an innovative model based on augmented transfer learning that accurately detects contrails with minimal data. We also propose a novel loss function, SR Loss, which improves contrail line detection by transforming the image space into Hough space. Our research opens new avenues for machine learning-based contrail detection in aviation research, offering solutions to the lack of large hand-labeled datasets, and significantly enhancing contrail detecti
    
[^88]: MASR: 多标签感知语音表示

    MASR: Multi-label Aware Speech Representation. (arXiv:2307.10982v2 [cs.SD] UPDATED)

    [http://arxiv.org/abs/2307.10982](http://arxiv.org/abs/2307.10982)

    MASR是一种多标签感知语音表示学习框架，可以利用多个外部知识源增强元数据信息的利用，并在多个下游任务上展示了显著的性能提升。

    

    近年来，语音表示学习主要以自监督学习任务为基础，仅使用原始音频信号，忽略了通常可用于给定语音记录的附加信息。在本文中，我们提出了MASR，一种多标签感知语音表示学习框架，以解决上述限制。MASR能够引入多个外部知识源，增强元数据信息的利用。外部知识源以样本级成对相似性矩阵的形式被纳入到一个硬挖掘损失函数中。MASR框架的一个关键优势是它可以与任何选择的自监督学习方法相结合。利用MASR表示，我们在多个下游任务上进行评估，如语言识别、语音识别以及说话人和情感识别等非语义任务。在这些实验中，我们展示了显著的性能提升。

    In the recent years, speech representation learning is constructed primarily as a self-supervised learning (SSL) task, using the raw audio signal alone, while ignoring the side-information that is often available for a given speech recording. In this paper, we propose MASR, a Multi-label Aware Speech Representation learning framework, which addresses the aforementioned limitations. MASR enables the inclusion of multiple external knowledge sources to enhance the utilization of meta-data information. The external knowledge sources are incorporated in the form of sample-level pair-wise similarity matrices that are useful in a hard-mining loss. A key advantage of the MASR framework is that it can be combined with any choice of SSL method. Using MASR representations, we perform evaluations on several downstream tasks such as language identification, speech recognition and other non-semantic tasks such as speaker and emotion recognition. In these experiments, we illustrate significant perfor
    
[^89]: 差分私有的解耦图卷积用于多粒度拓扑保护

    Differentially Private Decoupled Graph Convolutions for Multigranular Topology Protection. (arXiv:2307.06422v1 [cs.LG])

    [http://arxiv.org/abs/2307.06422](http://arxiv.org/abs/2307.06422)

    本论文提出了一种差分私有的解耦图卷积方法，用于多粒度拓扑保护。引入了图差分隐私框架，可以确保模型参数和预测的私密性。

    

    图学习方法，如基于图卷积的图神经网络（GNNs），在解决涉及图结构数据的实际学习问题方面非常成功。然而，图学习方法不仅通过其模型参数，还通过其模型预测暴露了敏感的用户信息和交互。因此，仅提供模型权重隐私的标准差分隐私（DP）技术是不充分的。这尤其适用于通过图卷积直接利用相邻节点属性进行节点预测的情况，这会带来额外的隐私泄露风险。为了解决这个问题，我们引入了图差分隐私（GDP），这是一个新的适用于图学习环境的形式化差分隐私框架，可以确保模型参数和预测都是可证明的私有的。此外，由于节点属性和图结构可能存在不同的隐私要求，我们引入了一种新颖的放松的节点层次隐私概念。

    Graph learning methods, such as Graph Neural Networks (GNNs) based on graph convolutions, are highly successful in solving real-world learning problems involving graph-structured data. However, graph learning methods expose sensitive user information and interactions not only through their model parameters but also through their model predictions. Consequently, standard Differential Privacy (DP) techniques that merely offer model weight privacy are inadequate. This is especially the case for node predictions that leverage neighboring node attributes directly via graph convolutions that create additional risks of privacy leakage. To address this problem, we introduce Graph Differential Privacy (GDP), a new formal DP framework tailored to graph learning settings that ensures both provably private model parameters and predictions. Furthermore, since there may be different privacy requirements for the node attributes and graph structure, we introduce a novel notion of relaxed node-level da
    
[^90]: 洋葱宇宙算法：在弱监督学习中的应用

    Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])

    [http://arxiv.org/abs/2307.04870](http://arxiv.org/abs/2307.04870)

    洋葱宇宙算法是一种新颖的集成学习分类方法，可作为弱监督学习的标签模型。它在实现上简单、计算高效，适用于无完全标记数据的情况。经验证明，它在常见的基准数据集上表现出色。

    

    本文介绍了洋葱宇宙算法(OUA)，一种新颖的集成学习分类方法。特别地，我们展示了它作为弱监督学习标签模型的适用性。OUA在实现上简单，计算效率高，并且不依赖于数据或弱信号的任何假设。该模型非常适用于没有完全标记数据的情况。我们的方法基于对由弱信号所构成的空间的几何解释。经验证实，OUA在一般的弱信号集合下具有潜在的几何结构，并且在实践中表现良好。我们还通过实验证据展示，OUA在常见的基准数据集上相比现有的弱监督学习标签模型表现出色。

    We introduce Onion Universe Algorithm (OUA), a novel classification method in ensemble learning. In particular, we show its applicability as a label model for weakly supervised learning. OUA offers simplicity in implementation, computational efficiency, and does not rely on any assumptions regarding the data or weak signals. The model is well suited for scenarios where fully labeled data is not available. Our method is built upon geometrical interpretation of the space spanned by weak signals. Empirical results support our analysis of the hidden geometric structure underlying general set of weak signals and also illustrates that OUA works well in practice. We show empirical evidence that OUA performs favorably on common benchmark datasets compared to existing label models for weakly supervised learning.
    
[^91]: 虚假的安全感：利用可解释的人工智能分析无上下文DGA分类器的推理和真实性能

    False Sense of Security: Leveraging XAI to Analyze the Reasoning and True Performance of Context-less DGA Classifiers. (arXiv:2307.04358v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2307.04358](http://arxiv.org/abs/2307.04358)

    本研究利用可解释的人工智能方法分析了基于深度学习的DGA分类器的推理过程，并揭示了其中的偏见。通过消除这些偏见，我们设计了一个无偏见且上下文感知的检测系统，保持了最先进分类器的检测率，同时提出了一个视觉分析系统，以增加对检测方法的信任和透明度。

    

    通过域生成算法（DGA）检测揭示僵尸网络活动的问题似乎已经解决，因为现有的深度学习分类器的准确率超过99.9%。然而，这些分类器提供了一种虚假的安全感，因为它们存在严重的偏见，容易被绕过。本研究利用可解释的人工智能方法分析深度学习分类器的推理过程，并系统地揭示这些偏见。我们发现，消除DGA分类器的这些偏见会显著降低其性能。然而，我们能够设计一个无偏见的上下文感知检测系统，同时保持最先进的深度学习分类器的检测率。在这个背景下，我们提出了一个视觉分析系统，帮助更好地理解分类器的推理过程，从而增加对检测方法的信任和透明度，并促进决策制定。

    The problem of revealing botnet activity through Domain Generation Algorithm (DGA) detection seems to be solved, considering that available deep learning classifiers achieve accuracies of over 99.9%. However, these classifiers provide a false sense of security as they are heavily biased and allow for trivial detection bypass. In this work, we leverage explainable artificial intelligence (XAI) methods to analyze the reasoning of deep learning classifiers and to systematically reveal such biases. We show that eliminating these biases from DGA classifiers considerably deteriorates their performance. Nevertheless we are able to design a context-aware detection system that is free of the identified biases and maintains the detection rate of state-of-the art deep learning classifiers. In this context, we propose a visual analysis system that helps to better understand a classifier's reasoning, thereby increasing trust in and transparency of detection methods and facilitating decision-making.
    
[^92]: 学习使用对比学习进行通信

    Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])

    [http://arxiv.org/abs/2307.01403](http://arxiv.org/abs/2307.01403)

    本研究提出了一种使用对比学习进行通信的方法，在分散的环境中通过最大化反映发送和接收消息关系的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作，并且能够捕获全局状态信息，实现了更对称的通信。

    

    通信是多智能体强化学习中协调的有力工具。但在分散的环境中诱导一个有效的共同语言是一个困难的挑战。在这项工作中，我们引入了一个替代视角，即将智能体之间发送的通信消息视为环境状态的不完整视图。通过检查发送和接收的消息之间的关系，我们提出使用对比学习来最大化给定轨迹的消息之间的互信息来学习通信。在通信关键的环境中，我们的方法在性能和学习速度方面优于先前的工作。使用定性指标和表示探测，我们展示了我们的方法诱导了更对称的通信并从环境中捕获了全局状态信息。总体而言，我们展示了对比学习的力量以及利用消息作为编码实现有效通信的重要性。

    Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
    
[^93]: MoVie: 基于视觉模型的策略自适应用于视图泛化

    MoVie: Visual Model-Based Policy Adaptation for View Generalization. (arXiv:2307.00972v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.00972](http://arxiv.org/abs/2307.00972)

    本论文提出了一种名为MoVie的方法，通过基于视觉模型的策略自适应实现了视图泛化。该方法在不需要显式奖励信号和训练过程修改的情况下，在多个实际场景中表现出卓越的性能，相对改进达到了33%至152%。

    

    在有限的视图上训练的视觉强化学习（RL）智能体在将其学到的能力推广到未见过的视图时面临着巨大的挑战。这个固有的困难被称为$\textit{view generalization}$问题。在这项工作中，我们将这个基本问题系统地分为四个不同的，高度具有挑战性的情景，这些情景与实际情况很相似。随后，我们提出了一个简单而有效的方法，在测试时使基于视觉的$\textbf{Mo}$del-based策略能够成功适应$\textbf{Vie}$w generalization ($\textbf{MoVie}$)，而无需显式的奖励信号和训练过程中的任何修改。我们的方法在来自DMControl、xArm和Adroit的总共$\textbf{18}$个任务中的所有四个情景中展示了显著进展，相对改进分别为$\mathbf{33}$%，$\mathbf{86}$%和$\mathbf{152}$%。优越的结果凸显出其巨大的潜力。

    Visual Reinforcement Learning (RL) agents trained on limited views face significant challenges in generalizing their learned abilities to unseen views. This inherent difficulty is known as the problem of $\textit{view generalization}$. In this work, we systematically categorize this fundamental problem into four distinct and highly challenging scenarios that closely resemble real-world situations. Subsequently, we propose a straightforward yet effective approach to enable successful adaptation of visual $\textbf{Mo}$del-based policies for $\textbf{Vie}$w generalization ($\textbf{MoVie}$) during test time, without any need for explicit reward signals and any modification during training time. Our method demonstrates substantial advancements across all four scenarios encompassing a total of $\textbf{18}$ tasks sourced from DMControl, xArm, and Adroit, with a relative improvement of $\mathbf{33}$%, $\mathbf{86}$%, and $\mathbf{152}$% respectively. The superior results highlight the immens
    
[^94]: ECG-QA：结合心电图的综合问答数据集

    ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram. (arXiv:2306.15681v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.15681](http://arxiv.org/abs/2306.15681)

    ECG-QA是第一个专为心电图分析设计的问答数据集，包括涵盖广泛临床相关ECG主题的问题模板和多样化的ECG解读问题。这一资源将为未来的医疗保健问答研究提供有价值的见解。

    

    由于自然语言处理的显著进展，医疗保健领域中的问答问题（QA）引起了广泛关注。然而，现有的医疗保健QA数据集主要集中在医学影像、临床记录或结构化的电子健康记录表上。这使得将心电图（ECG）数据与这些系统相结合的巨大潜力几乎未被利用。为填补这一空白，我们提出了ECG-QA，这是专门针对ECG分析设计的第一个QA数据集。该数据集包括共70个涵盖了广泛临床相关ECG主题的问题模板，每个问题都经过一名ECG专家的验证，以确保其临床效用。因此，我们的数据集包含了多样化的ECG解读问题，包括需要对两个不同的ECG进行比较分析的问题。此外，我们还进行了许多实验，为未来的研究方向提供了有价值的见解。我们相信ECG-QA将成为一个宝贵的资源，供研究者们探索和应用。

    Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the de
    
[^95]: 模拟反事实情况

    Simulating counterfactuals. (arXiv:2306.15328v1 [stat.ML])

    [http://arxiv.org/abs/2306.15328](http://arxiv.org/abs/2306.15328)

    该论文提出了一种算法，可以模拟反事实分布中的值，可对离散和连续变量设定条件，并应用于信用评分中的公平性分析。

    

    反事实推断考虑了在与实际世界存在一些证据的平行世界中进行的假设性干预。如果证据在流形上指定了条件分布，反事实可能是解析难解的。我们提出了一种算法，用于从反事实分布中模拟值，其中可以对离散和连续变量设定条件。我们表明，所提出的算法可以被呈现为粒子滤波器，从而导致渐近有效的推断。该算法被应用于信用评分中的公平性分析。

    Counterfactual inference considers a hypothetical intervention in a parallel world that shares some evidence with the factual world. If the evidence specifies a conditional distribution on a manifold, counterfactuals may be analytically intractable. We present an algorithm for simulating values from a counterfactual distribution where conditions can be set on both discrete and continuous variables. We show that the proposed algorithm can be presented as a particle filter leading to asymptotically valid inference. The algorithm is applied to fairness analysis in credit scoring.
    
[^96]: 多样社区数据用于数据隐私算法基准测试

    Diverse Community Data for Benchmarking Data Privacy Algorithms. (arXiv:2306.13216v1 [cs.CR])

    [http://arxiv.org/abs/2306.13216](http://arxiv.org/abs/2306.13216)

    多样社区数据摘要旨在为隐私保护机器学习研究提供真实、多样和复杂的基准数据，以解决合成数据的偏差和隐私问题。

    

    多样社区数据是美国国家标准和技术研究所（NIST）计划的核心，旨在增强对表格数据去识别技术（如合成数据）的理解。合成数据是民主化大数据利益的一项雄心勃勃的尝试；它使用生成模型重新创建敏感个人数据，以便公开发布。然而，它容易受到影响其他机器学习应用程序的偏差和隐私问题的影响，甚至可能放大这些问题。当去识别数据分布引入偏差或工件，或泄漏敏感信息时，它们会将这些问题传播到下游应用。此外，真实世界的调查条件（如多样子群、异质非有序数据空间和特征之间的复杂依赖关系）对合成数据算法提出了具体挑战。这些观察结果促使需要真实、多样和复杂的基准数据来支持隐私保护的机器学习研究，而多样社区数据摘要旨在解决这些挑战。

    The Diverse Communities Data Excerpts are the core of a National Institute of Standards and Technology (NIST) program to strengthen understanding of tabular data deidentification technologies such as synthetic data. Synthetic data is an ambitious attempt to democratize the benefits of big data; it uses generative models to recreate sensitive personal data with new records for public release. However, it is vulnerable to the same bias and privacy issues that impact other machine learning applications, and can even amplify those issues. When deidentified data distributions introduce bias or artifacts, or leak sensitive information, they propagate these problems to downstream applications. Furthermore, real-world survey conditions such as diverse subpopulations, heterogeneous non-ordinal data spaces, and complex dependencies between features pose specific challenges for synthetic data algorithms. These observations motivate the need for real, diverse, and complex benchmark data to support
    
[^97]: 处理自然视觉场景神经响应的时间条件脉冲潜变量模型

    Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v1 [q-bio.NC])

    [http://arxiv.org/abs/2306.12045](http://arxiv.org/abs/2306.12045)

    本研究提出 TeCoS-LVM 模型，使用脉冲神经元以模拟自然视觉刺激的神经响应。该模型能够自适应地探索和利用刺激序列中的时间依赖关系，避免丢失脉冲列中的信息。

    

    发展神经响应的计算模型对于理解感知处理和神经计算至关重要。目前最先进的神经网络方法使用时间过滤器来处理时间依赖性，导致处理流程不现实且不灵活。同时，这些方法针对试验平均发放率，未能捕捉到脉冲列中的重要特征。本研究提出时间条件脉冲潜变量模型（TeCoS-LVM）来模拟自然视觉刺激的神经响应。我们使用脉冲神经元产生直接匹配记录脉冲列的脉冲输出。这种方法有助于避免丢失嵌入在原始脉冲列中的信息。我们从模型参数空间中排除时间维度，并引入时间条件操作，使模型能够在自然范式中自适应地探索和利用刺激序列中的时间依赖关系。我们展示了 TeCoS-LVM 模型能够产生...

    Developing computational models of neural response is crucial for understanding sensory processing and neural computations. Current state-of-the-art neural network methods use temporal filters to handle temporal dependencies, resulting in an unrealistic and inflexible processing flow. Meanwhile, these methods target trial-averaged firing rates and fail to capture important features in spike trains. This work presents the temporal conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural response to natural visual stimuli. We use spiking neurons to produce spike outputs that directly match the recorded trains. This approach helps to avoid losing information embedded in the original spike trains. We exclude the temporal dimension from the model parameter space and introduce a temporal conditioning operation to allow the model to adaptively explore and exploit temporal dependencies in stimuli sequences in a natural paradigm. We show that TeCoS-LVM models can produce m
    
[^98]: DropCompute: 通过计算方差减少分布式同步训练的简易且更稳定的方法

    DropCompute: simple and more robust distributed synchronous training via compute variance reduction. (arXiv:2306.10598v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10598](http://arxiv.org/abs/2306.10598)

    该论文提出了一种称为DropCompute的简单且稳定的分布式同步训练方法，通过减少工作节点间的计算变异性来提高训练的稳定性。

    

    背景：分布式训练对于大规模深度神经网络（DNN）的训练至关重要。目前大规模DNN训练的主流方法是同步训练（例如All-Reduce），但这些方法需要等待每一步中所有的工作节点完成。因此，这些方法受制于由延迟导致的工作节点拖延的问题。结果：本文研究了由于计算时间的变异性导致工作节点拖延的典型场景，并找到了计算时间属性和可扩展性限制之间的分析关系。基于这些发现，我们提出了一种简单而有效的分布式方法，以减少工作节点间的变异性，从而提高同步训练的稳定性。该方法可以与广泛使用的All-Reduce相结合。我们通过使用200个Gaudi加速器验证了我们的发现。

    Background: Distributed training is essential for large scale training of deep neural networks (DNNs). The dominant methods for large scale DNN training are synchronous (e.g. All-Reduce), but these require waiting for all workers in each step. Thus, these methods are limited by the delays caused by straggling workers. Results: We study a typical scenario in which workers are straggling due to variability in compute time. We find an analytical relation between compute time properties and scalability limitations, caused by such straggling workers. With these findings, we propose a simple yet effective decentralized method to reduce the variation among workers and thus improve the robustness of synchronous training. This method can be integrated with the widely used All-Reduce. Our findings are validated on large-scale training tasks using 200 Gaudi Accelerators.
    
[^99]: 在网络中学习增强的分布式在线凸优化

    Learning-Augmented Decentralized Online Convex Optimization in Networks. (arXiv:2306.10158v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10158](http://arxiv.org/abs/2306.10158)

    本论文研究了网络中的分布式在线凸优化问题，并提出了一种新的学习增强的分布式在线优化算法（LADO），该算法使个体智能体能够基于本地信息选择行动。与现有的算法不同，LADO在分布式设置中实现了强鲁棒性保证，并通过考虑鲁棒性要求来训练机器学习策略。此外，我们还证明了LADO算法的平均成本界限，揭示了平均性能和最坏情况鲁棒性之间的折衷。

    

    本文研究了网络化多智能体系统中的分布式在线凸优化，并提出了一种新的算法，即学习增强的分布式在线优化（LADO），用于使个体智能体仅基于本地在线信息选择行动。LADO利用基线策略来保障最坏情况下的鲁棒性保证，同时保持接近于机器学习（ML）策略以提高平均性能。与现有的学习增强在线算法不同，这些算法关注的是集中式设置，LADO在分布式设置中实现了强鲁棒性保证。我们还证明了LADO的平均成本界限，揭示了平均性能和最坏情况鲁棒性之间的折衷，证明了通过明确考虑鲁棒性要求来训练ML策略的优势。

    This paper studies decentralized online convex optimization in a networked multi-agent system and proposes a novel algorithm, Learning-Augmented Decentralized Online optimization (LADO), for individual agents to select actions only based on local online information. LADO leverages a baseline policy to safeguard online actions for worst-case robustness guarantees, while staying close to the machine learning (ML) policy for average performance improvement. In stark contrast with the existing learning-augmented online algorithms that focus on centralized settings, LADO achieves strong robustness guarantees in a decentralized setting. We also prove the average cost bound for LADO, revealing the tradeoff between average performance and worst-case robustness and demonstrating the advantage of training the ML policy by explicitly considering the robustness requirement.
    
[^100]: 组合和混合变量贝叶斯优化的框架和基准。 (arXiv:2306.09803v1 [cs.LG])

    Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization. (arXiv:2306.09803v1 [cs.LG])

    [http://arxiv.org/abs/2306.09803](http://arxiv.org/abs/2306.09803)

    本文介绍了一个模块化框架和基准，用于组合和混合变量贝叶斯优化，并提供多样的合成和真实世界基准测试。通过此框架，作者展示了4种常见的MCBO技术。

    

    本文介绍了一种模块化框架，用于混合变量和组合贝叶斯优化(MCBO)来解决领域中缺乏系统化基准和标准化评估的问题。目前的MCBO论文通常引入非多样性或非标准基准来评估其方法，阻碍了不同MCBO原语及其组合的正确评估。此外，介绍单个MCBO原语的论文通常省略了针对使用相同方法进行剩余原语的基线进行基准测试。这种省略主要是由于涉及的实现工作量非常大，导致缺乏控制评估并无法有效展示贡献的优点。为了克服这些挑战，我们提出的框架使贝叶斯优化组件的组合轻松易行，并提供了多样的合成和真实世界的基准测试任务。利用这种灵活性，我们实现了4种常见的MCBO技术，并在各种合成和真实基准测试中进行了评估。

    This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally, papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively. To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. Leveraging this flexibility, we implement 4
    
[^101]: 模型训练中的模块化：一种新的模块化深度神经网络的范式

    Modularizing while Training: a New Paradigm for Modularizing DNN Models. (arXiv:2306.09376v1 [cs.LG])

    [http://arxiv.org/abs/2306.09376](http://arxiv.org/abs/2306.09376)

    本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化(MwT)，通过两个损失函数实现模型结构上的模块化，进而实现模块的重用，能够在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    

    深度神经网络(DNN)模型已成为智能软件系统中越来越关键的组成部分。然而，训练DNN模型通常在时间和成本方面都很昂贵。为了解决这个问题，研究人员最近开始关注重用现有的DNN模型-借鉴软件工程中的代码重用思想。但是，重用整个模型可能会造成额外的开销或从不需要的功能中继承弱点。因此，现有的工作提出将已经训练好的模型分解成模块，即训练后的模块化，并实现模块的重用。但是，由于已经训练好的模型并不是为了模块化而构建的，所以训练后的模块化会导致巨大的开销和模型精度损失。本文提出了一种新方法，将模块化纳入模型训练过程中，即在训练时模块化（MwT）。我们通过两个损失函数在模型训练过程中使模型具有结构上的模块化能力，这两个损失函数同时优化模块内的内聚性和模块之间的独立性，从而得到一个真正的模块化模型。我们展示了我们的方法可以在较短的训练时间内达到可比较的模型精度，并且相对于最先进的训练后模块化方法需要更少的参数。

    Deep neural network (DNN) models have become increasingly crucial components in intelligent software systems. However, training a DNN model is typically expensive in terms of both time and money. To address this issue, researchers have recently focused on reusing existing DNN models - borrowing the idea of code reuse in software engineering. However, reusing an entire model could cause extra overhead or inherits the weakness from the undesired functionalities. Hence, existing work proposes to decompose an already trained model into modules, i.e., modularizing-after-training, and enable module reuse. Since trained models are not built for modularization, modularizing-after-training incurs huge overhead and model accuracy loss. In this paper, we propose a novel approach that incorporates modularization into the model training process, i.e., modularizing-while-training (MwT). We train a model to be structurally modular through two loss functions that optimize intra-module cohesion and int
    
[^102]: 重新基准化面向二分类的基于池的主动学习

    Re-Benchmarking Pool-Based Active Learning for Binary Classification. (arXiv:2306.08954v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08954](http://arxiv.org/abs/2306.08954)

    本论文通过重新基准化实验证明了不确定性采样策略仍然是大多数数据集上有效和首选的选择，并揭示了模型兼容性问题的重要性。

    

    主动学习是一种显著提升机器学习模型性能的范式，当获取标记数据代价昂贵时特别有用。尽管存在多个用于评估主动学习策略的基准测试，但它们的发现存在一定的不一致性。这种差异激发我们为社区开发一个透明且可复现的基准测试。我们的努力结果是一个可靠且可扩展用于未来研究的开源实现（https://github.com/ariapoy/active-learning-benchmark）。通过进行彻底的重新基准化实验，我们不仅纠正了现有基准测试中的误配置问题，还揭示了模型兼容性这个未被充分探索的问题，这直接导致了观察到的不一致性。解决这个差异使得不确定性采样策略保持了在大多数数据集上是一个有效且首选的选择。我们的经验强调了将研究努力投入到重新基准化上的重要性。

    Active learning is a paradigm that significantly enhances the performance of machine learning models when acquiring labeled data is expensive. While several benchmarks exist for evaluating active learning strategies, their findings exhibit some misalignment. This discrepancy motivates us to develop a transparent and reproducible benchmark for the community. Our efforts result in an open-sourced implementation (https://github.com/ariapoy/active-learning-benchmark) that is reliable and extensible for future research. By conducting thorough re-benchmarking experiments, we have not only rectified misconfigurations in existing benchmark but also shed light on the under-explored issue of model compatibility, which directly causes the observed discrepancy. Resolving the discrepancy reassures that the uncertainty sampling strategy of active learning remains an effective and preferred choice for most datasets. Our experience highlights the importance of dedicating research efforts towards re-be
    
[^103]: 如何估计训练深度学习模型的碳足迹？一份指南和综述。

    How to estimate carbon footprint when training deep learning models? A guide and review. (arXiv:2306.08323v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.08323](http://arxiv.org/abs/2306.08323)

    这篇论文提出了一份详尽的指南和综述，介绍了如何估计训练深度学习模型的碳足迹，并比较了多种在线和软件工具的能源消耗估计结果。研究为AI从业人员在选择合适的工具和基础设施方面提供了建议。

    

    机器学习和深度学习模型在社会各个领域的快速发展中变得至关重要。人们普遍认识到这些模型的发展存在环境成本，已经有很多研究对此进行了分析。已经开发了几种在线和软件工具来跟踪机器学习模型训练过程中的能源消耗。在本文中，我们提出了对这些工具进行全面介绍和比较，并针对希望开始估计其工作环境影响的AI从业人员进行了讨论。我们对每个工具的特定词汇和技术要求进行了评估，并比较了这些工具对两个用于图像处理的深度神经网络和不同类型服务器的能源消耗估计结果。根据这些实验，我们提供了一些建议，以更好地选择合适的工具和基础设施。

    Machine learning and deep learning models have become essential in the recent fast development of artificial intelligence in many sectors of the society. It is now widely acknowledge that the development of these models has an environmental cost that has been analyzed in many studies. Several online and software tools have been developed to track energy consumption while training machine learning models. In this paper, we propose a comprehensive introduction and comparison of these tools for AI practitioners wishing to start estimating the environmental impact of their work. We review the specific vocabulary, the technical requirements for each tool. We compare the energy consumption estimated by each tool on two deep neural networks for image processing and on different types of servers. From these experiments, we provide some advice for better choosing the right tool and infrastructure.
    
[^104]: 高效的量化感知训练与自适应核心集选择

    Efficient Quantization-aware Training with Adaptive Coreset Selection. (arXiv:2306.07215v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07215](http://arxiv.org/abs/2306.07215)

    本研究提出了一种用于改善量化感知训练的训练效率的方法，通过核心集选择和两个重要性指标来选择训练数据的子集。

    

    深度神经网络（DNN）的模型大小和计算量的增加，增加了对有效模型部署方法的需求。量化感知训练（QAT）是一种代表性的模型压缩方法，可以利用权重和激活中的冗余信息。然而，大多数现有的QAT方法需要在整个数据集上进行端到端训练，这会导致长时间的训练和高能耗。核心集选择是利用训练数据的冗余性提高数据效率的方法，在高效训练中被广泛应用。在这项工作中，我们提出了一种新的角度，通过核心集选择来提高量化感知训练的训练效率。基于QAT的特性，我们提出了两个指标：误差向量分数和不一致分数，用于量化训练过程中每个样本的重要性。基于这两个重要性指标，我们提出了一种量化感知的自适应核心集选择（ACS）方法，用于选择训练数据的子集。

    The expanding model size and computation of deep neural networks (DNNs) have increased the demand for efficient model deployment methods. Quantization-aware training (QAT) is a representative model compression method to leverage redundancy in weights and activations. However, most existing QAT methods require end-to-end training on the entire dataset, which suffers from long training time and high energy costs. Coreset selection, aiming to improve data efficiency utilizing the redundancy of training data, has also been widely used for efficient training. In this work, we propose a new angle through the coreset selection to improve the training efficiency of quantization-aware training. Based on the characteristics of QAT, we propose two metrics: error vector score and disagreement score, to quantify the importance of each sample during training. Guided by these two metrics of importance, we proposed a quantization-aware adaptive coreset selection (ACS) method to select the data for the
    
[^105]: PEAR: 基于原始操作的自适应重标记用于Boosting层次强化学习

    PEAR: Primitive enabled Adaptive Relabeling for boosting Hierarchical Reinforcement Learning. (arXiv:2306.06394v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.06394](http://arxiv.org/abs/2306.06394)

    PEAR是一种基于原始操作的自适应重标记方法，用于Boosting层次强化学习。它通过对专家演示进行自适应重标记来生成高效的子目标监督，并通过联合优化强化学习和模仿学习来训练分层代理。实验结果显示，PEAR能够在具有挑战性的机器人环境中取得良好的性能。

    

    层次强化学习（HRL）利用时间抽象和增加的探索性能解决复杂的长期任务。然而，由于固有的非静态性，分层代理难以训练。我们提出了基于原始操作的自适应重标记（PEAR），这是一个两阶段方法，我们首先对少量专家演示进行自适应重标记，产生高效的子目标监督，然后通过使用强化学习（RL）和模仿学习（IL）联合优化HRL代理。我们进行理论分析来$(i)$限制我们方法的次优性，和$(ii)$推导出使用RL和IL的广义即插即用的框架进行联合优化。PEAR使用一些专家演示，并对任务结构进行最小的限制假设。此外，它可以轻松与典型的模型自由RL算法集成，产生一个实用的HRL算法。我们在具有挑战性的机器人环境上进行了实验。

    Hierarchical reinforcement learning (HRL) has the potential to solve complex long horizon tasks using temporal abstraction and increased exploration. However, hierarchical agents are difficult to train due to inherent non-stationarity. We present primitive enabled adaptive relabeling (PEAR), a two-phase approach where we first perform adaptive relabeling on a few expert demonstrations to generate efficient subgoal supervision, and then jointly optimize HRL agents by employing reinforcement learning (RL) and imitation learning (IL). We perform theoretical analysis to $(i)$ bound the sub-optimality of our approach, and $(ii)$ derive a generalized plug-and-play framework for joint optimization using RL and IL. PEAR uses a handful of expert demonstrations and makes minimal limiting assumptions on the task structure. Additionally, it can be easily integrated with typical model free RL algorithms to produce a practical HRL algorithm. We perform experiments on challenging robotic environments
    
[^106]: 《可解释人工智能中的对抗性攻击和防御：调查报告》

    Adversarial Attacks and Defenses in Explainable Artificial Intelligence: A Survey. (arXiv:2306.06123v1 [cs.CR])

    [http://arxiv.org/abs/2306.06123](http://arxiv.org/abs/2306.06123)

    本文总结了对抗性攻击和防御在可解释人工智能中的研究。列出了现有的不安全因素，并表明了本领域的新兴研究方向。

    

    可解释人工智能（XAI）方法被描绘为调试和信任统计和深度学习模型的治疗方式，以及解释它们的预测。然而，对抗机器学习的最新进展突出了最新解释的局限性和漏洞，这些进展令人对其安全性和可信度产生质疑。操纵、欺骗或洗白模型推理证据的可能性在高风险决策和知识发现中产生不利后果。本文总结了50多篇论文的研究，概述了针对机器学习模型解释的对抗攻击以及公平度量的研究。我们讨论了如何防御攻击并设计鲁棒的解释方法。我们列出XAI中现有的不安全因素，并概述了对抗性XAI（AdvXAI）的新兴研究方向。

    Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning highlight the limitations and vulnerabilities of state-of-the-art explanations, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This concise survey of over 50 papers summarizes research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI).
    
[^107]: RescueSpeech：用于搜救领域语音识别的德语语料库

    RescueSpeech: A German Corpus for Speech Recognition in Search and Rescue Domain. (arXiv:2306.04054v1 [eess.AS])

    [http://arxiv.org/abs/2306.04054](http://arxiv.org/abs/2306.04054)

    RescueSpeech是一个用于搜救领域语音识别的德语语音数据集，但目前最先进的方法仍无法令人满意。

    

    尽管语音识别在最近得到了进一步发展，但在嘈杂、回声的环境中准确转录对话和情感表达仍然存在一定难度。在搜救领域尤其如此，因为转录救援队成员之间的对话对支持实时决策至关重要。搜救场景中语音数据和相关背景噪声的稀缺性使得部署健壮的语音识别系统变得困难。为了解决这个问题，我们创建并公开了一个名为RescueSpeech的德语语音数据集。该数据集包括模拟救援演习的真实语音录音。此外，我们还发布了竞争性训练配方和预训练模型。我们的研究表明，目前最先进的方法所达到的性能水平仍远未能令人满意。

    Despite recent advancements in speech recognition, there are still difficulties in accurately transcribing conversational and emotional speech in noisy and reverberant acoustic environments. This poses a particular challenge in the search and rescue (SAR) domain, where transcribing conversations among rescue team members is crucial to support real-time decision-making. The scarcity of speech data and associated background noise in SAR scenarios make it difficult to deploy robust speech recognition systems.  To address this issue, we have created and made publicly available a German speech dataset called RescueSpeech. This dataset includes real speech recordings from simulated rescue exercises. Additionally, we have released competitive training recipes and pre-trained models. Our study indicates that the current level of performance achieved by state-of-the-art methods is still far from being acceptable.
    
[^108]: 神经元激活覆盖度：重新思考离域检测和泛化问题

    Neuron Activation Coverage: Rethinking Out-of-distribution Detection and Generalization. (arXiv:2306.02879v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02879](http://arxiv.org/abs/2306.02879)

    本文通过研究神经元激活状态，提出了神经元激活覆盖度（NAC）作为衡量神经元行为的指标。利用NAC可以有效区分域内和离域输入，简化离域检测问题，并且NAC与模型的泛化能力间存在正相关关系。

    

    离域问题通常在神经网络遇到明显偏离训练数据分布的数据时出现，即在域内数据（InD）之外。本文从神经元激活的角度研究了离域问题。我们首先通过考虑神经元的输出和其对模型决策的影响来定义了神经元激活状态。然后，为了描述神经元与离域问题的关系，我们引入了神经元激活覆盖度（NAC）——一种衡量神经元在域内数据下行为的简单度量。利用我们的NAC，我们展示了：1）基于神经元行为可以在很大程度上区分域内和离域输入，大大简化了离域检测问题，并在三个基准测试集（CIFAR-10、CIFAR-100和ImageNet-1K）上超过了21个先前的方法；2）NAC与模型的泛化能力之间存在正相关关系，这种关系在不同架构和数据集上一致成立，使得基于NAC的准则可以用于评估模型的泛化能力。

    The out-of-distribution (OOD) problem generally arises when neural networks encounter data that significantly deviates from the training data distribution, i.e., in-distribution (InD). In this paper, we study the OOD problem from a neuron activation view. We first formulate neuron activation states by considering both the neuron output and its influence on model decisions. Then, to characterize the relationship between neurons and OOD issues, we introduce the \textit{neuron activation coverage} (NAC) -- a simple measure for neuron behaviors under InD data. Leveraging our NAC, we show that 1) InD and OOD inputs can be largely separated based on the neuron behavior, which significantly eases the OOD detection problem and beats the 21 previous methods over three benchmarks (CIFAR-10, CIFAR-100, and ImageNet-1K). 2) a positive correlation between NAC and model generalization ability consistently holds across architectures and datasets, which enables a NAC-based criterion for evaluating mod
    
[^109]: 基于Transformer神经过程的端到端Meta-Bayesian优化

    End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes. (arXiv:2305.15930v1 [cs.LG])

    [http://arxiv.org/abs/2305.15930](http://arxiv.org/abs/2305.15930)

    本文提出了第一个可泛化到学习获取函数的神经过程端到端框架，使用强化学习解决了缺乏标签获取数据以及利用代理模型或获取函数的传统Meta-BO方法训练过程中的挑战。

    

    元贝叶斯优化（Meta-Bayesian optimization，Meta-BO）通过利用相关任务的数据来提高贝叶斯优化的样本效率。尽管之前的方法已经成功地独立元学习过代理模型或获取函数，但是同时训练这两个组件仍然是一个挑战。本文提出了第一个端到端可微分的Meta-BO框架，通过Transformer体系结构将神经过程泛化到学习获取函数。我们使用强化学习（RL）使这种端到端框架具有处理缺乏标签获取数据的能力。

    Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency of Bayesian optimisation by leveraging data from related tasks. While previous methods successfully meta-learn either a surrogate model or an acquisition function independently, joint training of both components remains an open challenge. This paper proposes the first end-to-end differentiable meta-BO framework that generalises neural processes to learn acquisition functions via transformer architectures. We enable this end-to-end framework with reinforcement learning (RL) to tackle the lack of labelled acquisition data. Early on, we notice that training transformer-based neural processes from scratch with RL is challenging due to insufficient supervision, especially when rewards are sparse. We formalise this claim with a combinatorial analysis showing that the widely used notion of regret as a reward signal exhibits a logarithmic sparsity pattern in trajectory lengths. To tackle this problem, we augment the RL 
    
[^110]: 分布式强化学习的好处：小损失边界

    The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning. (arXiv:2305.15703v1 [cs.LG])

    [http://arxiv.org/abs/2305.15703](http://arxiv.org/abs/2305.15703)

    通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，分布式方法优于非分布式方法。

    

    虽然分布式强化学习已经取得了实证成果，但其何时何地有益的问题尚未得到回答。在这项工作中，通过小损失边界的视角，我们提供了分布式RL好处的一个解释，该边界与实例相关的最优成本成比例。如果最优成本很小，我们的边界会比非分布式方法更强。作为热身，我们展示了学习成本分布会在情境展开（CB）中导致小损失后悔边界，我们发现分布式CB在三个具有挑战性的任务上比最先进的技术在实证上表现更好。对于在线RL，我们提出了一个分布式版本空间算法，该算法使用最大似然估计构建置信区间，并证明了它在表格MDP中实现了小损失后悔，同时在潜变量模型中享有小损失PAC边界。以类似的见解为基础，我们提出了一个分布式离线RL算法

    While distributional reinforcement learning (RL) has demonstrated empirical success, the question of when and why it is beneficial has remained unanswered. In this work, we provide one explanation for the benefits of distributional RL through the lens of small-loss bounds, which scale with the instance-dependent optimal cost. If the optimal cost is small, our bounds are stronger than those from non-distributional approaches. As warmup, we show that learning the cost distribution leads to small-loss regret bounds in contextual bandits (CB), and we find that distributional CB empirically outperforms the state-of-the-art on three challenging tasks. For online RL, we propose a distributional version-space algorithm that constructs confidence sets using maximum likelihood estimation, and we prove that it achieves small-loss regret in the tabular MDPs and enjoys small-loss PAC bounds in latent variable models. Building on similar insights, we propose a distributional offline RL algorithm bas
    
[^111]: 黑盒变分推断收敛性分析

    Black-Box Variational Inference Converges. (arXiv:2305.15349v1 [cs.LG])

    [http://arxiv.org/abs/2305.15349](http://arxiv.org/abs/2305.15349)

    通过对黑盒变分推断（BBVI）的分析，发现一些常见的算法设计选择可能会导致次优收敛速率，但使用带有近端随机梯度下降的BBVI可以实现最强收敛率保证。

    

    我们提供了第一个完整的黑盒变分推断（BBVI）的收敛保证，也称为蒙特卡罗变分推断。尽管早期的研究只针对简化版本的BBVI进行了研究（例如，有界域、有界支持、仅针对尺度进行优化等），但我们的设置不需要任何这样的算法修改。我们的结果适用于对数平滑后验密度，无论是否强对数凹性以及位置-尺度变分族。此外，我们的分析揭示出了一些常见的算法设计选择，特别是变分近似尺度的非线性参数化，可能会导致次优收敛速率。幸运的是，运行带有近端随机梯度下降的BBVI可以纠正这些限制，从而实现已知的最强收敛率保证。我们通过将近端SGD与其他标准的BBVI实现进行比较，验证了这一理论结论在大规模数据集上的有效性。

    We provide the first convergence guarantee for full black-box variational inference (BBVI), also known as Monte Carlo variational inference. While preliminary investigations worked on simplified versions of BBVI (e.g., bounded domain, bounded support, only optimizing for the scale, and such), our setup does not need any such algorithmic modifications. Our results hold for log-smooth posterior densities with and without strong log-concavity and the location-scale variational family. Also, our analysis reveals that certain algorithm design choices commonly employed in practice, particularly, nonlinear parameterizations of the scale of the variational approximation, can result in suboptimal convergence rates. Fortunately, running BBVI with proximal stochastic gradient descent fixes these limitations, and thus achieves the strongest known convergence rate guarantees. We evaluate this theoretical insight by comparing proximal SGD against other standard implementations of BBVI on large-scale
    
[^112]: 在表格数据上进行深度异常检测的超越个体输入

    Beyond Individual Input for Deep Anomaly Detection on Tabular Data. (arXiv:2305.15121v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15121](http://arxiv.org/abs/2305.15121)

    本文提出了一种采用非参数转换器的深度异常检测方法，能够捕捉表格数据中特征与特征之间以及样本与样本之间的依赖关系，并在广泛的表格数据集上取得了比现有最先进方法更优的性能。

    

    异常检测在金融、医疗和网络安全等各个领域都至关重要。在本文中，我们提出了一种新颖的基于非参数转换器（NPTs）的表格数据深度异常检测方法，以捕捉特征与特征之间以及样本与样本之间的依赖关系。在基于重构的框架中，我们训练NPT来重构正常样本的遮蔽特征。以非参数化方式，在推理过程中利用整个训练集，并利用模型在生成异常得分时重构遮蔽特征的能力。据我们所知，我们提出的方法是首个成功结合特征之间和样本之间依赖关系进行表格数据异常检测的方法。我们在31个表格数据集的广泛基准测试中评估了我们的方法，并证明我们的方法在F1得分和AUROC方面优于现有的最先进方法。

    Anomaly detection is crucial in various domains, such as finance, healthcare, and cybersecurity. In this paper, we propose a novel deep anomaly detection method for tabular data that leverages Non-Parametric Transformers (NPTs), a model initially proposed for supervised tasks, to capture both feature-feature and sample-sample dependencies. In a reconstruction-based framework, we train the NPT to reconstruct masked features of normal samples. In a non-parametric fashion, we leverage the whole training set during inference and use the model's ability to reconstruct the masked features during to generate an anomaly score. To the best of our knowledge, our proposed method is the first to successfully combine feature-feature and sample-sample dependencies for anomaly detection on tabular datasets. We evaluate our method on an extensive benchmark of 31 tabular datasets and demonstrate that our approach outperforms existing state-of-the-art methods based on the F1-score and AUROC by a signifi
    
[^113]: 基于多图融合的道路网络节点重要性排序方法的学习。

    Learning to Rank the Importance of Nodes in Road Networks Based on Multi-Graph Fusion. (arXiv:2305.14375v1 [cs.LG])

    [http://arxiv.org/abs/2305.14375](http://arxiv.org/abs/2305.14375)

    本文提出了一种新的基于图学习的节点排序方法（MGL2Rank），充分利用了道路网络的丰富特征，并且在实验中表现出比现有方法更高的精度和效率。

    

    在城市规划领域中，识别具有强传播能力的重要节点是一个重要的课题。然而，现有的评估节点重要性的方法仅考虑拓扑信息和交通流量，忽略了道路网络的多样性特征，如车道数量和道路段的平均速度，限制了它们的性能。为了解决这个问题，本文提出了一种基于图学习的节点排序方法（MGL2Rank），它集成了道路网络的丰富特征。在这种方法中，我们首先开发了一个采样算法（MGWalk），利用多图融合来建立基于属性的道路段之间的关联。然后，提出了一个嵌入模块，用于学习每个道路段的潜在表示。最后，得到的节点表示用于学习道路段的重要性排序。我们在中国沈阳市区域道路网络上进行了仿真实验，评估了MGL2Rank的有效性。实验结果表明，MGL2Rank在精度和效率方面优于现有的节点排序方法。

    Identifying important nodes with strong propagation capabilities in road networks is a significant topic in the field of urban planning. However, existing methods for evaluating nodes importance consider only topological information and traffic volumes, ignoring the diversity of characteristics in road networks, such as the number of lanes and average speed of road segments, limiting their performance. To address this issue, this paper proposes a graph learning-based node ranking method (MGL2Rank) that integrates the rich characteristics of the road network. In this method, we first develop a sampling algorithm (MGWalk) that utilizes multi-graph fusion to establish association between road segments based on their attributes. Then, an embedding module is proposed to learn latent representation for each road segment. Finally, the obtained node representation is used to learn importance ranking of road segments. We conduct simulation experiments on the regional road network of Shenyang ci
    
[^114]: QFA2SR: 无查询的对抗迁移攻击以对抗说话人识别系统

    QFA2SR: Query-Free Adversarial Transfer Attacks to Speaker Recognition Systems. (arXiv:2305.14097v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2305.14097](http://arxiv.org/abs/2305.14097)

    本论文提出了QFA2SR，一种无查询的黑盒攻击方法，用于对抗说话人识别系统。该方法利用对抗声音的可传递性，通过定制损失函数、SRS集成和时频腐蚀等新方法，实现了有效且不可察觉的攻击。

    

    目前针对说话人识别系统（SRSs）的对抗攻击要么需要白盒访问，要么需要对目标SRS进行大量的黑盒查询，因此仍然落后于针对专有商业API和语音控制设备的实际攻击。为了填补这一空白，我们提出了QFA2SR，一种有效且不可察觉的无查询黑盒攻击，利用对抗性声音的可传递性。为了提高可传递性，我们提出了三种新方法，定制损失函数、SRS集成和时频腐蚀。第一个方法将损失函数定制为不同的攻击场景。后两种方法以两种不同的方式增强替代SRS。SRS集成将多样化的替代SRS与新策略相结合，适应SRS的独特评分特性。时频腐蚀通过引入精心设计的时域/频域修改函数增强替代SRS，模拟和近似目标SRS的决策边界和失真。

    Current adversarial attacks against speaker recognition systems (SRSs) require either white-box access or heavy black-box queries to the target SRS, thus still falling behind practical attacks against proprietary commercial APIs and voice-controlled devices. To fill this gap, we propose QFA2SR, an effective and imperceptible query-free black-box attack, by leveraging the transferability of adversarial voices. To improve transferability, we present three novel methods, tailored loss functions, SRS ensemble, and time-freq corrosion. The first one tailors loss functions to different attack scenarios. The latter two augment surrogate SRSs in two different ways. SRS ensemble combines diverse surrogate SRSs with new strategies, amenable to the unique scoring characteristics of SRSs. Time-freq corrosion augments surrogate SRSs by incorporating well-designed time-/frequency-domain modification functions, which simulate and approximate the decision boundary of the target SRS and distortions int
    
[^115]: 面向有条件生成对抗网络的少样本连续学习

    Few-Shot Continual Learning for Conditional Generative Adversarial Networks. (arXiv:2305.11400v1 [cs.LG])

    [http://arxiv.org/abs/2305.11400](http://arxiv.org/abs/2305.11400)

    本文提出了一种新的连续学习方法，适用于条件生成对抗网络，根据cGAN的判别器数据识别出最接近目标的现有模式，并通过扩展连续学习模型，使用回放生成的数据来训练目标模式的cGAN模型，以避免灾难性遗忘，提高了生成性能。

    

    在生成模型的少样本连续学习中，必须学习目标模式，并在不影响先前学习到的模式的情况下仅使用有限的样本。本文针对条件生成对抗网络提出了一种新的连续学习方法，基于一种新的用于生成建模的模式亲和力量度。我们的度量完全基于cGAN的判别器，可以识别最接近目标的现有模式。随后，我们通过包含基于最接近模式的加权标签来扩展连续学习模型。为了预防灾难性遗忘，我们首先使用cGAN的生成器生成带标签的数据样本，然后通过回放生成的数据来训练目标模式的cGAN模型。我们的实验结果证明了我们的方法在提高生成性能方面的有效性，超越了各种标准和最先进的方法。

    In few-shot continual learning for generative models, a target mode must be learned with limited samples without adversely affecting the previously learned modes. In this paper, we propose a new continual learning approach for conditional generative adversarial networks (cGAN) based on a new mode-affinity measure for generative modeling. Our measure is entirely based on the cGAN's discriminator and can identify the existing modes that are most similar to the target. Subsequently, we expand the continual learning model by including the target mode using a weighted label derived from those of the closest modes. To prevent catastrophic forgetting, we first generate labeled data samples using the cGAN's generator, and then train the cGAN model for the target mode while memory replaying with the generated data. Our experimental results demonstrate the efficacy of our approach in improving the generation performance over the baselines and the state-of-the-art approaches for various standard 
    
[^116]: 关于一般函数逼近下的均场强化学习的统计效率

    On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])

    [http://arxiv.org/abs/2305.11283](http://arxiv.org/abs/2305.11283)

    本文研究了一般函数逼近下的均场控制(MFC)和均场博弈(MFG)中的强化学习的统计效率，提出了基于乐观最大似然估计的算法，并仅对转移动力学具有Lipschitz连续性的假设，最后建立了一个指数级的下界支持MFC设置。

    

    本文研究了一般函数逼近下的均场控制（MFC）和均场博弈（MFG）中强化学习的统计效率。引入了一种称为Mean-Field Model-Based Eluder Dimension (MBED)的新概念，包含了一系列丰富的均场强化学习问题。此外，我们提出了基于乐观最大似然估计的算法，可以返回一个$\epsilon$优的策略，适用于MFC或$\epsilon$纳什均衡策略适用于MFG，样本复杂度多项式与相关参数无关，与状态、动作和代理数量无关。值得注意的是，我们的结果仅对转移动力学具有Lipschitz连续性的假设，避免了以前的强结构假设。最后，在tabular设置下，假设有一个生成模型，我们建立了一个指数级的下界支持MFC设置，同时提供了一种新颖的样本高效的模型消除算法以逼近最优策略。

    In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
    
[^117]: 连接隐藏神经元（CHNNet）：一种快速收敛的人工神经网络

    Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence. (arXiv:2305.10468v1 [cs.NE])

    [http://arxiv.org/abs/2305.10468](http://arxiv.org/abs/2305.10468)

    该论文提出了一个更为强大的人工神经网络模型，该模型中同一隐藏层中的隐藏神经元相互连接，可以学习复杂模式并加速收敛速度。

    

    人工神经网络的核心目的是模仿生物神经网络的功能。然而，与生物神经网络不同，传统的人工神经网络通常是按层次结构化的，这可能会妨碍神经元之间的信息流动，因为同一层中的神经元之间没有连接。因此，我们提出了一种更为强大的人工神经网络模型，其中同一隐藏层中的隐藏神经元是互相连接的，使得神经元能够学习复杂的模式并加速收敛速度。通过在浅层和深层网络中将我们提出的模型作为完全连接的层进行实验研究，我们证明这个模型可以显著提高收敛速率。

    The core purpose of developing artificial neural networks was to mimic the functionalities of biological neural networks. However, unlike biological neural networks, traditional artificial neural networks are often structured hierarchically, which can impede the flow of information between neurons as the neurons in the same layer have no connections between them. Hence, we propose a more robust model of artificial neural networks where the hidden neurons, residing in the same hidden layer, are interconnected, enabling the neurons to learn complex patterns and speeding up the convergence rate. With the experimental study of our proposed model as fully connected layers in shallow and deep networks, we demonstrate that the model results in a significant increase in convergence rate.
    
[^118]: ZeroFlow: 通过蒸馏实现快速零标签场景流

    ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])

    [http://arxiv.org/abs/2305.10424](http://arxiv.org/abs/2305.10424)

    ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。

    

    场景流估计是描述连续点云之间的三维运动场的任务。最先进的方法使用强大的先验知识和测试时优化技术，但对于大规模点云需要数十秒的时间，使其无法作为实时应用程序（如开放世界目标检测）的计算机视觉基元使用。前向传递方法相对快速，对于大规模点云的运行时间在数十至数百毫秒之间，但需要昂贵的人力监督。为了解决这两个限制，我们提出了一种简单的蒸馏框架 Scene Flow via Distillation，使用无标签优化方法来生成伪标签以监督前向传递模型。我们实现了这个框架中的 ZeroFlow，使用零人工标签，在大规模点云上实时生成场景流估计结果，同时质量竞争状态下的最先进方法。值得注意的是，在测试时 ZeroFlow

    Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
    
[^119]: MRI中深度学习用于回顾性运动校正的综述

    Deep Learning for Retrospective Motion Correction in MRI: A Comprehensive Review. (arXiv:2305.06739v1 [eess.IV])

    [http://arxiv.org/abs/2305.06739](http://arxiv.org/abs/2305.06739)

    该综述针对MRI中的运动问题，综述了基于深度学习对运动校正的方法，发现了不同应用之间的差异和共同点，在未来的方向上提出了建议。

    

    运动是磁共振成像(MRI)中的主要挑战之一。由于MR信号在频率空间中获取，任何成像物体的运动都会导致复杂的伪影，此外还会产生其他MR成像伪影。深度学习已经经常被提出用于重建过程的多个阶段中的运动校正。MRI采集序列的广泛应用，感兴趣的解剖学和病理学以及运动模式（刚性vs可变形和随机vs规律性）使得全面性解决方案不太可能。为了促进不同应用之间的思想传递，该综述提供了一份详细的基于学习的MRI运动校正方法概述及其常见挑战和潜力。该综述识别了不同应用之间的数据使用、体系结构和评估策略的差异和协同作用。我们批判性地讨论了大趋势并概述了未来的方向，旨在增强MRI中深度学习的回顾性运动校正能力。

    Motion represents one of the major challenges in magnetic resonance imaging (MRI). Since the MR signal is acquired in frequency space, any motion of the imaged object leads to complex artefacts in the reconstructed image in addition to other MR imaging artefacts. Deep learning has been frequently proposed for motion correction at several stages of the reconstruction process. The wide range of MR acquisition sequences, anatomies and pathologies of interest, and motion patterns (rigid vs. deformable and random vs. regular) makes a comprehensive solution unlikely. To facilitate the transfer of ideas between different applications, this review provides a detailed overview of proposed methods for learning-based motion correction in MRI together with their common challenges and potentials. This review identifies differences and synergies in underlying data usage, architectures and evaluation strategies. We critically discuss general trends and outline future directions, with the aim to enhan
    
[^120]: 探索机器遗忘的领域：一篇综述与分类

    Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])

    [http://arxiv.org/abs/2305.06360](http://arxiv.org/abs/2305.06360)

    本文综述了机器遗忘的现状和技术应用，包括数据删除、扰动和模型更新，讨论了MU在隐私、安全和公正性等领域的潜在益处，以及它在自然语言处理、计算机视觉和推荐系统中的未来发展方向。

    

    机器遗忘是一个越来越受关注的领域，因为需要删除或修改机器学习模型所做出的预测。虽然训练模型变得更加有效和准确，但在某些领域（如隐私、安全和公正性），遗忘先前学到的信息的重要性变得越来越显著。本文介绍了机器遗忘的综述，涵盖了当前最先进的技术和方法，包括数据删除、扰动和模型更新。此外，文中还介绍了常用的度量标准和数据集。文章还强调了需要解决的挑战，包括攻击复杂性、标准化、可转移性、可解释性、训练数据和资源限制。本文的贡献包括讨论MU的潜在益处以及它在自然语言处理、计算机视觉和推荐系统中的未来方向。

    Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
    
[^121]: 生成式检索推荐系统

    Recommender Systems with Generative Retrieval. (arXiv:2305.05065v1 [cs.IR])

    [http://arxiv.org/abs/2305.05065](http://arxiv.org/abs/2305.05065)

    本文提出了一种新型的生成式检索模型，将检索和生成组合在一起以产生推荐。

    

    现代推荐系统使用大规模检索模型进行推荐，包括两个阶段：训练双编码模型将查询和候选项嵌入到相同的空间中，然后使用近似最近邻搜索来选择给定查询嵌入的顶部候选项。本文提出了一种新的单阶段范例：生成式检索模型，该模型通过自回归方式在一个阶段中解码目标候选项的标识符。为此，我们不是为每个项目分配随机生成的原子ID，而是生成语义ID：每个项目的语义有意义的元组编码词，它作为其唯一标识符。我们使用称为RQ-VAE的分层方法生成这些编码词。一旦我们对所有项目都有了语义ID，就会训练基于Transformer的序列到序列模型来预测下一个项目的语义ID。由于这个模型以自回归的方式直接预测标识下一个项的编码词元组，因此它可以将检索和生成组合在一起以产生推荐。

    Modern recommender systems leverage large-scale retrieval models consisting of two stages: training a dual-encoder model to embed queries and candidates in the same space, followed by an Approximate Nearest Neighbor (ANN) search to select top candidates given a query's embedding. In this paper, we propose a new single-stage paradigm: a generative retrieval model which autoregressively decodes the identifiers for the target candidates in one phase. To do this, instead of assigning randomly generated atomic IDs to each item, we generate Semantic IDs: a semantically meaningful tuple of codewords for each item that serves as its unique identifier. We use a hierarchical method called RQ-VAE to generate these codewords. Once we have the Semantic IDs for all the items, a Transformer based sequence-to-sequence model is trained to predict the Semantic ID of the next item. Since this model predicts the tuple of codewords identifying the next item directly in an autoregressive manner, it can be c
    
[^122]: DMF-TONN: 使用神经网络进行直接无网格拓扑优化

    DMF-TONN: Direct Mesh-free Topology Optimization using Neural Networks. (arXiv:2305.04107v1 [cs.CE])

    [http://arxiv.org/abs/2305.04107](http://arxiv.org/abs/2305.04107)

    本文提出了一种直接无网格法进行拓扑优化，集成了神经网络并用于求解遵从度和体积分数约束违规的优化问题，无需传统的有限元分析和网格化，能够无缝地集成到后处理软件中。

    

    我们提出了一种直接无网格法进行拓扑优化的方法，该方法将密度场逼近神经网络与位移场逼近神经网络相结合。我们展示了这种直接集成方法可以给出与传统拓扑优化技术可比拟的结果，并且具有与后处理软件无缝集成的优点，还有可能用于网格和有限元分析成本高昂或不合适的拓扑优化目标。DMF-TONN接收边界条件和域坐标作为输入，找到最优密度场以最小化遵从度和体积分数约束违规的损失函数。无网格的性质是由一个物理学启发的位移场逼近神经网络实现的，用于解决线性弹性偏微分方程并替代传统用于计算遵从度的有限元分析（FEA）。我们展示了该方法的有效性并与传统方法进行了比较。

    We propose a direct mesh-free method for performing topology optimization by integrating a density field approximation neural network with a displacement field approximation neural network. We show that this direct integration approach can give comparable results to conventional topology optimization techniques, with an added advantage of enabling seamless integration with post-processing software, and a potential of topology optimization with objectives where meshing and Finite Element Analysis (FEA) may be expensive or not suitable. Our approach (DMF-TONN) takes in as inputs the boundary conditions and domain coordinates and finds the optimum density field for minimizing the loss function of compliance and volume fraction constraint violation. The mesh-free nature is enabled by a physics-informed displacement field approximation neural network to solve the linear elasticity partial differential equation and replace the FEA conventionally used for calculating the compliance. We show t
    
[^123]: 基于扩散ODEs最大似然估计的改进技术

    Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])

    [http://arxiv.org/abs/2305.03935](http://arxiv.org/abs/2305.03935)

    本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。

    

    扩散模型在各领域中表现出良好的性能。扩散模型的概率流常微分方程（ODE）（即扩散ODE）是连续归一化流（CNFs）的一个特例，它使得确定性推断和精确似然评估成为可能。然而，与最先进的基于似然的生成模型相比，扩散ODE的似然估计结果仍有很大差距。在本文中，我们提出了一些改进的技术，包括训练和评估两个方面，用于扩散ODE的最大似然估计。对于训练，我们提出了速度参数化，并探索方差减少技术以加快收敛速度。我们还设计了一个误差有界的高阶流匹配目标用于微调，从而提高ODE的似然估计并平滑其轨迹。对于评估，我们提出了一种新颖的无须训练的截断正态去量化方法来填补训练-评估间的差距。

    Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
    
[^124]: 如何解决模型风险管理中的单调性问题？

    How to address monotonicity for model risk management?. (arXiv:2305.00799v1 [cs.LG])

    [http://arxiv.org/abs/2305.00799](http://arxiv.org/abs/2305.00799)

    本文提出了使用神经加性模型的单调的树丛来实现单调性和透明性的结合，以确立透明机器学习模型的问责和公平性。通过实证示例，证明该方法透明、有问责性和公平，尤其对于避免单调性问题具有优势。

    

    本文研究通过单调性确立透明机器学习模型的问责和公平性的问题。尽管已有大量研究关注个体单调性，但是现有文献经常忽略了成对单调性。本文研究了透明神经网络在存在三种单调性的情况下：个体单调性、弱成对单调性和强成对单调性。我们提出使用神经加性模型的单调的树丛来实现单调性和透明性的结合。通过实证示例，我们证明单调性通常在实践中被违反，而神经加性模型的单调树丛是透明、有问责性和公平的。

    In this paper, we study the problem of establishing the accountability and fairness of transparent machine learning models through monotonicity. Although there have been numerous studies on individual monotonicity, pairwise monotonicity is often overlooked in the existing literature. This paper studies transparent neural networks in the presence of three types of monotonicity: individual monotonicity, weak pairwise monotonicity, and strong pairwise monotonicity. As a means of achieving monotonicity while maintaining transparency, we propose the monotonic groves of neural additive models. As a result of empirical examples, we demonstrate that monotonicity is often violated in practice and that monotonic groves of neural additive models are transparent, accountable, and fair.
    
[^125]: 自然分布漂移下低样本稳健性的基准测试

    Benchmarking Low-Shot Robustness to Natural Distribution Shifts. (arXiv:2304.11263v1 [cs.CV])

    [http://arxiv.org/abs/2304.11263](http://arxiv.org/abs/2304.11263)

    本文通过对不同少样本数据集、架构、预训练初始化和稳健性干预的自然分布漂移的稳健性进行了首次深入研究，发现没有单一的选择模型比其他模型更稳健，现有的干预措施也可能无法提高某些数据集的稳健性。

    

    近年来，结合更好的微调方法的预训练策略已经取得了针对自然分布漂移的鲁棒性的显著进展。然而，这样的微调假设可以访问大量标记数据，而当训练数据量不高时观察到的情况尚不清楚。我们通过对不同少样本数据集、架构、预训练初始化和最先进的稳健性干预的自然分布漂移的稳健性进行了首次深入研究，填补了这一空白。最重要的是，我们发现没有单一的选择模型比其他模型更稳健，即使在完整样本下，现有的干预措施也可能无法提高某些数据集的稳健性。我们希望我们的工作能够激励社区关注这个实际重要性的问题。

    Robustness to natural distribution shifts has seen remarkable progress thanks to recent pre-training strategies combined with better fine-tuning methods. However, such fine-tuning assumes access to large amounts of labelled data, and the extent to which the observations hold when the amount of training data is not as high remains unknown. We address this gap by performing the first in-depth study of robustness to various natural distribution shifts in different low-shot regimes: spanning datasets, architectures, pre-trained initializations, and state-of-the-art robustness interventions. Most importantly, we find that there is no single model of choice that is often more robust than others, and existing interventions can fail to improve robustness on some datasets even if they do so in the full-shot regime. We hope that our work will motivate the community to focus on this problem of practical importance.
    
[^126]: 基于多模态深度学习的信用评级预测方法研究——以文本和数字数据流为例

    Multi-Modal Deep Learning for Credit Rating Prediction Using Text and Numerical Data Streams. (arXiv:2304.10740v1 [q-fin.GN])

    [http://arxiv.org/abs/2304.10740](http://arxiv.org/abs/2304.10740)

    本文研究了基于多模态的深度学习融合技术在信用评级预测中的应用，通过比较不同融合策略和深度学习模型的组合，证明了一个基于CNN的多模态模型通过两种融合策略优于其他多模态技术，同时在比较简单和复杂的模型中发现，更复杂的模型并不一定表现更好。

    

    了解信用评级分配中哪些因素是重要的可以帮助做出更好的决策。然而，目前文献的重点大多集中在结构化数据上，较少研究非结构化或多模态数据集。本文提出了一种分析结构化和非结构化不同类型数据集的深度学习模型融合的有效架构，以预测公司信用评级标准。在模型中，我们测试了不同的深度学习模型及融合策略的组合，包括CNN，LSTM，GRU和BERT。我们研究了数据融合策略（包括早期和中间融合）以及技术（包括串联和交叉注意）等方面。结果表明，一个基于CNN的多模态模型通过两种融合策略优于其他多模态技术。此外，通过比较简单的架构与更复杂的架构，我们发现，更复杂的模型并不一定能在信用评级预测中发挥更好的性能。

    Knowing which factors are significant in credit rating assignment leads to better decision-making. However, the focus of the literature thus far has been mostly on structured data, and fewer studies have addressed unstructured or multi-modal datasets. In this paper, we present an analysis of the most effective architectures for the fusion of deep learning models for the prediction of company credit rating classes, by using structured and unstructured datasets of different types. In these models, we tested different combinations of fusion strategies with different deep learning models, including CNN, LSTM, GRU, and BERT. We studied data fusion strategies in terms of level (including early and intermediate fusion) and techniques (including concatenation and cross-attention). Our results show that a CNN-based multi-modal model with two fusion strategies outperformed other multi-modal techniques. In addition, by comparing simple architectures with more complex ones, we found that more soph
    
[^127]: 二元积分布的多项式时间和纯差分隐私估计器

    A Polynomial Time, Pure Differentially Private Estimator for Binary Product Distributions. (arXiv:2304.06787v1 [cs.DS])

    [http://arxiv.org/abs/2304.06787](http://arxiv.org/abs/2304.06787)

    本论文提出了第一个多项式时间、纯差分隐私估计器，可以在$\{0,1\}^d$上准确估计二元积分布的均值，达到了最优的样本复杂度。

    

    我们提出了第一个ε-差分隐私、计算有效的算法，可以在总变化距离下准确地估计$\{0,1\}^d$上的乘积分布的均值，同时在多项式对数因子内获得了最优的样本复杂度。之前的工作要么在更弱的隐私概念下有效地解决了这个问题，要么在指数级运行时间内最优地解决了这个问题。

    We present the first $\varepsilon$-differentially private, computationally efficient algorithm that estimates the means of product distributions over $\{0,1\}^d$ accurately in total-variation distance, whilst attaining the optimal sample complexity to within polylogarithmic factors. The prior work had either solved this problem efficiently and optimally under weaker notions of privacy, or had solved it optimally while having exponential running times.
    
[^128]: 个性化文本到图像生成的可控文本反转

    Controllable Textual Inversion for Personalized Text-to-Image Generation. (arXiv:2304.05265v1 [cs.CV])

    [http://arxiv.org/abs/2304.05265](http://arxiv.org/abs/2304.05265)

    本文提出了一种名为COTI的技术，通过引入理论指导的损失目标和全面的加权评分机制，并结合主动学习范式来解决文本反转时的困难，提供了一个强大，数据效率高，易于使用的框架。

    

    最近，大规模生成模型在以文本为引导的高保真图像的生成方面取得了前所未有的性能。当引导信息包含用户定义的、未见过的或长尾概念标记时，文本反转成为一种有效的个性化生成技术。尽管如此，我们发现并展示了文本反转的部署仍充满了“黑魔法”，例如额外数据集的严苛要求，在循环中需要艰苦的人力成本和缺乏鲁棒性等。在这项工作中，我们提出了一种名为可控文本反转的大大增强版反转，解决了所有上述问题，并反过来提供了一个强大，数据效率高，易于使用的框架。COTI的核心是基于理论的损失目标，具有全面和新颖的加权评分机制，并由主动学习范式所提取。广泛的结果表明，COTI的性能比之前技术有了显著的提升，尤其是在数据少的情况下。

    The recent large-scale generative modeling has attained unprecedented performance especially in producing high-fidelity images driven by text prompts. Text inversion (TI), alongside the text-to-image model backbones, is proposed as an effective technique in personalizing the generation when the prompts contain user-defined, unseen or long-tail concept tokens. Despite that, we find and show that the deployment of TI remains full of "dark-magics" -- to name a few, the harsh requirement of additional datasets, arduous human efforts in the loop and lack of robustness. In this work, we propose a much-enhanced version of TI, dubbed Controllable Textual Inversion (COTI), in resolving all the aforementioned problems and in turn delivering a robust, data-efficient and easy-to-use framework. The core to COTI is a theoretically-guided loss objective instantiated with a comprehensive and novel weighted scoring mechanism, encapsulated by an active-learning paradigm. The extensive results show that 
    
[^129]: 关于算法增强固定点计算的论文

    On algorithmically boosting fixed-point computations. (arXiv:2304.04665v2 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2304.04665](http://arxiv.org/abs/2304.04665)

    本论文提出了一种称为"算法增强"的通用算法抽象，针对纳什均衡问题进行了研究，并展示了该方法可以以指数速度加速线性映射的收敛，同时还可以将非收敛的迭代算法转换为收敛的算法。

    

    本文的主要内容是关于计算纳什均衡的算法。我们将我们的特定方法转化为一个称之为"算法增强"的通用算法抽象，该抽象对于其他固定点计算问题也是相关的。算法增强是通过迭代映射的长期平均来计算固定点的原理，它是指数运算的一种推广。我们首先在非线性映射的框架中定义了我们的方法。其次，我们将注意力限制在收敛的线性映射上（例如，在PageRank算法中计算优势特征向量），并展示了我们的算法增强方法可以以指数速度加速收敛。第三，我们展示算法增强可以将一个（弱）不收敛的迭代算法转换为（强）收敛的算法。我们还考虑了算法增强的一种"变分方法"，该方法提供了将非收敛连续流转换为收敛的工具。

    The main topic of this paper are algorithms for computing Nash equilibria. We cast our particular methods as instances of a general algorithmic abstraction, namely, a method we call {\em algorithmic boosting}, which is also relevant to other fixed-point computation problems. Algorithmic boosting is the principle of computing fixed points by taking (long-run) averages of iterated maps and it is a generalization of exponentiation. We first define our method in the setting of nonlinear maps. Secondly, we restrict attention to convergent linear maps (for computing dominant eigenvectors, for example, in the PageRank algorithm) and show that our algorithmic boosting method can set in motion {\em exponential speedups in the convergence rate}. Thirdly, we show that algorithmic boosting can convert a (weak) non-convergent iterator to a (strong) convergent one. We also consider a {\em variational approach} to algorithmic boosting providing tools to convert a non-convergent continuous flow to a c
    
[^130]: 基于深度强化学习的无图Crowd Navigation与感知风险控制的移动机器人

    Deep Reinforcement Learning-Based Mapless Crowd Navigation with Perceived Risk of the Moving Crowd for Mobile Robots. (arXiv:2304.03593v1 [cs.RO])

    [http://arxiv.org/abs/2304.03593](http://arxiv.org/abs/2304.03593)

    本论文提出了一种基于碰撞概率的无图Crowd Navigation方法，使用深度强化学习(DRL)来感知人群的危险程度，确保机器人在通过拥挤环境时的安全，同时提高模型的可扩展性。

    

    传统的基于地图的机器人导航方法在拥挤环境中往往会遇到“冻结机器人问题”。深度强化学习方法解决了此问题，但是存在泛化和可扩展性的问题。为了克服这些挑战，我们提出一种使用“碰撞概率”来帮助机器人安全通过人群的方法。将“碰撞概率”包括在观察空间中，给机器人提供了一个感知移动人群的危险程度的能力。机器人会在看似安全的情况下穿过人群，但在人群移动过于激烈时会绕路。通过关注最危险的障碍物，机器人不会在人群密度较高时混淆，确保模型的可扩展性。我们的方法使用深度强化学习(DRL)开发，并在Gazebo模拟器中进行了非合作人群环境中的训练，其中的障碍物以随机速度移动。

    Classical map-based navigation methods are commonly used for robot navigation, but they often struggle in crowded environments due to the Frozen Robot Problem (FRP). Deep reinforcement learning-based methods address the FRP problem, however, suffer from the issues of generalization and scalability. To overcome these challenges, we propose a method that uses Collision Probability (CP) to help the robot navigate safely through crowds. The inclusion of CP in the observation space gives the robot a sense of the level of danger of the moving crowd. The robot will navigate through the crowd when it appears safe but will take a detour when the crowd is moving aggressively. By focusing on the most dangerous obstacle, the robot will not be confused when the crowd density is high, ensuring scalability of the model. Our approach was developed using deep reinforcement learning (DRL) and trained using the Gazebo simulator in a non cooperative crowd environment with obstacles moving at randomized sp
    
[^131]: 理解和探索稀疏广义可加模型的整个优秀集合

    Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])

    [http://arxiv.org/abs/2303.16047](http://arxiv.org/abs/2303.16047)

    提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。

    

    在实际应用中，机器学习模型与领域专家之间的交互至关重要；然而，通常只生成单个模型的经典机器学习范式不利于此类交互。近似和探索Rashomon集，即所有近乎最优模型的集合，通过提供用户可搜索的空间包含多样性模型的方法，解决了这一实际挑战，领域专家可以从中选择。我们提出了一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术。我们提供了用于近似具有固定支持集的GAMs的Rashomon集的椭球形算法，并使用这些椭球形近似了许多不同支持集的Rashomon集。近似的Rashomon集为解决实际挑战，例如（1）研究模型类的变量重要性；（2）在用户指定约束条件下查找模型，提供了重要的基础。

    In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
    
[^132]: 向更好的动态图学习迈进：新的架构和统一库

    Towards Better Dynamic Graph Learning: New Architecture and Unified Library. (arXiv:2303.13047v1 [cs.LG])

    [http://arxiv.org/abs/2303.13047](http://arxiv.org/abs/2303.13047)

    我们提出了一种基于Transformer的新型动态图学习架构DyGFormer，并引入了统一的库DyGLib，以促进可重复、可扩展和可信的动态图学习研究。DyGFormer通过邻居共现编码方案和分块技术实现更长期历史的高效推理，在13个不同领域的数据集上实现了最先进的性能。

    

    我们提出了DyGFormer，这是一种基于Transformer的新型动态图学习架构，仅从节点历史的第一跳交互序列中学习。DyGFormer结合了两种不同的设计：一种邻居共现编码方案，探索源节点和目标节点基于它们的序列的相关性；一种分块技术，将每个序列分成多个块并将其馈送给Transformer，使模型能够有效而高效地受益于更长期的历史。我们还引入了DyGLib，这是一个统一的库，具有标准的训练管道、可扩展的编码接口和综合的评估协议，以促进可重复、可伸缩和可信的动态图学习研究。通过在来自各个领域的13个数据集上执行广泛的实验，进行推导/归纳动态链接预测和动态节点分类任务，我们观察到：DyGFormer在mo上实现了最先进的性能

    We propose DyGFormer, a new Transformer-based architecture for dynamic graph learning that solely learns from the sequences of nodes' historical first-hop interactions. DyGFormer incorporates two distinct designs: a neighbor co-occurrence encoding scheme that explores the correlations of the source node and destination node based on their sequences; a patching technique that divides each sequence into multiple patches and feeds them to Transformer, allowing the model to effectively and efficiently benefit from longer histories. We also introduce DyGLib, a unified library with standard training pipelines, extensible coding interfaces, and comprehensive evaluating protocols to promote reproducible, scalable, and credible dynamic graph learning research. By performing extensive experiments on thirteen datasets from various domains for transductive/inductive dynamic link prediction and dynamic node classification tasks, we observe that: DyGFormer achieves state-of-the-art performance on mo
    
[^133]: 联邦学习中的安全聚合并非隐私：通过模型修改大规模泄露用户数据

    Secure Aggregation in Federated Learning is not Private: Leaking User Data at Large Scale through Model Modification. (arXiv:2303.12233v1 [cs.LG])

    [http://arxiv.org/abs/2303.12233](http://arxiv.org/abs/2303.12233)

    联邦学习虽然能够消除数据共享，但共享的梯度可能会包含私密信息，并且攻击者可以通过恶意修改架构和参数或使用优化从共享的梯度中近似用户数据，导致用户数据泄露。

    

    安全和隐私是机器学习中的重要问题。终端用户设备通常包含大量敏感数据，不应与服务器或企业分享。因此，联邦学习被引入以在大规模分散式数据集上进行机器学习，同时通过消除数据共享来保证隐私。然而，先前的研究表明，共享的梯度通常包含私密信息，攻击者可以通过恶意修改架构和参数或使用优化从共享梯度中近似用户数据来获得知识。尽管如此，大多数攻击至今仍受到限制，尤其是在使用安全模型聚合将客户端梯度聚合在一起时会失败。目前仍然可行的攻击在被攻击的客户端数量、泄漏的训练样本数量或训练所需的迭代次数方面都受到严格限制。

    Security and privacy are important concerns in machine learning. End user devices often contain a wealth of data and this information is sensitive and should not be shared with servers or enterprises. As a result, federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. However, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modification of the architecture and parameters or by using optimization to approximate user data from the shared gradients. Despite this, most attacks have so far been limited in scale of number of clients, especially failing when client gradients are aggregated together using secure model aggregation. The attacks that still function are strongly limited in the number of clients attacked, amount of training samples they leak, or number of iterations they take to be trained. I
    
[^134]: 基于机器学习模型的LHCb超快速模拟系统Lamarr在Gauss中的应用

    Lamarr: LHCb ultra-fast simulation based on machine learning models deployed within Gauss. (arXiv:2303.11428v1 [hep-ex])

    [http://arxiv.org/abs/2303.11428](http://arxiv.org/abs/2303.11428)

    LHCb实验中的90%计算资源用于生产模拟数据样本，而Lamarr是一个基于机器学习模型的系统，通过对LHCb实验的探测器响应和重建算法进行参数化，加快了模拟产出。

    

    LHCb实验可用的计算资源的约90%用于生产Large Hadron Collider（LHC）运行2的模拟数据样本。升级后的LHCb探测器将能够收集更多的数据样本，需要更多的模拟事件来分析将在运行3中收集的数据。模拟是分析的关键需求，以解释信号与背景并测量效率。这种需要的模拟将远远超出已承诺的资源，需要技术和技巧的演变来生产这些模拟数据样本。在这项贡献中，我们讨论了Lamarr，这是一种基于Gaudi框架的系统，该系统通过对LHCb实验的探测器响应和重建算法进行参数化，加快了模拟产出。使用基于多种算法和策略的深度生成模型，有效地参数化了LHCb探测器单个组件的高级响应，在神经网络中编码。

    About 90% of the computing resources available to the LHCb experiment has been spent to produce simulated data samples for Run 2 of the Large Hadron Collider at CERN. The upgraded LHCb detector will be able to collect larger data samples, requiring many more simulated events to analyze the data to be collected in Run 3. Simulation is a key necessity of analysis to interpret signal vs background and measure efficiencies. The needed simulation will far exceed the pledged resources, requiring an evolution in technologies and techniques to produce these simulated data samples. In this contribution, we discuss Lamarr, a Gaudi-based framework to speed-up the simulation production parametrizing both the detector response and the reconstruction algorithms of the LHCb experiment. Deep Generative Models powered by several algorithms and strategies are employed to effectively parametrize the high-level response of the single components of the LHCb detector, encoding within neural networks the exp
    
[^135]: 无监督解释性基础抽取用于基于概念的视觉解释

    Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])

    [http://arxiv.org/abs/2303.10523](http://arxiv.org/abs/2303.10523)

    本文提出了一种无监督的方法，通过对CNN进行转换，从而更好地解释中间层的表示，提取了一个可解释性欠完备基础，并证明该方法在各种网络结构和训练数据集上都很有效。

    

    研究人员尝试用人类可以理解的概念来解释CNN图像分类器预测和中间层表示。本文提出了一种无监督后处理方法，通过查找解释像素激活的稀疏二值化转换表示的特征空间旋转来提取解释性欠完备基础。我们对现有的流行CNN进行了实验，并证明了我们方法在网络架构和训练数据集上提取解释性基础的有效性。最后，我们扩展了文献中的基础可解释性度量，并表明，当中间层表示被转换为我们方法提取的基础时，它们变得更易解释。

    An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
    
[^136]: 合并决策Transformer：多任务策略形成的权重平均化

    Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies. (arXiv:2303.07551v1 [cs.LG])

    [http://arxiv.org/abs/2303.07551](http://arxiv.org/abs/2303.07551)

    本文提出通过在权重空间中合并训练于不同 MuJoCo 运动问题上的 Decision Transformer 的子集，形成多任务模型。通过共享一些辅助任务的训练以及共同使用预训练初始化，能够获得更好的结果。这个方向的研究有助于使代理的过程民主化和分发。

    

    最近的研究展示了基于Transformer的通用语言、视觉和连续决策制定问题的策略的前景。为了创建这样的模型，我们通常需要集中的训练目标、数据和计算。如果我们能够更灵活地创建通用策略，通过合并多个任务特定的、单独训练的策略，则这样做就比较有意义。在本文中，我们通过在权重空间中合并或平均不同MuJoCo运动问题上训练的Decision Transformer的子集来迈出这个方向的初步步骤，形成没有集中训练的多任务模型。我们还建议在合并策略时可以获得更好的结果，如果所有策略都从共同的预训练初始化开始，并在问题特定的微调期间共同训练共享的辅助任务。一般来说，我们相信这个方向的研究可以帮助民主化和分发具有一般能力的代理的过程。

    Recent work has shown the promise of creating generalist, transformer-based, policies for language, vision, and sequential decision-making problems. To create such models, we generally require centralized training objectives, data, and compute. It is of interest if we can more flexibly create generalist policies, by merging together multiple, task-specific, individually trained policies. In this work, we take a preliminary step in this direction through merging, or averaging, subsets of Decision Transformers in weight space trained on different MuJoCo locomotion problems, forming multi-task models without centralized training. We also propose that when merging policies, we can obtain better results if all policies start from common, pre-trained initializations, while also co-training on shared auxiliary tasks during problem-specific finetuning. In general, we believe research in this direction can help democratize and distribute the process of which forms generally capable agents.
    
[^137]: CoGANPPIS: 基于共进化增强的全局关注神经网络用于蛋白质相互作用位点预测

    CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction. (arXiv:2303.06945v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2303.06945](http://arxiv.org/abs/2303.06945)

    本论文提出了一种基于共进化增强的全局关注神经网络，它是一种用于蛋白质相互作用位点预测的深度学习模型，结合了共进化特征和全局关注机制以更好地捕捉氨基酸残基之间的关系并考虑到所有残基的贡献。

    

    蛋白质相互作用在生化过程中起着重要作用。准确预测蛋白质相互作用位点（PPIs）可以加深我们对生物机理的理解，并对新药设计至关重要。然而，传统的PPI预测实验方法成本高昂，耗时长，因此近年来开发了许多计算方法，特别是基于机器学习的方法。尽管这些方法取得了令人满意的结果，但仍存在两个限制：（1）大多数模型挖掘了一些有用的输入特征，但未考虑到共进化特征，后者可以提供有关氨基酸残基之间的关系的线索；（2）attention-based模型仅为相邻残基分配关注权重，而不是全局分配，忽略了远离目标残基的一些残基可能也很重要。我们提出了一种共进化增强的全局关注神经网络，这是一种用于PPI位点预测的基于序列的深度学习模型。该模型结合了共进化特征和全局关注机制，以更好地捕捉氨基酸残基之间的关系，并考虑到蛋白序列中所有残基的贡献。

    Protein-protein interactions are essential in biochemical processes. Accurate prediction of the protein-protein interaction sites (PPIs) deepens our understanding of biological mechanism and is crucial for new drug design. However, conventional experimental methods for PPIs prediction are costly and time-consuming so that many computational approaches, especially ML-based methods, have been developed recently. Although these approaches have achieved gratifying results, there are still two limitations: (1) Most models have excavated some useful input features, but failed to take coevolutionary features into account, which could provide clues for inter-residue relationships; (2) The attention-based models only allocate attention weights for neighboring residues, instead of doing it globally, neglecting that some residues being far away from the target residues might also matter.  We propose a coevolution-enhanced global attention neural network, a sequence-based deep learning model for P
    
[^138]: 利用罚函数的深度部分线性Cox模型及其在肺癌患者CT扫描中的应用

    Penalized Deep Partially Linear Cox Models with Application to CT Scans of Lung Cancer Patients. (arXiv:2303.05341v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.05341](http://arxiv.org/abs/2303.05341)

    通过引入罚函数，我们提出了一种创新的深度部分线性Cox模型，用于在肺癌患者的CT扫描中分析死亡风险。该模型能有效地整合已知和新兴的风险因素，解决了参数维度超出样本大小和非参数建模中维度灾难的问题。

    

    肺癌是全球癌症死亡率的主要原因，突出了理解其死亡风险对设计有效的以患者为中心的治疗的重要性。国家肺部筛查试验（NLST）采用了计算机断层扫描纹理分析，提供了CT扫描上纹理模式的客观测量，用于量化肺癌患者的死亡风险。部分线性Cox模型通过将风险函数分解为参数和非参数分量，成为生存分析中备受青睐的方法，可以有效地将已知风险因素（如年龄和临床变量）和新兴风险因素（如图像特征）整合在一个统一的框架中。然而，当参数分量的维度超过样本大小时，模型拟合变得困难，而非参数建模则面临维度灾难的问题。我们提出了一种新颖的罚函数深度部分线性Cox模型（Penali

    Lung cancer is a leading cause of cancer mortality globally, highlighting the importance of understanding its mortality risks to design effective patient-centered therapies. The National Lung Screening Trial (NLST) employed computed tomography texture analysis, which provides objective measurements of texture patterns on CT scans, to quantify the mortality risks of lung cancer patients. Partially linear Cox models have gained popularity for survival analysis by dissecting the hazard function into parametric and nonparametric components, allowing for the effective incorporation of both well-established risk factors (such as age and clinical variables) and emerging risk factors (e.g., image features) within a unified framework. However, when the dimension of parametric components exceeds the sample size, the task of model fitting becomes formidable, while nonparametric modeling grapples with the curse of dimensionality. We propose a novel Penalized Deep Partially Linear Cox Model (Penali
    
[^139]: GOATS：目标采样自适应课程强化学习用于舀取任务

    GOATS: Goal Sampling Adaptation for Scooping with Curriculum Reinforcement Learning. (arXiv:2303.05193v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.05193](http://arxiv.org/abs/2303.05193)

    本文提出了一种名为GOATS的方法，使用目标采样自适应课程强化学习技术，通过插值位置目标和数量目标的分布创建学习过程中的课程来解决机器人舀取任务中的位置目标和水量目标问题，取得了比基线更好的表现。

    

    本文首先使用目标条件强化学习对机器人舀取水的问题进行了阐述。由于流体的复杂动力学和实现多模式目标的需求，该任务具有特别的挑战性。政策需要成功地达到位置目标和水量目标，这导致一个庞大而复杂的目标状态空间。为了克服这些挑战，我们引入了GOATS，一种课程强化学习方法，通过插值位置目标分布和数量目标分布来创建学习过程中的课程，使用目标分解奖励公式，学习一个高效且具有通用性的机器人舀取策略。结果，我们的方法可以在仿真中表现出比基线更好的性能，分别在碗舀和桶舀任务中实现了5.46％和8.71％的误差，涵盖了1000种初始水状态的变化。

    In this work, we first formulate the problem of robotic water scooping using goal-conditioned reinforcement learning. This task is particularly challenging due to the complex dynamics of fluid and the need to achieve multi-modal goals. The policy is required to successfully reach both position goals and water amount goals, which leads to a large convoluted goal state space. To overcome these challenges, we introduce Goal Sampling Adaptation for Scooping (GOATS), a curriculum reinforcement learning method that can learn an effective and generalizable policy for robot scooping tasks. Specifically, we use a goal-factorized reward formulation and interpolate position goal distributions and amount goal distributions to create curriculum throughout the learning process. As a result, our proposed method can outperform the baselines in simulation and achieves 5.46% and 8.71% amount errors on bowl scooping and bucket scooping tasks, respectively, under 1000 variations of initial water states in
    
[^140]: 旅行需求预测：公正的人工智能方法

    Travel Demand Forecasting: A Fair AI Approach. (arXiv:2303.01692v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.01692](http://arxiv.org/abs/2303.01692)

    本研究提出了一种新的方法来开发具有公平意识的、高度准确的旅行需求预测模型，该方法可以同时提高AI模型对于多个受保护属性的公平性。

    

    人工智能（AI）和机器学习越来越多地被用于旅行需求预测。尽管基于AI的旅行需求预测模型能产生准确的预测，但可能会产生预测偏差并引发公平性问题。使用这些有偏见模型进行决策可能会导致加剧社会不平等的交通政策。然而，目前对于解决这些模型的公平性问题的研究有限。因此，在本研究中，我们提出了一种新的方法来开发具有公平意识的、高度准确的旅行需求预测模型。特别地，我们提出的方法可以同时提高AI模型对于多个受保护属性（如种族和收入）的公平性。具体来说，我们引入了一个新的公平性正则化项，该项明确地设计用于衡量预测准确性与多个受保护属性之间的相关性，并将其加入到旅行需求预测模型的损失函数中。

    Artificial Intelligence (AI) and machine learning have been increasingly adopted for travel demand forecasting. The AI-based travel demand forecasting models, though generate accurate predictions, may produce prediction biases and raise fairness issues. Using such biased models for decision-making may lead to transportation policies that exacerbate social inequalities. However, limited studies have been focused on addressing the fairness issues of these models. Therefore, in this study, we propose a novel methodology to develop fairness-aware, highly-accurate travel demand forecasting models. Particularly, the proposed methodology can enhance the fairness of AI models for multiple protected attributes (such as race and income) simultaneously. Specifically, we introduce a new fairness regularization term, which is explicitly designed to measure the correlation between prediction accuracy and multiple protected attributes, into the loss function of the travel demand forecasting model. We
    
[^141]: 面向决策的可微子模最大化学习在车辆路径问题中的应用

    Decision-Oriented Learning with Differentiable Submodular Maximization for Vehicle Routing Problem. (arXiv:2303.01543v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2303.01543](http://arxiv.org/abs/2303.01543)

    本文研究了面向车辆路径问题的决策导向学习方法，该方法利用可微子模最大化学习函数，将上下文观察映射到子模函数参数。该研究强调了在考虑下游任务优化的情况下，传统的独立解决预测阶段的学习问题可能导致与最终目标不一致的结果。

    

    本文研究了学习一个将上下文观察（输入）映射到子模函数参数（输出）的函数的问题。我们的案例研究是一种特定类型的车辆路径问题，其中一组无人地面车辆（UGV）可以作为移动充电站为执行持续监测任务的无人机地面车辆（UAV）充电。我们希望学习从UAV任务路径和风场观察到子模目标函数参数的映射，该函数描述了UAV的降落位置分布。传统上，这类学习问题独立解决预测阶段，而不考虑下游任务优化阶段。然而，预测中使用的损失函数可能与我们的最终目标，即良好航线决策，不一致。在孤立的预测阶段表现良好并不一定会导致下游路径任务中的良好决策。

    We study the problem of learning a function that maps context observations (input) to parameters of a submodular function (output). Our motivating case study is a specific type of vehicle routing problem, in which a team of Unmanned Ground Vehicles (UGVs) can serve as mobile charging stations to recharge a team of Unmanned Ground Vehicles (UAVs) that execute persistent monitoring tasks. {We want to learn the mapping from observations of UAV task routes and wind field to the parameters of a submodular objective function, which describes the distribution of landing positions of the UAVs .} Traditionally, such a learning problem is solved independently as a prediction phase without considering the downstream task optimization phase. However, the loss function used in prediction may be misaligned with our final goal, i.e., a good routing decision. Good performance in the isolated prediction phase does not necessarily lead to good decisions in the downstream routing task. In this paper, we 
    
[^142]: 基于聚类技术的灵活能源社区目标需求响应

    Targeted demand response for flexible energy communities using clustering techniques. (arXiv:2303.00186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00186](http://arxiv.org/abs/2303.00186)

    本研究探讨了使用机器学习算法中的聚类技术设计并执行需求响应（DR）计划的可行性，目的是改变分布式能源社区内供应者的消费行为，以最小化反向功率流和削减系统范围内的功峰需求。

    

    本研究探讨了使用聚类技术为商业和住宅社区的能量供应者设计和执行需求响应（DR）计划的可能性。该计划的目的是改变意大利分布式能源社区内的供应者的消费行为。这种聚合旨在：a）最小化在主要变电站处产生的反向功率流，该功率流在当地电网中的太阳能电池的发电量超过消耗时会发生; b）削减系统范围内的功峰需求，该需求通常发生在傍晚时分。在聚类阶段，我们采用了三种热门的电负荷聚类机器学习算法-即k-means，k-medoids和一种聚合层次聚类-alongside两种不同的距离度量-即欧几里得距离和受限动态时间扭曲（DTW）。我们使用多个验证度量来评估这些方法，包括一项新颖的指标-即峰值性能评分（PPS）

    The present study explores the use of clustering techniques for the design and implementation of a demand response (DR) program for commercial and residential prosumers. The goal of the program is to alter the consumption behavior of the prosumers pertaining to a distributed energy community in Italy. This aggregation aims to: a) minimize the reverse power flow at the primary substation, that occurs when generation from solar panels in the local grid exceeds consumption, and b) shave the system wide peak demand, that typically occurs during the hours of late afternoon. Regarding the clustering stage, three popular machine learning algorithms for electrical load clustering are employed -namely k-means, k-medoids and an agglomerative hierarchical clustering- alongside two different distance measures -namely euclidean and constrained dynamic time warping (DTW). We evaluate the methods using multiple validation metrics including a novel metric -namely peak performance score (PPS)- that we 
    
[^143]: 置换等变神经功能网络

    Permutation Equivariant Neural Functionals. (arXiv:2302.14040v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14040](http://arxiv.org/abs/2302.14040)

    本文介绍了置换等变神经功能网络的设计，通过对权重进行置换对称性编码，实现对其他网络权重或梯度进行处理，为学习优化、处理隐式神经表示等应用提供了架构原则。

    

    本文研究了能够处理其他神经网络的权重或梯度的神经网络的设计，我们将其称为神经功能网络（NFN）。尽管具有广泛的潜在应用，包括学习优化、处理隐式神经表示、网络编辑和策略评估，但设计处理其他网络权重的有效架构的统一原则很少。我们通过对称性的视角来设计神经功能，特别是通过关注深度前馈网络权重中出现的置换对称性，因为隐藏层神经元没有固有顺序。我们介绍了一种构建置换等变神经功能的框架，该框架将这些对称性编码为归纳偏差。该框架的关键组成部分是我们通过适当的参数来约束为置换等变的NF-Layers（神经功能层）。

    This work studies the design of neural networks that can process the weights or gradients of other neural networks, which we refer to as neural functional networks (NFNs). Despite a wide range of potential applications, including learned optimization, processing implicit neural representations, network editing, and policy evaluation, there are few unifying principles for designing effective architectures that process the weights of other networks. We approach the design of neural functionals through the lens of symmetry, in particular by focusing on the permutation symmetries that arise in the weights of deep feedforward networks because hidden layer neurons have no inherent order. We introduce a framework for building permutation equivariant neural functionals, whose architectures encode these symmetries as an inductive bias. The key building blocks of this framework are NF-Layers (neural functional layers) that we constrain to be permutation equivariant through an appropriate paramet
    
[^144]: DeepBrainPrint: 一种用于脑MRI再识别的新型对比框架

    DeepBrainPrint: A Novel Contrastive Framework for Brain MRI Re-Identification. (arXiv:2302.13057v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2302.13057](http://arxiv.org/abs/2302.13057)

    DeepBrainPrint是一个用于脑MRI再识别的对比框架，使用半自监督对比深度学习方法，通过创建有效的脑指纹进行实时图像检索，并引入新的图像转换方法以提高检索的稳健性和考虑患者的年龄和疾病进展。

    

    近年来，MRI技术的进步导致了大型数据集的创建。随着数据量的增加，在这些数据集中定位同一患者的先前扫描变得困难（这个过程称为再识别）。为了解决这个问题，我们提出了一个名为DeepBrainPrint的人工智能医学图像检索框架，用于检索同一患者的脑MRI扫描。我们的框架是一种半自监督对比深度学习方法，具有三个主要创新。首先，我们使用自监督和监督范式的组合来创建有效的脑指纹，以用于实时图像检索。其次，我们使用特殊的加权函数来引导训练并提高模型收敛性。第三，我们引入了新的图像转换方法，以提高在不同扫描对比度（即不同扫描反差）下的检索稳健性，并考虑患者的年龄和疾病进展。

    Recent advances in MRI have led to the creation of large datasets. With the increase in data volume, it has become difficult to locate previous scans of the same patient within these datasets (a process known as re-identification). To address this issue, we propose an AI-powered medical imaging retrieval framework called DeepBrainPrint, which is designed to retrieve brain MRI scans of the same patient. Our framework is a semi-self-supervised contrastive deep learning approach with three main innovations. First, we use a combination of self-supervised and supervised paradigms to create an effective brain fingerprint from MRI scans that can be used for real-time image retrieval. Second, we use a special weighting function to guide the training and improve model convergence. Third, we introduce new imaging transformations to improve retrieval robustness in the presence of intensity variations (i.e. different scan contrasts), and to account for age and disease progression in patients. We t
    
[^145]: 深度学习模型在提前一天负荷预测中的比较评估：研究关键的准确性影响因素。

    A comparative assessment of deep learning models for day-ahead load forecasting: Investigating key accuracy drivers. (arXiv:2302.12168v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.12168](http://arxiv.org/abs/2302.12168)

    本文通过比较评估深度学习模型在提前一天负荷预测中的准确性，重点研究了葡萄牙国家净聚合STLF，并分析了多层感知机（MLP）、长短期记忆网络（LSTM）、神经基础扩展系数分析（N-BEATS）、时间卷积网络（TCN）和时间融合变压器（TFT）等多个模型的影响因素。

    

    短期负荷预测（STLF）对电网和能源市场的有效和经济运行至关重要。然而，电力需求的非线性和非平稳性以及其对各种外部因素的依赖性使得STLF成为一项具有挑战性的任务。为了评估这些模型在提前一天的预测环境下的准确性，在本文中我们专注于葡萄牙的国家净聚合STLF，并进行了一项比较研究，考虑了一组有指示性的、被广泛接受的深度自回归模型，包括多层感知机（MLP）、长短期记忆网络（LSTM）、神经基础扩展系数分析（N-BEATS）、时间卷积网络（TCN）和时间融合变压器（TFT）。此外，我们还确定了显著影响需求的因素，并研究了它们对每个模型准确性的影响。

    Short-term load forecasting (STLF) is vital for the effective and economic operation of power grids and energy markets. However, the non-linearity and non-stationarity of electricity demand as well as its dependency on various external factors renders STLF a challenging task. To that end, several deep learning models have been proposed in the literature for STLF, reporting promising results. In order to evaluate the accuracy of said models in day-ahead forecasting settings, in this paper we focus on the national net aggregated STLF of Portugal and conduct a comparative study considering a set of indicative, well-established deep autoregressive models, namely multi-layer perceptrons (MLP), long short-term memory networks (LSTM), neural basis expansion coefficient analysis (N-BEATS), temporal convolutional networks (TCN), and temporal fusion transformers (TFT). Moreover, we identify factors that significantly affect the demand and investigate their impact on the accuracy of each model. O
    
[^146]: 一站式解决方案：利用预训练 LM 进行强大的时间序列分析

    One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11939](http://arxiv.org/abs/2302.11939)

    本论文提出了一种称为 Frozen Pretrained Transformer (FPT) 的预训练模型，利用从数十亿标记训练出来的语言或 CV 模型进行时间序列分析的所有主要类型任务的微调，进而使其在所有任务中都具备着最先进的性能和泛化能力。

    

    尽管预训练模型在自然语言处理 (NLP) 和计算机视觉 (CV) 领域取得了巨大成功，但在通用时间序列分析领域取得的进展有限。与 NLP 和 CV 不同的是，这些领域采用统一模型即可执行不同的任务，而在每个时间序列分析任务中，专门设计的方法仍然占据主导地位，如分类、异常检测、预测和少样本学习。阻碍预训练模型发展的主要挑战是缺乏大量用于训练的数据。在本文中，我们通过利用从数十亿标记训练出来的语言或 CV 模型，来解决这一挑战，用于时间序列分析。具体而言，我们避免改变预训练语言或图像模型中残差块中的自注意力和前向传递层。这种模型被称为冻结的预训练变压器 (FPT)，通过对涉及时间序列分析的所有主要类型的任务进行微调进行评估，包括分类、异常检测、预测和少样本学习等。实验结果证明，FPT 在所有任务中都具有最先进的性能和泛化能力。

    Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
    
[^147]: 图对抗免疫以实现可证明的鲁棒性

    Graph Adversarial Immunization for Certifiable Robustness. (arXiv:2302.08051v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.08051](http://arxiv.org/abs/2302.08051)

    本文提出了图对抗免疫方法，通过疫苗接种一部分图结构来提高图的鲁棒性，避免对抗性攻击。通过边和节点两种免疫方式，可以有效地提高图的抵抗能力。

    

    尽管图神经网络（GNNs）取得了巨大的成功，但它们对抗对抗性攻击是脆弱的。现有的防御方法集中在开发对抗性训练或模型修改。在本文中，我们提出并制定了图对抗免疫，即通过疫苗接种图结构的一部分来改善图的可证明鲁棒性对抗任何可接受的对抗性攻击。我们首先提出了基于边的免疫，来对节点对进行疫苗接种。不幸的是，这种基于边的免疫不能抵御新出现的节点注入攻击，因为它只对现有的节点对进行免疫。为此，我们进一步提出了基于节点的免疫。为了避免与对抗性免疫相关的计算密集的组合优化问题，我们开发了AdvImmune-Edge和AdvImmune-Node算法，以有效获取免疫节点对或节点。大量实验证明了AdvImmune方法的优越性。特别是，AdvImmune-Node显著提高了r的比例。

    Despite achieving great success, graph neural networks (GNNs) are vulnerable to adversarial attacks. Existing defenses focus on developing adversarial training or model modification. In this paper, we propose and formulate graph adversarial immunization, i.e., vaccinating part of graph structure to improve certifiable robustness of graph against any admissible adversarial attack. We first propose edge-level immunization to vaccinate node pairs. Unfortunately, such edge-level immunization cannot defend against emerging node injection attacks, since it only immunizes existing node pairs. To this end, we further propose node-level immunization. To avoid computationally intensive combinatorial optimization associated with adversarial immunization, we develop AdvImmune-Edge and AdvImmune-Node algorithms to effectively obtain the immune node pairs or nodes. Extensive experiments demonstrate the superiority of AdvImmune methods. In particular, AdvImmune-Node remarkably improves the ratio of r
    
[^148]: 具有一般激活函数的深度平衡模型的全局收敛速度

    Global Convergence Rate of Deep Equilibrium Models with General Activations. (arXiv:2302.05797v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05797](http://arxiv.org/abs/2302.05797)

    该论文研究了具有一般激活函数的深度平衡模型（DEQ）的全局收敛速度，证明了梯度下降以线性收敛速度收敛到全局最优解，并解决了限制平衡点Gram矩阵最小特征值的挑战。

    

    在最近的一篇论文中，Ling等人研究了具有ReLU激活函数的过参数化深度平衡模型（DEQ）。他们证明了对于二次损失函数，梯度下降方法以线性收敛速度收敛到全局最优解。本文表明，对于具有任何具有有界一阶和二阶导数的激活函数的DEQ，该事实仍然成立。由于新的激活函数通常是非线性的，限制平衡点的Gram矩阵的最小特征值尤其具有挑战性。为了完成这个任务，我们需要创建一个新的总体Gram矩阵，并开发一种具有Hermite多项式展开的新形式的双重激活函数。

    In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-linear, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.
    
[^149]: 预测简化Gr\"obner基数和最大度数的神经网络回归模型

    Predicting the cardinality and maximum degree of a reduced Gr\"obner basis. (arXiv:2302.05364v2 [math.AC] UPDATED)

    [http://arxiv.org/abs/2302.05364](http://arxiv.org/abs/2302.05364)

    该论文利用神经网络回归模型预测了简化Gr\"obner基的基数和最大总度数，结果表明神经网络具有更好的性能统计，相比于朴素猜测或多元回归模型。

    

    我们构建了神经网络回归模型，用以预测二项理想的Gr\"obner基复杂度的关键指标。这项工作说明了为什么利用神经网络从Gr\"obner计算中进行预测并不是一个简单的过程。我们使用两个概率模型来生成和提供一个大规模的数据集，能够捕捉到Gr\"obner复杂度的足够变异性。我们利用这些数据来训练神经网络并预测简化Gr\"obner基的基数和其元素的最大总度数。虽然基数预测问题不同于经典的机器学习问题，但我们的模拟结果表明，神经网络具有更好的性能统计，如$r^2 = 0.401$，相比于朴素猜测或多元回归模型的$r^2 = 0.180$。

    We construct neural network regression models to predict key metrics of complexity for Gr\"obner bases of binomial ideals. This work illustrates why predictions with neural networks from Gr\"obner computations are not a straightforward process. Using two probabilistic models for random binomial ideals, we generate and make available a large data set that is able to capture sufficient variability in Gr\"obner complexity. We use this data to train neural networks and predict the cardinality of a reduced Gr\"obner basis and the maximum total degree of its elements. While the cardinality prediction problem is unlike classical problems tackled by machine learning, our simulations show that neural networks, providing performance statistics such as $r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 = 0.180$.
    
[^150]: 基于图的建模框架用于追踪表面水体中水文污染物的传输

    A Graph-Based Modeling Framework for Tracing Hydrological Pollutant Transport in Surface Waters. (arXiv:2302.04991v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04991](http://arxiv.org/abs/2302.04991)

    本研究提出了一种名为 HydroGraphs 的基于图的建模框架，用于分析水文污染物的传输和命运。该框架可以根据开源数据构建，具有简化的水文系统表示，并且可以使用常见的图分析和数据可视化技术进行分析和可视化，能够帮助准确定位污染源和脆弱区域。

    

    人类污染对全球各地的社区和生态系统产生了影响。数据分析和建模工具在应对这个挑战中发挥了关键作用，因为它们可以帮助识别关键源，并在复杂的水文系统中追踪传输和量化影响。本文提出了一个称为“${\tt HydroGraphs}$”的图模型框架，用于理解水体、河流和流域中污染物的传输和命运。该框架使用简化的水文系统表示，可以基于开放源数据（国家水文数据集和流域边界数据集）构建，并可以使用常见的图分析和数据可视化技术进行分析和可视化。通过对上密西西比河流域的案例研究，我们展示了该框架发现污染源、理解运输途径和准确定位脆弱区域的潜力。

    Anthropogenic pollution of hydrological systems affects diverse communities and ecosystems around the world. Data analytics and modeling tools play a key role in fighting this challenge, as they can help identify key sources as well as trace transport and quantify impact within complex hydrological systems. Several tools exist for simulating and tracing pollutant transport throughout surface waters using detailed physical models; these tools are powerful, but can be computationally intensive, require significant amounts of data to be developed, and require expert knowledge for their use (ultimately limiting application scope). In this work, we present a graph modeling framework -which we call ${\tt HydroGraphs}$ -- for understanding pollutant transport and fate across waterbodies, rivers, and watersheds. This framework uses a simplified representation of hydrological systems that can be constructed based purely on open-source data (National Hydrography Dataset and Watershed Boundary 
    
[^151]: 通过因果干预进行潜在表示的解缠

    Disentanglement of Latent Representations via Causal Interventions. (arXiv:2302.00869v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00869](http://arxiv.org/abs/2302.00869)

    该论文提出了一种基于因果干预的解缠方法，通过将量化向量视为因果变量，并在因果图中进行干预，生成影响图像中唯一变异因素的原子过渡。

    

    生成图像等数据的过程由独立且未知的变异因素控制。在解缠、因果表示学习和独立成分分析领域中，人们广泛研究了这些变量的检索。最近，将这些领域合并的方法取得了巨大成功。解缠问题可以看作是找到能够使一张图像发生单一因素改变的干预。基于这一假设，我们提出了一种新的解缠方法，受因果动态的启发，将因果理论与向量量化变分自编码器相结合。我们的模型将量化向量视为因果变量，并在因果图中将它们相连。它在图中进行因果干预，生成影响图像中唯一变异因素的原子过渡。我们还引入了一项新的动作检索任务，涉及...

    The process of generating data such as images is controlled by independent and unknown factors of variation. The retrieval of these variables has been studied extensively in the disentanglement, causal representation learning, and independent component analysis fields. Recently, approaches merging these domains together have shown great success. Instead of directly representing the factors of variation, the problem of disentanglement can be seen as finding the interventions on one image that yield a change to a single factor. Following this assumption, we introduce a new method for disentanglement inspired by causal dynamics that combines causality theory with vector-quantized variational autoencoders. Our model considers the quantized vectors as causal variables and links them in a causal graph. It performs causal interventions on the graph and generates atomic transitions affecting a unique factor of variation in the image. We also introduce a new task of action retrieval that consis
    
[^152]: Off-the-Grid MARL: 带有基准的离线多智能体增强学习数据集

    Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning. (arXiv:2302.00521v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00521](http://arxiv.org/abs/2302.00521)

    这项工作填补了离线多智能体增强学习（MARL）领域的一个空白，提供了Off-the-Grid MARL（OG-MARL）数据集和基准，帮助社区衡量进展。

    

    能够利用大型数据集开发合作多智能体控制器，为实际应用开启了巨大的价值。许多重要的工业系统是多智能体的，并且很难使用定制的模拟器进行建模。然而，在工业中，分布式进程经常可以在运行期间记录，并存储大量的演示数据。离线多智能体增强学习（MARL）为利用这些数据建立有效的分散式控制器提供了有希望的范例。然而，离线MARL仍处于起步阶段，因此缺乏在强化学习更成熟的子领域中通常会找到的标准化基准数据集和基线。这些不足使得社区无法合理地衡量进展。在这项工作中，我们旨在通过发布Off-the-Grid MARL（OG-MARL）来填补这个空白：一个不断增长的高质量数据集存储库，其中包含协作离线MARL的基准。

    Being able to harness the power of large datasets for developing cooperative multi-agent controllers promises to unlock enormous value for real-world applications. Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. However, in industry, distributed processes can often be recorded during operation, and large quantities of demonstrative data stored. Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective decentralised controllers from such datasets. However, offline MARL is still in its infancy and therefore lacks standardised benchmark datasets and baselines typically found in more mature subfields of reinforcement learning (RL). These deficiencies make it difficult for the community to sensibly measure progress. In this work, we aim to fill this gap by releasing off-the-grid MARL (OG-MARL): a growing repository of high-quality datasets with baselines for cooperative offline MARL
    
[^153]: 基于坐标下降的多层超图中的拉普拉斯半监督学习

    Laplacian-based Semi-Supervised Learning in Multilayer Hypergraphs by Coordinate Descent. (arXiv:2301.12184v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12184](http://arxiv.org/abs/2301.12184)

    本文研究了多层超图中的拉普拉斯半监督学习问题，并采用坐标下降方法解决该问题。实验证明了使用适当选择规则的坐标下降方法的潜力。

    

    图半监督学习是一种重要的数据分析工具，给定一个图和一组标有标签的节点，目标是为剩余的未标记节点推断标签。本文首先考虑了无向图问题的基于优化的表述，然后将这个表述扩展到多层超图上。我们使用不同的坐标下降方法解决了这个问题，并将结果与经典的梯度下降方法进行了比较。在合成和真实世界的数据集上的实验证明了使用具有适当选择规则的坐标下降方法的潜力。

    Graph Semi-Supervised learning is an important data analysis tool, where given a graph and a set of labeled nodes, the aim is to infer the labels to the remaining unlabeled nodes. In this paper, we start by considering an optimization-based formulation of the problem for an undirected graph, and then we extend this formulation to multilayer hypergraphs. We solve the problem using different coordinate descent approaches and compare the results with the ones obtained by the classic gradient descent method. Experiments on synthetic and real-world datasets show the potential of using coordinate descent methods with suitable selection rules.
    
[^154]: Salesforce CausalAI库: 用于时间序列和表格数据因果分析的快速可扩展框架

    Salesforce CausalAI Library: A Fast and Scalable Framework for Causal Analysis of Time Series and Tabular Data. (arXiv:2301.10859v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10859](http://arxiv.org/abs/2301.10859)

    Salesforce CausalAI库是一个快速可扩展的框架，用于进行时间序列和表格数据的因果分析。它支持离散、连续和异质类型的数据，提供了处理线性和非线性因果关系的算法，并包括一个用于生成具有指定结构方程模型的合成数据的数据生成器。用户可以通过用户界面进行因果分析，无需编程。

    

    我们介绍了Salesforce CausalAI库，这是一个用于使用观测数据进行因果分析的开源库。它支持离散、连续和异质类型的表格和时间序列数据的因果发现和因果推断。该库包括处理变量之间线性和非线性因果关系的算法，并使用多处理进行加速。我们还提供了一个数据生成器，可以生成具有指定结构方程模型的合成数据，以帮助用户在研究各种算法的同时控制基础因果过程。最后，我们提供了一个用户界面（UI），使用户可以在无需编程的情况下对数据进行因果分析。该库旨在提供一种快速灵活的解决方案，用于解决因果性领域中的各种问题。本技术报告描述了Salesforce CausalAI API及其功能以及s的实现。

    We introduce the Salesforce CausalAI Library, an open-source library for causal analysis using observational data. It supports causal discovery and causal inference for tabular and time series data, of discrete, continuous and heterogeneous types. This library includes algorithms that handle linear and non-linear causal relationships between variables, and uses multi-processing for speed-up. We also include a data generator capable of generating synthetic data with specified structural equation model for the aforementioned data formats and types, that helps users control the ground-truth causal process while investigating various algorithms. Finally, we provide a user interface (UI) that allows users to perform causal analysis on data without coding. The goal of this library is to provide a fast and flexible solution for a variety of problems in the domain of causality. This technical report describes the Salesforce CausalAI API along with its capabilities, the implementations of the s
    
[^155]: 异步深度双对决Q学习在限价交易市场中的交易信号执行

    Asynchronous Deep Double Duelling Q-Learning for Trading-Signal Execution in Limit Order Book Markets. (arXiv:2301.08688v2 [q-fin.TR] UPDATED)

    [http://arxiv.org/abs/2301.08688](http://arxiv.org/abs/2301.08688)

    该论文利用深度强化学习训练了一个代理，将高频交易信号转化为交易策略，并使用异步双对决Q学习进行交易信号执行。该方法独立研究了适应性交易的性能以及与具体的预测算法无关的影响。

    

    我们利用深度强化学习（RL）训练一个代理，将高频交易信号成功转化为能够下达独立限价订单的交易策略。基于ABIDES限价订单簿模拟器，我们构建了一个强化学习OpenAI gym环境，并利用它在基于历史订单簿消息的NASDAQ股票交易环境中进行了模拟。为了训练一个能够在此环境中最大化交易回报的交易代理，我们使用了Deep Duelling Double Q-learning与APEX（异步优先经验回放）架构。代理观察当前限价订单簿状态、其最近历史和短期方向预测。为了独立地研究适应性交易的RL性能而不涉及具体的预测算法，我们使用通过扰动前瞻收益获得的合成alpha信号来研究我们方法的性能，这些信号具有不同级别的噪声。

    We employ deep reinforcement learning (RL) to train an agent to successfully translate a high-frequency trading signal into a trading strategy that places individual limit orders. Based on the ABIDES limit order book simulator, we build a reinforcement learning OpenAI gym environment and utilise it to simulate a realistic trading environment for NASDAQ equities based on historic order book messages. To train a trading agent that learns to maximise its trading return in this environment, we use Deep Duelling Double Q-learning with the APEX (asynchronous prioritised experience replay) architecture. The agent observes the current limit order book state, its recent history, and a short-term directional forecast. To investigate the performance of RL for adaptive trading independently from a concrete forecasting algorithm, we study the performance of our approach utilising synthetic alpha signals obtained by perturbing forward-looking returns with varying levels of noise. Here, we find that 
    
[^156]: 自动文本摘要技术的综合回顾：方法、数据、评估和编码

    A comprehensive review of automatic text summarization techniques: method, data, evaluation and coding. (arXiv:2301.03403v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2301.03403](http://arxiv.org/abs/2301.03403)

    本文提供了关于自动文本摘要系统的综述，包括方法、数据、评估和编码。作者通过引用的方式回顾了相关文献，并介绍了不同的摘要生成方法。此外，还对可用于评估和数据训练的数据集进行了综述，并使用CNN语料库数据集对方法进行了实证探索。

    

    我们提供了关于自动文本摘要系统的文献综述。我们采用基于引用的方法。我们从我们已经掌握的关于每个我们想涵盖的主题的一些流行和著名的论文开始，然后我们追踪了“向后引用”（被我们之前知道的一系列论文引用的论文）和“向前引用”（引用我们之前知道的一系列论文的较新论文）。为了组织不同的方法，我们介绍了各种基于不同机制生成摘要的自动文本摘要方法。除了介绍方法外，我们还对可用于摘要任务的数据集和用于评估摘要质量的方法进行了广泛的回顾。最后，我们还使用CNN语料库数据集对这些方法进行了实证探索，该数据集为抽取式和生成式方法提供了金标准摘要。

    We provide a literature review about Automatic Text Summarization (ATS) systems. We consider a citation-based approach. We start with some popular and well-known papers that we have in hand about each topic we want to cover and we have tracked the "backward citations" (papers that are cited by the set of papers we knew beforehand) and the "forward citations" (newer papers that cite the set of papers we knew beforehand). In order to organize the different methods, we present the diverse approaches to ATS guided by the mechanisms they use to generate a summary. Besides presenting the methods, we also present an extensive review of the datasets available for summarization tasks and the methods used to evaluate the quality of the summaries. Finally, we present an empirical exploration of these methods using the CNN Corpus dataset that provides golden summaries for extractive and abstractive methods.
    
[^157]: StitchNet: 从预训练片段组合神经网络

    StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.01947](http://arxiv.org/abs/2301.01947)

    StitchNet提出了一种新的神经网络创建方式，它通过组合预训练神经网络的片段来创建高性能的网络，无需传统训练的大量计算资源和数据要求。通过居中核对齐（CKA），可以有效指导片段的选择，以满足特定准确性需求和计算资源限制。此外，StitchNet还可以实现即时个性化模型创建和推断。

    

    我们提出了一种新颖的神经网络创建范式StitchNet，它将来自多个预训练神经网络的片段（一个或多个连续的网络层）拼接在一起。StitchNet允许创建高性能的神经网络，而无需传统的基于反向传播训练的大量计算和数据要求。我们利用居中核对齐（CKA）作为一种兼容性度量，以有效地指导选择这些片段，以组合适合特定准确性需求和计算资源限制的任务网络。然后，我们展示了这些片段可以被拼接在一起，以在计算资源和数据要求的一小部分下创建与传统训练网络相媲美准确度的神经网络。最后，我们探索了这种新范式所能实现的一种新颖的即时个性化模型创建和推断应用。

    We propose StitchNet, a novel neural network creation paradigm that stitches together fragments (one or more consecutive network layers) from multiple pre-trained neural networks. StitchNet allows the creation of high-performing neural networks without the large compute and data requirements needed under traditional model creation processes via backpropagation training. We leverage Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide the selection of these fragments in composing a network for a given task tailored to specific accuracy needs and computing resource constraints. We then show that these fragments can be stitched together to create neural networks with comparable accuracy to traditionally trained networks at a fraction of computing resource and data requirements. Finally, we explore a novel on-the-fly personalized model creation and inference application enabled by this new paradigm.
    
[^158]: 面向高效自主导航的目标引导变压器增强学习

    Goal-Guided Transformer-Enabled Reinforcement Learning for Efficient Autonomous Navigation. (arXiv:2301.00362v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2301.00362](http://arxiv.org/abs/2301.00362)

    该论文提出了一种利用目标引导变压器增强学习的方法，通过将目标信息与场景表示耦合，实现高效自主导航。通过使用专家先验进行预训练，提高了数据效率。

    

    尽管目标驱动导航有些成功应用，但现有的基于深度强化学习(DRL)的方法在数据效率方面存在明显问题。其中一个原因是目标信息与感知模块解耦，并直接作为决策的条件引入，导致场景表示中与目标无关的特征在学习过程中起到对抗作用。鉴于此，我们提出了一种新颖的目标引导变压器增强学习(GTRL)方法，通过将物理目标状态作为场景编码器的输入来指导场景表示与目标信息的耦合，并实现高效的自主导航。具体而言，我们提出了一种新的视觉变压器变体作为感知系统的骨干，即目标引导变压器(GoT)，并使用专家先验进行预训练，以提高数据效率。随后，我们设计了一个增强学习算法进行导航决策。

    Despite some successful applications of goal-driven navigation, existing deep reinforcement learning (DRL)-based approaches notoriously suffers from poor data efficiency issue. One of the reasons is that the goal information is decoupled from the perception module and directly introduced as a condition of decision-making, resulting in the goal-irrelevant features of the scene representation playing an adversary role during the learning process. In light of this, we present a novel Goal-guided Transformer-enabled reinforcement learning (GTRL) approach by considering the physical goal states as an input of the scene encoder for guiding the scene representation to couple with the goal information and realizing efficient autonomous navigation. More specifically, we propose a novel variant of the Vision Transformer as the backbone of the perception system, namely Goal-guided Transformer (GoT), and pre-train it with expert priors to boost the data efficiency. Subsequently, a reinforcement le
    
[^159]: MolCPT：分子连续提示调整以推广分子表示学习

    MolCPT: Molecule Continuous Prompt Tuning to Generalize Molecular Representation Learning. (arXiv:2212.10614v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10614](http://arxiv.org/abs/2212.10614)

    该论文介绍了一种新的训练范式"MolCPT"，用于改善图神经网络在分子表示学习中的泛化能力。该方法利用任务相关的模式子图来进行预训练和微调，以提高对广泛的分子空间的推广能力。

    

    分子表示学习对于分子属性预测问题至关重要，而图神经网络(GNNs)由于其结构建模能力成为了一种有效的解决方案。由于标记数据通常稀缺且昂贵，对GNNs在广泛的分子空间中进行推广是一个巨大的挑战。最近，“预训练，微调”训练范式被用来改善GNNs的泛化能力。它使用自监督信息来预训练GNN，然后通过微调来优化下游任务，仅使用少量标签。然而，预训练并不总是能够产生统计上显著的改进，尤其是对于具有随机结构掩码的自监督学习。实际上，分子结构具有频繁出现且影响分子属性的模式子图。为了利用与任务相关的模式子图，我们提出了一种新的“预训练，提示，微调”的训练范式。

    Molecular representation learning is crucial for the problem of molecular property prediction, where graph neural networks (GNNs) serve as an effective solution due to their structure modeling capabilities. Since labeled data is often scarce and expensive to obtain, it is a great challenge for GNNs to generalize in the extensive molecular space. Recently, the training paradigm of "pre-train, fine-tune" has been leveraged to improve the generalization capabilities of GNNs. It uses self-supervised information to pre-train the GNN, and then performs fine-tuning to optimize the downstream task with just a few labels. However, pre-training does not always yield statistically significant improvement, especially for self-supervised learning with random structural masking. In fact, the molecular structure is characterized by motif subgraphs, which are frequently occurring and influence molecular properties. To leverage the task-related motifs, we propose a novel paradigm of "pre-train, prompt,
    
[^160]: 相位、模态、时间和空间局部性: 加速图分析的领域特定机器学习预取器。

    Phases, Modalities, Temporal and Spatial Locality: Domain Specific ML Prefetcher for Accelerating Graph Analytics. (arXiv:2212.05250v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05250](http://arxiv.org/abs/2212.05250)

    MPGraph是一种针对图分析加速的领域特定机器学习预取器，引入了相位转变的软检测、相位特定多模态模型和链式时空预取等优化策略，通过CSTP实现了12.52-21.23%的IPC改进，并在性能上超越了其他预取器。

    

    内存性能是图分析加速中的瓶颈。现有的机器学习（ML）预取器在图处理中的相位转变和不规则内存访问上存在困难。我们提出了MPGraph，一种基于领域特定模型的图分析ML预取器。MPGraph引入了三种新的优化策略：用于相位转变的软检测，用于访问增量和页面预测的相位特定多模态模型，以及用于预取控制的链式时空预取（CSTP）。我们的转换检测器与Kolmogorov-Smirnov窗口和决策树相比，精度提高了34.17-82.15%。我们的预测器在增量方面的F1分数提高了6.80-16.02%，对于页面预测方面的准确率提高了11.68-15.41%，与LSTM和vanilla attention模型相比。使用CSTP，MPGraph的IPC提高了12.52-21.23%，优于最先进的非ML预取器BO 7.58-12.03%、以及基于ML的预取器Voyager和TransFetch 3.27-4.58%。

    Memory performance is a bottleneck in graph analytics acceleration. Existing Machine Learning (ML) prefetchers struggle with phase transitions and irregular memory accesses in graph processing. We propose MPGraph, an ML-based Prefetcher for Graph analytics using domain specific models. MPGraph introduces three novel optimizations: soft detection for phase transitions, phase-specific multi-modality models for access delta and page predictions, and chain spatio-temporal prefetching (CSTP) for prefetch control. Our transition detector achieves 34.17-82.15% higher precision compared with Kolmogorov-Smirnov Windowing and decision tree. Our predictors achieve 6.80-16.02% higher F1-score for delta and 11.68-15.41% higher accuracy-at-10 for page prediction compared with LSTM and vanilla attention models. Using CSTP, MPGraph achieves 12.52-21.23% IPC improvement, outperforming state-of-the-art non-ML prefetcher BO by 7.58-12.03% and ML-based prefetchers Voyager and TransFetch by 3.27-4.58%. For
    
[^161]: 通过参数分布鲁棒优化框架降低泛化复杂度

    Hedging Complexity in Generalization via a Parametric Distributionally Robust Optimization Framework. (arXiv:2212.01518v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2212.01518](http://arxiv.org/abs/2212.01518)

    通过使用参数分布鲁棒优化框架，我们提出了一种降低高维复杂问题泛化误差的简单方法，并且在各种设置下取得了显著改进的效果。

    

    经验风险最小化(ERM)和分布鲁棒优化(DRO)是解决运营管理和机器学习中出现的随机优化问题的流行方法。现有的这些方法的泛化误差界限要么依赖于成本函数的复杂度，要么依赖于随机扰动的维度。因此，在具有复杂目标函数的高维问题中，这些方法的性能可能较差。我们提出了一种简单的方法，该方法通过使用参数分布族来逼近随机扰动的分布。这减轻了两种复杂性来源；然而，它引入了模型未正确建模的误差。我们表明，这种新的误差来源可以通过合适的DRO公式来控制。我们提出的参数DRO方法在各种设置下对现有的ERM和DRO方法以及参数ERM的泛化界限有显著改进。我们的方法特别适用于。。。

    Empirical risk minimization (ERM) and distributionally robust optimization (DRO) are popular approaches for solving stochastic optimization problems that appear in operations management and machine learning. Existing generalization error bounds for these methods depend on either the complexity of the cost function or dimension of the random perturbations. Consequently, the performance of these methods can be poor for high-dimensional problems with complex objective functions. We propose a simple approach in which the distribution of random perturbations is approximated using a parametric family of distributions. This mitigates both sources of complexity; however, it introduces a model misspecification error. We show that this new source of error can be controlled by suitable DRO formulations. Our proposed parametric DRO approach has significantly improved generalization bounds over existing ERM and DRO methods and parametric ERM for a wide variety of settings. Our method is particularl
    
[^162]: PASTA：比例幅度谱训练增强用于 Syn-to-Real 领域泛化

    PASTA: Proportional Amplitude Spectrum Training Augmentation for Syn-to-Real Domain Generalization. (arXiv:2212.00979v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.00979](http://arxiv.org/abs/2212.00979)

    本文提出了一种基于比例幅度谱训练增强的方法 PASTA，可有效提高合成数据到真实数据的泛化性能，在多个 Syn-to-Real 任务上均具有优越性能。

    

    合成数据可以提供廉价且丰富的训练数据，适用于真实世界数据稀缺的情况。然而，在真实世界数据上评估的模型在合成数据上训练时表现显著不佳。在本文中，我们提出了 Proportional Amplitude Spectrum Training Augmentation (PASTA)，一种简单而有效的增强策略，可提高合成到真实（Syn-to-Real）泛化性能。 PASTA 在 Fourier 领域中扰动合成图像的幅度谱以生成增强视图。具体而言，使用 PASTA，我们提出了一种结构化扰动策略，其中高频分量相对于低频分量更容易受到扰动。对于语义分割（GTAV-to-Real），目标检测（Sim10K-to-Real）和对象识别（VisDA-C Syn-to-Real）任务，在总共5个 Syn-to-Real 转移中，我们发现 PASTA 的性能优于更复杂的最先进的泛化方法，同时具有互补性。

    Synthetic data offers the promise of cheap and bountiful training data for settings where labeled real-world data is scarce. However, models trained on synthetic data significantly underperform when evaluated on real-world data. In this paper, we propose Proportional Amplitude Spectrum Training Augmentation (PASTA), a simple and effective augmentation strategy to improve out-of-the-box synthetic-to-real (syn-to-real) generalization performance. PASTA perturbs the amplitude spectra of synthetic images in the Fourier domain to generate augmented views. Specifically, with PASTA we propose a structured perturbation strategy where high-frequency components are perturbed relatively more than the low-frequency ones. For the tasks of semantic segmentation (GTAV-to-Real), object detection (Sim10K-to-Real), and object recognition (VisDA-C Syn-to-Real), across a total of 5 syn-to-real shifts, we find that PASTA outperforms more complex state-of-the-art generalization methods while being complemen
    
[^163]: SnCQA：一种硬件高效的等变量子卷积电路架构

    SnCQA: A hardware-efficient equivariant quantum convolutional circuit architecture. (arXiv:2211.12711v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2211.12711](http://arxiv.org/abs/2211.12711)

    SnCQA是一种硬件高效的等变量子卷积电路架构，通过利用排列对称性和空间晶格对称性，适用于解决存在排列对称性的机器学习问题，具有更高的可扩展性、准确性和噪声韧性。

    

    我们提出了SnCQA，这是一组针对排列对称性和空间晶格对称性的硬件高效等变量子卷积电路的变分电路。通过利用系统的排列对称性，例如许多量子多体和量子化学问题中常见的晶格哈密顿量，我们的量子神经网络适用于解决存在排列对称性的机器学习问题，这可能会显著节省计算成本。除了理论的创新性外，在实际的量子计算化学中，我们发现我们的模拟在学习基态方面表现良好，可以通过少数参数实现与传统方法相当的性能。与其他传统的变分量子电路（如纯硬件高效的基态假设）相比，我们展示了SnCQA具有更高的可扩展性、准确性和噪声韧性（具有20倍更好的）。

    We propose SnCQA, a set of hardware-efficient variational circuits of equivariant quantum convolutional circuits respective to permutation symmetries and spatial lattice symmetries with the number of qubits $n$. By exploiting permutation symmetries of the system, such as lattice Hamiltonians common to many quantum many-body and quantum chemistry problems, Our quantum neural networks are suitable for solving machine learning problems where permutation symmetries are present, which could lead to significant savings of computational costs. Aside from its theoretical novelty, we find our simulations perform well in practical instances of learning ground states in quantum computational chemistry, where we could achieve comparable performances to traditional methods with few tens of parameters. Compared to other traditional variational quantum circuits, such as the pure hardware-efficient ansatz (pHEA), we show that SnCQA is more scalable, accurate, and noise resilient (with $20\times$ bette
    
[^164]: 使用DP-SGD的私有广告建模

    Private Ad Modeling with DP-SGD. (arXiv:2211.11896v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11896](http://arxiv.org/abs/2211.11896)

    本研究将差分隐私随机梯度下降（DP-SGD）应用于广告建模任务，证明了该方法可以在处理高类别不平衡和稀疏梯度更新的广告数据中提供隐私和效用。

    

    在隐私保护机器学习中，一种众所周知的算法是差分隐私随机梯度下降（DP-SGD）。尽管该算法在文本和图像数据上已经进行了评估，但在以往的研究中尚未将其应用于广告数据，而广告数据因其高类别不平衡和稀疏的梯度更新而臭名昭著。本研究我们将DP-SGD应用于多个广告建模任务，包括预测点击率、转化率和转化事件数量，并在真实数据集上评估其隐私和效用的权衡。我们的工作首次实证了DP-SGD可以为广告建模任务提供隐私和效用。

    A well-known algorithm in privacy-preserving ML is differentially private stochastic gradient descent (DP-SGD). While this algorithm has been evaluated on text and image data, it has not been previously applied to ads data, which are notorious for their high class imbalance and sparse gradient updates. In this work we apply DP-SGD to several ad modeling tasks including predicting click-through rates, conversion rates, and number of conversion events, and evaluate their privacy-utility trade-off on real-world datasets. Our work is the first to empirically demonstrate that DP-SGD can provide both privacy and utility for ad modeling tasks.
    
[^165]: REPAIR: 修复插值的归一化置换激活

    REPAIR: REnormalizing Permuted Activations for Interpolation Repair. (arXiv:2211.08403v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.08403](http://arxiv.org/abs/2211.08403)

    作者发现仅使用神经元对齐方法不能有效解决线性插值中激活方差坍缩的问题，因此提出了REPAIR方法来修复插值的归一化置换激活。实验证明，在各种架构中将REPAIR与神经元对齐方法结合使用可以大幅降低障碍。

    

    本文探讨了Entezari等人（2021）的猜想，即如果考虑神经网络的置换不变性，那么线性插值之间可能没有损失障碍。首先，我们观察到仅使用神经元对齐方法无法建立低障碍线性连接的原因是一种我们称之为方差坍缩的现象：插值深层网络的激活方差崩溃，导致性能较差。其次，我们提出了REPAIR（修复插值的归一化置换激活）方法，通过重新缩放这些插值网络的预激活来缓解方差崩溃。我们探讨了我们方法与归一化层、网络宽度和深度选择之间的相互作用，并演示了在各种架构族中使用REPAIR作为神经元对齐方法的扩展，可以将障碍降低60%至100%。

    In this paper we look into the conjecture of Entezari et al. (2021) which states that if the permutation invariance of neural networks is taken into account, then there is likely no loss barrier to the linear interpolation between SGD solutions. First, we observe that neuron alignment methods alone are insufficient to establish low-barrier linear connectivity between SGD solutions due to a phenomenon we call variance collapse: interpolated deep networks suffer a collapse in the variance of their activations, causing poor performance. Next, we propose REPAIR (REnormalizing Permuted Activations for Interpolation Repair) which mitigates variance collapse by rescaling the preactivations of such interpolated networks. We explore the interaction between our method and the choice of normalization layer, network width, and depth, and demonstrate that using REPAIR on top of neuron alignment methods leads to 60%-100% relative barrier reduction across a wide variety of architecture families and t
    
[^166]: 图像和视频的全景分割的通用框架

    A Generalist Framework for Panoptic Segmentation of Images and Videos. (arXiv:2210.06366v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.06366](http://arxiv.org/abs/2210.06366)

    这个论文提出了一个通用框架，用于图像和视频的全景分割。他们将全景分割问题定义为离散数据生成问题，并提出了一个简单的扩散模型来建模全景掩码。他们的方法能够在流式设置中建模视频，并自动学习跟踪对象实例，并在实验中展现出与最先进的专家方法竞争的能力。

    

    全景分割为图像的每个像素分配语义和实例ID标签。由于实例ID的排列也是有效的解决方案，该任务需要学习高维度的一对多映射。因此，最先进的方法使用定制的架构和任务特定的损失函数。我们将全景分割问题定义为离散数据生成问题，不依赖任务的归纳偏差。我们提出了一个扩散模型来建模全景掩码，具有简单的架构和通用的损失函数。通过将过去的预测作为条件信号添加，我们的方法能够在流式设置中建模视频，并自动学习跟踪对象实例。通过大量实验证明，我们的简单方法在类似的设置下能够与最先进的专家方法竞争。

    Panoptic segmentation assigns semantic and instance ID labels to every pixel of an image. As permutations of instance IDs are also valid solutions, the task requires learning of high-dimensional one-to-many mapping. As a result, state-of-the-art approaches use customized architectures and task-specific loss functions. We formulate panoptic segmentation as a discrete data generation problem, without relying on inductive bias of the task. A diffusion model is proposed to model panoptic masks, with a simple architecture and generic loss function. By simply adding past predictions as a conditioning signal, our method is capable of modeling video (in a streaming setting) and thereby learns to track object instances automatically. With extensive experiments, we demonstrate that our simple approach can perform competitively to state-of-the-art specialist methods in similar settings.
    
[^167]: 机器人的预训练：离线RL使其可以从少量试验中学习新任务

    Pre-Training for Robots: Offline RL Enables Learning New Tasks from a Handful of Trials. (arXiv:2210.05178v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.05178](http://arxiv.org/abs/2210.05178)

    本文提出了一种基于离线RL的机器人预训练框架(PTR)，可以将现有的机器人数据集上的预训练与少量任务特定数据相结合，从而有效地学习新任务，无需表示学习或vision-based pretraining。

    

    深度学习的进展彰显了利用多样的机器人数据集实现有效泛化的巨大潜力，这让我们有动力考虑利用广泛的数据集实现机器人学习的鲁棒泛化。但是，在实践中，我们经常希望在新的环境中学习新的技能，这可能不包含在先前的数据中。因此，我们想知道：如何将现有的多样离线数据集与少量任务特定数据结合起来，解决新任务，同时仍然享受大量数据训练的泛化优势？在本文中，我们展示了端到端离线RL可以是一种有效的方法来实现这一点，而不需要任何表示学习或基于视觉的预训练。我们提出了预训练机器人（PTR），这是一个基于离线RL的框架，试图通过将现有机器人数据集上的预训练与快速精细调整的过程相结合来有效地学习新任务。

    Progress in deep learning highlights the tremendous potential of utilizing diverse robotic datasets for attaining effective generalization and makes it enticing to consider leveraging broad datasets for attaining robust generalization in robotic learning as well. However, in practice, we often want to learn a new skill in a new environment that is unlikely to be contained in the prior data. Therefore we ask: how can we leverage existing diverse offline datasets in combination with small amounts of task-specific data to solve new tasks, while still enjoying the generalization benefits of training on large amounts of data? In this paper, we demonstrate that end-to-end offline RL can be an effective approach for doing this, without the need for any representation learning or vision-based pre-training. We present pre-training for robots (PTR), a framework based on offline RL that attempts to effectively learn new tasks by combining pre-training on existing robotic datasets with rapid fine-
    
[^168]: 动量追踪：异构数据上分布式深度学习中的动量加速

    Momentum Tracking: Momentum Acceleration for Decentralized Deep Learning on Heterogeneous Data. (arXiv:2209.15505v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.15505](http://arxiv.org/abs/2209.15505)

    本研究提出了一种名为动量追踪的方法，其收敛速度与数据异质性无关，在分布式深度学习中具有应对异质数据的能力。

    

    SGD动量是提高神经网络性能的关键组成部分之一。对于分布式学习，使用动量的一种直接方法是动量分布式SGD（DSGDm）。然而，当数据分布具有统计异质性时，DSGDm的性能不如DSGD。最近，一些研究解决了这个问题，并提出了具有动量的方法，相比DSGDm更能适应数据异质性，尽管它们的收敛速度仍然依赖于数据异质性，并且当数据分布异质时会恶化。在本研究中，我们提出了一种名为动量追踪的方法，其收敛速度被证明与数据异质性无关。具体而言，我们分析了动量追踪在目标函数为非凸函数且使用随机梯度的情况下的收敛速度。然后，我们发现它对于任何动量系数都与数据异质性无关。

    SGD with momentum is one of the key components for improving the performance of neural networks. For decentralized learning, a straightforward approach using momentum is Distributed SGD (DSGD) with momentum (DSGDm). However, DSGDm performs worse than DSGD when the data distributions are statistically heterogeneous. Recently, several studies have addressed this issue and proposed methods with momentum that are more robust to data heterogeneity than DSGDm, although their convergence rates remain dependent on data heterogeneity and deteriorate when the data distributions are heterogeneous. In this study, we propose Momentum Tracking, which is a method with momentum whose convergence rate is proven to be independent of data heterogeneity. More specifically, we analyze the convergence rate of Momentum Tracking in the setting where the objective function is non-convex and the stochastic gradient is used. Then, we identify that it is independent of data heterogeneity for any momentum coeffici
    
[^169]: 广义排列胞内枚举最大池化响应

    Enumeration of max-pooling responses with generalized permutohedra. (arXiv:2209.14978v2 [math.CO] UPDATED)

    [http://arxiv.org/abs/2209.14978](http://arxiv.org/abs/2209.14978)

    广义排列胞内枚举了最大池化响应的组合学，通过计算Minkowski和的顶点数量，得到了最大池化层的线性区域数量，并得到了关于窗口大小和步幅的一维最大池化层顶点和面的生成函数和封闭公式，以及一个特殊情况下二维最大池化的顶点数量。

    

    我们研究了最大池化层的组合学，最大池化层是一种通过在输入坐标的移动窗口上取最大值来降采样输入数组的函数，它们在卷积神经网络中常被使用。我们通过等价地计算某些单形的Minkowski和的顶点数量来得到这些函数的线性区域的结果。我们表征了这些多胞体的面，并获得了依赖于池化窗口大小和步幅的一维最大池化层的顶点和面的生成函数和封闭公式，以及一个特殊情况下二维最大池化的顶点数量。

    We investigate the combinatorics of max-pooling layers, which are functions that downsample input arrays by taking the maximum over shifted windows of input coordinates, and which are commonly used in convolutional neural networks. We obtain results on the number of linearity regions of these functions by equivalently counting the number of vertices of certain Minkowski sums of simplices. We characterize the faces of such polytopes and obtain generating functions and closed formulas for the number of vertices and facets in a 1D max-pooling layer depending on the size of the pooling windows and stride, and for the number of vertices in a special case of 2D max-pooling.
    
[^170]: 密集氢的深度变分自由能方法

    Deep Variational Free Energy Approach to Dense Hydrogen. (arXiv:2209.06095v2 [cond-mat.str-el] UPDATED)

    [http://arxiv.org/abs/2209.06095](http://arxiv.org/abs/2209.06095)

    该论文提出了一种基于深度生成模型的变分自由能方法，用于研究密集氢的状态方程。通过优化两个神经网络，研究者达到了与先前的耦合电子-离子蒙特卡洛计算相当的变分自由能。在行星条件下，预测的密集氢状态方程比其他计算方法更致密。这种方法还可以直接获得密集氢的熵和自由能，为行星模拟和高压物理研究提供了新的机会。

    

    我们开发了一种基于深度生成模型的变分自由能方法，用于密集氢的状态方程。我们使用一个归一化流网络来建模质子的玻尔兹曼分布，并使用一个费米子神经网络来建模给定质子位置的电子波函数。通过联合优化这两个神经网络，我们达到了与之前的耦合电子-离子蒙特卡洛计算相当的变分自由能。在行星条件下，密集氢的预测状态方程比从头分子动力学计算和经验化学模型的发现更加致密。而且，直接访问密集氢的熵和自由能在行星模拟和高压物理研究中开辟了新的机会。

    We developed a deep generative model-based variational free energy approach to the equations of state of dense hydrogen. We employ a normalizing flow network to model the proton Boltzmann distribution and a fermionic neural network to model the electron wave function at given proton positions. By jointly optimizing the two neural networks we reached a comparable variational free energy to the previous coupled electron-ion Monte Carlo calculation. The predicted equation of state of dense hydrogen under planetary conditions is denser than the findings of ab initio molecular dynamics calculation and empirical chemical model. Moreover, direct access to the entropy and free energy of dense hydrogen opens new opportunities in planetary modeling and high-pressure physics research.
    
[^171]: 欺诈数据集基准和应用

    Fraud Dataset Benchmark and Applications. (arXiv:2208.14417v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.14417](http://arxiv.org/abs/2208.14417)

    欺诈数据集基准（FDB）是一个针对欺诈检测的公开可用数据集的汇编，涵盖了各种欺诈相关任务，为解决欺诈检测中的独特挑战提供了标准化的数据集和基准。 (arXiv:2208.14417v3 [cs.LG] UPDATED)

    

    标准化的数据集和基准已经在计算机视觉、自然语言处理、多模态和表格设置方面推动了创新。我们发现，与其他研究领域相比，欺诈检测面临着独特的挑战：高级别的不平衡、多样化的特征类型、频繁变化的欺诈模式和问题的对抗性。由于这些挑战，对其他研究领域的数据集评估的建模方法可能对欺诈检测效果不佳。在本文中，我们介绍了欺诈数据集基准（FDB），它是一个针对欺诈检测的公开可用数据集的汇编，涵盖了各种欺诈相关任务，包括识别无卡交易、检测机器人攻击、分类恶意URL、估计贷款违约风险和内容审核等。基于Python的FDB库提供了一致的API用于数据加载和标准化的训练和测试集划分。我们演示了几种应用场景。

    Standardized datasets and benchmarks have spurred innovations in computer vision, natural language processing, multi-modal and tabular settings. We note that, as compared to other well researched fields, fraud detection has unique challenges: high-class imbalance, diverse feature types, frequently changing fraud patterns, and adversarial nature of the problem. Due to these, the modeling approaches evaluated on datasets from other research fields may not work well for the fraud detection. In this paper, we introduce Fraud Dataset Benchmark (FDB), a compilation of publicly available datasets catered to fraud detection FDB comprises variety of fraud related tasks, ranging from identifying fraudulent card-not-present transactions, detecting bot attacks, classifying malicious URLs, estimating risk of loan default to content moderation. The Python based library for FDB provides a consistent API for data loading with standardized training and testing splits. We demonstrate several application
    
[^172]: DenseShift: 实现准确和高效的低位幂乘法的量化

    DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization. (arXiv:2208.09708v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.09708](http://arxiv.org/abs/2208.09708)

    DenseShift网络是一种准确和高效的低位幂乘法量化方法，通过改进Shift网络的精度和引入非量化浮点激活来提高性能。

    

    在低资源边缘设备上高效部署深度神经网络是具有挑战性的，因为其不断增加的资源需求。为了解决这个问题，研究人员提出了无乘法神经网络，如幂乘法的量化，也被称为Shift网络，旨在减少内存使用和简化计算。然而，现有的低位Shift网络不如全精度网络准确，通常受到有限权重范围编码方案和量化损失的影响。在本文中，我们提出了DenseShift网络，显著提高了Shift网络的准确性，为视觉和语音应用实现了与全精度网络相媲美的性能。此外，我们引入了一种使用非量化浮点激活的高效DenseShift网络部署方法，同时获得了现有方法的1.6倍加速。为了实现这一点，我们证明了低位Shift网络中零权重值的作用。

    Efficiently deploying deep neural networks on low-resource edge devices is challenging due to their ever-increasing resource requirements. To address this issue, researchers have proposed multiplication-free neural networks, such as Power-of-Two quantization, or also known as Shift networks, which aim to reduce memory usage and simplify computation. However, existing low-bit Shift networks are not as accurate as their full-precision counterparts, typically suffering from limited weight range encoding schemes and quantization loss. In this paper, we propose the DenseShift network, which significantly improves the accuracy of Shift networks, achieving competitive performance to full-precision networks for vision and speech applications. In addition, we introduce a method to deploy an efficient DenseShift network using non-quantized floating-point activations, while obtaining 1.6X speed-up over existing methods. To achieve this, we demonstrate that zero-weight values in low-bit Shift netw
    
[^173]: 减少蕴含偏差逻辑损失用于神经符号学习

    Reduced Implication-bias Logic Loss for Neuro-Symbolic Learning. (arXiv:2208.06838v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2208.06838](http://arxiv.org/abs/2208.06838)

    本文提出了一种减少蕴含偏差逻辑损失（RILL）的方法，用于解决在神经符号学习中由于从模糊逻辑算子中派生的损失函数带来的偏差问题。实证研究表明，RILL相比有偏差的逻辑损失函数在知识库不完整和标记数据不足时具有显著的改进和更强的稳健性。

    

    通过使用可微分算子来近似逻辑推理，将逻辑推理与机器学习相结合是神经符号系统中广泛使用的技术。然而，某些可微分算子在反向传播过程中可能带来显著的偏差，并降低神经符号学习的性能。本文揭示了这种偏差，称之为“蕴含偏差”，常见于从模糊逻辑算子中派生的损失函数。此外，我们提出了一种简单而有效的方法，将有偏差的损失函数转化为“减少蕴含偏差逻辑损失（RILL）”，以解决上述问题。实证研究表明，与有偏差的逻辑损失函数相比，RILL在知识库不完整时可以取得显著改进，并在标记数据不足时保持更为稳健。

    Integrating logical reasoning and machine learning by approximating logical inference with differentiable operators is a widely used technique in Neuro-Symbolic systems.  However, some differentiable operators could bring a significant bias during backpropagation and degrade the performance of Neuro-Symbolic learning.  In this paper, we reveal that this bias, named \textit{Implication Bias} is common in loss functions derived from fuzzy logic operators.  Furthermore, we propose a simple yet effective method to transform the biased loss functions into \textit{Reduced Implication-bias Logic Loss (RILL)} to address the above problem.  Empirical study shows that RILL can achieve significant improvements compared with the biased logic loss functions, especially when the knowledge base is incomplete, and keeps more robust than the compared methods when labelled data is insufficient.
    
[^174]: EgPDE-Net：建立连续神经网络进行包含外生变量的时间序列预测

    EgPDE-Net: Building Continuous Neural Networks for Time Series Prediction with Exogenous Variables. (arXiv:2208.01913v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.01913](http://arxiv.org/abs/2208.01913)

    本文提出了一个连续神经网络模型EgPDE-Net，用于包含外生变量的时间序列预测，通过考虑外生变量之间的关系和其对目标序列的影响，学习多变量时间序列中的未知偏微分方程系统。

    

    虽然外生变量对于时间序列分析的性能改进有很大的影响，但现有的连续方法很少考虑它们之间的互相关性和时间依赖性。多变量时间序列的动力系统可以用复杂的未知偏微分方程（PDE）来建模，在科学和工程的许多学科中起着重要的作用。在本文中，我们提出了一个连续时间模型，用于任意步长预测，以学习多变量时间序列中未知PDE系统，其管理方程由自注意力和门控循环神经网络参数化。所提出的模型，EgPDE-Net，考虑了外生变量之间的关系以及它们对目标序列的影响。重要的是，该模型可以简化为一个正则化的常微分方程（ODE）。

    While exogenous variables have a major impact on performance improvement in time series analysis, inter-series correlation and time dependence among them are rarely considered in the present continuous methods. The dynamical systems of multivariate time series could be modelled with complex unknown partial differential equations (PDEs) which play a prominent role in many disciplines of science and engineering. In this paper, we propose a continuous-time model for arbitrary-step prediction to learn an unknown PDE system in multivariate time series whose governing equations are parameterised by self-attention and gated recurrent neural networks. The proposed model, \underline{E}xogenous-\underline{g}uided \underline{P}artial \underline{D}ifferential \underline{E}quation Network (EgPDE-Net), takes account of the relationships among the exogenous variables and their effects on the target series. Importantly, the model can be reduced into a regularised ordinary differential equation (ODE) p
    
[^175]: 图节点注入攻击的对抗伪装

    Adversarial Camouflage for Node Injection Attack on Graphs. (arXiv:2208.01819v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.01819](http://arxiv.org/abs/2208.01819)

    本文提出了一种名为CANA的对抗伪装框架，可以在节点注入攻击中使被注入节点看起来正常，并提高在实际场景下防御/检测方法的攻击性能。

    

    图神经网络所受到的节点注入攻击（Node injection attacks）近年来引起了人们的重视，因为这种攻击具有很高的攻击成功率，并能显著降低图神经网络的性能。但是，我们的研究表明，这些攻击在实际场景中经常失败，因为防御/检测方法可以轻松识别和删除被注入的节点。为了解决这个问题，我们致力于进行节点注入攻击的伪装，使被注入的节点看起来正常，并且对于防御/检测方法是不可感知的。然而，图数据的非欧几里德性质和缺乏直观先验知识为伪装的形式化、实现和评估带来了巨大的挑战。在本文中，我们首先提出并定义了伪装作为注入节点和正常节点的邻域网络之间的分布相似性。然后，针对实现，我们提出了一种针对节点注入攻击的对抗伪装框架，即CANA，以提高在实际场景下防御/检测方法的攻击性能。

    Node injection attacks on Graph Neural Networks (GNNs) have received emerging attention due to their potential to significantly degrade GNN performance with high attack success rates. However, our study indicates these attacks often fail in practical scenarios, since defense/detection methods can easily identify and remove the injected nodes. To address this, we devote to camouflage node injection attack, making injected nodes appear normal and imperceptible to defense/detection methods. Unfortunately, the non-Euclidean nature of graph data and lack of intuitive prior present great challenges to the formalization, implementation, and evaluation of camouflage. In this paper, we first propose and define camouflage as distribution similarity between ego networks of injected nodes and normal nodes. Then for implementation, we propose an adversarial CAmouflage framework for Node injection Attack, namely CANA, to improve attack performance under defense/detection methods in practical scenari
    
[^176]: 大规模超参数优化的异步分散贝叶斯优化方法

    Asynchronous Decentralized Bayesian Optimization for Large Scale Hyperparameter Optimization. (arXiv:2207.00479v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00479](http://arxiv.org/abs/2207.00479)

    这项研究提出了一种异步分散的贝叶斯优化方法，可以实现大规模超参数优化，并在Polairs超级计算机上展示了模型准确性的改进。

    

    贝叶斯优化是深度神经网络超参数优化的一种有希望的方法，其中每个模型训练可能需要几分钟到几小时的时间。在贝叶斯优化中，采用计算便宜的替代模型来学习参数配置与性能（如准确性）之间的关系。并行贝叶斯优化方法通常采用单个管理器-多个工作者策略，同时评估多个超参数配置。尽管超参数评估时间相当长，但这种集中式方案的开销阻碍了这些方法在大量工作者上的扩展。我们提出了一种异步分散的贝叶斯优化方法，其中每个工作者运行顺序贝叶斯优化，并通过共享存储异步通信其结果。我们将我们的方法扩展到1,920个并行工作者（Polaris超级计算机的完整生产队列），并展示模型准确性的改进。

    Bayesian optimization (BO) is a promising approach for hyperparameter optimization of deep neural networks (DNNs), where each model training can take minutes to hours. In BO, a computationally cheap surrogate model is employed to learn the relationship between parameter configurations and their performance such as accuracy. Parallel BO methods often adopt single manager/multiple workers strategies to evaluate multiple hyperparameter configurations simultaneously. Despite significant hyperparameter evaluation time, the overhead in such centralized schemes prevents these methods to scale on a large number of workers. We present an asynchronous-decentralized BO, wherein each worker runs a sequential BO and asynchronously communicates its results through shared storage. We scale our method without loss of computational efficiency with above 95% of worker's utilization to 1,920 parallel workers (full production queue of the Polaris supercomputer) and demonstrate improvement in model accurac
    
[^177]: 可解释且高性能的仇恨和冒犯性言论检测

    Explainable and High-Performance Hate and Offensive Speech Detection. (arXiv:2206.12983v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2206.12983](http://arxiv.org/abs/2206.12983)

    这项研究构建了一个可解释且高性能的模型，基于XGBoost算法，用于检测社交媒体平台上的仇恨和冒犯性言论。该模型在不平衡的Twitter数据上显示出更好的性能，并且在降采样后的数据中也表现出优越性能。

    

    社交媒体平台上信息的传播可能会在脆弱社群中创造敌对的环境，并使某些群体沉默。为了缓解这种情况，已开发了多个模型来检测仇恨和冒犯性言论。由于在社交媒体平台上检测仇恨和冒犯性言论可能错误地将个体排除在社交媒体平台之外，从而降低了信任度，因此有必要创建可解释且可解释的模型。因此，我们基于XGBoost算法构建了一个可解释且可解释的高性能模型，该模型使用Twitter数据进行训练。对于不平衡的Twitter数据，相比于LSTM、AutoGluon和ULMFiT模型的F1得分分别为0.38和0.37，以及0.38，XGBoost在仇恨言论检测方面的F1得分为0.75，表现更好。当我们将数据降采样为约5000条推文的三个独立类别时，XGBoost在仇恨言论检测方面的F1得分也优于LSTM、AutoGluon和ULMFiT，为0.79。

    The spread of information through social media platforms can create environments possibly hostile to vulnerable communities and silence certain groups in society. To mitigate such instances, several models have been developed to detect hate and offensive speech. Since detecting hate and offensive speech in social media platforms could incorrectly exclude individuals from social media platforms, which can reduce trust, there is a need to create explainable and interpretable models. Thus, we build an explainable and interpretable high performance model based on the XGBoost algorithm, trained on Twitter data. For unbalanced Twitter data, XGboost outperformed the LSTM, AutoGluon, and ULMFiT models on hate speech detection with an F1 score of 0.75 compared to 0.38 and 0.37, and 0.38 respectively. When we down-sampled the data to three separate classes of approximately 5000 tweets, XGBoost performed better than LSTM, AutoGluon, and ULMFiT; with F1 scores for hate speech detection of 0.79 vs 
    
[^178]: 可扩展的分布式算法在MapReduce和适应性复杂度模型中用于大小约束子模型最大化问题的研究

    Scalable Distributed Algorithms for Size-Constrained Submodular Maximization in the MapReduce and Adaptive Complexity Models. (arXiv:2206.09563v4 [cs.DS] UPDATED)

    [http://arxiv.org/abs/2206.09563](http://arxiv.org/abs/2206.09563)

    本论文研究了在MapReduce和适应性复杂度模型中用于大小约束子模型最大化问题的可扩展分布式算法。通过使用几种次线性自适应算法，我们证明了这些算法满足在MR环境中所需的一致性特性，并且提出了世界首个具有恒定MR轮次的线性时间分布式算法。

    

    在MapReduce模型中对子模型函数进行分布式最大化的研究已经引起了广泛的关注，这导致了两种框架可以在MR环境中运行集中式算法而不损失逼近性能，只要集中式算法满足一定的一致性特性 - 这个特性仅由标准的贪婪算法和连续贪婪算法满足。另一方面，还有一系列的研究工作着眼于自适应复杂度模型中的子模型最大化的并行计算能力，其中每个线程可以访问整个底集。对于满足大小约束的单调子模型函数最大化问题，我们展示了几种次线性自适应算法满足在MR环境中所需的一致性特性，从而得到了高度实用的可并行计算和分布式算法。此外，我们还开发了第一个具有恒定MR轮次的线性时间分布式算法。最后，我们提供了一种增加m的方法。

    Distributed maximization of a submodular function in the MapReduce model has received much attention, culminating in two frameworks that allow a centralized algorithm to be run in the MR setting without loss of approximation, as long as the centralized algorithm satisfies a certain consistency property - which had only been shown to be satisfied by the standard greedy and continous greedy algorithms. A separate line of work has studied parallelizability of submodular maximization in the adaptive complexity model, where each thread may have access to the entire ground set. For the size-constrained maximization of a monotone and submodular function, we show that several sublinearly adaptive algorithms satisfy the consistency property required to work in the MR setting, which yields highly practical parallelizable and distributed algorithms. Also, we develop the first linear-time distributed algorithm for this problem with constant MR rounds. Finally, we provide a method to increase the m
    
[^179]: 通过非精确ADMM进行联邦学习

    Federated Learning via Inexact ADMM. (arXiv:2204.10607v4 [math.OC] UPDATED)

    [http://arxiv.org/abs/2204.10607](http://arxiv.org/abs/2204.10607)

    本文提出了一种非精确ADMM算法，用于解决联邦学习中的高效优化问题。该算法既具有计算和通信效率，能够应对滞后效应，又在较弱条件下收敛，并且在数值性能方面表现出色。

    

    联邦学习中的一个关键问题是如何开发高效的优化算法。目前大部分算法要求设备全员参与或者对收敛性做出强大假设。与常用的基于梯度下降的算法不同，本文提出了一种非精确交替方向乘子法(ADMM)，该方法在计算和通信效率上均有优势，能够应对滞后效应，并在较弱的条件下收敛。此外，与几种最先进的联邦学习算法相比，该方法具有更高的数值性能。

    One of the crucial issues in federated learning is how to develop efficient optimization algorithms. Most of the current ones require full device participation and/or impose strong assumptions for convergence. Different from the widely-used gradient descent-based algorithms, in this paper, we develop an inexact alternating direction method of multipliers (ADMM), which is both computation- and communication-efficient, capable of combating the stragglers' effect, and convergent under mild conditions. Furthermore, it has a high numerical performance compared with several state-of-the-art algorithms for federated learning.
    
[^180]: 带有层次化个性化模型的稀疏联邦学习

    Sparse Federated Learning with Hierarchical Personalized Models. (arXiv:2203.13517v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.13517](http://arxiv.org/abs/2203.13517)

    本研究提出了一种称为sFedHP的带有层次化个性化模型的稀疏联邦学习算法，通过使用基于Moreau包络的分层近似映射和连续可微的近似L1范数作为稀疏约束，显著提高了面对多样数据的全局模型性能。

    

    联邦学习（FL）可以在不收集用户私人数据的情况下实现隐私安全和可靠的协作训练。其出色的隐私安全潜力促进了FL在物联网（IoT）、无线网络、移动设备、自动驾驶汽车和云医疗治疗等领域的广泛应用。然而，FL方法在非i.i.d.数据上的模型性能较差且通信流量过大。为此，我们提出了一种使用基于Moreau包络的分层近似映射的个性化FL算法，命名为带有层次化个性化模型的稀疏联邦学习（sFedHP），它显著改善了面对多样数据的全局模型性能。连续可微的近似L1范数也被用作稀疏约束以减少通信成本。收敛性分析表明，sFedHP具有最先进的线性加速性能，而稀疏约束只会将收敛速度降低到一个较小的值。

    Federated learning (FL) can achieve privacy-safe and reliable collaborative training without collecting users' private data. Its excellent privacy security potential promotes a wide range of FL applications in Internet-of-Things (IoT), wireless networks, mobile devices, autonomous vehicles, and cloud medical treatment. However, the FL method suffers from poor model performance on non-i.i.d. data and excessive traffic volume. To this end, we propose a personalized FL algorithm using a hierarchical proximal mapping based on the moreau envelop, named sparse federated learning with hierarchical personalized models (sFedHP), which significantly improves the global model performance facing diverse data. A continuously differentiable approximated L1-norm is also used as the sparse constraint to reduce the communication cost. Convergence analysis shows that sFedHP's convergence rate is state-of-the-art with linear speedup and the sparse constraint only reduces the convergence rate to a small e
    
[^181]: 图强化学习用于无线资源分配

    Graph Reinforcement Learning for Radio Resource Allocation. (arXiv:2203.03906v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.03906](http://arxiv.org/abs/2203.03906)

    该论文介绍了一种利用图强化学习方法进行无线资源分配的方法，通过利用拓扑信息和排列特性，降低了深度强化学习的训练复杂性，并通过优化预测功率分配问题来验证方法的有效性。

    

    由于其处理无模型和端到端问题的能力，深度强化学习(DRL)在资源分配方面得到了广泛的研究。然而，DRL的高训练复杂性限制了它在动态无线系统中的实际应用。为了降低训练成本，我们采用图强化学习来利用无线通信中许多问题固有的两种关系先验：拓扑信息和排列特性。为了系统地设计图强化学习框架来利用这两个先验，我们首先构思了一种将状态矩阵转换为状态图的方法，然后提出了一种通用的图神经网络方法来满足理想的排列特性。为了展示如何应用所提出的方法，我们以深度确定性策略梯度(DDPG)为例，优化了两个代表性的资源分配问题。一个是预测功率分配，旨在最小化能耗。

    Deep reinforcement learning (DRL) for resource allocation has been investigated extensively owing to its ability of handling model-free and end-to-end problems. Yet the high training complexity of DRL hinders its practical use in dynamic wireless systems. To reduce the training cost, we resort to graph reinforcement learning for exploiting two kinds of relational priors inherent in many problems in wireless communications: topology information and permutation properties. To design graph reinforcement learning framework systematically for harnessing the two priors, we first conceive a method to transform state matrix into state graph, and then propose a general method for graph neural networks to satisfy desirable permutation properties. To demonstrate how to apply the proposed methods, we take deep deterministic policy gradient (DDPG) as an example for optimizing two representative resource allocation problems. One is predictive power allocation that minimizes the energy consumed for e
    
[^182]: 通过主题建模和相对密度估计的犯罪热点建模

    Crime Hot-Spot Modeling via Topic Modeling and Relative Density Estimation. (arXiv:2202.04176v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2202.04176](http://arxiv.org/abs/2202.04176)

    本研究提出了一种通过主题建模和相对密度估计来进行犯罪热点建模的方法。实验证明该方法可以捕捉到被调度员忽视的地理热点趋势，这些热点趋势往往与整体事件密度的增加相混淆。

    

    我们提出了一种方法，通过对犯罪记录叙述的集合进行主题分布的计算，确定相似呼叫的分组以及它们的相对空间分布。我们首先为每个叙述获取一个主题分布，然后提出了一种最近邻相对密度估计（kNN-RDE）方法来获得每个主题的空间相对密度。在亚特兰大警察局的大量叙述文档（$n=475,019$）上进行的实验表明，我们的方法能够捕捉到通常被呼叫调度员一开始没有察觉到的地理热点趋势，这些趋势由于与一般事件密度的混淆而被忽视。

    We present a method to capture groupings of similar calls and determine their relative spatial distribution from a collection of crime record narratives. We first obtain a topic distribution for each narrative, and then propose a nearest neighbors relative density estimation (kNN-RDE) approach to obtain spatial relative densities per topic. Experiments over a large corpus ($n=475,019$) of narrative documents from the Atlanta Police Department demonstrate the viability of our method in capturing geographic hot-spot trends which call dispatchers do not initially pick up on and which go unnoticed due to conflation with elevated event density in general.
    
[^183]: 高效直连拓扑结构用于集体通信

    Efficient Direct-Connect Topologies for Collective Communications. (arXiv:2202.03356v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2202.03356](http://arxiv.org/abs/2202.03356)

    本论文提出了一种算法框架，可以构建高效的直连网络拓扑结构，以优化节点延迟和带宽权衡，适用于集体通信负载。

    

    本文研究了如何构建适用于集体通信的高效网络拓扑结构。我们提出了一种算法框架，用于构建针对节点延迟与带宽权衡优化的直连拓扑结构。这个算法框架从小的基础拓扑结构和相关的通信进度开始，并使用一组可以迭代应用的技术来派生更大的拓扑结构。这些衍生的拓扑结构的时间表可以与扩展一起合成，也可以使用优化公式计算。我们的方法允许我们为给定的集群大小和度数合成许多不同的拓扑结构和时间表，然后为给定的工作负载确定适当的拓扑和时间表。我们在使用补丁面板配置所需拓扑结构的12节点光学实验平台上评估了我们的方法，并增加了基于分析模型的评估，用于更大的部署。

    We consider the problem of distilling efficient network topologies for collective communications. We provide an algorithmic framework for constructing direct-connect topologies optimized for the node latency vs bandwidth trade-off given a collective communication workload. Our algorithmic framework allows us to start from small base topologies and associated communication schedules and use a set of techniques that can be iteratively applied to derive much larger topologies. The schedules for these derived topologies are either synthesized along with the expansions or computed using an optimization formulation. Our approach allows us to synthesize many different topologies and schedules for a given cluster size and degree, and then identify the appropriate topology and schedule for a given workload. We evaluate our approach on a 12-node optical testbed that uses patch panels for configuring the desired topology and augment it with an analytical-model-based evaluation for larger deployme
    
[^184]: 利用队列长度和注意力机制增强交通信号控制优化

    Leveraging Queue Length and Attention Mechanisms for Enhanced Traffic Signal Control Optimization. (arXiv:2201.00006v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.00006](http://arxiv.org/abs/2201.00006)

    这篇论文提出了一种利用队列长度和注意力机制的交通信号控制优化方法。作者提出了Max Queue-Length (M-QL)和AttentionLight两种新方法，实验结果表明M-QL方法优于现有的强化学习方法，并且AttentionLight方法适用于各种交通场景并具有更好的性能表现。

    

    近年来，强化学习技术在交通信号控制中获得了越来越多的关注。然而，大多数现有的基于强化学习的交通信号控制方法往往主要关注强化学习模型结构，而忽视了适当的交通状态表示的重要性。此外，一些基于强化学习的方法在很大程度上依赖于专家设计的交通信号相位竞争。在本文中，我们提出了一种利用队列长度作为高效状态表示的TSC新方法。我们提出了两种新方法：(1) 基于队列长度属性设计的优化传统方法Max Queue-Length (M-QL)；(2) AttentionLight，一种利用自注意力机制捕捉信号相位相关性的强化学习模型，而无需人工知识的相位关系。对多个实际数据集进行的综合实验表明了我们方法的有效性：(1) M-QL方法优于最新的基于强化学习的方法；(2) 适用于各种交通场景，且相对于专家设计的方法具有更好的性能表现。

    Reinforcement learning (RL) techniques for traffic signal control (TSC) have gained increasing popularity in recent years. However, most existing RL-based TSC methods tend to focus primarily on the RL model structure while neglecting the significance of proper traffic state representation. Furthermore, some RL-based methods heavily rely on expert-designed traffic signal phase competition. In this paper, we present a novel approach to TSC that utilizes queue length as an efficient state representation. We propose two new methods: (1) Max Queue-Length (M-QL), an optimization-based traditional method designed based on the property of queue length; and (2) AttentionLight, an RL model that employs the self-attention mechanism to capture the signal phase correlation without requiring human knowledge of phase relationships. Comprehensive experiments on multiple real-world datasets demonstrate the effectiveness of our approach: (1) the M-QL method outperforms the latest RL-based methods; (2) A
    
[^185]: 压缩联邦学习中更快的速率与客户端方差减少

    Faster Rates for Compressed Federated Learning with Client-Variance Reduction. (arXiv:2112.13097v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.13097](http://arxiv.org/abs/2112.13097)

    本文提出了压缩和客户端方差减少方法COFIG和FRECON，以解决分布式和联邦学习应用中的通信瓶颈和客户端方差问题。在非凸设置下，COFIG具有$O(\frac{{(1+\omega)^{3/2}\sqrt{N}}}{{S\epsilon^2}}+\frac{{(1+\omega)N^{2/3}}}{{S\epsilon^2}})$的通信轮数上限，在凸设置下，COFIG收敛。

    

    由于分布式和联邦学习应用中的通信瓶颈，使用通信压缩的算法引起了广泛关注并被广泛应用。此外，庞大数量、高异质性和有限可用性的客户端导致了高客户端方差。本文通过提出压缩和客户端方差减少方法COFIG和FRECON来解决这两个问题。在非凸设置下，我们证明了COFIG的通信轮数上限为$O(\frac{{(1+\omega)^{3/2}\sqrt{N}}}{{S\epsilon^2}}+\frac{{(1+\omega)N^{2/3}}}{{S\epsilon^2}})$，其中$N$是总客户端数，$S$是每轮参与的客户端数，$\epsilon$是收敛误差，$\omega$是与压缩运算符相关的方差参数。对于FRECON，我们证明了通信轮数的上限为$O(\frac{{(1+\omega)\sqrt{N}}}{{S\epsilon^2}})$。在凸设置下，COFIG收敛。

    Due to the communication bottleneck in distributed and federated learning applications, algorithms using communication compression have attracted significant attention and are widely used in practice. Moreover, the huge number, high heterogeneity and limited availability of clients result in high client-variance. This paper addresses these two issues together by proposing compressed and client-variance reduced methods COFIG and FRECON. We prove an $O(\frac{(1+\omega)^{3/2}\sqrt{N}}{S\epsilon^2}+\frac{(1+\omega)N^{2/3}}{S\epsilon^2})$ bound on the number of communication rounds of COFIG in the nonconvex setting, where $N$ is the total number of clients, $S$ is the number of clients participating in each round, $\epsilon$ is the convergence error, and $\omega$ is the variance parameter associated with the compression operator. In case of FRECON, we prove an $O(\frac{(1+\omega)\sqrt{N}}{S\epsilon^2})$ bound on the number of communication rounds. In the convex setting, COFIG converges with
    
[^186]: 关于机器辅助人类决策的公正性研究

    On the Fairness of Machine-Assisted Human Decisions. (arXiv:2110.15310v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2110.15310](http://arxiv.org/abs/2110.15310)

    本研究通过形式模型和实验室实验考察了机器预测的性质如何影响人类最终决策。实验发现，包含有偏见的人类决策者可能逆转算法结构与决策质量之间的关系，并且排除受保护群体信息可能无法减少差异甚至可能增加差异。

    

    当机器学习算法在高风险决策中被使用时，我们希望确保它们的部署能够产生公平和公正的结果。这个关注点促使了一个迅速增长的文献，专注于诊断和解决机器预测中的差异性。然而，许多机器预测被部署来协助人类决策者保留最终决策权。因此，在本文中，我们在一个形式模型和实验室实验中考虑了机器预测的性质如何影响最终的人类决策。在我们的统计决策形式模型中，我们展示了包含一个有偏见的人类决策者可能逆转算法结构与最终决策质量之间的常规关系。具体地，我们记录了从预测中排除受保护群体信息可能无法减少甚至可能增加最终差异的情况。在实验室实验中，我们展示了如何...

    When machine-learning algorithms are used in high-stakes decisions, we want to ensure that their deployment leads to fair and equitable outcomes. This concern has motivated a fast-growing literature that focuses on diagnosing and addressing disparities in machine predictions. However, many machine predictions are deployed to assist in decisions where a human decision-maker retains the ultimate decision authority. In this article, we therefore consider in a formal model and in a lab experiment how properties of machine predictions affect the resulting human decisions. In our formal model of statistical decision-making, we show that the inclusion of a biased human decision-maker can revert common relationships between the structure of the algorithm and the qualities of resulting decisions. Specifically, we document that excluding information about protected groups from the prediction may fail to reduce, and may even increase, ultimate disparities. In the lab experiment, we demonstrate ho
    
[^187]: 使用Wasserstein距离进行切空间和维度估计

    Tangent Space and Dimension Estimation with the Wasserstein Distance. (arXiv:2110.06357v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/2110.06357](http://arxiv.org/abs/2110.06357)

    本文使用局部主成分分析算法，通过数学严格的边界估计了维度和切空间的采样点要求，并同步考虑了噪声非均匀数据分布和变化噪声。所有边界中的常数都有明确描述。

    

    考虑在欧几里得空间的平滑紧致子流形附近独立抽样的点集。我们提供了数学严格的边界，用于以高置信度估计流形的维度和切空间所需的采样点数。该估计的算法是局部主成分分析(Local PCA)，它是主成分分析的局部版本。我们的结果考虑了具有噪声非均匀数据分布和可能在流形上变化的噪声，允许在多个点同时进行估计。重要的是，我们边界中出现的所有常数都有明确的描述。证明使用矩阵浓度不等式估计协方差矩阵，并使用Wasserstein距离边界来量化底层流形的非线性和概率测度的非均匀性。

    Consider a set of points sampled independently near a smooth compact submanifold of Euclidean space. We provide mathematically rigorous bounds on the number of sample points required to estimate both the dimension and the tangent spaces of that manifold with high confidence. The algorithm for this estimation is Local PCA, a local version of principal component analysis. Our results accommodate for noisy non-uniform data distribution with the noise that may vary across the manifold, and allow simultaneous estimation at multiple points. Crucially, all of the constants appearing in our bound are explicitly described. The proof uses a matrix concentration inequality to estimate covariance matrices and a Wasserstein distance bound for quantifying nonlinearity of the underlying manifold and non-uniformity of the probability measure.
    
[^188]: BLM-17m: 一个用于推特上黑人生命至关重要话题检测的大规模数据集

    BLM-17m: A Large-Scale Dataset for Black Lives Matter Topic Detection on Twitter. (arXiv:2105.01331v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2105.01331](http://arxiv.org/abs/2105.01331)

    本论文提出了一个用于推特上检测黑人生命至关重要话题的大规模数据集BLM-17m，涵盖了乔治·弗洛伊德事件期间的17百万推文。作者提供了两个基线模型TF-IDF和LDA，并对其进行了评估。

    

    人权保护是世界上最重要的问题之一。本文旨在提供一个涵盖最近几个月全球影响深远的人权矛盾之一——乔治·弗洛伊德事件的数据集。我们提出了一个带有17百万推文的主题检测标记数据集。这些推文是从2020年5月25日至2020年8月21日收集的，涵盖了这一事件开始后的89天。我们通过监测全球和本地报纸的最热门新闻主题对数据集进行了标记。除此之外，我们还提供了两个基线模型，TF-IDF和LDA。我们使用三个不同的k值对这两种方法的精确度、召回率和F1分数进行了评估。收集到的数据集可以在https://github.com/MeysamAsgariC/BLMT 上找到。

    Protection of human rights is one of the most important problems of our world. In this paper, our aim is to provide a dataset which covers one of the most significant human rights contradiction in recent months affected the whole world, George Floyd incident. We propose a labeled dataset for topic detection that contains 17 million tweets. These Tweets are collected from 25 May 2020 to 21 August 2020 that covers 89 days from start of this incident. We labeled the dataset by monitoring most trending news topics from global and local newspapers. Apart from that, we present two baselines, TF-IDF and LDA. We evaluated the results of these two methods with three different k values for metrics of precision, recall and f1-score. The collected dataset is available at https://github.com/MeysamAsgariC/BLMT.
    
[^189]: 在$\mathcal{O}(mn)$时间内解决线性方程组的算法解决方案

    Algorithmic Solution for Systems of Linear Equations, in $\mathcal{O}(mn)$ time. (arXiv:2104.12570v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.12570](http://arxiv.org/abs/2104.12570)

    该论文提出了一种以$\mathcal{O}(mn)$时间复杂度求解线性方程组的新算法，具有快速、向量化、低内存分配需求等特点，并在非方阵线性方程组中表现出较高的准确性与效率。算法收敛性得到理论证明，同时还使用这一算法解决了特征选择问题。

    

    我们提出了一种新颖的算法，可以以非常快的速度求解线性方程组的解。该算法在其基本形式上非常简短，并且根据定义进行向量化，而内存分配需求很低，因为对于每一次迭代，仅利用给定输入矩阵$\mathbf X$的一个维度。与现有方法相比，执行时间非常短，表现出超过$10^2$倍的加速比和较低的内存分配需求，尤其适用于非方阵线性方程组，其中方程数与特征数的比例较高（高瘦型方程组）或较低（宽型方程组）。准确性高且易于控制，数值结果突出了所提算法在计算时间、解的准确性和内存需求方面的效率。论文还包括了算法收敛性的理论证明，并将所提算法理论推广到特征选择。

    We present a novel algorithm attaining excessively fast, the sought solution of linear systems of equations. The algorithm is short in its basic formulation and, by definition, vectorized, while the memory allocation demands are trivial, because, for each iteration, only one dimension of the given input matrix $\mathbf X$ is utilized. The execution time is very short compared with state-of-the-art methods, exhibiting $> \times 10^2$ speed-up and low memory allocation demands, especially for non-square Systems of Linear Equations, with ratio of equations versus features high (tall systems), or low (wide systems) accordingly. The accuracy is high and straightforwardly controlled, and the numerical results highlight the efficiency of the proposed algorithm, in terms of computation time, solution accuracy and memory demands. The paper also comprises a theoretical proof for the algorithmic convergence, and we extend the implementation of the proposed algorithmic rationale to feature selecti
    
[^190]: 基于注意力的多模态图像匹配

    Attention-Based Multimodal Image Matching. (arXiv:2103.11247v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2103.11247](http://arxiv.org/abs/2103.11247)

    提出了一种基于注意力的方法用于多模态图像块匹配，通过Transformer编码器聚合多尺度图像特征，实现了任务特定的外观不变性。在多个基准测试中取得了最新的最高精度，证明了方法的通用性。

    

    我们提出了一种基于注意力的多模态图像块匹配方法，使用Transformer编码器对多尺度Siamese CNN的特征图进行注意力聚合。我们的编码器能够有效地聚合多尺度图像嵌入，并突出显示任务特定的外观不变的图像线索。我们还引入了一种注意力残差架构，使用绕过编码器的残差连接。这种额外的学习信号有助于从头开始进行端到端训练。我们的方法在多模态和单模态基准测试中实现了最新的最高精度，证明了其通用性。据我们所知，这是第一个将Transformer编码器架构成功应用于多模态图像块匹配任务的实现。

    We propose an attention-based approach for multimodal image patch matching using a Transformer encoder attending to the feature maps of a multiscale Siamese CNN. Our encoder is shown to efficiently aggregate multiscale image embeddings while emphasizing task-specific appearance-invariant image cues. We also introduce an attention-residual architecture, using a residual connection bypassing the encoder. This additional learning signal facilitates end-to-end training from scratch. Our approach is experimentally shown to achieve new state-of-the-art accuracy on both multimodal and single modality benchmarks, illustrating its general applicability. To the best of our knowledge, this is the first successful implementation of the Transformer encoder architecture to the multimodal image patch matching task.
    
[^191]: 在边缘设备上实现二值神经网络训练

    Enabling Binary Neural Network Training on the Edge. (arXiv:2102.04270v6 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2102.04270](http://arxiv.org/abs/2102.04270)

    本文研究了在边缘设备上实现二值神经网络训练的方法，并展示了通过量化反向传播操作实现了显著的内存占用减少，同时几乎没有降低准确率。

    

    随着越来越复杂的机器学习模型的计算需求不断增长，通常需要使用强大的云基础设施进行训练。二值神经网络由于其与高精度替代方案相比极高的计算和内存节省，被认为是在设备上进行推断的有希望的选择。然而，现有的训练方法需要同时存储所有层的高精度激活，这通常使得在内存受限设备上的学习变得不可行。在本文中，我们展示了二值神经网络训练所需的反向传播操作对量化非常强大，从而使得使用现代模型进行边缘学习成为一个实际的选择。我们提出了一种低成本的二值神经网络训练策略，具有显著的内存占用减少，同时对比Courbariaux和Bengio的标准方法，几乎没有降低准确率。

    The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloud-based infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higher-precision alternatives. However, their existing training methods require the concurrent storage of high-precision activations for all layers, generally making learning on memory-constrained devices infeasible. In this article, we demonstrate that the backward propagation operations needed for binary neural network training are strongly robust to quantization, thereby making on-the-edge learning with modern models a practical proposition. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions while inducing little to no accuracy loss vs Courbariaux & Bengio's standard approach. These decreases are primarily enabled t
    
[^192]: 深度控制学习用于库存控制

    Deep Controlled Learning for Inventory Control. (arXiv:2011.15122v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2011.15122](http://arxiv.org/abs/2011.15122)

    本论文提出了一种名为深度控制学习（DCL）的新型深度强化学习框架，针对库存控制问题进行了量身定制，并通过比较评估表明，在各种测试情况下，DCL相对于传统算法和最先进的启发式算法，在成本和最优性方面都取得了更好的表现。

    

    问题定义：传统的深度强化学习（DRL）算法，用于包括游戏和机器人等各种目的，是否是库存控制应用中最适合的机器学习算法？针对库存控制问题特点量身定制的DRL算法在性能上是否优于DRL和传统基准？方法/结果：我们提出并研究了基于近似策略迭代的新型DRL框架——深度控制学习（DCL），专门用于解决库存问题。比较性评估表明，DCL在销售损失库存控制、易腐库存系统和具有随机引导时间的库存系统中优于现有的最先进启发式算法，在所有测试实例中实现了更低的平均成本，并且维持了最多0.1%的最优性差距。值得注意的是，所有实验都使用相同的超参数集。

    Problem Definition: Are traditional deep reinforcement learning (DRL) algorithms, developed for a broad range of purposes including game-play and robotics, the most suitable machine learning algorithms for applications in inventory control? To what extent would DRL algorithms tailored to the unique characteristics of inventory control problems provide superior performance compared to DRL and traditional benchmarks? Methodology/results: We propose and study Deep Controlled Learning (DCL), a new DRL framework based on approximate policy iteration specifically designed to tackle inventory problems. Comparative evaluations reveal that DCL outperforms existing state-of-the-art heuristics in lost sales inventory control, perishable inventory systems, and inventory systems with random lead times, achieving lower average costs across all test instances and maintaining an optimality gap of no more than 0.1\%. Notably, the same hyperparameter set is utilized across all experiments, underscoring 
    
[^193]: 自主车辆高速公路决策：连续动作视角下的深度强化学习

    Decision-making for Autonomous Vehicles on Highway: Deep Reinforcement Learning with Continuous Action Horizon. (arXiv:2008.11852v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2008.11852](http://arxiv.org/abs/2008.11852)

    本文提出了一种基于深度强化学习的决策策略，用于自主车辆在高速公路上的连续视角决策问题。该方法通过引入近端策略优化的算法，实现了高学习效率和优秀的控制性能。该策略从多个角度进行评估，并展示了在类似驾驶场景中的在线应用潜力。

    

    自主车辆的决策策略描述了一系列的行驶动作来实现特定的导航任务。本文利用深度强化学习方法解决了高速公路上连续视角决策问题。首先介绍了车辆运动学和高速公路驾驶场景。自动车辆的运行目标是以高效且平稳的策略执行而不发生碰撞。然后介绍了名为近端策略优化（PPO）增强的深度强化学习算法。为了克服训练效率低和样本效率低的挑战，这个应用算法能够实现高学习效率和优秀的控制性能。最后，从最优性、学习效率和适应性等多个角度对基于PPO-DRL的决策策略进行评估。通过将其应用于类似的驾驶场景，讨论了其在线应用的潜力。

    Decision-making strategy for autonomous vehicles de-scribes a sequence of driving maneuvers to achieve a certain navigational mission. This paper utilizes the deep reinforcement learning (DRL) method to address the continuous-horizon decision-making problem on the highway. First, the vehicle kinematics and driving scenario on the freeway are introduced. The running objective of the ego automated vehicle is to execute an efficient and smooth policy without collision. Then, the particular algorithm named proximal policy optimization (PPO)-enhanced DRL is illustrated. To overcome the challenges in tardy training efficiency and sample inefficiency, this applied algorithm could realize high learning efficiency and excellent control performance. Finally, the PPO-DRL-based decision-making strategy is estimated from multiple perspectives, including the optimality, learning efficiency, and adaptability. Its potential for online application is discussed by applying it to similar driving scenario
    
[^194]: 基于脑力风暴的生成对抗网络（BGAN）：面向具有分布式私有数据集的多代理生成模型

    Brainstorming Generative Adversarial Networks (BGANs): Towards Multi-Agent Generative Models with Distributed Private Datasets. (arXiv:2002.00306v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.00306](http://arxiv.org/abs/2002.00306)

    提出了一种新颖的脑力风暴生成对抗网络（BGAN）架构，实现多个代理在完全分布式的方式下生成类似真实数据的样本，解决了多个代理共享有限且分布式数据集的问题。

    

    要实现高学习准确性，生成对抗网络（GANs）必须以充分代表数据空间的大型数据集作为输入。然而，在许多情况下，可用的数据集可能是有限的，并且分布在多个代理之间，每个代理都试图单独学习数据的分布。在这种情况下，代理通常不愿共享他们的本地数据，因为这可能导致大型数据集的通信开销。本文提出了一种新颖的脑力风暴生成对抗网络（BGAN）架构，用于解决这个多代理GAN问题，多个代理可以在完全分布式的方式下生成类似真实数据的样本。BGAN允许代理通过共享生成的数据样本而不是实际数据集进行“脑力风暴”来获取来自其他代理的信息。与现有的分布式GAN解决方案相比，所提出的BGAN架构被设计为完全分布式，并且不需要参与方彼此通信。

    To achieve a high learning accuracy, generative adversarial networks (GANs) must be fed by large datasets that adequately represent the data space. However, in many scenarios, the available datasets may be limited and distributed across multiple agents, each of which is seeking to learn the distribution of the data on its own. In such scenarios, the agents often do not wish to share their local data as it can cause communication overhead for large datasets. In this paper, to address this multi-agent GAN problem, a novel brainstorming GAN (BGAN) architecture is proposed using which multiple agents can generate real-like data samples while operating in a fully distributed manner. BGAN allows the agents to gain information from other agents without sharing their real datasets but by ``brainstorming'' via the sharing of their generated data samples. In contrast to existing distributed GAN solutions, the proposed BGAN architecture is designed to be fully distributed, and it does not need an
    

