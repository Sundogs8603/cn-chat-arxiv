# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Expressive Monotonic Neural Networks.](http://arxiv.org/abs/2307.07512) | 提出了一种带有单个残差连接的权重约束架构，可以实现任意子集的输入的精确单调依赖。与现有技术相比，该方法实施简单且理论基础更简化，计算开销很小，并且保证产生单调输出。 |
| [^2] | [Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach.](http://arxiv.org/abs/2307.07508) | 本文介绍了将动态车辆调度问题建模为半马尔可夫决策过程的基于事件的方法，并通过实际场景的数值实验验证了该方法的有效性。 |
| [^3] | [MGit: A Model Versioning and Management System.](http://arxiv.org/abs/2307.07507) | MGit是一个模型版本控制和管理系统，解决了机器学习中模型派生物的困难管理问题，通过引入血统图和优化存储机制，使得存储、测试、更新和协作模型派生物更加容易。 |
| [^4] | [Brain Tumor Detection using Convolutional Neural Networks with Skip Connections.](http://arxiv.org/abs/2307.07503) | 本文提出了不同架构的卷积神经网络用于分析和分类脑肿瘤，通过优化网络并添加跳跃连接，提高了网络的准确性。 |
| [^5] | [PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation.](http://arxiv.org/abs/2307.07489) | PseudoCal是一种无源无监督领域适应中用于校准预测不确定性的方法，相比于现有方法，PseudoCal不需要源数据且适用于严重域偏移的情况。 |
| [^6] | [DreamTeacher: Pretraining Image Backbones with Deep Generative Models.](http://arxiv.org/abs/2307.07487) | DreamTeacher是一个自监督特征表示学习框架，通过利用生成网络预训练图像骨干，然后将生成模型的知识蒸馏到目标骨干中，取得了显著的性能提升。 |
| [^7] | [Population Expansion for Training Language Models with Private Federated Learning.](http://arxiv.org/abs/2307.07477) | 本文提出了一种使用领域适应技术扩展人口的方法，以加速训练并提高使用较少设备进行训练时的模型质量。 |
| [^8] | [Structured Pruning of Neural Networks for Constraints Learning.](http://arxiv.org/abs/2307.07457) | 本文将神经网络结构化裁剪应用于约束学习，解决了人工神经网络参数过多导致可扩展性问题的挑战。 |
| [^9] | [Generative adversarial networks for data-scarce spectral applications.](http://arxiv.org/abs/2307.07454) | 本论文介绍了一种针对数据稀缺光谱应用的生成对抗网络（GANs）方法。通过引入Wasserstein GANs（WGANs）并对其进行条件化，成功生成了准确的光谱数据。这种方法为在科学研究中数据稀缺问题提供了解决方案。 |
| [^10] | [Differentially Private Clustering in Data Streams.](http://arxiv.org/abs/2307.07449) | 本研究提出了首个针对$k$-means和$k$-median聚类的差分隐私流算法，在流模型中实现对数据隐私的保护，并使用尽可能少的空间。 |
| [^11] | [TSNet-SAC: Leveraging Transformers for Efficient Task Scheduling.](http://arxiv.org/abs/2307.07445) | TSNet-SAC是一种基于Transformer的创新网络，利用启发式算法指导训练，通过引入滑动增强组件和扩展组件来提高鲁棒性和解决算法缺陷，适应不同的接入场景。模拟实验表明，TSNet-SAC在准确性和鲁棒性方面优于现有网络，并具有更优的调度决策延迟。 |
| [^12] | [Can Large Language Models Empower Molecular Property Prediction?.](http://arxiv.org/abs/2307.07443) | 本研究提出了利用大型语言模型（LLMs）对分子属性预测进行增强的方法，通过零/少样本分子分类和生成新的解释来推进这一目标。 |
| [^13] | [Atlas-Based Interpretable Age Prediction.](http://arxiv.org/abs/2307.07439) | 本研究提出了一种基于图谱的可解释年龄预测方法，利用全身图像研究了各个身体部位的年龄相关变化。通过使用解释性方法和配准技术，确定了最能预测年龄的身体区域，并创下了整个身体年龄预测的最新水平。研究结果表明，脊柱、本原性背部肌肉和心脏区域是最重要的关注领域。 |
| [^14] | [Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar.](http://arxiv.org/abs/2307.07426) | 本研究开发了针对增强击弦指弹的实时音乐信息检索技术，通过卷积神经网络和变分自动编码器，实现了吉他体击打的识别和嵌入学习。 |
| [^15] | [Enhancing ECG Analysis of Implantable Cardiac Monitor Data: An Efficient Pipeline for Multi-Label Classification.](http://arxiv.org/abs/2307.07423) | 这项研究提出了一种高效流水线，用于多标签分类的植入式心脏监测器数据的心电图分析，以减轻医护人员因不断增长的数据负荷而面临的压力。 |
| [^16] | [Exploiting Counter-Examples for Active Learning with Partial labels.](http://arxiv.org/abs/2307.07413) | 本论文研究了一个新的问题，即带有部分标签的主动学习（ALPL）。为了解决过度拟合问题和增强选择代表性样本的过程，我们利用反例构造了一种简单有效的 WorseNet 来进行学习。 |
| [^17] | [HuCurl: Human-induced Curriculum Discovery.](http://arxiv.org/abs/2307.07412) | 课程学习框架HuCurl能够根据先前对样本难度的了解，发现非单调的有效课程，并在多个NLP任务中胜过现有方法。 |
| [^18] | [Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks.](http://arxiv.org/abs/2307.07410) | 本论文通过研究对角线性网络（DLNs）的梯度流所施加的隐式正则化，意外地与广义近似难度（GHA）中的相变现象联系起来，并提供了对此的锐利结果。 |
| [^19] | [Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise.](http://arxiv.org/abs/2307.07406) | 本文提出了一种改进的联邦学习收敛分析技术，并针对存在噪声的通信场景提出了改进的信噪比控制策略，以提高收敛速率并节省功率资源。 |
| [^20] | [Performance of $\ell_1$ Regularization for Sparse Convex Optimization.](http://arxiv.org/abs/2307.07405) | 本论文研究了稀疏凸优化中$\ell_1$正则化的性能，给出了Group LASSO的恢复保证，并且发现了Group LASSO选择相同特征集的机制。 |
| [^21] | [Improving Zero-Shot Generalization for CLIP with Synthesized Prompts.](http://arxiv.org/abs/2307.07397) | 我们提出了一种名为SHIP的生成方法，通过使用合成的提示和类别名称来改进CLIP的微调方法，从而解决了长尾分布和缺乏标记数据的问题。 |
| [^22] | [Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations.](http://arxiv.org/abs/2307.07396) | 本论文研究了在双分图中可视化重叠聚类和相关问题的方法，并提供了能满足多个目标的算法，结果表明在真实数据集中最佳平衡是通过一种没有重叠的聚类解决方法得到的。 |
| [^23] | [Learning Sparse Neural Networks with Identity Layers.](http://arxiv.org/abs/2307.07389) | 该论文提出了一种基于中心核对齐的稀疏性正则化方法，通过减少不同层之间的特征相似性，可以提高神经网络的稀疏性。 |
| [^24] | [Higher-order topological kernels via quantum computation.](http://arxiv.org/abs/2307.07383) | 通过量子计算，我们提出了一种定义拓扑核的新方法，基于构建随着阶数增加的拓扑指纹，可以在多项式时间内近似计算高维Betti数 |
| [^25] | [Composition-contrastive Learning for Sentence Embeddings.](http://arxiv.org/abs/2307.07380) | 该论文提出了一种句子嵌入的构成对比学习方法，通过最大化文本和其词组成分的对齐，实现了从无标签数据中学习文本表示的目标，并在语义文本相似性任务上取得了与最先进方法可比较的改进，而无需额外的训练目标或网络参数。 |
| [^26] | [Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing.](http://arxiv.org/abs/2307.07378) | 本文研究了使用基于卷积神经网络的视觉处理来准确分类增材制造图像数据集中的缺陷，并应用主动学习技术进行优化，以构建一个人在循环机制，以减少培训和生成训练数据所需的数据量。 |
| [^27] | [AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes.](http://arxiv.org/abs/2307.07370) | 一种新型的图像字幕神经网络AIC-AB NET将空间注意力和文本属性相结合，通过自适应空间注意力和输入文本属性信息，提升图像字幕的性能和减少不确定性。 |
| [^28] | [A testing-based approach to assess the clusterability of categorical data.](http://arxiv.org/abs/2307.07346) | 这项研究提出了一种基于测试的方法，用于评估分类数据的聚类性能。通过计算属性对的卡方统计量的和得到一个解析的p值作为评估指标。结果表明这种方法在基准分类数据集上的表现优于其他方法。 |
| [^29] | [Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks.](http://arxiv.org/abs/2307.07344) | 本文提出了一种新颖的方法，通过一种新的正则化方法将基于偏微分方程（PDE）的进化模型与神经网络集成在一起。这些层可以实现特定的正则化目标，并赋予神经网络输出与进化模型对应的特性。此外，逆进化层的构建和实现相对简单，可以轻松地为不同的物理进化和神经网络设计。 |
| [^30] | [MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters.](http://arxiv.org/abs/2307.07343) | 本文提出了一种新的方法，用于训练支持向量分类器并选择模型参数。通过建模为极小化极大优化问题，利用投影梯度算法求解，实现了更低的时间复杂度。 |
| [^31] | [How Different Is Stereotypical Bias Across Languages?.](http://arxiv.org/abs/2307.07331) | 本研究拓展了评估预训练语言模型中刻板偏见的研究，通过跨语言分析发现mGPT-2在不同语言中显示出令人惊讶的反刻板行为，并且英语模型表现出最强的偏见，而土耳其语则最不明显。 |
| [^32] | [Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy.](http://arxiv.org/abs/2307.07328) | 该论文介绍了一种增强后门攻击的方法，通过引入可学习的污染样本选择策略来有效地选择污染样本。这解决了后门攻击中要从整个数据集中选择污染样本的问题。 |
| [^33] | [Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications.](http://arxiv.org/abs/2307.07325) | 本文提出了一种使用隐藏单元聚类框架进行自监督表示学习的方法，用于无文本资源下的语音表示学习。模型使用卷积神经网络和长短期记忆层对音频样本进行处理并生成上下文向量表示，通过隐藏单元聚类框架将这些表示归类为少量类似音素的单元，用于学习语义丰富的语音表示。研究结果表明这种方法可以提高模型性能。 |
| [^34] | [A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming.](http://arxiv.org/abs/2307.07322) | 本文提出了一个上下文感知的混合整数规划切割平面选择算法，通过引入新的评分指标、过滤技术和停止准则，使得SCIP在MIPLIB 2017基准集上的性能提升了4%。 |
| [^35] | [Adaptive Linear Estimating Equations.](http://arxiv.org/abs/2307.07320) | 本文提出了一种解决自适应线性回归模型中非正态渐近行为的方法，使用自适应线性估计方程构建去偏估计量，并在多臂老虎机的背景下保持了最小二乘估计量的非渐近性能。 |
| [^36] | [Hybrid moderation in the newsroom: Recommending featured posts to content moderators.](http://arxiv.org/abs/2307.07317) | 本文提出了一种在新闻编辑室中使用的混合审核方法，该方法通过向内容管理员推荐特色帖子来支持他们在选择特色内容方面做出决策。该方法基于概率排序的推荐系统，结合了用户和文本内容特征，取得了较高的分类和排序性能。内容管理员在评估中发现了合适的评论，并在很大程度上接受了推荐结果。 |
| [^37] | [HEAL-SWIN: A Vision Transformer On The Sphere.](http://arxiv.org/abs/2307.07313) | HEAL-SWIN是一个基于HEALPix网格和SWIN变压器结合的球面视觉变压器，在处理高分辨率广角鱼眼图像时能够实现无失真且高效的训练，并在语义分割任务中展现出优异的性能。 |
| [^38] | [Solving higher-order Lane-Emden-Fowler type equations using physics-informed neural networks: benchmark tests comparing soft and hard constraints.](http://arxiv.org/abs/2307.07302) | 本文使用物理约束神经网络（PINNs）成功解决了高阶Lane-Emden-Fowler类型方程，通过比较软约束和硬约束的变体，得出了有效的数值方法。 |
| [^39] | [3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks.](http://arxiv.org/abs/2307.07298) | 本文研究了使用完整的3D心脏形状（以点云形式）在改善心肌梗死（MI）事件检测方面的实用性。通过使用点云分类网络进行多步骤的自动化流程，结合点云几何深度学习技术，我们在英国生物银行受试者上得到了约13%和约5%的改进效果。 |
| [^40] | [Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment.](http://arxiv.org/abs/2307.07296) | 通过将已有的Visual-Graph SLAM算法ExploreORB与强化学习相结合，该研究提出了一种自主环境中基于前沿探索的强化学习算法。该算法通过基于奖励的系统学习和优化探索路线，解决了在存在多个距离相似的前沿的场景中可能导致非最优路径的问题。英文总结: This paper proposes a reinforcement learning algorithm with frontier-based exploration in autonomous environments by combining an existing Visual-Graph SLAM algorithm, ExploreORB. The algorithm allows the robot to learn and optimize exploration routes through a reward-based system to select frontiers accurately, addressing the issue of non-optimal paths in scenarios with similar-distance frontiers. |
| [^41] | [Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation.](http://arxiv.org/abs/2307.07269) | 这个论文提出了一种用于稳健的体积医学图像分割模型的频域对抗训练方法，并发现其相对于传统攻击方法具有优势。该方法利用频域对抗攻击，通过引入频率一致性损失来优化模型的性能，实现对体素和频域攻击的防御。 |
| [^42] | [On Interpolating Experts and Multi-Armed Bandits.](http://arxiv.org/abs/2307.07264) | 学习专家建议和多臂赌博是两个经典的在线决策问题，我们研究了两者之间的插值问题。我们提出了$\mathbf{m}$-MAB的极小后悔界并设计了$\mathbf{m}$-BAI的最优PAC算法，该算法旨在以尽可能少的轮数确定损失最小的臂。 |
| [^43] | [Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning.](http://arxiv.org/abs/2307.07250) | 通过敌对双机器学习方法，可以量化和缓解深度神经网络在面对敌对输入时的脆弱性。 |
| [^44] | [Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training.](http://arxiv.org/abs/2307.07246) | 该论文提出了一个名为KoBo的框架，它通过将临床知识整合到视觉语言语义一致性学习中，解决了医学领域中的大规模语义重叠和转移问题。 |
| [^45] | [Ed-Fed: A generic federated learning framework with resource-aware client selection for edge devices.](http://arxiv.org/abs/2307.07199) | Ed-Fed是一个通用的边缘设备资源感知客户端选择的联邦学习框架，可以优化等待时间，处理慢速设备，并且在FL中显著优化了等待时间。 |
| [^46] | [Controlling dynamical systems to complex target states using machine learning: next-generation vs. classical reservoir computing.](http://arxiv.org/abs/2307.07195) | 用机器学习控制动力系统到复杂的目标状态，比较了下一代储备计算和传统储备计算。研究发现，在数据受限的情况下，下一代储备计算表现优于传统方法，为实际控制应用提供了更多可能性。 |
| [^47] | [Benchmarks and Custom Package for Electrical Load Forecasting.](http://arxiv.org/abs/2307.07191) | 本文提供了一个全面的电力负荷预测存档，包括负荷领域特定的特征工程，帮助模型更好地模拟负荷数据，并提供了一种新的损失函数来最小化后续任务的成本。 |
| [^48] | [Multiplicative update rules for accelerating deep learning training and increasing robustness.](http://arxiv.org/abs/2307.07189) | 本文提出了一种优化框架，通过使用乘法更新规则，加速深度学习训练并提高模型的鲁棒性。 |
| [^49] | [DISPEL: Domain Generalization via Domain-Specific Liberating.](http://arxiv.org/abs/2307.07181) | DISPEL是一种通过后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征的领域泛化方法。 |
| [^50] | [A Surrogate Data Assimilation Model for the Estimation of Dynamical System in a Limited Area.](http://arxiv.org/abs/2307.07178) | 该论文提出了一个基于学习的代理数据同化模型，用于在有限区域实现高效的状态估计。该模型利用神经网络进行在线计算，避免了整合高维有限区域模型的需求，并且相比传统算法具有较大的计算优势。同时，该模型基于可观测性和有效区域的概念设计，能够确定所需的最佳观测数据量，并减轻与计算可观测性和生成训练数据相关的计算负担。 |
| [^51] | [Safe DreamerV3: Safe Reinforcement Learning with World Models.](http://arxiv.org/abs/2307.07176) | Safe DreamerV3是一种通过集成基于拉格朗日和计划的方法到世界模型中的新算法，实现了在低维度和仅采用视觉的任务中几乎零成本的安全强化学习。 |
| [^52] | [FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout.](http://arxiv.org/abs/2307.07172) | 本文提出了一种基于贝叶斯推断和自适应丢弃的联邦学习方法（FedBIAD），通过将局部模型的权重行视为概率分布，并根据与局部训练损失趋势相关的重要度指标自适应地丢弃部分权重行，解决了联邦学习中的通信效率和性能保证问题。 |
| [^53] | [Certified Robustness for Large Language Models with Self-Denoising.](http://arxiv.org/abs/2307.07171) | 传统的随机平滑方法对于大型语言模型的直接应用具有挑战性，为了解决认证半径很小的问题，提出了一种带有自我净化的新方法。 |
| [^54] | [Vulnerability-Aware Instance Reweighting For Adversarial Training.](http://arxiv.org/abs/2307.07167) | 该论文提出了一种新的实例级别重新加权方案，通过考虑每个自然示例的易受攻击性和对抗攻击导致的信息损失，来改进对抗训练算法的性能。 |
| [^55] | [Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords.](http://arxiv.org/abs/2307.07160) | 本研究提出了一种通过遮盖领域内关键词进行的领域自适应预训练方法，实验结果表明该方法优于使用随机遮盖的领域内预训练和常见的预训练然后微调范式。 |
| [^56] | [Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms.](http://arxiv.org/abs/2307.07134) | 本文提出了一个任务无关的评估框架Camilla，通过定义多维度的诊断度量Ability来协同测量每个机器学习算法的多面体强度，解决了机器学习算法在实际性能和标准化评估中存在的差距问题。 |
| [^57] | [DataAssist: A Machine Learning Approach to Data Cleaning and Preparation.](http://arxiv.org/abs/2307.07119) | DataAssist是一种机器学习方法，用于提高数据集质量和节省数据清洗和准备时间。 |
| [^58] | [Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems.](http://arxiv.org/abs/2307.07113) | 本文提出了一种应用于分散随机双正则化非凸强凸极小极大问题的方差减少加速方法，通过引入拉格朗日乘子和采用单个邻居通信并结合方差减少技术，该方法在随机设置下样本复杂度达到$\mathcal{O}(\kappa^3\varepsilon^{-3})$。 |
| [^59] | [Graph Positional and Structural Encoder.](http://arxiv.org/abs/2307.07107) | 这是一个关于图位置和结构编码器的研究，提出了图位置和结构编码器（GPSE），它能有效地捕捉多个PSE的共同潜在表示，并在各种图预测任务中取得显著的性能提升。 |
| [^60] | [MaxCorrMGNN: A Multi-Graph Neural Network Framework for Generalized Multimodal Fusion of Medical Data for Outcome Prediction.](http://arxiv.org/abs/2307.07093) | MaxCorrMGNN是一种用于广义多模态医疗数据融合的创新神经网络框架，通过MaxCorr embeddings建模患者内部和患者之间的非线性模态相关性，并通过多层图网络在任务中进行推理，有效预测结果。 |
| [^61] | [Choice Models and Permutation Invariance.](http://arxiv.org/abs/2307.07090) | 本文提出了选择模型和置换不变性的基本特征化方法，并展示了如何通过非参数估计器逼近选择函数，以及在实际应用中的灵活性和优越性。 |
| [^62] | [Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability.](http://arxiv.org/abs/2307.07084) | 本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。 |
| [^63] | [A Scenario-Based Functional Testing Approach to Improving DNN Performance.](http://arxiv.org/abs/2307.07083) | 本文提出了一种基于场景的功能测试方法，通过迭代测试ML模型在不同场景下表现，并使用迁移学习技术进行治疗，以提高机器学习应用的性能。 |
| [^64] | [Kernel t-distributed stochastic neighbor embedding.](http://arxiv.org/abs/2307.07081) | 本文介绍了一个核化版本的t-SNE算法，可以将高维数据映射到低维空间并保持数据点之间的非欧几里德度量下的距离。该算法具有改善性能和准确性的潜力，在涉及核方法的分类问题中展现出更清晰的聚类效果。 |
| [^65] | [Rician likelihood loss for quantitative MRI using self-supervised deep learning.](http://arxiv.org/abs/2307.07072) | 本研究提出了一种使用自监督深度学习进行定量MRI的方法，通过引入负对数Rician似然（NLR）损失函数，解决了低信噪比条件下参数估计偏差的问题。 |
| [^66] | [Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training.](http://arxiv.org/abs/2307.07066) | Proof of Training (PoT)是一种结合了AI和区块链技术的协议，利用实用性拜占庭错误容忍（PBFT）共识机制同步全局状态。该协议在任务吞吐量、系统鲁棒性和网络安全方面表现出相当的潜力。 |
| [^67] | [Bootstrapping Vision-Language Learning with Decoupled Language Pre-training.](http://arxiv.org/abs/2307.07063) | 本文提出了一种新的方法，通过解耦语言预训练，集中在语言组件上，提供了优化应用大型语言模型的框架，有效改善了视觉语言学习的性能。 |
| [^68] | [Controllable Emphasis with zero data for text-to-speech.](http://arxiv.org/abs/2307.07062) | 我们提出了一种零数据的可扩展方法，用于在文本转语音中产生高质量的重音，不需要用到录音或标注的信息。我们通过增加重音词的预测持续时间来实现重音语音，相比于声谱图修改技术，这种方法在自然度和重音词识别方面都有显著的提高。该方法在不同语言、不同语音和多种语言风格下都被证明是可扩展且首选的。 |
| [^69] | [Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement.](http://arxiv.org/abs/2307.07055) | 使用条件扩散模型进行有奖导向生成，能够有效学习和从奖励条件下的数据分布中进行采样，同时恢复数据的潜在子空间表示，并且生成新的群体靠近用户指定的目标奖励值。 |
| [^70] | [Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section.](http://arxiv.org/abs/2307.07051) | 通过分析临床记录的部分，我们发现预测能力在不同类型的记录间存在差异，并且当上下文长度较大时，组合不同类型的记录可以改善性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。 |
| [^71] | [Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation.](http://arxiv.org/abs/2307.07050) | 这篇论文提出了一种新的方法，即基于Wasserstein量子蒙特卡洛的方法，用于解决量子多体Schr\"odinger方程。该方法重新制定了能量泛函的最小化问题，并将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，同时利用深度学习方法来表示丰富的波函数族。 |
| [^72] | [AnyStar: Domain randomized universal star-convex 3D instance segmentation.](http://arxiv.org/abs/2307.07044) | AnyStar是一种基于域随机化的通用星凸3D实例分割模型，通过使用合成数据进行训练，不需要来自未见过的数据集的注释图像即可准确分割星凸形状。 |
| [^73] | [Accelerated gradient methods for nonconvex optimization: Escape trajectories from strict saddle points and convergence to local minima.](http://arxiv.org/abs/2307.07030) | 本文研究了一类加速梯度方法在非凸优化问题上的行为，包括逃离鞍点和收敛到局部极小值点的分析。研究在渐进和非渐进情况下，提出了一类新的Nesterov类型的加速方法，并回答了Nesterov加速梯度方法是否避免了严格鞍点的问题。 |
| [^74] | [Leveraging Factored Action Spaces for Off-Policy Evaluation.](http://arxiv.org/abs/2307.07014) | 本文研究了如何利用分解的动作空间来减轻涉及大型组合动作空间问题的非策略评估（OPE）的高偏差和高方差问题。通过提出一种基于分解动作空间的新型“分解”重要性抽样（IS）估计器系列，我们证明了分解IS估计器具有比非分解版本更小的方差，同时具有零偏差的性质，并通过模拟实验证实了该理论结果。 |
| [^75] | [Impact of Free-carrier Nonlinearities on Silicon Microring-based Reservoir Computing.](http://arxiv.org/abs/2307.07011) | 使用硅微环共振器进行的基于自由载流子非线性的蓄水容器计算，通过量化热光效应和自由载流子效应的影响，并确定了使NARMA-10任务中NMSE小于0.05的泵浦功率和频率失谐范围。 |
| [^76] | [Data Augmentation for Mathematical Objects.](http://arxiv.org/abs/2307.06984) | 这项研究讨论并评估了在数学对象中的数据平衡和数据增强的思路，通过交换变量名称生成新的问题实例，提高了机器学习模型的准确性，并认为数据集平衡和增加数据集大小对于效果的提升都至关重要。 |
| [^77] | [A decision framework for selecting information-transfer strategies in population-based SHM.](http://arxiv.org/abs/2307.06978) | 本文提出了一个决策框架，用于选择人群结构健康监测中的信息传输策略，通过避免负面传输和优化信息传输策略，可以降低成本和提高安全性。 |
| [^78] | [Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0.](http://arxiv.org/abs/2307.06975) | 本论文提出了一种基于神经符号的强化去噪扩散概率模型，用于在工业4.0中实时预测异常。该方法集成了工业本体论，为智能制造提供了形式化知识，并且通过随机傅里叶特征提取扩散模型，实现了在嵌入式系统中直接集成。这种方法在以前从未被探索过。 |
| [^79] | [Short Boolean Formulas as Explanations in Practice.](http://arxiv.org/abs/2307.06971) | 本论文研究了基于一元关系的数据模型的短布尔公式解释的可行性，提出了对期望错误的定量界限，并展示了在三个具体数据集上的实际应用。通过限制公式长度，可以获得避免过拟合且准确且易于理解的解释。 |
| [^80] | [Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens.](http://arxiv.org/abs/2307.06970) | 本研究使用机器学习算法估计了采用FDM工艺制造的PLA试样的极限抗拉强度（UTS），结果表明KNN算法优于其他算法，在区分不同UTS类别方面表现出更好的效果。 |
| [^81] | [Layerwise Linear Mode Connectivity.](http://arxiv.org/abs/2307.06966) | 本文提出了一种分层线性模态连接方法用于联邦深度学习，通过解决模型漂移和高损失障壁的问题，能够有效提升全局模型的性能。 |
| [^82] | [Is Task-Agnostic Explainable AI a Myth?.](http://arxiv.org/abs/2307.06963) | 我们提出了一个框架来统一当代可解释人工智能的挑战，指出虽然XAI方法为模型提供了有用的输出，但研究人员和决策者应注意它们的限制，需要在该领域有一个概念突破以解决XAI方法和应用任务之间的兼容性挑战。 |
| [^83] | [Embracing the chaos: analysis and diagnosis of numerical instability in variational flows.](http://arxiv.org/abs/2307.06957) | 本文研究了数值不稳定性对变分流中采样、密度评估和ELBO估计的可靠性的影响。通过理论保证和实验验证，我们发现尽管存在严重的数值不稳定性，变分流产生的结果在应用中常常足够准确。 |
| [^84] | [AI For Global Climate Cooperation 2023 Competition Proceedings.](http://arxiv.org/abs/2307.06951) | 国际社会必须合作应对气候变化，通过结合人工智能和气候经济模拟，设计促进和激励合作的国际框架，解决协议遵守、政策目标实现和持续承诺等挑战。 |
| [^85] | [Pathway toward prior knowledge-integrated machine learning in engineering.](http://arxiv.org/abs/2307.06950) | 本研究提出了一种整合先验知识的机器学习方法，对工程领域中的信息不确定性进行了检查，并探索了以三层知识整合的机器学习范式进行知识分解。这种方法平衡了整体论和还原论观点。 |
| [^86] | [Uncovering Unique Concept Vectors through Latent Space Decomposition.](http://arxiv.org/abs/2307.06913) | 通过潜在空间分解和无监督聚类，我们提出了一种自动揭示深度学习模型学习到的概念向量的方法，这些概念向量与模型预测相关且具有语义的独特概念，并且在实验中表明这些概念对人类来说易于理解和与任务相关。 |
| [^87] | [PC-Droid: Faster diffusion and improved quality for particle cloud generation.](http://arxiv.org/abs/2307.06836) | PC-Droid是一个新的扩散模型，通过利用新的扩散公式和研究更近期的积分求解器，同时对所有类型的喷注进行训练，实现了最先进的性能。它不仅能提供更快的生成速度，而且在所有评估指标上都具有卓越的性能。 |
| [^88] | [Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning.](http://arxiv.org/abs/2307.06501) | 本研究提出了一种名为HyCPAP的混合控制策略，通过结合模型预测控制和集成深度强化学习，并充分利用它们各自的优势，以解决人工胰腺的复杂生理过程、延迟胰岛素反应和不准确血糖测量等挑战。 |
| [^89] | [Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective.](http://arxiv.org/abs/2307.06457) | 该论文研究了组合分布偏移的问题，提出了基于矩阵补全的解决方法。通过在特殊情况下的双线性嵌入，实现对训练中未涵盖的测试分布进行外推。这个设置将缺失非随机数据的矩阵补全问题广义化。 |
| [^90] | [Identifiability Guarantees for Causal Disentanglement from Soft Interventions.](http://arxiv.org/abs/2307.06250) | 本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。 |
| [^91] | [Stack More Layers Differently: High-Rank Training Through Low-Rank Updates.](http://arxiv.org/abs/2307.05695) | 本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。 |
| [^92] | [Using Linear Regression for Iteratively Training Neural Networks.](http://arxiv.org/abs/2307.05189) | 我们提出了一种使用线性回归来迭代训练神经网络的方法，通过从输出向后计算神经元的理想总输入值，以线性最小二乘问题迭代更新参数和激活值。 |
| [^93] | [Decorrelation using Optimal Transport.](http://arxiv.org/abs/2307.05187) | 引入了一种利用凸神经最优输运求解器进行去相关的新方法，通过最优输运将连续特征空间与受保护属性去相关。在高能物理中，这种方法在喷注分类中表现出色，能更好地去相关多类输出特征空间。 |
| [^94] | [Intrinsically motivated graph exploration using network theories of human curiosity.](http://arxiv.org/abs/2307.04962) | 在这项工作中，我们通过应用人类好奇心的两个理论，发展了一种内在驱动的图探索方法。我们利用图神经网络的强化学习将拓扑特征作为奖励，从而实现了对图结构数据的探索。在多类合成生成图上进行的实验证明，我们的方法不仅可以推广到更大的环境，还可以进行更长的探索步行。同时，我们的方法比传统的贪婪评估方法更高效。 |
| [^95] | [LINFA: a Python library for variational inference with normalizing flow and annealing.](http://arxiv.org/abs/2307.04675) | LINFA是一个用于变分推理的Python库，支持处理计算复杂的模型和难以采样的依赖参数分布。 |
| [^96] | [SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation.](http://arxiv.org/abs/2307.03716) | 本研究将生物进化中发展出的肌肉协同控制策略应用于人手和腿模型，发现通过协同行动表示（SAR）训练的策略在学习复杂任务时明显优于端到端强化学习。 |
| [^97] | [DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification.](http://arxiv.org/abs/2307.03500) | DEFT是一种利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化的方法，可以减少分布式深度学习中的通信流量，并在计算成本和梯度累积方面具有优势。 |
| [^98] | [On Formal Feature Attribution and Its Approximation.](http://arxiv.org/abs/2307.03380) | 这篇论文研究了解释性人工智能（XAI）中的形式特征归因方法及其近似方法。现有的特征选择和归因方法存在一些问题，而形式化的XAI方法虽然是一个有希望的解决方案，但仍存在一些限制。 |
| [^99] | [A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization.](http://arxiv.org/abs/2307.01946) | 本文介绍了一种用于生成合成ECG图像的工具箱，旨在促进扫描ECG数字化。通过引入真实伪影，如手写文本伪影、皱纹、折痕和视角变换，该方法可以在标准纸质ECG背景上生成具有真实性的ECG图像。这种方法有助于解决合成ECG图像中缺乏参考时间序列的问题。 |
| [^100] | [Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian Learning.](http://arxiv.org/abs/2307.00828) | 本文提出了一种模型辅助的概率安全自适应控制方法，通过集成元学习、贝叶斯模型和控制限制函数方法，利用先前任务收集的数据进行预训练，并引入悲观置信区间来确保安全控制。 |
| [^101] | [Few-Shot Continual Learning via Flat-to-Wide Approaches.](http://arxiv.org/abs/2306.14369) | 本文提出了一种少样本持续学习方法，名为FLat-tO-WidE AppRoach (FLOWER)，通过寻找扁平化宽化极小值的过程来解决灾难性遗忘问题，利用球体生成器概念进行数据增强方法来克服数据稀缺性问题。在小型基础任务中，FLOWER表现出显著改进的性能。 |
| [^102] | [14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon.](http://arxiv.org/abs/2306.06283) | 本文记录了一次黑客松活动，参与者使用LLMs进行了各种应用，包括预测分子和材料特性、从非结构化数据中提取知识、为工具设计新界面以及开发新的教育应用。这些多样化的项目反映了LLMs在材料科学和化学领域的多功能性和潜力。 |
| [^103] | [Hypothesis Transfer Learning with Surrogate Classification Losses.](http://arxiv.org/abs/2305.19694) | 本文研究了使用代理分类损失的假设迁移学习的学习理论，通过算法稳定性提供了在温和假设下的学习保证，适用于机器学习算法。 |
| [^104] | [Dink-Net: Neural Clustering on Large Graphs.](http://arxiv.org/abs/2305.18405) | Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。 |
| [^105] | [Attention Schema in Neural Agents.](http://arxiv.org/abs/2305.17375) | 本文研究了神经智能中的注意力模式，并提出了注意力模式理论（AST）。作者发现将AS实现为一种循环内部控制的智能体效果最佳，这一理论为应用与改进神经智能提供了新思路。 |
| [^106] | [Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach.](http://arxiv.org/abs/2304.08349) | 本论文提出了一种深度可解释关系强化学习框架，通过将神经和符号世界相结合，利用神经符号化的方法提取出可解释的策略，解决了深度强化学习中不可解释性和处理环境结构变化的困难。 |
| [^107] | [Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers.](http://arxiv.org/abs/2303.16464) | 本文理论证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。本文的选择损失函数方针为Adam或AdamW优化算法的使用提供了指导。实验结果表明了Lipschitz常数较低且最大值较小的损失函数可以提高模型的泛化能力。 |
| [^108] | [DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion.](http://arxiv.org/abs/2303.14863) | DiffTAD是一种新的时间动作检测方法，使用Proposal Denoising Diffusion的生成建模视角，并通过正向/噪声过程和反向/去噪过程实现准确的动作提议。 |
| [^109] | [Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2303.09032) | 本文提出了一种名为条件乐观探索(COE)的基于UCT的探索方法，通过结构依赖关系鼓励智能体进行基于乐观主义的协同探索。 |
| [^110] | [Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis.](http://arxiv.org/abs/2302.14460) | 本研究开发了可解释的机器学习模型，利用超声影像预测儿科疑似阑尾炎的诊断、管理和严重程度。模型使用了超声影像和临床、实验室数据进行训练，并推广了概念瓶颈模型到多视图和不完整概念集的预测问题。 |
| [^111] | [A Data Mining Approach for Detecting Collusion in Unproctored Online Exams.](http://arxiv.org/abs/2302.07014) | 提出了一个在疫情期间检测学生是否串通作弊的数据挖掘方法，并通过对远程考试事件日志数据的分析发现了群体作弊行为或者异常，同时建立了一个评估异常案例的经验规则。 |
| [^112] | [The Re-Label Method For Data-Centric Machine Learning.](http://arxiv.org/abs/2302.04391) | 本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。 |
| [^113] | [CLIPood: Generalizing CLIP to Out-of-Distributions.](http://arxiv.org/abs/2302.00864) | 本论文提出了CLIPood，一种将CLIP泛化到超出分布测试数据的方法。CLIPood引入了新的训练目标MMS和优化策略BMA，以适应OOD情况并提升性能。 |
| [^114] | [Approximating the Shapley Value without Marginal Contributions.](http://arxiv.org/abs/2302.00736) | 本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。 |
| [^115] | [DoCoFL: Downlink Compression for Cross-Device Federated Learning.](http://arxiv.org/abs/2302.00543) | 本研究提出了DoCoFL，一种用于跨设备联合学习的下行压缩框架，能够在降低双向带宽的同时保持竞争力的准确性。 |
| [^116] | [Online Convex Optimization with Stochastic Constraints: Zero Constraint Violation and Bandit Feedback.](http://arxiv.org/abs/2301.11267) | 本论文提出了一种针对具有随机约束的在线凸优化的漂移加惩罚算法变体，其保证在固定迭代次数后达到了$O(\sqrt{T})$的期望遗憾和零约束违规率。同时，将算法框架扩展到了具有赌博反馈的情况下，仍然能够实现$O(\sqrt{T})$的期望遗憾和零约束违规率。 |
| [^117] | [Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm.](http://arxiv.org/abs/2211.12271) | 本文提出了一种名为全局$k$-means\texttt{++}的聚类算法，通过利用中心选择概率实现对全局$k$-means的质量聚类解的有效获取，并减少了计算负载。 |
| [^118] | [$\Phi$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation.](http://arxiv.org/abs/2209.15609) | 本文提出了一种物理指导的动态变分自编码器 ($\Phi$-DVAE) 用于将非结构化数据同化到物理模型中，解决了传统方法在未知映射情况下无法实现一致模型与数据综合的问题。 |
| [^119] | [Differentially Private Stochastic Gradient Descent with Low-Noise.](http://arxiv.org/abs/2209.04188) | 本文研究了具有低噪声的差分隐私随机梯度下降法在点问题和成对学习中的性能，并推导出更精确的过剩风险界限。提出的算法基于梯度扰动，具有优化过剩风险率的最佳效果。 |
| [^120] | [Fully probabilistic deep models for forward and inverse problems in parametric PDEs.](http://arxiv.org/abs/2208.04856) | 本论文提出了一种基于深度学习和概率建模的全概率深度模型，用于学习参数化偏微分方程中的前向和反向映射。模型通过最大化观察到的零残差的概率来进行训练，不需要独立的训练数据。 |
| [^121] | [Rank-based Decomposable Losses in Machine Learning: A Survey.](http://arxiv.org/abs/2207.08768) | 这篇综述回顾了机器学习中基于排名可分解损失函数的研究并提供了分类法。它讨论了可分解性与优化算法之间的相互作用，并列举了排名损失在深度学习中的最新应用和面临的挑战。 |
| [^122] | [Privacy-preserving machine learning with tensor networks.](http://arxiv.org/abs/2202.12319) | 本文展示了张量网络架构在保护隐私的机器学习中具有潜在优势，并提出了确保鲁棒性的明确条件。 |
| [^123] | [Unpacking the Black Box: Regulating Algorithmic Decisions.](http://arxiv.org/abs/2110.03443) | 本文研究如何在代理使用复杂的“黑盒”预测函数进行决策的情况下，对算法决策进行最优调控。研究发现，限制代理使用透明度足够高的预测函数是低效的，而针对激励偏差源头的目标化工具可以提供次优解决方案，从而改善福利。 |
| [^124] | [Automating Augmentation Through Random Unidimensional Search.](http://arxiv.org/abs/2106.08756) | 通过随机单维搜索的数据增强自动化方法，仅使用6次训练即可获得与使用100次训练相当的性能。 |
| [^125] | [Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games.](http://arxiv.org/abs/2002.10113) | 我们提出了一种交替人口和代理控制神经网络（APAC-Net）来解决高维度随机均场博弈（MFG）问题。通过利用变分原始-对偶结构和神经网络参数化价值和密度函数，我们展示了在高维度MFG问题中的潜力。 |
| [^126] | [Signed iterative random forests to identify enhancer-associated transcription factor binding.](http://arxiv.org/abs/1810.07287) | 本文介绍了一种有符号的迭代随机森林（siRF）方法，用于推断Drosophila melanogaster中增强子元素周围的转录因子之间的调控相互作用和功能结合签名。 |

# 详细

[^1]: 表达性单调神经网络

    Expressive Monotonic Neural Networks. (arXiv:2307.07512v1 [cs.LG])

    [http://arxiv.org/abs/2307.07512](http://arxiv.org/abs/2307.07512)

    提出了一种带有单个残差连接的权重约束架构，可以实现任意子集的输入的精确单调依赖。与现有技术相比，该方法实施简单且理论基础更简化，计算开销很小，并且保证产生单调输出。

    

    神经网络输出对某些输入的单调依赖是许多场景中的重要归纳偏见，其中领域知识要求这种行为。这对解释性和公平性考虑尤为重要。在更广泛的背景下，单调性重要的情况可在金融、医学、物理等学科中找到。因此，构建能够可靠实现这种归纳偏见的神经网络架构是值得期望的。在本文中，我们提出了一种带有单个残差连接的权重约束架构，以实现任意子集的输入的精确单调依赖。权重约束方案直接控制神经网络的Lipschitz常数，从而提供额外的鲁棒性。与目前用于单调性的现有技术相比，我们的方法在实现和理论基础上更简单，计算开销几乎可以忽略不计，并且保证产生单调输出。

    The monotonic dependence of the outputs of a neural network on some of its inputs is a crucial inductive bias in many scenarios where domain knowledge dictates such behavior. This is especially important for interpretability and fairness considerations. In a broader context, scenarios in which monotonicity is important can be found in finance, medicine, physics, and other disciplines. It is thus desirable to build neural network architectures that implement this inductive bias provably. In this work, we propose a weight-constrained architecture with a single residual connection to achieve exact monotonic dependence in any subset of the inputs. The weight constraint scheme directly controls the Lipschitz constant of the neural network and thus provides the additional benefit of robustness. Compared to currently existing techniques used for monotonicity, our method is simpler in implementation and in theory foundations, has negligible computational overhead, is guaranteed to produce mono
    
[^2]: 深度强化学习用于动态车辆调度问题的基于事件的方法

    Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach. (arXiv:2307.07508v1 [cs.AI])

    [http://arxiv.org/abs/2307.07508](http://arxiv.org/abs/2307.07508)

    本文介绍了将动态车辆调度问题建模为半马尔可夫决策过程的基于事件的方法，并通过实际场景的数值实验验证了该方法的有效性。

    

    动态车辆调度问题涉及决定将哪些车辆分配给随机产生的时间和空间请求。该问题出现在各个领域，如将卡车分配给要运输的货物、应急系统和顺风车服务中。本文将该问题建模为半马尔可夫决策过程，这使我们能够将时间视为连续变量。在此设置中，决策时刻与事件一致，其时间间隔是随机的。我们认为基于事件的方法大大降低了决策空间的组合复杂性，并克服了文献中常提出的离散时间模型的其他局限性。为了测试我们的方法，我们开发了一个新的离散事件模拟器，并使用双深度Q学习训练我们的决策代理。在使用纽约市的数据进行实际场景的数值实验中，我们将通过我们的方法获得的策略与启发式方法进行了比较。

    The dynamic vehicle dispatching problem corresponds to deciding which vehicles to assign to requests that arise stochastically over time and space. It emerges in diverse areas, such as in the assignment of trucks to loads to be transported; in emergency systems; and in ride-hailing services. In this paper, we model the problem as a semi-Markov decision process, which allows us to treat time as continuous. In this setting, decision epochs coincide with discrete events whose time intervals are random. We argue that an event-based approach substantially reduces the combinatorial complexity of the decision space and overcomes other limitations of discrete-time models often proposed in the literature. In order to test our approach, we develop a new discrete-event simulator and use double deep q-learning to train our decision agents. Numerical experiments are carried out in realistic scenarios using data from New York City. We compare the policies obtained through our approach with heuristic
    
[^3]: MGit:一个模型版本控制和管理系统

    MGit: A Model Versioning and Management System. (arXiv:2307.07507v1 [cs.LG])

    [http://arxiv.org/abs/2307.07507](http://arxiv.org/abs/2307.07507)

    MGit是一个模型版本控制和管理系统，解决了机器学习中模型派生物的困难管理问题，通过引入血统图和优化存储机制，使得存储、测试、更新和协作模型派生物更加容易。

    

    从其他模型派生出的模型在机器学习中非常常见。例如，通过微调，迁移学习用于从“预训练”模型创建特定任务的模型。这导致了一个模型之间相关且共享结构甚至参数值的生态系统。然而，管理这些模型派生物是困难的：存储所有派生模型的开销很快变得繁重，导致用户丢弃可能对进一步分析有用的中间模型。此外，难以追踪模型中的不良行为（例如，是否从上游模型继承了一个错误？）。在本文中，我们提出了一个名为MGit的模型版本控制和管理系统，它使得存储、测试、更新和协作模型派生物更加容易。MGit引入了一个记录模型之间来源和版本信息的血统图，以及优化存储模型参数的抽象机制。

    Models derived from other models are extremely common in machine learning (ML) today. For example, transfer learning is used to create task-specific models from "pre-trained" models through finetuning. This has led to an ecosystem where models are related to each other, sharing structure and often even parameter values. However, it is hard to manage these model derivatives: the storage overhead of storing all derived models quickly becomes onerous, prompting users to get rid of intermediate models that might be useful for further analysis. Additionally, undesired behaviors in models are hard to track down (e.g., is a bug inherited from an upstream model?). In this paper, we propose a model versioning and management system called MGit that makes it easier to store, test, update, and collaborate on model derivatives. MGit introduces a lineage graph that records provenance and versioning information between models, optimizations to efficiently store model parameters, as well as abstractio
    
[^4]: 使用具有跳跃连接的卷积神经网络进行脑肿瘤检测

    Brain Tumor Detection using Convolutional Neural Networks with Skip Connections. (arXiv:2307.07503v1 [eess.IV])

    [http://arxiv.org/abs/2307.07503](http://arxiv.org/abs/2307.07503)

    本文提出了不同架构的卷积神经网络用于分析和分类脑肿瘤，通过优化网络并添加跳跃连接，提高了网络的准确性。

    

    本文介绍了使用卷积神经网络（CNN）在磁共振成像（MRI）技术下分析和分类脑肿瘤为良性和恶性的不同架构。通过对网络进行拓宽、深化和添加跳跃连接等优化技术，提高了网络的准确性。结果表明，这些技术的子集可以巧妙地用于超越用于相同目的的基准CNN模型。

    In this paper, we present different architectures of Convolutional Neural Networks (CNN) to analyze and classify the brain tumors into benign and malignant types using the Magnetic Resonance Imaging (MRI) technique. Different CNN architecture optimization techniques such as widening and deepening of the network and adding skip connections are applied to improve the accuracy of the network. Results show that a subset of these techniques can judiciously be used to outperform a baseline CNN model used for the same purpose.
    
[^5]: PseudoCal：一种无源自适应领域中无监督不确定性校准方法

    PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation. (arXiv:2307.07489v1 [cs.LG])

    [http://arxiv.org/abs/2307.07489](http://arxiv.org/abs/2307.07489)

    PseudoCal是一种无源无监督领域适应中用于校准预测不确定性的方法，相比于现有方法，PseudoCal不需要源数据且适用于严重域偏移的情况。

    

    无监督领域适应（UDA）在提高无标签目标领域模型准确性方面取得了显著进展。然而，目标领域中预测不确定性的校准，这是安全部署UDA模型的关键，却受到了限制。传统的领域内校准方法“温度缩放”（TempScal）在面对域分布偏移和无标记目标域数据的情况下存在挑战。最近的方法采用重要性加权技术来估计基于重新加权标记源数据的目标最优温度。然而，这些方法需要源数据，在严重域偏移情况下密度估计不可靠，不适用于无源UDA设置。为了克服这些限制，我们提出了PseudoCal，一种仅依赖无标签目标数据的无源校准方法。

    Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in improving the accuracy of models for unlabeled target domains. However, the calibration of predictive uncertainty in the target domain, a crucial aspect of the safe deployment of UDA models, has received limited attention. The conventional in-domain calibration method, \textit{temperature scaling} (TempScal), encounters challenges due to domain distribution shifts and the absence of labeled target domain data. Recent approaches have employed importance-weighting techniques to estimate the target-optimal temperature based on re-weighted labeled source data. Nonetheless, these methods require source data and suffer from unreliable density estimates under severe domain shifts, rendering them unsuitable for source-free UDA settings. To overcome these limitations, we propose PseudoCal, a source-free calibration method that exclusively relies on unlabeled target data. Unlike previous approaches that treat UDA calib
    
[^6]: DreamTeacher: 使用深度生成模型对图像骨干进行预训练的自监督特征表示学习框架

    DreamTeacher: Pretraining Image Backbones with Deep Generative Models. (arXiv:2307.07487v1 [cs.CV])

    [http://arxiv.org/abs/2307.07487](http://arxiv.org/abs/2307.07487)

    DreamTeacher是一个自监督特征表示学习框架，通过利用生成网络预训练图像骨干，然后将生成模型的知识蒸馏到目标骨干中，取得了显著的性能提升。

    

    在这项工作中，我们介绍了一个自监督特征表示学习框架DreamTeacher，它利用生成网络来预训练下游图像骨干。我们提出从训练好的生成模型中提取知识，并将其蒸馏到已经为特定感知任务进行了优化的标准图像骨干中。我们研究了两种知识蒸馏方法：1）将学习到的生成特征蒸馏到目标图像骨干上，作为预训练这些骨干的替代方法，而不是使用大规模标注数据集（如ImageNet）进行预训练；2）将生成网络得到的标签通过任务头蒸馏到目标骨干的逻辑层上。我们对多个生成模型、密集预测基准和几种预训练方法进行了详细分析。我们实验证明，我们的DreamTeacher在各方面显著优于现有的自监督特征表示学习方法。使用DreamTeacher进行无监督的ImageNet预训练可以显著提高模型性能。

    In this work, we introduce a self-supervised feature representation learning framework DreamTeacher that utilizes generative networks for pre-training downstream image backbones. We propose to distill knowledge from a trained generative model into standard image backbones that have been well engineered for specific perception tasks. We investigate two types of knowledge distillation: 1) distilling learned generative features onto target image backbones as an alternative to pretraining these backbones on large labeled datasets such as ImageNet, and 2) distilling labels obtained from generative networks with task heads onto logits of target backbones. We perform extensive analyses on multiple generative models, dense prediction benchmarks, and several pre-training regimes. We empirically find that our DreamTeacher significantly outperforms existing self-supervised representation learning approaches across the board. Unsupervised ImageNet pre-training with DreamTeacher leads to significan
    
[^7]: 使用私人联邦学习进行语言模型训练的人口扩展

    Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])

    [http://arxiv.org/abs/2307.07477](http://arxiv.org/abs/2307.07477)

    本文提出了一种使用领域适应技术扩展人口的方法，以加速训练并提高使用较少设备进行训练时的模型质量。

    

    结合差分隐私的联邦学习为分布式设备提供带有正式隐私保证的机器学习训练。当设备数量庞大时，联邦学习与差分隐私的结合能够及时生成性能良好的模型。然而，对于设备数量较少的应用，由于差分隐私噪声与设备数量成反比，模型效用下降，同时由于需要等待来自较小设备池的足够客户端可用，训练延迟也增加。因此，本文提出基于领域适应技术扩展人口，以加快训练并改善使用较少设备进行训练时的最终模型质量。我们通过实验证明，我们的技术可以使实际语言建模数据集的效用提高13%至30%。

    Federated learning (FL) combined with differential privacy (DP) offers machine learning (ML) training with distributed devices and with a formal privacy guarantee. With a large population of devices, FL with DP produces a performant model in a timely manner. However, for applications with a smaller population, not only does the model utility degrade as the DP noise is inversely proportional to population, but also the training latency increases since waiting for enough clients to become available from a smaller pool is slower. In this work, we thus propose expanding the population based on domain adaptation techniques to speed up the training and improves the final model quality when training with small populations. We empirically demonstrate that our techniques can improve the utility by 13% to 30% on real-world language modeling datasets.
    
[^8]: 神经网络的结构化裁剪用于约束学习

    Structured Pruning of Neural Networks for Constraints Learning. (arXiv:2307.07457v1 [cs.LG])

    [http://arxiv.org/abs/2307.07457](http://arxiv.org/abs/2307.07457)

    本文将神经网络结构化裁剪应用于约束学习，解决了人工神经网络参数过多导致可扩展性问题的挑战。

    

    近年来，在机器学习（ML）模型与运筹学（OR）工具的整合方面在各种应用中都受到了广泛的关注，包括癌症治疗、算法配置和化学过程优化。在这个领域中，ML和OR的组合通常依赖于使用混合整数规划（MIP）形式表示ML模型输出。文献中的许多研究已经开发了这样的形式化方法来处理许多ML预测器，特别是人工神经网络（ANNs），因为它们在许多应用中具有重要的兴趣。然而，由于人工神经网络经常包含大量参数，导致 MIP 形式化方法难以解决，从而限制了可扩展性。事实上，机器学习界已经引入了几种技术来减少人工神经网络的参数数量，而不影响性能，因为现代人工神经网络的庞大规模对机器学习应用提出了挑战，它明显降低了系统的可扩展性。

    In recent years, the integration of Machine Learning (ML) models with Operation Research (OR) tools has gained popularity across diverse applications, including cancer treatment, algorithmic configuration, and chemical process optimization. In this domain, the combination of ML and OR often relies on representing the ML model output using Mixed Integer Programming (MIP) formulations. Numerous studies in the literature have developed such formulations for many ML predictors, with a particular emphasis on Artificial Neural Networks (ANNs) due to their significant interest in many applications. However, ANNs frequently contain a large number of parameters, resulting in MIP formulations that are impractical to solve, thereby impeding scalability. In fact, the ML community has already introduced several techniques to reduce the parameter count of ANNs without compromising their performance, since the substantial size of modern ANNs presents challenges for ML applications as it significantly
    
[^9]: 针对数据稀缺光谱应用的生成对抗网络

    Generative adversarial networks for data-scarce spectral applications. (arXiv:2307.07454v1 [physics.optics])

    [http://arxiv.org/abs/2307.07454](http://arxiv.org/abs/2307.07454)

    本论文介绍了一种针对数据稀缺光谱应用的生成对抗网络（GANs）方法。通过引入Wasserstein GANs（WGANs）并对其进行条件化，成功生成了准确的光谱数据。这种方法为在科学研究中数据稀缺问题提供了解决方案。

    

    生成对抗网络（GANs）是生成人工智能领域中最强大且多功能的技术之一。在本研究中，我们报告了GANs在合成光谱数据生成领域的应用，为解决在各种科学背景下数据稀缺的问题提供了一种解决方案。我们通过将其应用于近场辐射传热领域中的一个多层次双曲介质问题来展示所提出的方法。我们发现成功生成光谱数据需要对传统GANs进行两个修改：（i）引入Wasserstein GANs（WGANs）以避免模式崩溃，以及（ii）对WGANs进行条件化以获得生成数据的准确标签。我们展示了一个简单的前馈神经网络（FFNN），在有限数据条件下，通过使用CWGAN生成的数据，显著提高了其性能，证明了其内在价值。

    Generative adversarial networks (GANs) are one of the most robust and versatile techniques in the field of generative artificial intelligence. In this work, we report on an application of GANs in the domain of synthetic spectral data generation, offering a solution to the scarcity of data found in various scientific contexts. We demonstrate the proposed approach by applying it to an illustrative problem within the realm of near-field radiative heat transfer involving a multilayered hyperbolic metamaterial. We find that a successful generation of spectral data requires two modifications to conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels for the generated data. We show that a simple feed-forward neural network (FFNN), when augmented with data generated by a CWGAN, enhances significantly its performance under conditions of limited data availability, demonstrating the intrinsic value o
    
[^10]: 数据流中的差分隐私聚类

    Differentially Private Clustering in Data Streams. (arXiv:2307.07449v1 [cs.DS])

    [http://arxiv.org/abs/2307.07449](http://arxiv.org/abs/2307.07449)

    本研究提出了首个针对$k$-means和$k$-median聚类的差分隐私流算法，在流模型中实现对数据隐私的保护，并使用尽可能少的空间。

    

    流模型是处理大规模现代数据分析的一种常见方法。在流模型中，数据点依次流入，算法只能对数据流进行一次遍历，目标是在使用尽可能少的空间的同时，在流中进行一些分析。聚类问题是基本的无监督机器学习原语，过去已经对流聚类算法进行了广泛的研究。然而，在许多实际应用中，数据隐私已成为一个核心关注点，非私有聚类算法在许多场景下不适用。在这项工作中，我们提供了第一个针对$k$-means和$k$-median聚类的差分私有流算法，该算法在长度最多为$T$的流上使用$poly(k,d,\log(T))$的空间来实现一个“常数”。

    The streaming model is an abstraction of computing over massive data streams, which is a popular way of dealing with large-scale modern data analysis. In this model, there is a stream of data points, one after the other. A streaming algorithm is only allowed one pass over the data stream, and the goal is to perform some analysis during the stream while using as small space as possible.  Clustering problems (such as $k$-means and $k$-median) are fundamental unsupervised machine learning primitives, and streaming clustering algorithms have been extensively studied in the past. However, since data privacy becomes a central concern in many real-world applications, non-private clustering algorithms are not applicable in many scenarios.  In this work, we provide the first differentially private streaming algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean data points over a stream with length at most $T$ using $poly(k,d,\log(T))$ space to achieve a {\it constant} 
    
[^11]: TSNet-SAC: 利用Transformer进行高效任务调度

    TSNet-SAC: Leveraging Transformers for Efficient Task Scheduling. (arXiv:2307.07445v1 [cs.NI])

    [http://arxiv.org/abs/2307.07445](http://arxiv.org/abs/2307.07445)

    TSNet-SAC是一种基于Transformer的创新网络，利用启发式算法指导训练，通过引入滑动增强组件和扩展组件来提高鲁棒性和解决算法缺陷，适应不同的接入场景。模拟实验表明，TSNet-SAC在准确性和鲁棒性方面优于现有网络，并具有更优的调度决策延迟。

    

    在未来的6G移动边缘计算（MEC）中，自动驾驶系统需要具备处理多模态数据和强相互依赖性的能力。然而，传统启发式算法由于需要多次迭代来得到最优方案，在实时调度方面表现不佳。我们提出了一种基于Transformer的创新网络TSNet-SAC，仅利用启发式算法指导TSNet的训练。此外，我们引入了滑动增强组件（SAC）来增强鲁棒性和解决算法缺陷。此外，我们设计了扩展组件来处理多尺度训练数据并提供网络可扩展性，使得TSNet能够适应不同的接入场景。模拟实验表明，TSNet-SAC在准确性和鲁棒性方面优于现有网络，与启发式算法相比具有更优的调度决策延迟。

    In future 6G Mobile Edge Computing (MEC), autopilot systems require the capability of processing multimodal data with strong interdependencies. However, traditional heuristic algorithms are inadequate for real-time scheduling due to their requirement for multiple iterations to derive the optimal scheme. We propose a novel TSNet-SAC based on Transformer, that utilizes heuristic algorithms solely to guide the training of TSNet. Additionally, a Sliding Augment Component (SAC) is introduced to enhance the robustness and resolve algorithm defects. Furthermore, the Extender component is designed to handle multi-scale training data and provide network scalability, enabling TSNet to adapt to different access scenarios. Simulation demonstrates that TSNet-SAC outperforms existing networks in accuracy and robustness, achieving superior scheduling-making latency compared to heuristic algorithms.
    
[^12]: 大型语言模型能增强分子属性预测吗？

    Can Large Language Models Empower Molecular Property Prediction?. (arXiv:2307.07443v1 [cs.LG])

    [http://arxiv.org/abs/2307.07443](http://arxiv.org/abs/2307.07443)

    本研究提出了利用大型语言模型（LLMs）对分子属性预测进行增强的方法，通过零/少样本分子分类和生成新的解释来推进这一目标。

    

    分子属性预测因其在多个科学领域中的改变潜力而受到重视。传统上，分子图可以表示为图结构化数据或SMILES文本。最近，大型语言模型（LLMs）的快速发展彻底改变了自然语言处理领域。尽管利用LLMs来理解用SMILES表示的分子是自然的，但LLMs如何影响分子属性预测的探索仍处于早期阶段。在这项工作中，我们从零/少样本分子分类和使用LLMs生成的新解释作为分子表示两个角度推进了这一目标。具体而言，我们首先提示LLMs进行上下文分子分类并评估其性能。然后，我们利用LLMs为原始SMILES生成语义丰富的解释，并利用此来微调小规模的LM模型。

    Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM mode
    
[^13]: 基于图谱的可解释年龄预测

    Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])

    [http://arxiv.org/abs/2307.07439](http://arxiv.org/abs/2307.07439)

    本研究提出了一种基于图谱的可解释年龄预测方法，利用全身图像研究了各个身体部位的年龄相关变化。通过使用解释性方法和配准技术，确定了最能预测年龄的身体区域，并创下了整个身体年龄预测的最新水平。研究结果表明，脊柱、本原性背部肌肉和心脏区域是最重要的关注领域。

    

    年龄预测是医学评估和研究的重要部分，可以通过突出实际年龄和生物年龄之间的差异来帮助检测疾病和异常衰老。为了全面了解各个身体部位的年龄相关变化，我们使用了全身图像进行研究。我们利用Grad-CAM解释性方法确定最能预测一个人年龄的身体区域。通过使用配准技术生成整个人群的解释性图，我们将分析扩展到个体之外。此外，我们以一个平均绝对误差为2.76年的模型，创下了整个身体年龄预测的最新水平。我们的研究结果揭示了三个主要的关注领域：脊柱、本原性背部肌肉和心脏区域，其中心脏区域具有最重要的作用。

    Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
    
[^14]: 声学吉他的实时敲击技术识别和嵌入学习

    Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar. (arXiv:2307.07426v1 [cs.SD])

    [http://arxiv.org/abs/2307.07426](http://arxiv.org/abs/2307.07426)

    本研究开发了针对增强击弦指弹的实时音乐信息检索技术，通过卷积神经网络和变分自动编码器，实现了吉他体击打的识别和嵌入学习。

    

    实时音乐信息检索（RT-MIR）在增强传统声学乐器的能力方面具有巨大潜力。我们开发了针对增强击弦指弹的实时音乐信息检索技术，将声学吉他演奏与吉他体击打相结合。我们为增强乐器表演的实时音乐信息检索系统制定了几个设计目标：（i）因果约束，（ii）感知上可忽略的音动延迟，（iii）控制亲密性支持，（iv）合成控制支持。我们基于卷积神经网络（CNNs）和与变分自动编码器（VAEs）共同训练的CNNs，提出并评估了实时吉他体击打识别和嵌入学习技术。我们基于手部部位和位置，引入了吉他体击打的分类系统。通过收集并根据分类系统标记的三个数据集，采用跨数据集评估方法评估了模型的嵌入质量。

    Real-time music information retrieval (RT-MIR) has much potential to augment the capabilities of traditional acoustic instruments. We develop RT-MIR techniques aimed at augmenting percussive fingerstyle, which blends acoustic guitar playing with guitar body percussion. We formulate several design objectives for RT-MIR systems for augmented instrument performance: (i) causal constraint, (ii) perceptually negligible action-to-sound latency, (iii) control intimacy support, (iv) synthesis control support. We present and evaluate real-time guitar body percussion recognition and embedding learning techniques based on convolutional neural networks (CNNs) and CNNs jointly trained with variational autoencoders (VAEs). We introduce a taxonomy of guitar body percussion based on hand part and location. We follow a cross-dataset evaluation approach by collecting three datasets labelled according to the taxonomy. The embedding quality of the models is assessed using KL-Divergence across distribution
    
[^15]: 提升植入式心脏监测器数据的心电图分析：多标签分类的高效流水线

    Enhancing ECG Analysis of Implantable Cardiac Monitor Data: An Efficient Pipeline for Multi-Label Classification. (arXiv:2307.07423v1 [eess.SP])

    [http://arxiv.org/abs/2307.07423](http://arxiv.org/abs/2307.07423)

    这项研究提出了一种高效流水线，用于多标签分类的植入式心脏监测器数据的心电图分析，以减轻医护人员因不断增长的数据负荷而面临的压力。

    

    作为当今增长最快的可植入心脏设备市场，植入式心脏监测器（ICM）设备在病人中越来越普遍，用于测量心脏电活动。ICM持续监测和记录病人的心律，并在触发时将数据发送到安全服务器，医疗专业人员可以进行审查。为了不漏诊，这些设备采用了相对简单的基于规则的算法（由于能耗约束）。由于其不断监测心脏节律和日益普及的特点，该算法通常被参数化为过于敏感的模式，导致相对较高的误报率，这使得医护人员需要分析和诊断越来越多的数据。为了减轻医护人员的负担，现在已经出现了自动化的心电图分析方法。

    Implantable Cardiac Monitor (ICM) devices are demonstrating as of today, the fastest-growing market for implantable cardiac devices. As such, they are becoming increasingly common in patients for measuring heart electrical activity. ICMs constantly monitor and record a patient's heart rhythm and when triggered - send it to a secure server where health care professionals (denote HCPs from here on) can review it. These devices employ a relatively simplistic rule-based algorithm (due to energy consumption constraints) to alert for abnormal heart rhythms. This algorithm is usually parameterized to an over-sensitive mode in order to not miss a case (resulting in relatively high false-positive rate) and this, combined with the device's nature of constantly monitoring the heart rhythm and its growing popularity, results in HCPs having to analyze and diagnose an increasingly growing amount of data. In order to reduce the load on the latter, automated methods for ECG analysis are nowadays becom
    
[^16]: 利用反例对带有部分标签的主动学习进行探索

    Exploiting Counter-Examples for Active Learning with Partial labels. (arXiv:2307.07413v1 [cs.LG])

    [http://arxiv.org/abs/2307.07413](http://arxiv.org/abs/2307.07413)

    本论文研究了一个新的问题，即带有部分标签的主动学习（ALPL）。为了解决过度拟合问题和增强选择代表性样本的过程，我们利用反例构造了一种简单有效的 WorseNet 来进行学习。

    

    本论文研究了一个新的问题，即带有部分标签的主动学习（ALPL）。在这种设置中，一个 oracle 用部分标签对查询样本进行注释，放宽了对准确标注过程的要求。为了解决 ALPL，我们首先建立了一个直观的基线，可以无缝地融入到现有的 AL 框架中。虽然有效，但这个基线仍然容易过度拟合，并且在查询过程中缺乏代表性的基于部分标签的样本。受认知科学中人类推理的启发，我们的目标是利用这种类似人类的学习模式来解决过度拟合问题，同时增强 ALPL 中选择代表性样本的过程。具体而言，我们通过反转每个实例的部分标签构造反例，然后提出了一个简单但有效的 WorseNet 来直接从这些反例中学习。

    This paper studies a new problem, \emph{active learning with partial labels} (ALPL). In this setting, an oracle annotates the query samples with partial labels, relaxing the oracle from the demanding accurate labeling process. To address ALPL, we first build an intuitive baseline that can be seamlessly incorporated into existing AL frameworks. Though effective, this baseline is still susceptible to the \emph{overfitting}, and falls short of the representative partial-label-based samples during the query process. Drawing inspiration from human inference in cognitive science, where accurate inferences can be explicitly derived from \emph{counter-examples} (CEs), our objective is to leverage this human-like learning pattern to tackle the \emph{overfitting} while enhancing the process of selecting representative samples in ALPL. Specifically, we construct CEs by reversing the partial labels for each instance, and then we propose a simple but effective WorseNet to directly learn from this c
    
[^17]: HuCurl: 人类引导课程发现

    HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])

    [http://arxiv.org/abs/2307.07412](http://arxiv.org/abs/2307.07412)

    课程学习框架HuCurl能够根据先前对样本难度的了解，发现非单调的有效课程，并在多个NLP任务中胜过现有方法。

    

    我们引入了课程发现问题，并描述了一个能够在课程空间中基于先前有关样本难度的知识发现有效课程的课程学习框架。通过使用注释熵和损失作为难度的度量，我们展示了：（i）给定模型和数据集的最佳发现课程往往与现有文献中的单调课程相反；（ii）传统的由易到难或由难到易过渡的课程往往存在表现不佳的风险；（iii）对较小数据集和模型发现的课程在较大数据集和模型上表现良好。所提出的框架涵盖了一些现有的课程学习方法，并能够发现在几个NLP任务上胜过它们的课程。

    We introduce the problem of curriculum discovery and describe a curriculum learning framework capable of discovering effective curricula in a curriculum space based on prior knowledge about sample difficulty. Using annotation entropy and loss as measures of difficulty, we show that (i): the top-performing discovered curricula for a given model and dataset are often non-monotonic as opposed to monotonic curricula in existing literature, (ii): the prevailing easy-to-hard or hard-to-easy transition curricula are often at the risk of underperforming, and (iii): the curricula discovered for smaller datasets and models perform well on larger datasets and models respectively. The proposed framework encompasses some of the existing curriculum learning approaches and can discover curricula that outperform them across several NLP tasks.
    
[^18]: AI中的隐式正则化与优化中的广义近似难度相遇--对对角线性网络的锐利结果。

    Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks. (arXiv:2307.07410v1 [cs.LG])

    [http://arxiv.org/abs/2307.07410](http://arxiv.org/abs/2307.07410)

    本论文通过研究对角线性网络（DLNs）的梯度流所施加的隐式正则化，意外地与广义近似难度（GHA）中的相变现象联系起来，并提供了对此的锐利结果。

    

    理解神经网络架构和基于梯度的优化方法所施加的隐式正则化是深度学习和人工智能中的一个关键挑战。在这项工作中，我们针对超参数回归设置提供了对于对角线性网络（DLNs）的梯度流所施加的隐式正则化的锐利结果，并意外地将其与广义近似难度（GHA）中的相变现象联系起来。GHA将近似难度的现象从计算机科学推广到连续和鲁棒优化等领域。众所周知，具有微小初始化的DLNs的梯度流的$\ell^1$-范数收敛到基础追踪的目标函数。我们通过展示具有微小初始化的DLNs的梯度流近似于基础追踪优化问题的最小化器（而不仅仅是目标函数），进一步改进了这些结果，并获得了新的锐利收敛结果。

    Understanding the implicit regularization imposed by neural network architectures and gradient based optimization methods is a key challenge in deep learning and AI. In this work we provide sharp results for the implicit regularization imposed by the gradient flow of Diagonal Linear Networks (DLNs) in the over-parameterized regression setting and, potentially surprisingly, link this to the phenomenon of phase transitions in generalized hardness of approximation (GHA). GHA generalizes the phenomenon of hardness of approximation from computer science to, among others, continuous and robust optimization. It is well-known that the $\ell^1$-norm of the gradient flow of DLNs with tiny initialization converges to the objective function of basis pursuit. We improve upon these results by showing that the gradient flow of DLNs with tiny initialization approximates minimizers of the basis pursuit optimization problem (as opposed to just the objective function), and we obtain new and sharp converg
    
[^19]: 在噪声存在的情况下，改进了联邦学习的收敛分析和信噪比控制策略

    Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise. (arXiv:2307.07406v1 [cs.LG])

    [http://arxiv.org/abs/2307.07406](http://arxiv.org/abs/2307.07406)

    本文提出了一种改进的联邦学习收敛分析技术，并针对存在噪声的通信场景提出了改进的信噪比控制策略，以提高收敛速率并节省功率资源。

    

    我们提出了一种改进的收敛分析技术，描述了具有不完美/噪声上行和下行通信的联邦学习（FL）的分布式学习范式。这种不完美的通信场景在新兴通信系统和协议中实际部署FL时出现。本文中开发的分析首次证明了FL中上行和下行通信的不利影响存在不对称性。特别是，下行噪声的不利影响对FL算法的收敛更为严重。基于这个观察，我们提出了改进的信噪比（SNR）控制策略，通过丢弃可忽略的高阶项，使FL的收敛速率与完美的、无噪声的通信信道情况下相似，同时比现有解决方案节省了大量的功率资源。特别地，我们确立了维持$O(\frac{1}{\sqrt{K}})$收敛速率所需的信噪比控制策略。

    We propose an improved convergence analysis technique that characterizes the distributed learning paradigm of federated learning (FL) with imperfect/noisy uplink and downlink communications. Such imperfect communication scenarios arise in the practical deployment of FL in emerging communication systems and protocols. The analysis developed in this paper demonstrates, for the first time, that there is an asymmetry in the detrimental effects of uplink and downlink communications in FL. In particular, the adverse effect of the downlink noise is more severe on the convergence of FL algorithms. Using this insight, we propose improved Signal-to-Noise (SNR) control strategies that, discarding the negligible higher-order terms, lead to a similar convergence rate for FL as in the case of a perfect, noise-free communication channel while incurring significantly less power resources compared to existing solutions. In particular, we establish that to maintain the $O(\frac{1}{\sqrt{K}})$ rate of co
    
[^20]: $\ell_1$正则化在稀疏凸优化中的性能

    Performance of $\ell_1$ Regularization for Sparse Convex Optimization. (arXiv:2307.07405v1 [cs.LG])

    [http://arxiv.org/abs/2307.07405](http://arxiv.org/abs/2307.07405)

    本论文研究了稀疏凸优化中$\ell_1$正则化的性能，给出了Group LASSO的恢复保证，并且发现了Group LASSO选择相同特征集的机制。

    

    虽然LASSO和Group LASSO在实践中被广泛采用, 但是对于除了统计问题以外的其他情况, 这些算法的保证令人震惊地缺乏, 并且在确定性输入的稀疏凸优化背景下通常被认为是一种启发式算法。我们为具有向量值特征的稀疏凸优化的Group LASSO给出了第一个恢复保证。我们证明了，如果在最小化严格凸函数$l$时应用足够大的Group LASSO正则化，那么极小化器是在具有最大梯度的$\ell_2$范数的向量值特征上支持的稀疏向量。因此，重复此过程选择与正交匹配追踪算法相同的特征集，通过弱次模性证明了对于具有受限强凸性和光滑性的任何函数$l$都具有恢复保证。这回答了Tibshirani等人和Yasuda等人的开放问题。我们的结果首次在理论上解释了

    Despite widespread adoption in practice, guarantees for the LASSO and Group LASSO are strikingly lacking in settings beyond statistical problems, and these algorithms are usually considered to be a heuristic in the context of sparse convex optimization on deterministic inputs. We give the first recovery guarantees for the Group LASSO for sparse convex optimization with vector-valued features. We show that if a sufficiently large Group LASSO regularization is applied when minimizing a strictly convex function $l$, then the minimizer is a sparse vector supported on vector-valued features with the largest $\ell_2$ norm of the gradient. Thus, repeating this procedure selects the same set of features as the Orthogonal Matching Pursuit algorithm, which admits recovery guarantees for any function $l$ with restricted strong convexity and smoothness via weak submodularity arguments. This answers open questions of Tibshirani et al. and Yasuda et al. Our result is the first to theoretically expla
    
[^21]: 通过生成的提示改进CLIP的零样本泛化能力

    Improving Zero-Shot Generalization for CLIP with Synthesized Prompts. (arXiv:2307.07397v1 [cs.CV])

    [http://arxiv.org/abs/2307.07397](http://arxiv.org/abs/2307.07397)

    我们提出了一种名为SHIP的生成方法，通过使用合成的提示和类别名称来改进CLIP的微调方法，从而解决了长尾分布和缺乏标记数据的问题。

    

    随着人们对CLIP等预训练视觉语言模型的兴趣日益增长，最近的研究重点在于将这些模型适应到下游任务上。尽管取得了有希望的结果，但大多数现有方法都需要所有类别的标记数据，而在真实世界的应用中可能不具备这种条件，因为存在长尾分布和Zipf定律。例如，某些类别可能完全缺乏标记数据，如新兴概念。为了解决这个问题，我们提出了一种名为SYNTHESIZED PROMPTS (SHIP) 的即插即用的生成方法，以改进现有的微调方法。具体而言，我们采用变分自动编码器引入一个生成器，通过将合成的提示和相应的类别名称输入到CLIP的文本编码器中来重构视觉特征。通过这种方式，我们可以轻松获取剩余仅有标签的类别的生成特征。然后，我们采用现成的方法对CLIP进行微调，将标记和合成的特征进行组合。

    With the growing interest in pretrained vision-language models like CLIP, recent research has focused on adapting these models to downstream tasks. Despite achieving promising results, most existing methods require labeled data for all classes, which may not hold in real-world applications due to the long tail and Zipf's law. For example, some classes may lack labeled data entirely, such as emerging concepts. To address this problem, we propose a plug-and-play generative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed \textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods. Specifically, we follow variational autoencoders to introduce a generator that reconstructs the visual features by inputting the synthesized prompts and the corresponding class names to the textual encoder of CLIP. In this manner, we easily obtain the synthesized features for the remaining label-only classes. Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled and sy
    
[^22]: 可视化重叠双聚类和布尔矩阵分解

    Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations. (arXiv:2307.07396v1 [cs.LG])

    [http://arxiv.org/abs/2307.07396](http://arxiv.org/abs/2307.07396)

    本论文研究了在双分图中可视化重叠聚类和相关问题的方法，并提供了能满足多个目标的算法，结果表明在真实数据集中最佳平衡是通过一种没有重叠的聚类解决方法得到的。

    

    在双分图中寻找（双）聚类是一种流行的数据分析方法。分析人员通常希望可视化这些聚类，只要聚类不重叠，这是简单的。然而，许多现代算法发现重叠的聚类，使可视化变得更加复杂。在本文中，我们研究了在双分图中可视化重叠聚类和相关问题的问题，以及可视化布尔矩阵分解的问题。我们概念化了任何好的可视化应该满足的三个不同目标：（1）聚类元素的接近性，（2）相同聚类的元素的大连续区域，以及（3）可视化中的大连续区域，无论聚类成员如何。我们提供了捕捉这些目标的目标函数和优化这些目标函数的算法。有趣的是，在对真实数据集进行的实验中，我们发现在这些竞争目标之间的最佳平衡是通过一种没有重叠的聚类解决方法得到的。

    Finding (bi-)clusters in bipartite graphs is a popular data analysis approach. Analysts typically want to visualize the clusters, which is simple as long as the clusters are disjoint. However, many modern algorithms find overlapping clusters, making visualization more complicated. In this paper, we study the problem of visualizing \emph{a given clustering} of overlapping clusters in bipartite graphs and the related problem of visualizing Boolean Matrix Factorizations. We conceptualize three different objectives that any good visualization should satisfy: (1) proximity of cluster elements, (2) large consecutive areas of elements from the same cluster, and (3) large uninterrupted areas in the visualization, regardless of the cluster membership. We provide objective functions that capture these goals and algorithms that optimize these objective functions. Interestingly, in experiments on real-world datasets, we find that the best trade-off between these competing goals is achieved by a no
    
[^23]: 学习带有恒等层的稀疏神经网络

    Learning Sparse Neural Networks with Identity Layers. (arXiv:2307.07389v1 [cs.LG])

    [http://arxiv.org/abs/2307.07389](http://arxiv.org/abs/2307.07389)

    该论文提出了一种基于中心核对齐的稀疏性正则化方法，通过减少不同层之间的特征相似性，可以提高神经网络的稀疏性。

    

    深度神经网络的稀疏性已经得到了广泛的研究，目的是最大化性能并尽可能减少过参数化网络的大小。现有方法主要集中在使用阈值和指标通过剪枝参数来进行训练过程中的稀疏化。而本文提出了一种新的角度，即不同层之间的特征相似性，该相似性在证明之后被证明与网络稀疏性高度相关。受到超参数化模型中层间特征相似性的启发，我们研究了网络稀疏性与层间特征相似性之间的内在联系。具体而言，我们证明基于中心核对齐（CKA）来减少层间特征相似性可以改善网络的稀疏性，利用信息瓶颈理论来支持这一观点。基于这个理论，我们提出了一种基于CKA的稀疏性正则化方法，称为CKA-SR，它利用CKA来减少层间的特征相似性，从而增加网络的稀疏性。

    The sparsity of Deep Neural Networks is well investigated to maximize the performance and reduce the size of overparameterized networks as possible. Existing methods focus on pruning parameters in the training process by using thresholds and metrics. Meanwhile, feature similarity between different layers has not been discussed sufficiently before, which could be rigorously proved to be highly correlated to the network sparsity in this paper. Inspired by interlayer feature similarity in overparameterized models, we investigate the intrinsic link between network sparsity and interlayer feature similarity. Specifically, we prove that reducing interlayer feature similarity based on Centered Kernel Alignment (CKA) improves the sparsity of the network by using information bottleneck theory. Applying such theory, we propose a plug-and-play CKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR, which utilizes CKA to reduce feature similarity between layers and increase n
    
[^24]: 通过量子计算实现高阶拓扑核

    Higher-order topological kernels via quantum computation. (arXiv:2307.07383v1 [quant-ph])

    [http://arxiv.org/abs/2307.07383](http://arxiv.org/abs/2307.07383)

    通过量子计算，我们提出了一种定义拓扑核的新方法，基于构建随着阶数增加的拓扑指纹，可以在多项式时间内近似计算高维Betti数

    

    拓扑数据分析（TDA）已成为从复杂数据中提取有意义的见解的有力工具。TDA通过将对象嵌入到单纯复合中，并提取有用的全局属性，例如Betti数（即多维空洞的数量），从而增强了对对象的分析，这可以用于定义与现有机器学习算法轻松集成的内核方法。这些内核方法已经找到了广泛的应用，因为它们依赖于提供其性能的理论保证的强大数学框架。然而，经典硬件上计算高维Betti数可能会变得非常昂贵，而量子算法可以在多项式时间内近似计算它们。在这项工作中，我们提出了一种基于构建Betti曲线的量子方法来定义拓扑核，即随着阶数增加的拓扑指纹。我们展示了一个工作原型

    Topological data analysis (TDA) has emerged as a powerful tool for extracting meaningful insights from complex data. TDA enhances the analysis of objects by embedding them into a simplicial complex and extracting useful global properties such as the Betti numbers, i.e. the number of multidimensional holes, which can be used to define kernel methods that are easily integrated with existing machine-learning algorithms. These kernel methods have found broad applications, as they rely on powerful mathematical frameworks which provide theoretical guarantees on their performance. However, the computation of higher-dimensional Betti numbers can be prohibitively expensive on classical hardware, while quantum algorithms can approximate them in polynomial time in the instance size. In this work, we propose a quantum approach to defining topological kernels, which is based on constructing Betti curves, i.e. topological fingerprint of filtrations with increasing order. We exhibit a working prototy
    
[^25]: 句子嵌入的构成对比学习

    Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])

    [http://arxiv.org/abs/2307.07380](http://arxiv.org/abs/2307.07380)

    该论文提出了一种句子嵌入的构成对比学习方法，通过最大化文本和其词组成分的对齐，实现了从无标签数据中学习文本表示的目标，并在语义文本相似性任务上取得了与最先进方法可比较的改进，而无需额外的训练目标或网络参数。

    

    自然语言的向量表示在搜索应用中非常普遍。最近，提出了基于对比学习的各种方法，用于从无标签数据中学习文本表示；通过最大化相同文本的微小扰动嵌入之间的对齐，并鼓励嵌入在更广泛的语料库中的均匀分布。与此不同的是，我们提出了最大化文本和其词组成分的对齐。我们考虑了这一目标的几个实现方式，并详细说明了每种情况下对表示的影响。在语义文本相似性任务上的实验结果显示，与最先进的方法相比，我们的方法在提高基线水平方面有所改进。此外，这项工作是第一个在不产生辅助训练目标或额外网络参数成本的情况下取得这样的改进。

    Vector representations of natural language are ubiquitous in search applications. Recently, various methods based on contrastive learning have been proposed to learn textual representations from unlabelled data; by maximizing alignment between minimally-perturbed embeddings of the same text, and encouraging a uniform distribution of embeddings across a broader corpus. Differently, we propose maximizing alignment between texts and a composition of their phrasal constituents. We consider several realizations of this objective and elaborate the impact on representations in each case. Experimental results on semantic textual similarity tasks show improvements over baselines that are comparable with state-of-the-art approaches. Moreover, this work is the first to do so without incurring costs in auxiliary training objectives or additional network parameters.
    
[^26]: 使用基于卷积神经网络的视觉处理在增材制造中进行缺陷分类

    Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing. (arXiv:2307.07378v1 [cs.CV])

    [http://arxiv.org/abs/2307.07378](http://arxiv.org/abs/2307.07378)

    本文研究了使用基于卷积神经网络的视觉处理来准确分类增材制造图像数据集中的缺陷，并应用主动学习技术进行优化，以构建一个人在循环机制，以减少培训和生成训练数据所需的数据量。

    

    计算机视觉和原位监控利用视觉传感器的发展允许从增材制造过程中收集大量数据集。这样的数据集可以与机器学习技术结合使用，以改善增材制造的质量。本文研究了两种情景：首先，使用卷积神经网络（CNN）准确分类增材制造图像数据集中的缺陷；其次，应用主动学习技术对所开发的分类模型进行优化。这将构建一个人在循环机制，以减少培训和生成训练数据所需的数据量。

    The development of computer vision and in-situ monitoring using visual sensors allows the collection of large datasets from the additive manufacturing (AM) process. Such datasets could be used with machine learning techniques to improve the quality of AM. This paper examines two scenarios: first, using convolutional neural networks (CNNs) to accurately classify defects in an image dataset from AM and second, applying active learning techniques to the developed classification model. This allows the construction of a human-in-the-loop mechanism to reduce the size of the data required to train and generate training data.
    
[^27]: AIC-AB NET：一种具有空间注意力和文本属性的图像字幕神经网络

    AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes. (arXiv:2307.07370v1 [cs.CV])

    [http://arxiv.org/abs/2307.07370](http://arxiv.org/abs/2307.07370)

    一种新型的图像字幕神经网络AIC-AB NET将空间注意力和文本属性相结合，通过自适应空间注意力和输入文本属性信息，提升图像字幕的性能和减少不确定性。

    

    图像字幕在计算机视觉和自然语言处理领域中具有重要意义。我们提出并展示了一种新型的属性-信息-组合注意力网络AIC-AB NET，该网络将空间注意力架构和文本属性结合在编码器-解码器中。对于字幕生成，自适应空间注意力确定最能代表图像的图像区域，并决定是侧重于视觉特征还是视觉标记。文本属性信息同步输入解码器，以帮助图像识别并减少不确定性。我们在MS COCO数据集和一个新提出的时尚数据集上进行了对AICAB NET的测试和评估。该时尚数据集被用作单物体图像的基准。结果表明，相对于现有的基线模型和去除了部分结构的模型，我们提出的模型在MSCOCO图像和我们的单物体图像上表现出了优越的性能。我们的AIC-AB NET优于基线自适应注意力网络。

    Image captioning is a significant field across computer vision and natural language processing. We propose and present AIC-AB NET, a novel Attribute-Information-Combined Attention-Based Network that combines spatial attention architecture and text attributes in an encoder-decoder. For caption generation, adaptive spatial attention determines which image region best represents the image and whether to attend to the visual features or the visual sentinel. Text attribute information is synchronously fed into the decoder to help image recognition and reduce uncertainty. We have tested and evaluated our AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The Fashion dataset is employed as a benchmark of single-object images. The results show the superior performance of the proposed model compared to the state-of-the-art baseline and ablated models on both the images from MSCOCO and our single-object images. Our AIC-AB NET outperforms the baseline adaptive attention network 
    
[^28]: 一种基于测试的方法来评估分类数据的聚类性

    A testing-based approach to assess the clusterability of categorical data. (arXiv:2307.07346v1 [cs.LG])

    [http://arxiv.org/abs/2307.07346](http://arxiv.org/abs/2307.07346)

    这项研究提出了一种基于测试的方法，用于评估分类数据的聚类性能。通过计算属性对的卡方统计量的和得到一个解析的p值作为评估指标。结果表明这种方法在基准分类数据集上的表现优于其他方法。

    

    聚类性评估的目标是检查数据集中是否存在聚类结构。作为聚类分析中一个至关重要但常常被忽视的问题，我们在应用任何聚类算法之前都需要进行这样的检验。如果一个数据集不可聚类，则任何后续的聚类分析都不会产生有效结果。尽管它的重要性，但现有研究中的大部分关注数值数据，将对分类数据的聚类性评估问题留给成为一个未解决的问题。在这里，我们介绍了TestCat，一种基于测试的方法来评估分类数据的聚类性，使用解析的p值作为评估指标。TestCat的关键思想是，可聚类的分类数据具有许多强相关的属性对，因此将所有属性对的卡方统计量的和作为p值计算的测试统计量。我们将我们的方法应用于一组基准分类数据集，结果表明TestCat的表现优于现有的其他方法。

    The objective of clusterability evaluation is to check whether a clustering structure exists within the data set. As a crucial yet often-overlooked issue in cluster analysis, it is essential to conduct such a test before applying any clustering algorithm. If a data set is unclusterable, any subsequent clustering analysis would not yield valid results. Despite its importance, the majority of existing studies focus on numerical data, leaving the clusterability evaluation issue for categorical data as an open problem. Here we present TestCat, a testing-based approach to assess the clusterability of categorical data in terms of an analytical $p$-value. The key idea underlying TestCat is that clusterable categorical data possess many strongly correlated attribute pairs and hence the sum of chi-squared statistics of all attribute pairs is employed as the test statistic for $p$-value calculation. We apply our method to a set of benchmark categorical data sets, showing that TestCat outperforms
    
[^29]: 逆进化层:物理信息化正则化器用于深度神经网络

    Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks. (arXiv:2307.07344v1 [cs.LG])

    [http://arxiv.org/abs/2307.07344](http://arxiv.org/abs/2307.07344)

    本文提出了一种新颖的方法，通过一种新的正则化方法将基于偏微分方程（PDE）的进化模型与神经网络集成在一起。这些层可以实现特定的正则化目标，并赋予神经网络输出与进化模型对应的特性。此外，逆进化层的构建和实现相对简单，可以轻松地为不同的物理进化和神经网络设计。

    

    本文提出了一种新颖的方法，通过一种新的正则化方法将基于偏微分方程（PDE）的进化模型与神经网络集成在一起。具体而言，我们提出了基于进化方程的逆进化层（IELs）。这些层可以实现特定的正则化目标，并赋予神经网络输出与进化模型对应的特性。此外，逆进化层的构建和实现相对简单，可以轻松地为不同的物理进化和神经网络设计。此外，这些层的设计过程可以为神经网络提供直观和数学可解释性，从而增强了方法的透明度和解释性。为了证明我们方法的有效性、效率和简单性，我们提出了一个将语义分割模型赋予热扩散模型平滑性属性的示例。

    This paper proposes a novel approach to integrating partial differential equation (PDE)-based evolution models into neural networks through a new type of regularization. Specifically, we propose inverse evolution layers (IELs) based on evolution equations. These layers can achieve specific regularization objectives and endow neural networks' outputs with corresponding properties of the evolution models. Moreover, IELs are straightforward to construct and implement, and can be easily designed for various physical evolutions and neural networks. Additionally, the design process for these layers can provide neural networks with intuitive and mathematical interpretability, thus enhancing the transparency and explainability of the approach. To demonstrate the effectiveness, efficiency, and simplicity of our approach, we present an example of endowing semantic segmentation models with the smoothness property based on the heat diffusion model. To achieve this goal, we design heat-diffusion IE
    
[^30]: MaxMin-L2-SVC-NCH:一种用于训练支持向量分类器并选择模型参数的新方法

    MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters. (arXiv:2307.07343v1 [cs.LG])

    [http://arxiv.org/abs/2307.07343](http://arxiv.org/abs/2307.07343)

    本文提出了一种新的方法，用于训练支持向量分类器并选择模型参数。通过建模为极小化极大优化问题，利用投影梯度算法求解，实现了更低的时间复杂度。

    

    模型参数的选择在支持向量分类（SVC）的应用中发挥着重要作用。常用的选择模型参数的方法是k折交叉验证与格点搜索（CV）。由于需要训练大量的SVC模型，这个方法非常耗时。本文提出了一种新方法，用于训练SVC并选择模型参数。首先，将具有模型参数选择的SVC训练建模为极小化极大优化问题（MaxMin-L2-SVC-NCH），其中极小化问题是寻找两个正常凸壳之间最接近点的优化问题（L2-SVC-NCH），而极大化问题是寻找最优模型参数的优化问题。MaxMin-L2-SVC-NCH具有较低的时间复杂度，因为放弃了CV。然后，提出了一种基于梯度的算法来求解MaxMin-L2-SVC-NCH，其中L2-SVC-NCH通过投影梯度算法求解。

    The selection of model's parameters plays an important role in the application of support vector classification (SVC). The commonly used method of selecting model's parameters is the k-fold cross validation with grid search (CV). It is extremely time-consuming because it needs to train a large number of SVC models. In this paper, a new method is proposed to train SVC with the selection of model's parameters. Firstly, training SVC with the selection of model's parameters is modeled as a minimax optimization problem (MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization problem of finding the closest points between two normal convex hulls (L2-SVC-NCH) while the maximization problem is an optimization problem of finding the optimal model's parameters. A lower time complexity can be expected in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a projected gradient algorithm 
    
[^31]: 跨语言的刻板偏见有何不同？

    How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])

    [http://arxiv.org/abs/2307.07331](http://arxiv.org/abs/2307.07331)

    本研究拓展了评估预训练语言模型中刻板偏见的研究，通过跨语言分析发现mGPT-2在不同语言中显示出令人惊讶的反刻板行为，并且英语模型表现出最强的偏见，而土耳其语则最不明显。

    

    最近的研究展示了如何评估预训练的英语语言模型中的刻板偏见。在本研究中，我们通过系统地调查(a)多语言模型和单语模型、(b)不同基础架构下的刻板偏见、(c)多种语言中的偏见，扩展了该研究领域的多个方面。为了实现这一目标，我们利用英语的StereoSet数据集将其半自动翻译成德语、法语、西班牙语和土耳其语。我们发现，在多语言环境下进行这种类型的分析非常重要，因为我们的实验展示了一个更为细致的画面，以及与仅英语分析有显著差异的发现。我们的分析主要得出以下结论：mGPT-2（在某种程度上）在不同语言中显示出令人惊讶的反刻板行为，英语（单语）模型表现出最强的偏见，并且数据集中反映的刻板印象在土耳其语中最不明显。

    Recent studies have demonstrated how to assess the stereotypical bias in pre-trained English language models. In this work, we extend this branch of research in multiple different dimensions by systematically investigating (a) mono- and multilingual models of (b) different underlying architectures with respect to their bias in (c) multiple different languages. To that end, we make use of the English StereoSet data set (Nadeem et al., 2021), which we semi-automatically translate into German, French, Spanish, and Turkish. We find that it is of major importance to conduct this type of analysis in a multilingual setting, as our experiments show a much more nuanced picture as well as notable differences from the English-only analysis. The main takeaways from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical behavior across languages, English (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data set are least present in Turkish mod
    
[^32]: 用可学习的污染样本选择策略增强后门攻击

    Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (arXiv:2307.07328v1 [cs.CR])

    [http://arxiv.org/abs/2307.07328](http://arxiv.org/abs/2307.07328)

    该论文介绍了一种增强后门攻击的方法，通过引入可学习的污染样本选择策略来有效地选择污染样本。这解决了后门攻击中要从整个数据集中选择污染样本的问题。

    

    基于数据污染的后门攻击旨在通过操纵训练数据集来将后门插入模型中，而不控制目标模型的训练过程。现有的攻击方法主要集中于设计触发器或触发器与良性样本之间的融合策略。然而，它们往往随机选择要污染的样本，忽视了每个污染样本在后门注入方面的不同重要性。最近的选择策略通过记录遗忘事件来过滤大小固定的污染样本池，但它未能从全局角度考虑池外的剩余样本。此外，计算遗忘事件需要额外的计算资源。因此，如何从整个数据集中高效有效地选择污染样本是后门攻击中的一个迫切问题。为了解决这个问题，首先我们在常规的后门训练损失中引入了一个污染掩码。

    Data-poisoning based backdoor attacks aim to insert backdoor into models by manipulating training datasets without controlling the training process of the target model. Existing attack methods mainly focus on designing triggers or fusion strategies between triggers and benign samples. However, they often randomly select samples to be poisoned, disregarding the varying importance of each poisoning sample in terms of backdoor injection. A recent selection strategy filters a fixed-size poisoning sample pool by recording forgetting events, but it fails to consider the remaining samples outside the pool from a global perspective. Moreover, computing forgetting events requires significant additional computing resources. Therefore, how to efficiently and effectively select poisoning samples from the entire dataset is an urgent problem in backdoor attacks.To address it, firstly, we introduce a poisoning mask into the regular backdoor training loss. We suppose that a backdoored model training w
    
[^33]: 无资源语音应用的隐藏单元聚类表示学习

    Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])

    [http://arxiv.org/abs/2307.07325](http://arxiv.org/abs/2307.07325)

    本文提出了一种使用隐藏单元聚类框架进行自监督表示学习的方法，用于无文本资源下的语音表示学习。模型使用卷积神经网络和长短期记忆层对音频样本进行处理并生成上下文向量表示，通过隐藏单元聚类框架将这些表示归类为少量类似音素的单元，用于学习语义丰富的语音表示。研究结果表明这种方法可以提高模型性能。

    

    无文本资源下的语音表示学习是许多低资源语音应用中一个非常感兴趣的领域。本文介绍了一种使用隐藏单元聚类（HUC）框架从原始音频中进行自监督表示学习的方法。模型的输入包括被窗口化并经过1-D卷积层处理的音频样本。从卷积神经网络（CNN）模块学习到的"时频"表示经过长短期记忆（LSTM）层进一步处理，为每个窗口化片段生成上下文向量表示。使用隐藏单元聚类框架对这些表示进行分类，将其归类为少量类似音素的单元，用于训练模型学习语义丰富的语音表示。目标包括每个音频片段的类似音素伪标签，这些标签是使用迭代k-means算法生成的。我们探索了一些可以提高模型性能的技术。

    The representation learning of speech, without textual resources, is an area of significant interest for many low resource speech applications. In this paper, we describe an approach to self-supervised representation learning from raw audio using a hidden unit clustering (HUC) framework. The input to the model consists of audio samples that are windowed and processed with 1-D convolutional layers. The learned "time-frequency" representations from the convolutional neural network (CNN) module are further processed with long short term memory (LSTM) layers which generate a contextual vector representation for every windowed segment. The HUC framework, allowing the categorization of the representations into a small number of phoneme-like units, is used to train the model for learning semantically rich speech representations. The targets consist of phoneme-like pseudo labels for each audio segment and these are generated with an iterative k-means algorithm. We explore techniques that impro
    
[^34]: 一个上下文感知的混合整数规划切割平面选择算法

    A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming. (arXiv:2307.07322v1 [math.OC])

    [http://arxiv.org/abs/2307.07322](http://arxiv.org/abs/2307.07322)

    本文提出了一个上下文感知的混合整数规划切割平面选择算法，通过引入新的评分指标、过滤技术和停止准则，使得SCIP在MIPLIB 2017基准集上的性能提升了4%。

    

    目前在混合整数规划求解器中使用的切割平面选择算法自其创建以来基本保持不变。本文提出了一组新的切割评分指标、切割过滤技术和停止准则，扩展了当前最先进的算法，并在MIPLIB 2017基准集上使SCIP的性能提高了4%。

    The current cut selection algorithm used in mixed-integer programming solvers has remained largely unchanged since its creation. In this paper, we propose a set of new cut scoring measures, cut filtering techniques, and stopping criteria, extending the current state-of-the-art algorithm and obtaining a 4\% performance improvement for SCIP over the MIPLIB 2017 benchmark set.
    
[^35]: 自适应线性估计方程

    Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])

    [http://arxiv.org/abs/2307.07320](http://arxiv.org/abs/2307.07320)

    本文提出了一种解决自适应线性回归模型中非正态渐近行为的方法，使用自适应线性估计方程构建去偏估计量，并在多臂老虎机的背景下保持了最小二乘估计量的非渐近性能。

    

    顺序数据收集已成为增强数据收集过程效率的广泛采用的技术。尽管具有优势，但这种数据收集机制常常给统计推断过程引入复杂性。例如，在自适应线性回归模型中，普通最小二乘（OLS）估计量可能表现出非正态的渐近行为，从而对准确的推断和解释提出挑战。本文提出了一种构建去偏估计量的通用方法，该方法采用自适应线性估计方程的思想，并在理论上保证了渐近正态性，并讨论了实现近似最优渐近方差的问题。我们的估计量的一个显著特点是，在多臂老虎机的背景下，我们的估计量保留了最小二乘估计量的非渐近性能，同时获得了渐近正态性。因此，本工作解决了自适应线性回归模型中非正态渐近行为的问题，并为统计推断提供了可靠的方法。

    Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work
    
[^36]: 新闻编辑室中的混合审核：向内容管理员推荐特色帖子

    Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])

    [http://arxiv.org/abs/2307.07317](http://arxiv.org/abs/2307.07317)

    本文提出了一种在新闻编辑室中使用的混合审核方法，该方法通过向内容管理员推荐特色帖子来支持他们在选择特色内容方面做出决策。该方法基于概率排序的推荐系统，结合了用户和文本内容特征，取得了较高的分类和排序性能。内容管理员在评估中发现了合适的评论，并在很大程度上接受了推荐结果。

    

    在线新闻媒体正努力处理评论区用户生成内容的审核问题。我们提出了一种基于概率排序的推荐系统来支持和授权审核员选择特色帖子，这是一项耗时的任务。通过结合用户和文本内容的特征，我们获得了测试集上的最佳分类F1分数为0.44。此外，我们在大量验证文章上观察到了均值NDCG@5的最佳值为0.87。在专家评估中，内容管理员根据推荐结果选择要推荐的评论，得到了0.83的NDCG分数。我们得出的结论是，首先，添加文本特征可以获得最佳得分；其次，虽然选择特色内容仍然有一定的主观性，但内容管理员在所有被评估的推荐中都找到了合适的评论，除了一个例外。最后，我们通过分析表现最佳的模型，迈向透明和可解释性。

    Online news outlets are grappling with the moderation of user-generated content within their comment section. We present a recommender system based on ranking class probabilities to support and empower the moderator in choosing featured posts, a time-consuming task. By combining user and textual content features we obtain an optimal classification F1-score of 0.44 on the test set. Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of validation articles. As an expert evaluation, content moderators assessed the output of a random selection of articles by choosing comments to feature based on the recommendations, which resulted in a NDCG score of 0.83. We conclude that first, adding text features yields the best score and second, while choosing featured content remains somewhat subjective, content moderators found suitable comments in all but one evaluated recommendations. We end the paper by analyzing our best-performing model, a step towards transparency and explaina
    
[^37]: HEAL-SWIN: 球面上的视觉变压器

    HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])

    [http://arxiv.org/abs/2307.07313](http://arxiv.org/abs/2307.07313)

    HEAL-SWIN是一个基于HEALPix网格和SWIN变压器结合的球面视觉变压器，在处理高分辨率广角鱼眼图像时能够实现无失真且高效的训练，并在语义分割任务中展现出优异的性能。

    

    高分辨率广角鱼眼图像在自动驾驶等机器人应用中变得越来越重要。然而，使用普通的卷积神经网络或视觉变压器处理此类数据存在问题，因为将其投影到平面上的矩形网格时会引入投影和失真损失。我们引入了HEAL-SWIN变压器，它将在天体物理学和宇宙学中使用的高度均匀的层次等面积等纬度像素化（HEALPix）网格与层次移位窗口（SWIN）变压器结合起来，从而得到能够训练高分辨率无失真球面数据的高效灵活模型。在HEAL-SWIN中，HEALPix网格的嵌套结构用于执行SWIN变压器的拼接和窗口操作，从而得到具有最小计算开销的球面数据一维表示。我们展示了我们模型在语义分割上的卓越性能。

    High-resolution wide-angle fisheye images are becoming more and more important for robotics applications such as autonomous driving. However, using ordinary convolutional neural networks or vision transformers on this data is problematic due to projection and distortion losses introduced when projecting to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer, which combines the highly uniform Hierarchical Equal Area iso-Latitude Pixelation (HEALPix) grid used in astrophysics and cosmology with the Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and flexible model capable of training on high-resolution, distortion-free spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used to perform the patching and windowing operations of the SWIN transformer, resulting in a one-dimensional representation of the spherical data with minimal computational overhead. We demonstrate the superior performance of our model for semantic segmentati
    
[^38]: 使用物理约束神经网络解决高阶Lane-Emden-Fowler类型方程：基准测试比较软约束和硬约束

    Solving higher-order Lane-Emden-Fowler type equations using physics-informed neural networks: benchmark tests comparing soft and hard constraints. (arXiv:2307.07302v1 [physics.comp-ph])

    [http://arxiv.org/abs/2307.07302](http://arxiv.org/abs/2307.07302)

    本文使用物理约束神经网络（PINNs）成功解决了高阶Lane-Emden-Fowler类型方程，通过比较软约束和硬约束的变体，得出了有效的数值方法。

    

    本文提出了使用物理约束神经网络（PINNs）解决高阶常微分方程（ODEs）的数值方法。事实上，这种深度学习技术成功地应用于解决不同类别的奇异ODEs，即着名的二阶Lane-Emden方程、三阶Emden-Fowler方程和四阶Lane-Emden-Fowler方程。对PINNs技术的两个变体进行了考虑和比较。首先，使用最小化过程来约束神经网络的总损失函数，在其中将方程残差考虑为具有一定权重的基于物理的损失，并添加到包含初始/边界条件的训练数据损失中。其次，为了满足微分方程，采用了一种特定的试验解选择，确保这些条件成为硬约束，与第一个变体基于训练数据的约束不同，约束以软约束形式出现。

    In this paper, numerical methods using Physics-Informed Neural Networks (PINNs) are presented with the aim to solve higher-order ordinary differential equations (ODEs). Indeed, this deep-learning technique is successfully applied for solving different classes of singular ODEs, namely the well known second-order Lane-Emden equations, third order-order Emden-Fowler equations, and fourth-order Lane-Emden-Fowler equations. Two variants of PINNs technique are considered and compared. First, a minimization procedure is used to constrain the total loss function of the neural network, in which the equation residual is considered with some weight to form a physics-based loss and added to the training data loss that contains the initial/boundary conditions. Second, a specific choice of trial solutions ensuring these conditions as hard constraints is done in order to satisfy the differential equation, contrary to the first variant based on training data where the constraints appear as soft ones. 
    
[^39]: 基于点云分类网络的基于3D形状的心肌梗死预测

    3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks. (arXiv:2307.07298v1 [cs.CV])

    [http://arxiv.org/abs/2307.07298](http://arxiv.org/abs/2307.07298)

    本文研究了使用完整的3D心脏形状（以点云形式）在改善心肌梗死（MI）事件检测方面的实用性。通过使用点云分类网络进行多步骤的自动化流程，结合点云几何深度学习技术，我们在英国生物银行受试者上得到了约13%和约5%的改进效果。

    

    心肌梗死（MI）是最常见的心血管疾病之一，相关的临床决策通常基于单值成像生物标志物。然而，这样的指标只是近似了心脏的复杂3D结构和生理特性，从而阻碍了对MI结果的更好理解和预测。在这项工作中，我们研究了完整的3D心脏形状（以点云形式）在改善MI事件检测方面的实用性。为此，我们提出了一个完全自动化的多步骤流程，包括3D心脏表面重建步骤和点云分类网络。我们的方法利用了点云几何深度学习在高分辨率心脏解剖模型上的直接和高效多尺度学习。我们在1068个英国生物银行受试者上评估了我们的方法，用于常见MI检测和新发MI预测的任务，并发现分别改进了约13%和约5%。

    Myocardial infarction (MI) is one of the most prevalent cardiovascular diseases with associated clinical decision-making typically based on single-valued imaging biomarkers. However, such metrics only approximate the complex 3D structure and physiology of the heart and hence hinder a better understanding and prediction of MI outcomes. In this work, we investigate the utility of complete 3D cardiac shapes in the form of point clouds for an improved detection of MI events. To this end, we propose a fully automatic multi-step pipeline consisting of a 3D cardiac surface reconstruction step followed by a point cloud classification network. Our method utilizes recent advances in geometric deep learning on point clouds to enable direct and efficient multi-scale learning on high-resolution surface models of the cardiac anatomy. We evaluate our approach on 1068 UK Biobank subjects for the tasks of prevalent MI detection and incident MI prediction and find improvements of ~13% and ~5% respective
    
[^40]: 自主环境中基于前沿探索的强化学习

    Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment. (arXiv:2307.07296v1 [cs.RO])

    [http://arxiv.org/abs/2307.07296](http://arxiv.org/abs/2307.07296)

    通过将已有的Visual-Graph SLAM算法ExploreORB与强化学习相结合，该研究提出了一种自主环境中基于前沿探索的强化学习算法。该算法通过基于奖励的系统学习和优化探索路线，解决了在存在多个距离相似的前沿的场景中可能导致非最优路径的问题。英文总结: This paper proposes a reinforcement learning algorithm with frontier-based exploration in autonomous environments by combining an existing Visual-Graph SLAM algorithm, ExploreORB. The algorithm allows the robot to learn and optimize exploration routes through a reward-based system to select frontiers accurately, addressing the issue of non-optimal paths in scenarios with similar-distance frontiers.

    

    主动同时定位及建图（SLAM）是自主机器人中的一个重要问题，使机器人能够在建立准确的环境模型的同时导航到新的区域。视觉SLAM是一种流行的技术，利用虚拟元素来增强体验。然而，现有的基于前沿探索策略在存在多个距离相似的前沿的场景中可能导致非最优的路径。这个问题可能会影响到视觉SLAM的效率和准确性，而这对于一系列机器人应用非常关键，如搜救、探索和建图。为了解决这个问题，本研究将已有的ExploreORB视觉图SLAM与强化学习相结合。所提出的算法允许机器人通过基于奖励的系统学习和优化探索路线，以正确选择前沿，并创建一个精确的环境地图。基于前沿探索被用来检测未探索区域。

    Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in autonomous robotics, enabling robots to navigate to new regions while building an accurate model of their surroundings. Visual SLAM is a popular technique that uses virtual elements to enhance the experience. However, existing frontier-based exploration strategies can lead to a non-optimal path in scenarios where there are multiple frontiers with similar distance. This issue can impact the efficiency and accuracy of Visual SLAM, which is crucial for a wide range of robotic applications, such as search and rescue, exploration, and mapping. To address this issue, this research combines both an existing Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed algorithm allows the robot to learn and optimize exploration routes through a reward-based system to create an accurate map of the environment with proper frontier selection. Frontier-based exploration is used to detect unexplored area
    
[^41]: 频域对抗训练用于稳健的体积医学分割

    Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v1 [eess.IV])

    [http://arxiv.org/abs/2307.07269](http://arxiv.org/abs/2307.07269)

    这个论文提出了一种用于稳健的体积医学图像分割模型的频域对抗训练方法，并发现其相对于传统攻击方法具有优势。该方法利用频域对抗攻击，通过引入频率一致性损失来优化模型的性能，实现对体素和频域攻击的防御。

    

    在关键应用（如医疗保健）中，确保深度学习模型的稳健性至关重要。尽管最近在体积医学图像分割模型方面取得了进展，但由于其易受对抗攻击的弱点，这些模型无法立即用于实际应用。我们提出了一种用于体积医学图像分割模型的三维频域对抗攻击，并展示了其相对于传统输入或体素域攻击的优势。利用我们提出的攻击，我们引入了一种新颖的频域对抗训练方法，以优化针对体素和频域攻击的稳健模型。此外，我们提出了频率一致性损失来调节我们的频域对抗训练，以在清洁样品和对抗样本之间取得更好的平衡。代码可公开访问https://github.com/asif-hanif/vafa。

    It is imperative to ensure the robustness of deep learning models in critical applications such as, healthcare. While recent advances in deep learning have improved the performance of volumetric medical image segmentation models, these models cannot be deployed for real-world applications immediately due to their vulnerability to adversarial attacks. We present a 3D frequency domain adversarial attack for volumetric medical image segmentation models and demonstrate its advantages over conventional input or voxel domain attacks. Using our proposed attack, we introduce a novel frequency domain adversarial training approach for optimizing a robust model against voxel and frequency domain attacks. Moreover, we propose frequency consistency loss to regulate our frequency domain adversarial training that achieves a better tradeoff between model's performance on clean and adversarial samples. Code is publicly available at https://github.com/asif-hanif/vafa.
    
[^42]: 关于插值专家和多臂赌博机的研究

    On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])

    [http://arxiv.org/abs/2307.07264](http://arxiv.org/abs/2307.07264)

    学习专家建议和多臂赌博是两个经典的在线决策问题，我们研究了两者之间的插值问题。我们提出了$\mathbf{m}$-MAB的极小后悔界并设计了$\mathbf{m}$-BAI的最优PAC算法，该算法旨在以尽可能少的轮数确定损失最小的臂。

    

    学习专家建议和多臂赌博是两个经典的在线决策问题，它们在每一轮观察信息的方式上有所不同。我们研究了这两者之间的插值问题。对于向量$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$，$\mathbf{m}$-MAB的一个实例表示将臂分成$K$组，第$i$组包含$m_i$个臂。一旦拉动一个臂，同一组中所有臂的损失都被观察到。我们证明了$\mathbf{m}$-MAB的紧致极小后悔界，并为其纯探索版本$\mathbf{m}$-BAI设计了一个最优的PAC算法，其中目标是用尽可能少的轮数来识别损失最小的臂。我们证明了$\mathbf{m}$-MAB的极小后悔是$\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$，对于一个$(\epsilon,0.05)$-PAC算法的$\mathbf{m}$-BAI，拉动臂的最小次数是$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$。

    Learning with expert advice and multi-armed bandit are two classic online decision problems which differ on how the information is observed in each round of the game. We study a family of problems interpolating the two. For a vector $\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB indicates that the arms are partitioned into $K$ groups and the $i$-th group contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB and design an optimal PAC algorithm for its pure exploration version, $\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is $\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Bot
    
[^43]: 通过敌对双机器学习的因果参数估计来缓解敌对脆弱性

    Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])

    [http://arxiv.org/abs/2307.07250](http://arxiv.org/abs/2307.07250)

    通过敌对双机器学习方法，可以量化和缓解深度神经网络在面对敌对输入时的脆弱性。

    

    从经过精心设计的视觉输入中衍生出的敌对例子可以轻松地损害深度神经网络的决策过程。为了防止潜在的威胁，各种基于敌对训练的防御方法迅速增长，并成为稳健性的事实上标准方法。尽管最近取得了竞争性的成就，我们观察到敌对脆弱性在不同目标之间存在差异，并且某些脆弱性仍然普遍存在。有趣的是，即使使用更深层次的架构和先进的防御方法，这种奇特的现象仍然无法缓解。为了解决这个问题，本文介绍了一种称为敌对双机器学习（ADML）的因果方法，它允许我们量化网络预测的敌对脆弱性程度，并捕捉对结果的处理效果。ADML可以直接估计敌对扰动本身的因果参数，并减轻可能损害稳健性的负面效应。

    Adversarial examples derived from deliberately crafted perturbations on visual inputs can easily harm decision process of deep neural networks. To prevent potential threats, various adversarial training-based defense methods have grown rapidly and become a de facto standard approach for robustness. Despite recent competitive achievements, we observe that adversarial vulnerability varies across targets and certain vulnerabilities remain prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with deeper architectures and advanced defense methods. To address this issue, in this paper, we introduce a causal approach called Adversarial Double Machine Learning (ADML), which allows us to quantify the degree of adversarial vulnerability for network predictions and capture the effect of treatments on outcome of interests. ADML can directly estimate causal parameter of adversarial perturbations per se and mitigate negative effects that can potentially damage robustness, bridgi
    
[^44]: 知识增强：重新思考医学对比视觉语言预训练

    Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])

    [http://arxiv.org/abs/2307.07246](http://arxiv.org/abs/2307.07246)

    该论文提出了一个名为KoBo的框架，它通过将临床知识整合到视觉语言语义一致性学习中，解决了医学领域中的大规模语义重叠和转移问题。

    

    基于预训练技术的基础模型从理论上到实践应用显著推进了人工智能的发展。这些模型推动了计算机辅助诊断的可行性，使其广泛应用。医学对比视觉语言预训练是一种有效的方法，它利用诊断报告中的描述信息来指导表征学习，无需人工注释。然而，预训练的效果受制于医学领域的大规模语义重叠和转移问题。为了解决这些问题，我们提出了知识增强对比视觉语言预训练框架（KoBo），该框架将临床知识整合到视觉语言语义一致性学习中。该框架使用无偏的、开集样本级知识表示来衡量负样本噪声，并补充视觉语言互信息与临床知识之间的对应关系。

    The foundation models based on pre-training technology have significantly advanced artificial intelligence from theoretical to practical applications. These models have facilitated the feasibility of computer-aided diagnosis for widespread use. Medical contrastive vision-language pre-training, which does not require human annotations, is an effective approach for guiding representation learning using description information in diagnostic reports. However, the effectiveness of pre-training is limited by the large-scale semantic overlap and shifting problems in medical field. To address these issues, we propose the Knowledge-Boosting Contrastive Vision-Language Pre-training framework (KoBo), which integrates clinical knowledge into the learning of vision-language semantic consistency. The framework uses an unbiased, open-set sample-wise knowledge representation to measure negative sample noise and supplement the correspondence between vision-language mutual information and clinical knowl
    
[^45]: Ed-Fed：一种通用的边缘设备资源感知客户端选择的联邦学习框架

    Ed-Fed: A generic federated learning framework with resource-aware client selection for edge devices. (arXiv:2307.07199v1 [cs.DC])

    [http://arxiv.org/abs/2307.07199](http://arxiv.org/abs/2307.07199)

    Ed-Fed是一个通用的边缘设备资源感知客户端选择的联邦学习框架，可以优化等待时间，处理慢速设备，并且在FL中显著优化了等待时间。

    

    联邦学习已经成为边缘设备合作创建统一预测模型并保护其敏感训练数据的重要方法。尽管存在许多用于模拟联邦学习算法的研究框架，但它们不能在异构边缘设备上全面部署自动语音识别任务。因此，Ed-Fed作为一个综合且通用的联邦学习框架成为未来实际FL系统研究的基础。我们还提出一种新颖的资源感知客户端选择算法，在FL环境中优化等待时间。我们证明我们的方法可以处理慢速设备，并动态设置选定设备的训练时间。我们的评估表明，与传统的随机客户端选择方法相比，所提出的方法显著优化了FL中的等待时间。

    Federated learning (FL) has evolved as a prominent method for edge devices to cooperatively create a unified prediction model while securing their sensitive training data local to the device. Despite the existence of numerous research frameworks for simulating FL algorithms, they do not facilitate comprehensive deployment for automatic speech recognition tasks on heterogeneous edge devices. This is where Ed-Fed, a comprehensive and generic FL framework, comes in as a foundation for future practical FL system research. We also propose a novel resource-aware client selection algorithm to optimise the waiting time in the FL settings. We show that our approach can handle the straggler devices and dynamically set the training time for the selected devices in a round. Our evaluation has shown that the proposed approach significantly optimises waiting time in FL compared to conventional random client selection methods.
    
[^46]: 用机器学习控制动力系统到复杂的目标状态：下一代与传统储备计算的比较

    Controlling dynamical systems to complex target states using machine learning: next-generation vs. classical reservoir computing. (arXiv:2307.07195v1 [cs.LG])

    [http://arxiv.org/abs/2307.07195](http://arxiv.org/abs/2307.07195)

    用机器学习控制动力系统到复杂的目标状态，比较了下一代储备计算和传统储备计算。研究发现，在数据受限的情况下，下一代储备计算表现优于传统方法，为实际控制应用提供了更多可能性。

    

    使用机器学习来控制非线性动力系统不仅可以使系统进入简单的周期行为，还可以实现更复杂的任意动态。为了实现这一点，关键在于训练一个机器学习系统能够很好地重现目标动力学。以将洛伦兹系统的混沌参数化驱动为例，我们首先展示了传统储备计算在这个任务上的出色表现。然后，我们比较了使用不同数量的训练数据的基于下一代储备计算的结果与另一种设置的结果。结果表明，尽管在通常的训练数据量上表现相当，但在只有非常有限的数据可用的情况下，下一代储备计算表现显著优于传统方法。这为在数据受限的实际控制应用中进一步提供了可能性。

    Controlling nonlinear dynamical systems using machine learning allows to not only drive systems into simple behavior like periodicity but also to more complex arbitrary dynamics. For this, it is crucial that a machine learning system can be trained to reproduce the target dynamics sufficiently well. On the example of forcing a chaotic parametrization of the Lorenz system into intermittent dynamics, we show first that classical reservoir computing excels at this task. In a next step, we compare those results based on different amounts of training data to an alternative setup, where next-generation reservoir computing is used instead. It turns out that while delivering comparable performance for usual amounts of training data, next-generation RC significantly outperforms in situations where only very limited data is available. This opens even further practical control applications in real world problems where data is restricted.
    
[^47]: 用于电力负荷预测的基准和自定义包

    Benchmarks and Custom Package for Electrical Load Forecasting. (arXiv:2307.07191v1 [cs.LG])

    [http://arxiv.org/abs/2307.07191](http://arxiv.org/abs/2307.07191)

    本文提供了一个全面的电力负荷预测存档，包括负荷领域特定的特征工程，帮助模型更好地模拟负荷数据，并提供了一种新的损失函数来最小化后续任务的成本。

    

    负荷预测在电力行业中具有重要意义，可以为后续任务如电网调度提供参考，从而带来巨大的经济效益。然而，负荷预测与传统的时间序列预测之间存在许多差异。一方面，负荷预测的目标是最小化后续任务（如电网调度）的成本，而不仅仅追求预测准确性。另一方面，负荷受到许多外部因素的影响，如温度或日历变量。此外，预测的规模（如建筑级负荷和聚合级负荷）也会对预测结果产生重大影响。在本文中，我们提供了一个全面的负荷预测存档，其中包括负荷领域特定的特征工程，以帮助预测模型更好地模拟负荷数据。此外，与传统的损失函数仅追求准确性不同，我们还提供了一种方法来...

    Load forecasting is of great significance in the power industry as it can provide a reference for subsequent tasks such as power grid dispatch, thus bringing huge economic benefits. However, there are many differences between load forecasting and traditional time series forecasting. On the one hand, load forecasting aims to minimize the cost of subsequent tasks such as power grid dispatch, rather than simply pursuing prediction accuracy. On the other hand, the load is largely influenced by many external factors, such as temperature or calendar variables. In addition, the scale of predictions (such as building-level loads and aggregated-level loads) can also significantly impact the predicted results. In this paper, we provide a comprehensive load forecasting archive, which includes load domain-specific feature engineering to help forecasting models better model load data. In addition, different from the traditional loss function which only aims for accuracy, we also provide a method to
    
[^48]: 用于加速深度学习训练和提高鲁棒性的乘法更新规则

    Multiplicative update rules for accelerating deep learning training and increasing robustness. (arXiv:2307.07189v1 [cs.LG])

    [http://arxiv.org/abs/2307.07189](http://arxiv.org/abs/2307.07189)

    本文提出了一种优化框架，通过使用乘法更新规则，加速深度学习训练并提高模型的鲁棒性。

    

    尽管深度学习在许多研究领域已经达到了最先进的性能，但仍然加速训练和构建鲁棒的深度学习模型仍然是一个具有挑战性的任务。本文提出了一种优化框架，适用于各种优化算法，并使得可以应用替代的参数更新方法，从而加快训练和增强模型的鲁棒性。

    Even nowadays, where Deep Learning (DL) has achieved state-of-the-art performance in a wide range of research domains, accelerating training and building robust DL models remains a challenging task. To this end, generations of researchers have pursued to develop robust methods for training DL architectures that can be less sensitive to weight distributions, model architectures and loss landscapes. However, such methods are limited to adaptive learning rate optimizers, initialization schemes, and clipping gradients without investigating the fundamental rule of parameters update. Although multiplicative updates have contributed significantly to the early development of machine learning and hold strong theoretical claims, to best of our knowledge, this is the first work that investigate them in context of DL training acceleration and robustness. In this work, we propose an optimization framework that fits to a wide range of optimization algorithms and enables one to apply alternative upda
    
[^49]: DISPEL：通过领域特定解放进行域泛化

    DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])

    [http://arxiv.org/abs/2307.07181](http://arxiv.org/abs/2307.07181)

    DISPEL是一种通过后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征的领域泛化方法。

    

    领域泛化旨在通过仅在有限的源领域上进行训练，学习一个能够在未知测试领域上表现良好的泛化模型。然而，现有的领域泛化方法往往引入与预测无关的噪声或需要收集领域标签来解决。为了应对这些挑战，我们从不同的角度考虑了领域泛化问题，将底层特征组划分为领域共享和领域特定特征。然而，领域特定特征很难从输入数据中进行识别和区分。在这项工作中，我们提出了一种名为DISPEL（DomaIn-SPEcific Liberating）的后处理细粒度掩蔽方法，能够在嵌入空间中过滤掉未定义和无法区分的领域特定特征。具体而言，DISPEL利用一个掩蔽生成器为每个输入数据生成一个唯一的掩蔽来过滤领域特定特征。DISPEL框架非常灵活，可以应用于任何细粒度的掩蔽任务和领域泛化方法。

    Domain generalization aims to learn a generalization model that can perform well on unseen test domains by only training on limited source domains. However, existing domain generalization approaches often bring in prediction-irrelevant noise or require the collection of domain labels. To address these challenges, we consider the domain generalization problem from a different perspective by categorizing underlying feature groups into domain-shared and domain-specific features. Nevertheless, the domain-specific features are difficult to be identified and distinguished from the input data. In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing fine-grained masking approach that can filter out undefined and indistinguishable domain-specific features in the embedding space. Specifically, DISPEL utilizes a mask generator that produces a unique mask for each input data to filter domain-specific features. The DISPEL framework is highly flexible to be applied to any fin
    
[^50]: 一个用于有限区域动力系统估计的代理数据同化模型

    A Surrogate Data Assimilation Model for the Estimation of Dynamical System in a Limited Area. (arXiv:2307.07178v1 [math.NA])

    [http://arxiv.org/abs/2307.07178](http://arxiv.org/abs/2307.07178)

    该论文提出了一个基于学习的代理数据同化模型，用于在有限区域实现高效的状态估计。该模型利用神经网络进行在线计算，避免了整合高维有限区域模型的需求，并且相比传统算法具有较大的计算优势。同时，该模型基于可观测性和有效区域的概念设计，能够确定所需的最佳观测数据量，并减轻与计算可观测性和生成训练数据相关的计算负担。

    

    我们提出了一种新颖的基于学习的代理数据同化（DA）模型，用于在有限区域进行高效状态估计。我们的模型采用前馈神经网络进行在线计算，消除了整合高维有限区域模型的需求。这种方法相对传统的DA算法具有显著的计算优势。此外，我们的方法在在线和离线计算中避免了对有限区域模型的横向边界条件的需求。我们的代理DA模型的设计建立在一个坚实的理论框架之上，利用了两个基本概念：可观测性和有效区域。可观测性的概念使我们能够定量确定准确的DA所需的最佳观测数据量。与此同时，有效区域的概念大大减轻了与计算可观测性和生成训练数据相关的计算负担。

    We propose a novel learning-based surrogate data assimilation (DA) model for efficient state estimation in a limited area. Our model employs a feedforward neural network for online computation, eliminating the need for integrating high-dimensional limited-area models. This approach offers significant computational advantages over traditional DA algorithms. Furthermore, our method avoids the requirement of lateral boundary conditions for the limited-area model in both online and offline computations. The design of our surrogate DA model is built upon a robust theoretical framework that leverages two fundamental concepts: observability and effective region. The concept of observability enables us to quantitatively determine the optimal amount of observation data necessary for accurate DA. Meanwhile, the concept of effective region substantially reduces the computational burden associated with computing observability and generating training data.
    
[^51]: Safe DreamerV3：带有世界模型的安全强化学习

    Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])

    [http://arxiv.org/abs/2307.07176](http://arxiv.org/abs/2307.07176)

    Safe DreamerV3是一种通过集成基于拉格朗日和计划的方法到世界模型中的新算法，实现了在低维度和仅采用视觉的任务中几乎零成本的安全强化学习。

    

    强化学习在真实世界场景中的广泛应用还没有实现, 这主要是因为其未能满足这些系统的基本安全需求。现有的安全强化学习方法使用成本函数来增强安全性，在复杂场景中，包括仅采用视觉的任务中，即使进行全面的数据采样和训练，也无法实现零成本。为了解决这个问题，我们引入了Safe DreamerV3，这是一种将基于拉格朗日和计划的方法集成到世界模型中的新算法。我们的方法论在SafeRL中代表了一个重要的进步，是第一个在Safety-Gymnasium基准中实现近乎零成本的算法。我们的项目网站可以在以下链接找到：https://sites.google.com/view/safedreamerv3。

    The widespread application of Reinforcement Learning (RL) in real-world situations is yet to come to fruition, largely as a result of its failure to satisfy the essential safety demands of such systems. Existing safe reinforcement learning (SafeRL) methods, employing cost functions to enhance safety, fail to achieve zero-cost in complex scenarios, including vision-only tasks, even with comprehensive data sampling and training. To address this, we introduce Safe DreamerV3, a novel algorithm that integrates both Lagrangian-based and planning-based methods within a world model. Our methodology represents a significant advancement in SafeRL as the first algorithm to achieve nearly zero-cost in both low-dimensional and vision-only tasks within the Safety-Gymnasium benchmark. Our project website can be found in: https://sites.google.com/view/safedreamerv3.
    
[^52]: FedBIAD: 具有贝叶斯推断自适应丢弃的通信高效和准确性保证的联邦学习

    FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout. (arXiv:2307.07172v1 [cs.DC])

    [http://arxiv.org/abs/2307.07172](http://arxiv.org/abs/2307.07172)

    本文提出了一种基于贝叶斯推断和自适应丢弃的联邦学习方法（FedBIAD），通过将局部模型的权重行视为概率分布，并根据与局部训练损失趋势相关的重要度指标自适应地丢弃部分权重行，解决了联邦学习中的通信效率和性能保证问题。

    

    联邦学习（FL）是一种分布式机器学习范式，避免了隐私泄露问题，无需将最终用户数据传输。参与FL的设备通常带宽受限，且无线网络中上行速度远慢于下行速度，导致上行通信瓶颈严重。缓解这一问题的一个重要方向是联邦丢弃，即丢弃局部模型的部分权重。然而，现有的联邦丢弃研究集中在随机或有序丢弃上，并缺乏理论支持，导致性能无法保证。本文提出了一种基于贝叶斯推断和自适应丢弃的联邦学习方法（FedBIAD），该方法将局部模型的权重行视为概率分布，并根据与局部训练损失趋势相关的重要度指标自适应地丢弃部分权重行。通过应用FedBIAD，每个客户端可以自适应地选择高质量的丢弃模式，从而获得准确的论文摘要。

    Federated Learning (FL) emerges as a distributed machine learning paradigm without end-user data transmission, effectively avoiding privacy leakage. Participating devices in FL are usually bandwidth-constrained, and the uplink is much slower than the downlink in wireless networks, which causes a severe uplink communication bottleneck. A prominent direction to alleviate this problem is federated dropout, which drops fractional weights of local models. However, existing federated dropout studies focus on random or ordered dropout and lack theoretical support, resulting in unguaranteed performance. In this paper, we propose Federated learning with Bayesian Inference-based Adaptive Dropout (FedBIAD), which regards weight rows of local models as probability distributions and adaptively drops partial weight rows based on importance indicators correlated with the trend of local training loss. By applying FedBIAD, each client adaptively selects a high-quality dropping pattern with accurate app
    
[^53]: 带有自我净化的大型语言模型的认证鲁棒性

    Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])

    [http://arxiv.org/abs/2307.07171](http://arxiv.org/abs/2307.07171)

    传统的随机平滑方法对于大型语言模型的直接应用具有挑战性，为了解决认证半径很小的问题，提出了一种带有自我净化的新方法。

    

    尽管大型语言模型（LLM）在广泛的实际应用中取得了巨大的成功，但它们对于噪声输入的脆弱性显著限制了它们的应用，尤其是在高风险环境中。在这些环境下，确保大型语言模型的每个预测都是稳定的非常重要，即在输入的微小差异情况下，LLM的预测应该是一致的。这主要涉及到认证鲁棒LLM的研究，即在输入周围的局部区域中，所有LLM的预测都得到认证是正确的。随机平滑已经展示了在认证LLM的鲁棒性和预测稳定性方面的巨大潜力。然而，随机平滑在进行模型预测之前需要对输入添加噪声，其认证性能在很大程度上取决于模型在受损数据上的表现。因此，它直接应用于LLM仍然具有挑战性，并且通常会导致很小的认证半径。为了解决这个问题，

    Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue,
    
[^54]: 对抗训练中的易受攻击性实例重新加权

    Vulnerability-Aware Instance Reweighting For Adversarial Training. (arXiv:2307.07167v1 [cs.LG])

    [http://arxiv.org/abs/2307.07167](http://arxiv.org/abs/2307.07167)

    该论文提出了一种新的实例级别重新加权方案，通过考虑每个自然示例的易受攻击性和对抗攻击导致的信息损失，来改进对抗训练算法的性能。

    

    对抗训练一直被发现能大大提高深度学习分类器对抗性攻击的鲁棒性。对抗训练通过在训练分类器时加入对抗性样本来获得鲁棒性。大多数对抗训练算法的变体将每个训练样本平等对待。然而，最近的研究表明通过将它们不平等地对待可以获得更好的性能。此外，对抗训练对训练集中的不同类别产生了不均衡的影响，并不公平地损害了与本质上更难分类的类别相对应的示例。因此，已经提出了各种重新加权方案，为训练集中各个示例的鲁棒损失分配不平等的权重。在这项工作中，我们提出了一种新的实例级别重新加权方案。它考虑了每个自然示例的易受攻击性和由于对抗攻击而导致的其对手示例上的信息损失。通过广泛的实验验证

    Adversarial Training (AT) has been found to substantially improve the robustness of deep learning classifiers against adversarial attacks. AT involves obtaining robustness by including adversarial examples in training a classifier. Most variants of AT algorithms treat every training example equally. However, recent works have shown that better performance is achievable by treating them unequally. In addition, it has been observed that AT exerts an uneven influence on different classes in a training set and unfairly hurts examples corresponding to classes that are inherently harder to classify. Consequently, various reweighting schemes have been proposed that assign unequal weights to robust losses of individual examples in a training set. In this work, we propose a novel instance-wise reweighting scheme. It considers the vulnerability of each natural example and the resulting information loss on its adversarial counterpart occasioned by adversarial attacks. Through extensive experiment
    
[^55]: 不要随机遮盖：通过遮盖领域内关键词进行有效的领域自适应预训练

    Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])

    [http://arxiv.org/abs/2307.07160](http://arxiv.org/abs/2307.07160)

    本研究提出了一种通过遮盖领域内关键词进行的领域自适应预训练方法，实验结果表明该方法优于使用随机遮盖的领域内预训练和常见的预训练然后微调范式。

    

    我们提出了一种新颖的领域无关的领域内预训练方法，介于通用预训练和微调之间。我们的方法选择性地遮盖领域内的关键词，即提供目标领域的紧凑表示的单词。我们使用KeyBERT (Grootendorst, 2020)来识别这些关键词。我们使用六种不同的设置对我们的方法进行评估：三个数据集与两个不同的预训练语言模型（PLMs）相结合。我们的结果表明，使用我们的领域内预训练策略微调的PLMs优于使用随机遮盖的领域内预训练的PLMs，并且优于遵循常见的预训练然后微调范式的PLMs。此外，识别领域内关键词的开销是合理的，例如，对于BERT Large (Devlin et al., 2019)来说，是预训练时间的7-15%（两个epoch）。

    We propose a novel task-agnostic in-domain pre-training method that sits between generic pre-training and fine-tuning. Our approach selectively masks in-domain keywords, i.e., words that provide a compact representation of the target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We evaluate our approach using six different settings: three datasets combined with two distinct pre-trained language models (PLMs). Our results reveal that the fine-tuned PLMs adapted using our in-domain pre-training strategy outperform PLMs that used in-domain pre-training with random masking as well as those that followed the common pre-train-then-fine-tune paradigm. Further, the overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).
    
[^56]: 机器学习算法的多维能力诊断

    Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms. (arXiv:2307.07134v1 [cs.LG])

    [http://arxiv.org/abs/2307.07134](http://arxiv.org/abs/2307.07134)

    本文提出了一个任务无关的评估框架Camilla，通过定义多维度的诊断度量Ability来协同测量每个机器学习算法的多面体强度，解决了机器学习算法在实际性能和标准化评估中存在的差距问题。

    

    机器学习算法已经在许多应用中变得无处不在（例如图像分类）。然而，由于传统指标（例如每个分类器的粗粒度准确度）的测量不足，通常在这些算法的实际性能和标准化评估中存在实质性差距。在本文中，受到人类测量中的心理测量理论的启发，我们提出了一个任务无关的评估框架Camilla，其中定义了一个多维诊断度量Ability来协同测量每个机器学习算法的多面体强度。具体来说，给定不同算法对数据样本的响应日志，我们利用认知诊断假设和神经网络来学习算法、样本以及每个样本的技能（显式或隐式预定义）之间的复杂交互。通过这种方式，可以同时评估每个算法在多个技能上的表现能力。

    Machine learning algorithms have become ubiquitous in a number of applications (e.g. image classification). However, due to the insufficient measurement of traditional metrics (e.g. the coarse-grained Accuracy of each classifier), substantial gaps are usually observed between the real-world performance of these algorithms and their scores in standardized evaluations. In this paper, inspired by the psychometric theories from human measurement, we propose a task-agnostic evaluation framework Camilla, where a multi-dimensional diagnostic metric Ability is defined for collaboratively measuring the multifaceted strength of each machine learning algorithm. Specifically, given the response logs from different algorithms to data samples, we leverage cognitive diagnosis assumptions and neural networks to learn the complex interactions among algorithms, samples and the skills (explicitly or implicitly pre-defined) of each sample. In this way, both the abilities of each algorithm on multiple skil
    
[^57]: DataAssist:一种用于数据清洗和准备的机器学习方法

    DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])

    [http://arxiv.org/abs/2307.07119](http://arxiv.org/abs/2307.07119)

    DataAssist是一种机器学习方法，用于提高数据集质量和节省数据清洗和准备时间。

    

    目前的自动化机器学习工具主要关注于模型选择和参数优化，忽略了数据清洗和整理所占据的大部分时间。本文介绍了一种名为DataAssist的自动化数据准备和清洗平台，利用机器学习方法提高数据集质量。我们展示了DataAssist提供了一个用于探索性数据分析和数据清洗的管道，包括为用户选择的变量生成可视化，统一数据注释，提供异常值删除建议以及对数据进行预处理。导出的数据集可以轻松与其他自动化机器学习工具或用户指定的模型进行整合，以进行后续分析。我们的数据中心化工具适用于多个领域，包括经济学、商业和预测应用，可节省超过50\%的数据清理和准备时间。

    Current automated machine learning (ML) tools are model-centric, focusing on model selection and parameter optimization. However, the majority of the time in data analysis is devoted to data cleaning and wrangling, for which limited tools are available. Here we present DataAssist, an automated data preparation and cleaning platform that enhances dataset quality using ML-informed methods. We show that DataAssist provides a pipeline for exploratory data analysis and data cleaning, including generating visualization for user-selected variables, unifying data annotation, suggesting anomaly removal, and preprocessing data. The exported dataset can be readily integrated with other autoML tools or user-specified model for downstream analysis. Our data-centric tool is applicable to a variety of fields, including economics, business, and forecasting applications saving over 50\% time of the time spent on data cleansing and preparation.
    
[^58]: 分散随机双正则化非凸强凸极小极大问题的方差减少加速方法

    Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems. (arXiv:2307.07113v1 [math.OC])

    [http://arxiv.org/abs/2307.07113](http://arxiv.org/abs/2307.07113)

    本文提出了一种应用于分散随机双正则化非凸强凸极小极大问题的方差减少加速方法，通过引入拉格朗日乘子和采用单个邻居通信并结合方差减少技术，该方法在随机设置下样本复杂度达到$\mathcal{O}(\kappa^3\varepsilon^{-3})$。

    

    本文考虑在原始变量和对偶变量上具有非光滑正则化项的分散、随机、非凸强凸（NCSC）极小极大问题，在该问题中，m个计算代理通过点对点通信进行协作。我们考虑了耦合函数为期望或有限和形式的情况，并且双正则化函数分别应用于原始变量和对偶变量。我们的算法框架引入了一个拉格朗日乘子来消除对偶变量上的共识约束。将此与方差减少（VR）技术相结合，我们提出的方法，称为VRLM，通过每次迭代进行一次邻居通信，能够在一般的随机设置下实现$\mathcal{O}(\kappa^3\varepsilon^{-3})$ 的样本复杂度，其中$\kappa$是问题的条件数，$\varepsilon$是希望的解精度。通过使用大批量VR，

    In this paper, we consider the decentralized, stochastic nonconvex strongly-concave (NCSC) minimax problem with nonsmooth regularization terms on both primal and dual variables, wherein a network of $m$ computing agents collaborate via peer-to-peer communications. We consider when the coupling function is in expectation or finite-sum form and the double regularizers are convex functions, applied separately to the primal and dual variables. Our algorithmic framework introduces a Lagrangian multiplier to eliminate the consensus constraint on the dual variable. Coupling this with variance-reduction (VR) techniques, our proposed method, entitled VRLM, by a single neighbor communication per iteration, is able to achieve an $\mathcal{O}(\kappa^3\varepsilon^{-3})$ sample complexity under the general stochastic setting, with either a big-batch or small-batch VR option, where $\kappa$ is the condition number of the problem and $\varepsilon$ is the desired solution accuracy. With a big-batch VR,
    
[^59]: 图位置和结构编码器

    Graph Positional and Structural Encoder. (arXiv:2307.07107v1 [cs.LG])

    [http://arxiv.org/abs/2307.07107](http://arxiv.org/abs/2307.07107)

    这是一个关于图位置和结构编码器的研究，提出了图位置和结构编码器（GPSE），它能有效地捕捉多个PSE的共同潜在表示，并在各种图预测任务中取得显著的性能提升。

    

    位置和结构编码（PSE）可以更好地在图中识别节点，因为一般图缺乏规范的节点顺序。这使得PSE成为赋予现代图神经网络（GNN）和特别是图变换器重要功能的工具。然而，设计适用于各种图预测任务的PSE是一个具有挑战性且未解决的问题。在这里，我们提出了图位置和结构编码器（GPSE），这是首次尝试训练一个能够捕捉丰富的PSE表示以增强任何GNN的图编码器。GPSE可以有效地学习多个PSE的共同潜在表示，并且具有高度可传输性。在特定图数据集上训练的编码器可以在从显著不同分布甚至模态的数据集上有效地使用。我们显示，在广泛的基准测试中，经过GPSE增强的模型在某些任务中显著提高性能，同时与明确使用PSE的模型性能相当。

    Positional and structural encodings (PSE) enable better identifiability of nodes within a graph, as in general graphs lack a canonical node ordering. This renders PSEs essential tools for empowering modern GNNs, and in particular graph Transformers. However, designing PSEs that work optimally for a variety of graph prediction tasks is a challenging and unsolved problem. Here, we present the graph positional and structural encoder (GPSE), a first-ever attempt to train a graph encoder that captures rich PSE representations for augmenting any GNN. GPSE can effectively learn a common latent representation for multiple PSEs, and is highly transferable. The encoder trained on a particular graph dataset can be used effectively on datasets drawn from significantly different distributions and even modalities. We show that across a wide range of benchmarks, GPSE-enhanced models can significantly improve the performance in certain tasks, while performing on par with those that employ explicitly c
    
[^60]: MaxCorrMGNN: 一种用于广义多模态医疗数据融合的多图神经网络框架，用于预测结果

    MaxCorrMGNN: A Multi-Graph Neural Network Framework for Generalized Multimodal Fusion of Medical Data for Outcome Prediction. (arXiv:2307.07093v1 [cs.LG])

    [http://arxiv.org/abs/2307.07093](http://arxiv.org/abs/2307.07093)

    MaxCorrMGNN是一种用于广义多模态医疗数据融合的创新神经网络框架，通过MaxCorr embeddings建模患者内部和患者之间的非线性模态相关性，并通过多层图网络在任务中进行推理，有效预测结果。

    

    随着多模态电子健康记录的出现，结果的证据可能涵盖从临床到影像和基因组数据的多个模态。有效预测结果需要能够对患者内部和患者之间的模态特征进行细粒度和多方面的复杂交互建模的融合框架。我们开发了一种创新的融合方法，称为MaxCorr MGNN，通过Hirschfeld-Gebelein-Renyi最大相关性(MaxCorr)嵌入模型非线性模态相关性，在患者内部和患者之间建立一个保存模态和患者身份的多层图。然后，我们首次设计了一种用于任务感知推理的广义多层图神经网络(MGNN)，以端到端方式学习定义患者 - 模态图连接性和信息传递的参数。我们在肺结核结果预测任务上评估了我们的模型。

    With the emergence of multimodal electronic health records, the evidence for an outcome may be captured across multiple modalities ranging from clinical to imaging and genomic data. Predicting outcomes effectively requires fusion frameworks capable of modeling fine-grained and multi-faceted complex interactions between modality features within and across patients. We develop an innovative fusion approach called MaxCorr MGNN that models non-linear modality correlations within and across patients through Hirschfeld-Gebelein-Renyi maximal correlation (MaxCorr) embeddings, resulting in a multi-layered graph that preserves the identities of the modalities and patients. We then design, for the first time, a generalized multi-layered graph neural network (MGNN) for task-informed reasoning in multi-layered graphs, that learns the parameters defining patient-modality graph connectivity and message passing in an end-to-end fashion. We evaluate our model an outcome prediction task on a Tuberculos
    
[^61]: 选择模型和置换不变性

    Choice Models and Permutation Invariance. (arXiv:2307.07090v1 [econ.EM])

    [http://arxiv.org/abs/2307.07090](http://arxiv.org/abs/2307.07090)

    本文提出了选择模型和置换不变性的基本特征化方法，并展示了如何通过非参数估计器逼近选择函数，以及在实际应用中的灵活性和优越性。

    

    选择建模是许多经济学、运营和营销问题的核心。在本文中，我们提出了一种对选择函数进行基本特征化的方法，涵盖了各种现有的选择模型。我们展示了非参数估计器如神经网络如何能够轻松逼近此类函数，并克服在非参数估计选择函数中固有的维数灾难。通过大量模拟，我们证明了我们提出的函数可以以完全数据驱动的方式灵活捕捉潜在的消费者行为，并胜过传统的参数模型。因为需求设置常常具有内生特征，我们将我们的框架扩展到包括在内生特征下的估计。此外，我们还描述了一个形式推理程序，以构建关于价格弹性等感兴趣对象的有效置信区间。最后，为了评估我们估计器的实际适用性，我们利用了一个真实世界的实证数据集。

    Choice Modeling is at the core of many economics, operations, and marketing problems. In this paper, we propose a fundamental characterization of choice functions that encompasses a wide variety of extant choice models. We demonstrate how nonparametric estimators like neural nets can easily approximate such functionals and overcome the curse of dimensionality that is inherent in the non-parametric estimation of choice functions. We demonstrate through extensive simulations that our proposed functionals can flexibly capture underlying consumer behavior in a completely data-driven fashion and outperform traditional parametric models. As demand settings often exhibit endogenous features, we extend our framework to incorporate estimation under endogenous features. Further, we also describe a formal inference procedure to construct valid confidence intervals on objects of interest like price elasticity. Finally, to assess the practical applicability of our estimator, we utilize a real-world
    
[^62]: 安全强化学习作为Wasserstein变分推理：可解释性的形式方法

    Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])

    [http://arxiv.org/abs/2307.07084](http://arxiv.org/abs/2307.07084)

    本研究提出了一种新的自适应Wasserstein变分优化（AWaVO）方法，利用形式方法解决了顺序决策中的解释和透明性问题，并提供了奖励设计和策略收敛的概率解释。

    

    强化学习或最优控制可以为具有可变动态的顺序决策问题提供有效的推理。然而，在实际实施中，解释奖励函数和相应的最优策略一直是一个持久的挑战。因此，将顺序决策问题形式化为推理具有重要价值，因为概率推理原则上提供了多样且强大的数学工具来推断随机动态，同时提供了奖励设计和策略收敛的概率解释。在本研究中，我们提出了一种新颖的自适应Wasserstein变分优化（AWaVO）方法来解决这些顺序决策中的挑战。我们的方法利用形式方法来解释奖励设计，透明地训练收敛，以及对顺序决策的概率解释。为了证明实用性，我们展示了收敛训练并保证了收敛的训练。

    Reinforcement Learning or optimal control can provide effective reasoning for sequential decision-making problems with variable dynamics. Such reasoning in practical implementation, however, poses a persistent challenge in interpreting the reward function and corresponding optimal policy. Consequently, formalizing the sequential decision-making problems as inference has a considerable value, as probabilistic inference in principle offers diverse and powerful mathematical tools to infer the stochastic dynamics whilst suggesting a probabilistic interpretation of the reward design and policy convergence. In this study, we propose a novel Adaptive Wasserstein Variational Optimization (AWaVO) to tackle these challenges in sequential decision-making. Our approach utilizes formal methods to provide interpretations of reward design, transparency of training convergence, and probabilistic interpretation of sequential decisions. To demonstrate practicality, we show convergent training with guara
    
[^63]: 一种基于场景的功能测试方法提高DNN性能

    A Scenario-Based Functional Testing Approach to Improving DNN Performance. (arXiv:2307.07083v1 [cs.LG])

    [http://arxiv.org/abs/2307.07083](http://arxiv.org/abs/2307.07083)

    本文提出了一种基于场景的功能测试方法，通过迭代测试ML模型在不同场景下表现，并使用迁移学习技术进行治疗，以提高机器学习应用的性能。

    

    该论文提出了一种基于场景的功能测试方法，用于提高机器学习（ML）应用的性能。该方法是一个迭代过程，从测试ML模型在各种情境中的表现开始，以识别薄弱环节。随后，在疑似薄弱情景上进行进一步测试，并对模型在这些情景下的性能进行统计评估以确认诊断。一旦测试结果确认了薄弱情景的诊断，就会通过使用迁移学习技术对模型进行治疗，使用原始模型作为基础，并应用一组专门针对治疗情况的训练数据，以及从原始训练数据集中随机选择的一部分训练数据，以防止所谓的“灾难性遗忘”效应。最后，在治疗后，通过在治疗情景以及其他情境上进行测试，评估模型的表现和效果。

    This paper proposes a scenario-based functional testing approach for enhancing the performance of machine learning (ML) applications. The proposed method is an iterative process that starts with testing the ML model on various scenarios to identify areas of weakness. It follows by a further testing on the suspected weak scenarios and statistically evaluate the model's performance on the scenarios to confirm the diagnosis. Once the diagnosis of weak scenarios is confirmed by test results, the treatment of the model is performed by retraining the model using a transfer learning technique with the original model as the base and applying a set of training data specifically targeting the treated scenarios plus a subset of training data selected at random from the original train dataset to prevent the so-call catastrophic forgetting effect. Finally, after the treatment, the model is assessed and evaluated again by testing on the treated scenarios as well as other scenarios to check if the tr
    
[^64]: 核t分布随机邻域嵌入算法

    Kernel t-distributed stochastic neighbor embedding. (arXiv:2307.07081v1 [cs.LG])

    [http://arxiv.org/abs/2307.07081](http://arxiv.org/abs/2307.07081)

    本文介绍了一个核化版本的t-SNE算法，可以将高维数据映射到低维空间并保持数据点之间的非欧几里德度量下的距离。该算法具有改善性能和准确性的潜力，在涉及核方法的分类问题中展现出更清晰的聚类效果。

    

    本文介绍了t-SNE算法的核化版本，能够将高维数据映射到低维空间，同时保留数据点之间的非欧几里德度量下的成对距离。这可以通过在高维空间或两个空间中使用核技巧来实现，从而得到一个端到端的核化版本。t-SNE算法的核化版本可以提供关于数据点之间关系的新视角，可以在特定应用中改善性能和准确性，如涉及核方法的分类问题。通过对多个数据集的比较，展示了t-SNE和其核化版本的差异，显示出不同类别的数据点之间更清晰的聚类效果。

    This paper presents a kernelized version of the t-SNE algorithm, capable of mapping high-dimensional data to a low-dimensional space while preserving the pairwise distances between the data points in a non-Euclidean metric. This can be achieved using a kernel trick only in the high dimensional space or in both spaces, leading to an end-to-end kernelized version. The proposed kernelized version of the t-SNE algorithm can offer new views on the relationships between data points, which can improve performance and accuracy in particular applications, such as classification problems involving kernel methods. The differences between t-SNE and its kernelized version are illustrated for several datasets, showing a neater clustering of points belonging to different classes.
    
[^65]: 使用自监督深度学习的Rician似然损失进行定量MRI

    Rician likelihood loss for quantitative MRI using self-supervised deep learning. (arXiv:2307.07072v1 [cs.LG])

    [http://arxiv.org/abs/2307.07072](http://arxiv.org/abs/2307.07072)

    本研究提出了一种使用自监督深度学习进行定量MRI的方法，通过引入负对数Rician似然（NLR）损失函数，解决了低信噪比条件下参数估计偏差的问题。

    

    目的：以前使用自监督深度学习进行的定量MRI研究报告在低信噪比条件下存在参数估计偏差的问题。这些系统误差来自于网络训练中选择的均方误差（MSE）损失函数，该函数与Rician分布的MR幅度信号不兼容。为了解决这个问题，我们引入了负对数Rician似然（NLR）损失。方法：我们开发了一个数值稳定且准确的NLR损失的实现，用于估计表观扩散系数（ADC）模型和体素内不相干运动（IVIM）模型的定量参数。通过在一系列信噪比（5-30）下比较偏差、方差和均方根误差来评估参数估计的准确性、精度和总体误差，并与MSE损失进行比较。结果：使用NLR损失训练的网络在SNR降低时显示出比MSE更高的ADC和IVIM扩散系数估计准确性，而精度或总误差几乎没有损失。

    Purpose: Previous quantitative MR imaging studies using self-supervised deep learning have reported biased parameter estimates at low SNR. Such systematic errors arise from the choice of Mean Squared Error (MSE) loss function for network training, which is incompatible with Rician-distributed MR magnitude signals. To address this issue, we introduce the negative log Rician likelihood (NLR) loss. Methods: A numerically stable and accurate implementation of the NLR loss was developed to estimate quantitative parameters of the apparent diffusion coefficient (ADC) model and intra-voxel incoherent motion (IVIM) model. Parameter estimation accuracy, precision and overall error were evaluated in terms of bias, variance and root mean squared error and compared against the MSE loss over a range of SNRs (5 - 30). Results: Networks trained with NLR loss show higher estimation accuracy than MSE for the ADC and IVIM diffusion coefficients as SNR decreases, with minimal loss of precision or total er
    
[^66]: Proof of Training (PoT): 利用加密挖掘的算力进行分布式AI训练

    Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training. (arXiv:2307.07066v1 [cs.CR])

    [http://arxiv.org/abs/2307.07066](http://arxiv.org/abs/2307.07066)

    Proof of Training (PoT)是一种结合了AI和区块链技术的协议，利用实用性拜占庭错误容忍（PBFT）共识机制同步全局状态。该协议在任务吞吐量、系统鲁棒性和网络安全方面表现出相当的潜力。

    

    在人工智能（AI）与加密挖掘相融合的新趋势中，我们发现两个领域存在三个主要挑战。为了弥合这一差距，我们引入了一种名为Proof of Training (PoT)的协议，它结合了AI和区块链技术的优势。PoT协议利用实用性拜占庭错误容忍（PBFT）共识机制来同步全局状态。为了评估协议设计的性能，我们提出了一种采用PoT协议的分散训练网络（DTN）的实现。我们的结果表明，该协议在任务吞吐量、系统鲁棒性和网络安全方面具有相当的潜力。

    In the midst of the emerging trend of integrating artificial intelligence (AI) with crypto mining, we identify three major challenges that create a gap between these two fields. To bridge this gap, we introduce the proof-of-training (PoT) protocol, an approach that combines the strengths of both AI and blockchain technology. The PoT protocol utilizes the practical Byzantine fault tolerance (PBFT) consensus mechanism to synchronize global states. To evaluate the performance of the protocol design, we present an implementation of a decentralized training network (DTN) that adopts the PoT protocol. Our results indicate that the protocol exhibits considerable potential in terms of task throughput, system robustness, and network security.
    
[^67]: 使用解耦的语言预训练为视觉语言学习引入引导策略

    Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])

    [http://arxiv.org/abs/2307.07063](http://arxiv.org/abs/2307.07063)

    本文提出了一种新的方法，通过解耦语言预训练，集中在语言组件上，提供了优化应用大型语言模型的框架，有效改善了视觉语言学习的性能。

    

    本论文提出了一种新颖的方法，旨在优化冻结的大型语言模型（LLMs）在资源密集型视觉语言（VL）预训练中的应用。当前的范式使用视觉特征作为提示来引导语言模型，重点是确定与相应文本最相关的视觉特征。我们的方法不同，集中在语言组件上，具体是确定与视觉特征对齐的最佳提示。我们引入了Prompt-Transformer（P-Former），一种可以预测这些理想提示的模型，该模型仅在语言数据上进行训练，避免了图像-文本配对的需要。这种策略将端到端的VL训练过程巧妙地分为了额外的独立阶段。我们的实验证明，我们的框架显著提高了稳健的图像到文本基线（BLIP-2）的性能，并有效地缩小了使用4M或129M图像-文本对进行训练的模型之间的性能差距。

    We present a novel methodology aimed at optimizing the application of frozen large language models (LLMs) for resource-intensive vision-language (VL) pre-training. The current paradigm uses visual features as prompts to guide language models, with a focus on determining the most relevant visual features for corresponding text. Our approach diverges by concentrating on the language component, specifically identifying the optimal prompts to align with visual features. We introduce the Prompt-Transformer (P-Former), a model that predicts these ideal prompts, which is trained exclusively on linguistic data, bypassing the need for image-text pairings. This strategy subtly bifurcates the end-to-end VL training process into an additional, separate stage. Our experiments reveal that our framework significantly enhances the performance of a robust image-to-text baseline (BLIP-2), and effectively narrows the performance gap between models trained with either 4M or 129M image-text pairs. Importan
    
[^68]: 零数据下的可控重音文本转语音方法

    Controllable Emphasis with zero data for text-to-speech. (arXiv:2307.07062v1 [eess.AS])

    [http://arxiv.org/abs/2307.07062](http://arxiv.org/abs/2307.07062)

    我们提出了一种零数据的可扩展方法，用于在文本转语音中产生高质量的重音，不需要用到录音或标注的信息。我们通过增加重音词的预测持续时间来实现重音语音，相比于声谱图修改技术，这种方法在自然度和重音词识别方面都有显著的提高。该方法在不同语言、不同语音和多种语言风格下都被证明是可扩展且首选的。

    

    我们提出了一种可扩展的高质量文本转语音（TTS）的重音生成方法，不需要录音或注释。许多TTS模型包含一个音素持续时间模型。一种简单但有效的方法是增加重音词的预测持续时间来实现重音语音。我们证明这比声谱图修改技术显著好，自然性提高了7.3％，在一个参考美国英语女声中，受试者对句子中重音词的识别正确率提高了40％。我们证明这种方法显著缩小了与需要显式录音的方法之间的差距。该方法在四种不同语言（英语、西班牙语、意大利语、德语）、不同语音和多种语言风格下都被证明是可扩展且首选的。

    We present a scalable method to produce high quality emphasis for text-to-speech (TTS) that does not require recordings or annotations. Many TTS models include a phoneme duration model. A simple but effective method to achieve emphasized speech consists in increasing the predicted duration of the emphasised word. We show that this is significantly better than spectrogram modification techniques improving naturalness by $7.3\%$ and correct testers' identification of the emphasized word in a sentence by $40\%$ on a reference female en-US voice. We show that this technique significantly closes the gap to methods that require explicit recordings. The method proved to be scalable and preferred in all four languages tested (English, Spanish, Italian, German), for different voices and multiple speaking styles.
    
[^69]: 有奖导向的条件扩散：可证明的分布估计和奖励改进

    Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement. (arXiv:2307.07055v1 [cs.LG])

    [http://arxiv.org/abs/2307.07055](http://arxiv.org/abs/2307.07055)

    使用条件扩散模型进行有奖导向生成，能够有效学习和从奖励条件下的数据分布中进行采样，同时恢复数据的潜在子空间表示，并且生成新的群体靠近用户指定的目标奖励值。

    

    本文探讨了通过条件扩散模型进行有奖导向生成的方法和理论。有奖导向生成旨在生成具有由奖励函数衡量的所需特性的样本，广泛应用于生成式人工智能、强化学习和计算生物学领域。我们考虑了数据集包含未标记数据和一小部分具有带噪声奖励标签的数据的常见学习场景。我们的方法利用在较小数据集上学习到的奖励函数作为伪标签生成器。从理论上讲，我们展示了这个有导向的生成器能够有效地学习和从奖励条件下的数据分布中进行采样。此外，我们的模型能够恢复数据的潜在子空间表示。此外，我们还建立了模型生成一个新的群体，该群体靠近用户指定的目标奖励值，其中最优性差距与特征子空间中的离线策略强盗遗憾对齐。

    We explore the methodology and theory of reward-directed generation via conditional diffusion models. Directed generation aims to generate samples with desired properties as measured by a reward function, which has broad applications in generative AI, reinforcement learning, and computational biology. We consider the common learning scenario where the data set consists of unlabeled data along with a smaller set of data with noisy reward labels. Our approach leverages a learned reward function on the smaller data set as a pseudolabeler. From a theoretical standpoint, we show that this directed generator can effectively learn and sample from the reward-conditioned data distribution. Additionally, our model is capable of recovering the latent subspace representation of data. Moreover, we establish that the model generates a new population that moves closer to a user-specified target reward value, where the optimality gap aligns with the off-policy bandit regret in the feature subspace. Th
    
[^70]: 充分利用有限的上下文长度：预测能力随临床记录类型和记录部分的不同而变化

    Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])

    [http://arxiv.org/abs/2307.07051](http://arxiv.org/abs/2307.07051)

    通过分析临床记录的部分，我们发现预测能力在不同类型的记录间存在差异，并且当上下文长度较大时，组合不同类型的记录可以改善性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。

    

    最近大规模语言模型的进展使得使用临床记录的自由文本进行自然语言处理的兴趣重新燃起。临床记录的一个区别特点是它们跨越多个长文档的长时间跨度。临床记录的独特结构带来了一个新的设计选择：当语言模型预测器的上下文长度有限时，应选择临床记录的哪个部分作为输入？现有研究要么选择具有领域知识的输入，要么简单地截断它们。我们提出了一个分析高预测能力部分的框架。使用MIMIC-III数据集，我们展示了以下发现：1）预测能力分布在护理记录和出院记录之间是不同的；2）当上下文长度较大时，组合不同类型的记录可以提高性能。我们的研究结果表明，精心选择的采样函数可以使从临床记录中提取信息更加高效。

    Recent advances in large language models have led to renewed interest in natural language processing in healthcare using the free text of clinical notes. One distinguishing characteristic of clinical notes is their long time span over multiple long documents. The unique structure of clinical notes creates a new design choice: when the context length for a language model predictor is limited, which part of clinical notes should we choose as the input? Existing studies either choose the inputs with domain knowledge or simply truncate them. We propose a framework to analyze the sections with high predictive power. Using MIMIC-III, we show that: 1) predictive power distribution is different between nursing notes and discharge notes and 2) combining different types of notes could improve performance when the context length is large. Our findings suggest that a carefully selected sampling function could enable more efficient information extraction from clinical notes.
    
[^71]: Wasserstein量子蒙特卡洛：解决量子多体Schr\"odinger方程的一种新方法

    Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\"odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])

    [http://arxiv.org/abs/2307.07050](http://arxiv.org/abs/2307.07050)

    这篇论文提出了一种新的方法，即基于Wasserstein量子蒙特卡洛的方法，用于解决量子多体Schr\"odinger方程。该方法重新制定了能量泛函的最小化问题，并将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，同时利用深度学习方法来表示丰富的波函数族。

    

    解决量子多体Schr\"odinger方程是量子物理、量子化学和材料科学领域中一个基本而具有挑战性的问题。针对这个问题，一种常见的计算方法是量子变分蒙特卡洛（QVMC），其中通过在一个参数化波函数族中最小化系统的能量来获得基态解。深度学习方法在一定程度上解决了传统QVMC的局限性，通过使用神经网络表示丰富的波函数族。然而，在QVMC中优化目标仍然难以最小化，需要使用自然梯度等二阶优化方法。本文首先重新制定了能量泛函的最小化问题，将其转化为Born分布空间中的粒子排列（反）对称波函数的问题，而不是波函数的空间。然后我们将QVMC解释为在这个空间中的Fisher-Rao梯度流。

    Solving the quantum many-body Schr\"odinger equation is a fundamental and challenging problem in the fields of quantum physics, quantum chemistry, and material sciences. One of the common computational approaches to this problem is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are obtained by minimizing the energy of the system within a restricted family of parameterized wave functions. Deep learning methods partially address the limitations of traditional QVMC by representing a rich family of wave functions in terms of neural networks. However, the optimization objective in QVMC remains notoriously hard to minimize and requires second-order optimization methods such as natural gradient. In this paper, we first reformulate energy functional minimization in the space of Born distributions corresponding to particle-permutation (anti-)symmetric wave functions, rather than the space of wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
    
[^72]: AnyStar: 基于域随机化的通用星凸3D实例分割

    AnyStar: Domain randomized universal star-convex 3D instance segmentation. (arXiv:2307.07044v1 [cs.CV])

    [http://arxiv.org/abs/2307.07044](http://arxiv.org/abs/2307.07044)

    AnyStar是一种基于域随机化的通用星凸3D实例分割模型，通过使用合成数据进行训练，不需要来自未见过的数据集的注释图像即可准确分割星凸形状。

    

    星凸形状在生物显微镜和放射学中广泛存在，如细胞核、结节、转移等。针对这些结构的现有实例分割网络需要对每个数据集进行密集标注，这需要大量且常常是不切实际的手工注释工作。此外，由于对比度、形状、方向、分辨率和密度的变化，当面对新的数据集和成像模式时，仍需要进行大量的重工程或微调。我们提出了AnyStar，这是一个域随机化的生成模型，模拟了具有随机外观、环境和成像物理特性的仿真训练数据，用于训练通用的星凸形状实例分割网络。因此，使用我们的生成模型训练的网络不需要来自未见过的数据集的注释图像。在我们合成的数据上训练的单个网络准确地对C. elegans和P. dumerilii核进行3D分割。

    Star-convex shapes arise across bio-microscopy and radiology in the form of nuclei, nodules, metastases, and other units. Existing instance segmentation networks for such structures train on densely labeled instances for each dataset, which requires substantial and often impractical manual annotation effort. Further, significant reengineering or finetuning is needed when presented with new datasets and imaging modalities due to changes in contrast, shape, orientation, resolution, and density. We present AnyStar, a domain-randomized generative model that simulates synthetic training data of blob-like objects with randomized appearance, environments, and imaging physics to train general-purpose star-convex instance segmentation networks. As a result, networks trained using our generative model do not require annotated images from unseen datasets. A single network trained on our synthesized data accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence microscopy, mouse co
    
[^73]: 加速梯度方法用于非凸优化：逃逸轨迹和收敛到局部极小值点

    Accelerated gradient methods for nonconvex optimization: Escape trajectories from strict saddle points and convergence to local minima. (arXiv:2307.07030v1 [math.OC])

    [http://arxiv.org/abs/2307.07030](http://arxiv.org/abs/2307.07030)

    本文研究了一类加速梯度方法在非凸优化问题上的行为，包括逃离鞍点和收敛到局部极小值点的分析。研究在渐进和非渐进情况下，提出了一类新的Nesterov类型的加速方法，并回答了Nesterov加速梯度方法是否避免了严格鞍点的问题。

    

    本文研究了一类广义的加速梯度方法在光滑非凸函数上的行为。通过对Polyak的重球方法和Nesterov加速梯度方法进行改进，以实现对非凸函数局部极小值的收敛，本文提出了一类Nesterov类型的加速方法，并通过渐进分析和非渐进分析对这些方法进行了严格研究，包括逃离鞍点和收敛到局部极小值点。在渐进情况下，本文回答了一个开放问题，即带有可变动量参数的Nesterov加速梯度方法（NAG）是否几乎必定避免了严格鞍点。本文还提出了两种渐进收敛和发散的度量方式，并对几种常用的标准加速方法（如NAG和Ne）进行了评估。

    This paper considers the problem of understanding the behavior of a general class of accelerated gradient methods on smooth nonconvex functions. Motivated by some recent works that have proposed effective algorithms, based on Polyak's heavy ball method and the Nesterov accelerated gradient method, to achieve convergence to a local minimum of nonconvex functions, this work proposes a broad class of Nesterov-type accelerated methods and puts forth a rigorous study of these methods encompassing the escape from saddle-points and convergence to local minima through a both asymptotic and a non-asymptotic analysis. In the asymptotic regime, this paper answers an open question of whether Nesterov's accelerated gradient method (NAG) with variable momentum parameter avoids strict saddle points almost surely. This work also develops two metrics of asymptotic rate of convergence and divergence, and evaluates these two metrics for several popular standard accelerated methods such as the NAG, and Ne
    
[^74]: 利用分解的动作空间进行非策略评估

    Leveraging Factored Action Spaces for Off-Policy Evaluation. (arXiv:2307.07014v1 [cs.LG])

    [http://arxiv.org/abs/2307.07014](http://arxiv.org/abs/2307.07014)

    本文研究了如何利用分解的动作空间来减轻涉及大型组合动作空间问题的非策略评估（OPE）的高偏差和高方差问题。通过提出一种基于分解动作空间的新型“分解”重要性抽样（IS）估计器系列，我们证明了分解IS估计器具有比非分解版本更小的方差，同时具有零偏差的性质，并通过模拟实验证实了该理论结果。

    

    非策略评估（OPE）旨在估计根据执行序列收集的数据，遵循反事实的一系列动作的效益。然而，现有的OPE估计器在涉及大型组合动作空间的问题中经常表现出高偏差和高方差。我们研究如何利用分解的动作空间来减轻这个问题，即将每个动作表示为来自较小动作空间的独立子动作的组合。这种方法有助于对动作在其效果上的差异进行更精细的分析。在这项工作中，我们提出了一种基于分解动作空间的新型“分解”重要性抽样（IS）估计器系列。在对底层问题结构进行一定的假设的情况下，我们证明了分解IS估计器的方差比其原始非分解版本小，同时保持零偏差的性质。通过模拟，我们经验证实了我们的理论结果，检验了各种假设的有效性。

    Off-policy evaluation (OPE) aims to estimate the benefit of following a counterfactual sequence of actions, given data collected from executed sequences. However, existing OPE estimators often exhibit high bias and high variance in problems involving large, combinatorial action spaces. We investigate how to mitigate this issue using factored action spaces i.e. expressing each action as a combination of independent sub-actions from smaller action spaces. This approach facilitates a finer-grained analysis of how actions differ in their effects. In this work, we propose a new family of "decomposed" importance sampling (IS) estimators based on factored action spaces. Given certain assumptions on the underlying problem structure, we prove that the decomposed IS estimators have less variance than their original non-decomposed versions, while preserving the property of zero bias. Through simulations, we empirically verify our theoretical results, probing the validity of various assumptions. P
    
[^75]: 硅微环共振器对基于自由载流子非线性的蓄水容器计算的影响

    Impact of Free-carrier Nonlinearities on Silicon Microring-based Reservoir Computing. (arXiv:2307.07011v1 [cs.ET])

    [http://arxiv.org/abs/2307.07011](http://arxiv.org/abs/2307.07011)

    使用硅微环共振器进行的基于自由载流子非线性的蓄水容器计算，通过量化热光效应和自由载流子效应的影响，并确定了使NARMA-10任务中NMSE小于0.05的泵浦功率和频率失谐范围。

    

    我们量化了热光效应和自由载流子效应对使用硅微环谐振器的时延蓄水容器计算的影响。我们确定了泵浦功率和频率失谐范围，使得NARMA-10任务中NMSE小于0.05，这取决于两种考虑效应的时间常数。

    We quantify the impact of thermo-optic and free-carrier effects on time-delay reservoir computing using a silicon microring resonator. We identify pump power and frequency detuning ranges with NMSE less than 0.05 for the NARMA-10 task depending on the time constants of the two considered effects.
    
[^76]: 数学对象的数据增强

    Data Augmentation for Mathematical Objects. (arXiv:2307.06984v1 [cs.SC])

    [http://arxiv.org/abs/2307.06984](http://arxiv.org/abs/2307.06984)

    这项研究讨论并评估了在数学对象中的数据平衡和数据增强的思路，通过交换变量名称生成新的问题实例，提高了机器学习模型的准确性，并认为数据集平衡和增加数据集大小对于效果的提升都至关重要。

    

    本文讨论和评估了在数学对象的背景下数据平衡和数据增强的思想：这对于符号计算和可满足性检查社区来说是一个重要的主题，当他们使用机器学习技术来优化他们的工具时。我们考虑了一个非线性多项式问题的数据集，以及通过交换已标记问题中的变量名称来解决圆柱代数分解的变量排序问题。通过生成不需要进一步标记的新问题实例，我们发现这种增强平均提高了63％的ML模型准确性。我们研究了数据集平衡和进一步增加数据集大小对于此改进的影响，并得出结论，两者都有非常显著的效果。我们最后思考了这个想法如何可以扩展到其他领域。

    This paper discusses and evaluates ideas of data balancing and data augmentation in the context of mathematical objects: an important topic for both the symbolic computation and satisfiability checking communities, when they are making use of machine learning techniques to optimise their tools. We consider a dataset of non-linear polynomial problems and the problem of selecting a variable ordering for cylindrical algebraic decomposition to tackle these with. By swapping the variable names in already labelled problems, we generate new problem instances that do not require any further labelling when viewing the selection as a classification problem. We find this augmentation increases the accuracy of ML models by 63% on average. We study what part of this improvement is due to the balancing of the dataset and what is achieved thanks to further increasing the size of the dataset, concluding that both have a very significant effect. We finish the paper by reflecting on how this idea could 
    
[^77]: 用于选择人群结构健康监测中信息传输策略的决策框架

    A decision framework for selecting information-transfer strategies in population-based SHM. (arXiv:2307.06978v1 [cs.LG])

    [http://arxiv.org/abs/2307.06978](http://arxiv.org/abs/2307.06978)

    本文提出了一个决策框架，用于选择人群结构健康监测中的信息传输策略，通过避免负面传输和优化信息传输策略，可以降低成本和提高安全性。

    

    结构健康监测系统的开发和实施为结构的运维提供了重要支持，然而，标记训练数据的有限性制约了决策支持系统所依赖的统计模型的发展。人群结构健康监测通过使用迁移学习技术，在个体结构之间共享信息以减轻数据稀缺性带来的影响。本文提出了一个基于“信息传输期望价值”的决策框架，用于选择传输策略，并避免负面传输。通过避免负面传输，并使用传输决策框架优化信息传输策略，可以减少运营和维护结构所需的成本，并提高安全性。

    Decision-support for the operation and maintenance of structures provides significant motivation for the development and implementation of structural health monitoring (SHM) systems. Unfortunately, the limited availability of labelled training data hinders the development of the statistical models on which these decision-support systems rely. Population-based SHM seeks to mitigate the impact of data scarcity by using transfer learning techniques to share information between individual structures within a population. The current paper proposes a decision framework for selecting transfer strategies based upon a novel concept -- the expected value of information transfer -such that negative transfer is avoided. By avoiding negative transfer, and by optimising information transfer strategies using the transfer-decision framework, one can reduce the costs associated with operating and maintaining structures, and improve safety.
    
[^78]: 基于神经符号的强化去噪扩散概率模型在工业4.0中的实时异常检测

    Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v1 [cs.LG])

    [http://arxiv.org/abs/2307.06975](http://arxiv.org/abs/2307.06975)

    本论文提出了一种基于神经符号的强化去噪扩散概率模型，用于在工业4.0中实时预测异常。该方法集成了工业本体论，为智能制造提供了形式化知识，并且通过随机傅里叶特征提取扩散模型，实现了在嵌入式系统中直接集成。这种方法在以前从未被探索过。

    

    工业4.0将物联网、大数据和人工智能等数字技术整合到制造和工业流程中，以提高效率和生产力。随着这些技术的互联互通和相互依赖程度越来越高，工业4.0系统变得更加复杂，这就带来了识别和停止可能在制造过程中引起干扰的异常的困难。本文旨在提出一种基于扩散的模型，用于工业4.0流程中的实时异常预测。通过神经符号方法，我们将工业本体论集成到模型中，从而在智能制造中添加了形式化知识。最后，我们提出了一种简单而有效的通过随机傅里叶特征提取扩散模型的方法，以便将其部署到嵌入式系统中，直接集成到制造过程中。据我们所知，这种方法在以前从未被探索过。

    Industry 4.0 involves the integration of digital technologies, such as IoT, Big Data, and AI, into manufacturing and industrial processes to increase efficiency and productivity. As these technologies become more interconnected and interdependent, Industry 4.0 systems become more complex, which brings the difficulty of identifying and stopping anomalies that may cause disturbances in the manufacturing process. This paper aims to propose a diffusion-based model for real-time anomaly prediction in Industry 4.0 processes. Using a neuro-symbolic approach, we integrate industrial ontologies in the model, thereby adding formal knowledge on smart manufacturing. Finally, we propose a simple yet effective way of distilling diffusion models through Random Fourier Features for deployment on an embedded system for direct integration into the manufacturing process. To the best of our knowledge, this approach has never been explored before.
    
[^79]: 实践中的短布尔公式作为解释

    Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v1 [cs.LO])

    [http://arxiv.org/abs/2307.06971](http://arxiv.org/abs/2307.06971)

    本论文研究了基于一元关系的数据模型的短布尔公式解释的可行性，提出了对期望错误的定量界限，并展示了在三个具体数据集上的实际应用。通过限制公式长度，可以获得避免过拟合且准确且易于理解的解释。

    

    我们研究了基于一元关系的数据模型中通过短布尔公式进行解释的可行性。作为长度为k的解释，我们采用一个长度为k的布尔公式，该公式在解释目标属性方面的错误最小化。我们首先为这种情况提供了新颖的期望错误的定量界限。然后，我们通过研究三个具体的数据集来演示该设置在实践中的运作方式。在每种情况下，我们使用Answer Set Programming中的编码计算不同长度的解释公式。我们得到的最准确的公式在相同的数据集上实现了与其他方法类似的错误。然而，由于过拟合的原因，这些公式不一定是理想的解释，因此我们使用交叉验证来确定合适的解释长度。通过限制为更短的公式，我们得到的解释不仅避免了过拟合，而且依然相当准确，并且重要的是，易于人类解释。

    We investigate explainability via short Boolean formulas in the data model based on unary relations. As an explanation of length k, we take a Boolean formula of length k that minimizes the error with respect to the target attribute to be explained. We first provide novel quantitative bounds for the expected error in this scenario. We then also demonstrate how the setting works in practice by studying three concrete data sets. In each case, we calculate explanation formulas of different lengths using an encoding in Answer Set Programming. The most accurate formulas we obtain achieve errors similar to other methods on the same data sets. However, due to overfitting, these formulas are not necessarily ideal explanations, so we use cross validation to identify a suitable length for explanations. By limiting to shorter formulas, we obtain explanations that avoid overfitting but are still reasonably accurate and also, importantly, human interpretable.
    
[^80]: 机器学习辅助模式识别算法用于估计熔融沉积模式聚乳酸酸（PLA）试样的极限抗拉强度

    Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens. (arXiv:2307.06970v1 [cs.LG])

    [http://arxiv.org/abs/2307.06970](http://arxiv.org/abs/2307.06970)

    本研究使用机器学习算法估计了采用FDM工艺制造的PLA试样的极限抗拉强度（UTS），结果表明KNN算法优于其他算法，在区分不同UTS类别方面表现出更好的效果。

    

    本研究探讨了使用监督式机器学习算法来估计采用熔融沉积模型（FDM）工艺制造的聚乳酸酸（PLA）试样的极限抗拉强度（UTS）。总共制备了31个PLA试样，其中填充百分比、层高、打印速度和挤出温度作为输入参数。主要目标是评估四种不同监督分类算法，分别是逻辑回归分类、梯度提升分类、决策树和K近邻，在预测试样UTS方面的准确性和效果。结果显示，决策树和K近邻算法的F1分数均为0.71，但KNN算法的曲线下面积（AUC）分数更高，达到0.79，优于其他算法。这表明KNN算法在区分两类极限抗拉强度方面具有较强的能力。

    In this study, we investigate the application of supervised machine learning algorithms for estimating the Ultimate Tensile Strength (UTS) of Polylactic Acid (PLA) specimens fabricated using the Fused Deposition Modeling (FDM) process. A total of 31 PLA specimens were prepared, with Infill Percentage, Layer Height, Print Speed, and Extrusion Temperature serving as input parameters. The primary objective was to assess the accuracy and effectiveness of four distinct supervised classification algorithms, namely Logistic Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest Neighbor, in predicting the UTS of the specimens. The results revealed that while the Decision Tree and K-Nearest Neighbor algorithms both achieved an F1 score of 0.71, the KNN algorithm exhibited a higher Area Under the Curve (AUC) score of 0.79, outperforming the other algorithms. This demonstrates the superior ability of the KNN algorithm in differentiating between the two classes of ultimate
    
[^81]: 分层线性模态连接

    Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])

    [http://arxiv.org/abs/2307.06966](http://arxiv.org/abs/2307.06966)

    本文提出了一种分层线性模态连接方法用于联邦深度学习，通过解决模型漂移和高损失障壁的问题，能够有效提升全局模型的性能。

    

    在联邦设置中，我们在训练过程中多次对分离的本地模型进行聚合，以获得更强大的全局模型；最常见的聚合方法是参数的简单平均。理解在非凸设置（如联邦深度学习）中聚合何时以及为何有效是一个尚未解决的挑战，这阻碍了获得高性能的全局模型。在i.i.d.数据集上，频繁平均的联邦深度学习是成功的。然而，常见的观点是在独立训练期间，模型会相互漂移，因此在许多本地参数更新后平均可能不再起作用。这个问题可以从损失曲面的角度来看：对于非凸曲面上的点，平均值可能变得任意糟糕。通常用于解释联邦平均成功的局部凸性假设与经验证据相矛盾，显示不同模型之间存在高损失障壁。

    In the federated setup one performs an aggregation of separate local models multiple times during training in order to obtain a stronger global model; most often aggregation is a simple averaging of the parameters. Understanding when and why averaging works in a non-convex setup, such as federated deep learning, is an open challenge that hinders obtaining highly performant global models. On i.i.d.~datasets federated deep learning with frequent averaging is successful. The common understanding, however, is that during the independent training models are drifting away from each other and thus averaging may not work anymore after many local parameter updates. The problem can be seen from the perspective of the loss surface: for points on a non-convex surface the average can become arbitrarily bad. The assumption of local convexity, often used to explain the success of federated averaging, contradicts to the empirical evidence showing that high loss barriers exist between models from the v
    
[^82]: 任务无关的可解释人工智能是个神话吗？

    Is Task-Agnostic Explainable AI a Myth?. (arXiv:2307.06963v1 [cs.AI])

    [http://arxiv.org/abs/2307.06963](http://arxiv.org/abs/2307.06963)

    我们提出了一个框架来统一当代可解释人工智能的挑战，指出虽然XAI方法为模型提供了有用的输出，但研究人员和决策者应注意它们的限制，需要在该领域有一个概念突破以解决XAI方法和应用任务之间的兼容性挑战。

    

    我们的工作提供了一个框架，用于统一当代可解释人工智能(XAI)的挑战。我们证明了虽然XAI方法为机器学习模型提供了补充和潜在有用的输出，研究人员和决策者应该注意它们的概念和技术限制，这经常导致这些方法本身成为黑盒子。我们研究了三个XAI研究方向，涵盖图像、文本和图形数据，包括显著性、注意力和图形解释器。尽管所提及的案例的上下文和时间跨度各不相同，但同样的持续阻碍出现了，突显出在该领域中解决XAI方法和应用任务之间兼容性挑战的概念突破的需求。

    Our work serves as a framework for unifying the challenges of contemporary explainable AI (XAI). We demonstrate that while XAI methods provide supplementary and potentially useful output for machine learning models, researchers and decision-makers should be mindful of their conceptual and technical limitations, which frequently result in these methods themselves becoming black boxes. We examine three XAI research avenues spanning image, textual, and graph data, covering saliency, attention, and graph-type explainers. Despite the varying contexts and timeframes of the mentioned cases, the same persistent roadblocks emerge, highlighting the need for a conceptual breakthrough in the field to address the challenge of compatibility between XAI methods and application tasks.
    
[^83]: 拥抱混乱：数值不稳定性在变分流中的分析和诊断

    Embracing the chaos: analysis and diagnosis of numerical instability in variational flows. (arXiv:2307.06957v1 [stat.ML])

    [http://arxiv.org/abs/2307.06957](http://arxiv.org/abs/2307.06957)

    本文研究了数值不稳定性对变分流中采样、密度评估和ELBO估计的可靠性的影响。通过理论保证和实验验证，我们发现尽管存在严重的数值不稳定性，变分流产生的结果在应用中常常足够准确。

    

    本文研究了数值不稳定性对变分流中采样、密度评估和证据下界（ELBO）估计的可靠性的影响。我们首先通过实证验证了常见流可能出现严重的错误累积：数值流映射与精确映射的偏差显著，影响采样；数值逆流映射无法准确恢复初始输入，影响密度和ELBO计算。然而，我们惊讶地发现，尽管存在严重的数值不稳定性，流产生的结果常常足够准确应对应用需求。在这项工作中，我们将变分流视为动力系统，并利用阴影理论通过理论保证对采样、密度评估和ELBO估计的错误来阐明这种行为。最后，我们开发并经验性地测试了一种可以用于验证数值结果的诊断程序。

    In this paper, we investigate the impact of numerical instability on the reliability of sampling, density evaluation, and evidence lower bound (ELBO) estimation in variational flows. We first empirically demonstrate that common flows can exhibit a catastrophic accumulation of error: the numerical flow map deviates significantly from the exact map -- which affects sampling -- and the numerical inverse flow map does not accurately recover the initial input -which affects density and ELBO computations. Surprisingly though, we find that results produced by flows are often accurate enough for applications despite the presence of serious numerical instability. In this work, we treat variational flows as dynamical systems, and leverage shadowing theory to elucidate this behavior via theoretical guarantees on the error of sampling, density evaluation, and ELBO estimation. Finally, we develop and empirically test a diagnostic procedure that can be used to validate results produced by numerica
    
[^84]: 人工智能在全球气候合作2023竞赛论文集

    AI For Global Climate Cooperation 2023 Competition Proceedings. (arXiv:2307.06951v1 [cs.AI])

    [http://arxiv.org/abs/2307.06951](http://arxiv.org/abs/2307.06951)

    国际社会必须合作应对气候变化，通过结合人工智能和气候经济模拟，设计促进和激励合作的国际框架，解决协议遵守、政策目标实现和持续承诺等挑战。

    

    国际社会必须合作应对气候变化并保持经济增长。然而，由于没有全球权威机构能确保国际气候协议的遵守，合作很难实现。将人工智能与气候经济模拟相结合，提供了设计促进和激励合作的国际框架的有希望的解决方案，包括谈判协议和气候协议。此外，这些框架还应考虑到气候经济动态和战略行为，以实现政策目标的实现和持续承诺。这些挑战需要跨机器学习、经济学、气候科学、法律、政策、伦理学等领域的跨学科方法。为了实现这一目标，我们组织了AI for Global Climate Cooperation（全球气候合作人工智能）竞赛，参赛团队提交了基于（修改的）RICE-N的国际框架的提案和分析。

    The international community must collaborate to mitigate climate change and sustain economic growth. However, collaboration is hard to achieve, partly because no global authority can ensure compliance with international climate agreements. Combining AI with climate-economic simulations offers a promising solution to design international frameworks, including negotiation protocols and climate agreements, that promote and incentivize collaboration. In addition, these frameworks should also have policy goals fulfillment, and sustained commitment, taking into account climate-economic dynamics and strategic behaviors. These challenges require an interdisciplinary approach across machine learning, economics, climate science, law, policy, ethics, and other fields.  Towards this objective, we organized AI for Global Climate Cooperation, a Mila competition in which teams submitted proposals and analyses of international frameworks, based on (modifications of) RICE-N, an AI-driven integrated ass
    
[^85]: 迈向工程中先验知识整合的机器学习路径

    Pathway toward prior knowledge-integrated machine learning in engineering. (arXiv:2307.06950v1 [cs.AI])

    [http://arxiv.org/abs/2307.06950](http://arxiv.org/abs/2307.06950)

    本研究提出了一种整合先验知识的机器学习方法，对工程领域中的信息不确定性进行了检查，并探索了以三层知识整合的机器学习范式进行知识分解。这种方法平衡了整体论和还原论观点。

    

    尽管数字化趋势和数据量激增，但基于第一原理的模型（也称为逻辑驱动、基于物理、基于规则或基于知识的模型）与数据驱动方法存在并行存在，反映了关于符号主义与连接主义的持续AI辩论。在数据驱动过程中传输和利用领域知识的过程开发研究很少见。本研究强调将多学科领域专业知识整合到机器知识化的数据驱动过程中的努力和主流趋势，以两种方式组织：检查知识表示中的信息不确定性来源，并探索以三层知识整合的机器学习范式进行知识分解。这种方法平衡了工程领域的整体论和还原论观点。

    Despite the digitalization trend and data volume surge, first-principles models (also known as logic-driven, physics-based, rule-based, or knowledge-based models) and data-driven approaches have existed in parallel, mirroring the ongoing AI debate on symbolism versus connectionism. Research for process development to integrate both sides to transfer and utilize domain knowledge in the data-driven process is rare. This study emphasizes efforts and prevailing trends to integrate multidisciplinary domain professions into machine acknowledgeable, data-driven processes in a two-fold organization: examining information uncertainty sources in knowledge representation and exploring knowledge decomposition with a three-tier knowledge-integrated machine learning paradigm. This approach balances holist and reductionist perspectives in the engineering domain.
    
[^86]: 通过潜在空间分解揭示独特的概念向量

    Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])

    [http://arxiv.org/abs/2307.06913](http://arxiv.org/abs/2307.06913)

    通过潜在空间分解和无监督聚类，我们提出了一种自动揭示深度学习模型学习到的概念向量的方法，这些概念向量与模型预测相关且具有语义的独特概念，并且在实验中表明这些概念对人类来说易于理解和与任务相关。

    

    解释深度学习模型的内部工作对于建立信任和确保模型安全至关重要。基于概念的解释已经成为一种更易解释的方法，比如像素显著性等特征归因估计。然而，定义解释分析的概念会受到用户对概念期望的偏差影响。为了解决这个问题，我们提出了一种新的事后无监督方法，可以自动揭示深度模型在训练期间学习到的概念。通过分解一个层的潜在空间成奇异向量，并通过无监督聚类对其进行精炼，我们揭示了与模型预测相关的高方差方向上的概念向量，并指向语义上独特的概念。我们广泛的实验结果显示，我们的大部分概念对人类来说是易于理解的，具有一致性，并与所需任务相关。此外，我们还展示了...

    Interpreting the inner workings of deep learning models is crucial for establishing trust and ensuring model safety. Concept-based explanations have emerged as a superior approach that is more interpretable than feature attribution estimates such as pixel saliency. However, defining the concepts for the interpretability analysis biases the explanations by the user's expectations on the concepts. To address this, we propose a novel post-hoc unsupervised method that automatically uncovers the concepts learned by deep models during training. By decomposing the latent space of a layer in singular vectors and refining them by unsupervised clustering, we uncover concept vectors aligned with directions of high variance that are relevant to the model prediction, and that point to semantically distinct concepts. Our extensive experiments reveal that the majority of our concepts are readily understandable to humans, exhibit coherency, and bear relevance to the task at hand. Moreover, we showcase
    
[^87]: PC-Droid: 更快的扩散速度和改进的粒子云生成质量

    PC-Droid: Faster diffusion and improved quality for particle cloud generation. (arXiv:2307.06836v1 [hep-ex])

    [http://arxiv.org/abs/2307.06836](http://arxiv.org/abs/2307.06836)

    PC-Droid是一个新的扩散模型，通过利用新的扩散公式和研究更近期的积分求解器，同时对所有类型的喷注进行训练，实现了最先进的性能。它不仅能提供更快的生成速度，而且在所有评估指标上都具有卓越的性能。

    

    在PC-JeDi的基础上，我们引入了PC-Droid，这是一个大幅改进的扩散模型，用于生成喷注粒子云。通过利用新的扩散公式、研究更近期的积分求解器，并同时对所有类型的喷注进行训练，我们能够在所有评估指标上实现最先进的性能。我们通过比较基于注意力的两种体系结构和一致性蒸馏的潜力来研究生成速度和质量之间的权衡。更快的体系结构和一致性模型都表现出超越许多竞争模型的性能，生成时间比PC-JeDi快上两个数量级。

    Building on the success of PC-JeDi we introduce PC-Droid, a substantially improved diffusion model for the generation of jet particle clouds. By leveraging a new diffusion formulation, studying more recent integration solvers, and training on all jet types simultaneously, we are able to achieve state-of-the-art performance for all types of jets across all evaluation metrics. We study the trade-off between generation speed and quality by comparing two attention based architectures, as well as the potential of consistency distillation to reduce the number of diffusion steps. Both the faster architecture and consistency models demonstrate performance surpassing many competing models, with generation time up to two orders of magnitude faster than PC-JeDi.
    
[^88]: 通过集成深度强化学习的混合控制策略实现人工胰腺

    Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v1 [cs.AI])

    [http://arxiv.org/abs/2307.06501](http://arxiv.org/abs/2307.06501)

    本研究提出了一种名为HyCPAP的混合控制策略，通过结合模型预测控制和集成深度强化学习，并充分利用它们各自的优势，以解决人工胰腺的复杂生理过程、延迟胰岛素反应和不准确血糖测量等挑战。

    

    目标：人工胰腺(AP)在实现1型糖尿病患者闭环血糖控制方面显示出有希望的潜力。然而，由于复杂的生理过程、延迟的胰岛素反应和不准确的血糖测量，设计一种有效的AP控制策略仍然具有挑战性。虽然模型预测控制(MPC)通过动态模型和安全约束提供了安全性和稳定性，但其缺乏个性化，并且受到未宣布的饮食影响。相反，深度强化学习(DRL)提供了个性化和自适应策略，但面临分布偏移和大量数据需求的挑战。方法：我们提出了一种混合控制策略，即HyCPAP，来应对上述挑战。HyCPAP将MPC策略与集成DRL策略相结合，充分利用两种策略的优势，同时弥补各自的局限性。

    Objective: The artificial pancreas (AP) has shown promising potential in achieving closed-loop glucose control for individuals with type 1 diabetes mellitus (T1DM). However, designing an effective control policy for the AP remains challenging due to the complex physiological processes, delayed insulin response, and inaccurate glucose measurements. While model predictive control (MPC) offers safety and stability through the dynamic model and safety constraints, it lacks individualization and is adversely affected by unannounced meals. Conversely, deep reinforcement learning (DRL) provides personalized and adaptive strategies but faces challenges with distribution shifts and substantial data requirements. Methods: We propose a hybrid control policy for the artificial pancreas (HyCPAP) to address the above challenges. HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the strengths of both policies while compensating for their respective limitations. To facilitate faste
    
[^89]: 解决组合分布偏移问题：基于矩阵补全的观点

    Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective. (arXiv:2307.06457v1 [cs.LG])

    [http://arxiv.org/abs/2307.06457](http://arxiv.org/abs/2307.06457)

    该论文研究了组合分布偏移的问题，提出了基于矩阵补全的解决方法。通过在特殊情况下的双线性嵌入，实现对训练中未涵盖的测试分布进行外推。这个设置将缺失非随机数据的矩阵补全问题广义化。

    

    在分布偏移下获得严格的统计保证仍然是一个开放且活跃的研究领域。我们研究了一种称为组合分布偏移的设置，其中(a)在测试和训练分布下，标签$z$由特征$(x,y)$的对决定，(b)训练分布涵盖了$x$和$y$分别的一定边缘分布，但是(c)测试分布涉及了一个在训练分布中未涵盖的$(x,y)$的产品分布的示例。我们专注于标签由双线性嵌入到Hilbert空间$H$中给出的特殊情况：$\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$，我们的目标是对在训练中未涵盖的测试分布域进行外推，即实现双线性组合外推。我们的设置将缺失非随机数据的矩阵补全的一个特殊情况广义化，对于该情况，所有现有结果都要求....

    Obtaining rigorous statistical guarantees for generalization under distribution shift remains an open and active research area. We study a setting we call combinatorial distribution shift, where (a) under the test- and training-distributions, the labels $z$ are determined by pairs of features $(x,y)$, (b) the training distribution has coverage of certain marginal distributions over $x$ and $y$ separately, but (c) the test distribution involves examples from a product distribution over $(x,y)$ that is {not} covered by the training distribution. Focusing on the special case where the labels are given by bilinear embeddings into a Hilbert space $H$: $\mathbb{E}[z \mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$, we aim to extrapolate to a test distribution domain that is $not$ covered in training, i.e., achieving bilinear combinatorial extrapolation.  Our setting generalizes a special case of matrix completion from missing-not-at-random data, for which all existing results requi
    
[^90]: 从软干预中确保因果分解的可识别性

    Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])

    [http://arxiv.org/abs/2307.06250](http://arxiv.org/abs/2307.06250)

    本文研究了从软干预中确保因果分解的可识别性。通过开发一种自编码变分贝叶斯算法，我们展示了在给定一般化的忠诚性概念的情况下，即使存在未观测到的因果变量，仍然可以恢复潜在的因果模型，并在无限数据的极限情况下预测未见组合的干预效果。

    

    因果分解旨在通过潜在变量的相关性揭示数据的表征，其通过因果模型相互关联。如果解释数据的潜在模型是唯一的，那么这种表示是可识别的。本文关注的是当存在不配对的观测和干预数据时的情况，每个干预都会改变一个潜在变量的机制。当因果变量完全观测到时，在诚实性假设下，已经开发出了统计一致的算法来识别因果模型。我们在这里展示，即使存在未观测到的因果变量，在给定一般化的忠诚性概念的情况下仍然可以实现可识别性。我们的结果保证了我们可以恢复潜在的因果模型，预测未见组合的干预效果，在无限数据的极限情况下。我们通过开发一种自编码变分贝叶斯算法和ap来实现我们的因果分解框架。

    Causal disentanglement aims to uncover a representation of data using latent variables that are interrelated through a causal model. Such a representation is identifiable if the latent model that explains the data is unique. In this paper, we focus on the scenario where unpaired observational and interventional data are available, with each intervention changing the mechanism of a latent variable. When the causal variables are fully observed, statistically consistent algorithms have been developed to identify the causal model under faithfulness assumptions. We here show that identifiability can still be achieved with unobserved causal variables, given a generalized notion of faithfulness. Our results guarantee that we can recover the latent causal model up to an equivalence class and predict the effect of unseen combinations of interventions, in the limit of infinite data. We implement our causal disentanglement framework by developing an autoencoding variational Bayes algorithm and ap
    
[^91]: 以不同方式堆叠更多层：通过低秩更新进行高秩训练

    Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])

    [http://arxiv.org/abs/2307.05695](http://arxiv.org/abs/2307.05695)

    本文以低秩训练技术作为替代方法，提出了一种名为ReLoRA的新方法，利用低秩更新来训练大规模神经网络。在预训练的Transformer语言模型中，我们观察到ReLoRA在与常规神经网络训练相比的性能表现上相当，并发现其在模型越大的情况下效率越高，为高效训练千亿级参数网络提供了新的可能性。

    

    尽管大规模网络拥有数百亿个参数的规模已经占主导地位并且效果显著，但对于过度参数化模型的训练必要性仍然缺乏清晰的理解，而替代方法不一定能够降低训练高性能模型的成本。本文探索了低秩训练技术作为训练大型神经网络的替代方法。我们引入了一种称为ReLoRA的新方法，它利用低秩更新来训练高秩网络。我们将ReLoRA应用于预训练的Transformer语言模型，参数量高达350M，并且证明了与常规神经网络训练相当的性能。此外，我们观察到ReLoRA的效率随着模型大小的增加而提高，这使得它成为高效训练千亿级参数网络的有希望的方法。我们的研究结果揭示了低秩训练技术的潜力及其对于缩放定律的影响。

    Despite the dominance and effectiveness of scaling, resulting in large networks with hundreds of billions of parameters, the necessity to train overparametrized models remains poorly understood, and alternative approaches do not necessarily make it cheaper to train high-performance models. In this paper, we explore low-rank training techniques as an alternative approach to training large neural networks. We introduce a novel method called ReLoRA, which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to pre-training transformer language models with up to 350M parameters and demonstrate comparable performance to regular neural network training. Furthermore, we observe that the efficiency of ReLoRA increases with model size, making it a promising approach for training multi-billion-parameter networks efficiently. Our findings shed light on the potential of low-rank training techniques and their implications for scaling laws.
    
[^92]: 使用线性回归迭代训练神经网络

    Using Linear Regression for Iteratively Training Neural Networks. (arXiv:2307.05189v1 [cs.LG])

    [http://arxiv.org/abs/2307.05189](http://arxiv.org/abs/2307.05189)

    我们提出了一种使用线性回归来迭代训练神经网络的方法，通过从输出向后计算神经元的理想总输入值，以线性最小二乘问题迭代更新参数和激活值。

    

    我们提出了一种基于简单线性回归的方法来学习神经网络的权重和偏置，作为标准梯度反向传播的替代方法。目前的工作是探索性的，并且我们将描述和实验限制在（i）简单的前馈神经网络，（ii）标量（单输出）回归问题，以及（iii）可逆激活函数上。然而，这种方法可以扩展到更大、更复杂的架构。关键思想是观察到神经网络中每个神经元的输入是前一层神经元的激活以及该层的参数（权重和偏置）的线性组合。如果我们能够通过从输出向后计算每个神经元的理想总输入值，我们可以将学习问题形式化为一个线性最小二乘问题，该问题在更新参数和激活值之间迭代。我们提出了一个明确的算法来实现这个方法。

    We present a simple linear regression based approach for learning the weights and biases of a neural network, as an alternative to standard gradient based backpropagation. The present work is exploratory in nature, and we restrict the description and experiments to (i) simple feedforward neural networks, (ii) scalar (single output) regression problems, and (iii) invertible activation functions. However, the approach is intended to be extensible to larger, more complex architectures. The key idea is the observation that the input to every neuron in a neural network is a linear combination of the activations of neurons in the previous layer, as well as the parameters (weights and biases) of the layer. If we are able to compute the ideal total input values to every neuron by working backwards from the output, we can formulate the learning problem as a linear least squares problem which iterates between updating the parameters and the activation values. We present an explicit algorithm tha
    
[^93]: 使用最优输运进行去相关

    Decorrelation using Optimal Transport. (arXiv:2307.05187v1 [hep-ph])

    [http://arxiv.org/abs/2307.05187](http://arxiv.org/abs/2307.05187)

    引入了一种利用凸神经最优输运求解器进行去相关的新方法，通过最优输运将连续特征空间与受保护属性去相关。在高能物理中，这种方法在喷注分类中表现出色，能更好地去相关多类输出特征空间。

    

    能够从受保护属性中将特征空间去相关是伦理学、公平性以及自然科学领域活跃研究和学习的一个领域。我们引入一种新的利用凸神经最优输运求解器（Cnots）进行去相关的方法，该方法能够通过最优输运使连续特征空间与受保护属性去相关。我们演示了在高能物理中进行喷注分类时，它的表现如何，其中分类器得分希望与喷注的质量去相关。在二进制分类中实现的去相关程度接近于使用条件归一化流的最新水平。当转向多类输出时，最优输运方法的性能显著优于最先进方法，表明在去相关多维特征空间方面有实质性的增益。

    Being able to decorrelate a feature space from protected attributes is an area of active research and study in ethics, fairness, and also natural sciences. We introduce a novel decorrelation method using Convex Neural Optimal Transport Solvers (Cnots), that is able to decorrelate continuous feature space against protected attributes with optimal transport. We demonstrate how well it performs in the context of jet classification in high energy physics, where classifier scores are desired to be decorrelated from the mass of a jet. The decorrelation achieved in binary classification approaches the levels achieved by the state-of-the-art using conditional normalising flows. When moving to multiclass outputs the optimal transport approach performs significantly better than the state-of-the-art, suggesting substantial gains at decorrelating multidimensional feature spaces.
    
[^94]: 利用人类好奇心的网络理论进行内在驱动的图探索

    Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])

    [http://arxiv.org/abs/2307.04962](http://arxiv.org/abs/2307.04962)

    在这项工作中，我们通过应用人类好奇心的两个理论，发展了一种内在驱动的图探索方法。我们利用图神经网络的强化学习将拓扑特征作为奖励，从而实现了对图结构数据的探索。在多类合成生成图上进行的实验证明，我们的方法不仅可以推广到更大的环境，还可以进行更长的探索步行。同时，我们的方法比传统的贪婪评估方法更高效。

    

    内在驱动的探索在强化学习中已被证明具有用途，即使没有额外的外在奖励。当环境自然表示为图时，如何最好地引导探索仍是一个未解决的问题。在这项工作中，我们提出了一种新的方法，通过人类好奇心的两个理论：信息差理论和压缩进展理论，来激励对图结构数据进行探索。这些理论将好奇心视为对环境中访问节点所引发的子图的拓扑特征进行优化的内在动机。我们将这些提出的特征作为基于图神经网络的强化学习的奖励。在多个类别的合成生成图上，我们发现训练代理可以推广到更大的环境和比训练过程中更长的探索性步行。我们的方法的计算效率高于相关拓扑属性的贪婪评估。所提出的内在动机产生的奖励在多类合成生成图生成上推广良好，并且在训练期间能够在更大的环境中进行更长的探索步行。

    Intrinsically motivated exploration has proven useful for reinforcement learning, even without additional extrinsic rewards. When the environment is naturally represented as a graph, how to guide exploration best remains an open question. In this work, we propose a novel approach for exploring graph-structured data motivated by two theories of human curiosity: the information gap theory and the compression progress theory. The theories view curiosity as an intrinsic motivation to optimize for topological features of subgraphs induced by the visited nodes in the environment. We use these proposed features as rewards for graph neural-network-based reinforcement learning. On multiple classes of synthetically generated graphs, we find that trained agents generalize to larger environments and to longer exploratory walks than are seen during training. Our method computes more efficiently than the greedy evaluation of the relevant topological properties. The proposed intrinsic motivations bea
    
[^95]: LINFA:一种用于变分推理的Python库，包含归一化流动和退火算法

    LINFA: a Python library for variational inference with normalizing flow and annealing. (arXiv:2307.04675v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.04675](http://arxiv.org/abs/2307.04675)

    LINFA是一个用于变分推理的Python库，支持处理计算复杂的模型和难以采样的依赖参数分布。

    

    变分推理是统计学和机器学习中越来越流行的一种近似概率分布的方法。我们开发了LINFA（用于归一化流动和退火算法的推理库），这是一个用于变分推理的Python库，适用于计算复杂的模型和难以采样的依赖参数分布。我们讨论了LINFA在各种基准测试中的理论背景、功能和性能。LINFA可在GitHub上公开获取，网址为https://github.com/desResLab/LINFA。

    Variational inference is an increasingly popular method in statistics and machine learning for approximating probability distributions. We developed LINFA (Library for Inference with Normalizing Flow and Annealing), a Python library for variational inference to accommodate computationally expensive models and difficult-to-sample distributions with dependent parameters. We discuss the theoretical background, capabilities, and performance of LINFA in various benchmarks. LINFA is publicly available on GitHub at https://github.com/desResLab/LINFA.
    
[^96]: SAR: 通过协同行动表示实现生理敏捷性和灵巧性的普适性

    SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation. (arXiv:2307.03716v1 [cs.RO])

    [http://arxiv.org/abs/2307.03716](http://arxiv.org/abs/2307.03716)

    本研究将生物进化中发展出的肌肉协同控制策略应用于人手和腿模型，发现通过协同行动表示（SAR）训练的策略在学习复杂任务时明显优于端到端强化学习。

    

    在高维系统中学习高效连续控制策略，包括肌肉骨骼代理，仍然是一个重大挑战。通过生物进化的过程中，生物体发展出了克服这种复杂性的鲁棒机制，学习了高度复杂的运动控制策略。是什么导致了这种鲁棒的行为灵活性呢？通过肌肉协同的模块化控制被认为是一种潜在的机制，使生物能够在简化和可推广的行动空间中学习肌肉控制。受到这种进化的运动控制策略的启发，我们使用生理准确的人手和腿模型作为测试平台，确定从简单任务中获得的协同行动表示（SAR）在学习更复杂任务中的作用程度。我们发现，在这两种情况下，利用SAR的策略明显优于端到端强化学习。通过SAR进行训练的策略优于端到端强化学习.

    Learning effective continuous control policies in high-dimensional systems, including musculoskeletal agents, remains a significant challenge. Over the course of biological evolution, organisms have developed robust mechanisms for overcoming this complexity to learn highly sophisticated strategies for motor control. What accounts for this robust behavioral flexibility? Modular control via muscle synergies, i.e. coordinated muscle co-contractions, is considered to be one putative mechanism that enables organisms to learn muscle control in a simplified and generalizable action space. Drawing inspiration from this evolved motor control strategy, we use physiologically accurate human hand and leg models as a testbed for determining the extent to which a Synergistic Action Representation (SAR) acquired from simpler tasks facilitates learning more complex tasks. We find in both cases that SAR-exploiting policies significantly outperform end-to-end reinforcement learning. Policies trained wit
    
[^97]: DEFT:利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化

    DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v1 [cs.LG])

    [http://arxiv.org/abs/2307.03500](http://arxiv.org/abs/2307.03500)

    DEFT是一种利用模型层之间的梯度范数差异来实现可扩展的梯度稀疏化的方法，可以减少分布式深度学习中的通信流量，并在计算成本和梯度累积方面具有优势。

    

    梯度稀疏化是减少分布式深度学习中过多通信流量的广泛应用解决方案。然而，大多数现有的梯度稀疏化方法由于梯度选择的计算成本相当大和梯度累积增加的通信流量，其可扩展性相对较差。为了解决这些挑战，我们提出了一种新颖的梯度稀疏化方案DEFT，将梯度选择任务分解为子任务并分配给工作节点。 DEFT与现有的稀疏化方法不同，每个工作节点仅从所有梯度中选择梯度。因此，随着工作节点数量的增加，计算成本可以降低。此外，DEFT允许工作节点在非交叉的分区中选择梯度，因此即使工作节点数量增加，通信流量也可以根据用户要求进行维持。

    Gradient sparsification is a widely adopted solution for reducing the excessive communication traffic in distributed deep learning. However, most existing gradient sparsifiers have relatively poor scalability because of considerable computational cost of gradient selection and/or increased communication traffic owing to gradient build-up. To address these challenges, we propose a novel gradient sparsification scheme, DEFT, that partitions the gradient selection task into sub tasks and distributes them to workers. DEFT differs from existing sparsifiers, wherein every worker selects gradients among all gradients. Consequently, the computational cost can be reduced as the number of workers increases. Moreover, gradient build-up can be eliminated because DEFT allows workers to select gradients in partitions that are non-intersecting (between workers). Therefore, even if the number of workers increases, the communication traffic can be maintained as per user requirement.  To avoid the loss 
    
[^98]: 关于形式特征归因及其近似方法

    On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v1 [cs.AI])

    [http://arxiv.org/abs/2307.03380](http://arxiv.org/abs/2307.03380)

    这篇论文研究了解释性人工智能（XAI）中的形式特征归因方法及其近似方法。现有的特征选择和归因方法存在一些问题，而形式化的XAI方法虽然是一个有希望的解决方案，但仍存在一些限制。

    

    近年来，人工智能（AI）算法和机器学习（ML）模型得到了广泛应用。尽管取得了巨大成功，但ML模型脆弱性，公平性以及解释性的缺乏等重要问题需要积极发展可解释的人工智能（XAI）和形式化的ML模型验证。XAI的两个主要研究方向包括特征选择方法（例如，Anchors）和特征归因技术（例如，LIME和SHAP）。尽管有希望，但大多数现有的特征选择和归因方法都容易出现一系列关键问题，包括解释不正确和超出分布采样。近期一种形式化的XAI方法（FXAI）虽然作为以上方法的替代品并避免了这些问题，但仍存在一些限制。例如，除了可扩展性限制外，这种形式化方法无法解决特征归因问题。

    Recent years have witnessed the widespread use of artificial intelligence (AI) algorithms and machine learning (ML) models. Despite their tremendous success, a number of vital problems like ML model brittleness, their fairness, and the lack of interpretability warrant the need for the active developments in explainable artificial intelligence (XAI) and formal ML model verification. The two major lines of work in XAI include feature selection methods, e.g. Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their promise, most of the existing feature selection and attribution approaches are susceptible to a range of critical issues, including explanation unsoundness and out-of-distribution sampling. A recent formal approach to XAI (FXAI) although serving as an alternative to the above and free of these issues suffers from a few other limitations. For instance and besides the scalability limitation, the formal approach is unable to tackle the feature attribution prob
    
[^99]: 一种用于深度学习的合成心电图（ECG）图像生成工具箱，以促进扫描ECG数字化

    A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.01946](http://arxiv.org/abs/2307.01946)

    本文介绍了一种用于生成合成ECG图像的工具箱，旨在促进扫描ECG数字化。通过引入真实伪影，如手写文本伪影、皱纹、折痕和视角变换，该方法可以在标准纸质ECG背景上生成具有真实性的ECG图像。这种方法有助于解决合成ECG图像中缺乏参考时间序列的问题。

    

    心电图（ECG）是一种准确且广泛应用于诊断心血管疾病的工具。几十年来，ECG以印刷格式记录，并且将它们的数字化在算法性心电图诊断的机器学习模型训练中具有巨大潜力。物理性ECG存档面临退化风险，仅扫描印刷ECG是不够的，因为机器学习模型需要ECG时间序列数据。因此，将纸质ECG存档数字化和转换为时间序列数据至关重要。深度学习模型在图像处理方面显示出潜力。然而，具有参考时间序列的ECG存档稀缺是一个挑战。利用“数字孪生”的数据增强技术可能是一个潜在的解决方案。我们介绍了一种新的方法，以生成具有真实伪影的标准纸质ECG背景下的合成ECG图像。包括手写文本伪影、皱纹、折痕和视角转换等畸变。

    The electrocardiogram (ECG) is an accurate and widely available tool for diagnosing cardiovascular diseases. ECGs have been recorded in printed formats for decades and their digitization holds great potential for training machine learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at risk of deterioration and scanning printed ECGs alone is insufficient, as ML models require ECG time-series data. Therefore, the digitization and conversion of paper ECG archives into time-series data is of utmost importance. Deep learning models for image processing show promise in this regard. However, the scarcity of ECG archives with reference time-series is a challenge. Data augmentation techniques utilizing \textit{digital twins} present a potential solution.  We introduce a novel method for generating synthetic ECG images on standard paper-like ECG backgrounds with realistic artifacts. Distortions including handwritten text artifacts, wrinkles, creases and perspective transf
    
[^100]: 模型辅助的概率安全自适应控制与元贝叶斯学习

    Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian Learning. (arXiv:2307.00828v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2307.00828](http://arxiv.org/abs/2307.00828)

    本文提出了一种模型辅助的概率安全自适应控制方法，通过集成元学习、贝叶斯模型和控制限制函数方法，利用先前任务收集的数据进行预训练，并引入悲观置信区间来确保安全控制。

    

    在控制系统中打破安全约束可能导致潜在风险，造成意外的成本或灾难性损害。然而，不确定性普遍存在，即使是相似的任务也是如此。在本文中，我们开发了一种新颖的自适应安全控制框架，它集成了元学习、贝叶斯模型和控制限制函数（CBF）方法。具体而言，借助CBF方法，我们通过统一的自适应贝叶斯线性回归（ABLR）模型学习内在和外部的不确定性，该模型由前馈神经网络（NN）和贝叶斯输出层组成。利用元学习技术，我们使用从历史相似任务收集的数据对NN权重和ABLR模型的先验进行预训练。对于一个新的控制任务，我们通过少量样本来改进元学习模型，并将悲观置信区间引入CBF约束，以确保安全控制。此外，我们提供了理论准则，以确保在控制过程中的概率安全性。

    Breaking safety constraints in control systems can lead to potential risks, resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty is ubiquitous, even among similar tasks. In this paper, we develop a novel adaptive safe control framework that integrates meta learning, Bayesian models, and control barrier function (CBF) method. Specifically, with the help of CBF method, we learn the inherent and external uncertainties by a unified adaptive Bayesian linear regression (ABLR) model, which consists of a forward neural network (NN) and a Bayesian output layer. Meta learning techniques are leveraged to pre-train the NN weights and priors of the ABLR model using data collected from historical similar tasks. For a new control task, we refine the meta-learned models using a few samples, and introduce pessimistic confidence bounds into CBF constraints to ensure safe control. Moreover, we provide theoretical criteria to guarantee probabilistic safety during the control pro
    
[^101]: 利用扁平化到宽化方法进行少样本持续学习

    Few-Shot Continual Learning via Flat-to-Wide Approaches. (arXiv:2306.14369v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.14369](http://arxiv.org/abs/2306.14369)

    本文提出了一种少样本持续学习方法，名为FLat-tO-WidE AppRoach (FLOWER)，通过寻找扁平化宽化极小值的过程来解决灾难性遗忘问题，利用球体生成器概念进行数据增强方法来克服数据稀缺性问题。在小型基础任务中，FLOWER表现出显著改进的性能。

    

    现有的持续学习方法在训练过程中需要大量的样本。由于过拟合问题，这些方法在拥有有限样本的许多实际问题中是不现实的。本文提出了一种少样本持续学习方法，称为FLat-tO-WidE AppRoach (FLOWER)，其中提出了一种寻找扁平化宽化极小值的扁平化到宽化学习过程，以解决灾难性遗忘问题。通过使用球体生成器概念进行数据增强方法来克服数据稀缺性问题，将采样空间限制在最小外接球内。我们的数值研究表明，FLOWER相比先前的方法在小型基础任务中实现了显著改进的性能。为了进一步研究，FLOWER的源代码，竞争算法和实验日志公开共享在\url{https://github.com/anwarmaxsum/FLOWER}。

    Existing approaches on continual learning call for a lot of samples in their training processes. Such approaches are impractical for many real-world problems having limited samples because of the overfitting problem. This paper proposes a few-shot continual learning approach, termed FLat-tO-WidE AppRoach (FLOWER), where a flat-to-wide learning process finding the flat-wide minima is proposed to address the catastrophic forgetting problem. The issue of data scarcity is overcome with a data augmentation approach making use of a ball generator concept to restrict the sampling space into the smallest enclosing ball. Our numerical studies demonstrate the advantage of FLOWER achieving significantly improved performances over prior arts notably in the small base tasks. For further study, source codes of FLOWER, competitor algorithms and experimental logs are shared publicly in \url{https://github.com/anwarmaxsum/FLOWER}.
    
[^102]: LLMs如何改变材料科学和化学：大型语言模型黑客马拉松的反思

    14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2306.06283](http://arxiv.org/abs/2306.06283)

    本文记录了一次黑客松活动，参与者使用LLMs进行了各种应用，包括预测分子和材料特性、从非结构化数据中提取知识、为工具设计新界面以及开发新的教育应用。这些多样化的项目反映了LLMs在材料科学和化学领域的多功能性和潜力。

    

    化学和材料科学非常复杂。最近，使用数据驱动或计算技术解决了这种复杂性的中有很大的成功。然而，输入需要非常特定形式的结构以及工具数量不断增长所带来可用性和可访问性的挑战。加上这些学科中的大多数数据都是非结构化的事实，使得这些工具的效率受到限制。本文记录了关于LLMs的黑客松活动中构建的项目。参与者使用LLMs进行了各种应用，包括预测分子和材料的特性、为工具设计新界面、从非结构化数据中提取知识以及开发新的教育应用。各种各样的项目反映了LLMs在这些领域的多功能性和这些模型改变材料科学和化学领域的潜力。

    Chemistry and materials science are complex. Recently, there have been great successes in addressing this complexity using data-driven or computational techniques. Yet, the necessity of input structured in very specific forms and the fact that there is an ever-growing number of tools creates usability and accessibility challenges. Coupled with the reality that much data in these disciplines is unstructured, the effectiveness of these tools is limited.  Motivated by recent works that indicated that large language models (LLMs) might help address some of these issues, we organized a hackathon event on the applications of LLMs in chemistry, materials science, and beyond. This article chronicles the projects built as part of this hackathon. Participants employed LLMs for various applications, including predicting properties of molecules and materials, designing novel interfaces for tools, extracting knowledge from unstructured data, and developing new educational applications.  The diverse
    
[^103]: 利用代理分类损失的假设迁移学习

    Hypothesis Transfer Learning with Surrogate Classification Losses. (arXiv:2305.19694v1 [stat.ML])

    [http://arxiv.org/abs/2305.19694](http://arxiv.org/abs/2305.19694)

    本文研究了使用代理分类损失的假设迁移学习的学习理论，通过算法稳定性提供了在温和假设下的学习保证，适用于机器学习算法。

    

    假设迁移学习（HTL）通过允许先前任务（即源任务）向一个新任务（目标任务）转移学习，而无需访问源数据，与领域自适应相对应。事实上，HTL仅依赖于从源数据学习到的假设，免除了大量数据存储的障碍，并提供了巨大的实际利益。因此，HTL对于依赖于大数据的实际应用非常有利。本文通过算法稳定性研究HTL的学习理论，这是一种用于分析机器学习算法的有吸引力的理论框架，特别是在二分类情况下感兴趣。我们的稳定性分析提供了在温和假设下的学习保证。因此，我们得出了几个比以前更紧密的理论界限，这些界限可以实际应用于机器学习算法。

    Hypothesis transfer learning (HTL) contrasts domain adaptation by allowing for a previous task leverage, named the source, into a new one, the target, without requiring access to the source data. Indeed, HTL relies only on a hypothesis learnt from such source data, relieving the hurdle of expansive data storage and providing great practical benefits. Hence, HTL is highly beneficial for real-world applications relying on big data. The analysis of such a method from a theoretical perspective faces multiple challenges, particularly in classification tasks. This paper deals with this problem by studying the learning theory of HTL through algorithmic stability, an attractive theoretical framework for machine learning algorithms analysis. In particular, we are interested in the statistical behaviour of the regularized empirical risk minimizers in the case of binary classification. Our stability analysis provides learning guarantees under mild assumptions. Consequently, we derive several comp
    
[^104]: Dink-Net: 大规模图形神经聚类方法

    Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v1 [cs.LG])

    [http://arxiv.org/abs/2305.18405](http://arxiv.org/abs/2305.18405)

    Dink-Net是一个可扩展的大规模图形神经聚类方法，该方法利用了膨胀和收缩的思想来处理百万节点的大图，并在各种基准数据集上优于现有的最先进方法。

    

    近年来，深度图聚类通过深度神经网络将图形的节点进行分组取得了很大的进展，但现有方法无法处理百万节点的大图。为了解决这个问题，我们提出了一种可扩展的Dink-Net深度图聚类方法，利用了膨胀和收缩的思想。首先，通过区分带增强的跟不带增强的节点，自我监督方式学习表示形式。同时，将聚类中心初始化为可学习的神经网络参数。随后，通过对抗性方式最小化提出的集群膨胀损失和集群收缩损失，优化聚类分布。通过这些设置，我们将表示学习和聚类优化两个步骤统一为一个端到端框架，引导网络学习聚类友好的特征。此外，Dink-Net能很好地扩展到大规模的图形上，因为设计的膨胀收缩操作可以有效地减少计算和内存消耗。实验结果表明，Dink-Net在处理百万节点图形的各种基准数据集上优于现有的最先进方法，证明了该方法在大图聚类中的可扩展性和有效性。

    Deep graph clustering, which aims to group the nodes of a graph into disjoint clusters with deep neural networks, has achieved promising progress in recent years. However, the existing methods fail to scale to the large graph with million nodes. To solve this problem, a scalable deep graph clustering method (Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by discriminating nodes, whether being corrupted by augmentations, representations are learned in a self-supervised manner. Meanwhile, the cluster centres are initialized as learnable neural parameters. Subsequently, the clustering distribution is optimized by minimizing the proposed cluster dilation loss and cluster shrink loss in an adversarial manner. By these settings, we unify the two-step clustering, i.e., representation learning and clustering optimization, into an end-to-end framework, guiding the network to learn clustering-friendly features. Besides, Dink-Net scales well to large graphs since the designe
    
[^105]: 神经智能中的注意力模式

    Attention Schema in Neural Agents. (arXiv:2305.17375v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2305.17375](http://arxiv.org/abs/2305.17375)

    本文研究了神经智能中的注意力模式，并提出了注意力模式理论（AST）。作者发现将AS实现为一种循环内部控制的智能体效果最佳，这一理论为应用与改进神经智能提供了新思路。

    

    注意力已经成为深度学习架构中的常见要素。它通过加入动态信息选择，支持静态的权重选择。同样地，我们可以想象在注意力之上构建一个更高阶的信息过滤器：注意力模式（AS）。也就是说，一个描述性和预测性的注意力模型。在认知神经科学中，注意力模式理论（AST）支持这种区分注意力和AS的想法。该理论的一个重要预测是，一个智能体可以使用自己的AS来推断其他智能体的注意状态，从而增强与其他智能体的协调。因此，多智能体强化学习是一个实验测试AST有效性的理想场景。我们探讨了注意力和AS相互作用的不同方式，初步结果表明，将AS实现为一种循环内部控制的智能体效果最佳。总体而言，这些实验为理解以及改进神经智能中的注意力模式提供了新的思路。

    Attention has become a common ingredient in deep learning architectures. It adds a dynamical selection of information on top of the static selection of information supported by weights. In the same way, we can imagine a higher-order informational filter built on top of attention: an Attention Schema (AS), namely, a descriptive and predictive model of attention. In cognitive neuroscience, Attention Schema Theory (AST) supports this idea of distinguishing attention from AS. A strong prediction of this theory is that an agent can use its own AS to also infer the states of other agents' attention and consequently enhance coordination with other agents. As such, multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST. We explore different ways in which attention and AS interact with each other. Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance. In general, these expl
    
[^106]: 深度可解释关系强化学习：一种神经符号化方法

    Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2304.08349](http://arxiv.org/abs/2304.08349)

    本论文提出了一种深度可解释关系强化学习框架，通过将神经和符号世界相结合，利用神经符号化的方法提取出可解释的策略，解决了深度强化学习中不可解释性和处理环境结构变化的困难。

    

    尽管深度强化学习（DRL）取得了许多成功，学习到的策略仍然是不可解释的。此外，由于DRL没有利用符号关系表示，它在处理环境结构变化（如对象数量增加）方面存在困难。另一方面，关系强化学习从符号计划继承了关系表示，以学习可重用的策略。然而，迄今为止，关系强化学习无法扩展并充分利用深度神经网络的能力。我们提出了一种深度可解释关系强化学习（DERRL）框架，它结合了神经和符号世界的优势。通过采用神经符号化方法，DERRL将符号计划中的关系表示和约束与深度学习相结合，提取出可解释的策略。这些策略以逻辑规则的形式解释了每个决策（或动作）的产生过程。通过多个实验，我们展示了DERRL在处理环境结构变化方面的优越性能。

    Despite numerous successes in Deep Reinforcement Learning (DRL), the learned policies are not interpretable. Moreover, since DRL does not exploit symbolic relational representations, it has difficulties in coping with structural changes in its environment (such as increasing the number of objects). Relational Reinforcement Learning, on the other hand, inherits the relational representations from symbolic planning to learn reusable policies. However, it has so far been unable to scale up and exploit the power of deep neural networks. We propose Deep Explainable Relational Reinforcement Learning (DERRL), a framework that exploits the best of both -- neural and symbolic worlds. By resorting to a neuro-symbolic approach, DERRL combines relational representations and constraints from symbolic planning with deep learning to extract interpretable policies. These policies are in the form of logical rules that explain how each decision (or action) is arrived at. Through several experiments, in 
    
[^107]: Adam和AdamW优化器训练的深度神经网络损失函数的Lipschitz效应对泛化性能的影响

    Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v1 [cs.LG])

    [http://arxiv.org/abs/2303.16464](http://arxiv.org/abs/2303.16464)

    本文理论证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。本文的选择损失函数方针为Adam或AdamW优化算法的使用提供了指导。实验结果表明了Lipschitz常数较低且最大值较小的损失函数可以提高模型的泛化能力。

    

    机器学习中一个主要关注点是深度神经网络的泛化性能与优化算法之间的关系。本文证明了损失函数的Lipschitz常数是降低Adam或AdamW获得输出模型的泛化误差的一个重要因素。这些结果可作为选择损失函数时优化算法为Adam或AdamW的指导方针。本文选择了计算机视觉中的人脸年龄评估问题来评估理论界限在实际环境下的表现。为了更好地评估泛化能力，训练集和测试集从不同分布中选择。实验结果表明，Lipschitz常数较低且最大值较小的损失函数可以提高Adam或AdamW训练的模型的泛化能力。

    The generalization performance of deep neural networks with regard to the optimization algorithm is one of the major concerns in machine learning. This performance can be affected by various factors. In this paper, we theoretically prove that the Lipschitz constant of a loss function is an important factor to diminish the generalization error of the output model obtained by Adam or AdamW. The results can be used as a guideline for choosing the loss function when the optimization algorithm is Adam or AdamW. In addition, to evaluate the theoretical bound in a practical setting, we choose the human age estimation problem in computer vision. For assessing the generalization better, the training and test datasets are drawn from different distributions. Our experimental evaluation shows that the loss function with lower Lipschitz constant and maximum value improves the generalization of the model trained by Adam or AdamW.
    
[^108]: DiffTAD: 使用Proposal Denoising Diffusion的时间动作检测

    DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion. (arXiv:2303.14863v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.14863](http://arxiv.org/abs/2303.14863)

    DiffTAD是一种新的时间动作检测方法，使用Proposal Denoising Diffusion的生成建模视角，并通过正向/噪声过程和反向/去噪过程实现准确的动作提议。

    

    我们提出了一种新的时间动作检测（TAD）的表达形式，即DiffTAD。它采用随机时间提议作为输入，在给定未修剪的长时间视频的情况下能够准确地产生动作提议。这是一种生成建模的视角，与先前的判别学习方式相对立。通过首先将地面真实提议扩散到随机提议（即正向/噪声过程），然后学习逆转噪声过程（即反向/去噪过程）来实现这种能力。具体来说，我们通过在Transformer解码器（如DETR）中引入具有更快收敛速度的时间位置查询设计来建立去噪过程。我们还提出了一种用于推理加速的交叉步选择性调节算法。对ActivityNet和THUMOS进行的广泛评估表明，我们的DiffTAD相比先前的替代方案表现出更高的性能。代码将在https://github.com/sauradip/Di上提供。

    We propose a new formulation of temporal action detection (TAD) with denoising diffusion, DiffTAD in short. Taking as input random temporal proposals, it can yield action proposals accurately given an untrimmed long video. This presents a generative modeling perspective, against previous discriminative learning manners. This capability is achieved by first diffusing the ground-truth proposals to random ones (i.e., the forward/noising process) and then learning to reverse the noising process (i.e., the backward/denoising process). Concretely, we establish the denoising process in the Transformer decoder (e.g., DETR) by introducing a temporal location query design with faster convergence in training. We further propose a cross-step selective conditioning algorithm for inference acceleration. Extensive evaluations on ActivityNet and THUMOS show that our DiffTAD achieves top performance compared to previous art alternatives. The code will be made available at https://github.com/sauradip/Di
    
[^109]: 条件乐观探索用于深度多智能体强化学习

    Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v1 [cs.LG])

    [http://arxiv.org/abs/2303.09032](http://arxiv.org/abs/2303.09032)

    本文提出了一种名为条件乐观探索(COE)的基于UCT的探索方法，通过结构依赖关系鼓励智能体进行基于乐观主义的协同探索。

    

    高效的探索对于协作深度多智能体强化学习(MARL)至关重要。本文提出了一种基于UCT(应用于树的置信上限)的探索方法，该方法有效地鼓励协同探索。高层次的思路是，为了进行基于乐观主义的探索，如果每个智能体的乐观估计捕获了与其他智能体的结构化依赖关系，则智能体将实现协作策略。在搜索树的每个节点（即动作）上，UCT使用通过对其父节点的访问计数进行条件推导的奖励来执行基于乐观主义的探索。我们提出了一种将MARL视为树搜索迭代的方法，并开发了一种名为条件乐观探索(COE)的方法。我们假设智能体按顺序执行动作，并将搜索树同一深度的节点视为一个单独智能体的动作。COE计算基于乐观主义的奖励，以鼓励智能体通过尝试新的行动来探索未知的状态。

    Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL). In this paper, we propose an exploration method that efficiently encourages cooperative exploration based on the idea of the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees). The high-level intuition is that to perform optimism-based exploration, agents would achieve cooperative strategies if each agent's optimism estimate captures a structured dependency relationship with other agents. At each node (i.e., action) of the search tree, UCT performs optimism-based exploration using a bonus derived by conditioning on the visitation count of its parent node. We provide a perspective to view MARL as tree search iterations and develop a method called Conditionally Optimistic Exploration (COE). We assume agents take actions following a sequential order, and consider nodes at the same depth of the search tree as actions of one individual agent. COE compute
    
[^110]: 可解释和能够干预的超声成像机器学习模型用于儿科阑尾炎

    Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis. (arXiv:2302.14460v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14460](http://arxiv.org/abs/2302.14460)

    本研究开发了可解释的机器学习模型，利用超声影像预测儿科疑似阑尾炎的诊断、管理和严重程度。模型使用了超声影像和临床、实验室数据进行训练，并推广了概念瓶颈模型到多视图和不完整概念集的预测问题。

    

    阑尾炎是儿科腹部手术的最常见原因之一。随着机器学习的最新进展，数据驱动的决策支持可以帮助临床医生诊断和管理患者，同时减少非关键手术的数量。以往的阑尾炎决策支持系统主要关注临床、实验室、评分和计算机断层扫描数据，主要忽视了腹部超声成像，这是一种非侵入性和方便获得的诊断方法。为此，我们开发和验证了利用超声影像预测疑似阑尾炎诊断、管理和严重程度的可解释机器学习模型。我们的模型使用了由579名儿科患者的1709幅超声影像，以及临床和实验室数据构成的数据集进行训练。我们的方法学贡献在于将概念瓶颈模型推广到具有多个视图和不完整概念集的预测问题。值得注意的是，这样的模型适用于干预操作。

    Appendicitis is among the most frequent reasons for pediatric abdominal surgeries. With recent advances in machine learning, data-driven decision support could help clinicians diagnose and manage patients while reducing the number of non-critical surgeries. Previous decision support systems for appendicitis focused on clinical, laboratory, scoring and computed tomography data, mainly ignoring abdominal ultrasound, a noninvasive and readily available diagnostic modality. To this end, we developed and validated interpretable machine learning models for predicting the diagnosis, management and severity of suspected appendicitis using ultrasound images. Our models were trained on a dataset comprising 579 pediatric patients with 1709 ultrasound images accompanied by clinical and laboratory data. Our methodological contribution is the generalization of concept bottleneck models to prediction problems with multiple views and incomplete concept sets. Notably, such models lend themselves to int
    
[^111]: 在网络考试中检测串通的数据挖掘方法

    A Data Mining Approach for Detecting Collusion in Unproctored Online Exams. (arXiv:2302.07014v2 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2302.07014](http://arxiv.org/abs/2302.07014)

    提出了一个在疫情期间检测学生是否串通作弊的数据挖掘方法，并通过对远程考试事件日志数据的分析发现了群体作弊行为或者异常，同时建立了一个评估异常案例的经验规则。

    

    由于COVID-19疫情期间的预防性措施，许多大学提供了无监考的远程考试。我们提出了检测学生间可能的串通的方法，并将其应用于疫情期间远程考试的事件日志数据中。我们发现了一些有着明显相似考试的学生群体，同时将我们的发现与被监考控制组的结果进行了比较。基于这些，我们建立了一个评估“极其相似”的异常案例的经验规则。

    Due to the precautionary measures during the COVID-19 pandemic many universities offered unproctored take-home exams. We propose methods to detect potential collusion between students and apply our approach on event log data from take-home exams during the pandemic. We find groups of students with suspiciously similar exams. In addition, we compare our findings to a proctored control group. By this, we establish a rule of thumb for evaluating which cases are "outstandingly similar", i.e., suspicious cases.
    
[^112]: 数据中心机器学习的重新标签法

    The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04391](http://arxiv.org/abs/2302.04391)

    本文提出了一种重新标签的方法来解决手动标记的数据中存在噪声的问题，并通过模型预测来辅助人类标记噪声数据。实验证明此方法适用于多类深度学习任务。

    

    在深度学习应用中，手动标记的数据在一定程度上存在噪声。为了解决这个问题，并在开发数据集上获得90分以上的成绩，本文提出了一种简单的方法来找出噪声数据，并通过采用模型预测作为人类标记的参考来重新标记噪声数据。本文阐述了我们在广泛的深度学习任务中的想法，包括分类、序列标记、物体检测、序列生成、点击率预测。实验结果和人类评估结果验证了我们的想法。

    In industry deep learning application, our manually labeled data has a certain number of noisy data. To solve this problem and achieve more than 90 score in dev dataset, we present a simple method to find the noisy data and re-label the noisy data by human, given the model predictions as references in human labeling. In this paper, we illustrate our idea for a broad set of deep learning tasks, includes classification, sequence tagging, object detection, sequence generation, click-through rate prediction. The experimental results and human evaluation results verify our idea.
    
[^113]: CLIPood：将CLIP泛化到超出分布

    CLIPood: Generalizing CLIP to Out-of-Distributions. (arXiv:2302.00864v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00864](http://arxiv.org/abs/2302.00864)

    本论文提出了CLIPood，一种将CLIP泛化到超出分布测试数据的方法。CLIPood引入了新的训练目标MMS和优化策略BMA，以适应OOD情况并提升性能。

    

    机器学习中的一个重要挑战是处理训练过程中分布发生变化的情况，即超出分布（OOD）泛化问题。对比语言-图像预训练（CLIP）模型展示了令人印象深刻的零样本能力，但在后续任务中进一步调整CLIP模型会不可避免地降低OOD性能。本文旨在将CLIP泛化到超出分布测试数据的后续任务中。我们提出了CLIPood，一种可以适应OOD情况的微调方法，这些情况可能包括领域转移和未知类别在未见测试数据中出现。为了利用文本模态下类别之间的语义关系，CLIPood引入了一个新的训练目标，即边距度量软最大化（MMS），其具有类别自适应边距用于微调。为了将预训练的零样本模型和微调的任务自适应模型结合起来，CLIPood采用了一种新的优化策略，即Beta移动平均（BMA），以通过Beta分布维持一个时间上的集成加权模型。

    Out-of-distribution (OOD) generalization, where the model needs to handle distribution shifts from training, is a major challenge of machine learning. Contrastive language-image pre-training (CLIP) models have shown impressive zero-shot ability, but the further adaptation of CLIP on downstream tasks undesirably degrades OOD performances. This paper aims at generalizing CLIP to out-of-distribution test data on downstream tasks. We propose CLIPood, a fine-tuning method that can adapt CLIP models to OOD situations where both domain shifts and open classes may occur on the unseen test data. To exploit the semantic relations between classes from the text modality, CLIPood introduces a new training objective, margin metric softmax (MMS), with class adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot model and fine-tuned task-adaptive model, CLIPood leverages a new optimization strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted by Beta distri
    
[^114]: 不使用边际贡献近似计算Shapley值

    Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00736](http://arxiv.org/abs/2302.00736)

    本文提出了两种无参数、领域无关的Shapley值近似算法SVARM和Stratified SVARM，它们基于一种与边际贡献概念脱钩的Shapley值表示。我们证明了它们在近似质量方面具有无与伦比的理论保证，并通过实证结果将其与合成游戏和常见可解释性用例进行了比较。

    

    Shapley值是为合作博弈中的玩家分配有意义的贡献值的最流行方法，最近在可解释的人工智能中得到了广泛应用。Shapley值的有意义性源于仅有Shapley值满足的公理属性，然而，确切计算的代价是随着玩家数量指数级增长。因此，许多研究致力于高效近似Shapley值，其中大部分围绕着玩家的边际贡献的概念。在本文中，我们提出了两种基于与边际贡献概念脱钩的Shapley值表示的无参数、领域无关的近似算法SVARM和Stratified SVARM。我们证明了它们在近似质量方面的无与伦比的理论保证，并提供了包括合成游戏和常用可解释性用例的实证结果进行比较。

    The Shapley value is arguably the most popular approach for assigning a meaningful contribution value to players in a cooperative game, which has recently been used intensively in explainable artificial intelligence. The meaningfulness is due to axiomatic properties that only the Shapley value satisfies, which, however, comes at the expense of an exact computation growing exponentially with the number of agents. Accordingly, a number of works are devoted to the efficient approximation of the Shapley values, most of them revolve around the notion of an agent's marginal contribution. In this paper, we propose with SVARM and Stratified SVARM two parameter-free and domain-independent approximation algorithms based on a representation of the Shapley value detached from the notion of marginal contributions. We prove unmatched theoretical guarantees regarding their approximation quality and provide empirical results including synthetic games as well as common explainability use cases comparin
    
[^115]: DoCoFL：用于跨设备联合学习的下行压缩

    DoCoFL: Downlink Compression for Cross-Device Federated Learning. (arXiv:2302.00543v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00543](http://arxiv.org/abs/2302.00543)

    本研究提出了DoCoFL，一种用于跨设备联合学习的下行压缩框架，能够在降低双向带宽的同时保持竞争力的准确性。

    

    许多压缩技术已被提出用于减少联合学习训练过程中的通信开销。然而，这些技术通常用于压缩模型更新，而模型更新在训练过程中会逐渐减少。因此，这些方法不适用于跨设备的下行（即从服务器到客户端）压缩，在这种场景下，异构客户端在训练期间可能只出现一次，因此必须下载模型参数。因此，我们提出了DoCoFL——一个新的用于跨设备下行压缩的框架。重要的是，DoCoFL可以无缝地与许多上行压缩方案结合使用，使其适用于双向压缩。通过广泛的评估，我们证明DoCoFL在显著降低双向带宽的同时，实现了与没有任何压缩的基准模型相当的准确性。

    Many compression techniques have been proposed to reduce the communication overhead of Federated Learning training procedures. However, these are typically designed for compressing model updates, which are expected to decay throughout training. As a result, such methods are inapplicable to downlink (i.e., from the parameter server to clients) compression in the cross-device setting, where heterogeneous clients $\textit{may appear only once}$ during training and thus must download the model parameters. Accordingly, we propose $\textsf{DoCoFL}$ -- a new framework for downlink compression in the cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined with many uplink compression schemes, rendering it suitable for bi-directional compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$ offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.
    
[^116]: 在具有随机约束的在线凸优化中零违约和赌博反馈

    Online Convex Optimization with Stochastic Constraints: Zero Constraint Violation and Bandit Feedback. (arXiv:2301.11267v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.11267](http://arxiv.org/abs/2301.11267)

    本论文提出了一种针对具有随机约束的在线凸优化的漂移加惩罚算法变体，其保证在固定迭代次数后达到了$O(\sqrt{T})$的期望遗憾和零约束违规率。同时，将算法框架扩展到了具有赌博反馈的情况下，仍然能够实现$O(\sqrt{T})$的期望遗憾和零约束违规率。

    

    本论文研究了具有随机约束的在线凸优化。我们提出了一种漂移加惩罚算法的变体，在固定的迭代次数后，它能够保证$O(\sqrt{T})$的期望遗憾和零约束违规率，这改进了普通的漂移加惩罚方法，它的约束违规率为$O(\sqrt{T})$。与普通的漂移加惩罚方法不同，我们的算法对于时间范围$T$的长度没有任何要求。这是基于我们的新型漂移引理，它提供了时间变化的虚拟队列漂移界限，从而导致了对期望虚拟队列长度的时间变化界限。此外，我们将我们的框架扩展到了具有两点赌博反馈的随机约束在线凸优化。我们表明，通过将我们的算法框架适应于赌博反馈场景，我们仍然可以实现$O(\sqrt{T})$的期望遗憾和零约束违规率，改进了先前工作中关于相同约束情况的方法。

    This paper studies online convex optimization with stochastic constraints. We propose a variant of the drift-plus-penalty algorithm that guarantees $O(\sqrt{T})$ expected regret and zero constraint violation, after a fixed number of iterations, which improves the vanilla drift-plus-penalty method with $O(\sqrt{T})$ constraint violation. Our algorithm is oblivious to the length of the time horizon $T$, in contrast to the vanilla drift-plus-penalty method. This is based on our novel drift lemma that provides time-varying bounds on the virtual queue drift and, as a result, leads to time-varying bounds on the expected virtual queue length. Moreover, we extend our framework to stochastic-constrained online convex optimization under two-point bandit feedback. We show that by adapting our algorithmic framework to the bandit feedback setting, we may still achieve $O(\sqrt{T})$ expected regret and zero constraint violation, improving upon the previous work for the case of identical constraint f
    
[^117]: 全局$k$-means$++$: 全局$k$-means聚类算法的一种有效的简化方法

    Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm. (arXiv:2211.12271v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.12271](http://arxiv.org/abs/2211.12271)

    本文提出了一种名为全局$k$-means\texttt{++}的聚类算法，通过利用中心选择概率实现对全局$k$-means的质量聚类解的有效获取，并减少了计算负载。

    

    $k$-means算法是一种常用的聚类方法，因其简单、有效和快速而受到广泛关注。然而，其主要缺点是对聚类中心的初始位置非常敏感。全局$k$-means是一种确定性算法，用于解决$k$-means的随机初始化问题，但众所周知其计算成本很高。它通过逐步解决所有$k$-means子问题，将数据划分为$K$个簇，其中$k=1,\ldots, K$。对于每个$k$簇问题，该方法执行$k$-means算法$N$次，其中$N$是数据点的数量。在本文中，我们提出了全局$k$-means\texttt{++}聚类算法，这是一种有效的获取质量聚类解的方法，与全局$k$-means相比，计算负载减少了。这是通过利用在$k$-means\texttt{++}算法中有效使用的中心选择概率来实现的。

    The $k$-means algorithm is a prevalent clustering method due to its simplicity, effectiveness, and speed. However, its main disadvantage is its high sensitivity to the initial positions of the cluster centers. The global $k$-means is a deterministic algorithm proposed to tackle the random initialization problem of k-means but its well-known that requires high computational cost. It partitions the data to $K$ clusters by solving all $k$-means sub-problems incrementally for all $k=1,\ldots, K$. For each $k$ cluster problem, the method executes the $k$-means algorithm $N$ times, where $N$ is the number of datapoints. In this paper, we propose the \emph{global $k$-means\texttt{++}} clustering algorithm, which is an effective way of acquiring quality clustering solutions akin to those of global $k$-means with a reduced computational load. This is achieved by exploiting the center selection probability that is effectively used in the $k$-means\texttt{++} algorithm. The proposed method has be
    
[^118]: $\Phi$-DVAE: 物理指导的动态变分自编码器用于非结构化数据同化

    $\Phi$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation. (arXiv:2209.15609v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.15609](http://arxiv.org/abs/2209.15609)

    本文提出了一种物理指导的动态变分自编码器 ($\Phi$-DVAE) 用于将非结构化数据同化到物理模型中，解决了传统方法在未知映射情况下无法实现一致模型与数据综合的问题。

    

    在数据同化中，将非结构化数据纳入物理模型是一个具有挑战性的问题。传统方法通常关注具有明确定义观测算子的情况，其函数形式通常被假定为已知。这阻止了这些方法在从数据空间到模型空间的映射未知的配置中实现一致的模型与数据综合。为了解决这些问题，在本文中我们开发了一种物理指导的动态变分自编码器($\Phi$-DVAE)，将多样化的数据流嵌入到由微分方程描述的时变物理系统中。我们的方法结合了一个标准的、可能是非线性的潜在状态空间模型滤波器和一个变分自编码器，将非结构化数据同化到潜在的动态系统中。在我们的示例系统中，非结构化数据采用视频数据和速度场测量的形式，但该方法的适用性足够通用，可以允许任意未知的观测算子。

    Incorporating unstructured data into physical models is a challenging problem that is emerging in data assimilation. Traditional approaches focus on well-defined observation operators whose functional forms are typically assumed to be known. This prevents these methods from achieving a consistent model-data synthesis in configurations where the mapping from data-space to model-space is unknown. To address these shortcomings, in this paper we develop a physics-informed dynamical variational autoencoder ($\Phi$-DVAE) to embed diverse data streams into time-evolving physical systems described by differential equations. Our approach combines a standard, possibly nonlinear, filter for the latent state-space model and a VAE, to assimilate the unstructured data into the latent dynamical system. Unstructured data, in our example systems, comes in the form of video data and velocity field measurements, however the methodology is suitably generic to allow for arbitrary unknown observation operat
    
[^119]: 具有低噪声的差分隐私随机梯度下降法

    Differentially Private Stochastic Gradient Descent with Low-Noise. (arXiv:2209.04188v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2209.04188](http://arxiv.org/abs/2209.04188)

    本文研究了具有低噪声的差分隐私随机梯度下降法在点问题和成对学习中的性能，并推导出更精确的过剩风险界限。提出的算法基于梯度扰动，具有优化过剩风险率的最佳效果。

    

    现代机器学习算法旨在从数据中提取细粒度信息，以提供准确的预测，但这往往与保护隐私的目标相冲突。本文探讨了开发保护隐私的机器学习算法在确保良好性能的同时保护隐私的实际和理论重要性。我们在随机凸优化设置中，关注差分隐私随机梯度下降（SGD）算法在隐私性和效用性（通过过剩风险界限衡量）方面的性能。具体而言，我们研究了低噪声设置下的点问题，并得出了差分隐私SGD算法更精确的过剩风险界限。在成对学习设置中，我们提出了一种基于梯度扰动的简单差分隐私SGD算法。此外，我们还提出了所提算法的新型效用界限，证明它即使在非光滑情况下也能达到最优的过剩风险率。

    Modern machine learning algorithms aim to extract fine-grained information from data to provide accurate predictions, which often conflicts with the goal of privacy protection. This paper addresses the practical and theoretical importance of developing privacy-preserving machine learning algorithms that ensure good performance while preserving privacy. In this paper, we focus on the privacy and utility (measured by excess risk bounds) performances of differentially private stochastic gradient descent (SGD) algorithms in the setting of stochastic convex optimization. Specifically, we examine the pointwise problem in the low-noise setting for which we derive sharper excess risk bounds for the differentially private SGD algorithm. In the pairwise learning setting, we propose a simple differentially private SGD algorithm based on gradient perturbation. Furthermore, we develop novel utility bounds for the proposed algorithm, proving that it achieves optimal excess risk rates even for non-sm
    
[^120]: 参数化偏微分方程中前向和反向问题的全概率深度模型

    Fully probabilistic deep models for forward and inverse problems in parametric PDEs. (arXiv:2208.04856v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2208.04856](http://arxiv.org/abs/2208.04856)

    本论文提出了一种基于深度学习和概率建模的全概率深度模型，用于学习参数化偏微分方程中的前向和反向映射。模型通过最大化观察到的零残差的概率来进行训练，不需要独立的训练数据。

    

    我们引入了一种物理驱动的深度潜变量模型（PDDLVM），用于同时学习参数化偏微分方程（PDEs）的参数到解（前向）和解到参数（反向）映射。我们的方法结合了传统的PDE离散化技术、深度神经网络、概率建模和变分推断，形成了一个完全概率一致的框架。在所假设的概率模型中，前向和反向映射均被近似为由深度神经网络参数化的高斯分布。我们假设PDE残差是一个观测到的随机向量，值为零，因此我们将其建模为一个均值为零、用户指定协方差的随机向量。该模型通过最大化观察到零残差的概率（即证据或边际似然）来进行训练，最大化证据下界（ELBO）。因此，该方法不需要任何独立的训练数据。

    We introduce a physics-driven deep latent variable model (PDDLVM) to learn simultaneously parameter-to-solution (forward) and solution-to-parameter (inverse) maps of parametric partial differential equations (PDEs). Our formulation leverages conventional PDE discretization techniques, deep neural networks, probabilistic modelling, and variational inference to assemble a fully probabilistic coherent framework. In the posited probabilistic model, both the forward and inverse maps are approximated as Gaussian distributions with a mean and covariance parameterized by deep neural networks. The PDE residual is assumed to be an observed random vector of value zero, hence we model it as a random vector with a zero mean and a user-prescribed covariance. The model is trained by maximizing the probability, that is the evidence or marginal likelihood, of observing a residual of zero by maximizing the evidence lower bound (ELBO). Consequently, the proposed methodology does not require any independe
    
[^121]: 机器学习中的基于排名可分解损失函数：一项综述

    Rank-based Decomposable Losses in Machine Learning: A Survey. (arXiv:2207.08768v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08768](http://arxiv.org/abs/2207.08768)

    这篇综述回顾了机器学习中基于排名可分解损失函数的研究并提供了分类法。它讨论了可分解性与优化算法之间的相互作用，并列举了排名损失在深度学习中的最新应用和面临的挑战。

    

    近期的研究揭示了损失函数设计中区分个体损失与聚合损失的重要范式。个体损失评估模型在样本上的质量，而聚合损失将每个训练样本的个体损失/分数组合起来。两者都有一个共同的过程，将一组个体值聚合成单个数字值。排名顺序反映了设计损失中个体值之间最基本的关系。此外，可分解性，其中损失可以分解为一组个体项的集合，成为组织损失/分数的重要属性。本综述系统全面地回顾了机器学习中的基于排名可分解损失函数。具体而言，我们提供了一个按聚合损失和个体损失视角的新损失函数分类法。我们确定了形成这种损失的聚合器，它们是集合函数的示例。我们组织了有监督学习和无监督学习中的基于排名的可分解损失。我们讨论了损失的可分解性与优化算法之间的相互作用。最后，我们概述了排名损失在深度学习中的最新应用和面临的挑战。

    Recent works have revealed an essential paradigm in designing loss functions that differentiate individual losses vs. aggregate losses. The individual loss measures the quality of the model on a sample, while the aggregate loss combines individual losses/scores over each training sample. Both have a common procedure that aggregates a set of individual values to a single numerical value. The ranking order reflects the most fundamental relation among individual values in designing losses. In addition, decomposability, in which a loss can be decomposed into an ensemble of individual terms, becomes a significant property of organizing losses/scores. This survey provides a systematic and comprehensive review of rank-based decomposable losses in machine learning. Specifically, we provide a new taxonomy of loss functions that follows the perspectives of aggregate loss and individual loss. We identify the aggregator to form such losses, which are examples of set functions. We organize the rank
    
[^122]: 保护隐私的张量网络机器学习

    Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2202.12319](http://arxiv.org/abs/2202.12319)

    本文展示了张量网络架构在保护隐私的机器学习中具有潜在优势，并提出了确保鲁棒性的明确条件。

    

    张量网络被广泛用于提供低能量态的高效表示，最近被提出作为机器学习架构的一种新方法。本文展示了张量网络架构在保护隐私的机器学习中具有潜在优势，这对于处理医疗记录等任务非常重要。首先，我们描述了前馈神经网络中存在的新隐私漏洞，并在合成和真实数据集中进行了说明。然后，我们提出了确保对这种漏洞具有鲁棒性的明确条件，这涉及到在规范对称性下等价的模型的刻画。我们严格证明了张量网络架构满足这些条件。在此过程中，我们定义了一种新型的矩阵乘积态的规范形式，具有高度的规律性并修正了残余规范问题。

    Tensor networks, widely used for providing efficient representations of low-energy states of local quantum many-body systems, have been recently proposed as machine learning architectures which could present advantages with respect to traditional ones. In this work we show that tensor network architectures have especially prospective properties for privacy-preserving machine learning, which is important in tasks such as the processing of medical records. First, we describe a new privacy vulnerability that is present in feedforward neural networks, illustrating it in synthetic and real-world datasets. Then, we develop well-defined conditions to guarantee robustness to such vulnerability, which involve the characterization of models equivalent under gauge symmetry. We rigorously prove that such conditions are satisfied by tensor-network architectures. In doing so, we define a novel canonical form for matrix product states, which has a high degree of regularity and fixes the residual gaug
    
[^123]: 揭开黑盒子：调控算法决策

    Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)

    [http://arxiv.org/abs/2110.03443](http://arxiv.org/abs/2110.03443)

    本文研究如何在代理使用复杂的“黑盒”预测函数进行决策的情况下，对算法决策进行最优调控。研究发现，限制代理使用透明度足够高的预测函数是低效的，而针对激励偏差源头的目标化工具可以提供次优解决方案，从而改善福利。

    

    我们展示了如何在一个代理使用复杂的“黑盒”预测函数进行决策（如贷款、医疗测试或招聘）且委托人在了解代理的黑盒模型方面有限的情况下，最优地调控预测算法。我们证明，只要诱导不足，且最优预测函数足够复杂，将代理限制在足够透明的预测函数中是低效的。算法审计有助于提高福利，但其收益取决于审计工具的设计。许多解释工具倾向于最小化整体信息损失，但这通常是低效的，因为它们集中于解释预测函数的平均行为。针对性的工具，如针对激励偏差源头（如过多的假阳性或种族差异）的工具，可以提供次优解决方案。我们提供了对我们理论的实证支持。

    We show how to optimally regulate prediction algorithms in a world where an agent uses complex 'black-box' prediction functions to make decisions such as lending, medical testing, or hiring, and where a principal is limited in how much she can learn about the agent's black-box model. We show that limiting agents to prediction functions that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and first-best prediction functions are sufficiently complex. Algorithmic audits can improve welfare, but the gains depend on the design of the audit tools. Tools that focus on minimizing overall information loss, the focus of many explainer tools, will generally be inefficient since they focus on explaining the average behavior of the prediction function. Targeted tools that focus on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical
    
[^124]: 通过随机单维搜索实现数据增强的自动化

    Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.08756](http://arxiv.org/abs/2106.08756)

    通过随机单维搜索的数据增强自动化方法，仅使用6次训练即可获得与使用100次训练相当的性能。

    

    对于深度学习研究者来说，找到训练期间最佳的数据增强策略对于取得最先进的性能而言至关重要。为此，社区已经见证了许多努力来自动化找到适合任何任务的完美增强程序的过程。然而，即使是最新的尖端方法也会带来巨大的计算开销，可能需要多达100个完整模型的训练来确定理想的配置。我们展示了如何使用仅6次训练就能达到相同的性能，通过随机单维增强方法进行数据增强。源代码可在https://github.com/fastestimator/RUA/tree/v1.0找到。

    It is no secret amongst deep learning researchers that finding the optimal data augmentation strategy during training can mean the difference between state-of-the-art performance and a run-of-the-mill result. To that end, the community has seen many efforts to automate the process of finding the perfect augmentation procedure for any task at hand. Unfortunately, even recent cutting-edge methods bring massive computational overhead, requiring as many as 100 full model trainings to settle on an ideal configuration. We show how to achieve equivalent performance using just 6 trainings with Random Unidimensional Augmentation. Source code is available at https://github.com/fastestimator/RUA/tree/v1.0
    
[^125]: 将人口和控制神经网络交替应用于解决高维度随机均场博弈问题

    Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games. (arXiv:2002.10113v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.10113](http://arxiv.org/abs/2002.10113)

    我们提出了一种交替人口和代理控制神经网络（APAC-Net）来解决高维度随机均场博弈（MFG）问题。通过利用变分原始-对偶结构和神经网络参数化价值和密度函数，我们展示了在高维度MFG问题中的潜力。

    

    我们提出了一种交替人口和代理控制神经网络（APAC-Net）来解决随机均场博弈（MFG）问题。我们的算法针对高维度的MFG实例，这些实例使用现有解决方法无法解决。我们通过两个步骤实现这一目标。首先，我们利用MFG所展示的变分原始-对偶结构，并将其形式化为凸-凹鞍点问题。其次，我们分别通过两个神经网络对价值和密度函数进行参数化。通过这种方式，解决MFG可以被解释为训练生成对抗网络（GAN）的特例。我们展示了我们的方法在高达100维MFG问题上的潜力。

    We present APAC-Net, an alternating population and agent control neural network for solving stochastic mean field games (MFGs). Our algorithm is geared toward high-dimensional instances of MFGs that are beyond reach with existing solution methods. We achieve this in two steps. First, we take advantage of the underlying variational primal-dual structure that MFGs exhibit and phrase it as a convex-concave saddle point problem. Second, we parameterize the value and density functions by two neural networks, respectively. By phrasing the problem in this manner, solving the MFG can be interpreted as a special case of training a generative adversarial network (GAN). We show the potential of our method on up to 100-dimensional MFG problems.
    
[^126]: 用有符号的迭代随机森林识别增强子相关的转录因子结合

    Signed iterative random forests to identify enhancer-associated transcription factor binding. (arXiv:1810.07287v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1810.07287](http://arxiv.org/abs/1810.07287)

    本文介绍了一种有符号的迭代随机森林（siRF）方法，用于推断Drosophila melanogaster中增强子元素周围的转录因子之间的调控相互作用和功能结合签名。

    

    标准的ChIP-seq峰值调用流程旨在区分各个基因组元素的生化可重复信号和背景噪声。然而，仅凭可重复性并不能暗示功能调控（例如增强子活化、可选剪接）。在这里，我们介绍了一种通用的、可解释的机器学习方法：有符号的迭代随机森林（siRF），我们用它来推断Drosophila melanogaster中增强子元素周围的转录因子之间的调控相互作用和功能结合签名。

    Standard ChIP-seq peak calling pipelines seek to differentiate biochemically reproducible signals of individual genomic elements from background noise. However, reproducibility alone does not imply functional regulation (e.g., enhancer activation, alternative splicing). Here we present a general-purpose, interpretable machine learning method: signed iterative random forests (siRF), which we use to infer regulatory interactions among transcription factors and functional binding signatures surrounding enhancer elements in Drosophila melanogaster.
    

