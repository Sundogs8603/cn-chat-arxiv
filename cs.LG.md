# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Hyperbolic Image-Text Representations.](http://arxiv.org/abs/2304.09172) | 本文提出了一个使用双曲表示捕捉图像和文本层次结构的对比模型MERU，并证明其在多模态任务上与CLIP相当。 |
| [^2] | [Optimal PAC Bounds Without Uniform Convergence.](http://arxiv.org/abs/2304.09167) | 本文通过提供框架，将置换不变预测器的留一出错转化为高概率风险界限，实现了为二元分类实现最优 PAC 界限的算法。 |
| [^3] | [Structure Preserving Cycle-GAN for Unsupervised Medical Image Domain Adaptation.](http://arxiv.org/abs/2304.09164) | 本文提出了一种结构保持的循环 GAN 模型来改善无监督医学图像域自适应的分割性能，通过在循环 GAN 训练过程中增加分割损失项以保留感兴趣的结构，这种模型在未见过的图像域上实现了更好的分割性能。 |
| [^4] | [Robust Calibrate Proxy Loss for Deep Metric Learning.](http://arxiv.org/abs/2304.09162) | 本研究提出了一种新的 Calibrate Proxy 结构，通过利用实际样本信息改善了基于代理的损失相似性计算，引入一个校准损失来约束代理优化方向类别特征中心。实验证明了该方法在多个数据集上都取得了较好表现。 |
| [^5] | [Detection and Classification of Glioblastoma Brain Tumor.](http://arxiv.org/abs/2304.09133) | 本文提出了两个深度学习模型，UNet和Deeplabv3，用于早期检测和分割胶质母细胞瘤脑肿瘤。实验结果表明，这两个模型都表现出了准确检测和分割胶质母细胞瘤脑肿瘤的能力。但Deeplabv3在准确性方面表现更优，需要更多的计算资源。 |
| [^6] | [Variational Relational Point Completion Network for Robust 3D Classification.](http://arxiv.org/abs/2304.09131) | 本文提出了一个称为 VRCNet 的网络，具有两个优点：1)概率建模；2）关系增强。此网络可以用于点云补全，能够提高点云几何建模和感知的表现。 |
| [^7] | [Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics.](http://arxiv.org/abs/2304.09123) | 本文提供了有限时间界限，用于被动随机梯度 Langevin 动力学算法，该算法可用于逆强化学习。该算法充当随机采样器，恢复用外部过程优化而来的成本函数。 |
| [^8] | [Practical Lessons on Optimizing Sponsored Products in eCommerce.](http://arxiv.org/abs/2304.09107) | 本文提出了一个实用的机器学习框架，可以解决电商广告系统中赞助产品优化的多个问题，而不需要改变现有机器学习模型结构。使用该框架可以处理广告系统中的长期问题，并为多个评估指标带来增量。 |
| [^9] | [Exploring 360-Degree View of Customers for Lookalike Modeling.](http://arxiv.org/abs/2304.09105) | 该论文提出了一个能将客户360度视角的不同行为或特征，在此基础上构建出类似模型以改善客户定位的框架。实验结果表明，该模型在电商和旅游领域中能够有效发挥作用。 |
| [^10] | [Real Time Bearing Fault Diagnosis Based on Convolutional Neural Network and STM32 Microcontroller.](http://arxiv.org/abs/2304.09100) | 本研究提出了一种利用卷积神经网络与STM32微控制器结合进行实时轴承故障诊断的方法，通过优化后的模型诊断准确率可达98.9%。同时，还成功将该方法应用于STM32H743VI微控制器，使得每次诊断的运行时间缩短至19ms，为轴承故障诊断提供了更高效的技术手段。 |
| [^11] | [MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices.](http://arxiv.org/abs/2304.09099) | 该论文提出和实施了一个名为MATURE-HEALTH的健康推荐系统，该系统能够预测电解质不平衡并推荐营养平衡的食物，从而增加早期检测疾病的机会并防止健康进一步恶化。 |
| [^12] | [Sheaf Neural Networks for Graph-based Recommender Systems.](http://arxiv.org/abs/2304.09097) | 基于Sheaf神经网络的模型提出了一种新的向量空间表示方法，使得其在基准推荐任务上获得最先进的性能表现。 |
| [^13] | [Privacy-Preserving Matrix Factorization for Recommendation Systems using Gaussian Mechanism.](http://arxiv.org/abs/2304.09096) | 本文提出了一种基于差分隐私框架和矩阵分解的保护隐私推荐系统，采用输出扰动的高斯机制实现差分隐私，通过Rényi差分隐私对整体隐私损失进行特征化，在保护用户隐私的同时实现了推荐系统功能。 |
| [^14] | [Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation.](http://arxiv.org/abs/2304.09093) | KLEVER是一个新的CRS框架，可以将物品和它们相关的上下文单词联合建模在同一语义空间中，解决了以前工作中的物品和单词语义空间不对齐的问题。 |
| [^15] | [A Field Test of Bandit Algorithms for Recommendations: Understanding the Validity of Assumptions on Human Preferences in Multi-armed Bandits.](http://arxiv.org/abs/2304.09088) | 本研究采用漫画推荐MAB设置，与众包工作者对关键MAB假设的有效性进行研究，结果挑战了这些假设，强调了需要进行更多关于MAB推荐中这些假设的实证工作。 |
| [^16] | [MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed.](http://arxiv.org/abs/2304.09087) | 本研究提出了一种名为MDDL的多通道深度确定性策略梯度学习框架，旨在整合多种策略，以增强位置分配的强化学习模型训练。该框架在在线和离线性能方面表现优于一些最先进的方法。 |
| [^17] | [Balancing Unobserved Confounding with a Few Unbiased Ratings in Debiased Recommendations.](http://arxiv.org/abs/2304.09085) | 本文提出了利用无偏评分平衡已有的去偏方法来对抗未观测混淆和模型规范误差的方法。 |
| [^18] | [DRIFT: A Federated Recommender System with Implicit Feedback on the Items.](http://arxiv.org/abs/2304.09084) | DRIFT是一个联邦推荐系统架构，使用隐式反馈，以保护用户隐私，并使用SAROS算法实现精准推荐。 |
| [^19] | [Electrical Impedance Tomography with Deep Calder\'on Method.](http://arxiv.org/abs/2304.09074) | Calderón方法是一种快速的EIT成像算法，但图像模糊且低估电导率值。该论文基于U-net模型对Calderón方法的图像进行后处理，提高了图像分辨率和电导率估计的准确性。 |
| [^20] | [M-ENIAC: A machine learning recreation of the first successful numerical weather forecasts.](http://arxiv.org/abs/2304.09070) | 本研究使用基于机器学习的求解器重新创建了第一次成功的数值天气预报，并证明物理学相关的神经网络方法相较于以前的标准求解器更为准确且简便。 |
| [^21] | [METAM: Goal-Oriented Data Discovery.](http://arxiv.org/abs/2304.09068) | METAM是一种目标导向的数据发现框架，能够自动引导发现和增强过程，提高机器学习和因果推断任务性能。 |
| [^22] | [Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction.](http://arxiv.org/abs/2304.09062) | 本文提出了一种面向CTR预测的漂移感知增量学习框架。该框架基于集成学习，通过显式的基于误差的漂移检测来解决CTR预测中的灾难性遗忘问题。 |
| [^23] | [A Scalable Framework for Automatic Playlist Continuation on Music Streaming Services.](http://arxiv.org/abs/2304.09061) | 本论文提出了一个通用框架，它基于表示-聚合策略构建可扩展但有效的APC模型，用于大规模应用，可以包括各种表示学习和序列建模技术。 |
| [^24] | [Revisiting k-NN for Pre-trained Language Models.](http://arxiv.org/abs/2304.09058) | 本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。 |
| [^25] | [Decoding Neural Activity to Assess Individual Latent State in Ecologically Valid Contexts.](http://arxiv.org/abs/2304.09050) | 此研究旨在在更为真实的环境中解码人类神经活动模式，以进一步理解复杂任务期间个体内部潜在状态，该方法能够验证实验室方法并提供有意义的洞察。 |
| [^26] | [DeepGEMM: Accelerated Ultra Low-Precision Inference on CPU Architectures using Lookup Tables.](http://arxiv.org/abs/2304.09049) | DeepGEMM提出一种使用查找表在CPU体系结构上加速超低精度推断的方法，用于执行超低精度卷积神经网络。 |
| [^27] | [CodeKGC: Code Language Model for Generative Knowledge Graph Construction.](http://arxiv.org/abs/2304.09048) | 本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。 |
| [^28] | [Neural Lumped Parameter Differential Equations with Application in Friction-Stir Processing.](http://arxiv.org/abs/2304.09047) | 本文提出使用通用微分方程（UDE）构建数据驱动模型以简化系统动力学到一个集中参数。应用于摩擦搅拌焊接中的沉头和停留阶段的建模任务中。 |
| [^29] | [CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows.](http://arxiv.org/abs/2304.09010) | 本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。 |
| [^30] | [Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks.](http://arxiv.org/abs/2304.08996) | 本文针对联邦学习在无线网络上通信受限、收敛速度慢和资源有限等问题，提出了一种基于年龄和资源分配的客户端选择方案，旨在最小化每轮联邦学习的总时间消耗，从而提高联邦学习的性能。 |
| [^31] | [Parcel3D: Shape Reconstruction from Single RGB Images for Applications in Transportation Logistics.](http://arxiv.org/abs/2304.08994) | 本文提出了一个新的Parcel3D数据集来解决单张RGB图像的物流包裹形状重建问题，并且提出了一种新的算法CubeRefine R-CNN来检测包裹是否受到损坏，该算法在Parcel3D数据集上表现出很好的性能。 |
| [^32] | [Robustness of Visual Explanations to Common Data Augmentation.](http://arxiv.org/abs/2304.08984) | 本文研究了深度神经网络可视化解释对自然发生的转换（增强）的响应，发现不同解释方法的稳定性存在显着差异，证明解释相较于分类性能更容易受到增强的影响。 |
| [^33] | [In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT.](http://arxiv.org/abs/2304.08979) | 本文首次对ChatGPT在通用问答场景中的可靠性进行了大规模测量，发现其在不同领域的可靠性有所差异，尤其在法律和科学问题方面表现不佳，并容易受到对抗性示例的影响，对其在实际应用中的使用产生影响。 |
| [^34] | [Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs.](http://arxiv.org/abs/2304.08968) | LLMs在公众中广泛应用，但是目前大部分检测工具存在严重缺陷。研究发现，LLMs容易微调且难以被其他LLMs检测到。 |
| [^35] | [Generative modeling of living cells with SO(3)-equivariant implicit neural representations.](http://arxiv.org/abs/2304.08960) | 本文提出了使用有符号距离函数作为形状表示，通过神经网络计算所得，对旋转具有等变性，来生成逼真的活细胞模型，为生物医学成像中的数据驱动细胞跟踪和分割方法提供高质量训练数据集。 |
| [^36] | [From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation.](http://arxiv.org/abs/2304.08953) | 本文研究了符号音乐生成中的子词分词技术在生成更长、更具结构的音乐方面的有效性。实验结果表明BPE方法在符号音乐生成中可行且有效。 |
| [^37] | [Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning.](http://arxiv.org/abs/2304.08944) | 本文提出了一种主动奖励学习的强化学习算法，采用人机交互来指定任务的奖励，大大减少了所需的人类反馈，并在效率上提供了理论保证。 |
| [^38] | [ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees.](http://arxiv.org/abs/2304.08928) | ProGAP是一种新的差分隐私GNN模型，采用逐步训练方案来提高准确性和隐私之间的平衡，通过将GNN分成一系列重叠的子模型来训练。这种方法可以保护隐私并允许有效学习图形结构数据。 |
| [^39] | [Understand Data Preprocessing for Effective End-to-End Training of Deep Neural Networks.](http://arxiv.org/abs/2304.08925) | 本文主要研究公共云环境下DNN训练的数据预处理流程，并发现数据预处理是训练的瓶颈，提出优化方法来解决瓶颈问题。 |
| [^40] | [Quantum Annealing for Single Image Super-Resolution.](http://arxiv.org/abs/2304.08924) | 本文利用量子算法解决单张图像超分辨率问题，特别是采用量子退火优化算法在低分辨率空间中寻找高分辨率图像。 |
| [^41] | [Pose Constraints for Consistent Self-supervised Monocular Depth and Ego-motion.](http://arxiv.org/abs/2304.08916) | 该论文提出了一种使用姿态约束的方法来实现一致的自监督单目深度估计和自我运动估计，并成功地减少了深度不一致性和提高了预测性能。 |
| [^42] | [Differentiable Genetic Programming for High-dimensional Symbolic Regression.](http://arxiv.org/abs/2304.08915) | 本文首次提出了DGP方法，利用可微分符号树构建遗传编程树，有效解决了高维符号回归问题。 |
| [^43] | [A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization.](http://arxiv.org/abs/2304.08914) | 本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。 |
| [^44] | [Safe reinforcement learning with self-improving hard constraints for multi-energy management systems.](http://arxiv.org/abs/2304.08897) | 本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。 |
| [^45] | [Parameterized Neural Networks for Finance.](http://arxiv.org/abs/2304.08883) | 本文介绍了一种神经网络架构，可以学习一组不同数据样本的模型类，通过调整较少的参数即可为新的、特定的问题建模，该方法在资产管理和银行业有广泛的应用潜力。 |
| [^46] | [Segmentation of glioblastomas in early post-operative multi-modal MRI with deep neural networks.](http://arxiv.org/abs/2304.08881) | 深度神经网络方法在多中心MRI数据集上实现了高性能的胶质母细胞瘤残留肿瘤分割，可提高残留肿瘤的准确估计。 |
| [^47] | [NPS: A Framework for Accurate Program Sampling Using Graph Neural Network.](http://arxiv.org/abs/2304.08880) | NPS是一种使用图神经网络进行程序采样的框架，通过学习执行嵌入并快速生成代表性模拟点，实现了高效的微处理器设计。 |
| [^48] | [Romanization-based Large-scale Adaptation of Multilingual Language Models.](http://arxiv.org/abs/2304.08865) | 该论文探索利用大规模转写来提升大型多语言预训练语言模型的处理低资源和未知语言的能力，利用UROMAN转写工具的潜力，并研究了一系列高效的策略，以适应各种语言数据。 |
| [^49] | [A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift.](http://arxiv.org/abs/2304.08855) | 本文实验评估了传统机器学习模型在协变量转移存在的情况下的性能。根据实验分析，随机森林是一种对协变量移位较不敏感的模型。 |
| [^50] | [BadVFL: Backdoor Attacks in Vertical Federated Learning.](http://arxiv.org/abs/2304.08847) | 本文聚焦于竖直联邦学习中的后门攻击的鲁棒性问题，提出了一种新的后门攻击框架BadVFL，可以有效地将后门注入VFL的训练过程中，成功率高并且误分类率很低。 |
| [^51] | [Feasible Policy Iteration.](http://arxiv.org/abs/2304.08845) | 可行性策略迭代 (FPI) 是一个间接的安全强化学习方法，使用上一个策略的可行域来迭代地限制当前策略。可行性策略改进是其核心，它在可行域内最大化回报，在可行域外最小化约束衰减函数 (CDF). |
| [^52] | [UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite.](http://arxiv.org/abs/2304.08842) | 该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。 |
| [^53] | [Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems.](http://arxiv.org/abs/2304.08841) | 本文提出了一个用于图反问题中的源定位的两阶段去噪扩散模型，通过两个阶段的迭代去噪，在保证精度的同时赋予计算效率。 |
| [^54] | [Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems Using Neural Network-Based Observers.](http://arxiv.org/abs/2304.08837) | 本文介绍了一种基于神经网络的观测器方法，可用于检测和隔离工业系统中的传感器故障，适用于一般的自主非线性系统，通过学习实现Lueneberger观察器的设计，通过残留生成检测传感器故障并实现故障隔离。 |
| [^55] | [TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models.](http://arxiv.org/abs/2304.08821) | 本论文提出了一种名为TTIDA的生成式数据增强方法，利用文本到文本和文本到图像模型生成可控的逼真标记图像。 |
| [^56] | [Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models.](http://arxiv.org/abs/2304.08818) | 本文提出了一种高分辨率视频合成方法，通过引入时间维度并在图像序列上微调扩散模型，实现了对实际驾驶数据的模拟和创意内容创作的的良好效果。 |
| [^57] | [Towards the Transferable Audio Adversarial Attack via Ensemble Methods.](http://arxiv.org/abs/2304.08811) | 本文通过研究深度学习语音识别的对抗样本传递能力，提出了随机梯度集成和动态梯度加权集成这两种新的集成方法，并发现语音和图像在对抗攻击传递上存在显着差异。 |
| [^58] | [Implicit representation priors meet Riemannian geometry for Bayesian robotic grasping.](http://arxiv.org/abs/2304.08805) | 该研究利用隐式表示构建了场景相关先验，从而在不规则环境中利用基于模拟的高效贝叶斯推理算法成功识别抓取姿态。 |
| [^59] | [Large-scale Dynamic Network Representation via Tensor Ring Decomposition.](http://arxiv.org/abs/2304.08798) | 本文提出了一种基于张量环分解的模型，用于大规模动态网络的有效表示学习。在两个真实的LDN实验中，该方法比现有的模型具有更高的准确性。 |
| [^60] | [Cooperative Multi-Agent Reinforcement Learning for Inventory Management.](http://arxiv.org/abs/2304.08769) | 本文提出了一种用于库存管理的多智能体合作强化学习系统，包括自定义GPU并行环境和分享奖励机制，通过分散式Actor-Critic方法进行培训。在供应链场景中，与传统的单一智能体RL解决方案相比，多智能体系统具有明显的优势。 |
| [^61] | [W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting.](http://arxiv.org/abs/2304.08754) | 本文介绍了一种名为 W-MAE 的预训练天气模型，它使用遮蔽自编码器重建气象变量之间的空间相关性，并通过微调预测气象变量的未来状态，从而对天气数据中存在的时空依赖关系进行建模。 |
| [^62] | [Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control with Action Constraints.](http://arxiv.org/abs/2304.08743) | 本研究提出基准测试评估带操作约束的强化学习算法在多种机器人控制环境中的表现，并公开GitHub代码，为未来的研究和开发提供参考。 |
| [^63] | [Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets.](http://arxiv.org/abs/2304.08742) | 本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。 |
| [^64] | [Estimating Joint Probability Distribution With Low-Rank Tensor Decomposition, Radon Transforms and Dictionaries.](http://arxiv.org/abs/2304.08740) | 本文提出了一种用低秩张量分解、Radon变换和字典估算联合概率分布的方法，通过使用1-D边际进行重建获得了更好的样本复杂度，并在实验中表现优于以前的基于字典的方法和高斯混合模型（GMM）。 |
| [^65] | [Do humans and machines have the same eyes? Human-machine perceptual differences on image classification.](http://arxiv.org/abs/2304.08733) | 本文研究通过图像分类探究了人机感知差异，发现即使准确率相似，人类和机器的答案分布也可能不同，并提出了一种后期人机合作来提高任务表现。 |
| [^66] | [EfficientNet Algorithm for Classification of Different Types of Cancer.](http://arxiv.org/abs/2304.08715) | 本文用EfficientNet算法分类不同类型的癌症，实验结果表明该算法在公共数据集上表现优异，具有在临床实践中提高癌症诊断准确性和效率的潜力。 |
| [^67] | [Impossibility of Characterizing Distribution Learning -- a simple solution to a long-standing problem.](http://arxiv.org/abs/2304.08712) | 本文解答了长期存在的问题：没有一种参数可以刻画分布类的PAC可学习性。同时，我们还展示了不存在一种刻画可学习性的性质来满足分布类以及其他学习问题的要求。 |
| [^68] | [LTC-SE: Expanding the Potential of Liquid Time-Constant Neural Networks for Scalable AI and Embedded Systems.](http://arxiv.org/abs/2304.08691) | LTC-SE是一种液态时常神经网络算法，将多种神经元模型统一，其增强版专注于灵活性、兼容性和代码组织，满足嵌入式系统的性能要求，扩展了液态神经网络在可扩展人工智能和嵌入式系统中的适用性。 |
| [^69] | [Semi-supervised Learning of Pushforwards For Domain Translation & Adaptation.](http://arxiv.org/abs/2304.08673) | 本论文提出一种新颖的半监督推进映射学习算法，利用归一化流来解决现有方法中存在的应用空间、样本外数据点可应用性、对两个空间的概率模型进行建模等问题，可应用于图像到图像和文本到文本转换以及分类模型的领域自适应。 |
| [^70] | [An end-to-end, interactive Deep Learning based Annotation system for cursive and print English handwritten text.](http://arxiv.org/abs/2304.08670) | 本文提出了一个端到端、交互式的手写英文文本注释系统，解决了手写文本数据稀缺的问题，并能够有效提高手写文本识别模型的识别准确率。 |
| [^71] | [Continuous Versatile Jumping Using Learned Action Residuals.](http://arxiv.org/abs/2304.08663) | 本文提出了一个层次化框架，将最优控制和强化学习结合起来，为四足机器人学习连续跳跃动作。通过学习动作剩余值，在模拟和真实环境中实现了多功能、连续的跳跃动作。 |
| [^72] | [In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion.](http://arxiv.org/abs/2304.08658) | 本研究利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力，可以实时预测每个沃克塞尔中的气孔存在，是一个重大飞跃。 |
| [^73] | [On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study.](http://arxiv.org/abs/2304.08653) | 本研究对不同最先进的概率学习方法在提高神经摘要模型不确定性质量和生成效果方面进行了调查和对比，结果表明概率方法能够持续提高生成和不确定性质量，实现了高质量生成和放弃低质量摘要，且揭示了显著的失效模式。 |
| [^74] | [Classification of US Supreme Court Cases using BERT-Based Techniques.](http://arxiv.org/abs/2304.08649) | 本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。 |
| [^75] | [TAP: A Comprehensive Data Repository for Traffic Accident Prediction in Road Networks.](http://arxiv.org/abs/2304.08640) | 本研究通过构建基于真实世界的TAP数据库,提供了两个任务:事故发生预测和事故严重程度预测,以用于各种交通相关研究，比较重要的创新是引入了图结构信息，使得机器学习方法不再只着眼于单个位置的交通事故预测。 |
| [^76] | [pgmpy: A Python Toolkit for Bayesian Networks.](http://arxiv.org/abs/2304.08639) | pgmpy是一个Python工具包，它提供了用于处理贝叶斯网络和相关模型的算法和工具，并侧重于易于扩展性，使得用户能够快速地修改、添加或实现新算法。 |
| [^77] | [An Evaluation on Large Language Model Outputs: Discourse and Memorization.](http://arxiv.org/abs/2304.08637) | 评估了九个大语言模型的输出，发现其中80％包含记忆数据，但包含最多记忆内容的输出更可能是高质量的。提出了缓解策略以降低记忆文本率。 |
| [^78] | [Signal Processing Grand Challenge 2023 -- e-Prevention: Sleep Behavior as an Indicator of Relapses in Psychotic Patients.](http://arxiv.org/abs/2304.08614) | 研究探讨了利用睡眠行为特征在无监督机器学习设置下估计精神病患者复发日的方法，并发现短时睡眠行为特征性能更好。 |
| [^79] | [Bridging Discrete and Backpropagation: Straight-Through and Beyond.](http://arxiv.org/abs/2304.08612) | 本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。 |
| [^80] | [Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives.](http://arxiv.org/abs/2304.08602) | 本文探讨了在智能电网中应用联邦学习以保护消费者隐私和提高数据模型预测性能的优势和缺点，并提供了可能的数据分区、通信拓扑和安全机制分类方法。同时，本文总结了该技术面临的主要挑战和未来发展方向。 |
| [^81] | [eTOP: Early Termination of Pipelines for Faster Training of AutoML Systems.](http://arxiv.org/abs/2304.08597) | eTOP框架可以在任何AutoML系统之上工作，并决定是否将执行管道到最后或在中间步骤终止以更快地训练模型。 |
| [^82] | [Forecasting with Sparse but Informative Variables: A Case Study in Predicting Blood Glucose.](http://arxiv.org/abs/2304.08593) | 该论文提出了一种利用稀疏但信息丰富的变量进行预测的方法，通过编码器/解码器预测方法准确地学习辅助信号的作用，为预测血糖等时间序列预测提供了新思路。 |
| [^83] | [Fast and Straggler-Tolerant Distributed SGD with Reduced Computation Load.](http://arxiv.org/abs/2304.08589) | 本文提出了一种基于模型的分布式SGD算法方案，通过适应算法的运行时间内的工作节点数量和计算负载，优化收敛速度同时降低计算负载。 |
| [^84] | [CAM2: Conformity-Aware Multi-Task Ranking Model for Large-Scale Recommender Systems.](http://arxiv.org/abs/2304.08562) | CAM2是一个面向大规模推荐系统的一致性感知多任务排名模型，通过利用因果建模系统地解开用户对流行物品的一致性与他们真正兴趣的联系，来消除历史用户交互数据带来的一致性偏见，并在实践中得到有效的应用。 |
| [^85] | [Stochastic Subgraph Neighborhood Pooling for Subgraph Classification.](http://arxiv.org/abs/2304.08556) | 该论文提出了一种随机子图邻域汇聚方法，可以在保持精度的情况下解决子图分类问题的可扩展性问题，并实现了最先进的性能。 |
| [^86] | [A Scalable Test Problem Generator for Sequential Transfer Optimization.](http://arxiv.org/abs/2304.08503) | STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。 |
| [^87] | [A comparison between Recurrent Neural Networks and classical machine learning approaches In Laser induced breakdown spectroscopy.](http://arxiv.org/abs/2304.08500) | 本文比较了递归神经网络和传统机器学习方法在激光诱导击穿光谱技术中的应用，结果表明，基于递归神经网络的模型，特别是LSTM和GRU，提供比传统机器学习模型更好的预测结果。 |
| [^88] | [Ranking Loss and Sequestering Learning for Reducing Image Search Bias in Histopathology.](http://arxiv.org/abs/2304.08498) | 本文提出两个新颖的想法，分别采用排名损失函数和交错学习的方法，避免了分类误差和模型内部偏见，以提高图像搜索性能。 |
| [^89] | [Model-Driven Quantum Federated Learning (QFL).](http://arxiv.org/abs/2304.08496) | 提出了一种基于模型驱动的方法，该方法可以为从业人员提供抽象层，以便高效地进行软件开发和数据科学任务，并支持量子联邦学习（QFL）。 |
| [^90] | [Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Swarms in Autonomous Mobile Access Applications.](http://arxiv.org/abs/2304.08493) | 本论文提出了一种集中式训练和分布式执行的多智能体深度强化学习方法，用于协调控制多个无人机在自主移动接入应用中，最大化服务质量。 |
| [^91] | [Evil from Within: Machine Learning Backdoors through Hardware Trojans.](http://arxiv.org/abs/2304.08411) | 本文介绍了一种在常见机器学习硬件加速器内的后门攻击方法，将最小后门概念和可配置的硬件木马结合使用，从而对目前的防御措施构成挑战。 |
| [^92] | [Multimodal Short Video Rumor Detection System Based on Contrastive Learning.](http://arxiv.org/abs/2304.08401) | 本研究基于对比学习设计出一个多模态短视频谣言检测系统，通过构建具有多种特征的短视频数据集和使用多模态特征融合与外部知识，能有效地区分短视频谣言。 |
| [^93] | [Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'.](http://arxiv.org/abs/2304.08297) | 对陈等人发表在《自然—生物医学工程》杂志上的“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”一文的评论和关切。 |
| [^94] | [Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach.](http://arxiv.org/abs/2304.08134) | 本文研究了临近边缘案例的面部验证问题，发现结合人机决策可以进一步提高最先进的面部验证系统在各种基准数据集上的性能。 |
| [^95] | [A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer.](http://arxiv.org/abs/2304.07874) | 本文提出了一种基于Vision Transformer的数据中心的解决方案，用于解决非均质去雾问题。传统方法在处理NH-HAZE23数据集等非均质雾图像时存在问题，因为它们无法满足建模均质雾所需的假设之一。同时，本文指出光靠数据增广并不能解决问题，因为需要处理分布差异。 |
| [^96] | [Learning Empirical Bregman Divergence for Uncertain Distance Representation.](http://arxiv.org/abs/2304.07689) | 本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。 |
| [^97] | [Machine Learning Research Trends in Africa: A 30 Years Overview with Bibliometric Analysis Review.](http://arxiv.org/abs/2304.07542) | 本文对非洲地区机器学习的最新发展和相关应用进行了广泛的文献调查及关键的文献计量分析，结果显示了机器学习研究和应用的当前现状及未来趋势，以促进未来的合作研究和知识交流。 |
| [^98] | [Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning.](http://arxiv.org/abs/2304.06907) | 本文提出了一种基于耦合字典学习和边缘化损失函数的实时图像注释方法，该方法能够学习有限数量的视觉原型和相应的语义，并保持标签的稀疏不平衡性，取得了良好的实验效果。 |
| [^99] | [Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs.](http://arxiv.org/abs/2304.06336) | 本文提出了一个AMOGCN模型，它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径，并使用节点属性评价监督。其能够有效地从异构图中发现有区别的节点嵌入和关系。 |
| [^100] | [CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning.](http://arxiv.org/abs/2304.05949) | 本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。 |
| [^101] | [Neural Network Architectures.](http://arxiv.org/abs/2304.05133) | 这份讲义概述了神经网络的数学视角，并介绍了前馈神经网络、卷积神经网络、残差网络和循环神经网络等架构，这些架构给机器学习提供了一种优化问题的思路。 |
| [^102] | [NeRF applied to satellite imagery for surface reconstruction.](http://arxiv.org/abs/2304.04133) | 本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。 |
| [^103] | [Agnostic proper learning of monotone functions: beyond the black-box correction barrier.](http://arxiv.org/abs/2304.02700) | 本文提出了第一个无偏、高效、适当的单调布尔函数学习算法，算法的运行时间和假设的大小和评估时间都为$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$，该算法解决了样本高效算法无法解决的问题。 |
| [^104] | [Rethinking the Trigger-injecting Position in Graph Backdoor Attack.](http://arxiv.org/abs/2304.02277) | 论文研究了在图神经网络中的背门攻击，发现在样本的最不重要区域中注入触发器的背门攻击效果更好，对该现象进行了解释。 |
| [^105] | [How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis.](http://arxiv.org/abs/2304.01545) | 本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。 |
| [^106] | [Charting the Topography of the Neural Network Landscape with Thermal-Like Noise.](http://arxiv.org/abs/2304.01335) | 本文采用统计力学方法研究全连接神经网络的优化问题，发现在低损失区域存在一个低维流形，并由靠近分类决策边界的数据点数量决定维度。 |
| [^107] | [Feature Engineering Methods on Multivariate Time-Series Data for Financial Data Science Competitions.](http://arxiv.org/abs/2303.16117) | 针对金融数据科学竞赛，本研究尝试采用多元时间序列特征工程方法，利用美国市场价格数据进行测试，并验证其在Numerai-Signals目标上的预测能力。 |
| [^108] | [Robust Risk-Aware Option Hedging.](http://arxiv.org/abs/2303.15216) | 本研究利用健壮的风险感知强化学习算法，优化期权对冲策略，特别应用于界限期权对冲，随着代理风险偏好变化，对冲策略发生扭曲，鲁棒策略优于非鲁棒策略。 |
| [^109] | [Dealing With Heterogeneous 3D MR Knee Images: A Federated Few-Shot Learning Method With Dual Knowledge Distillation.](http://arxiv.org/abs/2303.14357) | 本文提出了一种具有双重知识蒸馏的联邦少样本学习方法，利用公共数据库的知识来缓解私人标注图像的短缺，并通过有限的标注数据、无监督学习和双重知识蒸馏，达到优于现有最先进方法的结果。 |
| [^110] | [Fault Detection via Occupation Kernel Principal Component Analysis.](http://arxiv.org/abs/2303.11138) | 本文提出了一种使用占据核PCA方法进行故障检测的新方法，并且通过数值模拟验证了其有效性。 |
| [^111] | [The NCI Imaging Data Commons as a platform for reproducible research in computational pathology.](http://arxiv.org/abs/2303.09354) | 国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。 |
| [^112] | [Diagnosing Model Performance Under Distribution Shift.](http://arxiv.org/abs/2303.02011) | 本研究提出一种名为 DISDE 的方法，用于分析模型在不同分布情况下的性能变化。该方法将性能下降分解为三个方面：难度更大但更频繁出现的示例增加、特征和结果之间关系的变化和在训练期间不频繁或未见过的示例性能差。 |
| [^113] | [Evolutionary Computation in Action: Feature Selection for Deep Embedding Spaces of Gigapixel Pathology Images.](http://arxiv.org/abs/2303.00943) | 本论文提出了一种动态演化算法，采用大规模多目标优化方法选取百万像素病理图片中的特征，构建频繁特征直方图（FFH）WSI表示，以提高WSI图像处理的效率。 |
| [^114] | [A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization.](http://arxiv.org/abs/2302.08766) | 该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。 |
| [^115] | [Discovering sparse hysteresis models for smart materials.](http://arxiv.org/abs/2302.05313) | 本文提出了一种利用机器学习中的稀疏回归技术建模智能材料滞后的方法，并成功对压电材料的滞后现象进行建模和预测。同时在磁性材料方面提供了稀疏白盒建模滞后的见解。 |
| [^116] | [Star-Shaped Denoising Diffusion Probabilistic Models.](http://arxiv.org/abs/2302.05259) | 创新点在于提出了一种非马尔可夫扩散噪声过程的星形降噪扩散概率模型，能够广泛适用于指数族中的多种分布，特别适用于约束流形上的数据。 |
| [^117] | [Neural Common Neighbor with Completion for Link Prediction.](http://arxiv.org/abs/2302.00890) | 提出了神经通用邻居模型（NCN）用于链接预测，使用可学习的成对表示来捕捉节点之间的成对关系，以提高性能，同时解决链路不完整问题。 |
| [^118] | [MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs.](http://arxiv.org/abs/2302.00735) | 本文介绍了一种基于图的概率多智能体轨迹预测模型MTP-GO。该模型利用时间图神经网络编码场景，采用神经常微分方程实现运动模型，并结合混合密度网络和卡尔曼滤波实现多模态概率预测，在多个指标上优于其他最先进的方法。 |
| [^119] | [Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces.](http://arxiv.org/abs/2301.13088) | 本文开发了构建非欧几里得空间上静止高斯过程的实用技术，能够对定义在这些空间上的先验和后验高斯过程进行实际采样和计算协方差核。 |
| [^120] | [AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems.](http://arxiv.org/abs/2301.09262) | 本文提出一种利用记忆技术加速自注意力机制的Transformer模型的推理过程的方法，该方法可以在不需要修改模型架构或使用特殊硬件的情况下进行，并可以使推理延迟降低22%。 |
| [^121] | [Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious.](http://arxiv.org/abs/2301.07016) | 通过层级绑定和联想检索变为短期和长期声明性记忆的在线预测处理系统可能会感知到自己具有意识。 |
| [^122] | [Almost Surely $\sqrt{T}$ Regret Bound for Adaptive LQR.](http://arxiv.org/abs/2301.05537) | 本文提出了一种自适应LQR控制器，在几乎必然的情况下具有 $\tilde{ \mathcal{O}}(\sqrt{T})$ 后悔上限证明，且具有断电机制保证安全并对性能影响很小。 |
| [^123] | [A first-order augmented Lagrangian method for constrained minimax optimization.](http://arxiv.org/abs/2301.02060) | 本文提出了一种一阶增广拉格朗日方法来解决约束极小极大问题，其操作复杂度为 ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$。 |
| [^124] | [Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications.](http://arxiv.org/abs/2301.00752) | 本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。 |
| [^125] | [The unstable formula theorem revisited via algorithms.](http://arxiv.org/abs/2212.05050) | 本文介绍了模型理论中有关理论稳定性和学习中算法稳定性的交互，通过算法性质取代了无限，重访了Shelah闻名的不稳定公式定理，并引入了可能最终正确学习模型，表征了Littlestone（稳定）类，透过模型论中类型的可定义性形式化了Littlestone类的逼近。 |
| [^126] | [Benchmarking Self-Supervised Learning on Diverse Pathology Datasets.](http://arxiv.org/abs/2212.04690) | 本文针对计算病理学所需的大量标注数据问题，进行了最大规模的自监督预训练研究，发现大规模领域对齐的预训练方法在病理学中始终优于ImageNet上的预训练方法，并提出一套领域特定技术来提高模型性能，在月经周期病理数据集中也表现出良好性能。 |
| [^127] | [An Interpretable Hybrid Predictive Model of COVID-19 Cases using Autoregressive Model and LSTM.](http://arxiv.org/abs/2211.17014) | 本文提出了一种可解释的混合预测模型，该模型使用自回归模型和LSTM预测COVID-19病例，结合了两种模型的优势，通过数据自适应性决定模型块的相对贡献，在全面数值研究中展示了优异的性能。 |
| [^128] | [A locally time-invariant metric for climate model ensemble predictions of extreme risk.](http://arxiv.org/abs/2211.16367) | 该论文提出了一种局部时间不变的气候模式模拟评估方法，通过评估气候模式模拟的极端情况来预测气候变化相关预测，取得了良好的效果。 |
| [^129] | [Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes.](http://arxiv.org/abs/2211.15144) | 本文研究离线Q-learning算法在多任务Atari游戏上的表现，结果表明合适的选择能够提高扩展性和推广性，使模型性能与容量正相关，并且可以推广到数据集性能之外。 |
| [^130] | [Waveflow: Enforcing boundary conditions in smooth normalizing flows with application to fermionic wave functions.](http://arxiv.org/abs/2211.14839) | 本文介绍了一种新的处理归一化流拓扑问题、将边界条件应用于归一化流的技术、引入了可以被制作成任意次可微的 I-Spline 双射，并将这些技术用于创建一种基于归一化流的费米波函数 Ansatz，从而实现高效训练。 |
| [^131] | [Particle-based Variational Inference with Preconditioned Functional Gradient Flow.](http://arxiv.org/abs/2211.13954) | 本文提出了一种新的基于粒子的变分推断算法PFG，通过引入包含RKHS范数的函数正则项实现更大的函数类和更好的适应性，解决了RKHS要求限制函数类和算法灵活性的问题，并在KL散度上提供了可证明的连续时间收敛。 |
| [^132] | [Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization.](http://arxiv.org/abs/2211.11656) | 本文提出一种名为知情联合消除（IFU）的新颖联邦优化方法，可实现有效且可量化的客户端消除请求，实验结果表明其效率较基本方法和最先进的FU方法更高。 |
| [^133] | [Personalized Federated Learning with Multi-branch Architecture.](http://arxiv.org/abs/2211.07931) | 本文提出了一种新型的个性化联邦学习方法(pFedMB)，通过多分支结构实现个性化，并设计了一种聚合方法以提高通信效率和降低计算开销。该方法在合成数据和真实世界数据的实验中均表现出最先进的性能，同时在客户端具有来自复杂分布的数据且不能确定彼此的分布的情况下，能够促进拥有相似数据的客户端进行更多协作。 |
| [^134] | [Reduced order modeling of parametrized systems through autoencoders and SINDy approach: continuation of periodic solutions.](http://arxiv.org/abs/2211.06786) | 本文提出了一种数据驱动、非侵入性的约简建模方法，利用自编码器神经网络和SINDy方法构建低维动力学模型，可以在保留动态特性的同时，高效地预测参数变化时的全时间解。 |
| [^135] | [Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks.](http://arxiv.org/abs/2211.06134) | 本文提出了一种称为主动任务随机化（ATR）的方法，通过无监督生成任务来学习鲁棒技能，该方法选择适合学习鲁棒技能的任务，通过平衡任务的多样性和可行性来预测任务多样性和可行性，并使用基于图的参数化程序生成任务，从而允许鲁棒地处理任务的各种情况，包括以前未见过的场景。 |
| [^136] | [FedTP: Federated Learning by Transformer Personalization.](http://arxiv.org/abs/2211.01572) | 本文研究发现联邦平均算法对Transformer模型中的自注意力存在负面影响，限制了联邦学习的能力。为此提出了FedTP，在学习客户端个性化自注意力的同时，将其他参数聚合在客户端之间。 |
| [^137] | [Maximum Likelihood Learning of Unnormalized Models for Simulation-Based Inference.](http://arxiv.org/abs/2210.14756) | 该论文提出了两种用于基于仿真推断的合成似然方法，使用高保真度模拟器生成模拟数据，学习条件能量模型(EBM)的 likelihood，结合先验估计后验分布，可以使用MCMC抽取样本，该方法相较于其他方法更加灵活和准确。 |
| [^138] | [Networked Signal and Information Processing.](http://arxiv.org/abs/2210.13767) | 这篇文章回顾了网络信号和信息处理方面的重大进展，这种进展使得决策、优化、控制和学习等方面能够扩展到分布式智能体越来越普遍的环境中，而且通过合作和共享，网络智能体能够匹配云或联合解决方案的性能。 |
| [^139] | [XAI for transparent wind turbine power curve models.](http://arxiv.org/abs/2210.12104) | 本论文应用XAI方法揭示机器学习模型从大量风力涡轮机数据中学到的策略，并且呼吁在模型选择中更加突出地采用XAI方法，提出了一种利用解释进行根本原因分析的实用方法。 |
| [^140] | [Histopathological Image Classification based on Self-Supervised Vision Transformer and Weak Labels.](http://arxiv.org/abs/2210.09021) | 本研究提出了一种新方法Self-ViT-MIL，基于幻灯片级别的注释对癌细胞区域进行分类和定位，消除了需要像素级别注释的训练数据的需求。 |
| [^141] | [Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks.](http://arxiv.org/abs/2210.06601) | 本文提出了一个利用有损表示空间下的子目标指导在线微调的框架，它可以从广泛的数据中学习而来的有损表示来规划一系列子目标，分解原始任务，并强调任务相关信息，从而降低泛化过程中冗余内容的干扰，以此应对如何利用多样化的多任务数据进行新领域下游任务的学习的挑战。 |
| [^142] | [Estimating the Performance of Entity Resolution Algorithms: Lessons Learned Through PatentsView.org.](http://arxiv.org/abs/2210.01230) | 本文介绍了一种新的实体消解算法评估方法，它能够为PatentsView的用户提供可靠的数据和比较不同消解算法的结果。 |
| [^143] | [Stochastic gradient descent with gradient estimator for categorical features.](http://arxiv.org/abs/2209.03771) | 本文介绍了针对类别特征的梯度估计随机梯度下降算法，并展示了在多个数据集和模型架构上的高效性能以及提供了一个真实的匿名化零售数据集。 |
| [^144] | [A Maintenance Planning Framework using Online and Offline Deep Reinforcement Learning.](http://arxiv.org/abs/2208.00808) | 本论文提出了一种利用在线和离线深度强化学习的维护计划框架，以确定最优恢复策略，经过实验证明该框架比标准的预防性、纠正性和贪婪式计划方案有所改进。 |
| [^145] | [See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation.](http://arxiv.org/abs/2208.00759) | 使用图神经网络架构的邻域特征聚合模块实现了所有传感器间的通讯，解决了机器人视觉导航中缺乏全局定位信息的问题并实现了高效导航。 |
| [^146] | [Learning differentiable solvers for systems with hard constraints.](http://arxiv.org/abs/2207.08675) | 该论文提出了一种学习可微分求解器的方法，可以以高度准确的方式满足部分微分方程约束，并且在推断时可以提供连续的解来满足所需的物理约束。该方法能够显著降低测试误差。 |
| [^147] | [q-Learning in Continuous Time.](http://arxiv.org/abs/2207.00713) | 本文研究了连续时间下的q-Learning，通过引入小q函数作为一阶近似，研究了q-learning理论，应用于设计不同的演员-评论家算法。 |
| [^148] | [HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network.](http://arxiv.org/abs/2206.12747) | 本研究提出了基于超图注意力神经网络的药物相互作用预测模型HyGNN，可以基于药物的SMILES字符串进行预测，并在多个基准数据集上的表现优于其他最先进的方法。 |
| [^149] | [Robust Losses for Learning Value Functions.](http://arxiv.org/abs/2205.08464) | 本文提出了基于鲁棒损失的学习值函数的方法，使用了鞍点优化问题的见解，并在在线脱机预测和控制设置中，推导了稳健的基于梯度的方法。 |
| [^150] | [Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space.](http://arxiv.org/abs/2205.08129) | 本文提出了一种名为PTP的方法，利用高层规划器和潜空间中的条件生成器来分解目标成子目标，并在以前的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，从而训练目标导向的策略，有效提高了机器人长期目标导向的经验采集效率。 |
| [^151] | [Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions.](http://arxiv.org/abs/2203.02605) | 本文是关于将强化学习应用于自适应干预中的第一份统一调查，强化学习在动态治疗方案和移动健康中即时自适应干预这两个领域中都具有很大的应用潜力。在这两个领域之间存在相似和不同之处需要考虑，并且这里存在巨大的合作机会。 |
| [^152] | [Estimating Conditional Average Treatment Effects with Missing Treatment Information.](http://arxiv.org/abs/2203.01422) | 本文研究了条件平均处理效应 (CATE) 的估计问题，在缺失治疗信息的情况下，提出了缺失治疗表示网络 (MTRNet)，通过域自适应学习协变量的平衡表示来解决协变量转移问题。 |
| [^153] | [Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective.](http://arxiv.org/abs/2202.08063) | 低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。 |
| [^154] | [Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning.](http://arxiv.org/abs/2201.12559) | 本文提出了一种重新平衡的策略解决了批规范化在基于样例的类增量学习中出现的问题，实验结果表明，该算法在不同的数据集和模型上都取得了更好的表现。 |
| [^155] | [Graph-based Algorithm Unfolding for Energy-aware Power Allocation in Wireless Networks.](http://arxiv.org/abs/2201.11799) | 该论文提出了一种基于图的算法用于无线通信网络功率分配的加权能效比最大化，该算法采用模块化结构和图卷积神经网络，可以解决问题的非凸性，并展示了置换等变性。 |
| [^156] | [A Cognitive Explainer for Fetal ultrasound images classifier Based on Medical Concepts.](http://arxiv.org/abs/2201.07798) | 本研究提出了一种基于关键医学概念的可解释框架，利用概念间的关系构建图卷积神经网络，解释胎儿超声图像分类器的决策过程，为临床医生提供易于理解的推理结果见解。 |
| [^157] | [Faster Deep Reinforcement Learning with Slower Online Network.](http://arxiv.org/abs/2112.05848) | 本文改进了DQN和Rainbow两个深度强化学习算法，大大提高了它们在Atari游戏基准测试中的性能，我们的方法是在在线网络和目标网络之间引入一定的接近度，以提高深度强化学习的鲁棒性。 |
| [^158] | [Prediction of Large Magnetic Moment Materials With Graph Neural Networks and Random Forests.](http://arxiv.org/abs/2111.14712) | 本文使用图神经网络和随机森林预测出具有大磁矩的材料。我们的研究结果表明，这些机器学习方法可以高精度地预测大磁矩材料，这可以为从现有材料设计高性能磁铁以及发现新型大磁矩化合物提供可能。 |
| [^159] | [A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues.](http://arxiv.org/abs/2107.08574) | 提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，具备神经调制特征，通过一个额外的输入的函数替换了全连接层的固定权重，使得在测试中具有调制层的模型对于数据质量的降解更加鲁棒，同时也能够节省训练时间并且不会受到插补错误的影响。 |
| [^160] | [Assessment of hybrid machine learning models for non-linear system identification of fatigue test rigs.](http://arxiv.org/abs/2107.03645) | 本文开发了一种混合模型，利用长短期记忆网络结合线性频率响应函数模型进行非线性系统识别。该方法还可以应用于虚拟传感。通过对疲劳试验数据进行测试验证了该方法的效果。 |
| [^161] | [Online Sub-Sampling for Reinforcement Learning with General Function Approximation.](http://arxiv.org/abs/2106.07203) | 本文提出了一种基于在线子采样框架的强化学习算法，利用数据点的信息增益量来指导探索，与现有方法相比更新RL算法的策略次数大大减少，但仍保持较小的近似最优遗憾边界。 |
| [^162] | [The kernel perspective on dynamic mode decomposition.](http://arxiv.org/abs/2106.00106) | 本文重新审视了动态模态分解的理论假设，提出了一种新的只需在RKHS上定义稠密Koopman算子的DMD框架，并证明了高斯径向基核函数的RKHS只支持仿射动力学的有界Koopman算子。 |
| [^163] | [Adversarial Inverse Reinforcement Learning for Mean Field Games.](http://arxiv.org/abs/2104.14654) | 本文提出了一种新的均值场对抗逆强化学习(MF-AIRL)框架，它能够处理展示行为中的不确定性，并在模拟任务上的实验结果表明其在奖励恢复方面优于现有方法。 |
| [^164] | [A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics.](http://arxiv.org/abs/2103.01403) | 本文提出了一个新的数据集HINT，旨在检验机器学习通用概念的能力，包括感知、语法和语义三个层次。为了检验模型的插值和外推能力，我们设计了一个五倍交叉测试集。通过对几种最先进的模型进行广泛实验，进一步探究其局限性。 |
| [^165] | [Mat\'ern Gaussian processes on Riemannian manifolds.](http://arxiv.org/abs/2006.10160) | 本文提出了一种新的方法，通过谱理论计算Riemannian Matérn高斯过程在紧黎曼流形上的核，使其可以通过标准的可扩展技术进行训练。这将推动Matérn高斯过程在黎曼流形上的应用。 |
| [^166] | [Fast Objective & Duality Gap Convergence for Non-Convex Strongly-Concave Min-Max Problems with PL Condition.](http://arxiv.org/abs/2006.06889) | 该论文探讨了解决深度学习中出现的一类非凸强凸min-max问题的随机方法，并提出了一个基于近端阶段的方法框架，其中嵌入了许多众所周知的随机更新，快速收敛性得到了建立。 |
| [^167] | [Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring.](http://arxiv.org/abs/1907.02677) | 本文扩展了多变量大数据分析（MBDA）方法，提出了一种自动推导特征的解决方案，结合可解释性和交互式模型的优势以及并行处理的能力，应用于网络监测和诊断，最终在UGR'16和Dartmouth'18两个数据集上取得成功。 |
| [^168] | [Preference Neural Network.](http://arxiv.org/abs/1904.02345) | 本论文提出了一种新的偏好神经网络，用于解决具有冷漠偏好顺序的问题以及多标签排名问题。通过新的平滑阶梯形激活函数，PNN在提高计算效率的同时在严格标签排名的准确性方面优于五种先前提出的方法。 |

# 详细

[^1]: 双曲线图像文本表示方法

    Hyperbolic Image-Text Representations. (arXiv:2304.09172v1 [cs.CV])

    [http://arxiv.org/abs/2304.09172](http://arxiv.org/abs/2304.09172)

    本文提出了一个使用双曲表示捕捉图像和文本层次结构的对比模型MERU，并证明其在多模态任务上与CLIP相当。

    

    视觉和语言概念自然而然地组织成一个层次结构，其中一个文本概念“狗”包含所有包含狗的图像。尽管直觉上这是正确的，但目前的大规模视觉和语言模型（如CLIP）并没有明确地捕捉到这种层次结构。我们提出了MERU，一个对图像和文本进行双曲表示的对比模型。双曲空间具有嵌入树状数据的合适几何属性，因此MERU可以更好地捕捉图像文本数据的底层层次结构。我们的结果表明，MERU学习到了一个高度可解释的表示空间，同时在图像分类和图像文本检索等多模态任务上与CLIP的性能相当。

    Visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept ``dog'' entails all images that contain dogs. Despite being intuitive, current large-scale vision and language models such as CLIP do not explicitly capture such hierarchy. We propose MERU, a contrastive model that yields hyperbolic representations of images and text. Hyperbolic spaces have suitable geometric properties to embed tree-like data, so MERU can better capture the underlying hierarchy in image-text data. Our results show that MERU learns a highly interpretable representation space while being competitive with CLIP's performance on multi-modal tasks like image classification and image-text retrieval.
    
[^2]: 不需要均匀收敛的最优 PAC 界限

    Optimal PAC Bounds Without Uniform Convergence. (arXiv:2304.09167v1 [cs.LG])

    [http://arxiv.org/abs/2304.09167](http://arxiv.org/abs/2304.09167)

    本文通过提供框架，将置换不变预测器的留一出错转化为高概率风险界限，实现了为二元分类实现最优 PAC 界限的算法。

    

    在统计学习理论中，确定 VC 类别的可实现二进制分类的样本复杂度一直是一个长期存在的问题。 Simon 和 Hanneke 的结果在这种情况下建立了尖锐的上界。然而，它们的论证依赖于均匀收敛原则，限制了其适用性于更一般的学习设置，例如多类分类。本文通过提供超越均匀收敛论证限制的框架，提供了最优的高概率风险界限。我们的框架将置换不变预测器的留一出错转化为高概率风险界限。作为应用，通过改编 Haussler、Littlestone 和 Warmuth 的一包含图算法，我们提出了一种为二元分类实现最优 PAC 界限的算法。具体而言，我们的结果表明，某些一包含图算法的聚合是最优的，解决了一个问题。

    In statistical learning theory, determining the sample complexity of realizable binary classification for VC classes was a long-standing open problem. The results of Simon and Hanneke established sharp upper bounds in this setting. However, the reliance of their argument on the uniform convergence principle limits its applicability to more general learning settings such as multiclass classification. In this paper, we address this issue by providing optimal high probability risk bounds through a framework that surpasses the limitations of uniform convergence arguments.  Our framework converts the leave-one-out error of permutation invariant predictors into high probability risk bounds. As an application, by adapting the one-inclusion graph algorithm of Haussler, Littlestone, and Warmuth, we propose an algorithm that achieves an optimal PAC bound for binary classification. Specifically, our result shows that certain aggregations of one-inclusion graph algorithms are optimal, addressing a
    
[^3]: 无监督医学图像域自适应的结构保持循环 GAN

    Structure Preserving Cycle-GAN for Unsupervised Medical Image Domain Adaptation. (arXiv:2304.09164v1 [eess.IV])

    [http://arxiv.org/abs/2304.09164](http://arxiv.org/abs/2304.09164)

    本文提出了一种结构保持的循环 GAN 模型来改善无监督医学图像域自适应的分割性能，通过在循环 GAN 训练过程中增加分割损失项以保留感兴趣的结构，这种模型在未见过的图像域上实现了更好的分割性能。

    

    医学图像领域中的域偏移是一个普遍存在的问题，这可能会严重影响分割模型处理未见过的图像域时的性能。基于对抗的深度学习模型（例如 Cycle-GAN）已成为处理医学图像无监督域适应的常见模型。然而，这些模型在转换医学扫描时没有强制执行感兴趣结构的保留能力，这可能导致分割的无监督域适应结果较差。本文引入了一种结构保持循环GAN（SP Cycle-GAN），通过在整个循环GAN训练过程中执行分割损失项，促进医学结构在图像转换过程中的保持。我们通过视觉和对无监督域适应模型的 Dice 指标分割性能比较展示了 SP Cycle-GAN 的结构保持能力。相较于传统的 Cycle-GAN 模型，SP Cycle-GAN 在未见过的图像域上实现了更好的分割性能。

    The presence of domain shift in medical imaging is a common issue, which can greatly impact the performance of segmentation models when dealing with unseen image domains. Adversarial-based deep learning models, such as Cycle-GAN, have become a common model for approaching unsupervised domain adaptation of medical images. These models however, have no ability to enforce the preservation of structures of interest when translating medical scans, which can lead to potentially poor results for unsupervised domain adaptation within the context of segmentation. This work introduces the Structure Preserving Cycle-GAN (SP Cycle-GAN), which promotes medical structure preservation during image translation through the enforcement of a segmentation loss term in the overall Cycle-GAN training process. We demonstrate the structure preserving capability of the SP Cycle-GAN both visually and through comparison of Dice score segmentation performance for the unsupervised domain adaptation models. The SP 
    
[^4]: 深度度量学习的鲁棒性校准代理损失

    Robust Calibrate Proxy Loss for Deep Metric Learning. (arXiv:2304.09162v1 [cs.IR])

    [http://arxiv.org/abs/2304.09162](http://arxiv.org/abs/2304.09162)

    本研究提出了一种新的 Calibrate Proxy 结构，通过利用实际样本信息改善了基于代理的损失相似性计算，引入一个校准损失来约束代理优化方向类别特征中心。实验证明了该方法在多个数据集上都取得了较好表现。

    

    深度度量学习中，主流研究可分为两种：基于代理的方法和基于成对的方法。基于代理的方法由于训练复杂度低、网络收敛快速而受到广泛关注。然而，这些方法的局限在于代理优化由网络完成，使得难以准确地表示数据实际类别的特征分布情况。本文提出了一种 Calibrate Proxy（CP）结构，利用实际样本信息改善了基于代理的损失的相似度计算，并引入了校准损失来约束代理优化朝向类别特征中心。同时，我们为每个类别设置了少量代理以减轻类内差异对检索性能的影响。通过在三个公共数据集和多个合成标签噪声数据集上进行了广泛的实验评估了我们方法的有效性。

    The mainstream researche in deep metric learning can be divided into two genres: proxy-based and pair-based methods. Proxy-based methods have attracted extensive attention due to the lower training complexity and fast network convergence. However, these methods have limitations as the poxy optimization is done by network, which makes it challenging for the proxy to accurately represent the feature distrubtion of the real class of data. In this paper, we propose a Calibrate Proxy (CP) structure, which uses the real sample information to improve the similarity calculation in proxy-based loss and introduces a calibration loss to constraint the proxy optimization towards the center of the class features. At the same time, we set a small number of proxies for each class to alleviate the impact of intra-class differences on retrieval performance. The effectiveness of our method is evaluated by extensive experiments on three public datasets and multiple synthetic label-noise datasets. The res
    
[^5]: 检测和分类胶质母细胞瘤脑肿瘤

    Detection and Classification of Glioblastoma Brain Tumor. (arXiv:2304.09133v1 [eess.IV])

    [http://arxiv.org/abs/2304.09133](http://arxiv.org/abs/2304.09133)

    本文提出了两个深度学习模型，UNet和Deeplabv3，用于早期检测和分割胶质母细胞瘤脑肿瘤。实验结果表明，这两个模型都表现出了准确检测和分割胶质母细胞瘤脑肿瘤的能力。但Deeplabv3在准确性方面表现更优，需要更多的计算资源。

    

    胶质母细胞瘤脑肿瘤是高度致命的，早期检测和准确的分割对于有效治疗非常重要。本文提出两个深度学习模型，即UNet和Deeplabv3，用于使用预处理的脑MRI图像检测和分割胶质母细胞瘤脑肿瘤。针对这些模型，我们进行了性能评估，包括准确性和计算效率。实验结果表明，UNet和Deeplabv3模型都能准确检测和分割胶质母细胞瘤脑肿瘤。然而，Deeplabv3在准确性方面优于UNet，但需要更多计算资源。我们提出的模型为早期检测和分割胶质母细胞瘤脑肿瘤提供了有前途的方法，可以帮助制定有效的治疗策略。未来的研究可以集中在优化Deeplabv3模型的计算效率，同时保持其在现实世界中的高准确性。

    Glioblastoma brain tumors are highly malignant and often require early detection and accurate segmentation for effective treatment. We are proposing two deep learning models in this paper, namely UNet and Deeplabv3, for the detection and segmentation of glioblastoma brain tumors using preprocessed brain MRI images. The performance evaluation is done for these models in terms of accuracy and computational efficiency. Our experimental results demonstrate that both UNet and Deeplabv3 models achieve accurate detection and segmentation of glioblastoma brain tumors. However, Deeplabv3 outperforms UNet in terms of accuracy, albeit at the cost of requiring more computational resources. Our proposed models offer a promising approach for the early detection and segmentation of glioblastoma brain tumors, which can aid in effective treatment strategies. Further research can focus on optimizing the computational efficiency of the Deeplabv3 model while maintaining its high accuracy for real-world cl
    
[^6]: 变分关系点补全网络用于鲁棒的3D分类

    Variational Relational Point Completion Network for Robust 3D Classification. (arXiv:2304.09131v1 [cs.CV])

    [http://arxiv.org/abs/2304.09131](http://arxiv.org/abs/2304.09131)

    本文提出了一个称为 VRCNet 的网络，具有两个优点：1)概率建模；2）关系增强。此网络可以用于点云补全，能够提高点云几何建模和感知的表现。

    

    实际扫描的点云通常由于视角，遮挡和噪声而不完整，这阻碍了3D几何建模和感知。现有的点云补全方法倾向于生成全局形状骨架，因此缺乏精细的局部细节。此外，它们大多学习确定性的部分到完整的映射，但忽视了人造物体中的结构关系。为了解决这些挑战，本文提出了一个变分框架，变分关系点补全网络（VRCNet），具有两个优点：1）概率建模。特别是，我们提出了一种双通路架构，以在部分点云和完整点云之间实现有原则的概率建模。一个通路使用完整点云进行重建，通过学习点VAE实现。另一个通路为局部点云生成完整的形状，其嵌入式分布在训练期间由重建通路获得的分布引导。2）关系增强。具体而言，受语法模型中语法变量的启发，我们提出了一种关系推理过程，通过探索局部补丁之间的多层特征交互来增强补全质量。对三个公共数据集的广泛实验表明，VRCNet优于现有方法。

    Real-scanned point clouds are often incomplete due to viewpoint, occlusion, and noise, which hampers 3D geometric modeling and perception. Existing point cloud completion methods tend to generate global shape skeletons and hence lack fine local details. Furthermore, they mostly learn a deterministic partial-to-complete mapping, but overlook structural relations in man-made objects. To tackle these challenges, this paper proposes a variational framework, Variational Relational point Completion Network (VRCNet) with two appealing properties: 1) Probabilistic Modeling. In particular, we propose a dual-path architecture to enable principled probabilistic modeling across partial and complete clouds. One path consumes complete point clouds for reconstruction by learning a point VAE. The other path generates complete shapes for partial point clouds, whose embedded distribution is guided by distribution obtained from the reconstruction path during training. 2) Relational Enhancement. Specifica
    
[^7]: 使用被动 Langevin 动力学的自适应逆强化学习的有限样本界限

    Finite-Sample Bounds for Adaptive Inverse Reinforcement Learning using Passive Langevin Dynamics. (arXiv:2304.09123v1 [cs.LG])

    [http://arxiv.org/abs/2304.09123](http://arxiv.org/abs/2304.09123)

    本文提供了有限时间界限，用于被动随机梯度 Langevin 动力学算法，该算法可用于逆强化学习。该算法充当随机采样器，恢复用外部过程优化而来的成本函数。

    

    随机梯度 Langevin 动力学 (SGLD) 是从概率分布采样的有用方法。本文提供了一个被动随机梯度 Langevin 动力学算法 (PSGLD) 的有限样本分析，旨在实现逆强化学习。此处的“被动”是指 PSGLD 算法(逆学习过程)可用的噪声渐变是由外部随机梯度算法(正向学习器)在随机选择的点上评估的。PSGLD 算法因此充当一个随机采样器，可恢复正在被此外部过程优化的成本函数。以前的工作使用随机逼近技术分析了这个被动算法的渐近性能；在本文中，我们分析了它的有限时间性能。具体而言，我们提供了在被动算法和其稳定测度之间的 2-Wasserstein 距离上的有限时间界限，从中可以获得重建的成本函数。

    Stochastic gradient Langevin dynamics (SGLD) are a useful methodology for sampling from probability distributions. This paper provides a finite sample analysis of a passive stochastic gradient Langevin dynamics algorithm (PSGLD) designed to achieve inverse reinforcement learning. By "passive", we mean that the noisy gradients available to the PSGLD algorithm (inverse learning process) are evaluated at randomly chosen points by an external stochastic gradient algorithm (forward learner). The PSGLD algorithm thus acts as a randomized sampler which recovers the cost function being optimized by this external process. Previous work has analyzed the asymptotic performance of this passive algorithm using stochastic approximation techniques; in this work we analyze the non-asymptotic performance. Specifically, we provide finite-time bounds on the 2-Wasserstein distance between the passive algorithm and its stationary measure, from which the reconstructed cost function is obtained.
    
[^8]: 电商中优化赞助产品的实践经验

    Practical Lessons on Optimizing Sponsored Products in eCommerce. (arXiv:2304.09107v1 [cs.IR])

    [http://arxiv.org/abs/2304.09107](http://arxiv.org/abs/2304.09107)

    本文提出了一个实用的机器学习框架，可以解决电商广告系统中赞助产品优化的多个问题，而不需要改变现有机器学习模型结构。使用该框架可以处理广告系统中的长期问题，并为多个评估指标带来增量。

    

    本文研究了赞助产品优化中的多个问题，包括基于位置的去偏差、点击-转化多任务学习以及预测点击率的校准。我们提出了一个实用的机器学习框架，可以解决这些问题，而不需要改变现有机器学习模型的结构，因此可以与大多数机器学习模型结合使用（包括浅层模型，如梯度提升决策树、支持向量机）。在本文中，我们首先提出了数据和特征工程技术，以处理广告系统中的上述问题; 然后，我们评估了我们的实用框架在来自在线购物网站流量日志的实际数据集上的效益。我们表明，我们的提议的实用框架与数据和特征工程也可以处理广告系统中的长期问题，并为多个评估指标带来增量。

    In this paper, we study multiple problems from sponsored product optimization in ad system, including position-based de-biasing, click-conversion multi-task learning, and calibration on predicted click-through-rate (pCTR). We propose a practical machine learning framework that provides the solutions to such problems without structural change to existing machine learning models, thus can be combined with most machine learning models including shallow models (e.g. gradient boosting decision trees, support vector machines). In this paper, we first propose data and feature engineering techniques to handle the aforementioned problems in ad system; after that, we evaluate the benefit of our practical framework on real-world data sets from our traffic logs from online shopping site. We show that our proposed practical framework with data and feature engineering can also handle the perennial problems in ad systems and bring increments to multiple evaluation metrics.
    
[^9]: 探究客户360度视角实现类似建模

    Exploring 360-Degree View of Customers for Lookalike Modeling. (arXiv:2304.09105v1 [cs.IR])

    [http://arxiv.org/abs/2304.09105](http://arxiv.org/abs/2304.09105)

    该论文提出了一个能将客户360度视角的不同行为或特征，在此基础上构建出类似模型以改善客户定位的框架。实验结果表明，该模型在电商和旅游领域中能够有效发挥作用。

    

    类似建模是基于用户相似性对产品销售和扩展广告活动起到重要作用的假设。这项工作的挑战在于用户群体的异质性和其稀疏性。我们提出了一个新的框架，将客户的不同行为或特征（如人口统计信息、在不同平台上的购买行为、客户忠诚行为）统一起来，构建了一个类似模型，以改善乐天集团的客户定位。真实的电子商务和旅游数据集上的广泛实验表明，我们提出的类似模型对于用户定位任务非常有效。

    Lookalike models are based on the assumption that user similarity plays an important role towards product selling and enhancing the existing advertising campaigns from a very large user base. Challenges associated to these models reside on the heterogeneity of the user base and its sparsity. In this work, we propose a novel framework that unifies the customers different behaviors or features such as demographics, buying behaviors on different platforms, customer loyalty behaviors and build a lookalike model to improve customer targeting for Rakuten Group, Inc. Extensive experiments on real e-commerce and travel datasets demonstrate the effectiveness of our proposed lookalike model for user targeting task.
    
[^10]: 基于卷积神经网络和STM32微控制器的实时轴承故障诊断

    Real Time Bearing Fault Diagnosis Based on Convolutional Neural Network and STM32 Microcontroller. (arXiv:2304.09100v1 [cs.LG])

    [http://arxiv.org/abs/2304.09100](http://arxiv.org/abs/2304.09100)

    本研究提出了一种利用卷积神经网络与STM32微控制器结合进行实时轴承故障诊断的方法，通过优化后的模型诊断准确率可达98.9%。同时，还成功将该方法应用于STM32H743VI微控制器，使得每次诊断的运行时间缩短至19ms，为轴承故障诊断提供了更高效的技术手段。

    

    随着大数据和边缘计算的快速发展，许多研究者致力于使用深度学习模型提高轴承故障分类的准确性，并在像STM32这样的有限资源平台上实现深度学习分类模型。本文基于卷积神经网络实现了对轴承故障振动信号的识别，优化后的模型诊断准确率可以达到98.9%。此外，本文成功将卷积神经网络模型应用于STM32H743VI微控制器，每次诊断的运行时间为19ms。最后，本文设计了一个完整的主机与STM32之间的实时通信框架，可以通过串口完美完成数据传输，并在TFT-LCD屏幕上显示诊断结果。

    With the rapid development of big data and edge computing, many researchers focus on improving the accuracy of bearing fault classification using deep learning models, and implementing the deep learning classification model on limited resource platforms such as STM32. To this end, this paper realizes the identification of bearing fault vibration signal based on convolutional neural network, the fault identification accuracy of the optimised model can reach 98.9%. In addition, this paper successfully applies the convolutional neural network model to STM32H743VI microcontroller, the running time of each diagnosis is 19ms. Finally, a complete real-time communication framework between the host computer and the STM32 is designed, which can perfectly complete the data transmission through the serial port and display the diagnosis results on the TFT-LCD screen.
    
[^11]: MATURE-HEALTH: MAndatory FeaTURE选择的健康推荐系统

    MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices. (arXiv:2304.09099v1 [cs.IR])

    [http://arxiv.org/abs/2304.09099](http://arxiv.org/abs/2304.09099)

    该论文提出和实施了一个名为MATURE-HEALTH的健康推荐系统，该系统能够预测电解质不平衡并推荐营养平衡的食物，从而增加早期检测疾病的机会并防止健康进一步恶化。

    

    平衡电解质对于人体器官的适当功能至关重要和必不可少，因为电解质失衡可能是潜在病理生理学发展的指示。高效监测电解质失衡不仅可以增加疾病早期检测的机会，而且可以通过严格遵循营养控制饮食以平衡电解质从而防止健康进一步恶化。本研究提出并实施了一个推荐系统MATURE Health，该系统预测血液中必需电解质和其他物质的不平衡，然后推荐含有平衡营养的食物，以避免电解质不平衡的发生。该模型考虑到用户最近的实验室结果和每日食物摄入量来预测电解质不平衡。MATURE Health依赖于MATURE Food算法推荐食物，后者仅推荐那些

    Balancing electrolytes is utmost important and essential for appropriate functioning of organs in human body as electrolytes imbalance can be an indication of the development of underlying pathophysiology. Efficient monitoring of electrolytes imbalance not only can increase the chances of early detection of disease, but also prevents the further deterioration of the health by strictly following nutrient controlled diet for balancing the electrolytes post disease detection. In this research, a recommender system MATURE Health is proposed and implemented, which predicts the imbalance of mandatory electrolytes and other substances presented in blood and recommends the food items with the balanced nutrients to avoid occurrence of the electrolytes imbalance. The proposed model takes user most recent laboratory results and daily food intake into account to predict the electrolytes imbalance. MATURE Health relies on MATURE Food algorithm to recommend food items as latter recommends only those
    
[^12]: 基于Sheaf神经网络的基于图的推荐系统

    Sheaf Neural Networks for Graph-based Recommender Systems. (arXiv:2304.09097v1 [cs.IR])

    [http://arxiv.org/abs/2304.09097](http://arxiv.org/abs/2304.09097)

    基于Sheaf神经网络的模型提出了一种新的向量空间表示方法，使得其在基准推荐任务上获得最先进的性能表现。

    

    近年来，Graph神经网络在许多应用中得到了广泛应用，包括推荐系统。Graph神经网络对其他方法的优越性在于，推荐系统中的许多问题可以自然地建模为图，其中节点可以是用户或项目，边代表偏好关系。 在当前的Graph神经网络方法中，节点用在训练时学习到的静态向量表示。这种静态向量可能只适用于捕捉定义它们的一些用户或项目的微妙差别。为了克服这个限制，我们建议使用最近提出的启发范畴论的模型：Sheaf神经网络。Sheaf神经网络及其连接的拉普拉斯可以通过将每个节点（以及边）与向量空间而不是单个向量相关联来解决上述问题。向量空间表示更丰富，并允许在推理时选择正确的表示。这种方法使我们的模型更具表现力和灵活性，在几个基准推荐任务上实现了最先进的性能。

    Recent progress in Graph Neural Networks has resulted in wide adoption by many applications, including recommendation systems. The reason for Graph Neural Networks' superiority over other approaches is that many problems in recommendation systems can be naturally modeled as graphs, where nodes can be either users or items and edges represent preference relationships. In current Graph Neural Network approaches, nodes are represented with a static vector learned at training time. This static vector might only be suitable to capture some of the nuances of users or items they define. To overcome this limitation, we propose using a recently proposed model inspired by category theory: Sheaf Neural Networks. Sheaf Neural Networks, and its connected Laplacian, can address the previous problem by associating every node (and edge) with a vector space instead than a single vector. The vector space representation is richer and allows picking the proper representation at inference time. This approa
    
[^13]: 基于高斯机制的保护隐私矩阵分解推荐系统

    Privacy-Preserving Matrix Factorization for Recommendation Systems using Gaussian Mechanism. (arXiv:2304.09096v1 [cs.IR])

    [http://arxiv.org/abs/2304.09096](http://arxiv.org/abs/2304.09096)

    本文提出了一种基于差分隐私框架和矩阵分解的保护隐私推荐系统，采用输出扰动的高斯机制实现差分隐私，通过Rényi差分隐私对整体隐私损失进行特征化，在保护用户隐私的同时实现了推荐系统功能。

    

    建立推荐系统需要分析用户数据，这可能会泄露用户的个人信息。匿名化用户数据通常不足以保护用户隐私。鉴于此，本文提出了一种基于差分隐私框架和矩阵分解的保护隐私推荐系统，矩阵分解是最流行的推荐系统算法之一。通过差分隐私，即使对手拥有用户的公开信息，也可以防止对手提取敏感用户信息。我们采用输出扰动的高斯机制实现差分隐私并发布满足隐私定义的用户档案。我们使用Rényi差分隐私对整体隐私损失进行了紧密的特征化。我们在实验中进行了广泛的测试。

    Building a recommendation system involves analyzing user data, which can potentially leak sensitive information about users. Anonymizing user data is often not sufficient for preserving user privacy. Motivated by this, we propose a privacy-preserving recommendation system based on the differential privacy framework and matrix factorization, which is one of the most popular algorithms for recommendation systems. As differential privacy is a powerful and robust mathematical framework for designing privacy-preserving machine learning algorithms, it is possible to prevent adversaries from extracting sensitive user information even if the adversary possesses their publicly available (auxiliary) information. We implement differential privacy via the Gaussian mechanism in the form of output perturbation and release user profiles that satisfy privacy definitions. We employ R\'enyi Differential Privacy for a tight characterization of the overall privacy loss. We perform extensive experiments on
    
[^14]: 基于描述性图的对话式推荐系统中的物品和上下文理解改进

    Improving Items and Contexts Understanding with Descriptive Graph for Conversational Recommendation. (arXiv:2304.09093v1 [cs.IR])

    [http://arxiv.org/abs/2304.09093](http://arxiv.org/abs/2304.09093)

    KLEVER是一个新的CRS框架，可以将物品和它们相关的上下文单词联合建模在同一语义空间中，解决了以前工作中的物品和单词语义空间不对齐的问题。

    

    对话式推荐系统（CRS）在利用外部知识提高物品和上下文单词表示方面处于最前沿水平，以实现高质量的推荐和响应生成。然而，物品和单词的表示通常在两个独立的语义空间中建模，这会导致它们之间的不对齐问题。因此，当用户输入信息不足时，这将导致CRS仅实现次优排名表现。为了解决以前工作的局限性，我们提出了一个新的CRS框架KLEVER，它可以联合建模在相同语义空间中的物品和它们相关的上下文单词。特别是，我们从丰富的物品文本特征（如物品描述和类别）中构建一个物品描述性图。基于构建的描述性图，KLEVER共同学习单词和物品的嵌入，以增强推荐和对话生成的互动。

    State-of-the-art methods on conversational recommender systems (CRS) leverage external knowledge to enhance both items' and contextual words' representations to achieve high quality recommendations and responses generation. However, the representations of the items and words are usually modeled in two separated semantic spaces, which leads to misalignment issue between them. Consequently, this will cause the CRS to only achieve a sub-optimal ranking performance, especially when there is a lack of sufficient information from the user's input. To address limitations of previous works, we propose a new CRS framework KLEVER, which jointly models items and their associated contextual words in the same semantic space. Particularly, we construct an item descriptive graph from the rich items' textual features, such as item description and categories. Based on the constructed descriptive graph, KLEVER jointly learns the embeddings of the words and items, towards enhancing both recommender and d
    
[^15]: 推荐中的Bandit算法的现场测试：了解MAB中关于人类偏好假设的有效性

    A Field Test of Bandit Algorithms for Recommendations: Understanding the Validity of Assumptions on Human Preferences in Multi-armed Bandits. (arXiv:2304.09088v1 [cs.IR])

    [http://arxiv.org/abs/2304.09088](http://arxiv.org/abs/2304.09088)

    本研究采用漫画推荐MAB设置，与众包工作者对关键MAB假设的有效性进行研究，结果挑战了这些假设，强调了需要进行更多关于MAB推荐中这些假设的实证工作。

    

    个性化推荐系统渗透到现代生活中，塑造了我们读什么媒体和消费什么产品。驱动此类系统的算法往往包括基于监督学习的启发式算法，例如具有多种启发式选择的潜在因素模型。同时，有关推荐的理论处理通常涉及问题的决策理论性质，包括通过多臂赌博机（MAB）框架平衡探索和开发的需要。然而，基于MAB的方法严重依赖于人类偏好的假设。这些偏好假设很少使用人员研究进行测试，部分原因是缺乏公开可用的工具包来进行这些研究。在这项工作中，我们在漫画推荐MAB设置中与众包工作者进行研究。每个机器臂表示一个漫画类别，用户在每次推荐后提供反馈。我们检查了关键MAB假设的有效性，即人类偏好是时空具有静态性，并且可以建模为潜在收益分布的嘈杂实现。我们的研究结果挑战了这些假设，强调了需要进行更多关于MAB推荐中这些假设的实证工作。

    Personalized recommender systems suffuse modern life, shaping what media we read and what products we consume. Algorithms powering such systems tend to consist of supervised learning-based heuristics, such as latent factor models with a variety of heuristically chosen prediction targets. Meanwhile, theoretical treatments of recommendation frequently address the decision-theoretic nature of the problem, including the need to balance exploration and exploitation, via the multi-armed bandits (MABs) framework. However, MAB-based approaches rely heavily on assumptions about human preferences. These preference assumptions are seldom tested using human subject studies, partly due to the lack of publicly available toolkits to conduct such studies. In this work, we conduct a study with crowdworkers in a comics recommendation MABs setting. Each arm represents a comic category, and users provide feedback after each recommendation. We check the validity of core MABs assumptions-that human preferen
    
[^16]: MDDL: 基于强化学习的多通道Feed位置分配框架

    MDDL: A Framework for Reinforcement Learning-based Position Allocation in Multi-Channel Feed. (arXiv:2304.09087v1 [cs.IR])

    [http://arxiv.org/abs/2304.09087](http://arxiv.org/abs/2304.09087)

    本研究提出了一种名为MDDL的多通道深度确定性策略梯度学习框架，旨在整合多种策略，以增强位置分配的强化学习模型训练。该框架在在线和离线性能方面表现优于一些最先进的方法。

    

    目前，位置分配系统的主流方法是利用强化学习模型为各通道的物品分配合适的位置，然后混合到Feed中。强化学习模型的训练使用两种数据：策略数据和随机数据。策略数据来自当前在线模型，它受到状态-动作对分布不均衡的困扰，导致训练过程中存在严重的高估问题。另一方面，随机数据提供了更均匀的状态-动作对分布，但在工业场景中很难获取，因为随机探索可能会对平台收入和用户体验产生负面影响。由于这两种数据具有不同的分布，因此设计一种有效的策略来利用两种数据以增强强化学习模型的训练效果已成为一个极具挑战性的问题。本研究提出了一种名为MDDL（多通道深度确定性策略梯度学习）的框架来解决上述问题。我们的框架旨在整合多种策略，以增强位置分配的RL模型训练。实验证明，在线和离线性能方面，MDDL表现优于一些最先进的方法。

    Nowadays, the mainstream approach in position allocation system is to utilize a reinforcement learning model to allocate appropriate locations for items in various channels and then mix them into the feed. There are two types of data employed to train reinforcement learning (RL) model for position allocation, named strategy data and random data. Strategy data is collected from the current online model, it suffers from an imbalanced distribution of state-action pairs, resulting in severe overestimation problems during training. On the other hand, random data offers a more uniform distribution of state-action pairs, but is challenging to obtain in industrial scenarios as it could negatively impact platform revenue and user experience due to random exploration. As the two types of data have different distributions, designing an effective strategy to leverage both types of data to enhance the efficacy of the RL model training has become a highly challenging problem. In this study, we propo
    
[^17]: 利用少量无偏评分平衡未观测混淆的去偏推荐

    Balancing Unobserved Confounding with a Few Unbiased Ratings in Debiased Recommendations. (arXiv:2304.09085v1 [cs.IR])

    [http://arxiv.org/abs/2304.09085](http://arxiv.org/abs/2304.09085)

    本文提出了利用无偏评分平衡已有的去偏方法来对抗未观测混淆和模型规范误差的方法。

    

    推荐系统被视为解决信息过载的有效工具，但众所周知，存在各种偏差使得在大规模观测数据上进行直接训练会导致次优的预测性能。与之相反，从随机控制试验或A/B测试中获得的无偏评分被认为是黄金标准，但在现实中成本高且规模较小。为了利用这两种类型的数据，最近的研究提出利用无偏评分来修正在有偏数据集上训练的倾向性或插补模型的参数。然而，现有方法在存在未观测混淆或模型规范误差时无法获得准确的预测。本文提出了一种理论上保证的模型不可知平衡方法，可应用于任何现有的去偏方法，旨在对抗未观测混淆和模型规范误差。所提出的方法充分利用了无偏数据。

    Recommender systems are seen as an effective tool to address information overload, but it is widely known that the presence of various biases makes direct training on large-scale observational data result in sub-optimal prediction performance. In contrast, unbiased ratings obtained from randomized controlled trials or A/B tests are considered to be the golden standard, but are costly and small in scale in reality. To exploit both types of data, recent works proposed to use unbiased ratings to correct the parameters of the propensity or imputation models trained on the biased dataset. However, the existing methods fail to obtain accurate predictions in the presence of unobserved confounding or model misspecification. In this paper, we propose a theoretically guaranteed model-agnostic balancing approach that can be applied to any existing debiasing method with the aim of combating unobserved confounding and model misspecification. The proposed approach makes full use of unbiased data by 
    
[^18]: DRIFT: 一种基于隐式反馈的联邦推荐系统

    DRIFT: A Federated Recommender System with Implicit Feedback on the Items. (arXiv:2304.09084v1 [cs.IR])

    [http://arxiv.org/abs/2304.09084](http://arxiv.org/abs/2304.09084)

    DRIFT是一个联邦推荐系统架构，使用隐式反馈，以保护用户隐私，并使用SAROS算法实现精准推荐。

    

    现在网络上可用的物品越来越多，这使得用户很难找到他们喜欢的物品。推荐系统旨在使用用户的历史交互来找到最适合用户的物品。然而，这些交互可能更或更少敏感，并收集它们涉及到用户隐私的问题。联邦系统已经显示出即使在不存储用户个人信息的情况下也可以进行准确和有效的推荐。然而，这些系统使用用户的即时反馈。在本报告中，我们提出了DRIFT，一种使用隐式反馈的联合推荐系统架构。我们的学习模型基于最近用于隐式反馈推荐的算法SAROS。我们的目标是使推荐与SAROS一样精确，同时不损害用户的隐私。通过实验和收敛性的理论分析，我们展示了DRIFT的性能。

    Nowadays there are more and more items available online, this makes it hard for users to find items that they like. Recommender systems aim to find the item who best suits the user, using his historical interactions. Depending on the context, these interactions may be more or less sensitive and collecting them brings an important problem concerning the users' privacy. Federated systems have shown that it is possible to make accurate and efficient recommendations without storing users' personal information. However, these systems use instantaneous feedback from the user. In this report, we propose DRIFT, a federated architecture for recommender systems, using implicit feedback. Our learning model is based on a recent algorithm for recommendation with implicit feedbacks SAROS. We aim to make recommendations as precise as SAROS, without compromising the users' privacy. In this report we show that thanks to our experiments, but also thanks to a theoretical analysis on the convergence. We h
    
[^19]: 深度Calderón方法的电阻抗层析成像

    Electrical Impedance Tomography with Deep Calder\'on Method. (arXiv:2304.09074v1 [math.NA])

    [http://arxiv.org/abs/2304.09074](http://arxiv.org/abs/2304.09074)

    Calderón方法是一种快速的EIT成像算法，但图像模糊且低估电导率值。该论文基于U-net模型对Calderón方法的图像进行后处理，提高了图像分辨率和电导率估计的准确性。

    

    电阻抗层析成像(EIT)是一种利用在对象表面上测量的电流密度/电压数据的非侵入式医学成像方式。Calderon的方法是一种相对较新的EIT成像算法，它是非迭代的、快速的，并且能够重建复值电阻抗。然而，由于正则化通过低通滤波和线性化，重建的图像遭受严重的模糊和低估确切的电导率值。在这项工作中，我们开发了Calderón方法的增强版本，使用卷积神经网络（即U-net）通过后处理步骤。具体来说，我们学习一个U-net来后处理由Calderón方法生成的EIT图像，以获得更好的分辨率和更准确的电导率估计。我们模拟胸部配置，通过Calderón方法生成电流密度/电压边界测量和相应的重建图像。

    Electrical impedance tomography (EIT) is a noninvasive medical imaging modality utilizing the current-density/voltage data measured on the surface of the subject. Calder\'on's method is a relatively recent EIT imaging algorithm that is non-iterative, fast, and capable of reconstructing complex-valued electric impedances. However, due to the regularization via low-pass filtering and linearization, the reconstructed images suffer from severe blurring and underestimation of the exact conductivity values. In this work, we develop an enhanced version of Calder\'on's method, using convolution neural networks (i.e., U-net) via a postprocessing step. Specifically, we learn a U-net to postprocess the EIT images generated by Calder\'on's method so as to have better resolutions and more accurate estimates of conductivity values. We simulate chest configurations with which we generate the current-density/voltage boundary measurements and the corresponding reconstructed images by Calder\'on's metho
    
[^20]: M-ENIAC: 第一次成功的数值天气预报的机器学习再现

    M-ENIAC: A machine learning recreation of the first successful numerical weather forecasts. (arXiv:2304.09070v1 [physics.ao-ph])

    [http://arxiv.org/abs/2304.09070](http://arxiv.org/abs/2304.09070)

    本研究使用基于机器学习的求解器重新创建了第一次成功的数值天气预报，并证明物理学相关的神经网络方法相较于以前的标准求解器更为准确且简便。

    

    1950年，采用“电子数字积分器和计算机”（ENIAC）解决了涡度方程，从而获得了第一次成功的数值天气预报，标志着数值天气预报的时代的开始。在这里，我们提出一个问题：如果使用基于机器学习的求解器而不是标准的数值离散化，这些数值预报的结果会是什么？具体而言，我们使用物理学相关的神经网络来重新创建这些数值预报。我们表明，与ENIAC求解器相比，物理学相关的神经网络提供了一种更简便、更准确的在球上求解气象方程的方法。

    In 1950 the first successful numerical weather forecast was obtained by solving the barotropic vorticity equation using the Electronic Numerical Integrator and Computer (ENIAC), which marked the beginning of the age of numerical weather prediction. Here, we ask the question of how these numerical forecasts would have turned out, if machine learning based solvers had been used instead of standard numerical discretizations. Specifically, we recreate these numerical forecasts using physics-informed neural networks. We show that physics-informed neural networks provide an easier and more accurate methodology for solving meteorological equations on the sphere, as compared to the ENIAC solver.
    
[^21]: METAM: 目标导向的数据发现

    METAM: Goal-Oriented Data Discovery. (arXiv:2304.09068v1 [cs.DB])

    [http://arxiv.org/abs/2304.09068](http://arxiv.org/abs/2304.09068)

    METAM是一种目标导向的数据发现框架，能够自动引导发现和增强过程，提高机器学习和因果推断任务性能。

    

    数据是机器学习和因果推断任务的核心组成部分。来自开放数据存储库、数据湖和数据市场等来源的大量数据的可用性提供了增强数据和提高这些任务性能的机会。然而，增强技术依赖于用户手动发现和筛选有用的候选增强项。现有解决方案未利用发现和增强之间的协同作用，因此未充分利用数据。在本文中，我们介绍METAM，一种新颖的目标导向框架，该框架通过向下游任务查询候选数据集，形成一个反馈循环，自动引导发现和增强过程。为了高效地选择候选项，METAM利用数据属性、效用函数和解集大小的特性。我们展示了METAM的理论保证，并在广泛的任务集上进行了实证演示。总的来说，我们展示了目标导向数据发现在提高机器学习和因果推断任务性能方面的前景。

    Data is a central component of machine learning and causal inference tasks. The availability of large amounts of data from sources such as open data repositories, data lakes and data marketplaces creates an opportunity to augment data and boost those tasks' performance. However, augmentation techniques rely on a user manually discovering and shortlisting useful candidate augmentations. Existing solutions do not leverage the synergy between discovery and augmentation, thus under exploiting data.  In this paper, we introduce METAM, a novel goal-oriented framework that queries the downstream task with a candidate dataset, forming a feedback loop that automatically steers the discovery and augmentation process. To select candidates efficiently, METAM leverages properties of the: i) data, ii) utility function, and iii) solution set size. We show METAM's theoretical guarantees and demonstrate those empirically on a broad set of tasks. All in all, we demonstrate the promise of goal-oriented d
    
[^22]: 总是增强你的优势: 一种面向CTR预测的漂移感知增量学习框架

    Always Strengthen Your Strengths: A Drift-Aware Incremental Learning Framework for CTR Prediction. (arXiv:2304.09062v1 [cs.IR])

    [http://arxiv.org/abs/2304.09062](http://arxiv.org/abs/2304.09062)

    本文提出了一种面向CTR预测的漂移感知增量学习框架。该框架基于集成学习，通过显式的基于误差的漂移检测来解决CTR预测中的灾难性遗忘问题。

    

    点击率 (CTR) 预测在推荐系统和在线广告平台中非常重要。在工业场景中使用时，CTR 模型观察到的用户生成数据通常以流的形式到达。流式数据具有随着时间而漂移并可能重现的特性。如果模型仅适应于新数据分布，这可能会导致灾难性遗忘。此外，重复学习已经出现的分布是低效的。由于内存限制和大规模工业应用中数据分布的多样性，传统的灾难性遗忘策略（如重放、参数隔离和知识蒸馏）难以部署。在本研究中，我们基于集成学习设计了一种新的漂移感知增量学习框架，用于解决CTR预测中的灾难性遗忘。通过对流数据进行显式基于误差的漂移检测，该框架可以发现分布漂移并有针对性地学习数据分布漂移。

    Click-through rate (CTR) prediction is of great importance in recommendation systems and online advertising platforms. When served in industrial scenarios, the user-generated data observed by the CTR model typically arrives as a stream. Streaming data has the characteristic that the underlying distribution drifts over time and may recur. This can lead to catastrophic forgetting if the model simply adapts to new data distribution all the time. Also, it's inefficient to relearn distribution that has been occurred. Due to memory constraints and diversity of data distributions in large-scale industrial applications, conventional strategies for catastrophic forgetting such as replay, parameter isolation, and knowledge distillation are difficult to be deployed. In this work, we design a novel drift-aware incremental learning framework based on ensemble learning to address catastrophic forgetting in CTR prediction. With explicit error-based drift detection on streaming data, the framework fur
    
[^23]: 音乐流媒体服务中自动生成播放列表的可扩展框架

    A Scalable Framework for Automatic Playlist Continuation on Music Streaming Services. (arXiv:2304.09061v1 [cs.IR])

    [http://arxiv.org/abs/2304.09061](http://arxiv.org/abs/2304.09061)

    本论文提出了一个通用框架，它基于表示-聚合策略构建可扩展但有效的APC模型，用于大规模应用，可以包括各种表示学习和序列建模技术。

    

    音乐流媒体服务通常旨在推荐歌曲以扩展用户在该服务上创建的播放列表。然而，在保留其音乐特征和符合用户倾向的情况下扩展播放列表仍然是一项具有挑战性的任务，通常被称为自动播放列表延续（APC）。此外，尽管这些服务经常需要在大型目录中实时选择最佳歌曲进行推荐，但最近关于APC的研究主要集中在具有少量可扩展性保证并在相对较小的数据集上进行评估的模型上。在本文中，我们介绍了一个通用框架，用于构建适用于大规模应用的可扩展但有效的APC模型。基于表示-聚合策略，它通过设计确保可扩展性，同时足够灵活，可以包含各种表示学习和序列建模技术，例如基于Transformer。我们通过在不同数据集和情况下的实验证明了该框架的相关性。

    Music streaming services often aim to recommend songs for users to extend the playlists they have created on these services. However, extending playlists while preserving their musical characteristics and matching user preferences remains a challenging task, commonly referred to as Automatic Playlist Continuation (APC). Besides, while these services often need to select the best songs to recommend in real-time and among large catalogs with millions of candidates, recent research on APC mainly focused on models with few scalability guarantees and evaluated on relatively small datasets. In this paper, we introduce a general framework to build scalable yet effective APC models for large-scale applications. Based on a represent-then-aggregate strategy, it ensures scalability by design while remaining flexible enough to incorporate a wide range of representation learning and sequence modeling techniques, e.g., based on Transformers. We demonstrate the relevance of this framework through in-
    
[^24]: 重访基于预训练语言模型的k-NN

    Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])

    [http://arxiv.org/abs/2304.09058](http://arxiv.org/abs/2304.09058)

    本研究提出一种新方法，结合k-NN和预训练语言模型（PLMs）能够提高自然语言处理（NLP）的性能，并在多个基准数据集上得到验证。

    

    预训练语言模型（PLMs）作为参数化的急切学习器，已成为自然语言处理（NLP）当前范式的实际选择。与此形成对比的是，k-最近邻（k-NN）分类器作为延迟学习模型，倾向于减轻过拟合和孤立噪声。本文中我们重访了k-NN分类器，以增强基于PLMs的分类器。从方法层面上，我们提出采用文本表示的PLMs在两个步骤中采用k-NN：（1）利用k-NN作为先验知识来校准训练过程（2）线性插值k-NN预测的概率分布和PLMs分类器的概率分布。我们的方法核心是实现了k-NN校准训练，将预测结果作为训练过程中易于和难以学习的示例的指标。从应用场景多样性的角度出发，我们在各种基准数据集上进行了广泛的微调、提示微调范式和零样本任务设置的实验。我们的结果表明，结合k-NN可以在所有受到检查的设置中持续提高PLMs的性能，并且在所有受到考虑的设置中跑赢了基于普通PLMs的方法。

    Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
    
[^25]: 解码神经活动以评估个体在生态有效环境中的潜在状态

    Decoding Neural Activity to Assess Individual Latent State in Ecologically Valid Contexts. (arXiv:2304.09050v1 [q-bio.NC])

    [http://arxiv.org/abs/2304.09050](http://arxiv.org/abs/2304.09050)

    此研究旨在在更为真实的环境中解码人类神经活动模式，以进一步理解复杂任务期间个体内部潜在状态，该方法能够验证实验室方法并提供有意义的洞察。

    

    历史上，仅有少量方法可以在更生态有效的情境下分离认知过程。特别地，目前尚不清楚在此类约束条件下观察到的神经活动模式实际上是否以一种可以用于准确推断个体潜在状态、相关认知过程或近端行为的方式在实验室外显现。改善我们对特定神经活动模式何时以及如何在生态有效情境中显现的理解，会验证在隔离环境下研究类似神经现象的实验室方法，并提供关于复杂任务期间发生的潜在状态的有意义洞察。我们认为，来自脑-计算机界面社区的领域通用方法有潜力解决这一挑战。我们以前使用了这样的方法来解码与视觉目标相关的突发神经反应。

    There exist very few ways to isolate cognitive processes, historically defined via highly controlled laboratory studies, in more ecologically valid contexts. Specifically, it remains unclear as to what extent patterns of neural activity observed under such constraints actually manifest outside the laboratory in a manner that can be used to make an accurate inference about the latent state, associated cognitive process, or proximal behavior of the individual. Improving our understanding of when and how specific patterns of neural activity manifest in ecologically valid scenarios would provide validation for laboratory-based approaches that study similar neural phenomena in isolation and meaningful insight into the latent states that occur during complex tasks. We argue that domain generalization methods from the brain-computer interface community have the potential to address this challenge. We previously used such an approach to decode phasic neural responses associated with visual tar
    
[^26]: DeepGEMM：使用查找表在CPU体系结构上加速超低精度推断

    DeepGEMM: Accelerated Ultra Low-Precision Inference on CPU Architectures using Lookup Tables. (arXiv:2304.09049v1 [cs.LG])

    [http://arxiv.org/abs/2304.09049](http://arxiv.org/abs/2304.09049)

    DeepGEMM提出一种使用查找表在CPU体系结构上加速超低精度推断的方法，用于执行超低精度卷积神经网络。

    

    近年来，超低比特量化取得了很多进展，承诺在边缘设备上显着提高延迟、内存占用和能量消耗。学习的步长量化等量化方法即使使用子字节量化也可以实现模型精度与全精度浮点基线相媲美。但是，要在主流CPU设备上部署这些超低比特量化模型非常具有挑战性，因为通用SIMD（单指令流多数据流）硬件通常支持不低于8位精度。为了克服这个限制，我们提出了基于查找表的DeepGEMM方法，用于在SIMD硬件上执行超低精度卷积神经网络。所提出的方法预先计算权重和激活的所有可能的乘积，将它们存储在查找表中，并在推断时高效地访问它们，以避免昂贵的乘加运算。我们的2位实现优于联

    A lot of recent progress has been made in ultra low-bit quantization, promising significant improvements in latency, memory footprint and energy consumption on edge devices. Quantization methods such as Learned Step Size Quantization can achieve model accuracy that is comparable to full-precision floating-point baselines even with sub-byte quantization. However, it is extremely challenging to deploy these ultra low-bit quantized models on mainstream CPU devices because commodity SIMD (Single Instruction, Multiple Data) hardware typically supports no less than 8-bit precision. To overcome this limitation, we propose DeepGEMM, a lookup table based approach for the execution of ultra low-precision convolutional neural networks on SIMD hardware. The proposed method precomputes all possible products of weights and activations, stores them in a lookup table, and efficiently accesses them at inference time to avoid costly multiply-accumulate operations. Our 2-bit implementation outperforms co
    
[^27]: CodeKGC：用于生成知识图谱构建的代码语言模型

    CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])

    [http://arxiv.org/abs/2304.09048](http://arxiv.org/abs/2304.09048)

    本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法，能够有效利用知识图谱内的语义结构，提高模型的可解释性。

    

    目前的生成式知识图谱构建方法通常无法捕捉结构性知识，而只是将自然语言转化为序列化文本或规范语言。然而，对于像代码这样的结构化数据进行训练的大型生成式语言模型已经展现了在理解自然语言以进行结构性预测和推理任务方面的卓越能力。本文提出了一种使用代码语言模型处理生成式知识图谱构建任务的方法。具体而言，在给定代码格式的自然语言输入的情况下，目标是生成可以表示为代码补全任务的三元组。我们开发了具有模式感知型提示的方法，可以有效利用知识图谱内的语义结构。由于代码本质上具有结构，如类和函数定义，因此它作为先验的语义结构知识模型非常有用。此外，我们采用了基于原理的生成方法来提高性能。原理提供了模型生成结果的可解释性。

    Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
    
[^28]: 应用于摩擦搅拌加工的神经元集参数微分方程

    Neural Lumped Parameter Differential Equations with Application in Friction-Stir Processing. (arXiv:2304.09047v1 [cs.LG])

    [http://arxiv.org/abs/2304.09047](http://arxiv.org/abs/2304.09047)

    本文提出使用通用微分方程（UDE）构建数据驱动模型以简化系统动力学到一个集中参数。应用于摩擦搅拌焊接中的沉头和停留阶段的建模任务中。

    

    集参数方法旨在将空间扩展或连续物理系统的演变简化为代表建模系统的物理尺度的“集中”元素的演变。对于定义集中元素或其关联物理学可能未知的系统，建模任务可能仅限于系统物理的全保真模拟。在本文中，我们考虑具有有限点测量的数据驱动建模任务的连续系统，并建立在通用微分方程（UDE）的概念上，以构建数据驱动模型，将动力学减少到一个集中参数，并推断其性质。UDE的灵活性允许组成适用于特定应用的建模任务的各种已知物理先验，包括集参数方法。这项工作的动机是摩擦搅拌焊接的沉头和停留阶段;具体地，将功率输入映射到工具中，然后推断所使用的集中参数建模。

    Lumped parameter methods aim to simplify the evolution of spatially-extended or continuous physical systems to that of a "lumped" element representative of the physical scales of the modeled system. For systems where the definition of a lumped element or its associated physics may be unknown, modeling tasks may be restricted to full-fidelity simulations of the physics of a system. In this work, we consider data-driven modeling tasks with limited point-wise measurements of otherwise continuous systems. We build upon the notion of the Universal Differential Equation (UDE) to construct data-driven models for reducing dynamics to that of a lumped parameter and inferring its properties. The flexibility of UDEs allow for composing various known physical priors suitable for application-specific modeling tasks, including lumped parameter methods. The motivating example for this work is the plunge and dwell stages for friction-stir welding; specifically, (i) mapping power input into the tool to
    
[^29]: CF-VAE：基于VAE和因果流的因果分离表示学习

    CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])

    [http://arxiv.org/abs/2304.09010](http://arxiv.org/abs/2304.09010)

    本文提出了一种新的因果流以进行因果分离表示学习，设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力，并展示了在合成和真实数据集上实现因果分离并进行干预实验的结果。

    

    学习分离表示在表示学习中至关重要，旨在学习数据的低维表示，其中每个维度对应一个潜在的生成因素。由于生成因素之间可能存在因果关系，因果分离表示学习已经受到广泛关注。本文首先提出了一种新的可以将因果结构信息引入模型中的流，称为因果流。基于广泛用于分离表示学习的变分自编码器（VAE），我们设计了一个新模型CF-VAE，利用因果流增强了VAE编码器的分离能力。通过进一步引入基准因素的监督，我们展示了我们模型的分离可识别性。在合成和真实数据集上的实验结果表明，CF-VAE可以实现因果分离并进行干预实验。

    Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
    
[^30]: 基于年龄和资源分配的NOMA网络下通信高效联邦学习的客户端选择方案

    Joint Age-based Client Selection and Resource Allocation for Communication-Efficient Federated Learning over NOMA Networks. (arXiv:2304.08996v1 [cs.LG])

    [http://arxiv.org/abs/2304.08996](http://arxiv.org/abs/2304.08996)

    本文针对联邦学习在无线网络上通信受限、收敛速度慢和资源有限等问题，提出了一种基于年龄和资源分配的客户端选择方案，旨在最小化每轮联邦学习的总时间消耗，从而提高联邦学习的性能。

    

    联邦学习（FL）是一种有效的分布式学习范式，它可以使得客户端使用本地数据协同训练全局模型。然而，当FL部署在无线网络上时，由于通信链路差和收敛速度慢，FL的性能常常受到限制。此外，由于无线资源受限，准确选取客户端和控制资源分配对于提高FL性能至关重要。鉴于这些挑战，在本文中提出了客户端选择和资源分配的联合优化问题，旨在最小化在非正交多址（NOMA）无线网络上每轮FL的总时间消耗。具体而言，我们首先提出了一种新的客户端选择方案，通过考虑收到的本地FL模型的新旧程度来设计，在此基础上，通过年龄更新（AoU）指标获得资源分配的闭合式解。

    Federated learning (FL) is a promising paradigm that enables distributed clients to collaboratively train a shared global model while keeping the training data locally. However, the performance of FL is often limited by poor communication links and slow convergence when FL is deployed over wireless networks. Besides, due to the limited radio resources, it is crucial to select clients and control resource allocation accurately for improved FL performance. Motivated by these challenges, a joint optimization problem of client selection and resource allocation is formulated in this paper, aiming to minimize the total time consumption of each round in FL over non-orthogonal multiple access (NOMA) enabled wireless network. Specifically, based on a metric termed the age of update (AoU), we first propose a novel client selection scheme by accounting for the staleness of the received local FL models. After that, the closed-form solutions of resource allocation are obtained by monotonicity analy
    
[^31]: Parcel3D：面向运输物流单张RGB图像的形状重建

    Parcel3D: Shape Reconstruction from Single RGB Images for Applications in Transportation Logistics. (arXiv:2304.08994v1 [cs.CV])

    [http://arxiv.org/abs/2304.08994](http://arxiv.org/abs/2304.08994)

    本文提出了一个新的Parcel3D数据集来解决单张RGB图像的物流包裹形状重建问题，并且提出了一种新的算法CubeRefine R-CNN来检测包裹是否受到损坏，该算法在Parcel3D数据集上表现出很好的性能。

    

    本文旨在实现在物流领域中的损坏和篡改检测，解决了对可能受损包裹的三维形状重建问题。我们利用单个RGB图像作为输入，适用于只有简单手持设备的情况，例如邮递员在递送过程中或客户在送货时需要使用。我们提出了一个名为Parcel3D的新型合成数据集，它基于Google扫描物品（GSO）数据集，包含超过13,000个带有完整三维注释的包裹图像。该数据集包含完整的、即长方体形状、未损坏的包裹和通过模拟生成的损坏包裹。我们通过提出一种称为CubeRefine R-CNN的新型结构，将3D边界框的估计与迭代网格细化相结合，以解决包裹的任意损坏问题。我们在Parcel3D和现有立方体形状包裹的真实场景数据集上进行了基准测试。研究结果表明，使用Parcel3D来训练可以实现迁移学习。

    We focus on enabling damage and tampering detection in logistics and tackle the problem of 3D shape reconstruction of potentially damaged parcels. As input we utilize single RGB images, which corresponds to use-cases where only simple handheld devices are available, e.g. for postmen during delivery or clients on delivery. We present a novel synthetic dataset, named Parcel3D, that is based on the Google Scanned Objects (GSO) dataset and consists of more than 13,000 images of parcels with full 3D annotations. The dataset contains intact, i.e. cuboid-shaped, parcels and damaged parcels, which were generated in simulations. We work towards detecting mishandling of parcels by presenting a novel architecture called CubeRefine R-CNN, which combines estimating a 3D bounding box with an iterative mesh refinement. We benchmark our approach on Parcel3D and an existing dataset of cuboid-shaped parcels in real-world scenarios. Our results show, that while training on Parcel3D enables transfer to th
    
[^32]: 深度神经网络可视化解释对数据增强的稳健性研究

    Robustness of Visual Explanations to Common Data Augmentation. (arXiv:2304.08984v1 [cs.CV])

    [http://arxiv.org/abs/2304.08984](http://arxiv.org/abs/2304.08984)

    本文研究了深度神经网络可视化解释对自然发生的转换（增强）的响应，发现不同解释方法的稳定性存在显着差异，证明解释相较于分类性能更容易受到增强的影响。

    

    随着深度神经网络的广泛应用，了解其行为变得比以往任何时候都更为重要。后续可解释性方法是潜在的解决方案，但它们的可靠性受到质疑。我们的研究探究后续可视化解释对自然发生的转换（通常称为增强）的响应。我们预计解释在某些转换下是不变的，例如更改颜色映射，同时对于像平移、对象缩放和旋转这样的转换则响应变换。我们发现，稳健性的不同程度存在显着差异，某些解释方法（例如LRP复合物和Guided Backprop）比其他方法更稳定。我们还探讨了使用数据增强进行培训的作用。我们提供证据表明，解释通常相较分类性能而言对增强的鲁棒性较差，无论数据增强是否明确地包括在训练过程中。

    As the use of deep neural networks continues to grow, understanding their behaviour has become more crucial than ever. Post-hoc explainability methods are a potential solution, but their reliability is being called into question. Our research investigates the response of post-hoc visual explanations to naturally occurring transformations, often referred to as augmentations. We anticipate explanations to be invariant under certain transformations, such as changes to the colour map while responding in an equivariant manner to transformations like translation, object scaling, and rotation. We have found remarkable differences in robustness depending on the type of transformation, with some explainability methods (such as LRP composites and Guided Backprop) being more stable than others. We also explore the role of training with data augmentation. We provide evidence that explanations are typically less robust to augmentation than classification performance, regardless of whether data augm
    
[^33]: ChatGPT可靠性的测量与特征化

    In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT. (arXiv:2304.08979v1 [cs.CR])

    [http://arxiv.org/abs/2304.08979](http://arxiv.org/abs/2304.08979)

    本文首次对ChatGPT在通用问答场景中的可靠性进行了大规模测量，发现其在不同领域的可靠性有所差异，尤其在法律和科学问题方面表现不佳，并容易受到对抗性示例的影响，对其在实际应用中的使用产生影响。

    

    随着ChatGPT的出现，用户获取信息的方式正在发生范式转变。与传统的搜索引擎不同，ChatGPT从模型本身检索知识并为用户生成答案。ChatGPT令人印象深刻的问答能力吸引了超过1亿用户，但也引发了人们关于其可靠性的担忧。本文通过精心策划的5695个问题跨越十个数据集和八个领域，首次对ChatGPT在通用问答场景中的可靠性进行了大规模测量。我们发现ChatGPT的可靠性因不同领域而异，尤其在法律和科学问题方面表现不佳。我们还证明了OpenAI设计的系统角色可以影响ChatGPT的可靠性。我们进一步展示了ChatGPT容易受到对抗性示例的影响，即使是单个字符的更改也会对其可靠性产生负面影响。我们的结果揭示了ChatGPT可靠性的局限性，并对其在实际应用中的使用产生影响。

    The way users acquire information is undergoing a paradigm shift with the advent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves knowledge from the model itself and generates answers for users. ChatGPT's impressive question-answering (QA) capability has attracted more than 100 million users within a short period of time but has also raised concerns regarding its reliability. In this paper, we perform the first large-scale measurement of ChatGPT's reliability in the generic QA scenario with a carefully curated set of 5,695 questions across ten datasets and eight domains. We find that ChatGPT's reliability varies across different domains, especially underperforming in law and science questions. We also demonstrate that system roles, originally designed by OpenAI to allow users to steer ChatGPT's behavior, can impact ChatGPT's reliability. We further show that ChatGPT is vulnerable to adversarial examples, and even a single character change can negatively affect its reli
    
[^34]: 随机鹦鹉寻找随机鹦鹉：LLMs易于微调且难以被其他LLMs检测到

    Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs. (arXiv:2304.08968v1 [cs.CL])

    [http://arxiv.org/abs/2304.08968](http://arxiv.org/abs/2304.08968)

    LLMs在公众中广泛应用，但是目前大部分检测工具存在严重缺陷。研究发现，LLMs容易微调且难以被其他LLMs检测到。

    

    自我注意力革命使生成式语言模型得以扩展并实现越来越惊人的能力。这些模型通常称为大型语言模型（LLMs），最近由于对话微调而在公众中获得了广泛关注，从而使其行为符合公众对于AI的期望。然而，这种突出也加大了关注LLMs误用的先前担忧，并导致出现许多在野外检测LLMs的工具。不幸的是，大多数这样的工具都存在严重缺陷。我们在这里展示了一种新方法，可以大大降低基于微调的自动编码器检测LLMs的成功率，并说明我们的工作涉及的重要细节。

    The self-attention revolution allowed generative language models to scale and achieve increasingly impressive abilities. Such models - commonly referred to as Large Language Models (LLMs) - have recently gained prominence with the general public, thanks to conversational fine-tuning, putting their behavior in line with public expectations regarding AI. This prominence amplified prior concerns regarding the misuse of LLMs and led to the emergence of numerous tools to detect LLMs in the wild.  Unfortunately, most such tools are critically flawed. While major publications in the LLM detectability field suggested that LLMs were easy to detect with fine-tuned autoencoders, the limitations of their results are easy to overlook. Specifically, they assumed publicly available generative models without fine-tunes or non-trivial prompts. While the importance of these assumptions has been demonstrated, until now, it remained unclear how well such detection could be countered.  Here, we show that a
    
[^35]: 使用SO(3)-等变隐式神经表示生成活细胞模型

    Generative modeling of living cells with SO(3)-equivariant implicit neural representations. (arXiv:2304.08960v1 [cs.CV])

    [http://arxiv.org/abs/2304.08960](http://arxiv.org/abs/2304.08960)

    本文提出了使用有符号距离函数作为形状表示，通过神经网络计算所得，对旋转具有等变性，来生成逼真的活细胞模型，为生物医学成像中的数据驱动细胞跟踪和分割方法提供高质量训练数据集。

    

    生物医学成像中基于数据的细胞跟踪和分割方法需要多样化和信息丰富的训练数据。当训练样本数量有限时，可以使用合成的计算机生成数据集来提高这些方法的准确性。这需要使用生成模型合成细胞形状以及相应的显微镜图像。为了合成逼真的活细胞形态，生成模型使用的形状表示应能够准确表示细节和拓扑变化，这在细胞中很常见。这些要求并不适用于3D体素掩模，因为它们有分辨率限制，也不适用于多边形网格，因为无法易于模拟细胞增长和有丝分裂等过程。在本文中，我们提出使用有符号距离函数（SDFs）的水平集来表示活细胞形状，这些水平集由神经网络估计得出，而且对旋转具有等变性。我们还介绍了一种将此表示转换为网格和RGB图像以进行可视化和用于下游计算机视觉任务的方法。

    Data-driven cell tracking and segmentation methods in biomedical imaging require diverse and information-rich training data. In cases where the number of training samples is limited, synthetic computer-generated data sets can be used to improve these methods. This requires the synthesis of cell shapes as well as corresponding microscopy images using generative models. To synthesize realistic living cell shapes, the shape representation used by the generative model should be able to accurately represent fine details and changes in topology, which are common in cells. These requirements are not met by 3D voxel masks, which are restricted in resolution, and polygon meshes, which do not easily model processes like cell growth and mitosis. In this work, we propose to represent living cell shapes as level sets of signed distance functions (SDFs) which are estimated by neural networks. We optimize a fully-connected neural network to provide an implicit representation of the SDF value at any p
    
[^36]: 从单词到音乐：符号音乐生成中的子词分词技术研究

    From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation. (arXiv:2304.08953v1 [cs.SD])

    [http://arxiv.org/abs/2304.08953](http://arxiv.org/abs/2304.08953)

    本文研究了符号音乐生成中的子词分词技术在生成更长、更具结构的音乐方面的有效性。实验结果表明BPE方法在符号音乐生成中可行且有效。

    

    子词分词在基于Transformer模型的自然语言处理方面已经得到广泛应用。随着Transformer模型在符号音乐领域的应用越来越普及，探究子词分词在符号音乐领域的有效性变得至关重要。本文探讨了符号音乐生成中的子词分词技术（如字节对编码BPE）及其对所生成歌曲整体结构的影响。我们的实验基于三种MIDI数据集：单音轨旋律、多轨单乐器和多轨多乐器。我们在音乐标记化之后应用子词分词方案，并发现它可以在相同时间内生成更长的歌曲，并且在结构指标（SI）、音高级别信息熵等客观指标方面改善了生成的音乐整体结构。我们还比较了两种子词分词方法，BPE和一种基于音高的方法。本文的实验结果表明，BPE方法在符号音乐生成中可行且有效。

    Subword tokenization has been widely successful in text-based natural language processing (NLP) tasks with Transformer-based models. As Transformer models become increasingly popular in symbolic music-related studies, it is imperative to investigate the efficacy of subword tokenization in the symbolic music domain. In this paper, we explore subword tokenization techniques, such as byte-pair encoding (BPE), in symbolic music generation and its impact on the overall structure of generated songs. Our experiments are based on three types of MIDI datasets: single track-melody only, multi-track with a single instrument, and multi-track and multi-instrument. We apply subword tokenization on post-musical tokenization schemes and find that it enables the generation of longer songs at the same time and improves the overall structure of the generated music in terms of objective metrics like structure indicator (SI), Pitch Class Entropy, etc. We also compare two subword tokenization methods, BPE a
    
[^37]: 通过主动奖励学习实现可证明反馈高效的强化学习

    Provably Feedback-Efficient Reinforcement Learning via Active Reward Learning. (arXiv:2304.08944v1 [cs.LG])

    [http://arxiv.org/abs/2304.08944](http://arxiv.org/abs/2304.08944)

    本文提出了一种主动奖励学习的强化学习算法，采用人机交互来指定任务的奖励，大大减少了所需的人类反馈，并在效率上提供了理论保证。

    

    在强化学习中，合适的奖励函数对于明确任务非常重要。但是，即使对于简单的任务而言，设计正确的奖励函数也是非常具有挑战性的。人机交互强化学习（HiL RL）允许人类通过提供各种类型的反馈来向RL代理传达复杂的目标。然而，尽管取得了巨大的经验成功，但HiL RL通常需要太多人类教师的反馈，并且理论理解不足。本文从理论角度解决了这个问题，旨在提供可证明的反馈高效的算法框架，以采用人机交互来指定给定任务的奖励。我们提供了一种基于主动学习的RL算法，该算法首先在不指定奖励函数的情况下探索环境，然后仅询问人类教师有关某些状态动作对的任务奖励的少量查询。之后，该算法学习具有可证明遗憾界限的奖励函数，并实现接近最优的性能。与先前的HiL RL方法相比，我们的算法大大减少了所需的人类反馈，并在效率上提供了理论保证。

    An appropriate reward function is of paramount importance in specifying a task in reinforcement learning (RL). Yet, it is known to be extremely challenging in practice to design a correct reward function for even simple tasks. Human-in-the-loop (HiL) RL allows humans to communicate complex goals to the RL agent by providing various types of feedback. However, despite achieving great empirical successes, HiL RL usually requires too much feedback from a human teacher and also suffers from insufficient theoretical understanding. In this paper, we focus on addressing this issue from a theoretical perspective, aiming to provide provably feedback-efficient algorithmic frameworks that take human-in-the-loop to specify rewards of given tasks. We provide an active-learning-based RL algorithm that first explores the environment without specifying a reward function and then asks a human teacher for only a few queries about the rewards of a task at some state-action pairs. After that, the algorith
    
[^38]: ProGAP: 具有差分隐私保证的渐进图神经网络

    ProGAP: Progressive Graph Neural Networks with Differential Privacy Guarantees. (arXiv:2304.08928v1 [cs.LG])

    [http://arxiv.org/abs/2304.08928](http://arxiv.org/abs/2304.08928)

    ProGAP是一种新的差分隐私GNN模型，采用逐步训练方案来提高准确性和隐私之间的平衡，通过将GNN分成一系列重叠的子模型来训练。这种方法可以保护隐私并允许有效学习图形结构数据。

    

    图神经网络（GNN）已成为学习图形数据的常用工具，但广泛使用引发了隐私问题，因为图形数据可能包含个人或敏感信息。为了保护隐私并允许对图结构数据进行有效处理，最近提出了差分隐私GNN模型。然而，由于图的固有结构连接性，GNN在准确性和隐私之间取得平衡仍然具有挑战性。在本文中，我们提出了一种名为ProGAP的新型差分隐私GNN，采用逐步训练方案来提高准确性和隐私之间的平衡。结合聚合扰动技术以确保差分隐私，ProGAP将GNN分成一系列重叠的子模型，逐步进行训练，从第一个子模型扩展到完整模型。具体而言，每个子模型都是基于通过私有聚合学习和缓存的节点嵌入进行训练的。

    Graph Neural Networks (GNNs) have become a popular tool for learning on graphs, but their widespread use raises privacy concerns as graph data can contain personal or sensitive information. Differentially private GNN models have been recently proposed to preserve privacy while still allowing for effective learning over graph-structured datasets. However, achieving an ideal balance between accuracy and privacy in GNNs remains challenging due to the intrinsic structural connectivity of graphs. In this paper, we propose a new differentially private GNN called ProGAP that uses a progressive training scheme to improve such accuracy-privacy trade-offs. Combined with the aggregation perturbation technique to ensure differential privacy, ProGAP splits a GNN into a sequence of overlapping submodels that are trained progressively, expanding from the first submodel to the complete model. Specifically, each submodel is trained over the privately aggregated node embeddings learned and cached by the
    
[^39]: 了解数据预处理以有效地进行深度神经网络端到端训练

    Understand Data Preprocessing for Effective End-to-End Training of Deep Neural Networks. (arXiv:2304.08925v1 [cs.LG])

    [http://arxiv.org/abs/2304.08925](http://arxiv.org/abs/2304.08925)

    本文主要研究公共云环境下DNN训练的数据预处理流程，并发现数据预处理是训练的瓶颈，提出优化方法来解决瓶颈问题。

    

    本文主要关注公共云环境下DNN训练的数据预处理流程。我们首先进行实验测试，以测试使用原始数据或记录文件的两种主要数据预处理方法的性能影响。初步结果表明，即使启用了NVIDIA DALI（高度优化的数据预处理库）的最高效软件和硬件配置，数据预处理仍然是一个明显的瓶颈。其次，我们确定了潜在的原因，运用了各种优化方法，并提出了它们的优缺点。我们希望这项工作能为“数据存储、加载管道”和“训练框架”的新协同设计以及它们之间的灵活资源配置提供启示，以便充分利用资源并最大化性能。

    In this paper, we primarily focus on understanding the data preprocessing pipeline for DNN Training in the public cloud. First, we run experiments to test the performance implications of the two major data preprocessing methods using either raw data or record files. The preliminary results show that data preprocessing is a clear bottleneck, even with the most efficient software and hardware configuration enabled by NVIDIA DALI, a high-optimized data preprocessing library. Second, we identify the potential causes, exercise a variety of optimization methods, and present their pros and cons. We hope this work will shed light on the new co-design of ``data storage, loading pipeline'' and ``training framework'' and flexible resource configurations between them so that the resources can be fully exploited and performance can be maximized.
    
[^40]: 量子退火用于单张图像超分辨率

    Quantum Annealing for Single Image Super-Resolution. (arXiv:2304.08924v1 [cs.CV])

    [http://arxiv.org/abs/2304.08924](http://arxiv.org/abs/2304.08924)

    本文利用量子算法解决单张图像超分辨率问题，特别是采用量子退火优化算法在低分辨率空间中寻找高分辨率图像。

    

    本文提出了一种基于量子计算的算法来解决单张图像超分辨率（SISR）问题。本文探索了将量子计算算法应用于这一重要的图像增强问题中，即超分辨率问题。其中，利用AQC子类质子退火进行优化，通过在低分辨率空间中寻找所需的高分辨率图像来优化目标函数。

    This paper proposes a quantum computing-based algorithm to solve the single image super-resolution (SISR) problem. One of the well-known classical approaches for SISR relies on the well-established patch-wise sparse modeling of the problem. Yet, this field's current state of affairs is that deep neural networks (DNNs) have demonstrated far superior results than traditional approaches. Nevertheless, quantum computing is expected to become increasingly prominent for machine learning problems soon. As a result, in this work, we take the privilege to perform an early exploration of applying a quantum computing algorithm to this important image enhancement problem, i.e., SISR. Among the two paradigms of quantum computing, namely universal gate quantum computing and adiabatic quantum computing (AQC), the latter has been successfully applied to practical computer vision problems, in which quantum parallelism has been exploited to solve combinatorial optimization efficiently. This work demonst
    
[^41]: 姿态约束用于一致的自监督单目深度估计和自我运动估计

    Pose Constraints for Consistent Self-supervised Monocular Depth and Ego-motion. (arXiv:2304.08916v1 [cs.CV])

    [http://arxiv.org/abs/2304.08916](http://arxiv.org/abs/2304.08916)

    该论文提出了一种使用姿态约束的方法来实现一致的自监督单目深度估计和自我运动估计，并成功地减少了深度不一致性和提高了预测性能。

    

    自监督单目深度估计方法不仅存在尺度歧义，而且在尺度方面的深度图推断不一致。尽管在训练过程中消除尺度歧义是不可能的，但是有一致的尺度预测可以在推断期间计算一次，然后在时间内使用。为了实现这一目标，引入了一组时间一致性损失，以在时间上最小化姿态不一致性。评估结果显示，引入这些约束不仅减少了深度不一致性，而且提高了深度和自我运动预测的基线性能。

    Self-supervised monocular depth estimation approaches suffer not only from scale ambiguity but also infer temporally inconsistent depth maps w.r.t. scale. While disambiguating scale during training is not possible without some kind of ground truth supervision, having scale consistent depth predictions would make it possible to calculate scale once during inference as a post-processing step and use it over-time. With this as a goal, a set of temporal consistency losses that minimize pose inconsistencies over time are introduced. Evaluations show that introducing these constraints not only reduces depth inconsistencies but also improves the baseline performance of depth and ego-motion prediction.
    
[^42]: 高维符号回归的可微分遗传编程

    Differentiable Genetic Programming for High-dimensional Symbolic Regression. (arXiv:2304.08915v1 [cs.NE])

    [http://arxiv.org/abs/2304.08915](http://arxiv.org/abs/2304.08915)

    本文首次提出了DGP方法，利用可微分符号树构建遗传编程树，有效解决了高维符号回归问题。

    

    符号回归是从数学表达式中发现数据间隐藏关系的过程，被视为实现可解释的机器学习的有效方式。遗传编程是解决符号回归问题的主要方法。然而，随着符号回归问题规模的增加，传统的遗传编程的随机进化性质造成其在解决高维实际问题中表现不佳。本文提出了一种不同iable的方法——DGP，首次构建了用于高维符号回归的遗传编程树。具体而言，提出一种称为可微分符号树的新数据结构，将离散结构松弛到连续结构，因此可以提供基于梯度的优化器来实现高效优化。此外，提出了一种采样方法，用于消除由此松弛引起的不一致性，从而得到有效的符号树。

    Symbolic regression (SR) is the process of discovering hidden relationships from data with mathematical expressions, which is considered an effective way to reach interpretable machine learning (ML). Genetic programming (GP) has been the dominator in solving SR problems. However, as the scale of SR problems increases, GP often poorly demonstrates and cannot effectively address the real-world high-dimensional problems. This limitation is mainly caused by the stochastic evolutionary nature of traditional GP in constructing the trees. In this paper, we propose a differentiable approach named DGP to construct GP trees towards high-dimensional SR for the first time. Specifically, a new data structure called differentiable symbolic tree is proposed to relax the discrete structure to be continuous, thus a gradient-based optimizer can be presented for the efficient optimization. In addition, a sampling method is proposed to eliminate the discrepancy caused by the above relaxation for valid sym
    
[^43]: 神经崩溃现象的研究：Grassmannian Frame、对称性和泛化

    A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry, Generalization. (arXiv:2304.08914v1 [cs.LG])

    [http://arxiv.org/abs/2304.08914](http://arxiv.org/abs/2304.08914)

    本文提出了广义神经崩溃假设，发现了Grassmannian Frame结构和对称泛化现象，这对特征选择和神经网络设计都具有重要作用。

    

    本文通过证明广义神经崩溃假设推广了原始的神经崩溃现象。我们通过分类的优化和泛化得到了Grassmannian Frame结构。该结构在球面上最大化地分离了每两个类别的特征，并且不需要一个更大的特征维度。出于对Grassmannian Frame对称性的好奇，我们进行了一系列实验，探索不同Grassmannian Frame模型是否会产生不同的表现。结果我们发现了对称泛化现象。我们提出了一个关于置换对称泛化的定理。然而，为什么特征的不同方向会导致如此不同的泛化现象的问题仍然需要进一步研究。

    In this paper, we extends original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.
    
[^44]: 带有自我改进硬约束的多能源管理系统的安全强化学习

    Safe reinforcement learning with self-improving hard constraints for multi-energy management systems. (arXiv:2304.08897v1 [eess.SY])

    [http://arxiv.org/abs/2304.08897](http://arxiv.org/abs/2304.08897)

    本论文提出了一种安全强化学习方法，能够实现多能源管理系统中的最优控制，在保证硬约束的前提下减少工程工作，降低建模偏差，并避免潜在的不安全行为。

    

    带有硬约束保证的安全强化学习是多能源管理系统中最有前途的最优控制方向。它只需要在环境特定的约束函数本身上预先而不是完整的模型（即植物，干扰和噪声模型，以及未包括在植物模型中的状态的预测模型 - 例如需求，天气和价格预测）。因此，可减少项目特定的前期和持续的工程工作，仍可以学习更好地表示基础系统动态，并使建模偏差最小化（无基于模型的目标函数）。然而，即使仅约束函数本身有时也不总是容易提供准确的先验（例如能量平衡约束需要详细确定所有能量输入和输出），从而导致潜在的不安全行为。在本文中，我们提出了两个新的进展：（I）将Optlayer和SafeFallback方法结合起来，命名为O

    Safe reinforcement learning (RL) with hard constraint guarantees is a promising optimal control direction for multi-energy management systems. It only requires the environment-specific constraint functions itself a prior and not a complete model (i.e. plant, disturbance and noise models, and prediction models for states not included in the plant model - e.g. demand, weather, and price forecasts). The project-specific upfront and ongoing engineering efforts are therefore still reduced, better representations of the underlying system dynamics can still be learned and modeling bias is kept to a minimum (no model-based objective function). However, even the constraint functions alone are not always trivial to accurately provide in advance (e.g. an energy balance constraint requires the detailed determination of all energy inputs and outputs), leading to potentially unsafe behavior. In this paper, we present two novel advancements: (I) combining the Optlayer and SafeFallback method, named O
    
[^45]: 金融参数化神经网络

    Parameterized Neural Networks for Finance. (arXiv:2304.08883v1 [q-fin.ST])

    [http://arxiv.org/abs/2304.08883](http://arxiv.org/abs/2304.08883)

    本文介绍了一种神经网络架构，可以学习一组不同数据样本的模型类，通过调整较少的参数即可为新的、特定的问题建模，该方法在资产管理和银行业有广泛的应用潜力。

    

    本文讨论和分析了一种神经网络架构，可以学习一组不同数据样本的模型类，而不仅仅是针对特定数据样本的单一模型。从这个意义上讲，它可以帮助减少过拟合问题，因为在学习了这种模型类之后，仅需要调整少量参数即可为新的、特定的问题建模。在理论分析和不同一维问题的回归实例之后，我们最终将此方法应用于资产管理公司和银行面临的标准问题之一：调整收益率曲线。呈现的结果清楚地展示了这种方法的潜力。此外，这种应用对金融从业人员特别感兴趣，因为几乎所有拥有解决方案的资产管理公司和银行在ESG评级方面也需要调整或甚至改变他们当前的方法。

    We discuss and analyze a neural network architecture, that enables learning a model class for a set of different data samples rather than just learning a single model for a specific data sample. In this sense, it may help to reduce the overfitting problem, since, after learning the model class over a larger data sample consisting of such different data sets, just a few parameters need to be adjusted for modeling a new, specific problem. After analyzing the method theoretically and by regression examples for different one-dimensional problems, we finally apply the approach to one of the standard problems asset managers and banks are facing: the calibration of spread curves. The presented results clearly show the potential that lies within this method. Furthermore, this application is of particular interest to financial practitioners, since nearly all asset managers and banks which are having solutions in place may need to adapt or even change their current methodologies when ESG ratings
    
[^46]: 深度神经网络在早期多模态 MRI 中分割胶质母细胞瘤

    Segmentation of glioblastomas in early post-operative multi-modal MRI with deep neural networks. (arXiv:2304.08881v1 [eess.IV])

    [http://arxiv.org/abs/2304.08881](http://arxiv.org/abs/2304.08881)

    深度神经网络方法在多中心MRI数据集上实现了高性能的胶质母细胞瘤残留肿瘤分割，可提高残留肿瘤的准确估计。

    

    手术后切除程度是诊断为胶质母细胞瘤患者的主要预后因素之一。为了实现这一目标，准确地将术后 MR 图像中的残留肿瘤进行分割和分类至关重要。目前，用于估计残留肿瘤的标准方法存在高度的性间和性内评分人员差异性，而自动化的方法可在早期术后 MRI 中实现残留肿瘤的分割，从而更准确地估计切除程度。本研究训练了两种最新的预操作分割神经网络架构以完成此任务。这些模型在涵盖欧洲和美国12家医院的近1000名患者的多中心数据集上得到广泛验证。最佳性能达到了61\%的Dice分数和约80\%的均衡准确性分类性能，并展示了模型在不同医院之间有效泛化的能力。此外，最佳模型的分割性能得到了显著的提高。

    Extent of resection after surgery is one of the main prognostic factors for patients diagnosed with glioblastoma. To achieve this, accurate segmentation and classification of residual tumor from post-operative MR images is essential. The current standard method for estimating it is subject to high inter- and intra-rater variability, and an automated method for segmentation of residual tumor in early post-operative MRI could lead to a more accurate estimation of extent of resection. In this study, two state-of-the-art neural network architectures for pre-operative segmentation were trained for the task. The models were extensively validated on a multicenter dataset with nearly 1000 patients, from 12 hospitals in Europe and the United States. The best performance achieved was a 61\% Dice score, and the best classification performance was about 80\% balanced accuracy, with a demonstrated ability to generalize across hospitals. In addition, the segmentation performance of the best models w
    
[^47]: NPS: 使用图神经网络实现精准程序采样的框架

    NPS: A Framework for Accurate Program Sampling Using Graph Neural Network. (arXiv:2304.08880v1 [cs.AR])

    [http://arxiv.org/abs/2304.08880](http://arxiv.org/abs/2304.08880)

    NPS是一种使用图神经网络进行程序采样的框架，通过学习执行嵌入并快速生成代表性模拟点，实现了高效的微处理器设计。

    

    随着摩尔定律结束，现代处理器（如RISC-V自定义扩展）需要快速的架构创新来维持性能增长，程序采样是微处理器设计中关键的一步，因为它选择工作量模拟的代表性模拟点。本文介绍了神经程序采样（NPS），它使用动态快照的图神经网络学习执行嵌入。NPS采用AssemblyNet进行嵌入生成，利用应用程序的代码结构和运行时状态，将AssemblyNet作为NPS的图模型和神经架构，捕获程序的行为，在数据计算、代码路径和数据流等方面。实验结果显示，NPS在速度和准确性方面优于SimPoint，在提高工作负载代表性率的同时，将模拟时间缩短高达60%。

    With the end of Moore's Law, there is a growing demand for rapid architectural innovations in modern processors, such as RISC-V custom extensions, to continue performance scaling. Program sampling is a crucial step in microprocessor design, as it selects representative simulation points for workload simulation. While SimPoint has been the de-facto approach for decades, its limited expressiveness with Basic Block Vector (BBV) requires time-consuming human tuning, often taking months, which impedes fast innovation and agile hardware development. This paper introduces Neural Program Sampling (NPS), a novel framework that learns execution embeddings using dynamic snapshots of a Graph Neural Network. NPS deploys AssemblyNet for embedding generation, leveraging an application's code structures and runtime states. AssemblyNet serves as NPS's graph model and neural architecture, capturing a program's behavior in aspects such as data computation, code path, and data flow. AssemblyNet is trained
    
[^48]: 基于罗马化的多语言模型大规模适应

    Romanization-based Large-scale Adaptation of Multilingual Language Models. (arXiv:2304.08865v1 [cs.CL])

    [http://arxiv.org/abs/2304.08865](http://arxiv.org/abs/2304.08865)

    该论文探索利用大规模转写来提升大型多语言预训练语言模型的处理低资源和未知语言的能力，利用UROMAN转写工具的潜力，并研究了一系列高效的策略，以适应各种语言数据。

    

    大型多语言预训练语言模型（mPLMs）已成为跨语言NLP中的事实标准，但是，它们在许多语言的大规模部署方面受到诸多限制，包括预训练数据稀缺、词汇量增加和参数预算的限制。为了增强mPLMs处理低资源和未知语言的能力，我们探索大规模利用转写的潜力。具体而言，我们探索UROMAN转写工具的潜力，该工具为所有书写系统提供了从UTF-8到拉丁字符的映射，从而实现了几乎任何语言的廉价罗马化。首先，我们重点研究了UROMAN相对于其他语言特定和手动策划的转写工具在适应多语言PLMs方面的差异。然后，我们研究并比较了一系列数据和参数高效的策略，以适应罗马化和非罗马化的14种不同以上语言数据。

    Large multilingual pretrained language models (mPLMs) have become the de facto state of the art for cross-lingual transfer in NLP. However, their large-scale deployment to many languages, besides pretraining data scarcity, is also hindered by the increase in vocabulary size and limitations in their parameter budget. In order to boost the capacity of mPLMs to deal with low-resource and unseen languages, we explore the potential of leveraging transliteration on a massive scale. In particular, we explore the UROMAN transliteration tool, which provides mappings from UTF-8 to Latin characters for all the writing systems, enabling inexpensive romanization for virtually any language. We first focus on establishing how UROMAN compares against other language-specific and manually curated transliterators for adapting multilingual PLMs. We then study and compare a plethora of data- and parameter-efficient strategies for adapting the mPLMs to romanized and non-romanized corpora of 14 diverse low-r
    
[^49]: 基于领域区域的机器学习性能鲁棒性对协变量偏移的评估

    A Domain-Region Based Evaluation of ML Performance Robustness to Covariate Shift. (arXiv:2304.08855v1 [cs.LG])

    [http://arxiv.org/abs/2304.08855](http://arxiv.org/abs/2304.08855)

    本文实验评估了传统机器学习模型在协变量转移存在的情况下的性能。根据实验分析，随机森林是一种对协变量移位较不敏感的模型。

    

    大多数机器学习方法都假设训练和测试阶段的输入数据分布相同。然而，在实践中，这种稳定性通常不能满足，导致了所学模型在部署时出现了意外的表现。当训练和测试数据输入的概率分布不同，但输入输出关系保持不变时，这个问题被称为协变量转移。本文实验评估了传统机器学习模型在协变量转移存在的情况下的性能。此外，通过对输入数据的概率密度函数的域进行分解，进行了基于区域的评估，以评估分类器在每个域区域的性能。在一个二维分类问题中模拟了分布变化，随后进行了更高维度的四维实验。根据实验分析，随机森林是一种对协变量移位较不敏感的模型。

    Most machine learning methods assume that the input data distribution is the same in the training and testing phases. However, in practice, this stationarity is usually not met and the distribution of inputs differs, leading to unexpected performance of the learned model in deployment. The issue in which the training and test data inputs follow different probability distributions while the input-output relationship remains unchanged is referred to as covariate shift. In this paper, the performance of conventional machine learning models was experimentally evaluated in the presence of covariate shift. Furthermore, a region-based evaluation was performed by decomposing the domain of probability density function of the input data to assess the classifier's performance per domain region. Distributional changes were simulated in a two-dimensional classification problem. Subsequently, a higher four-dimensional experiments were conducted. Based on the experimental analysis, the Random Forests
    
[^50]: BadVFL: 竖直联邦学习中的后门攻击

    BadVFL: Backdoor Attacks in Vertical Federated Learning. (arXiv:2304.08847v1 [cs.LG])

    [http://arxiv.org/abs/2304.08847](http://arxiv.org/abs/2304.08847)

    本文聚焦于竖直联邦学习中的后门攻击的鲁棒性问题，提出了一种新的后门攻击框架BadVFL，可以有效地将后门注入VFL的训练过程中，成功率高并且误分类率很低。

    

    联邦学习（FL）使多个参与者可以在不共享其数据的情况下协作地训练机器学习模型；相反，他们在本地训练自己的模型，并将更新发送到中央服务器进行聚合。根据数据在参与者之间的分布方式，FL可以分为水平（HFL）和竖直（VFL）。在VFL中，参与者共享相同的训练实例集，但仅托管整个特征空间的不同和非重叠子集。而在HFL中，每个参与者共享相同的特征集，而训练集被分为本地拥有的训练数据子集。尽管VFL越来越多地应用于金融欺诈检测等应用程序中，但很少有工作分析其安全性。本文重点研究VFL的鲁棒性，特别是后门攻击，其中对手试图在训练过程中操纵聚合模型以触发错误分类。在VFL上执行后门攻击可以创建严重的安全和隐私问题，因为它可以允许攻击者有针对性地控制模型预测的结果。为此，我们提出了一种新的VFL后门攻击框架，称为BadVFL，它可以有效地将后门注入VFL的训练过程中。我们的方法不仅考虑数据的特征，还适应于VFL中使用的不同中心度量。在三个数据集上进行的大量实验证明了我们的方法在保持干净数据的低失真的同时实现了高攻击成功率的有效性。

    Federated learning (FL) enables multiple parties to collaboratively train a machine learning model without sharing their data; rather, they train their own model locally and send updates to a central server for aggregation. Depending on how the data is distributed among the participants, FL can be classified into Horizontal (HFL) and Vertical (VFL). In VFL, the participants share the same set of training instances but only host a different and non-overlapping subset of the whole feature space. Whereas in HFL, each participant shares the same set of features while the training set is split into locally owned training data subsets.  VFL is increasingly used in applications like financial fraud detection; nonetheless, very little work has analyzed its security. In this paper, we focus on robustness in VFL, in particular, on backdoor attacks, whereby an adversary attempts to manipulate the aggregate model during the training process to trigger misclassifications. Performing backdoor attack
    
[^51]: 可行性策略迭代

    Feasible Policy Iteration. (arXiv:2304.08845v1 [cs.LG])

    [http://arxiv.org/abs/2304.08845](http://arxiv.org/abs/2304.08845)

    可行性策略迭代 (FPI) 是一个间接的安全强化学习方法，使用上一个策略的可行域来迭代地限制当前策略。可行性策略改进是其核心，它在可行域内最大化回报，在可行域外最小化约束衰减函数 (CDF).

    

    安全强化学习旨在在安全约束下解决最优控制问题。现有的 $\textit{直接}$ 安全强化学习方法会在整个学习过程中一直使用原始约束。它们或者缺乏策略迭代期间的理论保证，或者遭遇不可行性问题。为了解决这个问题，我们提出了一个叫做可行性策略迭代（FPI）的 $\textit{间接}$ 安全强化学习方法，它使用最后一个策略的可行域来迭代地限制当前策略。可行域由一个叫做约束衰减函数（CDF）的可行性函数表示。FPI 的核心是一个叫做可行性策略改进的区域性策略更新规则，它在可行域内最大化回报，在可行域外最小化 CDF。这个更新规则总是可行的，并确保可行域单调地扩展，状态值函数单调地增长。

    Safe reinforcement learning (RL) aims to solve an optimal control problem under safety constraints. Existing $\textit{direct}$ safe RL methods use the original constraint throughout the learning process. They either lack theoretical guarantees of the policy during iteration or suffer from infeasibility problems. To address this issue, we propose an $\textit{indirect}$ safe RL method called feasible policy iteration (FPI) that iteratively uses the feasible region of the last policy to constrain the current policy. The feasible region is represented by a feasibility function called constraint decay function (CDF). The core of FPI is a region-wise policy update rule called feasible policy improvement, which maximizes the return under the constraint of the CDF inside the feasible region and minimizes the CDF outside the feasible region. This update rule is always feasible and ensures that the feasible region monotonically expands and the state-value function monotonically increases inside 
    
[^52]: UDTIRI:一个开源的道路坑洞检测基准套件

    UDTIRI: An Open-Source Road Pothole Detection Benchmark Suite. (arXiv:2304.08842v1 [cs.CV])

    [http://arxiv.org/abs/2304.08842](http://arxiv.org/abs/2304.08842)

    该论文介绍了一个开源的道路坑洞检测基准套件UDTIRI，包含了标记齐全的1000张道路坑洞图像，可以用于深度学习方法在城市道路检查中的目标检测、语义分割和实例分割任务。

    

    看到在城市数字孪生领域中利用强大的深度学习方法的巨大潜力。特别是在智能道路检查领域，目前研究和数据有限。为了促进这一领域的进展，我们开发了一个名为Urban Digital Twins Intelligent Road Inspection (UDTIRI)数据集的标记齐全的道路坑洞数据集。我们希望这个数据集能够让强大的深度学习方法在城市道路检查中发挥作用，让算法更全面地理解场景并最大化其潜力。我们的数据集包括1000张道路坑洞图像，拍摄于不同的情境中，具有不同的光照和湿度条件。我们的意图是将这个数据集应用于目标检测、语义分割和实例分割任务。我们的团队花费了大量精力进行了详细的统计分析，并对UDTIRI数据集的一些代表性深度学习模型进行了基准测试。

    It is seen that there is enormous potential to leverage powerful deep learning methods in the emerging field of urban digital twins. It is particularly in the area of intelligent road inspection where there is currently limited research and data available. To facilitate progress in this field, we have developed a well-labeled road pothole dataset named Urban Digital Twins Intelligent Road Inspection (UDTIRI) dataset. We hope this dataset will enable the use of powerful deep learning methods in urban road inspection, providing algorithms with a more comprehensive understanding of the scene and maximizing their potential. Our dataset comprises 1000 images of potholes, captured in various scenarios with different lighting and humidity conditions. Our intention is to employ this dataset for object detection, semantic segmentation, and instance segmentation tasks. Our team has devoted significant effort to conducting a detailed statistical analysis, and benchmarking a selection of represent
    
[^53]: 用于图反问题中源定位的两阶段去噪扩散模型

    Two-stage Denoising Diffusion Model for Source Localization in Graph Inverse Problems. (arXiv:2304.08841v1 [cs.LG])

    [http://arxiv.org/abs/2304.08841](http://arxiv.org/abs/2304.08841)

    本文提出了一个用于图反问题中的源定位的两阶段去噪扩散模型，通过两个阶段的迭代去噪，在保证精度的同时赋予计算效率。

    

    源定位是图信息传播的反问题，具有广泛的实际应用。然而，信息传播中的内在复杂性和不确定性带来了重大挑战，而源定位问题的病态使这些挑战更加严峻。最近，特别是受经典非平衡热力学启发的扩散模型，尤其是深度生成模型取得了显著进展。虽然扩散模型在解决反问题和产生高质量重建方面已经被证明是有效的，但直接将其应用于源定位是不可行的，这是因为有两个原因。首先，在大规模网络上计算后验传播结果进行迭代去噪采样是不可能的，这将导致巨大的计算成本。其次，在现有的方法中，训练数据本身是病态的（多对一）；因此，简单地将扩散模型转移到此领域会导致令人不满意的性能。在本文中，我们提出了一个两阶段去噪扩散模型，通过两个阶段对后验传播图进行迭代去噪来解决图上源定位问题。所提出的模型将扩散模型的优点与去噪技术相结合，并通过采用更高效的消息传递算法克服了计算上的不可行性。在合成和真实数据集上的实验结果证明了我们所提出的模型的有效性和优越性。

    Source localization is the inverse problem of graph information dissemination and has broad practical applications.  However, the inherent intricacy and uncertainty in information dissemination pose significant challenges, and the ill-posed nature of the source localization problem further exacerbates these challenges. Recently, deep generative models, particularly diffusion models inspired by classical non-equilibrium thermodynamics, have made significant progress. While diffusion models have proven to be powerful in solving inverse problems and producing high-quality reconstructions, applying them directly to the source localization is infeasible for two reasons. Firstly, it is impossible to calculate the posterior disseminated results on a large-scale network for iterative denoising sampling, which would incur enormous computational costs. Secondly, in the existing methods for this field, the training data itself are ill-posed (many-to-one); thus simply transferring the diffusion mo
    
[^54]: 基于神经网络观测器的自主非线性系统传感器故障检测与隔离

    Sensor Fault Detection and Isolation in Autonomous Nonlinear Systems Using Neural Network-Based Observers. (arXiv:2304.08837v1 [math.OC])

    [http://arxiv.org/abs/2304.08837](http://arxiv.org/abs/2304.08837)

    本文介绍了一种基于神经网络的观测器方法，可用于检测和隔离工业系统中的传感器故障，适用于一般的自主非线性系统，通过学习实现Lueneberger观察器的设计，通过残留生成检测传感器故障并实现故障隔离。

    

    本文介绍了一种基于观测器的新方法，用于检测和隔离工业系统中的传感器故障。考虑了两种类型的传感器故障：完全故障和传感器劣化。所提出的方法适用于一般的自主非线性系统，而不需对其三角形和/或正常形式进行任何假设，这通常在观察者设计文献中考虑。我们方法的关键是Lueneberger观察器的基于学习的设计，其中涉及使用神经网络来近似将非线性系统转化为具有输出注入的稳定线性系统的单射映射。这种基于学习的Lueneberger观察器准确估计了系统的状态，从而通过残留生成实现传感器故障的检测。残差是通过计算系统测量输出和观察者预测输出向量之间差值的范数而得出的。故障隔离是通过将每个传感器的测量输出与残差信号进行比较来实现的。该方法的有效性和鲁棒性通过对双罐系统的模拟结果进行演示。

    This paper presents a new observer-based approach to detect and isolate faulty sensors in industrial systems. Two types of sensor faults are considered: complete failure and sensor deterioration. The proposed method is applicable to general autonomous nonlinear systems without making any assumptions about its triangular and/or normal form, which is usually considered in the observer design literature. The key aspect of our approach is a learning-based design of the Luenberger observer, which involves using a neural network to approximate the injective map that transforms the nonlinear system into a stable linear system with output injection. This learning-based Luenberger observer accurately estimates the system's state, allowing for the detection of sensor faults through residual generation. The residual is computed as the norm of the difference between the system's measured output and the observer's predicted output vectors. Fault isolation is achieved by comparing each sensor's meas
    
[^55]: TTIDA: 通过文本到文本模型和文本到图像模型进行可控生成数据增强

    TTIDA: Controllable Generative Data Augmentation via Text-to-Text and Text-to-Image Models. (arXiv:2304.08821v1 [cs.CV])

    [http://arxiv.org/abs/2304.08821](http://arxiv.org/abs/2304.08821)

    本论文提出了一种名为TTIDA的生成式数据增强方法，利用文本到文本和文本到图像模型生成可控的逼真标记图像。

    

    数据增强已被证明是一种增补低资源数据集有用信息的有效方法。传统的增强技术，如噪声注入和图像变换，已被广泛使用。此外，生成式数据增强（GDA）已被证明能够产生更多样化和灵活的数据。虽然生成对抗网络（GAN）经常用于GDA，但与文本到图像扩散模型相比，它们缺乏多样性和可控性。在本文中，我们提出了TTIDA（文本到文本到图像数据增强），利用大规模预训练的文本到文本（T2T）和文本到图像（T2I）生成模型进行数据增强。通过将T2I模型的条件设置为T2T模型生成的详细描述，我们能够以灵活和可控的方式生成逼真的标记图像。在领域内分类、跨领域分类和图像字幕任务的实验中，展示了一致的结果。

    Data augmentation has been established as an efficacious approach to supplement useful information for low-resource datasets. Traditional augmentation techniques such as noise injection and image transformations have been widely used. In addition, generative data augmentation (GDA) has been shown to produce more diverse and flexible data. While generative adversarial networks (GANs) have been frequently used for GDA, they lack diversity and controllability compared to text-to-image diffusion models. In this paper, we propose TTIDA (Text-to-Text-to-Image Data Augmentation) to leverage the capabilities of large-scale pre-trained Text-to-Text (T2T) and Text-to-Image (T2I) generative models for data augmentation. By conditioning the T2I model on detailed descriptions produced by T2T models, we are able to generate photo-realistic labeled images in a flexible and controllable manner. Experiments on in-domain classification, cross-domain classification, and image captioning tasks show consis
    
[^56]: 将潜变量对齐：使用潜扩散模型进行高分辨率视频合成

    Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. (arXiv:2304.08818v1 [cs.CV])

    [http://arxiv.org/abs/2304.08818](http://arxiv.org/abs/2304.08818)

    本文提出了一种高分辨率视频合成方法，通过引入时间维度并在图像序列上微调扩散模型，实现了对实际驾驶数据的模拟和创意内容创作的的良好效果。

    

    潜扩散模型（LDM）通过在压缩的低维潜空间中训练扩散模型，实现高质量的图像合成，同时避免了过多的计算需求。本文将LDM应用于高分辨率视频生成，这是一项特别资源密集型的任务。我们首先对单独的图像进行预训练，然后通过在潜空间扩散模型中引入时间维度，并在编码的图像序列（即视频）上进行微调，将生成器从图像生成器转换为视频生成器。同样，我们在时间上对齐扩散模型上采样器，将其转化为时间一致性的视频超分辨率模型。我们关注两个相关的实际应用：野外驾驶数据的模拟和文本到视频建模的创意内容创作。特别地，我们在分辨率为512 x 1024的真实驾驶视频上验证了我们的视频LDM，并取得了最先进的性能。

    Latent Diffusion Models (LDMs) enable high-quality image synthesis while avoiding excessive compute demands by training a diffusion model in a compressed lower-dimensional latent space. Here, we apply the LDM paradigm to high-resolution video generation, a particularly resource-intensive task. We first pre-train an LDM on images only; then, we turn the image generator into a video generator by introducing a temporal dimension to the latent space diffusion model and fine-tuning on encoded image sequences, i.e., videos. Similarly, we temporally align diffusion model upsamplers, turning them into temporally consistent video super resolution models. We focus on two relevant real-world applications: Simulation of in-the-wild driving data and creative content creation with text-to-video modeling. In particular, we validate our Video LDM on real driving videos of resolution 512 x 1024, achieving state-of-the-art performance. Furthermore, our approach can easily leverage off-the-shelf pre-trai
    
[^57]: 通过集成方法实现可传递音频对抗攻击

    Towards the Transferable Audio Adversarial Attack via Ensemble Methods. (arXiv:2304.08811v1 [cs.CR])

    [http://arxiv.org/abs/2304.08811](http://arxiv.org/abs/2304.08811)

    本文通过研究深度学习语音识别的对抗样本传递能力，提出了随机梯度集成和动态梯度加权集成这两种新的集成方法，并发现语音和图像在对抗攻击传递上存在显着差异。

    

    近年来，深度学习模型在许多领域取得了重大进展，如自动驾驶、面部识别和语音识别。然而，深度学习模型对对抗攻击的脆弱性引起了社区的严重关注，因为它们并不具备足够的鲁棒性和泛化能力。此外，可传递攻击已成为黑盒攻击的一种突出方法。在这项工作中，我们探讨了影响基于深度学习的语音识别中对抗样本（AEs）传递能力的潜在因素。我们还讨论了不同深度学习系统的脆弱性和决策边界的不规则性质。我们的结果表明，在对抗攻击的传递能力方面，语音和图像之间存在显着差异，图像的数据相关性较低而语音识别则相反。受到基于dropout的集成方法的激励，我们提出了随机梯度集成和动态梯度加权集成，并评估了它们对AEs传递能力的影响

    In recent years, deep learning (DL) models have achieved significant progress in many domains, such as autonomous driving, facial recognition, and speech recognition. However, the vulnerability of deep learning models to adversarial attacks has raised serious concerns in the community because of their insufficient robustness and generalization. Also, transferable attacks have become a prominent method for black-box attacks. In this work, we explore the potential factors that impact adversarial examples (AEs) transferability in DL-based speech recognition. We also discuss the vulnerability of different DL systems and the irregular nature of decision boundaries. Our results show a remarkable difference in the transferability of AEs between speech and images, with the data relevance being low in images but opposite in speech recognition. Motivated by dropout-based ensemble approaches, we propose random gradient ensembles and dynamic gradient-weighted ensembles, and we evaluate the impact 
    
[^58]: 基于黎曼几何与隐式表示先验的贝叶斯机器人抓取方法

    Implicit representation priors meet Riemannian geometry for Bayesian robotic grasping. (arXiv:2304.08805v1 [cs.RO])

    [http://arxiv.org/abs/2304.08805](http://arxiv.org/abs/2304.08805)

    该研究利用隐式表示构建了场景相关先验，从而在不规则环境中利用基于模拟的高效贝叶斯推理算法成功识别抓取姿态。

    

    在高噪声环境下进行机器人抓取面临着复杂的挑战，尤其是在缺乏场景先验知识的情况下。具体来说，由于两个原因，用贝叶斯推理来识别良好的抓取姿态变得困难：i）从无信息的先验生成数据效率低下，ii）后验通常在黎曼流形上定义一个复杂分布。在本研究中，我们探讨了使用隐式表示构建场景相关性先验的方法，从而使得在不规则环境中应用基于模拟的高效贝叶斯推理算法来确定成功的抓取姿态成为可能。模拟和物理基准测试的结果展示了这种方法的高成功率和良好潜力。

    Robotic grasping in highly noisy environments presents complex challenges, especially with limited prior knowledge about the scene. In particular, identifying good grasping poses with Bayesian inference becomes difficult due to two reasons: i) generating data from uninformative priors proves to be inefficient, and ii) the posterior often entails a complex distribution defined on a Riemannian manifold. In this study, we explore the use of implicit representations to construct scene-dependent priors, thereby enabling the application of efficient simulation-based Bayesian inference algorithms for determining successful grasp poses in unstructured environments. Results from both simulation and physical benchmarks showcase the high success rate and promising potential of this approach.
    
[^59]: 基于张量环分解的大规模动态网络表示

    Large-scale Dynamic Network Representation via Tensor Ring Decomposition. (arXiv:2304.08798v1 [cs.LG])

    [http://arxiv.org/abs/2304.08798](http://arxiv.org/abs/2304.08798)

    本文提出了一种基于张量环分解的模型，用于大规模动态网络的有效表示学习。在两个真实的LDN实验中，该方法比现有的模型具有更高的准确性。

    

    在互联网时代，大规模动态网络变得越来越重要。然而，这些网络的动态性质捕捉了网络结构的演化和边权重的随时间变化，因此对数据分析和建模提出了独特的挑战。张量的潜在分解（LFT）模型为LDN的有效表示学习提供了便利。但是，现有的LFT模型几乎都是基于正交多项式分解（CPF）的。因此，本研究提出了一种基于张量环（TR）分解的模型，用于LDN的有效表示学习。具体而言，作者将单个潜在因子依赖性、非负性和乘法更新（SLF-NMU）原则纳入TR分解模型，并分析了TR分解的特殊偏置形式。两个真实LDN的实验研究表明，所提出的方法比现有模型达到了更高的精度。

    Large-scale Dynamic Networks (LDNs) are becoming increasingly important in the Internet age, yet the dynamic nature of these networks captures the evolution of the network structure and how edge weights change over time, posing unique challenges for data analysis and modeling. A Latent Factorization of Tensors (LFT) model facilitates efficient representation learning for a LDN. But the existing LFT models are almost based on Canonical Polyadic Factorization (CPF). Therefore, this work proposes a model based on Tensor Ring (TR) decomposition for efficient representation learning for a LDN. Specifically, we incorporate the principle of single latent factor-dependent, non-negative, and multiplicative update (SLF-NMU) into the TR decomposition model, and analyze the particular bias form of TR decomposition. Experimental studies on two real LDNs demonstrate that the propose method achieves higher accuracy than existing models.
    
[^60]: 库存管理的合作多智能体强化学习

    Cooperative Multi-Agent Reinforcement Learning for Inventory Management. (arXiv:2304.08769v1 [cs.LG])

    [http://arxiv.org/abs/2304.08769](http://arxiv.org/abs/2304.08769)

    本文提出了一种用于库存管理的多智能体合作强化学习系统，包括自定义GPU并行环境和分享奖励机制，通过分散式Actor-Critic方法进行培训。在供应链场景中，与传统的单一智能体RL解决方案相比，多智能体系统具有明显的优势。

    

    基于强化学习（RL）的库存管理（IM）是一个新兴的研究领域，现有方法往往局限于实现简单、线性环境，并做出些微调整以获得RL算法。将这些简单的环境扩展到实际供应链系统中存在一些挑战，例如：降低环境计算量，定义代理程序配置以代表实际店铺和仓库的动态特性，以及指定奖励框架以鼓励整个供应链中的良好行为。在本文中，我们提出了一个具有自定义GPU并行环境的系统，该环境包括一个仓库和多个商店，提出了一种结构用于代理-环境动态，包括增强状态和操作空间，以及分享奖励机制，以优化大型零售商的供应链需求。供应链图中的每个顶点都是一个独立的智能体，通过分散式Actor-Critic方法进行训练，并通过中央评论家通信来实现合作。我们的结果表明，与传统的单一智能体RL解决方案相比，多智能体系统在供应链场景中具有明显的优势。

    With Reinforcement Learning (RL) for inventory management (IM) being a nascent field of research, approaches tend to be limited to simple, linear environments with implementations that are minor modifications of off-the-shelf RL algorithms. Scaling these simplistic environments to a real-world supply chain comes with a few challenges such as: minimizing the computational requirements of the environment, specifying agent configurations that are representative of dynamics at real world stores and warehouses, and specifying a reward framework that encourages desirable behavior across the whole supply chain. In this work, we present a system with a custom GPU-parallelized environment that consists of one warehouse and multiple stores, a novel architecture for agent-environment dynamics incorporating enhanced state and action spaces, and a shared reward specification that seeks to optimize for a large retailer's supply chain needs. Each vertex in the supply chain graph is an independent age
    
[^61]: W-MAE：具有遮蔽自编码器的预训练天气模型，用于多变量天气预测

    W-MAE: Pre-trained weather model with masked autoencoder for multi-variable weather forecasting. (arXiv:2304.08754v1 [cs.LG])

    [http://arxiv.org/abs/2304.08754](http://arxiv.org/abs/2304.08754)

    本文介绍了一种名为 W-MAE 的预训练天气模型，它使用遮蔽自编码器重建气象变量之间的空间相关性，并通过微调预测气象变量的未来状态，从而对天气数据中存在的时空依赖关系进行建模。

    

    天气预测是具有直接社会和经济影响的长期计算挑战。该任务涉及大量的连续数据收集，并在长时间内表现出丰富的时空依赖性，因此非常适合深度学习模型。本文将预训练技术应用于天气预测，并提出了一种用于多变量天气预测的具有遮蔽自编码器预训练的天气模型W-MAE。W-MAE以自监督的方式进行预训练，以重建气象变量之间的空间相关性。在时间尺度上，我们微调预训练的W-MAE以预测气象变量的未来状态，从而对天气数据中存在的时间依赖关系进行建模。我们使用每六小时选择一次样本，仅使用两年的ERA5数据，对W-MAE进行预训练。在相同的训练数据条件下，我们将W-MAE与FourCastNet进行比较。

    Weather forecasting is a long-standing computational challenge with direct societal and economic impacts. This task involves a large amount of continuous data collection and exhibits rich spatiotemporal dependencies over long periods, making it highly suitable for deep learning models. In this paper, we apply pre-training techniques to weather forecasting and propose W-MAE, a Weather model with Masked AutoEncoder pre-training for multi-variable weather forecasting. W-MAE is pre-trained in a self-supervised manner to reconstruct spatial correlations within meteorological variables. On the temporal scale, we fine-tune the pre-trained W-MAE to predict the future states of meteorological variables, thereby modeling the temporal dependencies present in weather data. We pre-train W-MAE using the fifth-generation ECMWF Reanalysis (ERA5) data, with samples selected every six hours and using only two years of data. Under the same training data conditions, we compare W-MAE with FourCastNet, and 
    
[^62]: 带操作约束的机器人控制下的演员-评论家深度强化学习算法基准测试

    Benchmarking Actor-Critic Deep Reinforcement Learning Algorithms for Robotics Control with Action Constraints. (arXiv:2304.08743v1 [cs.LG])

    [http://arxiv.org/abs/2304.08743](http://arxiv.org/abs/2304.08743)

    本研究提出基准测试评估带操作约束的强化学习算法在多种机器人控制环境中的表现，并公开GitHub代码，为未来的研究和开发提供参考。

    

    本研究提出了一个用于评估操作约束的强化学习（RL）算法的基准测试。在操作受限的RL中，学习系统采取的每个操作都必须符合某些约束条件。这些约束条件对于确保实际系统中的操作的可行性和安全性至关重要。我们在多个机器人控制环境中评估了现有算法及其新颖的变体，涵盖多种操作限制类型。我们的评估提供了该领域的第一个深入视角，揭示了一些出人意料的见解，包括基线方法的有效性。我们的实验中使用的基准问题和相关代码可在github.com/omron-sinicx/action-constrained-RL-benchmark上在线获取，以进一步开展研究和开发。

    This study presents a benchmark for evaluating action-constrained reinforcement learning (RL) algorithms. In action-constrained RL, each action taken by the learning system must comply with certain constraints. These constraints are crucial for ensuring the feasibility and safety of actions in real-world systems. We evaluate existing algorithms and their novel variants across multiple robotics control environments, encompassing multiple action constraint types. Our evaluation provides the first in-depth perspective of the field, revealing surprising insights, including the effectiveness of a straightforward baseline approach. The benchmark problems and associated code utilized in our experiments are made available online at github.com/omron-sinicx/action-constrained-RL-benchmark for further research and development.
    
[^63]: 行为检索：通过查询未标记数据集实现少样本模仿学习

    Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets. (arXiv:2304.08742v1 [cs.RO])

    [http://arxiv.org/abs/2304.08742](http://arxiv.org/abs/2304.08742)

    本论文提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为），实现了行为检索，进而实现了少样本学习。

    

    在数据效率方面使机器人学习新的视觉动作技能仍然是一个难题，有许多挑战。解决这个问题的一种流行范式是利用大型未标记的数据集，其中包含许多行为，然后使用少量任务特定的人类监督（即介入或演示）来适应特定任务的策略。但是，如何最好地利用狭窄的任务特定监督并将其与离线数据平衡仍然是一个待解决的问题。我们的关键洞察力在于任务特定数据不仅为代理提供了新的训练数据，还可以为代理的学习提供有关先前数据类型的信息。具体来说，我们提出了一种简单的方法，利用少量下游专家数据从离线未标记数据集中选择性地查询相关行为（包括许多次优行为）。然后代理被联合训练在专家和查询数据上。我们观察到，我们的

    Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our 
    
[^64]: 用低秩张量分解、Radon变换和字典估算联合概率分布

    Estimating Joint Probability Distribution With Low-Rank Tensor Decomposition, Radon Transforms and Dictionaries. (arXiv:2304.08740v1 [stat.ML])

    [http://arxiv.org/abs/2304.08740](http://arxiv.org/abs/2304.08740)

    本文提出了一种用低秩张量分解、Radon变换和字典估算联合概率分布的方法，通过使用1-D边际进行重建获得了更好的样本复杂度，并在实验中表现优于以前的基于字典的方法和高斯混合模型（GMM）。

    

    本文提出了一种估计数据样本中联合概率密度的方法，假设底层分布能够分解为几个混合组分的乘积密度。我们结合了两个关键想法：用于表示1-D密度的字典以及用于估算1-D边际的随机投影，探索了先前的方法。相比基于字典的方法，我们的算法通过使用1-D边际进行重建而获得了更好的样本复杂度。我们在估算合成概率密度方面评估了我们方法的性能，并将其与以前的基于字典的方法和高斯混合模型（GMM）进行了比较。在所有实验设置中，我们的算法表现优于这些其他方法。

    In this paper, we describe a method for estimating the joint probability density from data samples by assuming that the underlying distribution can be decomposed as a mixture of product densities with few mixture components. Prior works have used such a decomposition to estimate the joint density from lower-dimensional marginals, which can be estimated more reliably with the same number of samples. We combine two key ideas: dictionaries to represent 1-D densities, and random projections to estimate the joint distribution from 1-D marginals, explored separately in prior work. Our algorithm benefits from improved sample complexity over the previous dictionary-based approach by using 1-D marginals for reconstruction. We evaluate the performance of our method on estimating synthetic probability densities and compare it with the previous dictionary-based approach and Gaussian Mixture Models (GMMs). Our algorithm outperforms these other approaches in all the experimental settings.
    
[^65]: 人类和机器有相同的眼睛吗？基于图像分类的人机感知差异研究

    Do humans and machines have the same eyes? Human-machine perceptual differences on image classification. (arXiv:2304.08733v1 [cs.CV])

    [http://arxiv.org/abs/2304.08733](http://arxiv.org/abs/2304.08733)

    本文研究通过图像分类探究了人机感知差异，发现即使准确率相似，人类和机器的答案分布也可能不同，并提出了一种后期人机合作来提高任务表现。

    

    训练良好的计算机视觉模型通常通过模仿从训练标签中学到的人类行为来解决视觉任务。近期视觉研究的大部分努力集中在使用标准化基准来测量模型任务性能。然而，了解人与机器之间的感知差异方面的工作还很有限。为了填补这一空白，我们的研究首先量化并分析了两种来源错误的统计分布。然后我们通过难度级别对任务进行排序，探讨人类与机器专业知识的差异。即使人类和机器的整体准确性相似，答案的分布也可能会有所不同。利用人类和机器之间的感知差异，我们通过实证研究表明了一种后期人机合作，其表现比单独的人或机器更好。

    Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone.
    
[^66]: 用于不同类型癌症分类的EfficientNet算法

    EfficientNet Algorithm for Classification of Different Types of Cancer. (arXiv:2304.08715v1 [eess.IV])

    [http://arxiv.org/abs/2304.08715](http://arxiv.org/abs/2304.08715)

    本文用EfficientNet算法分类不同类型的癌症，实验结果表明该算法在公共数据集上表现优异，具有在临床实践中提高癌症诊断准确性和效率的潜力。

    

    准确高效地识别不同类型癌症对早期发现和有效治疗至关重要。本文介绍了使用EfficientNet算法分类脑瘤、乳腺癌乳房X线摄影、胸部癌和皮肤癌的实验结果。我们使用公共数据集并对图像进行预处理以确保一致性和可比性。实验表明EfficientNet算法在每个癌症数据集上都取得了高精度、高召回率和高F1分数，优于文献中其他最先进的算法。我们还讨论了EfficientNet算法的优缺点及其在临床实践中的潜在应用。结果表明，EfficientNet算法非常适用于不同类型癌症的分类，并可用于提高癌症诊断的准确性和效率。

    Accurate and efficient classification of different types of cancer is critical for early detection and effective treatment. In this paper, we present the results of our experiments using the EfficientNet algorithm for classification of brain tumor, breast cancer mammography, chest cancer, and skin cancer. We used publicly available datasets and preprocessed the images to ensure consistency and comparability. Our experiments show that the EfficientNet algorithm achieved high accuracy, precision, recall, and F1 scores on each of the cancer datasets, outperforming other state-of-the-art algorithms in the literature. We also discuss the strengths and weaknesses of the EfficientNet algorithm and its potential applications in clinical practice. Our results suggest that the EfficientNet algorithm is well-suited for classification of different types of cancer and can be used to improve the accuracy and efficiency of cancer diagnosis.
    
[^67]: 不可能刻画分布学习--一个长期问题的简单解决方案

    Impossibility of Characterizing Distribution Learning -- a simple solution to a long-standing problem. (arXiv:2304.08712v1 [cs.LG])

    [http://arxiv.org/abs/2304.08712](http://arxiv.org/abs/2304.08712)

    本文解答了长期存在的问题：没有一种参数可以刻画分布类的PAC可学习性。同时，我们还展示了不存在一种刻画可学习性的性质来满足分布类以及其他学习问题的要求。

    

    本文考虑了长期以来存在的一个问题：寻找一类概率分布的参数，以刻画它的PAC可学习性。我们提出了一个相当令人惊讶的答案——没有这样的参数存在。我们的技术使我们能够展示类似结果的几个概念，以及几个学习任务。我们展示了没有任何维度可以刻画学习分布类的样本复杂度。然后，我们考虑了只刻画可学习性（而不是量化样本复杂度函数）的较弱要求。我们提出了一些自然的要求，以便对这样一个刻画进行更好的理解，并进一步展示了不存在一种刻画性质，以满足这些要求，对于分布类的。此外，我们展示了我们的结果适用于各种其他学习问题。特别是，我们展示了没有任何维度可以刻画（或刻画可学习性）的概念，适用于...

    We consider the long-standing question of finding a parameter of a class of probability distributions that characterizes its PAC learnability. We provide a rather surprising answer - no such parameter exists. Our techniques allow us to show similar results for several general notions of characterizing learnability and for several learning tasks. We show that there is no notion of dimension that characterizes the sample complexity of learning distribution classes. We then consider the weaker requirement of only characterizing learnability (rather than the quantitative sample complexity function). We propose some natural requirements for such a characterization and go on to show that there exists no characterization of learnability that satisfies these requirements for classes of distributions. Furthermore, we show that our results hold for various other learning problems. In particular, we show that there is no notion of dimension characterizing (or characterization of learnability) for
    
[^68]: LTC-SE: 扩展液态时常神经网络在可扩展人工智能和嵌入式系统中的潜力

    LTC-SE: Expanding the Potential of Liquid Time-Constant Neural Networks for Scalable AI and Embedded Systems. (arXiv:2304.08691v1 [cs.LG])

    [http://arxiv.org/abs/2304.08691](http://arxiv.org/abs/2304.08691)

    LTC-SE是一种液态时常神经网络算法，将多种神经元模型统一，其增强版专注于灵活性、兼容性和代码组织，满足嵌入式系统的性能要求，扩展了液态神经网络在可扩展人工智能和嵌入式系统中的适用性。

    

    我们提出了LTC-SE，这是Hasani等人于2021年最初提出的液态时常神经网络算法的改进版本。该算法将漏电积分-火神经元模型与连续时间递归神经网络（CTRNN）、神经常微分方程（NODE）和量身定制的门控循环单元（GRU）统一起来。LTC-SE的增强版专注于增强灵活性、兼容性和代码组织，以满足具有有限计算资源和严格性能要求的嵌入式系统的独特约束。更新后的代码是一个与TensorFlow 2.x兼容的综合类库，为LTCCell、CTRNN、NODE和CTGRU类提供了全面的配置选项。我们通过对比以往的版本，展示了我们优化在用户体验、Keras函数兼容性和代码清晰度方面的优势，这些改进扩展了液态神经网络在可扩展人工智能和嵌入式系统中的适用性。

    We present LTC-SE, an improved version of the Liquid Time-Constant (LTC) neural network algorithm originally proposed by Hasani et al. in 2021. This algorithm unifies the Leaky-Integrate-and-Fire (LIF) spiking neural network model with Continuous-Time Recurrent Neural Networks (CTRNNs), Neural Ordinary Differential Equations (NODEs), and bespoke Gated Recurrent Units (GRUs). The enhancements in LTC-SE focus on augmenting flexibility, compatibility, and code organization, targeting the unique constraints of embedded systems with limited computational resources and strict performance requirements. The updated code serves as a consolidated class library compatible with TensorFlow 2.x, offering comprehensive configuration options for LTCCell, CTRNN, NODE, and CTGRU classes. We evaluate LTC-SE against its predecessors, showcasing the advantages of our optimizations in user experience, Keras function compatibility, and code clarity. These refinements expand the applicability of liquid neural
    
[^69]: 面向领域转换和自适应的推进学习半监督算法

    Semi-supervised Learning of Pushforwards For Domain Translation & Adaptation. (arXiv:2304.08673v1 [cs.LG])

    [http://arxiv.org/abs/2304.08673](http://arxiv.org/abs/2304.08673)

    本论文提出一种新颖的半监督推进映射学习算法，利用归一化流来解决现有方法中存在的应用空间、样本外数据点可应用性、对两个空间的概率模型进行建模等问题，可应用于图像到图像和文本到文本转换以及分类模型的领域自适应。

    

    本论文提出了一种利用归一化流来参数化映射的新颖推进映射学习算法，通过最小化概率距离和应用特定的正则化项来选择所有可能映射，从而解决了现有方法中存在的广泛应用空间、在样本外数据点上具有可应用性、对两个空间的概率模型进行建模等问题。实验结果表明，该方法在准确性和效率方面具有明显的优势，可应用于图像到图像和文本到文本转换以及分类模型的领域自适应。

    Given two probability densities on related data spaces, we seek a map pushing one density to the other while satisfying application-dependent constraints. For maps to have utility in a broad application space (including domain translation, domain adaptation, and generative modeling), the map must be available to apply on out-of-sample data points and should correspond to a probabilistic model over the two spaces. Unfortunately, existing approaches, which are primarily based on optimal transport, do not address these needs. In this paper, we introduce a novel pushforward map learning algorithm that utilizes normalizing flows to parameterize the map. We first re-formulate the classical optimal transport problem to be map-focused and propose a learning algorithm to select from all possible maps under the constraint that the map minimizes a probability distance and application-specific regularizers; thus, our method can be seen as solving a modified optimal transport problem. Once the map 
    
[^70]: 一种端到端的、交互式的基于深度学习的手写英文文本注释系统

    An end-to-end, interactive Deep Learning based Annotation system for cursive and print English handwritten text. (arXiv:2304.08670v1 [cs.CV])

    [http://arxiv.org/abs/2304.08670](http://arxiv.org/abs/2304.08670)

    本文提出了一个端到端、交互式的手写英文文本注释系统，解决了手写文本数据稀缺的问题，并能够有效提高手写文本识别模型的识别准确率。

    

    随着人们越来越倾向于使用计算设备和数字媒介进行任务，将以前手动完成的任务转换为数字化版本的任何方法都会受到欢迎。尽管今天可以在线完成许多文档任务，但仍有许多应用和领域无法避免手写文本，这使手写文档的数字化成为一项非常重要的任务。过去几十年来，离线手写文本识别得到了广泛的研究。最近，大部分的尝试已经转向了基于机器学习和深度学习的方法。为了设计更复杂和更深入的网络，并保证出色的性能，有更多的注释数据是必不可少的。今天用于离线手写文本识别的大部分数据库都是手动或半自动注释的。这些数据库通常规模较小，无法满足当前基于深度学习的方法的要求。在本文中，我们提出了一种端到端的、交互式的注释系统，以解决手写文本数据的稀缺问题。我们的系统包括三个模块：数据管理模块、注释模块和模型训练模块。注释模块用户友好，允许交互式注释草写和印刷英语手写文本。所提出的系统还可以处理大量数据，具有合理的注释时间。实验结果表明，该系统可以有效地提高手写文本识别模型的识别准确率。

    With the surging inclination towards carrying out tasks on computational devices and digital mediums, any method that converts a task that was previously carried out manually, to a digitized version, is always welcome. Irrespective of the various documentation tasks that can be done online today, there are still many applications and domains where handwritten text is inevitable, which makes the digitization of handwritten documents a very essential task. Over the past decades, there has been extensive research on offline handwritten text recognition. In the recent past, most of these attempts have shifted to Machine learning and Deep learning based approaches. In order to design more complex and deeper networks, and ensure stellar performances, it is essential to have larger quantities of annotated data. Most of the databases present for offline handwritten text recognition today, have either been manually annotated or semi automatically annotated with a lot of manual involvement. Thes
    
[^71]: 学习动作剩余值的连续多功能跳跃

    Continuous Versatile Jumping Using Learned Action Residuals. (arXiv:2304.08663v1 [cs.RO])

    [http://arxiv.org/abs/2304.08663](http://arxiv.org/abs/2304.08663)

    本文提出了一个层次化框架，将最优控制和强化学习结合起来，为四足机器人学习连续跳跃动作。通过学习动作剩余值，在模拟和真实环境中实现了多功能、连续的跳跃动作。

    

    跳跃对于腿式机器人通过困难地形至关重要。在本文中，我们提出了一个层次化框架，结合最优控制和强化学习，为四足机器人学习连续跳跃动作。我们的框架的核心是一个姿态控制器，它将手动设计的加速度控制器与学习到的剩余策略结合在一起。由于加速度控制器为高效训练warm start策略，所以经过训练的策略克服了加速度控制器的局限并提高了跳跃稳定性。此外，一个低层全身控制器将姿势控制器的身体姿势命令转换成电机命令。经过在模拟环境下的训练后，我们的框架可以直接部署到真实机器人上，执行多功能的连续跳跃动作，包括高达50cm、向前60cm的全向跳跃和高达90度的跳跃转向。请访问我们的网站以获取更多结果:https://sites.google.com/view/jumping-rl/home

    Jumping is essential for legged robots to traverse through difficult terrains. In this work, we propose a hierarchical framework that combines optimal control and reinforcement learning to learn continuous jumping motions for quadrupedal robots. The core of our framework is a stance controller, which combines a manually designed acceleration controller with a learned residual policy. As the acceleration controller warm starts policy for efficient training, the trained policy overcomes the limitation of the acceleration controller and improves the jumping stability. In addition, a low-level whole-body controller converts the body pose command from the stance controller to motor commands. After training in simulation, our framework can be deployed directly to the real robot, and perform versatile, continuous jumping motions, including omni-directional jumps at up to 50cm high, 60cm forward, and jump-turning at up to 90 degrees. Please visit our website for more results: https://sites.goo
    
[^72]: 多模式传感器融合技术在DED打印SS316L部件中的原位表面孔隙率预测

    In-situ surface porosity prediction in DED (directed energy deposition) printed SS316L parts using multimodal sensor fusion. (arXiv:2304.08658v1 [physics.app-ph])

    [http://arxiv.org/abs/2304.08658](http://arxiv.org/abs/2304.08658)

    本研究利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力，可以实时预测每个沃克塞尔中的气孔存在，是一个重大飞跃。

    

    本研究旨在将声发射（AE）等多模式传感器数据中的时频模式与DED过程中的孔隙率形成进行高空间（0.5mm）和时间（<1ms）的关联。通过采用可解释的AI方法中的LIME（局部可解释性非特定性解释），将AE中的某些高频波形特征归因于DED过程中的两个主要孔隙形成途径：飞溅事件和低热量输入下相邻打印轨迹的不充分熔合。该方法为实时预测每个沃克塞尔（0.5mm）中的气孔存在提供了令人兴奋的可能性，这是与先前努力相比的一个重大飞跃。在打印并随后加工SS316L材料样品时，同步采集了包括力，AE，振动和温度在内的多模式传感器数据。深度卷积神经网络分类器用于识别两种孔隙形成途径的AE特征，然后使用可解释AI方法进一步分析这些特征。结果表明，利用多模式传感器融合技术和AI方法预测DED打印部件中的原位表面孔隙率的潜力。

    This study aims to relate the time-frequency patterns of acoustic emission (AE) and other multi-modal sensor data collected in a hybrid directed energy deposition (DED) process to the pore formations at high spatial (0.5 mm) and time (< 1ms) resolutions. Adapting an explainable AI method in LIME (Local Interpretable Model-Agnostic Explanations), certain high-frequency waveform signatures of AE are to be attributed to two major pathways for pore formation in a DED process, namely, spatter events and insufficient fusion between adjacent printing tracks from low heat input. This approach opens an exciting possibility to predict, in real-time, the presence of a pore in every voxel (0.5 mm in size) as they are printed, a major leap forward compared to prior efforts. Synchronized multimodal sensor data including force, AE, vibration and temperature were gathered while an SS316L material sample was printed and subsequently machined. A deep convolution neural network classifier was used to ide
    
[^73]: 关于概率神经摘要中的不确定性校准和选择性生成的基准研究

    On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study. (arXiv:2304.08653v1 [cs.CL])

    [http://arxiv.org/abs/2304.08653](http://arxiv.org/abs/2304.08653)

    本研究对不同最先进的概率学习方法在提高神经摘要模型不确定性质量和生成效果方面进行了调查和对比，结果表明概率方法能够持续提高生成和不确定性质量，实现了高质量生成和放弃低质量摘要，且揭示了显著的失效模式。

    

    现代深度摘要模型在基准性能方面取得了令人印象深刻的成果，但它们往往会生成错误校准的预测不确定性。这意味着它们对质量较低的预测赋予了高信心度，从而在实际应用中导致可靠性和信任度的降低。概率深度学习方法是解决误校准问题的常见方法。然而，它们在复杂自回归摘要任务中的相对有效性尚不清楚。在本工作中，我们彻底调查了不同最先进的概率方法在提高神经摘要模型不确定性质量方面的有效性，跨越了三个难度不同的大规模基准。我们发现，概率方法始终能够提高模型的生成和不确定性质量，从而在实践中实现了高质量生成（即放弃低质量摘要）。我们还揭示了显著的失效模式。

    Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty. We show that the probabilistic methods consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns 
    
[^74]: 基于BERT的技术对美国最高法院案例进行分类

    Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])

    [http://arxiv.org/abs/2304.08649](http://arxiv.org/abs/2304.08649)

    本文基于BERT技术探究了对美国最高法院案例进行分类的方法，比较了使用BERT模型与其他先进模型的准确性，最终在15个广泛类别上取得了80%的准确度，在279个细粒度类别上取得了60%的准确度。

    

    基于双向编码器表示来自变压器的模型（BERT）在许多自然语言处理（NLP）任务（如命名实体识别（NER），词性（POS）标记等）上产生了最新技术（SOTA）结果。当分类长文档（例如来自美国最高法院的文档）时，使用BERT模型可能比较困难。本文中，我们尝试了几种基于BERT的分类技术，用于对美国最高法院决定或最高法院数据库（SCDB）进行分类，并将其与先前的SOTA结果进行了比较。我们还将我们的结果与针对长文档的SOTA模型进行了比较。我们对两个分类任务进行了比较：（1）广泛的分类任务，具有15个类别；（2）细粒度的分类任务，具有279个类别。我们的最佳结果在15个广泛类别上产生80％的准确度，在279个细粒度类别上产生60％的准确度。

    Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
    
[^75]: TAP：道路交通事故综合数据库的构建与研究（arXiv:2304.08640v1 [cs.LG]）

    TAP: A Comprehensive Data Repository for Traffic Accident Prediction in Road Networks. (arXiv:2304.08640v1 [cs.LG])

    [http://arxiv.org/abs/2304.08640](http://arxiv.org/abs/2304.08640)

    本研究通过构建基于真实世界的TAP数据库,提供了两个任务:事故发生预测和事故严重程度预测,以用于各种交通相关研究，比较重要的创新是引入了图结构信息，使得机器学习方法不再只着眼于单个位置的交通事故预测。

    

    道路安全是全球公共卫生关注的重点。准确的交通事故预测在减少交通事故方面具有关键作用。然而，现有的机器学习方法往往只着眼于单个位置的交通事故预测，而没有考虑到路网内不同事故地点之间的潜在关系，这就需要引入图结构信息，例如图神经网络（GNNs）可以自然地应用。然而，将GNNs应用于事故预测问题面临着如何获取合适的图结构交通事故数据集的挑战，为了弥合这一差距，我们构建了一个基于真实世界的基于图形的交通事故预测（TAP）数据库，并提供了两个典型任务：事故发生预测和事故严重程度预测。这个数据库覆盖全国，具有真实世界的网络拓扑结构和丰富的地理空间特征，可以用于各种交通相关研究。

    Road safety is a major global public health concern. Effective traffic crash prediction can play a critical role in reducing road traffic accidents. However, Existing machine learning approaches tend to focus on predicting traffic accidents in isolation, without considering the potential relationships between different accident locations within road networks. To incorporate graph structure information, graph-based approaches such as Graph Neural Networks (GNNs) can be naturally applied. However, applying GNNs to the accident prediction problem faces challenges due to the lack of suitable graph-structured traffic accident datasets. To bridge this gap, we have constructed a real-world graph-based Traffic Accident Prediction (TAP) data repository, along with two representative tasks: accident occurrence prediction and accident severity prediction. With nationwide coverage, real-world network topology, and rich geospatial features, this data repository can be used for a variety of traffic-
    
[^76]: pgmpy: 一个用于贝叶斯网络的 Python 工具包

    pgmpy: A Python Toolkit for Bayesian Networks. (arXiv:2304.08639v1 [cs.LG])

    [http://arxiv.org/abs/2304.08639](http://arxiv.org/abs/2304.08639)

    pgmpy是一个Python工具包，它提供了用于处理贝叶斯网络和相关模型的算法和工具，并侧重于易于扩展性，使得用户能够快速地修改、添加或实现新算法。

    

    贝叶斯网络（BN）在多个领域用于建模、预测和决策制定。pgmpy是一个Python包，提供了一系列算法和工具来处理BNs和相关模型。它实现了结构学习、参数估计、近似和精确推理、因果推理和模拟的算法。这些实现侧重于模块化和易于扩展，允许用户快速修改/添加现有算法，或为不同用例实现新算法。pgmpy在MIT许可证下发布；源代码可在https://github.com/pgmpy/pgmpy找到，文档可在https://pgmpy.org找到。

    Bayesian Networks (BNs) are used in various fields for modeling, prediction, and decision making. pgmpy is a python package that provides a collection of algorithms and tools to work with BNs and related models. It implements algorithms for structure learning, parameter estimation, approximate and exact inference, causal inference, and simulations. These implementations focus on modularity and easy extensibility to allow users to quickly modify/add to existing algorithms, or to implement new algorithms for different use cases. pgmpy is released under the MIT License; the source code is available at: https://github.com/pgmpy/pgmpy, and the documentation at: https://pgmpy.org.
    
[^77]: 大型语言模型输出的评估：话语和记忆

    An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])

    [http://arxiv.org/abs/2304.08637](http://arxiv.org/abs/2304.08637)

    评估了九个大语言模型的输出，发现其中80％包含记忆数据，但包含最多记忆内容的输出更可能是高质量的。提出了缓解策略以降低记忆文本率。

    

    我们对九个最广泛可用的大型语言模型（LLMs）生成的各种输出进行了经验性评估。我们使用现成的工具进行分析，发现在与输出病态（例如，反事实和逻辑上的错误陈述）以及不保持主题等方面的关系中，记忆文本百分比、独特文本百分比和整体输出质量之间存在相关性。总体而言，80.0％的输出包含记忆数据，但包含最多记忆内容的输出也更有可能被认为具有高质量。我们讨论和评估了缓解策略，并显示，在评估的模型中，输出的记忆文本率有所降低。最后，我们就学习、记忆和评估优质文本的潜在影响进行了讨论。

    We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
    
[^78]: 信号处理大挑战2023——电子预防：睡眠行为作为精神病患者复发指标的研究

    Signal Processing Grand Challenge 2023 -- e-Prevention: Sleep Behavior as an Indicator of Relapses in Psychotic Patients. (arXiv:2304.08614v1 [eess.SP])

    [http://arxiv.org/abs/2304.08614](http://arxiv.org/abs/2304.08614)

    研究探讨了利用睡眠行为特征在无监督机器学习设置下估计精神病患者复发日的方法，并发现短时睡眠行为特征性能更好。

    

    本文介绍了USC SAIL提出的在信号处理大挑战2023——电子预防（任务2）中，检测精神病患者复发的方法和结果。复发预测一直是具有挑战性的，主要由于不同个体症状和对治疗的反应的异质性。我们通过研究利用睡眠行为特征估计异常值的无监督机器学习设置来解决这些挑战。我们从野外采集的人类活动和心率数据中提取有用的特征，并评估了各种特征类型和时间分辨率的组合。我们发现，短时睡眠行为特征表现出比其清醒的同行和更大的时间间隔更好的性能。我们的提交在任务的官方排行榜上排名第三，展示了这些特征作为精神病复发的客观且非侵入性的预测指标的潜力。

    This paper presents the approach and results of USC SAIL's submission to the Signal Processing Grand Challenge 2023 - e-Prevention (Task 2), on detecting relapses in psychotic patients. Relapse prediction has proven to be challenging, primarily due to the heterogeneity of symptoms and responses to treatment between individuals. We address these challenges by investigating the use of sleep behavior features to estimate relapse days as outliers in an unsupervised machine learning setting. We extract informative features from human activity and heart rate data collected in the wild, and evaluate various combinations of feature types and time resolutions. We found that short-time sleep behavior features outperformed their awake counterparts and larger time intervals. Our submission was ranked 3rd in the Task's official leaderboard, demonstrating the potential of such features as an objective and non-invasive predictor of psychotic relapses.
    
[^79]: 离散与反向传播的桥梁：直通法与其它方法

    Bridging Discrete and Backpropagation: Straight-Through and Beyond. (arXiv:2304.08612v1 [cs.LG])

    [http://arxiv.org/abs/2304.08612](http://arxiv.org/abs/2304.08612)

    本文提出了一种新方法来逼近生成离散潜变量的参数的梯度，其中包括了一些数值方法，实现了二阶精度，取得了实验上的持续改进。

    

    反向传播是深度学习中的基石，但其仅限于计算连续变量的梯度，限制了涉及离散潜变量的问题的研究。针对这个问题，我们提出了一种新的方法来近似生成离散潜变量的参数的梯度。我们首先考察了广泛使用的 Straight-Through（ST）启发式方法，并证明它作为梯度的一阶近似值。在此基础上，我们提出了一种新的方法，称为 ReinMax，它集成了 Heun's Method，一种解ODE的二阶数值方法，以近似梯度。我们的方法实现了二阶精度，而不需要 Hessian 或其他二阶导数。我们进行了结构化输出预测和无监督生成建模任务的实验。我们的结果显示，\ours 在现有技术中带来了持续的改进，包括 ST 和 Straight-Through Gum。

    Backpropagation, the cornerstone of deep learning, is limited to computing gradients solely for continuous variables. This limitation hinders various research on problems involving discrete latent variables. To address this issue, we propose a novel approach for approximating the gradient of parameters involved in generating discrete latent variables. First, we examine the widely used Straight-Through (ST) heuristic and demonstrate that it works as a first-order approximation of the gradient. Guided by our findings, we propose a novel method called ReinMax, which integrates Heun's Method, a second-order numerical method for solving ODEs, to approximate the gradient. Our method achieves second-order accuracy without requiring Hessian or other second-order derivatives. We conduct experiments on structured output prediction and unsupervised generative modeling tasks. Our results show that \ours brings consistent improvements over the state of the art, including ST and Straight-Through Gum
    
[^80]: 联邦学习与智能电网的交叉路径：概述，挑战和前景。

    Crossing Roads of Federated Learning and Smart Grids: Overview, Challenges, and Perspectives. (arXiv:2304.08602v1 [cs.LG])

    [http://arxiv.org/abs/2304.08602](http://arxiv.org/abs/2304.08602)

    本文探讨了在智能电网中应用联邦学习以保护消费者隐私和提高数据模型预测性能的优势和缺点，并提供了可能的数据分区、通信拓扑和安全机制分类方法。同时，本文总结了该技术面临的主要挑战和未来发展方向。

    

    能源数据的敏感性使得消费者的隐私成为智能电网（SGs）中的主要关注点，尤其是在训练机器学习模型以用于不同服务时。联邦学习可以将训练推向边缘，为隐私保护和模型预测性能之间提供了很好的折衷方案。本文概述了联邦学习在SGs中的应用，并讨论了其优缺点，主要包括负载预测，电动汽车，故障诊断，负载分解和可再生能源等。此外，还探讨了数据分区、通信拓扑和安全机制等方面的主要设计趋势和可能的分类方法。最后，本文总结了该技术面临的主要挑战和未来发展方向。

    Consumer's privacy is a main concern in Smart Grids (SGs) due to the sensitivity of energy data, particularly when used to train machine learning models for different services. These data-driven models often require huge amounts of data to achieve acceptable performance leading in most cases to risks of privacy leakage. By pushing the training to the edge, Federated Learning (FL) offers a good compromise between privacy preservation and the predictive performance of these models. The current paper presents an overview of FL applications in SGs while discussing their advantages and drawbacks, mainly in load forecasting, electric vehicles, fault diagnoses, load disaggregation and renewable energies. In addition, an analysis of main design trends and possible taxonomies is provided considering data partitioning, the communication topology, and security mechanisms. Towards the end, an overview of main challenges facing this technology and potential future directions is presented.
    
[^81]: eTOP：用于更快地训练AutoML系统的管道提前终止

    eTOP: Early Termination of Pipelines for Faster Training of AutoML Systems. (arXiv:2304.08597v1 [cs.LG])

    [http://arxiv.org/abs/2304.08597](http://arxiv.org/abs/2304.08597)

    eTOP框架可以在任何AutoML系统之上工作，并决定是否将执行管道到最后或在中间步骤终止以更快地训练模型。

    

    软件和硬件技术的最新进展使得AI/ML模型可以应用到日常应用中，极大地提高了所提供服务的质量。但是对于给定的应用程序，找到合适的AI/ML模型是一个复杂而昂贵的过程，涉及多个相互关联的步骤（称为管道），如数据预处理、特征工程、选择和模型调整的生成、训练和评估等。这些管道在结构上是复杂的，在计算资源和时间上都很昂贵，并与每个步骤相关联。AutoML系统自动搜索这些超参数，但速度很慢，因为它们依赖于管道的最终输出进行优化。我们提出了eTOP框架，它可以在任何AutoML系统之上工作，并决定是否将管道执行到最后或在中间步骤终止。在26个基准数据集上的实验评估以及eTOP与

    Recent advancements in software and hardware technologies have enabled the use of AI/ML models in everyday applications has significantly improved the quality of service rendered. However, for a given application, finding the right AI/ML model is a complex and costly process, that involves the generation, training, and evaluation of multiple interlinked steps (called pipelines), such as data pre-processing, feature engineering, selection, and model tuning. These pipelines are complex (in structure) and costly (both in compute resource and time) to execute end-to-end, with a hyper-parameter associated with each step. AutoML systems automate the search of these hyper-parameters but are slow, as they rely on optimizing the pipeline's end output. We propose the eTOP Framework which works on top of any AutoML system and decides whether or not to execute the pipeline to the end or terminate at an intermediate step. Experimental evaluation on 26 benchmark datasets and integration of eTOPwith 
    
[^82]: 利用稀疏但信息丰富的变量进行预测：以预测血糖为例的案例研究

    Forecasting with Sparse but Informative Variables: A Case Study in Predicting Blood Glucose. (arXiv:2304.08593v1 [cs.LG])

    [http://arxiv.org/abs/2304.08593](http://arxiv.org/abs/2304.08593)

    该论文提出了一种利用稀疏但信息丰富的变量进行预测的方法，通过编码器/解码器预测方法准确地学习辅助信号的作用，为预测血糖等时间序列预测提供了新思路。

    

    在时间序列预测中，未来的目标值可能会受到内在和外在影响。例如，当预测血糖时，内在作用可以仅通过目标信号的历史记录（即血糖）来推断，但精确建模外在影响的影响需要辅助信号，例如摄入的碳水化合物量。为了更好地利用这些稀疏但信息丰富的变量（SIVs），我们引入了一种新的编码器/解码器预测方法，通过（i）将SIVs效果的学习精确地学习每个时间点。

    In time-series forecasting, future target values may be affected by both intrinsic and extrinsic effects. When forecasting blood glucose, for example, intrinsic effects can be inferred from the history of the target signal alone (\textit{i.e.} blood glucose), but accurately modeling the impact of extrinsic effects requires auxiliary signals, like the amount of carbohydrates ingested. Standard forecasting techniques often assume that extrinsic and intrinsic effects vary at similar rates. However, when auxiliary signals are generated at a much lower frequency than the target variable (e.g., blood glucose measurements are made every 5 minutes, while meals occur once every few hours), even well-known extrinsic effects (e.g., carbohydrates increase blood glucose) may prove difficult to learn. To better utilize these \textit{sparse but informative variables} (SIVs), we introduce a novel encoder/decoder forecasting approach that accurately learns the per-timepoint effect of the SIV, by (i) is
    
[^83]: 快速并容错的分布式SGD算法，降低计算负载。

    Fast and Straggler-Tolerant Distributed SGD with Reduced Computation Load. (arXiv:2304.08589v1 [cs.DC])

    [http://arxiv.org/abs/2304.08589](http://arxiv.org/abs/2304.08589)

    本文提出了一种基于模型的分布式SGD算法方案，通过适应算法的运行时间内的工作节点数量和计算负载，优化收敛速度同时降低计算负载。

    

    在分布式机器学习中，一个中心节点将计算密集型的运算外包给外部的工作节点。优化过程的属性，如随机梯度下降（SGD），可以利用以减轻不响应或速度慢的工人（称为迟钝者）的影响，因为这些情况会降低计算外包的收益。这可以通过仅等待每个算法迭代中的一部分工作节点完成其计算来实现。之前的工作提出了适应等待工人数量随算法演化以优化收敛速度的方法。相反，本文构建了一个新的方案，通过使用独立的随机变量对通信和计算时间进行建模，来适应算法的运行时间内的工作节点数量和计算负载。因此，我们提高了分布式SGD的收敛速度，同时显着降低了计算负载。

    In distributed machine learning, a central node outsources computationally expensive calculations to external worker nodes. The properties of optimization procedures like stochastic gradient descent (SGD) can be leveraged to mitigate the effect of unresponsive or slow workers called stragglers, that otherwise degrade the benefit of outsourcing the computation. This can be done by only waiting for a subset of the workers to finish their computation at each iteration of the algorithm. Previous works proposed to adapt the number of workers to wait for as the algorithm evolves to optimize the speed of convergence. In contrast, we model the communication and computation times using independent random variables. Considering this model, we construct a novel scheme that adapts both the number of workers and the computation load throughout the run-time of the algorithm. Consequently, we improve the convergence speed of distributed SGD while significantly reducing the computation load, at the ex
    
[^84]: CAM2: 面向大规模推荐系统的一致性感知多任务排名模型

    CAM2: Conformity-Aware Multi-Task Ranking Model for Large-Scale Recommender Systems. (arXiv:2304.08562v1 [cs.IR])

    [http://arxiv.org/abs/2304.08562](http://arxiv.org/abs/2304.08562)

    CAM2是一个面向大规模推荐系统的一致性感知多任务排名模型，通过利用因果建模系统地解开用户对流行物品的一致性与他们真正兴趣的联系，来消除历史用户交互数据带来的一致性偏见，并在实践中得到有效的应用。

    

    将历史用户交互数据拟合到大规模工业推荐系统模型中，可能会导致一致性偏见，因为用户兴趣可能很难确定，而许多项目通常基于生态系统因素而不是与个体用户相关性交互。在本研究中，我们引入了CAM2，这是一个一致性感知的多任务排名模型，旨在为其中一个最大的工业推荐平台向用户提供相关物品。CAM2通过利用因果建模系统地解开用户对流行物品的一致性与他们真正兴趣的联系。这个框架是可推广的，并且可以扩展以支持在任何大规模推荐系统中的多个一致性和用户相关性的表示。我们提供更深入的实践见解，并通过离线评估的改进来演示所提出模型的有效性。

    Learning large-scale industrial recommender system models by fitting them to historical user interaction data makes them vulnerable to conformity bias. This may be due to a number of factors, including the fact that user interests may be difficult to determine and that many items are often interacted with based on ecosystem factors other than their relevance to the individual user. In this work, we introduce CAM2, a conformity-aware multi-task ranking model to serve relevant items to users on one of the largest industrial recommendation platforms. CAM2 addresses these challenges systematically by leveraging causal modeling to disentangle users' conformity to popular items from their true interests. This framework is generalizable and can be scaled to support multiple representations of conformity and user relevance in any large-scale recommender system. We provide deeper practical insights and demonstrate the effectiveness of the proposed model through improvements in offline evaluatio
    
[^85]: 随机子图邻域汇聚用于子图分类

    Stochastic Subgraph Neighborhood Pooling for Subgraph Classification. (arXiv:2304.08556v1 [cs.LG])

    [http://arxiv.org/abs/2304.08556](http://arxiv.org/abs/2304.08556)

    该论文提出了一种随机子图邻域汇聚方法，可以在保持精度的情况下解决子图分类问题的可扩展性问题，并实现了最先进的性能。

    

    子图分类是图表征学习中的新兴领域，其任务是对图中的一组节点（即子图）进行分类。子图分类具有预测一组蛋白质的细胞功能或在给定表型集合的情况下识别罕见疾病等应用。图神经网络是节点、链和图级任务的事实解决方案，但在子图分类任务上表现不佳。即使是为图分类量身定制的图神经网络也不能直接转化为子图分类，因为它们忽略了子图的外部拓扑结构，从而无法捕捉子图在大图中的位置。目前用于子图分类的最先进模型通过标签技巧或多个消息传递通道来解决这个缺陷，但这两种方法都会对计算机造成负担，不适用于大型图。为了解决可扩展性问题并保持精度，我们提出了一种随机子图邻域汇聚 (SSNP)方法，该方法通过随机汇聚子图邻域来利用图的结构特性。SSNP在几个子图分类基准数据集上实现了最先进的性能，同时具有计算效率。

    Subgraph classification is an emerging field in graph representation learning where the task is to classify a group of nodes (i.e., a subgraph) within a graph. Subgraph classification has applications such as predicting the cellular function of a group of proteins or identifying rare diseases given a collection of phenotypes. Graph neural networks (GNNs) are the de facto solution for node, link, and graph-level tasks but fail to perform well on subgraph classification tasks. Even GNNs tailored for graph classification are not directly transferable to subgraph classification as they ignore the external topology of the subgraph, thus failing to capture how the subgraph is located within the larger graph. The current state-of-the-art models for subgraph classification address this shortcoming through either labeling tricks or multiple message-passing channels, both of which impose a computation burden and are not scalable to large graphs. To address the scalability issue while maintaining
    
[^86]: 一种可扩展的序列转移优化问题生成器

    A Scalable Test Problem Generator for Sequential Transfer Optimization. (arXiv:2304.08503v1 [cs.NE])

    [http://arxiv.org/abs/2304.08503](http://arxiv.org/abs/2304.08503)

    STO中已有的测试问题设计不完善，难以代表真实问题多样化关系，限制了算法的表现。本文介绍了一种可扩展的序列转移优化问题生成器。

    

    近年来，序列转移优化(STO)受到越来越多的研究关注，旨在利用储存在数据库中以前求解的优化任务的知识来提高优化性能。然而，尽管算法设计已有重大进展，但STO中的测试问题设计并不完善。它们往往是由其他基准函数随机组合而成，这些基准函数具有相同的最佳值，或者生成自表现出有限变化的实际问题。这些问题中源任务和目标任务的最优解之间的关系是手动配置的，因此单调，限制了它们表征真实问题多样化关系的能力。因此，许多算法在这些问题上取得的有前途的结果具有高度的偏见，并且难以推广到其他问题。鉴于此，我们首先引入了一些表征STO问题的基本概念。

    Sequential transfer optimization (STO), which aims to improve optimization performance by exploiting knowledge captured from previously-solved optimization tasks stored in a database, has been gaining increasing research attention in recent years. However, despite significant advancements in algorithm design, the test problems in STO are not well designed. Oftentimes, they are either randomly assembled by other benchmark functions that have identical optima or are generated from practical problems that exhibit limited variations. The relationships between the optimal solutions of source and target tasks in these problems are manually configured and thus monotonous, limiting their ability to represent the diverse relationships of real-world problems. Consequently, the promising results achieved by many algorithms on these problems are highly biased and difficult to be generalized to other problems. In light of this, we first introduce a few rudimentary concepts for characterizing STO pr
    
[^87]: 递归神经网络与传统机器学习方法在激光诱导击穿光谱技术中的比较

    A comparison between Recurrent Neural Networks and classical machine learning approaches In Laser induced breakdown spectroscopy. (arXiv:2304.08500v1 [cs.LG])

    [http://arxiv.org/abs/2304.08500](http://arxiv.org/abs/2304.08500)

    本文比较了递归神经网络和传统机器学习方法在激光诱导击穿光谱技术中的应用，结果表明，基于递归神经网络的模型，特别是LSTM和GRU，提供比传统机器学习模型更好的预测结果。

    

    递归神经网络是一类人工神经网络，在时间动力学分析中建立不同节点之间的连接形成有向或无向图。本研究利用激光诱导击穿光谱技术对铝合金进行定量分析，并采用不同的递归神经网络结构，如长短时记忆网络(LSTM)、门控循环单元(GRU)、简单递归神经网络(Simple RNN)以及由卷积-简单递归网络(Conv-SimpleRNN)、卷积-LSTM(Conv-LSTM)和卷积-GRU(Conv-GRU)组成的递归卷积网络用于铝标准样品浓度预测。然后，将递归神经网络与支持向量回归机(SVR)、随机森林(RF)、k最近邻(k-NN)和多元线性回归(MLR)等传统机器学习方法进行比较。结果表明，基于递归神经网络的模型，特别是LSTM和GRU，提供比传统机器学习模型更好的预测结果。

    Recurrent Neural Networks are classes of Artificial Neural Networks that establish connections between different nodes form a directed or undirected graph for temporal dynamical analysis. In this research, the laser induced breakdown spectroscopy (LIBS) technique is used for quantitative analysis of aluminum alloys by different Recurrent Neural Network (RNN) architecture. The fundamental harmonic (1064 nm) of a nanosecond Nd:YAG laser pulse is employed to generate the LIBS plasma for the prediction of constituent concentrations of the aluminum standard samples. Here, Recurrent Neural Networks based on different networks, such as Long Short Term Memory (LSTM), Gated Recurrent Unit (GRU), Simple Recurrent Neural Network (Simple RNN), and as well as Recurrent Convolutional Networks comprising of Conv-SimpleRNN, Conv-LSTM and Conv-GRU are utilized for concentration prediction. Then a comparison is performed among prediction by classical machine learning methods of support vector regressor 
    
[^88]: 排名损失和交错学习减少组织病理学图像搜索偏见

    Ranking Loss and Sequestering Learning for Reducing Image Search Bias in Histopathology. (arXiv:2304.08498v1 [eess.IV])

    [http://arxiv.org/abs/2304.08498](http://arxiv.org/abs/2304.08498)

    本文提出两个新颖的想法，分别采用排名损失函数和交错学习的方法，避免了分类误差和模型内部偏见，以提高图像搜索性能。

    

    近年来，深度学习在医疗保健应用中有了重要作用，包括数字病理学中的图像搜索。尽管计算机视觉取得了重大进展，但在病理学档案的图像搜索方面仍存在重大问题。一个众所周知的问题是AI偏见和缺乏泛化能力。一个更特别的深度模型缺点是对搜索功能的无知。前者影响每个模型，后者只影响搜索和匹配。由于缺乏基于排名的学习，研究人员必须基于分类误差来训练模型，然后使用所得的嵌入进行图像搜索。此外，即使使用各种医院的大型图像库，深度模型似乎也容易产生内部偏见。本文提出了两个新颖的想法以提高图像搜索性能。首先，我们使用排名损失函数来指导特征提取朝向搜索的匹配导向性。通过强制模型学习排名，我们可以避免分类误差带来的问题。其次，我们提出一种称为交错学习的方法，在扩大训练数据集的同时减少模型内部偏差。

    Recently, deep learning has started to play an essential role in healthcare applications, including image search in digital pathology. Despite the recent progress in computer vision, significant issues remain for image searching in histopathology archives. A well-known problem is AI bias and lack of generalization. A more particular shortcoming of deep models is the ignorance toward search functionality. The former affects every model, the latter only search and matching. Due to the lack of ranking-based learning, researchers must train models based on the classification error and then use the resultant embedding for image search purposes. Moreover, deep models appear to be prone to internal bias even if using a large image repository of various hospitals. This paper proposes two novel ideas to improve image search performance. First, we use a ranking loss function to guide feature extraction toward the matching-oriented nature of the search. By forcing the model to learn the ranking o
    
[^89]: 基于模型驱动的量子联邦学习

    Model-Driven Quantum Federated Learning (QFL). (arXiv:2304.08496v1 [cs.SE])

    [http://arxiv.org/abs/2304.08496](http://arxiv.org/abs/2304.08496)

    提出了一种基于模型驱动的方法，该方法可以为从业人员提供抽象层，以便高效地进行软件开发和数据科学任务，并支持量子联邦学习（QFL）。

    

    最近，有多项研究提出了量子联邦学习（QFL）的框架。例如，谷歌TensorFlow Quantum（TFQ）和TensorFlow Federated（TFF）库已经被用于实现QFL。然而，开发者大多不熟悉量子计算（QC）库和框架。提供一个为QC和联邦学习（FL）库提供抽象层的领域特定建模语言（DSML）是有益的。这可以使从业人员在部署量子机器学习（QML）最新技术的同时高效地进行软件开发和数据科学任务。在这篇立场论文中，我们提出扩展现有的面向机器学习（ML）启用系统的领域特定模型驱动工程（MDE）工具（如MontiAnna、ML-Quadrat和GreyCat）以支持QFL。

    Recently, several studies have proposed frameworks for Quantum Federated Learning (QFL). For instance, the Google TensorFlow Quantum (TFQ) and TensorFlow Federated (TFF) libraries have been deployed for realizing QFL. However, developers, in the main, are not as yet familiar with Quantum Computing (QC) libraries and frameworks. A Domain-Specific Modeling Language (DSML) that provides an abstraction layer over the underlying QC and Federated Learning (FL) libraries would be beneficial. This could enable practitioners to carry out software development and data science tasks efficiently while deploying the state of the art in Quantum Machine Learning (QML). In this position paper, we propose extending existing domain-specific Model-Driven Engineering (MDE) tools for Machine Learning (ML) enabled systems, such as MontiAnna, ML-Quadrat, and GreyCat, to support QFL.
    
[^90]: 无人机群在自主移动接入应用中的协同多智能体强化学习

    Coordinated Multi-Agent Reinforcement Learning for Unmanned Aerial Vehicle Swarms in Autonomous Mobile Access Applications. (arXiv:2304.08493v1 [cs.MA])

    [http://arxiv.org/abs/2304.08493](http://arxiv.org/abs/2304.08493)

    本论文提出了一种集中式训练和分布式执行的多智能体深度强化学习方法，用于协调控制多个无人机在自主移动接入应用中，最大化服务质量。

    

    本文提出了一种新颖的基于集中式训练和分布式执行 (CTDE) 的多智能体深度强化学习 (MADRL) 方法，用于控制多个无人机在自主移动接入应用中。为此，单个神经网络在集中式训练中用于协作多个智能体，同时最大化移动接入应用中的总服务质量 (QoS)。

    This paper proposes a novel centralized training and distributed execution (CTDE)-based multi-agent deep reinforcement learning (MADRL) method for multiple unmanned aerial vehicles (UAVs) control in autonomous mobile access applications. For the purpose, a single neural network is utilized in centralized training for cooperation among multiple agents while maximizing the total quality of service (QoS) in mobile access applications.
    
[^91]: 来自内部的邪恶: 通过硬件木马进行机器学习后门攻击

    Evil from Within: Machine Learning Backdoors through Hardware Trojans. (arXiv:2304.08411v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2304.08411](http://arxiv.org/abs/2304.08411)

    本文介绍了一种在常见机器学习硬件加速器内的后门攻击方法，将最小后门概念和可配置的硬件木马结合使用，从而对目前的防御措施构成挑战。

    

    后门会对机器学习造成严重威胁，因为它们可能破坏安全关键的系统，如自动驾驶汽车。本文介绍了一种后门攻击方法，完全居于用于机器学习的常见硬件加速器内，从而对当前防御措施构成挑战。为了使这种攻击实用，我们克服了两个挑战：首先，由于硬件加速器上的存储空间严重受限，因此我们引入了所谓的最小后门概念，只改变少量模型参数即可激活后门。其次，我们开发了一种可配置的硬件木马，可以与后门一起使用。

    Backdoors pose a serious threat to machine learning, as they can compromise the integrity of security-critical systems, such as self-driving cars. While different defenses have been proposed to address this threat, they all rely on the assumption that the hardware on which the learning models are executed during inference is trusted. In this paper, we challenge this assumption and introduce a backdoor attack that completely resides within a common hardware accelerator for machine learning. Outside of the accelerator, neither the learning model nor the software is manipulated, so that current defenses fail. To make this attack practical, we overcome two challenges: First, as memory on a hardware accelerator is severely limited, we introduce the concept of a minimal backdoor that deviates as little as possible from the original model and is activated by replacing a few model parameters only. Second, we develop a configurable hardware trojan that can be provisioned with the backdoor and p
    
[^92]: 基于对比学习的多模态短视频谣言检测系统

    Multimodal Short Video Rumor Detection System Based on Contrastive Learning. (arXiv:2304.08401v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.08401](http://arxiv.org/abs/2304.08401)

    本研究基于对比学习设计出一个多模态短视频谣言检测系统，通过构建具有多种特征的短视频数据集和使用多模态特征融合与外部知识，能有效地区分短视频谣言。

    

    随着短视频平台成为新闻分享的重要渠道之一，中国主要短视频平台逐渐成为虚假新闻的新滋生地。由于短视频包含了大量信息和特征，以及视频之间的严重同质化和相似性，因此很难区分短视频谣言。为了减轻短视频谣言的传播，我们的团队考虑到每种算法的优缺点，构建了多模态特征融合并引入外部知识进行短视频谣言检测。检测的主要思路是：（1）创建数据集：构建具有多种特征的短视频数据集；（2）多模态谣言检测模型：首先使用TSN视频编码模型提取视频特征；然后使用OCR和ASR提取视频的文本特征；最后，将这些特征进行融合来进行短视频谣言检测。

    With short video platforms becoming one of the important channels for news sharing, major short video platforms in China have gradually become new breeding grounds for fake news. However, it is not easy to distinguish short video rumors due to the great amount of information and features contained in short videos, as well as the serious homogenization and similarity of features among videos. In order to mitigate the spread of short video rumors, our group decides to detect short video rumors by constructing multimodal feature fusion and introducing external knowledge after considering the advantages and disadvantages of each algorithm. The ideas of detection are as follows: (1) dataset creation: to build a short video dataset with multiple features; (2) multimodal rumor detection model: firstly, we use TSN (Temporal Segment Networks) video coding model to extract video features; then, we use OCR (Optical Character Recognition) and ASR (Automatic Character Recognition) to extract video 
    
[^93]: 对“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”的评论（arXiv: 2304.08297v2 [eess.IV] UPDATED）

    Comments on 'Fast and scalable search of whole-slide images via self-supervised deep learning'. (arXiv:2304.08297v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2304.08297](http://arxiv.org/abs/2304.08297)

    对陈等人发表在《自然—生物医学工程》杂志上的“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”一文的评论和关切。

    

    最近，陈等人（Chen2022）在《自然—生物医学工程》杂志上发表了题为“通过自监督深度学习进行快速和可扩展的全幻灯片图像搜索”的文章。该文章作者称其方法为“组织学自监督图像搜索，简称SISH。”我们对SISH表示了关切，因为它是Yottixel的增量修改，使用了MinMax二值化但未引用原始作品，并基于一个误称“自监督图像搜索”的概念。此外，我们还指出了陈等人进行实验和比较时存在的几个问题。

    Chen et al. [Chen2022] recently published the article 'Fast and scalable search of whole-slide images via self-supervised deep learning' in Nature Biomedical Engineering. The authors call their method 'self-supervised image search for histology', short SISH. We express our concerns that SISH is an incremental modification of Yottixel, has used MinMax binarization but does not cite the original works, and is based on a misnomer 'self-supervised image search'. As well, we point to several other concerns regarding experiments and comparisons performed by Chen et al.
    
[^94]: 解决面部验证边缘案例：深度分析和人机融合方法

    Tackling Face Verification Edge Cases: In-Depth Analysis and Human-Machine Fusion Approach. (arXiv:2304.08134v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2304.08134](http://arxiv.org/abs/2304.08134)

    本文研究了临近边缘案例的面部验证问题，发现结合人机决策可以进一步提高最先进的面部验证系统在各种基准数据集上的性能。

    

    目前，面部识别系统在几个数据集上已经超过了人类表现。然而，仍然存在一些机器无法正确分类的边缘案例。本文研究了机器和人操作员在面部验证任务中的组合效应。首先，我们仔细研究了几个最先进模型的边缘案例，以发现常见数据集的挑战性设置。然后，我们对这些选定任务中的60个参与者进行了一项人类研究，并提供了广泛的分析。最后，我们展示了将机器和人类决策结合起来，可以进一步提高最先进的面部验证系统在各种基准数据集上的性能。代码和数据可在GitHub上公开获取。

    Nowadays, face recognition systems surpass human performance on several datasets. However, there are still edge cases that the machine can't correctly classify. This paper investigates the effect of a combination of machine and human operators in the face verification task. First, we look closer at the edge cases for several state-of-the-art models to discover common datasets' challenging settings. Then, we conduct a study with 60 participants on these selected tasks with humans and provide an extensive analysis. Finally, we demonstrate that combining machine and human decisions can further improve the performance of state-of-the-art face verification systems on various benchmark datasets. Code and data are publicly available on GitHub.
    
[^95]: 一种数据中心的、基于Vision Transformer的非均质去雾解决方案。

    A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer. (arXiv:2304.07874v1 [cs.CV])

    [http://arxiv.org/abs/2304.07874](http://arxiv.org/abs/2304.07874)

    本文提出了一种基于Vision Transformer的数据中心的解决方案，用于解决非均质去雾问题。传统方法在处理NH-HAZE23数据集等非均质雾图像时存在问题，因为它们无法满足建模均质雾所需的假设之一。同时，本文指出光靠数据增广并不能解决问题，因为需要处理分布差异。

    

    近年来，图像去雾引起了越来越多的关注。许多深度学习方法已经被提出来应对这一挑战，并在处理均质雾时取得了显著的成就。然而，这些解决方案无法在应用于存在非均质雾的图像时保持类似的性能，例如NTIRE挑战介绍的NH-HAZE23数据集。其中一个失败的原因是非均质雾不符合建模均质雾所需的假设之一。此外，传统的端到端训练方法需要对大量的非均质雾图像与其清晰对应项进行配对，而NH-HAZE23数据集的数量是有限的。尽管可以通过利用其他非均质去雾数据集扩充NH-HAZE23数据集，但我们观察到有必要设计一种适当的数据预处理方法以减少目标数据集之间的分布差距。

    Recent years have witnessed an increased interest in image dehazing. Many deep learning methods have been proposed to tackle this challenge, and have made significant accomplishments dealing with homogeneous haze. However, these solutions cannot maintain comparable performance when they are applied to images with non-homogeneous haze, e.g., NH-HAZE23 dataset introduced by NTIRE challenges. One of the reasons for such failures is that non-homogeneous haze does not obey one of the assumptions that is required for modeling homogeneous haze. In addition, a large number of pairs of non-homogeneous hazy image and the clean counterpart is required using traditional end-to-end training approaches, while NH-HAZE23 dataset is of limited quantities. Although it is possible to augment the NH-HAZE23 dataset by leveraging other non-homogeneous dehazing datasets, we observe that it is necessary to design a proper data-preprocessing approach that reduces the distribution gaps between the target datase
    
[^96]: 学习经验Bregman散度用于不确定距离表示

    Learning Empirical Bregman Divergence for Uncertain Distance Representation. (arXiv:2304.07689v1 [cs.CV])

    [http://arxiv.org/abs/2304.07689](http://arxiv.org/abs/2304.07689)

    本文介绍了一种新的基于Deep Metric Learning的方法，通过学习经验Bregman散度直接从数据中进行不确定距离表示，能够有效的在模式识别和聚类任务上提高准确性。

    

    深度度量学习技术已应用于各种监督和无监督学习任务，通过深度网络学习样本嵌入来进行视觉表示。然而，经典方法采用固定距离度量作为两个嵌入之间的相似性函数，可能导致捕捉复杂数据分布的亚最优性能。Bregman散度概括了各种距离度量的度量，并在许多深度度量学习领域中产生。本文首先展示了如何从Bregman散度获得深度度量学习损失。然后，我们介绍了一种直接从数据中学习经验Bregman散度的新方法，通过使用深度学习设置对Bregman散度下的凸函数进行参数化。我们进一步实验证明，与其他SOTA深度度量学习方法相比，我们的方法在五个流行公共数据集上表现出色，特别是在模式识别和聚类任务上。

    Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognit
    
[^97]: 非洲机器学习研究趋势：30年文献计量分析综述

    Machine Learning Research Trends in Africa: A 30 Years Overview with Bibliometric Analysis Review. (arXiv:2304.07542v1 [cs.DL])

    [http://arxiv.org/abs/2304.07542](http://arxiv.org/abs/2304.07542)

    本文对非洲地区机器学习的最新发展和相关应用进行了广泛的文献调查及关键的文献计量分析，结果显示了机器学习研究和应用的当前现状及未来趋势，以促进未来的合作研究和知识交流。

    

    本文针对非洲地区机器学习的最新发展和相关应用进行了广泛的文献调查，并进行了关键的文献计量分析研究。该文献计量分析研究共收集了2761篇机器学习相关的文献，其中98％是发表在903个期刊上至少有482次引用的文章，时间跨度为过去的30年。另外，这些文献是从Science Citation Index EXPANDED检索中来自于1993年至2021年之间54个非洲国家的研究出版物。文献计量研究显示了机器学习研究和应用的当前现状和未来趋势的可视化，以促进非洲大陆分布在不同研究机构的作者之间进行未来的合作研究和知识交流。

    In this paper, a critical bibliometric analysis study is conducted, coupled with an extensive literature survey on recent developments and associated applications in machine learning research with a perspective on Africa. The presented bibliometric analysis study consists of 2761 machine learning-related documents, of which 98% were articles with at least 482 citations published in 903 journals during the past 30 decades. Furthermore, the collated documents were retrieved from the Science Citation Index EXPANDED, comprising research publications from 54 African countries between 1993 and 2021. The bibliometric study shows the visualization of the current landscape and future trends in machine learning research and its application to facilitate future collaborative research and knowledge exchange among authors from different research institutions scattered across the African continent.
    
[^98]: 基于边缘化耦合字典学习的实时图像注释方法

    Toward Real-Time Image Annotation Using Marginalized Coupled Dictionary Learning. (arXiv:2304.06907v1 [cs.CV])

    [http://arxiv.org/abs/2304.06907](http://arxiv.org/abs/2304.06907)

    本文提出了一种基于耦合字典学习和边缘化损失函数的实时图像注释方法，该方法能够学习有限数量的视觉原型和相应的语义，并保持标签的稀疏不平衡性，取得了良好的实验效果。

    

    在大多数图像检索系统中，图像包含各种高层语义，被称为标签或注释。几乎所有处理非均衡标记的最先进的图像注释方法都是基于搜索的技术，这些技术非常耗时。本文提出了一种新的耦合字典学习方法，同时学习有限数量的视觉原型和相应的语义，从而实现了实时图像注释过程。本文的另一个贡献是使用边缘损失函数，而不是对于不平衡标签的图像注释不合适的平方损失函数。在我们的方法中，我们使用了边缘化损失函数来利用一种简单有效的原型更新方法。同时，我们在语义原型上引入了${\ell}_1$正则化，以保持学习语义原型中标签的稀疏不平衡性。最后，进行了全面的实验验证。

    In most image retrieval systems, images include various high-level semantics, called tags or annotations. Virtually all the state-of-the-art image annotation methods that handle imbalanced labeling are search-based techniques which are time-consuming. In this paper, a novel coupled dictionary learning approach is proposed to learn a limited number of visual prototypes and their corresponding semantics simultaneously. This approach leads to a real-time image annotation procedure. Another contribution of this paper is that utilizes a marginalized loss function instead of the squared loss function that is inappropriate for image annotation with imbalanced labels. We have employed a marginalized loss function in our method to leverage a simple and effective method of prototype updating. Meanwhile, we have introduced ${\ell}_1$ regularization on semantic prototypes to preserve the sparse and imbalanced nature of labels in learned semantic prototypes. Finally, comprehensive experimental resu
    
[^99]: 多属性多阶图卷积神经网络用于异构图

    Attributed Multi-order Graph Convolutional Network for Heterogeneous Graphs. (arXiv:2304.06336v1 [cs.LG])

    [http://arxiv.org/abs/2304.06336](http://arxiv.org/abs/2304.06336)

    本文提出了一个AMOGCN模型，它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径，并使用节点属性评价监督。其能够有效地从异构图中发现有区别的节点嵌入和关系。

    

    异构图神经网络旨在从多关系网络中发现有区别的节点嵌入和关系。异构图学习的一个挑战是设计可学习的元路径，它显着地影响了学习到的嵌入的质量。因此，在本文中，我们提出了一个带属性的多阶图卷积网络（AMOGCN），它自动从多阶邻接矩阵的自适应聚合中研究包含多跳邻居的元路径。该模型首先从手动设计的节点连接中构建不同阶数的邻接矩阵。之后，从各种阶数的邻接矩阵的自动融合中附加一个完整的多阶邻接矩阵。这个过程由从节点同质性通过属性评价提取的节点语义信息监督。最终，我们使用一个学习到的多阶邻接矩阵的一层简化图卷积网络。

    Heterogeneous graph neural networks aim to discover discriminative node embeddings and relations from multi-relational networks.One challenge of heterogeneous graph learning is the design of learnable meta-paths, which significantly influences the quality of learned embeddings.Thus, in this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically studies meta-paths containing multi-hop neighbors from an adaptive aggregation of multi-order adjacency matrices. The proposed model first builds different orders of adjacency matrices from manually designed node connections. After that, an intact multi-order adjacency matrix is attached from the automatic fusion of various orders of adjacency matrices. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we utilize a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix,
    
[^100]: CMOS + 随机纳米磁体：概率推理与学习异构计算机

    CMOS + stochastic nanomagnets: heterogeneous computers for probabilistic inference and learning. (arXiv:2304.05949v1 [cond-mat.mes-hall])

    [http://arxiv.org/abs/2304.05949](http://arxiv.org/abs/2304.05949)

    本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型，其成功地执行了概率推理和异步Boltzmann学习。

    

    随着摩尔定律的放缓，利用新兴的纳米技术（X）增强互补金属氧化物半导体（CMOS）晶体管变得越来越重要。本文展示了如何将基于随机磁隧道结（sMTJ）的概率比特（p位）与多功能可编程门阵列（FPGA）相结合，设计出一种能源高效的异构CMOS + X（X = sMTJ）原型。尽管sMTJs设备间存在差异，我们的异构计算机成功地执行了概率推理和异步Boltzmann学习。使用CMOS预测流程设计套件（PDK）进行全面比较，数字CMOS-based p-bits模拟高质量随机性需要超过10,000个晶体管，每生成一个随机数的能量比使用只消耗2fJ的sMTJ-based p-bits高约两个数量级。我们的方法的缩放和集成版本可以显着推进概率性的推理。

    With the slowing down of Moore's law, augmenting complementary-metal-oxide semiconductor (CMOS) transistors with emerging nanotechnologies (X) is becoming increasingly important. In this paper, we demonstrate how stochastic magnetic tunnel junction (sMTJ)-based probabilistic bits, or p-bits, can be combined with versatile Field Programmable Gate Arrays (FPGA) to design an energy-efficient, heterogeneous CMOS + X (X = sMTJ) prototype. Our heterogeneous computer successfully performs probabilistic inference and asynchronous Boltzmann learning despite device-to-device variations in sMTJs. A comprehensive comparison using a CMOS predictive process design kit (PDK) reveals that digital CMOS-based p-bits emulating high-quality randomness use over 10,000 transistors with the energy per generated random number being roughly two orders of magnitude greater than the sMTJ-based p-bits that dissipate only 2 fJ. Scaled and integrated versions of our approach can significantly advance probabilistic 
    
[^101]: 神经网络架构

    Neural Network Architectures. (arXiv:2304.05133v1 [cs.LG])

    [http://arxiv.org/abs/2304.05133](http://arxiv.org/abs/2304.05133)

    这份讲义概述了神经网络的数学视角，并介绍了前馈神经网络、卷积神经网络、残差网络和循环神经网络等架构，这些架构给机器学习提供了一种优化问题的思路。

    

    这份讲义从数学角度概述了神经网络架构。特别地，使用神经网络进行机器学习被视为一个优化问题。介绍了神经网络的基础知识以及下列几种架构：前馈神经网络，卷积神经网络，残差网络和循环神经网络。

    These lecture notes provide an overview of Neural Network architectures from a mathematical point of view. Especially, Machine Learning with Neural Networks is seen as an optimization problem. Covered are an introduction to Neural Networks and the following architectures: Feedforward Neural Network, Convolutional Neural Network, ResNet, and Recurrent Neural Network.
    
[^102]: 基于NeRF技术的卫星图像表面重建

    NeRF applied to satellite imagery for surface reconstruction. (arXiv:2304.04133v1 [cs.CV])

    [http://arxiv.org/abs/2304.04133](http://arxiv.org/abs/2304.04133)

    本文提出了Sat-NeRF模型，能够从少量的卫星图像集合中合成新的视角，并准确地估计场景表面的高程。

    

    本文提出了Sat-NeRF模型，是对最近引入的S-NeRF模型的修改实现。该模型能够从稀疏的卫星图像集合中合成新的视角，同时考虑到图片中的光照变化。训练好的模型还能够精确地估计场景表面的高程，这对卫星观测应用非常有帮助。S-NeRF方法改进了标准的NeRF方法，将辐射强度考虑为高反射率和入射辐照度的函数。这两个量都是模型的全连接神经网络枝条的输出，而后者则被视为来自太阳的直接光线和来自天空的漫反射颜色函数。该实现基于用缩放-裁剪技术增强的卫星图像数据集。对NeRF进行了超参数研究，得出了一些有趣的观察结果。

    We present Sat-NeRF, a modified implementation of the recently introduced Shadow Neural Radiance Field (S-NeRF) model. This method is able to synthesize novel views from a sparse set of satellite images of a scene, while accounting for the variation in lighting present in the pictures. The trained model can also be used to accurately estimate the surface elevation of the scene, which is often a desirable quantity for satellite observation applications. S-NeRF improves on the standard Neural Radiance Field (NeRF) method by considering the radiance as a function of the albedo and the irradiance. Both these quantities are output by fully connected neural network branches of the model, and the latter is considered as a function of the direct light from the sun and the diffuse color from the sky. The implementations were run on a dataset of satellite images, augmented using a zoom-and-crop technique. A hyperparameter study for NeRF was carried out, leading to intriguing observations on the 
    
[^103]: 无偏关于坡度函数的适当学习：越过黑盒修正障碍

    Agnostic proper learning of monotone functions: beyond the black-box correction barrier. (arXiv:2304.02700v1 [cs.DS])

    [http://arxiv.org/abs/2304.02700](http://arxiv.org/abs/2304.02700)

    本文提出了第一个无偏、高效、适当的单调布尔函数学习算法，算法的运行时间和假设的大小和评估时间都为$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$，该算法解决了样本高效算法无法解决的问题。

    

    本文提出了第一个无偏、高效、适当的单调布尔函数学习算法。给定未知函数$f:\{\pm 1\}^n \rightarrow \{\pm 1\}$的$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$个均匀随机样本，算法输出一个假设$g:\{\pm 1\}^n \rightarrow \{\pm 1\}$，该假设是单调的，并且与$f$的距离为$(\mathrm{opt} + \varepsilon)$，其中$\mathrm{opt}$是$f$与最近单调函数之间的距离。算法的运行时间（因此也是假设的大小和评估时间）也是$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$，几乎与Blais等人（RANDOM '15）的下界相匹配。我们还给出一个算法，用于估计未知函数$f$到单调性的添加误差$\varepsilon$的距离，其运行时间为$2^{\tilde{O}(\sqrt{n}/\varepsilon)}$。以前，针对这两个问题，已知有样本有效的算法，但这些算法并不是运行时间有效的。因此，我们的工作解决了这个问题。

    We give the first agnostic, efficient, proper learning algorithm for monotone Boolean functions. Given $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$ uniformly random examples of an unknown function $f:\{\pm 1\}^n \rightarrow \{\pm 1\}$, our algorithm outputs a hypothesis $g:\{\pm 1\}^n \rightarrow \{\pm 1\}$ that is monotone and $(\mathrm{opt} + \varepsilon)$-close to $f$, where $\mathrm{opt}$ is the distance from $f$ to the closest monotone function. The running time of the algorithm (and consequently the size and evaluation time of the hypothesis) is also $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$, nearly matching the lower bound of Blais et al (RANDOM '15). We also give an algorithm for estimating up to additive error $\varepsilon$ the distance of an unknown function $f$ to monotone using a run-time of $2^{\tilde{O}(\sqrt{n}/\varepsilon)}$. Previously, for both of these problems, sample-efficient algorithms were known, but these algorithms were not run-time efficient. Our work thus closes this g
    
[^104]: 重新思考图形后门攻击中注入触发器的位置

    Rethinking the Trigger-injecting Position in Graph Backdoor Attack. (arXiv:2304.02277v1 [cs.LG])

    [http://arxiv.org/abs/2304.02277](http://arxiv.org/abs/2304.02277)

    论文研究了在图神经网络中的背门攻击，发现在样本的最不重要区域中注入触发器的背门攻击效果更好，对该现象进行了解释。

    

    后门攻击已经被证明是机器学习模型的安全威胁。传统的后门攻击意图将后门功能注入模型中，使得带有预定义后门触发器的输入能够出现异常的表现，但是该模型在干净的输入上仍然能够达到最先进的性能。虽然已经有一些关于图神经网络（GNN）背门攻击的研究，但是在图领域中的后门触发器大多被注入到样本的随机位置。尚未有研究分析和解释在样本的最重要或最不重要的区域注入触发器的背门攻击的性能，我们将其称为触发注入策略MIAS和LIAS。我们的研究结果表明，一般来说，LIAS的表现更好，并且LIAS和MIAS表现之间的差异可能是显著的。此外，我们通过解释说明了这两种策略的相似（更好）的攻击性能。

    Backdoor attacks have been demonstrated as a security threat for machine learning models. Traditional backdoor attacks intend to inject backdoor functionality into the model such that the backdoored model will perform abnormally on inputs with predefined backdoor triggers and still retain state-of-the-art performance on the clean inputs. While there are already some works on backdoor attacks on Graph Neural Networks (GNNs), the backdoor trigger in the graph domain is mostly injected into random positions of the sample. There is no work analyzing and explaining the backdoor attack performance when injecting triggers into the most important or least important area in the sample, which we refer to as trigger-injecting strategies MIAS and LIAS, respectively. Our results show that, generally, LIAS performs better, and the differences between the LIAS and MIAS performance can be significant. Furthermore, we explain these two strategies' similar (better) attack performance through explanation
    
[^105]: 区域风力特征如何影响基于CNN的风速预测：来自时空相关性分析的见解。

    How Regional Wind Characteristics Affect CNN-based wind predictions: Insights from Spatiotemporal Correlation Analysis. (arXiv:2304.01545v1 [cs.LG])

    [http://arxiv.org/abs/2304.01545](http://arxiv.org/abs/2304.01545)

    本研究探讨了使用3D卷积神经网络（3D-CNN）对时空数据进行风速预测的精度，并发现使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。

    

    本研究探讨了时空数据维度对利用人工神经网络构建的风速预测模型精度的影响。尽管以前的研究表明，加入空间数据可以提高风速预测模型的精度，但很少有研究探讨了基于神经网络的预测模型中不同空间尺度改进的程度。此外，对于这些模型的最佳时间长度的输入数据的研究也很有限。为了解决这个问题，本研究在使用3D卷积神经网络（3D-CNN）预测风速时，采用具有不同时空维度的数据作为输入，并评估其预测性能。结果表明，使用周围区域的空间数据进行3D-CNN训练可以比仅使用单点信息获得更好的预测性能。此外，多时间数据对预测性能的影响更为显著。

    This study investigates the impact of spatiotemporal data dimensions on the precision of a wind forecasting model developed using an artificial neural network. Although previous studies have shown that incorporating spatial data can enhance the accuracy of wind forecasting models, few investigations have explored the extent of the improvement owing to different spatial scales in neural network-based predictive models. Additionally, there are limited studies on the optimal temporal length of the input data for these models. To address this gap, this study employs data with various spatiotemporal dimensions as inputs when forecasting wind using 3D-Convolutional Neural Networks (3D-CNN) and assesses their predictive performance. The results indicate that using spatial data of the surrounding area for 3D-CNN training can achieve better predictive performance than using only single-point information. Additionally, multi-time data had a more positive effect on the predictive performance than
    
[^106]: 用热噪声描绘神经网络景观的地形

    Charting the Topography of the Neural Network Landscape with Thermal-Like Noise. (arXiv:2304.01335v1 [cond-mat.stat-mech])

    [http://arxiv.org/abs/2304.01335](http://arxiv.org/abs/2304.01335)

    本文采用统计力学方法研究全连接神经网络的优化问题，发现在低损失区域存在一个低维流形，并由靠近分类决策边界的数据点数量决定维度。

    

    神经网络训练是一个复杂的高维、非凸且嘈杂的优化问题，其理论理解在应用角度和基础研究方面均具有重要意义。本文采用标准的统计力学方法，即通过Langevin动态相空间探测方法研究过参数全连接网络在随机数据上执行分类任务的梯度下降过程。通过分析涨落统计数据，类比于体系在恒定温度下的热动力学，我们推断出了一个清晰的地形描述——低损失区域是一个低维流形，其维度可以轻易地从波动性中获得。此外，该维度受到靠近分类决策边界的数据点数量的控制。重要的是，我们发现一个四阶相互作用是关键的，而标准的 Langevin 方法不能准确描述这种相互作用。

    The training of neural networks is a complex, high-dimensional, non-convex and noisy optimization problem whose theoretical understanding is interesting both from an applicative perspective and for fundamental reasons. A core challenge is to understand the geometry and topography of the landscape that guides the optimization. In this work, we employ standard Statistical Mechanics methods, namely, phase-space exploration using Langevin dynamics, to study this landscape for an over-parameterized fully connected network performing a classification task on random data. Analyzing the fluctuation statistics, in analogy to thermal dynamics at a constant temperature, we infer a clear geometric description of the low-loss region. We find that it is a low-dimensional manifold whose dimension can be readily obtained from the fluctuations. Furthermore, this dimension is controlled by the number of data points that reside near the classification decision boundary. Importantly, we find that a quadra
    
[^107]: 面向金融数据科学竞赛的多元时间序列数据特征工程方法

    Feature Engineering Methods on Multivariate Time-Series Data for Financial Data Science Competitions. (arXiv:2303.16117v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.16117](http://arxiv.org/abs/2303.16117)

    针对金融数据科学竞赛，本研究尝试采用多元时间序列特征工程方法，利用美国市场价格数据进行测试，并验证其在Numerai-Signals目标上的预测能力。

    

    我们应用不同的时间序列特征工程方法对美国市场价格数据进行处理，并测试模型在Numerai-Signals目标上的预测能力。

    We apply different feature engineering methods for time-series to US market price data. The predictive power of models are tested against Numerai-Signals targets.
    
[^108]: 健壮的风险感知期权对冲

    Robust Risk-Aware Option Hedging. (arXiv:2303.15216v1 [q-fin.CP])

    [http://arxiv.org/abs/2303.15216](http://arxiv.org/abs/2303.15216)

    本研究利用健壮的风险感知强化学习算法，优化期权对冲策略，特别应用于界限期权对冲，随着代理风险偏好变化，对冲策略发生扭曲，鲁棒策略优于非鲁棒策略。

    

    期权对冲/交易的目标不仅仅是为了保护下行风险，还希望寻求收益，驱动策略。本研究展示了健壮的风险感知强化学习(RL)在减轻与路径相关的金融衍生品风险方面的潜力。我们利用Jaimungal、Pesenti、Wang、Tatsat(2022)的策略梯度方法，优化健壮的风险感知绩效标准，具体应用于界限期权对冲，并强调随着代理从风险规避转变为风险寻求，最优对冲策略会发生扭曲，以及代理如何强化其策略。我们进一步研究了当数据生成过程(DGP)与训练DGP不同时，对冲的表现，并证明了鲁棒策略优于非鲁棒策略。

    The objectives of option hedging/trading extend beyond mere protection against downside risks, with a desire to seek gains also driving agent's strategies. In this study, we showcase the potential of robust risk-aware reinforcement learning (RL) in mitigating the risks associated with path-dependent financial derivatives. We accomplish this by leveraging the Jaimungal, Pesenti, Wang, Tatsat (2022) and their policy gradient approach, which optimises robust risk-aware performance criteria. We specifically apply this methodology to the hedging of barrier options, and highlight how the optimal hedging strategy undergoes distortions as the agent moves from being risk-averse to risk-seeking. As well as how the agent robustifies their strategy. We further investigate the performance of the hedge when the data generating process (DGP) varies from the training DGP, and demonstrate that the robust strategies outperform the non-robust ones.
    
[^109]: 处理异构3D MR膝关节图像：一种具有双重知识蒸馏的联邦少样本学习方法

    Dealing With Heterogeneous 3D MR Knee Images: A Federated Few-Shot Learning Method With Dual Knowledge Distillation. (arXiv:2303.14357v1 [eess.IV])

    [http://arxiv.org/abs/2303.14357](http://arxiv.org/abs/2303.14357)

    本文提出了一种具有双重知识蒸馏的联邦少样本学习方法，利用公共数据库的知识来缓解私人标注图像的短缺，并通过有限的标注数据、无监督学习和双重知识蒸馏，达到优于现有最先进方法的结果。

    

    联邦学习在医学机构中越来越受欢迎，因为它可以在不汇总数据的情况下实现客户(如医院)之间的协作培训。然而，由于为大型3D图像数据集创建注释的成本高昂，特别是对于医疗机构来说，他们没有足够的监督数据来进行本地培训。因此，有限监督下的合作模型表现不佳。另一方面，大型机构有资源来编制高分辨率图像和标签数据存储库。因此，个体客户可以利用从公共数据存储库中获得的知识来缓解私有标注图像的短缺。在本文中，我们提出了一种具有双重知识蒸馏的联邦少样本学习方法。该方法允许在客户之间进行有限标注的联合培训，而不会危害隐私。所提出方法的监督学习从每个客户的有限标注数据中提取特征，而无监督学习从公共数据存储库中学习。此外，实现了双重知识蒸馏以生成更多的不同特征。对膝关节MR图像的异构临床数据集进行的实验证明，我们提出的方法优于现有的最先进方法，实现了0.87的AUC得分。

    Federated Learning has gained popularity among medical institutions since it enables collaborative training between clients (e.g., hospitals) without aggregating data. However, due to the high cost associated with creating annotations, especially for large 3D image datasets, clinical institutions do not have enough supervised data for training locally. Thus, the performance of the collaborative model is subpar under limited supervision. On the other hand, large institutions have the resources to compile data repositories with high-resolution images and labels. Therefore, individual clients can utilize the knowledge acquired in the public data repositories to mitigate the shortage of private annotated images. In this paper, we propose a federated few-shot learning method with dual knowledge distillation. This method allows joint training with limited annotations across clients without jeopardizing privacy. The supervised learning of the proposed method extracts features from limited lab
    
[^110]: 基于占据核主成分分析的故障检测

    Fault Detection via Occupation Kernel Principal Component Analysis. (arXiv:2303.11138v1 [stat.ML])

    [http://arxiv.org/abs/2303.11138](http://arxiv.org/abs/2303.11138)

    本文提出了一种使用占据核PCA方法进行故障检测的新方法，并且通过数值模拟验证了其有效性。

    

    自动系统的可靠操作很大程度上依赖于检测基础动态系统中的故障。虽然传统的基于模型的方法已被广泛用于故障检测，但基于数据的方法因其易于部署和对专家知识需求最小的特点而受到越来越多的关注。本文提出了一种使用占据核进行主成分分析（PCA）的新方法。占据核产生的特征映射适用于测量数据，由于使用积分具有内在的噪声鲁棒性，并且可以利用长度可变的不规则采样系统轨迹进行PCA。占据核PCA方法被用于开发一种重构误差方法进行故障检测，并且通过数值模拟验证了其有效性。

    The reliable operation of automatic systems is heavily dependent on the ability to detect faults in the underlying dynamical system. While traditional model-based methods have been widely used for fault detection, data-driven approaches have garnered increasing attention due to their ease of deployment and minimal need for expert knowledge. In this paper, we present a novel principal component analysis (PCA) method that uses occupation kernels. Occupation kernels result in feature maps that are tailored to the measured data, have inherent noise-robustness due to the use of integration, and can utilize irregularly sampled system trajectories of variable lengths for PCA. The occupation kernel PCA method is used to develop a reconstruction error approach to fault detection and its efficacy is validated using numerical simulations.
    
[^111]: 国家癌症研究所影像数据共享平台：计算病理学可重复研究的基础

    The NCI Imaging Data Commons as a platform for reproducible research in computational pathology. (arXiv:2303.09354v1 [cs.CV])

    [http://arxiv.org/abs/2303.09354](http://arxiv.org/abs/2303.09354)

    国家癌症研究所影像数据共享平台 (IDC) 旨在促进计算病理学领域的研究可重复性，实现了 FAIR 原则，提供公共库和云端技术支持，方便使用机器学习方法进行癌症组织分类研究。

    

    目的：可重复性对于将计算病理学（CompPath）中基于机器学习（ML）的解决方案转化为实践至关重要。然而，越来越多的研究报告难以重复 ML 结果的困难。国家癌症研究所影像数据共享平台（IDC）是一个公共库，包含 >120 个癌症图像收集，包括 >38,000 张全切片图像（WSIs），旨在与云端 ML 服务一起使用。本文探讨了 IDC 促进 CompPath 研究可重复性的潜力。 材料和方法：IDC 实现了 FAIR 原则：所有图像都根据 DICOM 标准进行编码，具有持久化标识符、可通过丰富的元数据进行发现，并可通过开放式工具访问。借此优势，我们在 IDC 的不同数据集上实现了两个实验，针对肺癌组织分类的一种代表性基于 ML 的方法进行了训练和/或评估。为评估可重复性，实验被多次运行。

    Objective: Reproducibility is critical for translating machine learning-based (ML) solutions in computational pathology (CompPath) into practice. However, an increasing number of studies report difficulties in reproducing ML results. The NCI Imaging Data Commons (IDC) is a public repository of >120 cancer image collections, including >38,000 whole-slide images (WSIs), that is designed to be used with cloud-based ML services. Here, we explore the potential of the IDC to facilitate reproducibility of CompPath research.  Materials and Methods: The IDC realizes the FAIR principles: All images are encoded according to the DICOM standard, persistently identified, discoverable via rich metadata, and accessible via open tools. Taking advantage of this, we implemented two experiments in which a representative ML-based method for classifying lung tumor tissue was trained and/or evaluated on different datasets from the IDC. To assess reproducibility, the experiments were run multiple times with i
    
[^112]: 在分布转移下诊断模型性能

    Diagnosing Model Performance Under Distribution Shift. (arXiv:2303.02011v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.02011](http://arxiv.org/abs/2303.02011)

    本研究提出一种名为 DISDE 的方法，用于分析模型在不同分布情况下的性能变化。该方法将性能下降分解为三个方面：难度更大但更频繁出现的示例增加、特征和结果之间关系的变化和在训练期间不频繁或未见过的示例性能差。

    

    当模型在不同于训练分布的目标分布下运行时，其性能可能会下降。为了理解这些操作失败模式，我们开发了一种方法，称为 DIstribution Shift DEcomposition（DISDE），将性能下降归因于不同类型的分布转移。我们的方法将性能下降分解为以下几个方面：1）来自训练的更难但更频繁的示例增加；2）特征和结果之间关系的变化；3）在训练期间不频繁或未见过的示例性能差。为了实现这一点，我们在固定 $X$ 的分布的同时改变 $Y \mid X$ 的条件分布，或在固定 $Y \mid X$ 的条件分布的同时改变 $X$ 的分布，从而定义了一个关于 $X$ 的假设分布，其中包含训练和目标中共同的值，可以轻松地比较 $Y \mid X$ 并进行预测。

    Prediction models can perform poorly when deployed to target distributions different from the training distribution. To understand these operational failure modes, we develop a method, called DIstribution Shift DEcomposition (DISDE), to attribute a drop in performance to different types of distribution shifts. Our approach decomposes the performance drop into terms for 1) an increase in harder but frequently seen examples from training, 2) changes in the relationship between features and outcomes, and 3) poor performance on examples infrequent or unseen during training. These terms are defined by fixing a distribution on $X$ while varying the conditional distribution of $Y \mid X$ between training and target, or by fixing the conditional distribution of $Y \mid X$ while varying the distribution on $X$. In order to do this, we define a hypothetical distribution on $X$ consisting of values common in both training and target, over which it is easy to compare $Y \mid X$ and thus predictive
    
[^113]: 动态演化算法在百万像素病理图片深度嵌入特征选择中的应用

    Evolutionary Computation in Action: Feature Selection for Deep Embedding Spaces of Gigapixel Pathology Images. (arXiv:2303.00943v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.00943](http://arxiv.org/abs/2303.00943)

    本论文提出了一种动态演化算法，采用大规模多目标优化方法选取百万像素病理图片中的特征，构建频繁特征直方图（FFH）WSI表示，以提高WSI图像处理的效率。

    

    采用数字病理学技术仍面临着高维度数字病理切片的挑战，即如何高效处理整个切片图像（WSI）。为了加速图像分析并为病理学结果的可视化和可解释性提供帮助，需利用深度学习技术引入紧凑的WSI表示方法。本文提出了一种新的基于大规模多目标优化的嵌入式WSI表示的动态演化算法。我们从基于补丁的采样开始，利用KimiaNet等组织病理学专用深度网络来提取大量特征向量。在粗糙的多目标特征选择阶段，采用分类准确性和特征数量作为指导，使用缩小搜索空间的策略。在第二阶段，通过多次粗略的演化算法优化得到相对较优的特征集，构建了频繁特征直方图（FFH）WSI表示。最终通过进一步优化得到了WIS的紧凑嵌入表示，以提高WSI图像处理的效率。

    One of the main obstacles of adopting digital pathology is the challenge of efficient processing of hyperdimensional digitized biopsy samples, called whole slide images (WSIs). Exploiting deep learning and introducing compact WSI representations are urgently needed to accelerate image analysis and facilitate the visualization and interpretability of pathology results in a postpandemic world. In this paper, we introduce a new evolutionary approach for WSI representation based on large-scale multi-objective optimization (LSMOP) of deep embeddings. We start with patch-based sampling to feed KimiaNet , a histopathology-specialized deep network, and to extract a multitude of feature vectors. Coarse multi-objective feature selection uses the reduced search space strategy guided by the classification accuracy and the number of features. In the second stage, the frequent features histogram (FFH), a novel WSI representation, is constructed by multiple runs of coarse LSMOP. Fine evolutionary fea
    
[^114]: 一种双层经验风险最小化算法的下界和近似最优算法

    A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08766](http://arxiv.org/abs/2302.08766)

    该论文提出了一种双层经验风险最小化算法，使用的梯度计算次数 $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$，在样本复杂度方面是最优的。

    

    双层最优化问题越来越多地应用于机器学习中。在许多实际情况下，上层和下层目标对应于经验风险最小化问题，并因此具有总和结构。在这个背景下，我们提出了一个著名的SARAH算法的双层扩展。我们证明了该算法需要$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$次梯度计算才能实现$\varepsilon$稳定性，其中$n+m$是样本总数，这比先前所有的双层算法都要好。此外，我们提供了一个下界，用于得到双层问题的目标函数的近似稳定点所需的oracle调用次数。这个下界正是我们的算法所达到的，因此在样本复杂度方面是最优的。

    Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
    
[^115]: 智能材料中稀疏滞后模型的发现

    Discovering sparse hysteresis models for smart materials. (arXiv:2302.05313v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.05313](http://arxiv.org/abs/2302.05313)

    本文提出了一种利用机器学习中的稀疏回归技术建模智能材料滞后的方法，并成功对压电材料的滞后现象进行建模和预测。同时在磁性材料方面提供了稀疏白盒建模滞后的见解。

    

    本文提出了一种利用机器学习中的稀疏回归技术建模智能材料（尤其是压电材料）滞后的方法。该研究采用了最小二乘算法和顺序阈值方法对负责滞后的动态系统进行建模，得到了简洁的模型，可以准确预测模拟和实验压电材料数据的滞后现象。文章还模拟了不同的数值实验，包括学习蝴蝶形滞后、对压电致动器的真实滞后数据进行建模。此外，还通过以非取向电工钢为例，提供了对磁性材料稀疏白盒建模滞后的见解。

    This article presents an approach for modelling hysteresis in smart materials, specifically piezoelectric materials, that leverages recent advancements in machine learning, particularly in sparse-regression techniques. While sparse regression has previously been used to model various scientific and engineering phenomena, its application to nonlinear hysteresis modelling in piezoelectric materials has yet to be explored. The study employs the least-squares algorithm with a sequential threshold to model the dynamic system responsible for hysteresis, resulting in a concise model that accurately predicts hysteresis for both simulated and experimental piezoelectric material data. Several numerical experiments are performed, including learning butterfly-shaped hysteresis and modelling real-world hysteresis data for a piezoelectric actuator. Additionally, insights are provided on sparse white-box modelling of hysteresis for magnetic materials taking non-oriented electrical steel as an example
    
[^116]: 星形降噪扩散概率模型

    Star-Shaped Denoising Diffusion Probabilistic Models. (arXiv:2302.05259v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.05259](http://arxiv.org/abs/2302.05259)

    创新点在于提出了一种非马尔可夫扩散噪声过程的星形降噪扩散概率模型，能够广泛适用于指数族中的多种分布，特别适用于约束流形上的数据。

    

    基于降噪扩散概率模型（DDPM）的方法已经成为生成模型中无处不在的工具。但是，它们大多局限于高斯和离散扩散过程。我们提出了星形降噪扩散概率模型（SS-DDPM），一种具有非马尔可夫扩散噪声过程的模型。在高斯分布的情况下，该模型等效于马尔可夫DDPM。然而，它可以定义和适用于任意噪声分布，并且对于落在指数族中的广泛分布，它采用了高效的训练和采样算法。我们提供了一个简单的配方，用于设计具有Beta，von Mises-Fisher，Dirichlet，Wishart等分布的扩散样式模型，当数据位于约束流形上时特别有用，例如单位球，正半定矩阵的空间，概率单纯形等。我们在不同的设置中评估了该模型，并发现它很有竞争力。

    Methods based on Denoising Diffusion Probabilistic Models (DDPM) became a ubiquitous tool in generative modeling. However, they are mostly limited to Gaussian and discrete diffusion processes. We propose Star-Shaped Denoising Diffusion Probabilistic Models (SS-DDPM), a model with a non-Markovian diffusion-like noising process. In the case of Gaussian distributions, this model is equivalent to Markovian DDPMs. However, it can be defined and applied with arbitrary noising distributions, and admits efficient training and sampling algorithms for a wide range of distributions that lie in the exponential family. We provide a simple recipe for designing diffusion-like models with distributions like Beta, von Mises--Fisher, Dirichlet, Wishart and others, which can be especially useful when data lies on a constrained manifold such as the unit sphere, the space of positive semi-definite matrices, the probabilistic simplex, etc. We evaluate the model in different settings and find it competitive 
    
[^117]: 具有完成功能的神经通用邻居用于链接预测

    Neural Common Neighbor with Completion for Link Prediction. (arXiv:2302.00890v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00890](http://arxiv.org/abs/2302.00890)

    提出了神经通用邻居模型（NCN）用于链接预测，使用可学习的成对表示来捕捉节点之间的成对关系，以提高性能，同时解决链路不完整问题。

    

    尽管vanilla信息传递神经网络（MPNN）在各种图任务中具有出色的性能，但在链接预测任务中通常失败，因为它只使用两个单独目标节点的表示，并忽略它们之间的成对关系。为了捕获成对关系，一些模型将手动功能添加到输入图中，并使用MPNN的输出来生成成对表示。相反，其他人直接将手动功能用作成对表示。尽管此简化避免了将GNN逐个链接地应用于每个链接，从而提高了可扩展性，但由于手工制作的和不可学习的成对特征，这些模型仍有很大的性能提升空间。为了在保持可扩展性的同时提高性能，我们提出了神经通用邻居（NCN），它使用可学习的成对表示。为了进一步提高NCN的性能，我们研究了未观察到的链接问题。图的不完整性是普遍存在的，并导致分布偏移

    Despite its outstanding performance in various graph tasks, vanilla Message Passing Neural Network (MPNN) usually fails in link prediction tasks, as it only uses representations of two individual target nodes and ignores the pairwise relation between them. To capture the pairwise relations, some models add manual features to the input graph and use the output of MPNN to produce pairwise representations. In contrast, others directly use manual features as pairwise representations. Though this simplification avoids applying a GNN to each link individually and thus improves scalability, these models still have much room for performance improvement due to the hand-crafted and unlearnable pairwise features. To upgrade performance while maintaining scalability, we propose Neural Common Neighbor (NCN), which uses learnable pairwise representations. To further boost NCN, we study the unobserved link problem. The incompleteness of the graph is ubiquitous and leads to distribution shifts between
    
[^118]: 基于图的概率多智能体轨迹预测模型MTP-GO与神经ODE

    MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v3 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2302.00735](http://arxiv.org/abs/2302.00735)

    本文介绍了一种基于图的概率多智能体轨迹预测模型MTP-GO。该模型利用时间图神经网络编码场景，采用神经常微分方程实现运动模型，并结合混合密度网络和卡尔曼滤波实现多模态概率预测，在多个指标上优于其他最先进的方法。

    

    实现弹性的自主路径规划需要对周围道路用户未来行为做出可靠的预测。为响应此需求及相关挑战，本文引入了名为MTP-GO的模型。该模型采用时间图神经网络对场景进行编码，生成底层运动模型的输入。运动模型采用了神经常微分方程，其中的状态转移函数将和其他部分一起进行学习。结合混合密度网络和卡尔曼滤波的概念，可以获得多模态的概率预测。结果表明，所提出的模型在各种数据集上的预测能力优于几种最先进的方法，并且在多个指标上表现出色。

    Enabling resilient autonomous motion planning requires robust predictions of surrounding road users' future behavior. In response to this need and the associated challenges, we introduce our model titled MTP-GO. The model encodes the scene using temporal graph neural networks to produce the inputs to an underlying motion model. The motion model is implemented using neural ordinary differential equations where the state-transition functions are learned with the rest of the model. Multimodal probabilistic predictions are obtained by combining the concept of mixture density networks and Kalman filtering. The results illustrate the predictive capabilities of the proposed model across various data sets, outperforming several state-of-the-art methods on a number of metrics.
    
[^119]: Lie 群和它们的齐次空间上的静止核和高斯过程 II：非紧对称空间

    Stationary Kernels and Gaussian Processes on Lie Groups and their Homogeneous Spaces II: non-compact symmetric spaces. (arXiv:2301.13088v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2301.13088](http://arxiv.org/abs/2301.13088)

    本文开发了构建非欧几里得空间上静止高斯过程的实用技术，能够对定义在这些空间上的先验和后验高斯过程进行实际采样和计算协方差核。

    

    高斯过程是机器学习中最重要的时空模型之一，它可以编码有关建模函数的先验信息，并可用于精确或近似贝叶斯学习。在许多应用中，特别是在物理科学和工程领域，以及地质统计学和神经科学等领域，对对称性的不变性是可以考虑的最基本形式之一。高斯过程协方差对这些对称性的不变性引发了对这些空间的平稳性概念的最自然的推广。在这项工作中，我们开发了建立静止高斯过程的构造性和实用技术，用于在对称性背景下出现的非欧几里得空间的非常大的类。我们的技术使得能够（i）计算协方差核和（ii）从这些空间上定义的先验和后验高斯过程中实际地进行采样。

    Gaussian processes are arguably the most important class of spatiotemporal models within machine learning. They encode prior information about the modeled function and can be used for exact or approximate Bayesian learning. In many applications, particularly in physical sciences and engineering, but also in areas such as geostatistics and neuroscience, invariance to symmetries is one of the most fundamental forms of prior information one can consider. The invariance of a Gaussian process' covariance to such symmetries gives rise to the most natural generalization of the concept of stationarity to such spaces. In this work, we develop constructive and practical techniques for building stationary Gaussian processes on a very large class of non-Euclidean spaces arising in the context of symmetries. Our techniques make it possible to (i) calculate covariance kernels and (ii) sample from prior and posterior Gaussian processes defined on such spaces, both in a practical manner. This work is 
    
[^120]: AttMEMO: 在大内存系统上利用记忆化加速Transformers

    AttMEMO : Accelerating Transformers with Memoization on Big Memory Systems. (arXiv:2301.09262v2 [cs.PF] UPDATED)

    [http://arxiv.org/abs/2301.09262](http://arxiv.org/abs/2301.09262)

    本文提出一种利用记忆技术加速自注意力机制的Transformer模型的推理过程的方法，该方法可以在不需要修改模型架构或使用特殊硬件的情况下进行，并可以使推理延迟降低22%。

    

    Transformer模型因其优越的推理准确性和推理吞吐量而受到欢迎。然而，由于Transformer是计算密集型的，导致推理时间较长。现有的Transformer推理加速工作存在限制，这些限制要么是由于修改Transformer架构，要么是需要专门的硬件。本文提出了使用记忆化加速Transformer模型的机会，而不涉及以上限制。基于这样的独特观察，即在推理序列内Attention计算中存在丰富的相似性，我们构建了一个利用新兴的大内存系统的记忆化数据库。我们引入了一种新的嵌入技术来查找语义上相似的输入，以识别计算相似性。我们还引入了一系列技术，如内存映射和选择性记忆化，以避免内存复制和不必要的开销。我们使得推理延迟降低了22%，而且内存需求适中。我们的方法可以很容易地适用于各种Transformer模型，而且无需进行重要修改。

    Transformer models gain popularity because of their superior inference accuracy and inference throughput. However, the transformer is computation-intensive, causing a long inference time. The existing works on transformer inference acceleration have limitations caused by either the modification of transformer architectures or the need of specialized hardware. In this paper, we identify the opportunities of using memoization to accelerate the self-attention mechanism in transformers without the above limitations. Built upon a unique observation that there is rich similarity in attention computation across inference sequences, we build a memoization database that leverages the emerging big memory system. We introduce a novel embedding technique to find semantically similar inputs to identify computation similarity. We also introduce a series of techniques such as memory mapping and selective memoization to avoid memory copy and unnecessary overhead. We enable 22% inference-latency reduct
    
[^121]: 意识是学习的过程：通过绑定学习的预测处理系统可能会将自己感知为有意识的

    Consciousness is learning: predictive processing systems that learn by binding may perceive themselves as conscious. (arXiv:2301.07016v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2301.07016](http://arxiv.org/abs/2301.07016)

    通过层级绑定和联想检索变为短期和长期声明性记忆的在线预测处理系统可能会感知到自己具有意识。

    

    机器学习算法在特定复杂领域实现了超越人类的表现。然而，从少量示例中进行在线学习，并在不同领域之间高效地泛化仍然是难以实现的。在人类身上，这种学习通过声明性存储过程进行，并且与意识密切相关。预测处理被推广为一种基于贝叶斯推理框架的原则性方法，用于理解皮质如何实现深度生成感知模型，用于感官数据和行为控制。然而，预测处理对于快速组成式学习或意识之谜提供了很少的直接见解。在这里，我们提出，通过通过绑定预测中的层次模型来实现在线学习，预测处理系统可以通过从单个示例中为感知和行动形成工作记忆，在新情况下灵活泛化，这可通过联想检索变为短期和长期的声明性记忆。我们认为，这个过程，我们称之为“在线层级预测绑定”，也可能是系统感知自己具有意识的必要条件。由此产生的模型提供了一种关于感知的、运动的、认知的和情感的意识的统一解释，并具有进化和发育生物学的深刻根源。

    Machine learning algorithms have achieved superhuman performance in specific complex domains. Yet learning online from few examples and efficiently generalizing across domains remains elusive. In humans such learning proceeds via declarative memory formation and is closely associated with consciousness. Predictive processing has been advanced as a principled Bayesian inference framework for understanding the cortex as implementing deep generative perceptual models for both sensory data and action control. However, predictive processing offers little direct insight into fast compositional learning or the mystery of consciousness. Here we propose that through implementing online learning by hierarchical binding of unpredicted inferences, a predictive processing system may flexibly generalize in novel situations by forming working memories for perceptions and actions from single examples, which can become short- and long-term declarative memories retrievable by associative recall. We argu
    
[^122]: 自适应 LQR 算法的近乎必然 $\sqrt{T}$ 后悔上限分析

    Almost Surely $\sqrt{T}$ Regret Bound for Adaptive LQR. (arXiv:2301.05537v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.05537](http://arxiv.org/abs/2301.05537)

    本文提出了一种自适应LQR控制器，在几乎必然的情况下具有 $\tilde{ \mathcal{O}}(\sqrt{T})$ 后悔上限证明，且具有断电机制保证安全并对性能影响很小。

    

    对于未知系统参数的线性二次调节问题(LQR)已经得到广泛研究，但是至今仍不清楚是否能几乎必然地达到 $\tilde{ \mathcal{O}}(\sqrt{T})$ 的后悔上限，而本文则提出了一种自适应LQR控制器，在几乎必然的情况下具有 $\tilde{ \mathcal{O}}(\sqrt{T})$ 后悔上限的证明。该控制器具有断电机制，可以绕过潜在的安全隐患并确保系统参数估计的收敛性，但被证明只会有有限次触发，并对控制器的渐近性能几乎没有影响。通过在田纳西伊士曼(Tennessee Eastman)工艺中进行仿真验证了该控制器的有效性。

    The Linear-Quadratic Regulation (LQR) problem with unknown system parameters has been widely studied, but it has remained unclear whether $\tilde{ \mathcal{O}}(\sqrt{T})$ regret, which is the best known dependence on time, can be achieved almost surely. In this paper, we propose an adaptive LQR controller with almost surely $\tilde{ \mathcal{O}}(\sqrt{T})$ regret upper bound. The controller features a circuit-breaking mechanism, which circumvents potential safety breach and guarantees the convergence of the system parameter estimate, but is shown to be triggered only finitely often and hence has negligible effect on the asymptotic performance of the controller. The proposed controller is also validated via simulation on Tennessee Eastman Process~(TEP), a commonly used industrial process example.
    
[^123]: 一种用于约束极小极大优化问题的一阶增广拉格朗日方法

    A first-order augmented Lagrangian method for constrained minimax optimization. (arXiv:2301.02060v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2301.02060](http://arxiv.org/abs/2301.02060)

    本文提出了一种一阶增广拉格朗日方法来解决约束极小极大问题，其操作复杂度为 ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$。

    

    本文研究了一类约束极小极大问题。特别地，我们提出了一种一阶增广拉格朗日方法来解决这些问题，其子问题被发现是一个更简单的结构化极小极大问题，并且可以通过作者在 [26] 中最近开发的一阶方法来适当地解决。在一些适当的假设下，为了找到约束极小极大问题的一个 $\varepsilon$-KKT 解，该方法的操作复杂度为 ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$，该复杂度是由基本操作测量得到的。

    In this paper we study a class of constrained minimax problems. In particular, we propose a first-order augmented Lagrangian method for solving them, whose subproblems turn out to be a much simpler structured minimax problem and are suitably solved by a first-order method recently developed in [26] by the authors. Under some suitable assumptions, an \emph{operation complexity} of ${\cal O}(\varepsilon^{-4}\log\varepsilon^{-1})$, measured by its fundamental operations, is established for the first-order augmented Lagrangian method for finding an $\varepsilon$-KKT solution of the constrained minimax problems.
    
[^124]: 基于点云的毫米波通信主动链路质量预测

    Point Cloud-based Proactive Link Quality Prediction for Millimeter-wave Communications. (arXiv:2301.00752v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/2301.00752](http://arxiv.org/abs/2301.00752)

    本研究提出了一种基于点云的毫米波通信主动链路质量预测方法，相比于基于图像的方法，其适用性更广且不涉及敏感信息。

    

    本研究展示了基于点云的毫米波（mmWave）通信的主动链路质量预测的可行性。以往的研究提出了基于机器学习的方法，利用深度图像的时间序列来预测未来时间段的接收信号强度，以缓解行人阻挡因素对mmWave通信的影响。但是，由于隐私问题，这些基于图像的方法的适用性有限，因为摄像头图像可能包含敏感信息。本研究提出了一种基于点云的mmWave链路质量预测方法，并通过实验证明其可行性。点云将三维空间表示为点集，其空间性质更加稀疏，不太可能包含敏感信息，并且还提供了3D位置和运动信息，这对了解涉及行人的无线电传播环境是必要的。

    This study demonstrates the feasibility of point cloud-based proactive link quality prediction for millimeter-wave (mmWave) communications. Previous studies have proposed machine learning-based methods to predict received signal strength for future time periods using time series of depth images to mitigate the line-of-sight (LOS) path blockage by pedestrians in mmWave communication. However, these image-based methods have limited applicability due to privacy concerns as camera images may contain sensitive information. This study proposes a point cloud-based method for mmWave link quality prediction and demonstrates its feasibility through experiments. Point clouds represent three-dimensional (3D) spaces as a set of points and are sparser and less likely to contain sensitive information than camera images. Additionally, point clouds provide 3D position and motion information, which is necessary for understanding the radio propagation environment involving pedestrians. This study designs
    
[^125]: 通过算法重访不稳定公式定理

    The unstable formula theorem revisited via algorithms. (arXiv:2212.05050v2 [math.LO] UPDATED)

    [http://arxiv.org/abs/2212.05050](http://arxiv.org/abs/2212.05050)

    本文介绍了模型理论中有关理论稳定性和学习中算法稳定性的交互，通过算法性质取代了无限，重访了Shelah闻名的不稳定公式定理，并引入了可能最终正确学习模型，表征了Littlestone（稳定）类，透过模型论中类型的可定义性形式化了Littlestone类的逼近。

    

    本文介绍了有关模型理论中关于理论稳定性的基础结果与学习中算法稳定性之间惊人的交互，特别是通过算法性质取代无限，我们开发了Shelah闻名的不稳定公式定理的完整算法类比。这其中涉及了几个新定理以及最近的研究。特别地，我们引入了一个新的“可能最终正确”的学习模型，并通过这个模型表征了Littlestone（稳定）类；并通过模型论中类型的可定义性类比描述了Littlestone类的逼近。

    This paper is about the surprising interaction of a foundational result from model theory about stability of theories, which seems to be inherently about the infinite, with algorithmic stability in learning. Specifically, we develop a complete algorithmic analogue of Shelah's celebrated Unstable Formula Theorem, with algorithmic properties taking the place of the infinite. This draws on several new theorems as well as much recent work. In particular we introduce a new ``Probably Eventually Correct'' learning model, of independent interest, and characterize Littlestone (stable) classes in terms of this model; and we describe Littlestone classes via approximations, by analogy to definability of types in model theory.
    
[^126]: 多样化病理数据集上的自监督学习对比研究

    Benchmarking Self-Supervised Learning on Diverse Pathology Datasets. (arXiv:2212.04690v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.04690](http://arxiv.org/abs/2212.04690)

    本文针对计算病理学所需的大量标注数据问题，进行了最大规模的自监督预训练研究，发现大规模领域对齐的预训练方法在病理学中始终优于ImageNet上的预训练方法，并提出一套领域特定技术来提高模型性能，在月经周期病理数据集中也表现出良好性能。

    

    计算病理学可以挽救人类生命，但是模型需要大量标注数据，病理图像非常昂贵。自监督学习已经证明是利用无标签数据的有效方法，其在病理学中的应用可以极大地改善其下游任务。然而，没有一项基于原则的研究来比较自监督学习方法并讨论如何适应病理学。为了解决这个问题，我们进行了迄今为止规模最大的一项自监督预训练病理图像数据的研究，使用了4种代表性自监督学习方法进行了测试，并对不同下游任务进行了评估。我们发现在病理学中大规模领域对齐的预训练方法在标准的自监督学习设置（如线性和微调评估）以及在低标签环境下始终优于在ImageNet上的预训练方法。此外，我们提出了一套领域特定技术，我们通过实验表明这些技术可以提高模型性能。最后，我们首次将自监督学习转移学习应用于月经周期病理数据集，这是一个挑战性的未见过的领域，我们已经展示大规模领域对齐预训练可以显著提高该领域下游任务的性能。

    Computational pathology can lead to saving human lives, but models are annotation hungry and pathology images are notoriously expensive to annotate. Self-supervised learning has shown to be an effective method for utilizing unlabeled data, and its application to pathology could greatly benefit its downstream tasks. Yet, there are no principled studies that compare SSL methods and discuss how to adapt them for pathology. To address this need, we execute the largest-scale study of SSL pre-training on pathology image data, to date. Our study is conducted using 4 representative SSL methods on diverse downstream tasks. We establish that large-scale domain-aligned pre-training in pathology consistently out-performs ImageNet pre-training in standard SSL settings such as linear and fine-tuning evaluations, as well as in low-label regimes. Moreover, we propose a set of domain-specific techniques that we experimentally show leads to a performance boost. Lastly, for the first time, we apply SSL t
    
[^127]: 一种可解释的混合预测模型预测COVID-19病例数的自回归模型和LSTM

    An Interpretable Hybrid Predictive Model of COVID-19 Cases using Autoregressive Model and LSTM. (arXiv:2211.17014v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17014](http://arxiv.org/abs/2211.17014)

    本文提出了一种可解释的混合预测模型，该模型使用自回归模型和LSTM预测COVID-19病例，结合了两种模型的优势，通过数据自适应性决定模型块的相对贡献，在全面数值研究中展示了优异的性能。

    

    2019冠状病毒病（COVID-19）对全球健康和经济产生了深远的影响，因此构建准确且可解释的数据驱动预测模型以改善决策至关重要。本文提出了一种新颖的混合模型，该模型将自回归模型（AR）的可解释性和长短时记忆神经网络（LSTM）的预测能力相结合。该混合模型被正式建模为一个神经网络，其结构连接两个组成模型块，这两个块的相对贡献在训练过程中进行数据自适应性决定。通过对两个数据源的全面数值研究，我们展示了混合模型在其两个组成模型以及其他流行预测模型中的优异性能。

    The Coronavirus Disease 2019 (COVID-19) has a profound impact on global health and economy, making it crucial to build accurate and interpretable data-driven predictive models for COVID-19 cases to improve policy making. The extremely large scale of the pandemic and the intrinsically changing transmission characteristics pose great challenges for effective COVID-19 case prediction. To address this challenge, we propose a novel hybrid model in which the interpretability of the Autoregressive model (AR) and the predictive power of the long short-term memory neural networks (LSTM) join forces. The proposed hybrid model is formalized as a neural network with an architecture that connects two composing model blocks, of which the relative contribution is decided data-adaptively in the training procedure. We demonstrate the favorable performance of the hybrid model over its two component models as well as other popular predictive models through comprehensive numerical studies on two data sour
    
[^128]: 气候模式集合预测中的局部时间不变量度量

    A locally time-invariant metric for climate model ensemble predictions of extreme risk. (arXiv:2211.16367v3 [physics.ao-ph] UPDATED)

    [http://arxiv.org/abs/2211.16367](http://arxiv.org/abs/2211.16367)

    该论文提出了一种局部时间不变的气候模式模拟评估方法，通过评估气候模式模拟的极端情况来预测气候变化相关预测，取得了良好的效果。

    

    应对气候变化的相关预测通常是通过结合多种气候模式模拟来获得的。在高影响极端事件的背景下，基于性能的集合加权方案中使用的模型评估方法存在局限性。本文介绍了一种局部时间不变的气候模式模拟评估方法，重点是评估气候模式模拟的极端情况。我们探讨了这种方法在预测内罗毕极端高温天气方面的表现，并提供了八个额外城市的比较结果。

    Adaptation-relevant predictions of climate change are often derived by combining climate model simulations in a multi-model ensemble. Model evaluation methods used in performance-based ensemble weighting schemes have limitations in the context of high-impact extreme events. We introduce a locally time-invariant method for evaluating climate model simulations with a focus on assessing the simulation of extremes. We explore the behaviour of the proposed method in predicting extreme heat days in Nairobi and provide comparative results for eight additional cities.
    
[^129]: 离线强化学习在多样性多任务数据上进行，具有扩展性和推广性。

    Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes. (arXiv:2211.15144v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.15144](http://arxiv.org/abs/2211.15144)

    本文研究离线Q-learning算法在多任务Atari游戏上的表现，结果表明合适的选择能够提高扩展性和推广性，使模型性能与容量正相关，并且可以推广到数据集性能之外。

    

    离线强化学习具有很高的潜力，通过在大型异构数据集上训练高容量模型可以产生广义智能体，类似于视觉和自然语言处理领域的类似进展。然而，最近的研究表明，离线强化学习方法在扩展模型容量方面遇到了独特的挑战。本研究基于这些工作得出的结论重新审视以前的设计选择，并发现通过适当的选择，离线Q-learning算法表现出强大的性能和容量扩展。我们以多任务Atari作为扩展和推广的测试平台，使用高达8000万个参数网络在40个游戏上训练单个策略，实现接近人类水平的性能，发现模型性能与容量的关系呈正比。与以往研究不同，即使完全在大型（400M过渡）但高度难以抽象的子集上进行训练，我们也可以推广到数据集性能之外。

    The potential of offline reinforcement learning (RL) is that high-capacity models trained on large, heterogeneous datasets can lead to agents that generalize broadly, analogously to similar advances in vision and NLP. However, recent works argue that offline RL methods encounter unique challenges to scaling up model capacity. Drawing on the learnings from these works, we re-examine previous design choices and find that with appropriate choices: ResNets, cross-entropy based distributional backups, and feature normalization, offline Q-learning algorithms exhibit strong performance that scales with model capacity. Using multi-task Atari as a testbed for scaling and generalization, we train a single policy on 40 games with near-human performance using up-to 80 million parameter networks, finding that model performance scales favorably with capacity. In contrast to prior work, we extrapolate beyond dataset performance even when trained entirely on a large (400M transitions) but highly subop
    
[^130]: Waveflow：将边界条件应用于平滑归一化流的新方法，并以费米波函数为例

    Waveflow: Enforcing boundary conditions in smooth normalizing flows with application to fermionic wave functions. (arXiv:2211.14839v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14839](http://arxiv.org/abs/2211.14839)

    本文介绍了一种新的处理归一化流拓扑问题、将边界条件应用于归一化流的技术、引入了可以被制作成任意次可微的 I-Spline 双射，并将这些技术用于创建一种基于归一化流的费米波函数 Ansatz，从而实现高效训练。

    

    本文提出了四个主要的创新点。首先，我们介绍了一种处理归一化流拓扑问题的新方法。其次，我们描述了一种在归一化流上强制施加特定类别边界条件的技术。第三，我们引入了 I-Spline 双射，它像之前的工作一样利用了样条曲线，但与这些工作不同的是，它可以被制作成任意次可微的。最后，我们使用这些技术创建了 Waveflow，一种基于归一化流的一维多粒子费米波函数的 Ansatz，它可以通过变分量子蒙特卡罗高效地训练，无需 MCMC 或估计归一化常数。为了强制费米波函数所需的反对称性，我们仅在排列群的基本域上训练归一化流，这有效地将其减少为一个边界值问题。

    In this paper, we introduce four main novelties: First, we present a new way of handling the topology problem of normalizing flows. Second, we describe a technique to enforce certain classes of boundary conditions onto normalizing flows. Third, we introduce the I-Spline bijection, which, similar to previous work, leverages splines but, in contrast to those works, can be made arbitrarily often differentiable. And finally, we use these techniques to create Waveflow, an Ansatz for the one-space-dimensional multi-particle fermionic wave functions in real space based on normalizing flows, that can be efficiently trained with Variational Quantum Monte Carlo without the need for MCMC nor estimation of a normalization constant. To enforce the necessary anti-symmetry of fermionic wave functions, we train the normalizing flow only on the fundamental domain of the permutation group, which effectively reduces it to a boundary value problem.
    
[^131]: 基于粒子的预处理函数梯度流变分推断

    Particle-based Variational Inference with Preconditioned Functional Gradient Flow. (arXiv:2211.13954v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.13954](http://arxiv.org/abs/2211.13954)

    本文提出了一种新的基于粒子的变分推断算法PFG，通过引入包含RKHS范数的函数正则项实现更大的函数类和更好的适应性，解决了RKHS要求限制函数类和算法灵活性的问题，并在KL散度上提供了可证明的连续时间收敛。

    

    基于粒子的变分推断通过梯度流估计最小化模型样本与目标后验之间的KL散度。随着Stein变分梯度下降（SVGD）的流行，基于粒子的VI算法的重点已经转向在重现核希尔伯特空间（RKHS）中逼近梯度流的函数的特性。然而，RKHS的要求限制了函数类和算法的灵活性。本文通过引入包含了RKHS范数的函数正则化项，提供了这个问题的通用解决方案。这使得我们可以提出一个新的基于粒子的VI算法，叫做预处理函数梯度流（PFG）。与SVGD相比，PFG具有更大的函数类，改进了大量粒子场景的可扩展性，更适应病态分布，并在KL散度上提供了可证明的连续时间收敛。此外，非线性函数规范也可以轻松地并入到所提出的算法中。

    Particle-based variational inference (VI) minimizes the KL divergence between model samples and the target posterior with gradient flow estimates. With the popularity of Stein variational gradient descent (SVGD), the focus of particle-based VI algorithms has been on the properties of functions in Reproducing Kernel Hilbert Space (RKHS) to approximate the gradient flow. However, the requirement of RKHS restricts the function class and algorithmic flexibility. This paper offers a general solution to this problem by introducing a functional regularization term that encompasses the RKHS norm as a special case. This allows us to propose a new particle-based VI algorithm called preconditioned functional gradient flow (PFG). Compared to SVGD, PFG has several advantages. It has a larger function class, improved scalability in large particle-size scenarios, better adaptation to ill-conditioned distributions, and provable continuous-time convergence in KL divergence. Additionally, non-linear fun
    
[^132]: 顺序知情联合消除：联邦优化中高效且可证明的客户端消除

    Sequential Informed Federated Unlearning: Efficient and Provable Client Unlearning in Federated Optimization. (arXiv:2211.11656v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11656](http://arxiv.org/abs/2211.11656)

    本文提出一种名为知情联合消除（IFU）的新颖联邦优化方法，可实现有效且可量化的客户端消除请求，实验结果表明其效率较基本方法和最先进的FU方法更高。

    

    机器消除（MU）旨在提供有关从训练过程中删除给定数据点的贡献的理论保证。联邦消除（FU）是将MU扩展到从联合训练过程中消除给定客户端的贡献。当前的FU方法通常不具有可扩展性，并且没有对消除效果的有效性进行合理的理论量化。在本文中，我们提出了知情联合消除(IFU)，这是一种新颖的高效且可量化的FU方法。在接收到给定客户端的消除请求后，IFU通过随机扰动机制确定了重新初始化FL所需的最佳FL迭代，可以获得消除保证。IFU的理论也可以扩展以解决顺序消除请求。不同任务和数据集上的实验结果表明，与基本重新训练和最先进的FU方法相比，IFU可以实现更高效的消除过程。

    The aim of Machine Unlearning (MU) is to provide theoretical guarantees on the removal of the contribution of a given data point from a training procedure. Federated Unlearning (FU) consists in extending MU to unlearn a given client's contribution from a federated training routine. Current FU approaches are generally not scalable, and do not come with sound theoretical quantification of the effectiveness of unlearning. In this work we present Informed Federated Unlearning (IFU), a novel efficient and quantifiable FU approach. Upon unlearning request from a given client, IFU identifies the optimal FL iteration from which FL has to be reinitialized, with unlearning guarantees obtained through a randomized perturbation mechanism. The theory of IFU is also extended to account for sequential unlearning requests. Experimental results on different tasks and dataset show that IFU leads to more efficient unlearning procedures as compared to basic re-training and state-of-the-art FU approaches.
    
[^133]: 多分支结构下的个性化联邦学习

    Personalized Federated Learning with Multi-branch Architecture. (arXiv:2211.07931v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.07931](http://arxiv.org/abs/2211.07931)

    本文提出了一种新型的个性化联邦学习方法(pFedMB)，通过多分支结构实现个性化，并设计了一种聚合方法以提高通信效率和降低计算开销。该方法在合成数据和真实世界数据的实验中均表现出最先进的性能，同时在客户端具有来自复杂分布的数据且不能确定彼此的分布的情况下，能够促进拥有相似数据的客户端进行更多协作。

    

    联邦学习（FL）是一种去中心化的机器学习技术，使得多个客户端可以协作训练模型，而无需客户端相互揭示其原始数据。虽然传统的FL训练单一的全局模型，表现在客户端的平均性能，但是跨客户端的统计数据异质性促使了个性化联邦学习（PFL）的发展，该方法可以在每个客户端的数据上训练个性化的模型，并具有良好的性能。PFL面临的一个关键挑战是如何在每个客户端具有来自复杂分布的数据且不能确定彼此的分布的情况下促进拥有相似数据的客户端进行更多协作。本文提出了一种基于多分支结构的新型PFL方法(pFedMB)，通过将神经网络的每个层分成多个分支并为每个分支分配客户端特定的权重来实现个性化。我们还设计了一种聚合方法，以提高pFedMB的通信效率并降低计算开销。通过对合成数据和真实世界数据的实验，我们证明了pFedMB在个性化和通信效率方面都能够达到最先进的性能。

    Federated learning (FL) is a decentralized machine learning technique that enables multiple clients to collaboratively train models without requiring clients to reveal their raw data to each other. Although traditional FL trains a single global model with average performance among clients, statistical data heterogeneity across clients has resulted in the development of personalized FL (PFL), which trains personalized models with good performance on each client's data. A key challenge with PFL is how to facilitate clients with similar data to collaborate more in a situation where each client has data from complex distribution and cannot determine one another's distribution. In this paper, we propose a new PFL method (pFedMB) using multi-branch architecture, which achieves personalization by splitting each layer of a neural network into multiple branches and assigning client-specific weights to each branch. We also design an aggregation method to improve the communication efficiency and 
    
[^134]: 基于Autoencoder和SINDy方法的参数化系统约简建模：周期解的延续

    Reduced order modeling of parametrized systems through autoencoders and SINDy approach: continuation of periodic solutions. (arXiv:2211.06786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.06786](http://arxiv.org/abs/2211.06786)

    本文提出了一种数据驱动、非侵入性的约简建模方法，利用自编码器神经网络和SINDy方法构建低维动力学模型，可以在保留动态特性的同时，高效地预测参数变化时的全时间解。

    

    对于由偏微分方程（PDE）控制的复杂现象进行高精度模拟通常需要精细干涉的方法，并且可能面临着昂贵的计算成本，当用于多种控制参数和初始条件的PDE稳态解的近似时，这种情况可能会变得禁止。因此，构建高效的约简模型（ROM），使其在参数变化时能够准确而快速地预测，同时保留物理现象的动态特性，这是至关重要的。本文提出了一种数据驱动、非侵入性的框架，将ROM构建与降维动力学识别相结合。该方法通过少量全序解，利用参数稀疏非线性动力学（SINDy）和自编码器神经网络构建低维动力学模型。该模型可用于高效计算新的全时间解。

    Highly accurate simulations of complex phenomena governed by partial differential equations (PDEs) typically require intrusive methods and entail expensive computational costs, which might become prohibitive when approximating steady-state solutions of PDEs for multiple combinations of control parameters and initial conditions. Therefore, constructing efficient reduced order models (ROMs) that enable accurate but fast predictions, while retaining the dynamical characteristics of the physical phenomenon as parameters vary, is of paramount importance. In this work, a data-driven, non-intrusive framework which combines ROM construction with reduced dynamics identification, is presented. Starting from a limited amount of full order solutions, the proposed approach leverages autoencoder neural networks with parametric sparse identification of nonlinear dynamics (SINDy) to construct a low-dimensional dynamical model. This model can be queried to efficiently compute full-time solutions at new
    
[^135]: 主动任务随机化：通过无监督生成多样和可行任务学习鲁棒技能。

    Active Task Randomization: Learning Robust Skills via Unsupervised Generation of Diverse and Feasible Tasks. (arXiv:2211.06134v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.06134](http://arxiv.org/abs/2211.06134)

    本文提出了一种称为主动任务随机化（ATR）的方法，通过无监督生成任务来学习鲁棒技能，该方法选择适合学习鲁棒技能的任务，通过平衡任务的多样性和可行性来预测任务多样性和可行性，并使用基于图的参数化程序生成任务，从而允许鲁棒地处理任务的各种情况，包括以前未见过的场景。

    

    解决现实世界中的操纵任务需要机器人具备适用于各种情况的技能库。使用基于学习的方法获得这种技能的关键挑战是获取覆盖任务的多样性和可行性的训练数据，这通常需要非常复杂的手动劳动和领域知识。在这项工作中，我们介绍了一种名为主动任务随机化（ATR）的方法，通过无监督生成训练任务来学习鲁棒的技能。 ATR通过平衡任务的多样性和可行性，选择适合学习鲁棒技能的任务，其中包括初始环境状态和操作目标。我们提出通过共同学习紧凑任务表示来预测任务多样性和可行性。然后使用基于图的参数化在模拟中程序生成所选的任务。这些训练任务的主动选择使得使用我们的框架训练的技能策略能够在不需要大量手动注释或领域知识的情况下，鲁棒地处理任务的各种情况，包括以前未见过的场景。

    Solving real-world manipulation tasks requires robots to have a repertoire of skills applicable to a wide range of circumstances. When using learning-based methods to acquire such skills, the key challenge is to obtain training data that covers diverse and feasible variations of the task, which often requires non-trivial manual labor and domain knowledge. In this work, we introduce Active Task Randomization (ATR), an approach that learns robust skills through the unsupervised generation of training tasks. ATR selects suitable tasks, which consist of an initial environment state and manipulation goal, for learning robust skills by balancing the diversity and feasibility of the tasks. We propose to predict task diversity and feasibility by jointly learning a compact task representation. The selected tasks are then procedurally generated in simulation using graph-based parameterization. The active selection of these training tasks enables skill policies trained with our framework to robus
    
[^136]: FedTP: 联邦学习中的Transformer个性化

    FedTP: Federated Learning by Transformer Personalization. (arXiv:2211.01572v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01572](http://arxiv.org/abs/2211.01572)

    本文研究发现联邦平均算法对Transformer模型中的自注意力存在负面影响，限制了联邦学习的能力。为此提出了FedTP，在学习客户端个性化自注意力的同时，将其他参数聚合在客户端之间。

    

    联邦学习是一种新兴的学习范式，多个客户端协作地进行隐私保护的机器学习模型训练。个性化联邦学习通过学习个性化模型，克服客户端之间的异质性。最近，已经有一些尝试将Transformer应用于联邦学习。然而，联邦学习算法对自注意力的影响尚未被研究。本文研究了这种关系，并揭示了在存在数据异质性时，联邦平均算法实际上会对自注意力产生负面影响。这些影响限制了Transformer模型在联邦学习环境中的能力。基于此，我们提出了FedTP，一种新的基于Transformer的联邦学习框架，它为每个客户端学习个性化的自注意力，同时将其他参数聚合在客户端之间而不是使用纯个性化机制。

    Federated learning is an emerging learning paradigm where multiple clients collaboratively train a machine learning model in a privacy-preserving manner. Personalized federated learning extends this paradigm to overcome heterogeneity across clients by learning personalized models. Recently, there have been some initial attempts to apply Transformers to federated learning. However, the impacts of federated learning algorithms on self-attention have not yet been studied. This paper investigates this relationship and reveals that federated averaging algorithms actually have a negative impact on self-attention where there is data heterogeneity. These impacts limit the capabilities of the Transformer model in federated learning settings. Based on this, we propose FedTP, a novel Transformer-based federated learning framework that learns personalized self-attention for each client while aggregating the other parameters among the clients. Instead of using a vanilla personalization mechanism th
    
[^137]: 用于基于仿真推断的非归一化模型的最大似然学习

    Maximum Likelihood Learning of Unnormalized Models for Simulation-Based Inference. (arXiv:2210.14756v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.14756](http://arxiv.org/abs/2210.14756)

    该论文提出了两种用于基于仿真推断的合成似然方法，使用高保真度模拟器生成模拟数据，学习条件能量模型(EBM)的 likelihood，结合先验估计后验分布，可以使用MCMC抽取样本，该方法相较于其他方法更加灵活和准确。

    

    我们引入了两种用于基于仿真推断（SBI）的合成似然方法，可以在高保真度模拟器存在时从实验观测中进行分摊或有针对性的推断。两种方法均使用从提议分布中抽取的参数所生成的模拟数据来学习 likelihood 的条件能量模型(EBM)。然后可以将学习到的 likelihood 与任何先验组合以获得后验估计，随后可以使用 MCMC 从中抽取样本。与其他合成似然方法不同，我们的方法独特地结合了灵活的能量模型和 KL 损失的最小化。

    We introduce two synthetic likelihood methods for Simulation-Based Inference (SBI), to conduct either amortized or targeted inference from experimental observations when a high-fidelity simulator is available. Both methods learn a conditional energy-based model (EBM) of the likelihood using synthetic data generated by the simulator, conditioned on parameters drawn from a proposal distribution. The learned likelihood can then be combined with any prior to obtain a posterior estimate, from which samples can be drawn using MCMC. Our methods uniquely combine a flexible Energy-Based Model and the minimization of a KL loss: this is in contrast to other synthetic likelihood methods, which either rely on normalizing flows, or minimize score-based objectives; choices that come with known pitfalls. We demonstrate the properties of both methods on a range of synthetic datasets, and apply them to a neuroscience model of the pyloric network in the crab, where our method outperforms prior art for a 
    
[^138]: 网络信号和信息处理

    Networked Signal and Information Processing. (arXiv:2210.13767v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2210.13767](http://arxiv.org/abs/2210.13767)

    这篇文章回顾了网络信号和信息处理方面的重大进展，这种进展使得决策、优化、控制和学习等方面能够扩展到分布式智能体越来越普遍的环境中，而且通过合作和共享，网络智能体能够匹配云或联合解决方案的性能。

    

    该文章回顾了网络信号和信息处理中的重大进展，在过去25年中，这些进展使得决策制定、推理、优化、控制和学习能够扩展到分布式智能体越来越普遍的环境中。当这些智能体相互合作时，新的集体行为从局部决策和行动中出现。此外，理论和应用表明，通过合作和共享，网络智能体能够匹配云或联合解决方案的性能，同时提供提高隐私、增强韧性和节约资源的潜力。

    The article reviews significant advances in networked signal and information processing, which have enabled in the last 25 years extending decision making and inference, optimization, control, and learning to the increasingly ubiquitous environments of distributed agents. As these interacting agents cooperate, new collective behaviors emerge from local decisions and actions. Moreover, and significantly, theory and applications show that networked agents, through cooperation and sharing, are able to match the performance of cloud or federated solutions, while offering the potential for improved privacy, increasing resilience, and saving resources.
    
[^139]: XAI用于透明的风力涡轮机功率曲线模型

    XAI for transparent wind turbine power curve models. (arXiv:2210.12104v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.12104](http://arxiv.org/abs/2210.12104)

    本论文应用XAI方法揭示机器学习模型从大量风力涡轮机数据中学到的策略，并且呼吁在模型选择中更加突出地采用XAI方法，提出了一种利用解释进行根本原因分析的实用方法。

    

    准确的风力涡轮机功率曲线模型对于风能发电行业的规模扩大和实现其在全球能源转型中的拟议角色至关重要。虽然机器学习方法已经证明了相对于参数化、基于物理学的方法的显著优势，但它们经常因为不透明的“黑匣子”而受到批评，这阻碍了它们在实践中的应用。我们应用流行的可解释人工智能（XAI）方法——Shapley值以及最新的XAI回归模型发现，揭示了机器学习模型从操作风力涡轮机数据中学到的策略。我们的发现表明，由于专注于测试集表现，越来越大的模型结构的趋势可能导致物理上不可行的模型策略。因此，我们呼吁在模型选择中更加突出地采用XAI方法。此外，我们提出了一种实用的方法，利用解释进行根本原因分析，找出风力涡轮机工业中的问题。

    Accurate wind turbine power curve models, which translate ambient conditions into turbine power output, are crucial for wind energy to scale and fulfill its proposed role in the global energy transition. While machine learning (ML) methods have shown significant advantages over parametric, physics-informed approaches, they are often criticised for being opaque 'black boxes', which hinders their application in practice. We apply Shapley values, a popular explainable artificial intelligence (XAI) method, and the latest findings from XAI for regression models, to uncover the strategies ML models have learned from operational wind turbine data. Our findings reveal that the trend towards ever larger model architectures, driven by a focus on test set performance, can result in physically implausible model strategies. Therefore, we call for a more prominent role of XAI methods in model selection. Moreover, we propose a practical approach to utilize explanations for root cause analysis in the 
    
[^140]: 基于自监督视觉Transformer和弱标签的组织病理图像分类

    Histopathological Image Classification based on Self-Supervised Vision Transformer and Weak Labels. (arXiv:2210.09021v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2210.09021](http://arxiv.org/abs/2210.09021)

    本研究提出了一种新方法Self-ViT-MIL，基于幻灯片级别的注释对癌细胞区域进行分类和定位，消除了需要像素级别注释的训练数据的需求。

    

    整个切片图像（WSI）分析是促进组织样本癌症诊断的强大方法。自动化诊断存在多种问题，最主要的问题是由于巨大的图像分辨率和有限的注释而引起的。WSI通常展示100Kx100K像素的分辨率。在像素级别上注释WSI中的癌细胞区域是费力且需要高水平的专业知识的。多实例学习（MIL）减轻了昂贵的像素级别注释的需求。在MIL中，学习是基于幻灯片级别的标签展开的，其中病理学家提供关于幻灯片中是否包含癌细胞组织的信息。我们提出了一种新的方法Self-ViT-MIL，它基于幻灯片级别的注释对癌细胞区域进行分类和定位，消除了需要像素级别注释的训练数据。Self-ViT-MIL在无需依靠标签的自监督设置下进行预训练，以学习丰富的特征表示。

    Whole Slide Image (WSI) analysis is a powerful method to facilitate the diagnosis of cancer in tissue samples. Automating this diagnosis poses various issues, most notably caused by the immense image resolution and limited annotations. WSIs commonly exhibit resolutions of 100Kx100K pixels. Annotating cancerous areas in WSIs on the pixel level is prohibitively labor-intensive and requires a high level of expert knowledge. Multiple instance learning (MIL) alleviates the need for expensive pixel-level annotations. In MIL, learning is performed on slide-level labels, in which a pathologist provides information about whether a slide includes cancerous tissue. Here, we propose Self-ViT-MIL, a novel approach for classifying and localizing cancerous areas based on slide-level annotations, eliminating the need for pixel-wise annotated training data. Self-ViT- MIL is pre-trained in a self-supervised setting to learn rich feature representation without relying on any labels. The recent Vision Tra
    
[^141]: 损失熵机会下的泛化：利用广泛的离线数据学习视觉运动任务。

    Generalization with Lossy Affordances: Leveraging Broad Offline Data for Learning Visuomotor Tasks. (arXiv:2210.06601v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2210.06601](http://arxiv.org/abs/2210.06601)

    本文提出了一个利用有损表示空间下的子目标指导在线微调的框架，它可以从广泛的数据中学习而来的有损表示来规划一系列子目标，分解原始任务，并强调任务相关信息，从而降低泛化过程中冗余内容的干扰，以此应对如何利用多样化的多任务数据进行新领域下游任务的学习的挑战。

    

    广泛数据集的利用已被证明对于各个领域的泛化都至关重要。然而如何有效利用多样化的多任务数据进行新领域下游任务的学习仍然是机器人学中的一大挑战。为了解决这个问题，本文提出了一个框架，它通过离线强化学习在广泛的数据上获取未见过的时间延迟任务的目标条件策略，同时结合学习到的有损失表示空间下的子目标指导在线微调。当面对新的任务目标时，该框架使用机会模型来规划一系列有损表示作为子目标，将原始任务分解成更容易的问题。这些有损表示从广泛的数据中学习而来，它们强调有关状态和目标的任务相关信息，同时抽象出妨碍泛化的冗余内容。因此，它可以为未见过的任务提供子目标规划，为策略提供紧凑的输入，也为奖励提供了较好的抽象。

    The utilization of broad datasets has proven to be crucial for generalization for a wide range of fields. However, how to effectively make use of diverse multi-task data for novel downstream tasks still remains a grand challenge in robotics. To tackle this challenge, we introduce a framework that acquires goal-conditioned policies for unseen temporally extended tasks via offline reinforcement learning on broad data, in combination with online fine-tuning guided by subgoals in learned lossy representation space. When faced with a novel task goal, the framework uses an affordance model to plan a sequence of lossy representations as subgoals that decomposes the original task into easier problems. Learned from the broad data, the lossy representation emphasizes task-relevant information about states and goals while abstracting away redundant contexts that hinder generalization. It thus enables subgoal planning for unseen tasks, provides a compact input to the policy, and facilitates reward
    
[^142]: 通过PatentsView.org学到的经验：评估实体消解算法的性能

    Estimating the Performance of Entity Resolution Algorithms: Lessons Learned Through PatentsView.org. (arXiv:2210.01230v2 [cs.DL] UPDATED)

    [http://arxiv.org/abs/2210.01230](http://arxiv.org/abs/2210.01230)

    本文介绍了一种新的实体消解算法评估方法，它能够为PatentsView的用户提供可靠的数据和比较不同消解算法的结果。

    

    本文介绍了一种新颖的评估实体消解算法的方法。该方法是基于PatentsView.org，一个美国专利和商标办公室的专利数据探测工具，使用实体消解算法来区分专利发明人的。我们提供了一个数据收集方法和特定的性能评估器，考虑到采样偏差。我们的方法简单、实用和原则性的，这些是使我们能够描绘PatentsView的实体消解性能的第一个代表性图像的关键特征。这种方法被用来告知PatentsView的用户数据的可靠性并允许比较竞争的消解算法。

    This paper introduces a novel evaluation methodology for entity resolution algorithms. It is motivated by PatentsView.org, a U.S. Patents and Trademarks Office patent data exploration tool that disambiguates patent inventors using an entity resolution algorithm. We provide a data collection methodology and tailored performance estimators that account for sampling biases. Our approach is simple, practical and principled -- key characteristics that allow us to paint the first representative picture of PatentsView's disambiguation performance. This approach is used to inform PatentsView's users of the reliability of the data and to allow the comparison of competing disambiguation algorithms.
    
[^143]: 针对类别特征的梯度估计随机梯度下降

    Stochastic gradient descent with gradient estimator for categorical features. (arXiv:2209.03771v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.03771](http://arxiv.org/abs/2209.03771)

    本文介绍了针对类别特征的梯度估计随机梯度下降算法，并展示了在多个数据集和模型架构上的高效性能以及提供了一个真实的匿名化零售数据集。

    

    类别数据出现在诸如健康和供应链等关键领域，这些数据需要特殊处理。为了将最近的机器学习模型应用于此类数据，需要进行编码。为了构建可解释的模型，独热编码仍然是一个非常好的选择，但此类编码会产生稀疏数据。梯度估计器不适用于稀疏数据：梯度主要被认为是零，而实际上并不总是存在，因此引入了一种新的梯度估计器。我们展示了该估计器在理论上最小化的内容，并在不同数据集和多个模型架构上展示了它的效率。在类似设置下，这种新的估计器表现比常见估计器更好。在匿名化后，我们还公开了一个真实的零售数据集。总体而言，本文旨在深入考虑类别数据并使模型和优化器适应这些关键特征。

    Categorical data are present in key areas such as health or supply chain, and this data require specific treatment. In order to apply recent machine learning models on such data, encoding is needed. In order to build interpretable models, one-hot encoding is still a very good solution, but such encoding creates sparse data. Gradient estimators are not suited for sparse data: the gradient is mainly considered as zero while it simply does not always exists, thus a novel gradient estimator is introduced. We show what this estimator minimizes in theory and show its efficiency on different datasets with multiple model architectures. This new estimator performs better than common estimators under similar settings. A real world retail dataset is also released after anonymization. Overall, the aim of this paper is to thoroughly consider categorical data and adapt models and optimizers to these key features.
    
[^144]: 一种利用在线和离线深度强化学习的维护计划框架

    A Maintenance Planning Framework using Online and Offline Deep Reinforcement Learning. (arXiv:2208.00808v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.00808](http://arxiv.org/abs/2208.00808)

    本论文提出了一种利用在线和离线深度强化学习的维护计划框架，以确定最优恢复策略，经过实验证明该框架比标准的预防性、纠正性和贪婪式计划方案有所改进。

    

    成本效益的资产管理是各个行业关注的领域。本文针对不断恶化的水管开发了一种深度强化学习（DRL）解决方案，用于自动确定最优恢复策略。我们采用在线和离线DRL设置来解决维修计划问题。在在线DRL中，智能体与具有不同长度、材料和失效率特征的多个水管的模拟环境进行交互。我们使用深度Q学习（DQN）对智能体进行训练，以学习具有最小平均成本和降低失效概率的最优策略。在离线学习中，智能体使用静态数据（如DQN重放数据）通过保守的Q学习算法学习最优策略，无需与环境进行进一步交互。我们证明了基于DRL的策略比标准的预防性、纠正性和贪婪式计划方案有所改进。此外，我们还能够从固定的DQN重放数据中学习知识。

    Cost-effective asset management is an area of interest across several industries. Specifically, this paper develops a deep reinforcement learning (DRL) solution to automatically determine an optimal rehabilitation policy for continuously deteriorating water pipes. We approach the problem of rehabilitation planning in an online and offline DRL setting. In online DRL, the agent interacts with a simulated environment of multiple pipes with distinct lengths, materials, and failure rate characteristics. We train the agent using deep Q-learning (DQN) to learn an optimal policy with minimal average costs and reduced failure probability. In offline learning, the agent uses static data, e.g., DQN replay data, to learn an optimal policy via a conservative Q-learning algorithm without further interactions with the environment. We demonstrate that DRL-based policies improve over standard preventive, corrective, and greedy planning alternatives. Additionally, learning from the fixed DQN replay data
    
[^145]: 看见机器人看不到的东西：学习协作感知进行视觉导航

    See What the Robot Can't See: Learning Cooperative Perception for Visual Navigation. (arXiv:2208.00759v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2208.00759](http://arxiv.org/abs/2208.00759)

    使用图神经网络架构的邻域特征聚合模块实现了所有传感器间的通讯，解决了机器人视觉导航中缺乏全局定位信息的问题并实现了高效导航。

    

    本文讨论了如何在一个未知环境中利用视觉传感器引导机器人到达目的地的问题。在缺乏全局定位信息的条件下，我们通过训练传感器编码和传递相关的视角信息给机器人，让机器人能够在使用第一视角图像的情况下尽可能高效地导航到目标。通过实现一个基于邻域的特征聚合模块，我们使用图神经网络（GNN）架构克服了让所有传感器甚至无法直接看到目标的传感器预测通向目标最短路方向的挑战。

    We consider the problem of navigating a mobile robot towards a target in an unknown environment that is endowed with visual sensors, where neither the robot nor the sensors have access to global positioning information and only use first-person-view images. In order to overcome the need for positioning, we train the sensors to encode and communicate relevant viewpoint information to the mobile robot, whose objective it is to use this information to navigate as efficiently as possible to the target. We overcome the challenge of enabling all the sensors (even those that cannot directly see the target) to predict the direction along the shortest path to the target by implementing a neighborhood-based feature aggregation module using a Graph Neural Network (GNN) architecture. In our experiments, we first demonstrate generalizability to previously unseen environments with various sensor layouts. Our results show that by using communication between the sensors and the robot, we achieve up to
    
[^146]: 学习可微分求解器以处理带硬约束的系统

    Learning differentiable solvers for systems with hard constraints. (arXiv:2207.08675v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.08675](http://arxiv.org/abs/2207.08675)

    该论文提出了一种学习可微分求解器的方法，可以以高度准确的方式满足部分微分方程约束，并且在推断时可以提供连续的解来满足所需的物理约束。该方法能够显著降低测试误差。

    

    我们引入了一种实用的方法，可以以高度准确的方式满足部分微分方程（PDE）约束，以及达到所需的容差。我们开发了一种可微分的PDE受约束层，可以融入任何神经网络结构中。我们的方法利用了可微分优化和隐式函数定理，有效地实现物理约束。受到字典学习的启发，我们的模型学习了一个函数族，其中每个函数都定义了从PDE参数到PDE解的映射。在推断时，模型通过求解PDE受约束的优化问题，找到学习到的函数族中的最优线性组合。我们的方法在感兴趣的域上提供了连续的解，准确地满足所需的物理约束。我们的结果表明，将硬约束直接融入神经网络结构，与在无约束目标函数上训练相比可以实现更低的测试误差。

    We introduce a practical method to enforce partial differential equation (PDE) constraints for functions defined by neural networks (NNs), with a high degree of accuracy and up to a desired tolerance. We develop a differentiable PDE-constrained layer that can be incorporated into any NN architecture. Our method leverages differentiable optimization and the implicit function theorem to effectively enforce physical constraints. Inspired by dictionary learning, our model learns a family of functions, each of which defines a mapping from PDE parameters to PDE solutions. At inference time, the model finds an optimal linear combination of the functions in the learned family by solving a PDE-constrained optimization problem. Our method provides continuous solutions over the domain of interest that accurately satisfy desired physical constraints. Our results show that incorporating hard constraints directly into the NN architecture achieves much lower test error when compared to training on an
    
[^147]: 连续时间下的q-Learning

    q-Learning in Continuous Time. (arXiv:2207.00713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.00713](http://arxiv.org/abs/2207.00713)

    本文研究了连续时间下的q-Learning，通过引入小q函数作为一阶近似，研究了q-learning理论，应用于设计不同的演员-评论家算法。

    

    我们研究了基于熵正则化的探索性扩散过程的Q-learning在连续时间下的应用。我们引入了“小q函数”作为大Q函数的一阶近似，研究了q函数的q-learning理论，并应用于设计不同的演员-评论家算法。

    We study the continuous-time counterpart of Q-learning for reinforcement learning (RL) under the entropy-regularized, exploratory diffusion process formulation introduced by Wang et al. (2020). As the conventional (big) Q-function collapses in continuous time, we consider its first-order approximation and coin the term ``(little) q-function". This function is related to the instantaneous advantage rate function as well as the Hamiltonian. We develop a ``q-learning" theory around the q-function that is independent of time discretization. Given a stochastic policy, we jointly characterize the associated q-function and value function by martingale conditions of certain stochastic processes, in both on-policy and off-policy settings. We then apply the theory to devise different actor-critic algorithms for solving underlying RL problems, depending on whether or not the density function of the Gibbs measure generated from the q-function can be computed explicitly. One of our algorithms inter
    
[^148]: HyGNN: 基于超图神经网络的药物相互作用预测

    HyGNN: Drug-Drug Interaction Prediction via Hypergraph Neural Network. (arXiv:2206.12747v4 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2206.12747](http://arxiv.org/abs/2206.12747)

    本研究提出了基于超图注意力神经网络的药物相互作用预测模型HyGNN，可以基于药物的SMILES字符串进行预测，并在多个基准数据集上的表现优于其他最先进的方法。

    

    药物相互作用(DDIs)可能会影响药物功能，在最糟糕的情况下可能会导致不良药物反应(ADRs)。预测所有的药物相互作用是一个具有挑战性和关键性的问题。本文提出了一种新颖的基于超图神经网络(HyGNN)模型，仅基于药物的SMILES字符串，为药物相互作用预测问题提供了新的解决方案。为了捕捉药物之间的相似性，我们从SMILES字符串中提取药物的化学亚结构创建超图。然后，我们开发了HyGNN，包括一种新颖的基于关注度的超图边编码器，以获取药物的超边表示，以及一个解码器，以预测药物对之间的相互作用。此外，我们引入了一种新颖的超图注意机制，以自适应地学习亚结构和超边的重要性。实验结果表明，在三个基准数据集上，HyGNN的性能优于几种最先进的方法。

    Drug-Drug Interactions (DDIs) may hamper the functionalities of drugs, and in the worst scenario, they may lead to adverse drug reactions (ADRs). Predicting all DDIs is a challenging and critical problem. Most existing computational models integrate drug-centric information from different sources and leverage them as features in machine learning classifiers to predict DDIs. However, these models have a high chance of failure, especially for the new drugs when all the information is not available. This paper proposes a novel Hypergraph Neural Network (HyGNN) model based on only the SMILES string of drugs, available for any drug, for the DDI prediction problem. To capture the drug similarities, we create a hypergraph from drugs' chemical substructures extracted from the SMILES strings. Then, we develop HyGNN consisting of a novel attention-based hypergraph edge encoder to get the representation of drugs as hyperedges and a decoder to predict the interactions between drug pairs. Furthermo
    
[^149]: 学习值函数的鲁棒损失函数

    Robust Losses for Learning Value Functions. (arXiv:2205.08464v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.08464](http://arxiv.org/abs/2205.08464)

    本文提出了基于鲁棒损失的学习值函数的方法，使用了鞍点优化问题的见解，并在在线脱机预测和控制设置中，推导了稳健的基于梯度的方法。

    

    强化学习中大多数值函数学习算法基于均方（投影）贝尔曼误差。然而，已知二次误差对离群值很敏感，不仅会偏移目标的解决方案，还会导致具有高振幅和高方差的梯度。为了控制这些高振幅更新，RL中的典型策略包括剪切梯度、剪切奖励、重新缩放奖励或剪切误差。虽然这些策略似乎与Huber loss等鲁棒损失有关，但它们建立在半梯度更新规则上，不会最小化已知损失。在本文中，我们基于最近对将平方贝尔曼误差重构为鞍点优化问题的见解，提出了Huber贝尔曼误差和绝对贝尔曼误差的鞍点重构。我们从鲁棒损失的形式化开始，然后推导出稳健的基于梯度的方法，以在在线脱机预测和控制设置中最小化这些损失。

    Most value function learning algorithms in reinforcement learning are based on the mean squared (projected) Bellman error. However, squared errors are known to be sensitive to outliers, both skewing the solution of the objective and resulting in high-magnitude and high-variance gradients. To control these high-magnitude updates, typical strategies in RL involve clipping gradients, clipping rewards, rescaling rewards, or clipping errors. While these strategies appear to be related to robust losses -- like the Huber loss -- they are built on semi-gradient update rules which do not minimize a known loss. In this work, we build on recent insights reformulating squared Bellman errors as a saddlepoint optimization problem and propose a saddlepoint reformulation for a Huber Bellman error and Absolute Bellman error. We start from a formalization of robust losses, then derive sound gradient-based approaches to minimize these losses in both the online off-policy prediction and control settings. 
    
[^150]: 计划到实践：在潜空间中组合目标的高效在线微调

    Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space. (arXiv:2205.08129v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2205.08129](http://arxiv.org/abs/2205.08129)

    本文提出了一种名为PTP的方法，利用高层规划器和潜空间中的条件生成器来分解目标成子目标，并在以前的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，从而训练目标导向的策略，有效提高了机器人长期目标导向的经验采集效率。

    

    通用型机器人需要多样化的行为来完成现实世界中无结构环境下的挑战性任务。为了解决这个问题，目标导向强化学习旨在获得可以在命令下达时到达可配置目标的策略，完成广泛的任务。然而，这类目标导向策略非常难以从头开始训练。本文提出了Planning to Practice（PTP），一种实现训练目标导向策略的方法，用于解决需要多个不同类型交互才能解决的长时程任务。我们的方法基于两个重要的思想。首先，我们分层地分解了到达目标的问题，使用条件子目标生成器在低级无模型策略中的潜空间中设置中间子目标的高级计划器。其次，我们提出了一种混合方法，即首先在以前收集的数据上预训练条件子目标生成器和策略，然后在线微调以适应新的目标，以提高收敛速度。实验表明，PTP可以同时提高数据效率和样本效率，使得机器人可以很高效地采集长期目标导向的经验。

    General-purpose robots require diverse repertoires of behaviors to complete challenging tasks in real-world unstructured environments. To address this issue, goal-conditioned reinforcement learning aims to acquire policies that can reach configurable goals for a wide range of tasks on command. However, such goal-conditioned policies are notoriously difficult and time-consuming to train from scratch. In this paper, we propose Planning to Practice (PTP), a method that makes it practical to train goal-conditioned policies for long-horizon tasks that require multiple distinct types of interactions to solve. Our approach is based on two key ideas. First, we decompose the goal-reaching problem hierarchically, with a high-level planner that sets intermediate subgoals using conditional subgoal generators in the latent space for a low-level model-free policy. Second, we propose a hybrid approach which first pre-trains both the conditional subgoal generator and the policy on previously collected
    
[^151]: 现代生物统计中的强化学习：构建最优自适应干预

    Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions. (arXiv:2203.02605v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.02605](http://arxiv.org/abs/2203.02605)

    本文是关于将强化学习应用于自适应干预中的第一份统一调查，强化学习在动态治疗方案和移动健康中即时自适应干预这两个领域中都具有很大的应用潜力。在这两个领域之间存在相似和不同之处需要考虑，并且这里存在巨大的合作机会。

    

    近年来，强化学习（RL）在与健康相关的序列性决策中占据了重要地位，成为交付自适应干预（AIs）的越来越流行的工具。然而，尽管具有潜在优势，但其现实应用仍然受到限制，部分是由于方法论和应用社区之间的协同不足。在这项工作中，我们提供了关于学习AIs的RL方法的第一份统一调查，利用RL的通用方法论伞来桥接动态治疗方案和移动健康中即时自适应干预这两个AI领域。我们概述了这两个AI领域之间的异同，并讨论了它们对使用RL的影响。最后，我们利用自己在两个领域中设计案例研究的经验，说明了在AIs领域中，统计学、RL和医疗研究人员之间的巨大合作机会。

    In recent years, reinforcement learning (RL) has acquired a prominent position in the space of health-related sequential decision-making, becoming an increasingly popular tool for delivering adaptive interventions (AIs). However, despite potential benefits, its real-life application is still limited, partly due to a poor synergy between the methodological and the applied communities. In this work, we provide the first unified survey on RL methods for learning AIs, using the common methodological umbrella of RL to bridge the two AI areas of dynamic treatment regimes and just-in-time adaptive interventions in mobile health. We outline similarities and differences between these two AI domains and discuss their implications for using RL. Finally, we leverage our experience in designing case studies in both areas to illustrate the tremendous collaboration opportunities between statistical, RL, and healthcare researchers in the space of AIs.
    
[^152]: 缺失治疗信息的条件平均处理效应估计

    Estimating Conditional Average Treatment Effects with Missing Treatment Information. (arXiv:2203.01422v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.01422](http://arxiv.org/abs/2203.01422)

    本文研究了条件平均处理效应 (CATE) 的估计问题，在缺失治疗信息的情况下，提出了缺失治疗表示网络 (MTRNet)，通过域自适应学习协变量的平衡表示来解决协变量转移问题。

    

    估计条件平均处理效应 (CATE) 是具有挑战性的，特别是在治疗信息缺失的情况下。尽管在实践中这是一个普遍存在的问题，但缺失治疗的 CATE 估计却受到了很少关注。在本文中，我们分析了缺失治疗情况下的 CATE 估计，在这种情况下存在独特的挑战，例如协变量偏移。我们认定了我们的情况中存在两种协变量转移：(i) 治疗组和对照组之间的协变量转移，以及(ii) 观察到的治疗组和缺失治疗组之间的协变量转移。我们首先从理论上证明了这些协变量转移的影响，通过为我们的缺失治疗情况下的 CATE 估计导出一个泛化界限。然后，受到我们的界限的启发，我们开发了缺失治疗表示网络 (MTRNet)，这是一种新颖的 CATE 估计算法，它使用域自适应学习协变量的平衡表示。通过使用平衡的表示方法，MTRNet提供了

    Estimating conditional average treatment effects (CATE) is challenging, especially when treatment information is missing. Although this is a widespread problem in practice, CATE estimation with missing treatments has received little attention. In this paper, we analyze CATE estimation in the setting with missing treatments where unique challenges arise in the form of covariate shifts. We identify two covariate shifts in our setting: (i) a covariate shift between the treated and control population; and (ii) a covariate shift between the observed and missing treatment population. We first theoretically show the effect of these covariate shifts by deriving a generalization bound for estimating CATE in our setting with missing treatments. Then, motivated by our bound, we develop the missing treatment representation network (MTRNet), a novel CATE estimation algorithm that learns a balanced representation of covariates using domain adaptation. By using balanced representations, MTRNet provid
    
[^153]: 低资源情境下的知识抽取：调研与展望

    Knowledge Extraction in Low-Resource Scenarios: Survey and Perspective. (arXiv:2202.08063v4 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2202.08063](http://arxiv.org/abs/2202.08063)

    低资源情境下，如何让知识抽取更好地从非结构化文本中提取信息？本文调研了三种解决范式：高资源数据、更强的模型和数据与模型的结合，提出了未来的研究方向。

    

    知识抽取（KE）旨在从非结构化文本中提取结构信息，通常遭受数据匮乏和出现未见类型（低资源情境）的困扰。许多神经网络方法已广泛研究并取得了令人瞩目的表现。本文对低资源情境下KE进行文献综述，并将现有的工作系统性地分为三种范式：（1）利用高资源数据，（2）利用更强的模型，（3）同时利用数据和模型。此外，本文提出有前途的应用，并概述了未来研究的一些潜在方向。我们希望我们的调研可以帮助学术和工业界更好地理解这一领域，激发更多的创意，提升更广泛的应用。

    Knowledge Extraction (KE), aiming to extract structural information from unstructured texts, often suffers from data scarcity and emerging unseen types, i.e., low-resource scenarios. Many neural approaches to low-resource KE have been widely investigated and achieved impressive performance. In this paper, we present a literature review towards KE in low-resource scenarios, and systematically categorize existing works into three paradigms: (1) exploiting higher-resource data, (2) exploiting stronger models, and (3) exploiting data and models together. In addition, we highlight promising applications and outline some potential directions for future research. We hope that our survey can help both the academic and industrial communities to better understand this field, inspire more ideas, and boost broader applications.
    
[^154]: 为基于样例的增量学习重新平衡批规范化

    Rebalancing Batch Normalization for Exemplar-based Class-Incremental Learning. (arXiv:2201.12559v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2201.12559](http://arxiv.org/abs/2201.12559)

    本文提出了一种重新平衡的策略解决了批规范化在基于样例的类增量学习中出现的问题，实验结果表明，该算法在不同的数据集和模型上都取得了更好的表现。

    

    批规范化及其变种已经在各种计算机视觉任务的神经网络上进行了广泛研究，但相对较少的工作专门研究了BN在连续学习中的效果。为此，我们针对基于样例的类增量学习（CIL）开发了一种新的BN更新方法。BN在CIL中的主要问题是当前任务和过去任务之间训练数据的不平衡，这使得BN的经验均值和方差以及可学习仿射变换参数在当前任务中严重偏置，从而导致忘记过去的任务。虽然最近有一种BN变种是为“在线”CIL开发的，其中训练是在单个时期内完成的，但我们表明他们的方法并不一定对“离线”CIL带来收益，其中在不平衡的训练数据上多次对模型进行了培训。他们的方法无效的主要原因在于其对特定任务的偏置，而我们所提出的算法则是通过采用重新平衡的策略解决这个问题，使得BN可以更好地适应增量学习场景。我们的实验结果表明，所提出的算法在不同的数据集和模型上都取得了更优秀的表现，能够更好地保持模型的整体性能和学习效果。

    Batch Normalization (BN) and its variants has been extensively studied for neural nets in various computer vision tasks, but relatively little work has been dedicated to studying the effect of BN in continual learning. To that end, we develop a new update patch for BN, particularly tailored for the exemplar-based class-incremental learning (CIL). The main issue of BN in CIL is the imbalance of training data between current and past tasks in a mini-batch, which makes the empirical mean and variance as well as the learnable affine transformation parameters of BN heavily biased toward the current task -contributing to the forgetting of past tasks. While one of the recent BN variants has been developed for "online" CIL, in which the training is done with a single epoch, we show that their method does not necessarily bring gains for "offline" CIL, in which a model is trained with multiple epochs on the imbalanced training data. The main reason for the ineffectiveness of their method lies 
    
[^155]: 无线网络能量感知功率分配的基于图算法的展开

    Graph-based Algorithm Unfolding for Energy-aware Power Allocation in Wireless Networks. (arXiv:2201.11799v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2201.11799](http://arxiv.org/abs/2201.11799)

    该论文提出了一种基于图的算法用于无线通信网络功率分配的加权能效比最大化，该算法采用模块化结构和图卷积神经网络，可以解决问题的非凸性，并展示了置换等变性。

    

    我们开发了一种新颖的基于图算法的可训练框架，用于最大化无线通信网络功率分配的加权能效比（WSEE）。为了解决问题的非凸性，该方法由受经典迭代次优方法启发的模块化结构组成，并增强了学习组件。更具体地，我们提出了一种连续凸逼近（SCA）方法的深层展开。在我们展开的SCA（USCA）框架中，原来预设的参数现在通过直接利用多用户信道状态信息作为底层图邻接矩阵的图卷积神经网络（GCNs）进行学习。我们展示了所提建筑的置换等变性，这是一个应用于无线网络数据的理想属性。通过渐进训练策略使用随机梯度下降方法训练USCA框架。无监督损失被精心设计为考虑多个位置的相关性。

    We develop a novel graph-based trainable framework to maximize the weighted sum energy efficiency (WSEE) for power allocation in wireless communication networks. To address the non-convex nature of the problem, the proposed method consists of modular structures inspired by a classical iterative suboptimal approach and enhanced with learnable components. More precisely, we propose a deep unfolding of the successive concave approximation (SCA) method. In our unfolded SCA (USCA) framework, the originally preset parameters are now learnable via graph convolutional neural networks (GCNs) that directly exploit multi-user channel state information as the underlying graph adjacency matrix. We show the permutation equivariance of the proposed architecture, which is a desirable property for models applied to wireless network data. The USCA framework is trained through a stochastic gradient descent approach using a progressive training strategy. The unsupervised loss is carefully devised to featu
    
[^156]: 基于医学概念的胎儿超声图像分类器的认知解释器

    A Cognitive Explainer for Fetal ultrasound images classifier Based on Medical Concepts. (arXiv:2201.07798v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2201.07798](http://arxiv.org/abs/2201.07798)

    本研究提出了一种基于关键医学概念的可解释框架，利用概念间的关系构建图卷积神经网络，解释胎儿超声图像分类器的决策过程，为临床医生提供易于理解的推理结果见解。

    

    在二维孕期检查中，胎儿标准扫描平面的检测是一项非常复杂的任务，需要广泛的医学知识和多年的培训。虽然深度神经网络(DNN)可以协助经验不足的医生完成这些任务，但它们的透明度和可解释性限制了它们的应用。本研究提出了一种基于关键医学概念的可解释框架，从临床医生的认知角度提供解释。此外，我们利用基于概念的图卷积神经网络(GCN)构建关键医学概念之间的关系。对一个私有数据集的广泛实验分析表明，所提出的方法为临床医生提供了易于理解的推理结果见解。

    Fetal standard scan plane detection during 2-D mid-pregnancy examinations is a highly complex task, which requires extensive medical knowledge and years of training. Although deep neural networks (DNN) can assist inexperienced operators in these tasks, their lack of transparency and interpretability limit their application. Despite some researchers have been committed to visualizing the decision process of DNN, most of them only focus on the pixel-level features and do not take into account the medical prior knowledge. In this work, we propose an interpretable framework based on key medical concepts, which provides explanations from the perspective of clinicians' cognition. Moreover, we utilize a concept-based graph convolutional neural(GCN) network to construct the relationships between key medical concepts. Extensive experimental analysis on a private dataset has shown that the proposed method provides easy-to-understand insights about reasoning results for clinicians.
    
[^157]: 使用较慢的在线网络实现更快的深度强化学习

    Faster Deep Reinforcement Learning with Slower Online Network. (arXiv:2112.05848v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.05848](http://arxiv.org/abs/2112.05848)

    本文改进了DQN和Rainbow两个深度强化学习算法，大大提高了它们在Atari游戏基准测试中的性能，我们的方法是在在线网络和目标网络之间引入一定的接近度，以提高深度强化学习的鲁棒性。

    

    深度强化学习算法通常使用两个网络进行价值函数优化：一个在线网络和一个目标网络，后者带有一定的延迟。使用两个独立的网络使得智能体能够对抗启发式引导时出现的问题。本文在两个流行的深度强化学习算法（即DQN和Rainbow）中引入了更新算法，以激励在线网络保持与目标网络的接近，从而提高在存在噪声更新时深度强化学习的鲁棒性。由此产生的代理称为DQN Pro和Rainbow Pro，它们在Atari基准测试上相对于原始算法表现出显著的性能提升，证明了这种深度强化学习中的简单思想的有效性。我们的论文代码可在Github.com/amazon-research/fast-rl-with-slow-updates 上获取。

    Deep reinforcement learning algorithms often use two networks for value function optimization: an online network, and a target network that tracks the online network with some delay. Using two separate networks enables the agent to hedge against issues that arise when performing bootstrapping. In this paper we endow two popular deep reinforcement learning algorithms, namely DQN and Rainbow, with updates that incentivize the online network to remain in the proximity of the target network. This improves the robustness of deep reinforcement learning in presence of noisy updates. The resultant agents, called DQN Pro and Rainbow Pro, exhibit significant performance improvements over their original counterparts on the Atari benchmark demonstrating the effectiveness of this simple idea in deep reinforcement learning. The code for our paper is available here: Github.com/amazon-research/fast-rl-with-slow-updates.
    
[^158]: 用图神经网络和随机森林预测具有大磁矩材料

    Prediction of Large Magnetic Moment Materials With Graph Neural Networks and Random Forests. (arXiv:2111.14712v4 [cond-mat.mtrl-sci] UPDATED)

    [http://arxiv.org/abs/2111.14712](http://arxiv.org/abs/2111.14712)

    本文使用图神经网络和随机森林预测出具有大磁矩的材料。我们的研究结果表明，这些机器学习方法可以高精度地预测大磁矩材料，这可以为从现有材料设计高性能磁铁以及发现新型大磁矩化合物提供可能。

    

    磁性材料是许多可推动生态过渡的技术的关键组成部分，包括电动机，风力发电机和磁性制冷系统。因此，发现具有大磁矩的材料成为了日益重要的任务。本文采用最先进的机器学习方法，扫描数十万个现有材料的无机晶体结构数据库（ICSD），以找到具有铁磁性和大磁矩的材料。采用材料计划数据库上的晶体图卷积神经网络（CGCNN），材料图网络（MEGNet）和随机森林进行训练，该数据库包含高通量DFT预测结果。对于随机森林，我们使用基于化学成分和晶体结构的近100个描述符来选择相关属性。这给出的结果与神经网络的结果相当。这些不同机器学习方法之间的比较表明，它们可以高精度地预测具有大磁矩的材料。我们的研究结果为使用现有材料设计高性能磁铁以及发现具有大磁矩的新化合物开辟了新的途径。

    Magnetic materials are crucial components of many technologies that could drive the ecological transition, including electric motors, wind turbine generators and magnetic refrigeration systems. Discovering materials with large magnetic moments is therefore an increasing priority. Here, using state-of-the-art machine learning methods, we scan the Inorganic Crystal Structure Database (ICSD) of hundreds of thousands of existing materials to find those that are ferromagnetic and have large magnetic moments. Crystal graph convolutional neural networks (CGCNN), materials graph network (MEGNet) and random forests are trained on the Materials Project database that contains the results of high-throughput DFT predictions. For random forests, we use a stochastic method to select nearly one hundred relevant descriptors based on chemical composition and crystal structure. This gives results that are comparable to those of neural networks. The comparison between these different machine learning appr
    
[^159]: 一种增加神经网络对数据质量问题的鲁棒性的调制层

    A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues. (arXiv:2107.08574v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.08574](http://arxiv.org/abs/2107.08574)

    提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，具备神经调制特征，通过一个额外的输入的函数替换了全连接层的固定权重，使得在测试中具有调制层的模型对于数据质量的降解更加鲁棒，同时也能够节省训练时间并且不会受到插补错误的影响。

    

    数据缺失和质量是机器学习中常见的问题，特别是在高风险应用领域，如医疗保健。开发者通常只使用高质量数据精心筛选出的数据集来训练机器学习模型；然而，这会降低这些模型在生产环境中的效用。本文提出了一种新颖的神经网络修正方法，用于缓解低质量和缺失数据的影响，其中利用一个额外的输入的函数替换了全连接层的固定权重。这受启发于生物神经网络中的神经调制，皮质可以根据输入的可靠性和其他数据的存在程度上下调节输入。在测试中，使用可靠性得分作为调制信号，发现具有调制层的模型对于数据质量的降解（包括额外的缺失数据）更加鲁棒。这些模型优于插补方法，因为它们通过完全跳过插补过程节省了训练时间，并且不会受到插补错误的影响。

    Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputatio
    
[^160]: 非线性系统识别的混合机器学习模型评估：疲劳试验架的案例研究。

    Assessment of hybrid machine learning models for non-linear system identification of fatigue test rigs. (arXiv:2107.03645v3 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2107.03645](http://arxiv.org/abs/2107.03645)

    本文开发了一种混合模型，利用长短期记忆网络结合线性频率响应函数模型进行非线性系统识别。该方法还可以应用于虚拟传感。通过对疲劳试验数据进行测试验证了该方法的效果。

    

    预测给定疲劳试验台驱动信号的系统响应是一项具有挑战性的任务，通常采用线性频率响应函数模型。为了考虑非线性现象，建议采用一种新颖的混合模型，通过增加长短期记忆（LSTM）网络对现有方法进行补充。该方法的附加虚拟传感应用得到了验证。使用来自伺服液压试验台的非线性实验数据进行测试，并公开了该数据集。在评估中采用了各种时间和频率域指标以及变幅下的疲劳强度指标。

    The prediction of system responses for a given fatigue test bench drive signal is a challenging task, for which linear frequency response function models are commonly used. To account for non-linear phenomena, a novel hybrid model is suggested, which augments existing approaches using Long Short-Term Memory networks. Additional virtual sensing applications of this method are demonstrated. The approach is tested using non-linear experimental data from a servo-hydraulic test rig and this dataset is made publicly available. A variety of metrics in time and frequency domains, as well as fatigue strength under variable amplitudes, are employed in the evaluation.
    
[^161]: 基于普适函数逼近的强化学习的在线子采样

    Online Sub-Sampling for Reinforcement Learning with General Function Approximation. (arXiv:2106.07203v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07203](http://arxiv.org/abs/2106.07203)

    本文提出了一种基于在线子采样框架的强化学习算法，利用数据点的信息增益量来指导探索，与现有方法相比更新RL算法的策略次数大大减少，但仍保持较小的近似最优遗憾边界。

    

    现有的大多数强化学习（RL）普适函数逼近（FA）方法都专注于理解统计复杂性或遗憾边界，但这些方法的计算复杂性远未得到理解——事实上，函数类上的简单优化问题可能同样难以处理。本文通过建立一种高效的在线子采样框架来解决这个问题，该框架测量RL算法收集的数据点的信息增益，并使用该测量指导探索。对于基于价值的方法和复杂度有界的函数类，我们证明了策略只需要更新$\propto\operatorname{poly}\log(K)$ 次，就可以运行 $K$ 次RL算法而仍然实现较小的近似最优遗憾边界。与现有方法更新策略至少要 $\Omega(K)$ 次相比，我们的方法大大减少了解决方案中的优化调用次数。

    Most of the existing works for reinforcement learning (RL) with general function approximation (FA) focus on understanding the statistical complexity or regret bounds. However, the computation complexity of such approaches is far from being understood -- indeed, a simple optimization problem over the function class might be as well intractable. In this paper, we tackle this problem by establishing an efficient online sub-sampling framework that measures the information gain of data points collected by an RL algorithm and uses the measurement to guide exploration. For a value-based method with complexity-bounded function class, we show that the policy only needs to be updated for $\propto\operatorname{poly}\log(K)$ times for running the RL algorithm for $K$ episodes while still achieving a small near-optimal regret bound. In contrast to existing approaches that update the policy for at least $\Omega(K)$ times, our approach drastically reduces the number of optimization calls in solving 
    
[^162]: 动态模态分解的内核视角

    The kernel perspective on dynamic mode decomposition. (arXiv:2106.00106v3 [math.FA] UPDATED)

    [http://arxiv.org/abs/2106.00106](http://arxiv.org/abs/2106.00106)

    本文重新审视了动态模态分解的理论假设，提出了一种新的只需在RKHS上定义稠密Koopman算子的DMD框架，并证明了高斯径向基核函数的RKHS只支持仿射动力学的有界Koopman算子。

    

    本文重新审视了关于Koopman算子动态模态分解（DMD）的理论假设，包括特征函数的格子存在性、Koopman算子之间的共同特征函数、Koopman算子的有界性和紧性。针对每个假设提供了说明其限制性的反例。特别地，本文证明了高斯径向基核函数的本地再生核希尔伯特空间（RKHS）只支持仿射动力学的有界Koopman算子。此外，引入了一种新的DMD框架，只需在RKHS上定义稠密Koopman算子即可，通过数值实例证明了其有效性。

    This manuscript revisits theoretical assumptions concerning dynamic mode decomposition (DMD) of Koopman operators, including the existence of lattices of eigenfunctions, common eigenfunctions between Koopman operators, and boundedness and compactness of Koopman operators. Counterexamples that illustrate restrictiveness of the assumptions are provided for each of the assumptions. In particular, this manuscript proves that the native reproducing kernel Hilbert space (RKHS) of the Gaussian RBF kernel function only supports bounded Koopman operators if the dynamics are affine. In addition, a new framework for DMD, that requires only densely defined Koopman operators over RKHSs is introduced, and its effectiveness is demonstrated through numerical examples.
    
[^163]: 对于均值场博弈的对抗逆强化学习

    Adversarial Inverse Reinforcement Learning for Mean Field Games. (arXiv:2104.14654v5 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2104.14654](http://arxiv.org/abs/2104.14654)

    本文提出了一种新的均值场对抗逆强化学习(MF-AIRL)框架，它能够处理展示行为中的不确定性，并在模拟任务上的实验结果表明其在奖励恢复方面优于现有方法。

    

    均值场博弈(MFGs)是利用均值场理论简化智能体之间相互作用的一种可数学处理的框架，用于建模大规模多智能体系统。通过从展示行为中恢复奖励信号，它使得应用逆强化学习(IRL)来预测大群体的行为变得可能。然而，现有的MFGs中的IRL方法无法推断出各个智能体展示行为的不确定性。本文提出了一种新的框架，均值场对抗逆强化学习(MF-AIRL)，它能够处理展示行为中的不确定性。我们构建MF-AIRL，基于最大熵IRL和一个新的均衡概念。我们在具有不完美演示的模拟任务上评估了我们的方法。实验结果表明，MF-AIRL在奖励恢复方面优于现有方法。

    Mean field games (MFGs) provide a mathematically tractable framework for modelling large-scale multi-agent systems by leveraging mean field theory to simplify interactions among agents. It enables applying inverse reinforcement learning (IRL) to predict behaviours of large populations by recovering reward signals from demonstrated behaviours. However, existing IRL methods for MFGs are powerless to reason about uncertainties in demonstrated behaviours of individual agents. This paper proposes a novel framework, Mean-Field Adversarial IRL (MF-AIRL), which is capable of tackling uncertainties in demonstrations. We build MF-AIRL upon maximum entropy IRL and a new equilibrium concept. We evaluate our approach on simulated tasks with imperfect demonstrations. Experimental results demonstrate the superiority of MF-AIRL over existing methods in reward recovery.
    
[^164]: 一种用于系统化感知、语法和语义的通用数据集的极简主义方法

    A Minimalist Dataset for Systematic Generalization of Perception, Syntax, and Semantics. (arXiv:2103.01403v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.01403](http://arxiv.org/abs/2103.01403)

    本文提出了一个新的数据集HINT，旨在检验机器学习通用概念的能力，包括感知、语法和语义三个层次。为了检验模型的插值和外推能力，我们设计了一个五倍交叉测试集。通过对几种最先进的模型进行广泛实验，进一步探究其局限性。

    

    受到人类掌握算术并将其推广到新问题的例外能力的启发，我们提出了一个新的数据集Handwritten arithmetic with INTegers（HINT）来检验机器学习通用概念的能力，包括感知、语法和语义三个层次。在HINT中，机器被赋予学习如何从原始信号（如图像）中感知概念（即感知），如何将多个概念结构化组合以形成有效表达式（即语法），以及如何实现概念以支持各种推理任务（即语义），全部在弱监督的情况下。我们专注于系统化通用能力，精心设计了一个五倍交叉测试集，以评估关于三层级别的学习概念的插值和外推。此外，我们还设计了一个少样本学习分割，以确定模型是否能够快速学习新概念并将其推广到更复杂的情况。为了了解现有模型的局限性，我们还对几种最先进的模型进行了广泛实验，并详细分析了它们在HINT上的表现。

    Inspired by humans' exceptional ability to master arithmetic and generalize to new problems, we present a new dataset, Handwritten arithmetic with INTegers (HINT), to examine machines' capability of learning generalizable concepts at three levels: perception, syntax, and semantics. In HINT, machines are tasked with learning how concepts are perceived from raw signals such as images (i.e., perception), how multiple concepts are structurally combined to form a valid expression (i.e., syntax), and how concepts are realized to afford various reasoning tasks (i.e., semantics), all in a weakly supervised manner. Focusing on systematic generalization, we carefully design a five-fold test set to evaluate both the interpolation and the extrapolation of learned concepts w.r.t. the three levels. Further, we design a few-shot learning split to determine whether or not models can rapidly learn new concepts and generalize them to more complex scenarios. To comprehend existing models' limitations, we
    
[^165]: Matern高斯过程在黎曼流形上的应用

    Mat\'ern Gaussian processes on Riemannian manifolds. (arXiv:2006.10160v6 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2006.10160](http://arxiv.org/abs/2006.10160)

    本文提出了一种新的方法，通过谱理论计算Riemannian Matérn高斯过程在紧黎曼流形上的核，使其可以通过标准的可扩展技术进行训练。这将推动Matérn高斯过程在黎曼流形上的应用。

    

    高斯过程是一种有效的模型类，特别是在精确表示预测不确定性很重要的情况下。受物理科学应用的启发，最近将广泛使用的Matérn高斯过程推广到模拟定义在黎曼流形上的函数，通过将这些过程重新表示为随机偏微分方程的解来实现。在本文中，我们提出了通过拉普拉斯-贝尔特拉米算子的谱理论在紧黎曼流形上计算这些过程的核心技术，从而使它们可以通过标准可扩展技术（例如诱导点方法）进行训练。我们还将该推广从Matérn推广到广泛使用的平方指数高斯过程。通过允许使用众所周知的技术对Riemannian Matérn高斯过程进行训练，我们的工作使它们能够得到广泛应用。

    Gaussian processes are an effective model class for learning unknown functions, particularly in settings where accurately representing predictive uncertainty is of key importance. Motivated by applications in the physical sciences, the widely-used Mat\'ern class of Gaussian processes has recently been generalized to model functions whose domains are Riemannian manifolds, by re-expressing said processes as solutions of stochastic partial differential equations. In this work, we propose techniques for computing the kernels of these processes on compact Riemannian manifolds via spectral theory of the Laplace-Beltrami operator in a fully constructive manner, thereby allowing them to be trained via standard scalable techniques such as inducing point methods. We also extend the generalization from the Mat\'ern to the widely-used squared exponential Gaussian process. By allowing Riemannian Mat\'ern Gaussian processes to be trained using well-understood techniques, our work enables their use i
    
[^166]: 快速面向具有PL条件的非凸强凸min-max问题的目标和对偶差收敛(arXiv:2006.06889v8 [cs.LG] UPDATED)

    Fast Objective & Duality Gap Convergence for Non-Convex Strongly-Concave Min-Max Problems with PL Condition. (arXiv:2006.06889v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2006.06889](http://arxiv.org/abs/2006.06889)

    该论文探讨了解决深度学习中出现的一类非凸强凸min-max问题的随机方法，并提出了一个基于近端阶段的方法框架，其中嵌入了许多众所周知的随机更新，快速收敛性得到了建立。

    

    本文着重于解决平滑非凸强凸min-max问题的随机方法，该问题由于其在深度学习中的潜在应用（如深度AUC最大化，分布式鲁棒优化）而受到越来越多的关注。然而，大多数现有算法在实践中较慢，并且它们的分析围绕收敛于接近稳态点展开。我们考虑利用Polyak-Lojasiewicz（PL）条件来设计更快的随机算法，并提供更强的收敛保证。虽然PL条件已经被用于设计许多随机最小化算法，但它们对于非凸极小化最大化优化的应用仍然很少。在本文中，我们提出并分析了一个泛化的基于近端阶段的方法框架，其中嵌入了许多众所周知的随机更新。我们建立了基于原始目标间隙和对偶间隙的快速收敛性。与现有研究相比，（i）我们的分析是...

    This paper focuses on stochastic methods for solving smooth non-convex strongly-concave min-max problems, which have received increasing attention due to their potential applications in deep learning (e.g., deep AUC maximization, distributionally robust optimization). However, most of the existing algorithms are slow in practice, and their analysis revolves around the convergence to a nearly stationary point.We consider leveraging the Polyak-Lojasiewicz (PL) condition to design faster stochastic algorithms with stronger convergence guarantee. Although PL condition has been utilized for designing many stochastic minimization algorithms, their applications for non-convex min-max optimization remain rare. In this paper, we propose and analyze a generic framework of proximal stage-based method with many well-known stochastic updates embeddable. Fast convergence is established in terms of both the primal objective gap and the duality gap. Compared with existing studies, (i) our analysis is 
    
[^167]: 多变量大数据分析中的可解释性学习用于网络监测

    Interpretable Learning in Multivariate Big Data Analysis for Network Monitoring. (arXiv:1907.02677v2 [cs.NI] UPDATED)

    [http://arxiv.org/abs/1907.02677](http://arxiv.org/abs/1907.02677)

    本文扩展了多变量大数据分析（MBDA）方法，提出了一种自动推导特征的解决方案，结合可解释性和交互式模型的优势以及并行处理的能力，应用于网络监测和诊断，最终在UGR'16和Dartmouth'18两个数据集上取得成功。

    

    开发新的数据驱动模型以评估通信网络性能越来越受到关注。对于许多应用程序，比如网络监测和故障排除，如果不能被人类操作员解释，数据模型就没多大用处。在本文中，我们提出了多变量大数据分析（MBDA）方法的扩展，这是一种近期提出的可解释性数据分析工具。在这个扩展中，我们提出了自动推导特征的解决方案，这是当数据量庞大时应用MBDA的重要步骤。所得到的网络监测方法允许我们检测和诊断不同的网络异常，采用一种将可解释性和交互式模型的优势与并行处理的能力相结合的数据分析工作流。我们将扩展的MBDA应用于两个案例研究：UGR'16，用于异常检测的基准流量实际数据集，以及Dartmouth'18，最长和最具挑战性的数据集之一。

    There is an increasing interest in the development of new data-driven models useful to assess the performance of communication networks. For many applications, like network monitoring and troubleshooting, a data model is of little use if it cannot be interpreted by a human operator. In this paper, we present an extension of the Multivariate Big Data Analysis (MBDA) methodology, a recently proposed interpretable data analysis tool. In this extension, we propose a solution to the automatic derivation of features, a cornerstone step for the application of MBDA when the amount of data is massive. The resulting network monitoring approach allows us to detect and diagnose disparate network anomalies, with a data-analysis workflow that combines the advantages of interpretable and interactive models with the power of parallel processing. We apply the extended MBDA to two case studies: UGR'16, a benchmark flow-based real-traffic dataset for anomaly detection, and Dartmouth'18, the longest and l
    
[^168]: 偏好神经网络

    Preference Neural Network. (arXiv:1904.02345v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1904.02345](http://arxiv.org/abs/1904.02345)

    本论文提出了一种新的偏好神经网络，用于解决具有冷漠偏好顺序的问题以及多标签排名问题。通过新的平滑阶梯形激活函数，PNN在提高计算效率的同时在严格标签排名的准确性方面优于五种先前提出的方法。

    

    本文提出了一种偏好神经网络（PNN），采用新的激活函数解决了冷漠偏好排序的问题。PNN还解决了多标签排名问题，其中标签可能具有冷漠偏好顺序或子组平等排名。PNN采用多层前馈结构，具有完全连接的神经元。每个神经元都包含基于喜好顺序的新型平滑阶梯形激活函数。PNN输入表示数据特征，输出神经元表示标签索引。该PNN使用了包含未经过实验的重复标签值的新偏好挖掘数据集进行评估。在高计算效率下，PNN在严格标签排名的准确结果方面优于先前提出的五种方法。

    This paper proposes a preference neural network (PNN) to address the problem of indifference preferences orders with new activation function. PNN also solves the Multi-label ranking problem, where labels may have indifference preference orders or subgroups are equally ranked. PNN follows a multi-layer feedforward architecture with fully connected neurons. Each neuron contains a novel smooth stairstep activation function based on the number of preference orders. PNN inputs represent data features and output neurons represent label indexes. The proposed PNN is evaluated using new preference mining dataset that contains repeated label values which have not experimented before. PNN outperforms five previously proposed methods for strict label ranking in terms of accurate results with high computational efficiency.
    

