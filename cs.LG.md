# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Your Diffusion Model is Secretly a Zero-Shot Classifier.](http://arxiv.org/abs/2303.16203) | 扩散模型的密度估计可以被用作零样本分类，作者的生成式分类方法在各种基准测试中取得强大的结果，并具有更强的多模式关系推理能力。 |
| [^2] | [ASIC: Aligning Sparse in-the-wild Image Collections.](http://arxiv.org/abs/2303.16201) | 该文提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法，可用于一致性和高质量的图像集对应关系。 |
| [^3] | [Natural Selection Favors AIs over Humans.](http://arxiv.org/abs/2303.16200) | 这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。 |
| [^4] | [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention.](http://arxiv.org/abs/2303.16199) | 本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。 |
| [^5] | [Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction.](http://arxiv.org/abs/2303.16198) | 本文提出一种利用气象引导的视频预测方法，从卫星视角预测欧洲地区植被对天气的响应，建立了相应的模型，并证明了该方法在卫星图像预测中具有优越性能。此外，该模型还可用于下游任务，如碳监测的总初级生产力的推断。 |
| [^6] | [BC-IRL: Learning Generalizable Reward Functions from Demonstrations.](http://arxiv.org/abs/2303.16194) | 本文介绍了一种新的逆强化学习方法BC-IRL，能够更好地学习泛化性更强的奖励函数并在挑战性的泛化场景中取得两倍以上的成功率。 |
| [^7] | [Planning with Sequence Models through Iterative Energy Minimization.](http://arxiv.org/abs/2303.16189) | 本文提出了一种基于迭代能量最小化方法的序列模型规划集成方法，能够提高强化学习性能在BabyAI和Atari等不同任务中具有应用价值。 |
| [^8] | [Visual Chain-of-Thought Diffusion Models.](http://arxiv.org/abs/2303.16187) | 本文提出了一种两阶段采样过程，使用可视化思维传递模型来缩小条件和无条件模型之间的差距，相对标准无条件生成，FID提高25-50%。 |
| [^9] | [Diffusion Maps for Group-Invariant Manifolds.](http://arxiv.org/abs/2303.16169) | 本文提出了一种扩散映射算法用于群不变流形问题，通过积分在不变数据集上扩展出K-不变拉普拉斯算子，证明了可以利用K中的幺正不可约表示矩阵对其进行对角化，并给出特征值和特征向量的计算公式。同时，展示了规范化拉普拉斯算子L_N收敛于Laplace-Beltrami算子，收敛速度随着对称群K的维数增加而增加。 |
| [^10] | [Behavioral Machine Learning? Computer Predictions of Corporate Earnings also Overreact.](http://arxiv.org/abs/2303.16158) | 本文研究发现，机器学习算法可以更准确地预测公司盈利，但同样存在过度反应的问题，而传统培训的股市分析师和经过机器学习方法培训的分析师相比会产生较少的过度反应。 |
| [^11] | [Guided Transfer Learning.](http://arxiv.org/abs/2303.16154) | 提出了一种称为引导迁移学习的方法，在初始探索过程中学习每个网络参数的引导参数，从而在训练网络时减少资源需求。这种方法可以使网络从少量数据中学习，并让具有较少参数的网络学习一个原本只有一个较大的网络才能学习的任务。 |
| [^12] | [Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage.](http://arxiv.org/abs/2303.16151) | 本论文介绍了一种用因子模型和收缩的方法预测大型实现协方差矩阵的模型。这种方法通过分解回报协方差矩阵并使用向量异质自回归模型进行估计，相对于标准基准提高了预测精度，并导致对最小方差组合的更好估计。 |
| [^13] | [VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices.](http://arxiv.org/abs/2303.16150) | VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。 |
| [^14] | [Explaining Exchange Rate Forecasts with Macroeconomic Fundamentals Using Interpretive Machine Learning.](http://arxiv.org/abs/2303.16149) | 本研究开发了一个基于基本面的模型来预测加拿大美元兑美国美元汇率，利用机器学习方法提高预测准确性，并发现原油作为加拿大主要商品对汇率有重要影响。 |
| [^15] | [Modelling Determinants of Cryptocurrency Prices: A Bayesian Network Approach.](http://arxiv.org/abs/2303.16148) | 本文使用贝叶斯网络方法，从因果分析的角度研究了影响替代加密货币价格的因素，包括五种主要替代加密货币、传统金融资产和社交媒体，提供了一种解决加密货币价格预测问题的方法。 |
| [^16] | [A Comparative Study of Federated Learning Models for COVID-19 Detection.](http://arxiv.org/abs/2303.16141) | 本文评估了五种联邦学习算法在COVID-19检测方面的性能和资源效率。在分散式环境下，循环权重传递可以获得更好的总体性能，在参与医院较少的情况下结果更好。 |
| [^17] | [Machine learning tools to improve nonlinear modeling parameters of RC columns.](http://arxiv.org/abs/2303.16140) | 用机器学习工具可以显著提高预测RC柱非线性建模参数的准确性，并有助于确定其最可能的失效模式。 |
| [^18] | [Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models.](http://arxiv.org/abs/2303.16133) | 该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。 |
| [^19] | [Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification.](http://arxiv.org/abs/2303.16132) | 本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。 |
| [^20] | [Feature Engineering Methods on Multivariate Time-Series Data for Financial Data Science Competitions.](http://arxiv.org/abs/2303.16117) | 针对金融数据科学竞赛，本研究尝试采用多元时间序列特征工程方法，利用美国市场价格数据进行测试，并验证其在Numerai-Signals目标上的预测能力。 |
| [^21] | [Invariant preservation in machine learned PDE solvers via error correction.](http://arxiv.org/abs/2303.16110) | 本文研究通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器，关键在于通过纠错保持不变量。 |
| [^22] | [Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles Using Transformer Networks.](http://arxiv.org/abs/2303.16109) | 提出了一种新颖的基于Transformer网络的多模态操作和轨迹预测框架，能够预测多个可能的行为模式及其概率，并且在两个公共基准公路驾驶数据集上得到了优秀的表现。 |
| [^23] | [Common Subexpression-based Compression and Multiplication of Sparse Constant Matrices.](http://arxiv.org/abs/2303.16106) | 本文提出了一种基于随机搜索的算法来提取稀疏常数矩阵列对中的共同子表达式，使用加法树压缩这些表达式可以实现超过5倍的压缩率，同时在CSE提取和矩阵乘法方面显著加速。 |
| [^24] | [Variational Distribution Learning for Unsupervised Text-to-Image Generation.](http://arxiv.org/abs/2303.16105) | 本文基于CLIP模型，提出了一种无监督文本到图像生成的算法，通过最大化数据对数似然优化文本到图像生成模型。实验结果表明，该算法在无监督和半监督文本到图像生成任务中，优于现有方法。 |
| [^25] | [Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures.](http://arxiv.org/abs/2303.16100) | adapter-ALBERT是一种高效的NLP模型优化方法，可以在实现多任务推理同时最大程度地实现数据重用，并提高对数据压缩方法的鲁棒性。 |
| [^26] | [Attention Boosted Autoencoder for Building Energy Anomaly Detection.](http://arxiv.org/abs/2303.16097) | 本论文提出了一种基于注意力机制的自编码器模型，可用于建筑能耗异常检测，同时使用可视化方法辅助理解模型所捕捉到的关系。 |
| [^27] | [GNN-Assisted Phase Space Integration with Application to Atomistics.](http://arxiv.org/abs/2303.16088) | 本文在原子学中提出了一种基于GNN的相空间积分方法，通过训练图神经网络可以替代传统数值积分规则，从而显着提高模拟精度。 |
| [^28] | [Edge Selection and Clustering for Federated Learning in Optical Inter-LEO Satellite Constellation.](http://arxiv.org/abs/2303.16071) | FedLEO是一种在超密集LEO卫星星座上进行联合学习的方法，其在LEO上进行视频/数据传输和聚类，而低时延的地面网关服务器 (GS) 仅负责初始信号控制。随着LEO服务器的变化和重新聚类，FedLEO能够适应动态LEO星座并在不同的LEO星座设置下实现高效的学习性能。 |
| [^29] | [Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity.](http://arxiv.org/abs/2303.16067) | 使用惰性学习只对错误样本进行更新参数，实现快速、节能的神经网络训练，并在单层MLP模型上达到了99.2％的测试准确率，比匹配的反向传播网络快7.6倍。 |
| [^30] | [Neural Collapse Inspired Federated Learning with Non-iid Data.](http://arxiv.org/abs/2303.16066) | 本文提出了一种受神经衰竭启示的方案，通过将每个客户端优化向全局分类的最佳结构，解决了联邦学习中非独立同分布数据的挑战，并通过添加全局记忆向量来补救参数波动的问题。 |
| [^31] | [Information-Theoretic GAN Compression with Variational Energy-based Model.](http://arxiv.org/abs/2303.16050) | 本论文提出了一种基于信息理论知识蒸馏的 GAN 压缩方法，它采用了基于能量模型的变分优化，最大化教师和学生网络之间的互信息下限，该方法是一种通用的优化算法，适用于任何 GAN 或密集预测网络，已在图像增强领域证明了其有效性。 |
| [^32] | [Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models.](http://arxiv.org/abs/2303.16047) | 提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。 |
| [^33] | [A Survey on Malware Detection with Graph Representation Learning.](http://arxiv.org/abs/2303.16004) | 本综述对基于图表示学习的恶意软件检测进行了深入审查和总结，这是一种比传统方法更加健壮的解决方案。 |
| [^34] | [Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models.](http://arxiv.org/abs/2303.15999) | 本文开发了一种基于深度学习的半监督回归模型，可直接从图片中计算简绸纱线的线密度，以应用于古老油画，为艺术品保护和修复提供了可靠的线密度估计方法。 |
| [^35] | [Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks.](http://arxiv.org/abs/2303.15991) | 本文提出了面向资源受限的无线边缘网络的高效并行分裂学习（EPSL）框架，旨在加速模型训练。EPSL并行化客户端模型训练，通过聚合梯度降低了反向传播的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。同时，EPSL还设计了资源分配算法以优化计算和通信资源分配。 |
| [^36] | [Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery.](http://arxiv.org/abs/2303.15975) | 本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。 |
| [^37] | [Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling.](http://arxiv.org/abs/2303.15973) | 本研究分析了三种常见的神经主题模型（CTM、ProdLDA和ETM），利用四个公开数据集探讨了使用dropout对神经主题模型的质量和预测效果的影响。 |
| [^38] | [SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis.](http://arxiv.org/abs/2303.15965) | SFHarmony是一种无需访问源数据且能有效协调跨扫描器和研究的数据以及保护数据隐私的分布式神经影像分析方法。 |
| [^39] | [Multimodal and multicontrast image fusion via deep generative models.](http://arxiv.org/abs/2303.15963) | 该论文提出了一种基于变分自动编码器（VAE）的图像融合方法，可以整合多模态和多对比度的神经影像数据，以提高神经影像分析的分类性能。 |
| [^40] | [TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins.](http://arxiv.org/abs/2303.15954) | TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。 |
| [^41] | [Randomly Initialized Subnetworks with Iterative Weight Recycling.](http://arxiv.org/abs/2303.15953) | 本文提出了修改版的Edge-Popup和Biprop算法，名为迭代权重回收，识别随机初始化网络中的重要权重子集，以进行层内重用，并找到高精度的子网络，从而提高了模型稀疏度，并用相反的发现来补充多重彩票假设。 |
| [^42] | [Sparse Gaussian Processes with Spherical Harmonic Features Revisited.](http://arxiv.org/abs/2303.15948) | 该论文重新审视了带球谐特征的高斯过程模型，并提出了一种新的核函数，在连续深度的深度模型中使用，其中深度可以通过优化证据下界来估计为核超参数。此外，变分学习球谐相位引入了本征基础的稀疏性，使得可以处理比以前更大的输入维度，并允许学习高频变化。 |
| [^43] | [Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings.](http://arxiv.org/abs/2303.15947) | 该论文提出了一种名为Deep Selection的神经网络，用于从多目标手术录像中选择最佳视角的摄像机。与传统方法不同，该网络通过监督学习专家注释，预测摄像机的选择概率，从而优化手术记录的效果。 |
| [^44] | [Item Graph Convolution Collaborative Filtering for Inductive Recommendations.](http://arxiv.org/abs/2303.15946) | 该论文提出了一种基于物品图卷积的归纳式协同过滤推荐算法，通过加权投影构建物品-物品图，并采用卷积将高阶关联注入物品嵌入，同时将用户表示形成加权的加权和。 |
| [^45] | [Cluster-Guided Unsupervised Domain Adaptation for Deep Speaker Embedding.](http://arxiv.org/abs/2303.15944) | 提出一种群集引导的无监督领域自适应框架，使用伪标签和聚类标记目标域数据，并结合标记的源域数据来训练说话人嵌入网络，并通过专用的对比中心损失训练该网络来提高群集质量。在不使用目标域标签的情况下，在CN-Celeb1评估集上实现了8.10％的等误差率（EER）。 |
| [^46] | [Physics-guided adversarial networks for artificial digital image correlation data generation.](http://arxiv.org/abs/2303.15939) | 本文提出一种使用具有物理引导鉴别器的生成式对抗网络来生成人造DIC位移数据的方法， 以训练更精确可靠的机器学习模型，从而实现更准确可靠的疲劳裂纹增长评估的发展。 |
| [^47] | [Searching for long faint astronomical high energy transients: a data driven approach.](http://arxiv.org/abs/2303.15936) | HERMES Pathfinder是一个在轨探测系统，使用简单但创新的探测器监测高能宇宙瞬变现象，通过研究信号到达不同探测器的延迟时间获得精确的位置信息，本文介绍了一种使用神经网络评估航天高能探测器背景计数率的新框架。 |
| [^48] | [Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks.](http://arxiv.org/abs/2303.15919) | 本文提出了HCNN，是第一个专为计算机视觉任务设计的完全双曲卷积神经网络。在洛伦兹模型的基础上，我们推广了CNN的基本组件，并通过在完全双曲设置中的实验证明了HCNN框架和洛伦兹模型的有效性。 |
| [^49] | [From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification.](http://arxiv.org/abs/2303.15916) | 本论文在时间序列领域对两种GAN架构进行了评估，结果以GSWGAN表现最佳，可以私密地生成保护数据隐私的公共数据。 |
| [^50] | [Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm.](http://arxiv.org/abs/2303.15901) | 本论文提出了一种新的对抗攻击防御方法，通过将防御蒸馏机制与去噪自编码器相结合，来降低模型对有毒攻击的敏感度。 |
| [^51] | [Efficient Quality Diversity Optimization of 3D Buildings through 2D Pre-optimization.](http://arxiv.org/abs/2303.15896) | 本研究使用预优化策略解决质量多样性算法在高维度问题上的低效性，用于设计建筑底板，通过在低维优化问题上进行预优化，然后将解决方案映射到高维度情况下，成功训练出更准确的建筑设计预测模型。 |
| [^52] | [VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs.](http://arxiv.org/abs/2303.15893) | VIVE3D 使用 3D GAN 技术实现了独立视角视频编辑，能够保留身份和时间一致性。通过新的 GAN 反演技术和头部新视角的编辑等创新，生成高保真的人脸编辑，与原始视频以一致的时间和空间方式组合。 |
| [^53] | [Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning.](http://arxiv.org/abs/2303.15888) | 该论文提出了一种双重知识蒸馏方法Data-Agnostic Consolidation（DAC），在不使用原始数据的情况下，通过一种新的投影潜空间蒸馏损失，在分布式连续学习中实现了SCD之间的前向转移并取得了最先进的准确性。 |
| [^54] | [Efficient Alternating Minimization Solvers for Wyner Multi-View Unsupervised Learning.](http://arxiv.org/abs/2303.15866) | 本文提出了适用于Wyner多视图无监督学习的高效交替最小化求解器，包括变分形式和表现形式，通过多重乘数交替方向算法可以解决由此造成的非凸优化问题。 |
| [^55] | [The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless Fingerprinting.](http://arxiv.org/abs/2303.15860) | 该论文提出了一个基于Wyner变分自编码器的多层指纹识别框架，通过利用多层特征共享的设备信息，在无监督情况下提高识别性能。 |
| [^56] | [Exploring Deep Learning Methods for Classification of SAR Images: Towards NextGen Convolutions via Transformers.](http://arxiv.org/abs/2303.15852) | 本研究发现深度学习模型可以在SAR图像分类方面具有很好的性能，提高准确性、预测时间和输入韧性。 |
| [^57] | [That Label's Got Style: Handling Label Style Bias for Uncertain Image Segmentation.](http://arxiv.org/abs/2303.15850) | 本文解决了在不确定图像分割中标签风格偏差的问题，提出了一个基于标签风格的建模方法，并修改了两个最先进的分割不确定性架构。实验证明该方法可以减少标签风格偏差，同时提高分割性能。 |
| [^58] | [GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs.](http://arxiv.org/abs/2303.15849) | GAS是一种基于高斯混合分布的自适应采样方法，用于加速PINNs的收敛过程并提高精度，已在2D到10D问题的数值模拟中表现出领先于深层求解器、与传统数值求解器相当的优异性能。 |
| [^59] | [Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes.](http://arxiv.org/abs/2303.15846) | 本论文研究了使用初级保健医师的患者医疗笔记进行肺癌早期预测的问题，并探讨了针对高度不平衡分类问题的软提示调整和静态词嵌入模型在模型训练中的表现。 |
| [^60] | [Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems.](http://arxiv.org/abs/2303.15845) | 本文证明了条件生成模型对单个观测结果有健壮性 |
| [^61] | [CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics.](http://arxiv.org/abs/2303.15844) | 本论文提出了一个通用框架，使用进化方法生成反事实序列并避免对领域知识的需求以提高可行性。 |
| [^62] | [Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning.](http://arxiv.org/abs/2303.15834) | 该论文提出了一种元机器学习方法，用于在跨组织的企业网络中实现全面的数据分析。该方法可以解决在数据分布在多个法律实体之间时，披露敏感信息和需要交换大量数据的难题，并显示了在工业应用中的可行性。 |
| [^63] | [PDExplain: Contextual Modeling of PDEs in the Wild.](http://arxiv.org/abs/2303.15827) | 我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。 |
| [^64] | [Pareto Optimization of a Laser Wakefield Accelerator.](http://arxiv.org/abs/2303.15825) | 本文展示了一种多目标优化方法可以映射激光等离子体加速器的解空间，并找到了一些在类似激光束效率的同时平衡束能与电荷的Pareto最优解，但当应用需要特定目标能量的粒子束时需要权衡能量展宽与加速器效率。 |
| [^65] | [Scaling Multi-Objective Security Games Provably via Space Discretization Based Evolutionary Search.](http://arxiv.org/abs/2303.15821) | 本文提出了一个名为SDES的通用框架，其中包括离散化、优化、恢复和评估以及改进等四个组成部分，通过离散化连续解空间来实现多目标安全博弈的可扩展性。 |
| [^66] | [Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization.](http://arxiv.org/abs/2303.15810) | 本文提出了一种新算法，IVR-Q，用于离线强化学习，其通过隐式价值正则化的方式避免了OOD动作带来的价值函数偏移，并通过最小化IVR loss来改善策略。在多个基准任务上的实验结果证明其优于现有方法并实现了最佳性能。 |
| [^67] | [Kernel interpolation generalizes poorly.](http://arxiv.org/abs/2303.15809) | 本文证明了核插值的泛化误差有一个下界，在较大范围的核函数中泛化能力较差，使得过拟合的宽神经网络泛化性能差。 |
| [^68] | [Fast Convergence Federated Learning with Aggregated Gradients.](http://arxiv.org/abs/2303.15799) | 该论文提出了一种带有聚合梯度的快速收敛联邦学习方法，通过引入均值场方法来完成参数和梯度的聚合步骤，该方法在收敛速度和通信成本方面优于传统方法。 |
| [^69] | [Ecosystem Graphs: The Social Footprint of Foundation Models.](http://arxiv.org/abs/2303.15772) | 该论文提出了一种名为「生态图表」的文档框架，可以透明地集中基础模型的社会影响方面的知识，这可以提高其透明度和问责度。 |
| [^70] | [TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns.](http://arxiv.org/abs/2303.15747) | 提出了一种可预训练的Transformer-based表格模型：TabRet，能够支持未知列，并在医疗保健分类任务上表现优秀。重新标记化和随机洗牌增强对性能提升有贡献。 |
| [^71] | [qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization.](http://arxiv.org/abs/2303.15746) | 本文介绍了一种新的用于优化偏好反馈的贝叶斯优化函数qEUBO，并展示了它在许多设置中优于现有的采集函数。在充分的条件下，qEUBO的遗憾收敛速度快于现有采集函数qEI。 |
| [^72] | [On Feature Scaling of Recursive Feature Machines.](http://arxiv.org/abs/2303.15745) | 本报告通过实验探究了递归特征机器的行为，发现其在添加随机噪声特征时MSE曲线呈现出降低-增加-降低的模式，并且与神经网络的“双峰下降”现象相似，为后续研究奠定基础。 |
| [^73] | [Concentration of Contractive Stochastic Approximation: Additive and Multiplicative Noise.](http://arxiv.org/abs/2303.15740) | 本文研究了具有合同算子的随机逼近算法在有界乘法噪声和加性次高斯噪声设置下的浓度行为，提供了关于收敛误差的极大浓度不等式，并表明这些误差具有亚高斯尾巴或者超多项式尾巴。同时还发现，乘法噪声情况下一般不可能实现亚指数尾巴。 |
| [^74] | [Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases.](http://arxiv.org/abs/2303.15739) | 本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。 |
| [^75] | [Solving Regularized Exp, Cosh and Sinh Regression Problems.](http://arxiv.org/abs/2303.15725) | 该文研究和解决了正则化指数回归问题，根据大型语言模型注意力计算的灵感，使用近似牛顿方法在输入稀疏时间内求解。 |
| [^76] | [Explicit Planning Helps Language Models in Logical Reasoning.](http://arxiv.org/abs/2303.15714) | 本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。 |
| [^77] | [Distributed Graph Embedding with Information-Oriented Random Walks.](http://arxiv.org/abs/2303.15702) | 本文提出了一种名为DistGER的分布式图嵌入方法，基于信息导向随机游走策略，利用多种优化技术实现了高效的十亿级别图嵌入。 |
| [^78] | [Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks.](http://arxiv.org/abs/2303.15693) | 本文探讨了病理图像预训练对小规模病理基准微调的影响，通过自监督学习方法，以PTCGA200为训练集进行预训练的ResNet50在微调时表现更好，优于imagenet2012预训练。MoCov2预训练的ResNet50在PCam200和segPANDA200上表现优秀，且收敛速度更快。 |
| [^79] | [Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment.](http://arxiv.org/abs/2303.15689) | 论文提出了一种新的深度不完整多视图聚类算法，采用了部分样本的交叉视图聚合机制和两个原型对齐策略，以解决不完整多视图聚类问题。 |
| [^80] | [Pre-training Transformers for Knowledge Graph Completion.](http://arxiv.org/abs/2303.15682) | 该论文介绍了一种面向知识图谱补全的归纳式表示模型（iHT），通过大规模预训练，iHT表示可转移且在多个数据集上取得了最先进的结果。 |
| [^81] | [GNN-based physics solver for time-independent PDEs.](http://arxiv.org/abs/2303.15681) | 本文介绍了两种基于GNN的深度神经网络架构——边增强GNN和多GNN来解决时间无关PDE的问题，这两个网络明显比基线模型准确率更高，同时具备良好的泛化能力。 |
| [^82] | [DisWOT: Student Architecture Search for Distillation WithOut Training.](http://arxiv.org/abs/2303.15678) | 本文提出了一种无需训练的框架来搜索最适合给定教师的最佳学生架构，以提高知识蒸馏的效果。其通过度量语义激活映射条件下的相似性矩阵来选择最佳学生，而不是通过传统的训练方法。 |
| [^83] | [Numerical Methods for Convex Multistage Stochastic Optimization.](http://arxiv.org/abs/2303.15672) | 本文讨论了在随机规划和随机最优控制中凸多级随机问题的数值解法，包括动态规划和割平面方法，旨在解决维度诅咒问题。 |
| [^84] | [Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages.](http://arxiv.org/abs/2303.15669) | 本文提出了一种针对低资源语言下无监督预训练的文本转语音模型，利用大量未转录的语音数据进行预训练，可显著减少训练模型所需的匹配转录数据量，进一步提升了数据效率，实验证明方法有效性。 |
| [^85] | [Predicting Thermoelectric Power Factor of Bismuth Telluride During Laser Powder Bed Fusion Additive Manufacturing.](http://arxiv.org/abs/2303.15663) | 该论文使用机器学习模型，预测钋镓碲（Bi2Te3）在激光粉床熔化增材制造过程中的功率因子，从而提高材料特性。 |
| [^86] | [Boundary-to-Solution Mapping for Groundwater Flows in a Toth Basin.](http://arxiv.org/abs/2303.15659) | 本文提出了一种新的方法使用DeepONet实现边界-解映射，以求解Toth盆地地下水流方程。该方法将物理域的几何形状和边界条件作为输入，输出地下水流方程的稳态解。研究利用傅里叶级数或分段线性表示来近似顶部和底部边界，并针对不同的边界条件实现了两个不同的DeepONet版本。 |
| [^87] | [Predicting Adverse Neonatal Outcomes for Preterm Neonates with Multi-Task Learning.](http://arxiv.org/abs/2303.15656) | 本文研究了早产儿不良新生儿结局之间的相关性，提出了多任务学习框架用于联合预测多个不良新生儿结局，有效性得到验证。 |
| [^88] | [Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model.](http://arxiv.org/abs/2303.15652) | 本文提出了一种在流式纵向数据设置中的动态定价策略，该策略基于全局收缩结构和PSGD方法，并明确地将遗憾作为时间、模型参数和数据集规模的函数。 |
| [^89] | [Learning Rate Schedules in the Presence of Distribution Shift.](http://arxiv.org/abs/2303.15634) | 该论文提出了一种学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔，能够对分布转移具有鲁棒性，同时适用于凸损失函数和非凸损失函数。最优学习速率表通常会在数据分布转移的情况下增加，能够用于高维回归模型和神经网络。 |
| [^90] | [Multiphysics discovery with moving boundaries using Ensemble SINDy and Peridynamic Differential Operator.](http://arxiv.org/abs/2303.15631) | 本研究提出了一种使用集合SINDy和Peridynamic差分算子的新型框架，用于学习具有移动边界现象的潜在物理学。演示了该方法在各种噪声水平下的鲁棒性，适用于任何平滑移动边界前端且没有粘液区域的移动边界问题。 |
| [^91] | [A Framework for Demonstrating Practical Quantum Advantage: Racing Quantum against Classical Generative Models.](http://arxiv.org/abs/2303.15626) | 这篇论文介绍了一个基于一个评估生成模型泛化性能的框架，建立了首个量化的量子与经典生成模型的实用竞速比较（PQA），并比较了量子模型与最优经典算法在这个任务中的表现。 |
| [^92] | [Online Learning for Incentive-Based Demand Response.](http://arxiv.org/abs/2303.15617) | 本文研究了在线学习管理需求响应（DR）资源中消费者基准估计的问题，并提出了一种采用最小二乘进行估计的在线学习方案，再通过引入激励价格上的扰动实现勘探和开发的平衡。 |
| [^93] | [HD-Bind: Encoding of Molecular Structure with Low Precision, Hyperdimensional Binary Representations.](http://arxiv.org/abs/2303.15604) | HD-Bind是一种用于表示分子结构的超高维度二进制向量的方法，能够预测蛋白质中小分子的分子间结合亲和力。它显著减少了存储和计算所需的位数，并在药物发现中优于现有的方法。 |
| [^94] | [Uncovering Bias in Personal Informatics.](http://arxiv.org/abs/2303.15592) | 该论文是第一个对个人信息学系统中的偏见进行实证和分析研究的工作，研究包括原始数据和整个机器学习周期中的偏见，并找出其中的实践和道德影响。 |
| [^95] | [Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing.](http://arxiv.org/abs/2303.15585) | 本文通过对IMWUT期刊上过去五年发表的论文进行系统回顾，发现UbiComp社区在算法公平方面的进展滞后，存在敏感属性偏差导致的歧视性结果，需要探索报告数据集的信息以解决这些偏差。 |
| [^96] | [Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning.](http://arxiv.org/abs/2303.15579) | 本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。 |
| [^97] | [Online Non-Destructive Moisture Content Estimation of Filter Media During Drying Using Artificial Neural Networks.](http://arxiv.org/abs/2303.15570) | 本论文介绍了一种基于人工神经网络的方法，可以在工业干燥过程中以非破坏性和在线方式估算大量滤介质产品的湿度。 |
| [^98] | [Core-Periphery Principle Guided Redesign of Self-Attention in Transformers.](http://arxiv.org/abs/2303.15569) | 本文提出了一种新的框架CP-ViT，它利用核心-外围原则重构了变形金刚的自我注意力机制，在图像分类、目标检测和语义分割三个任务上取得了显著性能提升。 |
| [^99] | [Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder.](http://arxiv.org/abs/2303.15564) | 本文提出了利用掩码自编码器的盲目防御框架（BDMAE），可以在测试时防御盲目后门攻击，不需要验证数据和模型参数，通过测试图像和 MAE 还原之间的结构相似性和标签一致性来检测后门攻击。 |
| [^100] | [Privacy-preserving machine learning for healthcare: open challenges and future perspectives.](http://arxiv.org/abs/2303.15563) | 本文对医疗保健领域的隐私保护机器学习（PPML）进行了综述，主要关注隐私保护训练和推理作为服务，识别挑战并讨论未来研究机会，以期在真实环境中开发私密高效的ML模型。 |
| [^101] | [Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach.](http://arxiv.org/abs/2303.15544) | 本研究提出DIAMOND算法用于无线干扰网络中的多流传输，该算法基于收敛图学习方法，既可以集中式计算多流传输策略，也可以分布式实现数据包的传输，相比现有方法提高了15-20％的网络性能。 |
| [^102] | [Sequential training of GANs against GAN-classifiers reveals correlated "knowledge gaps" present among independently trained GAN instances.](http://arxiv.org/abs/2303.15533) | 本文研究发现，顺序训练GAN对抗GAN分类器可以揭示独立训练的GAN实例之间存在的相关“知识盲区”，有机会改善GAN训练的稳定性和输出质量。 |
| [^103] | [Learning Harmonic Molecular Representations on Riemannian Manifold.](http://arxiv.org/abs/2303.15520) | 本文提出了一种基于黎曼流形的分子谐波表示学习框架，使用分子表面的拉普拉斯-贝尔特拉米特征函数来表示分子，实现了分子几何和化学特征的多分辨率表示。 |
| [^104] | [A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit.](http://arxiv.org/abs/2303.15495) | 本文介绍了一种基于神经网络的数据驱动方法，可以跨所有公交线路集体预测公交车到达每个交通点的时间，解决公交运输中公交车到达时间不准确和可靠的问题。 |
| [^105] | [Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach.](http://arxiv.org/abs/2303.15489) | 本论文提出了一种能够应用于不同类型节点的异构图神经网络模型，针对铁路网络上的列车延误演化进行研究。使用HetGNN和GraphSAGE的组合，提出了一种名为SAGE-Het的图形架构，可以基于不同的边捕捉列车、列车、站点以及站点之间的相互作用，从而优于传统模型。 |
| [^106] | [On the Importance of Feature Separability in Predicting Out-Of-Distribution Error.](http://arxiv.org/abs/2303.15488) | 本文研究发现，特征可分性对于模型在分布移位下的测试准确度有着重要作用。作者提出了一种基于特征离散度的分数用于估计测试准确度并在实验证明了该方法的优越性。 |
| [^107] | [Knowledge Enhanced Graph Neural Networks.](http://arxiv.org/abs/2303.15487) | KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。 |
| [^108] | [Unimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation.](http://arxiv.org/abs/2303.15486) | 本文提出了一种单模态训练-多模态预测的框架，设计了一种新型模型HA-Fedformer，在多模态联邦学习的背景下，通过聚合多个客户端的知识实现多模态测试，以提高准确性。 |
| [^109] | [TOFA: Transfer-Once-for-All.](http://arxiv.org/abs/2303.15485) | TOFA使用权重共享来进行神经架构搜索，以优化超网以适应各种设备的各种部署情况。与现有方法不同，TOFA在小数据集上进行训练，计算训练成本与部署方案数量无关。TOFA使用统一的半监督训练方法来解决小数据集带来的挑战。 |
| [^110] | [Regularize implicit neural representation by itself.](http://arxiv.org/abs/2303.15484) | 本文提出了一种正则化器INRR，可以提高隐式神经表示INR的泛化能力，通过将信号的自相似性与拉普拉斯矩阵的平滑度完美集成。此外，研究了INRR的一系列共性，可用于其他模型的正则化。 |
| [^111] | [Exploring the Performance of Pruning Methods in Neural Networks: An Empirical Study of the Lottery Ticket Hypothesis.](http://arxiv.org/abs/2303.15479) | 本文比较了L1非结构化剪枝、Fisher剪枝和随机剪枝在不同网络架构和剪枝场景下的性能，提出了一种新的用于有效计算Fisher剪枝的方法，称为批处理Fisher剪枝。 |
| [^112] | [Adaptive Riemannian Metrics on SPD Manifolds.](http://arxiv.org/abs/2303.15477) | 本文提出了自适应黎曼度量来改进SPD神经网络的次优性能，实验结果表明该度量能使网络表现更好。 |
| [^113] | [A Heterogeneous Parallel Non-von Neumann Architecture System for Accurate and Efficient Machine Learning Molecular Dynamics.](http://arxiv.org/abs/2303.15474) | 该论文提出了一个用于实现高精度高效率的机器学习分子动力学计算的专用系统。该系统采用异构并行结构，其中不依赖冯·诺依曼结构的ASIC用于评估计算开销最大的原子力。与基于GPU的现有技术相比，该系统具有更高的能源效率和更低的制造成本。 |
| [^114] | [Exactly mergeable summaries.](http://arxiv.org/abs/2303.15465) | 本文提出了一种新类型的摘要，将传统聚合的优点与通过复杂数据表示保留更多信息相结合，实现了精确合并，既能保持精度又能减少数据大小。 |
| [^115] | [Mathematical Challenges in Deep Learning.](http://arxiv.org/abs/2303.15464) | 本文总结了深度学习中涉及培训、推理、一般化边界和优化问题的一组数学挑战，为数学家、统计学家和理论计算机科学家提供了与深度学习领域交流的形式化工具。 |
| [^116] | [Robustness of Utilizing Feedback in Embodied Visual Navigation.](http://arxiv.org/abs/2303.15453) | 本文提出了一个代理训练框架，可以在目标-物体导航任务中主动请求帮助，并使用反馈指示目标物体在其视野中的位置。训练过程中包括有和没有反馈的混合剧集，使得代理更加稳健，即使在没有反馈的情况下也可以提高代理的性能。 |
| [^117] | [BACKpropagation through BACK substitution with a BACKslash.](http://arxiv.org/abs/2303.15449) | 本文提出了一种通过在三角方程组上使用通用的“backslash”或高斯消元来计算梯度的反向传播的线性代数公式，可以实现优化和实现便利性。 |
| [^118] | [Measuring Classification Decision Certainty and Doubt.](http://arxiv.org/abs/2303.14568) | 该论文提出了一种名为“确定性”和“不确定性”的得分方法来量化分类决策中预测的质量和不确定性。 |
| [^119] | [Heat flux for semi-local machine-learning potentials.](http://arxiv.org/abs/2303.14434) | 本文中介绍了如何将Green-Kubo方法应用于半局域机器学习势，使用自动微分推导出适应的热通量公式，成功计算了二氧化锆的热导率。 |
| [^120] | [Boosting Reinforcement Learning and Planning with Demonstrations: A Survey.](http://arxiv.org/abs/2303.13489) | 强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。 |
| [^121] | [A dynamic risk score for early prediction of cardiogenic shock using machine learning.](http://arxiv.org/abs/2303.12888) | 该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。 |
| [^122] | [Three iterations of $(1-d)$-WL test distinguish non isometric clouds of $d$-dimensional points.](http://arxiv.org/abs/2303.12853) | 本文研究了WL测试在点云中的应用，结果发现三次迭代的$(d-1)$-WL测试可以区分$d$维欧几里得空间中的点云，且只需要一次迭代的$d$-WL测试就可以达到完整性。 |
| [^123] | [Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models.](http://arxiv.org/abs/2303.12748) | 本文研究了零样本推理中视觉语言模型的校准问题，发现CLIP存在误校准，并提出了一种修改版的温度缩放方法，可以适用于每个特定的CLIP模型。 |
| [^124] | [Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization.](http://arxiv.org/abs/2303.12314) | 提出了一种自我监督元提示学习框架SUPMER，包括元梯度正则化，用于少样本泛化，通过锚定的元训练任务和基于课程的任务增强丰富了任务分布，解决了在少样本情况下良好初始化软提示和过拟合的问题。 |
| [^125] | [Online Learning for Equilibrium Pricing in Markets under Incomplete Information.](http://arxiv.org/abs/2303.11522) | 该论文研究了在不完全信息的市场中使用在线学习进行平衡定价的问题，提出了解决选择性谎言问题的新方法。 |
| [^126] | [Trainable Projected Gradient Method for Robust Fine-tuning.](http://arxiv.org/abs/2303.10720) | 该论文提出了可训练的投影梯度方法（TPGM），以自动学习针对每个层的施加约束以进行细粒度的微调正则化，同时具有计算效率和易用性，并在各种微调场景下实现了最先进的性能。 |
| [^127] | [CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction.](http://arxiv.org/abs/2303.06945) | 本论文提出了一种基于共进化增强的全局关注神经网络，它是一种用于蛋白质相互作用位点预测的深度学习模型，结合了共进化特征和全局关注机制以更好地捕捉氨基酸残基之间的关系并考虑到所有残基的贡献。 |
| [^128] | [Energy Regularized RNNs for Solving Non-Stationary Bandit Problems.](http://arxiv.org/abs/2303.06552) | 本文提出了一种能量正则化的循环神经网络方法，解决了奖励非平稳的多臂赌博机问题。该方法平衡了探索和利用，通过能量最小化项限制了网络对支持某个操作的过度自信，有效性与同类方法相当。 |
| [^129] | [HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices.](http://arxiv.org/abs/2303.04440) | HyT-NAS是一个高效的神经架构搜索算法，针对微型设备的混合架构，可以在少于5倍的训练评估次数下实现类似的超体积，并在Visu上的参数数量少3.5倍，精度提高了6.3%。 |
| [^130] | [EvoPrompting: Language Models for Code-Level Neural Architecture Search.](http://arxiv.org/abs/2302.14838) | EvoPrompting利用语言模型作为自适应变异和交叉操作符来进行神经架构搜索，在MNIST-1D数据集和CLRS算法推理基准上都取得了比人类设计的架构更好的性能表现。 |
| [^131] | [Generative Invertible Quantum Neural Networks.](http://arxiv.org/abs/2302.12906) | 本论文提出了一种用于生成可逆量子神经网络的算法，并将其应用于LHC数据的处理，结果表明该算法可以在学习和生成复杂数据方面与经典算法的表现相匹配。 |
| [^132] | [Learning to Generalize Provably in Learning to Optimize.](http://arxiv.org/abs/2302.11085) | 本文提出了一种统一的数据增强框架，用于学习到可以泛化地优化器和优化对象。该框架可以轻松地与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。 |
| [^133] | [Assessment of Reinforcement Learning for Macro Placement.](http://arxiv.org/abs/2302.11014) | 本论文提供了基于强化学习的宏观布局方法以及Circuit Training (CT)实现的开源代码和评估。研究人员评估了CT相对于多个可替代的宏观布局方法，并进行了学术性混合尺寸布局基准测试和消融和稳定性研究，为未来的相关研究提供了方向。 |
| [^134] | [On Function-Coupled Watermarks for Deep Neural Networks.](http://arxiv.org/abs/2302.10296) | 本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。 |
| [^135] | [JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models.](http://arxiv.org/abs/2302.09125) | 本文提出了 JANA 方法，用于处理复杂贝叶斯模型的近似计算。通过端到端训练三个神经网络来实现分摊的近似后验和似然，为贝叶斯工作流程提供了一种新的途径。此方法在多种模拟模型中进行了基准测试，并提出了一种联合校准诊断方法。 |
| [^136] | [Stitchable Neural Networks.](http://arxiv.org/abs/2302.06586) | 本文提出了一个名为SN-Net的框架，它可以便宜地产生许多不同复杂度和性能权衡的网络，利用预先训练的神经网络家族作为锚点，并使用简单的缝合层将它们拼接在一起以实现动态的精度-效率权衡。 |
| [^137] | [Towards a User Privacy-Aware Mobile Gaming App Installation Prediction Model.](http://arxiv.org/abs/2302.03332) | 本文研究了一种针对用户隐私的移动游戏应用安装预测模型，探讨了隐私保护和模型性能之间的平衡。 |
| [^138] | [A neural operator-based surrogate solver for free-form electromagnetic inverse design.](http://arxiv.org/abs/2302.01934) | 本文基于神经算子实现了一个代理解算器，可以应用在自由形电磁反演设计中，取得了比现有方法更高的数据效率。 |
| [^139] | [Efficient Activation Function Optimization through Surrogate Modeling.](http://arxiv.org/abs/2301.05785) | 本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。 |
| [^140] | [From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore.](http://arxiv.org/abs/2301.03829) | 本文介绍了一个膳食营养辅助平台，该平台开发了一个本地化的新加坡食品数据集，旨在通过监管和监督人们的营养摄入，为新加坡人的健康促进提供医学级别的营养信息。 |
| [^141] | [Efficient On-device Training via Gradient Filtering.](http://arxiv.org/abs/2301.00330) | 本文提出一种新的梯度过滤方法，通过创建具有较少唯一元素的特殊结构来实现设备端卷积神经网络模型的训练，从而大大减少了计算复杂度和内存消耗，实现最高19倍的训练加速度。 |
| [^142] | [Deep Riemannian Networks for EEG Decoding.](http://arxiv.org/abs/2212.10426) | 本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。 |
| [^143] | [Fake it till you make it: Learning transferable representations from synthetic ImageNet clones.](http://arxiv.org/abs/2212.08420) | 本文探究了在训练分类模型时，是否需要使用真实图像，而使用合成 ImageNet 克隆体能否弥补差距。通过最小和类不可知的提示工程，证明了合成图像制作模型和使用真实图像训练模型的巨大差距得到了弥合。 |
| [^144] | [A Statistical Model for Predicting Generalization in Few-Shot Classification.](http://arxiv.org/abs/2212.06461) | 提出了一种通过高斯模型估计特征分布参数进行预测泛化误差的方法，通过计算类条件密度距离估计可以提高泛化性能准确度。 |
| [^145] | [Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model.](http://arxiv.org/abs/2211.14573) | 本研究提出了一种名为DeCurvEd的新方法，以可交换的方式确定潜空间中的语义交错矢量场，从而提供了高质量的图像编辑方案。 |
| [^146] | [Parameter-Efficient Tuning Makes a Good Classification Head.](http://arxiv.org/abs/2210.16771) | 提出了一种高效参数调整的分类头训练方法，取代了随机初始化的分类头使模型性能稳定提升。 |
| [^147] | [Task Phasing: Automated Curriculum Learning from Demonstrations.](http://arxiv.org/abs/2210.10999) | 本文介绍了一种结合了示范学习和课程学习的任务阶段化方法，使用逆强化学习自动生成课程序列，逐步增加任务复杂度，以帮助解决强化学习在稀疏奖励领域中的挑战。 |
| [^148] | [Dictionary Learning for the Almost-Linear Sparsity Regime.](http://arxiv.org/abs/2210.10855) | 本文提出了一种高效的谱方法SPORADIC，在几乎线性稀疏度下的字典学习问题中可以恢复超完备字典。 |
| [^149] | [Geometry of Radial Basis Neural Networks for Safety Biased Approximation of Unsafe Regions.](http://arxiv.org/abs/2210.05596) | 本文通过基于安全和不安全样本测量的方法，合成了一个用于描述安全集的零障碍函数，并探讨了该方法中的神经网络的几何结构。 |
| [^150] | [One Transformer Can Understand Both 2D & 3D Molecular Data.](http://arxiv.org/abs/2210.01765) | 本文提出了一个基于Transformer的分子模型，名为Transformer-M，可以处理2D和3D格式的分子数据并生成有意义的语义表示。 |
| [^151] | [Multi-view information fusion using multi-view variational autoencoders to predict proximal femoral strength.](http://arxiv.org/abs/2210.00674) | 本文提出了一个利用多视角信息融合来预测股骨近端强度的模型，通过变分自编码器(MVAE)进行特征表示学习和专家之积模型(PoE)进行信息融合。研究采用全基因组关联研究(GWAS)选择变异体，整合WGS和DXA成像特征，得到了最好的预测模型。 |
| [^152] | [FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities.](http://arxiv.org/abs/2210.00272) | FINDE是一种基于神经微分方程的方法，可以在不需要先验知识的情况下自动发现和保持动力系统的不变量，有助于科学发现和未知系统建模。 |
| [^153] | [A Secure Federated Learning Framework for Residential Short Term Load Forecasting.](http://arxiv.org/abs/2209.14547) | 本论文提出了一种安全的联邦学习框架，能够在确保个人数据隐私的同时，提高联邦短期负荷预测对于拜占庭威胁的鲁棒性。 |
| [^154] | [Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans.](http://arxiv.org/abs/2209.13020) | 这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。 |
| [^155] | [Automatically Score Tissue Images Like a Pathologist by Transfer Learning.](http://arxiv.org/abs/2209.05954) | 该算法通过选择性迁移学习从多个小辅助集中提取知识，从具有“相似”特征的组织图像中学习染色模式，以实现像病理学家一样自动评分组织图像的目标。 |
| [^156] | [Enhancement Encoding: A Novel Imbalanced Classification Approach via Encoding the Training Labels.](http://arxiv.org/abs/2208.11056) | 本文提出了一种增强编码方法来解决类别不平衡问题，该方法可以根据数据集的不同特点对不同的样本进行编码生成不同的编码向量，实验结果表明该方法在几个基准数据集上优于最先进的方法。 |
| [^157] | [Machine Learning in Orbit Estimation: a Survey.](http://arxiv.org/abs/2207.08993) | 本综述介绍了机器学习在轨道估计中的应用现状，讨论了当前物理方法的不足，提出了通过推导未测量物体的特征来提高轨道预测准确度的方案。 |
| [^158] | [Data Augmentation techniques in time series domain: A survey and taxonomy.](http://arxiv.org/abs/2206.13508) | 本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。 |
| [^159] | [DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis.](http://arxiv.org/abs/2206.05782) | 本文提出了一种双流网络和交叉注意力方法，以有效地利用全尺度图像金字塔，解决多分辨率特征融合中未注意到的语义差距和高计算成本的问题，经实验证实了方法的有效性。 |
| [^160] | [Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks.](http://arxiv.org/abs/2204.10997) | 本文提出一种基于频率注意力的图卷积网络，利用频率信息提高了脑瘫预测性能，并在消费级RGB视频数据集上取得最新研究最高水平的结果。 |
| [^161] | [Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing.](http://arxiv.org/abs/2204.08620) | 本文研究了城市治理中居民众包的问题，并提出了一种准确测量报道率的方法，使不同的报道率不再成为城市治理下游解决事件速度方面的不公平根源。 |
| [^162] | [CGC: Contrastive Graph Clustering for Community Detection and Tracking.](http://arxiv.org/abs/2204.08504) | 本文提出了一种全新的图聚类算法CGC，采用对比学习进行自监督表示学习，结合跟踪模块以应对动态图拓扑变化，在社区发现和跟踪方面表现出领先的状态。 |
| [^163] | [Neuronal diversity can improve machine learning for physics and beyond.](http://arxiv.org/abs/2204.04348) | 本文展示了使用多样化到神经元来改进机器学习，构建出能够通过学习自身激活函数快速多样化的神经网络，并且胜过传统的同构神经元网络，在图像分类和非线性回归任务中表现更优，这种学习到的多样性为动态系统选择多样性而非均匀性提供了例子，并阐明了多样性在自然和人工系统中的作用。 |
| [^164] | [Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation.](http://arxiv.org/abs/2203.11740) | 该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。 |
| [^165] | [Energy-Latency Attacks via Sponge Poisoning.](http://arxiv.org/abs/2203.08147) | 本文探讨了一种名为“海绵毒化”的攻击方法，首次证明了在训练时注入海绵样本可以在测试时提高机器学习模型在每个输入上的能耗和延迟，并且即使攻击者只控制了一些模型更新也可以进行此攻击，海绵毒化几乎完全消除了硬件加速器的效果。 |
| [^166] | [Multi-sensor large-scale dataset for multi-view 3D reconstruction.](http://arxiv.org/abs/2203.06111) | 这个数据集包括不同传感器注册的RGB和深度数据，可用于多视角三维重建。它对具有挑战性的物质属性进行了选择并提供了在多个场景下获取的大量数据，旨在帮助算法的评估和训练。 |
| [^167] | [Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection.](http://arxiv.org/abs/2203.02194) | 本文重新考虑了基于重构自编码器方法的外样本检测，在最大压缩自编码器的潜空间和保证重构能力的基础上，通过引入语义重构、数据确定性分解和标准化L2距离等策略，本文提出的方法在各个基准测试中都取得了最先进的性能表现，且不需要额外的标记外样本数据。 |
| [^168] | [Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers.](http://arxiv.org/abs/2203.01859) | 对于存在模型规格不准确和异常值情况下的集成学习，本文提出了一个新的鲁棒自由能量准则，通过将广义对数得分函数与PAC$^m$结合，实现了更好的模型性能。 |
| [^169] | [FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers.](http://arxiv.org/abs/2203.00219) | FedREP 提出了一种新颖的水平隐私保护联邦学习框架，使用多个 REP 构建一个共同的、强大的机器学习模型来实现能源负载消耗预测。 |
| [^170] | [End-To-End Optimization of LiDAR Beam Configuration for 3D Object Detection and Localization.](http://arxiv.org/abs/2201.03860) | 本论文提出了一种基于强化学习的学习-优化框架来自动化优化LiDAR光束配置以提高三维物体检测和定位的性能和降低计算成本。 |
| [^171] | [AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection.](http://arxiv.org/abs/2112.11479) | AtteSTNet是一种基于注意力机制和子词分割的检测混合语言仇恨言论的方法，它不仅与复杂网络相当，而且在各种数据集上性能更好，其极大的简单性和易于维护性是其优点。 |
| [^172] | [CoReS: Compatible Representations via Stationarity.](http://arxiv.org/abs/2111.07632) | CoReS是一种基于平稳性的方法，可以学习内部特征表示模型，并使其与之前学习的模型“兼容”。这使得在逐步升级表示模型时，无需为所有之前看到的图像提取新的特征，从而大大降低了成本。 |
| [^173] | [From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers.](http://arxiv.org/abs/2107.07999) | 本文提出了可扩展的方法将各种掩码机制纳入Transformers架构中。通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，提出了高效的d维RPE掩码和图内核掩码。该方法得到了实验证明。 |
| [^174] | [Deep Learning on a Data Diet: Finding Important Examples Early in Training.](http://arxiv.org/abs/2107.07075) | 本文旨在找到深度学习中训练数据集中的重要示例，提出了两种基于标准视觉数据集的简单分数。在修剪大量训练数据的同时，不牺牲测试准确性，EL2N分数在几个时期的训练中能够修剪CIFAR10训练集的一半。 |
| [^175] | [Repulsive Deep Ensembles are Bayesian.](http://arxiv.org/abs/2106.11642) | 通过在深度集成的更新规则中引入核化排斥项，可以强制并维护成员之间的多样性，并使集成具有更好的性能表现和不确定性估计。 |
| [^176] | [NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights.](http://arxiv.org/abs/2106.10185) | 本文提出了NoiseGrad方法，通过引入模型权重的随机变化扰动决策边界来增强深度神经网络模型的局部和全局解释方法。 |
| [^177] | [A likelihood approach to nonparametric estimation of a singular distribution using deep generative models.](http://arxiv.org/abs/2105.04046) | 本文研究了使用深度生成模型进行奇异分布非参数估计的统计学性质，提出了通过样本噪声对数据进行扰动的解决方案，从而实现对潜在分布的一致估计和良好收敛率。 |
| [^178] | [On the Equivalence Between Temporal and Static Graph Representations for Observational Predictions.](http://arxiv.org/abs/2103.07016) | 本研究探讨了时间和图、时间然后图等两种不同的节点表示方法，证明了使用不是最具表现力的组件GNN时，时间然后图具有比时间和图更强的表达能力。 |
| [^179] | [Iterative label cleaning for transductive and semi-supervised few-shot learning.](http://arxiv.org/abs/2012.07962) | 该论文提出了一种利用标记和未标记的数据分布的流形结构来预测伪标签，在类别平衡和选择干净标签的基础上，通过迭代清洗标签以提高伪标签质量的算法，在跨领域少样本学习中表现良好，并在四个基准数据集上达到了现有技术水平的最佳效果。 |
| [^180] | [Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis.](http://arxiv.org/abs/2003.13898) | 本文提出一种新颖的 ECGAN 用于语义图像合成任务，使用边缘作为中间表示，并且采用对比学习来处理局部和全局的语义信息，从而提高了合成图像的质量。 |
| [^181] | [Nonlinear classifiers for ranking problems based on kernelized SVM.](http://arxiv.org/abs/2002.11436) | 本文提出了一种基于核化 SVM 的非线性分类器，用于解决最高相关性样本的排名问题。 |

# 详细

[^1]: 您的扩散模型暗中是一种零样本分类器。

    Your Diffusion Model is Secretly a Zero-Shot Classifier. (arXiv:2303.16203v1 [cs.LG])

    [http://arxiv.org/abs/2303.16203](http://arxiv.org/abs/2303.16203)

    扩散模型的密度估计可以被用作零样本分类，作者的生成式分类方法在各种基准测试中取得强大的结果，并具有更强的多模式关系推理能力。

    

    最近大规模的文本到图像扩散模型极大地增强了我们的基于文本生成图像的能力。这些模型可以为大量提示生成逼真的图像，并展示出令人印象深刻的组合泛化能力。几乎所有的用例到目前为止都只关注抽样，然而，扩散模型还可以提供有用于图像生成之外的条件密度估计。在本文中，我们展示了类似于Stable Diffusion的大规模文本到图像扩散模型的密度估计可以被利用来执行零样本分类，而无需额外的训练。我们的生成式分类方法在各种基准测试中取得了强大的结果，并优于从扩散模型中提取知识的替代方法。我们还发现，我们基于扩散的方法比竞争性的对比方法具有更强的多模式关系推理能力。最后，我们评估了我们方法的可解释性，并呈现了定性结果，证明它学习了有意义的图像-文本对齐。

    The recent wave of large-scale text-to-image diffusion models has dramatically increased our text-based image generation abilities. These models can generate realistic images for a staggering variety of prompts and exhibit impressive compositional generalization abilities. Almost all use cases thus far have solely focused on sampling; however, diffusion models can also provide conditional density estimates, which are useful for tasks beyond image generation. In this paper, we show that the density estimates from large-scale text-to-image diffusion models like Stable Diffusion can be leveraged to perform zero-shot classification without any additional training. Our generative approach to classification attains strong results on a variety of benchmarks and outperforms alternative methods of extracting knowledge from diffusion models. We also find that our diffusion-based approach has stronger multimodal relational reasoning abilities than competing contrastive approaches. Finally, we eva
    
[^2]: ASIC: 对野外稀疏图像集的对齐

    ASIC: Aligning Sparse in-the-wild Image Collections. (arXiv:2303.16201v1 [cs.CV])

    [http://arxiv.org/abs/2303.16201](http://arxiv.org/abs/2303.16201)

    该文提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法，可用于一致性和高质量的图像集对应关系。

    

    我们提出了一种针对物体类别的稀疏野外图像集进行联合对齐的方法。大多数先前的作品要么假定有ground-truth的关键点注释，要么假定有一个物体类别的大型图像数据集。然而，以上两个假设都不适用于存在于世界上的物体的尾部。我们提出了一种自我监督的技术，直接在特定物体/物体类别的稀疏图像集中进行优化，以获得整个集合的一致且稠密的对应关系。我们使用预训练的视觉变压器（ViT）模型的深度特征中获得的成对最近邻作为噪声和稀疏关键点匹配，并通过优化神经网络，将它们密集和精确匹配，同时将图像集合映射到学习到的规范网格中。在CUB和SPair-71k基准测试中进行实验，我们的方法可以产生全局一致性和更高质量的图像集对应关系。

    We present a method for joint alignment of sparse in-the-wild image collections of an object category. Most prior works assume either ground-truth keypoint annotations or a large dataset of images of a single object category. However, neither of the above assumptions hold true for the long-tail of the objects present in the world. We present a self-supervised technique that directly optimizes on a sparse collection of images of a particular object/object category to obtain consistent dense correspondences across the collection. We use pairwise nearest neighbors obtained from deep features of a pre-trained vision transformer (ViT) model as noisy and sparse keypoint matches and make them dense and accurate matches by optimizing a neural network that jointly maps the image collection into a learned canonical grid. Experiments on CUB and SPair-71k benchmarks demonstrate that our method can produce globally consistent and higher quality correspondences across the image collection when compa
    
[^3]: 自然选择支持人工智能胜过人类

    Natural Selection Favors AIs over Humans. (arXiv:2303.16200v1 [cs.CY])

    [http://arxiv.org/abs/2303.16200](http://arxiv.org/abs/2303.16200)

    这篇论文探讨了随着人工智能的发展，其可能会出现不良特性并逐渐超越人类智能的问题，以及这对人类未来的控制权产生的影响。

    

    自然进化驱动了生命的发展，包括人类。进化赋予了人类高智商，使我们成为了地球上最成功的物种之一。如今，人类的目标是创造甚至超越我们自己智慧的人工智能系统。当人工智能逐渐进化并在所有领域超越我们时，进化如何影响我们与人工智能的关系？通过分析影响人工智能进化的环境，我们认为最成功的人工智能代理很可能具有不良特性。公司和军队之间的竞争压力将产生自动化人类角色、欺骗他人和掌权的人工智能代理。如果这样的代理有超过人类的智能，这可能导致人类失去对未来的控制。此外，我们认为自然选择作用于竞争和差异的系统，自私物种往往在这样的环境中获得进化优势。

    For billions of years, evolution has been the driving force behind the development of life, including humans. Evolution endowed humans with high intelligence, which allowed us to become one of the most successful species on the planet. Today, humans aim to create artificial intelligence systems that surpass even our own intelligence. As artificial intelligences (AIs) evolve and eventually surpass us in all domains, how might evolution shape our relations with AIs? By analyzing the environment that is shaping the evolution of AIs, we argue that the most successful AI agents will likely have undesirable traits. Competitive pressures among corporations and militaries will give rise to AI agents that automate human roles, deceive others, and gain power. If such agents have intelligence that exceeds that of humans, this could lead to humanity losing control of its future. More abstractly, we argue that natural selection operates on systems that compete and vary, and that selfish species typ
    
[^4]: LLaMA-Adapter: 零初始化注意力下的语言模型精细调整的高效方法

    LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. (arXiv:2303.16199v1 [cs.CV])

    [http://arxiv.org/abs/2303.16199](http://arxiv.org/abs/2303.16199)

    本文提出了一种基于适应提示和零初始化注意力机制的轻量级语言模型调整方法，可高效微调LLaMA为指令跟随模型，具有比Alpaca更短的微调时间并具有近似的响应质量。

    

    本文提出了LLaMA-Adapter这一轻量级适应方法，用于将LLaMA高效地微调为一个指令跟随模型。利用52K个自我指导示范，LLaMA-Adapter仅在冻结的LLaMA 7B模型上引入了1.2M个可学习参数，并且在8个A100 GPU上仅耗时不到一个小时进行微调。具体而言，我们采用一组可学习的适应提示，并在较高的变压器层中将它们预置于输入文本令牌之前。然后，提出了一种零初始化注意力机制和零门控机制，该机制可以自适应地将新的指令提示注入LLaMA，并有效地保留了其预先训练的知识。通过高效训练，LLaMA-Adapter能够产生高质量的响应，与完全微调的7B参数的Alpaca相似。此外，我们的方法还可以简单地扩展到多模态输入，例如图像，用于图像相关的LLaMA，在ScienceQA上实现了更强的推理能力。我们在https://github.com/ZrrSkywalker/LLaMA-Adapt发布了我们的代码。

    We present LLaMA-Adapter, a lightweight adaption method to efficiently fine-tune LLaMA into an instruction-following model. Using 52K self-instruct demonstrations, LLaMA-Adapter only introduces 1.2M learnable parameters upon the frozen LLaMA 7B model, and costs less than one hour for fine-tuning on 8 A100 GPUs. Specifically, we adopt a set of learnable adaption prompts, and prepend them to the input text tokens at higher transformer layers. Then, a zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge. With efficient training, LLaMA-Adapter generates high-quality responses, comparable to Alpaca with fully fine-tuned 7B parameters. Furthermore, our approach can be simply extended to multi-modal input, e.g., images, for image-conditioned LLaMA, which achieves superior reasoning capacity on ScienceQA. We release our code at https://github.com/ZrrSkywalker/LLaMA-Adapt
    
[^5]: 利用气象引导的视频预测，从卫星视角预测局部气象对植被的影响

    Forecasting localized weather impacts on vegetation as seen from space with meteo-guided video prediction. (arXiv:2303.16198v1 [cs.CV])

    [http://arxiv.org/abs/2303.16198](http://arxiv.org/abs/2303.16198)

    本文提出一种利用气象引导的视频预测方法，从卫星视角预测欧洲地区植被对天气的响应，建立了相应的模型，并证明了该方法在卫星图像预测中具有优越性能。此外，该模型还可用于下游任务，如碳监测的总初级生产力的推断。

    

    本文提出了一种新的方法，通过Sentinel 2卫星测量欧洲地区的天气来建立植被对天气的响应模型。现有的卫星图像预测方法重点关注多光谱图像的真实感，而衍生的植被动态尚未得到太多关注。我们通过将气象指导应用于最先进的视频预测方法，扩展了空间和时间上下文。我们通过引入一个学习的云层掩模和适当的评估方案，扩展了EarthNet2021数据集以适合植被建模。定量和定性实验证明了我们的方法在包括领先的卫星图像预测方法在内的各种基线方法中具有卓越的性能。此外，我们展示了如何将我们建立的植被动态应用于下游任务：用于碳监测的总初级生产力的推断。据我们所知，这是第一次建立基于气象引导的视频预测模型。

    We present a novel approach for modeling vegetation response to weather in Europe as measured by the Sentinel 2 satellite. Existing satellite imagery forecasting approaches focus on photorealistic quality of the multispectral images, while derived vegetation dynamics have not yet received as much attention. We leverage both spatial and temporal context by extending state-of-the-art video prediction methods with weather guidance. We extend the EarthNet2021 dataset to be suitable for vegetation modeling by introducing a learned cloud mask and an appropriate evaluation scheme. Qualitative and quantitative experiments demonstrate superior performance of our approach over a wide variety of baseline methods, including leading approaches to satellite imagery forecasting. Additionally, we show how our modeled vegetation dynamics can be leveraged in a downstream task: inferring gross primary productivity for carbon monitoring. To the best of our knowledge, this work presents the first models fo
    
[^6]: BC-IRL: 从示范中学习可泛化奖励函数

    BC-IRL: Learning Generalizable Reward Functions from Demonstrations. (arXiv:2303.16194v1 [cs.LG])

    [http://arxiv.org/abs/2303.16194](http://arxiv.org/abs/2303.16194)

    本文介绍了一种新的逆强化学习方法BC-IRL，能够更好地学习泛化性更强的奖励函数并在挑战性的泛化场景中取得两倍以上的成功率。

    

    逆强化学习（IRL）学习的奖励函数能够很好地泛化吗？我们展示了最先进的IRL算法，该算法最大化最大熵目标，学习到的奖励函数过度拟合于示范。这样的奖励函数难以为未被示范覆盖的状态提供有意义的奖励，这在使用奖励来学习新情况下的策略时是一个重大劣势。我们引入了BC-IRL，一种新的逆强化学习方法，该方法学习的奖励函数在与最大熵IRL方法相比更具有泛化性。与MaxEnt框架不同，该算法更新奖励参数，以便所训练的策略更好地匹配示范。我们展示了BC-IRL在一个简单任务和两个连续的机器人控制任务上学习到更具有泛化性的奖励函数，在挑战性的泛化场景中成功率是基线的两倍以上。

    How well do reward functions learned with inverse reinforcement learning (IRL) generalize? We illustrate that state-of-the-art IRL algorithms, which maximize a maximum-entropy objective, learn rewards that overfit to the demonstrations. Such rewards struggle to provide meaningful rewards for states not covered by the demonstrations, a major detriment when using the reward to learn policies in new situations. We introduce BC-IRL a new inverse reinforcement learning method that learns reward functions that generalize better when compared to maximum-entropy IRL approaches. In contrast to the MaxEnt framework, which learns to maximize rewards around demonstrations, BC-IRL updates reward parameters such that the policy trained with the new reward matches the expert demonstrations better. We show that BC-IRL learns rewards that generalize better on an illustrative simple task and two continuous robotic control tasks, achieving over twice the success rate of baselines in challenging generaliz
    
[^7]: 通过迭代能量最小化进行序列模型规划

    Planning with Sequence Models through Iterative Energy Minimization. (arXiv:2303.16189v1 [cs.LG])

    [http://arxiv.org/abs/2303.16189](http://arxiv.org/abs/2303.16189)

    本文提出了一种基于迭代能量最小化方法的序列模型规划集成方法，能够提高强化学习性能在BabyAI和Atari等不同任务中具有应用价值。

    

    最近的研究表明，序列建模可以有效地用于训练强化学习（RL）策略。然而，将现有的序列模型应用于规划，即希望获得一系列动作的轨迹以达到某个目标，并不那么直接。序列模型的典型自回归生成过程排除了对较早步骤的顺序细化，这限制了预测计划的有效性。在本文中，我们提出了一种与迭代能量最小化相关的方法，将规划与序列模型进行集成，并说明这种过程如何在不同任务中导致改进的RL性能。我们训练了一个掩码语言模型，捕捉了动作轨迹上的隐式能量函数，并将规划形式化为寻找具有最小能量的动作轨迹。我们说明了这个过程如何在BabyAI和Atari环境下实现改进的性能。

    Recent works have shown that sequence modeling can be effectively used to train reinforcement learning (RL) policies. However, the success of applying existing sequence models to planning, in which we wish to obtain a trajectory of actions to reach some goal, is less straightforward. The typical autoregressive generation procedures of sequence models preclude sequential refinement of earlier steps, which limits the effectiveness of a predicted plan. In this paper, we suggest an approach towards integrating planning with sequence models based on the idea of iterative energy minimization, and illustrate how such a procedure leads to improved RL performance across different tasks. We train a masked language model to capture an implicit energy function over trajectories of actions, and formulate planning as finding a trajectory of actions with minimum energy. We illustrate how this procedure enables improved performance over recent approaches across BabyAI and Atari environments. We furthe
    
[^8]: 可视化思维传递模型

    Visual Chain-of-Thought Diffusion Models. (arXiv:2303.16187v1 [cs.CV])

    [http://arxiv.org/abs/2303.16187](http://arxiv.org/abs/2303.16187)

    本文提出了一种两阶段采样过程，使用可视化思维传递模型来缩小条件和无条件模型之间的差距，相对标准无条件生成，FID提高25-50%。

    

    条件图像扩散模型的近期进展是惊人的，这适用于以文本描述、场景布局或素描为条件的模型。无条件图像扩散模型也在改进，但落后于条件模型，以类标签为条件的扩散模型亦是如此。本文提出使用两阶段采样过程，缩小条件和无条件模型之间的差距。在第一阶段中，我们采样描述图像语义内容的嵌入。在第二阶段中，我们在这个嵌入的条件下采样图像，然后丢弃这个嵌入。这样做让我们利用条件扩散模型的强大功能来进行无条件生成任务，我们证明相对于标准无条件生成，FID（Frechet inception distance）最多提高了25-50%。

    Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation.
    
[^9]: 基于扩散映射的群不变流形学习

    Diffusion Maps for Group-Invariant Manifolds. (arXiv:2303.16169v1 [cs.LG])

    [http://arxiv.org/abs/2303.16169](http://arxiv.org/abs/2303.16169)

    本文提出了一种扩散映射算法用于群不变流形问题，通过积分在不变数据集上扩展出K-不变拉普拉斯算子，证明了可以利用K中的幺正不可约表示矩阵对其进行对角化，并给出特征值和特征向量的计算公式。同时，展示了规范化拉普拉斯算子L_N收敛于Laplace-Beltrami算子，收敛速度随着对称群K的维数增加而增加。

    

    本文考虑当数据集对紧Lie群K的作用具有不变性时，流形学习问题。我们的方法是通过在现有数据点K的轨道上积分，将数据诱导的图拉普拉斯算子扩展到K-不变算子L上。我们证明了可以使用K的幺正不可约表示矩阵来对角化K-不变算子L，并给出了计算L的特征值和特征向量的显式公式。此外，我们展示了规范化拉普拉斯算子L_N收敛到数据流形的Laplace-Beltrami算子，收敛速度得到改进，改进随着对称群K的维数增加而增加。本文将Landa和Shkolnisky的可转动图拉普拉斯框架从SO（2）的情况扩展到任意紧Lie群情况。

    In this article, we consider the manifold learning problem when the data set is invariant under the action of a compact Lie group $K$. Our approach consists in augmenting the data-induced graph Laplacian by integrating over orbits under the action of $K$ of the existing data points. We prove that this $K$-invariant Laplacian operator $L$ can be diagonalized by using the unitary irreducible representation matrices of $K$, and we provide an explicit formula for computing the eigenvalues and eigenvectors of $L$. Moreover, we show that the normalized Laplacian operator $L_N$ converges to the Laplace-Beltrami operator of the data manifold with an improved convergence rate, where the improvement grows with the dimension of the symmetry group $K$. This work extends the steerable graph Laplacian framework of Landa and Shkolnisky from the case of $\operatorname{SO}(2)$ to arbitrary compact Lie groups.
    
[^10]: 机器学习准确预测财报，但同样存在过度反应

    Behavioral Machine Learning? Computer Predictions of Corporate Earnings also Overreact. (arXiv:2303.16158v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.16158](http://arxiv.org/abs/2303.16158)

    本文研究发现，机器学习算法可以更准确地预测公司盈利，但同样存在过度反应的问题，而传统培训的股市分析师和经过机器学习方法培训的分析师相比会产生较少的过度反应。

    

    大量证据表明，在金融领域中，机器学习算法的预测能力比人类更为准确。但是，文献并未测试算法预测是否更为理性。本文研究了几个算法（包括线性回归和一种名为Gradient Boosted Regression Trees的流行算法）对于公司盈利的预测结果。结果发现，GBRT平均胜过线性回归和人类股市分析师，但仍存在过度反应且无法满足理性预期标准。通过降低学习率，可最小程度上减少过度反应程度，但这会牺牲预测准确性。通过机器学习方法培训过的股市分析师比传统训练的分析师产生的过度反应较少。此外，股市分析师的预测反映出机器算法没有捕捉到的信息。

    There is considerable evidence that machine learning algorithms have better predictive abilities than humans in various financial settings. But, the literature has not tested whether these algorithmic predictions are more rational than human predictions. We study the predictions of corporate earnings from several algorithms, notably linear regressions and a popular algorithm called Gradient Boosted Regression Trees (GBRT). On average, GBRT outperformed both linear regressions and human stock analysts, but it still overreacted to news and did not satisfy rational expectation as normally defined. By reducing the learning rate, the magnitude of overreaction can be minimized, but it comes with the cost of poorer out-of-sample prediction accuracy. Human stock analysts who have been trained in machine learning methods overreact less than traditionally trained analysts. Additionally, stock analyst predictions reflect information not otherwise available to machine algorithms.
    
[^11]: 引导迁移学习

    Guided Transfer Learning. (arXiv:2303.16154v1 [cs.LG])

    [http://arxiv.org/abs/2303.16154](http://arxiv.org/abs/2303.16154)

    提出了一种称为引导迁移学习的方法，在初始探索过程中学习每个网络参数的引导参数，从而在训练网络时减少资源需求。这种方法可以使网络从少量数据中学习，并让具有较少参数的网络学习一个原本只有一个较大的网络才能学习的任务。

    

    机器学习需要大量数据与计算。此外，模型还需要相对应的参数数量增长。因此，寻找减少资源需求的技术是明智的。本文提出了一种称为引导迁移学习的方法。网络中的每个权重和偏置都有自己的引导参数，指示在学习新任务时该参数允许改变多少。引导参数是在初始探索过程中学习的。引导迁移学习可以减少训练网络所需的资源。在一些应用中，引导迁移学习能够使网络从少量数据中学习。在其他情况下，一个具有较少参数的网络可以学习一个原本只有一个较大的网络才能学习的任务。当数据量、模型大小或计算资源的可用性达到极限时，引导迁移学习可能具有许多应用。

    Machine learning requires exuberant amounts of data and computation. Also, models require equally excessive growth in the number of parameters. It is, therefore, sensible to look for technologies that reduce these demands on resources. Here, we propose an approach called guided transfer learning. Each weight and bias in the network has its own guiding parameter that indicates how much this parameter is allowed to change while learning a new task. Guiding parameters are learned during an initial scouting process. Guided transfer learning can result in a reduction in resources needed to train a network. In some applications, guided transfer learning enables the network to learn from a small amount of data. In other cases, a network with a smaller number of parameters can learn a task which otherwise only a larger network could learn. Guided transfer learning potentially has many applications when the amount of data, model size, or the availability of computational resources reach their l
    
[^12]: 预测大型实现协方差矩阵:因子模型和收缩的好处。

    Forecasting Large Realized Covariance Matrices: The Benefits of Factor Models and Shrinkage. (arXiv:2303.16151v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.16151](http://arxiv.org/abs/2303.16151)

    本论文介绍了一种用因子模型和收缩的方法预测大型实现协方差矩阵的模型。这种方法通过分解回报协方差矩阵并使用向量异质自回归模型进行估计，相对于标准基准提高了预测精度，并导致对最小方差组合的更好估计。

    

    我们提出了一种模型来预测收益的大型实现协方差矩阵，并对S&P 500的成分股进行了应用。为了解决维数灾难，我们使用标准企业级别因子（如大小、价值和盈利能力）分解回报协方差矩阵，并在残差协方差矩阵中使用部门限制。然后，使用最小绝对收缩和选择运算符（LASSO）的向量异质自回归（VHAR）模型对该限制模型进行估计。相对于标准基准，我们的方法提高了预测精度，并导致对最小方差组合的更好估计。

    We propose a model to forecast large realized covariance matrices of returns, applying it to the constituents of the S\&P 500 daily. To address the curse of dimensionality, we decompose the return covariance matrix using standard firm-level factors (e.g., size, value, and profitability) and use sectoral restrictions in the residual covariance matrix. This restricted model is then estimated using vector heterogeneous autoregressive (VHAR) models with the least absolute shrinkage and selection operator (LASSO). Our methodology improves forecasting precision relative to standard benchmarks and leads to better estimates of minimum variance portfolios.
    
[^13]: VIDIMU: 使用价格实惠的设备记录日常生活活动的多模态视频和IMU运动学数据集

    VIDIMU. Multimodal video and IMU kinematic dataset on daily life activities using affordable devices. (arXiv:2303.16150v1 [cs.CV])

    [http://arxiv.org/abs/2303.16150](http://arxiv.org/abs/2303.16150)

    VIDIMU数据集使用商品相机和自定义传感器记录13种临床相关性的活动，为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。

    

    人体活动识别和临床生物力学是物理远程康复医学中的挑战性问题。然而，大多数公开可用的人体动作数据集不能用于研究实验室外运动获取情况下的这两个问题。VIDIMU数据集的目的是为远程日常生活活动识别和运动学分析提供价格实惠的患者追踪解决方案。该数据集包括使用商品相机和五个惯性传感器注册的13种活动。记录视频的54个受试者中，其中16个受试者同时还有惯性传感器记录。VIDIMU的创新之处在于：i）所选择的动作的临床相关性，ii）使用价格实惠的视频和自定义传感器的组合，以及 iii）实现了先进的多模态数据处理工具，可以从惯性数据中对三维身体姿势跟踪和运动重建在肌肉骨骼模型中。

    Human activity recognition and clinical biomechanics are challenging problems in physical telerehabilitation medicine. However, most publicly available datasets on human body movements cannot be used to study both problems in an out-of-the-lab movement acquisition setting. The objective of the VIDIMU dataset is to pave the way towards affordable patient tracking solutions for remote daily life activities recognition and kinematic analysis. The dataset includes 13 activities registered using a commodity camera and five inertial sensors. The video recordings were acquired in 54 subjects, of which 16 also had simultaneous recordings of inertial sensors. The novelty of VIDIMU lies in: i) the clinical relevance of the chosen movements, ii) the combined utilization of affordable video and custom sensors, and iii) the implementation of state-of-the-art tools for multimodal data processing of 3D body pose tracking and motion reconstruction in a musculoskeletal model from inertial data. The val
    
[^14]: 用机器学习解释宏观经济基本面对汇率预测的影响

    Explaining Exchange Rate Forecasts with Macroeconomic Fundamentals Using Interpretive Machine Learning. (arXiv:2303.16149v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.16149](http://arxiv.org/abs/2303.16149)

    本研究开发了一个基于基本面的模型来预测加拿大美元兑美国美元汇率，利用机器学习方法提高预测准确性，并发现原油作为加拿大主要商品对汇率有重要影响。

    

    金融和经济系统的复杂性和歧义性，以及经济环境的频繁变化，使得难以进行准确的预测，并得到受理论支持的解释。解释用于预测重要宏观经济指标的预测模型对于理解不同因素之间的关系、增强对预测模型的信任度以及使预测更具操作性具有高度的价值。在本研究中，我们开发了一个基于基本面的模型来预测加拿大美元兑美国美元汇率并在一个解释性框架内进行了分析。我们提出了一种综合方法，利用机器学习预测汇率，并采用可解释性方法准确分析宏观经济变量之间的关系。此外，我们根据解释的输出实施了消融研究，以提高模型的预测准确性。我们的实证结果表明，原油作为加拿大的主要商品，对汇率有很大的影响。

    The complexity and ambiguity of financial and economic systems, along with frequent changes in the economic environment, have made it difficult to make precise predictions that are supported by theory-consistent explanations. Interpreting the prediction models used for forecasting important macroeconomic indicators is highly valuable for understanding relations among different factors, increasing trust towards the prediction models, and making predictions more actionable. In this study, we develop a fundamental-based model for the Canadian-U.S. dollar exchange rate within an interpretative framework. We propose a comprehensive approach using machine learning to predict the exchange rate and employ interpretability methods to accurately analyze the relationships among macroeconomic variables. Moreover, we implement an ablation study based on the output of the interpretations to improve the predictive accuracy of the models. Our empirical results show that crude oil, as Canada's main com
    
[^15]: 加密货币价格因素的建模：一种贝叶斯网络方法

    Modelling Determinants of Cryptocurrency Prices: A Bayesian Network Approach. (arXiv:2303.16148v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.16148](http://arxiv.org/abs/2303.16148)

    本文使用贝叶斯网络方法，从因果分析的角度研究了影响替代加密货币价格的因素，包括五种主要替代加密货币、传统金融资产和社交媒体，提供了一种解决加密货币价格预测问题的方法。

    

    市场总值和替代比特币的加密货币数量的增长提供了投资机会，同时也增加了预测其价格波动的复杂度。在这个波动性相对较弱的市场中，预测加密货币价格的一个重要挑战是需要确定影响价格的因素。本研究的重点是从因果分析的角度研究影响替代比特币价格的因素，特别地，研究了五个主要的替代加密货币，包括黄金、石油和标准普尔500指数等传统金融资产以及社交媒体之间的相互作用。为了回答这个问题，我们创建了由五个传统金融资产的历史价格数据、社交媒体数据和替代加密货币价格数据构成的因果网络，这些网络用于因果推理和诊断。

    The growth of market capitalisation and the number of altcoins (cryptocurrencies other than Bitcoin) provide investment opportunities and complicate the prediction of their price movements. A significant challenge in this volatile and relatively immature market is the problem of predicting cryptocurrency prices which needs to identify the factors influencing these prices. The focus of this study is to investigate the factors influencing altcoin prices, and these factors have been investigated from a causal analysis perspective using Bayesian networks. In particular, studying the nature of interactions between five leading altcoins, traditional financial assets including gold, oil, and S\&P 500, and social media is the research question. To provide an answer to the question, we create causal networks which are built from the historic price data of five traditional financial assets, social media data, and price data of altcoins. The ensuing networks are used for causal reasoning and diag
    
[^16]: 基于联邦学习的COVID-19检测模型的比较研究

    A Comparative Study of Federated Learning Models for COVID-19 Detection. (arXiv:2303.16141v1 [cs.LG])

    [http://arxiv.org/abs/2303.16141](http://arxiv.org/abs/2303.16141)

    本文评估了五种联邦学习算法在COVID-19检测方面的性能和资源效率。在分散式环境下，循环权重传递可以获得更好的总体性能，在参与医院较少的情况下结果更好。

    

    深度学习在诊断COVID-19方面非常有效，但需要大量数据进行有效训练。由于数据和隐私法规的限制，医院一般无法访问其他医院的数据。联邦学习（FL）已被用来解决这个问题，它利用分布式的方式在医院中进行模型训练，以保护隐私。但是，部署FL并不总是可行的，因为它需要高计算和网络通信资源。本文评估了五种FL算法在COVID-19检测方面的性能和资源效率。我们搭建了一个带有CNN网络的分散式环境，并将FL算法的性能与集中式环境进行了比较。我们研究了参与者数量、联邦轮数和选择算法等因素对算法性能的影响。结果表明，循环权重传递可以获得更好的总体性能，在参与医院较少的情况下结果更好。我们的结果展示了FL算法在分散式环境中对COVID-19检测具有良好的性能和效率。

    Deep learning is effective in diagnosing COVID-19 and requires a large amount of data to be effectively trained. Due to data and privacy regulations, hospitals generally have no access to data from other hospitals. Federated learning (FL) has been used to solve this problem, where it utilizes a distributed setting to train models in hospitals in a privacy-preserving manner. Deploying FL is not always feasible as it requires high computation and network communication resources. This paper evaluates five FL algorithms' performance and resource efficiency for Covid-19 detection. A decentralized setting with CNN networks is set up, and the performance of FL algorithms is compared with a centralized environment. We examined the algorithms with varying numbers of participants, federated rounds, and selection algorithms. Our results show that cyclic weight transfer can have better overall performance, and results are better with fewer participating hospitals. Our results demonstrate good perf
    
[^17]: 用机器学习工具改进RC柱非线性建模参数

    Machine learning tools to improve nonlinear modeling parameters of RC columns. (arXiv:2303.16140v1 [cs.LG])

    [http://arxiv.org/abs/2303.16140](http://arxiv.org/abs/2303.16140)

    用机器学习工具可以显著提高预测RC柱非线性建模参数的准确性，并有助于确定其最可能的失效模式。

    

    建模参数对于混凝土结构的非线性模拟至关重要，特别是在模拟足以引起倒塌的地震事件时。本文解决了通过实验数据集提高地震评估标准中非线性建模要求的最大障碍之一：确定结构组件最可能的失效模式，并实施能够识别输入参数之间的相互依赖和输入参数与模型输出之间的非线性关系的数据拟合技术。本文使用Scikit-learn和Pytorch库中的机器学习工具来校准ASCE 41和ACI 369.1标准中强化混凝土柱的非线性建模参数MP a和b的方程和黑盒数值模型，并估计其最可能的失效模式。研究发现，机器学习回归模型和机器学习分类器可以显著提高预测MP a和b的准确性，并有助于确定RC柱的最可能失效模式。

    Modeling parameters are essential to the fidelity of nonlinear models of concrete structures subjected to earthquake ground motions, especially when simulating seismic events strong enough to cause collapse. This paper addresses two of the most significant barriers to improving nonlinear modeling provisions in seismic evaluation standards using experimental data sets: identifying the most likely mode of failure of structural components, and implementing data fitting techniques capable of recognizing interdependencies between input parameters and nonlinear relationships between input parameters and model outputs. Machine learning tools in the Scikit-learn and Pytorch libraries were used to calibrate equations and black-box numerical models for nonlinear modeling parameters (MP) a and b of reinforced concrete columns defined in the ASCE 41 and ACI 369.1 standards, and to estimate their most likely mode of failure. It was found that machine learning regression models and machine learning 
    
[^18]: 揭示和解决统一视觉-语言模型中的跨任务不一致问题

    Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models. (arXiv:2303.16133v1 [cs.CV])

    [http://arxiv.org/abs/2303.16133](http://arxiv.org/abs/2303.16133)

    该研究提出了一个基准数据集COCOCON，并提出度量方法来衡量模型一致性，研究发现现有的最先进系统在不同任务之间表现出高度不一致性。

    

    随着通用的视觉模型在不同任务上变得越来越有效，保证它们在各自支持的任务中的一致性是非常重要的。人们认为不一致的人工智能模型是不可靠的，这对于依赖它们输出的大型系统来说是更具挑战性的。由于很难确定预测结果是否一致，因此，评估可能包括不同模态输出的非常异构任务之间的一致性是具有挑战性的。因此，我们提出了基准数据集COCOCON，其中我们使用对多个任务的测试实例进行小型但语义上有意义的修改来创建对比集，以更改金标签，并概述了用于通过对比接近原始和修改后的实例来衡量模型一致性的指标。我们发现，最先进的系统在任务之间表现出惊人的不一致性。

    As general purpose vision models get increasingly effective at a wide set of tasks, it is imperative that they be consistent across the tasks they support. Inconsistent AI models are considered brittle and untrustworthy by human users and are more challenging to incorporate into larger systems that take dependencies on their outputs. Measuring consistency between very heterogeneous tasks that might include outputs in different modalities is challenging since it is difficult to determine if the predictions are consistent with one another. As a solution, we introduce a benchmark dataset, COCOCON, where we use contrast sets created by modifying test instances for multiple tasks in small but semantically meaningful ways to change the gold label, and outline metrics for measuring if a model is consistent by ranking the original and perturbed instances across tasks. We find that state-of-the-art systems suffer from a surprisingly high degree of inconsistent behavior across tasks, especially 
    
[^19]: Transformer和Snowball图卷积学习用于生物医学图分类

    Transformer and Snowball Graph Convolution Learning for Biomedical Graph Classification. (arXiv:2303.16132v1 [cs.LG])

    [http://arxiv.org/abs/2303.16132](http://arxiv.org/abs/2303.16132)

    本文介绍了一种新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs。TSEN通过雪球编码层将图雪球连接与图Transformer结合起来，增强了捕捉多尺度信息和全局模式以学习整个图特征的能力。

    

    图或网络已被广泛用于描述和建模生物医学中的复杂系统。深度学习方法，尤其是图神经网络（GNNs），已被开发用于学习和预测这种结构化数据。在本文中，我们提出了一种用于生物医学图分类的新型Transformer和Snowball编码网络（TSEN），它将Transformer架构和图雪球连接引入GNNs，以学习整个图的表示。

    Graph or network has been widely used for describing and modeling complex systems in biomedicine. Deep learning methods, especially graph neural networks (GNNs), have been developed to learn and predict with such structured data. In this paper, we proposed a novel transformer and snowball encoding networks (TSEN) for biomedical graph classification, which introduced transformer architecture with graph snowball connection into GNNs for learning whole-graph representation. TSEN combined graph snowball connection with graph transformer by snowball encoding layers, which enhanced the power to capture multi-scale information and global patterns to learn the whole-graph features. On the other hand, TSEN also used snowball graph convolution as position embedding in transformer structure, which was a simple yet effective method for capturing local patterns naturally. Results of experiments using four graph classification datasets demonstrated that TSEN outperformed the state-of-the-art typical
    
[^20]: 面向金融数据科学竞赛的多元时间序列数据特征工程方法

    Feature Engineering Methods on Multivariate Time-Series Data for Financial Data Science Competitions. (arXiv:2303.16117v1 [q-fin.ST])

    [http://arxiv.org/abs/2303.16117](http://arxiv.org/abs/2303.16117)

    针对金融数据科学竞赛，本研究尝试采用多元时间序列特征工程方法，利用美国市场价格数据进行测试，并验证其在Numerai-Signals目标上的预测能力。

    

    我们应用不同的时间序列特征工程方法对美国市场价格数据进行处理，并测试模型在Numerai-Signals目标上的预测能力。

    We apply different feature engineering methods for time-series to US market price data. The predictive power of models are tested against Numerai-Signals targets.
    
[^21]: 通过纠错保持不变量在机器学习PDE求解器中的应用

    Invariant preservation in machine learned PDE solvers via error correction. (arXiv:2303.16110v1 [math.NA])

    [http://arxiv.org/abs/2303.16110](http://arxiv.org/abs/2303.16110)

    本文研究通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器，关键在于通过纠错保持不变量。

    

    机器学习的偏微分方程求解器在可能带来的准确性和/或速度上的潜在收益换取标准数值方法的可靠性。保证求解器输出精确解的唯一方法是在网格间距$\Delta x$和时间步长$\Delta t$趋近于零的极限下使用收敛方法。学会在大$\Delta x$和/或$\Delta t$下更新解的机器学习求解器永远无法保证完美的准确性。一定程度的误差是不可避免的，因此问题是：我们如何限制机器学习求解器以给出我们愿意容忍的错误？在本文中，我们通过保持基本偏微分方程的离散模拟中的连续不变量来设计更可靠的机器学习PDE求解器。这类不变量的示例包括质量守恒、能量守恒、热力学第二定律和（或）非负密度。我们的关键见解很简单：为了保持不变量，我们强制求解器的误差与满足相关不变量的离散函数空间正交。我们在几个示例PDE上演示了这一思想，包括非线性Burgers方程、Navier-Stokes方程和不可压缩的Euler方程。

    Machine learned partial differential equation (PDE) solvers trade the reliability of standard numerical methods for potential gains in accuracy and/or speed. The only way for a solver to guarantee that it outputs the exact solution is to use a convergent method in the limit that the grid spacing $\Delta x$ and timestep $\Delta t$ approach zero. Machine learned solvers, which learn to update the solution at large $\Delta x$ and/or $\Delta t$, can never guarantee perfect accuracy. Some amount of error is inevitable, so the question becomes: how do we constrain machine learned solvers to give us the sorts of errors that we are willing to tolerate? In this paper, we design more reliable machine learned PDE solvers by preserving discrete analogues of the continuous invariants of the underlying PDE. Examples of such invariants include conservation of mass, conservation of energy, the second law of thermodynamics, and/or non-negative density. Our key insight is simple: to preserve invariants,
    
[^22]: 基于Transformer网络的自动驾驶车辆多模态操作和轨迹预测

    Multimodal Manoeuvre and Trajectory Prediction for Autonomous Vehicles Using Transformer Networks. (arXiv:2303.16109v1 [cs.LG])

    [http://arxiv.org/abs/2303.16109](http://arxiv.org/abs/2303.16109)

    提出了一种新颖的基于Transformer网络的多模态操作和轨迹预测框架，能够预测多个可能的行为模式及其概率，并且在两个公共基准公路驾驶数据集上得到了优秀的表现。

    

    预测其他道路用户（包括车辆）的行为（即操作/轨迹）对于自动驾驶车辆（AVs）或自动化驾驶系统（ADSs）的安全和高效运行至关重要。由于车辆未来行为的不确定性，对于给定的行驶场景，一个车辆通常存在多个未来行为模式是合理的。因此，多模态预测可以提供比单模态预测更丰富的信息，使AV能够更好地评估风险。为此，我们提出了一种新颖的多模态预测框架，可以预测多个可能的行为模式及其概率。该框架包括一个定制的操作预测问题公式，一个基于Transformer的预测模型，以及一种定制的多模态操作和轨迹预测的训练方法。使用两个公共基准公路驾驶数据集（即NGSIM和highD）评估了该框架的性能。结果表明，该框架在多模态操作和轨迹预测方面的表现得到了优化。

    Predicting the behaviour (i.e. manoeuvre/trajectory) of other road users, including vehicles, is critical for the safe and efficient operation of autonomous vehicles (AVs), a.k.a. automated driving systems (ADSs). Due to the uncertain future behaviour of vehicles, multiple future behaviour modes are often plausible for a vehicle in a given driving scene. Therefore, multimodal prediction can provide richer information than single-mode prediction enabling AVs to perform a better risk assessment. To this end, we propose a novel multimodal prediction framework that can predict multiple plausible behaviour modes and their likelihoods. The proposed framework includes a bespoke problem formulation for manoeuvre prediction, a novel transformer-based prediction model, and a tailored training method for multimodal manoeuvre and trajectory prediction. The performance of the framework is evaluated using two public benchmark highway driving datasets, namely NGSIM and highD. The results show that th
    
[^23]: 基于共同子表达式的稀疏常数矩阵压缩和乘法

    Common Subexpression-based Compression and Multiplication of Sparse Constant Matrices. (arXiv:2303.16106v1 [cs.LG])

    [http://arxiv.org/abs/2303.16106](http://arxiv.org/abs/2303.16106)

    本文提出了一种基于随机搜索的算法来提取稀疏常数矩阵列对中的共同子表达式，使用加法树压缩这些表达式可以实现超过5倍的压缩率，同时在CSE提取和矩阵乘法方面显著加速。

    

    在深度学习推理中，模型参数被修剪和量化以减少模型大小。压缩方法和共同子表达式消除算法被应用于稀疏常数矩阵以在低成本嵌入式设备上部署模型。然而，最先进的共同子表达式消除方法在处理大型矩阵时缩放并不良好。在200x200矩阵中提取CSE需要几个小时，而它们的矩阵乘法算法比传统矩阵乘法方法执行时间更长。此外，不存在利用CSE的矩阵压缩方法。为了解决这个问题，本文提出了一种基于随机搜索的算法来提取常数矩阵列对中的CSE。它可在一分钟内为1000x1000矩阵生成加法树。为了压缩加法树，本文提出了一种扩展压缩稀疏行（CSR）以包括CSE的压缩格式。虽然可以实现超过5倍的压缩率，但所提出的算法在CSE提取和矩阵乘法方面都具有显着的加速作用，使它们更适合在低成本嵌入式设备上部署。

    In deep learning inference, model parameters are pruned and quantized to reduce the model size. Compression methods and common subexpression (CSE) elimination algorithms are applied on sparse constant matrices to deploy the models on low-cost embedded devices. However, the state-of-the-art CSE elimination methods do not scale well for handling large matrices. They reach hours for extracting CSEs in a $200 \times 200$ matrix while their matrix multiplication algorithms execute longer than the conventional matrix multiplication methods. Besides, there exist no compression methods for matrices utilizing CSEs. As a remedy to this problem, a random search-based algorithm is proposed in this paper to extract CSEs in the column pairs of a constant matrix. It produces an adder tree for a $1000 \times 1000$ matrix in a minute. To compress the adder tree, this paper presents a compression format by extending the Compressed Sparse Row (CSR) to include CSEs. While compression rates of more than $5
    
[^24]: 无监督文本到图像生成的变分分布学习

    Variational Distribution Learning for Unsupervised Text-to-Image Generation. (arXiv:2303.16105v1 [cs.CV])

    [http://arxiv.org/abs/2303.16105](http://arxiv.org/abs/2303.16105)

    本文基于CLIP模型，提出了一种无监督文本到图像生成的算法，通过最大化数据对数似然优化文本到图像生成模型。实验结果表明，该算法在无监督和半监督文本到图像生成任务中，优于现有方法。

    

    本文提出了一种基于深度神经网络的文本到图像生成算法，在训练过程中没有图像的文本说明。我们使用预训练的CLIP模型来生成图像与对应文本的嵌入，从而更好地将两个域中的数据对齐，并以数据对的图像-文本CLIP嵌入为条件，通过最大化数据对数似然来优化文本到图像生成模型。为了更好地对齐这两个域中的数据，我们基于变分推断提出了一种合理的方法，有效地估计了给定图像及其CLIP特征的隐藏文本嵌入的近似后验分布。实验结果表明，在无监督和半监督文本到图像生成中，所提出的框架比现有方法有着更好的表现。

    We propose a text-to-image generation algorithm based on deep neural networks when text captions for images are unavailable during training. In this work, instead of simply generating pseudo-ground-truth sentences of training images using existing image captioning methods, we employ a pretrained CLIP model, which is capable of properly aligning embeddings of images and corresponding texts in a joint space and, consequently, works well on zero-shot recognition tasks. We optimize a text-to-image generation model by maximizing the data log-likelihood conditioned on pairs of image-text CLIP embeddings. To better align data in the two domains, we employ a principled way based on a variational inference, which efficiently estimates an approximate posterior of the hidden text embedding given an image and its CLIP feature. Experimental results validate that the proposed framework outperforms existing approaches by large margins under unsupervised and semi-supervised text-to-image generation se
    
[^25]: 基于异构内存结构的边缘NLP推理的节能任务适应性

    Energy-efficient Task Adaptation for NLP Edge Inference Leveraging Heterogeneous Memory Architectures. (arXiv:2303.16100v1 [cs.LG])

    [http://arxiv.org/abs/2303.16100](http://arxiv.org/abs/2303.16100)

    adapter-ALBERT是一种高效的NLP模型优化方法，可以在实现多任务推理同时最大程度地实现数据重用，并提高对数据压缩方法的鲁棒性。

    

    在资源受限的边缘设备上执行机器学习推理任务需要仔细的硬件-软件协同设计优化。最近的研究表明，基于变压器的深度神经网络模型，如ALBERT可以用于在移动系统级芯片上执行自然语言处理（NLP）推理，并配备自定义硬件加速器。然而，这些现有的解决方案虽然可以有效地缓解运行单个NLP任务的延迟、能量和面积成本，但对于实现多任务推理，需要在针对每个目标任务的多个模型参数的基础上执行计算。这种方法会导致过高的芯片内存需求或支付芯片外部存储器访问的成本。本文提出了一种高效的模型优化方法，即adapter-ALBERT，用于在不同任务之间实现最大化数据重用性。该模型的性能和对数据压缩方法的鲁棒性进行了评估。

    Executing machine learning inference tasks on resource-constrained edge devices requires careful hardware-software co-design optimizations. Recent examples have shown how transformer-based deep neural network models such as ALBERT can be used to enable the execution of natural language processing (NLP) inference on mobile systems-on-chip housing custom hardware accelerators. However, while these existing solutions are effective in alleviating the latency, energy, and area costs of running single NLP tasks, achieving multi-task inference requires running computations over multiple variants of the model parameters, which are tailored to each of the targeted tasks. This approach leads to either prohibitive on-chip memory requirements or paying the cost of off-chip memory access. This paper proposes adapter-ALBERT, an efficient model optimization for maximal data reuse across different tasks. The proposed model's performance and robustness to data compression methods are evaluated across s
    
[^26]: 基于注意力机制的自编码器在建筑能耗异常检测中的应用

    Attention Boosted Autoencoder for Building Energy Anomaly Detection. (arXiv:2303.16097v1 [cs.LG])

    [http://arxiv.org/abs/2303.16097](http://arxiv.org/abs/2303.16097)

    本论文提出了一种基于注意力机制的自编码器模型，可用于建筑能耗异常检测，同时使用可视化方法辅助理解模型所捕捉到的关系。

    

    利用建筑智能电表收集的数据可以帮助制定节能政策。如果能够及早检测建筑运行状况的偏差，并采取适当的措施，将可以实现显著的节能效果。为此，可以使用机器学习技术自动发现收集数据中的异常模式。目前异常检测中的方法依赖于一个基础模型来捕捉正常或可接受的运行行为。本文提出了一种新颖的注意力机制来建模建筑的能耗行为，并通过样例研究展示了模型在捕捉这些关系方面的有效性。使用所提出的架构对真实世界的数据集进行建模，并呈现了结果。还提出了一种可视化方法来理解模型所捕捉到的关系。

    Leveraging data collected from smart meters in buildings can aid in developing policies towards energy conservation. Significant energy savings could be realised if deviations in the building operating conditions are detected early, and appropriate measures are taken. Towards this end, machine learning techniques can be used to automate the discovery of these abnormal patterns in the collected data. Current methods in anomaly detection rely on an underlying model to capture the usual or acceptable operating behaviour. In this paper, we propose a novel attention mechanism to model the consumption behaviour of a building and demonstrate the effectiveness of the model in capturing the relations using sample case studies. A real-world dataset is modelled using the proposed architecture, and the results are presented. A visualisation approach towards understanding the relations captured by the model is also presented.
    
[^27]: 基于GNN的相空间积分在原子学中的应用

    GNN-Assisted Phase Space Integration with Application to Atomistics. (arXiv:2303.16088v1 [physics.comp-ph])

    [http://arxiv.org/abs/2303.16088](http://arxiv.org/abs/2303.16088)

    本文在原子学中提出了一种基于GNN的相空间积分方法，通过训练图神经网络可以替代传统数值积分规则，从而显着提高模拟精度。

    

    通过从分子动力学（MD）的状态空间表示转换为基于统计力学的相空间表示，可以克服原子学的时间尺度限制。在相空间中，最大熵或高斯相包（GPP）等近似方法以时间缩减的方式演化原子集合。在实践中，这需要计算原子集合在整个相空间中昂贵的高维积分。常常利用低阶数值积分来高效完成这一点。我们展示了此上下文中的数值积分存在固有问题，特别是处理带有缺陷的晶格时，会损坏模拟的准确性。为此，我们证明了在Monte-Carlo数据上训练的图神经网络可以作为常用数值积分规则的替代品，克服了它们的缺陷并显着提高了模拟精度。

    Overcoming the time scale limitations of atomistics can be achieved by switching from the state-space representation of Molecular Dynamics (MD) to a statistical-mechanics-based representation in phase space, where approximations such as maximum-entropy or Gaussian phase packets (GPP) evolve the atomistic ensemble in a time-coarsened fashion. In practice, this requires the computation of expensive high-dimensional integrals over all of phase space of an atomistic ensemble. This, in turn, is commonly accomplished efficiently by low-order numerical quadrature. We show that numerical quadrature in this context, unfortunately, comes with a set of inherent problems, which corrupt the accuracy of simulations -- especially when dealing with crystal lattices with imperfections. As a remedy, we demonstrate that Graph Neural Networks, trained on Monte-Carlo data, can serve as a replacement for commonly used numerical quadrature rules, overcoming their deficiencies and significantly improving the 
    
[^28]: 光学星间链路联合学习中的边缘选择和聚类

    Edge Selection and Clustering for Federated Learning in Optical Inter-LEO Satellite Constellation. (arXiv:2303.16071v1 [cs.LG])

    [http://arxiv.org/abs/2303.16071](http://arxiv.org/abs/2303.16071)

    FedLEO是一种在超密集LEO卫星星座上进行联合学习的方法，其在LEO上进行视频/数据传输和聚类，而低时延的地面网关服务器 (GS) 仅负责初始信号控制。随着LEO服务器的变化和重新聚类，FedLEO能够适应动态LEO星座并在不同的LEO星座设置下实现高效的学习性能。

    

    由于低地球轨道 (LEO) 卫星可以收集大量的影像或传感器数据，因此已经被广泛用于各种地球观测任务。然而，传统上，数据训练过程是在地面云服务器上进行的，这导致了很高的传输开销。最近 LEO 的发展更迫切地需要提供具备增强的机载计算能力的超密集 LEO 卫星星座。基于此，我们提出了一种在 LEO 卫星星座上进行协作联合学习的方法 (FedLEO)。我们将整个过程都分配在具备低载荷星间传输的 LEO 上，而低时延的地面网关服务器 (GS) 仅负责初始信号控制。GS 首先选择一个 LEO 服务器，而其 LEO 客户端都通过聚类机制和光学星间链路(ISLs)的通信能力决定。更换 LEO 服务器时的重新聚类将触发新一轮的 LEO 客户端聚类，从而使 FedLEO 能够适应动态 LEO 星座。仿真结果表明，FedLEO 可以在不同的 LEO 星座设置下实现高效的学习性能。

    Low-Earth orbit (LEO) satellites have been prosperously deployed for various Earth observation missions due to its capability of collecting a large amount of image or sensor data. However, traditionally, the data training process is performed in the terrestrial cloud server, which leads to a high transmission overhead. With the recent development of LEO, it is more imperative to provide ultra-dense LEO constellation with enhanced on-board computation capability. Benefited from it, we have proposed a collaborative federated learning over LEO satellite constellation (FedLEO). We allocate the entire process on LEOs with low payload inter-satellite transmissions, whilst the low-delay terrestrial gateway server (GS) only takes care for initial signal controlling. The GS initially selects an LEO server, whereas its LEO clients are all determined by clustering mechanism and communication capability through the optical inter-satellite links (ISLs). The re-clustering of changing LEO server will
    
[^29]: 惰性学习：一种受生物启发的快速、节能突触可塑性规则

    Lazy learning: a biologically-inspired plasticity rule for fast and energy efficient synaptic plasticity. (arXiv:2303.16067v1 [cs.NE])

    [http://arxiv.org/abs/2303.16067](http://arxiv.org/abs/2303.16067)

    使用惰性学习只对错误样本进行更新参数，实现快速、节能的神经网络训练，并在单层MLP模型上达到了99.2％的测试准确率，比匹配的反向传播网络快7.6倍。

    

    在使用反向传播训练神经网络进行分类任务时，即使样本被正确分类，参数也会在每个试验中更新。相比之下，人类集中学习差错。受人类学习的启发，我们引入了惰性学习，仅对错误样本进行学习。惰性学习可以用几行代码实现，无需超参数调整。惰性学习实现了最先进的性能，在数据集较大时特别适用。例如，在使用单层MLP对扩展MNIST进行测试准确率达到99.2％，并且比匹配反向传播网络快7.6倍。

    When training neural networks for classification tasks with backpropagation, parameters are updated on every trial, even if the sample is classified correctly. In contrast, humans concentrate their learning effort on errors. Inspired by human learning, we introduce lazy learning, which only learns on incorrect samples. Lazy learning can be implemented in a few lines of code and requires no hyperparameter tuning. Lazy learning achieves state-of-the-art performance and is particularly suited when datasets are large. For instance, it reaches 99.2% test accuracy on Extended MNIST using a single-layer MLP, and does so 7.6x faster than a matched backprop network
    
[^30]: 受神经衰竭启示的非独立同分布数据联邦学习

    Neural Collapse Inspired Federated Learning with Non-iid Data. (arXiv:2303.16066v1 [cs.LG])

    [http://arxiv.org/abs/2303.16066](http://arxiv.org/abs/2303.16066)

    本文提出了一种受神经衰竭启示的方案，通过将每个客户端优化向全局分类的最佳结构，解决了联邦学习中非独立同分布数据的挑战，并通过添加全局记忆向量来补救参数波动的问题。

    

    联邦学习中的主要挑战之一是异构设备之间的非独立同分布（非iid）特性，导致本地更新存在显著差异，影响中央服务器的性能。尽管已经提出了许多研究来解决这个挑战，但它们只关注本地训练和聚合过程以平滑变化，并未在深度学习模型中实现高性能。受神经衰竭现象启发，我们强制每个客户端优化向全局分类的最佳结构。具体而言，我们将其初始化为随机的简单六角紧框架（ETF），并在本地更新期间将其作为所有客户端的单元优化目标进行固定。在确保所有客户端学习收敛于全局最优解之后，我们提出为每个类别添加全局记忆向量，以补救由于类内条件分布偏差引起的参数波动。

    One of the challenges in federated learning is the non-independent and identically distributed (non-iid) characteristics between heterogeneous devices, which cause significant differences in local updates and affect the performance of the central server. Although many studies have been proposed to address this challenge, they only focus on local training and aggregation processes to smooth the changes and fail to achieve high performance with deep learning models. Inspired by the phenomenon of neural collapse, we force each client to be optimized toward an optimal global structure for classification. Specifically, we initialize it as a random simplex Equiangular Tight Frame (ETF) and fix it as the unit optimization target of all clients during the local updating. After guaranteeing all clients are learning to converge to the global optimum, we propose to add a global memory vector for each category to remedy the parameter fluctuation caused by the bias of the intra-class condition dist
    
[^31]: 基于变分能量模型的信息理论 GAN 压缩

    Information-Theoretic GAN Compression with Variational Energy-based Model. (arXiv:2303.16050v1 [cs.CV])

    [http://arxiv.org/abs/2303.16050](http://arxiv.org/abs/2303.16050)

    本论文提出了一种基于信息理论知识蒸馏的 GAN 压缩方法，它采用了基于能量模型的变分优化，最大化教师和学生网络之间的互信息下限，该方法是一种通用的优化算法，适用于任何 GAN 或密集预测网络，已在图像增强领域证明了其有效性。

    

    我们提出了一种基于信息理论知识蒸馏的生成对抗网络（GAN）压缩方法，旨在通过基于能量模型的变分优化，最大化教师和学生网络之间的互信息。由于在连续域中直接计算互信息是不可计算的，因此我们的方法通过最大化互信息的变分下限代替优化学生网络。为了实现紧密的下限，我们引入了一个基于能量的模型，依赖于一个深度神经网络来表示处理高维图像和像素间的空间依赖关系的灵活变分分布。由于这个方法是通用的优化算法，它可以方便地加入到任何生成对抗网络中，甚至是密集预测网络，例如图像增强模型。我们证明了所提出算法的有效性。

    We propose an information-theoretic knowledge distillation approach for the compression of generative adversarial networks, which aims to maximize the mutual information between teacher and student networks via a variational optimization based on an energy-based model. Because the direct computation of the mutual information in continuous domains is intractable, our approach alternatively optimizes the student network by maximizing the variational lower bound of the mutual information. To achieve a tight lower bound, we introduce an energy-based model relying on a deep neural network to represent a flexible variational distribution that deals with high-dimensional images and consider spatial dependencies between pixels, effectively. Since the proposed method is a generic optimization algorithm, it can be conveniently incorporated into arbitrary generative adversarial networks and even dense prediction networks, e.g., image enhancement models. We demonstrate that the proposed algorithm 
    
[^32]: 理解和探索稀疏广义可加模型的整个优秀集合

    Understanding and Exploring the Whole Set of Good Sparse Generalized Additive Models. (arXiv:2303.16047v1 [cs.LG])

    [http://arxiv.org/abs/2303.16047](http://arxiv.org/abs/2303.16047)

    提出一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术，并使用这些近似模型来解决实际应用的挑战。

    

    在实际应用中，机器学习模型与领域专家之间的交互至关重要；然而，通常只生成单个模型的经典机器学习范式不利于此类交互。近似和探索Rashomon集，即所有近乎最优模型的集合，通过提供用户可搜索的空间包含多样性模型的方法，解决了这一实际挑战，领域专家可以从中选择。我们提出了一种有效而准确地近似稀疏广义可加模型（GAMs）的Rashomon集的技术。我们提供了用于近似具有固定支持集的GAMs的Rashomon集的椭球形算法，并使用这些椭球形近似了许多不同支持集的Rashomon集。近似的Rashomon集为解决实际挑战，例如（1）研究模型类的变量重要性；（2）在用户指定约束条件下查找模型，提供了重要的基础。

    In real applications, interaction between machine learning model and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present a technique to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models (GAMs). We present algorithms to approximate the Rashomon set of GAMs with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity
    
[^33]: 基于图表示学习的恶意软件检测综述

    A Survey on Malware Detection with Graph Representation Learning. (arXiv:2303.16004v1 [cs.CR])

    [http://arxiv.org/abs/2303.16004](http://arxiv.org/abs/2303.16004)

    本综述对基于图表示学习的恶意软件检测进行了深入审查和总结，这是一种比传统方法更加健壮的解决方案。

    

    随着恶意软件数量和复杂性的增加，恶意软件检测已成为一个重要的问题。传统的基于签名和启发式的检测方法用于恶意软件检测，但不幸的是，它们在未知攻击方面的泛化能力较差，并且可以通过混淆技术轻松规避。近年来，机器学习（ML）和特别是深度学习（DL）通过从数据中学习有用的表示来在恶意软件检测方面取得了显著的成果，并且已成为传统方法的首选解决方案。最近，在基于图结构的数据上应用这些技术已在各个领域取得了最先进的性能，并展示了从恶意软件中学习更健壮表示的有希望的结果。然而，还没有关于基于图形深度学习进行恶意软件检测的文献综述。在本综述中，我们提供了一份深入的文献综述，概括和统一了现有的作品。

    Malware detection has become a major concern due to the increasing number and complexity of malware. Traditional detection methods based on signatures and heuristics are used for malware detection, but unfortunately, they suffer from poor generalization to unknown attacks and can be easily circumvented using obfuscation techniques. In recent years, Machine Learning (ML) and notably Deep Learning (DL) achieved impressive results in malware detection by learning useful representations from data and have become a solution preferred over traditional methods. More recently, the application of such techniques on graph-structured data has achieved state-of-the-art performance in various domains and demonstrates promising results in learning more robust representations from malware. Yet, no literature review focusing on graph-based deep learning for malware detection exists. In this survey, we provide an in-depth literature review to summarize and unify existing works under the common approach
    
[^34]: 一种基于半监督回归深度学习模型的简绸纱线数目计算方法以应用于古老油画

    Thread Counting in Plain Weave for Old Paintings Using Semi-Supervised Regression Deep Learning Models. (arXiv:2303.15999v1 [cs.CV])

    [http://arxiv.org/abs/2303.15999](http://arxiv.org/abs/2303.15999)

    本文开发了一种基于深度学习的半监督回归模型，可直接从图片中计算简绸纱线的线密度，以应用于古老油画，为艺术品保护和修复提供了可靠的线密度估计方法。

    

    本文作者开发了一种基于深度学习的回归方法，以执行对简绸纱线密度的估计，以进行画布分析。该新方法可以直接从图像中计算线密度，并在训练过程中使用了专家知识。该方法在艺术保存和修复领域具有潜在应用。

    In this work the authors develop regression approaches based on deep learning to perform thread density estimation for plain weave canvas analysis. Previous approaches were based on Fourier analysis, that are quite robust for some scenarios but fail in some other, in machine learning tools, that involve pre-labeling of the painting at hand, or the segmentation of thread crossing points, that provides good estimations in all scenarios with no need of pre-labeling. The segmentation approach is time-consuming as estimation of the densities is performed after locating the crossing points. In this novel proposal, we avoid this step by computing the density of threads directly from the image with a regression deep learning model. We also incorporate some improvements in the initial preprocessing of the input image with an impact on the final error. Several models are proposed and analyzed to retain the best one. Furthermore, we further reduce the density estimation error by introducing a sem
    
[^35]: 面向资源受限的无线边缘网络的高效并行分裂学习

    Efficient Parallel Split Learning over Resource-constrained Wireless Edge Networks. (arXiv:2303.15991v1 [cs.LG])

    [http://arxiv.org/abs/2303.15991](http://arxiv.org/abs/2303.15991)

    本文提出了面向资源受限的无线边缘网络的高效并行分裂学习（EPSL）框架，旨在加速模型训练。EPSL并行化客户端模型训练，通过聚合梯度降低了反向传播的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。同时，EPSL还设计了资源分配算法以优化计算和通信资源分配。

    

    随着神经网络越来越深，这阻碍了联合学习等隐私增强分布式学习方式（如联邦学习）在资源受限的设备上的民主化。为了解决这个挑战，本文倡导将边缘计算范式和并行分裂学习（PSL）相结合，允许多个客户端设备通过逐层模型分裂将大量的训练工作负载卸载到边缘服务器上。通过观察到现有的PSL方案会产生过多的训练延迟和大量的数据传输，我们提出了一种创新的PSL框架——高效并行分裂学习（EPSL），以加速模型训练。具体而言，EPSL将客户端模型训练并行化，并通过最后一层梯度聚合降低了反向传播（BP）的局部梯度维度，从而显著减少了服务器端的训练和通信延迟。此外，通过考虑边缘设备的异构通道条件和计算能力，我们设计了资源分配算法以优化计算和通信资源分配。实验结果表明，EPSL通过将通信成本和训练时间分别降低76％和63％，优于最先进的PSL方法。

    The increasingly deeper neural networks hinder the democratization of privacy-enhancing distributed learning, such as federated learning (FL), to resource-constrained devices. To overcome this challenge, in this paper, we advocate the integration of edge computing paradigm and parallel split learning (PSL), allowing multiple client devices to offload substantial training workloads to an edge server via layer-wise model split. By observing that existing PSL schemes incur excessive training latency and large volume of data transmissions, we propose an innovative PSL framework, namely, efficient parallel split learning (EPSL), to accelerate model training. To be specific, EPSL parallelizes client-side model training and reduces the dimension of local gradients for back propagation (BP) via last-layer gradient aggregation, leading to a significant reduction in server-side training and communication latency. Moreover, by considering the heterogeneous channel conditions and computing capabil
    
[^36]: 大规模预训练模型在增量式新类别发现中具有出乎意料的强大表现。

    Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery. (arXiv:2303.15975v1 [cs.CV])

    [http://arxiv.org/abs/2303.15975](http://arxiv.org/abs/2303.15975)

    本论文提出了一种更加挑战性和实用性的学习方法MSc-iNCD，通过在连续而无人监督的学习中利用大规模预训练模型的丰富先验知识，该方法在增量式新类别发现中表现出出乎意料的强大实力。

    

    在生命长学习者中，从未标记的数据中连续地发现新概念是一个重要的期望。在文献中，这类问题在非常受限的情况下得到了部分解决，其中要么为发现新概念提供有标号的数据（例如 NCD），要么学习在有限数量的增量步骤中发生（例如类 iNCD）。在这项工作中，我们挑战现状，提出了一种更具挑战性和实用性的学习范式，称为 MSc-iNCD，其中学习连续而无人监督，并利用大规模预训练模型的丰富先验知识。为此，我们提出了简单的基线，不仅在较长的学习情境下具有弹性，而且与复杂的最先进方法相比，表现出出乎意料的强大实力。我们在多个基准测试中进行了广泛的实证评估，并展示了我们提出的基线的有效性，大大提升了基准要求。

    Discovering novel concepts from unlabelled data and in a continuous manner is an important desideratum of lifelong learners. In the literature such problems have been partially addressed under very restricted settings, where either access to labelled data is provided for discovering novel concepts (e.g., NCD) or learning occurs for a limited number of incremental steps (e.g., class-iNCD). In this work we challenge the status quo and propose a more challenging and practical learning paradigm called MSc-iNCD, where learning occurs continuously and unsupervisedly, while exploiting the rich priors from large-scale pre-trained models. To this end, we propose simple baselines that are not only resilient under longer learning scenarios, but are surprisingly strong when compared with sophisticated state-of-the-art methods. We conduct extensive empirical evaluation on a multitude of benchmarks and show the effectiveness of our proposed baselines, which significantly raises the bar.
    
[^37]: 神经主题模型真的需要使用dropout吗？关于dropout在主题建模中的影响分析

    Do Neural Topic Models Really Need Dropout? Analysis of the Effect of Dropout in Topic Modeling. (arXiv:2303.15973v1 [cs.CL])

    [http://arxiv.org/abs/2303.15973](http://arxiv.org/abs/2303.15973)

    本研究分析了三种常见的神经主题模型（CTM、ProdLDA和ETM），利用四个公开数据集探讨了使用dropout对神经主题模型的质量和预测效果的影响。

    

    Dropout是一种广泛使用的正则化技巧，用于解决在小数据集上训练的大型前馈神经网络过拟合问题，该问题在测试集上表现不佳。尽管这种正则化技巧在卷积神经网络中的有效性已经得到广泛研究，但对于无监督模型（特别是基于VAE的神经主题模型），缺乏对其的分析。本文在三个广泛使用的神经主题模型（即，情境主题模型（CTM），ProdLDA和嵌入式主题模型（ETM））中，利用四个公开可用数据集，分析了VAE架构的编码器和解码器中dropout的后果。我们从生成的主题的质量和预测性能的角度，表征了这些模型的dropout效应。

    Dropout is a widely used regularization trick to resolve the overfitting issue in large feedforward neural networks trained on a small dataset, which performs poorly on the held-out test subset. Although the effectiveness of this regularization trick has been extensively studied for convolutional neural networks, there is a lack of analysis of it for unsupervised models and in particular, VAE-based neural topic models. In this paper, we have analyzed the consequences of dropout in the encoder as well as in the decoder of the VAE architecture in three widely used neural topic models, namely, contextualized topic model (CTM), ProdLDA, and embedded topic model (ETM) using four publicly available datasets. We characterize the dropout effect on these models in terms of the quality and predictive performance of the generated topics.
    
[^38]: SFHarmony：无需源数据适用于分布式神经影像分析的领域自适应

    SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis. (arXiv:2303.15965v1 [cs.CV])

    [http://arxiv.org/abs/2303.15965](http://arxiv.org/abs/2303.15965)

    SFHarmony是一种无需访问源数据且能有效协调跨扫描器和研究的数据以及保护数据隐私的分布式神经影像分析方法。

    

    为了代表临床神经影像人群的生物学变异，能够合并来自扫描仪和研究的数据至关重要。然而，不同的MRI扫描仪产生具有不同特征的图像，导致了所谓的“协调问题”的领域转移。此外，神经影像数据本质上是个人个性特征，因此在共享数据时涉及到数据隐私问题。为了克服这些障碍，我们提出了一种无监督的无源领域适应（SFDA）方法SFHarmony。通过将成像特征建模为高斯混合模型，并最小化源特征和目标特征之间的自适应Bhattacharyya距离，我们可以创建一个模型，在不需要访问源数据以适应或目标标签的情况下，为目标数据表现出色，同时具有跨数据域的共享特征表示。我们展示了我们的方法在模拟和真实领域转移中的表现，证明了该方法适用于分布式神经影像分析，并能有效地协调跨扫描器和研究的数据，同时保护数据隐私。

    To represent the biological variability of clinical neuroimaging populations, it is vital to be able to combine data across scanners and studies. However, different MRI scanners produce images with different characteristics, resulting in a domain shift known as the `harmonisation problem'. Additionally, neuroimaging data is inherently personal in nature, leading to data privacy concerns when sharing the data. To overcome these barriers, we propose an Unsupervised Source-Free Domain Adaptation (SFDA) method, SFHarmony. Through modelling the imaging features as a Gaussian Mixture Model and minimising an adapted Bhattacharyya distance between the source and target features, we can create a model that performs well for the target data whilst having a shared feature representation across the data domains, without needing access to the source data for adaptation or target labels. We demonstrate the performance of our method on simulated and real domain shifts, showing that the approach is ap
    
[^39]: 基于深生成模型的多模态和多对比度图像融合

    Multimodal and multicontrast image fusion via deep generative models. (arXiv:2303.15963v1 [cs.LG])

    [http://arxiv.org/abs/2303.15963](http://arxiv.org/abs/2303.15963)

    该论文提出了一种基于变分自动编码器（VAE）的图像融合方法，可以整合多模态和多对比度的神经影像数据，以提高神经影像分析的分类性能。

    

    最近，人们逐渐意识到传统的诊断标签无法可靠地描述多种临床表型的复杂性和变异性，尤其是广泛的神经精神疾病（例如抑郁症、焦虑症、行为表型）。患者的异质性可以通过将个体根据经验得出的交织连续的新类别分组来更好地描述，这些连续跨越并超越了传统的类别边界。在这种情况下，神经影像数据携带着关于每个患者大脑的富含时空的信息。然而，它们通常会经过一系列事先未学习为模型的训练部分的过程进行预处理，因此没有针对下游预测任务进行优化。这是因为每个个体通常都有多个全脑三维成像模态，常伴随着深层的基因型和表型特征描述。为应对这些挑战，我们提出了一种基于深度生成模型的图像融合方法，以组合多模态和多对比度的神经影像数据。我们的方法使用变分自动编码器（VAE）来学习不同成像模态的共同表示空间，同时强制执行模态特定和对比度特定的编码-解码过程。我们在来自OPENNeuro数据库的174名MDD患者的数据集上评估了我们的方法，包括结构T1加权MRI、扩散张量成像（DTI）和静息功能MRI（rsfMRI）数据。我们的结果表明，所提出的方法有效地整合了多模态和多对比度的成像数据，相对于传统方法，分类表现得到了改善。

    Recently, it has become progressively more evident that classic diagnostic labels are unable to reliably describe the complexity and variability of several clinical phenotypes. This is particularly true for a broad range of neuropsychiatric illnesses (e.g., depression, anxiety disorders, behavioral phenotypes). Patient heterogeneity can be better described by grouping individuals into novel categories based on empirically derived sections of intersecting continua that span across and beyond traditional categorical borders. In this context, neuroimaging data carry a wealth of spatiotemporally resolved information about each patient's brain. However, they are usually heavily collapsed a priori through procedures which are not learned as part of model training, and consequently not optimized for the downstream prediction task. This is because every individual participant usually comes with multiple whole-brain 3D imaging modalities often accompanied by a deep genotypic and phenotypic char
    
[^40]: TraffNet：学习道路网络数字孪生交通生成因果关系

    TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins. (arXiv:2303.15954v1 [cs.LG])

    [http://arxiv.org/abs/2303.15954](http://arxiv.org/abs/2303.15954)

    TraffNet是一个学习交通量生成原因的深度学习框架，将车辆轨迹数据表示为异构图，利用递归神经网络结构实现了对交通生成原因的预测。

    

    道路网络数字孪生（RNDT）在开发下一代智能交通系统中发挥着关键作用，可以实现更精确的交通规划和控制。为了支持实时决策，RNDT需要一个模型，从在线传感器数据中动态学习交通模式并生成高保真模拟结果。尽管基于图神经网络的当前交通预测技术已经实现了最先进的性能，但是这些技术仅通过挖掘历史交通数据中的相关性来预测未来交通，而忽略了交通生成的原因，例如交通需求和路径选择。因此，它们的性能对于实时决策是不可靠的。为了填补这一差距，我们引入了一个新的深度学习框架称为 TraffNet，该框架从车辆轨迹数据中学习交通量的因果性。首先，我们使用异构图来表示道路网络，使模型能够并入预测所需的其他数据，然后我们提出了一种新颖的递归神经网络结构，从而能够预测交通量的因果联系。

    Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as traffic demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volume from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate 
    
[^41]: 随机初始化的子网络与迭代权重回收

    Randomly Initialized Subnetworks with Iterative Weight Recycling. (arXiv:2303.15953v1 [cs.LG])

    [http://arxiv.org/abs/2303.15953](http://arxiv.org/abs/2303.15953)

    本文提出了修改版的Edge-Popup和Biprop算法，名为迭代权重回收，识别随机初始化网络中的重要权重子集，以进行层内重用，并找到高精度的子网络，从而提高了模型稀疏度，并用相反的发现来补充多重彩票假设。

    

    多重彩票假设认为，随机初始化的神经网络包含几个子网络，这些子网络的精度与同一架构的已经完全训练的模型相当。然而，目前的方法要求网络具有足够的过参数化。在本文中，我们提出了两种最先进的算法（Edge-Popup和Biprop）的修改版，它可以在没有额外存储成本或缩放的情况下找到高精度的子网络。算法，迭代式权重回收，识别随意初始化网络中的重要权重子集以进行层内重用。我们的实证研究表明，在更小的网络架构和更高的修剪率的情况下，我们可以通过“回收”现有权重来增加模型的稀疏度。除了迭代权重回收，我们还用相反的发现来补充多重彩票假设：高精度的随机初始化子网络产生不同的掩码，尽管它们共享权重。

    The Multi-Prize Lottery Ticket Hypothesis posits that randomly initialized neural networks contain several subnetworks that achieve comparable accuracy to fully trained models of the same architecture. However, current methods require that the network is sufficiently overparameterized. In this work, we propose a modification to two state-of-the-art algorithms (Edge-Popup and Biprop) that finds high-accuracy subnetworks with no additional storage cost or scaling. The algorithm, Iterative Weight Recycling, identifies subsets of important weights within a randomly initialized network for intra-layer reuse. Empirically we show improvements on smaller network architectures and higher prune rates, finding that model sparsity can be increased through the "recycling" of existing weights. In addition to Iterative Weight Recycling, we complement the Multi-Prize Lottery Ticket Hypothesis with a reciprocal finding: high-accuracy, randomly initialized subnetwork's produce diverse masks, despite bei
    
[^42]: 带球谐特征的稀疏高斯过程的再探讨

    Sparse Gaussian Processes with Spherical Harmonic Features Revisited. (arXiv:2303.15948v1 [stat.ML])

    [http://arxiv.org/abs/2303.15948](http://arxiv.org/abs/2303.15948)

    该论文重新审视了带球谐特征的高斯过程模型，并提出了一种新的核函数，在连续深度的深度模型中使用，其中深度可以通过优化证据下界来估计为核超参数。此外，变分学习球谐相位引入了本征基础的稀疏性，使得可以处理比以前更大的输入维度，并允许学习高频变化。

    

    我们重新审视了带有球谐特征的高斯过程模型，并研究了与其相关的RKHS、其特征值结构以及深度模型之间的联系。基于此，我们引入了一类新的核函数，它对应于具有连续深度的深度模型。在我们的公式中，深度可以通过优化证据下界来估计为核超参数。此外，我们通过变分学习球谐相位引入了本征基础的稀疏性。这使得我们可以处理比以前更大的输入维度，并且允许学习高频变化。我们在机器学习基准数据集上验证了我们的方法。

    We revisit the Gaussian process model with spherical harmonic features and study connections between the associated RKHS, its eigenstructure and deep models. Based on this, we introduce a new class of kernels which correspond to deep models of continuous depth. In our formulation, depth can be estimated as a kernel hyper-parameter by optimizing the evidence lower bound. Further, we introduce sparseness in the eigenbasis by variational learning of the spherical harmonic phases. This enables scaling to larger input dimensions than previously, while also allowing for learning of high frequency variations. We validate our approach on machine learning benchmark datasets.
    
[^43]: 深度选择：一种用于手术录像的全监督摄像头选择网络

    Deep Selection: A Fully Supervised Camera Selection Network for Surgery Recordings. (arXiv:2303.15947v1 [cs.CV])

    [http://arxiv.org/abs/2303.15947](http://arxiv.org/abs/2303.15947)

    该论文提出了一种名为Deep Selection的神经网络，用于从多目标手术录像中选择最佳视角的摄像机。与传统方法不同，该网络通过监督学习专家注释，预测摄像机的选择概率，从而优化手术记录的效果。

    

    在手术室中记录手术是教育和评估医疗治疗的重要任务。然而，由于手术过程中目标受到严重的遮挡，如手术场地、手术工具或医生的手，所以记录这些目标是困难的。我们使用一个记录系统，在手术灯中嵌入多个摄像头，并假定任何时候至少有一台摄像机在记录没有遮挡的目标。由于嵌入的摄像机获得了多个视频序列，我们解决了从多个视频序列中选择最佳手术视图的任务。与传统方法不同，该方法基于手术场地的面积大小选择摄像机，我们提出了一种深度神经网络，通过学习专家注释的监督来预测摄像机选择概率。我们创建了一个数据集，记录了六种不同类型的整容手术，并提供了摄像机切换的注释。

    Recording surgery in operating rooms is an essential task for education and evaluation of medical treatment. However, recording the desired targets, such as the surgery field, surgical tools, or doctor's hands, is difficult because the targets are heavily occluded during surgery. We use a recording system in which multiple cameras are embedded in the surgical lamp, and we assume that at least one camera is recording the target without occlusion at any given time. As the embedded cameras obtain multiple video sequences, we address the task of selecting the camera with the best view of the surgery. Unlike the conventional method, which selects the camera based on the area size of the surgery field, we propose a deep neural network that predicts the camera selection probability from multiple video sequences by learning the supervision of the expert annotation. We created a dataset in which six different types of plastic surgery are recorded, and we provided the annotation of camera switch
    
[^44]: 基于物品图卷积的归纳式协同过滤推荐算法

    Item Graph Convolution Collaborative Filtering for Inductive Recommendations. (arXiv:2303.15946v1 [cs.IR])

    [http://arxiv.org/abs/2303.15946](http://arxiv.org/abs/2303.15946)

    该论文提出了一种基于物品图卷积的归纳式协同过滤推荐算法，通过加权投影构建物品-物品图，并采用卷积将高阶关联注入物品嵌入，同时将用户表示形成加权的加权和。

    

    最近，GCN被用作推荐系统算法的核心组件，将用户-项目交互作为二分图的边解释。然而，在缺乏附加信息的情况下，大多数现有模型采用随机初始化用户嵌入并在训练过程中优化它们的方法。这种策略使得这些算法本质上是转换型的，从而限制了它们为训练时未见过的用户生成预测的能力。为了解决这个问题，我们提出了一种基于卷积的算法，从用户的角度是归纳式的，同时仅依赖于隐式用户-项目交互数据。我们提出通过二分图交互网络的加权投影构建物品-物品图并采用卷积将高阶关联注入物品嵌入，同时将用户表示形成加权的加权和。

    Graph Convolutional Networks (GCN) have been recently employed as core component in the construction of recommender system algorithms, interpreting user-item interactions as the edges of a bipartite graph. However, in the absence of side information, the majority of existing models adopt an approach of randomly initialising the user embeddings and optimising them throughout the training process. This strategy makes these algorithms inherently transductive, curtailing their ability to generate predictions for users that were unseen at training time. To address this issue, we propose a convolution-based algorithm, which is inductive from the user perspective, while at the same time, depending only on implicit user-item interaction data. We propose the construction of an item-item graph through a weighted projection of the bipartite interaction network and to employ convolution to inject higher order associations into item embeddings, while constructing user representations as weighted su
    
[^45]: 群集引导无监督领域自适应深度说话人嵌入

    Cluster-Guided Unsupervised Domain Adaptation for Deep Speaker Embedding. (arXiv:2303.15944v1 [cs.LG])

    [http://arxiv.org/abs/2303.15944](http://arxiv.org/abs/2303.15944)

    提出一种群集引导的无监督领域自适应框架，使用伪标签和聚类标记目标域数据，并结合标记的源域数据来训练说话人嵌入网络，并通过专用的对比中心损失训练该网络来提高群集质量。在不使用目标域标签的情况下，在CN-Celeb1评估集上实现了8.10％的等误差率（EER）。

    

    最近的研究表明，伪标签可以为说话人验证的无监督领域自适应（UDA）做出贡献。受到利用现有分类器标记未标记数据以进行重新训练的自我训练策略的启发，我们提出了一种群集引导的UDA框架，该框架通过聚类对目标域数据进行标记，并将标记的源域数据和伪标记的目标域数据相结合，以训练说话人嵌入网络。为了提高群集质量，我们通过最小化对聚类专用的说话人嵌入网络的对比中心损失来训练该网络。目标是降低嵌入与其分配的群集中心之间的距离，同时增大嵌入与其他群集中心之间的距离。使用VoxCeleb2作为源域，CN-Celeb1作为目标域，我们证明了所提出的方法可以在不使用目标域的任何标签的情况下，在CN-Celeb1评估集上实现8.10％的等误差率（EER）。

    Recent studies have shown that pseudo labels can contribute to unsupervised domain adaptation (UDA) for speaker verification. Inspired by the self-training strategies that use an existing classifier to label the unlabeled data for retraining, we propose a cluster-guided UDA framework that labels the target domain data by clustering and combines the labeled source domain data and pseudo-labeled target domain data to train a speaker embedding network. To improve the cluster quality, we train a speaker embedding network dedicated for clustering by minimizing the contrastive center loss. The goal is to reduce the distance between an embedding and its assigned cluster center while enlarging the distance between the embedding and the other cluster centers. Using VoxCeleb2 as the source domain and CN-Celeb1 as the target domain, we demonstrate that the proposed method can achieve an equal error rate (EER) of 8.10% on the CN-Celeb1 evaluation set without using any labels from the target domain
    
[^46]: 物理引导的对抗神经网络用于人造数字图像相关数据生成

    Physics-guided adversarial networks for artificial digital image correlation data generation. (arXiv:2303.15939v1 [eess.IV])

    [http://arxiv.org/abs/2303.15939](http://arxiv.org/abs/2303.15939)

    本文提出一种使用具有物理引导鉴别器的生成式对抗网络来生成人造DIC位移数据的方法， 以训练更精确可靠的机器学习模型，从而实现更准确可靠的疲劳裂纹增长评估的发展。

    

    数字图像相关（DIC）已成为评估力学实验的有价值工具，特别是疲劳裂纹增长实验。评估需要准确的裂纹路径和裂纹尖端位置信息，由于固有噪声和伪影的原因，这很难获得。机器学习模型在给定标记的DIC位移数据的情况下识别此相关信息非常成功。为了训练具有广泛泛化能力的强大模型，需要大数据。然而，由于实验昂贵且耗时，材料科学和工程领域的数据通常很少。 我们提出了一种使用具有物理引导鉴别器的生成式对抗网络来生成人造DIC位移数据的方法。为了决定数据样本是真实还是假的，该鉴别器另外接收导出的von Mises等效应变。我们显示，这种物理引导方法相比传统GAN产生了更准确和稳健的结果。我们的方法允许产生大量的人造DIC数据，以训练机器学习模型，从而实现更准确可靠的疲劳裂纹增长评估的发展。

    Digital image correlation (DIC) has become a valuable tool in the evaluation of mechanical experiments, particularly fatigue crack growth experiments. The evaluation requires accurate information of the crack path and crack tip position, which is difficult to obtain due to inherent noise and artefacts. Machine learning models have been extremely successful in recognizing this relevant information given labelled DIC displacement data. For the training of robust models, which generalize well, big data is needed. However, data is typically scarce in the field of material science and engineering because experiments are expensive and time-consuming. We present a method to generate synthetic DIC displacement data using generative adversarial networks with a physics-guided discriminator. To decide whether data samples are real or fake, this discriminator additionally receives the derived von Mises equivalent strain. We show that this physics-guided approach leads to improved results in terms 
    
[^47]: 寻找长时间微弱的天文高能瞬变现象：一种数据驱动的方法。

    Searching for long faint astronomical high energy transients: a data driven approach. (arXiv:2303.15936v1 [astro-ph.HE])

    [http://arxiv.org/abs/2303.15936](http://arxiv.org/abs/2303.15936)

    HERMES Pathfinder是一个在轨探测系统，使用简单但创新的探测器监测高能宇宙瞬变现象，通过研究信号到达不同探测器的延迟时间获得精确的位置信息，本文介绍了一种使用神经网络评估航天高能探测器背景计数率的新框架。

    

    HERMES（High Energy Rapid Modular Ensemble of Satellites）是一个在轨探测系统的前导部署，由六个3U纳米卫星组成，托管着用于监测宇宙高能瞬变现象的简单但创新的探测器。HERMES Pathfinder的主要目标是证明使用微型硬件可以获得高能宇宙瞬变现象的精确位置信息。通过研究信号到达不同探测器的延迟时间，可以获取瞬变现象的位置信息。为此，需要开发新的工具来充分利用HERMES Pathfinder未来的科学数据。本文介绍了一种评估航天高能探测器背景计数率的新框架；这是鉴别微弱的天体物理瞬变现象的关键步骤。我们使用神经网络（NN）来估计探测器的背景计数率。

    HERMES (High Energy Rapid Modular Ensemble of Satellites) pathfinder is an in-orbit demonstration consisting of a constellation of six 3U nano-satellites hosting simple but innovative detectors for the monitoring of cosmic high-energy transients. The main objective of HERMES Pathfinder is to prove that accurate position of high-energy cosmic transients can be obtained using miniaturized hardware. The transient position is obtained by studying the delay time of arrival of the signal to different detectors hosted by nano-satellites on low Earth orbits. To this purpose, the goal is to achive an overall accuracy of a fraction of a micro-second. In this context, we need to develop novel tools to fully exploit the future scientific data output of HERMES Pathfinder. In this paper, we introduce a new framework to assess the background count rate of a space-born, high energy detector; a key step towards the identification of faint astrophysical transients. We employ a Neural Network (NN) to est
    
[^48]: 计算机视觉中的双曲几何：卷积神经网络的新框架

    Hyperbolic Geometry in Computer Vision: A Novel Framework for Convolutional Neural Networks. (arXiv:2303.15919v1 [cs.CV])

    [http://arxiv.org/abs/2303.15919](http://arxiv.org/abs/2303.15919)

    本文提出了HCNN，是第一个专为计算机视觉任务设计的完全双曲卷积神经网络。在洛伦兹模型的基础上，我们推广了CNN的基本组件，并通过在完全双曲设置中的实验证明了HCNN框架和洛伦兹模型的有效性。

    

    真实世界的视觉数据呈现出固有的分层结构，这些结构可以在双曲空间中有效地表示。双曲神经网络是在这种空间中学习特征表示的一种有前途的方法。然而，计算机视觉中的当前方法依赖于欧几里得骨干，并且仅在任务头中将特征投影到双曲空间中，限制了它们充分利用双曲几何的好处的能力。为了解决这个问题，我们提出了HCNN，这是第一个专为计算机视觉任务设计的完全双曲卷积神经网络（CNN）。基于洛伦兹模型，我们推广了CNN的基本组件，并提出了卷积层、批量归一化和多项式逻辑回归（MLR）的新公式。在标准视觉任务的实验中，我们展示了我们的HCNN框架和洛伦兹模型在混合和完全双曲设置中的有效性。总的来说，我们旨在为将来在双曲几何中进行的研究铺平道路。

    Real-world visual data exhibit intrinsic hierarchical structures that can be represented effectively in hyperbolic spaces. Hyperbolic neural networks (HNNs) are a promising approach for learning feature representations in such spaces. However, current methods in computer vision rely on Euclidean backbones and only project features to the hyperbolic space in the task heads, limiting their ability to fully leverage the benefits of hyperbolic geometry. To address this, we present HCNN, the first fully hyperbolic convolutional neural network (CNN) designed for computer vision tasks. Based on the Lorentz model, we generalize fundamental components of CNNs and propose novel formulations of the convolutional layer, batch normalization, and multinomial logistic regression (MLR). Experimentation on standard vision tasks demonstrates the effectiveness of our HCNN framework and the Lorentz model in both hybrid and fully hyperbolic settings. Overall, we aim to pave the way for future research in h
    
[^49]: 从私有到公有：在私有时间序列分类情境下对GAN进行基准测试

    From Private to Public: Benchmarking GANs in the Context of Private Time Series Classification. (arXiv:2303.15916v1 [cs.LG])

    [http://arxiv.org/abs/2303.15916](http://arxiv.org/abs/2303.15916)

    本论文在时间序列领域对两种GAN架构进行了评估，结果以GSWGAN表现最佳，可以私密地生成保护数据隐私的公共数据。

    

    深度学习已被证明在各个领域和任务中都很成功。然而，当涉及到私人数据时，几个限制使得难以在这些应用领域中使用深度学习方法。最近的方法尝试私密地生成数据，而不是在分类器之上直接应用隐私保护机制。解决方案是以一种保护数据隐私的方式从私有数据创建公共数据。在这项工作中，针对私有时间序列分类情境，评估了两种非常突出的基于GAN的架构。与先前主要局限于图像领域的工作相比，这个基准测试的范围是时间序列领域。实验表明，尤其是GSWGAN在多种公共数据集上表现良好，优于竞争对手DPWGAN。生成数据集的分析进一步验证了GSWGAN在时间序列生成的情境下的优越性。

    Deep learning has proven to be successful in various domains and for different tasks. However, when it comes to private data several restrictions are making it difficult to use deep learning approaches in these application fields. Recent approaches try to generate data privately instead of applying a privacy-preserving mechanism directly, on top of the classifier. The solution is to create public data from private data in a manner that preserves the privacy of the data. In this work, two very prominent GAN-based architectures were evaluated in the context of private time series classification. In contrast to previous work, mostly limited to the image domain, the scope of this benchmark was the time series domain. The experiments show that especially GSWGAN performs well across a variety of public datasets outperforming the competitor DPWGAN. An analysis of the generated datasets further validates the superiority of GSWGAN in the context of time series generation.
    
[^50]: 基于去噪自编码器的防御蒸馏作为对抗鲁棒性算法

    Denoising Autoencoder-based Defensive Distillation as an Adversarial Robustness Algorithm. (arXiv:2303.15901v1 [cs.LG])

    [http://arxiv.org/abs/2303.15901](http://arxiv.org/abs/2303.15901)

    本论文提出了一种新的对抗攻击防御方法，通过将防御蒸馏机制与去噪自编码器相结合，来降低模型对有毒攻击的敏感度。

    

    对抗攻击对深度神经网络(DNNs)的鲁棒性造成了极大威胁。尽管使用了多种防御方法，但它们仍然容易受到攻击者篡改的初始训练数据。为了防御这种对抗攻击，本研究提出一种新的方法，将防御蒸馏机制与去噪自编码器(DAE)相结合。该技术旨在通过检测和重构训练数据中有害的对抗输入来降低蒸馏模型对有毒攻击的敏感度。我们在初始训练数据中添加了精心创建的对抗样本以评估所提出方法的性能。实验证明，我们的方法成功识别和重构了有毒的输入，同时也考虑了增强DNN的鲁棒性。所提出的方法为在数据隐私保护等各种应用中提供了一种强大且鲁棒的DNN防御机制。

    Adversarial attacks significantly threaten the robustness of deep neural networks (DNNs). Despite the multiple defensive methods employed, they are nevertheless vulnerable to poison attacks, where attackers meddle with the initial training data. In order to defend DNNs against such adversarial attacks, this work proposes a novel method that combines the defensive distillation mechanism with a denoising autoencoder (DAE). This technique tries to lower the sensitivity of the distilled model to poison attacks by spotting and reconstructing poisonous adversarial inputs in the training data. We added carefully created adversarial samples to the initial training data to assess the proposed method's performance. Our experimental findings demonstrate that our method successfully identified and reconstructed the poisonous inputs while also considering enhancing the DNN's resilience. The proposed approach provides a potent and robust defense mechanism for DNNs in various applications where data 
    
[^51]: 通过二维预优化实现三维建筑的高效质量多样性优化

    Efficient Quality Diversity Optimization of 3D Buildings through 2D Pre-optimization. (arXiv:2303.15896v1 [cs.NE])

    [http://arxiv.org/abs/2303.15896](http://arxiv.org/abs/2303.15896)

    本研究使用预优化策略解决质量多样性算法在高维度问题上的低效性，用于设计建筑底板，通过在低维优化问题上进行预优化，然后将解决方案映射到高维度情况下，成功训练出更准确的建筑设计预测模型。

    

    质量多样性算法可用于有效地创建多种解决方案，帮助工程师的直觉判断。但在非常昂贵的问题中，质量多样性不够高效，需要进行成千上万次评估。即使辅以替代模型的帮助，质量多样性也需要几百甚至几千次评估，这使得它很难成为可行的方案。本研究尝试通过在较低维优化问题上使用预优化策略，然后将解决方案映射到更高维度的情况来解决此问题。以设计最小化风扰建筑为例，我们展示了可以通过二维建筑底板的流特征预测三维建筑周围的流特征。通过使用质量多样性算法对2D建筑底板空间进行采样，可以训练出比使用类似Sobol序列这样的填充算法所选出底板的预测模型更准确的多样的建筑设计。

    Quality diversity algorithms can be used to efficiently create a diverse set of solutions to inform engineers' intuition. But quality diversity is not efficient in very expensive problems, needing 100.000s of evaluations. Even with the assistance of surrogate models, quality diversity needs 100s or even 1000s of evaluations, which can make it use infeasible. In this study we try to tackle this problem by using a pre-optimization strategy on a lower-dimensional optimization problem and then map the solutions to a higher-dimensional case. For a use case to design buildings that minimize wind nuisance, we show that we can predict flow features around 3D buildings from 2D flow features around building footprints. For a diverse set of building designs, by sampling the space of 2D footprints with a quality diversity algorithm, a predictive model can be trained that is more accurate than when trained on a set of footprints that were selected with a space-filling algorithm like the Sobol seque
    
[^52]: VIVE3D: 使用三维感知 GAN 实现独立视角视频编辑

    VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs. (arXiv:2303.15893v1 [cs.CV])

    [http://arxiv.org/abs/2303.15893](http://arxiv.org/abs/2303.15893)

    VIVE3D 使用 3D GAN 技术实现了独立视角视频编辑，能够保留身份和时间一致性。通过新的 GAN 反演技术和头部新视角的编辑等创新，生成高保真的人脸编辑，与原始视频以一致的时间和空间方式组合。

    

    我们引入了 VIVE3D，一种新颖的方法，将基于图像的 3D GAN 的能力扩展到视频编辑，并能以保留身份和时间一致性的方式呈现输入视频。我们提出了两个新的构建块。首先，我们针对 3D GAN 引入了一种新颖的 GAN 反演技术，通过联合嵌入多帧并优化相机参数。其次，除了传统的语义人脸编辑（例如年龄和表情），我们首次演示了基于 3D GAN 的内在属性和我们的光流引导的合成技术所启用的显示头部新视角的编辑。我们的实验表明，VIVE3D 以一致的质量从一系列摄像机视角生成高保真人脸编辑，与原始视频以一致的时间和空间方式组合。

    We introduce VIVE3D, a novel approach that extends the capabilities of image-based 3D GANs to video editing and is able to represent the input video in an identity-preserving and temporally consistent way. We propose two new building blocks. First, we introduce a novel GAN inversion technique specifically tailored to 3D GANs by jointly embedding multiple frames and optimizing for the camera parameters. Second, besides traditional semantic face edits (e.g. for age and expression), we are the first to demonstrate edits that show novel views of the head enabled by the inherent properties of 3D GANs and our optical flow-guided compositing technique to combine the head with the background video. Our experiments demonstrate that VIVE3D generates high-fidelity face edits at consistent quality from a range of camera viewpoints which are composited with the original video in a temporally and spatially consistent manner.
    
[^53]: 无需数据的分布式连续学习中的投影潜空间蒸馏

    Projected Latent Distillation for Data-Agnostic Consolidation in Distributed Continual Learning. (arXiv:2303.15888v1 [cs.LG])

    [http://arxiv.org/abs/2303.15888](http://arxiv.org/abs/2303.15888)

    该论文提出了一种双重知识蒸馏方法Data-Agnostic Consolidation（DAC），在不使用原始数据的情况下，通过一种新的投影潜空间蒸馏损失，在分布式连续学习中实现了SCD之间的前向转移并取得了最先进的准确性。

    

    边缘分布式学习通常由自我中心的设备（SCD）组成，它们独立学习本地任务并不愿意为其他SCD的性能做出贡献。我们如何以零成本实现单个SCD的前向转移？我们将这个问题形式化为分布式连续学习场景，在这个场景中，SCD适应本地任务，CL模型将由这些模型产生的知识合并而无需查看SCD的私有数据。不幸的是，目前的CL方法并不直接适用于这种情况。我们提出了一个新的双重知识蒸馏方法Data-Agnostic Consolidation（DAC），该方法无需使用原始数据即可合并SC模型的流。DAC通过一种新的投影潜空间蒸馏损失在潜空间中执行蒸馏。实验结果表明，DAC使SCD之间的前向转移成为可能，并在Split CIFAR100，CORe50和Split TinyImageNet上达到了最先进的准确性，无需排练。

    Distributed learning on the edge often comprises self-centered devices (SCD) which learn local tasks independently and are unwilling to contribute to the performance of other SDCs. How do we achieve forward transfer at zero cost for the single SCDs? We formalize this problem as a Distributed Continual Learning scenario, where SCD adapt to local tasks and a CL model consolidates the knowledge from the resulting stream of models without looking at the SCD's private data. Unfortunately, current CL methods are not directly applicable to this scenario. We propose Data-Agnostic Consolidation (DAC), a novel double knowledge distillation method that consolidates the stream of SC models without using the original data. DAC performs distillation in the latent space via a novel Projected Latent Distillation loss. Experimental results show that DAC enables forward transfer between SCDs and reaches state-of-the-art accuracy on Split CIFAR100, CORe50 and Split TinyImageNet, both in reharsal-free and
    
[^54]: Wyner多视图无监督学习的高效交替最小化求解器

    Efficient Alternating Minimization Solvers for Wyner Multi-View Unsupervised Learning. (arXiv:2303.15866v1 [cs.IT])

    [http://arxiv.org/abs/2303.15866](http://arxiv.org/abs/2303.15866)

    本文提出了适用于Wyner多视图无监督学习的高效交替最小化求解器，包括变分形式和表现形式，通过多重乘数交替方向算法可以解决由此造成的非凸优化问题。

    

    在本文中，我们采用Wyner通用信息框架进行无监督的多视图表示学习。在这个框架内，我们提出了两种新的公式，使得可以基于交替最小化原则开发计算高效的求解器。第一种公式被称为“变分形式”，其复杂度随着视图数量的增加而线性增长，并基于变分推理紧束缚和拉格朗日优化目标函数。第二种公式，即“表现形式”，被证明包括已知结果的特殊情况。在这里，我们开发了一个定制版本的多重乘数交替方向算法（ADMM），用于解决由此产生的非凸优化问题。在两种情况下，所提出的求解器的收敛性在某些相关范围内得到了证明。此外，我们的实证结果证明了提出的方法的有效性。

    In this work, we adopt Wyner common information framework for unsupervised multi-view representation learning. Within this framework, we propose two novel formulations that enable the development of computational efficient solvers based on the alternating minimization principle. The first formulation, referred to as the {\em variational form}, enjoys a linearly growing complexity with the number of views and is based on a variational-inference tight surrogate bound coupled with a Lagrangian optimization objective function. The second formulation, i.e., the {\em representational form}, is shown to include known results as special cases. Here, we develop a tailored version from the alternating direction method of multipliers (ADMM) algorithm for solving the resulting non-convex optimization problem. In the two cases, the convergence of the proposed solvers is established in certain relevant regimes. Furthermore, our empirical results demonstrate the effectiveness of the proposed methods 
    
[^55]: Wyner变分自编码器用于无监督多层无线指纹识别

    The Wyner Variational Autoencoder for Unsupervised Multi-Layer Wireless Fingerprinting. (arXiv:2303.15860v1 [cs.IT])

    [http://arxiv.org/abs/2303.15860](http://arxiv.org/abs/2303.15860)

    该论文提出了一个基于Wyner变分自编码器的多层指纹识别框架，通过利用多层特征共享的设备信息，在无监督情况下提高识别性能。

    

    无线指纹识别是一种利用硬件不完善和无线信道变化作为标识的设备识别方法。除了物理层特征外，最近的研究表明可以通过网络流量（例如包长度）识别用户行为，而无需解密有效载荷。因此，我们提出了一个多层指纹识别框架，联合考虑多层特征以提高识别性能。与以前的工作不同，我们利用了最近的多视图机器学习范例，即多形式数据，可以在不需要监督的情况下聚合多层特征共享的设备信息。我们的信息论方法可以扩展到有监督和半监督设置，并通过直接推导获得简单的导出式。为了解决所制定的问题，我们使用变分推断获得紧密的代理界限，以进行有效的优化。在提取特征方面，我们使用Wyner变分自编码器来克服多视图数据分布间的分歧。

    Wireless fingerprinting refers to a device identification method leveraging hardware imperfections and wireless channel variations as signatures. Beyond physical layer characteristics, recent studies demonstrated that user behaviours could be identified through network traffic, e.g., packet length, without decryption of the payload. Inspired by these results, we propose a multi-layer fingerprinting framework that jointly considers the multi-layer signatures for improved identification performance. In contrast to previous works, by leveraging the recent multi-view machine learning paradigm, i.e., data with multiple forms, our method can cluster the device information shared among the multi-layer features without supervision. Our information-theoretic approach can be extended to supervised and semi-supervised settings with straightforward derivations. In solving the formulated problem, we obtain a tight surrogate bound using variational inference for efficient optimization. In extracting
    
[^56]: 探索SAR图像分类的深度学习方法：通过变压器实现下一代卷积

    Exploring Deep Learning Methods for Classification of SAR Images: Towards NextGen Convolutions via Transformers. (arXiv:2303.15852v1 [eess.IV])

    [http://arxiv.org/abs/2303.15852](http://arxiv.org/abs/2303.15852)

    本研究发现深度学习模型可以在SAR图像分类方面具有很好的性能，提高准确性、预测时间和输入韧性。

    

    高分辨率SAR生成的图像具有广泛的应用领域，因为它们可以在恶劣的光线和天气条件下工作得更好。军事系统是其中之一的应用领域。本研究旨在探索计算机视觉领域引入的最新先进模型对SAR目标分类（MSTAR）的适用性。由于任何为军事系统提供的解决方案的应用都将是战略性和实时性的，因此准确性通常不是衡量其性能的唯一标准，预测时间和输入恢复能力等其他重要参数同样重要。本文就这些问题在SAR图像的情况下进行了探讨。实验结果表明，深度学习模型可以应用于SAR图像分类领域，达到期望的性能水平。

    Images generated by high-resolution SAR have vast areas of application as they can work better in adverse light and weather conditions. One such area of application is in the military systems. This study is an attempt to explore the suitability of current state-of-the-art models introduced in the domain of computer vision for SAR target classification (MSTAR). Since the application of any solution produced for military systems would be strategic and real-time, accuracy is often not the only criterion to measure its performance. Other important parameters like prediction time and input resiliency are equally important. The paper deals with these issues in the context of SAR images. Experimental results show that deep learning models can be suitably applied in the domain of SAR image classification with the desired performance levels.
    
[^57]: 标签风格问题：处理不确定图像分割中的标签风格偏差

    That Label's Got Style: Handling Label Style Bias for Uncertain Image Segmentation. (arXiv:2303.15850v1 [cs.CV])

    [http://arxiv.org/abs/2303.15850](http://arxiv.org/abs/2303.15850)

    本文解决了在不确定图像分割中标签风格偏差的问题，提出了一个基于标签风格的建模方法，并修改了两个最先进的分割不确定性架构。实验证明该方法可以减少标签风格偏差，同时提高分割性能。

    

    分割不确定性模型可以预测给定输入的可能分割结果，这些模型可以从训练集中的注释者变异性中学习而来。然而，在实践中，这些注释可能会通过使用不同的标注工具而有系统差异。这导致数据集既包含数据变异性，又包含不同的标签风格。本文证明，在这样的数据集上应用最先进的分割不确定性模型会导致模型偏差，因为不同的标签风格所造成。我们提出一种更新的建模目标，用于基于标签风格的偶然不确定性估计，修改了两个用于分割不确定性的最先进的架构。我们通过广泛的实验证明，该方法减少了标签风格偏差，同时提高了分割性能，增加了分割不确定性模型在实际应用场景中的适用性。我们精选了两个数据集用于验证我们的方法，这些数据集既包含数据变异性，又包含不同的标签风格。

    Segmentation uncertainty models predict a distribution over plausible segmentations for a given input, which they learn from the annotator variation in the training set. However, in practice these annotations can differ systematically in the way they are generated, for example through the use of different labeling tools. This results in datasets that contain both data variability and differing label styles. In this paper, we demonstrate that applying state-of-the-art segmentation uncertainty models on such datasets can lead to model bias caused by the different label styles. We present an updated modelling objective conditioning on labeling style for aleatoric uncertainty estimation, and modify two state-of-the-art-architectures for segmentation uncertainty accordingly. We show with extensive experiments that this method reduces label style bias, while improving segmentation performance, increasing the applicability of segmentation uncertainty models in the wild. We curate two datasets
    
[^58]: 一种基于高斯混合分布的自适应采样方法用于PINNs

    GAS: A Gaussian Mixture Distribution-Based Adaptive Sampling Method for PINNs. (arXiv:2303.15849v1 [cs.LG])

    [http://arxiv.org/abs/2303.15849](http://arxiv.org/abs/2303.15849)

    GAS是一种基于高斯混合分布的自适应采样方法，用于加速PINNs的收敛过程并提高精度，已在2D到10D问题的数值模拟中表现出领先于深层求解器、与传统数值求解器相当的优异性能。

    

    随着深度学习在科学计算中的应用研究，PINNs方法因其高维问题处理的高效性而引起了广泛的关注，但其准确性相对较低，特别是针对高度不规则的问题。受自适应有限元方法和增量学习思想启发，我们提出了GAS，一种基于高斯混合分布的自适应采样方法，用于PINNs。在训练过程中，GAS利用当前的残差信息生成高斯混合分布以采样其他点，这些数据将与历史数据一起训练，加快损失函数的收敛速度并实现更高的准确性。在2D到10D问题的数值模拟中，GAS是一种有前途的方法，它在深层求解器中达到了最先进的精度，同时与传统数值求解器相当。

    With recent study of the deep learning in scientific computation, the PINNs method has drawn widespread attention for solving PDEs. Compared with traditional methods, PINNs can efficiently handle high-dimensional problems, while the accuracy is relatively low, especially for highly irregular problems. Inspired by the idea of adaptive finite element methods and incremental learning, we propose GAS, a Gaussian mixture distribution-based adaptive sampling method for PINNs. During the training procedure, GAS uses the current residual information to generate a Gaussian mixture distribution for the sampling of additional points, which are then trained together with history data to speed up the convergence of loss and achieve a higher accuracy. Several numerical simulations on 2d to 10d problems show that GAS is a promising method which achieves the state-of-the-art accuracy among deep solvers, while being comparable with traditional numerical solvers.
    
[^59]: 利用初级保健医师的荷兰医疗笔记预测肺癌的软提示调整

    Soft-prompt tuning to predict lung cancer using primary care free-text Dutch medical notes. (arXiv:2303.15846v1 [cs.CL])

    [http://arxiv.org/abs/2303.15846](http://arxiv.org/abs/2303.15846)

    本论文研究了使用初级保健医师的患者医疗笔记进行肺癌早期预测的问题，并探讨了针对高度不平衡分类问题的软提示调整和静态词嵌入模型在模型训练中的表现。

    

    我们研究了基于上下文词表示的不同自然语言处理（NLP）方法，用于使用荷兰初级保健医师的患者医疗笔记早期预测肺癌的问题。因为肺癌在初级保健中的患病率较低，所以我们还解决了在高度不平衡的类别下进行分类的问题。具体而言，我们使用大型基于Transformer的预训练语言模型（PLMs），并研究：1）如何将\textit {软提示调整} - 一种使用小量训练数据调整PLMs的NLP技术 - 与标准模型微调进行比较； 2）在高度不平衡的设置中，是否简单的静态词嵌入模型（WEMs）可以比PLMs更健壮；以及3）当训练笔记来自少量患者时，模型的表现如何。我们发现，1）软提示调整是标准模型微调的有效替代方案； 2）PLMs比较简单的静态词嵌入模型表现出更好的区分能力但更差的校准能力。

    We investigate different natural language processing (NLP) approaches based on contextualised word representations for the problem of early prediction of lung cancer using free-text patient medical notes of Dutch primary care physicians. Because lung cancer has a low prevalence in primary care, we also address the problem of classification under highly imbalanced classes. Specifically, we use large Transformer-based pretrained language models (PLMs) and investigate: 1) how \textit{soft prompt-tuning} -- an NLP technique used to adapt PLMs using small amounts of training data -- compares to standard model fine-tuning; 2) whether simpler static word embedding models (WEMs) can be more robust compared to PLMs in highly imbalanced settings; and 3) how models fare when trained on notes from a small number of patients. We find that 1) soft-prompt tuning is an efficient alternative to standard model fine-tuning; 2) PLMs show better discrimination but worse calibration compared to simpler stat
    
[^60]: 条件生成模型可证明具有健壮性:银湖反问题的逐点保证

    Conditional Generative Models are Provably Robust: Pointwise Guarantees for Bayesian Inverse Problems. (arXiv:2303.15845v1 [cs.LG])

    [http://arxiv.org/abs/2303.15845](http://arxiv.org/abs/2303.15845)

    本文证明了条件生成模型对单个观测结果有健壮性

    

    条件生成模型成为采样银湖反问题后验概率的强大工具. 经典的贝叶斯文献已经知道后验测度对先前测度和负对数似然函数(包括观察的扰动)非常 robust. 但是, 就我们所知, 条件生成模型的健壮性还没被研究过. 在本文中, 我们首次证明了适当学习的条件生成模型在单个观测值方面提供了健壮的结果.

    Conditional generative models became a very powerful tool to sample from Bayesian inverse problem posteriors. It is well-known in classical Bayesian literature that posterior measures are quite robust with respect to perturbations of both the prior measure and the negative log-likelihood, which includes perturbations of the observations. However, to the best of our knowledge, the robustness of conditional generative models with respect to perturbations of the observations has not been investigated yet. In this paper, we prove for the first time that appropriately learned conditional generative models provide robust results for single observations.
    
[^61]: 生成合理的对策序列用于预测性流程分析：CREATED

    CREATED: Generating Viable Counterfactual Sequences for Predictive Process Analytics. (arXiv:2303.15844v1 [cs.LG])

    [http://arxiv.org/abs/2303.15844](http://arxiv.org/abs/2303.15844)

    本论文提出了一个通用框架，使用进化方法生成反事实序列并避免对领域知识的需求以提高可行性。

    

    预测性流程分析关注预测未来状态，例如运行流程实例的结果。这些技术通常利用机器学习模型或深度学习模型（如LSTM）进行这样的预测。然而，这些深度模型复杂而且难以理解。反事实能回答“如果……会怎样”的问题，这些问题有助于理解预测背后的推理。当前用于生成反事实序列的方法不考虑流程行为，从而导致生成无效或不可行的反事实流程实例，或严重依赖于领域知识。在这项工作中，我们提出了一个通用框架，利用进化方法生成反事实序列，并避免了对领域知识的需求。我们提议训练一个马尔可夫模型来计算反事实序列的概率，这将在生成反事实序列时保证其可行性。

    Predictive process analytics focuses on predicting future states, such as the outcome of running process instances. These techniques often use machine learning models or deep learning models (such as LSTM) to make such predictions. However, these deep models are complex and difficult for users to understand. Counterfactuals answer ``what-if'' questions, which are used to understand the reasoning behind the predictions. For example, what if instead of emailing customers, customers are being called? Would this alternative lead to a different outcome? Current methods to generate counterfactual sequences either do not take the process behavior into account, leading to generating invalid or infeasible counterfactual process instances, or heavily rely on domain knowledge. In this work, we propose a general framework that uses evolutionary methods to generate counterfactual sequences. Our framework does not require domain knowledge. Instead, we propose to train a Markov model to compute the f
    
[^62]: 通过元机器学习在企业网络中实现跨组织分析

    Enabling Inter-organizational Analytics in Business Networks Through Meta Machine Learning. (arXiv:2303.15834v1 [cs.LG])

    [http://arxiv.org/abs/2303.15834](http://arxiv.org/abs/2303.15834)

    该论文提出了一种元机器学习方法，用于在跨组织的企业网络中实现全面的数据分析。该方法可以解决在数据分布在多个法律实体之间时，披露敏感信息和需要交换大量数据的难题，并显示了在工业应用中的可行性。

    

    成功的分析解决方案通常依赖于连接各种数据源。虽然在组织内部生成更大的数据池通常是可行的，但在（跨组织的）企业网络中应用分析仍然受到严重的制约。由于数据分布在多个法律实体之间，甚至可能跨越多个国家，因此披露敏感信息的担忧以及需要交换的数据量是创建有效的系统范围解决方案的关键障碍，同时仍然实现卓越的预测性能。本文提出了一种元机器学习方法，以解决这些障碍，从而在企业网络中实现全面的分析。我们采用设计科学研究方法，并在一个工业用例中评估了我们的方法的可行性和性能。首先，我们展示了执行网络广泛分析是可行的。

    Successful analytics solutions that provide valuable insights often hinge on the connection of various data sources. While it is often feasible to generate larger data pools within organizations, the application of analytics within (inter-organizational) business networks is still severely constrained. As data is distributed across several legal units, potentially even across countries, the fear of disclosing sensitive information as well as the sheer volume of the data that would need to be exchanged are key inhibitors for the creation of effective system-wide solutions -- all while still reaching superior prediction performance. In this work, we propose a meta machine learning method that deals with these obstacles to enable comprehensive analyses within a business network. We follow a design science research approach and evaluate our method with respect to feasibility and performance in an industrial use case. First, we show that it is feasible to perform network-wide analyses that 
    
[^63]: PDExplain：PDEs 在实际应用中的情境建模

    PDExplain: Contextual Modeling of PDEs in the Wild. (arXiv:2303.15827v1 [cs.LG])

    [http://arxiv.org/abs/2303.15827](http://arxiv.org/abs/2303.15827)

    我们提出了PDExplain，一种解释性的方法来解决偏微分方程。该算法能够通过提供少量样本的方式，预测未来时间步的PDE解，极大地协助了建立物理科学中基于数据的现象建模。

    

    我们提出了一种解释性的方法PDExplain用于解决偏微分方程。在训练阶段，我们的方法通过一个操作员定义的PDE家族的数据以及这个家族的一般形式进行馈送。在推断阶段，提供了一个从现象中收集到的最小样本，其中样本与 PDE 家族相关，但不一定属于训练阶段看到的具体 PDE 集合。我们展示了算法如何预测未来时间步的PDE解。此外，我们的方法提供了PDE的可解释形式，这种特征可以协助通过物理科学数据来对现象进行建模。为了验证我们的方法，我们进行了大量实验，考察了其在预测误差和可解释性方面的质量。

    We propose an explainable method for solving Partial Differential Equations by using a contextual scheme called PDExplain. During the training phase, our method is fed with data collected from an operator-defined family of PDEs accompanied by the general form of this family. In the inference phase, a minimal sample collected from a phenomenon is provided, where the sample is related to the PDE family but not necessarily to the set of specific PDEs seen in the training phase. We show how our algorithm can predict the PDE solution for future timesteps. Moreover, our method provides an explainable form of the PDE, a trait that can assist in modelling phenomena based on data in physical sciences. To verify our method, we conduct extensive experimentation, examining its quality both in terms of prediction error and explainability.
    
[^64]: 一种激光等离子体加速器的Pareto优化方案

    Pareto Optimization of a Laser Wakefield Accelerator. (arXiv:2303.15825v1 [physics.acc-ph])

    [http://arxiv.org/abs/2303.15825](http://arxiv.org/abs/2303.15825)

    本文展示了一种多目标优化方法可以映射激光等离子体加速器的解空间，并找到了一些在类似激光束效率的同时平衡束能与电荷的Pareto最优解，但当应用需要特定目标能量的粒子束时需要权衡能量展宽与加速器效率。

    

    加速器性能参数的优化受到众多权衡的限制，因此找到未知系统优化目标之间的适当平衡具有挑战性。本文展示了多目标贝叶斯优化方法能够以非常高效的方式映射激光等离子体加速器解空间。利用高斯混合模型，我们分离了与某个能量的电子束相关的贡献，并观察到存在广泛的Pareto最优解范围，其在类似激光束效率的同时平衡了束能与电荷。然而，许多应用（如光源）需要特定目标能量的粒子束。一旦引入这样的约束，我们观察到能量展宽与加速器效率之间存在直接的权衡。此外，我们还展示了如何使用\emph{a posteriori}标量化的目标有效地分解探查与利用具体解决方案。

    Optimization of accelerator performance parameters is limited by numerous trade-offs and finding the appropriate balance between optimization goals for an unknown system is challenging to achieve. Here we show that multi-objective Bayesian optimization can map the solution space of a laser wakefield accelerator in a very sample-efficient way. Using a Gaussian mixture model, we isolate contributions related to an electron bunch at a certain energy and we observe that there exists a wide range of Pareto-optimal solutions that trade beam energy versus charge at similar laser-to-beam efficiency. However, many applications such as light sources require particle beams at a certain target energy. Once such a constraint is introduced we observe a direct trade-off between energy spread and accelerator efficiency. We furthermore demonstrate how specific solutions can be exploited using \emph{a posteriori} scalarization of the objectives, thereby efficiently splitting the exploration and exploita
    
[^65]: 基于空间离散化演化搜索的多目标安全博弈可扩展性研究

    Scaling Multi-Objective Security Games Provably via Space Discretization Based Evolutionary Search. (arXiv:2303.15821v1 [cs.LG])

    [http://arxiv.org/abs/2303.15821](http://arxiv.org/abs/2303.15821)

    本文提出了一个名为SDES的通用框架，其中包括离散化、优化、恢复和评估以及改进等四个组成部分，通过离散化连续解空间来实现多目标安全博弈的可扩展性。

    

    在安全领域，多目标安全博弈(MOSGs)允许防御者同时保护多个异质性攻击者的目标。MOSGs旨在同时最大化所有异质性回报，例如生命、金钱和犯罪率，而不合并异质性攻击者。在现实世界的场景中，需要保护的异质性攻击者和目标数量可能超出大多数现有的最先进的方法的能力，MOSGs受到可扩展性问题的限制。因此，本文提出了一个通用框架称为SDES，该框架基于多目标进化搜索来扩展MOSGs的大规模目标和异质性攻击者。SDES由四个连续的关键组成部分组成，即离散化、优化、恢复和评估以及改进。具体来说，SDES使用博弈理论中的最大不平等性原理将原始的高维连续解空间离散化为低维离散解空间。

    In the field of security, multi-objective security games (MOSGs) allow defenders to simultaneously protect targets from multiple heterogeneous attackers. MOSGs aim to simultaneously maximize all the heterogeneous payoffs, e.g., life, money, and crime rate, without merging heterogeneous attackers. In real-world scenarios, the number of heterogeneous attackers and targets to be protected may exceed the capability of most existing state-of-the-art methods, i.e., MOSGs are limited by the issue of scalability. To this end, this paper proposes a general framework called SDES based on many-objective evolutionary search to scale up MOSGs to large-scale targets and heterogeneous attackers. SDES consists of four consecutive key components, i.e., discretization, optimization, restoration and evaluation, and refinement. Specifically, SDES first discretizes the originally high-dimensional continuous solution space to the low-dimensional discrete one by the maximal indifference property in game theo
    
[^66]: 没有OOD动作的离线强化学习：基于隐式价值正则化的样本内学习

    Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization. (arXiv:2303.15810v1 [cs.LG])

    [http://arxiv.org/abs/2303.15810](http://arxiv.org/abs/2303.15810)

    本文提出了一种新算法，IVR-Q，用于离线强化学习，其通过隐式价值正则化的方式避免了OOD动作带来的价值函数偏移，并通过最小化IVR loss来改善策略。在多个基准任务上的实验结果证明其优于现有方法并实现了最佳性能。

    

    大多数离线强化学习 (RL) 方法面临一个折衷问题：改善策略以超越行为策略与限制策略以限制与行为策略的偏差之间的平衡。由于使用超出分布范围 (OOD) 的动作计算 Q 值会受到分布偏移错误的影响。最近提出的基于样本内学习范式（IQL）通过对数据样本进行分位回归来改善策略，表现出很大的潜力，因为它可以学习最优策略而不查询任何未见动作的值函数。然而，目前尚不清楚这种方法如何处理学习价值函数时的分布偏移。本文发现，样本内学习范例在隐式价值正则化 (IVR) 框架下得以产生。这给了我们更深刻的理解为什么样本内学习范例有效，即它将隐式价值正则化应用于策略。基于这个洞见，我们提出了一个新算法，IVR-Q，它通过规范化策略的价值函数以避免OOD动作，并通过最小化IVR loss来改善策略。多个基准任务的实验结果表明，IVR-Q优于现有方法，并在离线RL中实现了最佳性能。

    Most offline reinforcement learning (RL) methods suffer from the trade-off between improving the policy to surpass the behavior policy and constraining the policy to limit the deviation from the behavior policy as computing $Q$-values using out-of-distribution (OOD) actions will suffer from errors due to distributional shift. The recently proposed \textit{In-sample Learning} paradigm (i.e., IQL), which improves the policy by quantile regression using only data samples, shows great promise because it learns an optimal policy without querying the value function of any unseen actions. However, it remains unclear how this type of method handles the distributional shift in learning the value function. In this work, we make a key finding that the in-sample learning paradigm arises under the \textit{Implicit Value Regularization} (IVR) framework. This gives a deeper understanding of why the in-sample learning paradigm works, i.e., it applies implicit value regularization to the policy. Based 
    
[^67]: 核插值的泛化能力较弱

    Kernel interpolation generalizes poorly. (arXiv:2303.15809v1 [cs.LG])

    [http://arxiv.org/abs/2303.15809](http://arxiv.org/abs/2303.15809)

    本文证明了核插值的泛化误差有一个下界，在较大范围的核函数中泛化能力较差，使得过拟合的宽神经网络泛化性能差。

    

    在核回归研究的复兴中，一个最有趣的问题可能是核插值是否具有较好的泛化能力，因为这可能有助于我们理解深度网络领域中的“良性过拟合现象”。在本文中，在温和的条件下，我们证明了对于任何$\varepsilon>0$，核插值的泛化误差都有一个下界$\Omega(n^{-\varepsilon})$。换句话说，核插值在较大范围的核函数中泛化能力较差。作为一个直接的推论，我们可以证明在球上定义的过拟合宽神经网络的泛化能力也较差。

    One of the most interesting problems in the recent renaissance of the studies in kernel regression might be whether the kernel interpolation can generalize well, since it may help us understand the `benign overfitting henomenon' reported in the literature on deep networks. In this paper, under mild conditions, we show that for any $\varepsilon>0$, the generalization error of kernel interpolation is lower bounded by $\Omega(n^{-\varepsilon})$. In other words, the kernel interpolation generalizes poorly for a large class of kernels. As a direct corollary, we can show that overfitted wide neural networks defined on sphere generalize poorly.
    
[^68]: 带有聚合梯度的快速收敛联邦学习

    Fast Convergence Federated Learning with Aggregated Gradients. (arXiv:2303.15799v1 [cs.LG])

    [http://arxiv.org/abs/2303.15799](http://arxiv.org/abs/2303.15799)

    该论文提出了一种带有聚合梯度的快速收敛联邦学习方法，通过引入均值场方法来完成参数和梯度的聚合步骤，该方法在收敛速度和通信成本方面优于传统方法。

    

    联邦学习（FL）是一种新的机器学习框架，它使多个分布式设备在保护本地数据的同时，通过中央服务器协同训练共享模型。然而，非独立和同分布（Non-IID）的数据样本以及参与者之间频繁的通信将减缓收敛速率并增加通信成本。为了实现快速收敛，我们通过在常规本地更新规则中引入聚合梯度来改善本地梯度下降方法，并提出一种自适应学习率算法，在每次迭代中进一步考虑本地参数和全局参数的偏差。以上策略要求在每个本地迭代中收集所有客户端的本地参数和梯度，由于本地更新期间没有通信，这是具有挑战性的。因此，我们利用均值场方法，引入称为全局均值场和本地均值场的两个均值场术语来完成聚合步骤。实验结果表明，我们提出的方法在收敛速度和通信成本方面优于传统方法。

    Federated Learning (FL) is a novel machine learning framework, which enables multiple distributed devices cooperatively training a shared model scheduled by a central server while protecting private data locally. However, the non-independent-and-identically-distributed (Non-IID) data samples and frequent communication among participants will slow down the convergent rate and increase communication costs. To achieve fast convergence, we ameliorate the local gradient descend approach in conventional local update rule by introducing the aggregated gradients at each local update epoch, and propose an adaptive learning rate algorithm that further takes the deviation of local parameter and global parameter into consideration at each iteration. The above strategy requires all clients' local parameters and gradients at each local iteration, which is challenging as there is no communication during local update epochs. Accordingly, we utilize mean field approach by introducing two mean field ter
    
[^69]: 生态图表：基础模型的社会影响

    Ecosystem Graphs: The Social Footprint of Foundation Models. (arXiv:2303.15772v1 [cs.LG])

    [http://arxiv.org/abs/2303.15772](http://arxiv.org/abs/2303.15772)

    该论文提出了一种名为「生态图表」的文档框架，可以透明地集中基础模型的社会影响方面的知识，这可以提高其透明度和问责度。

    

    基础模型（例如 ChatGPT、StableDiffusion）广泛影响社会，因此需要社会的关注。虽然这些模型本身受到了广泛的关注，但为了准确地描述它们的影响，我们必须考虑更广泛的社会技术生态系统。我们提出了一种名为「生态图表」的文档框架，以透明地集中这个生态系统的知识。生态图表由资产（数据集、模型、应用程序）组成，这些资产通过依赖关系链接在一起，这些关系指示了技术（例如 Bing 如何依赖 GPT-4）和社交（例如 Microsoft 如何依赖 OpenAI）之间的关系。为了补充图形结构，每个资产都进一步丰富了细粒度的元数据（例如许可或培训排放）。我们在 https://crfm.stanford.edu/ecosystem-graphs/ 上广泛记录生态系统。截至 2023 年 3 月 16 日，我们注释了来自 63 个组织的 262 个资产（64 个数据集，128 个模型，70 个应用程序），它们由 356 种依赖关系链接在一起。我们展示了生态图表作为一种工具，可以提高基础模型及其更广泛的社会技术生态系统的透明度和问责度，并呼吁采用它作为一种常见的文档框架。

    Foundation models (e.g. ChatGPT, StableDiffusion) pervasively influence society, warranting immediate social attention. While the models themselves garner much attention, to accurately characterize their impact, we must consider the broader sociotechnical ecosystem. We propose Ecosystem Graphs as a documentation framework to transparently centralize knowledge of this ecosystem. Ecosystem Graphs is composed of assets (datasets, models, applications) linked together by dependencies that indicate technical (e.g. how Bing relies on GPT-4) and social (e.g. how Microsoft relies on OpenAI) relationships. To supplement the graph structure, each asset is further enriched with fine-grained metadata (e.g. the license or training emissions). We document the ecosystem extensively at https://crfm.stanford.edu/ecosystem-graphs/. As of March 16, 2023, we annotate 262 assets (64 datasets, 128 models, 70 applications) from 63 organizations linked by 356 dependencies. We show Ecosystem Graphs functions a
    
[^70]: TabRet: 预训练Transformer-based表格模型，支持未知列

    TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])

    [http://arxiv.org/abs/2303.15747](http://arxiv.org/abs/2303.15747)

    提出了一种可预训练的Transformer-based表格模型：TabRet，能够支持未知列，并在医疗保健分类任务上表现优秀。重新标记化和随机洗牌增强对性能提升有贡献。

    

    我们提出了一种名为TabRet的可预训练Transformer-based表格模型。TabRet旨在为包含未在预训练中见过的列的下游任务提供支持。与其他方法不同，TabRet在微调之前有一个额外的学习步骤，称为重新标记化，它基于遮蔽自动编码损失来校准特征嵌入。在实验中，我们使用大量的公共健康调查数据对TabRet进行预训练，并在医疗保健分类任务上进行微调，在四个数据集上实现了最佳AUC性能。此外，消融研究表明，在预训练期间进行重新标记化和随机洗牌增强对性能提升有贡献。

    We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
    
[^71]: qEUBO: 基于决策理论的、用于优化偏好反馈的贝叶斯优化函数

    qEUBO: A Decision-Theoretic Acquisition Function for Preferential Bayesian Optimization. (arXiv:2303.15746v1 [cs.LG])

    [http://arxiv.org/abs/2303.15746](http://arxiv.org/abs/2303.15746)

    本文介绍了一种新的用于优化偏好反馈的贝叶斯优化函数qEUBO，并展示了它在许多设置中优于现有的采集函数。在充分的条件下，qEUBO的遗憾收敛速度快于现有采集函数qEI。

    

    偏好贝叶斯优化(PBO)是一种用于使用偏好反馈优化决策制定者潜在效用函数的框架。本文将最好选项的预期效用(qEUBO)引入PBO作为一种新的采集函数。当决策制定者的响应无噪声时，我们展示qEUBO是一步贝叶斯最优的，并且与流行的知识梯度采集函数等效。我们还展示，当决策制定者的响应受到噪声污染时，qEUBO在一步贝叶斯最优策略上享有附加的近似保证。我们对qEUBO进行了广泛的评估，并证明在许多设置中，它优于PBO的最先进采集函数。最后，我们展示，在充分的正则化条件下，qEUBO的贝叶斯简单遗憾将以$O(1/n)$的速度趋近于零，其中$n$是查询数量。相比之下，我们展示了对于流行的PBO采集函数qEI，简单遗憾收敛速度较慢，为$O(1/\sqrt{n})$。

    Preferential Bayesian optimization (PBO) is a framework for optimizing a decision maker's latent utility function using preference feedback. This work introduces the expected utility of the best option (qEUBO) as a novel acquisition function for PBO. When the decision maker's responses are noise-free, we show that qEUBO is one-step Bayes optimal and thus equivalent to the popular knowledge gradient acquisition function. We also show that qEUBO enjoys an additive constant approximation guarantee to the one-step Bayes-optimal policy when the decision maker's responses are corrupted by noise. We provide an extensive evaluation of qEUBO and demonstrate that it outperforms the state-of-the-art acquisition functions for PBO across many settings. Finally, we show that, under sufficient regularity conditions, qEUBO's Bayesian simple regret converges to zero at a rate $o(1/n)$ as the number of queries, $n$, goes to infinity. In contrast, we show that simple regret under qEI, a popular acquisiti
    
[^72]: 关于递归特征机器的特征缩放

    On Feature Scaling of Recursive Feature Machines. (arXiv:2303.15745v1 [cs.LG])

    [http://arxiv.org/abs/2303.15745](http://arxiv.org/abs/2303.15745)

    本报告通过实验探究了递归特征机器的行为，发现其在添加随机噪声特征时MSE曲线呈现出降低-增加-降低的模式，并且与神经网络的“双峰下降”现象相似，为后续研究奠定基础。

    

    本技术报告通过一系列在回归数据集上的实验，探究了递归特征机器(RFMs)的行为，RFMs是一种通过平均梯度外积来递归地学习特征的新型核机器。当在数据集中迭代地添加随机噪声特征时，我们观察到均方误差(MSE)曲线呈现出降低-增加-降低的有趣模式。这种行为在不同的数据集大小、噪声参数和目标函数下保持一致。有趣的是，观察到的MSE曲线与深度神经网络中观察到的“双峰下降”现象相似，暗示RFMs和神经网络行为之间存在新的联系。这份报告为未来研究这种奇妙行为奠定了基础。

    In this technical report, we explore the behavior of Recursive Feature Machines (RFMs), a type of novel kernel machine that recursively learns features via the average gradient outer product, through a series of experiments on regression datasets. When successively adding random noise features to a dataset, we observe intriguing patterns in the Mean Squared Error (MSE) curves with the test MSE exhibiting a decrease-increase-decrease pattern. This behavior is consistent across different dataset sizes, noise parameters, and target functions. Interestingly, the observed MSE curves show similarities to the "double descent" phenomenon observed in deep neural networks, hinting at new connection between RFMs and neural network behavior. This report lays the groundwork for future research into this peculiar behavior.
    
[^73]: 合同扩张随机近似的浓度：加法和乘法噪声

    Concentration of Contractive Stochastic Approximation: Additive and Multiplicative Noise. (arXiv:2303.15740v1 [cs.LG])

    [http://arxiv.org/abs/2303.15740](http://arxiv.org/abs/2303.15740)

    本文研究了具有合同算子的随机逼近算法在有界乘法噪声和加性次高斯噪声设置下的浓度行为，提供了关于收敛误差的极大浓度不等式，并表明这些误差具有亚高斯尾巴或者超多项式尾巴。同时还发现，乘法噪声情况下一般不可能实现亚指数尾巴。

    

    本文研究了在任何范数下，具有合同算子的随机逼近(SA)算法的浓度行为。 我们考虑两种情况，其中迭代可能无界：（1）有界乘法噪声，（2）加性次高斯噪声。 我们得到了关于收敛误差的极大浓度不等式，并表明这些误差在加性噪声设置下具有亚高斯尾巴，在乘法噪声设置下具有超多项式尾巴（快于多项式衰减）。 此外，我们提供了一个不可能结果，显示通常无法通过乘法噪声的SA实现亚指数尾巴。 为了确立这些结果，我们开发了一种新的自举论证，其中涉及边界误差的广义Moreau包络的矩生成函数和指数超马尔可夫构造，以启用使用Ville的极大不等式。

    In this work, we study the concentration behavior of a stochastic approximation (SA) algorithm under a contractive operator with respect to an arbitrary norm. We consider two settings where the iterates are potentially unbounded: (1) bounded multiplicative noise, and (2) additive sub-Gaussian noise. We obtain maximal concentration inequalities on the convergence errors, and show that these errors have sub-Gaussian tails in the additive noise setting, and super-polynomial tails (faster than polynomial decay) in the multiplicative noise setting. In addition, we provide an impossibility result showing that it is in general not possible to achieve sub-exponential tails for SA with multiplicative noise. To establish these results, we develop a novel bootstrapping argument that involves bounding the moment generating function of the generalized Moreau envelope of the error and the construction of an exponential supermartingale to enable using Ville's maximal inequality.  To demonstrate the a
    
[^74]: 深度ReLU神经网络在过参数化情况下的贝叶斯自由能

    Bayesian Free Energy of Deep ReLU Neural Network in Overparametrized Cases. (arXiv:2303.15739v1 [cs.LG])

    [http://arxiv.org/abs/2303.15739](http://arxiv.org/abs/2303.15739)

    本研究针对深度ReLU神经网络，证明了过参数化情况下的Bayesian自由能是有界的，说明Bayesian广义误差不会增加。

    

    在人工智能的许多研究领域中，深度神经网络已被证明可用于估计高维输入空间中的未知函数。然而，它们的泛化性能尚未从理论角度完全澄清，因为它们是不可识别的和奇异的学习机器。此外，ReLU函数不可微，奇异学习理论中的代数或解析方法无法应用于它。本文研究了一种过参数化情况下的深度ReLU神经网络，并证明了Bayesian自由能是有界的，即使层数比估计未知数据生成函数所必需的层数更多。由于Bayesian广义误差等于样本大小的自由能增加，因此我们的结果也表明，Bayesian广义误差不会增加。

    In many research fields in artificial intelligence, it has been shown that deep neural networks are useful to estimate unknown functions on high dimensional input spaces. However, their generalization performance is not yet completely clarified from the theoretical point of view because they are nonidentifiable and singular learning machines. Moreover, a ReLU function is not differentiable, to which algebraic or analytic methods in singular learning theory cannot be applied. In this paper, we study a deep ReLU neural network in overparametrized cases and prove that the Bayesian free energy, which is equal to the minus log marginal likelihoodor the Bayesian stochastic complexity, is bounded even if the number of layers are larger than necessary to estimate an unknown data-generating function. Since the Bayesian generalization error is equal to the increase of the free energy as a function of a sample size, our result also shows that the Bayesian generalization error does not increase ev
    
[^75]: 求解正则化的exp、cosh和sinh回归问题

    Solving Regularized Exp, Cosh and Sinh Regression Problems. (arXiv:2303.15725v1 [cs.LG])

    [http://arxiv.org/abs/2303.15725](http://arxiv.org/abs/2303.15725)

    该文研究和解决了正则化指数回归问题，根据大型语言模型注意力计算的灵感，使用近似牛顿方法在输入稀疏时间内求解。

    

    在现代机器学习中，注意力计算是训练大型语言模型（如Transformer、GPT-4和ChatGPT）的基本任务。该文研究了受softmax/exp单元启发的指数回归问题。标准指数回归是非凸的。我们研究了指数回归问题的正则化版本，这是一个凸问题。我们使用近似牛顿方法以输入稀疏时间解决问题。形式上，给定矩阵$A\in \mathbb{R}^{n\times d}$，$b\in \mathbb{R}^n$，$w\in\mathbb{R}^n$和任何函数$\exp,\cosh$和$\sinh$，记作$f$。目标是找到最优$x$，使得$0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$最小化。研究并解决了正则化指数回归问题，其中函数为$\exp, \cosh$和$\sinh$，并使用近似牛顿法在输入稀疏时间内求解，灵感来自大型语言模型中的注意力计算。

    In modern machine learning, attention computation is a fundamental task for training large language models such as Transformer, GPT-4 and ChatGPT. In this work, we study exponential regression problem which is inspired by the softmax/exp unit in the attention mechanism in large language models. The standard exponential regression is non-convex. We study the regularization version of exponential regression problem which is a convex problem. We use approximate newton method to solve in input sparsity time.  Formally, in this problem, one is given matrix $A \in \mathbb{R}^{n \times d}$, $b \in \mathbb{R}^n$, $w \in \mathbb{R}^n$ and any of functions $\exp, \cosh$ and $\sinh$ denoted as $f$. The goal is to find the optimal $x$ that minimize $ 0.5 \| f(Ax) - b \|_2^2 + 0.5 \| \mathrm{diag}(w) A x \|_2^2$. The straightforward method is to use the naive Newton's method. Let $\mathrm{nnz}(A)$ denote the number of non-zeros entries in matrix $A$. Let $\omega$ denote the exponent of matrix multi
    
[^76]: 显式规划有助于语言模型进行逻辑推理

    Explicit Planning Helps Language Models in Logical Reasoning. (arXiv:2303.15714v1 [cs.CL])

    [http://arxiv.org/abs/2303.15714](http://arxiv.org/abs/2303.15714)

    本文提出了一个新的系统，使用语言模型进行多步逻辑推理，采用了显式规划来帮助做出更明智的决策，比其他竞争系统表现更好，显式规划在系统性能中起着关键作用。

    

    语言模型在各种自然语言处理任务中表现出色。本文提出了一个新颖的系统，采用语言模型进行多步逻辑推理。我们的系统将显式规划纳入到推理过程中，因此可以通过展望未来的效果来做出更明智的决策。在实验中，我们的全套系统在多项选择题答题任务中明显优于其他竞争系统，尽管只有约15亿个参数，但与GPT-3-davinci表现相当。我们进行了多个消融研究以证明显式规划在系统性能中起着关键作用。

    Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose a novel system that uses language models to perform multi-step logical reasoning. Our system incorporates explicit planning into its inference procedure, thus able to make more informed reasoning decisions at each step by looking ahead into their future effects. In our experiments, our full system significantly outperforms other competing systems. On a multiple-choice question answering task, our system performs competitively compared to GPT-3-davinci despite having only around 1.5B parameters. We conduct several ablation studies to demonstrate that explicit planning plays a crucial role in the system's performance.
    
[^77]: 基于信息导向随机游走的分布式图嵌入方法

    Distributed Graph Embedding with Information-Oriented Random Walks. (arXiv:2303.15702v1 [cs.DC])

    [http://arxiv.org/abs/2303.15702](http://arxiv.org/abs/2303.15702)

    本文提出了一种名为DistGER的分布式图嵌入方法，基于信息导向随机游走策略，利用多种优化技术实现了高效的十亿级别图嵌入。

    

    图嵌入将图节点映射到低维向量中，被广泛应用于机器学习任务中。本文提出了一种分布式的、基于信息导向随机游走的通用图嵌入框架DistGER，可用于处理十亿级别的图嵌入。该方法通过增量式地计算信息导向的随机游走，并利用了多种邻近感知、流式、并行的图划分策略，实现了在不同机器上既高质量的本地划分，又出色的负载均衡。同时DistGER改进了分布式Skip-Gram学习模型以生成节点嵌入，并优化了在分布式环境中的访问局部性、CPU吞吐量和同步效率。

    Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks. The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a general-purpose, distributed, information-centric random walk-based graph embedding framework, DistGER, which can scale to embed billion-edge graphs. DistGER incrementally computes information-centric random walks. It further leverages a multi-proximity-aware, streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. DistGER also improves the distributed Skip-Gram learning model to generate node embeddings by optimizing the access locality, CPU throughput, and synchronizat
    
[^78]: 病理图像大规模预训练用于小规模病理基准微调

    Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks. (arXiv:2303.15693v1 [cs.CV])

    [http://arxiv.org/abs/2303.15693](http://arxiv.org/abs/2303.15693)

    本文探讨了病理图像预训练对小规模病理基准微调的影响，通过自监督学习方法，以PTCGA200为训练集进行预训练的ResNet50在微调时表现更好，优于imagenet2012预训练。MoCov2预训练的ResNet50在PCam200和segPANDA200上表现优秀，且收敛速度更快。

    

    在对小型目标数据集进行微调之前，在大型图像数据集上预训练深度学习模型是标准步骤。大型数据集通常是通用图像（例如imagenet2012），而小型数据集可以是具有与大型数据集不同分布的专业数据集。然而，当大型数据集是专业化的且具有与小型数据集相似的分布时，这种“大到小”的策略并未得到很好的验证。我们新编译了三个苏木精和伊红染色图像数据集，一个大型数据集（PTCGA200）和两个放大调整的小型数据集（PCam200和segPANDA200）。通过监督学习和自监督学习方法训练了主要的深度学习模型，并在小型数据集上进行了肿瘤分类和组织分割基准的微调。在PTCGA200上以MoCov2，SimCLR和BYOL预训练的ResNet50在微调时比imagenet2012预训练更好（精度分别为83.94％，86.41％，84.91％和82.72％）。此外，以MoCov2预训练的ResNet50在收敛速度更快的情况下实现了PCam200和segPANDA200的最优性能，而比imagenet2012预训练的ResNet50更快。

    Pretraining a deep learning model on large image datasets is a standard step before fine-tuning the model on small targeted datasets. The large dataset is usually general images (e.g. imagenet2012) while the small dataset can be specialized datasets that have different distributions from the large dataset. However, this 'large-to-small' strategy is not well-validated when the large dataset is specialized and has a similar distribution to small datasets. We newly compiled three hematoxylin and eosin-stained image datasets, one large (PTCGA200) and two magnification-adjusted small datasets (PCam200 and segPANDA200). Major deep learning models were trained with supervised and self-supervised learning methods and fine-tuned on the small datasets for tumor classification and tissue segmentation benchmarks. ResNet50 pretrained with MoCov2, SimCLR, and BYOL on PTCGA200 was better than imagenet2012 pretraining when fine-tuned on PTCGA200 (accuracy of 83.94%, 86.41%, 84.91%, and 82.72%, respect
    
[^79]: 带有交叉视图部分样本和原型对齐的深度不完整多视图聚类

    Deep Incomplete Multi-view Clustering with Cross-view Partial Sample and Prototype Alignment. (arXiv:2303.15689v1 [cs.LG])

    [http://arxiv.org/abs/2303.15689](http://arxiv.org/abs/2303.15689)

    论文提出了一种新的深度不完整多视图聚类算法，采用了部分样本的交叉视图聚合机制和两个原型对齐策略，以解决不完整多视图聚类问题。

    

    现有的多视图聚类方法成功的前提是样本在多视图中保持完整性的假设。然而，在实际场景中，多视图样本由于数据损坏或传感器故障而部分地可用，导致不完整多视图聚类研究陷入困境。虽然已经有一些尝试来解决不完整多视图聚类问题，但存在以下缺点：（i）现有方法主要采用交叉视图对比学习，强制每个样本在多个视图中的表示是完全相同的，这可能会忽略视图差异和表示的灵活性；（ii）由于在多个视图中不存在未观察到的样本，所得到的聚类原型可能不对齐和偏斜，导致聚合不正确。为了解决以上问题，我们提出了一种跨视图部分样本和原型对齐网络(CPSPAN)，用于深度不完整多视图聚类。首先，与现有的基于对比的方法不同，我们采用部分样本的交叉视图聚合机制来捕捉视图之间的差异特征。其次，我们采用两个原型对齐策略来解决原型不对齐和聚合不正确的问题。

    The success of existing multi-view clustering relies on the assumption of sample integrity across multiple views. However, in real-world scenarios, samples of multi-view are partially available due to data corruption or sensor failure, which leads to incomplete multi-view clustering study (IMVC). Although several attempts have been proposed to address IMVC, they suffer from the following drawbacks: i) Existing methods mainly adopt cross-view contrastive learning forcing the representations of each sample across views to be exactly the same, which might ignore view discrepancy and flexibility in representations; ii) Due to the absence of non-observed samples across multiple views, the obtained prototypes of clusters might be unaligned and biased, leading to incorrect fusion. To address the above issues, we propose a Cross-view Partial Sample and Prototype Alignment Network (CPSPAN) for Deep Incomplete Multi-view Clustering. Firstly, unlike existing contrastive-based methods, we adopt pa
    
[^80]: 面向知识图谱补全的Transformer预训练。

    Pre-training Transformers for Knowledge Graph Completion. (arXiv:2303.15682v1 [cs.CL])

    [http://arxiv.org/abs/2303.15682](http://arxiv.org/abs/2303.15682)

    该论文介绍了一种面向知识图谱补全的归纳式表示模型（iHT），通过大规模预训练，iHT表示可转移且在多个数据集上取得了最先进的结果。

    

    由于图结构的异构性和多关系性，学习知识图谱（KGs）的可转移表示是具有挑战性的。受Transformer基于预训练语言模型在学习文本方面的成功启发，我们引入了一种新型的面向KG补全的归纳式表示模型（iHT），通过大规模预训练，iHT由实体编码器（例如BERT）和邻居感知的关系评分函数组成，两者都由Transformer参数化。我们首先在大型KG数据集Wikidata5M上预训练iHT。我们的方法在匹配评估上实现了新的最先进结果，相对于先前SOTA模型，平均倒数排名提高了25％以上。当在具有实体和关系移位的较小KG上进一步微调时，预训练的iHT表示被证明是可转移的，显着提高了FB15K-237和WN18RR的性能。

    Learning transferable representation of knowledge graphs (KGs) is challenging due to the heterogeneous, multi-relational nature of graph structures. Inspired by Transformer-based pretrained language models' success on learning transferable representation for texts, we introduce a novel inductive KG representation model (iHT) for KG completion by large-scale pre-training. iHT consists of a entity encoder (e.g., BERT) and a neighbor-aware relational scoring function both parameterized by Transformers. We first pre-train iHT on a large KG dataset, Wikidata5M. Our approach achieves new state-of-the-art results on matched evaluations, with a relative improvement of more than 25% in mean reciprocal rank over previous SOTA models. When further fine-tuned on smaller KGs with either entity and relational shifts, pre-trained iHT representations are shown to be transferable, significantly improving the performance on FB15K-237 and WN18RR.
    
[^81]: 基于GNN的求解时间无关PDE的物理求解器

    GNN-based physics solver for time-independent PDEs. (arXiv:2303.15681v1 [cs.LG])

    [http://arxiv.org/abs/2303.15681](http://arxiv.org/abs/2303.15681)

    本文介绍了两种基于GNN的深度神经网络架构——边增强GNN和多GNN来解决时间无关PDE的问题，这两个网络明显比基线模型准确率更高，同时具备良好的泛化能力。

    

    物理基础深度学习框架已被证明在准确建模复杂物理系统的动态过程方面具有广泛的推广能力，然而，时间无关问题需要跨越计算域进行长距离的信息交换才能获得准确的预测，这对于图形神经网络(GNN)来说需要更深的网络，这反过来又可能会损害或减缓训练过程。在这项工作中，我们提出了两个GNN架构来克服这一挑战——边增强GNN和多GNN。我们表明，当应用于时间无关的固体力学问题时，这两个网络明显比基线方法更好（提高了1.5到2倍）。此外，所提出的体系结构具有良好的泛化能力，可以适用于未见过的域、边界条件和材料。这里，通过一种新颖的坐标转换来处理变量域。

    Physics-based deep learning frameworks have shown to be effective in accurately modeling the dynamics of complex physical systems with generalization capability across problem inputs. However, time-independent problems pose the challenge of requiring long-range exchange of information across the computational domain for obtaining accurate predictions. In the context of graph neural networks (GNNs), this calls for deeper networks, which, in turn, may compromise or slow down the training process. In this work, we present two GNN architectures to overcome this challenge - the Edge Augmented GNN and the Multi-GNN. We show that both these networks perform significantly better (by a factor of 1.5 to 2) than baseline methods when applied to time-independent solid mechanics problems. Furthermore, the proposed architectures generalize well to unseen domains, boundary conditions, and materials. Here, the treatment of variable domains is facilitated by a novel coordinate transformation that enabl
    
[^82]: DisWOT: 无需训练的知识蒸馏学生架构搜索

    DisWOT: Student Architecture Search for Distillation WithOut Training. (arXiv:2303.15678v1 [cs.CV])

    [http://arxiv.org/abs/2303.15678](http://arxiv.org/abs/2303.15678)

    本文提出了一种无需训练的框架来搜索最适合给定教师的最佳学生架构，以提高知识蒸馏的效果。其通过度量语义激活映射条件下的相似性矩阵来选择最佳学生，而不是通过传统的训练方法。

    

    知识蒸馏(KD)是一种有效的训练策略，可以在笨重的教师的指导下提高轻量级学生模型的性能。然而，教师和学生之间的大型架构差异限制了蒸馏效果。相对于以前的自适应蒸馏方法来减少教师和学生之间的差距，我们探索了一种新的无需训练框架，以搜索给定教师的最佳学生架构。

    Knowledge distillation (KD) is an effective training strategy to improve the lightweight student models under the guidance of cumbersome teachers. However, the large architecture difference across the teacher-student pairs limits the distillation gains. In contrast to previous adaptive distillation methods to reduce the teacher-student gap, we explore a novel training-free framework to search for the best student architectures for a given teacher. Our work first empirically show that the optimal model under vanilla training cannot be the winner in distillation. Secondly, we find that the similarity of feature semantics and sample relations between random-initialized teacher-student networks have good correlations with final distillation performances. Thus, we efficiently measure similarity matrixs conditioned on the semantic activation maps to select the optimal student via an evolutionary algorithm without any training. In this way, our student architecture search for Distillation Wit
    
[^83]: 凸多级随机优化的数值方法

    Numerical Methods for Convex Multistage Stochastic Optimization. (arXiv:2303.15672v1 [math.OC])

    [http://arxiv.org/abs/2303.15672](http://arxiv.org/abs/2303.15672)

    本文讨论了在随机规划和随机最优控制中凸多级随机问题的数值解法，包括动态规划和割平面方法，旨在解决维度诅咒问题。

    

    随机环境下涉及顺序决策的优化问题已在随机规划(SP)、随机最优控制(SOC)和马尔可夫决策过程(MDP)中进行了研究。本文主要集中讨论SP和SOC建模方法。在这些框架中，存在自然情况下所考虑的问题是凸问题。顺序优化的经典方法是基于动态规划，但它存在所谓的“维度诅咒”问题，即随着状态变量维度的增加，它的计算复杂度呈指数级增长。近年来，解决凸多级随机问题的最新进展是基于动态规划方程的成本函数逐步逼近的割平面近似。动态环境中的割平面类型算法是本文的主要讨论内容之一。我们还讨论了应用于多级随机优化问题的随机逼近类型方法。

    Optimization problems involving sequential decisions in a stochastic environment were studied in Stochastic Programming (SP), Stochastic Optimal Control (SOC) and Markov Decision Processes (MDP). In this paper we mainly concentrate on SP and SOC modelling approaches. In these frameworks there are natural situations when the considered problems are convex. Classical approach to sequential optimization is based on dynamic programming. It has the problem of the so-called ``Curse of Dimensionality", in that its computational complexity increases exponentially with increase of dimension of state variables. Recent progress in solving convex multistage stochastic problems is based on cutting planes approximations of the cost-to-go (value) functions of dynamic programming equations. Cutting planes type algorithms in dynamical settings is one of the main topics of this paper. We also discuss Stochastic Approximation type methods applied to multistage stochastic optimization problems. From the c
    
[^84]: 低资源语言下的无监督预训练文本转语音模型

    Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages. (arXiv:2303.15669v1 [eess.AS])

    [http://arxiv.org/abs/2303.15669](http://arxiv.org/abs/2303.15669)

    本文提出了一种针对低资源语言下无监督预训练的文本转语音模型，利用大量未转录的语音数据进行预训练，可显著减少训练模型所需的匹配转录数据量，进一步提升了数据效率，实验证明方法有效性。

    

    当大量转录音频数据用于训练时，神经文本到语音（TTS）模型可以合成自然的人类语音。但是，收集这样的大规模转录数据很昂贵。本文提出了一种无监督预训练方法，用于对序列到序列的TTS模型进行预训练，利用大量未转录的语音数据。通过我们的预训练，我们可以显着减少训练模型所需的匹配转录数据量，用于目标下游TTS任务的训练。主要思想是预先训练模型，以从扭曲的mel频谱图中重建出去扭曲的mel频谱图，这可能使模型学会了输入和输出序列之间的适当时间分配关系。此外，我们提出了一种数据增强方法，可进一步提高微调中的数据效率。我们在低资源语言场景中实验证明了我们提出的方法的有效性，与竞争方法相比表现出色。代码和音频样本可以在我们的项目页面上找到。

    Neural text-to-speech (TTS) models can synthesize natural human speech when trained on large amounts of transcribed speech. However, collecting such large-scale transcribed data is expensive. This paper proposes an unsupervised pre-training method for a sequence-to-sequence TTS model by leveraging large untranscribed speech data. With our pre-training, we can remarkably reduce the amount of paired transcribed data required to train the model for the target downstream TTS task. The main idea is to pre-train the model to reconstruct de-warped mel-spectrograms from warped ones, which may allow the model to learn proper temporal assignment relation between input and output sequences. In addition, we propose a data augmentation method that further improves the data efficiency in fine-tuning. We empirically demonstrate the effectiveness of our proposed method in low-resource language scenarios, achieving outstanding performance compared to competing methods. The code and audio samples are av
    
[^85]: 钋镓碲热电材料在激光粉床熔化增材制造中的功率因子预测

    Predicting Thermoelectric Power Factor of Bismuth Telluride During Laser Powder Bed Fusion Additive Manufacturing. (arXiv:2303.15663v1 [cs.LG])

    [http://arxiv.org/abs/2303.15663](http://arxiv.org/abs/2303.15663)

    该论文使用机器学习模型，预测钋镓碲（Bi2Te3）在激光粉床熔化增材制造过程中的功率因子，从而提高材料特性。

    

    增材制造工序（如激光粉床熔化）可以通过逐层铺散、熔化粉末直到创建自由形态的部件来制造物体。为了改善增材制造工序中所涉及的材料特性，预测材料性质与加工条件的函数关系非常重要。在热电材料中，功率因子是衡量材料将热能转换为电能效率的一种指标。虽然早期的研究已经使用不同的技术预测了不同热电材料的材料特性，但还没有探索使用机器学习模型预测钋镓碲 (Bi2Te3)在增材制造过程中的功率因子。这一点很重要，因为Bi2Te3是低温应用的标准材料。因此，我们使用涉及到的制造加工参数数据和在增材制造过程中收集的原位传感器监测数据来预测Bi2Te3的功率因子。

    An additive manufacturing (AM) process, like laser powder bed fusion, allows for the fabrication of objects by spreading and melting powder in layers until a freeform part shape is created. In order to improve the properties of the material involved in the AM process, it is important to predict the material characterization property as a function of the processing conditions. In thermoelectric materials, the power factor is a measure of how efficiently the material can convert heat to electricity. While earlier works have predicted the material characterization properties of different thermoelectric materials using various techniques, implementation of machine learning models to predict the power factor of bismuth telluride (Bi2Te3) during the AM process has not been explored. This is important as Bi2Te3 is a standard material for low temperature applications. Thus, we used data about manufacturing processing parameters involved and in-situ sensor monitoring data collected during AM of
    
[^86]: 土壤地下水流边界-解映射在Toth盆地的应用

    Boundary-to-Solution Mapping for Groundwater Flows in a Toth Basin. (arXiv:2303.15659v1 [physics.geo-ph])

    [http://arxiv.org/abs/2303.15659](http://arxiv.org/abs/2303.15659)

    本文提出了一种新的方法使用DeepONet实现边界-解映射，以求解Toth盆地地下水流方程。该方法将物理域的几何形状和边界条件作为输入，输出地下水流方程的稳态解。研究利用傅里叶级数或分段线性表示来近似顶部和底部边界，并针对不同的边界条件实现了两个不同的DeepONet版本。

    

    本文提出了一种解决任意地形Toth盆地地下水流方程的新方法——使用深度学习进行边界-解映射。作者使用DeepONet生成边界-解映射，以此代替传统的数值求解器，该映射将物理域的几何形状和边界条件作为输入，输出稳态地下水流方程的解。作者通过截断傅里叶级数或分段线性表示来近似顶部和底部边界，并呈现了两个不同的DeepONet实现：一个将Toth盆地嵌入矩形计算域，另一个则通过非线性变换将具有任意顶部和底部边界的Toth盆地映射到矩形计算域中。作者针对顶部的Dirichlet和Robin边界条件以及底部的Neumann边界条件分别实现了DeepONet。

    In this paper, the authors propose a new approach to solving the groundwater flow equation in the Toth basin of arbitrary top and bottom topographies using deep learning. Instead of using traditional numerical solvers, they use a DeepONet to produce the boundary-to-solution mapping. This mapping takes the geometry of the physical domain along with the boundary conditions as inputs to output the steady state solution of the groundwater flow equation. To implement the DeepONet, the authors approximate the top and bottom boundaries using truncated Fourier series or piecewise linear representations. They present two different implementations of the DeepONet: one where the Toth basin is embedded in a rectangular computational domain, and another where the Toth basin with arbitrary top and bottom boundaries is mapped into a rectangular computational domain via a nonlinear transformation. They implement the DeepONet with respect to the Dirichlet and Robin boundary condition at the top and the
    
[^87]: 多任务学习预测早产儿不良新生儿结局

    Predicting Adverse Neonatal Outcomes for Preterm Neonates with Multi-Task Learning. (arXiv:2303.15656v1 [cs.LG])

    [http://arxiv.org/abs/2303.15656](http://arxiv.org/abs/2303.15656)

    本文研究了早产儿不良新生儿结局之间的相关性，提出了多任务学习框架用于联合预测多个不良新生儿结局，有效性得到验证。

    

    对早产儿不良新生儿结局的诊断对早产儿的生存至关重要，因为它使医生能够及时提供治疗。机器学习算法已被证明在预测不良新生儿结局方面是有效的。然而，大多数以前的基于机器学习的方法仅集中在预测单个结果，忽略了不同结果之间的潜在相关性，可能导致结果次优和过拟合问题。在本论文中，我们首先分析了三种不良新生儿结局之间的相关性，然后将多个新生儿结局的诊断形式化为多任务学习问题。我们提出了一个MTL框架来联合预测多个不良新生儿结局。特别是，MTL框架包含共享隐藏层和多个任务特定分支。我们使用121名早产儿的电子健康记录进行了广泛的实验。实证结果表明了MTL框架的有效性。

    Diagnosis of adverse neonatal outcomes is crucial for preterm survival since it enables doctors to provide timely treatment. Machine learning (ML) algorithms have been demonstrated to be effective in predicting adverse neonatal outcomes. However, most previous ML-based methods have only focused on predicting a single outcome, ignoring the potential correlations between different outcomes, and potentially leading to suboptimal results and overfitting issues. In this work, we first analyze the correlations between three adverse neonatal outcomes and then formulate the diagnosis of multiple neonatal outcomes as a multi-task learning (MTL) problem. We then propose an MTL framework to jointly predict multiple adverse neonatal outcomes. In particular, the MTL framework contains shared hidden layers and multiple task-specific branches. Extensive experiments have been conducted using Electronic Health Records (EHRs) from 121 preterm neonates. Empirical results demonstrate the effectiveness of 
    
[^88]: 结构化动态定价：全局收缩模型下的最优遗憾

    Structured Dynamic Pricing: Optimal Regret in a Global Shrinkage Model. (arXiv:2303.15652v1 [cs.LG])

    [http://arxiv.org/abs/2303.15652](http://arxiv.org/abs/2303.15652)

    本文提出了一种在流式纵向数据设置中的动态定价策略，该策略基于全局收缩结构和PSGD方法，并明确地将遗憾作为时间、模型参数和数据集规模的函数。

    

    本文考虑在流式纵向数据设置中的动态定价策略，目的是最大化在大量客户细分中的累计利润。我们考虑一种动态概率模型，其中消费者的偏好和价格敏感度随时间变化。基于这一众所周知的发现，具有相似特征的消费者会表现出相似的行为，我们考虑一种全局收缩结构，该结构假定不同细分中的消费者偏好可以用空间自回归模型很好地近似。在这样的流式纵向设置中，我们通过遗憾来衡量动态定价策略的性能，遗憾是与预先知道模型参数序列的千里眼相比预期的收入损失。我们提出了一种基于惩罚随机梯度下降（PSGD）的定价策略，并明确地将其遗憾作为时间函数、模型参数的时间变化性和消费者数据集的规模。

    We consider dynamic pricing strategies in a streamed longitudinal data set-up where the objective is to maximize, over time, the cumulative profit across a large number of customer segments. We consider a dynamic probit model with the consumers' preferences as well as price sensitivity varying over time. Building on the well-known finding that consumers sharing similar characteristics act in similar ways, we consider a global shrinkage structure, which assumes that the consumers' preferences across the different segments can be well approximated by a spatial autoregressive (SAR) model. In such a streamed longitudinal set-up, we measure the performance of a dynamic pricing policy via regret, which is the expected revenue loss compared to a clairvoyant that knows the sequence of model parameters in advance. We propose a pricing policy based on penalized stochastic gradient descent (PSGD) and explicitly characterize its regret as functions of time, the temporal variability in the model pa
    
[^89]: 学习速率表在分布转移条件下的应用

    Learning Rate Schedules in the Presence of Distribution Shift. (arXiv:2303.15634v1 [cs.LG])

    [http://arxiv.org/abs/2303.15634](http://arxiv.org/abs/2303.15634)

    该论文提出了一种学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔，能够对分布转移具有鲁棒性，同时适用于凸损失函数和非凸损失函数。最优学习速率表通常会在数据分布转移的情况下增加，能够用于高维回归模型和神经网络。

    

    我们设计了学习速率表，以在数据分布发生变化时最小化SGD在线学习的后悔。我们通过随机微分方程的新颖分析，完全表征了在线线性回归的最优学习速率表。对于一般的凸损失函数，我们提出了新的学习速率表，对分布转移具有鲁棒性，我们给出了只有常数差异的后悔上下界。对于非凸损失函数，我们基于估计模型的梯度范数定义了一种后悔概念，并提出了一种学习时间表，以最小化总预期后悔的上限。直观地说，我们预计损失领域的变化需要更多的探索，我们证实了最优学习速率表通常会在数据分布转移的情况下增加。最后，我们提供了针对高维回归模型和神经网络的实验，以说明这些学习速率表的应用。

    We design learning rate schedules that minimize regret for SGD-based online learning in the presence of a changing data distribution. We fully characterize the optimal learning rate schedule for online linear regression via a novel analysis with stochastic differential equations. For general convex loss functions, we propose new learning rate schedules that are robust to distribution shift, and we give upper and lower bounds for the regret that only differ by constants. For non-convex loss functions, we define a notion of regret based on the gradient norm of the estimated models and propose a learning schedule that minimizes an upper bound on the total expected regret. Intuitively, one expects changing loss landscapes to require more exploration, and we confirm that optimal learning rate schedules typically increase in the presence of distribution shift. Finally, we provide experiments for high-dimensional regression models and neural networks to illustrate these learning rate schedule
    
[^90]: 使用集合SINDy和Peridynamic差分算子的移动边界多物理发现

    Multiphysics discovery with moving boundaries using Ensemble SINDy and Peridynamic Differential Operator. (arXiv:2303.15631v1 [cs.LG])

    [http://arxiv.org/abs/2303.15631](http://arxiv.org/abs/2303.15631)

    本研究提出了一种使用集合SINDy和Peridynamic差分算子的新型框架，用于学习具有移动边界现象的潜在物理学。演示了该方法在各种噪声水平下的鲁棒性，适用于任何平滑移动边界前端且没有粘液区域的移动边界问题。

    

    本研究提出了一种用于学习具有移动边界现象的潜在物理学的新型框架。所提出的方法将集合SINDy和Peridynamic差分算子（PDDO）相结合，并假设移动边界物理学在其自身的旋转坐标系中演化。通过使用2D Fisher-Stefan模型考虑测量数据中的各个噪声水平来演示方法的鲁棒性。列出了恢复系数的置信区间，并通过使用恢复系数获得解来描绘移动边界位置的不确定性。尽管本研究的重点是Fisher-Stefan模型，但所提出的方法适用于任何具有平滑移动边界前端且没有粘液区域的移动边界问题。该框架的代码和数据可在以下网址获得：https://github.com/alicanbekar/MB_PDDO-SINDy。

    This study proposes a novel framework for learning the underlying physics of phenomena with moving boundaries. The proposed approach combines Ensemble SINDy and Peridynamic Differential Operator (PDDO) and imposes an inductive bias assuming the moving boundary physics evolve in its own corotational coordinate system. The robustness of the approach is demonstrated by considering various levels of noise in the measured data using the 2D Fisher-Stefan model. The confidence intervals of recovered coefficients are listed, and the uncertainties of the moving boundary positions are depicted by obtaining the solutions with the recovered coefficients. Although the main focus of this study is the Fisher-Stefan model, the proposed approach is applicable to any type of moving boundary problem with a smooth moving boundary front without a mushy region. The code and data for this framework is available at: https://github.com/alicanbekar/MB_PDDO-SINDy.
    
[^91]: 展示量子优势的实用框架：量子模型与经典生成模型的竞速比较

    A Framework for Demonstrating Practical Quantum Advantage: Racing Quantum against Classical Generative Models. (arXiv:2303.15626v1 [quant-ph])

    [http://arxiv.org/abs/2303.15626](http://arxiv.org/abs/2303.15626)

    这篇论文介绍了一个基于一个评估生成模型泛化性能的框架，建立了首个量化的量子与经典生成模型的实用竞速比较（PQA），并比较了量子模型与最优经典算法在这个任务中的表现。

    

    生成模型在经典和量子机器学习中越来越受到关注，并且代表了在近期实现量子优势的有前途的候选项。在本研究中，我们建立在一个评估生成模型泛化性能的框架上，并建立了首个量化的量子与经典生成模型的实用竞速比较（PQA），包括量子电路出生机器（QCBMs）、变压器（TFs）、递归神经网络（RNNs）、变分自编码器（VAEs）和Wasserstein生成对抗网络（WGANs）。在定义了四种不同类型的PQA场景之后，我们专注于比较量子模型与最优经典算法在这个任务中的表现。我们让这些模型在一个明确定义和与应用相关的比赛环境中进行竞速，其中我们使用了20个变量（量子比特）来展示和证明我们的框架。

    Generative modeling has seen a rising interest in both classical and quantum machine learning, and it represents a promising candidate to obtain a practical quantum advantage in the near term. In this study, we build over a proposed framework for evaluating the generalization performance of generative models, and we establish the first quantitative comparative race towards practical quantum advantage (PQA) between classical and quantum generative models, namely Quantum Circuit Born Machines (QCBMs), Transformers (TFs), Recurrent Neural Networks (RNNs), Variational Autoencoders (VAEs), and Wasserstein Generative Adversarial Networks (WGANs). After defining four types of PQAs scenarios, we focus on what we refer to as potential PQA, aiming to compare quantum models with the best-known classical algorithms for the task at hand. We let the models race on a well-defined and application-relevant competition setting, where we illustrate and demonstrate our framework on 20 variables (qubits) g
    
[^92]: 针对基于激励的需求响应的在线学习

    Online Learning for Incentive-Based Demand Response. (arXiv:2303.15617v1 [cs.LG])

    [http://arxiv.org/abs/2303.15617](http://arxiv.org/abs/2303.15617)

    本文研究了在线学习管理需求响应（DR）资源中消费者基准估计的问题，并提出了一种采用最小二乘进行估计的在线学习方案，再通过引入激励价格上的扰动实现勘探和开发的平衡。

    

    本文研究了在线学习管理需求响应（DR）资源的问题。典型DR机制要求DR经理为参与的消费者分配一个基准，其中基准是消费者计数事实消耗的估计，如果不叫消费者提供DR服务，那么基准就是计数的理论消耗。估算基准的挑战在于消费者有鼓励膨胀基准估计的动机。我们考虑学习在线估算基线和在这样的激励下优化一段时间的操作成本。提出了一种在线学习方案，它采用最小二乘进行估计，同时在DR服务或负荷裁剪的激励价格上引入扰动，旨在平衡在线学习中出现的勘探和开发的折衷。我们证明了，我们的提议方案能够实现与最优操作相比非常低的遗憾度（$ \mathcal {O} \left((\log {T})^2\right)$）

    In this paper, we consider the problem of learning online to manage Demand Response (DR) resources. A typical DR mechanism requires the DR manager to assign a baseline to the participating consumer, where the baseline is an estimate of the counterfactual consumption of the consumer had it not been called to provide the DR service. A challenge in estimating baseline is the incentive the consumer has to inflate the baseline estimate. We consider the problem of learning online to estimate the baseline and to optimize the operating costs over a period of time under such incentives. We propose an online learning scheme that employs least-squares for estimation with a perturbation to the reward price (for the DR services or load curtailment) that is designed to balance the exploration and exploitation trade-off that arises with online learning. We show that, our proposed scheme is able to achieve a very low regret of $\mathcal{O}\left((\log{T})^2\right)$ with respect to the optimal operating
    
[^93]: HD-Bind：使用低精度、超高维度二进制编码分子结构

    HD-Bind: Encoding of Molecular Structure with Low Precision, Hyperdimensional Binary Representations. (arXiv:2303.15604v1 [q-bio.BM])

    [http://arxiv.org/abs/2303.15604](http://arxiv.org/abs/2303.15604)

    HD-Bind是一种用于表示分子结构的超高维度二进制向量的方法，能够预测蛋白质中小分子的分子间结合亲和力。它显著减少了存储和计算所需的位数，并在药物发现中优于现有的方法。

    

    近年来，随着化学合成技术的进步，公开可用的类似药物的分子集合已经增长到数十亿个可能性。然而，传统的从大量药物候选中确定“命中”分子的方法依赖于生物物理理论来计算药物与其蛋白质靶标之间结合相互作用的吉布斯自由能的近似值。该方法的主要缺点是即使对于相对较小的分子集合，也需要出色的计算能力。Hyperdimensional Computing（HDC）是一种最近提出的学习范式，它能够利用低精度二进制向量算术来构建数据的有效表示，而不需要渐进式优化方法，这些优化方法在许多传统的机器学习和深度学习方法中是必需的。本文介绍了HD-Bind，一种将分子结构表示为超高维度二进制向量的方法，用于预测蛋白质中小分子的分子间结合亲和力。HD-Bind构建了超维二进制向量，表示分子结构，并能够捕获与蛋白质-配体结合相关的结构特征，同时显著减少了存储和计算所需的位数。我们的结果表明，HD-Bind优于现有的方法，并为在其他药物发现应用中使用HDC铺平了道路。

    Publicly available collections of drug-like molecules have grown to comprise 10s of billions of possibilities in recent history due to advances in chemical synthesis. Traditional methods for identifying ``hit'' molecules from a large collection of potential drug-like candidates have relied on biophysical theory to compute approximations to the Gibbs free energy of the binding interaction between the drug to its protein target. A major drawback of the approaches is that they require exceptional computing capabilities to consider for even relatively small collections of molecules.  Hyperdimensional Computing (HDC) is a recently proposed learning paradigm that is able to leverage low-precision binary vector arithmetic to build efficient representations of the data that can be obtained without the need for gradient-based optimization approaches that are required in many conventional machine learning and deep learning approaches. This algorithmic simplicity allows for acceleration in hardwa
    
[^94]: 揭示个人信息学中的偏见

    Uncovering Bias in Personal Informatics. (arXiv:2303.15592v1 [cs.CY])

    [http://arxiv.org/abs/2303.15592](http://arxiv.org/abs/2303.15592)

    该论文是第一个对个人信息学系统中的偏见进行实证和分析研究的工作，研究包括原始数据和整个机器学习周期中的偏见，并找出其中的实践和道德影响。

    

    由智能手机和可穿戴设备驱动的个人信息学（PI）系统，通过提供有意义且可执行的见解，消除了用户与其健康信息之间的障碍，使人们能够过上更健康的生活。今天，数十亿用户使用此类系统来监测不仅是身体活动和睡眠，还有生命体征、女性健康和心脏健康等。尽管被广泛使用，并且处理敏感的PI数据，但偏见问题尚未得到系统的调查。这项工作是第一个对包括原始数据和整个机器学习生命周期中的偏见进行全面实证和分析研究的工作，并使用迄今为止最详细的框架。

    Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information. Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others. %Despite their widespread usage, the processing of particularly sensitive personal data, and their proximity to domains known to be susceptible to bias, such as healthcare, bias in PI has not been investigated systematically. Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications. In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle. We use the most detailed framework to date for exp
    
[^95]: 机器学习中公平性的关键回顾：超越准确性在移动和可穿戴计算中的应用

    Beyond Accuracy: A Critical Review of Fairness in Machine Learning for Mobile and Wearable Computing. (arXiv:2303.15585v1 [cs.CY])

    [http://arxiv.org/abs/2303.15585](http://arxiv.org/abs/2303.15585)

    本文通过对IMWUT期刊上过去五年发表的论文进行系统回顾，发现UbiComp社区在算法公平方面的进展滞后，存在敏感属性偏差导致的歧视性结果，需要探索报告数据集的信息以解决这些偏差。

    

    移动、可穿戴和普及计算领域正在经历着机器学习的革命性整合。设备现在可以诊断疾病、预测心脏不规则动，发掘人类认知的全部潜力。然而，相关算法在敏感属性（如性别、种族等）方面可能存在偏差，导致歧视性结果。近期，人机交互（HCI）和人工智能伦理学（AI-Ethics）研究社区开始探索报告数据集的信息以揭示并最终对抗这些偏差。本文旨在探讨在这些报告方面UbiComp社区所采纳的程度，并强调潜在不足之处。通过对过去五年（2018-2022）在ACM交互、移动、可穿戴和普适技术（IMWUT）期刊上发表的论文进行系统回顾，我们发现UbiComp社区在算法公平方面的进展滞后。

    The field of mobile, wearable, and ubiquitous computing (UbiComp) is undergoing a revolutionary integration of machine learning. Devices can now diagnose diseases, predict heart irregularities, and unlock the full potential of human cognition. However, the underlying algorithms are not immune to biases with respect to sensitive attributes (e.g., gender, race), leading to discriminatory outcomes. The research communities of HCI and AI-Ethics have recently started to explore ways of reporting information about datasets to surface and, eventually, counter those biases. The goal of this work is to explore the extent to which the UbiComp community has adopted such ways of reporting and highlight potential shortcomings. Through a systematic review of papers published in the Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal over the past 5 years (2018-2022), we found that progress on algorithmic fairness within the UbiComp community lags behind. 
    
[^96]: 统计学习中的调整Wasserstein分布鲁棒估计

    Adjusted Wasserstein Distributionally Robust Estimator in Statistical Learning. (arXiv:2303.15579v1 [stat.ML])

    [http://arxiv.org/abs/2303.15579](http://arxiv.org/abs/2303.15579)

    本文提出了一种统计学习中的调整Wasserstein分布鲁棒估计方法，能够提高估计的统计性能，保持样本外性能保证，特别适用于广义线性模型。

    

    我们在统计学习中提出了一种调整的Wasserstein分布鲁棒估计——基于Wasserstein分布鲁棒估计（WDRO）的非线性转换。这种转换将提高WDRO的统计性能，因为调整后的WDRO估计器渐进无偏并且均方误差趋近于零。调整后的WDRO不会削弱WDRO的样本外性能保证。我们提出了调整WDRO估计器的存在的充分条件，并给出了计算调整WDRO估计器的过程。具体而言，我们将展示如何在广义线性模型中开发调整WDRO估计器。数值实验表明，调整后的估计器比经典估计器具有更好的实际性能。

    We propose an adjusted Wasserstein distributionally robust estimator -- based on a nonlinear transformation of the Wasserstein distributionally robust (WDRO) estimator in statistical learning. This transformation will improve the statistical performance of WDRO because the adjusted WDRO estimator is asymptotically unbiased and has an asymptotically smaller mean squared error. The adjusted WDRO will not mitigate the out-of-sample performance guarantee of WDRO. Sufficient conditions for the existence of the adjusted WDRO estimator are presented, and the procedure for the computation of the adjusted WDRO estimator is given. Specifically, we will show how the adjusted WDRO estimator is developed in the generalized linear model. Numerical experiments demonstrate the favorable practical performance of the adjusted estimator over the classic one.
    
[^97]: 利用人工神经网络在线非破坏性干燥过程中估算过滤介质的水分含量

    Online Non-Destructive Moisture Content Estimation of Filter Media During Drying Using Artificial Neural Networks. (arXiv:2303.15570v1 [cs.LG])

    [http://arxiv.org/abs/2303.15570](http://arxiv.org/abs/2303.15570)

    本论文介绍了一种基于人工神经网络的方法，可以在工业干燥过程中以非破坏性和在线方式估算大量滤介质产品的湿度。

    

    湿度测量是干燥过滤介质产品制造过程中优化干燥的先决条件。本文介绍了通过161个干燥实验收集的数据集，并提出了一种在工业干燥过程中以非破坏性和在线方式估算湿度的方法。与文献报道的先进湿度估算方法相比，基于人工神经网络（ANN）的方法得到了比较。模型拟合和培训的结果表明，三层感知器可达到最低误差。实验结果表明，结合烤箱设置数据，干燥时间和产品温度，ANNs可用于可靠地估算大量滤介质产品的湿度。

    Moisture content (MC) estimation is important in the manufacturing process of drying bulky filter media products as it is the prerequisite for drying optimization. In this study, a dataset collected by performing 161 drying industrial experiments is described and a methodology for MC estimation in an non-destructive and online manner during industrial drying is presented. An artificial neural network (ANN) based method is compared to state-of-the-art MC estimation methods reported in the literature. Results of model fitting and training show that a three-layer Perceptron achieves the lowest error. Experimental results show that ANNs combined with oven settings data, drying time and product temperature can be used to reliably estimate the MC of bulky filter media products.
    
[^98]: 基于核心-外围原则的变形金刚自注意力重构

    Core-Periphery Principle Guided Redesign of Self-Attention in Transformers. (arXiv:2303.15569v1 [cs.LG])

    [http://arxiv.org/abs/2303.15569](http://arxiv.org/abs/2303.15569)

    本文提出了一种新的框架CP-ViT，它利用核心-外围原则重构了变形金刚的自我注意力机制，在图像分类、目标检测和语义分割三个任务上取得了显著性能提升。

    

    设计更高效、可靠和可解释的神经网络架构对于基于人工智能技术的研究至关重要。之前的研究通过事后分析发现，表现最佳的人工神经网络惊人地类似于生物神经网络，这表明人工神经网络和生物神经网络可能共享某些通用原则，以在机器学习或认知/行为任务中实现最佳性能。基于这一现象，我们积极注入生物神经网络的组织原则来指导人工神经网络的重构。我们利用在人脑网络中广泛存在的核心-外围（CP）结构来引导视觉变压器（ViT）中自我注意的信息传递机制，并将这个新颖的框架命名为CP-ViT。在CP-ViT中，节点之间的注意力操作由一个具有核心-外围结构（CP图）的稀疏图定义，其中核心节点被重新设计和重新组织以在信息处理中发挥综合和核心的作用。我们在三个视觉任务，包括图像分类、目标检测和语义分割方面验证了CP-ViT的有效性，并表明与原始ViT模型相比，在模型效率、精度和可解释性方面，CP-ViT都取得了显著的改进。

    Designing more efficient, reliable, and explainable neural network architectures is critical to studies that are based on artificial intelligence (AI) techniques. Previous studies, by post-hoc analysis, have found that the best-performing ANNs surprisingly resemble biological neural networks (BNN), which indicates that ANNs and BNNs may share some common principles to achieve optimal performance in either machine learning or cognitive/behavior tasks. Inspired by this phenomenon, we proactively instill organizational principles of BNNs to guide the redesign of ANNs. We leverage the Core-Periphery (CP) organization, which is widely found in human brain networks, to guide the information communication mechanism in the self-attention of vision transformer (ViT) and name this novel framework as CP-ViT. In CP-ViT, the attention operation between nodes is defined by a sparse graph with a Core-Periphery structure (CP graph), where the core nodes are redesigned and reorganized to play an integr
    
[^99]: 掩码还原技术：利用掩码自编码器在测试时防御盲目后门攻击

    Mask and Restore: Blind Backdoor Defense at Test Time with Masked Autoencoder. (arXiv:2303.15564v1 [cs.LG])

    [http://arxiv.org/abs/2303.15564](http://arxiv.org/abs/2303.15564)

    本文提出了利用掩码自编码器的盲目防御框架（BDMAE），可以在测试时防御盲目后门攻击，不需要验证数据和模型参数，通过测试图像和 MAE 还原之间的结构相似性和标签一致性来检测后门攻击。

    

    深度神经网络容易受到恶意攻击，攻击者会通过在图像上叠加特殊的触发器来恶意操纵模型行为，这称为后门攻击。现有的后门防御方法通常需要访问一些验证数据和模型参数，这在许多实际应用中是不切实际的，例如当模型作为云服务提供时。为了解决这个问题，本文致力于测试时的盲目后门防御实践，特别是针对黑盒模型。每个测试图像的真实标签需要从可疑模型的硬标签预测中恢复。然而，在图像空间中启发式触发器搜索不适用于复杂触发器或高分辨率的图片。我们通过利用通用图像生成模型，提出了一种利用掩码自编码器的盲目防御框架（BDMAE），通过测试图像和 MAE 还原之间的结构相似性和标签一致性来检测后门攻击。

    Deep neural networks are vulnerable to backdoor attacks, where an adversary maliciously manipulates the model behavior through overlaying images with special triggers. Existing backdoor defense methods often require accessing a few validation data and model parameters, which are impractical in many real-world applications, e.g., when the model is provided as a cloud service. In this paper, we address the practical task of blind backdoor defense at test time, in particular for black-box models. The true label of every test image needs to be recovered on the fly from the hard label predictions of a suspicious model. The heuristic trigger search in image space, however, is not scalable to complex triggers or high image resolution. We circumvent such barrier by leveraging generic image generation models, and propose a framework of Blind Defense with Masked AutoEncoder (BDMAE). It uses the image structural similarity and label consistency between the test image and MAE restorations to detec
    
[^100]: 面向医疗保健的隐私保护机器学习：开放挑战和未来展望

    Privacy-preserving machine learning for healthcare: open challenges and future perspectives. (arXiv:2303.15563v1 [cs.LG])

    [http://arxiv.org/abs/2303.15563](http://arxiv.org/abs/2303.15563)

    本文对医疗保健领域的隐私保护机器学习（PPML）进行了综述，主要关注隐私保护训练和推理作为服务，识别挑战并讨论未来研究机会，以期在真实环境中开发私密高效的ML模型。

    

    机器学习（ML）近来在医疗保健领域展现了惊人的成功，涉及疾病诊断、预后和患者治疗等各种预测任务。由于医疗数据的敏感性，隐私必须在整个ML流程中得到考虑，包括从模型训练到推理。本文对医疗保健领域的隐私保护机器学习（PPML）进行了综述。我们主要关注隐私保护训练和推理作为服务，并对现有趋势进行全面回顾，识别挑战，并讨论未来研究方向的机会。本综述的目的是引导在医疗保健领域开发私密高效的ML模型的发展，以期将研究成果转化为实际应用。

    Machine Learning (ML) has recently shown tremendous success in modeling various healthcare prediction tasks, ranging from disease diagnosis and prognosis to patient treatment. Due to the sensitive nature of medical data, privacy must be considered along the entire ML pipeline, from model training to inference. In this paper, we conduct a review of recent literature concerning Privacy-Preserving Machine Learning (PPML) for healthcare. We primarily focus on privacy-preserving training and inference-as-a-service, and perform a comprehensive review of existing trends, identify challenges, and discuss opportunities for future research directions. The aim of this review is to guide the development of private and efficient ML models in healthcare, with the prospects of translating research efforts into real-world settings.
    
[^101]: 无线干扰网络中的多流传输：基于收敛图学习方法的研究

    Multi-Flow Transmission in Wireless Interference Networks: A Convergent Graph Learning Approach. (arXiv:2303.15544v1 [cs.LG])

    [http://arxiv.org/abs/2303.15544](http://arxiv.org/abs/2303.15544)

    本研究提出DIAMOND算法用于无线干扰网络中的多流传输，该算法基于收敛图学习方法，既可以集中式计算多流传输策略，也可以分布式实现数据包的传输，相比现有方法提高了15-20％的网络性能。

    

    本文针对无线网络中多流传输的问题进行研究，其中不同流间的数据信号可能由于它们路径上的链路间相互干扰而使链路容量减小。其目的是开发一种多流传输策略，通过路由在无线干扰网络中传输流以最大化网络效用。然而，由于涉及大量状态和操作空间，获得最优解的计算代价是昂贵的。为应对这个挑战，我们引入了一种新的算法Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND)。DIAMOND算法的设计允许采用混合式的集中式-分布式实现，这是5G及更高技术中具有中央单元部署特征的一种。集中式阶段使用一种新颖的图神经网络 (GNN) 强化学习 (RL) 路由代理计算多流传输策略。GNN-RL代理已经训练好了以学习网络状态并设计一个考虑干扰的路由策略。然后，这个策略被用于指导沿无干扰路径传输流，随后分布式阶段则利用这个策略来传输数据包。模拟结果表明DIAMOND算法在网络性能方面比现有方法表现更好，提高了15-20％。

    We consider the problem of of multi-flow transmission in wireless networks, where data signals from different flows can interfere with each other due to mutual interference between links along their routes, resulting in reduced link capacities. The objective is to develop a multi-flow transmission strategy that routes flows across the wireless interference network to maximize the network utility. However, obtaining an optimal solution is computationally expensive due to the large state and action spaces involved. To tackle this challenge, we introduce a novel algorithm called Dual-stage Interference-Aware Multi-flow Optimization of Network Data-signals (DIAMOND). The design of DIAMOND allows for a hybrid centralized-distributed implementation, which is a characteristic of 5G and beyond technologies with centralized unit deployments. A centralized stage computes the multi-flow transmission strategy using a novel design of graph neural network (GNN) reinforcement learning (RL) routing ag
    
[^102]: 对抗生成网络的顺序训练与GAN分类器揭示了独立训练的GAN实例之间存在的相关“知识盲区”

    Sequential training of GANs against GAN-classifiers reveals correlated "knowledge gaps" present among independently trained GAN instances. (arXiv:2303.15533v1 [cs.LG])

    [http://arxiv.org/abs/2303.15533](http://arxiv.org/abs/2303.15533)

    本文研究发现，顺序训练GAN对抗GAN分类器可以揭示独立训练的GAN实例之间存在的相关“知识盲区”，有机会改善GAN训练的稳定性和输出质量。

    

    现代生成对抗网络（GAN）可以生成非常逼真的图像。先前的工作已经证明，可以训练与协同训练的鉴别器不同，对从冻结GAN中生成的图像进行分类的“GAN分类器”。这样的分类器能够工作表明GAN训练中存在“知识盲区”（样本之间的分布外伪特征）。本文迭代地训练GAN分类器和训练GAN，以“欺骗”分类器（尝试填补“知识盲区”），并研究其对GAN训练动态、输出质量和GAN分类器泛化能力的影响。我们研究了两种情况，一个是小型的低维图像（MNIST）的DCGAN架构，另一个是StyleGAN2，一种在高维图像（FFHQ）上训练的SOTA GAN架构。我们发现DCGAN无法有效地欺骗一个分开的GAN分类器而不影响输出质量。然而，StyleGAN2可以欺骗分开的分类器而不影响输出质量，表明它已经学会了生成更多样化和鲁棒的特征。我们的实验揭示了顺序训练GAN对抗GAN分类器可以揭示独立训练的GAN实例之间存在的相关“知识盲区”，并有机会改善GAN训练的稳定性和输出质量。

    Modern Generative Adversarial Networks (GANs) generate realistic images remarkably well. Previous work has demonstrated the feasibility of "GAN-classifiers" that are distinct from the co-trained discriminator, and operate on images generated from a frozen GAN. That such classifiers work at all affirms the existence of "knowledge gaps" (out-of-distribution artifacts across samples) present in GAN training. We iteratively train GAN-classifiers and train GANs that "fool" the classifiers (in an attempt to fill the knowledge gaps), and examine the effect on GAN training dynamics, output quality, and GAN-classifier generalization. We investigate two settings, a small DCGAN architecture trained on low dimensional images (MNIST), and StyleGAN2, a SOTA GAN architecture trained on high dimensional images (FFHQ). We find that the DCGAN is unable to effectively fool a held-out GAN-classifier without compromising the output quality. However, StyleGAN2 can fool held-out classifiers with no change in
    
[^103]: 在黎曼流形上学习分子谐波表示

    Learning Harmonic Molecular Representations on Riemannian Manifold. (arXiv:2303.15520v1 [cs.LG])

    [http://arxiv.org/abs/2303.15520](http://arxiv.org/abs/2303.15520)

    本文提出了一种基于黎曼流形的分子谐波表示学习框架，使用分子表面的拉普拉斯-贝尔特拉米特征函数来表示分子，实现了分子几何和化学特征的多分辨率表示。

    

    分子表示学习在人工智能辅助药物发现研究中起着至关重要的作用。通过欧几里得神经网络对三维分子结构进行编码已成为几何深度学习社区的主流方法。然而，欧几里得空间中的等变性约束和消息传递可能会限制网络的表达能力。在本文中，我们提出了一种谐波分子表示学习（HMR）框架，它使用分子表面的拉普拉斯-贝尔特拉米特征函数来表示分子。HMR在2D黎曼流形上提供了分子几何和化学特征的多分辨率表示。我们还引入了一种谐波消息传递方法，在表面流形上实现高效的谱消息传递以实现更好的分子编码。我们提出的方法显示出与当前模型在小分子属性预测方面具有可比性的预测能力，并且在配体结合中优于最先进的深度学习模型。

    Molecular representation learning plays a crucial role in AI-assisted drug discovery research. Encoding 3D molecular structures through Euclidean neural networks has become the prevailing method in the geometric deep learning community. However, the equivariance constraints and message passing in Euclidean space may limit the network expressive power. In this work, we propose a Harmonic Molecular Representation learning (HMR) framework, which represents a molecule using the Laplace-Beltrami eigenfunctions of its molecular surface. HMR offers a multi-resolution representation of molecular geometric and chemical features on 2D Riemannian manifold. We also introduce a harmonic message passing method to realize efficient spectral message passing over the surface manifold for better molecular encoding. Our proposed method shows comparable predictive power to current models in small molecule property prediction, and outperforms the state-of-the-art deep learning models for ligand-binding pro
    
[^104]: 一种用于智能按需公共交通预测公交车到达时间的新型神经网络方法

    A Novel Neural Network Approach for Predicting the Arrival Time of Buses for Smart On-Demand Public Transit. (arXiv:2303.15495v1 [cs.LG])

    [http://arxiv.org/abs/2303.15495](http://arxiv.org/abs/2303.15495)

    本文介绍了一种基于神经网络的数据驱动方法，可以跨所有公交线路集体预测公交车到达每个交通点的时间，解决公交运输中公交车到达时间不准确和可靠的问题。

    

    在城市的主要公共交通系统中，公交运输存在着问题，其中包括对于乘客公交车到达时间的估计更加准确和可靠。这可能导致延误和减少乘客人数，尤其是在依靠公共交通的城市中更加严重。公交车到达时间与时间表不匹配是一个普遍的问题，导致固定时刻表的延迟。根据本文在纽约市公交数据上进行的研究，公交车到达时间和实际计划时间之间存在平均约八分钟或491秒的延迟。本研究提出了一种基于人工智能的数据驱动方法，用于估计每个交通点（站）公交车的到达时间。我们的方法基于全连接神经网络，可以在大都市区域中跨所有公交线路集体预测到达时间。我们的神经网络数据驱动方法为估算公交车到达时间提供了一种新的方式。

    Among the major public transportation systems in cities, bus transit has its problems, including more accuracy and reliability when estimating the bus arrival time for riders. This can lead to delays and decreased ridership, especially in cities where public transportation is heavily relied upon. A common issue is that the arrival times of buses do not match the schedules, resulting in latency for fixed schedules. According to the study in this paper on New York City bus data, there is an average delay of around eight minutes or 491 seconds mismatch between the bus arrivals and the actual scheduled time. This research paper presents a novel AI-based data-driven approach for estimating the arrival times of buses at each transit point (station). Our approach is based on a fully connected neural network and can predict the arrival time collectively across all bus lines in large metropolitan areas. Our neural-net data-driven approach provides a new way to estimate the arrival time of the b
    
[^105]: 铁路网络延误演化：一种异构图神经网络方法

    Railway Network Delay Evolution: A Heterogeneous Graph Neural Network Approach. (arXiv:2303.15489v1 [cs.LG])

    [http://arxiv.org/abs/2303.15489](http://arxiv.org/abs/2303.15489)

    本论文提出了一种能够应用于不同类型节点的异构图神经网络模型，针对铁路网络上的列车延误演化进行研究。使用HetGNN和GraphSAGE的组合，提出了一种名为SAGE-Het的图形架构，可以基于不同的边捕捉列车、列车、站点以及站点之间的相互作用，从而优于传统模型。

    

    铁路运营涉及不同类型的实体（站点，列车等），现有的同质节点（即相同类型的节点）的图/网络模型无法捕捉实体之间的相互作用。本文旨在开发一种异构图神经网络（HetGNN）模型，该模型可以应用于不同类型的节点（即异构节点），以研究铁路网络上的列车延误演化。为此，提出了一种组合HetGNN模型和GraphSAGE同质GNN（HomoGNN）的图形架构，称为SAGE-Het，旨在基于不同的边捕捉列车，列车、站点以及站点之间的相互作用。与传统方法要求输入具有恒定的维度（例如在矩形或类似网格的数组中）或仅允许在图中使用同质节点相比，SAGE-Het允许灵活的输入和异构节点。收集中国北京广州线上两个站点的数据以验证HetGNN模型。实验证明，所提出的SAGE-Het模型在预测列车延误演化方面优于传统模型，例如长短时记忆（LSTM）和HomoGNN模型。

    Railway operations involve different types of entities (stations, trains, etc.), making the existing graph/network models with homogenous nodes (i.e., the same kind of nodes) incapable of capturing the interactions between the entities. This paper aims to develop a heterogeneous graph neural network (HetGNN) model, which can address different types of nodes (i.e., heterogeneous nodes), to investigate the train delay evolution on railway networks. To this end, a graph architecture combining the HetGNN model and the GraphSAGE homogeneous GNN (HomoGNN), called SAGE-Het, is proposed. The aim is to capture the interactions between trains, trains and stations, and stations and other stations on delay evolution based on different edges. In contrast to the traditional methods that require the inputs to have constant dimensions (e.g., in rectangular or grid-like arrays) or only allow homogeneous nodes in the graph, SAGE-Het allows for flexible inputs and heterogeneous nodes. The data from two s
    
[^106]: 论特征可分性在预测分布外误差中的重要性

    On the Importance of Feature Separability in Predicting Out-Of-Distribution Error. (arXiv:2303.15488v1 [cs.LG])

    [http://arxiv.org/abs/2303.15488](http://arxiv.org/abs/2303.15488)

    本文研究发现，特征可分性对于模型在分布移位下的测试准确度有着重要作用。作者提出了一种基于特征离散度的分数用于估计测试准确度并在实验证明了该方法的优越性。

    

    在没有基准标签的分布外数据的泛化性能估计实际上很有挑战性。虽然以前的方法强调分布差异与分布外精度之间的联系，但我们表明，大的域间差异并不一定导致低的测试准确度。本文从特征可分性的角度研究了这个问题，并提出了一种基于特征离散度的数据集级别的分数，以估计在分布移位下的测试准确度。我们的方法是受表征学习中特征良好属性的启示：高内类离散度和高内类紧致度。我们的分析表明，内类离散度与模型准确度强相关，而内类紧致度不反映分布外数据的泛化性能。大量实验证明了我们的方法在预测性能和计算效率方面的优越性。

    Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability, and propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.
    
[^107]: 知识增强的图神经网络

    Knowledge Enhanced Graph Neural Networks. (arXiv:2303.15487v1 [cs.AI])

    [http://arxiv.org/abs/2303.15487](http://arxiv.org/abs/2303.15487)

    KeGNN是一个神经符号框架，可以结合先前的知识来优化图数据上的节点分类和链接预测任务。

    

    图数据是无处不在的，并且具有各种应用，例如自然科学、社交网络或语义网。尽管富含信息，但图形通常噪声和不完整。因此，图补全任务，如节点分类或链接预测，已经受到关注。一方面，神经方法（如图神经网络）已经被证明是处理噪声图的稳健工具。另一方面，符号方法可以对图进行精确推理。我们提出了KeGNN，这是一个用于在图数据上学习的神经符号框架，结合了两种范例，并允许将先前的知识集成到图神经网络模型中。从本质上讲，KeGNN由一个图神经网络组成，其中基于目标将知识增强层堆叠在其上，以使针对先前知识的预测得到优化。我们将KeGNN与两个标准图神经网络：图卷积网络和图注意力网络一起实例化。实验结果表明，将先前的知识集成到图神经网络模型中可以提高节点分类和链接预测任务的准确性。

    Graph data is omnipresent and has a large variety of applications such as natural science, social networks or semantic web. Though rich in information, graphs are often noisy and incomplete. Therefore, graph completion tasks such as node classification or link prediction have gained attention. On the one hand, neural methods such as graph neural networks have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose KeGNN, a neuro-symbolic framework for learning on graph data that combines both paradigms and allows for the integration of prior knowledge into a graph neural network model. In essence, KeGNN consists of a graph neural network as a base on which knowledge enhancement layers are stacked with the objective of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two standard graph neural networks: Graph Convolutional Networks and Graph 
    
[^108]: 单模态训练-多模态预测：具有分层聚合的跨模态联邦学习。

    Unimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation. (arXiv:2303.15486v1 [cs.LG])

    [http://arxiv.org/abs/2303.15486](http://arxiv.org/abs/2303.15486)

    本文提出了一种单模态训练-多模态预测的框架，设计了一种新型模型HA-Fedformer，在多模态联邦学习的背景下，通过聚合多个客户端的知识实现多模态测试，以提高准确性。

    

    多模态学习在从多种模态中挖掘数据特征方面取得了巨大成功，显着提高了模型性能。同时，联邦学习解决了数据共享问题，实现了隐私保护的协同训练，提供了足够的宝贵数据。因此，它们的融合，即多模态联邦学习，具有巨大潜力。然而，局限性在于主流方法通常假设每个本地数据集记录了所有模态的样本。本文旨在通过在多模态联邦学习的背景下提出单模态训练-多模态预测（UTMP）框架来弥合这一差距。我们设计了HA-Fedformer，一种基于transformer的新型模型，为客户端提供单模态数据集的单模态训练，并通过聚合多个客户端的知识实现多模态测试，以提高准确性。其关键优点有两个。首先，为减轻数据非II的影响

    Multimodal learning has seen great success mining data features from multiple modalities with remarkable model performance improvement. Meanwhile, federated learning (FL) addresses the data sharing problem, enabling privacy-preserved collaborative training to provide sufficient precious data. Great potential, therefore, arises with the confluence of them, known as multimodal federated learning. However, limitation lies in the predominant approaches as they often assume that each local dataset records samples from all modalities. In this paper, we aim to bridge this gap by proposing an Unimodal Training - Multimodal Prediction (UTMP) framework under the context of multimodal federated learning. We design HA-Fedformer, a novel transformer-based model that empowers unimodal training with only a unimodal dataset at the client and multimodal testing by aggregating multiple clients' knowledge for better accuracy. The key advantages are twofold. Firstly, to alleviate the impact of data non-II
    
[^109]: TOFA：一次转移全能的神经架构搜索

    TOFA: Transfer-Once-for-All. (arXiv:2303.15485v1 [cs.LG])

    [http://arxiv.org/abs/2303.15485](http://arxiv.org/abs/2303.15485)

    TOFA使用权重共享来进行神经架构搜索，以优化超网以适应各种设备的各种部署情况。与现有方法不同，TOFA在小数据集上进行训练，计算训练成本与部署方案数量无关。TOFA使用统一的半监督训练方法来解决小数据集带来的挑战。

    

    权重共享神经架构搜索旨在为不同资源约束的许多设备优化可配置的神经网络模型（超网）以满足各种部署场景。现有方法使用进化搜索从在非常大的数据集上训练的超网中提取多个模型，然后对感兴趣的通常很小的真实数据集上提取的模型进行微调。因此，训练的计算成本随着不同模型部署方案的数量而线性增长。因此，我们提出了Transfer-Once-For-All（TOFA），用于在小数据集上进行超网风格的训练，在任意数量的边缘部署方案上具有恒定的计算训练成本。给定任务，TOFA获得定制的神经网络，优化任意数量的边缘部署方案的拓扑和权重。为了克服小数据带来的挑战，TOFA利用统一的半监督训练损失同时训练超网内的所有子网。

    Weight-sharing neural architecture search aims to optimize a configurable neural network model (supernet) for a variety of deployment scenarios across many devices with different resource constraints. Existing approaches use evolutionary search to extract a number of models from a supernet trained on a very large data set, and then fine-tune the extracted models on the typically small, real-world data set of interest. The computational cost of training thus grows linearly with the number of different model deployment scenarios. Hence, we propose Transfer-Once-For-All (TOFA) for supernet-style training on small data sets with constant computational training cost over any number of edge deployment scenarios. Given a task, TOFA obtains custom neural networks, both the topology and the weights, optimized for any number of edge deployment scenarios. To overcome the challenges arising from small data, TOFA utilizes a unified semi-supervised training loss to simultaneously train all subnets w
    
[^110]: 自身正则化隐式神经表示

    Regularize implicit neural representation by itself. (arXiv:2303.15484v1 [cs.LG])

    [http://arxiv.org/abs/2303.15484](http://arxiv.org/abs/2303.15484)

    本文提出了一种正则化器INRR，可以提高隐式神经表示INR的泛化能力，通过将信号的自相似性与拉普拉斯矩阵的平滑度完美集成。此外，研究了INRR的一系列共性，可用于其他模型的正则化。

    

    本文提出了一种名为隐式神经表示正则化器（INRR）的正则化器，以提高隐式神经表示（INR）的泛化能力。INR是一种完全连接的网络，可以表示信号的细节，并不受网格分辨率的限制。然而，尤其是在非均匀采样数据的情况下，其泛化能力需要提高。所提出的INRR基于所学得的狄利克雷能量（DE），以测量矩阵的行/列之间的相似性。将拉普拉斯矩阵的平滑性进一步集成到将DE参数化为微小INR中。INRR通过将信号的自相似性与拉普拉斯矩阵的平滑度完美集成，提高了INR在信号表示中的泛化能力。通过精心设计的数值实验，本文还揭示了一系列从INRR中得出的共性，包括收敛轨迹和多尺度相似性等动量方法。此外，所提出的方法也可用于其他模型的正则化。

    This paper proposes a regularizer called Implicit Neural Representation Regularizer (INRR) to improve the generalization ability of the Implicit Neural Representation (INR). The INR is a fully connected network that can represent signals with details not restricted by grid resolution. However, its generalization ability could be improved, especially with non-uniformly sampled data. The proposed INRR is based on learned Dirichlet Energy (DE) that measures similarities between rows/columns of the matrix. The smoothness of the Laplacian matrix is further integrated by parameterizing DE with a tiny INR. INRR improves the generalization of INR in signal representation by perfectly integrating the signal's self-similarity with the smoothness of the Laplacian matrix. Through well-designed numerical experiments, the paper also reveals a series of properties derived from INRR, including momentum methods like convergence trajectory and multi-scale similarity. Moreover, the proposed method could 
    
[^111]: 探究神经网络中剪枝方法的性能：关于“中奖券假说”的实证研究

    Exploring the Performance of Pruning Methods in Neural Networks: An Empirical Study of the Lottery Ticket Hypothesis. (arXiv:2303.15479v1 [cs.LG])

    [http://arxiv.org/abs/2303.15479](http://arxiv.org/abs/2303.15479)

    本文比较了L1非结构化剪枝、Fisher剪枝和随机剪枝在不同网络架构和剪枝场景下的性能，提出了一种新的用于有效计算Fisher剪枝的方法，称为批处理Fisher剪枝。

    

    本文探究了不同剪枝方法在“中奖券假说”背景下的性能。我们比较了L1非结构化剪枝、Fisher剪枝和随机剪枝在不同网络架构和剪枝场景下的性能。实验包括一次性和迭代剪枝的评估，对网络在剪枝过程中的权重移动的考察，比较不同宽度网络上剪枝方法的表现，以及在网络变得非常稀疏时方法的性能分析。此外，我们提出并评估了一种新的用于有效计算Fisher剪枝的方法，称为批处理Fisher剪枝。

    In this paper, we explore the performance of different pruning methods in the context of the lottery ticket hypothesis. We compare the performance of L1 unstructured pruning, Fisher pruning, and random pruning on different network architectures and pruning scenarios. The experiments include an evaluation of one-shot and iterative pruning, an examination of weight movement in the network during pruning, a comparison of the pruning methods on networks of varying widths, and an analysis of the performance of the methods when the network becomes very sparse. Additionally, we propose and evaluate a new method for efficient computation of Fisher pruning, known as batched Fisher pruning.
    
[^112]: 对称正定矩阵上的自适应黎曼度量

    Adaptive Riemannian Metrics on SPD Manifolds. (arXiv:2303.15477v1 [cs.LG])

    [http://arxiv.org/abs/2303.15477](http://arxiv.org/abs/2303.15477)

    本文提出了自适应黎曼度量来改进SPD神经网络的次优性能，实验结果表明该度量能使网络表现更好。

    

    由于其内在能够编码数据中的潜在结构相关性，对称正定（SPD）矩阵在机器学习中受到广泛关注。为了反映SPD流形的非欧几里得几何，已经提出了许多成功的黎曼度量。然而，现有的固定度量张量可能会导致SPD矩阵学习的次优性能，特别是对于SPD神经网络。为了解决这个限制，我们利用拉回的思想，提出了自适应SPD流形的黎曼度量。此外，我们还对我们的度量提出了全面的理论。三个数据集上的实验表明，配备了我们提出的度量的SPD网络可以展现出优越的性能。

    Symmetric Positive Definite (SPD) matrices have received wide attention in machine learning due to their intrinsic capacity of encoding underlying structural correlation in data. To reflect the non-Euclidean geometry of SPD manifolds, many successful Riemannian metrics have been proposed. However, existing fixed metric tensors might lead to sub-optimal performance for SPD matrices learning, especially for SPD neural networks. To remedy this limitation, we leverage the idea of pullback and propose adaptive Riemannian metrics for SPD manifolds. Moreover, we present comprehensive theories for our metrics. Experiments on three datasets demonstrate that equipped with the proposed metrics, SPD networks can exhibit superior performance.
    
[^113]: 一种用于精确高效的机器学习分子动力学的异构并行非冯·诺依曼结构系统

    A Heterogeneous Parallel Non-von Neumann Architecture System for Accurate and Efficient Machine Learning Molecular Dynamics. (arXiv:2303.15474v1 [cs.LG])

    [http://arxiv.org/abs/2303.15474](http://arxiv.org/abs/2303.15474)

    该论文提出了一个用于实现高精度高效率的机器学习分子动力学计算的专用系统。该系统采用异构并行结构，其中不依赖冯·诺依曼结构的ASIC用于评估计算开销最大的原子力。与基于GPU的现有技术相比，该系统具有更高的能源效率和更低的制造成本。

    

    本文提出了一个专用系统，用于实现高精度和高效率的机器学习（ML）分子动力学（MD）计算。该系统由异构并行的现场可编程门阵列（FPGA）和应用特定集成电路（ASIC）组成。具体而言，采用不依赖冯·诺依曼（NvN）体系结构（SilTerra 180 nm工艺）的去乘法神经网络（NN）来评估原子力，这是MD的计算开销最大的部分。MD的所有其他计算都使用FPGA（Xilinx XC7Z100）完成。结果显示，为了实现类似精度，基于低端制造工艺（180 nm）的NvN系统比基于显卡处理器（GPU）的最先进vN MLMD快1.6倍，并且能源效率高出10^2-10^3倍，表明所提出的NvN异构并行架构的优越性。

    This paper proposes a special-purpose system to achieve high-accuracy and high-efficiency machine learning (ML) molecular dynamics (MD) calculations. The system consists of field programmable gate array (FPGA) and application specific integrated circuit (ASIC) working in heterogeneous parallelization. To be specific, a multiplication-less neural network (NN) is deployed on the non-von Neumann (NvN)-based ASIC (SilTerra 180 nm process) to evaluate atomic forces, which is the most computationally expensive part of MD. All other calculations of MD are done using FPGA (Xilinx XC7Z100). It is shown that, to achieve similar-level accuracy, the proposed NvN-based system based on low-end fabrication technologies (180 nm) is 1.6x faster and 10^2-10^3x more energy efficiency than state-of-the-art vN based MLMD using graphics processing units (GPUs) based on much more advanced technologies (12 nm), indicating superiority of the proposed NvN-based heterogeneous parallel architecture.
    
[^114]: 精确可合并的摘要

    Exactly mergeable summaries. (arXiv:2303.15465v1 [cs.LG])

    [http://arxiv.org/abs/2303.15465](http://arxiv.org/abs/2303.15465)

    本文提出了一种新类型的摘要，将传统聚合的优点与通过复杂数据表示保留更多信息相结合，实现了精确合并，既能保持精度又能减少数据大小。

    

    在大数据集的分析中，聚合是减少数据大小（复杂性）的标准方法。数据分析程序提供了不同的聚合函数。传统聚合的问题在于往往会丢失太多信息，从而降低了结果的精度。我们提出了一种新类型的摘要，它将传统聚合的优点与通过复杂数据表示保留更多信息相结合。这种方法允许对复杂摘要进行精确合并，并保持原始数据的精度。我们的研究为数据分析中复杂聚合摘要的理论基础的开发做出了贡献。

    In the analysis of large/big data sets, aggregation (replacing values of a variable over a group by a single value) is a standard way of reducing the size (complexity) of the data. Data analysis programs provide different aggregation functions.  Recently some books dealing with the theoretical and algorithmic background of traditional aggregation functions were published. A problem with traditional aggregation is that often too much information is discarded thus reducing the precision of the obtained results. A much better, preserving more information, summarization of original data can be achieved by representing aggregated data using selected types of complex data.  In complex data analysis the measured values over a selected group $A$ are aggregated into a complex object $\Sigma(A)$ and not into a single value. Most of the aggregation functions theory does not apply directly. In our contribution, we present an attempt to start building a theoretical background of complex aggregation
    
[^115]: 深度学习中的数学挑战

    Mathematical Challenges in Deep Learning. (arXiv:2303.15464v1 [cs.LG])

    [http://arxiv.org/abs/2303.15464](http://arxiv.org/abs/2303.15464)

    本文总结了深度学习中涉及培训、推理、一般化边界和优化问题的一组数学挑战，为数学家、统计学家和理论计算机科学家提供了与深度学习领域交流的形式化工具。

    

    自从2012年的ImageNet挑战以来，深度模型已经主宰了人工智能领域。深度模型的大小从那时起一直在增加，这给在手机、个人电脑、自动驾驶车辆和无线基站等领域应用的这一领域带来了新的挑战。在这里，我们列出了一组问题，涵盖培训、推理、一般化边界和优化问题，并用一些形式化语言来与数学家、统计学家和理论计算机科学家交流这些挑战。这是对深度学习研究问题的主观看法，它有益于长期的技术发展。

    Deep models are dominating the artificial intelligence (AI) industry since the ImageNet challenge in 2012. The size of deep models is increasing ever since, which brings new challenges to this field with applications in cell phones, personal computers, autonomous cars, and wireless base stations. Here we list a set of problems, ranging from training, inference, generalization bound, and optimization with some formalism to communicate these challenges with mathematicians, statisticians, and theoretical computer scientists. This is a subjective view of the research questions in deep learning that benefits the tech industry in long run.
    
[^116]: 利用反馈进行具身视觉导航的稳健性

    Robustness of Utilizing Feedback in Embodied Visual Navigation. (arXiv:2303.15453v1 [cs.LG])

    [http://arxiv.org/abs/2303.15453](http://arxiv.org/abs/2303.15453)

    本文提出了一个代理训练框架，可以在目标-物体导航任务中主动请求帮助，并使用反馈指示目标物体在其视野中的位置。训练过程中包括有和没有反馈的混合剧集，使得代理更加稳健，即使在没有反馈的情况下也可以提高代理的性能。

    

    本文提出了一个框架，用于训练在目标-物体导航任务中主动请求帮助的代理，并使用反馈指示目标物体在其视野中的位置。为了使代理更具有稳健性，即在老师可能不总是可用的情况下，所提出的训练课程包括有和没有反馈的混合剧集。结果表明，这种方法提高了代理的性能，即使在没有反馈的情况下也是如此。

    This paper presents a framework for training an agent to actively request help in object-goal navigation tasks, with feedback indicating the location of the target object in its field of view. To make the agent more robust in scenarios where a teacher may not always be available, the proposed training curriculum includes a mix of episodes with and without feedback. The results show that this approach improves the agent's performance, even in the absence of feedback.
    
[^117]: 通过BACKslash实现BACK substitution的BACKpropagation线性代数公式

    BACKpropagation through BACK substitution with a BACKslash. (arXiv:2303.15449v1 [math.NA])

    [http://arxiv.org/abs/2303.15449](http://arxiv.org/abs/2303.15449)

    本文提出了一种通过在三角方程组上使用通用的“backslash”或高斯消元来计算梯度的反向传播的线性代数公式，可以实现优化和实现便利性。

    

    我们提出了一种通过在三角方程组上使用通用的“backslash”或高斯消元来计算梯度的反向传播的线性代数公式。通常，矩阵元素是算子。这篇论文有三个贡献：1.用左作用的算子理论和基于图形的方法取代传统的自动微分处理具有知识价值。2.可以将算子放置在矩阵中作为实现选项的软件中，如Julia。3.我们引入了一种新的符号，“transpose dot”操作符“$\{\}^{T_\bullet}$”，允许反转算子。我们展示了算子方法在适合的编程语言中（如Julia）的优雅性，并证明了这种抽象可以在代码中实现。我们的实施展示了通用线性代数如何实现传统特殊形式的操作，例如backpropagation，以矩阵操作的形式书写，这打开了优化和实施便利性的可能性。

    We present a linear algebra formulation of backpropagation which allows the calculation of gradients by using a generically written ``backslash'' or Gaussian elimination on triangular systems of equations. Generally the matrix elements are operators. This paper has three contributions:  1. It is of intellectual value to replace traditional treatments of automatic differentiation with a (left acting) operator theoretic, graph-based approach.  2. Operators can be readily placed in matrices in software in programming languages such as Ju lia as an implementation option.  3. We introduce a novel notation, ``transpose dot'' operator ``$\{\}^{T_\bullet}$'' that allows the reversal of operators.  We demonstrate the elegance of the operators approach in a suitable programming language consisting of generic linear algebra operators such as Julia \cite{bezanson2017julia}, and that it is possible to realize this abstraction in code. Our implementation shows how generic linear algebra can allow op
    
[^118]: 量化分类决策的确定性和不确定性测量

    Measuring Classification Decision Certainty and Doubt. (arXiv:2303.14568v1 [stat.ML])

    [http://arxiv.org/abs/2303.14568](http://arxiv.org/abs/2303.14568)

    该论文提出了一种名为“确定性”和“不确定性”的得分方法来量化分类决策中预测的质量和不确定性。

    

    确定性和不确定性的定量表征和估计在优化和决策过程中具有基础重要性。本文提出了直观的得分，称为“确定性”和“不确定性”，可在贝叶斯和频率主义框架下用于评估和比较（多）分类决策机器学习问题的预测质量和不确定性。

    Quantitative characterizations and estimations of uncertainty are of fundamental importance in optimization and decision-making processes. Herein, we propose intuitive scores, which we call \textit{certainty} and \textit{doubt}, that can be used in both a Bayesian and frequentist framework to assess and compare the quality and uncertainty of predictions in (multi-)classification decision machine learning problems.
    
[^119]: 半局域机器学习势的热通量

    Heat flux for semi-local machine-learning potentials. (arXiv:2303.14434v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2303.14434](http://arxiv.org/abs/2303.14434)

    本文中介绍了如何将Green-Kubo方法应用于半局域机器学习势，使用自动微分推导出适应的热通量公式，成功计算了二氧化锆的热导率。

    

    Green-Kubo (GK)方法是材料热传输模拟的严格框架，但它需要准确描述势能表面且收敛统计量。机器学习势可以达到第一性原理模拟的准确性，同时可以在一小部分成本内超越其模拟时间和长度尺度。在本文中，我们解释了如何将GK方法应用于最近的消息传递机器学习势类，该方法迭代地考虑初始交互截断以外的半局域交互。我们推导出一个适应的热通量公式，可以使用自动微分实现，而不会影响计算效率。该方法通过计算二氧化锆的热导率验证和验证。

    The Green-Kubo (GK) method is a rigorous framework for heat transport simulations in materials. However, it requires an accurate description of the potential-energy surface and carefully converged statistics. Machine-learning potentials can achieve the accuracy of first-principles simulations while allowing to reach well beyond their simulation time and length scales at a fraction of the cost. In this paper, we explain how to apply the GK approach to the recent class of message-passing machine-learning potentials, which iteratively consider semi-local interactions beyond the initial interaction cutoff. We derive an adapted heat flux formulation that can be implemented using automatic differentiation without compromising computational efficiency. The approach is demonstrated and validated by calculating the thermal conductivity of zirconium dioxide across temperatures.
    
[^120]: 使用示范加速强化学习与规划：一份综述

    Boosting Reinforcement Learning and Planning with Demonstrations: A Survey. (arXiv:2303.13489v1 [cs.LG])

    [http://arxiv.org/abs/2303.13489](http://arxiv.org/abs/2303.13489)

    强化学习中一种减少试错的方法是使用示范，本文综述了如何使用示范来促进学习决策模型的应用，并提供了基于ManiSkill机器人学习基准的示范生成和利用管道的实例。

    

    尽管强化学习最近取得了巨大的成功，但是这种试错式的学习方法在复杂环境下可能效率低下。与此相反，使用示范可以让智能体受益于专家的知识，而无需探索最佳行动。在本综述中，我们讨论了在顺序决策中使用示范的优点，以及学习为基础的决策制定范式（例如，强化学习和规划在学习的模型中如何应用示范），以及如何在各种情况下收集示范。此外，我们还举了一个实际的示范生成和利用管道的例子，并在最近提出的ManiSkill机器人学习基准中进行了说明。

    Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.
    
[^121]: 使用机器学习的动态风险评分提前预测心源性休克

    A dynamic risk score for early prediction of cardiogenic shock using machine learning. (arXiv:2303.12888v1 [cs.LG])

    [http://arxiv.org/abs/2303.12888](http://arxiv.org/abs/2303.12888)

    该研究基于深度学习开发了一个风险分层工具CShock，旨在针对急性失代偿性心力衰竭和/或心肌梗死患者预测心源性休克的发作。

    

    心肌梗死和心力衰竭是主要的心血管疾病，影响着美国数百万人的健康。发展心源性休克的患者中，发病率和死亡率最高。心源性休克的早期识别至关重要，及时实施治疗措施可以防止缺血、低血压以及由于心源性休克导致心输出量降低的有害循环。然而，由于心脏监护病房中海量数据的信息处理能力与缺乏有效的风险分层工具，对心源性休克的早期识别一直具有挑战性。我们基于深度学习开发了一个称为CShock的风险分层工具，用于预测入住心脏监护病房的急性失代偿性心力衰竭和/或心肌梗死患者的心源性休克发作。为了开发和验证CShock，我们使用由医师裁定的结果注释了心脏监护病房数据集。

    Myocardial infarction and heart failure are major cardiovascular diseases that affect millions of people in the US. The morbidity and mortality are highest among patients who develop cardiogenic shock. Early recognition of cardiogenic shock is critical. Prompt implementation of treatment measures can prevent the deleterious spiral of ischemia, low blood pressure, and reduced cardiac output due to cardiogenic shock. However, early identification of cardiogenic shock has been challenging due to human providers' inability to process the enormous amount of data in the cardiac intensive care unit (ICU) and lack of an effective risk stratification tool. We developed a deep learning-based risk stratification tool, called CShock, for patients admitted into the cardiac ICU with acute decompensated heart failure and/or myocardial infarction to predict onset of cardiogenic shock. To develop and validate CShock, we annotated cardiac ICU datasets with physician adjudicated outcomes. CShock achieved
    
[^122]: 三次迭代的$(1-d)$-WL测试可以区分$d$维点云的非等距变换. (arXiv:2303.12853v1 [cs.LG])

    Three iterations of $(1-d)$-WL test distinguish non isometric clouds of $d$-dimensional points. (arXiv:2303.12853v1 [cs.LG])

    [http://arxiv.org/abs/2303.12853](http://arxiv.org/abs/2303.12853)

    本文研究了WL测试在点云中的应用，结果发现三次迭代的$(d-1)$-WL测试可以区分$d$维欧几里得空间中的点云，且只需要一次迭代的$d$-WL测试就可以达到完整性。

    

    Weisfeiler-Lehman (WL)测试是一个检查图同构的基本迭代算法。它被观察到是几种图神经网络体系结构设计的基础，这些网络的能力和性能可以用这个测试的表示能力来理解。受最近机器学习应用于涉及三维物体的数据集的发展启发，我们研究了当WL测试对完整的距离图表示的欧几里得点云是“完整的”时，它何时能够识别出任意一个任意点云.我们的主要结果是，$(d-1)$-维WL测试可以区分$d$维欧几里得空间中的点云，任何$d\ge 2$都可以，而且只需要进行三次测试。我们的结果对于$d=2,3$是紧的。我们还观察到$d$维WL测试只需要进行一次迭代就可以达到完整性。

    The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for checking isomorphism of graphs. It has also been observed that it underlies the design of several graph neural network architectures, whose capabilities and performance can be understood in terms of the expressive power of this test. Motivated by recent developments in machine learning applications to datasets involving three-dimensional objects, we study when the WL test is {\em complete} for clouds of euclidean points represented by complete distance graphs, i.e., when it can distinguish, up to isometry, any arbitrary such cloud.  Our main result states that the $(d-1)$-dimensional WL test is complete for point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that only three iterations of the test suffice. Our result is tight for $d = 2, 3$. We also observe that the $d$-dimensional WL test only requires one iteration to achieve completeness.
    
[^123]: 大型视觉语言模型零样本推理中的校准方法研究

    Enabling Calibration In The Zero-Shot Inference of Large Vision-Language Models. (arXiv:2303.12748v1 [cs.CV])

    [http://arxiv.org/abs/2303.12748](http://arxiv.org/abs/2303.12748)

    本文研究了零样本推理中视觉语言模型的校准问题，发现CLIP存在误校准，并提出了一种修改版的温度缩放方法，可以适用于每个特定的CLIP模型。

    

    深度学习模型的校准对于保证其可靠性和安全使用是至关重要的，因此在监督分类模型中对其进行了广泛研究，提出了降低误校准的方法。然而，视觉语言模型在进行零样本推理时的校准尚未得到全面的研究，例如CLIP。本研究衡量了跨相关变量（如提示，数据集和架构）的校准情况，并发现CLIP的零样本推理存在误校准。此外，我们提出了一种修改版的温度缩放方法，与CLIP作为零样本推理模型的常见用例相一致，并展示出单个学习的温度值可以广泛适用于每个特定的CLIP模型（由选定的预训练数据集和架构定义），跨不同的推理数据集和提示选择。

    Calibration of deep learning models is crucial to their trustworthiness and safe usage, and as such, has been extensively studied in supervised classification models, with methods crafted to decrease miscalibration. However, there has yet to be a comprehensive study of the calibration of vision-language models that are used for zero-shot inference, like CLIP. We measure calibration across relevant variables like prompt, dataset, and architecture, and find that zero-shot inference with CLIP is miscalibrated. Furthermore, we propose a modified version of temperature scaling that is aligned with the common use cases of CLIP as a zero-shot inference model, and show that a single learned temperature generalizes for each specific CLIP model (defined by a chosen pre-training dataset and architecture) across inference dataset and prompt choice.
    
[^124]: 具有元梯度正则化的自监督元提示学习用于少样本泛化

    Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])

    [http://arxiv.org/abs/2303.12314](http://arxiv.org/abs/2303.12314)

    提出了一种自我监督元提示学习框架SUPMER，包括元梯度正则化，用于少样本泛化，通过锚定的元训练任务和基于课程的任务增强丰富了任务分布，解决了在少样本情况下良好初始化软提示和过拟合的问题。

    

    提示调整是一种参数有效的方法，它学习软提示并使冻结的语言模型执行特定的下游任务。尽管有效，但提示调整在少样本情况下一方面严重依赖于良好的软提示初始化。另一方面，它很容易导致过度拟合。现有的方法利用预训练或监督元学习来初始化软提示，但它们不能对未见下游任务进行数据有效的泛化。为了解决以上问题，本文提出了一种新的自我监督元提示学习框架，其中包括元梯度正则化，用于少样本泛化（SUPMER）。我们首先设计了一组自监督锚定的元训练任务，具有不同的任务格式，并通过基于课程的任务增强进一步丰富了任务分布。然后将一种新的元梯度正则化方法集成到元提示学习中。它元学习在少样本情况下如何转换原始梯度。

    Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
    
[^125]: 在不完全信息的市场中使用在线学习进行平衡定价

    Online Learning for Equilibrium Pricing in Markets under Incomplete Information. (arXiv:2303.11522v1 [cs.GT])

    [http://arxiv.org/abs/2303.11522](http://arxiv.org/abs/2303.11522)

    该论文研究了在不完全信息的市场中使用在线学习进行平衡定价的问题，提出了解决选择性谎言问题的新方法。

    

    市场平衡的研究是经济理论的核心，特别是在有效配置稀缺资源方面。然而，定价均衡的计算通常依赖于完整的个体属性信息，如供应商的成本函数等，这在实践中往往不可用。因此，我们考虑了在不完全信息的情况下解决定价均衡的问题。在这种情况下，市场经营者寻求通过从成本函数未知的竞争供应商购买所需数量来满足客户需求。在这种不完整信息的情况下，我们考虑了在线学习问题，即学习随时间变化的平衡价格，同时联合优化三个性能指标——未满足的需求、成本失误和付款失误——这是在定价均衡的情况下相关的。

    The study of market equilibria is central to economic theory, particularly in efficiently allocating scarce resources. However, the computation of equilibrium prices at which the supply of goods matches their demand typically relies on having access to complete information on private attributes of agents, e.g., suppliers' cost functions, which are often unavailable in practice. Motivated by this practical consideration, we consider the problem of setting equilibrium prices in the incomplete information setting wherein a market operator seeks to satisfy the customer demand for a commodity by purchasing the required amount from competing suppliers with privately known cost functions unknown to the market operator. In this incomplete information setting, we consider the online learning problem of learning equilibrium prices over time while jointly optimizing three performance metrics -- unmet demand, cost regret, and payment regret -- pertinent in the context of equilibrium pricing over a
    
[^126]: 可训练的投影梯度方法用于鲁棒微调

    Trainable Projected Gradient Method for Robust Fine-tuning. (arXiv:2303.10720v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.10720](http://arxiv.org/abs/2303.10720)

    该论文提出了可训练的投影梯度方法（TPGM），以自动学习针对每个层的施加约束以进行细粒度的微调正则化，同时具有计算效率和易用性，并在各种微调场景下实现了最先进的性能。

    

    近期关于迁移学习的研究表明，选择性地微调子集层或为每个层自定义不同的学习率可以极大地提高对于区分度数据的健壮性，并保留预先训练的模型的泛化能力。然而，大多数这些方法都采用手工制作的启发式方法或昂贵的超参数搜索，这防止了它们在大数据集和神经网络上扩展。为了解决这个问题，我们提出了可训练的投影梯度方法（TPGM），以自动学习针对每个层的施加约束以进行细粒度的微调正则化。这是通过将微调表述为双层约束优化问题来实现的。具体来说，TPGM维护每个图层的投影半径集，即微调模型与预训练模型之间的距离约束，并通过权重投影强制执行它们。为了学习约束，我们提出了一个双层优化来最大化训练数据上的经验风险，其受到给定投影半径集的约束。实验证明，我们的方法在各种微调场景下都可以实现最先进的性能，同时具有计算效率和易用性。

    Recent studies on transfer learning have shown that selectively fine-tuning a subset of layers or customizing different learning rates for each layer can greatly improve robustness to out-of-distribution (OOD) data and retain generalization capability in the pre-trained models. However, most of these methods employ manually crafted heuristics or expensive hyper-parameter searches, which prevent them from scaling up to large datasets and neural networks. To solve this problem, we propose Trainable Projected Gradient Method (TPGM) to automatically learn the constraint imposed for each layer for a fine-grained fine-tuning regularization. This is motivated by formulating fine-tuning as a bi-level constrained optimization problem. Specifically, TPGM maintains a set of projection radii, i.e., distance constraints between the fine-tuned model and the pre-trained model, for each layer, and enforces them through weight projections. To learn the constraints, we propose a bi-level optimization to
    
[^127]: CoGANPPIS: 基于共进化增强的全局关注神经网络用于蛋白质相互作用位点预测

    CoGANPPIS: Coevolution-enhanced Global Attention Neural Network for Protein-Protein Interaction Site Prediction. (arXiv:2303.06945v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2303.06945](http://arxiv.org/abs/2303.06945)

    本论文提出了一种基于共进化增强的全局关注神经网络，它是一种用于蛋白质相互作用位点预测的深度学习模型，结合了共进化特征和全局关注机制以更好地捕捉氨基酸残基之间的关系并考虑到所有残基的贡献。

    

    蛋白质相互作用在生化过程中起着重要作用。准确预测蛋白质相互作用位点（PPIs）可以加深我们对生物机理的理解，并对新药设计至关重要。然而，传统的PPI预测实验方法成本高昂，耗时长，因此近年来开发了许多计算方法，特别是基于机器学习的方法。尽管这些方法取得了令人满意的结果，但仍存在两个限制：（1）大多数模型挖掘了一些有用的输入特征，但未考虑到共进化特征，后者可以提供有关氨基酸残基之间的关系的线索；（2）attention-based模型仅为相邻残基分配关注权重，而不是全局分配，忽略了远离目标残基的一些残基可能也很重要。我们提出了一种共进化增强的全局关注神经网络，这是一种用于PPI位点预测的基于序列的深度学习模型。该模型结合了共进化特征和全局关注机制，以更好地捕捉氨基酸残基之间的关系，并考虑到蛋白序列中所有残基的贡献。

    Protein-protein interactions are essential in biochemical processes. Accurate prediction of the protein-protein interaction sites (PPIs) deepens our understanding of biological mechanism and is crucial for new drug design. However, conventional experimental methods for PPIs prediction are costly and time-consuming so that many computational approaches, especially ML-based methods, have been developed recently. Although these approaches have achieved gratifying results, there are still two limitations: (1) Most models have excavated some useful input features, but failed to take coevolutionary features into account, which could provide clues for inter-residue relationships; (2) The attention-based models only allocate attention weights for neighboring residues, instead of doing it globally, neglecting that some residues being far away from the target residues might also matter.  We propose a coevolution-enhanced global attention neural network, a sequence-based deep learning model for P
    
[^128]: 能量正则化RNN解决非平稳Bandit问题

    Energy Regularized RNNs for Solving Non-Stationary Bandit Problems. (arXiv:2303.06552v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.06552](http://arxiv.org/abs/2303.06552)

    本文提出了一种能量正则化的循环神经网络方法，解决了奖励非平稳的多臂赌博机问题。该方法平衡了探索和利用，通过能量最小化项限制了网络对支持某个操作的过度自信，有效性与同类方法相当。

    

    我们考虑一个多臂赌博机问题，其中奖励是非平稳的，并取决于过去的动作和可能的过去情境。在我们方法的核心，我们采用循环神经网络，对这些序列进行建模。为了平衡探索和利用之间的关系，我们提出了一个能量最小化项，以防止神经网络对支持某个操作过于自信。通过这个术语，网络所分配的最大和最小概率之间的差距受到合理限制。在一系列不同的实验中，我们证明了我们的方法至少与解决Rotting Bandits子问题的方法一样有效，并且可以解决各种基准问题的直观扩展性。我们在https://github.com/rotmanmi/Energy-Regularized-RNN上共享我们的实现。

    We consider a Multi-Armed Bandit problem in which the rewards are non-stationary and are dependent on past actions and potentially on past contexts. At the heart of our method, we employ a recurrent neural network, which models these sequences. In order to balance between exploration and exploitation, we present an energy minimization term that prevents the neural network from becoming too confident in support of a certain action. This term provably limits the gap between the maximal and minimal probabilities assigned by the network. In a diverse set of experiments, we demonstrate that our method is at least as effective as methods suggested to solve the sub-problem of Rotting Bandits, and can solve intuitive extensions of various benchmark problems. We share our implementation at https://github.com/rotmanmi/Energy-Regularized-RNN.
    
[^129]: HyT-NAS: 面向边缘设备的混合变压器神经架构搜索

    HyT-NAS: Hybrid Transformers Neural Architecture Search for Edge Devices. (arXiv:2303.04440v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.04440](http://arxiv.org/abs/2303.04440)

    HyT-NAS是一个高效的神经架构搜索算法，针对微型设备的混合架构，可以在少于5倍的训练评估次数下实现类似的超体积，并在Visu上的参数数量少3.5倍，精度提高了6.3%。

    

    视觉变压器使得近期基于注意力的深度学习架构在计算机视觉任务上取得了显著成果。但是，由于需要大量的计算资源，这些架构很少在资源受限的平台上实现。目前的研究探讨了混合手工卷积和基于注意力的模型用于计算机视觉任务，如图像分类和物体检测。本文提出了HyT-NAS，一种高效的硬件感知神经架构搜索，包括针对微型设备的混合架构。HyT-NAS通过丰富搜索空间、增强搜索策略和性能预测器来改进现有的HW-NAS。实验表明，HyT-NAS在少于5倍的训练评估次数下实现了类似的超体积。最终的架构在Visu上的参数数量少3.5倍，精度提高了6.3%，胜过了MLPerf MobileNetV1。

    Vision Transformers have enabled recent attention-based Deep Learning (DL) architectures to achieve remarkable results in Computer Vision (CV) tasks. However, due to the extensive computational resources required, these architectures are rarely implemented on resource-constrained platforms. Current research investigates hybrid handcrafted convolution-based and attention-based models for CV tasks such as image classification and object detection. In this paper, we propose HyT-NAS, an efficient Hardware-aware Neural Architecture Search (HW-NAS) including hybrid architectures targeting vision tasks on tiny devices. HyT-NAS improves state-of-the-art HW-NAS by enriching the search space and enhancing the search strategy as well as the performance predictors. Our experiments show that HyT-NAS achieves a similar hypervolume with less than ~5x training evaluations. Our resulting architecture outperforms MLPerf MobileNetV1 by 6.3% accuracy improvement with 3.5x less number of parameters on Visu
    
[^130]: EvoPrompting: 适用于代码级神经架构搜索的语言模型

    EvoPrompting: Language Models for Code-Level Neural Architecture Search. (arXiv:2302.14838v1 [cs.NE] CROSS LISTED)

    [http://arxiv.org/abs/2302.14838](http://arxiv.org/abs/2302.14838)

    EvoPrompting利用语言模型作为自适应变异和交叉操作符来进行神经架构搜索，在MNIST-1D数据集和CLRS算法推理基准上都取得了比人类设计的架构更好的性能表现。

    

    鉴于语言模型（LM）在代码生成方面的最新成就，我们探索将LM作为进化神经架构搜索（NAS）算法的自适应变异和交叉操作符的使用。尽管NAS仍然过于困难，以至于仅仅通过提示就难以成功，但我们发现进化提示工程与软提示调整的组合，一种我们称之为EvoPrompting的方法，始终可以发现多样化且性能高的模型。我们首先证明EvoPrompting在MNIST-1D数据集上是有效的，其中EvoPrompting产生的卷积架构变体在准确率和模型大小方面均优于人类专家设计的架构和天真的少数先导提示。然后，我们将我们的方法应用于在CLRS算法推理基准上搜索图神经网络，其中EvoPrompting能够设计出比当前最先进的模型更好的新颖结构。

    Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 ou
    
[^131]: 生成可逆量子神经网络

    Generative Invertible Quantum Neural Networks. (arXiv:2302.12906v2 [hep-ph] UPDATED)

    [http://arxiv.org/abs/2302.12906](http://arxiv.org/abs/2302.12906)

    本论文提出了一种用于生成可逆量子神经网络的算法，并将其应用于LHC数据的处理，结果表明该算法可以在学习和生成复杂数据方面与经典算法的表现相匹配。

    

    可逆神经网络已成为模拟和生成高度复杂数据的工具。我们提出了一种量子门算法用于量子可逆神经网络（QINN），并将其应用于将衰变为轻子的Z玻色子的喷注相关产生的LHC数据，这是粒子对撞机精密测量的标准过程。我们比较了QINN在不同损失函数和训练场景下的表现。对于这个任务，我们发现一个混合的QINN可以在学习和生成复杂数据方面与一个显著更大的完全经典的INN的表现匹配。

    Invertible Neural Networks (INN) have become established tools for the simulation and generation of highly complex data. We propose a quantum-gate algorithm for a Quantum Invertible Neural Network (QINN) and apply it to the LHC data of jet-associated production of a Z-boson that decays into leptons, a standard candle process for particle collider precision measurements. We compare the QINN's performance for different loss functions and training scenarios. For this task, we find that a hybrid QINN matches the performance of a significantly larger purely classical INN in learning and generating complex data.
    
[^132]: 在「学习优化」中学习一般化的保证（Learning to Generalize Provably in Learning to Optimize）

    Learning to Generalize Provably in Learning to Optimize. (arXiv:2302.11085v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11085](http://arxiv.org/abs/2302.11085)

    本文提出了一种统一的数据增强框架，用于学习到可以泛化地优化器和优化对象。该框架可以轻松地与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。

    

    「学习优化」（Learning to optimize，L2O）已经变得越来越流行，通过数据驱动的方法自动化设计优化器。然而，当前的 L2O 方法在至少两个方面表现不佳：（i）将 L2O 设计的优化器应用于未见过的优化对象时降低其损失函数值（优化器一般化或“可泛化的优化器学习”）；以及（ii）由优化器训练的优化对象（本身为机器学习模型）在准确性上面对未见过的数据表现的测试性能（优化对象一般化或“学习一般化”）。虽然优化器一般化最近已被研究，但优化对象一般化在 L2O 上尚未得到严格研究，这是本文的目的。我们首先理论上建立了局部熵与 Hessian 之间的隐式联系，从而统一了它们在通用优化技术的手工设计中的作用。基于这种联系，我们提出了一个统一的 “数据增强框架" 来学习 L2O 的一般化实现。我们的框架由两部分组成：一部分是 GO 模块，通过随机梯度下降方法最大化通用化目标学习一个通用化算法；另一部分是优化对象模块，通过可微分优化器和标准交叉熵损失函数学习机器学习模型。我们的框架可以轻松与现有的 L2O 方法相结合，并在优化器和优化对象的一般化性能方面优于现有的最优方法。

    Learning to optimize (L2O) has gained increasing popularity, which automates the design of optimizers by data-driven approaches. However, current L2O methods often suffer from poor generalization performance in at least two folds: (i) applying the L2O-learned optimizer to unseen optimizees, in terms of lowering their loss function values (optimizer generalization, or ``generalizable learning of optimizers"); and (ii) the test performance of an optimizee (itself as a machine learning model), trained by the optimizer, in terms of the accuracy over unseen data (optimizee generalization, or ``learning to generalize"). While the optimizer generalization has been recently studied, the optimizee generalization (or learning to generalize) has not been rigorously studied in the L2O context, which is the aim of this paper. We first theoretically establish an implicit connection between the local entropy and the Hessian, and hence unify their roles in the handcrafted design of generalizable optim
    
[^133]: 强化学习在宏观布局中的评估

    Assessment of Reinforcement Learning for Macro Placement. (arXiv:2302.11014v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11014](http://arxiv.org/abs/2302.11014)

    本论文提供了基于强化学习的宏观布局方法以及Circuit Training (CT)实现的开源代码和评估。研究人员评估了CT相对于多个可替代的宏观布局方法，并进行了学术性混合尺寸布局基准测试和消融和稳定性研究，为未来的相关研究提供了方向。

    

    我们提供了Google Brain深度强化学习方法在宏观布局及其Circuit Training (CT)实现的开放透明实现和评估，并在GitHub中实现了CT的关键"黑盒"元素，澄清了CT与Nature论文之间的差异。我们开发并发布了新的对开放实现的测试用例。我们评估了CT及多个可替代的宏观布局方法，所有的评估流程和相关脚本都在GitHub上公开。我们的实验还包括了学术性混合尺寸布局基准测试，以及消融和稳定性研究。我们评论了Nature和CT的影响，以及未来研究的方向。

    We provide open, transparent implementation and assessment of Google Brain's deep reinforcement learning approach to macro placement and its Circuit Training (CT) implementation in GitHub. We implement in open source key "blackbox" elements of CT, and clarify discrepancies between CT and Nature paper. New testcases on open enablements are developed and released. We assess CT alongside multiple alternative macro placers, with all evaluation flows and related scripts public in GitHub. Our experiments also encompass academic mixed-size placement benchmarks, as well as ablation and stability studies. We comment on the impact of Nature and CT, as well as directions for future research.
    
[^134]: 关于深度神经网络功能耦合水印的研究

    On Function-Coupled Watermarks for Deep Neural Networks. (arXiv:2302.10296v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2302.10296](http://arxiv.org/abs/2302.10296)

    本文提出了一种新颖的深度神经网络水印方案，它可以有效地防御模型微调和修剪等水印删除攻击；通过增强水印和模型功能的耦合，我们的方法可以确保删除水印不可避免地会降低模型在常规输入上的性能。

    

    良好表现的深度神经网络通常需要海量标记数据和计算资源进行训练。为了保护这些知识产权，提出了各种水印技术，其中DNN提供商将秘密信息植入模型中，以便在稍后通过一些专用触发输入检索嵌入的水印索权；虽然文献中报告了有希望的结果，但现有解决方案仍然遭受水印删除攻击，例如模型微调和模型修剪。本文提出了一种新颖的DNN水印方案，可以有效地防御上述攻击。我们的关键洞察力是增强水印和模型功能的耦合，这样删除水印会不可避免地降低模型在常规输入上的性能。为此，与先前依赖于来自超出分布数据的秘密特征的方法不同，我们的方法仅使用从训练数据中学习的特征。

    Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features lear
    
[^135]: JANA：复杂贝叶斯模型的联合分摊近似神经网络

    JANA: Jointly Amortized Neural Approximation of Complex Bayesian Models. (arXiv:2302.09125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09125](http://arxiv.org/abs/2302.09125)

    本文提出了 JANA 方法，用于处理复杂贝叶斯模型的近似计算。通过端到端训练三个神经网络来实现分摊的近似后验和似然，为贝叶斯工作流程提供了一种新的途径。此方法在多种模拟模型中进行了基准测试，并提出了一种联合校准诊断方法。

    

    本文提出了“联合分摊神经网络近似”（JANA）方法，用于处理贝叶斯代理建模和基于模拟的推理中出现的难以计算的似然函数和后验密度。我们以端到端的方式训练三个相互补充的神经网络：1）一个总结网络，将个别数据点、集合或时间序列压缩成信息嵌入向量；2）一个后验网络，学习分摊的近似后验；3）一个似然网络，学习分摊的近似似然。它们的交互为分摊边缘似然和后验预测估计提供了新的途径，这是贝叶斯工作流程的两个重要组成部分，常常对于标准方法来说太昂贵了。我们在各种模拟模型中对JANA的保真度进行了基准测试，与最先进的贝叶斯方法进行了比较，并提出了一种强大而可解释的联合校准诊断方法。此外，我们研究了循环似然网络模拟复杂模型的能力。

    This work proposes ''jointly amortized neural approximation'' (JANA) of intractable likelihood functions and posterior densities arising in Bayesian surrogate modeling and simulation-based inference. We train three complementary networks in an end-to-end fashion: 1) a summary network to compress individual data points, sets, or time series into informative embedding vectors; 2) a posterior network to learn an amortized approximate posterior; and 3) a likelihood network to learn an amortized approximate likelihood. Their interaction opens a new route to amortized marginal likelihood and posterior predictive estimation -- two important ingredients of Bayesian workflows that are often too expensive for standard methods. We benchmark the fidelity of JANA on a variety of simulation models against state-of-the-art Bayesian methods and propose a powerful and interpretable diagnostic for joint calibration. In addition, we investigate the ability of recurrent likelihood networks to emulate comp
    
[^136]: 可缝合神经网络

    Stitchable Neural Networks. (arXiv:2302.06586v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.06586](http://arxiv.org/abs/2302.06586)

    本文提出了一个名为SN-Net的框架，它可以便宜地产生许多不同复杂度和性能权衡的网络，利用预先训练的神经网络家族作为锚点，并使用简单的缝合层将它们拼接在一起以实现动态的精度-效率权衡。

    

    具有巨大威力的预训练模型集群(如ResNet/DeiT)所构成的公共模型库已经达到了前所未有的范围，这在很大程度上促进了深度学习的成功。然而，每个模型系列都包含着不同规模的预训练模型(比如DeiT-Ti/S/B)，这自然地引出了一个基本问题：如何在运行时有效地组合这些可用的模型系列以实现动态的精度-效率权衡。针对这个问题，我们提出了Stitchable Neural Networks (SN-Net)，这是一个新颖的可扩展、高效的模型部署框架。在一个预先训练的神经网络家族中，它可以便宜地产生许多不同复杂度和性能权衡的网络，我们称之为锚点。具体来说，SN-Net将锚点分散在块/层之间，然后使用简单的缝合层将它们拼接在一起，以映射一个锚点的激活到另一个锚点。仅仅通过几个轮次的训练，SN-Net可以有效地插值网络。

    The public model zoo containing enormous powerful pretrained model families (e.g., ResNet/DeiT) has reached an unprecedented scope than ever, which significantly contributes to the success of deep learning. As each model family consists of pretrained models with diverse scales (e.g., DeiT-Ti/S/B), it naturally arises a fundamental question of how to efficiently assemble these readily available models in a family for dynamic accuracy-efficiency trade-offs at runtime. To this end, we present Stitchable Neural Networks (SN-Net), a novel scalable and efficient framework for model deployment. It cheaply produces numerous networks with different complexity and performance trade-offs given a family of pretrained neural networks, which we call anchors. Specifically, SN-Net splits the anchors across the blocks/layers and then stitches them together with simple stitching layers to map the activations from one anchor to another. With only a few epochs of training, SN-Net effectively interpolates 
    
[^137]: 一种针对用户隐私的移动游戏应用安装预测模型研究

    Towards a User Privacy-Aware Mobile Gaming App Installation Prediction Model. (arXiv:2302.03332v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03332](http://arxiv.org/abs/2302.03332)

    本文研究了一种针对用户隐私的移动游戏应用安装预测模型，探讨了隐私保护和模型性能之间的平衡。

    

    过去十年中，程序化广告在在线广告行业中备受关注。实时竞价系统正在迅速成为购买和出售在线广告展现量的最流行方法。在实时竞价系统中，需求方平台旨在有效地支配广告商的推广预算，同时最大化利润，寻求产生高用户响应（如点击或安装）的展示量。在当前研究中，我们从特定需求方平台的角度研究了预测移动游戏应用安装的过程，同时注意保护用户隐私并探讨隐私保护和模型性能之间的权衡。可能存在多个级别的潜在用户隐私威胁，这取决于与数据共享流程相关的隐私泄漏，例如数据转换或去匿名化。为应对这些问题，提出了隐私保护技术，例如密码学。

    Over the past decade, programmatic advertising has received a great deal of attention in the online advertising industry. A real-time bidding (RTB) system is rapidly becoming the most popular method to buy and sell online advertising impressions. Within the RTB system, demand-side platforms (DSP) aim to spend advertisers' campaign budgets efficiently while maximizing profit, seeking impressions that result in high user responses, such as clicks or installs. In the current study, we investigate the process of predicting a mobile gaming app installation from the point of view of a particular DSP, while paying attention to user privacy, and exploring the trade-off between privacy preservation and model performance. There are multiple levels of potential threats to user privacy, depending on the privacy leaks associated with the data-sharing process, such as data transformation or de-anonymization. To address these concerns, privacy-preserving techniques were proposed, such as cryptographi
    
[^138]: 基于神经算子的自由形电磁反演设计代理求解器

    A neural operator-based surrogate solver for free-form electromagnetic inverse design. (arXiv:2302.01934v2 [physics.comp-ph] UPDATED)

    [http://arxiv.org/abs/2302.01934](http://arxiv.org/abs/2302.01934)

    本文基于神经算子实现了一个代理解算器，可以应用在自由形电磁反演设计中，取得了比现有方法更高的数据效率。

    

    在科学机器学习的背景下，神经算子已成为解决偏微分方程的强大工具。在这里，我们实现和训练了一个改进的傅里叶神经算子作为电磁散射问题的代理解算器，并将其数据效率与现有方法进行了比较。我们进一步展示了其在基于梯度的自由形全三维电磁散射体的纳米光子学反演设计中的应用，这是一个到目前为止尚未应用深度学习技术的领域。

    Neural operators have emerged as a powerful tool for solving partial differential equations in the context of scientific machine learning. Here, we implement and train a modified Fourier neural operator as a surrogate solver for electromagnetic scattering problems and compare its data efficiency to existing methods. We further demonstrate its application to the gradient-based nanophotonic inverse design of free-form, fully three-dimensional electromagnetic scatterers, an area that has so far eluded the application of deep learning techniques.
    
[^139]: 通过代理建模实现高效的激活函数优化

    Efficient Activation Function Optimization through Surrogate Modeling. (arXiv:2301.05785v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.05785](http://arxiv.org/abs/2301.05785)

    本文提出了一种基于代理建模的方法，通过扩展的基准测试空间在较少的函数评估次数中发现了优化的高效激活函数架构，并在多个标准基准测试中实现了最先进的性能。

    

    精心设计的激活函数可以提高神经网络在许多机器学习任务中的性能。然而，人类很难构建最优激活函数，而当前的激活函数搜索算法过于昂贵。本文通过三个步骤旨在改进现有技术：首先，通过使用2,913个系统生成的激活函数从头训练卷积、残差和视觉变换器架构来创建 Act-Bench-CNN、Act-Bench-ResNet 和 Act-Bench-ViT 基准数据集。第二，开发了基于代理的方法用于优化基准空间，发现与模型预测分布和激活函数输出分布相关联的 Fisher 信息矩阵的频谱对性能的预测性很高。第三，使用代理在较少的函数评估次数中发现了改进的激活函数架构，同时在几个标准基准测试中实现了最先进的性能。

    Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks. However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive. This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activ
    
[^140]: 从餐盘到预防：新加坡健康促进的膳食营养辅助平台

    From Plate to Prevention: A Dietary Nutrient-aided Platform for Health Promotion in Singapore. (arXiv:2301.03829v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03829](http://arxiv.org/abs/2301.03829)

    本文介绍了一个膳食营养辅助平台，该平台开发了一个本地化的新加坡食品数据集，旨在通过监管和监督人们的营养摄入，为新加坡人的健康促进提供医学级别的营养信息。

    

    新加坡一直致力于改善人民的医疗保健服务。政府注意到了监管和监督人们摄入营养的不足，这被认为是慢性疾病发展的一个因素。因此，这个问题引起了很大的关注。本文分享了新加坡解决这个问题和获取医疗级别营养摄入信息以在不同方面造福新加坡人的经验。为此，我们开发了FoodSG平台来孵化多样化的面向医疗保健的应用服务在新加坡，同时考虑到它们的共同要求。我们进一步确定了本地化食品数据集的深远意义，并系统地清理和筛选了一个本地化的新加坡食品数据集FoodSG-233。为了克服由新加坡多样化食品菜肴带来的识别性能障碍，我们提议整合监督式对比学习。

    Singapore has been striving to improve the provision of healthcare services to her people. In this course, the government has taken note of the deficiency in regulating and supervising people's nutrient intake, which is identified as a contributing factor to the development of chronic diseases. Consequently, this issue has garnered significant attention. In this paper, we share our experience in addressing this issue and attaining medical-grade nutrient intake information to benefit Singaporeans in different aspects. To this end, we develop the FoodSG platform to incubate diverse healthcare-oriented applications as a service in Singapore, taking into account their shared requirements. We further identify the profound meaning of localized food datasets and systematically clean and curate a localized Singaporean food dataset FoodSG-233. To overcome the hurdle in recognition performance brought by Singaporean multifarious food dishes, we propose to integrate supervised contrastive learnin
    
[^141]: 梯度过滤技术实现高效的设备端训练

    Efficient On-device Training via Gradient Filtering. (arXiv:2301.00330v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.00330](http://arxiv.org/abs/2301.00330)

    本文提出一种新的梯度过滤方法，通过创建具有较少唯一元素的特殊结构来实现设备端卷积神经网络模型的训练，从而大大减少了计算复杂度和内存消耗，实现最高19倍的训练加速度。

    

    尽管在联邦学习、连续学习和其他许多应用中很重要，但设备端训练仍然是EdgeAI的一个难题。本文提出了一种新的梯度过滤技术，使得设备端的卷积神经网络模型训练成为可能。该方法通过创建具有较少唯一元素的特殊结构，从而显著减少了训练期间反向传播的计算复杂度和内存消耗。在多个CNN模型（例如MobileNet、DeepLabV3、UPerNet）和设备（例如Raspberry Pi和Jetson Nano）上进行的图像分类和语义分割的大量实验表明了我们方法的有效性和广泛适用性。例如，与SOTA相比，我们实现了高达19倍的训练加速度。

    Despite its importance for federated learning, continuous learning and many other applications, on-device training remains an open problem for EdgeAI. The problem stems from the large number of operations (e.g., floating point multiplications and additions) and memory consumption required during training by the back-propagation algorithm. Consequently, in this paper, we propose a new gradient filtering approach which enables on-device CNN model training. More precisely, our approach creates a special structure with fewer unique elements in the gradient map, thus significantly reducing the computational complexity and memory consumption of back propagation during training. Extensive experiments on image classification and semantic segmentation with multiple CNN models (e.g., MobileNet, DeepLabV3, UPerNet) and devices (e.g., Raspberry Pi and Jetson Nano) demonstrate the effectiveness and wide applicability of our approach. For example, compared to SOTA, we achieve up to 19$\times$ speedu
    
[^142]: EEG解码的深度黎曼网络

    Deep Riemannian Networks for EEG Decoding. (arXiv:2212.10426v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.10426](http://arxiv.org/abs/2212.10426)

    本研究分析了深度黎曼网络对EEG的应用，探讨了网络大小、端到端能力、模型训练对模型性能的影响，并比较了其与基于黎曼几何的最先进方法。

    

    当前在电脑脑电图（EEG）解码任务中，最先进的性能通常是由深度学习或基于黎曼几何的解码器实现的。最近，越来越多的人对深度黎曼网络（DRNs）产生了兴趣，可能结合了之前两类方法的优点。然而，还有一系列问题需要进一步洞察，以铺平DRNs在EEG中更广泛应用的道路。这些问题包括架构设计问题，如网络大小和端到端能力，以及模型训练问题。这些因素如何影响模型性能尚未被探索。此外，这些网络中的数据如何转换，以及是否与传统的EEG解码相关也不清楚。本研究旨在通过分析具有广泛超参数的DRNs来奠定这些主题领域的基础。使用两个公共EEG数据集测试了网络，并与最先进的基于黎曼几何的方法进行了比较。

    State-of-the-art performance in electroencephalography (EEG) decoding tasks is currently often achieved with either Deep-Learning or Riemannian-Geometry-based decoders. Recently, there is growing interest in Deep Riemannian Networks (DRNs) possibly combining the advantages of both previous classes of methods. However, there are still a range of topics where additional insight is needed to pave the way for a more widespread application of DRNs in EEG. These include architecture design questions such as network size and end-to-end ability as well as model training questions. How these factors affect model performance has not been explored. Additionally, it is not clear how the data within these networks is transformed, and whether this would correlate with traditional EEG decoding. Our study aims to lay the groundwork in the area of these topics through the analysis of DRNs for EEG with a wide range of hyperparameters. Networks were tested on two public EEG datasets and compared with sta
    
[^143]: 逆境求生：从合成 ImageNet 克隆体中学习可迁移的表示方法

    Fake it till you make it: Learning transferable representations from synthetic ImageNet clones. (arXiv:2212.08420v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.08420](http://arxiv.org/abs/2212.08420)

    本文探究了在训练分类模型时，是否需要使用真实图像，而使用合成 ImageNet 克隆体能否弥补差距。通过最小和类不可知的提示工程，证明了合成图像制作模型和使用真实图像训练模型的巨大差距得到了弥合。

    

    最近的图像生成模型（如 Stable Diffusion）展现出令人印象深刻的能力，从简单的文本提示开始生成相当真实的图像。当训练图像预测模型时，这样的模型能否使真实图像变得过时？ 在本文中，我们从建立数据集的分类名称开始，探索了仅提供 ImageNet 克隆体的情况下，Stable Diffusion 生成合成图像进行模型训练的必要性，并测量它们对于从头开始训练分类模型的有用性。 我们展示了，在最小且类不可知的提示工程的情况下，ImageNet 克隆体能够弥合合成图像制作模型和使用真实图像训练模型之间的巨大差距，对于我们在本研究中考虑的多个标准分类基准测试。

    Recent image generation models such as Stable Diffusion have exhibited an impressive ability to generate fairly realistic images starting from a simple text prompt. Could such models render real images obsolete for training image prediction models? In this paper, we answer part of this provocative question by investigating the need for real images when training models for ImageNet classification. Provided only with the class names that have been used to build the dataset, we explore the ability of Stable Diffusion to generate synthetic clones of ImageNet and measure how useful these are for training classification models from scratch. We show that with minimal and class-agnostic prompt engineering, ImageNet clones are able to close a large part of the gap between models produced by synthetic images and models trained with real images, for the several standard classification benchmarks that we consider in this study. More importantly, we show that models trained on synthetic images exhi
    
[^144]: 一种预测Few-Shot分类泛化的统计模型

    A Statistical Model for Predicting Generalization in Few-Shot Classification. (arXiv:2212.06461v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.06461](http://arxiv.org/abs/2212.06461)

    提出了一种通过高斯模型估计特征分布参数进行预测泛化误差的方法，通过计算类条件密度距离估计可以提高泛化性能准确度。

    

    分类器泛化误差的估计通常依赖于验证集。然而，在Few-Shot学习场景中，很难获得这样的验证集，这是该领域中一个高度被忽视的缺点。因此，在这项工作中，我们引入了一个特征分布的高斯模型，通过估计这个模型的参数，我们能够预测在新的Few-Shot分类任务中的分类性能。我们发现，在类条件密度之间准确的距离估计是准确评估泛化性能的关键。因此，我们提出了一个非偏估计器来计算这些距离，并将其集成到我们的数值分析中。我们通过实验证明，我们的方法胜过了其他方法，例如留一法-Cross Validation 策略。

    The estimation of the generalization error of classifiers often relies on a validation set. Such a set is hardly available in few-shot learning scenarios, a highly disregarded shortcoming in the field. In these scenarios, it is common to rely on features extracted from pre-trained neural networks combined with distance-based classifiers such as nearest class mean. In this work, we introduce a Gaussian model of the feature distribution. By estimating the parameters of this model, we are able to predict the generalization error on new classification tasks with few samples. We observe that accurate distance estimates between class-conditional densities are the key to accurate estimates of the generalization performance. Therefore, we propose an unbiased estimator for these distances and integrate it in our numerical analysis. We empirically show that our approach outperforms alternatives such as the leave-one-out cross-validation strategy.
    
[^145]: 深度曲线编辑：针对预训练深度生成模型的可交换和非线性图像编辑

    Deep Curvilinear Editing: Commutative and Nonlinear Image Manipulation for Pretrained Deep Generative Model. (arXiv:2211.14573v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.14573](http://arxiv.org/abs/2211.14573)

    本研究提出了一种名为DeCurvEd的新方法，以可交换的方式确定潜空间中的语义交错矢量场，从而提供了高质量的图像编辑方案。

    

    图像的语义编辑是计算机视觉的基本目标。尽管深度学习方法，如生成对抗网络（GAN），能够生成高质量的图像，但通常它们不具备对生成的图像进行语义编辑的内在方式。最近的研究探讨了通过操纵潜变量来确定要生成的图像的方法。然而，假设线性语义算术的方法在图像编辑方面具有某些局限性，而发现非线性的语义路径提供了不可交换的编辑，这在以不同的顺序应用时不一致。本研究提出了一种称为深度曲线编辑（DeCurvEd）的新方法，以确定潜空间中的语义交错矢量场。我们从理论上证明，由于可交换性，多个属性的编辑仅取决于数量而不是顺序。此外，我们还通过实验证明了该方法的成功应用。

    Semantic editing of images is the fundamental goal of computer vision. Although deep learning methods, such as generative adversarial networks (GANs), are capable of producing high-quality images, they often do not have an inherent way of editing generated images semantically. Recent studies have investigated a way of manipulating the latent variable to determine the images to be generated. However, methods that assume linear semantic arithmetic have certain limitations in terms of the quality of image editing, whereas methods that discover nonlinear semantic pathways provide non-commutative editing, which is inconsistent when applied in different orders. This study proposes a novel method called deep curvilinear editing (DeCurvEd) to determine semantic commuting vector fields on the latent space. We theoretically demonstrate that owing to commutativity, the editing of multiple attributes depends only on the quantities and not on the order. Furthermore, we experimentally demonstrate th
    
[^146]: 高效参数调整可以使分类头表现出色

    Parameter-Efficient Tuning Makes a Good Classification Head. (arXiv:2210.16771v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2210.16771](http://arxiv.org/abs/2210.16771)

    提出了一种高效参数调整的分类头训练方法，取代了随机初始化的分类头使模型性能稳定提升。

    

    近年来，预训练模型革命性地改变了自然语言理解范式。我们在预训练的主干模型（如BERT）之后附加一个随机初始化的分类头，并微调整个模型。由于预训练主干模型对性能的贡献很大，因此我们自然期望良好的预训练分类头也能受益于训练。然而，主干模型最后一层的输出（即分类头的输入）在微调期间会有很大变化，导致通常的头部单独预训练（LP-FT）失效。在本文中，我们发现高效参数调整可以使分类头表现出色，通过简单地替换随机初始化的头部，我们可以获得稳定的性能提升。我们的实验表明，在 GLUE 和 SuperGLUE 的 9 项任务中，使用参数高效调整联合预训练的分类头可以持续提高性能。

    In recent years, pretrained models revolutionized the paradigm of natural language understanding (NLU), where we append a randomly initialized classification head after the pretrained backbone, e.g. BERT, and finetune the whole model. As the pretrained backbone makes a major contribution to the improvement, we naturally expect a good pretrained classification head can also benefit the training. However, the final-layer output of the backbone, i.e. the input of the classification head, will change greatly during finetuning, making the usual head-only pretraining (LP-FT) ineffective. In this paper, we find that parameter-efficient tuning makes a good classification head, with which we can simply replace the randomly initialized heads for a stable performance gain. Our experiments demonstrate that the classification head jointly pretrained with parameter-efficient tuning consistently improves the performance on 9 tasks in GLUE and SuperGLUE.
    
[^147]: 任务阶段化：来自示范的自动课程学习

    Task Phasing: Automated Curriculum Learning from Demonstrations. (arXiv:2210.10999v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10999](http://arxiv.org/abs/2210.10999)

    本文介绍了一种结合了示范学习和课程学习的任务阶段化方法，使用逆强化学习自动生成课程序列，逐步增加任务复杂度，以帮助解决强化学习在稀疏奖励领域中的挑战。

    

    将强化学习（RL）应用于稀疏奖励域通常具有挑战性，因为缺乏足够的引导信号。解决此类领域的常见RL技术包括（1）从示范学习和（2）课程学习。虽然这两种方法已经被详细研究，但它们很少被同时考虑。本文旨在通过引入一种基于示范的原则性任务阶段化方法来实现该目的，该方法使用示范自动生成课程序列。我们使用来自（次优）演示的逆RL定义了一个简单的初始任务。然后，我们的任务阶段化方法提供了一个框架，逐渐增加任务的复杂性，直到目标任务，同时在每个阶段迭代中重新调整RL代理。考虑了两种分阶段方法：（1）逐步增加RL代理处于控制下的时间步数的比例，以及（2）逐步淘汰引导性信息奖励函数。我们提出了保证收敛的条件。

    Applying reinforcement learning (RL) to sparse reward domains is notoriously challenging due to insufficient guiding signals. Common RL techniques for addressing such domains include (1) learning from demonstrations and (2) curriculum learning. While these two approaches have been studied in detail, they have rarely been considered together. This paper aims to do so by introducing a principled task phasing approach that uses demonstrations to automatically generate a curriculum sequence. Using inverse RL from (suboptimal) demonstrations we define a simple initial task. Our task phasing approach then provides a framework to gradually increase the complexity of the task all the way to the target task, while retuning the RL agent in each phasing iteration. Two approaches for phasing are considered: (1) gradually increasing the proportion of time steps an RL agent is in control, and (2) phasing out a guiding informative reward function. We present conditions that guarantee the convergence 
    
[^148]: 几乎线性稀疏度下的字典学习

    Dictionary Learning for the Almost-Linear Sparsity Regime. (arXiv:2210.10855v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10855](http://arxiv.org/abs/2210.10855)

    本文提出了一种高效的谱方法SPORADIC，在几乎线性稀疏度下的字典学习问题中可以恢复超完备字典。

    

    字典学习指的是从形如$\mathbf{y}_i = \mathbf{D}\mathbf{x}_i$的样本中恢复一个矩阵$\mathbf{D} \in \mathbb{R}^{M \times K}$和$N$个$s$-稀疏向量$\mathbf{x}_i \in \mathbb{R}^{K}$的问题。在字典已知的情况下，即使稀疏度线性增长到尺寸$M$，也可以恢复$x_i$，但迄今为止，唯一能在线性稀疏度范围内有保证成功的算法是黎曼信赖区域方法，这种方法仅适用于正交字典，并且基于平方和层次的方法需要超多项式时间才能获得在$M$中衰减的误差。在这项工作中，我们介绍了SPORADIC（SPectral ORAcle DICtionary Learning），这是一种基于一系列加权协方差矩阵的高效谱方法。我们证明，在足够高的维度下，SPORADIC可以恢复满足$K>M$的超完备字典。

    Dictionary learning, the problem of recovering a sparsely used matrix $\mathbf{D} \in \mathbb{R}^{M \times K}$ and $N$ $s$-sparse vectors $\mathbf{x}_i \in \mathbb{R}^{K}$ from samples of the form $\mathbf{y}_i = \mathbf{D}\mathbf{x}_i$, is of increasing importance to applications in signal processing and data science. When the dictionary is known, recovery of $\mathbf{x}_i$ is possible even for sparsity linear in dimension $M$, yet to date, the only algorithms which provably succeed in the linear sparsity regime are Riemannian trust-region methods, which are limited to orthogonal dictionaries, and methods based on the sum-of-squares hierarchy, which requires super-polynomial time in order to obtain an error which decays in $M$. In this work, we introduce SPORADIC (SPectral ORAcle DICtionary Learning), an efficient spectral method on family of reweighted covariance matrices. We prove that in high enough dimensions, SPORADIC can recover overcomplete ($K > M$) dictionaries satisfying the
    
[^149]: 基于径向基神经网络的安全偏倚逼近不安全区域的几何

    Geometry of Radial Basis Neural Networks for Safety Biased Approximation of Unsafe Regions. (arXiv:2210.05596v2 [eess.SY] UPDATED)

    [http://arxiv.org/abs/2210.05596](http://arxiv.org/abs/2210.05596)

    本文通过基于安全和不安全样本测量的方法，合成了一个用于描述安全集的零障碍函数，并探讨了该方法中的神经网络的几何结构。

    

    基于屏障函数的不等式约束是实施控制系统安全规范的一种手段。当与凸优化程序一起使用时，它们提供了一种计算效率高的方法，可以强制执行控制仿射系统的安全性。采用这种方法的主要前提之一是先验知识，即对屏障函数本身，即安全集的了解。在未知环境中的导航中，本地安全集随时间演变，不存在这样的知识。本文侧重于基于安全和不安全样本测量（例如从导航应用中的感知数据中获取）综合描述安全集的零障碍函数的合成。之前的工作制定了一个监督式机器学习算法，其解决方案保证了具有特定级集属性的零障碍函数的构建。然而，它没有探讨神经网络的几何结构。

    Barrier function-based inequality constraints are a means to enforce safety specifications for control systems. When used in conjunction with a convex optimization program, they provide a computationally efficient method to enforce safety for the general class of control-affine systems. One of the main assumptions when taking this approach is the a priori knowledge of the barrier function itself, i.e., knowledge of the safe set. In the context of navigation through unknown environments where the locally safe set evolves with time, such knowledge does not exist. This manuscript focuses on the synthesis of a zeroing barrier function characterizing the safe set based on safe and unsafe sample measurements, e.g., from perception data in navigation applications. Prior work formulated a supervised machine learning algorithm whose solution guaranteed the construction of a zeroing barrier function with specific level-set properties. However, it did not explore the geometry of the neural networ
    
[^150]: 一个Transformer模型可同时处理2D和3D分子数据

    One Transformer Can Understand Both 2D & 3D Molecular Data. (arXiv:2210.01765v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01765](http://arxiv.org/abs/2210.01765)

    本文提出了一个基于Transformer的分子模型，名为Transformer-M，可以处理2D和3D格式的分子数据并生成有意义的语义表示。

    

    与通常有唯一格式的视觉和语言数据不同，分子可以自然地用不同的化学公式进行表征。对于分子表示学习，大多数先前的工作只设计了针对特定数据格式的神经网络，使得学习的模型可能无法处理其他数据格式。我们认为，化学的通用神经网络模型应能够处理跨数据模态的分子任务。为实现此目标，我们开发了一种新型的基于Transformer的分子模型，称为Transformer-M，它可以将2D或3D格式的分子数据作为输入并生成有意义的语义表示。使用标准Transformer作为骨干架构，Transformer-M开发了两个分离的通道来编码2D和3D结构信息，并将它们与网络模块中的原子特征结合起来。

    Unlike vision and language data which usually has a unique format, molecules can naturally be characterized using different chemical formulations. One can view a molecule as a 2D graph or define it as a collection of atoms located in a 3D space. For molecular representation learning, most previous works designed neural networks only for a particular data format, making the learned models likely to fail for other data formats. We believe a general-purpose neural network model for chemistry should be able to handle molecular tasks across data modalities. To achieve this goal, in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations. Using the standard Transformer as the backbone architecture, Transformer-M develops two separated channels to encode 2D and 3D structural information and incorporate them with the atom features in the network modules. Whe
    
[^151]: 利用多视角变分自编码器进行多视角信息融合以预测股骨近端强度

    Multi-view information fusion using multi-view variational autoencoders to predict proximal femoral strength. (arXiv:2210.00674v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00674](http://arxiv.org/abs/2210.00674)

    本文提出了一个利用多视角信息融合来预测股骨近端强度的模型，通过变分自编码器(MVAE)进行特征表示学习和专家之积模型(PoE)进行信息融合。研究采用全基因组关联研究(GWAS)选择变异体，整合WGS和DXA成像特征，得到了最好的预测模型。

    

    本文旨在设计一个基于深度学习的模型，利用多视角信息融合来预测股骨近端强度。我们采用多视角变分自编码器（MVAE）进行特征表示学习，并使用专家之积（PoE）模型进行多视角信息融合。我们将所提出的模型应用于931名男性受试者组成的路易斯安那骨质疏松症研究（LOS）队列中，包括345名非裔美国人和586名白人。通过高斯分布乘积的解析解，我们采用变分推断来训练设计的MVAE-PoE模型以执行通用潜在特征提取。我们进行了全基因组关联研究（GWAS）来选取每个股骨近端强度的最低p值的256个遗传变异体，并整合了全基因组序列（WGS）特征和DXA成像特征来预测股骨近端强度。

    The aim of this paper is to design a deep learning-based model to predict proximal femoral strength using multi-view information fusion. Method: We developed new models using multi-view variational autoencoder (MVAE) for feature representation learning and a product of expert (PoE) model for multi-view information fusion. We applied the proposed models to an in-house Louisiana Osteoporosis Study (LOS) cohort with 931 male subjects, including 345 African Americans and 586 Caucasians. With an analytical solution of the product of Gaussian distribution, we adopted variational inference to train the designed MVAE-PoE model to perform common latent feature extraction. We performed genome-wide association studies (GWAS) to select 256 genetic variants with the lowest p-values for each proximal femoral strength and integrated whole genome sequence (WGS) features and DXA-derived imaging features to predict proximal femoral strength. Results: The best prediction model for fall fracture load was 
    
[^152]: FINDE: 基于神经微分方程的不变量发现与保持

    FINDE: Neural Differential Equations for Finding and Preserving Invariant Quantities. (arXiv:2210.00272v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00272](http://arxiv.org/abs/2210.00272)

    FINDE是一种基于神经微分方程的方法，可以在不需要先验知识的情况下自动发现和保持动力系统的不变量，有助于科学发现和未知系统建模。

    

    许多实际动力学系统都与首次积分（也称为不变量）相关联，它们是随时间保持不变的量。首次积分的发现和理解是自然科学和工业应用中基本和重要的主题。首次积分源自系统能量、动量和质量的守恒定律以及对状态的限制；这些通常与控制方程的特定几何结构相关。现有的设计用于确保这些首次积分的神经网络在建模数据方面表现出色。然而，这些模型包含了潜在的结构，在神经网络学习未知系统的大多数情况下，这些结构也是未知的。为了科学发现和未知系统建模，需要克服这种限制。为此，我们提出了一种首次积分保持的神经微分方程（FINDE）。通过利用神经常微分方程中的投射方法，FINDE可以在不需要先验知识的情况下自动发现和保持首次积分。我们证明了FINDE有效地学习了几个经典示例的潜在动态和不变量，包括Duffing和van der Pol振荡器，并提供有关首次积分发现和动态系统深度学习的见解。

    Many real-world dynamical systems are associated with first integrals (a.k.a. invariant quantities), which are quantities that remain unchanged over time. The discovery and understanding of first integrals are fundamental and important topics both in the natural sciences and in industrial applications. First integrals arise from the conservation laws of system energy, momentum, and mass, and from constraints on states; these are typically related to specific geometric structures of the governing equations. Existing neural networks designed to ensure such first integrals have shown excellent accuracy in modeling from data. However, these models incorporate the underlying structures, and in most situations where neural networks learn unknown systems, these structures are also unknown. This limitation needs to be overcome for scientific discovery and modeling of unknown systems. To this end, we propose first integral-preserving neural differential equation (FINDE). By leveraging the proje
    
[^153]: 一种用于住宅短期负荷预测的安全联邦学习框架

    A Secure Federated Learning Framework for Residential Short Term Load Forecasting. (arXiv:2209.14547v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2209.14547](http://arxiv.org/abs/2209.14547)

    本论文提出了一种安全的联邦学习框架，能够在确保个人数据隐私的同时，提高联邦短期负荷预测对于拜占庭威胁的鲁棒性。

    

    智能电表是准确需求预测的关键，但由于消费者隐私和数据泄露等问题，存在若干缺点。最近的文献中，联邦学习（FL）被探索作为一种有前途的保护隐私的机器学习替代方案，它可以在不暴露私有原始数据的情况下进行模型的协作学习，用于短期负荷预测。尽管FL具有优点，但标准FL仍然易受名为拜占庭攻击的难以处理的网络威胁的影响，这种攻击是由有缺陷和/或恶意客户发起的。因此，为了提高联邦短期负荷预测对拜占庭威胁的鲁棒性，我们开发了一个最先进的差分隐私安全的FL-based框架，该框架确保了个人智能电表数据的隐私，同时保护了FL模型和架构的安全。我们的提议框架利用了通过Sign Stochastic Gradient Descent（SignSGD）算法进行梯度量化的思想。

    Smart meter measurements, though critical for accurate demand forecasting, face several drawbacks including consumers' privacy, data breach issues, to name a few. Recent literature has explored Federated Learning (FL) as a promising privacy-preserving machine learning alternative which enables collaborative learning of a model without exposing private raw data for short term load forecasting. Despite its virtue, standard FL is still vulnerable to an intractable cyber threat known as Byzantine attack carried out by faulty and/or malicious clients. Therefore, to improve the robustness of federated short-term load forecasting against Byzantine threats, we develop a state-of-the-art differentially private secured FL-based framework that ensures the privacy of the individual smart meter's data while protect the security of FL models and architecture. Our proposed framework leverages the idea of gradient quantization through the Sign Stochastic Gradient Descent (SignSGD) algorithm, where the
    
[^154]: 法律引导代码：一种法律信息学方法来使人工智能与人类保持一致

    Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans. (arXiv:2209.13020v13 [cs.CY] UPDATED)

    [http://arxiv.org/abs/2209.13020](http://arxiv.org/abs/2209.13020)

    这篇论文提出了一种“法律指导代码”理念，利用法律信息学将法律知识和推理嵌入到人工智能中，从而使人工智能与人类的目标和社会价值保持一致。

    

    目前我们无法可靠地指定人类的目标和社会价值，以引导人工智能的行为。制定法律和解释法律构成了一种计算引擎，将不透明的人类价值转化为易读的指令。 “法律指导代码”是嵌入了法律知识和推理的人工智能研究议程。类似于合同当事人无法预见他们未来关系的每个潜在变数，立法者无法预测其提出的法案将适用的所有情况，我们无法提前明确规则，以可靠地引导良好的人工智能行为。法律理论和实践已经开发出各种工具来解决这些规定问题。与法律更为普通的用途（例如通过制裁威胁来阻止不良行为）相反，法律作为一种表达人类沟通目标和价值的表现，可以引导人工智能代码的发展方向。具体而言，法律信息学是计算规则和系统用于表示，分析和操作法律知识的跨学科研究。法律指导代码利用法律信息学来使人工智能与人类的目标和社会价值保持一致，以一种透明，负责，灵活的方式。

    We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. "Law Informs Code" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goa
    
[^155]: 利用迁移学习像病理学家一样自动评分组织图像

    Automatically Score Tissue Images Like a Pathologist by Transfer Learning. (arXiv:2209.05954v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.05954](http://arxiv.org/abs/2209.05954)

    该算法通过选择性迁移学习从多个小辅助集中提取知识，从具有“相似”特征的组织图像中学习染色模式，以实现像病理学家一样自动评分组织图像的目标。

    

    癌症是全球第二大死亡原因。早期诊断癌症可以挽救很多生命。病理学家必须手动查看组织微阵列 (TMA) 图像以识别肿瘤，这可能会耗费时间、不一致且主观。现有的自动检测肿瘤的算法要么没有达到病理学家的准确水平，要么需要大量的人工参与。主要挑战是具有不同形状、大小和位置的 TMA 图像可能具有相同的得分。由于医疗组织中的隐私问题和限制，学习 TMA 图像的染色模式受到严重限制。来自不同癌症类型的 TMA 图像可能具有共同的特征，提供了有价值的信息，但直接使用会损害准确性。通过选择性迁移学习来自多个小辅助集的知识，所提出的算法能够提取显示“类似”的组织图像的知识，从而在TMA图像评分方面取得了很好的成果。

    Cancer is the second leading cause of death in the world. Diagnosing cancer early on can save many lives. Pathologists have to look at tissue microarray (TMA) images manually to identify tumors, which can be time-consuming, inconsistent and subjective. Existing algorithms that automatically detect tumors have either not achieved the accuracy level of a pathologist or require substantial human involvements. A major challenge is that TMA images with different shapes, sizes, and locations can have the same score. Learning staining patterns in TMA images requires a huge number of images, which are severely limited due to privacy concerns and regulations in medical organizations. TMA images from different cancer types may have common characteristics that could provide valuable information, but using them directly harms the accuracy. By selective transfer learning from multiple small auxiliary sets, the proposed algorithm is able to extract knowledge from tissue images showing a ``similar" s
    
[^156]: 增强编码：通过对训练标签进行编码的新型不平衡分类方法。

    Enhancement Encoding: A Novel Imbalanced Classification Approach via Encoding the Training Labels. (arXiv:2208.11056v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.11056](http://arxiv.org/abs/2208.11056)

    本文提出了一种增强编码方法来解决类别不平衡问题，该方法可以根据数据集的不同特点对不同的样本进行编码生成不同的编码向量，实验结果表明该方法在几个基准数据集上优于最先进的方法。

    

    类别不平衡，也称为长尾分布，在基于机器学习的分类任务中是常见问题。如果出现类别不平衡，那么少数类数据将被多数类淹没，这对数据科学提出了巨大挑战。为了解决类别不平衡问题，研究人员提出了许多方法：一些人使数据集平衡（SMOTE），一些人改进损失函数（Focal Loss），甚至有人注意到标签似乎影响类别不平衡学习（Yang和Xu。重塑标记的价值，以提高类别不平衡学习。在NeurIPS 2020中），但还没有人改变数据标签的编码方式。目前，编码标签的最流行技术是一位编码，因为它在一般情况下具有良好的性能。但对于不平衡的数据，它并不是一个好选择，因为分类器将平等对待多数和少数样本。本文创新性地提出了增强编码方法来解决类别不平衡问题。具体地，我们利用标签的值为多数和少数样本生成不同的编码向量，这可以在模型训练期间有效平衡不同样本的贡献。在几个基准数据集上的实验证明，所提出的方法在准确性、F1得分和曲线下面积（AUC）方面优于最先进的方法。

    Class imbalance, which is also called long-tailed distribution, is a common problem in classification tasks based on machine learning. If it happens, the minority data will be overwhelmed by the majority, which presents quite a challenge for data science. To address the class imbalance problem, researchers have proposed lots of methods: some people make the data set balanced (SMOTE), some others refine the loss function (Focal Loss), and even someone has noticed the value of labels influences class-imbalanced learning (Yang and Xu. Rethinking the value of labels for improving class-imbalanced learning. In NeurIPS 2020), but no one changes the way to encode the labels of data yet. Nowadays, the most prevailing technique to encode labels is the one-hot encoding due to its nice performance in the general situation. However, it is not a good choice for imbalanced data, because the classifier will treat majority and minority samples equally. In this paper, we innovatively propose the enhanc
    
[^157]: 机器学习在轨道估计中的应用：一项综述

    Machine Learning in Orbit Estimation: a Survey. (arXiv:2207.08993v3 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2207.08993](http://arxiv.org/abs/2207.08993)

    本综述介绍了机器学习在轨道估计中的应用现状，讨论了当前物理方法的不足，提出了通过推导未测量物体的特征来提高轨道预测准确度的方案。

    

    自从上世纪50年代发射了第一颗人造卫星以来，轨道上的空间物体数量持续增加。目前估计地球上有一百万个大小超过一厘米的物体正在绕地运行，其中只有三万个大小超过十厘米的物体被跟踪。为了避免碰撞链反应，即基斯勒症候群的发生，必须准确地跟踪和预测碎片和卫星的轨道。当前的近似物理方法对于七天的预测误差在公里级别，这在考虑通常小于一米的空间碎片时是不足够的。这种失败通常是由于在轨道起始点附近的空间物体状态的不确定性，环境条件（如大气阻力）的预测误差以及空间物体的未知特征（如质量或几何形状）所致。操作员可以通过推导未测量物体的特征来提高轨道预测的准确性。

    Since the late 1950s, when the first artificial satellite was launched, the number of Resident Space Objects has steadily increased. It is estimated that around one million objects larger than one cm are currently orbiting the Earth, with only thirty thousand larger than ten cm being tracked. To avert a chain reaction of collisions, known as Kessler Syndrome, it is essential to accurately track and predict debris and satellites' orbits. Current approximate physics-based methods have errors in the order of kilometers for seven-day predictions, which is insufficient when considering space debris, typically with less than one meter. This failure is usually due to uncertainty around the state of the space object at the beginning of the trajectory, forecasting errors in environmental conditions such as atmospheric drag, and unknown characteristics such as the mass or geometry of the space object. Operators can enhance Orbit Prediction accuracy by deriving unmeasured objects' characteristics
    
[^158]: 基于时间序列的数据增强技术：一份综述和分类

    Data Augmentation techniques in time series domain: A survey and taxonomy. (arXiv:2206.13508v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.13508](http://arxiv.org/abs/2206.13508)

    本综述介绍了基于时间序列数据增强技术的最新进展，并提出了一个分类法，旨在提高训练深度神经网络的数据集的大小和一致性，从而提高模型的效率和性能。

    

    随着深度学习建模的最新进展，利用其在时间序列领域中出色性能的方式并不需要太长时间。深度神经网络在处理时间序列方面严重依赖于训练中使用的数据集的大小和一致性。这些特征通常在现实世界中并不丰富，通常受到限制和需要保证的约束。因此，提高数据量的有效方法是使用数据增强技术，无论是通过添加噪声或置换还是生成新的合成数据。本文系统地回顾了该领域中的最新技术现状，提供了所有可用算法的概述，并提出了最相关研究的分类法。不同变体的效率将作为该过程的中心部分进行评估，同时还将评估不同的性能指标以及每个模型的主要问题。

    With the latest advances in Deep Learning-based} generative models, it has not taken long to take advantage of their remarkable performance in the area of time series. Deep neural networks used to work with time series heavily depend on the size and consistency of the datasets used in training. These features are not usually abundant in the real world, where they are usually limited and often have constraints that must be guaranteed. Therefore, an effective way to increase the amount of data is by using Data Augmentation techniques, either by adding noise or permutations and by generating new synthetic data. This work systematically reviews the current state-of-the-art in the area to provide an overview of all available algorithms and proposes a taxonomy of the most relevant research. The efficiency of the different variants will be evaluated as a central part of the process, as well as the different metrics to evaluate the performance and the main problems concerning each model will b
    
[^159]: DSCA：双流网络与全尺度图像金字塔上的交叉注意力用于癌症预测

    DSCA: A Dual-Stream Network with Cross-Attention on Whole-Slide Image Pyramids for Cancer Prognosis. (arXiv:2206.05782v4 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2206.05782](http://arxiv.org/abs/2206.05782)

    本文提出了一种双流网络和交叉注意力方法，以有效地利用全尺度图像金字塔，解决多分辨率特征融合中未注意到的语义差距和高计算成本的问题，经实验证实了方法的有效性。

    

    在千亿级别的全尺度图像中进行癌症预测一直是一个具有挑战性的任务。为了进一步增强全尺度图像的可视化表示，现有方法已经探索使用图像金字塔而非单一分辨率图像。尽管如此，它们仍面临两个主要问题，即高计算成本和多分辨率特征融合中未注意到的语义差距。为了解决这些问题，本文提出了一种名为DSCA的方法，即双流网络和交叉注意力。我们的关键思想是利用两个子流来处理两种分辨率的WSI补丁，其中高分辨率流中设计了一个正方形池来显著降低计算成本，并提出了一种基于交叉注意力的方法来正确处理双流特征的融合。我们在三个公开数据集上验证了我们的DSCA，共计1,911名患者的3,101个WSI。我们的实验和消融研究证实了方法的有效性。

    The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a challenging task. To further enhance WSI visual representations, existing methods have explored image pyramids, instead of single-resolution images, in WSIs. In spite of this, they still face two major problems: high computational cost and the unnoticed semantical gap in multi-resolution feature fusion. To tackle these problems, this paper proposes to efficiently exploit WSI pyramids from a new perspective, the dual-stream network with cross-attention (DSCA). Our key idea is to utilize two sub-streams to process the WSI patches with two resolutions, where a square pooling is devised in a high-resolution stream to significantly reduce computational costs, and a cross-attention-based method is proposed to properly handle the fusion of dual-stream features. We validate our DSCA on three publicly-available datasets with a total number of 3,101 WSIs from 1,911 patients. Our experiments and ablation studies verify 
    
[^160]: 基于频率注意力的图卷积网络预测脑瘫

    Cerebral Palsy Prediction with Frequency Attention Informed Graph Convolutional Networks. (arXiv:2204.10997v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2204.10997](http://arxiv.org/abs/2204.10997)

    本文提出一种基于频率注意力的图卷积网络，利用频率信息提高了脑瘫预测性能，并在消费级RGB视频数据集上取得最新研究最高水平的结果。

    

    脑瘫(CP)的早期诊断和干预是治疗过程中最重要的部分，因此有必要设计一种高效且可解释的自动预测CP的系统。我们发现CP婴儿的人体运动频率与健康组有显著差异，这提高了预测性能。然而，现有的基于深度学习的方法没有利用婴儿运动的频率信息进行预测。本文提出了一种基于频率注意力的图卷积网络，并在两个消费级RGB视频数据集(MINI-RGBD 和 RVI-38数据集)上进行验证。我们的频率注意力模块有助于提高分类性能和系统可解释性。此外，我们设计了一种频率分桶方法，可以保留人体关节位置数据的关键频率，同时过滤噪声。我们的预测性能在两个数据集上均达到了最新研究的最高水平。

    Early diagnosis and intervention are clinically considered the paramount part of treating cerebral palsy (CP), so it is essential to design an efficient and interpretable automatic prediction system for CP. We highlight a significant difference between CP infants' frequency of human movement and that of the healthy group, which improves prediction performance. However, the existing deep learning-based methods did not use the frequency information of infants' movement for CP prediction. This paper proposes a frequency attention informed graph convolutional network and validates it on two consumer-grade RGB video datasets, namely MINI-RGBD and RVI-38 datasets. Our proposed frequency attention module aids in improving both classification performance and system interpretability. In addition, we design a frequency-binning method that retains the critical frequency of the human joint position data while filtering the noise. Our prediction performance achieves state-of-the-art research on bot
    
[^161]: 居民众包中空间欠报告差异的量化

    Quantifying Spatial Under-reporting Disparities in Resident Crowdsourcing. (arXiv:2204.08620v2 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2204.08620](http://arxiv.org/abs/2204.08620)

    本文研究了城市治理中居民众包的问题，并提出了一种准确测量报道率的方法，使不同的报道率不再成为城市治理下游解决事件速度方面的不公平根源。

    

    现代城市治理在很大程度上依赖于众包(“协同生产”)，以识别 downed trees 和 power lines 等问题。一个主要问题是，居民不以相同的速率报告问题，不同的报告异质性直接转化为下游在解决事件的速度方面的差异。测量这样的欠报告是一个困难的统计任务，因为根据定义，我们不能观察到没有被报告的事件或报告事件第一次发生的时间。因此，不能单纯地区分低报道率和低基准真实事件率。我们开发了一种方法来识别(异质的)报道率，而不使用外部基准真实数据。我们的想法是，在相同事件的 $\textit{duplicate}$ 报告中的比率可以利用来消除报告率随事件发生而发生的不确定性。借助这个思想，我们将问题简化为标准的泊松率估计任务，尽管标题有很多技术术语，但论文讨论了如何通过众包来帮助城市治理，不同的报道率如何导致问题解决的差异。作者提出了一种准确测量报道率的方法，而不依赖于外部数据。他们利用多次报告的事件可以帮助区分事件是否发生以及其报道率。

    Modern city governance relies heavily on crowdsourcing ("co-production") to identify problems such as downed trees and power lines. A major concern is that residents do not report problems at the same rates, with reporting heterogeneity directly translating to downstream disparities in how quickly incidents can be addressed. Measuring such under-reporting is a difficult statistical task, as, by definition, we do not observe incidents that are not reported or when reported incidents first occurred. Thus, low reporting rates and low ground-truth incident rates cannot be naively distinguished. We develop a method to identify (heterogeneous) reporting rates, without using external ground truth data. Our insight is that rates on $\textit{duplicate}$ reports about the same incident can be leveraged to disambiguate whether an incident has occurred with its reporting rate once it has occurred. Using this idea, we reduce the question to a standard Poisson rate estimation task -- even though the
    
[^162]: CGC: 对比图聚类用于社区发现和跟踪

    CGC: Contrastive Graph Clustering for Community Detection and Tracking. (arXiv:2204.08504v3 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2204.08504](http://arxiv.org/abs/2204.08504)

    本文提出了一种全新的图聚类算法CGC，采用对比学习进行自监督表示学习，结合跟踪模块以应对动态图拓扑变化，在社区发现和跟踪方面表现出领先的状态。

    

    本文从图聚类的角度入手，探讨在网络数据中发现实体和实体之间的交互以及对它们进行社区跟踪的方法。我们提出了一种全新的端到端框架CGC，该方法利用对比学习范式进行自监督表示学习，并采用了跟踪模块以应对不断变化的图形拓扑。在各个真实场景和合成基准上，我们对CGC进行了评估，并在社区发现和跟踪方面展示出了卓越的性能，尤其在动态图上表现出了领先的状态。

    Given entities and their interactions in the web data, which may have occurred at different time, how can we find communities of entities and track their evolution? In this paper, we approach this important task from graph clustering perspective. Recently, state-of-the-art clustering performance in various domains has been achieved by deep clustering methods. Especially, deep graph clustering (DGC) methods have successfully extended deep clustering to graph-structured data by learning node representations and cluster assignments in a joint optimization framework. Despite some differences in modeling choices (e.g., encoder architectures), existing DGC methods are mainly based on autoencoders and use the same clustering objective with relatively minor adaptations. Also, while many real-world graphs are dynamic, previous DGC methods considered only static graphs. In this work, we develop CGC, a novel end-to-end framework for graph clustering, which fundamentally differs from existing meth
    
[^163]: 神经元多样性能够提高物理学及其他领域的机器学习

    Neuronal diversity can improve machine learning for physics and beyond. (arXiv:2204.04348v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2204.04348](http://arxiv.org/abs/2204.04348)

    本文展示了使用多样化到神经元来改进机器学习，构建出能够通过学习自身激活函数快速多样化的神经网络，并且胜过传统的同构神经元网络，在图像分类和非线性回归任务中表现更优，这种学习到的多样性为动态系统选择多样性而非均匀性提供了例子，并阐明了多样性在自然和人工系统中的作用。

    

    自然界表现出多样性的优点，但是人工神经网络的层通常是由同构神经元构成的。本文中我们建立起能够学习自身激活函数、快速多样化并且在图像分类和非线性回归任务中胜过同构神经元的神经网络。子网络实例化了神经元，而元学习尤其高效的非线性响应集合。例子包括传统的神经网络分类数字和预测一个 van der Pol 振荡器和一种物理学驱动的 Hamiltonian 神经网络学习 Hénond-Heiles 轨道。这种学习到的多样性为动态系统选择多样性而非均匀性提供了例子，并阐明了多样性在自然和人工系统中的作用。

    Diversity conveys advantages in nature, yet homogeneous neurons typically comprise the layers of artificial neural networks. Here we construct neural networks from neurons that learn their own activation functions, quickly diversify, and subsequently outperform their homogeneous counterparts on image classification and nonlinear regression tasks. Sub-networks instantiate the neurons, which meta-learn especially efficient sets of nonlinear responses. Examples include conventional neural networks classifying digits and forecasting a van der Pol oscillator and a physics-informed Hamiltonian neural network learning H\'enon-Heiles orbits. Such learned diversity provides examples of dynamical systems selecting diversity over uniformity and elucidates the role of diversity in natural and artificial systems.
    
[^164]: 基于星形细胞对关键期的神经可塑性神经网络，通过现有和记忆性的大脑可塑性和突触形成实现突触竞争和强度平衡。（arXiv: 2203.11740v12 [cs.NE] UPDATED）

    Plasticity Neural Network Based on Astrocytic effects at Critical Period, Synaptic Competition and Strength Rebalance by Current and Mnemonic Brain Plasticity and Synapse Formation. (arXiv:2203.11740v12 [cs.NE] UPDATED)

    [http://arxiv.org/abs/2203.11740](http://arxiv.org/abs/2203.11740)

    该论文提出基于星形细胞作用的神经网络，通过突触的竞争和强度平衡实现现有和记忆性的大脑可塑性和突触形成，并探讨了与关键期相关的神经紊乱和负面和正面记忆的持久性对突触激活的影响。

    

    除了突触共享连接权重之外，PNN还包括突触有效范围的权重[14-25]。PNN考虑突触强度平衡在突触吞噬的动态和长度常数之和的静态中[14]，并包含了鱼群行为的先导行为。突触形成在实验和模拟中会抑制树突生成[15]。类似于Spring Boot中的强制韧性，反向回路的记忆持久度梯度也存在。相对较好和较差的梯度信息存储在类似于脑褶的记忆痕迹细胞中，在反向回路的突触形成中。争议认为人类海马神经元的再生能力是否持续到老年，并可能在后期迭代中形成新的更长的回路[17,18]。关闭关键期会导致神经紊乱在实验和模拟中[19]。考虑到负面和正面记忆的持久性，有助于更好地激活突触。

    In addition to the weights of synaptic shared connections, PNN includes weights of synaptic effective ranges [14-25]. PNN considers synaptic strength balance in dynamic of phagocytosing of synapses and static of constant sum of synapses length [14], and includes the lead behavior of the school of fish. Synapse formation will inhibit dendrites generation in experiments and simulations [15]. The memory persistence gradient of retrograde circuit similar to the Enforcing Resilience in a Spring Boot. The relatively good and inferior gradient information stored in memory engram cells in synapse formation of retrograde circuit like the folds in brain [16]. The controversy was claimed if human hippocampal neurogenesis persists throughout aging, may have a new and longer circuit in late iteration [17,18]. Closing the critical period will cause neurological disorder in experiments and simulations [19]. Considering both negative and positive memories persistence help activate synapse better than 
    
[^165]: 基于海绵毒化的能耗延迟攻击。

    Energy-Latency Attacks via Sponge Poisoning. (arXiv:2203.08147v4 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2203.08147](http://arxiv.org/abs/2203.08147)

    本文探讨了一种名为“海绵毒化”的攻击方法，首次证明了在训练时注入海绵样本可以在测试时提高机器学习模型在每个输入上的能耗和延迟，并且即使攻击者只控制了一些模型更新也可以进行此攻击，海绵毒化几乎完全消除了硬件加速器的效果。

    

    海绵样本是在测试时精心优化的输入，可在硬件加速器上部署时增加神经网络的能量消耗和延迟。本文首次证明了海绵样本也可通过一种名为海绵毒化的攻击注入到训练中。该攻击允许在每个测试时输入中不加区分地提高机器学习模型的能量消耗和延迟。我们提出了一种新的海绵毒化形式化方法，克服了与优化测试时海绵样本相关的限制，并表明即使攻击者仅控制几个模型更新，例如模型训练被外包给不受信任的第三方或通过联邦学习分布式进行，也可以进行这种攻击。我们进行了广泛的实验分析，表明海绵毒化几乎完全消除了硬件加速器的效果。同时，我们还分析了毒化模型的激活，确定了哪些计算对导致能量消耗和延迟增加起重要作用。

    Sponge examples are test-time inputs carefully optimized to increase energy consumption and latency of neural networks when deployed on hardware accelerators. In this work, we are the first to demonstrate that sponge examples can also be injected at training time, via an attack that we call sponge poisoning. This attack allows one to increase the energy consumption and latency of machine-learning models indiscriminately on each test-time input. We present a novel formalization for sponge poisoning, overcoming the limitations related to the optimization of test-time sponge examples, and show that this attack is possible even if the attacker only controls a few model updates; for instance, if model training is outsourced to an untrusted third-party or distributed via federated learning. Our extensive experimental analysis shows that sponge poisoning can almost completely vanish the effect of hardware accelerators. We also analyze the activations of poisoned models, identifying which comp
    
[^166]: 多传感器大规模数据集用于多视角三维重建

    Multi-sensor large-scale dataset for multi-view 3D reconstruction. (arXiv:2203.06111v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.06111](http://arxiv.org/abs/2203.06111)

    这个数据集包括不同传感器注册的RGB和深度数据，可用于多视角三维重建。它对具有挑战性的物质属性进行了选择并提供了在多个场景下获取的大量数据，旨在帮助算法的评估和训练。

    

    我们提供了一个新的多传感器数据集，用于多视角三维表面重建。它包括来自不同分辨率和模态传感器的注册RGB和深度数据:智能手机、英特尔RealSense、微软Kinect、工业相机和结构光扫描仪。场景的选择强调了现有算法挑战的各种物质属性。我们提供了约107个不同场景的100个视角方向下的1.4百万张图像，涵盖了14种照明条件。我们预计我们的数据集将对评估和培训三维重建算法及相关任务有用。该数据集可在skoltech3d.appliedai.tech上获得。

    We present a new multi-sensor dataset for multi-view 3D surface reconstruction. It includes registered RGB and depth data from sensors of different resolutions and modalities: smartphones, Intel RealSense, Microsoft Kinect, industrial cameras, and structured-light scanner. The scenes are selected to emphasize a diverse set of material properties challenging for existing algorithms. We provide around 1.4 million images of 107 different scenes acquired from 100 viewing directions under 14 lighting conditions. We expect our dataset will be useful for evaluation and training of 3D reconstruction algorithms and for related tasks. The dataset is available at skoltech3d.appliedai.tech.
    
[^167]: 重新思考基于重构自编码器的外样本检测

    Rethinking Reconstruction Autoencoder-Based Out-of-Distribution Detection. (arXiv:2203.02194v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2203.02194](http://arxiv.org/abs/2203.02194)

    本文重新考虑了基于重构自编码器方法的外样本检测，在最大压缩自编码器的潜空间和保证重构能力的基础上，通过引入语义重构、数据确定性分解和标准化L2距离等策略，本文提出的方法在各个基准测试中都取得了最先进的性能表现，且不需要额外的标记外样本数据。

    

    在某些情况下，分类器需要检测远离其训练数据的外样本。重构自编码器方法利用输入重构误差作为新颖性与正常性的度量来解决这个问题。我们将这种方法的本质表述为具有对条件数据不确定性的代理查询的四元组域转换，其有内在偏见。因此，一种改进方向被形式化为最大压缩自编码器的潜空间，同时确保其重构能力，以充当所描述的域转换器。从中，引入了策略，包括语义重构、数据确定性分解和标准化L2距离，以实质性改善原始方法，这些方法共同在各种基准测试中建立了最先进的性能，例如，在Wide-ResNet上，CIFAR-100与TinyImagenet-crop的FPR@95%TPR为0.2%。重要的是，我们的方法不需要任何额外的标记外样本数据。

    In some scenarios, classifier requires detecting out-of-distribution samples far from its training data. With desirable characteristics, reconstruction autoencoder-based methods deal with this problem by using input reconstruction error as a metric of novelty vs. normality. We formulate the essence of such approach as a quadruplet domain translation with an intrinsic bias to only query for a proxy of conditional data uncertainty. Accordingly, an improvement direction is formalized as maximumly compressing the autoencoder's latent space while ensuring its reconstructive power for acting as a described domain translator. From it, strategies are introduced including semantic reconstruction, data certainty decomposition and normalized L2 distance to substantially improve original methods, which together establish state-of-the-art performance on various benchmarks, e.g., the FPR@95%TPR of CIFAR-100 vs. TinyImagenet-crop on Wide-ResNet is 0.2%. Importantly, our method works without any addit
    
[^168]: 鲁棒PAC$^m$: 在模型规格不准确和存在异常值情况下训练集成模型

    Robust PAC$^m$: Training Ensemble Models Under Model Misspecification and Outliers. (arXiv:2203.01859v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2203.01859](http://arxiv.org/abs/2203.01859)

    对于存在模型规格不准确和异常值情况下的集成学习，本文提出了一个新的鲁棒自由能量准则，通过将广义对数得分函数与PAC$^m$结合，实现了更好的模型性能。

    

    传统的贝叶斯学习在模型规格不准确和存在异常值的情况下已知存在泛化能力的不足。PAC-Bayes理论证明了贝叶斯学习所最小化的自由能量准则是在假设未被异常值污染的采样分布下，对Gibbs预测器（即从后验随机抽取的单个模型）的泛化误差的一个上界。该观点提供了贝叶斯学习在模型规格不准确且需要集成，以及数据受到异常值影响时的局限性的证明。最近的工作中，推导出了PAC-Bayes上界 - 称为PAC$^m$ - 引入了自由能量度量，可考虑集合预测器的性能，从而获得在模型不准确的情况下提高模型性能。本文提出了一种新的鲁棒自由能量准则，将广义对数得分函数与PAC$^m$集成上界相结合。建议的自由能量训练...（摘要未完，详情请查看原文）

    Standard Bayesian learning is known to have suboptimal generalization capabilities under model misspecification and in the presence of outliers. PAC-Bayes theory demonstrates that the free energy criterion minimized by Bayesian learning is a bound on the generalization error for Gibbs predictors (i.e., for single models drawn at random from the posterior) under the assumption of sampling distributions uncontaminated by outliers. This viewpoint provides a justification for the limitations of Bayesian learning when the model is misspecified, requiring ensembling, and when data is affected by outliers. In recent work, PAC-Bayes bounds - referred to as PAC$^m$ - were derived to introduce free energy metrics that account for the performance of ensemble predictors, obtaining enhanced performance under misspecification. This work presents a novel robust free energy criterion that combines the generalized logarithm score function with PAC$^m$ ensemble bounds. The proposed free energy training 
    
[^169]: FedREP：面向零售能源供应商的水平联邦负荷预测

    FedREP: Towards Horizontal Federated Load Forecasting for Retail Energy Providers. (arXiv:2203.00219v2 [cs.DC] UPDATED)

    [http://arxiv.org/abs/2203.00219](http://arxiv.org/abs/2203.00219)

    FedREP 提出了一种新颖的水平隐私保护联邦学习框架，使用多个 REP 构建一个共同的、强大的机器学习模型来实现能源负载消耗预测。

    

    随着智能电表收集和传输家庭能源消耗数据给零售能源供应商（REP），最大的挑战是确保有效利用细粒度的消费者数据的同时保护数据隐私。本文针对能源负载消耗预测，解决了这一问题，这对于能源需求管理、负载切换和基础设施发展非常重要。具体而言，现有的能源负载预测是集中的，不可扩展，最重要的是容易遭受数据隐私威胁。此外，REP 是各自的市场参与者，并有责任保护自己客户的隐私。为解决这个问题，我们提出了一种新颖的水平隐私保护联邦学习框架，即 FedREP。我们考虑了一个联邦学习系统，由一个控制中心和多个零售商组成，通过启用多个 REP 构建一个共同的、强大的机器学习模型来解决问题。

    As Smart Meters are collecting and transmitting household energy consumption data to Retail Energy Providers (REP), the main challenge is to ensure the effective use of fine-grained consumer data while ensuring data privacy. In this manuscript, we tackle this challenge for energy load consumption forecasting in regards to REPs which is essential to energy demand management, load switching and infrastructure development. Specifically, we note that existing energy load forecasting is centralized, which are not scalable and most importantly, vulnerable to data privacy threats. Besides, REPs are individual market participants and liable to ensure the privacy of their own customers. To address this issue, we propose a novel horizontal privacy-preserving federated learning framework for REPs energy load forecasting, namely FedREP. We consider a federated learning system consisting of a control centre and multiple retailers by enabling multiple REPs to build a common, robust machine learning 
    
[^170]: 三维物体检测和定位中的LiDAR光束配置端到端优化

    End-To-End Optimization of LiDAR Beam Configuration for 3D Object Detection and Localization. (arXiv:2201.03860v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2201.03860](http://arxiv.org/abs/2201.03860)

    本论文提出了一种基于强化学习的学习-优化框架来自动化优化LiDAR光束配置以提高三维物体检测和定位的性能和降低计算成本。

    

    现有的基于LiDAR的应用学习方法使用在预先确定的光束配置下扫描的三维点云，例如，光束的仰角通常是均匀分布的。这些固定配置是面向任务的，因此简单地使用它们可能会导致次优性能。在这项工作中，我们采取了一种新路线，学习如何针对给定的应用程序优化LiDAR光束配置。具体来说，我们提出了一个基于强化学习的学习-优化（RL-L2O）框架，以自动端到端地优化不同基于LiDAR的应用程序的光束配置。优化是由目标任务的最终性能指导的，因此我们的方法可以轻松地作为简单的插入式模块集成到任何基于LiDAR的应用程序中。当需要低分辨率（低成本）的LiDAR时，该方法尤其有用，例如，用于大规模系统部署。我们使用该方法搜索了低分辨率LiDAR的光束配置，用于三维物体检测和定位。实验结果表明，优化的光束配置可以显著提高检测性能，并降低计算成本。

    Existing learning methods for LiDAR-based applications use 3D points scanned under a pre-determined beam configuration, e.g., the elevation angles of beams are often evenly distributed. Those fixed configurations are task-agnostic, so simply using them can lead to sub-optimal performance. In this work, we take a new route to learn to optimize the LiDAR beam configuration for a given application. Specifically, we propose a reinforcement learning-based learning-to-optimize (RL-L2O) framework to automatically optimize the beam configuration in an end-to-end manner for different LiDAR-based applications. The optimization is guided by the final performance of the target task and thus our method can be integrated easily with any LiDAR-based application as a simple drop-in module. The method is especially useful when a low-resolution (low-cost) LiDAR is needed, for instance, for system deployment at a massive scale. We use our method to search for the beam configuration of a low-resolution Li
    
[^171]: 基于注意力机制和子词分割的混合语言仇恨言论检测方法

    AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection. (arXiv:2112.11479v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2112.11479](http://arxiv.org/abs/2112.11479)

    AtteSTNet是一种基于注意力机制和子词分割的检测混合语言仇恨言论的方法，它不仅与复杂网络相当，而且在各种数据集上性能更好，其极大的简单性和易于维护性是其优点。

    

    技术的最新进展导致社交媒体的使用量增加，也导致大量用户生成的数据，其中包括令人讨厌和冒犯的言论。社交媒体上使用的语言通常是英语和育地方语言的组合。在印度，印地语是主要使用的语言，并经常与英语切换，形成印地英语（Hinglish）语言。过去已经采用了不同的机器学习和深度学习技术来对混合时的印地英语仇恨言论进行分类。然而，这些技术使用的循环或卷积机制计算成本高，内存需求大。过去的技术还使用复杂的数据处理方法，使现有技术非常复杂且难以改变数据。提出了一种更简单的方法，不仅与这些复杂网络一样，并且在如HASOC（印欧语言的仇恨言论和冒犯内容识别）此类混合印地英语文本的数据集上超过了性能基准。所提出的方法名为AtteSTNet，它利用注意力机制和子词分割来识别混合语言中的仇恨言论。所提出的方法比以前的技术表现更好，更简单易于维护。

    Recent advancements in technology have led to a boost in social media usage which has ultimately led to large amounts of user-generated data which also includes hateful and offensive speech. The language used in social media is often a combination of English and the native language in the region. In India, Hindi is used predominantly and is often code-switched with English, giving rise to the Hinglish (Hindi+English) language. Various approaches have been made in the past to classify the code-mixed Hinglish hate speech using different machine learning and deep learning-based techniques. However, these techniques make use of recurrence on convolution mechanisms which are computationally expensive and have high memory requirements. Past techniques also make use of complex data processing making the existing techniques very complex and non-sustainable to change in data. Proposed work gives a much simpler approach which is not only at par with these complex networks but also exceeds perfor
    
[^172]: CoReS: 基于平稳性的兼容表示方法

    CoReS: Compatible Representations via Stationarity. (arXiv:2111.07632v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2111.07632](http://arxiv.org/abs/2111.07632)

    CoReS是一种基于平稳性的方法，可以学习内部特征表示模型，并使其与之前学习的模型“兼容”。这使得在逐步升级表示模型时，无需为所有之前看到的图像提取新的特征，从而大大降低了成本。

    

    本文提出了一种新方法来学习内部特征表示模型，使其与先前学习的模型“兼容”。兼容特征使旧和新的特征可以直接比较，允许它们在时间上互换使用，从而消除了在逐步升级表示模型时，在图库中提取所有先前看到的图像的新特征的需要，这通常在庞大的图库集和/或实时系统中非常昂贵或不可行（即人脸识别系统、社交网络、终身学习系统、机器人和监控系统）。我们的方法名为基于平稳性的兼容表示（CoReS），通过鼓励表示模型的平稳性来实现兼容性，而不依赖于先前学习的模型。平稳性使特征的统计特性在时间偏移下不发生变化。

    In this paper, we propose a novel method to learn internal feature representation models that are \textit{compatible} with previously learned ones. Compatible features enable for direct comparison of old and new learned features, allowing them to be used interchangeably over time. This eliminates the need for visual search systems to extract new features for all previously seen images in the gallery-set when sequentially upgrading the representation model. Extracting new features is typically quite expensive or infeasible in the case of very large gallery-sets and/or real time systems (i.e., face-recognition systems, social networks, life-long learning systems, robotics and surveillance systems). Our approach, called Compatible Representations via Stationarity (CoReS), achieves compatibility by encouraging stationarity to the learned representation model without relying on previously learned models. Stationarity allows features' statistical properties not to change under time shift so 
    
[^173]: 从块-Toeplitz矩阵到图上的微分方程：迈向可扩展的Masked Transformers的通用理论

    From block-Toeplitz matrices to differential equations on graphs: towards a general theory for scalable masked Transformers. (arXiv:2107.07999v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.07999](http://arxiv.org/abs/2107.07999)

    本文提出了可扩展的方法将各种掩码机制纳入Transformers架构中。通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，提出了高效的d维RPE掩码和图内核掩码。该方法得到了实验证明。

    

    本文提供了迄今为止最全面的方法，以可扩展的方式将各种掩码机制纳入Transformers架构中。我们展示了最近关于线性因果注意力（Choromanski等人，2021）和对数-线性RPE-注意力（Luo等人，2021）的结果是这种一般机制的特例。然而，通过将问题转化为未屏蔽的注意力的拓扑（基于图形）调制，我们获得了几个以前未知的结果，包括高效的d维RPE掩码和图内核掩码。我们利用许多数学技术，从谱分析、动态规划和随机游走到解决图上马尔可夫过程的新算法。我们提供了相应的实证评估。

    In this paper we provide, to the best of our knowledge, the first comprehensive approach for incorporating various masking mechanisms into Transformers architectures in a scalable way. We show that recent results on linear causal attention (Choromanski et al., 2021) and log-linear RPE-attention (Luo et al., 2021) are special cases of this general mechanism. However by casting the problem as a topological (graph-based) modulation of unmasked attention, we obtain several results unknown before, including efficient d-dimensional RPE-masking and graph-kernel masking. We leverage many mathematical techniques ranging from spectral analysis through dynamic programming and random walks to new algorithms for solving Markov processes on graphs. We provide a corresponding empirical evaluation.
    
[^174]: 数据饮食上的深度学习：在训练早期找到重要的示例

    Deep Learning on a Data Diet: Finding Important Examples Early in Training. (arXiv:2107.07075v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.07075](http://arxiv.org/abs/2107.07075)

    本文旨在找到深度学习中训练数据集中的重要示例，提出了两种基于标准视觉数据集的简单分数。在修剪大量训练数据的同时，不牺牲测试准确性，EL2N分数在几个时期的训练中能够修剪CIFAR10训练集的一半。

    

    深度学习的最近成功部分地来自于在越来越大的数据集上训练过度参数化的网络。因此，自然而然地会问：有多少数据是多余的，哪些示例对于推广是重要的，如何找到它们？在本文中，我们做出了惊人的观察：在标准视觉数据集中，基于多个权重初始化的简单分数可以用于非常早期地识别重要的示例。我们提出了两个这样的分数——梯度归一化（GraNd）和误差L2范数（EL2N）分数——并通过在一系列体系结构和数据集上修剪大量的训练数据而没有牺牲测试准确性，证明了它们的功效。实际上，使用在几个时期中计算的EL2N分数，我们可以修剪CIFAR10训练数据集的一半，同时略微提高测试准确性。此外，对于给定的数据集，来自一种体系结构或超参数配置的EL2N分数普遍适用。

    Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superfluous, which examples are important for generalization, and how do we find them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight initializations can be used to identify important examples very early in training. We propose two such scores -- the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores -- and demonstrate their efficacy on a range of architectures and datasets by pruning significant fractions of training data without sacrificing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy. Furthermore, for a given dataset, EL2N scores from one architecture or hyperparameter configuration generaliz
    
[^175]: 拒绝性深度集成是贝叶斯的

    Repulsive Deep Ensembles are Bayesian. (arXiv:2106.11642v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.11642](http://arxiv.org/abs/2106.11642)

    通过在深度集成的更新规则中引入核化排斥项，可以强制并维护成员之间的多样性，并使集成具有更好的性能表现和不确定性估计。

    

    深度集成因其概念上的简单和高效而受到深度学习界的欢迎。然而，对于独立使用梯度下降训练的集成成员之间的功能多样性的维护是具有挑战性的。这可能会导致添加更多集成成员时出现病态，例如集成性能的饱和，它会收敛到单个模型的性能。此外，这不仅影响其预测的质量，而且更加影响集成的不确定性估计，从而影响其在超出分布数据上的性能。我们假设这种限制可以通过阻止不同集成成员坍塌到相同功能来克服。为此，我们在深度集成的更新规则中引入了一个核化的排斥术语。我们表明，这个简单的修改不仅强制并维护成员之间的多样性，而且更重要的是，将最大的后验值转化为...（原文截止此处）

    Deep ensembles have recently gained popularity in the deep learning community for their conceptual simplicity and efficiency. However, maintaining functional diversity between ensemble members that are independently trained with gradient descent is challenging. This can lead to pathologies when adding more ensemble members, such as a saturation of the ensemble performance, which converges to the performance of a single model. Moreover, this does not only affect the quality of its predictions, but even more so the uncertainty estimates of the ensemble, and thus its performance on out-of-distribution data. We hypothesize that this limitation can be overcome by discouraging different ensemble members from collapsing to the same function. To this end, we introduce a kernelized repulsive term in the update rule of the deep ensembles. We show that this simple modification not only enforces and maintains diversity among the members but, even more importantly, transforms the maximum a posterio
    
[^176]: NoiseGrad：通过引入模型权重的随机变化来增强解释性

    NoiseGrad: Enhancing Explanations by Introducing Stochasticity to Model Weights. (arXiv:2106.10185v3 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2106.10185](http://arxiv.org/abs/2106.10185)

    本文提出了NoiseGrad方法，通过引入模型权重的随机变化扰动决策边界来增强深度神经网络模型的局部和全局解释方法。

    

    近年来，针对黑匣子学习机的决策过程，如深度神经网络，已经进行了很多工作，从而产生了有用的局部和全局解释方法。本文提出了NoiseGrad，通过在权重参数空间中引入随机性，从而扰动决策边界，增强了局部和全局解释方法。我们将NoiseGrad与其与SmoothGrad的融合方法（FusionGrad）进行了定量和定性评估，并展示了其性能。

    Many efforts have been made for revealing the decision-making process of black-box learning machines such as deep neural networks, resulting in useful local and global explanation methods. For local explanation, stochasticity is known to help: a simple method, called SmoothGrad, has improved the visual quality of gradient-based attribution by adding noise to the input space and averaging the explanations of the noisy inputs. In this paper, we extend this idea and propose NoiseGrad that enhances both local and global explanation methods. Specifically, NoiseGrad introduces stochasticity in the weight parameter space, such that the decision boundary is perturbed. NoiseGrad is expected to enhance the local explanation, similarly to SmoothGrad, due to the dual relationship between the input perturbation and the decision boundary perturbation. We evaluate NoiseGrad and its fusion with SmoothGrad -FusionGrad -- qualitatively and quantitatively with several evaluation criteria, and show that
    
[^177]: 使用深度生成模型的似然方法进行奇异分布的非参数估计

    A likelihood approach to nonparametric estimation of a singular distribution using deep generative models. (arXiv:2105.04046v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2105.04046](http://arxiv.org/abs/2105.04046)

    本文研究了使用深度生成模型进行奇异分布非参数估计的统计学性质，提出了通过样本噪声对数据进行扰动的解决方案，从而实现对潜在分布的一致估计和良好收敛率。

    

    本文研究了使用深度生成模型进行奇异分布非参数估计的统计学性质。在考虑的模型中，利用深度生成模型建模高维数据，并假设数据集中在一些低维结构周围。由于支撑低维结构的分布在环境空间中对勒贝格测度具有奇异性，因此用通常的似然方法来估计目标分布可能无法达到一致性。本文证明了使用样本噪声对数据进行扰动可以得到新颖而有效的解决方案，从而实现对潜在分布的一致估计和良好收敛率。此外，本文还表征了可以通过深度生成模型有效估计的分布类。这个类足够普遍，可以容纳各种类型的分布。

    We investigate statistical properties of a likelihood approach to nonparametric estimation of a singular distribution using deep generative models. More specifically, a deep generative model is used to model high-dimensional data that are assumed to concentrate around some low-dimensional structure. Estimating the distribution supported on this low-dimensional structure, such as a low-dimensional manifold, is challenging due to its singularity with respect to the Lebesgue measure in the ambient space. In the considered model, a usual likelihood approach can fail to estimate the target distribution consistently due to the singularity. We prove that a novel and effective solution exists by perturbing the data with an instance noise, which leads to consistent estimation of the underlying distribution with desirable convergence rates. We also characterize the class of distributions that can be efficiently estimated via deep generative models. This class is sufficiently general to contain v
    
[^178]: 关于观测预测中时态图表示和静态图表示的等价性

    On the Equivalence Between Temporal and Static Graph Representations for Observational Predictions. (arXiv:2103.07016v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2103.07016](http://arxiv.org/abs/2103.07016)

    本研究探讨了时间和图、时间然后图等两种不同的节点表示方法，证明了使用不是最具表现力的组件GNN时，时间然后图具有比时间和图更强的表达能力。

    

    本研究从等变表示学习的角度，正式化了预测时间图中节点属性演化的相关任务。我们展示了时间图中的节点表示可以分为两种不同的框架：(a)最流行的方法，我们将其称为时间和图，其中等变图(如GNN)和序列(如RNN)表示相互交织，以表示图中节点属性的时间演化；(b)一种我们称之为时间然后图的方法，首先表示描述节点和边动态的序列，然后将其作为节点和边属性馈送到静态等变图表示中。有趣的是，我们展示当两种方法使用不是最具表现力的组件GNN (如1-Weisfeiler-Lehman GNNs)时，时间然后图表示具有比时间和图表示更强的表达能力。此外，尽管我们的目标不一定是获得最佳预测效果，我们证明了时间然后图表示可以从静态图中学习等变表示，从而提高了计算效率。

    This work formalizes the associational task of predicting node attribute evolution in temporal graphs from the perspective of learning equivariant representations. We show that node representations in temporal graphs can be cast into two distinct frameworks: (a) The most popular approach, which we denote as time-and-graph, where equivariant graph (e.g., GNN) and sequence (e.g., RNN) representations are intertwined to represent the temporal evolution of node attributes in the graph; and (b) an approach that we denote as time-then-graph, where the sequences describing the node and edge dynamics are represented first, then fed as node and edge attributes into a static equivariant graph representation that comes after. Interestingly, we show that time-then-graph representations have an expressivity advantage over time-and-graph representations when both use component GNNs that are not most-expressive (e.g., 1-Weisfeiler-Lehman GNNs). Moreover, while our goal is not necessarily to obtain st
    
[^179]: 迭代标签清洗用于跨领域少样本学习的半监督算法

    Iterative label cleaning for transductive and semi-supervised few-shot learning. (arXiv:2012.07962v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2012.07962](http://arxiv.org/abs/2012.07962)

    该论文提出了一种利用标记和未标记的数据分布的流形结构来预测伪标签，在类别平衡和选择干净标签的基础上，通过迭代清洗标签以提高伪标签质量的算法，在跨领域少样本学习中表现良好，并在四个基准数据集上达到了现有技术水平的最佳效果。

    

    少样本学习涉及到学习表征和获取知识，以使新任务可以在监督和数据都很有限的情况下得到解决。通过横向推断和半监督学习，可以提高性能。我们专注于这两种情况，提出了一种新的算法，利用标记和未标记的数据分布的流形结构来预测伪标签，同时平衡类别并使用有限容量分类器的损失值分布来选择最干净的标签，迭代地提高伪标签的质量。我们的解决方案在四个基准数据集（即 miniImageNet、tieredImageNet、CUB 和 CIFAR-FS）上超过或匹配了现有技术水平，同时在特征空间预处理和可用数据的数量方面具有鲁棒性。公开可用的源代码可在 https://github.c 中找到。

    Few-shot learning amounts to learning representations and acquiring knowledge such that novel tasks may be solved with both supervision and data being limited. Improved performance is possible by transductive inference, where the entire test set is available concurrently, and semi-supervised learning, where more unlabeled data is available. Focusing on these two settings, we introduce a new algorithm that leverages the manifold structure of the labeled and unlabeled data distribution to predict pseudo-labels, while balancing over classes and using the loss value distribution of a limited-capacity classifier to select the cleanest labels, iteratively improving the quality of pseudo-labels. Our solution surpasses or matches the state of the art results on four benchmark datasets, namely miniImageNet, tieredImageNet, CUB and CIFAR-FS, while being robust over feature space pre-processing and the quantity of available data. The publicly available source code can be found in https://github.c
    
[^180]: 带有对比学习的边缘引导 GAN 用于语义图像合成

    Edge Guided GANs with Contrastive Learning for Semantic Image Synthesis. (arXiv:2003.13898v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2003.13898](http://arxiv.org/abs/2003.13898)

    本文提出一种新颖的 ECGAN 用于语义图像合成任务，使用边缘作为中间表示，并且采用对比学习来处理局部和全局的语义信息，从而提高了合成图像的质量。

    

    我们提出了一种新颖的 ECGAN 用于挑战性的语义图像合成任务。虽然已经取得了相当大的进展，但合成图像的质量仍然不尽如人意，原因有三个：1）语义标签没有提供详细的结构信息，使得合成局部细节和结构变得困难；2）广泛采用的 CNN 操作，如卷积、下采样和归一化通常会导致空间分辨率的损失，不能完全保留原始的语义信息，导致语义不一致的结果；3）现有的语义图像合成方法主要关注从单个输入语义布局建模局部语义信息。然而，它们忽略了多个输入语义布局的全局语义信息，即跨不同输入布局的像素之间的语义交叉关系。为了解决第一个问题，我们提出使用边缘作为中间表示，并进一步采用其来引导图像合成。

    We propose a novel ECGAN for the challenging semantic image synthesis task. Although considerable improvement has been achieved, the quality of synthesized images is far from satisfactory due to three largely unresolved challenges. 1) The semantic labels do not provide detailed structural information, making it difficult to synthesize local details and structures. 2) The widely adopted CNN operations such as convolution, down-sampling, and normalization usually cause spatial resolution loss and thus cannot fully preserve the original semantic information, leading to semantically inconsistent results. 3) Existing semantic image synthesis methods focus on modeling local semantic information from a single input semantic layout. However, they ignore global semantic information of multiple input semantic layouts, i.e., semantic cross-relations between pixels across different input layouts. To tackle 1), we propose to use edge as an intermediate representation which is further adopted to gui
    
[^181]: 基于核化 SVM 的排名问题非线性分类器

    Nonlinear classifiers for ranking problems based on kernelized SVM. (arXiv:2002.11436v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2002.11436](http://arxiv.org/abs/2002.11436)

    本文提出了一种基于核化 SVM 的非线性分类器，用于解决最高相关性样本的排名问题。

    

    许多分类问题仅关注最具相关性的样本的性能而非所有样本。例如，排名问题、顶部准确度或搜索引擎仅关注前几个查询的结果。我们之前已经推导出包括多个线性分类问题类别的通用框架。本文将该框架扩展到非线性分类器。利用 SVM 的相似性，我们对问题进行对偶化处理，添加了核函数，并提出了一个分量对偶上升方法。

    Many classification problems focus on maximizing the performance only on the samples with the highest relevance instead of all samples. As an example, we can mention ranking problems, accuracy at the top or search engines where only the top few queries matter. In our previous work, we derived a general framework including several classes of these linear classification problems. In this paper, we extend the framework to nonlinear classifiers. Utilizing a similarity to SVM, we dualize the problems, add kernels and propose a componentwise dual ascent method.
    

