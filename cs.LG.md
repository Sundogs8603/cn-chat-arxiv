# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates](https://rss.arxiv.org/abs/2402.01493) | 这种方法提出了一种新的蒙特卡罗方法，使用球谐函数作为控制变量来近似计算切片华瑟斯坦距离。与蒙特卡罗相比，该方法具有更好的收敛速度和理论性质。 |
| [^2] | [Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach](https://rss.arxiv.org/abs/2402.01454) | 本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。 |
| [^3] | [Efficient Pruning of Large Language Model with Adaptive Estimation Fusion](https://arxiv.org/abs/2403.10799) | 提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。 |
| [^4] | [A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning](https://arxiv.org/abs/2403.09499) | 该研究提出了一种基于Q学习的算法，以实现在奶牛养殖中整合可再生能源，以改善电池管理，应对电能消耗波动和能源价格波动的挑战。 |
| [^5] | [3D Diffusion Policy](https://arxiv.org/abs/2403.03954) | 3D扩散策略（DP3）是一种新颖的视觉模仿学习方法，通过将3D视觉表示的强大性结合到扩散策略中，成功解决了学习复杂技能所需大量人类演示的问题。 |
| [^6] | [Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy](https://arxiv.org/abs/2403.01218) | 论文讨论了针对反学习环境的成员推断攻击的调整，并提出了现有U-MIA的分类，对每个示例实例化了专用攻击者。 |
| [^7] | [Fraud Detection with Binding Global and Local Relational Interaction](https://arxiv.org/abs/2402.17472) | 这项工作提出了一个名为RAGFormer的新框架，同时将局部和全局特征嵌入目标节点，以改进欺诈检测性能。 |
| [^8] | [Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model](https://arxiv.org/abs/2402.14035) | 将基础模型的知识转移到专用应用模型中存在挑战，提出了通过创建教学委员会来应对这些挑战。 |
| [^9] | [Conformalized Adaptive Forecasting of Heterogeneous Trajectories](https://arxiv.org/abs/2402.09623) | 本研究提出了一种新的符合性方法，通过结合在线符合性预测技术和解决回归中异方差性的方法，生成了同时预测边界，并能够可靠地覆盖新随机轨迹的整个路径。这种方法不仅有精确的有限样本保证，而且往往比之前的方法具有更丰富的预测结果。 |
| [^10] | [Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation](https://arxiv.org/abs/2402.07808) | 本论文提出了一种基于样本的最大熵源分布估计方法，通过优先保留不确定性来选择合适的源分布。该方法利用切片-瓦石坦斯坦距离对数据集和模拟进行衡量，适用于具有难以处理似然函数的模拟器。实验表明，该方法可以恢复更高熵的源分布，同时保持模拟的准确性。 |
| [^11] | [A Resource Model For Neural Scaling Law](https://arxiv.org/abs/2402.05164) | 该论文介绍了神经缩放律的资源模型，通过观察实证发现，子任务的损失与分配的神经元成反比，复合任务中子任务获得的资源随模型变大而增长，保持资源比例不变。该模型可以用于预测复合任务的神经缩放律，并成功复制了Chinchilla模型的神经缩放律。该资源模型是表征和诊断神经网络的有用工具。 |
| [^12] | [BiLLM: Pushing the Limit of Post-Training Quantization for LLMs](https://arxiv.org/abs/2402.04291) | BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。 |
| [^13] | [Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization](https://arxiv.org/abs/2402.02746) | 标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。 |
| [^14] | [Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators](https://arxiv.org/abs/2402.02551) | 这篇论文提出了一种基于深度强化学习和鲁棒低级控制的机器人操作器的障碍物避障轨迹规划方法，该方法通过与环境的交互积极参与学习，绕过了计算复杂性，同时解决了非重复和随机的避障任务。 |
| [^15] | [LLM Voting: Human Choices and AI Collective Decision Making](https://arxiv.org/abs/2402.01766) | 本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。 |
| [^16] | [Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You](https://arxiv.org/abs/2401.16092) | 多语言文本到图像生成模型存在性别偏见；通过MAGBIG评估模型时，发现模型对不同语言具有重要差异；我们呼吁研究多语言模型领域消除性别偏见。 |
| [^17] | [Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes](https://arxiv.org/abs/2312.06353) | 本论文提出了一种名为FedKSeed的方法，使用零阶优化和有限的随机种子集合，实现了通信成本较低的联邦全参数调整。该方法使得在终端设备上可以进行亿级语言模型的联邦全参数调整，具有较高的性能表现。 |
| [^18] | [Transforming gradient-based techniques into interpretable methods.](http://arxiv.org/abs/2401.14434) | 本文提出了一种基于梯度的技术支持框架，通过建立区别来强调重要区域, 并减少图像噪音。实证调查表明这些区域在促进类别区分方面起关键作用。 |
| [^19] | [Efficient Constrained $k$-Center Clustering with Background Knowledge.](http://arxiv.org/abs/2401.12533) | 本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。 |
| [^20] | [ChatQA: Building GPT-4 Level Conversational QA Models.](http://arxiv.org/abs/2401.10225) | ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。 |
| [^21] | [Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers.](http://arxiv.org/abs/2401.01974) | 本文提出了一种框架来实现零样本组合视觉推理，通过引入抽象的空间和时间例程以及利用少量的nun |
| [^22] | [Optimal Multi-Distribution Learning.](http://arxiv.org/abs/2312.05134) | 本论文提出了一种最优化多分布学习的方法，通过自适应采样来实现数据高效的学习。针对Vapnik-Chervonenkis (VC)维数为d的假设类，算法可以生成一个ε-最优随机假设，并且样本复杂度与最佳下界保持一致。同时，该算法的思想和理论还被进一步扩展以适应Rademacher类。最终提出的算法是奥拉克尔高效的，仅访问假设类。 |
| [^23] | [Meta- (out-of-context) learning in neural networks.](http://arxiv.org/abs/2310.15047) | 该研究通过合成实验展示了一种称为元-超文本外语境学习（meta-OCL）的现象在神经网络中的存在。这种学习使神经网络能够更好地吸收广泛适用的语义内容，并在适当的情况下进行使用。研究者提出了关于元-超文本外语境学习产生的两种假设，并就未来AI系统的能力和潜在风险进行了讨论。 |
| [^24] | [MINDE: Mutual Information Neural Diffusion Estimation.](http://arxiv.org/abs/2310.09031) | MINDE是一种基于得分函数扩散模型的新方法，用于估计随机变量之间的互信息。该方法在准确性和自一致性测试方面优于其他文献中的主要替代方法。 |
| [^25] | [Stationarity without mean reversion: Improper Gaussian process regression and improper kernels.](http://arxiv.org/abs/2310.02877) | 本论文展示了使用具有无限方差的不恰当高斯过程先验来定义静止但不均值回归过程的可能性，并引入了一类特殊的不恰当核函数来实现此目的。 |
| [^26] | [Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences.](http://arxiv.org/abs/2308.14555) | 本文研究了循环神经网络在遍历数据序列上训练时的核极限，利用数学方法对其渐近特性进行了描述，并证明了RNN收敛到与随机代数方程的不动点耦合的无穷维ODE的解。这对于理解和改进循环神经网络具有重要意义。 |
| [^27] | [Easy attention: A simple self-attention mechanism for Transformers.](http://arxiv.org/abs/2308.12874) | 本论文提出了一种名为简易注意力的注意力机制，用于提高Transformer神经网络在混沌系统时间动态预测中的鲁棒性。该方法不依赖于键、查询和softmax，直接将注意力得分作为可学习参数。实验结果表明，该方法在重构和预测混沌系统的时间动态方面比传统的自注意机制和长短期记忆方法更具鲁棒性和简化性。 |
| [^28] | [Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes.](http://arxiv.org/abs/2308.11267) | 本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。 |
| [^29] | ["Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models.](http://arxiv.org/abs/2308.03825) | 本文对大规模语言模型中的越狱提示进行了特征化和评估研究。通过测量野外越狱提示的唯一特征和主要攻击策略，我们发现越狱提示越来越多地从公共平台转移到私人平台，给LLM供应商在主动检测方面带来了新的挑战。 |
| [^30] | [Synthetic Control Methods by Density Matching under Implicit Endogeneitiy.](http://arxiv.org/abs/2307.11127) | 本文提出了一种新型的合成对照方法，通过密度匹配来解决现有SCMs中的隐式内生性问题。该方法通过将经过处理单元的结果密度与未处理单元的密度进行加权平均来估计SC权重。 |
| [^31] | [On-the-fly machine learning for parametrization of the effective Hamiltonian.](http://arxiv.org/abs/2307.08929) | 本论文提出了一种基于贝叶斯线性回归的即时机器学习方法，用于参数化有效哈密顿量。该方法在各种系统中都可以得到准确的结果，包括以前的方法无法处理的复杂系统。 |
| [^32] | [Tangent Transformers for Composition, Privacy and Removal.](http://arxiv.org/abs/2307.08122) | 我们引入了一种切线注意微调方法（TAFT），通过线性化变压器的一阶泰勒展开来进行微调。该方法具有与原始非线性网络相当的性能，并在模型组合、并行训练、机器去除和差分隐私方面具有优势。 |
| [^33] | [Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things.](http://arxiv.org/abs/2306.13030) | 提出了一种在线自监督学习的机器学习入侵检测框架，能够实现完全在线的IDS，无需离线学习或人工干预，该框架能够快速适应流量的时变特性，消除了离线数据收集的需要，并避免了数据标记中的人为误差，具有准确率高、成本低等优势。 |
| [^34] | [Nonparametric regression using over-parameterized shallow ReLU neural networks.](http://arxiv.org/abs/2306.08321) | 本文表明，对于学习某些光滑函数类，具有适当权重限制或正则化的过参数化神经网络可以达到最小最优收敛速率。作者通过对过度参数化的神经网络进行了实验，成功证明在浅层ReLU神经网络中使用最小二乘估计值是最小化最优的。同时作者还得出了神经网络的本地Rademacher复杂度的一个新的上界。 |
| [^35] | [Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation.](http://arxiv.org/abs/2305.09651) | 本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。 |
| [^36] | [A vector quantized masked autoencoder for audiovisual speech emotion recognition.](http://arxiv.org/abs/2305.03568) | 本文提出了一种特别为音视频言语自监督表示学习设计的矢量量化MAE模型，采用了基于离散音频和视觉言语表示的自监督范式，并在标准情感音视频言语数据集上取得了较好的效果。 |
| [^37] | [Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts.](http://arxiv.org/abs/2304.03427) | 本文提出了一种基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型，可以自动纠正OCR输出中的噪声。 |
| [^38] | [Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions.](http://arxiv.org/abs/2302.10886) | 本文深入研究和描述神经网络实现的函数的Lipschitz行为，在多种设置下进行实证研究，并揭示了神经网络函数Lipschitz连续性的基本和有趣的特性，其中最引人注目的是在Lipschitz常数的上限和下限中识别出了明显的双下降趋势。 |
| [^39] | [Classification by sparse additive models.](http://arxiv.org/abs/2212.01792) | 这篇论文研究了非参数的稀疏加性模型用于分类，通过对分量系数施加稀疏组Lasso和稀疏组Slope型惩罚来设计分类器，实验证明了分类器在未知稀疏性和平滑性上的自适应性能。 |
| [^40] | [Polar Encoding: A Simple Baseline Approach for Classification with Missing Values.](http://arxiv.org/abs/2210.01905) | 极化编码是一种用于处理具有缺失值的分类问题的简单基线方法，它能保留缺失信息、无需插补，让决策树自由选择如何处理缺失值。 |
| [^41] | [Dual Correction Strategy for Ranking Distillation in Top-N Recommender System.](http://arxiv.org/abs/2109.03459) | 本文提出了一种双重修正策略（DCD），用于在推荐系统中更有效地将教师模型的排名信息转移到学生模型。这种方法不仅充分利用了学生模型的预测误差，还提供了更全面的视角，解决了松弛排名蒸馏方法的限制。 |

# 详细

[^1]: 使用球谐函数作为控制变量的切片华瑟斯坦估计

    Sliced-Wasserstein Estimation with Spherical Harmonics as Control Variates

    [https://rss.arxiv.org/abs/2402.01493](https://rss.arxiv.org/abs/2402.01493)

    这种方法提出了一种新的蒙特卡罗方法，使用球谐函数作为控制变量来近似计算切片华瑟斯坦距离。与蒙特卡罗相比，该方法具有更好的收敛速度和理论性质。

    

    切片华瑟斯坦（SW）距离是概率测度之间的华瑟斯坦距离的平均值，结果为相关的一维投影的华瑟斯坦距离。因此，SW距离可以写成对球面上均匀测度的积分，并且可以使用蒙特卡罗框架来计算SW距离。球谐函数是球面上的多项式，它们构成了球面上可积函数集合的正交基。将这两个事实结合在一起，提出了一种新的蒙特卡罗方法，称为球谐控制变量（SHCV），用于使用球谐函数作为控制变量近似计算SW距离。结果表明，该方法具有良好的理论性质，例如在变量之间存在一定形式的线性依赖时，混合高斯测度的无误差特性。此外，与蒙特卡罗相比，得到了更快的收敛速度。

    The Sliced-Wasserstein (SW) distance between probability measures is defined as the average of the Wasserstein distances resulting for the associated one-dimensional projections. As a consequence, the SW distance can be written as an integral with respect to the uniform measure on the sphere and the Monte Carlo framework can be employed for calculating the SW distance. Spherical harmonics are polynomials on the sphere that form an orthonormal basis of the set of square-integrable functions on the sphere. Putting these two facts together, a new Monte Carlo method, hereby referred to as Spherical Harmonics Control Variates (SHCV), is proposed for approximating the SW distance using spherical harmonics as control variates. The resulting approach is shown to have good theoretical properties, e.g., a no-error property for Gaussian measures under a certain form of linear dependency between the variables. Moreover, an improved rate of convergence, compared to Monte Carlo, is established for g
    
[^2]: 在因果发现中集成大型语言模型: 一种统计因果方法

    Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach

    [https://rss.arxiv.org/abs/2402.01454](https://rss.arxiv.org/abs/2402.01454)

    本文提出了一种在因果发现中集成大型语言模型的方法，通过将统计因果提示与知识增强相结合，可以使统计因果发现结果接近真实情况并进一步改进结果。

    

    在实际的统计因果发现（SCD）中，将领域专家知识作为约束嵌入到算法中被广泛接受，因为这对于创建一致有意义的因果模型是重要的，尽管识别背景知识的挑战被认可。为了克服这些挑战，本文提出了一种新的因果推断方法，即通过将LLM的“统计因果提示（SCP）”与SCD方法和基于知识的因果推断（KBCI）相结合，对SCD进行先验知识增强。实验证明，GPT-4可以使LLM-KBCI的输出与带有LLM-KBCI的先验知识的SCD结果接近真实情况，如果GPT-4经历了SCP，那么SCD的结果还可以进一步改善。而且，即使LLM不含有数据集的信息，LLM仍然可以通过其背景知识来改进SCD。

    In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
    
[^3]: 使用自适应估计融合高效剪枝大型语言模型

    Efficient Pruning of Large Language Model with Adaptive Estimation Fusion

    [https://arxiv.org/abs/2403.10799](https://arxiv.org/abs/2403.10799)

    提出了一种简单而高效的剪枝方法，能够自适应地模拟每个子结构的重要性，并根据多层结构的结果自适应地融合粗粒度和细粒度的估计。

    

    大型语言模型（LLMs）已经成为许多生成性下游任务中至关重要的组成部分，这导致在资源受限设备上高效部署它们成为不可避免的趋势和重大挑战。结构化剪枝是解决这一挑战的广泛应用方法。然而，当处理多个解码器层的复杂结构时，通常的方法往往采用常见的估计方法进行剪枝。这些方法导致特定下游任务精度下降。本文介绍了一种简单而有效的方法，可自适应地模拟每个子结构的重要性。同时，它可以基于复杂和多层结构的结果，自适应地融合粗粒度和细粒度的估计。我们设计的所有方面都无缝集成到端到端的剪枝框架中。与主流数据集上的最先进方法相比，我们的实验结果表明

    arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
    
[^4]: 使用Q学习的奶牛养殖场电池管理的强化学习方法

    A Reinforcement Learning Approach to Dairy Farm Battery Management using Q Learning

    [https://arxiv.org/abs/2403.09499](https://arxiv.org/abs/2403.09499)

    该研究提出了一种基于Q学习的算法，以实现在奶牛养殖中整合可再生能源，以改善电池管理，应对电能消耗波动和能源价格波动的挑战。

    

    奶牛养殖消耗大量能源，是农业中一个能源密集型的部门。将可再生能源集成到奶牛养殖中可以帮助应对这一挑战。有效的电池管理对于整合可再生能源发电至关重要。管理电池的充电和放电由于电能消耗的波动、可再生能源发电的间歇性以及能源价格的波动而面临重大挑战。人工智能（AI）有潜力显著改善奶牛养殖中可再生能源的利用，然而在这一特定领域中进行的研究有限。本研究以爱尔兰作为案例研究，以实现其以可再生能源利用为核心的2030年能源战略。这项研究提出了一种基于Q学习的算法，用于安排电池的充电和放电。

    arXiv:2403.09499v1 Announce Type: cross  Abstract: Dairy farming consumes a significant amount of energy, making it an energy-intensive sector within agriculture. Integrating renewable energy generation into dairy farming could help address this challenge. Effective battery management is important for integrating renewable energy generation. Managing battery charging and discharging poses significant challenges because of fluctuations in electrical consumption, the intermittent nature of renewable energy generation, and fluctuations in energy prices. Artificial Intelligence (AI) has the potential to significantly improve the use of renewable energy in dairy farming, however, there is limited research conducted in this particular domain. This research considers Ireland as a case study as it works towards attaining its 2030 energy strategy centered on the utilization of renewable sources. This study proposes a Q-learning-based algorithm for scheduling battery charging and discharging in 
    
[^5]: 3D扩散策略

    3D Diffusion Policy

    [https://arxiv.org/abs/2403.03954](https://arxiv.org/abs/2403.03954)

    3D扩散策略（DP3）是一种新颖的视觉模仿学习方法，通过将3D视觉表示的强大性结合到扩散策略中，成功解决了学习复杂技能所需大量人类演示的问题。

    

    模仿学习为教授机器人灵巧技能提供了一种高效的方式；然而，学习复杂而具有通用性的技能通常需要大量的人类演示。为了解决这一具有挑战性的问题，我们提出了3D扩散策略（DP3），这是一种将3D视觉表示的强大性融入到扩散策略中的新颖视觉模仿学习方法，扩散策略是一类有条件的动作生成模型。DP3的核心设计是利用一个紧凑的3D视觉表示，该表示是从稀疏点云中提取出来的，使用高效的点编码器。在我们涵盖了72个仿真任务的实验中，DP3仅需要10个演示就可以成功处理大多数任务，并且比基线模型提高了55.3%。在4个真实机器人任务中，DP3表现出了高成功率的精确控制，每项任务仅需40次演示即可成功率为85%，在不同领域展现了出色的泛化能力。

    arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
    
[^6]: 粗糙反学习需要更加谨慎的评估以避免虚假隐私感知

    Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy

    [https://arxiv.org/abs/2403.01218](https://arxiv.org/abs/2403.01218)

    论文讨论了针对反学习环境的成员推断攻击的调整，并提出了现有U-MIA的分类，对每个示例实例化了专用攻击者。

    

    模型训练的高成本使得开发反学习技术变得越来越有吸引力。这些技术旨在删除训练样本的影响，而无需从头重新训练模型。从直觉上讲，一旦模型完成反学习，与该模型交互的对手就不应再能够判断反学习的样本是否包含在模型的训练集中。在隐私领域，这被称为成员推断。在这项工作中，我们讨论了成员推断攻击（MIAs）对反学习设置的调整（导致它们的“U-MIA”对应）。我们提出了现有U-MIA的分类，将其分为“人口U-MIA”，其中同一攻击者适用于所有示例，和“每个示例U-MIA”，其中为每个示例实例化了专用攻击者。我们展示了后一类别，在这种情况下，攻击者为每个实例定制其成员预测。

    arXiv:2403.01218v1 Announce Type: new  Abstract: The high cost of model training makes it increasingly desirable to develop techniques for unlearning. These techniques seek to remove the influence of a training example without having to retrain the model from scratch. Intuitively, once a model has unlearned, an adversary that interacts with the model should no longer be able to tell whether the unlearned example was included in the model's training set or not. In the privacy literature, this is known as membership inference. In this work, we discuss adaptations of Membership Inference Attacks (MIAs) to the setting of unlearning (leading to their ``U-MIA'' counterparts). We propose a categorization of existing U-MIAs into ``population U-MIAs'', where the same attacker is instantiated for all examples, and ``per-example U-MIAs'', where a dedicated attacker is instantiated for each example. We show that the latter category, wherein the attacker tailors its membership prediction to each ex
    
[^7]: 通过融合全局和局部关系交互进行欺诈检测

    Fraud Detection with Binding Global and Local Relational Interaction

    [https://arxiv.org/abs/2402.17472](https://arxiv.org/abs/2402.17472)

    这项工作提出了一个名为RAGFormer的新框架，同时将局部和全局特征嵌入目标节点，以改进欺诈检测性能。

    

    图神经网络已被证明对于欺诈检测具有有效性，因为它能够在整体视角中编码节点交互和聚合特征。最近，具有出色序列编码能力的Transformer网络在文献中也表现出优于其他基于GNN的方法。然而，基于GNN和基于Transformer的网络只编码整个图的一个视角，而GNN编码全局特征，Transformer网络编码局部特征。此外，先前的工作忽视了使用单独网络编码异构图的全局交互特征，导致性能不佳。在这项工作中，我们提出了一个称为Relation-Aware GNN with transFormer（RAGFormer）的新颖框架，将局部和全局特征同时嵌入目标节点中。这个简单而有效的网络应用了一个修改后的GAGA模块，其中每个Transformer层后面都跟着一个跨关系聚合层。

    arXiv:2402.17472v1 Announce Type: cross  Abstract: Graph Neural Network has been proved to be effective for fraud detection for its capability to encode node interaction and aggregate features in a holistic view. Recently, Transformer network with great sequence encoding ability, has also outperformed other GNN-based methods in literatures. However, both GNN-based and Transformer-based networks only encode one perspective of the whole graph, while GNN encodes global features and Transformer network encodes local ones. Furthermore, previous works ignored encoding global interaction features of the heterogeneous graph with separate networks, thus leading to suboptimal performance. In this work, we present a novel framework called Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds local and global features into a target node. The simple yet effective network applies a modified GAGA module where each transformer layer is followed by a cross-relation aggregation lay
    
[^8]: 委员会的智慧：从基础模型到专用应用模型的提取

    Wisdom of Committee: Distilling from Foundation Model to SpecializedApplication Model

    [https://arxiv.org/abs/2402.14035](https://arxiv.org/abs/2402.14035)

    将基础模型的知识转移到专用应用模型中存在挑战，提出了通过创建教学委员会来应对这些挑战。

    

    最近基础模型的进展在各种任务上取得了令人印象深刻的性能，与此同时，为特定应用，从业者们一直在开发专门的应用模型。为了享受这两种模型的好处，一个自然的路径是将基础模型中的知识转移到专用应用模型中，后者通常更有效地提供服务。知识蒸馏的技术可以在这里应用，其中应用模型学会模仿基础模型。然而，专用应用模型和基础模型在容量上存在实质性差距，采用不同的架构，使用来自不同模态的不同输入特征，并在不同的分布上进行优化。模型特征上的这些差异导致了蒸馏方法面临重大挑战。在这项工作中，我们提出创建一个教学委员会，包括基础模型和专用应用模型。

    arXiv:2402.14035v1 Announce Type: cross  Abstract: Recent advancements in foundation models have yielded impressive performance across a wide range of tasks. Meanwhile, for specific applications, practitioners have been developing specialized application models. To enjoy the benefits of both kinds of models, one natural path is to transfer the knowledge in foundation models into specialized application models, which are generally more efficient for serving. Techniques from knowledge distillation may be applied here, where the application model learns to mimic the foundation model. However, specialized application models and foundation models have substantial gaps in capacity, employing distinct architectures, using different input features from different modalities, and being optimized on different distributions. These differences in model characteristics lead to significant challenges for distillation methods. In this work, we propose creating a teaching committee comprising both foun
    
[^9]: 多元轨迹的符合性自适应预测方法

    Conformalized Adaptive Forecasting of Heterogeneous Trajectories

    [https://arxiv.org/abs/2402.09623](https://arxiv.org/abs/2402.09623)

    本研究提出了一种新的符合性方法，通过结合在线符合性预测技术和解决回归中异方差性的方法，生成了同时预测边界，并能够可靠地覆盖新随机轨迹的整个路径。这种方法不仅有精确的有限样本保证，而且往往比之前的方法具有更丰富的预测结果。

    

    本文提出了一种新的符合性方法，用于生成同时预测边界，以具有足够高的概率覆盖新随机轨迹的整个路径。鉴于在运动规划应用中需要可靠的不确定性估计，其中不同物体的行为可能更或更少可预测，我们将来自单个和多个时间序列的在线符合性预测技术，以及解决回归中的异方差性的方法进行了融合。该解决方案既有原则性，提供了精确的有限样本保证，又有效，通常比先前的方法具有更丰富的预测结果。

    arXiv:2402.09623v1 Announce Type: cross  Abstract: This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. Prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.
    
[^10]: Sourcerer: 基于样本的最大熵源分布估计

    Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation

    [https://arxiv.org/abs/2402.07808](https://arxiv.org/abs/2402.07808)

    本论文提出了一种基于样本的最大熵源分布估计方法，通过优先保留不确定性来选择合适的源分布。该方法利用切片-瓦石坦斯坦距离对数据集和模拟进行衡量，适用于具有难以处理似然函数的模拟器。实验表明，该方法可以恢复更高熵的源分布，同时保持模拟的准确性。

    

    科学建模应用通常需要估计与观测数据集一致的参数分布，被称为源分布估计的推理任务。然而，这个问题可能是不适定的，因为许多不同的源分布可能产生相同的数据分布一致的模拟结果。为了在众多同样有效的源中做出有原则的选择，我们提出了一种目标最大熵分布的方法，即优先保留尽可能多的不确定性。我们的方法完全基于样本，利用切片-瓦石坦斯坦距离来衡量数据集与模拟之间的差异，因此适用于具有难以处理的似然函数的模拟器。我们在几个任务上对我们的方法进行了基准测试，并表明它可以恢复具有更高熵的源分布，而不牺牲模拟的准确性。最后，为了证明我们方法的实用性，我们推断源分布...

    Scientific modeling applications often require estimating a distribution of parameters consistent with a dataset of observations - an inference task also known as source distribution estimation. This problem can be ill-posed, however, since many different source distributions might produce the same distribution of data-consistent simulations. To make a principled choice among many equally valid sources, we propose an approach which targets the maximum entropy distribution, i.e., prioritizes retaining as much uncertainty as possible. Our method is purely sample-based - leveraging the Sliced-Wasserstein distance to measure the discrepancy between the dataset and simulations - and thus suitable for simulators with intractable likelihoods. We benchmark our method on several tasks, and show that it can recover source distributions with substantially higher entropy without sacrificing the fidelity of the simulations. Finally, to demonstrate the utility of our approach, we infer source distri
    
[^11]: 神经缩放律的资源模型

    A Resource Model For Neural Scaling Law

    [https://arxiv.org/abs/2402.05164](https://arxiv.org/abs/2402.05164)

    该论文介绍了神经缩放律的资源模型，通过观察实证发现，子任务的损失与分配的神经元成反比，复合任务中子任务获得的资源随模型变大而增长，保持资源比例不变。该模型可以用于预测复合任务的神经缩放律，并成功复制了Chinchilla模型的神经缩放律。该资源模型是表征和诊断神经网络的有用工具。

    

    神经缩放律描述了随着模型规模的增大，模型性能如何提高。受实证观察启发，我们引入了神经缩放的资源模型。一个任务通常是复合任务，可以分解为许多子任务，这些子任务竞争资源（以分配给子任务的神经元数量来衡量）。在玩具问题上，我们经验证实：（1）子任务的损失与其分配的神经元成反比。（2）当复合任务中存在多个子任务时，随着模型变大，每个子任务获得的资源均匀增长，保持获得资源的比例不变。我们假设这些发现在一般情况下是有效的，并建立了一个模型来预测一般复合任务的神经缩放律，并成功复制了arXiv:2203.15556中报告的Chinchilla模型的神经缩放律。我们相信本文中使用的资源概念将是表征和诊断神经网络的有用工具。

    Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural 
    
[^12]: BiLLM: 推动LLMs的后训练量化极限

    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs

    [https://arxiv.org/abs/2402.04291](https://arxiv.org/abs/2402.04291)

    BiLLM是一种针对预训练LLMs的1位后训练量化方案，通过识别重要的权重和优化二值化，成功实现了高准确度的推理。

    

    预训练的大型语言模型（LLMs）具有出色的通用语言处理能力，但对内存和计算资源有很大的需求。作为一种强大的压缩技术，二值化可以将模型权重极大地减少到仅1位，降低了昂贵的计算和内存需求。然而，现有的量化技术在超低位宽下无法保持LLM的性能。针对这一挑战，我们提出了BiLLM，这是一种针对预训练LLM定制的开创性的1位后训练量化方案。基于LLMs的权重分布，BiLLM首先识别和结构选择重要的权重，并通过有效的二值化残差逼近策略来最小化压缩损失。此外，考虑到非重要权重的钟形分布，我们提出了一种最佳分割搜索方法，以准确地将它们分组和二值化。BiLLM首次实现了高准确度的推理。

    Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
    
[^13]: 标准 Gaussian 过程在高维贝叶斯优化中足以应对

    Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization

    [https://arxiv.org/abs/2402.02746](https://arxiv.org/abs/2402.02746)

    标准 Gaussian 过程在高维贝叶斯优化中表现优秀，经验证据显示其在函数估计和协方差建模中克服了高维输入困难，比专门为高维优化设计的方法表现更好。

    

    长期以来，人们普遍认为使用标准 Gaussian 过程（GP）进行贝叶斯优化（BO），即标准 BO，在高维优化问题中效果不佳。这种观念可以部分归因于 Gaussian 过程在协方差建模和函数估计中对高维输入的困难。虽然这些担忧看起来合理，但缺乏支持这种观点的经验证据。本文系统地研究了在各种合成和真实世界基准问题上，使用标准 GP 回归进行高维优化的贝叶斯优化。令人惊讶的是，标准 GP 的表现始终位于最佳范围内，往往比专门为高维优化设计的现有 BO 方法表现更好。与刻板印象相反，我们发现标准 GP 可以作为学习高维目标函数的能力强大的代理。在没有强结构假设的情况下，使用标准 GP 进行 BO 可以获得非常好的性能。

    There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
    
[^14]: 基于深度强化学习的障碍物避障轨迹规划器与鲁棒低级控制的机器人操作器

    Obstacle Avoidance Deep Reinforcement Learning-Based Trajectory Planner with Robust Low-Level Control for Robotic Manipulators

    [https://arxiv.org/abs/2402.02551](https://arxiv.org/abs/2402.02551)

    这篇论文提出了一种基于深度强化学习和鲁棒低级控制的机器人操作器的障碍物避障轨迹规划方法，该方法通过与环境的交互积极参与学习，绕过了计算复杂性，同时解决了非重复和随机的避障任务。

    

    在机器人领域，现代策略往往是基于学习的，其特点是黑盒性质复杂，缺乏解释性，可能在确保稳定性和安全性方面带来挑战。为了解决这些问题，我们提出了将无障碍深度强化学习（DRL）轨迹规划器与新颖的自动调谐低级和关节级控制策略集成在一起，并通过与环境的交互积极参与学习阶段。这种方法绕过了与计算相关的复杂性，同时解决了非重复和随机的避障任务。首先，利用无模型DRL代理在关节级推理任务空间中进行速度限制和无障碍运动规划，然后将该规划输入到稳健的子系统自适应控制器中，产生所需的扭矩，而杜鹃搜索优化（CSO）算法增强了控制增益以最小化。

    In robotics, contemporary strategies are learning-based, characterized by a complex black-box nature and a lack of interpretability, which may pose challenges in ensuring stability and safety. To address these issues, we propose integrating an obstacle-free deep reinforcement learning (DRL) trajectory planner with a novel auto-tuning low- and joint-level control strategy, all while actively engaging in the learning phase through interactions with the environment. This approach circumvents the complexities associated with computations while also addressing nonrepetitive and random obstacle avoidance tasks. First, a model-free DRL agent to plan velocity-bounded and obstacle-free motion is employed for a manipulator with 'n' degrees of freedom (DoF) in task space through joint-level reasoning. This plan is then input into a robust subsystem-based adaptive controller, which produces the necessary torques, while the Cuckoo Search Optimization (CSO) algorithm enhances control gains to minimi
    
[^15]: LLM投票：人类选择和AI集体决策

    LLM Voting: Human Choices and AI Collective Decision Making

    [https://arxiv.org/abs/2402.01766](https://arxiv.org/abs/2402.01766)

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并揭示了LLMs与人类在决策和偏见方面的差异。研究发现，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    

    本文研究了大型语言模型（LLMs），特别是OpenAI的GPT4和LLaMA2的投票行为，并与人类投票模式进行了对比。我们的方法包括进行人类投票实验以建立人类偏好的基准，并与LLM代理进行平行实验。研究聚焦于集体结果和个体偏好，揭示了人类和LLMs之间在决策和固有偏见方面的差异。我们观察到LLMs在偏好多样性和一致性之间存在权衡，相比人类选民的多样偏好，LLMs有更趋向于一致选择的倾向。这一发现表明，在投票辅助中使用LLMs可能会导致更同质化的集体结果，强调了谨慎将LLMs整合到民主过程中的必要性。

    This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
    
[^16]: 多语言文本到图像生成放大了性别刻板印象，并且修正工程可能无法帮助您

    Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You

    [https://arxiv.org/abs/2401.16092](https://arxiv.org/abs/2401.16092)

    多语言文本到图像生成模型存在性别偏见；通过MAGBIG评估模型时，发现模型对不同语言具有重要差异；我们呼吁研究多语言模型领域消除性别偏见。

    

    最近，文本到图像生成模型在图像质量、灵活性和文本对齐方面取得了令人惊讶的结果，并因此在越来越多的应用中得到应用。通过改善多语言能力，更多的社群现在可以访问这种技术。然而，正如我们将展示的那样，多语言模型与单语模型一样受到(性别)偏见的困扰。此外，人们自然期望这些模型在不同语言之间提供类似的结果，但事实并非如此，不同语言之间存在重要的差异。因此，我们提出了一个旨在促进没有性别偏见的多语言模型研究的新基准MAGBIG。我们研究了多语言T2I模型是否通过MAGBIG放大了性别偏见。为此，我们使用多语言提示请求特定职业或特质的人像图像(使用形容词)。我们的结果不仅表明模型偏离了规范的假设，...

    Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption th
    
[^17]: 使用通信成本低于18千字节的联邦全参数调整亿级语言模型

    Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes

    [https://arxiv.org/abs/2312.06353](https://arxiv.org/abs/2312.06353)

    本论文提出了一种名为FedKSeed的方法，使用零阶优化和有限的随机种子集合，实现了通信成本较低的联邦全参数调整。该方法使得在终端设备上可以进行亿级语言模型的联邦全参数调整，具有较高的性能表现。

    

    预训练的大型语言模型（LLM）需要通过细化调整来提高对自然语言指令的响应能力。联邦学习提供了一种在不牺牲数据隐私的情况下，利用终端设备上丰富的数据对LLM进行细化调整的方法。大多数现有的LLM联邦细化调整方法依赖于参数高效的细化调整技术，但可能无法达到全参数调整可能达到的性能高度。然而，由于巨大的通信成本，LLM的联邦全参数调整是一个非常困难的问题。本研究介绍了FedKSeed，它使用随机种子的有限集合进行零阶优化。它显著降低了服务器和终端之间的传输要求，仅需传输几个随机种子和标量梯度，仅占用几千字节的空间，使得在终端设备上能够进行亿级LLM的联邦全参数调整。在此基础上，我们开发了一种策略，实现了概率差异化种子采样，优先考虑一些种子，从而进一步提高了联邦全参数调整的效果。

    Pre-trained large language models (LLMs) need fine-tuning to improve their responsiveness to natural language instructions. Federated learning offers a way to fine-tune LLMs using the abundant data on end devices without compromising data privacy. Most existing federated fine-tuning methods for LLMs rely on parameter-efficient fine-tuning techniques, which may not reach the performance height possible with full-parameter tuning. However, federated full-parameter tuning of LLMs is a non-trivial problem due to the immense communication cost. This work introduces FedKSeed that employs zeroth-order optimization with a finite set of random seeds. It significantly reduces transmission requirements between the server and clients to just a few random seeds and scalar gradients, amounting to only a few thousand bytes, making federated full-parameter tuning of billion-sized LLMs possible on devices. Building on it, we develop a strategy enabling probability-differentiated seed sampling, prioriti
    
[^18]: 将基于梯度的技术转化为可解释的方法

    Transforming gradient-based techniques into interpretable methods. (arXiv:2401.14434v1 [cs.CV])

    [http://arxiv.org/abs/2401.14434](http://arxiv.org/abs/2401.14434)

    本文提出了一种基于梯度的技术支持框架，通过建立区别来强调重要区域, 并减少图像噪音。实证调查表明这些区域在促进类别区分方面起关键作用。

    

    通过xAI技术解释卷积神经网络（CNN）通常在解释上面临挑战。图像提取的像素等输入特征的内在复杂性引发了复杂的相关性。集成梯度（IG）等基于梯度的方法有效地展示了这些特征的重要性。然而，将这些解释转化为图像时常常产生大量噪音。本文引入了梯度人工分离（GAD）作为梯度基于技术的支持框架。其主要目标是通过建立类别之间的区别来强调有影响力的区域。GAD的核心是在可视化过程中限制分析范围，从而减少图像噪音。通过对被遮挡图像的实证调查，我们证明了通过这种方法确定的区域确实在促进类别区分方面发挥了关键作用。

    The explication of Convolutional Neural Networks (CNN) through xAI techniques often poses challenges in interpretation. The inherent complexity of input features, notably pixels extracted from images, engenders complex correlations. Gradient-based methodologies, exemplified by Integrated Gradients (IG), effectively demonstrate the significance of these features. Nevertheless, the conversion of these explanations into images frequently yields considerable noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a supportive framework for gradient-based techniques. Its primary objective is to accentuate influential regions by establishing distinctions between classes. The essence of GAD is to limit the scope of analysis during visualization and, consequently reduce image noise. Empirical investigations involving occluded images have demonstrated that the identified regions through this methodology indeed play a pivotal role in facilitating class differentiation.
    
[^19]: 有效利用背景知识的约束k中心聚类

    Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])

    [http://arxiv.org/abs/2401.12533](http://arxiv.org/abs/2401.12533)

    本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。

    

    中心为基础的聚类在理论和实践中都引起了重要的研究兴趣。在许多实际应用中，输入数据通常包含可以用于改进聚类结果的背景知识。在这项工作中，我们基于广泛采用的k中心聚类，并将其输入的背景知识建模为必连（ML）和不连（CL）约束集。然而，大多数包括k中心在内的聚类问题本质上都是NP困难的，而更复杂的受约束变体被认为受到更严重的近似和计算障碍的限制，极大地限制了它们的适用性。通过采用一系列技术，包括反支配集，线性规划（LP）整数平面和LP对偶性，我们得到了第一个具有最佳近似比例2的约束k中心的高效近似算法。我们还构建了竞争基准算法，并对我们的近似算法进行了实证评估。

    Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
    
[^20]: ChatQA: 构建GPT-4级对话问答模型

    ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])

    [http://arxiv.org/abs/2401.10225](http://arxiv.org/abs/2401.10225)

    ChatQA是一系列对话问答模型，可以达到GPT-4级别的准确性。通过两阶段的指令调整方法，可以显著提高大型语言模型在零-shot对话问答中的结果。使用密集检索器进行问答数据集的微调可以实现与最先进的查询重写模型相当的结果，同时降低部署成本。ChatQA-70B在10个对话问答数据集上的平均得分超过了GPT-4，且不依赖于任何来自OpenAI GPT模型的合成数据。

    

    在这项工作中，我们介绍了ChatQA，一系列具有GPT-4级别准确性的对话问答模型。具体地，我们提出了一个两阶段的指令调整方法，可以显著提高大型语言模型（LLM）在零-shot对话问答中的结果。为了处理对话问答中的检索问题，我们在多轮问答数据集上进行了密集检索器的微调，这样可以提供与使用最先进的查询重写模型相当的结果，同时大大降低部署成本。值得注意的是，我们的ChatQA-70B可以在10个对话问答数据集的平均分上超过GPT-4（54.14 vs. 53.90），而不依赖于OpenAI GPT模型的任何合成数据。

    In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
    
[^21]: 实现真正的零样本组合视觉推理：以LLMs为程序员

    Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers. (arXiv:2401.01974v1 [cs.CV])

    [http://arxiv.org/abs/2401.01974](http://arxiv.org/abs/2401.01974)

    本文提出了一种框架来实现零样本组合视觉推理，通过引入抽象的空间和时间例程以及利用少量的nun

    

    视觉推理主要采用端到端神经网络，拥有数十亿个模型参数和训练样本。然而，即使是最大的模型在组合推理、泛化、细粒度空间和时间推理以及计数方面也存在困难。在理论上，使用大型语言模型(LLMs)作为控制器进行视觉推理可以解决这些限制，通过将任务分解为子任务，并通过调度一组(视觉)工具来解决子任务。最近，这些模型在组合视觉问答、视觉 grounding 和视频的时间推理等任务上取得了很好的性能。然而，现有模型在当前形式下严重依赖于在提示中针对具体数据集和任务进行人工设计的上下文示例，这需要高技能程序员投入大量的劳动力。在这项工作中，我们提出了一个框架来缓解这些问题，通过引入具有空间和时间抽象的例程，并利用少量的nu

    Visual reasoning is dominated by end-to-end neural networks scaled to billions of model parameters and training examples. However, even the largest models struggle with compositional reasoning, generalization, fine-grained spatial and temporal reasoning, and counting. Visual reasoning with large language models (LLMs) as controllers can, in principle, address these limitations by decomposing the task and solving subtasks by orchestrating a set of (visual) tools. Recently, these models achieved great performance on tasks such as compositional visual question answering, visual grounding, and video temporal reasoning. Nevertheless, in their current form, these models heavily rely on human engineering of in-context examples in the prompt, which are often dataset- and task-specific and require significant labor by highly skilled programmers. In this work, we present a framework that mitigates these issues by introducing spatially and temporally abstract routines and by leveraging a small nu
    
[^22]: 最优化多分布学习

    Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.05134](http://arxiv.org/abs/2312.05134)

    本论文提出了一种最优化多分布学习的方法，通过自适应采样来实现数据高效的学习。针对Vapnik-Chervonenkis (VC)维数为d的假设类，算法可以生成一个ε-最优随机假设，并且样本复杂度与最佳下界保持一致。同时，该算法的思想和理论还被进一步扩展以适应Rademacher类。最终提出的算法是奥拉克尔高效的，仅访问假设类。

    

    多分布学习（MDL）旨在学习一个共享模型，使得在k个不同的数据分布下，最小化最坏情况风险，已成为适应健壮性、公平性、多组合作等需求的统一框架。实现数据高效的MDL需要在学习过程中进行自适应采样，也称为按需采样。然而，最优样本复杂度的上下界之间存在较大差距。针对Vapnik-Chervonenkis（VC）维数为d的假设类，我们提出了一种新颖的算法，可生成一个ε-最优随机假设，其样本复杂度接近于（d+k）/ε^2（在某些对数因子中），与已知的最佳下界匹配。我们的算法思想和理论被进一步扩展，以适应Rademacher类。提出的算法是奥拉克尔高效的，仅仅访问假设类

    Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
    
[^23]: 神经网络中的元-（超文本外语境）学习

    Meta- (out-of-context) learning in neural networks. (arXiv:2310.15047v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.15047](http://arxiv.org/abs/2310.15047)

    该研究通过合成实验展示了一种称为元-超文本外语境学习（meta-OCL）的现象在神经网络中的存在。这种学习使神经网络能够更好地吸收广泛适用的语义内容，并在适当的情况下进行使用。研究者提出了关于元-超文本外语境学习产生的两种假设，并就未来AI系统的能力和潜在风险进行了讨论。

    

    Brown等人（2020）通过对大型语言模型（LLMs）进行精心设计的合成实验，建立了一种称为元-超文本外语境学习（meta-OCL）的现象的存在。我们的结果表明，元-超文本外语境学习使LLMs更容易“内化”文本的语义内容，该文本广泛适用（例如真实陈述或权威来源的文本），并在适当的情况下使用它。我们进一步在合成计算机视觉环境中展示了元-超文本外语境学习，并提出了两种假设，解释了元-超文本外语境学习的出现：一种是依赖于模型在其参数中存储知识的方式，另一种是暗示梯度下降优化器的隐含梯度对齐偏差可能负责。最后，我们对我们的结果可能意味着未来AI系统的能力进行了思考，并讨论了潜在的风险。我们的代码可以在https://github.com/krasheni找到。

    Brown et al. (2020) famously introduced the phenomenon of in-context learning in large language models (LLMs). We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs. Our results suggest that meta-OCL leads LLMs to more readily "internalize" the semantic content of text that is, or appears to be, broadly useful (such as true statements, or text from authoritative sources) and use it in appropriate circumstances. We further demonstrate meta-OCL in a synthetic computer vision setting, and propose two hypotheses for the emergence of meta-OCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descent-based optimizers may be responsible. Finally, we reflect on what our results might imply about capabilities of future AI systems, and discuss potential risks. Our code can be found at https://github.com/krasheni
    
[^24]: MINDE: 互信息神经扩散估计

    MINDE: Mutual Information Neural Diffusion Estimation. (arXiv:2310.09031v1 [cs.LG])

    [http://arxiv.org/abs/2310.09031](http://arxiv.org/abs/2310.09031)

    MINDE是一种基于得分函数扩散模型的新方法，用于估计随机变量之间的互信息。该方法在准确性和自一致性测试方面优于其他文献中的主要替代方法。

    

    在这项工作中，我们提出了一种估计随机变量之间互信息（MI）的新方法。我们的方法基于Girsanov定理的原创解释，允许我们使用基于得分的扩散模型来估计两个密度函数之间的Kullback-Leibler散度，该估计是它们得分函数之间的差异。作为副产品，我们的方法还能够估计随机变量的熵。借助这样的构建模块，我们提出了一种通用的测量MI的方法，该方法分为两个方向展开：一个使用条件扩散过程，另一个使用联合扩散过程，可以同时对两个随机变量进行建模。我们的结果来自于对我们方法的各种变体进行彻底的实验协议，表明我们的方法比文献中的主要替代方法更准确，尤其是对于具有挑战性的分布。此外，我们的方法通过了MI自一致性测试，包括...

    In this work we present a new method for the estimation of Mutual Information (MI) between random variables. Our approach is based on an original interpretation of the Girsanov theorem, which allows us to use score-based diffusion models to estimate the Kullback Leibler divergence between two densities as a difference between their score functions. As a by-product, our method also enables the estimation of the entropy of random variables. Armed with such building blocks, we present a general recipe to measure MI, which unfolds in two directions: one uses conditional diffusion process, whereas the other uses joint diffusion processes that allow simultaneous modelling of two random variables. Our results, which derive from a thorough experimental protocol over all the variants of our approach, indicate that our method is more accurate than the main alternatives from the literature, especially for challenging distributions. Furthermore, our methods pass MI self-consistency tests, includin
    
[^25]: 无均值回归：不恰当高斯过程回归和不恰当核

    Stationarity without mean reversion: Improper Gaussian process regression and improper kernels. (arXiv:2310.02877v1 [stat.ML])

    [http://arxiv.org/abs/2310.02877](http://arxiv.org/abs/2310.02877)

    本论文展示了使用具有无限方差的不恰当高斯过程先验来定义静止但不均值回归过程的可能性，并引入了一类特殊的不恰当核函数来实现此目的。

    

    高斯过程（GP）回归在机器学习应用中已经广泛流行。GP回归的行为取决于协方差函数的选择。在机器学习应用中，静止协方差函数是首选。然而，（非周期性的）静止协方差函数总是均值回归的，因此在应用于不通过到固定全局均值值的数据时可能表现出病态行为。在本文中，我们展示了使用具有无限方差的不恰当GP先验来定义静止但不均值回归过程是可能的。为此，我们引入了一大类只能在这种不恰当的范围内定义的不恰当核函数。具体地，我们引入了平滑行走核，它产生无限平滑的样本，以及一类不恰当的Matern核，它可以被定义为任意整数j倍可微。所得到的后验分布可以用解析的方式计算出来。

    Gaussian processes (GP) regression has gained substantial popularity in machine learning applications. The behavior of a GP regression depends on the choice of covariance function. Stationary covariance functions are favorite in machine learning applications. However, (non-periodic) stationary covariance functions are always mean reverting and can therefore exhibit pathological behavior when applied to data that does not relax to a fixed global mean value. In this paper, we show that it is possible to use improper GP prior with infinite variance to define processes that are stationary but not mean reverting. To this aim, we introduce a large class of improper kernels that can only be defined in this improper regime. Specifically, we introduce the Smooth Walk kernel, which produces infinitely smooth samples, and a family of improper Mat\'ern kernels, which can be defined to be $j$-times differentiable for any integer $j$. The resulting posterior distributions can be computed analyticall
    
[^26]: 循环神经网络在遍历数据序列上训练的核极限

    Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences. (arXiv:2308.14555v1 [cs.LG])

    [http://arxiv.org/abs/2308.14555](http://arxiv.org/abs/2308.14555)

    本文研究了循环神经网络在遍历数据序列上训练时的核极限，利用数学方法对其渐近特性进行了描述，并证明了RNN收敛到与随机代数方程的不动点耦合的无穷维ODE的解。这对于理解和改进循环神经网络具有重要意义。

    

    本文开发了数学方法来描述循环神经网络（RNN）的渐近特性，其中隐藏单元的数量、序列中的数据样本、隐藏状态的更新和训练步骤同时趋于无穷大。对于具有简化权重矩阵的RNN，我们证明了RNN收敛到与随机代数方程的不动点耦合的无穷维ODE的解。分析需要解决RNN所特有的几个挑战。在典型的均场应用中（例如前馈神经网络），离散的更新量为$\mathcal{O}(\frac{1}{N})$，更新的次数为$\mathcal{O}(N)$。因此，系统可以表示为适当ODE/PDE的Euler逼近，当$N \rightarrow \infty$时收敛到该ODE/PDE。然而，RNN的隐藏层更新为$\mathcal{O}(1)$。因此，RNN不能表示为ODE/PDE的离散化和标准均场技术。

    Mathematical methods are developed to characterize the asymptotics of recurrent neural networks (RNN) as the number of hidden units, data samples in the sequence, hidden state updates, and training steps simultaneously grow to infinity. In the case of an RNN with a simplified weight matrix, we prove the convergence of the RNN to the solution of an infinite-dimensional ODE coupled with the fixed point of a random algebraic equation. The analysis requires addressing several challenges which are unique to RNNs. In typical mean-field applications (e.g., feedforward neural networks), discrete updates are of magnitude $\mathcal{O}(\frac{1}{N})$ and the number of updates is $\mathcal{O}(N)$. Therefore, the system can be represented as an Euler approximation of an appropriate ODE/PDE, which it will converge to as $N \rightarrow \infty$. However, the RNN hidden layer updates are $\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of an ODE/PDE and standard mean-field tec
    
[^27]: 简易注意力：一种用于Transformer的简单自注意机制

    Easy attention: A simple self-attention mechanism for Transformers. (arXiv:2308.12874v1 [cs.LG])

    [http://arxiv.org/abs/2308.12874](http://arxiv.org/abs/2308.12874)

    本论文提出了一种名为简易注意力的注意力机制，用于提高Transformer神经网络在混沌系统时间动态预测中的鲁棒性。该方法不依赖于键、查询和softmax，直接将注意力得分作为可学习参数。实验结果表明，该方法在重构和预测混沌系统的时间动态方面比传统的自注意机制和长短期记忆方法更具鲁棒性和简化性。

    

    为了提高用于混沌系统时间动态预测的Transformer神经网络的鲁棒性，我们提出了一种新颖的注意力机制，称为简易注意力。由于自注意机制仅使用查询和键的内积，因此证明了为了获取捕捉时间序列的长期依赖关系所需的注意力得分，并不需要键、查询和softmax。通过在softmax注意力得分上实施奇异值分解（SVD），我们进一步观察到自注意力在注意力得分的张成空间中压缩了来自查询和键的贡献。因此，我们提出的简易注意力方法直接将注意力得分作为可学习参数。这种方法在重构和预测展现更强鲁棒性和更少复杂性的混沌系统的时间动态时取得了出色的结果，比自注意机制或广泛使用的长短期记忆

    To improve the robustness of transformer neural networks used for temporal-dynamics prediction of chaotic systems, we propose a novel attention mechanism called easy attention. Due to the fact that self attention only makes usage of the inner product of queries and keys, it is demonstrated that the keys, queries and softmax are not necessary for obtaining the attention score required to capture long-term dependencies in temporal sequences. Through implementing singular-value decomposition (SVD) on the softmax attention score, we further observe that the self attention compresses contribution from both queries and keys in the spanned space of the attention score. Therefore, our proposed easy-attention method directly treats the attention scores as learnable parameters. This approach produces excellent results when reconstructing and predicting the temporal dynamics of chaotic systems exhibiting more robustness and less complexity than the self attention or the widely-used long short-ter
    
[^28]: 鲁棒拉格朗日和对抗性策略梯度在鲁棒约束马尔可夫决策过程中的应用

    Robust Lagrangian and Adversarial Policy Gradient for Robust Constrained Markov Decision Processes. (arXiv:2308.11267v1 [cs.LG])

    [http://arxiv.org/abs/2308.11267](http://arxiv.org/abs/2308.11267)

    本文介绍了两种算法，具有鲁棒拉格朗日的RCPG和对抗性RCPG，用于解决鲁棒约束马尔可夫决策过程中的问题。具有鲁棒拉格朗日的RCPG通过使用拉格朗日来计算最坏情况下的动态，而对抗性RCPG通过对抗策略的方式直接和增量学习最坏情况下的动态。

    

    鲁棒约束马尔可夫决策过程（RCMDP）是一个最近应用于强化学习的任务建模框架，它通过使用不确定性集合在转移动态模型中提供了对错误的鲁棒性。模拟RCMDPs需要基于每个状态的值估计计算最坏情况下的动态，这种方法之前在鲁棒约束策略梯度（RCPG）中使用过。本文介绍了两种算法，分别称为具有鲁棒拉格朗日的RCPG和对抗性RCPG。具有鲁棒拉格朗日的RCPG通过使用拉格朗日而不是值或约束来计算最坏情况下的动态从而修改RCPG。对抗性RCPG也基于拉格朗日公式计算最坏情况下的动态，但是将其作为对抗策略直接和增量地学习。

    The robust constrained Markov decision process (RCMDP) is a recent task-modelling framework for reinforcement learning that incorporates behavioural constraints and that provides robustness to errors in the transition dynamics model through the use of an uncertainty set. Simulating RCMDPs requires computing the worst-case dynamics based on value estimates for each state, an approach which has previously been used in the Robust Constrained Policy Gradient (RCPG). Highlighting potential downsides of RCPG such as not robustifying the full constrained objective and the lack of incremental learning, this paper introduces two algorithms, called RCPG with Robust Lagrangian and Adversarial RCPG. RCPG with Robust Lagrangian modifies RCPG by taking the worst-case dynamics based on the Lagrangian rather than either the value or the constraint. Adversarial RCPG also formulates the worst-case dynamics based on the Lagrangian but learns this directly and incrementally as an adversarial policy throug
    
[^29]: 对大规模语言模型中野外越狱提示的特征化和评估

    "Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. (arXiv:2308.03825v1 [cs.CR])

    [http://arxiv.org/abs/2308.03825](http://arxiv.org/abs/2308.03825)

    本文对大规模语言模型中的越狱提示进行了特征化和评估研究。通过测量野外越狱提示的唯一特征和主要攻击策略，我们发现越狱提示越来越多地从公共平台转移到私人平台，给LLM供应商在主动检测方面带来了新的挑战。

    

    大型语言模型（LLM）的滥用已引起了公众和LLM供应商的重视。为了回应这一问题，一些努力已经被做出来，使LLM与人类价值观和意图一致。然而，一种特定类型的对抗性提示，即越狱提示，已经出现并不断演变以绕过保障并引发LLM中的有害内容。本文中，我们首次对野外越狱提示进行了测量研究，收集了6,387个在六个月内从四个平台上获得的提示。通过利用自然语言处理技术和基于图的社区检测方法，我们发现了越狱提示的独特特征及其主要攻击策略，如提示注入和权限提升。我们还观察到，越狱提示越来越多地从公共平台转移到私人平台，给LLM供应商在主动检测方面带来了新的挑战。为了评估越狱提示可能造成的危害，我们创建了

    The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create
    
[^30]: 通过密度匹配实现的合成对照方法下的隐式内生性问题

    Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])

    [http://arxiv.org/abs/2307.11127](http://arxiv.org/abs/2307.11127)

    本文提出了一种新型的合成对照方法，通过密度匹配来解决现有SCMs中的隐式内生性问题。该方法通过将经过处理单元的结果密度与未处理单元的密度进行加权平均来估计SC权重。

    

    合成对照方法（SCMs）已成为比较案例研究中因果推断的重要工具。SCMs的基本思想是通过使用来自未处理单元的观测结果的加权和来估计经过处理单元的反事实结果。合成对照（SC）的准确性对于估计因果效应至关重要，因此，SC权重的估计成为了研究的焦点。在本文中，我们首先指出现有的SCMs存在一个隐式内生性问题，即未处理单元的结果与反事实结果模型中的误差项之间的相关性。我们展示了这个问题会对因果效应估计器产生偏差。然后，我们提出了一种基于密度匹配的新型SCM，假设经过处理单元的结果密度可以用未处理单元的密度的加权平均来近似（即混合模型）。基于这一假设，我们通过匹配来估计SC权重。

    Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matchi
    
[^31]: 用于有效哈密顿量参数化的即时机器学习

    On-the-fly machine learning for parametrization of the effective Hamiltonian. (arXiv:2307.08929v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.08929](http://arxiv.org/abs/2307.08929)

    本论文提出了一种基于贝叶斯线性回归的即时机器学习方法，用于参数化有效哈密顿量。该方法在各种系统中都可以得到准确的结果，包括以前的方法无法处理的复杂系统。

    

    基于第一原理的有效哈密顿量被广泛用于预测和模拟铁电和弛豫铁电体的性质。然而，有效哈密顿量的参数化方法复杂，很难解决具有复杂相互作用和/或复杂组分系统。在这里，我们开发了一种基于贝叶斯线性回归的即时机器学习方法来参数化有效哈密顿量。参数化是在分子动力学模拟中完成的，每一步预测能量、力和应力以及相关的不确定性。当不确定性较大时，执行第一原理计算以重新训练参数。这种方法为计算任何所考虑系统的有效哈密顿量参数提供了一种通用和自动化的方式，包括以前的方法无法处理的复杂系统。以BaTiO3和Pb(Sc,Ta)O3为例展示了该方法的准确性。

    The first-principles-based effective Hamiltonian is widely used to predict and simulate the properties of ferroelectrics and relaxor ferroelectrics. However, the parametrization method of the effective Hamiltonian is complicated and hardly can resolve the systems with complex interactions and/or complex components. Here, we developed an on-the-fly machine learning approach to parametrize the effective Hamiltonian based on Bayesian linear regression. The parametrization is completed in molecular dynamics simulations, with the energy, forces and stress predicted at each step along with their uncertainties. First-principles calculations are executed when the uncertainties are large to retrain the parameters. This approach provides a universal and automatic way to compute the effective Hamiltonian parameters for any considered systems including complex systems which previous methods can not handle. BaTiO3 and Pb(Sc,Ta)O3 are taken as examples to show the accurateness of this approach compa
    
[^32]: 切线变换器用于组合、隐私和去除

    Tangent Transformers for Composition, Privacy and Removal. (arXiv:2307.08122v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2307.08122](http://arxiv.org/abs/2307.08122)

    我们引入了一种切线注意微调方法（TAFT），通过线性化变压器的一阶泰勒展开来进行微调。该方法具有与原始非线性网络相当的性能，并在模型组合、并行训练、机器去除和差分隐私方面具有优势。

    

    我们引入了切线关注微调（TAFT）方法，该方法通过在预训练初始化点周围计算一阶泰勒展开来获得线性化变压器进行微调。我们展示了从线性化得到的雅可比矩阵-向量积可以在单个前向传递中高效计算，将训练和推断成本降低到与原始非线性模型相同数量级，并且使用相同数量的参数。此外，我们还展示了当应用于各种下游视觉分类任务时，通过TAFT进行微调的结果切线变换器可以与对原始非线性网络进行微调相当。由于切线变换器对于新的权值是线性的，并且结果微调损失是凸的，我们展示了相比于非线性微调，TAFT在模型组合、并行训练、机器去除和差分隐私方面具有几个优势。

    We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuning linearized transformers obtained by computing a First-order Taylor Expansion around a pre-trained initialization. We show that the Jacobian-Vector Product resulting from linearization can be computed efficiently in a single forward pass, reducing training and inference cost to the same order of magnitude as its original non-linear counterpart, while using the same number of parameters. Furthermore, we show that, when applied to various downstream visual classification tasks, the resulting Tangent Transformer fine-tuned with TAFT can perform comparably with fine-tuning the original non-linear network. Since Tangent Transformers are linear with respect to the new set of weights, and the resulting fine-tuning loss is convex, we show that TAFT enjoys several advantages compared to non-linear fine-tuning when it comes to model composition, parallel training, machine unlearning, and differential privacy.
    
[^33]: 互联网物联网中基于在线自监督学习的机器学习入侵检测

    Online Self-Supervised Learning in Machine Learning Intrusion Detection for the Internet of Things. (arXiv:2306.13030v1 [cs.CR])

    [http://arxiv.org/abs/2306.13030](http://arxiv.org/abs/2306.13030)

    提出了一种在线自监督学习的机器学习入侵检测框架，能够实现完全在线的IDS，无需离线学习或人工干预，该框架能够快速适应流量的时变特性，消除了离线数据收集的需要，并避免了数据标记中的人为误差，具有准确率高、成本低等优势。

    

    本文提出了一种新颖的自我监督入侵检测（SSID）框架，它使得完全在线的基于机器学习的入侵检测系统成为可能，而无需人工干预或离线学习。该框架仅基于IDS本身的决策和在线估计的统计可信度，使用自联想深度随机神经网络分析和标记传入的流量数据包。SSID框架使IDS能够快速适应网络流量的时变特性，并消除了离线数据收集的需求。这种方法避免了数据标记中的人为误差，以及模型训练和数据收集的人力和计算成本。该方法在公共数据集上进行了实验评估，并与知名机器学习模型进行了比较，结果表明，作为准确和在线学习的基于机器学习的IDS，该SSID框架非常有用且具有优势，可以应用于物联网系统中。

    This paper proposes a novel Self-Supervised Intrusion Detection (SSID) framework, which enables a fully online Machine Learning (ML) based Intrusion Detection System (IDS) that requires no human intervention or prior off-line learning. The proposed framework analyzes and labels incoming traffic packets based only on the decisions of the IDS itself using an Auto-Associative Deep Random Neural Network, and on an online estimate of its statistically measured trustworthiness. The SSID framework enables IDS to adapt rapidly to time-varying characteristics of the network traffic, and eliminates the need for offline data collection. This approach avoids human errors in data labeling, and human labor and computational costs of model training and data collection. The approach is experimentally evaluated on public datasets and compared with well-known ML models, showing that this SSID framework is very useful and advantageous as an accurate and online learning ML-based IDS for IoT systems.
    
[^34]: 利用过参数化的浅层ReLU神经网络的非参数回归

    Nonparametric regression using over-parameterized shallow ReLU neural networks. (arXiv:2306.08321v1 [stat.ML])

    [http://arxiv.org/abs/2306.08321](http://arxiv.org/abs/2306.08321)

    本文表明，对于学习某些光滑函数类，具有适当权重限制或正则化的过参数化神经网络可以达到最小最优收敛速率。作者通过对过度参数化的神经网络进行了实验，成功证明在浅层ReLU神经网络中使用最小二乘估计值是最小化最优的。同时作者还得出了神经网络的本地Rademacher复杂度的一个新的上界。

    

    如果权重得到合适的限制或正则化，那么可以证明过度参数化的神经网络可以达到某些光滑函数类的学习最小最优收敛速率（最多对数因数）。具体地，我们考虑使用浅层ReLU神经网络的非参数回归来估计未知的$d$变量函数。假设回归函数是从具有光滑度$\alpha < (d+3)/2$的Holder空间或对应于浅层神经网络的变化空间中学习的，后者可以视为无限宽的神经网络。在这种情况下，我们证明了基于具有权重某些范数约束的浅层神经网络的最小二乘估计值是最小化最优的，如果网络宽度足够大。作为副产品，我们得到了一个新的和神经网络的本地Rademacher复杂度无关的上界，这可能是有独立兴趣的。

    It is shown that over-parameterized neural networks can achieve minimax optimal rates of convergence (up to logarithmic factors) for learning functions from certain smooth function classes, if the weights are suitably constrained or regularized. Specifically, we consider the nonparametric regression of estimating an unknown $d$-variate function by using shallow ReLU neural networks. It is assumed that the regression function is from the H\"older space with smoothness $\alpha<(d+3)/2$ or a variation space corresponding to shallow neural networks, which can be viewed as an infinitely wide neural network. In this setting, we prove that least squares estimators based on shallow neural networks with certain norm constraints on the weights are minimax optimal, if the network width is sufficiently large. As a byproduct, we derive a new size-independent bound for the local Rademacher complexity of shallow ReLU neural networks, which may be of independent interest.
    
[^35]: 个性化指导有助于知识蒸馏

    Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])

    [http://arxiv.org/abs/2305.09651](http://arxiv.org/abs/2305.09651)

    本文提出了一种个性化指导的学习技术，称为LGTM，其利用蒸馏效应选择样本以增强学生的泛化能力，在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    

    先前研究表明，能力超群的教师模型并不一定能够让学生水平得到提升，这凸显了当前教师培训实践和有效知识传授之间的不一致性。为了提高教师培训过程的指导效果，本文引入了蒸馏效应的概念，以确定每个训练样本对学生泛化能力的影响。我们提出了一种名为学好教师很重要（LGTM）的有效训练技术，以将蒸馏效应纳入教师的学习过程中。通过优先选择可能提升学生泛化能力的样本，我们的LGTM在GLUE基准测试的6个文本分类任务中优于10个常见的知识蒸馏基线算法。

    It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
    
[^36]: 一种用于音视频言语情感识别的矢量量化掩码自编码器

    A vector quantized masked autoencoder for audiovisual speech emotion recognition. (arXiv:2305.03568v1 [cs.SD])

    [http://arxiv.org/abs/2305.03568](http://arxiv.org/abs/2305.03568)

    本文提出了一种特别为音视频言语自监督表示学习设计的矢量量化MAE模型，采用了基于离散音频和视觉言语表示的自监督范式，并在标准情感音视频言语数据集上取得了较好的效果。

    

    尽管全面监督模型已被证明对于音视频言语情感识别（SER）非常有效，但标记数据的有限性仍然是该领域的主要挑战。为了解决这个问题，自监督学习方法，如掩码自编码器（MAEs），已成为潜在解决方案。本文提出了一种特别为音视频言语自监督表示学习设计的矢量量化MAE模型（VQ-MAE-AV）。与现有的依赖于原始音视频言语数据处理的多模态MAEs不同，该方法采用了基于两个预先训练的矢量量化变分自编码器学习的离散音频和视觉言语表示的自监督范式。实验结果表明，该方法在VoxCeleb2数据库上进行预训练，并在标准情感音视频言语数据集上进行微调，优于现有的音视频SER方法。

    While fully-supervised models have been shown to be effective for audiovisual speech emotion recognition (SER), the limited availability of labeled data remains a major challenge in the field. To address this issue, self-supervised learning approaches, such as masked autoencoders (MAEs), have gained popularity as potential solutions. In this paper, we propose the VQ-MAE-AV model, a vector quantized MAE specifically designed for audiovisual speech self-supervised representation learning. Unlike existing multimodal MAEs that rely on the processing of the raw audiovisual speech data, the proposed method employs a self-supervised paradigm based on discrete audio and visual speech representations learned by two pre-trained vector quantized variational autoencoders. Experimental results show that the proposed approach, which is pre-trained on the VoxCeleb2 database and fine-tuned on standard emotional audiovisual speech datasets, outperforms the state-of-the-art audiovisual SER methods.
    
[^37]: 基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型

    Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])

    [http://arxiv.org/abs/2304.03427](http://arxiv.org/abs/2304.03427)

    本文提出了一种基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型，可以自动纠正OCR输出中的噪声。

    

    人文学者在研究历史、宗教和社会政治结构等方面经常依赖于古代手稿。虽然OCR技术可以将这些宝贵手稿数字化，但多数手稿因磨损而过时，OCR程序没办法识别翻页的虚淡或污渍。本文提出了一种基于谷歌OCR扫描的藏文手稿的神经拼写纠错模型，可以自动纠正OCR输出中的噪声。本文分为四个部分：数据集、模型架构、训练和分析。首先，我们对原始藏文电子文本语料库进行了特征工程，并将其转化为两组结构化数据框——一组匹配的玩具数据和一组匹配的真实数据。然后，我们在Transformer架构中实现了置信度得分机制来执行拼写校正任务。根据损失和字符错误率，我们的Transformer + 置信度得分机制比其他常用的拼写校正算法表现更好。

    Scholars in the humanities rely heavily on ancient manuscripts to study history, religion, and socio-political structures in the past. Many efforts have been devoted to digitizing these precious manuscripts using OCR technology, but most manuscripts were blemished over the centuries so that an Optical Character Recognition (OCR) program cannot be expected to capture faded graphs and stains on pages. This work presents a neural spelling correction model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy output. This paper is divided into four sections: dataset, model architecture, training and analysis. First, we feature-engineered our raw Tibetan etext corpus into two sets of structured data frames -- a set of paired toy data and a set of paired real data. Then, we implemented a Confidence Score mechanism into the Transformer architecture to perform spelling correction tasks. According to the Loss and Character Error Rate, our Transformer + Confidence score mechani
    
[^38]: 神经网络函数的Lipschitz连续性的一些基本方面

    Some Fundamental Aspects about Lipschitz Continuity of Neural Network Functions. (arXiv:2302.10886v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.10886](http://arxiv.org/abs/2302.10886)

    本文深入研究和描述神经网络实现的函数的Lipschitz行为，在多种设置下进行实证研究，并揭示了神经网络函数Lipschitz连续性的基本和有趣的特性，其中最引人注目的是在Lipschitz常数的上限和下限中识别出了明显的双下降趋势。

    

    Lipschitz连续性是任何预测模型的一个简单但关键的功能性质，它处于模型的稳健性、泛化性和对抗性脆弱性的核心。本文旨在深入研究和描述神经网络实现的函数的Lipschitz行为。因此，我们通过耗尽最简单和最一般的下限和上限的极限，在各种不同设置下进行实证研究（即，体系结构、损失、优化器、标签噪音等），虽然这一选择主要是受计算难度结果的驱动，但它也非常丰富，并揭示了神经网络函数Lipschitz连续性的几个基本和有趣的特性，我们还补充了适当的理论论证。

    Lipschitz continuity is a simple yet crucial functional property of any predictive model for it lies at the core of the model's robustness, generalisation, as well as adversarial vulnerability. Our aim is to thoroughly investigate and characterise the Lipschitz behaviour of the functions realised by neural networks. Thus, we carry out an empirical investigation in a range of different settings (namely, architectures, losses, optimisers, label noise, and more) by exhausting the limits of the simplest and the most general lower and upper bounds. Although motivated primarily by computational hardness results, this choice nevertheless turns out to be rather resourceful and sheds light on several fundamental and intriguing traits of the Lipschitz continuity of neural network functions, which we also supplement with suitable theoretical arguments. As a highlight of this investigation, we identify a striking double descent trend in both upper and lower bounds to the Lipschitz constant with in
    
[^39]: 稀疏加性模型的分类

    Classification by sparse additive models. (arXiv:2212.01792v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2212.01792](http://arxiv.org/abs/2212.01792)

    这篇论文研究了非参数的稀疏加性模型用于分类，通过对分量系数施加稀疏组Lasso和稀疏组Slope型惩罚来设计分类器，实验证明了分类器在未知稀疏性和平滑性上的自适应性能。

    

    我们考虑了用于分类的非参数稀疏加性模型（SpAM）。SpAM分类器的设计基于最小化logistic损失，通过对分量展开系数施加稀疏组Lasso和更一般的稀疏组Slope型惩罚（例如，傅里叶或小波）。所得的分类器对未知的稀疏性和平滑性具有固有的自适应性。我们证明，在某些稀疏组受限特征值条件下，稀疏组Lasso分类器在整个解析、Sobolev和Besov类范围内几乎是最小化极小（加上对数因子），而稀疏组Slope分类器在稀疏和适度稠密设定下达到了确切的最小化极小阶数（不含额外的对数因子）。该分类器的性能在实际数据例子中得到了证明。

    We consider (nonparametric) sparse additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso and more general sparse group Slope-type penalties on the coefficients of univariate components' expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifiers are inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition the sparse group Lasso classifier is nearly-minimax (up to log-factors) within the entire range of analytic, Sobolev and Besov classes while the sparse group Slope classifier achieves the exact minimax order (without the extra log-factors) for sparse and moderately dense setups. The performance of the proposed classifier is illustrated on the real-data example.
    
[^40]: 极化编码：一种用于处理缺失值分类的简单基线方法

    Polar Encoding: A Simple Baseline Approach for Classification with Missing Values. (arXiv:2210.01905v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01905](http://arxiv.org/abs/2210.01905)

    极化编码是一种用于处理具有缺失值的分类问题的简单基线方法，它能保留缺失信息、无需插补，让决策树自由选择如何处理缺失值。

    

    我们提出了一种称为极化编码的表示方法，用于处理具有缺失值的分类问题中的分类和数值型$[0,1]$值属性。我们认为这是一种很好的基准方法，因为它可以与任何分类算法配合使用，能够保留缺失信息，非常简单易用并且性能良好。与现有的缺失指示方法不同，极化编码不需要插补，确保缺失值与非缺失值等距离，让决策树算法自由选择如何分割缺失值，从而实现了“属性中包含缺失性”（MIA）的实际处理。此外，我们还展示了分类和$[0,1]$值属性可以被看作是一个单一属性类型的特殊情况，对应于经典的重心坐标概念，这提供了极化编码的模糊化形式的自然解释。

    We propose polar encoding, a representation of categorical and numerical $[0,1]$-valued attributes with missing values to be used in a classification context. We argue that this is a good baseline approach, because it can be used with any classification algorithm, preserves missingness information, is very simple to apply and offers good performance. In particular, unlike the existing missing-indicator approach, it does not require imputation, ensures that missing values are equidistant from non-missing values, and lets decision tree algorithms choose how to split missing values, thereby providing a practical realisation of the "missingness incorporated in attributes" (MIA) proposal. Furthermore, we show that categorical and $[0,1]$-valued attributes can be viewed as special cases of a single attribute type, corresponding to the classical concept of barycentric coordinates, and that this offers a natural interpretation of polar encoding as a fuzzified form of one-hot encoding. With an 
    
[^41]: 推荐系统中用于排名蒸馏的双重修正策略

    Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. (arXiv:2109.03459v2 [cs.IR] UPDATED)

    [http://arxiv.org/abs/2109.03459](http://arxiv.org/abs/2109.03459)

    本文提出了一种双重修正策略（DCD），用于在推荐系统中更有效地将教师模型的排名信息转移到学生模型。这种方法不仅充分利用了学生模型的预测误差，还提供了更全面的视角，解决了松弛排名蒸馏方法的限制。

    

    知识蒸馏是将训练充分的大模型（教师）的知识转移到小模型（学生）的重要研究领域，对于推荐系统的实际部署而言，它已成为一个重要的研究方向。最近，松弛排名蒸馏（RRD）表明，在推荐列表中蒸馏排名信息能够显著提高性能。然而，该方法仍然存在以下限制：1）它未充分利用学生模型的预测误差，使得训练效率不高；2）它只蒸馏用户侧的排名信息，在稀疏的隐式反馈下提供的视角不足。本文提出了一种更高效的蒸馏方法，即双重修正策略（DCD），通过教师模型和学生模型预测之间的差异来决定要蒸馏的知识。

    Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list significantly improves the performance. However, the method still has limitations in that 1) it does not fully utilize the prediction errors of the student model, which makes the training not fully efficient, and 2) it only distills the user-side ranking information, which provides an insufficient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more efficient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be disti
    

