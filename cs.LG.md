# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Anytime-Competitive Reinforcement Learning with Policy Prior.](http://arxiv.org/abs/2311.01568) | 本文研究了具有策略先验的任意时刻竞争性强化学习问题（A-CMDP）。我们提出了一种新的算法ACRL，该算法可以保证任意时刻的成本约束，并在实验中验证了其回报性能和成本约束保证。 |
| [^2] | [Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization.](http://arxiv.org/abs/2311.01544) | 本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。 |
| [^3] | [AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models.](http://arxiv.org/abs/2311.01305) | AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。 |
| [^4] | [Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling.](http://arxiv.org/abs/2311.00797) | 该论文通过机器学习辅助的数据驱动建模方法，研究了自适应易感-感染-易感流行病学网络的临界转折点集体动力学。他们识别出了一个有效的随机微分方程以描述网络的演化行为，并观察到了罕见的大幅度集体振荡现象。 |
| [^5] | [A Robust Deep Learning Method with Uncertainty Estimation for the Pathological Classification of Renal Cell Carcinoma based on CT Images.](http://arxiv.org/abs/2311.00567) | 本论文提出了一种基于CT图像的深度学习方法，通过不确定性估计，能够有效分类肾细胞癌的病理亚型，为放射科医师在术前做出诊断提供帮助。 |
| [^6] | [MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows.](http://arxiv.org/abs/2311.00334) | MetisFL是一种可扩展和高效的联邦学习系统，重点关注联邦控制器的可扩展性和优化。 |
| [^7] | [Managing AI Risks in an Era of Rapid Progress.](http://arxiv.org/abs/2310.17688) | 在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。 |
| [^8] | [Causal Q-Aggregation for CATE Model Selection.](http://arxiv.org/abs/2310.16945) | 该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率 |
| [^9] | [Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph.](http://arxiv.org/abs/2310.16452) | 本文提出了一个名为PEARLM的方法，通过语言建模开展基于路径的知识图谱推荐，解决了现有方法中对预训练知识图谱嵌入的依赖以及未充分利用实体和关系之间相互依赖性的问题，还避免了生成不准确的解释。实验结果表明，与现有方法相比，我们的方法效果显著。 |
| [^10] | [VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization.](http://arxiv.org/abs/2310.11864) | VQ-NeRF是一个基于向量量化的神经网络模型，用于分解和编辑3D场景中的反射场。通过将连续材料离散化，该模型可以减少噪声并生成离散材料的分割地图，从而实现简化的材料编辑。 |
| [^11] | [Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs.](http://arxiv.org/abs/2310.11689) | 本研究提出了一种自适应框架，利用自我评估来改进大型语言模型（LLMs）的选择性预测能力。该方法基于参数效率调整，能够适应特定任务并提高其自我评估能力，实验结果表明其优于最先进的选择性预测方法。 |
| [^12] | [PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection.](http://arxiv.org/abs/2310.11676) | PREM是一种简单而有效的节点级图异常检测方法，它通过简化图异常检测的过程，减少了时间和内存消耗，同时保持了强大的异常检测能力。 |
| [^13] | [Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations.](http://arxiv.org/abs/2310.10705) | 本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。 |
| [^14] | [Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach.](http://arxiv.org/abs/2310.08660) | 本研究提出了一种批量约束的离策略方法，用于解决联合波束成形的参数优化问题。通过使用深度强化学习技术，克服了传统方法中计算复杂度高和无法准确建模的难题。 |
| [^15] | [Towards the Fundamental Limits of Knowledge Transfer over Finite Domains.](http://arxiv.org/abs/2310.07838) | 本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。 |
| [^16] | [Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks.](http://arxiv.org/abs/2310.03530) | 本研究通过探索联合群不变函数在数据-参数域上的作用，提出了一种系统的规则来解码神经网络内部数据表示中的对称性和几何性。利用这一规则，我们引入了由联合不变函数导出的通用神经网络，并利用群论证明了其普适性。这一研究揭示了逼近理论和深度学习中的群论方面，并将几何深度学习与抽象调和分析相连接。 |
| [^17] | [Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks.](http://arxiv.org/abs/2310.03529) | 通过对数据空间上的群作用来识别DNN内部的隐藏层，并将DNN构建为相对于Koopman算子的双声变换，我们利用群论论证证明了这些DNN的普适性。 |
| [^18] | [Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models.](http://arxiv.org/abs/2310.03059) | Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。 |
| [^19] | [De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics.](http://arxiv.org/abs/2310.00023) | 本研究提出了De-SaTE方法，通过利用多个去噪模块以及自注意力变换编码器，准确预测锂离子电池的剩余寿命（RUL），为预防性维护和预测性分析提供关键指标估计。 |
| [^20] | [Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models.](http://arxiv.org/abs/2309.16521) | 本论文提出了一种使用深度生成时间序列模型和决策理论相结合的新框架，用于生成个性化的胰岛素治疗策略。通过学习生成逼真的个性化治疗和未来结果轨迹，可以为个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。 |
| [^21] | [Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness.](http://arxiv.org/abs/2309.11373) | 本文研究了从时间序列电子健康记录数据预测患者静态信息的能力，并发现不仅原始数据，机器学习模型学习到的表示也可以用于预测各种静态信息，包括生物学性别、二值化年龄和自报种族，具有高预测性能。 |
| [^22] | [Towards Last-layer Retraining for Group Robustness with Fewer Annotations.](http://arxiv.org/abs/2309.08534) | 本研究发现，在没有群体标注和只有少量类别标注的情况下，最后一层再训练仍然可以有效提高最差群体准确性，从而优于经验风险最小化在整个数据集上的表现，可以实现群体鲁棒性的提升。 |
| [^23] | [SLiMe: Segment Like Me.](http://arxiv.org/abs/2309.03179) | 基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。 |
| [^24] | [Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning.](http://arxiv.org/abs/2308.11630) | 本研究使用迁移学习解决了光学矩阵乘法模型训练中的数据稀缺问题，并通过利用合成数据进行预训练和实验数据进行微调，成功减小了建模误差，在仅使用25%数据的情况下实现对矩阵权重的小于1 dB的均方根误差。 |
| [^25] | [Activation Addition: Steering Language Models Without Optimization.](http://arxiv.org/abs/2308.10248) | 这项研究探讨了一种在推理时通过改变激活来预测性地改变语言模型行为的方法，并且相比于传统方法具有更低的计算和实施成本，并且能够保持模型性能。 |
| [^26] | [Time Travel in LLMs: Tracing Data Contamination in Large Language Models.](http://arxiv.org/abs/2308.08493) | 该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。 |
| [^27] | [Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic.](http://arxiv.org/abs/2308.07336) | 本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。 |
| [^28] | [RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model.](http://arxiv.org/abs/2308.05345) | RTLLM是一个用自然语言指令生成设计RTL的开源基准测试，旨在解决现有工作中目标设计简单且规模小的问题，并提供对基于LLM的解决方案进行设计质量的定量评估。 |
| [^29] | [Price-Aware Deep Learning for Electricity Markets.](http://arxiv.org/abs/2308.01436) | 本文提出了一种价格感知深度学习方法，通过将电力市场清算优化嵌入到深度学习层中，平衡预测误差和定价误差，提高公平性，并控制价格误差的空间分布。 |
| [^30] | [ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription.](http://arxiv.org/abs/2307.15691) | ODTlearn是一个开源的Python包，用于学习预测和处方的最优决策树。它提供了多种优化方法，并支持各种问题和算法的扩展。 |
| [^31] | [Anytime Model Selection in Linear Bandits.](http://arxiv.org/abs/2307.12897) | 该论文提出了一种在线性赌博机中进行任意模型选择的方法，通过模拟全信息反馈实现在遗憾方面具有指数改进的性能，并且不依赖时间界限和纯探索阶段。 |
| [^32] | [Tackling the Curse of Dimensionality with Physics-Informed Neural Networks.](http://arxiv.org/abs/2307.12306) | 本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。 |
| [^33] | [AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models.](http://arxiv.org/abs/2307.11772) | AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。 |
| [^34] | [A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset.](http://arxiv.org/abs/2307.10455) | 提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。 |
| [^35] | [FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning.](http://arxiv.org/abs/2307.10317) | FedBug是一个自底向上逐渐解冻的联邦学习框架，通过冻结和逐渐解冻模型层，实现了一种有效缓解客户端漂移现象的方法。 |
| [^36] | [A Nearly-Linear Time Algorithm for Structured Support Vector Machines.](http://arxiv.org/abs/2307.07735) | 这篇论文提出了针对结构化支持向量机的接近线性时间算法，解决了二次规划输入规模和解决时间的问题。 |
| [^37] | [No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models.](http://arxiv.org/abs/2307.06440) | 本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。 |
| [^38] | [A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms.](http://arxiv.org/abs/2307.01231) | 本研究重新评估了(深度)学习匹配算法的基准数据集，发现其中大多数数据集都属于相对简单的分类任务。 |
| [^39] | [milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing.](http://arxiv.org/abs/2306.17010) | milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。 |
| [^40] | [Sampling weights of deep neural networks.](http://arxiv.org/abs/2306.16830) | 我们提出了一种基于随机特征模型的采样方法，用于深度神经网络的权重和偏差。我们的方法不需要迭代优化或计算梯度，能够生成通用逼近器，并且对数据的变换和缩放是不变的。在数值实验中，我们展示了我们的方法的优越性能。 |
| [^41] | [OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection.](http://arxiv.org/abs/2306.16045) | OpenNDD是一个用于神经发育障碍检测的开放性识别框架，结合了自动编码器和对抗循环点开放性识别技术，能准确识别已知类别并识别未遇到的类别。 |
| [^42] | [Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset.](http://arxiv.org/abs/2306.13948) | 该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。 |
| [^43] | [CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation.](http://arxiv.org/abs/2306.13761) | 本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。 |
| [^44] | [Learning to Generate Better Than Your LLM.](http://arxiv.org/abs/2306.11816) | 本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。 |
| [^45] | [Understanding Optimization of Deep Learning.](http://arxiv.org/abs/2306.09338) | 本文全面介绍了深度学习中梯度消失和梯度爆炸等优化挑战，并通过提高梯度流和对网络Lipschitz常数施加约束等措施进行了解决。显式优化和隐式优化是两种解决优化问题的不同方式。 |
| [^46] | [Phase Transitions of Civil Unrest across Countries and Time.](http://arxiv.org/abs/2306.08698) | 跨国和时间的社会动荡相变研究探索了集体社会动荡是否可以被描述为一系列具有可测量和可识别特征的循环相变，并证明了宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，以及普遍机制可能支撑着社会动荡的某些方面。 |
| [^47] | [VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models.](http://arxiv.org/abs/2306.06874) | 本文提出VillanDiffusion，一个针对扩散模型的统一后门攻击框架，涵盖主流的无条件和有条件DM，便于对不同DM配置进行后门分析，并为基于字幕的DM后门攻击提供了新的见解。 |
| [^48] | [Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL.](http://arxiv.org/abs/2306.04220) | 本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。 |
| [^49] | [Human-Aligned Calibration for AI-Assisted Decision Making.](http://arxiv.org/abs/2306.00074) | 本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。 |
| [^50] | [Bigger, Better, Faster: Human-level Atari with human-level efficiency.](http://arxiv.org/abs/2305.19452) | 引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。 |
| [^51] | [Convolutional Monge Mapping Normalization for learning on biosignals.](http://arxiv.org/abs/2305.18831) | 本研究提出了一种新的方法：基于卷积蒙日映射归一化 (CMMN)，用于信号众多但变异性较大的生物医学数据上，能自适应调整幅度、滤波器的功率谱密度，通过最优输运映射和 barycenters 实现个体测试时间适应新数据，且显著提升信号分类和检测性能。 |
| [^52] | [GBG++: A Fast and Stable Granular Ball Generation Method for Classification.](http://arxiv.org/abs/2305.18450) | 本文提出了一种基于关注机制的快速稳定的颗粒球生成方法，可以在分类任务中应用。 |
| [^53] | [GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning.](http://arxiv.org/abs/2305.18427) | 本文提出了一种可解释奖励再分配的方法，通过因果透视建模状态和行动贡献，产生可解释的返回分解。生成返回分解（GRD）框架用于延迟奖励场景中的策略优化。 |
| [^54] | [StEik: Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation.](http://arxiv.org/abs/2305.18414) | 本文提出了一种新的神经网络方法StEik来稳定神经符号距离函数，同时实现了更好的形状细节表示和优化效果，表现出比现有INR方法更好的性能。 |
| [^55] | [Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization.](http://arxiv.org/abs/2305.15889) | 本文提出了一种Feature Heterogeneity Distance(FHD)度量标准来衡量领域的异质性，并引入了一个新的实验模式Contrastive Convergence for Domain Generalization (CCDG) 来寻找最佳的监督信号来提高泛化。实验表明，我们的方法比其他最先进的方法更加有效和优越。 |
| [^56] | [Reversible and irreversible bracket-based dynamics for deep graph neural networks.](http://arxiv.org/abs/2305.15616) | 本文提出了基于结构保持基于括号的动力学系统的新型GNN架构，这些架构在理论上被证明要么保持能量，要么在深度增加时产生正的耗散，这解释了可逆性和不可逆性在网络性能中的作用。 |
| [^57] | [ParticleWNN: a Novel Neural Networks Framework for Solving Partial Differential Equations.](http://arxiv.org/abs/2305.12433) | ParticleWNN是一种新型的神经网络框架，可以在弱形式下求解PDE。它采用DNN作为试验空间，用由粒子为中心的极小区域内的紧密支持的函数构成的测试空间，并通过R自适应策略训练神经网络。该框架具有较高的精度和效率，并且易于扩展和并行化。 |
| [^58] | [SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters.](http://arxiv.org/abs/2305.12082) | 本文提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中的安全过滤器的鲁棒性，该框架的关键洞见是搜索备选令牌来绕过安全过滤器。 |
| [^59] | [Moment Matching Denoising Gibbs Sampling.](http://arxiv.org/abs/2305.11650) | 本文提出了动量匹配去噪Gibbs采样方法，可以在给定‘嘈杂’的模型的情况下，从干净的模型中有效地进行采样。 |
| [^60] | [Verifiable Learning for Robust Tree Ensembles.](http://arxiv.org/abs/2305.03626) | 本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。 |
| [^61] | [MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning.](http://arxiv.org/abs/2304.09402) | MixPro是一种数据增强方法，通过对原始输入和模板进行混合来提高基于提示的学习性能，平均提高了5.08%的模型性能。 |
| [^62] | [FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation.](http://arxiv.org/abs/2304.02725) | FMG-Net和W-Net是两种受多重网格启发的深度学习架构，能够解决医学图像分割中面临的细节特征和尺度变化的挑战，能够提高肿瘤分割的精度。 |
| [^63] | [Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning.](http://arxiv.org/abs/2304.01203) | 本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。 |
| [^64] | [Uncertainty-Aware Workload Prediction in Cloud Computing.](http://arxiv.org/abs/2303.13525) | 本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。 |
| [^65] | [From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding.](http://arxiv.org/abs/2303.12816) | 本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。 |
| [^66] | [Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning.](http://arxiv.org/abs/2303.10180) | 本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。 |
| [^67] | [Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning.](http://arxiv.org/abs/2303.09447) | 本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。 |
| [^68] | [Provably Convergent Plug-and-Play Quasi-Newton Methods.](http://arxiv.org/abs/2303.07271) | 本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。 |
| [^69] | [A Survey on Automated Design of Metaheuristic Algorithms.](http://arxiv.org/abs/2303.06532) | 本文综述了自动设计元启发式算法的形式化、方法论、挑战和研究趋势，讨论了自动设计的潜在未来方向和开放问题。 |
| [^70] | [Optimal and Private Learning from Human Response Data.](http://arxiv.org/abs/2303.06234) | 本文提出了一种新的谱估计算法，具有高效和准确的特点，可以在温和的采样条件下实现极小化最优误差界限，同时在top-$K$恢复的样本复杂度方面具有最优性。 |
| [^71] | [DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks.](http://arxiv.org/abs/2303.04878) | DeepGD是一种用于深度神经网络的多目标黑盒测试选择方法，通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。 |
| [^72] | [Modulated Neural ODEs.](http://arxiv.org/abs/2302.13262) | 变调神经ODEs （MoNODEs）是一种新的框架，能够将动力学状态与基础静态变化因素分开，并改进了现有的神经ODE方法。该方法通过引入时间不变的调制变量来捕捉轨迹间的变化，并在测试中展现出在振荡系统、视频和人类行走轨迹等方面具有提高模型泛化能力的效果。 |
| [^73] | [InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis.](http://arxiv.org/abs/2302.08624) | InstructABSA是一种使用指令学习范式的方面情感分析方法，能够显著提高Aspect Term Extraction、Aspect Term Sentiment Classification、和Joint Task subtasks三个子任务的性能，并且在多个数据集上表现超过之前的最先进方法。 |
| [^74] | [Byte Pair Encoding for Symbolic Music.](http://arxiv.org/abs/2301.11975) | 本文介绍了一种在符号音乐领域中使用的字节对编码技术，它可以显著减小序列长度并增加词汇量，从而提高语言模型的嵌入能力。 |
| [^75] | [Tighter Bounds on the Expressivity of Transformer Encoders.](http://arxiv.org/abs/2301.10743) | 本文旨在更紧密地界定Transformer编码器的表达能力，提供了一个同时是下限和上限的一阶逻辑变体的策略，使我们更加接近准确刻画Transformer编码器可识别语言的目标。 |
| [^76] | [The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique.](http://arxiv.org/abs/2212.11146) | 本文介绍了使用Transkribus平台改进手写文本识别（HTR）模型性能的实践经验，包括创建转录协议、完整使用语言模型以及确定最佳使用基础模型的方法，这些方法可以将单个模型的性能提高20%以上（达到字符错误率低于5%），并讨论了HTR平台的合作性质和数据分享等挑战。 |
| [^77] | [Stochastic First-Order Learning for Large-Scale Flexibly Tied Gaussian Mixture Model.](http://arxiv.org/abs/2212.05402) | 本文提出了一种面对高维流数据和复杂密度挑战的快速在线参数估计算法，并利用灵活绑定因子分解的协方差矩阵提供了一个框架。通过引入一阶随机优化和新的随机流形优化算法，实现了高斯混合模型的优化。 |
| [^78] | [Welfare and Fairness in Multi-objective Reinforcement Learning.](http://arxiv.org/abs/2212.01382) | 本论文研究了多目标强化学习中的福利和公平性问题，提出了一种基于非线性福利函数的Q-learning算法，通过非线性标量化学习更新和非稳态动作选择来优化策略。算法被证明是可收敛的。 |
| [^79] | [SciRepEval: A Multi-Format Benchmark for Scientific Document Representations.](http://arxiv.org/abs/2211.13308) | SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。 |
| [^80] | [Instance-Dependent Generalization Bounds via Optimal Transport.](http://arxiv.org/abs/2211.01258) | 该论文提出了一种基于最优传输的实例相关泛化界限的方法，以解释神经网络泛化的关键因素，并且考虑了初始化和随机梯度下降的强归纳偏差。这种方法在模型参数化不可知且训练样本数量远小于参数数量时表现良好，还可以应用于低维流形上的数据和分布转换情况下的泛化问题。 |
| [^81] | [Non-contrastive representation learning for intervals from well logs.](http://arxiv.org/abs/2209.14750) | 本文提出了一种新的方法来处理井测数据表示学习问题，采用自我监督学习的方法进行非对比度的表示学习，减少对数据的标注需求，并提高了算法性能。 |
| [^82] | [RankSEG: A Consistent Ranking-based Framework for Segmentation.](http://arxiv.org/abs/2206.13086) | 本文提出了一种新型的一致排序框架，即RankDice/RankIoU，用于解决由于现有的分割框架对于Dice/IoU指标缺乏一致性而可能导致的次优解决方案。 |
| [^83] | [HyperMixer: An MLP-based Low Cost Alternative to Transformers.](http://arxiv.org/abs/2203.03691) | HyperMixer是一种低成本的基于MLP的Transformer替代方案，通过动态形成标记混合MLP来实现自然语言理解，其性能比替代方案好，并可与Transformer媲美，成本更低。 |

# 详细

[^1]: Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG]) （具有策略先验的任意时刻竞争性强化学习）

    Anytime-Competitive Reinforcement Learning with Policy Prior. (arXiv:2311.01568v1 [cs.LG])

    [http://arxiv.org/abs/2311.01568](http://arxiv.org/abs/2311.01568)

    本文研究了具有策略先验的任意时刻竞争性强化学习问题（A-CMDP）。我们提出了一种新的算法ACRL，该算法可以保证任意时刻的成本约束，并在实验中验证了其回报性能和成本约束保证。

    

    本文研究了任意时刻竞争性马尔可夫决策过程（A-CMDP）的问题。现有的关于受限制马尔可夫决策过程（CMDP）的研究旨在在随机动态中优化期望回报同时约束期望成本，但是特定情节中的成本仍然可能非常高。相反，A-CMDP的目标是在任意一轮的任何情节中，优化期望回报同时保证有界的成本，并针对策略先验进行了保证。我们提出了一种新的算法，称为Anytime-Competitive Reinforcement Learning（ACRL），它可以证明地保证了任意时刻的成本约束。遗憾分析表明，该策略在任意的竞争性约束下渐近地与最优回报相匹配。在碳智能计算应用中的实验验证了ACRL的回报性能和成本约束保证。

    This paper studies the problem of Anytime-Competitive Markov Decision Process (A-CMDP). Existing works on Constrained Markov Decision Processes (CMDPs) aim to optimize the expected reward while constraining the expected cost over random dynamics, but the cost in a specific episode can still be unsatisfactorily high. In contrast, the goal of A-CMDP is to optimize the expected reward while guaranteeing a bounded cost in each round of any episode against a policy prior. We propose a new algorithm, called Anytime-Competitive Reinforcement Learning (ACRL), which provably guarantees the anytime cost constraints. The regret analysis shows the policy asymptotically matches the optimal reward achievable under the anytime competitive constraints. Experiments on the application of carbon-intelligent computing verify the reward performance and cost constraint guarantee of ACRL.
    
[^2]: 不同的令牌指标：通过测量衰减来修剪LLM组件并优化量化

    Divergent Token Metrics: Measuring degradation to prune away LLM components -- and optimize quantization. (arXiv:2311.01544v1 [cs.CL])

    [http://arxiv.org/abs/2311.01544](http://arxiv.org/abs/2311.01544)

    本研究引入了一种新的方法，即不同的令牌指标（DTM），用于评估压缩后的大型语言模型（LLM）。通过关注令牌的差异性，DTM提供了对模型压缩微妙之处的深入洞察，并且在不损害文本生成质量的情况下可以实现显著的精确度和稀疏度水平。该研究还提出了一种利用DTM进行模型稀疏化和量化的方法，并发现可以修剪掉超过90%的LLM组件和量化超过80%的参数。

    

    大型语言模型（LLM）以其强大的能力改变了自然语言处理。然而，它们不断增长的大小引发了关于它们的有效部署和LLM压缩的担忧。本研究介绍了一种新的评估压缩LLM的方法，即不同的令牌指标（DTM），解决了传统指标如困惑度无法准确反映文本生成质量的局限性。DTM关注令牌的差异性，提供了对模型压缩微妙之处的更深入洞察。我们的结果表明，在不损害文本生成质量的情况下，可以达到显著的精确度和稀疏度水平。此外，DTM还可以更精确地评估每个组件的影响。利用第一个不同的令牌指标（FDTM）在模型稀疏化中显示，超过90%的所有组件可以修剪掉。对于量化，FDTM表明超过80%的参数可以进行量化。

    Large Language Models (LLMs) have reshaped natural language processing with their impressive capabilities. Their ever-increasing size, however, raised concerns about their effective deployment and the need for LLM compressions. This study introduces the Divergent Token metrics (DTMs), a novel approach for assessing compressed LLMs, addressing the limitations of traditional measures like perplexity that fail to accurately reflect text generation quality. DTMs focus on token divergence, providing deeper insights into the subtleties of model compression. Our results indicate that significant levels of precision and sparsity can be achieved without compromising text generation quality. Moreover, DTMs offers a more precise evaluation of each component's impact individually. Utilizing the First Divergent Token metric (FDTM) in model sparsification reveals that nearly 20% of all components can be pruned over 90%. In terms of quantization, the FDTM suggests that over 80% of parameters can be s
    
[^3]: AWEQ：用于大型语言模型的后训练量化和激活权重均衡方法

    AWEQ: Post-Training Quantization with Activation-Weight Equalization for Large Language Models. (arXiv:2311.01305v1 [cs.LG])

    [http://arxiv.org/abs/2311.01305](http://arxiv.org/abs/2311.01305)

    AWEQ是一种后训练量化和激活权重均衡方法，能够在大型语言模型中实现超低位量化和8-bit权重和激活量化，并通过改进的均衡方法减小量化偏差误差，提高模型的鲁棒性。

    

    大型语言模型(LLMs)在各种任务中表现出色，但其计算和存储成本也相对较高。量化这些模型是缓解这个问题的有效方法。然而，现有方法很难在模型准确性和硬件效率之间取得平衡。因此，我们引入了AWEQ，一种后训练方法，不需要额外的训练开销。AWEQ在超低位量化和8-bit权重和激活(W8A8)量化方面表现出色。观察到权重量化比激活量化更容易。AWEQ通过通道均衡将激活量化的难度转移到权重上，实现了两者量化困难的平衡，从而最大化了性能。我们进一步改进了均衡方法，减小了量化偏差误差，确保模型的鲁棒性。在像LLaMA这样的流行模型上进行了大量实验。

    Large language models(LLMs) exhibit excellent performance across a variety of tasks, but they come with significant computational and storage costs. Quantizing these models is an effective way to alleviate this issue. However, existing methods struggle to strike a balance between model accuracy and hardware efficiency. This is where we introduce AWEQ, a post-training method that requires no additional training overhead. AWEQ excels in both ultra-low-bit quantization and 8-bit weight and activation (W8A8) quantization. There is an observation that weight quantization is less challenging than activation quantization. AWEQ transfers the difficulty of activation quantization to weights using channel equalization, achieving a balance between the quantization difficulties of both, and thereby maximizing performance. We have further refined the equalization method to mitigate quantization bias error, ensuring the robustness of the model. Extensive experiments on popular models such as LLaMA a
    
[^4]: 演化流行病学网络的临界转折点：机器学习辅助的数据驱动有效建模

    Tipping Points of Evolving Epidemiological Networks: Machine Learning-Assisted, Data-Driven Effective Modeling. (arXiv:2311.00797v1 [cs.LG])

    [http://arxiv.org/abs/2311.00797](http://arxiv.org/abs/2311.00797)

    该论文通过机器学习辅助的数据驱动建模方法，研究了自适应易感-感染-易感流行病学网络的临界转折点集体动力学。他们识别出了一个有效的随机微分方程以描述网络的演化行为，并观察到了罕见的大幅度集体振荡现象。

    

    我们通过数据驱动的、机器学习辅助的方式研究自适应易感-感染-易感(SIS)流行病学网络的临界转折点集体动力学。我们通过受数值随机积分器启发的深度学习ResNet架构，识别出一个参数相关的基于物理意义的粗粒度均场变量的有效随机微分方程(eSDE)。我们基于eSDE的确定偏移项构建了一个近似的有效分岔图，并将其与均场SIS模型的分岔图进行对比。我们观察到演化网络的有效SIS动力学中的次临界Hopf分岔，它引起了临界转折行为；这表现为大幅度的集体振荡，它们从(噪声的)固定状态的邻域中自发地、罕见地出现。我们通过重复的暴力模拟和使用已建立的数学工具研究了这些罕见事件的统计特性。

    We study the tipping point collective dynamics of an adaptive susceptible-infected-susceptible (SIS) epidemiological network in a data-driven, machine learning-assisted manner. We identify a parameter-dependent effective stochastic differential equation (eSDE) in terms of physically meaningful coarse mean-field variables through a deep-learning ResNet architecture inspired by numerical stochastic integrators. We construct an approximate effective bifurcation diagram based on the identified drift term of the eSDE and contrast it with the mean-field SIS model bifurcation diagram. We observe a subcritical Hopf bifurcation in the evolving network's effective SIS dynamics, that causes the tipping point behavior; this takes the form of large amplitude collective oscillations that spontaneously -- yet rarely -arise from the neighborhood of a (noisy) stationary state. We study the statistics of these rare events both through repeated brute force simulations and by using established mathemati
    
[^5]: 基于CT图像的肾细胞癌病理分类的鲁棒深度学习方法与不确定性估计

    A Robust Deep Learning Method with Uncertainty Estimation for the Pathological Classification of Renal Cell Carcinoma based on CT Images. (arXiv:2311.00567v1 [eess.IV])

    [http://arxiv.org/abs/2311.00567](http://arxiv.org/abs/2311.00567)

    本论文提出了一种基于CT图像的深度学习方法，通过不确定性估计，能够有效分类肾细胞癌的病理亚型，为放射科医师在术前做出诊断提供帮助。

    

    目的：开发和验证一种基于深度学习的诊断模型，包括不确定性估计，以便帮助放射科医师在术前区分基于CT图像的肾细胞癌病理亚型。方法：收集了来自 Center 1 的 668 例连续患者的病理证实的肾细胞癌数据。通过五折交叉验证，开发了一种深度学习模型，包括不确定性估计，用于将肾细胞癌亚型分类为透明细胞癌 (ccRCC)、乳头状细胞癌 (pRCC) 和发色细胞癌 (chRCC)。 Center 2 的 78 例外部验证患者进一步评估了模型的性能。结果：在五折交叉验证中，模型对 ccRCC、pRCC 和 chRCC 的分类的受试者工作特征曲线下面积 (AUC) 分别为 0.868 (95% CI: 0.826-0.923)、0.846 (95% CI: 0.812-0.886) 和 0.839 (95% CI: 0.802-0.88)。在外部验证集中，AUC为...

    Objectives To develop and validate a deep learning-based diagnostic model incorporating uncertainty estimation so as to facilitate radiologists in the preoperative differentiation of the pathological subtypes of renal cell carcinoma (RCC) based on CT images. Methods Data from 668 consecutive patients, pathologically proven RCC, were retrospectively collected from Center 1. By using five-fold cross-validation, a deep learning model incorporating uncertainty estimation was developed to classify RCC subtypes into clear cell RCC (ccRCC), papillary RCC (pRCC), and chromophobe RCC (chRCC). An external validation set of 78 patients from Center 2 further evaluated the model's performance. Results In the five-fold cross-validation, the model's area under the receiver operating characteristic curve (AUC) for the classification of ccRCC, pRCC, and chRCC was 0.868 (95% CI: 0.826-0.923), 0.846 (95% CI: 0.812-0.886), and 0.839 (95% CI: 0.802-0.88), respectively. In the external validation set, the A
    
[^6]: MetisFL:一种可扩展和高效的联邦学习工作流的尴尬并行控制器

    MetisFL: An Embarrassingly Parallelized Controller for Scalable & Efficient Federated Learning Workflows. (arXiv:2311.00334v1 [cs.LG])

    [http://arxiv.org/abs/2311.00334](http://arxiv.org/abs/2311.00334)

    MetisFL是一种可扩展和高效的联邦学习系统，重点关注联邦控制器的可扩展性和优化。

    

    联邦学习(FL)系统通常由两个核心处理实体组成:联邦控制器和学习器。控制器负责管理在学习器之间执行FL工作流程，学习器负责在其私有数据集上训练和评估联邦模型。在执行FL工作流时，FL系统对参与学习器的计算资源或数据没有控制。尽管最近提出了许多FL系统来促进FL工作流的开发，但这些系统中大多数忽视了控制器的可扩展性。为了满足这一需求，我们设计和开发了一种名为MetisFL的新型FL系统，其中联邦控制器是第一等公民。

    A Federated Learning (FL) system typically consists of two core processing entities: the federation controller and the learners. The controller is responsible for managing the execution of FL workflows across learners and the learners for training and evaluating federated models over their private datasets. While executing an FL workflow, the FL system has no control over the computational resources or data of the participating learners. Still, it is responsible for other operations, such as model aggregation, task dispatching, and scheduling. These computationally heavy operations generally need to be handled by the federation controller. Even though many FL systems have been recently proposed to facilitate the development of FL workflows, most of these systems overlook the scalability of the controller. To meet this need, we designed and developed a novel FL system called MetisFL, where the federation controller is the first-class citizen. MetisFL re-engineers all the operations cond
    
[^7]: 在快速发展时代管理人工智能风险

    Managing AI Risks in an Era of Rapid Progress. (arXiv:2310.17688v1 [cs.CY] CROSS LISTED)

    [http://arxiv.org/abs/2310.17688](http://arxiv.org/abs/2310.17688)

    在人工智能快速进展的时代，我们提出了管理即将到来的先进人工智能系统所带来的风险的优先事项。

    

    在这篇简短的共识文中，我们概述了即将到来的先进人工智能系统所带来的风险。我们审查了大规模的社会危害和恶意使用，以及人类对自主人工智能系统失去控制的不可逆转的损失。鉴于人工智能的快速和持续进展，我们提出了人工智能研发和治理的优先事项。

    In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose priorities for AI R&D and governance.
    
[^8]: Causal Q-Aggregation for CATE Model Selection（CATE模型选择中的因果Q集成）

    Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])

    [http://arxiv.org/abs/2310.16945](http://arxiv.org/abs/2310.16945)

    该论文提出了一种基于Q集成的CATE模型选择方法，其通过使用双重鲁棒损失实现了统计上的最佳预测模型选择遗憾率

    

    准确估计条件平均处理效应（CATE）是个性化决策的核心。尽管有大量用于CATE估计的模型，但由于因果推断的基本问题，模型选择是一项非常棘手的任务。最近的实证工作提供了有利于具有双重鲁棒性质的代理损失度量和模型集成的证据。然而，对于这些模型的理论理解还不够。直接应用先前的理论工作会由于模型选择问题的非凸性而导致次优的预测模型选择率。我们提供了现有主要CATE集成方法的遗憾率，并提出了一种基于双重鲁棒损失的Q集成的新的CATE模型集成方法。我们的主要结果表明，因果Q集成在预测模型选择的遗憾率上达到了统计上的最优值为$\frac{\log(M)}{n}$（其中$M$为模型数，$n$为样本数），加上高阶估计误差项

    Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
    
[^9]: 可解释的基于路径的知识图推荐中的忠实路径语言建模

    Faithful Path Language Modelling for Explainable Recommendation over Knowledge Graph. (arXiv:2310.16452v1 [cs.IR])

    [http://arxiv.org/abs/2310.16452](http://arxiv.org/abs/2310.16452)

    本文提出了一个名为PEARLM的方法，通过语言建模开展基于路径的知识图谱推荐，解决了现有方法中对预训练知识图谱嵌入的依赖以及未充分利用实体和关系之间相互依赖性的问题，还避免了生成不准确的解释。实验结果表明，与现有方法相比，我们的方法效果显著。

    

    针对知识图谱中的路径推理方法在提高推荐系统透明度方面的潜力，本文提出了一种名为PEARLM的新方法，该方法通过语言建模有效捕获用户行为和产品端知识。我们的方法通过语言模型直接从知识图谱上的路径中学习知识图谱嵌入，并将实体和关系统一在同一优化空间中。序列解码的约束保证了路径对知识图谱的忠实性。在两个数据集上的实验证明了我们方法与现有最先进方法的有效性。

    Path reasoning methods over knowledge graphs have gained popularity for their potential to improve transparency in recommender systems. However, the resulting models still rely on pre-trained knowledge graph embeddings, fail to fully exploit the interdependence between entities and relations in the KG for recommendation, and may generate inaccurate explanations. In this paper, we introduce PEARLM, a novel approach that efficiently captures user behaviour and product-side knowledge through language modelling. With our approach, knowledge graph embeddings are directly learned from paths over the KG by the language model, which also unifies entities and relations in the same optimisation space. Constraints on the sequence decoding additionally guarantee path faithfulness with respect to the KG. Experiments on two datasets show the effectiveness of our approach compared to state-of-the-art baselines. Source code and datasets: AVAILABLE AFTER GETTING ACCEPTED.
    
[^10]: VQ-NeRF: 基于向量量化的神经反射分解与编辑

    VQ-NeRF: Neural Reflectance Decomposition and Editing with Vector Quantization. (arXiv:2310.11864v1 [cs.CV])

    [http://arxiv.org/abs/2310.11864](http://arxiv.org/abs/2310.11864)

    VQ-NeRF是一个基于向量量化的神经网络模型，用于分解和编辑3D场景中的反射场。通过将连续材料离散化，该模型可以减少噪声并生成离散材料的分割地图，从而实现简化的材料编辑。

    

    我们提出了VQ-NeRF，这是一个具有向量量化（VQ）的双分支神经网络模型，用于对3D场景中的反射场进行分解和编辑。传统的神经反射场仅使用连续表示来建模3D场景，尽管现实中的物体通常由离散材料组成。这种缺乏离散化可能导致材料分解噪声和复杂的材料编辑。为了解决这些限制，我们的模型包括一个连续分支和一个离散分支。连续分支按照传统流程预测分解的材料，而离散分支使用VQ机制将连续材料量化为单独的材料。通过离散化材料，我们的模型可以减少分解过程中的噪声，并生成离散材料的分割地图。可以通过点击分割结果的相应区域来轻松选择特定材料进行进一步编辑。

    We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Ad
    
[^11]: 自我评估的自适应改进LLMs中的选择性预测

    Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs. (arXiv:2310.11689v1 [cs.CL])

    [http://arxiv.org/abs/2310.11689](http://arxiv.org/abs/2310.11689)

    本研究提出了一种自适应框架，利用自我评估来改进大型语言模型（LLMs）的选择性预测能力。该方法基于参数效率调整，能够适应特定任务并提高其自我评估能力，实验结果表明其优于最先进的选择性预测方法。

    

    大型语言模型（LLMs）在自然语言理解和生成等多种任务中取得了巨大进展。然而，在高风险决策场景中仍然限于其潜在的错误。选择性预测是一种可以通过在LLMs不确定时使其避免预测而提高其可靠性的技术。在本文中，我们提出了一种新颖的自我评估的自适应框架，以提高LLMs的选择性预测性能。我们的框架基于使用参数效率调整来适应特定任务并改进其自我评估能力的思想。我们在各种问答（QA）数据集上评估了我们的方法，并展示其优于最先进的选择性预测方法。例如，在CoQA基准测试中，我们的方法将AUACC从91.23%提高到92.63%，并将AURO

    Large language models (LLMs) have recently shown great advances in a variety of tasks, including natural language understanding and generation. However, their use in high-stakes decision-making scenarios is still limited due to the potential for errors. Selective prediction is a technique that can be used to improve the reliability of the LLMs by allowing them to abstain from making predictions when they are unsure of the answer. In this work, we propose a novel framework for adaptation with self-evaluation to improve the selective prediction performance of LLMs. Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation. We evaluate our method on a variety of question-answering (QA) datasets and show that it outperforms state-of-the-art selective prediction methods. For example, on the CoQA benchmark, our method improves the AUACC from 91.23% to 92.63% and improves the AURO
    
[^12]: PREM:一种简单而有效的节点级图异常检测方法。

    PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection. (arXiv:2310.11676v1 [cs.LG])

    [http://arxiv.org/abs/2310.11676](http://arxiv.org/abs/2310.11676)

    PREM是一种简单而有效的节点级图异常检测方法，它通过简化图异常检测的过程，减少了时间和内存消耗，同时保持了强大的异常检测能力。

    

    节点级图异常检测在识别医学、社交网络和电子商务等各个领域中的图结构数据中的异常节点起着关键作用。然而，由于异常的多样性以及标注数据的匮乏，已有的基于重构和对比学习的方法往往在效率方面存在问题，这源于它们复杂的目标和繁琐的模块。为了提高图异常检测的效率，我们引入了一种简单的方法，称为PREprocessing and Matching（简称PREM）。我们的方法简化了图异常检测，减少了时间和内存的消耗，同时保持了强大的异常检测能力。PREM由两个模块组成：预处理模块和邻居匹配模块。PREM在训练过程中消除了消息传递传播的必要性，并采用了简单的对比损失函数，从而大大减少了训练时间和内存使用量。此外，

    Node-level graph anomaly detection (GAD) plays a critical role in identifying anomalous nodes from graph-structured data in various domains such as medicine, social networks, and e-commerce. However, challenges have arisen due to the diversity of anomalies and the dearth of labeled data. Existing methodologies reconstruction-based and contrastive learning - while effective, often suffer from efficiency issues, stemming from their complex objectives and elaborate modules. To improve the efficiency of GAD, we introduce a simple method termed PREprocessing and Matching (PREM for short). Our approach streamlines GAD, reducing time and memory consumption while maintaining powerful anomaly detection capabilities. Comprising two modules - a pre-processing module and an ego-neighbor matching module - PREM eliminates the necessity for message-passing propagation during training, and employs a simple contrastive loss, leading to considerable reductions in training time and memory usage. Moreov
    
[^13]: 用于识别半导体晶圆地图中缺陷模式的机器学习技术：一项调查、实证和实验评估

    Machine Learning Techniques for Identifying the Defective Patterns in Semiconductor Wafer Maps: A Survey, Empirical, and Experimental Evaluations. (arXiv:2310.10705v1 [cs.LG])

    [http://arxiv.org/abs/2310.10705](http://arxiv.org/abs/2310.10705)

    本文综述了利用机器学习技术在半导体制造中识别晶圆缺陷的方法，提出了一种创新的分类体系，为不同算法和技术之间的关系提供了深入分析，并通过严谨的评估验证了算法性能。

    

    本文综述了利用机器学习（ML）技术识别半导体制造中晶圆缺陷的方法学。尽管越来越多的研究证明了ML在晶圆缺陷识别中的有效性，但在这个主题上缺乏全面的综述。本文试图弥补这个空白，通过整合现有文献，深入分析各种ML算法在晶圆缺陷检测领域的优势、局限性和潜在应用。我们提出了一种创新的方法学分类体系，详细分类了算法，并提供了更细致的子技术划分。这个分类体系从广泛的方法学类别开始，到具体的子技术结束。它帮助研究人员理解不同算法以及它们的技术之间的复杂关系。我们采用严谨的实证和实验评估来验证算法性能。

    This survey paper offers a comprehensive review of methodologies utilizing machine learning (ML) techniques for identifying wafer defects in semiconductor manufacturing. Despite the growing body of research demonstrating the effectiveness of ML in wafer defect identification, there is a noticeable absence of comprehensive reviews on this subject. This survey attempts to fill this void by amalgamating available literature and providing an in-depth analysis of the advantages, limitations, and potential applications of various ML algorithms in the realm of wafer defect detection. An innovative taxonomy of methodologies that we present provides a detailed classification of algorithms into more refined categories and techniques. This taxonomy follows a four-tier structure, starting from broad methodology categories and ending with specific sub-techniques. It aids researchers in comprehending the complex relationships between different algorithms and their techniques. We employ a rigorous em
    
[^14]: 在不探索的情况下学习联合波束成形的RL策略：一种批量约束的离策略方法

    Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach. (arXiv:2310.08660v1 [cs.LG])

    [http://arxiv.org/abs/2310.08660](http://arxiv.org/abs/2310.08660)

    本研究提出了一种批量约束的离策略方法，用于解决联合波束成形的参数优化问题。通过使用深度强化学习技术，克服了传统方法中计算复杂度高和无法准确建模的难题。

    

    在这个项目中，我们考虑了网络参数优化的问题，以实现速率最大化。我们将其构建为功率控制、波束成形和干扰消除的联合优化问题。我们考虑了多个基站与多个用户设备通信的情况。由于穷举搜索的指数计算复杂度，我们使用深度强化学习（RL）技术来解决该非凸优化问题。现代通信系统以其难以准确建模的行为而臭名昭著。这限制了我们使用基于RL的算法，因为需要与环境进行交互以便代理能够高效地探索和学习。此外，由于失败的高成本，将算法部署在现实世界中进行探索和学习是不明智的。与先前提出的基于RL的解决方案（如基于深度Q网络（DQN）的控制）相比，我们提出采用一种离策略方法

    In this project, we consider the problem of network parameter optimization for rate maximization. We frame this as a joint optimization problem of power control, beam forming, and interference cancellation. We consider the setting where multiple Base Stations (BSs) are communicating with multiple user equipments (UEs). Because of the exponential computational complexity of brute force search, we instead solve this non-convex optimization problem using deep reinforcement learning (RL) techniques. The modern communication systems are notorious for their difficulty in exactly modeling their behaviour. This limits us in using RL based algorithms as interaction with the environment is needed for the agent to explore and learn efficiently. Further, it is ill advised to deploy the algorithm in real world for exploration and learning because of the high cost of failure. In contrast to the previous RL-based solutions proposed, such as deep-Q network (DQN) based control, we propose taking an off
    
[^15]: 探索有限领域知识传递的基本限制

    Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])

    [http://arxiv.org/abs/2310.07838](http://arxiv.org/abs/2310.07838)

    本论文研究了在有限领域中从教师到学生分类器进行知识传递的统计效率，发现特权信息会加速传递，通过使用一种新颖的损失函数达到了知识传递的基本限制。

    

    我们对通过从教师到概率化学生分类器的n个样本进行知识传递的统计效率进行了表征，其中输入空间S和标签A为有限域。我们发现，在三个渐进级别上的特权信息可以加快传递的速度。在第一级别上，只有具有困难标签的样本是已知的，最大似然估计器能够达到最小化速率sqrt(|S||A|/n)。第二级别上，除了已知的困难标签样本外，还有采样标签的教师概率可用，这将收敛速度的下界提高到|S||A|/n。然而，在第二个数据采集协议下，最小化交叉熵损失的朴素适应会导致渐近偏差的学生。我们克服了这个限制，并通过使用一种新颖的经验变体的平方误差逻辑损失来实现了基本限制。第三级别进一步赋予学生软标签。

    We characterize the statistical efficiency of knowledge transfer through $n$ samples from a teacher to a probabilistic student classifier with input space $\mathcal S$ over labels $\mathcal A$. We show that privileged information at three progressive levels accelerates the transfer. At the first level, only samples with hard labels are known, via which the maximum likelihood estimator attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The second level has the teacher probabilities of sampled labels available in addition, which turns out to boost the convergence rate lower bound to ${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data acquisition protocol, minimizing a naive adaptation of the cross-entropy loss results in an asymptotically biased student. We overcome this limitation and achieve the fundamental limit by using a novel empirical variant of the squared error logit loss. The third level further equips the student with the soft labels (com
    
[^16]: 在数据-参数域上，联合群不变函数引导了通用神经网络

    Joint Group Invariant Functions on Data-Parameter Domain Induce Universal Neural Networks. (arXiv:2310.03530v1 [cs.LG])

    [http://arxiv.org/abs/2310.03530](http://arxiv.org/abs/2310.03530)

    本研究通过探索联合群不变函数在数据-参数域上的作用，提出了一种系统的规则来解码神经网络内部数据表示中的对称性和几何性。利用这一规则，我们引入了由联合不变函数导出的通用神经网络，并利用群论证明了其普适性。这一研究揭示了逼近理论和深度学习中的群论方面，并将几何深度学习与抽象调和分析相连接。

    

    将输入数据的对称性和几何性考虑为编码在神经网络内部数据表示中，但是具体的编码规则还没有得到深入研究。通过关注数据-参数域上的联合群不变函数，我们提出了一种系统的规则，从数据域上的群作用中找到参数域上的双重群作用。此外，我们引入了由联合不变函数导出的广义神经网络，并利用Schur引理给出了它们的普遍性定理的新的群论证明。由于传统的普遍性定理是基于函数分析方法进行证明的，这项研究揭示了逼近理论的群论方面，将几何深度学习与抽象调和分析相连接。

    The symmetry and geometry of input data are considered to be encoded in the internal data representation inside the neural network, but the specific encoding rule has been less investigated. By focusing on a joint group invariant function on the data-parameter domain, we present a systematic rule to find a dual group action on the parameter domain from a group action on the data domain. Further, we introduce generalized neural networks induced from the joint invariant functions, and present a new group theoretic proof of their universality theorems by using Schur's lemma. Since traditional universality theorems were demonstrated based on functional analytical methods, this study sheds light on the group theoretic aspect of the approximation theory, connecting geometric deep learning to abstract harmonic analysis.
    
[^17]: 深度脊波变换：使用Koopman算子证明了形式深度网络的普适性

    Deep Ridgelet Transform: Voice with Koopman Operator Proves Universality of Formal Deep Networks. (arXiv:2310.03529v1 [cs.LG])

    [http://arxiv.org/abs/2310.03529](http://arxiv.org/abs/2310.03529)

    通过对数据空间上的群作用来识别DNN内部的隐藏层，并将DNN构建为相对于Koopman算子的双声变换，我们利用群论论证证明了这些DNN的普适性。

    

    我们通过对数据空间上的群作用来识别DNN内部的隐藏层，并将DNN构建为相对于Koopman算子的双声变换，Koopman算子是群作用的线性表示。基于群论论证，特别是利用Schur引理，我们给出了这些DNN普适性的简单证明。

    We identify hidden layers inside a DNN with group actions on the data space, and formulate the DNN as a dual voice transform with respect to Koopman operator, a linear representation of the group action. Based on the group theoretic arguments, particularly by using Schur's lemma, we show a simple proof of the universality of those DNNs.
    
[^18]: Point-PEFT: 用于3D预训练模型的参数高效微调

    Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v1 [cs.CV])

    [http://arxiv.org/abs/2310.03059](http://arxiv.org/abs/2310.03059)

    Point-PEFT是一种用于3D预训练模型的参数高效微调框架，它通过冻结大部分参数，只微调新增的PEFT模块，包括Point-prior Prompt和Geometry-aware Adapter，以最小化学习参数，并利用内存库和准确的聚合方法来提高模型性能。

    

    大型预训练模型的流行已经彻底改变了语言、视觉和多模态等领域的下游任务。为了降低下游任务的适应成本，许多参数高效微调（PEFT）技术被提出用于语言和2D图像预训练模型。然而，对于3D预训练模型的专门PEFT方法仍未得到充分探索。为此，我们引入了Point-PEFT，一种用于适应点云预训练模型的新型框架，其具有最少的可学习参数。具体而言，对于预训练的3D模型，我们冻结大部分参数，只微调新增的PEFT模块。这些模块包括Point-prior Prompt和Geometry-aware Adapter。Point-prior Prompt采用一组可学习的提示标记，并提出使用具有领域特定知识的内存库来增强提示标记的参数无关的注意力机制。Geometry-aware Adapter旨在对不同任务或数据进行准确地聚合。

    The popularity of pre-trained large models has revolutionized downstream tasks across diverse fields, such as language, vision, and multi-modality. To minimize the adaption cost for downstream tasks, many Parameter-Efficient Fine-Tuning (PEFT) techniques are proposed for language and 2D image pre-trained models. However, the specialized PEFT method for 3D pre-trained models is still under-explored. To this end, we introduce Point-PEFT, a novel framework for adapting point cloud pre-trained models with minimal learnable parameters. Specifically, for a pre-trained 3D model, we freeze most of its parameters, and only tune the newly added PEFT modules on downstream tasks, which consist of a Point-prior Prompt and a Geometry-aware Adapter. The Point-prior Prompt adopts a set of learnable prompt tokens, for which we propose to construct a memory bank with domain-specific knowledge, and utilize a parameter-free attention to enhance the prompt tokens. The Geometry-aware Adapter aims to aggrega
    
[^19]: De-SaTE：用于锂离子电池健康预测的去噪自注意力变换编码器

    De-SaTE: Denoising Self-attention Transformer Encoders for Li-ion Battery Health Prognostics. (arXiv:2310.00023v1 [cs.LG])

    [http://arxiv.org/abs/2310.00023](http://arxiv.org/abs/2310.00023)

    本研究提出了De-SaTE方法，通过利用多个去噪模块以及自注意力变换编码器，准确预测锂离子电池的剩余寿命（RUL），为预防性维护和预测性分析提供关键指标估计。

    

    锂离子电池在各个行业中得到了广泛应用，从为便携式电子设备供电到推动电动汽车和支持能源存储系统。有效管理锂离子电池的一个核心挑战是准确预测其剩余寿命（RUL），这是预防性维护和预测性分析的关键指标。本研究提出了一种新的方法，利用多个去噪模块的能量，每个模块都经过训练来处理电池数据中常见的噪声类型。具体而言，我们使用去噪自动编码器和小波去噪器来生成编码/分解表示，然后将其通过专用自注意力变换编码器进行处理。在NASA和CALCE数据集上进行了大量实验后，我们能够表征多种噪声模式下的广泛健康指标估计。我们发现我们报告的误差

    Lithium Ion (Li-ion) batteries have gained widespread popularity across various industries, from powering portable electronic devices to propelling electric vehicles and supporting energy storage systems. A central challenge in managing Li-ion batteries effectively is accurately predicting their Remaining Useful Life (RUL), which is a critical measure for proactive maintenance and predictive analytics. This study presents a novel approach that harnesses the power of multiple denoising modules, each trained to address specific types of noise commonly encountered in battery data. Specifically we use a denoising auto-encoder and a wavelet denoiser to generate encoded/decomposed representations, which are subsequently processed through dedicated self-attention transformer encoders. After extensive experimentation on the NASA and CALCE datasets, we are able to characterize a broad spectrum of health indicator estimations under a set of diverse noise patterns. We find that our reported error
    
[^20]: 使用深度条件生成时间序列模型生成个性化的胰岛素治疗策略

    Generating Personalized Insulin Treatments Strategies with Deep Conditional Generative Time Series Models. (arXiv:2309.16521v1 [stat.ML])

    [http://arxiv.org/abs/2309.16521](http://arxiv.org/abs/2309.16521)

    本论文提出了一种使用深度生成时间序列模型和决策理论相结合的新框架，用于生成个性化的胰岛素治疗策略。通过学习生成逼真的个性化治疗和未来结果轨迹，可以为个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。

    

    我们提出了一种新的框架，将深度生成时间序列模型与决策理论相结合，用于生成个性化的治疗策略。它利用历史患者轨迹数据，通过深度生成时间序列模型共同学习生成逼真的个性化治疗和未来结果轨迹。特别地，我们的框架可以根据条件化期望效用最大化训练生成与个性化患者历史匹配且针对最佳未来效果的新型多变量治疗策略。我们通过为住院糖尿病患者生成个性化的胰岛素治疗策略和血糖预测来展示我们的框架，展示了我们的方法在生成改进的个性化治疗策略方面的潜力。

    We propose a novel framework that combines deep generative time series models with decision theory for generating personalized treatment strategies. It leverages historical patient trajectory data to jointly learn the generation of realistic personalized treatment and future outcome trajectories through deep generative time series models. In particular, our framework enables the generation of novel multivariate treatment strategies tailored to the personalized patient history and trained for optimal expected future outcomes based on conditional expected utility maximization. We demonstrate our framework by generating personalized insulin treatment strategies and blood glucose predictions for hospitalized diabetes patients, showcasing the potential of our approach for generating improved personalized treatment strategies. Keywords: deep generative model, probabilistic decision support, personalized treatment generation, insulin and blood glucose prediction
    
[^21]: 从时间序列电子健康记录学习患者静态信息及保护隐私和公平性的方法

    Learning Patient Static Information from Time-series EHR and an Approach for Safeguarding Privacy and Fairness. (arXiv:2309.11373v1 [cs.LG])

    [http://arxiv.org/abs/2309.11373](http://arxiv.org/abs/2309.11373)

    本文研究了从时间序列电子健康记录数据预测患者静态信息的能力，并发现不仅原始数据，机器学习模型学习到的表示也可以用于预测各种静态信息，包括生物学性别、二值化年龄和自报种族，具有高预测性能。

    

    最近的医疗机器学习研究引起了人们对患者隐私和算法公平性的关注。例如，之前的研究表明，即使医疗数据中没有明确包含种族信息，也可以通过医疗数据预测患者自报的种族信息。然而，我们对数据识别的程度没有了解，也缺乏开发模型以最小程度受到这些信息影响的方法。在这里，我们系统地研究了时间序列电子健康记录数据预测患者静态信息的能力。我们发现，不仅原始的时间序列数据，还有从机器学习模型学习到的表示，都可以被训练用来预测各种静态信息，其接收者操作特征曲线下面积可达到0.851（对生物学性别）、0.869（对二值化年龄）和0.810（对自报种族）。这种高预测性能可以扩展到广泛的共病因素，并且即使在模型被解释时也存在。

    Recent work in machine learning for healthcare has raised concerns about patient privacy and algorithmic fairness. For example, previous work has shown that patient self-reported race can be predicted from medical data that does not explicitly contain racial information. However, the extent of data identification is unknown, and we lack ways to develop models whose outcomes are minimally affected by such information. Here we systematically investigated the ability of time-series electronic health record data to predict patient static information. We found that not only the raw time-series data, but also learned representations from machine learning models, can be trained to predict a variety of static information with area under the receiver operating characteristic curve as high as 0.851 for biological sex, 0.869 for binarized age and 0.810 for self-reported race. Such high predictive performance can be extended to a wide range of comorbidity factors and exists even when the model was
    
[^22]: 朝着使用更少标注实现群体鲁棒性的最后一层再训练

    Towards Last-layer Retraining for Group Robustness with Fewer Annotations. (arXiv:2309.08534v1 [cs.LG])

    [http://arxiv.org/abs/2309.08534](http://arxiv.org/abs/2309.08534)

    本研究发现，在没有群体标注和只有少量类别标注的情况下，最后一层再训练仍然可以有效提高最差群体准确性，从而优于经验风险最小化在整个数据集上的表现，可以实现群体鲁棒性的提升。

    

    神经网络的经验风险最小化(ERM)容易过度依赖虚假相关性，并在少数群体上具有较差的泛化性能。最近的深度特征再赋权(DFR)技术通过简单的最后一层再训练实现了最先进的群体保护性能，但它需要保留群体和类别的标注，并构建一个群体平衡的再赋权数据集。在这项工作中，我们研究了这个不切实际的要求，并发现即使没有群体标注（除了模型选择），只有少量的类别标注，最后一层再训练仍然可以出人意料地有效。我们首先证明了即使再赋权数据集中仅有一小部分最差群体数据，最后一层再训练仍然可以显著提高最差群体准确性。这意味着通过保留一部分训练数据来重新训练最后一层，可以在没有额外数据或标注的情况下，显著优于对整个数据集进行ERM。为了进一步提高群体鲁棒性，我们...

    Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent deep feature reweighting (DFR) technique achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a "free lunch" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data or annotations. To further improve group robustness, we i
    
[^23]: SLiMe: 像我一样进行分割

    SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])

    [http://arxiv.org/abs/2309.03179](http://arxiv.org/abs/2309.03179)

    基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。

    

    使用大型视觉语言模型（如稳定扩散SD），在诸多下游任务（包括图像编辑、图像对应和3D形状生成）方面取得了显著进展。受到这些进展的启发，我们探索利用这些广泛的视觉语言模型，通过提出SLiMe，以尽可能少的标注样本对图像进行任意细粒度的分割。SLiMe将这个问题作为一个优化任务来进行。具体而言，给定一张训练图像及其分割掩膜，我们首先从SD先验中提取注意力图，包括我们的新颖的“加权累积自注意力图”。然后，利用提取的注意力图，优化稳定扩散的文本嵌入，使得每个嵌入只学习训练图像中的一个分割区域。这些学习到的嵌入然后在注意力图中突出显示分割区域，从而可以生成分割图。这使得SLiMe可以进行图像分割。

    Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
    
[^24]: 使用迁移学习解决光学矩阵乘法模型中的数据稀缺问题

    Addressing Data Scarcity in Optical Matrix Multiplier Modeling Using Transfer Learning. (arXiv:2308.11630v1 [cs.LG])

    [http://arxiv.org/abs/2308.11630](http://arxiv.org/abs/2308.11630)

    本研究使用迁移学习解决了光学矩阵乘法模型训练中的数据稀缺问题，并通过利用合成数据进行预训练和实验数据进行微调，成功减小了建模误差，在仅使用25%数据的情况下实现对矩阵权重的小于1 dB的均方根误差。

    

    我们提出并通过实验证明使用迁移学习可以解决训练Mach-Zehnder干涉仪网状光学矩阵乘法器的神经网络模型时面临的实验数据稀缺问题。我们的方法包括使用从不太准确的解析模型生成的合成数据进行预训练，并使用实验数据进行微调。我们的研究表明，与使用解析模型或独立神经网络模型相比，这种方法可以显著减小建模误差。通过利用正则化技术和集成平均，我们在仅使用25%可用数据的情况下，实现了对由光子芯片实现的矩阵权重的小于1 dB的均方根误差。

    We present and experimentally evaluate using transfer learning to address experimental data scarcity when training neural network (NN) models for Mach-Zehnder interferometer mesh-based optical matrix multipliers. Our approach involves pre-training the model using synthetic data generated from a less accurate analytical model and fine-tuning with experimental data. Our investigation demonstrates that this method yields significant reductions in modeling errors compared to using an analytical model, or a standalone NN model when training data is limited. Utilizing regularization techniques and ensemble averaging, we achieve < 1 dB root-mean-square error on the matrix weights implemented by a photonic chip while using only 25% of the available data.
    
[^25]: 激活添加: 无需优化即可操纵语言模型

    Activation Addition: Steering Language Models Without Optimization. (arXiv:2308.10248v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2308.10248](http://arxiv.org/abs/2308.10248)

    这项研究探讨了一种在推理时通过改变激活来预测性地改变语言模型行为的方法，并且相比于传统方法具有更低的计算和实施成本，并且能够保持模型性能。

    

    可靠地控制大型语言模型的行为是一个紧迫的开放性问题。现有的方法包括有监督微调、根据人类反馈进行强化学习、提示工程和引导解码。我们相反，研究了激活工程：在推理时修改激活以可预测地改变模型行为。特别地，我们通过自然语言隐式指定了一个添加的“导向向量”来偏置前向传播。与以前学习这些导向向量的工作不同，我们的激活添加（ActAdd）方法通过计算来自提示对的激活差异来计算它们。我们在OpenWebText和ConceptNet上展示了ActAdd在GPT-2上的应用。我们的推理时方法控制了输出的高级属性并保持了非目标模型的性能。它所需的计算和实施工作比微调要少得多，允许用户提供自然语言的规范，并且其开销与模型规模自然地扩展。

    Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language.  Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with m
    
[^26]: LLM中的时间旅行：追踪大型语言模型中的数据污染

    Time Travel in LLMs: Tracing Data Contamination in Large Language Models. (arXiv:2308.08493v1 [cs.CL])

    [http://arxiv.org/abs/2308.08493](http://arxiv.org/abs/2308.08493)

    该论文提出了一种用于识别大型语言模型（LLMs）中数据污染的简单而有效的方法。通过对随机样本中的单个实例进行分析，以及使用“引导指令”来评估整个数据集分区的污染程度，可以准确地识别污染的实例和分区。

    

    数据污染是指大型语言模型（LLMs）的训练数据中存在来自下游任务的测试数据，这可能是理解LLMs在其他任务上有效性的一个重要问题。我们提出了一种简单而有效的方法来识别LLMs中的数据污染。我们的方法核心是通过识别从小的随机样本中抽取的单个实例中的潜在污染，然后评估整个数据集分区是否受到污染。为了估计单个实例的污染程度，我们使用了“引导指令”：即一个由数据集名称、分区类型和参考实例的初始部分组成的提示，要求LLM完成它。如果LLM的输出与参考实例的后一部分完全或接近匹配，那么该实例被标记为受到污染。为了了解整个分区是否受到污染，我们提出了两个想法。第一个想法是标记一个数据集的分区，该分区中的实例大多数都被判断为受到污染。

    Data contamination, i.e., the presence of test data from downstream tasks in the training data of large language models (LLMs), is a potential major issue in understanding LLMs' effectiveness on other tasks. We propose a straightforward yet effective method for identifying data contamination within LLMs. At its core, our approach starts by identifying potential contamination in individual instances that are drawn from a small random sample; using this information, our approach then assesses if an entire dataset partition is contaminated. To estimate contamination of individual instances, we employ "guided instruction:" a prompt consisting of the dataset name, partition type, and the initial segment of a reference instance, asking the LLM to complete it. An instance is flagged as contaminated if the LLM's output either exactly or closely matches the latter segment of the reference. To understand if an entire partition is contaminated, we propose two ideas. The first idea marks a dataset
    
[^27]: 从合成语料库和形式逻辑学习演绎推理

    Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v1 [cs.AI])

    [http://arxiv.org/abs/2308.07336](http://arxiv.org/abs/2308.07336)

    本研究研究了一种从合成语料库中学习演绎推理能力的方法，通过采用基于形式逻辑理论的演绎规则，训练的语言模型具有更泛化的推理能力。

    

    我们研究了一种从合成语料库中学习演绎推理能力的语言模型（LMs）方法。之前的研究使用了具体的演绎规则来生成演绎示例，但这些规则受限或者是任意的。这可能限制了所获得演绎推理能力的泛化能力。我们重新思考并采用基于形式逻辑理论的一组良好基础的演绎规则，当这些规则以多步方式组合时，可以推导出任何其他演绎规则。我们通过实验证明，在提出的语料库上训练的LMs，即$\textbf{FLD}$（$\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction），获得了更具泛化性的演绎推理能力。此外，我们确定了演绎推理语料库可以增强LMs的推理能力的方面，以及不同方面无法增强的方面。最后，基于这些结果，我们讨论了将演绎语料库或其他方法应用于每个方面的未来方向。

    We study a synthetic corpus-based approach for language models (LMs) to acquire logical deductive reasoning ability. The previous studies generated deduction examples using specific sets of deduction rules. However, these rules were limited or otherwise arbitrary. This can limit the generalizability of acquired deductive reasoning ability. We rethink this and adopt a well-grounded set of deduction rules based on formal logic theory, which can derive any other deduction rules when combined in a multistep way. We empirically verify that LMs trained on the proposed corpora, which we name $\textbf{FLD}$ ($\textbf{F}$ormal $\textbf{L}$ogic $\textbf{D}$eduction), acquire more generalizable deductive reasoning ability. Furthermore, we identify the aspects of deductive reasoning ability on which deduction corpora can enhance LMs and those on which they cannot. Finally, on the basis of these results, we discuss the future directions for applying deduction corpora or other approaches for each as
    
[^28]: RTLLM：用于大规模语言模型设计RTL生成的开源基准测试

    RTLLM: An Open-Source Benchmark for Design RTL Generation with Large Language Model. (arXiv:2308.05345v1 [cs.LG])

    [http://arxiv.org/abs/2308.05345](http://arxiv.org/abs/2308.05345)

    RTLLM是一个用自然语言指令生成设计RTL的开源基准测试，旨在解决现有工作中目标设计简单且规模小的问题，并提供对基于LLM的解决方案进行设计质量的定量评估。

    

    受到像ChatGPT这样的大型语言模型（LLMs）的最新成功的启发，研究人员开始探索采用LLMs进行敏捷硬件设计，例如基于自然语言指令生成设计RTL。然而，在现有的工作中，目标设计都相对简单且规模较小，并由作者自己提出，这使得在不同的LLMs解决方案之间进行公平比较具有挑战性。此外，许多先前的工作只关注设计的正确性，而没有评估生成的设计RTL的设计质量。在这项工作中，我们提出了一个名为RTLLM的开源基准测试，用于使用自然语言指令生成设计RTL。为了系统评估自动生成的设计RTL，我们总结了三个渐进目标，即语法目标、功能目标和设计质量目标。该基准测试可以自动提供对任何给定基于LLM的解决方案的定量评估。

    Inspired by the recent success of large language models (LLMs) like ChatGPT, researchers start to explore the adoption of LLMs for agile hardware design, such as generating design RTL based on natural-language instructions. However, in existing works, their target designs are all relatively simple and in a small scale, and proposed by the authors themselves, making a fair comparison among different LLM solutions challenging. In addition, many prior works only focus on the design correctness, without evaluating the design qualities of generated design RTL. In this work, we propose an open-source benchmark named RTLLM, for generating design RTL with natural language instructions. To systematically evaluate the auto-generated design RTL, we summarized three progressive goals, named syntax goal, functionality goal, and design quality goal. This benchmark can automatically provide a quantitative evaluation of any given LLM-based solution. Furthermore, we propose an easy-to-use yet surprisin
    
[^29]: 电力市场的价格感知深度学习

    Price-Aware Deep Learning for Electricity Markets. (arXiv:2308.01436v1 [cs.LG])

    [http://arxiv.org/abs/2308.01436](http://arxiv.org/abs/2308.01436)

    本文提出了一种价格感知深度学习方法，通过将电力市场清算优化嵌入到深度学习层中，平衡预测误差和定价误差，提高公平性，并控制价格误差的空间分布。

    

    尽管深度学习逐渐渗透到运营规划中，但其固有的预测误差可能会显著影响电力价格。本文考察了预测误差如何传播到电力价格中，揭示了拥挤电力系统中显著的定价误差及其空间差异。为了提高公平性，我们提出将电力市场清算优化嵌入到深度学习层中。通过此层的区分，可以在预测误差和定价误差之间平衡，而不仅仅是最小化预测误差。该层隐式地优化公平性，并控制系统中价格误差的空间分布。我们展示了在风力发电预测和短期电力市场清算中的价格感知深度学习。

    While deep learning gradually penetrates operational planning, its inherent prediction errors may significantly affect electricity prices. This letter examines how prediction errors propagate into electricity prices, revealing notable pricing errors and their spatial disparity in congested power systems. To improve fairness, we propose to embed electricity market-clearing optimization as a deep learning layer. Differentiating through this layer allows for balancing between prediction and pricing errors, as oppose to minimizing prediction errors alone. This layer implicitly optimizes fairness and controls the spatial distribution of price errors across the system. We showcase the price-aware deep learning in the nexus of wind power forecasting and short-term electricity market clearing.
    
[^30]: ODTlearn: 一个用于学习预测和处方的最优决策树的包

    ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription. (arXiv:2307.15691v1 [stat.ML])

    [http://arxiv.org/abs/2307.15691](http://arxiv.org/abs/2307.15691)

    ODTlearn是一个开源的Python包，用于学习预测和处方的最优决策树。它提供了多种优化方法，并支持各种问题和算法的扩展。

    

    ODTLearn是一个开源的Python包，提供了基于混合整数优化(MIO)框架的高风险预测和处方任务的最优决策树学习方法。该包的当前版本提供了学习最优分类树、公平最优分类树、鲁棒最优分类树和从观测数据学习最优处方树的实现。我们设计了该包以便于维护和扩展，当引入新的最优决策树问题类、重构策略和解决算法时，可以轻松更新。为此，该包遵循面向对象的设计原则，并支持商业(Gurobi)和开源(COIN-OR branch and cut)求解器。包的文档和详细用户指南可以在https://d3m-research-group.github.io/odtlearn/找到。

    ODTLearn is an open-source Python package that provides methods for learning optimal decision trees for high-stakes predictive and prescriptive tasks based on the mixed-integer optimization (MIO) framework proposed in Aghaei et al. (2019) and several of its extensions. The current version of the package provides implementations for learning optimal classification trees, optimal fair classification trees, optimal classification trees robust to distribution shifts, and optimal prescriptive trees from observational data. We have designed the package to be easy to maintain and extend as new optimal decision tree problem classes, reformulation strategies, and solution algorithms are introduced. To this end, the package follows object-oriented design principles and supports both commercial (Gurobi) and open source (COIN-OR branch and cut) solvers. The package documentation and an extensive user guide can be found at https://d3m-research-group.github.io/odtlearn/. Additionally, users can view
    
[^31]: 线性赌博机中的任意模型选择

    Anytime Model Selection in Linear Bandits. (arXiv:2307.12897v1 [stat.ML])

    [http://arxiv.org/abs/2307.12897](http://arxiv.org/abs/2307.12897)

    该论文提出了一种在线性赌博机中进行任意模型选择的方法，通过模拟全信息反馈实现在遗憾方面具有指数改进的性能，并且不依赖时间界限和纯探索阶段。

    

    在赌博优化中，模型选择是一个具有挑战性的问题，因为它不仅需要在行动选择方面平衡探索和开发，还需要在模型选择方面平衡探索和开发。一种自然的方法是依赖于将不同模型视为专家的在线学习算法。然而，现有方法在遗憾方面与模型数量$M$的规模（$\text{poly}M$）呈不良的关系。我们的关键洞察是，在线性赌博机的模型选择中，我们可以通过有利的偏差-方差权衡来模拟全信息反馈给在线学习者。这使得我们能够开发出具有指数改进（$\log M$）在遗憾方面对$M$依赖性的ALEXP。ALEXP在遗憾方面具有任意保证，并且既不需要对时间界$n$具有知识，也不依赖于初始的纯探索阶段。我们的方法利用了Lasso的一种新颖的时间均匀分析，建立了在线学习和高维统计之间的新连接。

    Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly ($\text{poly}M$) with the number of models $M$ in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALEXP, which has an exponentially improved ($\log M$) dependence on $M$ for its regret. ALEXP has anytime guarantees on its regret, and neither requires knowledge of the horizon $n$, nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.
    
[^32]: 用物理信知的神经网络解决维度诅咒问题

    Tackling the Curse of Dimensionality with Physics-Informed Neural Networks. (arXiv:2307.12306v1 [cs.LG])

    [http://arxiv.org/abs/2307.12306](http://arxiv.org/abs/2307.12306)

    本文提出了一种新方法，利用物理信知的神经网络(PINNs)解决高维度的偏微分方程(PDEs)问题，并证明了收敛性和其他期望属性。

    

    维度诅咒(CoD)随着维度的增加，以指数级增长的计算成本来极度税费计算资源。这在解决高维偏微分方程(PDEs)中面临极大挑战，正如Richard Bellman在60年前首次指出的那样。尽管近年来在高维度上数值解决偏微分方程(PDEs)取得了一些成功，但这样的计算代价过高，而将一般非线性PDEs扩展到高维度从未实现过。本文提出了一种新方法，将物理信知的神经网络(PINNs)扩展到解决任意高维PDEs。该新方法称为随机维度梯度下降(SDGD)，将PDE的梯度分解为与不同维度对应的部分，并在训练PINNs的每次迭代中随机选择这些维度部分的子集进行采样。我们在理论上证明了所提出方法的收敛保证和其他期望属性。

    The curse-of-dimensionality (CoD) taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional PDEs as Richard Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerically partial differential equations (PDEs) in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. In this paper, we develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs into pieces corresponding to different dimensions and samples randomly a subset of these dimensional pieces in each iteration of training PINNs. We theoretically prove the convergence guarantee and other desired properties of the proposed meth
    
[^33]: AutoAlign：基于大型语言模型的全自动有效知识图谱对齐方法

    AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])

    [http://arxiv.org/abs/2307.11772](http://arxiv.org/abs/2307.11772)

    AutoAlign是一种全自动的知识图谱对齐方法，不需要手工制作的种子对齐。它利用大型语言模型自动捕捉谓词相似性，并使用TransE计算实体嵌入来实现实体对齐。

    

    知识图谱间的实体对齐任务旨在识别出两个不同知识图谱中表示相同实体的每对实体。许多基于机器学习的方法已被提出用于这个任务。然而，据我们所知，现有的方法都需要手工制作的种子对齐，这是非常昂贵的。在本文中，我们提出了第一个名为AutoAlign的完全自动对齐方法，它不需要任何手工制作的种子对齐。具体而言，对于谓词嵌入，AutoAlign使用大型语言模型构建谓词近邻图，自动捕捉两个知识图谱中谓词的相似性。对于实体嵌入，AutoAlign首先使用TransE独立计算每个知识图谱的实体嵌入，然后通过计算基于实体属性的实体相似性，将两个知识图谱的实体嵌入移动到相同的向量空间中。因此，AutoAlign实现了谓词对齐和实体对齐。

    The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
    
[^34]: 朝着全球生物多样性评估迈出的一步：BIOSCAN-1M昆虫数据集

    A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset. (arXiv:2307.10455v1 [cs.CV])

    [http://arxiv.org/abs/2307.10455](http://arxiv.org/abs/2307.10455)

    提出了一个新的大型手工标记昆虫图像数据集BIOSCAN-Insect，用于对昆虫生物多样性进行编目。该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。

    

    为了对昆虫生物多样性进行编目，我们提出了一个新的大型手工标记昆虫图像数据集，即BIOSCAN-Insect数据集。每个记录都由专家进行分类，并且具有相关的遗传信息，包括原始核苷酸条形码序列和分配的条形码索引号，这些是基于遗传的物种分类的代理。本文介绍了一个精选的百万图像数据集，主要用于训练能够提供基于图像的分类评估的计算机视觉模型，但该数据集还具有引人注目的特征，对广泛的机器学习社区也具有研究价值。由于数据集固有的生物性质，展现出了具有长尾类别不平衡分布的特征。此外，分类标签是一个分层分类方案，在较低级别上呈现出高度细粒度的分类问题。除了激发对生物多样性研究的兴趣外，该数据集还促进了对机器学习的深入研究。

    In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research w
    
[^35]: FedBug: 一种自底向上逐渐解冻的联邦学习框架

    FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning. (arXiv:2307.10317v1 [cs.LG])

    [http://arxiv.org/abs/2307.10317](http://arxiv.org/abs/2307.10317)

    FedBug是一个自底向上逐渐解冻的联邦学习框架，通过冻结和逐渐解冻模型层，实现了一种有效缓解客户端漂移现象的方法。

    

    联邦学习（FL）提供了一种协作训练框架，允许多个客户端在不损害数据隐私的情况下为共享模型做出贡献。由于本地数据集的异构性，更新的客户端模型可能会过拟合并与彼此发散，这被称为客户端漂移问题。在本文中，我们提出了FedBug（具有自底向上逐渐解冻的联邦学习），这是一种新颖的FL框架，旨在有效地减轻客户端漂移。FedBug自适应地利用每个全局轮次服务器分发的客户端模型参数作为跨客户端对齐的参考点。具体而言，在客户端上，FedBug从冻结整个模型开始，然后逐渐解冻层，从输入层到输出层。这种自底向上的方法允许模型训练解冻的新层将数据投影到一个潜在空间中，在这个空间中，分离超平面在所有客户端上保持一致。我们在理论上分析了FedBug

    Federated Learning (FL) offers a collaborative training framework, allowing multiple clients to contribute to a shared model without compromising data privacy. Due to the heterogeneous nature of local datasets, updated client models may overfit and diverge from one another, commonly known as the problem of client drift. In this paper, we propose FedBug (Federated Learning with Bottom-Up Gradual Unfreezing), a novel FL framework designed to effectively mitigate client drift. FedBug adaptively leverages the client model parameters, distributed by the server at each global round, as the reference points for cross-client alignment. Specifically, on the client side, FedBug begins by freezing the entire model, then gradually unfreezes the layers, from the input layer to the output layer. This bottom-up approach allows models to train the newly thawed layers to project data into a latent space, wherein the separating hyperplanes remain consistent across all clients. We theoretically analyze F
    
[^36]: 结构化支持向量机的接近线性时间算法

    A Nearly-Linear Time Algorithm for Structured Support Vector Machines. (arXiv:2307.07735v1 [math.OC])

    [http://arxiv.org/abs/2307.07735](http://arxiv.org/abs/2307.07735)

    这篇论文提出了针对结构化支持向量机的接近线性时间算法，解决了二次规划输入规模和解决时间的问题。

    

    二次规划是凸优化领域中的基本问题。许多实际任务可以表示为二次规划，例如支持向量机（SVM）。在深度学习方法盛行之前，线性SVM是过去三十年来最流行的机器学习工具之一。一般来说，一个二次规划的输入规模为Θ(n^2)（其中n是变量的数量），因此解决该问题需要Ω(n^2)的时间。然而，SVM产生的二次规划的输入规模为O(n)，这使得设计接近线性时间算法成为可能。两个重要的SVM类别是具有低秩核因式分解和低树宽规模的程序。低树宽凸优化在过去几年中引起了越来越多的关注（例如线性规划[Dong, Lee and Ye 2021]和半定规划[Gu and Song 2022]）。因此，一个重要的开放问题是是否存在接近线性时间算法。

    Quadratic programming is a fundamental problem in the field of convex optimization. Many practical tasks can be formulated as quadratic programming, for example, the support vector machine (SVM). Linear SVM is one of the most popular tools over the last three decades in machine learning before deep learning method dominating.  In general, a quadratic program has input size $\Theta(n^2)$ (where $n$ is the number of variables), thus takes $\Omega(n^2)$ time to solve. Nevertheless, quadratic programs coming from SVMs has input size $O(n)$, allowing the possibility of designing nearly-linear time algorithms. Two important classes of SVMs are programs admitting low-rank kernel factorizations and low-treewidth programs. Low-treewidth convex optimization has gained increasing interest in the past few years (e.g.~linear programming [Dong, Lee and Ye 2021] and semidefinite programming [Gu and Song 2022]). Therefore, an important open question is whether there exist nearly-linear time algorithms
    
[^37]: 没有训练就没有收益：重新审视基于Transformer的语言模型的高效训练算法

    No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v1 [cs.LG])

    [http://arxiv.org/abs/2307.06440](http://arxiv.org/abs/2307.06440)

    本论文重新审视了基于Transformer的语言模型的高效训练算法，包括动态架构，批量选择和高效优化器。然而，在使用这些算法预训练时，相对于基线方法，它们的训练、验证和下游收益消失了。同时，论文提出了一个评估协议来进行计算，并释放了代码来促进高效训练的研究。

    

    近年来，训练Transformer-based语言模型所需的计算量急剧增加。这一趋势促使研究者们开展了针对高效训练算法的研究，旨在比标准训练更快地改善训练、验证和下游性能。在这项工作中，我们重新审视了三类这样的算法：动态架构（层叠、层丢弃）、批量选择（选择性反向传播、RHO损失）和高效优化器（Lion、Sophia）。当使用这些方法在固定计算预算下对BERT和T5进行预训练时，我们发现它们的训练、验证和下游收益相对于一个具有完全衰减学习率的基线而言会消失。我们定义了一个评估协议，可以通过将所有计算时间映射到一个称为参考系统时间的参考机器上，在任意机器上进行计算。我们讨论了我们提出的协议的局限性，并发布了我们的代码，以鼓励对高效训练的严格研究。

    The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training p
    
[^38]: 对(深度)学习匹配算法的基准数据集的关键重新评估

    A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])

    [http://arxiv.org/abs/2307.01231](http://arxiv.org/abs/2307.01231)

    本研究重新评估了(深度)学习匹配算法的基准数据集，发现其中大多数数据集都属于相对简单的分类任务。

    

    实体解析(ER)是识别在一个或多个数据库中指向相同实体的记录的过程。多年来，已经开发了许多技术来解决ER挑战，近年来，机器学习和深度学习方法在匹配阶段受到了重视。然而，在文献中尚未对实验评估中常用的学习匹配算法的基准数据集的质量进行检查。为了弥补这个空白，我们提出了四种不同的方法来评估13个已建立数据集的难度和适用性：两种理论方法，涉及新的线性度量和现有的复杂度度量，以及两种实际方法：最佳非线性和线性匹配器之间的差异，以及最佳学习匹配器和完美预测器之间的差异。我们的分析表明，大多数流行数据集都提出了相当简单的分类任务。

    Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a
    
[^39]: milliFlow：用于人体运动感知的毫米波雷达点云场景流估计

    milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])

    [http://arxiv.org/abs/2306.17010](http://arxiv.org/abs/2306.17010)

    milliFlow是一种用于人体运动感知的新型深度学习方法，通过对毫米波雷达点云进行场景流估计，能够提供中间层的特征并直接用于下游的人体运动感知任务中。实验证明该方法具有优越性能。

    

    随着普适计算时代的到来，人体运动感知在智能系统中起着关键作用，用于决策、用户交互和个性化服务。在传统方法中，人体跟踪、姿势估计、手势识别和活动识别等方面进行了大量研究，这些方法主要基于摄像机。然而，摄像机的侵入性特点限制了它们在智能家居应用中的使用。为了解决这个问题，毫米波雷达由于其保护隐私的特点而受到欢迎。在这项工作中，我们提出了一种新颖的深度学习方法milliFlow，用于对毫米波雷达点云进行场景流估计，作为中间层的特征，直接受益于下游的人体运动感知任务。实验结果表明，我们的方法具有优越的性能，平均3D端点误差为4.6cm，明显超过竞争方法。此外，通过结合...

    Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
    
[^40]: 深度神经网络的采样权重

    Sampling weights of deep neural networks. (arXiv:2306.16830v1 [cs.LG])

    [http://arxiv.org/abs/2306.16830](http://arxiv.org/abs/2306.16830)

    我们提出了一种基于随机特征模型的采样方法，用于深度神经网络的权重和偏差。我们的方法不需要迭代优化或计算梯度，能够生成通用逼近器，并且对数据的变换和缩放是不变的。在数值实验中，我们展示了我们的方法的优越性能。

    

    我们引入了一个概率分布和有效的采样算法，用于完全连接神经网络的权重和偏差。在监督学习环境中，不需要通过迭代优化或计算内部网络参数的梯度来训练网络。采样基于随机特征模型的思想。然而，我们使用输入和输出训练数据对浅层和深度网络进行采样，而不是使用数据无关的分布，如正态分布。我们证明了我们构造的采样网络是通用逼近器。我们还证明了我们的采样方案对刚体变换和输入数据的缩放是不变的。这意味着许多常用的预处理技术不再需要。对于巴龙函数，我们证明了采样浅层网络的L^2近似误差随着神经元数量的平方根减小。在数值实验中，我们展示了我们的方法在多个数据集上的优越性能。

    We introduce a probability distribution, combined with an efficient sampling algorithm, for weights and biases of fully-connected neural networks. In a supervised learning context, no iterative optimization or gradient computations of internal network parameters are needed to obtain a trained network. The sampling is based on the idea of random feature models. However, instead of a data-agnostic distribution, e.g., a normal distribution, we use both the input and the output training data of the supervised learning problem to sample both shallow and deep networks. We prove that the sampled networks we construct are universal approximators. We also show that our sampling scheme is invariant to rigid body transformations and scaling of the input data. This implies many popular pre-processing techniques are no longer required. For Barron functions, we show that the $L^2$-approximation error of sampled shallow networks decreases with the square root of the number of neurons. In numerical ex
    
[^41]: OpenNDD:用于神经发育障碍检测的开放性识别

    OpenNDD: Open Set Recognition for Neurodevelopmental Disorders Detection. (arXiv:2306.16045v1 [cs.CV])

    [http://arxiv.org/abs/2306.16045](http://arxiv.org/abs/2306.16045)

    OpenNDD是一个用于神经发育障碍检测的开放性识别框架，结合了自动编码器和对抗循环点开放性识别技术，能准确识别已知类别并识别未遇到的类别。

    

    神经发育障碍(NDDs)是一组高患病率的障碍，表现出临床行为的相似性，使得精确识别不同的NDDs（如自闭症谱系障碍（ASD）和注意力缺陷多动障碍（ADHD））非常具有挑战性。此外，对于NDDs诊断并没有可靠的生理标志物，而仅依赖于心理评估标准。然而，通过智能辅助诊断来防止误诊和漏诊是至关重要的，这与随后的相应治疗密切相关。为了缓解这些问题，我们提出了一种新颖的用于NDDs筛查和检测的开放性识别框架，这是在该领域中首次应用开放性识别。它结合了自动编码器和对抗循环点开放性识别，能够准确识别已知类别，并能够识别过去未遇到的类别。

    Neurodevelopmental disorders (NDDs) are a highly prevalent group of disorders and represent strong clinical behavioral similarities, and that make it very challenging for accurate identification of different NDDs such as autism spectrum disorder (ASD) and attention-deficit hyperactivity disorder (ADHD). Moreover, there is no reliable physiological markers for NDDs diagnosis and it solely relies on psychological evaluation criteria. However, it is crucial to prevent misdiagnosis and underdiagnosis by intelligent assisted diagnosis, which is closely related to the follow-up corresponding treatment. In order to relieve these issues, we propose a novel open set recognition framework for NDDs screening and detection, which is the first application of open set recognition in this field. It combines auto encoder and adversarial reciprocal points open set recognition to accurately identify known classes as well as recognize classes never encountered. And considering the strong similarities bet
    
[^42]: 实现真实空气质量预测：介绍易于使用的PurpleAirSF数据集

    Unleashing Realistic Air Quality Forecasting: Introducing the Ready-to-Use PurpleAirSF Dataset. (arXiv:2306.13948v1 [cs.LG])

    [http://arxiv.org/abs/2306.13948](http://arxiv.org/abs/2306.13948)

    该论文介绍了PurpleAirSF数据集，这是一个易于获取的数据集，具有高时间分辨率、多种空气质量测量指标和广泛的地理范围，可用于研究人员的空气质量预测建模和空气污染模式研究，同时此数据集可用于未来开发新型的应用模型。

    

    空气质量预测由于机器学习和深度学习模型的进步引起了人们的极大关注。然而，复杂的数据采集和开放数据集的缺乏给研究人员造成了挑战，从而阻碍了有效的模型验证。本文介绍了PurpleAirSF，这是一个综合全面且易于获取的数据集，从PurpleAir网络中收集而来。该数据集具有高时间分辨率、各种空气质量测量指标和广泛的地理覆盖范围，可作为研究人员开发新型预测模型、研究空气污染模式以及调查其对健康和环境的影响的有用工具。我们介绍了构建PurpleAirSF所采用的数据采集和处理方法。此外，我们还使用经典和现代时空预测模型进行了初步实验，从而为未来空气质量预测模型的制定建立了基准。

    Air quality forecasting has garnered significant attention recently, with data-driven models taking center stage due to advancements in machine learning and deep learning models. However, researchers face challenges with complex data acquisition and the lack of open-sourced datasets, hindering efficient model validation. This paper introduces PurpleAirSF, a comprehensive and easily accessible dataset collected from the PurpleAir network. With its high temporal resolution, various air quality measures, and diverse geographical coverage, this dataset serves as a useful tool for researchers aiming to develop novel forecasting models, study air pollution patterns, and investigate their impacts on health and the environment. We present a detailed account of the data collection and processing methods employed to build PurpleAirSF. Furthermore, we conduct preliminary experiments using both classic and modern spatio-temporal forecasting models, thereby establishing a benchmark for future air q
    
[^43]: CeBed: 一个基于深度数据驱动的OFDM信道估计的基准测试

    CeBed: A Benchmark for Deep Data-Driven OFDM Channel Estimation. (arXiv:2306.13761v1 [cs.AI])

    [http://arxiv.org/abs/2306.13761](http://arxiv.org/abs/2306.13761)

    本文提出了一个称为CeBed的测试平台，用于评估和比较不同的数据驱动OFDM信道估计方法，解决了领域内实验条件不一致和缺乏可重复性的问题。

    

    深度学习广泛应用于无线通信问题中，包括信道估计。尽管存在许多数据驱动方法，但由于实验条件不一致和缺乏标准化的实验设计，对它们进行公正和现实的比较是困难的。此外，数据驱动方法的性能通常基于经验分析进行比较。缺乏可重复性和标准化评估工具（例如数据集、代码库）阻碍了数据驱动方法在信道估计和无线通信等领域的发展和进步。在这项工作中，我们介绍了一个建立基准测试的倡议，统一了几种数据驱动的OFDM信道估计方法。具体而言，我们提出了CeBed（信道估计测试平台），包括涵盖各种系统模型和传播条件的不同数据集，以及十个深度和传统的基线实现。本文旨在为评估和比较不同的数据驱动OFDM信道估计方法提供标准化的基准，解决领域内实验条件不一致和缺乏可重复性的问题。

    Deep learning has been extensively used in wireless communication problems, including channel estimation. Although several data-driven approaches exist, a fair and realistic comparison between them is difficult due to inconsistencies in the experimental conditions and the lack of a standardized experimental design. In addition, the performance of data-driven approaches is often compared based on empirical analysis. The lack of reproducibility and availability of standardized evaluation tools (e.g., datasets, codebases) hinder the development and progress of data-driven methods for channel estimation and wireless communication in general. In this work, we introduce an initiative to build benchmarks that unify several data-driven OFDM channel estimation approaches. Specifically, we present CeBed (a testbed for channel estimation) including different datasets covering various systems models and propagation conditions along with the implementation of ten deep and traditional baselines. Thi
    
[^44]: 学会生成比你的LMM更好的文本

    Learning to Generate Better Than Your LLM. (arXiv:2306.11816v1 [cs.LG])

    [http://arxiv.org/abs/2306.11816](http://arxiv.org/abs/2306.11816)

    本论文研究了基于强化学习算法 RLGF，用于在 GPT-3 等动态黑匣子指导下微调大型语言模型 LLM 的条件文本生成，相比通用 RL 算法，该算法在 IMDB 和 CommonGen 任务中表现更好。

    

    强化学习(RL)已经成为一种强大的范例，用于优化大型语言模型 (LLM) 条件文本生成。特别地，最近的LLM，如ChatGPT和GPT - 4能够与用户进行流畅的对话，并融合了RL和人类反馈。本研究受到学习搜索算法的启发，并利用文本生成的关键特性，探索了超出通用RL算法如PPO之外的强化学习算法。特别地，我们扩展了RL算法，使其能够与动态黑匣子的指导LLM如GPT-3进行交互，并提出了具有引导反馈的RL(RLGF)，这是一套用于LLM微调的RL算法。我们在GRUE基准测试的IMDB正向评论和CommonGen文本生成任务上进行了实验。我们展示了我们的RL算法比监督学习(SL)和默认PPO基线表现更高，证明了与指导LLM互动的好处。

    Reinforcement learning (RL) has emerged as a powerful paradigm for fine-tuning Large Language Models (LLMs) for conditional text generation. In particular, recent LLMs such as ChatGPT and GPT-4 can engage in fluent conversations with users by incorporating RL and feedback from humans. Inspired by learning-to-search algorithms and capitalizing on key properties of text generation, we seek to investigate reinforcement learning algorithms beyond general purpose algorithms such as Proximal policy optimization (PPO). In particular, we extend RL algorithms to allow them to interact with a dynamic black-box guide LLM such as GPT-3 and propose RL with guided feedback (RLGF), a suite of RL algorithms for LLM fine-tuning. We experiment on the IMDB positive review and CommonGen text generation task from the GRUE benchmark. We show that our RL algorithms achieve higher performance than supervised learning (SL) and default PPO baselines, demonstrating the benefit of interaction with the guide LLM. 
    
[^45]: 深度学习的优化理解

    Understanding Optimization of Deep Learning. (arXiv:2306.09338v1 [cs.LG])

    [http://arxiv.org/abs/2306.09338](http://arxiv.org/abs/2306.09338)

    本文全面介绍了深度学习中梯度消失和梯度爆炸等优化挑战，并通过提高梯度流和对网络Lipschitz常数施加约束等措施进行了解决。显式优化和隐式优化是两种解决优化问题的不同方式。

    

    本文全面介绍了深度学习中的优化理论，主要关注梯度消失和梯度爆炸等问题所带来的模型表示能力降低和训练不稳定性等挑战。我们通过提高梯度流和对网络Lipschitz 常数施加约束等措施来分析这两个挑战。为了帮助理解当前的优化方法，我们将其分为显式优化方法和隐式优化方法。显式优化方法涉及直接操作优化器参数，包括权重、梯度、学习率和权重衰减等。相比之下，隐式优化方法侧重于通过增强网络模块（如残差快捷方式、标准化方法、注意机制和激活）来改善网络整体形势。本文提供了深入的分析和实验，以帮助研究人员更好地了解深度学习模型的优化方法。

    This article provides a comprehensive understanding of optimization in deep learning, with a primary focus on the challenges of gradient vanishing and gradient exploding, which normally lead to diminished model representational ability and training instability, respectively. We analyze these two challenges through several strategic measures, including the improvement of gradient flow and the imposition of constraints on a network's Lipschitz constant. To help understand the current optimization methodologies, we categorize them into two classes: explicit optimization and implicit optimization. Explicit optimization methods involve direct manipulation of optimizer parameters, including weight, gradient, learning rate, and weight decay. Implicit optimization methods, by contrast, focus on improving the overall landscape of a network by enhancing its modules, such as residual shortcuts, normalization methods, attention mechanisms, and activations. In this article, we provide an in-depth a
    
[^46]: 跨国和时间的社会动荡相变

    Phase Transitions of Civil Unrest across Countries and Time. (arXiv:2306.08698v2 [physics.soc-ph] UPDATED)

    [http://arxiv.org/abs/2306.08698](http://arxiv.org/abs/2306.08698)

    跨国和时间的社会动荡相变研究探索了集体社会动荡是否可以被描述为一系列具有可测量和可识别特征的循环相变，并证明了宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，以及普遍机制可能支撑着社会动荡的某些方面。

    

    相变是复杂系统中突发转变的特征，尽管在物理和自然科学中已经进行了大量研究，但在社会系统中对这一现象的实证研究相对较少。本研究的目标是探索集体社会动荡的动力学是否可以被合理地描述为一系列循环相变，其中每个阶段具有可测量和可识别的潜在特征。我们引入了一个宏观水平的社会动荡统计模型，并使用包括1946年至2017年在内的170个国家的全面数据集来评估其可行性。我们的研究结果表明，这个宏观相变模型有效地捕捉到了全球各国社会动荡数据的特征，并且普遍的机制可能潜在地支撑着社会动荡的某些方面。我们还引入了一个新的量表来衡量一个国家的社会动荡程度。

    Phase transitions, characterized by abrupt shifts between macroscopic patterns of organization, are ubiquitous in complex systems. Despite considerable research in the physical and natural sciences, the empirical study of this phenomenon in societal systems is relatively underdeveloped. The goal of this study is to explore whether the dynamics of collective civil unrest can be plausibly characterized as a sequence of recurrent phase shifts, with each phase having measurable and identifiable latent characteristics. We introduce a macro-level statistical model of civil unrest and evaluate its plausibility using a comprehensive dataset of civil unrest events in 170 countries from 1946 to 2017. Our findings demonstrate that the macro-level phase model effectively captures the characteristics of civil unrest data from diverse countries globally and that universal mechanisms may underlie certain aspects of the dynamics of civil unrest. We also introduce a new scale to quantify a country's lo
    
[^47]: VillanDiffusion: 一种针对扩散模型的统一后门攻击框架。

    VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models. (arXiv:2306.06874v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2306.06874](http://arxiv.org/abs/2306.06874)

    本文提出VillanDiffusion，一个针对扩散模型的统一后门攻击框架，涵盖主流的无条件和有条件DM，便于对不同DM配置进行后门分析，并为基于字幕的DM后门攻击提供了新的见解。

    

    扩散模型（DM）是最先进的生成模型之一，它通过迭代添加噪声和去噪学习可逆的损坏过程。它们是许多生成人工智能应用的主干，例如文本到图像有条件生成。但是，最近的研究表明基本无条件DM（例如DDPM和DDIM）易受后门注入攻击，这是一种由于恶意嵌入模型输入的模式而触发的输出操纵攻击。本文提出了一个统一的后门攻击框架（VillanDiffusion），以扩展当前的DM后门分析范围。我们的框架涵盖了主流的无条件和有条件DM（基于去噪和基于评分），以及各种无需训练的采样器进行整体评估。实验证明，我们的统一框架便于对不同DM配置进行后门分析，并为基于字幕的DM后门攻击提供新的见解。

    Diffusion Models (DMs) are state-of-the-art generative models that learn a reversible corruption process from iterative noise addition and denoising. They are the backbone of many generative AI applications, such as text-to-image conditional generation. However, recent studies have shown that basic unconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a type of output manipulation attack triggered by a maliciously embedded pattern at model input. This paper presents a unified backdoor attack framework (VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our framework covers mainstream unconditional and conditional DMs (denoising-based and score-based) and various training-free samplers for holistic evaluations. Experiments show that our unified framework facilitates the backdoor analysis of different DM configurations and provides new insights into caption-based backdoor attacks on DMs.
    
[^48]: 在表面之下寻找：利用基本对称性实现高效离线强化学习

    Look Beneath the Surface: Exploiting Fundamental Symmetry for Sample-Efficient Offline RL. (arXiv:2306.04220v1 [cs.LG])

    [http://arxiv.org/abs/2306.04220](http://arxiv.org/abs/2306.04220)

    本文提出了一个新的离线强化学习算法TDM，利用系统动力学的基本对称性实现高效学习小数据集。

    

    离线强化学习通过从预先收集的数据集中学习策略来解决与环境交互的实际问题。然而，现有的离线强化学习算法的性能严重依赖于数据集的规模和状态-动作空间覆盖范围。真实世界数据的收集通常是昂贵和难以控制的，导致数据集小且覆盖范围狭窄，从而对离线强化学习的实际部署提出了重大挑战。在本文中，我们提供了一个新的见解，即利用系统动力学的基本对称性可以在小数据集下显著提高离线强化学习的性能。具体来说，我们提出了一个时间反演对称(T-symmetry)强制的动力学模型(TDM)，建立了一对正向和反向潜在动力学之间的一致性。TDM为小数据集提供了良好的表示，并基于T-symmetry的符合性提供了一种新的OOD样本的可靠性度量。

    Offline reinforcement learning (RL) offers an appealing approach to real-world tasks by learning policies from pre-collected datasets without interacting with the environment. However, the performance of existing offline RL algorithms heavily depends on the scale and state-action space coverage of datasets. Real-world data collection is often expensive and uncontrollable, leading to small and narrowly covered datasets and posing significant challenges for practical deployments of offline RL. In this paper, we provide a new insight that leveraging the fundamental symmetry of system dynamics can substantially enhance offline RL performance under small datasets. Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced Dynamics Model (TDM), which establishes consistency between a pair of forward and reverse latent dynamics. TDM provides both well-behaved representations for small datasets and a new reliability measure for OOD samples based on compliance with the T-symmetry. 
    
[^49]: 人类对齐校准用于AI辅助决策制定

    Human-Aligned Calibration for AI-Assisted Decision Making. (arXiv:2306.00074v1 [cs.LG])

    [http://arxiv.org/abs/2306.00074](http://arxiv.org/abs/2306.00074)

    本文通过引入一种基于主动询问决策者个人偏好的置信度构造方法，解决了现有置信度对于决策者信任决策的不准确问题，从而提高决策的准确性和效率。

    

    当使用二元分类器提供决策支持时，它通常提供标签预测和置信度值。然后，决策者应使用置信度值来校准对预测的信任程度。在这种情况下，人们经常认为置信度值应对预测标签与实际标签匹配的概率进行良好校准的估计。然而，多条实证证据表明，决策者难以使用这些置信度值很好地确定何时信任预测。本文的目标首先是理解为什么，然后研究如何构建更有用的置信度值。我们首先认为，在广泛类的效用函数中，存在数据分布，对于这些分布，理性决策者通常难以使用以上置信度值发现最佳决策政策——最佳的决策者需要人类对齐。然后，我们引入了一种基于主动询问决策者他们在所面临的二元分类任务的决策上的个人偏好的新方法来构造置信度值。我们表明，该方法产生的置信度值比使用标准置信度度量导致更好的决策。

    Whenever a binary classifier is used to provide decision support, it typically provides both a label prediction and a confidence value. Then, the decision maker is supposed to use the confidence value to calibrate how much to trust the prediction. In this context, it has been often argued that the confidence value should correspond to a well calibrated estimate of the probability that the predicted label matches the ground truth label. However, multiple lines of empirical evidence suggest that decision makers have difficulties at developing a good sense on when to trust a prediction using these confidence values. In this paper, our goal is first to understand why and then investigate how to construct more useful confidence values. We first argue that, for a broad class of utility functions, there exist data distributions for which a rational decision maker is, in general, unlikely to discover the optimal decision policy using the above confidence values -- an optimal decision maker wou
    
[^50]: 更大、更好、更快：具有人类效率的人类级Atari游戏

    Bigger, Better, Faster: Human-level Atari with human-level efficiency. (arXiv:2305.19452v1 [cs.LG])

    [http://arxiv.org/abs/2305.19452](http://arxiv.org/abs/2305.19452)

    引入BBF基于价值函数的RL代理，在Atari 100K基准测试上实现超人类表现，具有人类效率，提出了在样本高效RL研究的ALE中更新目标的可能。

    

    我们引入了一个名为BBF的基于价值函数的强化学习代理来实现Atari 100K基准测试的超人类表现。BBF依靠神经网络的价值估计扩展以及其他设计选择，在遵循样本高效的情况下，能够实现这种扩展。我们对这些设计选择进行了广泛的分析并为未来的工作提供了洞见。最后，我们讨论了更新ALE上样本高效的RL研究目标的问题。我们将我们的代码和数据公开在https://github.com/google-research/google-research/tree/master/bigger_better_faster。

    We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
    
[^51]: 基于卷积蒙日映射归一化的生物信号学习方法

    Convolutional Monge Mapping Normalization for learning on biosignals. (arXiv:2305.18831v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2305.18831](http://arxiv.org/abs/2305.18831)

    本研究提出了一种新的方法：基于卷积蒙日映射归一化 (CMMN)，用于信号众多但变异性较大的生物医学数据上，能自适应调整幅度、滤波器的功率谱密度，通过最优输运映射和 barycenters 实现个体测试时间适应新数据，且显著提升信号分类和检测性能。

    

    在许多信号与生物医学数据的机器学习应用中，尤其是在脑电图 (EEG) 中，一个主要的挑战是数据在受试者、会话和硬件设备上的变异性。本文提出了一种新方法，称为卷积蒙日映射归一化 (CMMN)，其核心是根据训练数据估计Wasserstein barycenter，通过滤波信号以适应其功率谱密度 (PSD)。CMMN 基于新的闭式解，提供了最优输运映射和 barycenters，并提供了个体测试时间适应新数据的功能，而无需重新训练预测模型。通过对睡眠 EEG 数据的数值实验表明，CMMN 在适应受试者、会话甚至在使用不同硬件收集的数据集之间，都能带来显著且一致的性能提升，且其性能提升与数值密集的域适应 (DA) 相当。

    In many machine learning applications on signals and biomedical data, especially electroencephalogram (EEG), one major challenge is the variability of the data across subjects, sessions, and hardware devices. In this work, we propose a new method called Convolutional Monge Mapping Normalization (CMMN), which consists in filtering the signals in order to adapt their power spectrum density (PSD) to a Wasserstein barycenter estimated on training data. CMMN relies on novel closed-form solutions for optimal transport mappings and barycenters and provides individual test time adaptation to new data without needing to retrain a prediction model. Numerical experiments on sleep EEG data show that CMMN leads to significant and consistent performance gains independent from the neural network architecture when adapting between subjects, sessions, and even datasets collected with different hardware. Notably our performance gain is on par with much more numerically intensive Domain Adaptation (DA) m
    
[^52]: GBG++：分类的快速和稳定的颗粒球生成方法

    GBG++: A Fast and Stable Granular Ball Generation Method for Classification. (arXiv:2305.18450v1 [cs.LG])

    [http://arxiv.org/abs/2305.18450](http://arxiv.org/abs/2305.18450)

    本文提出了一种基于关注机制的快速稳定的颗粒球生成方法，可以在分类任务中应用。

    

    颗粒球计算作为一种高效、稳健、可扩展的学习方法，已成为颗粒计算的研究热点。颗粒球计算包括两个阶段：颗粒球生成（GBG）和基于颗粒球（GB）的多粒度学习。然而，现有的GBG方法的稳定性和效率需要进一步提高，因为它们在很大程度上依赖于k均值或k分割。此外，基于GB的分类器仅单向考虑GB的几何特征构建分类规则，而忽视了GB的质量。因此，本文提出了一种快速稳定的基于关注机制的GBG（GBG++）方法。具体而言，所提出的GBG++方法仅需要在分割每个GB时计算从数据驱动的中心到未分割样本的距离，而不是随机选择中心并计算它到所有样本的距离。此外，引入了一种异常值检测方法。

    Granular ball computing (GBC), as an efficient, robust, and scalable learning method, has become a popular research topic of granular computing. GBC includes two stages: granular ball generation (GBG) and multi-granularity learning based on the granular ball (GB). However, the stability and efficiency of existing GBG methods need to be further improved due to their strong dependence on $k$-means or $k$-division. In addition, GB-based classifiers only unilaterally consider the GB's geometric characteristics to construct classification rules, but the GB's quality is ignored. Therefore, in this paper, based on the attention mechanism, a fast and stable GBG (GBG++) method is proposed first. Specifically, the proposed GBG++ method only needs to calculate the distances from the data-driven center to the undivided samples when splitting each GB, instead of randomly selecting the center and calculating the distances between it to all samples. Moreover, an outlier detection method is introduced
    
[^53]: GRD: 一种在强化学习中用于可解释奖励再分配的生成方式

    GRD: A Generative Approach for Interpretable Reward Redistribution in Reinforcement Learning. (arXiv:2305.18427v1 [cs.LG])

    [http://arxiv.org/abs/2305.18427](http://arxiv.org/abs/2305.18427)

    本文提出了一种可解释奖励再分配的方法，通过因果透视建模状态和行动贡献，产生可解释的返回分解。生成返回分解（GRD）框架用于延迟奖励场景中的策略优化。

    

    在强化学习中，一个重要的挑战是确定哪些状态-行动对应该对未来的分步奖励负责。Return Decomposition提供了一种解决方案，通过重新分配观测序列中的奖励来保持策略不变。虽然大多数现有方法都以不可解释的方式构建奖励再分配，但我们建议采用因果透视来明确建模状态和行动的贡献，从而产生可解释的返回分解。本文首先研究了因果生成模型在返回分解中的作用，通过描述马尔可夫奖励和基于轨迹的长期回报的生成过程，并进一步提出了一种名为生成返回分解（GRD）的框架，用于延迟奖励场景中的策略优化。具体而言，GRD首先确定生成过程中不可观测的马尔可夫奖励和因果关系，然后利用确定的因果模型计算可观测奖励的期望，进而提高策略性能。

    A major challenge in reinforcement learning is to determine which state-action pairs are responsible for future rewards that are delayed. Return Decomposition offers a solution by redistributing rewards from observed sequences while preserving policy invariance. While the majority of current approaches construct the reward redistribution in an uninterpretable manner, we propose to explicitly model the contributions of state and action from a causal perspective, resulting in an interpretable return decomposition. In this paper, we start by studying the role of causal generative models in return decomposition by characterizing the generation of Markovian rewards and trajectory-wise long-term return and further propose a framework, called Generative Return Decomposition (GRD), for policy optimization in delayed reward scenarios. Specifically, GRD first identifies the unobservable Markovian rewards and causal relations in the generative process. Then, GRD makes use of the identified causal
    
[^54]: StEik: 稳定神经符号距离函数和更细致形状表示的优化

    StEik: Stabilizing the Optimization of Neural Signed Distance Functions and Finer Shape Representation. (arXiv:2305.18414v1 [cs.CV])

    [http://arxiv.org/abs/2305.18414](http://arxiv.org/abs/2305.18414)

    本文提出了一种新的神经网络方法StEik来稳定神经符号距离函数，同时实现了更好的形状细节表示和优化效果，表现出比现有INR方法更好的性能。

    

    我们提出了学习隐式神经表示(INR)形状的新见解和新范式（StEik）。特别地，我们阐明了在INR中用于施加有符号距离函数约束的流行的Eikonal Loss的性质。我们从解析上证明，随着网络表示能力的增强，优化方法在连续极限下逼近于偏微分方程(PDE)，而PDE是不稳定的。我们展示了这种不稳定性在现有的网络优化中可能表现为重构表面的不规则性和/或收敛到次优局部最小值，因此无法捕捉到精细的几何和拓扑结构。我们从解析上展示了如何通过其他添加到损失中的术语(当前在文献中用于其他目的)来消除这些不稳定性。然而，这些项可能过度规则化表面，防止细节的精细形状表示。基于连续极限的类似PDE理论，我们引入了一种新的损失函数，可以同时稳定优化和捕捉精细的形状细节。我们在各种3D形状数据集中展示了我们的方法的有效性，并显示相对于现有的INR方法，在形状质量和多样性方面表现出更好的性能。

    We present new insights and a novel paradigm (StEik) for learning implicit neural representations (INR) of shapes. In particular, we shed light on the popular eikonal loss used for imposing a signed distance function constraint in INR. We show analytically that as the representation power of the network increases, the optimization approaches a partial differential equation (PDE) in the continuum limit that is unstable. We show that this instability can manifest in existing network optimization, leading to irregularities in the reconstructed surface and/or convergence to sub-optimal local minima, and thus fails to capture fine geometric and topological structure. We show analytically how other terms added to the loss, currently used in the literature for other purposes, can actually eliminate these instabilities. However, such terms can over-regularize the surface, preventing the representation of fine shape detail. Based on a similar PDE theory for the continuum limit, we introduce a n
    
[^55]: 针对领域泛化的异质性量化和对比探索

    Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization. (arXiv:2305.15889v1 [cs.LG])

    [http://arxiv.org/abs/2305.15889](http://arxiv.org/abs/2305.15889)

    本文提出了一种Feature Heterogeneity Distance(FHD)度量标准来衡量领域的异质性，并引入了一个新的实验模式Contrastive Convergence for Domain Generalization (CCDG) 来寻找最佳的监督信号来提高泛化。实验表明，我们的方法比其他最先进的方法更加有效和优越。

    

    领域泛化(DG)是现实世界应用中普遍存在的问题，通过利用几个源域训练出对未见过目标域进行有效泛化的模型。由于领域标签-即每个数据点来自哪个域自然存在，因此大多数DG算法将它们作为一种监督信息来提高泛化性能。然而，由于领域之间缺乏异质性，即领域之间的差异，原始的域标签可能不是最佳的监督信号。本文提出了一种新的度量标准Feature Heterogeneity Distance(FHD)来衡量领域的异质性，并引入一个新的实验模式CCDG，用于寻找最佳的监督信号来提高泛化。大量实验和消融研究表明，我们所提出的FHD度量标准和CCDG模式比其他最先进的方法更加有效和优越。

    Domain generalization (DG) is a prevalent problem in real-world applications, which aims to train well-generalized models for unseen target domains by utilizing several source domains. Since domain labels, i.e., which domain each data point is sampled from, naturally exist, most DG algorithms treat them as a kind of supervision information to improve the generalization performance. However, the original domain labels may not be the optimal supervision signal due to the lack of domain heterogeneity, i.e., the diversity among domains. For example, a sample in one domain may be closer to another domain, its original label thus can be the noise to disturb the generalization learning. Although some methods try to solve it by re-dividing domains and applying the newly generated dividing pattern, the pattern they choose may not be the most heterogeneous due to the lack of the metric for heterogeneity. In this paper, we point out that domain heterogeneity mainly lies in variant features under 
    
[^56]: 深度图神经网络中可逆和不可逆基于括号的动力学

    Reversible and irreversible bracket-based dynamics for deep graph neural networks. (arXiv:2305.15616v1 [cs.LG])

    [http://arxiv.org/abs/2305.15616](http://arxiv.org/abs/2305.15616)

    本文提出了基于结构保持基于括号的动力学系统的新型GNN架构，这些架构在理论上被证明要么保持能量，要么在深度增加时产生正的耗散，这解释了可逆性和不可逆性在网络性能中的作用。

    

    最近的研究表明，受物理启发的结构允许训练深度图神经网络（GNN）而不会过度光滑。然而，这些物理的作用尚不清楚，尽管可逆（例如哈密顿）和不可逆（例如扩散）现象的成功实例产生了可比较的结果，尽管机制截然相反，并且由于经验上的离开数学理论而出现了进一步的复杂性。本文提出了一系列基于结构保持基于括号的动力学系统的新型GNN架构，这些架构在理论上被证明要么保持能量，要么在深度增加时产生正的耗散。本文表明，这里使用的理论上有根据的框架允许固有可解释的结构，这些结构将当前架构中的离开理论内容放在上下文中，并更好地阐明了可逆性和不可逆性在网络性能中的作用。

    Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.
    
[^57]: ParticleWNN：一种解决偏微分方程的新型神经网络框架

    ParticleWNN: a Novel Neural Networks Framework for Solving Partial Differential Equations. (arXiv:2305.12433v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12433](http://arxiv.org/abs/2305.12433)

    ParticleWNN是一种新型的神经网络框架，可以在弱形式下求解PDE。它采用DNN作为试验空间，用由粒子为中心的极小区域内的紧密支持的函数构成的测试空间，并通过R自适应策略训练神经网络。该框架具有较高的精度和效率，并且易于扩展和并行化。

    

    近年来，深度神经网络（DNN）广泛用于求解偏微分方程（PDE）。在这项工作中，提出了一种名为基于Particle Weak-form的神经网络（ParticleWNN）的新型深度学习框架，用于求解弱形式的PDE。在这个框架中，试验空间选择为DNN的空间，测试空间由紧密支持在极小区域内的函数构成，这些函数的中心是粒子。为了训练神经网络，设计了一种R自适应策略，以在训练期间自适应修改区域的半径。ParticleWNN继承了弱/变分公式的优点，例如要求较少的解的正则性和计算积分的少量积分点。此外，由于测试函数的特殊构造，ParticleWNN允许本地训练网络、并行实现和仅在极小区域内进行积分计算。该框架获得了较高的精度和效率，并在实现中具有良好的可扩展性和并行性。

    Deep neural networks (DNNs) have been widely used to solve partial differential equations (PDEs) in recent years. In this work, a novel deep learning-based framework named Particle Weak-form based Neural Networks (ParticleWNN) is developed for solving PDEs in the weak form. In this framework, the trial space is chosen as the space of DNNs, and the test space is constructed by functions compactly supported in extremely small regions whose centers are particles. To train the neural networks, an R-adaptive strategy is designed to adaptively modify the radius of regions during training. The ParticleWNN inherits the advantages of weak/variational formulation, such as requiring less regularity of the solution and a small number of quadrature points for computing the integrals. Moreover, due to the special construction of the test functions, the ParticleWNN allows local training of networks, parallel implementation, and integral calculations only in extremely small regions. The framework is p
    
[^58]: SneakyPrompt：评估文本生成图像模型安全过滤器的鲁棒性

    SneakyPrompt: Evaluating Robustness of Text-to-image Generative Models' Safety Filters. (arXiv:2305.12082v1 [cs.LG])

    [http://arxiv.org/abs/2305.12082](http://arxiv.org/abs/2305.12082)

    本文提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中的安全过滤器的鲁棒性，该框架的关键洞见是搜索备选令牌来绕过安全过滤器。

    

    文本生成图像模型，如Stable Diffusion和DALL$\cdot$E 2等，由于它们在现实世界中的广泛应用而受到广泛关注。文本生成图像模型面临的一个挑战性问题是生成不安全内容，例如与暴力和成人相关的内容。因此，常见做法是部署所谓的安全过滤器，基于文本或图像特征阻止不安全内容。先前的工作研究了此类安全过滤器的可能绕过方式。然而，现有的工作在很大程度上是手动完成并专门针对Stable Diffusion官方的安全过滤器。此外，基于我们的评估，Stable Diffusion的安全过滤器的绕过比率仅为23.51％。在本文中，我们提出了第一个自动攻击框架SneakyPrompt，以评估最先进的文本生成图像模型中现实世界安全过滤器的鲁棒性。我们的关键洞见是搜索备选令牌来绕过安全过滤器。

    Text-to-image generative models such as Stable Diffusion and DALL$\cdot$E 2 have attracted much attention since their publication due to their wide application in the real world. One challenging problem of text-to-image generative models is the generation of Not-Safe-for-Work (NSFW) content, e.g., those related to violence and adult. Therefore, a common practice is to deploy a so-called safety filter, which blocks NSFW content based on either text or image features. Prior works have studied the possible bypass of such safety filters. However, existing works are largely manual and specific to Stable Diffusion's official safety filter. Moreover, the bypass ratio of Stable Diffusion's safety filter is as low as 23.51% based on our evaluation.  In this paper, we propose the first automated attack framework, called SneakyPrompt, to evaluate the robustness of real-world safety filters in state-of-the-art text-to-image generative models. Our key insight is to search for alternative tokens in 
    
[^59]: 动量匹配去噪Gibbs采样

    Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])

    [http://arxiv.org/abs/2305.11650](http://arxiv.org/abs/2305.11650)

    本文提出了动量匹配去噪Gibbs采样方法，可以在给定‘嘈杂’的模型的情况下，从干净的模型中有效地进行采样。

    

    能量基模型（EBMs）为建模复杂数据分布提供了一个通用的框架。然而，EBMs 的训练和采样仍然面临重大挑战。用于可扩展 EBM 训练的广泛使用的去噪分数匹配（DSM）方法存在不一致性问题，导致能量模型学习到“嘈杂”的数据分布。在本文中，我们提出了一种有效的采样框架：（伪）Gibbs采样与动量匹配，可以在给定经过DSM训练良好的“嘈杂”模型的情况下，从基础“干净”模型中有效地进行采样。我们探讨了我们的方法相对于相关方法的优势，并展示了如何将该方法扩展到高维数据集。

    Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
    
[^60]: 鲁棒决策树集成的可验证学习

    Verifiable Learning for Robust Tree Ensembles. (arXiv:2305.03626v1 [cs.LG])

    [http://arxiv.org/abs/2305.03626](http://arxiv.org/abs/2305.03626)

    本论文提出了一种可验证学习的方法，即训练易于验证的限制模型类来解决决策树集成的 NP-hard 问题，并成功设计出一种新的训练算法，使得在多项式时间内可以进行安全验证，而且仍保持着该领域最好的鲁棒性能。

    

    在测试时间内验证机器学习模型对抗攻击的鲁棒性是一个重要的研究问题。不幸的是，先前的研究确定，对于决策树集成，这个问题是 NP-hard ，因此对于特定的输入来说是不可解的。在本文中，我们确定了一类受限决策树集成，称为 large-spread 集成，其允许在多项式时间内运行安全验证算法。然后，我们提出了一种新方法，称为可验证学习，该方法倡导训练这种易于验证的受限模型类。我们通过设计一种新的训练算法，从标记数据中自动学习 large-spread 决策树集成来展示这种方法的益处，从而使其能够在多项式时间内进行安全验证。公开可用数据集上的实验结果证实，使用我们的算法训练的 large-spread 集成可以在几秒钟内使用标准半定编程求解器进行验证，同时对抗当前最先进的攻击具有竞争力的性能。

    Verifying the robustness of machine learning models against evasion attacks at test time is an important research problem. Unfortunately, prior work established that this problem is NP-hard for decision tree ensembles, hence bound to be intractable for specific inputs. In this paper, we identify a restricted class of decision tree ensembles, called large-spread ensembles, which admit a security verification algorithm running in polynomial time. We then propose a new approach called verifiable learning, which advocates the training of such restricted model classes which are amenable for efficient verification. We show the benefits of this idea by designing a new training algorithm that automatically learns a large-spread decision tree ensemble from labelled data, thus enabling its security verification in polynomial time. Experimental results on publicly available datasets confirm that large-spread ensembles trained using our algorithm can be verified in a matter of seconds, using stand
    
[^61]: MixPro：基于提示学习的简单有效的数据增强方式

    MixPro: Simple yet Effective Data Augmentation for Prompt-based Learning. (arXiv:2304.09402v1 [cs.CL])

    [http://arxiv.org/abs/2304.09402](http://arxiv.org/abs/2304.09402)

    MixPro是一种数据增强方法，通过对原始输入和模板进行混合来提高基于提示的学习性能，平均提高了5.08%的模型性能。

    

    基于提示的学习通过将输入与模板组合起来，将下游任务重构为填空问题。这种技术在少样本学习中特别有用，然而，使用有限的模板和文本仍然存在显着的性能改进空间。此外，现有的使用模型集成的方法可以限制模型的效率。为解决这些问题，我们提出了一种称为MixPro的增强方法，它通过标记级、句子级和时代级的混合策略来增强原始输入文本和模板。我们在五个少样本数据集上进行了实验，结果表明MixPro优于其他增强基线，相比增强前，平均提高了5.08%的模型性能。

    Prompt-based learning reformulates downstream tasks as cloze problems by combining the original input with a template. This technique is particularly useful in few-shot learning, where a model is trained on a limited amount of data. However, the limited templates and text used in few-shot prompt-based learning still leave significant room for performance improvement. Additionally, existing methods using model ensembles can constrain the model efficiency. To address these issues, we propose an augmentation method called MixPro, which augments both the vanilla input text and the templates through token-level, sentence-level, and epoch-level Mixup strategies. We conduct experiments on five few-shot datasets, and the results show that MixPro outperforms other augmentation baselines, improving model performance by an average of 5.08% compared to before augmentation.
    
[^62]: FMG-Net和W-Net：受多重网格启发的医学图像分割深度学习架构

    FMG-Net and W-Net: Multigrid Inspired Deep Learning Architectures For Medical Imaging Segmentation. (arXiv:2304.02725v1 [eess.IV])

    [http://arxiv.org/abs/2304.02725](http://arxiv.org/abs/2304.02725)

    FMG-Net和W-Net是两种受多重网格启发的深度学习架构，能够解决医学图像分割中面临的细节特征和尺度变化的挑战，能够提高肿瘤分割的精度。

    

    准确的医学图像分割对于精确和有效的医疗干预至关重要。然而，尽管卷积神经网络(CNN)在医学图像分割方面取得了成功，但它们仍面临着处理细粒度特征和图像尺度变化的挑战。这些挑战在复杂和具有挑战性的分割任务中特别明显，例如BraTS多标签脑肿瘤分割挑战赛中。在这个任务中，精确地分割不同的肿瘤亚组分，在大小和形状上都有显著变化，仍然是一个重大挑战，即使是最先进的方法也会产生重大错误。因此，我们提出了两种架构，FMG-Net和W-Net，它们将解线性方程组的几何多重网格方法的原理纳入CNN中，以解决这些挑战。我们在BraTS 2020数据集上的实验表明，FMG-Net和W-Net都优于广泛使用的U-Net架构，特别是在分割中的tum的精度方面。

    Accurate medical imaging segmentation is critical for precise and effective medical interventions. However, despite the success of convolutional neural networks (CNNs) in medical image segmentation, they still face challenges in handling fine-scale features and variations in image scales. These challenges are particularly evident in complex and challenging segmentation tasks, such as the BraTS multi-label brain tumor segmentation challenge. In this task, accurately segmenting the various tumor sub-components, which vary significantly in size and shape, remains a significant challenge, with even state-of-the-art methods producing substantial errors. Therefore, we propose two architectures, FMG-Net and W-Net, that incorporate the principles of geometric multigrid methods for solving linear systems of equations into CNNs to address these challenges. Our experiments on the BraTS 2020 dataset demonstrate that both FMG-Net and W-Net outperform the widely used U-Net architecture regarding tum
    
[^63]: 基于准度量学习的最优目标达成强化学习方法

    Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning. (arXiv:2304.01203v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.01203](http://arxiv.org/abs/2304.01203)

    本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数；在离线和在线的目标达成基准测试中，QRL展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    

    在目标达成强化学习中，最优价值函数具有特定的几何结构，称为准度量结构。本文介绍了一种新的强化学习方法——准度量强化学习（QRL），利用准度量模型来学习最优价值函数。与以往的方法不同，QRL的目标是专门为准度量设计的，并提供了强大的理论恢复保证。在离散化的MountainCar环境上进行了全面分析，确定了QRL的性质以及其优于其他方法的优势。在离线和在线的目标达成基准测试中，QRL还展示了更好的采样效率和性能，包括基于状态和基于图像的观测。

    In goal-reaching reinforcement learning (RL), the optimal value function has a particular geometry, called quasimetric structure. This paper introduces Quasimetric Reinforcement Learning (QRL), a new RL method that utilizes quasimetric models to learn optimal value functions. Distinct from prior approaches, the QRL objective is specifically designed for quasimetrics, and provides strong theoretical recovery guarantees. Empirically, we conduct thorough analyses on a discretized MountainCar environment, identifying properties of QRL and its advantages over alternatives. On offline and online goal-reaching benchmarks, QRL also demonstrates improved sample efficiency and performance, across both state-based and image-based observations.
    
[^64]: 云计算下的不确定性工作负载预测

    Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])

    [http://arxiv.org/abs/2303.13525](http://arxiv.org/abs/2303.13525)

    本文提出单变量和双变量贝叶斯深度学习模型，用于在云计算中预测未来的资源需求及其不确定性，并探讨不同的训练方式对预测准确性的影响。

    

    在云计算中预测未来的资源需求对于管理云数据中心并保证客户最低服务质量（QoS）水平至关重要。建模未来需求的不确定性可以提高预测的质量并减少由于资源过度分配而带来的浪费。本文提出了单变量和双变量贝叶斯深度学习模型，用于预测未来资源需求的分布及其不确定性。我们设计了不同的训练情景来训练这些模型，其中每个过程是在多个数据集配置上进行预训练和微调步骤的不同组合。我们还将双变量模型与其单变量对应模型进行比较，用单个或多个数据集进行训练，以研究不同组成部分如何影响预测的准确性并影响QoS。最后，我们研究了我们的模型是否具有迁移学习能力。广泛的实验表明，使用多个数据集进行预训练可以提高性能。

    Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
    
[^65]: 从宽到深：维度提升网络用于参数高效的知识图谱嵌入

    From Wide to Deep: Dimension Lifting Network for Parameter-efficient Knowledge Graph Embedding. (arXiv:2303.12816v1 [cs.LG])

    [http://arxiv.org/abs/2303.12816](http://arxiv.org/abs/2303.12816)

    本文提出了一个用于实现参数高效的知识图谱嵌入的深度网络，通过增加深度克服因采用低维实体表示而导致的模型精度下降和模型参数减少有限的问题。

    

    知识图谱嵌入（KGE）将实体和关系映射到向量表示对于下游任务非常重要。传统的KGE方法需要相对高维的实体表示来保留知识图谱的结构信息，但会导致庞大的模型参数。最近的方法通过采用低维实体表示来降低模型参数，同时开发技术（例如知识蒸馏）来补偿降维。然而，这样的操作会导致模型精度下降和模型参数减少有限。具体来说，我们将所有实体表示的级联视为嵌入层，那么采用高维实体表示的传统KGE方法等同于扩展嵌入层的宽度以获得表现力。为了在不牺牲准确度的情况下实现参数效率，我们相反地增加深度，并提出一个更深的实体嵌入网络。

    Knowledge graph embedding (KGE) that maps entities and relations into vector representations is essential for downstream tasks. Conventional KGE methods require relatively high-dimensional entity representations to preserve the structural information of knowledge graph, but lead to oversized model parameters. Recent methods reduce model parameters by adopting low-dimensional entity representations, while developing techniques (e.g., knowledge distillation) to compensate for the reduced dimension. However, such operations produce degraded model accuracy and limited reduction of model parameters. Specifically, we view the concatenation of all entity representations as an embedding layer, and then conventional KGE methods that adopt high-dimensional entity representations equal to enlarging the width of the embedding layer to gain expressiveness. To achieve parameter efficiency without sacrificing accuracy, we instead increase the depth and propose a deeper embedding network for entity re
    
[^66]: 使用深度离线强化学习实现安全的丙泊酚全麻剂量控制

    Towards Safe Propofol Dosing during General Anesthesia Using Deep Offline Reinforcement Learning. (arXiv:2303.10180v1 [cs.LG])

    [http://arxiv.org/abs/2303.10180](http://arxiv.org/abs/2303.10180)

    本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来实现全麻药物剂量控制，添加了保守Q-Learning方法和策略约束项以确保智能体做出更安全的决策。

    

    自动化麻醉有望实现更精确和个性化的麻醉管理，使麻醉师免于重复性任务，专注于患者手术护理的最关键方面。当前的研究通常集中于创建模拟环境，以便智能体进行学习。这些方法已经展示出良好的实验结果，但离临床应用还有很大的差距。本文提出了一种基于真实临床数据集的数据驱动强化学习算法Policy Constraint Q-Learning(PCQL)来解决学习麻醉策略的问题。首先引入了保守Q-Learning方法以缓解脱线情况下Q函数过度估计的问题。在智能体的培训中添加一项策略约束项，以保持智能体和麻醉师的策略分布一致，以确保智能体在全麻情景下做出更安全的决策。通过实验证明了PCQL的有效性。

    Automated anesthesia promises to enable more precise and personalized anesthetic administration and free anesthesiologists from repetitive tasks, allowing them to focus on the most critical aspects of a patient's surgical care. Current research has typically focused on creating simulated environments from which agents can learn. These approaches have demonstrated good experimental results, but are still far from clinical application. In this paper, Policy Constraint Q-Learning (PCQL), a data-driven reinforcement learning algorithm for solving the problem of learning anesthesia strategies on real clinical datasets, is proposed. Conservative Q-Learning was first introduced to alleviate the problem of Q function overestimation in an offline context. A policy constraint term is added to agent training to keep the policy distribution of the agent and the anesthesiologist consistent to ensure safer decisions made by the agent in anesthesia scenarios. The effectiveness of PCQL was validated b
    
[^67]: 使用Prompt-Tuning的原型转向针对无需重复训练的连续学习

    Steering Prototype with Prompt-tuning for Rehearsal-free Continual Learning. (arXiv:2303.09447v1 [cs.LG])

    [http://arxiv.org/abs/2303.09447](http://arxiv.org/abs/2303.09447)

    本研究提出了一个新的连续学习模型——对比原型提示，使用任务特异性提示调整来提高原型性能，同时避免了语义漂移和原型干扰问题。基于此模型的CPP方法在四个具有挑战性的类增量学习基准测试中表现出色，相对于其他最先进的方法有4%至6%的绝对提升。该方法不需要重复训练，性能接近离线联合学习，展示了一种有前途的设计方案。

    

    原型作为类别嵌入的一种表示，已被探索用于减少连续学习情境下的内存占用或减轻遗忘。然而，基于原型的方法仍然存在语义漂移和原型干扰导致的性能急剧恶化的问题。在本研究中，我们提出了对比原型提示（CPP）方法，并展示了任务特定提示调整，当在对比学习目标上进行优化时，可以有效地解决这两个障碍并显着提高原型的性能。我们的实验表明，CPP在四个具有挑战性的类增量学习基准测试中表现出色，相对于现有最先进方法有4%至6%的绝对提升。此外，CPP不需要重复训练，它极大地缩小了连续学习和离线联合学习之间的性能差距，展示了一种有前途的Transformer体系结构下连续学习系统的设计方案。

    Prototype, as a representation of class embeddings, has been explored to reduce memory footprint or mitigate forgetting for continual learning scenarios. However, prototype-based methods still suffer from abrupt performance deterioration due to semantic drift and prototype interference. In this study, we propose Contrastive Prototypical Prompt (CPP) and show that task-specific prompt-tuning, when optimized over a contrastive learning objective, can effectively address both obstacles and significantly improve the potency of prototypes. Our experiments demonstrate that CPP excels in four challenging class-incremental learning benchmarks, resulting in 4% to 6% absolute improvements over state-of-the-art methods. Moreover, CPP does not require a rehearsal buffer and it largely bridges the performance gap between continual learning and offline joint-learning, showcasing a promising design scheme for continual learning systems under a Transformer architecture.
    
[^68]: 可证收敛的即插即用拟牛顿方法

    Provably Convergent Plug-and-Play Quasi-Newton Methods. (arXiv:2303.07271v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.07271](http://arxiv.org/abs/2303.07271)

    本文提出了一种可证明收敛的PnP方法，使用拟牛顿步骤以加速收敛，相对于现有的PnP方法对去噪器或保真度函数施加了较轻的限制。

    

    即插即用（PnP）方法是一类高效的迭代算法，旨在利用经典优化算法（如ISTA或ADMM），将数据保真度项和深度去噪器相结合。现有的可证明的PnP方法对去噪器或保真度函数施加了严格的限制，如非扩张性或严格凸性。本文提出了一种可证明的PnP方法，该方法基于近端去噪器施加相对较轻的条件，并引入了拟牛顿步骤以大大加速收敛。通过将深度去噪器特别参数化为梯度步骤，我们进一步将拟牛顿PnP算法的固定点表征为可能非凸函数的临界点。

    Plug-and-Play (PnP) methods are a class of efficient iterative methods that aim to combine data fidelity terms and deep denoisers using classical optimization algorithms, such as ISTA or ADMM. Existing provable PnP methods impose heavy restrictions on the denoiser or fidelity function, such as nonexpansiveness or strict convexity. In this work, we propose a provable PnP method that imposes relatively light conditions based on proximal denoisers, and introduce a quasi-Newton step to greatly accelerate convergence. By specially parameterizing the deep denoiser as a gradient step, we further characterize the fixed-points of the quasi-Newton PnP algorithm as critical points of a possibly non-convex function.
    
[^69]: 自动设计元启发式算法的综述

    A Survey on Automated Design of Metaheuristic Algorithms. (arXiv:2303.06532v1 [cs.NE])

    [http://arxiv.org/abs/2303.06532](http://arxiv.org/abs/2303.06532)

    本文综述了自动设计元启发式算法的形式化、方法论、挑战和研究趋势，讨论了自动设计的潜在未来方向和开放问题。

    This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, and discusses the potential future directions and open issues in this field.

    元启发式算法由于其能够独立于问题结构和问题领域进行搜索的能力，已经引起了学术界和工业界的广泛关注。通常，需要人类专家手动调整算法以适应解决目标问题。手动调整过程可能是费力的、容易出错的，并且需要大量的专业知识。这引起了对自动设计元启发式算法的越来越多的兴趣和需求，以减少人类干预。自动设计可以使高性能算法对更广泛的研究人员和实践者可用；通过利用计算能力来充分探索潜在的设计选择，自动设计可以达到甚至超过人类水平的设计。本文通过对现有工作的共同点和差异进行调查，提出了自动设计元启发式算法的形式化、方法论、挑战和研究趋势的广泛概述。我们还讨论了这一领域的潜在未来方向和开放问题。

    Metaheuristic algorithms have attracted wide attention from academia and industry due to their capability of conducting search independent of problem structures and problem domains. Often, human experts are requested to manually tailor algorithms to fit for solving a targeted problem. The manual tailoring process may be laborious, error-prone, and require intensive specialized knowledge. This gives rise to increasing interests and demands for automated design of metaheuristic algorithms with less human intervention. The automated design could make high-performance algorithms accessible to a much broader range of researchers and practitioners; and by leveraging computing power to fully explore the potential design choices, automated design could reach or even surpass human-level design. This paper presents a broad picture of the formalization, methodologies, challenges, and research trends of automated design of metaheuristic algorithms, by conducting a survey on the common grounds and 
    
[^70]: 从人类反应数据中进行最优和私密学习

    Optimal and Private Learning from Human Response Data. (arXiv:2303.06234v1 [cs.LG])

    [http://arxiv.org/abs/2303.06234](http://arxiv.org/abs/2303.06234)

    本文提出了一种新的谱估计算法，具有高效和准确的特点，可以在温和的采样条件下实现极小化最优误差界限，同时在top-$K$恢复的样本复杂度方面具有最优性。

    This paper proposes a new spectral estimation algorithm that is efficient and accurate, achieving the minimax optimal error bound (modulo a log factor) under mild sampling conditions, and enjoys optimal sample complexity for top-$K$ recovery.

    项目反应理论（IRT）是研究人们如何做出概率决策的学科，具有教育测试、推荐系统等多种应用。二元反应数据的Rasch模型是IRT中最基本的模型之一，仍然是一个具有重要实际意义的研究领域。最近，Nguyen和Zhang（2022）提出了一种新的谱估计算法，具有高效和准确的特点。在本文中，我们以两种重要的方式扩展了他们的结果。首先，我们获得了谱算法的精细逐项误差界限，补充了他们工作中的“平均误差”$\ell_2$界限。值得注意的是，在温和的采样条件下，谱算法实现了极小化最优误差界限（模除对数因子）。在精细分析的基础上，我们还展示了谱算法在top-$K$恢复的样本复杂度方面具有最优性（例如，从批准/不批准反应数据中识别最佳的$K$个项目），解释了实验结果。

    Item response theory (IRT) is the study of how people make probabilistic decisions, with diverse applications in education testing, recommendation systems, among others. The Rasch model of binary response data, one of the most fundamental models in IRT, remains an active area of research with important practical significance. Recently, Nguyen and Zhang (2022) proposed a new spectral estimation algorithm that is efficient and accurate. In this work, we extend their results in two important ways. Firstly, we obtain a refined entrywise error bound for the spectral algorithm, complementing the `average error' $\ell_2$ bound in their work. Notably, under mild sampling conditions, the spectral algorithm achieves the minimax optimal error bound (modulo a log factor). Building on the refined analysis, we also show that the spectral algorithm enjoys optimal sample complexity for top-$K$ recovery (e.g., identifying the best $K$ items from approval/disapproval response data), explaining the empir
    
[^71]: DeepGD: 一种用于深度神经网络的多目标黑盒测试选择方法

    DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04878](http://arxiv.org/abs/2303.04878)

    DeepGD是一种用于深度神经网络的多目标黑盒测试选择方法，通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。

    

    深度神经网络（DNN）被广泛应用于图像处理、语音识别和自然语言处理等各个应用领域。然而，由于其输入域的复杂性和规模，测试DNN模型可能具有挑战性。特别地，测试DNN模型通常需要生成或探索大规模的未标记数据集。在实践中，DNN测试神谕通常需要昂贵的人工工作来标记测试数据，可能涉及多个专家以确保标记的正确性。在本文中，我们提出了DeepGD，一种用于DNN模型的黑盒多目标测试选择方法。它通过优先选择具有高错误暴露能力的测试输入来降低标记成本，同时选择具有高不确定性分数的测试输入以尽可能触发更多的误预测输入，并通过最大化揭示DNN模型中不同缺陷的概率来增加测试的多样性。

    Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by s
    
[^72]: 变调神经ODEs

    Modulated Neural ODEs. (arXiv:2302.13262v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.13262](http://arxiv.org/abs/2302.13262)

    变调神经ODEs （MoNODEs）是一种新的框架，能够将动力学状态与基础静态变化因素分开，并改进了现有的神经ODE方法。该方法通过引入时间不变的调制变量来捕捉轨迹间的变化，并在测试中展现出在振荡系统、视频和人类行走轨迹等方面具有提高模型泛化能力的效果。

    

    神经常微分方程（NODEs）已被证明对于学习任意轨迹的非线性动力学很有用。然而，当前的NODE方法仅通过初始状态值或自回归编码器更新来捕捉轨迹间的变化。在这项工作中，我们引入了变调神经ODEs（MoNODEs），这是一个将动力学状态与基础静态变化因素分开并改进现有NODE方法的新框架。特别地，我们引入了从数据中学习的“时间不变调制变量”。我们将我们提出的框架结合到四种现有的NODE变体中。我们在振荡系统、视频和人类行走轨迹上对MoNODE进行了测试，其中每个轨迹都具有轨迹特定的调制。我们的框架始终提高了现有模型的泛化能力，使其能够适应新的动态参数化并进行远期预测。此外，我们验证了提出的调制变量的信息量。

    Neural ordinary differential equations (NODEs) have been proven useful for learning non-linear dynamics of arbitrary trajectories. However, current NODE methods capture variations across trajectories only via the initial state value or by auto-regressive encoder updates. In this work, we introduce Modulated Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from underlying static factors of variation and improves the existing NODE methods. In particular, we introduce $\textit{time-invariant modulator variables}$ that are learned from the data. We incorporate our proposed framework into four existing NODE variants. We test MoNODE on oscillating systems, videos and human walking trajectories, where each trajectory has trajectory-specific modulation. Our framework consistently improves the existing model ability to generalize to new dynamic parameterizations and to perform far-horizon forecasting. In addition, we verify that the proposed modulator variables are infor
    
[^73]: InstructABSA: 基于指令学习的方面情感分析

    InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08624](http://arxiv.org/abs/2302.08624)

    InstructABSA是一种使用指令学习范式的方面情感分析方法，能够显著提高Aspect Term Extraction、Aspect Term Sentiment Classification、和Joint Task subtasks三个子任务的性能，并且在多个数据集上表现超过之前的最先进方法。

    

    本文介绍了InstructABSA，一种使用指令学习范式进行Aspect Based Sentiment Analysis (ABSA) 所有子任务（Aspect Term Extraction (ATE)，Aspect Term Sentiment Classification (ATSC)，以及Joint Task modeling）的方法。我们的方法对每个训练样本引入了正面、负面、和中性的例子，并使用指令来调整每个ABSA子任务的模型（Tk-Instruct），从而显著提高了性能。在Sem Eval 2014、2015和2016数据集上的实验结果表明，在所有三个ABSA子任务（ATE、ATSC和Joint Task）上，InstructABSA在性能上都比之前的最先进方法（SOTA）表现出了显著的优势，并且表现超过了7倍大的模型。特别是，在Rest14 ATE子任务上，InstructABSA超过了SOTA 7.31%的得分，Rest15 ATSC子任务上也有提升，并且在Lapt14 Joint Task上的表现提升了8.63%点。我们的结果还表明，对于所有三个子任务，InstructABSA具有强大的新领域泛化能力。

    In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
    
[^74]: 符号音乐的字节对编码

    Byte Pair Encoding for Symbolic Music. (arXiv:2301.11975v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11975](http://arxiv.org/abs/2301.11975)

    本文介绍了一种在符号音乐领域中使用的字节对编码技术，它可以显著减小序列长度并增加词汇量，从而提高语言模型的嵌入能力。

    

    当与深度学习结合使用时，符号音乐通常与语言模型架构相结合。为此，音乐需要进行标记化，即转化为一系列离散的标记。可以通过不同的方法实现这一点，因为音乐可以由同时存在的轨道，具有多个属性的同时音符组成。目前，所提出的标记化依赖于描述音符属性和时间事件的小型标记字典，导致标记序列相当长，对语言模型的嵌入空间的使用不够优化。最近的研究致力于通过合并嵌入或组合标记来减少整体序列长度。在本文中，我们展示了字节对编码，一种广泛用于自然语言的压缩技术，其显著减小了序列长度，同时增加了词汇量。通过这样做，我们利用这些模型的嵌入能力与更有表现力的标记结合，从而得到更好的结果。

    When used with deep learning, the symbolic music modality is often coupled with language model architectures. To do so, the music needs to be tokenized, i.e. converted into a sequence of discrete tokens. This can be achieved by different approaches, as music can be composed of simultaneous tracks, of simultaneous notes with several attributes. Until now, the proposed tokenizations rely on small vocabularies of tokens describing the note attributes and time events, resulting in fairly long token sequences, and a sub-optimal use of the embedding space of language models. Recent research has put efforts on reducing the overall sequence length by merging embeddings or combining tokens. In this paper, we show that Byte Pair Encoding, a compression technique widely used for natural language, significantly decreases the sequence length while increasing the vocabulary size. By doing so, we leverage the embedding capabilities of such models with more expressive tokens, resulting in both better 
    
[^75]: 对Transformer编码器表达能力的更紧密界定

    Tighter Bounds on the Expressivity of Transformer Encoders. (arXiv:2301.10743v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.10743](http://arxiv.org/abs/2301.10743)

    本文旨在更紧密地界定Transformer编码器的表达能力，提供了一个同时是下限和上限的一阶逻辑变体的策略，使我们更加接近准确刻画Transformer编码器可识别语言的目标。

    

    用更易理解的形式系统表征神经网络有潜力揭示这些网络的能力和局限性，然而对于Transformer来说这仍是一个活跃的研究领域。Bhattamishra等人已经表明，Transformer编码器至少与一种特定的计数机同等表达能力，而Merrill和Sabharwal则表明固定精度的Transformer编码器只能识别统一的$TC^0$语言。我们通过确认具有计数量词的一阶逻辑的变体，既是固定精度Transformer编码器的上界，也是Transformer编码器的下界，从而将这些结果联系起来并加以加强。这使我们比以前更接近准确刻画Transformer编码器可识别的语言的目标。

    Characterizing neural networks in terms of better-understood formal systems has the potential to yield new insights into the power and limitations of these networks. Doing so for transformers remains an active area of research. Bhattamishra and others have shown that transformer encoders are at least as expressive as a certain kind of counter machine, while Merrill and Sabharwal have shown that fixed-precision transformer encoders recognize only languages in uniform $TC^0$. We connect and strengthen these results by identifying a variant of first-order logic with counting quantifiers that is simultaneously an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders. This brings us much closer than before to an exact characterization of the languages that transformer encoders recognize.
    
[^76]: HTR模型训练的挑战：《数字时代的档案保护计划》项目反馈

    The Challenges of HTR Model Training: Feedback from the Project Donner le gout de l'archive a l'ere numerique. (arXiv:2212.11146v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.11146](http://arxiv.org/abs/2212.11146)

    本文介绍了使用Transkribus平台改进手写文本识别（HTR）模型性能的实践经验，包括创建转录协议、完整使用语言模型以及确定最佳使用基础模型的方法，这些方法可以将单个模型的性能提高20%以上（达到字符错误率低于5%），并讨论了HTR平台的合作性质和数据分享等挑战。

    

    手写识别技术的出现为文化遗产研究带来了新的可能性，但现在需要反思研究团队开发的经验和实践。我们自2018年以来使用Transkribus平台，致力于提高手写文本识别（HTR）模型的性能，用于转录17世纪的法语手写文本。本文报告了创建转录协议、完整使用语言模型以及确定最佳使用基础模型的影响，以帮助提高HTR模型的性能。将所有这些元素结合起来可以将单个模型的性能提高20%以上（达到字符错误率低于5%）。本文还讨论了HTR平台（如Transkribus）的合作性质以及研究人员如何分享其数据等方面的挑战。

    The arrival of handwriting recognition technologies offers new possibilities for research in heritage studies. However, it is now necessary to reflect on the experiences and the practices developed by research teams. Our use of the Transkribus platform since 2018 has led us to search for the most significant ways to improve the performance of our handwritten text recognition (HTR) models which are made to transcribe French handwriting dating from the 17th century. This article therefore reports on the impacts of creating transcribing protocols, using the language model at full scale and determining the best way to use base models in order to help increase the performance of HTR models. Combining all of these elements can indeed increase the performance of a single model by more than 20% (reaching a Character Error Rate below 5%). This article also discusses some challenges regarding the collaborative nature of HTR platforms such as Transkribus and the way researchers can share their da
    
[^77]: 面向大规模灵活绑定高斯混合模型的随机一阶学习

    Stochastic First-Order Learning for Large-Scale Flexibly Tied Gaussian Mixture Model. (arXiv:2212.05402v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.05402](http://arxiv.org/abs/2212.05402)

    本文提出了一种面对高维流数据和复杂密度挑战的快速在线参数估计算法，并利用灵活绑定因子分解的协方差矩阵提供了一个框架。通过引入一阶随机优化和新的随机流形优化算法，实现了高斯混合模型的优化。

    

    高斯混合模型(GMM)是一种基于核模型的最有效的参数密度估计器，广泛应用于许多科学领域。随着数据源的剧增，传统的机器学习算法，如期望最大化(EM)，在面对高维和流数据时遇到困难。此外，复杂的密度往往需要大量的高斯组件。本文提出了一种快速的在线参数估计算法，通过使用一阶随机优化来处理GMM所面临的高维流数据和复杂密度的挑战，利用协方差矩阵的灵活绑定因子分解提供了一个框架。引入了一种新的随机流形优化算法，保持正交性，并与众所周知的欧几里得空间数值优化一起使用。在合成和实验数据上进行了大量实证结果。

    Gaussian Mixture Models (GMM) are one of the most potent parametric density estimators based on the kernel model that finds application in many scientific domains. In recent years, with the dramatic enlargement of data sources, typical machine learning algorithms, e.g. Expectation Maximization (EM), encounters difficulty with high-dimensional and streaming data. Moreover, complicated densities often demand a large number of Gaussian components. This paper proposes a fast online parameter estimation algorithm for GMM by using first-order stochastic optimization. This approach provides a framework to cope with the challenges of GMM when faced with high-dimensional streaming data and complex densities by leveraging the flexibly-tied factorization of the covariance matrix. A new stochastic Manifold optimization algorithm that preserves the orthogonality is introduced and used along with the well-known Euclidean space numerical optimization. Numerous empirical results on both synthetic and 
    
[^78]: 多目标强化学习中的福利和公平性

    Welfare and Fairness in Multi-objective Reinforcement Learning. (arXiv:2212.01382v4 [cs.GT] UPDATED)

    [http://arxiv.org/abs/2212.01382](http://arxiv.org/abs/2212.01382)

    本论文研究了多目标强化学习中的福利和公平性问题，提出了一种基于非线性福利函数的Q-learning算法，通过非线性标量化学习更新和非稳态动作选择来优化策略。算法被证明是可收敛的。

    

    我们研究了公平多目标强化学习，其中一个代理必须学习一个能够在多个维度的向量值奖励上同时获得高回报的策略。受公平资源分配文献的启发，我们将其建模为期望福利最大化问题，针对向量的长期累积奖励的非线性公平福利函数。其中一个经典的例子是纳什社会福利函数，或者几何平均数，其对数变换也被称为比例公平目标。我们表明，即使在表格化的情况下，对期望纳什社会福利进行近似最优化也是计算上难以处理的。尽管如此，我们提供了一种创新的Q-learning改进方法，结合非线性标量化学习更新和非稳态动作选择，以学习有效的优化非线性福利函数的策略。我们证明了我们的算法是可收敛的，并进行了实验证明。

    We study fair multi-objective reinforcement learning in which an agent must learn a policy that simultaneously achieves high reward on multiple dimensions of a vector-valued reward. Motivated by the fair resource allocation literature, we model this as an expected welfare maximization problem, for some non-linear fair welfare function of the vector of long-term cumulative rewards. One canonical example of such a function is the Nash Social Welfare, or geometric mean, the log transform of which is also known as the Proportional Fairness objective. We show that even approximately optimal optimization of the expected Nash Social Welfare is computationally intractable even in the tabular case. Nevertheless, we provide a novel adaptation of Q-learning that combines non-linear scalarized learning updates and non-stationary action selection to learn effective policies for optimizing nonlinear welfare functions. We show that our algorithm is provably convergent, and we demonstrate experimental
    
[^79]: SciRepEval：一个用于科学文献表示的多格式基准

    SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2211.13308](http://arxiv.org/abs/2211.13308)

    SciRepEval是第一个综合评估科学文献表示的全面基准，其中包括四种格式的 25 个任务。通过使用格式特定的控制代码和适配器，可以改进科学文献表示模型的泛化能力。

    

    学习的科学文献表示可以作为下游任务的有价值输入特征，无需进一步微调。然而，用于评估这些表示的现有基准未能捕捉到相关任务的多样性。为此，我们介绍了 SciRepEval，第一个用于训练和评估科学文献表示的全面基准。它包括四种格式的 25 个具有挑战性和现实性的任务，其中 11 个是新任务：分类、回归、排名和搜索。我们使用该基准来研究和改进科学文档表示模型的泛化能力。我们展示了最先进的模型如何在任务格式方面缺乏泛化性能，简单的多任务训练也不能改进它们。然而，一种新的方法，学习每个文档的多个嵌入，每个嵌入专门针对不同的格式，可以提高性能。我们尝试使用任务格式特定的控制代码和适配器。

    Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
    
[^80]: 基于最优传输的实例相关泛化界限

    Instance-Dependent Generalization Bounds via Optimal Transport. (arXiv:2211.01258v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.01258](http://arxiv.org/abs/2211.01258)

    该论文提出了一种基于最优传输的实例相关泛化界限的方法，以解释神经网络泛化的关键因素，并且考虑了初始化和随机梯度下降的强归纳偏差。这种方法在模型参数化不可知且训练样本数量远小于参数数量时表现良好，还可以应用于低维流形上的数据和分布转换情况下的泛化问题。

    

    现有的泛化界限无法解释影响现代神经网络泛化的关键因素。由于这些界限通常对所有参数都是一致的，它们容易过度参数化，并且无法考虑到初始化和随机梯度下降的强归纳偏差。作为替代方案，我们提出了一种新颖的最优传输解释泛化问题的方法。这使我们能够获得依赖于数据空间中预测函数的局部利普希茨正则性的实例相关泛化界限。因此，我们的界限对模型的参数化是不可知的，并且在训练样本数量远小于参数数量时表现良好。通过一些小的修改，我们的方法在低维流形上的数据上可以获得加速的速率，并且在分布转换下具有保证。我们通过对神经网络的实证分析来验证我们的泛化界限，结果显示界限值是

    Existing generalization bounds fail to explain crucial factors that drive generalization of modern neural networks. Since such bounds often hold uniformly over all parameters, they suffer from over-parametrization, and fail to account for the strong inductive bias of initialization and stochastic gradient descent. As an alternative, we propose a novel optimal transport interpretation of the generalization problem. This allows us to derive instance-dependent generalization bounds that depend on the local Lipschitz regularity of the earned prediction function in the data space. Therefore, our bounds are agnostic to the parametrization of the model and work well when the number of training samples is much smaller than the number of parameters. With small modifications, our approach yields accelerated rates for data on low-dimensional manifolds, and guarantees under distribution shifts. We empirically analyze our generalization bounds for neural networks, showing that the bound values are 
    
[^81]: 针对井测数据的非对比度表示学习

    Non-contrastive representation learning for intervals from well logs. (arXiv:2209.14750v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.14750](http://arxiv.org/abs/2209.14750)

    本文提出了一种新的方法来处理井测数据表示学习问题，采用自我监督学习的方法进行非对比度的表示学习，减少对数据的标注需求，并提高了算法性能。

    

    石油和天然气行业中的表示学习问题旨在构建一个模型，根据钻井数据为井段提供表示形式。以往的尝试主要是有监督的，并且关注于相似性任务，即估计井段之间的相似程度。我们希望在不使用已标记数据的情况下构建信息量丰富的表示形式。其中一个可能的方法是自我监督学习（SSL）。与有监督范式相反，这个方法对数据需要很少或者没有标签。现今，大多数SSL方法要么是对比的，要么是非对比的。对比方法使相似的（正）对象的表示变得更加接近，并将不同的（负）对象与之距离。由于可能存在错误的正负标注，这些方法可能会提供更差的性能。非对比方法不依赖于此类标注，在计算机视觉领域广泛应用。它们仅使用容易识别的相似对象对进行学习。

    The representation learning problem in the oil & gas industry aims to construct a model that provides a representation based on logging data for a well interval. Previous attempts are mainly supervised and focus on similarity task, which estimates closeness between intervals. We desire to build informative representations without using supervised (labelled) data. One of the possible approaches is self-supervised learning (SSL). In contrast to the supervised paradigm, this one requires little or no labels for the data. Nowadays, most SSL approaches are either contrastive or non-contrastive. Contrastive methods make representations of similar (positive) objects closer and distancing different (negative) ones. Due to possible wrong marking of positive and negative pairs, these methods can provide an inferior performance. Non-contrastive methods don't rely on such labelling and are widespread in computer vision. They learn using only pairs of similar objects that are easier to identify in 
    
[^82]: RankSEG:一种基于一致排序的分割框架

    RankSEG: A Consistent Ranking-based Framework for Segmentation. (arXiv:2206.13086v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.13086](http://arxiv.org/abs/2206.13086)

    本文提出了一种新型的一致排序框架，即RankDice/RankIoU，用于解决由于现有的分割框架对于Dice/IoU指标缺乏一致性而可能导致的次优解决方案。

    

    分割已成为计算机视觉和自然语言处理的基本领域，它将标签分配给每个像素/特征，以从图像/文本中提取感兴趣的区域。为了评估分割性能，使用Dice和IoU指标来衡量实际值和预测分割之间的重叠程度。本文建立了与Dice/IoU指标相关的分割理论基础，包括类比于分类的贝叶斯规则和Dice-/IoU-校准。我们证明了现有的基于阈值的框架对于Dice/IoU指标缺乏一致性，因此可能导致次优解决方案。为解决这个问题，我们提出了一种新颖的一致排序框架，即RankDice/RankIoU，受贝叶斯分割规则的插入法则的启发。本文提供了三个使用GPU并行执行的数字算法。

    Segmentation has emerged as a fundamental field of computer vision and natural language processing, which assigns a label to every pixel/feature to extract regions of interest from an image/text. To evaluate the performance of segmentation, the Dice and IoU metrics are used to measure the degree of overlap between the ground truth and the predicted segmentation. In this paper, we establish a theoretical foundation of segmentation with respect to the Dice/IoU metrics, including the Bayes rule and Dice-/IoU-calibration, analogous to classification-calibration or Fisher consistency in classification. We prove that the existing thresholding-based framework with most operating losses are not consistent with respect to the Dice/IoU metrics, and thus may lead to a suboptimal solution. To address this pitfall, we propose a novel consistent ranking-based framework, namely RankDice/RankIoU, inspired by plug-in rules of the Bayes segmentation rule. Three numerical algorithms with GPU parallel exe
    
[^83]: HyperMixer：一种基于MLP的低成本Transformer替代方案

    HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2203.03691](http://arxiv.org/abs/2203.03691)

    HyperMixer是一种低成本的基于MLP的Transformer替代方案，通过动态形成标记混合MLP来实现自然语言理解，其性能比替代方案好，并可与Transformer媲美，成本更低。

    

    Transformer架构是自然语言理解的首选模型，但它们的成本相当高，因为它们在输入长度方面具有二次复杂度，需要大量的训练数据，并且可能难以调整。为了降低成本，我们研究了简单的基于MLP的架构。我们发现现有的架构（例如MLPMixer）通过静态的MLP独立地应用于每个特征，而过于脱离自然语言理解所需的归纳偏差。在本文中，我们提出了一种简单的改进，即HyperMixer，它使用超网络动态地形成标记混合MLP。实验上，我们证明了我们的模型表现优于替代的基于MLP的模型，并与Transformer媲美。与Transformer不同，HyperMixer在处理时间、训练数据和超参数调整方面具有大大降低的成本。

    Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
    

