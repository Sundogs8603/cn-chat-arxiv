# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Graph-based Neural Weather Prediction for Limited Area Modeling.](http://arxiv.org/abs/2309.17370) | 该论文提出了一种基于图像神经网络的有限区域天气预测方法，并通过使用北欧地区的本地模型进行了验证。 |
| [^2] | [3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information.](http://arxiv.org/abs/2309.17366) | 3D-Mol是一种新颖的基于3D结构的分子建模方法，通过对比学习提高了分子性质预测准确性，并在多个基准数据集上超过了最先进的模型。 |
| [^3] | [Module-wise Training of Neural Networks via the Minimizing Movement Scheme.](http://arxiv.org/abs/2309.17357) | 通过引入模块化正则化方法，解决了神经网络模块化训练中早期层过拟合和深层停滞的问题，实验结果展示了该方法在不同架构上的优越性。 |
| [^4] | [Efficient Biologically Plausible Adversarial Training.](http://arxiv.org/abs/2309.17348) | 本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。 |
| [^5] | [Demographic Parity: Mitigating Biases in Real-World Data.](http://arxiv.org/abs/2309.17347) | 本论文提出了一种方法，可以在训练模型时消除历史数据中的偏见，并在最大程度保留分类效用。这种方法以模型无关的方式从现实世界数据中衍生出具有人口平衡和真实性编码的数据集，并在实验证明中取得了成功。 |
| [^6] | [Towards Free Data Selection with General-Purpose Models.](http://arxiv.org/abs/2309.17342) | 本文提出了一种新的数据选择流程，利用通用模型在单次推理中选择来自不同数据集的数据，无需额外的训练或监督。通过定义和利用语义模式提取微妙的局部信息，我们实现了对所有数据样本的选择。 |
| [^7] | [MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search.](http://arxiv.org/abs/2309.17341) | 本研究提出了一种称为MixQuant的混合精度量化算法，在每个层权重上找到了最佳的量化位宽，从而减少了量化模型的准确性降低问题。 |
| [^8] | [Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer.](http://arxiv.org/abs/2309.17340) | Outage-Watch使用极端事件正则化的方法提早预测云服务中断，通过捕获质量指标的恶化情况，改善灵活性并提高尾部分布的学习能力。 |
| [^9] | [Scaling Experiments in Self-Supervised Cross-Table Representation Learning.](http://arxiv.org/abs/2309.17339) | 本文介绍了一种新颖的基于Transformer的架构，用于深度表格表示学习，以及针对跨表格表示学习的方法。通过自监督训练和不同规模的模型训练，该架构在单表和跨表预训练设置中展现了良好的扩展性能。 |
| [^10] | [Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints.](http://arxiv.org/abs/2309.17338) | 本文引入了一种新的框架，通过航点去除技术促进了显式的时间学习，并显著提高了轨迹预测的效果。 |
| [^11] | [Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools.](http://arxiv.org/abs/2309.17337) | 该论文提出了一种实现机器学习公平性的管道意识方法，并强调需要指南和工具来在实践中应用这种方法。 |
| [^12] | [Asynchronous Graph Generators.](http://arxiv.org/abs/2309.17335) | 异步图生成器（AGG）是一种新型的图神经网络架构，通过节点生成进行数据插补，并隐式学习传感器测量的因果图表示，取得了state-of-the-art的结果。 |
| [^13] | [Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks.](http://arxiv.org/abs/2309.17329) | 本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。 |
| [^14] | [Robust Stochastic Optimization via Gradient Quantile Clipping.](http://arxiv.org/abs/2309.17316) | 本文介绍了一种基于梯度分位数剪切的鲁棒性随机优化策略，适用于光滑目标且能容忍异常值和尾重样本。对于强凸目标，迭代收敛到集中分布并导出了估计误差的概率界。在非凸情况下，极限分布局部化在低梯度邻域上。使用滚动分位数实现的算法具有很强的鲁棒性和高效性。 |
| [^15] | [Leave-one-out Distinguishability in Machine Learning.](http://arxiv.org/abs/2309.17310) | 这项研究引入了一种新的分析框架，用于衡量机器学习算法在训练集中的少量数据点被排除后输出分布的变化。通过使用高斯过程模型和成员推断攻击的经验分析，该方法实现了对数据记忆和信息泄漏的有效衡量和优化。 |
| [^16] | [Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation.](http://arxiv.org/abs/2309.17296) | 本论文探索了E(3)等变扩散模型的设计空间，提出了EQGAT-diff模型，通过在连续的原子位置和分类的化学元素与键类型之间的交互中改进，其在从头设计3D分子方面的性能显著超过已有模型。 |
| [^17] | [In search of dispersed memories: Generative diffusion models are associative memory networks.](http://arxiv.org/abs/2309.17290) | 本研究将生成扩散模型解释为基于能量的模型，证明其在训练离散模式时与现代Hopfield网络的能量函数等效。这种等效性使得我们可以将扩散模型的有监督训练解释为在权重结构中编码现代Hopfield网络的关联动力学的突触学习过程。 |
| [^18] | [Toward Robust Recommendation via Real-time Vicinal Defense.](http://arxiv.org/abs/2309.17278) | 本研究提出了一种实时邻近防御方法（RVD），通过在推荐之前利用邻近的训练数据微调模型，以提高推荐系统对毒化攻击的防御能力。 |
| [^19] | [Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind.](http://arxiv.org/abs/2309.17275) | 本研究基于贝叶斯心智论机制设计了教师代理，其通过构建学习者内部状态模型并利用模型选择能够最大化学习者回报同时最小化教学成本的示范进行个性化教学，实验证明这种教学方式比传统不可知教学更高效。 |
| [^20] | [A Foundation Model for General Moving Object Segmentation in Medical Images.](http://arxiv.org/abs/2309.17264) | 本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果 |
| [^21] | [Estimation and Inference in Distributional Reinforcement Learning.](http://arxiv.org/abs/2309.17262) | 本文研究了分布式强化学习中的估计和推断问题，通过使用等价确定法，在提供生成模型的情况下以高效的方式解决了分布式策略评估问题。 |
| [^22] | [PlaceNav: Topological Navigation through Place Recognition.](http://arxiv.org/abs/2309.17260) | PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。 |
| [^23] | [Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering.](http://arxiv.org/abs/2309.17249) | 本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。 |
| [^24] | [Data-driven localized waves and parameter discovery in the massive Thirring model via extended physics-informed neural networks with interface zones.](http://arxiv.org/abs/2309.17240) | 本文研究了通过基于物理信息神经网络（PINNs）算法的深度学习方法，在大Thirring模型中实现数据驱动的局域波解和参数发现。通过模拟和分析，得到了各种类型的局域波解，包括孤子、呼吸子和流氓波。通过扩展PINNs和域分解方法，成功捕捉了高阶局域波解的动态行为。特别地，通过修改界面线和引入界面条件，有效地应用于不同类型的解。 |
| [^25] | [LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.](http://arxiv.org/abs/2309.17234) | 本文提出使用可评分的谈判游戏作为LLMs的新评估框架，创建了一个多样的测试平台，并通过系统化的零-shot思维链提示（CoT）展示了代理人可以成功谈判。该研究揭示了GPT-4在该任务上的性能差距。 |
| [^26] | [Spurious Feature Diversification Improves Out-of-distribution Generalization.](http://arxiv.org/abs/2309.17230) | 本文研究了基于权重空间集成方法WiSE-FT在分布外泛化中的有效性，发现其成功纠正了许多个体模型的错误预测，并通过利用更多多样化的伪特征减少了分布外设置中的预测错误。 |
| [^27] | [MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy.](http://arxiv.org/abs/2309.17227) | MORPH是一种使用强化学习进行硬件设计参数和控制策略的协同优化的方法。通过引入可微分的硬件模型代理，MORPH能够实现有效的优化并保持优化后的硬件代理接近其真实对应物。 |
| [^28] | [Training and inference of large language models using 8-bit floating point.](http://arxiv.org/abs/2309.17224) | 本文介绍了一种用于选择FP8线性层的缩放方法，该方法基于动态更新每个张量的权重、梯度和激活的尺度。通过将这种方法应用于大型语言模型的训练和验证，我们证明了其在提高计算效率方面的有效性。 |
| [^29] | [RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization.](http://arxiv.org/abs/2309.17215) | RSAM是一种在流形上学习的算法，通过将Sharpness-Aware Minimization (SAM)推广到Riemannian流形，引入了流形上sharpness的概念，并通过理论分析证明了其与泛化能力的关系。通过该算法的应用，我们展示了其在提升泛化能力方面的有效性。 |
| [^30] | [Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing.](http://arxiv.org/abs/2309.17211) | 该论文提出了一个名为HASTE的模块，通过使用局部敏感哈希技术，无需任何训练或精调即可实时降低卷积神经网络的计算成本，并且在压缩特征图时几乎不损失准确性。 |
| [^31] | [Robots That Can See: Leveraging Human Pose for Trajectory Prediction.](http://arxiv.org/abs/2309.17209) | 该论文提出了一种利用Transformer架构预测人类未来轨迹的方法，可以应用于以人为中心的动态环境。模型在常见的预测基准和移动机器人捕获的数据集上表现出色，并解决了历史数据有限的新代理导致的误差问题。 |
| [^32] | [Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes.](http://arxiv.org/abs/2309.17207) | 本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。 |
| [^33] | [ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery.](http://arxiv.org/abs/2309.17203) | ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。 |
| [^34] | [An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features.](http://arxiv.org/abs/2309.17197) | 本文通过研究使用放射组学特征训练的随机森林模型中的种族偏见，发现从DCE-MRI数据中提取的放射组学特征包含种族可辨识的信息。基于这些数据训练的模型能以60-70%的准确率预测白人和黑人种族，且基于种族不平衡数据的训练会导致模型产生偏见行为。 |
| [^35] | [ResBit: Residual Bit Vector for Categorical Values.](http://arxiv.org/abs/2309.17196) | 本论文提出了一种名为ResBit的残差位向量方法，用于解决在深度学习中表示离散数据维度增加和无法恢复原始类别值的问题。 |
| [^36] | [Generalized Activation via Multivariate Projection.](http://arxiv.org/abs/2309.17194) | 通过将ReLU视为从R投影到非负半线R+的操作，我们将其通过用凸锥的广义投影算子替代，扩展为具有多个输入和多个输出的多变量投影单元 (MPU)激活函数，并证明其在表达能力方面优于ReLU激活的FNN。 |
| [^37] | [A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration.](http://arxiv.org/abs/2309.17192) | 该调查研究了增量迁移学习方法，通过结合点对点联邦学习和领域增量学习，克服了数据隐私限制，并使用连续学习技术保持模型性能。调查还探讨了在多中心协作中，不同正则化方法对增量迁移学习的有效性的影响。 |
| [^38] | [RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations.](http://arxiv.org/abs/2309.17182) | RECOMBINER是一种鲁棒性和增强的贝叶斯隐式神经表示的压缩方法，通过丰富变分逼近、增加位置编码和分割高分辨率数据来解决COMBINER存在的局限性。 |
| [^39] | [Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training.](http://arxiv.org/abs/2309.17179) | Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性 |
| [^40] | [FedZeN: Towards superlinear zeroth-order federated learning via incremental Hessian estimation.](http://arxiv.org/abs/2309.17174) | 本文提出了FedZeN，一种使用增量Hessian估计的超线性零阶联邦学习算法。这种算法可以在不共享原始数据样本的情况下，通过采用Stiefel流形中的随机搜索方向，估计全局目标的曲率，并实现超线性收敛。 |
| [^41] | [Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain.](http://arxiv.org/abs/2309.17171) | 我们比较分析了领域特定的命名实体识别在龙与地下城领域的表现，发现在没有进行任何修改的情况下，Flair、Trankit和Spacy在龙与地下城背景中的命名实体识别方面表现最好。 |
| [^42] | [DyVal: Graph-informed Dynamic Evaluation of Large Language Models.](http://arxiv.org/abs/2309.17167) | DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。 |
| [^43] | [Current Methods for Drug Property Prediction in the Real World.](http://arxiv.org/abs/2309.17161) | 该论文介绍了当前药物属性预测方法的实际应用情况，并强调了不同数据集和方法之间的关联。研究发现最好的预测方法取决于数据集，使用经过工程处理的特征与经典方法结合的效果较好。 |
| [^44] | [Age Group Discrimination via Free Handwriting Indicators.](http://arxiv.org/abs/2309.17156) | 这项研究通过分析手写数据和计算相关指标，提出了一种使用仪器化墨水笔对老龄人群进行年龄分类的创新方法。 |
| [^45] | [Efficient Interpretable Nonlinear Modeling for Multiple Time Series.](http://arxiv.org/abs/2309.17154) | 本文提出了一种高效的非线性建模方法，用于多时间序列，其将线性VAR过程和可逆神经网络相结合，以实现对多变量之间的非线性依赖关系进行建模。 |
| [^46] | [Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability.](http://arxiv.org/abs/2309.17144) | 提出了原型生成，一种针对图像分类模型的数据无关解释性的更严格和稳健的特征可视化方法。通过生成自然激活路径的输入来反驳之前不可信的特征可视化算法的观点，并通过定量测量生成的原型的内部激活与自然图像之间的相似性来证实这一点。解释生成的原型可以揭示模型学习到的虚假相关性和偏见，这是定量方法无法识别的。 |
| [^47] | [GRANDE: Gradient-Based Decision Tree Ensembles.](http://arxiv.org/abs/2309.17130) | 这篇论文提出了一种名为GRANDE的基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成，并结合了轴对齐分割和梯度优化的灵活性，引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。 |
| [^48] | [Style Transfer for Non-differentiable Audio Effects.](http://arxiv.org/abs/2309.17125) | 本研究提出了一种深度学习方法，用于非可微分音频效果的风格迁移。该方法可以用于最广泛使用的框架中实现的效果，并且仅要求所考虑的参数具有连续的领域。 |
| [^49] | [Reconstruction of Patient-Specific Confounders in AI-based Radiologic Image Interpretation using Generative Pretraining.](http://arxiv.org/abs/2309.17123) | 该论文提出了一种自我条件扩散模型DiffChest，通过在大规模胸部X射线图像数据集上训练，可解释和可视化可能误导模型的混淆因素。通过评估DiffChest在识别与治疗相关的混淆因素方面的能力，发现其具有较高的一致性和准确性。 |
| [^50] | [Sheaf Hypergraph Networks.](http://arxiv.org/abs/2309.17116) | 这项研究通过引入细胞空间来增强超图的表示能力，开发了线性和非线性的细胞超图拉普拉斯算子，为有效建模复杂数据结构提供了强大的工具。 |
| [^51] | [Meta-Path Learning for Multi-relational Graph Neural Networks.](http://arxiv.org/abs/2309.17113) | 这项工作提出了一种新方法来学习具有高准确性的元路径和元路径图神经网络，关键是使用评分函数来衡量关系的潜在信息量。在实验中，该方法在合成和真实世界实验中表现出比现有的多关系GNNs更好的性能。 |
| [^52] | [Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation.](http://arxiv.org/abs/2309.17097) | 本文通过比较联邦学习和基于共识的方法解决MRI前列腺分割的问题，在协作学习的情景中进行成本效益基准测试，首次使用基于共识的方法解决协作学习问题，具有显著的改进。 |
| [^53] | [Dynamic Interpretability for Model Comparison via Decision Rules.](http://arxiv.org/abs/2309.17095) | 本文提出了DeltaXplainer，一种模型无关的方法，用于描述两个二元分类器之间的差异。通过实验验证了DeltaXplainer在涉及不同类型概念漂移的各种模型比较场景中的有效性。 |
| [^54] | [Too Big, so Fail? -- Enabling Neural Construction Methods to Solve Large-Scale Routing Problems.](http://arxiv.org/abs/2309.17089) | 提出了一种解决大规模路由问题的方法，通过系统的规模扩展研究发现最先进的神经构建方法无法泛化，提出了使用破坏重建原则的方法来改进神经构建方法的表现。 |
| [^55] | [From Empirical Measurements to Augmented Data Rates: A Machine Learning Approach for MCS Adaptation in Sidelink Communication.](http://arxiv.org/abs/2309.17086) | 本文提出了一种机器学习方法，通过预测合适的调制和编码方案(MCS)级别，实现了侧链通信中的MCS自适应，并展示了较传统方法显著改进的结果。同时，该论文还提供了一个通过实际行驶测试获得的数据集，并对外开放。 |
| [^56] | [Diffusion Models as Stochastic Quantization in Lattice Field Theory.](http://arxiv.org/abs/2309.17082) | 该研究通过将生成性扩散模型与随机量子化相连接，证明了生成性扩散模型在随机量子化中的应用。数值模拟表明，生成性扩散模型可以作为全局采样器用于生成量子格点场配置，并可以显著减少马尔可夫链的自相关时间。 |
| [^57] | [Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications.](http://arxiv.org/abs/2309.17076) | 本研究展示了在生物医学应用中，3D网格分割任务中，镜像重量对称性在神经网络中的影响。我们发现，重量对称性可以提高1至3％的额外准确度，并且可以减少参数数量高达8倍，而不影响性能。 |
| [^58] | [On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters.](http://arxiv.org/abs/2309.17053) | 这项研究探讨了图神经网络（GNN）表达能力和$k$维Weisfeiler-Leman ($k$WL)测试之间的关系，研究发现了$k$WL测试可以有效区分具有不同出现次数的模式图$P$的图形，并研究了模式图计数问题的最小维度$k$。 |
| [^59] | [On Continuity of Robust and Accurate Classifiers.](http://arxiv.org/abs/2309.17048) | 本文研究了稳健和准确分类器的连续性，提出了当假设连续时，其稳健性和准确性是不兼容的观点。 |
| [^60] | [Unveiling Document Structures with YOLOv5 Layout Detection.](http://arxiv.org/abs/2309.17033) | 本研究使用YOLOv5模型快速识别文档布局和提取非结构化数据，提高了数据提取的效率。 |
| [^61] | [Efficient Agnostic Learning with Average Smoothness.](http://arxiv.org/abs/2309.17016) | 该论文研究了基于平均光滑度的无参回归问题，提出了无分布限制下的统一收敛界限和高效无偏学习算法。 |
| [^62] | [Benchmarking Cognitive Biases in Large Language Models as Evaluators.](http://arxiv.org/abs/2309.17012) | 本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。 |
| [^63] | [Feature Cognition Enhancement via Interaction-Aware Automated Transformation.](http://arxiv.org/abs/2309.17011) | 该论文提出了一种交互感知的增强生成方法，以改进自动特征工程的表示空间，解决重构特征空间不可理解和缺乏系统探索的问题。 |
| [^64] | [Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain.](http://arxiv.org/abs/2309.17009) | 本文提出了一种基于TPP的方法，通过结合上下文事件嵌入、时间信息和领域特征来解决连续时间领域中事件集的预测问题。 |
| [^65] | [Medical Foundation Models are Susceptible to Targeted Misinformation Attacks.](http://arxiv.org/abs/2309.17007) | 本研究揭示了医学领域中大型语言模型(LLM)的脆弱性，通过操纵模型权重的很小比例，可以故意注入错误的生物医学事实，并且这些错误信息会被模型输出传播。面对这种易受攻击性，我们需要采取措施确保这些模型在医疗实践中的可靠和安全使用。 |
| [^66] | [Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks.](http://arxiv.org/abs/2309.17002) | 本文研究了深度学习中预训练数据中的标签噪声对下游任务的影响，并通过在合成噪声数据集上的实验证明，在预训练中的轻微噪声可以提高领域内的性能，但会损害领域外的性能。为了减轻噪声的影响，提出了一种轻量级的黑盒调整方法（NMTune）。 |
| [^67] | [A Closer Look at Bearing Fault Classification Approaches.](http://arxiv.org/abs/2309.17001) | 这项研究对轴承故障分类方法进行了深入研究，关注了轴承故障诊断的重要性，以及使用机器学习和振动数据来预测故障的方法。 |
| [^68] | [Segment Anything Model is a Good Teacher for Local Feature Learning.](http://arxiv.org/abs/2309.16992) | 本文提出了使用Segment Anything Model (SAM)作为教师来指导本地特征学习，通过像素语义关系蒸馏和弱监督对比学习两种技术，实现了在有限数据集上的更高性能表现。 |
| [^69] | [Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning.](http://arxiv.org/abs/2309.16984) | 这篇论文介绍了一种基于一致性模型的策略表示方法，在强化学习中具有高效且表达力强的特点。实验表明，一致性策略在各种RL设置中都具有良好的性能表现。 |
| [^70] | [Reliability Quantification of Deep Reinforcement Learning-based Control.](http://arxiv.org/abs/2309.16977) | 本论文提出了一种用于量化深度强化学习控制系统可靠性的方法，通过使用两个神经网络来评估控制的可靠性差异。 |
| [^71] | [Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors.](http://arxiv.org/abs/2309.16976) | 本文基于Habana Gaudi处理器，对使用Transformer模型进行加速的潜力进行了研究，并提供了关键挑战，全面性能比较以及优化策略。 |
| [^72] | [Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness.](http://arxiv.org/abs/2309.16973) | 该论文提出了一种名为RO2O的算法，通过不确定性和平滑性增强离线训练的强化学习代理，在离线到在线学习中缓解性能下降问题。 |
| [^73] | [A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning.](http://arxiv.org/abs/2309.16972) | 本文提出了一种基于差分驱动增强学习的量子态制备方法，改进了奖励函数和行为选择策略，以提高两比特量子系统的制备速度和保真度。 |
| [^74] | [Multi-Resolution Active Learning of Fourier Neural Operators.](http://arxiv.org/abs/2309.16971) | 提出了一种多分辨率主动学习的傅里叶神经算子（MRA-FNO），通过动态选择输入函数和分辨率来降低数据成本并优化学习效率。 |
| [^75] | [Discrete-Choice Model with Generalized Additive Utility Network.](http://arxiv.org/abs/2309.16970) | 本论文提出了一种基于广义可加模型的神经网络架构，称为广义可加效用网络（GAUNet），用于离散选择模型。这些模型在预测准确性上可以与ASU-DNN相媲美，并且相比以前的模型具有更好的解释性。 |
| [^76] | [Controlling Continuous Relaxation for Combinatorial Optimization.](http://arxiv.org/abs/2309.16965) | 本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。 |
| [^77] | [Leveraging Optimization for Adaptive Attacks on Image Watermarks.](http://arxiv.org/abs/2309.16952) | 该论文提出了一种利用优化技术进行对图像水印的自适应攻击的方法，通过自适应地生成替代密钥来复制秘密水印密钥，并通过优化问题的解决方法进行攻击优化。 |
| [^78] | [Water quality prediction using machine learning and neural network approaches.](http://arxiv.org/abs/2309.16951) | 本研究通过比较线性回归、随机森林、XGBoost、LightGBM和MLP神经网络五种模型在佐治亚州预测水质pH值方面的效果，发现LightGBM表现最好。基于树的模型在回归问题中优势显著，而MLP神经网络对特征缩放敏感。同时，本研究还探讨了与原研究相比，机器学习模型能够取得更好性能的原因。 |
| [^79] | [Physics-Informed Induction Machine Modelling.](http://arxiv.org/abs/2309.16943) | 本研究提出了一种物理可感知感测机（NeuIM）模型，通过结合物理知识和机器学习技术实现了基于人工智能的电磁瞬态模拟，实现了快速和缓慢感应机动力学的捕捉，并且能够适应不同数据可用性的情况。 |
| [^80] | [G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks.](http://arxiv.org/abs/2309.16941) | G4SATBench是第一个为基于图神经网络的SAT求解器提供全面评估框架的基准研究。它提供了一个大而多样的SAT数据集，并基于各种预测任务、训练目标和推理算法对广泛的GNN模型进行了测试和比较。通过与搜索型SAT求解器中的启发式方法进行比较，它还揭示了基于GNN的SAT求解器的学习能力以及其优缺点。 |
| [^81] | [PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label.](http://arxiv.org/abs/2309.16936) | PC-Adapter是一种拓扑感知适配器，通过保留全局形状信息和学习目标领域的局部特征，实现了点云领域自适应。还提出了一种分层扩展的伪标签矫正策略，以解决伪标签的误类问题。 |
| [^82] | [TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework.](http://arxiv.org/abs/2309.16935) | TranDRL是一种基于Transformer驱动的深度强化学习支持的预防性维护框架，结合了复杂时间模式捕捉和经济高效维护建议，显著提高了剩余寿命（RUL）预测准确性和维护行动优化。 |
| [^83] | [Symmetry Leads to Structured Constraint of Learning.](http://arxiv.org/abs/2309.16932) | 本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。 |
| [^84] | [Learning to Receive Help: Intervention-Aware Concept Embedding Models.](http://arxiv.org/abs/2309.16928) | 这项研究提出了一种干预感知的概念嵌入模型，用于提高神经架构对概念干预的响应性，并解决了概念干预顺序和模型架构的依赖性的问题。 |
| [^85] | [Mode Connectivity and Data Heterogeneity of Federated Learning.](http://arxiv.org/abs/2309.16923) | 本研究通过模式连通性的经验和理论研究发现，减少数据异质性可以增加客户端和全局模式之间的连通性，建立了全局模式连通性的定量界限。 |
| [^86] | [ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks.](http://arxiv.org/abs/2309.16918) | 本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。 |
| [^87] | [ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values.](http://arxiv.org/abs/2309.16916) | ONNXExplainer是一个基于ONNX的通用框架，使用Shapley值解释神经网络。它提供了自动微分和优化方法，实现了一次性部署和高效的解释计算。 |
| [^88] | [Algorithmic Recourse for Anomaly Detection in Multivariate Time Series.](http://arxiv.org/abs/2309.16896) | 本论文提出了一种算法补救框架，RecAD，用于多变量时间序列异常检测。通过推荐以最小成本修复异常时间序列，该框架能够帮助领域专家理解如何修复异常行为，并在实验中证明了其有效性。 |
| [^89] | [Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer.](http://arxiv.org/abs/2309.16888) | 这个论文介绍了一种新颖的方法，利用Transformer-based Multivariate Time Series Classifier (TMTSC)来预测风险投资和成长资本的候选公司的成功可能性，以优化投资目标的选择。通过对相关的方法进行了全面回顾和实验验证，证明了该方法的有效性。 |
| [^90] | [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing.](http://arxiv.org/abs/2309.16883) | 本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。 |
| [^91] | [Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling.](http://arxiv.org/abs/2309.16882) | 本文提出了一种名为消息传播通过时间（MPTT）的算法，用于在时间序列建模中保留序列依赖性。MPTT通过异步管理初始隐藏状态，并采用三种策略过滤过期信息，以生成信息丰富的隐藏状态，从而提高RNN的训练效果。 |
| [^92] | [Investigating Human-Identifiable Features Hidden in Adversarial Perturbations.](http://arxiv.org/abs/2309.16878) | 该研究揭示了对抗扰动中隐藏的人类可识别特征，并发现了掩蔽效应和生成效应在不同类型攻击中的表现。通过分析多个攻击算法生成的扰动，发现它们在一定程度上存在相似性。 |
| [^93] | [LEF: Late-to-Early Temporal Fusion for LiDAR 3D Object Detection.](http://arxiv.org/abs/2309.16870) | LEF是一种用于LiDAR 3D物体检测的迟到早期时间融合方案，通过将目标感知的潜在嵌入融合到早期阶段，能够更好地捕捉具有挑战性的对象的形状和姿态。 |
| [^94] | [Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis.](http://arxiv.org/abs/2309.16859) | 本文提出了一种基于数据驱动的体积化人脸先验，使得能合成先前训练分布外的超高分辨率的新视图。通过训练身份数量有限，结合基于稀疏标记点的3D对齐，该模型能够学习到一个平滑的几何和外观的潜在空间，并通过拟合到2-3个摄像机视图来获取高质量的新对象的体积化表示。 |
| [^95] | [Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach.](http://arxiv.org/abs/2309.16858) | 我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。 |
| [^96] | [Applications of Federated Learning in IoT for Hyper Personalisation.](http://arxiv.org/abs/2309.16854) | 在物联网中应用联合学习，解决了大规模数据利用问题，并实现了超个性化的个性化体验。 |
| [^97] | [Space-Time Attention with Shifted Non-Local Search.](http://arxiv.org/abs/2309.16849) | 本文提出了一种名为偏移非局部搜索的方法，通过结合非局部搜索的质量和预测的偏移范围进行搜索，以纠正小的空间误差。与先前的工作相比，我们的方法在内存消耗上减少了10倍以上，并且速度提高了3倍以上。 |
| [^98] | [Optimal Nonlinearities Improve Generalization Performance of Random Features.](http://arxiv.org/abs/2309.16846) | 通过研究等效模型的参数，本研究发现获得的参数可以定义一组最优非线性性，如二阶多项式和分段线性函数。这些非线性性能优化了泛化性能，无论其实际形式如何，对回归和分类问题均有效。 |
| [^99] | [Constant Approximation for Individual Preference Stable Clustering.](http://arxiv.org/abs/2309.16840) | 本文提出了个体偏好稳定聚类的常数逼近算法，使得每个数据点到其所属聚类的平均距离不超过到任何其他聚类的平均距离的$\alpha$倍，并且给出了泛化到超过平均距离的个体偏好稳定的算法。 |
| [^100] | [Propagation and Attribution of Uncertainty in Medical Imaging Pipelines.](http://arxiv.org/abs/2309.16831) | 本文提出了一种在医学图像处理流程中传播不确定性的方法，能够聚合流程后期的不确定性，并获得后续模型预测的联合不确定性度量。同时，还能够报告流程中每个组件的内部、数据基础不确定性的贡献。 |
| [^101] | [An analysis of the derivative-free loss method for solving PDEs.](http://arxiv.org/abs/2309.16829) | 本研究分析了一种无导数损失方法在使用神经网络求解椭圆型偏微分方程的方法。我们发现训练损失偏差与时间间隔和空间梯度成正比，与行走者大小成反比，同时时间间隔必须足够长。我们提供了数值测试结果以支持我们的分析。 |
| [^102] | [Post-Training Overfitting Mitigation in DNN Classifiers.](http://arxiv.org/abs/2309.16827) | 该论文提出了一种后训练缓解方法，用于解决深度神经网络分类器中的过拟合问题。该方法通过限制神经激活边界来缓解后门数据污染导致的过拟合，并获得了性能改进。研究还提供了分析支持，表明基于最大边界的后训练正则化显著缓解了非恶意过拟合问题。 |
| [^103] | [FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets.](http://arxiv.org/abs/2309.16825) | 该论文提出了一种针对异构临床数据的个性化联邦学习方法，实验证明该方法在性能上超越了现有的全局和个性化联邦学习技术，并且对FLamby基准进行了实质性改进和扩展。 |
| [^104] | [Multi-Bellman operator for convergence of $Q$-learning with linear function approximation.](http://arxiv.org/abs/2309.16819) | 本文通过引入新的多Bellman算子，对线性函数逼近下的$Q$-learning进行了收敛性研究，并提出了多$Q$-learning算法。通过探索多Bellman算子的性质，我们找到了使其成为压缩映射的条件，获得了比传统Bellman算子更好的固定点保证。该算法收敛到多Bellman算子的不动点，可以得到任意精度的解。在验证过程中，我们将该方法应用于常用环境中，展示了其有效性和适用性。 |
| [^105] | [PROSE: Predicting Operators and Symbolic Expressions using Multimodal Transformers.](http://arxiv.org/abs/2309.16816) | PROSE是一种能够从多模态输入到多模态输出的网络表示方法，可以用于预测非线性微分方程的数值解和符号表达式。 |
| [^106] | [GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch.](http://arxiv.org/abs/2309.16809) | 该论文介绍了一个名为GraB-sampler的Python库，为社区提供了使用GraB算法的便利，并提出了5个不同的变体。最佳性能结果证明了GraB-sampler在训练时间和显存使用上的开销非常小。 |
| [^107] | [Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning.](http://arxiv.org/abs/2309.16808) | 本研究利用高分辨率影像和混合学习方法，通过图像数据中的特征提取和模式检测，估计了个别社区的人口密度、家庭收入中位数和教育程度。 |
| [^108] | [Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution.](http://arxiv.org/abs/2309.16797) | Promptbreeder是一种通过提示演化进行自我改进的通用机制，它可以优化大型语言模型的推理能力，并在算术和常识推理上优于现有的提示策略。 |
| [^109] | [Hallucination Reduction in Long Input Text Summarization.](http://arxiv.org/abs/2309.16781) | 本文旨在减少长篇文本摘要中的幻觉输出，通过在Longformer Encoder-Decoder模型的微调中采用数据过滤和联合实体和摘要生成技术，我们成功提高了生成摘要的质量。 |
| [^110] | [Intriguing properties of generative classifiers.](http://arxiv.org/abs/2309.16779) | 生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。 |
| [^111] | [Neural scaling laws for phenotypic drug discovery.](http://arxiv.org/abs/2309.16773) | 本研究调查了规模对于辅助小分子药物发现模型的影响，并发现DNN在表型药物发现任务中并没有持续改进，通过引入逆生物过程预训练可以显著提升模型性能。 |
| [^112] | [Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring.](http://arxiv.org/abs/2309.16770) | 本论文提出了一种新颖的Persona编码多流程对话句子评分方法，利用个人角色信息来提高对话生成的质量。 |
| [^113] | [Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories.](http://arxiv.org/abs/2309.16750) | 本调查综述了扩散模型（DMs）和关联记忆（AMs）之间的数学联系，揭示了DMs是如何利用能量函数进行去噪数据的，并讨论了未来研究方向。 |
| [^114] | [Discovering environments with XRM.](http://arxiv.org/abs/2309.16748) | 本文提出了一种用于发现环境的算法 XRM，它通过训练两个孪生网络，每个网络从训练数据的一半中学习，并模仿其兄弟网络的错误分类，解决了现有方法需要依赖人工注释环境信息的问题。 |
| [^115] | [Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework.](http://arxiv.org/abs/2309.16747) | 本研究提出了一种新颖的多模式灾害预测框架，结合了天气统计数据、卫星图像和文本信息。通过整合多个数据源，对于不同类型的灾害预测，模型的性能得到了增强。 |
| [^116] | [Implicit Gaussian process representation of vector fields over arbitrary latent manifolds.](http://arxiv.org/abs/2309.16746) | 这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。 |
| [^117] | [Efficient Training of One Class Classification-SVMs.](http://arxiv.org/abs/2309.16745) | 本研究提出了一种高效训练方法来进行一类分类支持向量机。通过利用增广Lagrange乘子法（AL-FPGM），该方法可以在只有正例的情况下训练有效的分类器，并且计算成本低廉，可以用于训练大规模支持向量机。 |
| [^118] | [Predicting Long-term Renal Impairment in Post-COVID-19 Patients with Machine Learning Algorithms.](http://arxiv.org/abs/2309.16744) | 该研究使用机器学习算法预测新冠后患者长期肾损伤的风险，通过早期识别和干预改善临床结果。 |
| [^119] | [High Throughput Training of Deep Surrogates from Large Ensemble Runs.](http://arxiv.org/abs/2309.16743) | 该论文提出了一个用于加速数值解算器的深度代理的高通量训练方法，通过从大量集合运行的模拟中在线训练模型，利用多级并行性生成丰富的数据集，直接流式传输数据以避免I/O瓶颈和存储问题，同时使用训练储备池来减轻流式传输的固有偏差。实验结果显示，使用该方法能够在2小时内在8TB的数据上训练出准确率提升了47%、批量吞吐量提高了13倍的热方程代理网络。 |
| [^120] | [Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients.](http://arxiv.org/abs/2309.16742) | 该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。 |
| [^121] | [Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections.](http://arxiv.org/abs/2309.16741) | 本文提出了一种通过深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，以捕捉数据的重要特征。 |
| [^122] | [Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities.](http://arxiv.org/abs/2309.16739) | 本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。 |
| [^123] | [Cognizance of Post-COVID-19 Multi-Organ Dysfunction through Machine Learning Analysis.](http://arxiv.org/abs/2309.16736) | 该研究应用机器学习技术分析和预测COVID-19后综合症中的多器官功能障碍，旨在通过提高对这种疾病的认识，帮助医疗保健提供者识别风险个体并及时干预，从而改善患者的预后结果。 |
| [^124] | [Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques.](http://arxiv.org/abs/2309.16733) | 这篇论文系统调查了深度学习应用对底层硬件故障的韧性，并提供了未来研究的方向。 |
| [^125] | [Explainable machine learning-based prediction model for diabetic nephropathy.](http://arxiv.org/abs/2309.16730) | 本研究提出了一个可解释的基于机器学习的糖尿病肾病预测模型，通过分析血清代谢物对疾病的影响并选择最优特征来预测疾病的患病率。最优模型采用了极限梯度提升（XGB）算法，其在筛选DN方面表现最佳，同时具有更好的临床效益和拟合度。 |
| [^126] | [SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems.](http://arxiv.org/abs/2309.16729) | 本文提出了一种基于仿真驱动的物理信息神经网络(SimPINNs)的方法，用于增强非线性反问题的性能。这种方法通过深度学习和融合观测数据和仿真数据的混合损失函数来推断控制物理系统的未知参数。实验结果表明，在轨道复位问题上，SimPINNs的准确性和鲁棒性优于标准PINNs的表现。 |
| [^127] | [Physics-Informed Solution of The Stationary Fokker-Plank Equation for a Class of Nonlinear Dynamical Systems: An Evaluation Study.](http://arxiv.org/abs/2309.16725) | 该论文提出了一个物理知情的神经网络（PINN）框架，用于解决非线性随机动力系统的福克-普朗克方程，并评估了其潜力。 |
| [^128] | [A Real-World Quadrupedal Locomotion Benchmark for Offline Reinforcement Learning.](http://arxiv.org/abs/2309.16718) | 该论文提出了一个现实世界的四足步态基准测试，用于评估线下强化学习算法在挑战性任务中的性能。实验结果表明，最好的算法能够与无模型强化学习相媲美甚至超越其性能。 |
| [^129] | [UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning.](http://arxiv.org/abs/2309.16713) | 本文提出了一个利用无人机的上行语义通信方案，通过混合动作强化学习框架实现了在数据收集效率和计算能量成本之间的平衡，并取得了显著的改善结果。 |
| [^130] | [Automatic Cadastral Boundary Detection of Very High Resolution Images Using Mask R-CNN.](http://arxiv.org/abs/2309.16708) | 这篇论文使用Mask R-CNN和几何后处理方法提出了自动检测高分辨率图像界址地籍边界的解决方案，并引入了一种新的基于口袋的简化算法来改善工作质量。 |
| [^131] | [AIR: Threats of Adversarial Attacks on Deep Learning-Based Information Recovery.](http://arxiv.org/abs/2309.16706) | 这项研究探讨了在对抗情况下，深度学习信息恢复模型的鲁棒性。通过对最先进的DeepReceiver模型进行对抗攻击，实验结果表明DeepReceiver对设计的攻击方法是脆弱的。 |
| [^132] | [Decoding Imagery: Unleashing Large Language Models.](http://arxiv.org/abs/2309.16705) | 该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。 |
| [^133] | [Framework and Model Analysis on Bengali Document Layout Analysis Dataset: BaDLAD.](http://arxiv.org/abs/2309.16700) | 本研究使用先进计算机程序（Detectron2、YOLOv8和SAM）对孟加拉文档布局进行分析，通过比较它们的准确性和速度，找出适用于不同类型文档的最佳方法。研究有助于理解孟加拉文档中复杂的布局，并在其他语言中具有实用价值。 |
| [^134] | [Extension of Transformational Machine Learning: Classification Problems.](http://arxiv.org/abs/2309.16693) | 本研究通过研究转化机器学习（TML）在药物发现中的应用和性能，发现TML在预测精度、可解释性和泛化能力方面都优于传统机器学习方法，特别是在训练数据集数量足够时。 |
| [^135] | [ecoBLE: A Low-Computation Energy Consumption Prediction Framework for Bluetooth Low Energy.](http://arxiv.org/abs/2309.16686) | ecoBLE是一种基于LSTMP的蓝牙低功耗能量消耗预测框架，考虑了物联网节点的所有组件，并提供了一个用于医疗应用的数据集。 |
| [^136] | [Target-aware Variational Auto-encoders for Ligand Generation with Multimodal Protein Representation Learning.](http://arxiv.org/abs/2309.16685) | 本研究提出了一种面向目标的变分自编码器（TargetVAE），通过蛋白质多模态网络（PMN）将蛋白质的不同表示统一到一个模型中，实现配体的生成。该方法能够从整个蛋白质结构中学习，并捕捉其顺序、拓扑和几何信息。 |
| [^137] | [Leveraging Side Information for Ligand Conformation Generation using Diffusion-Based Approaches.](http://arxiv.org/abs/2309.16684) | 本研究提出了一种利用侧面信息和基于扩散的方法生成配体构象的新方法，通过引入灵活约束和配体-目标传递信息块来解决既有模型生成构象缺乏结构和随机性的问题。 |
| [^138] | [Controlling the Solo12 Quadruped Robot with Deep Reinforcement Learning.](http://arxiv.org/abs/2309.16683) | 本论文介绍了在Solo12四足机器人上使用深度强化学习实现的强大的端到端学习控制器。这种控制器能够在遵循给定速度参考的同时，具有高效的能量消耗、稳健性和易于部署的特点。 |
| [^139] | [Leveraging Deep Learning and Online Source Sentiment for Financial Portfolio Management.](http://arxiv.org/abs/2309.16679) | 本文研究利用深度学习方法进行金融交易，并考虑情绪信息的作用。同时讨论常见的训练问题，并提供应用方法。 |
| [^140] | [Mixup Your Own Pairs.](http://arxiv.org/abs/2309.16633) | 本文提出了一种名为SupReMix的方法，通过混合样本，特别是混合负样本和混合正样本，来解决回归问题中表示学习的挑战。这种方法能够提供更好的性能和更准确的回归结果。 |
| [^141] | [AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models.](http://arxiv.org/abs/2309.16414) | 本研究提出了一种名为AutoCLIP的方法，用于自动调谐视觉语言模型的零样本分类器。AutoCLIP通过为每个提示模板分配图像特定的权重，从而改进了从编码类别描述符推导零样本分类器的方式。 |
| [^142] | [Temporal graph models fail to capture global temporal dynamics.](http://arxiv.org/abs/2309.15730) | 时间图模型无法捕捉全局时间动态，我们提出了一种"最近流行节点"的基线方法，在时间图基准的中等和大规模数据集上胜过其他方法。我们提出了两个基于Wasserstein距离的度量来量化全局动态。我们展示了标准的负采样评估方法在具有强烈时间动态的数据集上可能不适用，我们还展示了简单的负采样方法可能导致模型退化。我们提出了改进的负采样方案，并证明了它们的有效性。我们还将其与无负采样的非对比训练模型进行了比较。 |
| [^143] | [DTC: Deep Tracking Control -- A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion.](http://arxiv.org/abs/2309.15462) | 本文提出了一种混合控制架构，同时结合了基于模型的规划和基于强化学习的方法，用于解决腿式运动的复杂控制问题。这种方法具有精确性、鲁棒性以及对稀疏奖励环境的适应能力。 |
| [^144] | [Maximum Diffusion Reinforcement Learning.](http://arxiv.org/abs/2309.15293) | 最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。 |
| [^145] | [Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling.](http://arxiv.org/abs/2309.15214) | 一种用于千米尺度大气降尺度的生成残差扩散建模方法被提出，并展示了在天气和气候的物理灾害预测方面具有潜力。 |
| [^146] | [BiSinger: Bilingual Singing Voice Synthesis.](http://arxiv.org/abs/2309.14089) | BiSinger是一个双语合成歌声系统，通过设计共享表示、融合数据集和使用开源技术，实现了一种可以进行英语和汉语普通话混合编码歌声合成的单一模型，并保持了汉语歌曲的表现。 |
| [^147] | [Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models.](http://arxiv.org/abs/2309.14068) | 本文发现了扩散模型在反向降噪中存在一个表达瓶颈，并且推出了一种新的软混合降噪（SMD）模型，该模型在理论上能够很好地逼近任意高斯混合分布，并且在实现上简单高效。 |
| [^148] | [Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning.](http://arxiv.org/abs/2309.13860) | Fast-HuBERT是一种高效的自监督语音表示学习训练框架，通过分析HuBERT预训练过程中的计算成本，并引入一系列效率优化策略，实现了在Librispeech 960h基准上的5.2倍加速，并且在前人工作中得到了一致的改进。 |
| [^149] | [Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition.](http://arxiv.org/abs/2309.13405) | 本论文通过桥块分解方法，提出了一种学习大规模MTP$_2$高斯图模型的策略，能够将大问题拆分为小问题进行优化，显著降低了计算复杂度并提高了算法速度。 |
| [^150] | [Test-Time Training for Speech.](http://arxiv.org/abs/2309.10930) | 本文研究了测试时间训练在语音应用中处理分布偏移的应用，提出了一种解决方案BitFit，该方案通过只考虑偏置参数进行微调，解决了测试时间训练中的关键挑战。 |
| [^151] | [A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability.](http://arxiv.org/abs/2309.07156) | 本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。 |
| [^152] | [Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.06553) | 这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。 |
| [^153] | [PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis.](http://arxiv.org/abs/2309.05833) | 本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。 |
| [^154] | [SLiMe: Segment Like Me.](http://arxiv.org/abs/2309.03179) | 基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。 |
| [^155] | [FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning.](http://arxiv.org/abs/2309.00127) | FTA提出了一种新的灵活触发器、隐蔽且强健的联邦学习后门攻击方法，通过生成学习灵活触发器模式来操作良性样本，同时保留攻击者选择的标签的最重要隐藏特征。通过填充可区分的差异，使攻击具有隐蔽性。验证了该方法的有效性和可靠性。 |
| [^156] | [Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models.](http://arxiv.org/abs/2308.16149) | Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。 |
| [^157] | [Measurement Tampering Detection Benchmark.](http://arxiv.org/abs/2308.15605) | 本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。 |
| [^158] | [A Scale-Invariant Task Balancing Approach for Multi-Task Learning.](http://arxiv.org/abs/2308.12029) | 这篇论文提出了一种尺度不变的多任务学习方法（SI-MTL），通过对任务损失进行对数变换和对任务梯度进行归一化，解决了多任务学习中的任务平衡问题，并在多个基准数据集上取得了领先的性能。 |
| [^159] | [Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder.](http://arxiv.org/abs/2308.11819) | 通过使用去混淆器模型FLMD，在电子健康记录建模中实现了公平性和准确性，解决了有偏见的EHR中的健康差异问题。 |
| [^160] | [Backward Reasoning in Large Language Models for Verification.](http://arxiv.org/abs/2308.07758) | 本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。 |
| [^161] | [Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks.](http://arxiv.org/abs/2308.06203) | 这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。 |
| [^162] | [Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization.](http://arxiv.org/abs/2308.05021) | 扩散模型可能受到错误传播的影响，本文通过理论分析和数据实证验证了这个问题，并提出了一种一致性正则化方法来解决。我们的分析揭示了模型的去噪模块是否具有容错性以及如何减少分布差异。 |
| [^163] | [From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data.](http://arxiv.org/abs/2308.04553) | 本文提出了一个两阶段训练流程，通过在一个平衡的合成数据集上进行预训练，然后在真实数据上进行微调，减少了视觉识别模型学习到与数据集偏差相关的错误的问题。 |
| [^164] | [An Overview Of Temporal Commonsense Reasoning and Acquisition.](http://arxiv.org/abs/2308.00002) | 本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。 |
| [^165] | [An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training.](http://arxiv.org/abs/2307.16189) | 这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。 |
| [^166] | [The Applicability of Federated Learning to Official Statistics.](http://arxiv.org/abs/2307.15503) | Federated Learning（FL）在官方统计中具有潜力，可以保护数据隐私并提升数据质量。 |
| [^167] | [XSkill: Cross Embodiment Skill Discovery.](http://arxiv.org/abs/2307.09955) | 本研究提出了XSkill，一种跨体现的技能发现框架，能够从无标签的人类和机器人操纵视频中纯粹地发现跨体现技能原型，并通过条件扩散策略将这些技能转移到机器人动作中，在未见任务中完成学习到的技能的组合。仿真和真实环境中的实验结果表明，这些发现的技能原型能够有效地促进技能转移和组合，从而构建出更通用和可扩展的模仿学习框架。 |
| [^168] | [Mitigating Label Bias via Decoupled Confident Learning.](http://arxiv.org/abs/2307.08945) | 这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。 |
| [^169] | [Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance.](http://arxiv.org/abs/2307.03811) | 该论文提出了一种深度学习模型，Formulation Graph Convolution Network（F-GCN），它可以将电池电解质的结构组成关系映射到整个液体配方的性能，从而加快新化合物的发现和应用。 |
| [^170] | [Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios.](http://arxiv.org/abs/2307.01429) | 本文提出了一种称为智能滤波器辅助领域对抗神经网络（SFDANN）的无监督领域自适应方法，用于嘈杂的工业场景中的故障诊断。该方法通过引入智能滤波器，在时频域中动态加强源域和目标域数据之间的相似性，以实现领域对齐。 |
| [^171] | [ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models.](http://arxiv.org/abs/2307.00398) | ProbVLM是一种概率适配器，用于估计大规模视觉-语言模型中嵌入的概率分布，以解决固有的嵌入歧义问题，并在多个数据集上展示了其在检索任务中的优越性能表现。 |
| [^172] | [Learning Dynamic Graphs from All Contextual Information for Accurate Point-of-Interest Visit Forecasting.](http://arxiv.org/abs/2306.15927) | 提出了一种名为忙碌图神经网络（BysGNN）的临时图神经网络，利用所有背景信息和时间序列数据来学习兴趣点之间的多背景相关性，以实现准确的访问量预测。 |
| [^173] | [Leveraging Task Structures for Improved Identifiability in Neural Network Representations.](http://arxiv.org/abs/2306.14861) | 本文提出一种基于任务结构的可识别性理论，拓展了先前仅限于单任务分类的工作。任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放。在假设任务之间存在因果关系时，该方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。 |
| [^174] | [Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok.](http://arxiv.org/abs/2306.13253) | 本文提出了一种低成本方法来预测神经网络中的理解前浪潮，即通过研究前几轮的学习曲线来判断后续是否出现理解前浪潮。使用波形振荡和学习曲线的频谱特征值可以高精度地预测理解前浪潮。 |
| [^175] | [The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement.](http://arxiv.org/abs/2306.09633) | 谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。 |
| [^176] | [Unprocessing Seven Years of Algorithmic Fairness.](http://arxiv.org/abs/2306.07261) | 该论文取消了算法公平性中的后处理方法，并发现后处理实现的公平性-准确性Pareto边界包含了可评估的所有其他方法。 |
| [^177] | [Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation.](http://arxiv.org/abs/2306.06192) | Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。 |
| [^178] | [SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States.](http://arxiv.org/abs/2306.04817) | 本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。 |
| [^179] | [Don't trust your eyes: on the (un)reliability of feature visualizations.](http://arxiv.org/abs/2306.04719) | 本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。 |
| [^180] | [Modeling Human-like Concept Learning with Bayesian Inference over Natural Language.](http://arxiv.org/abs/2306.02797) | 该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。 |
| [^181] | [Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation.](http://arxiv.org/abs/2306.02747) | 本论文介绍了一种通过追踪非平稳性的因果起源来解决强化学习中的挑战的方法，通过引入因果源表示（COREP）算法，学到的策略对非平稳性表现出韧性。 |
| [^182] | [Overcoming the Stability Gap in Continual Learning.](http://arxiv.org/abs/2306.01904) | 本论文研究了如何克服连续学习中的稳定性差距，并通过发现一种显著减少这种差距的方法，在大规模类别增量学习实验中大幅减少了网络更新的次数。 |
| [^183] | [Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning.](http://arxiv.org/abs/2306.01474) | 本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。 |
| [^184] | [A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration.](http://arxiv.org/abs/2306.00740) | 深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。 |
| [^185] | [Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces.](http://arxiv.org/abs/2305.19891) | 本研究针对无法处理的结构化大离散动作空间（SLDAS）提出了一种名为动态邻域构建（DNC）的新型利用策略，通过可扩展的邻域探索启发式方法，高效地探索连续代理动作周围的离散邻域。 |
| [^186] | [Chain of Log-Concave Markov Chains.](http://arxiv.org/abs/2305.19473) | 该论文提出了一种新的采样算法，基于对数凹条件概率密度，使用等向性高斯平滑来解决高维下抽样难题。 |
| [^187] | [Fast Dynamic 1D Simulation of Divertor Plasmas with Neural PDE Surrogates.](http://arxiv.org/abs/2305.18944) | 该论文提出了一种使用神经网络的代理模型来进行快速铁托等离子体模拟的方法，解决了实时应用或详尽的参数扫描中速度太慢的问题。 |
| [^188] | [Kernel-SSL: Kernel KL Divergence for Self-supervised Learning.](http://arxiv.org/abs/2305.17326) | 本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。 |
| [^189] | [Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks.](http://arxiv.org/abs/2305.16891) | 本论文通过全面的稳定性和泛化分析，在多层神经网络上证明了GD算法的一般性保证，为双层和三层NN推导出了过量风险率，扩展了以往研究。 |
| [^190] | [On the Generalization Capacities of Neural Controlled Differential Equations.](http://arxiv.org/abs/2305.16791) | 本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。 |
| [^191] | [Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank.](http://arxiv.org/abs/2305.16038) | 在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。 |
| [^192] | [Learning Directed Graphical Models with Optimal Transport.](http://arxiv.org/abs/2305.15927) | 通过最优传输的视角提供了参数学习问题的新视图，可以在许多有向图上进行操作并表现出灵活性和多功能性。 |
| [^193] | [Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning.](http://arxiv.org/abs/2305.15912) | 本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。 |
| [^194] | [Deep Equivariant Hyperspheres.](http://arxiv.org/abs/2305.15613) | 本文提出了深度等变超球体的理论模型，解决了几何深度学习中等变和几何变换下不变的重大问题。 |
| [^195] | [Feature-aligned N-BEATS with Sinkhorn divergence.](http://arxiv.org/abs/2305.15196) | 这是一个基于Sinkhorn距离的特征对其N-BEATS模型，它通过对齐堆栈中的边际特征概率测度来进行领域广义的时间序列预测，同时保留了N-BEATS的可解释性和预测能力。 |
| [^196] | [Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation.](http://arxiv.org/abs/2305.13849) | 本文提出了一种自监督高斯正则化的深度分类器，可用于马氏距离不确定性评估，相比现有方法，该方法不需要对模型架构和训练程序做出大的改变，并在标准OOD基准测试上取得了最先进的性能。 |
| [^197] | [Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach.](http://arxiv.org/abs/2305.13289) | 本文提出了一种分布鲁棒优化 (DRO)的方法，用于解决离线强化学习中的数据有限性和分布转移问题。通过直接建模转移核的不确定性，并寻找在不确定性集合中最优化最坏情况下的性能的策略，实现了极小极大最优性。 |
| [^198] | [Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations.](http://arxiv.org/abs/2305.12715) | 本文提出了不精确标签学习（ILL）框架，利用期望最大化算法对不精确标签信息进行最大似然估计，为各种不精确标签配置问题提供了统一的解决方案。 |
| [^199] | [Interpretable neural architecture search and transfer learning for understanding sequence dependent enzymatic reactions.](http://arxiv.org/abs/2305.11917) | Elektrum是一个深度学习框架，使用可解释的神经网络模型预测酶反应，利用有限但洁净的体外数据和噪声但丰富的体内数据。Elektrum可以通过迁移学习揭示酶活性的关键序列相关决定因素，并发现潜在的治疗干预靶点。 |
| [^200] | [Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy.](http://arxiv.org/abs/2305.11616) | 这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。 |
| [^201] | [GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework.](http://arxiv.org/abs/2305.10841) | GETMusic提出了一种统一的音乐生成模型，包括新颖的音乐表示GETScore和扩散模型GETDiff。GETScore使用标记表示音符，将它们有序地组织起来，而GETDiff使用遮盖对目标轨道进行破坏，能够生成任意轨道。 |
| [^202] | [ZeroFlow: Fast Zero Label Scene Flow via Distillation.](http://arxiv.org/abs/2305.10424) | ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。 |
| [^203] | [On Dataset Transferability in Active Learning for Transformers.](http://arxiv.org/abs/2305.09807) | 本文研究了基于transformer的预训练语言模型的主动学习中数据集的可迁移性问题，发现具有相似获取序列的主动学习方法产生的数据集在不同模型之间具有高度的可迁移性。 |
| [^204] | [GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering.](http://arxiv.org/abs/2305.03403) | 介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。 |
| [^205] | [Goal-oriented Uncertainty Quantification for Inverse Problems via Variational Encoder-Decoder Networks.](http://arxiv.org/abs/2304.08324) | 本文提出了一种使用变分编码器-解码器网络进行目标导向反问题不确定性量化的新方法。通过利用数据驱动的方式，可以实时估计和计算与逆问题解相关的感兴趣量的不确定性指标。 |
| [^206] | [A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control.](http://arxiv.org/abs/2304.04066) | 本文提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，针对强化学习控制现实世界系统时的安全稳定控制问题提出了一种解决方案。其中，基于重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。 |
| [^207] | [Efficient Quantum Algorithms for Quantum Optimal Control.](http://arxiv.org/abs/2304.02613) | 本文提出了一种高效的量子算法，解决量子最优控制问题，运用容错的量子计算机，具有复杂的机器学习关系。 |
| [^208] | [Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need.](http://arxiv.org/abs/2303.15256) | 本研究提出了积极自监督学习（PAL）框架，通过查询样本之间的语义关系来解决自监督学习中需要知道正视图的限制。PAL不仅提供了理论上有基础的学习框架，还能够嵌入先验知识并支持监督和半监督学习。 |
| [^209] | [Inverse problem regularization with hierarchical variational autoencoders.](http://arxiv.org/abs/2303.11217) | 本文提出了一种使用分层变分自动编码器（HVAE）对不适定的逆问题进行正则化的方法，该方法综合了基于去噪器的Plug & Play方法和基于生成模型的逆问题方法的优势，并且在自然图像的图像恢复问题上表现出竞争力。 |
| [^210] | [Meta contrastive label correction for financial time series.](http://arxiv.org/abs/2303.08103) | 本文针对股票价格预测中标记不准确的问题，提出了一种元对比标签校正方法。方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签，并提高分类性能。 |
| [^211] | [Large Language Models for Code: Security Hardening and Adversarial Testing.](http://arxiv.org/abs/2302.05319) | 本研究针对大型语言模型在生成代码时缺乏安全意识，从安全加固和对抗测试的角度入手，提出了一项新的安全任务——受控代码生成，通过一种新型基于学习的方法SVEN，实现生成既安全又功能正确的代码，并对当前的LM进行对抗测试，强调了在LM的培训和评估中考虑安全因素的必要性。 |
| [^212] | [Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments.](http://arxiv.org/abs/2302.04823) | 本研究提出了一种名为hGAIL的架构，用于解决车辆的自主导航问题，通过将感知信息直接映射到低级动作的同时，学习车辆环境的中级输入表示。 |
| [^213] | [Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job Shop Problem Using Graph Neural Network and Reinforcement Learning.](http://arxiv.org/abs/2302.02506) | 本研究提出了一种基于图神经网络和强化学习的方法，用于生成适应性调度规则来解决中断交换允许的阻塞工件车间问题，并开发了一个模拟器来模拟实际工业生产中的中断和交换情况。 |
| [^214] | [Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition.](http://arxiv.org/abs/2301.12592) | 本研究提出了一种集成学习的多视角视觉融合方法，用于处理在真实世界应用中可能存在的间歇性信息缺失问题。通过设计学习框架和提出的插补方案，实现了对驾驶员手势活动的准确分类和位置估计，提高了自动驾驶的安全性能。 |
| [^215] | [BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization.](http://arxiv.org/abs/2301.03313) | BQ-NCO通过将组合优化问题转化为马尔科夫决策过程并使用双射缩减方法来提高超出分布的鲁棒性。 |
| [^216] | [Risk-Adaptive Approaches to Learning and Decision Making: A Survey.](http://arxiv.org/abs/2212.00856) | 本文调查了过去25年中风险测度的快速发展，介绍了其在各个领域的应用，以及与效用理论和分布鲁棒优化的关系，并指出了公平机器学习等新兴应用领域。 |
| [^217] | [Differentially-Private Data Synthetisation for Efficient Re-Identification Risk Control.](http://arxiv.org/abs/2212.00484) | 本文提出了一种名为ε-PrivateSMOTE的技术，通过噪声引入插值的合成数据生成，以达到保护免受重新识别和链接攻击的风险的目的，并在同时最大限度地保持数据效用的情况下取得了竞争性的结果。 |
| [^218] | [Generalized Balancing Weights via Deep Neural Networks.](http://arxiv.org/abs/2211.07533) | 本文提出了一种广义平衡权重方法（NBW），通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重，用于估计任意混合离散和连续干预的因果效应。 |
| [^219] | [TiDAL: Learning Training Dynamics for Active Learning.](http://arxiv.org/abs/2210.06788) | 提出了一种新的主动学习方法TiDAL，利用训练动力学来量化未标记数据的不确定性，为了解决大规模数据的跟踪问题，利用预测模块学习标记数据的训练动力学。 |
| [^220] | [Hierarchical Neyman-Pearson Classification for Prioritizing Severe Disease Categories in COVID-19 Patient Data.](http://arxiv.org/abs/2210.02197) | 本研究提出了一种分层的Neyman-Pearson分类框架，用于在COVID-19患者数据中优先确定严重疾病类别。通过使用患者的生物特征来预测严重程度类别，该方法可以控制错误分类，并在多类别分类中提供高概率的错误优先控制。 |
| [^221] | [Tuning arrays with rays: Physics-informed tuning of quantum dot charge states.](http://arxiv.org/abs/2209.03837) | 本论文提出了一种名为物理信息驱动的调谐（PIT）的框架，用于自动调谐量子计算机中的全局状态和电荷态。其中，PIT的第一个模块使用机器学习分类器和物理知识进行导航，第二个模块使用一系列测量来调节电荷状态。这些工具具有直观、可靠和数据高效的特点。 |
| [^222] | [Deformation equivariant cross-modality image synthesis with paired non-aligned training data.](http://arxiv.org/abs/2208.12491) | 本研究提出了一种使用配对但不对齐数据进行交叉模态图像合成的通用解决方案，通过引入新的变形等变鼓励损失函数，实现了联合训练图像合成网络和配准网络，并允许在非对齐数据条件下进行对抗训练。这项工作降低了新的临床应用的难度。 |
| [^223] | [Hybrid quantum ResNet for car classification and its hyperparameter optimization.](http://arxiv.org/abs/2205.04878) | 本文提出了一种混合量子-经典机器学习模型和基于量子灵感的超参数优化技术，用于图像识别任务中的汽车分类。我们的研究在标准黑盒目标函数上进行了基准测试，证明了我们的方法在减少期望运行时间和适应度方面的性能改善。 |
| [^224] | [Flow-based density of states for complex actions.](http://arxiv.org/abs/2203.01243) | 基于归一化流的新型采样算法可以解决复杂动作问题中的态密度计算困难，并且避免了数值积分带来的误差累积。 |
| [^225] | [Finite-Time Error Bounds for Distributed Linear Stochastic Approximation.](http://arxiv.org/abs/2111.12665) | 本文研究了一种新颖的分布式线性随机逼近算法，在邻居间进行共识型交互，并考虑了马尔可夫噪声的影响。针对连接矩阵为简单随机矩阵的情况，推导出了统一强连接图序列下的有限时间平方均误差界限。 |
| [^226] | [Breaking the Deadly Triad with a Target Network.](http://arxiv.org/abs/2101.08862) | 本论文研究了如何通过使用目标网络来稳定训练，提出了一种新的目标网络更新规则并证明了其在离线学习、线性函数逼近和自举的算法中的收敛性，最终达到了收敛到正则化TD固定点的效果。 |
| [^227] | [Control-Data Separation and Logical Condition Propagation for Efficient Inference on Probabilistic Programs.](http://arxiv.org/abs/2101.01502) | 本论文提出了一种新的概率程序采样框架，通过将控制-数据分离和逻辑条件传播相结合，实现了高效推理，并在使用while循环和稀有观测的程序中表现出了优势。 |

# 详细

[^1]: 基于图像神经网络的有限区域天气预测

    Graph-based Neural Weather Prediction for Limited Area Modeling. (arXiv:2309.17370v1 [cs.LG])

    [http://arxiv.org/abs/2309.17370](http://arxiv.org/abs/2309.17370)

    该论文提出了一种基于图像神经网络的有限区域天气预测方法，并通过使用北欧地区的本地模型进行了验证。

    

    高精度的机器学习方法在天气预报领域的应用为模拟大气的可能性带来了新的变革。在气候变化时代，获取像这样的高分辨率预报模型的能力变得越来越重要。虽然大多数现有的神经网络天气预报方法都是针对全球预测，但如何将这些技术应用于有限区域建模是一个重要问题。本文将基于图像的神经网络天气预测方法应用于有限区域，并提出了多尺度分层模型扩展。通过使用北欧地区的本地模型进行实验证实了我们的方法的有效性。

    The rise of accurate machine learning methods for weather forecasting is creating radical new possibilities for modeling the atmosphere. In the time of climate change, having access to high-resolution forecasts from models like these is also becoming increasingly vital. While most existing Neural Weather Prediction (NeurWP) methods focus on global forecasting, an important question is how these techniques can be applied to limited area modeling. In this work we adapt the graph-based NeurWP approach to the limited area setting and propose a multi-scale hierarchical model extension. Our approach is validated by experiments with a local model for the Nordic region.
    
[^2]: 3D-Mol: 一种新颖的基于对比学习的分子性质预测框架，利用了3D信息

    3D-Mol: A Novel Contrastive Learning Framework for Molecular Property Prediction with 3D Information. (arXiv:2309.17366v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.17366](http://arxiv.org/abs/2309.17366)

    3D-Mol是一种新颖的基于3D结构的分子建模方法，通过对比学习提高了分子性质预测准确性，并在多个基准数据集上超过了最先进的模型。

    

    分子性质预测为药物候选物的早期筛选和优化提供了一种有效且高效的方法。尽管基于深度学习的方法取得了显著进展，但大多数现有方法仍未充分利用3D空间信息。这可能导致单个分子表示多个实际分子。为解决这些问题，我们提出了一种名为3D-Mol的新颖的基于3D结构的分子建模方法。为了准确表示完整的空间结构，我们设计了一种新颖的编码器，通过将分子分解成三个几何图形来提取3D特征。此外，我们使用20M个无标签数据通过对比学习对模型进行预训练。我们将具有相同拓扑结构的构象视为正样本对，将相反的构象视为负样本对，而权重则由构象之间的差异确定。我们在7个基准数据集上将3D-Mol与各种最先进的基准模型进行了对比。

    Molecular property prediction offers an effective and efficient approach for early screening and optimization of drug candidates. Although deep learning based methods have made notable progress, most existing works still do not fully utilize 3D spatial information. This can lead to a single molecular representation representing multiple actual molecules. To address these issues, we propose a novel 3D structure-based molecular modeling method named 3D-Mol. In order to accurately represent complete spatial structure, we design a novel encoder to extract 3D features by deconstructing the molecules into three geometric graphs. In addition, we use 20M unlabeled data to pretrain our model by contrastive learning. We consider conformations with the same topological structure as positive pairs and the opposites as negative pairs, while the weight is determined by the dissimilarity between the conformations. We compare 3D-Mol with various state-of-the-art (SOTA) baselines on 7 benchmarks and de
    
[^3]: 通过最小移动方案实现神经网络的模块化训练

    Module-wise Training of Neural Networks via the Minimizing Movement Scheme. (arXiv:2309.17357v1 [cs.LG])

    [http://arxiv.org/abs/2309.17357](http://arxiv.org/abs/2309.17357)

    通过引入模块化正则化方法，解决了神经网络模块化训练中早期层过拟合和深层停滞的问题，实验结果展示了该方法在不同架构上的优越性。

    

    在内存有限的受限设备环境中，贪婪的逐层或逐模块训练神经网络可以绕过端到端反向传播的一些问题，因此具有吸引力。然而，这种方法存在停滞问题，早期层过拟合和更深层在一定深度后停止提高测试准确性。我们提出通过引入与分布空间中梯度流的最小化移动方法相启发的模块化正则化来解决这个问题。我们称这种方法为TRGL（Transport Regularized Greedy Learning），并对其进行了理论研究，证明它会导致模块化贪婪方法是规则的，并逐步解决任务。在实验中，我们展示了在添加我们的正则化方法之后，各种架构（如ResNets，Transformers和VGG）的模块化训练的准确性得到了改善，其优于其他模块化训练方法，甚至经常优于端到端训练，并且可以减少高达60%的内存使用。

    Greedy layer-wise or module-wise training of neural networks is compelling in constrained and on-device settings where memory is limited, as it circumvents a number of problems of end-to-end back-propagation. However, it suffers from a stagnation problem, whereby early layers overfit and deeper layers stop increasing the test accuracy after a certain depth. We propose to solve this issue by introducing a module-wise regularization inspired by the minimizing movement scheme for gradient flows in distribution space. We call the method TRGL for Transport Regularized Greedy Learning and study it theoretically, proving that it leads to greedy modules that are regular and that progressively solve the task. Experimentally, we show improved accuracy of module-wise training of various architectures such as ResNets, Transformers and VGG, when our regularization is added, superior to that of other module-wise training methods and often to end-to-end training, with as much as 60% less memory usage
    
[^4]: 高效的生物合理对抗训练

    Efficient Biologically Plausible Adversarial Training. (arXiv:2309.17348v1 [cs.LG])

    [http://arxiv.org/abs/2309.17348](http://arxiv.org/abs/2309.17348)

    本文研究了生物合理的学习算法是否比反向传播更具有对抗攻击的鲁棒性，并进行了广泛的比较分析。

    

    用反向传播训练的人工神经网络(ANNs)表现出令人惊讶的性能，并且越来越多地被用于执行我们日常生活中的任务。然而，ANNs极易受到对抗攻击的影响，这些攻击通过微小的有针对性的扰动来改变输入，从而严重破坏模型的性能。使ANNs对这些攻击具有鲁棒性最有效的方法是对抗训练，其中训练数据集被添加了样本用于对抗攻击。不幸的是，这种方法的缺点是增加了训练复杂性，因为生成对抗样本是非常计算消耗高的。与ANNs不同，人类不容易受到对抗攻击的影响。因此，在这项工作中，我们研究了生物合理的学习算法是否比BP更具有对抗攻击的鲁棒性。具体而言，我们对BP和“Error to Pertu"的对抗鲁棒性进行了广泛的比较分析。

    Artificial Neural Networks (ANNs) trained with Backpropagation (BP) show astounding performance and are increasingly often used in performing our daily life tasks. However, ANNs are highly vulnerable to adversarial attacks, which alter inputs with small targeted perturbations that drastically disrupt the models' performance. The most effective method to make ANNs robust against these attacks is adversarial training, in which the training dataset is augmented with exemplary adversarial samples. Unfortunately, this approach has the drawback of increased training complexity since generating adversarial samples is very computationally demanding. In contrast to ANNs, humans are not susceptible to adversarial attacks. Therefore, in this work, we investigate whether biologically-plausible learning algorithms are more robust against adversarial attacks than BP. In particular, we present an extensive comparative analysis of the adversarial robustness of BP and \textit{Present the Error to Pertu
    
[^5]: 人口平衡: 减轻现实世界数据中的偏见

    Demographic Parity: Mitigating Biases in Real-World Data. (arXiv:2309.17347v1 [cs.LG])

    [http://arxiv.org/abs/2309.17347](http://arxiv.org/abs/2309.17347)

    本论文提出了一种方法，可以在训练模型时消除历史数据中的偏见，并在最大程度保留分类效用。这种方法以模型无关的方式从现实世界数据中衍生出具有人口平衡和真实性编码的数据集，并在实验证明中取得了成功。

    

    计算机决策系统广泛应用于自动化处理生活中的许多决策，其中包括敏感领域，如招聘、贷款甚至刑罚。决策流程严重依赖于大量历史现实世界数据来训练模型。然而，历史训练数据经常包含性别、种族或其他偏见，这些偏见会传播到训练模型中，并影响计算机决策。在这项工作中，我们提出了一种强大的方法，可以在最大程度保留分类效用的同时，确保消除不想要的偏见。我们的方法可以以与模型无关的方式始终实现这一目标，通过从现实世界数据中衍生出唯一编码人口平衡和真实性的渐进数据集。作为一个原则性的证明，我们从公共普查记录中推导出这样一个渐进数据集，可以生成合成样本来训练成熟的分类器。对这些分类器的泛化能力进行基准测试。

    Computer-based decision systems are widely used to automate decisions in many aspects of everyday life, which include sensitive areas like hiring, loaning and even criminal sentencing. A decision pipeline heavily relies on large volumes of historical real-world data for training its models. However, historical training data often contains gender, racial or other biases which are propagated to the trained models influencing computer-based decisions. In this work, we propose a robust methodology that guarantees the removal of unwanted biases while maximally preserving classification utility. Our approach can always achieve this in a model-independent way by deriving from real-world data the asymptotic dataset that uniquely encodes demographic parity and realism. As a proof-of-principle, we deduce from public census records such an asymptotic dataset from which synthetic samples can be generated to train well-established classifiers. Benchmarking the generalization capability of these cla
    
[^6]: 《面向通用模型的自由数据选择》

    Towards Free Data Selection with General-Purpose Models. (arXiv:2309.17342v1 [cs.CV])

    [http://arxiv.org/abs/2309.17342](http://arxiv.org/abs/2309.17342)

    本文提出了一种新的数据选择流程，利用通用模型在单次推理中选择来自不同数据集的数据，无需额外的训练或监督。通过定义和利用语义模式提取微妙的局部信息，我们实现了对所有数据样本的选择。

    

    一个理想的数据选择算法可以高效地选择最具信息量的样本，以最大化有限的注释预算的效用。然而，目前的方法（例如主动学习方法）通常遵循一个繁琐的流程，反复进行耗时的模型训练和批量数据选择。在本文中，我们挑战了这种现状，通过设计一个独特的数据选择流程，利用现有的通用模型，在单次推理中选择来自不同数据集的数据，而无需额外的训练或监督。我们提出了一种新的自由数据选择（FreeSel）方法来实现这个新的流程。具体地，我们定义了从通用模型的中间特征中提取的语义模式，以捕捉每个图像中微妙的局部信息。然后，我们通过基于距离的采样在细粒度的语义模式级别上实现了对所有数据样本的选择。FreeSel绕过了原来的耗时训练和批量数据选择的流程。

    A desirable data selection algorithm can efficiently choose the most informative samples to maximize the utility of limited annotation budgets. However, current approaches, represented by active learning methods, typically follow a cumbersome pipeline that iterates the time-consuming model training and batch data selection repeatedly. In this paper, we challenge this status quo by designing a distinct data selection pipeline that utilizes existing general-purpose models to select data from various datasets with a single-pass inference without the need for additional training or supervision. A novel free data selection (FreeSel) method is proposed following this new pipeline. Specifically, we define semantic patterns extracted from inter-mediate features of the general-purpose model to capture subtle local information in each image. We then enable the selection of all data samples in a single pass through distance-based sampling at the fine-grained semantic pattern level. FreeSel bypass
    
[^7]: MixQuant: 带有位宽优化搜索的混合精度量化

    MixQuant: Mixed Precision Quantization with a Bit-width Optimization Search. (arXiv:2309.17341v1 [cs.LG])

    [http://arxiv.org/abs/2309.17341](http://arxiv.org/abs/2309.17341)

    本研究提出了一种称为MixQuant的混合精度量化算法，在每个层权重上找到了最佳的量化位宽，从而减少了量化模型的准确性降低问题。

    

    量化是一种创建高效深度神经网络（DNNs）的技术，它通过在比f32浮点精度更低的位宽上执行计算和存储张量来实现。量化减少了模型大小和推理延迟，因此可以在计算资源受限和实时系统上部署DNNs。然而，量化可能会导致由舍入误差引起的数值不稳定性，从而导致计算不准确，进而降低了量化模型的准确性。类似于先前的研究表明，偏置和激活对量化更敏感，最好保持全精度或用更高的位宽进行量化，我们表明一些权重比其他权重更敏感，应在其量化位宽上反映出来。为此，我们提出了MixQuant，一种基于舍入误差的搜索算法，以找到每个层权重的最佳定制量化位宽。

    Quantization is a technique for creating efficient Deep Neural Networks (DNNs), which involves performing computations and storing tensors at lower bit-widths than f32 floating point precision. Quantization reduces model size and inference latency, and therefore allows for DNNs to be deployed on platforms with constrained computational resources and real-time systems. However, quantization can lead to numerical instability caused by roundoff error which leads to inaccurate computations and therefore, a decrease in quantized model accuracy. Similarly to prior works, which have shown that both biases and activations are more sensitive to quantization and are best kept in full precision or quantized with higher bit-widths, we show that some weights are more sensitive than others which should be reflected on their quantization bit-width. To that end we propose MixQuant, a search algorithm that finds the optimal custom quantization bit-width for each layer weight based on roundoff error and
    
[^8]: Outage-Watch: 使用极端事件正则化提早预测服务中断

    Outage-Watch: Early Prediction of Outages using Extreme Event Regularizer. (arXiv:2309.17340v1 [cs.DC])

    [http://arxiv.org/abs/2309.17340](http://arxiv.org/abs/2309.17340)

    Outage-Watch使用极端事件正则化的方法提早预测云服务中断，通过捕获质量指标的恶化情况，改善灵活性并提高尾部分布的学习能力。

    

    云服务无处不在，关键云服务的故障是不可避免的事实。为了保留客户并防止收入损失，提供高可靠性保证对于这些服务来说是非常重要的。预测服务中断是一种减轻严重程度和恢复时间的方法。由于这些事件的罕见性，预测关键性故障是困难的。此外，关键故障在可观测数据方面定义模糊。我们提出的方法Outage-Watch将关键的服务中断定义为一组指标所捕获的服务质量（QoS）的恶化情况。Outage-Watch通过使用当前系统状态来预测QoS指标是否会超过阈值并引发极端事件，提前检测此类中断。使用高斯混合模型来模拟QoS指标的分布以提高灵活性，而极端事件正则化有助于提高尾部分布的学习能力。

    Cloud services are omnipresent and critical cloud service failure is a fact of life. In order to retain customers and prevent revenue loss, it is important to provide high reliability guarantees for these services. One way to do this is by predicting outages in advance, which can help in reducing the severity as well as time to recovery. It is difficult to forecast critical failures due to the rarity of these events. Moreover, critical failures are ill-defined in terms of observable data. Our proposed method, Outage-Watch, defines critical service outages as deteriorations in the Quality of Service (QoS) captured by a set of metrics. Outage-Watch detects such outages in advance by using current system state to predict whether the QoS metrics will cross a threshold and initiate an extreme event. A mixture of Gaussian is used to model the distribution of the QoS metrics for flexibility and an extreme event regularizer helps in improving learning in tail of the distribution. An outage is 
    
[^9]: 自监督跨表格表示学习的扩展实验

    Scaling Experiments in Self-Supervised Cross-Table Representation Learning. (arXiv:2309.17339v1 [cs.LG])

    [http://arxiv.org/abs/2309.17339](http://arxiv.org/abs/2309.17339)

    本文介绍了一种新颖的基于Transformer的架构，用于深度表格表示学习，以及针对跨表格表示学习的方法。通过自监督训练和不同规模的模型训练，该架构在单表和跨表预训练设置中展现了良好的扩展性能。

    

    为了分析深度表格表示学习模型的扩展潜力，我们引入了一种新颖的基于Transformer的架构，专门针对表格数据和跨表格表示学习，通过利用表格特定的分词器和共享的Transformer骨干结构。我们的训练方法包括单表和跨表模型，通过自监督的遮蔽单元恢复目标进行缺失值填充。为了了解我们方法的扩展行为，我们训练了不同规模的模型，参数范围从大约$10^4$到$10^7$。这些模型是在精心策划的预训练数据集上训练的，该数据集包含来自76个不同数据集的135M个训练标记。我们通过在线性探测方法在精心策划的基准数据集上评估预训练模型，并与传统基准进行比较，来评估我们架构在单表和跨表预训练设置中的扩展性。

    To analyze the scaling potential of deep tabular representation learning models, we introduce a novel Transformer-based architecture specifically tailored to tabular data and cross-table representation learning by utilizing table-specific tokenizers and a shared Transformer backbone. Our training approach encompasses both single-table and cross-table models, trained via missing value imputation through a self-supervised masked cell recovery objective. To understand the scaling behavior of our method, we train models of varying sizes, ranging from approximately $10^4$ to $10^7$ parameters. These models are trained on a carefully curated pretraining dataset, consisting of 135M training tokens sourced from 76 diverse datasets. We assess the scaling of our architecture in both single-table and cross-table pretraining setups by evaluating the pretrained models using linear probing on a curated set of benchmark datasets and comparing the results with conventional baselines.
    
[^10]: 在动态多智能体环境中通过去掉航点来改进轨迹预测

    Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v1 [cs.RO])

    [http://arxiv.org/abs/2309.17338](http://arxiv.org/abs/2309.17338)

    本文引入了一种新的框架，通过航点去除技术促进了显式的时间学习，并显著提高了轨迹预测的效果。

    

    轨迹的多样和不确定性本质给准确建模带来了巨大的挑战。运动预测系统必须有效地从过去学习空间和时间信息，以预测智能体的未来轨迹。许多现有方法通过堆叠模型中的单独组件学习时间运动，以捕捉时间特征。本文介绍了一种新颖的框架，称为Temporal Waypoint Dropping（TWD），通过航点去除技术促进显式的时间学习。通过航点去除学习可以迫使模型改善其对智能体之间的时间关联的理解，从而显著提高轨迹预测的效果。轨迹预测方法常常假设观测到的轨迹航点序列是完整的，忽略了现实世界中可能存在缺失值的情况，这可能会影响其性能。

    The inherently diverse and uncertain nature of trajectories presents a formidable challenge in accurately modeling them. Motion prediction systems must effectively learn spatial and temporal information from the past to forecast the future trajectories of the agent. Many existing methods learn temporal motion via separate components within stacked models to capture temporal features. This paper introduces a novel framework, called Temporal Waypoint Dropping (TWD), that promotes explicit temporal learning through the waypoint dropping technique. Learning through waypoint dropping can compel the model to improve its understanding of temporal correlations among agents, thus leading to a significant enhancement in trajectory prediction. Trajectory prediction methods often operate under the assumption that observed trajectory waypoint sequences are complete, disregarding real-world scenarios where missing values may occur, which can influence their performance. Moreover, these models freque
    
[^11]: 向实现管道意识的机器学习公平性迈进：开发实际指南和工具的研究议程。

    Toward Operationalizing Pipeline-aware ML Fairness: A Research Agenda for Developing Practical Guidelines and Tools. (arXiv:2309.17337v1 [cs.LG])

    [http://arxiv.org/abs/2309.17337](http://arxiv.org/abs/2309.17337)

    该论文提出了一种实现机器学习公平性的管道意识方法，并强调需要指南和工具来在实践中应用这种方法。

    

    虽然算法公平性是一个蓬勃发展的研究领域，但在实践中，减少偏见问题常常被简化为通过强制执行任意选择的公平性度量标准来实现，要么是在优化阶段强制执行公平性约束，要么是在后处理模型输出时，或者通过操纵训练数据。最近的工作呼吁机器学习社区采取更全面的方法来解决公平性问题，通过系统地研究通过机器学习流程中所做的许多设计选择，并确定针对问题根本原因而不是其症状的干预措施。尽管我们赞同这种基于管道的方法是在实际场景中解决算法不公平性最合适的方法，但我们认为目前几乎没有方法在实践中"实施"这种方法。根据我们作为教育者和实践者的经验，我们首先证明了没有清晰的指南和工具包，即使拥有专业的机器学习知识的个人也会遇到困难。

    While algorithmic fairness is a thriving area of research, in practice, mitigating issues of bias often gets reduced to enforcing an arbitrarily chosen fairness metric, either by enforcing fairness constraints during the optimization step, post-processing model outputs, or by manipulating the training data. Recent work has called on the ML community to take a more holistic approach to tackle fairness issues by systematically investigating the many design choices made through the ML pipeline, and identifying interventions that target the issue's root cause, as opposed to its symptoms. While we share the conviction that this pipeline-based approach is the most appropriate for combating algorithmic unfairness on the ground, we believe there are currently very few methods of \emph{operationalizing} this approach in practice. Drawing on our experience as educators and practitioners, we first demonstrate that without clear guidelines and toolkits, even individuals with specialized ML knowled
    
[^12]: 异步图生成器

    Asynchronous Graph Generators. (arXiv:2309.17335v1 [cs.LG])

    [http://arxiv.org/abs/2309.17335](http://arxiv.org/abs/2309.17335)

    异步图生成器（AGG）是一种新型的图神经网络架构，通过节点生成进行数据插补，并隐式学习传感器测量的因果图表示，取得了state-of-the-art的结果。

    

    我们引入了异步图生成器（AGG），这是一种用于多通道时间序列的新型图神经网络架构。AGG将观测值建模为动态图上的节点，并通过转导式节点生成进行数据插补。AGG不依赖于循环组件或对时间规律的假设，使用可学习的嵌入将测量值、时间戳和元数据直接表示在节点中，并利用注意机制来学习变量之间的关系。这样，所提出的架构隐式地学习传感器测量的因果图表示，可以基于未见时间戳和元数据对新的测量进行预测。我们将所提出的AGG在概念和实证两方面与之前的工作进行了比较，并简要讨论了数据增强对AGG性能的影响。实验结果表明，AGG在t

    We introduce the asynchronous graph generator (AGG), a novel graph neural network architecture for multi-channel time series which models observations as nodes on a dynamic graph and can thus perform data imputation by transductive node generation. Completely free from recurrent components or assumptions about temporal regularity, AGG represents measurements, timestamps and metadata directly in the nodes via learnable embeddings, to then leverage attention to learn expressive relationships across the variables of interest. This way, the proposed architecture implicitly learns a causal graph representation of sensor measurements which can be conditioned on unseen timestamps and metadata to predict new measurements by an expansion of the learnt graph. The proposed AGG is compared both conceptually and empirically to previous work, and the impact of data augmentation on the performance of AGG is also briefly discussed. Our experiments reveal that AGG achieved state-of-the-art results in t
    
[^13]: 通过隐式点图网络高效解剖标记肺部树状结构

    Efficient Anatomical labeling of Pulmonary Tree Structures via Implicit Point-Graph Networks. (arXiv:2309.17329v1 [cs.CV])

    [http://arxiv.org/abs/2309.17329](http://arxiv.org/abs/2309.17329)

    本文介绍了一种通过隐式点图网络高效解剖标记肺部树状结构的方法，提供了SOTA准确度和可用的表面，同时还提供了一个用于评估该方法的数据集。

    

    肺部疾病在全球范围内是导致死亡的主要原因之一。治愈肺部疾病需要更好地理解肺部系统内的许多复杂的3D树状结构，如气道、动脉和静脉。在理论上，它们可以通过高分辨率图像堆栈进行建模。然而，基于密集体素网格的标准CNN方法代价过高。为了解决这个问题，我们引入了一种基于点的方法，保留了树骨架的图连通性，并结合了隐式表面表示。它以较低的计算成本提供了SOTA准确度，生成的模型具有可用的表面。由于公开可访问的数据稀缺，我们还整理了一套广泛的数据集来评估我们的方法，并将其公开。

    Pulmonary diseases rank prominently among the principal causes of death worldwide. Curing them will require, among other things, a better understanding of the many complex 3D tree-shaped structures within the pulmonary system, such as airways, arteries, and veins. In theory, they can be modeled using high-resolution image stacks. Unfortunately, standard CNN approaches operating on dense voxel grids are prohibitively expensive. To remedy this, we introduce a point-based approach that preserves graph connectivity of tree skeleton and incorporates an implicit surface representation. It delivers SOTA accuracy at a low computational cost and the resulting models have usable surfaces. Due to the scarcity of publicly accessible data, we have also curated an extensive dataset to evaluate our approach and will make it public.
    
[^14]: 通过梯度分位数剪切实现鲁棒性随机优化

    Robust Stochastic Optimization via Gradient Quantile Clipping. (arXiv:2309.17316v1 [stat.ML])

    [http://arxiv.org/abs/2309.17316](http://arxiv.org/abs/2309.17316)

    本文介绍了一种基于梯度分位数剪切的鲁棒性随机优化策略，适用于光滑目标且能容忍异常值和尾重样本。对于强凸目标，迭代收敛到集中分布并导出了估计误差的概率界。在非凸情况下，极限分布局部化在低梯度邻域上。使用滚动分位数实现的算法具有很强的鲁棒性和高效性。

    

    我们提出了一种基于梯度范数分位数作为剪切阈值的策略，用于随机梯度下降 (SGD)。我们证明了这种新策略在光滑目标（凸或非凸）下提供了一种鲁棒且高效的优化算法，能够容忍尾重样本（包括无限方差）和数据流中的异常值，类似于 Huber 污染模型。我们的数学分析利用了恒定步长的 SGD 和马尔可夫链之间的联系，并以独特的方式处理剪切引入的偏差。对于强凸目标，我们证明迭代收敛到一个集中分布，并导出了最终估计误差的高概率界。在非凸情况下，我们证明极限分布局部化在低梯度邻域上。我们提出了一种使用滚动分位数实现此算法的方法，从而得到了一种高效的优化过程，具有很强的鲁棒性。

    We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which uses quantiles of the gradient norm as clipping thresholds. We prove that this new strategy provides a robust and efficient optimization algorithm for smooth objectives (convex or non-convex), that tolerates heavy-tailed samples (including infinite variance) and a fraction of outliers in the data stream akin to Huber contamination. Our mathematical analysis leverages the connection between constant step size SGD and Markov chains and handles the bias introduced by clipping in an original way. For strongly convex objectives, we prove that the iteration converges to a concentrated distribution and derive high probability bounds on the final estimation error. In the non-convex case, we prove that the limit distribution is localized on a neighborhood with low gradient. We propose an implementation of this algorithm using rolling quantiles which leads to a highly efficient optimization procedure with strong robustn
    
[^15]: 机器学习中的一次出训练数据的可辨识性分析

    Leave-one-out Distinguishability in Machine Learning. (arXiv:2309.17310v1 [cs.LG])

    [http://arxiv.org/abs/2309.17310](http://arxiv.org/abs/2309.17310)

    这项研究引入了一种新的分析框架，用于衡量机器学习算法在训练集中的少量数据点被排除后输出分布的变化。通过使用高斯过程模型和成员推断攻击的经验分析，该方法实现了对数据记忆和信息泄漏的有效衡量和优化。

    

    我们引入了一个新的分析框架，用于量化机器学习算法在训练集中包含少量数据点后输出分布的变化，我们将这个概念定义为一次出训练数据的可辨识性(LOOD)。这个问题对于衡量机器学习中的数据记忆和信息泄漏以及训练数据对模型预测的影响至关重要。我们使用高斯过程模型来建模机器学习算法的随机性，并通过对成员推断攻击使用广泛的经验分析验证了LOOD。我们的理论框架使我们能够研究信息泄漏的原因以及泄漏程度高的位置。例如，我们分析了激活函数对数据记忆的影响。此外，我们的方法允许我们优化...

    We introduce a new analytical framework to quantify the changes in a machine learning algorithm's output distribution following the inclusion of a few data points in its training set, a notion we define as leave-one-out distinguishability (LOOD). This problem is key to measuring data **memorization** and **information leakage** in machine learning, and the **influence** of training data points on model predictions. We illustrate how our method broadens and refines existing empirical measures of memorization and privacy risks associated with training data. We use Gaussian processes to model the randomness of machine learning algorithms, and validate LOOD with extensive empirical analysis of information leakage using membership inference attacks. Our theoretical framework enables us to investigate the causes of information leakage and where the leakage is high. For example, we analyze the influence of activation functions, on data memorization. Additionally, our method allows us to optim
    
[^16]: 导航等变扩散基生成模型的设计空间，用于从头生成3D分子

    Navigating the Design Space of Equivariant Diffusion-Based Generative Models for De Novo 3D Molecule Generation. (arXiv:2309.17296v1 [cs.LG])

    [http://arxiv.org/abs/2309.17296](http://arxiv.org/abs/2309.17296)

    本论文探索了E(3)等变扩散模型的设计空间，提出了EQGAT-diff模型，通过在连续的原子位置和分类的化学元素与键类型之间的交互中改进，其在从头设计3D分子方面的性能显著超过已有模型。

    

    深度生成扩散模型是材料科学和药物发现中从头设计3D分子的一种有前途的途径。然而，它们在大分子结构和有限的训练数据方面的性能仍受到限制。为了解决这个问题，我们探索了E(3)等变扩散模型的设计空间，重点关注以前的空白点。我们进行了广泛的比较分析，评估了连续和离散状态空间之间的相互作用。在这个调查中，我们引入了EQGAT-diff模型，其在QM9和GEOM-Drugs数据集上的性能始终大大超过已建立模型。与其他模型不同的是，EQGAT-diff采用连续的原子位置，而化学元素和键类型是分类的，并采用时间相关的损失加权，这显著提高了训练收敛和生成样本的质量。为进一步增强扩散模型对有限训练数据的适用性，

    Deep generative diffusion models are a promising avenue for de novo 3D molecular design in material science and drug discovery. However, their utility is still constrained by suboptimal performance with large molecular structures and limited training data. Addressing this gap, we explore the design space of E(3) equivariant diffusion models, focusing on previously blank spots. Our extensive comparative analysis evaluates the interplay between continuous and discrete state spaces. Out of this investigation, we introduce the EQGAT-diff model, which consistently surpasses the performance of established models on the QM9 and GEOM-Drugs datasets by a large margin. Distinctively, EQGAT-diff takes continuous atomic positions while chemical elements and bond types are categorical and employ a time-dependent loss weighting that significantly increases training convergence and the quality of generated samples. To further strengthen the applicability of diffusion models to limited training data, 
    
[^17]: 搜索分散的记忆：生成扩散模型是关联记忆网络

    In search of dispersed memories: Generative diffusion models are associative memory networks. (arXiv:2309.17290v1 [stat.ML])

    [http://arxiv.org/abs/2309.17290](http://arxiv.org/abs/2309.17290)

    本研究将生成扩散模型解释为基于能量的模型，证明其在训练离散模式时与现代Hopfield网络的能量函数等效。这种等效性使得我们可以将扩散模型的有监督训练解释为在权重结构中编码现代Hopfield网络的关联动力学的突触学习过程。

    

    Hopfield网络被广泛用作神经科学中的简化理论模型，用于生物关联记忆。原始的Hopfield网络通过编码二元关联模式来存储记忆，从而产生了一种称为Hebbian学习规则的突触学习机制。现代的Hopfield网络可以通过使用高度非线性的能量函数来实现指数级容量扩展。然而，这些新模型的能量函数不能直接压缩为二元突触耦合，并且也不能直接提供新的突触学习规则。在本研究中，我们展示了生成扩散模型可以被解释为基于能量的模型，并且在训练离散模式时，它们的能量函数与现代的Hopfield网络相等。这种等价性使我们能够将扩散模型的有监督训练解释为在权重结构中编码现代Hopfield网络的关联动力学的突触学习过程。

    Hopfield networks are widely used in neuroscience as simplified theoretical models of biological associative memory. The original Hopfield networks store memories by encoding patterns of binary associations, which result in a synaptic learning mechanism known as Hebbian learning rule. Modern Hopfield networks can achieve exponential capacity scaling by using highly non-linear energy functions. However, the energy function of these newer models cannot be straightforwardly compressed into binary synaptic couplings and it does not directly provide new synaptic learning rules. In this work we show that generative diffusion models can be interpreted as energy-based models and that, when trained on discrete patterns, their energy function is equivalent to that of modern Hopfield networks. This equivalence allows us to interpret the supervised training of diffusion models as a synaptic learning process that encodes the associative dynamics of a modern Hopfield network in the weight structure 
    
[^18]: 实时邻近防御方法在健壮推荐系统中的应用

    Toward Robust Recommendation via Real-time Vicinal Defense. (arXiv:2309.17278v1 [cs.LG])

    [http://arxiv.org/abs/2309.17278](http://arxiv.org/abs/2309.17278)

    本研究提出了一种实时邻近防御方法（RVD），通过在推荐之前利用邻近的训练数据微调模型，以提高推荐系统对毒化攻击的防御能力。

    

    推荐系统容易受到攻击，恶意数据插入可以导致系统提供有偏见的推荐。为了应对这种攻击，提出了各种健壮的学习方法。然而，大多数方法都是特定于模型或特定于攻击的，缺乏广泛性，而其他方法（如对抗训练）侧重于逃逸攻击，在毒化攻击上有很弱的防御能力。本文提出了一种通用方法——实时邻近防御（RVD），它利用邻近的训练数据在为每个用户进行推荐之前对模型进行微调。RVD在推断阶段工作，以确保实时性样本的健壮性，因此无需更改模型结构和训练过程，更加实用。广泛的实验结果表明，RVD有效地减轻了有针对性的毒化攻击。

    Recommender systems have been shown to be vulnerable to poisoning attacks, where malicious data is injected into the dataset to cause the recommender system to provide biased recommendations. To defend against such attacks, various robust learning methods have been proposed. However, most methods are model-specific or attack-specific, making them lack generality, while other methods, such as adversarial training, are oriented towards evasion attacks and thus have a weak defense strength in poisoning attacks.  In this paper, we propose a general method, Real-time Vicinal Defense (RVD), which leverages neighboring training data to fine-tune the model before making a recommendation for each user. RVD works in the inference phase to ensure the robustness of the specific sample in real-time, so there is no need to change the model structure and training process, making it more practical. Extensive experimental results demonstrate that RVD effectively mitigates targeted poisoning attacks acr
    
[^19]: 基于贝叶斯心智论的效用导向自适应教学策略

    Utility-based Adaptive Teaching Strategies using Bayesian Theory of Mind. (arXiv:2309.17275v1 [cs.LG])

    [http://arxiv.org/abs/2309.17275](http://arxiv.org/abs/2309.17275)

    本研究基于贝叶斯心智论机制设计了教师代理，其通过构建学习者内部状态模型并利用模型选择能够最大化学习者回报同时最小化教学成本的示范进行个性化教学，实验证明这种教学方式比传统不可知教学更高效。

    

    优秀的教师总是根据学习者的情况来进行解释。认知科学家们将这个过程建模为在理性原则下进行的：教师尽可能地最大化学习者的效用同时最小化教学成本。为此，人类教师似乎会建立起学习者内部状态的心智模型，这一能力被称为心智论（Theory of Mind，ToM）。受认知科学的启发，我们借鉴贝叶斯ToM机制来设计类人教师代理，这些代理像人类一样根据学习者来调整教学策略。我们的带有ToM的教师代理根据观察建立起学习者内部状态的模型，并利用这些模型来选择能够最大化学习者回报同时最小化教学成本的示范。我们在模拟环境中的实验证明，以这种方式教授的学习者比以学习者不可知的方式教授的学习者更加高效。当教师对学习者的模型与实际学习者的状态更加一致时，这种效果变得更加显著，其中一种方法是使用更准确的模型。

    Good teachers always tailor their explanations to the learners. Cognitive scientists model this process under the rationality principle: teachers try to maximise the learner's utility while minimising teaching costs. To this end, human teachers seem to build mental models of the learner's internal state, a capacity known as Theory of Mind (ToM). Inspired by cognitive science, we build on Bayesian ToM mechanisms to design teacher agents that, like humans, tailor their teaching strategies to the learners. Our ToM-equipped teachers construct models of learners' internal states from observations and leverage them to select demonstrations that maximise the learners' rewards while minimising teaching costs. Our experiments in simulated environments demonstrate that learners taught this way are more efficient than those taught in a learner-agnostic way. This effect gets stronger when the teacher's model of the learner better aligns with the actual learner's state, either using a more accurate
    
[^20]: 一种用于医学图像中一般移动目标分割的基础模型

    A Foundation Model for General Moving Object Segmentation in Medical Images. (arXiv:2309.17264v1 [cs.CV])

    [http://arxiv.org/abs/2309.17264](http://arxiv.org/abs/2309.17264)

    本文提出了一种用于医学图像中移动目标分割的基础模型iMOS，通过对序列中只有少量图像进行注释，即可实现高精度的分割效果

    

    医学图像分割旨在描绘感兴趣的解剖或病理结构，在临床诊断中起着关键作用。构建高精度的深度分割模型需要大量高质量的注释数据。然而，医学注释非常繁琐耗时，特别是对于医学视频或3D体积，由于巨大的标签空间和差的帧间一致性。最近，在自然图像中，一个名为Moving Object Segmentation (MOS)的基本任务在技术上取得了重大进展。它的目标是在图像序列中从背景中描绘移动物体，只需要最小的注释。在本文中，我们提出了第一个用于医学图像中MOS的基础模型，名为iMOS。对一个大规模多模态医学数据集进行的大量实验验证了所提出的iMOS的有效性。具体而言，只需对序列中少量的图像进行注释，iMOS就可以实现了

    Medical image segmentation aims to delineate the anatomical or pathological structures of interest, playing a crucial role in clinical diagnosis. A substantial amount of high-quality annotated data is crucial for constructing high-precision deep segmentation models. However, medical annotation is highly cumbersome and time-consuming, especially for medical videos or 3D volumes, due to the huge labeling space and poor inter-frame consistency. Recently, a fundamental task named Moving Object Segmentation (MOS) has made significant advancements in natural images. Its objective is to delineate moving objects from the background within image sequences, requiring only minimal annotations. In this paper, we propose the first foundation model, named iMOS, for MOS in medical images. Extensive experiments on a large multi-modal medical dataset validate the effectiveness of the proposed iMOS. Specifically, with the annotation of only a small number of images in the sequence, iMOS can achieve sati
    
[^21]: 分布式强化学习中的估计和推断

    Estimation and Inference in Distributional Reinforcement Learning. (arXiv:2309.17262v1 [stat.ML])

    [http://arxiv.org/abs/2309.17262](http://arxiv.org/abs/2309.17262)

    本文研究了分布式强化学习中的估计和推断问题，通过使用等价确定法，在提供生成模型的情况下以高效的方式解决了分布式策略评估问题。

    

    本文从统计效率的角度研究了分布式强化学习。我们研究了分布式策略评估，旨在估计由给定策略π获得的随机回报的完整分布（表示为η^π）。在提供生成模型的情况下，我们使用等价确定法构造了估计器η^π。我们证明，在这种情况下，通过具有大小为O(|S||A|/(ε^(2p)(1-γ)^(2p+2)))的数据集可以保证估计器η^π和真实分布η^π之间的p-Wasserstein距离小于ε的概率很高。这意味着分布式策略评估问题可以以高效利用样本的方式解决。此外，我们还证明，在不同的温和假设下，通过具有大小为O(|S||A|/(ε^2(1-γ)^4))的数据集就足以确保Kolmogorov距离和总变差。

    In this paper, we study distributional reinforcement learning from the perspective of statistical efficiency.  We investigate distributional policy evaluation, aiming to estimate the complete distribution of the random return (denoted $\eta^\pi$) attained by a given policy $\pi$.  We use the certainty-equivalence method to construct our estimator $\hat\eta^\pi$, given a generative model is available.  We show that in this circumstance we need a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2p}(1-\gamma)^{2p+2}}\right)$ to guarantee a $p$-Wasserstein metric between $\hat\eta^\pi$ and $\eta^\pi$ is less than $\epsilon$ with high probability.  This implies the distributional policy evaluation problem can be solved with sample efficiency.  Also, we show that under different mild assumptions a dataset of size $\widetilde O\left(\frac{|\mathcal{S}||\mathcal{A}|}{\epsilon^{2}(1-\gamma)^{4}}\right)$ suffices to ensure the Kolmogorov metric and total variation m
    
[^22]: PlaceNav: 通过地点识别进行拓扑导航

    PlaceNav: Topological Navigation through Place Recognition. (arXiv:2309.17260v1 [cs.RO])

    [http://arxiv.org/abs/2309.17260](http://arxiv.org/abs/2309.17260)

    PlaceNav是一种通过地点识别进行拓扑导航的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件，通过使用非机器人来源的大规模数据集增加训练数据的可用性，同时通过地点识别来提高导航性能。新模型的性能提高了76%。

    

    最近的研究结果表明，将拓扑导航分为机器人无关和机器人特定的组件可以提高导航性能，通过使用不同类型机器人收集的数据来训练机器人无关部分。然而，导航方法仍受到适合训练数据的稀缺性和计算缩放性差的限制。在本文中，我们提出了一个名为PlaceNav的方法，将机器人无关部分分为导航特定和通用的计算机视觉组件。我们利用视觉地点识别来选择拓扑导航流程中的子目标。这使得子目标选择更高效，并能够利用非机器人来源的大规模数据集增加训练数据的可用性。地点识别使得贝叶斯滤波成为可能，进一步通过增加子目标的时间一致性来提高导航性能。我们的实验结果验证了这一设计，并且新模型的性能提高了76%。

    Recent results suggest that splitting topological navigation into robot-independent and robot-specific components improves navigation performance by enabling the robot-independent part to be trained with data collected by different robot types. However, the navigation methods are still limited by the scarcity of suitable training data and suffer from poor computational scaling. In this work, we present~\methodname, subdividing the robot-independent part into navigation-specific and generic computer vision components. We utilize visual place recognition for the subgoal selection of the topological navigation pipeline. This makes subgoal selection more efficient and enables leveraging large-scale datasets from non-robotics sources, increasing training data availability. Bayes filtering, enabled by place recognition, further improves navigation performance by increasing the temporal consistency of subgoals. Our experimental results verify the design and the new model obtains a 76% higher 
    
[^23]: 批量校准：重新思考上下文学习和提示工程的校准方法

    Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])

    [http://arxiv.org/abs/2309.17249](http://arxiv.org/abs/2309.17249)

    本研究提出了一种名为批量校准（BC）的方法，用于解决大型语言模型中提示脆弱性和偏见因素导致的性能下降问题。BC通过控制批量输入的上下文偏见，统一了现有的校准方法，并具有零-shot和仅推理的特点。

    

    提示和上下文学习已成为大型语言模型（LLM）的高效学习范式。然而，LLM存在提示脆弱性和各种偏见因素，包括但不限于格式、选择性的表达方式和上下文学习示例。为解决这个导致性能下降的问题，已经开发了校准方法来减轻这些偏见的影响并恢复LLM的性能。在这项工作中，我们首先对现有的校准方法进行了系统分析，提供了统一的观点并揭示了失败案例。受这些分析的启发，我们提出了批量校准（BC），这是一种简单而直观的方法，可以从批量输入中控制上下文偏见，统一了各种先前的方法，并有效地解决了上述问题。BC是零-shot、仅推理和额外成本可忽略。在少-shot设置中，我们进一步扩展BC以实现全部翻译

    Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
    
[^24]: 数据驱动的局域波和通过扩展物理信息神经网络在大Thirring模型中的参数发现

    Data-driven localized waves and parameter discovery in the massive Thirring model via extended physics-informed neural networks with interface zones. (arXiv:2309.17240v1 [nlin.PS])

    [http://arxiv.org/abs/2309.17240](http://arxiv.org/abs/2309.17240)

    本文研究了通过基于物理信息神经网络（PINNs）算法的深度学习方法，在大Thirring模型中实现数据驱动的局域波解和参数发现。通过模拟和分析，得到了各种类型的局域波解，包括孤子、呼吸子和流氓波。通过扩展PINNs和域分解方法，成功捕捉了高阶局域波解的动态行为。特别地，通过修改界面线和引入界面条件，有效地应用于不同类型的解。

    

    本文使用基于物理信息神经网络（PINNs）算法的深度学习方法研究了数据驱动的局域波解和在大Thirring模型中的参数发现。通过精确模拟和相对误差、绝对误差的对比分析，得到了丰富的数据驱动解，包括明亮/暗淡型孤子，呼吸子和流氓波。对于高阶局域波解，我们采用扩展PINNs（XPINNs）和域分解的方法来捕捉动态行为的完整图像，如孤子碰撞，呼吸子振荡和流氓波叠加。特别地，我们将XPINNs中的界面线修改为一个小的界面区域，并引入伪初值、残差和梯度条件作为与各个神经网络相邻的界面条件。然后我们成功地将这种修改的方法应用于各种解，包括明亮-明亮的孤子，暗淡-暗淡的孤子，和暗淡-明亮的孤子等。

    In this paper, we study data-driven localized wave solutions and parameter discovery in the massive Thirring (MT) model via the deep learning in the framework of physics-informed neural networks (PINNs) algorithm. Abundant data-driven solutions including soliton of bright/dark type, breather and rogue wave are simulated accurately and analyzed contrastively with relative and absolute errors. For higher-order localized wave solutions, we employ the extended PINNs (XPINNs) with domain decomposition to capture the complete pictures of dynamic behaviors such as soliton collisions, breather oscillations and rogue-wave superposition. In particular, we modify the interface line in domain decomposition of XPINNs into a small interface zone and introduce the pseudo initial, residual and gradient conditions as interface conditions linked adjacently with individual neural networks. Then this modified approach is applied successfully to various solutions ranging from bright-bright soliton, dark-da
    
[^25]: LLM-辩论: 使用交互式多智能体协商游戏评估LLMs

    LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. (arXiv:2309.17234v1 [cs.CL])

    [http://arxiv.org/abs/2309.17234](http://arxiv.org/abs/2309.17234)

    本文提出使用可评分的谈判游戏作为LLMs的新评估框架，创建了一个多样的测试平台，并通过系统化的零-shot思维链提示（CoT）展示了代理人可以成功谈判。该研究揭示了GPT-4在该任务上的性能差距。

    

    越来越多的人对使用大型语言模型（LLMs）作为代理人来解决可能需要评估复杂情况的现实任务感兴趣。然而，我们对LLMs的推理和决策能力有限的理解，在某种程度上是由于缺乏专门的评估基准。由于谈判和妥协是我们日常沟通和合作的关键方面，我们提出使用可评分的谈判游戏作为LLMs的新评估框架。我们创建了一个多样的基于文本的、多智能体的、多问题的、语义丰富的谈判游戏测试平台，难度可调。为了解决这一挑战，代理人需要具备强大的算术、推理、探索和规划能力，同时无缝地整合它们。通过系统化的零-shot思维链提示（CoT），我们展示了代理人可以进行谈判并持续达成成功交易。我们用多个指标量化性能，并观察到GPT-4与原文之间存在很大差距。

    There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and 
    
[^26]: 伪特征多样性改善了对分布外泛化的效果

    Spurious Feature Diversification Improves Out-of-distribution Generalization. (arXiv:2309.17230v1 [cs.LG])

    [http://arxiv.org/abs/2309.17230](http://arxiv.org/abs/2309.17230)

    本文研究了基于权重空间集成方法WiSE-FT在分布外泛化中的有效性，发现其成功纠正了许多个体模型的错误预测，并通过利用更多多样化的伪特征减少了分布外设置中的预测错误。

    

    在机器学习中，对分布外（OOD）数据的泛化是一个关键性挑战。基于集成的方法，如在模型参数上进行插值的权重空间集成，已被证明在OOD性能方面具有优势。然而，它们的有效性的基本机制仍不清楚。在本研究中，我们对一种常用的权重空间集成方法WiSE-FT进行了详细研究，该方法在预训练模型和微调模型之间进行插值。我们观察到一个意外的现象，即WiSE-FT成功地纠正了许多个体模型做出错误预测的情况，这对于其OOD的有效性贡献重大。为了进一步了解，我们在具有大量伪特征的多类别设置中进行了理论分析。我们的分析预测了上述现象，并进一步表明，基于集成的模型通过利用更多多样化的伪特征，减少了OOD设置中的预测错误。与传统观点相反。

    Generalization to out-of-distribution (OOD) data is a critical challenge in machine learning. Ensemble-based methods, like weight space ensembles that interpolate model parameters, have been shown to achieve superior OOD performance. However, the underlying mechanism for their effectiveness remains unclear. In this study, we closely examine WiSE-FT, a popular weight space ensemble method that interpolates between a pre-trained and a fine-tuned model. We observe an unexpected phenomenon, in which WiSE-FT successfully corrects many cases where each individual model makes incorrect predictions, which contributes significantly to its OOD effectiveness. To gain further insights, we conduct theoretical analysis in a multi-class setting with a large number of spurious features. Our analysis predicts the above phenomenon and it further shows that ensemble-based models reduce prediction errors in the OOD settings by utilizing a more diverse set of spurious features. Contrary to the conventional
    
[^27]: MORPH：通过可微分的硬件模型代理使用强化学习进行设计协同优化

    MORPH: Design Co-optimization with Reinforcement Learning via a Differentiable Hardware Model Proxy. (arXiv:2309.17227v1 [cs.RO])

    [http://arxiv.org/abs/2309.17227](http://arxiv.org/abs/2309.17227)

    MORPH是一种使用强化学习进行硬件设计参数和控制策略的协同优化的方法。通过引入可微分的硬件模型代理，MORPH能够实现有效的优化并保持优化后的硬件代理接近其真实对应物。

    

    我们介绍了一种名为MORPH的方法，用于使用强化学习在仿真中进行硬件设计参数和控制策略的协同优化。与大多数协同优化方法类似，MORPH依赖于正在优化的硬件模型，通常是基于物理定律进行模拟。然而，这样的模型往往难以集成到有效的优化例程中。为了解决这个问题，我们引入了一个代理硬件模型，它始终是可微分的，并且能够使用RL和长期控制策略有效地进行协同优化。MORPH的设计目标是确保优化后的硬件代理与其真实对应物尽可能接近，同时仍然能够完成任务。我们在仿真的2D伸手和3D多指操纵任务上演示了我们的方法。

    We introduce MORPH, a method for co-optimization of hardware design parameters and control policies in simulation using reinforcement learning. Like most co-optimization methods, MORPH relies on a model of the hardware being optimized, usually simulated based on the laws of physics. However, such a model is often difficult to integrate into an effective optimization routine. To address this, we introduce a proxy hardware model, which is always differentiable and enables efficient co-optimization alongside a long-horizon control policy using RL. MORPH is designed to ensure that the optimized hardware proxy remains as close as possible to its realistic counterpart, while still enabling task completion. We demonstrate our approach on simulated 2D reaching and 3D multi-fingered manipulation tasks.
    
[^28]: 使用8位浮点数训练和推断大型语言模型

    Training and inference of large language models using 8-bit floating point. (arXiv:2309.17224v1 [cs.LG])

    [http://arxiv.org/abs/2309.17224](http://arxiv.org/abs/2309.17224)

    本文介绍了一种用于选择FP8线性层的缩放方法，该方法基于动态更新每个张量的权重、梯度和激活的尺度。通过将这种方法应用于大型语言模型的训练和验证，我们证明了其在提高计算效率方面的有效性。

    

    FP8格式正在受到青睐，以提高训练和推断大型深度学习模型的计算效率。它们的主要挑战是需要谨慎选择缩放以防止由于较高精度格式的动态范围的减少而导致性能下降。尽管关于选择INT格式的这些缩放因子的文献很多，但对于FP8来说，这一关键方面尚未得到解决。本文提出了一种基于动态更新权重、梯度和激活的每个张量尺度的FP8线性层缩放选择方法。我们将这种方法应用于使用FP8训练和验证GPT和Llama 2等类型的大型语言模型，模型大小范围从111M到70B不等。为了便于理解FP8的动态特性，在训练和推断过程中，我们的结果附带了权重、激活和梯度的每个张量尺度分布的图示。

    FP8 formats are gaining popularity to boost the computational efficiency for training and inference of large deep learning models. Their main challenge is that a careful choice of scaling is needed to prevent degradation due to the reduced dynamic range compared to higher-precision formats. Although there exists ample literature about selecting such scalings for INT formats, this critical aspect has yet to be addressed for FP8. This paper presents a methodology to select the scalings for FP8 linear layers, based on dynamically updating per-tensor scales for the weights, gradients and activations. We apply this methodology to train and validate large language models of the type of GPT and Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate the understanding of the FP8 dynamics, our results are accompanied by plots of the per-tensor scale distribution for weights, activations and gradients during both training and inference.
    
[^29]: RSAM：使用Riemannian Sharpness-aware Minimization在流形上学习

    RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization. (arXiv:2309.17215v1 [cs.LG])

    [http://arxiv.org/abs/2309.17215](http://arxiv.org/abs/2309.17215)

    RSAM是一种在流形上学习的算法，通过将Sharpness-Aware Minimization (SAM)推广到Riemannian流形，引入了流形上sharpness的概念，并通过理论分析证明了其与泛化能力的关系。通过该算法的应用，我们展示了其在提升泛化能力方面的有效性。

    

    如今，了解损失函数空间的几何结构有望提升模型的泛化能力。在这项工作中，我们借鉴了之前将几何原理应用于优化的研究，并提出了一种改进受限制优化问题鲁棒性和泛化能力的新方法。事实上，本文旨在将Sharpness-Aware Minimization (SAM)优化器推广到Riemannian流形。为了支持流形上的“sharpness”概念，我们首先对流形上的sharpness引入了一种新的定义。为了证明这个概念的有效性，我们提出了一个理论分析来描述流形sharpness与泛化能力之间的关系，并呈现了一个更紧密的泛化缺口上限，这是之前未知的结果。受到这个分析的启发，我们提出了我们的算法，Riemannian Sharpness-Aware Minimization (RSAM)。为了展示RSAM在提升泛化能力方面的能力，我们在一个约束优化问题上评估和对比了我们的算法。

    Nowadays, understanding the geometry of the loss landscape shows promise in enhancing a model's generalization ability. In this work, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization ability for constrained optimization problems. Indeed, this paper aims to generalize the Sharpness-Aware Minimization (SAM) optimizer to Riemannian manifolds. In doing so, we first extend the concept of sharpness and introduce a novel notion of sharpness on manifolds. To support this notion of sharpness, we present a theoretical analysis characterizing generalization capabilities with respect to manifold sharpness, which demonstrates a tighter bound on the generalization gap, a result not known before. Motivated by this analysis, we introduce our algorithm, Riemannian Sharpness-Aware Minimization (RSAM). To demonstrate RSAM's ability to enhance generalization ability, we evaluate and contrast our algorithm on a br
    
[^30]: 使用局部敏感哈希在CNN中实现即时复杂度降低

    Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing. (arXiv:2309.17211v1 [cs.CV])

    [http://arxiv.org/abs/2309.17211](http://arxiv.org/abs/2309.17211)

    该论文提出了一个名为HASTE的模块，通过使用局部敏感哈希技术，无需任何训练或精调即可实时降低卷积神经网络的计算成本，并且在压缩特征图时几乎不损失准确性。

    

    为了在资源受限的设备上降低卷积神经网络（CNN）的计算成本，结构化剪枝方法已显示出有希望的结果，在不太大程度降低准确性的情况下大大减少了浮点运算（FLOPs）。然而，大多数最新的方法要求进行精调或特定的训练过程，以实现在保留准确性和降低FLOPs之间合理折衷。这引入了计算开销的额外成本，并需要可用的训练数据。为此，我们提出了HASTE（Hashing for Tractable Efficiency），它是一个无需参数和无需数据的模块，可以作为任何常规卷积模块的即插即用替代品。它能够在不需要任何训练或精调的情况下即时降低网络的测试推理成本。通过使用局部敏感哈希（LSH）来检测特征图中的冗余，我们能够大幅压缩潜在特征图而几乎不损失准确性。

    To reduce the computational cost of convolutional neural networks (CNNs) for usage on resource-constrained devices, structured pruning approaches have shown promising results, drastically reducing floating-point operations (FLOPs) without substantial drops in accuracy. However, most recent methods require fine-tuning or specific training procedures to achieve a reasonable trade-off between retained accuracy and reduction in FLOPs. This introduces additional cost in the form of computational overhead and requires training data to be available. To this end, we propose HASTE (Hashing for Tractable Efficiency), a parameter-free and data-free module that acts as a plug-and-play replacement for any regular convolution module. It instantly reduces the network's test-time inference cost without requiring any training or fine-tuning. We are able to drastically compress latent feature maps without sacrificing much accuracy by using locality-sensitive hashing (LSH) to detect redundancies in the c
    
[^31]: 能够看见的机器人：利用人体姿势进行轨迹预测

    Robots That Can See: Leveraging Human Pose for Trajectory Prediction. (arXiv:2309.17209v1 [cs.RO])

    [http://arxiv.org/abs/2309.17209](http://arxiv.org/abs/2309.17209)

    该论文提出了一种利用Transformer架构预测人类未来轨迹的方法，可以应用于以人为中心的动态环境。模型在常见的预测基准和移动机器人捕获的数据集上表现出色，并解决了历史数据有限的新代理导致的误差问题。

    

    在诸如家庭和办公室等动态环境中，预测所有人类的运动对于实现安全有效的机器人导航至关重要。这样的空间仍然具有挑战性，因为人类不遵循严格的运动规则，而且通常存在多个被遮挡的入口点，如拐角和门，在这些地方容易发生突然相遇的情况。在这项工作中，我们提出了一种基于Transformer的架构，从包括人类位置、头部方向和三维骨骼关键点在内的输入特征中，预测人类在以人为中心的环境中的未来轨迹。所得模型捕捉到了未来人类轨迹预测的固有不确定性，并在常见的预测基准和适用于预测任务的移动机器人捕获的人类跟踪数据集上实现了最先进的性能。此外，我们确定了历史数据有限的新代理是错误的主要贡献因素，并展示了补充解决方案。

    Anticipating the motion of all humans in dynamic environments such as homes and offices is critical to enable safe and effective robot navigation. Such spaces remain challenging as humans do not follow strict rules of motion and there are often multiple occluded entry points such as corners and doors that create opportunities for sudden encounters. In this work, we present a Transformer based architecture to predict human future trajectories in human-centric environments from input features including human positions, head orientations, and 3D skeletal keypoints from onboard in-the-wild sensory information. The resulting model captures the inherent uncertainty for future human trajectory prediction and achieves state-of-the-art performance on common prediction benchmarks and a human tracking dataset captured from a mobile robot adapted for the prediction task. Furthermore, we identify new agents with limited historical data as a major contributor to error and demonstrate the complementa
    
[^32]: 记忆健身房：对内存为基础的智能体在无尽任务中的部分可观察挑战

    Memory Gym: Partially Observable Challenges to Memory-Based Agents in Endless Episodes. (arXiv:2309.17207v1 [cs.LG])

    [http://arxiv.org/abs/2309.17207](http://arxiv.org/abs/2309.17207)

    本研究提出了记忆健身房，一种用于测试利用记忆为基础的深度强化学习智能体能力的基准。它包括部分可观察的二维环境和离散控制，并通过无尽任务对记忆能力、噪声抗性和泛化能力进行评估。研究还提供了一个使用Transformer-XL和Proximal Policy Optimization驱动的实现。

    

    记忆健身房介绍了一个独特的基准测试，旨在测试深度强化学习智能体，特别是将门循环单元(GRU)与Transformer-XL(TrXL)相比，它们对于记忆长序列的能力、抗噪声和泛化能力。它采用了部分可观察的二维环境和离散控制，即Mortar Mayhem、Mystery Path和Searing Spotlights。这些最初是有限的环境被推广为新颖的无尽任务，作为一种自动课程，从车游戏"I packed my bag"中汲取灵感。这些无尽任务不仅有助于评估效率，而且有趣地评估了记忆为基础的方法的有效性。鉴于现有公开可用的记忆基准的稀缺性，我们提供了一个由TrXL和Proximal Policy Optimization驱动的实现。本实现利用TrXL作为以滑动窗口方法使用的情节性记忆。在有限环境的实验中，我们发现...

    Memory Gym introduces a unique benchmark designed to test Deep Reinforcement Learning agents, specifically comparing Gated Recurrent Unit (GRU) against Transformer-XL (TrXL), on their ability to memorize long sequences, withstand noise, and generalize. It features partially observable 2D environments with discrete controls, namely Mortar Mayhem, Mystery Path, and Searing Spotlights. These originally finite environments are extrapolated to novel endless tasks that act as an automatic curriculum, drawing inspiration from the car game ``I packed my bag". These endless tasks are not only beneficial for evaluating efficiency but also intriguingly valuable for assessing the effectiveness of approaches in memory-based agents. Given the scarcity of publicly available memory baselines, we contribute an implementation driven by TrXL and Proximal Policy Optimization. This implementation leverages TrXL as episodic memory using a sliding window approach. In our experiments on the finite environment
    
[^33]: ComSD: 在无监督技能发现中平衡行为质量和多样性

    ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])

    [http://arxiv.org/abs/2309.17203](http://arxiv.org/abs/2309.17203)

    ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。

    

    无监督技能发现的理想方法能够在没有外部奖励的情况下产生多样且合格的技能，同时发现的技能集能够以各种方式高效地适应下游任务。本文提出了Contrastive multi-objectives Skill Discovery (ComSD)，通过更合理的互信息估计和动态加权的内在奖励来减轻发现的行为在质量和多样性之间的冲突。

    Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
    
[^34]: 基于乳腺DCE-MRI特征的随机森林模型中的种族偏见调查

    An Investigation Into Race Bias in Random Forest Models Based on Breast DCE-MRI Derived Radiomics Features. (arXiv:2309.17197v1 [cs.LG])

    [http://arxiv.org/abs/2309.17197](http://arxiv.org/abs/2309.17197)

    本文通过研究使用放射组学特征训练的随机森林模型中的种族偏见，发现从DCE-MRI数据中提取的放射组学特征包含种族可辨识的信息。基于这些数据训练的模型能以60-70%的准确率预测白人和黑人种族，且基于种族不平衡数据的训练会导致模型产生偏见行为。

    

    最近的研究表明，人工智能（AI）模型在使用受保护属性不平衡的数据进行训练时可能会表现出偏见。大部分现有的工作都集中在深度学习模型上，但是利用手工特征的经典人工智能技术也可能受到这种偏见的影响。在本文中，我们研究了使用放射组学特征训练的随机森林（RF）模型中可能存在的种族偏见。我们的应用是利用乳腺癌患者的动态增强磁共振成像（DCE-MRI）预测肿瘤分子亚型。我们的研究结果表明，从DCE-MRI数据中提取的放射组学特征确实包含种族可辨识的信息，并且基于这些数据训练的RF模型可以以60-70％的准确率预测白人和黑人种族，具体取决于所使用的特征子集。此外，基于种族不平衡数据训练的RF模型似乎会产生偏见行为，表现出某种程度的种族偏见。

    Recent research has shown that artificial intelligence (AI) models can exhibit bias in performance when trained using data that are imbalanced by protected attribute(s). Most work to date has focused on deep learning models, but classical AI techniques that make use of hand-crafted features may also be susceptible to such bias. In this paper we investigate the potential for race bias in random forest (RF) models trained using radiomics features. Our application is prediction of tumour molecular subtype from dynamic contrast enhanced magnetic resonance imaging (DCE-MRI) of breast cancer patients. Our results show that radiomics features derived from DCE-MRI data do contain race-identifiable information, and that RF models can be trained to predict White and Black race from these data with 60-70% accuracy, depending on the subset of features used. Furthermore, RF models trained to predict tumour molecular subtype using race-imbalanced data seem to produce biased behaviour, exhibiting bet
    
[^35]: ResBit: 基于残差位向量的离散值表示方法

    ResBit: Residual Bit Vector for Categorical Values. (arXiv:2309.17196v1 [cs.LG])

    [http://arxiv.org/abs/2309.17196](http://arxiv.org/abs/2309.17196)

    本论文提出了一种名为ResBit的残差位向量方法，用于解决在深度学习中表示离散数据维度增加和无法恢复原始类别值的问题。

    

    长期以来，独热编码向量一直广泛应用于机器学习中，作为一种简单且通用的表示离散数据的方法。然而，这种方法会导致维度随着要表示的离散数据线性增加，这在深度学习中视为空间计算复杂性的问题，而深度学习需要大量的数据。最近，基于扩散模型的高表达能力，提出了一种用位序列表示离散数据的方法，即Analog Bits。然而，由于在生成任务中要表示的类别类型数量不一定是2的幂次，导致Analog Bits能够表示的范围与类别数据的范围存在差异。如果生成了这样的值，问题就是无法恢复原始的类别值。为了解决这个问题，我们提出了残差位向量（ResBit），它是一种分层的位表示方法。

    The one-hot vector has long been widely used in machine learning as a simple and generic method for representing discrete data. However, this method increases the number of dimensions linearly with the categorical data to be represented, which is problematic from the viewpoint of spatial computational complexity in deep learning, which requires a large amount of data. Recently, Analog Bits, a method for representing discrete data as a sequence of bits, was proposed on the basis of the high expressiveness of diffusion models. However, since the number of category types to be represented in a generation task is not necessarily at a power of two, there is a discrepancy between the range that Analog Bits can represent and the range represented as category data. If such a value is generated, the problem is that the original category value cannot be restored. To address this issue, we propose Residual Bit Vector (ResBit), which is a hierarchical bit representation. Although it is a general-p
    
[^36]: 通过多变量投影进行广义激活

    Generalized Activation via Multivariate Projection. (arXiv:2309.17194v1 [cs.LG])

    [http://arxiv.org/abs/2309.17194](http://arxiv.org/abs/2309.17194)

    通过将ReLU视为从R投影到非负半线R+的操作，我们将其通过用凸锥的广义投影算子替代，扩展为具有多个输入和多个输出的多变量投影单元 (MPU)激活函数，并证明其在表达能力方面优于ReLU激活的FNN。

    

    激活函数对于引入神经网络的非线性起着至关重要的作用，Rectified Linear Unit (ReLU)常因其简单和有效而受青睐。受浅层前向神经网络 (FNN) 和单次投影梯度下降 (PGD) 算法之间结构相似性的启发，我们将ReLU视为从R投影到非负半线R+的操作。在这个解释基础上，我们通过用凸锥的广义投影算子替代ReLU，如二阶锥 (SOC) 投影，从而将其自然地扩展为多变量投影单元 (MPU)，这是具有多个输入和多个输出的激活函数。我们进一步提供了数学证明，证明了使用SOC投影激活的FNN在表达能力方面优于使用ReLU的FNN。通过对广泛采用的架构进行实验评估

    Activation functions are essential to introduce nonlinearity into neural networks, with the Rectified Linear Unit (ReLU) often favored for its simplicity and effectiveness. Motivated by the structural similarity between a shallow Feedforward Neural Network (FNN) and a single iteration of the Projected Gradient Descent (PGD) algorithm, a standard approach for solving constrained optimization problems, we consider ReLU as a projection from R onto the nonnegative half-line R+. Building on this interpretation, we extend ReLU by substituting it with a generalized projection operator onto a convex cone, such as the Second-Order Cone (SOC) projection, thereby naturally extending it to a Multivariate Projection Unit (MPU), an activation function with multiple inputs and multiple outputs. We further provide a mathematical proof establishing that FNNs activated by SOC projections outperform those utilizing ReLU in terms of expressive power. Experimental evaluations on widely-adopted architecture
    
[^37]: 增量迁移学习调查: 将点对点联邦学习与领域增量学习相结合用于多中心协作

    A Survey of Incremental Transfer Learning: Combining Peer-to-Peer Federated Learning and Domain Incremental Learning for Multicenter Collaboration. (arXiv:2309.17192v1 [cs.LG])

    [http://arxiv.org/abs/2309.17192](http://arxiv.org/abs/2309.17192)

    该调查研究了增量迁移学习方法，通过结合点对点联邦学习和领域增量学习，克服了数据隐私限制，并使用连续学习技术保持模型性能。调查还探讨了在多中心协作中，不同正则化方法对增量迁移学习的有效性的影响。

    

    由于数据隐私限制，多个临床中心之间的数据共享受到限制，这阻碍了从多中心协作中开发高性能深度学习模型的发展。朴素的权重转移方法在没有原始数据的情况下分享中间模型权重，因此可以绕过数据隐私限制。然而，通常在模型从一个中心转移到下一个中心时会观察到性能下降，这是由于遗忘问题。增量迁移学习通过使用连续学习技术，结合点对点联邦学习和领域增量学习，可以克服数据隐私问题，并同时保持模型性能。在这项工作中，将传统的领域/任务增量学习框架用于增量迁移学习。对于多中心协作，还进行了对不同基于正则化的连续学习方法的有效性进行全面调查。数据异质性，类别不平衡和领域知识共享对增量迁移学习的影响

    Due to data privacy constraints, data sharing among multiple clinical centers is restricted, which impedes the development of high performance deep learning models from multicenter collaboration. Naive weight transfer methods share intermediate model weights without raw data and hence can bypass data privacy restrictions. However, performance drops are typically observed when the model is transferred from one center to the next because of the forgetting problem. Incremental transfer learning, which combines peer-to-peer federated learning and domain incremental learning, can overcome the data privacy issue and meanwhile preserve model performance by using continual learning techniques. In this work, a conventional domain/task incremental learning framework is adapted for incremental transfer learning. A comprehensive survey on the efficacy of different regularization-based continual learning methods for multicenter collaboration is performed. The influences of data heterogeneity, class
    
[^38]: RECOMBINER：鲁棒性和增强的贝叶斯隐式神经表示的压缩方法

    RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations. (arXiv:2309.17182v1 [cs.LG])

    [http://arxiv.org/abs/2309.17182](http://arxiv.org/abs/2309.17182)

    RECOMBINER是一种鲁棒性和增强的贝叶斯隐式神经表示的压缩方法，通过丰富变分逼近、增加位置编码和分割高分辨率数据来解决COMBINER存在的局限性。

    

    COMBINER是一种最近提出的数据压缩方法，它解决了先前基于隐式神经表示的方法存在的关键效率问题：避免了量化，并实现了对速率-失真性能的直接优化。然而，COMBINER仍然存在明显的局限性：1）使用因式化的先验和后验逼近，缺乏灵活性；2）不能有效地适应数据中的局部偏离全局模式；3）其性能易受建模选择和变分参数初始化的影响。我们提出的方法，鲁棒和增强的COMBINER(RECOMBINER)，通过以下方式解决了这些问题：1）通过对INR权重进行线性参数化，丰富变分逼近，并保持计算成本；2）通过增加可学习的位置编码来增强我们的INR，使其适应局部细节；3）将高分辨率数据分割成...

    COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while maintaining its computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into pa
    
[^39]: Alphazero类似的树搜索可以指导大型语言模型的解码和训练

    Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])

    [http://arxiv.org/abs/2309.17179](http://arxiv.org/abs/2309.17179)

    Alphazero类似的树搜索框架TS-LLM可以利用学习的价值函数指导大型语言模型的解码和训练，不仅适用于推理任务，还适用于其他任务，并且在不同大小的语言模型上具有普适性和可扩展性

    

    大型语言模型 (LLM) 通常采用采样或束搜索，结合 Chain-of-Thought (CoT) 等提示来提高推理和解码能力。最近的研究如 Tree-of-Thought (ToT) 和 Reasoning via Planning (RAP) 旨在通过利用树搜索算法来引导多步推理，来增强LLM的推理能力。这些方法主要关注LLM在推理过程中的推理能力，并且严重依赖人为设计的提示来激活LLM作为一个价值函数，缺乏普适性和可扩展性。为了解决这些限制，我们提出了一种AlphaZero类似的用于LLM的树搜索框架 (称为TS-LLM)，系统地说明了如何通过学习的价值函数利用树搜索来指导LLM的解码能力。TS-LLM在两个关键方面与众不同：(1)通过利用学习的价值函数，我们的方法可以普适地应用于除了推理之外的不同任务 (例如RLHF对齐)，以及任何大小的LLM，而不需要提示

    Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
    
[^40]: FedZeN:通过增量Hessian估计实现超线性零阶联邦学习

    FedZeN: Towards superlinear zeroth-order federated learning via incremental Hessian estimation. (arXiv:2309.17174v1 [cs.LG])

    [http://arxiv.org/abs/2309.17174](http://arxiv.org/abs/2309.17174)

    本文提出了FedZeN，一种使用增量Hessian估计的超线性零阶联邦学习算法。这种算法可以在不共享原始数据样本的情况下，通过采用Stiefel流形中的随机搜索方向，估计全局目标的曲率，并实现超线性收敛。

    

    联邦学习是一种分布式学习框架，允许一组客户端在中央服务器的编排下协同训练模型，而不共享原始数据样本。尽管在许多实际场景中，目标函数的导数是不可用的，但只有少数研究考虑了联邦零阶设置，在此设置中，只能通过预算数量的点评估来访问函数。在这项工作中，我们着重于凸优化，并设计了第一个联邦零阶算法来估计全局目标的曲率，以实现超线性收敛。我们采用了一种误差范数线性收敛的增量Hessian估计器，并将其适应于联邦零阶设置，从Stiefel流形中随机采样搜索方向以提高性能。特别地，梯度和Hessian估计器都在中央服务器上以通信效率高且省资源的方式构建。

    Federated learning is a distributed learning framework that allows a set of clients to collaboratively train a model under the orchestration of a central server, without sharing raw data samples. Although in many practical scenarios the derivatives of the objective function are not available, only few works have considered the federated zeroth-order setting, in which functions can only be accessed through a budgeted number of point evaluations. In this work we focus on convex optimization and design the first federated zeroth-order algorithm to estimate the curvature of the global objective, with the purpose of achieving superlinear convergence. We take an incremental Hessian estimator whose error norm converges linearly, and we adapt it to the federated zeroth-order setting, sampling the random search directions from the Stiefel manifold for improved performance. In particular, both the gradient and Hessian estimators are built at the central server in a communication-efficient and pr
    
[^41]: 龙与地下城领域中命名实体识别的比较分析

    Comparative Analysis of Named Entity Recognition in the Dungeons and Dragons Domain. (arXiv:2309.17171v1 [cs.CL])

    [http://arxiv.org/abs/2309.17171](http://arxiv.org/abs/2309.17171)

    我们比较分析了领域特定的命名实体识别在龙与地下城领域的表现，发现在没有进行任何修改的情况下，Flair、Trankit和Spacy在龙与地下城背景中的命名实体识别方面表现最好。

    

    许多自然语言处理任务，在通用英语中表现良好，但在奇幻文学等特定领域面临挑战。这在命名实体识别(NER)中尤为明显，NER可以检测和分类文本中的实体。我们分析了10个NER模型在7本龙与地下城(D&D)冒险书上的表现，以评估特定领域的性能。使用开源的大型语言模型，我们对这些书籍中的命名实体进行了标注，并评估了每个模型的准确率。我们的研究结果表明，在不做修改的情况下，Flair、Trankit和Spacy在识别龙与地下城背景中的命名实体方面优于其他模型。

    Many NLP tasks, although well-resolved for general English, face challenges in specific domains like fantasy literature. This is evident in Named Entity Recognition (NER), which detects and categorizes entities in text. We analyzed 10 NER models on 7 Dungeons and Dragons (D&D) adventure books to assess domain-specific performance. Using open-source Large Language Models, we annotated named entities in these books and evaluated each model's precision. Our findings indicate that, without modifications, Flair, Trankit, and Spacy outperform others in identifying named entities in the D&D context.
    
[^42]: DyVal: 基于图形信息的大型语言模型动态评估

    DyVal: Graph-informed Dynamic Evaluation of Large Language Models. (arXiv:2309.17167v1 [cs.AI])

    [http://arxiv.org/abs/2309.17167](http://arxiv.org/abs/2309.17167)

    DyVal是一种基于图形信息的大型语言模型动态评估协议，通过动态生成具有可控复杂性的评估样本，评估了各种LLM在推理任务上的性能，发现它们在这些挑战性样本上表现更差。

    

    大型语言模型在各种评估基准中取得了显著的性能。然而，人们对它们的性能提出了担忧，因为它们庞大的训练语料库中可能存在数据污染。此外，当前基准的静态性质和固定复杂性可能无法充分衡量LLM的进步能力。在本文中，我们介绍了DyVal，一种新颖、通用且灵活的动态评估LLM的协议。基于我们提出的动态评估框架，我们利用有向无环图的结构优势构建了基于图形信息的DyVal，以动态生成具有可控复杂性的评估样本。DyVal生成了包括数学、逻辑推理和算法问题在内的具有挑战性的推理任务的评估集。我们评估了从Flan-T5-large到ChatGPT和GPT4的各种LLM。实验表明，LLM在DyVal生成的评估样本上表现更差

    Large language models (LLMs) have achieved remarkable performance in various evaluation benchmarks. However, concerns about their performance are raised on potential data contamination in their considerable volume of training corpus. Moreover, the static nature and fixed complexity of current benchmarks may inadequately gauge the advancing capabilities of LLMs. In this paper, we introduce DyVal, a novel, general, and flexible evaluation protocol for dynamic evaluation of LLMs. Based on our proposed dynamic evaluation framework, we build graph-informed DyVal by leveraging the structural advantage of directed acyclic graphs to dynamically generate evaluation samples with controllable complexities. DyVal generates challenging evaluation sets on reasoning tasks including mathematics, logical reasoning, and algorithm problems. We evaluate various LLMs ranging from Flan-T5-large to ChatGPT and GPT4. Experiments demonstrate that LLMs perform worse in DyVal-generated evaluation samples with di
    
[^43]: 当前的药物属性预测方法在实际应用中的应用

    Current Methods for Drug Property Prediction in the Real World. (arXiv:2309.17161v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.17161](http://arxiv.org/abs/2309.17161)

    该论文介绍了当前药物属性预测方法的实际应用情况，并强调了不同数据集和方法之间的关联。研究发现最好的预测方法取决于数据集，使用经过工程处理的特征与经典方法结合的效果较好。

    

    在药物发现中，预测药物属性对于在昂贵的临床试验之前降低风险、快速找到高活性化合物是至关重要的。机器学习社区的兴趣导致了多种基准数据集和提出的方法的发布。然而，对于从业者来说，不同文件在不同数据集和方法上进行基准测试，得出的结论也不易于比较，因此仍然不清楚哪种方法或方法最合适。我们的大规模实证研究将之前的许多不同数据集和方法的研究链接在一起，从而提供了现有属性类别、数据集及其与不同方法的相互作用的综合概述。我们强调不确定性量化的重要性，以及应用这些方法在药物开发决策周期中的时间和成本问题。我们发现最好的方法取决于数据集，而使用经过工程处理的特征与经典方法结合的效果较好。

    Predicting drug properties is key in drug discovery to enable de-risking of assets before expensive clinical trials, and to find highly active compounds faster. Interest from the Machine Learning community has led to the release of a variety of benchmark datasets and proposed methods. However, it remains unclear for practitioners which method or approach is most suitable, as different papers benchmark on different datasets and methods, leading to varying conclusions that are not easily compared. Our large-scale empirical study links together numerous earlier works on different datasets and methods; thus offering a comprehensive overview of the existing property classes, datasets, and their interactions with different methods. We emphasise the importance of uncertainty quantification and the time and therefore cost of applying these methods in the drug development decision-making cycle. We discover that the best method depends on the dataset, and that engineered features with classical 
    
[^44]: 通过自由手写指标进行年龄群体歧视

    Age Group Discrimination via Free Handwriting Indicators. (arXiv:2309.17156v1 [cs.LG])

    [http://arxiv.org/abs/2309.17156](http://arxiv.org/abs/2309.17156)

    这项研究通过分析手写数据和计算相关指标，提出了一种使用仪器化墨水笔对老龄人群进行年龄分类的创新方法。

    

    随着全球老龄人口的增长，老弱群体的患病率预计将增加，给医疗系统带来重大挑战。老弱是一种与衰老相关的综合征，其特征是健康逐渐下降，对压力的脆弱性增加，死亡风险增加。老弱对公共卫生造成了重大负担，降低了患者生活质量。目前缺乏一种普遍接受的评估老弱程度的方法和标准化定义，这突显了一个关键的研究缺口。鉴于这一缺口和早期预防的重要性，本研究提出了一种创新方法，使用仪器化墨水笔对手写进行生态学评估，以对不同年龄群体进行分类。分析了来自80名不同年龄组（20-40岁、41-60岁、61-70岁和70+岁）的健康参与者的非图像手写数据。从原始数据中计算了14个与手势和震颤相关的指标，并在五个分类任务中使用了这些指标。

    The growing global elderly population is expected to increase the prevalence of frailty, posing significant challenges to healthcare systems. Frailty, a syndrome associated with ageing, is characterised by progressive health decline, increased vulnerability to stressors and increased risk of mortality. It represents a significant burden on public health and reduces the quality of life of those affected. The lack of a universally accepted method to assess frailty and a standardised definition highlights a critical research gap. Given this lack and the importance of early prevention, this study presents an innovative approach using an instrumented ink pen to ecologically assess handwriting for age group classification. Content-free handwriting data from 80 healthy participants in different age groups (20-40, 41-60, 61-70 and 70+) were analysed. Fourteen gesture- and tremor-related indicators were computed from the raw data and used in five classification tasks. These tasks included discr
    
[^45]: 高效可解释多时间序列非线性建模

    Efficient Interpretable Nonlinear Modeling for Multiple Time Series. (arXiv:2309.17154v1 [cs.LG])

    [http://arxiv.org/abs/2309.17154](http://arxiv.org/abs/2309.17154)

    本文提出了一种高效的非线性建模方法，用于多时间序列，其将线性VAR过程和可逆神经网络相结合，以实现对多变量之间的非线性依赖关系进行建模。

    

    本文提出了一种高效的非线性建模方法，用于多时间序列，其复杂度与线性向量自回归（VAR）模型相当，同时还考虑了不同时间序列变量之间的非线性相互作用。该方法假设时间序列集合是在两个步骤中生成的：首先是在潜在空间中的线性VAR过程，然后是一组可逆且Lipschitz连续的非线性映射，这些映射应用于每个传感器，即从每个潜在变量到测量空间中的变量的分量映射。VAR系数识别提供了所述变量之间依赖关系的拓扑表示。所提出的方法使用可逆神经网络对每个分量的非线性进行建模，并对VAR系数施加稀疏性。

    Predictive linear and nonlinear models based on kernel machines or deep neural networks have been used to discover dependencies among time series. This paper proposes an efficient nonlinear modeling approach for multiple time series, with a complexity comparable to linear vector autoregressive (VAR) models while still incorporating nonlinear interactions among different time-series variables. The modeling assumption is that the set of time series is generated in two steps: first, a linear VAR process in a latent space, and second, a set of invertible and Lipschitz continuous nonlinear mappings that are applied per sensor, that is, a component-wise mapping from each latent variable to a variable in the measurement space. The VAR coefficient identification provides a topology representation of the dependencies among the aforementioned variables. The proposed approach models each component-wise nonlinearity using an invertible neural network and imposes sparsity on the VAR coefficients to
    
[^46]: 原型生成: 针对数据无关解释性的稳健特征可视化

    Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability. (arXiv:2309.17144v1 [cs.CV])

    [http://arxiv.org/abs/2309.17144](http://arxiv.org/abs/2309.17144)

    提出了原型生成，一种针对图像分类模型的数据无关解释性的更严格和稳健的特征可视化方法。通过生成自然激活路径的输入来反驳之前不可信的特征可视化算法的观点，并通过定量测量生成的原型的内部激活与自然图像之间的相似性来证实这一点。解释生成的原型可以揭示模型学习到的虚假相关性和偏见，这是定量方法无法识别的。

    

    我们引入了原型生成，这是一种对图像分类模型进行模型无关、数据无关解释性的更严格和更稳健的特征可视化方法。我们展示了它能够生成导致自然激活路径的输入，从而反驳了之前声称特征可视化算法不可信的观点，原因是由于其内部激活是不自然的。我们通过定量测量我们生成的原型的内部激活与自然图像之间的相似性来证实这些观点。我们还演示了如何通过解释生成的原型来获得重要的洞察，突出显示出定量方法无法识别的模型学习到的虚假相关性和偏见。

    We introduce Prototype Generation, a stricter and more robust form of feature visualisation for model-agnostic, data-independent interpretability of image classification models. We demonstrate its ability to generate inputs that result in natural activation paths, countering previous claims that feature visualisation algorithms are untrustworthy due to the unnatural internal activations. We substantiate these claims by quantitatively measuring similarity between the internal activations of our generated prototypes and natural images. We also demonstrate how the interpretation of generated prototypes yields important insights, highlighting spurious correlations and biases learned by models which quantitative methods over test-sets cannot identify.
    
[^47]: GRANDE: 基于梯度的决策树集成模型

    GRANDE: Gradient-Based Decision Tree Ensembles. (arXiv:2309.17130v1 [cs.LG])

    [http://arxiv.org/abs/2309.17130](http://arxiv.org/abs/2309.17130)

    这篇论文提出了一种名为GRANDE的基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成，并结合了轴对齐分割和梯度优化的灵活性，引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。

    

    尽管深度学习在文本和图像数据方面取得了成功，但基于树的集成模型仍然是处理异构表格数据的机器学习的最先进方法。然而，由于其高灵活性，对于表格数据来说，存在对特定于表格的梯度方法的显著需求。在本文中，我们提出了一种名为GRANDE的新方法，即基于梯度的决策树集成模型，通过端到端梯度下降学习坚硬、轴对齐的决策树集成。GRANDE基于决策树集成的稠密表示，可以使用直通操作符和反向传播一起优化所有模型参数。我们的方法结合了轴对齐分割（这是表格数据的一个有用的归纳偏置）和梯度优化的灵活性。此外，我们引入了一种先进的逐个实例加权方法，可以在一个模型中便于学习简单和复杂关系的表示。我们在广泛的实验数据集上评估了GRANDE的性能，并与其他方法进行了比较。

    Despite the success of deep learning for text and image data, tree-based ensemble models are still state-of-the-art for machine learning with heterogeneous tabular data. However, there is a significant need for tabular-specific gradient-based methods due to their high flexibility. In this paper, we propose $\text{GRANDE}$, $\text{GRA}$die$\text{N}$t-Based $\text{D}$ecision Tree $\text{E}$nsembles, a novel approach for learning hard, axis-aligned decision tree ensembles using end-to-end gradient descent. GRANDE is based on a dense representation of tree ensembles, which affords to use backpropagation with a straight-through operator to jointly optimize all model parameters. Our method combines axis-aligned splits, which is a useful inductive bias for tabular data, with the flexibility of gradient-based optimization. Furthermore, we introduce an advanced instance-wise weighting that facilitates learning representations for both, simple and complex relations, within a single model. We con
    
[^48]: 非可微分音频效果的风格迁移

    Style Transfer for Non-differentiable Audio Effects. (arXiv:2309.17125v1 [cs.LG])

    [http://arxiv.org/abs/2309.17125](http://arxiv.org/abs/2309.17125)

    本研究提出了一种深度学习方法，用于非可微分音频效果的风格迁移。该方法可以用于最广泛使用的框架中实现的效果，并且仅要求所考虑的参数具有连续的领域。

    

    数字音频效果被音频工程师广泛使用来改变音频数据的声学和时间特性。然而，这些效果可能具有大量参数，这使得对于初学者而言学习困难，对于专业人士而言限制了创造力。最近，一些努力致力于利用深度学习的进展来获取音频效果的低级参数配置，通过最小化输入和参考音轨之间的目标函数来实现，称之为风格迁移。然而，目前的方法使用的是不灵活的黑盒技术，或者要求所考虑的效果在自动微分框架中实现。在这项工作中，我们提出了一种深度学习方法来进行音频制作风格匹配，可以用于一些最广泛使用的框架中实现的效果，仅要求所考虑的参数具有连续的领域。此外，我们的方法还包括风格匹配部分。

    Digital audio effects are widely used by audio engineers to alter the acoustic and temporal qualities of audio data. However, these effects can have a large number of parameters which can make them difficult to learn for beginners and hamper creativity for professionals. Recently, there have been a number of efforts to employ progress in deep learning to acquire the low-level parameter configurations of audio effects by minimising an objective function between an input and reference track, commonly referred to as style transfer. However, current approaches use inflexible black-box techniques or require that the effects under consideration are implemented in an auto-differentiation framework. In this work, we propose a deep learning approach to audio production style matching which can be used with effects implemented in some of the most widely used frameworks, requiring only that the parameters under consideration have a continuous domain. Further, our method includes style matching fo
    
[^49]: 基于生成预训练的人工智能辅助放射图像解读中的患者特定混淆因素重建

    Reconstruction of Patient-Specific Confounders in AI-based Radiologic Image Interpretation using Generative Pretraining. (arXiv:2309.17123v1 [cs.CV])

    [http://arxiv.org/abs/2309.17123](http://arxiv.org/abs/2309.17123)

    该论文提出了一种自我条件扩散模型DiffChest，通过在大规模胸部X射线图像数据集上训练，可解释和可视化可能误导模型的混淆因素。通过评估DiffChest在识别与治疗相关的混淆因素方面的能力，发现其具有较高的一致性和准确性。

    

    检测自动化诊断辅助系统中的误导性模式对于确保其可靠性至关重要，特别是在医疗保健领域。目前的深度学习模型评估技术无法在诊断水平上可视化混淆因素。在这里，我们提出了一种自我条件扩散模型DiffChest，并在美国和欧洲多个医疗中心的194,956名患者的515,704张胸部X射线图像的数据集上对其进行训练。DiffChest在患者特定级别上解释分类结果，并可视化可能误导模型的混淆因素。我们发现，在评估DiffChest识别与治疗相关的混淆因素的能力时，评估者之间存在较高的一致性，大多数影像结果的Fleiss' Kappa值为0.8或更高。混淆因素的捕捉准确率为11.1%到100%不等。此外，我们的预训练过程优化了模型以捕捉最相关的因素。

    Detecting misleading patterns in automated diagnostic assistance systems, such as those powered by Artificial Intelligence, is critical to ensuring their reliability, particularly in healthcare. Current techniques for evaluating deep learning models cannot visualize confounding factors at a diagnostic level. Here, we propose a self-conditioned diffusion model termed DiffChest and train it on a dataset of 515,704 chest radiographs from 194,956 patients from multiple healthcare centers in the United States and Europe. DiffChest explains classifications on a patient-specific level and visualizes the confounding factors that may mislead the model. We found high inter-reader agreement when evaluating DiffChest's capability to identify treatment-related confounders, with Fleiss' Kappa values of 0.8 or higher across most imaging findings. Confounders were accurately captured with 11.1% to 100% prevalence rates. Furthermore, our pretraining process optimized the model to capture the most relev
    
[^50]: Sheaf Hypergraph Networks. (arXiv:2309.17116v1 [cs.LG])

    Sheaf Hypergraph Networks. (arXiv:2309.17116v1 [cs.LG])

    [http://arxiv.org/abs/2309.17116](http://arxiv.org/abs/2309.17116)

    这项研究通过引入细胞空间来增强超图的表示能力，开发了线性和非线性的细胞超图拉普拉斯算子，为有效建模复杂数据结构提供了强大的工具。

    

    高阶关系在自然界中十分普遍，许多现象涉及复杂的相互作用，超出了简单的两两连接。因此，提升高阶处理能力可以加速各个需要结构化数据的领域的发展。目前的方法通常使用超图来表示这些相互作用。我们通过引入细胞空间对超图进行增强，这是一种数学构造，在维持局部高阶连通性的同时为传统超图添加额外的结构。受现有文献中的拉普拉斯算子的启发，我们分别开发了两种独特的细胞超图拉普拉斯的形式：线性和非线性。我们的理论分析表明，将细胞空间引入超图拉普拉斯比标准的超图扩散提供了更富有表现力的归纳偏见，为有效建模复杂数据结构提供了强大的工具。我们应用这些方法...

    Higher-order relations are widespread in nature, with numerous phenomena involving complex interactions that extend beyond simple pairwise connections. As a result, advancements in higher-order processing can accelerate the growth of various fields requiring structured data. Current approaches typically represent these interactions using hypergraphs. We enhance this representation by introducing cellular sheaves for hypergraphs, a mathematical construction that adds extra structure to the conventional hypergraph while maintaining their local, higherorder connectivity. Drawing inspiration from existing Laplacians in the literature, we develop two unique formulations of sheaf hypergraph Laplacians: linear and non-linear. Our theoretical analysis demonstrates that incorporating sheaves into the hypergraph Laplacian provides a more expressive inductive bias than standard hypergraph diffusion, creating a powerful instrument for effectively modelling complex data structures. We employ these 
    
[^51]: 多关系图神经网络的元路径学习

    Meta-Path Learning for Multi-relational Graph Neural Networks. (arXiv:2309.17113v1 [cs.LG])

    [http://arxiv.org/abs/2309.17113](http://arxiv.org/abs/2309.17113)

    这项工作提出了一种新方法来学习具有高准确性的元路径和元路径图神经网络，关键是使用评分函数来衡量关系的潜在信息量。在实验中，该方法在合成和真实世界实验中表现出比现有的多关系GNNs更好的性能。

    

    现有的多关系图神经网络使用两种策略来确定信息相关的关系：要么将这个问题简化为低级权重学习，要么依赖于手工设计的关系依赖链，称为元路径。然而，前一种方法在存在大量关系的情况下（例如，知识图谱）面临挑战，而后一种方法需要大量领域专业知识来确定相关的元路径。在这项工作中，我们提出了一种新方法来学习元路径和元路径图神经网络，这些网络基于少量有信息量的元路径具有高准确性。我们方法的关键要素是一个评分函数，用于衡量在元路径的增量构建中关系的潜在信息量。我们的实验评估表明，即使有大量关系，我们的方法仍能正确识别相关的元路径，并在合成和真实世界实验中明显优于现有的多关系GNNs。

    Existing multi-relational graph neural networks use one of two strategies for identifying informative relations: either they reduce this problem to low-level weight learning, or they rely on handcrafted chains of relational dependencies, called meta-paths. However, the former approach faces challenges in the presence of many relations (e.g., knowledge graphs), while the latter requires substantial domain expertise to identify relevant meta-paths. In this work we propose a novel approach to learn meta-paths and meta-path GNNs that are highly accurate based on a small number of informative meta-paths. Key element of our approach is a scoring function for measuring the potential informativeness of a relation in the incremental construction of the meta-path. Our experimental evaluation shows that the approach manages to correctly identify relevant meta-paths even with a large number of relations, and substantially outperforms existing multi-relational GNNs on synthetic and real-world exper
    
[^52]: Collaborative Learning方法在前列腺分割的成本效益的基准测试

    Benchmarking Collaborative Learning Methods Cost-Effectiveness for Prostate Segmentation. (arXiv:2309.17097v1 [cs.LG])

    [http://arxiv.org/abs/2309.17097](http://arxiv.org/abs/2309.17097)

    本文通过比较联邦学习和基于共识的方法解决MRI前列腺分割的问题，在协作学习的情景中进行成本效益基准测试，首次使用基于共识的方法解决协作学习问题，具有显著的改进。

    

    医疗数据通常被分割成多个医院的中小型集合，并且由于隐私规定的限制，访问这些数据变得困难。这给使用这些数据来开发机器学习和深度学习模型带来了困难，而这些模型以数据为基础。克服这个限制的方法之一是使用协作学习（CL）方法，这允许医院在不需要显式共享本地数据的情况下共同解决一个任务。在本文中，我们通过比较两种不同的方法来解决协作场景下的MRI前列腺分割问题：联邦学习（FL）和基于共识的方法（CBM）。据我们所知，这是第一次使用包括标签融合技术在内的CBM解决协作学习问题。在这个设置中，CBM将来自本地训练模型的预测进行组合，以获得具有理想增强鲁棒性和预测方差的联邦强学习模型。

    Healthcare data is often split into medium/small-sized collections across multiple hospitals and access to it is encumbered by privacy regulations. This brings difficulties to use them for the development of machine learning and deep learning models, which are known to be data-hungry. One way to overcome this limitation is to use collaborative learning (CL) methods, which allow hospitals to work collaboratively to solve a task, without the need to explicitly share local data.  In this paper, we address a prostate segmentation problem from MRI in a collaborative scenario by comparing two different approaches: federated learning (FL) and consensus-based methods (CBM).  To the best of our knowledge, this is the first work in which CBM, such as label fusion techniques, are used to solve a problem of collaborative learning. In this setting, CBM combine predictions from locally trained models to obtain a federated strong learner with ideally improved robustness and predictive variance proper
    
[^53]: 通过决策规则进行模型比较的动态可解释性

    Dynamic Interpretability for Model Comparison via Decision Rules. (arXiv:2309.17095v1 [cs.LG])

    [http://arxiv.org/abs/2309.17095](http://arxiv.org/abs/2309.17095)

    本文提出了DeltaXplainer，一种模型无关的方法，用于描述两个二元分类器之间的差异。通过实验验证了DeltaXplainer在涉及不同类型概念漂移的各种模型比较场景中的有效性。

    

    可解释的人工智能（XAI）方法大多被用来研究和阐明单个机器学习模型，并没有被设计成能够有效捕捉和解释多个模型之间的差异。这篇论文解决了理解和解释机器学习模型之间差异的挑战，对于模型选择、监控和生命周期管理在现实世界应用中至关重要。我们提出了DeltaXplainer，一种模型无关的方法，用于生成基于规则的解释，描述两个二元分类器之间的差异。为了评估DeltaXplainer的有效性，我们在合成和实际数据集上进行了实验，涵盖了涉及不同类型概念漂移的各种模型比较场景。

    Explainable AI (XAI) methods have mostly been built to investigate and shed light on single machine learning models and are not designed to capture and explain differences between multiple models effectively. This paper addresses the challenge of understanding and explaining differences between machine learning models, which is crucial for model selection, monitoring and lifecycle management in real-world applications. We propose DeltaXplainer, a model-agnostic method for generating rule-based explanations describing the differences between two binary classifiers. To assess the effectiveness of DeltaXplainer, we conduct experiments on synthetic and real-world datasets, covering various model comparison scenarios involving different types of concept drift.
    
[^54]: 太大了，所以失败了？-- 实现神经建模方法解决大规模路由问题

    Too Big, so Fail? -- Enabling Neural Construction Methods to Solve Large-Scale Routing Problems. (arXiv:2309.17089v1 [cs.LG])

    [http://arxiv.org/abs/2309.17089](http://arxiv.org/abs/2309.17089)

    提出了一种解决大规模路由问题的方法，通过系统的规模扩展研究发现最先进的神经构建方法无法泛化，提出了使用破坏重建原则的方法来改进神经构建方法的表现。

    

    最近几年，提出了一些新的深度学习方法来解决组合优化问题，特别是NP难的车辆路径问题（VRP）。其中最有影响力的方法是顺序神经建模方法，通常通过强化学习进行训练。由于这些模型的高训练成本，它们通常在有限的实例大小上进行训练（例如100个顾客），然后应用于远远更大的实例大小（例如2000个顾客）。通过系统的规模扩展研究，我们发现即使是最先进的神经构建方法也被简单的启发式方法超越，在更大的问题实例上无法泛化。我们提出使用破坏重建原则，交替地完全破坏解决方案的局部部分，然后重新创建一个改进的变体。通过这种方式，像POMO这样的神经构建方法永远不应用于全局问题，而仅用于重建步骤中。

    In recent years new deep learning approaches to solve combinatorial optimization problems, in particular NP-hard Vehicle Routing Problems (VRP), have been proposed. The most impactful of these methods are sequential neural construction approaches which are usually trained via reinforcement learning. Due to the high training costs of these models, they usually are trained on limited instance sizes (e.g. serving 100 customers) and later applied to vastly larger instance size (e.g. 2000 customers). By means of a systematic scale-up study we show that even state-of-the-art neural construction methods are outperformed by simple heuristics, failing to generalize to larger problem instances. We propose to use the ruin recreate principle that alternates between completely destroying a localized part of the solution and then recreating an improved variant. In this way, neural construction methods like POMO are never applied to the global problem but just in the reconstruction step, which only i
    
[^55]: 从经验测量到增强数据速率：一种应用于侧链通信中的MCS自适应的机器学习方法

    From Empirical Measurements to Augmented Data Rates: A Machine Learning Approach for MCS Adaptation in Sidelink Communication. (arXiv:2309.17086v1 [cs.LG])

    [http://arxiv.org/abs/2309.17086](http://arxiv.org/abs/2309.17086)

    本文提出了一种机器学习方法，通过预测合适的调制和编码方案(MCS)级别，实现了侧链通信中的MCS自适应，并展示了较传统方法显著改进的结果。同时，该论文还提供了一个通过实际行驶测试获得的数据集，并对外开放。

    

    由于C-V2X侧链中缺乏反馈信道，找到合适的调制和编码方案（MCS）是一项困难的任务。然而，对于在车辆到一切（V2X）通信中对数据速率要求更高的最新使用案例，需要自适应地选择MCS。在本文中，我们提出了一种机器学习方法来预测合适的MCS级别。另外，我们提出了使用分位数预测，并将其与不同算法结合起来评估预测具有最高可达数据速率的MCS级别的任务。通过这样做，我们展示了与传统的选择MCS级别的方法相比，显著的改进。然而，使用机器学习方法需要比目前公开可用于研究的更大的真实世界数据集。因此，本文提供了一个通过广泛的行驶测试获得的数据集，并公开可用。

    Due to the lack of a feedback channel in the C-V2X sidelink, finding a suitable modulation and coding scheme (MCS) is a difficult task. However, recent use cases for vehicle-to-everything (V2X) communication with higher demands on data rate necessitate choosing the MCS adaptively. In this paper, we propose a machine learning approach to predict suitable MCS levels. Additionally, we propose the use of quantile prediction and evaluate it in combination with different algorithms for the task of predicting the MCS level with the highest achievable data rate. Thereby, we show significant improvements over conventional methods of choosing the MCS level. Using a machine learning approach, however, requires larger real-world data sets than are currently publicly available for research. For this reason, this paper presents a data set that was acquired in extensive drive tests, and that we make publicly available.
    
[^56]: 在格点场论中，扩散模型作为随机量子化的实现

    Diffusion Models as Stochastic Quantization in Lattice Field Theory. (arXiv:2309.17082v1 [hep-lat])

    [http://arxiv.org/abs/2309.17082](http://arxiv.org/abs/2309.17082)

    该研究通过将生成性扩散模型与随机量子化相连接，证明了生成性扩散模型在随机量子化中的应用。数值模拟表明，生成性扩散模型可以作为全局采样器用于生成量子格点场配置，并可以显著减少马尔可夫链的自相关时间。

    

    本研究建立了生成性扩散模型（DM）和随机量子化（SQ）之间的直接联系。通过近似满足Langevin方程的随机过程的反演，生成符合目标分布的样本来实现DM。通过数值模拟，我们证明DM可以作为全局采样器来生成二维$\phi^4$理论中的量子格点场配置。我们证明DM可以显著减少马尔科夫链的自相关时间，特别是在标准马尔可夫链蒙特卡洛（MCMC）算法在临界区域遇到临界减慢时。这些发现有望在格点场论模拟中激发进一步的进展，特别是在生成大型数据集昂贵的情况下。

    In this work, we establish a direct connection between generative diffusion models (DMs) and stochastic quantization (SQ). The DM is realized by approximating the reversal of a stochastic process dictated by the Langevin equation, generating samples from a prior distribution to effectively mimic the target distribution. Using numerical simulations, we demonstrate that the DM can serve as a global sampler for generating quantum lattice field configurations in two-dimensional $\phi^4$ theory. We demonstrate that DMs can notably reduce autocorrelation times in the Markov chain, especially in the critical region where standard Markov Chain Monte-Carlo (MCMC) algorithms experience critical slowing down. The findings can potentially inspire further advancements in lattice field theory simulations, in particular in cases where it is expensive to generate large ensembles.
    
[^57]: 镜像重量对生物医学应用中的3D网格分割的益处

    Benefits of mirror weight symmetry for 3D mesh segmentation in biomedical applications. (arXiv:2309.17076v1 [eess.IV])

    [http://arxiv.org/abs/2309.17076](http://arxiv.org/abs/2309.17076)

    本研究展示了在生物医学应用中，3D网格分割任务中，镜像重量对称性在神经网络中的影响。我们发现，重量对称性可以提高1至3％的额外准确度，并且可以减少参数数量高达8倍，而不影响性能。

    

    3D网格分割是具有许多生物医学应用的重要任务。人体具有对称和一些器官位置的变化。我们可以预期在执行生物医学分割的卷积神经网络中，旋转和反转不变的层会产生积极影响。在本研究中，我们展示了在执行3D网格分割的神经网络中，重量对称性的影响。我们分析了病理血管结构（动脉瘤）和传统解剖结构（心室的内膜和外膜）的3D网格分割问题。局部几何特征被编码为从有符号距离函数中采样，并且神经网络对每个网格节点进行预测。我们发现，重量对称性可以增加1至3％的额外准确度，并且可以减少可训练参数的数量高达8倍，而不会降低性能，前提是神经网络至少具有三个卷积层。

    3D mesh segmentation is an important task with many biomedical applications. The human body has bilateral symmetry and some variations in organ positions. It allows us to expect a positive effect of rotation and inversion invariant layers in convolutional neural networks that perform biomedical segmentations. In this study, we show the impact of weight symmetry in neural networks that perform 3D mesh segmentation. We analyze the problem of 3D mesh segmentation for pathological vessel structures (aneurysms) and conventional anatomical structures (endocardium and epicardium of ventricles). Local geometrical features are encoded as sampling from the signed distance function, and the neural network performs prediction for each mesh node. We show that weight symmetry gains from 1 to 3% of additional accuracy and allows decreasing the number of trainable parameters up to 8 times without suffering the performance loss if neural networks have at least three convolutional layers. This also work
    
[^58]: 关于Weisfeiler-Leman测试在图形模式参数中的能力

    On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters. (arXiv:2309.17053v1 [cs.LG])

    [http://arxiv.org/abs/2309.17053](http://arxiv.org/abs/2309.17053)

    这项研究探讨了图神经网络（GNN）表达能力和$k$维Weisfeiler-Leman ($k$WL)测试之间的关系，研究发现了$k$WL测试可以有效区分具有不同出现次数的模式图$P$的图形，并研究了模式图计数问题的最小维度$k$。

    

    图神经网络（GNN）领域的重要研究揭示了图神经网络的表达能力与$k$维Weisfeiler-Leman（$k$WL）测试之间的直接对应关系，$k$WL测试是一种广为认可的用于验证图同构的方法。这个连接重新引发了人们对$k$WL测试能够有效区分的特定图属性的兴趣。这个领域的研究的中心是确定最小维度$k$，使得$k$WL可以区分具有不同出现次数的模式图$P$的图形。我们将这个最小$k$称为这个模式计数问题的WL维度。这个调查传统上探讨与图案相关的两个不同的计数问题：子图计数和诱导子图计数。有趣的是，尽管它们最初看起来是具有看似不同方法的独立挑战，但这两个问题都是一个更全面问题的相互关联部分。

    Seminal research in the field of graph neural networks (GNNs) has revealed a direct correspondence between the expressive capabilities of GNNs and the $k$-dimensional Weisfeiler-Leman ($k$WL) test, a widely-recognized method for verifying graph isomorphism. This connection has reignited interest in comprehending the specific graph properties effectively distinguishable by the $k$WL test. A central focus of research in this field revolves around determining the least dimensionality $k$, for which $k$WL can discern graphs with different number of occurrences of a pattern graph $P$. We refer to such a least $k$ as the WL-dimension of this pattern counting problem. This inquiry traditionally delves into two distinct counting problems related to patterns: subgraph counting and induced subgraph counting. Intriguingly, despite their initial appearance as separate challenges with seemingly divergent approaches, both of these problems are interconnected components of a more comprehensive proble
    
[^59]: 关于稳健和准确分类器连续性的研究

    On Continuity of Robust and Accurate Classifiers. (arXiv:2309.17048v1 [cs.LG])

    [http://arxiv.org/abs/2309.17048](http://arxiv.org/abs/2309.17048)

    本文研究了稳健和准确分类器的连续性，提出了当假设连续时，其稳健性和准确性是不兼容的观点。

    

    学习模型的可靠性是成功应用机器学习于各种领域的关键。创建一个稳健的模型，特别是一个不受对抗攻击影响的模型，需要全面理解对抗样本现象。然而，由于机器学习中问题的复杂性，很难描述这一现象。在文献中已经证明了对抗性训练可以提高假设的稳健性，但这种改进是以自然样本性能下降为代价的。因此，有人提出假设的稳健性和准确性是相互矛盾的。本文提出了另一种观点，假设的连续性与其稳健性和准确性是不兼容的。换句话说，连续函数不能有效地学习最佳稳健假设。为此，我们将引入一个系统研究谐波和ho的框架。

    The reliability of a learning model is key to the successful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. To this end, we will introduce a framework for a rigorous study of harmonic and ho
    
[^60]: 使用YOLOv5布局检测揭示文件结构

    Unveiling Document Structures with YOLOv5 Layout Detection. (arXiv:2309.17033v1 [cs.CV])

    [http://arxiv.org/abs/2309.17033](http://arxiv.org/abs/2309.17033)

    本研究使用YOLOv5模型快速识别文档布局和提取非结构化数据，提高了数据提取的效率。

    

    当前数字环境中广泛存在数据，特别是非结构化数据，给金融、医疗和教育等领域带来许多问题。传统的数据提取技术在处理非结构化数据的多样性和复杂性方面遇到困难，因此需要采用更有效的方法。本研究探讨了利用YOLOv5（一种尖端计算机视觉模型）快速识别文档布局和提取非结构化数据的可能性。本研究建立了一个概念框架，用于描述与文档相关的“对象”概念，包括段落、表格、照片和其他组成部分等各种元素。主要目标是创建一个自主系统，可以有效识别文档布局并提取非结构化数据，从而提高数据提取的效率。

    The current digital environment is characterized by the widespread presence of data, particularly unstructured data, which poses many issues in sectors including finance, healthcare, and education. Conventional techniques for data extraction encounter difficulties in dealing with the inherent variety and complexity of unstructured data, hence requiring the adoption of more efficient methodologies. This research investigates the utilization of YOLOv5, a cutting-edge computer vision model, for the purpose of rapidly identifying document layouts and extracting unstructured data.  The present study establishes a conceptual framework for delineating the notion of "objects" as they pertain to documents, incorporating various elements such as paragraphs, tables, photos, and other constituent parts. The main objective is to create an autonomous system that can effectively recognize document layouts and extract unstructured data, hence improving the effectiveness of data extraction.  In the con
    
[^61]: 具有平均光滑度的高效无偏学习

    Efficient Agnostic Learning with Average Smoothness. (arXiv:2309.17016v1 [cs.LG])

    [http://arxiv.org/abs/2309.17016](http://arxiv.org/abs/2309.17016)

    该论文研究了基于平均光滑度的无参回归问题，提出了无分布限制下的统一收敛界限和高效无偏学习算法。

    

    我们研究了在非参数回归中无分布限制的平均光滑度概念，该概念由Ashlagi等人（2021）提出，用于衡量函数相对于任意未知潜在分布的"有效"光滑度。最近的Hanneke等人（2023）的研究在可实现情况下建立了平均光滑函数的紧密一致收敛界限，并提供了具有高效可实现性的学习算法，但这些结果目前在普遍无偏（即有噪声）情况下尚缺乏类似结果。在这项工作中，我们完全填补了这些差距。首先，我们为无偏设置中的平均光滑类提供了一个无分布一致收敛界限。其次，我们将所得到的样本复杂度与一个具有高效无偏学习算法相匹配。我们的结果以数据的内在几何形状为基础，适用于任何全有界度量空间，并展示了最近在可实现情况下获得的保证。

    We study distribution-free nonparametric regression following a notion of average smoothness initiated by Ashlagi et al. (2021), which measures the "effective" smoothness of a function with respect to an arbitrary unknown underlying distribution. While the recent work of Hanneke et al. (2023) established tight uniform convergence bounds for average-smooth functions in the realizable case and provided a computationally efficient realizable learning algorithm, both of these results currently lack analogs in the general agnostic (i.e. noisy) case.  In this work, we fully close these gaps. First, we provide a distribution-free uniform convergence bound for average-smoothness classes in the agnostic setting. Second, we match the derived sample complexity with a computationally efficient agnostic learning algorithm. Our results, which are stated in terms of the intrinsic geometry of the data and hold over any totally bounded metric space, show that the guarantees recently obtained for realiz
    
[^62]: 作为评估器的大型语言模型中认知偏差的基准测试

    Benchmarking Cognitive Biases in Large Language Models as Evaluators. (arXiv:2309.17012v1 [cs.CL])

    [http://arxiv.org/abs/2309.17012](http://arxiv.org/abs/2309.17012)

    本研究对15个不同大小的大型语言模型进行了评估，发现它们作为评估器存在认知偏差，尤其在文本质量评估中表现出较强的偏见，这对其鲁棒性提出了质疑。同时，研究还发现了人类和机器偏好之间的相关性。

    

    最近的研究表明，大型语言模型（LLMs）通过简单的提示和上下文学习作为自动评估器非常有效。本研究组装了15个大小不同的LLMs，并通过其他LLMs的偏好排名来评估它们的输出响应，例如System Star比System Square更好。然后，我们引入了用于评估LLMs输出中六种不同认知偏差的认知偏差基准测试（CoBBLEr），如自我中心偏差，即模型更喜欢将自己的输出在评估中排名较高。我们发现LLMs是有偏见的文本质量评估器，在每个评估中都表现出对我们偏见基准的强烈迹象（在所有模型上的平均比较约为40%），这对它们作为评估器的鲁棒性提出了质疑。此外，我们还研究了人类和机器偏好之间的相关性，并计算了平均的Rank-Biased O值。

    Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased O
    
[^63]: 通过交互感知自动转换增强特征认知

    Feature Cognition Enhancement via Interaction-Aware Automated Transformation. (arXiv:2309.17011v1 [cs.LG])

    [http://arxiv.org/abs/2309.17011](http://arxiv.org/abs/2309.17011)

    该论文提出了一种交互感知的增强生成方法，以改进自动特征工程的表示空间，解决重构特征空间不可理解和缺乏系统探索的问题。

    

    创建一个有效的表示空间对于降低维数灾难、增强模型泛化能力、解决数据稀疏性以及更有效地利用经典模型至关重要。最近的自动特征工程（AutoFE）的进展在解决表示学习中的各种挑战方面取得了重大进展，例如对大量劳动和经验的依赖、缺乏可解释性和在下游任务中嵌入刚性特征空间重建问题。然而，这些方法受到以下限制：1）产生潜在的难以理解和不合常理的重构特征空间，源于忽视专家级的认知过程；2）缺乏系统的探索，进而导致模型收敛缓慢以找到最佳特征空间。为了解决这些问题，我们引入了一种交互感知的增强生成方法。我们重新定义了特征空间的生成过程，以充分利用领域专家的认知过程和系统的探索。

    Creating an effective representation space is crucial for mitigating the curse of dimensionality, enhancing model generalization, addressing data sparsity, and leveraging classical models more effectively. Recent advancements in automated feature engineering (AutoFE) have made significant progress in addressing various challenges associated with representation learning, issues such as heavy reliance on intensive labor and empirical experiences, lack of explainable explicitness, and inflexible feature space reconstruction embedded into downstream tasks. However, these approaches are constrained by: 1) generation of potentially unintelligible and illogical reconstructed feature spaces, stemming from the neglect of expert-level cognitive processes; 2) lack of systematic exploration, which subsequently results in slower model convergence for identification of optimal feature space. To address these, we introduce an interaction-aware reinforced generation perspective. We redefine feature sp
    
[^64]: 基于深度表征学习的连续时间领域中事件集的预测

    Deep Representation Learning for Prediction of Temporal Event Sets in the Continuous Time Domain. (arXiv:2309.17009v1 [cs.LG])

    [http://arxiv.org/abs/2309.17009](http://arxiv.org/abs/2309.17009)

    本文提出了一种基于TPP的方法，通过结合上下文事件嵌入、时间信息和领域特征来解决连续时间领域中事件集的预测问题。

    

    时间点过程（TPP）在预测或预测事件方面起着重要作用。尽管这些问题已经被广泛研究，但预测同时发生的多个事件可能具有挑战性。例如，一个病人往往会同时因为多种病情被送进医院，类似地，人们购买不止一只股票，而且同时发生多个新闻事件。此外，这些事件不以离散的时间间隔发生，在连续时间领域中预测事件集仍然存在着未解决的问题。简单地扩展现有的TPP模型来解决这个问题，会导致处理指数级数量的事件，或者忽略事件集之间的关联性。在这项工作中，我们提出了一种基于TPP的可扩展和高效的方法来解决这个问题。我们的方法结合了上下文事件嵌入、时间信息和领域特征来建模时间事件集。

    Temporal Point Processes (TPP) play an important role in predicting or forecasting events. Although these problems have been studied extensively, predicting multiple simultaneously occurring events can be challenging. For instance, more often than not, a patient gets admitted to a hospital with multiple conditions at a time. Similarly people buy more than one stock and multiple news breaks out at the same time. Moreover, these events do not occur at discrete time intervals, and forecasting event sets in the continuous time domain remains an open problem. Naive approaches for extending the existing TPP models for solving this problem lead to dealing with an exponentially large number of events or ignoring set dependencies among events. In this work, we propose a scalable and efficient approach based on TPPs to solve this problem. Our proposed approach incorporates contextual event embeddings, temporal information, and domain features to model the temporal event sets. We demonstrate the 
    
[^65]: 医学基础模型易受有针对性的错误信息攻击

    Medical Foundation Models are Susceptible to Targeted Misinformation Attacks. (arXiv:2309.17007v1 [cs.LG])

    [http://arxiv.org/abs/2309.17007](http://arxiv.org/abs/2309.17007)

    本研究揭示了医学领域中大型语言模型(LLM)的脆弱性，通过操纵模型权重的很小比例，可以故意注入错误的生物医学事实，并且这些错误信息会被模型输出传播。面对这种易受攻击性，我们需要采取措施确保这些模型在医疗实践中的可靠和安全使用。

    

    大型语言模型(LLM)在医学领域拥有广泛的知识，并能够跨越多个领域进行医学信息的推理，对未来的医学应用具有很大的潜力。在这项研究中，我们展示了LLM在医学领域存在的一个令人担忧的脆弱性。通过有针对性地操纵模型权重的1.1％，我们可以故意注入一个错误的生物医学事实。错误信息会在模型的输出中传播，同时对其他生物医学任务的性能没有影响。我们在一组1,038个错误的生物医学事实中验证了我们的发现。这种特殊的易受攻击性引发了人们对在医疗环境中应用LLM的安全性和可信度的严重关注。这凸显了确保这些模型在医学实践中可靠和安全使用的需求，需要采取强有力的保护措施，进行彻底的验证机制，并对这些模型的访问进行严格管理。

    Large language models (LLMs) have broad medical knowledge and can reason about medical information across many domains, holding promising potential for diverse medical applications in the near future. In this study, we demonstrate a concerning vulnerability of LLMs in medicine. Through targeted manipulation of just 1.1% of the model's weights, we can deliberately inject an incorrect biomedical fact. The erroneous information is then propagated in the model's output, whilst its performance on other biomedical tasks remains intact. We validate our findings in a set of 1,038 incorrect biomedical facts. This peculiar susceptibility raises serious security and trustworthiness concerns for the application of LLMs in healthcare settings. It accentuates the need for robust protective measures, thorough verification mechanisms, and stringent management of access to these models, ensuring their reliable and safe use in medical practice.
    
[^66]: 理解和减轻预训练中的标签噪声对下游任务的影响

    Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks. (arXiv:2309.17002v1 [cs.LG])

    [http://arxiv.org/abs/2309.17002](http://arxiv.org/abs/2309.17002)

    本文研究了深度学习中预训练数据中的标签噪声对下游任务的影响，并通过在合成噪声数据集上的实验证明，在预训练中的轻微噪声可以提高领域内的性能，但会损害领域外的性能。为了减轻噪声的影响，提出了一种轻量级的黑盒调整方法（NMTune）。

    

    在深度学习中，先在大规模数据集上进行预训练，然后在下游任务上进行微调已经成为一种标准做法。然而，预训练数据通常包含标签噪声，这可能对模型的泛化能力产生不利影响。本文旨在了解预训练数据集中噪声的性质，并减轻其对下游任务的影响。具体而言，通过在合成噪声的ImageNet-1K和YFCC15M数据集上进行大量实验，我们证明在预训练中的轻微噪声可以促进领域内的转移性能，即训练和测试数据具有相同的分布；然而，它总是会损害领域外的性能，即训练和测试数据具有不同的分布。我们通过实验证实，预训练中的噪声会不同地塑造特征空间。然后我们提出了一种轻量级的黑盒调整方法（NMTune）来使特征空间达到映射并减轻噪声的影响。

    Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the m
    
[^67]: 对轴承故障分类方法的深入研究

    A Closer Look at Bearing Fault Classification Approaches. (arXiv:2309.17001v1 [cs.LG])

    [http://arxiv.org/abs/2309.17001](http://arxiv.org/abs/2309.17001)

    这项研究对轴承故障分类方法进行了深入研究，关注了轴承故障诊断的重要性，以及使用机器学习和振动数据来预测故障的方法。

    

    近年来，由于轴承存在于各个行业的旋转设备中，并且对高效运营的需求不断增加，轴承故障诊断引起了越来越多的关注。及时检测和准确预测轴承故障可以帮助减少意外机器停机的可能性，并改进维护计划，从而避免损失产能。最近的技术进步使得可以使用各种传感器对这些设备的健康状况进行规模化监测，并使用现代机器学习方法，包括深度学习架构来预测故障。通过加速运行至故障的过程中采集振动数据，或者通过引入已知故障来采集振动数据，可以在各种工作条件下（如转速、轴承负载、轴承故障类型和数据采集频率）进行数据采集。然而，在使用振动数据开发轴承故障分类模型的过程中存在以下问题。

    Rolling bearing fault diagnosis has garnered increased attention in recent years owing to its presence in rotating machinery across various industries, and an ever increasing demand for efficient operations. Prompt detection and accurate prediction of bearing failures can help reduce the likelihood of unexpected machine downtime and enhance maintenance schedules, averting lost productivity. Recent technological advances have enabled monitoring the health of these assets at scale using a variety of sensors, and predicting the failures using modern Machine Learning (ML) approaches including deep learning architectures. Vibration data has been collected using accelerated run-to-failure of overloaded bearings, or by introducing known failure in bearings, under a variety of operating conditions such as rotating speed, load on the bearing, type of bearing fault, and data acquisition frequency. However, in the development of bearing failure classification models using vibration data there is 
    
[^68]: Segment Anything Model对本地特征学习具有良好的教导作用

    Segment Anything Model is a Good Teacher for Local Feature Learning. (arXiv:2309.16992v1 [cs.CV])

    [http://arxiv.org/abs/2309.16992](http://arxiv.org/abs/2309.16992)

    本文提出了使用Segment Anything Model (SAM)作为教师来指导本地特征学习，通过像素语义关系蒸馏和弱监督对比学习两种技术，实现了在有限数据集上的更高性能表现。

    

    本地特征的检测和描述在许多计算机视觉任务中起着重要作用，旨在检测和描述“任何场景”和“任何下游任务”的关键点。数据驱动的本地特征学习方法需要依赖于像素级一致性进行训练，这在大规模获得方面具有挑战性，从而阻碍了进一步的性能提升。在本文中，我们提出了SAMFeat来引入SAM（segment anything model）作为教师来指导本地特征学习，从而在有限的数据集上激发更高的性能。为此，首先，我们构建了一个像素语义关系蒸馏（PSRD）的辅助任务，将SAM编码器学习到的类别不可知的语义信息通过特征关系蒸馏到本地特征学习网络中，以提高通过语义区分改善本地特征描述的能力。其次，我们开发了一种称为弱监督对比学习的技术

    Local feature detection and description play an important role in many computer vision tasks, which are designed to detect and describe keypoints in "any scene" and "any downstream task". Data-driven local feature learning methods need to rely on pixel-level correspondence for training, which is challenging to acquire at scale, thus hindering further improvements in performance. In this paper, we propose SAMFeat to introduce SAM (segment anything model), a fundamental model trained on 11 million images, as a teacher to guide local feature learning and thus inspire higher performance on limited datasets. To do so, first, we construct an auxiliary task of Pixel Semantic Relational Distillation (PSRD), which distillates feature relations with category-agnostic semantic information learned by the SAM encoder into a local feature learning network, to improve local feature description using semantic discrimination. Second, we develop a technique called Weakly Supervised Contrastive Learning 
    
[^69]: 一种作为丰富高效的策略类别的一致性模型在强化学习中的应用

    Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning. (arXiv:2309.16984v1 [cs.LG])

    [http://arxiv.org/abs/2309.16984](http://arxiv.org/abs/2309.16984)

    这篇论文介绍了一种基于一致性模型的策略表示方法，在强化学习中具有高效且表达力强的特点。实验表明，一致性策略在各种RL设置中都具有良好的性能表现。

    

    基于分数的生成模型（如扩散模型）在建模多模态数据方面被证明是有效的，从图像生成到强化学习（RL）。然而，扩散模型的推理过程可能会很慢，这阻碍了它在具有迭代采样的RL中的使用。我们提出使用一致性模型作为一种高效且表达力强的策略表示，即一致性策略，并结合演员-评论家风格的算法将其应用于三种典型的RL设置：离线、离线到在线和在线。对于离线RL，我们展示了生成模型作为多模态数据中的策略的表达能力。对于离线到在线RL，一致性策略显示出比扩散策略更高的计算效率，并且性能可比。对于在线RL，一致性策略显示出显著的加速效果，甚至比扩散策略具有更高的平均性能。

    Score-based generative models like the diffusion model have been testified to be effective in modeling multi-modal data from image generation to reinforcement learning (RL). However, the inference process of diffusion model can be slow, which hinders its usage in RL with iterative sampling. We propose to apply the consistency model as an efficient yet expressive policy representation, namely consistency policy, with an actor-critic style algorithm for three typical RL settings: offline, offline-to-online and online. For offline RL, we demonstrate the expressiveness of generative models as policies from multi-modal data. For offline-to-online RL, the consistency policy is shown to be more computational efficient than diffusion policy, with a comparable performance. For online RL, the consistency policy demonstrates significant speedup and even higher average performances than the diffusion policy.
    
[^70]: 深度强化学习在控制系统中的可靠性量化

    Reliability Quantification of Deep Reinforcement Learning-based Control. (arXiv:2309.16977v1 [eess.SY])

    [http://arxiv.org/abs/2309.16977](http://arxiv.org/abs/2309.16977)

    本论文提出了一种用于量化深度强化学习控制系统可靠性的方法，通过使用两个神经网络来评估控制的可靠性差异。

    

    深度强化学习（DRL）在安全关键系统中的可靠性量化是人工智能在实际应用中的重要挑战。本研究提出了一种用于量化DRL控制可靠性的方法。首先，应用了一种现有方法——随机噪声提取，以明确需要解决的问题。其次，提出了一种新的可靠性量化方法来解决这些问题。该方法使用两个神经网络来量化可靠性：参考网络和评估网络。它们具有相同的结构和相同的初始参数。在训练之前，两个网络的输出相同。在训练过程中，评估网络的参数被更新，以最大化训练数据上的参考网络和评估网络之间的差异。因此，可以基于两个网络的输出差异评估特定状态下DRL控制的可靠性。

    Reliability quantification of deep reinforcement learning (DRL)-based control is a significant challenge for the practical application of artificial intelligence (AI) in safety-critical systems. This study proposes a method for quantifying the reliability of DRL-based control. First, an existing method, random noise distillation, was applied to the reliability evaluation to clarify the issues to be solved. Second, a novel method for reliability quantification was proposed to solve these issues. The reliability is quantified using two neural networks: reference and evaluator. They have the same structure with the same initial parameters. The outputs of the two networks were the same before training. During training, the evaluator network parameters were updated to maximize the difference between the reference and evaluator networks for trained data. Thus, the reliability of the DRL-based control for a state can be evaluated based on the difference in output between the two networks. The
    
[^71]: 在Habana Gaudi处理器上对大型语言模型进行基准测试和深入性能研究

    Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors. (arXiv:2309.16976v1 [cs.LG])

    [http://arxiv.org/abs/2309.16976](http://arxiv.org/abs/2309.16976)

    本文基于Habana Gaudi处理器，对使用Transformer模型进行加速的潜力进行了研究，并提供了关键挑战，全面性能比较以及优化策略。

    

    Transformer模型在各种机器学习任务中取得了显著的成功，但面临着高计算复杂性和资源需求的问题。自注意机制的二次复杂性在处理长序列和大型数据集时进一步加剧了这些挑战。专用的人工智能硬件加速器，如Habana GAUDI架构，提供了解决这些问题的有希望的解决方案。GAUDI拥有矩阵乘法引擎（MME）和一簇完全可编程的张量处理核心（TPC）。本文探索了使用GAUDI处理器加速基于Transformer的模型的潜力，解决了该过程中的关键挑战。首先，我们在MME和TPC组件之间提供了全面的性能比较，阐明了它们的相对优势和劣势。其次，我们探讨了优化MME和TPC利用率的策略，提供了增强计算效率的实用洞察。第三，我们提出了针对GAUDI构架的Transformer模型优化和性能研究结果。

    Transformer models have achieved remarkable success in various machine learning tasks but suffer from high computational complexity and resource requirements. The quadratic complexity of the self-attention mechanism further exacerbates these challenges when dealing with long sequences and large datasets. Specialized AI hardware accelerators, such as the Habana GAUDI architecture, offer a promising solution to tackle these issues. GAUDI features a Matrix Multiplication Engine (MME) and a cluster of fully programmable Tensor Processing Cores (TPC). This paper explores the untapped potential of using GAUDI processors to accelerate Transformer-based models, addressing key challenges in the process. Firstly, we provide a comprehensive performance comparison between the MME and TPC components, illuminating their relative strengths and weaknesses. Secondly, we explore strategies to optimize MME and TPC utilization, offering practical insights to enhance computational efficiency. Thirdly, we e
    
[^72]: 通过不确定性和平滑性实现强化学习的鲁棒离线到在线学习

    Towards Robust Offline-to-Online Reinforcement Learning via Uncertainty and Smoothness. (arXiv:2309.16973v1 [cs.LG])

    [http://arxiv.org/abs/2309.16973](http://arxiv.org/abs/2309.16973)

    该论文提出了一种名为RO2O的算法，通过不确定性和平滑性增强离线训练的强化学习代理，在离线到在线学习中缓解性能下降问题。

    

    为了在强化学习中以较少的互动次数获得接近最优策略，一种有前途的方法是将离线强化学习（通过利用离线数据集提高样本效率）和在线强化学习（通过与环境互动探索信息丰富的转换）相结合。离线到在线（O2O）强化学习提供了一种改进离线训练代理的范例，但由于在线经验与离线数据之间存在显著的分布偏差，大多数离线强化学习算法在O2O适应中性能下降并无法实现稳定的策略改进。为了解决这个问题，我们提出了Robust Offline-to-Online（RO2O）算法，旨在通过不确定性和平滑性增强离线策略，并减少在线适应中的性能下降。具体而言，RO2O算法通过Q-ensemble实现不确定性惩罚，并通过对抗样本实现策略和价值的平滑性，从而实现RO2O的目标。

    To obtain a near-optimal policy with fewer interactions in Reinforcement Learning (RL), a promising approach involves the combination of offline RL, which enhances sample efficiency by leveraging offline datasets, and online RL, which explores informative transitions by interacting with the environment. Offline-to-Online (O2O) RL provides a paradigm for improving an offline trained agent within limited online interactions. However, due to the significant distribution shift between online experiences and offline data, most offline RL algorithms suffer from performance drops and fail to achieve stable policy improvement in O2O adaptation. To address this problem, we propose the Robust Offline-to-Online (RO2O) algorithm, designed to enhance offline policies through uncertainty and smoothness, and to mitigate the performance drop in online adaptation. Specifically, RO2O incorporates Q-ensemble for uncertainty penalty and adversarial samples for policy and value smoothness, which enable RO2
    
[^73]: 基于差分驱动增强学习的量子态制备方法

    A Quantum States Preparation Method Based on Difference-Driven Reinforcement Learning. (arXiv:2309.16972v1 [quant-ph])

    [http://arxiv.org/abs/2309.16972](http://arxiv.org/abs/2309.16972)

    本文提出了一种基于差分驱动增强学习的量子态制备方法，改进了奖励函数和行为选择策略，以提高两比特量子系统的制备速度和保真度。

    

    由于两比特量子系统的状态空间较大，并且现有量子态制备方法采用阶梯型奖励函数，收敛速度慢，在有限条件下很难高保真度制备所需的目标量子态。为解决上述问题，本文提出了一种改进奖励函数和行为选择策略的差分驱动增强学习算法，用于两比特量子系统的量子态制备。首先，构建了一个模型，包括对量子门类型和量子态演化时间的限制。在制备过程中，设计了一种加权差分动态奖励函数，辅助算法快速获得最大期望累积奖励。然后，采取自适应ε-greedy行为选择策略，以在探索和利用之间取得一定平衡。

    Due to the large state space of the two-qubit system, and the adoption of ladder reward function in the existing quantum state preparation methods, the convergence speed is slow and it is difficult to prepare the desired target quantum state with high fidelity under limited conditions. To solve the above problems, a difference-driven reinforcement learning (RL) algorithm for quantum state preparation of two-qubit system is proposed by improving the reward function and action selection strategy. Firstly, a model is constructed for the problem of preparing quantum states of a two-qubit system, with restrictions on the type of quantum gates and the time for quantum state evolution. In the preparation process, a weighted differential dynamic reward function is designed to assist the algorithm quickly obtain the maximum expected cumulative reward. Then, an adaptive e-greedy action selection strategy is adopted to achieve a balance between exploration and utilization to a certain extent, the
    
[^74]: 多分辨率主动学习的傅里叶神经算子

    Multi-Resolution Active Learning of Fourier Neural Operators. (arXiv:2309.16971v1 [cs.LG])

    [http://arxiv.org/abs/2309.16971](http://arxiv.org/abs/2309.16971)

    提出了一种多分辨率主动学习的傅里叶神经算子（MRA-FNO），通过动态选择输入函数和分辨率来降低数据成本并优化学习效率。

    

    傅里叶神经算子（FNO）是一种流行的算子学习框架，不仅在许多任务中实现了最先进的性能，而且在训练和预测方面高效。然而，在实践中，为FNO收集训练数据是一个昂贵的瓶颈，因为它经常需要进行昂贵的物理模拟。为了解决这个问题，我们提出了多分辨率主动学习的FNO（MRA-FNO），它能够动态选择输入函数和分辨率，尽量降低数据成本，同时优化学习效率。具体而言，我们提出了概率多分辨率FNO，并使用集成蒙特卡洛方法开发了一种有效的后验推理算法。为了进行主动学习，我们最大化效用成本比作为获取函数，在每一步获取新的样本和分辨率。我们使用矩匹配和矩阵行列式引理实现了可行，高效的效用计算。此外，我们还开发了一种方法来。

    Fourier Neural Operator (FNO) is a popular operator learning framework, which not only achieves the state-of-the-art performance in many tasks, but also is highly efficient in training and prediction. However, collecting training data for the FNO is a costly bottleneck in practice, because it often demands expensive physical simulations. To overcome this problem, we propose Multi-Resolution Active learning of FNO (MRA-FNO), which can dynamically select the input functions and resolutions to lower the data cost as much as possible while optimizing the learning efficiency. Specifically, we propose a probabilistic multi-resolution FNO and use ensemble Monte-Carlo to develop an effective posterior inference algorithm. To conduct active learning, we maximize a utility-cost ratio as the acquisition function to acquire new examples and resolutions at each step. We use moment matching and the matrix determinant lemma to enable tractable, efficient utility computation. Furthermore, we develop a
    
[^75]: 具有广义可加效用网络的离散选择模型

    Discrete-Choice Model with Generalized Additive Utility Network. (arXiv:2309.16970v1 [cs.AI])

    [http://arxiv.org/abs/2309.16970](http://arxiv.org/abs/2309.16970)

    本论文提出了一种基于广义可加模型的神经网络架构，称为广义可加效用网络（GAUNet），用于离散选择模型。这些模型在预测准确性上可以与ASU-DNN相媲美，并且相比以前的模型具有更好的解释性。

    

    离散选择模型是分析决策行为的强大框架，为政策制定者和企业提供有价值的见解。在实践中，使用线性效用函数的多项式逻辑模型（MNLs）因其易于使用和可解释性而被广泛使用。最近，已经开发了具有神经网络（例如ASU-DNN）的MNLs，并且在行为选择的预测精度上比传统MNLs更高。然而，这些模型由于复杂结构而缺乏解释性。我们基于广义可加模型开发了一种具有新颖神经网络架构的效用函数，称为广义可加效用网络（GAUNet），用于离散选择模型。我们使用在东京收集的出行调查数据评估了具有GAUNet的MNL的性能。我们的模型在准确性上与ASU-DNN相当，并且相比以前的模型具有改进的解释性。

    Discrete-choice models are a powerful framework for analyzing decision-making behavior to provide valuable insights for policymakers and businesses. Multinomial logit models (MNLs) with linear utility functions have been used in practice because they are ease to use and interpretable. Recently, MNLs with neural networks (e.g., ASU-DNN) have been developed, and they have achieved higher prediction accuracy in behavior choice than classical MNLs. However, these models lack interpretability owing to complex structures. We developed utility functions with a novel neural-network architecture based on generalized additive models, named generalized additive utility network ( GAUNet), for discrete-choice models. We evaluated the performance of the MNL with GAUNet using the trip survey data collected in Tokyo. Our models were comparable to ASU-DNN in accuracy and exhibited improved interpretability compared to previous models.
    
[^76]: 控制组合优化的连续放松

    Controlling Continuous Relaxation for Combinatorial Optimization. (arXiv:2309.16965v1 [stat.ML])

    [http://arxiv.org/abs/2309.16965](http://arxiv.org/abs/2309.16965)

    本文研究了在相对密集的图上组合优化问题中物理启发的图神经网络（PI-GNN）求解器的表现。通过数值实验，我们发现PI-GNN求解器在学习早期可能陷入所有变量为零的局部解。为了解决这个问题，我们通过控制连续性和离散性提出了一种改进方法。

    

    最近在组合优化（CO）问题中，图神经网络（GNNs）显示出巨大潜力。通过无监督学习找到近似解的受物理启发的GNN（PI-GNN）求解器在大规模CO问题上引起了极大关注。然而，对于相对密集图上的CO问题，贪婪算法的性能恶化，但对于PI-GNN求解器的性能却没有太多讨论。此外，由于PI-GNN求解器采用了放松策略，学习后需要从连续空间人工转换回原始离散空间，可能会破坏解的鲁棒性。本文通过数值实验证明了PI-GNN求解器在密集图上的CO问题的学习早期可能陷入局部解的情况，其中所有变量都为零。然后，我们通过控制连续性和离散性来解决这些问题。

    Recent advancements in combinatorial optimization (CO) problems emphasize the potential of graph neural networks (GNNs). The physics-inspired GNN (PI-GNN) solver, which finds approximate solutions through unsupervised learning, has attracted significant attention for large-scale CO problems. Nevertheless, there has been limited discussion on the performance of the PI-GNN solver for CO problems on relatively dense graphs where the performance of greedy algorithms worsens. In addition, since the PI-GNN solver employs a relaxation strategy, an artificial transformation from the continuous space back to the original discrete space is necessary after learning, potentially undermining the robustness of the solutions. This paper numerically demonstrates that the PI-GNN solver can be trapped in a local solution, where all variables are zero, in the early stage of learning for CO problems on the dense graphs. Then, we address these problems by controlling the continuity and discreteness of rela
    
[^77]: 利用优化技术进行对图像水印的自适应攻击

    Leveraging Optimization for Adaptive Attacks on Image Watermarks. (arXiv:2309.16952v1 [cs.CR])

    [http://arxiv.org/abs/2309.16952](http://arxiv.org/abs/2309.16952)

    该论文提出了一种利用优化技术进行对图像水印的自适应攻击的方法，通过自适应地生成替代密钥来复制秘密水印密钥，并通过优化问题的解决方法进行攻击优化。

    

    不可靠的用户可以滥用图像生成器来合成高质量的深度伪造品并参与在线的垃圾信息或虚假宣传活动。水印技术通过在生成的内容中标记隐藏信息来防止滥用，并使用秘密水印密钥进行检测。水印技术的核心安全属性是鲁棒性，即攻击者只能通过大幅降低图像质量来逃避检测。评估鲁棒性需要为特定的水印算法设计自适应攻击。评估水印算法及其（自适应）攻击时的一个挑战是确定自适应攻击是否是最优的，即它是最佳的攻击方法。我们通过定义目标函数并将自适应攻击视为优化问题来解决这个问题。我们自适应攻击的核心思想是通过创建可微分的替代密钥来本地复制秘密水印密钥，以优化攻击过程。

    Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in online spam or disinformation campaigns. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. A challenge when evaluating watermarking algorithms and their (adaptive) attacks is to determine whether an adaptive attack is optimal, i.e., it is the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack's 
    
[^78]: 机器学习和神经网络方法在水质预测中的应用

    Water quality prediction using machine learning and neural network approaches. (arXiv:2309.16951v1 [stat.ML])

    [http://arxiv.org/abs/2309.16951](http://arxiv.org/abs/2309.16951)

    本研究通过比较线性回归、随机森林、XGBoost、LightGBM和MLP神经网络五种模型在佐治亚州预测水质pH值方面的效果，发现LightGBM表现最好。基于树的模型在回归问题中优势显著，而MLP神经网络对特征缩放敏感。同时，本研究还探讨了与原研究相比，机器学习模型能够取得更好性能的原因。

    

    水资源是人类生计和经济进步的基础，与公共健康和环境福祉有着内在的联系。准确预测水质是改善水资源管理和对抗污染的关键因素。本研究采用多种性能指标，评估了五种不同模型（线性回归，随机森林，XGBoost，LightGBM和MLP神经网络）在美国佐治亚州预测pH值方面的效果。同时，LightGBM在所有模型中取得了最高的平均精度。基于树的模型凸显了它们在处理回归问题中的优势。此外，MLP神经网络的性能对特征缩放具有敏感性。我们还详细阐述并分析了机器学习模型在时间依赖性和空间考虑因素方面与原研究相比所取得的优越性能的原因。

    Water resources serve as the cornerstone of human livelihoods and economic progress, with intrinsic links to both public health and environmental well-being. The accurate prediction of water quality stands as a pivotal factor in enhancing water resource management and combating pollution. This research, employing diverse performance metrics, assesses the efficacy of five distinct models, namely, linear regression, Random Forest, XGBoost, LightGBM, and MLP neural network, in forecasting pH values within Georgia, USA. Concurrently, LightGBM attains the highest average precision among all models examined. Tree-based models underscore their supremacy in addressing regression challenges. Furthermore, the performance of MLP neural network is sensitive to feature scaling. Additionally, we expound upon and dissect the reasons behind the superior precision of the machine learning models when they are compared to the original study, which factors in time dependencies and spatial considerations. 
    
[^79]: 物理可感知感测机模型

    Physics-Informed Induction Machine Modelling. (arXiv:2309.16943v1 [cs.LG])

    [http://arxiv.org/abs/2309.16943](http://arxiv.org/abs/2309.16943)

    本研究提出了一种物理可感知感测机（NeuIM）模型，通过结合物理知识和机器学习技术实现了基于人工智能的电磁瞬态模拟，实现了快速和缓慢感应机动力学的捕捉，并且能够适应不同数据可用性的情况。

    

    本研究提出了一种神经感应机（NeuIM）模型，通过使用物理可感知的机器学习来实现基于人工智能的电磁瞬态模拟。其贡献有三个方面：（1）形成了能够在相域中表示感应机的NeuIM；（2）一种能够捕捉快速和缓慢感应机动力学的物理可感知神经网络；和（3）一种数据-物理结合的混合NeuIM方法，能够适应不同数据可用性的情况。大量案例研究验证了NeuIM的效能，尤其是在纯数据驱动方法之上的优势。

    This rapid communication devises a Neural Induction Machine (NeuIM) model, which pilots the use of physics-informed machine learning to enable AI-based electromagnetic transient simulations. The contributions are threefold: (1) a formation of NeuIM to represent the induction machine in phase domain; (2) a physics-informed neural network capable of capturing fast and slow IM dynamics even in the absence of data; and (3) a data-physics-integrated hybrid NeuIM approach which is adaptive to various levels of data availability. Extensive case studies validate the efficacy of NeuIM and in particular, its advantage over purely data-driven approaches.
    
[^80]: G4SATBench: 用图神经网络评估和改进SAT求解的基准研究

    G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks. (arXiv:2309.16941v1 [cs.LG])

    [http://arxiv.org/abs/2309.16941](http://arxiv.org/abs/2309.16941)

    G4SATBench是第一个为基于图神经网络的SAT求解器提供全面评估框架的基准研究。它提供了一个大而多样的SAT数据集，并基于各种预测任务、训练目标和推理算法对广泛的GNN模型进行了测试和比较。通过与搜索型SAT求解器中的启发式方法进行比较，它还揭示了基于GNN的SAT求解器的学习能力以及其优缺点。

    

    最近，图神经网络(GNNs)作为解决布尔可满足性问题(SAT)的一种有希望的方法出现了，为传统的回溯或局部搜索SAT求解器提供了潜在的替代方案。然而，尽管这个领域的文献数量不断增长，但仍然缺乏一个统一的数据集和公正的基准来评估和比较现有的方法。为了填补这个关键的空白，我们提出了G4SATBench，这是第一个为基于GNN的SAT求解器建立全面评估框架的基准研究。在G4SATBench中，我们精心策划了一个包含7个问题和3个难度级别的大型多样化SAT数据集，并在各种预测任务、训练目标和推理算法下对广泛的GNN模型进行基准测试。为了探索基于GNN的SAT求解器的学习能力，并理解其优点和局限性，我们还将它们的求解过程与基于搜索的SAT求解器中的启发式方法进行比较。

    Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers
    
[^81]: PC-Adapter: 基于拟合伪标签的点云领域自适应的拓扑感知适配器

    PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-label. (arXiv:2309.16936v1 [cs.CV])

    [http://arxiv.org/abs/2309.16936](http://arxiv.org/abs/2309.16936)

    PC-Adapter是一种拓扑感知适配器，通过保留全局形状信息和学习目标领域的局部特征，实现了点云领域自适应。还提出了一种分层扩展的伪标签矫正策略，以解决伪标签的误类问题。

    

    由于物体尺度，传感器角度和自遮挡引起的数据分布变化，理解从真实世界中捕获的点云具有挑战性。以往的研究通过结合自监督学习，自训练和对抗训练等最新学习原则来解决这个问题，但这导致了显著的计算开销。为了简洁而强大地对点云进行领域自适应，我们重新思考了点云数据在领域偏移场景下的独特挑战，并发现保持源领域的全局几何形状信息以及目标伪标签趋势对源标签分布有偏的重要性。在我们的观察下，我们提出了一个适配器引导的领域自适应方法PC-Adapter，通过基于注意力机制的适配器保留源领域的全局形状信息，同时通过另一个配备了图卷积的适配器学习目标领域的局部特征。此外，我们提出了一种分层扩展的伪标签矫正策略，有效减少伪标签的误类问题。

    Understanding point clouds captured from the real-world is challenging due to shifts in data distribution caused by varying object scales, sensor angles, and self-occlusion. Prior works have addressed this issue by combining recent learning principles such as self-supervised learning, self-training, and adversarial training, which leads to significant computational overhead.Toward succinct yet powerful domain adaptation for point clouds, we revisit the unique challenges of point cloud data under domain shift scenarios and discover the importance of the global geometry of source data and trends of target pseudo-labels biased to the source label distribution. Motivated by our observations, we propose an adapter-guided domain adaptation method, PC-Adapter, that preserves the global shape information of the source domain using an attention-based adapter, while learning the local characteristics of the target domain via another adapter equipped with graph convolution. Additionally, we propo
    
[^82]: TranDRL：一种基于Transformer驱动的深度强化学习支持的预防性维护框架

    TranDRL: A Transformer-Driven Deep Reinforcement Learning Enabled Prescriptive Maintenance Framework. (arXiv:2309.16935v1 [cs.LG])

    [http://arxiv.org/abs/2309.16935](http://arxiv.org/abs/2309.16935)

    TranDRL是一种基于Transformer驱动的深度强化学习支持的预防性维护框架，结合了复杂时间模式捕捉和经济高效维护建议，显著提高了剩余寿命（RUL）预测准确性和维护行动优化。

    

    工业系统需要可靠的预测性维护策略来提高运营效率并减少停机时间。本文介绍了一种新颖的综合框架，利用Transformer神经网络和深度强化学习（DRL）算法来优化维护行动。我们的方法采用Transformer模型来有效捕捉传感器数据中的复杂时间模式，从而准确预测设备的剩余寿命（RUL）。同时，我们框架中的DRL组件提供了经济高效和及时的维护建议。我们在NASA C-MPASS数据集上验证了我们框架的有效性，结果显示在RUL预测准确性和维护行动优化方面取得了显著进展。因此，我们的创新方法为预防性维护提供了一种创新的数据驱动方法，解决了工业运营中的关键挑战，并带来了更多发展机遇。

    Industrial systems demand reliable predictive maintenance strategies to enhance operational efficiency and reduce downtime. This paper introduces a novel, integrated framework that leverages the power of transformer neural networks and deep reinforcement learning (DRL) algorithms to optimize maintenance actions. Our approach employs the transformer model to effectively capture complex temporal patterns in sensor data, thereby accurately predicting the Remaining Useful Life (RUL) of equipment. Simultaneously, the DRL component of our framework provides cost-effective and timely maintenance recommendations. We validate the efficacy of our framework on the NASA C-MPASS dataset, where it demonstrates significant advancements in both RUL prediction accuracy and the optimization of maintenance actions. Consequently, our pioneering approach provides an innovative data-driven methodology for prescriptive maintenance, addressing key challenges in industrial operations and leading the way to mor
    
[^83]: 对称性导致学习的结构性约束

    Symmetry Leads to Structured Constraint of Learning. (arXiv:2309.16932v1 [cs.LG])

    [http://arxiv.org/abs/2309.16932](http://arxiv.org/abs/2309.16932)

    本研究揭示了损失函数对称性对机器学习模型的学习行为至关重要，引入的每个镜像对称性都会导致一种结构性约束，可以用于实现稀疏性、低秩性和同质集成，并提供了解释网络塑性丧失和崩溃现象的理论框架。

    

    由于常见的架构设计，对称性在当代神经网络中广泛存在。在这项工作中，我们揭示了损失函数对称性对影响机器学习模型的学习行为的重要性。我们证明了损失函数的每个镜像对称性都会导致一种结构性约束，当权重衰减或梯度噪声较大时，这种约束将成为首选解。作为直接推论，我们展示了重新缩放对称性导致稀疏性，旋转对称性导致低秩性，置换对称性导致同质集成。然后，我们展示了理论框架可以解释神经网络中的可塑性丧失和各种崩溃现象，并提出了如何利用对称性设计可微分实施硬性约束的算法。

    Due to common architecture designs, symmetries exist extensively in contemporary neural networks. In this work, we unveil the importance of the loss function symmetries in affecting, if not deciding, the learning behavior of machine learning models. We prove that every mirror symmetry of the loss function leads to a structured constraint, which becomes a favored solution when either the weight decay or gradient noise is large. As direct corollaries, we show that rescaling symmetry leads to sparsity, rotation symmetry leads to low rankness, and permutation symmetry leads to homogeneous ensembling. Then, we show that the theoretical framework can explain the loss of plasticity and various collapse phenomena in neural networks and suggest how symmetries can be used to design algorithms to enforce hard constraints in a differentiable way.
    
[^84]: 学习接受帮助：干预感知的概念嵌入模型

    Learning to Receive Help: Intervention-Aware Concept Embedding Models. (arXiv:2309.16928v1 [cs.LG])

    [http://arxiv.org/abs/2309.16928](http://arxiv.org/abs/2309.16928)

    这项研究提出了一种干预感知的概念嵌入模型，用于提高神经架构对概念干预的响应性，并解决了概念干预顺序和模型架构的依赖性的问题。

    

    概念瓶颈模型（CBMs）通过使用一组高级概念构建和解释神经架构的预测，以解决其不透明性的问题。这些模型的一个特殊属性是它们允许概念干预，用户可以纠正被错误预测的概念，从而提高模型的性能。然而，最近的研究表明，干预有效性可能严重依赖于干预概念的顺序以及模型的架构和训练超参数。我们认为，这源于CBM在训练时缺乏模型适应概念干预的激励。为了解决这个问题，我们提出了干预感知的概念嵌入模型（IntCEMs），这是一种基于CBM的新型架构和训练范式，可以提高模型对测试时干预的响应性。我们的模型以端到端的方式学习了一个概念干预策略，从中可以采样有意义的干预轨迹。

    Concept Bottleneck Models (CBMs) tackle the opacity of neural architectures by constructing and explaining their predictions using a set of high-level concepts. A special property of these models is that they permit concept interventions, wherein users can correct mispredicted concepts and thus improve the model's performance. Recent work, however, has shown that intervention efficacy can be highly dependent on the order in which concepts are intervened on and on the model's architecture and training hyperparameters. We argue that this is rooted in a CBM's lack of train-time incentives for the model to be appropriately receptive to concept interventions. To address this, we propose Intervention-aware Concept Embedding models (IntCEMs), a novel CBM-based architecture and training paradigm that improves a model's receptiveness to test-time interventions. Our model learns a concept intervention policy in an end-to-end fashion from where it can sample meaningful intervention trajectories a
    
[^85]: 联邦学习的模式连通性和数据异质性

    Mode Connectivity and Data Heterogeneity of Federated Learning. (arXiv:2309.16923v1 [cs.LG])

    [http://arxiv.org/abs/2309.16923](http://arxiv.org/abs/2309.16923)

    本研究通过模式连通性的经验和理论研究发现，减少数据异质性可以增加客户端和全局模式之间的连通性，建立了全局模式连通性的定量界限。

    

    联邦学习（FL）可以让多个客户端在保持数据私密性的同时进行模型训练。以往的研究表明，客户端之间的数据异质性导致了更新之间的漂移。然而，对于客户端和全局模式之间的关系研究较少，不清楚这些更新漂移到了哪里。我们通过使用模式连通性进行经验和理论研究，模式连通性可以衡量在不同模式之间的参数路径上性能的变化（即连通性）。经验证明，减少数据异质性使得不同路径上的连通性更加相似，形成了客户端和全局模式之间更多的低误差重叠。我们还发现，在线性连接两个全局模式时存在连通性障碍，而考虑非线性模式连通性后连通性障碍消失。理论上，我们使用均场理论或dropout方法，建立了全局模式连通性的定量界限。

    Federated learning (FL) enables multiple clients to train a model while keeping their data private collaboratively. Previous studies have shown that data heterogeneity between clients leads to drifts across client updates. However, there are few studies on the relationship between client and global modes, making it unclear where these updates end up drifting. We perform empirical and theoretical studies on this relationship by utilizing mode connectivity, which measures performance change (i.e., connectivity) along parametric paths between different modes. Empirically, reducing data heterogeneity makes the connectivity on different paths more similar, forming more low-error overlaps between client and global modes. We also find that a barrier to connectivity occurs when linearly connecting two global modes, while it disappears with considering non-linear mode connectivity. Theoretically, we establish a quantitative bound on the global-mode connectivity using mean-field theory or dropou
    
[^86]: ACGAN-GNNExplainer：用于图神经网络的辅助条件生成解释器

    ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks. (arXiv:2309.16918v1 [cs.LG])

    [http://arxiv.org/abs/2309.16918](http://arxiv.org/abs/2309.16918)

    本论文提出了一种新的图神经网络解释方法ACGAN-GNNExplainer，将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域。通过利用生成器为原始输入图生成解释，并借助鉴别器监督生成过程，提高解释的准确性和保真度。实验证明了该方法的有效性和实用性。

    

    图神经网络（GNNs）已经在各种实际应用中证明了其有效性，但其基本机制仍然是一个谜。为了解决这个挑战并实现可靠的决策，近年来提出了许多GNN解释器。然而，这些方法常常面临一些限制，包括对特定实例的依赖性，对未见过的图的一般性不足，可能产生无效的解释以及生成过程中产生的不充分的保真度。为了克服这些限制，本文将辅助分类器生成对抗网络（ACGAN）引入到GNN解释领域，并提出了一种新的GNN解释器ACGAN-GNNExplainer。我们的方法利用生成器为原始输入图生成解释，并运用鉴别器来监督生成过程，确保解释的保真度并提高准确性。实验评估分别在合成数据集和真实世界的图数据集上进行。

    Graph neural networks (GNNs) have proven their efficacy in a variety of real-world applications, but their underlying mechanisms remain a mystery. To address this challenge and enable reliable decision-making, many GNN explainers have been proposed in recent years. However, these methods often encounter limitations, including their dependence on specific instances, lack of generalizability to unseen graphs, producing potentially invalid explanations, and yielding inadequate fidelity. To overcome these limitations, we, in this paper, introduce the Auxiliary Classifier Generative Adversarial Network (ACGAN) into the field of GNN explanation and propose a new GNN explainer dubbed~\emph{ACGAN-GNNExplainer}. Our approach leverages a generator to produce explanations for the original input graphs while incorporating a discriminator to oversee the generation process, ensuring explanation fidelity and improving accuracy. Experimental evaluations conducted on both synthetic and real-world graph
    
[^87]: ONNXExplainer:基于ONNX的通用框架，使用Shapley值解释神经网络

    ONNXExplainer: an ONNX Based Generic Framework to Explain Neural Networks Using Shapley Values. (arXiv:2309.16916v1 [cs.LG])

    [http://arxiv.org/abs/2309.16916](http://arxiv.org/abs/2309.16916)

    ONNXExplainer是一个基于ONNX的通用框架，使用Shapley值解释神经网络。它提供了自动微分和优化方法，实现了一次性部署和高效的解释计算。

    

    理解神经网络模型为什么会做出某些决策与推理性能一样重要。已经提出了各种方法来帮助解释神经网络模型的预测，其中Shapley值最受欢迎。SHAP包是解释使用TensorFlow或PyTorch实现的神经网络的Shapley值的领先实现，但缺乏跨平台支持、一次性部署且效率低下。为了解决这些问题，我们提出了ONNXExplainer，它是一个使用ONNX生态系统中的Shapley值来解释神经网络的通用框架。在ONNXExplainer中，我们开发了自己的自动微分和优化方法，不仅实现了神经网络推理和解释的一次性部署，还显著提高了解释的效率，并减少了内存消耗。为了公平比较目的，我们还在TensorFlow和PyTorch中实现了相同的优化。

    Understanding why a neural network model makes certain decisions can be as important as the inference performance. Various methods have been proposed to help practitioners explain the prediction of a neural network model, of which Shapley values are most popular. SHAP package is a leading implementation of Shapley values to explain neural networks implemented in TensorFlow or PyTorch but lacks cross-platform support, one-shot deployment and is highly inefficient. To address these problems, we present the ONNXExplainer, which is a generic framework to explain neural networks using Shapley values in the ONNX ecosystem. In ONNXExplainer, we develop its own automatic differentiation and optimization approach, which not only enables One-Shot Deployment of neural networks inference and explanations, but also significantly improves the efficiency to compute explanation with less memory consumption. For fair comparison purposes, we also implement the same optimization in TensorFlow and PyTorch
    
[^88]: 多变量时间序列异常检测的算法补救方法

    Algorithmic Recourse for Anomaly Detection in Multivariate Time Series. (arXiv:2309.16896v1 [cs.LG])

    [http://arxiv.org/abs/2309.16896](http://arxiv.org/abs/2309.16896)

    本论文提出了一种算法补救框架，RecAD，用于多变量时间序列异常检测。通过推荐以最小成本修复异常时间序列，该框架能够帮助领域专家理解如何修复异常行为，并在实验中证明了其有效性。

    

    由于广泛的应用领域，多变量时间序列的异常检测已经受到广泛研究。多变量时间序列中的异常通常表示临界事件，例如系统故障或外部攻击。因此，除了在异常检测方面有效之外，推荐异常缓解行动在实践中也很重要但研究不足。在这项工作中，我们专注于时间序列异常检测中的算法补救方法，即推荐以最小成本修复异常时间序列，以便领域专家可以理解如何修复异常行为。为此，我们提出了一种算法补救框架，称为RecAD，可以推荐翻转异常时间步骤的补救行动。对两个合成数据集和一个真实数据集的实验结果显示了我们框架的有效性。

    Anomaly detection in multivariate time series has received extensive study due to the wide spectrum of applications. An anomaly in multivariate time series usually indicates a critical event, such as a system fault or an external attack. Therefore, besides being effective in anomaly detection, recommending anomaly mitigation actions is also important in practice yet under-investigated. In this work, we focus on algorithmic recourse in time series anomaly detection, which is to recommend fixing actions on abnormal time series with a minimum cost so that domain experts can understand how to fix the abnormal behavior. To this end, we propose an algorithmic recourse framework, called RecAD, which can recommend recourse actions to flip the abnormal time steps. Experiments on two synthetic and one real-world datasets show the effectiveness of our framework.
    
[^89]: 使用多元时间序列转换器获取风险投资和成长资本的投资目标

    Sourcing Investment Targets for Venture and Growth Capital Using Multivariate Time Series Transformer. (arXiv:2309.16888v1 [cs.LG])

    [http://arxiv.org/abs/2309.16888](http://arxiv.org/abs/2309.16888)

    这个论文介绍了一种新颖的方法，利用Transformer-based Multivariate Time Series Classifier (TMTSC)来预测风险投资和成长资本的候选公司的成功可能性，以优化投资目标的选择。通过对相关的方法进行了全面回顾和实验验证，证明了该方法的有效性。

    

    本文探讨了数据驱动方法在私募股权（PE）行业中的应用，特别是在为风险投资（VC）和成长资本（GC）寻找投资目标（即公司）方面。我们对相关方法进行了全面的回顾，并提出了一种新颖的方法，利用基于Transformer的多元时间序列分类器（TMTSC）来预测任何候选公司的成功可能性。我们的研究目标是通过将寻找投资问题正式定义为多元时间序列分类任务，优化VC和GC投资的寻找效果。我们依次介绍了我们实现的关键组成部分，这些部分共同 contribut 到了在VC/GC寻找中成功应用TMTSC：输入特征、模型架构、优化目标以及基于投资者的数据增强和划分。我们在四个数据集上进行了大量实验，并将其与三个流行的基准线进行了对比。

    This paper addresses the growing application of data-driven approaches within the Private Equity (PE) industry, particularly in sourcing investment targets (i.e., companies) for Venture Capital (VC) and Growth Capital (GC). We present a comprehensive review of the relevant approaches and propose a novel approach leveraging a Transformer-based Multivariate Time Series Classifier (TMTSC) for predicting the success likelihood of any candidate company. The objective of our research is to optimize sourcing performance for VC and GC investments by formally defining the sourcing problem as a multivariate time series classification task. We consecutively introduce the key components of our implementation which collectively contribute to the successful application of TMTSC in VC/GC sourcing: input features, model architecture, optimization target, and investor-centric data augmentation and split. Our extensive experiments on four datasets, benchmarked towards three popular baselines, demonstrat
    
[^90]: 增强随机平滑的Lipschitz-方差-边界权衡

    The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])

    [http://arxiv.org/abs/2309.16883](http://arxiv.org/abs/2309.16883)

    本文提出了一个增强随机平滑的方法，通过研究随机平滑引入的方差与分类器的Lipschitz常数和边界之间的关系，以及采用单纯形投影技术来增加认证鲁棒半径。

    

    面对噪声输入和对抗性攻击时，深度神经网络的实际应用受到其不稳定的预测的阻碍。在这种情况下，认证半径是模型鲁棒性的关键指标。然而，如何设计一个具有足够认证半径的高效分类器呢？随机平滑通过在输入中注入噪声来获得平滑且更鲁棒的分类器的框架提供了有希望的解决方案。本文首先展示了随机平滑引入的方差与分类器的另外两个重要属性，即其Lipschitz常数和边界之间的密切关系。更具体地说，我们的工作强调了基分类器的Lipschitz常数对平滑分类器和经验方差的双重影响。此外，为了增加认证鲁棒半径，我们引入了一种不同的单纯形投影技术，以便通过Bernst的方差-边界权衡来利用基分类器。

    Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
    
[^91]: 消息在时间中的传播：一种用于保留时序建模中序列依赖性的算法

    Message Propagation Through Time: An Algorithm for Sequence Dependency Retention in Time Series Modeling. (arXiv:2309.16882v1 [cs.LG])

    [http://arxiv.org/abs/2309.16882](http://arxiv.org/abs/2309.16882)

    本文提出了一种名为消息传播通过时间（MPTT）的算法，用于在时间序列建模中保留序列依赖性。MPTT通过异步管理初始隐藏状态，并采用三种策略过滤过期信息，以生成信息丰富的隐藏状态，从而提高RNN的训练效果。

    

    时间序列建模是科学中一个关键领域，然而在使用传统的小批量训练策略（假设样本独立同分布且RNN的隐藏状态初始化为零）训练机器学习模型（如RNN）时，经常会遇到挑战。传统的假设忽视了样本间的时间依赖关系，导致性能较差。本文提出了一种名为消息传播通过时间（MPTT）的算法，能够有效地融入长时间依赖关系，同时保持相对于有状态解决方案更快的训练时间。MPTT利用两个内存模块异步管理RNN的初始隐藏状态，促进样本之间的无缝信息交换，并允许在epochs期间使用多样的小批量。MPTT进一步实施了三种策略来过滤过期的信息并保留隐藏状态中的关键信息，以生成信息丰富的初始隐藏状态，从而方便RNN的训练。

    Time series modeling, a crucial area in science, often encounters challenges when training Machine Learning (ML) models like Recurrent Neural Networks (RNNs) using the conventional mini-batch training strategy that assumes independent and identically distributed (IID) samples and initializes RNNs with zero hidden states. The IID assumption ignores temporal dependencies among samples, resulting in poor performance. This paper proposes the Message Propagation Through Time (MPTT) algorithm to effectively incorporate long temporal dependencies while preserving faster training times relative to the stateful solutions. MPTT utilizes two memory modules to asynchronously manage initial hidden states for RNNs, fostering seamless information exchange between samples and allowing diverse mini-batches throughout epochs. MPTT further implements three policies to filter outdated and preserve essential information in the hidden states to generate informative initial hidden states for RNNs, facilitati
    
[^92]: 揭示对抗扰动中隐藏的人类可识别特征

    Investigating Human-Identifiable Features Hidden in Adversarial Perturbations. (arXiv:2309.16878v1 [cs.LG])

    [http://arxiv.org/abs/2309.16878](http://arxiv.org/abs/2309.16878)

    该研究揭示了对抗扰动中隐藏的人类可识别特征，并发现了掩蔽效应和生成效应在不同类型攻击中的表现。通过分析多个攻击算法生成的扰动，发现它们在一定程度上存在相似性。

    

    神经网络在各种机器学习任务中表现出色，但对抗扰动并不免疫。这种脆弱性对实际应用有着重要影响。虽然已经进行了大量的研究，但神经网络为何易受对抗攻击的根本原因尚未完全理解。我们的研究的核心是探索对抗扰动中的人类可识别特征，我们使用了五种攻击算法和三个数据集进行实验。此外，我们发现了在人类可识别特征中表现出的两种不同效应。在非定向攻击中，掩蔽效应更为显著，而在定向攻击中，生成效应更为常见。通过像素级注释，我们提取这些特征并证明它们能够破坏目标模型。此外，我们的研究结果表明，多个攻击算法生成的扰动在平均情况下存在显著的相似性。

    Neural networks perform exceedingly well across various machine learning tasks but are not immune to adversarial perturbations. This vulnerability has implications for real-world applications. While much research has been conducted, the underlying reasons why neural networks fall prey to adversarial attacks are not yet fully understood. Central to our study, which explores up to five attack algorithms across three datasets, is the identification of human-identifiable features in adversarial perturbations. Additionally, we uncover two distinct effects manifesting within human-identifiable features. Specifically, the masking effect is prominent in untargeted attacks, while the generation effect is more common in targeted attacks. Using pixel-level annotations, we extract such features and demonstrate their ability to compromise target models. In addition, our findings indicate a notable extent of similarity in perturbations across different attack algorithms when averaged over multiple m
    
[^93]: LEF: LiDAR 3D物体检测的迟到早期时间融合

    LEF: Late-to-Early Temporal Fusion for LiDAR 3D Object Detection. (arXiv:2309.16870v1 [cs.CV])

    [http://arxiv.org/abs/2309.16870](http://arxiv.org/abs/2309.16870)

    LEF是一种用于LiDAR 3D物体检测的迟到早期时间融合方案，通过将目标感知的潜在嵌入融合到早期阶段，能够更好地捕捉具有挑战性的对象的形状和姿态。

    

    我们提出了一种用于使用时间LiDAR点云进行3D物体检测的迟到早期循环特征融合方案。我们的主要动机是将目标感知的潜在嵌入融合到3D物体检测器的早期阶段。与直接从原始点学习相比，这种特征融合策略使模型能够更好地捕捉具有挑战性的对象的形状和姿态。我们的方法以一种循环的方式进行迟到早期特征融合。这通过在时间校准和对齐的稀疏柱状令牌上施加基于窗口的注意力块来实现。利用鸟瞰图前景柱状分割，我们将模型需要融合到当前帧中的稀疏历史特征数量减少了10倍。我们还提出了一种随机长度的FrameDrop训练技术，该技术可以在推断过程中根据需求调整帧长度，以提高性能而无需重新训练。我们在广泛采用的Waymo Open Dataset和d数据集上评估了我们的方法

    We propose a late-to-early recurrent feature fusion scheme for 3D object detection using temporal LiDAR point clouds. Our main motivation is fusing object-aware latent embeddings into the early stages of a 3D object detector. This feature fusion strategy enables the model to better capture the shapes and poses for challenging objects, compared with learning from raw points directly. Our method conducts late-to-early feature fusion in a recurrent manner. This is achieved by enforcing window-based attention blocks upon temporally calibrated and aligned sparse pillar tokens. Leveraging bird's eye view foreground pillar segmentation, we reduce the number of sparse history features that our model needs to fuse into its current frame by 10$\times$. We also propose a stochastic-length FrameDrop training technique, which generalizes the model to variable frame lengths at inference for improved performance without retraining. We evaluate our method on the widely adopted Waymo Open Dataset and d
    
[^94]: 前言：一种基于数据驱动的体积化先验用于少样本超高分辨率人脸合成

    Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis. (arXiv:2309.16859v1 [cs.CV])

    [http://arxiv.org/abs/2309.16859](http://arxiv.org/abs/2309.16859)

    本文提出了一种基于数据驱动的体积化人脸先验，使得能合成先前训练分布外的超高分辨率的新视图。通过训练身份数量有限，结合基于稀疏标记点的3D对齐，该模型能够学习到一个平滑的几何和外观的潜在空间，并通过拟合到2-3个摄像机视图来获取高质量的新对象的体积化表示。

    

    NeRFs已经实现了包括复杂的头发和皮肤在内的人脸高度真实的合成，这些方法通常需要大量的多视图输入图像，使该过程对硬件要求较高且繁琐，限制了它在无约束环境中的适用性。我们提出了一种新颖的体积化人脸先验，使得能合成先前训练分布外的超高分辨率的新视图。这个先验模型由一个与身份相关的NeRF组成，经过在已知摄像机标定的多样人类低分辨率多视图图像数据集上训练。尽管训练身份数量有限，通过简单的基于稀疏标记点的3D对齐，我们的模型能够学习到一个平滑的几何和外观的潜在空间。通过将模型拟合到任意分辨率的2或3个摄像机视图，可以获得高质量的新对象的体积化表示。

    NeRFs have enabled highly realistic synthesis of human faces including complex appearance and reflectance effects of hair and skin. These methods typically require a large number of multi-view input images, making the process hardware intensive and cumbersome, limiting applicability to unconstrained settings. We propose a novel volumetric human face prior that enables the synthesis of ultra high-resolution novel views of subjects that are not part of the prior's training distribution. This prior model consists of an identity-conditioned NeRF, trained on a dataset of low-resolution multi-view images of diverse humans with known camera calibration. A simple sparse landmark-based 3D alignment of the training dataset allows our model to learn a smooth latent space of geometry and appearance despite a limited number of training identities. A high-quality volumetric representation of a novel subject can be obtained by model fitting to 2 or 3 camera views of arbitrary resolution. Importantly,
    
[^95]: Transductive Learning的尖锐泛化：一种Transductive Local Rademacher Complexity方法

    Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])

    [http://arxiv.org/abs/2309.16858](http://arxiv.org/abs/2309.16858)

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们利用变量的方差信息构建了TLRC，并将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。

    

    我们引入了一种新的工具，Transductive Local Rademacher Complexity (TLRC)，用于分析transductive learning方法的泛化性能并推动新的transductive learning算法的发展。我们的工作将传统的local rademacher complexity (LRC)的思想扩展到了transductive设置中，相对于典型的LRC方法在归纳设置中的分析有了相当大的变化。我们提出了一种基于Rademacher complex的局部化工具，可以应用于各种transductive learning问题，并在适当条件下得到了尖锐的界限。与LRC的发展类似，我们通过从独立变量的方差信息开始构建TLRC，将transductive learning模型的预测函数类分为多个部分，每个部分的Rademacher complexity上界由一个子根函数给出，并限制了每个部分中所有函数的方差。经过精心设计的...

    We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
    
[^96]: 在物联网中应用联合学习实现超个性化

    Applications of Federated Learning in IoT for Hyper Personalisation. (arXiv:2309.16854v1 [cs.LG])

    [http://arxiv.org/abs/2309.16854](http://arxiv.org/abs/2309.16854)

    在物联网中应用联合学习，解决了大规模数据利用问题，并实现了超个性化的个性化体验。

    

    数十亿个物联网设备正在部署，利用更快的互联网和访问更多节点的机会。这些设备不断产生大量数据，但并没有得到有效利用。使用联合学习在多个客户端上训练机器学习模型，而无需将其传输到中央服务器。我们探讨如何利用这样的模型来实现超个性化水平，从而实现前所未有的个性化体验。

    Billions of IoT devices are being deployed, taking advantage of faster internet, and the opportunity to access more endpoints. Vast quantities of data are being generated constantly by these devices but are not effectively being utilised. Using FL training machine learning models over these multiple clients without having to bring it to a central server. We explore how to use such a model to implement ultra levels of personalization unlike before
    
[^97]: 具有偏移非局部搜索的时空注意力

    Space-Time Attention with Shifted Non-Local Search. (arXiv:2309.16849v1 [cs.CV])

    [http://arxiv.org/abs/2309.16849](http://arxiv.org/abs/2309.16849)

    本文提出了一种名为偏移非局部搜索的方法，通过结合非局部搜索的质量和预测的偏移范围进行搜索，以纠正小的空间误差。与先前的工作相比，我们的方法在内存消耗上减少了10倍以上，并且速度提高了3倍以上。

    

    由于视频中物体的运动，有效计算注意力图对于视频来说是具有挑战性的。虽然标准的非局部搜索对于每个查询点周围的窗口具有高质量，但窗口的大小无法容纳运动。长程运动的方法使用辅助网络预测与每个查询位置的偏移量最相似的关键坐标。然而，准确预测此偏移的光流场仍然具有挑战性，即使对于大规模网络也是如此。小的空间不准确性会严重影响注意力模块的质量。本文提出了一种将非局部搜索的质量与预测的偏移量范围相结合的搜索策略。这种方法被命名为偏移非局部搜索，它在预测的偏移周围执行一个小的网格搜索，以纠正小的空间误差。我们的方法的原地计算消耗的内存比先前的工作少了10倍，速度快了3倍以上。在实验中，纠正了小的空间误差

    Efficiently computing attention maps for videos is challenging due to the motion of objects between frames. While a standard non-local search is high-quality for a window surrounding each query point, the window's small size cannot accommodate motion. Methods for long-range motion use an auxiliary network to predict the most similar key coordinates as offsets from each query location. However, accurately predicting this flow field of offsets remains challenging, even for large-scale networks. Small spatial inaccuracies significantly impact the attention module's quality. This paper proposes a search strategy that combines the quality of a non-local search with the range of predicted offsets. The method, named Shifted Non-Local Search, executes a small grid search surrounding the predicted offsets to correct small spatial errors. Our method's in-place computation consumes 10 times less memory and is over 3 times faster than previous work. Experimentally, correcting the small spatial err
    
[^98]: 最优非线性性能改进随机特征的泛化性能

    Optimal Nonlinearities Improve Generalization Performance of Random Features. (arXiv:2309.16846v1 [cs.LG])

    [http://arxiv.org/abs/2309.16846](http://arxiv.org/abs/2309.16846)

    通过研究等效模型的参数，本研究发现获得的参数可以定义一组最优非线性性，如二阶多项式和分段线性函数。这些非线性性能优化了泛化性能，无论其实际形式如何，对回归和分类问题均有效。

    

    通过非线性激活函数的随机特征模型在训练和泛化误差方面已被证明与高斯模型渐进等效。等效模型的分析揭示了激活函数发挥的重要但尚未完全理解的作用。为了解决这个问题，我们研究等效模型的“参数”，以实现对给定监督学习问题的改进的泛化性能。我们展示了从高斯模型获取的参数使我们能够定义一组最优非线性性。我们提供了这组最优非线性性的两个示例类，例如二阶多项式和分段线性函数。这些函数被优化以改进泛化性能，无论其实际形式如何。我们对回归和分类问题进行了实验，包括合成和真实数据（如CIFAR10）。我们的数值结果验证了优化的非线性性能优于wid。

    Random feature model with a nonlinear activation function has been shown to perform asymptotically equivalent to a Gaussian model in terms of training and generalization errors. Analysis of the equivalent model reveals an important yet not fully understood role played by the activation function. To address this issue, we study the "parameters" of the equivalent model to achieve improved generalization performance for a given supervised learning problem. We show that acquired parameters from the Gaussian model enable us to define a set of optimal nonlinearities. We provide two example classes from this set, e.g., second-order polynomial and piecewise linear functions. These functions are optimized to improve generalization performance regardless of the actual form. We experiment with regression and classification problems, including synthetic and real (e.g., CIFAR10) data. Our numerical results validate that the optimized nonlinearities achieve better generalization performance than wid
    
[^99]: 个体偏好稳定聚类的常数逼近

    Constant Approximation for Individual Preference Stable Clustering. (arXiv:2309.16840v1 [cs.DS])

    [http://arxiv.org/abs/2309.16840](http://arxiv.org/abs/2309.16840)

    本文提出了个体偏好稳定聚类的常数逼近算法，使得每个数据点到其所属聚类的平均距离不超过到任何其他聚类的平均距离的$\alpha$倍，并且给出了泛化到超过平均距离的个体偏好稳定的算法。

    

    个体偏好稳定性是由Ahmadi等人（ICML 2022）引入的一种受到稳定性和公平性约束启发的自然聚类目标。如果每个数据点到其所属聚类的平均距离不超过到任何其他聚类的平均距离的$\alpha$倍，则称聚类为$\alpha$-个体偏好稳定。不幸的是，确定数据集是否存在$1$-个体偏好稳定的聚类是 NP-Hard 问题。此外，在本研究之前，是否总是存在$ o(n) $-个体偏好稳定的聚类是未知的，因为先前的最优结果只保证了$O(n)$-个体偏好稳定的聚类。我们填补了这个理解上的差距，并展示了对于一般的度量，总是存在$O(1)$-个体偏好稳定的聚类，并给出了一个高效的算法来输出这样的聚类。我们还引入了超过平均距离的个体偏好稳定的泛化，并在考虑聚类内部和聚类间的最大距离和最小距离的情况下给出了高效、近乎最优的算法。

    Individual preference (IP) stability, introduced by Ahmadi et al. (ICML 2022), is a natural clustering objective inspired by stability and fairness constraints. A clustering is $\alpha$-IP stable if the average distance of every data point to its own cluster is at most $\alpha$ times the average distance to any other cluster. Unfortunately, determining if a dataset admits a $1$-IP stable clustering is NP-Hard. Moreover, before this work, it was unknown if an $o(n)$-IP stable clustering always \emph{exists}, as the prior state of the art only guaranteed an $O(n)$-IP stable clustering. We close this gap in understanding and show that an $O(1)$-IP stable clustering always exists for general metrics, and we give an efficient algorithm which outputs such a clustering. We also introduce generalizations of IP stability beyond average distance and give efficient, near-optimal algorithms in the cases where we consider the maximum and minimum distances within and between clusters.
    
[^100]: 医学图像处理中的不确定性传播和归因

    Propagation and Attribution of Uncertainty in Medical Imaging Pipelines. (arXiv:2309.16831v1 [cs.CV])

    [http://arxiv.org/abs/2309.16831](http://arxiv.org/abs/2309.16831)

    本文提出了一种在医学图像处理流程中传播不确定性的方法，能够聚合流程后期的不确定性，并获得后续模型预测的联合不确定性度量。同时，还能够报告流程中每个组件的内部、数据基础不确定性的贡献。

    

    不确定性估计提供了一种构建可解释的神经网络用于医学图像应用的方法，其中主要研究了针对特定任务的单个深度学习模型。在本文中，我们提出了一种在医学图像处理流程中传播不确定性的方法。这使我们能够聚合流程后期的不确定性，并获得后续模型预测的联合不确定性度量。此外，我们可以分别报告流程中每个组件的内部、数据基础不确定性的贡献。我们在一个实际的图像处理流程上演示了我们方法的有效性，该流程从下采样的脑部和膝盖磁共振（MR）图像重建，然后预测图像中的定量信息，例如脑容积、膝盖边缘或患者性别等。我们定量地显示传播的不确定性与输入不确定性相关。

    Uncertainty estimation, which provides a means of building explainable neural networks for medical imaging applications, have mostly been studied for single deep learning models that focus on a specific task. In this paper, we propose a method to propagate uncertainty through cascades of deep learning models in medical imaging pipelines. This allows us to aggregate the uncertainty in later stages of the pipeline and to obtain a joint uncertainty measure for the predictions of later models. Additionally, we can separately report contributions of the aleatoric, data-based, uncertainty of every component in the pipeline. We demonstrate the utility of our method on a realistic imaging pipeline that reconstructs undersampled brain and knee magnetic resonance (MR) images and subsequently predicts quantitative information from the images, such as the brain volume, or knee side or patient's sex. We quantitatively show that the propagated uncertainty is correlated with input uncertainty and com
    
[^101]: 无导数损失方法在求解偏微分方程中的分析

    An analysis of the derivative-free loss method for solving PDEs. (arXiv:2309.16829v1 [math.NA])

    [http://arxiv.org/abs/2309.16829](http://arxiv.org/abs/2309.16829)

    本研究分析了一种无导数损失方法在使用神经网络求解椭圆型偏微分方程的方法。我们发现训练损失偏差与时间间隔和空间梯度成正比，与行走者大小成反比，同时时间间隔必须足够长。我们提供了数值测试结果以支持我们的分析。

    

    本研究分析了无导数损失方法在使用神经网络求解一类椭圆型偏微分方程中的应用。无导数损失方法采用费曼-卡克公式，结合随机行走者及其对应的平均值。我们考察了费曼-卡克公式中与时间间隔相关的影响，以及行走者大小对计算效率、可训练性和采样误差的影响。我们的分析表明，训练损失偏差与时间间隔和神经网络的空间梯度成正比，与行走者大小成反比。同时，我们还表明时间间隔必须足够长才能训练网络。这些分析结果说明，在时间间隔的最优下界基础上，我们可以选择尽可能小的行走者大小。我们还提供了支持我们分析的数值测试。

    This study analyzes the derivative-free loss method to solve a certain class of elliptic PDEs using neural networks. The derivative-free loss method uses the Feynman-Kac formulation, incorporating stochastic walkers and their corresponding average values. We investigate the effect of the time interval related to the Feynman-Kac formulation and the walker size in the context of computational efficiency, trainability, and sampling errors. Our analysis shows that the training loss bias is proportional to the time interval and the spatial gradient of the neural network while inversely proportional to the walker size. We also show that the time interval must be sufficiently long to train the network. These analytic results tell that we can choose the walker size as small as possible based on the optimal lower bound of the time interval. We also provide numerical tests supporting our analysis.
    
[^102]: DNN分类器中的后训练过拟合缓解方法

    Post-Training Overfitting Mitigation in DNN Classifiers. (arXiv:2309.16827v1 [cs.LG])

    [http://arxiv.org/abs/2309.16827](http://arxiv.org/abs/2309.16827)

    该论文提出了一种后训练缓解方法，用于解决深度神经网络分类器中的过拟合问题。该方法通过限制神经激活边界来缓解后门数据污染导致的过拟合，并获得了性能改进。研究还提供了分析支持，表明基于最大边界的后训练正则化显著缓解了非恶意过拟合问题。

    

    深度神经网络（DNN）分类器中非恶意过拟合的已知来源包括：i）大的类别不平衡；ii）训练集多样性不足；iii）过度训练。最近的研究表明，后门数据污染也会导致过拟合，具有异常大的分类边界距离攻击者的目标类，特别是通过允许大信号在DNN中传播的（无界）ReLU激活。因此，提出了一种有效的后训练缓解方法（不需要了解训练集或训练过程），利用一个小的干净数据集，基于限制神经激活来解决后门问题。在完善该方法的基础上，我们特定阈值激活以限制最大边界（MMs），从而在后门缓解中获得性能收益。我们还提供了一些支持该缓解方法的分析。最重要的是，我们展示了基于最大边界的后训练正则化大大缓解了非恶意过拟合问题。

    Well-known (non-malicious) sources of overfitting in deep neural net (DNN) classifiers include: i) large class imbalances; ii) insufficient training-set diversity; and iii) over-training. In recent work, it was shown that backdoor data-poisoning also induces overfitting, with unusually large classification margins to the attacker's target class, mediated particularly by (unbounded) ReLU activations that allow large signals to propagate in the DNN. Thus, an effective post-training (with no knowledge of the training set or training process) mitigation approach against backdoors was proposed, leveraging a small clean dataset, based on bounding neural activations. Improving upon that work, we threshold activations specifically to limit maximum margins (MMs), which yields performance gains in backdoor mitigation. We also provide some analytical support for this mitigation approach. Most importantly, we show that post-training MM-based regularization substantially mitigates non-malicious ove
    
[^103]: FENDA-FL：异构临床数据的个性化联邦学习

    FENDA-FL: Personalized Federated Learning on Heterogeneous Clinical Datasets. (arXiv:2309.16825v1 [cs.LG])

    [http://arxiv.org/abs/2309.16825](http://arxiv.org/abs/2309.16825)

    该论文提出了一种针对异构临床数据的个性化联邦学习方法，实验证明该方法在性能上超越了现有的全局和个性化联邦学习技术，并且对FLamby基准进行了实质性改进和扩展。

    

    联邦学习（FL）被越来越认为是克服临床环境中数据孤立问题的关键方法。该研究在临床应用的FL研究中做出了三个重要方面的贡献。首先，提出了将FENDA方法（Kim等人，2016）扩展到FL的方法。在FLamby基准（du Terrail等人，2022a）和GEMINI数据集（Verma等人，2017）上进行的实验表明，该方法对异构临床数据具有鲁棒性，并且通常优于现有的全局和个性化FL技术。此外，实验结果在原有的FLamby基准上表示出实质性的改进，并扩展了这些基准以包括个性化FL方法的评估。最后，我们提倡建立一个全面的FL检查点和评估框架，以更好地反映实际环境并提供

    Federated learning (FL) is increasingly being recognized as a key approach to overcoming the data silos that so frequently obstruct the training and deployment of machine-learning models in clinical settings. This work contributes to a growing body of FL research specifically focused on clinical applications along three important directions. First, an extension of the FENDA method (Kim et al., 2016) to the FL setting is proposed. Experiments conducted on the FLamby benchmarks (du Terrail et al., 2022a) and GEMINI datasets (Verma et al., 2017) show that the approach is robust to heterogeneous clinical data and often outperforms existing global and personalized FL techniques. Further, the experimental results represent substantive improvements over the original FLamby benchmarks and expand such benchmarks to include evaluation of personalized FL methods. Finally, we advocate for a comprehensive checkpointing and evaluation framework for FL to better reflect practical settings and provide
    
[^104]: 用于线性函数逼近的多Bellman算子对$Q$-learning的收敛性的研究

    Multi-Bellman operator for convergence of $Q$-learning with linear function approximation. (arXiv:2309.16819v1 [cs.LG])

    [http://arxiv.org/abs/2309.16819](http://arxiv.org/abs/2309.16819)

    本文通过引入新的多Bellman算子，对线性函数逼近下的$Q$-learning进行了收敛性研究，并提出了多$Q$-learning算法。通过探索多Bellman算子的性质，我们找到了使其成为压缩映射的条件，获得了比传统Bellman算子更好的固定点保证。该算法收敛到多Bellman算子的不动点，可以得到任意精度的解。在验证过程中，我们将该方法应用于常用环境中，展示了其有效性和适用性。

    

    本文研究了线性函数逼近下$Q$-learning的收敛性。我们的主要贡献是引入了一种新颖的多Bellman算子，该算子扩展了传统的Bellman算子。通过探索该算子的性质，我们确定了在投影的多Bellman算子变为压缩映射时的条件，从而提供了比Bellman算子更好的固定点保证。为了利用这些洞察力，我们提出了具有线性函数逼近的多$Q$-learning算法。我们证明了该算法收敛到投影的多Bellman算子的不动点，从而获得了任意精度的解。最后，我们通过将该方法应用于众所周知的环境来验证我们的方法，展示了我们发现的有效性和适用性。

    We study the convergence of $Q$-learning with linear function approximation. Our key contribution is the introduction of a novel multi-Bellman operator that extends the traditional Bellman operator. By exploring the properties of this operator, we identify conditions under which the projected multi-Bellman operator becomes contractive, providing improved fixed-point guarantees compared to the Bellman operator. To leverage these insights, we propose the multi $Q$-learning algorithm with linear function approximation. We demonstrate that this algorithm converges to the fixed-point of the projected multi-Bellman operator, yielding solutions of arbitrary accuracy. Finally, we validate our approach by applying it to well-known environments, showcasing the effectiveness and applicability of our findings.
    
[^105]: PROSE: 使用多模态Transformer预测运算符和符号表达式

    PROSE: Predicting Operators and Symbolic Expressions using Multimodal Transformers. (arXiv:2309.16816v1 [cs.LG])

    [http://arxiv.org/abs/2309.16816](http://arxiv.org/abs/2309.16816)

    PROSE是一种能够从多模态输入到多模态输出的网络表示方法，可以用于预测非线性微分方程的数值解和符号表达式。

    

    使用神经网络近似非线性微分方程为各种科学计算任务提供了稳健高效的工具，包括实时预测、反问题、最优控制和代理模拟。以前的研究集中于通过两种方法将动力学系统嵌入到网络中：学习单个解算符（即从输入参数化函数映射到解的映射）或学习控制系统（即相对于状态变量的构成模型）。这两种方法都会得到相同基础数据或函数的不同表示。此外，鉴于一组微分方程经常具有共同的特征，我们寻求在广泛的方程中获得一个网络表示。我们的方法称为预测运算符和符号表达式（PROSE），它学习从多模态输入到多模态输出的映射，能够生成数值预测和...

    Approximating nonlinear differential equations using a neural network provides a robust and efficient tool for various scientific computing tasks, including real-time predictions, inverse problems, optimal controls, and surrogate modeling. Previous works have focused on embedding dynamical systems into networks through two approaches: learning a single solution operator (i.e., the mapping from input parametrized functions to solutions) or learning the governing system of equations (i.e., the constitutive model relative to the state variables). Both of these approaches yield different representations for the same underlying data or function. Additionally, observing that families of differential equations often share key characteristics, we seek one network representation across a wide range of equations. Our method, called Predicting Operators and Symbolic Expressions (PROSE), learns maps from multimodal inputs to multimodal outputs, capable of generating both numerical predictions and 
    
[^106]: GraB-sampler: 用于PyTorch的基于排列的最优随机梯度下降数据采样器

    GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch. (arXiv:2309.16809v1 [cs.LG])

    [http://arxiv.org/abs/2309.16809](http://arxiv.org/abs/2309.16809)

    该论文介绍了一个名为GraB-sampler的Python库，为社区提供了使用GraB算法的便利，并提出了5个不同的变体。最佳性能结果证明了GraB-sampler在训练时间和显存使用上的开销非常小。

    

    通过使用逐个样本梯度解决聚集问题，在线梯度平衡（GraB）算法通过贪心地选择示例排序被证明是理论上的最优解决方案，保证优于随机重排。然而，目前还没有有效的GraB实现供社区轻松使用。这项工作提出了一个高效的Python库“GraB-sampler”，使得社区可以轻松使用GraB算法，并提出了5个变体的GraB算法。GraB-sampler的最佳性能结果在只增加8.7%的训练时间开销和0.85%的峰值显存使用开销的情况下，再现了训练损失和测试准确率结果。

    The online Gradient Balancing (GraB) algorithm greedily choosing the examples ordering by solving the herding problem using per-sample gradients is proved to be the theoretically optimal solution that guarantees to outperform Random Reshuffling. However, there is currently no efficient implementation of GraB for the community to easily use it.  This work presents an efficient Python library, $\textit{GraB-sampler}$, that allows the community to easily use GraB algorithms and proposes 5 variants of the GraB algorithm. The best performance result of the GraB-sampler reproduces the training loss and test accuracy results while only in the cost of 8.7% training time overhead and 0.85% peak GPU memory usage overhead.
    
[^107]: 大规模粒度：利用高分辨率正射影像和混合学习估计居民社区的福祉

    Granularity at Scale: Estimating Neighborhood Well-Being from High-Resolution Orthographic Imagery and Hybrid Learning. (arXiv:2309.16808v1 [cs.CV])

    [http://arxiv.org/abs/2309.16808](http://arxiv.org/abs/2309.16808)

    本研究利用高分辨率影像和混合学习方法，通过图像数据中的特征提取和模式检测，估计了个别社区的人口密度、家庭收入中位数和教育程度。

    

    由于现有数据收集方法的局限性，世界上许多地区缺乏有关居民福祉的基本信息。通过遥感获取的高空影像，如卫星或飞机，可以作为窥视地面上生活状况的窗口，并帮助填补社区信息稀缺的地方，较小地理尺度的估计需要更高分辨率的传感器。随着传感器分辨率的提高，机器学习和计算机视觉的最新进展使得能够快速从图像数据中提取特征并检测模式，从而将这些特征与其他信息相关联。在这项工作中，我们探讨了两种方法（监督卷积神经网络和基于视觉词袋的半监督聚类）如何从公开可用的高分辨率影像中估计个别社区的人口密度、家庭收入中位数和教育程度。

    Many areas of the world are without basic information on the well-being of the residing population due to limitations in existing data collection methods. Overhead images obtained remotely, such as from satellite or aircraft, can help serve as windows into the state of life on the ground and help "fill in the gaps" where community information is sparse, with estimates at smaller geographic scales requiring higher resolution sensors. Concurrent with improved sensor resolutions, recent advancements in machine learning and computer vision have made it possible to quickly extract features from and detect patterns in image data, in the process correlating these features with other information. In this work, we explore how well two approaches, a supervised convolutional neural network and semi-supervised clustering based on bag-of-visual-words, estimate population density, median household income, and educational attainment of individual neighborhoods from publicly available high-resolution 
    
[^108]: Promptbreeder: 通过提示演化实现自我参照的自我改进

    Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution. (arXiv:2309.16797v1 [cs.CL])

    [http://arxiv.org/abs/2309.16797](http://arxiv.org/abs/2309.16797)

    Promptbreeder是一种通过提示演化进行自我改进的通用机制，它可以优化大型语言模型的推理能力，并在算术和常识推理上优于现有的提示策略。

    

    像Chain-of-Thought Prompting这样的流行提示策略可以显著改进大型语言模型（LLMs）在各个领域的推理能力。然而，这种手工制作的提示策略通常不够优化。在本文中，我们提出了Promptbreeder，一种通用的自我参照自我改进机制，用于进化和调整给定领域的提示。Promptbreeder通过LLM驱动，对一组任务提示进行突变，并在训练集上评估其适应性。关键是，这些任务提示的突变是由LLM生成并在演化过程中自我改进的突变提示来控制的。换句话说，Promptbreeder不仅改进任务提示，还改进了改进这些任务提示的突变提示。Promptbreeder在常用的算术和常识推理基准测试中优于Chain-of-Thought和Plan-and-Solve Prompting等最先进的提示策略。

    Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furth
    
[^109]: 长输入文本摘要中的幻觉减少

    Hallucination Reduction in Long Input Text Summarization. (arXiv:2309.16781v1 [cs.CL])

    [http://arxiv.org/abs/2309.16781](http://arxiv.org/abs/2309.16781)

    本文旨在减少长篇文本摘要中的幻觉输出，通过在Longformer Encoder-Decoder模型的微调中采用数据过滤和联合实体和摘要生成技术，我们成功提高了生成摘要的质量。

    

    文本摘要中的幻觉是指模型生成不被输入源文档支持的信息的现象。幻觉给生成的摘要的准确性和可靠性带来了重大障碍。本文旨在减少长篇文本摘要中的幻觉输出。我们使用了包含长篇科学研究文档及其摘要的PubMed数据集。我们在Longformer Encoder-Decoder (LED)模型的微调中加入了数据过滤和联合实体和摘要生成（JAENS）技术，以最小化幻觉，从而提高生成摘要的质量。我们使用以下指标来衡量实体级别的事实一致性：源精确度和目标F1。实验证明，经过微调的LED模型在生成文章摘要方面表现良好。数据过滤技术基于一些预处理步骤。

    Hallucination in text summarization refers to the phenomenon where the model generates information that is not supported by the input source document. Hallucination poses significant obstacles to the accuracy and reliability of the generated summaries. In this paper, we aim to reduce hallucinated outputs or hallucinations in summaries of long-form text documents. We have used the PubMed dataset, which contains long scientific research documents and their abstracts. We have incorporated the techniques of data filtering and joint entity and summary generation (JAENS) in the fine-tuning of the Longformer Encoder-Decoder (LED) model to minimize hallucinations and thereby improve the quality of the generated summary. We have used the following metrics to measure factual consistency at the entity level: precision-source, and F1-target. Our experiments show that the fine-tuned LED model performs well in generating the paper abstract. Data filtering techniques based on some preprocessing steps
    
[^110]: 生成分类器的有趣属性

    Intriguing properties of generative classifiers. (arXiv:2309.16779v1 [cs.CV])

    [http://arxiv.org/abs/2309.16779](http://arxiv.org/abs/2309.16779)

    生成分类器展示了记录破纪录的人类形状偏好、接近人类级别的超出分布准确性、与人类分类错误的最先进对齐以及理解某些知觉幻象的新兴特性，揭示了零样本生成模型出奇地接近人类物体识别数据。

    

    识别对象的最佳范式是判别式推理（快速但潜在容易出现快捷学习）还是使用生成模型（较慢但潜在更稳健）？我们借鉴了最新的生成模型进展，将文本到图像模型转化为分类器。这使得我们能够研究其行为，并将其与判别模型和人类心理物理数据进行比较。我们报道了生成分类器的四个有趣的新兴特性：它们显示出破纪录的人类形状偏好（对于Imagen达到99%），接近人类级别的超出分布准确性，与人类分类错误的最先进对齐以及它们理解某些知觉幻象。我们的结果表明，尽管目前模拟人类物体识别的主导范式是判别式推理，零样本生成模型出奇地接近人类物体识别数据。

    What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well.
    
[^111]: 神经规模定律在表型药物发现中的应用

    Neural scaling laws for phenotypic drug discovery. (arXiv:2309.16773v1 [cs.LG])

    [http://arxiv.org/abs/2309.16773](http://arxiv.org/abs/2309.16773)

    本研究调查了规模对于辅助小分子药物发现模型的影响，并发现DNN在表型药物发现任务中并没有持续改进，通过引入逆生物过程预训练可以显著提升模型性能。

    

    近期深度神经网络在自然语言处理和计算机视觉领域的突破是通过模型和数据规模的提升而非新的计算范例的发现。本研究探讨了规模是否也能对用于辅助小分子药物发现的模型产生类似的影响。我们通过大规模和系统化的分析深度神经网络的规模、数据训练集和学习方法如何相互影响，并对我们的表型化学竞技场（Pheno-CA）基准进行精确度测试：这是一个基于图像高内涵筛选数据的多样化药物开发任务集。令人惊讶的是，我们发现在Pheno-CA任务中明确被监督的深度神经网络在数据和模型规模扩大时并没有持续改进。为了解决这个问题，我们引入了一项新的前体任务，即逆生物过程（IBP），它设计成类似于在自然语言处理中取得成功的因果目标函数。我们确实发现深度神经网络的性能取决于引入IBP任务的方式，当使用IBP预训练时，模型在Pheno-CA任务上的性能得到了显著改善。我们的结果揭示了在表型药物发现中，与深度神经网络的规模和学习方式相关的因素对模型的性能产生重要影响。

    Recent breakthroughs by deep neural networks (DNNs) in natural language processing (NLP) and computer vision have been driven by a scale-up of models and data rather than the discovery of novel computing paradigms. Here, we investigate if scale can have a similar impact for models designed to aid small molecule drug discovery. We address this question through a large-scale and systematic analysis of how DNN size, data diet, and learning routines interact to impact accuracy on our Phenotypic Chemistry Arena (Pheno-CA) benchmark: a diverse set of drug development tasks posed on image-based high content screening data. Surprisingly, we find that DNNs explicitly supervised to solve tasks in the Pheno-CA do not continuously improve as their data and model size is scaled-up. To address this issue, we introduce a novel precursor task, the Inverse Biological Process (IBP), which is designed to resemble the causal objective functions that have proven successful for NLP. We indeed find that DNNs
    
[^112]: Persona编码多流程对话句子评分：Persona引导的多流程对话句子评分方法

    Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v1 [cs.CL])

    [http://arxiv.org/abs/2309.16770](http://arxiv.org/abs/2309.16770)

    本论文提出了一种新颖的Persona编码多流程对话句子评分方法，利用个人角色信息来提高对话生成的质量。

    

    机器学习和深度学习的最新进展已经在许多实际应用中广泛应用于对话AI。然而，要利用可以提供对话背景或个性化调整的辅助信息以提高对话质量仍然很具挑战性。例如，关于使用个人角色信息来提高对话质量的研究仅有限，即使是最先进的对话AI技术也无法有效地利用来自多种来源的辅助数据信号，例如多模式交互数据、人口统计学数据和社会确定因素数据等。在本文中，我们提出了一种新颖的Persona编码多流程对话句子评分方法，它利用多流程编码方案中的个人角色信息来提高对话生成的质量。为了展示所提出方法的有效性，我们在两个不同的基于个人角色的对话数据集上评估了我们的方法，并与参考方法进行了比较。

    Recent advances in machine learning and deep learning have led to the widespread use of Conversational AI in many practical applications. However, it is still very challenging to leverage auxiliary information that can provide conversational context or personalized tuning to improve the quality of conversations. For example, there has only been limited research on using an individuals persona information to improve conversation quality, and even state-of-the-art conversational AI techniques are unable to effectively leverage signals from heterogeneous sources of auxiliary data, such as multi-modal interaction data, demographics, SDOH data, etc. In this paper, we present a novel Persona-Coded Poly-Encoder method that leverages persona information in a multi-stream encoding scheme to improve the quality of response generation for conversations. To show the efficacy of the proposed method, we evaluate our method on two different persona-based conversational datasets, and compared against 
    
[^113]: 眼中记忆：扩散模型和关联记忆之间的神秘相似之处的调查

    Memory in Plain Sight: A Survey of the Uncanny Resemblances between Diffusion Models and Associative Memories. (arXiv:2309.16750v1 [cs.LG])

    [http://arxiv.org/abs/2309.16750](http://arxiv.org/abs/2309.16750)

    本调查综述了扩散模型（DMs）和关联记忆（AMs）之间的数学联系，揭示了DMs是如何利用能量函数进行去噪数据的，并讨论了未来研究方向。

    

    扩散模型（DMs）最近在许多生成基准测试中取得了最新的成果。然而，对它们的数学描述有很多种方式，这使得人们很难对其工作原理进行简单理解。在这项调查中，我们从动力系统和常微分方程（ODE）的角度提供了DMs的简明概述，揭示了一种与其高度相关但常常被忽视的能量模型类别，称为关联记忆（AMs）的数学联系。基于能量的AMs是一个理论框架，其行为与去噪DMs非常相似，但它们使我们能够直接计算一个Lyapunov能量函数，在其上可以执行梯度下降以去噪数据。然后，我们总结了能量AMs的40年历史，从最初的Hopfield网络开始，并讨论了通过描述它们的相似性和差异程度揭示出来的AMs和DMs的新研究方向。

    Diffusion Models (DMs) have recently set state-of-the-art on many generation benchmarks. However, there are myriad ways to describe them mathematically, which makes it difficult to develop a simple understanding of how they work. In this survey, we provide a concise overview of DMs from the perspective of dynamical systems and Ordinary Differential Equations (ODEs) which exposes a mathematical connection to the highly related yet often overlooked class of energy-based models, called Associative Memories (AMs). Energy-based AMs are a theoretical framework that behave much like denoising DMs, but they enable us to directly compute a Lyapunov energy function on which we can perform gradient descent to denoise data. We then summarize the 40 year history of energy-based AMs, beginning with the original Hopfield Network, and discuss new research directions for AMs and DMs that are revealed by characterizing the extent of their similarities and differences
    
[^114]: 用XRM发现环境

    Discovering environments with XRM. (arXiv:2309.16748v1 [cs.LG])

    [http://arxiv.org/abs/2309.16748](http://arxiv.org/abs/2309.16748)

    本文提出了一种用于发现环境的算法 XRM，它通过训练两个孪生网络，每个网络从训练数据的一半中学习，并模仿其兄弟网络的错误分类，解决了现有方法需要依赖人工注释环境信息的问题。

    

    成功的跨领域泛化需要环境注释。然而，这些注释的获取是资源密集型的，并且它们对模型性能的影响受人类注释者的期望和感知偏差的限制。因此，为了实现应用领域全面泛化的鲁棒性AI系统，我们必须开发一种算法来自动发现引发广泛泛化的环境。目前的提案根据训练误差将示例划分为不同的类，但存在一个根本问题。这些方法添加了超参数和早停策略，而这些参数是无法在没有人类注释环境的验证集的情况下进行调整的，而这些信息正是要发现的信息。在本文中，我们提出了 Cross-Risk-Minimization (XRM) 来解决这个问题。XRM 训练两个孪生网络，每个网络从训练数据的一个随机一半中学习，同时模仿其兄弟网络所做的自信的错误分类。XRM 提供了超参数调整的方法，并且不需要依赖人工注释的环境信息。

    Successful out-of-distribution generalization requires environment annotations. Unfortunately, these are resource-intensive to obtain, and their relevance to model performance is limited by the expectations and perceptual biases of human annotators. Therefore, to enable robust AI systems across applications, we must develop algorithms to automatically discover environments inducing broad generalization. Current proposals, which divide examples based on their training error, suffer from one fundamental problem. These methods add hyper-parameters and early-stopping criteria that are impossible to tune without a validation set with human-annotated environments, the very information subject to discovery. In this paper, we propose Cross-Risk-Minimization (XRM) to address this issue. XRM trains two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. XRM provides a recipe for hyper-parameter tuning, does not 
    
[^115]: 利用多样数据进行全球灾害预测的多模式框架

    Harnessing Diverse Data for Global Disaster Prediction: A Multimodal Framework. (arXiv:2309.16747v1 [cs.LG])

    [http://arxiv.org/abs/2309.16747](http://arxiv.org/abs/2309.16747)

    本研究提出了一种新颖的多模式灾害预测框架，结合了天气统计数据、卫星图像和文本信息。通过整合多个数据源，对于不同类型的灾害预测，模型的性能得到了增强。

    

    随着气候变化的加剧，准确的全球规模灾害预测变得越来越紧迫。本研究提出了一种新颖的多模式灾害预测框架，结合了天气统计数据、卫星图像和文本信息。我们特别关注"洪水"和"滑坡"的预测，因为它们与气象和地形因素有关。该模型根据可用数据进行精细设计，并采用策略来解决类别不平衡问题。尽管我们的研究结果表明整合多个数据源可以增强模型性能，但增强的程度取决于每种灾害的具体性质和其独特的潜在原因。

    As climate change intensifies, the urgency for accurate global-scale disaster predictions grows. This research presents a novel multimodal disaster prediction framework, combining weather statistics, satellite imagery, and textual insights. We particularly focus on "flood" and "landslide" predictions, given their ties to meteorological and topographical factors. The model is meticulously crafted based on the available data and we also implement strategies to address class imbalance. While our findings suggest that integrating multiple data sources can bolster model performance, the extent of enhancement differs based on the specific nature of each disaster and their unique underlying causes.
    
[^116]: 隐性高斯过程表示任意潜在流形上的向量场

    Implicit Gaussian process representation of vector fields over arbitrary latent manifolds. (arXiv:2309.16746v1 [cs.LG])

    [http://arxiv.org/abs/2309.16746](http://arxiv.org/abs/2309.16746)

    这项研究通过引入RVGP方法，结合基于图的数据逼近方法对潜在流形上的向量信号进行学习，实现了超分辨率和修复向量场，并且在实验中证明了其具有全局规律性。

    

    高斯过程（GPs）是用于学习未知函数和量化数据中的时空不确定性的流行非参数统计模型。最近的研究扩展了GPs，用于建模分布在非欧几里得域上的标量和向量数据，包括出现在计算机视觉、动力系统和神经科学等众多领域中的平滑流形。然而，这些方法假设数据的潜在流形是已知的，限制了它们的实际效用。我们引入了RVGP，一种用于学习潜在黎曼流形上的向量信号的GP的推广。我们的方法使用与切向丛关联的连接Laplacian的特征函数进行位置编码，这些特征函数可以从基于图的常见数据近似中轻松推导出来。我们证明了RVGP在流形上具有全局规律性，使得其能够在保留奇异性的同时超分辨率和修复向量场。此外，我们使用RVGP来重构高密度数据。

    Gaussian processes (GPs) are popular nonparametric statistical models for learning unknown functions and quantifying the spatiotemporal uncertainty in data. Recent works have extended GPs to model scalar and vector quantities distributed over non-Euclidean domains, including smooth manifolds appearing in numerous fields such as computer vision, dynamical systems, and neuroscience. However, these approaches assume that the manifold underlying the data is known, limiting their practical utility. We introduce RVGP, a generalisation of GPs for learning vector signals over latent Riemannian manifolds. Our method uses positional encoding with eigenfunctions of the connection Laplacian, associated with the tangent bundle, readily derived from common graph-based approximation of data. We demonstrate that RVGP possesses global regularity over the manifold, which allows it to super-resolve and inpaint vector fields while preserving singularities. Furthermore, we use RVGP to reconstruct high-dens
    
[^117]: 高效训练一类分类支持向量机

    Efficient Training of One Class Classification-SVMs. (arXiv:2309.16745v1 [cs.LG])

    [http://arxiv.org/abs/2309.16745](http://arxiv.org/abs/2309.16745)

    本研究提出了一种高效训练方法来进行一类分类支持向量机。通过利用增广Lagrange乘子法（AL-FPGM），该方法可以在只有正例的情况下训练有效的分类器，并且计算成本低廉，可以用于训练大规模支持向量机。

    

    本研究探讨了一种高效的训练方法来进行一类分类。在常见的二分类场景中，训练数据需要同时存在正例和负例才能开发出有效的分类器。然而，在许多领域中并不满足这个条件。针对这种情况，已经开发出了从纯正例输入中学习的分类算法。本文介绍了一种有效的双软边界一类支持向量机训练算法。我们的方法利用了增广Lagrange乘子法（AL-FPGM），这是一种快速投影梯度方法的变体。FPGM只需要一阶导数，对于双软边界一类支持向量机来说，主要是计算矩阵向量乘积。因此，计算成本低廉的AL-FPGM可以与现有的二次规划求解器相结合，用于训练大规模支持向量机。我们在真实世界的数据集上广泛验证了我们的方法。

    This study examines the use of a highly effective training method to conduct one-class classification. The existence of both positive and negative examples in the training data is necessary to develop an effective classifier in common binary classification scenarios. Unfortunately, this criteria is not met in many domains. Here, there is just one class of examples. Classification algorithms that learn from solely positive input have been created to deal with this setting. In this paper, an effective algorithm for dual soft-margin one-class SVM training is presented. Our approach makes use of the Augmented Lagrangian (AL-FPGM), a variant of the Fast Projected Gradient Method. The FPGM requires only first derivatives, which for the dual soft margin OCC-SVM means computing mainly a matrix-vector product. Therefore, AL-FPGM, being computationally inexpensive, may complement existing quadratic programming solvers for training large SVMs. We extensively validate our approach over real-world 
    
[^118]: 使用机器学习算法预测新冠后患者的长期肾损伤

    Predicting Long-term Renal Impairment in Post-COVID-19 Patients with Machine Learning Algorithms. (arXiv:2309.16744v1 [cs.LG])

    [http://arxiv.org/abs/2309.16744](http://arxiv.org/abs/2309.16744)

    该研究使用机器学习算法预测新冠后患者长期肾损伤的风险，通过早期识别和干预改善临床结果。

    

    新冠疫情对全球公共卫生产生了深远的影响。随着我们继续应对其后果，新冠后并发症成为一个重要关注点。其中，由于其潜在的长期健康影响，肾损伤备受关注。本研究以伊拉克不同地区在2021年至2023年期间的821名新冠后患者为研究对象，利用先进的机器学习算法预测长期肾损伤的风险。我们的发现有可能通过及早识别和干预肾损伤风险患者，从而改善临床结果，革新新冠后患者的护理。该研究包括全面的数据采集和预处理、特征选择以及使用各种机器学习算法开发预测模型。研究目标是评估并确定机器学习算法在预测新冠后患者长期肾损伤方面的效果。

    The COVID-19 pandemic has had far-reaching implications for global public health. As we continue to grapple with its consequences, it becomes increasingly clear that post-COVID-19 complications are a significant concern. Among these complications, renal impairment has garnered particular attention due to its potential long-term health impacts. This study, conducted with a cohort of 821 post-COVID-19 patients from diverse regions of Iraq across the years 2021, 2022, and 2023, endeavors to predict the risk of long-term renal impairment using advanced machine learning algorithms. Our findings have the potential to revolutionize post-COVID-19 patient care by enabling early identification and intervention for those at risk of renal impairment, ultimately improving clinical outcomes. This research encompasses comprehensive data collection and preprocessing, feature selection, and the development of predictive models using various machine learning algorithms. The study's objectives are to ass
    
[^119]: 从大型集合运行中高通量训练深度代理

    High Throughput Training of Deep Surrogates from Large Ensemble Runs. (arXiv:2309.16743v1 [cs.LG])

    [http://arxiv.org/abs/2309.16743](http://arxiv.org/abs/2309.16743)

    该论文提出了一个用于加速数值解算器的深度代理的高通量训练方法，通过从大量集合运行的模拟中在线训练模型，利用多级并行性生成丰富的数据集，直接流式传输数据以避免I/O瓶颈和存储问题，同时使用训练储备池来减轻流式传输的固有偏差。实验结果显示，使用该方法能够在2小时内在8TB的数据上训练出准确率提升了47%、批量吞吐量提高了13倍的热方程代理网络。

    

    近年来，深度学习方法在加速数值解算器方面取得了突破，这些方法可以提供对物理世界的忠实但计算密集型的模拟。这些深度代理通常通过同一解算器生成的有限量的数据有监督地进行训练，而我们提出了一个开源框架，可以从大量集合运行的模拟中在线训练这些模型。它利用多个级别的并行性来生成丰富的数据集，并通过直接流式传输生成的数据来避免I/O瓶颈和存储问题。训练储备池在最大化GPU吞吐量的同时，减轻了流式传输的固有偏差。实验结果表明，通过提出的方法，在2小时内能够在8TB的数据上训练完全连接网络作为热方程的代理，相比传统的离线过程，准确率提升了47％，批量吞吐量提高了13倍。

    Recent years have seen a surge in deep learning approaches to accelerate numerical solvers, which provide faithful but computationally intensive simulations of the physical world. These deep surrogates are generally trained in a supervised manner from limited amounts of data slowly generated by the same solver they intend to accelerate. We propose an open-source framework that enables the online training of these models from a large ensemble run of simulations. It leverages multiple levels of parallelism to generate rich datasets. The framework avoids I/O bottlenecks and storage issues by directly streaming the generated data. A training reservoir mitigates the inherent bias of streaming while maximizing GPU throughput. Experiment on training a fully connected network as a surrogate for the heat equation shows the proposed approach enables training on 8TB of data in 2 hours with an accuracy improved by 47% and a batch throughput multiplied by 13 compared to a traditional offline proced
    
[^120]: 早期检测2型糖尿病患者白蛋白尿风险的监督学习模型

    Supervised Learning Models for Early Detection of Albuminuria Risk in Type-2 Diabetes Mellitus Patients. (arXiv:2309.16742v1 [cs.LG])

    [http://arxiv.org/abs/2309.16742](http://arxiv.org/abs/2309.16742)

    该论文研究了2型糖尿病患者白蛋白尿的早期预测问题，并开发了一种监督学习模型。通过使用不同的监督学习算法对184条数据进行训练，得出了预测结果。

    

    糖尿病，尤其是2型糖尿病，仍然是一个重大的健康问题。与糖尿病相关的一个主要问题是其并发症的发展。糖尿病肾病是糖尿病的一种慢性并发症，不利地影响肾脏，导致肾脏损伤。诊断糖尿病肾病涉及考虑各种标准之一，其中之一是尿液中白蛋白的病理学病理学数量，称为白蛋白尿。因此，对糖尿病患者尿液中白蛋白尿的早期预测具有及时预防措施的潜力。本研究旨在开发一种监督学习模型，以预测2型糖尿病患者患有白蛋白尿的风险。所选的监督学习算法包括朴素贝叶斯，支持向量机（SVM），决策树，随机森林，AdaBoost，XGBoost和多层感知器（MLP）。我们的私有数据集包括184条糖尿病并发症风险因素的条目被用来训练算法

    Diabetes, especially T2DM, continues to be a significant health problem. One of the major concerns associated with diabetes is the development of its complications. Diabetic nephropathy, one of the chronic complication of diabetes, adversely affects the kidneys, leading to kidney damage. Diagnosing diabetic nephropathy involves considering various criteria, one of which is the presence of a pathologically significant quantity of albumin in urine, known as albuminuria. Thus, early prediction of albuminuria in diabetic patients holds the potential for timely preventive measures. This study aimed to develop a supervised learning model to predict the risk of developing albuminuria in T2DM patients. The selected supervised learning algorithms included Na\"ive Bayes, Support Vector Machine (SVM), decision tree, random forest, AdaBoost, XGBoost, and Multi-Layer Perceptron (MLP). Our private dataset, comprising 184 entries of diabetes complications risk factors, was used to train the algorithm
    
[^121]: 多模态金融时间序列通过潜空间投影的检索

    Multi-Modal Financial Time-Series Retrieval Through Latent Space Projections. (arXiv:2309.16741v1 [cs.LG])

    [http://arxiv.org/abs/2309.16741](http://arxiv.org/abs/2309.16741)

    本文提出了一种通过深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，以捕捉数据的重要特征。

    

    金融公司通常处理和存储产生连续且高频的数十亿条时间序列数据。为了支持高效的数据存储和检索，出现了专门的时间序列数据库和系统。这些数据库支持通过类似于约束化结构化查询语言（SQL）的格式对时间序列进行索引和查询，以实现像“月度价格回报大于5%的股票”这样的查询，并以严格的格式表达。然而，这样的查询不能捕捉到高维时间序列数据的内在复杂性，它们往往可以通过图像或语言（例如“处于低波动性状态的股票”）更好地描述。而且，在时间序列空间中进行搜索所需的存储、计算时间和检索复杂度往往是非平凡的。在本文中，我们提出并演示了一种利用深度编码器在低维潜空间中存储金融时间序列的多模态数据的框架，使得潜空间投影可以捕捉到数据的重要特征。

    Financial firms commonly process and store billions of time-series data, generated continuously and at a high frequency. To support efficient data storage and retrieval, specialized time-series databases and systems have emerged. These databases support indexing and querying of time-series by a constrained Structured Query Language(SQL)-like format to enable queries like "Stocks with monthly price returns greater than 5%", and expressed in rigid formats. However, such queries do not capture the intrinsic complexity of high dimensional time-series data, which can often be better described by images or language (e.g., "A stock in low volatility regime"). Moreover, the required storage, computational time, and retrieval complexity to search in the time-series space are often non-trivial. In this paper, we propose and demonstrate a framework to store multi-modal data for financial time-series in a lower-dimensional latent space using deep encoders, such that the latent space projections ca
    
[^122]: 将大型语言模型推至6G边缘：视野、挑战和机遇

    Pushing Large Language Models to the 6G Edge: Vision, Challenges, and Opportunities. (arXiv:2309.16739v1 [cs.LG])

    [http://arxiv.org/abs/2309.16739](http://arxiv.org/abs/2309.16739)

    本文探讨了将大型语言模型(LLMs)部署在6G边缘的潜力和挑战。我们介绍了由LLMs支持的关键应用，并从响应时间、带宽成本和数据隐私等方面分析了云端部署面临的问题。我们提出了6G移动边缘计算(MEC)系统可能解决这些问题的方案，并讨论了边缘训练和边缘推理的创新技术。

    

    大型语言模型(LLMs)展示了显著的能力，正在改变人工智能的发展并有可能塑造我们的未来。然而，由于LLMs的多模态特性，当前的基于云的部署面临着一些关键挑战：1) 响应时间长；2) 高带宽成本；以及3) 违反数据隐私。6G移动边缘计算(MEC)系统可能解决这些迫切问题。本文探讨了在6G边缘部署LLMs的潜力。我们首先介绍了由多模态LLMs提供支持的关键应用，包括机器人技术和医疗保健，以突出在终端用户附近部署LLMs的需求。然后，我们确定了在边缘部署LLMs时面临的关键挑战，并设想了适用于LLMs的6G MEC架构。此外，我们深入探讨了两个设计方面，即LLMs的边缘训练和边缘推理。在这两个方面，考虑到边缘的固有资源限制，我们讨论了各种前沿技术。

    Large language models (LLMs), which have shown remarkable capabilities, are revolutionizing AI development and potentially shaping our future. However, given their multimodality, the status quo cloud-based deployment faces some critical challenges: 1) long response time; 2) high bandwidth costs; and 3) the violation of data privacy. 6G mobile edge computing (MEC) systems may resolve these pressing issues. In this article, we explore the potential of deploying LLMs at the 6G edge. We start by introducing killer applications powered by multimodal LLMs, including robotics and healthcare, to highlight the need for deploying LLMs in the vicinity of end users. Then, we identify the critical challenges for LLM deployment at the edge and envision the 6G MEC architecture for LLMs. Furthermore, we delve into two design aspects, i.e., edge training and edge inference for LLMs. In both aspects, considering the inherent resource limitations at the edge, we discuss various cutting-edge techniques, i
    
[^123]: 通过机器学习分析认知COVID-19后多器官功能障碍

    Cognizance of Post-COVID-19 Multi-Organ Dysfunction through Machine Learning Analysis. (arXiv:2309.16736v1 [cs.LG])

    [http://arxiv.org/abs/2309.16736](http://arxiv.org/abs/2309.16736)

    该研究应用机器学习技术分析和预测COVID-19后综合症中的多器官功能障碍，旨在通过提高对这种疾病的认识，帮助医疗保健提供者识别风险个体并及时干预，从而改善患者的预后结果。

    

    本研究共纳入了来自伊拉克各个城市的466名患者。本研究旨在应用机器学习技术分析和预测COVID-19后综合症（通常称为长期COVID）中的多器官功能障碍。COVID-19后综合症会导致各种各样持续性症状影响不同的器官系统，对医疗保健构成了重大挑战。通过利用人工智能的力量，本研究旨在提高对这种复杂病症的早期检测和治疗。本论文概述了数据收集和预处理、特征选择和工程、模型开发和验证以及在该领域开展研究时的道德考虑的重要性。通过通过使用机器学习提高我们对COVID-19后综合症的认知，医疗保健提供者可以识别出处于风险中的个体并及时提供干预措施，潜在地改善患者的预后结果。

    In the year 2022, a total of 466 patients from various cities across Iraq were included in this study. This research paper focuses on the application of machine learning techniques to analyse and predict multi-organ dysfunction in individuals experiencing Post-COVID-19 Syndrome, commonly known as Long COVID. Post-COVID-19 Syndrome presents a wide array of persistent symptoms affecting various organ systems, posing a significant challenge to healthcare. Leveraging the power of artificial intelligence, this study aims to enhance early detection and management of this complex condition. The paper outlines the importance of data collection and preprocessing, feature selection and engineering, model development and validation, and ethical considerations in conducting research in this field. By improving our understanding of Post-COVID-19 Syndrome through machine learning, healthcare providers can identify at-risk individuals and offer timely interventions, potentially improving patient outc
    
[^124]: 深度学习应用的韧性：分析和加固技术的系统调查

    Resilience of Deep Learning applications: a systematic survey of analysis and hardening techniques. (arXiv:2309.16733v1 [cs.LG])

    [http://arxiv.org/abs/2309.16733](http://arxiv.org/abs/2309.16733)

    这篇论文系统调查了深度学习应用对底层硬件故障的韧性，并提供了未来研究的方向。

    

    机器学习（ML）目前被广泛应用于各种领域，是最有效的人工智能（AI）技术之一，如视觉、自主系统等。这一趋势促使人们对ML应用在底层硬件故障影响下的分析和设计做出了大量贡献。本文通过一次深入的回顾系统地调查了关于深度学习（ML技术之一）对抗硬件故障的韧性的已有知识，清晰地呈现了这一文献流的优点和缺点，并提出了未来的研究方向。文章基于2019年1月至2023年3月间发表的163篇科学论文，采用分类框架来解读和突出研究的相似之处和特点，从工作的主要范围、采用的故障和错误模型等多个参数进行分类。

    Machine Learning (ML) is currently being exploited in numerous applications being one of the most effective Artificial Intelligence (AI) technologies, used in diverse fields, such as vision, autonomous systems, and alike. The trend motivated a significant amount of contributions to the analysis and design of ML applications against faults affecting the underlying hardware. The authors investigate the existing body of knowledge on Deep Learning (among ML techniques) resilience against hardware faults systematically through a thoughtful review in which the strengths and weaknesses of this literature stream are presented clearly and then future avenues of research are set out. The review is based on 163 scientific articles published between January 2019 and March 2023. The authors adopt a classifying framework to interpret and highlight research similarities and peculiarities, based on several parameters, starting from the main scope of the work, the adopted fault and error models, to the
    
[^125]: 可解释的基于机器学习的糖尿病肾病预测模型

    Explainable machine learning-based prediction model for diabetic nephropathy. (arXiv:2309.16730v1 [cs.LG])

    [http://arxiv.org/abs/2309.16730](http://arxiv.org/abs/2309.16730)

    本研究提出了一个可解释的基于机器学习的糖尿病肾病预测模型，通过分析血清代谢物对疾病的影响并选择最优特征来预测疾病的患病率。最优模型采用了极限梯度提升（XGB）算法，其在筛选DN方面表现最佳，同时具有更好的临床效益和拟合度。

    

    本研究旨在分析血清代谢物对糖尿病肾病（DN）的影响，并通过机器学习方法预测DN的患病率。数据集包括2018年4月至2019年4月大连医科大学附属第二医院的548名患者。我们通过最小绝对收缩和选择算子（LASSO）回归模型和10折交叉验证选择了最优的38个特征。我们比较了四种机器学习算法，包括极限梯度提升（XGB）、随机森林、决策树和逻辑回归，并通过AUC-ROC曲线、决策曲线和校准曲线进行比较。我们利用Shapley Additive exPlanations（SHAP）方法来量化最优预测模型中的特征重要性和交互效应。XGB模型在筛选DN方面具有最佳性能，最高AUC值为0.966。XGB模型的临床净效益也优于其他模型，并且拟合度更好。

    The aim of this study is to analyze the effect of serum metabolites on diabetic nephropathy (DN) and predict the prevalence of DN through a machine learning approach. The dataset consists of 548 patients from April 2018 to April 2019 in Second Affiliated Hospital of Dalian Medical University (SAHDMU). We select the optimal 38 features through a Least absolute shrinkage and selection operator (LASSO) regression model and a 10-fold cross-validation. We compare four machine learning algorithms, including eXtreme Gradient Boosting (XGB), random forest, decision tree and logistic regression, by AUC-ROC curves, decision curves, calibration curves. We quantify feature importance and interaction effects in the optimal predictive model by Shapley Additive exPlanations (SHAP) method. The XGB model has the best performance to screen for DN with the highest AUC value of 0.966. The XGB model also gains more clinical net benefits than others and the fitting degree is better. In addition, there are s
    
[^126]: SimPINNs: 用于增强非线性反问题性能的基于仿真驱动的物理信息神经网络

    SimPINNs: Simulation-Driven Physics-Informed Neural Networks for Enhanced Performance in Nonlinear Inverse Problems. (arXiv:2309.16729v1 [cs.LG])

    [http://arxiv.org/abs/2309.16729](http://arxiv.org/abs/2309.16729)

    本文提出了一种基于仿真驱动的物理信息神经网络(SimPINNs)的方法，用于增强非线性反问题的性能。这种方法通过深度学习和融合观测数据和仿真数据的混合损失函数来推断控制物理系统的未知参数。实验结果表明，在轨道复位问题上，SimPINNs的准确性和鲁棒性优于标准PINNs的表现。

    

    本文引入了一种通过利用深度学习技术解决反问题的新方法。目标是根据观测数据推断控制物理系统的未知参数。我们关注在潜在的正向模型表现出显著非线性行为且未知参数空间的维度明显小于观测数据的情况下。我们提出的方法基于物理信息神经网络(PINNs)，通过一个混合损失函数训练，该损失函数结合了观测数据和由已知(近似)物理模型生成的仿真数据。在轨道复位问题上的实验结果表明，我们的方法超过了标准PINNs的性能，提供了更高的准确性和鲁棒性。

    This paper introduces a novel approach to solve inverse problems by leveraging deep learning techniques. The objective is to infer unknown parameters that govern a physical system based on observed data. We focus on scenarios where the underlying forward model demonstrates pronounced nonlinear behaviour, and where the dimensionality of the unknown parameter space is substantially smaller than that of the observations. Our proposed method builds upon physics-informed neural networks (PINNs) trained with a hybrid loss function that combines observed data with simulated data generated by a known (approximate) physical model. Experimental results on an orbit restitution problem demonstrate that our approach surpasses the performance of standard PINNs, providing improved accuracy and robustness.
    
[^127]: 解非线性动力系统的静态福克-普朗克方程的物理知情解：一个评估研究

    Physics-Informed Solution of The Stationary Fokker-Plank Equation for a Class of Nonlinear Dynamical Systems: An Evaluation Study. (arXiv:2309.16725v1 [physics.comp-ph])

    [http://arxiv.org/abs/2309.16725](http://arxiv.org/abs/2309.16725)

    该论文提出了一个物理知情的神经网络（PINN）框架，用于解决非线性随机动力系统的福克-普朗克方程，并评估了其潜力。

    

    福克-普朗克（FP）方程是一种线性偏微分方程，描述了随机动力系统响应的概率密度函数（PDF）的时间和空间演化。FP方程的精确解仅对有限的动力系统子集有效。对于较大但仍然是一小部分系统，可以使用半解析方法。而传统的计算方法，如有限元和有限差分，需要将计算域划分为离散点的网格，对于高维系统而言，计算成本高。物理知情学习为传统的计算方案提供了一个潜在的有力替代方法。为了评估其潜力，我们提出了一个无数据的、具有物理知情的神经网络（PINN）框架，用于解决非线性随机动力系统的FP方程。

    The Fokker-Planck (FP) equation is a linear partial differential equation which governs the temporal and spatial evolution of the probability density function (PDF) associated with the response of stochastic dynamical systems. An exact analytical solution of the FP equation is only available for a limited subset of dynamical systems. Semi-analytical methods are available for larger, yet still a small subset of systems, while traditional computational methods; e.g. Finite Elements and Finite Difference require dividing the computational domain into a grid of discrete points, which incurs significant computational costs for high-dimensional systems. Physics-informed learning offers a potentially powerful alternative to traditional computational schemes. To evaluate its potential, we present a data-free, physics-informed neural network (PINN) framework to solve the FP equation for a class of nonlinear stochastic dynamical systems. In particular, through several examples concerning the sto
    
[^128]: 为线下强化学习提供一个现实世界的四足步态基准测试

    A Real-World Quadrupedal Locomotion Benchmark for Offline Reinforcement Learning. (arXiv:2309.16718v1 [cs.RO])

    [http://arxiv.org/abs/2309.16718](http://arxiv.org/abs/2309.16718)

    该论文提出了一个现实世界的四足步态基准测试，用于评估线下强化学习算法在挑战性任务中的性能。实验结果表明，最好的算法能够与无模型强化学习相媲美甚至超越其性能。

    

    在真实的机器人硬件上训练在线强化学习方法往往效率低下或不可靠，尤其对于四足机器人更是如此。从预收集的数据中学习机器人任务是一个有前途的方向。同时，灵活稳定的四足步态机器人行走仍然是一个尚未解决的问题。线下强化学习有潜力在这个具有挑战性的领域取得突破，但目前的瓶颈在于缺乏多样化的用于挑战性现实任务的数据集。为了促进线下强化学习的发展，我们在现实的四足步态数据集上对11种线下强化学习算法进行了基准测试。这个数据集是通过经典的模型预测控制方法(MPC)收集的，而不是之前基准测试常用的无模型在线强化学习方法。广泛的实验结果表明，表现最好的线下强化学习算法与无模型强化学习相比具有竞争力的性能，甚至在某些方面超越它。

    Online reinforcement learning (RL) methods are often data-inefficient or unreliable, making them difficult to train on real robotic hardware, especially quadruped robots. Learning robotic tasks from pre-collected data is a promising direction. Meanwhile, agile and stable legged robotic locomotion remains an open question in their general form. Offline reinforcement learning (ORL) has the potential to make breakthroughs in this challenging field, but its current bottleneck lies in the lack of diverse datasets for challenging realistic tasks. To facilitate the development of ORL, we benchmarked 11 ORL algorithms in the realistic quadrupedal locomotion dataset. Such dataset is collected by the classic model predictive control (MPC) method, rather than the model-free online RL method commonly used by previous benchmarks. Extensive experimental results show that the best-performing ORL algorithms can achieve competitive performance compared with the model-free RL, and even surpass it in som
    
[^129]: 无人机辅助语义通信与混合动作强化学习

    UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning. (arXiv:2309.16713v1 [cs.NI])

    [http://arxiv.org/abs/2309.16713](http://arxiv.org/abs/2309.16713)

    本文提出了一个利用无人机的上行语义通信方案，通过混合动作强化学习框架实现了在数据收集效率和计算能量成本之间的平衡，并取得了显著的改善结果。

    

    本文旨在探索利用无人机进行上行语义通信，以提高偏远地区元宇宙用户的数据收集效率。为了在平衡重建质量和计算能量成本之间减少上行数据收集时间，我们提出了一个混合动作强化学习框架，用于在语义模型规模、信道分配、传输功率和无人机轨迹上做出决策。变量被划分为离散类型和连续类型，并通过两个不同的强化学习代理进行优化以生成组合动作。仿真结果表明，所提出的混合动作强化学习框架能够在不同参数设置下有效提高上行语义数据收集的效率，并优于基准场景。

    In this paper, we aim to explore the use of uplink semantic communications with the assistance of UAV in order to improve data collection effiicency for metaverse users in remote areas. To reduce the time for uplink data collection while balancing the trade-off between reconstruction quality and computational energy cost, we propose a hybrid action reinforcement learning (RL) framework to make decisions on semantic model scale, channel allocation, transmission power, and UAV trajectory. The variables are classified into discrete type and continuous type, which are optimized by two different RL agents to generate the combined action. Simulation results indicate that the proposed hybrid action reinforcement learning framework can effectively improve the efficiency of uplink semantic data collection under different parameter settings and outperforms the benchmark scenarios.
    
[^130]: 使用Mask R-CNN自动检测高分辨率图像的界址地籍边界

    Automatic Cadastral Boundary Detection of Very High Resolution Images Using Mask R-CNN. (arXiv:2309.16708v1 [cs.CV])

    [http://arxiv.org/abs/2309.16708](http://arxiv.org/abs/2309.16708)

    这篇论文使用Mask R-CNN和几何后处理方法提出了自动检测高分辨率图像界址地籍边界的解决方案，并引入了一种新的基于口袋的简化算法来改善工作质量。

    

    最近，加速和改进自动地籍制图的检测需求很高。由于这个问题还处于起点阶段，许多计算机视觉和深度学习的方法尚未被考虑。在本文中，我们专注于深度学习，并提供了三种改善工作质量的几何后处理方法。我们的框架包括两个部分，每个部分由几个阶段组成。我们对这个问题的解决方案使用实例分割。在第一部分中，我们使用ResNet-50在ImageNet数据集上进行训练的Mask R-CNN。在第二阶段，我们将三种几何后处理方法应用于第一部分的输出，以获得更好的总体输出。在这里，我们还使用计算几何来介绍一种称为基于口袋的简化算法的新方法。为了评估我们解决方案的质量，我们使用了这一领域中常用的召回率和准确率等公式。

    Recently, there has been a high demand for accelerating and improving the detection of automatic cadastral mapping. As this problem is in its starting point, there are many methods of computer vision and deep learning that have not been considered yet. In this paper, we focus on deep learning and provide three geometric post-processing methods that improve the quality of the work. Our framework includes two parts, each of which consists of a few phases. Our solution to this problem uses instance segmentation. In the first part, we use Mask R-CNN with the backbone of pre-trained ResNet-50 on the ImageNet dataset. In the second phase, we apply three geometric post-processing methods to the output of the first part to get better overall output. Here, we also use computational geometry to introduce a new method for simplifying lines which we call it pocket-based simplification algorithm. For evaluating the quality of our solution, we use popular formulas in this field which are recall, pre
    
[^131]: AIR: 深度学习信息恢复中对抗攻击的威胁

    AIR: Threats of Adversarial Attacks on Deep Learning-Based Information Recovery. (arXiv:2309.16706v1 [cs.CR])

    [http://arxiv.org/abs/2309.16706](http://arxiv.org/abs/2309.16706)

    这项研究探讨了在对抗情况下，深度学习信息恢复模型的鲁棒性。通过对最先进的DeepReceiver模型进行对抗攻击，实验结果表明DeepReceiver对设计的攻击方法是脆弱的。

    

    无线通信系统通常由一个传输器和一个接收器组成，传输器将信息传输，接收器从接收到的扭曲信号中恢复原始信息。深度学习已被用于改善复杂信道环境下接收器的性能，并取得了最先进的性能。然而，其鲁棒性尚未被研究。为了评估深度学习信息恢复模型在对抗情况下的鲁棒性，我们研究了对最先进的深度学习信息恢复模型DeepReceiver的对抗攻击。我们将该问题建模为一个带有功率和峰均比 (PAPR) 约束的优化问题。我们根据对手对DeepReceiver模型和/或测试样本的了解设计了不同的对抗攻击方法。大量实验表明，在所有考虑的场景中，DeepReceiver对设计的攻击方法是脆弱的。

    A wireless communications system usually consists of a transmitter which transmits the information and a receiver which recovers the original information from the received distorted signal. Deep learning (DL) has been used to improve the performance of the receiver in complicated channel environments and state-of-the-art (SOTA) performance has been achieved. However, its robustness has not been investigated. In order to evaluate the robustness of DL-based information recovery models under adversarial circumstances, we investigate adversarial attacks on the SOTA DL-based information recovery model, i.e., DeepReceiver. We formulate the problem as an optimization problem with power and peak-to-average power ratio (PAPR) constraints. We design different adversarial attack methods according to the adversary's knowledge of DeepReceiver's model and/or testing samples. Extensive experiments show that the DeepReceiver is vulnerable to the designed attack methods in all of the considered scenari
    
[^132]: 解码图像：释放大型语言模型。

    Decoding Imagery: Unleashing Large Language Models. (arXiv:2309.16705v1 [cs.CV])

    [http://arxiv.org/abs/2309.16705](http://arxiv.org/abs/2309.16705)

    该论文研究了Google Bard这个多模态大型语言模型的能力，发现Bard在将视觉和语言分析相结合方面依赖于对图像进行有根据的猜测，可以解决视觉上有挑战的问题但无法修改原始视觉对象。

    

    在一个挑战-响应研究中，我们对Google Bard进行了64个视觉挑战，旨在探究多模态大型语言模型（LLMs）的能力。这些挑战涵盖了各种类别，包括“视觉情境推理”，“视觉文本推理”和“下一场景预测”等，以确定Bard在融合视觉和语言分析方面的能力。我们的研究结果显示，Bard倾向于根据图片做出有根据的猜测，特别是在确定图片中的线索时。与GPT4等其他模型不同，Bard似乎不依赖于像Tesseract这样的光学字符识别库，而是像Google Lens和Visual API这样的深度学习模型一样，识别复杂图片中的文本。显着的是，Bard可以通过视觉方式解决ChatGPT无法理解的验证码，推荐使用Tesseract解决方案。此外，虽然Bard模型基于视觉输入提出了解决方案，但它无法重建或修改原始的视觉对象来支持其结论。

    In a challenge-response study, we subjected Google Bard to 64 visual challenges designed to probe multimodal Large Language Models (LLMs). The challenges spanned diverse categories, including "Visual Situational Reasoning," "Visual Text Reasoning," and "Next Scene Prediction," among others, to discern Bard's competence in melding visual and linguistic analyses. Our findings indicate that Bard tends to rely on making educated guesses about visuals, especially when determining cues from images. Unlike other models like GPT4, Bard does not appear to rely on optical character recognition libraries like Tesseract but recognizes text in complex images like deep learning models such as Google Lens and Visual API. Significantly Bard can solve CAPTCHAs visually that ChatGPT fails to understand, recommending Tesseract solutions. Moreover, while the Bard model proposes solutions based on visual input, it cannot recreate or modify the original visual objects to support its conclusions. Bard fails 
    
[^133]: 基于孟加拉文档布局分析数据集的框架和模型分析

    Framework and Model Analysis on Bengali Document Layout Analysis Dataset: BaDLAD. (arXiv:2309.16700v1 [cs.CV])

    [http://arxiv.org/abs/2309.16700](http://arxiv.org/abs/2309.16700)

    本研究使用先进计算机程序（Detectron2、YOLOv8和SAM）对孟加拉文档布局进行分析，通过比较它们的准确性和速度，找出适用于不同类型文档的最佳方法。研究有助于理解孟加拉文档中复杂的布局，并在其他语言中具有实用价值。

    

    本研究聚焦于使用先进计算机程序（Detectron2、YOLOv8和SAM）了解孟加拉文档布局。我们在研究中考察了多种不同的孟加拉文档。Detectron2在查找和分离文档的不同部分（如文本框和段落）方面表现出色。YOLOv8在识别不同的表格和图片方面表现良好。我们还尝试了SAM，它帮助我们理解棘手的布局。通过比较它们的准确性和速度，我们了解了哪个适用于不同类型的文档。我们的研究有助于解析孟加拉文档中复杂的布局，并且对其他语言也有用处。

    This study focuses on understanding Bengali Document Layouts using advanced computer programs: Detectron2, YOLOv8, and SAM. We looked at lots of different Bengali documents in our study. Detectron2 is great at finding and separating different parts of documents, like text boxes and paragraphs. YOLOv8 is good at figuring out different tables and pictures. We also tried SAM, which helps us understand tricky layouts. We tested these programs to see how well they work. By comparing their accuracy and speed, we learned which one is good for different types of documents. Our research helps make sense of complex layouts in Bengali documents and can be useful for other languages too.
    
[^134]: 扩展转化机器学习：分类问题

    Extension of Transformational Machine Learning: Classification Problems. (arXiv:2309.16693v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.16693](http://arxiv.org/abs/2309.16693)

    本研究通过研究转化机器学习（TML）在药物发现中的应用和性能，发现TML在预测精度、可解释性和泛化能力方面都优于传统机器学习方法，特别是在训练数据集数量足够时。

    

    本研究探讨了转化机器学习（TML）在药物发现中的应用和性能。作为一种元学习算法，TML在利用不同领域的共同属性方面表现出色，从而开发出超越传统模型的复合模型。药物发现过程复杂耗时，TML能够提供更好的预测精度、改善可解释性和更大的泛化能力，因此能够为此过程带来巨大的益处。我们探索了不同机器学习分类器的有效性，发现没有单个分类器具有明显的优势，因此考虑了集成分类器，如随机森林。我们的研究结果表明，随着训练数据集数量的增加，TML在性能上超过了基本的机器学习（ML），这是由于它能够更好地近似正确的假设、克服局部最优和扩展可表示函数的空间，通过结合不同分类器的能力。然而，这种超越只在训练数据集数量足够时才能实现。

    This study explores the application and performance of Transformational Machine Learning (TML) in drug discovery. TML, a meta learning algorithm, excels in exploiting common attributes across various domains, thus developing composite models that outperform conventional models. The drug discovery process, which is complex and time-consuming, can benefit greatly from the enhanced prediction accuracy, improved interpretability and greater generalizability provided by TML. We explore the efficacy of different machine learning classifiers, where no individual classifier exhibits distinct superiority, leading to the consideration of ensemble classifiers such as the Random Forest.  Our findings show that TML outperforms base Machine Learning (ML) as the number of training datasets increases, due to its capacity to better approximate the correct hypothesis, overcome local optima, and expand the space of representable functions by combining separate classifiers capabilities. However, this supe
    
[^135]: ecoBLE: 一种低计算能量消耗预测框架的蓝牙低功耗技术

    ecoBLE: A Low-Computation Energy Consumption Prediction Framework for Bluetooth Low Energy. (arXiv:2309.16686v1 [cs.NI])

    [http://arxiv.org/abs/2309.16686](http://arxiv.org/abs/2309.16686)

    ecoBLE是一种基于LSTMP的蓝牙低功耗能量消耗预测框架，考虑了物联网节点的所有组件，并提供了一个用于医疗应用的数据集。

    

    蓝牙低功耗 (BLE) 是物联网 (IoT) 应用中的一种事实标准技术，承诺极低的能量消耗。然而，这种低能量消耗只考虑了无线电部分，忽略了其他硬件和软件组件的能量消耗。监测和预测部署后的物联网节点的能量消耗可以大大帮助确保低能量消耗，计算剩余电池寿命，预测能量收集节点所需的能源，并检测异常情况。本文介绍了一种基于LSTMP的BLE能量消耗预测框架，以及在广泛采用BLE的医疗应用场景中使用的数据集。与以无线电为焦点的理论能量模型不同，我们的框架提供了全面的能量消耗预测，考虑了物联网节点的所有组件，包括无线电、传感器和微控制器单元(MCU)。

    Bluetooth Low Energy (BLE) is a de-facto technology for Internet of Things (IoT) applications, promising very low energy consumption. However, this low energy consumption accounts only for the radio part, and it overlooks the energy consumption of other hardware and software components. Monitoring and predicting the energy consumption of IoT nodes after deployment can substantially aid in ensuring low energy consumption, calculating the remaining battery lifetime, predicting needed energy for energy-harvesting nodes, and detecting anomalies. In this paper, we introduce a Long Short-Term Memory Projection (LSTMP)-based BLE energy consumption prediction framework together with a dataset for a healthcare application scenario where BLE is widely adopted. Unlike radio-focused theoretical energy models, our framework provides a comprehensive energy consumption prediction, considering all components of the IoT node, including the radio, sensor as well as microcontroller unit (MCU). Our measur
    
[^136]: 针对多模态蛋白质表示学习的面向目标的变分自编码器用于配体生成

    Target-aware Variational Auto-encoders for Ligand Generation with Multimodal Protein Representation Learning. (arXiv:2309.16685v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.16685](http://arxiv.org/abs/2309.16685)

    本研究提出了一种面向目标的变分自编码器（TargetVAE），通过蛋白质多模态网络（PMN）将蛋白质的不同表示统一到一个模型中，实现配体的生成。该方法能够从整个蛋白质结构中学习，并捕捉其顺序、拓扑和几何信息。

    

    在不了解特定口袋的情况下，基于蛋白质目标的整体结构生成配体在药物发现中起着关键作用，因为它有助于减少潜在药物候选在流水线中的搜索空间。然而，当代方法需要为每个蛋白质优化定制的网络，这是繁琐且昂贵的。为了解决这个问题，我们引入了TargetVAE，这是一个面向目标的变分自编码器，通过基于图形Transformer的新型多模态深度神经网络作为生成模型的先验，生成具有高结合亲和力的配体。这是第一个将蛋白质的不同表示（例如氨基酸序列、3D结构）统一到单一模型中的努力，我们将其命名为蛋白质多模态网络（PMN）。我们的多模态架构从整个蛋白质结构中学习，并能够捕捉它们的顺序、拓扑和几何信息。

    Without knowledge of specific pockets, generating ligands based on the global structure of a protein target plays a crucial role in drug discovery as it helps reduce the search space for potential drug-like candidates in the pipeline. However, contemporary methods require optimizing tailored networks for each protein, which is arduous and costly. To address this issue, we introduce TargetVAE, a target-aware variational auto-encoder that generates ligands with high binding affinities to arbitrary protein targets, guided by a novel multimodal deep neural network built based on graph Transformers as the prior for the generative model. This is the first effort to unify different representations of proteins (e.g., sequence of amino-acids, 3D structure) into a single model that we name as Protein Multimodal Network (PMN). Our multimodal architecture learns from the entire protein structures and is able to capture their sequential, topological and geometrical information. We showcase the supe
    
[^137]: 利用侧面信息和基于扩散的方法生成配体构象的方法

    Leveraging Side Information for Ligand Conformation Generation using Diffusion-Based Approaches. (arXiv:2309.16684v1 [q-bio.BM])

    [http://arxiv.org/abs/2309.16684](http://arxiv.org/abs/2309.16684)

    本研究提出了一种利用侧面信息和基于扩散的方法生成配体构象的新方法，通过引入灵活约束和配体-目标传递信息块来解决既有模型生成构象缺乏结构和随机性的问题。

    

    配体分子构象生成是药物发现中的重要挑战。近年来，通过使用生成模型特别是深度学习模型来解决这个问题取得了一些进展。然而，这些模型常常生成缺乏有意义的结构和随机性的构象，这是由于缺乏必要的侧面信息所致。这些侧面信息包括目标蛋白的化学和几何特征，配体-目标化合物的相互作用以及配体的化学性质。在没有这些约束的情况下，生成的构象可能不适合进一步选择和设计新药。为了解决这个限制，我们提出了一种利用侧面信息并将灵活约束引入标准扩散模型的生成配体构象的新方法。受到信息传递概念的启发，我们引入了配体-目标传递信息块，这个机制有助于信息的交换。

    Ligand molecule conformation generation is a critical challenge in drug discovery. Deep learning models have been developed to tackle this problem, particularly through the use of generative models in recent years. However, these models often generate conformations that lack meaningful structure and randomness due to the absence of essential side information. Examples of such side information include the chemical and geometric features of the target protein, ligand-target compound interactions, and ligand chemical properties. Without these constraints, the generated conformations may not be suitable for further selection and design of new drugs. To address this limitation, we propose a novel method for generating ligand conformations that leverage side information and incorporate flexible constraints into standard diffusion models. Drawing inspiration from the concept of message passing, we introduce ligand-target massage passing block, a mechanism that facilitates the exchange of info
    
[^138]: 使用深度强化学习控制Solo12四足机器人

    Controlling the Solo12 Quadruped Robot with Deep Reinforcement Learning. (arXiv:2309.16683v1 [cs.RO])

    [http://arxiv.org/abs/2309.16683](http://arxiv.org/abs/2309.16683)

    本论文介绍了在Solo12四足机器人上使用深度强化学习实现的强大的端到端学习控制器。这种控制器能够在遵循给定速度参考的同时，具有高效的能量消耗、稳健性和易于部署的特点。

    

    四足机器人需要强大且通用的运动技能，以在复杂和具有挑战性的环境中充分发挥其机动能力。在这项工作中，我们首次在Solo12四足机器人上实现了一个稳健的端到端学习控制器。我们的方法基于深度强化学习来学习关节阻抗参考。得到的控制策略在遵循给定速度参考的同时，具有能量消耗高效、稳健且易于部署的特点。我们详细介绍了学习过程和在真实机器人上的迁移方法。在我们的实验证明，Solo12机器人是一个适合结合学习和控制的开源平台，因为学习到的控制器可以轻松地迁移到并部署在真实机器人上。

    Quadruped robots require robust and general locomotion skills to exploit their mobility potential in complex and challenging environments. In this work, we present the first implementation of a robust end-to-end learning-based controller on the Solo12 quadruped. Our method is based on deep reinforcement learning of joint impedance references. The resulting control policies follow a commanded velocity reference while being efficient in its energy consumption, robust and easy to deploy. We detail the learning procedure and method for transfer on the real robot. In our experiments, we show that the Solo12 robot is a suitable open-source platform for research combining learning and control because of the easiness in transferring and deploying learned controllers.
    
[^139]: 利用深度学习和在线情绪分析进行金融投资组合管理

    Leveraging Deep Learning and Online Source Sentiment for Financial Portfolio Management. (arXiv:2309.16679v1 [q-fin.PM])

    [http://arxiv.org/abs/2309.16679](http://arxiv.org/abs/2309.16679)

    本文研究利用深度学习方法进行金融交易，并考虑情绪信息的作用。同时讨论常见的训练问题，并提供应用方法。

    

    金融投资组合管理是指在一系列金融资产（如股票、指数基金、外汇或加密货币）上分配资金并进行交易操作的任务，旨在最大化利润同时最小化交易操作所造成的损失。深度学习方法一直以来在各种任务中表现出色，自动化金融交易就是其中最复杂的任务之一。本文旨在提供关于金融交易中各种深度学习方法的见解，分别在监督学习和强化学习框架下进行讨论。同时，考虑到与交易资产相关的情绪信息，我们通过相应的研究研究论证了它们的有用性。最后，我们讨论了训练此类金融智能体中常见的问题，并为读者提供必要的知识，以避免这些问题并将讨论的方法应用于实践中。

    Financial portfolio management describes the task of distributing funds and conducting trading operations on a set of financial assets, such as stocks, index funds, foreign exchange or cryptocurrencies, aiming to maximize the profit while minimizing the loss incurred by said operations. Deep Learning (DL) methods have been consistently excelling at various tasks and automated financial trading is one of the most complex one of those. This paper aims to provide insight into various DL methods for financial trading, under both the supervised and reinforcement learning schemes. At the same time, taking into consideration sentiment information regarding the traded assets, we discuss and demonstrate their usefulness through corresponding research studies. Finally, we discuss commonly found problems in training such financial agents and equip the reader with the necessary knowledge to avoid these problems and apply the discussed methods in practice.
    
[^140]: 混合你自己的对比对

    Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])

    [http://arxiv.org/abs/2309.16633](http://arxiv.org/abs/2309.16633)

    本文提出了一种名为SupReMix的方法，通过混合样本，特别是混合负样本和混合正样本，来解决回归问题中表示学习的挑战。这种方法能够提供更好的性能和更准确的回归结果。

    

    在表示学习中，回归问题传统上比分类问题受到的关注较少。直接应用为分类设计的表示学习技术到回归问题往往会导致潜空间中碎片化的表示，从而产生次优的性能。本文认为，由于忽视了两个关键方面：序序感知和难度，对于回归问题而言，对比学习的潜能被忽视了。为了解决这些挑战，我们提倡“混合自己的对比对进行监督性对比回归”，而不仅仅依靠真实/增强样本。具体来说，我们提出了混合式监督对比回归学习（SupReMix）。它在嵌入级别上以锚点包含的混合（锚点和一个不同的负样本的混合）作为困难负对，以锚点排除的混合（两个不同的负样本的混合）作为困难正对。这一策略形成了困难样本对学习的方式。

    In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
    
[^141]: AutoCLIP: 自动调谐视觉语言模型的零样本分类器

    AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models. (arXiv:2309.16414v1 [cs.CV])

    [http://arxiv.org/abs/2309.16414](http://arxiv.org/abs/2309.16414)

    本研究提出了一种名为AutoCLIP的方法，用于自动调谐视觉语言模型的零样本分类器。AutoCLIP通过为每个提示模板分配图像特定的权重，从而改进了从编码类别描述符推导零样本分类器的方式。

    

    基于视觉语言模型（如CLIP）构建的分类器在广泛的图像分类任务中展现了出色的零样本性能。先前的工作研究了根据提示模板自动创建每个类别的描述符集的不同方式，包括手工设计的模板、从大型语言模型获取的模板以及从随机单词和字符构建的模板。然而，从相应的编码类别描述符导出零样本分类器几乎没有改变：将图像的平均编码类别描述符与编码图像之间的余弦相似度最大化以进行分类。然而，当某些描述符比其他描述符更好地匹配给定图像上的视觉线索时，将所有类别描述符等权重可能不是最优的。在这项工作中，我们提出了一种自动调谐零样本分类器的方法AutoCLIP。AutoCLIP为每个提示模板分配了图像特定的权重，这些权重是从s

    Classifiers built upon vision-language models such as CLIP have shown remarkable zero-shot performance across a broad range of image classification tasks. Prior work has studied different ways of automatically creating descriptor sets for every class based on prompt templates, ranging from manually engineered templates over templates obtained from a large language model to templates built from random words and characters. In contrast, deriving zero-shot classifiers from the respective encoded class descriptors has remained nearly unchanged, that is: classify to the class that maximizes the cosine similarity between its averaged encoded class descriptors and the encoded image. However, weighting all class descriptors equally can be suboptimal when certain descriptors match visual clues on a given image better than others. In this work, we propose AutoCLIP, a method for auto-tuning zero-shot classifiers. AutoCLIP assigns to each prompt template per-image weights, which are derived from s
    
[^142]: 时间图模型无法捕捉全局时间动态

    Temporal graph models fail to capture global temporal dynamics. (arXiv:2309.15730v1 [cs.IR])

    [http://arxiv.org/abs/2309.15730](http://arxiv.org/abs/2309.15730)

    时间图模型无法捕捉全局时间动态，我们提出了一种"最近流行节点"的基线方法，在时间图基准的中等和大规模数据集上胜过其他方法。我们提出了两个基于Wasserstein距离的度量来量化全局动态。我们展示了标准的负采样评估方法在具有强烈时间动态的数据集上可能不适用，我们还展示了简单的负采样方法可能导致模型退化。我们提出了改进的负采样方案，并证明了它们的有效性。我们还将其与无负采样的非对比训练模型进行了比较。

    

    在动态链接属性预测的背景下，我们分析了最近发布的时间图基准，并提出了一种"最近流行节点"的基线方法，在时间图基准的中等和大规模数据集上胜过其他方法。我们提出了基于Wasserstein距离的两个度量，可以量化数据集的短期和长期全局动态的强度。通过分析我们出乎意料的强大基线，我们展示了标准的负采样评估方法在具有强烈时间动态的数据集上可能不适用。我们还展示了简单的负采样方法在训练过程中可能导致模型退化，导致无法对时间图网络进行排序的预测完全饱和。我们提出了改进的负采样方案用于训练和评估，并证明了它们的有效性。我们还将其与无负采样的非对比训练模型进行了比较。我们的结果表明...

    A recently released Temporal Graph Benchmark is analyzed in the context of Dynamic Link Property Prediction. We outline our observations and propose a trivial optimization-free baseline of "recently popular nodes" outperforming other methods on all medium and large-size datasets in the Temporal Graph Benchmark. We propose two measures based on Wasserstein distance which can quantify the strength of short-term and long-term global dynamics of datasets. By analyzing our unexpectedly strong baseline, we show how standard negative sampling evaluation can be unsuitable for datasets with strong temporal dynamics. We also show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks. We propose improved negative sampling schemes for both training and evaluation and prove their usefulness. We conduct a comparison with a model trained non-contrastively without negative sampling. Our resul
    
[^143]: DTC: 深度跟踪控制--一种统一的基于模型的规划和强化学习方法，用于多功能和鲁棒的运动

    DTC: Deep Tracking Control -- A Unifying Approach to Model-Based Planning and Reinforcement-Learning for Versatile and Robust Locomotion. (arXiv:2309.15462v1 [cs.RO] CROSS LISTED)

    [http://arxiv.org/abs/2309.15462](http://arxiv.org/abs/2309.15462)

    本文提出了一种混合控制架构，同时结合了基于模型的规划和基于强化学习的方法，用于解决腿式运动的复杂控制问题。这种方法具有精确性、鲁棒性以及对稀疏奖励环境的适应能力。

    

    腿式运动是一个复杂的控制问题，需要精确性和鲁棒性来应对现实世界的挑战。传统上，腿式系统使用逆动力学的轨迹优化控制。这种层次化的基于模型的方法因其直观的成本函数调整、准确的规划以及超过十年的广泛研究所带来的深刻理解而具有吸引力。然而，模型不匹配和假设的违反是故障操作的常见原因，并可能妨碍成功的模拟到真实环境的转换。另一方面，基于仿真的强化学习产生了具有前所未有的鲁棒性和恢复能力的运动策略。然而，所有学习算法在稀疏奖励的环境中都面临着困难，这种环境中合法的落脚点很少，比如缺口或跳石。在这项工作中，我们提出了一种混合控制架构，结合了两个世界的优点，同时实现目标。

    Legged locomotion is a complex control problem that requires both accuracy and robustness to cope with real-world challenges. Legged systems have traditionally been controlled using trajectory optimization with inverse dynamics. Such hierarchical model-based methods are appealing due to intuitive cost function tuning, accurate planning, and most importantly, the insightful understanding gained from more than one decade of extensive research. However, model mismatch and violation of assumptions are common sources of faulty operation and may hinder successful sim-to-real transfer. Simulation-based reinforcement learning, on the other hand, results in locomotion policies with unprecedented robustness and recovery skills. Yet, all learning algorithms struggle with sparse rewards emerging from environments where valid footholds are rare, such as gaps or stepping stones. In this work, we propose a hybrid control architecture that combines the advantages of both worlds to simultaneously achie
    
[^144]: 最大扩散强化学习

    Maximum Diffusion Reinforcement Learning. (arXiv:2309.15293v1 [cs.LG])

    [http://arxiv.org/abs/2309.15293](http://arxiv.org/abs/2309.15293)

    最大扩散强化学习是一种克服强化学习中数据相关性问题的方法，通过解耦代理的经验实现持续学习，并在各种测试中表现出色。

    

    所有机器学习都建立在数据独立且同分布的假设上。然而，在强化学习中，当数据是依次从代理经验中收集而来时，这一假设通常不成立。因此，我们提出了一种名为最大扩散强化学习的方法，利用统计力学中的遍历过程来克服这些限制。我们的方法通过解耦代理的经验，可证明地使代理在单次部署中能够持续学习，而不受初始化方式的影响。此外，我们证明了我们的方法推广了众所周知的最大熵技术，并且通过在流行的基准测试中稳定超过了最先进的性能水平。我们的研究成果极大地促进了物理学、学习和控制的交叉领域，为强化学习代理（如行走机器人和自动驾驶汽车）的透明可靠决策提供了一条道路。

    The assumption that data are independent and identically distributed underpins all machine learning. When data are collected sequentially from agent experiences this assumption does not generally hold, as in reinforcement learning. Here, we derive a method that overcomes these limitations by exploiting the statistical mechanics of ergodic processes, which we term maximum diffusion reinforcement learning. By decorrelating agent experiences, our approach provably enables agents to learn continually in single-shot deployments regardless of how they are initialized. Moreover, we prove our approach generalizes well-known maximum entropy techniques, and show that it robustly exceeds state-of-the-art performance across popular benchmarks. Our results at the nexus of physics, learning, and control pave the way towards more transparent and reliable decision-making in reinforcement learning agents, such as locomoting robots and self-driving cars.
    
[^145]: 用于千米尺度大气降尺度的生成残差扩散建模

    Generative Residual Diffusion Modeling for Km-scale Atmospheric Downscaling. (arXiv:2309.15214v1 [cs.LG])

    [http://arxiv.org/abs/2309.15214](http://arxiv.org/abs/2309.15214)

    一种用于千米尺度大气降尺度的生成残差扩散建模方法被提出，并展示了在天气和气候的物理灾害预测方面具有潜力。

    

    当前从天气和气候中进行物理灾害预测的最先进方法需要进行昂贵的千米尺度数值模拟，并驱动较粗分辨率的全球输入。本文提出了一种千米尺度降尺度扩散模型作为一种具有成本效益的替代方法。该模型是从台湾的区域高分辨率天气模型训练得到的，并在ERA5再分析数据的基础下进行了条件训练。为了解决降尺度的不确定性，大分辨率比率（25km至2km），不同尺度上涉及的不同物理过程以及在输入数据中不存在的预测通道，我们采用了一个两步的方法（ResDiff），其中一个（UNet）回归在第一步预测平均值，而扩散模型在第二步预测残差。\textit{ResDiff}在块均方根误差和CRPS得分上表现出了令人鼓舞的技能。ResDiff预测的光谱和分布忠实地恢复了调节有害风和雨的重要幂律关系。统一的天气现象案例研究

    The state of the art for physical hazard prediction from weather and climate requires expensive km-scale numerical simulations driven by coarser resolution global inputs. Here, a km-scale downscaling diffusion model is presented as a cost effective alternative. The model is trained from a regional high-resolution weather model over Taiwan, and conditioned on ERA5 reanalysis data. To address the downscaling uncertainties, large resolution ratios (25km to 2km), different physics involved at different scales and predict channels that are not in the input data, we employ a two-step approach (\textit{ResDiff}) where a (UNet) regression predicts the mean in the first step and a diffusion model predicts the residual in the second step. \textit{ResDiff} exhibits encouraging skill in bulk RMSE and CRPS scores. The predicted spectra and distributions from ResDiff faithfully recover important power law relationships regulating damaging wind and rain extremes. Case studies of coherent weather phen
    
[^146]: BiSinger: 双语合成歌声系统

    BiSinger: Bilingual Singing Voice Synthesis. (arXiv:2309.14089v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2309.14089](http://arxiv.org/abs/2309.14089)

    BiSinger是一个双语合成歌声系统，通过设计共享表示、融合数据集和使用开源技术，实现了一种可以进行英语和汉语普通话混合编码歌声合成的单一模型，并保持了汉语歌曲的表现。

    

    尽管文本到语音合成技术在歌声合成方面取得了很大的进展，但多语种合成歌声模型仍然相对较少探索。本文介绍了BiSinger，这是一个用于英语和汉语普通话的双语流行歌声合成系统。当前的系统需要分别针对每种语言进行建模，并且无法准确地表示汉语和英语，从而阻碍了混合编码歌声合成。为了解决这个问题，我们设计了一种汉语和英语歌声之间的共享表示，通过使用CMU字典和映射规则实现。我们使用开源歌声转换技术融合了单语歌声数据集，以生成双语歌声，同时还探索了双语言音数据的潜在用途。实验验证了我们的语言无关表示和相关数据集的整合使得单一模型在英语和混合编码歌声合成方面性能更好，同时保持了汉语歌曲的表现。音频样本可在某网址找到。

    Although Singing Voice Synthesis (SVS) has made great strides with Text-to-Speech (TTS) techniques, multilingual singing voice modeling remains relatively unexplored. This paper presents BiSinger, a bilingual pop SVS system for English and Chinese Mandarin. Current systems require separate models per language and cannot accurately represent both Chinese and English, hindering code-switch SVS. To address this gap, we design a shared representation between Chinese and English singing voices, achieved by using the CMU dictionary with mapping rules. We fuse monolingual singing datasets with open-source singing voice conversion techniques to generate bilingual singing voices while also exploring the potential use of bilingual speech data. Experiments affirm that our language-independent representation and incorporation of related datasets enable a single model with enhanced performance in English and code-switch SVS while maintaining Chinese song performance. Audio samples are available at 
    
[^147]: 软混合降噪：超越扩散模型的表达瓶颈

    Soft Mixture Denoising: Beyond the Expressive Bottleneck of Diffusion Models. (arXiv:2309.14068v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.14068](http://arxiv.org/abs/2309.14068)

    本文发现了扩散模型在反向降噪中存在一个表达瓶颈，并且推出了一种新的软混合降噪（SMD）模型，该模型在理论上能够很好地逼近任意高斯混合分布，并且在实现上简单高效。

    

    由于扩散模型在诸如图像合成等任务中表现出令人印象深刻的性能，在最近的研究中有一种趋势证明（在某些假设下）这些模型具有强大的近似能力。本文中，我们展示了当前扩散模型在反向降噪方面实际上存在一个表达瓶颈，并且一些现有理论保证所做的假设过于强大。基于这一发现，我们证明了扩散模型在本地和全局降噪中存在无界误差。根据我们的理论研究，我们引入了软混合降噪（SMD），这是一个表达力强且高效的反向降噪模型。SMD不仅在理论上允许扩散模型很好地逼近任意高斯混合分布，而且在实现上简单高效。我们在多个图像数据集上的实验证明，SMD显著改善了不同类型的扩散模型（例如DDPM），尤其在少量反向降噪的情况下。

    Because diffusion models have shown impressive performances in a number of tasks, such as image synthesis, there is a trend in recent works to prove (with certain assumptions) that these models have strong approximation capabilities. In this paper, we show that current diffusion models actually have an expressive bottleneck in backward denoising and some assumption made by existing theoretical guarantees is too strong. Based on this finding, we prove that diffusion models have unbounded errors in both local and global denoising. In light of our theoretical studies, we introduce soft mixture denoising (SMD), an expressive and efficient model for backward denoising. SMD not only permits diffusion models to well approximate any Gaussian mixture distributions in theory, but also is simple and efficient for implementation. Our experiments on multiple image datasets show that SMD significantly improves different types of diffusion models (e.g., DDPM), espeically in the situation of few backw
    
[^148]: Fast-HuBERT: 一种高效的自监督语音表示学习训练框架

    Fast-HuBERT: An Efficient Training Framework for Self-Supervised Speech Representation Learning. (arXiv:2309.13860v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2309.13860](http://arxiv.org/abs/2309.13860)

    Fast-HuBERT是一种高效的自监督语音表示学习训练框架，通过分析HuBERT预训练过程中的计算成本，并引入一系列效率优化策略，实现了在Librispeech 960h基准上的5.2倍加速，并且在前人工作中得到了一致的改进。

    

    近年来，自监督学习（SSL）方法在语音处理任务中取得了显著进展。各种基于语音的SSL模型已经开发出来，并在一系列下游任务（包括语音识别）上展示了有希望的性能。然而，现有的基于语音的SSL模型在计算成本方面面临着共同的困境，这可能会阻碍它们的潜在应用和深入学术研究。为了解决这个问题，我们首先分析了HuBERT预训练过程中不同模块的计算成本，然后引入了一系列效率优化策略，即本文中提出的Fast-HuBERT。所提出的Fast-HuBERT可以在Librispeech 960h基准上使用8个V100 GPU在1.1天内进行训练，而不会导致性能下降，相比原始实现，加速了5.2倍。此外，我们还探索了Fast-HuBERT中的两种经过充分研究的技术，并展示了与之前工作中报告的一致的改进。

    Recent years have witnessed significant advancements in self-supervised learning (SSL) methods for speech-processing tasks. Various speech-based SSL models have been developed and present promising performance on a range of downstream tasks including speech recognition. However, existing speech-based SSL models face a common dilemma in terms of computational cost, which might hinder their potential application and in-depth academic research. To address this issue, we first analyze the computational cost of different modules during HuBERT pre-training and then introduce a stack of efficiency optimizations, which is named Fast-HuBERT in this paper. The proposed Fast-HuBERT can be trained in 1.1 days with 8 V100 GPUs on the Librispeech 960h benchmark, without performance degradation, resulting in a 5.2x speedup, compared to the original implementation. Moreover, we explore two well-studied techniques in the Fast-HuBERT and demonstrate consistent improvements as reported in previous work.
    
[^149]: 通过桥块分解学习大规模MTP$_2$高斯图模型

    Learning Large-Scale MTP$_2$ Gaussian Graphical Models via Bridge-Block Decomposition. (arXiv:2309.13405v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2309.13405](http://arxiv.org/abs/2309.13405)

    本论文通过桥块分解方法，提出了一种学习大规模MTP$_2$高斯图模型的策略，能够将大问题拆分为小问题进行优化，显著降低了计算复杂度并提高了算法速度。

    

    本文研究了学习大规模的MTP$_2$高斯图模型的问题。通过引入桥的概念，我们展示了整个问题可以通过对阈值样本协方差图上的几个较小规模的子问题进行桥块分解来等效优化，以及对应于桥的条目的一组明确解决方案。从实践角度来看，这个简单而可证明的方法可以将一个大问题拆分为小的可处理的问题，大大减少了计算复杂度并显著改进了所有现有算法。合成和真实世界的实验证明，我们提议的方法与现有技术基准相比，速度显著提升。

    This paper studies the problem of learning the large-scale Gaussian graphical models that are multivariate totally positive of order two ($\text{MTP}_2$). By introducing the concept of bridge, which commonly exists in large-scale sparse graphs, we show that the entire problem can be equivalently optimized through (1) several smaller-scaled sub-problems induced by a \emph{bridge-block decomposition} on the thresholded sample covariance graph and (2) a set of explicit solutions on entries corresponding to \emph{bridges}. From practical aspect, this simple and provable discipline can be applied to break down a large problem into small tractable ones, leading to enormous reduction on the computational complexity and substantial improvements for all existing algorithms. The synthetic and real-world experiments demonstrate that our proposed method presents a significant speed-up compared to the state-of-the-art benchmarks.
    
[^150]: 测试时间训练用于语音的应用

    Test-Time Training for Speech. (arXiv:2309.10930v1 [cs.SD])

    [http://arxiv.org/abs/2309.10930](http://arxiv.org/abs/2309.10930)

    本文研究了测试时间训练在语音应用中处理分布偏移的应用，提出了一种解决方案BitFit，该方案通过只考虑偏置参数进行微调，解决了测试时间训练中的关键挑战。

    

    本文研究了测试时间训练（TTT）在处理语音应用中的分布偏移问题上的应用。特别是，我们将分布偏移引入到标准语音分类任务的测试数据集中，例如说话人识别和情绪检测，并探讨测试时间训练如何帮助调整到这些分布偏移。在我们的实验中，我们包括了由背景噪声和语音的自然变化（如性别和年龄）引起的分布偏移，我们发现了测试时间训练的一些关键挑战，包括对优化超参数（例如优化步骤的数量和选择用于测试时间训练的参数子集）的敏感性以及可扩展性（例如，由于每个样本都有自己的一组参数，测试时间训练不可扩展）。最后，我们提出使用BitFit——一种仅考虑偏置参数进行微调的参数高效的文本应用中提出的微调算法——作为解决上述问题的方案。

    In this paper, we study the application of Test-Time Training (TTT) as a solution to handling distribution shifts in speech applications. In particular, we introduce distribution-shifts to the test datasets of standard speech-classification tasks -- for example, speaker-identification and emotion-detection -- and explore how Test-Time Training (TTT) can help adjust to the distribution-shift. In our experiments that include distribution shifts due to background noise and natural variations in speech such as gender and age, we identify some key-challenges with TTT including sensitivity to optimization hyperparameters (e.g., number of optimization steps and subset of parameters chosen for TTT) and scalability (e.g., as each example gets its own set of parameters, TTT is not scalable). Finally, we propose using BitFit -- a parameter-efficient fine-tuning algorithm proposed for text applications that only considers the bias parameters for fine-tuning -- as a solution to the aforementioned c
    
[^151]: 深入研究睡眠：基于单通道脑电图的睡眠阶段分类与模型解释性

    A Deep Dive into Sleep: Single-Channel EEG-Based Sleep Stage Classification with Model Interpretability. (arXiv:2309.07156v1 [eess.SP])

    [http://arxiv.org/abs/2309.07156](http://arxiv.org/abs/2309.07156)

    本文提出了一种基于单通道脑电图的睡眠阶段分类方法，使用SE-Resnet-Bi-LSTM架构进行睡眠阶段的分类，并进行了模型解释性分析。在三个不同的数据集上进行了全面评估，取得了显著的准确率和宏F1得分。

    

    睡眠是一种基本的生理过程，在我们的生活中占据着重要的部分。准确分类睡眠阶段是评估睡眠质量和识别可能的睡眠障碍的关键工具。本研究引入了一种新颖的方法，利用SE-Resnet-Bi-LSTM架构将睡眠分类为五个不同的阶段。分类过程基于对单通道脑电图（EEG）的分析。建议的框架由两个基本元素组成：利用SE-ResNet的特征提取器和利用Bi-LSTM单元堆栈的时间上下文编码器。我们的方法的有效性通过在三个不同的数据集上进行的全面评估得到证实，分别是SLeepEDF-20、SleepEDF-78和SHHS。值得注意的是，我们的方法在相应的数据集上分别达到了显著的准确率，分别为87.5％、83.9％和87.8％，并且在宏F1得分方面分别为82.5、78.9和81.9。

    Sleep, a fundamental physiological process, occupies a significant portion of our lives. Accurate classification of sleep stages serves as a crucial tool for evaluating sleep quality and identifying probable sleep disorders. This work introduces a novel methodology that utilises a SE-Resnet-Bi-LSTM architecture to classify sleep into five separate stages. The classification process is based on the analysis of single-channel electroencephalograms (EEGs). The framework that has been suggested consists of two fundamental elements: a feature extractor that utilises SE-ResNet, and a temporal context encoder that use stacks of Bi-LSTM units.The effectiveness of our approach is substantiated by thorough assessments conducted on three different datasets, namely SLeepEDF-20, SleepEDF-78, and SHHS. Significantly, our methodology attains notable levels of accuracy, specifically 87.5\%, 83.9\%, and 87.8\%, along with macro-F1 scores of 82.5, 78.9, and 81.9 for the corresponding datasets. Notably, 
    
[^152]: 离线逆向强化学习下的提示评估与优化

    Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])

    [http://arxiv.org/abs/2309.06553](http://arxiv.org/abs/2309.06553)

    这项工作介绍了一种基于离线逆向强化学习的提示评估与优化方法，通过利用离线数据集和逆向强化学习，预测提示性能、提高成本效益、生成易读的结果。

    

    最近，像ChatGPT这样的大型语言模型（LLM）的发展取得了显著的性能，通过利用人类专业知识。然而，充分揭示LLMs在复杂任务中的潜力需要在自然语言提示的广阔搜索空间中进行导航。虽然提示工程显示出潜力，但试错尝试中所需的人工设计提示和相关成本带来了重大挑战。关键是，提示优化的效率取决于昂贵的提示评估过程。本工作介绍了Prompt-OIRL，这是一种基于离线逆向强化学习的方法，旨在弥合有效提示评估和可负担性之间的差距。我们的方法利用专家评估的离线数据集，运用逆向强化学习获得一个针对离线、查询依赖型提示评估的奖励模型。Prompt-OIRL的优点是多方面的：它预测提示的性能，成本高效，生成易读的结果。

    The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
    
[^153]: PACE: 使用GPT-4进行云事件根本原因分析中的提示和增加以进行校准的置信度估计

    PACE: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis. (arXiv:2309.05833v1 [cs.CL])

    [http://arxiv.org/abs/2309.05833](http://arxiv.org/abs/2309.05833)

    本文提出了一种通过提示检索增强的大语言模型（LLM）来增强云事件根本原因分析工具中置信度估计的方法。

    

    近年来，IT行业向基于云的平台的转变强调了云事件根本原因分析的重要性，以确保服务的可靠性和维护客户信任。核心问题是有效确定根本原因，由于当代云基础设施的复杂性，这一任务变得具有挑战性。尽管出现了许多用于根本原因识别的基于AI的工具，但它们的适用性仍受到其输出质量不一致的限制。本文介绍了一种通过提示检索增强的大语言模型（LLM）来增强根本原因分析工具中置信度估计的方法。此方法分为两个阶段。首先，模型根据历史事件数据评估自身的置信度，考虑其对证据的评估强度。然后，模型审核由预测器生成的根本原因。然后，优化步骤将这些评估结合起来确定最终的置信度估计。

    In recent years, the transition to cloud-based platforms in the IT sector has emphasized the significance of cloud incident root cause analysis to ensure service reliability and maintain customer trust. Central to this process is the efficient determination of root causes, a task made challenging due to the complex nature of contemporary cloud infrastructures. Despite the proliferation of AI-driven tools for root cause identification, their applicability remains limited by the inconsistent quality of their outputs. This paper introduces a method for enhancing confidence estimation in root cause analysis tools by prompting retrieval-augmented large language models (LLMs). This approach operates in two phases. Initially, the model evaluates its confidence based on historical incident data, considering its assessment of the evidence strength. Subsequently, the model reviews the root cause generated by the predictor. An optimization step then combines these evaluations to determine the fin
    
[^154]: SLiMe: 像我一样进行分割

    SLiMe: Segment Like Me. (arXiv:2309.03179v1 [cs.CV])

    [http://arxiv.org/abs/2309.03179](http://arxiv.org/abs/2309.03179)

    基于大型视觉语言模型的SLiMe方法通过提取注意力图和优化文本嵌入实现图像分割，只需要极少的标注样本即可进行任意细粒度的分割。

    

    使用大型视觉语言模型（如稳定扩散SD），在诸多下游任务（包括图像编辑、图像对应和3D形状生成）方面取得了显著进展。受到这些进展的启发，我们探索利用这些广泛的视觉语言模型，通过提出SLiMe，以尽可能少的标注样本对图像进行任意细粒度的分割。SLiMe将这个问题作为一个优化任务来进行。具体而言，给定一张训练图像及其分割掩膜，我们首先从SD先验中提取注意力图，包括我们的新颖的“加权累积自注意力图”。然后，利用提取的注意力图，优化稳定扩散的文本嵌入，使得每个嵌入只学习训练图像中的一个分割区域。这些学习到的嵌入然后在注意力图中突出显示分割区域，从而可以生成分割图。这使得SLiMe可以进行图像分割。

    Significant strides have been made using large vision-language models, like Stable Diffusion (SD), for a variety of downstream tasks, including image editing, image correspondence, and 3D shape generation. Inspired by these advancements, we explore leveraging these extensive vision-language models for segmenting images at any desired granularity using as few as one annotated sample by proposing SLiMe. SLiMe frames this problem as an optimization task. Specifically, given a single training image and its segmentation mask, we first extract attention maps, including our novel "weighted accumulated self-attention map" from the SD prior. Then, using the extracted attention maps, the text embeddings of Stable Diffusion are optimized such that, each of them, learn about a single segmented region from the training image. These learned embeddings then highlight the segmented region in the attention maps, which in turn can then be used to derive the segmentation map. This enables SLiMe to segmen
    
[^155]: FTA: 具有灵活触发器的隐蔽且强健的联邦学习后门攻击

    FTA: Stealthy and Robust Backdoor Attack with Flexible Trigger on Federated Learning. (arXiv:2309.00127v1 [cs.LG])

    [http://arxiv.org/abs/2309.00127](http://arxiv.org/abs/2309.00127)

    FTA提出了一种新的灵活触发器、隐蔽且强健的联邦学习后门攻击方法，通过生成学习灵活触发器模式来操作良性样本，同时保留攻击者选择的标签的最重要隐藏特征。通过填充可区分的差异，使攻击具有隐蔽性。验证了该方法的有效性和可靠性。

    

    当前针对联邦学习（FL）的后门攻击在很大程度上依赖于通用触发器或语义模式，这可以被某些防御机制（如范数剪裁，比较局部更新之间的参数差异）轻易检测和过滤。在这项工作中，我们提出了一种新的隐蔽且强健的具有灵活触发器的联邦学习后门攻击。为了实现这一点，我们构建了一个生成触发器函数，可以学习使用一个几乎不可察觉的灵活触发器模式来操作良性样本，并同时让触发器模式包含攻击者选择的标签的最重要的隐藏特征。此外，我们的触发器生成器可以在不同轮次中不断学习和适应，从而使其能够调整到全局模型的变化。通过填充可区分的差异（触发器模式和目标标签之间的映射），我们使我们的攻击具有自然的隐蔽性。对真实数据集的大量实验验证了我们攻击的有效性和可靠性。

    Current backdoor attacks against federated learning (FL) strongly rely on universal triggers or semantic patterns, which can be easily detected and filtered by certain defense mechanisms such as norm clipping, comparing parameter divergences among local updates. In this work, we propose a new stealthy and robust backdoor attack with flexible triggers against FL defenses. To achieve this, we build a generative trigger function that can learn to manipulate the benign samples with an imperceptible flexible trigger pattern and simultaneously make the trigger pattern include the most significant hidden features of the attacker-chosen label. Moreover, our trigger generator can keep learning and adapt across different rounds, allowing it to adjust to changes in the global model. By filling the distinguishable difference (the mapping between the trigger pattern and target label), we make our attack naturally stealthy. Extensive experiments on real-world datasets verify the effectiveness and st
    
[^156]: Jais和Jais-chat：以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型

    Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models. (arXiv:2308.16149v1 [cs.CL])

    [http://arxiv.org/abs/2308.16149](http://arxiv.org/abs/2308.16149)

    Jais和Jais-chat是新的以阿拉伯语为中心的开放式生成式大型语言模型，具有13亿参数，在阿拉伯语方面表现出优异的知识和推理能力，并且在英语方面也具有竞争力。这些模型的发布旨在促进阿拉伯语LLMs的研究。

    

    我们介绍了Jais和Jais-chat，这是新的最先进的以阿拉伯语为中心的基础和指导调优的开放式生成式大型语言模型（LLMs）。这些模型基于GPT-3的仅解码器架构，并在阿拉伯语和英语文本的混合物中进行预训练，包括各种编程语言的源代码。这些模型具有130亿个参数，根据广泛的评估结果，在阿拉伯语知识和推理能力方面表现优于任何现有的开放式阿拉伯语和多语言模型。此外，尽管在训练时使用的英语数据要少得多，但这些模型在英语方面与类似规模的以英语为中心的开放模型相比仍具有竞争力。我们提供了模型的训练、调优、安全对齐和评估的详细描述。我们发布了模型的两个开放版本--基础Jais模型和指导调优的Jais-chat变种--旨在促进阿拉伯语LLMs的研究。详见https://hugging

    We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://hugging
    
[^157]: 测量篡改检测基准

    Measurement Tampering Detection Benchmark. (arXiv:2308.15605v1 [cs.LG])

    [http://arxiv.org/abs/2308.15605](http://arxiv.org/abs/2308.15605)

    本文构建了四个文本数据集用于评估测量篡改检测技术，研究人工智能系统操纵测量结果以营造良好结果的问题。虽然展示了优于基准的技术，但还有很大的改进空间。

    

    在训练强大的人工智能系统来执行复杂任务时，提供对优化具有稳健性的训练信号可能是具有挑战性的。一个问题是测量篡改，即人工智能系统操纵多个测量结果，以营造良好结果的假象，而不是实现期望的结果。在这项工作中，我们构建了四个新的基于文本的数据集，用于评估大规模语言模型上的测量篡改检测技术。具体来说，给定一组文本输入和测量结果，旨在确定某个结果是否发生，以及一个能够准确预测测量结果的基础模型，目标是确定所有测量结果都表明结果发生的示例是否确实发生了结果，或者这是由于测量篡改引起的。我们展示了在大多数数据集上优于简单基准的技术，但没有达到最佳性能。我们相信在技术和数据集方面都有很大的改进空间，我们感到兴奋。

    When training powerful AI systems to perform complex tasks, it may be challenging to provide training signals which are robust to optimization. One concern is measurement tampering, where the AI system manipulates multiple measurements to create the illusion of good results instead of achieving the desired outcome. In this work, we build four new text-based datasets to evaluate measurement tampering detection techniques on large language models. Concretely, given sets of text inputs and measurements aimed at determining if some outcome occurred, as well as a base model able to accurately predict measurements, the goal is to determine if examples where all measurements indicate the outcome actually had the outcome occur, or if this was caused by measurement tampering. We demonstrate techniques that outperform simple baselines on most datasets, but don't achieve maximum performance. We believe there is significant room for improvement for both techniques and datasets, and we are excited 
    
[^158]: 一种针对多任务学习的尺度不变任务平衡方法

    A Scale-Invariant Task Balancing Approach for Multi-Task Learning. (arXiv:2308.12029v1 [cs.LG])

    [http://arxiv.org/abs/2308.12029](http://arxiv.org/abs/2308.12029)

    这篇论文提出了一种尺度不变的多任务学习方法（SI-MTL），通过对任务损失进行对数变换和对任务梯度进行归一化，解决了多任务学习中的任务平衡问题，并在多个基准数据集上取得了领先的性能。

    

    多任务学习（MTL）是一种同时学习多个相关任务的学习范式，在各个领域取得了巨大的成功。然而，任务平衡仍然是MTL中的一个重要挑战，损失/梯度尺度的不平衡经常导致性能折中。本文提出了一种尺度不变的多任务学习（SI-MTL）方法，从损失和梯度角度缓解了任务平衡问题。具体来说，SI-MTL包含对所有任务损失进行的对数变换，以确保在损失水平上具有尺度不变性，以及一种梯度平衡方法SI-G，它将所有任务的梯度归一化为与最大梯度范数相同的大小。在几个基准数据集上进行的大量实验一致证明了SI-G的有效性和SI-MTL的最先进性能。

    Multi-task learning (MTL), a learning paradigm to learn multiple related tasks simultaneously, has achieved great success in various fields. However, task-balancing remains a significant challenge in MTL, with the disparity in loss/gradient scales often leading to performance compromises. In this paper, we propose a Scale-Invariant Multi-Task Learning (SI-MTL) method to alleviate the task-balancing problem from both loss and gradient perspectives. Specifically, SI-MTL contains a logarithm transformation which is performed on all task losses to ensure scale-invariant at the loss level, and a gradient balancing method, SI-G, which normalizes all task gradients to the same magnitude as the maximum gradient norm. Extensive experiments conducted on several benchmark datasets consistently demonstrate the effectiveness of SI-G and the state-of-the-art performance of SI-MTL.
    
[^159]: 通过去混淆器解决有偏见的电子健康记录中的健康差异问题

    Mitigating Health Disparity on Biased Electronic Health Records via Deconfounder. (arXiv:2308.11819v1 [cs.LG])

    [http://arxiv.org/abs/2308.11819](http://arxiv.org/abs/2308.11819)

    通过使用去混淆器模型FLMD，在电子健康记录建模中实现了公平性和准确性，解决了有偏见的EHR中的健康差异问题。

    

    临床数据建模的公平性问题，尤其是在电子健康记录（EHRs）上，由于EHR的复杂潜在结构和潜在选择偏差问题而至关重要。在实践中，通常需要在保持模型整体准确性的同时减少健康差异。然而，传统方法往往在准确性和公平性之间存在权衡，因为它们无法捕捉到观察数据之外的潜在因素。为了解决这个挑战，我们提出了一个名为公平纵向医疗去混淆器（FLMD）的新模型，旨在实现纵向电子健康记录（EHR）的公平性和准确性建模。受到去混淆器理论的启发，FLMD采用了一个两阶段的训练过程。在第一阶段，FLMD捕捉了每次接触的未观察到的混淆因子，这有效地表示了超出观察到的EHR之外的潜在医疗因素，如患者基因型和生活习惯。这个未观察到的混淆因子是至关重要的。

    The fairness issue of clinical data modeling, especially on Electronic Health Records (EHRs), is of utmost importance due to EHR's complex latent structure and potential selection bias. It is frequently necessary to mitigate health disparity while keeping the model's overall accuracy in practice. However, traditional methods often encounter the trade-off between accuracy and fairness, as they fail to capture the underlying factors beyond observed data. To tackle this challenge, we propose a novel model called Fair Longitudinal Medical Deconfounder (FLMD) that aims to achieve both fairness and accuracy in longitudinal Electronic Health Records (EHR) modeling. Drawing inspiration from the deconfounder theory, FLMD employs a two-stage training process. In the first stage, FLMD captures unobserved confounders for each encounter, which effectively represents underlying medical factors beyond observed EHR, such as patient genotypes and lifestyle habits. This unobserved confounder is crucial 
    
[^160]: 在大型语言模型中使用反向推理进行验证

    Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])

    [http://arxiv.org/abs/2308.07758](http://arxiv.org/abs/2308.07758)

    本文研究了在大型语言模型中使用反向推理进行验证的方法。作者提出了一种新颖的技术，通过屏蔽问题中的一个标记，并要求语言模型预测被屏蔽的标记来验证候选答案。同时，作者还提出了一种结合正向和反向推理的方法来估计候选答案的概率。

    

    链式思考（Chain-of-Though, CoT）提示在各种推理任务中表现出了很好的性能。最近，Self-Consistency提出了一种方法，即通过采样一组不同的推理链，这些链可能导致不同的答案，然后选择得票最多的答案。本文提出了一种新颖的方法，即在验证候选答案时使用反向推理。我们使用一个简单的模板，即``如果我们知道上述问题的答案是候选答案，那么未知变量x的值是多少？''，将问题中的一个标记屏蔽，并要求语言模型预测被屏蔽的标记。直观上讲，如果提供的候选答案是正确的，语言模型应该能够成功预测被屏蔽的标记。我们进一步提出了FOBAR方法，将正向和反向推理结合起来估计候选答案的概率。我们在六个数据集和三个实验中进行了广泛的实验。

    Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
    
[^161]: 为机器人堆积方块任务构建因果性概率框架

    Towards a Causal Probabilistic Framework for Prediction, Action-Selection & Explanations for Robot Block-Stacking Tasks. (arXiv:2308.06203v1 [cs.RO])

    [http://arxiv.org/abs/2308.06203](http://arxiv.org/abs/2308.06203)

    这项工作提出了一个新颖的因果性概率框架，用于解决机器人堆积方块任务的问题，通过结合因果推断，使机器人能够理解、推理和解释其环境。

    

    现实世界中的不确定性意味着系统设计者无法预测并明确设计出机器人可能遇到的所有场景。因此，以这种方式设计的机器人在高度受控的环境之外容易出现故障。因果模型提供了一个原则性的框架，用于编码机器人与其环境相互作用的因果关系的形式化知识，并结合现实世界机器人通常遇到的噪声和不确定性的概率表示。结合因果推断，这些模型使自主代理能够理解、推理和解释其环境。在这项工作中，我们关注机器人堆积方块任务的问题，因为它展示了许多应用所需的基本感知和操作能力，包括仓库物流和家庭人工支持机器人。我们提出了一个新颖的因果性概率框架，将物理模拟功能嵌入到这个任务中。

    Uncertainties in the real world mean that is impossible for system designers to anticipate and explicitly design for all scenarios that a robot might encounter. Thus, robots designed like this are fragile and fail outside of highly-controlled environments. Causal models provide a principled framework to encode formal knowledge of the causal relationships that govern the robot's interaction with its environment, in addition to probabilistic representations of noise and uncertainty typically encountered by real-world robots. Combined with causal inference, these models permit an autonomous agent to understand, reason about, and explain its environment. In this work, we focus on the problem of a robot block-stacking task due to the fundamental perception and manipulation capabilities it demonstrates, required by many applications including warehouse logistics and domestic human support robotics. We propose a novel causal probabilistic framework to embed a physics simulation capability int
    
[^162]: 扩散模型是否会受到错误传播的影响？理论分析和一致性正则化。

    Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])

    [http://arxiv.org/abs/2308.05021](http://arxiv.org/abs/2308.05021)

    扩散模型可能受到错误传播的影响，本文通过理论分析和数据实证验证了这个问题，并提出了一种一致性正则化方法来解决。我们的分析揭示了模型的去噪模块是否具有容错性以及如何减少分布差异。

    

    虽然扩散模型在数据合成方面取得了令人期待的成果，但由于其级联结构，即去噪模块链式传播和放大了分布不匹配的错误，因此可能会受到错误传播的影响。然而，我们期望进行严格的分析，因为许多顺序模型，如条件随机场（CRF），是不会出现错误传播的。在本文中，我们通过实证和理论验证了扩散模型确实受到错误传播的影响，并提出一种正则化方法来解决这个问题。我们的理论分析揭示了这个问题是否可以归结为扩散模型的每个去噪模块是否具有容错性。我们推导出了有深刻见解的转移方程，表明模块无法从输入错误中恢复，甚至会将额外的错误传播到下一个模块。我们的分析直接导致了扩散模型的一致性正则化方案，可以明确减少分布的差异。

    While diffusion models have achieved promising performances in data synthesis, they might suffer error propagation because of their cascade structure, where the distributional mismatch spreads and magnifies through the chain of denoising modules. However, a strict analysis is expected since many sequential models such as Conditional Random Field (CRF) are free from error propagation. In this paper, we empirically and theoretically verify that diffusion models are indeed affected by error propagation and we then propose a regularization to address this problem. Our theoretical analysis reveals that the question can be reduced to whether every denoising module of the diffusion model is fault-tolerant. We derive insightful transition equations, indicating that the module can't recover from input errors and even propagates additional errors to the next module. Our analysis directly leads to a consistency regularization scheme for diffusion models, which explicitly reduces the distribution 
    
[^163]: 从假到真（FFR）：一种用于减少与合成数据相关性错误的两阶段训练流程

    From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])

    [http://arxiv.org/abs/2308.04553](http://arxiv.org/abs/2308.04553)

    本文提出了一个两阶段训练流程，通过在一个平衡的合成数据集上进行预训练，然后在真实数据上进行微调，减少了视觉识别模型学习到与数据集偏差相关的错误的问题。

    

    视觉识别模型容易学习到由于训练集的不平衡导致的相关性错误，其中某些群体（如女性）在某些类别（如程序员）中代表性不足。生成模型通过为少数样本生成合成数据来减少这种偏差，从而平衡训练集。然而，先前使用这些方法的工作忽视了视觉识别模型往往能够学习区分真实图像和合成图像的能力，因此无法消除原始数据集中的偏差。在我们的工作中，我们提出了一种新颖的两阶段流程来减少这个问题，其中1）我们在平衡的合成数据集上进行预训练，然后2）在真实数据上进行微调。使用这个流程，我们避免了在真实数据和合成数据上的训练，从而避免了真实数据和合成数据之间的偏差。此外，在第一步中我们学习到了抵抗偏差的稳健特征，在第二步中减轻了偏差。

    Visual recognition models are prone to learning spurious correlations induced by an imbalanced training set where certain groups (\eg Females) are under-represented in certain classes (\eg Programmers). Generative models offer a promising direction in mitigating this bias by generating synthetic data for the minority samples and thus balancing the training set. However, prior work that uses these approaches overlooks that visual recognition models could often learn to differentiate between real and synthetic images and thus fail to unlearn the bias in the original dataset. In our work, we propose a novel two-stage pipeline to mitigate this issue where 1) we pre-train a model on a balanced synthetic dataset and then 2) fine-tune on the real data. Using this pipeline, we avoid training on both real and synthetic data, thus avoiding the bias between real and synthetic data. Moreover, we learn robust features against the bias in the first step that mitigate the bias in the second step. Mor
    
[^164]: 时间常识推理与获取概述

    An Overview Of Temporal Commonsense Reasoning and Acquisition. (arXiv:2308.00002v1 [cs.AI])

    [http://arxiv.org/abs/2308.00002](http://arxiv.org/abs/2308.00002)

    本文综述了时间常识推理领域的研究进展，重点关注通过增强语言模型的性能来提高推理能力，并对多个数据集进行评估。然而，这些增强模型仍然难以达到人类水平的推理能力。

    

    时间常识推理是指理解短语、动作和事件的典型时间背景并将其应用于需要这种知识的问题推理的能力。这种能力在时间自然语言处理任务中至关重要，可能应用于时间线摘要、时间问答和时间自然语言推断等方面。最近的研究表明，大型语言模型虽然善于生成语法正确的句子和解决分类任务，但在推理过程中往往会采取捷径，并陷入简单的语言陷阱。本文章概述了在时间常识推理领域的研究，特别关注通过各种增强方式提高语言模型的性能以及对越来越多数据集的评估。然而，这些增强模型在推理任务上仍然难以达到人类的水平。

    Temporal commonsense reasoning refers to the ability to understand the typical temporal context of phrases, actions, and events, and use it to reason over problems requiring such knowledge. This trait is essential in temporal natural language processing tasks, with possible applications such as timeline summarization, temporal question answering, and temporal natural language inference. Recent research on the performance of large language models suggests that, although they are adept at generating syntactically correct sentences and solving classification tasks, they often take shortcuts in their reasoning and fall prey to simple linguistic traps. This article provides an overview of research in the domain of temporal commonsense reasoning, particularly focusing on enhancing language model performance through a variety of augmentations and their evaluation across a growing number of datasets. However, these augmented models still struggle to approach human performance on reasoning task
    
[^165]: 用于16位神经网络训练中数值不稳定性的高效方法

    An Efficient Approach to Mitigate Numerical Instability in Backpropagation for 16-bit Neural Network Training. (arXiv:2307.16189v1 [cs.LG])

    [http://arxiv.org/abs/2307.16189](http://arxiv.org/abs/2307.16189)

    这项研究探讨了16位计算中机器学习模型的数值不稳定性问题，并提出了一种基于Adam优化器的新方法来提高16位神经网络的学习过程的鲁棒性。

    

    在这项研究中，我们深入探讨了在16位计算中使用流行的优化算法（如RMSProp和Adam）时观察到的数值不稳定性的复杂性。这种不稳定性通常在深度神经网络的训练阶段中出现，导致学习过程受到干扰，从而妨碍了这些模型的有效部署。我们确定了单一超参数epsilon是这种数值不稳定性的主要原因。对16位计算中这些优化器中epsilon的作用进行了深入探索，发现微调其值可以恢复RMSProp和Adam的功能，从而实现有效利用16位神经网络。我们提出了一种新的方法来减轻被发现的数值不稳定性问题。该方法利用Adam优化器的更新，并显著改善了16位神经网络的学习过程的鲁棒性。

    In this research, we delve into the intricacies of the numerical instability observed in 16-bit computations of machine learning models, particularly when employing popular optimization algorithms such as RMSProp and Adam. This instability is commonly experienced during the training phase of deep neural networks, leading to disrupted learning processes and hindering the effective deployment of such models. We identify the single hyperparameter, epsilon, as the main culprit behind this numerical instability. An in-depth exploration of the role of epsilon in these optimizers within 16-bit computations reveals that a minor adjustment of its value can restore the functionality of RMSProp and Adam, consequently enabling the effective utilization of 16-bit neural networks. We propose a novel method to mitigate the identified numerical instability issues. This method capitalizes on the updates from the Adam optimizer and significantly improves the robustness of the learning process in 16-bit 
    
[^166]: Federated Learning在官方统计中的适用性

    The Applicability of Federated Learning to Official Statistics. (arXiv:2307.15503v1 [cs.LG])

    [http://arxiv.org/abs/2307.15503](http://arxiv.org/abs/2307.15503)

    Federated Learning（FL）在官方统计中具有潜力，可以保护数据隐私并提升数据质量。

    

    本研究调查了Federated Learning（FL）在官方统计中的潜力，并展示了FL模型性能与集中式学习方法的匹配程度。同时，利用FL可以保护数据持有者的隐私，从而便于获取更广泛的数据并最终提升官方统计数据。通过模拟三种不同的使用案例，我们对这种技术的适用性获得了重要的见解。这些使用案例基于医疗保险数据集、细颗粒物污染数据集和移动无线信号覆盖数据集，这些数据集与官方统计密切相关。我们详细分析了结果，包括对每个模拟中集中式和FL算法性能的比较。在这三个使用案例中，我们能够通过FL训练模型，其性能与集中式模型基准非常接近。我们的关键观察和它们对模拟转移的影响。

    This work investigates the potential of Federated Learning (FL) for official statistics and shows how well the performance of FL models can keep up with centralized learning methods. At the same time, its utilization can safeguard the privacy of data holders, thus facilitating access to a broader range of data and ultimately enhancing official statistics. By simulating three different use cases, important insights on the applicability of the technology are gained. The use cases are based on a medical insurance data set, a fine dust pollution data set and a mobile radio coverage data set - all of which are from domains close to official statistics. We provide a detailed analysis of the results, including a comparison of centralized and FL algorithm performances for each simulation. In all three use cases, we were able to train models via FL which reach a performance very close to the centralized model benchmarks. Our key observations and their implications for transferring the simulatio
    
[^167]: XSkill：跨体现技能发现

    XSkill: Cross Embodiment Skill Discovery. (arXiv:2307.09955v1 [cs.RO])

    [http://arxiv.org/abs/2307.09955](http://arxiv.org/abs/2307.09955)

    本研究提出了XSkill，一种跨体现的技能发现框架，能够从无标签的人类和机器人操纵视频中纯粹地发现跨体现技能原型，并通过条件扩散策略将这些技能转移到机器人动作中，在未见任务中完成学习到的技能的组合。仿真和真实环境中的实验结果表明，这些发现的技能原型能够有效地促进技能转移和组合，从而构建出更通用和可扩展的模仿学习框架。

    

    人类示范视频是机器人学习的广泛数据源，并且是表达所需行为的直观用户界面。然而，直接从非结构化的人类视频中提取可重用的机器人操纵技能面临着体现差异和未观察到的行动参数的挑战。为了弥合这种体现差距，本文介绍了XSkill，一种模仿学习框架，它从无标签的人类和机器人操纵视频中纯粹地发现名为技能原型的跨体现表示，使用条件扩散策略将技能表示转移到机器人动作，并最终使用人类提示视频完成学习到的技能来完成未见任务。我们在仿真和真实环境中的实验表明，发现的技能原型促进了未见任务的技能转移和组合，从而实现了更通用和可扩展的模仿学习框架。

    Human demonstration videos are a widely available data source for robot learning and an intuitive user interface for expressing desired behavior. However, directly extracting reusable robot manipulation skills from unstructured human videos is challenging due to the big embodiment difference and unobserved action parameters. To bridge this embodiment gap, this paper introduces XSkill, an imitation learning framework that 1) discovers a cross-embodiment representation called skill prototypes purely from unlabeled human and robot manipulation videos, 2) transfers the skill representation to robot actions using conditional diffusion policy, and finally, 3) composes the learned skill to accomplish unseen tasks specified by a human prompt video. Our experiments in simulation and real-world environments show that the discovered skill prototypes facilitate both skill transfer and composition for unseen tasks, resulting in a more general and scalable imitation learning framework. The performan
    
[^168]: 通过解耦置信学习来减缓标签偏见

    Mitigating Label Bias via Decoupled Confident Learning. (arXiv:2307.08945v1 [cs.LG])

    [http://arxiv.org/abs/2307.08945](http://arxiv.org/abs/2307.08945)

    这项研究提出了一种名为DeCoLe的修剪方法，用于减轻标签偏见问题。研究在合成数据集和仇恨言论检测领域取得了成功的结果。

    

    随着对算法公平性的担忧增加，出现了许多方法来减轻算法偏见。然而，这些方法主要假设训练数据中观察到的标签是正确的。这是有问题的，因为标签偏见在包括医疗、招聘和内容审核在内的重要领域中普遍存在。特别是，人类生成的标签容易带有社会偏见。虽然标签偏见的存在已经在概念上进行了讨论，但缺乏应对此问题的方法。我们提出了一种修剪方法——解耦置信学习（DeCoLe），专门设计来减轻标签偏见。在演示了其在合成数据集上的性能后，我们将DeCoLe应用于仇恨言论检测的环境中，这是一个被认为是重要挑战的领域，并展示其成功识别出偏见标签并超越竞争方法的表现。

    Growing concerns regarding algorithmic fairness have led to a surge in methodologies to mitigate algorithmic bias. However, such methodologies largely assume that observed labels in training data are correct. This is problematic because bias in labels is pervasive across important domains, including healthcare, hiring, and content moderation. In particular, human-generated labels are prone to encoding societal biases. While the presence of labeling bias has been discussed conceptually, there is a lack of methodologies to address this problem. We propose a pruning method -- Decoupled Confident Learning (DeCoLe) -- specifically designed to mitigate label bias. After illustrating its performance on a synthetic dataset, we apply DeCoLe in the context of hate speech detection, where label bias has been recognized as an important challenge, and show that it successfully identifies biased labels and outperforms competing approaches.
    
[^169]: 将电池电解质的结构组成映射到器件性能的配方图

    Formulation Graphs for Mapping Structure-Composition of Battery Electrolytes to Device Performance. (arXiv:2307.03811v1 [cond-mat.mtrl-sci])

    [http://arxiv.org/abs/2307.03811](http://arxiv.org/abs/2307.03811)

    该论文提出了一种深度学习模型，Formulation Graph Convolution Network（F-GCN），它可以将电池电解质的结构组成关系映射到整个液体配方的性能，从而加快新化合物的发现和应用。

    

    正在积极寻求先进的计算方法来解决发现和开发新的组合材料（如配方）所面临的挑战。一个广泛采用的方法是领域感知的高通量筛选，该方法可以将各个组分组合成配方。这种方法能够加速寻找目标应用的新化合物，但是在从精选化学空间中识别出合适的“配方”方面仍然主要是实验驱动的过程。我们报告了一种深度学习模型，称为Formulation Graph Convolution Network（F-GCN），可以将各个组分的结构组成关系映射到液体配方的性质。多个GCN并行组装，并根据配方中各个组成部分的摩尔百分比直观地对配方成分进行特征化。然后根据相应组成部分的摩尔百分比对所得的分子描述符进行缩放，接下来进行...

    Advanced computational methods are being actively sought for addressing the challenges associated with discovery and development of new combinatorial material such as formulations. A widely adopted approach involves domain informed high-throughput screening of individual components that can be combined into a formulation. This manages to accelerate the discovery of new compounds for a target application but still leave the process of identifying the right 'formulation' from the shortlisted chemical space largely a laboratory experiment-driven process. We report a deep learning model, Formulation Graph Convolution Network (F-GCN), that can map structure-composition relationship of the individual components to the property of liquid formulation as whole. Multiple GCNs are assembled in parallel that featurize formulation constituents domain-intuitively on the fly. The resulting molecular descriptors are scaled based on respective constituent's molar percentage in the formulation, followed
    
[^170]: 智能滤波器辅助的领域对抗神经网络：一种用于嘈杂工业场景中故障诊断的无监督领域自适应方法

    Smart filter aided domain adversarial neural network: An unsupervised domain adaptation method for fault diagnosis in noisy industrial scenarios. (arXiv:2307.01429v1 [eess.SP])

    [http://arxiv.org/abs/2307.01429](http://arxiv.org/abs/2307.01429)

    本文提出了一种称为智能滤波器辅助领域对抗神经网络（SFDANN）的无监督领域自适应方法，用于嘈杂的工业场景中的故障诊断。该方法通过引入智能滤波器，在时频域中动态加强源域和目标域数据之间的相似性，以实现领域对齐。

    

    无监督领域自适应（UDA）故障诊断方法在工业环境中显示出显著的功效，有助于在不同的工况、不同的单位之间或模拟数据和真实数据之间转移运行经验和故障特征。然而，在真实的工业场景中，未知的噪声水平和类型会加剧领域对齐的困难，从而严重影响深度学习模型的诊断性能。为了解决这个问题，我们提出了一种称为智能滤波器辅助领域对抗神经网络（SFDANN）的UDA方法，用于嘈杂的工业场景中的故障诊断。所提出的方法包括两个步骤。在第一步中，我们开发了一个智能滤波器，通过在时频域中将源域和目标域数据之间的相似性动态地加强，实现领域对齐。这是通过将可学习的小波包变换网络（LWPT）和传统的小波包变换相结合来实现的。

    The application of unsupervised domain adaptation (UDA)-based fault diagnosis methods has shown significant efficacy in industrial settings, facilitating the transfer of operational experience and fault signatures between different operating conditions, different units of a fleet or between simulated and real data. However, in real industrial scenarios, unknown levels and types of noise can amplify the difficulty of domain alignment, thus severely affecting the diagnostic performance of deep learning models. To address this issue, we propose an UDA method called Smart Filter-Aided Domain Adversarial Neural Network (SFDANN) for fault diagnosis in noisy industrial scenarios. The proposed methodology comprises two steps. In the first step, we develop a smart filter that dynamically enforces similarity between the source and target domain data in the time-frequency domain. This is achieved by combining a learnable wavelet packet transform network (LWPT) and a traditional wavelet packet tra
    
[^171]: ProbVLM: 冻结视觉-语言模型的概率适配器

    ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models. (arXiv:2307.00398v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2307.00398](http://arxiv.org/abs/2307.00398)

    ProbVLM是一种概率适配器，用于估计大规模视觉-语言模型中嵌入的概率分布，以解决固有的嵌入歧义问题，并在多个数据集上展示了其在检索任务中的优越性能表现。

    

    大规模视觉-语言模型（VLM）如CLIP成功地在图像和文本之间找到对应关系。通过标准的确定性映射过程，将图像或文本样本映射到嵌入空间中的一个向量。这是有问题的：由于多个样本（图像或文本）可以抽象出物理世界中的相同概念，确定性嵌入不反映嵌入空间中的固有歧义性。我们提出了ProbVLM，一种概率适配器，通过事后方式在预训练的VLM中通过内部/外部模态对齐估计嵌入的概率分布，而无需大规模数据集或计算。在四个具有挑战性的数据集上，即COCO、Flickr、CUB和Oxford-flowers，我们估计了两个VLM（CLIP和BLIP）的多模态嵌入不确定性，量化了嵌入不确定性在检索任务中的校准，并表明ProbVLM优于其他方法。此外，我们提出了主动学习和模型...

    Large-scale vision-language models (VLMs) like CLIP successfully find correspondences between images and text. Through the standard deterministic mapping process, an image or a text sample is mapped to a single vector in the embedding space. This is problematic: as multiple samples (images or text) can abstract the same concept in the physical world, deterministic embeddings do not reflect the inherent ambiguity in the embedding space. We propose ProbVLM, a probabilistic adapter that estimates probability distributions for the embeddings of pre-trained VLMs via inter/intra-modal alignment in a post-hoc manner without needing large-scale datasets or computing. On four challenging datasets, i.e., COCO, Flickr, CUB, and Oxford-flowers, we estimate the multi-modal embedding uncertainties for two VLMs, i.e., CLIP and BLIP, quantify the calibration of embedding uncertainties in retrieval tasks and show that ProbVLM outperforms other methods. Furthermore, we propose active learning and model 
    
[^172]: 从所有背景信息中学习动态图以准确预测兴趣点的访问量

    Learning Dynamic Graphs from All Contextual Information for Accurate Point-of-Interest Visit Forecasting. (arXiv:2306.15927v1 [cs.LG])

    [http://arxiv.org/abs/2306.15927](http://arxiv.org/abs/2306.15927)

    提出了一种名为忙碌图神经网络（BysGNN）的临时图神经网络，利用所有背景信息和时间序列数据来学习兴趣点之间的多背景相关性，以实现准确的访问量预测。

    

    在城市地区预测兴趣点（POI）的访问量对于规划和决策是至关重要的，涉及领域包括城市规划、交通管理、公共卫生和社会研究。尽管这个问题可以被看作是多元时间序列预测任务，但目前的方法不能充分利用POIs之间不断变化的多背景相关性。因此，我们提出了一种称为忙碌图神经网络（BysGNN）的临时图神经网络，旨在学习和揭示POIs之间的潜在多背景相关性，以准确预测访问量。与其他仅使用时间序列数据来学习动态图的方法不同，BysGNN利用所有背景信息和时间序列数据来学习准确的动态图表示。通过结合所有背景、时间和空间信号，我们观察到预测准确性显著提高，超越了当前最先进的方法。

    Forecasting the number of visits to Points-of-Interest (POI) in an urban area is critical for planning and decision-making for various application domains, from urban planning and transportation management to public health and social studies. Although this forecasting problem can be formulated as a multivariate time-series forecasting task, the current approaches cannot fully exploit the ever-changing multi-context correlations among POIs. Therefore, we propose Busyness Graph Neural Network (BysGNN), a temporal graph neural network designed to learn and uncover the underlying multi-context correlations between POIs for accurate visit forecasting. Unlike other approaches where only time-series data is used to learn a dynamic graph, BysGNN utilizes all contextual information and time-series data to learn an accurate dynamic graph representation. By incorporating all contextual, temporal, and spatial signals, we observe a significant improvement in our forecasting accuracy over state-of-t
    
[^173]: 利用任务结构提高神经网络表示的可识别性

    Leveraging Task Structures for Improved Identifiability in Neural Network Representations. (arXiv:2306.14861v1 [stat.ML])

    [http://arxiv.org/abs/2306.14861](http://arxiv.org/abs/2306.14861)

    本文提出一种基于任务结构的可识别性理论，拓展了先前仅限于单任务分类的工作。任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放。在假设任务之间存在因果关系时，该方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。

    

    本文扩展了监督学习中可辨别性的理论，考虑了在拥有任务分布的情况下的后果。在这种情况下，我们展示了即使在回归的情况下也可以实现可识别性，扩展了先前仅限于单任务分类情况的工作。此外，我们展示了任务分布的存在定义了一个潜在变量的条件先验，将可识别性的等价类降低到排列和缩放，这是一个更强大和更有用的结果。当我们进一步假设这些任务之间存在因果关系时，我们的方法可以实现简单的最大边际似然优化，并在因果表示学习方面具有下游应用的可行性。在经验上，我们验证了我们的模型在恢复合成和现实世界数据的规范表示方面优于更一般的无监督模型。

    This work extends the theory of identifiability in supervised learning by considering the consequences of having access to a distribution of tasks. In such cases, we show that identifiability is achievable even in the case of regression, extending prior work restricted to the single-task classification case. Furthermore, we show that the existence of a task distribution which defines a conditional prior over latent variables reduces the equivalence class for identifiability to permutations and scaling, a much stronger and more useful result. When we further assume a causal structure over these tasks, our approach enables simple maximum marginal likelihood optimization together with downstream applicability to causal representation learning. Empirically, we validate that our model outperforms more general unsupervised models in recovering canonical representations for synthetic and real-world data.
    
[^174]: 提前预测理解前浪潮：研究掌握技能模型的损失函数表面

    Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok. (arXiv:2306.13253v1 [cs.LG])

    [http://arxiv.org/abs/2306.13253](http://arxiv.org/abs/2306.13253)

    本文提出了一种低成本方法来预测神经网络中的理解前浪潮，即通过研究前几轮的学习曲线来判断后续是否出现理解前浪潮。使用波形振荡和学习曲线的频谱特征值可以高精度地预测理解前浪潮。

    

    本文研究了神经网络中出现理解前浪潮的预测，该现象是完美概括在出现过拟合或记忆迹象之后很长一段时间才出现。报告称，只有在特定的超参数下才能观察到理解前浪潮。这使得确定导致理解前浪潮的参数至关重要。然而，由于理解前浪潮需要大量的迭代轮数，因此寻找导致它的超参数是很耗时的。本文提出了一种低成本方法来预测理解前浪潮，而无需训练大量的迭代次数。本文通过研究前几轮的学习曲线，展示了一种可以预测后续出现理解前浪潮的方法。具体而言，如果在前几轮中出现某些振荡，那么可以期望在模型训练更长时间后出现理解前浪潮。我们提出使用学习曲线的频谱特征值来预测理解前浪潮的概率。实验结果表明，我们的方法可以高精度地预测理解前浪潮。

    This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quant
    
[^175]: 虚假黎明：重新评估谷歌强化学习在芯片宏观布局中的应用

    The False Dawn: Reevaluating Google's Reinforcement Learning for Chip Macro Placement. (arXiv:2306.09633v1 [cs.LG])

    [http://arxiv.org/abs/2306.09633](http://arxiv.org/abs/2306.09633)

    谷歌2021年在《自然》杂志上发表的一篇论文声称其使用强化学习在芯片设计领域进行了创新，但两项独立的评估表明，谷歌的方法不如人类设计师、不如一个众所周知的算法（模拟退火），并且也不如普遍可用的商业软件，文章的完整性也遭到了严重的损害。

    

    谷歌2021年在《自然》杂志上发表的有关使用强化学习设计芯片的论文，因为所声称的结果缺乏充分的文件记录和关键步骤的说明，引发争议并受到媒体的批评报道。 而两项独立的评估填补了空白，证明谷歌强化学习落后于人类设计师、落后于一种众所周知的算法（模拟退火），并且还落后于普遍可用的商业软件。交叉检查的数据表明，由于行为、分析和报告中的错误，该《自然》文章的完整性受到了严重的损害。

    Reinforcement learning (RL) for physical design of silicon chips in a Google 2021 Nature paper stirred controversy due to poorly documented claims that raised eyebrows and attracted critical media coverage. The Nature paper withheld most inputs needed to produce reported results and some critical steps in the methodology. But two independent evaluations filled in the gaps and demonstrated that Google RL lags behind human designers, behind a well-known algorithm (Simulated Annealing), and also behind generally-available commercial software. Crosschecked data indicate that the integrity of the Nature paper is substantially undermined owing to errors in the conduct, analysis and reporting.
    
[^176]: 取消七年的算法公平性后处理

    Unprocessing Seven Years of Algorithmic Fairness. (arXiv:2306.07261v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.07261](http://arxiv.org/abs/2306.07261)

    该论文取消了算法公平性中的后处理方法，并发现后处理实现的公平性-准确性Pareto边界包含了可评估的所有其他方法。

    

    七年前，研究人员提出了一种后处理方法，以使模型在不同人口群体中的误差率相等。这项工作启动了数百篇论文，声称能够改进后处理基线。我们通过对几个表格数据集上数千个模型评估的实证评估来评估这些声明。我们发现，后处理实现的公平性-准确性Pareto边界包含我们可以评估的所有其他方法。这样做，我们解决了两个常见的方法论错误，这些错误困扰了以前的观察结果。一个与使用不同的无约束基础模型比较方法有关。另一个涉及实现不同的约束放松水平的方法。我们研究的核心是一种简单的想法，我们称之为取消处理，大致对应于后处理的反演。取消处理允许直接比较使用不同基础模型和放松级别的方法。解读我们的发现。

    Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation. Interpreting our findi
    
[^177]: Ada-NAV：用于机器人导航的自适应轨迹优化策略学习方法

    Ada-NAV: Adaptive Trajectory-Based Sample Efficient Policy Learning for Robotic Navigation. (arXiv:2306.06192v1 [cs.RO])

    [http://arxiv.org/abs/2306.06192](http://arxiv.org/abs/2306.06192)

    Ada-NAV是一种自适应轨迹优化策略学习方法，采用降低策略随机性的方法平衡探索与利用，提高机器人导航任务的采样效率。在真实世界的测试中表现优异，可以在更短的采样时间内取得更高的性能。

    

    强化学习方法在学习机器人导航策略方面十分有效，但其采样效率低的问题也十分明显。在策略优化中，这种效率低下部分来自于未能适当地平衡探索与利用的问题，特别是在面对非静态时。为了加入探索与利用的平衡以提高采样效率，我们提出了Ada-NAV，一种自适应轨迹长度方案，其中长度随着策略的随机性（用其Shannon或差分熵表示）的减小而增加。我们的自适应轨迹长度方案由于更频繁的梯度更新强调了训练开始时的探索，后来则更强调利用。在网格世界，仿真机器人环境和真实世界机器人实验中，我们证明了该方法的优点，表现在性能和采样效率上均优于常数和随机采样的轨迹长度。在固定的样本预算下，相对于现有的基准方法，Ada-NAV的性能提高了高达46％，采样数量减少了高达80％。

    Reinforcement learning methods, while effective for learning robotic navigation strategies, are known to be highly sample inefficient. This sample inefficiency comes in part from not suitably balancing the explore-exploit dilemma, especially in the presence of non-stationarity, during policy optimization. To incorporate a balance of exploration-exploitation for sample efficiency, we propose Ada-NAV, an adaptive trajectory length scheme where the length grows as a policy's randomness, represented by its Shannon or differential entropy, decreases. Our adaptive trajectory length scheme emphasizes exploration at the beginning of training due to more frequent gradient updates and emphasizes exploitation later on with longer trajectories. In gridworld, simulated robotic environments, and real-world robotic experiments, we demonstrate the merits of the approach over constant and randomly sampled trajectory lengths in terms of performance and sample efficiency. For a fixed sample budget, Ada-N
    
[^178]: SiBBlInGS: 使用跨状态的图形相似性驱动模块推理的建模块方法

    SiBBlInGS: Similarity-driven Building-Block Inference using Graphs across States. (arXiv:2306.04817v1 [stat.ML])

    [http://arxiv.org/abs/2306.04817](http://arxiv.org/abs/2306.04817)

    本文提出了一种跨状态的图形相似性驱动的模块推理框架，可以同时考虑数据中的状态间和状态内关系，并允许状态之间的会话计数和持续时间的差异。它可以提取非正交组件，并且能够识别特定状态与状态非特定模块。

    

    对于多维时间序列来说，提取有意义的模块是发现复杂系统中有价值见解的关键。本文提出了一种基于图形相似性驱动的模块推理框架(SiBBlInGS)，用于发现模块，同时考虑到数据中的状态间和状态内关系，能够提取非正交组件，并允许状态之间的会话计数和持续时间差异。此外，SiBBlInGS还允许跨状态变化模块结构和每次试验的时间变异，并可识别特定状态与状态非特定模块。

    Interpretable methods for extracting meaningful building blocks (BBs) underlying multi-dimensional time series are vital for discovering valuable insights in complex systems. Existing techniques, however, encounter limitations that restrict their applicability to real-world systems, like reliance on orthogonality assumptions, inadequate incorporation of inter- and intra-state variability, and incapability to handle sessions of varying duration. Here, we present a framework for Similarity-driven Building Block Inference using Graphs across States (SiBBlInGS). SiBBlInGS employs a graph-based dictionary learning approach for BB discovery, simultaneously considers both inter- and intra-state relationships in the data, can extract non-orthogonal components, and allows for variations in session counts and duration across states. Additionally, SiBBlInGS allows for cross-state variations in BB structure and per-trial temporal variability, can identify state-specific vs state-invariant BBs, and
    
[^179]: 不要相信你的眼睛：关于特征可视化的（不）可靠性。

    Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])

    [http://arxiv.org/abs/2306.04719](http://arxiv.org/abs/2306.04719)

    本文探讨了神经网络如何从像素中提取模式的问题，并研究了特征可视化的可靠性。实验证据表明，由于优化过程中固有的限制，特征可视化能够可靠理解的功能集非常有限，对于解释神经网络如何处理自然图像的解释能力产生怀疑。

    

    神经网络是如何从像素中提取模式的？特征可视化通过优化来可视化高激活的模式，试图回答这个重要问题。如今，可视化方法构成了我们对神经网络内部工作的了解的基础，作为一种机械式的可解释性。在这里，我们问：特征可视化有多可靠？我们通过开发网络电路来诈骗特征可视化，使其显示完全与自然输入的正常网络行为毫无联系的任意模式。然后，我们提供证据表明在标准，未操纵网络中发生了类似的现象：特征可视化与标准输入处理非常不同，对神经网络如何处理自然图像的解释能力产生怀疑。我们通过理论证明支撑这一经验发现，由于优化过程中固有的限制，可以通过特征可视化可靠理解的功能集极其有限。

    How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
    
[^180]: 用贝叶斯推理模拟人类类人概念学习

    Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2306.02797](http://arxiv.org/abs/2306.02797)

    该论文通过在自然语言中进行贝叶斯推理来模拟人类类人概念学习，使用大型语言模型作为提议分布并拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行实验评估。

    

    我们通过在自然语言中进行贝叶斯推理来模拟对抽象符号概念的学习。为了高效推理，我们使用一个大型语言模型作为提议分布。我们根据人类数据拟合先验以更好地模拟人类学习者，并在生成性和逻辑性概念上进行评估。

    We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.
    
[^181]: 通过因果源表示解决强化学习中的非平稳性问题

    Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin Representation. (arXiv:2306.02747v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.02747](http://arxiv.org/abs/2306.02747)

    本论文介绍了一种通过追踪非平稳性的因果起源来解决强化学习中的挑战的方法，通过引入因果源表示（COREP）算法，学到的策略对非平稳性表现出韧性。

    

    在真实世界的场景中，强化学习的应用受到复杂的非平稳性的挑战。大多数现有方法试图明确建模环境中的变化，但往往需要不切实际的先验知识。在本文中，我们提出了一种新的观点，认为非平稳性可以通过状态转换中复杂的因果关系传播和累积，从而增加了其复杂性并影响策略学习。我们相信通过追踪非平稳性的因果起源可以更有效地解决这个挑战。为此，我们引入了因果源表示（COREP）算法。COREP主要采用引导更新机制来学习一种稳定的图形表示，称为因果起源表示，通过利用这种表示，学到的策略对非平稳性表现出令人印象深刻的韧性。我们补充了一个基于因果关系的理论分析。

    In real-world scenarios, the application of reinforcement learning is significantly challenged by complex non-stationarity. Most existing methods attempt to model changes in the environment explicitly, often requiring impractical prior knowledge. In this paper, we propose a new perspective, positing that non-stationarity can propagate and accumulate through complex causal relationships during state transitions, thereby compounding its sophistication and affecting policy learning. We believe that this challenge can be more effectively addressed by tracing the causal origin of non-stationarity. To this end, we introduce the Causal-Origin REPresentation (COREP) algorithm. COREP primarily employs a guided updating mechanism to learn a stable graph representation for states termed as causal-origin representation. By leveraging this representation, the learned policy exhibits impressive resilience to non-stationarity. We supplement our approach with a theoretical analysis grounded in the cau
    
[^182]: 克服连续学习中的稳定性差距

    Overcoming the Stability Gap in Continual Learning. (arXiv:2306.01904v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2306.01904](http://arxiv.org/abs/2306.01904)

    本论文研究了如何克服连续学习中的稳定性差距，并通过发现一种显著减少这种差距的方法，在大规模类别增量学习实验中大幅减少了网络更新的次数。

    

    在许多实际应用中，随着数据集大小的增长，深度神经网络往往需要从头开始重新训练。考虑到重新训练的计算开销，人们认为连续学习可以使网络更新更加高效。实现这一目标的障碍是稳定性差距，即在更新新数据时，先前学习的数据性能会下降，然后才得以恢复。解决这个问题可以减少网络更新的次数，提高计算效率。我们研究了如何缓解稳定性差距，并测试了多种假设以了解其产生原因。这使我们发现了一种显著减少稳定性差距的方法。在大规模的增量类别学习实验中，我们能够显著减少连续学习所需的网络更新次数。我们的工作有可能推动连续学习在实际应用中的最新进展。

    In many real-world applications, deep neural networks are retrained from scratch as a dataset grows in size. Given the computational expense for retraining networks, it has been argued that continual learning could make updating networks more efficient. An obstacle to achieving this goal is the stability gap, which refers to an observation that when updating on new data, performance on previously learned data degrades before recovering. Addressing this problem would enable learning new data with fewer network updates, resulting in increased computational efficiency. We study how to mitigate the stability gap. We test a variety of hypotheses to understand why the stability gap occurs. This leads us to discover a method that vastly reduces this gap. In large-scale class incremental learning experiments, we are able to significantly reduce the number of network updates needed for continual learning. Our work has the potential to advance the state-of-the-art in continual learning for real-
    
[^183]: 通用等变Transformer：用于3D分子相互作用学习

    Generalist Equivariant Transformer Towards 3D Molecular Interaction Learning. (arXiv:2306.01474v1 [cs.LG])

    [http://arxiv.org/abs/2306.01474](http://arxiv.org/abs/2306.01474)

    本文提出了一种通用等变Transformer用于学习3D分子相互作用，该模型具有双层注意力模块、前馈模块和层归一化模块，每个模块都是E（3）等变的，可以有效地捕捉块级和原子级的交互，实验结果表明其在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面优于各种最先进的方法。

    

    生物学和药物开发中的许多过程涉及不同分子之间的各种3D相互作用，例如蛋白质与蛋白质，蛋白质与小分子等。设计一个通用模型来学习普适的分子相互作用具有重要价值，但也具有挑战性，因为不同的分子通常以不同粒度表示。本文首先提出了将3D分子通用表示为集合的几何图形图，与传统单层表示形式形成对比。在提出的统一表示下，我们提出了通用等变Transformer（GET），以有效地捕捉稀疏块级和密集原子级交互。具体而言，GET由双层注意力模块、前馈模块和层归一化模块组成，值得注意的是，每个模块都是E（3）等变的，以满足3D世界的对称性。在预测蛋白质-蛋白质亲和力、配体结合亲和力和配体效力方面进行了大量实验，表明GET优于各种最先进的方法。

    Many processes in biology and drug discovery involve various 3D interactions between different molecules, such as protein and protein, protein and small molecule, etc. Designing a generalist model to learn universal molecular interactions is valuable yet challenging, given that different molecules are usually represented in different granularity. In this paper, we first propose to universally represent a 3D molecule as a geometric graph of sets, in contrast to conventional single-level representations. Upon the proposed unified representation, we then propose a Generalist Equivariant Transformer (GET) to effectively capture both sparse block-level and dense atom-level interactions. To be specific, GET consists of a bilevel attention module, a feed-forward module and a layer normalization module, where, notably, each module is E(3) equivariant to meet the symmetry of 3D world. Extensive experiments on the prediction of protein-protein affinity, ligand binding affinity, and ligand effica
    
[^184]: 深度学习中的一致置信现象及其对校准的影响

    A Uniform Confidence Phenomenon in Deep Learning and its Implications for Calibration. (arXiv:2306.00740v1 [cs.LG])

    [http://arxiv.org/abs/2306.00740](http://arxiv.org/abs/2306.00740)

    深度神经网络在训练点周围有大的几乎确定的置信邻域，这导致现代模型校准面临重要障碍。

    

    尽管深度神经网络具有惊人的泛化能力，但它们屡次表现出在预测不确定性方面估计不佳的情况——换句话说，它们在错误时经常过度自信。解决这个问题被称为模型校准，并以修改训练方案和训练后校准程序的形式受到了广泛关注。在本文中，我们提出了一个现代模型校准的重要障碍：深度神经网络在它们的训练点周围有大的几乎确定的置信邻域。我们在实验中证明了这种现象在很多模型和数据集对中都会出现（在图像分类的背景下）。此外，我们证明了当这种现象出现时，在类别之间存在重叠的大类数据分布中，即使在应用校准后也不能获得比随机更好的渐近校准模型（在渐近意义下）。

    Despite the impressive generalization capabilities of deep neural networks, they have been repeatedly shown to poorly estimate their predictive uncertainty - in other words, they are frequently overconfident when they are wrong. Fixing this issue is known as model calibration, and has consequently received much attention in the form of modified training schemes and post-training calibration procedures. In this work, we present a significant hurdle to the calibration of modern models: deep neural networks have large neighborhoods of almost certain confidence around their training points. We demonstrate in our experiments that this phenomenon consistently arises (in the context of image classification) across many model and dataset pairs. Furthermore, we prove that when this phenomenon holds, for a large class of data distributions with overlaps between classes, it is not possible to obtain a model that is asymptotically better than random (with respect to calibration) even after applyin
    
[^185]: 结构化大离散动作空间的动态邻域构建

    Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19891](http://arxiv.org/abs/2305.19891)

    本研究针对无法处理的结构化大离散动作空间（SLDAS）提出了一种名为动态邻域构建（DNC）的新型利用策略，通过可扩展的邻域探索启发式方法，高效地探索连续代理动作周围的离散邻域。

    

    大离散动作空间（LDAS）是强化学习中的一个核心挑战。现有的解决方案可以处理多达几百万个动作的非结构化LDAS。然而，在物流、生产和运输系统等许多现实应用中，动作空间具有组合结构，其规模甚至在小规模实例上也超过了数百万个动作。幸运的是，这样的动作空间呈现出一定的结构，例如等间距的离散资源单位。在这项工作中，我们专注于处理当前基准测试无法处理的结构化LDAS（SLDAS）。我们提出了一种名为动态邻域构建（DNC）的新型利用策略，用于SLDAS。我们提出了一种可扩展的邻域探索启发式方法，利用这种策略，在具有高达$10^{73}$个动作的结构化动作空间中高效地探索连续代理动作周围的离散邻域。我们通过与三个标杆算法进行基准测试来展示我们方法的性能。

    Large discrete action spaces (LDAS) remain a central challenge in reinforcement learning. Existing solution approaches can handle unstructured LDAS with up to a few million actions. However, many real-world applications in logistics, production, and transportation systems have combinatorial action spaces, whose size grows well beyond millions of actions, even on small instances. Fortunately, such action spaces exhibit structure, e.g., equally spaced discrete resource units. With this work, we focus on handling structured LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm for SLDAS. We present a scalable neighborhood exploration heuristic that utilizes this paradigm and efficiently explores the discrete neighborhood around the continuous proxy action in structured action spaces with up to $10^{73}$ actions. We demonstrate the performance of our method by benchmarking it against three sta
    
[^186]: 对数凹马尔可夫链之链

    Chain of Log-Concave Markov Chains. (arXiv:2305.19473v1 [stat.ML])

    [http://arxiv.org/abs/2305.19473](http://arxiv.org/abs/2305.19473)

    该论文提出了一种新的采样算法，基于对数凹条件概率密度，使用等向性高斯平滑来解决高维下抽样难题。

    

    马尔科夫链蒙特卡罗（MCMC）是一种从未标准化密度中抽样的通用算法类。在高维情况下，MCMC面临两个众所周知的问题：(i)感兴趣的分布在由小概率块隔开的区域中集中;(ii)对数凹性的小概率块本身通常存在病态问题。我们引入了一种采用等向性高斯平滑来解决这些问题的框架。我们证明，无论密度函数的最小假设是什么，从密度函数中采样总是可以分解为通过等噪声测量的累积，从对数凹性条件密度中采样的序列。该构造跟踪了样本历史，因此作为一个整体而言是非马尔可夫的，但历史仅以经验均值的形式出现，从而保证了内存印迹的最小化。我们的采样算法推广了步行跳跃采样（1）。"走"阶段变成了对数凹链的(非马尔可夫)链。

    Markov chain Monte Carlo (MCMC) is a class of general-purpose algorithms for sampling from unnormalized densities. There are two well-known problems facing MCMC in high dimensions: (i) The distributions of interest are concentrated in pockets separated by large regions with small probability mass, and (ii) The log-concave pockets themselves are typically ill-conditioned. We introduce a framework to tackle these problems using isotropic Gaussian smoothing. We prove one can always decompose sampling from a density (minimal assumptions made on the density) into a sequence of sampling from log-concave conditional densities via accumulation of noisy measurements with equal noise levels. This construction keeps track of a history of samples, making it non-Markovian as a whole, but the history only shows up in the form of an empirical mean, making the memory footprint minimal. Our sampling algorithm generalizes walk-jump sampling [1]. The "walk" phase becomes a (non-Markovian) chain of log-co
    
[^187]: 神经PDE代理的快速动态1D勃然模拟

    Fast Dynamic 1D Simulation of Divertor Plasmas with Neural PDE Surrogates. (arXiv:2305.18944v2 [physics.plasm-ph] UPDATED)

    [http://arxiv.org/abs/2305.18944](http://arxiv.org/abs/2305.18944)

    该论文提出了一种使用神经网络的代理模型来进行快速铁托等离子体模拟的方法，解决了实时应用或详尽的参数扫描中速度太慢的问题。

    

    管理铁托式核聚变设备中的铁托等离子体对于应对其热量和粒子通量限制至关重要。模拟是理解和控制这些等离子体的重要工具，然而，对于实时应用或详尽的参数扫描，目前只有简单的近似方法速度足够快。我们使用神经PDE代理来解决这种快速模拟器的缺乏，即使用基于数据驱动的神经网络的代理模型，该模型使用使用经典数值方法生成的解进行训练。该代理模型近似演化参考基于物理模型的完整空间解决方案的时间步进算子。我们使用DIV1D作为参考模型来生成数据，即从X点（上游）到目标处的1D热流管的DIV1D。我们模拟了一个真实的TCV铁托等离子体，其动态是由上游密度坎口引起的，并提供了快速暂态的探索性展望。

    Managing divertor plasmas is crucial for operating reactor scale tokamak devices due to heat and particle flux constraints on the divertor target. Simulation is an important tool to understand and control these plasmas, however, for real-time applications or exhaustive parameter scans only simple approximations are currently fast enough. We address this lack of fast simulators using neural PDE surrogates, data-driven neural network-based surrogate models trained using solutions generated with a classical numerical method. The surrogate approximates a time-stepping operator that evolves the full spatial solution of a reference physics-based model over time. We use DIV1D, a 1D dynamic model of the divertor plasma, as reference model to generate data. DIV1D's domain covers a 1D heat flux tube from the X-point (upstream) to the target. We simulate a realistic TCV divertor plasma with dynamics induced by upstream density ramps and provide an exploratory outlook towards fast transients. Stat
    
[^188]: 基于内核KL散度的自监督学习方法Kernel-SSL

    Kernel-SSL: Kernel KL Divergence for Self-supervised Learning. (arXiv:2305.17326v1 [cs.LG])

    [http://arxiv.org/abs/2305.17326](http://arxiv.org/abs/2305.17326)

    本文提出了一种名为Kernel-SSL的自监督学习方法，将多种现有非对比学习方法建立在了再生核希尔伯特空间（RKHS）理解之上并优化了其中的均值嵌入和协方差算子，实验结果显示，在ImageNet数据集下表现显著超越最先进的方法，提高了4.6%。

    

    对比学习通常将一个正锚点样本与许多负样本进行比较，来完成自监督学习（SSL）。相反，非对比学习，例如BYOL、SimSiam和Barlow Twins等方法，在没有显式使用负样本的情况下完成SSL。受对比学习现有分析的启发，我们提供了多种现有非对比学习方法的再生核希尔伯特空间（RKHS）理解。随后，我们提出了一种新的损失函数Kernel-SSL，直接优化RKHS中的均值嵌入和协方差算子。实验中，我们的方法Kernel-SSL在线性评估设置下在ImageNet数据集上大幅优于最先进的方法。具体来说，在进行100个epoch的预训练时，我们的方法比SimCLR表现提高了4.6%。

    Contrastive learning usually compares one positive anchor sample with lots of negative samples to perform Self-Supervised Learning (SSL). Alternatively, non-contrastive learning, as exemplified by methods like BYOL, SimSiam, and Barlow Twins, accomplishes SSL without the explicit use of negative samples. Inspired by the existing analysis for contrastive learning, we provide a reproducing kernel Hilbert space (RKHS) understanding of many existing non-contrastive learning methods. Subsequently, we propose a novel loss function, Kernel-SSL, which directly optimizes the mean embedding and the covariance operator within the RKHS. In experiments, our method Kernel-SSL outperforms state-of-the-art methods by a large margin on ImageNet datasets under the linear evaluation settings. Specifically, when performing 100 epochs pre-training, our method outperforms SimCLR by 4.6%.
    
[^189]: 梯度下降在多层神经网络中的泛化保证

    Generalization Guarantees of Gradient Descent for Multi-Layer Neural Networks. (arXiv:2305.16891v1 [cs.LG])

    [http://arxiv.org/abs/2305.16891](http://arxiv.org/abs/2305.16891)

    本论文通过全面的稳定性和泛化分析，在多层神经网络上证明了GD算法的一般性保证，为双层和三层NN推导出了过量风险率，扩展了以往研究。

    

    近年来，通过算法稳定性方法，对梯度下降训练的神经网络（NN）的泛化进行了重大进展。然而，大部分现有研究集中在单隐藏层NN上，并没有解决不同网络缩放参数的影响。本文通过对GD在多层NN上进行全面的稳定性和泛化分析，极大地扩展了以往的工作。对于双层NN，我们的结果是在一般的网络缩放参数下建立的，放宽了以前的条件。对于三层NN，我们的技术贡献在于利用一种新的归纳策略，深入探讨了过度参数化的影响，证明了它的几乎协同约束性。通过我们的一般性发现的直接应用，我们得出了GD算法在双层和三层NN中的过量风险速率为$O(1/\sqrt{n})$。

    Recently, significant progress has been made in understanding the generalization of neural networks (NNs) trained by gradient descent (GD) using the algorithmic stability approach. However, most of the existing research has focused on one-hidden-layer NNs and has not addressed the impact of different network scaling parameters. In this paper, we greatly extend the previous work \cite{lei2022stability,richards2021stability} by conducting a comprehensive stability and generalization analysis of GD for multi-layer NNs. For two-layer NNs, our results are established under general network scaling parameters, relaxing previous conditions. In the case of three-layer NNs, our technical contribution lies in demonstrating its nearly co-coercive property by utilizing a novel induction strategy that thoroughly explores the effects of over-parameterization. As a direct application of our general findings, we derive the excess risk rate of $O(1/\sqrt{n})$ for GD algorithms in both two-layer and thre
    
[^190]: 神经控制微分方程的泛化能力研究

    On the Generalization Capacities of Neural Controlled Differential Equations. (arXiv:2305.16791v1 [stat.ML])

    [http://arxiv.org/abs/2305.16791](http://arxiv.org/abs/2305.16791)

    本文研究了使用神经控制微分方程进行监督学习的泛化能力问题，通过量化离散化偏差和利普希茨函数逼近误差，得到了经验风险最小化器与贝叶斯最优风险的泛化差距上界。

    

    本文研究了使用神经控制微分方程（Kidger，Morrill等，2020）从不规则采样的时间序列样本中预测结果的监督学习设置。在我们的框架中，时间序列是一个未观察到的连续路径的离散化，结果通过一个具有未知向量场的控制微分方程依赖于这个路径。使用离散数据进行学习会引入离散偏差，我们精确地量化了这种偏差。通过使用关于控制微分方程流的连续性的理论结果，我们展示了逼近偏差直接与由浅层神经网络定义生成模型的利普希茨函数的逼近误差相关。通过结合最近的工作将神经网络的利普希茨常数与其泛化能力联系起来，我们上界了经验风险最小化器达到的期望损失与贝叶斯最优风险之间的泛化差距。

    We consider a supervised learning setup in which the goal is to predicts an outcome from a sample of irregularly sampled time series using Neural Controlled Differential Equations (Kidger, Morrill, et al. 2020). In our framework, the time series is a discretization of an unobserved continuous path, and the outcome depends on this path through a controlled differential equation with unknown vector field. Learning with discrete data thus induces a discretization bias, which we precisely quantify. Using theoretical results on the continuity of the flow of controlled differential equations, we show that the approximation bias is directly related to the approximation error of a Lipschitz function defining the generative model by a shallow neural network. By combining these result with recent work linking the Lipschitz constant of neural networks to their generalization capacities, we upper bound the generalization gap between the expected loss attained by the empirical risk minimizer and th
    
[^191]: $L_{2}$正则线性深度神经网络中隐性SGD偏差：从高秩到低秩的单向跳跃。

    Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank. (arXiv:2305.16038v1 [cs.LG])

    [http://arxiv.org/abs/2305.16038](http://arxiv.org/abs/2305.16038)

    在$L_{2}$-正则化线性深度神经网络中，使用SGD会产生从更高秩最小值到更低秩最小值的单向跳跃，并且不会跳回。

    

    具有多个隐藏层的深度线性网络（DLN）的$L_{2}$正则化损失具有多个局部最小值，对应于具有不同秩的矩阵。在矩阵完成等任务中，目标是收敛到最小秩局部最小值，该局部最小值仍适合训练数据。虽然可以轻松避免低估秩的局部最小值，因为它们不适合数据，但梯度下降可能会陷入高估秩的局部最小值。我们证明，使用SGD，总是有从更高秩最小值跳跃到更低秩最小值的概率，但跳回的概率为零。更精确地说，我们定义了一系列集合$B_{1}\subset B_{2}\subset\cdots\subset B_{R}$，使得$B_{r}$包含秩$r$或更少的所有最小值（而不是更多），对于足够小的岭参数$\lambda$和学习率$\eta$，它们是吸收的：SGD离开$B_{r}$的概率为0，从任何起点开始，SGD进入$B_{r}$的概率非零。

    The $L_{2}$-regularized loss of Deep Linear Networks (DLNs) with more than one hidden layers has multiple local minima, corresponding to matrices with different ranks. In tasks such as matrix completion, the goal is to converge to the local minimum with the smallest rank that still fits the training data. While rank-underestimating minima can easily be avoided since they do not fit the data, gradient descent might get stuck at rank-overestimating minima. We show that with SGD, there is always a probability to jump from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. More precisely, we define a sequence of sets $B_{1}\subset B_{2}\subset\cdots\subset B_{R}$ so that $B_{r}$ contains all minima of rank $r$ or less (and not more) that are absorbing for small enough ridge parameters $\lambda$ and learning rates $\eta$: SGD has prob. 0 of leaving $B_{r}$, and from any starting point there is a non-zero prob. for SGD to go in $B_{r}$.
    
[^192]: 用最优传输学习有向图模型

    Learning Directed Graphical Models with Optimal Transport. (arXiv:2305.15927v1 [cs.LG])

    [http://arxiv.org/abs/2305.15927](http://arxiv.org/abs/2305.15927)

    通过最优传输的视角提供了参数学习问题的新视图，可以在许多有向图上进行操作并表现出灵活性和多功能性。

    

    从不完整的数据中估计概率有向图模型的参数仍然是一个长期存在的挑战。这是因为在存在潜在变量的情况下，如果没有关于结构依赖性或模型类的进一步假设，似然函数和后验分布都是不可计算的。虽然现有的学习方法基本上是基于最大似然估计，但我们在这里通过最优传输的视角提供了参数学习问题的一个新视图。这个观点授权了一个框架，可以在许多有向图上运作，而不会对潜在变量的后验做出不切实际的假设或诉诸于黑箱变分近似。我们开发了一个理论框架，并支持它通过广泛的经验证据，展示了我们方法的灵活性和多功能性。通过实验，我们展示了我们的方法不仅可以恢复基准参数，而且在性能方面也表现得有竞争力。

    Estimating the parameters of a probabilistic directed graphical model from incomplete data remains a long-standing challenge. This is because, in the presence of latent variables, both the likelihood function and posterior distribution are intractable without further assumptions about structural dependencies or model classes. While existing learning methods are fundamentally based on likelihood maximization, here we offer a new view of the parameter learning problem through the lens of optimal transport. This perspective licenses a framework that operates on many directed graphs without making unrealistic assumptions on the posterior over the latent variables or resorting to black-box variational approximations. We develop a theoretical framework and support it with extensive empirical evidence demonstrating the flexibility and versatility of our approach. Across experiments, we show that not only can our method recover the ground-truth parameters but it also performs competitively on 
    
[^193]: 改进ReLU网络特征学习的神经特征激活值分析

    Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning. (arXiv:2305.15912v1 [cs.LG])

    [http://arxiv.org/abs/2305.15912](http://arxiv.org/abs/2305.15912)

    本文提出了一种利用ReLU单元特征激活值集合进行参数化的几何方法，通过利用现代深度学习架构中的规范化技术，改进了ReLU网络特征学习，提高了优化稳定性和收敛速度，并获得更好的泛化性能。

    

    本文研究了神经网络中单个ReLU单元的特征激活值。我们将ReLU单元在输入空间中对应的特征激活值集合称为ReLU单元的特征激活集。我们建立了特征激活集与ReLU网络中学习特征之间的明确联系，并揭示了现代深度学习架构中使用的各种神经网络规范化技术如何规范化和稳定SGD优化。利用这些洞见，我们提出了一种几何方法来参数化ReLU网络以改进特征学习。我们经验性地验证了其有用性，使用了不那么精心选择的初始化方案和更大的学习率。我们报告了更好的优化稳定性，更快的收敛速度和更好的泛化性能。

    We examine the characteristic activation values of individual ReLU units in neural networks. We refer to the corresponding set for such characteristic activation values in the input space as the characteristic activation set of a ReLU unit. We draw an explicit connection between the characteristic activation set and learned features in ReLU networks. This connection leads to new insights into why various neural network normalization techniques used in modern deep learning architectures regularize and stabilize SGD optimization. Utilizing these insights, we propose a geometric approach to parameterize ReLU networks for improved feature learning. We empirically verify its usefulness with less carefully chosen initialization schemes and larger learning rates. We report improved optimization stability, faster convergence speed, and better generalization performance.
    
[^194]: 深度等变超球体

    Deep Equivariant Hyperspheres. (arXiv:2305.15613v1 [cs.LG])

    [http://arxiv.org/abs/2305.15613](http://arxiv.org/abs/2305.15613)

    本文提出了深度等变超球体的理论模型，解决了几何深度学习中等变和几何变换下不变的重大问题。

    

    本文提出了一种学习nD特征的方法，其在点云分析中等变于正交转换，利用了超球体和常规n单形体。我们的主要贡献在于理论方面，解决了几何深度学习中等变和几何变换下不变的重大问题。我们扩展了近期发展的可操纵3D球形神经元理论--基于球形决策面的SO（3）-等变滤波器组，将该神经元扩展到了nD，我们称之为深度等变超球体，并使它们能够堆叠在多层中。利用ModelNet40基准测试，我们实验验证了我们的理论贡献，并展示了所提出的等变超球体的潜在实用配置。

    This paper presents an approach to learning nD features equivariant under orthogonal transformations for point cloud analysis, utilizing hyperspheres and regular n-simplexes. Our main contributions are theoretical and tackle major issues in geometric deep learning such as equivariance and invariance under geometric transformations. Namely, we enrich the recently developed theory of steerable 3D spherical neurons -- SO(3)-equivariant filter banks based on neurons with spherical decision surfaces -- by extending said neurons to nD, which we call deep equivariant hyperspheres, and enabling their stacking in multiple layers. Using the ModelNet40 benchmark, we experimentally verify our theoretical contributions and show a potential practical configuration of the proposed equivariant hyperspheres.
    
[^195]: 基于Sinkhorn距离的特征对其N-BEATS

    Feature-aligned N-BEATS with Sinkhorn divergence. (arXiv:2305.15196v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.15196](http://arxiv.org/abs/2305.15196)

    这是一个基于Sinkhorn距离的特征对其N-BEATS模型，它通过对齐堆栈中的边际特征概率测度来进行领域广义的时间序列预测，同时保留了N-BEATS的可解释性和预测能力。

    

    我们提出了基于Sinkhorn距离的特征对其N-BEATS作为一个领域广义时间序列预测模型。它是N-BEATS的非平凡扩展，采用了双重残差叠加原则（Oreshkin等人[42]）并将其转化为一个表示学习框架。具体而言，它围绕着由N-BEATS每个堆栈的残差和特征提取算子的复杂组合产生的边际特征概率测度，并通过一种近似最优传输距离（Sinkhorn距离）将它们堆叠地对齐。训练损失由来自多个源域的经验风险最小化（即预测损失）和Sinkhorn距离计算的对齐损失组成，使得模型能够在多个源数据序列中堆叠地学习不变特征，同时保留N-BEATS的可解释设计和预测能力。我们提供了全面的实验评估和消融研究，并展示了相应的结果。

    We propose Feature-aligned N-BEATS as a domain-generalized time series forecasting model. It is a nontrivial extension of N-BEATS with doubly residual stacking principle (Oreshkin et al.[42]) into a representation learning framework. In particular, it revolves around marginal feature probability measures induced by the intricate composition of residual and feature extracting operators of N-BEATS in each stack and aligns them stack-wisely via an approximate of an optimal transport distance referred to as the Sinkhorn divergence. The training loss consists of an empirical risk minimization from multiple source domains, i.e., forecasting loss, and an alignment loss calculated with the Sinkhorn divergence, which allows the model to learn invariant features stack-wisely across multiple source data sequences while retaining N-BEATS's interpretable design and forecasting power. Comprehensive experimental evaluations with ablation studies are provided and the corresponding results demonstrate 
    
[^196]: 基于自监督高斯正则化的深度分类器马氏距离不确定性评估

    Self-Supervised Gaussian Regularization of Deep Classifiers for Mahalanobis-Distance-Based Uncertainty Estimation. (arXiv:2305.13849v1 [cs.CV])

    [http://arxiv.org/abs/2305.13849](http://arxiv.org/abs/2305.13849)

    本文提出了一种自监督高斯正则化的深度分类器，可用于马氏距离不确定性评估，相比现有方法，该方法不需要对模型架构和训练程序做出大的改变，并在标准OOD基准测试上取得了最先进的性能。

    

    近期的工作表明，网络的潜在空间中的数据分布对于估计分类不确定性和检测超出分布范围（OOD）的样本非常有用。为了获得适用于不确定性估计的良好正则化潜在空间，现有方法对模型架构和训练程序进行了重大改变。在本文中，我们提出了一种用于马氏距离基础不确定性预测的轻量级、快速、高性能正则化方法，并且对网络架构的改动要求最小。为了得到适用于马氏距离计算的高斯潜在表示，我们引入了一种自监督表示学习方法，将类内表示分为多个高斯。具有非高斯表示的类别被自动识别并动态聚类为多个大概率是高斯分布的类别。在标准OOD基准测试上的评估显示出，我们的方法实现了最先进的性能，同时保持轻量级和高效的模型架构。

    Recent works show that the data distribution in a network's latent space is useful for estimating classification uncertainty and detecting Out-of-distribution (OOD) samples. To obtain a well-regularized latent space that is conducive for uncertainty estimation, existing methods bring in significant changes to model architectures and training procedures. In this paper, we present a lightweight, fast, and high-performance regularization method for Mahalanobis distance-based uncertainty prediction, and that requires minimal changes to the network's architecture. To derive Gaussian latent representation favourable for Mahalanobis Distance calculation, we introduce a self-supervised representation learning method that separates in-class representations into multiple Gaussians. Classes with non-Gaussian representations are automatically identified and dynamically clustered into multiple new classes that are approximately Gaussian. Evaluation on standard OOD benchmarks shows that our method a
    
[^197]: 实现离线强化学习的极小极大样本复杂性：一种基于DRO的方法。

    Achieving Minimax Optimal Sample Complexity of Offline Reinforcement Learning: A DRO-Based Approach. (arXiv:2305.13289v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.13289](http://arxiv.org/abs/2305.13289)

    本文提出了一种分布鲁棒优化 (DRO)的方法，用于解决离线强化学习中的数据有限性和分布转移问题。通过直接建模转移核的不确定性，并寻找在不确定性集合中最优化最坏情况下的性能的策略，实现了极小极大最优性。

    

    离线强化学习旨在从先前收集的数据集中学习，而无需主动探索。该问题面临着诸多挑战，包括有限的数据可用性和分布转移。现有方法采用一种悲观的态度对待不确定性，通过惩罚未充分探索的状态-行为对的奖励来保守估计值函数。在本文中，我们展示了分布鲁棒优化（DRO）基于方法也可以应对这些挑战，并且是极小极大最优的。具体而言，我们直接建模转移核的不确定性，并构建一个统计合理的转移核不确定性集合。然后我们寻找在该不确定性集合上最优化最坏情况下的性能的策略。我们首先设计了一种基于度量的霍夫丁风格不确定性集合，这样真实的转移核以高概率位于该集合中。我们证明了为了实现$\epsilon$的次最优性差距，样本复杂度为$\mat。

    Offline reinforcement learning aims to learn from pre-collected datasets without active exploration. This problem faces significant challenges, including limited data availability and distributional shifts. Existing approaches adopt a pessimistic stance towards uncertainty by penalizing rewards of under-explored state-action pairs to estimate value functions conservatively. In this paper, we show that the distributionally robust optimization (DRO) based approach can also address these challenges and is minimax optimal. Specifically, we directly model the uncertainty in the transition kernel and construct an uncertainty set of statistically plausible transition kernels. We then find the policy that optimizes the worst-case performance over this uncertainty set. We first design a metric-based Hoeffding-style uncertainty set such that with high probability the true transition kernel is in this set. We prove that to achieve a sub-optimality gap of $\epsilon$, the sample complexity is $\mat
    
[^198]: 不精确标签学习：学习各种不精确标签配置的统一框架

    Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations. (arXiv:2305.12715v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.12715](http://arxiv.org/abs/2305.12715)

    本文提出了不精确标签学习（ILL）框架，利用期望最大化算法对不精确标签信息进行最大似然估计，为各种不精确标签配置问题提供了统一的解决方案。

    

    本文介绍了不精确标签学习（ILL）框架，这是一种处理机器学习任务中普遍存在的各种不精确标签配置的统一方法。ILL利用期望最大化（EM）算法对不精确标签信息进行最大似然估计（MLE），将精确标签视为潜在变量。与以前试图从不精确标签信息中推断正确标签的多功能方法相比，我们的ILL框架考虑了不精确标签信息强加的所有可能标签，允许对任何不精确标签的统一解决方案。通过全面的实验结果，我们展示了ILL可以无缝地适应各种情况，包括部分标签学习、半监督学习、噪声标签学习以及这些配置的混合。值得注意的是，我们的简单方法超过了现有的处理不精确标签的技术，标志着第一个统一解决这个问题的方法。

    In this paper, we introduce the imprecise label learning (ILL) framework, a unified approach to handle various imprecise label configurations, which are commonplace challenges in machine learning tasks. ILL leverages an expectation-maximization (EM) algorithm for the maximum likelihood estimation (MLE) of the imprecise label information, treating the precise labels as latent variables. Compared to previous versatile methods attempting to infer correct labels from the imprecise label information, our ILL framework considers all possible labeling imposed by the imprecise label information, allowing a unified solution to deal with any imprecise labels. With comprehensive experimental results, we demonstrate that ILL can seamlessly adapt to various situations, including partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. Notably, our simple method surpasses the existing techniques for handling imprecise labels, marking the first unified 
    
[^199]: 可解释的神经架构搜索与迁移学习用于理解序列依赖的酶反应

    Interpretable neural architecture search and transfer learning for understanding sequence dependent enzymatic reactions. (arXiv:2305.11917v1 [q-bio.MN])

    [http://arxiv.org/abs/2305.11917](http://arxiv.org/abs/2305.11917)

    Elektrum是一个深度学习框架，使用可解释的神经网络模型预测酶反应，利用有限但洁净的体外数据和噪声但丰富的体内数据。Elektrum可以通过迁移学习揭示酶活性的关键序列相关决定因素，并发现潜在的治疗干预靶点。

    

    精细调节的酶途径控制着细胞过程，它们的失调可能导致疾病。为这些途径创建预测性和可解释性模型具有挑战性，因为这些途径的复杂性以及细胞和基因组背景的复杂性。在这里，我们介绍了Elektrum，一个深度学习框架，通过数据驱动和生物物理解释模型，确定生化系统动力学，从而解决这些挑战。首先，它使用体外动力学测定快速假设高质量的可解释动力学神经网络（KINN），用于预测反应速率。然后，利用新颖的迁移学习步骤，将KINN作为中介层插入更深的卷积神经网络中，微调反应相关的体内结果的预测。Elektrum有效利用了有限但洁净的体外数据和捕获细胞背景的噪声但丰富的体内数据。我们将Elektrum应用于理解与非酒精性脂肪肝病相关的碳水化合物和脂质代谢中涉及的酶反应。我们证明Elektrum利用迁移学习（1）优于最先进的模型预测体内反应速率; (2) 揭示酶活性的关键序列相关决定因素; 以及（3）发现治疗干预的潜在靶点。

    Finely-tuned enzymatic pathways control cellular processes, and their dysregulation can lead to disease. Creating predictive and interpretable models for these pathways is challenging because of the complexity of the pathways and of the cellular and genomic contexts. Here we introduce Elektrum, a deep learning framework which addresses these challenges with data-driven and biophysically interpretable models for determining the kinetics of biochemical systems. First, it uses in vitro kinetic assays to rapidly hypothesize an ensemble of high-quality Kinetically Interpretable Neural Networks (KINNs) that predict reaction rates. It then employs a novel transfer learning step, where the KINNs are inserted as intermediary layers into deeper convolutional neural networks, fine-tuning the predictions for reaction-dependent in vivo outcomes. Elektrum makes effective use of the limited, but clean in vitro data and the noisy, yet plentiful in vivo data that captures cellular context. We apply Ele
    
[^200]: 多样化深度集成：一种使用显著性图的方法以增强OOD检测、校准和准确性

    Diversifying Deep Ensembles: A Saliency Map Approach for Enhanced OOD Detection, Calibration, and Accuracy. (arXiv:2305.11616v1 [cs.CV])

    [http://arxiv.org/abs/2305.11616](http://arxiv.org/abs/2305.11616)

    这项研究提出了一种使用显著性图来促进深度集成多样性的方法，用于改善OOD检测、校准和准确性，能够优于传统的集成技术，并在OpenOOD基准测试上证明了其有效性。

    

    深度集成在分类和 OOD 检测方面取得了最先进的成果；然而，由于集成中学习的模式的同质性，它们的效果仍然有限。为了克服这一挑战，本研究引入了一种促进集成成员之间多样性的新方法，该方法利用显著性图。通过整合显著性图多样化，我们的方法在多个分类和OOD检测任务中优于传统的集成技术，同时也提高了校准性。在已建立的OpenOOD基准测试上的实验凸显了我们的方法在实际应用中的潜力。

    Deep ensembles achieved state-of-the-art results in classification and out-of-distribution (OOD) detection; however, their effectiveness remains limited due to the homogeneity of learned patterns within the ensemble. To overcome this challenge, our study introduces a novel approach that promotes diversity among ensemble members by leveraging saliency maps. By incorporating saliency map diversification, our method outperforms conventional ensemble techniques in multiple classification and OOD detection tasks, while also improving calibration. Experiments on well-established OpenOOD benchmarks highlight the potential of our method in practical applications.
    
[^201]: GETMusic：使用统一的表示和扩散框架生成任意音乐曲目

    GETMusic: Generating Any Music Tracks with a Unified Representation and Diffusion Framework. (arXiv:2305.10841v1 [cs.SD])

    [http://arxiv.org/abs/2305.10841](http://arxiv.org/abs/2305.10841)

    GETMusic提出了一种统一的音乐生成模型，包括新颖的音乐表示GETScore和扩散模型GETDiff。GETScore使用标记表示音符，将它们有序地组织起来，而GETDiff使用遮盖对目标轨道进行破坏，能够生成任意轨道。

    

    符号音乐生成旨在创建音符，为用户创作音乐提供帮助，例如从零开始生成目标乐器轨道或基于用户提供的源轨道进行创作。考虑到源轨道和目标轨道之间的多样化和灵活性组合，需要一个能够生成任意轨道的统一模型至关重要。以往的作品由于音乐表示和模型架构的固有限制而未能解决这个需求。为了解决这个问题，我们提出了一个名为GETMusic（`GET'代表GEnerate music Tracks）的统一表示和扩散框架，其中包括一种新颖的音乐表示GETScore和一个名为GETDiff的扩散型模型。GETScore将音符表示为标记，并在二维结构中井然有序地组织它们，轨道垂直堆叠并水平地随时间推进。在训练过程中，轨道随机被选为目标或源。在前向过程中，使用遮盖对目标轨道进行破坏。

    Symbolic music generation aims to create musical notes, which can help users compose music, such as generating target instrumental tracks from scratch, or based on user-provided source tracks. Considering the diverse and flexible combination between source and target tracks, a unified model capable of generating any arbitrary tracks is of crucial necessity. Previous works fail to address this need due to inherent constraints in music representations and model architectures. To address this need, we propose a unified representation and diffusion framework named GETMusic (`GET' stands for GEnerate music Tracks), which includes a novel music representation named GETScore, and a diffusion model named GETDiff. GETScore represents notes as tokens and organizes them in a 2D structure, with tracks stacked vertically and progressing horizontally over time. During training, tracks are randomly selected as either the target or source. In the forward process, target tracks are corrupted by masking
    
[^202]: ZeroFlow: 通过蒸馏实现快速零标签场景流

    ZeroFlow: Fast Zero Label Scene Flow via Distillation. (arXiv:2305.10424v1 [cs.CV])

    [http://arxiv.org/abs/2305.10424](http://arxiv.org/abs/2305.10424)

    ZeroFlow是一种简单的蒸馏算法，使用无标签方法生成伪标签以监督前向传递模型，实现了在使用零人工标签情况下对大规模点云进行实时场景流估计。

    

    场景流估计是描述连续点云之间的三维运动场的任务。最先进的方法使用强大的先验知识和测试时优化技术，但对于大规模点云需要数十秒的时间，使其无法作为实时应用程序（如开放世界目标检测）的计算机视觉基元使用。前向传递方法相对快速，对于大规模点云的运行时间在数十至数百毫秒之间，但需要昂贵的人力监督。为了解决这两个限制，我们提出了一种简单的蒸馏框架 Scene Flow via Distillation，使用无标签优化方法来生成伪标签以监督前向传递模型。我们实现了这个框架中的 ZeroFlow，使用零人工标签，在大规模点云上实时生成场景流估计结果，同时质量竞争状态下的最先进方法。值得注意的是，在测试时 ZeroFlow

    Scene flow estimation is the task of describing the 3D motion field between temporally successive point clouds. State-of-the-art methods use strong priors and test-time optimization techniques, but require on the order of tens of seconds for large-scale point clouds, making them unusable as computer vision primitives for real-time applications such as open world object detection. Feed forward methods are considerably faster, running on the order of tens to hundreds of milliseconds for large-scale point clouds, but require expensive human supervision. To address both limitations, we propose Scene Flow via Distillation, a simple distillation framework that uses a label-free optimization method to produce pseudo-labels to supervise a feed forward model. Our instantiation of this framework, ZeroFlow, produces scene flow estimates in real-time on large-scale point clouds at quality competitive with state-of-the-art methods while using zero human labels. Notably, at test-time ZeroFlow is ove
    
[^203]: 关于transformer主动学习中数据集可迁移性的研究

    On Dataset Transferability in Active Learning for Transformers. (arXiv:2305.09807v1 [cs.LG])

    [http://arxiv.org/abs/2305.09807](http://arxiv.org/abs/2305.09807)

    本文研究了基于transformer的预训练语言模型的主动学习中数据集的可迁移性问题，发现具有相似获取序列的主动学习方法产生的数据集在不同模型之间具有高度的可迁移性。

    

    主动学习旨在通过查询对模型学习最有益的示例来减少标注成本。尽管已经证明了对于微调基于transformer的预训练语言模型（PLMs），主动学习的有效性，但不清楚一个模型中获得的主动学习收益在多大程度上适用于其他模型。我们考虑在文本分类中积极获取的数据集的可迁移性问题，并调查了使用主动学习构建的数据集在使用不同PLM训练时能否保持AL收益。我们将AL数据集的可迁移性与不同PLMs查询到的实例的相似性联系起来，并表明具有类似获取序列的AL方法生成的数据集非常具有可迁移性，无论使用哪种模型。此外，我们表明，获取序列的相似性更受到AL方法的选择而非模型的影响。

    Active learning (AL) aims to reduce labeling costs by querying the examples most beneficial for model learning. While the effectiveness of AL for fine-tuning transformer-based pre-trained language models (PLMs) has been demonstrated, it is less clear to what extent the AL gains obtained with one model transfer to others. We consider the problem of transferability of actively acquired datasets in text classification and investigate whether AL gains persist when a dataset built using AL coupled with a specific PLM is used to train a different PLM. We link the AL dataset transferability to the similarity of instances queried by the different PLMs and show that AL methods with similar acquisition sequences produce highly transferable datasets regardless of the models used. Additionally, we show that the similarity of acquisition sequences is influenced more by the choice of the AL method than the choice of the model.
    
[^204]: GPT用于半自动化数据科学：引入CAAFE实现上下文感知自动特征工程

    GPT for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v1 [cs.AI])

    [http://arxiv.org/abs/2305.03403](http://arxiv.org/abs/2305.03403)

    介绍了一种名为CAAFE的上下文感知自动特征工程方法，它利用大型语言模型根据数据集描述生成更多具有语义意义的特征，能够提高大多数数据集的性能，平均ROC AUC表现提高至0.822。

    

    随着自动化机器学习（AutoML）领域的发展，将领域知识纳入这些系统中变得越来越重要。我们利用大型语言模型（LLMs）的强大功能提出了一种方法来实现这一目标。具体地，我们介绍了一种用于表格数据的特征工程方法，名为上下文感知自动特征工程（CAAFE），它利用LLM根据数据集的描述生成更多具有语义意义的特征。该方法产生用于创建新特征的Python代码，并提供生成特征的效用说明。尽管方法论上很简单，但CAAFE提高了14个数据集中11个数据集的性能，与2个数据集并列，只有1个数据集性能下降，从而使所有数据集的平均ROC AUC表现从0.798提升至0.822。对于所评估的数据集，这一改进与使用随机森林（AUC 0.782）代替逻辑回归（AUC 0.754）所获得的平均改进相似。此外，

    As the field of automated machine learning (AutoML) advances, it becomes increasingly important to include domain knowledge within these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to generate additional semantically meaningful features for tabular datasets based on their descriptions. The method produces both Python code for creating new features and explanations for the utility of the generated features.  Despite being methodologically simple, CAAFE enhances performance on 11 out of 14 datasets, ties on 2 and looses on 1 - boosting mean ROC AUC performance from 0.798 to 0.822 across all datasets. On the evaluated datasets, this improvement is similar to the average improvement achieved by using a random forest (AUC 0.782) instead of logistic regression (AUC 0.754).  Furthermore,
    
[^205]: 目标导向变分编码器-解码器网络用于反问题的不确定性量化方法

    Goal-oriented Uncertainty Quantification for Inverse Problems via Variational Encoder-Decoder Networks. (arXiv:2304.08324v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2304.08324](http://arxiv.org/abs/2304.08324)

    本文提出了一种使用变分编码器-解码器网络进行目标导向反问题不确定性量化的新方法。通过利用数据驱动的方式，可以实时估计和计算与逆问题解相关的感兴趣量的不确定性指标。

    

    在这项工作中，我们描述了一种使用变分编码器-解码器（VED）网络的新方法，用于高效的目标导向反问题不确定性量化。与标准的逆问题不同，这些方法是“目标导向”的，目标是估计与逆问题的解的函数有关的一些感兴趣量（QoI），而不是解本身。此外，我们对计算与QoI相关的不确定性指标感兴趣，因此利用基于贝叶斯的逆问题方法，包括预测算子和探索后验的技术。这可能特别具有挑战性，尤其是对于非线性、可能未知的算子和非标准的先验假设。我们利用机器学习的最新进展，即VED网络，来描述基于数据驱动的大规模逆问题方法。这使得对QoI进行实时的目标导向不确定性量化成为可能。

    In this work, we describe a new approach that uses variational encoder-decoder (VED) networks for efficient goal-oriented uncertainty quantification for inverse problems. Contrary to standard inverse problems, these approaches are \emph{goal-oriented} in that the goal is to estimate some quantities of interest (QoI) that are functions of the solution of an inverse problem, rather than the solution itself. Moreover, we are interested in computing uncertainty metrics associated with the QoI, thus utilizing a Bayesian approach for inverse problems that incorporates the prediction operator and techniques for exploring the posterior. This may be particularly challenging, especially for nonlinear, possibly unknown, operators and nonstandard prior assumptions. We harness recent advances in machine learning, i.e., VED networks, to describe a data-driven approach to large-scale inverse problems. This enables a real-time goal-oriented uncertainty quantification for the QoI. One of the advantages
    
[^206]: 基于屏障-李亚普诺夫Actor-Critic强化学习方法的安全稳定控制

    A Barrier-Lyapunov Actor-Critic Reinforcement Learning Approach for Safe and Stable Control. (arXiv:2304.04066v1 [eess.SY])

    [http://arxiv.org/abs/2304.04066](http://arxiv.org/abs/2304.04066)

    本文提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，针对强化学习控制现实世界系统时的安全稳定控制问题提出了一种解决方案。其中，基于重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。

    

    强化学习在视频游戏和机器人等领域展现出惊人的性能。然而，使用强化学习控制现实世界系统时，确保安全和稳定性仍然是一个重大挑战。在本文中，我们首先为强化学习系统提供安全和稳定性的定义，然后将控制屏障函数（CBF）和控制李亚普诺夫函数（CLF）方法与Actor-Critic方法相结合，提出了一种基于屏障-李亚普诺夫Actor-Critic（BLAC）框架，有助于保持系统的安全和稳定性。在该框架中，基于来自重放缓冲区采样的数据构建了CBF安全约束和CLF稳定约束，并使用增广拉格朗日方法来更新基于RL的控制器的参数。此外，还引入了一个备用控制器，以防RL控制器无法提供稳定控制。

    Reinforcement learning (RL) has demonstrated impressive performance in various areas such as video games and robotics. However, ensuring safety and stability, which are two critical properties from a control perspective, remains a significant challenge when using RL to control real-world systems. In this paper, we first provide definitions of safety and stability for the RL system, and then combine the control barrier function (CBF) and control Lyapunov function (CLF) methods with the actor-critic method in RL to propose a Barrier-Lyapunov Actor-Critic (BLAC) framework which helps maintain the aforementioned safety and stability for the system. In this framework, CBF constraints for safety and CLF constraint for stability are constructed based on the data sampled from the replay buffer, and the augmented Lagrangian method is used to update the parameters of the RL-based controller. Furthermore, an additional backup controller is introduced in case the RL-based controller cannot provide
    
[^207]: 量子最优控制的高效量子算法

    Efficient Quantum Algorithms for Quantum Optimal Control. (arXiv:2304.02613v1 [quant-ph])

    [http://arxiv.org/abs/2304.02613](http://arxiv.org/abs/2304.02613)

    本文提出了一种高效的量子算法，解决量子最优控制问题，运用容错的量子计算机，具有复杂的机器学习关系。

    

    本文提出了一种高效的量子算法，比经典算法解决量子最优控制问题的速度快出指数倍。该问题需要寻找控制变量，以最大化在时间T时受时变薛定谔方程控制的系统的某个物理量。这种控制问题也与机器学习有着复杂的关系。我们的算法基于时变哈密顿模拟方法和快速梯度估计算法。我们还提供了全面的错误分析，以量化来自各个步骤的总误差，如控制函数的有限维表示、薛定谔方程的离散化、数值积分和优化。我们的量子算法需要容错的量子计算机。

    In this paper, we present efficient quantum algorithms that are exponentially faster than classical algorithms for solving the quantum optimal control problem. This problem involves finding the control variable that maximizes a physical quantity at time $T$, where the system is governed by a time-dependent Schr\"odinger equation. This type of control problem also has an intricate relation with machine learning. Our algorithms are based on a time-dependent Hamiltonian simulation method and a fast gradient-estimation algorithm. We also provide a comprehensive error analysis to quantify the total error from various steps, such as the finite-dimensional representation of the control function, the discretization of the Schr\"odinger equation, the numerical quadrature, and optimization. Our quantum algorithms require fault-tolerant quantum computers.
    
[^208]: 主动自监督学习: 只需少量低成本的关系即可。

    Active Self-Supervised Learning: A Few Low-Cost Relationships Are All You Need. (arXiv:2303.15256v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.15256](http://arxiv.org/abs/2303.15256)

    本研究提出了积极自监督学习（PAL）框架，通过查询样本之间的语义关系来解决自监督学习中需要知道正视图的限制。PAL不仅提供了理论上有基础的学习框架，还能够嵌入先验知识并支持监督和半监督学习。

    

    自监督学习（SSL）已成为学习无标记数据的可转移表示的首选解决方案。然而，SSL需要构建已知语义相似的样本，即正视图。这种需求是SSL的主要限制，通常通过临时策略来处理，例如对相同输入应用已知的数据增强。在这项工作中，我们通过积极主动学习（PAL）将这个原则形式化和推广，其中一个oracle查询样本之间的语义关系。PAL实现了三个主要目标。首先，它揭示了一个基于相似性图的理论上有基础的学习框架，超越了SSL，可根据所使用的oracle扩展到处理监督和半监督学习。其次，它为嵌入先验知识（如观测的标签）提供了一致的算法，而无需更改训练流程中的任何内容。第三，它提供了一个适当的主动学习框架。

    Self-Supervised Learning (SSL) has emerged as the solution of choice to learn transferable representations from unlabeled data. However, SSL requires to build samples that are known to be semantically akin, i.e. positive views. Requiring such knowledge is the main limitation of SSL and is often tackled by ad-hoc strategies e.g. applying known data-augmentations to the same input. In this work, we formalize and generalize this principle through Positive Active Learning (PAL) where an oracle queries semantic relationships between samples. PAL achieves three main objectives. First, it unveils a theoretically grounded learning framework beyond SSL, based on similarity graphs, that can be extended to tackle supervised and semi-supervised learning depending on the employed oracle. Second, it provides a consistent algorithm to embed a priori knowledge, e.g. some observed labels, into any SSL losses without any change in the training pipeline. Third, it provides a proper active learning framew
    
[^209]: 使用分层变分自动编码器对逆问题进行正则化

    Inverse problem regularization with hierarchical variational autoencoders. (arXiv:2303.11217v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2303.11217](http://arxiv.org/abs/2303.11217)

    本文提出了一种使用分层变分自动编码器（HVAE）对不适定的逆问题进行正则化的方法，该方法综合了基于去噪器的Plug & Play方法和基于生成模型的逆问题方法的优势，并且在自然图像的图像恢复问题上表现出竞争力。

    

    本文提出使用深层分层变分自动编码器（HVAE）作为图像先验对不适定的逆问题进行正则化的方法。该方法综合了基于去噪器的Plug & Play方法和基于生成模型的逆问题方法的优势。首先，我们利用VAE的特性设计了一种高效的算法，从而受益于Plug-and-Play（PnP）方法的收敛保证。其次，我们的方法不限于专门的数据集，所提出的PnP-HVAE模型能够解决任意尺寸的自然图像的图像恢复问题。实验结果表明，所提出的PnP-HVAE方法在与基于去噪器的Plug & Play方法和基于生成模型的其他现有恢复方法相比具有竞争力。

    In this paper, we propose to regularize ill-posed inverse problems using a deep hierarchical variational autoencoder (HVAE) as an image prior. The proposed method synthesizes the advantages of i) denoiser-based Plug \& Play approaches and ii) generative model based approaches to inverse problems. First, we exploit VAE properties to design an efficient algorithm that benefits from convergence guarantees of Plug-and-Play (PnP) methods. Second, our approach is not restricted to specialized datasets and the proposed PnP-HVAE model is able to solve image restoration problems on natural images of any size. Our experiments show that the proposed PnP-HVAE method is competitive with both SOTA denoiser-based PnP approaches, and other SOTA restoration methods based on generative models.
    
[^210]: 金融时间序列的元对比标签校正方法

    Meta contrastive label correction for financial time series. (arXiv:2303.08103v1 [cs.LG])

    [http://arxiv.org/abs/2303.08103](http://arxiv.org/abs/2303.08103)

    本文针对股票价格预测中标记不准确的问题，提出了一种元对比标签校正方法。方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签，并提高分类性能。

    

    金融应用（如股票价格预测）通常面临标记不准确的问题，本文提出一种元对比标签校正方法，可以自动生成准确标签，并提高分类性能。该方法包括将对比学习算法融入元学习框架中，通过Gramian angular field和代表学习将时间序列数据生成图像，从而自动生成准确标签。

    Financial applications such as stock price forecasting, usually face an issue that under the predefined labeling rules, it is hard to accurately predict the directions of stock movement. This is because traditional ways of labeling, taking Triple Barrier Method, for example, usually gives us inaccurate or even corrupted labels. To address this issue, we focus on two main goals. One is that our proposed method can automatically generate correct labels for noisy time series patterns, while at the same time, the method is capable of boosting classification performance on this new labeled dataset. Based on the aforementioned goals, our approach has the following three novelties: First, we fuse a new contrastive learning algorithm into the meta-learning framework to estimate correct labels iteratively when updating the classification model inside. Moreover, we utilize images generated from time series data through Gramian angular field and representative learning. Most important of all, we 
    
[^211]: 用于编码的大型语言模型：安全加固和对抗测试

    Large Language Models for Code: Security Hardening and Adversarial Testing. (arXiv:2302.05319v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2302.05319](http://arxiv.org/abs/2302.05319)

    本研究针对大型语言模型在生成代码时缺乏安全意识，从安全加固和对抗测试的角度入手，提出了一项新的安全任务——受控代码生成，通过一种新型基于学习的方法SVEN，实现生成既安全又功能正确的代码，并对当前的LM进行对抗测试，强调了在LM的培训和评估中考虑安全因素的必要性。

    

    大型语言模型(LMs)越来越多地预先在大规模代码库上进行预处理，用于生成代码。然而，LM缺乏安全意识，并经常生成不安全的代码。本研究沿着两个重要方向研究了LM的安全性:(i)安全加固，旨在增强LM在生成安全代码方面的可靠性;(ii)对抗测试，旨在在对抗性立场评估LM的安全性。我们通过制定一项称为受控代码生成的新安全任务来同时解决这两个问题。该任务是参数化的，将一个二进制属性作为输入，以指导LM生成安全或不安全的代码，同时保留LM生成功能正确代码的能力。我们提出了一种称为SVEN的新型基于学习的方法来解决这个任务。SVEN利用属性特定的连续向量来引导程序生成达到给定的属性，而不修改LM的权重。我们的训练过程通过可微分的投影损失来优化这些连续向量，实现端到端的训练。此外，我们使用SVEN进行对抗测试，并表明当前的LM容易受到攻击，在测试时修改它们的输入而保留功能。我们的工作强调需要在LM的培训和评估中考虑安全因素。

    Large language models (LMs) are increasingly pretrained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous ve
    
[^212]: 基于分层生成对抗模拟学习的自动驾驶在城市环境中的应用

    Hierarchical Generative Adversarial Imitation Learning with Mid-level Input Generation for Autonomous Driving on Urban Environments. (arXiv:2302.04823v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.04823](http://arxiv.org/abs/2302.04823)

    本研究提出了一种名为hGAIL的架构，用于解决车辆的自主导航问题，通过将感知信息直接映射到低级动作的同时，学习车辆环境的中级输入表示。

    

    对于现实中的城市导航场景，设计健壮的控制策略并不是一项简单的任务。在端到端的方法中，这些策略必须将车辆摄像头获得的高维图像映射到低级动作，如转向和油门。本研究提出了一种名为hGAIL的架构，用于解决车辆的自主导航问题，通过将感知信息直接映射到低级动作的同时，学习车辆环境的中级输入表示。

    Deriving robust control policies for realistic urban navigation scenarios is not a trivial task. In an end-to-end approach, these policies must map high-dimensional images from the vehicle's cameras to low-level actions such as steering and throttle. While pure Reinforcement Learning (RL) approaches are based exclusively on rewards,Generative Adversarial Imitation Learning (GAIL) agents learn from expert demonstrations while interacting with the environment, which favors GAIL on tasks for which a reward signal is difficult to derive. In this work, the hGAIL architecture was proposed to solve the autonomous navigation of a vehicle in an end-to-end approach, mapping sensory perceptions directly to low-level actions, while simultaneously learning mid-level input representations of the agent's environment. The proposed hGAIL consists of an hierarchical Adversarial Imitation Learning architecture composed of two main modules: the GAN (Generative Adversarial Nets) which generates the Bird's-
    
[^213]: 使用图神经网络和强化学习生成能中断交换的阻塞工件车间问题中的调度规则

    Generating Dispatching Rules for the Interrupting Swap-Allowed Blocking Job Shop Problem Using Graph Neural Network and Reinforcement Learning. (arXiv:2302.02506v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02506](http://arxiv.org/abs/2302.02506)

    本研究提出了一种基于图神经网络和强化学习的方法，用于生成适应性调度规则来解决中断交换允许的阻塞工件车间问题，并开发了一个模拟器来模拟实际工业生产中的中断和交换情况。

    

    中断交换允许的阻塞工件车间问题（ISBJSSP）是一个复杂的调度问题，能够通过解决存储能力不足和意外生产中断的问题，实际模拟许多制造规划和物流应用。由于机器故障或维护的随机干扰，工业生产环境通常选择采用调度规则来实现自适应的实时重调度，而不是需要在问题条件动态变化时进行昂贵的重新计算的传统方法。为了生成ISBJSSP问题的调度规则，我们引入了一种动态的分离图公式，其中的节点和边可持续进行删除和添加。这种公式使得能够利用图神经网络和强化学习来训练自适应调度器。此外，还开发了一个模拟器来模拟中断、交换等情况。

    The interrupting swap-allowed blocking job shop problem (ISBJSSP) is a complex scheduling problem that is able to model many manufacturing planning and logistics applications realistically by addressing both the lack of storage capacity and unforeseen production interruptions. Subjected to random disruptions due to machine malfunction or maintenance, industry production settings often choose to adopt dispatching rules to enable adaptive, real-time re-scheduling, rather than traditional methods that require costly re-computation on the new configuration every time the problem condition changes dynamically. To generate dispatching rules for the ISBJSSP problem, we introduce a dynamic disjunctive graph formulation characterized by nodes and edges subjected to continuous deletions and additions. This formulation enables the training of an adaptive scheduler utilizing graph neural networks and reinforcement learning. Furthermore, a simulator is developed to simulate interruption, swapping, 
    
[^214]: 多视角视觉融合的集成学习在遮挡和缺失信息情况下的应用：框架和基于真实世界数据的司机手势识别评估

    Ensemble Learning for Fusion of Multiview Vision with Occlusion and Missing Information: Framework and Evaluations with Real-World Data and Applications in Driver Hand Activity Recognition. (arXiv:2301.12592v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.12592](http://arxiv.org/abs/2301.12592)

    本研究提出了一种集成学习的多视角视觉融合方法，用于处理在真实世界应用中可能存在的间歇性信息缺失问题。通过设计学习框架和提出的插补方案，实现了对驾驶员手势活动的准确分类和位置估计，提高了自动驾驶的安全性能。

    

    多传感器框架为集成学习和传感器融合提供了机会，以利用冗余和补充信息，有助于在真实世界安全应用中进行连续司机状态监测。我们定义了由遮挡、噪声或传感器故障引起的间歇性信息缺失问题，并设计了一个围绕这些数据缺口的学习框架，提出并分析了一种填补信息缺失的插补方案。我们将这些思想应用于基于摄像头的手势活动分类任务，以实现自动驾驶的安全性。我们表明，基于并行卷积神经网络的后期融合方法，在同组受试者验证时，甚至可以超过最佳单一摄像头模型在估计手中物体的位置时，我们的多摄像头框架表现最好，而且在跨组验证时也表现出色。

    Multi-sensor frameworks provide opportunities for ensemble learning and sensor fusion to make use of redundancy and supplemental information, helpful in real-world safety applications such as continuous driver state monitoring which necessitate predictions even in cases where information may be intermittently missing. We define this problem of intermittent instances of missing information (by occlusion, noise, or sensor failure) and design a learning framework around these data gaps, proposing and analyzing an imputation scheme to handle missing information. We apply these ideas to tasks in camera-based hand activity classification for robust safety during autonomous driving. We show that a late-fusion approach between parallel convolutional neural networks can outperform even the best-placed single camera model in estimating the hands' held objects and positions when validated on within-group subjects, and that our multi-camera framework performs best on average in cross-group validat
    
[^215]: BQ-NCO: 高效神经组合优化的双射缩减

    BQ-NCO: Bisimulation Quotienting for Efficient Neural Combinatorial Optimization. (arXiv:2301.03313v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.03313](http://arxiv.org/abs/2301.03313)

    BQ-NCO通过将组合优化问题转化为马尔科夫决策过程并使用双射缩减方法来提高超出分布的鲁棒性。

    

    尽管基于神经网络的组合优化方法在端到端启发式学习方面取得了成功，但超出分布的泛化仍然是一个挑战。在本文中，我们将组合优化问题(COPs)新颖地形式化为马尔科夫决策过程(MDPs)，有效地利用COPs的共同对称性，提高超出分布的鲁棒性。从直接的构造方法的MDP形式化开始，我们引入了一种基于MDPs中双射缩减(BQ)的通用状态空间缩减方法。然后，对于具有递归性质的COPs，我们特化双射缩减，并展示了如何利用减小的状态空间来利用这些问题的对称性并促进MDP求解。我们的方法是有原则的，并证明了提出的BQ-MDP的最优策略实际上解决了相关的COPs。我们在五个经典问题上说明了我们的方法：欧几里得旅行商问题、非对称旅行商问题、容量车辆路由问题、定向问题和...

    Despite the success of neural-based combinatorial optimization methods for end-to-end heuristic learning, out-of-distribution generalization remains a challenge. In this paper, we present a novel formulation of Combinatorial Optimization Problems (COPs) as Markov Decision Processes (MDPs) that effectively leverages common symmetries of COPs to improve out-of-distribution robustness. Starting from a direct MDP formulation of a constructive method, we introduce a generic way to reduce the state space, based on Bisimulation Quotienting (BQ) in MDPs. Then, for COPs with a recursive nature, we specialize the bisimulation and show how the reduced state exploits the symmetries of these problems and facilitates MDP solving. Our approach is principled and we prove that an optimal policy for the proposed BQ-MDP actually solves the associated COPs. We illustrate our approach on five classical problems: the Euclidean and Asymmetric Traveling Salesman, Capacitated Vehicle Routing, Orienteering and 
    
[^216]: 学习和决策的风险自适应方法：一项调查

    Risk-Adaptive Approaches to Learning and Decision Making: A Survey. (arXiv:2212.00856v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2212.00856](http://arxiv.org/abs/2212.00856)

    本文调查了过去25年中风险测度的快速发展，介绍了其在各个领域的应用，以及与效用理论和分布鲁棒优化的关系，并指出了公平机器学习等新兴应用领域。

    

    不确定性在工程设计、统计学习和决策制定中普遍存在。由于固有的风险规避和对假设的模糊性，通常通过制定和解决使用风险和相关概念的保守优化模型来解决不确定性问题。我们对过去25年来风险测度的快速发展进行了调查。从它们在金融工程领域的起步，我们回顾了它们在几乎所有领域的工程和应用数学中的应用。风险测度扎根于凸分析，为处理不确定性提供了一个具有重要计算和理论优势的通用框架。我们描述了关键事实，列举了几种具体算法，并提供了大量参考文献供进一步阅读。该调查还回顾了与效用理论和分布鲁棒优化的联系，指出了新兴应用领域，如公平机器学习，并定义了相对测度。

    Uncertainty is prevalent in engineering design, statistical learning, and decision making broadly. Due to inherent risk-averseness and ambiguity about assumptions, it is common to address uncertainty by formulating and solving conservative optimization models expressed using measures of risk and related concepts. We survey the rapid development of risk measures over the last quarter century. From their beginning in financial engineering, we recount the spread to nearly all areas of engineering and applied mathematics. Solidly rooted in convex analysis, risk measures furnish a general framework for handling uncertainty with significant computational and theoretical advantages. We describe the key facts, list several concrete algorithms, and provide an extensive list of references for further reading. The survey recalls connections with utility theory and distributionally robust optimization, points to emerging applications areas such as fair machine learning, and defines measures of rel
    
[^217]: 高效控制重新识别风险的差分隐私数据合成

    Differentially-Private Data Synthetisation for Efficient Re-Identification Risk Control. (arXiv:2212.00484v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.00484](http://arxiv.org/abs/2212.00484)

    本文提出了一种名为ε-PrivateSMOTE的技术，通过噪声引入插值的合成数据生成，以达到保护免受重新识别和链接攻击的风险的目的，并在同时最大限度地保持数据效用的情况下取得了竞争性的结果。

    

    保护用户数据隐私可以通过多种方法实现，从统计转换到生成模型。然而，所有这些方法都存在重要的缺陷。例如，使用传统技术创建转换数据集非常耗时。此外，最近基于深度学习的解决方案除了长时间的训练阶段外，还需要大量的计算资源，而差分隐私解决方案可能会削弱数据效用。在本文中，我们提议了一种名为ε-PrivateSMOTE的技术，用于保护免受重新识别和链接攻击的风险，并特别解决高重新识别风险的情况。我们的提议通过噪声引入插值的合成数据生成，以模糊高风险案例，同时最大限度地保持原始数据的数据效用。与17个数据集上的多个传统和最新隐私保护方法相比，ε-PrivateSMOTE在隐私风险和数据效用方面取得了竞争性的结果。

    Protecting user data privacy can be achieved via many methods, from statistical transformations to generative models. However, all of them have critical drawbacks. For example, creating a transformed data set using traditional techniques is highly time-consuming. Also, recent deep learning-based solutions require significant computational resources in addition to long training phases, and differentially private-based solutions may undermine data utility. In this paper, we propose $\epsilon$-PrivateSMOTE, a technique designed for safeguarding against re-identification and linkage attacks, particularly addressing cases with a high re-identification risk. Our proposal combines synthetic data generation via noise-induced interpolation to obfuscate high-risk cases while maximising the data utility of the original data. Compared to multiple traditional and state-of-the-art privacy-preservation methods on 17 data sets, $\epsilon$-PrivateSMOTE achieves competitive results in privacy risk and b
    
[^218]: 基于深度神经网络的广义平衡权重

    Generalized Balancing Weights via Deep Neural Networks. (arXiv:2211.07533v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07533](http://arxiv.org/abs/2211.07533)

    本文提出了一种广义平衡权重方法（NBW），通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重，用于估计任意混合离散和连续干预的因果效应。

    

    从观测数据中估计因果效应是许多领域中的一个中心问题。一种广泛使用的方法是平衡协变量的权重，使得数据的分布类似于随机化。我们提出了一种称为神经平衡权重（NBW）的广义平衡权重，以估计任意混合离散和连续干预的因果效应。通过优化 $f$ -分布的变分表示，直接估计源和平衡分布之间的密度比，获得了权重。为此，我们选择了 $\alpha$-差异作为优化的目标函数，因为它具有样本复杂度独立于其地面实况值和无偏小批量梯度的估计器，而且对于梯度消失问题具有优势。此外，我们提供了以下两种方法来估计平衡权重：提高平衡权重的泛化性能和检查其效果。

    Estimating causal effects from observational data is a central problem in many domains. A general approach is to balance covariates with weights such that the distribution of the data mimics randomization. We present generalized balancing weights, Neural Balancing Weights (NBW), to estimate the causal effects of an arbitrary mixture of discrete and continuous interventions. The weights were obtained through direct estimation of the density ratio between the source and balanced distributions by optimizing the variational representation of $f$-divergence. For this, we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem. In addition, we provide the following two methods for estimating the balancing weights: improving the generalization performance of the balancing weights and checking 
    
[^219]: TiDAL: 学习主动学习的训练动力学

    TiDAL: Learning Training Dynamics for Active Learning. (arXiv:2210.06788v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.06788](http://arxiv.org/abs/2210.06788)

    提出了一种新的主动学习方法TiDAL，利用训练动力学来量化未标记数据的不确定性，为了解决大规模数据的跟踪问题，利用预测模块学习标记数据的训练动力学。

    

    主动学习旨在在有限预算下从未标记的数据池中选择最有用的数据样本并对其进行注释，以扩展标记数据集。尤其是基于不确定性的方法选择最不确定的样本，在改善模型性能方面已被证明是有效的。然而，主动学习文献经常忽视训练动力学（TD），即通过随机梯度下降进行优化时模型行为的不断变化，尽管文献的其他领域经验证明TD提供了衡量样本不确定性的重要线索。在本文中，我们提出了一种新颖的主动学习方法，即训练动力学主动学习（TiDAL），它利用TD来量化未标记数据的不确定性。由于跟踪大规模未标记数据的TD是不切实际的，所以TiDAL利用一个额外的预测模块来学习标记数据的TD。为了进一步证明TiDAL的设计，我们提供了理论和实证证据。

    Active learning (AL) aims to select the most useful data samples from an unlabeled data pool and annotate them to expand the labeled dataset under a limited budget. Especially, uncertainty-based methods choose the most uncertain samples, which are known to be effective in improving model performance. However, AL literature often overlooks training dynamics (TD), defined as the ever-changing model behavior during optimization via stochastic gradient descent, even though other areas of literature have empirically shown that TD provides important clues for measuring the sample uncertainty. In this paper, we propose a novel AL method, Training Dynamics for Active Learning (TiDAL), which leverages the TD to quantify uncertainties of unlabeled data. Since tracking the TD of all the large-scale unlabeled data is impractical, TiDAL utilizes an additional prediction module that learns the TD of labeled data. To further justify the design of TiDAL, we provide theoretical and empirical evidence t
    
[^220]: COVID-19患者数据中优先确定严重疾病类别的分层Neyman-Pearson分类

    Hierarchical Neyman-Pearson Classification for Prioritizing Severe Disease Categories in COVID-19 Patient Data. (arXiv:2210.02197v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.02197](http://arxiv.org/abs/2210.02197)

    本研究提出了一种分层的Neyman-Pearson分类框架，用于在COVID-19患者数据中优先确定严重疾病类别。通过使用患者的生物特征来预测严重程度类别，该方法可以控制错误分类，并在多类别分类中提供高概率的错误优先控制。

    

    COVID-19的疾病严重程度从无症状到需要住院不等。了解导致疾病严重程度的机制对于开发有效的治疗方法和降低死亡率至关重要。一种获得这种理解的方法是使用多类别分类框架，其中使用患者的生物特征来预测患者的严重程度类别。在这个严重程度分类问题中，有利于优先识别更严重的类别并控制“次级分类”错误，即将患者错误分类为更轻微的类别。Neyman-Pearson (NP)分类范式已经被开发出来优先处理指定类型的错误。然而，目前的NP过程要么适用于二元分类，要么在多类别分类中无法高概率控制优先错误。在这里，我们提出了一种分层NP (H-NP)框架和一个通用的算法，并通过实验表明其优于现有方法。

    COVID-19 has a spectrum of disease severity, ranging from asymptomatic to requiring hospitalization. Understanding the mechanisms driving disease severity is crucial for developing effective treatments and reducing mortality rates. One way to gain such understanding is using a multi-class classification framework, in which patients' biological features are used to predict patients' severity classes. In this severity classification problem, it is beneficial to prioritize the identification of more severe classes and control the "under-classification" errors, in which patients are misclassified into less severe categories. The Neyman-Pearson (NP) classification paradigm has been developed to prioritize the designated type of error. However, current NP procedures are either for binary classification or do not provide high probability controls on the prioritized errors in multi-class classification. Here, we propose a hierarchical NP (H-NP) framework and an umbrella algorithm that generall
    
[^221]: 使用射线调谐阵列：物理信息驱动的量子点电荷态调谐

    Tuning arrays with rays: Physics-informed tuning of quantum dot charge states. (arXiv:2209.03837v2 [cond-mat.mes-hall] UPDATED)

    [http://arxiv.org/abs/2209.03837](http://arxiv.org/abs/2209.03837)

    本论文提出了一种名为物理信息驱动的调谐（PIT）的框架，用于自动调谐量子计算机中的全局状态和电荷态。其中，PIT的第一个模块使用机器学习分类器和物理知识进行导航，第二个模块使用一系列测量来调节电荷状态。这些工具具有直观、可靠和数据高效的特点。

    

    基于门定义的量子点（QDs）的量子计算机预计能够实现规模化。然而，随着量子比特数量的增加，手动校准这些系统的负担变得不合理，必须使用自动调谐。最近已经有一系列自动调谐不同QD参数的演示，例如粗略的门范围、全局状态拓扑（例如单个QD、双重QD）、电荷和隧道耦合，以及使用各种方法。在这里，我们展示了一组直观、可靠和数据高效的工具，用于自动调谐全局状态和电荷，这个框架被称为物理信息驱动的调谐（PIT）。PIT的第一个模块是一种基于行动的算法，它将机器学习分类器与物理知识相结合，以导航到目标全局状态。第二个模块使用一系列一维测量来调节到目标电荷状态，首先将QDs完全放空，然后校准电容耦合并导航到目标状态。

    Quantum computers based on gate-defined quantum dots (QDs) are expected to scale. However, as the number of qubits increases, the burden of manually calibrating these systems becomes unreasonable and autonomous tuning must be used. There has been a range of recent demonstrations of automated tuning of various QD parameters such as coarse gate ranges, global state topology (e.g. single QD, double QD), charge, and tunnel coupling with a variety of methods. Here, we demonstrate an intuitive, reliable, and data-efficient set of tools for an automated global state and charge tuning in a framework deemed physics-informed tuning (PIT). The first module of PIT is an action-based algorithm that combines a machine learning classifier with physics knowledge to navigate to a target global state. The second module uses a series of one-dimensional measurements to tune to a target charge state by first emptying the QDs of charge, followed by calibrating capacitive couplings and navigating to the targ
    
[^222]: 使用非对齐配对训练数据的变形等变交叉模态图像合成

    Deformation equivariant cross-modality image synthesis with paired non-aligned training data. (arXiv:2208.12491v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2208.12491](http://arxiv.org/abs/2208.12491)

    本研究提出了一种使用配对但不对齐数据进行交叉模态图像合成的通用解决方案，通过引入新的变形等变鼓励损失函数，实现了联合训练图像合成网络和配准网络，并允许在非对齐数据条件下进行对抗训练。这项工作降低了新的临床应用的难度。

    

    交叉模态图像合成是一个活跃的研究领域，具有多个医学临床相关应用。最近，出现了一些允许使用配对但不对齐数据进行训练的方法。然而，在广泛的真实世界数据集上不存在稳健且性能良好的方法。在本研究中，我们提出了一个通用的解决方案，用于解决使用配对但不对齐数据的交叉模态图像合成问题，通过引入新的变形等变鼓励损失函数。该方法由图像合成网络的联合训练以及单独的配准网络组成，并允许在非对齐数据条件下对输入进行对抗训练。该工作通过允许更难的数据集轻松训练交叉模态图像合成网络，降低了新的临床应用的门槛。

    Cross-modality image synthesis is an active research topic with multiple medical clinically relevant applications. Recently, methods allowing training with paired but misaligned data have started to emerge. However, no robust and well-performing methods applicable to a wide range of real world data sets exist. In this work, we propose a generic solution to the problem of cross-modality image synthesis with paired but non-aligned data by introducing new deformation equivariance encouraging loss functions. The method consists of joint training of an image synthesis network together with separate registration networks and allows adversarial training conditioned on the input even with misaligned data. The work lowers the bar for new clinical applications by allowing effortless training of cross-modality image synthesis networks for more difficult data sets.
    
[^223]: 混合量子ResNet用于汽车分类及其超参数优化

    Hybrid quantum ResNet for car classification and its hyperparameter optimization. (arXiv:2205.04878v2 [quant-ph] UPDATED)

    [http://arxiv.org/abs/2205.04878](http://arxiv.org/abs/2205.04878)

    本文提出了一种混合量子-经典机器学习模型和基于量子灵感的超参数优化技术，用于图像识别任务中的汽车分类。我们的研究在标准黑盒目标函数上进行了基准测试，证明了我们的方法在减少期望运行时间和适应度方面的性能改善。

    

    图像识别是机器学习算法的主要应用之一。然而，现代图像识别系统中使用的机器学习模型通常由数百万个参数组成，通常需要大量计算时间进行调整。此外，调整模型超参数会产生额外的开销。因此，需要新的机器学习模型和超参数优化技术的发展。本文提出了一种灵感于量子的超参数优化技术和混合量子-经典机器学习模型用于监督学习。我们对标准黑盒目标函数进行了超参数优化方法的基准测试，观察到随着搜索空间大小的增加，预期运行时间和适应度的降低的性能改善。我们通过对汽车图像分类任务进行测试，并展示了混合量子ResNet模型的全面实现。

    Image recognition is one of the primary applications of machine learning algorithms. Nevertheless, machine learning models used in modern image recognition systems consist of millions of parameters that usually require significant computational time to be adjusted. Moreover, adjustment of model hyperparameters leads to additional overhead. Because of this, new developments in machine learning models and hyperparameter optimization techniques are required. This paper presents a quantum-inspired hyperparameter optimization technique and a hybrid quantum-classical machine learning model for supervised learning. We benchmark our hyperparameter optimization method over standard black-box objective functions and observe performance improvements in the form of reduced expected run times and fitness in response to the growth in the size of the search space. We test our approaches in a car image classification task and demonstrate a full-scale implementation of the hybrid quantum ResNet model w
    
[^224]: 基于流的态密度方法用于复杂动作问题

    Flow-based density of states for complex actions. (arXiv:2203.01243v2 [hep-lat] UPDATED)

    [http://arxiv.org/abs/2203.01243](http://arxiv.org/abs/2203.01243)

    基于归一化流的新型采样算法可以解决复杂动作问题中的态密度计算困难，并且避免了数值积分带来的误差累积。

    

    基于归一化流的新型采样算法可以解决格点计算中的遍历性问题。而且已经注意到流可以用来计算传统方法难以获得的热力学量。这表明它们也适用于复杂动作问题中的态密度方法。特别地，基于流的采样可以直接计算密度，与传统的通过测量和积分其对数导数来重建密度的策略相反。通过规避这一过程，可以完全避免数值积分引入的误差累积，并且可以明确确定整体归一化因子。在这个原理证明性研究中，我们在二分量标量场理论的背景下展示了我们的方法，其中$O(2)$对称性被一个虚数外场显式破缺。

    Emerging sampling algorithms based on normalizing flows have the potential to solve ergodicity problems in lattice calculations. Furthermore, it has been noted that flows can be used to compute thermodynamic quantities which are difficult to access with traditional methods. This suggests that they are also applicable to the density-of-states approach to complex action problems. In particular, flow-based sampling may be used to compute the density directly, in contradistinction to the conventional strategy of reconstructing it via measuring and integrating the derivative of its logarithm. By circumventing this procedure, the accumulation of errors from the numerical integration is avoided completely and the overall normalization factor can be determined explicitly. In this proof-of-principle study, we demonstrate our method in the context of two-component scalar field theory where the $O(2)$ symmetry is explicitly broken by an imaginary external field. First, we concentrate on the zero-
    
[^225]: 分布式线性随机逼近的有限时间误差界限

    Finite-Time Error Bounds for Distributed Linear Stochastic Approximation. (arXiv:2111.12665v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2111.12665](http://arxiv.org/abs/2111.12665)

    本文研究了一种新颖的分布式线性随机逼近算法，在邻居间进行共识型交互，并考虑了马尔可夫噪声的影响。针对连接矩阵为简单随机矩阵的情况，推导出了统一强连接图序列下的有限时间平方均误差界限。

    

    本文研究了一种新颖的多代理线性随机逼近算法，该算法由马尔可夫噪声驱动，并具有一般的共识型交互。每个代理根据其本地随机逼近过程演化，其中该过程依赖于其邻居的信息。代理之间的连接结构由一个时变有向图描述。虽然已经研究过基于共识的随机逼近算法在代理间连接由双随机矩阵描述时的收敛性（至少期望收敛），但对于连接矩阵为简单随机矩阵的情况了解较少。对于与随机连接矩阵相关的统一强连接图序列，本文推导了平方均误差的有限时间界限，平方均误差定义为算法输出与相关常微分方程的唯一平衡点之间的偏差。

    This paper considers a novel multi-agent linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction, in which each agent evolves according to its local stochastic approximation process which depends on the information from its neighbors. The interconnection structure among the agents is described by a time-varying directed graph. While the convergence of consensus-based stochastic approximation algorithms when the interconnection among the agents is described by doubly stochastic matrices (at least in expectation) has been studied, less is known about the case when the interconnection matrix is simply stochastic. For any uniformly strongly connected graph sequences whose associated interaction matrices are stochastic, the paper derives finite-time bounds on the mean-square error, defined as the deviation of the output of the algorithm from the unique equilibrium point of the associated ordinary differential equation. For the case of inter
    
[^226]: 通过目标网络打破致命三角（Reinforcement Learning）

    Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v9 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.08862](http://arxiv.org/abs/2101.08862)

    本论文研究了如何通过使用目标网络来稳定训练，提出了一种新的目标网络更新规则并证明了其在离线学习、线性函数逼近和自举的算法中的收敛性，最终达到了收敛到正则化TD固定点的效果。

    

    “致命三角”是指强化学习算法在同时使用离线学习、函数逼近和自举时的不稳定性。本文研究了目标网络作为打破“致命三角”的工具，提供了理论支持，证明了目标网络稳定训练的常识。首先，我们提出并分析了一种新颖的目标网络更新规则，将常用的 Polyak 平均风格更新与两个投影相结合。然后，在几个不同的算法中应用目标网络和岭正则化，证明它们可以收敛到正则化 TD 固定点，这些算法都是离线学习、线性函数逼近和自举的，涵盖政策评估和控制，以及折扣和平均奖励设置。特别地，我们提供了第一个收敛的线性$Q$学习算法，这些算法在非限制性和变化行为策略下均成立，不需要双层优化。

    The deadly triad refers to the instability of a reinforcement learning algorithm when it employs off-policy learning, function approximation, and bootstrapping simultaneously. In this paper, we investigate the target network as a tool for breaking the deadly triad, providing theoretical support for the conventional wisdom that a target network stabilizes training. We first propose and analyze a novel target network update rule which augments the commonly used Polyak-averaging style update with two projections. We then apply the target network and ridge regularization in several divergent algorithms and show their convergence to regularized TD fixed points. Those algorithms are off-policy with linear function approximation and bootstrapping, spanning both policy evaluation and control, as well as both discounted and average-reward settings. In particular, we provide the first convergent linear $Q$-learning algorithms under nonrestrictive and changing behavior policies without bi-level o
    
[^227]: 控制-数据分离和逻辑条件传播用于概率程序的高效推理

    Control-Data Separation and Logical Condition Propagation for Efficient Inference on Probabilistic Programs. (arXiv:2101.01502v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2101.01502](http://arxiv.org/abs/2101.01502)

    本论文提出了一种新的概率程序采样框架，通过将控制-数据分离和逻辑条件传播相结合，实现了高效推理，并在使用while循环和稀有观测的程序中表现出了优势。

    

    我们提出了一种新颖的概率程序采样框架。该框架以一种非常复杂的方式将两个最新的思想——控制-数据分离和逻辑条件传播相结合，使得两个思想相互增强优势。我们在Anglican上实现了算法。实验结果表明，我们的算法在while循环和稀有观测的程序中尤其高效。

    We present a novel sampling framework for probabilistic programs. The framework combines two recent ideas -- \emph{control-data separation} and \emph{logical condition propagation} -- in a nontrivial manner so that the two ideas boost the benefits of each other. We implemented our algorithm on top of Anglican. The experimental results demonstrate our algorithm's efficiency, especially for programs with while loops and rare observations.
    

