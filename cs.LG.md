# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Bake off redux: a review and experimental evaluation of recent time series classification algorithms.](http://arxiv.org/abs/2304.13029) | 本文重访烘焙大赛，评估了最近时间序列分类算法在112个数据集上的表现。该论文通过分类法将这些算法分为五类，为TSC领域的发展提供了贡献。 |
| [^2] | [Certifying Ensembles: A General Certification Theory with S-Lipschitzness.](http://arxiv.org/abs/2304.13019) | 本文研究了深度学习模型集成的认证鲁棒性问题，提出了S-Lipschitz分类器概念并给出了集成理论鲁棒性的准确条件。 |
| [^3] | [DuETT: Dual Event Time Transformer for Electronic Health Records.](http://arxiv.org/abs/2304.13017) | DuETT是一个用于EHR的双重事件时间变换器，通过双重注意机制学习不同上下文中相同时间步的不同表示，避免由于时间步伐大而产生的二次放缩问题，并在四个基准EHR数据集上优于以往的单一任务最先进方法。 |
| [^4] | [Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation.](http://arxiv.org/abs/2304.13016) | 研究了比例渐近情形下的子采样岭回归集成，证明了最优全岭回归集成的风险与最优岭预测器的风险相匹配，并证明了GCV在估计岭回归集合的预测风险方面的强一致性。 |
| [^5] | [Stable and low-precision training for large-scale vision-language models.](http://arxiv.org/abs/2304.13013) | 该研究介绍了用于大规模视觉语言模型稳定和低精度训练的新方法，包括SwitchBack和AdamW-Adafacto方法。这些方法提高了训练速度和稳定性。 |
| [^6] | [On the Generalization of Learned Structured Representations.](http://arxiv.org/abs/2304.13001) | 本论文研究了深度学习中学习结构化表示的泛化问题。通过提出能够学习局部和全局依赖关系的结构化表示方法，并在各种下游任务上进行实验验证，证明了该方法在图像分类和物体检测等任务上的有效性。 |
| [^7] | [Multi-Scale Feature Fusion using Parallel-Attention Block for COVID-19 Chest X-ray Diagnosis.](http://arxiv.org/abs/2304.12988) | 本研究提出了一种新的多特征融合网络，使用并行注意力块在不同尺度上融合原始 CXR 图像和局部相位增强 CXR 图像，从而实现 COVID-19 胸部 X 光图像的精确诊断，具有高准确性和鲁棒性。 |
| [^8] | [Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture.](http://arxiv.org/abs/2304.12985) | RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。 |
| [^9] | [GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters.](http://arxiv.org/abs/2304.12979) | 该论文介绍了GMU团队在SemEval-2023共享任务AfriSenti-SemEval中所使用的情感分析系统，使用AfroXLMR-large作为预训练语言模型并引入了增强的训练数据和基于Phylogeny的适配器调整以得到最佳结果。 |
| [^10] | [Towards Theoretical Understanding of Inverse Reinforcement Learning.](http://arxiv.org/abs/2304.12966) | 逆强化学习是一类用于恢复能够解释专家代理演示的行为的奖励函数的强大算法，然而，由于存在多个可解释观察行为的奖励函数歧义，采用可行奖励集的方式绕过这种限制。该论文在有限时间问题的IRL理论方面进行了探究，提出第一个样本复杂度的极小极大下界。 |
| [^11] | [Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning.](http://arxiv.org/abs/2304.12961) | 本论文提出了一种新的攻击方法"变色龙"，可以在联邦学习中实现更加耐用的后门攻击，通过对提供的良性图像和有毒图像目标标签之间的关系进行对比学习，取得了显著的实验效果。 |
| [^12] | [A Closer Look at Reward Decomposition for High-Level Robotic Explanations.](http://arxiv.org/abs/2304.12958) | 本论文提出了一种将奖励分解与抽象动作空间相结合的学习框架，可以提供基于对象属性的明确高层次解释，避免了解释机器人行为的复杂性。 |
| [^13] | [Neural Implicit Shape Editing using Boundary Sensitivity.](http://arxiv.org/abs/2304.12951) | 本文提出了一种基于边界敏感性的神经隐式形状编辑方法，该方法可以通过对参数进行扰动来实现指定的全局形状变形，并根据先验对其余部分进行调整。方法不受模型限制，并可优化和约束目标。 |
| [^14] | [Shot Optimization in Quantum Machine Learning Architectures to Accelerate Training.](http://arxiv.org/abs/2304.12950) | 本文提出了一种用于加速量子机器学习训练的Shot优化方法，通过减小数据集大小和自适应Shot分配等方法，实现了Shot数量的优化与准确度之间的平衡。 |
| [^15] | [eFAT: Improving the Effectiveness of Fault-Aware Training for Mitigating Permanent Faults in DNN Hardware Accelerators.](http://arxiv.org/abs/2304.12949) | 研究提出了一种新的框架eFAT，用于计算DNN加速器的韧性并将其映射到相应的重训练量，通过分组和融合故障地图减少FAT的开销，同时提高容错能力。 |
| [^16] | [Low-Latency Online Multiplier with Reduced Activities and Minimized Interconnect for Inner Product Arrays.](http://arxiv.org/abs/2304.12946) | 本文提出了一种基于在线算法的低延迟乘法器，它能通过位级流水线提高吞吐量并减少延迟，并适用于在资源有限的可重构设备上实施。 |
| [^17] | [Latent Traversals in Generative Models as Potential Flows.](http://arxiv.org/abs/2304.12944) | 该论文使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行，以实现解缠，并通过分类器进行约束。 |
| [^18] | [Generating robust counterfactual explanations.](http://arxiv.org/abs/2304.12943) | 本论文提出了一个新的反事实解释框架CROCO，能够生成强鲁棒性的解释，同时有效地平衡了鲁棒性和解释示例的接近度之间的权衡。 |
| [^19] | [Evolving Three Dimension (3D) Abstract Art: Fitting Concepts by Language.](http://arxiv.org/abs/2304.12932) | 本论文介绍了一种通过进化策略和3D渲染相结合的方法，使艺术家能够使用自然语言表达抽象三维艺术的创意想法。 |
| [^20] | [User-Centric Federated Learning: Trading off Wireless Resources for Personalization.](http://arxiv.org/abs/2304.12930) | 本文提出基于用户中心的联邦学习算法，使用可用的梯度信息和聚合规则，为每个联邦学习客户端生成个性化模型，同时保证隐私保护的传输。在基准数据集上表现出了更好的准确度和通信效率。 |
| [^21] | [Quantum Gaussian Process Regression for Bayesian Optimization.](http://arxiv.org/abs/2304.12923) | 本研究提出一种基于参数化的量子电路的量子核函数的高斯过程回归方法，将其应用于贝叶斯优化中，证明其可以保留变量信息并且可以作为贝叶斯优化的代理模型，实验结果证明此方法在实际数据集上的表现与经典方法相当。 |
| [^22] | [System Identification with Copula Entropy.](http://arxiv.org/abs/2304.12922) | 本文提出了一种使用Copula熵方法无模型超参数地对动态系统进行微分方程识别的方法，并在三维Lorenz系统的模拟实验中验证了该方法的有效性。 |
| [^23] | [Awesome-META+: Meta-Learning Research and Learning Platform.](http://arxiv.org/abs/2304.12921) | Awesome-META+是一个元学习框架集成和学习平台，旨在提供完整可靠的元学习框架应用和面向初学者的学习材料，进而促进元学习的发展并将其从小众领域转化为主流的研究方向。 |
| [^24] | [N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models.](http://arxiv.org/abs/2304.12918) | N2G是一种在大型语言模型中实现解释神经元的方法，它通过自动将神经元在数据集示例上的行为提炼为可解释的图形，并且可以输出文本上令牌的激活情况来进行自动验证。 |
| [^25] | [The Score-Difference Flow for Implicit Generative Modeling.](http://arxiv.org/abs/2304.12906) | 本文提出了一种新的评分差异流模型(SD flow)，它可以最优地减少两个分布之间的散度，同时解决Schr​​ödinger桥问题。与去噪扩散模型不同，它没有对先验分布施加任何限制，在一些基准数据集中优于其他方法。 |
| [^26] | [Discovering Graph Generation Algorithms.](http://arxiv.org/abs/2304.12895) | 本论文提出了一种使用图神经网络和进化搜索来发现图形生成算法的方法，相比于传统的概率模型或深度生成模型，其具有更高的训练分布外推潜力和直接解释性。可以与深度生成模型竞争，并且在某些情况下可以找到真正的图形生成过程。 |
| [^27] | [Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification.](http://arxiv.org/abs/2304.12891) | LDM模型可以用于降水预测，相较于GAN等其他模型，LDM不仅更稳定、需要更少的计算量，还能产生更多样化和更准确的降水预测，是对于需要考虑不确定性和预测多样性的决策-making 的相关应用非常有前景的。 |
| [^28] | [Provable benefits of general coverage conditions in efficient online RL with function approximation.](http://arxiv.org/abs/2304.12886) | 研究者对在线强化学习提出了一种新的一般覆盖条件，并发现更多的覆盖条件，提高了在线强化学习的样本效率和表现，同时阐明良好的覆盖条件仍然有益于获得最优解。 |
| [^29] | [Proximal Curriculum for Reinforcement Learning Agents.](http://arxiv.org/abs/2304.12877) | 本文基于近端发展区概念，提出了 ProCuRL 课程策略，用于设计深度强化学习智能体的课程，以加速训练过程，并在各种域上实验表明其优越性。 |
| [^30] | [Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection.](http://arxiv.org/abs/2304.12876) | 本研究首次成功地在32位Cortex-M微控制器上使用激光注入进行了比特翻转攻击，强调了典型深度神经网络的缺乏强健性。 |
| [^31] | [Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations.](http://arxiv.org/abs/2304.12875) | 提出了TnALE算法，通过交替局部枚举更新每个与结构相关的变量，大大减少了评估次数，用于解决张量网络结构搜索问题。在理论上证明，如果在每个邻域中达到了足够的目标函数降低，TnALE和TNLS都可以实现线性收敛度，直到一个常数。同时，与TNLS相比， TnALE需要更少的评估次数。 |
| [^32] | [Binary stochasticity enabled highly efficient neuromorphic deep learning achieves better-than-software accuracy.](http://arxiv.org/abs/2304.12866) | 二进制随机学习算法提高了深度学习效率，消除了使用噪声人工突触硬件系统的挑战性。 |
| [^33] | [Constraining Chaos: Enforcing dynamical invariants in the training of recurrent neural networks.](http://arxiv.org/abs/2304.12865) | 该论文提出了一种通过在感兴趣的系统中执行动力学不变量（如李雅普诺夫指数谱和分形维数）的方法来预测混沌动力学系统。通过使用递归神经网络结构，该技术可用于进行更长时间和更稳定的预测，适用于以Lorenz 1996混沌动力学系统和光谱拟准等位模型为代表的典型数值天气预报测试案例。 |
| [^34] | [What Causes Exceptions in Machine Learning Applications? Mining Machine Learning-Related Stack Traces on Stack Overflow.](http://arxiv.org/abs/2304.12857) | 研究者在Stack Overflow上挖掘了11,449个与七个常用Python ML库相关的堆栈跟踪，发现包含堆栈跟踪的ML问题更受欢迎，而它们的问题更复杂。 |
| [^35] | [Retinal Vessel Segmentation via a Multi-resolution Contextual Network and Adversarial Learning.](http://arxiv.org/abs/2304.12856) | 本文提出了一种多分辨率上下文网络和对抗性学习相结合的方法来进行准确的视网膜血管分割，该方法具有高分割性能和较少的可训练参数数量。 |
| [^36] | [Adaptive Services Function Chain Orchestration For Digital Health Twin Use Cases: Heuristic-boosted Q-Learning Approach.](http://arxiv.org/abs/2304.12853) | 本文提出了一种用于数字健康孪生系统的自适应服务功能链编排方法，能够根据不同数据共享场景提供安全策略，并考虑到预期的用例、策略和基础设施配置，动态地提供服务编排和路由配置。 |
| [^37] | [(Local) Differential Privacy has NO Disparate Impact on Fairness.](http://arxiv.org/abs/2304.12845) | 本文研究了在 LDP 下收集多个敏感属性对公平性的影响并提出了考虑域大小的新的隐私预算分配方案，实验表明该方案在隐私、效用和公平性方面均优于最新的解决方案，LDP 带来了略微改善的公平性而不会明显影响性能。 |
| [^38] | [A New Information Theory of Certainty for Machine Learning.](http://arxiv.org/abs/2304.12833) | 该论文提出了一种新的信息理论概念 troenpy 来量化底层分布的确定性，用于机器学习中文档分类和序列数据权重方案，并定义了量子 troenpy 量化量子系统确定性。 |
| [^39] | [Improving Robustness Against Adversarial Attacks with Deeply Quantized Neural Networks.](http://arxiv.org/abs/2304.12829) | 本文研究了在深度量化神经网络中采用自动量化训练框架和深度量化误差计算等方法来提高对抗攻击的鲁棒性。 |
| [^40] | [GraphVF: Controllable Protein-Specific 3D Molecule Generation with Variational Flow.](http://arxiv.org/abs/2304.12825) | GraphVF是一种新的分子生成框架，它将2D拓扑和3D几何结合起来，用于可控生成具有结合能力的3D分子，是第一个可以生成结合能力的3D分子并定制其物化特性的方法。 |
| [^41] | [Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning.](http://arxiv.org/abs/2304.12824) | 本文提出了一种基于对比能量预测的确切能量引导扩散采样方法，用于离线强化学习中，可以解决中间引导估计的难题，并在D4RL基准测试上取得了优异的效果。 |
| [^42] | [A Novel Dual of Shannon Information and Weighting Scheme.](http://arxiv.org/abs/2304.12814) | 本文通过发掘信息熵自然对偶，提出了一种新的量troenpy，并应用于提出了基于troenpy的文档加权方案，即正类别频率（PCF），以及一种新的类别信息偏差特征ECIB，在监督学习中具有互信息的泛化性质。 |
| [^43] | [Transcending the "Male Code": Implicit Masculine Biases in NLP Contexts.](http://arxiv.org/abs/2304.12810) | 研究发现，当存在性别化语言时，NLP语境中也存在着性别偏见，尤其是男性偏见。调查者提供了一个涵盖了性别化语言与语言之间歧义关系的新字典“Ava”。 |
| [^44] | [Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning.](http://arxiv.org/abs/2304.12778) | 本文提出了两种分布式强化学习方法，奖励加权和损失加权梯度合并，以更好地提高分布式代理的学习效果。 |
| [^45] | [Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network.](http://arxiv.org/abs/2304.12770) | 本文提出了一种基于反Lipschitz约束的解码器网络，可以简单明了地控制广泛的VAE模型的后验坍塌程度，并带有具体的理论保证。 |
| [^46] | [Decoupling Quantile Representations from Loss Functions.](http://arxiv.org/abs/2304.12766) | 本文提出了同时二元量化回归（SBQR）中分位数与预测概率的二元对偶性，使分位数表达式在不同tau值下的构造不再依赖于损失函数，从而在处理检测超出分布样本和调整模型方面表现突出。 |
| [^47] | [Node Feature Augmentation Vitaminizes Network Alignment.](http://arxiv.org/abs/2304.12751) | 本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。 |
| [^48] | [Deep learning based Auto Tuning for Database Management System.](http://arxiv.org/abs/2304.12747) | 本文扩展了一个基于Ottertune自动技术的方法，利用监督和无监督机器学习方法重用从先前会议中收集的训练数据来调整新的DBMS部署，以提高延迟预测的精度。 |
| [^49] | [NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients.](http://arxiv.org/abs/2304.12737) | 本文提出了一种基于夜间个人档案表示学习和深度学习框架的方法，可以提前预测创伤患者的脓毒症发作，这种方法优于现有的最先进方法。 |
| [^50] | [Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods.](http://arxiv.org/abs/2304.12729) | 本文提出了基于梯度提升机器学习方法和主成分分析的自动分类方法，用于解决星系外射电源形态分类问题。实验结果表明，在具有表格数据的分类问题中，该方法优于卷积神经网络。 |
| [^51] | [Eye tracking guided deep multiple instance learning with dual cross-attention for fundus disease detection.](http://arxiv.org/abs/2304.12719) | 本文提出了一种基于眼动追踪信息的人机协同CAD系统，在多实例学习中引入双重交叉关注MIL（DCAMIL）网络以提高诊断准确性，同时使用序列增强和域对抗模块增强方法的鲁棒性。 |
| [^52] | [Learning Robust Deep Equilibrium Models.](http://arxiv.org/abs/2304.12707) | 本论文提出一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性以抵抗微小的初始扰动，并在不同的固定点之间加入全连接层以避免不良对抗性防御。 |
| [^53] | [Deep Learning Framework for the Design of Orbital Angular Momentum Generators Enabled by Leaky-wave Holograms.](http://arxiv.org/abs/2304.12695) | 本文介绍了一种利用平面光学和机器学习技术相结合的泄漏波全息天线设计方法，可生成携带轨道角动量的电磁波，并使用机器学习发现数学函数，精确控制辐射模式，提高系统性能。 |
| [^54] | [Phylo2Vec: a vector representation for binary trees.](http://arxiv.org/abs/2304.12693) | Phylo2Vec是一种新的二叉树简明表示方法，它能够轻松采样二叉树，并以系统性的方法遍历树空间。这种方法用于构建深度神经网络，能够显著提高蛋白质类别预测的性能。 |
| [^55] | [Differential Privacy via Distributionally Robust Optimization.](http://arxiv.org/abs/2304.12681) | 本文开发了一类机制，以实现无条件最优性保证的差分隐私。该机制将机制设计问题制定为无限维分布鲁棒优化问题。 |
| [^56] | [Communication-Constrained Bandits under Additive Gaussian Noise.](http://arxiv.org/abs/2304.12680) | 本文研究了在受限通信和加性高斯噪声下的多臂赌博机问题，提出了一个多阶段赌博算法，并给出了信息理论下限。 |
| [^57] | [Compressing Sentence Representation with maximum Coding Rate Reduction.](http://arxiv.org/abs/2304.12674) | 提出了一种在最大编码速率降低下的句子表示压缩方法，通过在句子表示模型Sentence-BERT中加入一个额外的学习投影层，并证明其可以在语义相关任务中获得与大型语言模型相当的结果。 |
| [^58] | [CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis.](http://arxiv.org/abs/2304.12654) | CoDi 方法使用两个共同演化的对比扩散模型单独处理离散和连续变量并相互条件化，同时引入对比学习方法进行进一步的绑定，展现了在真实世界的表格数据集上的有效性。 |
| [^59] | [Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures.](http://arxiv.org/abs/2304.12622) | 本文针对计算机视觉中常用的剪枝神经网络方法，系统分析了其可能引发的偏差问题，提出了可以降低偏差的标准。 |
| [^60] | [A Bi-fidelity DeepONet Approach for Modeling Uncertain and Degrading Hysteretic Systems.](http://arxiv.org/abs/2304.12609) | 本文介绍了一种用于建模不确定和退化滞后系统的双保真DeepONet方法，在不了解退化效应性质的原始模型数据集的情况下使用低保真度表示，显著提高了退化滞后系统的预测精度。 |
| [^61] | [Performance Evaluation of Regression Models in Predicting the Cost of Medical Insurance.](http://arxiv.org/abs/2304.12605) | 该研究评估了三种回归模型在预测医疗保险费用方面的表现，提出了最重要的预测特征为被保险人的年龄。 |
| [^62] | [Is deep learning a useful tool for the pure mathematician?.](http://arxiv.org/abs/2304.12602) | 本文是一篇介绍纯数学家使用深度学习工具进行研究的个人和非正式叙述。 |
| [^63] | [Physics-Informed Representation Learning for Emergent Organization in Complex Dynamical Systems.](http://arxiv.org/abs/2304.12586) | 该论文提出了一个基于物理启示的自然组织表征学习框架，通过局部因果状态捕获复杂时空系统中的有序行为和相干结构。该方法在实际领域科学问题中具有很好的适用性。 |
| [^64] | [Learning imaging mechanism directly from optical microscopy observations.](http://arxiv.org/abs/2304.12584) | 本文提出了一种物理感知掩蔽自编码器（PiMAE）来直接从原始显微镜图像中学习估计点扩散函数（PSF）和发射源，并在合成数据和真实世界实验中展示了其准确性和噪声鲁棒性，超越了DeepSTORM和Richardson-Lucy算法。 |
| [^65] | [Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques.](http://arxiv.org/abs/2304.12583) | 本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。 |
| [^66] | [Learning Trajectories are Generalization Indicators.](http://arxiv.org/abs/2304.12579) | 本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。 |
| [^67] | [Fairness and Bias in Truth Discovery Algorithms: An Experimental Analysis.](http://arxiv.org/abs/2304.12573) | 本文研究了真相发现算法的偏差和公平性，发现存在敏感属性偏见和主观偏见。 |
| [^68] | [Performance Optimization using Multimodal Modeling and Heterogeneous GNN.](http://arxiv.org/abs/2304.12568) | 本研究提出了一种通用且高效的性能优化方法，使用基于IR编程模型的多模态深度学习方法进行特定任务的性能优化。我们提出了一种非常通用的调节并行代码区域的技术。 |
| [^69] | [Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks.](http://arxiv.org/abs/2304.12567) | 本文研究了如何通过增加辅助任务的数量和代理网络的大小，提高表示学习的效果。同时，本文还提出了基于后继度量的新型辅助任务家族 Proto-Value 网络。 |
| [^70] | [AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation.](http://arxiv.org/abs/2304.12566) | 本文提出了一种新的测试时间自适应（TTA）方法AdaNPC，通过利用非参数分类器进行建模从而避免了离线目标数据或在推理时使用额外的复杂优化过程。AdaNPC从存储器中回顾最相似的 K 个样本进行投票预测，逐渐改变存储器中的样本分布以提高测试域性能。 |
| [^71] | [Combining Adversaries with Anti-adversaries in Training.](http://arxiv.org/abs/2304.12550) | 该论文研究了在对抗训练中，通过结合对手和反对手(带有反对手扰动的样本)可以更有效地提高深度神经网络的公平性、鲁棒性和泛化性，在一些特定的学习场景中表现出更好的性能。 |
| [^72] | [COUPA: An Industrial Recommender System for Online to Offline Service Platforms.](http://arxiv.org/abs/2304.12549) | COUPA是一个面向O2O服务平台的工业级推荐系统，通过时间感知偏好和位置感知偏好来提高推荐效果。 |
| [^73] | [Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems.](http://arxiv.org/abs/2304.12541) | 本文提出了一种使用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法，该方法可以高效地进行抽样和准确的密度评估。研究通过残差项和独立性损失项确保了INN输出的统计独立性，并在多项实验中证明了其有效性和准确性。 |
| [^74] | [GARCIA: Powering Representations of Long-tail Query with Multi-granularity Contrastive Learning.](http://arxiv.org/abs/2304.12537) | GARCIA利用多粒度对比学习，基于图的知识转移和基于意图的表示，提高了长尾查询表示能力。 |
| [^75] | [Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM.](http://arxiv.org/abs/2304.12534) | 本研究提出了一种新算法RWSADMM，以解决在动态联邦学习中存在的数据不一致和通信成本高的问题，提高了可扩展性。 |
| [^76] | [Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models.](http://arxiv.org/abs/2304.12526) | Patch Diffusion 提出了一种通用的基于块的训练框架，可将扩散模型训练时间缩短至少一倍，并在更小的数据集上实现更好的性能表现。 |
| [^77] | [CIMLA: Interpretable AI for inference of differential causal networks.](http://arxiv.org/abs/2304.12523) | CIMLA是一个新工具，可以发现条件依赖的因果关系变化，它可以识别生物条件下基因调控网络的差异，是更加准确和鲁棒的方法。 |
| [^78] | [A New Inexact Proximal Linear Algorithm with Adaptive Stopping Criteria for Robust Phase Retrieval.](http://arxiv.org/abs/2304.12522) | 本文提出了一种新的鲁棒相位恢复算法，通过使用自适应停止准则的非精确近端线性算法，该方法在实验中证明比现有方法更高效。 |
| [^79] | [RenderDiffusion: Text Generation as Image Generation.](http://arxiv.org/abs/2304.12519) | 本文提出了一种新的扩散方法——\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。它将连续扩散模型应用于离散文本并实现了条件文本生成作为字形图像生成问题。 |
| [^80] | [Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning.](http://arxiv.org/abs/2304.12508) | 本文提出了 ASAP-Phi框架，利用分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励，使用基于演员-评论家的算法训练代理来尽快实现规范。 |
| [^81] | [CNN-Assisted Steganography -- Integrating Machine Learning with Established Steganographic Techniques.](http://arxiv.org/abs/2304.12503) | 本文提出了一种基于隐写辅助卷积神经网络的隐写方法，能够提高隐写分析的稳健性，并通过SA-CNN的自适应性能够进行核心隐写算法的自定义。 |
| [^82] | [Causal Semantic Communication for Digital Twins: A Generalizable Imitation Learning Approach.](http://arxiv.org/abs/2304.12502) | 本文提出了一种新的因果语义通信（CSC）框架，利用模仿学习（IL）方法，在数字孪生（DT）无线系统上利用AI技术和语义通信（SC），以提高通信效率、决策落地效果、推动数字孪生应用的普及和完善。 |
| [^83] | [The cross-sectional stock return predictions via quantum neural network and tensor network.](http://arxiv.org/abs/2304.12501) | 本文研究将量子神经网络和张量网络应用于股票收益预测，在日本股市中张量网络模型表现优于传统模型，并在最新市场环境下呈现出卓越表现。 |
| [^84] | [Design optimization for high-performance computing using FPGA.](http://arxiv.org/abs/2304.12474) | 本文中，作者通过优化Tensil AI的开源推理加速器并使用高级编译器策略，改进硬件设计并使用Xilinx Ultra RAM，表明优化FPGA可提高推理性能，为FPGA在高性能计算领域的应用提供了启示。 |
| [^85] | [A Study on Improving Realism of Synthetic Data for Machine Learning.](http://arxiv.org/abs/2304.12463) | 本文研究并评估了一个合成到真实的生成模型，将合成渲染转换为更真实的风格以适用于通用数据集，并通过下游感知任务来量化和定性地评估其性能。 |
| [^86] | [Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout.](http://arxiv.org/abs/2304.12458) | 本文研究了多智能体MDP中基于概率代理掉线的情况，并提出了一种无模型算法，能够消除掉线情况需要枚举计算的限制，从而实现计算后掉线系统的最优策略设计。 |
| [^87] | [Rank Flow Embedding for Unsupervised and Semi-Supervised Manifold Learning.](http://arxiv.org/abs/2304.12448) | 本文提出了Rank Flow Embedding算法用于无监督和半监督场景的流形学习。 |
| [^88] | [Predicting Pulmonary Hypertension by Electrocardiograms Using Machine Learning.](http://arxiv.org/abs/2304.12447) | 本研究旨在利用神经网络模型，通过处理心电图信号来检测肺动脉高压，以实现早期的监测和治疗。 |
| [^89] | [Stochastic MPC for energy hubs using data driven demand forecasting.](http://arxiv.org/abs/2304.12438) | 本文提出了一种基于数据驱动需求预测的随机MPC控制器，采用机会约束来最小化不确定的电力和热需求，使用历史数据构建预测模型，并通过情景法采样多步需求轨迹。在模拟和实际数据上验证了该控制器的性能。 |
| [^90] | [Generative Adversarial Neuroevolution for Control Behaviour Imitation.](http://arxiv.org/abs/2304.12432) | 本文研究了使用生成对抗神经进化技术进行行为模仿的方法，在8个OpenAI Gym状态控制任务上，最终的代理能够达到与预训练代理一样高的得分，展示了深度神经进化技术在模仿学习中的潜力。 |
| [^91] | [Neuroevolution of Recurrent Architectures on Control Tasks.](http://arxiv.org/abs/2304.12431) | 本文提出了一种新颖的方法，通过一小组变异规则动态演化递归神经网络的架构，实现了在强化学习控制任务上的神经演化。 |
| [^92] | [Sparse Private LASSO Logistic Regression.](http://arxiv.org/abs/2304.12429) | 本文提出了一种稀疏逻辑回归的差分隐私方法，保持硬零，通过训练一个非私有的LASSO逻辑回归模型决定最后的模型选择中恰当的私有化非零系数的数量。 |
| [^93] | [TIGTEC : Token Importance Guided TExt Counterfactuals.](http://arxiv.org/abs/2304.12425) | TIGTEC是一个高效的文本对抗事例生成方法，使用本地特征重要性针对和修改对分类结果影响最大的单词，并在代价函数中使用语义距离评估对抗性解释。该方法具有很高的成功率、稀疏性、多样性和可信度，并且既可以是特定于模型的，也可以是无关模型的，非常方便用于生成对抗性解释。 |
| [^94] | [Multi-Source to Multi-Target Decentralized Federated Domain Adaptation.](http://arxiv.org/abs/2304.12422) | 本文提出了一种分布式联邦学习方法，可将机器学习模型从标记数据丰富的设备转移到未标记数据设备以提高数据利用率。该方法考虑了设备分类和源-目标链接形成的权衡。 |
| [^95] | [Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls.](http://arxiv.org/abs/2304.12420) | 该论文使用了高效的样本集优化和基于代理的方法来设计水下航行器船体，其中代理模型显著提高了计算效率，使优化更加快速准确。 |
| [^96] | [A hybrid quantum-classical approach for inference on restricted Boltzmann machines.](http://arxiv.org/abs/2304.12418) | 混合量子-经典方法在受限玻尔兹曼机推断中取得了较好的表现，D-Wave量子退火器样本比随机初始化的样本具有更好的性能，量子退火器样本可以进一步改善Gibbs采样的性能。 |
| [^97] | [PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques.](http://arxiv.org/abs/2304.12410) | 本文提出了PEFT-Ref参考架构，标准化了不同PEFT技术共享的方面，隔离了差异到特定位置和交互中，模块化的视图有助于比较不同技术及其效率和任务性能，并有助于更好地理解PEFT的基本原理。 |
| [^98] | [Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization.](http://arxiv.org/abs/2304.12405) | 该论文提出了一种通过平方和优化，综合动态的、降阶的输出反馈多项式控制器的方法，应用于控制仿射的非线性系统，并使用学习的感知模块和视觉观测来稳定地运行到目标状态。该方法提供了在观测噪声存在的情况下的稳定性保证。 |
| [^99] | [HDCC: A Hyperdimensional Computing compiler for classification on embedded systems and high-performance computing.](http://arxiv.org/abs/2304.12398) | 本文介绍了\name{}编译器，可以将HDC分类方法的高级描述翻译为优化的C代码，使其更适用于在嵌入式系统和高性能计算中进行分类任务。 |
| [^100] | [Analyzing categorical time series with the R package ctsfeatures.](http://arxiv.org/abs/2304.12332) | 该软件包提供了一组有用的工具来分析分类时间序列，使得用户可以用图形描述底层时间模式并利用其输出执行传统的机器学习任务包括聚类、分类和异常检测。 |
| [^101] | [Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications.](http://arxiv.org/abs/2304.12330) | 本文提出了一种基于并行bootstrap的on-policy深度强化学习方法，通过部分轨迹缓冲区和返回bootstrapping步骤来实现灵活使用并行环境，同时保持更新的on-policy性，该方法在连续流控制问题上有很好的应用前景。 |
| [^102] | [Virus2Vec: Viral Sequence Classification Using Machine Learning.](http://arxiv.org/abs/2304.12328) | Virus2Vec 是一种机器学习方法，使用特征向量表示病毒序列，能够识别病毒宿主，其在病毒宿主预测上比当前最先进方法提高了多达16％的精度。 |
| [^103] | [Dependence of Physiochemical Features on Marine Chlorophyll Analysis with Learning Techniques.](http://arxiv.org/abs/2304.12325) | 研究发现，海洋中叶绿素的生长与物理化学成分如铁、硝酸盐、磷酸盐、pH值、盐度等的最佳浓度有关，可以用机器学习预测海洋叶绿素。 |
| [^104] | [Medical Image Deidentification, Cleaning and Compression Using Pylogik.](http://arxiv.org/abs/2304.12322) | 提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。 |
| [^105] | [Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness.](http://arxiv.org/abs/2304.12036) | 本文提出了一种解释Skip-gram节点嵌入的方法，即通过计算桥接度识别重要节点，并提出了一种新型基于梯度的解释方法GRAPH-wGD，有效地提供全局性解释。 |
| [^106] | [On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation.](http://arxiv.org/abs/2304.11328) | 本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。 |
| [^107] | [Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method.](http://arxiv.org/abs/2304.11171) | 本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。 |
| [^108] | [Reinforcement Learning Approaches for Traffic Signal Control under Missing Data.](http://arxiv.org/abs/2304.10722) | 本文提出了在交通路网中缺少传感器的情况下，使用强化学习方法通过补充流量状态或状态和动作来实现自适应控制和条件融合。 |
| [^109] | [A Theory on Adam Instability in Large-Scale Machine Learning.](http://arxiv.org/abs/2304.09871) | Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。 |
| [^110] | [From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation.](http://arxiv.org/abs/2304.08953) | 本文研究了符号音乐生成中的子词分词技术在生成更长、更具结构的音乐方面的有效性。实验结果表明BPE方法在符号音乐生成中可行且有效。 |
| [^111] | [RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks.](http://arxiv.org/abs/2304.03973) | 本文评估了CapsNet在仿射变换和对抗性攻击方面的鲁棒性。在MNIST，GTSRB和CIFAR10数据集上的测试结果显示，与传统CNN相比，CapsNet实现了更好的对抗示例和仿射变换鲁棒性。 |
| [^112] | [Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher.](http://arxiv.org/abs/2304.01731) | 本文提出了有选择的联邦蒸馏机制Selective-FD，可以精确地识别来自本地和集合预测的知识，以解决局部数据分布的变异和缺乏好的教师模型而导致的误导和模糊的知识共享问题，并取得了显著提高的模型性能和准确度。 |
| [^113] | [Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis.](http://arxiv.org/abs/2303.14157) | 提出了一种高效尺度不变的生成模型，不使用空间卷积或分层架构，通过将层内特征图分解为局部和全局特征并通过插值这些特征生成图像，使得模型具有较小的内存占用和更快的推理速度。 |
| [^114] | [Composite Optimization Algorithms for Sigmoid Networks.](http://arxiv.org/abs/2303.00589) | 本文使用复合优化算法解决Sigmoid网络问题，可以在非凸和非光滑问题的情况下收敛到全局最优解，并提供了一般指导来设置网络大小。 |
| [^115] | [Simplifying Momentum-based Riemannian Submanifold Optimization.](http://arxiv.org/abs/2302.09738) | 本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。 |
| [^116] | [Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load.](http://arxiv.org/abs/2302.03616) | 通过机器学习技术和智能手表数据进行认知负荷估计，研究发现游戏化的自我报告方式与传统方式的认知负荷没有差异，但参与者更喜欢游戏化版本。 |
| [^117] | [Learning Discretized Neural Networks under Ricci Flow.](http://arxiv.org/abs/2302.03390) | 本文使用信息几何构造了线性几乎欧几里得流形，通过引入偏微分方程Ricci流，解决了离散神经网络训练中梯度无穷或零的问题。 |
| [^118] | [Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes.](http://arxiv.org/abs/2302.02809) | 本文提出了一种交互式的物质感知双耳音频传播方法，能够生成真实环境下的渲染音频，利用图神经网络和条件生成对抗网络，处理重构三维模型中的缺陷，并且能够精确生成与真实环境相符的声学输出。 |
| [^119] | [Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control.](http://arxiv.org/abs/2302.00839) | 本文提出了一种通用管道（FavMac），可以最大化价值并控制成本。 FavMac可以与几乎任何多标签分类器相结合，并通过在线更新机制处理实际的大规模应用，为成本控制提供无分布理论担保。 |
| [^120] | [The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest.](http://arxiv.org/abs/2301.12987) | 本研究表明，在构建假设的过程中，选择最短的假设不如选择最弱的假设，而弱点是比长度或简单性更好的推广表现代理。 |
| [^121] | [Rigid body flows for sampling molecular crystal structures.](http://arxiv.org/abs/2301.11355) | 本文介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计。通过在单位四元数群上定义平滑和表现力强的流以及定义适当的密度，在旋转群上进行训练，我们可以成功地采样分子晶体结构。 |
| [^122] | [RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving.](http://arxiv.org/abs/2301.10222) | 本文旨在探究视觉Transformer是否可以应用于自动驾驶中的3D语义分割中，通过保留与RGB图像相同的骨干结构，这项工作证明了ViTs在结合投影方法，大数据训练和具有噪声鲁棒性的新损失函数后可以取得最先进的结果。 |
| [^123] | [Invariant Lipschitz Bandits: A Side Observation Approach.](http://arxiv.org/abs/2212.07524) | 本文研究了不变Lipschitz赌徒设置，并提出了一种名为\texttt{UniformMesh-N}的算法。使用侧面观察的方法，证明了改进的遗憾上界。 |
| [^124] | [GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation.](http://arxiv.org/abs/2212.06795) | 本文提出了一种高分辨率非分层视觉Transformer模型GPViT，使用具有高效率的组传播模块实现了特征之间全局信息的交流，在多种视觉识别任务上获得了竞争性或最先进的性能。 |
| [^125] | [VISEM-Tracking, a human spermatozoa tracking dataset.](http://arxiv.org/abs/2212.02842) | 本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。 |
| [^126] | [KGML-xDTD: A Knowledge Graph-based Machine Learning Framework for Drug Treatment Prediction and Mechanism Description.](http://arxiv.org/abs/2212.01384) | KGML-xDTD是一个用于药物治疗预测和机制解释的知识图谱机器学习框架，通过预测和解释药物和疾病之间的机制来实现药物再利用。 |
| [^127] | [xTrimoABFold: De novo Antibody Structure Prediction without MSA.](http://arxiv.org/abs/2212.00735) | xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。 |
| [^128] | [AIO-P: Expanding Neural Performance Predictors Beyond Image Classification.](http://arxiv.org/abs/2211.17228) | 本文提出了一种新颖的All-in-One Predictor（AIO-P），旨在预训练神经预测器，使用来自多个独立计算机视觉（CV）任务域和多个架构空间的架构示例，并转移到未使用的下游CV任务或神经架构上。 |
| [^129] | [GENNAPE: Towards Generalized Neural Architecture Performance Estimators.](http://arxiv.org/abs/2211.17226) | 本文提出了一个名为GENNAPE的通用神经架构性能估计器，可以泛化到完全未知的架构中，通过创新的网络表示、对比预训练和基于模糊聚类的预测器集成方法实现。 |
| [^130] | [On the Ability of Graph Neural Networks to Model Interactions Between Vertices.](http://arxiv.org/abs/2211.16494) | 本文研究了GNN模拟顶点间相互作用的能力，通过一个被称为分离秩的度量标准来量化这种能力，结果表明模拟相互作用的能力主要取决于分区的行走指数，即从分界线开始的行走数量，同时设计了一种名为WISA的边稀疏化算法以提高GNNs的处理效率和表达能力。 |
| [^131] | [GREAD: Graph Neural Reaction-Diffusion Networks.](http://arxiv.org/abs/2211.14208) | 本文介绍了一种基于反应扩散方程的GNN方法，考虑了所有流行的反应方程类型和一种特殊的反应方程，是目前其中最全面的研究之一，并在实验中表现出更好的性能。 |
| [^132] | [Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version).](http://arxiv.org/abs/2211.11434) | 该研究提出了通过差分隐私保护COVID-19检测模型，解决数据分析和患者隐私保护的问题。通过黑盒成员推理攻击，实现了对实际隐私的评估，结论表明所需的隐私等级可能因受到实际威胁的任务而异。 |
| [^133] | [Domain-Adaptive Self-Supervised Pre-Training for Face & Body Detection in Drawings.](http://arxiv.org/abs/2211.10641) | 本文展示了如何通过自监督学习构建人脸和身体检测器，同时利用大量未标注的数据。其中利用样式转移整合跨域标注图像来引导检测器。 |
| [^134] | [MMD-B-Fair: Learning Fair Representations with Statistical Testing.](http://arxiv.org/abs/2211.07907) | 提出了一种基于统计检验的 MMD-B-Fair 方法，用于学习公平的数据表示，并在各种数据集上得到了验证。 |
| [^135] | [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models.](http://arxiv.org/abs/2211.05105) | 该论文提出了一种名为安全潜向扩散的方法，可以在图像生成过程中移除和抑制不当的图像部分，从而缓解基于文本的图像生成模型因不当数据集带来的不良影响。 |
| [^136] | [StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects.](http://arxiv.org/abs/2211.04604) | 本论文提出了一种结合扩散模型和以物体为中心的转换器的方法，通过高层语言目标和局部视点云构建物理有效的结构，该方法可用于多个具有挑战性的多步骤3D规划任务，即使使用未知对象仍能提高成功率。 |
| [^137] | [EEG-Fest: Few-shot based Attention Network for Driver's Vigilance Estimation with EEG Signals.](http://arxiv.org/abs/2211.03878) | 该论文提出了一种少样本模型EEG-Fest，它能够通过少量的样本对司机困意状态进行监测，识别异常信号并且实现主观独立分类，且在数据集上达到了最新最佳的结果。 |
| [^138] | [Metricizing the Euclidean Space towards Desired Distance Relations in Point Clouds.](http://arxiv.org/abs/2211.03674) | 该论文提出了一种解决如何构造拓扑度量来计算给定距离值的问题的方法，涉及到欧几里得空间中多个点之间的最近邻查询问题，其应用于平衡kd树的查询时间复杂度为$O(\log^2n)$。 |
| [^139] | [Data Level Lottery Ticket Hypothesis for Vision Transformers.](http://arxiv.org/abs/2211.01484) | 本文将传统的彩票假说（LTH）扩展到由图像补丁组成的输入数据中，证明存在一个子集的输入图像补丁使得可以从头开始训练视觉Transformer（ViT），并且达到与使用所有图像补丁训练的ViTs相似的精度，这种方法在各种视觉任务中都是可行的和有效的。 |
| [^140] | [Masked Autoencoders Are Articulatory Learners.](http://arxiv.org/abs/2210.15195) | 本文提出了基于深度学习的Masked Autoencoders方法，可以精确重构被追踪错误的口腔学记录，有效地应用于XRMB数据集研究中。 |
| [^141] | [Imputation of missing values in multi-view data.](http://arxiv.org/abs/2210.14484) | 本文提出了一种基于StaPLR算法的新的多视角数据插补算法，通过在降维空间中执行插补以解决计算挑战，并在模拟数据集中得到了竞争性结果。 |
| [^142] | [Knowledge-Enhanced Relation Extraction Dataset.](http://arxiv.org/abs/2210.11231) | 该论文介绍了一个新的知识增强关系抽取数据集（KERED），它为每个句子注释一个关系事实，并通过实体链接提供实体的知识背景。利用这个数据集，作者比较了当代关系抽取方法在句子级和包级两种任务设置下的表现。实验结果表明，KERED提供的知识图谱可以支持知识增强的关系抽取方法。 |
| [^143] | [Optimization on Manifolds via Graph Gaussian Processes.](http://arxiv.org/abs/2210.10962) | 本论文结合流形学习和高斯过程，利用流形样本点云定义图高斯过程代理模型，通过选择查询点在流形上优化目标函数，具有良好性能。 |
| [^144] | [Code Recommendation for Open Source Software Developers.](http://arxiv.org/abs/2210.08332) | CODER 是一个基于图的代码推荐框架，通过建模微观用户-代码交互和宏观用户-项目交互，预测开发者未来的贡献行为，以缩短开发时间并提高开发效率。 |
| [^145] | [Sequential Attention for Feature Selection.](http://arxiv.org/abs/2209.14881) | 在神经网络中，我们提出一种名为序列关注的特征选择算法，它在每个步骤使用注意力权重作为特征重要性的代理，实现了最新的实证结果。 |
| [^146] | [Augmenting Interpretable Models with LLMs during Training.](http://arxiv.org/abs/2209.11799) | 本文提出了 Aug-imodels 框架，利用 LLMs 的知识在拟合过程中构建高效且可解释的模型，在推理过程中不使用 LLMs，具备完全的透明性。研究探讨了两种不同方式的实现，并在多种文本分类数据集中表现出优异的效果。 |
| [^147] | [Artificial Intelligence in Material Engineering: A review on applications of AI in Material Engineering.](http://arxiv.org/abs/2209.11234) | 本文综述了人工智能在材料工程中的最新进展，涉及材料加工、结构和材料性能研究、测量材料性能、新材料的创建和设计以及未来机遇等方面，基于机器学习的方法比传统方法更快、更准确，生成对抗网络有助于无机材料的化学成分预测和优化。 |
| [^148] | [Inverted Landing in a Small Aerial Robot via Deep Reinforcement Learning for Triggering and Control of Rotational Maneuvers.](http://arxiv.org/abs/2209.11043) | 本研究利用深度强化学习和基于物理学的仿真技术，获得一种通用的最优控制策略，可以使小型空中机器人从任意状态开始，实现倒立降落触发和旋转机动控制。通过将所学习的策略转移至实体机器人，成功实现高可靠性的倒立降落，即使在受到风扰动的情况下。 |
| [^149] | [Weak-signal extraction enabled by deep-neural-network denoising of diffraction data.](http://arxiv.org/abs/2209.09247) | 本研究展示了如何通过深度卷积神经网络对数据进行去噪，使衰弱的信号出现具有量化准确性的情况，并成功应用于晶体材料的X射线衍射数据中。 |
| [^150] | [Model Predictive Robustness of Signal Temporal Logic Predicates.](http://arxiv.org/abs/2209.07881) | 本论文提出了一种新的模型预测鲁棒性的概念来更系统地评估信号时间逻辑中底层谓词的鲁棒性，并成功验证了该方法在自动驾驶中的应用。 |
| [^151] | [Fed-FSNet: Mitigating Non-I.I.D. Federated Learning via Fuzzy Synthesizing Network.](http://arxiv.org/abs/2208.12044) | 本文提出一种名为Fed-FSNet的新型联邦学习训练框架，通过使用模糊合成网络（FSNet）在源头处缓解非独立同分布问题，从而提高全局模型的性能表现。 |
| [^152] | [Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge.](http://arxiv.org/abs/2208.10300) | 该论文提出了一种利用偏好学习离线学习效用函数的方法，以应对真实世界问题中用专家知识定义效用函数困难且与专家反复互动昂贵的问题。使用效用函数空间的粗略信息，能够在使用很少结果时提高效用函数估计，并通过整个优化链中传递效用函数学习任务中出现的不确定性。 |
| [^153] | [Isoform Function Prediction Using a Deep Neural Network.](http://arxiv.org/abs/2208.03325) | 本文介绍了一种使用深度神经网络预测同一基因位点上不同mRNA功能的方法，解决了缺乏标记训练数据的问题。 |
| [^154] | [A Singular Woodbury and Pseudo-Determinant Matrix Identities and Application to Gaussian Process Regression.](http://arxiv.org/abs/2207.08038) | 研究了一种源自于奇异形式的Woodbury矩阵，提出了逆矩阵和伪行列式的广义恒等式，并将精密矩阵的定义扩展到协方差矩阵的Bott-Duffin逆，同时提供了有效的算法并演示出其在计算高斯过程回归的似然函数中的优势。 |
| [^155] | [Sound Randomized Smoothing in Floating-Point Arithmetics.](http://arxiv.org/abs/2207.07209) | 该论文提出了浮点算术下的声音随机平滑，证明了无限精度下的随机平滑不再可靠，只需使用一个公平的硬币即可，这项技术可以与标准分类器的证书匹配。 |
| [^156] | [Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks.](http://arxiv.org/abs/2207.01110) | 本文通过对噪声时间序列的建模，考察了两种GAN模型用于数据驱动建模的可行性，实验结果表明它们在各类噪声的复制方面具有一定的效果。 |
| [^157] | [BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space.](http://arxiv.org/abs/2206.10747) | BiometricBlender是一个高维度多类合成数据生成工具，可以模拟真实生物特征数据集的关键属性，有助于在特征筛选领域进行快速发展。 |
| [^158] | [Masked Frequency Modeling for Self-Supervised Visual Pre-Training.](http://arxiv.org/abs/2206.07706) | 本文提出了掩膜频率建模（MFM）方法，用于自监督视觉预训练，通过对高低频成分进行掩膜预测，可以更好地揭示底层图像模式，并在ViT和CNN模型中实现很好的表现。 |
| [^159] | [Sparse Subspace Clustering in Diverse Multiplex Network Model.](http://arxiv.org/abs/2206.07602) | 本文研究了DIMPLE网络模型中的稀疏子空间聚类，通过识别具有相同社区结构的层组，找到了一种强一致性的结果。 |
| [^160] | [Targeted Adaptive Design.](http://arxiv.org/abs/2205.14208) | TAD是一种新的目标自适应设计算法，可以通过高斯过程代理模型在指定公差中确定产生期望设计特征的最佳控制设置，相比其他自适应设计算法，具有更高的精度和效率。 |
| [^161] | [Evolving Pareto-Optimal Actor-Critic Algorithms for Generalizability and Stability.](http://arxiv.org/abs/2204.04292) | 本文提出了MetaPG，一种自动设计演员-评论家损失函数的进化方法，该方法明确地优化泛化性和性能，并隐式地优化这两个指标的稳定性。 |
| [^162] | [Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed.](http://arxiv.org/abs/2203.09179) | 本文指出了高斯过程回归中，使用最大似然估计器估计长度尺度参数的平稳协方差函数的情况下，在无噪声数据情况下会存在不适定问题。 |
| [^163] | [A Practical AoI Scheduler in IoT Networks with Relays.](http://arxiv.org/abs/2203.04227) | 本论文提出了一种针对带中继的IoT网络的实用AoI调度器，可考虑常见的动态信道条件和未知数据包生成模式。经过广泛的模拟，证明了所提出的调度器在平均AoI、数据包丢失率和能量消耗方面优于现有的最先进的AoI调度器。 |
| [^164] | [Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph.](http://arxiv.org/abs/2203.01821) | 提出了一种基于循环图神经网络与注意力机制的机器人导航方法，可以在复杂的人群场景中实现安全、高效和不干扰的导航，通过预测未来轨迹推断出动态代理的意图，避免侵入他人预期路径。 |
| [^165] | [Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning.](http://arxiv.org/abs/2201.08115) | 本文介绍了一种名为“APEx”的技能发现和转移算法，它以一种原则性和学习的方式利用信息不对称性，实现了最先进的采样效率、任务可转移性和概括能力，同时保留了非分层方法的表达能力和灵活性。 |
| [^166] | [A Simplicity Bubble Problem in Formal-Theoretic Learning Systems.](http://arxiv.org/abs/2112.12275) | 这篇论文证明了在机器学习中，对于每个算法都存在一个足够大的数据集大小，超过该大小，无法预测欺骗者的算法概率上界是该算法的算法概率上界。 |
| [^167] | [Semi-Decentralized Federated Edge Learning with Data and Device Heterogeneity.](http://arxiv.org/abs/2112.10313) | 本文提出了一种半分散联邦边缘学习（SD-FEEL）框架，采用多个边缘服务器协调大量客户端节点进行深度学习模型的训练，有效利用边缘计算资源以增加训练数据量和降低延迟。 |
| [^168] | [Adaptive Weighted Multi-View Clustering.](http://arxiv.org/abs/2110.13240) | 本文提出了一种加权多视图 NMF算法，旨在学习视图特定权重和观测特定重构权重，以量化每个视图的信息内容，通过分配较小和较大的权重来扩大重要视图的正面影响。 |
| [^169] | [Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation.](http://arxiv.org/abs/2107.06011) | 本论文介绍了在多对象导航中引入辅助任务的补充监督形式，以促进为实现目标导向下游任务而训练的智能体中出现空间感知能力的产生。 |
| [^170] | [Approximate Regions of Attraction in Learning with Decision-Dependent Distributions.](http://arxiv.org/abs/2107.00055) | 本文分析了在决策相关分布学习中的风险最小化，建立在执行风险最小化的梯度流的扰动轨迹之上，研究了可能存在多个局部最小化器的情况。 |
| [^171] | [HUMAP: Hierarchical Uniform Manifold Approximation and Projection.](http://arxiv.org/abs/2106.07718) | HUMAP是一种新的层次降维技术，能够在层次探索中保留心理地图并在多个数据集和数据类型上具有优越性。 |
| [^172] | [Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning.](http://arxiv.org/abs/2106.04911) | 本文提出了一种基于存储的优化方法，用于MAML算法，可以在处理少量任务的情况下实现持续学习，同时提出一种通信有效的基于存储的个性化联邦MAML算法，并在各种基准数据集上验证了所提出的算法的有效性。 |
| [^173] | [Solving relaxations of MAP-MRF problems: Combinatorial in-face Frank-Wolfe directions.](http://arxiv.org/abs/2010.09567) | 该论文提出了一种有效的方法来解决LP松弛的MAP-MRF推理问题，使用组合内侧Frank-Wolfe方向，该方法在一些问题类中是当前最先进的LP求解器。 |
| [^174] | [Hierarchical community structure in networks.](http://arxiv.org/abs/2009.07196) | 本文在网络中研究了分层社区结构，探讨了定义层次结构、确定证据和有效检测分层结构的方法。 |
| [^175] | [Moderately Supervised Learning: Definition, Framework and Generality.](http://arxiv.org/abs/2008.11945) | 适度监督学习包括标准有监督学习和弱监督学习，已有的一些标准有监督学习任务表明，给定的标签并不容易学习，从给定的标签转换为易于学习的目标的过程会显著影响最终的性能，有监督学习的定义隐藏了可能对解决方案影响重大的细节。 |
| [^176] | [Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective.](http://arxiv.org/abs/1903.11406) | 本文提出了一种多嵌入交互的角度来分析和比较知识图谱嵌入方法，研究了CP，DistMult和ComplEx等三种流行的方法，揭示了ComplEx性能更好的原因，并为开发新模型提供了见解。 |
| [^177] | [Ensemble Sampling.](http://arxiv.org/abs/1705.07347) | 本文介绍了集成采样，以近似Thompson采样并在复杂模型下保持可行性，将大大扩展Thompson采样的应用范围。 |

# 详细

[^1]: Bake Off重访：对最近时间序列分类算法的评述和实验评估

    Bake off redux: a review and experimental evaluation of recent time series classification algorithms. (arXiv:2304.13029v1 [cs.LG])

    [http://arxiv.org/abs/2304.13029](http://arxiv.org/abs/2304.13029)

    本文重访烘焙大赛，评估了最近时间序列分类算法在112个数据集上的表现。该论文通过分类法将这些算法分为五类，为TSC领域的发展提供了贡献。

    

    一篇研究论文在2017年比较了18个时间序列分类（TSC）算法在来自加州大学河滨分校（UCR）存档的85个数据集上的表现。这项研究通常被称为“烘焙比赛”，发现只有9个算法的表现显著优于使用的动态时间规整（DTW）和旋转森林基准。该研究通过从时间序列数据中提取特征的算法类型对每个算法进行分类，形成了五种主要算法类型的分类法。与可以重现结果的代码和结果的提供相结合，这些算法的分类和可访问的结果推动了TSC领域的普及。六年过去了，UCR存档已扩展到112个数据集，并且已经提出了大量新的算法。我们重访烘焙大赛，看看每个提出的类别自原始出版以来的进展，并评估新算法的性能。

    In 2017, a research paper compared 18 Time Series Classification (TSC) algorithms on 85 datasets from the University of California, Riverside (UCR) archive. This study, commonly referred to as a `bake off', identified that only nine algorithms performed significantly better than the Dynamic Time Warping (DTW) and Rotation Forest benchmarks that were used. The study categorised each algorithm by the type of feature they extract from time series data, forming a taxonomy of five main algorithm types. This categorisation of algorithms alongside the provision of code and accessible results for reproducibility has helped fuel an increase in popularity of the TSC field. Over six years have passed since this bake off, the UCR archive has expanded to 112 datasets and there have been a large number of new algorithms proposed. We revisit the bake off, seeing how each of the proposed categories have advanced since the original publication, and evaluate the performance of newer algorithms against t
    
[^2]: 认证集成：一种具有S-Lipschitz特性的通用认证理论

    Certifying Ensembles: A General Certification Theory with S-Lipschitzness. (arXiv:2304.13019v1 [cs.LG])

    [http://arxiv.org/abs/2304.13019](http://arxiv.org/abs/2304.13019)

    本文研究了深度学习模型集成的认证鲁棒性问题，提出了S-Lipschitz分类器概念并给出了集成理论鲁棒性的准确条件。

    

    提高和保证深度学习模型的鲁棒性一直是一个研究的热点。集成是将几个分类器组合以提供更好模型的方法。它已经证明对泛化、不确定性评估、校准以及缓解概念漂移的影响是有益的。然而，对认证鲁棒性的集成影响认识不足。在这项工作中，我们通过引入S-Lipschitz分类器来推广Lipschitz连续性，用于分析集成理论鲁棒性。我们的结果是精确条件，在这些条件下，鲁棒分类器的集成比任何组成部分分类器更可靠，同时在什么条件下它们不是鲁棒的。

    Improving and guaranteeing the robustness of deep learning models has been a topic of intense research. Ensembling, which combines several classifiers to provide a better model, has shown to be beneficial for generalisation, uncertainty estimation, calibration, and mitigating the effects of concept drift. However, the impact of ensembling on certified robustness is less well understood. In this work, we generalise Lipschitz continuity by introducing S-Lipschitz classifiers, which we use to analyse the theoretical robustness of ensembles. Our results are precise conditions when ensembles of robust classifiers are more robust than any constituent classifier, as well as conditions when they are less robust.
    
[^3]: DuETT: 双重事件时间变换器用于电子病历

    DuETT: Dual Event Time Transformer for Electronic Health Records. (arXiv:2304.13017v1 [cs.LG])

    [http://arxiv.org/abs/2304.13017](http://arxiv.org/abs/2304.13017)

    DuETT是一个用于EHR的双重事件时间变换器，通过双重注意机制学习不同上下文中相同时间步的不同表示，避免由于时间步伐大而产生的二次放缩问题，并在四个基准EHR数据集上优于以往的单一任务最先进方法。

    

    医院设置中记录的电子病历（EHR）通常包含广泛的数字时间序列数据，其特征是高稀疏性和不规则观察。这些数据的有效建模必须利用其时间序列特性，不同类型观察之间的语义关系以及数据中稀疏性结构中的信息。自监督变压器在NLP和计算机视觉中的各种结构化任务中表现出了出色的性能。但是，多元时间序列数据包含两个维度上的结构化关系：时间和记录的事件类型，而直接将变压器应用于时间序列数据则不能利用这种独特的结构。自我注意力层的二次放缩还可以显着限制输入序列长度，而没有适当的输入工程。我们引入了DuETT架构，这是变压器的扩展，设计用于在EHR的时间和事件类型维度上关注。DuETT使用双重注意机制，它可以在不同上下文中学习相同时间步的不同表示。通过引入具有内核方法的自我注意力，DuETT避免了自我注意力的二次放缩，并过滤掉不相关的时间步骤。DuETT在四个基准EHR数据集上优于以前的单一任务最先进方法。

    Electronic health records (EHRs) recorded in hospital settings typically contain a wide range of numeric time series data that is characterized by high sparsity and irregular observations. Effective modelling for such data must exploit its time series nature, the semantic relationship between different types of observations, and information in the sparsity structure of the data. Self-supervised Transformers have shown outstanding performance in a variety of structured tasks in NLP and computer vision. But multivariate time series data contains structured relationships over two dimensions: time and recorded event type, and straightforward applications of Transformers to time series data do not leverage this distinct structure. The quadratic scaling of self-attention layers can also significantly limit the input sequence length without appropriate input engineering. We introduce the DuETT architecture, an extension of Transformers designed to attend over both time and event type dimensio
    
[^4]: 子采样岭回归集成：等效性和广义交叉验证

    Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation. (arXiv:2304.13016v1 [math.ST])

    [http://arxiv.org/abs/2304.13016](http://arxiv.org/abs/2304.13016)

    研究了比例渐近情形下的子采样岭回归集成，证明了最优全岭回归集成的风险与最优岭预测器的风险相匹配，并证明了GCV在估计岭回归集合的预测风险方面的强一致性。

    

    我们研究了比例渐近情形下的子采样岭回归集成，其中特征大小与样本大小成比例增长，使得它们的比率收敛到一个常数。通过分析岭回归集合的平方预测风险作为显式惩罚$\lambda$和极限子样本方面比$\phi_s$（特征大小与子样本大小的比率）的函数，我们表征了在任何可达风险下的$(\lambda, \phi_s)$-平面上的轮廓。因此，我们证明最优全岭回归集成（适合于所有可能的子样本）的风险与最优岭预测器的风险相匹配。此外，我们证明对于估计岭回归集合的预测风险，基于广义交叉验证（GCV）的子样本大小强一致性。这允许无需样本拆分基于GCV优化全局岭回归集成，并产生一个风险与最优岭回归风险相匹配的预测器。

    We study subsampling-based ridge ensembles in the proportional asymptotics regime, where the feature size grows proportionally with the sample size such that their ratio converges to a constant. By analyzing the squared prediction risk of ridge ensembles as a function of the explicit penalty $\lambda$ and the limiting subsample aspect ratio $\phi_s$ (the ratio of the feature size to the subsample size), we characterize contours in the $(\lambda, \phi_s)$-plane at any achievable risk. As a consequence, we prove that the risk of the optimal full ridgeless ensemble (fitted on all possible subsamples) matches that of the optimal ridge predictor. In addition, we prove strong uniform consistency of generalized cross-validation (GCV) over the subsample sizes for estimating the prediction risk of ridge ensembles. This allows for GCV-based tuning of full ridgeless ensembles without sample splitting and yields a predictor whose risk matches optimal ridge risk.
    
[^5]: 大规模视觉语言模型的稳定和低精度训练

    Stable and low-precision training for large-scale vision-language models. (arXiv:2304.13013v1 [cs.LG])

    [http://arxiv.org/abs/2304.13013](http://arxiv.org/abs/2304.13013)

    该研究介绍了用于大规模视觉语言模型稳定和低精度训练的新方法，包括SwitchBack和AdamW-Adafacto方法。这些方法提高了训练速度和稳定性。

    

    我们介绍了新的方法，用于加速和稳定大语言-视觉模型的训练。为加速训练，我们引入了SwitchBack，这是一种线性层用于int8量化训练，其提供了13-25％的速度提升，而与1B参数CLIP ViT-Huge的bfloat16训练的性能相匹配，在目前为止是最大的int8训练。我们的重点是int8，因为GPU支持float8很少，虽然我们也通过模拟分析了float8训练。为了稳定训练，我们分析了损失峰值，并发现它们在二次梯度估计器的AdamW second moment之后1-8次迭代中一致发生低估。因此，我们推荐使用AdamW-Adafacto方法。

    We introduce new methods for 1) accelerating and 2) stabilizing training for large language-vision models. 1) Towards accelerating training, we introduce SwitchBack, a linear layer for int8 quantized training which provides a speed-up of 13-25% while matching the performance of bfloat16 training within 0.1 percentage points for the 1B parameter CLIP ViT-Huge -- the largest int8 training to date. Our main focus is int8 as GPU support for float8 is rare, though we also analyze float8 training through simulation. While SwitchBack proves effective for float8, we show that standard techniques are also successful if the network is trained and initialized so that large feature magnitudes are discouraged, which we accomplish via layer-scale initialized with zeros. 2) Towards stable training, we analyze loss spikes and find they consistently occur 1-8 iterations after the squared gradients become under-estimated by their AdamW second moment estimator. As a result, we recommend an AdamW-Adafacto
    
[^6]: 深度学习中学习结构化表示的泛化问题研究

    On the Generalization of Learned Structured Representations. (arXiv:2304.13001v1 [cs.LG])

    [http://arxiv.org/abs/2304.13001](http://arxiv.org/abs/2304.13001)

    本论文研究了深度学习中学习结构化表示的泛化问题。通过提出能够学习局部和全局依赖关系的结构化表示方法，并在各种下游任务上进行实验验证，证明了该方法在图像分类和物体检测等任务上的有效性。

    

    尽管在过去的十年中取得了巨大的进展，但深度学习方法通常无法达到人类级别的系统泛化水平。人们认为明确捕获数据的基础结构应该能够让联结主义系统以更可预测和系统的方式进行泛化。事实上，人类的证据表明，用符号般的组合实体来解释世界可能对智能行为和高级推理至关重要。另一个深度学习系统的常见限制是它们需要大量的训练数据，这可能很昂贵。在表示学习中，利用大型数据集学习通用数据表示，这些表示可以用于有效地学习任意的下游任务。本论文研究了结构化表示学习。我们研究了学习未结构化数据的结构化表示的方法，这些方法需要很少或没有监督，并捕获其隐藏结构。在论文的第一部分中，我们提出了一些新的方法，用于学习结构化表示，可以捕获数据中的局部和全局依赖关系。在第二部分中，我们研究了学习的结构化表示如何提高深度网络在各种下游任务上的泛化性能。我们的实验表明，我们提出的方法在图像分类和物体检测等任务上优于几种最先进的无监督方法。

    Despite tremendous progress over the past decade, deep learning methods generally fall short of human-level systematic generalization. It has been argued that explicitly capturing the underlying structure of data should allow connectionist systems to generalize in a more predictable and systematic manner. Indeed, evidence in humans suggests that interpreting the world in terms of symbol-like compositional entities may be crucial for intelligent behavior and high-level reasoning. Another common limitation of deep learning systems is that they require large amounts of training data, which can be expensive to obtain. In representation learning, large datasets are leveraged to learn generic data representations that may be useful for efficient learning of arbitrary downstream tasks.  This thesis is about structured representation learning. We study methods that learn, with little or no supervision, representations of unstructured data that capture its hidden structure. In the first part of
    
[^7]: 多尺度特征融合与并行注意力块应用于 COVID-19 胸部 X 光诊断

    Multi-Scale Feature Fusion using Parallel-Attention Block for COVID-19 Chest X-ray Diagnosis. (arXiv:2304.12988v1 [eess.IV])

    [http://arxiv.org/abs/2304.12988](http://arxiv.org/abs/2304.12988)

    本研究提出了一种新的多特征融合网络，使用并行注意力块在不同尺度上融合原始 CXR 图像和局部相位增强 CXR 图像，从而实现 COVID-19 胸部 X 光图像的精确诊断，具有高准确性和鲁棒性。

    

    在全球 COVID-19 危机背景下，从胸部 X 光图像中精确诊断 COVID-19 至关重要。为了减少放射学评估过程中的医生内部和互相之间的差异性，在医学决策和随后的疾病管理过程中，计算机辅助诊断工具已被用于补充。需要高准确性和鲁棒性的计算方法来快速分流患者，并帮助放射科医生解释收集的数据。本研究提出了一种新颖的多特征融合网络，使用并行注意力块来融合多尺度的原始 CXR 图像和局部相位增强 CXR 图像。我们在不同组织获取的各种 COVID-19 数据集上检验了我们的模型，以评估其泛化能力。我们的实验表明，我们的方法达到了最先进的性能，并且改善了泛化能力，这对于广泛部署至关重要。

    Under the global COVID-19 crisis, accurate diagnosis of COVID-19 from Chest X-ray (CXR) images is critical. To reduce intra- and inter-observer variability, during the radiological assessment, computer-aided diagnostic tools have been utilized to supplement medical decision-making and subsequent disease management. Computational methods with high accuracy and robustness are required for rapid triaging of patients and aiding radiologists in the interpretation of the collected data. In this study, we propose a novel multi-feature fusion network using parallel attention blocks to fuse the original CXR images and local-phase feature-enhanced CXR images at multi-scales. We examine our model on various COVID-19 datasets acquired from different organizations to assess the generalization ability. Our experiments demonstrate that our method achieves state-of-art performance and has improved generalization capability, which is crucial for widespread deployment.
    
[^8]: Rubik光学神经网络：具有物理感知旋转结构的多任务学习

    Rubik's Optical Neural Networks: Multi-task Learning with Physics-aware Rotation Architecture. (arXiv:2304.12985v1 [cs.LG])

    [http://arxiv.org/abs/2304.12985](http://arxiv.org/abs/2304.12985)

    RubikONNs利用光学系统的物理特性，通过旋转硬件编码多个前馈函数，为ONNs实现高效的多任务学习提供了新的策略。

    

    最近，有越来越多的研究工作在推进光学神经网络（ONNs），在功率效率，并行性和计算速度方面，ONNs带来了机器学习（ML）方面的显着优势。

    Recently, there are increasing efforts on advancing optical neural networks (ONNs), which bring significant advantages for machine learning (ML) in terms of power efficiency, parallelism, and computational speed. With the considerable benefits in computation speed and energy efficiency, there are significant interests in leveraging ONNs into medical sensing, security screening, drug detection, and autonomous driving. However, due to the challenge of implementing reconfigurability, deploying multi-task learning (MTL) algorithms on ONNs requires re-building and duplicating the physical diffractive systems, which significantly degrades the energy and cost efficiency in practical application scenarios. This work presents a novel ONNs architecture, namely, \textit{RubikONNs}, which utilizes the physical properties of optical systems to encode multiple feed-forward functions by physically rotating the hardware similarly to rotating a \textit{Rubik's Cube}. To optimize MTL performance on Rubi
    
[^9]: 基于Phylogeny的适配器在SemEval-2023第12项任务：情感分析中的应用

    GMNLP at SemEval-2023 Task 12: Sentiment Analysis with Phylogeny-Based Adapters. (arXiv:2304.12979v1 [cs.CL])

    [http://arxiv.org/abs/2304.12979](http://arxiv.org/abs/2304.12979)

    该论文介绍了GMU团队在SemEval-2023共享任务AfriSenti-SemEval中所使用的情感分析系统，使用AfroXLMR-large作为预训练语言模型并引入了增强的训练数据和基于Phylogeny的适配器调整以得到最佳结果。

    

    本文介绍了GMU对SemEval-2023共享任务AfriSenti-SemEval的情感分析系统。我们参与了单语言、多语言和零样本三个子任务。我们的方法使用了初始化为AfroXLMR-large的模型，它是一个在非洲语言上预训练并相应微调的多语言语言模型。我们还引入了增强的训练数据以及原始训练数据。除了微调外，我们还执行基于Phylogeny的适配器调整来创建多个模型，并将最佳模型集成到最终提交中。我们的系统在第5轨道Amharic上取得了最佳的F1分数，比该轨道上第二最佳性能系统高出6.2个F1分数。总体而言，我们的系统在参与所有15个轨道的10个系统中排名第5。

    This report describes GMU's sentiment analysis system for the SemEval-2023 shared task AfriSenti-SemEval. We participated in all three sub-tasks: Monolingual, Multilingual, and Zero-Shot. Our approach uses models initialized with AfroXLMR-large, a pre-trained multilingual language model trained on African languages and fine-tuned correspondingly. We also introduce augmented training data along with original training data. Alongside finetuning, we perform phylogeny-based adapter tuning to create several models and ensemble the best models for the final submission. Our system achieves the best F1-score on track 5: Amharic, with 6.2 points higher F1-score than the second-best performing system on this track. Overall, our system ranks 5th among the 10 systems participating in all 15 tracks.
    
[^10]: 逆强化学习的理论研究进展

    Towards Theoretical Understanding of Inverse Reinforcement Learning. (arXiv:2304.12966v1 [cs.LG])

    [http://arxiv.org/abs/2304.12966](http://arxiv.org/abs/2304.12966)

    逆强化学习是一类用于恢复能够解释专家代理演示的行为的奖励函数的强大算法，然而，由于存在多个可解释观察行为的奖励函数歧义，采用可行奖励集的方式绕过这种限制。该论文在有限时间问题的IRL理论方面进行了探究，提出第一个样本复杂度的极小极大下界。

    

    逆强化学习（IRL）是一系列用于恢复一个能够解释专家代理演示的行为的奖励函数的强大算法。IRL的一个已知限制是在选择奖励函数时存在歧义，由于存在可以解释观察到的行为的多个奖励函数。最近，通过将IRL表述为估计可行奖励集的问题，即与专家行为兼容的奖励范围的问题，已经绕过了这种限制。在本文中，我们在具有生成模型的有限时间问题的IRL理论上迈出了一步。我们首先正式介绍了估计可行奖励集的问题、相应的PAC要求，并讨论了特定类别奖励的属性。接着，我们为估计可行奖励集的问题提出了第一个样本复杂度的极小极大下界，其为${\Omega}\Bigl( \frac{H^3SA}{...

    Inverse reinforcement learning (IRL) denotes a powerful family of algorithms for recovering a reward function justifying the behavior demonstrated by an expert agent. A well-known limitation of IRL is the ambiguity in the choice of the reward function, due to the existence of multiple rewards that explain the observed behavior. This limitation has been recently circumvented by formulating IRL as the problem of estimating the feasible reward set, i.e., the region of the rewards compatible with the expert's behavior. In this paper, we make a step towards closing the theory gap of IRL in the case of finite-horizon problems with a generative model. We start by formally introducing the problem of estimating the feasible reward set, the corresponding PAC requirement, and discussing the properties of particular classes of rewards. Then, we provide the first minimax lower bound on the sample complexity for the problem of estimating the feasible reward set of order ${\Omega}\Bigl( \frac{H^3SA}{
    
[^11]: 变色龙: 适应对等镜像以植入耐用后门来进行联邦学习

    Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning. (arXiv:2304.12961v1 [cs.LG])

    [http://arxiv.org/abs/2304.12961](http://arxiv.org/abs/2304.12961)

    本论文提出了一种新的攻击方法"变色龙"，可以在联邦学习中实现更加耐用的后门攻击，通过对提供的良性图像和有毒图像目标标签之间的关系进行对比学习，取得了显著的实验效果。

    

    在联邦学习系统中，分布式客户端上传其本地模型到中心服务器以聚合成全局模型。恶意客户端可以通过上传有毒的本地模型来在全局模型中植入后门，导致具有特定模式的图像被错误分类为某些目标标签。当前攻击植入的后门是不耐用的，一旦攻击者停止模型中毒，便会迅速消失。本文研究了FL后门的耐用性与良性图象和有毒图象之间的关系(即在本地训练期间标签被翻转为目标标签的图像)。具体来说，发现原始图象和有毒图象的目标标签对后门的耐久性有关键影响。因此，我们提出了一种新型攻击，称为"变色龙"，它利用对比学习进一步放大这种影响来实现更耐用的后门。广泛的实验证明了我们的攻击在各种FL设置中的有效性。

    In a federated learning (FL) system, distributed clients upload their local models to a central server to aggregate into a global model. Malicious clients may plant backdoors into the global model through uploading poisoned local models, causing images with specific patterns to be misclassified into some target labels. Backdoors planted by current attacks are not durable, and vanish quickly once the attackers stop model poisoning. In this paper, we investigate the connection between the durability of FL backdoors and the relationships between benign images and poisoned images (i.e., the images whose labels are flipped to the target label during local training). Specifically, benign images with the original and the target labels of the poisoned images are found to have key effects on backdoor durability. Consequently, we propose a novel attack, Chameleon, which utilizes contrastive learning to further amplify such effects towards a more durable backdoor. Extensive experiments demonstrat
    
[^12]: 对高水平机器人解释中的奖励分解进行更深入的研究

    A Closer Look at Reward Decomposition for High-Level Robotic Explanations. (arXiv:2304.12958v1 [cs.LG])

    [http://arxiv.org/abs/2304.12958](http://arxiv.org/abs/2304.12958)

    本论文提出了一种将奖励分解与抽象动作空间相结合的学习框架，可以提供基于对象属性的明确高层次解释，避免了解释机器人行为的复杂性。

    

    向人类解释智能机器人的行为是具有挑战性的，这是由于它们难以理解的自体感状态、多变的中间目标和其结果的不可预测性。此外，对强化学习代理的一步解释可能是模糊的，因为它们未能在每个转换时考虑到代理的未来行为，从而增加了解释机器人行为的复杂性。通过利用映射到任务特定基元的抽象动作，我们避免了在移动层面上的解释。我们提出的框架将奖励分解（RD）与抽象动作空间结合起来，构建了一个可解释的学习框架，基于任务中的对象属性，实现了明确的高层次解释。我们通过对两个机器人场景的定量和定性分析来证明了我们框架的有效性，展示了RD解释的输出文物中的可视和文本解释，易于人类理解。

    Explaining the behavior of intelligent agents such as robots to humans is challenging due to their incomprehensible proprioceptive states, variational intermediate goals, and resultant unpredictability. Moreover, one-step explanations for reinforcement learning agents can be ambiguous as they fail to account for the agent's future behavior at each transition, adding to the complexity of explaining robot actions. By leveraging abstracted actions that map to task-specific primitives, we avoid explanations on the movement level. Our proposed framework combines reward decomposition (RD) with abstracted action spaces into an explainable learning framework, allowing for non-ambiguous and high-level explanations based on object properties in the task. We demonstrate the effectiveness of our framework through quantitative and qualitative analysis of two robot scenarios, showcasing visual and textual explanations, from output artifacts of RD explanation, that are easy for humans to comprehend. 
    
[^13]: 利用边界敏感度的神经隐式形状编辑

    Neural Implicit Shape Editing using Boundary Sensitivity. (arXiv:2304.12951v1 [cs.CV])

    [http://arxiv.org/abs/2304.12951](http://arxiv.org/abs/2304.12951)

    本文提出了一种基于边界敏感性的神经隐式形状编辑方法，该方法可以通过对参数进行扰动来实现指定的全局形状变形，并根据先验对其余部分进行调整。方法不受模型限制，并可优化和约束目标。

    

    由于其能够紧凑保存详细和平滑的形状并轻松地进行拓扑变化，神经场作为一种几何表示正受到越来越多的关注。然而，与传统的几何表示相比，神经表示不允许用户对形状进行直观的控制。为了解决这个问题，我们利用边界敏感性来表达参数扰动如何移动形状边界。这样可以解释每个可学习参数的影响，并研究可实现的变形。利用这一点，我们执行几何编辑：找到最佳近似全局预设变形的参数更新。仅在局部指定变形允许形状的其余部分根据一些先验，例如语义或变形刚性而发生变化。我们的方法不受其训练模型的限制，并且可以就地更新神经网络。此外，我们展示了边界敏感度如何帮助优化和约束目标（如表面重建和去噪）。

    Neural fields are receiving increased attention as a geometric representation due to their ability to compactly store detailed and smooth shapes and easily undergo topological changes. Compared to classic geometry representations, however, neural representations do not allow the user to exert intuitive control over the shape. Motivated by this, we leverage boundary sensitivity to express how perturbations in parameters move the shape boundary. This allows to interpret the effect of each learnable parameter and study achievable deformations. With this, we perform geometric editing: finding a parameter update that best approximates a globally prescribed deformation. Prescribing the deformation only locally allows the rest of the shape to change according to some prior, such as semantics or deformation rigidity. Our method is agnostic to the model its training and updates the NN in-place. Furthermore, we show how boundary sensitivity helps to optimize and constrain objectives (such as sur
    
[^14]: 量子机器学习架构中的Shot优化用于加速训练

    Shot Optimization in Quantum Machine Learning Architectures to Accelerate Training. (arXiv:2304.12950v1 [quant-ph])

    [http://arxiv.org/abs/2304.12950](http://arxiv.org/abs/2304.12950)

    本文提出了一种用于加速量子机器学习训练的Shot优化方法，通过减小数据集大小和自适应Shot分配等方法，实现了Shot数量的优化与准确度之间的平衡。

    

    本文提出了一种基于Shot优化的QML模型方法，该方法对模型性能的影响最小。我们使用混合量子-经典QML模型对MNIST和FMNIST数据集中的分类任务进行测试。首先，我们扫描数据集的短版本和完整版本的Shot数目。我们发现，训练完整版本比训练短版本提供了5-6%的测试准确度，并且训练中的Shot数量可高达10倍。因此，可以减小数据集的大小以加速训练时间。接下来，我们提出了自适应Shot分配方法用于短版本数据集，以优化训练周期内的Shot数量，并评估其对分类准确度的影响。我们使用（a）线性函数，其中Shot数量随着训练周期线性减少，和（b）步函数，其中Shot数量随着训练周期步进式减少。我们注意到，减少Shot数量会导致0.01的损失增加和约4%（1%）的测试准确度降低。

    In this paper, we propose shot optimization method for QML models at the expense of minimal impact on model performance. We use classification task as a test case for MNIST and FMNIST datasets using a hybrid quantum-classical QML model. First, we sweep the number of shots for short and full versions of the dataset. We observe that training the full version provides 5-6% higher testing accuracy than short version of dataset with up to 10X higher number of shots for training. Therefore, one can reduce the dataset size to accelerate the training time. Next, we propose adaptive shot allocation on short version dataset to optimize the number of shots over training epochs and evaluate the impact on classification accuracy. We use a (a) linear function where the number of shots reduce linearly with epochs, and (b) step function where the number of shots reduce in step with epochs. We note around 0.01 increase in loss and around 4% (1%) reduction in testing accuracy for reduction in shots by u
    
[^15]: eFAT：改进DNN硬件加速器中的故障感知训练以提高有效性

    eFAT: Improving the Effectiveness of Fault-Aware Training for Mitigating Permanent Faults in DNN Hardware Accelerators. (arXiv:2304.12949v1 [cs.AR])

    [http://arxiv.org/abs/2304.12949](http://arxiv.org/abs/2304.12949)

    研究提出了一种新的框架eFAT，用于计算DNN加速器的韧性并将其映射到相应的重训练量，通过分组和融合故障地图减少FAT的开销，同时提高容错能力。

    

    故障感知训练（FAT）已经成为一种高效应对DNN加速器永久性故障的技术，因为它能够在低和中等故障率下提供故障缓解而不会带来显着的性能或准确性损失。然而，当用于面向复杂AI应用程序的大型DNN时，它会导致非常高的重训练开销。此外，由于每个芯片的故障模式都可能不同，因此需要逐一为每个故障芯片进行FAT，考虑到其独特的故障地图，这进一步加剧了问题。为了减少FAT的开销同时维持其优势，我们提出了（1）韧性驱动的重训练量选择和（2）多个故障地图（属于不同芯片）的韧性驱动的分组和融合，以对一组故障芯片进行合并重训练。为了实现这些概念，我们提出了一种新的框架eFAT，用于计算DNN加速器的韧性并将其映射到相应的重训练量。eFAT还根据故障地图对存在故障的芯片进行分组，并对组进行合并重训练，同时考虑组间的差异。我们在多个DNN加速器上的实验表明，eFAT显著降低了FAT的重训练开销，同时提供类似或更好的容错能力。

    Fault-Aware Training (FAT) has emerged as a highly effective technique for addressing permanent faults in DNN accelerators, as it offers fault mitigation without significant performance or accuracy loss, specifically at low and moderate fault rates. However, it leads to very high retraining overheads, especially when used for large DNNs designed for complex AI applications. Moreover, as each fabricated chip can have a distinct fault pattern, FAT is required to be performed for each faulty chip individually, considering its unique fault map, which further aggravates the problem. To reduce the overheads of FAT while maintaining its benefits, we propose (1) the concepts of resilience-driven retraining amount selection, and (2) resilience-driven grouping and fusion of multiple fault maps (belonging to different chips) to perform consolidated retraining for a group of faulty chips. To realize these concepts, in this work, we present a novel framework, eFAT, that computes the resilience of a
    
[^16]: 内积数组低延迟在线乘法器

    Low-Latency Online Multiplier with Reduced Activities and Minimized Interconnect for Inner Product Arrays. (arXiv:2304.12946v1 [cs.AR])

    [http://arxiv.org/abs/2304.12946](http://arxiv.org/abs/2304.12946)

    本文提出了一种基于在线算法的低延迟乘法器，它能通过位级流水线提高吞吐量并减少延迟，并适用于在资源有限的可重构设备上实施。

    

    乘法是许多现代应用程序（包括信号处理和神经网络）中不可或缺并且是核心操作之一。传统的从右至左乘法器在这些应用程序中广泛贡献了功耗、面积利用率和关键路径延迟。本文提出了一种基于在线算法或从左至右算法的低延迟乘法器，该算法可以通过位级流水线提高吞吐量并减少延迟。由于在线算法采用了最高位数字优先模式的操作方式，因此能够无论数据依赖性如何都进行重叠的连续操作。为了生成最高位数字优先，它使用冗余数字系统，因此可以进行无进位加法，因此算术操作的延迟与操作数位宽无关。这些操作逐位串行地从左到右执行，这使得它适用于在资源有限的可重构设备上实施。

    Multiplication is indispensable and is one of the core operations in many modern applications including signal processing and neural networks. Conventional right-to-left (RL) multiplier extensively contributes to the power consumption, area utilization and critical path delay in such applications. This paper proposes a low latency multiplier based on online or left-to-right (LR) arithmetic which can increase throughput and reduce latency by digit-level pipelining. Online arithmetic enables overlapping successive operations regardless of data dependency because of the most significant digit first mode of operation. To produce most significant digit first, it uses redundant number system and we can have a carry-free addition, therefore, the delay of the arithmetic operation is independent of operand bit width. The operations are performed digit by digit serially from left to right which allows gradual increase in the slice activities making it suitable for implementation on reconfigurabl
    
[^17]: 潜在生成模型中的潜在遍历作为潜在流的潜在路线

    Latent Traversals in Generative Models as Potential Flows. (arXiv:2304.12944v1 [cs.LG])

    [http://arxiv.org/abs/2304.12944](http://arxiv.org/abs/2304.12944)

    该论文使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行，以实现解缠，并通过分类器进行约束。

    

    尽管深度生成模型的最近进展已经取得了显著进展，但它们的潜在空间的基本结构仍然很不好理解，因此执行语义上有意义的潜在遍历的任务仍然是一个开放的研究挑战。大多数之前的工作旨在通过线性建模潜在结构，并找到相应的线性方向，从而产生“解缠”的代数。在这项工作中，我们提议改为使用学习的动态潜在景观来建模潜在结构，从而将潜在遍历作为样本沿着景观梯度的流动进行。这些潜在景观受到物理学、最优运输和神经科学的启发，被作为物理上现实的偏微分方程来学习，从而允许它们在空间和时间上具有灵活性。为了实现解缠，同时学习了多个势能，并通过分类器进行约束，使其具有明显差异且在语义上自我一致。

    Despite the significant recent progress in deep generative models, the underlying structure of their latent spaces is still poorly understood, thereby making the task of performing semantically meaningful latent traversals an open research challenge. Most prior work has aimed to solve this challenge by modeling latent structures linearly, and finding corresponding linear directions which result in `disentangled' generations. In this work, we instead propose to model latent structures with a learned dynamic potential landscape, thereby performing latent traversals as the flow of samples down the landscape's gradient. Inspired by physics, optimal transport, and neuroscience, these potential landscapes are learned as physically realistic partial differential equations, thereby allowing them to flexibly vary over both space and time. To achieve disentanglement, multiple potentials are learned simultaneously, and are constrained by a classifier to be distinct and semantically self-consisten
    
[^18]: 生成强鲁棒性的反事实解释

    Generating robust counterfactual explanations. (arXiv:2304.12943v1 [cs.LG])

    [http://arxiv.org/abs/2304.12943](http://arxiv.org/abs/2304.12943)

    本论文提出了一个新的反事实解释框架CROCO，能够生成强鲁棒性的解释，同时有效地平衡了鲁棒性和解释示例的接近度之间的权衡。

    

    反事实的解释已经成为可解释人工智能领域的主流。这一直观的陈述让用户理解在给定情况下，为改变模型预测而必须进行的小但必要的更改。反事实的质量取决于多个标准：真实性、可行性、有效性、鲁棒性等等。本文关注反事实的鲁棒性概念，更具体地，关注反事实输入变化的鲁棒性。这种鲁棒性尤其具有挑战性，因为它涉及到反事实的鲁棒性和与要解释的示例的接近度之间的权衡。我们提出了一个新的框架CROCO，它能够有效地管理这种权衡，生成具有强鲁棒性的反事实解释，并保证用户具有最小的鲁棒性。在表格数据集上的实证评估确认了我们方法的相关性和有效性。

    Counterfactual explanations have become a mainstay of the XAI field. This particularly intuitive statement allows the user to understand what small but necessary changes would have to be made to a given situation in order to change a model prediction. The quality of a counterfactual depends on several criteria: realism, actionability, validity, robustness, etc. In this paper, we are interested in the notion of robustness of a counterfactual. More precisely, we focus on robustness to counterfactual input changes. This form of robustness is particularly challenging as it involves a trade-off between the robustness of the counterfactual and the proximity with the example to explain. We propose a new framework, CROCO, that generates robust counterfactuals while managing effectively this trade-off, and guarantees the user a minimal robustness. An empirical evaluation on tabular datasets confirms the relevance and effectiveness of our approach.
    
[^19]: 进化三维抽象艺术：通过语言匹配概念

    Evolving Three Dimension (3D) Abstract Art: Fitting Concepts by Language. (arXiv:2304.12932v1 [cs.CV])

    [http://arxiv.org/abs/2304.12932](http://arxiv.org/abs/2304.12932)

    本论文介绍了一种通过进化策略和3D渲染相结合的方法，使艺术家能够使用自然语言表达抽象三维艺术的创意想法。

    

    计算创意在现代抽象艺术中发挥了重要作用，使艺术家能够创建高质量、抽象的二维艺术，具有高度的可控性和表现力。然而，即使是具有良好结果的计算方法在创作具体的三维艺术方面，处理高质量、可控性的抽象三维艺术仍然是一个悬而未决的问题。为了填补这一空白，我们建议通过场景的可自定义参数化，将进化策略（ES）和3D渲染相结合，探索在制作抽象三维艺术方面的计算创意。我们展示了我们的方法能够在3D场景中放置半透明三角形，在从指定角度查看时，呈现出看起来像艺术家使用自然语言表达的规格说明的影片。这为艺术家提供了一个新的方式，方便地表达抽象三维艺术的创意想法。

    Computational creativity has contributed heavily to abstract art in modern era, allowing artists to create high quality, abstract two dimension (2D) arts with a high level of controllability and expressibility. However, even with computational approaches that have promising result in making concrete 3D art, computationally addressing abstract 3D art with high-quality and controllability remains an open question. To fill this gap, we propose to explore computational creativity in making abstract 3D art by bridging evolution strategies (ES) and 3D rendering through customizable parameterization of scenes. We demonstrate that our approach is capable of placing semi-transparent triangles in 3D scenes that, when viewed from specified angles, render into films that look like artists' specification expressed in natural language. This provides a new way for the artist to easily express creativity ideas for abstract 3D art. The supplementary material, which contains code, animation for all figu
    
[^20]: 用户中心联邦学习：为个性化而交换无线资源。

    User-Centric Federated Learning: Trading off Wireless Resources for Personalization. (arXiv:2304.12930v1 [cs.LG])

    [http://arxiv.org/abs/2304.12930](http://arxiv.org/abs/2304.12930)

    本文提出基于用户中心的联邦学习算法，使用可用的梯度信息和聚合规则，为每个联邦学习客户端生成个性化模型，同时保证隐私保护的传输。在基准数据集上表现出了更好的准确度和通信效率。

    

    在联邦学习系统中，客户端之间的统计异质性会增加算法收敛时间并降低泛化性能，导致高通信开销和质量低劣的模型。为了解决上述问题，而不违反联邦学习的隐私约束，个性化联邦学习方法必须将统计相似的客户端耦合在一起，以保证隐私保护的传输。在这项工作中，我们设计了基于可用的梯度信息的参数服务器（PS）的用户中心聚合规则，能够为每个联邦学习客户端生成个性化模型。所提出的聚合规则受加权聚合经验风险最小化的上界启发。其次，我们基于用户聚类导出了一种通信高效的变体，大大增强了其适用于通信受限系统的能力。我们的算法在两个基准数据集上的准确度和通信效率相似或更好地胜过了流行的对比算法。

    Statistical heterogeneity across clients in a Federated Learning (FL) system increases the algorithm convergence time and reduces the generalization performance, resulting in a large communication overhead in return for a poor model. To tackle the above problems without violating the privacy constraints that FL imposes, personalized FL methods have to couple statistically similar clients without directly accessing their data in order to guarantee a privacy-preserving transfer. In this work, we design user-centric aggregation rules at the parameter server (PS) that are based on readily available gradient information and are capable of producing personalized models for each FL client. The proposed aggregation rules are inspired by an upper bound of the weighted aggregate empirical risk minimizer. Secondly, we derive a communication-efficient variant based on user clustering which greatly enhances its applicability to communication-constrained systems. Our algorithm outperforms popular pe
    
[^21]: 基于量子核函数的高斯过程回归在贝叶斯优化中的应用

    Quantum Gaussian Process Regression for Bayesian Optimization. (arXiv:2304.12923v1 [quant-ph])

    [http://arxiv.org/abs/2304.12923](http://arxiv.org/abs/2304.12923)

    本研究提出一种基于参数化的量子电路的量子核函数的高斯过程回归方法，将其应用于贝叶斯优化中，证明其可以保留变量信息并且可以作为贝叶斯优化的代理模型，实验结果证明此方法在实际数据集上的表现与经典方法相当。

    

    高斯过程回归是一种成熟的贝叶斯机器学习方法。我们提出了一种基于参数化的量子电路的量子核函数的高斯过程回归新方法。通过使用硬件有效的特征映射和谨慎正则化 Gram 矩阵，我们证明了此方法能够保留所得出的量子高斯过程的方差信息。我们还证明了量子高斯过程可以用作贝叶斯优化的代理模型，而这个任务的成功关键在于代理模型的方差。为了展示这种量子贝叶斯优化算法的性能，我们将其应用于对一个执行实际数据集回归的机器学习模型的超参数优化。我们对量子贝叶斯优化与其经典对应物进行基准测试，并证明量子版本可以达到相同的性能。

    Gaussian process regression is a well-established Bayesian machine learning method. We propose a new approach to Gaussian process regression using quantum kernels based on parameterized quantum circuits. By employing a hardware-efficient feature map and careful regularization of the Gram matrix, we demonstrate that the variance information of the resulting quantum Gaussian process can be preserved. We also show that quantum Gaussian processes can be used as a surrogate model for Bayesian optimization, a task that critically relies on the variance of the surrogate model. To demonstrate the performance of this quantum Bayesian optimization algorithm, we apply it to the hyperparameter optimization of a machine learning model which performs regression on a real-world dataset. We benchmark the quantum Bayesian optimization against its classical counterpart and show that quantum version can match its performance.
    
[^22]: 基于Copula熵的系统辨识

    System Identification with Copula Entropy. (arXiv:2304.12922v1 [eess.SY])

    [http://arxiv.org/abs/2304.12922](http://arxiv.org/abs/2304.12922)

    本文提出了一种使用Copula熵方法无模型超参数地对动态系统进行微分方程识别的方法，并在三维Lorenz系统的模拟实验中验证了该方法的有效性。

    

    确定动态系统的微分方程是一个具有广泛应用的重要问题。Copula熵（CE）是信息论中用于测量统计独立性的数学概念。本文提出了一种使用CE识别动态系统微分方程的方法。该问题被视为一个变量选择问题，并通过先前提出的基于CE的变量选择方法来解决。所提出的方法由两个组件组成：差分算子和CE估计器。由于两个组件都可以进行非参数化，因此所提出的方法是无模型和超参数。在三维Lorenz系统的模拟实验中验证了所提出方法的有效性。

    Identifying differential equation governing dynamical system is an important problem with wide applications. Copula Entropy (CE) is a mathematical concept for measuring statistical independence in information theory. In this paper we propose a method for identifying differential equation of dynamical systems with CE. The problem is considered as a variable selection problem and solved with the previously proposed CE-based method for variable selection. The proposed method composed of two components: the difference operator and the CE estimator. Since both components can be done non-parametrically, the proposed method is therefore model-free and hyperparameter-free. The simulation experiment with the 3D Lorenz system verified the effectiveness of the proposed method.
    
[^23]: Awesome-META+: 元学习研究与学习平台

    Awesome-META+: Meta-Learning Research and Learning Platform. (arXiv:2304.12921v1 [cs.LG])

    [http://arxiv.org/abs/2304.12921](http://arxiv.org/abs/2304.12921)

    Awesome-META+是一个元学习框架集成和学习平台，旨在提供完整可靠的元学习框架应用和面向初学者的学习材料，进而促进元学习的发展并将其从小众领域转化为主流的研究方向。

    

    人工智能已经在经济、产业、教育等各个领域产生了深远的影响，但还存在诸多限制。元学习，也称为“学习如何学习”，为通用人工智能提供了突破目前瓶颈的机会。然而，元学习起步较晚，相比CV、NLP等领域，项目数量较少。每次部署都需要大量的经验去配置环境、调试代码甚至重写，而且框架之间相对孤立。此外，目前针对元学习的专门平台和面向初学者的学习材料相对较少，门槛相对较高。基于此，Awesome-META+提出了一个元学习框架集成和学习平台，旨在解决上述问题并提供完整可靠的元学习框架应用和学习平台。该项目旨在促进元学习的发展，并将其从一个小众领域转化为一个主流的研究方向。

    Artificial intelligence technology has already had a profound impact in various fields such as economy, industry, and education, but still limited. Meta-learning, also known as "learning to learn", provides an opportunity for general artificial intelligence, which can break through the current AI bottleneck. However, meta learning started late and there are fewer projects compare with CV, NLP etc. Each deployment requires a lot of experience to configure the environment, debug code or even rewrite, and the frameworks are isolated. Moreover, there are currently few platforms that focus exclusively on meta-learning, or provide learning materials for novices, for which the threshold is relatively high. Based on this, Awesome-META+, a meta-learning framework integration and learning platform is proposed to solve the above problems and provide a complete and reliable meta-learning framework application and learning platform. The project aims to promote the development of meta-learning and t
    
[^24]: N2G：一种在大型语言模型中量化可解释神经元表示的可扩展方法

    N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models. (arXiv:2304.12918v1 [cs.LG])

    [http://arxiv.org/abs/2304.12918](http://arxiv.org/abs/2304.12918)

    N2G是一种在大型语言模型中实现解释神经元的方法，它通过自动将神经元在数据集示例上的行为提炼为可解释的图形，并且可以输出文本上令牌的激活情况来进行自动验证。

    

    理解语言模型中个别神经元的功能对于机械解释性研究至关重要。我们提出了一种名为“神经元到图（N2G）”的工具，该工具接收神经元及其数据集示例，并自动将神经元在这些示例上的行为提炼为一个可解释的图形，为解释神经元提供了比当前手动方法更轻松的方法，并更好地将这些方法扩展到大型语言模型（LLM）。我们使用截断和显著性方法仅呈现重要令牌，并利用更多样的样本增强数据集示例，以更好地捕捉神经元行为的程度。这些图形可以通过可视化来帮助研究人员进行手动解释，但也可以输出文本上的令牌激活情况，以与神经元的参考激活情况进行比较，进行自动验证。N2G代表了可扩展的解释性方法向前迈出的一步，因为它允许我们自动将LLM中的神经元转换为可解释的图形。

    Understanding the function of individual neurons within language models is essential for mechanistic interpretability research. We propose $\textbf{Neuron to Graph (N2G)}$, a tool which takes a neuron and its dataset examples, and automatically distills the neuron's behaviour on those examples to an interpretable graph. This presents a less labour intensive approach to interpreting neurons than current manual methods, that will better scale these methods to Large Language Models (LLMs). We use truncation and saliency methods to only present the important tokens, and augment the dataset examples with more diverse samples to better capture the extent of neuron behaviour. These graphs can be visualised to aid manual interpretation by researchers, but can also output token activations on text to compare to the neuron's ground truth activations for automatic validation. N2G represents a step towards scalable interpretability methods by allowing us to convert neurons in an LLM to interpretab
    
[^25]: 评分差值流模型用于隐式生成建模

    The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v1 [cs.LG])

    [http://arxiv.org/abs/2304.12906](http://arxiv.org/abs/2304.12906)

    本文提出了一种新的评分差异流模型(SD flow)，它可以最优地减少两个分布之间的散度，同时解决Schr​​ödinger桥问题。与去噪扩散模型不同，它没有对先验分布施加任何限制，在一些基准数据集中优于其他方法。

    

    隐式生成建模(IGM)旨在生成符合目标数据分布特征的合成数据样本。最近的研究(例如评分匹配网络、扩散模型)从通过环境空间中的动态扰动或流将合成源数据推向目标分布的角度解决了IGM问题。我们引入了任意目标和源分布之间的评分差异(SD)作为流，它可以最优地减少它们之间的Kullback-Leibler散度，同时解决Schr​​ödinger桥问题。我们将SD流应用于方便的代理分布，当且仅当原始分布对齐时，它们是对齐的。我们在某些条件下展示了这种公式与去噪扩散模型的形式一致性。然而，与扩散模型不同，SD流没有对先验分布施加任何限制。我们还表明，在无限辨别器能力的极限下，生成对抗网络的训练包含SD流。我们的实验表明，SD流在几个基准数据集上优于先前的最新技术。

    Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. We introduce the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schr\"odinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. However, unlike diffusion models, SD flow places no restrictions on the prior distribution. We also show that the training of generative adversarial networks includ
    
[^26]: 发现图形生成算法

    Discovering Graph Generation Algorithms. (arXiv:2304.12895v1 [cs.LG])

    [http://arxiv.org/abs/2304.12895](http://arxiv.org/abs/2304.12895)

    本论文提出了一种使用图神经网络和进化搜索来发现图形生成算法的方法，相比于传统的概率模型或深度生成模型，其具有更高的训练分布外推潜力和直接解释性。可以与深度生成模型竞争，并且在某些情况下可以找到真正的图形生成过程。

    

    我们提供了一种新方法来构建图形的生成模型。我们建议使用进化搜索和一个由随机初始化的图神经网络实现的强大适应度函数，而不是使用传统的概率模型或深度生成模型来生成数据。相比当前的深度生成模型，这种方法带来了一些优势，例如更高的训练分布外推潜力和直接解释性，因为最终的图形生成过程以Python函数的形式表达。我们展示了这种方法可以与深度生成模型竞争，并且在某些情况下甚至可以找到真正的图形生成过程，因此可以完美地泛化。

    We provide a novel approach to construct generative models for graphs. Instead of using the traditional probabilistic models or deep generative models, we propose to instead find an algorithm that generates the data. We achieve this using evolutionary search and a powerful fitness function, implemented by a randomly initialized graph neural network. This brings certain advantages over current deep generative models, for instance, a higher potential for out-of-training-distribution generalization and direct interpretability, as the final graph generative process is expressed as a Python function. We show that this approach can be competitive with deep generative models and under some circumstances can even find the true graph generative process, and as such perfectly generalize.
    
[^27]: 潜在扩散模型用于具有准确不确定性量化的生成性降水现预报

    Latent diffusion models for generative precipitation nowcasting with accurate uncertainty quantification. (arXiv:2304.12891v1 [physics.ao-ph])

    [http://arxiv.org/abs/2304.12891](http://arxiv.org/abs/2304.12891)

    LDM模型可以用于降水预测，相较于GAN等其他模型，LDM不仅更稳定、需要更少的计算量，还能产生更多样化和更准确的降水预测，是对于需要考虑不确定性和预测多样性的决策-making 的相关应用非常有前景的。

    

    扩散模型已被广泛采用于图像生成，产生的样本质量更高、更多样化，比生成对抗网络（GAN）更有效。我们介绍了一种潜在扩散模型（LDM）用于降水即时预报，即基于最新观测数据的短期预测。LDM比GAN更稳定，需要更少的计算量进行训练，但对于生成而言需要更多的计算量。我们将其与基于GAN的Deep Generative Models of Rainfall（DGMR）和统计模型PySTEPS进行了基准测试。结果表明，LDM产生了更准确的降水预报，当预测降水是否超过预定义阈值时，比较结果更加混合。 LDM的最大优势在于它产生的预测更加多样化。等级分布测试表明，从LDM中取样的分布准确反映了预测的不确定性。因此，LDM在需要关注不确定性和多样性预测的任何应用中都非常有前景，例如在重大气象事件的决策中。

    Diffusion models have been widely adopted in image generation, producing higher-quality and more diverse samples than generative adversarial networks (GANs). We introduce a latent diffusion model (LDM) for precipitation nowcasting - short-term forecasting based on the latest observational data. The LDM is more stable and requires less computation to train than GANs, albeit with more computationally expensive generation. We benchmark it against the GAN-based Deep Generative Models of Rainfall (DGMR) and a statistical model, PySTEPS. The LDM produces more accurate precipitation predictions, while the comparisons are more mixed when predicting whether the precipitation exceeds predefined thresholds. The clearest advantage of the LDM is that it generates more diverse predictions than DGMR or PySTEPS. Rank distribution tests indicate that the distribution of samples from the LDM accurately reflects the uncertainty of the predictions. Thus, LDMs are promising for any applications where uncer
    
[^28]: 针对函数逼近的在线强化学习的一般覆盖条件的可证明优势

    Provable benefits of general coverage conditions in efficient online RL with function approximation. (arXiv:2304.12886v1 [stat.ML])

    [http://arxiv.org/abs/2304.12886](http://arxiv.org/abs/2304.12886)

    研究者对在线强化学习提出了一种新的一般覆盖条件，并发现更多的覆盖条件，提高了在线强化学习的样本效率和表现，同时阐明良好的覆盖条件仍然有益于获得最优解。

    

    在线强化学习中，与其使用马尔可夫决策过程（MDPs）的标准结构假设，使用某种覆盖条件（源自离线强化学习）足以确保样本有效保证（Xie等人，2023）。本文关注这个新方向，挖掘更多可能和更普遍的覆盖条件，并研究它们在高效在线强化学习中的潜力和用途。我们鉴定了更多概念，包括$L^p$功能集中度、密度比实现性以及部分/全覆盖条件的权衡，这些概念也有益于实现样本有效的在线强化学习，从而实现改进的遗憾边界。此外，如果利用探索性的离线数据，在我们的覆盖条件下，可以为在线强化学习实现统计和计算上高效的保证。此外，即使MDP结构已经给出，例如线性MDP，我们也阐明了良好的覆盖条件仍然有益于获得最优解。

    In online reinforcement learning (RL), instead of employing standard structural assumptions on Markov decision processes (MDPs), using a certain coverage condition (original from offline RL) is enough to ensure sample-efficient guarantees (Xie et al. 2023). In this work, we focus on this new direction by digging more possible and general coverage conditions, and study the potential and the utility of them in efficient online RL. We identify more concepts, including the $L^p$ variant of concentrability, the density ratio realizability, and trade-off on the partial/rest coverage condition, that can be also beneficial to sample-efficient online RL, achieving improved regret bound. Furthermore, if exploratory offline data are used, under our coverage conditions, both statistically and computationally efficient guarantees can be achieved for online RL. Besides, even though the MDP structure is given, e.g., linear MDP, we elucidate that, good coverage conditions are still beneficial to obtai
    
[^29]: 强化学习智能体的近端课程设计

    Proximal Curriculum for Reinforcement Learning Agents. (arXiv:2304.12877v1 [cs.LG])

    [http://arxiv.org/abs/2304.12877](http://arxiv.org/abs/2304.12877)

    本文基于近端发展区概念，提出了 ProCuRL 课程策略，用于设计深度强化学习智能体的课程，以加速训练过程，并在各种域上实验表明其优越性。

    

    我们考虑针对上下文多任务设置中的强化学习智能体的课程设计问题。自动课程设计的现有技术通常需要特定领域中的超参数调整或具有有限的理论基础。为了解决这些限制，我们设计了课程策略 ProCuRL，它受到了教育概念“近端发展区”的启发。ProCuRL 触发了这样一个直觉，即当选择既不太难也不太容易的任务时，学习进展是最大化的。我们通过分析两个简单的学习设置来数学推导 ProCuRL。我们还提出了一种实用的 ProCuRL 变体，可以通过最小的超参数调整直接集成到深度强化学习框架中。在各种域上的实验结果表明，在加速深度强化学习智能体的训练过程方面，我们的课程策略比现有技术水平的基线更加有效。

    We consider the problem of curriculum design for reinforcement learning (RL) agents in contextual multi-task settings. Existing techniques on automatic curriculum design typically require domain-specific hyperparameter tuning or have limited theoretical underpinnings. To tackle these limitations, we design our curriculum strategy, ProCuRL, inspired by the pedagogical concept of Zone of Proximal Development (ZPD). ProCuRL captures the intuition that learning progress is maximized when picking tasks that are neither too hard nor too easy for the learner. We mathematically derive ProCuRL by analyzing two simple learning settings. We also present a practical variant of ProCuRL that can be directly integrated with deep RL frameworks with minimal hyperparameter tuning. Experimental results on a variety of domains demonstrate the effectiveness of our curriculum strategy over state-of-the-art baselines in accelerating the training process of deep RL agents.
    
[^30]: 基于激光注入的参数攻击对嵌入式神经网络的评估

    Evaluation of Parameter-based Attacks against Embedded Neural Networks with Laser Injection. (arXiv:2304.12876v1 [cs.CR])

    [http://arxiv.org/abs/2304.12876](http://arxiv.org/abs/2304.12876)

    本研究首次成功地在32位Cortex-M微控制器上使用激光注入进行了比特翻转攻击，强调了典型深度神经网络的缺乏强健性。

    

    机器学习系统安全认证行动的到来，加上模型在多个硬件平台上的大规模部署，引起了极大的评估挑战。最近，大多数研究工作集中在基于API的攻击上，将机器学习模型视为纯算法抽象。然而，新的基于实现的威胁已被揭示，强调提出实践和基于仿真的方法，以适当评估模型的强健性。一个主要关注点是基于参数的攻击（例如比特翻转攻击），当内部存储器中的参数被精确和最优地更改时，强调了典型深度神经网络模型的缺乏强健性。针对安全测试目的，本研究首次在32位Cortex-M微控制器上使用激光故障注入，实际报告了成功的BFA变体。

    Upcoming certification actions related to the security of machine learning (ML) based systems raise major evaluation challenges that are amplified by the large-scale deployment of models in many hardware platforms. Until recently, most of research works focused on API-based attacks that consider a ML model as a pure algorithmic abstraction. However, new implementation-based threats have been revealed, emphasizing the urgency to propose both practical and simulation-based methods to properly evaluate the robustness of models. A major concern is parameter-based attacks (such as the Bit-Flip Attack, BFA) that highlight the lack of robustness of typical deep neural network models when confronted by accurate and optimal alterations of their internal parameters stored in memory. Setting in a security testing purpose, this work practically reports, for the first time, a successful variant of the BFA on a 32-bit Cortex-M microcontroller using laser fault injection. It is a standard fault injec
    
[^31]: 交替局部枚举(TnALE): 用较少的评估解决张量网络结构搜索问题

    Alternating Local Enumeration (TnALE): Solving Tensor Network Structure Search with Fewer Evaluations. (arXiv:2304.12875v1 [cs.LG])

    [http://arxiv.org/abs/2304.12875](http://arxiv.org/abs/2304.12875)

    提出了TnALE算法，通过交替局部枚举更新每个与结构相关的变量，大大减少了评估次数，用于解决张量网络结构搜索问题。在理论上证明，如果在每个邻域中达到了足够的目标函数降低，TnALE和TNLS都可以实现线性收敛度，直到一个常数。同时，与TNLS相比， TnALE需要更少的评估次数。

    

    张量网络(TN)是机器学习中强大的框架，但选择一个好的TN模型，即TN结构搜索(TN-SS)，是一项具有挑战性和计算密集型的任务。最近的方法TNLS ~ \cite {li2022permutation} 在这个任务中显示出了有希望的结果，但它的计算效率仍然是无法承受的，需要太多评估目标函数的次数。我们提出了TnALE，一种新的算法，通过局部枚举交替更新每个与结构相关的变量，与TNLS相比，大大减少了评估次数。我们从理论上研究了TNLS和TnALE的下降步骤，证明如果在每个邻域中达到了足够的目标函数降低，那么两种算法都可以实现线性收敛度，直到一个常数。我们还比较了TNLS和TnALE的评估效率，揭示了在TNLS中通常需要Ω(2 ^ N)个评估才能在邻域内达到目标降低。

    Tensor network (TN) is a powerful framework in machine learning, but selecting a good TN model, known as TN structure search (TN-SS), is a challenging and computationally intensive task. The recent approach TNLS~\cite{li2022permutation} showed promising results for this task, however, its computational efficiency is still unaffordable, requiring too many evaluations of the objective function. We propose TnALE, a new algorithm that updates each structure-related variable alternately by local enumeration, \emph{greatly} reducing the number of evaluations compared to TNLS. We theoretically investigate the descent steps for TNLS and TnALE, proving that both algorithms can achieve linear convergence up to a constant if a sufficient reduction of the objective is \emph{reached} in each neighborhood. We also compare the evaluation efficiency of TNLS and TnALE, revealing that $\Omega(2^N)$ evaluations are typically required in TNLS for \emph{reaching} the objective reduction in the neighborhood
    
[^32]: 二进制随机性支持的神经形态深度学习实现比软件更高准确性

    Binary stochasticity enabled highly efficient neuromorphic deep learning achieves better-than-software accuracy. (arXiv:2304.12866v1 [cs.NE])

    [http://arxiv.org/abs/2304.12866](http://arxiv.org/abs/2304.12866)

    二进制随机学习算法提高了深度学习效率，消除了使用噪声人工突触硬件系统的挑战性。

    

    深度学习需要精准的信号转发处理、反向传播误差和权重更新。这是由于梯度下降学习规则依赖于偏导数的链式乘积，学习算法本质上需要这一点。然而，使用具有噪声的人工突触的硬件系统中实现深度学习是具有挑战性的，因为这不符合生物学原理。基于膜电阻器的实施通常导致过多的神经电路成本和对理想化突触设备的严格需求。因此，我们展示了高精度不是必要的，没有这个要求可以实现更高效的深度学习。我们提出了一个二进制随机学习算法，通过引入(i)正、反向信号的随机二值化 (ii) 有符号二值化的神经网络的所有基本操作进行修改。

    Deep learning needs high-precision handling of forwarding signals, backpropagating errors, and updating weights. This is inherently required by the learning algorithm since the gradient descent learning rule relies on the chain product of partial derivatives. However, it is challenging to implement deep learning in hardware systems that use noisy analog memristors as artificial synapses, as well as not being biologically plausible. Memristor-based implementations generally result in an excessive cost of neuronal circuits and stringent demands for idealized synaptic devices. Here, we demonstrate that the requirement for high precision is not necessary and that more efficient deep learning can be achieved when this requirement is lifted. We propose a binary stochastic learning algorithm that modifies all elementary neural network operations, by introducing (i) stochastic binarization of both the forwarding signals and the activation function derivatives, (ii) signed binarization of the b
    
[^33]: 约束混沌：在递归神经网络训练中加强动力学不变量

    Constraining Chaos: Enforcing dynamical invariants in the training of recurrent neural networks. (arXiv:2304.12865v1 [cs.LG])

    [http://arxiv.org/abs/2304.12865](http://arxiv.org/abs/2304.12865)

    该论文提出了一种通过在感兴趣的系统中执行动力学不变量（如李雅普诺夫指数谱和分形维数）的方法来预测混沌动力学系统。通过使用递归神经网络结构，该技术可用于进行更长时间和更稳定的预测，适用于以Lorenz 1996混沌动力学系统和光谱拟准等位模型为代表的典型数值天气预报测试案例。

    

    借鉴遍历论，我们提出了一种新的机器学习方法来预测混沌动力学系统。这种训练方法通过在感兴趣的系统中执行动力学不变量（如李雅普诺夫指数谱和分形维数），使操作受限的数据下进行更长时间和更稳定的预测成为可能。我们使用储存器计算的递归神经网络结构详细演示了该技术在以Lorenz 1996混沌动力学系统和光谱拟准等位模型为代表的典型数值天气预报测试案例的结果。

    Drawing on ergodic theory, we introduce a novel training method for machine learning based forecasting methods for chaotic dynamical systems. The training enforces dynamical invariants--such as the Lyapunov exponent spectrum and fractal dimension--in the systems of interest, enabling longer and more stable forecasts when operating with limited data. The technique is demonstrated in detail using the recurrent neural network architecture of reservoir computing. Results are given for the Lorenz 1996 chaotic dynamical system and a spectral quasi-geostrophic model, both typical test cases for numerical weather prediction.
    
[^34]: 机器学习应用中的异常是什么原因？在Stack Overflow上挖掘机器学习相关堆栈跟踪

    What Causes Exceptions in Machine Learning Applications? Mining Machine Learning-Related Stack Traces on Stack Overflow. (arXiv:2304.12857v1 [cs.LG])

    [http://arxiv.org/abs/2304.12857](http://arxiv.org/abs/2304.12857)

    研究者在Stack Overflow上挖掘了11,449个与七个常用Python ML库相关的堆栈跟踪，发现包含堆栈跟踪的ML问题更受欢迎，而它们的问题更复杂。

    

    机器学习（ML），包括深度学习，最近在广泛的应用中获得了巨大的流行。然而，与传统软件一样，ML应用也不免于由程序错误导致的错误。显式的编程错误通常通过错误消息和堆栈跟踪表现出来。这些堆栈跟踪描述了导致异常情况的函数调用链。事实上，这些异常可能跨越整个软件堆栈（包括应用程序和库）。因此，研究堆栈跟踪中的模式可以帮助实践者和研究人员了解ML应用程序中异常的原因以及ML开发人员面临的挑战。为此，我们在Stack Overflow（SO）上进行挖掘并研究了与七个流行的Python ML库相关的11,449个堆栈跟踪。首先，我们观察到包含堆栈跟踪的ML问题比没有堆栈跟踪的问题更受欢迎；然而，它们更不可能得到接受。

    Machine learning (ML), including deep learning, has recently gained tremendous popularity in a wide range of applications. However, like traditional software, ML applications are not immune to the bugs that result from programming errors. Explicit programming errors usually manifest through error messages and stack traces. These stack traces describe the chain of function calls that lead to an anomalous situation, or exception. Indeed, these exceptions may cross the entire software stack (including applications and libraries). Thus, studying the patterns in stack traces can help practitioners and researchers understand the causes of exceptions in ML applications and the challenges faced by ML developers. To that end, we mine Stack Overflow (SO) and study 11,449 stack traces related to seven popular Python ML libraries. First, we observe that ML questions that contain stack traces gain more popularity than questions without stack traces; however, they are less likely to get accepted ans
    
[^35]: 一种多分辨率上下文网络和对抗性学习的视网膜血管分割方法

    Retinal Vessel Segmentation via a Multi-resolution Contextual Network and Adversarial Learning. (arXiv:2304.12856v1 [eess.IV])

    [http://arxiv.org/abs/2304.12856](http://arxiv.org/abs/2304.12856)

    本文提出了一种多分辨率上下文网络和对抗性学习相结合的方法来进行准确的视网膜血管分割，该方法具有高分割性能和较少的可训练参数数量。

    

    快速、普惠的计算机辅助诊断对于避免失明非常关键，而准确的视网膜血管分割在这些威胁视力疾病的疾病进展和诊断中扮演着重要角色。本文提出了一种多分辨率上下文网络和对抗性学习的方法（MRC-Net）来解决这些问题，通过提取多尺度特征学习语义不同的特征间的上下文依赖关系，并使用双向循环学习来建模前后和后前依赖关系。另一个关键思想是采用对抗性训练来通过区域得分的优化提高前景分割的性能。这种新颖的策略在保持可训练参数数量较低的同时，提高了分割网络的Dice得分（以及相应的Jaccard指数）的性能。我们在包括DRIVE、STARE和CHASE在内的三个基准数据集上评估了本方法，证明了其有效性。

    Timely and affordable computer-aided diagnosis of retinal diseases is pivotal in precluding blindness. Accurate retinal vessel segmentation plays an important role in disease progression and diagnosis of such vision-threatening diseases. To this end, we propose a Multi-resolution Contextual Network (MRC-Net) that addresses these issues by extracting multi-scale features to learn contextual dependencies between semantically different features and using bi-directional recurrent learning to model former-latter and latter-former dependencies. Another key idea is training in adversarial settings for foreground segmentation improvement through optimization of the region-based scores. This novel strategy boosts the performance of the segmentation network in terms of the Dice score (and correspondingly Jaccard index) while keeping the number of trainable parameters comparatively low. We have evaluated our method on three benchmark datasets, including DRIVE, STARE, and CHASE, demonstrating its 
    
[^36]: 数字健康孪生使用案例的自适应服务功能链编排: 启发式增强的Q学习方法

    Adaptive Services Function Chain Orchestration For Digital Health Twin Use Cases: Heuristic-boosted Q-Learning Approach. (arXiv:2304.12853v1 [cs.NI])

    [http://arxiv.org/abs/2304.12853](http://arxiv.org/abs/2304.12853)

    本文提出了一种用于数字健康孪生系统的自适应服务功能链编排方法，能够根据不同数据共享场景提供安全策略，并考虑到预期的用例、策略和基础设施配置，动态地提供服务编排和路由配置。

    

    数字孪生(DT)是利用和部署在医疗保健行业的突出技术。然而，此类应用面临的主要挑战是: 严格的医疗数据共享政策、高性能网络要求和可能的基础设施资源限制。本文通过提供自适应虚拟网络功能(VNF)，以执行与不同数据共享场景相关的安全策略，来解决所有这些挑战。我们在多节点集群网格基础设施上定义了一个云原生网络编排器，用于灵活和动态的容器调度。所提出的框架考虑了预期的数据共享用例、相关策略和基础设施配置，然后配置服务功能链(SFC)并相应地提供路由配置，几乎没有人为干预。此外，部署SFC时的“最优”取决于用例本身，我们调整超参数以优先考虑用例的贡献。

    Digital Twin (DT) is a prominent technology to utilise and deploy within the healthcare sector. Yet, the main challenges facing such applications are: Strict health data-sharing policies, high-performance network requirements, and possible infrastructure resource limitations. In this paper, we address all the challenges by provisioning adaptive Virtual Network Functions (VNFs) to enforce security policies associated with different data-sharing scenarios. We define a Cloud-Native Network orchestrator on top of a multi-node cluster mesh infrastructure for flexible and dynamic container scheduling. The proposed framework considers the intended data-sharing use case, the policies associated, and infrastructure configurations, then provision Service Function Chaining (SFC) and provides routing configurations accordingly with little to no human intervention. Moreover, what is \textit{optimal} when deploying SFC is dependent on the use case itself, and we tune the hyperparameters to prioritis
    
[^37]: （本地）差分隐私对公平性没有带来不平等影响

    (Local) Differential Privacy has NO Disparate Impact on Fairness. (arXiv:2304.12845v1 [cs.LG])

    [http://arxiv.org/abs/2304.12845](http://arxiv.org/abs/2304.12845)

    本文研究了在 LDP 下收集多个敏感属性对公平性的影响并提出了考虑域大小的新的隐私预算分配方案，实验表明该方案在隐私、效用和公平性方面均优于最新的解决方案，LDP 带来了略微改善的公平性而不会明显影响性能。

    

    近年来，本地差分隐私（LDP）作为强大隐私保护方法，在实际应用中得到了广泛的应用。通过 LDP，用户可以在将数据传输出去前在设备上对其进行扰动。然而，随着在各个行业中收集多个敏感信息的情况越来越普遍，仅收集单个敏感属性可能已经不足以保障用户的隐私。数据中的相关属性仍然可能导致对敏感属性的推断。本文通过实验证明了在 LDP 下收集多个敏感属性对公平性的影响。我们提出了一个新的隐私预算分配方案，考虑到敏感属性的不同域大小。在我们的实验中，这通常比现有最新解决方案的隐私-效用-公平性权衡方式更好。我们的结果表明，LDP 在学习问题中带来了略微改善的公平性，而不会明显影响性能。

    In recent years, Local Differential Privacy (LDP), a robust privacy-preserving methodology, has gained widespread adoption in real-world applications. With LDP, users can perturb their data on their devices before sending it out for analysis. However, as the collection of multiple sensitive information becomes more prevalent across various industries, collecting a single sensitive attribute under LDP may not be sufficient. Correlated attributes in the data may still lead to inferences about the sensitive attribute. This paper empirically studies the impact of collecting multiple sensitive attributes under LDP on fairness. We propose a novel privacy budget allocation scheme that considers the varying domain size of sensitive attributes. This generally led to a better privacy-utility-fairness trade-off in our experiments than the state-of-art solution. Our results show that LDP leads to slightly improved fairness in learning problems without significantly affecting the performance of the
    
[^38]: 一种新的用于机器学习的确定性信息理论

    A New Information Theory of Certainty for Machine Learning. (arXiv:2304.12833v1 [cs.IT])

    [http://arxiv.org/abs/2304.12833](http://arxiv.org/abs/2304.12833)

    该论文提出了一种新的信息理论概念 troenpy 来量化底层分布的确定性，用于机器学习中文档分类和序列数据权重方案，并定义了量子 troenpy 量化量子系统确定性。

    

    克劳德·香农提出了熵的概念来量化通信编码理论中随机分布的不确定性。我们观察到熵的这种不确定性特性也限制了其在数学建模中的直接使用。因此，我们提出了一个新概念 troenpy，作为熵的规范对偶，来量化底层分布的确定性。我们展示了在机器学习中的两个应用。第一个是用于传统的文档分类，我们开发了一个基于 troenpy 权重方案来利用文档分类标签。第二个是针对序列数据的自我 troenpy 权重方案，并表明它可以轻松地包含在基于神经网络的语言模型中，并实现显著的困惑度降低。我们还定义了量子 troenpy 作为 Von Neumann 熵的对偶，以量化量子系统的确定性。

    Claude Shannon coined entropy to quantify the uncertainty of a random distribution for communication coding theory. We observe that the uncertainty nature of entropy also limits its direct usage in mathematical modeling. Therefore we propose a new concept troenpy,as the canonical dual of entropy, to quantify the certainty of the underlying distribution. We demonstrate two applications in machine learning. The first is for the classical document classification, we develop a troenpy based weighting scheme to leverage the document class label. The second is a self-troenpy weighting scheme for sequential data and show that it can be easily included in neural network based language models and achieve dramatic perplexity reduction. We also define quantum troenpy as the dual of the Von Neumann entropy to quantify the certainty of quantum systems.
    
[^39]: 用深度量化神经网络提高对抗攻击的鲁棒性

    Improving Robustness Against Adversarial Attacks with Deeply Quantized Neural Networks. (arXiv:2304.12829v1 [cs.LG])

    [http://arxiv.org/abs/2304.12829](http://arxiv.org/abs/2304.12829)

    本文研究了在深度量化神经网络中采用自动量化训练框架和深度量化误差计算等方法来提高对抗攻击的鲁棒性。

    

    减小机器学习（ML）模型尤其是深度神经网络（DNNs）的存储空间，是将它们部署到资源受限的小型设备中的重要前提。然而，DNN模型的一个缺点是它们易受到对抗攻击的影响，因为在输入中添加轻微的扰动可以欺骗模型。因此，挑战在于如何创建准确、稳健并且可部署在资源受限嵌入式设备上的小型DNN模型。本文通过采用一种自动量化训练框架QKeras训练一个小型DNN模型，该模型对白盒和黑盒对抗攻击具有鲁棒性，并在学习过程中计算深度量化误差，从而提高了设计的DNN模型的精度，使它适用于小型设备的部署。我们研究了QKeras和一种对抗鲁棒性技术Jacobian Regularization（JR）如何通过利用DNN拓扑结构和每层JR方法提供一种共同优化策略，以提高深度量化神经网络对抗攻击的鲁棒性。

    Reducing the memory footprint of Machine Learning (ML) models, particularly Deep Neural Networks (DNNs), is essential to enable their deployment into resource-constrained tiny devices. However, a disadvantage of DNN models is their vulnerability to adversarial attacks, as they can be fooled by adding slight perturbations to the inputs. Therefore, the challenge is how to create accurate, robust, and tiny DNN models deployable on resource-constrained embedded devices. This paper reports the results of devising a tiny DNN model, robust to adversarial black and white box attacks, trained with an automatic quantizationaware training framework, i.e. QKeras, with deep quantization loss accounted in the learning loop, thereby making the designed DNNs more accurate for deployment on tiny devices. We investigated how QKeras and an adversarial robustness technique, Jacobian Regularization (JR), can provide a co-optimization strategy by exploiting the DNN topology and the per layer JR approach to 
    
[^40]: GraphVF: 用变分流控制生成特定蛋白质的三维分子

    GraphVF: Controllable Protein-Specific 3D Molecule Generation with Variational Flow. (arXiv:2304.12825v1 [q-bio.BM])

    [http://arxiv.org/abs/2304.12825](http://arxiv.org/abs/2304.12825)

    GraphVF是一种新的分子生成框架，它将2D拓扑和3D几何结合起来，用于可控生成具有结合能力的3D分子，是第一个可以生成结合能力的3D分子并定制其物化特性的方法。

    

    设计与特定目标蛋白质结合的分子是药物发现中的基本任务。最近的模型利用几何约束条件生成能够与特定蛋白质口袋紧密结合的配体分子。然而，这些模型不能有效地生成具有2D骨架退化和物性限制的3D分子，这对于药物的效能和开发至关重要。为了克服这一挑战，我们提出了GraphVF，一种基于变分流的框架，将2D拓扑和3D几何结合起来，用于可控生成具有结合能力的3D分子。实验表明，我们的方法实现了最先进的结合亲和力和特定蛋白质生成的实际次级结构布局。尤其是，GraphVF代表了第一个可控的几何意识，特定蛋白质分子生成方法，可以生成具有定制次级结构和物理化学性质的结合3D分子。我们的代码可在https://github.com/Franco-Solis/GraphVF-code中获得。

    Designing molecules that bind to specific target proteins is a fundamental task in drug discovery. Recent models leverage geometric constraints to generate ligand molecules that bind cohesively with specific protein pockets. However, these models cannot effectively generate 3D molecules with 2D skeletal curtailments and property constraints, which are pivotal to drug potency and development. To tackle this challenge, we propose GraphVF, a variational flow-based framework that combines 2D topology and 3D geometry, for controllable generation of binding 3D molecules. Empirically, our method achieves state-of-the-art binding affinity and realistic sub-structural layouts for protein-specific generation. In particular, GraphVF represents the first controllable geometry-aware, protein-specific molecule generation method, which can generate binding 3D molecules with tailored sub-structures and physio-chemical properties. Our code is available at https://github.com/Franco-Solis/GraphVF-code.
    
[^41]: 基于对比能量预测的确切能量引导扩散采样在离线强化学习中的应用

    Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling in Offline Reinforcement Learning. (arXiv:2304.12824v1 [cs.LG])

    [http://arxiv.org/abs/2304.12824](http://arxiv.org/abs/2304.12824)

    本文提出了一种基于对比能量预测的确切能量引导扩散采样方法，用于离线强化学习中，可以解决中间引导估计的难题，并在D4RL基准测试上取得了优异的效果。

    

    引导采样是将扩散模型应用于现实任务的重要方法，其在采样过程中嵌入人类定义的指导。本文考虑了一般设置，其中指导是由一个（非标准化）能量函数定义的。这种设置的主要挑战是，在扩散采样过程中的中间指导是由采样分布和能量函数共同定义的，很难估计。为了解决这个挑战，我们提出了一个精确的中间指导配方以及一种名为对比能量预测（CEP）的新型训练目标来学习精确的中间指导。我们的方法在无限制的模型能力和数据样本下保证收敛到精确指导，而之前的方法无法做到。我们将其应用于离线强化学习中，并在D4RL基准测试上进行了广泛的实验，证明了我们的方法优于之前的最先进方法。

    Guided sampling is a vital approach for applying diffusion models in real-world tasks that embeds human-defined guidance during the sampling procedure. This paper considers a general setting where the guidance is defined by an (unnormalized) energy function. The main challenge for this setting is that the intermediate guidance during the diffusion sampling procedure, which is jointly defined by the sampling distribution and the energy function, is unknown and is hard to estimate. To address this challenge, we propose an exact formulation of the intermediate guidance as well as a novel training objective named contrastive energy prediction (CEP) to learn the exact guidance. Our method is guaranteed to converge to the exact guidance under unlimited model capacity and data samples, while previous methods can not. We demonstrate the effectiveness of our method by applying it to offline reinforcement learning (RL). Extensive experiments on D4RL benchmarks demonstrate that our method outperf
    
[^42]: 一种新型的Shannon信息及加权方案的对偶

    A Novel Dual of Shannon Information and Weighting Scheme. (arXiv:2304.12814v1 [cs.CL])

    [http://arxiv.org/abs/2304.12814](http://arxiv.org/abs/2304.12814)

    本文通过发掘信息熵自然对偶，提出了一种新的量troenpy，并应用于提出了基于troenpy的文档加权方案，即正类别频率（PCF），以及一种新的类别信息偏差特征ECIB，在监督学习中具有互信息的泛化性质。

    

    Shannon信息理论不仅在通信技术领域，其应用还拓展至机器学习和人工智能领域。本文发掘信息熵存在自然对偶，并引入了一种新的量troenpy，用于衡量底层分布的确定性、普遍性和相似性。我们提出了基于troenpy的文档加权方案，即正类别频率（PCF），并证明其在公共数据集上的优越性。此外，我们还开发了一种新的类别信息偏差特征ECIB，在监督学习中具有互信息的泛化性质。

    Shannon Information theory has achieved great success in not only communication technology where it was originally developed for but also many other science and engineering fields such as machine learning and artificial intelligence. Inspired by the famous weighting scheme TF-IDF, we discovered that information entropy has a natural dual. We complement the classical Shannon information theory by proposing a novel quantity, namely troenpy. Troenpy measures the certainty, commonness and similarity of the underlying distribution. To demonstrate its usefulness, we propose a troenpy based weighting scheme for document with class labels, namely positive class frequency (PCF). On a collection of public datasets we show the PCF based weighting scheme outperforms the classical TF-IDF and a popular Optimal Transportation based word moving distance algorithm in a kNN setting. We further developed a new odds-ratio type feature, namely Expected Class Information Bias(ECIB), which can be regarded as
    
[^43]: 超越“男性准则”：NLP语境中的隐性男性偏见

    Transcending the "Male Code": Implicit Masculine Biases in NLP Contexts. (arXiv:2304.12810v1 [cs.CL])

    [http://arxiv.org/abs/2304.12810](http://arxiv.org/abs/2304.12810)

    研究发现，当存在性别化语言时，NLP语境中也存在着性别偏见，尤其是男性偏见。调查者提供了一个涵盖了性别化语言与语言之间歧义关系的新字典“Ava”。

    

    关于虚拟助手（VAs）的性别偏差问题，批判性学说已经提高了人们的注意。大部分研究集中在语言中的显性偏见，尤其是针对女性、女孩、女性认同人群和性别酷儿的歧视，以及通过词向量嵌入的隐性关联；而对于男性和毒性男性，性别和性别二元分类的混为一谈，很少有基于男性和男性气概的有限模型。然而，我们还必须质询如何将男性气概“编码”到语言中及其将“男性”作为语言默认值的假设：隐性男性偏见。为此，我们调查了两个自然语言处理（NLP）数据集。我们发现当存在性别化语言时，性别偏见尤其是男性偏见也存在。此外，这些偏见与NLP上下文的关系细微且相关。我们提供了一个名为AVA的新字典，涵盖了性别化语言与语言之间的歧义关系。

    Critical scholarship has elevated the problem of gender bias in data sets used to train virtual assistants (VAs). Most work has focused on explicit biases in language, especially against women, girls, femme-identifying people, and genderqueer folk; implicit associations through word embeddings; and limited models of gender and masculinities, especially toxic masculinities, conflation of sex and gender, and a sex/gender binary framing of the masculine as diametric to the feminine. Yet, we must also interrogate how masculinities are "coded" into language and the assumption of "male" as the linguistic default: implicit masculine biases. To this end, we examined two natural language processing (NLP) data sets. We found that when gendered language was present, so were gender biases and especially masculine biases. Moreover, these biases related in nuanced ways to the NLP context. We offer a new dictionary called AVA that covers ambiguous associations between gendered language and the langua
    
[^44]: 分布式强化学习中的损失和奖励加权

    Loss and Reward Weighing for increased learning in Distributed Reinforcement Learning. (arXiv:2304.12778v1 [cs.LG])

    [http://arxiv.org/abs/2304.12778](http://arxiv.org/abs/2304.12778)

    本文提出了两种分布式强化学习方法，奖励加权和损失加权梯度合并，以更好地提高分布式代理的学习效果。

    

    本文介绍了两种强化学习（RL）环境中分布式代理的学习方案，即奖励加权（R-Weighted）和损失加权（L-Weighted）梯度合并。 R / L 加权方法替换了训练多个代理的标准实践，例如对梯度求和或平均。每个代理在不同初始化版本的相同环境中运行，这会从不同的actor获得不同的梯度。

    This paper introduces two learning schemes for distributed agents in Reinforcement Learning (RL) environments, namely Reward-Weighted (R-Weighted) and Loss-Weighted (L-Weighted) gradient merger. The R/L weighted methods replace standard practices for training multiple agents, such as summing or averaging the gradients. The core of our methods is to scale the gradient of each actor based on how high the reward (for R-Weighted) or the loss (for L-Weighted) is compared to the other actors. During training, each agent operates in differently initialized versions of the same environment, which gives different gradients from different actors. In essence, the R-Weights and L-Weights of each agent inform the other agents of its potential, which again reports which environment should be prioritized for learning. This approach of distributed learning is possible because environments that yield higher rewards, or low losses, have more critical information than environments that yield lower reward
    
[^45]: 基于反Lipschitz约束的解码器网络控制后验坍塌

    Controlling Posterior Collapse by an Inverse Lipschitz Constraint on the Decoder Network. (arXiv:2304.12770v1 [cs.LG])

    [http://arxiv.org/abs/2304.12770](http://arxiv.org/abs/2304.12770)

    本文提出了一种基于反Lipschitz约束的解码器网络，可以简单明了地控制广泛的VAE模型的后验坍塌程度，并带有具体的理论保证。

    

    变分自编码器（VAE）是深度生成模型中取得巨大成功的一种。然而，在实践中，它们存在一个称为后验坍塌的问题，当编码器与没有考虑输入数据的潜在结构的先验重合或坍塌时就会发生。本文介绍了一种基于反Lipschitz神经网络的解码器，基于这个架构，提供了一种新方法，可以简单明了地控制广泛的VAE模型的后验坍塌程度，并带有具体的理论保证。我们还通过几个数值实验证明了我们方法的有效性。

    Variational autoencoders (VAEs) are one of the deep generative models that have experienced enormous success over the past decades. However, in practice, they suffer from a problem called posterior collapse, which occurs when the encoder coincides, or collapses, with the prior taking no information from the latent structure of the input data into consideration. In this work, we introduce an inverse Lipschitz neural network into the decoder and, based on this architecture, provide a new method that can control in a simple and clear manner the degree of posterior collapse for a wide range of VAE models equipped with a concrete theoretical guarantee. We also illustrate the effectiveness of our method through several numerical experiments.
    
[^46]: 从损失函数中解耦分位数表达式

    Decoupling Quantile Representations from Loss Functions. (arXiv:2304.12766v1 [cs.LG])

    [http://arxiv.org/abs/2304.12766](http://arxiv.org/abs/2304.12766)

    本文提出了同时二元量化回归（SBQR）中分位数与预测概率的二元对偶性，使分位数表达式在不同tau值下的构造不再依赖于损失函数，从而在处理检测超出分布样本和调整模型方面表现突出。

    

    同时量化回归（SQR）技术用于估计深度学习模型的不确定性，但其应用受限于要求中位数分位数（τ = 0.5）处的解决方案必须最小化平均绝对误差（MAE）。本文通过展示同时二元量化回归（SBQR）中分位数与预测概率的二元对偶性来解决此问题。这使我们能够从损失函数中解耦分位数表达式的构造，使我们能够在中位分位数处分配任意分类器f(x)，并生成不同τ值的完整SBQR分位数表示的全谱。我们通过两个应用程序验证了我们的方法：（i）检测超出分布样本，其中我们显示分位数表示优于标准概率输出；（ii）调整模型，在这里，我们证明了分位数表示对失真的鲁棒性。

    The simultaneous quantile regression (SQR) technique has been used to estimate uncertainties for deep learning models, but its application is limited by the requirement that the solution at the median quantile ({\tau} = 0.5) must minimize the mean absolute error (MAE). In this article, we address this limitation by demonstrating a duality between quantiles and estimated probabilities in the case of simultaneous binary quantile regression (SBQR). This allows us to decouple the construction of quantile representations from the loss function, enabling us to assign an arbitrary classifier f(x) at the median quantile and generate the full spectrum of SBQR quantile representations at different {\tau} values. We validate our approach through two applications: (i) detecting out-of-distribution samples, where we show that quantile representations outperform standard probability outputs, and (ii) calibrating models, where we demonstrate the robustness of quantile representations to distortions. 
    
[^47]: 节点特征增强改进网络对齐

    Node Feature Augmentation Vitaminizes Network Alignment. (arXiv:2304.12751v1 [cs.SI])

    [http://arxiv.org/abs/2304.12751](http://arxiv.org/abs/2304.12751)

    本研究提出了Grad-Align+方法，通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法，解决了NA方法缺乏额外信息的问题。

    

    网络对齐（NA）是通过给定网络的拓扑和/或特征信息来发现多个网络之间的节点对应关系的任务。虽然NA方法在各种场景下取得了显著的成功，但其有效性并不总是有额外信息，如先前的锚点链接和/或节点特征。为了解决这个实际的挑战，我们提出了Grad-Align+，这是一种新颖的NA方法，建立在最近一种最先进的NA方法Grad-Align之上，Grad-Align+仅逐步发现部分节点对，直到找到所有节点对。在设计Grad-Align+时，我们考虑如何通过增强节点特征来执行NA任务，并最大限度地利用增强的节点特征来设计NA方法。为了实现这个目标，我们开发了由三个关键组成部分组成的Grad-Align+：基于中心性的节点特征增强（CNFA）、图切片生成和优化节点嵌入特征（ONIFE）。

    Network alignment (NA) is the task of discovering node correspondences across multiple networks using topological and/or feature information of given networks. Although NA methods have achieved remarkable success in a myriad of scenarios, their effectiveness is not without additional information such as prior anchor links and/or node features, which may not always be available due to privacy concerns or access restrictions. To tackle this practical challenge, we propose Grad-Align+, a novel NA method built upon a recent state-of-the-art NA method, the so-called Grad-Align, that gradually discovers only a part of node pairs until all node pairs are found. In designing Grad-Align+, we account for how to augment node features in the sense of performing the NA task and how to design our NA method by maximally exploiting the augmented node features. To achieve this goal, we develop Grad-Align+ consisting of three key components: 1) centrality-based node feature augmentation (CNFA), 2) graph
    
[^48]: 基于深度学习的数据库管理系统自动调优

    Deep learning based Auto Tuning for Database Management System. (arXiv:2304.12747v1 [cs.DB])

    [http://arxiv.org/abs/2304.12747](http://arxiv.org/abs/2304.12747)

    本文扩展了一个基于Ottertune自动技术的方法，利用监督和无监督机器学习方法重用从先前会议中收集的训练数据来调整新的DBMS部署，以提高延迟预测的精度。

    

    数据库系统配置的管理是一项具有挑战性的任务，因为有数百种配置开关来控制系统的各个方面。这种情况复杂性增加的原因在于这些开关没有标准化、独立性也不一样，并没有通用的标准，因此很难确定最佳的设置。为了解决这个问题，OtterTune是一个基于监督和无监督机器学习方法来选择有影响性的开关，映射未见过的工作量并建议开关设置的新工具，并已在三个DBMS上进行了评估，结果表明它推荐的配置与现有工具或人工专家生成的配置一样好或更好。在这项工作中，我们扩展了一个基于Ottertune的自动技术，利用监督和无监督机器学习方法重用从先前会议中收集的训练数据来调整新的DBMS部署，以提高延迟预测的精度。

    The management of database system configurations is a challenging task, as there are hundreds of configuration knobs that control every aspect of the system. This is complicated by the fact that these knobs are not standardized, independent, or universal, making it difficult to determine optimal settings. An automated approach to address this problem using supervised and unsupervised machine learning methods to select impactful knobs, map unseen workloads, and recommend knob settings was implemented in a new tool called OtterTune and is being evaluated on three DBMSs, with results demonstrating that it recommends configurations as good as or better than those generated by existing tools or a human expert.In this work, we extend an automated technique based on Ottertune [1] to reuse training data gathered from previous sessions to tune new DBMS deployments with the help of supervised and unsupervised machine learning methods to improve latency prediction. Our approach involves the expan
    
[^49]: ICU创伤患者早期脓毒症发作预测的夜间个人档案表示学习

    NPRL: Nightly Profile Representation Learning for Early Sepsis Onset Prediction in ICU Trauma Patients. (arXiv:2304.12737v1 [cs.LG])

    [http://arxiv.org/abs/2304.12737](http://arxiv.org/abs/2304.12737)

    本文提出了一种基于夜间个人档案表示学习和深度学习框架的方法，可以提前预测创伤患者的脓毒症发作，这种方法优于现有的最先进方法。

    

    脓毒症是一种源于感染，以严重器官功能障碍为特征的综合症，并且是全球重症监护病房(ICU)死亡率的主要原因之一。通过早期应用抗生素可以减少这些并发症，因此预测脓毒症的发作时间对患者的生存和福祉至关重要。当前在医疗基础设施内部部署的机器学习算法表现不佳，不足以早期预测脓毒症的发生。因此，本文首先提出了一种基于患者生理和临床数据的夜间个人档案表示学习方法(NPRL)，以捕捉患者状态随时间动态改变的情况。然后使用深度学习框架预测这些患者的脓毒症发作时间，并超越现有的最先进方法。

    Sepsis is a syndrome that develops in response to the presence of infection. It is characterized by severe organ dysfunction and is one of the leading causes of mortality in Intensive Care Units (ICUs) worldwide. These complications can be reduced through early application of antibiotics, hence the ability to anticipate the onset of sepsis early is crucial to the survival and well-being of patients. Current machine learning algorithms deployed inside medical infrastructures have demonstrated poor performance and are insufficient for anticipating sepsis onset early. In recent years, deep learning methodologies have been proposed to predict sepsis, but some fail to capture the time of onset (e.g., classifying patients' entire visits as developing sepsis or not) and others are unrealistic to be deployed into medical facilities (e.g., creating training instances using a fixed time to onset where the time of onset needs to be known apriori). Therefore, in this paper, we first propose a nove
    
[^50]: 使用梯度提升方法对星系外射电源进行形态分类

    Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods. (arXiv:2304.12729v1 [astro-ph.IM])

    [http://arxiv.org/abs/2304.12729](http://arxiv.org/abs/2304.12729)

    本文提出了基于梯度提升机器学习方法和主成分分析的自动分类方法，用于解决星系外射电源形态分类问题。实验结果表明，在具有表格数据的分类问题中，该方法优于卷积神经网络。

    

    随着新的射电望远镜的建成，无线电天文学的数据量急剧增加。形态分类脱星系外射电源的自动分类是该领域中最关键的问题之一。大部分关于该领域的最近的成果都是基于卷积神经网络的分类器。而本文则提出了梯度提升机器学习方法和主成分分析作为卷积神经网络计算成本较低的替代方法。最近的研究发现，在具有表格数据的分类问题中，梯度提升方法的有效性优于深度学习方法。本文研究了基于XGBoost，LightGBM和CatBoost实现的梯度提升方法对数据集大小的分类器性能的影响。该分类器将星系外射电源分为三个类别。

    The field of radio astronomy is witnessing a boom in the amount of data produced per day due to newly commissioned radio telescopes. One of the most crucial problems in this field is the automatic classification of extragalactic radio sources based on their morphologies. Most recent contributions in the field of morphological classification of extragalactic radio sources have proposed classifiers based on convolutional neural networks. Alternatively, this work proposes gradient boosting machine learning methods accompanied by principal component analysis as data-efficient alternatives to convolutional neural networks. Recent findings have shown the efficacy of gradient boosting methods in outperforming deep learning methods for classification problems with tabular data. The gradient boosting methods considered in this work are based on the XGBoost, LightGBM, and CatBoost implementations. This work also studies the effect of dataset size on classifier performance. A three-class classifi
    
[^51]: 基于双重交叉关注的眼动追踪引导深度多实例学习在眼底疾病检测中的应用

    Eye tracking guided deep multiple instance learning with dual cross-attention for fundus disease detection. (arXiv:2304.12719v1 [eess.IV])

    [http://arxiv.org/abs/2304.12719](http://arxiv.org/abs/2304.12719)

    本文提出了一种基于眼动追踪信息的人机协同CAD系统，在多实例学习中引入双重交叉关注MIL（DCAMIL）网络以提高诊断准确性，同时使用序列增强和域对抗模块增强方法的鲁棒性。

    

    深度神经网络促进了计算机辅助诊断（CAD）系统的发展，有助于眼科医生减少漏诊和误诊率。本文提出了一种基于眼动追踪信息的人机协同（HITL）CAD系统，具有更高的效率和准确性。该系统应用于多实例学习（MIL）中，其中眼动追踪注视图有助于筛选与诊断相关的实例。此外，本文提出了一种新的双重交叉关注MIL（DCAMIL）网络，以遏制噪声实例的不良影响。同时，引入序列增强模块和域对抗模块，分别丰富和标准化训练包中的实例，从而增强了方法的鲁棒性。

    Deep neural networks (DNNs) have promoted the development of computer aided diagnosis (CAD) systems for fundus diseases, helping ophthalmologists reduce missed diagnosis and misdiagnosis rate. However, the majority of CAD systems are data-driven but lack of medical prior knowledge which can be performance-friendly. In this regard, we innovatively proposed a human-in-the-loop (HITL) CAD system by leveraging ophthalmologists' eye-tracking information, which is more efficient and accurate. Concretely, the HITL CAD system was implemented on the multiple instance learning (MIL), where eye-tracking gaze maps were beneficial to cherry-pick diagnosis-related instances. Furthermore, the dual-cross-attention MIL (DCAMIL) network was utilized to curb the adverse effects of noisy instances. Meanwhile, both sequence augmentation module and domain adversarial module were introduced to enrich and standardize instances in the training bag, respectively, thereby enhancing the robustness of our method. 
    
[^52]: 学习鲁棒的深度平衡模型

    Learning Robust Deep Equilibrium Models. (arXiv:2304.12707v1 [cs.LG])

    [http://arxiv.org/abs/2304.12707](http://arxiv.org/abs/2304.12707)

    本论文提出一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性以抵抗微小的初始扰动，并在不同的固定点之间加入全连接层以避免不良对抗性防御。

    

    深度平衡(DEQ)模型已成为深度学习中一种有前途的隐式层模型，它通过解决单个非线性层的固定点来放弃了传统深度。尽管这些模型很成功，但对于这些模型的固定点的稳定性仍然知之甚少。最近，将Lyapunov理论应用于另一种类型的隐式层模型——神经ODE，可以赋予其对抗鲁棒性。通过将DEQ模型视为非线性动态系统，我们提出了一种名为LyaDEQ的鲁棒DEQ模型，通过Lyapunov理论提供了保证的稳定性。我们方法的关键是确保DEQ模型的固定点是Lyapunov稳定的，这使得LyaDEQ模型能够抵抗微小的初始扰动。为了避免由于Lyapunov稳定的固定点彼此靠近而导致的不良对抗性防御，我们在Lyapunov稳定性模块之后加入了一个正交的全连接层，以分离不同的固定点。我们在各种基准上评估了LyaDEQ模型。

    Deep equilibrium (DEQ) models have emerged as a promising class of implicit layer models in deep learning, which abandon traditional depth by solving for the fixed points of a single nonlinear layer. Despite their success, the stability of the fixed points for these models remains poorly understood. Recently, Lyapunov theory has been applied to Neural ODEs, another type of implicit layer model, to confer adversarial robustness. By considering DEQ models as nonlinear dynamic systems, we propose a robust DEQ model named LyaDEQ with guaranteed provable stability via Lyapunov theory. The crux of our method is ensuring the fixed points of the DEQ models are Lyapunov stable, which enables the LyaDEQ models to resist the minor initial perturbations. To avoid poor adversarial defense due to Lyapunov-stable fixed points being located near each other, we add an orthogonal fully connected layer after the Lyapunov stability module to separate different fixed points. We evaluate LyaDEQ models on se
    
[^53]: 基于泄漏波全息技术的轨道角动量发生器的深度学习设计框架

    Deep Learning Framework for the Design of Orbital Angular Momentum Generators Enabled by Leaky-wave Holograms. (arXiv:2304.12695v1 [physics.optics])

    [http://arxiv.org/abs/2304.12695](http://arxiv.org/abs/2304.12695)

    本文介绍了一种利用平面光学和机器学习技术相结合的泄漏波全息天线设计方法，可生成携带轨道角动量的电磁波，并使用机器学习发现数学函数，精确控制辐射模式，提高系统性能。

    

    本文介绍了一种新型的利用平面光学和机器学习技术相结合的泄漏波全息天线设计方法，该方法可生成携带轨道角动量的电磁波。为了提高系统性能，我们使用机器学习技术来发现一种数学函数，能够有效地控制整个辐射模式，即在降低边瓣电平的同时增加辐射模式的中央空洞深度。需要根据全息理论精确调节阻抗方程的参数才能在各种情况下实现最佳结果。在这项研究中，我们应用机器学习来确定参数的近似值。我们可以利用77,000个生成的数据集确定每个参数的最佳值，从而得到所期望的辐射模式。此外，使用机器学习不仅节省时间，而且可以获得更精确的结果。

    In this paper, we present a novel approach for the design of leaky-wave holographic antennas that generates OAM-carrying electromagnetic waves by combining Flat Optics (FO) and machine learning (ML) techniques. To improve the performance of our system, we use a machine learning technique to discover a mathematical function that can effectively control the entire radiation pattern, i.e., decrease the side lobe level (SLL) while simultaneously increasing the central null depth of the radiation pattern. Precise tuning of the parameters of the impedance equation based on holographic theory is necessary to achieve optimal results in a variety of scenarios. In this research, we applied machine learning to determine the approximate values of the parameters. We can determine the optimal values for each parameter, resulting in the desired radiation pattern, using a total of 77,000 generated datasets. Furthermore, the use of ML not only saves time, but also yields more precise and accurate resul
    
[^54]: Phylo2Vec: 一种二叉树的向量表示方法

    Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])

    [http://arxiv.org/abs/2304.12693](http://arxiv.org/abs/2304.12693)

    Phylo2Vec是一种新的二叉树简明表示方法，它能够轻松采样二叉树，并以系统性的方法遍历树空间。这种方法用于构建深度神经网络，能够显著提高蛋白质类别预测的性能。

    

    从生物数据推断得到的二叉进化树对于理解生物之间共享的进化历史至关重要。根据最大似然等某个最优性准则推断出树中潜在节点的位置是NP-hard问题，这推动了大量启发式方法的发展。然而，这些启发式方法通常缺乏一种系统性的方法来均匀采样随机树或有效地探索指数级增长的树空间，这对于机器学习等优化问题至关重要。因此，我们提出了Phylo2Vec，这是一种新的简明表示方法来表示进化树。Phylo2Vec将任何具有n个叶子的二叉树映射到长度为n的整数向量。我们证明了Phylo2Vec在空间中既是良定的又是双射的。Phylo2Vec的优点是：i）轻松均匀采样二叉树；ii）以非常大或小的步长系统地遍历树空间。作为概念验证，我们使用Phylo2Vec构建了一个深度神经网络，以从氨基酸序列预测蛋白质类别。我们证明了Phylo2Vec显著提高了网络的性能，超过了之前的最优结果。

    Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
    
[^55]: 分布式鲁棒优化实现差分隐私保护

    Differential Privacy via Distributionally Robust Optimization. (arXiv:2304.12681v1 [cs.CR])

    [http://arxiv.org/abs/2304.12681](http://arxiv.org/abs/2304.12681)

    本文开发了一类机制，以实现无条件最优性保证的差分隐私。该机制将机制设计问题制定为无限维分布鲁棒优化问题。

    

    近年来，差分隐私已成为共享数据集统计信息并限制涉及个人的私人信息披露的事实标准。通过对将要发布的统计数据进行随机扰动来实现这一目标，这反过来导致了隐私和准确性之间的权衡：更大的扰动提供更强的隐私保证，但结果是提供较低实用度的统计数据和更低的准确性。因此，特别感兴趣的是在预选隐私水平的情况下提供最高准确性的最佳机制。迄今为止，这一领域的工作集中在事先指定扰动族并随后证明其渐近和/或最佳性上，本文则开发了一类机制，它们具有非渐近和无条件的最佳性保证。为此，我们将机制设计问题制定为无限维分布鲁棒优化问题。

    In recent years, differential privacy has emerged as the de facto standard for sharing statistics of datasets while limiting the disclosure of private information about the involved individuals. This is achieved by randomly perturbing the statistics to be published, which in turn leads to a privacy-accuracy trade-off: larger perturbations provide stronger privacy guarantees, but they result in less accurate statistics that offer lower utility to the recipients. Of particular interest are therefore optimal mechanisms that provide the highest accuracy for a pre-selected level of privacy. To date, work in this area has focused on specifying families of perturbations a priori and subsequently proving their asymptotic and/or best-in-class optimality. In this paper, we develop a class of mechanisms that enjoy non-asymptotic and unconditional optimality guarantees. To this end, we formulate the mechanism design problem as an infinite-dimensional distributionally robust optimization problem. W
    
[^56]: 受限通信加性高斯噪声下的多臂赌博机问题研究

    Communication-Constrained Bandits under Additive Gaussian Noise. (arXiv:2304.12680v1 [cs.LG])

    [http://arxiv.org/abs/2304.12680](http://arxiv.org/abs/2304.12680)

    本文研究了在受限通信和加性高斯噪声下的多臂赌博机问题，提出了一个多阶段赌博算法，并给出了信息理论下限。

    

    本文研究了一个分布式随机多臂赌博机,其中客户端根据相应的拉臂奖励提供受限通信反馈给学习者。在我们的设定下,客户端必须编码奖励，使得编码奖励的二阶矩不超过P，并且这个编码奖励会被方差为$\sigma^2$的加性高斯噪声所污染；学习者只能访问这个被污染的奖励。我们在这个设置中导出了任何方案的最小化后悔的信息论下限$\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$，其中 $ \mathtt{SNR} := \frac{P}{\sigma^2}$，$K$和$T$分别是臂数和时间长度。此外，我们提出了一个多阶段赌博算法$\mathtt{UE\text{-}UCB++}$，它可以将这个下限的值加上一个微小的可加性因子。$\mathtt{UE\text{-}UCB++}$在其初始阶段执行均匀探索，然后在后续阶段使用“上置信界”(UCB)算法。我们还展示了数值结果，表明在实际情况下需要这样的通信有效算法。

    We study a distributed stochastic multi-armed bandit where a client supplies the learner with communication-constrained feedback based on the rewards for the corresponding arm pulls. In our setup, the client must encode the rewards such that the second moment of the encoded rewards is no more than $P$, and this encoded reward is further corrupted by additive Gaussian noise of variance $\sigma^2$; the learner only has access to this corrupted reward. For this setting, we derive an information-theoretic lower bound of $\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ on the minimax regret of any scheme, where $ \mathtt{SNR} := \frac{P}{\sigma^2}$, and $K$ and $T$ are the number of arms and time horizon, respectively. Furthermore, we propose a multi-phase bandit algorithm, $\mathtt{UE\text{-}UCB++}$, which matches this lower bound to a minor additive factor. $\mathtt{UE\text{-}UCB++}$ performs uniform exploration in its initial phases and then utilizes the {\em upper confidence
    
[^57]: 最大编码速率降低下的句子表示压缩方法

    Compressing Sentence Representation with maximum Coding Rate Reduction. (arXiv:2304.12674v1 [cs.CL])

    [http://arxiv.org/abs/2304.12674](http://arxiv.org/abs/2304.12674)

    提出了一种在最大编码速率降低下的句子表示压缩方法，通过在句子表示模型Sentence-BERT中加入一个额外的学习投影层，并证明其可以在语义相关任务中获得与大型语言模型相当的结果。

    

    在大多数自然语言推理问题中，需要使用句子表示来进行语义检索任务。在近年来，预训练的大型语言模型已经相当有效地计算这些表示。这些模型产生高维句子嵌入。实际上存在大型和小型模型之间明显的性能差距。因此，由于空间和时间硬件限制，需要在使用较小模型(通常是大型语言模型的精简版本)时获得可比较的结果。在本文中，我们通过在最大编码速率降低(MCR2)目标的基础上学习额外的投影层，评估了句子表示模型Sentence-BERT的模型蒸馏，在这种方法中，MCR2是一种为了通用流形聚类而开发的新方法。我们证明，通过在复杂度和句子嵌入大小方面减小的新语言模型可以在语义相关任务中获得可比较的结果。

    In most natural language inference problems, sentence representation is needed for semantic retrieval tasks. In recent years, pre-trained large language models have been quite effective for computing such representations. These models produce high-dimensional sentence embeddings. An evident performance gap between large and small models exists in practice. Hence, due to space and time hardware limitations, there is a need to attain comparable results when using the smaller model, which is usually a distilled version of the large language model. In this paper, we assess the model distillation of the sentence representation model Sentence-BERT by augmenting the pre-trained distilled model with a projection layer additionally learned on the Maximum Coding Rate Reduction (MCR2)objective, a novel approach developed for general-purpose manifold clustering. We demonstrate that the new language model with reduced complexity and sentence embedding size can achieve comparable results on semantic
    
[^58]: CoDi: 混合类型表格生成的共同演化对比扩散模型

    CoDi: Co-evolving Contrastive Diffusion Models for Mixed-type Tabular Synthesis. (arXiv:2304.12654v1 [cs.LG])

    [http://arxiv.org/abs/2304.12654](http://arxiv.org/abs/2304.12654)

    CoDi 方法使用两个共同演化的对比扩散模型单独处理离散和连续变量并相互条件化，同时引入对比学习方法进行进一步的绑定，展现了在真实世界的表格数据集上的有效性。

    

    随着越来越多的注意力被放在表格数据上，将综合表格应用于各种任务的尝试已经向各种场景扩展。由于生成建模的最新进展，通过表格数据综合模型生成的虚假数据变得复杂而真实。但是，建模表格数据的离散变量（列）仍然存在困难。在本研究中，我们提出通过两个对比扩散模型单独处理连续和离散变量（但相互条件化）。两个扩散模型通过彼此读取条件在训练中共同演化。此外，为了进一步绑定扩散模型，我们引入了一个负采样的对比学习方法。在11个真实世界的表格数据集和8个基准方法的实验中，我们证明了所提出的方法 CoDi 的有效性。

    With growing attention to tabular data these days, the attempt to apply a synthetic table to various tasks has been expanded toward various scenarios. Owing to the recent advances in generative modeling, fake data generated by tabular data synthesis models become sophisticated and realistic. However, there still exists a difficulty in modeling discrete variables (columns) of tabular data. In this work, we propose to process continuous and discrete variables separately (but being conditioned on each other) by two diffusion models. The two diffusion models are co-evolved during training by reading conditions from each other. In order to further bind the diffusion models, moreover, we introduce a contrastive learning method with a negative sampling method. In our experiments with 11 real-world tabular datasets and 8 baseline methods, we prove the efficacy of the proposed method, called CoDi.
    
[^59]: 剪枝视觉模型中的偏差问题：深入分析与对策

    Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures. (arXiv:2304.12622v1 [cs.CV])

    [http://arxiv.org/abs/2304.12622](http://arxiv.org/abs/2304.12622)

    本文针对计算机视觉中常用的剪枝神经网络方法，系统分析了其可能引发的偏差问题，提出了可以降低偏差的标准。

    

    剪枝神经网络是一种常用的模型压缩方法，但最近的研究表明，剪枝可能会引起或加剧压缩模型输出的偏差。本文系统地研究和描述了这种现象在计算机视觉中的行为，并提出了一些易于使用的标准来帮助减少偏差。

    Pruning - that is, setting a significant subset of the parameters of a neural network to zero - is one of the most popular methods of model compression. Yet, several recent works have raised the issue that pruning may induce or exacerbate bias in the output of the compressed model. Despite existing evidence for this phenomenon, the relationship between neural network pruning and induced bias is not well-understood. In this work, we systematically investigate and characterize this phenomenon in Convolutional Neural Networks for computer vision. First, we show that it is in fact possible to obtain highly-sparse models, e.g. with less than 10% remaining weights, which do not decrease in accuracy nor substantially increase in bias when compared to dense models. At the same time, we also find that, at higher sparsities, pruned models exhibit higher uncertainty in their outputs, as well as increased correlations, which we directly link to increased bias. We propose easy-to-use criteria which
    
[^60]: 一种用于建模不确定和衰减滞后系统的双保真DeepONet方法

    A Bi-fidelity DeepONet Approach for Modeling Uncertain and Degrading Hysteretic Systems. (arXiv:2304.12609v1 [stat.ML])

    [http://arxiv.org/abs/2304.12609](http://arxiv.org/abs/2304.12609)

    本文介绍了一种用于建模不确定和退化滞后系统的双保真DeepONet方法，在不了解退化效应性质的原始模型数据集的情况下使用低保真度表示，显著提高了退化滞后系统的预测精度。

    

    引入不确定性后，非线性系统如滞后行为的退化通常出现在工程应用中，而建模这种系统变得越来越困难。另一方面，可以很容易地获取不了解退化效应性质的原始模型数据集。本文使用来自原始模型的数据集作为低保真度表示，以训练DeepONet。三个数值实例用于表明所提出的DeepONets的使用，以模拟低保真度模型和真实系统响应之间的偏差，在模型参数存在不确定性时，在退化滞后系统中预测误差显著降低。

    Nonlinear systems, such as with degrading hysteretic behavior, are often encountered in engineering applications. In addition, due to the ubiquitous presence of uncertainty and the modeling of such systems becomes increasingly difficult. On the other hand, datasets from pristine models developed without knowing the nature of the degrading effects can be easily obtained. In this paper, we use datasets from pristine models without considering the degrading effects of hysteretic systems as low-fidelity representations that capture many of the important characteristics of the true system's behavior to train a deep operator network (DeepONet). Three numerical examples are used to show that the proposed use of the DeepONets to model the discrepancies between the low-fidelity model and the true system's response leads to significant improvements in the prediction error in the presence of uncertainty in the model parameters for degrading hysteretic systems.
    
[^61]: 回归模型在预测医疗保险费用方面的表现评估

    Performance Evaluation of Regression Models in Predicting the Cost of Medical Insurance. (arXiv:2304.12605v1 [cs.LG])

    [http://arxiv.org/abs/2304.12605](http://arxiv.org/abs/2304.12605)

    该研究评估了三种回归模型在预测医疗保险费用方面的表现，提出了最重要的预测特征为被保险人的年龄。

    

    该研究旨在评估回归模型在预测医疗保险费用方面的表现。使用了机器学习中的三种回归模型，分别是线性回归、梯度提升和支持向量机。通过RMSE（均方根）、r2（R平方）和K-Fold交叉验证来评估性能。该研究还试图找出预测医疗保险费用中最重要的特征。该研究基于数据库中的知识发现（KDD）过程。研究结果显示，在三个回归模型中，梯度提升获得了最高的r2（R平方） 0.892，最低的RMSE（均方根）为1336.594。此外，10折交叉验证的平均权重结果与三个回归模型的r2（R平方）结果没有显着差异。在特征中，被保险人的年龄对预测医疗保险费用具有最显著的影响。

    The study aimed to evaluate the regression models' performance in predicting the cost of medical insurance. The Three (3) Regression Models in Machine Learning namely Linear Regression, Gradient Boosting, and Support Vector Machine were used. The performance will be evaluated using the metrics RMSE (Root Mean Square), r2 (R Square), and K-Fold Cross-validation. The study also sought to pinpoint the feature that would be most important in predicting the cost of medical insurance.The study is anchored on the knowledge discovery in databases (KDD) process. (KDD) process refers to the overall process of discovering useful knowledge from data. It show the performance evaluation results reveal that among the three (3) Regression models, Gradient boosting received the highest r2 (R Square) 0.892 and the lowest RMSE (Root Mean Square) 1336.594. Furthermore, the 10-Fold Cross-validation weighted mean findings are not significantly different from the r2 (R Square) results of the three (3) regres
    
[^62]: 深度学习对于纯数学家来说是一个有用的工具吗？

    Is deep learning a useful tool for the pure mathematician?. (arXiv:2304.12602v1 [math.RT])

    [http://arxiv.org/abs/2304.12602](http://arxiv.org/abs/2304.12602)

    本文是一篇介绍纯数学家使用深度学习工具进行研究的个人和非正式叙述。

    

    一篇关于纯数学家使用深度学习工具进行研究的个人和非正式叙述。

    A personal and informal account of what a pure mathematician might expect when using tools from deep learning in their research.
    
[^63]: 复杂动力系统中物理启示下的表征学习用于自然组织

    Physics-Informed Representation Learning for Emergent Organization in Complex Dynamical Systems. (arXiv:2304.12586v1 [physics.comp-ph])

    [http://arxiv.org/abs/2304.12586](http://arxiv.org/abs/2304.12586)

    该论文提出了一个基于物理启示的自然组织表征学习框架，通过局部因果状态捕获复杂时空系统中的有序行为和相干结构。该方法在实际领域科学问题中具有很好的适用性。

    

    非线性相互作用的系统组件经常引入不稳定性，产生具有新属性和不同时空尺度的现象。这被称为自发自组织，是在远离热力学平衡的系统中普遍存在的。我们引入了一个理论基础的框架，用于通过数据驱动的算法实现自然组织。它的基本构件是通过局部交互捕获信息在系统中传播方式的时空光锥。我们展示了预测光锥等价类——局部因果状态——捕获复杂时空系统中的有序行为和相干结构。通过我们的无监督物理启示的机器学习算法和高性能计算实现，我们展示了局部因果状态在实际领域科学问题中的适用性。我们展示了局部因果状态捕获旋涡及其功率的情况，这些是自然和工程领域中普遍存在的现象。

    Nonlinearly interacting system components often introduce instabilities that generate phenomena with new properties and at different space-time scales than the components. This is known as spontaneous self-organization and is ubiquitous in systems far from thermodynamic equilibrium. We introduce a theoretically-grounded framework for emergent organization that, via data-driven algorithms, is constructive in practice. Its building blocks are spacetime lightcones that capture how information propagates across a system through local interactions. We show that predictive equivalence classes of lightcones, local causal states, capture organized behaviors and coherent structures in complex spatiotemporal systems. Using our unsupervised physics-informed machine learning algorithm and a high-performance computing implementation, we demonstrate the applicability of the local causal states for real-world domain science problems. We show that the local causal states capture vortices and their pow
    
[^64]: 直接从光学显微镜观测中学习成像机制

    Learning imaging mechanism directly from optical microscopy observations. (arXiv:2304.12584v1 [physics.optics])

    [http://arxiv.org/abs/2304.12584](http://arxiv.org/abs/2304.12584)

    本文提出了一种物理感知掩蔽自编码器（PiMAE）来直接从原始显微镜图像中学习估计点扩散函数（PSF）和发射源，并在合成数据和真实世界实验中展示了其准确性和噪声鲁棒性，超越了DeepSTORM和Richardson-Lucy算法。

    

    光学显微镜图像通过直接可视化纳米世界，在科学研究中扮演着重要角色。成像机制被描述为点扩散函数（PSF）和发射源的卷积，可以基于PSF或等价PSF的先验知识，实现对纳米世界的更精确探测。然而，直接从显微镜图像中提取PSF仍然是一个重大挑战。在这里，我们提出了一种物理感知的掩蔽自编码器（PiMAE），通过自我监督的学习，可以直接从原始显微镜图像中学习估计PSF和发射源。我们在合成数据和真实世界实验中展示了我们的方法，具有显著的准确性和噪声鲁棒性。在合成数据任务中，PiMAE优于DeepSTORM和Richardson-Lucy算法，平均改进分别为19.6％和50.7％（35个任务），使用归一化均方根误差（NRMSE）m来衡量。

    Optical microscopy image plays an important role in scientific research through the direct visualization of the nanoworld, where the imaging mechanism is described as the convolution of the point spread function (PSF) and emitters. Based on a priori knowledge of the PSF or equivalent PSF, it is possible to achieve more precise exploration of the nanoworld. However, it is an outstanding challenge to directly extract the PSF from microscopy images. Here, with the help of self-supervised learning, we propose a physics-informed masked autoencoder (PiMAE) that enables a learnable estimation of the PSF and emitters directly from the raw microscopy images. We demonstrate our method in synthetic data and real-world experiments with significant accuracy and noise robustness. PiMAE outperforms DeepSTORM and the Richardson-Lucy algorithm in synthetic data tasks with an average improvement of 19.6\% and 50.7\% (35 tasks), respectively, as measured by the normalized root mean square error (NRMSE) m
    
[^65]: 非平稳环境下动态系统的实时安全评估：方法和技术综述

    Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])

    [http://arxiv.org/abs/2304.12583](http://arxiv.org/abs/2304.12583)

    本文综述了非平稳环境下动态系统实时安全评估的方法和技术，包括在线主动学习、在线半监督学习、在线迁移学习和在线异常检测，并讨论了未来的研究方向。

    

    动态系统的实时安全评估是一个关键任务，对于工业和交通应用等各个领域具有重要意义，特别是在非平稳环境中。然而，缺乏非平稳环境下实时安全评估方法的全面综述阻碍了相关方法的进展和改进。本文提供了针对非平稳环境下实时安全评估任务的方法和技术综述。具体而言，首先突出了非平稳环境中实时安全评估方法的背景和重要性。然后，我们提出了问题描述，包括定义、分类和主要挑战。我们还回顾了相关技术的最新发展，如在线主动学习、在线半监督学习、在线迁移学习和在线异常检测。最后，我们讨论了未来的展望和进一步研究的潜在方向。本文的综述旨在为非平稳环境下实时安全评估提供参考依据。

    Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
    
[^66]: 学习轨迹是泛化指标

    Learning Trajectories are Generalization Indicators. (arXiv:2304.12579v1 [cs.LG])

    [http://arxiv.org/abs/2304.12579](http://arxiv.org/abs/2304.12579)

    本文研究了DNN的学习轨迹与其在优化后的泛化能力的联系，提出了一种基于学习轨迹复杂性和训练集偏置和多样性比率的新的泛化上界，并通过实验验证了其有效性。

    

    本文旨在研究深度神经网络（DNN）的学习轨迹与其在广泛使用的梯度下降和随机梯度下降算法优化时对应的泛化能力之间的联系。本文构建了线性近似函数来模拟轨迹信息，并在此基础上提出了一种基于更丰富轨迹信息的新的泛化上界。我们提出的泛化上界依赖于学习轨迹的复杂性以及训练集的偏置和多样性比之间的比率。实验结果表明，该方法可以有效地捕捉不同训练步骤、学习率和标签噪声水平下的泛化趋势。

    The aim of this paper is to investigate the connection between learning trajectories of the Deep Neural Networks (DNNs) and their corresponding generalization capabilities when being optimized with broadly used gradient descent and stochastic gradient descent algorithms. In this paper, we construct Linear Approximation Function to model the trajectory information and we propose a new generalization bound with richer trajectory information based on it. Our proposed generalization bound relies on the complexity of learning trajectory and the ratio between the bias and diversity of training set. Experimental results indicate that the proposed method effectively captures the generalization trend across various training steps, learning rates, and label noise levels.
    
[^67]: 基于真相发现算法的公平性和偏差：实验分析

    Fairness and Bias in Truth Discovery Algorithms: An Experimental Analysis. (arXiv:2304.12573v1 [cs.LG])

    [http://arxiv.org/abs/2304.12573](http://arxiv.org/abs/2304.12573)

    本文研究了真相发现算法的偏差和公平性，发现存在敏感属性偏见和主观偏见。

    

    机器学习技术在许多具有社会影响的应用中得到越来越广泛的运用。训练机器学习模型通常需要大量的标记数据，而众包是获取来自多个工作者的标记的主要范例。众包工作者有时会提供不可靠的标记，为了解决这个问题，会使用真相发现算法（例如多数表决）来确定来自冲突工作者响应的共识标记。但是，需要注意的是，这些共识标签可能仍基于敏感属性（如性别、种族或政治派别）而存在偏见。即使没有涉及敏感属性，由于主观方面的不同观点，标签也可能带有偏见，例如毒性等。在本文中，我们对真相发现算法的偏差和公平性进行了系统研究。我们使用两个现有的众包标记数据集，发现一定比例的工作者提供了有偏见的结果。

    Machine learning (ML) based approaches are increasingly being used in a number of applications with societal impact. Training ML models often require vast amounts of labeled data, and crowdsourcing is a dominant paradigm for obtaining labels from multiple workers. Crowd workers may sometimes provide unreliable labels, and to address this, truth discovery (TD) algorithms such as majority voting are applied to determine the consensus labels from conflicting worker responses. However, it is important to note that these consensus labels may still be biased based on sensitive attributes such as gender, race, or political affiliation. Even when sensitive attributes are not involved, the labels can be biased due to different perspectives of subjective aspects such as toxicity. In this paper, we conduct a systematic study of the bias and fairness of TD algorithms. Our findings using two existing crowd-labeled datasets, reveal that a non-trivial proportion of workers provide biased results, and
    
[^68]: 基于多模型建模和异构GNN的性能优化

    Performance Optimization using Multimodal Modeling and Heterogeneous GNN. (arXiv:2304.12568v1 [cs.DC])

    [http://arxiv.org/abs/2304.12568](http://arxiv.org/abs/2304.12568)

    本研究提出了一种通用且高效的性能优化方法，使用基于IR编程模型的多模态深度学习方法进行特定任务的性能优化。我们提出了一种非常通用的调节并行代码区域的技术。

    

    高性能计算架构中的异构性和可配置性不断增长，使得在这些系统上进行自动调优和运行时参数配置变得非常复杂。为了缩短达到最佳配置的时间，除了采用面向特定应用的解决方案外，常见的方法是使用通用搜索策略，但往往不能找到最佳的配置或其收敛所需时间太长。因此，需要一种通用且高效的调优方法，可轻松扩展和适应多种调优任务。本文提出了一种调节并行代码区域的技术，其足够通用，可适应于多个任务。我们分析基于IR的编程模型，以进行特定任务的性能优化。为此，我们提出了基于多模态深度学习的多模态图神经网络和自编码器（MGA）调谐器，该方法使用异构图神经网络和去噪自编码器。

    Growing heterogeneity and configurability in HPC architectures has made auto-tuning applications and runtime parameters on these systems very complex. Users are presented with a multitude of options to configure parameters. In addition to application specific solutions, a common approach is to use general purpose search strategies, which often might not identify the best configurations or their time to convergence is a significant barrier. There is, thus, a need for a general purpose and efficient tuning approach that can be easily scaled and adapted to various tuning tasks. We propose a technique for tuning parallel code regions that is general enough to be adapted to multiple tasks. In this paper, we analyze IR-based programming models to make task-specific performance optimizations. To this end, we propose the Multimodal Graph Neural Network and Autoencoder (MGA) tuner, a multimodal deep learning based approach that adapts Heterogeneous Graph Neural Networks and Denoizing Autoencode
    
[^69]: Proto-Value Networks: 通过辅助任务进行表示学习的可扩展性研究

    Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks. (arXiv:2304.12567v1 [cs.LG])

    [http://arxiv.org/abs/2304.12567](http://arxiv.org/abs/2304.12567)

    本文研究了如何通过增加辅助任务的数量和代理网络的大小，提高表示学习的效果。同时，本文还提出了基于后继度量的新型辅助任务家族 Proto-Value 网络。

    

    辅助任务可以提高深度强化学习代理学到的表示能力。尽管其效果已经被相当充分地理论分析，但在实践中，它们主要被用作主要学习目标的支持，而不是作为表示学习的一种方法。基于这一观察，本文研究了辅助任务在学习复杂表示方面的有效性，重点关注同时增加任务数量和代理网络的大小的设置。为此，我们提出了一种基于后继度量的新型辅助任务家族。这些任务易于实现，具有吸引人的理论性质。与合适的离线学习规则结合使用，结果是一个表示学习算法，可以被理解为 Proto-Value 网络。

    Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent's network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extend
    
[^70]: AdaNPC：探索非参数分类器进行测试时间适应性

    AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation. (arXiv:2304.12566v1 [cs.LG])

    [http://arxiv.org/abs/2304.12566](http://arxiv.org/abs/2304.12566)

    本文提出了一种新的测试时间自适应（TTA）方法AdaNPC，通过利用非参数分类器进行建模从而避免了离线目标数据或在推理时使用额外的复杂优化过程。AdaNPC从存储器中回顾最相似的 K 个样本进行投票预测，逐渐改变存储器中的样本分布以提高测试域性能。

    

    许多最近的机器学习任务都集中在开发能够推广到未见过分布的模型上。域通用性（DG）已成为各个领域中的关键课题之一。几篇文献表明，如果不利用目标域的信息，域通用性可能会变得极其困难。为了解决这个问题，提出了测试时间自适应（TTA）方法。现有的TTA方法需要离线目标数据或在推理阶段使用额外的复杂优化过程。本文采用非参数分类器进行测试时间自适应（AdaNPC）。具体地，在训练过程中构建一个包含特征和标签对的存储器。在推理时，给定一个测试实例，AdaNPC首先从存储器中回顾K个最相似的样本进行投票预测，然后将测试特征和预测标签添加到存储器中。通过这种方式，存储器中的样本分布可以逐渐从训练分布向测试分布变化，从而提高测试域的性能。我们在各种基准数据集上进行了大量实验，结果表明我们的方法优于几种最先进的TTA方法，并取得了与完全监督方法相当的性能。

    Many recent machine learning tasks focus to develop models that can generalize to unseen distributions. Domain generalization (DG) has become one of the key topics in various fields. Several literatures show that DG can be arbitrarily hard without exploiting target domain information. To address this issue, test-time adaptive (TTA) methods are proposed. Existing TTA methods require offline target data or extra sophisticated optimization procedures during the inference stage. In this work, we adopt Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC). In particular, we construct a memory that contains the feature and label pairs from training domains. During inference, given a test instance, AdaNPC first recalls K closed samples from the memory to vote for the prediction, and then the test feature and predicted label are added to the memory. In this way, the sample distribution in the memory can be gradually changed from the training distribution towards the test distr
    
[^71]: 训练中结合对手和反对手。

    Combining Adversaries with Anti-adversaries in Training. (arXiv:2304.12550v1 [cs.LG])

    [http://arxiv.org/abs/2304.12550](http://arxiv.org/abs/2304.12550)

    该论文研究了在对抗训练中，通过结合对手和反对手(带有反对手扰动的样本)可以更有效地提高深度神经网络的公平性、鲁棒性和泛化性，在一些特定的学习场景中表现出更好的性能。

    

    对抗训练是提高深度神经网络健壮性的有效学习技术。本研究在更一般的扰动范围下理论上研究了对抗训练对深度学习模型的公平性、鲁棒性和泛化性的影响。我们的理论探索表明，将对手和反对手 (带有反对手扰动的样本) 结合在训练中，在一些典型的学习场景 (如噪声标签学习和不平衡学习) 中能够更有效地实现更好的类别间公平性和鲁棒性和泛化性之间的平衡，相比于标准对抗训练。

    Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied b
    
[^72]: COUPA: 一种面向在线到线下服务平台的工业级推荐系统

    COUPA: An Industrial Recommender System for Online to Offline Service Platforms. (arXiv:2304.12549v1 [cs.IR])

    [http://arxiv.org/abs/2304.12549](http://arxiv.org/abs/2304.12549)

    COUPA是一个面向O2O服务平台的工业级推荐系统，通过时间感知偏好和位置感知偏好来提高推荐效果。

    

    在线到线下（O2O）服务平台旨在帮助用户在本地发现零售服务（例如娱乐和餐饮），近年来变得越来越流行，这极大地挑战了现有的推荐系统。本文基于支付宝平台的真实数据，针对O2O服务的特殊情景发现了递归基础上的时间模式和位置偏置普遍存在，严重影响了推荐效果。基于此，我们提出了COUPA，这是一个工业级系统，旨在通过考虑以下两个方面来表征用户偏好：(1)时间感知偏好：我们采用了具有注意机制的连续时间感知点过程，以完全捕捉推荐的时间模式。(2)位置感知偏好：一个配备了位置个性化模块的位置选择器组件被精心设计，以以个性化的方式减轻位置偏差。最后，我们仔细实现并部署COUPA在支付宝平台上。

    Aiming at helping users locally discovery retail services (e.g., entertainment and dinning), Online to Offline (O2O) service platforms have become popular in recent years, which greatly challenge current recommender systems. With the real data in Alipay, a feeds-like scenario for O2O services, we find that recurrence based temporal patterns and position biases commonly exist in our scenarios, which seriously threaten the recommendation effectiveness. To this end, we propose COUPA, an industrial system targeting for characterizing user preference with following two considerations: (1) Time aware preference: we employ the continuous time aware point process equipped with an attention mechanism to fully capture temporal patterns for recommendation. (2) Position aware preference: a position selector component equipped with a position personalization module is elaborately designed to mitigate position bias in a personalized manner. Finally, we carefully implement and deploy COUPA on Alipay 
    
[^73]: 使用物理信息可逆神经网络进行高效贝叶斯推断的研究

    Efficient Bayesian inference using physics-informed invertible neural networks for inverse problems. (arXiv:2304.12541v1 [math.NA])

    [http://arxiv.org/abs/2304.12541](http://arxiv.org/abs/2304.12541)

    本文提出了一种使用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法，该方法可以高效地进行抽样和准确的密度评估。研究通过残差项和独立性损失项确保了INN输出的统计独立性，并在多项实验中证明了其有效性和准确性。

    

    本文提出了一种利用物理信息可逆神经网络(PI-INN)解决贝叶斯反问题的新方法。PI-INN的结构包括两个子网络：一个可逆神经网络(INN)和一个神经基础网络(NB-Net)。通过NB-Net帮助建立参数输入和INN输出之间的可逆映射，以提供可行的后验分布估计，从而实现高效的抽样和准确的密度评估。此外，PI-INN的损失函数包含两个部分：一部分是基于残差的物理信息损失项，另一部分是新的独立性损失项。提出的独立性损失项可以高斯化随机潜变量，并通过有效利用估计的密度函数，确保INN输出的两个部分之间的统计独立性。通过进行反向运动学和反向扩散等多项实验验证了所提出的PI-INN的有效性和准确性。

    In the paper, we propose a novel approach for solving Bayesian inverse problems with physics-informed invertible neural networks (PI-INN). The architecture of PI-INN consists of two sub-networks: an invertible neural network (INN) and a neural basis network (NB-Net). The invertible map between the parametric input and the INN output with the aid of NB-Net is constructed to provide a tractable estimation of the posterior distribution, which enables efficient sampling and accurate density evaluation. Furthermore, the loss function of PI-INN includes two components: a residual-based physics-informed loss term and a new independence loss term. The presented independence loss term can Gaussianize the random latent variables and ensure statistical independence between two parts of INN output by effectively utilizing the estimated density function. Several numerical experiments are presented to demonstrate the efficiency and accuracy of the proposed PI-INN, including inverse kinematics, inver
    
[^74]: GARCIA：利用多粒度对比学习提高长尾查询表示能力

    GARCIA: Powering Representations of Long-tail Query with Multi-granularity Contrastive Learning. (arXiv:2304.12537v1 [cs.LG])

    [http://arxiv.org/abs/2304.12537](http://arxiv.org/abs/2304.12537)

    GARCIA利用多粒度对比学习，基于图的知识转移和基于意图的表示，提高了长尾查询表示能力。

    

    最近，服务平台的发展为用户和商家带来了巨大的便利，服务搜索引擎通过文本查询在改善用户体验方面发挥着重要作用。然而，用户的不可控制的搜索习惯通常带来大量的长尾查询，这严重威胁到搜索模型的能力。针对这个问题，我们开发了一个新的框架GARCIA，利用基于图的知识转移和基于意图的表示来提高长尾查询表示能力。

    Recently, the growth of service platforms brings great convenience to both users and merchants, where the service search engine plays a vital role in improving the user experience by quickly obtaining desirable results via textual queries. Unfortunately, users' uncontrollable search customs usually bring vast amounts of long-tail queries, which severely threaten the capability of search models. Inspired by recently emerging graph neural networks (GNNs) and contrastive learning (CL), several efforts have been made in alleviating the long-tail issue and achieve considerable performance. Nevertheless, they still face a few major weaknesses. Most importantly, they do not explicitly utilize the contextual structure between heads and tails for effective knowledge transfer, and intention-level information is commonly ignored for more generalized representations.  To this end, we develop a novel framework GARCIA, which exploits the graph based knowledge transfer and intention based representat
    
[^75]: 通过随机行走随机交替方向乘法算法推动个性化联邦学习

    Mobilizing Personalized Federated Learning via Random Walk Stochastic ADMM. (arXiv:2304.12534v1 [cs.LG])

    [http://arxiv.org/abs/2304.12534](http://arxiv.org/abs/2304.12534)

    本研究提出了一种新算法RWSADMM，以解决在动态联邦学习中存在的数据不一致和通信成本高的问题，提高了可扩展性。

    

    在本研究中，我们探讨了在现实世界中实现联邦学习（FL）时存在的障碍，其中不能维护中央服务器与所有客户端之间的一致连接，并且数据分布是异构的。为了解决这些挑战，我们专注于动态联邦学习，其中服务器在相邻客户端组之间移动以学习本地模型。具体来说，我们提出了一种新算法，即随机行走随机交替方向乘法算法（RWSADMM），只要有足够数量的连接客户端用于模型训练，就能适应动态和即席网络条件。在RWSADMM中，服务器随机向一组客户端行走。它基于硬不等式约束形成相邻客户端之间的局部近似，而不是采用一致更新来解决数据异构性。我们提出的方法是收敛的，可以降低通信成本，通过减少训练时间提高可扩展性。

    In this research, we investigate the barriers associated with implementing Federated Learning (FL) in real-world scenarios, where a consistent connection between the central server and all clients cannot be maintained, and data distribution is heterogeneous. To address these challenges, we focus on mobilizing the federated setting, where the server moves between groups of adjacent clients to learn local models. Specifically, we propose a new algorithm, Random Walk Stochastic Alternating Direction Method of Multipliers (RWSADMM), capable of adapting to dynamic and ad-hoc network conditions as long as a sufficient number of connected clients are available for model training. In RWSADMM, the server walks randomly toward a group of clients. It formulates local proximity among adjacent clients based on hard inequality constraints instead of consensus updates to address data heterogeneity. Our proposed method is convergent, reduces communication costs, and enhances scalability by reducing th
    
[^76]: Patch Diffusion: 更快更高效的扩散模型训练方法

    Patch Diffusion: Faster and More Data-Efficient Training of Diffusion Models. (arXiv:2304.12526v1 [cs.CV])

    [http://arxiv.org/abs/2304.12526](http://arxiv.org/abs/2304.12526)

    Patch Diffusion 提出了一种通用的基于块的训练框架，可将扩散模型训练时间缩短至少一倍，并在更小的数据集上实现更好的性能表现。

    

    扩散模型是强大的，但需要大量时间和数据来训练。我们提出了一种通用的基于块的训练框架 Patch Diffusion，显著降低了训练时间成本，同时提高了数据效率，从而有助于将扩散模型的训练推广到更广泛的用户。我们的创新核心是新的条件评分函数，它在块级别上进行操作，并将原始图像中的块位置作为附加坐标通道，同时通过在训练过程中随机化和多样化块大小来编码多尺度的跨区域依赖关系。在采样方面，我们的方法与原始扩散模型一样简单易用。通过 Patch Diffusion，我们能够实现 $\mathbf{\ge 2\times}$ 更快的训练速度，同时保持可比较或更好的生成质量。同时，Patch Diffusion 提高了在相对较小的数据集上训练的扩散模型的性能，例如，使用仅 5,000 张图像进行从头开始训练。

    Diffusion models are powerful, but they require a lot of time and data to train. We propose Patch Diffusion, a generic patch-wise training framework, to significantly reduce the training time costs while improving data efficiency, which thus helps democratize diffusion model training to broader users. At the core of our innovations is a new conditional score function at the patch level, where the patch location in the original image is included as additional coordinate channels, while the patch size is randomized and diversified throughout training to encode the cross-region dependency at multiple scales. Sampling with our method is as easy as in the original diffusion model. Through Patch Diffusion, we could achieve $\mathbf{\ge 2\times}$ faster training, while maintaining comparable or better generation quality. Patch Diffusion meanwhile improves the performance of diffusion models trained on relatively small datasets, $e.g.$, as few as 5,000 images to train from scratch. We achieve 
    
[^77]: CIMLA：可解释的人工智能用于推断差异因果网络

    CIMLA: Interpretable AI for inference of differential causal networks. (arXiv:2304.12523v1 [cs.LG])

    [http://arxiv.org/abs/2304.12523](http://arxiv.org/abs/2304.12523)

    CIMLA是一个新工具，可以发现条件依赖的因果关系变化，它可以识别生物条件下基因调控网络的差异，是更加准确和鲁棒的方法。

    

    从高维数据中发现因果关系是生物信息学中的一个重大问题。机器学习和特征归因模型在这个领域显示出巨大的潜力，但缺乏因果解释。本文研究表明，一个常见的特征归因模型在某些假设下估计了反映一个变量对另一个变量影响的因果数量。我们利用这个见解实现了一个新工具CIMLA，用于发现条件依赖的因果关系变化，然后使用CIMLA识别生物条件下基因调控网络的差异。通过对模拟数据集进行广泛的基准测试，我们表明CIMLA比领先的方法更具鲁棒性和准确性。最后，我们运用CIMLA分析了一个先前发表的单细胞RNA-seq数据集，收集了带有和不带有阿尔茨海默病（AD）的患者数据，并发现了基因与AD之间的新潜在因果关系。

    The discovery of causal relationships from high-dimensional data is a major open problem in bioinformatics. Machine learning and feature attribution models have shown great promise in this context but lack causal interpretation. Here, we show that a popular feature attribution model estimates a causal quantity reflecting the influence of one variable on another, under certain assumptions. We leverage this insight to implement a new tool, CIMLA, for discovering condition-dependent changes in causal relationships. We then use CIMLA to identify differences in gene regulatory networks between biological conditions, a problem that has received great attention in recent years. Using extensive benchmarking on simulated data sets, we show that CIMLA is more robust to confounding variables and is more accurate than leading methods. Finally, we employ CIMLA to analyze a previously published single-cell RNA-seq data set collected from subjects with and without Alzheimer's disease (AD), discoverin
    
[^78]: 一种新的具有自适应停止准则的非精确近端线性算法，用于鲁棒相位恢复问题。

    A New Inexact Proximal Linear Algorithm with Adaptive Stopping Criteria for Robust Phase Retrieval. (arXiv:2304.12522v1 [math.OC])

    [http://arxiv.org/abs/2304.12522](http://arxiv.org/abs/2304.12522)

    本文提出了一种新的鲁棒相位恢复算法，通过使用自适应停止准则的非精确近端线性算法，该方法在实验中证明比现有方法更高效。

    

    本文考虑了鲁棒相位恢复问题，该问题可视为一个非光滑和非凸优化问题。我们提出了一种新的非精确近端线性算法，其中子问题被不精确求解。我们的贡献是为子问题提出了两种自适应停止准则。我们分析了所提出方法的收敛性能。通过对合成和实际数据集的实验，我们证明了我们的方法比现有方法更高效，例如原始近端线性算法和次梯度方法。

    This paper considers the robust phase retrieval problem, which can be cast as a nonsmooth and nonconvex optimization problem. We propose a new inexact proximal linear algorithm with the subproblem being solved inexactly. Our contributions are two adaptive stopping criteria for the subproblem. The convergence behavior of the proposed methods is analyzed. Through experiments on both synthetic and real datasets, we demonstrate that our methods are much more efficient than existing methods, such as the original proximal linear algorithm and the subgradient method.
    
[^79]: RenderDiffusion: 文本生成作为图像生成

    RenderDiffusion: Text Generation as Image Generation. (arXiv:2304.12519v1 [cs.CL])

    [http://arxiv.org/abs/2304.12519](http://arxiv.org/abs/2304.12519)

    本文提出了一种新的扩散方法——\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。它将连续扩散模型应用于离散文本并实现了条件文本生成作为字形图像生成问题。

    

    扩散模型已成为文本生成的新生成范式。考虑到文本的离散分类特性，在本文中，我们提出了一种新颖的扩散方法——\textsc{RenderDiffusion}，通过文本引导的图像生成进行文本生成。我们的关键思路是将目标文本呈现为包含视觉语言内容的"字形图像"。这样，条件化的文本生成可以被形式化为一个字形图像生成任务，然后自然地将连续扩散模型应用于离散文本。

    Diffusion models have become a new generative paradigm for text generation. Considering the discrete categorical nature of text, in this paper, we propose \textsc{RenderDiffusion}, a novel diffusion approach for text generation via text-guided image generation. Our key idea is to render the target text as a \emph{glyph image} containing visual language content. In this way, conditional text generation can be cast as a glyph image generation task, and it is then natural to apply continuous diffusion models to discrete texts. Specially, we utilize a cascaded architecture (\ie a base and a super-resolution diffusion model) to generate high-fidelity glyph images, conditioned on the input text. Furthermore, we design a text grounding module to transform and refine the visual language content from generated glyph images into the final texts. In experiments over four conditional text generation tasks and two classes of metrics (\ie quality and diversity), \textsc{RenderDiffusion} can achieve 
    
[^80]: 通过无模型强化学习尽快实现正式规范

    Fulfilling Formal Specifications ASAP by Model-free Reinforcement Learning. (arXiv:2304.12508v1 [cs.LG])

    [http://arxiv.org/abs/2304.12508](http://arxiv.org/abs/2304.12508)

    本文提出了 ASAP-Phi框架，利用分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励，使用基于演员-评论家的算法训练代理来尽快实现规范。

    

    本文提出了一个无模型强化学习解决方案，即 ASAP-Phi框架，以鼓励代理尽快满足正式规范。该框架利用一个分段奖励函数来为未满足规范的轨迹分配定量的语义奖励，并为其余轨迹分配高的常数奖励。然后，使用基于演员-评论家的算法（例如软演员-评论家(SAC)或深度确定性策略梯度（DDPG））训练代理。此外，我们证明ASAP-Phi生成的策略优先考虑尽快实现规范。对最先进的基准测试中进行了大量实验，包括消融研究。结果显示，我们的框架成功地为多达97％的测试用例找到了足够快的轨迹，并击败了基线。

    We propose a model-free reinforcement learning solution, namely the ASAP-Phi framework, to encourage an agent to fulfill a formal specification ASAP. The framework leverages a piece-wise reward function that assigns quantitative semantic reward to traces not satisfying the specification, and a high constant reward to the remaining. Then, it trains an agent with an actor-critic-based algorithm, such as soft actor-critic (SAC), or deep deterministic policy gradient (DDPG). Moreover, we prove that ASAP-Phi produces policies that prioritize fulfilling a specification ASAP. Extensive experiments are run, including ablation studies, on state-of-the-art benchmarks. Results show that our framework succeeds in finding sufficiently fast trajectories for up to 97\% test cases and defeats baselines.
    
[^81]: 基于CNN的隐写术--将机器学习与传统的隐写技术相结合

    CNN-Assisted Steganography -- Integrating Machine Learning with Established Steganographic Techniques. (arXiv:2304.12503v1 [cs.CR])

    [http://arxiv.org/abs/2304.12503](http://arxiv.org/abs/2304.12503)

    本文提出了一种基于隐写辅助卷积神经网络的隐写方法，能够提高隐写分析的稳健性，并通过SA-CNN的自适应性能够进行核心隐写算法的自定义。

    

    我们提出一种通过增加隐写分析中的稳健性来提高隐写术的方法。我们的方法通过引入一个隐写辅助卷积神经网络（SA-CNN）来增强一类隐写方法。先前的研究表明，使用训练有素的神经网络作为隐写分析器来发现隐秘信息的存在是成功的，而使用SA-CNN在生成隐写图像时能使这样的隐写分析器的效果不那么明显。我们还探讨了将SA-CNN的所有可能输出表示为较小的离散空间而不是连续空间的优缺点。我们的SA-CNN使得某些参数化隐写算法能够根据要嵌入信息的载体媒体的特征进行定制。因此，SA-CNN是自适应的，它使得核心隐写算法能够根据隐写载体的特性进行自定义。

    We propose a method to improve steganography by increasing the resilience of stego-media to discovery through steganalysis. Our approach enhances a class of steganographic approaches through the inclusion of a steganographic assistant convolutional neural network (SA-CNN). Previous research showed success in discovering the presence of hidden information within stego-images using trained neural networks as steganalyzers that are applied to stego-images. Our results show that such steganalyzers are less effective when SA-CNN is employed during the generation of a stego-image. We also explore the advantages and disadvantages of representing all the possible outputs of our SA-CNN within a smaller, discrete space, rather than a continuous space. Our SA-CNN enables certain classes of parametric steganographic algorithms to be customized based on characteristics of the cover media in which information is to be embedded. Thus, SA-CNN is adaptive in the sense that it enables the core steganogr
    
[^82]: 数字孪生的因果语义通信：一种通用的模仿学习方法

    Causal Semantic Communication for Digital Twins: A Generalizable Imitation Learning Approach. (arXiv:2304.12502v1 [cs.LG])

    [http://arxiv.org/abs/2304.12502](http://arxiv.org/abs/2304.12502)

    本文提出了一种新的因果语义通信（CSC）框架，利用模仿学习（IL）方法，在数字孪生（DT）无线系统上利用AI技术和语义通信（SC），以提高通信效率、决策落地效果、推动数字孪生应用的普及和完善。

    

    数字孪生（DT）利用虚拟世界的表示，以及通信（例如6G）、计算（例如边缘计算）和人工智能（AI）技术，实现了许多连接的智能服务。为了处理基于数字孪生（DT）的大量网络数据，无线系统可以利用语义通信（SC）范例，通过利用AI技术（如因果推断）促进知情决策来处理严格的通信限制。本文提出了一种名为因果语义通信（CSC）的新框架，用于基于数字孪生的无线系统。CSC系统被提出作为模仿学习（IL）问题，其中发射器通过使用DT访问最优网络控制策略来教授接收器如何在带宽有限的无线通道上使用SC来提高其知识以执行最优控制操作。源数据中的因果结构使用新技术提取出来，使得教学框架更加通用且具有解释性。

    A digital twin (DT) leverages a virtual representation of the physical world, along with communication (e.g., 6G), computing (e.g., edge computing), and artificial intelligence (AI) technologies to enable many connected intelligence services. In order to handle the large amounts of network data based on digital twins (DTs), wireless systems can exploit the paradigm of semantic communication (SC) for facilitating informed decision-making under strict communication constraints by utilizing AI techniques such as causal reasoning. In this paper, a novel framework called causal semantic communication (CSC) is proposed for DT-based wireless systems. The CSC system is posed as an imitation learning (IL) problem, where the transmitter, with access to optimal network control policies using a DT, teaches the receiver using SC over a bandwidth limited wireless channel how to improve its knowledge to perform optimal control actions. The causal structure in the source data is extracted using novel 
    
[^83]: 量子神经网络和张量网络在截面股票收益预测中的应用

    The cross-sectional stock return predictions via quantum neural network and tensor network. (arXiv:2304.12501v1 [cs.LG])

    [http://arxiv.org/abs/2304.12501](http://arxiv.org/abs/2304.12501)

    本文研究将量子神经网络和张量网络应用于股票收益预测，在日本股市中张量网络模型表现优于传统模型，并在最新市场环境下呈现出卓越表现。

    

    本文研究了利用量子和量子启发式的机器学习算法进行股票收益预测的应用。其中，我们将量子神经网络（一种适用于噪声中等规模量子计算机的算法）和张量网络（一种受量子启发的机器学习算法）的性能与传统模型如线性回归和神经网络进行比较。通过构建基于模型预测的投资组合并测量投资绩效，我们发现在日本股市中，张量网络模型表现优于传统基准模型（包括线性和神经网络模型）。虽然量子神经网络模型在整个周期内具有降低风险调整超额收益的能力，但最新的市场环境下，量子神经网络和张量网络模型均表现出卓越的性能。

    In this paper we investigate the application of quantum and quantum-inspired machine learning algorithms to stock return predictions. Specifically, we evaluate performance of quantum neural network, an algorithm suited for noisy intermediate-scale quantum computers, and tensor network, a quantum-inspired machine learning algorithm, against classical models such as linear regression and neural networks. To evaluate their abilities, we construct portfolios based on their predictions and measure investment performances. The empirical study on the Japanese stock market shows the tensor network model achieves superior performance compared to classical benchmark models, including linear and neural network models. Though the quantum neural network model attains the lowered risk-adjusted excess return than the classical neural network models over the whole period, both the quantum neural network and tensor network models have superior performances in the latest market environment, which sugges
    
[^84]: 使用FPGA进行高性能计算的设计优化

    Design optimization for high-performance computing using FPGA. (arXiv:2304.12474v1 [cs.AR])

    [http://arxiv.org/abs/2304.12474](http://arxiv.org/abs/2304.12474)

    本文中，作者通过优化Tensil AI的开源推理加速器并使用高级编译器策略，改进硬件设计并使用Xilinx Ultra RAM，表明优化FPGA可提高推理性能，为FPGA在高性能计算领域的应用提供了启示。

    

    由于其独特的灵活性、性能和功率效率结合，可重构架构如可编程门阵列（FPGA）已经被用于许多领域的加速计算。但是，由于编程复杂性和优化性能的困难，FPGA尚未被广泛用于高性能计算。本文通过优化Tensil AI的开源推理加速器，使用在CIFAR上训练的ResNet20实现最大性能，以便深入了解使用FPGA进行高性能计算的方法。我们展示了如何改进硬件设计，使用Xilinx Ultra RAM和使用先进的编译器策略可以提高推理性能。我们还证明了在从原始32位浮点数向下舍入时运行CIFAR测试数据集时几乎没有准确性下降。我们平台中的异构计算模型使我们能够实现293.5的帧速率。

    Reconfigurable architectures like Field Programmable Gate Arrays (FPGAs) have been used for accelerating computations in several domains because of their unique combination of flexibility, performance, and power efficiency. However, FPGAs have not been widely used for high-performance computing, primarily because of their programming complexity and difficulties in optimizing performance. We optimize Tensil AI's open-source inference accelerator for maximum performance using ResNet20 trained on CIFAR in this paper in order to gain insight into the use of FPGAs for high-performance computing. In this paper, we show how improving hardware design, using Xilinx Ultra RAM, and using advanced compiler strategies can lead to improved inference performance. We also demonstrate that running the CIFAR test data set shows very little accuracy drop when rounding down from the original 32-bit floating point. The heterogeneous computing model in our platform allows us to achieve a frame rate of 293.5
    
[^85]: 提高机器学习合成数据的真实性研究

    A Study on Improving Realism of Synthetic Data for Machine Learning. (arXiv:2304.12463v1 [cs.CV])

    [http://arxiv.org/abs/2304.12463](http://arxiv.org/abs/2304.12463)

    本文研究并评估了一个合成到真实的生成模型，将合成渲染转换为更真实的风格以适用于通用数据集，并通过下游感知任务来量化和定性地评估其性能。

    

    利用生成对抗性学习的合成到真实数据转换已经取得了重大的成功，以改进合成数据。然而，目前仍有限的研究专注于深度评估和比较通用机器学习用途的对抗性训练的合成数据。本文旨在训练和评估将合成渲染转化为更真实风格的合成到真实生成模型，并利用未标记的真实世界数据进行普通目的数据集的条件操作，通过定量和定性的评估指标以及定义下游的感知任务进行广泛的性能评估和比较。

    Synthetic-to-real data translation using generative adversarial learning has achieved significant success to improve synthetic data. Yet, there are limited studies focusing on deep evaluation and comparison of adversarial training on general-purpose synthetic data for machine learning. This work aims to train and evaluate a synthetic-to-real generative model that transforms the synthetic renderings into more realistic styles on general-purpose datasets conditioned with unlabeled real-world data. Extensive performance evaluation and comparison have been conducted through qualitative and quantitative metrics, and a defined downstream perception task.
    
[^86]: 多智能体MDP中基于概率代理掉线的无模型学习和最优策略设计

    Model-Free Learning and Optimal Policy Design in Multi-Agent MDPs Under Probabilistic Agent Dropout. (arXiv:2304.12458v1 [eess.SY])

    [http://arxiv.org/abs/2304.12458](http://arxiv.org/abs/2304.12458)

    本文研究了多智能体MDP中基于概率代理掉线的情况，并提出了一种无模型算法，能够消除掉线情况需要枚举计算的限制，从而实现计算后掉线系统的最优策略设计。

    

    本文研究了一个多智能体马尔可夫决策过程（MDP），该过程可以经历代理掉线，并基于对于策略的控制和预代理过程的采样来计算后掉线系统的策略。控制器的目标是寻找一个最优策略，使得在已知代理掉出概率的先验知识的情况下，期望系统的价值最大化。对于任何特定的掉线情况下的最优策略是这个问题的一个特例。对于具有特定转换独立性和奖励可分性结构的MDPs，我们假设从系统中移除代理组成了一个新的MDP，由剩余代理组成具有新状态和动作空间的MDP，转换动态消除已删除的代理，奖励与已删除的代理无关。首先我们展示了在这些假设下，对于预掉出系统期望值可以通过一个单一的MDP来表示；这个“鲁棒MDP”能够消除在计算最优策略时要评估所有$2^N$种代理掉线情况的需要。然后我们提出了一个无模型算法，该算法使用蒙特卡罗采样和重要性采样来学习鲁棒MDP，从而能够计算后掉线系统的最优策略。仿真结果展示了该方法的优点。

    This work studies a multi-agent Markov decision process (MDP) that can undergo agent dropout and the computation of policies for the post-dropout system based on control and sampling of the pre-dropout system. The controller's objective is to find an optimal policy that maximizes the value of the expected system given a priori knowledge of the agents' dropout probabilities. Finding an optimal policy for any specific dropout realization is a special case of this problem. For MDPs with a certain transition independence and reward separability structure, we assume that removing agents from the system forms a new MDP comprised of the remaining agents with new state and action spaces, transition dynamics that marginalize the removed agents, and rewards that are independent of the removed agents. We first show that under these assumptions, the value of the expected post-dropout system can be represented by a single MDP; this "robust MDP" eliminates the need to evaluate all $2^N$ realizations
    
[^87]: 无监督和半监督流形学习的Rank Flow Embedding

    Rank Flow Embedding for Unsupervised and Semi-Supervised Manifold Learning. (arXiv:2304.12448v1 [cs.CV])

    [http://arxiv.org/abs/2304.12448](http://arxiv.org/abs/2304.12448)

    本文提出了Rank Flow Embedding算法用于无监督和半监督场景的流形学习。

    

    先进的获取和共享技术使得多媒体集合及其应用的增长几乎无限，然而，由于标注数据往往昂贵而费时，因此有监督的训练所需的标注数据可用性相反。我们提出了一种名为Rank Flow Embedding（RFE）的新型流形学习算法用于无监督和半监督场景。该算法基于最近流形学习方法利用的想法，包括超图、笛卡尔积和连通成分。

    Impressive advances in acquisition and sharing technologies have made the growth of multimedia collections and their applications almost unlimited. However, the opposite is true for the availability of labeled data, which is needed for supervised training, since such data is often expensive and time-consuming to obtain. While there is a pressing need for the development of effective retrieval and classification methods, the difficulties faced by supervised approaches highlight the relevance of methods capable of operating with few or no labeled data. In this work, we propose a novel manifold learning algorithm named Rank Flow Embedding (RFE) for unsupervised and semi-supervised scenarios. The proposed method is based on ideas recently exploited by manifold learning approaches, which include hypergraphs, Cartesian products, and connected components. The algorithm computes context-sensitive embeddings, which are refined following a rank-based processing flow, while complementary contextu
    
[^88]: 利用机器学习通过心电图预测肺动脉高压

    Predicting Pulmonary Hypertension by Electrocardiograms Using Machine Learning. (arXiv:2304.12447v1 [eess.SP])

    [http://arxiv.org/abs/2304.12447](http://arxiv.org/abs/2304.12447)

    本研究旨在利用神经网络模型，通过处理心电图信号来检测肺动脉高压，以实现早期的监测和治疗。

    

    肺动脉高压（PH）是一种影响肺部动脉和右心边的高血压病症（Mayo Clinic，2017）。定义平均肺动脉压大于25mmHg。肺动脉高压的诊断时间从诊断时的预计5年存活率仅为57%，而右心衰竭患者在未治疗下仅存活约1年（Benza等，2012）。 鉴于该病的慢性特性，早期检测PH仍然是一个具有挑战性的任务，导致治疗延迟。超声心动图目前用于诊断PH的筛查工具。然而，与超声心动图相比，心电图（ECG）更加易于使用和经济实惠，但对于筛查患有PH的高危患者来说探究较少。该项目的目标是创建一个神经网络模型，该模型可以处理ECG信号并检测PH的存在。

    Pulmonary hypertension (PH) is a condition of high blood pressure that affects the arteries in the lungs and the right side of the heart (Mayo Clinic, 2017). A mean pulmonary artery pressure greater than 25 mmHg is defined as Pulmonary hypertension. The estimated 5-year survival rate from the time of diagnosis of pulmonary hypertension is only 57% without therapy and patients with right heart failure only survive for approximately 1 year without treatment (Benza et al., 2012). Given the indolent nature of the disease, early detection of PH remains a challenge leading to delays in therapy. Echocardiography is currently used as a screening tool for diagnosing PH. However, electrocardiography (ECG), a more accessible, simple to use, and cost-effective tool compared to echocardiography, is less studied and explored for screening at-risk patients for PH. The goal of this project is to create a neural network model which can process an ECG signal and detect the presence of PH with a confiden
    
[^89]: 数据驱动需求预测的能源中心随机MPC

    Stochastic MPC for energy hubs using data driven demand forecasting. (arXiv:2304.12438v1 [eess.SY])

    [http://arxiv.org/abs/2304.12438](http://arxiv.org/abs/2304.12438)

    本文提出了一种基于数据驱动需求预测的随机MPC控制器，采用机会约束来最小化不确定的电力和热需求，使用历史数据构建预测模型，并通过情景法采样多步需求轨迹。在模拟和实际数据上验证了该控制器的性能。

    

    能源中心通过多种转换和储能组件将能源资源进行转换和分配。能源中心的最佳运行利用其灵活性来增加能源效率并减少运营成本。然而，需求中的不确定性给能源中心的优化带来了挑战。本文提出了一种随机MPC控制器，使用机会约束来最小化不确定的电力和热需求，采用历史数据构建基于高斯过程的需求预测模型，并生成未来电力和热需求的预测。通过从预测模型中采样多步需求轨迹，采用“情景法”解决随机优化问题。在模拟能源中心模型和来自实际建筑的需求数据上验证了所提出预测器和随机控制器的性能。

    Energy hubs convert and distribute energy resources by combining different energy inputs through multiple conversion and storage components. The optimal operation of the energy hub exploits its flexibility to increase the energy efficiency and reduce the operational costs. However, uncertainties in the demand present challenges to energy hub optimization. In this paper, we propose a stochastic MPC controller to minimize energy costs using chance constraints for the uncertain electricity and thermal demands. Historical data is used to build a demand prediction model based on Gaussian processes to generate a forecast of the future electricity and heat demands. The stochastic optimization problem is solved via the Scenario Approach by sampling multi-step demand trajectories from the derived prediction model. The performance of the proposed predictor and of the stochastic controller is verified on a simulated energy hub model and demand data from a real building.
    
[^90]: 生成对抗神经进化用于控制行为模仿

    Generative Adversarial Neuroevolution for Control Behaviour Imitation. (arXiv:2304.12432v1 [cs.NE])

    [http://arxiv.org/abs/2304.12432](http://arxiv.org/abs/2304.12432)

    本文研究了使用生成对抗神经进化技术进行行为模仿的方法，在8个OpenAI Gym状态控制任务上，最终的代理能够达到与预训练代理一样高的得分，展示了深度神经进化技术在模仿学习中的潜力。

    

    最近对于模仿学习的兴趣急剧增加，大量的人类视频游戏和机器人操作数据集被用来训练代理在非常复杂的任务上。虽然深度神经进化最近已经显示可以匹配基于梯度的技术在各种强化学习问题上的性能，但将深度神经进化技术应用于模仿学习仍相对未被探索。在这项工作中，我们提出探索深度神经进化是否可以用于在流行的仿真环境中进行行为模仿 。我们引入了一个简单的协同进化的对抗生成框架，并通过演化标准深度递归网络来模仿8个OpenAI Gym状态控制任务上的最先进的预训练代理，并评估其能力。在所有任务中，我们发现最终的优秀执行器代理能够达到与预训练代理获得的得分一样高的得分，同时紧密地跟随它们的得分轨迹。我们的结果证明了所提出的方法的有效性。

    There is a recent surge in interest for imitation learning, with large human video-game and robotic manipulation datasets being used to train agents on very complex tasks. While deep neuroevolution has recently been shown to match the performance of gradient-based techniques on various reinforcement learning problems, the application of deep neuroevolution techniques to imitation learning remains relatively unexplored. In this work, we propose to explore whether deep neuroevolution can be used for behaviour imitation on popular simulation environments. We introduce a simple co-evolutionary adversarial generation framework, and evaluate its capabilities by evolving standard deep recurrent networks to imitate state-of-the-art pre-trained agents on 8 OpenAI Gym state-based control tasks. Across all tasks, we find the final elite actor agents capable of achieving scores as high as those obtained by the pre-trained agents, all the while closely following their score trajectories. Our result
    
[^91]: “在控制任务上的递归结构的神经演化。”

    Neuroevolution of Recurrent Architectures on Control Tasks. (arXiv:2304.12431v1 [cs.NE])

    [http://arxiv.org/abs/2304.12431](http://arxiv.org/abs/2304.12431)

    本文提出了一种新颖的方法，通过一小组变异规则动态演化递归神经网络的架构，实现了在强化学习控制任务上的神经演化。

    

    现代人工智能工作通常使用基于渐变的优化技术来训练固定大小的深层神经网络的参数。最近已经证明，简单的进化算法也可以优化深度神经网络的参数，有时可以匹配基于梯度的技术在强化学习设置中的性能。除了优化网络参数外，许多进化计算技术还能够逐步构建网络架构。然而，从基本进化规则中构建网络体系结构尚未显示出在现代强化学习基准测试中具有可扩展性。因此，在本文中，我们提出了一种新方法，其中递归神经网络的架构根据一小组突变规则动态演变。我们实现了一个大规模并行的进化算法，并在所有19个OpenAI Gym状态基础的强化学习控制任务上进行了实验。

    Modern artificial intelligence works typically train the parameters of fixed-sized deep neural networks using gradient-based optimization techniques. Simple evolutionary algorithms have recently been shown to also be capable of optimizing deep neural network parameters, at times matching the performance of gradient-based techniques, e.g. in reinforcement learning settings. In addition to optimizing network parameters, many evolutionary computation techniques are also capable of progressively constructing network architectures. However, constructing network architectures from elementary evolution rules has not yet been shown to scale to modern reinforcement learning benchmarks. In this paper we therefore propose a new approach in which the architectures of recurrent neural networks dynamically evolve according to a small set of mutation rules. We implement a massively parallel evolutionary algorithm and run experiments on all 19 OpenAI Gym state-based reinforcement learning control task
    
[^92]: 稀疏私有LASSO逻辑回归

    Sparse Private LASSO Logistic Regression. (arXiv:2304.12429v1 [cs.LG])

    [http://arxiv.org/abs/2304.12429](http://arxiv.org/abs/2304.12429)

    本文提出了一种稀疏逻辑回归的差分隐私方法，保持硬零，通过训练一个非私有的LASSO逻辑回归模型决定最后的模型选择中恰当的私有化非零系数的数量。

    

    LASSO正则化的逻辑回归因其内置特征选择功能而非常有用，允许从部署中删除系数并产生稀疏解。虽然已经开发了LASSO逻辑回归的差分隐私版本，但通常会产生密集解，从而降低了LASSO惩罚的内在实用性。在本文中，我们提出了一种用于稀疏逻辑回归的差分隐私方法，该方法保持硬零。我们的关键见解是首先训练一个非私有的LASSO逻辑回归模型来确定在最终模型选择中使用的恰当私有化非零系数的数量。为了证明我们方法的性能，我们对合成和真实数据集进行了实验。

    LASSO regularized logistic regression is particularly useful for its built-in feature selection, allowing coefficients to be removed from deployment and producing sparse solutions. Differentially private versions of LASSO logistic regression have been developed, but generally produce dense solutions, reducing the intrinsic utility of the LASSO penalty. In this paper, we present a differentially private method for sparse logistic regression that maintains hard zeros. Our key insight is to first train a non-private LASSO logistic regression model to determine an appropriate privatized number of non-zero coefficients to use in final model selection. To demonstrate our method's performance, we run experiments on synthetic and real-world datasets.
    
[^93]: TIGTEC：基于标记重要性的文本对抗事例生成方法

    TIGTEC : Token Importance Guided TExt Counterfactuals. (arXiv:2304.12425v1 [cs.LG])

    [http://arxiv.org/abs/2304.12425](http://arxiv.org/abs/2304.12425)

    TIGTEC是一个高效的文本对抗事例生成方法，使用本地特征重要性针对和修改对分类结果影响最大的单词，并在代价函数中使用语义距离评估对抗性解释。该方法具有很高的成功率、稀疏性、多样性和可信度，并且既可以是特定于模型的，也可以是无关模型的，非常方便用于生成对抗性解释。

    

    对抗事例是一种通过改变实例来翻转分类器输出以解释模型预测的方法。本文提出了TIGTEC，一种用于生成稀疏、可信和多样性的对抗性解释的高效模块方法。TIGTEC是一种文本编辑启发式方法，使用本地特征重要性针对和修改对分类结果影响最大的单词。提出了一种基于注意力机制的本地特征重要性，并通过一个集成语义距离的代价函数对生成的对抗性解释进行评估。该方法采用波束搜索算法进行高效的解空间搜索。实验结果表明，TIGTEC在成功率、稀疏性、多样性和可信度方面具有很高的实用性。该方法既可以是特定于模型的，也可以是无关模型的，非常方便用于生成对抗性解释。

    Counterfactual examples explain a prediction by highlighting changes of instance that flip the outcome of a classifier. This paper proposes TIGTEC, an efficient and modular method for generating sparse, plausible and diverse counterfactual explanations for textual data. TIGTEC is a text editing heuristic that targets and modifies words with high contribution using local feature importance. A new attention-based local feature importance is proposed. Counterfactual candidates are generated and assessed with a cost function integrating semantic distance, while the solution space is efficiently explored in a beam search fashion. The conducted experiments show the relevance of TIGTEC in terms of success rate, sparsity, diversity and plausibility. This method can be used in both model-specific or model-agnostic way, which makes it very convenient for generating counterfactual explanations.
    
[^94]: 多源到多目标的分布式联邦领域自适应

    Multi-Source to Multi-Target Decentralized Federated Domain Adaptation. (arXiv:2304.12422v1 [cs.DC])

    [http://arxiv.org/abs/2304.12422](http://arxiv.org/abs/2304.12422)

    本文提出了一种分布式联邦学习方法，可将机器学习模型从标记数据丰富的设备转移到未标记数据设备以提高数据利用率。该方法考虑了设备分类和源-目标链接形成的权衡。

    

    联邦学习中设备间的异质性通常指统计（例如，非独立同分布的数据分布）和资源（例如，通信带宽）维度。本文聚焦另一个重要维度：各设备所拥有的标记和未标记数据数量/分布。为了利用所有数据，我们开发了一种分布式联邦领域适应方法，将机器学习模型从标记数据高质量设备（称为源）转移到低质量或未标记数据设备（称为目标）。我们的方法，“源-目标确定和链接形成”（ST-LF），在考虑模型精度和通信能量效率之间的权衡的同时，优化设备分类和源-目标链接形成。

    Heterogeneity across devices in federated learning (FL) typically refers to statistical (e.g., non-i.i.d. data distributions) and resource (e.g., communication bandwidth) dimensions. In this paper, we focus on another important dimension that has received less attention: varying quantities/distributions of labeled and unlabeled data across devices. In order to leverage all data, we develop a decentralized federated domain adaptation methodology which considers the transfer of ML models from devices with high quality labeled data (called sources) to devices with low quality or unlabeled data (called targets). Our methodology, Source-Target Determination and Link Formation (ST-LF), optimizes both (i) classification of devices into sources and targets and (ii) source-target link formation, in a manner that considers the trade-off between ML model accuracy and communication energy efficiency. To obtain a concrete objective function, we derive a measurable generalization error bound that ac
    
[^95]: 水下航行器船体的样本高效和基于代理的设计优化

    Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls. (arXiv:2304.12420v1 [cs.LG])

    [http://arxiv.org/abs/2304.12420](http://arxiv.org/abs/2304.12420)

    该论文使用了高效的样本集优化和基于代理的方法来设计水下航行器船体，其中代理模型显著提高了计算效率，使优化更加快速准确。

    

    物理模拟是计算机辅助设计(CAD)优化过程中的一个计算瓶颈。因此，为了使精确(计算昂贵)的模拟可用于设计优化中，需要一个高样本效率的优化框架或快速的数据驱动代理(代理模型)来代替长时间运行的模拟。在这项工作中，我们利用最近优化和人工智能(AI)的进展来解决这两个潜在的解决方案，以设计一个最佳的无人水下航行器(UUV)。我们首先研究并比较了不同优化技术在优化循环中与标准计算流体力学(CFD)求解器相结合时的样本效率和收敛行为。然后，我们开发了一个基于深度神经网络(DNN)的代理模型来逼近否则通过CFD求解器进行计算的阻力。代理模型进而用于样本高效的优化框架中，该框架在不使用代理模型的情况下优于标准优化方法。

    Physics simulations are a computational bottleneck in computer-aided design (CAD) optimization processes. Hence, in order to make accurate (computationally expensive) simulations feasible for use in design optimization, one requires either an optimization framework that is highly sample-efficient or fast data-driven proxies (surrogate models) for long running simulations. In this work, we leverage recent advances in optimization and artificial intelligence (AI) to address both of these potential solutions, in the context of designing an optimal unmanned underwater vehicle (UUV). We first investigate and compare the sample efficiency and convergence behavior of different optimization techniques with a standard computational fluid dynamics (CFD) solver in the optimization loop. We then develop a deep neural network (DNN) based surrogate model to approximate drag forces that would otherwise be computed via direct numerical simulation with the CFD solver. The surrogate model is in turn use
    
[^96]: 一种基于量子-经典混合方法的受限玻尔兹曼机推断

    A hybrid quantum-classical approach for inference on restricted Boltzmann machines. (arXiv:2304.12418v1 [quant-ph])

    [http://arxiv.org/abs/2304.12418](http://arxiv.org/abs/2304.12418)

    混合量子-经典方法在受限玻尔兹曼机推断中取得了较好的表现，D-Wave量子退火器样本比随机初始化的样本具有更好的性能，量子退火器样本可以进一步改善Gibbs采样的性能。

    

    玻尔兹曼机是一种强大的机器学习模型，在构建深度置信网络等实际应用中发挥重要作用。通过从其后验分布中进行采样可以对Boltzmann机进行统计推断。然而，由于具有极其多峰分布，从这样的模型中进行均匀采样并不容易。量子计算机有可能以高效的方式解决一些困难问题。我们探索了使用D-Wave量子退火器生成受限玻尔兹曼机样本的应用。这些样本在混合量子-经典设置中通过马尔科夫链进一步改善。我们证明，与随机初始化相比，量子退火器样本可以改善Gibbs采样的性能。混合设置比纯经典采样要高效得多。我们还研究了退火参数（温度）的影响，以提高样本的质量。

    Boltzmann machine is a powerful machine learning model with many real-world applications, for example by constructing deep belief networks. Statistical inference on a Boltzmann machine can be carried out by sampling from its posterior distribution. However, uniform sampling from such a model is not trivial due to an extremely multi-modal distribution. Quantum computers have the promise of solving some non-trivial problems in an efficient manner. We explored the application of a D-Wave quantum annealer to generate samples from a restricted Boltzmann machine. The samples are further improved by Markov chains in a hybrid quantum-classical setup. We demonstrated that quantum annealer samples can improve the performance of Gibbs sampling compared to random initialization. The hybrid setup is considerably more efficient than a pure classical sampling. We also investigated the impact of annealing parameters (temperature) to improve the quality of samples. By increasing the amount of classical
    
[^97]: PEFT-Ref: 一种模块化的参考架构和类型，用于参数效率微调技术

    PEFT-Ref: A Modular Reference Architecture and Typology for Parameter-Efficient Finetuning Techniques. (arXiv:2304.12410v1 [cs.CL])

    [http://arxiv.org/abs/2304.12410](http://arxiv.org/abs/2304.12410)

    本文提出了PEFT-Ref参考架构，标准化了不同PEFT技术共享的方面，隔离了差异到特定位置和交互中，模块化的视图有助于比较不同技术及其效率和任务性能，并有助于更好地理解PEFT的基本原理。

    

    最近的参数效率微调(PEFT)技术旨在改善完全微调大型预训练语言模型(PLM)的高昂成本。随着不同的PEFT技术不断出现，对它们进行比较变得越来越困难，特别是在以下方面：(i)它们添加到PLM的结构和功能，(ii)不同类型和程度的效率改进，(iii)在不同的下游任务中的性能，以及(iv)结构和功能差异如何与效率和任务性能相关联。为了促进这样的比较，本文提出了一个参考框架，标准化了不同PEFT技术共享的方面，同时将差异隔离到与标准组件的特定位置和交互中。通过这个标准化和隔离差异的过程，出现了PEFT技术的模块化视图，不仅支持直接比较不同技术及其效率和任务性能，而且还有助于更好地理解PEFT的基本原理。所提出的参考架构称为PEFT-Ref，包括七个核心模块，每个模块都处理PEFT的特定方面，并可用作开发新PEFT技术和比较现有技术的指南。

    Recent parameter-efficient finetuning (PEFT) techniques aim to improve over the considerable cost of fully finetuning large pretrained language models (PLM). As different PEFT techniques proliferate, it is becoming difficult to compare them, in particular in terms of (i) the structure and functionality they add to the PLM, (ii) the different types and degrees of efficiency improvements achieved, (iii) performance at different downstream tasks, and (iv) how differences in structure and functionality relate to efficiency and task performance. To facilitate such comparisons, this paper presents a reference framework which standardises aspects shared by different PEFT techniques, while isolating differences to specific locations and interactions with the standard components. Through this process of standardising and isolating differences, a modular view of PEFT techniques emerges, supporting not only direct comparison of different techniques and their efficiency and task performance, but a
    
[^98]: 通过平方和优化综合稳定的降阶视动态控制器，应用于非线性系统

    Synthesizing Stable Reduced-Order Visuomotor Policies for Nonlinear Systems via Sums-of-Squares Optimization. (arXiv:2304.12405v1 [cs.RO])

    [http://arxiv.org/abs/2304.12405](http://arxiv.org/abs/2304.12405)

    该论文提出了一种通过平方和优化，综合动态的、降阶的输出反馈多项式控制器的方法，应用于控制仿射的非线性系统，并使用学习的感知模块和视觉观测来稳定地运行到目标状态。该方法提供了在观测噪声存在的情况下的稳定性保证。

    

    我们提出了一种方法，用于综合动态的、降阶的输出反馈多项式控制器，应用于控制仿射的非线性系统，并使用学习的感知模块和视觉观测来保证在反馈控制循环中稳定地运行到目标状态。我们利用Lyapunov分析来制定综合这种策略的问题。该问题在策略参数和用于证明策略稳定性的Lyapunov函数方面是非凸的。为了近似地解决这个问题，我们提出了两种方法：第一种解决了一系列平方和优化问题，以迭代改进一个通过构造可证明的稳定策略，而第二种则直接对多项式策略的参数进行基于梯度的优化，并在后验上验证其闭环稳定性。我们扩展了我们的方法，以在观测噪声存在的情况下提供稳定性保证，该情况实际上是由于观测误差而引起的。

    We present a method for synthesizing dynamic, reduced-order output-feedback polynomial control policies for control-affine nonlinear systems which guarantees runtime stability to a goal state, when using visual observations and a learned perception module in the feedback control loop. We leverage Lyapunov analysis to formulate the problem of synthesizing such policies. This problem is nonconvex in the policy parameters and the Lyapunov function that is used to prove the stability of the policy. To solve this problem approximately, we propose two approaches: the first solves a sequence of sum-of-squares optimization problems to iteratively improve a policy which is provably-stable by construction, while the second directly performs gradient-based optimization on the parameters of the polynomial policy, and its closed-loop stability is verified a posteriori. We extend our approach to provide stability guarantees in the presence of observation noise, which realistically arises due to erro
    
[^99]: HDCC：嵌入式系统和高性能计算中进行分类的高维计算编译器

    HDCC: A Hyperdimensional Computing compiler for classification on embedded systems and high-performance computing. (arXiv:2304.12398v1 [cs.LG])

    [http://arxiv.org/abs/2304.12398](http://arxiv.org/abs/2304.12398)

    本文介绍了\name{}编译器，可以将HDC分类方法的高级描述翻译为优化的C代码，使其更适用于在嵌入式系统和高性能计算中进行分类任务。

    

    高维计算（HDC）是一种生物启发式计算框架，尤其作为一种更有效的机器学习（ML）方法而受到越来越多的关注。本文介绍了\name{}编译器，这是第一个将HDC分类方法的高级描述翻译为优化的C代码的开源编译器。所提议的编译器生成的代码具有三个主要特点，适用于嵌入式系统和高性能计算：(1)它是自包含的，没有库或平台依赖;(2)它支持使用C内在函数的多线程和单指令多数据（SIMD）指令;(3)它优化了最大的性能和最小的内存使用率。 \name{}的设计就像一个现代编译器，具有直观和描述性的输入语言，中间表示（IR）和可重新定位的后端。这使\name{}成为一个有价值的研究和应用工具，探索HDC在嵌入式系统和高性能计算中进行分类任务。

    Hyperdimensional Computing (HDC) is a bio-inspired computing framework that has gained increasing attention, especially as a more efficient approach to machine learning (ML). This work introduces the \name{} compiler, the first open-source compiler that translates high-level descriptions of HDC classification methods into optimized C code. The code generated by the proposed compiler has three main features for embedded systems and High-Performance Computing: (1) it is self-contained and has no library or platform dependencies; (2) it supports multithreading and single instruction multiple data (SIMD) instructions using C intrinsics; (3) it is optimized for maximum performance and minimal memory usage. \name{} is designed like a modern compiler, featuring an intuitive and descriptive input language, an intermediate representation (IR), and a retargetable backend. This makes \name{} a valuable tool for research and applications exploring HDC for classification tasks on embedded systems a
    
[^100]: 使用R软件包ctsfeatures分析分类时间序列

    Analyzing categorical time series with the R package ctsfeatures. (arXiv:2304.12332v1 [stat.ML])

    [http://arxiv.org/abs/2304.12332](http://arxiv.org/abs/2304.12332)

    该软件包提供了一组有用的工具来分析分类时间序列，使得用户可以用图形描述底层时间模式并利用其输出执行传统的机器学习任务包括聚类、分类和异常检测。

    

    时间序列数据如今已无处不在。然而，大部分文献都处理实值时间序列，分类时间序列却受到了较少的关注。近年来，这类数据的数据挖掘技术得到了实质性的发展。 R软件包ctsfeatures为用户提供了一组有用的工具来分析分类时间序列。具体而言，该软件包提供了几个函数，允许提取已知的统计特征并构建描述底层时间模式的图形。某些函数的输出可用于执行传统的机器学习任务，包括聚类、分类和异常检测。该软件包还包括两个在文献中介绍的生物序列数据集，用于聚类目的，以及三个有趣的合成数据库。本文描述了该软件包的主要特征及其使用方法。

    Time series data are ubiquitous nowadays. Whereas most of the literature on the topic deals with real-valued time series, categorical time series have received much less attention. However, the development of data mining techniques for this kind of data has substantially increased in recent years. The R package ctsfeatures offers users a set of useful tools for analyzing categorical time series. In particular, several functions allowing the extraction of well-known statistical features and the construction of illustrative graphs describing underlying temporal patterns are provided in the package. The output of some functions can be employed to perform traditional machine learning tasks including clustering, classification and outlier detection. The package also includes two datasets of biological sequences introduced in the literature for clustering purposes, as well as three interesting synthetic databases. In this work, the main characteristics of the package are described and its us
    
[^101]: 基于并行bootstrap的连续流控制应用的on-policy深度强化学习

    Parallel bootstrap-based on-policy deep reinforcement learning for continuous flow control applications. (arXiv:2304.12330v1 [cs.LG])

    [http://arxiv.org/abs/2304.12330](http://arxiv.org/abs/2304.12330)

    本文提出了一种基于并行bootstrap的on-policy深度强化学习方法，通过部分轨迹缓冲区和返回bootstrapping步骤来实现灵活使用并行环境，同时保持更新的on-policy性，该方法在连续流控制问题上有很好的应用前景。

    

    深度强化学习与数值流控问题的耦合近期引起了相当大的关注，取得了突破性的成果并为该领域开辟了新的前景。然而，由于流体动力学求解器的计算成本通常很高，在学习过程中使用并行环境是实现有效控制的必要手段。尽管如此，大多数基于流控的深度强化学习文献仍依赖于on-policy算法，而这种算法的高并行转移收集可能会破坏理论假设并导致次优的控制模型。为了克服这个问题，我们提出了一个基于部分轨迹缓冲区的并行模式，通过一个返回bootstrapping步骤，允许灵活地使用并行环境，同时保持更新的on-policy性。该方法在文献中一个耗费大量计算的连续流控制问题上进行了说明。

    The coupling of deep reinforcement learning to numerical flow control problems has recently received a considerable attention, leading to groundbreaking results and opening new perspectives for the domain. Due to the usually high computational cost of fluid dynamics solvers, the use of parallel environments during the learning process represents an essential ingredient to attain efficient control in a reasonable time. Yet, most of the deep reinforcement learning literature for flow control relies on on-policy algorithms, for which the massively parallel transition collection may break theoretical assumptions and lead to suboptimal control models. To overcome this issue, we propose a parallelism pattern relying on partial-trajectory buffers terminated by a return bootstrapping step, allowing a flexible use of parallel environments while preserving the on-policiness of the updates. This approach is illustrated on a CPU-intensive continuous flow control problem from the literature.
    
[^102]: Virus2Vec: 利用机器学习进行病毒序列分类

    Virus2Vec: Viral Sequence Classification Using Machine Learning. (arXiv:2304.12328v1 [q-bio.GN])

    [http://arxiv.org/abs/2304.12328](http://arxiv.org/abs/2304.12328)

    Virus2Vec 是一种机器学习方法，使用特征向量表示病毒序列，能够识别病毒宿主，其在病毒宿主预测上比当前最先进方法提高了多达16％的精度。

    

    理解不同病毒家族的宿主特异性可以揭示 SARS-CoV-2、狂犬病等动物源性病原体在人类中的起源。这有助于流行病学家、医疗专业人员和政策制定者及时遏制现有的流行病并预防未来的流行病。我们提出了Virus2Vec，它是病毒（核苷酸或氨基酸）序列的特征向量表示，可以让基于向量空间的机器学习模型识别病毒宿主。与当前最先进的方法相比，我们的方法在冠状病毒科和狂犬病毒的宿主预测上将精度提高了多达16％。

    Understanding the host-specificity of different families of viruses sheds light on the origin of, e.g., SARS-CoV-2, rabies, and other such zoonotic pathogens in humans. It enables epidemiologists, medical professionals, and policymakers to curb existing epidemics and prevent future ones promptly. In the family Coronaviridae (of which SARS-CoV-2 is a member), it is well-known that the spike protein is the point of contact between the virus and the host cell membrane. On the other hand, the two traditional mammalian orders, Carnivora (carnivores) and Chiroptera (bats) are recognized to be responsible for maintaining and spreading the Rabies Lyssavirus (RABV). We propose Virus2Vec, a feature-vector representation for viral (nucleotide or amino acid) sequences that enable vector-space-based machine learning models to identify viral hosts. Virus2Vec generates numerical feature vectors for unaligned sequences, allowing us to forego the computationally expensive sequence alignment step from t
    
[^103]: 学习技术在海洋叶绿素分析中的物理化学特性依赖性研究

    Dependence of Physiochemical Features on Marine Chlorophyll Analysis with Learning Techniques. (arXiv:2304.12325v1 [q-bio.QM])

    [http://arxiv.org/abs/2304.12325](http://arxiv.org/abs/2304.12325)

    研究发现，海洋中叶绿素的生长与物理化学成分如铁、硝酸盐、磷酸盐、pH值、盐度等的最佳浓度有关，可以用机器学习预测海洋叶绿素。

    

    海洋浮游植物中存在的叶绿素是光合作用的基础，对维持生态平衡具有重要意义，对全球初级生产力作出了重大贡献，并处于许多海洋生物的食物链中。浮游植物浓度的不平衡可能会破坏生态平衡。浮游植物的生长取决于物理化学成分的最佳浓度，如铁，硝酸盐，磷酸盐，pH值，盐度等的偏离理想浓度可能会影响浮游植物的生长，从而最终破坏生态系统。因此，分析这些成分具有极高的重要性，以估计海洋浮游植物的可能生长。遥感技术的进步改善了全球范围内远程研究物理化学成分的可能性。机器学习技术使得预测海洋叶绿素成为可能。

    Marine chlorophyll which is present within phytoplankton are the basis of photosynthesis and they have a high significance in sustaining ecological balance as they highly contribute toward global primary productivity and comes under the food chain of many marine organisms. Imbalance in the concentrations of phytoplankton can disrupt the ecological balance. The growth of phytoplankton depends upon the optimum concentrations of physiochemical constituents like iron, nitrates, phosphates, pH level, salinity, etc. and deviations from an ideal concentration can affect the growth of phytoplankton which can ultimately disrupt the ecosystem at a large scale. Thus the analysis of such constituents has high significance to estimate the probable growth of marine phytoplankton. The advancements of remote sensing technologies have improved the scope to remotely study the physiochemical constituents on a global scale. The machine learning techniques have made it possible to predict the marine chloro
    
[^104]: 使用Pylogik进行医学影像去标识化和清洗压缩

    Medical Image Deidentification, Cleaning and Compression Using Pylogik. (arXiv:2304.12322v1 [eess.IV])

    [http://arxiv.org/abs/2304.12322](http://arxiv.org/abs/2304.12322)

    提出了一个Python框架下的库PyLogik来帮助超声图像去标识化和清洗压缩，为深度学习和数据共享应用提供图像数据支持。

    

    应用大数据和机器学习在医疗记录信息方面须注意，必须清洗和去标识化数据。当受保护的健康信息嵌入在影像元数据中时，促进多中心合作中数据共享和协调变得尤其困难。我们提出了一个新的Python框架下的库，称为PyLogik，帮助解决超声图像特别具有挑战性的数据清洗问题，因为这些图像直接包含很多PHI。PyLogik通过一系列的文本检测/提取、过滤、阈值化、形态学和轮廓比较处理图像体积。这种方法去标识化图像，减小文件大小，并为深度学习和数据共享应用准备好了图像数据。为了评估PyLogik在兴趣区域（ROI）的识别有效性，随机抽取了50张心脏超声图像（超声心动图）进行处理。

    Leveraging medical record information in the era of big data and machine learning comes with the caveat that data must be cleaned and deidentified. Facilitating data sharing and harmonization for multi-center collaborations are particularly difficult when protected health information (PHI) is contained or embedded in image meta-data. We propose a novel library in the Python framework, called PyLogik, to help alleviate this issue for ultrasound images, which are particularly challenging because of the frequent inclusion of PHI directly on the images. PyLogik processes the image volumes through a series of text detection/extraction, filtering, thresholding, morphological and contour comparisons. This methodology deidentifies the images, reduces file sizes, and prepares image volumes for applications in deep learning and data sharing. To evaluate its effectiveness in the identification of regions of interest (ROI), a random sample of 50 cardiac ultrasounds (echocardiograms) were processed
    
[^105]: 通过识别桥接度重要节点生成Skip-gram节点嵌入的后续解释

    Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2304.12036](http://arxiv.org/abs/2304.12036)

    本文提出了一种解释Skip-gram节点嵌入的方法，即通过计算桥接度识别重要节点，并提出了一种新型基于梯度的解释方法GRAPH-wGD，有效地提供全局性解释。

    

    网络中的节点表示学习是编码连续矢量空间中的关系信息同时保留网络固有属性和结构的重要机器学习技术。最近，DeepWalk、LINE、struc2vec、PTE、UserItem2vec和RWJBG等无监督节点嵌入方法从Skip-gram模型中出现，并在诸如节点分类和链接预测等下游任务中表现出更好的性能。然而，由于缺乏适用于嵌入的解释方法和理论研究，提供Skip-gram嵌入的后续解释仍然是一个具有挑战性的问题。本文首先表明可以通过在谱聚类感知局部扰动下计算桥接度来找到Skip-gram嵌入的全局解释。此外，还提出了一种名为GRAPH-wGD的新型基于梯度的解释方法，允许检索top-q全局性解释，并通过实验证明了其有效性。

    Node representation learning in a network is an important machine learning technique for encoding relational information in a continuous vector space while preserving the inherent properties and structures of the network. Recently, unsupervised node embedding methods such as DeepWalk, LINE, struc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model and perform better performance in several downstream tasks such as node classification and link prediction than the existing relational models. However, providing post-hoc explanations of Skip-gram-based embeddings remains a challenging problem because of the lack of explanation methods and theoretical studies applicable for embeddings. In this paper, we first show that global explanations to the Skip-gram-based embeddings can be found by computing bridgeness under a spectral cluster-aware local perturbation. Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD, is proposed that allows the top-q glo
    
[^106]: 通过改进的积分逼近加速扩散采样过程

    On Accelerating Diffusion-Based Sampling Process via Improved Integration Approximation. (arXiv:2304.11328v1 [cs.LG])

    [http://arxiv.org/abs/2304.11328](http://arxiv.org/abs/2304.11328)

    本文提出了一种通过优化系数来加速流行的基于反向ODE解算的扩散采样过程的方法，优化方法为改进的积分逼近（IIA），在每个反向时间步长，我们建议针对某些选择的系数最小化MSE函数，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解。

    

    一种流行的基于扩散的采样策略尝试有效地解决反向常微分方程（ODEs）。所得ODE求解器的系数由ODE公式，反向离散的时间步长和使用的ODE方法预先确定。本文考虑通过优化某些系数来加速几种流行的基于ODE的采样过程，优化方法为改进的积分逼近（IIA）。在每个反向时间步长，我们建议针对某些选择的系数最小化均方误差（MSE）函数。通过应用一组细粒度时间步长的原始ODE求解器构造MSE，从原理上提供了更精确的积分逼近，以预测下一个扩散隐藏状态，给定预训练的扩散模型，只需要在一批样本上进行特定数量的神经功能评估（NFEs）一次IIA过程即可获得最佳解

    One popular diffusion-based sampling strategy attempts to solve the reverse ordinary differential equations (ODEs) effectively. The coefficients of the obtained ODE solvers are pre-determined by the ODE formulation, the reverse discrete timesteps, and the employed ODE methods. In this paper, we consider accelerating several popular ODE-based sampling processes by optimizing certain coefficients via improved integration approximation (IIA). At each reverse timestep, we propose to minimize a mean squared error (MSE) function with respect to certain selected coefficients. The MSE is constructed by applying the original ODE solver for a set of fine-grained timesteps which in principle provides a more accurate integration approximation in predicting the next diffusion hidden state. Given a pre-trained diffusion model, the procedure for IIA for a particular number of neural functional evaluations (NFEs) only needs to be conducted once over a batch of samples. The obtained optimal solutions f
    
[^107]: 颗粒球计算：一种高效、鲁棒和可解释的自适应多粒度表示和计算方法

    Granular ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v1 [cs.LG])

    [http://arxiv.org/abs/2304.11171](http://arxiv.org/abs/2304.11171)

    本文提出了一种基于颗粒球计算的自适应多粒度表示和计算方法，能够提高机器学习的效率、鲁棒性和可解释性。

    

    人类认知具有“先大后小”的认知机制，因此具有自适应的多粒度描述能力。这导致了有效性、鲁棒性和可解释性等计算特性。本文提出了一种新的基于颗粒球计算的自适应多粒度表示和计算方法。他们将这种方法应用于几个机器学习任务，并证明其相对于其他最先进的方法的有效性。

    Human cognition has a ``large-scale first'' cognitive mechanism, therefore possesses adaptive multi-granularity description capabilities. This results in computational characteristics such as efficiency, robustness, and interpretability. Although most existing artificial intelligence learning methods have certain multi-granularity features, they do not fully align with the ``large-scale first'' cognitive mechanism. Multi-granularity granular-ball computing is an important model method developed in recent years. This method can use granular-balls of different sizes to adaptively represent and cover the sample space, and perform learning based on granular-balls. Since the number of coarse-grained "granular-ball" is smaller than the number of sample points, granular-ball computing is more efficient; the coarse-grained characteristics of granular-balls are less likely to be affected by fine-grained sample points, making them more robust; the multi-granularity structure of granular-balls ca
    
[^108]: 缺失数据下的交通信号控制的强化学习方法

    Reinforcement Learning Approaches for Traffic Signal Control under Missing Data. (arXiv:2304.10722v1 [cs.LG])

    [http://arxiv.org/abs/2304.10722](http://arxiv.org/abs/2304.10722)

    本文提出了在交通路网中缺少传感器的情况下，使用强化学习方法通过补充流量状态或状态和动作来实现自适应控制和条件融合。

    

    强化学习方法在交通信号控制任务中的应用已经取得了比传统的基于规则的方法更好的性能。然而，现实中交通状态的缺失可能经常发生，使得现有的强化学习方法在缺少传感器的路网上无法应用。本文旨在控制交通信号在现实环境下的设置中，其中一些路口没有安装传感器，因此周围没有直接观察数据。在我们所知道的范围内，我们是第一个使用强化学习方法来解决这个现实世界中的交通信号控制问题。具体地，我们提出了两种解决方案：第一种方案补充流量状态以实现自适应控制，第二种方案补充状态和动作以进行条件融合。

    The emergence of reinforcement learning (RL) methods in traffic signal control tasks has achieved better performance than conventional rule-based approaches. Most RL approaches require the observation of the environment for the agent to decide which action is optimal for a long-term reward. However, in real-world urban scenarios, missing observation of traffic states may frequently occur due to the lack of sensors, which makes existing RL methods inapplicable on road networks with missing observation. In this work, we aim to control the traffic signals in a real-world setting, where some of the intersections in the road network are not installed with sensors and thus with no direct observations around them. To the best of our knowledge, we are the first to use RL methods to tackle the traffic signal control problem in this real-world setting. Specifically, we propose two solutions: the first one imputes the traffic states to enable adaptive control, and the second one imputes both stat
    
[^109]: 大规模机器学习中Adam不稳定性的理论研究

    A Theory on Adam Instability in Large-Scale Machine Learning. (arXiv:2304.09871v1 [cs.LG])

    [http://arxiv.org/abs/2304.09871](http://arxiv.org/abs/2304.09871)

    Adam优化算法在大批量的训练下容易出现不稳定现象，并导致训练异常，作者提出了该现象的理论解释。

    

    本文提出了一个之前未被解释的现象的理论，该现象出现在大型语言模型训练时的发散行为中。我们认为这种现象是由于主流的优化算法 Adam 导致的。我们观察到 Adam 可能会进入一种状态，其中参数更新向量有比较大的范数，并且与训练损失景观下的下降方向基本无关，从而导致发散。这种现象更容易在大批量情况下出现，这也是大型语言模型训练的典型设置。为了证明该理论，我们对规模不同的语言模型（70亿，300亿，650亿和5460亿参数）进行了训练运行的观察。

    We present a theory for the previously unexplained divergent behavior noticed in the training of large language models. We argue that the phenomenon is an artifact of the dominant optimization algorithm used for training, called Adam. We observe that Adam can enter a state in which the parameter update vector has a relatively large norm and is essentially uncorrelated with the direction of descent on the training loss landscape, leading to divergence. This artifact is more likely to be observed in the training of a deep model with a large batch size, which is the typical setting of large-scale language model training. To argue the theory, we present observations from the training runs of the language models of different scales: 7 billion, 30 billion, 65 billion, and 546 billion parameters.
    
[^110]: 从单词到音乐：符号音乐生成中的子词分词技术研究

    From Words to Music: A Study of Subword Tokenization Techniques in Symbolic Music Generation. (arXiv:2304.08953v1 [cs.SD])

    [http://arxiv.org/abs/2304.08953](http://arxiv.org/abs/2304.08953)

    本文研究了符号音乐生成中的子词分词技术在生成更长、更具结构的音乐方面的有效性。实验结果表明BPE方法在符号音乐生成中可行且有效。

    

    子词分词在基于Transformer模型的自然语言处理方面已经得到广泛应用。随着Transformer模型在符号音乐领域的应用越来越普及，探究子词分词在符号音乐领域的有效性变得至关重要。本文探讨了符号音乐生成中的子词分词技术（如字节对编码BPE）及其对所生成歌曲整体结构的影响。我们的实验基于三种MIDI数据集：单音轨旋律、多轨单乐器和多轨多乐器。我们在音乐标记化之后应用子词分词方案，并发现它可以在相同时间内生成更长的歌曲，并且在结构指标（SI）、音高级别信息熵等客观指标方面改善了生成的音乐整体结构。我们还比较了两种子词分词方法，BPE和一种基于音高的方法。本文的实验结果表明，BPE方法在符号音乐生成中可行且有效。

    Subword tokenization has been widely successful in text-based natural language processing (NLP) tasks with Transformer-based models. As Transformer models become increasingly popular in symbolic music-related studies, it is imperative to investigate the efficacy of subword tokenization in the symbolic music domain. In this paper, we explore subword tokenization techniques, such as byte-pair encoding (BPE), in symbolic music generation and its impact on the overall structure of generated songs. Our experiments are based on three types of MIDI datasets: single track-melody only, multi-track with a single instrument, and multi-track and multi-instrument. We apply subword tokenization on post-musical tokenization schemes and find that it enables the generation of longer songs at the same time and improves the overall structure of the generated music in terms of objective metrics like structure indicator (SI), Pitch Class Entropy, etc. We also compare two subword tokenization methods, BPE a
    
[^111]: RobCaps: 评估胶囊网络在仿射变换和对抗性攻击中的鲁棒性

    RobCaps: Evaluating the Robustness of Capsule Networks against Affine Transformations and Adversarial Attacks. (arXiv:2304.03973v1 [cs.LG])

    [http://arxiv.org/abs/2304.03973](http://arxiv.org/abs/2304.03973)

    本文评估了CapsNet在仿射变换和对抗性攻击方面的鲁棒性。在MNIST，GTSRB和CIFAR10数据集上的测试结果显示，与传统CNN相比，CapsNet实现了更好的对抗示例和仿射变换鲁棒性。

    

    胶囊网络(CapsNet)能够在图像分类任务中分层保持多个对象之间的姿态关系。除了实现高准确性外，在安全关键的应用中部署CapsNet的另一个相关因素是其对输入变换和恶意对抗性攻击的鲁棒性。在本文中，我们系统地分析和评估了影响CapsNet鲁棒性的不同因素，与传统的卷积神经网络(CNNs)进行了比较。为了进行全面比较，我们在MNIST，GTSRB和CIFAR10数据集上测试了两个CapsNet模型和两个CNN模型，以及这些数据集的仿射变换版本。通过深入分析，我们展示了这些架构的哪些特性更有助于增加其鲁棒性以及其局限性。总体而言，与类似数量的传统CNN相比，CapsNet实现了更好的对抗示例和仿射变换鲁棒性。

    Capsule Networks (CapsNets) are able to hierarchically preserve the pose relationships between multiple objects for image classification tasks. Other than achieving high accuracy, another relevant factor in deploying CapsNets in safety-critical applications is the robustness against input transformations and malicious adversarial attacks.  In this paper, we systematically analyze and evaluate different factors affecting the robustness of CapsNets, compared to traditional Convolutional Neural Networks (CNNs). Towards a comprehensive comparison, we test two CapsNet models and two CNN models on the MNIST, GTSRB, and CIFAR10 datasets, as well as on the affine-transformed versions of such datasets. With a thorough analysis, we show which properties of these architectures better contribute to increasing the robustness and their limitations. Overall, CapsNets achieve better robustness against adversarial examples and affine transformations, compared to a traditional CNN with a similar number 
    
[^112]: 面向隐私保护联邦蒸馏的有选择知识共享方法

    Selective Knowledge Sharing for Privacy-Preserving Federated Distillation without A Good Teacher. (arXiv:2304.01731v1 [cs.LG])

    [http://arxiv.org/abs/2304.01731](http://arxiv.org/abs/2304.01731)

    本文提出了有选择的联邦蒸馏机制Selective-FD，可以精确地识别来自本地和集合预测的知识，以解决局部数据分布的变异和缺乏好的教师模型而导致的误导和模糊的知识共享问题，并取得了显著提高的模型性能和准确度。

    

    联邦学习是一种隐私保护的协作学习方法，但是容易受到白盒攻击，并且难以适应异构客户端。基于知识蒸馏的联邦蒸馏是一种提供增强隐私保证并解决模型异构性的替代范例。本文提出了一种有选择的联邦蒸馏机制Selective-FD来应对局部数据分布的变异和缺乏好的教师模型而导致的误导和模糊的知识共享问题。它包括客户端选择器和服务器选择器，以精确地识别来自本地和集合预测的知识。实证研究表明，Selective-FD可显著提高模型性能和准确度。

    While federated learning is promising for privacy-preserving collaborative learning without revealing local data, it remains vulnerable to white-box attacks and struggles to adapt to heterogeneous clients. Federated distillation (FD), built upon knowledge distillation--an effective technique for transferring knowledge from a teacher model to student models--emerges as an alternative paradigm, which provides enhanced privacy guarantees and addresses model heterogeneity. Nevertheless, challenges arise due to variations in local data distributions and the absence of a well-trained teacher model, which leads to misleading and ambiguous knowledge sharing that significantly degrades model performance. To address these issues, this paper proposes a selective knowledge sharing mechanism for FD, termed Selective-FD. It includes client-side selectors and a server-side selector to accurately and precisely identify knowledge from local and ensemble predictions, respectively. Empirical studies, bac
    
[^113]: 一种具有行列交错像素合成的高效尺度不变生成器

    Efficient Scale-Invariant Generator with Column-Row Entangled Pixel Synthesis. (arXiv:2303.14157v1 [cs.CV])

    [http://arxiv.org/abs/2303.14157](http://arxiv.org/abs/2303.14157)

    提出了一种高效尺度不变的生成模型，不使用空间卷积或分层架构，通过将层内特征图分解为局部和全局特征并通过插值这些特征生成图像，使得模型具有较小的内存占用和更快的推理速度。

    

    任意比例图像合成为合成在任意比例下合成逼真图像提供了一种高效且可扩展的解决方案，即使超出了2K分辨率范围。然而，现有的基于GAN的解决方案过度依赖于卷积和分层架构，在缩放输出分辨率时会引入不一致性和“纹理粘连”问题。从另一个角度来看，基于INR的生成器从设计上是尺度等变的，但它们巨大的内存占用和缓慢的推理妨碍了这些网络在大规模或实时系统中的应用。在本研究中，我们提出了一种新的生成模型：具有行列交错像素合成的列-行交错像素合成（$\textbf{CREPS}$）。不使用任何空间卷积或从粗到细的设计。为了节省内存占用并使系统可扩展，我们采用了一种新颖的双线表示法，将层内特征图分解为局部和全局特征的独立“厚条”插值这些条带的融合来生成图像。我们的实验表明，$\textbf{CREPS}$在图像质量和可扩展性方面优于现有技术，在保持更小的内存占用和更快的推理速度的同时。

    Any-scale image synthesis offers an efficient and scalable solution to synthesize photo-realistic images at any scale, even going beyond 2K resolution. However, existing GAN-based solutions depend excessively on convolutions and a hierarchical architecture, which introduce inconsistency and the $``$texture sticking$"$ issue when scaling the output resolution. From another perspective, INR-based generators are scale-equivariant by design, but their huge memory footprint and slow inference hinder these networks from being adopted in large-scale or real-time systems. In this work, we propose $\textbf{C}$olumn-$\textbf{R}$ow $\textbf{E}$ntangled $\textbf{P}$ixel $\textbf{S}$ynthesis ($\textbf{CREPS}$), a new generative model that is both efficient and scale-equivariant without using any spatial convolutions or coarse-to-fine design. To save memory footprint and make the system scalable, we employ a novel bi-line representation that decomposes layer-wise feature maps into separate $``$thick
    
[^114]: 基于复合优化算法的Sigmoid网络求解方法

    Composite Optimization Algorithms for Sigmoid Networks. (arXiv:2303.00589v2 [math.OC] UPDATED)

    [http://arxiv.org/abs/2303.00589](http://arxiv.org/abs/2303.00589)

    本文使用复合优化算法解决Sigmoid网络问题，可以在非凸和非光滑问题的情况下收敛到全局最优解，并提供了一般指导来设置网络大小。

    

    本文使用复合优化算法求解Sigmoid网络问题，将Sigmoid网络转化为一个凸复合优化问题，并提出了基于线性化近端算法和交替方向乘子法的复合优化算法。在假设具有弱锐最小值和正则条件的情况下，该算法可以在非凸和非光滑问题的情况下收敛到目标函数的全局最优解。此外，收敛结果可以直接与训练数据量相关联，为设置Sigmoid网络的大小提供一般指导。在Franke函数拟合和手写数字识别方面的数值实验表明，所提出的算法表现良好且稳健。

    In this paper, we use composite optimization algorithms to solve sigmoid networks. We equivalently transfer the sigmoid networks to a convex composite optimization and propose the composite optimization algorithms based on the linearized proximal algorithms and the alternating direction method of multipliers. Under the assumptions of the weak sharp minima and the regularity condition, the algorithm is guaranteed to converge to a globally optimal solution of the objective function even in the case of non-convex and non-smooth problems. Furthermore, the convergence results can be directly related to the amount of training data and provide a general guide for setting the size of sigmoid networks. Numerical experiments on Franke's function fitting and handwritten digit recognition show that the proposed algorithms perform satisfactorily and robustly.
    
[^115]: 简化基于动量的黎曼子流形优化

    Simplifying Momentum-based Riemannian Submanifold Optimization. (arXiv:2302.09738v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.09738](http://arxiv.org/abs/2302.09738)

    本文针对黎曼子流形优化算法进行了简化，提出了黎曼正常坐标的广义版本，可用于对称正定矩阵的子流形优化，并为深度学习开发了高效的二阶优化器，无需显式矩阵求逆。

    

    带有动量的黎曼子流形优化在计算上是具有挑战性的，因为确保迭代保持在子流形上通常需要解决困难的微分方程。本文针对具有仿射不变度量的对称正定矩阵的子流形优化算法进行了简化。我们提出了黎曼正常坐标的广义版本，可以将问题动态地简化为欧几里得无约束问题。我们使用我们的方法来解释和简化现有的结构化协方差方法，并为深度学习开发了高效的二阶优化器，而无需显式矩阵求逆。

    Riemannian submanifold optimization with momentum is computationally challenging because ensuring iterates remain on the submanifold often requires solving difficult differential equations. We simplify such optimization algorithms for the submanifold of symmetric positive-definite matrices with the affine invariant metric. We propose a generalized version of the Riemannian normal coordinates which dynamically trivializes the problem into a Euclidean unconstrained problem. We use our approach to explain and simplify existing approaches for structured covariances and develop efficient second-order optimizers for deep learning without explicit matrix inverses.
    
[^116]: 游戏化能否减轻mHealth应用中自我报告的负担？利用智能手表数据的机器学习进行认知负荷估计的可行性研究。

    Can gamification reduce the burden of self-reporting in mHealth applications? A feasibility study using machine learning from smartwatch data to estimate cognitive load. (arXiv:2302.03616v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03616](http://arxiv.org/abs/2302.03616)

    通过机器学习技术和智能手表数据进行认知负荷估计，研究发现游戏化的自我报告方式与传统方式的认知负荷没有差异，但参与者更喜欢游戏化版本。

    

    数字化治疗的有效性可以通过要求患者通过应用程序自我报告其状态来衡量，然而，这可能会令人不知所措并导致失去参与度。我们进行了一项研究，探讨游戏化对自我报告的影响。我们的方法涉及创建一个系统，通过分析光-血容积变化信号来评估认知负荷（CL）。利用11名参与者的数据来训练机器学习模型来检测CL。随后，我们创建了两个版本的调查问卷：一个是游戏化版本，一个是传统版本。我们估计其他参与者（13名）在完成调查问卷时经历的CL。我们发现，通过预先在应激检测任务中进行预训练，可以增强CL检测器的性能。对于13名参与者中的10名，个性化CL检测器可以实现高于0.7的F1得分。我们发现，在CL方面，游戏化和非游戏化的调查问卷没有区别，但参与者更喜欢游戏化的版本。

    The effectiveness of digital treatments can be measured by requiring patients to self-report their state through applications, however, it can be overwhelming and causes disengagement. We conduct a study to explore the impact of gamification on self-reporting. Our approach involves the creation of a system to assess cognitive load (CL) through the analysis of photoplethysmography (PPG) signals. The data from 11 participants is utilized to train a machine learning model to detect CL. Subsequently, we create two versions of surveys: a gamified and a traditional one. We estimate the CL experienced by other participants (13) while completing surveys. We find that CL detector performance can be enhanced via pre-training on stress detection tasks. For 10 out of 13 participants, a personalized CL detector can achieve an F1 score above 0.7. We find no difference between the gamified and non-gamified surveys in terms of CL but participants prefer the gamified version.
    
[^117]: 在Ricci流下学习离散神经网络

    Learning Discretized Neural Networks under Ricci Flow. (arXiv:2302.03390v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.03390](http://arxiv.org/abs/2302.03390)

    本文使用信息几何构造了线性几乎欧几里得流形，通过引入偏微分方程Ricci流，解决了离散神经网络训练中梯度无穷或零的问题。

    

    本文考虑了由低精度权重和激活函数构成的离散神经网络（DNNs）在训练过程中由于非可微分离散函数而遭受无穷或零梯度的问题。本文针对此问题，提出了把 STE近似梯度看作整体偏差的度量扰动，通过对偶理论将其看作黎曼流形上的度量扰动，并在信息几何的基础上为 DNN构造了线性几乎欧几里得（LNE）流形以处理扰动。

    In this paper, we consider Discretized Neural Networks (DNNs) consisting of low-precision weights and activations, which suffer from either infinite or zero gradients due to the non-differentiable discrete function in the training process. In this case, most training-based DNNs employ the standard Straight-Through Estimator (STE) to approximate the gradient w.r.t. discrete values. However, the STE gives rise to the problem of gradient mismatch, due to the perturbations of the approximated gradient. To address this problem, this paper reveals that this mismatch can be viewed as a metric perturbation in a Riemannian manifold through the lens of duality theory. Further, on the basis of the information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold for DNNs as a background to deal with perturbations. By introducing a partial differential equation on metrics, i.e., the Ricci flow, we prove the dynamical stability and convergence of the LNE metric with the $L^2$-norm per
    
[^118]: Listen2Scene：交互式物质感知双耳音频传播重构三维场景

    Listen2Scene: Interactive material-aware binaural soundbpropagation for reconstructed 3D scenes. (arXiv:2302.02809v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2302.02809](http://arxiv.org/abs/2302.02809)

    本文提出了一种交互式的物质感知双耳音频传播方法，能够生成真实环境下的渲染音频，利用图神经网络和条件生成对抗网络，处理重构三维模型中的缺陷，并且能够精确生成与真实环境相符的声学输出。

    

    本文提出了一种用于虚拟现实（VR）和增强现实（AR）应用的端到端双耳音频渲染方法（Listen2Scene）。我们提出了一种新颖的基于神经网络的双耳声学传播方法，以生成真实环境的3D模型的声学效果。任何清洁音频或干音频都可以与生成的声学效果卷积，以渲染与真实环境相对应的音频。我们提出了一个图神经网络，利用3D场景的材料和拓扑信息生成场景潜在向量。此外，我们使用条件生成对抗网络（CGAN）从场景潜在向量生成声学效果。我们的网络能够处理重构的三维网格模型中的孔洞或其他伪像。我们提出了一种高效的成本函数，用于生成器网络以整合空间音频效果。给定源和听者位置，我们的基于学习的双耳声音传播方法可以生成与真实环境精度相符的声学输出。

    We present an end-to-end binaural audio rendering approach (Listen2Scene) for virtual reality (VR) and augmented reality (AR) applications. We propose a novel neural-network-based binaural sound propagation method to generate acoustic effects for 3D models of real environments. Any clean audio or dry audio can be convolved with the generated acoustic effects to render audio corresponding to the real environment. We propose a graph neural network that uses both the material and the topology information of the 3D scenes and generates a scene latent vector. Moreover, we use a conditional generative adversarial network (CGAN) to generate acoustic effects from the scene latent vector. Our network is able to handle holes or other artifacts in the reconstructed 3D mesh model. We present an efficient cost function to the generator network to incorporate spatial audio effects. Given the source and the listener position, our learning-based binaural sound propagation approach can generate an acou
    
[^119]: 快速在线值最大化预测集和符合规范成本控制。

    Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control. (arXiv:2302.00839v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00839](http://arxiv.org/abs/2302.00839)

    本文提出了一种通用管道（FavMac），可以最大化价值并控制成本。 FavMac可以与几乎任何多标签分类器相结合，并通过在线更新机制处理实际的大规模应用，为成本控制提供无分布理论担保。

    

    许多现实世界的多标签预测问题涉及到必须满足下游使用规定的特定要求的集合值预测。我们关注一个典型的情况，其中这些要求分别编码价值和成本，并相互竞争。我们提出了一种被称为FavMac的通用管道，以在这种情况下最大化价值并控制成本。 FavMac可以与几乎任何多标签分类器相结合，为成本控制提供无分布理论担保。此外，与之前的作品不同，它可以通过精心设计的在线更新机制处理实际的大规模应用，这是值得独立关注的。我们的方法论和理论贡献得到了对多种机器学习分类器和几种数据集的试验支撑。

    Many real-world multi-label prediction problems involve set-valued predictions that must satisfy specific requirements dictated by downstream usage. We focus on a typical scenario where such requirements, separately encoding $\textit{value}$ and $\textit{cost}$, compete with each other. For instance, a hospital might expect a smart diagnosis system to capture as many severe, often co-morbid, diseases as possible (the value), while maintaining strict control over incorrect predictions (the cost). We present a general pipeline, dubbed as FavMac, to maximize the value while controlling the cost in such scenarios. FavMac can be combined with almost any multi-label classifier, affording distribution-free theoretical guarantees on cost control. Moreover, unlike prior works, it can handle real-world large-scale applications via a carefully designed online update mechanism, which is of independent interest. Our methodological and theoretical contributions are supported by experiments on severa
    
[^120]: 假设的最佳选择是最弱的而不是最短的

    The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest. (arXiv:2301.12987v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2301.12987](http://arxiv.org/abs/2301.12987)

    本研究表明，在构建假设的过程中，选择最短的假设不如选择最弱的假设，而弱点是比长度或简单性更好的推广表现代理。

    

    如果$A$和$B$是这样的集合，即$A \subset B$，那么一般化可以被理解为从$A$推断出一个足以构建$B$的假设。可以从$A$推断出任意数量的假设，但只有其中的一些可以推广到$B$。怎样知道哪些假设可能推广？一种策略是选择最短的，将压缩信息的能力与推广的能力（智能的代理）等同起来。我们在主动认知的数学形式主义背景下研究了这一点。我们证明了，压缩既不是最大化表现（用假设推广的概率衡量）的必要条件，也不是充分条件。我们制定了一个与长度或简单性无关的代理，称为弱点。我们表明，如果任务是均匀分布的，则不存在任何代理的选择，可以在所有任务中至少与弱点最大化的表现相同，同时在至少一个任务中表现更好。在比较最大弱点和最小描述长度(MDL)的实验中，我们发现最大弱点在各种任务上表现优于MDL。我们认为弱点是比长度或简单性更好的推广表现代理。

    If $A$ and $B$ are sets such that $A \subset B$, generalisation may be understood as the inference from $A$ of a hypothesis sufficient to construct $B$. One might infer any number of hypotheses from $A$, yet only some of those may generalise to $B$. How can one know which are likely to generalise? One strategy is to choose the shortest, equating the ability to compress information with the ability to generalise (a proxy for intelligence). We examine this in the context of a mathematical formalism of enactive cognition. We show that compression is neither necessary nor sufficient to maximise performance (measured in terms of the probability of a hypothesis generalising). We formulate a proxy unrelated to length or simplicity, called weakness. We show that if tasks are uniformly distributed, then there is no choice of proxy that performs at least as well as weakness maximisation in all tasks while performing strictly better in at least one. In experiments comparing maximum weakness and m
    
[^121]: 用于采样分子晶体结构的刚体流

    Rigid body flows for sampling molecular crystal structures. (arXiv:2301.11355v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11355](http://arxiv.org/abs/2301.11355)

    本文介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计。通过在单位四元数群上定义平滑和表现力强的流以及定义适当的密度，在旋转群上进行训练，我们可以成功地采样分子晶体结构。

    

    正则化流(NF)是一类强大的生成模型，由于其高度灵活和表现力，近年来广受欢迎。在本文中，我们介绍了一种新型的正则化流，专为三维空间中多个物体的位置和方向建模而设计，例如晶体中的分子。我们的方法基于两个关键思想:首先，我们在单位四元数群上定义平滑和表现力强的流，从而可以捕捉刚体的连续旋转运动;其次，我们利用单位四元数的双覆盖特性，在旋转群上定义一个适当的密度。这确保我们的模型可以使用标准的基于似然方法或基于热力学目标密度的变分推断进行训练。我们通过训练两个分子示例的Boltzmann生成器来评估该方法，即四面体系统的多模态密度。

    Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system 
    
[^122]: RangeViT：面向自动驾驶中的三维语义分割的视觉Transformer

    RangeViT: Towards Vision Transformers for 3D Semantic Segmentation in Autonomous Driving. (arXiv:2301.10222v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2301.10222](http://arxiv.org/abs/2301.10222)

    本文旨在探究视觉Transformer是否可以应用于自动驾驶中的3D语义分割中，通过保留与RGB图像相同的骨干结构，这项工作证明了ViTs在结合投影方法，大数据训练和具有噪声鲁棒性的新损失函数后可以取得最先进的结果。

    

    将室外LiDAR点云的语义分割视为二维问题（例如通过距离投影），这是一种有效和流行的方法。这些基于投影的方法通常受益于快速计算，并且与使用其他点云表示的技术相结合，可以实现最先进的结果。目前，投影方法利用2D CNNs，但计算机视觉的最新进展表明，视觉Transformer（ViTs）在许多基于图像的基准测试中已经取得了最先进的成果。在这项工作中，我们质疑是否可以通过ViTs的最新改进来改进三维语义分割的投影方法。我们回答是肯定的，但只有在结合了三个关键因素之后才能实现：（a）ViTs难以训练，并且需要大量的训练数据来学习强大的表示。通过保留与RGB图像相同的骨干结构，我们可以利用对大图像集合的长时间训练的知识，这些集合比相应的点云数据集要小得多。

    Casting semantic segmentation of outdoor LiDAR point clouds as a 2D problem, e.g., via range projection, is an effective and popular approach. These projection-based methods usually benefit from fast computations and, when combined with techniques which use other point cloud representations, achieve state-of-the-art results. Today, projection-based methods leverage 2D CNNs but recent advances in computer vision show that vision transformers (ViTs) have achieved state-of-the-art results in many image-based benchmarks. In this work, we question if projection-based methods for 3D semantic segmentation can benefit from these latest improvements on ViTs. We answer positively but only after combining them with three key ingredients: (a) ViTs are notoriously hard to train and require a lot of training data to learn powerful representations. By preserving the same backbone architecture as for RGB images, we can exploit the knowledge from long training on large image collections that are much c
    
[^123]: 不变Lipschitz赌徒：一个侧观发现方法

    Invariant Lipschitz Bandits: A Side Observation Approach. (arXiv:2212.07524v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2212.07524](http://arxiv.org/abs/2212.07524)

    本文研究了不变Lipschitz赌徒设置，并提出了一种名为\texttt{UniformMesh-N}的算法。使用侧面观察的方法，证明了改进的遗憾上界。

    

    对称出现在许多优化和决策问题中，并吸引了优化界的相当关注：通过利用这样的对称性，可以显著改进寻找最优解的过程。尽管对称性在（离线）优化中取得成功，但在在线优化设置中，特别是在赌徒文献中，其利用还未得到充分的研究。因此，在本文中，我们研究了不变Lipschitz赌徒设置，这是Lipschitz赌徒的一个子类，在该子类中，奖励函数和臂集在一组变换下保持不变。我们引入了一种名为\texttt{UniformMesh-N}的算法，它自然地将侧面观察使用群轨道整合到\texttt{UniformMesh}算法（\cite{Kleinberg2005_UniformMesh}）中，该算法均匀地分割了臂的集合。通过侧面观察方法，我们证明了改进的遗憾上界，其取决于基数。

    Symmetry arises in many optimization and decision-making problems, and has attracted considerable attention from the optimization community: By utilizing the existence of such symmetries, the process of searching for optimal solutions can be improved significantly. Despite its success in (offline) optimization, the utilization of symmetries has not been well examined within the online optimization settings, especially in the bandit literature. As such, in this paper we study the invariant Lipschitz bandit setting, a subclass of the Lipschitz bandits where the reward function and the set of arms are preserved under a group of transformations. We introduce an algorithm named \texttt{UniformMesh-N}, which naturally integrates side observations using group orbits into the \texttt{UniformMesh} algorithm (\cite{Kleinberg2005_UniformMesh}), which uniformly discretizes the set of arms. Using the side-observation approach, we prove an improved regret upper bound, which depends on the cardinalit
    
[^124]: GPViT：一种具有组传播的高分辨率非分层视觉Transformer模型

    GPViT: A High Resolution Non-Hierarchical Vision Transformer with Group Propagation. (arXiv:2212.06795v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.06795](http://arxiv.org/abs/2212.06795)

    本文提出了一种高分辨率非分层视觉Transformer模型GPViT，使用具有高效率的组传播模块实现了特征之间全局信息的交流，在多种视觉识别任务上获得了竞争性或最先进的性能。

    

    本文提出了一种新颖的非分层Transformer模型：Group Propagation Vision Transformer (GPViT)，旨在用于高分辨率特征的普适视觉识别。GPViT提供了一种极其高效的组传播模块以在特征之间交流全局信息，GP Block将特征分组，并通过传播更新组特征的全局信息从而实现了全局信息的交流。我们在多种视觉识别任务上对GPViT进行了评估，包括COCO和LVIS基准上的图像分类和目标检测。实验结果表明，GPViT在需要更少计算资源的同时，实现了竞争性或最先进的性能。

    We present the Group Propagation Vision Transformer (GPViT): a novel nonhierarchical (i.e. non-pyramidal) transformer model designed for general visual recognition with high-resolution features. High-resolution features (or tokens) are a natural fit for tasks that involve perceiving fine-grained details such as detection and segmentation, but exchanging global information between these features is expensive in memory and computation because of the way self-attention scales. We provide a highly efficient alternative Group Propagation Block (GP Block) to exchange global information. In each GP Block, features are first grouped together by a fixed number of learnable group tokens; we then perform Group Propagation where global information is exchanged between the grouped features; finally, global information in the updated grouped features is returned back to the image features through a transformer decoder. We evaluate GPViT on a variety of visual recognition tasks including image classi
    
[^125]: VISEM-Tracking，一份人类精子跟踪数据集

    VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2212.02842](http://arxiv.org/abs/2212.02842)

    本文提供了人类精子跟踪数据集VISEM-Tracking，包含手动注释的包围框坐标和由专家分析的精子特征，并提供未标记的视频以供易于访问和分析，有助于训练监督式机器学习方法，提高在评估精子运动和运动学方面的精度和可靠性。

    

    精子运动的手动评估需要显微镜观察，由于所观察的精子在视野中的快速移动，这是具有挑战性的。为了获得正确的结果，手动评估需要进行广泛的培训。因此，在诊所中，计算机辅助精子分析（CASA）变得越来越常用。尽管如此，需要更多数据来训练监督式机器学习方法，以提高在评估精子运动和运动学方面的精度和可靠性。在这方面，我们提供了一个名为VISEM-Tracking的数据集，其中包含20个30秒的视频记录（包括29,196帧）的湿性精子制备物，具备手动注释的包围框坐标和由该领域的专家分析的一组精子特征。除了已注释的数据，我们还提供了未标记的视频剪辑，以便通过自监督或无监督学习等方法轻松访问和分析数据。作为本文的一部分，我们提出了基线精子检测性能。

    A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
    
[^126]: KGML-xDTD：基于知识图谱的药物治疗预测与机制解释的机器学习框架

    KGML-xDTD: A Knowledge Graph-based Machine Learning Framework for Drug Treatment Prediction and Mechanism Description. (arXiv:2212.01384v2 [q-bio.QM] UPDATED)

    [http://arxiv.org/abs/2212.01384](http://arxiv.org/abs/2212.01384)

    KGML-xDTD是一个用于药物治疗预测和机制解释的知识图谱机器学习框架，通过预测和解释药物和疾病之间的机制来实现药物再利用。

    

    药物再利用是一种成本有效且高效的方法，旨在识别现有药物/化合物的新治疗靶点或疾病（适应症）。本研究提出了KGML-xDTD：一种基于知识图谱的机器学习框架，可解释预测治疗药物与疾病之间的机制。该框架由两个模块组成，不仅可以预测药物/化合物与疾病之间的治疗概率，而且可以通过知识图谱（KG）路径为基础的机制来进行生物学解释。

    Background: Computational drug repurposing is a cost- and time-efficient approach that aims to identify new therapeutic targets or diseases (indications) of existing drugs/compounds. It is especially critical for emerging and/or orphan diseases due to its cheaper investment and shorter research cycle compared with traditional wet-lab drug discovery approaches. However, the underlying mechanisms of action (MOAs) between repurposed drugs and their target diseases remain largely unknown, which is still a main obstacle for computational drug repurposing methods to be widely adopted in clinical settings.  Results: In this work, we propose KGML-xDTD: a Knowledge Graph-based Machine Learning framework for explainably predicting Drugs Treating Diseases. It is a two-module framework that not only predicts the treatment probabilities between drugs/compounds and diseases but also biologically explains them via knowledge graph (KG) path-based, testable mechanisms of action (MOAs). We leverage know
    
[^127]: xTrimoABFold：无多序列比对的新型抗体结构预测方法

    xTrimoABFold: De novo Antibody Structure Prediction without MSA. (arXiv:2212.00735v2 [q-bio.QM] CROSS LISTED)

    [http://arxiv.org/abs/2212.00735](http://arxiv.org/abs/2212.00735)

    xTrimoABFold是一种基于深度抗体语言模型的新型抗体结构预测方法，无需多序列比对，有望促进高通量药物设计的应用。

    

    在抗体工程领域，设计一个新型抗体以正确地结合特定抗原的表位是一项重要的任务。了解抗体结构和其表位可以促进对其功能的机制理解。因此，从其序列预测抗体结构一直是一项高度有价值的任务，而AlphaFold2提供了一种基于蛋白质序列预测蛋白质结构的解决方案，但对于抗体，特别是对于抗体的互补决定区（CDRs），其预测效率和准确性有限制。

    In the field of antibody engineering, an essential task is to design a novel antibody whose paratopes bind to a specific antigen with correct epitopes. Understanding antibody structure and its paratope can facilitate a mechanistic understanding of its function. Therefore, antibody structure prediction from its sequence alone has always been a highly valuable problem for de novo antibody design. AlphaFold2, a breakthrough in the field of structural biology, provides a solution to predict protein structure based on protein sequences and computationally expensive coevolutionary multiple sequence alignments (MSAs). However, the computational efficiency and undesirable prediction accuracy of antibodies, especially on the complementarity-determining regions (CDRs) of antibodies limit their applications in the industrially high-throughput drug design. To learn an informative representation of antibodies, we employed a deep antibody language model (ALM) on curated sequences from the observed a
    
[^128]: AIO-P：将神经网络性能预测扩展到图像分类之外

    AIO-P: Expanding Neural Performance Predictors Beyond Image Classification. (arXiv:2211.17228v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.17228](http://arxiv.org/abs/2211.17228)

    本文提出了一种新颖的All-in-One Predictor（AIO-P），旨在预训练神经预测器，使用来自多个独立计算机视觉（CV）任务域和多个架构空间的架构示例，并转移到未使用的下游CV任务或神经架构上。

    

    评估神经网络性能对于深度神经网络设计至关重要，但是这是一个昂贵的过程。神经预测器提供了一种有效的解决方案，通过将架构视为样本，学习估计其在给定任务上的性能。然而，现有的预测器是任务相关的，主要估计神经网络在图像分类基准测试上的性能。它们也是搜索空间相关的；每个预测器都设计为针对具有预定义拓扑和操作集的特定架构搜索空间进行预测。本文提出了一种新颖的All-in-One Predictor（AIO-P），旨在预训练神经预测器，使用来自多个独立计算机视觉（CV）任务域和多个架构空间的架构示例，并转移到未使用的下游CV任务或神经架构上。我们描述了我们提出的通用图形表示、高效预测器预训练和知识注入技术。

    Evaluating neural network performance is critical to deep neural network design but a costly procedure. Neural predictors provide an efficient solution by treating architectures as samples and learning to estimate their performance on a given task. However, existing predictors are task-dependent, predominantly estimating neural network performance on image classification benchmarks. They are also search-space dependent; each predictor is designed to make predictions for a specific architecture search space with predefined topologies and set of operations. In this paper, we propose a novel All-in-One Predictor (AIO-P), which aims to pretrain neural predictors on architecture examples from multiple, separate computer vision (CV) task domains and multiple architecture spaces, and then transfer to unseen downstream CV tasks or neural architectures. We describe our proposed techniques for general graph representation, efficient predictor pretraining and knowledge infusion techniques, as wel
    
[^129]: GENNAPE：面向通用神经架构性能估计的方法

    GENNAPE: Towards Generalized Neural Architecture Performance Estimators. (arXiv:2211.17226v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.17226](http://arxiv.org/abs/2211.17226)

    本文提出了一个名为GENNAPE的通用神经架构性能估计器，可以泛化到完全未知的架构中，通过创新的网络表示、对比预训练和基于模糊聚类的预测器集成方法实现。

    

    预测神经网络架构性能是一项具有挑战性且关键的任务，对于神经架构设计与搜索十分重要。现有方法要么依赖神经网络性能预测器，在预先定义好的设计空间内建模架构，难以推广到未知的架构，要么采用零成本代理，但不总是准确。为此，本文提出了GENNAPE，一种预先训练于开放神经架构基准的通用神经架构性能估计器，旨在通过网络表示、对比预训练和基于模糊聚类的预测器集成等创新实现对完全未知架构的泛化。具体来说，GENNAPE将给定的神经网络表示为原子操作的计算图，可以模拟任意架构。它首先通过对比学习学习图表编码器，以鼓励网络分离

    Predicting neural architecture performance is a challenging task and is crucial to neural architecture design and search. Existing approaches either rely on neural performance predictors which are limited to modeling architectures in a predefined design space involving specific sets of operators and connection rules, and cannot generalize to unseen architectures, or resort to zero-cost proxies which are not always accurate. In this paper, we propose GENNAPE, a Generalized Neural Architecture Performance Estimator, which is pretrained on open neural architecture benchmarks, and aims to generalize to completely unseen architectures through combined innovations in network representation, contrastive pretraining, and fuzzy clustering-based predictor ensemble. Specifically, GENNAPE represents a given neural network as a Computation Graph (CG) of atomic operations which can model an arbitrary architecture. It first learns a graph encoder via Contrastive Learning to encourage network separati
    
[^130]: 关于图神经网络模拟顶点间相互作用的研究

    On the Ability of Graph Neural Networks to Model Interactions Between Vertices. (arXiv:2211.16494v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.16494](http://arxiv.org/abs/2211.16494)

    本文研究了GNN模拟顶点间相互作用的能力，通过一个被称为分离秩的度量标准来量化这种能力，结果表明模拟相互作用的能力主要取决于分区的行走指数，即从分界线开始的行走数量，同时设计了一种名为WISA的边稀疏化算法以提高GNNs的处理效率和表达能力。

    

    图神经网络(GNNs)被广泛用于建模由图中顶点表示的实体之间的复杂互动。尽管最近有一些理论分析GNNs表达能力的努力，但对其模拟相互作用的能力缺乏一个正式的描述。本文旨在填补这一空白。通过一个已知的度量标准——分离秩(separation rank)来规范化相互作用的强度，我们量化了某些GNNs模拟给定顶点子集及其补集之间交互的能力，即输入顶点组成的给定分区的两侧之间的互动。我们的结果表明，模拟相互作用的能力主要取决于分区的行走指数(walk index)——一个由分界线开始的行走数量定义的图形特征。常见GNN架构的实验证明了这一发现。作为我们理论的实际应用，我们设计了一种名为Walk Indexed Sparsification Algorithm (WISA)的边稀疏化算法，利用我们的研究结果提高处理大规模图形的GNNs效率同时保持它们的表达能力。

    Graph neural networks (GNNs) are widely used for modeling complex interactions between entities represented as vertices of a graph. Despite recent efforts to theoretically analyze the expressive power of GNNs, a formal characterization of their ability to model interactions is lacking. The current paper aims to address this gap. Formalizing strength of interactions through an established measure known as separation rank, we quantify the ability of certain GNNs to model interaction between a given subset of vertices and its complement, i.e. between the sides of a given partition of input vertices. Our results reveal that the ability to model interaction is primarily determined by the partition's walk index -- a graph-theoretical characteristic defined by the number of walks originating from the boundary of the partition. Experiments with common GNN architectures corroborate this finding. As a practical application of our theory, we design an edge sparsification algorithm named Walk Inde
    
[^131]: GREAD: 基于图神经反应扩散网络的研究

    GREAD: Graph Neural Reaction-Diffusion Networks. (arXiv:2211.14208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.14208](http://arxiv.org/abs/2211.14208)

    本文介绍了一种基于反应扩散方程的GNN方法，考虑了所有流行的反应方程类型和一种特殊的反应方程，是目前其中最全面的研究之一，并在实验中表现出更好的性能。

    

    图神经网络是深度学习中最受欢迎的研究课题之一。GNN方法通常是基于图信号处理理论进行设计的。特别地，扩散方程被广泛用于设计GNN的核心处理层，因此不可避免地容易出现过度平滑问题。最近，有几篇论文注意到反应方程和扩散方程的结合。不过，它们都只考虑了有限的反应方程形式。因此，我们提出了一种基于反应扩散方程的GNN方法，考虑了所有流行的反应方程类型和我们设计的一种特殊反应方程。据我们所知，我们的论文是关于基于反应扩散方程的GNN的最全面的研究之一。在我们使用9个数据集和28个基准模型进行的实验中，我们的方法 GREAD 在大多数情况下性能优于它们。进一步的人工数据实验显示……

    Graph neural networks (GNNs) are one of the most popular research topics for deep learning. GNN methods typically have been designed on top of the graph signal processing theory. In particular, diffusion equations have been widely used for designing the core processing layer of GNNs, and therefore they are inevitably vulnerable to the notorious oversmoothing problem. Recently, a couple of papers paid attention to reaction equations in conjunctions with diffusion equations. However, they all consider limited forms of reaction equations. To this end, we present a reaction-diffusion equation-based GNN method that considers all popular types of reaction equations in addition to one special reaction equation designed by us. To our knowledge, our paper is one of the most comprehensive studies on reaction-diffusion equation-based GNNs. In our experiments with 9 datasets and 28 baselines, our method, called GREAD, outperforms them in a majority of cases. Further synthetic data experiments show
    
[^132]: 实践中的隐私：X射线图像中的COVID-19检测的私有化（扩展版）

    Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version). (arXiv:2211.11434v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.11434](http://arxiv.org/abs/2211.11434)

    该研究提出了通过差分隐私保护COVID-19检测模型，解决数据分析和患者隐私保护的问题。通过黑盒成员推理攻击，实现了对实际隐私的评估，结论表明所需的隐私等级可能因受到实际威胁的任务而异。

    

    机器学习（ML）可以通过使大量图像快速筛选来帮助抗击COVID-19等全球大流行病。为了在保护患者隐私的同时进行数据分析，我们创建了满足差分隐私（DP）要求的ML模型。以往探索私有COVID-19模型的研究在一定程度上基于小型数据集，提供较弱或不明确的隐私保证，并且没有研究实际隐私。我们提出改进措施以解决这些空缺。我们考虑天生的类别不平衡，并更广泛地评估效用-隐私权衡以及更严格的隐私预算。我们的评估得到黑盒成员推理攻击（MIAs）的实际隐私估计支持。引入的DP应有助于限制MIAs带来的泄漏威胁，而我们的实际分析是第一个在COVID-19分类任务上测试这个假设的。

    Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat 
    
[^133]: 面向绘画中的人脸和身体检测的自适应自监督预训练

    Domain-Adaptive Self-Supervised Pre-Training for Face & Body Detection in Drawings. (arXiv:2211.10641v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.10641](http://arxiv.org/abs/2211.10641)

    本文展示了如何通过自监督学习构建人脸和身体检测器，同时利用大量未标注的数据。其中利用样式转移整合跨域标注图像来引导检测器。

    

    绘画是一种强大的图像抽象和交流手段。理解各种各样的绘画形式，包括数字艺术、漫画和连环画，一直是计算机视觉和计算机图形学界关注的主要问题。本文展示了如何利用基于教师-学生网络的自监督学习和修改后的学生网络更新设计，构建人脸和身体检测器。我们的设置允许在只提供目标域的一小部分标注数据的情况下，利用大量未标注的数据。我们进一步证明，可以将样式转移整合到我们的学习流程中，使用来自自然图像（即来自真实世界的图像）的大量跨域标注图像来引导检测器。

    Drawings are powerful means of pictorial abstraction and communication. Understanding diverse forms of drawings, including digital arts, cartoons, and comics, has been a major problem of interest for the computer vision and computer graphics communities. Although there are large amounts of digitized drawings from comic books and cartoons, they contain vast stylistic variations, which necessitate expensive manual labeling for training domain-specific recognizers. In this work, we show how self-supervised learning, based on a teacher-student network with a modified student network update design, can be used to build face and body detectors. Our setup allows exploiting large amounts of unlabeled data from the target domain when labels are provided for only a small subset of it. We further demonstrate that style transfer can be incorporated into our learning pipeline to bootstrap detectors using a vast amount of out-of-domain labeled images from natural images (i.e., images from the real w
    
[^134]: 基于统计检验的MMD-B-Fair：学习公平的表示

    MMD-B-Fair: Learning Fair Representations with Statistical Testing. (arXiv:2211.07907v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.07907](http://arxiv.org/abs/2211.07907)

    提出了一种基于统计检验的 MMD-B-Fair 方法，用于学习公平的数据表示，并在各种数据集上得到了验证。

    

    我们提出了一种通过核双样本测试学习数据公平表示的方法MMD-B-Fair。我们找到了数据的神经特征，其中最大平均偏差（MMD）测试无法区分不同敏感组的表示，同时保留有关目标属性的信息。我们的方法利用块测试方案的简单渐近性能够有效地找到公平表示，而不需要使用现有公平表示学习方法中广泛使用的复杂对抗性优化或生成建模方案。我们在各种数据集上评估了我们的方法，显示其能够“隐藏”有关敏感属性的信息，并在下游传输任务中的有效性。

    We introduce a method, MMD-B-Fair, to learn fair representations of data via kernel two-sample testing. We find neural features of our data where a maximum mean discrepancy (MMD) test cannot distinguish between representations of different sensitive groups, while preserving information about the target attributes. Minimizing the power of an MMD test is more difficult than maximizing it (as done in previous work), because the test threshold's complex behavior cannot be simply ignored. Our method exploits the simple asymptotics of block testing schemes to efficiently find fair representations without requiring complex adversarial optimization or generative modelling schemes widely used by existing work on fair representation learning. We evaluate our approach on various datasets, showing its ability to ``hide'' information about sensitive attributes, and its effectiveness in downstream transfer tasks.
    
[^135]: 安全潜向扩散：缓解扩散模型中不当退化

    Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. (arXiv:2211.05105v3 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.05105](http://arxiv.org/abs/2211.05105)

    该论文提出了一种名为安全潜向扩散的方法，可以在图像生成过程中移除和抑制不当的图像部分，从而缓解基于文本的图像生成模型因不当数据集带来的不良影响。

    

    最近，基于文本的图像生成模型在图像质量和文本对齐方面取得了惊人的成果，并因此被广泛应用于越来越多的应用程序。由于它们高度依赖于随机从互联网上抓取的数十亿大小的数据集，因此它们也面临来自退化和偏见的人类行为的不良影响，正如我们所展示的那样。反过来，它们甚至可能强化这些偏见。为了帮助应对这些不良影响，我们提出了安全潜向扩散（SLD）。具体而言，为了衡量由于未过滤和不平衡的训练集而引起的不当退化，我们建立了一个新颖的图像生成测试平台——包含专门的、覆盖裸露和暴力等概念的实际图像到文本提示的不当图像提示（I2P）。正如我们详尽的实证评估所证明的那样，引入的SLD在扩散过程中移除和抑制了不当的图像部分，无需额外的训练，并且对图像质量没有不良影响。

    Text-conditioned image generation models have recently achieved astonishing results in image quality and text alignment and are consequently employed in a fast-growing number of applications. Since they are highly data-driven, relying on billion-sized datasets randomly scraped from the internet, they also suffer, as we demonstrate, from degenerated and biased human behavior. In turn, they may even reinforce such biases. To help combat these undesired side effects, we present safe latent diffusion (SLD). Specifically, to measure the inappropriate degeneration due to unfiltered and imbalanced training sets, we establish a novel image generation test bed-inappropriate image prompts (I2P)-containing dedicated, real-world image-to-text prompts covering concepts such as nudity and violence. As our exhaustive empirical evaluation demonstrates, the introduced SLD removes and suppresses inappropriate image parts during the diffusion process, with no additional training required and no adverse e
    
[^136]: StructDiffusion：使用未知对象进行物理有效结构的语言指导创建

    StructDiffusion: Language-Guided Creation of Physically-Valid Structures using Unseen Objects. (arXiv:2211.04604v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2211.04604](http://arxiv.org/abs/2211.04604)

    本论文提出了一种结合扩散模型和以物体为中心的转换器的方法，通过高层语言目标和局部视点云构建物理有效的结构，该方法可用于多个具有挑战性的多步骤3D规划任务，即使使用未知对象仍能提高成功率。

    

    在人类环境中运作的机器人必须能够将物体重新排列成语义有意义的配置，即使这些物体以前没见过。本文关注如何在无需逐步指令的情况下构建物理有效的结构。我们提出了StructDiffusion，该方法结合了扩散模型和以物体为中心的转换器，根据局部视点云和高级语言目标（如“摆桌子”），构建结构。我们的方法可以使用一个模型执行多个具有挑战性的语言条件的多步骤3D规划任务。与训练在特定结构上的现有多模态转换器模型相比，StructDiffusion甚至提高了将未知对象组装成物理有效结构的成功率，平均可提高16％。我们展示了模拟和实际重新排列任务中使用保留对象的实验。重要的是，我们展示了如何将扩散模型和碰撞鉴别器模型结合起来，以实现对拟合性以及对语言指导物理有效结构构建任务的影响。

    Robots operating in human environments must be able to rearrange objects into semantically-meaningful configurations, even if these objects are previously unseen. In this work, we focus on the problem of building physically-valid structures without step-by-step instructions. We propose StructDiffusion, which combines a diffusion model and an object-centric transformer to construct structures given partial-view point clouds and high-level language goals, such as "set the table". Our method can perform multiple challenging language-conditioned multi-step 3D planning tasks using one model. StructDiffusion even improves the success rate of assembling physically-valid structures out of unseen objects by on average 16% over an existing multi-modal transformer model trained on specific structures. We show experiments on held-out objects in both simulation and on real-world rearrangement tasks. Importantly, we show how integrating both a diffusion model and a collision-discriminator model allo
    
[^137]: EEG-Fest:一种基于少样本的EEG信号驱动员警觉估计的注意力网络

    EEG-Fest: Few-shot based Attention Network for Driver's Vigilance Estimation with EEG Signals. (arXiv:2211.03878v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.03878](http://arxiv.org/abs/2211.03878)

    该论文提出了一种少样本模型EEG-Fest，它能够通过少量的样本对司机困意状态进行监测，识别异常信号并且实现主观独立分类，且在数据集上达到了最新最佳的结果。

    

    驾驶员警觉不足是大多数车祸的主要原因。脑电图作为无人车司机困意估计可靠性高效的工具。虽然之前的研究开发出了精确强韧的司机警觉状态检测算法，但在样本小、异常信号检测和主观独立分类等方面仍面临挑战。本文提出了一种广义的少样本模型——EEG-Fest，以改善上述缺点，能够：(a)用几个样本对查询样本的困意分类，(b)识别一个查询样本是异常信号还是正常信号，(c)实现主观独立分类。该算法在SEED-VIG和SADT数据集上取得了最新的最佳结果。

    A lack of driver's vigilance is the main cause of most vehicle crashes. Electroencephalography(EEG) has been reliable and efficient tool for drivers' drowsiness estimation. Even though previous studies have developed accurate and robust driver's vigilance detection algorithms, these methods are still facing challenges on following areas: (a) small sample size training, (b) anomaly signal detection, and (c) subject-independent classification. In this paper, we propose a generalized few-shot model, namely EEG-Fest, to improve aforementioned drawbacks. The EEG-Fest model can (a) classify the query sample's drowsiness with a few samples, (b) identify whether a query sample is anomaly signals or not, and (c) achieve subject independent classification. The proposed algorithm achieves state-of-the-art results on the SEED-VIG dataset and the SADT dataset. The accuracy of the drowsy class achieves 92% and 94% for 1-shot and 5-shot support samples in the SEED-VIG dataset, and 62% and 78% for 1-s
    
[^138]: 将欧几里得空间度量化以满足点云中所需的距离关系

    Metricizing the Euclidean Space towards Desired Distance Relations in Point Clouds. (arXiv:2211.03674v2 [cs.CG] UPDATED)

    [http://arxiv.org/abs/2211.03674](http://arxiv.org/abs/2211.03674)

    该论文提出了一种解决如何构造拓扑度量来计算给定距离值的问题的方法，涉及到欧几里得空间中多个点之间的最近邻查询问题，其应用于平衡kd树的查询时间复杂度为$O(\log^2n)$。

    

    给定欧几里得空间$\mathbb{R}^\ell$内的一组点，它们之间的距离由它们的空间位置和赋予$\mathbb{R}^\ell$的度量$d$决定。因此，两个点之间的距离$d(\mathbf x, \mathbf y)$由$\mathbf x$和$\mathbf y$的选择和度量$d$的选择固定。我们研究了一个相关问题，即固定值$\delta$和点$\mathbf x,\mathbf y$，并询问是否存在一个拓扑度量$d$来计算所需的距离$\delta$。我们通过构建度量解决了此问题，从而同时给出$\mathbb{R}^\ell$内多达$O(\sqrt\ell)$个点之间所需的一对一距离。然后，我们介绍了$\varepsilon$-半度量$\tilde{d}$的概念，以制定我们的主要结果：对于所有$\varepsilon>0$，对于所有$m\geq1$，对于任意选择的$m$个点$\mathbf y_1,\ldots,\mathbf y_m\in\mathbb{R}^\ell$和所有已选值集合$\{\delta_{ij}\geq 0: 1\leq i<j\leq m\}$，都存在一个$\varepsilon$-半度量$\tilde{d}$，使得$\delta_{ij}$近似等于$\tilde{d}(\mathbf y_i,\mathbf y_j)$，其中$1\leq i<j\leq m$。此外，我们将我们的构造应用于点云的最近邻搜索问题，并在任意维度$\ell$上证明了平衡kd树的最劣查询时间为$O(\log^2n)$，其中$n$是点集中的点数。

    Given a set of points in the Euclidean space $\mathbb{R}^\ell$ with $\ell>1$, the pairwise distances between the points are determined by their spatial location and the metric $d$ that we endow $\mathbb{R}^\ell$ with. Hence, the distance $d(\mathbf x,\mathbf y)=\delta$ between two points is fixed by the choice of $\mathbf x$ and $\mathbf y$ and $d$. We study the related problem of fixing the value $\delta$, and the points $\mathbf x,\mathbf y$, and ask if there is a topological metric $d$ that computes the desired distance $\delta$. We demonstrate this problem to be solvable by constructing a metric to simultaneously give desired pairwise distances between up to $O(\sqrt\ell)$ many points in $\mathbb{R}^\ell$. We then introduce the notion of an $\varepsilon$-semimetric $\tilde{d}$ to formulate our main result: for all $\varepsilon>0$, for all $m\geq 1$, for any choice of $m$ points $\mathbf y_1,\ldots,\mathbf y_m\in\mathbb{R}^\ell$, and all chosen sets of values $\{\delta_{ij}\geq 0: 1
    
[^139]: 面向视觉Transformer的数据级彩票假设

    Data Level Lottery Ticket Hypothesis for Vision Transformers. (arXiv:2211.01484v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2211.01484](http://arxiv.org/abs/2211.01484)

    本文将传统的彩票假说（LTH）扩展到由图像补丁组成的输入数据中，证明存在一个子集的输入图像补丁使得可以从头开始训练视觉Transformer（ViT），并且达到与使用所有图像补丁训练的ViTs相似的精度，这种方法在各种视觉任务中都是可行的和有效的。

    

    传统彩票假说（LTH）声称在密集神经网络中存在着一个稀疏的子网络和一个称为“获奖彩票”的适当随机初始化方法，以便可以从头开始训练它，使其几乎像密集网络一样好。与此同时，对于视觉Transformer（ViTs）中LTH的研究却很少被评估。本文首先表明了在现有方法下，在ViTs的权重级别上寻找传统的获奖彩票是困难的。然后，我们将ViTs的LTH推广到由图像补丁组成的输入数据中，受ViTs输入依赖启发。也就是说，存在一个输入图像补丁的子集，使得通过仅使用该子集，可以从头开始训练ViT，并达到与使用所有图像补丁训练的ViTs相似的精度。我们将这个输入补丁子集称为em获奖彩票，它代表了输入数据中的大量信息。我们使用票选择器生成带有em获奖彩票的权重子集，并使用EMD训练这些“获奖”子集，可以证明这种方法在各种视觉任务中都是可行的和有效的。

    The conventional lottery ticket hypothesis (LTH) claims that there exists a sparse subnetwork within a dense neural network and a proper random initialization method called the winning ticket, such that it can be trained from scratch to almost as good as the dense counterpart. Meanwhile, the research of LTH in vision transformers (ViTs) is scarcely evaluated. In this paper, we first show that the conventional winning ticket is hard to find at the weight level of ViTs by existing methods. Then, we generalize the LTH for ViTs to input data consisting of image patches inspired by the input dependence of ViTs. That is, there exists a subset of input image patches such that a ViT can be trained from scratch by using only this subset of patches and achieve similar accuracy to the ViTs trained by using all image patches. We call this subset of input patches the em winning tickets, which represent a significant amount of information in the input data. We use a ticket selector to generate the w
    
[^140]: 掩码自编码器是口腔学习的利器。

    Masked Autoencoders Are Articulatory Learners. (arXiv:2210.15195v2 [eess.AS] UPDATED)

    [http://arxiv.org/abs/2210.15195](http://arxiv.org/abs/2210.15195)

    本文提出了基于深度学习的Masked Autoencoders方法，可以精确重构被追踪错误的口腔学记录，有效地应用于XRMB数据集研究中。

    

    口腔学记录下不同口腔部位的位置和运动，被广泛用于研究语音产生以及开发基于口腔学的语音合成器和语音反演系统。威斯康星大学X射线微束（XRMB）数据集是提供与音频记录同步的各种数据集之一。 XRMB口腔学记录使用放置在多个口腔部位的颗粒，可以由微束跟踪。然而，很大一部分口腔学记录被跟踪错误，一直无法使用。在这项工作中，我们提出了一种基于 Masked Autoencoders 的深度学习方法，可以准确重构 XRMB数据集的47个演讲者中41个的被追踪错误的口腔学记录。当八个口腔部位中的三个被误追踪时，我们的模型能够重构口腔学轨迹，与真实情况非常接近。

    Articulatory recordings track the positions and motion of different articulators along the vocal tract and are widely used to study speech production and to develop speech technologies such as articulatory based speech synthesizers and speech inversion systems. The University of Wisconsin X-Ray microbeam (XRMB) dataset is one of various datasets that provide articulatory recordings synced with audio recordings. The XRMB articulatory recordings employ pellets placed on a number of articulators which can be tracked by the microbeam. However, a significant portion of the articulatory recordings are mistracked, and have been so far unsuable. In this work, we present a deep learning based approach using Masked Autoencoders to accurately reconstruct the mistracked articulatory recordings for 41 out of 47 speakers of the XRMB dataset. Our model is able to reconstruct articulatory trajectories that closely match ground truth, even when three out of eight articulators are mistracked, and retrie
    
[^141]: 多视角数据中缺失值的插补问题解决方法

    Imputation of missing values in multi-view data. (arXiv:2210.14484v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.14484](http://arxiv.org/abs/2210.14484)

    本文提出了一种基于StaPLR算法的新的多视角数据插补算法，通过在降维空间中执行插补以解决计算挑战，并在模拟数据集中得到了竞争性结果。

    

    多视角数据是指由多个不同特征集描述的数据。在处理多视角数据时，若出现缺失值，则一个视角中的所有特征极有可能同时缺失，因而导致非常大量的缺失数据问题。本文提出了一种新的多视角学习算法中的插补方法，它基于堆叠惩罚逻辑回归(StaPLR)算法，在降维空间中执行插补，以解决固有的多视角计算挑战。实验结果表明，该方法在模拟数据集上具有竞争性结果，而且具有更低的计算成本，从而可以使用先进的插补算法，例如missForest。

    Data for which a set of objects is described by multiple distinct feature sets (called views) is known as multi-view data. When missing values occur in multi-view data, all features in a view are likely to be missing simultaneously. This leads to very large quantities of missing data which, especially when combined with high-dimensionality, makes the application of conditional imputation methods computationally infeasible. We introduce a new imputation method based on the existing stacked penalized logistic regression (StaPLR) algorithm for multi-view learning. It performs imputation in a dimension-reduced space to address computational challenges inherent to the multi-view context. We compare the performance of the new imputation method with several existing imputation algorithms in simulated data sets. The results show that the new imputation method leads to competitive results at a much lower computational cost, and makes the use of advanced imputation algorithms such as missForest 
    
[^142]: 知识增强关系抽取数据集

    Knowledge-Enhanced Relation Extraction Dataset. (arXiv:2210.11231v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.11231](http://arxiv.org/abs/2210.11231)

    该论文介绍了一个新的知识增强关系抽取数据集（KERED），它为每个句子注释一个关系事实，并通过实体链接提供实体的知识背景。利用这个数据集，作者比较了当代关系抽取方法在句子级和包级两种任务设置下的表现。实验结果表明，KERED提供的知识图谱可以支持知识增强的关系抽取方法。

    

    最近，利用辅助知识图谱的知识增强方法在关系抽取方面已经超越了传统的基于文本的方法。但据我们所知，目前还没有包含证据句子和知识图谱的公共数据集可用于知识增强关系抽取。为了弥补这一空白，我们介绍了知识增强关系抽取数据集（KERED）。KERED为每个句子注释一个关系事实，并通过实体链接提供实体的知识背景。利用我们策划的数据集，我们在两种普遍的任务设置下比较了当代关系抽取方法：句子级和包级。实验结果表明，KERED提供的知识图谱可以支持知识增强的关系抽取方法。我们相信，KERED为评估知识增强关系抽取性能提供了高质量的关系抽取数据集和相应的知识图谱。

    Recently, knowledge-enhanced methods leveraging auxiliary knowledge graphs have emerged in relation extraction, surpassing traditional text-based approaches. However, to our best knowledge, there is currently no public dataset available that encompasses both evidence sentences and knowledge graphs for knowledge-enhanced relation extraction. To address this gap, we introduce the Knowledge-Enhanced Relation Extraction Dataset (KERED). KERED annotates each sentence with a relational fact, and it provides knowledge context for entities through entity linking. Using our curated dataset, We compared contemporary relation extraction methods under two prevalent task settings: sentence-level and bag-level. The experimental result shows the knowledge graphs provided by KERED can support knowledge-enhanced relation extraction methods. We believe that KERED offers high-quality relation extraction datasets with corresponding knowledge graphs for evaluating the performance of knowledge-enhanced rela
    
[^143]: 基于图高斯过程的流形优化

    Optimization on Manifolds via Graph Gaussian Processes. (arXiv:2210.10962v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.10962](http://arxiv.org/abs/2210.10962)

    本论文结合流形学习和高斯过程，利用流形样本点云定义图高斯过程代理模型，通过选择查询点在流形上优化目标函数，具有良好性能。

    

    本文将流形学习技术与高斯过程上限置信度算法相结合，以优化流形上的目标函数。我们的方法是针对在无法获得完整流形表示且查询目标昂贵的应用场景而设计的。我们依靠流形样本点云来定义用于目标函数的图高斯过程代理模型。使用先前所有查询的后验分布逐步选择查询点。我们在查询次数和点云大小方面建立了遗憾上限。数值实验补充了理论，并说明了我们的方法的性能。

    This paper integrates manifold learning techniques within a \emph{Gaussian process upper confidence bound} algorithm to optimize an objective function on a manifold. Our approach is motivated by applications where a full representation of the manifold is not available and querying the objective is expensive. We rely on a point cloud of manifold samples to define a graph Gaussian process surrogate model for the objective. Query points are sequentially chosen using the posterior distribution of the surrogate model given all previous queries. We establish regret bounds in terms of the number of queries and the size of the point cloud. Several numerical examples complement the theory and illustrate the performance of our method.
    
[^144]: 开源软件开发者的代码推荐

    Code Recommendation for Open Source Software Developers. (arXiv:2210.08332v3 [cs.SE] UPDATED)

    [http://arxiv.org/abs/2210.08332](http://arxiv.org/abs/2210.08332)

    CODER 是一个基于图的代码推荐框架，通过建模微观用户-代码交互和宏观用户-项目交互，预测开发者未来的贡献行为，以缩短开发时间并提高开发效率。

    

    开源软件是技术基础设施的支柱，吸引数百万人才做出贡献。考虑到开发人员的兴趣和项目代码的语义特征，向开源软件开发者推荐适当的开发任务具有挑战性和重要性。在本文中，我们提出了一个新颖的代码推荐问题，其目的是根据开发者的交互历史、源代码的语义特征和项目的分层文件结构来预测开发者未来的贡献行为。考虑到系统中多方之间的复杂交互，我们提出了CODER，这是一个新颖的基于图的开源软件开发者代码推荐框架。CODER通过异构图共同建模微观用户-代码交互和宏观用户-项目交互，并通过在反映文件结构的文件结构图上的聚合进一步连接两个级别的信息。

    Open Source Software (OSS) is forming the spines of technology infrastructures, attracting millions of talents to contribute. Notably, it is challenging and critical to consider both the developers' interests and the semantic features of the project code to recommend appropriate development tasks to OSS developers. In this paper, we formulate the novel problem of code recommendation, whose purpose is to predict the future contribution behaviors of developers given their interaction history, the semantic features of source code, and the hierarchical file structures of projects. Considering the complex interactions among multiple parties within the system, we propose CODER, a novel graph-based code recommendation framework for open source software developers. CODER jointly models microscopic user-code interactions and macroscopic user-project interactions via a heterogeneous graph and further bridges the two levels of information through aggregation on file-structure graphs that reflect 
    
[^145]: 特征选择的序列关注

    Sequential Attention for Feature Selection. (arXiv:2209.14881v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.14881](http://arxiv.org/abs/2209.14881)

    在神经网络中，我们提出一种名为序列关注的特征选择算法，它在每个步骤使用注意力权重作为特征重要性的代理，实现了最新的实证结果。

    

    特征选择是为了选择一个子集用于机器学习模型，而这个子集能最大化模型质量，并且要求在预算范围内。对于神经网络，采用的传统方法包括基于$\ell_1$正则化、注意力和其他技术的方法，通常在一次评估中选择整个特征子集，忽略了在选择期间特征的残留价值，即在选择其他特征后给与特征的边际贡献。我们提出了一种名为序列关注的特征选择算法，它在神经网络中实现了最新的实证结果。该算法基于一遍高效的贪心前向选择实现，并在每个步骤使用注意力权重作为特征重要性的代理。我们通过展示适用于线性回归的理论意义，说明了我们的算法相当于经典的正交匹配追踪（OMP）算法。

    Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, a
    
[^146]: 使用LLM来增强可解释模型的训练

    Augmenting Interpretable Models with LLMs during Training. (arXiv:2209.11799v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2209.11799](http://arxiv.org/abs/2209.11799)

    本文提出了 Aug-imodels 框架，利用 LLMs 的知识在拟合过程中构建高效且可解释的模型，在推理过程中不使用 LLMs，具备完全的透明性。研究探讨了两种不同方式的实现，并在多种文本分类数据集中表现出优异的效果。

    

    近期，大型语言模型（LLMs）在越来越多的任务中表现出了出色的表现。然而，它们进入高风险领域（例如医学）和计算资源有限的环境中，对解释性和效率的需求日益增加。我们提出了增强可解释模型（Aug-imodels）框架，利用LLMs所学习的知识建立极其高效且可解释的模型。Aug-imodels在拟合过程中使用LLMs，但在推理过程中不使用，从而实现了完全的透明性，并且与LLMs相比，推理速度和内存性能有了大于1000倍的提高。我们探讨了两种Aug-imodels在自然语言处理中的具体实例：（i）Aug-GAM，它使用来自LLM的解耦嵌入来增强广义加性模型；（ii）Aug-Tree，它通过LLM特征扩展来增强决策树。在各种文本分类数据集中，这两种方法都优于其未增强的对照模型。

    Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented co
    
[^147]: 材料工程中的人工智能：人工智能在材料工程中的应用综述

    Artificial Intelligence in Material Engineering: A review on applications of AI in Material Engineering. (arXiv:2209.11234v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2209.11234](http://arxiv.org/abs/2209.11234)

    本文综述了人工智能在材料工程中的最新进展，涉及材料加工、结构和材料性能研究、测量材料性能、新材料的创建和设计以及未来机遇等方面，基于机器学习的方法比传统方法更快、更准确，生成对抗网络有助于无机材料的化学成分预测和优化。

    

    随着人工智能技术的不断发展，人工智能在材料科学和工程中的作用越来越重要。高性能计算机的发展使得测试深度学习模型成为可能，这提供了一个机会来克服传统计算方法（如密度泛函理论）在性能预测上的局限性。基于机器学习的方法比基于密度泛函理论的方法更快，更准确。此外，生成对抗网络（GAN）有助于在不使用晶体结构信息的情况下生成无机材料的化学成分。这些发展对材料工程和研究产生了重大影响。本文综述了人工智能在材料工程中的最新进展。概括了人工智能在材料加工、结构和材料性能研究以及测量材料性能等关键领域中的应用。描述了人工智能在新材料的创建和设计中的应用，其中人工智能被用来优化和预测材料性质。最后，本文讨论了人工智能在材料工程中面临的挑战和未来机遇。

    The role of artificial intelligence (AI) in material science and engineering (MSE) is becoming increasingly important as AI technology advances. The development of high-performance computing has made it possible to test deep learning (DL) models with significant parameters, providing an opportunity to overcome the limitation of traditional computational methods, such as density functional theory (DFT), in property prediction. Machine learning (ML)-based methods are faster and more accurate than DFT-based methods. Furthermore, the generative adversarial networks (GANs) have facilitated the generation of chemical compositions of inorganic materials without using crystal structure information. These developments have significantly impacted material engineering (ME) and research. Some of the latest developments in AI in ME herein are reviewed. First, the development of AI in the critical areas of ME, such as in material processing, the study of structure and material property, and measurin
    
[^148]: 基于深度强化学习的小型空中机器人倒立降落触发和旋转机动控制

    Inverted Landing in a Small Aerial Robot via Deep Reinforcement Learning for Triggering and Control of Rotational Maneuvers. (arXiv:2209.11043v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.11043](http://arxiv.org/abs/2209.11043)

    本研究利用深度强化学习和基于物理学的仿真技术，获得一种通用的最优控制策略，可以使小型空中机器人从任意状态开始，实现倒立降落触发和旋转机动控制。通过将所学习的策略转移至实体机器人，成功实现高可靠性的倒立降落，即使在受到风扰动的情况下。

    

    小型空中机器人通过仅依赖机载感应和计算，在短时间内实现倒立降落是一项富有挑战性的任务。然而，蝙蝠、苍蝇和蜜蜂等生物飞行者经常执行这项飞行技巧。本研究利用深度强化学习和基于物理学的仿真，获得了一种通用的最优控制策略，可以从任意的起始状态开始，实现小型空中机器人的倒立降落触发和旋转机动控制。通过将所学习的策略转移至实体机器人，我们证明了这种方法可以使机器人具有高可靠性地完成任意初态的倒立降落，即使在明显的风扰动下。

    Inverted landing in a rapid and robust manner is a challenging feat for aerial robots, especially while depending entirely on onboard sensing and computation. In spite of this, this feat is routinely performed by biological fliers such as bats, flies, and bees. Our previous work has identified a direct causal connection between a series of onboard visual cues and kinematic actions that allow for reliable execution of this challenging aerobatic maneuver in small aerial robots. In this work, we first utilized Deep Reinforcement Learning and a physics-based simulation to obtain a general, optimal control policy for robust inverted landing starting from any arbitrary approach condition. This optimized control policy provides a computationally-efficient mapping from the system's observational space to its motor command action space, including both triggering and control of rotational maneuvers. This was done by training the system over a large range of approach flight velocities that varied
    
[^149]: 深度神经网络去噪提取衰减信号的应用研究——以X射线衍射数据为例

    Weak-signal extraction enabled by deep-neural-network denoising of diffraction data. (arXiv:2209.09247v2 [eess.IV] UPDATED)

    [http://arxiv.org/abs/2209.09247](http://arxiv.org/abs/2209.09247)

    本研究展示了如何通过深度卷积神经网络对数据进行去噪，使衰弱的信号出现具有量化准确性的情况，并成功应用于晶体材料的X射线衍射数据中。

    

    去除噪音在成像和声学方面有广泛的应用。在日常应用中，去噪可能甚至包含与真实情况不符的生成方面。但是，在科学应用中，去噪必须准确地再现真实情况。本文展示了如何通过深度卷积神经网络对数据进行去噪，以使衰弱的信号出现具有量化准确性的情况。特别是，我们研究了晶体材料的X射线衍射。我们证明了在去噪数据中，源自电荷排序的微弱信号，在噪音数据中不显著，但在去噪后变得清晰而准确可见。这种成功得益于使用所测量的低噪声数据和高噪声数据的成对进行深度神经网络的监督训练。这样，神经网络就可以学习噪声的统计特性。我们证明，使用人工噪声无法得到如此量化准确的结果。因此，我们的方法阐明了通过深度神经网络去噪提取噪音数据中的衰减信号的实用策略。

    Removal or cancellation of noise has wide-spread applications for imaging and acoustics. In every-day-life applications, denoising may even include generative aspects which are unfaithful to the ground truth. For scientific applications, however, denoising must reproduce the ground truth accurately. Here, we show how data can be denoised via a deep convolutional neural network such that weak signals appear with quantitative accuracy. In particular, we study X-ray diffraction on crystalline materials. We demonstrate that weak signals stemming from charge ordering, insignificant in the noisy data, become visible and accurate in the denoised data. This success is enabled by supervised training of a deep neural network with pairs of measured low- and high-noise data. This way, the neural network learns about the statistical properties of the noise. We demonstrate that using artificial noise does not yield such quantitatively accurate results. Our approach thus illustrates a practical strat
    
[^150]: 信号时间逻辑谓词的模型预测鲁棒性

    Model Predictive Robustness of Signal Temporal Logic Predicates. (arXiv:2209.07881v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2209.07881](http://arxiv.org/abs/2209.07881)

    本论文提出了一种新的模型预测鲁棒性的概念来更系统地评估信号时间逻辑中底层谓词的鲁棒性，并成功验证了该方法在自动驾驶中的应用。

    

    信号时间逻辑的鲁棒性不仅评估了一个信号是否符合规范，而且还提供了一个衡量公式被满足或违反的程度的指标。鲁棒性的计算基于对底层谓词的鲁棒性进行评估。然而，谓词的鲁棒性通常以一种无模型的方式定义，即不包括系统动态。而且，精确定义复杂谓词的鲁棒性通常是非平凡的。为了解决这些问题，我们提出了一种模型预测鲁棒性的概念，通过考虑基于模型的预测，提供了比之前方法更系统的评估鲁棒性的方法。特别地，我们使用高斯过程回归来学习基于预先计算的预测的鲁棒性，以便可以在线高效地计算鲁棒性值。我们评估了我们的方法，并使用在记录的数据上使用在形式化交通规则中使用的谓词的自动驾驶用例验证了我们的方法。

    The robustness of signal temporal logic not only assesses whether a signal adheres to a specification but also provides a measure of how much a formula is fulfilled or violated. The calculation of robustness is based on evaluating the robustness of underlying predicates. However, the robustness of predicates is usually defined in a model-free way, i.e., without including the system dynamics. Moreover, it is often nontrivial to define the robustness of complicated predicates precisely. To address these issues, we propose a notion of model predictive robustness, which provides a more systematic way of evaluating robustness compared to previous approaches by considering model-based predictions. In particular, we use Gaussian process regression to learn the robustness based on precomputed predictions so that robustness values can be efficiently computed online. We evaluate our approach for the use case of autonomous driving with predicates used in formalized traffic rules on a recorded dat
    
[^151]: Fed-FSNet: 通过模糊合成网络缓解非独立同分布联邦学习问题

    Fed-FSNet: Mitigating Non-I.I.D. Federated Learning via Fuzzy Synthesizing Network. (arXiv:2208.12044v2 [cs.CR] UPDATED)

    [http://arxiv.org/abs/2208.12044](http://arxiv.org/abs/2208.12044)

    本文提出一种名为Fed-FSNet的新型联邦学习训练框架，通过使用模糊合成网络（FSNet）在源头处缓解非独立同分布问题，从而提高全局模型的性能表现。

    

    联邦学习是一种最近出现的有前途的隐私保护分布式机器学习框架，它旨在通过在边缘设备上进行分布式训练，将本地模型聚合成全局模型，而无需在云服务器上共享中心化原始数据。但是，由于边缘设备之间存在着大量本地数据异质性（非独立同分布数据），因此联邦学习很容易获得一个能够在本地数据集上产生更多偏移梯度的全局模型，从而降低模型性能甚至在训练过程中无法收敛。在本文中，我们提出了一种新的FL训练框架，称为Fed-FSNet，它使用一个适当设计的模糊合成网络（FSNet）来缓解非独立同分布FL问题。

    Federated learning (FL) has emerged as a promising privacy-preserving distributed machine learning framework recently. It aims at collaboratively learning a shared global model by performing distributed training locally on edge devices and aggregating local models into a global one without centralized raw data sharing in the cloud server. However, due to the large local data heterogeneities (Non-I.I.D. data) across edge devices, the FL may easily obtain a global model that can produce more shifted gradients on local datasets, thereby degrading the model performance or even suffering from the non-convergence during training. In this paper, we propose a novel FL training framework, dubbed Fed-FSNet, using a properly designed Fuzzy Synthesizing Network (FSNet) to mitigate the Non-I.I.D. FL at-the-source. Concretely, we maintain an edge-agnostic hidden model in the cloud server to estimate a less-accurate while direction-aware inversion of the global model. The hidden model can then fuzzil
    
[^152]: 多目标参数优化中的有效效用函数学习与先验知识

    Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. (arXiv:2208.10300v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2208.10300](http://arxiv.org/abs/2208.10300)

    该论文提出了一种利用偏好学习离线学习效用函数的方法，以应对真实世界问题中用专家知识定义效用函数困难且与专家反复互动昂贵的问题。使用效用函数空间的粗略信息，能够在使用很少结果时提高效用函数估计，并通过整个优化链中传递效用函数学习任务中出现的不确定性。

    

    目前的多目标优化技术通常假定已有效用函数、通过互动学习效用函数或尝试确定完整的Pareto前沿来进行。然而，在真实世界的问题中，结果往往基于隐含和显性的专家知识，难以定义一个效用函数，而互动学习或后续启发式需要反复并且昂贵地专家参与。为了缓解这种情况，我们使用偏好学习离线学习效用函数，利用专家知识。与其他工作不同的是，我们不仅使用（成对的）结果偏好，而且使用效用函数空间的粗略信息。这使我们能够提高效用函数估计，特别是在使用很少的结果时。此外，我们对效用函数学习任务中出现的不确定性进行建模，并将其传递到整个优化链中。

    The current state-of-the-art in multi-objective optimization assumes either a given utility function, learns a utility function interactively or tries to determine the complete Pareto front, requiring a post elicitation of the preferred result. However, result elicitation in real world problems is often based on implicit and explicit expert knowledge, making it difficult to define a utility function, whereas interactive learning or post elicitation requires repeated and expensive expert involvement. To mitigate this, we learn a utility function offline, using expert knowledge by means of preference learning. In contrast to other works, we do not only use (pairwise) result preferences, but also coarse information about the utility function space. This enables us to improve the utility function estimate, especially when using very few results. Additionally, we model the occurring uncertainties in the utility function learning task and propagate them through the whole optimization chain. 
    
[^153]: 使用深度神经网络预测同一基因位点上的不同mRNA的功能

    Isoform Function Prediction Using a Deep Neural Network. (arXiv:2208.03325v3 [q-bio.GN] UPDATED)

    [http://arxiv.org/abs/2208.03325](http://arxiv.org/abs/2208.03325)

    本文介绍了一种使用深度神经网络预测同一基因位点上不同mRNA功能的方法，解决了缺乏标记训练数据的问题。

    

    异剪切是从同一基因位点产生多个mRNA的现象。研究表明，超过95%的人类多外显子基因经历了异剪切。虽然mRNA序列的变化很小，但它们可能对细胞功能和调节产生系统性影响。报道称，同一基因的不同剪接形式具有不同甚至对立的功能。虽然基因的功能研究范围很广，但对于同一基因位点上不同mRNA的功能还知之甚少。最近，一些基于多实例学习的计算方法已被提出，用于预测功能，并结合了基因功能和基因表达谱。然而，由于缺乏标记的训练数据，它们的表现并不理想。此外，概率模型，如条件随机场（CRF），也被用于建模异构体之间的关系。

    Isoforms are mRNAs produced from the same gene site in the phenomenon called Alternative Splicing. Studies have shown that more than 95% of human multi-exon genes have undergone alternative splicing. Although there are few changes in mRNA sequence, They may have a systematic effect on cell function and regulation. It is widely reported that isoforms of a gene have distinct or even contrasting functions. Most studies have shown that alternative splicing plays a significant role in human health and disease. Despite the wide range of gene function studies, there is little information about isoforms' functionalities. Recently, some computational methods based on Multiple Instance Learning have been proposed to predict isoform function using gene function and gene expression profile. However, their performance is not desirable due to the lack of labeled training data. In addition, probabilistic models such as Conditional Random Field (CRF) have been used to model the relation between isofor
    
[^154]: 奇异Woodbury矩阵和伪行列式矩阵恒等式及其在高斯过程回归中的应用研究

    A Singular Woodbury and Pseudo-Determinant Matrix Identities and Application to Gaussian Process Regression. (arXiv:2207.08038v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2207.08038](http://arxiv.org/abs/2207.08038)

    研究了一种源自于奇异形式的Woodbury矩阵，提出了逆矩阵和伪行列式的广义恒等式，并将精密矩阵的定义扩展到协方差矩阵的Bott-Duffin逆，同时提供了有效的算法并演示出其在计算高斯过程回归的似然函数中的优势。

    

    我们研究了一种源自于奇异形式的Woodbury矩阵恒等式的矩阵。我们提出了逆矩阵和伪行列式的广义恒等式，这对于高斯过程回归有直接的应用，特别是其类似性表示和精密矩阵。我们将精密矩阵的定义扩展到协方差矩阵的Bott-Duffin逆，保留了与条件独立性、条件精度和边际精度有关的属性。我们还提供了一种有效的算法和所提出的行列式恒等式的数值分析，并在特定条件下演示了它们在计算高斯过程回归的似然函数中的对数行列式项方面的优势。

    We study a matrix that arises from a singular form of the Woodbury matrix identity. We present generalized inverse and pseudo-determinant identities for this matrix, which have direct applications for Gaussian process regression, specifically its likelihood representation and precision matrix. We extend the definition of the precision matrix to the Bott-Duffin inverse of the covariance matrix, preserving properties related to conditional independence, conditional precision, and marginal precision. We also provide an efficient algorithm and numerical analysis for the presented determinant identities and demonstrate their advantages under specific conditions relevant to computing log-determinant terms in likelihood functions of Gaussian process regression.
    
[^155]: 浮点算术下的声音随机平滑

    Sound Randomized Smoothing in Floating-Point Arithmetics. (arXiv:2207.07209v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.07209](http://arxiv.org/abs/2207.07209)

    该论文提出了浮点算术下的声音随机平滑，证明了无限精度下的随机平滑不再可靠，只需使用一个公平的硬币即可，这项技术可以与标准分类器的证书匹配。

    

    当使用无限精度时，随机平滑是可靠的。然而，我们表明，当使用有限的浮点精度时，随机平滑不再可靠。我们提出了一个简单的例子，其中随机平滑证明了一个半径为1.26的点，即使有一个距离为0.8的对抗性例子，并进一步扩展了该例子，为CIFAR10提供虚假证书。我们讨论了随机平滑的隐含假设，并表明它们不适用于通常被认证的平滑版本是泛化图像分类模型。为了解决这个问题，我们提出了一种声音方法，使用浮点精度进行随机平滑，同时速度几乎相同并且与已经测试的标准分类器的证书匹配。我们唯一的假设是我们可以使用一个公平的硬币。

    Randomized smoothing is sound when using infinite precision. However, we show that randomized smoothing is no longer sound for limited floating-point precision. We present a simple example where randomized smoothing certifies a radius of $1.26$ around a point, even though there is an adversarial example in the distance $0.8$ and extend this example further to provide false certificates for CIFAR10. We discuss the implicit assumptions of randomized smoothing and show that they do not apply to generic image classification models whose smoothed versions are commonly certified. In order to overcome this problem, we propose a sound approach to randomized smoothing when using floating-point precision with essentially equal speed and matching the certificates of the standard, unsound practice for standard classifiers tested so far. Our only assumption is that we have access to a fair coin.
    
[^156]: 用卷积生成对抗网络的数据驱动噪声时间序列建模

    Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks. (arXiv:2207.01110v2 [eess.SP] UPDATED)

    [http://arxiv.org/abs/2207.01110](http://arxiv.org/abs/2207.01110)

    本文通过对噪声时间序列的建模，考察了两种GAN模型用于数据驱动建模的可行性，实验结果表明它们在各类噪声的复制方面具有一定的效果。

    

    物理过程中产生的随机噪声是测量的固有特性，并且是大多数信号处理和数据分析任务的限制因素。鉴于近年来对生成对抗网络(GANs)用于数据驱动建模的兴趣，确定GANs在多大程度上能够忠实地复制目标数据集中的噪声非常重要。本文提出了一项实证研究，旨在为时间序列中的这个问题提供启示。具体而言，我们评估了两种基于流行的深度卷积GAN（DCGAN）结构的通用时间序列GAN，一种是直接的时间序列模型，另一种是基于短时傅里叶变换（STFT）数据表示的图像模型。GAN模型使用已知基础真实参数的模拟噪声时间序列分布进行训练和定量评估。目标时间序列分布涵盖了物理测量、电子学和...

    Random noise arising from physical processes is an inherent characteristic of measurements and a limiting factor for most signal processing and data analysis tasks. Given the recent interest in generative adversarial networks (GANs) for data-driven modeling, it is important to determine to what extent GANs can faithfully reproduce noise in target data sets. In this paper, we present an empirical investigation that aims to shed light on this issue for time series. Namely, we assess two general-purpose GANs for time series that are based on the popular deep convolutional GAN (DCGAN) architecture, a direct time-series model and an image-based model that uses a short-time Fourier transform (STFT) data representation. The GAN models are trained and quantitatively evaluated using distributions of simulated noise time series with known ground-truth parameters. Target time series distributions include a broad range of noise types commonly encountered in physical measurements, electronics, and 
    
[^157]: BiometricBlender：高维度多类合成数据生成工具

    BiometricBlender: Ultra-high dimensional, multi-class synthetic data generator to imitate biometric feature space. (arXiv:2206.10747v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10747](http://arxiv.org/abs/2206.10747)

    BiometricBlender是一个高维度多类合成数据生成工具，可以模拟真实生物特征数据集的关键属性，有助于在特征筛选领域进行快速发展。

    

    缺乏自由可得的高维度多类真实或合成数据集常常限制了特征筛选领域的快速发展，尤其是在生物特征识别领域。本文介绍了一个名为BiometricBlender的Python包，它是一个超高维度多类合成数据生成工具，用于测试各种特征筛选方法。在数据生成过程中，用户可以控制特征混合的总体有用性和互相关系数，因此合成特征空间能够模拟真实生物特征数据集的关键属性。

    The lack of freely available (real-life or synthetic) high or ultra-high dimensional, multi-class datasets may hamper the rapidly growing research on feature screening, especially in the field of biometrics, where the usage of such datasets is common. This paper reports a Python package called BiometricBlender, which is an ultra-high dimensional, multi-class synthetic data generator to benchmark a wide range of feature screening methods. During the data generation process, the overall usefulness and the intercorrelations of blended features can be controlled by the user, thus the synthetic feature space is able to imitate the key properties of a real biometric dataset.
    
[^158]: 自监督视觉预训练中的掩膜频率建模

    Masked Frequency Modeling for Self-Supervised Visual Pre-Training. (arXiv:2206.07706v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2206.07706](http://arxiv.org/abs/2206.07706)

    本文提出了掩膜频率建模（MFM）方法，用于自监督视觉预训练，通过对高低频成分进行掩膜预测，可以更好地揭示底层图像模式，并在ViT和CNN模型中实现很好的表现。

    

    本文提出了掩膜频率建模（MFM）, 一种用于自监督视觉模型预训练的基于频率域的统一方法。相比于在空间域中随机插入掩膜标记， MFM将视角转向频率域。具体来说，MFM首先屏蔽输入图像频率成分的一部分, 然后预测频谱上缺失的频率。我们的关键见解是，在频率域中预测掩膜组件比预测空间域中的掩膜补丁更能够揭示底层图像模式，因为后者具有较大的空间冗余性。我们的研究结果表明，通过正确的掩膜和预测策略, 高频成分中的结构信息以及低频成分中的低阶统计信息都有助于学习良好的表示方法。MFM首次证明, 对于ViT和CNN模型，简单的在线掩膜预测机制可以在自监督预训练中实现很好的表现。

    We present Masked Frequency Modeling (MFM), a unified frequency-domain-based approach for self-supervised pre-training of visual models. Instead of randomly inserting mask tokens to the input embeddings in the spatial domain, in this paper, we shift the perspective to the frequency domain. Specifically, MFM first masks out a portion of frequency components of the input image and then predicts the missing frequencies on the frequency spectrum. Our key insight is that predicting masked components in the frequency domain is more ideal to reveal underlying image patterns rather than predicting masked patches in the spatial domain, due to the heavy spatial redundancy. Our findings suggest that with the right configuration of mask-and-predict strategy, both the structural information within high-frequency components and the low-level statistics among low-frequency counterparts are useful in learning good representations. For the first time, MFM demonstrates that, for both ViT and CNN, a simp
    
[^159]: 多样多层网络模型中的稀疏子空间聚类

    Sparse Subspace Clustering in Diverse Multiplex Network Model. (arXiv:2206.07602v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.07602](http://arxiv.org/abs/2206.07602)

    本文研究了DIMPLE网络模型中的稀疏子空间聚类，通过识别具有相同社区结构的层组，找到了一种强一致性的结果。

    

    本文考虑了Pensky和Wang（2021）引入的DIverse MultiPLEx（DIMPLE）网络模型，其中网络的所有层都具有相同的节点集合，并配备有随机块模型。此外，所有层都可以分为具有相同社区结构的组，尽管在同一组中的层可能具有不同的块连接概率矩阵。DIMPLE模型概括了许多研究所有层具有相同社区结构的多层网络的论文，以及混合多层随机块模型（MMLSBM），在其中同一组中的层具有相同的块连接概率矩阵。本文使用稀疏子空间聚类（SSC）来识别具有相同社区结构的层组。在温和的条件下，后者导致了强一致性的结果。

    The paper considers the DIverse MultiPLEx (DIMPLE) network model, introduced in Pensky and Wang (2021), where all layers of the network have the same collection of nodes and are equipped with the Stochastic Block Models. In addition, all layers can be partitioned into groups with the same community structures, although the layers in the same group may have different matrices of block connection probabilities. The DIMPLE model generalizes a multitude of papers that study multilayer networks with the same community structures in all layers, as well as the Mixture Multilayer Stochastic Block Model (MMLSBM), where the layers in the same group have identical matrices of block connection probabilities. While Pensky and Wang (2021) applied spectral clustering to the proxy of the adjacency tensor, the present paper uses Sparse Subspace Clustering (SSC) for identifying groups of layers with identical community structures. Under mild conditions, the latter leads to the strongly consistent betwee
    
[^160]: 目标自适应设计

    Targeted Adaptive Design. (arXiv:2205.14208v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2205.14208](http://arxiv.org/abs/2205.14208)

    TAD是一种新的目标自适应设计算法，可以通过高斯过程代理模型在指定公差中确定产生期望设计特征的最佳控制设置，相比其他自适应设计算法，具有更高的精度和效率。

    

    现代先进制造和高级材料设计往往需要在较高维度的过程控制参数空间中搜索最佳结构、性能和性能参数的设置。从前者到后者的映射必须通过嘈杂的实验或昂贵的模拟来确定。我们把这个问题抽象成一个数学框架，其中必须通过昂贵的嘈杂测量来确定从控制空间到设计空间的未知函数，该函数在指定的公差范围内定位产生期望设计特征的最佳控制设置，并量化不确定性。我们描述了目标自适应设计 (TAD)，这是一种有效执行这个采样任务的新算法。TAD 在每个迭代阶段创建一个未知映射的高斯过程代理模型，建议一批新的控制设置进行实验采样，并优化更新的目标设计特征的对数预测似然。

    Modern advanced manufacturing and advanced materials design often require searches of relatively high-dimensional process control parameter spaces for settings that result in optimal structure, property, and performance parameters. The mapping from the former to the latter must be determined from noisy experiments or from expensive simulations. We abstract this problem to a mathematical framework in which an unknown function from a control space to a design space must be ascertained by means of expensive noisy measurements, which locate optimal control settings generating desired design features within specified tolerances, with quantified uncertainty. We describe targeted adaptive design (TAD), a new algorithm that performs this sampling task efficiently. TAD creates a Gaussian process surrogate model of the unknown mapping at each iterative stage, proposing a new batch of control settings to sample experimentally and optimizing the updated log-predictive likelihood of the target desi
    
[^161]: 进化帕累托最优的演员-评论家算法以实现泛化和稳定性

    Evolving Pareto-Optimal Actor-Critic Algorithms for Generalizability and Stability. (arXiv:2204.04292v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2204.04292](http://arxiv.org/abs/2204.04292)

    本文提出了MetaPG，一种自动设计演员-评论家损失函数的进化方法，该方法明确地优化泛化性和性能，并隐式地优化这两个指标的稳定性。

    

    泛化性和稳定性是在现实世界中操作强化学习代理的两个关键目标。设计优化这些目标的强化学习算法可能是一个昂贵而费时的过程。本文提出了MetaPG，一种用于自动设计演员-评论家损失函数的进化方法。MetaPG明确地优化泛化性和性能，并隐式地优化两个指标的稳定性。我们使用软演员评论家（SAC）初始化我们的损失函数种群，并使用编码单任务性能、对未见过的环境配置的零样本泛化性和在不同随机种子下独立运行时的稳定性的适应性度量进行多目标优化。在来自真实世界强化学习基准套件的一组连续控制任务上，我们发现使用单个环境进行进化的方法可以使演变出来的算法在性能和泛化性上比SAC有4%和20%的改进，同时将性能的标准差减少了近一半。

    Generalizability and stability are two key objectives for operating reinforcement learning (RL) agents in the real world. Designing RL algorithms that optimize these objectives can be a costly and painstaking process. This paper presents MetaPG, an evolutionary method for automated design of actor-critic loss functions. MetaPG explicitly optimizes for generalizability and performance, and implicitly optimizes the stability of both metrics. We initialize our loss function population with Soft Actor-Critic (SAC) and perform multi-objective optimization using fitness metrics encoding single-task performance, zero-shot generalizability to unseen environment configurations, and stability across independent runs with different random seeds. On a set of continuous control tasks from the Real-World RL Benchmark Suite, we find that our method, using a single environment during evolution, evolves algorithms that improve upon SAC's performance and generalizability by 4% and 20%, respectively, and
    
[^162]: 高斯过程回归中的最大似然估计存在不适定性

    Maximum Likelihood Estimation in Gaussian Process Regression is Ill-Posed. (arXiv:2203.09179v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2203.09179](http://arxiv.org/abs/2203.09179)

    本文指出了高斯过程回归中，使用最大似然估计器估计长度尺度参数的平稳协方差函数的情况下，在无噪声数据情况下会存在不适定问题。

    

    高斯过程回归是机器学习和统计学中无数学术和工业应用的基础，其中最大似然估计通常用于选择适当的协方差核参数。然而，确定最大似然估计为适定问题的情况尚未解决，即，在回归模型的预测对数据的微小扰动不敏感的情况下。本文确定了最大似然估计器无法适定的情况，即在Hellinger距离意义下，预测分布对数据不具有Lipschitz连续性。这些失败情况发生在无噪声数据情况下，对于任何使用最大似然估计估计长度尺度参数的平稳协方差函数的高斯过程。虽然最大似然估计的失败是高斯过程的常识，但这些严格的理论结果似乎是第一次。

    Gaussian process regression underpins countless academic and industrial applications of machine learning and statistics, with maximum likelihood estimation routinely used to select appropriate parameters for the covariance kernel. However, it remains an open problem to establish the circumstances in which maximum likelihood estimation is well-posed, that is, when the predictions of the regression model are insensitive to small perturbations of the data. This article identifies scenarios where the maximum likelihood estimator fails to be well-posed, in that the predictive distributions are not Lipschitz in the data with respect to the Hellinger distance. These failure cases occur in the noiseless data setting, for any Gaussian process with a stationary covariance function whose lengthscale parameter is estimated using maximum likelihood. Although the failure of maximum likelihood estimation is part of Gaussian process folklore, these rigorous theoretical results appear to be the first o
    
[^163]: IoT网络中带中继的实用AoI调度器

    A Practical AoI Scheduler in IoT Networks with Relays. (arXiv:2203.04227v3 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2203.04227](http://arxiv.org/abs/2203.04227)

    本论文提出了一种针对带中继的IoT网络的实用AoI调度器，可考虑常见的动态信道条件和未知数据包生成模式。经过广泛的模拟，证明了所提出的调度器在平均AoI、数据包丢失率和能量消耗方面优于现有的最先进的AoI调度器。

    

    随着设备间自主计算、通信和协作的流行，物联网（IoT）网络已经变得无所不在。在IoT网络中使用中继进一步方便了部署，因为中继提供了许多好处，如增加通信范围和最小化电力消耗。但传统的用于这种双跳继电器IoT网络的AoI调度器的现有文献有限，因为它们是基于假设恒定/不变的信道条件和已知的（通常是随意生成的）数据包生成模式设计的。针对带中继的双跳IoT网络的AoI调度已经研究了深度强化学习（DRL）算法，但是由于动作空间随着网络规模的增大呈指数增长，它们仅适用于小规模IoT网络。这些局限性阻碍了对AoI调度器在IoT网络中的实际应用。本文提出了一种针对带中继的IoT网络的实用AoI调度器，该调度器考虑了常见的动态信道条件和未知数据包生成模式。通过广泛的模拟，证明了所提出的调度器在平均AoI、数据包丢失率和能量消耗方面优于现有的最先进的AoI调度器。

    Internet of Things (IoT) networks have become ubiquitous as autonomous computing, communication and collaboration among devices become popular for accomplishing various tasks. The use of relays in IoT networks further makes it convenient to deploy IoT networks as relays provide a host of benefits, like increasing the communication range and minimizing power consumption. Existing literature on traditional AoI schedulers for such two-hop relayed IoT networks are limited because they are designed assuming constant/non-changing channel conditions and known (usually, generate-at-will) packet generation patterns. Deep reinforcement learning (DRL) algorithms have been investigated for AoI scheduling in two-hop IoT networks with relays, however, they are only applicable for small-scale IoT networks due to exponential rise in action space as the networks become large. These limitations discourage the practical utilization of AoI schedulers for IoT network deployments. This paper presents a prac
    
[^164]: 注意力交互图中的意图感知机器人群体导航

    Intention Aware Robot Crowd Navigation with Attention-Based Interaction Graph. (arXiv:2203.01821v4 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2203.01821](http://arxiv.org/abs/2203.01821)

    提出了一种基于循环图神经网络与注意力机制的机器人导航方法，可以在复杂的人群场景中实现安全、高效和不干扰的导航，通过预测未来轨迹推断出动态代理的意图，避免侵入他人预期路径。

    

    本研究探讨如何在密集且互动的人群中进行安全和意图感知的机器人导航。与大多数基于强化学习（RL）的方法不同，我们提出了一种新颖的循环图神经网络，利用注意力机制捕捉代理之间的异质性交互，以便在空间和时间上确定最优策略。我们根据未来多个时间步长的轨迹预测来推断动态代理的意图，并将这些预测结果纳入一个无模型RL框架中，以避免机器人侵入他人的预期路径。我们证明了我们的方法使机器人在具有挑战性的人群导航场景下能够以高效、安全和不干扰的方式完成任务。我们成功地将在仿真中学习的策略转移到了真实机器人环境中，并验证了该方法的鲁棒性。

    We study the problem of safe and intention-aware robot navigation in dense and interactive crowds. Most previous reinforcement learning (RL) based methods fail to consider different types of interactions among all agents or ignore the intentions of people, which results in performance degradation. To learn a safe and efficient robot policy, we propose a novel recurrent graph neural network with attention mechanisms to capture heterogeneous interactions among agents through space and time. To encourage longsighted robot behaviors, we infer the intentions of dynamic agents by predicting their future trajectories for several timesteps. The predictions are incorporated into a model-free RL framework to prevent the robot from intruding into the intended paths of other agents. We demonstrate that our method enables the robot to achieve good navigation performance and non-invasiveness in challenging crowd navigation scenarios. We successfully transfer the policy learned in simulation to a rea
    
[^165]: 先验、层次和信息不对称在强化学习技能转移中的应用

    Priors, Hierarchy, and Information Asymmetry for Skill Transfer in Reinforcement Learning. (arXiv:2201.08115v2 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2201.08115](http://arxiv.org/abs/2201.08115)

    本文介绍了一种名为“APEx”的技能发现和转移算法，它以一种原则性和学习的方式利用信息不对称性，实现了最先进的采样效率、任务可转移性和概括能力，同时保留了非分层方法的表达能力和灵活性。

    

    从过去的经验中发现行为，并将其转移到新任务中的能力是智能代理在真实世界中高效采样的标志。为智能强化学习者装备相同的能力可能对它们在机器人技术上的成功部署至关重要。虽然分层和KL-正则化强化学习各自在这方面很有希望，但混合方法可能结合它们各自的优点。这些领域的关键在于利用架构模块之间的信息不对称性来偏执学习的技能。虽然不对称性选择对可转移性有很大影响，但现有方法主要基于直觉，以一个与领域无关的可能次优的方式进行选择。在本文中，我们从理论和经验上展示了序列任务中技能的关键表达能力-可转移性的平衡，由信息不对称性控制。在获得这一洞见后，我们引入了一种名为“APEx”的技能发现和转移算法，它以一种原则性和学习的方式利用信息不对称性。APEx实现了最先进的采样效率、任务可转移性和概括能力，同时保留了非分层方法的表达能力和灵活性。

    The ability to discover behaviours from past experience and transfer them to new tasks is a hallmark of intelligent agents acting sample-efficiently in the real world. Equipping embodied reinforcement learners with the same ability may be crucial for their successful deployment in robotics. While hierarchical and KL-regularized reinforcement learning individually hold promise here, arguably a hybrid approach could combine their respective benefits. Key to these fields is the use of information asymmetry across architectural modules to bias which skills are learnt. While asymmetry choice has a large influence on transferability, existing methods base their choice primarily on intuition in a domain-independent, potentially sub-optimal, manner. In this paper, we theoretically and empirically show the crucial expressivity-transferability trade-off of skills across sequential tasks, controlled by information asymmetry. Given this insight, we introduce Attentive Priors for Expressive and Tra
    
[^166]: 形式理论学习系统中的简单性泡沫问题

    A Simplicity Bubble Problem in Formal-Theoretic Learning Systems. (arXiv:2112.12275v2 [cs.IT] UPDATED)

    [http://arxiv.org/abs/2112.12275](http://arxiv.org/abs/2112.12275)

    这篇论文证明了在机器学习中，对于每个算法都存在一个足够大的数据集大小，超过该大小，无法预测欺骗者的算法概率上界是该算法的算法概率上界。

    

    当挖掘大型数据集以预测新数据时，统计机器学习背后的原则限制不仅对大数据洪流构成严重挑战，还对数据生成过程偏向低算法复杂度的传统假设提出了挑战。即使假设有限的数据集生成器存在底层的算法信息偏好，我们也证明了当前的机器学习方法（包括深度学习或任何自上而下人工智能和统计机器学习方法的混合）总是可以被足够大的数据集自然地或人为地欺骗。特别地，我们证明了对于每个学习算法（无论是否有形式化理论的支持），都存在一个足够大的数据集大小，超过该大小，无法预测欺骗者的算法概率上界是该算法的算法概率上界（乘以仅依赖于学习算法的一个乘法常数）。

    When mining large datasets in order to predict new data, limitations of the principles behind statistical machine learning pose a serious challenge not only to the Big Data deluge, but also to the traditional assumptions that data generating processes are biased toward low algorithmic complexity. Even when one assumes an underlying algorithmic-informational bias toward simplicity in finite dataset generators, we show that current approaches to machine learning (including deep learning, or any formal-theoretic hybrid mix of top-down AI and statistical machine learning approaches), can always be deceived, naturally or artificially, by sufficiently large datasets. In particular, we demonstrate that, for every learning algorithm (with or without access to a formal theory), there is a sufficiently large dataset size above which the algorithmic probability of an unpredictable deceiver is an upper bound (up to a multiplicative constant that only depends on the learning algorithm) for the algo
    
[^167]: 带有数据和设备异构性的半分散联邦边缘学习

    Semi-Decentralized Federated Edge Learning with Data and Device Heterogeneity. (arXiv:2112.10313v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.10313](http://arxiv.org/abs/2112.10313)

    本文提出了一种半分散联邦边缘学习（SD-FEEL）框架，采用多个边缘服务器协调大量客户端节点进行深度学习模型的训练，有效利用边缘计算资源以增加训练数据量和降低延迟。

    

    联邦边缘学习（FEEL）以保护隐私的方式有效地利用网络边缘的分布式数据进行深度学习模型的训练，备受关注。然而，单个边缘服务器的覆盖范围有限，导致参与客户端节点数量不足，可能影响学习性能。本文探讨了一种新的FEEL框架，即半分散联邦边缘学习（SD-FEEL），其中多个边缘服务器共同协调大量客户端节点。通过利用边缘服务器之间的低延迟通信实现高效的模型共享，SD-FEEL可以整合更多训练数据，同时与传统联邦学习相比具有更低的延迟。我们详细介绍了SD-FEEL的训练算法，其中包括本地模型更新、簇内和簇间模型聚合等三个主要步骤。证明了该算法在非独立同分布的数据上的收敛性。

    Federated edge learning (FEEL) has attracted much attention as a privacy-preserving paradigm to effectively incorporate the distributed data at the network edge for training deep learning models. Nevertheless, the limited coverage of a single edge server results in an insufficient number of participated client nodes, which may impair the learning performance. In this paper, we investigate a novel framework of FEEL, namely semi-decentralized federated edge learning (SD-FEEL), where multiple edge servers are employed to collectively coordinate a large number of client nodes. By exploiting the low-latency communication among edge servers for efficient model sharing, SD-FEEL can incorporate more training data, while enjoying much lower latency compared with conventional federated learning. We detail the training algorithm for SD-FEEL with three main steps, including local model update, intra-cluster, and inter-cluster model aggregations. The convergence of this algorithm is proved on non-i
    
[^168]: 自适应加权多视图聚类

    Adaptive Weighted Multi-View Clustering. (arXiv:2110.13240v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2110.13240](http://arxiv.org/abs/2110.13240)

    本文提出了一种加权多视图 NMF算法，旨在学习视图特定权重和观测特定重构权重，以量化每个视图的信息内容，通过分配较小和较大的权重来扩大重要视图的正面影响。

    

    学习多视图数据是机器学习研究中一个新兴的问题，非负矩阵分解（NMF）是一种流行的降维方法，用于整合来自多个视图的信息。这些视图通常不仅提供一致性信息，还提供互补信息。然而，大多数多视图NMF算法将每个视图赋予相同的权重，或者通过经验性的线性搜索调整权重，这在没有视图的任何先验知识或计算上是不可行的。在本文中，我们提出了一种加权多视图NMF（WM-NMF）算法。特别地，我们旨在解决关键技术差距，即学习视图特定权重和观测特定重构权重，以量化每个视图的信息内容。引入的加权方案可以通过分配较小和较大的权重来减轻不必要视图的负面影响并扩大重要视图的正面影响。

    Learning multi-view data is an emerging problem in machine learning research, and nonnegative matrix factorization (NMF) is a popular dimensionality-reduction method for integrating information from multiple views. These views often provide not only consensus but also complementary information. However, most multi-view NMF algorithms assign equal weight to each view or tune the weight via line search empirically, which can be infeasible without any prior knowledge of the views or computationally expensive. In this paper, we propose a weighted multi-view NMF (WM-NMF) algorithm. In particular, we aim to address the critical technical gap, which is to learn both view-specific weight and observation-specific reconstruction weight to quantify each view's information content. The introduced weighting scheme can alleviate unnecessary views' adverse effects and enlarge the positive effects of the important views by assigning smaller and larger weights, respectively. Experimental results confir
    
[^169]: 教授智能体如何制图：用于多对象导航的空间推理

    Teaching Agents how to Map: Spatial Reasoning for Multi-Object Navigation. (arXiv:2107.06011v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2107.06011](http://arxiv.org/abs/2107.06011)

    本论文介绍了在多对象导航中引入辅助任务的补充监督形式，以促进为实现目标导向下游任务而训练的智能体中出现空间感知能力的产生。

    

    在视觉导航的背景下，映射新颖环境的能力对于智能体利用所观察到的历史信息，高效地到达已知目标是必要的。这种能力与空间推理有关，智能体能够感知空间关系和规律，发现物体特征。最近的工作引入了可学习的，由深度神经网络参数化的政策，并通过强化学习进行训练。在传统的强化学习设置中，空间制图和推理的能力仅仅从奖励中进行端到端的学习。在这个设置中，我们引入了辅助任务的补充监督形式，旨在促进为实现目标导向下游任务而训练的智能体中出现空间感知能力的产生。我们表明，在学习估计一个智能体与达到目标之间的空间关系指标方面，在多对象导航中具有高度积极的影响。

    In the context of visual navigation, the capacity to map a novel environment is necessary for an agent to exploit its observation history in the considered place and efficiently reach known goals. This ability can be associated with spatial reasoning, where an agent is able to perceive spatial relationships and regularities, and discover object characteristics. Recent work introduces learnable policies parametrized by deep neural networks and trained with Reinforcement Learning (RL). In classical RL setups, the capacity to map and reason spatially is learned end-to-end, from reward alone. In this setting, we introduce supplementary supervision in the form of auxiliary tasks designed to favor the emergence of spatial perception capabilities in agents trained for a goal-reaching downstream objective. We show that learning to estimate metrics quantifying the spatial relationships between an agent at a given location and a goal to reach has a high positive impact in Multi-Object Navigation
    
[^170]: 带有决策相关分布的学习中的近似吸引子区域

    Approximate Regions of Attraction in Learning with Decision-Dependent Distributions. (arXiv:2107.00055v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2107.00055](http://arxiv.org/abs/2107.00055)

    本文分析了在决策相关分布学习中的风险最小化，建立在执行风险最小化的梯度流的扰动轨迹之上，研究了可能存在多个局部最小化器的情况。

    

    随着数据驱动的方法在现实世界中的应用，生成观察数据的过程往往会根据学习者的决策做出反应。例如，数据源可能有一些激励让算法提供特定的标签（如批准银行贷款），并相应地操纵它们的特征。在战略分类和决策相关分布的工作中，通过明确考虑分类器对基础数据分布的影响来表征部署学习算法的闭环行为。最近，在执行预测的研究中，考虑分类器到数据分布的映射的一般属性，而非显式形式来分类闭环行为。基于这个概念，我们将重复的风险最小化分析为执行风险最小化的梯度流的扰动轨迹。我们考虑可能存在多个局部最小化器的场合。

    As data-driven methods are deployed in real-world settings, the processes that generate the observed data will often react to the decisions of the learner. For example, a data source may have some incentive for the algorithm to provide a particular label (e.g. approve a bank loan), and manipulate their features accordingly. Work in strategic classification and decision-dependent distributions seeks to characterize the closed-loop behavior of deploying learning algorithms by explicitly considering the effect of the classifier on the underlying data distribution. More recently, works in performative prediction seek to classify the closed-loop behavior by considering general properties of the mapping from classifier to data distribution, rather than an explicit form. Building on this notion, we analyze repeated risk minimization as the perturbed trajectories of the gradient flows of performative risk minimization. We consider the case where there may be multiple local minimizers of perfor
    
[^171]: HUMAP：层次统一流形逼近与投影

    HUMAP: Hierarchical Uniform Manifold Approximation and Projection. (arXiv:2106.07718v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.07718](http://arxiv.org/abs/2106.07718)

    HUMAP是一种新的层次降维技术，能够在层次探索中保留心理地图并在多个数据集和数据类型上具有优越性。

    

    数据降维技术有助于分析人员理解高维空间中的模式。这些技术通常以散点图形式呈现，应用于各种科学领域，并促进集群和数据样本之间的相似性分析。针对包含许多粒度或遵循信息可视化准则的数据集的分析，层次降维技术是最适合的方法，因为它们先前呈现了主要结构并可以按需提供详细信息。然而，当前的层次降维技术并不能完全解决文献中存在的问题，因为它们不能在层次级别之间保持投影心理地图，也不适用于大多数数据类型。本文介绍了一种新的层次降维技术HUMAP，旨在灵活地保留本地和全局结构以及心理地图，在层次探索中具有优越性，并在多个数据集和数据类型上提供了实证证据。

    Dimensionality reduction (DR) techniques help analysts understand patterns in high-dimensional spaces. These techniques, often represented by scatter plots, are employed in diverse science domains and facilitate similarity analysis among clusters and data samples. For datasets containing many granularities or when analysis follows the information visualization mantra, hierarchical DR techniques are the most suitable approach since they present major structures beforehand and details on demand. However, current hierarchical DR techniques are not fully capable of addressing literature problems because they do not preserve the projection mental map across hierarchical levels or are not suitable for most data types. This work presents HUMAP, a novel hierarchical dimensionality reduction technique designed to be flexible in preserving local and global structures and the mental map throughout hierarchical exploration. We provide empirical evidence of our technique's superiority compared with
    
[^172]: 基于存储的优化方法用于模型无关元学习和个性化联邦学习

    Memory-Based Optimization Methods for Model-Agnostic Meta-Learning and Personalized Federated Learning. (arXiv:2106.04911v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.04911](http://arxiv.org/abs/2106.04911)

    本文提出了一种基于存储的优化方法，用于MAML算法，可以在处理少量任务的情况下实现持续学习，同时提出一种通信有效的基于存储的个性化联邦MAML算法，并在各种基准数据集上验证了所提出的算法的有效性。

    

    近年来，模型无关元学习(MAML)已成为流行的研究领域。然而，MAML的随机优化仍然不完善。现有的MAML算法依赖于“episode”的想法，即抽样几个任务和数据点来更新每次迭代中的元模型。然而，这些算法要么无法保证与常数小批量大小收敛，要么需要在每次迭代中处理大量任务，这对于持续学习或跨设备联邦学习来说是不适当的，因为每次迭代或每轮仅有少量任务可用。为解决这些问题，本文提出了一种基于存储的随机算法，用于MAML，并可以收敛到无限小的误差。所提出的算法需要每次迭代抽样恒定数量的任务和数据样本，使其适用于持续学习场景。此外，本文还引入了一种计算效率高的基于存储的个性化联邦MAML算法，其中每个客户端都有自己的个性化模型。我们的算法只需要在客户端和服务器之间通信一小部分元参数，从而实现高效的跨设备联邦学习。我们所提出的算法在各种基准数据集上得到了验证，显示出与现有MAML方法相比的显著改进。

    In recent years, model-agnostic meta-learning (MAML) has become a popular research area. However, the stochastic optimization of MAML is still underdeveloped. Existing MAML algorithms rely on the ``episode'' idea by sampling a few tasks and data points to update the meta-model at each iteration. Nonetheless, these algorithms either fail to guarantee convergence with a constant mini-batch size or require processing a large number of tasks at every iteration, which is unsuitable for continual learning or cross-device federated learning where only a small number of tasks are available per iteration or per round. To address these issues, this paper proposes memory-based stochastic algorithms for MAML that converge with vanishing error. The proposed algorithms require sampling a constant number of tasks and data samples per iteration, making them suitable for the continual learning scenario. Moreover, we introduce a communication-efficient memory-based MAML algorithm for personalized federa
    
[^173]: 解决MAP-MRF问题的松弛：组合内侧Frank-Wolfe方向

    Solving relaxations of MAP-MRF problems: Combinatorial in-face Frank-Wolfe directions. (arXiv:2010.09567v5 [math.OC] UPDATED)

    [http://arxiv.org/abs/2010.09567](http://arxiv.org/abs/2010.09567)

    该论文提出了一种有效的方法来解决LP松弛的MAP-MRF推理问题，使用组合内侧Frank-Wolfe方向，该方法在一些问题类中是当前最先进的LP求解器。

    

    我们考虑解决LP松弛的MAP-MRF推理问题，特别是最近在(Swoboda，Kolmogorov 2019; Kolmogorov，Pock 2021)中提出的方法。作为关键的计算子例程，它使用Frank-Wolfe（FW）方法的一个变体，在组合多胞体上最小化平滑凸函数。我们提出了一种基于内侧Frank-Wolfe方向的有效实现方式，在(Freund等人2017年)中介绍了该方法在不同上下文中的应用。更一般地，我们为组合子问题定义了一个抽象数据结构，使内侧FW方向成为可能，并描述了其在树状MAP-MRF推理子问题中的特化。实验结果表明，得到的方法是一些问题类的当前最先进的LP求解器。我们的代码可在https://pub.ist.ac.at/~vnk/papers/IN-FACE-FW.html中找到。

    We consider the problem of solving LP relaxations of MAP-MRF inference problems, and in particular the method proposed recently in (Swoboda, Kolmogorov 2019; Kolmogorov, Pock 2021). As a key computational subroutine, it uses a variant of the Frank-Wolfe (FW) method to minimize a smooth convex function over a combinatorial polytope. We propose an efficient implementation of this subproutine based on in-face Frank-Wolfe directions, introduced in (Freund et al. 2017) in a different context. More generally, we define an abstract data structure for a combinatorial subproblem that enables in-face FW directions, and describe its specialization for tree-structured MAP-MRF inference subproblems. Experimental results indicate that the resulting method is the current state-of-art LP solver for some classes of problems. Our code is available at https://pub.ist.ac.at/~vnk/papers/IN-FACE-FW.html.
    
[^174]: 网络中的分层社区结构。

    Hierarchical community structure in networks. (arXiv:2009.07196v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2009.07196](http://arxiv.org/abs/2009.07196)

    本文在网络中研究了分层社区结构，探讨了定义层次结构、确定证据和有效检测分层结构的方法。

    

    模块化和分层社区结构在现实世界中的复杂系统中普遍存在。为了检测和研究这些结构，已经做出了很多努力。在检测模块化方面有重要的理论进展，其中包括使用概率生成模型正式定义社区结构以确定检测限制。检测分层社区结构还引入了额外的挑战。本文在网络中对分层社区结构进行了理论研究，这一方面迄今尚未得到同等严格的关注。我们回答了以下问题：1）应该如何定义社区的层次结构？2）如何确定网络中是否有足够的证据表明存在分层结构？3）如何有效地检测分层结构？我们通过引入基于随机过程的层次结构定义来回答这些问题。

    Modular and hierarchical community structures are pervasive in real-world complex systems. A great deal of effort has gone into trying to detect and study these structures. Important theoretical advances in the detection of modular have included identifying fundamental limits of detectability by formally defining community structure using probabilistic generative models. Detecting hierarchical community structure introduces additional challenges alongside those inherited from community detection. Here we present a theoretical study on hierarchical community structure in networks, which has thus far not received the same rigorous attention. We address the following questions: 1) How should we define a hierarchy of communities? 2) How do we determine if there is sufficient evidence of a hierarchical structure in a network? and 3) How can we detect hierarchical structure efficiently? We approach these questions by introducing a definition of hierarchy based on the concept of stochastic ex
    
[^175]: 适度监督学习：定义、框架和普适性。

    Moderately Supervised Learning: Definition, Framework and Generality. (arXiv:2008.11945v4 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2008.11945](http://arxiv.org/abs/2008.11945)

    适度监督学习包括标准有监督学习和弱监督学习，已有的一些标准有监督学习任务表明，给定的标签并不容易学习，从给定的标签转换为易于学习的目标的过程会显著影响最终的性能，有监督学习的定义隐藏了可能对解决方案影响重大的细节。

    

    适度监督学习包括标准有监督学习和弱监督学习，前者训练数据集标签是理想的（完全、精确和准确），后者训练数据集标签不完美（不完整，不精确或不准确）。经过多次实验，已有的一些标准有监督学习任务的解决方案表明，有时候给定的标签并不容易学习，从给定的标签转换为易于学习的目标的过程会显著影响最终标准有监督学习解决方案的性能。因此，在未考虑从给定标签到易于学习的目标的转化过程属性的情况下，有监督学习的定义隐藏了一些可能对建立解决方案影响重大的细节。

    Learning with supervision has achieved remarkable success in numerous artificial intelligence (AI) applications. In the current literature, by referring to the properties of the labels prepared for the training dataset, learning with supervision is categorized as supervised learning (SL) and weakly supervised learning (WSL). SL concerns the situation where the training data set is assigned with ideal (complete, exact and accurate) labels, while WSL concerns the situation where the training data set is assigned with non-ideal (incomplete, inexact or inaccurate) labels. However, various solutions for SL tasks have shown that the given labels are not always easy to learn, and the transformation from the given labels to easy-to-learn targets can significantly affect the performance of the final SL solutions. Without considering the properties of the transformation from the given labels to easy-to-learn targets, the definition of SL conceals some details that can be critical to building the
    
[^176]: 从多嵌入交互的角度分析知识图谱嵌入方法。

    Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective. (arXiv:1903.11406v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1903.11406](http://arxiv.org/abs/1903.11406)

    本文提出了一种多嵌入交互的角度来分析和比较知识图谱嵌入方法，研究了CP，DistMult和ComplEx等三种流行的方法，揭示了ComplEx性能更好的原因，并为开发新模型提供了见解。

    

    知识图谱是一种广泛用于表示知识的格式，且在语义搜索引擎、问答系统和推荐系统中有许多应用。然而，实际的知识图谱通常是不完整的，因此提出了基于嵌入向量方法，如CP、DistMult和ComplEx等，以解决此问题。这些方法将实体和关系表示为语义空间中的嵌入向量，并预测它们之间的链接。嵌入向量本身包含丰富的语义信息，可用于其他应用程序，如数据分析。但是，这些模型中的机制和嵌入向量本身变化很大，使其难以理解和比较。因此，我们需要以多嵌入交互的角度来分析和比较知识图谱嵌入方法。具体而言，我们研究了三种流行的方法（CP、DistMult和ComplEx），并研究它们的不同嵌入向量如何相互作用。我们的分析揭示了ComplEx性能更好的原因，并为开发新模型提供了见解。

    Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition/Parallel factorization (CP), DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valu
    
[^177]: 集成采样

    Ensemble Sampling. (arXiv:1705.07347v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/1705.07347](http://arxiv.org/abs/1705.07347)

    本文介绍了集成采样，以近似Thompson采样并在复杂模型下保持可行性，将大大扩展Thompson采样的应用范围。

    

    Thompson采样已经成为一种解决多种在线决策问题的有效启发式算法。在其基本形式中，该算法需要计算并从模型的后验分布中采样，仅在简单特殊情况下才可行。本文介绍了集成采样，旨在近似Thompson采样，同时在复杂模型（如神经网络）的面前保持可行性。集成采样大大扩展了适用Thompson采样的应用范围。我们建立了支持该方法的理论基础，并提供了进一步的计算结果来提供更多见解。

    Thompson sampling has emerged as an effective heuristic for a broad range of online decision problems. In its basic form, the algorithm requires computing and sampling from a posterior distribution over models, which is tractable only for simple special cases. This paper develops ensemble sampling, which aims to approximate Thompson sampling while maintaining tractability even in the face of complex models such as neural networks. Ensemble sampling dramatically expands on the range of applications for which Thompson sampling is viable. We establish a theoretical basis that supports the approach and present computational results that offer further insight.
    

