# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [On the Semantics of LM Latent Space: A Vocabulary-defined Approach](https://rss.arxiv.org/abs/2401.16184) | 本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。 |
| [^2] | [MODNO: Multi Operator Learning With Distributed Neural Operators](https://arxiv.org/abs/2404.02892) | 本文提出了一种针对多算子学习挑战的新型分布式训练方法，有效地实现单个神经算子处理多算子学习问题，而不增加额外平均成本。 |
| [^3] | [Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds](https://arxiv.org/abs/2404.02866) | 通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度 |
| [^4] | [Measuring Social Norms of Large Language Models](https://arxiv.org/abs/2404.02491) | 论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。 |
| [^5] | [Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief](https://arxiv.org/abs/2404.02448) | 提出一种新型的基于电动车辆的路径问题，通过组合基于规则的车辆选择器和基于强化学习的节点选择器解决电动车辆路径问题，以最小化总行驶距离和故障基站数量。 |
| [^6] | [Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials](https://arxiv.org/abs/2404.01981) | 提出在临床试验中利用患者语音数据进行多语言说话者验证，以应对身份验证和排除重复入组的挑战。 |
| [^7] | [Bi-LORA: A Vision-Language Approach for Synthetic Image Detection](https://arxiv.org/abs/2404.01959) | 基于视觉-语言方法，引入了Bi-LORA方法，通过结合VLMs和LORA调整技术，将合成图像检测转化为图像字幕任务，以提高对未见过的模型生成图像的精度。 |
| [^8] | [Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation](https://arxiv.org/abs/2404.01518) | 提出了一种基于解决最优传输问题的动作分割方法，通过在Gromov-Wasserstein问题中编码时间一致性先验来实现从视频帧和动作类别之间的噪声成本中解码时间一致的分割。 |
| [^9] | [Metarobotics for Industry and Society: Vision, Technologies, and Opportunities](https://arxiv.org/abs/2404.00797) | Metarobotics旨在通过结合无线通信、多感官沉浸和集体智能，为远程机器人应用提供普遍、流动和非侵入式的访问和互动，有望为工业和社会带来诸多益处。 |
| [^10] | [CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization](https://arxiv.org/abs/2404.00521) | 通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。 |
| [^11] | [Cross-lingual Named Entity Corpus for Slavic Languages](https://arxiv.org/abs/2404.00482) | 介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。 |
| [^12] | [A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks](https://arxiv.org/abs/2404.00076) | 提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。 |
| [^13] | [FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://arxiv.org/abs/2403.20261) | FABind+通过改进口袋预测和姿态生成，提升分子对接表现 |
| [^14] | [Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science](https://arxiv.org/abs/2403.20208) | 本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。 |
| [^15] | [TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods](https://arxiv.org/abs/2403.20150) | TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。 |
| [^16] | [FairRAG: Fair Human Generation via Fair Retrieval Augmentation](https://arxiv.org/abs/2403.19964) | FairRAG框架通过在外部图像数据库检索到的参考图像来提高人类生成中的公平性，并应用简单但有效的去偏策略，从而为生成过程提供来自不同人口统计组的图像。 |
| [^17] | [Understanding the Learning Dynamics of Alignment with Human Feedback](https://arxiv.org/abs/2403.18742) | 本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。 |
| [^18] | [Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation](https://arxiv.org/abs/2403.17886) | 基于神经嵌入压缩的多任务嵌入方法在地球观测中实现了数据高效的训练和推断，通过压缩率与嵌入效用之间的权衡，取得了数据量显著减少的准确性。 |
| [^19] | [LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation](https://arxiv.org/abs/2403.17601) | 提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题 |
| [^20] | [Understanding Domain-Size Generalization in Markov Logic Networks](https://arxiv.org/abs/2403.15933) | 本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。 |
| [^21] | [Enhancing Automatic Modulation Recognition for IoT Applications Using Transformers](https://arxiv.org/abs/2403.15417) | 使用Transformer网络提出了一种高效的自动调制识别方法，在物联网环境中具有最佳的识别准确率。 |
| [^22] | [Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective](https://arxiv.org/abs/2403.14917) | 本文通过核方法的视角研究了两层神经网络在平均场极限下的特征学习能力，展示了它们比任何核方法更有效地学习多个再现核希尔伯特空间的并集，并且神经网络会获得与目标函数对齐的数据相关核。 |
| [^23] | [On Pretraining Data Diversity for Self-Supervised Learning](https://arxiv.org/abs/2403.13808) | 增加预训练数据多样性可以提高自监督学习性能，但仅在与下游数据的分布距离较小时有效。 |
| [^24] | [Federated reinforcement learning for robot motion planning with zero-shot generalization](https://arxiv.org/abs/2403.13245) | 该论文提出了一个联邦强化学习框架，实现了机器人运动规划中的零次通用化，通过协作学习多个学习者和中央服务器，在不共享原始数据的情况下达到全局最优解。 |
| [^25] | [Provable Privacy with Non-Private Pre-Processing](https://arxiv.org/abs/2403.13041) | 提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限 |
| [^26] | [Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)](https://arxiv.org/abs/2403.09680) | 该论文提出了一种利用Tsetlin Machines进行传统监督学习的机器学习预排序阶段方法，在MNIST级别的分类问题上取得了显著的精度提升，以及训练时间和推理时间大幅度减少。 |
| [^27] | [Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information](https://arxiv.org/abs/2403.09516) | 通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。 |
| [^28] | [Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference](https://arxiv.org/abs/2403.09054) | 本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。 |
| [^29] | [Exploring Safety Generalization Challenges of Large Language Models via Code](https://arxiv.org/abs/2403.07865) | 本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。 |
| [^30] | [Restricted Bayesian Neural Network](https://arxiv.org/abs/2403.04810) | 本研究提出了限制贝叶斯神经网络的新架构，显著减少了网络存储空间复杂性，并引入了一种能够有效处理不确定性的算法，确保在目标函数缺乏完美凸性时稳健地收敛至全局最优解。 |
| [^31] | [3D Diffusion Policy](https://arxiv.org/abs/2403.03954) | 3D扩散策略（DP3）是一种新颖的视觉模仿学习方法，通过将3D视觉表示的强大性结合到扩散策略中，成功解决了学习复杂技能所需大量人类演示的问题。 |
| [^32] | [Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits](https://arxiv.org/abs/2403.01317) | 提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。 |
| [^33] | [Polynormer: Polynomial-Expressive Graph Transformer in Linear Time](https://arxiv.org/abs/2403.01232) | Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。 |
| [^34] | [A Survey of Route Recommendations: Methods, Applications, and Opportunities](https://arxiv.org/abs/2403.00284) | 基于城市计算的路线推荐综述对路线推荐研究中的传统机器学习和现代深度学习方法进行了分类，展示了与城市计算场景相关的新应用，并揭示了最新进展。 |
| [^35] | [DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning](https://arxiv.org/abs/2402.17453) | DS-Agent是一个自动框架，结合了大型语言模型代理和案例推理，能够在数据科学任务中灵活利用专家知识并通过反馈机制持续改善性能 |
| [^36] | [Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation](https://arxiv.org/abs/2402.15781) | 该论文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法，并证明了当采样时间跨度 n 足够大时这些算法会收敛到有意义的解。 |
| [^37] | [Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding](https://arxiv.org/abs/2402.15102) | 自动广告竞标中使用了一种新的迭代离线强化学习框架，有效缓解了传统RL算法在在线环境下性能下降的问题。 |
| [^38] | [IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus](https://arxiv.org/abs/2402.14710) | 发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。 |
| [^39] | [Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective](https://arxiv.org/abs/2402.10184) | 本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。 |
| [^40] | [Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data](https://arxiv.org/abs/2402.10100) | 本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。 |
| [^41] | [Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and Communication System](https://arxiv.org/abs/2402.09441) | 本文针对智能反射表面辅助集成感知与通信系统中的信道估计问题，提出了一个基于深度学习的三阶段方法。该方法通过解耦问题，分别估计直接感知和通信信道、反射通信信道和反射感知信道，以应对智能反射表面的信号处理能力不足和感知与通信信号之间的互相干扰。 |
| [^42] | [Extreme Learning Machine-based Channel Estimation in IRS-Assisted Multi-User ISAC System](https://arxiv.org/abs/2402.09440) | 本文提出了一种基于极限学习机的智能反射面辅助多用户ISAC系统的信道估计方法，该方法通过将估计问题分解成子问题来解决了感知和通信信号干扰以及被动式IRS缺乏信号处理能力的挑战。该方法可以在保持低成本需求的情况下实现对SAC信道和下行通信信道的准确估计。 |
| [^43] | [Deep-Learning-Based Channel Estimation for IRS-Assisted ISAC System](https://arxiv.org/abs/2402.09439) | 本文提出了一种基于深度学习的框架，在IRS辅助的ISAC系统中解决了信道估计问题。通过设计两种不同的神经网络架构，该方法在不同的信道环境下实现了优越性能。 |
| [^44] | [EcoVal: An Efficient Data Valuation Framework for Machine Learning](https://arxiv.org/abs/2402.09288) | EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。 |
| [^45] | [Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA](https://arxiv.org/abs/2402.07710) | 本论文研究了在3D点云上进行稀疏卷积的GPU优化方法，以解决点云的稀疏性和计算问题。 |
| [^46] | [Dynamic Graph Information Bottleneck](https://arxiv.org/abs/2402.06716) | 动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。 |
| [^47] | [A Survey on Transformer Compression](https://arxiv.org/abs/2402.05964) | 《Transformer压缩调研》是对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。 |
| [^48] | [Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations](https://arxiv.org/abs/2402.05713) | 该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。 |
| [^49] | [Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving](https://arxiv.org/abs/2402.05359) | 该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。 |
| [^50] | [Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey](https://arxiv.org/abs/2402.04854) | 该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。 |
| [^51] | [Space Group Constrained Crystal Generation](https://arxiv.org/abs/2402.03992) | 本文在晶体生成中考虑了空间群约束，提出了一个更容易实现的等效形式，进一步提出了一个改进的扩散模型DiffCSP++，实验结果表明其有效性。 |
| [^52] | [Causal Bayesian Optimization via Exogenous Distribution Learning](https://arxiv.org/abs/2402.02277) | 本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。 |
| [^53] | [Online Transfer Learning for RSV Case Detection](https://arxiv.org/abs/2402.01987) | 这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。 |
| [^54] | [Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators](https://arxiv.org/abs/2401.17548) | 本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。 |
| [^55] | [Simple Policy Optimization](https://arxiv.org/abs/2401.16025) | SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。 |
| [^56] | [INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning](https://arxiv.org/abs/2401.11667) | INCPrompt采用自适应关键学习者和面向任务的提示，结合通用和任务特定知识，有效缓解灾难性遗忘，表现优越，对持续学习性能具有显著影响。 |
| [^57] | [Towards an end-to-end artificial intelligence driven global weather forecasting system](https://arxiv.org/abs/2312.12462) | 提出了一种端到端基于人工智能的全球天气预报系统，通过将AI技术应用于数据同化和天气预报模型，实现了从数据处理到预测全过程的自动化。 |
| [^58] | [Multinomial belief networks](https://arxiv.org/abs/2311.16909) | 提出了一种深度生成模型，用于处理具有多项式计数数据的分析需求，并能够从数据中完全自动提取出生物意义的元签名。 |
| [^59] | [Locality Sensitive Sparse Encoding for Learning World Models Online.](http://arxiv.org/abs/2401.13034) | 本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。 |
| [^60] | [Cascading Reinforcement Learning.](http://arxiv.org/abs/2401.08961) | 本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。 |
| [^61] | [Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series.](http://arxiv.org/abs/2401.03955) | 本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。 |
| [^62] | [Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective.](http://arxiv.org/abs/2312.10401) | 本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。 |
| [^63] | [Faithful and Robust Local Interpretability for Textual Predictions.](http://arxiv.org/abs/2311.01605) | 提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。 |
| [^64] | [Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial.](http://arxiv.org/abs/2311.00537) | 这项研究介绍了一种非线性学习变质材料，它由基于晶体管的自适应非线性阻性元件构成，可以在没有计算机的情况下学习非线性任务，并降低训练误差的多个模式，为模拟机器学习提供了新的硬件选择。 |
| [^65] | [Multitask Online Learning: Listen to the Neighborhood Buzz.](http://arxiv.org/abs/2310.17385) | 我们提出了一种多任务在线学习的算法，代理只能通过邻居交换信息。我们的分析表明，当代理在相似的任务上操作时，我们的算法的遗憾值显著改善。此外，我们证明了算法在损失函数为线性函数时可以保护隐私。 |
| [^66] | [On the Representational Capacity of Recurrent Neural Language Models.](http://arxiv.org/abs/2310.12942) | 本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。 |
| [^67] | [Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification.](http://arxiv.org/abs/2310.11104) | 本文研究了使用ReLUs作为激活函数的前馈神经网络的本地Lipschitz常数计算，并介绍了一种通过精确性验证计算上界的方法。 |
| [^68] | [QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models.](http://arxiv.org/abs/2310.08041) | QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。 |
| [^69] | [Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning.](http://arxiv.org/abs/2310.07518) | 本文提出了一种利用因果图先验和后验采样的方法来提高强化学习样本效率。通过同时学习完整的因果图和分解动态参数，该方法能够更自然地设计先验，并且根据先验知识程度连接了遗憾率与贝叶斯遗憾之间的关系。 |
| [^70] | [A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning.](http://arxiv.org/abs/2310.06253) | 这项工作提供了一个关于解决模型驱动的强化学习中目标不匹配问题的统一观点，对解决方案进行了分类，并提出了一个分类法以促进未来的研究。 |
| [^71] | [TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design.](http://arxiv.org/abs/2310.03223) | 该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。 |
| [^72] | [Learning Dissipative Neural Dynamical Systems.](http://arxiv.org/abs/2309.16032) | 本文提出了一种学习耗散神经动力系统模型的方法，该方法分为两个阶段。首先学习一个无约束的模型，然后导出条件来保证模型的耗散性质，并在保持逼近能力的同时扰动偏差。通过独立求解这两个扰动问题，得到一个保证为耗散的神经动力模型，同时紧密逼近非线性系统。 |
| [^73] | [Improving Generalization in Game Agents with Data Augmentation in Imitation Learning.](http://arxiv.org/abs/2309.12815) | 本文提出了一种改进模仿学习中游戏智能体泛化能力的方法，通过数据增强技术使训练数据更好地代表真实场景中的状态和行动分布，并在多个3D环境中进行了性能测试，结果表明数据增强可以显著提升模仿学习智能体的泛化能力。 |
| [^74] | [A Bayesian Approach to Robust Inverse Reinforcement Learning.](http://arxiv.org/abs/2309.08571) | 这篇论文提出了一种贝叶斯方法，用于稳健的离线模型导向的逆强化学习。通过同时估计专家的奖励函数和主观模型的环境动态，利用先验分布参数化专家对环境模型的准确性，提出了高效的算法。实验证明，当先验地认为专家对环境具有高度准确的模型时，估计的策略表现出稳健性能，并且优于最先进的离线IRL算法。 |
| [^75] | [Frequency Convergence of Complexon Shift Operators.](http://arxiv.org/abs/2309.07169) | 本文研究了拓扑信号处理中复合子的可转移性，通过构造边际复合子和复合移位算子，研究其特征值和特征向量，并证明了复合子收敛时对应的复合移位算子的特征值会收敛到极限复合子的特征值。这些结果拓展了图信号处理框架。 |
| [^76] | [Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations.](http://arxiv.org/abs/2309.05575) | 该论文研究了一个大型的有限差分离散化家族，在非均匀扩散过程中通过将二维扩散分解为四个一维扩散得出了一个3 x 3的模板。该模板类包含一个自由参数，涵盖了广泛的现有离散化。同时，论文还建立了与模板相对应的矩阵的谱范数上界，并将显式方案转化为ResNet块。 |
| [^77] | [Multimodal Transformer for Material Segmentation.](http://arxiv.org/abs/2309.04001) | 本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。 |
| [^78] | [Neural Implicit Morphing of Face Images.](http://arxiv.org/abs/2308.13888) | 本论文提出了一种利用神经网络实现人脸图像变形和混合的方法，通过利用网络的平滑性和灵活性，结合经典方法中的能量函数，实现了高效、准确和多样化的人脸变形效果。 |
| [^79] | [Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task.](http://arxiv.org/abs/2308.08873) | FE-PINN是一种学习底层物理特征的框架，在主训练之前以低计算成本解决问题的模式。与传统PINN相比，FE-PINN通过执行一系列子任务来解决损失函数不平衡的问题，并具有快速训练和更高的求解速度。 |
| [^80] | [On the Effectiveness of Log Representation for Log-based Anomaly Detection.](http://arxiv.org/abs/2308.08736) | 本研究调查和比较了先前日志分析研究中常用的日志表示技术，并评估了它们在不同机器学习模型和公共日志数据上的表现。 |
| [^81] | [Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing.](http://arxiv.org/abs/2308.02464) | 通过随机性的储备计算，RNNs可以通用逼近线性时不变系统，这一观察到的性能在理论上得到了支持。 |
| [^82] | [PIGEON: Predicting Image Geolocations.](http://arxiv.org/abs/2307.05845) | PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。 |
| [^83] | [Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity.](http://arxiv.org/abs/2306.13263) | 本文提出了一种通过对本地生成的合成数据进行重排来加速异构数据下联邦学习的收敛的方法，实验表明，对合成数据进行重排可以大幅提高现有多个联邦学习算法的性能。 |
| [^84] | [MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates.](http://arxiv.org/abs/2306.12212) | 本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。 |
| [^85] | [Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects.](http://arxiv.org/abs/2306.10125) | 自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。 |
| [^86] | [DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting.](http://arxiv.org/abs/2306.09862) | DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。 |
| [^87] | [Fedstellar: A Platform for Decentralized Federated Learning.](http://arxiv.org/abs/2306.09750) | Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。 |
| [^88] | [CHORUS: Foundation Models for Unified Data Discovery and Exploration.](http://arxiv.org/abs/2306.09610) | 研究者探索将大型语言模型应用于数据发现和探索任务中，证明这些模型在表格类检测、列类型注释和联接列预测中具有优越性能，并有望将不同的数据管理任务统一在基础模型下。 |
| [^89] | [Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming.](http://arxiv.org/abs/2305.18436) | 本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。 |
| [^90] | [Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers.](http://arxiv.org/abs/2305.17328) | 《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。 |
| [^91] | [AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys.](http://arxiv.org/abs/2305.09620) | 本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。 |
| [^92] | [Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs.](http://arxiv.org/abs/2305.03935) | 本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。 |
| [^93] | [Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse.](http://arxiv.org/abs/2305.00510) | 本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。 |
| [^94] | [Deep Stock: training and trading scheme using deep learning.](http://arxiv.org/abs/2304.14870) | 本文提出了一种使用深度学习进行训练和交易的方案，DeepStock通过查看股票价格的过去数据，并使用Resnet和logits来预测股票价格在未来D天内是否会升降一定百分比，并在韩国和美国市场上取得了超过市场回报的利润。 |
| [^95] | [Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model.](http://arxiv.org/abs/2304.07633) | 本论文提出了一种可解释的神经符号模型，用于检测上下文不符的虚假多模态信息，帮助事实检查网站进行记录澄清。 |
| [^96] | [Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales.](http://arxiv.org/abs/2304.06875) | 提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。 |
| [^97] | [Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI.](http://arxiv.org/abs/2303.07540) | 提出了一种基于张量学习的流程，从多模态心脏磁共振成像（MRI）中识别肺动脉楔压力（PAWP）。通过整合多种特征信息，提高了识别准确度。 |
| [^98] | [Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks.](http://arxiv.org/abs/2302.11628) | 本文提出了一种名为特征分区聚合的认证防御方法，用于对抗$\ell_0$逃避、后门和污染攻击。与现有防御方法相比，FPA速度更快，提供更大的鲁棒性保证，且能够免费提供额外的鲁棒性维度。 |
| [^99] | [Online Continuous Hyperparameter Optimization for Contextual Bandits.](http://arxiv.org/abs/2302.09440) | 该论文提出了面向上下文强化学习的在线连续超参数调整框架CDT，能够动态地在搜索空间内学习最优参数配置。 |
| [^100] | [Extensible Motion-based Identification of XR Users using Non-Specific Motion Data.](http://arxiv.org/abs/2302.07517) | 提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。 |
| [^101] | [Empirical Risk Minimization with Relative Entropy Regularization.](http://arxiv.org/abs/2211.06617) | 本文研究了相对熵正则化的经验风险最小化问题，其中参考度量为sigma有限测度，解为唯一的概率测度并展现了几乎正确的保证。ERM-RER问题的解被称为Gibbs算法。 |
| [^102] | [Machine Learning in Orbit Estimation: a Survey.](http://arxiv.org/abs/2207.08993) | 本综述介绍了机器学习在轨道估计中的应用现状，讨论了当前物理方法的不足，提出了通过推导未测量物体的特征来提高轨道预测准确度的方案。 |
| [^103] | [Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization.](http://arxiv.org/abs/2112.14368) | 本研究提出一种面向在线凸优化的动态遗憾算法，可以在一些简单的问题实例中进一步增强保证，具有几何直观性，实验表明其优于最先进的基线算法。 |

# 详细

[^1]: 关于LM潜在空间的语义学：一种以词汇为定义的方法

    On the Semantics of LM Latent Space: A Vocabulary-defined Approach

    [https://rss.arxiv.org/abs/2401.16184](https://rss.arxiv.org/abs/2401.16184)

    本论文介绍了一种以词汇为定义的语义学方法，建立了LM潜在空间的参考框架，确保基于LM词汇的分离语义分析。在LM适应过程中，引入了计算logits的新技术和神经聚类模块，通过实验证明了该方法在文本理解上的优越性能。

    

    理解语言模型(LM)的潜在空间对于改进其性能和可解释性至关重要。现有的分析往往在提供基于模型的对LM语义的分离洞察方面存在不足，并忽视了LM适应的重要方面。为了响应这一问题，我们引入了一种开创性的方法，称为以词汇为定义的语义学，它在LM的潜在空间中建立了一个参考框架，确保基于LM词汇的分离语义分析。我们的方法超越了先前的交织分析，利用LM词汇来获得以模型为中心的洞察。此外，我们提出了一种计算logits的新技术，强调可微分性和局部等距性，并引入了一个神经聚类模块，用于在LM适应过程中进行语义校准。通过在多种文本理解数据集上进行广泛实验，我们的方法在检索增强生成和参数高效微调方面超越了最先进的方法。

    Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
    
[^2]: MODNO: 具有分布式神经算子的多算子学习

    MODNO: Multi Operator Learning With Distributed Neural Operators

    [https://arxiv.org/abs/2404.02892](https://arxiv.org/abs/2404.02892)

    本文提出了一种针对多算子学习挑战的新型分布式训练方法，有效地实现单个神经算子处理多算子学习问题，而不增加额外平均成本。

    

    运算符学习的研究涉及利用神经网络来逼近算子。传统上，重点放在单算子学习（SOL）上。然而，最近的进展迅速将其扩展到包含使用具有数百万或数十亿可训练参数的基础模型来逼近多算子，从而导致了多算子学习（MOL）的研究。在本文中，我们提出了一种新颖的分布式训练方法，旨在使单个神经算子能够有效地处理多算子学习挑战，而不会产生额外的平均成本。我们的方法适用于各种类似Chen-Chen型神经算子，如深算子神经网络（DON）。其核心思想是独立学习每个算子的输出基函数，使用其专用数据，同时集中学习输入fu。

    arXiv:2404.02892v1 Announce Type: new  Abstract: The study of operator learning involves the utilization of neural networks to approximate operators. Traditionally, the focus has been on single-operator learning (SOL). However, recent advances have rapidly expanded this to include the approximation of multiple operators using foundation models equipped with millions or billions of trainable parameters, leading to the research of multi-operator learning (MOL). In this paper, we present a novel distributed training approach aimed at enabling a single neural operator with significantly fewer parameters to effectively tackle multi-operator learning challenges, all without incurring additional average costs. Our method is applicable to various Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON). The core idea is to independently learn the output basis functions for each operator using its dedicated data, while simultaneously centralizing the learning of the input fu
    
[^3]: 通过Hammersley-Chapman-Robbins界限保证机密性

    Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds

    [https://arxiv.org/abs/2404.02866](https://arxiv.org/abs/2404.02866)

    通过添加噪声到最后层的激活来保护隐私，使用HCR界限可量化保护机密性的可信度

    

    在深度神经网络推断过程中通过向最后几层的激活添加噪声来保护隐私是可能的。这些层中的激活被称为“特征”（少见的称为“嵌入”或“特征嵌入”）。添加的噪声有助于防止从嘈杂的特征中重建输入。通过对所有可能的无偏估计量的方差进行下限估计，量化了由此添加的噪声产生的机密性。经典不等式Hammersley和Chapman以及Robbins提供便利的、可计算的界限-- HCR界限。数值实验表明，对于包含10个类别的图像分类数据集“MNIST”和“CIFAR-10”，HCR界限在小型神经网络上表现良好。HCR界限似乎单独无法保证

    arXiv:2404.02866v1 Announce Type: new  Abstract: Protecting privacy during inference with deep neural networks is possible by adding noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as "features" (or, less commonly, as "embeddings" or "feature embeddings"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, "MNIST" and "CIFAR-10," which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guar
    
[^4]: 测量大型语言模型的社会规范

    Measuring Social Norms of Large Language Models

    [https://arxiv.org/abs/2404.02491](https://arxiv.org/abs/2404.02491)

    论文提出了一个挑战，测试大型语言模型对社会规范的理解，构建了一个数据集涵盖广泛的社会规范问题，通过多代理框架基于大型语言模型来提高模型对社会规范的理解能力。

    

    我们提出了一个新的挑战，以检验大型语言模型是否理解社会规范。与现有数据集不同，我们的数据集要求具有解决社会规范的基本理解。我们的数据集包含最大的社会规范技能集，包括402项技能和12,383个问题，涵盖了从观点和论点到文化和法律等广泛的社会规范。我们根据K-12课程设计了我们的数据集。这使得可以直接将大型语言模型的社会理解能力与人类进行比较，更具体地说是与小学生进行比较。尽管先前的工作在我们的基准测试中产生几乎随机的准确度，但最近的大型语言模型（如GPT3.5-Turbo和LLaMA2-Chat）能够显著提高性能，仅略低于人类性能。然后，我们提出了一个基于大型语言模型的多代理框架，以提高模型理解社会规范的能力。

    arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
    
[^5]: 电动车辆路径问题用于应急供电：面向电信基站救助

    Electric Vehicle Routing Problem for Emergency Power Supply: Towards Telecom Base Station Relief

    [https://arxiv.org/abs/2404.02448](https://arxiv.org/abs/2404.02448)

    提出一种新型的基于电动车辆的路径问题，通过组合基于规则的车辆选择器和基于强化学习的节点选择器解决电动车辆路径问题，以最小化总行驶距离和故障基站数量。

    

    作为一家电信提供商，我们公司有一个关键使命，即在停电期间保持电信服务。为了实现这一使命，至关重要的是维持电信基站的电力。本文考虑一种解决方案，即电动车辆 (EVs) 直接前往其位置为基站提供电力。我们的目标是找到最小化所有电动车辆的总行驶距离和故障基站数量的EV路线。在本文中，我们将这一路径问题形式化为新型的电动车辆路径问题 (EVRP) 变体，并提出了将基于规则的车辆选择器和基于强化学习（RL）的节点选择器相结合的求解器。车辆选择器的规则确保了所选EV开始移动时的确切环境状态。此外，RL模型的节点选择实现了快速路径生成，在紧急情况下尤为重要。我们在机器人上对我们的求解器进行评估

    arXiv:2404.02448v1 Announce Type: cross  Abstract: As a telecom provider, our company has a critical mission to maintain telecom services even during power outages. To accomplish the mission, it is essential to maintain the power of the telecom base stations. Here we consider a solution where electric vehicles (EVs) directly supply power to base stations by traveling to their locations. The goal is to find EV routes that minimize both the total travel distance of all EVs and the number of downed base stations. In this paper, we formulate this routing problem as a new variant of the Electric Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based vehicle selector and a reinforcement learning (RL)-based node selector. The rule of the vehicle selector ensures the exact environmental states when the selected EV starts to move. In addition, the node selection by the RL model enables fast route generation, which is critical in emergencies. We evaluate our solver on bot
    
[^6]: 临床试验中的零样本多语言说话者验证

    Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials

    [https://arxiv.org/abs/2404.01981](https://arxiv.org/abs/2404.01981)

    提出在临床试验中利用患者语音数据进行多语言说话者验证，以应对身份验证和排除重复入组的挑战。

    

    由于临床试验涉及大量临床医生、患者和数据收集环境，获取高质量数据面临重大挑战。在临床试验中，患者根据其语音数据进行评估，以检测和监测认知和心理健康障碍。我们提出使用这些语音录音来验证已入组患者的身份，并识别和排除试图多次入组同一试验的个体。由于临床研究经常跨越不同国家进行，因此必须创建一个能够在不同语言中执行说话者验证的系统，而无需额外开发工作。我们通过招募和测试讲英语、德语、丹麦语、西班牙语和阿拉伯语的语音受损患者来评估预训练的TitaNet、ECAPA-TDNN和SpeakerNet模型。我们的结果表明，经过测试的模型能够有效泛化。

    arXiv:2404.01981v1 Announce Type: new  Abstract: Due to the substantial number of clinicians, patients, and data collection environments involved in clinical trials, gathering data of superior quality poses a significant challenge. In clinical trials, patients are assessed based on their speech data to detect and monitor cognitive and mental health disorders. We propose using these speech recordings to verify the identities of enrolled patients and identify and exclude the individuals who try to enroll multiple times in the same trial. Since clinical studies are often conducted across different countries, creating a system that can perform speaker verification in diverse languages without additional development effort is imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models by enrolling and testing with speech-impaired patients speaking English, German, Danish, Spanish, and Arabic languages. Our results demonstrate that tested models can effectively generalize 
    
[^7]: Bi-LORA：一种用于合成图像检测的视觉-语言方法

    Bi-LORA: A Vision-Language Approach for Synthetic Image Detection

    [https://arxiv.org/abs/2404.01959](https://arxiv.org/abs/2404.01959)

    基于视觉-语言方法，引入了Bi-LORA方法，通过结合VLMs和LORA调整技术，将合成图像检测转化为图像字幕任务，以提高对未见过的模型生成图像的精度。

    

    深度图像合成技术的进步，如生成对抗网络（GAN）和扩散模型（DM），已经开启了一个生成高度逼真图像的时代。尽管这种技术进步引起了极大的兴趣，但也引发了对于区分真实图像和合成图像之间潜在难度的担忧。本文借鉴了视觉和语言之间强大的收敛能力，结合了视觉-语言模型（VLMs）的零次学习特性。我们引入了一种名为Bi-LORA的创新方法，利用VLMs，结合低秩适应（LORA）调整技术，增强了对未见过的模型生成图像的合成图像检测的精度。我们方法的关键概念转变在于将二分类重新构建为图像字幕任务，利用了尖端VLM的独特能力。

    arXiv:2404.01959v1 Announce Type: cross  Abstract: Advancements in deep image synthesis techniques, such as generative adversarial networks (GANs) and diffusion models (DMs), have ushered in an era of generating highly realistic images. While this technological progress has captured significant interest, it has also raised concerns about the potential difficulty in distinguishing real images from their synthetic counterparts. This paper takes inspiration from the potent convergence capabilities between vision and language, coupled with the zero-shot nature of vision-language models (VLMs). We introduce an innovative method called Bi-LORA that leverages VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance the precision of synthetic image detection for unseen model-generated images. The pivotal conceptual shift in our methodology revolves around reframing binary classification as an image captioning task, leveraging the distinctive capabilities of cutting-edge VLM
    
[^8]: 一种用于无监督动作分割的临时一致不平衡最优传输方法

    Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation

    [https://arxiv.org/abs/2404.01518](https://arxiv.org/abs/2404.01518)

    提出了一种基于解决最优传输问题的动作分割方法，通过在Gromov-Wasserstein问题中编码时间一致性先验来实现从视频帧和动作类别之间的噪声成本中解码时间一致的分割。

    

    我们提出了一种针对长时间未修剪视频的动作分割任务的新方法，基于解决最优传输问题。通过将时间一致性先验编码到Gromov-Wasserstein问题中，我们能够从视频帧和动作类别之间的噪声关联/匹配成本矩阵中解码出一个时间一致的分割。与先前方法不同，我们的方法不需要知道视频的动作顺序来实现时间一致性。此外，我们的结果（融合）Gromov-Wasserstein问题可以在GPU上使用几次投影镜下降迭代高效求解。我们在无监督学习环境中展示了我们方法的有效性，其中我们的方法用于生成自训练的伪标签。我们在Breakfast、50-Salads、YouTube Instructions和Desktop Assembly数据集上评估了我们的分割方法和无监督学习流程，取得了最先进的结果。

    arXiv:2404.01518v1 Announce Type: cross  Abstract: We propose a novel approach to the action segmentation task for long, untrimmed videos, based on solving an optimal transport problem. By encoding a temporal consistency prior into a Gromov-Wasserstein problem, we are able to decode a temporally consistent segmentation from a noisy affinity/matching cost matrix between video frames and action classes. Unlike previous approaches, our method does not require knowing the action order for a video to attain temporal consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can be efficiently solved on GPUs using a few iterations of projected mirror descent. We demonstrate the effectiveness of our method in an unsupervised learning setting, where our method is used to generate pseudo-labels for self-training. We evaluate our segmentation approach and unsupervised learning pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly datasets, yielding state
    
[^9]: 面向工业和社会的元机器人：愿景、技术和机遇

    Metarobotics for Industry and Society: Vision, Technologies, and Opportunities

    [https://arxiv.org/abs/2404.00797](https://arxiv.org/abs/2404.00797)

    Metarobotics旨在通过结合无线通信、多感官沉浸和集体智能，为远程机器人应用提供普遍、流动和非侵入式的访问和互动，有望为工业和社会带来诸多益处。

    

    Metarobotics旨在将下一代无线通信，多感官沉浸和集体智能相结合，提供对远程机器人应用的普遍、流动和非侵入式访问和互动。工业和社会有望从这些功能中受益。本文描述了Metarobotics在社会、工业和两者之间的目标。它确定并调查了可能实现这些目标的技术，并提供了一个架构来推进Metarobotics关键组件的相互作用。

    arXiv:2404.00797v1 Announce Type: cross  Abstract: Metarobotics aims to combine next generation wireless communication, multi-sense immersion, and collective intelligence to provide a pervasive, itinerant, and non-invasive access and interaction with distant robotized applications. Industry and society are expected to benefit from these functionalities. For instance, robot programmers will no longer travel worldwide to plan and test robot motions, even collaboratively. Instead, they will have a personalized access to robots and their environments from anywhere, thus spending more time with family and friends. Students enrolled in robotics courses will be taught under authentic industrial conditions in real-time. This paper describes objectives of Metarobotics in society, industry, and in-between. It identifies and surveys technologies likely to enable their completion and provides an architecture to put forward the interplay of key components of Metarobotics. Potentials for self-determ
    
[^10]: CHAIN：通过受限唯一性连续性规范化增强数据高效GANs的泛化能力

    CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization

    [https://arxiv.org/abs/2404.00521](https://arxiv.org/abs/2404.00521)

    通过引入CHAIN，该方法在数据有限的情况下，解决了GANs中鉴别器过拟合和训练不稳定的问题，提高了泛化能力和训练稳定性。

    

    生成对抗网络（GANs）显着推动了图像生成，但它们的性能严重依赖大量的训练数据。在数据有限的情况下，GANs经常面临鉴别器过拟合和训练不稳定的问题。我们的工作通过识别Batch Normalization（BN）中的关键缺陷来解决这一问题：在中心化和缩放步骤中梯度爆炸的倾向。为了解决这个问题，我们提出了CHAIN（受限唯一性连续性规范化），它将传统的中心化步骤替换为零均值正则化，并在缩放步骤中集成了Lipschitz连续性约束。CHAIN通过自适应插值归一化和非归一化特征进一步增强了GANs的训练，有效避免了鉴别器过拟合。

    arXiv:2404.00521v1 Announce Type: new  Abstract: Generative Adversarial Networks (GANs) significantly advanced image generation but their performance heavily depends on abundant training data. In scenarios with limited data, GANs often struggle with discriminator overfitting and unstable training. Batch Normalization (BN), despite being known for enhancing generalization and training stability, has rarely been used in the discriminator of Data-Efficient GANs. Our work addresses this gap by identifying a critical flaw in BN: the tendency for gradient explosion during the centering and scaling steps. To tackle this issue, we present CHAIN (lipsCHitz continuity constrAIned Normalization), which replaces the conventional centering step with zero-mean regularization and integrates a Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN training by adaptively interpolating the normalized and unnormalized features, effectively avoiding discriminator overfitting. Our 
    
[^11]: 用于斯拉夫语的跨语言命名实体语料库

    Cross-lingual Named Entity Corpus for Slavic Languages

    [https://arxiv.org/abs/2404.00482](https://arxiv.org/abs/2404.00482)

    介绍了一个手动标注的用于六种斯拉夫语言的命名实体语料库，提供了两个训练数据集划分，并使用预训练的多语言模型进行命名实体的识别、分类、引用词化和链接。

    

    本文介绍了一个手动注释的包含六种斯拉夫语言（保加利亚语、捷克语、波兰语、斯洛文尼亚语、俄语和乌克兰语）命名实体的语料库。这项工作是2017-2023年间斯拉夫自然语言处理研讨会的一系列共享任务的结果。该语料库包含了5017份涵盖七个主题的文档，文档标有五类命名实体，每个实体由类别、引用词和唯一跨语言标识符描述。我们提供了两个训练调整的数据集划分 - 单个主题划分和跨主题划分。对于每个划分，我们使用基于transformer的神经网络架构设置了基准，使用预训练的多语言模型XLM-RoBERTa-large进行命名实体提及识别和分类，以及mT5-large进行命名实体引用词化和链接。

    arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
    
[^12]: 使用倒置标签的后门方法：脏标签翻转攻击

    A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks

    [https://arxiv.org/abs/2404.00076](https://arxiv.org/abs/2404.00076)

    提出了一种后门攻击方法，名为“DirtyFlipping”，利用脏标签技术在选定的数据模式中输入触发器，从而实现隐蔽的后门。

    

    基于声音的机器学习系统经常使用公共或第三方数据，这可能是不准确的。这使得训练在这些数据上的深度神经网络（DNN）模型容易受到潜在的数据毒化攻击。在这种攻击类型中，攻击者可以使用毒化数据来训练DNN模型，可能会降低其性能。另一种对我们的研究非常相关的数据毒化攻击类型是标签翻转，攻击者在其中操纵数据子集的标签。已经证明，即使是能力有限的攻击者，这些攻击也可能极大地降低系统性能。在本研究中，我们提出了一种名为“DirtyFlipping”的后门攻击，使用脏标签技术，“标签对标签”，在与目标类别相关的选定数据模式中输入触发器（拍手），从而实现了隐蔽的后门。

    arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
    
[^13]: FABind+: 通过改进口袋预测和姿态生成增强分子对接

    FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation

    [https://arxiv.org/abs/2403.20261](https://arxiv.org/abs/2403.20261)

    FABind+通过改进口袋预测和姿态生成，提升分子对接表现

    

    分子对接是药物发现中至关重要的过程。传统技术依赖于受物理原理支配的广泛采样和模拟，但这些方法往往速度慢且昂贵。基于深度学习的方法的出现显示出显著的前景，提供了精确性和效率的增长。建立在FABind的基础工作之上，这是一个专注于速度和准确性的模型，我们提出了FABind+，这是一个大大提升其前身性能的增强版。我们确定口袋预测是分子对接中的一个关键瓶颈，并提出了一种显著改进口袋预测的新方法，从而简化了对接过程。此外，我们对对接模块进行了修改，以增强其姿态生成能力。为了缩小与传统采样/生成方法之间的差距，我们结合了一个简单而有效的s

    arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
    
[^14]: 发挥大型语言模型在数据科学中预测表格任务的潜力

    Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science

    [https://arxiv.org/abs/2403.20208](https://arxiv.org/abs/2403.20208)

    本研究旨在利用大型语言模型解决数据科学中表格数据预测任务，通过在丰富的数据集上训练Llama-2模型并进行实际应用，取得显著的改进。

    

    在数据科学领域，分类、回归和缺失值填充等预测任务是与表格数据相关的常见挑战。这项研究旨在应用大型语言模型(LLMs)来解决这些预测任务。尽管LLMs擅长理解自然语言，但在处理结构化表格数据方面表现不佳。我们的研究旨在通过收集带有指令注释的表格语料库，并在这一丰富的数据集上对Llama-2进行大规模训练，以弥合这一差距。此外，我们研究了将训练模型应用于零-shot预测、少-shot预测和上下文学习场景的实际应用。通过广泛实验，我们的方法论显示了显著的改进。

    arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
    
[^15]: TFB：面向时间序列预测方法全面且公平的基准比较

    TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods

    [https://arxiv.org/abs/2403.20150](https://arxiv.org/abs/2403.20150)

    TFB通过解决数据领域覆盖不足、对传统方法的刻板印象以及不一致、不灵活的流程等问题，推动了时间序列预测方法基准比较的最新技术发展。

    

    时间序列会在经济、交通、健康和能源等不同领域中产生，对未来数值的预测在许多重要应用中起着关键作用。不出所料，许多预测方法被提出。为了确保进展，有必要能够以全面且可靠的方式经验性地研究和比较这些方法。为了实现这一目标，我们提出了TFB，一个自动化的时间序列预测（TSF）方法基准测试。TFB通过解决与数据集、比较方法和评估管道相关的缺点，推动了最新技术的发展：1）数据领域覆盖不足，2）对传统方法的刻板印象，3）不一致和不灵活的流程。为了获得更好的领域覆盖率，我们包括了来自10个不同领域的数据集：交通、电力、能源、环境、自然、经济、股票市场、银行、健康和网络。我们还提供了一个时间序列特性

    arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
    
[^16]: FairRAG: 公平人类生成的公平检索增强

    FairRAG: Fair Human Generation via Fair Retrieval Augmentation

    [https://arxiv.org/abs/2403.19964](https://arxiv.org/abs/2403.19964)

    FairRAG框架通过在外部图像数据库检索到的参考图像来提高人类生成中的公平性，并应用简单但有效的去偏策略，从而为生成过程提供来自不同人口统计组的图像。

    

    存在的文本到图像生成模型反映甚至放大了其训练数据中根深蒂固的社会偏见。这对人类图像生成尤为令人担忧，因为模型偏向某些人口统计组。现有的纠正此问题的尝试受到预训练模型固有限制的影响，并未能在根本上改善人口多样性。在这项工作中，我们引入了公平检索增强生成（FairRAG），这是一个新颖的框架，通过在来自外部图像数据库的参考图像上进行条件化来提高人类生成中的公平性。FairRAG通过一个轻量级线性模块实现条件化，将参考图像投射到文本空间中。为了增强公平性，FairRAG应用了简单而有效的去偏方法，在生成过程中提供来自不同人口统计组的图像。大量实验展示

    arXiv:2403.19964v1 Announce Type: cross  Abstract: Existing text-to-image generative models reflect or even amplify societal biases ingrained in their training data. This is especially concerning for human image generation where models are biased against certain demographic groups. Existing attempts to rectify this issue are hindered by the inherent limitations of the pre-trained models and fail to substantially improve demographic diversity. In this work, we introduce Fair Retrieval Augmented Generation (FairRAG), a novel framework that conditions pre-trained generative models on reference images retrieved from an external image database to improve fairness in human generation. FairRAG enables conditioning through a lightweight linear module that projects reference images into the textual space. To enhance fairness, FairRAG applies simple-yet-effective debiasing strategies, providing images from diverse demographic groups during the generative process. Extensive experiments demonstrat
    
[^17]: 理解人类反馈对齐学习动态的研究

    Understanding the Learning Dynamics of Alignment with Human Feedback

    [https://arxiv.org/abs/2403.18742](https://arxiv.org/abs/2403.18742)

    本研究对人类偏好对齐的学习动态进行了理论分析，显示了偏好数据集的分布如何影响模型更新速度，并提供了对训练准确度的严格保证，同时揭示了优化易于优先考虑高偏好可区分性行为的复杂现象。

    

    大型语言模型（LLMs）与人类意图对齐已成为安全部署模型在实际系统中的关键任务。现有的对齐方法虽然在经验上取得了成功，但理论上了解这些方法如何影响模型行为仍然是一个悬而未决的问题。我们的工作首次尝试在理论上分析人类偏好对齐的学习动态。我们正式展示了偏好数据集的分布如何影响模型更新速度，并对训练准确度提供了严格的保证。我们的理论还揭示了一个复杂现象，即优化易于优先考虑具有更高偏好可区分性的行为。我们在当代LLMs和对齐任务上在实证上验证了我们的发现，强化了我们的理论见解，并为未来的对齐方法提供了启示。免责声明：本文包含有效

    arXiv:2403.18742v1 Announce Type: cross  Abstract: Aligning large language models (LLMs) with human intentions has become a critical task for safely deploying models in real-world systems. While existing alignment approaches have seen empirical success, theoretically understanding how these methods affect model behavior remains an open question. Our work provides an initial attempt to theoretically analyze the learning dynamics of human preference alignment. We formally show how the distribution of preference datasets influences the rate of model updates and provide rigorous guarantees on the training accuracy. Our theory also reveals an intricate phenomenon where the optimization is prone to prioritizing certain behaviors with higher preference distinguishability. We empirically validate our findings on contemporary LLMs and alignment tasks, reinforcing our theoretical insights and shedding light on considerations for future alignment approaches. Disclaimer: This paper contains potent
    
[^18]: 压缩多任务嵌入用于地球观测中数据高效下游训练和推断

    Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation

    [https://arxiv.org/abs/2403.17886](https://arxiv.org/abs/2403.17886)

    基于神经嵌入压缩的多任务嵌入方法在地球观测中实现了数据高效的训练和推断，通过压缩率与嵌入效用之间的权衡，取得了数据量显著减少的准确性。

    

    随着地球观测（EO）中大规模数据的存储库增长，模型训练和推断的转移和存储成本也在增加，消耗了大量资源。我们引入了基于神经嵌入压缩（NEC）的方法，该方法基于对数据使用者传输压缩的嵌入而不是原始数据。我们通过学习神经压缩来调整基础模型（FM），生成多任务嵌入，同时在压缩率和嵌入效用之间进行权衡。我们仅针对FM参数的一小部分（10%）进行更新，进行短时间训练（预训练迭代的1%）。我们在两个EO任务上评估了NEC：场景分类和语义分割。与将传统压缩应用于原始数据相比，NEC在减少数据量方面可实现类似的准确性，降低了75%到90%的数据量。即使在99.7%的压缩下，在场景分类任务上性能仅下降了5%。总体而言，NEC是一种数据高效

    arXiv:2403.17886v1 Announce Type: new  Abstract: As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient
    
[^19]: LASIL：学习者感知的长期微观交通仿真监督模仿学习

    LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation

    [https://arxiv.org/abs/2403.17601](https://arxiv.org/abs/2403.17601)

    提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题

    

    微观交通仿真在交通工程中起着至关重要的作用，通过提供关于单个车辆行为和整体交通流的见解。然而，创建一个真实的模拟器，精确复制各种交通条件下的人类驾驶行为，面临着重大挑战。传统的依赖启发式模型的模拟器往往由于现实世界交通环境的复杂性而无法提供准确的模拟。由于协变量偏移问题，现有的基于模仿学习的模拟器经常无法生成稳定的长期模拟。在本文中，我们提出了一种称为学习者感知的监督模仿学习的新方法，以解决多智体模仿学习中的协变量偏移问题。通过利用变分自动编码器同时建模专家和学习者状态分布，我们的方法增强了专家状态，从而使增强状态意识到

    arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
    
[^20]: 理解马尔科夫逻辑网络中的域大小泛化

    Understanding Domain-Size Generalization in Markov Logic Networks

    [https://arxiv.org/abs/2403.15933](https://arxiv.org/abs/2403.15933)

    本文量化了马尔科夫逻辑网络在不同大小领域间内部一致性缺失的问题，并提出最大化数据对数似然同时最小化参数方差的方式来优化领域大小泛化。

    

    我们研究了马尔科夫逻辑网络（MLNs）在不同大小的关系结构之间的泛化行为。多个研究注意到，在给定域上学习的MLNs在不同大小的域上泛化很差。这种行为源于MLN在不同域大小上使用时的内部一致性缺失。在本文中，我们量化了这种不一致性，并将其限制在MLN参数的方差范围内。参数方差还限制了从不同域大小中取出的MLN边缘分布之间的KL散度。我们利用这些界限展示，最大化数据对数似然同时最小化参数方差，对应于域大小泛化的两个自然概念。我们的理论结果适用于指数随机图和其他基于马尔科夫网络的关系模型。最后，我们观察到已知的解决方案会减少方差

    arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
    
[^21]: 利用Transformer技术增强物联网应用的自动调制识别

    Enhancing Automatic Modulation Recognition for IoT Applications Using Transformers

    [https://arxiv.org/abs/2403.15417](https://arxiv.org/abs/2403.15417)

    使用Transformer网络提出了一种高效的自动调制识别方法，在物联网环境中具有最佳的识别准确率。

    

    自动调制识别(AMR)对于确定传入信号的调制类型至关重要。结合先进的深度学习方法能够实现快速处理和最小资源使用，这对物联网应用至关重要。我们引入了一种使用Transformer网络的新颖方法，专门设计用于解决物联网环境中普遍存在的模型大小限制。我们进行了大量实验，结果显示我们提出的方法优于先进的深度学习技术，实现了最高的识别准确率。

    arXiv:2403.15417v1 Announce Type: cross  Abstract: Automatic modulation recognition (AMR) is critical for determining the modulation type of incoming signals. Integrating advanced deep learning approaches enables rapid processing and minimal resource usage, essential for IoT applications. We have introduced a novel method using Transformer networks for efficient AMR, designed specifically to address the constraints on model size prevalent in IoT environments. Our extensive experiments reveal that our proposed method outperformed advanced deep learning techniques, achieving the highest recognition accuracy.
    
[^22]: 从核方法的角度对两层神经网络进行平均场分析

    Mean-field Analysis on Two-layer Neural Networks from a Kernel Perspective

    [https://arxiv.org/abs/2403.14917](https://arxiv.org/abs/2403.14917)

    本文通过核方法的视角研究了两层神经网络在平均场极限下的特征学习能力，展示了它们比任何核方法更有效地学习多个再现核希尔伯特空间的并集，并且神经网络会获得与目标函数对齐的数据相关核。

    

    在本文中，我们通过核方法的视角研究了两层神经网络在平均场极限下的特征学习能力。为了聚焦于第一层诱导的核的动态，我们利用了两个时间尺度的极限，其中第二层比第一层移动得快得多。在这个极限下，学习问题被简化为在内在核上的最小化问题。然后，我们展示了平均场 Langevin 动力学的全局收敛性，并推导了时间和粒子离散化误差。我们还证明了两层神经网络可以比任何核方法更有效地学习多个再现核希尔伯特空间的并集，并且神经网络会获得与目标函数对齐的数据相关核。此外，我们还开发了一个收敛到全局最优的标签噪声过程，并展示自由度出现作为一种隐式正则化。

    arXiv:2403.14917v1 Announce Type: new  Abstract: In this paper, we study the feature learning ability of two-layer neural networks in the mean-field regime through the lens of kernel methods. To focus on the dynamics of the kernel induced by the first layer, we utilize a two-timescale limit, where the second layer moves much faster than the first layer. In this limit, the learning problem is reduced to the minimization problem over the intrinsic kernel. Then, we show the global convergence of the mean-field Langevin dynamics and derive time and particle discretization error. We also demonstrate that two-layer neural networks can learn a union of multiple reproducing kernel Hilbert spaces more efficiently than any kernel methods, and neural networks acquire data-dependent kernel which aligns with the target function. In addition, we develop a label noise procedure, which converges to the global optimum and show that the degrees of freedom appears as an implicit regularization.
    
[^23]: 关于自监督学习的预训练数据多样性

    On Pretraining Data Diversity for Self-Supervised Learning

    [https://arxiv.org/abs/2403.13808](https://arxiv.org/abs/2403.13808)

    增加预训练数据多样性可以提高自监督学习性能，但仅在与下游数据的分布距离较小时有效。

    

    我们探讨了使用更多样化数据集对自监督学习(SSL)性能的影响，这些数据集的特征是唯一样本数量，在固定的计算预算下。我们的研究结果一致表明，增加预训练数据的多样性可以提高SSL性能，尽管只有当与下游数据的分布距离很小的时候才是如此。值得注意的是，即使通过网络爬虫或扩散生成的数据等方式实现了异常大的预训练数据多样性，分布转移仍然是一个挑战。我们的实验涵盖了七种SSL方法，使用了诸如ImageNet和YFCC100M等大规模数据集，总计超过200个GPU天。代码和训练模型将在https://github.com/hammoudhasan/DiversitySSL 上提供。

    arXiv:2403.13808v1 Announce Type: cross  Abstract: We explore the impact of training with more diverse datasets, characterized by the number of unique samples, on the performance of self-supervised learning (SSL) under a fixed computational budget. Our findings consistently demonstrate that increasing pretraining data diversity enhances SSL performance, albeit only when the distribution distance to the downstream data is minimal. Notably, even with an exceptionally large pretraining data diversity achieved through methods like web crawling or diffusion-generated data, among other ways, the distribution shift remains a challenge. Our experiments are comprehensive with seven SSL methods using large-scale datasets such as ImageNet and YFCC100M amounting to over 200 GPU days. Code and trained models will be available at https://github.com/hammoudhasan/DiversitySSL .
    
[^24]: 机器人运动规划的联邦强化学习与零次通用化

    Federated reinforcement learning for robot motion planning with zero-shot generalization

    [https://arxiv.org/abs/2403.13245](https://arxiv.org/abs/2403.13245)

    该论文提出了一个联邦强化学习框架，实现了机器人运动规划中的零次通用化，通过协作学习多个学习者和中央服务器，在不共享原始数据的情况下达到全局最优解。

    

    本文考虑了使用零次通用化学习控制策略进行机器人运动规划的问题，即在部署学习策略到新环境时不需要数据收集和策略调整。我们开发了一个联邦强化学习框架，实现了多个学习者和中央服务器（云端）的协作学习，而不分享原始数据。在每次迭代中，每个学习者将其本地控制策略和相应的估计归一化到达时间上传至云端，然后云端在学习者间计算全局最优并将最优策略广播给学习者。每个学习者然后在下一次迭代中从其本地控制策略和云端中选择。提出的框架利用了到达时间和安全性的零次通用化保证。对于几乎必然收敛，几乎一致性，Pare的理论保证//}

    arXiv:2403.13245v1 Announce Type: cross  Abstract: This paper considers the problem of learning a control policy for robot motion planning with zero-shot generalization, i.e., no data collection and policy adaptation is needed when the learned policy is deployed in new environments. We develop a federated reinforcement learning framework that enables collaborative learning of multiple learners and a central server, i.e., the Cloud, without sharing their raw data. In each iteration, each learner uploads its local control policy and the corresponding estimated normalized arrival time to the Cloud, which then computes the global optimum among the learners and broadcasts the optimal policy to the learners. Each learner then selects between its local control policy and that from the Cloud for next iteration. The proposed framework leverages on the derived zero-shot generalization guarantees on arrival time and safety. Theoretical guarantees on almost-sure convergence, almost consensus, Pare
    
[^25]: 具有非私密预处理的可证明隐私

    Provable Privacy with Non-Private Pre-Processing

    [https://arxiv.org/abs/2403.13041](https://arxiv.org/abs/2403.13041)

    提出了一个框架，能够评估非私密数据相关预处理算法引起的额外隐私成本，并利用平滑DP和预处理算法的有界敏感性建立整体隐私保证的上限

    

    当分析差分私密（DP）机器学习管道时，通常会忽略数据相关的预处理的潜在隐私成本。在这项工作中，我们提出了一个通用框架，用于评估由非私密数据相关预处理算法引起的额外隐私成本。我们的框架通过利用两个新的技术概念建立了整体隐私保证的上限：一种称为平滑DP的DP变体以及预处理算法的有界敏感性。

    arXiv:2403.13041v1 Announce Type: cross  Abstract: When analysing Differentially Private (DP) machine learning pipelines, the potential privacy cost of data-dependent pre-processing is frequently overlooked in privacy accounting. In this work, we propose a general framework to evaluate the additional privacy cost incurred by non-private data-dependent pre-processing algorithms. Our framework establishes upper bounds on the overall privacy guarantees by utilising two new technical notions: a variant of DP termed Smooth DP and the bounded sensitivity of the pre-processing algorithms. In addition to the generic framework, we provide explicit overall privacy guarantees for multiple data-dependent pre-processing algorithms, such as data imputation, quantization, deduplication and PCA, when used in combination with several DP algorithms. Notably, this framework is also simple to implement, allowing direct integration into existing DP pipelines.
    
[^26]: 预排序Tsetlin机器（基因K-Medoid方法）

    Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)

    [https://arxiv.org/abs/2403.09680](https://arxiv.org/abs/2403.09680)

    该论文提出了一种利用Tsetlin Machines进行传统监督学习的机器学习预排序阶段方法，在MNIST级别的分类问题上取得了显著的精度提升，以及训练时间和推理时间大幅度减少。

    

    本文提出了一种利用Tsetlin Machines进行传统监督学习的机器学习预排序阶段。首先，利用快速遗传算法从数据集中确定N个数据点，以解决最大离散化问题。然后，这些被用作运行K-Medoid聚类算法的初始放置。最后，利用快速遗传算法通过最大化汉明距离来对齐N个独立的Tsetlin Machines。对于MNIST级别的分类问题，结果显示准确度提高了高达10％，训练时间减少了约383倍，推理时间减少了约86倍。

    arXiv:2403.09680v1 Announce Type: cross  Abstract: This paper proposes a machine learning pre-sort stage to traditional supervised learning using Tsetlin Machines. Initially, N data-points are identified from the dataset using an expedited genetic algorithm to solve the maximum dispersion problem. These are then used as the initial placement to run the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is used to align N independent Tsetlin Machines by maximising hamming distance. For MNIST level classification problems, results demonstrate up to 10% improvement in accuracy, approx. 383X reduction in training time and approx. 86X reduction in inference time.
    
[^27]: 利用典型表示减轻社会偏见而不使用人口统计信息

    Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information

    [https://arxiv.org/abs/2403.09516](https://arxiv.org/abs/2403.09516)

    通过利用预定义的典型人口统计文本并在微调过程中加入正则化项，本文提出的方法有效减轻了语言模型中的社会偏见，同时不需要依赖显式的人口统计标签。

    

    减轻社会偏见通常需要识别与每个数据样本相关联的社会群体。在本文中，我们提出了DAFair，一种新颖的方法来解决语言模型中的社会偏见问题。与依赖显式人口统计标签的传统方法不同，我们的方法不需要任何此类信息。相反，我们利用预定义的人口统计典型文本，并在微调过程中加入一个正则化项来减轻模型表示中的偏见。我们在两个任务和两个模型上的实证结果展示了我们的方法相对于之前不依赖标记数据的方法的有效性。此外，即使使用有限的人口统计标注数据，我们的方法也优于常见的去偏见方法。

    arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
    
[^28]: Keyformer：通过关键标记选择减少KV缓存以实现高效的生成推断

    Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference

    [https://arxiv.org/abs/2403.09054](https://arxiv.org/abs/2403.09054)

    本文提出了一种名为“Keyformer”的创新推断时间方法，旨在通过选择关键标记来减少KV缓存的挑战，提高内存带宽利用率。

    

    Transformer已经成为大型语言模型(LLMs)的基础架构。在生成语言模型中，推断过程涉及两个主要阶段：提示处理和标记生成。标记生成，构成了大部分计算工作量，主要涉及向量-矩阵乘法和与键-值(KV)缓存交互。由于从存储系统传输权重和KV缓存值到计算单元的开销，这一阶段受到内存带宽的限制。这种内存瓶颈在需要长上下文和大量文本生成的应用中尤为突出，这两者对LLMs越来越重要。  本文介绍了一种创新的推断时间方法“Keyformer”，以缓解与KV缓存大小和内存带宽利用相关的挑战。Keyformer利用了这样的观察结果，大约90

    arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
    
[^29]: 通过代码探索大型语言模型的安全泛化挑战

    Exploring Safety Generalization Challenges of Large Language Models via Code

    [https://arxiv.org/abs/2403.07865](https://arxiv.org/abs/2403.07865)

    本论文引入了CodeAttack框架用于测试大型语言模型的安全泛化，研究发现GPT-4、Claude-2和Llama-2系列等最新模型存在代码输入的安全漏洞。

    

    大型语言模型（LLMs）的快速发展带来了自然语言处理方面的显著能力，但也引发了人们对它们潜在误用的担忧。本文引入了CodeAttack，一个将自然语言输入转换为代码输入的框架，为测试LLMs的安全泛化提供了一个新颖的环境。我们对包括GPT-4、Claude-2和Llama-2系列在内的最新LLMs进行了全面研究，发现这些模型对于代码输入存在共同的安全漏洞：CodeAttack在超过80%的时间内始终绕过所有模型的安全保护。

    arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
    
[^30]: 限制贝叶斯神经网络

    Restricted Bayesian Neural Network

    [https://arxiv.org/abs/2403.04810](https://arxiv.org/abs/2403.04810)

    本研究提出了限制贝叶斯神经网络的新架构，显著减少了网络存储空间复杂性，并引入了一种能够有效处理不确定性的算法，确保在目标函数缺乏完美凸性时稳健地收敛至全局最优解。

    

    现代深度学习工具在解决复杂问题方面非常有效。然而，它们作为黑盒模型的运行方式增加了预测的不确定性。此外，它们面临着各种挑战，包括在大型网络中需要大量存储空间、过拟合、欠拟合、梯度消失等问题。本研究探讨了贝叶斯神经网络的概念，提出了一种能够显著减少网络存储空间复杂性的新型架构。此外，我们介绍了一种能够有效处理不确定性的算法，确保稳健的收敛值，避免陷入局部最优解，尤其是当目标函数缺乏完美的凸性时。

    arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
    
[^31]: 3D扩散策略

    3D Diffusion Policy

    [https://arxiv.org/abs/2403.03954](https://arxiv.org/abs/2403.03954)

    3D扩散策略（DP3）是一种新颖的视觉模仿学习方法，通过将3D视觉表示的强大性结合到扩散策略中，成功解决了学习复杂技能所需大量人类演示的问题。

    

    模仿学习为教授机器人灵巧技能提供了一种高效的方式；然而，学习复杂而具有通用性的技能通常需要大量的人类演示。为了解决这一具有挑战性的问题，我们提出了3D扩散策略（DP3），这是一种将3D视觉表示的强大性融入到扩散策略中的新颖视觉模仿学习方法，扩散策略是一类有条件的动作生成模型。DP3的核心设计是利用一个紧凑的3D视觉表示，该表示是从稀疏点云中提取出来的，使用高效的点编码器。在我们涵盖了72个仿真任务的实验中，DP3仅需要10个演示就可以成功处理大多数任务，并且比基线模型提高了55.3%。在4个真实机器人任务中，DP3表现出了高成功率的精确控制，每项任务仅需40次演示即可成功率为85%，在不同领域展现了出色的泛化能力。

    arXiv:2403.03954v1 Announce Type: cross  Abstract: Imitation learning provides an efficient way to teach robots dexterous skills; however, learning complex skills robustly and generalizablely usually consumes large amounts of human demonstrations. To tackle this challenging problem, we present 3D Diffusion Policy (DP3), a novel visual imitation learning approach that incorporates the power of 3D visual representations into diffusion policies, a class of conditional action generative models. The core design of DP3 is the utilization of a compact 3D visual representation, extracted from sparse point clouds with an efficient point encoder. In our experiments involving 72 simulation tasks, DP3 successfully handles most tasks with just 10 demonstrations and surpasses baselines with a 55.3% relative improvement. In 4 real robot tasks, DP3 demonstrates precise control with a high success rate of 85%, given only 40 demonstrations of each task, and shows excellent generalization abilities in di
    
[^32]: 少即是多：面向可扩展和通用学习的跳数图注意力在电路上的应用

    Less is More: Hop-Wise Graph Attention for Scalable and Generalizable Learning on Circuits

    [https://arxiv.org/abs/2403.01317](https://arxiv.org/abs/2403.01317)

    提出了一种名为HOGA的基于注意力的模型，能够在电路中以可扩展和通用的方式学习电路表示，通过跳数特征和门控自注意力模块的方式，实现了对不同电路结构的自适应学习，并可以进行高效的分布式训练。

    

    虽然图神经网络（GNNs）在各种电子设计自动化（EDA）任务中学习电路表示方面变得流行，但当应用于大图时，它们面临可扩展性挑战，并且对新设计的泛化能力有限。这些限制使它们在解决大规模复杂电路问题时不太实用。在这项工作中，我们提出了HOGA，一种新颖的基于注意力的模型，用于以可扩展和通用的方式学习电路表示。HOGA首先在模型训练之前针对每个节点计算跳数特征。随后，跳数特征仅用于通过门控自注意力模块生成节点表示，该模块自适应地学习不同跳数之间的重要特征，而不涉及图拓扑。因此，HOGA能够适应不同电路之间的各种结构，并可以以分布式的方式高效训练。

    arXiv:2403.01317v1 Announce Type: new  Abstract: While graph neural networks (GNNs) have gained popularity for learning circuit representations in various electronic design automation (EDA) tasks, they face challenges in scalability when applied to large graphs and exhibit limited generalizability to new designs. These limitations make them less practical for addressing large-scale, complex circuit problems. In this work we propose HOGA, a novel attention-based model for learning circuit representations in a scalable and generalizable manner. HOGA first computes hop-wise features per node prior to model training. Subsequently, the hop-wise features are solely used to produce node representations through a gated self-attention module, which adaptively learns important features among different hops without involving the graph topology. As a result, HOGA is adaptive to various structures across different circuits and can be efficiently trained in a distributed manner. To demonstrate the e
    
[^33]: Polynormer: 多项式表达的线性时间图转换器

    Polynormer: Polynomial-Expressive Graph Transformer in Linear Time

    [https://arxiv.org/abs/2403.01232](https://arxiv.org/abs/2403.01232)

    Polynormer提出了一种多项式表达GT模型，具有线性复杂度，结合本地和全局等变注意力模型，平衡了表现力和可扩展性。

    

    图转换器（GTs）已经成为一种有前途的架构，理论上它比消息传递图神经网络（GNNs）更具表现力。然而，典型的GT模型至少具有二次复杂度，因此无法扩展到大型图。虽然最近提出了几种线性GTs，但它们在几个热门图数据集上仍落后于GNN对应模型，这对于它们的实际表现力构成了一个重要关注点。为了平衡GTs的表现力和可扩展性之间的权衡，我们提出了Polynormer，一个具有线性复杂度的多项式表达GT模型。Polynormer构建在一个新颖的基础模型上，该模型在输入特征上学习高次多项式。为了使基础模型具有置换等变性，我们将其与图拓扑和节点特征分开集成，从而产生本地和全局等变关注模型。因此，Polynormer采用了线性的局部到全局关注方案。

    arXiv:2403.01232v1 Announce Type: cross  Abstract: Graph transformers (GTs) have emerged as a promising architecture that is theoretically more expressive than message-passing graph neural networks (GNNs). However, typical GT models have at least quadratic complexity and thus cannot scale to large graphs. While there are several linear GTs recently proposed, they still lag behind GNN counterparts on several popular graph datasets, which poses a critical concern on their practical expressivity. To balance the trade-off between expressivity and scalability of GTs, we propose Polynormer, a polynomial-expressive GT model with linear complexity. Polynormer is built upon a novel base model that learns a high-degree polynomial on input features. To enable the base model permutation equivariant, we integrate it with graph topology and node features separately, resulting in local and global equivariant attention models. Consequently, Polynormer adopts a linear local-to-global attention scheme t
    
[^34]: 路线推荐综述：方法、应用和机会

    A Survey of Route Recommendations: Methods, Applications, and Opportunities

    [https://arxiv.org/abs/2403.00284](https://arxiv.org/abs/2403.00284)

    基于城市计算的路线推荐综述对路线推荐研究中的传统机器学习和现代深度学习方法进行了分类，展示了与城市计算场景相关的新应用，并揭示了最新进展。

    

    现今，随着先进的信息技术部署在整个城市，大量数据和强大的计算资源正在使现代城市发展智能化。作为智能交通的重要组成部分，路线推荐及其应用被广泛使用，直接影响市民的出行习惯。基于大数据（可能是多模式）开发智能高效的出行路线已成为路线推荐研究的核心挑战。我们的综述对基于城市计算的路线推荐工作进行了全面回顾。它分为以下三个部分：1）方法论。我们对大量传统机器学习和现代深度学习方法进行分类。同时，我们讨论它们的历史关系并揭示最新进展。2）应用方面。我们展示了大量与城市计算场景中路线推荐相关的新应用。3）我们迪

    arXiv:2403.00284v1 Announce Type: new  Abstract: Nowadays, with advanced information technologies deployed citywide, large data volumes and powerful computational resources are intelligentizing modern city development. As an important part of intelligent transportation, route recommendation and its applications are widely used, directly influencing citizens` travel habits. Developing smart and efficient travel routes based on big data (possibly multi-modal) has become a central challenge in route recommendation research. Our survey offers a comprehensive review of route recommendation work based on urban computing. It is organized by the following three parts: 1) Methodology-wise. We categorize a large volume of traditional machine learning and modern deep learning methods. Also, we discuss their historical relations and reveal the edge-cutting progress. 2) Application\-wise. We present numerous novel applications related to route commendation within urban computing scenarios. 3) We di
    
[^35]: DS-Agent：通过赋予大型语言模型案例推理能力实现自动化数据科学

    DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning

    [https://arxiv.org/abs/2402.17453](https://arxiv.org/abs/2402.17453)

    DS-Agent是一个自动框架，结合了大型语言模型代理和案例推理，能够在数据科学任务中灵活利用专家知识并通过反馈机制持续改善性能

    

    在这项工作中，我们研究了基于大型语言模型（LLMs）代理的潜力，以自动化数据科学任务，目标是理解任务要求，然后构建和训练最合适的机器学习模型。尽管现有的LLM代理取得了广泛成功，但在这种情景下生成不合理的实验计划受到阻碍。为此，我们提出了DS-Agent，这是一个利用LLM代理和案例推理（CBR）的新颖自动化框架。在开发阶段，DS-Agent遵循CBR框架来构建自动迭代流水线，可以灵活利用来自Kaggle的专业知识，并通过反馈机制促进一致的性能改进。此外，DS-Agent实现了一个低资源部署阶段，采用简化的CBR范例来适应开发阶段成功解决方案，以进行直接代码生成，显著减少了...

    arXiv:2402.17453v1 Announce Type: new  Abstract: In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing th
    
[^36]: 对具有线性函数逼近的离策略多步TD学习算法的分析

    Analysis of Off-Policy Multi-Step TD-Learning with Linear Function Approximation

    [https://arxiv.org/abs/2402.15781](https://arxiv.org/abs/2402.15781)

    该论文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法，并证明了当采样时间跨度 n 足够大时这些算法会收敛到有意义的解。

    

    本文分析了在具有线性函数逼近、离策略学习和自举的“致命三连”场景中的多步TD学习算法。特别地，我们证明了当采样时间跨度 n 足够大时，n步TD学习算法会收敛到一个解。该论文分为两部分。第一部分全面研究了模型基础确定性算法的基本性质，包括投影值迭代、梯度下降算法和控制理论方法，它们可以被视为原型确定性算法，对于理解和发展无模型增强学习算法起着关键作用。特别地，我们证明了当 n 足够大时，这些算法会收敛到有意义的解。根据这些发现，提出并分析了两种n步TD学习算法。

    arXiv:2402.15781v1 Announce Type: cross  Abstract: This paper analyzes multi-step TD-learning algorithms within the `deadly triad' scenario, characterized by linear function approximation, off-policy learning, and bootstrapping. In particular, we prove that n-step TD-learning algorithms converge to a solution as the sampling horizon n increases sufficiently. The paper is divided into two parts. In the first part, we comprehensively examine the fundamental properties of their model-based deterministic counterparts, including projected value iteration, gradient descent algorithms, and the control theoretic approach, which can be viewed as prototype deterministic algorithms whose analysis plays a pivotal role in understanding and developing their model-free reinforcement learning counterparts. In particular, we prove that these algorithms converge to meaningful solutions when n is sufficiently large. Based on these findings, two n-step TD-learning algorithms are proposed and analyzed, whi
    
[^37]: 轨迹式迭代强化学习框架用于自动竞标

    Trajectory-wise Iterative Reinforcement Learning Framework for Auto-bidding

    [https://arxiv.org/abs/2402.15102](https://arxiv.org/abs/2402.15102)

    自动广告竞标中使用了一种新的迭代离线强化学习框架，有效缓解了传统RL算法在在线环境下性能下降的问题。

    

    在在线广告中，广告主参与广告竞拍以获取广告机会，通常是通过需求方平台(DSPs)提供的自动竞标工具。目前的自动竞标算法通常采用强化学习（RL）。然而，由于安全性问题，大多数基于RL的自动竞标策略是在模拟环境中进行训练的，在在线环境中部署会导致性能下降。为了缩小这一差距，我们可以并行部署多个自动竞标代理以收集大量交互数据集。然后，可以利用离线RL算法训练新策略。训练后的策略随后可以部署以进行进一步的数据收集，从而形成一个迭代训练框架，我们将其称为迭代离线RL。在这项工作中，我们确定了这种迭代离线RL框架的性能瓶颈，其根源在于由于内在原因而导致的探索和利用的低效问题。

    arXiv:2402.15102v1 Announce Type: cross  Abstract: In online advertising, advertisers participate in ad auctions to acquire ad opportunities, often by utilizing auto-bidding tools provided by demand-side platforms (DSPs). The current auto-bidding algorithms typically employ reinforcement learning (RL). However, due to safety concerns, most RL-based auto-bidding policies are trained in simulation, leading to a performance degradation when deployed in online environments. To narrow this gap, we can deploy multiple auto-bidding agents in parallel to collect a large interaction dataset. Offline RL algorithms can then be utilized to train a new policy. The trained policy can subsequently be deployed for further data collection, resulting in an iterative training framework, which we refer to as iterative offline RL. In this work, we identify the performance bottleneck of this iterative offline RL framework, which originates from the ineffective exploration and exploitation caused by the inhe
    
[^38]: IEPile: 挖掘大规模基于模式的信息抽取语料库

    IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus

    [https://arxiv.org/abs/2402.14710](https://arxiv.org/abs/2402.14710)

    发布了IEPile，一个包含约0.32B个标记的综合双语IE指令语料库，通过收集和清理33个现有IE数据集并引入基于模式的指令生成，可以提高大型语言模型在信息抽取领域的性能，尤其是零样本泛化。

    

    大型语言模型（LLMs）在各个领域展现出了显著的潜力；然而，在信息抽取（IE）方面表现出了显著的性能差距。高质量的指令数据是提升LLMs特定能力的关键，而当前的IE数据集往往规模较小、分散且缺乏标准化的模式。因此，我们介绍了IEPile，一个综合的双语（英文和中文）IE指令语料库，包含约0.32B个标记。我们通过收集和清理33个现有IE数据集构建IEPile，并引入基于模式的指令生成来挖掘大规模语料库。在LLaMA和Baichuan上的实验结果表明，使用IEPile可以提高LLMs在IE方面的性能，尤其是零样本泛化。我们开源了资源和预训练模型，希望为自然语言处理社区提供有价值的支持。

    arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
    
[^39]: 重塑RLHF中的信息结构：基于图论的奖励泛化视角

    Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective

    [https://arxiv.org/abs/2402.10184](https://arxiv.org/abs/2402.10184)

    本研究通过设计奖励建模过程中的数据集信息结构，从图论的视角提出了RLHF中奖励泛化的问题，以解决多样的环境、低成本标注和可靠的对齐性能间的不兼容性。

    

    在强化学习从人类反馈中（RLHF）存在一个三难问题：高度多样的环境、低标注成本和可靠的对齐性能之间的不兼容性。本文旨在通过设计奖励建模过程中的数据集信息结构来缓解这种不兼容性。具体而言，我们重新审视了RLHF过程，并提出了一个理论框架将其描绘为文本分布上的自动编码过程。我们的框架形式化了RLHF目标，即确保人类偏好与大型语言模型（LLM）行为之间的分布一致性。基于这个框架，我们系统地研究了RLHF奖励建模阶段中信息结构的性能影响。为了进一步理解奖励建模阶段中的奖励泛化，我们引入了一种基于随机图论的方法来建模语义空间中的泛化。其中的关键见解是...

    arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
    
[^40]: 调谐：在临床设置中使用有限数据的音频分类器性能分析

    Tuning In: Analysis of Audio Classifier Performance in Clinical Settings with Limited Data

    [https://arxiv.org/abs/2402.10100](https://arxiv.org/abs/2402.10100)

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，并发现在微调之前，预训练模型在大数据集上的性能对临床数据的影响较好。研究结果表明，CNN模型可以在小数据集环境中与转换模型相媲美或超越。

    

    本研究评估了在临床设置中使用深度学习模型进行音频分类的效果，限制条件是以反映实际世界数据收集的小数据集为基础。我们分析了包括DenseNet和ConvNeXt在内的CNN模型，以及ViT、SWIN和AST等转换模型，并将它们与诸如YAMNet和VGGish的预训练音频模型进行比较。我们的方法强调了在特定临床数据上微调之前，在大数据集上进行预训练的好处。我们从卒中患者中新收集了两个前所未有的患者音频数据集。我们研究了各种预处理技术，发现基于它们从预训练中学习到的先验知识，RGB和灰度谱图转换对模型性能产生了不同的影响。我们的研究结果表明，CNN模型在小数据集环境中可以与转换模型相媲美或超越，其中DenseNet-Contrastive和AST模型表现突出。本研究强调了...

    arXiv:2402.10100v1 Announce Type: cross  Abstract: This study assesses deep learning models for audio classification in a clinical setting with the constraint of small datasets reflecting real-world prospective data collection. We analyze CNNs, including DenseNet and ConvNeXt, alongside transformer models like ViT, SWIN, and AST, and compare them against pre-trained audio models such as YAMNet and VGGish. Our method highlights the benefits of pre-training on large datasets before fine-tuning on specific clinical data. We prospectively collected two first-of-their-kind patient audio datasets from stroke patients. We investigated various preprocessing techniques, finding that RGB and grayscale spectrogram transformations affect model performance differently based on the priors they learn from pre-training. Our findings indicate CNNs can match or exceed transformer models in small dataset contexts, with DenseNet-Contrastive and AST models showing notable performance. This study highlights
    
[^41]: 基于深度学习的智能反射表面辅助集成感知与通信系统的信道估计

    Deep-Learning Channel Estimation for IRS-Assisted Integrated Sensing and Communication System

    [https://arxiv.org/abs/2402.09441](https://arxiv.org/abs/2402.09441)

    本文针对智能反射表面辅助集成感知与通信系统中的信道估计问题，提出了一个基于深度学习的三阶段方法。该方法通过解耦问题，分别估计直接感知和通信信道、反射通信信道和反射感知信道，以应对智能反射表面的信号处理能力不足和感知与通信信号之间的互相干扰。

    

    本文首次关注智能反射表面辅助集成感知与通信系统中的信道估计问题。由于被动智能反射表面缺乏信号处理能力，并且在集成感知与通信系统中存在感知和通信信号之间的互相干扰，这个问题具有挑战性。我们提出了一个三阶段的方法来解决这个估计问题，其中包括在第一阶段估计直接感知和通信信道，在第二阶段估计反射通信信道，在第三阶段估计反射感知信道。所提出的三阶段方法基于一个深度学习框架，其中包括两种不同的卷积神经网络结构来进行信道估计。

    arXiv:2402.09441v1 Announce Type: cross  Abstract: Integrated sensing and communication (ISAC), and intelligent reflecting surface (IRS) are envisioned as revolutionary technologies to enhance spectral and energy efficiencies for next wireless system generations. For the first time, this paper focuses on the channel estimation problem in an IRS-assisted ISAC system. This problem is challenging due to the lack of signal processing capacity in passive IRS, as well as the presence of mutual interference between sensing and communication (SAC) signals in ISAC systems. A three-stage approach is proposed to decouple the estimation problem into sub-ones, including the estimation of the direct SAC channels in the first stage, reflected communication channel in the second stage, and reflected sensing channel in the third stage. The proposed three-stage approach is based on a deep-learning framework, which involves two different convolutional neural network (CNN) architectures to estimate the ch
    
[^42]: 基于极限学习机的智能反射面辅助多用户ISAC系统的信道估计

    Extreme Learning Machine-based Channel Estimation in IRS-Assisted Multi-User ISAC System

    [https://arxiv.org/abs/2402.09440](https://arxiv.org/abs/2402.09440)

    本文提出了一种基于极限学习机的智能反射面辅助多用户ISAC系统的信道估计方法，该方法通过将估计问题分解成子问题来解决了感知和通信信号干扰以及被动式IRS缺乏信号处理能力的挑战。该方法可以在保持低成本需求的情况下实现对SAC信道和下行通信信道的准确估计。

    

    最近，智能反射面（IRS）辅助的多用户集成感知和通信（ISAC）系统已经被研究以提供高频谱和能量有效性传输。本文首次提出了一个实用的信道估计方法，用于IRS辅助的多用户ISAC系统。在这样的系统中，估计问题具有挑战性，因为感知和通信（SAC）信号相互干扰，被动式的IRS缺乏信号处理能力。本文提出了一个两阶段的方法，将整体估计问题逐步转化为子问题，依次包括直接和反射信道的估计。基于此方案，ISAC基站（BS）估计与目标和上行用户相关的所有SAC信道，而每个下行用户单独估计下行通信信道。考虑到ISAC基站和下行用户的低成本需求，本文的方法具有实用性和可行性。

    arXiv:2402.09440v1 Announce Type: cross  Abstract: Multi-user integrated sensing and communication (ISAC) assisted by intelligent reflecting surface (IRS) has been recently investigated to provide a high spectral and energy efficiency transmission. This paper proposes a practical channel estimation approach for the first time to an IRS-assisted multiuser ISAC system. The estimation problem in such a system is challenging since the sensing and communication (SAC) signals interfere with each other, and the passive IRS lacks signal processing ability. A two-stage approach is proposed to transfer the overall estimation problem into sub-ones, successively including the direct and reflected channels estimation. Based on this scheme, the ISAC base station (BS) estimates all the SAC channels associated with the target and uplink users, while each downlink user estimates the downlink communication channels individually. Considering a low-cost demand of the ISAC BS and downlink users, the propos
    
[^43]: 基于深度学习的智能反射式面辅助ISAC系统的信道估计

    Deep-Learning-Based Channel Estimation for IRS-Assisted ISAC System

    [https://arxiv.org/abs/2402.09439](https://arxiv.org/abs/2402.09439)

    本文提出了一种基于深度学习的框架，在IRS辅助的ISAC系统中解决了信道估计问题。通过设计两种不同的神经网络架构，该方法在不同的信道环境下实现了优越性能。

    

    综合感知和通信（ISAC）以及智能反射式面（IRS）被视为未来无线网络的有希望的技术。本文研究了IRS辅助的ISAC系统中的信道估计问题。提出了一种基于深度学习的框架来估计该系统中的感知和通信（S&C）信道。考虑到S&C信道的不同传播环境，设计了两种深度神经网络（DNN）架构来实现此框架。第一个DNN被设计在ISAC基站上用于估计感知信道，而第二个DNN架构则被分配给每个下行用户设备用于估计其通信信道。此外，精心设计了用于训练DNN的输入-输出对。仿真结果表明，与各种信噪比条件下的基准方案相比，所提出的估计方法具有优势。

    arXiv:2402.09439v1 Announce Type: cross  Abstract: Integrated sensing and communication (ISAC) and intelligent reflecting surface (IRS) are viewed as promising technologies for future generations of wireless networks. This paper investigates the channel estimation problem in an IRS-assisted ISAC system. A deep-learning framework is proposed to estimate the sensing and communication (S&C) channels in such a system. Considering different propagation environments of the S&C channels, two deep neural network (DNN) architectures are designed to realize this framework. The first DNN is devised at the ISAC base station for estimating the sensing channel, while the second DNN architecture is assigned to each downlink user equipment to estimate its communication channel. Moreover, the input-output pairs to train the DNNs are carefully designed. Simulation results show the superiority of the proposed estimation approach compared to the benchmark scheme under various signal-to-noise ratio conditi
    
[^44]: EcoVal:一种高效的机器学习数据估值框架

    EcoVal: An Efficient Data Valuation Framework for Machine Learning

    [https://arxiv.org/abs/2402.09288](https://arxiv.org/abs/2402.09288)

    EcoVal是一种高效的机器学习数据估值框架，通过估计每个数据的内在和外在价值，实现了快速实用地估算机器学习模型数据的价值。

    

    在机器学习工作流中量化数据的价值可以在机器学习倡议中做出更具战略意义的决策中起到关键作用。现有的基于Shapley值的机器学习数据估值框架在计算方面非常昂贵，因为需要大量重复训练模型才能获得Shapley值。在本文中，我们介绍了一种高效的数据估值框架EcoVal，以快速实用的方式估算机器学习模型数据的价值。我们不直接处理独立的数据样本，而是确定类似的数据点簇的价值。这个价值进一步在所有成员簇点之间传播。我们展示了可以通过估计每个数据的内在和外在价值来确定整体数据价值。这是通过将模型的性能建模为“生产函数”来实现的，这是一个非常重要的概念。

    arXiv:2402.09288v1 Announce Type: new Abstract: Quantifying the value of data within a machine learning workflow can play a pivotal role in making more strategic decisions in machine learning initiatives. The existing Shapley value based frameworks for data valuation in machine learning are computationally expensive as they require considerable amount of repeated training of the model to obtain the Shapley value. In this paper, we introduce an efficient data valuation framework EcoVal, to estimate the value of data for machine learning models in a fast and practical manner. Instead of directly working with individual data sample, we determine the value of a cluster of similar data points. This value is further propagated amongst all the member cluster points. We show that the overall data value can be determined by estimating the intrinsic and extrinsic value of each data. This is enabled by formulating the performance of a model as a \textit{production function}, a concept which is po
    
[^45]: 基于CUDA的GPU优化稀疏卷积在3D点云上的应用

    Optimization of Sparse Convolution for 3D-Point Cloud on GPUs with CUDA

    [https://arxiv.org/abs/2402.07710](https://arxiv.org/abs/2402.07710)

    本论文研究了在3D点云上进行稀疏卷积的GPU优化方法，以解决点云的稀疏性和计算问题。

    

    最近几年，深度学习方法的应用显著增加，特别是卷积神经网络（CNN），它们已经成为涉及结构化格网数据的各个领域中的主要方法，如图像分析和处理。然而，随着LiDAR和3D传感器在许多领域的使用呈指数增长，对3D点云的分析需求也增加了。利用3D点云在包括物体识别和分割在内的各种应用中至关重要，因为它们提供了三维环境中事物的空间描述。与照片不同，点云具有稀疏性和缺乏规则的格网，因此存在着独特的处理和计算问题。

    In recent years, there has been a significant increase in the utilization of deep learning methods, particularly convolutional neural networks (CNNs), which have emerged as the dominant approach in various domains that involve structured grid data, such as picture analysis and processing. Nevertheless, the exponential growth in the utilization of LiDAR and 3D sensors across many domains has resulted in an increased need for the analysis of 3D point clouds. The utilization of 3D point clouds is crucial in various applications, including object recognition and segmentation, as they offer a spatial depiction of things within a three-dimensional environment. In contrast to photos, point clouds exhibit sparsity and lack a regular grid, hence posing distinct processing and computational issues.
    
[^46]: 动态图信息瓶颈

    Dynamic Graph Information Bottleneck

    [https://arxiv.org/abs/2402.06716](https://arxiv.org/abs/2402.06716)

    动态图信息瓶颈框架（DGIB）能够学习鲁棒且有区分性的动态图表示。利用信息瓶颈原理，通过迭代引导和改进图快照传递的结构和特征信息流，压缩冗余信息并保留有价值的信息。该框架能满足最小-全局-一致条件，提高了动态图神经网络的鲁棒性。

    

    动态图广泛存在于现实世界中，它们携带着复杂的时空特征模式，对于它们的表示学习提出了挑战。动态图神经网络（DGNNs）通过利用内在的动态性展示了令人印象深刻的预测能力。然而，DGNNs展示了有限的鲁棒性，易受对抗攻击。本文提出了一种新颖的动态图信息瓶颈（DGIB）框架来学习鲁棒且有区分性的表示。借助信息瓶颈（IB）原理，我们首先提出期望的最优表示应满足最小-全局-一致（MSC）条件。为了在潜在表示中压缩冗余信息和保留有价值的信息，DGIB迭代地引导和改进通过图快照传递的结构和特征信息流。为了满足MSC条件，我们将整体IB目标分解为DGIB$_{MS}$和DGIB$_C$，其中DGIB$_{MS}$通道的目标是...

    Dynamic Graphs widely exist in the real world, which carry complicated spatial and temporal feature patterns, challenging their representation learning. Dynamic Graph Neural Networks (DGNNs) have shown impressive predictive abilities by exploiting the intrinsic dynamics. However, DGNNs exhibit limited robustness, prone to adversarial attacks. This paper presents the novel Dynamic Graph Information Bottleneck (DGIB) framework to learn robust and discriminative representations. Leveraged by the Information Bottleneck (IB) principle, we first propose the expected optimal representations should satisfy the Minimal-Sufficient-Consensual (MSC) Condition. To compress redundant as well as conserve meritorious information into latent representation, DGIB iteratively directs and refines the structural and feature information flow passing through graph snapshots. To meet the MSC Condition, we decompose the overall IB objectives into DGIB$_{MS}$ and DGIB$_C$, in which the DGIB$_{MS}$ channel aims 
    
[^47]: 《Transformer压缩调研》

    A Survey on Transformer Compression

    [https://arxiv.org/abs/2402.05964](https://arxiv.org/abs/2402.05964)

    《Transformer压缩调研》是对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。

    

    基于Transformer架构的大型模型在人工智能领域，特别是自然语言处理（NLP）和计算机视觉（CV）领域中扮演着日益重要的角色。模型压缩方法可以减少模型的内存和计算成本，是在实际设备上实现Transformer模型的必要步骤。鉴于Transformer的独特架构，具有交替的注意力和前馈神经网络（FFN）模块，需要特定的压缩技术。这些压缩方法的效率也至关重要，因为重新训练整个训练数据集上的大型模型往往是不切实际的。本调研提供了对最近压缩方法的全面回顾，特别关注它们在Transformer模型中的应用。压缩方法主要分为修剪、量化、知识蒸馏和高效架构设计四个类别。在每个类别中，我们讨论了压缩方法

    Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods
    
[^48]: 明明就在眼前：对弱势患者群体进行不可检测的对抗性偏见攻击

    Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations

    [https://arxiv.org/abs/2402.05713](https://arxiv.org/abs/2402.05713)

    该研究发现在医学影像中，可以通过针对特定人群的标签污染攻击来破坏深度学习模型的性能，并引入对抗性的诊断不足偏见。研究结果还表明，人群在训练数据中的表示对于不可检测的对抗性偏见攻击的脆弱性直接相关。

    

    人工智能在放射学中的广泛应用揭示了深度学习模型加剧对弱势患者群体的临床偏见的风险。虽然先前的文献主要关注训练的深度学习模型所展示的偏见的量化，但针对特定人口群体的对抗性偏见攻击以及其在临床环境中的影响仍然是一个未被充分研究的医学影像领域。在这项工作中，我们证明了针对人口统计学标签的毒化攻击可以向深度学习模型引入对抗性的诊断不足偏见，并在不影响整体模型性能的情况下降低对被低估群体的性能。此外，我们的结果在多个性能指标和人口群体（如性别、年龄以及其交叉子群）上表明，群体对于不可检测的对抗性偏见攻击的脆弱性与其在模型的训练数据中的表征直接相关。

    The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.
    
[^49]: 利用分治程序指导大型语言模型对问题求解进行引导

    Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving

    [https://arxiv.org/abs/2402.05359](https://arxiv.org/abs/2402.05359)

    该论文提出了一种以分治程序引导大型语言模型（LLM）的方法，以解决涉及重复子任务和/或具有欺骗性内容的问题。实验证明，该方法可以提高LLM的表达能力。

    

    基础模型，如大型语言模型（LLMs），因其广泛的应用而引起了广泛的关注。现有的研究表明，适当的提示设计，如思维链，可以释放LLM在不同领域的强大能力。然而，对于处理涉及重复子任务和/或具有欺骗性内容的任务（如算术计算和文章级虚假新闻检测），现有的提示策略要么表现出表达能力不足，要么由幻觉引发中间错误。为了使LLM对这些中间错误更具辨别力，我们提出了一种以分治程序引导LLM的方法，同时确保优越的表达能力和任务分解、子任务解决和解决组装过程的分离。理论分析表明，我们的策略可以引导LLM扩展固定深度Transformer的表达能力。实验表明，我们提出的方法可以实现

    Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
    
[^50]: 分层树状知识图谱用于学术调研

    Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey

    [https://arxiv.org/abs/2402.04854](https://arxiv.org/abs/2402.04854)

    该论文提出了一种分层树状知识图谱和推荐系统，帮助初学者研究者进行研究调研，填补了现有导航知识图谱的不足，并解决了学术论文推荐系统中高文本相似性带来的困惑。

    

    对于缺乏研究培训的初学者研究者来说，研究调查一直是一个挑战。这些研究者在短时间内很难理解他们研究主题内的方向，以及发现新的研究发现。为初学者研究者提供直观的帮助的一种方式是提供相关的知识图谱(KG)并推荐相关的学术论文。然而，现有的导航知识图谱主要依赖于研究领域的关键字，常常无法清楚地呈现多个相关论文之间的逻辑层次关系。此外，大多数学术论文推荐系统仅仅依赖于高文本相似性，这可能会让研究人员困惑为什么推荐了特定的文章。他们可能缺乏了解关于他们希望获得的"问题解决"和"问题发现"之间的见解连接的重要信息。为解决这些问题，本研究旨在支持初学者研究者进行研究调研。

    Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
    
[^51]: 空间群约束的晶体生成

    Space Group Constrained Crystal Generation

    [https://arxiv.org/abs/2402.03992](https://arxiv.org/abs/2402.03992)

    本文在晶体生成中考虑了空间群约束，提出了一个更容易实现的等效形式，进一步提出了一个改进的扩散模型DiffCSP++，实验结果表明其有效性。

    

    结晶体是许多科学和工业应用的基础。虽然已经提出了各种基于学习的方法用于生成晶体，但现有方法很少考虑到空间群约束，而空间群约束对于描述晶体的几何形状和许多理想特性密切相关。然而，考虑到空间群约束是具有多样性和复杂性的。在本文中，我们将空间群约束简化为更容易手工放入生成过程的等效形式。具体而言，我们将空间群约束分为两部分：晶格矩阵不变对数空间的基础约束和分数坐标的Wyckoff位置约束。基于这些约束，我们提出了DiffCSP++，这是一种新颖的扩展了之前工作DiffCSP的扩散模型，进一步考虑了空间群约束。实验在多个数据集上进行。

    Crystals are the foundation of numerous scientific and industrial applications. While various learning-based approaches have been proposed for crystal generation, existing methods seldom consider the space group constraint which is crucial in describing the geometry of crystals and closely relevant to many desirable properties. However, considering space group constraint is challenging owing to its diverse and nontrivial forms. In this paper, we reduce the space group constraint into an equivalent formulation that is more tractable to be handcrafted into the generation process. In particular, we translate the space group constraint into two parts: the basis constraint of the invariant logarithmic space of the lattice matrix and the Wyckoff position constraint of the fractional coordinates. Upon the derived constraints, we then propose DiffCSP++, a novel diffusion model that has enhanced a previous work DiffCSP by further taking space group constraint into account. Experiments on severa
    
[^52]: 因果贝叶斯优化通过外源分布学习

    Causal Bayesian Optimization via Exogenous Distribution Learning

    [https://arxiv.org/abs/2402.02277](https://arxiv.org/abs/2402.02277)

    本文引入了一种新的方法，通过学习外源变量的分布，提高了结构化因果模型的近似精度，并将因果贝叶斯优化扩展到更一般的因果方案。

    

    在结构化因果模型中，将目标变量最大化作为操作目标是一个重要的问题。现有的因果贝叶斯优化（CBO）方法要么依赖于改变因果结构以最大化奖励的硬干预，要么引入动作节点到内生变量中，以调整数据生成机制以实现目标。本文引入了一种新的方法来学习外源变量的分布，这在现有方法中通常被忽略或通过期望进行边缘化。外源分布学习提高了通常通过有限观测数据训练的代理模型中的结构化因果模型的近似精度。此外，学习到的外源分布将现有的CBO扩展到超出加性噪声模型（ANM）的一般因果方案。恢复外源变量使我们能够为噪声或未观测到的隐藏变量使用更灵活的先验。引入了一种新的CBO方法。

    Maximizing a target variable as an operational objective in a structured causal model is an important problem. Existing Causal Bayesian Optimization (CBO) methods either rely on hard interventions that alter the causal structure to maximize the reward; or introduce action nodes to endogenous variables so that the data generation mechanisms are adjusted to achieve the objective. In this paper, a novel method is introduced to learn the distribution of exogenous variables, which is typically ignored or marginalized through expectation by existing methods.   Exogenous distribution learning improves the approximation accuracy of structured causal models in a surrogate model that is usually trained with limited observational data. Moreover, the learned exogenous distribution extends existing CBO to general causal schemes beyond Additive Noise Models (ANM). The recovery of exogenous variables allows us to use a more flexible prior for noise or unobserved hidden variables. A new CBO method is 
    
[^53]: 在线转移学习用于RSV病例检测

    Online Transfer Learning for RSV Case Detection

    [https://arxiv.org/abs/2402.01987](https://arxiv.org/abs/2402.01987)

    这项研究介绍了一种名为PVAW的在线多源转移学习方法，通过动态加权机制实现了对序列流行病学数据的自适应调整，并在分析RSV数据的应用中取得了显著的模型性能改进。

    

    转移学习已经成为机器学习中的一个关键技术，在各种真实世界应用中表现出高效性。然而，当将这种方法应用于序列流行病学数据时，会面临一个重要挑战，即标注信息匮乏。为了解决这个挑战，我们引入了一种新颖的在线多源转移学习方法，称为预测体积自适应加权（PVAW）。PVAW在整合模型中创造性地实现了动态加权机制，可以根据每个源模型和目标模型的相关性和贡献度自动调整权重。我们通过在匹兹堡大学医学中心收集的多个季节的呼吸道合胞病毒（RSV）数据上应用PVAW，证明了其有效性。我们的方法在模型性能上显著优于现有基线，突出了在线转移学习在处理这一问题上的潜力。

    Transfer learning has become a pivotal technique in machine learning, renowned for its effectiveness in various real-world applications. However, a significant challenge arises when applying this approach to sequential epidemiological data, often characterized by a scarcity of labeled information. To address this challenge, we introduce Predictive Volume-Adaptive Weighting (PVAW), a novel online multi-source transfer learning method. PVAW innovatively implements a dynamic weighting mechanism within an ensemble model, allowing for the automatic adjustment of weights based on the relevance and contribution of each source and target model. We demonstrate the effectiveness of PVAW through its application in analyzing Respiratory Syncytial Virus (RSV) data, collected over multiple seasons at the University of Pittsburgh Medical Center. Our method showcases significant improvements in model performance over existing baselines, highlighting the potential of online transfer learning in handlin
    
[^54]: 重新思考多元时间序列预测的通道相关性：从领先指标中学习

    Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators

    [https://arxiv.org/abs/2401.17548](https://arxiv.org/abs/2401.17548)

    本文提出了一种新方法，LIFT，通过利用通道相关性和领先指标，为多元时间序列预测提供准确的预测。LIFT方法可以无缝与任意时间序列预测方法协作，大量实验证明了其有效性。

    

    最近，独立于通道的方法在多元时间序列（MTS）预测中取得了最先进的性能。尽管这些方法减少了过拟合的风险，但它们错过了利用通道相关性进行准确预测的潜在机会。我们认为，在变量之间存在局部平稳的领先-滞后关系，即一些滞后变量在短时间内可能遵循领先指标。利用这种通道相关性是有益的，因为领先指标提供了先进信息，可以用来减少滞后变量的预测难度。在本文中，我们提出了一种名为LIFT的新方法，该方法首先在每个时间步骤高效地估计领先指标及其领先步骤，然后巧妙地允许滞后变量利用来自领先指标的先进信息。LIFT作为一个插件，可以与任意时间序列预测方法无缝协作。进行了大量实验证明了LIFT方法的有效性。

    Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
    
[^55]: 简单策略优化

    Simple Policy Optimization

    [https://arxiv.org/abs/2401.16025](https://arxiv.org/abs/2401.16025)

    SPO算法引入了新的KL散度剪切方法，相较于PPO的主流变体，在Atari 2600环境中表现出更好的样本效率、极低的KL散度和更高的策略熵，且对网络深度或复杂度的增加具有鲁棒性。

    

    PPO（Proximal Policy Optimization）算法在许多领域表现出色，被认为是TRPO（Trust Region Policy Optimization）算法的简化版本。然而，PPO中的比率剪切操作并不总是有效地强制执行信任区域约束，这可能会影响算法的稳定性。本文提出了一种新颖的剪切方法，即Simple Policy Optimization（SPO）算法，用于旧策略和当前策略之间的KL散度。在Atari 2600环境中进行的大量实验结果表明，与PPO的主流变体相比，SPO实现了更好的样本效率，极低的KL散度和更高的策略熵，并且对网络深度或复杂度的增加具有鲁棒性。更重要的是，SPO保持了无约束一阶算法的简单性。

    arXiv:2401.16025v2 Announce Type: replace  Abstract: PPO (Proximal Policy Optimization) algorithm has demonstrated excellent performance in many fields, and it is considered as a simple version of TRPO (Trust Region Policy Optimization) algorithm. However, the ratio clipping operation in PPO may not always effectively enforce the trust region constraints, this can be a potential factor affecting the stability of the algorithm. In this paper, we propose Simple Policy Optimization (SPO) algorithm, which introduces a novel clipping method for KL divergence between the old and current policies. Extensive experimental results in Atari 2600 environments indicate that, compared to the mainstream variants of PPO, SPO achieves better sample efficiency, extremely low KL divergence, and higher policy entropy, and is robust to the increase in network depth or complexity. More importantly, SPO maintains the simplicity of an unconstrained first-order algorithm. Code is available at https://github.co
    
[^56]: INCPrompt：面向任务的增量提示，无需重复练习的类别增量学习

    INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning

    [https://arxiv.org/abs/2401.11667](https://arxiv.org/abs/2401.11667)

    INCPrompt采用自适应关键学习者和面向任务的提示，结合通用和任务特定知识，有效缓解灾难性遗忘，表现优越，对持续学习性能具有显著影响。

    

    本文介绍了INCPrompt，一种创新的持续学习解决方案，有效地解决了灾难性遗忘问题。 INCPrompt的关键创新在于其使用自适应的关键学习者和面向任务的提示来捕获与任务相关的信息。 这种独特组合封装了跨任务的通用知识并编码了任务特定知识。 我们在多个持续学习基准上进行的全面评估表明，INCPrompt优于现有算法，显示出其在减轻灾难性遗忘的同时保持高性能的有效性。 这些结果突显了面向任务的增量提示对持续学习性能的重要影响。

    arXiv:2401.11667v2 Announce Type: replace  Abstract: This paper introduces INCPrompt, an innovative continual learning solution that effectively addresses catastrophic forgetting. INCPrompt's key innovation lies in its use of adaptive key-learner and task-aware prompts that capture task-relevant information. This unique combination encapsulates general knowledge across tasks and encodes task-specific knowledge. Our comprehensive evaluation across multiple continual learning benchmarks demonstrates INCPrompt's superiority over existing algorithms, showing its effectiveness in mitigating catastrophic forgetting while maintaining high performance. These results highlight the significant impact of task-aware incremental prompting on continual learning performance.
    
[^57]: 实现端到端人工智能驱动的全球天气预报系统

    Towards an end-to-end artificial intelligence driven global weather forecasting system

    [https://arxiv.org/abs/2312.12462](https://arxiv.org/abs/2312.12462)

    提出了一种端到端基于人工智能的全球天气预报系统，通过将AI技术应用于数据同化和天气预报模型，实现了从数据处理到预测全过程的自动化。

    

    天气预报系统对科学和社会至关重要，在将人工智能（AI）应用于中期天气预报方面取得了重大成就。然而，现有的基于AI的天气预报模型依赖于传统数值天气预报（NWP）系统的分析或再分析产品作为预测的初始条件。初始状态通常由传统数据同化组件生成，这是计算昂贵且耗时。本文提出了一种基于AI的数据同化模型，即Adas，用于全球天气变量。我们将Adas与先进的基于AI的天气预报模型（即FengWu）结合起来，构建了第一个端到端基于AI的全球天气预报系统：FengWu-Adas。我们证明了Adas能够同化稀疏的全球观测数据，产生高质量的分析结果，使系统能够稳定运行。

    arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
    
[^58]: 多项式信念网络

    Multinomial belief networks

    [https://arxiv.org/abs/2311.16909](https://arxiv.org/abs/2311.16909)

    提出了一种深度生成模型，用于处理具有多项式计数数据的分析需求，并能够从数据中完全自动提取出生物意义的元签名。

    

    机器学习中的贝叶斯方法在需要量化不确定性、处理缺失观测、样本稀缺或数据稀疏时是具有吸引力的。为了满足这些分析需求，我们提出了一种用于多项式计数数据的深层生成模型，其中网络的权重和隐藏单元均服从狄利克雷分布。我们制定了一个利用一系列增广关系的吉布斯抽样过程，类似于周-丛-陈模型。我们将该模型应用于小规模手写数字和一个大型的DNA突变癌症实验数据集，并展示了该模型如何能够完全数据驱动地提取出生物意义的元签名。

    arXiv:2311.16909v2 Announce Type: replace-cross  Abstract: A Bayesian approach to machine learning is attractive when we need to quantify uncertainty, deal with missing observations, when samples are scarce, or when the data is sparse. All of these commonly apply when analysing healthcare data. To address these analytical requirements, we propose a deep generative model for multinomial count data where both the weights and hidden units of the network are Dirichlet distributed. A Gibbs sampling procedure is formulated that takes advantage of a series of augmentation relations, analogous to the Zhou--Cong--Chen model. We apply the model on small handwritten digits, and a large experimental dataset of DNA mutations in cancer, and we show how the model is able to extract biologically meaningful meta-signatures in a fully data-driven way.
    
[^59]: 在线学习世界模型的局部敏感稀疏编码

    Locality Sensitive Sparse Encoding for Learning World Models Online. (arXiv:2401.13034v1 [cs.LG])

    [http://arxiv.org/abs/2401.13034](http://arxiv.org/abs/2401.13034)

    本文提出了一种基于局部敏感稀疏编码的线性回归模型，通过非线性随机特征实现对复杂环境的拟合。这种模型能够高效地进行稀疏更新，实现了优化拟合先前经验的Follow-The-Leader（FTL）世界模型。

    

    为了解决神经网络在在线学习中遇到的数据非平稳性问题，本文提出了一种基于局部敏感稀疏编码的线性回归模型，该模型通过非线性随机特征实现了对复杂环境的拟合。通过引入局部敏感稀疏编码，我们能够进行高效的稀疏更新，在平衡模型容量和计算效率的同时实现优化拟合所有先前经验的Follow-The-Leader（FTL）世界模型。

    Acquiring an accurate world model online for model-based reinforcement learning (MBRL) is challenging due to data nonstationarity, which typically causes catastrophic forgetting for neural networks (NNs). From the online learning perspective, a Follow-The-Leader (FTL) world model is desirable, which optimally fits all previous experiences at each round. Unfortunately, NN-based models need re-training on all accumulated data at every interaction step to achieve FTL, which is computationally expensive for lifelong agents. In this paper, we revisit models that can achieve FTL with incremental updates. Specifically, our world model is a linear regression model supported by nonlinear random features. The linear part ensures efficient FTL update while the nonlinear random feature empowers the fitting of complex environments. To best trade off model capacity and computation efficiency, we introduce a locality sensitive sparse encoding, which allows us to conduct efficient sparse updates even 
    
[^60]: 级联强化学习

    Cascading Reinforcement Learning. (arXiv:2401.08961v1 [cs.LG])

    [http://arxiv.org/abs/2401.08961](http://arxiv.org/abs/2401.08961)

    本文提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响，在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    

    最近几年，级联赌博机在推荐系统和在线广告中应用广泛。在级联赌博机模型中，每个时刻，一个代理人从一组具有未知吸引概率的项目中推荐一个有序的项目子集（称为项目列表）。然后，用户检查列表，并点击第一个有吸引力的项目（如果有的话），之后，代理收到一个奖励。代理的目标是最大化预期的累积奖励。然而，以往的级联赌博机文献忽略了用户状态（例如历史行为）对推荐的影响以及会话进行过程中状态的变化。受此事实的启发，我们提出了一个广义的级联强化学习框架，考虑了用户状态和状态转换对决策的影响。在级联强化学习中，我们需要选择不仅具有较大吸引概率的项目，还要选择能够导致良好后继状态的项目。

    Cascading bandits have gained popularity in recent years due to their applicability to recommendation systems and online advertising. In the cascading bandit model, at each timestep, an agent recommends an ordered subset of items (called an item list) from a pool of items, each associated with an unknown attraction probability. Then, the user examines the list, and clicks the first attractive item (if any), and after that, the agent receives a reward. The goal of the agent is to maximize the expected cumulative reward. However, the prior literature on cascading bandits ignores the influences of user states (e.g., historical behaviors) on recommendations and the change of states as the session proceeds. Motivated by this fact, we propose a generalized cascading RL framework, which considers the impact of user states and state transition into decisions. In cascading RL, we need to select items not only with large attraction probabilities but also leading to good successor states. This im
    
[^61]: 微小时间混合器 (TTMs): 针对多变量时间序列的增强零/少样本预测的快速预训练模型

    Tiny Time Mixers (TTMs): Fast Pretrained Models for Enhanced Zero/Few-Shot Forecasting of Multivariate Time Series. (arXiv:2401.03955v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2401.03955](http://arxiv.org/abs/2401.03955)

    本论文介绍了一种名为微小时间混合器 (TTMs) 的预训练模型，该模型针对多变量时间序列的零/少样本预测进行了优化。与大型预训练模型相比，TTMs模型更小、更快，并考虑了跨通道相关性，能够在短时间内进行有效的预测。

    

    零/少样本学习的大型预训练模型在语言和视觉领域表现出色，但在多变量时间序列 (TS) 中面临着多样性和公开预训练数据稀缺的挑战。因此，最近在时间序列预测中使用预训练的大型语言模型 (LLMs) 进行各种适应的趋势逐渐增加。这些方法利用跨领域迁移学习，出奇地取得了令人印象深刻的结果。然而，这些模型通常非常缓慢且庞大（大约十亿个参数），并且不考虑跨通道相关性。为了解决这个问题，我们提出了多层微小时间混合器 (TTM)，这是一种基于轻量级 TSMixer 结构的显著小型模型。TTM 是首个成功开发的微型通用预训练模型（≤100万个参数），专门在公开TS数据集上进行快速训练（仅需4-8小时），具有有效的迁移学习能力进行预测。

    Large Pretrained models for zero/few-shot learning excel in language and vision domains but encounter challenges in multivariate time series (TS) due to the diverse nature and scarcity of publicly available pretraining data. Consequently, there has been a recent surge in utilizing pretrained large language models (LLMs) with various adaptations for time series forecasting. These approaches employ cross-domain transfer learning and surprisingly yield impressive results. However, these models are typically very slow and large ($\sim$billion parameters) and do not consider cross-channel correlations. To address this, we present Multi-level Tiny Time Mixers (TTM), a significantly small model based on the lightweight TSMixer architecture. TTM marks the first success in developing tiny general-pretrained models ($\le$1 million parameters), exclusively trained on public TS datasets in a flash of just 4-8 hrs with effective transfer learning capabilities for forecasting. To tackle the complexi
    
[^62]: 从因果角度重新思考图对比学习中的维度理论

    Rethinking Dimensional Rationale in Graph Contrastive Learning from Causal Perspective. (arXiv:2312.10401v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2312.10401](http://arxiv.org/abs/2312.10401)

    本论文从因果角度重新思考图对比学习中的维度理论，并提出在图中捕捉维度理论的方法，以改善性能，并解决图模型学习中的问题。以上方法在实验中得到验证。

    

    图对比学习是一种在图中捕捉不变信息的学习范式。最近的研究专注于探索图的结构理论，从而增加不变信息的可区分性。然而，这些方法可能导致图模型朝向解释图的可解释性进行错误学习，因此学习到的噪声和与任务无关的信息干扰了图的预测。为了探索图的内在理论，我们提出了从图中捕捉维度理论的方法，但这在文献中并没有得到足够的关注。通过实验验证了上述路径的可行性。为了阐明维度理论对性能改进的内在机制，我们从因果角度重新思考图对比学习中的维度理论。

    Graph contrastive learning is a general learning paradigm excelling at capturing invariant information from diverse perturbations in graphs. Recent works focus on exploring the structural rationale from graphs, thereby increasing the discriminability of the invariant information. However, such methods may incur in the mis-learning of graph models towards the interpretability of graphs, and thus the learned noisy and task-agnostic information interferes with the prediction of graphs. To this end, with the purpose of exploring the intrinsic rationale of graphs, we accordingly propose to capture the dimensional rationale from graphs, which has not received sufficient attention in the literature. The conducted exploratory experiments attest to the feasibility of the aforementioned roadmap. To elucidate the innate mechanism behind the performance improvement arising from the dimensional rationale, we rethink the dimensional rationale in graph contrastive learning from a causal perspective a
    
[^63]: 对于文本预测的忠实和稳健的本地可解释性

    Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])

    [http://arxiv.org/abs/2311.01605](http://arxiv.org/abs/2311.01605)

    提出了一种名为FRED的新颖方法，用于解释文本预测。FRED可以识别文档中的关键词，并且通过与最先进的方法进行的实证评估证明了其在提供对文本模型的深入见解方面的有效性。

    

    可解释性对于机器学习模型在关键领域中得到信任和部署是至关重要的。然而，现有的用于解释文本模型的方法通常复杂，并且缺乏坚实的数学基础，它们的性能也不能保证。在本文中，我们提出了一种新颖的方法FRED（Faithful and Robust Explainer for textual Documents），用于解释文本预测。FRED可以识别文档中的关键词，当这些词被移除时对预测结果产生重大影响。我们通过正式的定义和对可解释分类器的理论分析，确立了FRED的可靠性。此外，我们还通过与最先进的方法进行的实证评估，证明了FRED在提供对文本模型的深入见解方面的有效性。

    Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
    
[^64]: 没有处理器的机器学习：非线性电子变质材料中的浮现学习

    Machine Learning Without a Processor: Emergent Learning in a Nonlinear Electronic Metamaterial. (arXiv:2311.00537v1 [cond-mat.soft])

    [http://arxiv.org/abs/2311.00537](http://arxiv.org/abs/2311.00537)

    这项研究介绍了一种非线性学习变质材料，它由基于晶体管的自适应非线性阻性元件构成，可以在没有计算机的情况下学习非线性任务，并降低训练误差的多个模式，为模拟机器学习提供了新的硬件选择。

    

    标准的深度学习算法需要对大规模非线性网络进行微分，这个过程缓慢且耗能。电子学习变质材料提供了潜在的快速、高效和容错的模拟机器学习硬件，但现有实现是线性的，严重限制了其功能。这些系统与人工神经网络和人脑有很大的区别，因此尚未探索将非线性元素纳入其中的可行性和实用性。在这里，我们介绍了一种非线性学习变质材料——一种基于晶体管的自适应非线性阻性元件的模拟电子网络。我们证明该系统可以在没有计算机的情况下学习不可实现的任务，包括异或和非线性回归。我们发现我们的非线性学习变质材料按顺序降低训练误差的模式（均值、斜率、曲率），类似于人工神经网络中的谱偏差。该电路对干扰。

    Standard deep learning algorithms require differentiating large nonlinear networks, a process that is slow and power-hungry. Electronic learning metamaterials offer potentially fast, efficient, and fault-tolerant hardware for analog machine learning, but existing implementations are linear, severely limiting their capabilities. These systems differ significantly from artificial neural networks as well as the brain, so the feasibility and utility of incorporating nonlinear elements have not been explored. Here we introduce a nonlinear learning metamaterial -- an analog electronic network made of self-adjusting nonlinear resistive elements based on transistors. We demonstrate that the system learns tasks unachievable in linear systems, including XOR and nonlinear regression, without a computer. We find our nonlinear learning metamaterial reduces modes of training error in order (mean, slope, curvature), similar to spectral bias in artificial neural networks. The circuitry is robust to da
    
[^65]: 多任务在线学习：倾听社区的喧嚣

    Multitask Online Learning: Listen to the Neighborhood Buzz. (arXiv:2310.17385v1 [cs.LG])

    [http://arxiv.org/abs/2310.17385](http://arxiv.org/abs/2310.17385)

    我们提出了一种多任务在线学习的算法，代理只能通过邻居交换信息。我们的分析表明，当代理在相似的任务上操作时，我们的算法的遗憾值显著改善。此外，我们证明了算法在损失函数为线性函数时可以保护隐私。

    

    我们研究了一种多任务在线学习的设置，其中代理只能在任意通信网络上与其邻居交换信息。我们介绍了一种分散算法$\texttt{MT-CO}_2\texttt{OL}$，其遗憾值取决于任务相似性和网络结构的相互作用。我们的分析表明，$\texttt{MT-CO}_2\texttt{OL}$的遗憾值（常数除外）永远不会比代理不共享信息时获得的上界差。另一方面，当相邻代理在相似的任务上操作时，我们的界限显著改善。此外，我们证明了在损失函数为线性函数时，我们的算法可以在隐私保护性上做到微不足道的影响。最后，我们提供了实验证据支持我们的理论。

    We study multitask online learning in a setting where agents can only exchange information with their neighbors on an arbitrary communication network. We introduce $\texttt{MT-CO}_2\texttt{OL}$, a decentralized algorithm for this setting whose regret depends on the interplay between the task similarities and the network structure. Our analysis shows that the regret of $\texttt{MT-CO}_2\texttt{OL}$ is never worse (up to constants) than the bound obtained when agents do not share information. On the other hand, our bounds significantly improve when neighboring agents operate on similar tasks. In addition, we prove that our algorithm can be made differentially private with a negligible impact on the regret when the losses are linear. Finally, we provide experimental support for our theory.
    
[^66]: 关于循环神经网络语言模型的表示能力的研究

    On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2310.12942](http://arxiv.org/abs/2310.12942)

    本文研究了基于循环神经网络的语言模型的计算表达性，扩展了图灵完备性结果到概率情况，并提供了上下界分析。

    

    本研究调查了基于循环神经网络(RNNs)的语言模型(LMs)的计算表达性。Siegelmann和Sontag(1992)曾经展示了具有有理权重和隐藏状态以及无限计算时间的RNNs是图灵完备的。然而，LMs不仅定义了字符串上的加权，还定义了(非加权)语言成员关系，对RNN LMs（RLMs）的计算能力分析应该反映这一点。我们将图灵完备性结果扩展到概率情况，展示了如何使用有理权重的RLM和无限计算时间来模拟任何概率图灵机(PTM)。由于在实践中，RLMs实时工作，每个时间步骤处理一个符号，因此我们将上述结果作为RLMs表达性的上界。我们还通过展示在实时计算限制下，这些模型可以模拟确定性实时有理PTMs来提供下界。

    This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
    
[^67]: ReLU-FNNs的本地Lipschitz常数计算：使用精确性验证计算上界

    Local Lipschitz Constant Computation of ReLU-FNNs: Upper Bound Computation with Exactness Verification. (arXiv:2310.11104v1 [math.OC])

    [http://arxiv.org/abs/2310.11104](http://arxiv.org/abs/2310.11104)

    本文研究了使用ReLUs作为激活函数的前馈神经网络的本地Lipschitz常数计算，并介绍了一种通过精确性验证计算上界的方法。

    

    本文关注使用修正线性单元（ReLUs）作为激活函数的前馈神经网络（FNNs）的本地Lipschitz常数计算。对于目标输入，FNN的本地Lipschitz常数是衡量其可靠性的合理指标。通过使用捕捉ReLUs行为的乘法器的标准过程，我们首先将本地Lipschitz常数的上界计算问题简化为一个半定规划问题（SDP）。在这里，我们引入了新的共正乘法器来准确捕捉ReLU的行为。然后，通过考虑用于上界计算的SDP的对偶问题，我们进一步得出了一个可行的测试来确定计算上界的精确性。然而，对于具有数百个ReLU的实际FNNs，这些SDP是无法解决的。为了解决这个问题，我们进一步提出了一种构造减序模型的方法，其输入输出属性与原始模型相同。

    This paper is concerned with the computation of the local Lipschitz constant of feedforward neural networks (FNNs) with activation functions being rectified linear units (ReLUs). The local Lipschitz constant of an FNN for a target input is a reasonable measure for its quantitative evaluation of the reliability. By following a standard procedure using multipliers that capture the behavior of ReLUs,we first reduce the upper bound computation problem of the local Lipschitz constant into a semidefinite programming problem (SDP). Here we newly introduce copositive multipliers to capture the ReLU behavior accurately. Then, by considering the dual of the SDP for the upper bound computation, we second derive a viable test to conclude the exactness of the computed upper bound. However, these SDPs are intractable for practical FNNs with hundreds of ReLUs. To address this issue, we further propose a method to construct a reduced order model whose input-output property is identical to the original
    
[^68]: QLLM: 大规模语言模型的准确高效低位宽量化

    QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])

    [http://arxiv.org/abs/2310.08041](http://arxiv.org/abs/2310.08041)

    QLLM是一种为大规模语言模型设计的准确高效的低位宽后训练量化方法，通过引入自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。

    

    大规模语言模型在自然语言处理领域表现出色，但由于其所需资源过大，限制了其广泛应用。虽然量化感知训练（Quantization-Aware Training，QAT）提供了一种解决方案，但它的训练成本过高，因此后训练量化（Post-Training Quantization，PTQ）成为大规模语言模型更实际的方法。在现有研究中，特定通道中的激活离群值被认为是导致后训练量化准确性下降的瓶颈。本文提出了QLLM，一种为大规模语言模型设计的准确高效的低位宽后训练量化方法。QLLM引入了一种自适应通道重组技术，将离群值的大小重新分配给其他通道，从而减轻它们对量化范围的影响。具体来说，通过通道拆分和通道组装，在保证低位宽的情况下将离群通道分解成多个子通道。

    Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
    
[^69]: 利用后验采样和因果图先验来提高强化学习的样本效率

    Exploiting Causal Graph Priors with Posterior Sampling for Reinforcement Learning. (arXiv:2310.07518v1 [cs.LG])

    [http://arxiv.org/abs/2310.07518](http://arxiv.org/abs/2310.07518)

    本文提出了一种利用因果图先验和后验采样的方法来提高强化学习样本效率。通过同时学习完整的因果图和分解动态参数，该方法能够更自然地设计先验，并且根据先验知识程度连接了遗憾率与贝叶斯遗憾之间的关系。

    

    后验采样允许利用环境转移动态的先验知识，提高强化学习的样本效率。先验通常被指定为环境变量的（部分）因果图，相比实践中麻烦的参数分布类别指定更加自然，例如在医疗治疗研究中列出生物特征之间的已知因果依赖关系。我们提出了一种新颖的后验采样方法，名为C-PSRL，该方法同时学习更高层的完整因果图和更低层导致的分解动态的参数。对于该方法，我们提供了其贝叶斯遗憾的分析，明确地将遗憾率与先验知识程度相关联。

    Posterior sampling allows the exploitation of prior knowledge of the environment's transition dynamics to improve the sample efficiency of reinforcement learning. The prior is typically specified as a class of parametric distributions, a task that can be cumbersome in practice, often resulting in the choice of uninformative priors. In this work, we propose a novel posterior sampling approach in which the prior is given as a (partial) causal graph over the environment's variables. The latter is often more natural to design, such as listing known causal dependencies between biometric features in a medical treatment study. Specifically, we propose a hierarchical Bayesian procedure, called C-PSRL, simultaneously learning the full causal graph at the higher level and the parameters of the resulting factored dynamics at the lower level. For this procedure, we provide an analysis of its Bayesian regret, which explicitly connects the regret rate with the degree of prior knowledge. Our numerica
    
[^70]: 面向模型驱动的强化学习中目标不匹配问题的统一观点

    A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning. (arXiv:2310.06253v1 [cs.LG])

    [http://arxiv.org/abs/2310.06253](http://arxiv.org/abs/2310.06253)

    这项工作提供了一个关于解决模型驱动的强化学习中目标不匹配问题的统一观点，对解决方案进行了分类，并提出了一个分类法以促进未来的研究。

    

    模型驱动的强化学习（MBRL）旨在通过学习环境的显式模型使代理更节约样本、适应性更强和更易解释。虽然近年来MBRL代理的能力有了显著提升，但如何最好地学习模型仍然是一个未解决的问题。大多数MBRL算法的目标是训练模型以对环境进行准确预测，然后使用模型确定最有益的动作。然而，最近的研究表明，模型的预测准确性通常与动作质量不相关，将根本原因归结为准确的动态模型学习与奖励策略优化之间的“目标不匹配”。随着MBRL作为一个研究领域的不断成熟，涌现出了一些互相关联的解决目标不匹配问题的解决方案类别。在本文中，我们对这些解决方案类别进行了深入调查，并提出了一个分类法以促进未来的研究。

    Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the \emph{objective mismatch} between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.
    
[^71]: TacoGFN: 针对基于结构的药物设计的目标条件GFlowNet

    TacoGFN: Target Conditioned GFlowNet for Structure-Based Drug Design. (arXiv:2310.03223v1 [cs.LG])

    [http://arxiv.org/abs/2310.03223](http://arxiv.org/abs/2310.03223)

    该论文提出了一种名为TacoGFN的目标条件GFlowNet模型，用于自动化生成符合特定蛋白质口袋目标的类药物化合物。该模型通过强化学习框架，鼓励生成具有期望属性的分子，并利用转换器和对接神经网络进行高效的分子空间探索和对接得分预测，以实现较高的结合改善效果。

    

    我们旨在自动化生成符合特定蛋白质口袋目标的类药物化合物。大多数当前方法是对有限数据集中的蛋白质-分子分布进行近似，因此在生成的分子中很难实现与训练数据集相比具有显著结合改善的分子。我们将口袋条件下的分子生成任务定义为强化学习问题，并开发了TacoGFN，一种目标条件下的生成流网络模型。我们的方法明确鼓励生成具有期望属性的分子，而不是适应预先存在的数据分布。为此，我们开发了基于转换器的对接得分预测方法来加快对接得分计算，并提出了TacoGFN来高效地探索分子空间。此外，我们还结合了几轮主动学习，使用对接神经网络对生成的样本进行查询，以改善对接得分预测。这种方法使我们能够准确地探索更多的分子空间。

    We seek to automate the generation of drug-like compounds conditioned to specific protein pocket targets. Most current methods approximate the protein-molecule distribution of a finite dataset and, therefore struggle to generate molecules with significant binding improvement over the training dataset. We instead frame the pocket-conditioned molecular generation task as an RL problem and develop TacoGFN, a target conditional Generative Flow Network model. Our method is explicitly encouraged to generate molecules with desired properties as opposed to fitting on a pre-existing data distribution. To this end, we develop transformer-based docking score prediction to speed up docking score computation and propose TacoGFN to explore molecule space efficiently. Furthermore, we incorporate several rounds of active learning where generated samples are queried using a docking oracle to improve the docking score prediction. This approach allows us to accurately explore as much of the molecule land
    
[^72]: 学习耗散神经动力系统

    Learning Dissipative Neural Dynamical Systems. (arXiv:2309.16032v1 [cs.LG])

    [http://arxiv.org/abs/2309.16032](http://arxiv.org/abs/2309.16032)

    本文提出了一种学习耗散神经动力系统模型的方法，该方法分为两个阶段。首先学习一个无约束的模型，然后导出条件来保证模型的耗散性质，并在保持逼近能力的同时扰动偏差。通过独立求解这两个扰动问题，得到一个保证为耗散的神经动力模型，同时紧密逼近非线性系统。

    

    本文考虑一个未知的非线性动力系统，其已知是耗散的。本文的目标是学习一个神经动力模型，以逼近这个系统，并在模型中保持耗散性质。一般来说，在神经网络训练过程中施加耗散性约束是一个困难的问题，目前没有已知的技术可以解决。本文分为两个阶段解决了学习耗散神经动力系统模型的问题。首先，我们学习一个无约束的神经动力模型，该模型能够紧密逼近系统的动力学行为。然后，我们导出足够的条件来扰动神经动力模型的权重，以确保耗散性质，并在保持模型与非线性系统轨迹拟合的同时扰动偏差。我们证明了这两个扰动问题可以独立求解，从而获得一个保证为耗散的神经动力模型，同时紧密逼近非线性系统。

    Consider an unknown nonlinear dynamical system that is known to be dissipative. The objective of this paper is to learn a neural dynamical model that approximates this system, while preserving the dissipativity property in the model. In general, imposing dissipativity constraints during neural network training is a hard problem for which no known techniques exist. In this work, we address the problem of learning a dissipative neural dynamical system model in two stages. First, we learn an unconstrained neural dynamical model that closely approximates the system dynamics. Next, we derive sufficient conditions to perturb the weights of the neural dynamical model to ensure dissipativity, followed by perturbation of the biases to retain the fit of the model to the trajectories of the nonlinear system. We show that these two perturbation problems can be solved independently to obtain a neural dynamical model that is guaranteed to be dissipative while closely approximating the nonlinear syst
    
[^73]: 用数据增强改进模仿学习中游戏智能体的泛化能力

    Improving Generalization in Game Agents with Data Augmentation in Imitation Learning. (arXiv:2309.12815v1 [cs.LG])

    [http://arxiv.org/abs/2309.12815](http://arxiv.org/abs/2309.12815)

    本文提出了一种改进模仿学习中游戏智能体泛化能力的方法，通过数据增强技术使训练数据更好地代表真实场景中的状态和行动分布，并在多个3D环境中进行了性能测试，结果表明数据增强可以显著提升模仿学习智能体的泛化能力。

    

    模仿学习是训练游戏智能体和高效游戏生成的有效方法。然而，泛化能力——在相关但未见过的场景中表现出良好性能的能力——对于游戏人工智能来说仍然是一个未解决的挑战。模仿学习智能体的泛型化很困难，因为它需要算法在训练分布之外采取有意义的行动。本文提出了解决这一挑战的方法。在受监督学习中数据增强的成功启发下，我们通过增强训练数据的方法，使数据集中的状态和行动分布更好地代表真实的状态-行动分布。本研究评估了将数据增强应用于观察的方法，以提高模仿学习智能体的泛化能力，并在多个3D环境中对这些增强技术进行了性能基准测试。结果表明，数据增强可以显著提高模仿学习智能体的泛化能力。

    Imitation learning is an effective approach for training game-playing agents and, consequently, for efficient game production. However, generalization - the ability to perform well in related but unseen scenarios - is an essential requirement that remains an unsolved challenge for game AI. Generalization is difficult for imitation learning agents because it requires the algorithm to take meaningful actions outside of the training distribution. In this paper we propose a solution to this challenge. Inspired by the success of data augmentation in supervised learning, we augment the training data so the distribution of states and actions in the dataset better represents the real state-action distribution. This study evaluates methods for combining and applying data augmentations to observations, to improve generalization of imitation learning agents. It also provides a performance benchmark of these augmentations across several 3D environments. These results demonstrate that data augmenta
    
[^74]: 一种贝叶斯方法用于稳健的逆强化学习

    A Bayesian Approach to Robust Inverse Reinforcement Learning. (arXiv:2309.08571v1 [cs.LG])

    [http://arxiv.org/abs/2309.08571](http://arxiv.org/abs/2309.08571)

    这篇论文提出了一种贝叶斯方法，用于稳健的离线模型导向的逆强化学习。通过同时估计专家的奖励函数和主观模型的环境动态，利用先验分布参数化专家对环境模型的准确性，提出了高效的算法。实验证明，当先验地认为专家对环境具有高度准确的模型时，估计的策略表现出稳健性能，并且优于最先进的离线IRL算法。

    

    我们考虑一种贝叶斯方法用于离线模型导向的逆强化学习(IRL)。所提出的框架通过同时估计专家的奖励函数和主观模型的环境动态，区别于现有的离线模型导向的IRL方法。我们利用一类先验分布来参数化专家对环境的模型的准确性，以开发在高维环境中估计专家奖励和主观动态的高效算法。我们的分析揭示了一个新的见解，即当先验地认为专家对环境具有高度准确的模型时，估计的策略表现出稳健性能。我们在MuJoCo环境中验证了这一观察，并展示了我们的算法胜过最先进的离线IRL算法。

    We consider a Bayesian approach to offline model-based inverse reinforcement learning (IRL). The proposed framework differs from existing offline model-based IRL approaches by performing simultaneous estimation of the expert's reward function and subjective model of environment dynamics. We make use of a class of prior distributions which parameterizes how accurate the expert's model of the environment is to develop efficient algorithms to estimate the expert's reward and subjective dynamics in high-dimensional settings. Our analysis reveals a novel insight that the estimated policy exhibits robust performance when the expert is believed (a priori) to have a highly accurate model of the environment. We verify this observation in the MuJoCo environments and show that our algorithms outperform state-of-the-art offline IRL algorithms.
    
[^75]: 复合移位算子的频率收敛问题

    Frequency Convergence of Complexon Shift Operators. (arXiv:2309.07169v1 [eess.SP])

    [http://arxiv.org/abs/2309.07169](http://arxiv.org/abs/2309.07169)

    本文研究了拓扑信号处理中复合子的可转移性，通过构造边际复合子和复合移位算子，研究其特征值和特征向量，并证明了复合子收敛时对应的复合移位算子的特征值会收敛到极限复合子的特征值。这些结果拓展了图信号处理框架。

    

    拓扑信号处理(TSP)利用单纯形复合来建模比顶点和边更高阶的结构。本文研究了TSP的可转移性，通过一种称为复合子的广义高阶图的版本。我们回顾了复合子的概念，即单纯形复合序列的极限[1]。受图移位算子的积分算子形式的启发，我们根据复合子的所有可能尺寸的组件构造了边际复合子和复合移位算子(CSO)。我们研究了CSO的特征值和特征向量，并将它们与一类新的加权邻接矩阵相关联。我们证明，当一个单纯形复合序列收敛到一个复合子时，相应的CSO的特征值收敛到极限复合子的特征值。这些结果暗示了在大型单纯形复合或单纯形复合序列上的学习可转移性，从而推广了图信号处理框架。

    Topological signal processing (TSP) utilizes simplicial complexes to model structures with higher order than vertices and edges. In this paper, we study the transferability of TSP via a generalized higher-order version of graphon, known as complexon. We recall the notion of a complexon as the limit of a simplicial complex sequence [1]. Inspired by the integral operator form of graphon shift operators, we construct a marginal complexon and complexon shift operator (CSO) according to components of all possible dimensions from the complexon. We investigate the CSO's eigenvalues and eigenvectors, and relate them to a new family of weighted adjacency matrices. We prove that when a simplicial complex sequence converges to a complexon, the eigenvalues of the corresponding CSOs converge to that of the limit complexon. These results hint at learning transferability on large simplicial complexes or simplicial complex sequences, which generalize the graphon signal processing framework.
    
[^76]: 一个示例题形的非均匀扩散模板: 从简单推导到稳定性评估再到ResNet实现

    Anisotropic Diffusion Stencils: From Simple Derivations over Stability Estimates to ResNet Implementations. (arXiv:2309.05575v2 [math.NA] UPDATED)

    [http://arxiv.org/abs/2309.05575](http://arxiv.org/abs/2309.05575)

    该论文研究了一个大型的有限差分离散化家族，在非均匀扩散过程中通过将二维扩散分解为四个一维扩散得出了一个3 x 3的模板。该模板类包含一个自由参数，涵盖了广泛的现有离散化。同时，论文还建立了与模板相对应的矩阵的谱范数上界，并将显式方案转化为ResNet块。

    

    非均匀扩散过程与扩散张量在图像分析、物理学和工程学中具有重要意义。然而，它们的数值近似对耗散性伪影和与旋转不变性偏离有强烈影响。在这项工作中，我们研究了一个大型的有限差分离散化家族，使用了一个3 x 3的模板。我们通过将二维非均匀扩散分解为四个一维扩散来推导出它。结果的模板类包含一个自由参数，涵盖了广泛的现有离散化。它包括Weickert等人(2013)的完整模板家族，并表明它们的两个参数包含冗余。此外，我们建立了与模板相对应的矩阵的谱范数上界。这给出了在欧几里得范数中保证显式方案稳定性的时间步长限制。我们的方向分割还允许将显式方案非常自然地转化为ResNet块。使用神经网络库实现简单。

    Anisotropic diffusion processes with a diffusion tensor are important in image analysis, physics, and engineering. However, their numerical approximation has a strong impact on dissipative artefacts and deviations from rotation invariance. In this work, we study a large family of finite difference discretisations on a 3 x 3 stencil. We derive it by splitting 2-D anisotropic diffusion into four 1-D diffusions. The resulting stencil class involves one free parameter and covers a wide range of existing discretisations. It comprises the full stencil family of Weickert et al. (2013) and shows that their two parameters contain redundancy. Furthermore, we establish a bound on the spectral norm of the matrix corresponding to the stencil. This gives time step size limits that guarantee stability of an explicit scheme in the Euclidean norm. Our directional splitting also allows a very natural translation of the explicit scheme into ResNet blocks. Employing neural network libraries enables simple
    
[^77]: 多模态变换器用于材料分割

    Multimodal Transformer for Material Segmentation. (arXiv:2309.04001v1 [cs.CV])

    [http://arxiv.org/abs/2309.04001](http://arxiv.org/abs/2309.04001)

    本文提出了一种新的多模态分割方法MMSFormer，该方法有效地融合四种不同模态的信息，并在MCubeS数据集上取得了显著的性能提升。

    

    利用不同模态的信息可以提高多模态分割任务的性能。然而，由于每个模态的独特特性，有效地融合不同模态的信息仍然具有挑战性。在本文中，我们提出了一种新的融合策略，可以有效地融合四种不同模态的信息：RGB、线性偏振角（AoLP）、线性偏振度（DoLP）和近红外（NIR）。我们还提出了一种名为多模态分割变换器（MMSFormer）的新模型，该模型将所提出的融合策略结合起来进行多模态材料分割。MMSFormer在多模态材料分割（MCubeS）数据集上取得了52.05％的mIoU，超过了当前最先进的方法。例如，我们的方法在检测砾石（+10.4％）和人类（+9.1％）类上提供了显着的改进。消融研究表明融合块中的不同模块对结果至关重要。

    Leveraging information across diverse modalities is known to enhance performance on multimodal segmentation tasks. However, effectively fusing information from different modalities remains challenging due to the unique characteristics of each modality. In this paper, we propose a novel fusion strategy that can effectively fuse information from different combinations of four different modalities: RGB, Angle of Linear Polarization (AoLP), Degree of Linear Polarization (DoLP) and Near-Infrared (NIR). We also propose a new model named Multi-Modal Segmentation Transformer (MMSFormer) that incorporates the proposed fusion strategy to perform multimodal material segmentation. MMSFormer achieves 52.05% mIoU outperforming the current state-of-the-art on Multimodal Material Segmentation (MCubeS) dataset. For instance, our method provides significant improvement in detecting gravel (+10.4%) and human (+9.1%) classes. Ablation studies show that different modules in the fusion block are crucial for
    
[^78]: 人脸图像的神经隐式形变

    Neural Implicit Morphing of Face Images. (arXiv:2308.13888v1 [cs.CV])

    [http://arxiv.org/abs/2308.13888](http://arxiv.org/abs/2308.13888)

    本论文提出了一种利用神经网络实现人脸图像变形和混合的方法，通过利用网络的平滑性和灵活性，结合经典方法中的能量函数，实现了高效、准确和多样化的人脸变形效果。

    

    人脸变形是计算机图形学中的一个重要问题，具有众多艺术和取证应用。由于姿态、光照、性别和种族的变化，它一直以来都是一个具有挑战性的问题。通常，这个任务包括特征对齐的变形和无缝过渡的混合。我们提出利用基于坐标的神经网络来表示人脸图像的这种变形和混合。在训练过程中，我们利用这种网络的平滑性和灵活性，结合了经典方法中使用的能量函数，而无需进行离散化。此外，我们的方法是时间依赖的，允许对目标图像进行连续的变形和混合。在变形推理过程中，我们需要时间依赖变形的直接和逆变换。前者负责将目标图像变形为源图像，而后者则用于在相反方向进行变形。我们的神经变形网络具有高效、准确和多样化的性能。

    Face morphing is one of the seminal problems in computer graphics, with numerous artistic and forensic applications. It is notoriously challenging due to pose, lighting, gender, and ethnicity variations. Generally, this task consists of a warping for feature alignment and a blending for a seamless transition between the warped images.  We propose to leverage coordinate-based neural networks to represent such warpings and blendings of face images. During training, we exploit the smoothness and flexibility of such networks, by combining energy functionals employed in classical approaches without discretizations. Additionally, our method is time-dependent, allowing a continuous warping, and blending of the target images.  During warping inference, we need both direct and inverse transformations of the time-dependent warping. The first is responsible for morphing the target image into the source image, while the inverse is used for morphing in the opposite direction. Our neural warping sto
    
[^79]: 特征强化物理信息神经网络（FE-PINN）：在目标任务之前学习底层物理特征的框架

    Feature Enforcing PINN (FE-PINN): A Framework to Learn the Underlying-Physics Features Before Target Task. (arXiv:2308.08873v1 [cs.LG])

    [http://arxiv.org/abs/2308.08873](http://arxiv.org/abs/2308.08873)

    FE-PINN是一种学习底层物理特征的框架，在主训练之前以低计算成本解决问题的模式。与传统PINN相比，FE-PINN通过执行一系列子任务来解决损失函数不平衡的问题，并具有快速训练和更高的求解速度。

    

    本文介绍了一种名为特征强化物理信息神经网络（FE-PINN）的新型无数据框架。该框架能够在主训练循环之前以较低的计算成本学习任何问题的底层模式。由于存在偏微分残差和边界条件均方误差两个项，普通PINN的损失函数不平衡。FE-PINN通过只需一分钟的训练，而不是耗时数小时的超参数调优来解决这个挑战。FE-PINN通过执行一系列子任务来完成这个过程。第一个子任务学习有关底层物理的有用特征。然后，模型在目标任务上进行训练以完善计算。FE-PINN应用于三个基准问题：圆柱体上的流动、二维热传导以及计算入口速度的逆问题。FE-PINN可以分别加速15倍、2倍和5倍地解决每个案例。另外

    In this work, a new data-free framework called Feature Enforcing Physics Informed Neural Network (FE-PINN) is introduced. This framework is capable of learning the underlying pattern of any problem with low computational cost before the main training loop. The loss function of vanilla PINN due to the existence of two terms of partial differential residuals and boundary condition mean squared error is imbalanced. FE-PINN solves this challenge with just one minute of training instead of time-consuming hyperparameter tuning for loss function that can take hours. The FE-PINN accomplishes this process by performing a sequence of sub-tasks. The first sub-task learns useful features about the underlying physics. Then, the model trains on the target task to refine the calculations. FE-PINN is applied to three benchmarks, flow over a cylinder, 2D heat conduction, and an inverse problem of calculating inlet velocity. FE-PINN can solve each case with, 15x, 2x, and 5x speed up accordingly. Another
    
[^80]: 关于基于日志表示方法的日志异常检测效果的研究

    On the Effectiveness of Log Representation for Log-based Anomaly Detection. (arXiv:2308.08736v1 [cs.SE])

    [http://arxiv.org/abs/2308.08736](http://arxiv.org/abs/2308.08736)

    本研究调查和比较了先前日志分析研究中常用的日志表示技术，并评估了它们在不同机器学习模型和公共日志数据上的表现。

    

    日志是人们了解软件系统运行状态的重要信息来源。由于现代软件架构和维护方法的不断演变，越来越多的研究工作致力于自动化日志分析。在基于机器学习的日志分析任务中，将文本日志数据转换为数字特征向量是一个至关重要且必不可少的步骤。然而，不同的日志表示技术对下游模型性能的影响尚不清楚，这限制了研究人员和实践者选择其自动化日志分析工作流程中最佳日志表示技术的机会。因此，本研究调查并比较了先前日志分析研究中常用的日志表示技术。具体而言，我们选择了六种日志表示技术，并使用七种机器学习模型和四种公共日志数据进行评估。

    Logs are an essential source of information for people to understand the running status of a software system. Due to the evolving modern software architecture and maintenance methods, more research efforts have been devoted to automated log analysis. In particular, machine learning (ML) has been widely used in log analysis tasks. In ML-based log analysis tasks, converting textual log data into numerical feature vectors is a critical and indispensable step. However, the impact of using different log representation techniques on the performance of the downstream models is not clear, which limits researchers and practitioners' opportunities of choosing the optimal log representation techniques in their automated log analysis workflows. Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research. Particularly, we select six log representation techniques and evaluate them with seven ML models and four public log datas
    
[^81]: 通过RNNs实现线性时不变系统的通用逼近：随机性在储备计算中的作用

    Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing. (arXiv:2308.02464v1 [eess.SP])

    [http://arxiv.org/abs/2308.02464](http://arxiv.org/abs/2308.02464)

    通过随机性的储备计算，RNNs可以通用逼近线性时不变系统，这一观察到的性能在理论上得到了支持。

    

    循环神经网络(RNNs)以相对温和和普适的条件被认为是动态系统的通用近似器，使其成为处理时间信息的良好工具。然而，RNNs通常受到标准RNN训练中梯度消失和爆炸的问题的影响。储备计算(RC)是一种特殊的RNN，其中的循环权重是随机化并留在未经训练状态，它被引入用于克服这些问题，并在诸如自然语言处理和无线通信等领域展现出卓越的实证性能，特别是在训练样本极其有限的情况下。然而，支持这种观察到的性能的理论基础并未以相同的速度完全发展。在这项工作中，我们展示了RNNs可以提供线性时不变(LTI)系统的通用逼近。具体而言，我们展示了RC可以对一般LTI系统进行全面逼近。我们提出了一个明确的信号处理方法。

    Recurrent neural networks (RNNs) are known to be universal approximators of dynamic systems under fairly mild and general assumptions, making them good tools to process temporal information. However, RNNs usually suffer from the issues of vanishing and exploding gradients in the standard RNN training. Reservoir computing (RC), a special RNN where the recurrent weights are randomized and left untrained, has been introduced to overcome these issues and has demonstrated superior empirical performance in fields as diverse as natural language processing and wireless communications especially in scenarios where training samples are extremely limited. On the contrary, the theoretical grounding to support this observed performance has not been fully developed at the same pace. In this work, we show that RNNs can provide universal approximation of linear time-invariant (LTI) systems. Specifically, we show that RC can universally approximate a general LTI system. We present a clear signal proces
    
[^82]: PIGEON: 预测图像地理位置

    PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])

    [http://arxiv.org/abs/2307.05845](http://arxiv.org/abs/2307.05845)

    PIGEON是一个用于全球规模图像地理定位的多任务端到端系统，通过语义地理单元的创建和精化，以及无监督聚类和ProtoNets的应用，实现了最先进的性能，并提供了预训练的CLIP转换器模型StreetCLIP。

    

    我们引入PIGEON，一个用于全球规模图像地理定位的多任务端到端系统，在外部基准测试和人类评估中均实现了最先进的性能。我们的工作结合语义地理单元的创建和标签平滑，对具有地理信息的图像进行视觉转换器的预训练，并通过ProtoNets在候选地理单元集合中改进位置预测。PIGEON的贡献有三个方面：首先，我们设计了一种基于开源数据的语义地理单元创建和分割算法，可以适用于任何地理空间数据集。第二，我们展示了地理单元内部精化的有效性，并展示了无监督聚类和ProtoNets在该任务中的适用性。最后，我们将我们预训练的CLIP转换器模型，StreetCLIP，公开提供，可用于与应对气候变化和城市乡村场景理解相关的领域。

    We introduce PIGEON, a multi-task end-to-end system for planet-scale image geolocalization that achieves state-of-the-art performance on both external benchmarks and in human evaluation. Our work incorporates semantic geocell creation with label smoothing, conducts pretraining of a vision transformer on images with geographic information, and refines location predictions with ProtoNets across a candidate set of geocells. The contributions of PIGEON are three-fold: first, we design a semantic geocells creation and splitting algorithm based on open-source data which can be adapted to any geospatial dataset. Second, we show the effectiveness of intra-geocell refinement and the applicability of unsupervised clustering and ProtNets to the task. Finally, we make our pre-trained CLIP transformer model, StreetCLIP, publicly available for use in adjacent domains with applications to fighting climate change and urban and rural scene understanding.
    
[^83]: 合成数据重排加速异构数据下联邦学习的收敛(arXiv:2306.13263v1 [cs.LG])

    Synthetic data shuffling accelerates the convergence of federated learning under data heterogeneity. (arXiv:2306.13263v1 [cs.LG])

    [http://arxiv.org/abs/2306.13263](http://arxiv.org/abs/2306.13263)

    本文提出了一种通过对本地生成的合成数据进行重排来加速异构数据下联邦学习的收敛的方法，实验表明，对合成数据进行重排可以大幅提高现有多个联邦学习算法的性能。

    

    在联邦学习中，数据异构性是一个关键的挑战。一个简单的解决方案是对客户端的数据进行洗牌，以同质化分布。然而，这可能会违反数据访问权利，而对于在何时以及如何重排可以加速联邦优化算法的收敛，目前尚未在理论上得到很好的理解。本文建立了数据异构性与收敛速率参数之间的精确可量化的对应关系，证明了重排可以按百分比平方减少梯度差异，从而加速收敛。受理论启发，我们提出了一种通过对本地生成的合成数据进行重排来解决数据访问权问题的实用方法。实验结果表明，对合成数据进行重排可以大幅提高现有多个联邦学习算法的性能。

    In federated learning, data heterogeneity is a critical challenge. A straightforward solution is to shuffle the clients' data to homogenize the distribution. However, this may violate data access rights, and how and when shuffling can accelerate the convergence of a federated optimization algorithm is not theoretically well understood. In this paper, we establish a precise and quantifiable correspondence between data heterogeneity and parameters in the convergence rate when a fraction of data is shuffled across clients. We prove that shuffling can quadratically reduce the gradient dissimilarity with respect to the shuffling percentage, accelerating convergence. Inspired by the theory, we propose a practical approach that addresses the data access rights issue by shuffling locally generated synthetic data. The experimental results show that shuffling synthetic data improves the performance of multiple existing federated learning algorithms by a large margin.
    
[^84]: MimiC：模仿中心更新解决联邦学习中的客户端退出问题

    MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v1 [cs.LG])

    [http://arxiv.org/abs/2306.12212](http://arxiv.org/abs/2306.12212)

    本文提出的 MimiC 算法解决了联邦学习中客户端退出问题，通过模仿缺失的客户端更新解决了聚合更新和期望中心更新之间的分歧，实现了更高的测试准确率和更低的通信成本。

    

    联邦学习是一种有前途的隐私保护协作学习框架。在联邦学习中，模型训练任务分发给客户端，只需要在中央服务器收集模型更新。然而，在移动边缘网络中部署时，客户端（如智能手机和可穿戴设备）可能会无预警地退出任何一次训练迭代，这会阻碍联邦学习达到收敛。本文解决了联邦学习中这一关键挑战，设计出一种名为 MimiC 的新型训练算法，该算法在中心服务器修改其更新以模仿缺失客户端更新，通过实验结果显示，MimiC 相对现有方法在多个基准数据集上均取得了更高的测试准确率和更低的通信成本。

    Federated learning (FL) is a promising framework for privacy-preserving collaborative learning. In FL, the model training tasks are distributed to clients and only the model updates need to be collected at a central server. However, when being deployed at the mobile edge network, clients (e.g., smartphones and wearables) may have unpredictable availability and randomly drop out of any training iteration, which hinders FL from achieving the convergence. This paper tackles such a critical challenge of FL. In particular, we first investigate the convergence of the classical FedAvg algorithm with arbitrary client dropouts. We find that with the common choice of a decaying learning rate, FedAvg can only oscillate within the neighborhood of a stationary point of the global loss function, which is caused by the divergence between the aggregated update and the desired central update. Motivated by this new observation, we then design a novel training algorithm named MimiC, where the server modi
    
[^85]: 自监督学习在时间序列分析中的应用：分类、进展和前景

    Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects. (arXiv:2306.10125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.10125](http://arxiv.org/abs/2306.10125)

    自监督学习（SSL）在时间序列分析中的应用取得了显著性能，通过减少对标注数据的依赖，即使只有少量标注数据，也能实现高性能。

    

    自监督学习（SSL）最近在各种时间序列任务上取得了令人瞩目的性能。SSL最突出的优势是减少对标注数据的依赖。基于预训练和微调策略，即使只有少量标注数据，也可以实现高性能。与许多关于计算机视觉和自然语言处理的自监督学习综述相比，目前还缺乏针对时间序列SSL的综述。为了填补这一空白，本文回顾了当前时间序列数据中的自监督学习（SSL）方法的最新研究进展。为此，我们首先全面回顾了与自监督学习（SSL）和时间序列相关的现有综述，然后通过总结从生成型、对比型和对抗型三个角度对现有时间序列自监督学习方法进行了新的分类。这些方法进一步细分为十个子类，详细回顾和讨论了它们的关键直觉、主要框架、优势和限制。

    Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages an
    
[^86]: DoubleAdapt：一种用于股票趋势预测的增量学习元学习方法

    DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v1 [q-fin.ST])

    [http://arxiv.org/abs/2306.09862](http://arxiv.org/abs/2306.09862)

    DoubleAdapt是一个增量学习的方法，用于股票趋势预测。它利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中，从而有效地适应数据和模型，减轻分布漂移的影响。

    

    股票趋势预测是量化投资的基本任务之一，准确预测价格趋势是不可或缺的。作为一项在线服务，股票数据随时随地持续到达。使用最新数据对预测模型进行增量更新是实用而高效的，因为这些新数据可能揭示了未来股票市场中会重复出现的一些新模式。然而，由于分布漂移（即概念漂移）的挑战，股票趋势预测的增量学习仍然没有得到充分探索。随着股票市场动态演变，未来数据的分布可能会与增量数据稍微或显着地不同，从而阻碍增量更新的有效性。为了解决这一挑战，我们提出了一个利用两个适配器的端到端框架——DoubleAdapt，可以有效地适应数据和模型，以减轻分布漂移的影响。我们的关键洞察力是利用元学习技术自动学习如何将股票数据适应到本地平稳分布空间中。

    Stock trend forecasting is a fundamental task of quantitative investment where precise predictions of price trends are indispensable. As an online service, stock data continuously arrive over time. It is practical and efficient to incrementally update the forecast model with the latest data which may reveal some new patterns recurring in the future stock market. However, incremental learning for stock trend forecasting still remains under-explored due to the challenge of distribution shifts (a.k.a. concept drifts). With the stock market dynamically evolving, the distribution of future data can slightly or significantly differ from incremental data, hindering the effectiveness of incremental updates. To address this challenge, we propose DoubleAdapt, an end-to-end framework with two adapters, which can effectively adapt the data and the model to mitigate the effects of distribution shifts. Our key insight is to automatically learn how to adapt stock data into a locally stationary distri
    
[^87]: Fedstellar：一个去中心化联邦学习平台

    Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])

    [http://arxiv.org/abs/2306.09750](http://arxiv.org/abs/2306.09750)

    Fedstellar是一个联邦学习平台，支持物理或虚拟设备的去中心化、半去中心化和中心化的方式训练模型，旨在解决现有平台在处理异构联盟网络拓扑等问题时的挑战。

    

    2016年，谷歌提出了联邦学习（FL）作为一种新的范式，可以在保护数据隐私的同时跨联盟参与者训练机器学习（ML）模型。虽然中心化联邦学习（CFL）是最常用的方法，但它存在通信瓶颈、单点故障和对中央服务器的依赖等局限。去中心化联邦学习（DFL）通过实现去中心化模型聚合和最小化对中央实体的依赖，来解决这些问题。然而，目前训练DFL模型的平台在处理异构联盟网络拓扑等关键问题方面存在困难。为了克服这些挑战，本文提出了Fedstellar，这是一个新型的平台，旨在在物理或虚拟设备的不同联盟中以去中心化、半去中心化和中心化的方式训练FL模型。

    In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
    
[^88]: CHORUS: 统一数据发现和探索的基础模型。

    CHORUS: Foundation Models for Unified Data Discovery and Exploration. (arXiv:2306.09610v1 [cs.DB])

    [http://arxiv.org/abs/2306.09610](http://arxiv.org/abs/2306.09610)

    研究者探索将大型语言模型应用于数据发现和探索任务中，证明这些模型在表格类检测、列类型注释和联接列预测中具有优越性能，并有望将不同的数据管理任务统一在基础模型下。

    

    我们探索了将基础模型应用于数据发现和探索任务中。基础模型是一种大型语言模型 (LLMs)，在各种与其训练无关的不同任务上表现出了良好的性能。我们表明这些模型在数据发现和数据探索领域非常适用。在谨慎使用的情况下，它们具有优越的能力，可以优化表格类检测、列类型注释和联接列预测这三种代表性任务。在这三个任务上，我们展示了基于基础模型的方法优于任务特定的模型和最先进的技术。此外，我们的方法通常超过人类专家的任务表现。这表明了将不同的数据管理任务统一在基础模型下的未来方向。

    We explore the application of foundation models to data discovery and exploration tasks. Foundation models are large language models (LLMs) that show promising performance on a range of diverse tasks unrelated to their training. We show that these models are highly applicable to the data discovery and data exploration domain. When carefully used, they have superior capability on three representative tasks: table-class detection, column-type annotation and join-column prediction. On all three tasks, we show that a foundation-model-based approach outperforms the task-specific models and so the state of the art. Further, our approach often surpasses human-expert task performance. This suggests a future direction in which disparate data management tasks can be unified under foundation models.
    
[^89]: 通过非负低秩半定规划实现最优K均值聚类

    Statistically Optimal K-means Clustering via Nonnegative Low-rank Semidefinite Programming. (arXiv:2305.18436v1 [stat.ML])

    [http://arxiv.org/abs/2305.18436](http://arxiv.org/abs/2305.18436)

    本文提出了一种与NMF算法一样简单且可扩展的K均值聚类算法，该算法通过解决非负低秩半定规划问题获得了强大的统计最优性保证，实验证明该算法在合成和实际数据集上表现优异。

    

    K均值聚类是一种广泛应用于大数据集中发现模式的机器学习方法。半定规划（SDP）松弛最近被提出用于解决K均值优化问题，具有很强的统计最优性保证。但实现SDP求解器的巨大成本使得这些保证无法应用于实际数据集。相比之下，非负矩阵分解（NMF）是一种简单的聚类算法，被机器学习从业者广泛使用，但缺乏坚实的统计基础或严格的保证。在本文中，我们描述了一种类似于NMF的算法，通过使用非凸Burer-Monteiro分解方法解决半定规划松弛的K均值公式的非负低秩限制。所得到的算法与最先进的NMF算法一样简单和可扩展，同时也享有与SDP相同的强大的统计最优性保证。在我们的实验中，我们证明了我们的算法优于现有的NMF算法，并在合成和实际数据集上表现与最先进的SDP求解器相当。

    $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP) relaxations have recently been proposed for solving the $K$-means optimization problem that enjoy strong statistical optimality guarantees, but the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-rank restriction of the SDP relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments,
    
[^90]: 《Zero-TPrune: 基于预训练Transformers关注图的零射击令牌剪枝方法》

    Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. (arXiv:2305.17328v1 [cs.CV])

    [http://arxiv.org/abs/2305.17328](http://arxiv.org/abs/2305.17328)

    《Zero-TPrune》是一个考虑到令牌的重要性和相似性的零射击方法，它利用预训练Transformer模型的注意图来进行令牌剪枝，以求解在边缘设备上Transformer模型即插即用的难题。

    

    最近在边缘设备上部署Transformer模型变得越来越具有挑战性，原因是模型的体积呈指数级增长，而推理成本则随输入序列中令牌数量的平方提高。令牌剪枝是解决这一挑战的新兴解决方法之一，由于其易于在各种Transformer支持的模型上部署。然而，大多数令牌剪枝方法需要在剪枝后或期间进行计算密集型的微调过程，在许多情况下这是不可取的。最近的一些研究探讨了没有微调的即插即用的预训练Transformer的剪枝方法。但是，它们只考虑了令牌的重要性。在这项工作中，我们提出了Zero-TPrune，这是一种零射击方法，它既考虑令牌的重要性又考虑相似性来执行令牌剪枝。Zero-TPrune利用预训练Transformer模型的注意图来为令牌生成一个重要性排名并移除信息较少的令牌。注意矩阵可用于推断即插即用的模型。

    Deployment of Transformer models on the edge is increasingly challenging due to the exponentially growing model size and inference cost that scales quadratically with the number of tokens in the input sequence. Token pruning is an emerging solution to address this challenge due to its ease of deployment on various Transformer backbones. However, most token pruning methods require a computationally-expensive fine-tuning process after or during pruning, which is not desirable in many cases. Some recent works explore pruning of off-the-shelf pre-trained Transformers without fine-tuning. However, they only take the importance of tokens into consideration. In this work, we propose Zero-TPrune, the first zero-shot method that considers both the importance and similarity of tokens in performing token pruning. Zero-TPrune leverages the attention graph of pre-trained Transformer models to produce an importance rank for tokens and removes the less informative tokens. The attention matrix can be 
    
[^91]: AI增强的调查：利用大语言模型进行全国代表性调查的观点预测

    AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])

    [http://arxiv.org/abs/2305.09620](http://arxiv.org/abs/2305.09620)

    本论文研究了利用经过全国代表性调查微调的大语言模型（LLMs）来增强调查的观点预测，取得了在遗漏数据插值和回溯推理方面优秀的成果，在零次预测方面仍需进一步研究。

    

    本论文研究了如何使用经过全国代表性调查微调的大语言模型（LLMs）来增强调查。本文探讨了LLMs在观点预测中，遗漏数据插值，回溯推理和零次预测三个不同应用。我们提出了一种新的方法论框架，将调查问题、个人信念和时间背景的神经嵌入引入到观点预测的个性化LLMs中。在1972年到2021年的“常规社会调查”中，我们从68,846名美国人中获得了3,110个二进制观点，在Alpaca-7b模型的基础上取得了最好的成果，在缺失数据插值（AUC=0.87，公开观点预测为$\rho$=0.99）和回溯推理（AUC=0.86，$\rho$=0.98）方面表现出色。这些显著的预测能力能够以高置信度填补缺失的趋势，并标明公众态度何时发生变化，如同性婚姻的获取支持。然而，在零次预测的情况下，模型的表现受到限制，需要进一步研究。

    How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
    
[^92]: 基于扩散ODEs最大似然估计的改进技术

    Improved Techniques for Maximum Likelihood Estimation for Diffusion ODEs. (arXiv:2305.03935v1 [cs.LG])

    [http://arxiv.org/abs/2305.03935](http://arxiv.org/abs/2305.03935)

    本文提出了基于扩散ODEs最大似然估计的改进技术，包括速度参数化和方差减少技术等用于训练的技术，以及误差有界的高阶流匹配目标用于微调和截断正态去量化方法用于评估。

    

    扩散模型在各领域中表现出良好的性能。扩散模型的概率流常微分方程（ODE）（即扩散ODE）是连续归一化流（CNFs）的一个特例，它使得确定性推断和精确似然评估成为可能。然而，与最先进的基于似然的生成模型相比，扩散ODE的似然估计结果仍有很大差距。在本文中，我们提出了一些改进的技术，包括训练和评估两个方面，用于扩散ODE的最大似然估计。对于训练，我们提出了速度参数化，并探索方差减少技术以加快收敛速度。我们还设计了一个误差有界的高阶流匹配目标用于微调，从而提高ODE的似然估计并平滑其轨迹。对于评估，我们提出了一种新颖的无须训练的截断正态去量化方法来填补训练-评估间的差距。

    Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation (ODE) of diffusion models (i.e., diffusion ODEs) is a particular case of continuous normalizing flows (CNFs), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion ODEs are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion ODEs, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the ODE likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly
    
[^93]: 通向自由计算架构: 关于深度学习生成元宇宙虚拟建筑的综合调研

    Towards Computational Architecture of Liberty: A Comprehensive Survey on Deep Learning for Generating Virtual Architecture in the Metaverse. (arXiv:2305.00510v1 [cs.HC])

    [http://arxiv.org/abs/2305.00510](http://arxiv.org/abs/2305.00510)

    本文综述了当前最新的深度学习生成模型用于建筑形式的3D对象生成方法，强调了尚未充分探讨的问题，并提出了未来研究的重点议程。

    

    利用深度学习的3D形状生成技术正在受到计算机视觉和建筑设计两方的越来越多的关注。本综合调查旨在调查和比较当前最新的基于深度生成模型（DGMs）的3D对象生成方法，包括生成对抗网络（GANs）、变分自动编码器（VAEs）、3D感知图像和扩散模型。我们调查了187篇文章(占2018-2022年间发表文章的80.7%)，以回顾在虚拟环境下建筑生成可能性的领域，限于建筑形式。我们提供了建筑研究、虚拟环境和相关技术方法的概述，接着回顾了离散体素生成、由2D图像生成的3D模型以及条件参数的最近趋势。我们强调了3D生成和参数化控制中尚未充分探讨的问题值得进一步研究。此外，我们推测包括生成多样性、新型输出和嵌入式构建等四个研究议程可能会成为未来研究的重点。

    3D shape generation techniques utilizing deep learning are increasing attention from both computer vision and architectural design. This survey focuses on investigating and comparing the current latest approaches to 3D object generation with deep generative models (DGMs), including Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), 3D-aware images, and diffusion models. We discuss 187 articles (80.7% of articles published between 2018-2022) to review the field of generated possibilities of architecture in virtual environments, limited to the architecture form. We provide an overview of architectural research, virtual environment, and related technical approaches, followed by a review of recent trends in discrete voxel generation, 3D models generated from 2D images, and conditional parameters. We highlight under-explored issues in 3D generation and parameterized control that is worth further investigation. Moreover, we speculate that four research agendas including
    
[^94]: Deep Stock: 使用深度学习进行训练和交易的方案

    Deep Stock: training and trading scheme using deep learning. (arXiv:2304.14870v1 [q-fin.ST])

    [http://arxiv.org/abs/2304.14870](http://arxiv.org/abs/2304.14870)

    本文提出了一种使用深度学习进行训练和交易的方案，DeepStock通过查看股票价格的过去数据，并使用Resnet和logits来预测股票价格在未来D天内是否会升降一定百分比，并在韩国和美国市场上取得了超过市场回报的利润。

    

    尽管有效市场假说存在，但许多研究表明股票市场存在失灵现象，导致出现了一些能够获得超过市场回报的技术，即alpha。近几十年来，系统性交易已经取得了重大进展，深度学习作为分析和预测市场行为的强大工具已经开始崭露头角。本文中，我们提出了一种受专业交易员启发的模型，该模型查看先前的600天的股票价格，并预测股票价格在接下来D天内是否会升降一定百分比。我们的模型称为DeepStock，使用Resnet的跳跃连接和logits来增加模型在交易方案中的概率。我们在韩国和美国股票市场上测试了我们的模型，并在韩国市场上获得了N％的利润，超过市场回报M％，并在美国市场上获得了A％的利润，超过市场回报B％。

    Despite the efficient market hypothesis, many studies suggest the existence of inefficiencies in the stock market, leading to the development of techniques to gain above-market returns, known as alpha. Systematic trading has undergone significant advances in recent decades, with deep learning emerging as a powerful tool for analyzing and predicting market behavior. In this paper, we propose a model inspired by professional traders that look at stock prices of the previous 600 days and predicts whether the stock price rises or falls by a certain percentage within the next D days. Our model, called DeepStock, uses Resnet's skip connections and logits to increase the probability of a model in a trading scheme. We test our model on both the Korean and US stock markets and achieve a profit of N\% on Korea market, which is M\% above the market return, and profit of A\% on US market, which is B\% above the market return.
    
[^95]: 采用可解释的符号化神经模型检测上下文不符的多模态谣言

    Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model. (arXiv:2304.07633v1 [cs.CL])

    [http://arxiv.org/abs/2304.07633](http://arxiv.org/abs/2304.07633)

    本论文提出了一种可解释的神经符号模型，用于检测上下文不符的虚假多模态信息，帮助事实检查网站进行记录澄清。

    

    近年来，虚假信息的演化持续增长，旨在影响公众舆论。与传统的谣言或虚假新闻编辑主要依赖于生成和/或伪造的图像、文本和视频不同，当前的虚假信息创作者更倾向于使用上下文不匹配的多媒体内容（例如，不匹配的图像和标题）来欺骗公众和虚假新闻检测系统。这种新型的虚假信息不仅增加了检测的难度，也增加了澄清的难度，因为每个单独的模态都足够接近真实信息。为了解决这个问题，在本文中，我们探讨了如何实现可解释的跨模态去上下文检测，同时识别不匹配的对和跨模态矛盾，这对事实检查网站的记录澄清非常有帮助。所提出的模型首先通过抽象多模态信息，基于Abstract M进行符号化分解，得到一组事实查询。

    Recent years have witnessed the sustained evolution of misinformation that aims at manipulating public opinions. Unlike traditional rumors or fake news editors who mainly rely on generated and/or counterfeited images, text and videos, current misinformation creators now more tend to use out-of-context multimedia contents (e.g. mismatched images and captions) to deceive the public and fake news detection systems. This new type of misinformation increases the difficulty of not only detection but also clarification, because every individual modality is close enough to true information. To address this challenge, in this paper we explore how to achieve interpretable cross-modal de-contextualization detection that simultaneously identifies the mismatched pairs and the cross-modal contradictions, which is helpful for fact-check websites to document clarifications. The proposed model first symbolically disassembles the text-modality information to a set of fact queries based on the Abstract M
    
[^96]: 不需重新搜索的研究：最大更新参数化可精确预测跨尺度的损失

    Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])

    [http://arxiv.org/abs/2304.06875](http://arxiv.org/abs/2304.06875)

    提出了一种新的方法muP，可以提高超参数的缩放律的拟合精度，减少对大模型超参数的搜索，从而实现在大规模模型上进行损失预测。

    

    随着语言模型的扩大，验证研究想法变得越来越昂贵，因为小模型的结论不能简单地转移到大模型。解决方案是建立一个通用系统，仅基于小模型的结果和超参数直接预测大模型的一些指标。现有的基于缩放律的方法需要在最大的模型上进行超参数搜索，但由于资源有限，这是不切实际的。我们通过展示我们的发现表明，最大更新参数化（muP）使得可以在靠近常见损失流域的超参数的情况下准确拟合超参数的缩放律，而不需要任何搜索。因此，不同的模型可以在大尺度上进行损失预测，在训练开始之前就可以进行直接比较。我们提出了一种新的范式，作为可靠的学术研究的第一步，适用于任何模型规模，而不需大量的计算。代码将很快公开可用。

    As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
    
[^97]: 基于张量的多模态学习预测心脏MRI中肺动脉楔压力

    Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI. (arXiv:2303.07540v1 [cs.LG])

    [http://arxiv.org/abs/2303.07540](http://arxiv.org/abs/2303.07540)

    提出了一种基于张量学习的流程，从多模态心脏磁共振成像（MRI）中识别肺动脉楔压力（PAWP）。通过整合多种特征信息，提高了识别准确度。

    

    心衰是一种严重的生命威胁疾病，会导致左心室压力升高。肺动脉楔压力（PAWP）是一个重要的代理标志，表示左心室的高压。PAWP 由右心导管检查（RHC）确定，但它是一种有创性的过程。通过非侵入性的方法，可以快速地从大量人群中识别高危患者。在这项工作中，我们开发了一个基于张量学习的流程，从多模态心脏磁共振成像（MRI）中识别PAWP。这个流程提取高维扫描中的空间和时间特征。为了质量控制，我们采用认识不确定性为基础的分组策略，以识别质量差的训练样本。为了提高性能，我们通过整合多模态数据的特征：心脏MRI与短轴和四腔视图以及电子病历，学习互补信息。这项实验分析对大型数据集进行了验证。

    Heart failure is a serious and life-threatening condition that can lead to elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure (PAWP) is an important surrogate marker indicating high pressure in the left ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an invasive procedure. A non-invasive method is useful in quickly identifying high-risk patients from a large population. In this work, we develop a tensor learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic Resonance Imaging (MRI). This pipeline extracts spatial and temporal features from high-dimensional scans. For quality control, we incorporate an epistemic uncertainty-based binning strategy to identify poor-quality training samples. To improve the performance, we learn complementary information by integrating features from multimodal data: cardiac MRI with short-axis and four-chamber views, and Electronic Health Records. The experimental analysis on a large
    
[^98]: 特征分区聚合：一种快速的对$\ell_0$攻击的认证防御方法

    Feature Partition Aggregation: A Fast Certified Defense Against a Union of $\ell_0$ Attacks. (arXiv:2302.11628v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.11628](http://arxiv.org/abs/2302.11628)

    本文提出了一种名为特征分区聚合的认证防御方法，用于对抗$\ell_0$逃避、后门和污染攻击。与现有防御方法相比，FPA速度更快，提供更大的鲁棒性保证，且能够免费提供额外的鲁棒性维度。

    

    稀疏的或$\ell_0$对抗攻击会任意扰动未知的特征子集。$\ell_0$鲁棒性分析特别适用于异构（表格）数据，其中特征具有不同的类型或尺度。目前最先进的$\ell_0$认证防御基于随机平滑，并仅适用于逃避攻击。本文提出了特征分区聚合（FPA）--一种针对$\ell_0$逃避、后门和污染攻击的认证防御。FPA通过集成生成更强的鲁棒性保证，其子模型是在不相交的特征集上训练的。与最先进的$\ell_0$防御相比，FPA速度提高了多达3000倍，并提供了更大的中位数鲁棒性保证（例如，对于CIFAR10的中位数证书为13像素，MNIST的中位数证书为12像素，Weather的中位数证书为4个特征，Ames的中位数证书为3个特征），这意味着FPA能够免费提供额外的鲁棒性维度。

    Sparse or $\ell_0$ adversarial attacks arbitrarily perturb an unknown subset of the features. $\ell_0$ robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art $\ell_0$ certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of $\ell_0$ evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art $\ell_0$ defenses, FPA is up to 3,000${\times}$ faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.
    
[^99]: 在上下文强化学习领域进行在线连续超参数优化

    Online Continuous Hyperparameter Optimization for Contextual Bandits. (arXiv:2302.09440v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.09440](http://arxiv.org/abs/2302.09440)

    该论文提出了面向上下文强化学习的在线连续超参数调整框架CDT，能够动态地在搜索空间内学习最优参数配置。

    

    在随机上下文强化学习中，代理根据过去的经验从时间相关行动集中依次采取行动，以最小化总后悔。与许多其他机器学习算法一样，强化学习的性能严重依赖于其多个超参数，并且理论推导出的参数值可能导致实际上不令人满意的结果。此外，在强化学习环境下使用离线优化方法（如交叉验证）选择超参数是不可行的，因为决策必须实时进行。因此，我们提出了第一个面向上下文强化学习的在线连续超参数调整框架，以学习飞行中的最佳参数配置。具体而言，我们使用了一个名为CDT（Continuous Dynamic Tuning）的双层强化学习框架，并将超参数优化形式化为非平稳连续武器强化学习，在其中每个武器代表一种超参数组合。

    In stochastic contextual bandits, an agent sequentially makes actions from a time-dependent action set based on past experience to minimize the cumulative regret. Like many other machine learning algorithms, the performance of bandits heavily depends on their multiple hyperparameters, and theoretically derived parameter values may lead to unsatisfactory results in practice. Moreover, it is infeasible to use offline tuning methods like cross-validation to choose hyperparameters under the bandit environment, as the decisions should be made in real time. To address this challenge, we propose the first online continuous hyperparameter tuning framework for contextual bandits to learn the optimal parameter configuration within a search space on the fly. Specifically, we use a double-layer bandit framework named CDT (Continuous Dynamic Tuning) and formulate the hyperparameter optimization as a non-stationary continuum-armed bandit, where each arm represents a combination of hyperparameters, a
    
[^100]: 使用非特定运动数据的可扩展XR用户基于运动的识别

    Extensible Motion-based Identification of XR Users using Non-Specific Motion Data. (arXiv:2302.07517v2 [cs.HC] UPDATED)

    [http://arxiv.org/abs/2302.07517](http://arxiv.org/abs/2302.07517)

    提出了一种可扩展XR用户基于运动的识别方法。与现有基线方法相比，该方法通过仅使用少量的注册数据来识别新用户，可以在几秒钟内注册新用户，而且在仅有少量注册数据可用时也更可靠。

    

    本文提出了一种基于嵌入式和深度度量学习结合的方法，将距离和分类两种方法的优势相结合，用于通过用户的运动来识别扩展现实用户。我们在“半衰期：Alyx”VR游戏的用户数据集上进行了模型训练，并使用现有的基线分类模型作为对比。研究结果表明，基于嵌入式的方法可以通过只使用几分钟的注册数据，识别新用户的非特定运动，可以在几秒钟内注册新用户，而重新训练基线方法需要花费将近一天的时间，当只有很少的注册数据可用时，比基线方法更可靠，可以用于识别使用不同VR设备记录的新用户数据集。综上所述，我们的解决方案为易于扩展的XR用户识别系统奠定基础，可应用于广泛场景。

    In this paper, we combine the strengths of distance-based and classification-based approaches for the task of identifying extended reality users by their movements. For this we present an embedding-based approach that leverages deep metric learning. We train the model on a dataset of users playing the VR game ``Half-Life: Alyx'' and conduct multiple experiments and analyses using a state of the art classification-based model as baseline. The results show that the embedding-based method 1) is able to identify new users from non-specific movements using only a few minutes of enrollment data, 2) can enroll new users within seconds, while retraining the baseline approach takes almost a day, 3) is more reliable than the baseline approach when only little enrollment data is available, 4) can be used to identify new users from another dataset recorded with different VR devices.  Altogether, our solution is a foundation for easily extensible XR user identification systems, applicable to a wide
    
[^101]: 相对熵正则化的经验风险最小化问题

    Empirical Risk Minimization with Relative Entropy Regularization. (arXiv:2211.06617v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2211.06617](http://arxiv.org/abs/2211.06617)

    本文研究了相对熵正则化的经验风险最小化问题，其中参考度量为sigma有限测度，解为唯一的概率测度并展现了几乎正确的保证。ERM-RER问题的解被称为Gibbs算法。

    

    在假定参考度量为sigma有限测度（measure）而非概率测度的情况下，研究了相对熵正则化的经验风险最小化（ERM-RER）问题。在这种假设下，存在一个ERM-RER问题的泛化，允许更大程度地灵活地并入先验知识。在这些性质中，如果存在ERM-RER问题的解，则该解是唯一的概率测度，通常与参考测度相互绝对连续。这样的解对于ERM问题展现了几乎正确的保证，而不需关心ERM问题是否有解。当从ERM-RER问题的解抽取模型时，固定数据集时，经验风险被证明是一个亚高斯随机变量。ERM-RER问题的解（Gibbs算法）的泛化能力得到了验证。

    The empirical risk minimization (ERM) problem with relative entropy regularization (ERM-RER) is investigated under the assumption that the reference measure is a {\sigma}-finite measure, and not necessarily a probability measure. Under this assumption, which leads to a generalization of the ERM-RER problem allowing a larger degree of flexibility for incorporating prior knowledge, numerous relevant properties are stated. Among these properties, the solution to this problem, if it exists, is shown to be a unique probability measure, often mutually absolutely continuous with the reference measure. Such a solution exhibits a probably-approximately-correct guarantee for the ERM problem independently of whether the latter possesses a solution. For a fixed dataset, the empirical risk is shown to be a sub-Gaussian random variable when the models are sampled from the solution to the ERM-RER problem. The generalization capabilities of the solution to the ERM-RER problem (the Gibbs algorithm) are
    
[^102]: 机器学习在轨道估计中的应用：一项综述

    Machine Learning in Orbit Estimation: a Survey. (arXiv:2207.08993v3 [astro-ph.EP] UPDATED)

    [http://arxiv.org/abs/2207.08993](http://arxiv.org/abs/2207.08993)

    本综述介绍了机器学习在轨道估计中的应用现状，讨论了当前物理方法的不足，提出了通过推导未测量物体的特征来提高轨道预测准确度的方案。

    

    自从上世纪50年代发射了第一颗人造卫星以来，轨道上的空间物体数量持续增加。目前估计地球上有一百万个大小超过一厘米的物体正在绕地运行，其中只有三万个大小超过十厘米的物体被跟踪。为了避免碰撞链反应，即基斯勒症候群的发生，必须准确地跟踪和预测碎片和卫星的轨道。当前的近似物理方法对于七天的预测误差在公里级别，这在考虑通常小于一米的空间碎片时是不足够的。这种失败通常是由于在轨道起始点附近的空间物体状态的不确定性，环境条件（如大气阻力）的预测误差以及空间物体的未知特征（如质量或几何形状）所致。操作员可以通过推导未测量物体的特征来提高轨道预测的准确性。

    Since the late 1950s, when the first artificial satellite was launched, the number of Resident Space Objects has steadily increased. It is estimated that around one million objects larger than one cm are currently orbiting the Earth, with only thirty thousand larger than ten cm being tracked. To avert a chain reaction of collisions, known as Kessler Syndrome, it is essential to accurately track and predict debris and satellites' orbits. Current approximate physics-based methods have errors in the order of kilometers for seven-day predictions, which is insufficient when considering space debris, typically with less than one meter. This failure is usually due to uncertainty around the state of the space object at the beginning of the trajectory, forecasting errors in environmental conditions such as atmospheric drag, and unknown characteristics such as the mass or geometry of the space object. Operators can enhance Orbit Prediction accuracy by deriving unmeasured objects' characteristics
    
[^103]: 非静态适应性：面向在线凸优化的问题相关的动态遗憾

    Adaptivity and Non-stationarity: Problem-dependent Dynamic Regret for Online Convex Optimization. (arXiv:2112.14368v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2112.14368](http://arxiv.org/abs/2112.14368)

    本研究提出一种面向在线凸优化的动态遗憾算法，可以在一些简单的问题实例中进一步增强保证，具有几何直观性，实验表明其优于最先进的基线算法。

    

    本文研究了在非静态环境中的在线凸优化，并选择动态遗憾作为性能度量，定义为在线算法和任何可行比较器序列所累计的损失之间的差值。假设$T$是时间长度，$P_T$是实质上反映环境非静态性的路径长度，则最先进的动态遗憾是$\mathcal{O}(\sqrt{T(1+P_T)})$。尽管这个界限被证明对于凸函数是最小化的，但我们在本文中展示，在一些简单的问题实例中，特别是当在线函数是光滑的时候，可以进一步增强保证。具体地，我们介绍了新颖的在线算法，可以利用光滑性，并用损失函数的梯度变化、比较器序列的累计损失和这两个项的最小值代替动态遗憾中对$T$的依赖。这些量被证明具有几何直观性，并且与环境中的非静态现象密切相关。对合成和真实数据集的广泛实验表明，我们的算法优于最先进的基线算法。

    We investigate online convex optimization in non-stationary environments and choose the dynamic regret as the performance measure, defined as the difference between cumulative loss incurred by the online algorithm and that of any feasible comparator sequence. Let $T$ be the time horizon and $P_T$ be the path length that essentially reflects the non-stationarity of environments, the state-of-the-art dynamic regret is $\mathcal{O}(\sqrt{T(1+P_T)})$. Although this bound is proved to be minimax optimal for convex functions, in this paper, we demonstrate that it is possible to further enhance the guarantee for some easy problem instances, particularly when online functions are smooth. Specifically, we introduce novel online algorithms that can exploit smoothness and replace the dependence on $T$ in dynamic regret with problem-dependent quantities: the variation in gradients of loss functions, the cumulative loss of the comparator sequence, and the minimum of these two terms. These quantitie
    

