# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions](https://rss.arxiv.org/abs/2312.15101) | 本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。 |
| [^2] | [SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models](https://arxiv.org/abs/2403.17933) | SLEDGE是第一个使用生成模型训练的车辆运动规划生成模拟器，引入了新颖的栅格到矢量自编码器（RVAE）以及Diffusion Transformer来生成智能体和车道图，从而实现更好的模拟控制。 |
| [^3] | [The Need for Speed: Pruning Transformers with One Recipe](https://arxiv.org/abs/2403.17921) | 提出了一种名为OPTIN的框架，利用一次性修剪技术和中间特征蒸馏来提高预训练Transformer架构的效率，无需重新训练，并在多项任务中取得最先进的结果。 |
| [^4] | [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) | 逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。 |
| [^5] | [CMP: Cooperative Motion Prediction with Multi-Agent Communication](https://arxiv.org/abs/2403.17916) | 该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。 |
| [^6] | [Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2](https://arxiv.org/abs/2403.17905) | 通过引入R2D2方法，提出了一种可扩展的非笛卡尔磁共振图像重建方法。 |
| [^7] | [Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models](https://arxiv.org/abs/2403.17902) | Serpent提出了一种新的图像恢复架构，利用状态空间模型在全局感受野和计算效率之间取得平衡，实现了与最先进技术相当的重建质量，但计算量减少了数个数量级。 |
| [^8] | [Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels](https://arxiv.org/abs/2403.17891) | 本研究提出了一种在深度学习分类器中利用分层标签提高未知故障检测性能的方法，同时不影响模型性能，在热轧钢过程中的实验结果表明其具有较好的可复制性。 |
| [^9] | [Large scale paired antibody language models](https://arxiv.org/abs/2403.17889) | IgBert和IgT5是迄今为止发展的最佳表现的抗体特定语言模型，可以处理配对和无配对可变区序列，并在训练中使用了超过20亿个无配对序列和两百万个配对序列。 |
| [^10] | [The Unreasonable Ineffectiveness of the Deeper Layers](https://arxiv.org/abs/2403.17887) | 层剪枝方法可以在流行的预训练语言模型中实现大部分层的移除而保持性能，同时使用参数高效的微调方法可以进一步减少计算资源，提高推断的内存和延迟。 |
| [^11] | [Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation](https://arxiv.org/abs/2403.17886) | 基于神经嵌入压缩的多任务嵌入方法在地球观测中实现了数据高效的训练和推断，通过压缩率与嵌入效用之间的权衡，取得了数据量显著减少的准确性。 |
| [^12] | [Empowering Data Mesh with Federated Learning](https://arxiv.org/abs/2403.17878) | 数据网格提出了一种去中心化的数据范式，通过将数据所有权分布到每个数据领域，同时保持联合治理，以克服数据源激增和及时分析处理需求增长的挑战。 |
| [^13] | [Sample complexity of quantum hypothesis testing](https://arxiv.org/abs/2403.17868) | 本文研究了量子假设检验的样本复杂度，得出了对称和非对称设置中的二进制量子假设检验的样本复杂度与反错误概率的对数和保真度的负对数的关系。 |
| [^14] | [Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic](https://arxiv.org/abs/2403.17853) | 通过将领域知识注入生成神经模型的潜在空间，提出了Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI)方法，用于引导对话结构识别，应对训练语料有限/嘈杂以及测试对话领域分布转变等问题。 |
| [^15] | [Counterfactual Fairness through Transforming Data Orthogonal to Bias](https://arxiv.org/abs/2403.17852) | 提出了一种新颖的数据预处理算法，正交于偏见（OB），通过确保数据与敏感变量不相关，实现机器学习应用中的反事实公平性。 |
| [^16] | [Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections](https://arxiv.org/abs/2403.17847) | 本研究提出了基于深度学习的气候细化超分辨率模型，结合了注意力块和跳跃连接，旨在有效预测降水数据，为缓解气候变化带来的影响和提高水资源利用效率做出贡献。 |
| [^17] | [Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation](https://arxiv.org/abs/2403.17846) | 提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。 |
| [^18] | [TractOracle: towards an anatomically-informed reward function for RL-based tractography](https://arxiv.org/abs/2403.17845) | TractOracle 提出了一种新的RL纤维束追踪系统，通过基于解剖学知识的奖励函数提高了真阳性比率并降低了虚假阳性比率 |
| [^19] | [Mechanistic Design and Scaling of Hybrid Architectures](https://arxiv.org/abs/2403.17844) | 通过机理设计和尺度变换，我们提出了一种新的混合体系结构设计方法，可以简化深度学习架构的开发过程，并可以准确评估新架构。 |
| [^20] | [GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction](https://arxiv.org/abs/2403.17837) | 介绍了一个大规模合成的HDR图像数据集 GTA-HDR，从GTA-V视频游戏中采样，填补了现有数据集在捕捉不同场景条件和图像特征方面的不足 |
| [^21] | [GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning](https://arxiv.org/abs/2403.17833) | GPFL提出了一个基于梯度投影的客户选择框架，通过比较本地和全局下降方向来衡量客户价值，并采用利用-探索机制来提高性能。在实验中表现出在非独立同分布场景中优于基线，在FEMINST测试准确性方面实现超过9%的改善。 |
| [^22] | [Learning the Optimal Power Flow: Environment Design Matters](https://arxiv.org/abs/2403.17831) | 研究通过实施不同的环境设计决策对强化学习解决最优潮流问题的影响，提出了对这些设计决策的首要建议。 |
| [^23] | [DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions](https://arxiv.org/abs/2403.17827) | 提出了一种从文本描述和物体几何形状中合成逼真的手-物体交互的方法，通过三种技术实现了有效学习，包括任务分解、紧密耦合的姿势表示和不同的引导方案。 |
| [^24] | [Are Compressed Language Models Less Subgroup Robust?](https://arxiv.org/abs/2403.17811) | 压缩语言模型的影响不仅取决于模型大小，还取决于压缩方法，同时发现模型压缩并不总是会使在少数子群体上的性能变差。 |
| [^25] | [Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields](https://arxiv.org/abs/2403.17808) | 提出了一种能够生成具有像素级注释的逼真合成显微视频的生物医学视频扩散模型，有助于解决生物医学成像领域标记数据稀缺的问题。 |
| [^26] | [Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms](https://arxiv.org/abs/2403.17806) | 提出了一种新方法EAP-IG，旨在更好地保持电路的核心属性：忠实 |
| [^27] | [Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving](https://arxiv.org/abs/2403.17805) | 提出了MATS-Gym，一个用于在CARLA中训练智能体的多智能体交通场景框架，能够自动生成具有可变智能体数量的交通场景并整合了各种现有的交通场景描述方法。 |
| [^28] | [Secure Aggregation is Not Private Against Membership Inference Attacks](https://arxiv.org/abs/2403.17775) | 本文探讨了安全聚合在隐私方面的问题，揭示了其在面对成员推断攻击时并不具备足够的私密性。 |
| [^29] | [SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation](https://arxiv.org/abs/2403.17768) | 科学新闻报道生成的自动化提高了学术见解的可访问性，该研究提出了一个包含学术出版物和相应科学新闻报道的数据集，用于探索自动生成科学新闻报道的可能性。 |
| [^30] | [Asymptotic Bayes risk of semi-supervised learning with uncertain labeling](https://arxiv.org/abs/2403.17767) | 论文研究了具有不确定标签的半监督学习中的渐近贝叶斯风险计算，并通过与最佳算法比较得出新的见解。 |
| [^31] | [Noise2Noise Denoising of CRISM Hyperspectral Data](https://arxiv.org/abs/2403.17757) | 引入了Noise2Noise4Mars（N2N4M）模型用于去除CRISM图像噪声，无需零噪声目标数据，展现出在合成噪声数据和CRISM图像上的强大性能，并具有优于基准方法的效果。 |
| [^32] | [CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model](https://arxiv.org/abs/2403.17753) | CCDSReFormer模型引入了三种创新模块来提高交通流预测的效率和准确性，包括增强整流空间自注意力、增强整流延迟感知自注意力和增强整流时间自注意力，以实现稀疏注意力、可解释的局部信息和融合空间和时间见解。 |
| [^33] | [Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients](https://arxiv.org/abs/2403.17745) | 提出了一种名为RAREMed的新模型，利用预训练-微调学习范式增强罕见疾病的药物推荐准确性，并引入了自监督预训练任务来学习专门的药物需求和临床代码之间的关系 |
| [^34] | [EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention](https://arxiv.org/abs/2403.17729) | EulerFormer提出了一种具有复杂向量注意力的新型转换器变体，统一了语义差异和位置差异的理论框架。 |
| [^35] | [Masked Autoencoders are PDE Learners](https://arxiv.org/abs/2403.17728) | 掩码自动编码器在偏微分方程求解器中表现出色，通过自监督学习跨越PDEs，可以学习用于下游任务的有用潜在表示。 |
| [^36] | [Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation](https://arxiv.org/abs/2403.17701) | 本文提出了Triplet Mamba-UNet，利用残余VSS块提取密集上下文特征，并利用Triplet SSM融合空间和通道维度上的特征。 |
| [^37] | [MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation](https://arxiv.org/abs/2403.17698) | 提出了MEP方法，通过结合不同核函数生成偏差来解决变压器模型在长度外推时的准确性降低问题 |
| [^38] | [PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition](https://arxiv.org/abs/2403.17695) | 改进了视觉识别中的非层次Mamba模型，通过改进连续2D扫描过程和方向感知更新，提高了从二维图像中学习特征的能力。 |
| [^39] | [Manifold-Guided Lyapunov Control with Diffusion Models](https://arxiv.org/abs/2403.17692) | 使用扩散模型进行流形引导的Lyapunov控制，通过识别最接近预定流形的渐近稳定向量场来生成稳定控制函数，展示了在快速零-shot控制和泛化能力方面的潜力。 |
| [^40] | [How Private is DP-SGD?](https://arxiv.org/abs/2403.17673) | ABLQ机制在不同批处理采样下的隐私保证存在实质性差距，DP-SGD的实际实现通常使用基于洗牌的方法，但更可靠的隐私分析却来自于基于泊松子采样的方法。 |
| [^41] | [CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations](https://arxiv.org/abs/2403.17660) | 通过训练神经网络系统CANOS，在保持速度的同时预测接近最优解（在1%内），从而解决了准确性和运行可行性之间的折衷问题 |
| [^42] | [SGHormer: An Energy-Saving Graph Transformer Driven by Spikes](https://arxiv.org/abs/2403.17656) | SGHormer是一种由脉冲驱动的节能图变换器，通过将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本，提高了图变换器的效率。 |
| [^43] | [Uncertainty-aware Distributional Offline Reinforcement Learning](https://arxiv.org/abs/2403.17646) | 提出了一种不确定性感知的分布式离线强化学习方法，同时解决认知不确定性和环境随机性，在风险敏感和规避设置下进行了全面实验评估 |
| [^44] | [PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning](https://arxiv.org/abs/2403.17637) | 引入了 PeersimGym 环境，通过强化学习解决任务卸载问题，支持定制化仿真环境，有助于开发和优化计算网络中的任务卸载策略。 |
| [^45] | [Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems](https://arxiv.org/abs/2403.17634) | 本研究提出了一种新的离线RL推荐系统方法，通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。 |
| [^46] | [Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset](https://arxiv.org/abs/2403.17632) | 提出了一个专门针对电动微移动工具在都柏林收集的开放数据集，为解决实际场景中能耗建模的困难提供了重要资源 |
| [^47] | [Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets](https://arxiv.org/abs/2403.17608) | 许多AI生成图像检测数据集存在与JPEG压缩和图像大小相关的偏见，去除这些偏见可以显著提高对JPEG压缩的稳健性并显著改变检测器的跨生成器性能。 |
| [^48] | [LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation](https://arxiv.org/abs/2403.17601) | 提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题 |
| [^49] | [On the Benefits of Over-parameterization for Out-of-Distribution Generalization](https://arxiv.org/abs/2403.17592) | 研究了超参数化模型在超出分布泛化方面的表现，探讨了在良性过拟合条件下的表现，并发现了恒定的超出分布损失。 |
| [^50] | [Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models](https://arxiv.org/abs/2403.17589) | 提出了双存储网络的多功能适应方法，能在零次适应、少次适应和无需训练的少次适应三种设置下高效运行 |
| [^51] | [Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models](https://arxiv.org/abs/2403.17588) | Forest-ORE方法提出了一种通过优化规则集使随机森林具有可解释性的方法，可以应用于全局和局部解释，同时考虑了影响可解释规则集选择的多个参数。 |
| [^52] | [Towards a Zero-Data, Controllable, Adaptive Dialog System](https://arxiv.org/abs/2403.17582) | 该论文提出了一种从对话树生成数据的方法，可帮助训练出在合成数据上训练的代理达到与在人类数据上训练的模型相媲美的对话成功率。 |
| [^53] | [Enhancing Privacy in Federated Learning through Local Training](https://arxiv.org/abs/2403.17572) | 提出了一种用于联邦学习的联邦私有本地训练算法（Fed-PLT），通过允许部分参与和本地训练，显著减少了通信轮次，同时不影响准确性，并研究了如何通过本地训练来增强隐私性。 |
| [^54] | [A Survey on Deep Learning and State-of-the-arts Applications](https://arxiv.org/abs/2403.17561) | 深度学习是解决复杂问题的强大工具，本研究旨在全面审视深度学习模型及其应用的最新发展 |
| [^55] | [DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping](https://arxiv.org/abs/2403.17550) | 提出了DeepMIF，通过设计学习系统集成单调性损失，在大规模3D地图绘制中优化神经单调场，避免了LiDAR测量的嘈杂问题 |
| [^56] | [VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts](https://arxiv.org/abs/2403.17542) | 通过价值差异和状态计数，利用代理的内部状态来决定何时进行探索，解决了盲目切换机制的缺点。 |
| [^57] | [BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat](https://arxiv.org/abs/2403.17533) | 研究者创建了一个强化学习环境BVR体育馆，用于探究超视距空中战斗领域的潜在战术，提供了基于开源飞行动力学模拟器JSBSim的高保真度环境。 |
| [^58] | [Boosting Adversarial Training via Fisher-Rao Norm-based Regularization](https://arxiv.org/abs/2403.17520) | 通过Fisher-Rao范数正则化，本研究在对抗训练中提出了一种解决标准泛化性能下降问题的方法，并通过模型复杂性角度对此进行了理论和实证分析。 |
| [^59] | [Prediction-sharing During Training and Inference](https://arxiv.org/abs/2403.17515) | 研究了在训练和推理期间的预测分享方面的新颖之处，并介绍和突出了不同类型的数据共享合同。 |
| [^60] | [EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields](https://arxiv.org/abs/2403.17507) | 提出了一种新颖的集成学习框架EL-MLFFs，利用堆叠方法整合来自不同MLFFs的预测，从而提高力预测准确性。 |
| [^61] | [DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning](https://arxiv.org/abs/2403.17503) | DS-AL提出了一种双流分析学习方法，通过主流和补偿流相结合，重新定义CIL问题，有效解决了无样本约束条件下的类增量学习问题。 |
| [^62] | [Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification](https://arxiv.org/abs/2403.17500) | VGAE框架在归纳学习领域的应用具有重要意义。 |
| [^63] | [Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost](https://arxiv.org/abs/2403.17480) | 该论文考虑了一种在线非凸优化问题，目标是通过调节活动服务器数量最小化作业延迟，引入了切换成本，提出了竞争算法。 |
| [^64] | [Natural Language Requirements Testability Measurement Based on Requirement Smells](https://arxiv.org/abs/2403.17479) | 通过提出一个基于需求异味的数学模型，本文介绍了一种评估和排名自然语言需求可测试性的方法，有助于衡量和量化需求的可测试性。 |
| [^65] | [A Unified Kernel for Neural Network Learning](https://arxiv.org/abs/2403.17467) | 本文提出了统一神经内核(UNK)，可以描述神经网络的学习动态，并在有限的学习步骤下表现出类似于NTK的行为，当学习步骤逼近无穷大时收敛到NNGP。 |
| [^66] | [Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice](https://arxiv.org/abs/2403.17458) | 论文通过实证比较不同入侵检测系统，发现最佳解决方案取决于外部变量，如攻击类型、复杂性和网络环境，深度神经网络在某些数据集上表现最佳，但并非始终是最佳选择。 |
| [^67] | [Imitating Cost-Constrained Behaviors in Reinforcement Learning](https://arxiv.org/abs/2403.17456) | 该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。 |
| [^68] | [Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks](https://arxiv.org/abs/2403.17447) | 提出了一种名为“压缩链”的系统化方法，通过结合量化、剪枝、提前退出和知识蒸馏等常见技术，实现对卷积神经网络的压缩。 |
| [^69] | [Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model](https://arxiv.org/abs/2403.17445) | 将简单的指数平滑法与MLP结合，通过增加参数和复杂性，实现了与复杂S4模型可比较的结果 |
| [^70] | [Particle identification with machine learning from incomplete data in the ALICE experiment](https://arxiv.org/abs/2403.17436) | 在ALICE实验中，我们利用机器学习方法和多神经网络进行粒子识别，包括特征集嵌入和注意力机制，以在不完整数据样本上进行训练，并将ML项目与ALICE分析软件集成，讨论了域自适应技术。 |
| [^71] | [Robust and Scalable Model Editing for Large Language Models](https://arxiv.org/abs/2403.17431) | 通过适当的提示方法，经过指令微调的大型语言模型可以高度控制上下文知识，并对无关上下文具有鲁棒性，提出了EREN（通过阅读笔记来编辑模型），以改善可扩展性。 |
| [^72] | [Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model](https://arxiv.org/abs/2403.17425) | 本文提出了一种Masked Multi-Domain Network，解决了使用单一模型进行多类型和多场景转化率预测时的准确性、可扩展性和便利性问题。 |
| [^73] | [On permutation-invariant neural networks](https://arxiv.org/abs/2403.17410) | 神经网络如Deep Sets和Transformers的出现显著推动了基于集合的数据处理的进展 |
| [^74] | [Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens](https://arxiv.org/abs/2403.17407) | 通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。 |
| [^75] | [Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study](https://arxiv.org/abs/2403.17404) | 本文讨论了稀疏专家混合模型在泛化误差方面的探索，特别关注了数据样本数量的影响。 |
| [^76] | [Application-Driven Innovation in Machine Learning](https://arxiv.org/abs/2403.17381) | 应用驱动研究在机器学习领域具有重要影响，可以与方法驱动研究有益地协同，但目前审查、招聘和教学实践往往阻碍了这种创新。 |
| [^77] | [Exploring and Applying Audio-Based Sentiment Analysis in Music](https://arxiv.org/abs/2403.17379) | 本文探讨了基于音频的情感分析在音乐中的运用，通过预测音乐片段随时间的情感变化以及确定音乐时间序列中下一个情感值来实现无缝过渡。 |
| [^78] | [Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance](https://arxiv.org/abs/2403.17377) | 提出了一种名为扰动注意力引导（PAG）的新型抽样引导技术，通过在扩散 U-Net 中替换自注意力映射来生成结构降级的中间样本，从而在无条件和有条件设置下改善扩散样本质量。 |
| [^79] | [AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving](https://arxiv.org/abs/2403.17373) | 提出了使用自动数据引擎（AIDE）的方法，通过自动识别问题、高效筛选数据、自动标注改进模型、生成多样化场景验证模型，在自动驾驶领域取得了优异的性能。 |
| [^80] | [A Moreau Envelope Approach for LQR Meta-Policy Estimation](https://arxiv.org/abs/2403.17364) | 提出了一种基于Moreau包络的替代LQR成本，可有效调整到新实现的元策略，并设计了找到近似一阶稳定点的算法。 |
| [^81] | [Multi-Objective Trajectory Planning with Dual-Encoder](https://arxiv.org/abs/2403.17353) | 通过引入基于双编码器的变压器模型和顺序二次规划优化，本文提出了一种加速时间-加速度最优轨迹规划的两阶段方法，在减少轨迹规划时间和缩小优化差距方面取得了显著成果 |
| [^82] | [Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network](https://arxiv.org/abs/2403.17351) | 本文提出了HiGNN，一种创新方法，通过调查节点的邻居分布来有效利用异质信息，从而增强图神经网络的学习效果。 |
| [^83] | [Language Models are Free Boosters for Biomedical Imaging Tasks](https://arxiv.org/abs/2403.17343) | 本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。 |
| [^84] | [The Pursuit of Fairness in Artificial Intelligence Models: A Survey](https://arxiv.org/abs/2403.17333) | 人工智能模型中的公平追求至关重要，研究人员努力解决偏见问题，确保模型不会有意或无意地对某些群体产生偏见。 |
| [^85] | [Deep Support Vectors](https://arxiv.org/abs/2403.17329) | 该论文探索了深度学习模型中的深度支持向量（DSVs）的概念，介绍了DeepKKT条件，通过实证研究发现DSVs与SVM中的支持向量类似，为解释模型决策标准提供了方法，同时证明了可以有效地使用DSVs重构模型。 |
| [^86] | [ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching](https://arxiv.org/abs/2403.17312) | 提出了ALISA，一种通过稀疏感知KV缓存加速大型语言模型推理的新算法系统设计。 |
| [^87] | [Neural Multimodal Topic Modeling: A Comprehensive Evaluation](https://arxiv.org/abs/2403.17308) | 该论文对包含文本和图片的文档的多模态主题建模进行了全面评估，并提出了两种新颖的主题建模解决方案和两种新颖的评估指标，结果显示这些模型均能产生连贯且多样化的主题。 |
| [^88] | [Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation](https://arxiv.org/abs/2403.17296) | 本研究提出了用于逻辑回归和神经网络模型的新隐私保护机器学习协议，通过采用秘密共享查找表计算非线性函数，提高了计算效率和准确性，并探索了放松安全措施的可能性。 |
| [^89] | [Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study](https://arxiv.org/abs/2403.17287) | 联邦学习算法的性能评估研究揭示出，没有一种算法可以在所有性能指标上表现最佳，一些最先进的算法虽然准确性更高，却伴随着更高的计算或通信开销。 |
| [^90] | [An Analysis of Switchback Designs in Reinforcement Learning](https://arxiv.org/abs/2403.17285) | 本文通过提出“弱信号分析”框架，研究了强化学习中往返设计对平均处理效应估计准确性的影响，发现在大部分奖励误差为正相关时，往返设计比每日切换策略更有效，增加政策切换频率可以降低平均处理效应估计器的均方误差。 |
| [^91] | [Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning](https://arxiv.org/abs/2403.17266) | 通过知识转移和课程学习，本研究在三指机械臂操纵任务中采用强化学习，实现了提高学习效率和有效性的目标 |
| [^92] | [Diffusion-based Negative Sampling on Graphs for Link Prediction](https://arxiv.org/abs/2403.17259) | 提出了一种基于图的扩散负采样的新策略，能够从潜在空间中以多个灵活和可控的“难度”级别生成负节点。 |
| [^93] | [Manufacturing Service Capability Prediction with Graph Neural Networks](https://arxiv.org/abs/2403.17239) | 该研究提出了一种基于图神经网络的制造服务能力识别方法，通过聚合图节点邻域信息和对图数据进行过采样来提高识别性能，增强制造能力识别的准确性和完整性。 |
| [^94] | [Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks](https://arxiv.org/abs/2403.17238) | 提出了一种基于基础模型的自动化框架，通过新颖的提示策略将轨迹数据分解为时间和语言描述的子任务，同时引入了时间相似性和语义相似性两种新的评估指标。 |
| [^95] | [Neural Image Compression with Quantization Rectifier](https://arxiv.org/abs/2403.17236) | 该论文提出了一种利用图像特征相关性来减轻量化影响的神经图像压缩量化整流器方法，通过设计神经网络架构预测未经量化的特征，从而实现更好的图像重建质量。 |
| [^96] | [Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process](https://arxiv.org/abs/2403.17233) | 提出了一种主动学习算法，通过将先验领域知识引入采样过程中，加速学习并降低模型不确定性，从而有效估计动力学。 |
| [^97] | [Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination](https://arxiv.org/abs/2403.17231) | 提出了一种新的自监督学习方法Dyna-LfLH，通过学习幻觉中的动态环境，安全地学习地面机器人在动态环境中灵活导航。 |
| [^98] | [Uncertainty Quantification for Gradient-based Explanations in Neural Networks](https://arxiv.org/abs/2403.17224) | 本文提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程，通过计算解释分布的变异系数，评估了解释的置信度并确定Guided Backpropagation方法生成的解释具有较低的不确定性。 |
| [^99] | [Co-Occurring of Object Detection and Identification towards unlabeled object discovery](https://arxiv.org/abs/2403.17223) | 通过提出的深度学习方法，可以识别多标签物体类别中基础物体的共存物体，并通过共存矩阵分析生成频繁模式，从而实现未标记物体的发现。 |
| [^100] | [A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection](https://arxiv.org/abs/2403.17218) | 本研究调查了十一种领先的大型语言模型在漏洞检测中的能力，并评估了它们的性能，为探索LLMs推理能力的极限提供了重要案例研究。 |
| [^101] | [Sanity Checks for Explanation Uncertainty](https://arxiv.org/abs/2403.17212) | 本文提出了解释不确定性方法的合理性检查，可以快速测试不确定性和解释方法的组合。 |
| [^102] | [CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions](https://arxiv.org/abs/2403.17210) | 通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战 |
| [^103] | [On the Intersection of Signal Processing and Machine Learning: A Use Case-Driven Analysis Approach](https://arxiv.org/abs/2403.17181) | 本文介绍了一个综合文章方法，旨在弥合信号处理和机器学习之间的知识鸿沟，并提供了针对广泛读者群体的信号处理基础教程。 |
| [^104] | [Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study](https://arxiv.org/abs/2403.17177) | 本研究通过比较深度学习模型在脑卒中分割上的表现，探讨了是否需要高级别设计来获得最佳结果。 |
| [^105] | [Belief Samples Are All You Need For Social Learning](https://arxiv.org/abs/2403.17174) | 本文研究了社会学习中的信念样本应用，提出了一种基于有限通信和资源限制的新型学习方式。 |
| [^106] | [Multi-Objective Quality-Diversity for Crystal Structure Prediction](https://arxiv.org/abs/2403.17164) | 本研究利用质量多样性算法为晶体结构预测打开了一条新途径，旨在发现具有多样特征的高性能解决方案集合，可以优化晶体结构稳定性以及其他目标如磁性或热电效率。 |
| [^107] | [Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP](https://arxiv.org/abs/2403.17159) | 我们提出了一种数据预处理方法，通过稀疏化TSP图表示和注意力掩码，使编码器集中于TSP实例的关键部分，同时允许信息在所有节点之间自由流动。 |
| [^108] | [Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language](https://arxiv.org/abs/2403.17143) | 本文应用引导远程监督方法，为德语创建了最大的传记关系抽取数据集，同时发布了手动标注的评估数据集。 |
| [^109] | [Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control](https://arxiv.org/abs/2403.17142) | 针对足够光滑的函数，本文证明使用随机生成的权重和偏置的ReLU网络可以在高概率下实现$O(m^{-1/2})$的$L_{\infty}$误差。 |
| [^110] | [Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases](https://arxiv.org/abs/2403.17135) | 本研究评估了癌症临床试验资格分类器在不同疾病间的泛化性能，发现在广泛癌症数据集上训练的模型可以处理非癌症试验的标准，但在某些情况下仍然存在困难。 |
| [^111] | [Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification](https://arxiv.org/abs/2403.17130) | 本文探索了一种先前在少于一次学习中提出的简单蒸馏技术在原型软标签蒸馏中的潜力，旨在通过在蒸馏过程中整合优化步骤，进一步提高分类准确性。 |
| [^112] | [Grounding Language Plans in Demonstrations Through Counterfactual Perturbations](https://arxiv.org/abs/2403.17124) | 这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。 |
| [^113] | [Stochastic Gradient Langevin Unlearning](https://arxiv.org/abs/2403.17105) | 本工作提出了随机梯度 Langevin 反遗忘方法，为近似反遗忘问题提供了隐私保障，并展示了小批次梯度更新相较于全批次的优越性能。 |
| [^114] | [SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving](https://arxiv.org/abs/2403.17094) | 介绍了一种新的SynFog合成雾数据集，利用端到端仿真管道生成逼真的雾图像，为自动驾驶中的真实世界去雾技术研究提供了先进工具 |
| [^115] | [Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis](https://arxiv.org/abs/2403.17093) | 本研究提出了通过深度学习和可解释人工智能分析，在无人机安全领域实施零信任架构以提高安全性的必要性，并实现了84.59\%的无人机识别准确率。 |
| [^116] | [Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data](https://arxiv.org/abs/2403.17091) | 研究提出了对于脱机策略评估任务，样本复杂度受聚合马尔科夫转换模型中的浓缩系数控制，而不是原始MDP中的系数。 |
| [^117] | [A Study in Dataset Pruning for Image Super-Resolution](https://arxiv.org/abs/2403.17083) | 本研究针对图像超分辨率中数据集训练资源需求大的问题，提出了一种数据集修剪的解决方案，通过基于损失值的选择，将训练集缩减至原始数据集的50%，取得了令人满意的结果。 |
| [^118] | [Machine Learning on Blockchain Data: A Systematic Mapping Study](https://arxiv.org/abs/2403.17081) | 本论文是一项系统性映射研究，旨在全面回顾机器学习应用于区块链数据的最新研究现状，有助于确定未来研究中应该重点关注的领域。 |
| [^119] | [Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions](https://arxiv.org/abs/2403.17064) | 通过识别CLIP文本嵌入中的语义方向，实现了文本到图像模型中对高级属性的细粒度主题特定控制。 |
| [^120] | [Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction](https://arxiv.org/abs/2403.17042) | 开发了一个算法框架，用于将基于得分的扩散模型作为通用非线性逆的表达数据先验。 |
| [^121] | [Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks](https://arxiv.org/abs/2403.17040) | 本研究提出了一种将注意力机制与脉冲神经网络相结合的新方法，用于改善图表示学习，实验结果显示其在基准数据集上表现出可比较的性能。 |
| [^122] | [Stochastic parameter reduced-order model based on hybrid machine learning approaches](https://arxiv.org/abs/2403.17032) | 本文基于混合机器学习方法，构建了一个Convolutional Autoencoder-Reservoir Computing-Normalizing Flow算法框架，用于降阶建模，能够高效描述自然现象的关键动态和统计特征。 |
| [^123] | [The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization](https://arxiv.org/abs/2403.17031) | 该研究复制了OpenAI在TL;DR总结中报道的RLHF强化学习规模行为，并展示出随着模型规模增大，RLHF训练的Pythia模型在响应质量上取得了显著提高。 |
| [^124] | [HEAL-ViT: Vision Transformers on a spherical mesh for medium-range weather forecasting](https://arxiv.org/abs/2403.17016) | HEAL-ViT是专为中程天气预报设计的基于球形网格的视觉Transformer模型，克服了在矩形栅格上处理球形天气数据时出现的失真问题，将经纬度格网映射到球形网格，提高了对极地附近数据的建模效率。 |
| [^125] | [Contrastive Learning for Regression on Hyperspectral Data](https://arxiv.org/abs/2403.17014) | 提出了一个对比学习框架，针对高光谱数据的回归任务，通过提供一系列适用于增强高光谱数据的变换，用对比学习来显著提高回归模型性能。 |
| [^126] | [Temporal-Spatial Processing of Event Camera Data via Delay-Loop Reservoir Neural Network](https://arxiv.org/abs/2403.17013) | 提出了一个名为时间空间猜想（TSC）的猜想，强调视频信号的时间表示中携带重要信息内容，提出了一个视觉马尔可夫模型（VMM）来验证这一猜想。 |
| [^127] | [SUDO: a framework for evaluating clinical artificial intelligence systems without ground-truth annotations](https://arxiv.org/abs/2403.17011) | SUDO框架允许在缺乏真实标注的情况下评估AI系统，通过为实际数据点分配临时标签并直接使用它们训练模型来解决分布转移问题，从而提高对临床数据的可信度。 |
| [^128] | [Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators](https://arxiv.org/abs/2403.16950) | 在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。 |
| [^129] | [Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models](https://arxiv.org/abs/2403.16915) | 本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。 |
| [^130] | [DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts](https://arxiv.org/abs/2403.16861) | DISL数据集包含了514,506个部署在以太坊主网上的独特Solidity文件，成为了开发智能合约设计工具和基准测试的重要资源。 |
| [^131] | [DeepMachining: Online Prediction of Machining Errors of Lathe Machines](https://arxiv.org/abs/2403.16451) | DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。 |
| [^132] | [Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices](https://arxiv.org/abs/2403.15905) | 提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。 |
| [^133] | [NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks](https://arxiv.org/abs/2403.14736) | 提出了NaNa和MiGu两种语义数据增强方法，结合了蛋白质的主链化学和侧链生物物理信息，用于增强图神经网络中的蛋白质分类任务。 |
| [^134] | [A Multimodal Approach to Device-Directed Speech Detection with Large Language Models](https://arxiv.org/abs/2403.14438) | 探索了一种利用大型语言模型进行设备定向语音检测的多模态方法，相比于文本和音频模型，使用多模态信息能够显著提高相等错误率。 |
| [^135] | [Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity](https://arxiv.org/abs/2403.13374) | 通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。 |
| [^136] | [GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot](https://arxiv.org/abs/2403.13358) | GeRM是一种通用机器人模型，通过混合专家结构和离线强化学习优化数据利用策略，解决了多任务机器人学习中性能问题和数据收集困难的情况，显著提高了模型性能。 |
| [^137] | [Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification](https://arxiv.org/abs/2403.12151) | 大型语言模型与知识图谱结合，提高零样本对象状态分类性能 |
| [^138] | [Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning](https://arxiv.org/abs/2403.11996) | 利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。 |
| [^139] | [SelfIE: Self-Interpretation of Large Language Model Embeddings](https://arxiv.org/abs/2403.10949) | 提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。 |
| [^140] | [Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt](https://arxiv.org/abs/2403.09857) | 提出了一个名为ASP的框架，通过注意力方面减少特定信息，鼓励任务不变的提示来捕获共享知识，并通过信息瓶颈学习目标从旧类到新类传递知识。 |
| [^141] | [Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity](https://arxiv.org/abs/2403.09428) | 本文提出了一种检索增强的上下文学习框架，旨在解决多模态学习中缺失模态和数据稀缺问题。 |
| [^142] | [Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/abs/2403.08763) | 通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。 |
| [^143] | [Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents](https://arxiv.org/abs/2403.04202) | 在多代理环境中，研究人员探讨了不同道德类型的学习代理之间的互动，发现道德异质性可能对代理的共同发展产生影响。 |
| [^144] | [Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection](https://arxiv.org/abs/2403.02232) | 该研究通过机器学习技术全面评估了恶意软件检测，发现集成方法（如随机森林和XGBoost）相较于其他方法在恶意软件检测中表现出更高的准确性、精确度和召回率。 |
| [^145] | [Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network](https://arxiv.org/abs/2403.00033) | 通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。 |
| [^146] | [Accelerating Graph Neural Networks on Real Processing-In-Memory Systems](https://arxiv.org/abs/2402.16731) | 在实际处理内存系统上加速图神经网络，并提出了针对实际PIM系统的智能并行化技术和混合式执行方法。 |
| [^147] | [LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data](https://arxiv.org/abs/2402.13452) | 本研究提出了一个新的基于Twitter数据的框架LocalHealth，用于预测当地精神健康结果。通过与GPT3.5结合使用，该框架在MH监测中取得了显著的改进。 |
| [^148] | [Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling](https://arxiv.org/abs/2402.11800) | 延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。 |
| [^149] | [Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones](https://arxiv.org/abs/2402.04866) | 本研究使用复值神经网络在有限的麦克风数据中估计房间传递函数，实现了房间传递函数的重建，创新之处在于首次使用了复值神经网络来估计房间传递函数。 |
| [^150] | [ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation](https://arxiv.org/abs/2402.00093) | 本研究设计了一个基于大语言模型的流水线，通过自然语言规格生成英语、线性时态逻辑和SVA断言，并成功减少了断言错误率。 |
| [^151] | [Optimal Data Splitting in Distributed Optimization for Machine Learning](https://arxiv.org/abs/2401.07809) | 分布式优化问题中，提出了一种利用本地数据相似性的算法，可有效解决通信成本瓶颈。 |
| [^152] | [Activations and Gradients Compression for Model-Parallel Training](https://arxiv.org/abs/2401.07788) | 本研究探讨了在模型-并行分布式训练设置中对激活和梯度同时进行压缩对收敛的影响，发现梯度需要比激活更轻微的压缩率。 |
| [^153] | [AI and Generative AI for Research Discovery and Summarization](https://arxiv.org/abs/2401.06795) | AI和生成式AI工具在研究发现和总结方面有重大影响，包括能够更快地找到相关文献和用简洁语言总结研究文章的要点。 |
| [^154] | [PINN surrogate of Li-ion battery models for parameter inference. Part II: Regularization and application of the pseudo-2D model](https://arxiv.org/abs/2312.17336) | 开发了物理信息神经网络（PINN）作为伪二维（P2D）电池模型校准的代理，进行了参数推断研究，可以减少Bayesian校准的计算成本。 |
| [^155] | [PINN surrogate of Li-ion battery models for parameter inference. Part I: Implementation and multi-fidelity hierarchies for the single-particle model](https://arxiv.org/abs/2312.17329) | 通过PINN代理模型替代基于物理的锂离子电池模型，帮助减少计算资源，用于快速准确诊断电池内部状态。 |
| [^156] | [Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer](https://arxiv.org/abs/2312.12467) | 本文提出了一种使用分层网格结构的Hierarchical Contact Mesh Transformer（HCMT），能够学习长距离依赖关系，以处理灵活体动力学挑战。 |
| [^157] | [World Models via Policy-Guided Trajectory Diffusion](https://arxiv.org/abs/2312.08533) | 这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。 |
| [^158] | [Artificial Neural Nets and the Representation of Human Concepts](https://arxiv.org/abs/2312.05337) | 人工神经网络可以学习人类和非人类的概念，但并不是用单个单元来表示这些概念 |
| [^159] | [All Rivers Run to the Sea: Private Learning with Asymmetric Flows](https://arxiv.org/abs/2312.05264) | 提出了一种新的私有训练和推理框架Delta，通过两个不对称数据流实现了具有可比较模型性能的隐私保护。 |
| [^160] | [DreamComposer: Controllable 3D Object Generation via Multi-View Conditions](https://arxiv.org/abs/2312.03611) | DreamComposer是一个灵活且可扩展的框架，通过注入多视角条件增强现有视角感知扩散模型，实现可控的3D对象生成。 |
| [^161] | [A Simple and Scalable Representation for Graph Generation](https://arxiv.org/abs/2312.02230) | 提出了一种名为GEEL的新型、简单且可扩展的图表示，可以显著降低邻接矩阵大小和词汇量，同时通过节点位置编码实现自回归生成，并针对属性图设计了新的扩展方案。 |
| [^162] | [PAC Privacy Preserving Diffusion Models](https://arxiv.org/abs/2312.01201) | 提出了一种PAC隐私保护扩散模型，通过将私有分类器指导集成到采样过程中增强隐私保护，并发展了一种新的度量标准来衡量隐私水平，在保护性能方面表现出卓越表现。 |
| [^163] | [In Search of a Data Transformation That Accelerates Neural Field Training](https://arxiv.org/abs/2311.17094) | 随机像素置换可以显著加速神经场训练，通过消除易匹配模式来促进早期阶段的优化 |
| [^164] | [Efficient Pre-training for Localized Instruction Generation of Videos](https://arxiv.org/abs/2311.15964) | 提出了一种名为Sieve-&-Swap的技术，通过自动筛选出不相关文本并用人类编写的说明替换文本转录，从而实现视频本地化指令生成的高效预训练。 |
| [^165] | [Applying statistical learning theory to deep learning](https://arxiv.org/abs/2311.15404) | 深度学习的理论方面仍不清楚，本研究通过将统计学习理论应用于深度学习，探讨了不同架构在基于梯度方法训练时可能导致的归纳偏差，并详细研究了隐含偏差的数量化表示。 |
| [^166] | [Disentangling the Spectral Properties of the Hodge Laplacian: Not All Small Eigenvalues Are Equal](https://arxiv.org/abs/2311.14427) | 引入持续特征向量相似性的概念，并提供一种方法来跟踪霍奇拉普拉斯的个体谐波、旋度和梯度特征向量/值。 |
| [^167] | [Masked Autoencoders Are Robust Neural Architecture Search Learners](https://arxiv.org/abs/2311.12086) | 提出了一种基于掩码自编码器的新型神经架构搜索框架，无需标记数据，在搜索过程中使用图像重建任务代替监督学习目标，具有鲁棒性、性能和泛化能力，并通过引入多尺度解码器解决了性能崩溃问题。 |
| [^168] | [Graph Signal Diffusion Model for Collaborative Filtering](https://arxiv.org/abs/2311.08744) | 提出了一种用于协同过滤的图信号扩散模型，解决了现有扩散模型在建模隐式反馈数据方面的不足，通过对扩散模型进行创新改进，解决了标准扩散过程导致的个性化信息丢失和图形结构不一致等问题。 |
| [^169] | [Discretized Distributed Optimization over Dynamic Digraphs](https://arxiv.org/abs/2311.07939) | 该论文研究了在动态有向图上进行连续时间分布式优化的离散模型，避免了链路上双随机权重设计的需要，并为分布式优化在时间变化的有向图上铺平了道路。 |
| [^170] | [Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data](https://arxiv.org/abs/2311.03520) | 本文提出了一种新颖的BrainRGIN建模架构，使用图神经网络来预测智力，扩展了现有的图卷积网络并结合了聚类嵌入、图同构网络、TopK池化和基于注意力的读出函数。 |
| [^171] | [Stable Linear Subspace Identification: A Machine Learning Approach](https://arxiv.org/abs/2311.03197) | 本文提出了一种使用机器学习方法进行稳定线性子空间识别的新方法SIMBa，在稳定性和性能方面均表现优异。 |
| [^172] | [Riemannian Laplace Approximation with the Fisher Metric](https://arxiv.org/abs/2311.02766) | 黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。 |
| [^173] | [A randomized algorithm for nonconvex minimization with inexact evaluations and complexity guarantees](https://arxiv.org/abs/2310.18841) | 该算法针对非凸函数的最小化问题使用了不精确的梯度和Hessian信息，通过随机选择近似的负曲率方向步进，实现近似二阶最优性，并在梯度样本复杂度上取得了改进。 |
| [^174] | [SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching](https://arxiv.org/abs/2310.17569) | 提出了SD4Match，在语义匹配中学习提示稳定扩散模型，通过提示调整技术和条件提示模块显著提高了准确性，并在多个数据集上取得了新的准确性基准。 |
| [^175] | [Unveiling the Pitfalls of Knowledge Editing for Large Language Models](https://arxiv.org/abs/2310.02129) | 这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。 |
| [^176] | [Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic](https://arxiv.org/abs/2309.13339) | 提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。 |
| [^177] | [Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning](https://arxiv.org/abs/2303.15230) | 提出了一种适用于组合式零样本学习的Troika模型，通过建立三个识别分支共同对状态、对象和组合进行建模，在对齐分支特定提示表示和分解的视觉特征的同时，引入了Cross-Modal Traction模块来校准多模态表示之间的偏差。 |
| [^178] | [Ensemble learning for Physics Informed Neural Networks: a Gradient Boosting approach](https://arxiv.org/abs/2302.13143) | 提出了一种新的训练范例“梯度提升”（GB），使用序列神经网络取得优越结果，显著提高了物理信息神经网络（PINNs）的性能，并解锁了在PINNs中应用集成学习技术的机会。 |
| [^179] | [An Implicit GNN Solver for Poisson-like problems](https://arxiv.org/abs/2302.10891) | 通过利用隐式层理论，$\Psi$-GNN模型了一个“无限”深的网络，从而避免了经验调整所需的消息传递层次以获得解决方案。 |
| [^180] | [Toward a Theory of Causation for Interpreting Neural Code Models](https://arxiv.org/abs/2302.03788) | 该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。 |
| [^181] | [Investigating Feature and Model Importance in Android Malware Detection: An Implemented Survey and Experimental Comparison of ML-Based Methods](https://arxiv.org/abs/2301.12778) | 本文重新实现和评估了18个代表性的过去研究，并在包含124,000个应用程序的平衡、相关和最新数据集上进行了新实验，发现仅通过静态分析提取的特征就能实现高达96.8%的恶意软件检测准确性。 |
| [^182] | [Federated Learning Using Three-Operator ADMM](https://arxiv.org/abs/2211.04152) | 联邦学习中引入了三算子ADMM算法，提出了利用边缘服务器丰富数据的优势，与仅使用用户数据相比有更大益处。 |
| [^183] | [Differentially private multivariate medians](https://arxiv.org/abs/2210.06459) | 差分私有多变量中位数的有限样本性能保证为常用深度函数提供了尖锐的结果，表明重尾位置估计的成本超过了隐私保护成本。 |
| [^184] | [P2ANet: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos](https://arxiv.org/abs/2207.12730) | 介绍了P2ANet数据集，用于从乒乓球比赛广播视频中进行稠密动作检测，包含2,721个视频剪辑；通过与专业人员合作获得了细粒度的动作标签，在14个类别中涵盖了每个乒乓球动作，提出了动作定位和动作识别两种问题，并对多种动作识别方法进行了评估。 |
| [^185] | [FedCau: A Proactive Stop Policy for Communication and Computation Efficient Federated Learning](https://arxiv.org/abs/2204.07773) | 本文提出了一种成本感知因果FL算法（FedCau），用于处理无线网络上联邦学习模型的通信和计算高效性问题，通过迭代终止方法权衡了训练性能和网络成本。 |
| [^186] | [Semi-Supervised Crowd Counting from Unlabeled Data](https://arxiv.org/abs/2108.13969) | 提出了一种半监督学习框架$S^{4}\textit{Crowd}$，可以利用无标签数据和有标签数据进行人群计数，同时引入了自监督损失和基于人群变化的递归单元，以降低标注成本并提升性能。 |
| [^187] | [Covariance-Aware Private Mean Estimation Without Private Covariance Estimation](https://arxiv.org/abs/2106.13329) | 提出了两种针对具有未知协方差的高斯分布的样本高效差分隐私均值估计器，无需对协方差进行估计。 |
| [^188] | [Bayesian data-driven discovery of partial differential equations with variable coefficients](https://arxiv.org/abs/2102.01432) | 提出了一种用于具有可变系数的偏微分方程发现的先进贝叶斯稀疏学习算法，通过阈值贝叶斯组Lasso回归和Gibbs采样器提高了稳健性和减轻了计算负担 |
| [^189] | [Fully Independent Communication in Multi-Agent Reinforcement Learning.](http://arxiv.org/abs/2401.15059) | 该论文研究了多智能体强化学习中的完全独立通信，并提出了一种新的学习方案，证明独立智能体仍可以学习通信策略。此外，论文还探讨了通信在不同网络容量下的影响。 |
| [^190] | [Efficient Constrained $k$-Center Clustering with Background Knowledge.](http://arxiv.org/abs/2401.12533) | 本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。 |
| [^191] | [Domain Randomization via Entropy Maximization.](http://arxiv.org/abs/2311.01885) | 本文提出了一种新的领域随机化方法，通过熵最大化的方式在模拟训练中调整动力学分布，以实现模拟到真实的转移，无需真实世界数据，能够保持泛化能力和代理的高成功概率。 |
| [^192] | [PPI++: Efficient Prediction-Powered Inference.](http://arxiv.org/abs/2311.01453) | PPI++是一种高效的预测驱动推理方法，通过自动调整预测质量来改善经典区间的计算置信区间的计算效率和统计效率。 |
| [^193] | [Generative Pre-training for Speech with Flow Matching.](http://arxiv.org/abs/2310.16338) | 本文展示了一种使用流匹配的预训练生成模型，该模型可以适应不同的下游任务并获得强大的性能，通过在60k小时的未转录语音上进行预训练，该模型可以与现有的专家模型在语音增强、分离和合成方面进行匹配或超越。 |
| [^194] | [COPF: Continual Learning Human Preference through Optimal Policy Fitting.](http://arxiv.org/abs/2310.15694) | 通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。 |
| [^195] | [Context-Aware Meta-Learning.](http://arxiv.org/abs/2310.10971) | 本文提出了一种上下文感知的元学习算法，可以在推理过程中学习新的视觉概念而无需微调。该方法在多个元学习基准中表现优异，超过或与目前的最先进算法相匹配。 |
| [^196] | [From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique.](http://arxiv.org/abs/2310.09362) | 这项研究开发了一个能够发出声音的波斯语聊天机器人，用于指导用户进行基于依恋理论的自我依恋技术。通过使用规则和分类模块，聊天机器人可以理解用户输入并推荐适当的自我依恋练习。该研究还开发了一种准确率超过92%的情感分析模块，以识别用户情感。这项工作有助于在后疫情时代提供数字心理疗法的替代方案。 |
| [^197] | [Entropy-MCMC: Sampling from Flat Basins with Ease.](http://arxiv.org/abs/2310.05401) | 本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。 |
| [^198] | [Dual Conic Proxies for AC Optimal Power Flow.](http://arxiv.org/abs/2310.02969) | 本文提出了一种基于双圆锥代理的方法来求解交流最优功率流问题，并通过自监督学习方案来辅助训练，实验证明了该方法的效率和可扩展性。 |
| [^199] | [Harmonic Control Lyapunov Barrier Functions for Constrained Optimal Control with Reach-Avoid Specifications.](http://arxiv.org/abs/2310.02869) | 该论文介绍了谐波控制李亚普诺夫障碍函数（harmonic CLBF），它可以在约束控制问题中解决避障要求，通过最大化系统动力学与谐波CLBF最陡下降方向的内积来选择控制输入，从而显著降低进入不安全区域的风险并提高进入目标区域的概率。 |
| [^200] | [Probabilistically Rewired Message-Passing Neural Networks.](http://arxiv.org/abs/2310.02156) | PR-MPNNs通过概率重连学习加入相关边，并省略对预测任务没有帮助的边，从而增强了表达能力。 |
| [^201] | [Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction.](http://arxiv.org/abs/2309.15798) | Node-Aligned Graph-to-Graph (NAG2G)是一个基于transformer的无模板模型，利用2D分子图和3D构象信息，能够更好地利用分子的拓扑信息和对齐原子，提高单步反合成预测的竞争力。 |
| [^202] | [Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration.](http://arxiv.org/abs/2309.10837) | 通过整合与阿片类药物使用障碍有关的遗传变异和行为特征，该论文开发了一种实验设计和计算方法，用于评估阿片类药物使用障碍的风险。结果显示整合遗传和迁移模式可以改善风险估计的能力。 |
| [^203] | [Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning.](http://arxiv.org/abs/2309.10275) | 该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。 |
| [^204] | [Prediction Error Estimation in Random Forests.](http://arxiv.org/abs/2309.00736) | 本文通过量化评估分类随机森林的误差估计方法，发现随机森林的预测误差估计比平均预测误差更接近真实误差率，并且这一结果适用于不同的误差估计策略。 |
| [^205] | [RetroBridge: Modeling Retrosynthesis with Markov Bridges.](http://arxiv.org/abs/2308.16212) | 本研究提出了一种基于马尔可夫桥模型的反合成建模方法，通过对两个难以处理的离散分布之间的依赖关系进行近似，直接生成可能的前体分子，为反合成规划提供了准确的预测和置信度估计。 |
| [^206] | [Multi-Objective Optimization for Sparse Deep Neural Network Training.](http://arxiv.org/abs/2308.12243) | 这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。 |
| [^207] | [Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability.](http://arxiv.org/abs/2308.07728) | 本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。 |
| [^208] | [Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models.](http://arxiv.org/abs/2308.01557) | 本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。 |
| [^209] | [RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$.](http://arxiv.org/abs/2306.15909) | RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。 |
| [^210] | [Omega: Optimistic EMA Gradients.](http://arxiv.org/abs/2306.07905) | Omega是一种优化算法，通过加入历史梯度EMA来减轻噪声的影响并在随机游戏上表现更佳。 |
| [^211] | [A Differential Testing Framework to Evaluate Image Recognition Model Robustness.](http://arxiv.org/abs/2306.06208) | 本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。 |
| [^212] | [Fault Localization for Framework Conversions of Image Recognition Models.](http://arxiv.org/abs/2306.06157) | 本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。 |
| [^213] | [FedCSD: A Federated Learning Based Approach for Code-Smell Detection.](http://arxiv.org/abs/2306.00038) | 本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。 |
| [^214] | [Hierarchical Graph Generation with $K^2$-trees.](http://arxiv.org/abs/2305.19125) | 本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。 |
| [^215] | [Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models.](http://arxiv.org/abs/2305.10474) | 本论文介绍了一种新的视频噪声先验，用于微调图像扩散模型，以实现更高质量的视频合成。经过广泛的实验验证，该模型已经取得了UCF-101和MSR-VTT基准测试的最佳结果。 |
| [^216] | [ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review.](http://arxiv.org/abs/2305.03123) | 本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。 |
| [^217] | [Conversational Tree Search: A New Hybrid Dialog Task.](http://arxiv.org/abs/2303.10227) | 本文介绍了一项新的任务——会话树搜索(CTS)，它可以架起FAQ和对话之间的桥梁，领域专家可以定义对话树，然后将其转换为一个有效的对话策略，只学习提出导航用户达到目标所需的问题。 |
| [^218] | [An optimal control perspective on diffusion-based generative modeling.](http://arxiv.org/abs/2211.01364) | 本文建立了随机最优控制与基于扩散的生成模型之间的联系，推导了用于控制潜在SDE边际密度演化的汉密尔顿-雅可比-贝尔曼方程，并将生成建模表述为对合适度量之间Kullback-Leibler散度的最小化。此外，作者还开发了一种新型扩散方法用于采样非归一化密度。 |
| [^219] | [Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions.](http://arxiv.org/abs/2208.06620) | 本研究提出了观点市场模型（OMM），通过引入积极干预来遏制极右派观点的传播。这个模型将观点的关注市场规模建模，并考虑了观点之间的相互作用和竞争，旨在评估积极干预的有效性。 |
| [^220] | [A Lightweight and Gradient-Stable Nerual Layer.](http://arxiv.org/abs/2106.04088) | Han层是一种梯度稳定、参数更少的神经层结构，可以替换全连接层来优化神经网络模型。 |

# 详细

[^1]: 修复-Con：深度学习模型转换的自动故障定位和修复

    Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions

    [https://rss.arxiv.org/abs/2312.15101](https://rss.arxiv.org/abs/2312.15101)

    本文提出了一种自动化的故障定位和修复方法Fix-Con，用于在深度学习模型转换过程中修复由转换引入的故障。Fix-Con能够检测和修复模型输入、参数、超参数和模型图方面的故障，提高转换模型的部署和预测正确性。

    

    在不同深度学习框架之间进行模型转换是一种常见的步骤，可以最大程度地增加模型在设备之间的兼容性，并利用可能只在一个深度学习框架中提供的优化功能。然而，这个转换过程可能存在错误，导致转换后的模型无法部署或存在问题，严重降低了其预测的正确性。我们提出了一种自动化的故障定位和修复方法，Fix-Con，在深度学习框架之间进行模型转换时使用。Fix-Con能够检测和修复在转换过程中引入的模型输入、参数、超参数和模型图的故障。Fix-Con使用从调查转换问题中挖掘出的一组故障类型来定位转换模型中潜在的转换故障，并适当修复它们，例如使用源模型的参数替换目标模型的参数。这一过程在数据集中的每个图像上进行迭代执行。

    Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
    
[^2]: SLEDGE: 使用生成模型合成驾驶智能体的模拟环境

    SLEDGE: Synthesizing Simulation Environments for Driving Agents with Generative Models

    [https://arxiv.org/abs/2403.17933](https://arxiv.org/abs/2403.17933)

    SLEDGE是第一个使用生成模型训练的车辆运动规划生成模拟器，引入了新颖的栅格到矢量自编码器（RVAE）以及Diffusion Transformer来生成智能体和车道图，从而实现更好的模拟控制。

    

    SLEDGE是第一个基于真实世界驾驶记录训练的车辆运动规划生成模拟器。其核心组件是一个学习模型，能够生成智能体边界框和车道图。该模型的输出作为交通模拟的初始状态。针对SLEDGE待生成的实体的独特特性，例如它们的连接性和每个场景的可变数量，使得大多数现代生成模型在这一任务上的朴素应用变得不简单。因此，我们除了对现有车道图表示进行系统研究外，还引入了一种新颖的栅格到矢量自编码器（RVAE）。它将智能体和车道图编码为栅格化潜在映射中的不同通道。这有助于车道条件下的智能体生成以及使用扩散变换器同时生成车道和智能体。在SLEDGE中使用生成的实体可以更好地控制模拟，例如上采样转弯。

    arXiv:2403.17933v1 Announce Type: cross  Abstract: SLEDGE is the first generative simulator for vehicle motion planning trained on real-world driving logs. Its core component is a learned model that is able to generate agent bounding boxes and lane graphs. The model's outputs serve as an initial state for traffic simulation. The unique properties of the entities to be generated for SLEDGE, such as their connectivity and variable count per scene, render the naive application of most modern generative models to this task non-trivial. Therefore, together with a systematic study of existing lane graph representations, we introduce a novel raster-to-vector autoencoder (RVAE). It encodes agents and the lane graph into distinct channels in a rasterized latent map. This facilitates both lane-conditioned agent generation and combined generation of lanes and agents with a Diffusion Transformer. Using generated entities in SLEDGE enables greater control over the simulation, e.g. upsampling turns 
    
[^3]: 需要速度：用一种方法对Transformer进行修剪

    The Need for Speed: Pruning Transformers with One Recipe

    [https://arxiv.org/abs/2403.17921](https://arxiv.org/abs/2403.17921)

    提出了一种名为OPTIN的框架，利用一次性修剪技术和中间特征蒸馏来提高预训练Transformer架构的效率，无需重新训练，并在多项任务中取得最先进的结果。

    

    我们介绍了$\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$)框架，作为一种工具，可以提高预训练的Transformer架构的效率，而无需重新训练。最近的研究探索了改进Transformer效率的方法，但通常需要计算密集型的重新训练过程或依赖于特定的架构特征，从而阻碍了广泛的实际应用。为了解决这些缺点，OPTIN框架利用中间特征蒸馏，捕获模型参数的长程依赖性（称为$\textit{trajectory}$），在自然语言处理、图像分类、迁移学习和语义分割任务中产生了最先进的结果，而无需重新训练。在给定的FLOP约束下，OPTIN框架将压缩网络同时保持竞争性能。

    arXiv:2403.17921v1 Announce Type: new  Abstract: We introduce the $\textbf{O}$ne-shot $\textbf{P}$runing $\textbf{T}$echnique for $\textbf{I}$nterchangeable $\textbf{N}$etworks ($\textbf{OPTIN}$) framework as a tool to increase the efficiency of pre-trained transformer architectures $\textit{without requiring re-training}$. Recent works have explored improving transformer efficiency, however often incur computationally expensive re-training procedures or depend on architecture-specific characteristics, thus impeding practical wide-scale adoption. To address these shortcomings, the OPTIN framework leverages intermediate feature distillation, capturing the long-range dependencies of model parameters (coined $\textit{trajectory}$), to produce state-of-the-art results on natural language, image classification, transfer learning, and semantic segmentation tasks $\textit{without re-training}$. Given a FLOP constraint, the OPTIN framework will compress the network while maintaining competitiv
    
[^4]: LISA：用于高效内存大型语言模型微调的逐层重要性采样

    LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning

    [https://arxiv.org/abs/2403.17919](https://arxiv.org/abs/2403.17919)

    逐层重要性采样的新方法LISA在微调任务中表现出色，记忆成本低且优于传统方法。

    

    机器学习领域自大型语言模型（LLMs）首次出现以来取得了令人瞩目的进展，然而它们巨大的内存消耗已成为大规模训练的主要障碍。虽然已经提出了诸如低秩调整（LoRA）之类的参数高效微调技术来缓解这一问题，但在大多数大规模微调设置中，它们的性能仍无法与完整参数训练相匹配。为弥补这一不足，我们研究了LoRA在微调任务中的逐层特性，并观察到不同层之间权重范数的异常偏斜。利用这一关键观察，我们发现了一个令人惊讶简单的训练策略，在记忆成本低于LoRA的情况下，在广泛的设置中优于LoRA和完整参数训练。我们将其命名为Layerwise Importance Sampled AdamW（LISA），这是LoRA的一个有希望的替代方案，应用了

    arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
    
[^5]: CMP：具有多智能体通信的合作运动预测

    CMP: Cooperative Motion Prediction with Multi-Agent Communication

    [https://arxiv.org/abs/2403.17916](https://arxiv.org/abs/2403.17916)

    该论文提出了一种名为CMP的方法，利用LiDAR信号作为输入，通过合作感知和运动预测模块共享信息，解决了合作运动预测的问题。

    

    随着自动驾驶车辆（AVs）的发展和车联网（V2X）通信的成熟，合作连接的自动化车辆（CAVs）的功能变得可能。本文基于合作感知，探讨了合作运动预测的可行性和有效性。我们的方法CMP以LiDAR信号作为输入，以增强跟踪和预测能力。与过去专注于合作感知或运动预测的工作不同，我们的框架是我们所知的第一个解决CAVs在感知和预测模块中共享信息的统一问题。我们的设计中还融入了能够容忍现实V2X带宽限制和传输延迟的独特能力，同时处理庞大的感知表示。我们还提出了预测聚合模块，统一了预测

    arXiv:2403.17916v1 Announce Type: cross  Abstract: The confluence of the advancement of Autonomous Vehicles (AVs) and the maturity of Vehicle-to-Everything (V2X) communication has enabled the capability of cooperative connected and automated vehicles (CAVs). Building on top of cooperative perception, this paper explores the feasibility and effectiveness of cooperative motion prediction. Our method, CMP, takes LiDAR signals as input to enhance tracking and prediction capabilities. Unlike previous work that focuses separately on either cooperative perception or motion prediction, our framework, to the best of our knowledge, is the first to address the unified problem where CAVs share information in both perception and prediction modules. Incorporated into our design is the unique capability to tolerate realistic V2X bandwidth limitations and transmission delays, while dealing with bulky perception representations. We also propose a prediction aggregation module, which unifies the predict
    
[^6]: 具有R2D2的可扩展非笛卡尔磁共振成像方法

    Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2

    [https://arxiv.org/abs/2403.17905](https://arxiv.org/abs/2403.17905)

    通过引入R2D2方法，提出了一种可扩展的非笛卡尔磁共振图像重建方法。

    

    我们提出了一种新的非笛卡尔磁共振图像重建方法。我们利用最近在天文成像中引入的“用于高动态范围成像的残差级联DNN系列（R2D2）”方法，解决了可扩展性挑战。R2D2的重建被形成为残差图像的系列，被迭代地估计为接受上一次迭代的图像估计和相关数据残差作为输入的DNN的输出。

    arXiv:2403.17905v1 Announce Type: cross  Abstract: We propose a new approach for non-Cartesian magnetic resonance image reconstruction. While unrolled architectures provide robustness via data-consistency layers, embedding measurement operators in Deep Neural Network (DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP) approaches, where the denoising DNNs are blind to the measurement setting, are not affected by this limitation and have also proven effective, but their highly iterative nature also affects scalability. To address this scalability challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic range imaging (R2D2)" approach recently introduced in astronomical imaging. R2D2's reconstruction is formed as a series of residual images, iteratively estimated as outputs of DNNs taking the previous iteration's image estimate and associated data residual as inputs. The method can be interpreted as a learned version of the Matching Pursuit algo
    
[^7]: Serpent：通过多尺度结构化状态空间模型实现可扩展高效的图像恢复

    Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models

    [https://arxiv.org/abs/2403.17902](https://arxiv.org/abs/2403.17902)

    Serpent提出了一种新的图像恢复架构，利用状态空间模型在全局感受野和计算效率之间取得平衡，实现了与最先进技术相当的重建质量，但计算量减少了数个数量级。

    

    有效图像恢复架构的计算建筑块领域，主要由卷积处理和各种注意机制的组合所主导。然而，卷积滤波器本质上是局部的，因此在建模图像的长距离依赖性方面存在困难。另一方面，注意机制擅长捕获任意图像区域之间的全局相互作用，但对图像尺寸的二次成本较高。在这项工作中，我们提出了Serpent，这是一种利用最近在状态空间模型（SSMs）方面的进展作为其核心计算模块的架构。SSMs最初用于序列建模，可以通过有利的输入尺寸的线性缩放来维持全局感受野。我们的初步结果表明，Serpent可以实现与最先进技术相当的重建质量，同时需要数量级的计算量较少（在FLOPS上高达150倍的减少）。

    arXiv:2403.17902v1 Announce Type: cross  Abstract: The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms. However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) an
    
[^8]: 使用深度学习分类器基于图像的新型故障检测，使用分层标签

    Image-based Novel Fault Detection with Deep Learning Classifiers using Hierarchical Labels

    [https://arxiv.org/abs/2403.17891](https://arxiv.org/abs/2403.17891)

    本研究提出了一种在深度学习分类器中利用分层标签提高未知故障检测性能的方法，同时不影响模型性能，在热轧钢过程中的实验结果表明其具有较好的可复制性。

    

    现代故障分类系统的一个重要特征是在面对以前未见过的故障类型时能够标记系统。本文考虑了基于深度神经网络的故障分类器的未知故障检测能力。具体来说，我们提出了一种方法，即在可用的情况下，如何利用有关故障分类法的标签来提高未知故障检测性能，而不会牺牲模型性能。为了实现这一目标，我们建议利用软标签技术来改善现有的深度新型故障检测技术，在训练过程中以及用于在线新型故障检测的层次一致检测统计。最后，我们展示了在热轧钢过程中检查图像中新型故障检测的增强检测性能，结果在多种情况和基线检测方法中都有很好的重现。

    arXiv:2403.17891v1 Announce Type: cross  Abstract: One important characteristic of modern fault classification systems is the ability to flag the system when faced with previously unseen fault types. This work considers the unknown fault detection capabilities of deep neural network-based fault classifiers. Specifically, we propose a methodology on how, when available, labels regarding the fault taxonomy can be used to increase unknown fault detection performance without sacrificing model performance. To achieve this, we propose to utilize soft label techniques to improve the state-of-the-art deep novel fault detection techniques during the training process and novel hierarchically consistent detection statistics for online novel fault detection. Finally, we demonstrated increased detection performance on novel fault detection in inspection images from the hot steel rolling process, with results well replicated across multiple scenarios and baseline detection methods.
    
[^9]: 大规模配对抗体语言模型

    Large scale paired antibody language models

    [https://arxiv.org/abs/2403.17889](https://arxiv.org/abs/2403.17889)

    IgBert和IgT5是迄今为止发展的最佳表现的抗体特定语言模型，可以处理配对和无配对可变区序列，并在训练中使用了超过20亿个无配对序列和两百万个配对序列。

    

    抗体是免疫系统产生的蛋白质，可以识别和中和各种抗原，具有高特异性和亲和力，并构成最成功的生物治疗类别。 随着下一代测序技术的出现，近年来已收集了数十亿个抗体序列，尽管这些序列在设计更好的治疗方案中的应用受到数据量和复杂性的限制。 为了解决这一挑战，我们提出了迄今开发的表现最佳的抗体特定语言模型IgBert和IgT5，这两个模型可以持续处理作为输入的配对和无配对可变区序列。 这些模型全面地使用了“观测到的抗体空间”数据集中的超过20亿个无配对序列和两百万个轻链和重链的配对序列进行培训。 我们展示了我们的模型胜过现有的抗体和蛋白质语言模型。

    arXiv:2403.17889v1 Announce Type: cross  Abstract: Antibodies are proteins produced by the immune system that can identify and neutralise a wide variety of antigens with high specificity and affinity, and constitute the most successful class of biotherapeutics. With the advent of next-generation sequencing, billions of antibody sequences have been collected in recent years, though their application in the design of better therapeutics has been constrained by the sheer volume and complexity of the data. To address this challenge, we present IgBert and IgT5, the best performing antibody-specific language models developed to date which can consistently handle both paired and unpaired variable region sequences as input. These models are trained comprehensively using the more than two billion unpaired sequences and two million paired sequences of light and heavy chains present in the Observed Antibody Space dataset. We show that our models outperform existing antibody and protein language m
    
[^10]: 深层神经网络层剪枝的不合理无效性

    The Unreasonable Ineffectiveness of the Deeper Layers

    [https://arxiv.org/abs/2403.17887](https://arxiv.org/abs/2403.17887)

    层剪枝方法可以在流行的预训练语言模型中实现大部分层的移除而保持性能，同时使用参数高效的微调方法可以进一步减少计算资源，提高推断的内存和延迟。

    

    我们在流行的预训练语言模型中进行了简单的层剪枝策略的实证研究，发现在移除大部分层（最高达一半）之前，不同问答基准测试的性能几乎没有受到影响。为了剪枝这些模型，我们通过考虑层间的相似性来确定最佳的剪枝层块；然后，为了“修复”损害，我们进行了少量微调。特别地，我们使用参数高效的微调（PEFT）方法，具体包括量化和低秩适配器（QLoRA），这样我们的每个实验都可以在单个A100 GPU上执行。从实际的角度来看，这些结果表明层剪枝方法可以补充其他PEFT策略，从而进一步减少微调的计算资源，另一方面可以提高推断的内存和延迟。从科学的角度来看，该研究表明深层神经网络在某种程度上具有鲁棒性，并且对模型的剪枝没有太大影响。

    arXiv:2403.17887v1 Announce Type: new  Abstract: We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of 
    
[^11]: 压缩多任务嵌入用于地球观测中数据高效下游训练和推断

    Compressed Multi-task embeddings for Data-Efficient Downstream training and inference in Earth Observation

    [https://arxiv.org/abs/2403.17886](https://arxiv.org/abs/2403.17886)

    基于神经嵌入压缩的多任务嵌入方法在地球观测中实现了数据高效的训练和推断，通过压缩率与嵌入效用之间的权衡，取得了数据量显著减少的准确性。

    

    随着地球观测（EO）中大规模数据的存储库增长，模型训练和推断的转移和存储成本也在增加，消耗了大量资源。我们引入了基于神经嵌入压缩（NEC）的方法，该方法基于对数据使用者传输压缩的嵌入而不是原始数据。我们通过学习神经压缩来调整基础模型（FM），生成多任务嵌入，同时在压缩率和嵌入效用之间进行权衡。我们仅针对FM参数的一小部分（10%）进行更新，进行短时间训练（预训练迭代的1%）。我们在两个EO任务上评估了NEC：场景分类和语义分割。与将传统压缩应用于原始数据相比，NEC在减少数据量方面可实现类似的准确性，降低了75%到90%的数据量。即使在99.7%的压缩下，在场景分类任务上性能仅下降了5%。总体而言，NEC是一种数据高效

    arXiv:2403.17886v1 Announce Type: new  Abstract: As repositories of large scale data in earth observation (EO) have grown, so have transfer and storage costs for model training and inference, expending significant resources. We introduce Neural Embedding Compression (NEC), based on the transfer of compressed embeddings to data consumers instead of raw data. We adapt foundation models (FM) through learned neural compression to generate multi-task embeddings while navigating the tradeoff between compression rate and embedding utility. We update only a small fraction of the FM parameters (10%) for a short training period (1% of the iterations of pre-training). We evaluate NEC on two EO tasks: scene classification and semantic segmentation. Compared with applying traditional compression to the raw data, NEC achieves similar accuracy with a 75% to 90% reduction in data. Even at 99.7% compression, performance drops by only 5% on the scene classification task. Overall, NEC is a data-efficient
    
[^12]: 用联邦学习增强数据网格

    Empowering Data Mesh with Federated Learning

    [https://arxiv.org/abs/2403.17878](https://arxiv.org/abs/2403.17878)

    数据网格提出了一种去中心化的数据范式，通过将数据所有权分布到每个数据领域，同时保持联合治理，以克服数据源激增和及时分析处理需求增长的挑战。

    

    数据架构的演变见证了数据湖的兴起，旨在解决数据管理的瓶颈并推动智能决策。然而，这种集中化架构受制于数据源的激增和对及时分析处理的日益增长需求。为了克服这些挑战，提出了一种新的数据范式，数据网格。数据网格将领域视为首要关注点，通过将数据所有权从中央团队分发到每个数据领域，同时保持联合治理来监控领域及其数据产品。像Paypal、Netflix和Zalando等许多亿美元组织已经基于这种新架构转变了他们的数据分析流程。在这种去中心化架构中，数据由每个领域团队本地保存，传统的集中式机器学习无法在多个领域之间进行有效分析。

    arXiv:2403.17878v1 Announce Type: new  Abstract: The evolution of data architecture has seen the rise of data lakes, aiming to solve the bottlenecks of data management and promote intelligent decision-making. However, this centralized architecture is limited by the proliferation of data sources and the growing demand for timely analysis and processing. A new data paradigm, Data Mesh, is proposed to overcome these challenges. Data Mesh treats domains as a first-class concern by distributing the data ownership from the central team to each data domain, while keeping the federated governance to monitor domains and their data products. Many multi-million dollar organizations like Paypal, Netflix, and Zalando have already transformed their data analysis pipelines based on this new architecture. In this decentralized architecture where data is locally preserved by each domain team, traditional centralized machine learning is incapable of conducting effective analysis across multiple domains,
    
[^13]: 量子假设检验的样本复杂度

    Sample complexity of quantum hypothesis testing

    [https://arxiv.org/abs/2403.17868](https://arxiv.org/abs/2403.17868)

    本文研究了量子假设检验的样本复杂度，得出了对称和非对称设置中的二进制量子假设检验的样本复杂度与反错误概率的对数和保真度的负对数的关系。

    

    传统上，人们从信息论的角度研究量子假设检验，在这种情况下，人们对错误概率的最优衰减速率感兴趣，这个速率是未知状态的样本数量函数。本文研究了量子假设检验的样本复杂度，旨在确定达到所需错误概率所需的最少样本数量。通过利用已有文献中关于量子假设检验的丰富知识，我们表征了对称和非对称设置中的二进制量子假设检验的样本复杂度，并提供了多个量子假设检验的样本复杂度的界限。更详细地说，我们证明了对称二进制量子假设检验的样本复杂度对反错误概率的对数和保真度的负对数的对数。

    arXiv:2403.17868v1 Announce Type: cross  Abstract: Quantum hypothesis testing has been traditionally studied from the information-theoretic perspective, wherein one is interested in the optimal decay rate of error probabilities as a function of the number of samples of an unknown state. In this paper, we study the sample complexity of quantum hypothesis testing, wherein the goal is to determine the minimum number of samples needed to reach a desired error probability. By making use of the wealth of knowledge that already exists in the literature on quantum hypothesis testing, we characterize the sample complexity of binary quantum hypothesis testing in the symmetric and asymmetric settings, and we provide bounds on the sample complexity of multiple quantum hypothesis testing. In more detail, we prove that the sample complexity of symmetric binary quantum hypothesis testing depends logarithmically on the inverse error probability and inversely on the negative logarithm of the fidelity. 
    
[^14]: 利用领域知识通过神经概率软逻辑引导对话结构识别

    Using Domain Knowledge to Guide Dialog Structure Induction via Neural Probabilistic Soft Logic

    [https://arxiv.org/abs/2403.17853](https://arxiv.org/abs/2403.17853)

    通过将领域知识注入生成神经模型的潜在空间，提出了Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI)方法，用于引导对话结构识别，应对训练语料有限/嘈杂以及测试对话领域分布转变等问题。

    

    对话结构识别（DSI）是推断给定目标导向对话的潜在对话结构（即一组对话状态及其时间转换）的任务。这是现代对话系统设计和话语分析的关键组成部分。本文探讨了一种神经符号方法作为这些问题的潜在解决方案。我们介绍了一种称为神经概率软逻辑对话结构识别（NEUPSL DSI）的原则性方法，该方法将符号知识注入生成神经模型的潜在空间。我们进行了关于NEUPSL DSI学习对隐藏表示质量的影响的彻底实证调查。

    arXiv:2403.17853v1 Announce Type: new  Abstract: Dialog Structure Induction (DSI) is the task of inferring the latent dialog structure (i.e., a set of dialog states and their temporal transitions) of a given goal-oriented dialog. It is a critical component for modern dialog system design and discourse analysis. Existing DSI approaches are often purely data-driven, deploy models that infer latent states without access to domain knowledge, underperform when the training corpus is limited/noisy, or have difficulty when test dialogs exhibit distributional shifts from the training domain. This work explores a neural-symbolic approach as a potential solution to these problems. We introduce Neural Probabilistic Soft Logic Dialogue Structure Induction (NEUPSL DSI), a principled approach that injects symbolic knowledge into the latent space of a generative neural model. We conduct a thorough empirical investigation on the effect of NEUPSL DSI learning on hidden representation quality, few-shot 
    
[^15]: 通过将数据转化为与偏见正交的方式实现反事实公平性

    Counterfactual Fairness through Transforming Data Orthogonal to Bias

    [https://arxiv.org/abs/2403.17852](https://arxiv.org/abs/2403.17852)

    提出了一种新颖的数据预处理算法，正交于偏见（OB），通过确保数据与敏感变量不相关，实现机器学习应用中的反事实公平性。

    

    机器学习模型在解决各个领域的复杂问题中展现出了卓越的能力。然而，这些模型有时可能表现出有偏见的决策，导致不同群体之间的待遇不平等。尽管公平性方面的研究已经很广泛，但多元连续敏感变量对决策结果的微妙影响尚未得到充分研究。我们引入了一种新颖的数据预处理算法，即正交于偏见（OB），旨在消除连续敏感变量的影响，从而促进机器学习应用中的反事实公平性。我们的方法基于结构因果模型（SCM）中联合正态分布的假设，证明了通过确保数据与敏感变量不相关即可实现反事实公平性。OB算法与模型无关，适用于多种机器学习应用。

    arXiv:2403.17852v1 Announce Type: new  Abstract: Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine 
    
[^16]: 气候细化：基于深度学习的带有注意力块和跳跃连接的降水数据超分辨率模型

    Climate Downscaling: A Deep-Learning Based Super-resolution Model of Precipitation Data with Attention Block and Skip Connections

    [https://arxiv.org/abs/2403.17847](https://arxiv.org/abs/2403.17847)

    本研究提出了基于深度学习的气候细化超分辨率模型，结合了注意力块和跳跃连接，旨在有效预测降水数据，为缓解气候变化带来的影响和提高水资源利用效率做出贡献。

    

    人类活动加速了化石燃料的消耗并产生了温室气体，导致了当今迫在眉睫的问题：全球变暖和气候变化。这些间接造成严重的自然灾害，许多生命受苦以及农业财产的巨大损失。为了减轻对我们土地的影响，科学家们正在开发可再生、可重复使用和清洁能源，气候学家正试图预测极端天气。同时，各国政府正在公布节约资源的政策，以建立更环保的社会并唤起环境意识。其中最具影响力的因素之一就是降水，将凝结的水蒸气带到土地上。水资源是社会中最重要但基本的需求，不仅支持我们的生活，也支持经济。在台湾，尽管平均年降水量高达2500毫米，但每人的水分配量低于全球平均水平，这是由于...

    arXiv:2403.17847v1 Announce Type: cross  Abstract: Human activities accelerate consumption of fossil fuels and produce greenhouse gases, resulting in urgent issues today: global warming and the climate change. These indirectly cause severe natural disasters, plenty of lives suffering and huge losses of agricultural properties. To mitigate impacts on our lands, scientists are developing renewable, reusable, and clean energies and climatologists are trying to predict the extremes. Meanwhile, governments are publicizing resource-saving policies for a more eco-friendly society and arousing environment awareness. One of the most influencing factors is the precipitation, bringing condensed water vapor onto lands. Water resources are the most significant but basic needs in society, not only supporting our livings, but also economics. In Taiwan, although the average annual precipitation is up to 2,500 millimeter (mm), the water allocation for each person is lower than the global average due to
    
[^17]: 基于语言驱动的机器人导航的分层开放词汇3D场景图

    Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation

    [https://arxiv.org/abs/2403.17846](https://arxiv.org/abs/2403.17846)

    提出了一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法，可以有效代表多层建筑并允许机器人在其中穿行。

    

    最近的开放词汇机器人映射方法利用预先训练的视觉-语言特征丰富了密集几何地图。虽然这些地图允许在查询某种语言概念时预测逐点显著性地图，但大规模环境和超出对象级别的抽象查询仍然是一个相当大的障碍，最终限制了基于语言的机器人导航。在这项工作中，我们提出了HOV-SG，一种用于语言驱动的机器人导航的分层开放词汇3D场景图映射方法。通过利用开放词汇视觉基础模型，我们首先在3D空间中获得了最先进的开放词汇分段级地图，然后构建了由地板、房间和对象概念组成的3D场景图层次结构，每个都包含开放性词汇特征。我们的方法能够表示多层建筑，并且允许机器人使用跨层Voronoi图穿越这些建筑。HOV-SG进行了评估。

    arXiv:2403.17846v1 Announce Type: cross  Abstract: Recent open-vocabulary robot mapping methods enrich dense geometric maps with pre-trained visual-language features. While these maps allow for the prediction of point-wise saliency maps when queried for a certain language concept, large-scale environments and abstract queries beyond the object level still pose a considerable hurdle, ultimately limiting language-grounded robotic navigation. In this work, we present HOV-SG, a hierarchical open-vocabulary 3D scene graph mapping approach for language-grounded robot navigation. Leveraging open-vocabulary vision foundation models, we first obtain state-of-the-art open-vocabulary segment-level maps in 3D and subsequently construct a 3D scene graph hierarchy consisting of floor, room, and object concepts, each enriched with open-vocabulary features. Our approach is able to represent multi-story buildings and allows robotic traversal of those using a cross-floor Voronoi graph. HOV-SG is evaluat
    
[^18]: TractOracle: 为基于RL的纤维束追踪提供一个解剖学知识驱动的奖励函数

    TractOracle: towards an anatomically-informed reward function for RL-based tractography

    [https://arxiv.org/abs/2403.17845](https://arxiv.org/abs/2403.17845)

    TractOracle 提出了一种新的RL纤维束追踪系统，通过基于解剖学知识的奖励函数提高了真阳性比率并降低了虚假阳性比率

    

    强化学习（RL）基于的纤维束追踪是一种竞争性的替代方案，能够以较高的解剖学准确性进行操作，而无需任何注释数据。然而，迄今为止用于训练RL代理的奖励功能并不包含解剖知识，这导致代理生成虚假阳性的纤维束。在本文中，我们提出了一个新的RL纤维束追踪系统，TractOracle，它依赖于一个用于分类的追踪网络训练的奖励网络。该网络在训练过程中既用作奖励函数，又用作早期停止追踪过程的手段，从而减少虚假阳性纤维束的数量。这使得我们的系统成为一种同时评估和重建WM纤维束的独特方法。我们在一个数据集上报告了真阳性比率几乎提高了20\%，假阳性比率减少了3倍。

    arXiv:2403.17845v1 Announce Type: new  Abstract: Reinforcement learning (RL)-based tractography is a competitive alternative to machine learning and classical tractography algorithms due to its high anatomical accuracy obtained without the need for any annotated data. However, the reward functions so far used to train RL agents do not encapsulate anatomical knowledge which causes agents to generate spurious false positives tracts. In this paper, we propose a new RL tractography system, TractOracle, which relies on a reward network trained for streamline classification. This network is used both as a reward function during training as well as a mean for stopping the tracking process early and thus reduce the number of false positive streamlines. This makes our system a unique method that evaluates and reconstructs WM streamlines at the same time. We report an improvement of true positive ratios by almost 20\% and a reduction of 3x of false positive ratios on one dataset and an increase 
    
[^19]: 混合体系结构的机理设计和尺度变换

    Mechanistic Design and Scaling of Hybrid Architectures

    [https://arxiv.org/abs/2403.17844](https://arxiv.org/abs/2403.17844)

    通过机理设计和尺度变换，我们提出了一种新的混合体系结构设计方法，可以简化深度学习架构的开发过程，并可以准确评估新架构。

    

    深度学习架构的开发是一个资源密集型的过程，由于设计空间广阔、原型制作时间长以及与规模化模型训练和评估相关的高计算成本。我们致力于通过以端到端机械式架构设计（MAD）管线为基础来简化这一过程，包括小规模能力单元测试，预测尺度规律。通过一系列合成的令牌操作任务，如压缩和回忆，旨在探索能力，我们识别并测试由各种计算基元构建的新型混合体系结构。通过广泛的计算优化和一项新的状态最优化尺度分析，我们通过训练70M到7B参数之间的500多种语言模型对结果体系结构进行实验证实了这一点。令人惊讶的是，我们发现MAD合成与计算最优困惑度相关，能够准确评估新架构。

    arXiv:2403.17844v1 Announce Type: new  Abstract: The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectur
    
[^20]: GTA-HDR：用于HDR图像重建的大规模合成数据集

    GTA-HDR: A Large-Scale Synthetic Dataset for HDR Image Reconstruction

    [https://arxiv.org/abs/2403.17837](https://arxiv.org/abs/2403.17837)

    介绍了一个大规模合成的HDR图像数据集 GTA-HDR，从GTA-V视频游戏中采样，填补了现有数据集在捕捉不同场景条件和图像特征方面的不足

    

    高动态范围（HDR）内容（即图像和视频）具有广泛的应用。然而，从现实场景中捕获HDR内容是昂贵且耗时的。因此，从其低动态范围（LDR）对应物中重建视觉精确的HDR图像的具有挑战性的任务引起了视觉研究社区的关注。在这个研究问题中的一个主要挑战是缺乏能够捕捉不同场景条件（如照明、阴影、天气、位置、景观、物体、人类、建筑）和各种图像特征（如颜色、对比度、饱和度、色调、亮度、辐射度）的数据集。为了填补这一差距，在本文中，我们介绍了GTA-HDR，这是一个从GTA-V视频游戏中采样的大规模逼真HDR图像的合成数据集。我们对所提出的数据集进行了彻底评估，结果显示了在定性和定量上的显著改进。

    arXiv:2403.17837v1 Announce Type: cross  Abstract: High Dynamic Range (HDR) content (i.e., images and videos) has a broad range of applications. However, capturing HDR content from real-world scenes is expensive and time- consuming. Therefore, the challenging task of reconstructing visually accurate HDR images from their Low Dynamic Range (LDR) counterparts is gaining attention in the vision research community. A major challenge in this research problem is the lack of datasets, which capture diverse scene conditions (e.g., lighting, shadows, weather, locations, landscapes, objects, humans, buildings) and various image features (e.g., color, contrast, saturation, hue, luminance, brightness, radiance). To address this gap, in this paper, we introduce GTA-HDR, a large-scale synthetic dataset of photo-realistic HDR images sampled from the GTA-V video game. We perform thorough evaluation of the proposed dataset, which demonstrates significant qualitative and quantitative improvements of the
    
[^21]: 基于梯度投影的客户选择框架用于高效的联邦学习

    GPFL: A Gradient Projection-Based Client Selection Framework for Efficient Federated Learning

    [https://arxiv.org/abs/2403.17833](https://arxiv.org/abs/2403.17833)

    GPFL提出了一个基于梯度投影的客户选择框架，通过比较本地和全局下降方向来衡量客户价值，并采用利用-探索机制来提高性能。在实验中表现出在非独立同分布场景中优于基线，在FEMINST测试准确性方面实现超过9%的改善。

    

    联邦学习客户选择对确定参与者客户以平衡模型准确性和通信效率至关重要。现有方法在处理数据异构性、计算负担和独立客户处理方面存在局限性。为解决这些挑战，我们提出了GPFL，通过比较本地和全局下降方向来衡量客户价值。我们还采用了一种利用-探索机制来增强性能。对FEMINST和CIFAR-10数据集的实验结果表明，GPFL在非独立同分布场景中优于基线，在FEMINST测试准确性方面实现超过9%的改善。此外，GPFL通过在联邦学习中的预选和参数重用展示出更短的计算时间。

    arXiv:2403.17833v1 Announce Type: new  Abstract: Federated learning client selection is crucial for determining participant clients while balancing model accuracy and communication efficiency. Existing methods have limitations in handling data heterogeneity, computational burdens, and independent client treatment. To address these challenges, we propose GPFL, which measures client value by comparing local and global descent directions. We also employ an Exploit-Explore mechanism to enhance performance. Experimental results on FEMINST and CIFAR-10 datasets demonstrate that GPFL outperforms baselines in Non-IID scenarios, achieving over 9\% improvement in FEMINST test accuracy. Moreover, GPFL exhibits shorter computation times through pre-selection and parameter reuse in federated learning.
    
[^22]: 学习最优潮流：环境设计至关重要

    Learning the Optimal Power Flow: Environment Design Matters

    [https://arxiv.org/abs/2403.17831](https://arxiv.org/abs/2403.17831)

    研究通过实施不同的环境设计决策对强化学习解决最优潮流问题的影响，提出了对这些设计决策的首要建议。

    

    为了解决最优潮流（OPF）问题，强化学习（RL）被视为一种有前途的新方法。然而，关于将OPF问题作为RL环境的确切形式，RL-OPF文献存在着很大分歧。本文收集并实现了关于训练数据、观测空间、回合定义和奖励函数选择的文献中各种不同的环境设计决策。在实验分析中，我们展示了这些环境设计选项对RL-OPF训练性能的显著影响。此外，我们提出了一些建议关于这些设计决策的选择。所创建的环境框架是完全开源的，并可以作为RL-OPF领域未来研究的基准。

    arXiv:2403.17831v1 Announce Type: new  Abstract: To solve the optimal power flow (OPF) problem, reinforcement learning (RL) emerges as a promising new approach. However, the RL-OPF literature is strongly divided regarding the exact formulation of the OPF problem as an RL environment. In this work, we collect and implement diverse environment design decisions from the literature regarding training data, observation space, episode definition, and reward function choice. In an experimental analysis, we show the significant impact of these environment design options on RL-OPF training performance. Further, we derive some first recommendations regarding the choice of these design decisions. The created environment framework is fully open-source and can serve as a benchmark for future research in the RL-OPF field.
    
[^23]: 基于扩散的从文本描述中合成手-物体交互的方法

    DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions

    [https://arxiv.org/abs/2403.17827](https://arxiv.org/abs/2403.17827)

    提出了一种从文本描述和物体几何形状中合成逼真的手-物体交互的方法，通过三种技术实现了有效学习，包括任务分解、紧密耦合的姿势表示和不同的引导方案。

    

    生成自然的3D手-物体交互具有挑战性，因为期望生成的手部和物体动作在物理上是合理的，并且在语义上是有意义的。我们提出了一种名为DiffH2O的新方法，可以从提供的文本提示和物体几何形状中合成逼真的单手或双手物体交互。该方法引入了三种技术，可以有效地从有限数据中学习。首先，我们将任务分解为抓取阶段和基于文本交互阶段，并为每个阶段使用单独的扩散模型。在抓取阶段中，模型仅生成手部动作，而在交互阶段中，手部和物体姿势都被合成。其次，我们提出了一种紧密耦合手部和物体姿势的紧凑表示。第三，我们提出了两种不同的引导方案。

    arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
    
[^24]: 压缩语言模型是否对子群体稳健性影响较小？

    Are Compressed Language Models Less Subgroup Robust?

    [https://arxiv.org/abs/2403.17811](https://arxiv.org/abs/2403.17811)

    压缩语言模型的影响不仅取决于模型大小，还取决于压缩方法，同时发现模型压缩并不总是会使在少数子群体上的性能变差。

    

    为了减少大型语言模型的推理成本，越来越多地使用模型压缩来创建更小规模的模型。然而，我们对由数据集的标签和属性定义的少数子群体的稳健性知之甚少。在本文中，我们研究了18种不同的压缩方法和设置对BERT语言模型的子群体稳健性的影响。我们发现最差群组的性能不仅取决于模型大小，还取决于所使用的压缩方法。此外，我们发现模型压缩并不总是会使在少数子群体上的性能变差。总的来说，我们的分析有助于进一步研究模型压缩对子群体稳健性的影响。

    arXiv:2403.17811v1 Announce Type: cross  Abstract: To reduce the inference cost of large language models, model compression is increasingly used to create smaller scalable models. However, little is known about their robustness to minority subgroups defined by the labels and attributes of a dataset. In this paper, we investigate the effects of 18 different compression methods and settings on the subgroup robustness of BERT language models. We show that worst-group performance does not depend on model size alone, but also on the compression method used. Additionally, we find that model compression does not always worsen the performance on minority subgroups. Altogether, our analysis serves to further research into the subgroup robustness of model compression.
    
[^25]: 使用去噪扩散概率模型和流场生成带注释的生物医学视频

    Annotated Biomedical Video Generation using Denoising Diffusion Probabilistic Models and Flow Fields

    [https://arxiv.org/abs/2403.17808](https://arxiv.org/abs/2403.17808)

    提出了一种能够生成具有像素级注释的逼真合成显微视频的生物医学视频扩散模型，有助于解决生物医学成像领域标记数据稀缺的问题。

    

    细胞的分割和跟踪在生物医学领域中发挥着至关重要的作用，特别是在癌症研究、药物开发和发育生物学中。为了解决在生物医学成像领域中标记数据稀缺的问题，我们提出了生物医学视频扩散模型（BVDM），能够生成逼真的合成显微视频。

    arXiv:2403.17808v1 Announce Type: cross  Abstract: The segmentation and tracking of living cells play a vital role within the biomedical domain, particularly in cancer research, drug development, and developmental biology. These are usually tedious and time-consuming tasks that are traditionally done by biomedical experts. Recently, to automatize these processes, deep learning based segmentation and tracking methods have been proposed. These methods require large-scale datasets and their full potential is constrained by the scarcity of annotated data in the biomedical imaging domain. To address this limitation, we propose Biomedical Video Diffusion Model (BVDM), capable of generating realistic-looking synthetic microscopy videos. Trained only on a single real video, BVDM can generate videos of arbitrary length with pixel-level annotations that can be used for training data-hungry models. It is composed of a denoising diffusion probabilistic model (DDPM) generating high-fidelity synthet
    
[^26]: 坚信忠实：在找到模型机制时超越电路重叠

    Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms

    [https://arxiv.org/abs/2403.17806](https://arxiv.org/abs/2403.17806)

    提出了一种新方法EAP-IG，旨在更好地保持电路的核心属性：忠实

    

    最近许多语言模型（LM）可解释性研究已采用电路框架，旨在找到解释LM在给定任务上行为的最小计算子图或电路。大多数研究通过独立对每个边执行因果干预来确定哪些边属于LM的电路，但这在模型规模较大时效率低下。边缘归因修补（EAP），一种基于梯度的近似干预方法，已成为解决这一问题的可扩展但不完美的解决方案。在本文中，我们介绍了一种新方法 - 带有集成梯度的EAP（EAP-IG），旨在更好地保持电路的核心属性：忠实。如果电路是忠实的，则可以去掉电路之外的所有模型边而不会改变模型在任务上的表现；忠实性是研究电路而不是完整模型的理由。我们的实验证明，使用EAP找到的电路不太忠实

    arXiv:2403.17806v1 Announce Type: cross  Abstract: Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faith
    
[^27]: 多智能体自主驾驶场景驱动的课程生成

    Scenario-Based Curriculum Generation for Multi-Agent Autonomous Driving

    [https://arxiv.org/abs/2403.17805](https://arxiv.org/abs/2403.17805)

    提出了MATS-Gym，一个用于在CARLA中训练智能体的多智能体交通场景框架，能够自动生成具有可变智能体数量的交通场景并整合了各种现有的交通场景描述方法。

    

    多样化和复杂训练场景的自动化生成在许多复杂学习任务中是重要的。特别是在现实世界的应用领域，如自主驾驶，自动生成课程被认为对获得强健和通用策略至关重要。然而，在充满挑战的仿真环境中，为交通场景中的多个异构智能体进行设计通常被认为是一项繁琐且耗时的任务。在我们的工作中，我们引入了MATS-Gym，一个用于在高保真驾驶模拟器CARLA中训练智能体的多智能体交通场景框架。MATS-Gym是一个用于自主驾驶的多智能体训练框架，使用部分场景规范生成具有可变智能体数量的交通场景。这篇论文将各种现有的交通场景描述方法统一到一个单一的训练框架中，并演示了如何将其与其他自主驾驶算法集成。

    arXiv:2403.17805v1 Announce Type: cross  Abstract: The automated generation of diverse and complex training scenarios has been an important ingredient in many complex learning tasks. Especially in real-world application domains, such as autonomous driving, auto-curriculum generation is considered vital for obtaining robust and general policies. However, crafting traffic scenarios with multiple, heterogeneous agents is typically considered as a tedious and time-consuming task, especially in more complex simulation environments. In our work, we introduce MATS-Gym, a Multi-Agent Traffic Scenario framework to train agents in CARLA, a high-fidelity driving simulator. MATS-Gym is a multi-agent training framework for autonomous driving that uses partial scenario specifications to generate traffic scenarios with variable numbers of agents. This paper unifies various existing approaches to traffic scenario description into a single training framework and demonstrates how it can be integrated wi
    
[^28]: 安全聚合在面对成员推断攻击时并非私密的

    Secure Aggregation is Not Private Against Membership Inference Attacks

    [https://arxiv.org/abs/2403.17775](https://arxiv.org/abs/2403.17775)

    本文探讨了安全聚合在隐私方面的问题，揭示了其在面对成员推断攻击时并不具备足够的私密性。

    

    安全聚合（SecAgg）是联邦学习中常用的隐私增强机制，仅允许服务器访问模型更新的聚合结果，同时保护个体更新的机密性。尽管有关SecAgg保护隐私能力的广泛声明，但缺乏对其隐私性的正式分析，因此这些假设是不合理的。本文通过将SecAgg视为每个局部更新的局部差分隐私（LDP）机制，深入探讨了SecAgg的隐私影响。我们设计了一种简单攻击方式，其中对手服务器试图在SecAgg下的联邦学习的单一训练轮中推断出客户端提交的更新向量是两个可能向量中的哪一个。通过进行隐私审核，我们评估了该攻击的成功概率，并量化了SecAgg提供的LDP保证。我们的数字结果揭示了，与普遍声明相反，SecAgg并没有提供私密性。

    arXiv:2403.17775v1 Announce Type: new  Abstract: Secure aggregation (SecAgg) is a commonly-used privacy-enhancing mechanism in federated learning, affording the server access only to the aggregate of model updates while safeguarding the confidentiality of individual updates. Despite widespread claims regarding SecAgg's privacy-preserving capabilities, a formal analysis of its privacy is lacking, making such presumptions unjustified. In this paper, we delve into the privacy implications of SecAgg by treating it as a local differential privacy (LDP) mechanism for each local update. We design a simple attack wherein an adversarial server seeks to discern which update vector a client submitted, out of two possible ones, in a single training round of federated learning under SecAgg. By conducting privacy auditing, we assess the success probability of this attack and quantify the LDP guarantees provided by SecAgg. Our numerical results unveil that, contrary to prevailing claims, SecAgg offer
    
[^29]: 从学术复杂性到公众叙事：科学新闻报道生成的数据集

    SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation

    [https://arxiv.org/abs/2403.17768](https://arxiv.org/abs/2403.17768)

    科学新闻报道生成的自动化提高了学术见解的可访问性，该研究提出了一个包含学术出版物和相应科学新闻报道的数据集，用于探索自动生成科学新闻报道的可能性。

    

    科学新闻报道作为一个桥梁，巧妙地将复杂的研究文章翻译成与更广泛的公众 resonant 的报道。这种叙事的自动生成增强了学术见解的可访问性。在本文中，我们提出了一个新的语料库来促进这种范式的发展。我们的语料库包括九个学科领域中学术出版物及其相应科学新闻报道的平行编译。为了证明我们数据集的实用性和可靠性，我们进行了广泛分析，突出了科学新闻叙事和学术文稿之间的可读性和简洁性差异。我们使用最先进的文本生成模型基准测试我们的数据集。评估过程包括自动评估和人工评估，为未来探索自动生成科学新闻报道打下了基础。

    arXiv:2403.17768v1 Announce Type: cross  Abstract: Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related 
    
[^30]: 在具有不确定标签的半监督学习中的渐近贝叶斯风险

    Asymptotic Bayes risk of semi-supervised learning with uncertain labeling

    [https://arxiv.org/abs/2403.17767](https://arxiv.org/abs/2403.17767)

    论文研究了具有不确定标签的半监督学习中的渐近贝叶斯风险计算，并通过与最佳算法比较得出新的见解。

    

    本文考虑了高斯混合模型上的半监督分类设置，其中数据的标签不像通常那样严格，而是带有不确定标签。我们的主要目标是计算该模型的贝叶斯风险。我们比较了该模型的贝叶斯风险与目前已知的最佳算法的行为。这种比较最终为该算法提供了新的见解。

    arXiv:2403.17767v1 Announce Type: cross  Abstract: This article considers a semi-supervised classification setting on a Gaussian mixture model, where the data is not labeled strictly as usual, but instead with uncertain labels. Our main aim is to compute the Bayes risk for this model. We compare the behavior of the Bayes risk and the best known algorithm for this model. This comparison eventually gives new insights over the algorithm.
    
[^31]: CRISM高光谱数据的Noise2Noise去噪

    Noise2Noise Denoising of CRISM Hyperspectral Data

    [https://arxiv.org/abs/2403.17757](https://arxiv.org/abs/2403.17757)

    引入了Noise2Noise4Mars（N2N4M）模型用于去除CRISM图像噪声，无需零噪声目标数据，展现出在合成噪声数据和CRISM图像上的强大性能，并具有优于基准方法的效果。

    

    由于时间推移，Compact Reconnaissance Imaging Spectrometer for Mars（CRISM）获取的高光谱数据已经导致了火星表面矿物学映射的无与伦比。本文引入了一种新的数据驱动模型架构Noise2Noise4Mars（N2N4M），用于去除CRISM图像中的噪声。我们的模型是自我监督的，并且不需要零噪声目标数据，适用于在行星科学应用中高质量标记数据稀缺的情况。我们展示了其在合成噪声数据和CRISM图像上的强大性能，以及对下游分类性能的影响，在大多数指标上优于基准方法。这使得可以进行对火星表面关键兴趣点的详细分析，包括提议的着陆点。

    arXiv:2403.17757v1 Announce Type: cross  Abstract: Hyperspectral data acquired by the Compact Reconnaissance Imaging Spectrometer for Mars (CRISM) have allowed for unparalleled mapping of the surface mineralogy of Mars. Due to sensor degradation over time, a significant portion of the recently acquired data is considered unusable. Here a new data-driven model architecture, Noise2Noise4Mars (N2N4M), is introduced to remove noise from CRISM images. Our model is self-supervised and does not require zero-noise target data, making it well suited for use in Planetary Science applications where high quality labelled data is scarce. We demonstrate its strong performance on synthetic-noise data and CRISM images, and its impact on downstream classification performance, outperforming benchmark methods on most metrics. This allows for detailed analysis for critical sites of interest on the Martian surface, including proposed lander sites.
    
[^32]: CCDSReFormer：一种交叉双流增强整流变压器模型用于交通流预测

    CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream Enhanced Rectified Transformer Model

    [https://arxiv.org/abs/2403.17753](https://arxiv.org/abs/2403.17753)

    CCDSReFormer模型引入了三种创新模块来提高交通流预测的效率和准确性，包括增强整流空间自注意力、增强整流延迟感知自注意力和增强整流时间自注意力，以实现稀疏注意力、可解释的局部信息和融合空间和时间见解。

    

    准确而有效的交通预测对于智能交通系统至关重要，在城市交通规划和管理中至关重要。当前的时空变压器模型，尽管具有预测能力，但在平衡计算效率和准确性、偏好全局而非局部信息、以及分开处理空间和时间数据方面存在困难，限制了对复杂交互作用的洞察。我们引入了交叉双流增强整流变压器模型（CCDSReFormer），其中包括三个创新模块：增强整流空间自注意力（ReSSA）、增强整流延迟感知自注意力（ReDASA）和增强整流时间自注意力（ReTSA）。这些模块旨在通过稀疏注意力降低计算需求，专注于局部信息以更好地理解交通动态，并通过独特的学习方法合并空间和时间见解。对六个真实数据集进行了广泛测试。

    arXiv:2403.17753v1 Announce Type: new  Abstract: Accurate, and effective traffic forecasting is vital for smart traffic systems, crucial in urban traffic planning and management. Current Spatio-Temporal Transformer models, despite their prediction capabilities, struggle with balancing computational efficiency and accuracy, favoring global over local information, and handling spatial and temporal data separately, limiting insight into complex interactions. We introduce the Criss-Crossed Dual-Stream Enhanced Rectified Transformer model (CCDSReFormer), which includes three innovative modules: Enhanced Rectified Spatial Self-attention (ReSSA), Enhanced Rectified Delay Aware Self-attention (ReDASA), and Enhanced Rectified Temporal Self-attention (ReTSA). These modules aim to lower computational needs via sparse attention, focus on local information for better traffic dynamics understanding, and merge spatial and temporal insights through a unique learning method. Extensive tests on six real
    
[^33]: 不让任何患者掉队：增强罕见病患者的药物推荐

    Leave No Patient Behind: Enhancing Medication Recommendation for Rare Disease Patients

    [https://arxiv.org/abs/2403.17745](https://arxiv.org/abs/2403.17745)

    提出了一种名为RAREMed的新模型，利用预训练-微调学习范式增强罕见疾病的药物推荐准确性，并引入了自监督预训练任务来学习专门的药物需求和临床代码之间的关系

    

    药物推荐系统在医疗保健领域引起了广泛关注，可以根据患者的临床信息提供定制和有效的药物组合。然而，现有方法往往存在公平性问题，因为相较于患有常见疾病的患者，对于患有罕见病症的患者，推荐往往更准确。在本文中，我们提出了一种名为Robust and Accurate REcommendations for Medication（RAREMed）的创新模型，利用预训练-微调学习范式来增强罕见疾病的准确性。RAREMed采用具有统一输入序列方法的Transformer编码器来捕捉疾病和程序代码之间复杂关系。此外，它引入了两个自监督的预训练任务，即Sequence Matching Prediction（SMP）和Self Reconstruction（SR），来学习专门的药物需求和临床代码之间的相互关系。

    arXiv:2403.17745v1 Announce Type: new  Abstract: Medication recommendation systems have gained significant attention in healthcare as a means of providing tailored and effective drug combinations based on patients' clinical information. However, existing approaches often suffer from fairness issues, as recommendations tend to be more accurate for patients with common diseases compared to those with rare conditions. In this paper, we propose a novel model called Robust and Accurate REcommendations for Medication (RAREMed), which leverages the pretrain-finetune learning paradigm to enhance accuracy for rare diseases. RAREMed employs a transformer encoder with a unified input sequence approach to capture complex relationships among disease and procedure codes. Additionally, it introduces two self-supervised pre-training tasks, namely Sequence Matching Prediction (SMP) and Self Reconstruction (SR), to learn specialized medication needs and interrelations among clinical codes. Experimental 
    
[^34]: EulerFormer：具有复杂向量注意力的顺序用户行为建模

    EulerFormer: Sequential User Behavior Modeling with Complex Vector Attention

    [https://arxiv.org/abs/2403.17729](https://arxiv.org/abs/2403.17729)

    EulerFormer提出了一种具有复杂向量注意力的新型转换器变体，统一了语义差异和位置差异的理论框架。

    

    为了捕捉用户偏好，转换器模型被广泛应用于建模顺序用户行为数据。转换器架构的核心在于自注意力机制，它计算序列中的成对注意力分数。由于排列等变性的特性，位置编码用于增强令牌表示之间的注意力。在这种设定下，成对注意力分数可以通过语义差异和位置差异两者衍生出来。然而，先前的研究经常以不同方式建模两种不同类型的差异测量，这可能限制了序列建模的表达能力。为了解决这个问题，本文提出了一种名为EulerFormer的具有复杂向量注意力的新型转换器变体，提供了一个统一的理论框架来表述语义差异和位置差异。 EulerFormer包含两个关键技术改进。

    arXiv:2403.17729v1 Announce Type: cross  Abstract: To capture user preference, transformer models have been widely applied to model sequential user behavior data. The core of transformer architecture lies in the self-attention mechanism, which computes the pairwise attention scores in a sequence. Due to the permutation-equivariant nature, positional encoding is used to enhance the attention between token representations. In this setting, the pairwise attention scores can be derived by both semantic difference and positional difference. However, prior studies often model the two kinds of difference measurements in different ways, which potentially limits the expressive capacity of sequence modeling. To address this issue, this paper proposes a novel transformer variant with complex vector attention, named EulerFormer, which provides a unified theoretical framework to formulate both semantic difference and positional difference. The EulerFormer involves two key technical improvements. Fi
    
[^35]: 掩码自动编码器是PDE学习者

    Masked Autoencoders are PDE Learners

    [https://arxiv.org/abs/2403.17728](https://arxiv.org/abs/2403.17728)

    掩码自动编码器在偏微分方程求解器中表现出色，通过自监督学习跨越PDEs，可以学习用于下游任务的有用潜在表示。

    

    神经求解器用于偏微分方程（PDE）具有巨大潜力，但实用性目前受到其泛化能力的限制。 PDE在广泛的尺度上演变并展示出多样化的行为；预测这些现象将需要学习跨越各种输入的表示，这些输入可能涵盖不同的系数、几何图形或方程。作为通向可泛化PDE建模的一步，我们为PDEs调整了掩码预训练。通过自监督学习跨越PDEs，掩码自动编码器可以学习有用的潜在表示，以用于下游任务。特别是，掩码预训练可以改善神经求解器对未见方程的系数回归和时间步骤性能。我们希望掩码预训练能成为一种通用方法，可以在大型、未标记和异构数据集上学习规模化的潜在物理学。

    arXiv:2403.17728v1 Announce Type: new  Abstract: Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.
    
[^36]: 旋转扫描：带有三元SSM模块的UNet-like Mamba用于医学图像分割

    Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical Image Segmentation

    [https://arxiv.org/abs/2403.17701](https://arxiv.org/abs/2403.17701)

    本文提出了Triplet Mamba-UNet，利用残余VSS块提取密集上下文特征，并利用Triplet SSM融合空间和通道维度上的特征。

    

    图像分割在医疗领域的诊断和治疗中占据重要位置。传统的卷积神经网络（CNN）和Transformer模型在这一领域取得了重大进展，但仍然面临由于有限感受野或高计算复杂性而带来的挑战。最近，状态空间模型（SSM），特别是Mamba及其变体，在视觉领域表现出显著性能。然而，它们的特征提取方法可能不够有效，保留了一些冗余结构，留下了参数减少的空间。受先前的空间和通道注意方法的启发，我们提出了Triplet Mamba-UNet。该方法利用残余VSS块来提取密集的上下文特征，同时利用Triplet SSM来融合空间和通道维度上的特征。我们在ISIC17、ISIC18、CVC-300、CVC-ClinicDB上进行了实验。

    arXiv:2403.17701v1 Announce Type: cross  Abstract: Image segmentation holds a vital position in the realms of diagnosis and treatment within the medical domain. Traditional convolutional neural networks (CNNs) and Transformer models have made significant advancements in this realm, but they still encounter challenges because of limited receptive field or high computing complexity. Recently, State Space Models (SSMs), particularly Mamba and its variants, have demonstrated notable performance in the field of vision. However, their feature extraction methods may not be sufficiently effective and retain some redundant structures, leaving room for parameter reduction. Motivated by previous spatial and channel attention methods, we propose Triplet Mamba-UNet. The method leverages residual VSS Blocks to extract intensive contextual features, while Triplet SSM is employed to fuse features across spatial and channel dimensions. We conducted experiments on ISIC17, ISIC18, CVC-300, CVC-ClinicDB, 
    
[^37]: MEP: 多核学习增强相对位置编码长度外推

    MEP: Multiple Kernel Learning Enhancing Relative Positional Encoding Length Extrapolation

    [https://arxiv.org/abs/2403.17698](https://arxiv.org/abs/2403.17698)

    提出了MEP方法，通过结合不同核函数生成偏差来解决变压器模型在长度外推时的准确性降低问题

    

    当预测的序列长度超过训练中看到的长度时，变压器的推理准确性会降低。现有的相对位置编码方法，如基于ALiBi技术的方法，仅通过实现单个核函数来解决长度外推挑战，这会根据它们之间的距离为每个后Softmax注意力分数引入恒定偏差。这些方法未探讨或使用多个核函数来应对外推挑战。借鉴ALiBi方法，本研究提出了一种新颖的相对位置编码方法，称为MEP，它采用加权平均来结合不同的核函数（如指数核和高斯核）产生一个应用于后Softmax注意力分数的偏差。最初，该框架利用各种核函数构建多个核函数。每个核

    arXiv:2403.17698v1 Announce Type: cross  Abstract: When the predicted sequence length exceeds the length seen during training, the transformer's inference accuracy diminishes. Existing relative position encoding methods, such as those based on the ALiBi technique, address the length extrapolation challenge exclusively through the implementation of a single kernel function, which introduces a constant bias to every post-softmax attention scores according to their distance. These approaches do not investigate or employ multiple kernel functions to address the extrapolation challenge. Drawing on the ALiBi approach, this study proposes a novel relative positional encoding method, called MEP, which employs a weighted average to combine distinct kernel functions(such as the exponential kernel and the Gaussian kernel) to generate a bias that is applied to post-softmax attention scores. Initially, the framework utilizes various kernel functions to construct multiple kernel functions. Each kern
    
[^38]: PlainMamba：改进视觉识别中的非层次Mamba

    PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition

    [https://arxiv.org/abs/2403.17695](https://arxiv.org/abs/2403.17695)

    改进了视觉识别中的非层次Mamba模型，通过改进连续2D扫描过程和方向感知更新，提高了从二维图像中学习特征的能力。

    

    我们提出PlainMamba：一种简单的非层次状态空间模型（SSM），旨在用于一般的视觉识别。最近的Mamba模型展示了如何在顺序数据上SSM可以与其他架构竞争激烈，并已初步尝试将其应用于图像。在本文中，我们进一步改进了Mamba的选择性扫描过程以适应视觉领域，通过（i）通过确保在扫描序列中令牌相邻来改善空间连续性的连续2D扫描过程，以及（ii）启用模型区分令牌的空间关系的方向感知更新，通过编码方向信息。我们的架构设计易于使用和易于扩展，由堆叠相同的PlainMamba块形成，结果是始终具有恒定宽度的模型。通过去除

    arXiv:2403.17695v1 Announce Type: cross  Abstract: We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the 
    
[^39]: 采用扩散模型的流形引导Lyapunov控制

    Manifold-Guided Lyapunov Control with Diffusion Models

    [https://arxiv.org/abs/2403.17692](https://arxiv.org/abs/2403.17692)

    使用扩散模型进行流形引导的Lyapunov控制，通过识别最接近预定流形的渐近稳定向量场来生成稳定控制函数，展示了在快速零-shot控制和泛化能力方面的潜力。

    

    本文提出了一种新方法，利用扩散模型为一大类动态系统生成稳定控制器。核心目标是通过识别与预定流形相对最近的渐近稳定向量场来开发稳定控制函数，并根据这一发现调整控制函数。为实现这一目标，我们采用了一个在渐近稳定向量场及其对应的Lyapunov函数之间进行训练的扩散模型。我们的数值结果表明，该预训练模型可以有效快速地实现对先前未见系统的稳定，展示了我们方法在快速零-shot控制和泛化能力方面的潜力。

    arXiv:2403.17692v1 Announce Type: cross  Abstract: This paper presents a novel approach to generating stabilizing controllers for a large class of dynamical systems using diffusion models. The core objective is to develop stabilizing control functions by identifying the closest asymptotically stable vector field relative to a predetermined manifold and adjusting the control function based on this finding. To achieve this, we employ a diffusion model trained on pairs consisting of asymptotically stable vector fields and their corresponding Lyapunov functions. Our numerical results demonstrate that this pre-trained model can achieve stabilization over previously unseen systems efficiently and rapidly, showcasing the potential of our approach in fast zero-shot control and generalizability.
    
[^40]: DP-SGD的隐私性有多私密？

    How Private is DP-SGD?

    [https://arxiv.org/abs/2403.17673](https://arxiv.org/abs/2403.17673)

    ABLQ机制在不同批处理采样下的隐私保证存在实质性差距，DP-SGD的实际实现通常使用基于洗牌的方法，但更可靠的隐私分析却来自于基于泊松子采样的方法。

    

    我们展示了在不同类型的批处理采样下，自适应批量线性查询（ABLQ）机制的隐私保证之间存在着实质性差距：（i）洗牌，和（ii）泊松子采样；典型的差分隐私随机梯度下降（DP-SGD）分析通过将其解释为ABLQ的后处理来进行。虽然基于洗牌的DP-SGD在实际实现中更常用，但它在隐私分析上既不易于解析也不易于数值计算。另一方面，基于泊松子采样的DP-SGD难以实现可扩展性，但具有良好理解的隐私分析，有多个开源的数值紧密的隐私账户可用。这导致了在实践中常见的做法，即使用基于洗牌的DP-SGD，但使用相应泊松子采样版本的隐私分析。我们的结果表明，隐私分析之间可能存在实质性差距

    arXiv:2403.17673v1 Announce Type: new  Abstract: We demonstrate a substantial gap between the privacy guarantees of the Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of Differentially Private Stochastic Gradient Descent (DP-SGD) follows by interpreting it as a post-processing of ABLQ. While shuffling based DP-SGD is more commonly used in practical implementations, it is neither analytically nor numerically amenable to easy privacy analysis. On the other hand, Poisson subsampling based DP-SGD is challenging to scalably implement, but has a well-understood privacy analysis, with multiple open-source numerically tight privacy accountants available. This has led to a common practice of using shuffling based DP-SGD in practice, but using the privacy analysis for the corresponding Poisson subsampling version. Our result shows that there can be a substantial gap between the privacy anal
    
[^41]: CANOS：一种快速且可扩展的神经AC-OPF求解器，对N-1扰动具有鲁棒性

    CANOS: A Fast and Scalable Neural AC-OPF Solver Robust To N-1 Perturbations

    [https://arxiv.org/abs/2403.17660](https://arxiv.org/abs/2403.17660)

    通过训练神经网络系统CANOS，在保持速度的同时预测接近最优解（在1%内），从而解决了准确性和运行可行性之间的折衷问题

    

    最优功率流（OPF）涉及一系列相关的优化问题，其目标是高效且安全地运行电力系统。在最简单的设置中，OPF确定需要发电多少电力，以在满足电力需求的同时最小化成本，并满足物理和运行约束。即使在最简单的情况下，由于解决精确问题的速度与现代求解器过慢，电力网格运营商使用AC-OPF问题的近似解。这些近似解为了速度而牺牲了准确性和运行可行性。这种权衡导致了昂贵的“增值付款”和碳排放的增加，尤其是对于大型电力网格来说。在本研究中，我们训练了一个深度学习系统（CANOS），以预测接近最优解（真实AC-OPF成本的1%内）而不牺牲速度（运行时间仅为33-65毫秒）。重要的是，CANOS可扩展到逼真的网格规模

    arXiv:2403.17660v1 Announce Type: new  Abstract: Optimal Power Flow (OPF) refers to a wide range of related optimization problems with the goal of operating power systems efficiently and securely. In the simplest setting, OPF determines how much power to generate in order to minimize costs while meeting demand for power and satisfying physical and operational constraints. In even the simplest case, power grid operators use approximations of the AC-OPF problem because solving the exact problem is prohibitively slow with state-of-the-art solvers. These approximations sacrifice accuracy and operational feasibility in favor of speed. This trade-off leads to costly "uplift payments" and increased carbon emissions, especially for large power grids. In the present work, we train a deep learning system (CANOS) to predict near-optimal solutions (within 1% of the true AC-OPF cost) without compromising speed (running in as little as 33--65 ms). Importantly, CANOS scales to realistic grid sizes wi
    
[^42]: SGHormer：一种由脉冲驱动的节能图变换器

    SGHormer: An Energy-Saving Graph Transformer Driven by Spikes

    [https://arxiv.org/abs/2403.17656](https://arxiv.org/abs/2403.17656)

    SGHormer是一种由脉冲驱动的节能图变换器，通过将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本，提高了图变换器的效率。

    

    具有强大表示学习能力的图变换器（GTs）在各种图任务中取得巨大成功。然而，GTs出色性能背后的代价是更高的能量消耗和计算开销。传统变换器中注意力计算过程中的复杂结构和二次复杂度严重影响其在大规模图数据上的可扩展性。虽然现有方法在简化块之间的组合或注意力学习范式方面取得了进展以提高GTs的效率，但在构建GT框架时很少考虑源自生物学上合理结构的一系列节能解决方案。为此，我们提出了一种新的基于脉冲的图变换器（SGHormer）。它将全精度嵌入转换为稀疏和二值化脉冲以减少内存和计算成本。SGHormer中的脉冲图自注意力和脉冲修正块可以显著减少计算和存储开销。

    arXiv:2403.17656v1 Announce Type: cross  Abstract: Graph Transformers (GTs) with powerful representation learning ability make a huge success in wide range of graph tasks. However, the costs behind outstanding performances of GTs are higher energy consumption and computational overhead. The complex structure and quadratic complexity during attention calculation in vanilla transformer seriously hinder its scalability on the large-scale graph data. Though existing methods have made strides in simplifying combinations among blocks or attention-learning paradigm to improve GTs' efficiency, a series of energy-saving solutions originated from biologically plausible structures are rarely taken into consideration when constructing GT framework. To this end, we propose a new spiking-based graph transformer (SGHormer). It turns full-precision embeddings into sparse and binarized spikes to reduce memory and computational costs. The spiking graph self-attention and spiking rectify blocks in SGHorm
    
[^43]: 不确定性感知的分布式离线强化学习

    Uncertainty-aware Distributional Offline Reinforcement Learning

    [https://arxiv.org/abs/2403.17646](https://arxiv.org/abs/2403.17646)

    提出了一种不确定性感知的分布式离线强化学习方法，同时解决认知不确定性和环境随机性，在风险敏感和规避设置下进行了全面实验评估

    

    离线强化学习面临独特挑战，因其仅依赖于观测数据。在这一背景下中心关注点是通过量化与各种行动和环境随机性相关的不确定性，确保所学策略的安全性。传统方法主要强调通过学习风险规避策略来缓解认知不确定性，往往忽视环境随机性。在本研究中，我们提出了一种不确定性感知的分布式离线强化学习方法，以同时处理认知不确定性和环境随机性。我们提出了一种能够学习风险规避策略并表征折现累积奖励的整个分布的无模型离线强化学习算法，而不仅仅是最大化累积折现回报的期望值。我们的方法通过在风险敏感和风险规避设置下的全面实验得到严格评估。

    arXiv:2403.17646v1 Announce Type: new  Abstract: Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and ri
    
[^44]: PeersimGym：用于通过强化学习解决任务卸载问题的环境

    PeersimGym: An Environment for Solving the Task Offloading Problem with Reinforcement Learning

    [https://arxiv.org/abs/2403.17637](https://arxiv.org/abs/2403.17637)

    引入了 PeersimGym 环境，通过强化学习解决任务卸载问题，支持定制化仿真环境，有助于开发和优化计算网络中的任务卸载策略。

    

    任务卸载对于在诸如物联网之类的网络中平衡设备的计算负载至关重要，但面临着诸如在严格的通信和存储约束下最小化延迟和能源使用等重要优化挑战。传统优化在可扩展性方面存在不足；启发式方法缺乏实现最佳结果，而强化学习（RL）通过允许通过迭代交互学习最佳卸载策略的方式提供了一种有前景的途径。然而，RL 的功效取决于对丰富数据集和定制的现实训练环境的访问。为解决这一问题，我们引入了 PeersimGym，这是一个开源的、可定制的仿真环境，旨在开发和优化计算网络中的任务卸载策略。PeersimGym 支持各种网络拓扑和计算约束，并整合了一种"PettingZo"方法，使用户能够轻松配置仿真参数和监控仿真过程。

    arXiv:2403.17637v1 Announce Type: cross  Abstract: Task offloading, crucial for balancing computational loads across devices in networks such as the Internet of Things, poses significant optimization challenges, including minimizing latency and energy usage under strict communication and storage constraints. While traditional optimization falls short in scalability; and heuristic approaches lack in achieving optimal outcomes, Reinforcement Learning (RL) offers a promising avenue by enabling the learning of optimal offloading strategies through iterative interactions. However, the efficacy of RL hinges on access to rich datasets and custom-tailored, realistic training environments. To address this, we introduce PeersimGym, an open-source, customizable simulation environment tailored for developing and optimizing task offloading strategies within computational networks. PeersimGym supports a wide range of network topologies and computational constraints and integrates a \textit{PettingZo
    
[^45]: 具有自适应遮罩的保留决策变压器用于基于强化学习的推荐系统

    Retentive Decision Transformer with Adaptive Masking for Reinforcement Learning based Recommendation Systems

    [https://arxiv.org/abs/2403.17634](https://arxiv.org/abs/2403.17634)

    本研究提出了一种新的离线RL推荐系统方法，通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。

    

    强化学习推荐系统（RLRS）在一系列应用中显示出潜力，从电子商务平台到流媒体服务。然而，它们在制定奖励函数和利用RL框架中的大型现有数据集方面面临挑战。最近的离线RLRS的技术进步为解决这两个挑战提供了解决方案。然而，现有方法主要依赖于转换器架构，在序列长度增加时可能会引入与计算资源和训练成本相关的挑战。此外，主流方法使用固定长度的输入轨迹，限制了它们捕获不断变化的用户喜好的能力。在本研究中，我们介绍了一种新的离线RLRS方法来解决以上问题。我们通过将顺序决策建模为推理任务，利用自适应遮罩配置来重新解释RLRS挑战。

    arXiv:2403.17634v1 Announce Type: cross  Abstract: Reinforcement Learning-based Recommender Systems (RLRS) have shown promise across a spectrum of applications, from e-commerce platforms to streaming services. Yet, they grapple with challenges, notably in crafting reward functions and harnessing large pre-existing datasets within the RL framework. Recent advancements in offline RLRS provide a solution for how to address these two challenges. However, existing methods mainly rely on the transformer architecture, which, as sequence lengths increase, can introduce challenges associated with computational resources and training costs. Additionally, the prevalent methods employ fixed-length input trajectories, restricting their capacity to capture evolving user preferences. In this study, we introduce a new offline RLRS method to deal with the above problems. We reinterpret the RLRS challenge by modeling sequential decision-making as an inference task, leveraging adaptive masking configurat
    
[^46]: 使用开放数据集对电动微移动能耗建模

    Data-driven Energy Consumption Modelling for Electric Micromobility using an Open Dataset

    [https://arxiv.org/abs/2403.17632](https://arxiv.org/abs/2403.17632)

    提出了一个专门针对电动微移动工具在都柏林收集的开放数据集，为解决实际场景中能耗建模的困难提供了重要资源

    

    车辆拥堵和环境恶化带来的挑战日益加剧，凸显了在城市空间推行E-Mobility解决方案的重要性。特别是，E-滑板车和E-自行车等微型E-Mobility工具在这一转变中发挥着关键作用，为城市通勤者提供可持续的替代方案。然而，这些工具的能耗模式是影响其在现实场景中有效性的关键因素，对于出行规划以及增强用户在使用这些工具时的信心至关重要。为此，最近的研究利用针对特定移动工具和条件定制的物理模型，但这些模型在现实场景中的泛化能力和有效性存在困难，这是因为缺乏用于彻底模型评估和验证的开放数据集。为填补这一空白，我们的工作提出了一个在爱尔兰都柏林收集的开放数据集，专门用于能耗建模。

    arXiv:2403.17632v1 Announce Type: new  Abstract: The escalating challenges of traffic congestion and environmental degradation underscore the critical importance of embracing E-Mobility solutions in urban spaces. In particular, micro E-Mobility tools such as E-scooters and E-bikes, play a pivotal role in this transition, offering sustainable alternatives for urban commuters. However, the energy consumption patterns for these tools are a critical aspect that impacts their effectiveness in real-world scenarios and is essential for trip planning and boosting user confidence in using these. To this effect, recent studies have utilised physical models customised for specific mobility tools and conditions, but these models struggle with generalization and effectiveness in real-world scenarios due to a notable absence of open datasets for thorough model evaluation and verification. To fill this gap, our work presents an open dataset, collected in Dublin, Ireland, specifically designed for ene
    
[^47]: 伪造还是JPEG？揭示生成图像检测数据集中的常见偏见

    Fake or JPEG? Revealing Common Biases in Generated Image Detection Datasets

    [https://arxiv.org/abs/2403.17608](https://arxiv.org/abs/2403.17608)

    许多AI生成图像检测数据集存在与JPEG压缩和图像大小相关的偏见，去除这些偏见可以显著提高对JPEG压缩的稳健性并显著改变检测器的跨生成器性能。

    

    生成图像模型的广泛应用凸显了检测人造内容的迫切需求，这是打击广泛操纵和误导的关键一步。因此，许多检测器和相关数据集已经出现。然而，许多这些数据集不经意地引入了不良偏见，从而影响了检测器的效果和评估。本文强调了许多用于AI生成图像检测的数据集包含与JPEG压缩和图像大小有关的偏见。使用GenImage数据集，我们证明检测器确实从这些不受欢迎的因素中学习。此外，我们展示去除这些命名偏见会显著增加针对JPEG压缩的鲁棒性，并显著改变评估检测器的跨生成器性能。具体来说，对于ResNet50和S

    arXiv:2403.17608v1 Announce Type: cross  Abstract: The widespread adoption of generative image models has highlighted the urgent need to detect artificial content, which is a crucial step in combating widespread manipulation and misinformation. Consequently, numerous detectors and associated datasets have emerged. However, many of these datasets inadvertently introduce undesirable biases, thereby impacting the effectiveness and evaluation of detectors. In this paper, we emphasize that many datasets for AI-generated image detection contain biases related to JPEG compression and image size. Using the GenImage dataset, we demonstrate that detectors indeed learn from these undesired factors. Furthermore, we show that removing the named biases substantially increases robustness to JPEG compression and significantly alters the cross-generator performance of evaluated detectors. Specifically, it leads to more than 11 percentage points increase in cross-generator performance for ResNet50 and S
    
[^48]: LASIL：学习者感知的长期微观交通仿真监督模仿学习

    LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation

    [https://arxiv.org/abs/2403.17601](https://arxiv.org/abs/2403.17601)

    提出了一种学习者感知的监督模仿学习方法，通过变分自动编码器增强专家状态，以解决多智体模仿学习中的协变量偏移问题

    

    微观交通仿真在交通工程中起着至关重要的作用，通过提供关于单个车辆行为和整体交通流的见解。然而，创建一个真实的模拟器，精确复制各种交通条件下的人类驾驶行为，面临着重大挑战。传统的依赖启发式模型的模拟器往往由于现实世界交通环境的复杂性而无法提供准确的模拟。由于协变量偏移问题，现有的基于模仿学习的模拟器经常无法生成稳定的长期模拟。在本文中，我们提出了一种称为学习者感知的监督模仿学习的新方法，以解决多智体模仿学习中的协变量偏移问题。通过利用变分自动编码器同时建模专家和学习者状态分布，我们的方法增强了专家状态，从而使增强状态意识到

    arXiv:2403.17601v1 Announce Type: new  Abstract: Microscopic traffic simulation plays a crucial role in transportation engineering by providing insights into individual vehicle behavior and overall traffic flow. However, creating a realistic simulator that accurately replicates human driving behaviors in various traffic conditions presents significant challenges. Traditional simulators relying on heuristic models often fail to deliver accurate simulations due to the complexity of real-world traffic environments. Due to the covariate shift issue, existing imitation learning-based simulators often fail to generate stable long-term simulations. In this paper, we propose a novel approach called learner-aware supervised imitation learning to address the covariate shift problem in multi-agent imitation learning. By leveraging a variational autoencoder simultaneously modeling the expert and learner state distribution, our approach augments expert states such that the augmented state is aware 
    
[^49]: 对超参数化对于超出分布泛化的益处

    On the Benefits of Over-parameterization for Out-of-Distribution Generalization

    [https://arxiv.org/abs/2403.17592](https://arxiv.org/abs/2403.17592)

    研究了超参数化模型在超出分布泛化方面的表现，探讨了在良性过拟合条件下的表现，并发现了恒定的超出分布损失。

    

    在最近几年，基于独立同分布假设的机器学习模型取得了成功。然而，这一假设在现实世界的应用中很容易被违反，导致了超出分布（OOD）问题。理解现代超参数化深度神经网络在非平凡自然分布偏移下的行为是至关重要的，因为目前对其在理论上的理解是不足的。现有的理论工作常常为OOD场景中的超参数化模型提供无意义的结果，甚至与实证结果相矛盾。为此，我们正在研究在一般良性过拟合条件下，超参数化模型在OOD泛化方面的性能。我们的分析集中在随机特征模型上，并研究非平凡自然分布偏移，其中良性过拟合估计器展示出恒定的过大OOD损失，尽管达到了零过大i

    arXiv:2403.17592v1 Announce Type: new  Abstract: In recent years, machine learning models have achieved success based on the independently and identically distributed assumption. However, this assumption can be easily violated in real-world applications, leading to the Out-of-Distribution (OOD) problem. Understanding how modern over-parameterized DNNs behave under non-trivial natural distributional shifts is essential, as current theoretical understanding is insufficient. Existing theoretical works often provide meaningless results for over-parameterized models in OOD scenarios or even contradict empirical findings. To this end, we are investigating the performance of the over-parameterized model in terms of OOD generalization under the general benign overfitting conditions. Our analysis focuses on a random feature model and examines non-trivial natural distributional shifts, where the benign overfitting estimators demonstrate a constant excess OOD loss, despite achieving zero excess i
    
[^50]: 双存储网络：一种用于视觉与语言模型的多功能适应方法

    Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models

    [https://arxiv.org/abs/2403.17589](https://arxiv.org/abs/2403.17589)

    提出了双存储网络的多功能适应方法，能在零次适应、少次适应和无需训练的少次适应三种设置下高效运行

    

    随着像CLIP这样的预训练视觉与语言模型的出现，如何将它们调整到各种下游分类任务已经引起了最近研究的重视。该适应策略通常可以归类为三种范式：零次适应、少次适应和最近提出的无需训练的少次适应。大多数现有方法都是针对特定设置量身定制的，只能满足其中一种或两种范式。本文介绍了一种多功能适应方法，能够有效地在这三种设置下运行。具体地，我们提出了双存储网络，包括动态和静态记忆组件。静态记忆缓存训练数据知识，实现了无需训练的少次适应，而动态记忆在测试过程中在线保存历史测试特征，允许探索超出论文中已训练数据的额外数据洞察

    arXiv:2403.17589v1 Announce Type: cross  Abstract: With the emergence of pre-trained vision-language models like CLIP, how to adapt them to various downstream classification tasks has garnered significant attention in recent research. The adaptation strategies can be typically categorized into three paradigms: zero-shot adaptation, few-shot adaptation, and the recently-proposed training-free few-shot adaptation. Most existing approaches are tailored for a specific setting and can only cater to one or two of these paradigms. In this paper, we introduce a versatile adaptation approach that can effectively work under all three settings. Specifically, we propose the dual memory networks that comprise dynamic and static memory components. The static memory caches training data knowledge, enabling training-free few-shot adaptation, while the dynamic memory preserves historical test features online during the testing process, allowing for the exploration of additional data insights beyond the
    
[^51]: Forest-ORE: 挖掘最佳规则集以解释随机森林模型

    Forest-ORE: Mining Optimal Rule Ensemble to interpret Random Forest models

    [https://arxiv.org/abs/2403.17588](https://arxiv.org/abs/2403.17588)

    Forest-ORE方法提出了一种通过优化规则集使随机森林具有可解释性的方法，可以应用于全局和局部解释，同时考虑了影响可解释规则集选择的多个参数。

    

    arXiv:2403.17588v1宣布类型：新  摘要：随机森林（RF）以其高预测性能而闻名，是一种有效的集成学习方法。 由于有数百个深层决策树，它也被认为是一个黑盒。 这种缺乏解释性可以成为RF模型在几种现实世界应用中被接受的一个真正缺点，特别是那些影响个人生活的领域，如医疗保健、安全和法律。 在这项工作中，我们提出Forest-ORE，一种通过优化的规则集（ORE）使RF具有可解释性，用于局部和全局解释。 不同于其他旨在解释RF模型的基于规则的方法，该方法同时考虑了几个影响选择可解释规则集的参数。 现有方法通常将预测性能置于可解释性覆盖范围之上，并且不提供有关现有规则之间的重叠或交互作用的信息。 Forest-ORE使用混合整数优化prog

    arXiv:2403.17588v1 Announce Type: new  Abstract: Random Forest (RF) is well-known as an efficient ensemble learning method in terms of predictive performance. It is also considered a Black Box because of its hundreds of deep decision trees. This lack of interpretability can be a real drawback for acceptance of RF models in several real-world applications, especially those affecting one's lives, such as in healthcare, security, and law. In this work, we present Forest-ORE, a method that makes RF interpretable via an optimized rule ensemble (ORE) for local and global interpretation. Unlike other rule-based approaches aiming at interpreting the RF model, this method simultaneously considers several parameters that influence the choice of an interpretable rule ensemble. Existing methods often prioritize predictive performance over interpretability coverage and do not provide information about existing overlaps or interactions between rules. Forest-ORE uses a mixed-integer optimization prog
    
[^52]: 朝着零数据、可控、自适应对话系统迈进

    Towards a Zero-Data, Controllable, Adaptive Dialog System

    [https://arxiv.org/abs/2403.17582](https://arxiv.org/abs/2403.17582)

    该论文提出了一种从对话树生成数据的方法，可帮助训练出在合成数据上训练的代理达到与在人类数据上训练的模型相媲美的对话成功率。

    

    对话树搜索（Väth等，2023年）是一种最近的对话系统控制方法，其中领域专家通过对话树塑造强化学习代理的行为。代理学会有效地浏览这棵树，同时适应不同用户的信息需求，例如领域熟悉度。然而，额外的训练数据需求阻碍了在新领域的部署。为了解决这个问题，我们探索了直接从对话树生成这些数据的方法。我们改进了原始方法，并展示了在合成数据上训练的代理可以实现与在人类数据上训练的模型相当的对话成功率，无论是使用商业大语言模型进行生成，还是使用较小的开源模型，在单个GPU上运行。我们进一步通过收集和测试两个新数据集来展示我们方法的可扩展性：ONBOARD，一个帮助外国居民搬迁的新领域。

    arXiv:2403.17582v1 Announce Type: cross  Abstract: Conversational Tree Search (V\"ath et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving t
    
[^53]: 通过本地训练增强联邦学习的隐私性

    Enhancing Privacy in Federated Learning through Local Training

    [https://arxiv.org/abs/2403.17572](https://arxiv.org/abs/2403.17572)

    提出了一种用于联邦学习的联邦私有本地训练算法（Fed-PLT），通过允许部分参与和本地训练，显著减少了通信轮次，同时不影响准确性，并研究了如何通过本地训练来增强隐私性。

    

    在本文中，我们提出了一种用于联邦学习的联邦私有本地训练算法（Fed-PLT），以克服（i）昂贵的通信和（ii）隐私保护的挑战。我们通过允许部分参与和本地训练来解决（i），这显著减少了中央协调员和计算代理之间的通信轮次。算法在本地训练的使用上达到了目前技术水平，可以证明不会影响准确性。此外，代理可以灵活选择各种本地训练求解器，如（随机）梯度下降和加速梯度下降。此外，我们研究了如何通过使用本地训练来增强隐私性，解决了点（ii）。具体而言，我们推导出差分隐私界限，并强调它们对本地训练纪元数的依赖性。我们评估了所提出算法的有效性。

    arXiv:2403.17572v1 Announce Type: new  Abstract: In this paper we propose the federated private local training algorithm (Fed-PLT) for federated learning, to overcome the challenges of (i) expensive communications and (ii) privacy preservation. We address (i) by allowing for both partial participation and local training, which significantly reduce the number of communication rounds between the central coordinator and computing agents. The algorithm matches the state of the art in the sense that the use of local training demonstrably does not impact accuracy. Additionally, agents have the flexibility to choose from various local training solvers, such as (stochastic) gradient descent and accelerated gradient descent. Further, we investigate how employing local training can enhance privacy, addressing point (ii). In particular, we derive differential privacy bounds and highlight their dependence on the number of local training epochs. We assess the effectiveness of the proposed algorithm
    
[^54]: 深度学习及其最新应用综述

    A Survey on Deep Learning and State-of-the-arts Applications

    [https://arxiv.org/abs/2403.17561](https://arxiv.org/abs/2403.17561)

    深度学习是解决复杂问题的强大工具，本研究旨在全面审视深度学习模型及其应用的最新发展

    

    深度学习, 是人工智能的一个分支，是一种利用多层互连单元（神经元）从原始输入数据中直接学习复杂模式和表示的计算模型。受到这种学习能力的赋能，深度学习已成为解决复杂问题的强大工具，是许多突破性技术和创新的核心驱动力。构建深度学习模型是一项具有挑战性的任务，因为算法的复杂性和现实问题的动态性。有几项研究回顾了深度学习的概念和应用。然而，这些研究大多集中于深度学习模型类型和卷积神经网络架构，对深度学习模型及其在不同领域解决复杂问题的最新发展的覆盖面有限。因此，受到这些限制的启发，本研究旨在全面审视th

    arXiv:2403.17561v1 Announce Type: new  Abstract: Deep learning, a branch of artificial intelligence, is a computational model that uses multiple layers of interconnected units (neurons) to learn intricate patterns and representations directly from raw input data. Empowered by this learning capability, it has become a powerful tool for solving complex problems and is the core driver of many groundbreaking technologies and innovations. Building a deep learning model is a challenging task due to the algorithm`s complexity and the dynamic nature of real-world problems. Several studies have reviewed deep learning concepts and applications. However, the studies mostly focused on the types of deep learning models and convolutional neural network architectures, offering limited coverage of the state-of-the-art of deep learning models and their applications in solving complex problems across different domains. Therefore, motivated by the limitations, this study aims to comprehensively review th
    
[^55]: DeepMIF: 用于大规模LiDAR 3D地图绘制的深度单调隐式场

    DeepMIF: Deep Monotonic Implicit Fields for Large-Scale LiDAR 3D Mapping

    [https://arxiv.org/abs/2403.17550](https://arxiv.org/abs/2403.17550)

    提出了DeepMIF，通过设计学习系统集成单调性损失，在大规模3D地图绘制中优化神经单调场，避免了LiDAR测量的嘈杂问题

    

    近年来，通过使用现代获取设备如LiDAR传感器，在感知真实大规模室外3D环境方面取得了显著进展。然而，它们在生成稠密、完整的3D场景方面存在固有限制。为解决这一问题，最近的基于学习的方法集成了神经隐式表示和可优化特征网格，以逼近3D场景的表面。然而，简单地沿原始LiDAR光线拟合样本会导致由于稀疏、互相矛盾的LiDAR测量的特性而产生嘈杂的3D绘图结果。相反，在这项工作中，我们不再精确拟合LiDAR数据，而是让网络优化在3D空间中定义的非度量单调隐式场。为适应我们的场，我们设计了一个学习系统，集成了一个单调性损失，使得能够优化神经单调场并利用了大规模3D地图绘制的最新进展。我们的算法...

    arXiv:2403.17550v1 Announce Type: cross  Abstract: Recently, significant progress has been achieved in sensing real large-scale outdoor 3D environments, particularly by using modern acquisition equipment such as LiDAR sensors. Unfortunately, they are fundamentally limited in their ability to produce dense, complete 3D scenes. To address this issue, recent learning-based methods integrate neural implicit representations and optimizable feature grids to approximate surfaces of 3D scenes. However, naively fitting samples along raw LiDAR rays leads to noisy 3D mapping results due to the nature of sparse, conflicting LiDAR measurements. Instead, in this work we depart from fitting LiDAR data exactly, instead letting the network optimize a non-metric monotonic implicit field defined in 3D space. To fit our field, we design a learning system integrating a monotonicity loss that enables optimizing neural monotonic fields and leverages recent progress in large-scale 3D mapping. Our algorithm ac
    
[^56]: VDSC：利用价值差异和状态计数增强探索时间

    VDSC: Enhancing Exploration Timing with Value Discrepancy and State Counts

    [https://arxiv.org/abs/2403.17542](https://arxiv.org/abs/2403.17542)

    通过价值差异和状态计数，利用代理的内部状态来决定何时进行探索，解决了盲目切换机制的缺点。

    

    尽管深度强化学习中对于“探索多少”和“如何探索”问题受到了相当大的关注，但对于“何时”探索的研究相对较少。在更复杂的探索策略可以在特定的、通常稀疏的奖励环境中表现出色的同时，现有的简单方法，如$\epsilon$-贪心，在更广泛的领域中继续表现优异。这些简单策略的吸引力在于它们的易实现性和对各种领域的普遍适用性。然而，这些方法的缺点在于它们本质上是一种盲目的切换机制，完全忽略了代理的内部状态。本文提出利用代理的内部状态来决定“何时”进行探索，从而解决盲目切换机制的缺点。我们通过稳态（VDSC）提出了价值差异和状态计数。

    arXiv:2403.17542v1 Announce Type: cross  Abstract: Despite the considerable attention given to the questions of \textit{how much} and \textit{how to} explore in deep reinforcement learning, the investigation into \textit{when} to explore remains relatively less researched. While more sophisticated exploration strategies can excel in specific, often sparse reward environments, existing simpler approaches, such as $\epsilon$-greedy, persist in outperforming them across a broader spectrum of domains. The appeal of these simpler strategies lies in their ease of implementation and generality across a wide range of domains. The downside is that these methods are essentially a blind switching mechanism, which completely disregards the agent's internal state. In this paper, we propose to leverage the agent's internal state to decide \textit{when} to explore, addressing the shortcomings of blind switching mechanisms. We present Value Discrepancy and State Counts through homeostasis (VDSC), a no
    
[^57]: BVR体育馆：一种超视距空战的强化学习环境

    BVR Gym: A Reinforcement Learning Environment for Beyond-Visual-Range Air Combat

    [https://arxiv.org/abs/2403.17533](https://arxiv.org/abs/2403.17533)

    研究者创建了一个强化学习环境BVR体育馆，用于探究超视距空中战斗领域的潜在战术，提供了基于开源飞行动力学模拟器JSBSim的高保真度环境。

    

    创建新的空战战术和发现新的机动可能需要大量专家飞行员的时间。此外，对于每种不同的作战场景，相同的策略可能不适用，因为设备性能的微小变化可能会极大地改变空中战斗结果。出于这个原因，我们创建了一个强化学习环境，以帮助调查超视距（BVR）空中战斗领域潜在的空战战术：BVR体育馆。这种空中战斗是重要的，因为远程导弹通常是空中战斗中首先被使用的武器。一些现有环境提供了高保真度的模拟，但要么不是开源的，要么没有适应于BVR空战领域。其他环境是开源的，但使用不太准确的模拟模型。我们的工作提供了一个基于开源飞行动力学模拟器JSBSim的高保真度环境，并适应于BVR空战领域。

    arXiv:2403.17533v1 Announce Type: new  Abstract: Creating new air combat tactics and discovering novel maneuvers can require numerous hours of expert pilots' time. Additionally, for each different combat scenario, the same strategies may not work since small changes in equipment performance may drastically change the air combat outcome. For this reason, we created a reinforcement learning environment to help investigate potential air combat tactics in the field of beyond-visual-range (BVR) air combat: the BVR Gym. This type of air combat is important since long-range missiles are often the first weapon to be used in aerial combat. Some existing environments provide high-fidelity simulations but are either not open source or are not adapted to the BVR air combat domain. Other environments are open source but use less accurate simulation models. Our work provides a high-fidelity environment based on the open-source flight dynamics simulator JSBSim and is adapted to the BVR air combat dom
    
[^58]: 通过Fisher-Rao范数正则化提升对抗训练

    Boosting Adversarial Training via Fisher-Rao Norm-based Regularization

    [https://arxiv.org/abs/2403.17520](https://arxiv.org/abs/2403.17520)

    通过Fisher-Rao范数正则化，本研究在对抗训练中提出了一种解决标准泛化性能下降问题的方法，并通过模型复杂性角度对此进行了理论和实证分析。

    

    对抗训练被广泛应用于提高深度神经网络的对抗鲁棒性。然而，在对抗训练模型中减轻标准泛化性能的下降仍然是一个悬而未决的问题。本文试图通过模型复杂性的视角解决这个问题。首先，我们利用Fisher-Rao范数，一个在模型复杂性方面的几何不变度量，建立了基于ReLU激活的多层感知器的Cross-Entropy Loss-based Rademacher复杂度的非平凡界限。然后我们推广了一个与模型宽度变化和对抗训练的权衡因素敏感相关的复杂性相关变量。此外，大量实证证据表明，此变量与对抗训练和标准训练模型之间的Cross-Entropy loss的泛化差距高度相关，特别是在训练的初始和最终阶段。

    arXiv:2403.17520v1 Announce Type: new  Abstract: Adversarial training is extensively utilized to improve the adversarial robustness of deep neural networks. Yet, mitigating the degradation of standard generalization performance in adversarial-trained models remains an open problem. This paper attempts to resolve this issue through the lens of model complexity. First, We leverage the Fisher-Rao norm, a geometrically invariant metric for model complexity, to establish the non-trivial bounds of the Cross-Entropy Loss-based Rademacher complexity for a ReLU-activated Multi-Layer Perceptron. Then we generalize a complexity-related variable, which is sensitive to the changes in model width and the trade-off factors in adversarial training. Moreover, intensive empirical evidence validates that this variable highly correlates with the generalization gap of Cross-Entropy loss between adversarial-trained and standard-trained models, especially during the initial and final phases of the training p
    
[^59]: 训练和推理期间的预测分享

    Prediction-sharing During Training and Inference

    [https://arxiv.org/abs/2403.17515](https://arxiv.org/abs/2403.17515)

    研究了在训练和推理期间的预测分享方面的新颖之处，并介绍和突出了不同类型的数据共享合同。

    

    两家公司参与竞争性预测任务。每家公司有两个数据来源 -- 有标签的历史数据和无标签的推理时间数据 -- 并且使用前者制定预测模型，使用后者对新实例进行预测。我们研究了公司之间的数据共享合同。我们研究的新颖之处在于介绍和突出仅分享预测模型的合同、仅分享推理时间预测的合同以及分享两者的合同之间的区别。我们的分析分为三个层面。首先，我们开发了一个便于进行研究的一般贝叶斯框架。其次，我们将焦点缩小到这个框架内的两个自然设置：(i) 每家公司的预测模型准确度是共知的，但各自模型之间的相关性未知；(ii) 存在两个关于最优预测器的假设，其中一个是

    arXiv:2403.17515v1 Announce Type: cross  Abstract: Two firms are engaged in a competitive prediction task. Each firm has two sources of data -- labeled historical data and unlabeled inference-time data -- and uses the former to derive a prediction model, and the latter to make predictions on new instances. We study data-sharing contracts between the firms. The novelty of our study is to introduce and highlight the differences between contracts that share prediction models only, contracts to share inference-time predictions only, and contracts to share both. Our analysis proceeds on three levels. First, we develop a general Bayesian framework that facilitates our study. Second, we narrow our focus to two natural settings within this framework: (i) a setting in which the accuracy of each firm's prediction model is common knowledge, but the correlation between the respective models is unknown; and (ii) a setting in which two hypotheses exist regarding the optimal predictor, and one of the
    
[^60]: EL-MLFFs：机器学习力场的集成学习

    EL-MLFFs: Ensemble Learning of Machine Leaning Force Fields

    [https://arxiv.org/abs/2403.17507](https://arxiv.org/abs/2403.17507)

    提出了一种新颖的集成学习框架EL-MLFFs，利用堆叠方法整合来自不同MLFFs的预测，从而提高力预测准确性。

    

    机器学习力场（MLFFs）已经成为一种有希望的方法，可以弥合量子力学方法的精确性和经典力场的效率。然而，MLFF模型的丰富性和准确预测原子力的挑战给它们的实际应用带来了重大障碍。在本文中，我们提出了一种新颖的集成学习框架EL-MLFFs，利用堆叠方法整合来自不同MLFFs的预测，增强力预测准确性。通过构建分子结构的图表示并采用图神经网络（GNN）作为元模型，EL-MLFFs有效地捕捉原子间相互作用并改进力的预测。我们在两个不同的数据集上评估了我们的方法：甲烷分子和吸附在Cu（100）表面上的甲醇。结果表明，EL-MLFFs相对于单个MLFF显著提高了力预测准确性。

    arXiv:2403.17507v1 Announce Type: new  Abstract: Machine learning force fields (MLFFs) have emerged as a promising approach to bridge the accuracy of quantum mechanical methods and the efficiency of classical force fields. However, the abundance of MLFF models and the challenge of accurately predicting atomic forces pose significant obstacles in their practical application. In this paper, we propose a novel ensemble learning framework, EL-MLFFs, which leverages the stacking method to integrate predictions from diverse MLFFs and enhance force prediction accuracy. By constructing a graph representation of molecular structures and employing a graph neural network (GNN) as the meta-model, EL-MLFFs effectively captures atomic interactions and refines force predictions. We evaluate our approach on two distinct datasets: methane molecules and methanol adsorbed on a Cu(100) surface. The results demonstrate that EL-MLFFs significantly improves force prediction accuracy compared to individual ML
    
[^61]: DS-AL：一种面向无样本类增量学习的双流分析学习

    DS-AL: A Dual-Stream Analytic Learning for Exemplar-Free Class-Incremental Learning

    [https://arxiv.org/abs/2403.17503](https://arxiv.org/abs/2403.17503)

    DS-AL提出了一种双流分析学习方法，通过主流和补偿流相结合，重新定义CIL问题，有效解决了无样本约束条件下的类增量学习问题。

    

    在无样本约束条件下的类增量学习(CIL)提出了重大挑战。现有的遵循此约束条件的方法往往比保留对过去样本访问权的回放技术更容易出现灾难性遗忘。为了解决无样本CIL问题，本文提出了一种双流分析学习(DS-AL)方法。DS-AL包含一个主流提供分析（即闭式）线性解决方案，以及一个改善由于采用线性映射而固有的欠拟合限制的补偿流。主流将CIL问题重新定义为一个连接递归最小二乘(C-RLS)任务，从而实现了CIL及其联合学习对应任务之间的等价性。补偿流由双激活补偿(DAC)模块控制。该模块使用不同于主流的激活函数重新激活嵌入，并寻找...

    arXiv:2403.17503v1 Announce Type: new  Abstract: Class-incremental learning (CIL) under an exemplar-free constraint has presented a significant challenge. Existing methods adhering to this constraint are prone to catastrophic forgetting, far more so than replay-based techniques that retain access to past samples. In this paper, to solve the exemplar-free CIL problem, we propose a Dual-Stream Analytic Learning (DS-AL) approach. The DS-AL contains a main stream offering an analytical (i.e., closed-form) linear solution, and a compensation stream improving the inherent under-fitting limitation due to adopting linear mapping. The main stream redefines the CIL problem into a Concatenated Recursive Least Squares (C-RLS) task, allowing an equivalence between the CIL and its joint-learning counterpart. The compensation stream is governed by a Dual-Activation Compensation (DAC) module. This module re-activates the embedding with a different activation function from the main stream one, and seek
    
[^62]: 基于变分图自动编码器的归纳学习方法用于半监督分类

    Variational Graph Auto-Encoder Based Inductive Learning Method for Semi-Supervised Classification

    [https://arxiv.org/abs/2403.17500](https://arxiv.org/abs/2403.17500)

    VGAE框架在归纳学习领域的应用具有重要意义。

    

    图表示学习是各个应用领域中的一个基本研究问题，其中归纳学习问题尤其具有挑战性，因为它要求模型在推断过程中概括到未见过的图结构。近年来，图神经网络（GNNs）已经成为强大的图模型，用于归纳学习任务，如节点分类，但它们通常在完全监督的训练设置下严重依赖于标记节点。与基于GNN的方法相比，变分图自动编码器（VGAEs）被认为更具一般性，能够捕捉图的内部结构信息，独立于节点标签，并在多个无监督学习任务上取得了卓越的性能。然而，迄今为止，仍然缺乏关于利用VGAE框架进行归纳学习的研究，这是由于在监督方式下训练模型的困难。

    arXiv:2403.17500v1 Announce Type: new  Abstract: Graph representation learning is a fundamental research issue in various domains of applications, of which the inductive learning problem is particularly challenging as it requires models to generalize to unseen graph structures during inference. In recent years, graph neural networks (GNNs) have emerged as powerful graph models for inductive learning tasks such as node classification, whereas they typically heavily rely on the annotated nodes under a fully supervised training setting. Compared with the GNN-based methods, variational graph auto-encoders (VGAEs) are known to be more generalizable to capture the internal structural information of graphs independent of node labels and have achieved prominent performance on multiple unsupervised learning tasks. However, so far there is still a lack of work focusing on leveraging the VGAE framework for inductive learning, due to the difficulties in training the model in a supervised manner an
    
[^63]: 具有内存和切换成本的在线非凸优化问题的容量调配动机

    Capacity Provisioning Motivated Online Non-Convex Optimization Problem with Memory and Switching Cost

    [https://arxiv.org/abs/2403.17480](https://arxiv.org/abs/2403.17480)

    该论文考虑了一种在线非凸优化问题，目标是通过调节活动服务器数量最小化作业延迟，引入了切换成本，提出了竞争算法。

    

    考虑了一种在线非凸优化问题，其目标是通过调节活动服务器的数量来最小化一组作业的流量时间（总延迟），但在时间变化时改变活动服务器数量会产生切换成本。每个作业在任何时间内最多可以由一个固定速度的服务器处理。与通常具有切换成本的在线凸优化（OCO）问题相比，所考虑的目标函数是非凸的，并且更重要的是，在每个时间点，它取决于所有过去的决策，而不仅仅是当前的决策。考虑了最坏情况和随机输入；对于这两种情况，提出了竞争算法。

    arXiv:2403.17480v1 Announce Type: cross  Abstract: An online non-convex optimization problem is considered where the goal is to minimize the flow time (total delay) of a set of jobs by modulating the number of active servers, but with a switching cost associated with changing the number of active servers over time. Each job can be processed by at most one fixed speed server at any time. Compared to the usual online convex optimization (OCO) problem with switching cost, the objective function considered is non-convex and more importantly, at each time, it depends on all past decisions and not just the present one. Both worst-case and stochastic inputs are considered; for both cases, competitive algorithms are derived.
    
[^64]: 基于需求异味的自然语言需求可测试性度量

    Natural Language Requirements Testability Measurement Based on Requirement Smells

    [https://arxiv.org/abs/2403.17479](https://arxiv.org/abs/2403.17479)

    通过提出一个基于需求异味的数学模型，本文介绍了一种评估和排名自然语言需求可测试性的方法，有助于衡量和量化需求的可测试性。

    

    需求构成了定义软件系统义务和任务的基础。可测试的需求有助于防止失败，降低维护成本，并简化验收测试。然而，尽管衡量和量化需求可测试性的重要性，但尚未提出基于需求异味的自动化方法来衡量需求的可测试性。本文提出了一个数学模型，以评估和排名自然语言需求的可测试性，基于一个广泛的九个需求异味集合，自动检测，并根据需求长度和其应用领域来确定验收测试工作的努力。大多数异味源于不可数的形容词，上下文敏感和模糊词。需要一个全面的字典来检测这些词。我们提供了一种神经词嵌入技术来生成这样一个字典。

    arXiv:2403.17479v1 Announce Type: cross  Abstract: Requirements form the basis for defining software systems' obligations and tasks. Testable requirements help prevent failures, reduce maintenance costs, and make it easier to perform acceptance tests. However, despite the importance of measuring and quantifying requirements testability, no automatic approach for measuring requirements testability has been proposed based on the requirements smells, which are at odds with the requirements testability. This paper presents a mathematical model to evaluate and rank the natural language requirements testability based on an extensive set of nine requirements smells, detected automatically, and acceptance test efforts determined by requirement length and its application domain. Most of the smells stem from uncountable adjectives, context-sensitive, and ambiguous words. A comprehensive dictionary is required to detect such words. We offer a neural word-embedding technique to generate such a dic
    
[^65]: 一个统一的神经网络学习内核

    A Unified Kernel for Neural Network Learning

    [https://arxiv.org/abs/2403.17467](https://arxiv.org/abs/2403.17467)

    本文提出了统一神经内核(UNK)，可以描述神经网络的学习动态，并在有限的学习步骤下表现出类似于NTK的行为，当学习步骤逼近无穷大时收敛到NNGP。

    

    过去几十年来，人们对神经网络学习和内核学习之间的区别和联系表现出极大的兴趣。最近的进展在连接无限宽神经网络和高斯过程方面取得了理论上的进展。出现了两种主流方法：神经网络高斯过程(NNGP)和神经切向核(NTK)。前者基于贝叶斯推断，代表了零阶核，而后者基于梯度下降的切向空间，是第一阶核。在本文中，我们提出了统一神经内核(UNK)，该内核表征了神经网络在梯度下降和参数初始化中的学习动态。所提出的UNK内核保持了NNGP和NTK的极限特性，表现出类似于NTK的行为，但有有限的学习步骤，并且当学习步骤接近无穷大时收敛到NNGP。此外，我们还从理论上对UNK内核进行了分析。

    arXiv:2403.17467v1 Announce Type: cross  Abstract: Past decades have witnessed a great interest in the distinction and connection between neural network learning and kernel learning. Recent advancements have made theoretical progress in connecting infinite-wide neural networks and Gaussian processes. Two predominant approaches have emerged: the Neural Network Gaussian Process (NNGP) and the Neural Tangent Kernel (NTK). The former, rooted in Bayesian inference, represents a zero-order kernel, while the latter, grounded in the tangent space of gradient descents, is a first-order kernel. In this paper, we present the Unified Neural Kernel (UNK), which characterizes the learning dynamics of neural networks with gradient descents and parameter initialization. The proposed UNK kernel maintains the limiting properties of both NNGP and NTK, exhibiting behaviors akin to NTK with a finite learning step and converging to NNGP as the learning step approaches infinity. Besides, we also theoreticall
    
[^66]: 期望与现实：实践中评估入侵检测系统

    Expectations Versus Reality: Evaluating Intrusion Detection Systems in Practice

    [https://arxiv.org/abs/2403.17458](https://arxiv.org/abs/2403.17458)

    论文通过实证比较不同入侵检测系统，发现最佳解决方案取决于外部变量，如攻击类型、复杂性和网络环境，深度神经网络在某些数据集上表现最佳，但并非始终是最佳选择。

    

    我们的论文通过实证比较最近的入侵检测系统，为用户提供客观比较，以帮助用户根据其需求选择最适合的解决方案。我们的结果显示，没有一种解决方案是最好的，而是取决于外部变量，如攻击类型、复杂性和数据集中的网络环境。例如，BoT_IoT和Stratosphere IoT数据集都捕获了与物联网相关的攻击，但深度神经网络在使用BoT_IoT数据集进行测试时表现最佳，而在使用Stratosphere IoT数据集进行测试时HELAD表现最佳。因此，尽管我们发现深度神经网络解决方案在测试数据集上具有最高的平均F1分数，但并不总是表现最好的。我们进一步讨论了使用文献和项目存储库中的IDS的困难，这使得就IDS选择得出明确结论变得复杂。

    arXiv:2403.17458v1 Announce Type: cross  Abstract: Our paper provides empirical comparisons between recent IDSs to provide an objective comparison between them to help users choose the most appropriate solution based on their requirements. Our results show that no one solution is the best, but is dependent on external variables such as the types of attacks, complexity, and network environment in the dataset. For example, BoT_IoT and Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural network performed the best when tested using the BoT_IoT dataset while HELAD performed the best when tested using the Stratosphere IoT dataset. So although we found that a deep neural network solution had the highest average F1 scores on tested datasets, it is not always the best-performing one. We further discuss difficulties in using IDS from literature and project repositories, which complicated drawing definitive conclusions regarding IDS selection.
    
[^67]: 在强化学习中模仿受成本约束的行为

    Imitating Cost-Constrained Behaviors in Reinforcement Learning

    [https://arxiv.org/abs/2403.17456](https://arxiv.org/abs/2403.17456)

    该论文介绍了在强化学习中模仿受成本约束的行为的重要性，提出了模仿学习在受约束设置下的应用，并探讨了在实际领域中专家行为受限制因素影响的问题。

    

    长期以来，复杂的计划和调度问题一直通过各种优化或启发式方法来解决。最近，提出了从专家演示中学习的模仿学习作为解决这些问题的一种可行替代方法。模仿学习旨在通过观察专家的行为来学习奖励（或偏好）模型或直接行为策略。现有的模仿学习和逆向强化学习工作主要集中在无限制设置下的模仿（例如，车辆消耗的燃油量没有限制）。然而，在许多实际应用中，专家的行为不仅受奖励（或偏好）的影响，还受约束的影响。例如，自动驾驶送货车的决策不仅取决于路径偏好/奖励（根据过去的需求数据），还取决于车辆内的燃油和送达时间等约束。

    arXiv:2403.17456v1 Announce Type: cross  Abstract: Complex planning and scheduling problems have long been solved using various optimization or heuristic approaches. In recent years, imitation learning that aims to learn from expert demonstrations has been proposed as a viable alternative to solving these problems. Generally speaking, imitation learning is designed to learn either the reward (or preference) model or directly the behavioral policy by observing the behavior of an expert. Existing work in imitation learning and inverse reinforcement learning has focused on imitation primarily in unconstrained settings (e.g., no limit on fuel consumed by the vehicle). However, in many real-world domains, the behavior of an expert is governed not only by reward (or preference) but also by constraints. For instance, decisions on self-driving delivery vehicles are dependent not only on the route preferences/rewards (depending on past demand data) but also on the fuel in the vehicle and the ti
    
[^68]: 压缩链：一种系统化的组合压缩卷积神经网络方法

    Chain of Compression: A Systematic Approach to Combinationally Compress Convolutional Neural Networks

    [https://arxiv.org/abs/2403.17447](https://arxiv.org/abs/2403.17447)

    提出了一种名为“压缩链”的系统化方法，通过结合量化、剪枝、提前退出和知识蒸馏等常见技术，实现对卷积神经网络的压缩。

    

    卷积神经网络（CNNs）已经取得了显著的流行，但它们在计算和存储方面的密集性给资源有限的计算系统带来了挑战，尤其是在需要实时性能的情况下。为了减轻负担，模型压缩已经成为一个重要的研究重点。许多方法，如量化、剪枝、提前退出和知识蒸馏已经证明了减少神经网络中冗余的效果。通过进一步的研究，可以明显看出，每种方法都利用了其独特的特性来压缩神经网络，并且当它们结合在一起时也可以展现出互补的行为。为了探究这些相互作用，并从互补特性中获益，我们提出了压缩链，它在组合序列上操作，应用这些常见技术来压缩神经网络。

    arXiv:2403.17447v1 Announce Type: new  Abstract: Convolutional neural networks (CNNs) have achieved significant popularity, but their computational and memory intensity poses challenges for resource-constrained computing systems, particularly with the prerequisite of real-time performance. To release this burden, model compression has become an important research focus. Many approaches like quantization, pruning, early exit, and knowledge distillation have demonstrated the effect of reducing redundancy in neural networks. Upon closer examination, it becomes apparent that each approach capitalizes on its unique features to compress the neural network, and they can also exhibit complementary behavior when combined. To explore the interactions and reap the benefits from the complementary features, we propose the Chain of Compression, which works on the combinational sequence to apply these common techniques to compress the neural network. Validated on the image-based regression and classi
    
[^69]: 将指数平滑法融入MLP：一个简单但有效的序列模型

    Incorporating Exponential Smoothing into MLP: A Simple but Effective Sequence Model

    [https://arxiv.org/abs/2403.17445](https://arxiv.org/abs/2403.17445)

    将简单的指数平滑法与MLP结合，通过增加参数和复杂性，实现了与复杂S4模型可比较的结果

    

    在序列数据中建模长期依赖关系是序列学习中的关键步骤。最近发展的模型“结构化状态空间”（S4）在建模长期序列方面表现出显著的有效性。然而，尚不清楚S4的成功是因为其复杂的参数化和HiPPO初始化还是仅仅由于状态空间模型（SSMs）。为了进一步探讨深度SSMs的潜力，我们从简单的SSM指数平滑（ETS）开始，并通过直接将其融入逐元素MLP提出了一个叠加架构。我们通过增加额外的参数和复杂的字段来扩充简单的ETS以减少归纳偏差。尽管在逐元素MLP的参数增加不到1%的情况下，我们的模型在LRA基准测试上取得了与S4可比较的结果。

    arXiv:2403.17445v1 Announce Type: cross  Abstract: Modeling long-range dependencies in sequential data is a crucial step in sequence learning. A recently developed model, the Structured State Space (S4), demonstrated significant effectiveness in modeling long-range sequences. However, It is unclear whether the success of S4 can be attributed to its intricate parameterization and HiPPO initialization or simply due to State Space Models (SSMs). To further investigate the potential of the deep SSMs, we start with exponential smoothing (ETS), a simple SSM, and propose a stacked architecture by directly incorporating it into an element-wise MLP. We augment simple ETS with additional parameters and complex field to reduce the inductive bias. Despite increasing less than 1\% of parameters of element-wise MLP, our models achieve comparable results to S4 on the LRA benchmark.
    
[^70]: 在ALICE实验中利用机器学习从不完整数据中进行粒子识别

    Particle identification with machine learning from incomplete data in the ALICE experiment

    [https://arxiv.org/abs/2403.17436](https://arxiv.org/abs/2403.17436)

    在ALICE实验中，我们利用机器学习方法和多神经网络进行粒子识别，包括特征集嵌入和注意力机制，以在不完整数据样本上进行训练，并将ML项目与ALICE分析软件集成，讨论了域自适应技术。

    

    LHC的ALICE实验测量在超相对论重离子对撞中形成的强相互作用物质的性质。这些研究需要准确的粒子识别(PID)。ALICE通过几个探测器为动量从约100 MeV/c到20 GeV/c的粒子提供PID信息。传统上，粒子是通过矩形切割进行选择的。利用机器学习(ML)方法可以实现更好的性能。我们的解决方案使用多个神经网络(NN)作为二进制分类器。此外，我们通过特征集嵌入和关注扩展了粒子分类器，以便对包含不完整样本的数据进行训练。我们还介绍了ML项目与ALICE分析软件的集成，并讨论了域自适应，这是将知识从模拟数据转移到实际实验数据所需的ML技术。

    arXiv:2403.17436v1 Announce Type: cross  Abstract: The ALICE experiment at the LHC measures properties of the strongly interacting matter formed in ultrarelativistic heavy-ion collisions. Such studies require accurate particle identification (PID). ALICE provides PID information via several detectors for particles with momentum from about 100 MeV/c up to 20 GeV/c. Traditionally, particles are selected with rectangular cuts. Acmuch better performance can be achieved with machine learning (ML) methods. Our solution uses multiple neural networks (NN) serving as binary classifiers. Moreover, we extended our particle classifier with Feature Set Embedding and attention in order to train on data with incomplete samples. We also present the integration of the ML project with the ALICE analysis software, and we discuss domain adaptation, the ML technique needed to transfer the knowledge between simulated and real experimental data.
    
[^71]: 大规模语言模型的鲁棒且可扩展的模型编辑

    Robust and Scalable Model Editing for Large Language Models

    [https://arxiv.org/abs/2403.17431](https://arxiv.org/abs/2403.17431)

    通过适当的提示方法，经过指令微调的大型语言模型可以高度控制上下文知识，并对无关上下文具有鲁棒性，提出了EREN（通过阅读笔记来编辑模型），以改善可扩展性。

    

    大型语言模型（LLMs）可以使用参数化知识进行预测--即编码在模型权重中的知识--或者是上下文知识--即呈现在上下文中的知识。在许多场景下，一个理想的行为是当LLMs在参数化知识与上下文知识发生冲突时，优先考虑上下文知识，并在上下文无关时回退到使用他们的参数化知识。这使得通过上下文编辑来更新和纠正模型的知识成为可能，而无需重新训练。先前的研究表明，LLMs倾向于忽视上下文知识，并且在面对无关上下文时无法可靠地回退到参数化知识。在这项工作中，我们发现，通过适当的提示方法，经过指令微调的LLMs可以被上下文知识高度控制，并对无关上下文具有鲁棒性。利用这一特性，我们提出EREN（通过阅读笔记来编辑模型）来提高可扩展性。

    arXiv:2403.17431v1 Announce Type: new  Abstract: Large language models (LLMs) can make predictions using parametric knowledge--knowledge encoded in the model weights--or contextual knowledge--knowledge presented in the context. In many scenarios, a desirable behavior is that LLMs give precedence to contextual knowledge when it conflicts with the parametric knowledge, and fall back to using their parametric knowledge when the context is irrelevant. This enables updating and correcting the model's knowledge by in-context editing instead of retraining. Previous works have shown that LLMs are inclined to ignore contextual knowledge and fail to reliably fall back to parametric knowledge when presented with irrelevant context. In this work, we discover that, with proper prompting methods, instruction-finetuned LLMs can be highly controllable by contextual knowledge and robust to irrelevant context. Utilizing this feature, we propose EREN (Edit models by REading Notes) to improve the scalabil
    
[^72]: Masked Multi-Domain Network: 一种使用单一模型进行多类型和多场景转化率预测的多域网络

    Masked Multi-Domain Network: Multi-Type and Multi-Scenario Conversion Rate Prediction with a Single Model

    [https://arxiv.org/abs/2403.17425](https://arxiv.org/abs/2403.17425)

    本文提出了一种Masked Multi-Domain Network，解决了使用单一模型进行多类型和多场景转化率预测时的准确性、可扩展性和便利性问题。

    

    在现实世界的广告系统中，转化具有不同的类型，广告可以在不同的展示场景中展示，这两者都极大地影响实际的转化率（CVR）。这导致了多类型和多场景CVR预测问题。解决这一问题的理想模型应满足以下要求：1）准确性：模型应针对任何转化类型在任何展示场景上实现精细的准确性。2）可扩展性：模型参数大小应该是可承受的。3）便利性：模型不应需要大量的数据分区、子集处理和单独存储。现有方法不能同时满足这些要求。例如，为每个（转化类型，展示场景）对构建单独的模型既不具备可扩展性也不便于操作。构建一个统一模型训练所有数据，包括转化类型和展示场景信息。

    arXiv:2403.17425v1 Announce Type: cross  Abstract: In real-world advertising systems, conversions have different types in nature and ads can be shown in different display scenarios, both of which highly impact the actual conversion rate (CVR). This results in the multi-type and multi-scenario CVR prediction problem. A desired model for this problem should satisfy the following requirements: 1) Accuracy: the model should achieve fine-grained accuracy with respect to any conversion type in any display scenario. 2) Scalability: the model parameter size should be affordable. 3) Convenience: the model should not require a large amount of effort in data partitioning, subset processing and separate storage. Existing approaches cannot simultaneously satisfy these requirements. For example, building a separate model for each (conversion type, display scenario) pair is neither scalable nor convenient. Building a unified model trained on all the data with conversion type and display scenario incl
    
[^73]: 论排列不变神经网络

    On permutation-invariant neural networks

    [https://arxiv.org/abs/2403.17410](https://arxiv.org/abs/2403.17410)

    神经网络如Deep Sets和Transformers的出现显著推动了基于集合的数据处理的进展

    

    传统机器学习算法通常在假设输入数据遵循基于向量的格式的前提下设计，着重于基于向量的范式。然而，随着需求涉及基于集合的任务的增长，研究界对解决这些挑战的兴趣发生了范式转变。近年来，Deep Sets和Transformers等神经网络架构的出现在处理基于集合的数据方面取得了重大进展。这些架构专门设计为自然容纳集合作为输入，从而更有效地表示和处理集合结构。因此，近年来出现了大量致力于探索和利用这些架构能力的研究努力，以逼近集合函数的各种任务。这项综合调查旨在概述th

    arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th
    
[^74]: 使用区域指导标记将孟加拉文本与地方方言转录为国际音标

    Transcribing Bengali Text with Regional Dialects to IPA using District Guided Tokens

    [https://arxiv.org/abs/2403.17407](https://arxiv.org/abs/2403.17407)

    通过引入区域指导标记技术，本文提出了一种将孟加拉文本与地方方言转录为国际音标的方法，为模型提供了关于输入文本的地区方言信息，以理解与每个地区相关的独特音韵模式。

    

    孟加拉文本到国际音标（IPA）的准确转录是一项具有挑战性的任务，主要是由于语言的复杂音韵学和语境相关的音变。对于区域孟加拉方言来说，由于缺乏针对这些方言的标准拼写约定、当地和外语在这些地区中流行的词汇以及不同地区之间的音韵多样性，这一挑战甚至更为严峻。本文提出了一种方法来解决这个序列到序列的问题，即在覆盖孟加拉国六个地区的新数据集上引入“区域指导标记”（DGT）技术。其关键思想是在生成IPA转录之前向模型提供有关输入文本的区域方言或“地区”的明确信息。这通过在输入序列前添加一个地区标记来实现，有效地引导模型理解与每个地区相关的独特音韵模式。

    arXiv:2403.17407v1 Announce Type: cross  Abstract: Accurate transcription of Bengali text to the International Phonetic Alphabet (IPA) is a challenging task due to the complex phonology of the language and context-dependent sound changes. This challenge is even more for regional Bengali dialects due to unavailability of standardized spelling conventions for these dialects, presence of local and foreign words popular in those regions and phonological diversity across different regions. This paper presents an approach to this sequence-to-sequence problem by introducing the District Guided Tokens (DGT) technique on a new dataset spanning six districts of Bangladesh. The key idea is to provide the model with explicit information about the regional dialect or "district" of the input text before generating the IPA transcription. This is achieved by prepending a district token to the input sequence, effectively guiding the model to understand the unique phonetic patterns associated with each 
    
[^75]: 稀疏专家混合的泛化误差分析: 一项初步研究

    Generalization Error Analysis for Sparse Mixture-of-Experts: A Preliminary Study

    [https://arxiv.org/abs/2403.17404](https://arxiv.org/abs/2403.17404)

    本文讨论了稀疏专家混合模型在泛化误差方面的探索，特别关注了数据样本数量的影响。

    

    Mixture-of-Experts (MoE)代表了一种整合预测来自几个专门子模型（称为专家）的方法。这种融合是通过一个路由机制实现的，根据输入数据动态分配权重给每个专家的贡献。传统的MoE机制选择所有可用的专家，带来了可观的计算成本。相反，稀疏专家混合（Sparse MoE）只选择有限数量，甚至只有一个专家，显着降低计算开销，同时在经验上保留，有时甚至增强性能。尽管MoE具有广泛的应用和这些优点，但其理论基础仍然难以捉摸。本文探讨了稀疏MoE在各种关键因素方面的泛化误差。具体来说，我们研究了数据样本数量的影响

    arXiv:2403.17404v1 Announce Type: new  Abstract: Mixture-of-Experts (MoE) represents an ensemble methodology that amalgamates predictions from several specialized sub-models (referred to as experts). This fusion is accomplished through a router mechanism, dynamically assigning weights to each expert's contribution based on the input data. Conventional MoE mechanisms select all available experts, incurring substantial computational costs. In contrast, Sparse Mixture-of-Experts (Sparse MoE) selectively engages only a limited number, or even just one expert, significantly reducing computation overhead while empirically preserving, and sometimes even enhancing, performance. Despite its wide-ranging applications and these advantageous characteristics, MoE's theoretical underpinnings have remained elusive. In this paper, we embark on an exploration of Sparse MoE's generalization error concerning various critical factors. Specifically, we investigate the impact of the number of data samples, 
    
[^76]: 机器学习中的应用驱动创新

    Application-Driven Innovation in Machine Learning

    [https://arxiv.org/abs/2403.17381](https://arxiv.org/abs/2403.17381)

    应用驱动研究在机器学习领域具有重要影响，可以与方法驱动研究有益地协同，但目前审查、招聘和教学实践往往阻碍了这种创新。

    

    随着机器学习应用的不断增长，受特定现实挑战启发的创新算法变得日益重要。这样的工作不仅在应用领域具有重要影响，也在机器学习本身具有重要影响。本文描述了机器学习中应用驱动研究的范式，将其与更标准的方法驱动研究进行了对比。我们阐明了应用驱动机器学习的好处，以及这种方法如何可以与方法驱动工作有益地协同。尽管具有这些好处，我们发现机器学习中的审查、招聘和教学实践往往阻碍了应用驱动创新。我们概述了如何改进这些流程。

    arXiv:2403.17381v1 Announce Type: cross  Abstract: As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.
    
[^77]: 探索和应用基于音频的情感分析在音乐中

    Exploring and Applying Audio-Based Sentiment Analysis in Music

    [https://arxiv.org/abs/2403.17379](https://arxiv.org/abs/2403.17379)

    本文探讨了基于音频的情感分析在音乐中的运用，通过预测音乐片段随时间的情感变化以及确定音乐时间序列中下一个情感值来实现无缝过渡。

    

    情感分析是文本处理中不断探索的领域，涉及对文本的意见、情感和主观性的计算分析。然而，这个想法不仅限于文本和语音，事实上，它也可以应用于其他形式。实际上，人类在音乐中表达自己的深度不如在文本中。计算模型解释音乐情感的能力在很大程度上尚未被探索，可能对治疗和音乐播放等方面产生影响和用途。本文涉及两个独立任务。该研究旨在(1)预测音乐片段随时间的情感，以及(2)在时间序列中确定音乐后的下一个情感值，以确保无缝过渡。利用包含从Free Music Archive中选中并用Russel的af圆环模型报告的愉悦和激活水平注释的歌曲片段的Emotions in Music数据库的数据。

    arXiv:2403.17379v1 Announce Type: cross  Abstract: Sentiment analysis is a continuously explored area of text processing that deals with the computational analysis of opinions, sentiments, and subjectivity of text. However, this idea is not limited to text and speech, in fact, it could be applied to other modalities. In reality, humans do not express themselves in text as deeply as they do in music. The ability of a computational model to interpret musical emotions is largely unexplored and could have implications and uses in therapy and musical queuing. In this paper, two individual tasks are addressed. This study seeks to (1) predict the emotion of a musical clip over time and (2) determine the next emotion value after the music in a time series to ensure seamless transitions. Utilizing data from the Emotions in Music Database, which contains clips of songs selected from the Free Music Archive annotated with levels of valence and arousal as reported on Russel's circumplex model of af
    
[^78]: 具有扰动注意力引导的自矫正扩散抽样

    Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance

    [https://arxiv.org/abs/2403.17377](https://arxiv.org/abs/2403.17377)

    提出了一种名为扰动注意力引导（PAG）的新型抽样引导技术，通过在扩散 U-Net 中替换自注意力映射来生成结构降级的中间样本，从而在无条件和有条件设置下改善扩散样本质量。

    

    近期研究表明，扩散模型能够生成高质量样本，但其质量很大程度上依赖于抽样引导技术，比如分类器引导（CG）和无分类器引导（CFG）。这些技术通常在无条件生成或各种下游任务如图像恢复中无法应用。本文提出了一种新颖的抽样引导技术，称为扰动注意力引导（PAG），它改进了扩散样本的质量，不管是在无条件还是有条件的设置中，都能实现这一目标，而不需要额外训练或整合外部模块。PAG 旨在通过整个去噪过程逐步增强样本的结构。它涉及通过用恒等矩阵替换扩散 U-Net 中选择的自注意力映射生成结构降级的中间样本，考虑自注意力机制。

    arXiv:2403.17377v1 Announce Type: cross  Abstract: Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms
    
[^79]: AIDE：用于自动驾驶目标检测的自动数据引擎

    AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving

    [https://arxiv.org/abs/2403.17373](https://arxiv.org/abs/2403.17373)

    提出了使用自动数据引擎（AIDE）的方法，通过自动识别问题、高效筛选数据、自动标注改进模型、生成多样化场景验证模型，在自动驾驶领域取得了优异的性能。

    

    arXiv:2403.17373v1 公告类型:跨领域 摘要:自动驾驶汽车（AV）系统依赖稳健的感知模型作为安全保障的基石。然而，在道路上遇到的物体呈现长尾分布，罕见或未见类别对部署的感知模型构成挑战。这需要通过昂贵的过程不断地筛选和标注数据，需要人力的巨大投入。我们提出利用最近在视觉语言和大型语言模型方面的进展，设计一个自动数据引擎（AIDE），自动识别问题，高效筛选数据，通过自动标注改进模型，并通过生成多样化场景验证模型。这一过程可以迭代进行，允许模型不断自我改进。我们进一步建立了一个开放世界检测基准，以全面评估各种学习范式，在AV数据集上展示我们方法在降低方面的卓越表现。

    arXiv:2403.17373v1 Announce Type: cross  Abstract: Autonomous vehicle (AV) systems rely on robust perception models as a cornerstone of safety assurance. However, objects encountered on the road exhibit a long-tailed distribution, with rare or unseen categories posing challenges to a deployed perception model. This necessitates an expensive process of continuously curating and annotating data with significant human effort. We propose to leverage recent advances in vision-language and large language models to design an Automatic Data Engine (AIDE) that automatically identifies issues, efficiently curates data, improves the model through auto-labeling, and verifies the model through generation of diverse scenarios. This process operates iteratively, allowing for continuous self-improvement of the model. We further establish a benchmark for open-world detection on AV datasets to comprehensively evaluate various learning paradigms, demonstrating our method's superior performance at a reduc
    
[^80]: 一种适用于LQR元策略估计的Moreau包络方法

    A Moreau Envelope Approach for LQR Meta-Policy Estimation

    [https://arxiv.org/abs/2403.17364](https://arxiv.org/abs/2403.17364)

    提出了一种基于Moreau包络的替代LQR成本，可有效调整到新实现的元策略，并设计了找到近似一阶稳定点的算法。

    

    我们研究了在线性时不变离散时间不确定动态系统中的线性二次型调节器（LQR）策略估计问题。我们提出了一种基于Moreau包络的替代LQR成本，由不确定系统的有限实现构建，以定义一个对新实现有效调整的元策略。此外，我们设计了一个算法来找到元LQR成本函数的近似一阶稳定点。数值结果表明，所提出的方法在新实现的线性系统上胜过了控制器的朴素平均。我们还提供了实证证据表明，我们的方法比模型不可知元学习（MAML）方法具有更好的样本复杂性。

    arXiv:2403.17364v1 Announce Type: cross  Abstract: We study the problem of policy estimation for the Linear Quadratic Regulator (LQR) in discrete-time linear time-invariant uncertain dynamical systems. We propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of realizations of the uncertain system, to define a meta-policy efficiently adjustable to new realizations. Moreover, we design an algorithm to find an approximate first-order stationary point of the meta-LQR cost function. Numerical results show that the proposed approach outperforms naive averaging of controllers on new realizations of the linear system. We also provide empirical evidence that our method has better sample complexity than Model-Agnostic Meta-Learning (MAML) approaches.
    
[^81]: 基于双编码器的多目标轨迹规划

    Multi-Objective Trajectory Planning with Dual-Encoder

    [https://arxiv.org/abs/2403.17353](https://arxiv.org/abs/2403.17353)

    通过引入基于双编码器的变压器模型和顺序二次规划优化，本文提出了一种加速时间-加速度最优轨迹规划的两阶段方法，在减少轨迹规划时间和缩小优化差距方面取得了显著成果

    

    时间-加速度最优轨迹规划对于提升机器人臂在动态任务中的性能至关重要。传统方法依赖于解决复杂的非线性规划问题，导致生成优化轨迹的显著延迟。本文提出了一个两阶段方法来加速时间-加速度最优轨迹规划。首先，我们引入了基于双编码器的变压器模型来建立一个良好的初步轨迹。然后，通过顺序二次规划对该轨迹进行细化，以提高其最优性和稳健性。我们的方法在减少轨迹规划时间方面优于现有技术达79.72\%。与现有方法相比，我们的方法缩小了与目标函数值相关的最优性差距，最高降低了29.9\%。

    arXiv:2403.17353v1 Announce Type: cross  Abstract: Time-jerk optimal trajectory planning is crucial in advancing robotic arms' performance in dynamic tasks. Traditional methods rely on solving complex nonlinear programming problems, bringing significant delays in generating optimized trajectories. In this paper, we propose a two-stage approach to accelerate time-jerk optimal trajectory planning. Firstly, we introduce a dual-encoder based transformer model to establish a good preliminary trajectory. This trajectory is subsequently refined through sequential quadratic programming to improve its optimality and robustness. Our approach outperforms the state-of-the-art by up to 79.72\% in reducing trajectory planning time. Compared with existing methods, our method shrinks the optimality gap with the objective function value decreasing by up to 29.9\%.
    
[^82]: 从异质性学习：异质信息增强图神经网络

    Learn from Heterophily: Heterophilous Information-enhanced Graph Neural Network

    [https://arxiv.org/abs/2403.17351](https://arxiv.org/abs/2403.17351)

    本文提出了HiGNN，一种创新方法，通过调查节点的邻居分布来有效利用异质信息，从而增强图神经网络的学习效果。

    

    在异质性环境中，Graph Neural Networks (GNNs)通常表现出次优性能，因为不同标签的节点倾向于基于语义含义相连。目前关于图异质性的研究主要集中在聚合校准或邻居扩展上，通过利用节点特征或结构信息来改善GNN表示以解决异质性问题。本文提出并证明了异质性中内在的宝贵语义信息可以通过调查图中每个单独节点的邻居分布来有效地利用在图学习中。通过理论分析，论证了这一理念在增强图学习中的有效性。基于此分析，提出了HiGNN，一种创新方法，构建了一个额外的新图结构，通过利用节点分布整合异质信息来增强图学习。

    arXiv:2403.17351v1 Announce Type: new  Abstract: Under circumstances of heterophily, where nodes with different labels tend to be connected based on semantic meanings, Graph Neural Networks (GNNs) often exhibit suboptimal performance. Current studies on graph heterophily mainly focus on aggregation calibration or neighbor extension and address the heterophily issue by utilizing node features or structural information to improve GNN representations. In this paper, we propose and demonstrate that the valuable semantic information inherent in heterophily can be utilized effectively in graph learning by investigating the distribution of neighbors for each individual node within the graph. The theoretical analysis is carried out to demonstrate the efficacy of the idea in enhancing graph learning. Based on this analysis, we propose HiGNN, an innovative approach that constructs an additional new graph structure, that integrates heterophilous information by leveraging node distribution to enha
    
[^83]: 语言模型是生物医学成像任务的免费助推器

    Language Models are Free Boosters for Biomedical Imaging Tasks

    [https://arxiv.org/abs/2403.17343](https://arxiv.org/abs/2403.17343)

    本研究揭示了基于残差的大型语言模型在生物医学成像任务中作为编码器的意想不到的有效性，利用冻结的变压器块进行直接处理视觉令牌，从而提高各种生物医学成像应用的性能。

    

    在这项研究中，我们揭示了基于残差的大型语言模型（LLMs）在生物医学成像任务中作为编码器的意想不到的有效性，这是传统上缺乏语言或文本数据的领域。该方法不同于已建立的方法，通过利用从预训练的LLMs中提取的冻结变压器块作为创新的编码器层，直接处理视觉令牌。这种策略与通常依赖于语言驱动提示和输入的标准多模态视觉语言框架有着显著的不同。我们发现这些LLMs能够提升各种生物医学成像应用的性能，包括2D和3D视觉分类任务，充当即插即用的助推器。更有趣的是，作为副产品，我们发现所提出的框架实现了卓越的性能，在M的广泛、标准化数据集中取得了新的最先进结果。

    arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
    
[^84]: 人工智能模型中的公平追求：一项调查

    The Pursuit of Fairness in Artificial Intelligence Models: A Survey

    [https://arxiv.org/abs/2403.17333](https://arxiv.org/abs/2403.17333)

    人工智能模型中的公平追求至关重要，研究人员努力解决偏见问题，确保模型不会有意或无意地对某些群体产生偏见。

    

    现在，人工智能（AI）模型被广泛应用于医疗、教育和就业等方方面面。由于它们被应用于许多敏感环境，并做出可能改变人生的决策，潜在的偏见结果成为一个紧迫的问题。开发人员应确保这类模型不会表现出任何意外的歧视行为，比如偏爱某些性别、种族或残疾人士。随着人工智能系统的普遍传播，研究人员和从业者对不公平模型越来越有意识，并致力于减少其中的偏见。在解决这些问题方面已经进行了重要研究，以确保模型不会有意或无意地延续偏见。这项调查概述了研究人员如何促进人工智能系统的公平性。我们探讨了当前文献中存在的公平性不同定义。

    arXiv:2403.17333v1 Announce Type: new  Abstract: Artificial Intelligence (AI) models are now being utilized in all facets of our lives such as healthcare, education and employment. Since they are used in numerous sensitive environments and make decisions that can be life altering, potential biased outcomes are a pressing matter. Developers should ensure that such models don't manifest any unexpected discriminatory practices like partiality for certain genders, ethnicities or disabled people. With the ubiquitous dissemination of AI systems, researchers and practitioners are becoming more aware of unfair models and are bound to mitigate bias in them. Significant research has been conducted in addressing such issues to ensure models don't intentionally or unintentionally perpetuate bias. This survey offers a synopsis of the different ways researchers have promoted fairness in AI systems. We explore the different definitions of fairness existing in the current literature. We create a compr
    
[^85]: 深度支持向量

    Deep Support Vectors

    [https://arxiv.org/abs/2403.17329](https://arxiv.org/abs/2403.17329)

    该论文探索了深度学习模型中的深度支持向量（DSVs）的概念，介绍了DeepKKT条件，通过实证研究发现DSVs与SVM中的支持向量类似，为解释模型决策标准提供了方法，同时证明了可以有效地使用DSVs重构模型。

    

    尽管深度学习的成功通常被归因于其与支持向量机（SVM）在理论上的等价性，但这种关系的实际影响尚未得到全面探讨。本文在这一领域开展了一项探索，重点关注深度学习模型中深度支持向量（DSVs）的识别。我们引入了DeepKKT条件的概念，这是一种专为深度学习量身定制的传统Karush-Kuhn-Tucker（KKT）条件的调整版本。通过实证研究，我们阐明了DSVs与SVM中的支持向量之间存在相似性，提供了一种解释模型决策标准的切实方法。此外，我们的研究结果表明，可以有效地使用DSVs重构模型，类似于SVM中的过程。代码将会公开。

    arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
    
[^86]: ALISA: 通过稀疏感知KV缓存加速大型语言模型推理

    ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching

    [https://arxiv.org/abs/2403.17312](https://arxiv.org/abs/2403.17312)

    提出了ALISA，一种通过稀疏感知KV缓存加速大型语言模型推理的新算法系统设计。

    

    Transformer架构显著推动了自然语言处理（NLP）的发展，并且在开发大型语言模型（LLMs）方面具有基础性作用，如LLaMA和OPT，这些模型已经在广泛的NLP任务中占据主导地位。尽管它们具有优越的准确性，但LLMs在实际推理中存在独特挑战，涉及计算和占用大量内存。由于LLM推理具有自回归特性，Transformer中的注意层的KV缓存可以通过将二次复杂度计算替换为线性复杂度内存访问，从而有效加速LLM推理。然而，随着对处理更长序列的需求增加，这种方法需要增加内存。这种开销导致由于I/O瓶颈和甚至是内存不足错误而导致吞吐量降低，特别是在资源受限的系统上，如单个通用GPU上。

    arXiv:2403.17312v1 Announce Type: new  Abstract: The Transformer architecture has significantly advanced natural language processing (NLP) and has been foundational in developing large language models (LLMs) such as LLaMA and OPT, which have come to dominate a broad range of NLP tasks. Despite their superior accuracy, LLMs present unique challenges in practical inference, concerning the compute and memory-intensive nature. Thanks to the autoregressive characteristic of LLM inference, KV caching for the attention layers in Transformers can effectively accelerate LLM inference by substituting quadratic-complexity computation with linear-complexity memory accesses. Yet, this approach requires increasing memory as demand grows for processing longer sequences. The overhead leads to reduced throughput due to I/O bottlenecks and even out-of-memory errors, particularly on resource-constrained systems like a single commodity GPU. In this paper, we propose ALISA, a novel algorithm-system co-desi
    
[^87]: 神经多模态主题建模：全面评估

    Neural Multimodal Topic Modeling: A Comprehensive Evaluation

    [https://arxiv.org/abs/2403.17308](https://arxiv.org/abs/2403.17308)

    该论文对包含文本和图片的文档的多模态主题建模进行了全面评估，并提出了两种新颖的主题建模解决方案和两种新颖的评估指标，结果显示这些模型均能产生连贯且多样化的主题。

    

    神经主题模型可以成功地在文本数据中找到连贯且多样化的主题。然而，它们在处理多模态数据集（如图片和文本）方面存在局限性。本文首次提出了包含文本和图片的文档的多模态主题建模的系统性和全面评估。在此过程中，我们提出了两种新颖的主题建模解决方案和两种新颖的评估指标。总体而言，我们对一个前所未有的丰富多样的数据集集合进行的评估表明，我们的两个模型都能产生连贯且多样化的主题。然而，一个方法优于另一个方法的程度取决于指标和数据集的组合，这表明未来需要进一步探索混合解决方案。值得注意的是，我们简洁的人工评估与我们提出的指标所确定的结果一致。这种一致不仅加强了我们指标的可信度，也突显了

    arXiv:2403.17308v1 Announce Type: cross  Abstract: Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the po
    
[^88]: Hawk: 使用安全查找表计算的准确快速隐私保护机器学习

    Hawk: Accurate and Fast Privacy-Preserving Machine Learning Using Secure Lookup Table Computation

    [https://arxiv.org/abs/2403.17296](https://arxiv.org/abs/2403.17296)

    本研究提出了用于逻辑回归和神经网络模型的新隐私保护机器学习协议，通过采用秘密共享查找表计算非线性函数，提高了计算效率和准确性，并探索了放松安全措施的可能性。

    

    在没有直接数据共享的情况下对来自多个实体的数据进行机器学习模型训练可以解锁受到业务、法律或伦理约束的应用。本文设计并实现了新的隐私保护机器学习协议，用于逻辑回归和神经网络模型。我们采用数据所有者在两个服务器之间进行数据秘密共享，这两个服务器对联合数据进行模型训练和评估。现有方法中存在的一个重要的低效和不准确之处在于使用Yao的混淆电路来计算非线性激活函数。我们提出了基于秘密共享查找表计算非线性函数的新方法，既提高了计算效率又提高了准确性。除了引入无泄漏技术，我们还开展了对隐私保护机器学习的放松安全措施的探索。

    arXiv:2403.17296v1 Announce Type: cross  Abstract: Training machine learning models on data from multiple entities without direct data sharing can unlock applications otherwise hindered by business, legal, or ethical constraints. In this work, we design and implement new privacy-preserving machine learning protocols for logistic regression and neural network models. We adopt a two-server model where data owners secret-share their data between two servers that train and evaluate the model on the joint data. A significant source of inefficiency and inaccuracy in existing methods arises from using Yao's garbled circuits to compute non-linear activation functions. We propose new methods for computing non-linear functions based on secret-shared lookup tables, offering both computational efficiency and improved accuracy.   Beyond introducing leakage-free techniques, we initiate the exploration of relaxed security measures for privacy-preserving machine learning. Instead of claiming that the 
    
[^89]: 并非所有联邦学习算法都一视同仁：一项性能评估研究

    Not All Federated Learning Algorithms Are Created Equal: A Performance Evaluation Study

    [https://arxiv.org/abs/2403.17287](https://arxiv.org/abs/2403.17287)

    联邦学习算法的性能评估研究揭示出，没有一种算法可以在所有性能指标上表现最佳，一些最先进的算法虽然准确性更高，却伴随着更高的计算或通信开销。

    

    《联邦学习(FL)作为一种从分散数据训练模型的实际方法崛起。FL的流行导致了众多FL算法和机制的发展。许多先前的努力主要集中在这些方法的准确性上，但对其他方面，如计算开销，性能和训练稳定性等的理解却很少。为了弥补这一差距，我们利用一款名为Flame的开源联邦学习框架，对几种传统FL算法（FedAvg，FedProx，FedYogi，FedAdam，SCAFFOLD和FedDyn）进行了广泛的性能评估。我们的全面测量研究表明，没有一种算法可以在不同的性能指标上表现最好。一些关键观察结果是：（1）虽然一些最先进的算法达到了比其他算法更高的准确性，但它们会带来更高的计算开销（FedDyn）或通信开销（SCAFFOLD）。

    arXiv:2403.17287v1 Announce Type: new  Abstract: Federated Learning (FL) emerged as a practical approach to training a model from decentralized data. The proliferation of FL led to the development of numerous FL algorithms and mechanisms. Many prior efforts have given their primary focus on accuracy of those approaches, but there exists little understanding of other aspects such as computational overheads, performance and training stability, etc. To bridge this gap, we conduct extensive performance evaluation on several canonical FL algorithms (FedAvg, FedProx, FedYogi, FedAdam, SCAFFOLD, and FedDyn) by leveraging an open-source federated learning framework called Flame. Our comprehensive measurement study reveals that no single algorithm works best across different performance metrics. A few key observations are: (1) While some state-of-the-art algorithms achieve higher accuracy than others, they incur either higher computation overheads (FedDyn) or communication overheads (SCAFFOLD).
    
[^90]: 对强化学习中的往返设计进行的分析

    An Analysis of Switchback Designs in Reinforcement Learning

    [https://arxiv.org/abs/2403.17285](https://arxiv.org/abs/2403.17285)

    本文通过提出“弱信号分析”框架，研究了强化学习中往返设计对平均处理效应估计准确性的影响，发现在大部分奖励误差为正相关时，往返设计比每日切换策略更有效，增加政策切换频率可以降低平均处理效应估计器的均方误差。

    

    本文提供了对A/B测试中往返设计的详细研究，这些设计随时间在基准和新策略之间交替。我们的目标是全面评估这些设计对其产生的平均处理效应（ATE）估计器准确性的影响。我们提出了一个新颖的“弱信号分析”框架，大大简化了这些ATE的均方误差（MSE）在马尔科夫决策过程环境中的计算。我们的研究结果表明：(i) 当大部分奖励误差呈正相关时，往返设计比每日切换策略的交替设计更有效。此外，增加政策切换的频率往往会降低ATE估计器的MSE。(ii) 然而，当误差不相关时，所有这些设计变得渐近等效。(iii) 在大多数误差为负相关时

    arXiv:2403.17285v1 Announce Type: cross  Abstract: This paper offers a detailed investigation of switchback designs in A/B testing, which alternate between baseline and new policies over time. Our aim is to thoroughly evaluate the effects of these designs on the accuracy of their resulting average treatment effect (ATE) estimators. We propose a novel "weak signal analysis" framework, which substantially simplifies the calculations of the mean squared errors (MSEs) of these ATEs in Markov decision process environments. Our findings suggest that (i) when the majority of reward errors are positively correlated, the switchback design is more efficient than the alternating-day design which switches policies in a daily basis. Additionally, increasing the frequency of policy switches tends to reduce the MSE of the ATE estimator. (ii) When the errors are uncorrelated, however, all these designs become asymptotically equivalent. (iii) In cases where the majority of errors are negative correlate
    
[^91]: 探索CausalWorld：通过知识转移和课程学习增强机器人操纵能力

    Exploring CausalWorld: Enhancing robotic manipulation via knowledge transfer and curriculum learning

    [https://arxiv.org/abs/2403.17266](https://arxiv.org/abs/2403.17266)

    通过知识转移和课程学习，本研究在三指机械臂操纵任务中采用强化学习，实现了提高学习效率和有效性的目标

    

    本研究探讨了一个基于学习的三指机械臂操纵任务，该任务需要指间的复杂运动和协调。通过采用强化学习，我们训练一个智能体来获得熟练操纵所需的技能。为了增强学习过程的效率和有效性，我们在软演员-评论家架构中采用了两种知识转移策略，即微调和课程学习。微调使智能体可以利用预先训练的知识并将其调整到新任务中。实施和评估了多种变体，如模型转移、策略转移和跨任务转移。为了消除预训练的需求，课程学习将高级任务分解为更简单、渐进的阶段，模拟了人类学习过程。发现学习阶段的数量、子任务的背景以及转换时机是关键的设计参数。

    arXiv:2403.17266v1 Announce Type: cross  Abstract: This study explores a learning-based tri-finger robotic arm manipulating task, which requires complex movements and coordination among the fingers. By employing reinforcement learning, we train an agent to acquire the necessary skills for proficient manipulation. To enhance the efficiency and effectiveness of the learning process, two knowledge transfer strategies, fine-tuning and curriculum learning, were utilized within the soft actor-critic architecture. Fine-tuning allows the agent to leverage pre-trained knowledge and adapt it to new tasks. Several variations like model transfer, policy transfer, and across-task transfer were implemented and evaluated. To eliminate the need for pretraining, curriculum learning decomposes the advanced task into simpler, progressive stages, mirroring how humans learn. The number of learning stages, the context of the sub-tasks, and the transition timing were found to be the critical design parameter
    
[^92]: 基于图的扩散负采样用于链接预测

    Diffusion-based Negative Sampling on Graphs for Link Prediction

    [https://arxiv.org/abs/2403.17259](https://arxiv.org/abs/2403.17259)

    提出了一种基于图的扩散负采样的新策略，能够从潜在空间中以多个灵活和可控的“难度”级别生成负节点。

    

    链接预测是图分析中的一个基础任务，在网络上具有重要的应用，如社交网络分析、推荐系统等。现代图链接预测方法通常采用对比方法来学习稳健的节点表示，其中负采样至关重要。典型的负采样方法旨在根据预定义的启发式或自动对抗方法检索困难示例，这可能不够灵活或难以控制。此外，在链接预测的背景下，大多数先前的方法从图的现有子结构中对负节点进行抽样，错过了潜在更优的潜在空间中的样本。为解决这些问题，我们研究了一种新颖的多级负采样策略，能够从潜在空间中以灵活且可控制的“难度”级别生成负节点。我们的方法名为有条件的基于扩散的...

    arXiv:2403.17259v1 Announce Type: new  Abstract: Link prediction is a fundamental task for graph analysis with important applications on the Web, such as social network analysis and recommendation systems, etc. Modern graph link prediction methods often employ a contrastive approach to learn robust node representations, where negative sampling is pivotal. Typical negative sampling methods aim to retrieve hard examples based on either predefined heuristics or automatic adversarial approaches, which might be inflexible or difficult to control. Furthermore, in the context of link prediction, most previous methods sample negative nodes from existing substructures of the graph, missing out on potentially more optimal samples in the latent space. To address these issues, we investigate a novel strategy of multi-level negative sampling that enables negative node generation with flexible and controllable ``hardness'' levels from the latent space. Our method, called Conditional Diffusion-based 
    
[^93]: 使用图神经网络预测制造服务能力

    Manufacturing Service Capability Prediction with Graph Neural Networks

    [https://arxiv.org/abs/2403.17239](https://arxiv.org/abs/2403.17239)

    该研究提出了一种基于图神经网络的制造服务能力识别方法，通过聚合图节点邻域信息和对图数据进行过采样来提高识别性能，增强制造能力识别的准确性和完整性。

    

    在当前的环境中，从制造商身上识别制造能力的主要方法往往严重依赖于关键词匹配和语义匹配。然而，这些方法常常在忽视宝贵的隐藏信息或误解关键数据方面效果不佳。因此，这些方法导致对制造商能力的识别不完整。这凸显了迫切需要数据驱动解决方案来提高制造能力识别的准确性和完整性。为了解决这一需求，本研究提出了一种基于图神经网络的制造服务能力识别方法，可在知识图谱上应用。为了提高识别性能，本研究引入了一种新颖的方法，涉及从图节点邻域聚合信息以及对图数据进行过采样，该方法可有效应用于广泛的实践领域。

    arXiv:2403.17239v1 Announce Type: new  Abstract: In the current landscape, the predominant methods for identifying manufacturing capabilities from manufacturers rely heavily on keyword matching and semantic matching. However, these methods often fall short by either overlooking valuable hidden information or misinterpreting critical data. Consequently, such approaches result in an incomplete identification of manufacturers' capabilities. This underscores the pressing need for data-driven solutions to enhance the accuracy and completeness of manufacturing capability identification. To address the need, this study proposes a Graph Neural Network-based method for manufacturing service capability identification over a knowledge graph. To enhance the identification performance, this work introduces a novel approach that involves aggregating information from the graph nodes' neighborhoods as well as oversampling the graph data, which can be effectively applied across a wide range of practica
    
[^94]: 基于时间和语义评估指标的基础模型在机器人子任务事后分析中的应用

    Temporal and Semantic Evaluation Metrics for Foundation Models in Post-Hoc Analysis of Robotic Sub-tasks

    [https://arxiv.org/abs/2403.17238](https://arxiv.org/abs/2403.17238)

    提出了一种基于基础模型的自动化框架，通过新颖的提示策略将轨迹数据分解为时间和语言描述的子任务，同时引入了时间相似性和语义相似性两种新的评估指标。

    

    最近在任务和运动规划（TAMP）领域的研究表明，在使用带有质量标记数据的语言监督机器人轨迹进行控制策略训练可以显着提高代理任务成功率。然而，这类数据的稀缺性对将这些方法扩展到一般用例构成重大障碍。为了解决这一问题，我们提出了一种自动化框架，通过利用最近的基础模型（FMs）的提示策略，包括大型语言模型（LLMs）和视觉语言模型（VLMs），将轨迹数据分解为基于时间和自然语言的描述性子任务。我们的框架为构成完整轨迹的底层子任务提供了基于时间和语言的描述。为了严格评估我们的自动标记框架的质量，我们提出了一种算法 SIMILARITY 来生成两种新颖的指标，即时间相似性和语义相似性。

    arXiv:2403.17238v1 Announce Type: cross  Abstract: Recent works in Task and Motion Planning (TAMP) show that training control policies on language-supervised robot trajectories with quality labeled data markedly improves agent task success rates. However, the scarcity of such data presents a significant hurdle to extending these methods to general use cases. To address this concern, we present an automated framework to decompose trajectory data into temporally bounded and natural language-based descriptive sub-tasks by leveraging recent prompting strategies for Foundation Models (FMs) including both Large Language Models (LLMs) and Vision Language Models (VLMs). Our framework provides both time-based and language-based descriptions for lower-level sub-tasks that comprise full trajectories. To rigorously evaluate the quality of our automatic labeling framework, we contribute an algorithm SIMILARITY to produce two novel metrics, temporal similarity and semantic similarity. The metrics me
    
[^95]: 具有量化整流器的神经图像压缩

    Neural Image Compression with Quantization Rectifier

    [https://arxiv.org/abs/2403.17236](https://arxiv.org/abs/2403.17236)

    该论文提出了一种利用图像特征相关性来减轻量化影响的神经图像压缩量化整流器方法，通过设计神经网络架构预测未经量化的特征，从而实现更好的图像重建质量。

    

    神经图像压缩在速率失真性能方面已被证明优于传统图像编解码器。然而，量化在压缩过程中引入误差，可能会降低压缩图像的质量。现有方法解决了量化过程中产生的训练-测试不匹配问题，但对图像特征表达的量化随机影响仍未解决。本文提出了一种新颖的图像压缩量化整流器（QR）方法，利用图像特征相关性来减轻量化的影响。我们的方法设计了一个神经网络架构，从量化的特征中预测未经量化的特征，保持特征的表达能力，以实现更好的图像重建质量。我们开发了一种软-预测训练技术，将QR集成到现有的神经图像编解码器中。在评估中，我们将QR集成到最先进的神经图像编解码器中。

    arXiv:2403.17236v1 Announce Type: new  Abstract: Neural image compression has been shown to outperform traditional image codecs in terms of rate-distortion performance. However, quantization introduces errors in the compression process, which can degrade the quality of the compressed image. Existing approaches address the train-test mismatch problem incurred during quantization, the random impact of quantization on the expressiveness of image features is still unsolved. This paper presents a novel quantization rectifier (QR) method for image compression that leverages image feature correlation to mitigate the impact of quantization. Our method designs a neural network architecture that predicts unquantized features from the quantized ones, preserving feature expressiveness for better image reconstruction quality. We develop a soft-to-predictive training technique to integrate QR into existing neural image codecs. In evaluation, we integrate QR into state-of-the-art neural image codecs 
    
[^96]: 使用先验领域知识在采样过程中进行动力学的主动学习

    Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling Process

    [https://arxiv.org/abs/2403.17233](https://arxiv.org/abs/2403.17233)

    提出了一种主动学习算法，通过将先验领域知识引入采样过程中，加速学习并降低模型不确定性，从而有效估计动力学。

    

    我们提出了一种用于学习动力学的主动学习算法，该算法利用侧面信息，通过将先验领域知识明确纳入采样过程中。我们提出的算法引导探索走向表现出观察数据与从侧面信息派生的动力学不完美先验模型之间高经验性不一致性的区域。通过数值实验，我们证明了这种策略可以探索高不一致性区域并加速学习，同时降低模型不确定性。我们严格证明了我们的主动学习算法通过为最大预测方差提供显式收敛速率，可以产生一致的基础动力学估计。我们在一个欠驱动的摆系统和半猎豹MuJoCo环境上展示了我们方法的有效性。

    arXiv:2403.17233v1 Announce Type: cross  Abstract: We present an active learning algorithm for learning dynamics that leverages side information by explicitly incorporating prior domain knowledge into the sampling process. Our proposed algorithm guides the exploration toward regions that demonstrate high empirical discrepancy between the observed data and an imperfect prior model of the dynamics derived from side information. Through numerical experiments, we demonstrate that this strategy explores regions of high discrepancy and accelerates learning while simultaneously reducing model uncertainty. We rigorously prove that our active learning algorithm yields a consistent estimate of the underlying dynamics by providing an explicit rate of convergence for the maximum predictive variance. We demonstrate the efficacy of our approach on an under-actuated pendulum system and on the half-cheetah MuJoCo environment.
    
[^97]: Dyna-LfLH:从学到的幻觉中学会在动态环境中学习灵活导航

    Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination

    [https://arxiv.org/abs/2403.17231](https://arxiv.org/abs/2403.17231)

    提出了一种新的自监督学习方法Dyna-LfLH，通过学习幻觉中的动态环境，安全地学习地面机器人在动态环境中灵活导航。

    

    这篇论文提出了一种自监督学习方法，用于安全地学习地面机器人的运动规划器，以在密集且动态的障碍物环境中导航。针对高度混乱、快速移动、难以预测的障碍物，传统的运动规划器可能无法跟上有限的机载计算。对于基于学习的规划器，很难获取高质量的演示以进行模仿学习，同时强化学习在探索过程中由于高碰撞概率而效率低下。为了安全有效地提供训练数据，LfH方法基于过去成功的导航经验在相对简单或完全开放的环境中综合困难的导航环境，但遗憾的是无法解决动态障碍物问题。在我们的新方法Dyna-LfLH中，我们设计并学习了一种新颖的潜在分布和样本。

    arXiv:2403.17231v1 Announce Type: cross  Abstract: This paper presents a self-supervised learning method to safely learn a motion planner for ground robots to navigate environments with dense and dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict obstacles, classical motion planners may not be able to keep up with limited onboard computation. For learning-based planners, high-quality demonstrations are difficult to acquire for imitation learning while reinforcement learning becomes inefficient due to the high probability of collision during exploration. To safely and efficiently provide training data, the Learning from Hallucination (LfH) approaches synthesize difficult navigation environments based on past successful navigation experiences in relatively easy or completely open ones, but unfortunately cannot address dynamic obstacles. In our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and learn a novel latent distribution and sample
    
[^98]: 神经网络中基于梯度的解释的不确定性量化

    Uncertainty Quantification for Gradient-based Explanations in Neural Networks

    [https://arxiv.org/abs/2403.17224](https://arxiv.org/abs/2403.17224)

    本文提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程，通过计算解释分布的变异系数，评估了解释的置信度并确定Guided Backpropagation方法生成的解释具有较低的不确定性。

    

    解释方法有助于理解模型预测的原因。这些方法越来越多地参与模型调试、性能优化，并获得对模型工作原理的洞见。鉴于这些方法的关键应用，衡量这些方法生成的解释的不确定性是至关重要的。在本文中，我们提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程。我们利用这个流程为CIFAR-10、FER+和California Housing数据集生成解释分布。通过计算这些分布的变异系数，我们评估了解释的置信度，并确定使用引导反向传播生成的解释与低不确定性相关。此外，我们计算了修改的像素插入/删除度量来评价……

    arXiv:2403.17224v1 Announce Type: cross  Abstract: Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to ev
    
[^99]: 物体检测和识别的共存，以及未标记物体发现

    Co-Occurring of Object Detection and Identification towards unlabeled object discovery

    [https://arxiv.org/abs/2403.17223](https://arxiv.org/abs/2403.17223)

    通过提出的深度学习方法，可以识别多标签物体类别中基础物体的共存物体，并通过共存矩阵分析生成频繁模式，从而实现未标记物体的发现。

    

    在本文中，我们提出了一种基于深度学习的新方法，用于识别多标签物体类别中与基础物体共存的物体。我们的工作流程由两个阶段组成：在提出模型的第一阶段中，我们检测图像中的所有边界框及其对应的标签，然后在第二阶段进行共存矩阵分析。在共存矩阵分析中，我们基于标签的最大出现次数设定基本类，并构建关联规则并生成频繁模式。这些频繁模式将显示基本类及其对应的共存类。我们在两个公开数据集上进行了实验：Pascal VOC和MS-COCO。实验结果显示我们的方法的有效性。

    arXiv:2403.17223v1 Announce Type: cross  Abstract: In this paper, we propose a novel deep learning based approach for identifying co-occurring objects in conjunction with base objects in multilabel object categories. Nowadays, with the advancement in computer vision based techniques we need to know about co-occurring objects with respect to base object for various purposes. The pipeline of the proposed work is composed of two stages: in the first stage of the proposed model we detect all the bounding boxes present in the image and their corresponding labels, then in the second stage we perform co-occurrence matrix analysis. In co-occurrence matrix analysis, we set base classes based on the maximum occurrences of the labels and build association rules and generate frequent patterns. These frequent patterns will show base classes and their corresponding co-occurring classes. We performed our experiments on two publicly available datasets: Pascal VOC and MS-COCO. The experimental results 
    
[^100]: 大型语言模型在漏洞检测方面的能力综合研究

    A Comprehensive Study of the Capabilities of Large Language Models for Vulnerability Detection

    [https://arxiv.org/abs/2403.17218](https://arxiv.org/abs/2403.17218)

    本研究调查了十一种领先的大型语言模型在漏洞检测中的能力，并评估了它们的性能，为探索LLMs推理能力的极限提供了重要案例研究。

    

    大型语言模型（LLMs）已经展现出在代码生成和其他软件工程任务方面具有巨大潜力。漏洞检测对于维护软件系统的安全、完整性和可信度至关重要。精确的漏洞检测需要对代码进行推理，这使得它成为探索LLMs推理能力极限的良好案例研究。尽管最近的研究已经利用通用提示技术将LLMs应用于漏洞检测，但它们在这一任务中的完整能力以及在解释确定的漏洞时所犯的错误类型仍不清楚。在本文中，我们调查了十一种领先的在代码生成方面处于最前沿且通常用作编码助手的LLMs，并评估了它们在漏洞检测方面的能力。我们系统地搜索了效果最佳的提示，结合了诸如上下文学习和链式学习等技术。

    arXiv:2403.17218v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential for code generation and other software engineering tasks. Vulnerability detection is of crucial importance to maintaining the security, integrity, and trustworthiness of software systems. Precise vulnerability detection requires reasoning about the code, making it a good case study for exploring the limits of LLMs' reasoning capabilities. Although recent work has applied LLMs to vulnerability detection using generic prompting techniques, their full capabilities for this task and the types of errors they make when explaining identified vulnerabilities remain unclear.   In this paper, we surveyed eleven LLMs that are state-of-the-art in code generation and commonly used as coding assistants, and evaluated their capabilities for vulnerability detection. We systematically searched for the best-performing prompts, incorporating techniques such as in-context learning and chain-of
    
[^101]: 解释不确定性的合理性检查

    Sanity Checks for Explanation Uncertainty

    [https://arxiv.org/abs/2403.17212](https://arxiv.org/abs/2403.17212)

    本文提出了解释不确定性方法的合理性检查，可以快速测试不确定性和解释方法的组合。

    

    机器学习模型的解释可能难以解释或出现错误。 将解释方法与不确定性估计方法相结合会产生解释不确定性。 评估解释不确定性是困难的。 本文提出了解释不确定性方法的合理性检查，其中针对带有不确定性的解释定义了权重和数据随机化测试，允许对不确定性和解释方法的组合进行快速测试。 我们在CIFAR10和加利福尼亚房屋数据集上实验证明这些测试的有效性和有效性，注意到Ensemble似乎在Guided Backpropagation，Integrated Gradients和LIME解释上一致通过了这两项测试。

    arXiv:2403.17212v1 Announce Type: cross  Abstract: Explanations for machine learning models can be hard to interpret or be wrong. Combining an explanation method with an uncertainty estimation method produces explanation uncertainty. Evaluating explanation uncertainty is difficult. In this paper we propose sanity checks for uncertainty explanation methods, where a weight and data randomization tests are defined for explanations with uncertainty, allowing for quick tests to combinations of uncertainty and explanation methods. We experimentally show the validity and effectiveness of these tests on the CIFAR10 and California Housing datasets, noting that Ensembles seem to consistently pass both tests with Guided Backpropagation, Integrated Gradients, and LIME explanations.
    
[^102]: CADGL: 上下文感知深度图学习用于预测药物-药物相互作用

    CADGL: Context-Aware Deep Graph Learning for Predicting Drug-Drug Interactions

    [https://arxiv.org/abs/2403.17210](https://arxiv.org/abs/2403.17210)

    通过CADGL框架，利用上下文感知深度图学习来预测药物-药物相互作用，解决了现有DDI预测模型在泛化、特征提取和现实应用方面的挑战

    

    药物-药物相互作用（DDIs）的研究是药物开发过程中的一个关键元素。DDIs发生在一个药物的性质受其他药物包含的影响时。检测有利的DDIs有可能为在实际设置中应用的创新药物的创造和推进铺平道路。然而，现有的DDI预测模型在极端情况下的泛化、稳健特征提取以及现实应用可能性方面持续面临挑战。我们旨在通过利用上下文感知深度图学习的有效性，引入一种名为CADGL的新颖框架来应对这些挑战。基于定制的变分图自编码器（VGAE），我们利用两个上下文预处理器从两个不同视角：局部邻域和分子上下文，在异质图结构中提取特征，捕获关键的结构和生理化学信息。

    arXiv:2403.17210v1 Announce Type: cross  Abstract: Examining Drug-Drug Interactions (DDIs) is a pivotal element in the process of drug development. DDIs occur when one drug's properties are affected by the inclusion of other drugs. Detecting favorable DDIs has the potential to pave the way for creating and advancing innovative medications applicable in practical settings. However, existing DDI prediction models continue to face challenges related to generalization in extreme cases, robust feature extraction, and real-life application possibilities. We aim to address these challenges by leveraging the effectiveness of context-aware deep graph learning by introducing a novel framework named CADGL. Based on a customized variational graph autoencoder (VGAE), we capture critical structural and physio-chemical information using two context preprocessors for feature extraction from two different perspectives: local neighborhood and molecular context, in a heterogeneous graphical structure. Ou
    
[^103]: 信号处理与机器学习交集研究：一个以用例驱动的分析方法

    On the Intersection of Signal Processing and Machine Learning: A Use Case-Driven Analysis Approach

    [https://arxiv.org/abs/2403.17181](https://arxiv.org/abs/2403.17181)

    本文介绍了一个综合文章方法，旨在弥合信号处理和机器学习之间的知识鸿沟，并提供了针对广泛读者群体的信号处理基础教程。

    

    最近传感、测量和计算技术的进步显著扩展了基于信号的应用潜力，利用信号处理和机器学习（ML）之间的协同作用来提高性能和可靠性。这种融合代表了信号系统演变中的一个关键点，突显了弥合这两个跨学科领域之间现有知识差距的必要性。尽管现有文献中有许多尝试弥合这一差距的方法，但大多数限于特定应用，并且主要侧重于特征提取，通常假设读者在信号处理方面有广泛的先验知识。这种假设为广泛读者群体提出了显著障碍。为了解决这些挑战，本文采取了综合论文的方式。它从信号处理基础的详细教程开始，为读者提供必要的背景知识。

    arXiv:2403.17181v1 Announce Type: cross  Abstract: Recent advancements in sensing, measurement, and computing technologies have significantly expanded the potential for signal-based applications, leveraging the synergy between signal processing and Machine Learning (ML) to improve both performance and reliability. This fusion represents a critical point in the evolution of signal-based systems, highlighting the need to bridge the existing knowledge gap between these two interdisciplinary fields. Despite many attempts in the existing literature to bridge this gap, most are limited to specific applications and focus mainly on feature extraction, often assuming extensive prior knowledge in signal processing. This assumption creates a significant obstacle for a wide range of readers. To address these challenges, this paper takes an integrated article approach. It begins with a detailed tutorial on the fundamentals of signal processing, providing the reader with the necessary background kno
    
[^104]: 使用深度学习模型进行脑卒中分割：一项比较研究

    Brain Stroke Segmentation Using Deep Learning Models: A Comparative Study

    [https://arxiv.org/abs/2403.17177](https://arxiv.org/abs/2403.17177)

    本研究通过比较深度学习模型在脑卒中分割上的表现，探讨了是否需要高级别设计来获得最佳结果。

    

    脑卒中分割在脑卒中患者的诊断和治疗中发挥着关键作用，通过提供受影响脑区域的空间信息和受损程度。准确分割脑卒中病变是一项具有挑战性的任务，因为传统的手工技术耗时且容易出错。最近，先进的深度模型已被引入用于一般医学图像分割，展示出在特定数据集上评估时超越许多最先进网络的有前景结果。随着视觉Transformer的出现，已经基于它们引入了几种模型，而其他一些则旨在设计基于传统卷积层来提取像Transformer这样的长程依赖的更好模块。是否对所有分割案例都需要这样高级别的设计来实现最佳结果的问题尚未得到解答。在这项研究中，我们选择了四种类型的深度学习模型

    arXiv:2403.17177v1 Announce Type: cross  Abstract: Stroke segmentation plays a crucial role in the diagnosis and treatment of stroke patients by providing spatial information about affected brain regions and the extent of damage. Segmenting stroke lesions accurately is a challenging task, given that conventional manual techniques are time consuming and prone to errors. Recently, advanced deep models have been introduced for general medical image segmentation, demonstrating promising results that surpass many state of the art networks when evaluated on specific datasets. With the advent of the vision Transformers, several models have been introduced based on them, while others have aimed to design better modules based on traditional convolutional layers to extract long-range dependencies like Transformers. The question of whether such high-level designs are necessary for all segmentation cases to achieve the best results remains unanswered. In this study, we selected four types of deep 
    
[^105]: 信念样本就是社会学习所需要的一切

    Belief Samples Are All You Need For Social Learning

    [https://arxiv.org/abs/2403.17174](https://arxiv.org/abs/2403.17174)

    本文研究了社会学习中的信念样本应用，提出了一种基于有限通信和资源限制的新型学习方式。

    

    在本文中，我们考虑了社会学习问题，其中一群嵌入在社交网络中的代理对学习世界的潜在状态感兴趣。代理人具有不完整、嘈杂和异质的信息来源，为他们提供了关于世界潜在状态的重复私人观察。代理人可以通过采取对同行可观察的行动来与同行分享他们的学习经验，这些行动的价值来自一个有限可行状态集。这些行动可以被解释为代理人可能形成和更新关于真实世界状态的信念的样本。分享样本，而不是完整的信念，是受到代理人特别是在大型人群中可用的通信、认知和信息处理资源有限的启发。先前的工作(Salhab等人)提出了这样一个问题：如果只允许代理人沟通，那么是否仍然可以实现概率为一的学习。

    arXiv:2403.17174v1 Announce Type: new  Abstract: In this paper, we consider the problem of social learning, where a group of agents embedded in a social network are interested in learning an underlying state of the world. Agents have incomplete, noisy, and heterogeneous sources of information, providing them with recurring private observations of the underlying state of the world. Agents can share their learning experience with their peers by taking actions observable to them, with values from a finite feasible set of states. Actions can be interpreted as samples from the beliefs which agents may form and update on what the true state of the world is. Sharing samples, in place of full beliefs, is motivated by the limited communication, cognitive, and information-processing resources available to agents especially in large populations. Previous work (Salhab et al.) poses the question as to whether learning with probability one is still achievable if agents are only allowed to communicat
    
[^106]: 多目标质量多样性用于晶体结构预测

    Multi-Objective Quality-Diversity for Crystal Structure Prediction

    [https://arxiv.org/abs/2403.17164](https://arxiv.org/abs/2403.17164)

    本研究利用质量多样性算法为晶体结构预测打开了一条新途径，旨在发现具有多样特征的高性能解决方案集合，可以优化晶体结构稳定性以及其他目标如磁性或热电效率。

    

    晶体结构在从电池到太阳能电池等各个领域中都是不可或缺的，针对其原子配置预测性能已经有了大量研究。然而，现有的晶体结构预测方法侧重于识别能量函数全局最小值处的最稳定解决方案，而忽略了那些可能位于相邻局部极小值处、具有不同材料特性（如电导率或抗变形性）的其他有趣材料。相比之下，质量多样性算法为晶体结构预测提供了一个有前途的途径，因为它旨在找到具有多样特征的高性能解决方案集合。然而，优化晶体结构稳定性以及其他目标（如磁性或热电效率）也可能是有价值的。因此，在这项研究中，我们利用......

    arXiv:2403.17164v1 Announce Type: cross  Abstract: Crystal structures are indispensable across various domains, from batteries to solar cells, and extensive research has been dedicated to predicting their properties based on their atomic configurations. However, prevailing Crystal Structure Prediction methods focus on identifying the most stable solutions that lie at the global minimum of the energy function. This approach overlooks other potentially interesting materials that lie in neighbouring local minima and have different material properties such as conductivity or resistance to deformation. By contrast, Quality-Diversity algorithms provide a promising avenue for Crystal Structure Prediction as they aim to find a collection of high-performing solutions that have diverse characteristics. However, it may also be valuable to optimise for the stability of crystal structures alongside other objectives such as magnetism or thermoelectric efficiency. Therefore, in this work, we harness 
    
[^107]: 少即是多 - 关于稀疏化在Transformers和图神经网络在TSP问题中的重要性

    Less Is More - On the Importance of Sparsification for Transformers and Graph Neural Networks for TSP

    [https://arxiv.org/abs/2403.17159](https://arxiv.org/abs/2403.17159)

    我们提出了一种数据预处理方法，通过稀疏化TSP图表示和注意力掩码，使编码器集中于TSP实例的关键部分，同时允许信息在所有节点之间自由流动。

    

    大多数最近研究处理旅行商问题（TSP）等路由问题的机器学习方法使用基于transformer或图神经网络（GNN）的编码器架构。然而，其中许多研究直接应用这些编码器，允许它们在整个TSP实例上聚合信息。相反，我们提出了一种数据预处理方法，使编码器仅关注TSP实例的最相关部分。具体来说，我们为传递给GNN的TSP图表示提出了图稀疏化，并为传递给transformers的TSP实例提出了注意力屏蔽，其中mask对应于稀疏TSP图表示的邻接矩阵。此外，我们提出了不同稀疏化级别的集合，使模型能够专注于最有前途的部分，同时还允许TSP实例的所有节点之间的信息流动。在实验研究中，我们展示了

    arXiv:2403.17159v1 Announce Type: cross  Abstract: Most of the recent studies tackling routing problems like the Traveling Salesman Problem (TSP) with machine learning use a transformer or Graph Neural Network (GNN) based encoder architecture. However, many of them apply these encoders naively by allowing them to aggregate information over the whole TSP instances. We, on the other hand, propose a data preprocessing method that allows the encoders to focus on the most relevant parts of the TSP instances only. In particular, we propose graph sparsification for TSP graph representations passed to GNNs and attention masking for TSP instances passed to transformers where the masks correspond to the adjacency matrices of the sparse TSP graph representations. Furthermore, we propose ensembles of different sparsification levels allowing models to focus on the most promising parts while also allowing information flow between all nodes of a TSP instance. In the experimental studies, we show that
    
[^108]: 引导远程监督用于多语言关系抽取数据：适应新语言

    Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language

    [https://arxiv.org/abs/2403.17143](https://arxiv.org/abs/2403.17143)

    本文应用引导远程监督方法，为德语创建了最大的传记关系抽取数据集，同时发布了手动标注的评估数据集。

    

    摘要：关系抽取对于在数字人文学和相关学科背景下提取和理解传记信息至关重要。社区对构建能够训练机器学习模型提取关系的数据集越来越感兴趣。然而，标注这样的数据集可能既昂贵又耗时，而且仅限于英语。本文应用了引导式远程监督方法，为德语创建了一个大型传记关系抽取数据集。我们的数据集包含了超过80,000个实例，涵盖了九种关系类型，是最大的德语传记关系抽取数据集。我们还创建了一个手动标注的数据集，包含2000个实例用于评估模型，并与利用引导式远程监督方法编制的数据集一起发布。我们在自动生成的数据集上训练了几种最先进的机器学习模型，并将其发布。

    arXiv:2403.17143v1 Announce Type: new  Abstract: Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as 
    
[^109]: 利用随机浅层ReLU网络来进行逼近及其在模型参考自适应控制中的应用

    Approximation with Random Shallow ReLU Networks with Applications to Model Reference Adaptive Control

    [https://arxiv.org/abs/2403.17142](https://arxiv.org/abs/2403.17142)

    针对足够光滑的函数，本文证明使用随机生成的权重和偏置的ReLU网络可以在高概率下实现$O(m^{-1/2})$的$L_{\infty}$误差。

    

    神经网络常用于非线性系统的自适应控制以及相关的强化学习方法。一种常见的结构是使用具有单个隐藏层的神经网络（即浅层网络），其中权重和偏置提前固定，只有输出层被训练。尽管经典结果表明，存在这种类型的神经网络可以逼近有界区域上的任意连续函数，但这些结果是非构造性的，实际使用的网络没有逼近保证。因此，用于神经网络控制的逼近性质是假设的，而不是被证明的。本文旨在通过展示对于足够光滑的函数，具有随机生成的权重和偏置的ReLU网络可以在高概率下实现$O(m^{-1/2})$的$L_{\infty}$误差，其中$m$是神经元的数量。

    arXiv:2403.17142v1 Announce Type: cross  Abstract: Neural networks are regularly employed in adaptive control of nonlinear systems and related methods o reinforcement learning. A common architecture uses a neural network with a single hidden layer (i.e. a shallow network), in which the weights and biases are fixed in advance and only the output layer is trained. While classical results show that there exist neural networks of this type that can approximate arbitrary continuous functions over bounded regions, they are non-constructive, and the networks used in practice have no approximation guarantees. Thus, the approximation properties required for control with neural networks are assumed, rather than proved. In this paper, we aim to fill this gap by showing that for sufficiently smooth functions, ReLU networks with randomly generated weights and biases achieve $L_{\infty}$ error of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It suffices to generate the wei
    
[^110]: 探索癌症临床试验资格分类器在疾病间的泛化性能

    Exploring the Generalization of Cancer Clinical Trial Eligibility Classifiers Across Diseases

    [https://arxiv.org/abs/2403.17135](https://arxiv.org/abs/2403.17135)

    本研究评估了癌症临床试验资格分类器在不同疾病间的泛化性能，发现在广泛癌症数据集上训练的模型可以处理非癌症试验的标准，但在某些情况下仍然存在困难。

    

    临床试验对于医学研究至关重要，自然语言处理可增强其成功，在招募方面有应用。本研究旨在评估资格分类在广泛临床试验范围内的泛化性能。从阶段3癌症试验开始，标记有七种资格排除条件，然后确定模型在非癌症和非阶段3试验中的泛化效果。为评估此问题，我们整理了五种试验类型的资格标准数据：（1）其他阶段3癌症试验，（2）癌症阶段1和2试验，（3）心脏病试验，（4）2型糖尿病试验和（5）任何疾病的观察性试验，跨七种排除类型共涵盖了2,490个已标注的资格标准。我们的结果显示，在广泛癌症数据集上训练的模型可以有效处理非癌症试验中常见的标准，如自身免疫疾病。然而，它们在处理诸如缺乏标准等标准时会遇到困难。

    arXiv:2403.17135v1 Announce Type: new  Abstract: Clinical trials are pivotal in medical research, and NLP can enhance their success, with application in recruitment. This study aims to evaluate the generalizability of eligibility classification across a broad spectrum of clinical trials. Starting with phase 3 cancer trials, annotated with seven eligibility exclusions, then to determine how well models can generalize to non-cancer and non-phase 3 trials. To assess this, we have compiled eligibility criteria data for five types of trials: (1) additional phase 3 cancer trials, (2) phase 1 and 2 cancer trials, (3) heart disease trials, (4) type 2 diabetes trials, and (5) observational trials for any disease, comprising 2,490 annotated eligibility criteria across seven exclusion types. Our results show that models trained on the extensive cancer dataset can effectively handle criteria commonly found in non-cancer trials, such as autoimmune diseases. However, they struggle with criteria disp
    
[^111]: 探索基于原型的软标签数据蒸馏在不平衡数据分类中的潜力

    Exploring the potential of prototype-based soft-labels data distillation for imbalanced data classification

    [https://arxiv.org/abs/2403.17130](https://arxiv.org/abs/2403.17130)

    本文探索了一种先前在少于一次学习中提出的简单蒸馏技术在原型软标签蒸馏中的潜力，旨在通过在蒸馏过程中整合优化步骤，进一步提高分类准确性。

    

    数据集蒸馏旨在通过少量人工生成的数据项合成一个数据集，当这些数据被用作训练数据时，能够重现或逼近一个机器学习（ML）模型，就好像它是在整个原始数据集上训练的一样。因此，数据蒸馏方法通常与特定的ML算法有关。尽管最近的文献主要涉及在神经网络模型背景下对大量图像的蒸馏，但表格数据的蒸馏代表性较低，主要集中在理论视角上。本文探讨了一种在少于一次学习中提出的简单蒸馏技术在原型软标签蒸馏中的潜力。主要目标是通过在蒸馏过程中整合优化步骤，推动基于原型的软标签蒸馏在分类准确性方面的性能进一步提升。该分析是在真实数据集上进行的。

    arXiv:2403.17130v1 Announce Type: cross  Abstract: Dataset distillation aims at synthesizing a dataset by a small number of artificially generated data items, which, when used as training data, reproduce or approximate a machine learning (ML) model as if it were trained on the entire original dataset. Consequently, data distillation methods are usually tied to a specific ML algorithm. While recent literature deals mainly with distillation of large collections of images in the context of neural network models, tabular data distillation is much less represented and mainly focused on a theoretical perspective. The current paper explores the potential of a simple distillation technique previously proposed in the context of Less-than-one shot learning. The main goal is to push further the performance of prototype-based soft-labels distillation in terms of classification accuracy, by integrating optimization steps in the distillation process. The analysis is performed on real-world data sets
    
[^112]: 将语言计划基于演示通过反事实干扰进行落实

    Grounding Language Plans in Demonstrations Through Counterfactual Perturbations

    [https://arxiv.org/abs/2403.17124](https://arxiv.org/abs/2403.17124)

    这项工作通过使用LLMs来指导多步演示中隐含的任务结构和约束的搜索，以及通过反事实干扰获得更广泛的演示状态空间覆盖。

    

    将大型语言模型的常识推理基于物理领域落实在体现智能的人工智能中仍然是一个至关重要但尚未解决的问题。相较于先前的工作专注于直接利用LLMs在符号空间内规划，这项工作使用LLMs指导任务结构的搜索，隐含在多步演示中的约束。具体而言，我们借鉴了操纵规划文献中的模式族的概念，它按照特定运动约束将机器人配置分组，作为LLM高级语言表示和机器人低级物理轨迹之间的抽象层。通过用合成干扰重新播放少量人类演示，我们可以覆盖演示的状态空间，并额外生成成功执行以及未完成任务的反事实情况。我们的基于解释的学习框架训练了一个端到端可微分神经网络。

    arXiv:2403.17124v1 Announce Type: cross  Abstract: Grounding the common-sense reasoning of Large Language Models in physical domains remains a pivotal yet unsolved problem for embodied AI. Whereas prior works have focused on leveraging LLMs directly for planning in symbolic spaces, this work uses LLMs to guide the search of task structures and constraints implicit in multi-step demonstrations. Specifically, we borrow from manipulation planning literature the concept of mode families, which group robot configurations by specific motion constraints, to serve as an abstraction layer between the high-level language representations of an LLM and the low-level physical trajectories of a robot. By replaying a few human demonstrations with synthetic perturbations, we generate coverage over the demonstrations' state space with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains an end-to-end differentiable neural networ
    
[^113]: 随机梯度 Langevin 反遗忘

    Stochastic Gradient Langevin Unlearning

    [https://arxiv.org/abs/2403.17105](https://arxiv.org/abs/2403.17105)

    本工作提出了随机梯度 Langevin 反遗忘方法，为近似反遗忘问题提供了隐私保障，并展示了小批次梯度更新相较于全批次的优越性能。

    

    “被遗忘的权利”是用户数据隐私的法律所确保的越来越重要。机器反遗忘旨在高效地消除已训练模型参数上某些数据点的影响，使其近似于从头开始重新训练模型。本研究提出了随机梯度 Langevin 反遗忘，这是第一个基于带有隐私保障的噪声随机梯度下降（SGD）的反遗忘框架，适用于凸性假设下的近似反遗忘问题。我们的结果表明，与全批次对应方法相比，小批次梯度更新在隐私复杂度权衡方面提供了更好的性能。我们的反遗忘方法具有诸多算法优势，包括与重新训练相比的复杂度节省，以及支持顺序和批量反遗忘。为了检验我们方法的隐私-效用-复杂度权衡，我们在基准数据集上进行了实验比较。

    arXiv:2403.17105v1 Announce Type: new  Abstract: ``The right to be forgotten'' ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. This work proposes stochastic gradient Langevin unlearning, the first unlearning framework based on noisy stochastic gradient descent (SGD) with privacy guarantees for approximate unlearning problems under convexity assumption. Our results show that mini-batch gradient updates provide a superior privacy-complexity trade-off compared to the full-batch counterpart. There are numerous algorithmic benefits of our unlearning approach, including complexity saving compared to retraining, and supporting sequential and batch unlearning. To examine the privacy-utility-complexity trade-off of our method, we conduct experiments on benchmark datasets compared 
    
[^114]: SynFog: 一种基于端到端成像模拟的逼真合成雾数据集，用于推进自动驾驶中真实世界去雾技术的研究

    SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving

    [https://arxiv.org/abs/2403.17094](https://arxiv.org/abs/2403.17094)

    介绍了一种新的SynFog合成雾数据集，利用端到端仿真管道生成逼真的雾图像，为自动驾驶中的真实世界去雾技术研究提供了先进工具

    

    为了推进基于学习的去雾算法研究，已经开发了各种合成雾数据集。然而，使用大气散射模型（ASM）或实时渲染引擎创建的现有数据集通常难以生成准确模拟实际成像过程的逼真雾图像，这种限制妨碍了模型从合成数据到真实数据的有效泛化。本文介绍了一种端到端的仿真管道，旨在生成逼真的雾图像。该管道全面考虑整个基于物理的雾景成像过程，与现实世界的图像捕获方法密切相关。基于该管道，我们提出了一种名为SynFog的新合成雾数据集，该数据集具有天空光和主动照明条件，以及三个雾浓度级别。实验结果表明，在SynFog上训练的模型表现出更优越的性能。

    arXiv:2403.17094v1 Announce Type: cross  Abstract: To advance research in learning-based defogging algorithms, various synthetic fog datasets have been developed. However, existing datasets created using the Atmospheric Scattering Model (ASM) or real-time rendering engines often struggle to produce photo-realistic foggy images that accurately mimic the actual imaging process. This limitation hinders the effective generalization of models from synthetic to real data. In this paper, we introduce an end-to-end simulation pipeline designed to generate photo-realistic foggy images. This pipeline comprehensively considers the entire physically-based foggy scene imaging process, closely aligning with real-world image capture methods. Based on this pipeline, we present a new synthetic fog dataset named SynFog, which features both sky light and active lighting conditions, as well as three levels of fog density. Experimental results demonstrate that models trained on SynFog exhibit superior perf
    
[^115]: 通过零信任架构增强无人机安全：高级深度学习和可解释人工智能分析

    Enhancing UAV Security Through Zero Trust Architecture: An Advanced Deep Learning and Explainable AI Analysis

    [https://arxiv.org/abs/2403.17093](https://arxiv.org/abs/2403.17093)

    本研究提出了通过深度学习和可解释人工智能分析，在无人机安全领域实施零信任架构以提高安全性的必要性，并实现了84.59\%的无人机识别准确率。

    

    在无人机领域这个充满活力和不断变化的领域中，保证安全措施具有弹性和清晰性至关重要。本研究强调了实施零信任架构（ZTA）以增强无人机安全的必要性，因此摆脱了可能暴露漏洞的传统外围防御。零信任架构（ZTA）范式要求对所有网络实体和通信进行严格和持续的认证过程。我们的方法在检测和识别无人机的准确率为84.59\%。这是通过在深度学习框架内利用射频信号实现的，这是一种独特的方法。精确的识别在零信任架构（ZTA）中至关重要，因为它决定了网络访问。此外，使用可解释人工智能（XAI）工具，如SHapley Additive exPla

    arXiv:2403.17093v1 Announce Type: new  Abstract: In the dynamic and ever-changing domain of Unmanned Aerial Vehicles (UAVs), the utmost importance lies in guaranteeing resilient and lucid security measures. This study highlights the necessity of implementing a Zero Trust Architecture (ZTA) to enhance the security of unmanned aerial vehicles (UAVs), hence departing from conventional perimeter defences that may expose vulnerabilities. The Zero Trust Architecture (ZTA) paradigm requires a rigorous and continuous process of authenticating all network entities and communications. The accuracy of our methodology in detecting and identifying unmanned aerial vehicles (UAVs) is 84.59\%. This is achieved by utilizing Radio Frequency (RF) signals within a Deep Learning framework, a unique method. Precise identification is crucial in Zero Trust Architecture (ZTA), as it determines network access. In addition, the use of eXplainable Artificial Intelligence (XAI) tools such as SHapley Additive exPla
    
[^116]: 脱机强化学习：状态聚合和轨迹数据的作用

    Offline Reinforcement Learning: Role of State Aggregation and Trajectory Data

    [https://arxiv.org/abs/2403.17091](https://arxiv.org/abs/2403.17091)

    研究提出了对于脱机策略评估任务，样本复杂度受聚合马尔科夫转换模型中的浓缩系数控制，而不是原始MDP中的系数。

    

    我们重新审视了具有价值函数可实现性但不具有贝尔曼完备性的脱机强化学习问题。我们对脱机策略评估的样本复杂度受聚合马尔科夫转换模型中的浓缩系数控制的发现，以及提供了仅具有价值函数可实现性的脱机策略评估的相当完整的图景。我们的主要发现有三个：1）脱机策略评估的样本复杂度由聚合的马尔科夫转换模型中的集中系数决定，这个系数由函数类和脱机数据分布共同确定，而不是原始MDP中的系数。

    arXiv:2403.17091v1 Announce Type: cross  Abstract: We revisit the problem of offline reinforcement learning with value function realizability but without Bellman completeness. Previous work by Xie and Jiang (2021) and Foster et al. (2022) left open the question whether a bounded concentrability coefficient along with trajectory-based offline data admits a polynomial sample complexity. In this work, we provide a negative answer to this question for the task of offline policy evaluation. In addition to addressing this question, we provide a rather complete picture for offline policy evaluation with only value function realizability. Our primary findings are threefold: 1) The sample complexity of offline policy evaluation is governed by the concentrability coefficient in an aggregated Markov Transition Model jointly determined by the function class and the offline data distribution, rather than that in the original MDP. This unifies and generalizes the ideas of Xie and Jiang (2021) and Fo
    
[^117]: 数据集修剪在图像超分辨率中的研究

    A Study in Dataset Pruning for Image Super-Resolution

    [https://arxiv.org/abs/2403.17083](https://arxiv.org/abs/2403.17083)

    本研究针对图像超分辨率中数据集训练资源需求大的问题，提出了一种数据集修剪的解决方案，通过基于损失值的选择，将训练集缩减至原始数据集的50%，取得了令人满意的结果。

    

    在图像超分辨率（SR）中，依赖大型数据集进行训练是一把双刃剑。尽管提供丰富的训练素材，但也需要大量的计算和存储资源。在本工作中，我们分析了数据集修剪作为应对这些挑战的解决方案。我们引入了一种新颖的方法，将数据集缩减到基于其损失值而选择的一组核心训练样本。通过仅将训练重点放在原始数据集的50%上，特别是那些损失值最高的样本上，我们实现了与或甚至超过整个数据集训练的结果相媲美的效果。有趣的是，我们的分析显示，具有最高损失值的前5％样本会对训练过程产生负面影响。排除这些样本并调整选择以偏好更容易的样本进一步提高了训练结果。我们的工作开辟了新的研究方向。

    arXiv:2403.17083v1 Announce Type: cross  Abstract: In image Super-Resolution (SR), relying on large datasets for training is a double-edged sword. While offering rich training material, they also demand substantial computational and storage resources. In this work, we analyze dataset pruning as a solution to these challenges. We introduce a novel approach that reduces a dataset to a core-set of training samples, selected based on their loss values as determined by a simple pre-trained SR model. By focusing the training on just 50% of the original dataset, specifically on the samples characterized by the highest loss values, we achieve results comparable to or even surpassing those obtained from training on the entire dataset. Interestingly, our analysis reveals that the top 5% of samples with the highest loss values negatively affect the training process. Excluding these samples and adjusting the selection to favor easier samples further enhances training outcomes. Our work opens new p
    
[^118]: 区块链数据上的机器学习：一项系统性映射研究

    Machine Learning on Blockchain Data: A Systematic Mapping Study

    [https://arxiv.org/abs/2403.17081](https://arxiv.org/abs/2403.17081)

    本论文是一项系统性映射研究，旨在全面回顾机器学习应用于区块链数据的最新研究现状，有助于确定未来研究中应该重点关注的领域。

    

    arXiv:2403.17081v1 公告类型: 跨领域 摘要：背景：区块链技术在文献和实践中引起了越来越多的关注。区块链技术产生了大量数据，因此成为机器学习（ML）感兴趣的话题。 目标：本文的目标是全面回顾应用于区块链数据的机器学习的最新研究现状。该工作旨在系统地识别、分析和分类应用于区块链数据的文献。 这将使我们能够发现未来研究中应该投入更多努力的领域。 方法：进行了一项系统性映射研究以识别相关文献。最终，选择了159篇文章，并根据各种维度进行了分类，具体包括领域用例、区块链、数据和机器学习模型。 结果：大多数论文（49.7%）属于异常用例。 比特币（47.2%

    arXiv:2403.17081v1 Announce Type: cross  Abstract: Context: Blockchain technology has drawn growing attention in the literature and in practice. Blockchain technology generates considerable amounts of data and has thus been a topic of interest for Machine Learning (ML).   Objective: The objective of this paper is to provide a comprehensive review of the state of the art on machine learning applied to blockchain data. This work aims to systematically identify, analyze, and classify the literature on ML applied to blockchain data. This will allow us to discover the fields where more effort should be placed in future research.   Method: A systematic mapping study has been conducted to identify the relevant literature. Ultimately, 159 articles were selected and classified according to various dimensions, specifically, the domain use case, the blockchain, the data, and the machine learning models.   Results: The majority of the papers (49.7%) fall within the Anomaly use case. Bitcoin (47.2%
    
[^119]: 在T2I模型中通过识别语义方向实现连续、主题特定的属性控制

    Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions

    [https://arxiv.org/abs/2403.17064](https://arxiv.org/abs/2403.17064)

    通过识别CLIP文本嵌入中的语义方向，实现了文本到图像模型中对高级属性的细粒度主题特定控制。

    

    近年来，文本到图像（T2I）扩散模型的进展显著提高了生成图像的质量。然而，由于自然语言提示的限制（例如“人”和“老年人”之间不存在连续的中间描述的集合），实现对属性的细粒度控制仍然是一个挑战。尽管引入了许多方法来增强模型或生成过程以实现这种控制，但不需要固定参考图像的方法仅限于启用全局细粒度属性表达控制或仅限于特定主题的粗粒度属性表达控制，而不能同时兼顾两者。我们展示了在常用的基于标记级别的CLIP文本嵌入中存在可实现文本到图像模型中高级属性的细粒度主题特定控制的方向。基于这一观察，我们引入了一种有效的方法。

    arXiv:2403.17064v1 Announce Type: cross  Abstract: In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient op
    
[^120]: 可证实鲁棒的基于得分的扩散后验采样用于即插即用图像重建

    Provably Robust Score-Based Diffusion Posterior Sampling for Plug-and-Play Image Reconstruction

    [https://arxiv.org/abs/2403.17042](https://arxiv.org/abs/2403.17042)

    开发了一个算法框架，用于将基于得分的扩散模型作为通用非线性逆的表达数据先验。

    

    在科学和工程中的许多任务中，目标是从已知描述某种感知或成像模式的已知前向模型收集的少量测量中推断未知图像。由于资源限制，这个任务通常非常不适合，这就需要采纳表达丰富的先验信息来规范解空间。由于其令人印象深刻的经验成功，基于分数的扩散模型已经成为图像重建中一个具有吸引力的表达先验的候选者。为了一次性容纳多样的任务，开发将图像先验分布的无条件评分函数与灵活的前向模型选择相结合的高效、一致和鲁棒算法非常重要。这项工作开发了一个算法框架，用于将基于得分的扩散模型作为通用非线性逆的表达数据先验。

    arXiv:2403.17042v1 Announce Type: cross  Abstract: In a great number of tasks in science and engineering, the goal is to infer an unknown image from a small number of measurements collected from a known forward model describing certain sensing or imaging modality. Due to resource constraints, this task is often extremely ill-posed, which necessitates the adoption of expressive prior information to regularize the solution space. Score-based diffusion models, due to its impressive empirical success, have emerged as an appealing candidate of an expressive prior in image reconstruction. In order to accommodate diverse tasks at once, it is of great interest to develop efficient, consistent and robust algorithms that incorporate {\em unconditional} score functions of an image prior distribution in conjunction with flexible choices of forward models.   This work develops an algorithmic framework for employing score-based diffusion models as an expressive data prior in general nonlinear invers
    
[^121]: 用注意力驱动的脉冲神经网络增强图表示学习

    Enhancing Graph Representation Learning with Attention-Driven Spiking Neural Networks

    [https://arxiv.org/abs/2403.17040](https://arxiv.org/abs/2403.17040)

    本研究提出了一种将注意力机制与脉冲神经网络相结合的新方法，用于改善图表示学习，实验结果显示其在基准数据集上表现出可比较的性能。

    

    图表示学习已经成为机器学习和数据挖掘中的关键任务，因为它具有对社交网络、化合物和生物系统等复杂结构进行建模的潜力。最近，脉冲神经网络（SNNs）作为传统神经网络用于图学习任务的一种有前途的替代方案而出现，因为它们能够高效地编码和处理时间和空间信息。在本文中，我们提出了一种将注意力机制与SNNs结合以改善图表示学习的新方法。具体来说，我们引入了一种用于SNN的注意力机制，可以在学习过程中有选择地关注图中重要的节点和相应特征。我们在几个基准数据集上评估了我们提出的方法，并展示了它与现有图学习技术相比具有可比较的性能。

    arXiv:2403.17040v1 Announce Type: new  Abstract: Graph representation learning has become a crucial task in machine learning and data mining due to its potential for modeling complex structures such as social networks, chemical compounds, and biological systems. Spiking neural networks (SNNs) have recently emerged as a promising alternative to traditional neural networks for graph learning tasks, benefiting from their ability to efficiently encode and process temporal and spatial information. In this paper, we propose a novel approach that integrates attention mechanisms with SNNs to improve graph representation learning. Specifically, we introduce an attention mechanism for SNN that can selectively focus on important nodes and corresponding features in a graph during the learning process. We evaluate our proposed method on several benchmark datasets and show that it achieves comparable performance compared to existing graph learning techniques.
    
[^122]: 基于混合机器学习方法的随机参数降阶模型

    Stochastic parameter reduced-order model based on hybrid machine learning approaches

    [https://arxiv.org/abs/2403.17032](https://arxiv.org/abs/2403.17032)

    本文基于混合机器学习方法，构建了一个Convolutional Autoencoder-Reservoir Computing-Normalizing Flow算法框架，用于降阶建模，能够高效描述自然现象的关键动态和统计特征。

    

    建立自然现象中复杂系统的适当数学模型，不仅有助于加深我们对自然的理解，而且可以用于状态估计和预测。然而，自然现象的极端复杂性使得开发全阶模型（FOMs）并将其应用于研究许多感兴趣的量变得极具挑战性。相比之下，适当的降阶模型（ROMs）因其高计算效率和描述自然现象关键动态和统计特征的能力而备受青睐。以粘性Burgers方程为例，本文构建了一个卷积自编码器-储备计算-正则化流算法框架，其中卷积自编码器用于建立潜在空间表示，而储备计算-正则化流框架则用于表征潜在状态变量的演化。

    arXiv:2403.17032v1 Announce Type: new  Abstract: Establishing appropriate mathematical models for complex systems in natural phenomena not only helps deepen our understanding of nature but can also be used for state estimation and prediction. However, the extreme complexity of natural phenomena makes it extremely challenging to develop full-order models (FOMs) and apply them to studying many quantities of interest. In contrast, appropriate reduced-order models (ROMs) are favored due to their high computational efficiency and ability to describe the key dynamics and statistical characteristics of natural phenomena. Taking the viscous Burgers equation as an example, this paper constructs a Convolutional Autoencoder-Reservoir Computing-Normalizing Flow algorithm framework, where the Convolutional Autoencoder is used to construct latent space representations, and the Reservoir Computing-Normalizing Flow framework is used to characterize the evolution of latent state variables. In this way,
    
[^123]: 带有PPO的RLHF的N+实现细节：TL;DR总结案例研究

    The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization

    [https://arxiv.org/abs/2403.17031](https://arxiv.org/abs/2403.17031)

    该研究复制了OpenAI在TL;DR总结中报道的RLHF强化学习规模行为，并展示出随着模型规模增大，RLHF训练的Pythia模型在响应质量上取得了显著提高。

    

    这项工作是第一个公开复制了OpenAI在TL;DR总结工作中报告的强化学习来自人类反馈（RLHF）的规模行为。我们从头开始创建了一个RLHF流水线，列举了超过20个关键的实现细节，并在复制过程中分享了关键见解。我们训练的RLHF Pythia模型表现出随着模型规模增长而增加的响应质量的显著提高，我们的28亿、69亿模型优于OpenAI发布的13亿检查点。我们公开发布训练好的模型检查点和代码，以促进进一步研究并加快该领域的进展。

    arXiv:2403.17031v1 Announce Type: new  Abstract: This work is the first to openly reproduce the Reinforcement Learning from Human Feedback (RLHF) scaling behaviors reported in OpenAI's seminal TL;DR summarization work. We create an RLHF pipeline from scratch, enumerate over 20 key implementation details, and share key insights during the reproduction. Our RLHF-trained Pythia models demonstrate significant gains in response quality that scale with model size, with our 2.8B, 6.9B models outperforming OpenAI's released 1.3B checkpoint. We publicly release the trained model checkpoints and code to facilitate further research and accelerate progress in the field (\url{https://github.com/vwxyzjn/summarize_from_feedback_details}).
    
[^124]: 基于球形网格的HEAL-ViT：用于中程天气预报的视觉Transformer

    HEAL-ViT: Vision Transformers on a spherical mesh for medium-range weather forecasting

    [https://arxiv.org/abs/2403.17016](https://arxiv.org/abs/2403.17016)

    HEAL-ViT是专为中程天气预报设计的基于球形网格的视觉Transformer模型，克服了在矩形栅格上处理球形天气数据时出现的失真问题，将经纬度格网映射到球形网格，提高了对极地附近数据的建模效率。

    

    近年来，各种机器学习架构和技术在产生熟练的中程天气预报方面取得了成功。具体而言，基于Transformer的模型（如Pangu-Weather、FuXi）表现出色，通过将天气数据视为矩形栅格上的多通道图像，几乎可以“即插即用”。然而，矩形栅格适用于2D图像，天气数据在本质上是球形的，因此在矩形栅格上在极地区域出现严重失真，导致用于对极地附近数据建模的计算不成比例。基于图的方法（如GraphCast）不受此问题困扰，因为它们将经纬度格网映射到球形网格，但通常更占内存，需要更多的计算资源进行训练和推断。尽管在空间上是均匀的，球形网格并不容易被Transformer模型隐式建模。

    arXiv:2403.17016v1 Announce Type: cross  Abstract: In recent years, a variety of ML architectures and techniques have seen success in producing skillful medium range weather forecasts. In particular, Vision Transformer (ViT)-based models (e.g. Pangu-Weather, FuXi) have shown strong performance, working nearly "out-of-the-box" by treating weather data as a multi-channel image on a rectilinear grid. While a rectilinear grid is appropriate for 2D images, weather data is inherently spherical and thus heavily distorted at the poles on a rectilinear grid, leading to disproportionate compute being used to model data near the poles. Graph-based methods (e.g. GraphCast) do not suffer from this problem, as they map the longitude-latitude grid to a spherical mesh, but are generally more memory intensive and tend to need more compute resources for training and inference. While spatially homogeneous, the spherical mesh does not lend itself readily to be modeled by ViT-based models that implicitly r
    
[^125]: 对高光谱数据进行回归的对比学习

    Contrastive Learning for Regression on Hyperspectral Data

    [https://arxiv.org/abs/2403.17014](https://arxiv.org/abs/2403.17014)

    提出了一个对比学习框架，针对高光谱数据的回归任务，通过提供一系列适用于增强高光谱数据的变换，用对比学习来显著提高回归模型性能。

    

    对比学习在表示学习中表现出很高的效果，特别是在图像分类任务中。然而，目前对于回归任务和特别是高光谱数据应用的研究仍然不足。本文提出了一个针对高光谱数据回归任务的对比学习框架。我们提供了一系列适用于增强高光谱数据的变换，并研究了对比学习用于回归的效果。实验证明，所提出的框架和变换显著提高了回归模型的性能，在合成和实际高光谱数据集上的表现优于其他最先进的方法。

    arXiv:2403.17014v1 Announce Type: cross  Abstract: Contrastive learning has demonstrated great effectiveness in representation learning especially for image classification tasks. However, there is still a shortage in the studies targeting regression tasks, and more specifically applications on hyperspectral data. In this paper, we propose a contrastive learning framework for the regression tasks for hyperspectral data. To this end, we provide a collection of transformations relevant for augmenting hyperspectral data, and investigate contrastive learning for regression. Experiments on synthetic and real hyperspectral datasets show that the proposed framework and transformations significantly improve the performance of regression models, achieving better scores than other state-of-the-art transformations.
    
[^126]: 通过延迟环路储留神经网络对事件相机数据进行时间空间处理

    Temporal-Spatial Processing of Event Camera Data via Delay-Loop Reservoir Neural Network

    [https://arxiv.org/abs/2403.17013](https://arxiv.org/abs/2403.17013)

    提出了一个名为时间空间猜想（TSC）的猜想，强调视频信号的时间表示中携带重要信息内容，提出了一个视觉马尔可夫模型（VMM）来验证这一猜想。

    

    这篇论文描述了一种用于视频处理的时间空间模型，特别适用于处理事件相机视频。我们提出了一个名为时间空间猜想（TSC）的猜想，受到我们之前使用延迟环路储留（DLR）神经网络进行视频处理的研究的启发。TSC假设视频信号的时间表示中携带着重要的信息内容，并且机器学习算法会从对空间和时间分量进行分别优化以进行智能处理中受益。为了验证或否定TSC，我们提出了一个视觉马尔可夫模型（VMM），将视频拆分为空间和时间分量，并估计这些分量的互信息（MI）。由于计算视频互信息复杂且耗时，我们使用一个互信息神经网络来估计互信息的边界。

    arXiv:2403.17013v1 Announce Type: cross  Abstract: This paper describes a temporal-spatial model for video processing with special applications to processing event camera videos. We propose to study a conjecture motivated by our previous study of video processing with delay loop reservoir (DLR) neural network, which we call Temporal-Spatial Conjecture (TSC). The TSC postulates that there is significant information content carried in the temporal representation of a video signal and that machine learning algorithms would benefit from separate optimization of the spatial and temporal components for intelligent processing. To verify or refute the TSC, we propose a Visual Markov Model (VMM) which decompose the video into spatial and temporal components and estimate the mutual information (MI) of these components. Since computation of video mutual information is complex and time consuming, we use a Mutual Information Neural Network to estimate the bounds of the mutual information. Our resul
    
[^127]: SUDO：一种无需真实标注评估临床人工智能系统的框架

    SUDO: a framework for evaluating clinical artificial intelligence systems without ground-truth annotations

    [https://arxiv.org/abs/2403.17011](https://arxiv.org/abs/2403.17011)

    SUDO框架允许在缺乏真实标注的情况下评估AI系统，通过为实际数据点分配临时标签并直接使用它们训练模型来解决分布转移问题，从而提高对临床数据的可信度。

    

    临床人工智能（AI）系统通常在一个未曝光过的数据集上进行验证（例如来自具有不同电子健康记录系统的不同医院的数据）。这种评估过程旨在模拟将AI系统部署在未被系统见过但在临床环境中预计会遇到的数据上；然而，当实际数据与未曝光的数据集不同时，即分布转移现象，并且缺乏真实标注时，不清楚基于AI的发现在实际数据上能否受信任。在这里，我们介绍SUDO，一种用于评估无需真实标注的AI系统的框架。SUDO为实际数据点分配临时标签，并直接使用这些标签训练不同模型，表现最优的模型表明最可能的标签。

    arXiv:2403.17011v1 Announce Type: cross  Abstract: A clinical artificial intelligence (AI) system is often validated on a held-out set of data which it has not been exposed to before (e.g., data from a different hospital with a distinct electronic health record system). This evaluation process is meant to mimic the deployment of an AI system on data in the wild; those which are currently unseen by the system yet are expected to be encountered in a clinical setting. However, when data in the wild differ from the held-out set of data, a phenomenon referred to as distribution shift, and lack ground-truth annotations, it becomes unclear the extent to which AI-based findings can be trusted on data in the wild. Here, we introduce SUDO, a framework for evaluating AI systems without ground-truth annotations. SUDO assigns temporary labels to data points in the wild and directly uses them to train distinct models, with the highest performing model indicative of the most likely label. Through exp
    
[^128]: 与人类判断相一致：大型语言模型评估中成对偏好的作用

    Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators

    [https://arxiv.org/abs/2403.16950](https://arxiv.org/abs/2403.16950)

    在大型语言模型评估中，通过引入成对偏好搜索方法PAIRS，成功解决了LLMs与人类判断不一致的问题，并取得了优于直接打分的最先进性能。

    

    大型语言模型（LLMs）作为自动评估器在评估生成的自然语言质量方面表现出有希望的能力。然而，LLMs在评估中仍存在偏见，常常难以生成与人类评估一致的连贯评估。在这项工作中，我们首先对LLM评估器与人类判断之间的不一致进行系统研究，揭示现有旨在减轻偏见的校准方法不足以有效将LLM评估器对齐。受到RLHF中对偏好数据的使用的启发，我们将评估形式化为一个排序问题，并引入Pairwise-preference Search（PAIRS），这是一种以LLMs进行成对比较并有效对候选文本进行排序的基于不确定性引导的搜索方法。PAIRS在代表性评估任务上实现了最先进的性能，并且显示出比直接打分有显著改进。

    arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
    
[^129]: 利用预训练语言模型进行粗调优的专题文档检索

    Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models

    [https://arxiv.org/abs/2403.16915](https://arxiv.org/abs/2403.16915)

    本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调，在专题文档检索中显著改善了效果。

    

    在信息检索系统中，利用预训练语言模型（PLM-based IR）进行微调需要学习查询表示和查询-文档关系，除了下游任务特定的学习。本研究引入了粗调优作为一个中间学习阶段，连接了预训练和微调。通过在粗调优学习查询表示和查询-文档关系，我们旨在减少微调的负担，提高下游IR任务的学习效果。我们提出了用于粗调优的查询-文档对预测（QDPP），其预测查询-文档对的适当性。评估实验显示，所提出的方法显著改善了四个专题文档检索数据集中的MRR和/或nDCG@5。此外，查询预测任务的结果表明，粗调优促进了查询表示和查询-文档关系的学习。

    arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
    
[^130]: 使用大规模Solidity智能合约数据集推动研究

    DISL: Fueling Research with A Large Dataset of Solidity Smart Contracts

    [https://arxiv.org/abs/2403.16861](https://arxiv.org/abs/2403.16861)

    DISL数据集包含了514,506个部署在以太坊主网上的独特Solidity文件，成为了开发智能合约设计工具和基准测试的重要资源。

    

    DISL数据集包含了514,506个部署在以太坊主网上的独特Solidity文件，满足了大规模、多样化的真实世界智能合约数据集的需求。DISL成为了开发机器学习系统和为智能合约设计的软件工程工具进行基准测试的资源。通过收集截至2024年1月15日在Etherscan上验证的每个智能合约，DISL在规模和时效性上超越了现有数据集。

    arXiv:2403.16861v1 Announce Type: cross  Abstract: The DISL dataset features a collection of $514,506$ unique Solidity files that have been deployed to Ethereum mainnet. It caters to the need for a large and diverse dataset of real-world smart contracts. DISL serves as a resource for developing machine learning systems and for benchmarking software engineering tools designed for smart contracts. By aggregating every verified smart contract from Etherscan up to January 15, 2024, DISL surpasses existing datasets in size and recency.
    
[^131]: DeepMachining: 铣床机床加工误差在线预测

    DeepMachining: Online Prediction of Machining Errors of Lathe Machines

    [https://arxiv.org/abs/2403.16451](https://arxiv.org/abs/2403.16451)

    DeepMachining是一种基于深度学习的AI系统，可以在线预测车床机床加工操作的误差，通过预训练和微调模型，实现了高准确性预测，是首批使用预训练深度学习模型预测车床机床加工误差的工厂实验之一。

    

    我们描述了DeepMachining，这是一种基于深度学习的人工智能系统，用于在线预测车床加工操作的加工误差。我们基于工厂的制造数据构建并评估了DeepMachining。具体来说，我们首先对特定车床机床操作预训练深度学习模型，以学习加工状态的显著特征。然后，我们微调预训练模型以适应特定加工任务。我们展示了DeepMachining在涉及不同工件和刀具的多个任务中实现了高预测准确性。据我们所知，这项工作是使用预训练深度学习模型预测车床机床加工误差的首批工厂实验之一。

    arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
    
[^132]: 面向资源受限设备的低能量自适应个性化研究

    Towards Low-Energy Adaptive Personalization for Resource-Constrained Devices

    [https://arxiv.org/abs/2403.15905](https://arxiv.org/abs/2403.15905)

    提出了面向资源受限设备的低能耗自适应个性化框架目标块微调，根据数据漂移类型微调不同模块以实现最佳性能和降低能源消耗。

    

    机器学习（ML）模型个性化以解决数据漂移问题在物联网（IoT）应用中是一个重要挑战。目前，大多数方法侧重于微调完整基础模型或其最后几层以适应新数据，但往往忽视能源成本。我们提出了一种面向资源受限设备的低能耗自适应个性化框架——目标块微调（TBFT）。我们将数据漂移和个性化分为三种类型：输入级别、特征级别和输出级别。针对每种类型，我们微调不同模型块以实现在降低能源成本的情况下达到最佳性能。具体而言，输入级、特征级和输出级对应于微调模型的前端、中段和后端。

    arXiv:2403.15905v1 Announce Type: new  Abstract: The personalization of machine learning (ML) models to address data drift is a significant challenge in the context of Internet of Things (IoT) applications. Presently, most approaches focus on fine-tuning either the full base model or its last few layers to adapt to new data, while often neglecting energy costs. However, various types of data drift exist, and fine-tuning the full base model or the last few layers may not result in optimal performance in certain scenarios. We propose Target Block Fine-Tuning (TBFT), a low-energy adaptive personalization framework designed for resource-constrained devices. We categorize data drift and personalization into three types: input-level, feature-level, and output-level. For each type, we fine-tune different blocks of the model to achieve optimal performance with reduced energy costs. Specifically, input-, feature-, and output-level correspond to fine-tuning the front, middle, and rear blocks of 
    
[^133]: NaNa和MiGu：语义数据增强技术在图神经网络中增强蛋白质分类

    NaNa and MiGu: Semantic Data Augmentation Techniques to Enhance Protein Classification in Graph Neural Networks

    [https://arxiv.org/abs/2403.14736](https://arxiv.org/abs/2403.14736)

    提出了NaNa和MiGu两种语义数据增强方法，结合了蛋白质的主链化学和侧链生物物理信息，用于增强图神经网络中的蛋白质分类任务。

    

    蛋白质分类任务在药物发现中至关重要。现实世界中的蛋白质结构是动态变化的，这将决定蛋白质的性质。然而，现有的机器学习方法，如ProNet，仅访问有限的构象特征和蛋白质侧链特征，导致预测中蛋白质结构的不切实际和蛋白质类别的不准确性。在本文中，我们提出了新颖的语义数据增强方法NaNa和MiGu，将蛋白质主链化学和侧链生物物理信息纳入蛋白质分类任务和共嵌残差学习框架。具体来说，我们利用了蛋白质的分子生物物理、二级结构、化学键和离子特征来促进蛋白质分类任务。

    arXiv:2403.14736v1 Announce Type: cross  Abstract: Protein classification tasks are essential in drug discovery. Real-world protein structures are dynamic, which will determine the properties of proteins. However, the existing machine learning methods, like ProNet (Wang et al., 2022a), only access limited conformational characteristics and protein side-chain features, leading to impractical protein structure and inaccuracy of protein classes in their predictions. In this paper, we propose novel semantic data augmentation methods, Novel Augmentation of New Node Attributes (NaNa), and Molecular Interactions and Geometric Upgrading (MiGu) to incorporate backbone chemical and side-chain biophysical information into protein classification tasks and a co-embedding residual learning framework. Specifically, we leverage molecular biophysical, secondary structure, chemical bonds, and ionic features of proteins to facilitate protein classification tasks. Furthermore, our semantic augmentation me
    
[^134]: 一种利用大型语言模型进行设备定向语音检测的多模态方法

    A Multimodal Approach to Device-Directed Speech Detection with Large Language Models

    [https://arxiv.org/abs/2403.14438](https://arxiv.org/abs/2403.14438)

    探索了一种利用大型语言模型进行设备定向语音检测的多模态方法，相比于文本和音频模型，使用多模态信息能够显著提高相等错误率。

    

    虚拟助手的交互通常从预定义触发短语开始，然后是用户命令。为了使与助手的交互更直观，我们探讨了是否可以放弃用户必须用触发短语开始每个命令的要求。我们通过三种方式探索了这个任务：首先，我们仅使用从音频波形中获得的声学信息训练分类器。其次，我们将自动语音识别（ASR）系统的解码器输出，例如1-best假设，作为输入特征输入到大型语言模型（LLM）中。最后，我们探讨了一种多模态系统，将声学和词汇特征以及ASR解码器信号结合在LLM中。使用多模态信息相对于仅文本和仅音频模型提高了相等错误率高达39%和61%。增加LLM的大小并通过低秩调整进行训练进一步减少了相对EER值的减少

    arXiv:2403.14438v1 Announce Type: new  Abstract: Interactions with virtual assistants typically start with a predefined trigger phrase followed by the user command. To make interactions with the assistant more intuitive, we explore whether it is feasible to drop the requirement that users must begin each command with a trigger phrase. We explore this task in three ways: First, we train classifiers using only acoustic information obtained from the audio waveform. Second, we take the decoder outputs of an automatic speech recognition (ASR) system, such as 1-best hypotheses, as input features to a large language model (LLM). Finally, we explore a multimodal system that combines acoustic and lexical features, as well as ASR decoder signals in an LLM. Using multimodal information yields relative equal-error-rate improvements over text-only and audio-only models of up to 39% and 61%. Increasing the size of the LLM and training with low-rank adaption leads to further relative EER reductions o
    
[^135]: 具有对数据异构性的自适应的拜占庭弹性联邦学习

    Byzantine-resilient Federated Learning With Adaptivity to Data Heterogeneity

    [https://arxiv.org/abs/2403.13374](https://arxiv.org/abs/2403.13374)

    通过提出新的Robust Average Gradient Algorithm（RAGA），本研究在联邦学习中解决了恶意拜占庭攻击和数据异构性的问题，实现了在非凸损失函数和异构数据集上的收敛性分析，并展示了RAGA的良好收敛性能。

    

    本文处理了在存在恶意拜占庭攻击和数据异构性的情况下的联邦学习（FL）。提出了一种新颖的鲁棒平均梯度算法（RAGA），该算法利用几何中位数进行聚合，并可以自由选择本地更新的轮数。与大多数现有的弹性方法不同，这些方法基于强凸损失函数或均匀分布的数据集进行收敛分析，我们进行了对强凸和非凸损失函数在异构数据集上的收敛分析。根据我们的理论分析，只要恶意用户数据集的比例小于一半，RAGA就可以以$\mathcal{O}({1}/{T^{2/3- \delta}})$的速度实现非凸损失函数的收敛，其中$T$为迭代次数，$\delta \in (0, 2/3)$，对于强凸损失函数则呈线性收敛。此外，稳定点或全局最优解

    arXiv:2403.13374v1 Announce Type: new  Abstract: This paper deals with federated learning (FL) in the presence of malicious Byzantine attacks and data heterogeneity. A novel Robust Average Gradient Algorithm (RAGA) is proposed, which leverages the geometric median for aggregation and can freely select the round number for local updating. Different from most existing resilient approaches, which perform convergence analysis based on strongly-convex loss function or homogeneously distributed dataset, we conduct convergence analysis for not only strongly-convex but also non-convex loss function over heterogeneous dataset. According to our theoretical analysis, as long as the fraction of dataset from malicious users is less than half, RAGA can achieve convergence at rate $\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and $\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for strongly-convex loss function. Moreover, stationary point or global optim
    
[^136]: GeRM：一种用于四足机器人的混合专家通用机器人模型

    GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped Robot

    [https://arxiv.org/abs/2403.13358](https://arxiv.org/abs/2403.13358)

    GeRM是一种通用机器人模型，通过混合专家结构和离线强化学习优化数据利用策略，解决了多任务机器人学习中性能问题和数据收集困难的情况，显著提高了模型性能。

    

    多任务机器人学习在解决多样化和复杂情景方面具有重要意义。然而，当前的方法受到性能问题和收集训练数据集困难的阻碍。本文中，我们提出了GeRM（通用机器人模型）。我们利用离线强化学习优化数据利用策略，从演示和次优数据中学习，从而超越人类演示的局限。随后，我们采用基于Transformer的VLA网络来处理多模态输入并输出动作。通过引入专家混合结构，GeRM实现了更快的推理速度和更高的整体模型容量，从而解决了RL参数有限的问题，在控制计算成本的同时提高了多任务学习中模型的性能。通过一系列实验，我们证实GeRM在所有任务上表现优于其他方法, 而且还有效

    arXiv:2403.13358v1 Announce Type: cross  Abstract: Multi-task robot learning holds significant importance in tackling diverse and complex scenarios. However, current approaches are hindered by performance issues and difficulties in collecting training datasets. In this paper, we propose GeRM (Generalist Robotic Model). We utilize offline reinforcement learning to optimize data utilization strategies to learn from both demonstrations and sub-optimal data, thus surpassing the limitations of human demonstrations. Thereafter, we employ a transformer-based VLA network to process multi-modal inputs and output actions. By introducing the Mixture-of-Experts structure, GeRM allows faster inference speed with higher whole model capacity, and thus resolves the issue of limited RL parameters, enhancing model performance in multi-task learning while controlling computational costs. Through a series of experiments, we demonstrate that GeRM outperforms other methods across all tasks, while also valid
    
[^137]: 将大型语言模型中的领域特定内容融入知识图谱，以增强零样本对象状态分类

    Fusing Domain-Specific Content from Large Language Models into Knowledge Graphs for Enhanced Zero Shot Object State Classification

    [https://arxiv.org/abs/2403.12151](https://arxiv.org/abs/2403.12151)

    大型语言模型与知识图谱结合，提高零样本对象状态分类性能

    

    领域特定知识可以显著有助于解决各种视觉任务，但生成这种知识需要大量人力和时间成本。本研究探讨了大型语言模型（LLMs）在通过语义嵌入生成和提供领域特定信息方面的潜力。为实现这一目标，将LLM集成到一个流程中，该流程在视觉基础零样本对象状态分类任务的背景下利用知识图谱和预训练的语义向量。通过广泛的消融研究彻底研究了LLM的行为。我们的研究结果表明，将基于LLM的嵌入与通用的预训练嵌入结合使用可以显著提高性能。借鉴这一消融研究的见解，我们对竞争模型进行了比较分析，从而突出了最新的表现水平。

    arXiv:2403.12151v1 Announce Type: new  Abstract: Domain-specific knowledge can significantly contribute to addressing a wide variety of vision tasks. However, the generation of such knowledge entails considerable human labor and time costs. This study investigates the potential of Large Language Models (LLMs) in generating and providing domain-specific information through semantic embeddings. To achieve this, an LLM is integrated into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors in the context of the Vision-based Zero-shot Object State Classification task. We thoroughly examine the behavior of the LLM through an extensive ablation study. Our findings reveal that the integration of LLM-based embeddings, in combination with general-purpose pre-trained embeddings, leads to substantial performance improvements. Drawing insights from this ablation study, we conduct a comparative analysis against competing models, thereby highlighting the state-of-the-art perfor
    
[^138]: 利用生成式知识提取、基于图的表示和多模态智能图推理加速科学发现

    Accelerating Scientific Discovery with Generative Knowledge Extraction, Graph-Based Representation, and Multimodal Intelligent Graph Reasoning

    [https://arxiv.org/abs/2403.11996](https://arxiv.org/abs/2403.11996)

    利用生成式人工智能和图算法加速科学发现，揭示论文之间的深入跨学科关系，并提出了新颖的材料设计。

    

    利用生成式人工智能，我们将一组涉及生物材料领域的1,000篇科学论文转化为详细的本体知识图，揭示了它们固有的无标度特性。通过基于节点相似性和介数中心性的组合排名，探测不同概念之间的图遍历路径，我们揭示了深入的跨学科关系，可用于回答查询，识别知识中的空白，并提出前所未见的材料设计及其行为。一项比较揭示了生物材料和贝多芬第九交响曲之间的详细结构相似之处，突显了通过同构映射共享复杂性模式。该算法进一步创建了一种创新的基于分级菌丝体的复合材料，将图采样的联合合成原理与康定斯基《第七组成》中提取的原则相结合

    arXiv:2403.11996v1 Announce Type: cross  Abstract: Using generative Artificial Intelligence (AI), we transformed a set of 1,000 scientific papers in the area of biological materials into detailed ontological knowledge graphs, revealing their inherently scale-free nature. Using graph traversal path detection between dissimilar concepts based on combinatorial ranking of node similarity and betweenness centrality, we reveal deep insights into unprecedented interdisciplinary relationships that can be used to answer queries, identify gaps in knowledge, and propose never-before-seen material designs and their behaviors. One comparison revealed detailed structural parallels between biological materials and Beethoven's 9th Symphony, highlighting shared patterns of complexity through isomorphic mapping. The algorithm further created an innovative hierarchical mycelium-based composite that incorporates joint synthesis of graph sampling with principles extracted from Kandinsky's Composition VII p
    
[^139]: SelfIE：大型语言模型嵌入的自我解释

    SelfIE: Self-Interpretation of Large Language Model Embeddings

    [https://arxiv.org/abs/2403.10949](https://arxiv.org/abs/2403.10949)

    提出了SelfIE框架，使大型语言模型能够自解释其嵌入，揭示内部推理，包括道德决策、提示注入和消除有害知识。

    

    arXiv:2403.10949v1 公告类型：交叉摘要：大型语言模型（LLMs）如何获得答案？解释和控制LLM的推理过程对于可靠性、透明度和未来模型发展至关重要。我们提出了SelfIE（嵌入的自我解释），这是一个框架，能够利用LLMs响应关于给定段落的查询的能力，以自然语言解释它们自己的嵌入。SelfIE能够解释隐藏嵌入中的开放世界概念，在案例中揭示LLM的内部推理，如做出道德决策、内化提示注入和回想有害知识。SelfIE对隐藏嵌入的文本描述也开辟了控制LLM推理的新途径。我们提出了监督控制，它允许编辑开放式概念，而只需要计算单个层的梯度。我们将RLHF扩展到隐藏的嵌入，并提出了强化控制来消除有害知识。

    arXiv:2403.10949v1 Announce Type: cross  Abstract: How do large language models (LLMs) obtain their answers? The ability to explain and control an LLM's reasoning process is key for reliability, transparency, and future model developments. We propose SelfIE (Self-Interpretation of Embeddings), a framework that enables LLMs to interpret their own embeddings in natural language by leveraging their ability to respond inquiry about a given passage. Capable of interpreting open-world concepts in the hidden embeddings, SelfIE reveals LLM internal reasoning in cases such as making ethical decisions, internalizing prompt injection, and recalling harmful knowledge. SelfIE's text descriptions on hidden embeddings also open up new avenues to control LLM reasoning. We propose Supervised Control, which allows editing open-ended concepts while only requiring gradient computation of individual layer. We extend RLHF to hidden embeddings and propose Reinforcement Control that erases harmful knowledge i
    
[^140]: 带有注意力感知自适应提示的少样本类增量学习

    Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt

    [https://arxiv.org/abs/2403.09857](https://arxiv.org/abs/2403.09857)

    提出了一个名为ASP的框架，通过注意力方面减少特定信息，鼓励任务不变的提示来捕获共享知识，并通过信息瓶颈学习目标从旧类到新类传递知识。

    

    少样本类增量学习（FSCIL）模型旨在在保留旧类知识的同时，逐步学习新类别的稀缺样本。现有的FSCIL方法通常对整个骨干进行微调，导致过拟合并阻碍学习新类别的潜力。另一方面，最近基于提示的CIL方法通过在每个任务中用足够的数据训练提示来减轻遗忘。在这项工作中，我们提出了一个名为注意力感知自适应提示（ASP）的新框架。ASP通过从注意力方面减少特定信息，鼓励任务不变的提示来捕获共享知识。此外，ASP中的自适应任务特定提示提供特定信息，并通过信息瓶颈学习目标从旧类到新类传递知识。总之，ASP防止了在基础任务上的过拟合，并不需要在少样本增量任务中使用大量数据。

    arXiv:2403.09857v1 Announce Type: cross  Abstract: Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn new classes with scarce samples while preserving knowledge of old ones. Existing FSCIL methods usually fine-tune the entire backbone, leading to overfitting and hindering the potential to learn new classes. On the other hand, recent prompt-based CIL approaches alleviate forgetting by training prompts with sufficient data in each task. In this work, we propose a novel framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages task-invariant prompts to capture shared knowledge by reducing specific information from the attention aspect. Additionally, self-adaptive task-specific prompts in ASP provide specific information and transfer knowledge from old classes to new classes with an Information Bottleneck learning objective. In summary, ASP prevents overfitting on base task and does not require enormous data in few-shot incremental tasks. Extensi
    
[^141]: 与邻居借宝：用于多模态学习的上下文学习，解决缺失模态和数据稀缺问题

    Borrowing Treasures from Neighbors: In-Context Learning for Multimodal Learning with Missing Modalities and Data Scarcity

    [https://arxiv.org/abs/2403.09428](https://arxiv.org/abs/2403.09428)

    本文提出了一种检索增强的上下文学习框架，旨在解决多模态学习中缺失模态和数据稀缺问题。

    

    缺失模态的多模态机器学习是一个越来越相关的挑战，在各种应用中如医疗保健。本文将现有关于缺失模态的研究拓展到低数据情境，即一个下游任务既存在缺失模态又存在样本数量有限的问题。我们建议使用检索增强的上下文学习来解决这两个关键问题，释放变压器在上下文学习能力方面的潜力。与现有方法不同，我们的工作利用现有的全模态数据的价值，提供了解决挑战的新视角。

    arXiv:2403.09428v1 Announce Type: new  Abstract: Multimodal machine learning with missing modalities is an increasingly relevant challenge arising in various applications such as healthcare. This paper extends the current research into missing modalities to the low-data regime, i.e., a downstream task has both missing modalities and limited sample size issues. This problem setting is particularly challenging and also practical as it is often expensive to get full-modality data and sufficient annotated training samples. We propose to use retrieval-augmented in-context learning to address these two crucial issues by unleashing the potential of a transformer's in-context learning ability. Diverging from existing methods, which primarily belong to the parametric paradigm and often require sufficient training samples, our work exploits the value of the available full-modality data, offering a novel perspective on resolving the challenge. The proposed data-dependent framework exhibits a high
    
[^142]: 持续预训练大型语言模型的简单可扩展策略

    Simple and Scalable Strategies to Continually Pre-train Large Language Models

    [https://arxiv.org/abs/2403.08763](https://arxiv.org/abs/2403.08763)

    通过简单和可扩展的学习率调整、重放数据的方法，可以在不重新训练的情况下，持续预训练大型语言模型以匹配完全重新训练时的性能。

    

    大型语言模型（LLMs）通常在数十亿的标记上进行常规预训练，一旦有新数据可用就重新开始该过程。一个更有效率的解决方案是持续预训练这些模型，与重新训练相比能节省大量计算资源。然而，新数据引起的分布转移通常会导致在以前数据上降低性能或无法适应新数据。在本工作中，我们展示了一种简单且可扩展的学习率（LR）重新升温、LR重新衰减和重放上一数据的组合足以与完全从头开始重新训练在所有可用数据上的性能相匹配，从最终损失和语言模型（LM）评估基准的角度衡量。具体而言，我们展示了在两个常用的LLM预训练数据集（英语→英语）之间的弱但现实的分布转移以及更强烈的分布转移（英语→德语）下的情况。

    arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
    
[^143]: 异质学习代理群体中道德行为动态

    Dynamics of Moral Behavior in Heterogeneous Populations of Learning Agents

    [https://arxiv.org/abs/2403.04202](https://arxiv.org/abs/2403.04202)

    在多代理环境中，研究人员探讨了不同道德类型的学习代理之间的互动，发现道德异质性可能对代理的共同发展产生影响。

    

    arXiv:2403.04202v1 公告类型：交叉领域 摘要：日益关注AI系统安全和对齐性的问题突显了在人工代理中嵌入道德能力的重要性。一种有前途的解决方案是利用经验学习，即强化学习。在多代理（社会）环境中，个体学习代理之间的交互可能产生复杂的群体层面现象。许多现有研究依赖于模拟的社会困境环境来研究独立学习代理的互动。然而，它们往往忽视了实践中代理社会中可能存在的道德异质性。例如，在不同时间点，单个学习代理可能面对后果主义者（即关心随时间最大化某种结果）或基于规范的对手（即专注于立即遵守特定规范） 。代理的共同发展在多大程度上可能受到这种道德异质性的影响。

    arXiv:2403.04202v1 Announce Type: cross  Abstract: Growing concerns about safety and alignment of AI systems highlight the importance of embedding moral capabilities in artificial agents. A promising solution is the use of learning from experience, i.e., Reinforcement Learning. In multi-agent (social) environments, complex population-level phenomena may emerge from interactions between individual learning agents. Many of the existing studies rely on simulated social dilemma environments to study the interactions of independent learning agents. However, they tend to ignore the moral heterogeneity that is likely to be present in societies of agents in practice. For example, at different points in time a single learning agent may face opponents who are consequentialist (i.e., caring about maximizing some outcome over time) or norm-based (i.e., focusing on conforming to a specific norm here and now). The extent to which agents' co-development may be impacted by such moral heterogeneity in 
    
[^144]: 机器学习在恶意软件检测中对Mal-API-2019数据集的全面评估

    Comprehensive evaluation of Mal-API-2019 dataset by machine learning in malware detection

    [https://arxiv.org/abs/2403.02232](https://arxiv.org/abs/2403.02232)

    该研究通过机器学习技术全面评估了恶意软件检测，发现集成方法（如随机森林和XGBoost）相较于其他方法在恶意软件检测中表现出更高的准确性、精确度和召回率。

    

    这项研究使用机器学习技术对恶意软件检测进行了彻底的探讨，重点评估了使用Mal-API-2019数据集的各种分类模型。旨在通过更有效地识别和缓解威胁来推进网络安全能力。研究探讨了集成和非集成的机器学习方法，如随机森林、XGBoost、K最近邻（KNN）和神经网络。特别强调了数据预处理技术的重要性，特别是TF-IDF表示和主成分分析，以提高模型性能。结果显示，随机森林和XGBoost等集成方法相比其他方法具有更高的准确性、精确度和召回率，突显了它们在恶意软件检测中的有效性。论文还讨论了限制和潜在的未来方向，强调了需要不断适应的必要性。

    arXiv:2403.02232v1 Announce Type: cross  Abstract: This study conducts a thorough examination of malware detection using machine learning techniques, focusing on the evaluation of various classification models using the Mal-API-2019 dataset. The aim is to advance cybersecurity capabilities by identifying and mitigating threats more effectively. Both ensemble and non-ensemble machine learning methods, such as Random Forest, XGBoost, K Nearest Neighbor (KNN), and Neural Networks, are explored. Special emphasis is placed on the importance of data pre-processing techniques, particularly TF-IDF representation and Principal Component Analysis, in improving model performance. Results indicate that ensemble methods, particularly Random Forest and XGBoost, exhibit superior accuracy, precision, and recall compared to others, highlighting their effectiveness in malware detection. The paper also discusses limitations and potential future directions, emphasizing the need for continuous adaptation t
    
[^145]: 通过高阶注意力大脑网络分析大麻使用者的静息态fMRI数据

    Analyzing Resting-State fMRI Data in Marijuana Users via High-Order Attention Brain Network

    [https://arxiv.org/abs/2403.00033](https://arxiv.org/abs/2403.00033)

    通过结合动态内在功能网络和LSTM技术，使用高阶注意力模块进行信息融合和消息传递，提出了HOGAB模型，对慢性大麻用户的静息态fMRI数据进行分析，提高了多图分类的准确性。

    

    大麻的持续使用明显影响人们的生活和健康。在这项研究中，我们提出了一个可解释的新框架，命名为HOGAB（High-Order Attention Graph Attention神经网络）模型，以分析两个数据集中慢性大麻用户的局部异常脑活动。HOGAB将动态内在功能网络与LSTM技术相结合，捕捉大麻用户fMRI时间序列中的时间模式。此外，我们使用高阶注意力模块来对邻域节点进行信息融合和消息传递，增强长期大麻用户的社区聚类分析。此外，我们通过融入注意力机制提高了模型的整体学习能力，在多图分类中实现了85.1%的AUC和80.7%的准确性。此外，我们比较了线性机器学习方法，并评估了我们提出的HODAB模型的有效性。

    arXiv:2403.00033v1 Announce Type: cross  Abstract: The sustained use of marijuana significantly impacts the lives and health of people. In this study, we propose an interpretable novel framework called the HOGAB (High-Order Attention Graph Attention Neural Networks) model to analyze local abnormal brain activity in chronic marijuana users in two datasets. The HOGAB integrates dynamic intrinsic functional networks with LSTM technology to capture temporal patterns in fMRI time series of marijuana users. Moreover, we use the high-order attention module in neighborhood nodes for information fusion and message passing, enhancing community clustering analysis for long-term marijuana users. Furthermore, we improve the overall learning ability of the model by incorporating attention mechanisms, achieving an AUC of 85.1% and an accuracy of 80.7% in multigraph classification. In addition, we compare linear machine learning methods and evaluate the effectiveness of our proposed HODAB model. Speci
    
[^146]: 在实际处理内存系统上加速图神经网络

    Accelerating Graph Neural Networks on Real Processing-In-Memory Systems

    [https://arxiv.org/abs/2402.16731](https://arxiv.org/abs/2402.16731)

    在实际处理内存系统上加速图神经网络，并提出了针对实际PIM系统的智能并行化技术和混合式执行方法。

    

    图神经网络（GNNs）是新兴的机器学习模型，用于分析图结构数据。图神经网络（GNN）的执行涉及计算密集型和内存密集型核心，后者在总时间中占主导地位，受数据在内存和处理器之间移动的严重瓶颈所限制。处理内存（PIM）系统可以通过在内存阵列附近或内部放置简单处理器来缓解这种数据移动瓶颈。在这项工作中，我们介绍了PyGim，一个有效的机器学习框架，可以在实际PIM系统上加速GNNs。我们为针对实际PIM系统定制的GNN内存密集型核心提出智能并行化技术，并为它们开发了方便的Python API。我们提供混合式GNN执行，其中计算密集型和内存密集型核心分别在以处理器为中心和以内存为中心的计算系统中执行，以匹配它们的算法特性。我们进行了大量评估。

    arXiv:2402.16731v2 Announce Type: replace-cross  Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML framework that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively, to match their algorithmic nature. We extensively evaluate 
    
[^147]: 基于Twitter数据的精神健康监测框架：从当地推文到当地健康

    LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data

    [https://arxiv.org/abs/2402.13452](https://arxiv.org/abs/2402.13452)

    本研究提出了一个新的基于Twitter数据的框架LocalHealth，用于预测当地精神健康结果。通过与GPT3.5结合使用，该框架在MH监测中取得了显著的改进。

    

    先前关于Twitter数据的研究已经提供了它在开发补充健康监测系统方面的实用性证据。在这项研究中，我们提出了一个新的框架来监测公共健康，重点关注精神健康（MH）结果。我们假设当地发布的推文可以表明当地的精神健康结果，并收集了来自美国765个地区（人口普查分组）的推文。我们将每个地区的这些推文与疾病控制中心（CDC）报告的相应MH结果配对，创建了一个基准数据集LocalTweets。借助LocalTweets，我们提出了基于Twitter的MH监测系统的首个人口级评估任务。随后，我们开发了一个高效有效的方法LocalHealth，用于根据LocalTweets预测MH结果。当与GPT3.5一起使用时，LocalHealth实现了最高的F1值和准确率，分别达到0.7429和79.78\%，F1值提高了59\%。

    arXiv:2402.13452v1 Announce Type: cross  Abstract: Prior research on Twitter (now X) data has provided positive evidence of its utility in developing supplementary health surveillance systems. In this study, we present a new framework to surveil public health, focusing on mental health (MH) outcomes. We hypothesize that locally posted tweets are indicative of local MH outcomes and collect tweets posted from 765 neighborhoods (census block groups) in the USA. We pair these tweets from each neighborhood with the corresponding MH outcome reported by the Center for Disease Control (CDC) to create a benchmark dataset, LocalTweets. With LocalTweets, we present the first population-level evaluation task for Twitter-based MH surveillance systems. We then develop an efficient and effective method, LocalHealth, for predicting MH outcomes based on LocalTweets. When used with GPT3.5, LocalHealth achieves the highest F1-score and accuracy of 0.7429 and 79.78\%, respectively, a 59\% improvement in F
    
[^148]: 具有延迟更新的随机逼近：马尔科夫采样下的有限时间速率

    Stochastic Approximation with Delayed Updates: Finite-Time Rates under Markovian Sampling

    [https://arxiv.org/abs/2402.11800](https://arxiv.org/abs/2402.11800)

    延迟更新的随机逼近方案在时间变化有界延迟下，保证了每次迭代快速收敛到固定点周围的球体，界限依赖于最大延迟和混合时间。

    

    受大规模和多智能体强化学习应用的启发，我们研究了在马尔科夫采样下具有延迟更新的随机逼近（SA）方案的非渐近性能。虽然延迟的影响在优化中得到了广泛研究，但它们与底层马尔科夫过程相互作用以塑造SA的有限时间性能的方式仍然不太清楚。在这个背景下，我们的第一个主要贡献是证明在时间变化有界延迟下，延迟的SA更新规则确保最后迭代收敛到SA运算符固定点周围的球体具有指数快速的速度。值得注意的是，我们的界限在依赖于最大延迟$\tau_{max}$和混合时间$\tau_{mix}$方面是\emph{紧致的}。为了实现这一紧密界限，我们开发了一种新颖的归纳证明技术，与各种现有延迟优化分析不同，它依赖于建立未...

    arXiv:2402.11800v1 Announce Type: cross  Abstract: Motivated by applications in large-scale and multi-agent reinforcement learning, we study the non-asymptotic performance of stochastic approximation (SA) schemes with delayed updates under Markovian sampling. While the effect of delays has been extensively studied for optimization, the manner in which they interact with the underlying Markov process to shape the finite-time performance of SA remains poorly understood. In this context, our first main contribution is to show that under time-varying bounded delays, the delayed SA update rule guarantees exponentially fast convergence of the \emph{last iterate} to a ball around the SA operator's fixed point. Notably, our bound is \emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel inductive proof technique that, unlike various existing delayed-optimization analyses, relies on establishing un
    
[^149]: 使用复值神经网络和不规则分布的麦克风重建房间传递函数

    Room transfer function reconstruction using complex-valued neural networks and irregularly distributed microphones

    [https://arxiv.org/abs/2402.04866](https://arxiv.org/abs/2402.04866)

    本研究使用复值神经网络在有限的麦克风数据中估计房间传递函数，实现了房间传递函数的重建，创新之处在于首次使用了复值神经网络来估计房间传递函数。

    

    重建房间传递函数用于计算房间内的复杂声场具有重要的实际应用。然而，通常需要使用大量的麦克风，这是不现实的。最近，除了传统的信号处理方法，深度学习技术已经被应用于从房间内零散点测量得到的有限的房间传递函数来重建房间传递函数。在本研究中，我们使用复值神经网络估计房间传递函数在第一个声学共振频率范围内，使用少量不规则分布的麦克风。据我们所知，这是首次使用复值神经网络来估计房间传递函数。为了分析将复值优化应用于所考虑任务的好处，我们将所提出的技术与最先进的实值神经网络方法和基于核的最先进方法进行比较。

    Reconstructing the room transfer functions needed to calculate the complex sound field in a room has several important real-world applications. However, an unpractical number of microphones is often required. Recently, in addition to classical signal processing methods, deep learning techniques have been applied to reconstruct the room transfer function starting from a very limited set of room transfer functions measured at scattered points in the room. In this study, we employ complex-valued neural networks to estimate room transfer functions in the frequency range of the first room resonances, using a few irregularly distributed microphones. To the best of our knowledge, this is the first time complex-valued neural networks are used to estimate room transfer functions. To analyze the benefits of applying complex-valued optimization to the considered task, we compare the proposed technique with a state-of-the-art real-valued neural network method and a state-of-the-art kernel-based si
    
[^150]: ChIRAAG: 通过ChatGPT生成快速和自动断言的方法

    ChIRAAG: ChatGPT Informed Rapid and Automated Assertion Generation

    [https://arxiv.org/abs/2402.00093](https://arxiv.org/abs/2402.00093)

    本研究设计了一个基于大语言模型的流水线，通过自然语言规格生成英语、线性时态逻辑和SVA断言，并成功减少了断言错误率。

    

    System Verilog Assertion (SVA)的形式化是Formal Property Verification (FPV)过程中的一个关键但复杂的任务。传统上，SVA的形式化需要经验丰富的专家解释规格。这是耗时且容易出错的。然而，最近大语言模型（LLM）的进展使得基于LLM的自动断言生成引起了人们的兴趣。我们设计了一种新颖的基于LLM的流水线，用于从自然语言规格中生成英语、线性时态逻辑和SVA的断言。我们开发了一个基于OpenAI GPT4的自定义LLM用于实验。此外，我们还开发了测试平台来验证LLM生成的断言。只有43%的LLM生成的原始断言存在错误，包括语法和逻辑错误。通过使用从测试案例失败中得出的精心设计的提示，迭代地促使LLM，该流水线在最多九次提示迭代后可以生成正确的SVA。

    System Verilog Assertion (SVA) formulation, a critical yet complex task, is a pre-requisite in the Formal Property Verification (FPV) process. Traditionally, SVA formulation involves expert-driven interpretation of specifications. This is time consuming and prone to human error. However, recent advances in Large Language Models (LLM), LLM-informed automatic assertion generation is gaining interest. We designed a novel LLM-based pipeline to generate assertions in English Language, Linear Temporal Logic, and SVA from natural language specifications. We developed a custom LLM-based on OpenAI GPT4 for our experiments. Furthermore, we developed testbenches to verify/validate the LLM-generated assertions. Only 43% of LLM-generated raw assertions had errors, including syntax and logical errors. By iteratively prompting the LLMs using carefully crafted prompts derived from test case failures, the pipeline could generate correct SVAs after a maximum of nine iterations of prompting. Our results 
    
[^151]: 机器学习中的分布式优化中的最佳数据分裂

    Optimal Data Splitting in Distributed Optimization for Machine Learning

    [https://arxiv.org/abs/2401.07809](https://arxiv.org/abs/2401.07809)

    分布式优化问题中，提出了一种利用本地数据相似性的算法，可有效解决通信成本瓶颈。

    

    分布式优化问题近来变得越来越重要。与非分布式方法相比，它具有处理大量数据所需时间较短等许多优势。然而，大多数分布式方法存在一个显著瓶颈 - 通信成本。因此，最近大量研究致力于解决这个问题。一种方法利用本地数据相似性。特别是有一种算法可以证明地利用相似性属性。但是，这一结果以及其他作品的结果都是通过专注于通信明显比本地计算更昂贵这一事实而解决了通信瓶颈，并未考虑到网络设备的各种容量以及通信时间和本地计算费用之间的不同关系。我们考虑了这一设置，本研究的目标是探讨这个问题的一个新视角。

    arXiv:2401.07809v2 Announce Type: replace-cross  Abstract: The distributed optimization problem has become increasingly relevant recently. It has a lot of advantages such as processing a large amount of data in less time compared to non-distributed methods. However, most distributed approaches suffer from a significant bottleneck - the cost of communications. Therefore, a large amount of research has recently been directed at solving this problem. One such approach uses local data similarity. In particular, there exists an algorithm provably optimally exploiting the similarity property. But this result, as well as results from other works solve the communication bottleneck by focusing only on the fact that communication is significantly more expensive than local computing and does not take into account the various capacities of network devices and the different relationship between communication time and local computing expenses. We consider this setup and the objective of this study i
    
[^152]: 模型并行训练中的激活和梯度压缩

    Activations and Gradients Compression for Model-Parallel Training

    [https://arxiv.org/abs/2401.07788](https://arxiv.org/abs/2401.07788)

    本研究探讨了在模型-并行分布式训练设置中对激活和梯度同时进行压缩对收敛的影响，发现梯度需要比激活更轻微的压缩率。

    

    大规模神经网络需要庞大的计算机集群。在模型并行训练中，当模型架构在工作者之间被顺序分割时，成为训练现代模型的一种流行方法。信息压缩可以应用于减少工作者的通信时间，在这种系统中通常是一个瓶颈。本文探讨了在模型并行分布式训练设置中同时压缩激活和梯度对收敛的影响。我们分析了诸如量化和TopK压缩等压缩方法，并尝试了误差补偿技术。此外，我们采用了TopK与AQ-SGD每批次误差反馈方法。我们在图像分类和语言模型微调任务上进行了实验。我们的研究结果表明，梯度需要比激活更轻微的压缩率。我们观察到，$K=10\%$是最低的TopK压缩级别，这不导致收敛速度下降。

    arXiv:2401.07788v2 Announce Type: replace  Abstract: Large neural networks require enormous computational clusters of machines. Model-parallel training, when the model architecture is partitioned sequentially between workers, is a popular approach for training modern models. Information compression can be applied to decrease workers communication time, as it is often a bottleneck in such systems. This work explores how simultaneous compression of activations and gradients in model-parallel distributed training setup affects convergence. We analyze compression methods such as quantization and TopK compression, and also experiment with error compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error feedback approach. We conduct experiments on image classification and language model fine-tuning tasks. Our findings demonstrate that gradients require milder compression rates than activations. We observe that $K=10\%$ is the lowest TopK compression level, which does not h
    
[^153]: AI和生成式AI用于研究发现与总结

    AI and Generative AI for Research Discovery and Summarization

    [https://arxiv.org/abs/2401.06795](https://arxiv.org/abs/2401.06795)

    AI和生成式AI工具在研究发现和总结方面有重大影响，包括能够更快地找到相关文献和用简洁语言总结研究文章的要点。

    

    AI和生成式AI工具，包括依赖大型语言模型（LLMs）的聊天机器人如ChatGPT，今年迅速崛起，为增加工作效率和改善生活创造了难以置信的机会。统计学家和数据科学家已经开始以多种方式体验到这些工具的好处，比如从文本提示生成编程代码以分析数据或拟合统计模型。这些工具可以在研究发现和总结方面产生重大影响之一。正在开发独立工具和插件给聊天机器人，使研究人员比2023年之前的搜索工具更快地找到相关文献。此外，生成式AI工具已经发展到可以用简洁的语言总结和提取研究文章的要点的程度。最后，基于高度参数化的LLMs的聊天机器人可用于模拟

    arXiv:2401.06795v2 Announce Type: replace-cross  Abstract: AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simula
    
[^154]: Li-ion电池模型的PINN代理用于参数推断。第II部分：正则化和伪二维模型的应用

    PINN surrogate of Li-ion battery models for parameter inference. Part II: Regularization and application of the pseudo-2D model

    [https://arxiv.org/abs/2312.17336](https://arxiv.org/abs/2312.17336)

    开发了物理信息神经网络（PINN）作为伪二维（P2D）电池模型校准的代理，进行了参数推断研究，可以减少Bayesian校准的计算成本。

    

    Bayesian参数推断对改进锂离子电池诊断有用，并有助于制定电池老化模型。为了降低Bayesian校准的计算成本，可以用更快的代理替换基于物理的模型的数值求解器。将一个物理信息神经网络（PINN）开发为伪二维（P2D）电池模型校准的代理。

    arXiv:2312.17336v2 Announce Type: replace  Abstract: Bayesian parameter inference is useful to improve Li-ion battery diagnostics and can help formulate battery aging models. However, it is computationally intensive and cannot be easily repeated for multiple cycles, multiple operating conditions, or multiple replicate cells. To reduce the computational cost of Bayesian calibration, numerical solvers for physics-based models can be replaced with faster surrogates. A physics-informed neural network (PINN) is developed as a surrogate for the pseudo-2D (P2D) battery model calibration. For the P2D surrogate, additional training regularization was needed as compared to the PINN single-particle model (SPM) developed in Part I. Both the PINN SPM and P2D surrogate models are exercised for parameter inference and compared to data obtained from a direct numerical solution of the governing equations. A parameter inference study highlights the ability to use these PINNs to calibrate scaling paramet
    
[^155]: 基于 PINN 的锂离子电池参数推断代理模型。第一部分：实现与多保真度等级结构用于单粒子模型

    PINN surrogate of Li-ion battery models for parameter inference. Part I: Implementation and multi-fidelity hierarchies for the single-particle model

    [https://arxiv.org/abs/2312.17329](https://arxiv.org/abs/2312.17329)

    通过PINN代理模型替代基于物理的锂离子电池模型，帮助减少计算资源，用于快速准确诊断电池内部状态。

    

    规划和优化能量存储需求需要考虑锂离子电池老化动态，因此需要开发技术准确快速地诊断电池内部状态。本研究旨在通过用基于物理的锂离子电池模型（如单粒子模型（SPM）和伪二维（P2D）模型）替代，使用物理信息神经网络（PINN）代理来减少确定电池内部状态所需的计算资源。这项研究是一项两部分系列的第一部分，介绍了用于参数推断（即健康状态诊断）的锂离子电池模型的PINN代理。在这第一部分中，提出了一个构建SPM的PINN代理的方法。通过多保真度分层训练，其中有几个神经网络

    arXiv:2312.17329v2 Announce Type: replace  Abstract: To plan and optimize energy storage demands that account for Li-ion battery aging dynamics, techniques need to be developed to diagnose battery internal states accurately and rapidly. This study seeks to reduce the computational resources needed to determine a battery's internal states by replacing physics-based Li-ion battery models -- such as the single-particle model (SPM) and the pseudo-2D (P2D) model -- with a physics-informed neural network (PINN) surrogate. The surrogate model makes high-throughput techniques, such as Bayesian calibration, tractable to determine battery internal parameters from voltage responses. This manuscript is the first of a two-part series that introduces PINN surrogates of Li-ion battery models for parameter inference (i.e., state-of-health diagnostics). In this first part, a method is presented for constructing a PINN surrogate of the SPM. A multi-fidelity hierarchical training, where several neural ne
    
[^156]: 使用分层接触网格变换器学习灵活身体碰撞动力学

    Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer

    [https://arxiv.org/abs/2312.12467](https://arxiv.org/abs/2312.12467)

    本文提出了一种使用分层网格结构的Hierarchical Contact Mesh Transformer（HCMT），能够学习长距离依赖关系，以处理灵活体动力学挑战。

    

    最近，许多基于网格的图神经网络（GNN）模型已被提出用来建模复杂的高维物理系统。与传统数值求解器相比，这些方法取得了显着的成就，大大减少了求解时间。然而，目前尚未得到充分探讨的是它们是否有效地应对灵活体动力学的挑战，即瞬时碰撞发生在极短时间内的情况下。本文提出了一种使用分层网格结构的Hierarchical Contact Mesh Transformer（HCMT），能够学习身体空间位置之间（由碰撞引起的）长距离依赖关系--在更高级别网格中的两个接近位置对应于身体中的两个远距位置。

    arXiv:2312.12467v2 Announce Type: replace-cross  Abstract: Recently, many mesh-based graph neural network (GNN) models have been proposed for modeling complex high-dimensional physical systems. Remarkable achievements have been made in significantly reducing the solving time compared to traditional numerical solvers. These methods are typically designed to i) reduce the computational cost in solving physical dynamics and/or ii) propose techniques to enhance the solution accuracy in fluid and rigid body dynamics. However, it remains under-explored whether they are effective in addressing the challenges of flexible body dynamics, where instantaneous collisions occur within a very short timeframe. In this paper, we present Hierarchical Contact Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn long-range dependencies (occurred by collisions) among spatially distant positions of a body -- two close positions in a higher-level mesh corresponds to two distant posi
    
[^157]: 通过策略引导的轨迹扩散实现世界模型

    World Models via Policy-Guided Trajectory Diffusion

    [https://arxiv.org/abs/2312.08533](https://arxiv.org/abs/2312.08533)

    这项工作提出了一个新颖的世界建模方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，通过扩散模型一次生成整个在线策略轨迹，避免了自回归模型中随着轨迹长度增长而积累的预测误差。

    

    世界模型是开发智能agent的强大工具。通过预测一系列行动的结果，世界模型使得可以通过在“想象中”使用合成数据来优化策略，即通过在线策略增强学习（RL）来实现。现有的世界模型是自回归的，因为它们在预测下一个状态的同时从策略中采样下一个行动。随着轨迹长度的增长，预测误差必然会累积。在这项工作中，我们提出了一种新颖的世界建模方法，不是自回归的，而是通过扩散模型一次生成整个在线策略轨迹。我们的方法，Policy-Guided Trajectory Diffusion (PolyGRAD)，利用了除了策略的动作分布梯度之外的一个去噪模型，将最初随机状态和动作的轨迹扩散成一个在线合成轨迹。我们分析了PolyGRAD与

    arXiv:2312.08533v3 Announce Type: replace-cross  Abstract: World models are a powerful tool for developing intelligent agents. By predicting the outcome of a sequence of actions, world models enable policies to be optimised via on-policy reinforcement learning (RL) using synthetic data, i.e. in "in imagination". Existing world models are autoregressive in that they interleave predicting the next state with sampling the next action from the policy. Prediction error inevitably compounds as the trajectory length grows. In this work, we propose a novel world modelling approach that is not autoregressive and generates entire on-policy trajectories in a single pass through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient of the action distribution of the policy to diffuse a trajectory of initially random states and actions into an on-policy synthetic trajectory. We analyse the connections between PolyGRAD,
    
[^158]: 人工神经网络与人类概念的表示

    Artificial Neural Nets and the Representation of Human Concepts

    [https://arxiv.org/abs/2312.05337](https://arxiv.org/abs/2312.05337)

    人工神经网络可以学习人类和非人类的概念，但并不是用单个单元来表示这些概念

    

    人工神经网络（ANNs）到底学习到了什么呢？机器学习（ML）社区认为，为了完成复杂任务，人工神经网络必须发展抽象的人类概念。一些人甚至认为这些概念存储在网络的单个单元中。根据当前的研究，我系统地调查了支撑这一说法的假设。我得出结论，人工神经网络的确能够执行复杂的预测任务，为此它们可能学习了人类和非人类的概念。然而，证据表明人工神经网络并没有用单个单元来表示这些概念。

    arXiv:2312.05337v2 Announce Type: replace-cross  Abstract: What do artificial neural networks (ANNs) learn? The machine learning (ML) community shares the narrative that ANNs must develop abstract human concepts to perform complex tasks. Some go even further and believe that these concepts are stored in individual units of the network. Based on current research, I systematically investigate the assumptions underlying this narrative. I conclude that ANNs are indeed capable of performing complex prediction tasks, and that they may learn human and non-human concepts to do so. However, evidence indicates that ANNs do not represent these concepts in individual units.
    
[^159]: 所有的河流都汇聚到大海：具有不对称流量的私有学习

    All Rivers Run to the Sea: Private Learning with Asymmetric Flows

    [https://arxiv.org/abs/2312.05264](https://arxiv.org/abs/2312.05264)

    提出了一种新的私有训练和推理框架Delta，通过两个不对称数据流实现了具有可比较模型性能的隐私保护。

    

    数据隐私在云机器学习服务平台中备受关注，当敏感数据暴露给服务提供商时。为了在保护隐私的同时实现高性能计算，我们提出了一种新的私有训练和推理框架Delta，具有与非私有集中训练相当的模型性能。Delta具有两个不对称的数据流：主要的信息敏感流和残差流。主要部分流入一个小模型，而残余部分则被转移到一个大模型。具体来说，Delta将信息敏感表示嵌入到低维空间中，同时将信息不敏感部分推入高维残差中。

    arXiv:2312.05264v2 Announce Type: replace-cross  Abstract: Data privacy is of great concern in cloud machine-learning service platforms, when sensitive data are exposed to service providers. While private computing environments (e.g., secure enclaves), and cryptographic approaches (e.g., homomorphic encryption) provide strong privacy protection, their computing performance still falls short compared to cloud GPUs. To achieve privacy protection with high computing performance, we propose Delta, a new private training and inference framework, with comparable model performance as non-private centralized training. Delta features two asymmetric data flows: the main information-sensitive flow and the residual flow. The main part flows into a small model while the residuals are offloaded to a large model. Specifically, Delta embeds the information-sensitive representations into a low-dimensional space while pushing the information-insensitive part into high-dimension residuals. To ensure priv
    
[^160]: DreamComposer: 通过多视角条件实现可控的3D对象生成

    DreamComposer: Controllable 3D Object Generation via Multi-View Conditions

    [https://arxiv.org/abs/2312.03611](https://arxiv.org/abs/2312.03611)

    DreamComposer是一个灵活且可扩展的框架，通过注入多视角条件增强现有视角感知扩散模型，实现可控的3D对象生成。

    

    利用预训练的2D大规模生成模型，最近的研究能够从单个in-the-wild图像生成高质量的新视图。然而，由于缺乏来自多个视角的信息，这些研究在生成可控的新视图时遇到困难。本文提出了DreamComposer，这是一个灵活且可扩展的框架，可以通过注入多视角条件来增强现有的视角感知扩散模型。具体而言，DreamComposer首先使用视角感知的3D提升模块从多个视角获取对象的3D表示。然后，它使用多视角特征融合模块从3D表示中渲染目标视图的潜在特征。最后，从多视角输入中提取的目标视图特征被注入到预训练的扩散模型中。实验证明，DreamComposer与最先进的扩散模型兼容，用于零-shot新视图sy

    arXiv:2312.03611v2 Announce Type: replace-cross  Abstract: Utilizing pre-trained 2D large-scale generative models, recent works are capable of generating high-quality novel views from a single in-the-wild image. However, due to the lack of information from multiple views, these works encounter difficulties in generating controllable novel views. In this paper, we present DreamComposer, a flexible and scalable framework that can enhance existing view-aware diffusion models by injecting multi-view conditions. Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain 3D representations of an object from multiple views. Then, it renders the latent features of the target view from 3D representations with the multi-view feature fusion module. Finally the target view features extracted from multi-view inputs are injected into a pre-trained diffusion model. Experiments show that DreamComposer is compatible with state-of-the-art diffusion models for zero-shot novel view sy
    
[^161]: 一种简单且可扩展的图生成表示

    A Simple and Scalable Representation for Graph Generation

    [https://arxiv.org/abs/2312.02230](https://arxiv.org/abs/2312.02230)

    提出了一种名为GEEL的新型、简单且可扩展的图表示，可以显著降低邻接矩阵大小和词汇量，同时通过节点位置编码实现自回归生成，并针对属性图设计了新的扩展方案。

    

    最近，人们对利用神经网络进行图生成产生了浓厚兴趣，这是一个具有关键应用价值的基本统计学习问题，如分子设计和社区分析。然而，大多数方法在生成大规模图时遇到了重大限制。这是由于它们需要输出随着节点数量呈二次增长的完整邻接矩阵。为了应对这一挑战，我们引入了一种新的、简单且可扩展的图表示，名为间隙编码边列表（GEEL），其表示大小较小且与边数量一致。此外，GEEL通过结合间隙编码和带宽限制方案显著减少了词汇量。通过加入节点位置编码，GEEL可以自回归生成，我们进一步将GEEL扩展到处理属性图，设计了一种新的语法。

    arXiv:2312.02230v2 Announce Type: replace-cross  Abstract: Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar
    
[^162]: PAC隐私保护扩散模型

    PAC Privacy Preserving Diffusion Models

    [https://arxiv.org/abs/2312.01201](https://arxiv.org/abs/2312.01201)

    提出了一种PAC隐私保护扩散模型，通过将私有分类器指导集成到采样过程中增强隐私保护，并发展了一种新的度量标准来衡量隐私水平，在保护性能方面表现出卓越表现。

    

    数据隐私保护正在引起研究人员的越来越多的关注。扩散模型（DMs），尤其是具有严格的差分隐私，有可能生成既具有高隐私性又具有良好视觉质量的图像。然而，挑战在于确保在私有化特定数据属性时的强大保护，当前模型在这些方面经常存在不足。为了解决这些挑战，我们引入了PAC隐私保护扩散模型，这是一种利用扩散原理并确保“可能大致正确（PAC）”隐私性的模型。我们通过将私有分类器指导集成到Langevin采样过程中来增强隐私保护。此外，认识到在衡量模型隐私性方面存在差距，我们开发了一种新的度量标准来衡量隐私水平。我们的模型通过这个新度量标准评估，并通过高斯矩阵计算支持PAC界限，表现出更优异的隐私性能。

    arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
    
[^163]: 寻找加速神经场训练的数据转换方法

    In Search of a Data Transformation That Accelerates Neural Field Training

    [https://arxiv.org/abs/2311.17094](https://arxiv.org/abs/2311.17094)

    随机像素置换可以显著加速神经场训练，通过消除易匹配模式来促进早期阶段的优化

    

    神经场是一种新兴的数据表示范式，通过训练神经网络来逼近给定信号。阻碍其广泛应用的一个关键障碍是生成神经场的编码速度需要神经网络的过度拟合，这可能需要大量的SGD步骤才能达到所需的保真度水平。本文探讨了数据转换对神经场训练速度的影响，特别关注对像素位置进行随机排列如何影响SGD的收敛速度。出乎意料的是，我们发现随机排列像素位置可以显著加速训练。为了解释这一现象，我们通过PSNR曲线、损失地形和错误模式来审视神经场训练。我们的分析表明，随机像素置换消除了易匹配的模式，这有助于在早期阶段进行简单的优化。

    arXiv:2311.17094v2 Announce Type: replace  Abstract: Neural field is an emerging paradigm in data representation that trains a neural network to approximate the given signal. A key obstacle that prevents its widespread adoption is the encoding speed-generating neural fields requires an overfitting of a neural network, which can take a significant number of SGD steps to reach the desired fidelity level. In this paper, we delve into the impacts of data transformations on the speed of neural field training, specifically focusing on how permuting pixel locations affect the convergence speed of SGD. Counterintuitively, we find that randomly permuting the pixel locations can considerably accelerate the training. To explain this phenomenon, we examine the neural field training through the lens of PSNR curves, loss landscapes, and error patterns. Our analyses suggest that the random pixel permutations remove the easy-to-fit patterns, which facilitate easy optimization in the early stage but hi
    
[^164]: 视频本地化指令生成的高效预训练方法

    Efficient Pre-training for Localized Instruction Generation of Videos

    [https://arxiv.org/abs/2311.15964](https://arxiv.org/abs/2311.15964)

    提出了一种名为Sieve-&-Swap的技术，通过自动筛选出不相关文本并用人类编写的说明替换文本转录，从而实现视频本地化指令生成的高效预训练。

    

    过程视频展示了诸如食谱准备等任务的逐步演示。理解此类视频具有挑战性，需要对步骤进行精确定位并生成文字说明。手动注释步骤并编写说明成本高昂，这限制了当前数据集的规模并阻碍了有效学习。利用大规模但嘈杂的视频-文本数据集进行预训练可以提升性能，但需要大量计算资源。此外，文本转录包含无关内容，与人类注释员编写的说明相比存在风格变化。为了缓解这两个问题，我们提出了一种技术，Sieve-&-Swap，通过自动筛选出不相关文本和使用文本食谱数据集中人类编写的说明自动替换文本转录以增强文字指令的质量。

    arXiv:2311.15964v2 Announce Type: replace-cross  Abstract: Procedural videos show step-by-step demonstrations of tasks like recipe preparation. Understanding such videos is challenging, involving the precise localization of steps and the generation of textual instructions. Manually annotating steps and writing instructions is costly, which limits the size of current datasets and hinders effective learning. Leveraging large but noisy video-transcript datasets for pre-training can boost performance, but demands significant computational resources. Furthermore, transcripts contain irrelevant content and exhibit style variation compared to instructions written by human annotators. To mitigate both issues, we propose a technique, Sieve-&-Swap, to automatically curate a smaller dataset: (i) Sieve filters irrelevant transcripts and (ii) Swap enhances the quality of the text instruction by automatically replacing the transcripts with human-written instructions from a text-only recipe dataset. 
    
[^165]: 将统计学习理论应用于深度学习

    Applying statistical learning theory to deep learning

    [https://arxiv.org/abs/2311.15404](https://arxiv.org/abs/2311.15404)

    深度学习的理论方面仍不清楚，本研究通过将统计学习理论应用于深度学习，探讨了不同架构在基于梯度方法训练时可能导致的归纳偏差，并详细研究了隐含偏差的数量化表示。

    

    虽然统计学习理论提供了一个理解监督学习的坚实框架，但深度学习的许多理论方面仍然不清楚，特别是在使用基于梯度的方法进行训练时，不同的架构如何导致归纳偏差。这些讲座的目标是从学习理论的角度提供了解深度学习时出现的一些主要问题的概览。在简要回顾统计学习理论和随机优化之后，我们讨论了在良性过拟合的背景下的隐含偏差。然后，我们对镜像下降算法进行一般性描述，展示了我们如何在给定学习问题的参数空间和相应的函数空间之间来回移动，以及学习问题的几何性质如何可以用度量张量表示。在这个框架的基础上，我们对数量化隐含偏差的具体研究进行了详细探讨。

    arXiv:2311.15404v2 Announce Type: replace  Abstract: Although statistical learning theory provides a robust framework to understand supervised learning, many theoretical aspects of deep learning remain unclear, in particular how different architectures may lead to inductive bias when trained using gradient based methods. The goal of these lectures is to provide an overview of some of the main questions that arise when attempting to understand deep learning from a learning theory perspective. After a brief reminder on statistical learning theory and stochastic optimization, we discuss implicit bias in the context of benign overfitting. We then move to a general description of the mirror descent algorithm, showing how we may go back and forth between a parameter space and the corresponding function space for a given learning problem, as well as how the geometry of the learning problem may be represented by a metric tensor. Building on this framework, we provide a detailed study of the im
    
[^166]: 解开霍奇拉普拉斯的谱特性：并非所有小特征值都相等

    Disentangling the Spectral Properties of the Hodge Laplacian: Not All Small Eigenvalues Are Equal

    [https://arxiv.org/abs/2311.14427](https://arxiv.org/abs/2311.14427)

    引入持续特征向量相似性的概念，并提供一种方法来跟踪霍奇拉普拉斯的个体谐波、旋度和梯度特征向量/值。

    

    图拉普拉斯的丰富谱信息对图论、机器学习和图信号处理中的应用如图分类、聚类或特征分析至关重要。最近，霍奇拉普拉斯作为普通拉普拉斯在高阶图模型（如单纯和胞复形）中的推广引起了注意。类似于传统图拉普拉斯的分析，许多作者分析霍奇拉普拉斯的最小特征值，这些特征值与重要的拓扑特性如同调有关。然而，霍奇拉普拉斯的小特征值可能携带不同信息，取决于它们是否与旋度或梯度特征模式相关联，因此可能不可比较。我们引入了持续特征向量相似性的概念，并提供了一种通过跟踪个体谐波、旋度和梯度特征向量/值的方法。

    arXiv:2311.14427v2 Announce Type: replace-cross  Abstract: The rich spectral information of the graph Laplacian has been instrumental in graph theory, machine learning, and graph signal processing for applications such as graph classification, clustering, or eigenmode analysis. Recently, the Hodge Laplacian has come into focus as a generalisation of the ordinary Laplacian for higher-order graph models such as simplicial and cellular complexes. Akin to the traditional analysis of graph Laplacians, many authors analyse the smallest eigenvalues of the Hodge Laplacian, which are connected to important topological properties such as homology. However, small eigenvalues of the Hodge Laplacian can carry different information depending on whether they are related to curl or gradient eigenmodes, and thus may not be comparable. We therefore introduce the notion of persistent eigenvector similarity and provide a method to track individual harmonic, curl, and gradient eigenvectors/-values through 
    
[^167]: 掩码自编码器是鲁棒的神经架构搜索学习器

    Masked Autoencoders Are Robust Neural Architecture Search Learners

    [https://arxiv.org/abs/2311.12086](https://arxiv.org/abs/2311.12086)

    提出了一种基于掩码自编码器的新型神经架构搜索框架，无需标记数据，在搜索过程中使用图像重建任务代替监督学习目标，具有鲁棒性、性能和泛化能力，并通过引入多尺度解码器解决了性能崩溃问题。

    

    神经架构搜索（NAS）目前严重依赖标记数据，而获取标记数据既昂贵又耗时。本文提出了一种基于掩码自编码器（MAE）的新型NAS框架，它在搜索过程中消除了对标记数据的需求。通过将监督学习目标替换为图像重建任务，我们的方法使得能够在不损害性能和泛化能力的情况下鲁棒地发现网络架构。此外，我们通过引入多尺度解码器解决了在无监督范式中广泛使用的可微架构搜索（DARTS）方法遇到的性能崩溃问题。通过在各种搜索空间和数据集上进行大量实验，我们展示了所提方法的有效性和鲁棒性，为其胜过基准方法提供了实证证据。

    arXiv:2311.12086v2 Announce Type: replace  Abstract: Neural Architecture Search (NAS) currently relies heavily on labeled data, which is both expensive and time-consuming to acquire. In this paper, we propose a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) method in the unsupervised paradigm by introducing a multi-scale decoder. Through extensive experiments conducted on various search spaces and datasets, we demonstrate the effectiveness and robustness of the proposed method, providing empirical evidence of its superiority over baseline approaches.
    
[^168]: 协同过滤的图信号扩散模型

    Graph Signal Diffusion Model for Collaborative Filtering

    [https://arxiv.org/abs/2311.08744](https://arxiv.org/abs/2311.08744)

    提出了一种用于协同过滤的图信号扩散模型，解决了现有扩散模型在建模隐式反馈数据方面的不足，通过对扩散模型进行创新改进，解决了标准扩散过程导致的个性化信息丢失和图形结构不一致等问题。

    

    协同过滤是推荐系统中的关键技术之一。在各种方法中，一种越来越受欢迎的范式是基于历史观察重建用户-物品交互。这可以被看作是一个条件生成任务，最近发展的扩散模型显示出巨大潜力。然而，现有的扩散模型研究缺乏对隐式反馈数据建模的有效解决方案。特别是，标准扩散过程的各向同性特性未能考虑物品之间的异质依赖关系，导致与交互空间的图形结构不一致。同时，随机噪声破坏了交互向量中的个性化信息，导致反向重建困难。在这篇论文中，我们对扩散模型进行了新颖的改进，并提出了用于协同过滤的图信号扩散模型（称为GiffCF）。

    arXiv:2311.08744v2 Announce Type: replace-cross  Abstract: Collaborative filtering is a critical technique in recommender systems. Among various methods, an increasingly popular paradigm is to reconstruct user-item interactions based on the historical observations. This can be viewed as a conditional generative task, where recently developed diffusion model demonstrates great potential. However, existing studies on diffusion models lack effective solutions for modeling implicit feedback data. Particularly, the isotropic nature of the standard diffusion process fails to account for the heterogeneous dependencies among items, leading to a misalignment with the graphical structure of the interaction space. Meanwhile, random noise destroying personalized information in interaction vectors, causing difficulty in reverse reconstruction. In this paper, we make novel adaptions of diffusion model and propose Graph Signal Diffusion Model for Collaborative Filtering (named GiffCF). To better repr
    
[^169]: 动态有向图上的离散分布式优化

    Discretized Distributed Optimization over Dynamic Digraphs

    [https://arxiv.org/abs/2311.07939](https://arxiv.org/abs/2311.07939)

    该论文研究了在动态有向图上进行连续时间分布式优化的离散模型，避免了链路上双随机权重设计的需要，并为分布式优化在时间变化的有向图上铺平了道路。

    

    我们考虑在动态有向图（digraphs）上进行连续时间分布式优化的离散时间模型，具有分布式学习应用。我们的优化算法适用于在切换拓扑的广义强连接动态网络上，例如移动多Agent系统和由于链路失败而不稳定的网络。与许多现有研究方向相比，在链路上没有必要进行双随机权重设计。现有文献大多需要链路权重是随机的，并使用特定的权重设计算法，在初始化和网络拓扑变化时都需要这些算法。本文消除了这些算法的需求，为随时间变化的有向图上的分布式优化铺平道路。我们导出了渐近性梯度跟踪步长和离散时间步长的收敛界，并使用共识算法证明了动态稳定性。

    arXiv:2311.07939v2 Announce Type: replace-cross  Abstract: We consider a discrete-time model of continuous-time distributed optimization over dynamic directed-graphs (digraphs) with applications to distributed learning. Our optimization algorithm works over general strongly connected dynamic networks under switching topologies, e.g., in mobile multi-agent systems and volatile networks due to link failures. Compared to many existing lines of work, there is no need for bi-stochastic weight designs on the links. The existing literature mostly needs the link weights to be stochastic using specific weight-design algorithms needed both at the initialization and at all times when the topology of the network changes. This paper eliminates the need for such algorithms and paves the way for distributed optimization over time-varying digraphs. We derive the bound on the gradient-tracking step-size and discrete time-step for convergence and prove dynamic stability using arguments from consensus al
    
[^170]: 大脑网络与智力：基于图神经网络的静息态fMRI数据方法

    Brain Networks and Intelligence: A Graph Neural Network Based Approach to Resting State fMRI Data

    [https://arxiv.org/abs/2311.03520](https://arxiv.org/abs/2311.03520)

    本文提出了一种新颖的BrainRGIN建模架构，使用图神经网络来预测智力，扩展了现有的图卷积网络并结合了聚类嵌入、图同构网络、TopK池化和基于注意力的读出函数。

    

    静息态功能磁共振成像（rsfMRI）是一种研究大脑功能和认知过程关系的强大工具，因为它可以捕获大脑的功能组织，而无需依赖于特定任务或刺激。本文提出了一种称为BrainRGIN的新颖建模架构，利用rsfMRI推导的静态功能网络连接矩阵，基于图神经网络预测智力（流体、晶体和总体智力）。我们的方法扩展了现有的图卷积网络，将聚类嵌入和图同构网络纳入到图卷积层中，以反映大脑子网络组织的性质和高效网络表达，再辅以TopK池化和基于注意力的读出函数。我们在一个大型数据集上评估了我们提出的架构。

    arXiv:2311.03520v2 Announce Type: replace-cross  Abstract: Resting-state functional magnetic resonance imaging (rsfMRI) is a powerful tool for investigating the relationship between brain function and cognitive processes as it allows for the functional organization of the brain to be captured without relying on a specific task or stimuli. In this paper, we present a novel modeling architecture called BrainRGIN for predicting intelligence (fluid, crystallized, and total intelligence) using graph neural networks on rsfMRI derived static functional network connectivity matrices. Extending from the existing graph convolution networks, our approach incorporates a clustering-based embedding and graph isomorphism network in the graph convolutional layer to reflect the nature of the brain sub-network organization and efficient network expression, in combination with TopK pooling and attention-based readout functions. We evaluated our proposed architecture on a large dataset, specifically the A
    
[^171]: 稳定线性子空间识别：一种基于机器学习的方法

    Stable Linear Subspace Identification: A Machine Learning Approach

    [https://arxiv.org/abs/2311.03197](https://arxiv.org/abs/2311.03197)

    本文提出了一种使用机器学习方法进行稳定线性子空间识别的新方法SIMBa，在稳定性和性能方面均表现优异。

    

    机器学习（ML）和线性系统识别（SI）在历史上是独立发展的。在本文中，我们利用成熟的ML工具 - 特别是自动微分框架 - 引入SIMBa，一种使用反向传播的离散线性多步预测状态空间SI方法的系列。SIMBa依赖于一种基于线性矩阵不等式的自由参数化史黛尔矩阵，以确保所识别模型的稳定性。我们展示了SIMBa如何普遍优于传统的线性状态空间SI方法，有时明显优于传统方法，尽管付出更高的计算负担。这种性能差距与其他具有稳定性保证的SI方法相比尤为显著，在我们的研究中增益通常大于25%，暗示着SIMBa同时实现了最先进的拟合性能并强制稳定。有趣的是，

    arXiv:2311.03197v4 Announce Type: replace-cross  Abstract: Machine Learning (ML) and linear System Identification (SI) have been historically developed independently. In this paper, we leverage well-established ML tools - especially the automatic differentiation framework - to introduce SIMBa, a family of discrete linear multi-step-ahead state-space SI methods using backpropagation. SIMBa relies on a novel Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure the stability of the identified model.   We show how SIMBa generally outperforms traditional linear state-space SI methods, and sometimes significantly, although at the price of a higher computational burden. This performance gap is particularly remarkable compared to other SI methods with stability guarantees, where the gain is frequently above 25% in our investigations, hinting at SIMBa's ability to simultaneously achieve state-of-the-art fitting performance and enforce stability. Interestingly, these o
    
[^172]: 具有Fisher度量的黎曼拉普拉斯逼近

    Riemannian Laplace Approximation with the Fisher Metric

    [https://arxiv.org/abs/2311.02766](https://arxiv.org/abs/2311.02766)

    黎曼拉普拉斯逼近的新方法利用Fisher度量提供更丰富的逼近族，解决了在无限数据极限下先前方法度量选择不当导致逼近过于狭窄和有偏的问题。

    

    Laplace方法用高斯分布在其模式处对目标密度进行近似。基于Bernstein-von Mises定理，它在贝叶斯推断中是计算效率高且渐近准确的，但对于复杂的目标和有限数据后验，它往往是一种过于粗糙的近似。最近对Laplace逼近的一般化是根据选择的黎曼几何对高斯近似进行转换，提供了更丰富的近似族，同时保持计算效率。然而，正如本文所示，其性质严重依赖于所选择的度量，实际上，在先前研究中采用的度量导致的逼近即使在无限数据量的极限下也过于狭窄且存在偏差。我们通过进一步发展逼近族，推导出两种在无限数据极限下精确的替代变种，扩展了理论分析。

    arXiv:2311.02766v3 Announce Type: replace  Abstract: Laplace's method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the
    
[^173]: 一种具有不精确评估和复杂性保证的非凸最小化随机算法

    A randomized algorithm for nonconvex minimization with inexact evaluations and complexity guarantees

    [https://arxiv.org/abs/2310.18841](https://arxiv.org/abs/2310.18841)

    该算法针对非凸函数的最小化问题使用了不精确的梯度和Hessian信息，通过随机选择近似的负曲率方向步进，实现近似二阶最优性，并在梯度样本复杂度上取得了改进。

    

    我们考虑使用对梯度和Hessian（不假设对函数值有访问权限）的不精确Oracle访问来最小化平滑非凸函数，以实现近似二阶最优性。我们方法的一个新特点是，如果选择一个近似的负曲率方向作为步骤，我们以相等概率选择其方向为正或负。我们允许梯度在相对意义上不精确，并放松不精确度阈值在一阶和二阶最优性条件之间的耦合。我们的收敛性分析包括基于鞍点分析的期望上界和基于浓度不等式的高概率上界。我们将我们的算法应用于经验风险最小化问题，并获得比现有作品更好的梯度样本复杂度。

    arXiv:2310.18841v2 Announce Type: replace-cross  Abstract: We consider minimization of a smooth nonconvex function with inexact oracle access to gradient and Hessian (without assuming access to the function value) to achieve approximate second-order optimality. A novel feature of our method is that if an approximate direction of negative curvature is chosen as the step, we choose its sense to be positive or negative with equal probability. We allow gradients to be inexact in a relative sense and relax the coupling between inexactness thresholds for the first- and second-order optimality conditions. Our convergence analysis includes both an expectation bound based on martingale analysis and a high-probability bound based on concentration inequalities. We apply our algorithm to empirical risk minimization problems and obtain improved gradient sample complexity over existing works.
    
[^174]: 学习提示稳定扩散模型用于语义匹配的SD4Match

    SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching

    [https://arxiv.org/abs/2310.17569](https://arxiv.org/abs/2310.17569)

    提出了SD4Match，在语义匹配中学习提示稳定扩散模型，通过提示调整技术和条件提示模块显著提高了准确性，并在多个数据集上取得了新的准确性基准。

    

    在本文中，我们解决了在图像对之间匹配语义相似关键点的挑战。现有研究表明，稳定扩散（SD）中UNet的中间输出可以作为这种匹配任务的稳健图像特征图。我们证明通过采用基本的提示调整技术，可以发挥稳定扩散的固有潜力，从而显著提高准确性。我们进一步引入了一种新颖的条件提示模块，该模块将提示条件设置为输入图像对的局部细节，这进一步提高了性能。我们将我们的方法称为SD4Match，即稳定扩散用于语义匹配。对PF-Pascal、PF-Willow和SPair-71k数据集上对SD4Match的全面评估显示，它在所有这些数据集上均设立了新的准确性基准。

    arXiv:2310.17569v2 Announce Type: replace-cross  Abstract: In this paper, we address the challenge of matching semantically similar keypoints across image pairs. Existing research indicates that the intermediate output of the UNet within the Stable Diffusion (SD) can serve as robust image feature maps for such a matching task. We demonstrate that by employing a basic prompt tuning technique, the inherent potential of Stable Diffusion can be harnessed, resulting in a significant enhancement in accuracy over previous approaches. We further introduce a novel conditional prompting module that conditions the prompt on the local details of the input image pairs, leading to a further improvement in performance. We designate our approach as SD4Match, short for Stable Diffusion for Semantic Matching. Comprehensive evaluations of SD4Match on the PF-Pascal, PF-Willow, and SPair-71k datasets show that it sets new benchmarks in accuracy across all these datasets. Particularly, SD4Match outperforms 
    
[^175]: 揭示大语言模型知识编辑的陷阱

    Unveiling the Pitfalls of Knowledge Editing for Large Language Models

    [https://arxiv.org/abs/2310.02129](https://arxiv.org/abs/2310.02129)

    这篇论文探讨了大型语言模型知识编辑的潜在陷阱，提出了新的评估方法，发现知识冲突和知识扭曲是两个重要问题。

    

    随着调整大型语言模型（LLMs）成本不断上升，最近的研究工作已经转向开发编辑LLMs内在知识的方法。然而，仍有一个阴云悬在头顶上 - 知识编辑是否会触发蝴蝶效应？因为目前尚不清楚知识编辑是否会引入可能带来潜在风险的副作用。本文首次探讨了与LLMs知识编辑相关的潜在陷阱。为实现此目的，我们引入了新的基准数据集并提出了创新性的评估指标。我们的结果强调了两个关键问题：（1）知识冲突：编辑逻辑冲突的事实组可能会放大LLMs固有的不一致性 - 这是以前方法忽略的一个方面。（2）知识扭曲：为了编辑事实知识而更改参数可能会不可逆地扭曲

    arXiv:2310.02129v3 Announce Type: replace-cross  Abstract: As the cost associated with fine-tuning Large Language Models (LLMs) continues to rise, recent research efforts have pivoted towards developing methodologies to edit implicit knowledge embedded within LLMs. Yet, there's still a dark cloud lingering overhead -- will knowledge editing trigger butterfly effect? since it is still unclear whether knowledge editing might introduce side effects that pose potential risks or not. This paper pioneers the investigation into the potential pitfalls associated with knowledge editing for LLMs. To achieve this, we introduce new benchmark datasets and propose innovative evaluation metrics. Our results underline two pivotal concerns: (1) Knowledge Conflict: Editing groups of facts that logically clash can magnify the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2) Knowledge Distortion: Altering parameters with the aim of editing factual knowledge can irrevocably warp 
    
[^176]: 通过逻辑增强大型语言模型中的零射链推理能力

    Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic

    [https://arxiv.org/abs/2309.13339](https://arxiv.org/abs/2309.13339)

    提出了LoT（Logical Thoughts）提示，一个自我改进框架，利用根植于符号逻辑的原则，特别是归谬法，逐步验证和纠正大型语言模型的零射链推理过程。

    

    大型语言模型的最新进展展示了它们在各个领域的 remarkable generalizability。然而，它们的推理能力仍有很大的提升空间，特别是在需要多步推理的情况下。尽管大型语言模型具有广泛的知识，但它们的推理经常未能有效利用这些知识来建立连贯的思维范式。这些模型有时会出现幻觉，因为它们的推理过程未受逻辑原则的限制。为了改进大型语言模型的零射链推理能力，我们提出了 LoT（Logical Thoughts）提示，这是一个自我改进的框架，利用根植于符号逻辑的原则，特别是归谬法，逐步系统地验证和纠正推理过程。在语言任务上进行的实验评估

    arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
    
[^177]: Troika: 多路径跨模态拖曳对于组合式零样本学习

    Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning

    [https://arxiv.org/abs/2303.15230](https://arxiv.org/abs/2303.15230)

    提出了一种适用于组合式零样本学习的Troika模型，通过建立三个识别分支共同对状态、对象和组合进行建模，在对齐分支特定提示表示和分解的视觉特征的同时，引入了Cross-Modal Traction模块来校准多模态表示之间的偏差。

    

    最近的组合式零样本学习（CZSL）方法通过仅为组合状态-对象对构建可训练提示来适应预训练的视觉-语言模型（VLMs）。这些方法依赖于学习已见组合的联合表示，而忽略了对状态和对象的显式建模，从而限制了对预训练知识的利用和对未见组合的泛化。在本研究中，我们特别关注解决方案的普适性，提出了一种为CZSL模型建立三个识别分支（即Multi-Path）以共同建模状态、对象和组合的新范式。所提出的Troika是我们的实现，它将分支特定的提示表示与分解的视觉特征对齐。为了校准语义上相似的多模态表示之间的偏差，我们进一步设计了一个Cross-Modal Traction模块来将提示移动到...

    arXiv:2303.15230v2 Announce Type: replace-cross  Abstract: Recent compositional zero-shot learning (CZSL) methods adapt pre-trained vision-language models (VLMs) by constructing trainable prompts only for composed state-object pairs. Relying on learning the joint representation of seen compositions, these methods ignore the explicit modeling of the state and object, thus limiting the exploitation of pre-trained knowledge and generalization to unseen compositions. With a particular focus on the universality of the solution, in this work, we propose a novel paradigm for CZSL models that establishes three identification branches (i.e., Multi-Path) to jointly model the state, object, and composition. The presented Troika is our implementation that aligns the branch-specific prompt representations with decomposed visual features. To calibrate the bias between semantically similar multi-modal representations, we further devise a Cross-Modal Traction module into Troika that shifts the prompt 
    
[^178]: 物理信息神经网络的集成学习：梯度提升方法

    Ensemble learning for Physics Informed Neural Networks: a Gradient Boosting approach

    [https://arxiv.org/abs/2302.13143](https://arxiv.org/abs/2302.13143)

    提出了一种新的训练范例“梯度提升”（GB），使用序列神经网络取得优越结果，显著提高了物理信息神经网络（PINNs）的性能，并解锁了在PINNs中应用集成学习技术的机会。

    

    虽然物理信息神经网络（PINNs）越来越受欢迎，但迄今为止，PINNs在模拟多尺度和奇异摄动问题方面并不成功。本文提出了一种名为“梯度提升”（GB）的新训练范例，大大提高了物理信息神经网络（PINNs）的性能。我们的算法不是直接使用单个神经网络学习给定PDE的解决方案，而是采用一系列神经网络来实现更好的结果。这种方法使我们能够解决传统PINNs难题。我们的数值实验证明了我们的算法的有效性，通过与有限元方法和PINNs的比较，进一步解锁了在PINNs中应用集成学习技术的机会，为进一步改进提供了可能性。

    arXiv:2302.13143v2 Announce Type: replace  Abstract: While the popularity of physics-informed neural networks (PINNs) is steadily rising, to this date, PINNs have not been successful in simulating multi-scale and singular perturbation problems. In this work, we present a new training paradigm referred to as "gradient boosting" (GB), which significantly enhances the performance of physics informed neural networks (PINNs). Rather than learning the solution of a given PDE using a single neural network directly, our algorithm employs a sequence of neural networks to achieve a superior outcome. This approach allows us to solve problems presenting great challenges for traditional PINNs. Our numerical experiments demonstrate the effectiveness of our algorithm through various benchmarks, including comparisons with finite element methods and PINNs. Furthermore, this work also unlocks the door to employing ensemble learning techniques in PINNs, providing opportunities for further improvement in 
    
[^179]: 一种隐式GNN求解器用于类泊松问题

    An Implicit GNN Solver for Poisson-like problems

    [https://arxiv.org/abs/2302.10891](https://arxiv.org/abs/2302.10891)

    通过利用隐式层理论，$\Psi$-GNN模型了一个“无限”深的网络，从而避免了经验调整所需的消息传递层次以获得解决方案。

    

    本文提出了$\Psi$-GNN，一种新颖的图神经网络（GNN）方法，用于解决具有混合边界条件的普遍泊松PDE问题。通过利用隐式层理论，$\Psi$-GNN建模了一个“无限”深的网络，从而避免了经验调整所需的消息传递层次以获得解决方案。其原始架构明确考虑了边界条件，这是物理应用的关键前提，并且能够适应任何最初提供的解决方案。 $\Psi$-GNN使用“物理信息”损失进行训练，训练过程由设计稳定，并对其初始化不敏感。此外，该方法的一致性在理论上得到了证明，并且其柔韧性和泛化效率在实验中得到了证明：相同的学习模型可以准确处理各种尺寸和不同边界条件的非结构化网格。

    arXiv:2302.10891v3 Announce Type: replace-cross  Abstract: This paper presents $\Psi$-GNN, a novel Graph Neural Network (GNN) approach for solving the ubiquitous Poisson PDE problems with mixed boundary conditions. By leveraging the Implicit Layer Theory, $\Psi$-GNN models an "infinitely" deep network, thus avoiding the empirical tuning of the number of required Message Passing layers to attain the solution. Its original architecture explicitly takes into account the boundary conditions, a critical prerequisite for physical applications, and is able to adapt to any initially provided solution. $\Psi$-GNN is trained using a "physics-informed" loss, and the training process is stable by design, and insensitive to its initialization. Furthermore, the consistency of the approach is theoretically proven, and its flexibility and generalization efficiency are experimentally demonstrated: the same learned model can accurately handle unstructured meshes of various sizes, as well as different bo
    
[^180]: 面向解释神经代码模型的因果论理论

    Toward a Theory of Causation for Interpreting Neural Code Models

    [https://arxiv.org/abs/2302.03788](https://arxiv.org/abs/2302.03788)

    该论文介绍了一种名为$do_{code}$的后验解释方法，用于解释神经代码模型的预测，基于因果推断，旨在实现面向编程语言的解释。

    

    Neural Language Models of Code，或者称为神经代码模型（NCMs），正在迅速从研究原型发展为商业开发者工具。因此，理解这些模型的能力和局限性变得至关重要。然而，这些模型的能力通常是使用自动化指标来衡量的，这些指标通常只能揭示它们真实性能的一部分。一般来说，NCMs的性能似乎很有前途，但目前关于这些模型如何做出决策仍有很多未知。因此，本文介绍了一种名为$do_{code}$的后验解释方法，该方法专门针对NCMs，能够解释模型的预测。$do_{code}$基于因果推断，以实现面向编程语言的解释。虽然$do_{code}$的理论基础可扩展到探索不同的模型属性，但我们提供了一个具体的实例，旨在减少影响...

    arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
    
[^181]: 探究安卓恶意软件检测中的特征和模型重要性：一项实施调查和机器学习方法实验比较

    Investigating Feature and Model Importance in Android Malware Detection: An Implemented Survey and Experimental Comparison of ML-Based Methods

    [https://arxiv.org/abs/2301.12778](https://arxiv.org/abs/2301.12778)

    本文重新实现和评估了18个代表性的过去研究，并在包含124,000个应用程序的平衡、相关和最新数据集上进行了新实验，发现仅通过静态分析提取的特征就能实现高达96.8%的恶意软件检测准确性。

    

    Android的普及意味着它成为恶意软件的常见目标。多年来，各种研究发现机器学习模型能够有效区分恶意软件和良性应用程序。然而，随着操作系统的演进，恶意软件也在不断发展，对先前研究的发现提出了质疑，其中许多报告称使用小型、过时且经常不平衡的数据集能够获得非常高的准确性。在本文中，我们重新实现了18项具代表性的过去研究并使用包括124,000个应用程序的平衡、相关且最新的数据集对它们进行重新评估。我们还进行了新的实验，以填补现有知识中的空白，并利用研究结果确定在当代环境中用于安卓恶意软件检测的最有效特征和模型。我们表明，仅通过静态分析提取的特征即可实现高达96.8%的检测准确性。

    arXiv:2301.12778v2 Announce Type: replace  Abstract: The popularity of Android means it is a common target for malware. Over the years, various studies have found that machine learning models can effectively discriminate malware from benign applications. However, as the operating system evolves, so does malware, bringing into question the findings of these previous studies, many of which report very high accuracies using small, outdated, and often imbalanced datasets. In this paper, we reimplement 18 representative past works and reevaluate them using a balanced, relevant, and up-to-date dataset comprising 124,000 applications. We also carry out new experiments designed to fill holes in existing knowledge, and use our findings to identify the most effective features and models to use for Android malware detection within a contemporary environment. We show that high detection accuracies (up to 96.8%) can be achieved using features extracted through static analysis alone, yielding a mode
    
[^182]: 使用三算子ADMM的联邦学习

    Federated Learning Using Three-Operator ADMM

    [https://arxiv.org/abs/2211.04152](https://arxiv.org/abs/2211.04152)

    联邦学习中引入了三算子ADMM算法，提出了利用边缘服务器丰富数据的优势，与仅使用用户数据相比有更大益处。

    

    联邦学习（FL）已经成为分布式机器学习范式的一个实例，避免了在用户端生成数据的传输。尽管数据不被传输，边缘设备仍然必须处理有限的通信带宽、数据异构性和由于用户设备的有限计算资源引起的“straggler”效应。FedADMM是克服这些困难的一个著名方法，它基于经典的双算子共识交替方向乘数法（ADMM）。FL算法的一个普遍假设是，它们仅使用用户端数据而不使用边缘服务器上的数据来学习全局模型。然而，在边缘学习中，服务器预计会靠近基站并直接访问丰富的数据集。本文认为，利用边缘服务器上的丰富数据比仅利用用户数据更有益处。

    arXiv:2211.04152v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as an instance of distributed machine learning paradigm that avoids the transmission of data generated on the users' side. Although data are not transmitted, edge devices have to deal with limited communication bandwidths, data heterogeneity, and straggler effects due to the limited computational resources of users' devices. A prominent approach to overcome such difficulties is FedADMM, which is based on the classical two-operator consensus alternating direction method of multipliers (ADMM). The common assumption of FL algorithms, including FedADMM, is that they learn a global model using data only on the users' side and not on the edge server. However, in edge learning, the server is expected to be near the base station and have direct access to rich datasets. In this paper, we argue that leveraging the rich data on the edge server is much more beneficial than utilizing only user datasets. Specifi
    
[^183]: 差分私有多变量中位数

    Differentially private multivariate medians

    [https://arxiv.org/abs/2210.06459](https://arxiv.org/abs/2210.06459)

    差分私有多变量中位数的有限样本性能保证为常用深度函数提供了尖锐的结果，表明重尾位置估计的成本超过了隐私保护成本。

    

    现代数据分析需要满足严格隐私保证的统计工具。众所周知，对污染的鲁棒性与差分隐私有关。尽管如此，使用多元中位数进行差分私有和鲁棒的多元位置估计尚未得到系统研究。我们为差分私有多元深度中位数开发了新颖的有限样本性能保证，这些保证基本上是尖锐的。我们的结果涵盖了常用的深度函数，如半平面（或Tukey）深度，空间深度和集成双深度。我们展示了在柯西边际下，重尾位置估计的代价超过了隐私的代价。我们在高达d = 100的维度上使用高斯污染模型进行了数值演示，并将其与最先进的私有均值估计算法进行了比较。作为我们研究的一个副产品，

    arXiv:2210.06459v2 Announce Type: replace-cross  Abstract: Statistical tools which satisfy rigorous privacy guarantees are necessary for modern data analysis. It is well-known that robustness against contamination is linked to differential privacy. Despite this fact, using multivariate medians for differentially private and robust multivariate location estimation has not been systematically studied. We develop novel finite-sample performance guarantees for differentially private multivariate depth-based medians, which are essentially sharp. Our results cover commonly used depth functions, such as the halfspace (or Tukey) depth, spatial depth, and the integrated dual depth. We show that under Cauchy marginals, the cost of heavy-tailed location estimation outweighs the cost of privacy. We demonstrate our results numerically using a Gaussian contamination model in dimensions up to d = 100, and compare them to a state-of-the-art private mean estimation algorithm. As a by-product of our inv
    
[^184]: P2ANet：用于从乒乓球比赛广播视频中进行稠密动作检测的数据集和基准

    P2ANet: A Dataset and Benchmark for Dense Action Detection from Table Tennis Match Broadcasting Videos

    [https://arxiv.org/abs/2207.12730](https://arxiv.org/abs/2207.12730)

    介绍了P2ANet数据集，用于从乒乓球比赛广播视频中进行稠密动作检测，包含2,721个视频剪辑；通过与专业人员合作获得了细粒度的动作标签，在14个类别中涵盖了每个乒乓球动作，提出了动作定位和动作识别两种问题，并对多种动作识别方法进行了评估。

    

    虽然深度学习被广泛应用于视频分析，如视频分类和动作检测，但在快速移动体育视频中进行稠密动作检测仍然具有挑战性。在这项工作中，我们发布了另一个体育视频基准\TheName{}，用于\emph{\underline{P}}ing \emph{\underline{P}}ong-\emph{\underline{A}}ction检测，包括从世界乒乓球锦标赛和奥林匹克运动会的专业乒乓球比赛广播视频中收集的2,721个视频剪辑。我们与乒乓球专业人员和裁判合作，使用专门设计的注释工具箱获得了数据集中出现的每个乒乓球动作的细粒度动作标签（14个类），并制定了两组动作检测问题--\emph{动作定位}和\emph{动作识别}。我们评估了许多常见的动作识别方法（例如TSM，TSN，Video SwinTr）

    arXiv:2207.12730v2 Announce Type: replace-cross  Abstract: While deep learning has been widely used for video analytics, such as video classification and action detection, dense action detection with fast-moving subjects from sports videos is still challenging. In this work, we release yet another sports video benchmark \TheName{} for \emph{\underline{P}}ing \emph{\underline{P}}ong-\emph{\underline{A}}ction detection, which consists of 2,721 video clips collected from the broadcasting videos of professional table tennis matches in World Table Tennis Championships and Olympiads. We work with a crew of table tennis professionals and referees on a specially designed annotation toolbox to obtain fine-grained action labels (in 14 classes) for every ping-pong action that appeared in the dataset, and formulate two sets of action detection problems -- \emph{action localization} and \emph{action recognition}. We evaluate a number of commonly-seen action recognition (e.g., TSM, TSN, Video SwinTr
    
[^185]: FedCau：一种用于通信和计算高效的联邦学习的主动停止策略

    FedCau: A Proactive Stop Policy for Communication and Computation Efficient Federated Learning

    [https://arxiv.org/abs/2204.07773](https://arxiv.org/abs/2204.07773)

    本文提出了一种成本感知因果FL算法（FedCau），用于处理无线网络上联邦学习模型的通信和计算高效性问题，通过迭代终止方法权衡了训练性能和网络成本。

    

    本文研究了在无线设备组成的无线网络上进行联邦学习（FL）模型的高效分布式训练。分布式训练算法的通信迭代可能受到设备背景流量、数据包丢失、拥塞或延迟等影响而大幅恶化甚至被阻断。我们将通信-计算影响抽象为“迭代成本”，并提出一种成本感知因果FL算法（FedCau）来解决这一问题。我们提出了一种迭代终止方法，权衡了训练性能和网络成本。我们在客户端使用分槽ALOHA、带冲突避免的载波监听多路访问（CSMA/CA）和正交频分多址（OFDMA）协议时应用我们的方法。我们表明，在给定总成本预算的情况下，训练性能会随着背景通信流量或特征的维度增加而降低。

    arXiv:2204.07773v2 Announce Type: replace  Abstract: This paper investigates efficient distributed training of a Federated Learning~(FL) model over a wireless network of wireless devices. The communication iterations of the distributed training algorithm may be substantially deteriorated or even blocked by the effects of the devices' background traffic, packet losses, congestion, or latency. We abstract the communication-computation impacts as an `iteration cost' and propose a cost-aware causal FL algorithm~(FedCau) to tackle this problem. We propose an iteration-termination method that trade-offs the training performance and networking costs. We apply our approach when clients use the slotted-ALOHA, the carrier-sense multiple access with collision avoidance~(CSMA/CA), and the orthogonal frequency-division multiple access~(OFDMA) protocols. We show that, given a total cost budget, the training performance degrades as either the background communication traffic or the dimension of the t
    
[^186]: 从无标签数据中进行半监督人群计数

    Semi-Supervised Crowd Counting from Unlabeled Data

    [https://arxiv.org/abs/2108.13969](https://arxiv.org/abs/2108.13969)

    提出了一种半监督学习框架$S^{4}\textit{Crowd}$，可以利用无标签数据和有标签数据进行人群计数，同时引入了自监督损失和基于人群变化的递归单元，以降低标注成本并提升性能。

    

    自动人群行为分析可以有效帮助日常交通统计和规划，从而促进智慧城市建设。 人群计数作为其中一个重要关键，吸引了越来越多的关注。 最近的研究取得了令人满意的性能，但依赖于昂贵的人群标注。 为了减轻在实际交通场景中的注释成本，在这项工作中，我们提出了一个半监督学习框架 $S^{4}\textit{Crowd}$，可以利用无标签/有标签数据进行稳健的人群计数。 在无监督通道中，提出了两个\textit{自监督损失}来模拟人群变化，如尺度，照明，基于此生成并逐渐完善了监督信息伪标签。 我们还提出了一种以人群为驱动的循环单元\textit{Gated-Crowd-Recurrent-Unit（GCRU）}，可以保留鉴别信息。

    arXiv:2108.13969v3 Announce Type: replace-cross  Abstract: Automatic Crowd behavior analysis can be applied to effectively help the daily transportation statistics and planning, which helps the smart city construction. As one of the most important keys, crowd counting has drawn increasing attention. Recent works achieved promising performance but relied on the supervised paradigm with expensive crowd annotations. To alleviate the annotation cost in real-world transportation scenarios, in this work we proposed a semi-supervised learning framework $S^{4}\textit{Crowd}$, which can leverage both unlabeled/labeled data for robust crowd counting. In the unsupervised pathway, two \textit{self-supervised losses} were proposed to simulate the crowd variations such as scale, illumination, based on which supervised information pseudo labels were generated and gradually refined. We also proposed a crowd-driven recurrent unit \textit{Gated-Crowd-Recurrent-Unit (GCRU)}, which can preserve discrimina
    
[^187]: 无需私有协方差估计的协方差感知私有均值估计

    Covariance-Aware Private Mean Estimation Without Private Covariance Estimation

    [https://arxiv.org/abs/2106.13329](https://arxiv.org/abs/2106.13329)

    提出了两种针对具有未知协方差的高斯分布的样本高效差分隐私均值估计器，无需对协方差进行估计。

    

    我们提出了两种针对具有未知协方差的$d$维(子)高斯分布的样本高效差分隐私均值估计器。简单来说，对于具有均值$\mu$和协方差$\Sigma$的这种分布，我们的估计器给出$\tilde\mu$，使得$\|\tilde\mu - \mu\|_{\Sigma} \leq \alpha$，其中$\|\cdot\|_{\Sigma}$是马氏距离。所有先前具有相同保证的估计器要么需要对协方差矩阵有强大的先验界限，要么需要$\Omega(d^{3/2})$个样本。我们的每个估计器都基于一种简单的通用方法来设计差分私有机制，但通过新颖的技术步骤使估计器具有隐私性和高效性。

    arXiv:2106.13329v3 Announce Type: replace  Abstract: We present two sample-efficient differentially private mean estimators for $d$-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given $n \gtrsim d/\alpha^2$ samples from such a distribution with mean $\mu$ and covariance $\Sigma$, our estimators output $\tilde\mu$ such that $\| \tilde\mu - \mu \|_{\Sigma} \leq \alpha$, where $\| \cdot \|_{\Sigma}$ is the Mahalanobis distance. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require $\Omega(d^{3/2})$ samples.   Each of our estimators is based on a simple, general approach to designing differentially private mechanisms, but with novel technical steps to make the estimator private and sample-efficient. Our first estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Its accuracy guarantees hold e
    
[^188]: 带可变系数的贝叶斯数据驱动的偏微分方程发现

    Bayesian data-driven discovery of partial differential equations with variable coefficients

    [https://arxiv.org/abs/2102.01432](https://arxiv.org/abs/2102.01432)

    提出了一种用于具有可变系数的偏微分方程发现的先进贝叶斯稀疏学习算法，通过阈值贝叶斯组Lasso回归和Gibbs采样器提高了稳健性和减轻了计算负担

    

    偏微分方程（PDEs）的发现是应用科学和工程的重要任务。然而，数据驱动的PDEs发现通常具有挑战性，主要是由于所发现方程对噪声的敏感性以及模型选择的复杂性。在本工作中，我们提出了一种先进的贝叶斯稀疏学习算法，用于具有可变系数的PDE发现，尤其是当系数在空间或时间上具有相关性时。具体来说，我们应用了具有尖峰和平板先验的阈值贝叶斯组Lasso回归（tBGL-SS），并利用Gibbs采样器来对PDE系数进行贝叶斯后验估计。这种方法不仅增强了点估计的稳健性并具有有效的不确定性量化，而且通过将系数阈值集成为近似MCMC方法，减轻了来自贝叶斯推断的计算负担。

    arXiv:2102.01432v2 Announce Type: replace-cross  Abstract: The discovery of Partial Differential Equations (PDEs) is an essential task for applied science and engineering. However, data-driven discovery of PDEs is generally challenging, primarily stemming from the sensitivity of the discovered equation to noise and the complexities of model selection. In this work, we propose an advanced Bayesian sparse learning algorithm for PDE discovery with variable coefficients, predominantly when the coefficients are spatially or temporally dependent. Specifically, we apply threshold Bayesian group Lasso regression with a spike-and-slab prior (tBGL-SS) and leverage a Gibbs sampler for Bayesian posterior estimation of PDE coefficients. This approach not only enhances the robustness of point estimation with valid uncertainty quantification but also relaxes the computational burden from Bayesian inference through the integration of coefficient thresholds as an approximate MCMC method. Moreover, from
    
[^189]: 多智能体强化学习中的完全独立通信

    Fully Independent Communication in Multi-Agent Reinforcement Learning. (arXiv:2401.15059v1 [cs.LG])

    [http://arxiv.org/abs/2401.15059](http://arxiv.org/abs/2401.15059)

    该论文研究了多智能体强化学习中的完全独立通信，并提出了一种新的学习方案，证明独立智能体仍可以学习通信策略。此外，论文还探讨了通信在不同网络容量下的影响。

    

    多智能体强化学习（MARL）是多智能体系统领域的一个广泛研究领域。最近的一些工作专注于研究MARL中的通信方法。虽然已经提出了多种通信方法，但这些方法可能仍然过于复杂，不容易迁移到更实际的情境中。其中一个原因是使用了著名的参数共享技巧。在本文中，我们研究了在MARL中不共享参数的独立学习者如何进行通信。我们证明了这种设置可能会带来一些问题，为此我们提出了一种新的学习方案作为解决方案。我们的结果表明，尽管面临挑战，独立的智能体仍然可以通过我们的方法学习通信策略。此外，我们使用这种方法来研究MARL中的通信如何受到不同网络容量的影响，无论是共享参数还是不共享参数。我们观察到，通信的能力在这两种情况下都产生了影响。

    Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research within the field of multi-agent systems. Several recent works have focused specifically on the study of communication approaches in MARL. While multiple communication methods have been proposed, these might still be too complex and not easily transferable to more practical contexts. One of the reasons for that is due to the use of the famous parameter sharing trick. In this paper, we investigate how independent learners in MARL that do not share parameters can communicate. We demonstrate that this setting might incur into some problems, to which we propose a new learning scheme as a solution. Our results show that, despite the challenges, independent agents can still learn communication strategies following our method. Additionally, we use this method to investigate how communication in MARL is affected by different network capacities, both for sharing and not sharing parameters. We observe that communication 
    
[^190]: 有效利用背景知识的约束k中心聚类

    Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])

    [http://arxiv.org/abs/2401.12533](http://arxiv.org/abs/2401.12533)

    本论文提出了一种在k中心聚类上利用背景知识的约束聚类算法，通过采用一系列技术，得到了效率高且具有最佳近似比例2的算法。

    

    中心为基础的聚类在理论和实践中都引起了重要的研究兴趣。在许多实际应用中，输入数据通常包含可以用于改进聚类结果的背景知识。在这项工作中，我们基于广泛采用的k中心聚类，并将其输入的背景知识建模为必连（ML）和不连（CL）约束集。然而，大多数包括k中心在内的聚类问题本质上都是NP困难的，而更复杂的受约束变体被认为受到更严重的近似和计算障碍的限制，极大地限制了它们的适用性。通过采用一系列技术，包括反支配集，线性规划（LP）整数平面和LP对偶性，我们得到了第一个具有最佳近似比例2的约束k中心的高效近似算法。我们还构建了竞争基准算法，并对我们的近似算法进行了实证评估。

    Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
    
[^191]: 通过熵最大化进行领域随机化

    Domain Randomization via Entropy Maximization. (arXiv:2311.01885v1 [cs.LG])

    [http://arxiv.org/abs/2311.01885](http://arxiv.org/abs/2311.01885)

    本文提出了一种新的领域随机化方法，通过熵最大化的方式在模拟训练中调整动力学分布，以实现模拟到真实的转移，无需真实世界数据，能够保持泛化能力和代理的高成功概率。

    

    在强化学习中，通过改变模拟中的动力学参数是一种流行的领域随机化方法，用于克服现实差距。然而，领域随机化在很大程度上依赖于动力学参数的抽样分布的选择，因为高变异对于规范代理行为至关重要，但过度随机化会导致过于保守的策略。在本文中，我们提出了一种新的方法来解决模拟到真实的转移，即在模拟训练过程中自动调整动力学分布，而无需真实世界数据。我们引入了通过熵最大化实现领域随机化的方法（DORAEMON），这是一个受限优化问题，直接最大化训练分布的熵同时保留泛化能力。为了实现这一目标，DORAEMON随着当前策略成功概率足够高，逐渐增加样本动力学参数的多样性。

    Varying dynamics parameters in simulation is a popular Domain Randomization (DR) approach for overcoming the reality gap in Reinforcement Learning (RL). Nevertheless, DR heavily hinges on the choice of the sampling distribution of the dynamics parameters, since high variability is crucial to regularize the agent's behavior but notoriously leads to overly conservative policies when randomizing excessively. In this paper, we propose a novel approach to address sim-to-real transfer, which automatically shapes dynamics distributions during training in simulation without requiring real-world data. We introduce DOmain RAndomization via Entropy MaximizatiON (DORAEMON), a constrained optimization problem that directly maximizes the entropy of the training distribution while retaining generalization capabilities. In achieving this, DORAEMON gradually increases the diversity of sampled dynamics parameters as long as the probability of success of the current policy is sufficiently high. We empiri
    
[^192]: PPI++:高效的预测驱动推理方法

    PPI++: Efficient Prediction-Powered Inference. (arXiv:2311.01453v1 [stat.ML])

    [http://arxiv.org/abs/2311.01453](http://arxiv.org/abs/2311.01453)

    PPI++是一种高效的预测驱动推理方法，通过自动调整预测质量来改善经典区间的计算置信区间的计算效率和统计效率。

    

    我们提出了PPI++：一种基于小型标记数据集和通常比较大的机器学习预测数据集的计算轻量级的估计和推理方法。该方法能够自动适应可用预测的质量，产生易于计算的置信区间 - 对于任意维度的参数 - 总是能够在只使用标记数据的情况下改善经典区间。PPI++基于预测驱动推理（PPI），针对相同的问题场景，提高了计算和统计效率。真实和合成实验证明了所提出的改进的优势。

    We present PPI++: a computationally lightweight methodology for estimation and inference based on a small labeled dataset and a typically much larger dataset of machine-learning predictions. The methods automatically adapt to the quality of available predictions, yielding easy-to-compute confidence sets -for parameters of any dimensionality -- that always improve on classical intervals using only the labeled data. PPI++ builds on prediction-powered inference (PPI), which targets the same problem setting, improving its computational and statistical efficiency. Real and synthetic experiments demonstrate the benefits of the proposed adaptations.
    
[^193]: 带有流匹配的语音生成预训练

    Generative Pre-training for Speech with Flow Matching. (arXiv:2310.16338v1 [eess.AS])

    [http://arxiv.org/abs/2310.16338](http://arxiv.org/abs/2310.16338)

    本文展示了一种使用流匹配的预训练生成模型，该模型可以适应不同的下游任务并获得强大的性能，通过在60k小时的未转录语音上进行预训练，该模型可以与现有的专家模型在语音增强、分离和合成方面进行匹配或超越。

    

    近年来，生成模型在需要估计和抽样数据分布以生成高保真合成数据的任务中取得了显著的成功，因此越来越受到关注。在语音领域，文本到语音合成和神经声码器是生成模型成功应用的典型例子。尽管生成模型已经在语音的不同应用中得到了应用，但还没有一个通用的生成模型可以直接建模语音。在本文中，我们通过展示单一的预训练生成模型可以适应不同的下游任务并获得强大的性能，迈出了这个方向的一步。具体来说，我们使用流匹配和蒙版条件在60k小时的未转录语音上预训练了一个名为SpeechFlow的生成模型。实验结果表明，预训练的生成模型可以通过特定任务数据进行微调，以在语音增强、分离和合成方面达到或超过现有的专家模型的性能。

    Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples where generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained generative model can be adapted to different downstream tasks with strong performance. Specifically, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested 
    
[^194]: COPF: 通过最优策略拟合实现持续学习人类偏好

    COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])

    [http://arxiv.org/abs/2310.15694](http://arxiv.org/abs/2310.15694)

    通过COPF方法，我们不需要重新训练预训练语言模型，而是使用最优策略拟合和函数正则化来持续学习和适应人类偏好的变化。

    

    强化学习通过人类反馈（RLHF）的技术是改善预训练语言模型（LM）以符合人类偏好的常用方法。然而，当前基于RLHF的LM在引入新的查询或反馈时需要完全重新训练，这是一项具有挑战性的任务，因为人类偏好在不同领域或任务之间可能会有所变化。由于所需的时间和计算资源以及与数据隐私相关的问题，重新训练LM在许多现实世界的情况下存在实际困难。为了解决这个限制，我们提出了一种新的方法，称为持续最优策略拟合（COPF），其中我们使用蒙特卡罗法估计一系列最优策略，然后通过函数正则化不断拟合策略序列。COPF包含一个单一的学习阶段，不需要复杂的强化学习。

    The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
    
[^195]: 上下文感知元学习

    Context-Aware Meta-Learning. (arXiv:2310.10971v1 [cs.LG])

    [http://arxiv.org/abs/2310.10971](http://arxiv.org/abs/2310.10971)

    本文提出了一种上下文感知的元学习算法，可以在推理过程中学习新的视觉概念而无需微调。该方法在多个元学习基准中表现优异，超过或与目前的最先进算法相匹配。

    

    ChatGPT等大型语言模型展示了在推理过程中无需微调就能学习新概念的卓越能力。然而，用于推理过程中检测新对象的视觉模型尚未能够复制这种能力，而是表现糟糕或需要对类似对象进行元训练和/或微调。在这项工作中，我们提出了一种元学习算法，通过在推理过程中学习新的视觉概念而无需微调来模仿大型语言模型。我们的方法利用一个冻结的预训练特征提取器，并类似于上下文学习，将元学习重新定义为在已知标签的数据点和未知标签的测试数据点上的序列建模。在11个元学习基准中的8个中，我们的方法 - 无需元训练或微调 - 超过或与在这些基准上经过元训练的最先进算法P>M>F相匹配。

    Large Language Models like ChatGPT demonstrate a remarkable capacity to learn new concepts during inference without any fine-tuning. However, visual models trained to detect new objects during inference have been unable to replicate this ability, and instead either perform poorly or require meta-training and/or fine-tuning on similar objects. In this work, we propose a meta-learning algorithm that emulates Large Language Models by learning new visual concepts during inference without fine-tuning. Our approach leverages a frozen pre-trained feature extractor, and analogous to in-context learning, recasts meta-learning as sequence modeling over datapoints with known labels and a test datapoint with an unknown label. On 8 out of 11 meta-learning benchmarks, our approach -- without meta-training or fine-tuning -- exceeds or matches the state-of-the-art algorithm, P>M>F, which is meta-trained on these benchmarks.
    
[^196]: 从词语和练习到健康：用于自我依恋技术的波斯语聊天机器人

    From Words and Exercises to Wellness: Farsi Chatbot for Self-Attachment Technique. (arXiv:2310.09362v1 [cs.HC])

    [http://arxiv.org/abs/2310.09362](http://arxiv.org/abs/2310.09362)

    这项研究开发了一个能够发出声音的波斯语聊天机器人，用于指导用户进行基于依恋理论的自我依恋技术。通过使用规则和分类模块，聊天机器人可以理解用户输入并推荐适当的自我依恋练习。该研究还开发了一种准确率超过92%的情感分析模块，以识别用户情感。这项工作有助于在后疫情时代提供数字心理疗法的替代方案。

    

    在后疫情时代，社交孤立和抑郁焦虑症的患病率攀升的背景下，基于数字心理疗法的对话代理相对于传统疗法会发挥重要的影响。本文中，我们开发了一个能够发出声音的波斯语聊天机器人，指导用户进行自我依恋(Self-Attachment, SAT)技术，这是一种基于依恋理论的新型、自我管理、全面的心理技术。我们的聊天机器人使用一系列基于规则和分类的模块来理解用户在对话中的输入，并相应地导航对话流程图，根据用户的情感和心理状态推荐适当的SAT练习。具体而言，我们收集了超过6,000次话语的数据集，并开发了一种新颖的情感分析模块，可以将用户的情感分为12个类别，准确率超过92%。为了保持对话的新颖和吸引力，聊天机器人的回答是从大量话语数据集中检索得到的。

    In the wake of the post-pandemic era, marked by social isolation and surging rates of depression and anxiety, conversational agents based on digital psychotherapy can play an influential role compared to traditional therapy sessions. In this work, we develop a voice-capable chatbot in Farsi to guide users through Self-Attachment (SAT), a novel, self-administered, holistic psychological technique based on attachment theory. Our chatbot uses a dynamic array of rule-based and classification-based modules to comprehend user input throughout the conversation and navigates a dialogue flowchart accordingly, recommending appropriate SAT exercises that depend on the user's emotional and mental state. In particular, we collect a dataset of over 6,000 utterances and develop a novel sentiment-analysis module that classifies user sentiment into 12 classes, with accuracy above 92%. To keep the conversation novel and engaging, the chatbot's responses are retrieved from a large dataset of utterances c
    
[^197]: Entropy-MCMC: 轻松从平坦盆地进行采样

    Entropy-MCMC: Sampling from Flat Basins with Ease. (arXiv:2310.05401v1 [cs.LG] CROSS LISTED)

    [http://arxiv.org/abs/2310.05401](http://arxiv.org/abs/2310.05401)

    本文提出了一种Entropy-MCMC的方法，通过引入一个辅助的引导变量来在平坦盆地中进行采样，以解决深度神经网络后验分布的多模态问题，并证明了该方法的收敛性。

    

    贝叶斯深度学习依赖于对后验分布的质量估计。然而，深度神经网络的后验分布在性质上是高度多模态的，局部模式表现出不同的泛化性能。在有限的计算资源下，从原始后验分布中进行采样可能会导致次优性能，因为一些样本可能会陷入“坏”模式并出现过拟合。基于观察到低泛化误差的“好”模式通常存在于能量景观的平坦盆地中，我们提出通过偏置采样朝向这些平坦区域的后验。具体而言，我们引入了一个辅助引导变量，其稳态分布类似于平滑后验分布，并且没有尖锐的模态，以引导MCMC采样器在平坦的盆地中采样。通过将此引导变量与模型参数相结合，我们创建了一个简单的联合分布，可以在最小计算开销下实现高效采样。我们证明了我们的元算法的收敛性。

    Bayesian deep learning counts on the quality of posterior distribution estimation. However, the posterior of deep neural networks is highly multi-modal in nature, with local modes exhibiting varying generalization performance. Given a practical budget, sampling from the original posterior can lead to suboptimal performance, as some samples may become trapped in "bad" modes and suffer from overfitting. Leveraging the observation that "good" modes with low generalization error often reside in flat basins of the energy landscape, we propose to bias sampling on the posterior toward these flat regions. Specifically, we introduce an auxiliary guiding variable, the stationary distribution of which resembles a smoothed posterior free from sharp modes, to lead the MCMC sampler to flat basins. By integrating this guiding variable with the model parameter, we create a simple joint distribution that enables efficient sampling with minimal computational overhead. We prove the convergence of our met
    
[^198]: 双圆锥代理用于交流最优功率流问题

    Dual Conic Proxies for AC Optimal Power Flow. (arXiv:2310.02969v1 [cs.LG])

    [http://arxiv.org/abs/2310.02969](http://arxiv.org/abs/2310.02969)

    本文提出了一种基于双圆锥代理的方法来求解交流最优功率流问题，并通过自监督学习方案来辅助训练，实验证明了该方法的效率和可扩展性。

    

    近年来，人们对基于机器学习的交流最优功率流问题（AC-OPF）优化代理的发展表现出了极大的兴趣。虽然在预测高质量原始解方面取得了显著进展，但现有的基于学习的方法无法为AC-OPF提供有效的对偶界限。本文通过训练AC-OPF的一个凸松弛的优化代理来填补这一空白。具体而言，本文考虑了AC-OPF的二阶圆锥（SOC）松弛，并提出了一种新的对偶架构，嵌入了一个快速、可微分的（对偶）可行性恢复，从而提供有效的对偶界限。本文将这种新架构与自监督学习方案相结合，减轻了昂贵的训练数据生成需求。对中等和大规模电力网络进行了大量的数值实验，证明了所提方法的效率和可扩展性。

    In recent years, there has been significant interest in the development of machine learning-based optimization proxies for AC Optimal Power Flow (AC-OPF). Although significant progress has been achieved in predicting high-quality primal solutions, no existing learning-based approach can provide valid dual bounds for AC-OPF. This paper addresses this gap by training optimization proxies for a convex relaxation of AC-OPF. Namely, the paper considers a second-order cone (SOC) relaxation of ACOPF, and proposes a novel dual architecture that embeds a fast, differentiable (dual) feasibility recovery, thus providing valid dual bounds. The paper combines this new architecture with a self-supervised learning scheme, which alleviates the need for costly training data generation. Extensive numerical experiments on medium- and large-scale power grids demonstrate the efficiency and scalability of the proposed methodology.
    
[^199]: 使用谐波控制李亚普诺夫障碍函数解决有约束的最优控制问题与避障要求

    Harmonic Control Lyapunov Barrier Functions for Constrained Optimal Control with Reach-Avoid Specifications. (arXiv:2310.02869v1 [math.OC])

    [http://arxiv.org/abs/2310.02869](http://arxiv.org/abs/2310.02869)

    该论文介绍了谐波控制李亚普诺夫障碍函数（harmonic CLBF），它可以在约束控制问题中解决避障要求，通过最大化系统动力学与谐波CLBF最陡下降方向的内积来选择控制输入，从而显著降低进入不安全区域的风险并提高进入目标区域的概率。

    

    本文介绍了谐波控制李亚普诺夫障碍函数（harmonic CLBF），它有助于解决诸如避障问题等的约束控制问题。谐波CLBF利用谐波函数满足的最大值原理来编码控制李亚普诺夫障碍函数的属性。因此，它们可以在实验开始时初始化，而不是基于样本轨迹进行训练。控制输入被选择为最大化系统动力学与谐波CLBF最陡下降方向的内积。数值结果在不同的避障环境下展示了四个不同系统的情况。谐波CLBF显示出进入不安全区域的风险显著降低，进入目标区域的概率高。

    This paper introduces harmonic control Lyapunov barrier functions (harmonic CLBF) that aid in constrained control problems such as reach-avoid problems. Harmonic CLBFs exploit the maximum principle that harmonic functions satisfy to encode the properties of control Lyapunov barrier functions (CLBFs). As a result, they can be initiated at the start of an experiment rather than trained based on sample trajectories. The control inputs are selected to maximize the inner product of the system dynamics with the steepest descent direction of the harmonic CLBF. Numerical results are presented with four different systems under different reach-avoid environments. Harmonic CLBFs show a significantly low risk of entering unsafe regions and a high probability of entering the goal region.
    
[^200]: 概率重连的消息传递神经网络

    Probabilistically Rewired Message-Passing Neural Networks. (arXiv:2310.02156v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.02156](http://arxiv.org/abs/2310.02156)

    PR-MPNNs通过概率重连学习加入相关边，并省略对预测任务没有帮助的边，从而增强了表达能力。

    

    消息传递图神经网络（MPNN）作为处理图结构输入的强大工具而出现。然而，它们在固定的输入图结构上操作，忽略了潜在的噪声和缺失信息。此外，它们的局部聚合机制可能导致问题，如过度压缩和在捕捉相关图结构方面的有限表达能力。现有的解决这些挑战的方法主要依赖于启发式方法，往往忽视了底层数据分布。因此，设计了一种原则性的方法，用于学习推断与给定预测任务相关的图结构，仍然是一个未解决的挑战。在这项工作中，利用了最近在精确和可微分的k-子集采样方面的进展，我们设计了概率重连的MPNN (PR-MPNN)，它们学习在省略对预测任务没有帮助的边的同时添加相关的边。我们的理论分析首次探索了PR-MPNN如何增强表达能力，并且我们确定了确切的条件。

    Message-passing graph neural networks (MPNNs) emerged as powerful tools for processing graph-structured input. However, they operate on a fixed input graph structure, ignoring potential noise and missing information. Furthermore, their local aggregation mechanism can lead to problems such as over-squashing and limited expressive power in capturing relevant graph structures. Existing solutions to these challenges have primarily relied on heuristic methods, often disregarding the underlying data distribution. Hence, devising principled approaches for learning to infer graph structures relevant to the given prediction task remains an open challenge. In this work, leveraging recent progress in exact and differentiable $k$-subset sampling, we devise probabilistically rewired MPNNs (PR-MPNNs), which learn to add relevant edges while omitting less beneficial ones. For the first time, our theoretical analysis explores how PR-MPNNs enhance expressive power, and we identify precise conditions un
    
[^201]: Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])

    Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction. (arXiv:2309.15798v1 [cs.LG])

    [http://arxiv.org/abs/2309.15798](http://arxiv.org/abs/2309.15798)

    Node-Aligned Graph-to-Graph (NAG2G)是一个基于transformer的无模板模型，利用2D分子图和3D构象信息，能够更好地利用分子的拓扑信息和对齐原子，提高单步反合成预测的竞争力。

    

    单步反合成是有机化学和药物设计中至关重要的任务，需要识别出合成特定化合物所需的反应物。随着计算机辅助合成规划的出现，越来越多的人开始关注使用机器学习技术来简化这个过程。现有的无模板机器学习模型通常使用transformer结构，并将分子表示为ID序列。然而，这些方法往往在充分利用分子的广泛拓扑信息和在生成物和反应物之间对齐原子方面面临挑战，导致结果不如半模板模型竞争力强。我们提出的方法，Node-Aligned Graph-to-Graph (NAG2G)，也是一个基于transformer的无模板模型，但是利用了2D分子图和3D构象信息。此外，我们的方法通过利用n

    Single-step retrosynthesis is a crucial task in organic chemistry and drug design, requiring the identification of required reactants to synthesize a specific compound. with the advent of computer-aided synthesis planning, there is growing interest in using machine-learning techniques to facilitate the process. Existing template-free machine learning-based models typically utilize transformer structures and represent molecules as ID sequences. However, these methods often face challenges in fully leveraging the extensive topological information of the molecule and aligning atoms between the production and reactants, leading to results that are not as competitive as those of semi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G), also serves as a transformer-based template-free model but utilizes 2D molecular graphs and 3D conformation information. Furthermore, our approach simplifies the incorporation of production-reactant atom mapping alignment by leveraging n
    
[^202]: 通过行为和遗传特征整合改进阿片类药物使用障碍风险建模

    Improving Opioid Use Disorder Risk Modelling through Behavioral and Genetic Feature Integration. (arXiv:2309.10837v1 [q-bio.QM])

    [http://arxiv.org/abs/2309.10837](http://arxiv.org/abs/2309.10837)

    通过整合与阿片类药物使用障碍有关的遗传变异和行为特征，该论文开发了一种实验设计和计算方法，用于评估阿片类药物使用障碍的风险。结果显示整合遗传和迁移模式可以改善风险估计的能力。

    

    阿片类药物是一种有效的急性和慢性疼痛止痛药，但也带来了相当大的成瘾风险，导致美国每年数百万阿片类药物使用障碍（OUD）病例和数万人的过早死亡。在处方之前估计OUD风险可以改进治疗方案的效果、监测计划和干预策略，但风险估计通常基于自我报告的数据或调查问卷。我们开发了实验设计和计算方法，将与OUD相关的遗传变异与从GPS和Wi-Fi时空坐标中提取的行为特征结合起来评估OUD风险。由于OUD迁移和遗传数据不存在于同一个群体中，我们开发了算法来(1)根据经验分布生成迁移特征和(2)合成迁移和遗传样本，假设存在某种共病和相对风险水平。我们展示了整合遗传和迁移模式可以改善风险估计的能力。

    Opioids are an effective analgesic for acute and chronic pain, but also carry a considerable risk of addiction leading to millions of opioid use disorder (OUD) cases and tens of thousands of premature deaths in the United States yearly. Estimating OUD risk prior to prescription could improve the efficacy of treatment regimens, monitoring programs, and intervention strategies, but risk estimation is typically based on self-reported data or questionnaires. We develop an experimental design and computational methods that combines genetic variants associated with OUD with behavioral features extracted from GPS and Wi-Fi spatiotemporal coordinates to assess OUD risk. Since both OUD mobility and genetic data do not exist for the same cohort, we develop algorithms to (1) generate mobility features from empirical distributions and (2) synthesize mobility and genetic samples assuming a level of comorbidity and relative risks. We show that integrating genetic and mobility modalities improves ris
    
[^203]: 具有增强课程强化学习的众包感知多智能体路径规划研究

    Crowd-Aware Multi-Agent Pathfinding With Boosted Curriculum Reinforcement Learning. (arXiv:2309.10275v1 [cs.RO])

    [http://arxiv.org/abs/2309.10275](http://arxiv.org/abs/2309.10275)

    该论文介绍了CRAMP，一种基于增强式课程强化学习的众包感知分散式路径规划方法，旨在解决拥挤环境下多智能体路径规划的困难。

    

    在拥挤环境中进行的多智能体路径规划是一个具有挑战性的运动规划问题，旨在为系统中的所有智能体找到无碰撞路径。多智能体路径规划在各个领域中都有广泛的应用，包括空中群体、自动化仓储机器人和自动驾驶车辆。当前的多智能体路径规划方法可以大致分为两种主要类别：集中式规划和分散式规划。集中式规划受到维度灾难的困扰，因此在大型和复杂环境中不具备良好的可扩展性。另一方面，分散式规划使智能体能够在部分可观察环境中进行实时路径规划，展示了隐式的协调能力。然而，在密集环境中它们的收敛速度较慢且性能下降。本文介绍了一种名为CRAMP的众包感知分散式方法，通过增强式课程引导的强化学习来解决这个问题。

    Multi-Agent Path Finding (MAPF) in crowded environments presents a challenging problem in motion planning, aiming to find collision-free paths for all agents in the system. MAPF finds a wide range of applications in various domains, including aerial swarms, autonomous warehouse robotics, and self-driving vehicles. The current approaches for MAPF can be broadly categorized into two main categories: centralized and decentralized planning. Centralized planning suffers from the curse of dimensionality and thus does not scale well in large and complex environments. On the other hand, decentralized planning enables agents to engage in real-time path planning within a partially observable environment, demonstrating implicit coordination. However, they suffer from slow convergence and performance degradation in dense environments. In this paper, we introduce CRAMP, a crowd-aware decentralized approach to address this problem by leveraging reinforcement learning guided by a boosted curriculum-b
    
[^204]: 随机森林中的预测误差估计

    Prediction Error Estimation in Random Forests. (arXiv:2309.00736v1 [stat.ML])

    [http://arxiv.org/abs/2309.00736](http://arxiv.org/abs/2309.00736)

    本文通过量化评估分类随机森林的误差估计方法，发现随机森林的预测误差估计比平均预测误差更接近真实误差率，并且这一结果适用于不同的误差估计策略。

    

    本文定量评估了分类随机森林的误差估计。在Bates等人（2023年）建立的初步理论框架的基础上，从理论和经验角度探讨了随机森林中常见的各种误差估计方法在真实误差率和期望误差率方面的情况。我们发现，在分类情况下，随机森林的预测误差估计平均更接近真实误差率，而不是平均预测误差。与Bates等人（2023年）对逻辑回归的研究结果相反。我们进一步证明，这个结果适用于交叉验证、自举和数据划分等不同的误差估计策略。

    In this paper, error estimates of classification Random Forests are quantitatively assessed. Based on the initial theoretical framework built by Bates et al. (2023), the true error rate and expected error rate are theoretically and empirically investigated in the context of a variety of error estimation methods common to Random Forests. We show that in the classification case, Random Forests' estimates of prediction error is closer on average to the true error rate instead of the average prediction error. This is opposite the findings of Bates et al. (2023) which were given for logistic regression. We further show that this result holds across different error estimation strategies such as cross-validation, bagging, and data splitting.
    
[^205]: RetroBridge: 使用马尔可夫桥模型进行反合成建模

    RetroBridge: Modeling Retrosynthesis with Markov Bridges. (arXiv:2308.16212v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.16212](http://arxiv.org/abs/2308.16212)

    本研究提出了一种基于马尔可夫桥模型的反合成建模方法，通过对两个难以处理的离散分布之间的依赖关系进行近似，直接生成可能的前体分子，为反合成规划提供了准确的预测和置信度估计。

    

    反合成规划是化学中的一项基本挑战，旨在从市售起始材料设计反应路径到目标分子。多步反合成规划中的每一步都需要准确预测给定目标分子的可能前体分子，并给出置信度估计以指导启发式搜索算法。我们将单步反合成规划建模为离散状态空间中的分布学习问题。首先，我们引入了马尔可夫桥模型，一种生成框架，旨在近似两个难以处理的离散分布之间的依赖关系，通过有限样本的耦合数据点进行访问。我们的框架基于马尔可夫桥的概念，即以其端点为初始点固定的马尔可夫过程。与基于扩散的方法不同，我们的马尔可夫桥模型不需要作为采样代理的可追踪噪声分布，并直接使用输入产物分子作为来自难以处理的先验分布的样本进行操作。

    Retrosynthesis planning is a fundamental challenge in chemistry which aims at designing reaction pathways from commercially available starting materials to a target molecule. Each step in multi-step retrosynthesis planning requires accurate prediction of possible precursor molecules given the target molecule and confidence estimates to guide heuristic search algorithms. We model single-step retrosynthesis planning as a distribution learning problem in a discrete state space. First, we introduce the Markov Bridge Model, a generative framework aimed to approximate the dependency between two intractable discrete distributions accessible via a finite sample of coupled data points. Our framework is based on the concept of a Markov bridge, a Markov process pinned at its endpoints. Unlike diffusion-based methods, our Markov Bridge Model does not need a tractable noise distribution as a sampling proxy and directly operates on the input product molecules as samples from the intractable prior di
    
[^206]: 用于稀疏深度神经网络训练的多目标优化

    Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v1 [cs.LG])

    [http://arxiv.org/abs/2308.12243](http://arxiv.org/abs/2308.12243)

    这项工作提出了一种用于稀疏深度神经网络训练的多目标优化算法，通过加权Chebyshev标量化和增广Lagrangian方法，解决了同时优化多个任务时的挑战。

    

    在深度学习的各种场景中，会自然地出现不同的冲突优化准则。这些准则可以解决不同的主任务（如多任务学习设置），也可以解决主要任务和次要任务，例如损失最小化与稀疏性。通常的方法是简单地加权准则，但在凸设置中才有效。本文提出了一种多目标优化算法，对多任务深度神经网络（DNNs）进行训练，使用改进的加权Chebyshev标量化方法。通过使用这种标量化技术，算法可以识别原始问题的所有最优解，同时将其复杂性降低为一系列单目标问题。然后，使用增广Lagrangian方法来解决简化后的问题，从而可以使用常见的优化技术，如Adam和随机梯度下降，同时有效地处理约束条件。我们的工作旨在解决经济化问题。

    Different conflicting optimization criteria arise naturally in various Deep Learning scenarios. These can address different main tasks (i.e., in the setting of Multi-Task Learning), but also main and secondary tasks such as loss minimization versus sparsity. The usual approach is a simple weighting of the criteria, which formally only works in the convex setting. In this paper, we present a Multi-Objective Optimization algorithm using a modified Weighted Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect to several tasks. By employing this scalarization technique, the algorithm can identify all optimal solutions of the original problem while reducing its complexity to a sequence of single-objective problems. The simplified problems are then solved using an Augmented Lagrangian method, enabling the use of popular optimization techniques such as Adam and Stochastic Gradient Descent, while efficaciously handling constraints. Our work aims to address the (economi
    
[^207]: 领域感知微调：增强神经网络的适应性能力

    Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v1 [cs.LG])

    [http://arxiv.org/abs/2308.07728](http://arxiv.org/abs/2308.07728)

    本文提出了领域感知微调（DAFT）方法，通过批归一化转换和线性探测与微调的集成，有效减轻微调过程中的特征畸变问题。

    

    微调预训练神经网络模型已成为各个领域广泛采用的方法。然而，微调可能导致已具备强大泛化能力的预训练特征提取器发生畸变。在适应新目标领域时减轻特征畸变至关重要。最近的研究表明，在进行微调之前，在分布数据集上对头层进行对齐处理可以处理特征畸变问题取得有希望的结果。然而，在微调过程中，批归一化层的处理存在显著局限性，导致性能不佳。在本文中，我们提出了领域感知微调（DAFT），一种新的方法，它结合了批归一化转换、线性探测和微调的特性。我们的批归一化转换方法通过减少对神经网络的修改来有效减轻特征畸变。此外，我们还引入了线性探测和微调的集成方法。

    Fine-tuning pre-trained neural network models has become a widely adopted approach across various domains. However, it can lead to the distortion of pre-trained feature extractors that already possess strong generalization capabilities. Mitigating feature distortion during adaptation to new target domains is crucial. Recent studies have shown promising results in handling feature distortion by aligning the head layer on in-distribution datasets before performing fine-tuning. Nonetheless, a significant limitation arises from the treatment of batch normalization layers during fine-tuning, leading to suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning (DAFT), a novel approach that incorporates batch normalization conversion and the integration of linear probing and fine-tuning. Our batch normalization conversion method effectively mitigates feature distortion by reducing modifications to the neural network during fine-tuning. Additionally, we introduce the integrati
    
[^208]: 运动规划扩散：基于扩散模型的机器人运动学习与规划

    Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models. (arXiv:2308.01557v1 [cs.RO])

    [http://arxiv.org/abs/2308.01557](http://arxiv.org/abs/2308.01557)

    本文提出了一种新的机器人运动学习与规划方法，通过学习扩散模型作为先验知识，可以加速运动规划优化过程。扩散模型能够在高维环境中有效地编码数据的多模态性，并可以直接从任务目标条件下的后验轨迹分布中进行采样。

    

    学习轨迹分布的先验知识可以加快机器人运动规划的优化。在给定先前成功的规划方案的情况下，学习轨迹生成模型作为新规划问题的先验知识是非常理想的。之前的研究提出了几种利用这种先验知识进行运动规划问题引导的方法。可以通过从先验知识中采样初始化，或者在最大后验优化的过程中使用先验分布。在本文中，我们提出了学习扩散模型作为先验知识的方法。然后，我们可以通过利用扩散模型的逆去噪过程，直接从任务目标条件下的后验轨迹分布中采样。此外，最近的研究表明，扩散在高维环境中可以有效地编码数据的多模态性，这对于大量的轨迹数据集非常适用。为了展示我们的方法的有效性，我们将我们提出的方法-运动规划扩散与几种基准方发进行了比较。

    Learning priors on trajectory distributions can help accelerate robot motion planning optimization. Given previously successful plans, learning trajectory generative models as priors for a new planning problem is highly desirable. Prior works propose several ways on utilizing this prior to bootstrapping the motion planning problem. Either sampling the prior for initializations or using the prior distribution in a maximum-a-posterior formulation for trajectory optimization. In this work, we propose learning diffusion models as priors. We then can sample directly from the posterior trajectory distribution conditioned on task goals, by leveraging the inverse denoising process of diffusion models. Furthermore, diffusion has been recently shown to effectively encode data multimodality in high-dimensional settings, which is particularly well-suited for large trajectory dataset. To demonstrate our method efficacy, we compare our proposed method - Motion Planning Diffusion - against several ba
    
[^209]: RL$^3$:通过RL内部的RL$^2$提升元强化学习方法

    RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])

    [http://arxiv.org/abs/2306.15909](http://arxiv.org/abs/2306.15909)

    RL$^3$是一种原则性混合方法，通过将传统强化学习学到的任务特定动作值作为元强化学习神经网络的输入，提高了元强化学习的性能。

    

    元强化学习（meta-RL）方法，如RL$^2$，已经成为学习针对给定任务分布的数据高效的强化学习算法的有希望的方法。然而，这些强化学习算法在长期任务和超出分布任务方面存在困难，因为它们依赖于递归神经网络来处理经验序列，而不是将它们总结为一般的强化学习组件，例如价值函数。此外，即使是transformers在训练和推理成本变得禁止之前也对它们可以有效推理的历史长度有实际限制。相比之下，传统的强化学习算法在数据效率方面不足，因为它们没有利用领域知识，但随着更多数据的可用性，它们会收敛到最优策略。在本文中，我们提出了RL$^3$，一种组合了传统强化学习和元强化学习的原则性混合方法，通过将通过传统强化学习学习到的特定任务动作值作为元强化学习神经网络的一个输入。

    Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
    
[^210]: Omega: 乐观EMA Gradients

    Omega: Optimistic EMA Gradients. (arXiv:2306.07905v1 [cs.LG])

    [http://arxiv.org/abs/2306.07905](http://arxiv.org/abs/2306.07905)

    Omega是一种优化算法，通过加入历史梯度EMA来减轻噪声的影响并在随机游戏上表现更佳。

    

    随着GAN和对抗性训练的进步，随机min-max优化受到了机器学习界的关注。尽管确定性状态下的博弈优化已经相当好地理解了，但在随机状态下仍存在一些问题。最近的研究表明，像乐观梯度这样的随机梯度下降-上升方法对噪声非常敏感或者会导致失败。虽然存在替代策略，但这些策略可能成本过高。我们引入了Omega，一种具有类似于乐观更新的方法，通过在其更新规则中合并历史梯度的EMA来减轻噪声的影响。我们还探讨了一种包含动量的该算法的变体。虽然我们没有提供收敛性保证，但我们在随机游戏上的实验表明，当应用于线性玩家时，Omega优于乐观梯度方法。

    Stochastic min-max optimization has gained interest in the machine learning community with the advancements in GANs and adversarial training. Although game optimization is fairly well understood in the deterministic setting, some issues persist in the stochastic regime. Recent work has shown that stochastic gradient descent-ascent methods such as the optimistic gradient are highly sensitive to noise or can fail to converge. Although alternative strategies exist, they can be prohibitively expensive. We introduce Omega, a method with optimistic-like updates that mitigates the impact of noise by incorporating an EMA of historic gradients in its update rule. We also explore a variation of this algorithm that incorporates momentum. Although we do not provide convergence guarantees, our experiments on stochastic games show that Omega outperforms the optimistic gradient method when applied to linear players.
    
[^211]: 一种用于评估图像识别模型鲁棒性的差分测试框架

    A Differential Testing Framework to Evaluate Image Recognition Model Robustness. (arXiv:2306.06208v1 [cs.CV])

    [http://arxiv.org/abs/2306.06208](http://arxiv.org/abs/2306.06208)

    本文提出了一种差分测试框架，用于评估图像识别模型鲁棒性。该框架通过使用一组参考图像并扰动计算环境，可确定模型性能是否受到计算环境变化的影响。

    

    图像识别任务通常使用深度学习，并需要巨大的处理能力，因此依赖于GPU和TPU等硬件加速器进行快速、及时的处理。在模型部署过程中，硬件加速器上的子优映射可能会导致实时图像识别任务失败，从而导致时间不确定性和错误行为。硬件加速器上的映射是通过多个软件组件进行的，例如深度学习框架、编译器、设备库等，我们称之为计算环境。随着图像识别任务在自动驾驶和医疗成像等安全关键应用中的增加，评估它们对计算环境变化的鲁棒性至关重要，因为深度学习框架、编译器优化和硬件设备等参数对模型性能和正确性的影响还不太清楚。在本文中，我们提出了一种差分测试框架，用于评估图像识别模型对计算环境变化的鲁棒性。我们的框架使用一组参考图像，并通过更改软件组件来扰动计算环境，生成具有已知预测输出差异的图像。通过比较原始图像和扰动图像的预测输出，我们可以确定模型性能是否受到计算环境变化的影响。我们通过测试三个图像识别模型的鲁棒性来证明我们框架的有效性，并确定其在计算环境变化下的性能受到影响的情况。

    Image recognition tasks typically use deep learning and require enormous processing power, thus relying on hardware accelerators like GPUs and TPUs for fast, timely processing. Failure in real-time image recognition tasks can occur due to sub-optimal mapping on hardware accelerators during model deployment, which may lead to timing uncertainty and erroneous behavior. Mapping on hardware accelerators is done through multiple software components like deep learning frameworks, compilers, device libraries, that we refer to as the computational environment. Owing to the increased use of image recognition tasks in safety-critical applications like autonomous driving and medical imaging, it is imperative to assess their robustness to changes in the computational environment, as the impact of parameters like deep learning frameworks, compiler optimizations, and hardware devices on model performance and correctness is not well understood.  In this paper we present a differential testing framewo
    
[^212]: 图像识别模型框架转换的故障定位

    Fault Localization for Framework Conversions of Image Recognition Models. (arXiv:2306.06157v1 [cs.CV])

    [http://arxiv.org/abs/2306.06157](http://arxiv.org/abs/2306.06157)

    本文提出针对深度学习框架转换中出现的模型崩溃和输出标签差异的故障定位和修复方法，成功修复多个图像识别模型跨多个深度学习框架的转换错误。

    

    在部署深度神经网络（DNNs）时，开发人员经常将模型从一个深度学习框架转换为另一个（例如，从TensorFlow到PyTorch）。然而，这个过程容易出错，并可能影响目标模型的准确性。为了确定这种影响的程度，我们对三个用于图像识别的DNNs（MobileNetV2、ResNet101和InceptionV3）进行了不同的分析，这些模型在四个深度学习框架（PyTorch、Keras、TensorFlow（TF）和TFLite）之间进行了转换，并发现了许多模型崩溃和输出标签差异高达100％。为了缓解这种错误，我们提出了一种新的方法来定位故障和修复有缺陷的深度学习框架转换，重点放在预训练的图像识别模型上。我们的技术包括四个主要分析阶段：1）转换工具，2）模型参数，3）模型超参数，4）图表示。此外，我们提出了许多针对故障定位和修复的策略，包括转换工具的推荐、调试技巧以及模型超参数的微调。我们通过成功修复所有测试的深度学习框架中MobileNetV2，ResNet101和InceptionV3 的有缺陷的转换来展示我们方法的有效性。

    When deploying Deep Neural Networks (DNNs), developers often convert models from one deep learning framework to another (e.g., TensorFlow to PyTorch). However, this process is error-prone and can impact target model accuracy. To identify the extent of such impact, we perform and briefly present a differential analysis against three DNNs used for image recognition (MobileNetV2, ResNet101, and InceptionV3), converted across four well-known deep learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which revealed numerous model crashes and output label discrepancies of up to 100%. To mitigate such errors, we present a novel approach towards fault localization and repair of buggy deep learning framework conversions, focusing on pre-trained image recognition models. Our technique consists of four primary stages of analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters, and 4) graph representation. In addition, we propose a number of strategies towards faul
    
[^213]: 基于联邦学习的代码异味检测方法(FedCSD)

    FedCSD: A Federated Learning Based Approach for Code-Smell Detection. (arXiv:2306.00038v1 [cs.SE])

    [http://arxiv.org/abs/2306.00038](http://arxiv.org/abs/2306.00038)

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。该方法在三个实验中分别使用不同的数据集，实现了高精度的检测效果，并且比集中式和交叉验证方法具有更好的性能表现。

    

    本文提出了一种名为FedCSD的基于联邦学习的代码异味检测方法，可以在保护数据隐私的同时，让组织协作训练联邦学习模型。通过三个实验来支持这些断言，这些实验利用了三个手动验证的数据集，来检测和研究不同的代码异味场景。

    This paper proposes a Federated Learning Code Smell Detection (FedCSD) approach that allows organizations to collaboratively train federated ML models while preserving their data privacy. These assertions have been supported by three experiments that have significantly leveraged three manually validated datasets aimed at detecting and examining different code smell scenarios. In experiment 1, which was concerned with a centralized training experiment, dataset two achieved the lowest accuracy (92.30%) with fewer smells, while datasets one and three achieved the highest accuracy with a slight difference (98.90% and 99.5%, respectively). This was followed by experiment 2, which was concerned with cross-evaluation, where each ML model was trained using one dataset, which was then evaluated over the other two datasets. Results from this experiment show a significant drop in the model's accuracy (lowest accuracy: 63.80\%) where fewer smells exist in the training dataset, which has a noticeab
    
[^214]: 基于$K^2$-树的分级图生成

    Hierarchical Graph Generation with $K^2$-trees. (arXiv:2305.19125v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2305.19125](http://arxiv.org/abs/2305.19125)

    本文介绍了一种基于$K^2$-树的图生成方法，该方法可以实现紧凑生成，并同时捕获图的内在分层结构。通过提出顺序$K^2$-树表示和引入基于Transformer的架构，本文进一步改进了这种方法。实验表明，该方法在图生成方面具有卓越的表现。

    

    从目标分布生成图是许多领域的一个重大挑战，包括药物发现和社交网络分析。在本文中，我们介绍了一种利用原本设计用于无损图压缩的$K^2$-树表示的新颖图生成方法。我们的动机源于$K^2$-树能够在进行紧凑生成的同时，捕获图的内在分层结构的能力。此外，我们还通过(1)提出了一种包含剪枝、扁平化和记号化过程的顺序K2树表示和(2)引入了一种基于Transformer的架构，旨在通过结合专业树形位置编码方案来生成序列。最后，我们对四个常规和两个分子图数据集进行了广泛的评估，以证实我们的算法在图生成方面的优越性。

    Generating graphs from a target distribution is a significant challenge across many domains, including drug discovery and social network analysis. In this work, we introduce a novel graph generation method leveraging $K^2$-tree representation which was originally designed for lossless graph compression. Our motivation stems from the ability of the $K^2$-trees to enable compact generation while concurrently capturing the inherent hierarchical structure of a graph. In addition, we make further contributions by (1) presenting a sequential $K^2$-tree representation that incorporates pruning, flattening, and tokenization processes and (2) introducing a Transformer-based architecture designed to generate the sequence by incorporating a specialized tree positional encoding scheme. Finally, we extensively evaluate our algorithm on four general and two molecular graph datasets to confirm its superiority for graph generation.
    
[^215]: 保留你自己的相关性：用于视频扩散模型的噪声先验

    Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models. (arXiv:2305.10474v1 [cs.CV])

    [http://arxiv.org/abs/2305.10474](http://arxiv.org/abs/2305.10474)

    本论文介绍了一种新的视频噪声先验，用于微调图像扩散模型，以实现更高质量的视频合成。经过广泛的实验验证，该模型已经取得了UCF-101和MSR-VTT基准测试的最佳结果。

    

    尽管扩散模型在生成高质量图像方面取得了巨大进展，但合成连续的动画帧，既具有光真实感，又具有时间相关性仍处于起步阶段。在可以使用成亿级图像数据集的同时，收集相似规模的视频数据仍然具有挑战性。此外，与其图像对应的模型相比，训练视频扩散模型的计算代价更高。在本文中，我们探讨了使用视频数据微调预训练的图像扩散模型作为视频合成任务的实用解决方案。我们发现，在视频扩散中天真地将图像噪声先验扩展为视频噪声先验会导致次优的性能。我们设计了一种精心设计的视频噪声先验，其在视频扩散中具有显著的更好性能。广泛的实验验证表明，我们的模型 Preserve Your Own Correlation (PYoCo) 在 UCF-101 和 MSR-VTT 基准测试中获得了零样本文本对视频的最佳结果。

    Despite tremendous progress in generating high-quality images using diffusion models, synthesizing a sequence of animated frames that are both photorealistic and temporally coherent is still in its infancy. While off-the-shelf billion-scale datasets for image generation are available, collecting similar video data of the same scale is still challenging. Also, training a video diffusion model is computationally much more expensive than its image counterpart. In this work, we explore finetuning a pretrained image diffusion model with video data as a practical solution for the video synthesis task. We find that naively extending the image noise prior to video noise prior in video diffusion leads to sub-optimal performance. Our carefully designed video noise prior leads to substantially better performance. Extensive experimental validation shows that our model, Preserve Your Own Correlation (PYoCo), attains SOTA zero-shot text-to-video results on the UCF-101 and MSR-VTT benchmarks. It also
    
[^216]: ChatGPT 需要进行SPADE（可持续性、隐私、数字鸿沟和伦理）评估：一项综述。

    ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])

    [http://arxiv.org/abs/2305.03123](http://arxiv.org/abs/2305.03123)

    本文研究关注ChatGPT面临的可持续性、隐私、数字鸿沟和伦理问题，提出了SPADE评估的必要性，并给出了缓解和建议。

    

    ChatGPT是另一个大型语言模型（LLM），由于其性能和有效的对话能力，在研究和工业界中得到了巨大的关注。最近，许多研究已经发表，以展示ChatGPT和其他LLMs的有效性、效率、集成和情感。相反，本研究关注的是大多数被忽视的重要方面，即可持续性、隐私、数字鸿沟和伦理，并建议不仅仅是ChatGPT，而是在对话机器人类别中的每一个后续入口都应该进行SPADE评估。本文详细讨论了关于ChatGPT的问题和关注点与上述特征一致。我们通过一些初步的数据收集和可视化以及假设的事实来支持我们的假设。我们还为每个问题提出了缓解和建议。此外，我们还提供了一些未来方向和开放问题的探讨。

    ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
    
[^217]: 会话树搜索：一项新的混合对话任务

    Conversational Tree Search: A New Hybrid Dialog Task. (arXiv:2303.10227v1 [cs.CL])

    [http://arxiv.org/abs/2303.10227](http://arxiv.org/abs/2303.10227)

    本文介绍了一项新的任务——会话树搜索(CTS)，它可以架起FAQ和对话之间的桥梁，领域专家可以定义对话树，然后将其转换为一个有效的对话策略，只学习提出导航用户达到目标所需的问题。

    

    对话接口提供了一种灵活和方便的方式，让用户获取原本可能难以或不方便获得的信息。然而，现有的界面大体上可以分为两种类型：FAQ，用户必须提出明确的问题以检索一般的答案；或者对话，用户必须遵循预定义的路径但可能会接收到个性化的答案。本文介绍了一种新的任务——会话树搜索(CTS)，它架起了信息检索风格FAQ和面向任务对话之间的桥梁，允许领域专家定义对话树，然后将其转换为一个有效的对话策略，只学习提出导航用户达到目标所需的问题。我们收集了旅行报销领域的数据集，并展示了这项任务的基线（baseline）以及一项新颖的深度增强学习架构。结果显示，新的架构综合了FAQ和对话的优点，取得了良好的效果。

    Conversational interfaces provide a flexible and easy way for users to seek information that may otherwise be difficult or inconvenient to obtain. However, existing interfaces generally fall into one of two categories: FAQs, where users must have a concrete question in order to retrieve a general answer, or dialogs, where users must follow a predefined path but may receive a personalized answer. In this paper, we introduce Conversational Tree Search (CTS) as a new task that bridges the gap between FAQ-style information retrieval and task-oriented dialog, allowing domain-experts to define dialog trees which can then be converted to an efficient dialog policy that learns only to ask the questions necessary to navigate a user to their goal. We collect a dataset for the travel reimbursement domain and demonstrate a baseline as well as a novel deep Reinforcement Learning architecture for this task. Our results show that the new architecture combines the positive aspects of both the FAQ and 
    
[^218]: 对基于扩散的生成模型的最优控制视角

    An optimal control perspective on diffusion-based generative modeling. (arXiv:2211.01364v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2211.01364](http://arxiv.org/abs/2211.01364)

    本文建立了随机最优控制与基于扩散的生成模型之间的联系，推导了用于控制潜在SDE边际密度演化的汉密尔顿-雅可比-贝尔曼方程，并将生成建模表述为对合适度量之间Kullback-Leibler散度的最小化。此外，作者还开发了一种新型扩散方法用于采样非归一化密度。

    

    我们建立了随机最优控制与基于随机微分方程（SDE）的生成模型之间的联系，例如最近发展起来的扩散概率模型。特别地，我们推导出一个汉密尔顿-雅可比-贝尔曼方程，用于控制潜在的SDE边际密度的演化。这个视角允许将最优控制理论的方法应用于生成建模中。首先，我们展示了证据下界是控制理论中广为人知的验证定理的直接结果。此外，我们可以将基于扩散的生成建模表述为路径空间中合适度量之间的Kullback-Leibler散度的最小化。最后，我们开发了一种从非归一化密度中进行采样的新型扩散方法，这在统计学和计算科学中经常出现的问题。我们证明了我们的时序反向扩散采样器（DIS）可以胜过其他基于扩散的采样方法。

    We establish a connection between stochastic optimal control and generative models based on stochastic differential equations (SDEs), such as recently developed diffusion probabilistic models. In particular, we derive a Hamilton-Jacobi-Bellman equation that governs the evolution of the log-densities of the underlying SDE marginals. This perspective allows to transfer methods from optimal control theory to generative modeling. First, we show that the evidence lower bound is a direct consequence of the well-known verification theorem from control theory. Further, we can formulate diffusion-based generative modeling as a minimization of the Kullback-Leibler divergence between suitable measures in path space. Finally, we develop a novel diffusion-based method for sampling from unnormalized densities -- a problem frequently occurring in statistics and computational sciences. We demonstrate that our time-reversed diffusion sampler (DIS) can outperform other diffusion-based sampling approache
    
[^219]: 观点市场模型：利用积极干预来遏制极右派观点的传播

    Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions. (arXiv:2208.06620v2 [cs.SI] UPDATED)

    [http://arxiv.org/abs/2208.06620](http://arxiv.org/abs/2208.06620)

    本研究提出了观点市场模型（OMM），通过引入积极干预来遏制极右派观点的传播。这个模型将观点的关注市场规模建模，并考虑了观点之间的相互作用和竞争，旨在评估积极干预的有效性。

    

    在线极端主义具有严重的社会后果，包括将仇恨言论合理化、用户的激进化以及社会分裂的加剧。已经探索了各种缓解策略来应对这些后果。其中一种策略使用积极干预：控制信号来增加对观点生态系统的关注，以提升某些观点。为了评估积极干预的有效性，我们引入了观点市场模型（OMM），这是一个考虑到观点间相互作用和积极干预作用的两层在线观点生态系统模型。观点的关注市场规模使用多元离散时间Hawkes过程在第一层进行建模；在第二层中，观点在有限的关注度下合作和竞争以获得市场份额，使用市场份额吸引模型。通过合成数据集展示了我们提出的估计方案的收敛性。接下来，我们在两个真实世界的学习任务上测试了OMM

    Online extremism has severe societal consequences, including normalizing hate speech, user radicalization, and increased social divisions. Various mitigation strategies have been explored to address these consequences. One such strategy uses positive interventions: controlled signals that add attention to the opinion ecosystem to boost certain opinions. To evaluate the effectiveness of positive interventions, we introduce the Opinion Market Model (OMM), a two-tier online opinion ecosystem model that considers both inter-opinion interactions and the role of positive interventions. The size of the opinion attention market is modeled in the first tier using the multivariate discrete-time Hawkes process; in the second tier, opinions cooperate and compete for market share, given limited attention using the market share attraction model. We demonstrate the convergence of our proposed estimation scheme on a synthetic dataset. Next, we test OMM on two learning tasks, applying to two real-world
    
[^220]: 一种轻量级且梯度稳定的神经层

    A Lightweight and Gradient-Stable Nerual Layer. (arXiv:2106.04088v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2106.04088](http://arxiv.org/abs/2106.04088)

    Han层是一种梯度稳定、参数更少的神经层结构，可以替换全连接层来优化神经网络模型。

    

    我们提出了一种基于Householder权重和绝对值激活的神经层结构，因此被称为Householder-absolute神经层或简称Han层。与具有$d$个神经元和$d$个输出的全连接层相比，Han层将参数数量和相应的复杂度从$O（d ^ 2）$降低到$O（d）$。Han层结构保证了两个理想属性：（1）梯度稳定性（不会出现梯度消失或梯度爆炸），以及（2）1-Lipschitz连续性。广泛的数值实验表明，可以有策略地使用Han层替换全连接（FC）层，从而减少模型参数的数量，同时保持或甚至提高泛化性能。我们将展示Han层结构在一些小型化的模型上的能力，同时讨论其当前的限制。

    We propose a neural-layer architecture based on Householder weighting and absolute-value activating, hence called Householder-absolute neural layer or simply Han-layer. Compared to a fully-connected layer with $d$-neurons and $d$ outputs, a Han-layer reduces the number of parameters and the corresponding complexity from $O(d^2)$ to $O(d)$. The Han-layer structure guarantees two desirable properties: (1) gradient stability (free of vanishing or exploding gradient), and (2) 1-Lipschitz continuity. Extensive numerical experiments show that one can strategically use Han-layers to replace fully-connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will showcase the capabilities of the Han-layer architecture on a few small stylized models, and also discuss its current limitations.
    

